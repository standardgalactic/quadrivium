An Introduction to Queueing 
Theory and Matrix-Analytic Methods
by
L. BREUER
University of Trier, Germany
and
University of Trier, Germany
D. BAUM

A C.I.P. Catalogue record for this book is available from the Library of Congress.
P.O. Box 17, 3300 AA Dordrecht, The Netherlands.
Printed on acid-free paper
All Rights Reserved
© 2005 Springer 
No part of this work may be reproduced, stored in a retrieval system, or transmitted
in any form or by any means, electronic, mechanical, photocopying, microfilming, recording
or otherwise, without written permission from the Publisher, with the exception
of any material supplied specifically for the purpose of being entered
and executed on a computer system, for exclusive use by the purchaser of the work.
Printed in the Netherlands.
www.springeronline.com
Published by Springer,
ISBN  978-1-4020-3630-9 (HB)
ISBN  978-1-4020-3631-6 (e-book)
ISBN  1-4020-3630-2 (HB)
ISBN  1-4020-3631-0 (e-book)

Contents
List of Figures
ix
Foreword
xi
1. QUEUES: THE ART OF MODELLING
1
Part I
Markovian Methods
2. MARKOV CHAINS AND QUEUES IN DISCRETE TIME
11
1
Deﬁnition
11
2
Classiﬁcation of States
15
3
Stationary Distributions
20
4
Restricted Markov Chains
27
5
Conditions for Positive Recurrence
29
6
The M/M/1 queue in discrete time
31
3. HOMOGENEOUS MARKOV PROCESSES ON DISCRETE
STATE SPACES
39
1
Deﬁnition
39
2
Stationary Distribution
46
4. MARKOVIAN QUEUES IN CONTINUOUS TIME
51
1
The M/M/1 Queue
51
2
Skip–Free Markov Processes
54
3
The M/M/∞Queue
55
4
The M/M/k Queue
56
5
The M/M/k/k Queue
58
6
The M/M/k/k+c/N Queue
59

vi
AN INTRODUCTION TO QUEUEING THEORY
5. MARKOVIAN QUEUEING NETWORKS
63
1
Balance Equations and Reversibility Properties
65
2
Jackson and Gordon-Newell Networks
80
3
Symmetric Service Disciplines
99
Part II
Semi–Markovian Methods
6. RENEWAL THEORY
113
1
Renewal Processes
113
2
Renewal Function and Renewal Equations
116
3
Renewal Theorems
118
4
Residual Life Times and Stationary Renewal Processes
124
5
Renewal Reward Processes
130
7. MARKOV RENEWAL THEORY
135
1
Regenerative Processes
135
2
Semi–Markov Processes
138
3
Semi–regenerative Processes
144
8. SEMI–MARKOVIAN QUEUES
147
1
The GI/M/1 Queue
147
2
The M/G/1 Queue
155
3
The GI/M/m Queue
160
Part III
Matrix–Analytic Methods
9. PHASE–TYPE DISTRIBUTIONS
169
1
Motivation
169
2
Deﬁnition and Examples
171
3
Moments
176
4
Closure Properties
178
10. MARKOVIAN ARRIVAL PROCESSES
185
1
The PH renewal process
185
2
From PH renewal processes to MAPs
187
3
From MAPs to BMAPs
188
4
Distribution of the Number of Arrivals
190
5
Expected Number of Arrivals
192

Contents
vii
11. THE GI/PH/1 QUEUE
197
1
The Embedded Markov Chain
198
2
Stationary Distribution at Arrival Instants
199
3
Ergodicity of the Embedded Markov Chain
204
4
Asymptotic Distribution of the System Process
208
12. THE BMAP/G/1 QUEUE
213
1
The Embedded Markov Chain
214
2
The Matrix G
215
3
Stationary Distribution at Service Completions
216
4
Asymptotic Distribution of the System Process
218
5
Stability Conditions
224
13. DISCRETE TIME APPROACHES
229
1
Discrete Phase–Type Distributions
229
2
232
3
Blockwise Skip–Free Markov Chains
234
4
The PH/PH/1 Queue in Discrete Time
236
14. SPATIAL MARKOVIAN ARRIVAL PROCESSES
239
1
Arrivals in Space
240
2
Properties of Spatial MAPs
245
15. APPENDIX
253
1
Conditional Expectations and Probabilities
253
2
Extension Theorems
256
3
Transforms
258
4
260
References
263
Index
269
BMAPs in Discrete Time
Gershgorin’s Circle Theorem

List of Figures
1.1
Single server queue
2
1.2
Multi–server queue
2
1.3
Total system time
6
3.1
Typical path
40
3.2
Poisson process
43
4.1
M/M/1 queue
51
4.2
Transition rates for the M/M/1 queue
52
4.3
A skip–free Markov process
54
4.4
M/M/k queue
57
4.5
A closed computer network
60
5.1
Open Queueing Network
64
5.2
Modiﬁed Network
91
5.3
Central Server Model
96
5.4
Modiﬁed Central Server Model
98
5.5
FCFS Order
103
5.6
Cox Distribution
103
5.7
LCFS Order
104
5.8
Simple model of a computer pool
108
6.1
Random variables of a renewal process
113
7.1
139
8.1
Fix point as intersection with diagonal
150
9.1
Erlang distribution
174
9.2
Generalized Erlang distribution
175
9.3
Hyper–exponential distribution
175
9.4
Cox distribution
176
Typical
T
path of a semi-Markov process

x
AN INTRODUCTION TO QUEUEING THEORY
9.5
Convolution of two PH distributions
179
9.6
Mixture of two PH distributions
180
9.7
Superposition of two PH distributions
182

Foreword
The present textbook contains the records of a two–semester course on queue-
ing theory, including an introduction to matrix–analytic methods. This course
comprises four hours of lectures and two hours of exercises per week and has
been taught at the University of Trier, Germany, for about ten years in se-
quence. The course is directed to last year undergraduate and ﬁrst year grad-
uate students of applied probability and computer science, who have already
completed an introduction to probability theory. Its purpose is to present ma-
terial that is close enough to concrete queueing models and their applications,
while providing a sound mathematical foundation for the analysis of these.
Thus the goal of the present book is two–fold.
On the one hand, students who are mainly interested in applications easily
feel bored by elaborate mathematical questions in the theory of stochastic
processes. The presentation of the mathematical foundations in our courses
is chosen to cover only the necessary results, which are needed for a solid
foundation of the methods of queueing analysis. Further, students oriented to-
wards applications expect to have a justiﬁcation for their mathematical efforts
in terms of immediate use in queueing analysis. This is the main reason why
we have decided to introduce new mathematical concepts only when they will
be used in the immediate sequel.
On the other hand, students of applied probability do not want any heuris-
tic derivations just for the sake of yielding fast results for the model at hand.
They want to see the close connections between queueing theory and the theory
of stochastic processes. For them, a systematic introduction to the necessary
concepts of Markov renewal theory is indispensable. Further, they are not in-
terested in any technical details of queueing applications, but want to see the
reﬂection of the mathematical concepts in the queueing model as purely as
possible.

xii
AN INTRODUCTION TO QUEUEING THEORY
A prominent part of the book will be devoted to matrix–analytic methods. This
is a collection of approaches which extend the applicability of Markov renewal
methods to queueing theory by introducing a ﬁnite number of auxiliary states.
For the embedded Markov chains this leads to transition matrices in block form
having the same structure as the classical models. With a few modiﬁcations
they can be analyzed in the same way.
Matrix–analytic methods have become quite popular in queueing theory dur-
ing the last twenty years. The intention to include these in a students’ intro-
duction to queueing theory has been the main motivation for the authors to
write the present book. Its aim is a presentation of the most important matrix–
analytic concepts like phase–type distributions, Markovian arrival processes,
the GI/PH/1 and BMAP/G/1 queues as well as QBDs and discrete time ap-
proaches. This is the content of part III of this book.
As an introductory course for students it is necessary to provide the required
results from Markov renewal theory before. This is done in part I, which con-
tains Markovian theory, and part II which combines the concepts of part I with
renewal theory in order to obtain a foundation for Markov renewal theory. Cer-
tainly only few students would like to acquire this theoretical body without
some motivating applications in classical queueing theory. These are intro-
duced as soon as the necessary theoretical background is provided.
The book is organized as follows. The ﬁrst chapter gives a short overview of
the diverse application areas for queueing theory and deﬁnes queues and their
system processes (number of users in the system). The appendix sections in
chapter 15 provide an easy reference to some basic concepts of analysis and
probability theory.
For the simple Markovian queueing models (in discrete and continuous time)
it sufﬁces to give a short introduction to Markov chains and processes, and
then present an analysis of some queueing examples. This is done in chapters
2 through 4. Chapter 5 gives an introduction to the analysis of simple queue-
ing networks, in particular Jackson and Gordon–Newell networks as well as
BCMP networks. This concludes the ﬁrst part of the book, which deals with
Markovian methods exclusively.
The second part is devoted to semi–Markovian methods. In chapter 6 the most
important results of renewal theory are provided. Chapter 7 contains a short
introduction to Markov renewal theory. This will be necessary for the analy-
sis of the classical semi–Markovian queues (namely the GI/M/1 and M/G/1
systems), which is presented in chapter 8.
More recent approaches which are usually subsumed under the term ”matrix–
analytic methods” are presented in the third part of the book. In chapters

Foreword
xiii
9 and 10 the basic concepts of phase–type distributions and Markovian ar-
rival processes are introduced. The matrix–analytic analogues to the GI/M/1
and M/G/1 queues, namely the GI/PH/1 and BMAP/G/1 systems are analyzed
in chapters 11 and 12. Chapter 13 gives a short overview on discrete time
analogues. Further blockwise skip–free Markov chains, also known as QBD
processes, are analyzed, with an application to the PH/PH/1 queue in dis-
crete time. Finally, in chapter 14 a generalization of BMAPs towards spatial
Markovian arrival processes is presented.
Of course, most of the more classical material can be found in existing text-
books on stochastic processes. For example, C¸ inlar [25] and Ross [75] still
C
contain, in our view, the most systematic treatment of semi–Markovian queues.
Also of great value, mostly for the theory of Markov chains and processes, are
the courses on stochastic processes by Karlin and Taylor [46, 47]. Further im-
portant results may be found in Doob [31], Asmussen [5], and Nelson [61].
The material on queueing networks can be found in Mitrani [60], Kelly [48],
and Kleinrock [50]. Monographs on matrix–analytic methods are the pioneer-
ing books by Neuts [65, 66], and Latouche and Ramaswami [52]. For discrete
time methods the overview paper by Alfa [2] was helpful.
However, some aspects of standard presentation have been changed in order to
alleviate the mathematical burden for the students. The stationary regime for
Markov chains has been introduced as an asymptotic mean over time in order
to avoid the introduction of periodicity of states. The deﬁnition of Markov
processes in chapter 3 is much closer to the derivation of immediate results. It
is not necessary to derive the standard path properties in lengthy preliminary
analyses, since these are already included in the deﬁnition. Nevertheless, the
close connection between the phenomena observed in queueing systems and
the deﬁnition given in our textbook is immediately clear to the student.
The introduction of renewal theory has been postponed to the second part of the
book in order to show a variety of queueing application of a purely Markovian
nature ﬁrst. The drawback that a proof for asymptotic behaviour of Markov
processes must be deferred appears bearable for an average student. The proof
of Blackwell’s theorem, and thus also for the equivalent key renewal theorem,
has been omitted as it is too technical for a student presentation in the authors’
opinion. The same holds for proofs regarding the necessity of the stability
condition for the queues GI/PH/1 and BMAP/G/1. Only proofs for sufﬁciency
have been included because they are easily based on the classical Foster crite-
ria.
At the end of each chapter there will be a collection of exercises, some of them
representing necessary auxiliary results to complete the proofs presented in

xiv
AN INTRODUCTION TO QUEUEING THEORY
the lectures. Additional material is given as exercises, too, e.g. examples of
computer networks or certain special queueing system.
The book is written according to the actual scripts of the lecture courses given
at the University of Trier, Germany. It is intended not only to collect material
which can be used for an introductory course on queueing theory, but to pro-
pose the scripts of the lectures themselves. The book contains exactly as much
material as the authors (as lecturers) could present in two semesters. Thus a
lecturer using this textbook does not need to choose and reassemble the ma-
terial for a course from sources which must be shortened because there is no
time to treat them completely. This entails saving the work of reformulating
notations and checking dependencies. For a course of only one semester we
propose to teach parts I and II of this book, leaving out sections 5.3 and 8.3.

Chapter 1
QUEUES: THE ART OF MODELLING
Stochastic modelling is the application of probability theory to the descrip-
tion and analysis of real world phenomena. It is thus comparable to physics,
with the distinguishing property that mostly technical and not natural systems
are investigated. These are usually so complex that deterministic laws cannot
be formulated, a circumstance that leads to pervasive use of stochastic con-
cepts. Application ﬁelds as telecommunication or insurance bring methods
and results of stochastic modelling to the attention of applied sciences such
as engineering or economics. On the other hand, often new technological de-
velopments give rise to new questions in an application ﬁeld, which in turn
may open a new direction in stochastic research, and thus provide an impetus
to applied probability. Stochastic modelling is a science with close interaction
between theory and practical applications. This is nice because it combines the
possibility of theoretical beauty with a real–world meaning of its key concepts.
On the other hand, it is difﬁcult to cover the whole width from theoretical foun-
dations to the details of practical applications. The present book is an essay to
give an introduction to the theory of stochastic modelling in a systematic way
without losing contact to its applicability.
One of the most important domains in stochastic modelling is the ﬁeld of
queueing theory. This shall be the topic of this treatise. Many real systems
can be reduced to components which can be modelled by the concept of a
so–called queue. The basic idea of this concept has been borrowed from the
every–day experience of the queues at the checkout counters in a supermarket.
A queue in the more exact scientiﬁc sense consists of a system into which there
comes a stream of users who demand some capacity of the system over a cer-
tain time interval before they leave the system again. It is said that the users are
served in the system by one or many servers. Thus a queueing system can be

2
AN INTRODUCTION TO QUEUEING THEORY
described by a (stochastic) speciﬁcation of the arrival stream and of the system
demand for every user as well as a deﬁnition of the service mechanism. The
former describe the input into a queue, while the latter represents the function-
ing of the inner mechanisms of a queueing system. Before we give an exact
deﬁnition of a queueing system, a few examples shall provide an idea of the
variety of applications.
Example 1.1 Single Server Queue
server
queue
arrival process
departure
process
Figure 1.1.
Single server queue
A queue in front of the checkout counter of a supermarket may serve as the
simplest illustration for a queueing system. There is one input stream, and one
server who serves the customers in order of their appearance at the counter.
This service discipline, which does not admit any preferences among users, is
called ﬁrst come ﬁrst served (abbr.: FCFS).
Example 1.2 Multi–Server Queue
queue
arrival process
departure
 process
servers
Figure 1.2.
Multi–server queue

Queues: The Art of Modelling
3
The ﬁrst real application of queueing theory, in fact the one that engendered
the development of the whole ﬁeld of research, has been the design and analy-
sis of telephone networks. In the early days of Erlang at the beginning of the
20th century, telephone calls ﬁrst went to an operator before they could be
conected to the person that was to be reached by the call. Thus an important
part of a telephone network could be modelled by a queueing system in which
the servers are the operators in a call center who connect the incoming calls
(which are modelled by the input stream of users) to their addressees. Here,
the time of connecting is represented by the service demand of a user. A cru-
cial performance measure of such a system is the probability that a person who
wants to get a connection for a call ﬁnds all operators busy and thus cannot
be served. This value is called the loss probability of the system. For a mod-
ern call center, where questions are answered instead of cables connected, the
service times represent the time of the call between the user and the operator.
Example 1.3 In recent times, computer networks (the most prominent exam-
ple is the internet) have increasingly become the object of applications of
queueing theory. For example, a server in a computer network receives de-
mands from its clients and needs to serve them. The demands make up the
input stream into the queueing sytem that represents the server utilization. A
service discipline that is often used in these kinds of application is the follow-
ing: The processing capacity of the server is divided into equal parts among
the jobs such that none of the clients is favoured, but each client’s service
time depends on the total number of clients that are resident at the same time.
Because of its prevalence in computer applications, this service discipline is
called processor sharing.
Example 1.4 Queues ﬁnd further applications in airport trafﬁc.
Here, the
servers are the several landing ﬁelds available for arriving airplanes, while the
latter are the users of the system. Obviously, there cannot be any queue of
planes waiting in the air, so that an arriving airplane ﬁnding all landing ﬁelds
in use needs instead to ﬂy an extra circle around the airport and then try again
for a possibility to land. Such a manoeuver is called a retrial, and the corre-
sponding queueing model is called a retrial queue. Since with every extra circle
that a plane has to perform its gasoline is reduced more, the priority of such
an aircraft to obtain a landing permission is increasing and should be higher
than that of more recent airplanes with fewer retrials. Such an inﬂuence on the
service schedule is called priority queueing.
Example 1.5 More complicated queueing models have been developed for the
design of trafﬁc lights at crossroads. In such a model, there are several distin-
guishable queues which represent the different roads leading to the intersec-
tion. A green light at one road means that vehicles waiting on it are served

4
AN INTRODUCTION TO QUEUEING THEORY
on a ﬁrst come ﬁrst served base. There are as many servers as there are trafﬁc
lights at the intersection, and it is obvious that these servers must function in
dependence on each other. Such queueing systems are called polling systems.
Example 1.6 In modern production systems an analysis of assembly lines has
become a fundamental necessity.
They are modelled by so–called tandem
queueing networks, which are deﬁned as a series of several single queueing
systems where the output of one queue forms the input of the next.
Example 1.7 Finally, perhaps the most interesting object of analysis for to-
day’s computer science, the internet, would merely appear as a highly complex
queueing network, at least so from the point of view of stochastic modelling.
These examples illustrate the very different interpretations and thus applica-
tions that queueing systems can assume. They should sufﬁce as a motivation
to undergo the strain of developing methods and concepts for the analysis of
queueing systems of the highest possible complexity and generality. Our in-
troduction to the theory of queues gives a (hopefully) balanced presentation of
potentially very general methods of analysis based on the theory of Markov
renewal processes, and at the same time tries to apply these to the practically
relevant analyses of queueing systems. Opening the exact part of the presenta-
tion we begin with a deﬁnition of the concept of a queue:
For every n ∈N, let Tn
T and Sn
S denote positive real–valued random variables
with Tn
T +1 > Tn
T
for all n ∈N. The sequence T = (Tn
T
: n ∈N0) is called
arrival point process and S = (Sn
S
: n ∈N) is the sequence of service
times. Further choose a number k of servers and the system capacity c, with
k, c ∈N ∪{∞}.
Finally a service discipline B needs to be speciﬁed. This can be ﬁrst come ﬁrst
served (FCFS), last come ﬁrst served (LCFS), processor sharing (PS), some-
times working with certain priorities or preemption rules. Normally we choose
FCFS, meaning that the ﬁrst user who arrives in the system will be the ﬁrst to
get access to a server. If other service disciplines will be used, they will be
explained whenever introduced.
The 5–tuple (T , S, k, c, B) is called a queue (or queueing system) with arrival
point process T , sequence of service times S, number k of servers, system
capacity c, and service discipline B.
Deﬁne further the nth inter–arrival time by Z1 := T1
T and Zn
Z
:= Tn
T −Tn
T −1
for all n ≥2. The standard way to specify a queue is the Kendall nota-
tion. This merely denotes the 5–tuple (T , S, k, c, B) in the above deﬁnition
by T /S/k/c/B and additionally sets some conventions for interpreting this

Queues: The Art of Modelling
5
notation: If the 4th or 5th parameter is left out, this is agreed to mean c = ∞
or B = FCFS, respectively. Further, for the ﬁrst two parameters the letters
M (resp. G) stand for geometric (resp. general) inter–arrival and service times
for queues in discrete time and for exponential (resp. general) inter–arrival
and service times for queues in continuous time. There are additional conven-
tions: D stands for deterministic (Dirac) distributions, Geo is the same as M
for discrete time systems, etc.
The main goal of any queueing analysis will be to specify and analyze the
system process Q = (Qt : t ≥0), where Qt is the number of users in the
queueing system (usually including the number of users in service) at time t.
An important measure (in case of existence) will be the asymptotic distribu-
tion of Qt for t tending to inﬁnity.
Our ﬁrst result concerns a sample path property of general conservative sys-
tems with inputs and outputs. Conservative systems do not create or destroy
users. Let α(t) denote the number of arrivals into the system until time t. De-
ﬁne λt := α(t)/t as the average arrival rate during the interval [0, t]. Further
deﬁne T as the average time a user spends in the system. Finally denote the
average number of users in the system during [0, t] by ¯Nt
N . Then we can state
Theorem 1.8 Little’s Result
If the limit λ = limt→∞λt and T do exist, then the limit ¯N = limt→∞¯Nt
N does
exist, too, and the relation
¯N = λT
holds.
Proof: We introduce the notation δ(t) for the number of departures from the
system during [0, t] and N(t) for the number of users in the system. If the
system starts empty, then these deﬁnitions imply the relation
N(t) = α(t) −δ(t)
for all times t (see the following ﬁgure).
Denote the total time that all users have spent in the system during [0, t] by
γ(t) :=
 t
0

N(s) ds
If we deﬁne Tt
T as the system time per user averaged over all users in the interval
[0, t], then the deﬁnitions of α(t) and γ(t) imply the relation
Tt
T = γ(t)
α(t)
(1.1)

6
AN INTRODUCTION TO QUEUEING THEORY
N(t)
time
δ
α
(t)
(t)
number of users
Figure 1.3.
Total system time
The average number of users in the system during [0, t] can be obtained as
¯Nt
N = γ(t)
t
= γ(t)
α(t) · α(t)
t
= λtTt
T
where the last equality comes from (1.1) and the deﬁnition of λt. If the limits
λ and T = limt→∞Tt
T exist, then the stated relation ¯N = λT follows for t
tending to inﬁnity.
□
For ease of reference, we ﬁnally provide a table of some basic probability
distributions which will occur frequently throughout the book.
Distribution
Density
Range
Parameters
Exponential
λe−λt
t > 0
λ > 0
Erlang
mµ(mµt)m−1
(m−1)!
e−mµt
t > 0
m ∈N, µ > 0
Poisson
λn
n! e−λ
n ∈N0
λ > 0
Geometric
(1 −p)pn
n ∈N0
p ∈]0, 1[
Binomial
N
n

pn(1 −p)N−n
0 ≤n ≤N
N ∈N, p ∈]0, 1[

Queues: The Art of Modelling
7
Notes
The ﬁrst formal proof for Little’s result appeared in Little [53]. The proof
presented here is taken from Kleinrock [50].


PART I
MARKOVIAN METHODS

Chapter 2
MARKOV CHAINS AND QUEUES IN DISCRETE
TIME
1.
Deﬁnition
Let Xn
X
with n ∈N0 denote random variables on a discrete space E. The
sequence X = (Xn
X : n ∈N0) is called a stochastic chain. If P is a probability
measure X such that
P (Xn
X +1 = j|X0 = i0, . . . , Xn
X = in) = P (Xn
X +1 = j|Xn
X = in)
(2.1)
for all i0, . . . , in, j ∈E and n ∈N0, then the sequence X shall be called a
Markov chain on E. The probability measure P is called the distribution of
X, and E is called the state space of X.
If the conditional probabilities P (Xn
X +1 = j|Xn
X = in) are independent of the
time index n ∈N0, then we call the Markov chain X homogeneous and denote
pij := P (Xn
X +1 = j|Xn
X = i)
for all i, j ∈E. The probability pij is called transition probability from state
i to state j. The matrix P := (pij)i,j∈E shall be called transition matrix of
the chain X. Condition (2.1) is referred to as the Markov property.
Example 2.1 If (Xn
X
: n ∈N0) are random variables on a discrete space E,
which are stochastically independent and identically distributed (shortly: iid),
then the chain X = (Xn
X : n ∈N0) is a homogeneous Markov chain.
Example 2.2 Discrete Random Walk
Set E := Z and let (Sn
S
: n ∈N) be a sequence of iid random variables with
values in Z and distribution π. Deﬁne X0 := 0 and Xn
X
:= n
k=1 Sk for all

12
AN INTRODUCTION TO QUEUEING THEORY
n ∈N. Then the chain X = (Xn
X
: n ∈N0) is a homogeneous Markov chain
with transition probabilities pij = πj
π −i. This chain is called discrete random
walk.
Example 2.3 Bernoulli process
Set E := N0 and choose any parameter 0 < p < 1. The deﬁnitions X0 := 0
as well as
pij :=

p,
j = i + 1
1 −p,
j = i
for i ∈N0 determine a homogeneous Markov chain X = (Xn
X : n ∈N0). It is
called Bernoulli process with parameter p.
So far, al examples have been chosen as to be homogeneous. The following
theorem shows that there is a good reason for this:
Theorem 2.4 Be X = (Xn
X
: n ∈N0) a Markov chain on a discrete state
space E. Then there is a homogeneous Markov chain X ′ = (X′
n
X
: n ∈N0)
on the state space E × N0 such that Xn
X
= pr1(X′
n
X ) for all n ∈N0, with pr1
denoting the projection to the ﬁrst dimension.
Proof: Let X be a Markov chain with transition probabilities
pn;ij := P(Xn
X +1 = j|Xn
X = i)
which may depend on the time instant n. Deﬁne the two–dimensional random
variables X′
n
X := (Xn
X , n) for all n ∈N0 and denote the resulting distribution of
the chain X ′ = (X′
n
X
: n ∈N0) by P′. By deﬁnition we obtain Xn
X
= pr1(X′
n
X )
for all n ∈N0.
Further P′(X′
0 = (i, k)) = δk0 · P(X0 = i) holds for all i ∈E, and all
transition probabilities
p′
(i,k),(j,l) = P′(X′
k+1 = (j, l)|X′
k = (i, k)) = δl,k+1 · pk;ij
can be expressed without a time index. Hence the Markov chain X ′ is homo-
geneous.
□
Because of this result, we will from now on treat only homogeneous Markov
chains and omit the adjective ”homogeneous”.
Let P denote the transition matrix of a Markov chain on E. Then as an im-
mediate consequence of its deﬁnition we obtain pij ∈[0, 1] for all i, j ∈E

Markov Chains and Queues in Discrete Time
13
and 
j∈E pij = 1 for all i ∈E. A matrix P with these properties is called
a stochastic matrix on E. In the following we shall demonstrate that, given
an initial distribution, a Markov chain is uniquely determined by its transition
matrix. Thus any stochastic matrix deﬁnes a family of Markov chains.
Theorem 2.5 Let X denote a homogeneous Markov chain on E with transi-
tion matrix P. Then the relation
P (Xn
X +1 = j1, . . . , Xn
X +m = jm|Xn
X = i) = pi,j1 · . . . · pjm−1,jm
holds for all n ∈N0, m ∈N, and i, j1, . . . , jm ∈E.
Proof: This is easily shown by induction on m. For m = 1 the statement holds
by deﬁnition of P. For m > 1 we can write
P(Xn
X +1 =j1, . . . , Xn
X +m = jm|Xn
X = i)
= P (Xn
X +1 = j1, . . . , Xn
X +m = jm, Xn
X = i)
P (Xn
X = i)
=
P (Xn
X +1 = j1, . . . , Xn
X +m = jm, Xn
X = i)
P (Xn
X +1 = j1, . . . , Xn
X +m−1 = jm−1, Xn
X = i)
× P (Xn
X +1 = j1, . . . , Xn
X +m−1 = jm−1, Xn
X = i)
P (Xn
X = i)
= P (Xn
X +m = jm|Xn
X = i, Xn
X +1 = j1, . . . , Xn
X +m−1 = jm−1)
× pi,j1 · . . . · pjm−2,jm−1
= pjm−1,jm · pi,j1 · . . . · pjm−2,jm−1
because of the induction hypothesis and the Markov property.
□
Let π be a probability distribution on E with P(X0 = i) = πi for all i ∈E.
Then theorem 2.5 immediately yields
P (X0 = j0, X1 = j1, . . . , Xm
X
= jm) = πj
π 0 · pj0,j1 . . . pjm−1,jm
(2.2)
for all m ∈N and j0, . . . , jm ∈E. The chain with this distribution P is
denoted by X π and called the π–version of X. The probability measure π is
called initial distribution for X.
Theorem 2.5 and the extension theorem by Tulcea (see appendix 2) show that
a Markov chain is uniquely determined by its transition matrix and its initial
distribution. Whenever the initial distribution π is not important or understood
from the context, we will simply write X instead of X π. However, in an exact
manner the notation X denotes the family of all the versions X π of X, indexed
by their initial distribution π.

14
AN INTRODUCTION TO QUEUEING THEORY
Theorem 2.6 Let X denote a homogeneous Markov chain with transition ma-
trix P. Then the relation
P(Xn
X +m = j|Xn
X = i) = P m(i, j)
holds for all m, n ∈N0 and i, j ∈E, with P m(i, j) denoting the (i, j)th entry
of the mth power of the matrix P. In particular, P 0 equals the identity matrix.
Proof: This follows by induction on m. For m = 1 the statement holds by
deﬁnition of P. For m > 1 we can write
P(Xn
X +m = j|Xn
X = i) = P (Xn
X +m = j, Xn
X = i)
P (Xn
X = i)
=

k∈E
P (Xn
X +m = j, Xn
X +m−1 = k, Xn
X = i)
P (Xn
X +m−1 = k, Xn
X = i)
× P (Xn
X +m−1 = k, Xn
X = i)
P (Xn
X = i)
=

k∈E
P (Xn
X +m = j|Xn
X +m−1 = k, Xn
X = i) · P m−1(i, k)
=

k∈E
pkj · P m−1(i, k) = P m(i, j)
because of the induction hypothesis and the Markov property.
□
Thus the probabilities for transitions in m steps are given by the mth power
of the transition matrix P. The rule P m+n = P mP n for the multiplication of
matrices and theorem 2.6 lead to the decompositions
P(Xm
X
+n = j|X0 = i) =

k∈E
P(Xm
X
= k|X0 = i) · P(Xn
X = j|X0 = k)
which are known as the Chapman–Kolmogorov equations.
For later purposes we will need a relation closely related to the Markov prop-
erty, which is called the strong Markov property. Let τ denote a random
variable with values in N0 ∪{∞}, such that the condition
P(τ ≤n|X) = P(τ ≤n|X0, . . . , Xn
X )
(2.3)
holds for all n ∈N0. Such a random variable is called a (discrete) stopping
time for X. The deﬁning condition means that the probability for the event
{τ ≤n} depends only on the evolution of the chain until time n. In other

Markov Chains and Queues in Discrete Time
15
words, the determination of a stopping time does not require any knowledge of
the future. Now the strong Markov property is stated in
Theorem 2.7 Let X denote a Markov chain and τ a stopping time for X with
P(τ < ∞) = 1. Then the relation
P(Xτ
X +m = j|X0 = i0, . . . , Xτ
X = iτ) = P(Xm
X
= j|X0 = iτ)
holds for all m ∈N and i0, . . . , iτ, j ∈E.
Proof: The fact that the stopping time τ is ﬁnite and may assume only count-
ably many values can be exploited in the transformation
P(Xτ
X +m = j|X0 = i0, . . . , Xτ
X = iτ)
=
∞

n=0
P(τ = n, Xτ
X +m = j|X0 = i0, . . . , Xτ
X = iτ)
=
∞

n=0
P(Xτ
X +m = j|τ = n, X0 = i0, . . . , Xτ
X = iτ)
× P(τ = n|X0 = i0, . . . , Xτ
X = iτ)
=
∞

n=0
P(Xn
X +m = j|Xn
X = iτ) · P(τ = n|X)
=
∞

n=0
P(τ = n|X) · P(Xm
X
= j|X0 = iτ)
which yields the statement, as τ is ﬁnite with probability one.
□
2.
Classiﬁcation of States
Let X denote a Markov chain with state space E and transition matrix P. We
call a state j ∈E accessible from a state i ∈E if there is a number m ∈N0
with P(Xm
X
= j|X0 = i) > 0. This relation shall be denoted by i →j. If for
two states i, j ∈E, the relations i →j and j →i hold, then i and j are said
to communicate, in notation i ↔j.
Theorem 2.8 The relation ↔of communication between states is an equiva-
lence relation.
Proof: Because of P 0 = I, communication is reﬂexive. Symmetry holds
by deﬁnition. Thus it remains to show transitivity. For this, assume i ↔j

16
AN INTRODUCTION TO QUEUEING THEORY
and j ↔k for three states i, j, k ∈E. This means that there are numbers
m, n ∈N0 with P m(i, j) > 0 and P n(j, k) > 0. Hence, by the Chapman–
Kolmogorov equation, we obtain
P(Xm
X
+n = k|X0 = i) =

h∈E
P(Xm
X
= h|X0 = i) · P(Xn
X = k|X0 = h)
≥P(Xm
X
= j|X0 = i) · P(Xn
X = k|X0 = j) > 0
which proves i →k. The remaining proof of k →i is completely analogous.
□
Because of this result and the countability, we can divide the state space E of
a Markov chain into a partition of countably many equivalence classes with
respect to the communication of states. Any such equivalence class shall be
called communication class. A communication class C ⊂E that does not
allow access to states outside itself, i.e. for which the implication
i →j,
i ∈C
⇒
j ∈C
holds, is called closed. If a closed equivalence class consists only of one state,
then this state shall be called absorbing. If a Markov chain has only one
communication class, i.e. if all states are communicating, then it is called irre-
ducible. Otherwise it is called reducible.
Example 2.9 Let X denote a discrete random walk (see example 2.2) with the
speciﬁcation π1 = p and π−1 = 1 −p for some parameter 0 < p < 1. Then X
is irreducible.
Example 2.10 The Bernoulli process (see example 2.3) with non–trivial pa-
rameter 0 < p < 1 is to the highest degree reducible. Every state x ∈N0
forms an own communication class. None of these is closed, thus there are no
absorbing states.
Theorem 2.11 Be X a Markov chain with state space E and transition matrix
P. Let C = {cn : n ∈I} ⊂E with I ⊂N be a closed communication class.
Deﬁne the matrix P ′ by its entries p′
ij := pci,cj for all i, j ∈I. Then P ′ is
stochastic.
Proof: By deﬁnition, p′
ij ∈[0, 1] for all i, j ∈I. Since C is closed, pci,k = 0
for all i ∈I and k /∈/ C. This implies

j∈I
p′
ij =

j∈I
pci,cj = 1 −

k/∈/C
pci,k = 1

Markov Chains and Queues in Discrete Time
17
for all i ∈I, as P is stochastic.
□
Thus the restriction of a Markov chain X with state space E to the states of
one of its closed communication classes C deﬁnes a new Markov chain with
state space C. If the states are relabeled according to their afﬁliation to a
communication class, the transition matrix of X can be displayed in a block
matrix form as
P =
⎡
⎢
⎡
⎢⎢
⎢⎢
⎢⎢
⎢⎢
⎣⎢
Q
Q1
Q2
Q3
Q4
. . .
0
P1
P
0
0
0
. . .
0
0
P2
P
0
0
. . .
0
0
0
P3
P
0
. . .
...
...
...
...
...
⎤
⎥
⎤
⎥⎥
⎥⎥
⎥⎥
⎥⎥
⎦⎥
(2.4)
with Pn
P
being stochastic matrices on the closed communication classes Cn
C .
The ﬁrst row contains the transition probabilities starting from communication
classes that are not closed.
Let X denote a Markov chain with state space E. In the rest of this section
we shall investigate distribution and expectation of the following random vari-
ables: Deﬁne τjτ as the stopping time of the ﬁrst visit to the state j ∈E, i.e.
τjτ := min{n ∈N : Xn
X = j}
Denote the distribution of τjτ by
Fk
F (i, j) := P(τjτ = k|X0 = i)
for all i, j ∈E and k ∈N.
Lemma 2.12 The conditional distribution of the ﬁrst visit to the state j ∈E,
given an initial state X0 = i, can be determined iteratively by
Fk
F (i, j) =

pij,
k = 1

h̸=
̸
j pihFk
F −1(h, j),
k ≥2
for all i, j ∈E.
Proof: For k = 1, the deﬁnition yields
F1
F (i, j) = P(τjτ = 1|X0 = i) = P(X1 = j|X0 = i) = pij
for all i, j ∈E. For k ≥2, conditioning upon X1 yields
Fk
F (i, j) = P(X1 ̸≠
j, . . . , Xk−1 ̸≠
j, Xk = j|X0 = i)

18
AN INTRODUCTION TO QUEUEING THEORY
=

h̸=
̸
j
P(X1 = h|X0 = i)
× P(X2 ̸≠
j, . . . , Xk−1 ̸≠
j, Xk = j|X0 = i, X1 = h)
=

h̸=
̸
j
pih · P(X1 ̸≠
j, . . . , Xk−2 ̸≠
j, Xk−1 = j|X0 = h)
due to the Markov property.
□
Now deﬁne
fij
f
:= P(τjτ < ∞|X0 = i) =
∞

k=1
Fk
F (i, j)
(2.5)
for all i, j ∈E, which represents the probability of ever visiting state j after
beginning in state i. Summing up over all k ∈N in the formula of Lemma
2.12 leads to
fij
f
= pij +

h̸=
̸
j
pihfhj
f
(2.6)
for all i, j ∈E. The proof is left as an exercise.
Deﬁne Nj
N as the random variable of the total number of visits to the state
j ∈E. Expression (2.6) is useful for computing the distribution of Nj
N :
Theorem 2.13 Let X denote a Markov chain with state space E. The total
number of visits to a state j ∈E under the condition that the chain starts in
state i is given by
P(Nj
N = m|X0 = j) = fm−1
jj
f
(1 −fjj
f )
and for i ̸≠
j
P(Nj
N = m|X0 = i) =

1 −fij
f ,
m = 0
fij
f fm−1
jj
f
(1 −fjj
f ),
m ≥1
Thus the distribution of Nj
N is modiﬁed geometric.
Proof: Deﬁne τ (1)
jτ
:= τjτ and τ (k+1)
jτ
:= min{n > τ (k)
jτ
: Xn
X
= j} for all
k ∈N, with the convention that min ∅= ∞. Note that τ (k)
jτ
= ∞implies
τ (l)
jτ
= ∞for all l > k.
Then the sequence (τ (k)
jτ
: k ∈N) is a sequence of stopping times. The event
{Nj
N
= m} is the same as the intersection of the events {τ (k)
jτ
< ∞} for

Markov Chains and Queues in Discrete Time
19
k = 1, . . . , M and {τ (M+1)
jτ
= ∞}, with M = m if i ̸≠
j and M = m −1 if
i = j. Now this event can be further described by the intersection of the events
{τ (k+1)
jτ
−τ (k)
jτ
< ∞} for k = 0, . . . , M −1 and {τ (M+1)
jτ
−τ (M)
jτ
= ∞}, with
M as above and the convention τ (0)
jτ
:= 0.
The subevent {τ (k+1)
jτ
−τ (k)
jτ
< ∞} has probability fij
f
for k = 0 and because
of the strong Markov property (see theorem 2.7) probability fjj
f
for k > 0. The
probability for {τ (M+1)
jτ
−τ (M)
jτ
= ∞} is 1 −fij
f
for M = 0 and 1 −fjj
f
for
M > 0. Once more the strong Markov property is the reason for independence
of the subevents. Now multiplication of the probabilities leads to the formulae
in the statement.
□
Summing over all m in the above theorem leads to
Corollary 2.14 For all j ∈E, the zero–one law
P(Nj
N < ∞|X0 = j) =

1,
fjj
f
< 1
0,
fjj
f
= 1
holds, i.e. depending on fjj
f
there are almost certainly inﬁnitely many visits to
a state j ∈E.
This result gives rise to the following deﬁnitions: A state j ∈E is called re-
current if fjj
f
= 1 and transient otherwise. Let us further deﬁne the potential
matrix R = (rij)i,j∈E of the Markov chain by its entries
rij := E(Nj
N |X0 = i)
for all i, j ∈E. Thus an entry rij gives the expected number of visits to the
state j ∈E under the condition that the chain starts in state i ∈E. As such,
rij can be computed by
rij =
∞

n=0
P n(i, j)
(2.7)
for all i, j ∈E. The results in theorem 2.13 and corollary 2.14 yield
Corollary 2.15 For all i, j ∈E the relations
rjj = (1 −fjj
f )−1
and
rij = fij
f rjj
hold, with the conventions 0−1 := ∞and 0 · ∞:= 0 included. In particular,
the expected number rjj of visits to the state j ∈E is ﬁnite if j is transient and
inﬁnite if j is recurrent.

20
AN INTRODUCTION TO QUEUEING THEORY
Theorem 2.16 Recurrence and transience of states are class properties with
respect to the relation ↔. Furthermore, a recurrent communication class is
always closed.
Proof: Assume that i ∈E is transient and i ↔j. Then there are numbers
m, n ∈N with 0 < P m(i, j) ≤1 and 0 < P n(j, i) ≤1. The inequalities
∞

k=0
P k(i, i) ≥
∞

h=0
P m+h+n(i, i) ≥P m(i, j)P n(j, i)
∞

k=0
P k(j, j)
now imply rjj < ∞because of representation (2.7). According to corollary
2.15 this means that j is transient, too.
If j is recurrent, then the same inequalities lead to
rii ≥P m(i, j)P n(j, i)rjj = ∞
which signiﬁes that i is recurrent, too. Since the above arguments are symmet-
ric in i and j, the proof of the ﬁrst statement is complete.
For the second statement assume that i ∈E belongs to a communication class
C ⊂E and pij > 0 for some state j ∈E \ C. Then
fii
f
= pii +

h̸=
̸
i
pihfhi
f
≤1 −pij < 1
according to formula (2.6), since fji
f
= 0 (otherwise i ↔j). Thus i is transient,
which proves the second statement.
□
Theorem 2.17 If the state j ∈E is transient, then limn→∞P n(i, j) = 0,
regardless of the initial state i ∈E.
Proof: If the state j is transient, then the ﬁrst equation in corollary 2.15 yields
rjj < ∞. The second equation in the same corollary now implies rij < ∞,
which by the representation (2.7) completes the proof.
□
3.
Stationary Distributions
Let X denote a Markov chain with state space E and π a measure on E. If
P(Xn
X
= i) = P(X0 = i) = πi for all n ∈N and i ∈E, then X π is called
stationary, and π is called a stationary measure for X. If furthermore π is a
probability measure, then it is called stationary distribution for X.

Markov Chains and Queues in Discrete Time
21
Theorem 2.18 Let X denote a Markov chain with state space E and transition
matrix P. Further, let π denote a probability distribution on E with πP = π,
i.e.
πi =

j∈E
πj
π pji
and

j∈E
πj
π = 1
for all i ∈E. Then π is a stationary distribution for X. If π is a stationary
distribution for X, then πP = π holds.
Proof: Let P(X0 = i) = πi for all i ∈E. Then P(Xn
X
= i) = P(X0 = i)
for all n ∈N and i ∈E follows by induction on n. The case n = 1 holds
by assumption, and the induction step follows by induction hypothesis and the
Markov property. The last statement is obvious.
□
The following examples show some features of stationary distributions:
Example 2.19 Let the transition matrix of a Markov chain X be given by
P =
⎛
⎜
⎛
⎜
⎜
⎝
⎜
0.8
0.2
0
0
0.2
0.8
0
0
0
0
0.4
0.6
0
0
0.6
0.4
⎞
⎟
⎞
⎟
⎟
⎠
⎟
Then π = (0.5, 0.5, 0, 0), π′ = (0, 0, 0.5, 0.5) as well as any linear combina-
tion of them are stationary distributions for X. This shows that a stationary
distribution does not need to be unique.
Example 2.20 Bernoulli process (see example 2.1)
The transition matrix of a Bernoulli process has the structure
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
1 −p
p
0
0
. . .
0
1 −p
p
0
...
0
0
1 −p
p
...
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
Hence πP = π implies ﬁrst
π0 · (1 −p) = π0
⇒
π0 = 0
since 0 < p < 1. Assume that πn = 0 for any n ∈N0. This and the condition
πP = π further imply for πn+1
πn · p + πn+1 · (1 −p) = πn+1
⇒
πn+1 = 0

22
AN INTRODUCTION TO QUEUEING THEORY
which completes an induction argument proving πn = 0 for all n ∈N0. Hence
the Bernoulli process does not have a stationary distribution.
Example 2.21 The solution of πP = π and 
j∈E πj
π = 1 is unique for
P =

1 −p
p
p
1 −p

with 0 < p < 1. Thus there are transition matrices which have exactly one
stationary distribution.
The question of existence and uniqueness of a stationary distribution is one of
the most important problems in the theory of Markov chains. A simple answer
can be given in the transient case (cf. example 2.20):
Theorem 2.22 A transient Markov chain (i.e. a Markov chain with transient
states only) has no stationary distribution.
Proof: Assume that πP = π holds for some distribution π and take any enu-
meration E = (sn : n ∈N) of the state space E. Choose any index m ∈N
with πsm > 0. Since ∞
n=1 πsn = 1 is bounded, there is an index M > m
such that ∞
n=M πsn < πsm. Set ε := πsm −∞
n=M πsn. According to the-
orem 2.17, there is an index N ∈N such that P n(si, sm) < ε for all i ≤M
and n ≥N. Then the stationarity of π implies
πsm =
∞

i=1
πsiP N(si, sm) =
M−1

i=1
πsiP N(si, sm) +
∞

i=M
πsiP N(si, sm)
< ε +
∞

i=M
πsi = πsm
which is a contradiction.
□
For the recurrent case, a ﬁner distinction will be necessary. While the expected
total number rjj of visits to a recurrent state j ∈E is always inﬁnite (see
corollary 2.15), there are differences in the rate of visits to a recurrent state.
In order to describe these, deﬁne Ni
N (n) as the number of visits to state i until
time n. Further deﬁne for a recurrent state i ∈E the mean time
mi := E(τiτ |X0 = i)
until the ﬁrst visit to i (after time zero) under the condition that the chain starts
in i. By deﬁnition mi > 0 for all i ∈E. The elementary renewal theorem

Markov Chains and Queues in Discrete Time
23
(which will be proven later as theorem 6.12) states that
lim
n→∞
E(Ni
N (n)|X0 = j)
n
= 1
mi
(2.8)
for all recurrent i ∈E and independently of j ∈E provided j ↔i, with the
convention of 1/∞:= 0. Thus the asymptotic rate of visits to a recurrent state
is determined by the mean recurrence time of this state. This gives reason to the
following deﬁnition: A recurrent state i ∈E with mi = E(τiτ |X0 = i) < ∞
will be called positive recurrent, otherwise i is called null recurrent. The
distinction between positive and null recurrence is supported by the equiva-
lence relation ↔, as shown in
Theorem 2.23 Positive recurrence and null recurrence are class properties
with respect to the relation of communication between states.
Proof: Assume that i ↔j for two states i, j ∈E and i is null recurrent. Thus
there are numbers m, n ∈N with P n(i, j) > 0 and P m(j, i) > 0. Because of
the representation E(Ni
N (k)|X0 = i) = k
l=0 P l(i, i), we obtain
0 = lim
k→∞
k
l=0 P l(i, i)
k
≥lim
k→∞
k−m−n
l=0
P l(j, j)
k
· P n(i, j)P m(j, i)
= lim
k→∞
k −m −n
k
·
k−m−n
l=0
P l(j, j)
k −m −n
· P n(i, j)P m(j, i)
= lim
k→∞
k
l=0 P l(j, j)
k
· P n(i, j)P m(j, i)
= P n(i, j)P m(j, i)
mj
and thus mj = ∞, which signiﬁes the null recurrence of j.
□
Thus we can call a communication class positive recurrent or null recurrent. In
the former case, a construction of a stationary distribution is given in
Theorem 2.24 Let i ∈E be positive recurrent and deﬁne the mean ﬁrst visit
time mi := E(τiτ |X0 = i). Then a stationary distribution π is given by
πj
π := m−1
i
·
∞

n=0
P(Xn
X = j, τiτ > n|X0 = i)

24
AN INTRODUCTION TO QUEUEING THEORY
for all j ∈E. In particular, πi = m−1
i
and πk = 0 for all states k outside of
the communication class belonging to i.
Proof: First of all, π is a probability measure since

j∈E
∞

n=0
P(Xn
X = j, τiτ > n|X0 = i) =
∞

n=0

j∈E
P(Xn
X = j, τiτ > n|X0 = i)
=
∞

n=0
P(τiτ > n|X0 = i) = mi
The particular statements in the theorem are obvious from theorem 2.16 and
the deﬁnition of π. The stationarity of π is shown as follows. First we obtain
πj
π = m−1
i
·
∞

n=0
P(Xn
X = j, τiτ > n|X0 = i)
= m−1
i
·
∞

n=1
P(Xn
X = j, τiτ ≥n|X0 = i)
= m−1
i
·
∞

n=1
P(Xn
X = j, τiτ > n −1|X0 = i)
since X0 = Xτ
X i
τ = i in the conditioning set {X0 = i}. Because of
P(Xn
X = j, τiτ > n −1|X0 = i)
= P(Xn
X = j, τiτ > n −1, X0 = i)
P(X0 = i)
=

k∈E
P(Xn
X = j, Xn
X −1 = k, τiτ > n −1, X0 = i)
P(X0 = i)
=
 P(Xn
X = j, Xn
X −1 = k, τiτ > n −1, X0 = i)
P(Xn
X −1 = k, τiτ > n −1, X0 = i)
× P(Xn
X −1 = k, τiτ > n −1, X0 = i)
P(X0 = i)
=

k∈E
pkjP(Xn
X −1 = k, τiτ > n −1|X0 = i)
we can transform further
πj
π = m−1
i
·
∞

n=1

k∈E
pkjP(Xn
X −1 = k, τiτ > n −1|X0 = i)
k∈E\{i}

Markov Chains and Queues in Discrete Time
25
=

k∈E
pkj · m−1
i
∞

n=0
P(Xn
X = k, τiτ > n|X0 = i) =

k∈E
πkpkj
which completes the proof.
□
Theorem 2.25 Let X denote an irreducible, positive recurrent Markov chain.
Then X has a unique stationary distribution.
Proof: Existence has been shown in theorem 2.24. Uniqueness of the station-
ary distribution can be seen as follows. Let π denote the stationary distribution
as constructed in theorem 2.24 and i the positive recurrent state that served
as recurrence point for π. Further, let ν denote any stationary distribution for
X. Then there is a state j ∈E with νj
ν
> 0 and a number m ∈N with
P m(j, i) > 0, since X is irreducible. Consequently we obtain
νiν =

k∈E
νkP m(k, i) ≥νj
ν P m(j, i) > 0
Hence we can multiply ν by a skalar factor c such that c · νiν = πi = 1/mi.
Denote ˜ν := c · ν.
Let ˜P denote the transition matrix P without the ith column, i.e. we deﬁne the
(j, k)th entry of ˜P by ˜pjk = pjk if k ̸≠
i and zero otherwise. Denote further
the Dirac measure on i by δi, i.e. δi
jδ = 1 if i = j and zero otherwise. Then the
stationary distribution π can be represented by π = m−1
i
· δi ∞
n=0 ˜P n.
We ﬁrst claim that mi˜ν = δi + mi˜ν ˜P. This is clear for the entry ˜νiν and easily
seen for ˜νj
ν with j ̸≠
i because in this case (˜ν ˜P)j = c · (νP)j = ˜νj
ν . Now we
can proceed with the same argument to see that
mi˜ν = δi + (δi + mi˜ν ˜P) ˜P = δi + δi ˜P + mi˜ν ˜P 2 = . . .
= δi
∞

n=0
˜P n = miπ
Hence ˜ν already is a probability measure and the skalar factor must be c = 1.
This yields ν = ˜ν = π and thus the statement.
□
Remark 2.26 At a closer look the assumption of irreducibility may be relaxed
to some extend. For example, if there is exactly one closed positive recurrent
communication class and a set of transient and inaccessible states (i.e. states j

26
AN INTRODUCTION TO QUEUEING THEORY
for which there is no state i with i →j), then the above statement still holds
although X is not irreducible.
A ﬁrst consequence of the uniqueness is the following simpler representation
of the stationary distribution:
Theorem 2.27 Let X denote an irreducible, positive recurrent Markov chain.
Then the stationary distribution π of X is given by
πj
π = m−1
j
=
1
E(τjτ |X0 = j)
for all j ∈E.
Proof: Since all states in E are positive recurrent, the construction in theorem
2.24 can be pursued for any inital state j. This yields πj
π = m−1
j
for all j ∈E.
The statement now follows from the uniqueness of the stationary distribution.
□
Corollary 2.28 For an irreducible, positive recurrent Markov chain, the sta-
tionary probability πj
π of a state j coincides with its asymptotic rate of recur-
rence, i.e.
lim
n→∞
E(Nj
N (n)|X0 = i)
n
= πj
π
for all j ∈E and independently of i ∈E. Further, if an asymptotic distribu-
tion p = limn→∞P(Xn
X
= .) does exist, then it coincides with the stationary
distribution. In particular, it is independent of the initial distribution of X.
Proof: The ﬁrst statement immediately follows from equation (2.8). For the
second statement, it sufﬁces to employ E(Nj
N (n)|X0 = i) = n
l=0 P l(i, j). If
an asymptotic distribution p does exist, then for any initial distribution ν we
obtain
pj = lim
n→∞(νP n)j =

i∈E
νiν lim
n→∞P n(i, j)
=

i∈E
νiν lim
n→∞
n
l=0 P l(i, j)
n
=

i∈E
νiν πj
π
= πj
π
independently of ν.
□

Markov Chains and Queues in Discrete Time
27
4.
Restricted Markov Chains
Now let F ⊂E denote any subset of the state space E. Deﬁne τF
τ (k) to be the
stopping time of the kth visit of X to the set F, i.e.
τF
τ (k + 1) := min{n > τF
τ (k) : Xn
X ∈F}
with τF
τ (0) := 0. If X is recurrent, then the strong Markov property (theorem
2.7) ensures that the chain X F = (XF
n
X
: n ∈N) with XF
n
X
:= Xτ
X F
τ (n) is a
recurrent Markov chain, too. It is called the Markov chain restricted to F. In
case of positive recurrence, we can obtain the stationary distribution of X F
from the stationary distribution of X in a simple manner:
Theorem 2.29 If the Markov chain X is positive recurrent, then the stationary
distribution of X F is given by
πF
j
π
=
πj
π

k∈F πk
for all j ∈F.
Proof: Choose any state i ∈F and recall from theorem 2.24 the expression
πj
π := m−1
i
·
∞

n=0
P(Xn
X = j, τiτ > n|X0 = i)
which holds for all j ∈F. For πF
j
π
we can perform the same construction with
respect to the chain X F . By the deﬁnition of X F it is clear that the number
of visits to the state j between two consecutive visits to i is the same for the
chains X and X F . Hence the sum expression for πF
j
π , which is the expectation
of that number of visits, remains the same as for πj
π . The other factor m−1
i
in the formula above is independent of j and serves only as a normalization
constant, i.e. in order to secure that 
j∈E πj
π = 1. Hence for a construction of
πF
j
π
with respect to X F this needs to be replaced by (mi · 
k∈F πk)−1, which
then yields the statement.
□
Theorem 2.30 Let X = (Xn
X
: n ∈N0) denote an irreducible and positive
recurrent Markov chain with discrete state space E. Further let F ⊂E denote
any subset of E, and X F the Markov chain restricted to F. Denote
τF
τ
:= min{n ∈N : Xn
X ∈F}

28
AN INTRODUCTION TO QUEUEING THEORY
Then a measure ν on E is stationary for X if and only if ν′ = (νiν : i ∈F) is
stationary for X F and
νj
ν =

k∈F
νk
∞

n=0
P(Xn
X = j, τF
τ
> n|X0 = k)
(2.9)
for all j ∈E \ F.
Proof: Due to theorem 2.29 it sufﬁces to prove equation (2.9) for j ∈E \ F.
Choose any state i ∈F and deﬁne
τiτ := min{n ∈N : Xn
X = i}
According to theorem 2.24 the stationary measure v for X is given by
νj
ν = νiν ·
∞

n=0
P(Xn
X = j, τiτ > n|X0 = i) = νiν · Ei
τi
τ −1

n=0
1Xn=j

for j ∈E \ F, where Ei denotes the conditional expectation given X0 = i.
Deﬁne further
τ F
iτ
:= min{n ∈N : XF
n
X
= i}
Because of the strong Markov property we can proceed as
νj
ν = νiν · Ei
⎛
⎝
⎛
τ F
i
τ
−1

n=0
EXF
n
τF
τ −1

m=0
1Xm=j
⎞
⎠
⎞
= νiν ·

k∈F
Ei
⎛
⎝
⎛
τ F
i
τ
−1

n=0
1XF
n =k
⎞
⎠
⎞
· Ek
τF
τ −1

m=0
1Xm=j

Regarding the restricted Markov chain X F , theorem 2.24 states that
Ei
⎛
⎝
⎛
τ F
i
τ
−1

n=0
1XF
n =k
⎞
⎠
⎞
=
∞

n=0
P(XF
n
X
= k, τ F
iτ
> n|XF
0 = i) = νk
νiν
for all k ∈F. Hence we obtain
νj
ν =

k∈F
νk
∞

n=0
P(Xn
X = j, τF
τ
> n|X0 = k)
which was to be proven.
□

Markov Chains and Queues in Discrete Time
29
5.
Conditions for Positive Recurrence
In the third part of this course we will need some results on the behaviour of a
Markov chain on a ﬁnite subset of its state space. As a ﬁrst fundamental result
we state
Theorem 2.31 An irreducible Markov chain with ﬁnite state space F is posi-
tive recurrent.
Proof: For all n ∈N and i ∈F we have 
j∈E P n(i, j) = 1. Hence it is
not possible that limn→∞P n(i, j) = 0 for all j ∈F. Thus there is one state
h ∈F such that rhh = ∞
n=0 P n(h, h) = ∞, which means by corollary 2.15
that h is recurrent and by irreducibility that the chain is recurrent.
If the chain were null recurrent, then according to the relation in (2.8)
lim
n→∞
1
n
n

k=1
P k(i, j) = 0
would hold for all j ∈F, independently of i because of irreducibility. But this
would imply that limn→∞P n(i, j) = 0 for all j ∈F, which contradicts our
ﬁrst observation in this proof. Hence the chain must be positive recurrent.
□
For irreducible Markov chains the condition E(τiτ |X0 = i) < ∞implies pos-
itive recurrence of state i and hence positive recurrence of the whole chain.
Writing τF
τ
for the time of the ﬁrst visit to the set F, we now can state the
following generalization of this condition:
Theorem 2.32 Let X denote an irreducible Markov chain with state space E
and be F ⊂E a ﬁnite subset of E. The chain X is positive recurrent if and
only if E(τF
τ |X0 = i) < ∞for all i ∈F.
Proof: If X is positive recurrent, then E(τF
τ |X0 = i) ≤E(τiτ |X0 = i) < ∞
for all i ∈F, by the deﬁnition of positive recurrence.
Now assume that E(τF
τ |X0 = i) < ∞for all i ∈F. Deﬁne the stopping times
σ(i) := min{k ∈N : XF
k = i} and random variables Yk
Y := τF
τ (k)−τF
τ (k−1).
Since F is ﬁnite, m := maxj∈F E(τF
τ |X0 = j) < ∞. We shall denote the

30
AN INTRODUCTION TO QUEUEING THEORY
conditional expectation given X0 = i by Ei. For i ∈F we now obtain
E(τiτ |X0 = i) = Ei
⎛
⎝
⎛
σ(i)

k=1
Yk
Y
⎞
⎠
⎞
=
∞

k=1
Ei

E(Yk
Y |Xτ
X F
τ (k−1)) · 1k≤σ(i)

≤m ·
∞

k=1
P(σ(i) ≥k|X0 = i) = m · E(σ(i)|X0 = i)
Since F is ﬁnite, X F is positive recurrent by theorem 2.31. Hence we know
that E(σ(i)|X0 = i) < ∞, and thus E(τiτ |X0 = i) < ∞which shows that X
is positive recurrent.
□
An often difﬁcult problem is to determine whether a given Markov chain is
positive recurrent or not. Concerning this, we now introduce one of the most
important criteria for the existence of stationary distributions of Markov chains
occuring in queueing theory. It is known as Foster’s criterion.
Theorem 2.33 Let X denote an irreducible Markov chain with countable state
space E and transition matrix P. Further let F denote a ﬁnite subset of E. If
there is a function h : E →R with inf{h(i) : i ∈E} > −∞, such that the
conditions

k∈E
pikh(k) < ∞
and

k∈E
pjkh(k) ≤h(j) −ε
hold for some ε > 0 and all i ∈F and j ∈E \F, then X is positive recurrent.
Proof: Without loss of generality we can assume h(i) ≥0 for all i ∈E,
since otherwise we only need to increase h by a suitable constant. Deﬁne the
stopping time τF
τ
:= min{n ∈N0 : Xn
X ∈F}. First we observe that
E(h(Xn
X +1) · 1τF
τ >n+1|X0, . . . , Xn
X ) ≤E(h(Xn
X +1) · 1τF
τ >n|X0, . . . , Xn
X )
= 1τF
τ >n ·

k∈E
pXn,kh(k)
≤1τF
τ >n · (h(Xn
X ) −ε)
= h(Xn
X ) · 1τF
τ >n −ε · 1τF
τ >n

Markov Chains and Queues in Discrete Time
31
holds for all n ∈N0, where the ﬁrst equality is due to (15.3). We now proceed
with
0 ≤E(h(Xn
X +1) · 1τF
τ >n+1|X0 = i)
= E(E(h(Xn
X +1) · 1τF
τ >n+1|X0, . . . , Xn
X )|X0 = i)
≤E(h(Xn
X ) · 1τF
τ >n|X0 = i) −εP(τF
τ
> n|X0 = i)
≤. . .
≤E(h(X0) · 1τF
τ >0|X0 = i) −ε
n

k=0
·P(τF
τ
> k|X0 = i)
which holds for all i ∈E \ F and n ∈N0. For n →∞this implies
E(τF
τ |X0 = i) =
∞

k=0
P(τF
τ
> k|X0 = i) ≤h(i)/ε < ∞
for i ∈E \ F. Now the mean return time to the state set F is bounded by
E(τF
τ |X0 = i) =

j∈F
pij +

j∈E\F
pijE(τF
τ
+ 1|X0 = j)
≤1 + ε−1 
j∈E
pijh(j) < ∞
for all i ∈F, which completes the proof.
□
6.
The M/M/1 queue in discrete time
Choose any parameters 0 < p, q < 1. Let the arrival process be distributed as
a Bernoulli process with parameter p and the service times (Sn
S
: n ∈N0) be
iid according to the geometric distribution with parameter q.
The geometric service time distribution and the Bernoulli arrival process have
been chosen because this simpliﬁes the formulation of the system process in
terms of a Markov model due to the following memoryless property:
Theorem 2.34 Let S be distributed geometrically with parameter q, i.e. let
P(S = k) = (1 −q)k−1q for all k ∈N. Then P(S = k|S > k −1) = q holds
for the conditional distribution, independently of k. Likewise, if Zn
Z is the nth
inter–arrival time of a Bernoulli process with parameter p, then the relation
P(Zn
Z = k|Zn
Z > k −1) = p holds, independently of k and n.

32
AN INTRODUCTION TO QUEUEING THEORY
Proof: First the proof for the geometric distribution: For all k ∈N, the argu-
ment
P(S = k|S > k −1) = P(S = k, S > k −1)
P(S > k −1)
=
P(S = k)
P(S > k −1)
= (1 −q)k−1q
(1 −q)k−1 = q
holds, which shows the ﬁrst statement. For a Bernoulli process, the nth inter–
arrival time Zn
Z
= Tn
T −Tn
T −1 is distributed geometrically with parameter p,
due to the strong Markov property. This completes the proof for the second
statement.
□
Thus the memoryless property states that no matter how long a service time or
an inter–arrival time has already passed, the probability of a service completion
or an arrival at the next time instant is always the same. Hence the system
process Q = (Qn : n ∈N0) of the M/M/1 queue in discrete time with arrival
process T and service times Sn
S
can be formulated easily as a homogeneous
Markov chain. It has state space E = N0 and transition probabilities p01 := p,
p00 := 1 −p, and
pij :=
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
p(1 −q),
j = i + 1
pq + (1 −p)(1 −q),
j = i
q(1 −p),
j = i −1
for i ≥1. Because of the simple state space, the transition matrix can be
displayed in the form of a triagonal matrix
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
1 −p
p
0
. . .
q(1 −p)
pq + (1 −p)(1 −q)
p(1 −q)
...
0
q(1 −p)
pq + (1 −p)(1 −q)
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
Since p, q > 0, the chain Q is irreducible. If p < q, then h(n) := n deﬁnes a
function which satisﬁes the conditions for Foster’s criterion, as
∞

k=0
pikh(k) = q(1 −p) · (i −1) + (qp + (1 −q)(1 −p)) · i
+ p(1 −q) · (i + 1)
= i −q(1 −p) + p(1 −q) = i −q + p ≤i −ε

Markov Chains and Queues in Discrete Time
33
for all i ∈N, with ε = q −p > 0, and ∞
k=0 p0k · h(k) = p < ∞show. The
ratio p/q is called the load of the queue. Thus the system process Q is positive
recurrent if the queue load is less than one.
In order to derive a stationary distribution for Q, we ﬁrst introduce notations
p′ := p(1 −q) and q′ := q(1 −p). Then we translate the condition πP = π
into the equations
π0 = π0(1 −p) + π1q′
(2.10)
(2.11)
πn = πn−1p′ + πn(1 −(p′ + q′)) + πn+1q′
(2.12)
for all n ≥2. For the solution, we guess the geometric form
πn+1 = πn · r
for all n ≥1, with r > 0. Thus equation (2.12) becomes
0 = πnp′ −πnr(p′ + q′) + πnr2q′ = πn

p′ −r(p′ + q′) + r2q′
for all n ≥1, which leads for non–trivial π ̸= 0
̸
to the roots r = 1 and
r = p′/q′ of the quadratic term.
In the ﬁrst case r = 1, we obtain πn+1 = πn for all n ≥1. This implies

j∈E πj
π = ∞and thus cannot lead to a stationary distribution. Hence in the
case r = 1 the geometric approach is not successful.
The second root r = p′/q′ allows solutions for the other equations (2.10) and
(2.11) too. This can be checked as follows: First, the relation
π1 = π0
p
q′ = π0
ρ
1 −p
is a requirement from equation (2.10). Then the second equation (2.11) yields
π2 = 1
q′

π1(p′ + q′) −π0p

= 1
q′
 p
q′ (p′ + q′) −p

π0
= π0
p
q′
p′ + q′
q′
−1

= π1
p′
q′
in accordance with our geometric approach. Now normalization of π leads to
1 =
∞

n=0
πn = π0

1 + p
q′
∞

n=1
p′
q′
n−1
π1 = π0p + π1(1 −p′ −q′) + π2q′

34
AN INTRODUCTION TO QUEUEING THEORY
from which we obtain
π0 =

1 + p
q′
∞

n=1
p′
q′
n−1−1
=

1 +
p
q′(1 −p′/q′)
−1
=

1 +
p
q′ −p′
−1
= (q′ −p′)(q′ −p′ + p)−1 = q −p
q
= 1 −ρ
with ρ := p/q, because of q′ −p′ = q −p. Hence the approach πn+1 = πn · r
with r = p′/q′ leads to a solution of πP = π.
Note that r < 1 if and only if p < q. Further, the mean inter–arrival time is
E(T1
T ) = 1/p and the mean service time is E(S1) = 1/q. Thus the geometric
approach is successful if the so–called stability condition
ρ = p
q = E(S1)
E(T1
T ) < 1
holds. This condition simply postulates that the mean service time be shorter
than the mean inter–arrival time. In this case, the stationary distribution π of
Q has the form
π0 = 1 −ρ
and
πn = (1 −ρ)
ρ
1 −prn−1
for all n ≥1. It thus is a modiﬁed geometric distribution with parameter
r = p′/q′ < 1.
Notes
Markov chains originate from a series of papers written by A. Markov at the
beginning of the 20th century. His ﬁrst application is given here as exercise
2.3. However, methods and terminology at that time were very different from
today’s presentations.
The literature on Markov chains is perhaps the most extensive in the ﬁeld of
stochastic processes. This is not surprising, as Markov chains form a simple
and useful starting point for the introduction of other processes.
Textbook presentations are given in Feller [34], Breiman [16], Karlin and Tay-
lor [46], or C¸ inlar [25], to name but a few. The treatment in Ross [75] contains
C
the useful concept of time–reversible Markov chains. An exhaustive introduc-
tion to Markov chains on general state spaces and conditions for their positive
recurrence is given in Meyn and Tweedie [59].

Markov Chains and Queues in Discrete Time
35
Exercise 2.1 Let (Xn
X
: n ∈N0) be a family of iid random variables with
discrete state space. Show that X = (Xn
X : n ∈N0) is a homogeneous Markov
chain.
Exercise 2.2 Let (Xn
X : n ∈N0) be iid random variables on N0 with probabil-
ities ai := P(Xn
X = i) for all n, i ∈N0. The event Xn
X > max(X0, . . . , Xn
X −1)
for n ≥1 is called a record at time n. Deﬁne Ti
T as the time of the ith record, i.e.
T0
T := 0 and Ti
T +1 := min{n ∈N : Xn
X
> XT
X
i
T } for all i ∈N0. Denote the ith
record value by Ri := XT
X
i
T . Show that (Ri : i ∈N0) and ((Ri, Ti
T ) : i ∈N0)
are Markov chains by determining their transition probabilities.
Exercise 2.3 Diffusion model by Bernoulli and Laplace
The following is a stochastic model for the ﬂow of two incompressible ﬂuids
between two containers: Two boxes contain m balls each. Of these 2m balls, b
are black and the others are white. The system is said to be in state i if the ﬁrst
box contains i black balls. A state transition is performed by choosing one ball
out of each box at random (meaning here that each ball is chosen with equal
probability) and then interchanging the two. Derive a Markov chain model for
the system and determine the transition probabilities.
Exercise 2.4 Let X denote a Markov chain with m < ∞states. Show that
if state j is accessible from state i, then it is accessible in at most m −1
transitions.
Exercise 2.5 Let p = (pn : n ∈N0) be a discrete probability distribution and
deﬁne
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
p0
p1
p2
. . .
p0
p1
...
p0
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
with all non–speciﬁed entries being zero. Let X denote a Markov chain with
state space N0 and transition matrix P. Derive an expression (in terms of
discrete convolutions) for the transition probabilities P(Xn
X +m = j|Xn
X
= i)
with n, m ∈N0 and i, j ∈N0. Apply the result to the special case of a
Bernoulli process (see example 2.3).
Exercise 2.6 Prove equation (2.6).
Exercise 2.7 Prove the equation P n(i, j) = n
k=1 Fk
F (i, j)P n−k(j, j) for all
n ∈N and i, j ∈E.

36
AN INTRODUCTION TO QUEUEING THEORY
Exercise 2.8 Let X denote a Markov chain with state space E = {1, . . . , 10}
and transition matrix
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
1/2
0
1/2
0
0
0
0
0
0
0
0
1/3
0
0
0
0
2/3
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1/3
1/3
0
0
0
1/3
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1/4
0
3/4
0
0
0
1/4
1/4
0
0
0
1/4
0
1/4
0
1
0
0
0
0
0
0
0
0
0
1/3
0
0
1/3
0
0
0
0
1/3
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
Reorder the states according to their communication classes and determine the
resulting form of the transition matrix as in representation (2.4). Determine
further a transition graph, in which










-










means that fij
f
> 0.
Exercise 2.9 Prove equation (2.7).
Hint: Derive a representation of Nj
N in terms of the random variables
An :=

1,
Xn
X = j
0,
Xn
X ̸≠
j
Exercise 2.10 Prove corollary 2.15.
Exercise 2.11 Prove remark 2.26.
Exercise 2.12 A server’s up time is k time units with probability pk = 2−k,
k ∈N. After failure the server is immediately replaced by an identical new
one. The up time of the new server is of course independent of the behaviour
of all preceding servers.
Let Xn
X denote the remaining up time of the server at time n ∈N0. Determine
the transition probabilities for the Markov chain X = (Xn
X
: n ∈N0) and
determine the stationary distribution of X.

Markov Chains and Queues in Discrete Time
37
Exercise 2.13 Let P denote the transition matrix of an irreducible Markov
chain X with discrete state space E = F ∪F c, where F c = E \ F. Write P
in block notation as
P =

PFF
P
PFF
P
c
PF
P
cF
PF
P
cF c

Show that the Markov chain X F restricted to the state space F has transition
matrix
P F = PFF
P
+ PFF
P
c(I −PF
P
cF c)−1PF
P
cF
with I denoting the identity matrix on F c.
Exercise 2.14 Let X denote a Markov chain with state space E = {0, . . . , m}
and transition matrix
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
p00
p01
p10
p11
p12
p21
p22
p23
...
...
...
pm,m−1
pmm
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
where pij > 0 for |i −j| = 1. Show that the stationary distribution π of X is
uniquely determined by
πn = π0 ·
n

i=1
pi−1,i
pi,i−1
and
π0 =
⎛
⎝
⎛
m

j=0
j
i=1
pi−1,i
pi,i−1
⎞
⎠
⎞−1
for all n = 1, . . . , m.
Use this result to determine the stationary distribution of the Bernoulli–Laplace
diffusion model with b = m (see exercise 2.3).
Exercise 2.15 Show that the second condition in theorem 2.33 can be substi-
tuted by the condition

j∈E
pijh(j) ≤h(i) −1
for all i ∈E \ F.
Exercise 2.16 Show the following complement to theorem 2.33: Let P denote
the transition matrix of a positive recurrent Markov chain with discrete state
space E. Then there is a function h : E →R and a ﬁnite subset F ⊂E such
that

j∈E
pijh(j) < ∞
for all i ∈F, and

j∈E
pijh(j) ≤h(i) −1
for all i ∈E \ F.

38
AN INTRODUCTION TO QUEUEING THEORY
Hint: Consider the conditional expectation of the remaining time until return-
ing to a ﬁxed set F of states.
Exercise 2.17 For the discrete, non–negative random walk with transition ma-
trix
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
p00
p01
p10
0
p12
p10
0
p12
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
determine the criterion of positive recurrence according to theorem 2.33.


Chapter 3
HOMOGENEOUS MARKOV PROCESSES
ON DISCRETE STATE SPACES
In the present chapter we will transfer the discrete time results of the previous
chapter to Markov processes in continuous time.
1.
Deﬁnition
Deﬁne T0
T := 0 and let (Tn
T : n ∈N) denote a sequence of positive real–valued
random variables with Tn
T +1 > Tn
T
for all n ∈N0 and Tn
T
→∞as n →∞.
Further, let E denote a countable state space and (Xn
X : n ∈N0) a sequence of
E–valued random variables. A process Y = (Yt
Y : t ∈R+
0 ) in continuous time
with
Yt
Y := Xn
X
for
Tn
T ≤t < Tn
T +1
is called a pure jump process. The variable Hn
H
:= Tn
T +1 −Tn
T (resp. Xn
X ) is
called the nth holding time (resp. the nth state) of the process Y. If further
X = (Xn
X
: n ∈N0) is a Markov chain with transition matrix P = (pij)i,j∈E
and the variables Hn
H
are independent and distributed exponentially with pa-
rameter λXn only depending on the state Xn
X , then Y is called homogeneous
Markov process with discrete state space E. The chain X is called the em-
bedded Markov chain of Y. As a technical assumption we always agree upon
the condition ˆλ := sup{λi : i ∈E} < ∞, i.e. the parameters for the exponen-
tial holding times shall be bounded.
An immediate consequence of the deﬁnition is that the paths of a Markov
process are step functions. The lengths of the holding times are almost cer-
tainly strictly positive, since exponential distributions are zero with probability
zero.

40
AN INTRODUCTION TO QUEUEING THEORY
time
Yt
X = X
X
X
T
T
T
T
1
2
3
4
T
0
2
1
3
. . .
H
H
H
H
0
1
2
3
Figure 3.1.
Typical path of a Markov process with discrete state space
Example 3.1 Poisson process
Deﬁne Xn
X
:= n deterministically. Then X = (Xn
X
: n ∈N0) is a Markov
chain with state space E = N0 and transition probabilities pn,n+1 = 1 for all
n ∈N0. Let the holding times Hn
H be distributed exponentially with identical
parameter λ > 0. Then the resulting process Y as deﬁned in the above deﬁni-
tion is a Markov process with state space N0. It is called Poisson process with
intensity (also: rate or parameter) λ.
Next we want to prove a property similar to the Markov property for Markov
chains in discrete time. To this aim, we need to show the memoryless prop-
erty for the exponential distribution, which is the analogue to the memoryless
property for geometric distributions in discrete time.
Lemma 3.2 Let H denote a random variable having an exponential distribu-
tion with parameter λ. Then the memoryless property
P(H > t + s|H > s) = P(H > t)
holds for all time durations s, t > 0.
Proof: We immediately check
P(H > t + s|H > s) = P(H > t + s, H > s)
P(H > s)
= P(H > t + s)
P(H > s)
= e−λ·(t+s)
e−λ·s
= e−λ·t = P(H > t)
which holds for all s, t > 0.
□

Homogeneous Markov Processes on Discrete State Spaces
41
Theorem 3.3 Let Y denote a Markov process with discrete state space E.
Then the Markov property
P(Yt
Y = j|Yu
Y : u ≤s) = P(Yt
Y = j|Ys
Y )
holds for all times s < t and states j ∈E.
Proof: Denote the state at time s by Ys
Y = i. Because of the memoryless prop-
erty of the exponential holding times, the remaining time in state i is distributed
exponentially with parameter λi, no matter how long the preceeding holding
time has been. After the holding time in the present state elapses, the process
changes to another state j according to the homogeneous Markov chain X.
Hence the probability for the next state being j is given by pij, independently
of any state of the process before time s. Now another exponential holding
time begins, and thus the past before time s will not have any inﬂuence on the
future of the process Y.
□
Analogous to the discrete time case, for any two time instances s < t the con-
ditional probabilities P(Yt
Y = j|Ys
Y = i) shall be called the transition proba-
bilities from time s to time t. We will now derive a recursion formula for the
transition probabilities of a Markov process by conditioning on the number of
jumps between time s and time t:
Theorem 3.4 The transition probabilities of a Markov process Y are given by
P(Yt
Y = j|Ys
Y = i) =
∞

n=0
P (n)
ij
P
(s, t)
for all times s < t and states i, j ∈E, with
P (0)
ij
P
(s, t) = δij · e−λi·(t−s)
and recursively
P (n+1)
ij
P
(s, t) =
 t
s

e−λi·uλi

k∈E
pikP (n)
kj
P
(u, t) du
for all n ∈N0.
Proof: The above representation follows immediately by conditioning on the
number of jumps in ]s, t]. The expressions P (n)
ij
P
(s, t) represent the condi-
tional probabilities that Yt
Y
= j and there are n jumps in ]s, t] given that

42
AN INTRODUCTION TO QUEUEING THEORY
Ys
Y = i. In the recursion formula the integral comprises all times u of a pos-
sible ﬁrst jump along with the Lebesgue density e−λi·uλi of this event, after
which the probability of n remaining jumps reaching state j at time t is given
by 
k∈E pikP (n)
kj
P
(u, t).
□
For every two time instances s < t, deﬁne the transition probability matrix
P(s, t) from time s to time t by its entries
Pij
P (s, t) := P(Yt
Y = j|Ys
Y = i)
Using the recursion formula, it is shown by induction on n that the conditional
probabilities P (n)
ij
P
(s, t) are homogeneous in time, i.e. they satisfy
P (n)
ij
P
(s, t) = P (n)
ij
P
(0, t −s)
for all s < t. Thus we can from now on restrict the analysis to the transition
probability matrices
P(t) := P(0, t)
with t ≥0. With this notation the Markov property yields the Chapman–
Kolmogorov equations
P(s + t) = P(s)P(t)
for all time durations s, t ≥0. Thus the family {P(t) : t ≥0} of transition
probability matrices forms a semi–group under the composition of matrix mul-
tiplication. In particular, we obtain for the neutral element of this semi–group
P(0) = IE
I
:= (δij)i,j∈E with δij = 1 for i = j and zero otherwise.
In order to derive a simpler expression for the transition probability matrices,
we need to introduce another concept, which will be called the generator ma-
trix. This is deﬁned as the matrix G = (gij)i,j∈E on E with entries
gij :=

−λi · (1 −pii),
i = j
λi · pij,
i ̸≠
j
for all states i, j ∈E. In particular, the relation
gii = −

j̸=
̸
i
gij
(3.1)
holds for all i ∈E.
The (i, j)th entry of the generator G is called the inﬁnitesimal transition rate
from state i to state j. Using these, we can illustrate the dynamics of a Markov
process in a directed graph where the nodes represent the states and an edge

Homogeneous Markov Processes on Discrete State Spaces
43










-
r










means that gij = r > 0. Such a graph is called a state transition graph of
the Markov process. With the convention pii = 0 the state transition graph
uniquely determines the Markov process.
Example 3.5 The state transition graph of the Poisson process with intensity
λ (see example 3.1) is given by










-
λ











-
λ










-
. . .
Figure 3.2.
Poisson process
Theorem 3.6 The transition probabilities Pij
P (t) of a Markov process satisfy
the systems
dPij
P (t)
dt
=

k∈E
Pik
P (t)gkj =

k∈E
gikPkj
P
(t)
of differential equations.
These are called the Kolmogorov forward and
backward equations.
Proof: From the representation in theorem 3.4, it follows by induction on the
number of jumps that all restricted probabilities P (n)(t) are Lebesgue inte-
grable with respect to t over ﬁnite intervals. Since the sum of all P (n)
ij
P
(t) is a
probability and thus bounded, we conclude by majorized convergence that also
P(t) is Lebesgue integrable with respect to t over ﬁnite intervals.
Now we can state the recursion
Pij
P (t) = e−λi·t · δij +
 t
0

e−λi·sλi

k∈E
pikPkj
P
(t −s) ds
which results from conditioning on the time s of the ﬁrst jump from state i. We
obtain further
Pij
P (t) = e−λi·t ·

δij +
 t
0

e+λi·uλi

k∈E
pikPkj
P
(u) du


44
AN INTRODUCTION TO QUEUEING THEORY
by substituting u = t −s in the integral. Since 
k∈E pik = 1 is bounded,
we conclude that P(t) is continuous in t. Further, we can differentiate P(t) as
given in the recursion and obtain
dPij
P (t)
dt
= −λie−λi·t ·

δij +
 t
0

f(u) du

+ e−λi·t · f(t)
with f denoting the integrand function. This means nothing else than
dPij
P (t)
dt
= −λiPij
P (t) + λi

k∈E
pikPkj
P
(t)
= −λi(1 −pii) · P (t) +

k̸=
̸
i
gikPkj
P
(t)
and thus proves the backward equations. For the forward equations, one only
needs to use the Chapman–Kolmogorov equations and apply the backward
equations in
dPij
P (t)
dt
= lim
h→0
Pij
P (t + h) −Pij
P (t)
h
= lim
h→0

k∈E
Pik
P (t)Pkj
P
(h) −δkj
h
=

k∈E
Pik
P (t) lim
h→0
Pkj
P
(h) −Pkj
P
(0)
h
=

k∈E
Pik
P (t)gkj
which holds for all i, j ∈E.
□
Theorem 3.7 The transition probability matrices can be expressed in terms of
the generator by
P(t) = eG·t :=
∞

n=0
tn
n!Gn
for all t ≥0, with Gn denoting the nth power of the matrix G.
Proof: First we validate the solution by
d
dteG·t = d
dt
∞

n=0
tn
n!Gn =
∞

n=1
Gn d
dt
tn
n! =
∞

n=1
Gn
tn−1
(n −1)! = GeG·t
which holds for all t ≥0. Furthermore, it is obvious that
GeG·t = G
∞

n=0
tn
n!Gn =
 ∞

n=0
tn
n!Gn

G = eG·tG
ij
P

Homogeneous Markov Processes on Discrete State Spaces
45
and thus P(t) = eG·t is a solution of Kolmogorov’s forward and backward
equations.
Now we show uniqueness of the solution. Let ˜P(t) denote another solution of
the forward equations. The differential equations with initial condition trans-
late into the integral equations
P(t) = IE
I
+
 t
0

P(u)G du
and
˜P(t) = IE
I
+
 t
0

˜P(u)G du
Deﬁne a norm for matrices M = (mij)i,j∈E on E by
∥M∥:= sup
⎧
⎨
⎧
⎩
⎨
j∈E
|mij| : i ∈E
⎫
⎬
⎫
⎭
⎬
Then ∥G∥≤2 · ˆλ and ∥AB∥≤∥A∥· ∥B∥for any two matrices A and B on
E. Further we obtain

P(t) −˜P(t)

 =



 t
0

P(u) −˜P(u) du G



≤
 t
0


P(u) −˜P(u)

 du · ∥G∥
(3.2)
≤∆t · t · ∥G∥
(3.3)
with ∆t := sup{∥P(u) −˜P(u)∥: u ≤t}, which is ﬁnite, since for all u ≥0
we know that ∥P(u)∥= ∥˜P(u)∥= 1. Plugging the result (3.3) into the right
hand of the bound (3.2) again (with time u instead of t), we obtain

P(t) −˜P(t)

 ≤
 t
0

∆t · u · ∥G∥du · ∥G∥= ∆t · t2
2 · ∥G∥2
Likewise, n–fold repetition of this step achieves the bound

P(t) −˜P(t)

 ≤∆t · tn
n! · ∥G∥n ≤∆t · (2ˆλ · t)n
n!
which in the limit n →∞yields 0 ≤

P(t) −˜P(t)

 ≤0 and consequently
P(t) = ˜P(t). As t has been chosen arbitrarily, the statement is proven.
□
Hence the generator of a Markov process uniquely determines all its transition
matrices. This can also be seen from the deﬁnition, if we agree (without loss

46
AN INTRODUCTION TO QUEUEING THEORY
of generality) upon the convention pii = 0 for all ∈E. Then the parameters
for the deﬁnition of the Markov process can be recovered by
λi = −gii
and
pij = gij
−gii
for all i ̸≠
j ∈E.
However, as in the discrete time case of Markov chains, Markov processes
are not completely determined by their transition probability matrices only.
The missing link to a complete characterization again is given by the initial
distribution π with πi = P(Y0
Y = X0 = i) for all i ∈E. Then we can express
all ﬁnite–dimensional marginal distributions as in
Theorem 3.8 For a Markov process Y with initial distribution π and time
instances 0 < t1 < . . . < tn, n ∈N, the equation
P(Yt
Y 1 = j1, . . . , Yt
Y n = jn)
=

i∈E
πiPi,j
P
1(t1)Pj
P 1,j2(t2 −t1) . . . Pj
P n−1,jn(tn −tn−1)
holds for all j1, . . . , jn ∈E.
The proof is left as an exercise. Thus a Markov process Y with transition
probability matrices (P(t) : t ≥0) admits a variety of versions depending on
the initial distribution π. Any such version shall be denoted by Yπ.
2.
Stationary Distribution
From now on we shall convene on the technical assumption
ˇλ := inf{λi : i ∈E} > 0
which holds for all queueing systems that we will examine. Then a Markov
process is called irreducible, transient, recurrent or positive recurrent if
the deﬁning Markov chain is.
An initial distribution π is called stationary if the process Yπ is stationary, i.e.
if
P(Y π
t
Y 1 = j1, . . . , Y π
t
Y n = jn) = P(Y π
t
Y 1+s = j1, . . . , Y π
t
Y n+s = jn)
for all n ∈N, 0 ≤t1 < . . . < tn, and states j1, . . . , jn ∈E, and s ≥0.
Theorem 3.9 A distribution π on E is stationary if and only if πG = 0 holds.

Homogeneous Markov Processes on Discrete State Spaces
47
Proof: First we obtain
πP(t) = πeG·t =
∞

n=0
tn
n!πGn = πIE
I
+
∞

n=1
tn
n!πGn = π + 0 = π
for all t ≥0, with 0 denoting the zero measure on E. With this, theorem 3.8
yields
P(Y π
t
Y 1 = j1, . . . , Y π
t
Y n = jn)
=

i∈E
πiPi,j
P
1(t1)Pj
P 1,j2(t2 −t1) . . . Pj
P n−1,jn(tn −tn−1)
= πj
π 1Pj
P 1,j2(t2 −t1) . . . Pj
P n−1,jn(tn −tn−1)
=

i∈E
πiPi,j
P
1(t1 + s)Pj
P 1,j2(t2 −t1) . . . Pj
P n−1,jn(tn −tn−1)
= P(Y π
t
Y 1+s = j1, . . . , Y π
t
Y n+s = jn)
for all times t1 < . . . < tn with n ∈N, and states j1, . . . , jn ∈E. Hence the
process Yπ is stationary.
On the other hand, if π is a stationary distribution, then we necessarily obtain
πP(t) = πeG·t = π for all t ≥0. As above, this means ∞
n=1
tn
n!πGn = 0 for
all t ≥0, which yields πG = 0 because of the uniqueness of the zero power
series.
□
By deﬁnition of the generator G and equation (3.1), the equation πG = 0 is
equivalent to an equation system

i̸=
̸
j
πigij = −πj
π gjj
g
⇐⇒

i̸=
̸
j
πigij = πj
π

i̸=
̸
j
gji
g
(3.4)
for all j ∈E. This system can be intepreted as follows. We call the value πigij
stochastic ﬂow from state i to state j in equilibrium. Then the above equations
mean that the accrued stochastic ﬂow into any state j equals the ﬂow out of this
state. Equations (3.4) are called the (global) balance equations.
Example 3.10 The generator of the Poisson process with parameter λ (see
example 3.1) is given by
G =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
−λ
λ
0
0
. . .
0
−λ
λ
0
...
0
0
−λ
λ
...
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟

48
AN INTRODUCTION TO QUEUEING THEORY
This process has no stationary distribution, which can be seen as follows. The
balance equations for the Poisson process are given by
π0λ = 0
and
πiλ = πi−1λ
for all i ≥1. It is immediately evident that these are solvable only by πi = 0
for all i ∈E, which means that there is no stationary distribution π.
The question of existence and uniqueness of a stationary distribution for Y
can be reduced to the same question for X, which we have examined in the
preceding chapter:
Theorem 3.11 Let the underlying Markov chain X in the deﬁnition of the
Markov process Y be irreducible and positive recurrent. Further assume that
ˇλ := inf{λi : i ∈E} > 0. Then there is a unique stationary distribution for
Y.
Proof: According to theorems 2.25 and 2.18, the transition matrix P of X
admits a unique stationary distribution ν with νP = ν. The generator G is
deﬁned by G = Λ(P −IE
I ), with Λ = diag(λi : i ∈E). Hence the measure
µ := νΛ−1 is stationary for Y. Since ˇλ > 0, the measure µ is ﬁnite, with total
mass bounded by ˇλ−1 < ∞. Now the normalization
πj
π :=
µj

i∈E µi
=
νj
ν /λj

i∈E νiν /λi
(3.5)
for all j ∈E yields a stationary distribution for Y. This is unique because ν is
unique and the construction of π from ν is reversible.
□
Finally we give two important results for the asymptotic behaviour of a Markov
process. These shall be proven in chapter 7 (see example 7.13). We call a
Markov process regular if it satisﬁes the conditions given in the preceding
theorem. If Y is a regular Markov process, then the limit
lim
t→∞P(Yt
Y = j) = πj
π
(3.6)
of the marginal distribution at time t tends to the stationary distribution as t
tends to inﬁnity. Further the limit
lim
t→∞Pij
P (t) = πj
π
(3.7)
holds for all i, j ∈E and is independent of i.

Homogeneous Markov Processes on Discrete State Spaces
49
Notes
An early text book on Markov processes with discrete state space is Chung
[27]. Other classical text book presentation are Karlin and Taylor [46], Breiman
[16], or C¸ inlar [25]. An exposition on non–homogeneous Markov processes on
C
discrete state spaces can be found under the name Markov jump processes in
Gikhman and Skorokhod [39, 38].
Exercise 3.1 Consider a population of male and female species. There is an
inﬁnitesimal rate λ > 0 that any male and female produce a single offspring,
which will be female with probability p. Determine a Markov process which
models the numbers Ft
F and Mt
M of female and male species at any time t.
Exercise 3.2 Let X and Y denote two independent random variables which
are distributed exponentially with parameters λ and µ, respectively. Prove the
following properties:
(a) X ̸≠
Y almost certainly.
(b) The random variable Z := min{X, Y } is distributed exponentially with
parameter λ + µ.
(c) P(X < Y ) = λ/(λ + µ)
Exercise 3.3 Let Y(1) and Y(2) denote independent Poisson processes with
intensities λ1 and λ2, respectively. Show that the process Y = (Yt
Y : t ∈R+
0 )
deﬁned by Yt
Y = Y (1)
t
Y
+ Y (2)
t
Y
for all t ≥0 is a Poisson process with intensity
λ = λ1 + λ2. The process Y is called the superposition of Y(1) and Y(2).
Exercise 3.4 Prove theorem 3.8.
Exercise 3.5 Determine the ﬁnite–dimensional marginal distributions for a
Poisson process with parameter λ.
Exercise 3.6 Let Y denote a Poisson process with parameter λ. Given that
there is exactly one arrival in the interval [0, t], show that the exact time of the
arrival within [0, t] is uniformly distributed.
Exercise 3.7 Verify the Chapman–Kolmogorov equations for a Poisson process.


Chapter 4
MARKOVIAN QUEUES IN CONTINUOUS TIME
The methods of analyzing Markov processes are already sufﬁcient for the treat-
ment of quite a variety of queueing systems. These are commonly known as
elementary or Markovian queues. The most classical of them shall be exam-
ined in this chapter.
1.
The M/M/1 Queue
The M/M/1 queue in continuous time is deﬁned by the following character-
istics: The arrival process is a Poisson process with some rate λ > 0. The
service times are iid and distributed exponentially with service rate µ > 0.
There is one server and the service discipline is ﬁrst come ﬁrst served (FCFS,
see example 1.1).
Poisson(λ)
Exp(µ)
Figure 4.1.
M/M/1 queue
For the Poisson process, the inter–arrival times are distributed exponentially
with parameter λ. Since the exponential distribution is memoryless, the system
process Q = (Qt : t ∈R+
0 ) can be modelled by a Markov process with state

52
AN INTRODUCTION TO QUEUEING THEORY
space E = N0 and generator
G =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
−λ
λ
0
0
. . .
µ
−λ −µ
λ
0
...
0
µ
−λ −µ
λ
...
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
Here, the ﬁrst line represents the possible transitions if the system is empty. In
this case there can only occur single arrivals according to the Poisson process
with rate λ. If the system is not empty, there are two possibilities: Either an
arrival occurs (with rate λ) or a service is completed (with rate µ). Contrary
to the M/M/1 queue in discrete time, arrivals and service completions cannot
occur at the same time. This follows from the memoryless property of the
exponential distribution and exercise 3.2. The parameter of the holding time
for the states of a non–empty system is explained by exercise 3.2.
Clearly, the structure of the matrix G shows that the process Q is irreducible
and hence there is at most one stationary distribution π for Q. According to
theorem 3.9, this must satisfy πG = 0, which translates into the system
π0λ = π1µ
(4.1)
πn(λ + µ) = πn−1λ + πn+1µ
for all
n ≥1
(4.2)
∞

n=0
πn = 1
(4.3)
of equations, where the latter is simply the normalization of the distribution π.
The ﬁrst two equations are the global balance equations and can be illustrated
by the following scheme:
λ
µ
λ
µ
...
m
λ
λ
λ
µ
µ
µ
...
Figure 4.2.
Transition rates for the M/M/1 queue
This gives the rates of jumps between the states of the system. If we encircle
any one state, then the sum of the rates belonging to the arcs reaching into this
state must equal the sum of the rates which belong to the arcs that go out of
this state. If this is the case, then we say that the system is in balance. The
conditions for this are given in equations (4.1) and (4.2).
0
1
2
1

Markovian Queues in Continuous Time
53
The solution of the above system of equations can be obtained by the following
considerations: The ﬁrst equation yields
π1 = π0
λ
µ =: π0ρ
with ρ := λ/µ. By induction on n we obtain from the second equation
πn+1 = 1
µ(πn(λ + µ) −πn−1λ) = πn
λ
µ + πn −πn−1
λ
µ
= πnρ
for all n ∈N, where the last equality holds by induction hypothesis. Thus the
geometric approach πn = π0ρn for all n ∈N0 solves the ﬁrst two equations.
The last equation now yields
1 =
∞

n=0
πn = π0
∞

n=0
ρn =
1
1 −ρπ0
if and only if ρ < 1, which means λ < µ. Hence there is a stationary distribu-
tion of the system, given by
πn = (1 −ρ)ρn
for all n ∈N0, if and only if the so–called queue load ρ = λ/µ remains
smaller than one.
In this case several performance measures of the queueing system can be de-
rived immediately. All of them are computed by means of the stationary dis-
tribution. Thus they hold only for the system being in equilibrium, which is
attained asymptotically.
For instance, the probability that the system is empty is given by π0 = 1 −ρ.
The mean and the variance of the number N of users in the system are given
as
E(N) =
∞

n=1
nπn = (1 −ρ)
∞

n=1
nρn =
ρ
1 −ρ
and Var(N) = ρ/(1 −ρ)2. The probability RK that there are at least K users
in the system is
RK =
∞

n=K
πn = (1 −ρ)
∞

n=K
ρn = ρK
As expected, these equations show that with increasing load ρ →1 the mean
number of users in the system grows and the probability of an idle system
decreases.

54
AN INTRODUCTION TO QUEUEING THEORY
2.
Skip–Free Markov Processes
There are many variations of the M/M/1 queue which can be analyzed by the
same method. In order to show this we ﬁrst put the analysis presented in the
preceding section in a more general context. This will be applicable to a large
variety of queueing models.
The Markov process which models the M/M/1 queue has the decisive property
that transitions are allowed to neighbouring states only, i.e. gij = 0 for states
i, j ∈N0 with |i−j| > 1. The result is a very simple state transition graph of a
linear form and correspondingly a set of balance equations, given by (4.1) and
(4.2), which can be solved easily. We can retain the same method of analysis
if we relax the special assumption that gi,i+1 and gi,i−1 be independent of i.
Thus we deﬁne a skip–free Markov process by the property that its generator
G = (gij)i,j∈E satisﬁes gij = 0 for all states i, j ∈E ⊂N0 with |i −
j| > 1. For queueing systems this means that there are only single arrivals or
departures. Thus every Markovian queueing system with single arrivals and
departures can be modelled by a skip–free Markov process.
Denote the remaining inﬁnitesimal transition rates by
λi := gi,i+1
and
µi := gi,i−1
for all possible values of i. The rates λi and µi are called arrival rates and
departure rates, respectively. The state transition graph of such a process
assumes the form
0
1
2
. . .
1
2
λ
λ
λ
µ
µ
µ
0
1
2
3
2
1
Figure 4.3.
A skip–free Markov process
Its balance equations are given by λ0π0 = µ1π1 and
(λi + µi)πi = λi−1πi−1 + µi+1πi+1
for all i ∈N. By induction on i it is easily shown that these are equivalent to
the equation system
λi−1πi−1 = µiπi
(4.4)

Markovian Queues in Continuous Time
55
for all i ∈N. This system is solved by successive elimination with a solution
of the form
πi = π0
i−1

j=0
λj
µj+1
= π0
λ0λ1 · · · λi−1
µ1µ2 · · · µi
(4.5)
for all i ≥1. The solution π is a probability distribution if and only if it can be
normalized, i.e. if 
n∈E πn = 1. This condition implies
1 =

n∈E
π0
n−1

j=0
λj
µj+1
= π0

n∈E
n−1

j=0
λj
µj+1
with the empty product being deﬁned as one. This means that
π0 =
⎛
⎝
⎛

n∈E
n−1

j=0
λj
µj+1
⎞
⎠
⎞−1
(4.6)
and thus π is a probability distribution if and only if the series in the brack-
ets converges. In this case, the stationary distribution of a skip–free Markov
process is given by (4.6) and (4.5).
3.
The M/M/∞Queue
The ﬁrst application of the analysis of the last section to a queueing system
shall be the M/M/∞queue. This is a queue without queueing: There are in-
ﬁnitely many servers such that every incoming user ﬁnds an idle server im-
mediately. Arrivals are governed by a Poisson process with intensity λ > 0,
and the service times are exponentially distributed with rate µ > 0, equal for
each server. Due to lemma 3.2, the system process is Markovian. Furthermore,
there are only single arrivals and departures. Hence the M/M/∞queue can be
modelled by a skip–free Markov process.
Since the arrival process is independent of the rest of the queue, the arrival
rates of the respective skip–free Markov process are constant. In the notation
of section 2 we can thus specify λn = λ for all n ∈N0. Departures occur
upon service completions. According to lemma 3.2 and due to the memoryless
property of the exponential distribution (see lemma 3.2), the departure rates
are given by µn = n · µ for all n ∈N.
Deﬁne ρ := λ/µ. Then the series in (4.6) assumes the value
∞

n=0
n−1

j=0
λj
µj+1
=
∞

n=0
ρn
n! = eρ

56
AN INTRODUCTION TO QUEUEING THEORY
and thus converges regardless of the value of ρ. This means that the M/M/∞
queue always has a stationary distribution, which is not surprising as inﬁnitely
many servers cannot be exhausted, whatever the arrival intensity amounts to.
Due to formulae (4.6) and (4.5), we obtain the stationary distribution π as given
by π0 = e−ρ and
πn = e−ρ · ρn
n!
for all n ∈N, which is a Poisson distribution with parameter ρ. Hence the
mean and the variance of the number N of users in the stationary system are
given by E(N) = Var(N) = ρ.
Since there is no queueing in the M/M/∞system, all waiting times are zero
and the mean sojourn time in the system equals 1/µ. This means that all users
passing through such a system are independently kept there for an exponen-
tially distributed time. In the context of queueing networks (see chapter 5), the
M/M/∞queue is therefore often called an (independent) delay system.
4.
The M/M/k Queue
The M/M/k queue is provided with k identical servers which can serve users
in parallel. Users arrive according to a Poisson process with intensity λ > 0,
and the service time distribution is exponential with parameter µ > 0 at all
servers. Whenever a user arrives and ﬁnds all servers busy (i.e. at least k users
in the system) he queues up in the waiting room. From there the next waiting
user is served in the order of a FIFO discipline as soon as one of the servers
becomes idle. An arriving user ﬁnding less than k users already in the system
(i.e. there are idle servers at the time of arrival) chooses any server and starts
service immediately.
For this type of queue the dynamics is a mixture between the M/M/∞queue
and the M/M/1 queue. Up to the value of k users in the system, the service
(and thus the departure) rate increases like µn = n · µ for 1 ≤n ≤k. Starting
from k users in the system there are no servers anymore to keep up with newly
arriving users, and the departure rate remains µn = k ·µ for all n ≥k +1. The
independence of the arrival process yields constant arrival rates λn = λ for all
n ∈N0.
Again we deﬁne ρ := λ/µ. The series in (4.6) speciﬁes to
∞

n=0
n−1

j=0
λj
µj+1
=
k−1

n=0
ρn
n! + ρk
k!
∞

n=0
 ρ
k
!n

Markovian Queues in Continuous Time
57
queue
arrival process
departure
 process
servers
λ
µ
µ
Figure 4.4.
M/M/k queue
which is ﬁnite if and only if ρ < k. In this case the stationary distribution π is
given by formulae (4.6) and (4.5) as
π0 =
k−1

n=0
ρn
n! +
ρk
(k −1)! · (k −ρ)
−1
and
πn = π0 · ρn
n! ,
1 ≤n ≤k
πn = πk ·
 ρ
k
!n−k
,
n > k
Here we see the M/M/∞form for n ≤k and the M/M/1 form beginning with
n ≥k, where πk substitutes the base value that is played by π0 for the pure
M/M/1 queue.
The fact that the M/M/k queue behaves for more than k users in the system like
an M/M/1 queue with load ρ/k is further illustrated by the following observa-
tion. Let N denote the number of users in the system that is in equilibrium.
Consider the conditional probability pn := P(N = n|N ≥k) for n ≥k. This
is computed as
pn =
πn
∞
i=k πi
= πk
 ρ
k
!n−k"
πk
∞

i=k
 ρ
k
!i−k
=
 ρ
k
!n−k  
1 −ρ
k
!
Since n−k is the number Nq
N of users waiting in the queue, the conditional dis-
tribution of Nq
N given that all servers are busy has exactly the same (geometric)
form as the stationary distribution for the M/M/1 system process.

58
AN INTRODUCTION TO QUEUEING THEORY
The probability P{N ≥k} of the conditioning event that in equilibrium all
servers are busy is given by
∞

n=k
πn =

1 + (k −1)! · (k −ρ) ·
k−1

n=0
ρn−k
n!
−1
(4.7)
This is the probability that a newly arriving user must wait before he is served.
The above formula for it is called Erlang’s delay formula.
5.
The M/M/k/k Queue
In stochastic modelling there always is a trade–off between the adaptation of
the model to reality and its simplicity, i.e. its analytic tractability. We have
seen that the nicest solutions could be derived for the M/M/1 queue (a geomet-
ric distribution) and the M/M/∞queue (a Poisson distribution). The solution
for the M/M/k queue, which is more realistic for most practical applications, is
also more involved. For all these models we kept the often unrealistic assump-
tion of an inﬁnite waiting room. The models in this and the following sections
stem from more realistic speciﬁcations. Historically, they belong to the ﬁrst
applications which founded the ﬁeld of queueing theory.
In the times of A.K. Erlang, at the beginning of the 20th century, telephone
calls had to be connected by an operator. The telephone companies installed
call centers where a number k of operators served call requests which arrived
from a large number of subscribers. Whenever all operators are busy with
serving call requests and a new subscriber calls to get a line, this subscriber
will be rejected.
If we model the arriving requests by a Poisson process and the duration of
the operators’ services by an exponential distribution, then we get an M/M/k/k
queue as a model for this application. The subscribers with their call requests
are the users and the operators are the servers. There are k servers and as many
places in the system, i.e. there is no additional waiting room.
Let the intensity of the Poisson arrival process and the rate of the exponential
service times be denoted by λ > 0 and µ > 0, respectively. Again we can use
a skip–free Markov process to analyze this system. In this notation, we obtain
λn = λ for all n = 0, . . . , k −1 and µn = n · µ for n = 1, . . . , k. The values
of λn and µn are zero for all other indices n. Deﬁne ρ := λ/µ. The series in
(4.6) is in this case

n∈E
n−1

j=0
λj
µj+1
=
k

n=0
ρn
n!

Markovian Queues in Continuous Time
59
which is ﬁnite, regardless of the value for ρ. Hence a stationary distribution π
always exists and is given by
π0 =
 k

n=0
ρn
n!
−1
and
πn = π0 · ρn
n!
for all n = 1, . . . , k. The main performance measure for this application is the
probability that all operators are busy and the company is unable to accept new
call requests. This is given by
πk = ρk
k!
 k

n=0
ρn
n!
−1
which of course is valid only under the stationary regime, i.e. in equilibrium.
This expression is known as Erlang’s loss formula.
Note that the expression of π0 for the M/M/∞queue is the limit of the re-
spective expression for the M/M/k/k model as k tends to inﬁnity. Even further,
the stationary distribution for the M/M/k/k queue converges to the stationary
distribution of the M/M/∞for increasing k.
6.
The M/M/k/k+c/N Queue
A simplifying assumption in the previous model has been the constant arrival
rates λn = λ. This implies that even for a high number of users in the queue
the intensity of new arrivals does not diminish. While this is a reasonable as-
sumption for an application to call centers, where the number of operators (and
thus the maximal number of users in the system) is only marginal compared to
the number of all subscribers, there are other applications for which such an
assumption would not be realistic.
Consider a closed computer network with k servers and N terminals. Every
terminal sends a job to one of the servers after some exponentially distributed
think time. If a server is available, i.e. idle, then this job is served, demanding
an exponential service time. A terminal that has a job in a server may not send
another job request during the service time. Whenever a terminal sends a job
request and all servers are busy at that time, then the job is put into a queue.
This queue has maximal capacity c, i.e. if a terminal sends a job request and
the queue is already ﬁlled with c jobs, then this new job request is rejected and
the terminal starts another think time.
This application can be modelled by an M/M/k/k+c/N queue if we interpret the
users in the system as the job requests that are in service or waiting. Denote

60
AN INTRODUCTION TO QUEUEING THEORY
λ
 servers
µ
µ
queue
λ
 terminals
Figure 4.5.
A closed computer network
the parameters of the exponential think time and service time distributions by
λ > 0 and µ > 0, respectively. Without loss of generality we may assume that
k + c ≤N. Then the queue in consideration is a skip–free Markov process
with arrival rates λn = (N −n) · λ for n = 0, . . . , k + c −1 and departure
rates µn = min(n, k) · µ for n = 1, . . . , k + c. As always, deﬁne ρ := λ/µ.
The series in (4.6) amounts to

n∈E
n−1

j=0
λj
µj+1
=
k

n=0
N
n

· ρn +
k+c

n=k+1
N! · ρn
(N −n)! · k! · kn−k
(4.8)
and thus is ﬁnite for every value of ρ. The stationary distribution π is given by
π0 =
 k

n=0
N
n

· ρn +
k+c

n=k+1
N! · ρn
(N −n)! · k! · kn−k
−1
and
πn = π0 ·
N
n

· ρn,
1 ≤n ≤k
πn = π0 ·
N! · ρn
(N −n)! · k! · kn−k ,
k + 1 ≤n ≤k + c
There are several interesting special cases. For c = 0 there is no room for a
queue of waiting jobs. Then the stationary distribution simpliﬁes to a binomial

Markovian Queues in Continuous Time
61
distribution with parameters (N, p), where p = ρ/(1 + ρ), which is truncated
to the states n = 0, . . . , k. Such a distribution is called an Engset distribution.
For c = N−k the queue has an important application in reliability theory. This
is known as the machine repair problem. In a production site there are N ma-
chines which are prone to failure. Each of them breaks down after a working
time which is exponentially distributed with parameter λ. There are k repair-
men that take care of the broken machines sequentially. The repair times are
exponential with parameter µ. Then the system process of the M/M/k/N/N
queue yields the number of broken machines.
Notes
The models presented in this chapter are the oldest within queueing theory.
Applications to telephone networks date back to the beginning of the 20th cen-
tury, notably Erlang [33] and Engset [32].
Skip–free Markov processes have been extensively used for populations mod-
els. Therefore the name birth–and–death processes is very popular for them,
with λi and µi denoting the transition rates for a birth and a death, respectively,
if the population has i members. However, the authors think that such a name
is inappropriate for queueing models and thus prefer the more technical term
skip–free.
For more Markovian queueing models see Kleinrock [50]. An analysis of non–
homogeneous (namely periodic) Markovian queues is given in Breuer [17, 22].
Exercise 4.1 Verify the formula Var(N) = ρ/(1 −ρ)2 for the stationary
variance of the number of users in the M/M/1 queue.
Exercise 4.2 Show that the equation system (4.4) is equivalent to the balance
equations for a skip–free Markov process. Prove the form (4.5) of its solution.
Exercise 4.3 Prove Erlang’s delay formula (4.7).
Exercise 4.4 Compare the stationary mean number of users in the system for
the following three queueing systems: (a) an M/M/1 queue with arrival inten-
sity λ and service rate µ, (b) an M/M/2 system with arrival intensity λ and
service rate µ/2, and (c) two independent M/M/1 queues with arrival intensity
λ/2 to each of them and equal service rate µ. Explain the differences.
Exercise 4.5 Explain equation (4.8).

62
AN INTRODUCTION TO QUEUEING THEORY
Exercise 4.6 Show that the stationary distribution for an M/M/k/k/N queue
is an Engset distribution.
Exercise 4.7 Analyze the M/M/1/c queue with arrival intensity λ and service
rate µ. This always has a stationary distribution π. Show that in the limit
c →∞, there are two possibilities: Either ρ < 1 and π converges to the
stationary distribution of the M/M/1 queue, or ρ ≥1 and π converges to the
zero measure.
Exercise 4.8 Examine the M/M/1 queue with users who are discouraged by
long queue lengths. This can be modelled by arrival rates λn = λ/(n + 1) for
all n ∈N0. Show that the stationary distribution is Poisson.

Chapter 5
MARKOVIAN QUEUEING NETWORKS
A set of interconnected queueing stations in which any user, upon departing
from one station, can join another or must leave the total system is called a
queueing network. The paths along which a user may travel from station to
station are determined by routing probabilities qij. Travel times, in general,
are assumed to be zero.
A queueing network may be regarded as a directed graph whose nodes rep-
resent the stations, and whose edges represent links between nodes. Between
nodes i and j an edge exists if and only if the routing probability qij, i.e. the
probability to join station j after service completion at station i, is greater
than zero. There may be also links from and to the outside of the network,
representing the possibility for users to enter or leave the system. Let qjq 0 de-
note the probability for a user to depart from the network after being served
at node j. Then M
k=0 qjk
q
= 1, with M the number of stations in the net-
work. The matrix Q = (qij)i,j≤M is called the routing matrix of the network.
Given Q, the probabilities for network departures are implicitly determined by
qjq 0 = 1 −M
k=1 qjk
q
.
Routing probabilities may be state dependent, where a network state usually is
deﬁned by the vector n = (n1, . . . , nM) of actual numbers ni of customers in
stations i = 1, . . . , M. More complex state deﬁnitions arise when customers
of different classes require different amounts of service and follow different
routes through the network. It may be the case that a particular routing behav-
iour is associated with a certain group of classes, while other groups follow
different rules. This leads to the notion of chains in a network. A chain de-
ﬁnes a particular subset (called category) of customers who travel through the
network according to a particular routing mechanism. Class changes of cus-
tomers within a chain are possible, but no customer can pass over to some class

64
AN INTRODUCTION TO QUEUEING THEORY
of another chain. The pair of class and chain identiﬁers is called a category
index.
"!
# 
"!
"!
!
 
!
j
"!
# 
"!
"!
 
!
j
"!
# 
"!
"!
!
 
!
j
-
-
-
-
?
6
?
6
6
?-
-

6

6
-
-
?
?
6
-?
-
6
-
i
j
k
qij
qjq 0
q0i
Figure 5.1.
Open Queueing Network
A network is called a closed network if there is no trafﬁc entering the network
from outside, and all departure probabilities qi0 are zero. A network that allows
incoming and outgoing trafﬁc is called an open network. Since we are only
interested in systems that eventually reach equilibrium, the cases with entering
but no outgoing trafﬁc, or vice versa, are excluded from investigation. As
mentioned above, it is further possible to discriminate between different user
classes and different chains in a network. Each chain is associated with its own
routing matrix Qc. In this case the network may be open for some chains, and
closed for others. Such a network is called a mixed network. In this book we
concentrate on the case of state independent routing.
In the simplest case, when customers are non-distinguishable, the dynamic be-
haviour of a queueing network is best described by a vector-valued stochastic
process (Nt
N : t ≥0) with state space NM
0 . In case that we consider different
customer classes and/or special characteristics of service or even inter-arrival
times, a network state, clearly, may be described differently. As a construct for
stochastic modelling, queueing networks are subject to performance analysis,
the performance measures of interest being similar to those for isolated sta-
tions. In most cases one is interested in the average delay (or system time) ¯T
that a user experiences when travelling through the network, and in the mean
throughput ¯S as well as the mean total number ¯N of customers that are resi-
dent. According to Little’s result (see theorem 1.9), these quantities are related
by
¯S · ¯T = ¯N.
(5.1)
They can easily be evaluated by means of the stationary state probabilities (that
hold in equilibrium), if those exist and are known. For instance, with pn denot-

Markovian Queueing Networks
65
ing the stationary probability for the network to be in state n = (n1, . . . , nM),
¯N =

n∈NM
0
pn ·
M

i=1
ni.
In general, the calculation of stationary probabilities, if they exist, represents
an unsolved or at least intricate problem. This is due to the fact that in most
cases no closed form expressions are known. Nevertheless, there are queueing
networks for which stationary state probabilities can be obtained by forming
the product of all those stationary state probabilities that are associated with
the network stations in isolation. Such queueing networks are called product
form (PF-) networks or separable networks. Among separable networks, the
best understood are Markovian queueing networks, i.e. networks for which the
stochastic process (Nt
N : t ≥0) is Markov, and which allow a product form
solution. We shall be concerned mainly with the class of PF-networks in the
following sections, and shall concentrate on the most simple cases only. For
continuing information about queueing networks of more complex structure
the reader is referred to the abundant literature on this topic.
1.
Balance Equations and Reversibility
Properties
Let N = (Nt
N : t ≥0) be a vector-valued continuous time Markov process
with state space E that describes a queueing network with M stations. In the
simplest case, when there is only one class of customers travelling through the
network, and no phase structures need to be considered for service (and inter-
arrival) time distributions, the state space E forms a subset of NM
0 .
N can be considered as a random walk process on the network graph. Let G =
(gmn)m,n∈E denote the generator matrix of N. Then, given that the process is
irreducible, it assumes equilibrium if and only if the system of equations
p G =

m∈E
pm gmn = o
(5.2)
possesses a ﬁnite positive solution p = (pn)n∈E (see theorem 3.9). Any such
solution p can be normed as to satisfy 
n∈E pn = 1 and to represent the
unique stationary distribution of N, i. e. the joint stationary queue length
distribution of the network.
For indistinguishable customers, irreducibility
of N is equivalent to the possibility for a customer to be able, upon leav-
ing a station i and subsequently travelling through the network, to ﬁnally
reach any other station j or, in case of an open network, to reach the exte-
rior of the network. Mathematically spoken, this means that there are integers

66
AN INTRODUCTION TO QUEUEING THEORY
k1 = i, k2, . . . , kn, kn+1 = j such that #n
ℓ=1 qkℓ,kℓ+1 > 0, where in case that
the exterior is meant by the source or the destination, the respective index i or
j has value 0.
Equation (5.2) mirrors a situation that we call global balance. A term of the
form pm gmn, where gmn is the instantaneous transition rate from state m to
state n, is called probability ﬂux or rate of ﬂow from m to n. Since G, as a
generator matrix, satisﬁes 
n∈E gmn = 0, (5.2) is equivalent to

m∈E
m̸=
̸
n
pm gmn =

m∈E
m̸=
̸
n
pn gnm,
(5.3)
stating that the probability ﬂux into state n equals the probability ﬂux out of
state n. As opposed to that, we speak of a detailed balance equation, if
pm gmn = pn gnm.
(5.4)
There are several concepts of balance between rates of ﬂow in Markov processes
and, correspondingly, in Markovian queueing networks. These concepts are
tightly connected with the property of reversibility in the theory of Markov
processes.
In order to illustrate this relationship, let us ﬁrst specify what
is meant by a reversed process N (r) = (N(r)
t
N
)t∈R+
0 associated with some
Markov process N = (Nt
N )t∈R+
0 with state space E.
The reversal N (r) of the Markov process N is a process that is develop-
ing in time in forward direction just as the original process does in back-
ward direction, on the same state space E, i. e., for some τ ∈R+
0 , we have
N(r)
t
N
= Nτ
N −t ∀t ∈R+
0 . If N is time-homogeneous and stationary, the value
of τ does not matter, and so
N(r)
t
N
= N−
N
t
for all
t ∈R+
0 .
N is called the forward process corresponding to the reversed or backward
process N (r). If the forward process N is time-homogeneous, irreducible, and
stationary, then so is the reversed process N (r).
Let G = (gmn)m,n∈E and G(r) = (g(r)
mn)m,n∈E denote the generator matrices
of N and N (r), respectively, with total transition rates
γm =

n∈E
n̸=
̸
m
gmn,
γ(r)
m =

n∈E
n̸=
̸
m
g(r)
mn
for any m ∈E.
As can easily be seen, the instantaneous transition rates gmn and g(r)
mn are,
in general, not the same for an arbitrary pair of states m, n. On the other

Markovian Queueing Networks
67
side, given that p = (pn)n∈E and p(r) = (p(r)
n )n∈E denote the stationary
distribution vectors of N and N (r), we have
p(r) = p.
(5.5)
This follows directly from the fact that reversing time does not alter the average
fraction of time the process spends in a state. Setting −t−dt =: t0, the deﬁning
equation
P(Nt
N +dt = n, Nt
N = m) = P(N(r)
−t−dt = n, N(r)
−t = m)
= P(N(r)
t
N 0+dt = m, N(r)
t
N 0 = n)
leads to pm · P(Nt
N +dt = n | Nt
N = m) = p(r)
n · P(N(r)
t
N 0+dt = m | N(r)
t
N 0
= n),
such that, by dividing both sides by dt, letting dt →0, and observing (5.5), we
obtain
pm · gmn = pn · g(r)
nm.
(5.6)
An important statement that characterizes the transition rates of the reversed
process is the following.
Theorem 5.1 Let N = (Nt
N )t∈R+
0 be a stationary Markov process with state
space E and generator G = (gmn)m,n∈E. Assume that there are nonnegative
numbers g∗
mn satisfying

n∈E
gmn =

n∈E
g∗
mn = 0 for all m ∈E,
and positive numbers pn, n ∈E, summing to 1, such that the equations
pmgmn = png∗
nm for all m ∈E
are satisﬁed.
Then (g∗
mn)m,n∈E = G(r) is the generator of the reversed
process, and the pn, n ∈E, form the stationary probability vector p for both,
the reversed and the forward process.
Proof: In order to show that p = (pn)n∈E is the stationary vector of N, ob-
serve that 
m∈E pm gmn = pn

m∈E g∗
nm
!= pn

m∈E gnm = o, saying
that p satisﬁes the global balance equation. Additionally, pmgmn = png∗
nm
implies g∗
nm = g(r)
nm according to (5.6).
□
Joint distributions of the original and the reversed process are not identical in
general. With t1 < . . . < tk, a kth-order joint distribution of N reads
pm1...mk(t1, . . . , tk) = P(Nt
N 1 = m1, . . . , Nt
N k = mk),

68
AN INTRODUCTION TO QUEUEING THEORY
whereas
P(N(r)
t
N 1 = m1, . . . , N(r)
t
N k = mk) = P(N−
N
t1 = m1, . . . , N−
N
tk = mk),
which need not be the same as pm1...mk(t1, . . . , tk).
Deﬁnition 5.2 A stochastic process is called reversible, if the joint distribu-
tions of the forward and the reversed process are identical, i.e.
P(Nt
N 1 = m1, . . . , Nt
N k = mk) = P(N(r)
t
N 1 = m1, . . . , N(r)
t
N k = mk).
Reversibility is related to the notion of detailed balance equations (5.4). First
note that any reversible Markov process is stationary, as can immediately be
deduced from the equality
P(Nt
N 1, . . . , Nt
N k) = P(Nt
N 1+τ, . . . , Nt
N k+τ) = P(N−
N
t1, . . . , N−
N
tk)
for any τ ∈R+
0 . Secondly, the following more general statement holds.
Theorem 5.3 A stationary Markov process N = (Nt
N )t∈R+
0 is reversible if and
only if the detailed balance equations (5.4) are satisﬁed for all m, n ∈E and
some positive vector p = (pn)n∈E with 
n∈E pn = 1.
Proof: 1. The properties of stationarity and reversibility imply that P(Nt
N = n)
does not depend on t. The numbers P(Nt
N = n) =: pn are positive and sum to
1. From reversibility and time-homogeneity we can conclude that
P(Nt
N +dt = n, Nt
N = m) = P(N(r)
t
N +dt = n, N(r)
t
N
= m)
= P(N−
N
t−dt = n, N−
N
t = m),
which (setting −t −dt =: t0) is equivalent to pm · P(Nt
N +dt = n | Nt
N = m) =
pn · P(Nt
N 0+dt = m | Nt
N 0 = n).1 Forming the differential quotient on each
side, one obtains (5.4).
2. The detailed balance equations guarantee global balance, so p represents the
equilibrium distribution of the Markov process. Considering now an arbitrary
interval [−T, T], we calculate the joint probability density for the event that
the process is in state m1 at time −T, jumps to state m2 at time −T + x1,
to state m3 at time −T + x1 + x2, and so forth, until it reaches state mk
at time −T + k−1
ν=1 xν, staying there until T, i. e. for some time interval of
length xk that satisﬁes k
ν=1 xν = 2T. The probability, that upon leaving a
1Remember, that it is even possible here to replace t0 by t due to time-homogeneity.

Markovian Queueing Networks
69
state mν the process jumps to state mν+1, is gmνmν+1/γmν. Further, since we
have a Markov process, the probability density of the sojourn time in state mν
equals γmν e−γmν ·xν, whereas we have P(sojourn time in state mk > xk) =
e−γmk·xk. As a consequence, the probability density for the above mentioned
process behaviour in [−T, T] reads
pm1e−γm1·x1gm1m2e−γm2·x2gm2m3 . . . e−γmk−1·xk−1gmk−1mke−γmk·xk.
Applying now the detailed balance property, we obtain for the same density
the expression
pmke−γm1·x1gmkmk−1e−γm2·x2gmk−1mk−2 . . . e−γmk−1·xk−1gm2m1e−γmk·xk
(since pm1 gm1m2 gm2m3 = pm2 gm2m1 gm2m3
!= pm3 gm3m2 gm2m1, etc.).
This density, but, describes a process behaviour, where the process starts at
time −T in state mk, stays there for some time xk, then jumps to state mk−1,
stays there for some time xk−1, and so forth, until it reaches state m1, where
it remains at least for a period of x1 time units. Consequently, the reversed
process (Nt
N )t∈[−T,T] proves to behave exactly in the same way as the reversed
process (N−
N
t)t∈[−T,T]. Since T has been arbitrarily chosen, N must be re-
versible.
□
For queueing network analyses the property of product form related to the state
probabilities of isolated stations is of paramount importance. The following
result relates reversibility with some other type of product form.
Theorem 5.4 The stationary distribution of any irreducible and reversible
Markov process can be calculated from a product of ratios of transition rates.
Proof: Let N be an irreducible and reversible stationary Markov process, such
that, according to theorem 5.3, the detailed balance equations (5.4) are satis-
ﬁed. We select an arbitrary state, say s = (s1, . . . , sM), as a ﬁxed ”starting
state”, from which any other state n is reachable due to irreducibility, that is,
there is at least one sequence s = m1, m2, . . . , mk = n such that
gm1m2 gm2m3 . . . gmk−1mk ̸= 0
̸
.
Using this fact, we further select for each state n ∈E one and only one con-
necting sequence of this form with m1 = s and mk = n, and deﬁne positive
numbers πn by
πn =

gm1m2 gm2m3...gmk−1mk
gmkmk−1 gmk−1mk−2...gm2m1
for
n ̸≠
s
1
for
n = s

70
AN INTRODUCTION TO QUEUEING THEORY
(clearly, the intermediate states mν as well as the value of the index k depend
on n). Next, setting 
n∈E πn = C, we show that the distribution vector
˜p = (˜n)n∈E with
˜pn
!= 1
C πn = 1
C
k(n)−1

ν=1
gmνmν+1
gmν+1mν
satisﬁes the global balance equation (5.2). For that purpose, observe that ac-
cording to detailed balance,
πn =
k(n)−1

ν=1
gmνmν+1
gmν+1mν
=
k(n)−1

ν=1
pmν+1
pmν
= pn
ps
,
which is true also for n = s. Consequently,

n∈E
˜pngnm = 1
C
 
gsm +

n∈E
n̸=
̸
s
pn
ps
· gnm
!
=
1
ps C

n∈E
pn gnm = 0,
implying that ˜p = p. This proves the assertion.
□
Let Ni,t
N
denote the random number of customers in a single queueing station i
at time t. If (Ni,t
N
)t∈R+
0 is a stationary reversible Markov process then we call i
a reversible queueing station. An important consequence from reversibility is
the so-called input-output property: For any reversible queueing station the
departure process has the same joint distribution as the arrival process. This is
due to the fact that, whereas the points in time when Ni,t
N
increases by 1 corre-
spond to arrivals, the points in time when Ni,t
N
decreases by 1 correspond to de-
partures and, by deﬁnition, to the epochs when the reverse process (N(r)
i,t
N
)t∈R+
0
increases by 1. Since joint distributions of the original and the reverse process
are the same, the arrival and the departure process exhibit the same joint sta-
tistics. As a consequence, we have the fact that a reversible queueing station,
when being fed by a Poisson (Markov) stream, causes a Poisson (Markov) out-
put stream. This property is called M ⇒M property.
Before considering other balance concepts, let us point to a general property
of stationary Markov processes. Assume, as we do mostly in this chapter, that
a stationary Markov process N can be interpreted as a random walk in a ﬁnite
graph G; then the rates of ﬂow in opposite directions across a cut in G are
identical. In other words, for some arbitrary subset A in the set E of nodes

Markovian Queueing Networks
71
(the state space) we have2

m∈A

n∈E\A
pm gmn =

m∈A

n∈E\A
pn gnm.
This is a direct consequence from global balance, since by summing on both
sides of (5.3) over all n, and subtracting

m∈A

n∈E
pm gmn =

m∈A

n∈A
pn gnm
, we obtain the above equation.
Opposed to the notions of global and detailed balance, the term partial bal-
ance plays an important role. In fact, the property of partial balance is the most
general property, since global and detailed balance as well as other terms (such
as station or local balance) can be regarded as special cases of partial balance.
An irreducible stationary Markov process with equilibrium distribution p =
(pn)n∈E and transition rates gmn is said to be in partial balance with respect
to a subset A of its state space E, if

m∈A
pm gmn =

m∈A
pn gnm,
n ∈A.
(5.7)
Notice, that the stationary distribution p satisﬁes the partial balance equations
(5.7) if and only if

m∈E\A
pm gmn =

m∈E\A
pn gnm,
n ∈A;
this follows from stationarity, i. e. the fact that the process is in global balance
and satisﬁes (5.3).
In many application oriented publications the property of partial balance is
described in somewhat vague terms, e. g. saying that partial balance for some
state m is present if the rate of ﬂow out of m due to changes of a particular
nature equals the rate of ﬂow into m due to changes of that very particular
nature.
It is here the point to pay attention to the fact that a network state, in general, is
determined by several actual values of system parameters, rather than just by
2The exterior of a queueing network is represented by a node ”0”, such that q0i is a routing probability into
node i from outside the network, and qj0 is the routing probability from node j to outside the network. The
node 0 is contained in the node set of G.

72
AN INTRODUCTION TO QUEUEING THEORY
the number of customers in each station (the latter deﬁnition leading to E =
NM
0 ). For example, in a multi-class network with R classes a state description
may contain information about the number of class r customers, their actual
waiting (and/or server occupancy) positions, and the actual phases of service at
every station i ∈{1, . . . , M}. Accordingly, the term partial balance includes a
variety of speciﬁc deﬁnitions, among which the notions of station balance and
local balance deserve particular notice.
Probably the most important property is that of local balance. Introduced by
Chandy et alii [26], this term depicts a situation, where the rate of ﬂow into
a network state m due to the arrival of a class r customer at a network queue
i is balanced against the rate of ﬂow out of the same network state due to
the departure of a class r customer from that network queue i. If the state
description contains information about the actual phase of service in case of
non-exponentially distributed service times, state changes are caused also by
phase transitions or by an entry into the ﬁrst phase of a service time distribu-
tion. Local balance, then, means that the probability ﬂux into network state m
due to the arrival of a class r customer at a network queue i by entering a ser-
vice phase ℓequals the probability ﬂux out of the same network state due to the
departure of a class r customer from that service phase ℓat queue i. Chandy
used the term ”stage of service” for the tripel (i, r, ℓ) of queue index i, class
index r, and phase index ℓ. Thus, a network is said to be in local balance, if
the rate of ﬂow into a stage (i, r, ℓ) of service is equal to the rate of ﬂow out of
the same stage (i, r, ℓ) of service for all admissible values of i, r, and ℓ.
Let us write gdepi(r)
m n
for the rate out of state m due to a departure of a class r
customer from queue i (this rate is zero if there is no such customer at i in state
m), and garri(r)
n m
for the rate into state m due to an arrival of a class r customer
at queue i. The local balance equations then read
pn garri(r)
nm
= pm gdepi(r)
mn
for all
i ∈{1, . . . , M}, 1 ≤r ≤R.
(5.8)
To illustrate the concept, consider a single class queueing network with M sta-
tions whose states are completely described by the vectors m = (m1, . . . , mM)
of station speciﬁc customer numbers. Let λi(mi) and µi(mi), respectively, de-
note the arrival rate into, and the service completion rate at station i, when there
are mi customers present (i = 1, . . . , M). According to the above deﬁnition
of local balance, the rate of ﬂow into some network state m due to an arrival
at queue i must be equal to the rate of ﬂow out of state m due to a departure
from queue i. Let ei denote a vector of length M that has a 1 at position i and
zeros at all other positions (the ith canonical row base vector). An arrival at
queue i can transfer a state n into m only if n equals m −ei (notice that a
transition from m −ei + ej to m would be due to a departure from queue j,

Markovian Queueing Networks
73
rather than due to an arrival at queue i). Similarly, a departure from queue i
can transfer the state m only to one of the two states m−ei or m−ei+ej. As
a consequence, the local balance equations for that simple single class network
with state space NM
0 read
pm−ei λi(mi −1) = pm µi(mi) qi0 +
M

j=1
pm µi(mi) qij
for all i ∈{1, . . . , M}, where qi0 and qij, respectively, are the routing proba-
bilities from station i to the exterior of the network and to station j. Observing
qi0 + M
i=1 qij = 1 and µi(0) = 0, we ﬁnally state that local balance means
pm−ei λi(mi −1) = pm µi(mi) if mi ≥1,
i = 1, . . . , M.
(5.9)
The next theorem should be considered as the central result with respect to the
notion of local balance. We provide an exemplary proof only for the most sim-
ple situation of a single class network with state space E = NM
0 , whose state
descriptions m = (m1, . . . , mM) reﬂect the station occupancies and whose
routing probabilities are state independent. The general case can be handled
similarly, although leading to more complex and intricate expressions. We
refer to the books of Kelly [48], Kant [45] and Nelson [61] for further details.
Theorem 5.5 Local balance implies global balance and product form.
Proof: (For the most simple case with only one class of customer and state
independent routing, where a state at some arbitrary point in time is determined
by the actual numbers of customers present in the network queues.)
1. Assume that a probability distribution (pn)n∈E satisﬁes the local balance
equations (5.8). We show that (pn)n∈E then satisﬁes global balance. Consider
all neighbouring states m ± ei and m ± ei ∓ej that are reachable from state
m, such that the probability ﬂux into state m is given by

n∈E
n̸=
̸
m
pngnm
=
M

i=1
pm−ei garri
m−ei m
+
M

i=1
pm+ei gdepi
m+ei m +
M

i=1
M

j=1
pm+ei−ej gdepi
m+ei−ej m.

74
AN INTRODUCTION TO QUEUEING THEORY
Applying (5.8), this yields

n∈E
n̸=
̸
m
pngnm
=
M

i=1
pm gdepi
m m−ei +
M

i=1
M

j=1
pm gdepi
m m−ei+ej +
+
M

i=1
pm+ei gdepi
m+ei m +
M

i=1
M

j=1
pm+ei−ej gdepi
m+ei−ej m.
Now express m as m = n −ei in the ﬁrst of the two sums in the second line,
and as m = n −ei + ej in the second one. Then these expressions can be
rewritten as
M

i=1
pn gdepi
n n−ei +
M

i=1
M

j=1
pn gdepi
n n−ei+ej
!=
M

i=1
pn−ei garri
n−ei n
=
M

i=1
pm garri
m m+ei,
such that the global ﬂux into state m reads

n∈E
n̸=
̸
m
pngnm = pm
⎧
⎨
⎧
⎩
⎨M

i=1
gdepi
m m−ei +
M

i=1
M

j=1
gdepi
m m−ei+ej +
M

i=1
garri
m m+ei
⎫
⎬
⎫
⎭
⎬
.
The right hand side, but, of this expression is nothing else than the total prob-
ability ﬂux out of state m, which proves that (pn)n∈E satisﬁes global balance
and, therefore, is the equilibrium state distribution of the network process N.
Consequently, all probability distributions over E that satisfy the local balance
equations must coincide with the unique equilibrium state distribution of the
network process N.
2. We show that the distribution vector (pn)n∈E that satisﬁes local balance has
product form. Take any network station i in isolation, i.e. decoupled from the
network, and provide the same input ﬂow to i that the station experiences when
communicationg with other stations in the network, such that the arrival and
departure rates are the same as before. Obviously, local balance implies that
i is in equilibrium. Let pi(mi) be the steady state probability for the isolated
station i to be in state mi, and let λi(mi) and µi(mi) denote the arrival rate
into i and the departure rate from i, respectively, when i is in state mi. Then,
equations (5.8) take the form
pi(mi −1)λi(mi −1) = pi(mi) µi(mi) ∀mi ≥1.

Markovian Queueing Networks
75
Deﬁne a probability vector ˜p = (˜n)n∈E over E = NM
0 by
˜pn =
1
CM
C
M

i=1
pi(ni) for n = (n1, . . . , nM),
where CM
C
= 
n∈E
#M
i=1 pi(ni). In the network, the arrival rates into station
i and the departure rates from station i, respectively, are
garri
m−ei m = λi(mi −1),
gdepi
m m−ei = µi(mi) qi0,
gdepi
m m−ei+ej = µi(mi) qij
for i, j ∈{1, . . . , M}. The construction of ˜p leads to
˜pm−ei garri
m−ei m = pi(mi −1)
pi(mi)
λi(mi −1),
˜pm
 
gdepi
m m−ei +
M

j=1
gdepi
m m−ei+ej
!
= ˜pm
 
µi(mi) qi0 +
M

j=1
µi(mi) qij
!
,
which implies that ˜pm−ei garri
m−ei m = ˜pm
 
gdepi
m m−ei +M
j=1 gdepi
m m−ei+ej
!
, i.e.
˜p satisﬁes the local balance equations. Consequently, ˜p coincides with the
uniquely determined equilibrium distribution p = (pn)n∈E of N.
□
In general, it is necessary to be careful when reading statements on local bal-
ance in the literature since, unfortunately, there are no uniform standards for
the deﬁnition of this notion. The reader who is interested in physical meanings
and practice oriented versions is referred to the book of Van Dijk [30].
Another remark is in place addressing the property of station balance. Here the
term ”station” does not stand for ”network station” in the sense of ”a queue in
the network” rather, it marks a position in the waiting or server room of a single
queue that is occupied by one customer! A queue, in turn, is viewed as a set of
stations. To illustrate the situation, consider an isolated multiple server queue
that is visited by customers from different classes. Obviously, for a ”ﬁrst-come
ﬁrst-served” (FCFS) or ”last-come ﬁrst-served” (LCFS) scheduling discipline,
the waiting positions at the top and the end of the queue, respectively, have
particular meanings. Additionally, in case that speciﬁc servers are associated
with speciﬁc classes, also the discrimination between servers (where service
completions are to be expected) may be of importance. In that case to each
server there is assigned a special subqueue containing customers at their wait-
ing positions.

76
AN INTRODUCTION TO QUEUEING THEORY
Bearing these peculiarities in mind, a ”station” is determined by a position
index j. A network queue i is viewed as a set of stations, and if there are in to-
tal ni customers resident at i, the occupied stations are indexed r1, . . . , rni,
with rj indicating the class of the customer at station (position) j.
Even
more complex descriptions are in use when routing chains are to be distin-
guished in the network, each containing users of different classes; we shall
mention that below. In the more simple case, when discriminating between
classes only, a possible single queue state deﬁnition is given by a 2ni - tupel
ni := (r1, . . . , rni, x1, . . . , xni), where i marks the queue in the network, ni
is the actual number of customers at this queue i, σi = (r1, . . . , rni) forms the
sequence of customer classes at positions 1, . . . , ni, and x1, . . . , xni is a vector
of remaining service requirements at these ni positions. So, we have
m = (n1, . . . , nM)
when speaking of state m ∈E. A queueing network is said to be in ”station”
balance if during state m for any position (”station”) j the actual fraction of
the service rate associated with that position is proportional to the probability
that a customer of the same category will arrive and be placed into this posi-
tion. ”Station” balance is tightly connected to the notion of symmetric service
disciplines that we shall deal with in section 3 below. There we shall give a
more precise deﬁnition. Clearly, ”station” balance implies local balance and,
consequently, global balance and product form. We have set the word ”sta-
tion” in quotation marks for two reasons: First, the term position in most cases
reﬂects more precisely what is meant when describing a speciﬁc network state
in a system with position depending dynamics and several chains and/or cus-
tomer classes. Second, we wish to reserve the term station in this introductory
book for a true queueing station in a network. There is a multitude of excellent
books on that topic, and for details we refer to the literature mentioned at the
end of this chapter.
Let us now turn back to the relationships between reversibility properties and
ﬂow balance. Asking for a property that guarantees partial balance we are
led to the notion of quasi-reversibility. Let again N = (Nt
N
: t ≥0) be a
Markov process with state space E that describes the dynamics of a queueing
system serving customers from R different classes. As we saw already, a state
n ∈E may be identiﬁed by a fairly complex description, rather than merely
by indicating the respective numbers of class speciﬁc customers in various
stations.
Deﬁnition 5.6 N is called quasi-reversible if for any time t0 the state Nt
N 0 is
independent of arrival times of class r customers after t0 and departure times
of class r customers prior to t0 (1 ≤r ≤R).

Markovian Queueing Networks
77
A quasi-reversible process, in general, is not reversible (see exercise 5.3), and
reversibility, in turn, does not imply quasi-reversibility. Accordingly, it should
be stressed that these two notions are completely unrelated. For queueing net-
works, but, the property of quasi-reversibility is of signiﬁcant pertinence. This
is due to the fact that — as we shall see below — quasi-reversibility gives rise
to product form expressions for the equilibrium state probabilities. Queues
in ”station” balance form an important subclass in the set of quasi-reversible
queues. We ﬁrst prove a result that is usually termed the input-output property
of quasi-reversible queues.
Lemma 5.7 (Input-Output Property) The arrival epochs as well as the de-
parture epochs of class r customers in a stationary quasi-reversible queue form
Poisson processes with class speciﬁc identical rates λr.
Proof: The set of all states n ∈E that provide the same state information as a
given state m except that there is one class r customer more in the system, is
marked S(m + r). Let G = (gmn)m,n∈E be the generator matrix of N; then
the rate of state changes due to class r arrivals when the state is mt at time t is
λr(mt) =

n∈S(mt+r)
gmtn.
1. According to quasi-reversibility the probability of a class r arrival during
the interval (t, t + dt] is independent of the state mt, and so is λr(mt) =
λr. Further, according to the Markov property, the path realization prior to t
has no inﬂuence on the probability for an arrival in (t, t + dt], which means
that the arrival process is memoryless with rate λr, independent of all earlier
states prior to t. Consequently, the class r arrival epochs form an independent
Poisson process with rate λr.
2. Interchanging the meaning of arrivals and departures of class r customers,
the reverse process N (r) again is to be interpreted as the state process of a
queue with R customer classes, and since N is quasi-reversible, so is N (r).
Therefore, the same reasoning applies, stating that the class r arrival process
of N (r) forms a Poisson process with rate δr = 
n∈S(mt+r) g(r)
mtn. This rate
is the class r departure rate of N, and so, due to stationarity, equals λr, which
proves the assertion.
□
We are now in the position to formulate the relationship between quasi-reversibil-
ity and partial balance.
Lemma 5.8 Any quasi-reversible Markov process N = (Nt
N : t ≥0) over
some state space E that describes the dynamics of a queueing system with R

78
AN INTRODUCTION TO QUEUEING THEORY
customer classes satisﬁes partial balance with respect to the set S(m + r) for
any m ∈E.
Proof: Remember, that S(m + r) describes the set of all states n ∈E that
provide the same state information as a given state m except that there is one
class r customer more in the system. From equation (5.6) we obtain
pm

n∈S(m+r)
g(r)
mn =

n∈S(m+r)
pn · gnm,
since the reversal of the reverse process is the original one. According to the
proof of lemma 5.7,
λr =

n∈S(m+r)
gmn =

n∈S(m+r)
g(r)
mn,
and so
pm

n∈S(m+r)
gmn =

n∈S(m+r)
pn · gnm.
□
Let us now consider a vector-valued continuous time Markov process N =
(Nt
N : t ≥0) with state space E that describes a multi-class queueing network
with M stations and R classes of customers. Upon completing service at one
station i, a class r customer not only may join another station j of the network
or depart from the network, but also may change its class before joining another
queue. In general, the probability to undergo such type of change may depend
on the history of the customer’s behaviour and on the state of the process.
The analysis of queueing networks of that generality has turned out to be very
complex, if not impossible. When speaking of Markovian queueing networks
in this chapter, we mean a subclass of networks that is characterized by the
property that the routing probabilities are memoryless and independent of the
network states, this way deﬁning the transition matrix of a Markov chain.
Let qir;jr′ denote the probability that a class r customer, after leaving queue
i, joins queue j as a class r′ customer, and set qir;00 for the probability that a
class r customer leaves the network after service completion at station i. Then
M
j=1
R
r′=1 qir;jr′ + qir;00 = 1, and the discrete time - discrete state Markov
chain deﬁned by
Q = (qir;jr′)i,j∈{0,...,M},r,r′∈{0,...,R}3
3Where, for j = 0 only r′ = 0 is possible, and vice versa.

Markovian Queueing Networks
79
is called the routing chain. A queueing network of that type is said to perform
Markov routing.
Remark 5.9 If an asymptotic distribution p for a Markovian network process
N = (Nt
N : t ≥0) does exist, then there is an asymptotic marginal distribution
pi = pi(k) for any queue i, too.
This can be seen from
pi(k)
=
lim
t→∞P(pri(Nt
N ) = k) = lim
t→∞

n∈pr−1
i
(k)
P(Nt
N = n)
=

n∈pr−1
i
(k)
lim
t→∞P(Nt
N = n) =

n∈pr−1
i
(k)
pn,
with pri denoting the projection on the ith station speciﬁc state component,
and pr−1
i
(k) = {n ∈NM
0 : ni = k}.
We close this section by formulating some sort of a quintessence from the
above treatment of quasi-reversibility.
Theorem 5.10 Let N = (Nt
N
: t ≥0) be a stationary vector-valued con-
tinuous time Markov process with state space E that describes a multi-class
queueing network with M stations and R classes of customers. If each queue-
ing station in isolation behaves as a quasi-reversible queue, and if the network
performs Markov routing, then N is again quasi-reversible, and its equilibrium
distribution p = (pn)n∈E assumes product form, i.e.
pn = 1
C
M

i=1
fif (ni),
where fif (ni) is a state depending function for an isolated station i in steady
state ni, and C is some normalization factor.
We give a sketch of the proof for the simple case of a network whose states
are deﬁned by class speciﬁc customer occupancies only, and in which no class
changes occur. For the more general cases we refer to the excellent treatments
given by Kelly [61], and Nelson [48].
Proof: Stationarity of the whole network implies that of any single station.
Consider a station i in isolation with the same class speciﬁc input streams,
and let pi(ni) = pi(ki1, . . . , kiR) for kr ∈N0 and r ∈{1, . . . , R} be its

80
AN INTRODUCTION TO QUEUEING THEORY
steady state distribution. Quasi-reversibility means that the input and the out-
put stream for each customer class r at i are Poisson (with same rate λir), so we
have pi(ki1, . . . , kir −1, . . . , kiR) λir = pi(ki1, . . . , kiR) µir(kir). Construct
a probability distribution by
pn =
1
C(M, R)
M

i=1
pi(ki1, . . . , kiR).
Then this distribution satisﬁes local balance (cf. proof of theorem 5.5) and,
therefore, also global balance.
□
Notice, that the essential property here for a product form to hold is the prop-
erty of each station to produce, when being fed by a Poisson input stream, an
output stream that again is Poisson. This is nothing else than the M ⇒M
property.
2.
Jackson and Gordon-Newell Networks
Let us consider now the simplest type of queueing network. This is an open or
closed single class network of, say, M queues, whose state space is determined
by the station speciﬁc numbers of customers only. Let N = (Nt
N : t ≥0) de-
note the stochastic process that describes the dynamics of such a network with
respect to the varying numbers of customers in the stations.4 Its state space E
is a subset of NM
0 .
As before, we denote with Q = (qij)i,j∈{1,...,M} the routing matrix, and with
G = (gmn)m,n∈NM
0 the generator of N. An open network of that kind is called
a Jackson network if the following conditions are satisﬁed.
1 Any user entering N at some node i may reach any other node in ﬁnitely
many steps with positive probability. Similarly, starting from some node i
a user can leave the network in ﬁnitely many steps with positive probability
(i ∈{1, . . . , M}).
2 The network performs Markov routing, i.e. Q represents the transition ma-
trix of a Markov chain.
3 Each queueing station i is of type ∗/M/si with si ∈N ∪{∞}, i.e. the
service time distribution at station i is exponential with parameter µi for
each of the si servers.
4The letter N may also stand for the queueing network itself, as long as no ambiguities are to be expected.

Markovian Queueing Networks
81
4 The total arrival stream from outside the network forms a Poisson stream
of intensity γ. The separate arrival streams to stations i = 1, . . . , M are
determined by the routing probabilities q0i with M
i=1 q0i = 1. They are
Poisson streams with intensities γ q0i =: γiγ .
N is irreducible due to property 1, and is Markov due to the memoryless prop-
erty of the exponential service and inter-arrival time distributions. As such, the
network assumes equilibrium if and only if there exists a positive ﬁnite solution
p G =

m∈E
pm gmn = o,
where G = (gmn)m,n∈NM
0 is the generator of N.
Let λi and δi, respectively, denote the total mean arrival and departure rates
at stations i = 1, . . . , M, each being independent of the actual occupancy at
the stations. Then, λi = γiγ + M
j=1 δjδ qji
q . In equilibrium, δi = λi for each
i ∈{1, . . . , M}, and so
λi = γiγ +
M

j=1
λj qji
q ,
i = 1, . . . , M.
(5.10)
(5.10) is called the system of trafﬁc equations for a Jackson network in equi-
librium. The next lemma shows that this system always possesses a unique
solution.
Lemma 5.11 For a Jackson network with routing matrix Q, the matrix I −Q
is invertible.
Proof: Consider a Markov chain X with transition matrix
P =

1
0
q0
Q

,
where q0 = (q10, . . . , qM0)T is a column vector, and 0 = (0, . . . , 0) is the
zero row vector. Irreducibility of the Jackson network implies that the set of
states {1, . . . , M} forms a transient communication class in the state space of
X, whereas the state zero is absorbing. Hence, according to corollary 2.15, the
submatrix ˜R of the potential matrix R of X (as deﬁned in (2.7)) that contains
entries R(ij) with i, j ∈{1, . . . , M} only, is ﬁnite. Due to the structure of P,
˜R = ∞
n=1 Qn, and since ˜R is ﬁnite, the Neumann series
˜R + I =
∞

n=0
Qn = (I −Q)−1
p = (pn)n∈E to the system of equations

82
AN INTRODUCTION TO QUEUEING THEORY
is ﬁnite, too. This proves the assertion.
□
We denote, as usual, by ρi = λi/µi the load factor of station i, 1 ≤i ≤M. In
general, the service completion rate at each station i is state dependent, given
by µi(ni) = µi min(si, ni) when there are ni customers present at i. Obvi-
ously, a necessary condition for the network process N to attain equilibrium is
that all individual station speciﬁc processes attain equilibrium, i.e. stationarity
of N implies
ρi < si
for all
i ∈{1, . . . , M}.
(5.11)
The following statement has ﬁrst been proven by Jackson as early as in 1963
[42]. It shows that (5.11) not only is a necessary, but also a sufﬁcient condition
for stationarity of N, and that any Jackson network is a product form (PF)
network.
Theorem 5.12 (Jackson) Let N denote a Markov process describing a Jack-
son network with M stations, and assume ρi < si for all 1 ≤i ≤M. Then a
stationary distribution of N exists and is given by
pn =
M

i=1
pi(ni),
(5.12)
for n = (n1, . . . , nM), where pi = (pi(ni))ni∈N0 is the stationary distribution
of an isolated M/M/si queueing system with arrival rate λi and service rate
µi at each server.
Proof: (pn)n∈NM
0
is a probability distribution since pi(ni) ≥0 for all i ∈
{1, . . . , M}, and

n∈NM
0
pn =
∞

n1=0
. . .
∞

nM=0
M

i=1
pi(ni) =
M

i=1
∞

ni=0
pi(ni) = 1.
From equations (5.11), (5.12), and (5.9) we know that
pm−ei λi = pm µi(si)
for all
i ∈{1, . . . , M},
which means that the distribution (5.9) satisﬁes local balance. Thus, by theo-
rem 5.5, (pn)n∈NM
0 is the equilibrium distribution of N.
□
Jackson has proved this theorem by directly establishing the global balance
relations. We repeat his rationale here for pedagogical reasons in order to

Markovian Queueing Networks
83
illustrate the interplay of input and output ﬂows in a Jackson network.
For a Jackson network, the transition rates gnm into a network state m =
(m1, . . . , mM) read
gnm =
⎧
⎨
⎧
⎩
⎨γiγ
if
n = m −ei
µj(nj + 1) · qji
q
if
n = m −ei + ej
µi(ni + 1) · qi0
if
n = m + ei
,
whereas the rates out of a network state m = (n1. . . . , nM) read
gmn =
⎧
⎨
⎧
⎩
⎨γiγ
if
n = m + ei
µi(ni) · qij
if
n = m −ei + ej
µi(ni) · qi0
if
n = m −ei
.
Due to ρi < si for all i, each network station in isolation with same arrival rates
assumes equilibrium, satisfying the local balance equations pi(mi+1) µi(mi+
1) = pi(mi) λi for all mi ≥0, 1 ≤i ≤M. Consequently, an expression of
the form (5.12) leads to5
pm−ei
=

k̸=
̸
i
pk(mk) pi(mi −1) = pm
µi(mi)
λi
,
pm−ei+ej
=

k̸=
̸
i,j
pk(mk) pi(mi −1) pj(mj + 1) = pm
µi(mi)
λi
λj
µj(mj + 1),
pm+ei
=

k̸=
̸
i
pk(nk) pi(ni + 1) = pm
λi
µi(mi + 1),
and the probability ﬂow into network state m is

n∈NM
0
pn gnm = pm
⎧
⎨
⎧
⎩
⎨M

i=1
µi(mi)
λi
+
M

i=1
M

j=1
µi(mi)
λi
λj qji
q
+
M

i=1
λi qi0
⎫
⎬
⎫
⎭
⎬
,
which, according to M
j=1 λj qji
q
= λi −γiγ (which follows from the trafﬁc
equations) and M
j=1 qij = 1 −qi0, reduces further to

n∈NM
0
pn gnm = pm
M

i=1
 
µi(mi) + γiγ
!
.
(5.13)
5Note that pi(ν) = 0 for ν < 0, and µi(ν) = 0 for ν ≤0, 1 ≤i ≤M.

84
AN INTRODUCTION TO QUEUEING THEORY
On the other side, by the same reasoning, the probability ﬂow out of network
state m can be rewritten as

n∈NM
0
pm gmn = pm
⎧
⎨
⎧
⎩
⎨M

i=1
γiγ +
M

i=1
M

j=1
µi(mi) qij +
M

i=1
µi(mi) qi0
⎫
⎬
⎫
⎭
⎬
,
and this, as is easily seen, is the same as (5.13), proving theorem 5.12.
A closed network possessing all the properties 1 - 3 of a Jackson network
(with the exception of property 4) is called a Gordon-Newell network, or GN
network for short. As shown by W. J. Gordon and G. F. Newell in 1967 [40],
such a network assumes equilibrium with stationary distribution
pn =
1
˜CM
C
(K)
M

i=1
pi(ni)
(5.14)
for n = (n1, . . . , nM), where again pi = (pi(ni))ni∈N0 is the stationary distri-
bution of an isolated M/M/si queueing system with arrival rate λi and service
rate µi at each server, and where ˜CM
C
(K) = 
n∈E
#M
i=1 pi(ni) represents a
normalization factor that guarantees 
n∈E pn = 1 (K the constant number of
customers in the network).
This statement is usually called the Theorem of Gordon-Newell. Its proof is
given by the same reasoning as for the theorem of Jackson by setting γiγ = 0
and qi0 = 0 for 1 ≤i ≤M. In both cases the participating network stations be-
have as if being completely independent, a result that is somewhat surprising,
since — at least for a Gordon-Newell network — the dependency of station
speciﬁc events is obvious: Given, that there are K customers in the network,
we always have M
i=1 ni = K. The reason behind is the M ⇒M property
that implies local balance.
The state space E = E(M, K) of a Gordon-Newell network with M stations
and K customers is given as the set of vectors
E(M, K) =

n = (n1, . . . , nM) : ni ≥0 ∀i,
M

i=1
ni = K
$
and has size
|E(M, K)| =
M + K −1
M −1

.
The latter is easily seen by induction: Obviously, |E(M, 0)| = |E(1, K)| = 1.
Further, we have |E(2, K)| = K + 1, since, according to n1 + n2 = K,
any state n = (n1, n2) is already determined by only one of its entries ni ∈

Markovian Queueing Networks
85
{0, 1, . . . , K}. Assume that |E(M −1, K)| =
M+K−2
M−2

. Adding another
node to the network that is appropriately connected with the former nodes, the
new node may contain ν ∈{0, 1, . . . , K} users when there are K −ν users at
the remaining M −1 nodes. Hence,
|E(M, K)|
=
K

ν=0
M + (K −ν) −2
M −2

=
K

ν=0
M −2 + ν
M −2

=
M −1
M −1

+
K

ν=1
M −2 + ν
M −2

,
and the well known relation
n
k

+

n
k + 1

=
n + 1
k + 1

,
with k = M −2 and n = M −2 + ν, yields
|E(M, K)|
=
M −1
M −1

+
K

ν=1
M −1 + ν
M −1

−
M −2 + ν
M −1

=
M −1 + K
M −1

.
2.1
The Performance of a Jackson Network
The performance measures of a Jackson network are easily obtained from those
of isolated M/M/si stations. As has previously been shown,
pi(0)
=
si−1

k=0
ρk
i
k! + ρsi
i
si!

1 −ρi
si
−1−1
,
pi(ni)
=
⎧
⎨
⎧
⎩
⎨pi(0) ρni
i
ni!
for
0 ≤ni ≤si
pi(0)
 
ρi
si
!ni ssi
i
si!
for
ni ≥si
,
where ρi = λi/µi.6 So, for a Jackson network, any state probability is im-
mediately obtained from (5.12), whereas for a Gordon-Newell network it is
necessary to additionally compute the normalization constant. Notice, that the
steady state probabilities depend only on the mean values ¯xi = 1/µi of the
6In some publications the quantity ρi is deﬁned as ρi = λi/(si µi) and termed utilization factor; this is
the mean fraction of active servers (see [50]).

86
AN INTRODUCTION TO QUEUEING THEORY
service time distributions, and not on higher moments. This property is com-
mon to all product form networks, and is called the product form network
insensitivity property.
Let, for an M/M/si station, ¯Ni
N denote the mean number of customers in the
station, ¯Ti
T the mean sojourn time, ¯W Q
i
W
the mean waiting time in the queue,
¯NQ
i
N
the mean queue length, and ¯Si
S the mean throughput through the station.
Then,
¯Ni
N
=
ρi + pi(0)
ρsi+1
i
(si −1)!(si −ρi)2 ,
¯Ti
T
=
1
µi

1 + pi(0)
ρsi
i
(si −1)!(si −ρi)2

,
¯W Q
i
W
=
1
µi
pi(0)
ρsi
i
(si −1)!(si −ρi)2 ,
¯NQ
i
N
=
¯Ni
N −ρi,
¯Si
S
=
λi,
and the total average number ¯N of customers is
¯N =
M

i=1
¯Ni
N .
Applying Little’s result (see theorem 1.9), the total mean sojourn time or net-
work delay a customer experiences is obtained as
¯T = 1
γ
¯N,
where γ = M
i=1 γiγ = ¯S is the total mean throughput through the network.
We denote by τiτ the mean time between a user’s arrival at node i and his ﬁ-
nal departure from the network. For this quantity we immediately realize the
relation
τiτ = ¯Ti
T +
M

j=1
qijτjτ ,
1 ≤i ≤M.
Let vi denote the mean number of visits a user makes at sation i. The total
number of customers that enter the network per unit time is γ, and each of
these customers visits node i in the average for vi times, so γ vi gives the
average ratio of arrivals per unit time at station i, implying that γ vi = λi. The
trafﬁc equations (5.10), therefore, yield
vi = γiγ
γ +
M

j=1
vj qij,
1 ≤i ≤M.
(5.15)

Markovian Queueing Networks
87
For any Jackson network the system (5.15) always possesses a unique non-
negative solution due to lemma 5.11.
2.2
Computational Methods for Gordon-Newell
Networks
The calculation of performance measures of a Gordon-Newell network is by
far not as easy as in the case of Jackson networks. The main problem consists
in computing the normalization constant
˜CM
C
(K) =

n∈E(M,K)
M

i=1
pi(ni).
(5.16)
What is the reason? It is simply the fact that the huge number of terms oc-
curring in (5.16) makes it very difﬁcult, in general, to numerically evaluate the
product form solution. Special algorithmic methods are in place here, and we
shall demonstrate one below.
The Convolution Algorithm
Consider the trafﬁc equations of a GN network,
λi =
M

j=1
λj qij,
1 ≤i ≤M.
(5.17)
Obviously, the quantities λi are only determined up to some non-zero constant,
and in order not to identify them with the ”true” arrival rates, it is convenient
to replace the term λi by yi and just look at these yi as solutions of the above
system (5.17). For technical reasons we set
xi(ni) := pi(ni)
pi(0) ,
and
CM
C
(K) :=
˜CM
C
(K)
#M
i=1 pi(0)
.
The product form equation (5.14) then takes the form
pn =
1
CM
C
(K)
M

i=1
xi(ni),
(5.18)
and according to the local balance equations pi(ni + 1) µi(ni + 1) = pi(ni) λi
as well as the convention λi = yi we have
xi(0) = 1,
xi(k) = xi(k −1)
yi
µi(k)
for 1 ≤k ≤K.
(5.19)

88
AN INTRODUCTION TO QUEUEING THEORY
Thus, given any solution y1, . . . , yM of the system of trafﬁc equations (5.17),
we can compute all the xi(ni) simply by iteration.
Let us pause here for a moment in order to introduce the notion of discrete
convolution of vectors of equal length (or even sequences with inﬁnitely many
components): Given a = (a1, . . . , aN) and b = (b1, . . . , bN), the convolution
of a and b is deﬁned as the vector c = (c1, . . . , cN) of same length that has
the components
ck =
k

ℓ=0
ak−ℓbℓ=
k

ℓ=0
aℓbk−ℓ,
0 ≤k ≤N
(N ≤∞). The common symbol for the convolution operation is the ”∗”, i.e.
we write c = a ∗b. It is obvious that (a ∗b) ∗c = a ∗(b ∗c) for arbitrary
vectors a, b, c ∈RN. For a convolution of some vector a ∈RN with itself we
write
a∗n = a∗n−1 ∗a
for n ≥1,
where a∗0 is deﬁned as a∗0 = (1, 0, . . . , 0), hence a∗0 ∗b = b for all b ∈RN.
We return now to the problem of computing the steady state probabilities
(5.18). Although the xi(ni) can easily be computed by iteration, the compu-
tation of the normalization constant CM
C
(K) = 
n∈EM(K)
#M
i=1 xi(ni) still
turns out to be rather difﬁcult if M and K attain large values. In this situation
J. Buzen [24] observed that (5.16) is nothing else than the Kth component of
the discrete convolution of the vectors
xi = (xi(0), xi(1), . . . , xi(K)).
Precisely, we have
CM
C
(K) =

n∈E(M,K)
M

i=1
xi(ni) != (
!
x1 ∗. . . ∗xM)(K).
(5.20)
Formally, expression (5.20) is characterized by the two parameters M and K,
and so it is suggesting itself that we deﬁne, for 1 ≤m ≤M, 1 ≤k ≤K, the
components Cm
C (k) of the convolution vector Cm = x1 ∗. . . ∗xm by
Cm
C (k) =

n∈E(m,k)
m

i=1
xi(ni) = (x1 ∗. . . ∗xm)(k),
where E(m, k) =
%
n = (n1, . . . , nm) : ni ≥0, m
i=1 ni = k
&
. Similarly,
the constant CM
C
(K) can be written as the Kth component of the convolution
of CM−1 and xM:
CM
C
(K) = (CM−1 ∗xM)(K).

Markovian Queueing Networks
89
In general terms, we arrive at
Cm
C (k) = (Cm−1 ∗xm)(k),
1 ≤m ≤M, 1 ≤k ≤K.
(5.21)
This, in fact, is the basis of Buzen’s convolution algorithm. It can roughly be
described as follows.
1 Set C0
C (0) = 1, and C0
C (ℓ) = 0 for 1 ≤ℓ≤K.
2 For all m, 1 ≤m ≤M, set xm(0) = 1.
3 Compute successively, for any m ∈{1, . . . , M} and k = 0, . . . , K, the
values xm(k) = xm−1 ym/µm(k) and Cm
C (k) = 
n∈Em(k)
#m
i=1 xi(ni).
Performance Measures
The computation of all the (normalization) constants Cm
C (k) opens the way for
an easy and direct evaluation of station speciﬁc performance measures. Note
that, by adequate renumbering, we always can achieve that an arbitrary station
has index M. Let pM(n; K) denote the marginal steady state probability to
ﬁnd n users at station M. Then, according to the product form (5.18),
pM(n; K)
=

n∈E(M−1,K−n)
p(n1,n2,...,nM−1,n)
=

n∈E(M−1,K−n)
#M−1
i=1 xi(ni) xM(n)
CM
C
(K)
=
CM
C
−1(K −n)
CM
C
(K)
· xM(n).
(5.22)
The mean number ¯NM
N
(K) of customers in station M is now immediately
obtained as7
¯NM
N
(K) =
K

n=0
pM(n; K) · n.
Hence,
¯NM
N
(K) =
1
CM
C
(K)
K

n=0
CM
C
−1(K −n) xM(n) · n.
(5.23)
7We intentionally indicate here and in the following in each term the total number of customers present in
the network.

90
AN INTRODUCTION TO QUEUEING THEORY
It may be worthwile to note that this expression again takes the form of a
convolution: Set zM = (0, xM(1), 2xM(2), . . . , KxM(K)); then
¯NM
N
(K) = (CM−1 ∗zM)(K)
CM
C
(K)
.
In equilibrium, the mean throughput rate ¯SM(K) through station M in a GN
network with K customers equals the mean arrival rate λM(K) (as well as the
mean departure rate δM(K)). It is given as
¯SM(K) =
K

n=1
pM(n; K) µm(n).
From this expression, by inserting (5.21) and exploiting (5.19), we obtain
¯SM(K) = CM
C
(K −1)
CM
C
(K)
· yM.
(5.24)
We proceed to calculate the mean time ¯TM
T
(K) a user spends in a station i
(mean system time, or mean sojourn time). According to Little’s result we
have ¯TM
T
(K) =
1
λM ¯NM
N
(K) =
¯
NM
N
(K)
¯SM(K) , and so the above results yield
¯TM
T
(K) =
K
n=1 CM
C
−1(K −n) xM(n) n
CM
C
(K −1) yM
.
(5.25)
The mean number ¯NQ
i
N (K, si) of customers waiting in the queue at some sta-
tion i that has si exponential servers is given as K
ni=si pi(ni; K) (ni −si).
Thus, by (5.21),
¯NQ
M
N
(K, si) =
K
n=sM CM
C
−1(K −n) xM(n) (n −sM)
CM
C
(K)
.
(5.26)
The Principle of Mean Value Analysis
In case that each network station either is a single server station or an inﬁnite
server station, an even easier way can be pursued, avoiding the explicit com-
putation of the values (5.21). In fact, it is possible to obtain all mean values
by some simple iteration process. For stations with more than 1 and less than
K servers, but, one still has to rely on (5.21) and related expressions. The ap-
proach in question, in its general form, is called mean value analysis (MVA)
and has been suggested by Reiser and Lavenberg in 1980 [73].

Markovian Queueing Networks
91
For a GN network the mean visiting numbers satisfy the equations
vi =
M

j=1
vj qji
q ,
1 ≤i ≤M.
(5.27)
These equations, clearly, do not possess a unique solution, as has been the
case for an open network. So, neither we can determine the exact values for
the visiting numbers vi and the mean arrival rates λi, nor we can compute
other mean values by imitating the previous approach. In order to achieve
yet similar results, we proceed by turning a closed network into an open one
without changing any of the performance criteria. The idea is the following:
Add another ﬁctitious node 0 to the network graph between two nodes i0 and
j0 that are connected by an edge (possibly i0 = j0), where i0, j0 ∈{1, . . . , M}
(see ﬁgure 5.2).







h







h







h
-
-
-
-
?
t
t
t
6
?
6
6
?
-
-

6

6
-
-




virtual node
0
i0
j
j0
qi0j
qjj
q
0
qi00
q0j0
Figure 5.2.
Modiﬁed Network
Any customer, who is routed to node j0 after service completion at node i0
is now assumed to depart from the network, and to be immediately replaced
by another new customer who enters the network at node j0. This way, the
number K of customers in the network is preserved all the time, and all net-
work parameters remain exactly the same. The construction allows to speak of
performance items like network delay T (i.e. the total time a customer spends
in the network), or throughput S through the network. In particular, we shall
be able to calculate the mean values ¯T, ¯S and ¯N = M
i=1 ¯Ni
N ,8 and to de-
termine the mean number vi of visits that a customer makes at station i for
i ∈{1, . . . , M}.
8These values depend on the number K of customers in the network, and we shall indicate this dependency
in the following.

92
AN INTRODUCTION TO QUEUEING THEORY
First observe that any ”newly arriving” customer from ”outside the network”
(i.e. from node 0) visits node i0 exactly k times with probability given by
qi0j0(1 −qi0j0)k−1, implying that the mean number vi0 of visits at node i0 for
any customer in this open network attains the value
vi0 =
∞

k=1
k qi0j0(1 −qi0j0)k−1 =
1
qi0j0
.
This determines all other visiting numbers vi according to equation (5.27):
vi =
M

j=1
j̸=
̸
i0
vj qji
q
+ qi0i
qi0j0
.
Equations (5.17) and (5.27) show that the vectors λ = (λ1, . . . , λM) and v =
(v1, . . . , vM) are proportional, λ = γ v. The λi (and so the constant γ) depend
on the number K, whereas the vi are functions of the routing probabilities only.
Since in equilibrium the average departure rate from node i0 equals its average
arrival rate λi0, the expression λi0 qi0j0 = λi0/vi0 represents the mean transfer
rate from node i0 to node j0 in the original GN network. Consequently, in our
artiﬁcial open network, the constant γ is nothing else than the total average
input rate from outside (or throughput rate ¯S through) the network:
λi0 qi0j0 = λi0/vi0 = γ = ¯S(K).
The visit numbers λi/γ = vi are sometimes referred to as relative throughput
rates.
Let again, for any station i ∈{1, . . . , M} in a network with K customers,
denote by ¯Ni
N (K, si) the mean number of customers in i, and by ¯Ti
T (K, si) the
mean sojourn time in station i if there are si servers at that station. We obtain
the following relations:
¯Ni
N (K, si)
=
λi ¯Ti
T (si)
(Little’s rule),
K
=
M

i=1
¯Ni
N (K, si) = ¯S(K)
M

i=1
vi ¯Ti
T (K, si),
¯T(K)
=
M

i=1
vi ¯Ti
T (K, si) =
M

i=1
λi
¯S(K)
¯Ti
T (K, si).
The last equation conﬁrms K = ¯S(K) ¯T(K) (Little’s rule). Notice, that the
mean system times ¯Ti
T (K, si) cannot be calculated as sojourn times of isolated
independent M/M/si stations as in case of a Jackson network, since the num-
bers Ni
N (K, si) are now dependent upon each other due to the second of the

Markovian Queueing Networks
93
above equations. Accordingly, we have to ﬁnd another way to compute the
¯Ti
T (K, si) in order to solve the equations for all other unknowns.
Let us call a customer, who has completed service at some station j and is
about to enter station i (but not yet there), to be a customer in transit to i. As-
sume that such a customer ”sees” Ai(K, si) customers in total, and AQ
i (K, si)
customers waiting in the queue at station i immediately before his entrance
there. Clearly, Ai(K, si) and AQ
i (K, si) are random numbers. Let ¯Ai(K, si)
and ¯AQ
i (K, si) denote their respective expectations. With ¯Ai(K, si) the mean
system times ¯Ti
T (K, si) for the cases si = 1 and si ≥K are given as
¯Ti
T (K, si) =

1
µi + 1
µi ¯Ai(K, si)
if
si = 1
1
µ
µi
if
si ≥K
(5.28)
(remember, that service times are exponentially distributed, and that 1/µi is
the mean of the service time at station i). We shall show later that the corre-
sponding value for the case 1 < si < K reads
¯Ti
T (K, si) = 1
µi
 
1 + 1
si
'
¯AQ
i (K, si) + bi(K −1)
(!
,
1 < si < K,
where bi(K −1) is the probability for the event that, in a closed network of
same type with K −1 customers, all of the si servers at station i are occupied.
The task here is to compute the mean values ¯AQ
i (K, si) as well as the proba-
bilities bi(K −1) for i ∈{1, . . . , M}.
In order to determine the ¯Ai(K, si) in (5.28) we mention an important general
feature of product form networks that is called the arrival property in case of
open networks, and the random observer property in case of closed ones.9
Here we conﬁne ourself to the case of a single class GN network, but it should
be clear from the proof below that this property also holds for multi-class (open
or closed) PF networks.
Theorem 5.13 (Random Observer Property) Let ai(n−ei) denote the prob-
ability for the event that a customer in transit to i ”sees” the state disposition
n −ei = (n1, . . . , ni −1, . . . , nM) immediately before his arrival at station i.
If N describes a closed GN network with, say, K customers, then this proba-
bility ai(n −ei) is the same as the steady state probability for state n −ei for
a network of same type with one customer less.
9For a single queueing station this is comparable with the PASTA property (Poisson arrivals see time aver-
ages).

94
AN INTRODUCTION TO QUEUEING THEORY
Proof: We denote by ηi(n) the mean number of customers in transit to i per
unit time who ”see” the state disposition n. Obviously, we have
ηi(n −ei) =
M

j=1
pn−ei+ej gn−ei+ej n.
(5.29)
The probability ai(n −ei) can be expressed as the relative portion of the rate
ηi(n −ei) compared with the sum over all rates ηi(m):
ai(n −ei) =
ηi(n −ei)

m∈E(M,K) ηi(m).
Observing gn−ei+ej n = µj(nj + 1)qji
q , as well as the local balance equations
pj(nj + 1) µj(nj + 1) = pj(nj) λj,
and exploiting the product form (5.14), we obtain from (5.29), that
ηi(n −ei) = pn−ei
M

j=1
λj qji
q
= pn−ei λi,
and likewise ηi(m) = pm λi for any m ∈E(M, K), i ∈{1, . . . , M}. This
proves an−ei = pn−ei.
□
The random observer property enables us to determine the values ¯Ai(K, si)
and ¯AQ
i (K, si) as
¯Ai(K, si) = ¯Ni
N (K −1, si),
¯AQ
i (K, si) = ¯NQ
i
N (K −1, si)
for i ∈{1, . . . , M}, where we indicate by si the number of servers, and by K
or K −1 the number of customers in the network. According to these results
the station speciﬁc mean system times in a GN network with K customers are
given by10
¯Ti
T (K, si) =
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
1
µi + 1
µi ¯Ni
N (K −1, si)
if
si = 1
1
µ
µi
if
si ≥K
1
µi
 
1 + 1
si
'
¯NQ
i
N (K −1, si) + bi(K −1)
(!
if
1 < si < K
.
(5.30)
10The third line expression for 1 < si < K will be derived later.

Markovian Queueing Networks
95
For a network that is built up by only single server or inﬁnite server sta-
tions we arrive at a system of recursion equations for the unknowns ¯Ti
T (K, si),
¯Ni
N (K, si), and ¯S(K), viz.
¯Ti
T (K, si)
=

1
µi + 1
µi ¯Ni
N (K −1, si)
if
si = 1
1
µ
µi
if
si ≥K
,
¯S(K)
=
K
M
i=1 vi ¯Ti
T (K, si)
,
¯Ni
N (K, si)
=
vi ¯S(K) ¯Ti
T (K, si).
(5.31)
In essence these expressions explain what is usually meant with ”mean value
analysis” for Gordon-Newell networks: It is a simple iteration process that
starts with ¯Ni
N (0, si) = 0 and ¯Ti
T (1, si) = 1/µi and requires to compute succes-
sively, for 1 ≤k ≤K, the values ¯Ti
T (k, si), ¯S(K), and ¯Ni
N (K, si) according to
(5.31).
The computational overhead is fairly small, and can even be further reduced if
approximate results are tolerated. The quantities ¯Ai(K, si) = ¯Ni
N (K −1, si)
may roughly be estimated as
¯Ai(K, si) ≈K −1
K
¯Ni
N (K, si),
a relation that is exact for K = 1, and tends, for increasing K, asymptotically
to an exact equation. It even provides, in many practical cases, good results for
intermediate values of K. Inserted in (5.31) we obtain
¯Ti
T (K, si) =

1
µi
 
1 + vi(K−1) ¯Ti
T (K,si)
M
ℓ=1 vℓ¯Tℓ
T (K,sℓ)
if
si = 1
1
µi
if
si ≥K
.
So we see that, if one accepts approximate results, the overhead for the compu-
tation of the Ti
T (K, si) can drastically be reduced, and limited to the solution of
some ﬁxed-point equations. It may also be the case that the total mean through-
put rate ¯S = γ is given in a concrete situation, meaning that we can measure
somehow the average transfer rate between two connected network nodes i0
and j0. Then the relative throughput rates λi/vi = γ and, consequently, the
exact arrival rates λi are obtained immediately, providing the ¯Ti
T (K, si) from
direct recursion:
¯Ti
T (K, si) =

1
µi + λi
µi ¯Ti
T (K −1, si)
if
si = 1
1
µ
µi
if
si ≥K
.
We refer the reader to the literature for more detailed descriptions of the prin-
ciples of mean value analysis. A practice oriented treatment, for instance, is

96
AN INTRODUCTION TO QUEUEING THEORY
given in the book of Bolch et al. [15], where several examples and algorithms
are presented.
Example (Central Server Systems).
A closed network in which all customers are routed through some particular
station before they can visit other network nodes is called a central server sys-
tem. Examples for real conﬁgurations that may be modelled this way are com-
puter multiprogramming systems (with a ﬁxed degree of multiprogramming),
multiprocessor systems connected with external memory modules, or a combi-
nation of independently working machines together with one single repair unit
that is visited whenever one of the machines fails. The latter conﬁguration is
representative for many related ones and is known as the machine-repairman
model (already encountered in section 6). Common to all is the possibility to
model the system as a closed queueing network of the above mentioned type.
Consider, for instance, a multiprocessor system in which each processor or
CPU is connected with a bank of memory modules. As soon as a processor
needs some data (e.g. instructions) from a memory module it sends a request
to the bank and stops working until the request is satisﬁed. The memory mod-
ules have buffers into which requests are arranged according to the ﬁrst-come
ﬁrst-served (FCFS) order. A request is ”served” by sending back the data to the
requesting processor. Such a system can be modelled as a closed network in
which the multiprocessor system represents one single inﬁnite server (IS) sta-
tion, and the, say, M −1 memory modules are single server queueing stations
(see ﬁgure 5.3).
...
6

?
...
j
j
j
j
-
-
-
-
A
A
A
A
A
A
A
e
A
eAA
A
e
%
%
%


%

%


%%






%

%

%
e
%
e
e
%
A
e
e
e
A
e
Ae
A
A
-
-%

-
e
-
-
-
-
-
B
B
B
\B\B
\BBB




B



-




B
-
-
B
B
BB

?
6
IS station M
1
2
M −1
Figure 5.3.
Central Server Model
The number K of processors usually is much higher (e.g. 28 = 256) than
the number of memory modules. Since a processor is assumed to wait (re-
mains in idle state) when a request to the memory bank has been sent, these
requests are to be interpreted as the ”users” of the single server stations num-
bered 1, . . . , M −1, whereas intermediately executed job partitions represent

Markovian Queueing Networks
97
the ”users” in the central IS station. In many cases it is well justiﬁed to as-
sume the time to satisfy a request being exponentially distributed. We shall
see below that — with respect to an expected product form solution — there
is no reason to restrict possible choices of service time distributions at the cen-
tral IS server to negative exponential distributions (section 3). Nevertheless,
in order to give a simple formulation, we conﬁne ourselves to the case of an
IS station with identical exponential servers with rate µM. As a consequence,
the multiprocessor system can be modelled as a closed PF (Gordon-Newell)
network.
After partially executing a job a processor may execute another partition or
send a request to the memory bank. In practice, the memory bank is needed
only for, say, κ times in the average. Let κ be identical for all processors. We
summarize the assumptions as follows:
Service times at the memory modules i = 1, . . . , M −1 are exponentially
distributed with parameters µi.
Processor execution times are exponentially distributed with mean 1/µM.
Each processor references memory module i in the long run with probabil-
ity qMi, where M−1
i=1 qMi = 1 (central server condition).
After κ execution times a job is ﬁnished, and starts anew (as another job)
immediately after at the same processor.
Turning the closed network into an open one by applying a similar construction
as mentioned in this section, the interpretation of restarts after job ﬁnishing
ﬁnds its adequate portrayal in the model.
Let a virtual node (the ”network exterior”) be inserted between the routing
edge from M to M, such that the mean number of visits to node M attains the
value vM = 1/qMM = κ (ﬁgure 5.4).
Then the routing probabilities satisfy the relations
qM0 +
M−1

i=1
qMi
=
1,
qiM
=
1
for all i ∈{1, . . . , M −1},
qij
=
0
for all i, j ∈{1, . . . , M −1}
qM0 = qM0 = qMM
=
1
vm
= 1
κ.
What are the performance measures to be computed? It is likely that one is
interested, in the ﬁrst line, in the mean time ¯T that is required to completely

98
AN INTRODUCTION TO QUEUEING THEORY
?
6
t
t
t?
t
...
...
j
j
j
j
-
-
-
-
A
A
A
A
A
A
A
e
A
eAA
A
e
%
%
%


%

%


%%






%

%

%
e
%
e
e
%
A
e
e
e
A
e
Ae
A
A
-
-%

-
e
-
-
-
-
-
B
B
B
B
\BBB\B
\BBB






-




B

-
B
B
BB

?
6
t
6
t
6
IS station M
1
2
M −1
q0M
qM0
Figure 5.4.
Modiﬁed Central Server Model
execute a job, the average delay ¯Ti
T at memory module i per request, the total
mean throughput ¯S of jobs through the multiprocessor system, and the average
number ¯Ni
N of requests waiting or being treated at some memory module i.
These quantities are easily obtained according to the mean value analysis prin-
ciple: Exploiting equations (5.30), merely the corresponding iteration process
has to be performed, using ¯Ni
N (0, si) = 0 as the starting value.
The General Case 1 < si < K
We close this section by turning back to the general case of GN networks that
contain multiple server stations with si < K. Relying on the results from con-
volution analysis, we can express the quantity ¯T(K; si) for a customer’s mean
sojourn time at some station i ∈{1, . . . , M} as a function of the quantities
¯NQ
i
N (K−1, si) and bi(K−1) as follows. Let a renumbering be performed such
that our station under consideration has index M. According to xM(0) = 1
and xM(n)/yM = xM(n −1)/µM(n), the expression (5.25) can be rewritten
as
¯TM
T
(K)
=
K

n=1
CM
C
−1(K −1 −[n −1]) xM(n −1) n
CM
C
(K −1) µM(n)
=
sM

n=1
CM
C
−1(K −1 −[n −1]) xM(n −1) n
CM
C
(K −1) n µM
+
K

n=sM+1
CM
C
−1(K −1 −[n −1]) xM(n −1) n
CM
C
(K −1) sM µM

Markovian Queueing Networks
99
=
K−1

n=0
CM
C
−1(K −1 −n) xM(n)
CM
C
(K −1) µM
+
1
sMµM
K

n=sM+1
CM
C
−1(K −1 −n) xM(n −1) (n −sM)
CM
C
(K −1)
.
Exploiting K−1
n=0 CM
C
−1(K −1 −n)xM(n) = (CM−1 ∗xM)(K −1) =
CM
C
(K −1) in the ﬁrst term, and setting n −sM = (n −1) −sM + 1 in the
second one, we obtain from (5.21) and (5.26)
¯TM
T
(K)
=
1
µM
+
1
µM sM
 K−1

n=sM
(n −sM)CM
C
−1(K −1 −n) xM(n)
CM
C
(K −1)
+
K−1

n=sM
CM
C
−1(K −1 −n) xM(n)
CM
C
(K −1)

=
1
µM
+
1
µM sM

¯NQ
M
N
(K −1, sM) +
K−1

n=sM
pM(n; K −1)

.
The sum K−1
n=sM pM(n; K −1) represents the probability for the event that,
in a network with K −1 customers, at least as many customers are present
at station M as there are servers, which is nothing else than the probability
bM(K −1) for the event that all servers sM are occupied. Hence, replacing
the index M by an arbitrary station index i ∈{1, . . . , M}, we have
¯Ti
T (K) = 1
µi
 
1 + 1
si
'
¯NQ
i
N (K −1, si) + bi(K −1)
(!
.
(5.32)
The remaining mean values are given by equations (5.23), (5.24), and (5.26).
In principle, all performance measures of a Gordon-Newell network with K
customers can be calculated from the corresponding expressions for a network
with one customer less, as is obvious from equations (5.30) and (5.32).
3.
Symmetric Service Disciplines
Consider a queueing network with several chains and R customer classes, and
remember that the pair (r, c) of class and chain identiﬁers deﬁnes the category
of a customer. Let µ(i)
jrc(ni) denote the mean service rate for a category (r, c)
customer at position j in station i, when the latter is in state ni, and denote by
π(i)
jrc
π
(ni) the probability for the event that a category (r, c) customer in transit
to i is going to enter this very position j when immediately before his entrance

100
AN INTRODUCTION TO QUEUEING THEORY
the state is ni. Further, let µ(i)(ni) be the total mean service rate at station i in
that state. Then
µ(i)
jrc(ni)
µ(i)(ni) =: ϕ(i)
jrc(ni)
represents the fraction of the service rate that category (r, c) customers in po-
sition j produce at station i in state ni.
Deﬁnition 5.14 The service discipline at a station i is called a symmetric ser-
vice discipline, and the station is said to be in station balance, if
ϕ(i)
jrc(ni + erc) = π(i)
jrc
π
(ni),
(5.33)
that is, if the service rate µ(i)
jrc(ni + erc) is proportional to the probability
π(i)
jrc
π
(ni).11
The main difference between station balance and local balance lies in the fact
that station balance, in comparing rates, links together the position of a cus-
tomer who completes service and the position that an arriving customer is
about to occupy, whereas local balance just relates arrival and departure rates
for customers of same type. Two conclusions are immediately to be drawn
from this fact:
Station balance implies local balance, but not vice versa.
A non-exponential service discipline can only be symmetric if any arriving
customer receives service immediately, i.e. as soon as he enters the system.
We now give some examples for symmetric disciplines. Thereby, in order to
illustrate the relationships between the arrangement probability upon arrival
and the fraction of service rate at some position, we conﬁne ourselves to the
case that only one class and only one chain exists, such that ϕ(i)
jrc(ni + erc) =:
ϕ(i)
j (ni + 1), and the condition for symmetry reads
ϕ(i)
j (ni + 1) = π(i)
j
π
(ni).
The reader should realize that this simpliﬁcation is unimportant for the exam-
ples given below, and that symmetry also holds in these cases when there are
several chains and several classes.
11Again, the vector erc is deﬁned as to contain a 1 at the entry of ni that belongs to the category (r, c), and
zeros anywhere else.

Markovian Queueing Networks
101
1. Processor sharing (PS) discipline. This is the limiting case for τ →0 of
a Round Robin discipline that provides service to each customer in form of
time slices of duration τ. Positions in the queue remain undeﬁned and can
be assumed to be equal. For the fraction of service in state ni +1 (i.e. when
there are ni+1 customers in station i) we have, for any j ∈{1, . . . , ni+1},
ϕ(i)
j (ni + 1) =
1
ni + 1.
If the state is ni immediately before an arrival, then the newly arriving cus-
tomer can be arranged in any of ni + 1 positions with same probability. So,
π(i)
j
π
(ni) =
1
ni+1 = ϕ(i)
j (ni +1), and the discipline proves to be symmetric.
2. Inﬁnite servers (IS) discipline. As in case of processor sharing the posi-
tion of a customer doesn’t play any role. The fraction ϕ(i)
j (ni + 1) of the
service rate that an (ni + 1)th customer receives is always the (ni + 1)th
part of the total service rate in this state, viz. ϕ(i)
j (ni+1) = 1/(ni+1). The
position where to be inserted is not important for an arriving customer, and
may be seen to be equal for each position among, or in front of, or behind,
the ni existing customers in the station. Thus, the probability π(i)
j
π
(ni) is
the same for all j, and π(i)
j
π
(ni) = 1/(ni + 1) = ϕ(i)
j (ni + 1), showing the
symmetry also in this case.
3. Last-come ﬁrst-served preemptive-resume (LCFS-PR) discipline. In this
discipline any newly arriving customer ousts the one in service from his
place. Let the position of the customer in service be 1. Then the fraction
ϕ(i)
1 (ni + 1) of service that the arriving customer receives, is one since all
other customers are not served during state ni + 1, i.e. ϕ(i)
1 (ni + 1) = 1.
On the other side, the probability π(i)
1 (ni) for the event that an arriving cus-
tomer is arranged in position 1 at station i (when station i was in state ni im-
mediately before his arrival) is one, too. Therefore, ϕ(i)
1 (ni+1) = π(i)
1 (ni).
A special role plays the ﬁrst-come ﬁrst-served (FCFS) discipline. As is eas-
ily seen, this discipline is not symmetric, since an arriving customer is always
added to the queue at its end whereas service is provided only to the customer
at its front (ﬁrst) position. If customers deserve service from different ser-
vice time distributions then (5.33) cannot be satisﬁed for all. There is, but,
one exception: If service times are chosen from the same exponential distri-
bution for all customers, then positions and customers are indistinguishable,
and the actual service completion rate at any time, also at an arrival instant,

102
AN INTRODUCTION TO QUEUEING THEORY
remains the same due to the memoryless property of the exponential distribu-
tion. That means that the rate / fraction equation (5.33) holds. Consequently,
an FCFS station providing exponential service with the same intensity to all its
customers attains station balance.
A network station that provides service according to one of the above disci-
plines is called a PS station, IS station, LCFS-PR station, or FCFS exponential
station, respectively.
We are now in the position to conclude, that a multiple chain/multiple class
queueing network that is fed by Poisson arrival streams (if open or mixed), and
is built up by stations of types PS, LCFS-PR, IS, or FCFS exponential, attains
station balance and, consequently, local balance and product form.
This result has ﬁrst been proven by Baskett, Chandy, Muntz, and Palacios in
1977 [8], and is well known as the BCMP theorem. The authors introduced
a numbering for the four types of service disciplines that has been adopted by
most experts in the ﬁeld. It runs as follows:
Type 1 service: The service discipline is FCFS, and all customers have the
same negative-exponential service time distribution. The service rate may
depend on the number of customers at the station (this is the case when
there are more than one servers available).
Type 2 service: The service discipline is PS, there is a single server at
the station, and each class of customer may have a distinct service time
distribution. The service time distributions have rational Laplace transform.
Type 3 service: The number of servers at the station is greater than or equal
to the maximum number of customers that may visit this station (IS disci-
pline). Each class of customer may have a distinct service time distribution.
The service time distributions have rational Laplace transform.
Type 4 service: There is a single server at the station, the service discipline
is LCFS-PR, and each class of customer may have a distinct service time
distribution. The service time distributions have rational Laplace transform.
The BCMP theorem explicitly describes the factors of the product form for
closed, open or mixed networks (with Poisson arrival streams). In order to
present these results adequately we have to explain some details. First, two
types of arrival process are distinguished: A single Poisson arrival stream
whose intensity γ may be a function of the state dependent total number K(n)
of customers in the network, or several chain speciﬁc Poisson streams with
intensities γc that in turn depend on the numbers Kc
K (n) of customers in the

Markovian Queueing Networks
103
respective chains (1 ≤c ≤V , V the total number of chains). Second, the state
descriptions are type speciﬁc as follows:
1 For type 1 service stations (exponential service, undistinguishable customers)
the queue speciﬁc states are represented by the vectors
ni = (ri1, . . . , rini),
where ni is the total number of customers present at station i, and rij is
the class of the customer at position j in the queue. Positions are counted
beginning from the ”server position” 1 up to the end of the queue ni. The
need for discriminating between classes will become clear below when we
specify fif (ni).




-










ri2
ri3
. . .
rini
Figure 5.5.
FCFS Order
2 For types 2 and 3 the service time distributions have rational Laplace trans-
form, so they belong to the family of Cox-distributions. A Cox distribution
is characterized by a sequence of exponential stages that are visited by the
customer in service in compliance with routing probabilities αirℓ. These
probabilities (or their complementary values 1 −αirℓ) steer the customer
to the next stage or to exit from service (see ﬁgure 5.4). Here r is the class
index of the customer in service, and i the station index.
















-



-
-
-
. . .
T
T
T
T
T

T
T
T

T
T
-
T

T
T
T
T
-
αir1 = 1
αir2
αir3
αiruir
1 −αir2 1 −αir3
1 −αiruir


T


T


T

2



















-
-
T
-
T
-












Figure 5.6.
Cox Distribution
The state vector ni of station i takes the form ni = (si1, . . . , siR), where
each sir = (sir1, . . . , siruir) is a vector of labels sirℓ, and
sirℓ=
)
number of class r customers (if any) at
station i, who are in stage ℓof service.

104
AN INTRODUCTION TO QUEUEING THEORY
sirℓis set to zero if there are no class r customers in station i. uir is the
number of exponential stages for a class r service time distribution at i
(1 ≤r ≤R).
3 Type 4 centers are characterized by the LCFS-PR scheduling discipline,
offering service according to Cox distributed service times. Whereas in
case of PS or IS disciplines (types 2 and 3) the customer position has no
signiﬁcance, here it is very important. The so-called LCFS order has the
opposite direction of FCFS order (see ﬁgure 5.5). The state vector ni re-
ﬂects the classes as well as the stages of service of all the customers at their
respective positions,
ni =
 
(r1, ℓ1), (r2, ℓ2), . . . , (rni, ℓni)
!
.
ni is the total number of customers in station i in that state, rj is the class,
and ℓj the stage of service of the customer in position j. Position ni is that
of the customer who arrived last and who is actually in service.




-








rini−1
ri2 . . .
ri1
Figure 5.7.
LCFS Order
If there exist open chains in the network, then one may count the customer
visits to the stations. The mean visit number to station i of a class r customer
who belongs to chain c is deﬁned as the ratio
virc =
λirc
γc(Kc
K (n)),
where λirc is the mean arrival rate of category (r, c) customers at station i,
and γc(Kc
K (n)) is the total chain c arrival rate from outside the network, that
may be dependent upon the number Kc
K (n) of class c customers in the network
during state n. Let Mc
M be the subset of stations visited by chain c, and Rc the
subset of classes occurring in chain c, and set Ec = Mc
M × Rc. Then the λirc
satisfy the trafﬁc equations
λirc = γc(Kc
K (n)) q0;irc +

(j,s)∈Ec
λjsc qjsc
q
;irc.
Consequently, the mean visit numbers virc (also called the relative through-
puts) satisfy the equations
virc = q0;irc +

(j,s)∈Ec
vjsc qjsc
q
;irc.
(5.34)

Markovian Queueing Networks
105
We are now in the position to formulate the result of Baskett, Chandy, Muntz,
and Palacios. Let R be the total number of customer classes, V the total num-
ber of chains, and Airℓthe product of steering probabilities in a Cox distribu-
tion (compare ﬁgure 5.6), i.e., for 1 ≤i ≤M, 1 ≤r ≤R, and 1 ≤ℓ≤uir,
Airℓ=
ℓ
ν=1
αirν.
Theorem 5.15 (BCMP theorem) Let an open, closed, or mixed queueing net-
work with V chains and R customer classes contain service stations of types
1, 2, 3, or 4, only. Assume, that in case of an open or mixed network the exter-
nal arrival streams are Poisson of type 1 or 2, respectively. Then, the network
attains equilibrium with a product form steady state distribution
pn = d(n)
C
M

i=1
fif (ni),
(5.35)
where the fif (ni) are service type dependent state functions, and the value d(n)
is deﬁned by
d(n) =
⎧
⎪
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
⎪
1
for a closed network (only 1 chain,
no external arrival process)
#K(n)−1
k=0
γ(k)
for an open network (only 1 chain,
external arrival process of ﬁrst type)
#V
c=1
#Kc(n)−1
k=0
γc(k)
for a mixed network (several chains,
external arrival processes of second type)
.
The state functions fif (ni) are given by the following expressions.
Type 1 (FCFS exponential):
fif (ni) =
ni

j=1
virij
µi(j).
Type 2 (PS, Cox distribution):
fif (ni) = ni!
R

r=1
uir

ℓ=1
vir Airℓ
µirℓ
sirℓ
1
sirℓ!.

106
AN INTRODUCTION TO QUEUEING THEORY
Type 3 (IS, Cox distribution):
fif (ni) =
R

r=1
uir

ℓ=1
vir Airℓ
µirℓ
sirℓ
1
sirℓ!.
Type 4 (LCFS-PR Cox distribution):
fif (ni) =
ni

j=1
virij Airijℓj
µirijℓj
.
Essentially, the BCMP theorem is a consequence from the fact that station
balance implies local balance and product form. The detailed elaboration of
the above mentioned concrete expressions for the factors in (5.35) can be per-
formed by applying the symmetry relations (5.33) and the resulting local bal-
ance equations to the product of state probabilities of isolated stations, just as
in case of Jackson or Gordon-Newell networks. This line of reasoning has been
pursued by Baskett, Chandy, Muntz, and Palacios. We do not repeat this here,
rather we refer to their original work in [8].
Notes
There is a multitude of additional results on queueing networks, including var-
ious algorithms for the exact and approximate treatment of product form (PF)
networks, reﬁned approximation methods for non-PF networks, generaliza-
tions to networks with blocking, approximation techniques for networks with
priority handling, and even maximum entropy methods. To cover all these
results would go far beyond of the scope of this introductory book. The el-
ements of queueing network theory can already be found in Kleinrock’s fun-
damental book on queueing systems (Volume I: Theory) [50], and in an early
overview on exact and approximate methods for the evaluation of steady state
probabilities of Markovian networks (including Jackson, Gordon-Newell, and
BCMP networks) by Gelenbe and Pujolle in 1987 [36]. Also in 1987 appeared
the excellent little introduction to performance analysis methods for computer
communication systems by I. Mitrani [60]. The beginner is well advised to
read this book ﬁrst. It presents a neatly formulated and easy to understand
explanation of the basic ideas behind various fundamental approaches.
A standard work on reversibility properties and their relationships to balance
behaviour is that of Kelly of the year 1979 [48]. The various techniques de-
veloped there are employed also by Nelson in his recommended treatise on
probability, stochastic processes, and queueing theory [61]. We further refer
to the more application oriented books of Van Dijk [30], who addresses the

Markovian Queueing Networks
107
physical background of ﬂow balance properties, and Harrison and Patel [41]
who — with respect to queueing networks — describe several applications to
computer networks and computer architectures. A more recently published
comprehensive treatment of queueing networks and Markov chains is that of
Bolch et alii [15]. This book covers all main aspects of modern queueing net-
work analysis, and presents up to date algorithmic methods. The reader may
also ﬁnd an exhaustive list of references in [15]. Finally, we refer to the excel-
lent investigation of queueing networks with discrete time scale that has been
presented in 2001 by Daduna [29]. Due to the discrete structure of most of
todays communication systems this approach should attain particular attention
in the future.
Exercise 5.1 Show that a stationary Markov process whose undirected state
transition diagram forms a tree is reversible. Hint: Use the fact that the proba-
bility ﬂux in one direction across a cut of the graph equals the ﬂux in opposite
direction.
Exercise 5.2 Show that any stationary birth-death process is quasi-reversible,
and conclude from this fact the Theorem of Burke for M/M/s queues: The
departure process of an M/M/s queue is Poisson with same rate as the arrival
process.
Exercise 5.3 Prove that a quasi-reversible process need not be reversible. Hint:
Consider an M/M/1 queue with mean arrival rate λ, whose state 1 is separated
in two different states 1′ and 1′′, such that 1′ is reached from state 0 with rate
λ · p, and state 1′′ is reached from state 0 with rate λ · (1 −p) for 0 < p < 1,
the departure rates remaining unchanged.
Exercise 5.4 A data transmission unit works as follows. Data packages ar-
rive at the unit according to a Poisson process with intensity λ. For each data
package there is an exponential time (with parameter µ) from the beginning of
transmission to the receipt of an acknowledgement. Arriving packages which
ﬁnd the transmission unit busy wait in a queue and are served in FCFS or-
der. The buffer for the queue is so large that it may be assumed to be inﬁnite.
With probability p, a data package incurs a transmission error and needs to be
retransmitted. The stream of data packages to retransmitted is added to the
regular arrival stream.
a) Derive a model for this kind of data transmission in terms of a Jackson net-
work.
b) Show that the combined stream of regularly arriving packages and the pack-
ages to be retransmitted is not a Poisson process.
c) Determine the mean time needed for a successful transmission of a data
package in the stationary regime.

108
AN INTRODUCTION TO QUEUEING THEORY
Exercise 5.5 A server in a computer pool is modelled as a queueing network
with two stations. The ﬁrst of these represents the CPU, the second one all
output devices. Service times in both stations are distributed exponentially,
with parameters µ1 and µ2. Jobs arrive from the pool as a Poisson process
with intensity λ. After service in the CPU, a job is done with probability p.
With probability 1 −p it needs additional service by one of the output devices.
 CPU
p
 output
λ
Figure 5.8.
Simple model of a computer pool
Determine the mean sojourn time of a job in the server under the stationary
regime. Networks consisting of some stations in series are called tandem
queues. However, in general tandem queues the service times do not need
to be exponential.
Exercise 5.6 For a cyclic closed network with M stations, the routing matrix
Q is given by
Q(i, j) :=
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
1,
j = i + 1, 1 ≤i < M
1,
j = 1, i = M
0,
else
Assume that there are K users in the network. Show that the stationary distri-
bution is given by
p(n) =
1
G(K) ·
µK−n1
1
#M
i=2 µni
i
with µi denoting the service rate at station i.
Exercise 5.7 An internet based company seeks to ensure constant online ac-
cess, because it cannot operate without. To this aim, two servers instead of one
are employed concurrently. Each of them has a failure rate λ > 0, meaning
that their up time is exponentially distributed with parameter λ. After failure,
a server is repaired with probability p. The repair time is distributed expo-
nentially with parameter µ1 > 0. With probability 1 −p, the server must be
replaced by a new one, which requires an installation time that is distributed
exponentially with parameter µ2 > 0. After the server is repaired, there is still
a probability q that it must be replaced by a new one, requiring additionally the
same installation time.

Markovian Queueing Networks
109
Derive a model for this situation in terms of a Gordon–Newell network. For the
values λ = 2, µ1 = 1, µ2 = 3, p = 3/4, and q = 1/3, determine the stationary
probability that both servers are down and the company cannot operate. Com-
pare this to the stationary probability that the company cannot operate for the
case that only one server is employed. Such questions are typical for reliability
theory.


Chapter 6
RENEWAL THEORY
1.
Renewal Processes
Be (Xn
X
: n ∈N0) a sequence of independent positive random variables, and
assume that (Xn
X
: n ∈N) are identically distributed. Deﬁne the sequence
S = (Sn
S
: n ∈N) by S1 := X0 and Sn
S +1 := Sn
S
+ Xn
X
for all n ∈N.
The random variable Sn
S , with n ∈N, is called the nth renewal time, while
the time duration Xn
X
is called the nth renewal interval. Further deﬁne the
random variable of the number of renewals until time t by
Nt
N := max{n ∈N : Sn
S ≤t}
for all t ≥0 with the convention max ∅= 0. Then the continuous time process
N = (Nt
N : t ∈R+
0 ) is called a renewal process. The random variable X0
is called the delay of N. If X0 and X1 have the same distribution, then N is
called an ordinary renewal process.
time
0
S
S
S
N = 2
X ~ G
X ~ F
X ~ F
. . . 
1
2
3
t
S
S
0
1
  G
X 
 G
X 
2 
t
Figure 6.1.
Random variables of a renewal process
We will always assume that P(X1 = 0) = 0 and m := E(X1) < ∞is ﬁnite.
The strong law of large numbers implies that Sn
S /n →m with probability one

114
AN INTRODUCTION TO QUEUEING THEORY
as n →∞. Hence Sn
S
< t cannot hold for inﬁnitely many n and thus Nt
N is
ﬁnite with probability one. By standard notation we will write
G(x) := P(X0 ≤x)
and
F(x) := P(X1 ≤x)
for all x ∈R+
0 .
Example 6.1 A light bulb has been installed at time zero. After a duration
X0, it will go out of order. We assume that it will be immediately replaced
by a new light bulb at time S1 = X0. Assume that the new light bulb is of a
type identical to the old one. Then the duration X1 until it goes out of order
is distributed identically to X0. Of course, the life times of the light bulbs
are independent from one another. Keeping up this rechangement policy over
time, the number Nt
N of used light bulbs until time t forms an ordinary renewal
process.
Remark 6.2 A Poisson process with intensity λ (see example 3.1) is an ordi-
nary renewal process with F(x) = G(x) = 1−e−λx, i.e. the renewal intervals
have an exponential distribution. Thus a renewal process can be seen as a gen-
eralization of the Poisson process with respect to the distribution of the renewal
intervals.
In order to derive an expression for the distribution and the expectation of Nt
N at
any time t, we need to introduce the concept of convolutions of a non–negative
function and a distribution function. Let F denote a distribution function on
R+
0 and g : R+
0 →R+
0 a Lebesgue–measurable function which is bounded on
all ﬁnite intervals [0, t] with t ≥0. Then the function deﬁned by
F ∗g(t) :=
 t
0

g(t −u) dF(u)
for all t ∈R is called the convolution of F and g. In particular, the deﬁnition
of a convolution applies if g is a distribution function. As an exercise the reader
can prove
Theorem 6.3 For any distribution functions F and G as well as non–negative
Lebesgue–measurable functions (gn : n ∈N) on R+
0 , the following properties
hold:
(1) The convolution F ∗G is a distribution function on R+
0 .
(2) F ∗G = G ∗F
(3) F ∗∞
n=1 gn = ∞
n=1 F ∗gn
(4) The Dirac measure δ0 on 0 with distribution function I0
I , which is deﬁned
by I0
I (t) := 1 for all t ≥0 and I0
I (t) := 0 otherwise, is neutral in regard to
convolutions, i.e. I0
I ∗G = G for all distribution functions G.

Renewal Theory
115
(5) If the random variables X and Y are independent and distributed accord-
ing to F and G, respectively, then P(X + Y ≤t) = F ∗G(t) for all t ≥0.
(6) F ∗(G ∗g) = (F ∗G) ∗g
Let F denote any distribution function for a real–valued random variable. De-
ﬁne the convolutional powers by F ∗1 := F and recursively F ∗n+1 := F ∗n∗F
for all n ∈N.
Because of property (4) in the above theorem, we deﬁne
F ∗0 := I0
I for every distribution function F.
Now denote the distribution function of the random variable X1 (and hence
of all Xn
X
with n ≥1) and X0 by F and G, respectively. Since the random
variables (Xn
X
: n ∈N) are iid, part (5) of the above theorem yields for all
n ∈N0 the relation P(Nt
N ≥n) = P(Sn
S
≤t) = G ∗F ∗n−1(t) and thus we
obtain P(Nt
N = 0) = 1 −G(t) and
P(Nt
N = n) = P(Sn
S ≤t) −P(Sn
S +1 ≤t) = G ∗F ∗n−1(t) −G ∗F ∗n(t)
for n ≥1. The expectation of Nt
N is given by
E(Nt
N ) =
∞

n=1
P(Nt
N ≥n) =
∞

n=1
P(Sn
S ≤t) = G ∗
∞

n=0
F ∗n(t)
(6.1)
for all t ≥0 (for the ﬁrst equality see Exercise 6.2). The rate of growth of a
renewal process is described by
Theorem 6.4 Let N = (Nt
N : t ≥0) denote a renewal process with renewal
intervals having mean length m < ∞. Then
lim
t→∞
Nt
N
t = 1
m
holds with probability one.
Proof: By deﬁnition of Nt
N (see picture below and ﬁgure 1), the inequalities
SNt
N ≤t ≤SNt
N +1 hold with probability one for all times t.
t
time
S
S
t
N
Nt
t + 1
Dividing these by Nt
N and using the strong law of large numbers, we obtain
m = lim
n→∞
Sn
S
n = lim
t→∞
SNt
N
Nt
N
≤lim
t→∞
t
Nt
N
≤lim
t→∞
 SNt
N +1
Nt
N + 1 · Nt
N + 1
Nt
N

= lim
n→∞
Sn
S +1
n + 1 · lim
n→∞
n + 1
n
= m · 1

116
AN INTRODUCTION TO QUEUEING THEORY
which proves the statement.
□
Because of this theorem, the inverse 1/m of the mean length of a renewal
interval is called the rate of the renewal process. It describes the asymptotic
rate at which renewals occur.
Example 6.5 Regarding a Poisson process N = (Nt
N : t ≥0) with intensity
λ > 0, it can be shown that
P(Nt
N = n) = (λt)n
n! e−λt
(6.2)
for all t ≥0 and n ∈N0. The expectation of Nt
N is given by E(Nt
N ) = λ · t.
Thus a Poisson process with intensity λ has at time t a Poisson distribution
with parameter λ · t. Moreover, the intensity λ is also the rate of the Poisson
process, since a mean renewal interval has length 1/λ.
Given an observed stream of events (e.g. job requests at a server) over some
time interval of length t, we can count the number N(t) of events that have
occurred in this interval. If we want to model such event streams by a Poisson
process, then we need to ﬁnd a statistical estimator for the intensity λ. Now
theorem 6.4 states that the fraction N(t)/t comes close to λ for large interval
lengths t. Thus a consistent statistical estimator for the intensity λ is given by
ˆλ = N(t)/t.
Example 6.6 There is a discrete–time analogue of the Poisson process, which
is called Bernoulli process. This is an ordinary renewal process with renewal
intervals that have a geometric distribution. Given a parameter p ∈]0, 1[, the
length of the renewal intervals is distributed as P(X1 = n) = p · (1 −p)n−1
for n ∈N.
2.
Renewal Function and Renewal Equations
The function deﬁned by R(t) := ∞
n=1 F ∗n(t) for all t ≥0 is called the
renewal function of the process N. The renewal function will play a central
role in renewal theory. First we need to show that it remains ﬁnite:
Theorem 6.7 If F(0) < 1, then R(t) = ∞
n=1 F ∗n(t) < ∞for all t ≥0.
Proof: Since F(0) < 1 and F is continuous to the right, there is a number
α > 0 such that F(α) < 1. Fix any t ≥0 and choose k ∈N such that
k · α > t. Then F ∗k(t) ≤1 −(1 −F(α))k =: 1 −β with 0 < β < 1. Thence

Renewal Theory
117
we obtain the bound F ∗mk(t) ≤(1 −β)m for any m ∈N. Since F(0−) = 0,
we can use F ∗n(t) ≥F ∗h(t) for all n < h ∈N. Putting these bounds together,
we obtain
R(t) =
∞

n=1
F ∗n(t) ≤k ·
∞

m=0
F ∗mk(t) ≤k ·
∞

m=0
(1 −β)m = k
β < ∞
since β > 0.
□
Theorem 6.8 An ordinary renewal process is uniquely determined by its re-
newal function.
Proof: First we take the Laplace–Stieltjes transform (LST, see appendix 3) on
both sides of the equation R(t) = ∞
n=1 F ∗n(t). This yields
˜R(s) =
∞

n=1
*
F ∗n(s) = ˜F(s) ·
∞

n=0
( ˜F(s))n =
˜F(s)
1 −˜F(s)
(6.3)
for s > 0, or
˜F(s) =
˜R(s)
1 + ˜R(s)
and thus determines the LST ˜F(s) of F uniquely in terms of ˜R(s). Now
uniqueness of the LST yields the statement.
□
For an ordinary renewal process we can derive an implicit integral equation for
the renewal function, which is known as a renewal equation. Note that for an
ordinary renewal process E(Nt
N ) = R(t) for all times t (see (6.1) with G = F).
Hence the function R is increasing. If we condition upon the length x of the
ﬁrst renewal interval X0, we obtain
E(Nt
N ) =
 ∞
0

E(Nt
N |X0 = x) dF(x)
Since E(Nt
N |X0 = x) = 1 + R(t −x) for t ≥x and E(Nt
N |X0 = x) = 0 for
t < x, we can simplify this equation to
R(t) =
 t
0

(1 + R(t −x)) dF(x) = F(t) +
 t
0

R(t −x) dF(x)
for all t ≥0. A renewal equation is the generalized form
g(t) = h(t) +
 t
0

g(t −x) dF(x),
t ≥0
(6.4)

118
AN INTRODUCTION TO QUEUEING THEORY
where a function h on [0, ∞[ and a distribution function F on [0, ∞[ are given
and the function g on [0, ∞[ is unknown. The solution is given in
Theorem 6.9 The unique solution g to equation (6.4) is given by
g(t) =
 t
0

where R(t) = ∞
n=1 F ∗n(t) denotes the renewal function for F.
Proof: Equation (6.4) can be written as g = h+g∗F. Because of the deﬁnition
R = ∞
n=1 F ∗n we obtain
F ∗(R ∗h + h) = F ∗h +
∞

n=1
F ∗n+1 ∗h =
∞

n=1
F ∗n ∗h = R ∗h
which shows that g = R ∗h + h is indeed a solution of (6.4).
Let g′ denote another solution and deﬁne the function
δ := g′ −R ∗h −h
Then (6.4) implies δ = F ∗δ and thus δ = F ∗n ∗δ for all n ∈N. Since
R(t) < ∞for any ﬁxed t ≥0, we infer that F ∗n →0 as n →∞. Hence
δ(t) = 0 for all t ≥0, which completes the proof.
□
3.
Renewal Theorems
In order to present the most powerful results of renewal theory, it will be useful
to introduce stopping times and Wald’s lemma. Recall from (2.3) that a random
variable S with values in N0 ∪{∞} is called a stopping time for the sequence
X = (X0 : n ∈N0) if
P(S ≤n|X) = P(S ≤n|X0, . . . , Xn
X )
(6.5)
holds for all n ∈N0.
Lemma 6.10 For a renewal process N with delay X0 and renewal intervals
(Xn
X
: n ∈N), the random variable Nt
N is a stopping time for the sequence
(Xn
X : n ∈N0).
Proof: This follows from the observation that Nt
N = k is equivalent to
k−1

n=0
Xn
X ≤t <
k

n=0
Xn
X
h(t −x) dR(x) + h(t)

Renewal Theory
119
which implies that the event Nt
N ≤k depends only on X0, . . . , Xk.
□
Lemma 6.11 Wald’s Lemma
Be X = (Xn
X
: n ∈N0) a sequence of stochastically independent positive
random variables with the same expectation E(Xn
X ) = m for all n ∈N. The
expectations E(X0) and E(X1) shall be ﬁnite. Further be S a stopping time of
the sequence X with E(S) < ∞. Then
E
 S

n=0
Xn
X

= E(X0) + E(S) · m
Proof: For all n ∈N0 deﬁne the random variables In
I := 1 on the set {S ≥n}
and In
I := 0 else. Then S
n=0 Xn
X = ∞
n=0 In
I Xn
X and hence
E
 S

n=0
Xn
X

= E
 ∞

n=0
In
I Xn
X

=
∞

n=0
E(In
I Xn
X )
by monotone convergence, as In
I and Xn
X are non–negative. S being a stopping
time for X, we obtain by deﬁnition P(S ≥0) = 1, and further
P(S ≥n|X) = 1 −P(S ≤n −1|X) = 1 −P(S ≤n −1|X0, . . . , Xn
X −1)
for all n ∈N. Since the Xn
X are independent, In
I and Xn
X are independent, too,
which implies E(I0
I X0) = E(X0) and
E(In
I Xn
X ) = E(In
I ) · E(Xn
X ) = P(S ≥n) · m
for all n ∈N. Now the relation ∞
n=1 P(S ≥n) = E(S) yields
E
 S

n=0
Xn
X

=
∞

n=0
E(In
I Xn
X ) = E(X0) +
∞

n=1
P(S ≥n) · m
= E(X0) + E(S) · m
□
Theorem 6.12 Elementary Renewal Theorem
Be N a renewal process with renewal intervals (Xn
X
: n ∈N) and mean
renewal time E(X1) = m > 0. Assume further that the mean delay is ﬁnite,
i.e. E(X0) < ∞. Then for the counting function Nt
N the limit
lim
t→∞
E(Nt
N )
t
= 1
m

120
AN INTRODUCTION TO QUEUEING THEORY
holds, with the convention 1/∞:= 0.
Proof: For every t ≥0, the bound t < Nt
N
n=0 Xn
X
holds almost surely. By
Wald’s lemma, this implies
t < E
 Nt
N

n=0
Xn
X

= E(X0) + E(Nt
N ) · m
and thence for m < ∞
1
m −E(X0)
m · t < E(Nt
N )
t
for all t ≥0. For E(X0) < ∞and t →∞, this yields the bound
lim inf
t→∞
E(Nt
N )
t
≥1
m
which trivially holds for the case m = ∞.
Now it remains to show that lim supt→∞E(Nt
N )/t ≤1/m. To this aim we
consider the truncated renewal process, denoted by ˜
N, with the same delay
˜X0 = X0 but renewal intervals ˜Xn
X
= min(Xn
X , M) for all n ∈N, with M
being a ﬁxed constant. Denote further ˜m = E( ˜X1).
Because of ˜Xn
X ≤M the bound  ˜
Nt
N
n=0 ˜Xn
X ≤t + M holds almost certainly for
all t ≥0. Taking expectations and applying Wald’s lemma, we obtain
E(X0) + E( ˜Nt
N ) · ˜m = E
⎛
⎝
⎛˜
Nt
N

n=0
˜Xn
X
⎞
⎠
⎞
≤t + M
For E(X0) < ∞and t →∞, this yields
lim sup
t→∞
E( ˜Nt
N )
t
≤1
˜m
Since ˜Xn
X
≤Xn
X for all n ∈N, we know that ˜Nt
N ≥Nt
N for all t ≥0. Thus we
obtain further
lim sup
t→∞
E(Nt
N )
t
≤1
˜m
for any constant M. Now the result follows for M →∞.
□

Renewal Theory
121
Remark 6.13 In view of theorem 6.4 one might be tempted to think that this
trivially implied the statement of the above theorem 6.12. However, the fol-
lowing example shows that a limit with probability one in general does not
imply a limit in expectation.
Let U denote a random variable which is uniformly distributed on the interval
]0, 1[. Further deﬁne the random variables (Vn
V : n ∈N) by
Vn
V :=

0,
U > 1/n
n,
U ≤1/n
Since U > 0 with probability one, we obtain the limit
Vn
V →0,
n →∞
with probability one. On the other hand, the expectation for Vn
V is given by
E(Vn
V ) = n · P(U ≤1/n) = n · 1
n = 1
for all n ∈N and thus E(Vn
V ) →1 as n →∞.
A non–negative random variable X (and also its distribution function F) is
called lattice if there is a positive number d > 0 with ∞
n=0 P(X = nd) = 1.
If X is lattice, then the largest such number d is called the period of X (and
F). The deﬁnition states that a lattice random variable X assumes only values
that are multiples of its period d.
The next result is proven in Feller [35]. The proof is lengthy and technical and
therefore not repeated.
Theorem 6.14 Blackwell’s Theorem
Be N a renewal process with renewal intervals (Xn
X
: n ∈N) and mean
renewal time E(X1) = m. If X1 is not lattice, then for any s > 0 the counting
function Nt
N behaves asymptotically as
lim
t→∞(E(Nt
N +s) −E(Nt
N )) = s
m
with the convention 1/∞:= 0.
Blackwell’s theorem suggests the following argument: Because of the identity
E(Nt
N ) = R(t), it states that asymptotically
R(t + s) −R(t) →s · 1
m
as
t →∞

122
AN INTRODUCTION TO QUEUEING THEORY
This means that increments of the renewal function t →R(t) tend to be linear
(with coefﬁcient 1/m) for large t. If we let s →0, this would suggest
dR(t) →1
mdt
as
t →∞
For functions g which behave nice enough and vanish at inﬁnity (i.e. g(t) →0
as t →∞), we thus can hope to establish
lim
t→∞
 t
0

g(t −x) dR(x) = 1
m
 ∞
0

g(t) dt
In order to do this, we ﬁrst need to deﬁne what we require as ”nice behaviour”
from g. Let g : R+
0 →R denote a real–valued function on the time axis and
deﬁne for a > 0 and n ∈N
Mn
M (a) := sup{g(x) : (n −1)a ≤x ≤na}
(6.6)
mn(a) := inf{g(x) : (n −1)a ≤x ≤na}
(6.7)
The function g is called directly Riemann integrable if ∞
n=1 |Mn
M (a)| and
∞
n=1 |mn(a)| are ﬁnite for some a > 0 (and then for all 0 < a′ < a), and
lim
a→0 a
∞

n=1
Mn
M (a) = lim
a→0 a
∞

n=1
mn(a)
(6.8)
Remark 6.15 Direct Riemann integrability is somewhat stronger than usual
Riemann integrability. The similarity is that upper and lower sums converge to
the same limit as a →0. The difference is that this must happen uniformly for
all intervals of the time axis.
In the rest of this book, we will deal with only two kinds of directly Riemann
integrable functions. For these we provide the following lemma, which the
reader may prove as an exercise.
Lemma 6.16 Assume that g(t) ≥0 for all t ≥0. If either
(1) g is non–increasing and Lebesgue integrable, or
(2) g is Riemann integrable and there is a function g∗with g(t) ≤g∗(t) for all
t ≥0, such that g∗is directly Riemann integrable,
then g is directly Riemann integrable.
Now we can state the main result of renewal theory:

Renewal Theory
123
Theorem 6.17 Key Renewal Theorem
Assume that m = E(X1) > 0, where X1 is not lattice, and let g denote a
directly Riemann integrable function. Then
lim
t→∞(R ∗g)(t) = lim
t→∞
 t
0

g(t −x) dR(x) = 1
m
 ∞
0

g(y) dy
holds with the convention 1/∞= 0.
Proof: Let (xn : n ∈N0) with x0 := 0 denote any countable partition of the
time axis R+
0 into intervals of the form In
I
:= [xn−1, xn[. Deﬁne the indicator
function of In
I by in(t) := 1 if t ∈In
I and in(t) := 0 otherwise. Then
(R ∗in)(t) =
 t
0

in(t −u) dR(u) =
 t−xn−1
t

−

xn
dR(u)
= R(t −xn−1) −R(t −xn)
for all t > xn. Now Blackwell’s theorem 6.14 yields
lim
t→∞(R ∗in)(t) = xn −xn−1
m
for every n ∈N.
For any ﬁnite interval [t−l, t[ of length l, the interpretation that R(t) = E(Nt
N )
for an ordinary renewal process N yields with Gt(x) := P(SNt
N +1 −t ≤x)
the bound
R(t + l) −R(t) = E(Nt
N +l −Nt
N ) =
 l
0

(R(l −x) + 1) dGt(x)
≤R(l) + 1 =: B(l) < ∞
for every l > 0.
For a function h = ∞
n=1 cnin with maximal interval length M and coefﬁ-
cients bounded by ∞
n=1 cn < ∞, we obtain thus
k

n=1
cn · (R ∗in)(t) ≤(R ∗h)(t) ≤
k

n=1
cn · (R ∗in)(t) + B(M) ·
∞

n=k+1
cn
for every k ∈N and t ≥0. Letting ﬁrst t →∞and then k →∞, the limit
lim
t→∞(R ∗h)(t) = 1
m
∞

n=1
cn · (xn −xn−1) = 1
m
 ∞
0

h(y) dy

124
AN INTRODUCTION TO QUEUEING THEORY
is established for any such function h.
Since g is directly Riemann integrable, there is a family of functions
flf :=
∞

n=1
mn(l)in
and
hl :=
∞

n=1
Mn
M (l)in
using the deﬁnitions (6.6) and (6.7). These functions satisfy flf ≤g ≤hl and
have the form of the function h above with interval length l. Then
R ∗flf ≤R ∗g ≤R ∗hl
for all l > 0, and the result follows for l →0 according to condition (6.8).
□
This proof shows that the key renewal theorem is a consequence of Blackwell’s
theorem. The simple case g := 1[0,s[, i.e. g(t) = 1 for 0 ≤t < s and g(t) = 0
for t ≥s yields
lim
t→∞(E(Nt
N +s) −E(Nt
N )) = lim
t→∞(E(Nt
N −Nt
N −s)) = lim
t→∞(R ∗g)(t) = 1
m · s
as an application of the key renewal theorem. Hence the statements in Black-
well’s and the key renewal theorem are equivalent.
Besides its central role in renewal theory, the key renewal theorem will serve
mainly two purposes in the further presentation. First, it will give a foundation
for the proof of the main limit theorem in Markov renewal theory (see chapter
7). Second, it yields a limit theorem for regenerative processes (see section 1)
as an immediate corollary.
4.
Residual Life Times and Stationary Renewal Processes
Choose any time t ≥0. Denote the duration from t until the next arrival by
Bt := SNt
N +1 −t and call it the residual life time (or the excess life) at t.
Further we deﬁne At := t −SNt
N and call At the age at t. The distribution of
Bt appeared already in the proof of theorem 6.17.
Theorem 6.18 Be N an ordinary renewal process with renewal intervals hav-
ing distribution function F. Then
P(Bt ≤x) = F(t + x) −
 t
0

(1 −F(t + x −y)) dR(y)
for all t ≥0. Further the limit
lim
t→∞P(Bt ≤x) = 1
m
 x
0

(1 −F(y)) dy
(6.9)

Renewal Theory
125
holds if F is not lattice.
Proof: Fix any x ≥0. First abbreviate g(t) := P(Bt > x) for all t ≥0.
Conditioning on X0 yields
g(t) =
 ∞
0

P(Bt > x|X0 = s) dF(s)
By deﬁnition the event {Bt > x} is equivalent to the event that there are no
renewals in the interval ]t, t+x]. This observation and the fact that the process
restarts at S1 = X0 yield
P(Bt > x|X0 = s) =
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
g(t −s),
s ≤t
0,
t < s ≤t + x
1,
s > t + x
Hence we obtain the renewal equation
g(t) =
 t
0

g(t −s) dF(s) + 1 −F(t + x)
(6.10)
with solution
This yields the ﬁrst statement. The second one is obtained by using the key
renewal theorem to equation (6.10). This is applicable by condition (1) of
lemma 6.16 and leads to
lim
t→∞g(t) = 1
m
 ∞
0

(1 −F(t + x)) dt = 1
m
 ∞
x

(1 −F(y)) dy
and because of m =
+ ∞
0
+
(1 −F(y)) dy we obtain further
lim
t→∞P(Bt ≤x) = 1 −1
m
 ∞
x

(1 −F(y)) dy = 1
m
 x
0

(1 −F(y)) dy
□
Remark 6.19 For m = ∞, equation (6.9) states that the residual life time
asymptotically tends to inﬁnity with probability one.
Because of the equality {At > x} = {Bt−x > x}, an immediate application
of theorem 6.18 is
g(t) =
 t
0

(1 −F(t + x −y)) dR(y) + 1 −F(t + x)

126
AN INTRODUCTION TO QUEUEING THEORY
Corollary 6.20 Be N an ordinary renewal process with renewal intervals
having distribution function F. Then
P(At ≤x) =

F(t) −
+ t−x
0
+
(1 −F(t −y)) dR(y),
x < t
1,
x ≥t
If F is not lattice, then the limit
lim
t→∞P(At ≤x) = 1
m
 x
0

(1 −F(y)) dy
holds.
Remark 6.21 The above results show that the distributions of age and residual
life time asymptotically tend to be the same. For m = ∞the same phenom-
enon as for the residual life time happens: The age asymptotically tends to
inﬁnity with probability one.
Theorem 6.22 If F is not lattice and E(X2
1) < ∞, then the limit
lim
t→∞E(Bt) = E(X2
1)
2m
holds.
Proof: Deﬁne the functions g(t) = E(Bt) and h(t) := E

Bt · 1{X0>t}

for
all t ≥0. Then the renewal equation
g(t) = h(t) +
 t
0

g(t −x) dF(x)
holds. The function h is positive, not increasing, and integrable with
 ∞
0

h(t) dt =
 ∞
t

=0

 ∞
x

=t
(x −t) dF(x) dt
=
 ∞
x

=0
 x
t

=0

(x −t) dt dF(x) =
 ∞
x

=0
x2
2 dF(x)
= E(X2
1)
2
Thus the key renewal theorem applies (due to condition (1) of lemma 6.16) and
yields
lim
t→∞E(Bt) = 1
m
 ∞
0

h(t) dt = E(X2
1)
2m

Renewal Theory
127
which completes the proof.
□
For a stationary renewal process we would postulate that the distribution
of the counts in an interval [s, s + t] be independent of s and thus equal the
distribution of Nt
N . If this holds for a process N, then we also say that N has
stationary increments. This implies in particular that the distribution of the
residual life time must be independent of t, i.e. it coincides with the distribution
of B0 and hence of X0. Regarding the limit given in (6.9), we ﬁrst guess that
it satisﬁes
P(X0 ≤x) = 1
m
 x
0

(1 −F(y)) dy
(6.11)
for all x ≥0, where F denotes the distribution function of X1 and further
m = E(X1) < ∞. Indeed we can show
Theorem 6.23 For a renewal process N deﬁned by (6.11) the following prop-
erties hold:
(1) E(Nt
N ) = t/m
for all t ≥0
(2) P(Bt ≤x) = m−1 + x
0
+
(1 −F(y)) dy
for all t ≥0
(3) N has stationary increments.
Proof: (1) The distribution G of X0 has a density g(t) = 1
m(1 −F(t)) Hence
the Laplace–Stieltjes transform (LST) of G is
˜G(s) =
 ∞
0

e−st 1
m(1 −F(t)) dt = 1
m
 ∞
0

e−st dt −
 ∞
0

e−stF(t) dt

= 1
m
1
s −1
s
 ∞
0

e−st dF(t)

= 1 −˜F(s)
sm
with ˜F(s) denoting the LST of F. According to (6.1) we have the repre-
sentation E(Nt
N ) = G ∗∞
n=0 F ∗n(t) for all t ≥0.
Hence the LST of
M(t) := E(Nt
N ) is given by
˜
M(s) =
˜G(s)
1 −˜F(s)
=
1
sm
for all s > 0, and thus coincides with the LST of the measure dx/m. Since the
LST uniquely determines a function on [0, ∞[, this proves the ﬁrst statement.

128
AN INTRODUCTION TO QUEUEING THEORY
(2) The joint distributions
P(Bt > x, Nt
N = 0) = 1 −G(t + x)
P(Bt > x, Nt
N = n) =
 ∞
0

P(Bt > x, Nt
N = n|Sn
S = y) dG ∗F ∗n−1(y)
=
 t
0

(1 −F(t + x −y)) dG ∗F ∗n−1(y)
for n ≥1 are immediate from the deﬁnition. Abbreviating F c(x) := 1−F(x),
Gc(x) := 1 −G(x), and denoting M(t) := E(Nt
N ), we can write
P(Bt > x) =
∞

n=0
P(Bt > x, Nt
N = n)
= Gc(t + x) +
∞

n=1
 t
0

F c(t + x −y) dG ∗F ∗n−1(y)
= Gc(t + x) +
 t
0

F c(t + x −y) d
 ∞

n=1
G ∗F ∗n−1

(y)
= Gc(t + x) +
 t
0

F c(t + x −y) dM(y)
Using statement (1) and the deﬁnition of G, we obtain
P(Bt > x) = 1 −1
m
 t+x
0

(1 −F(y)) dy + 1
m
 t
0

(1 −F(t + x −y)) dy
= 1 −1
m
 x
0

(1 −F(y)) dy
which proves the second statement.
(3) The difference Nt
N +s −Ns
N simply counts the number N′
t
N of events in time
t of the renewal process N ′ with the same distribution F of X1 but a delay
X′
0 ∼Bs. Now statement (2) shows that X0 ∼Bs = B0. Hence we obtain
N′
t
N = Nt
N = Nt
N +s −Ns
N in distribution, which was to be proven.
□
Because of the results above a renewal process which satisﬁes condition (6.11)
is called stationary renewal process. As one would expect, also the mean
residual life time E(Bt) of a stationary renewal process coincides with the
limit of the mean residual life time of an ordinary renewal process:
Lemma 6.24 For a non–negative random variable X the nth moment can be
expressed by
E(Xn) =
 ∞
0

P(X > x) · nxn−1 dx

Renewal Theory
129
Proof: This follows simply by writing
E(Xn) =
 ∞
0

P(Xn > z) dz =
 ∞
0

P(X > √
n z) dz
and substituting x = √
n z with nxn−1 dx = dz.
□
Theorem 6.25 For a stationary renewal process with E(X2
1) < ∞the mean
residual life time is given by
E(Bt) = E(X2
1)
2m
independently of t ≥0.
Proof: Using part (2) of theorem 6.23, we obtain
E(Bt) =
 ∞
0

P(Bt > y) dy = 1
m
 ∞
y

=0
 ∞
x

=y
(1 −F(x)) dx dy
= 1
m
 ∞
x

=0
 x
y

=0
(1 −F(x)) dy dx = 1
m
 ∞
x

=0
P(X1 > x) · x dx
and the statement follows from lemma 6.24.
□
Example 6.26 Waiting time at a bus stop
Consider a bus stop where buses are scheduled to arrive in intervals of length
T. However, due to trafﬁc variations the real inter–arrival times are uniformly
distributed within intervals [T −a, T + a] with some a > 0. Now suppose that
somebody arrives at the bus stop ”at random”, i.e. without knowing the bus
schedule. Then we can model the mean waiting time for the next bus by the
mean residual life time E(Bt) in a stationary renewal process with distribution
X1 ∼U(T −a, T + a). We obtain
E(X2
1) = 1
2a
 T+a
T

−a
x2 dx = 1
2a · 1
3

6T 2a + 2a3
= T 2 + a2
3
and by theorem 6.25
E(Bt) = T 2 + a2
3
2 · T
= T
2 +
a2
6 · T
Thus the mean waiting time for random inter–arrival times (meaning a > 0) is
longer than it would be for deterministic ones (namely T/2). This phenomenon
is called the waiting time paradox.

130
AN INTRODUCTION TO QUEUEING THEORY
5.
Renewal Reward Processes
Consider an ordinary renewal process where for every renewal interval Xn
X
there is a real–valued random variable Yn
Y , called the nth reward, which may
depend on Xn
X . If the pairs (Xn
X , Yn
Y ), n ∈N0 are iid, then the two–dimensional
stochastic chain ((Xn
X , Yn
Y ) : n ∈N0) is called an ordinary renewal reward
process. The random variable
Y (t) =
Nt
N −1

n=0
Yn
Y
is called the total reward until time t.
Theorem 6.27 If E(|Y1
Y |) and m = E(X1) are ﬁnite, then
lim
t→∞
Y (t)
t
= E(Y1
Y )
m
holds with probability one. If there is further a constant c ∈R with Y1
Y
> c
almost certainly, then
lim
t→∞
E(Y (t))
t
= E(Y1
Y )
m
Proof: The ﬁrst statement follows from
Y (t)
t
=
Nt
N −1
n=0 Yn
Y
Nt
N
· Nt
N
t
as the ﬁrst factor tends to E(Y1
Y ) by the strong law of large numbers and the
second tends to 1/m according to theorem 6.4.
For the second statement, we can assume without loss of generality that Y1
Y is
positive almost certainly, since otherwise we consider Zn
Z := Yn
Y +c instead. Nt
N
is a stopping time for the sequence (Yn
Y : n ∈N0), as {Nt
N ≤n} is independent
of (Xn
X +k : k ∈N) and thus independent of (Yn
Y +k : k ∈N). Hence we can
apply Wald’s lemma, which yields
and thus
E(Y (t))
t
= R(t)
t
· E(Y1
Y ) −E (YN
Y
t
N )
t
E
Nt
N −1

n=0
Yn
Y

= E
 Nt
N

n=0
Yn
Y

−E (YN
Y
t
N ) = R(t) · E(Y1
Y ) −E (YN
Y
t
N )

Renewal Theory
131
for all t > 0. Because of limt→∞R(t)/t = 1/m it now remains to show that
limt→∞E(YN
Y
t
N )/t = 0. To this aim we condition on X0 to obtain
g(t) := E (YN
Y
t
N ) =
 ∞
0

E (YN
Y
t
N |X0 = u) dF(u)
=
 t
0

E (YN
Y
t
N |X0 = u) dF(u) +
 ∞
t

E (YN
Y
t
N |X0 = u) dF(u)
for all t > 0. Abbreviating the latter integral by h(t) and recognizing that
E (YN
Y
t
N |X0 = u) = g(t −u), we obtain the renewal equation
g(t) =
 t
0

g(t −u) dF(u) + h(t)
Theorem 6.9 yields the unique solution
g(t) = h(t) +
 t
0

h(t −u) dR(u)
for all t > 0. As X0 > t implies Nt
N = 0, we know further that
h(t) =
 ∞
t

E (Y0
Y |X0 = u) dF(u) ≤E(Y0
Y ) < ∞
and h(t) →0 as t →∞. This means that for any given ε > 0 there is a T > 0
such that |h(t)| < ε for all t ≥T. Using this we obtain
|g(t)|
t
≤|h(t)|
t
+ 1
t
 t−T
0

|h(t −u)| dR(u) + 1
t
 t
t

−

T
|h(t −u)| dR(u)
≤ε
t + ε · R(t −T)
t
+ E(Y0
Y ) · R(t) −R(t −T)
t
for all t > T. For t →∞the right–hand side tends to ε/m by the elementary
renewal theorem, as R(t) −R(t −T) is bounded by R(T) + 1 (see the proof
of the key renewal theorem). This completes the proof, as ε can be chosen
arbitrarily small.
□
Example 6.28 Machine maintenance
Consider a machine that is prone to failure and may be either repaired or re-
placed by a new machine. Let Xn
X denote the run time of the machine after the
n −1st failure and assume λ := E(X1) < ∞. Since the state of the machine
after the nth repair is usually worse than after the n −1st repair, we model this
by the assumption that (an−1Xn
X : n ∈N) with a ≥1 forms a renewal process.
In particular, the Xn
X , n ∈N are independent random variables. The sequence

132
AN INTRODUCTION TO QUEUEING THEORY
(Xn
X : n ∈N) is called a non–increasing geometric process with parameter a.
The reward rate for the machine running is r = 1.
The duration of the nth repair is denoted by Yn
Y , n ∈N, with the assumption
µ := E(Y1
Y ) < ∞. As the machine becomes more and more difﬁcult to repair,
we assume that (bn−1Yn
Y
: n ∈N) with b ≤1 forms a renewal process. The
sequence (Yn
Y
: n ∈N) is called a non–decreasing geometric process with
parameter b. Again this implies that the Yn
Y , n ∈N are independent random
variables. Furthermore we assume that {Xn
X , Yn
Y
: n ∈N} is an independent
set of random variables. The cost (i.e. the negative reward) rate for the repair
of the machine is denoted by c1 > 0.
Instead of repairing the machine after a failure, we can choose to replace it by
a new machine. This incurs a ﬁxed cost c2 > c1. Given this information, we
want to determine the long–run expected reward per unit time for the machine.
This depends on the variable N ∈N which indicates the policy that a machine
is replaced after the Nth failure.
Clearly the replacement times (Tn
T
: n ∈N0) with T0
T := 0 form an ordinary
renewal process and the reward of a machine (i.e. between replacement times)
is independent from the rewards and life times of other machines. Denote the
life time and the reward of the nth machine by Ln := Tn
T
−Tn
T −1 and Rn,
respectively. Then ((Ln, Rn) : n ∈N) is a renewal reward process and the
long–run expected reward per unit time is given by
R(N) = E(R1)
E(L1) = λ N
k=1 a−(k−1) −c1 · µ N−1
k=1 b−(k−1) −c2
λ N
k=1 a−(k−1) + µ N−1
k=1 b−(k−1)
according to theorem 6.27. In order to ﬁnd the optimal replacement policy,
this equation can now be used to determine the value N which maximizes the
expected reward rate R(N).
Notes
A classical presentation of renewal theory is chapter 11 in Feller [35]. The
presentation in this chapter is largely adapted to Ross [74, 75] as well as Karlin
and Taylor [46]. The concept of regenerative processes has been developed by
Feller and Smith [80, 81]. Example 6.28 is taken from Lam Yeh [51].
Exercise 6.1 Prove theorem 6.3.
Exercise 6.2 In the proof for Wald’s lemma 6.11 we have used the relation
E(S) = ∞
n=0 P(S > n), and in theorem 6.18 E(F) =
+ ∞
0
+
(1 −F(y)) dy.
Give a proof for these equations.

Renewal Theory
133
Exercise 6.3 Show for a Poisson process N with intensity λ > 0 that
P(Nt
N = k) = (λt)k
k! e−λt
for all t ≥0 and k ∈N0, and E(Nt
N ) = λ · t.
Exercise 6.4 A plumber receives orders at time intervals which are distributed
exponentially with parameter λ. As soon as he has received an order he goes to
work, which takes an exponentially distributed time with parameter µ. During
work he cannot receive any orders. Assume that at time zero the plumber is
working. Give a model of the plumber receiving orders in terms of a renewal
process and determine the density of the renewal intervals’ distribution.
Exercise 6.5 An intelligence agency eavesdrops on telephone calls automati-
cally. If there occurs a suspicious sequence of words, a closer investigation is
initiated. The probabilitiy for such a sequence is one in a thousand for every
call. The length of a call is distributed exponentially with a mean of 20 sec-
onds. How long is the expected amount of time before a closer investigation
begins? Use Wald’s lemma.
Exercise 6.6 Let N = (Nt
N : t ≥0) denote an ordinary renewal process with
X1 ∼F. Show that the current life time XNt
N satisﬁes
P(XNt
N > x) ≥1 −F(x)
for all x ≥0.
Exercise 6.7 Give an example which shows why we need to assume in Black-
well’s theorem that the distribution of the renewal intervals is not lattice.
Exercise 6.8 Prove lemma 6.16.
Exercise 6.9 Show that the age At of a stationary renewal process is distrib-
uted as
P(At ≤x) = 1
m
 x
0

(1 −F(y)) dy
independently of t ≥0.
Exercise 6.10 Show that for an ordinary renewal process with E(X2
1) < ∞
and m := E(X1) the limit
lim
t→∞E(At) = E(X2
1)
2m

134
AN INTRODUCTION TO QUEUEING THEORY
holds.
Exercise 6.11 Passengers arrive at a train station according to an ordinary re-
newal process with rate 1/m. As soon as there are N passengers waiting, the
train departs. The cost for the ticket per passenger is C. Assume that the rail-
way company reimburses every passenger for the waiting time by an amount of
W per time unit that the passenger had to wait. Determine the minimal value
for C such that this train connection will be proﬁtable in the long run.
Exercise 6.12 A delayed renewal reward process is deﬁned as a stochastic
chain ((Xn
X , Yn
Y ) : n ∈N0) for which ((Xn
X , Yn
Y ) : n ∈N) is an ordinary
renewal reward process and X0 ≥0. The pair (X0, Y0
Y ) may have a different
distribution than (X1, Y1
Y ). Prove the statement of theorem 6.27 for a delayed
renewal reward process that satisﬁes E(X0) < ∞and E(|Y0
Y |) < ∞.

Chapter 7
MARKOV RENEWAL THEORY
1.
Regenerative Processes
Let Y = (Yt
Y : t ≥0) denote a stochastic process on a discrete state space
E with right–continuous paths. Further let T denote a random variable with
values in [0, ∞] such that the condition
P(T ≤t|Y) = P(T ≤t|Ys
Y : s ≤t)
(7.1)
holds for all t ∈R+
0 . Such a random variable is called a (continuous) stopping
time for the process Y. As in the analogue for discrete time, the deﬁning
condition means that the probability for the event {T ≤t} depends only on
the evolution of the process until Yt
Y . In other words, the determination of a
stopping time does not require any knowledge of the future.
If there is a sequence T = (Tn
T : n ∈N0) of stopping times for Y with T0
T := 0
and Tn
T +1 > Tn
T for all n ∈N0 such that T deﬁnes an ordinary renewal process,
and if further
P(YT
Y
n
T +t1 = j1, . . . , YT
Y
n
T +tk = jk|Yu
Y : u ≤Tn
T ) = P(Yt
Y 1 = j1, . . . , Yt
Y k = jk)
for all k ∈N, t1, . . . , tk ≥0 and n ∈N0 holds, then Y is called a regenerative
process. The Tn
T
are called regeneration times and the deﬁning condition
above is called regeneration property. The interval [Tn
T −1, Tn
T [ is called the
nth regeneration cycle.
Example 7.1 M/G/k Queue
The M/G/k queue has a Poisson arrival process and k servers with general
service time distribution. Whenever the queue is empty, all servers are idle and

136
AN INTRODUCTION TO QUEUEING THEORY
only the arrival process has an effect on the future. Thus the system process
regenerates at the points Tn
T of the system becoming idle for the nth time. The
durations Tn
T +1 −Tn
T
between these points are iid. Hence the M/G/k system
process is a regenerative process.
Theorem 7.2 If T1
T is not lattice and E(T1
T ) = m < ∞holds and if the func-
tion Kj
K (t) := P(T1
T > t, Yt
Y = j) is Riemann integrable, then
πj
π := lim
t→∞P(Yt
Y = j) = 1
m
 ∞
0

Kj
K (t) dt
for all j ∈E.
Proof: Let F denote the distribution function of T1
T . By conditioning on the
ﬁrst regeneration time T1
T , we obtain the equation
P(Yt
Y = j) = P(T1
T > t, Yt
Y = j) +
 t
0

P(Yt
Y = j|T1
T = s) dF(s)
= P(T1
T > t, Yt
Y = j) +
 t
0

P(Yt
Y −s = j) dF(s)
where the second equality is due to the regeneration property. The function
Kj
K (t) = P(T1
T > t, Yt
Y = j) is non–negative and bounded by P(T1
T > t), which
in turn is Lebesgue integrable and non–increasing. By assumption Kj
K (t) is
Riemann integrable. Hence lemma 6.16 yields that Kj
K (t) is directly Riemann
integrable. Thus the key renewal theorem 6.17 applies and yields the statement.
□
Introduce a real–valued function f : E →R on the state space of the process
Y. The value f(i) can be interpreted as a reward rate which is incurred in state
i ∈E.
Theorem 7.3 If E(T1
T ) < ∞and f is bounded, then
lim
t→∞
1
t
 t
0

f(Ys
Y ) ds = E
+ T1
T
0
+
f(Yt
Y ) dt
E(T1
T )
holds with probability one. If further E(T 2
1
T ) < ∞, then
lim
t→∞E
1
t
 t
0

f(Ys
Y ) ds

= E
+ T1
T
0
+
f(Yt
Y ) dt
E(T1
T )
Proof: Deﬁne Xn
X
:= Tn
T +1 −Tn
T
and Zn
Z
:=
+ Tn
T +1
T
+
n
T
f(Ys
Y ) ds for all n ∈N0.
Since Y is regenerative, the chain ((Xn
X , Zn
Z ) : n ∈N0) is a renewal reward

Markov Renewal Theory
137
process, with Nt
N := max{n ∈N0 : Tn
T
≤t} and Z(t) := Nt
N −1
n=0 Zn
Z deﬁned
as usual. We can write
 t
0

f(Ys
Y ) ds = Z(t) +
 t
T

N
T
t
f(Ys
Y ) ds
(7.2)
for all t ≥0. Since Z(t)/t converges to the fraction on the right–hand side
of the statement (see theorem 6.27), it remains to show that t−1 + t
T
+
N
T
t f(Ys
Y ) ds
tends to zero as t →∞. We obtain
1
t
 t
T

N
T
t
f(Ys
Y ) ds = 1
Nt
N
 t
T

N
T
t
f(Ys
Y ) ds · Nt
N
t ≤XNt
N
Nt
N
· sup
i∈E
|f(i)| · Nt
N
t
→lim
n→∞
Xn
X
n · sup
i∈E
|f(i)| · lim
t→∞
Nt
N
t
as t →∞. According to the strong law of large numbers, we know that
n
k=1 Xk/n →m < ∞almost certainly and hence Xn
X /n →0 as n →∞.
This and theorem 6.4 complete the proof for the ﬁrst statement.
The same partition (7.2) and theorem 6.27 show that for the second statement
it remains to show that t−1E
 + t
T
+
N
T
t f(Ys
Y ) ds
!
→0 as t →∞. However, this
follows from
 t
T

N
T
t
f(Ys
Y ) ds ≤At · sup
i∈E
|f(i)|
and limt→∞E(At) = E(X2
1)/(2m) < ∞by exercise 6.10 and the assumption
that E(T 2
1
T ) be ﬁnite.
□
Theorem 7.4 If T1
T is not lattice and E(T1
T ) as well as E
,,,
,,+ T1
T
0
+
f(Yt
Y ) dt
,,,
,, are
ﬁnite, then
E
+ T1
T
0
+
f(Yt
Y ) dt
E(T1
T )
=

j∈E
πj
π · f(j)
with πj
π as deﬁned in theorem 7.2. If T1
T is not lattice and E(T1
T ) < ∞, then
πj
π = lim
t→∞
1
t
 t
0

1{Ys
Y =j} ds
holds with probability one for all j ∈E. This means that the limiting proba-
bility πj
π of j equals the asymptotic proportion of time spent in state j for every
path.

138
AN INTRODUCTION TO QUEUEING THEORY
Proof: The Lebesgue construction of an integral yields
 T1
T
0

f(Yt
Y ) dt =

j∈E
f(j) ·
 T1
T
0

1{Yt
Y =j} dt
and after taking expectations we obtain
E
 T1
T
0

f(Yt
Y ) dt =

j∈E
f(j) ·
 ∞
0

Kj
K (t) dt
with Kj
K (t) = P(T1
T
> t, Yt
Y
= j). Now the ﬁrst statement follows from
theorem 7.2. The second statement follows from the ﬁrst one and theorem 7.3
for f(Yt
Y ) := 1{Yt
Y =j}.
□
2.
Semi–Markov Processes
In this section we will introduce a special class of regenerative processes which
is very useful for the analysis of many queueing systems.
Let E denote a countable state space. For every n ∈N0, let Xn
X
denote a
random variable on E and Tn
T
a random variable on R+
0 such that T0
T
:= 0,
Tn
T
< Tn
T +1 for all n ∈N0, and supn→∞Tn
T
= ∞almost surely. Deﬁne the
process Y = (Yt
Y : t ∈R+
0 ) by
Yt
Y := Xn
X
for
Tn
T ≤t < Tn
T +1
for all t ≥0. If
P(Xn
X +1 = j, Tn
T +1 −Tn
T ≤u|X0, . . . , Xn
X , T0
T , . . . , Tn
T )
= P(Xn
X +1 = j, Tn
T +1 −Tn
T ≤u|Xn
X )
(7.3)
holds for all n ∈N0, j ∈E, and u ∈R+
0 , then Y is called a semi–Markov
process on E. The sequence (X, T ) = ((Xn
X , Tn
T ) : n ∈N0) of random
variables is called the embedded Markov renewal chain. We will treat only
homogeneous semi–Markov processes, i.e. those for which
Fij
F (t) := P(Xn
X +1 = j, Tn
T +1 −Tn
T ≤t|Xn
X = i)
is independent of n. For all i, j ∈E, the functions t →Fij
F (t) are assumed
non–lattice.
By deﬁnition a semi–Markov process is a pure jump process. Thus the sample
paths are step functions:

Markov Renewal Theory
139
Figure 7.1.
Typical path of a semi–Markov process
By construction, the semi–Markov process Y is determined by the embedded
Markov renewal chain (X, T ) and vice versa.
Remark 7.5 Let Y denote an homogeneous Markov process with discrete
state space E and parameters λi, i ∈E, for the exponential holding times. The
embedded Markov chain X of Y shall have transition matrix P = (pij)i,j∈E.
Then Y is a semi–Markov process with
Fij
F (t) = pij ·
 
1 −e−λi·t!
for all i, j ∈E. Thus for a Markov process the distribution of Tn
T +1 −Tn
T
is exponential and independent of the state entered at time Tn
T +1. These are
the two features for which the semi–Markov process is a generalization of the
Markov process on a discrete state space.
Theorem 7.6 Let Y be a semi–Markov process with embedded Markov re-
newal chain (X, T ). Then X = (Xn
X : n ∈N0) is a Markov chain.
Proof: From the condition (7.3) we obtain for every n ∈N0
P(Xn
X +1 = j|X0, . . . , Xn
X ) = P(Xn
X +1 = j, Tn
T +1 −Tn
T < ∞|X0, . . . , Xn
X )
= P(Xn
X +1 = j, Tn
T +1 −Tn
T < ∞|Xn
X )
= P(Xn
X +1 = j|Xn
X )
since all Tn
T are ﬁnite by deﬁnition.
□
We denote the transition matrix of X by P = (pij)i,j∈E. Then the relation
pij := P(Xn
X +1 = j|Xn
X = i) = lim
t→∞Fij
F (t)

140
AN INTRODUCTION TO QUEUEING THEORY
obviously holds for all i, j ∈E. This means in particular that the functions
Fij
F (t) are distinct from distribution functions in the feature that the total mass
distributed by them may be less than one. Therefore they shall be called sub–
stochastic distribution functions.
According to its embedded Markov chain X, we call a semi–Markov process
irreducible, recurrent or transient. Clearly, an irreducible recurrent semi–
Markov process is regenerative, as one can ﬁx any initial state i ∈E and ﬁnd
the times of visiting this state to be a renewal process.
Deﬁne Gij(t) := Fij
F (t)/pij for all t ≥0 and i, j ∈E if pij > 0, while
Gij(t) := 0 otherwise. The deﬁnitions of P and F yield the interpretation
Gij(t) = P(Tn
T +1 −Tn
T ≤t|Xn
X = i, Xn
X +1 = j)
which in turn yields
Theorem 7.7 Let Y denote a semi–Markov process with state space E and
embedded Markov renewal chain (X, T ). Then
P(T1
T −T0
T ≤u1, . . . , Tn
T −Tn
T −1 ≤un|X0, . . . , Xn
X )
= GX0,X1(u1) . . . GXn−1,Xn(un)
for all n ∈N, meaning that the increments T1
T −T0
T , . . . , Tn
T −Tn
T −1 are con-
ditionally independent, given the values X0, . . . , Xn
X .
Remark 7.8 If the state space E is trivial, i.e. consisting of only one element,
then these increments are even iid. In this case, T
= (Tn
T
: n ∈N0) is
a renewal process. This property and theorem 7.6 justify the name Markov
renewal theory for the study of semi–Markov processes, as the latter generalize
Markov processes and renewal processes at the same time.
2.1
Transient distributions
For Markov chains and Markov processes we have been able to give formulae
for the transition matrices. Because of the Markov property, this in turn de-
termined all ﬁnite–dimensional marginal distributions and thus the complete
distribution of the process. In the case of a semi–Markov process, we can-
not give as much information. However, what we can derive are the transition
probabilities starting from a regeneration time. Since T0
T
:= 0 is a determin-
istic regeneration point, this yields together with a given initial distribution
the one–dimensional marginal distributions at any given time. These shall be
called transient distributions.
In order to determine the transient distributions of a semi–Markov process, we
will view the collection F = (Fij
F )i,j∈E of sub–stochastic distribution func-

Markov Renewal Theory
141
tions as a matrix with entries being functions instead of numbers. This matrix
contains all stochastic laws for the construction of the semi–Markov process
Y. Therefore it shall be called the characterizing matrix of Y.
We deﬁne a matrix convolution of two such matrices F and G by the entries
(F ∗G)ij(t) :=

k∈E
 t
0

Gkj(t −u) dFik
F (u)
for all i, j ∈E and t ≥0. Based on this deﬁnition, we deﬁne the matrix
convolutional powers by F ∗0 := IE
I , denoting the identity matrix on E, and by
recursion F ∗n+1 = F ∗n ∗F. Now we can state the following formula for the
transient distributions of a semi–Markov process:
Theorem 7.9 Let Y denote a semi–Markov process with characterizing matrix
F, and π any initial distribution of Y. Then the transient distribution of Yπ at
any time t is given by P(Y π
t
Y
= j) = 
i∈E πiPij
P (t) with
Pij
P (t) =
∞

n=0
 t
0


1 −

k∈E
Fjk
F
(t −u)

dF ∗n
ij
F
(u)
Proof: This expression is obtained by conditioning on the number n of renewal
intervals that have passed until time t.
□
2.2
Asymptotic behaviour
Next we want to examine the asymptotic behaviour of the transient distribu-
tions, i.e. we want to determine the limits limt→∞P(Yt
Y = j) for all j ∈E.
This will be achieved by specifying the results which have already been ob-
tained for regenerative processes.
If we want to use theorem 7.2, then we need the information for the respective
regenerative process with embedded renewals being the visits to any ﬁxed state
j ∈E. Deﬁne
mij := E(T1
T · 1X1=j|X0 = i) =
 ∞
0

t dFij
F (t)
mi := E(T1
T |X0 = i) =

j∈E
mij =
 ∞
0


1 −

k∈E
Fik
F (t)

dt
for all i, j ∈E. Further deﬁne
τjτ := min{Tn
T : Xn
X = j, n ∈N}
and
µij := E(τjτ |X0 = i)
(7.4)

142
AN INTRODUCTION TO QUEUEING THEORY
for all i, j ∈E. The random variable τjτ is called ﬁrst return time to state j.
Now consider any Markov renewal time Tn
T . If the Markov chain X is irre-
ducible and positive recurrent with stationary distribution ν = (νiν : i ∈E),
then we would expect a proportion νj
ν of sample paths with transition into j
at time Tn
T . Furthermore, mj represents the mean time spent in state j until
the next transition happens at time Tn
T +1. Therefore, if there is an asymptotic
distribution πj
π = limt→∞P(Yt
Y = j), we would expect it to be proportional to
νj
ν · mj, i.e.
πj
π =
νj
ν · mj

i∈E νiν · mi
We will prove this by examining the above mentioned embedded regenerative
process of visits to state j.
Lemma 7.10 The relation
µij = mi +

k̸=
̸
j
pikµkj
holds for all i, j ∈E.
Proof: Conditioning on the state X1 = k at time T1
T , we can write
µij =

k∈E
E(τjτ · 1X1=k|X0 = i) =

k̸=
̸
j
(pikµkj + mik) + mij
=

k∈E
mik +

k̸=
̸
j
pik · µkj = mi +

k̸=
̸
j
pikµkj
which is the statement.
□
Lemma 7.11 Let Y denote a semi–Markov process with embedded Markov
renewal chain (X, T ). Assume that X is positive recurrent and denote its
stationary distribution by ν = νP. Further assume that 
i∈E νiν mi < ∞.
Then the mean recurrence time of a state j ∈E can be expressed by
µjj = E(τjτ |X0 = j) = 1
νj
ν

i∈E
νiν mi
for all j ∈E.

Markov Renewal Theory
143
Proof: We multiply both sides of lemma 7.10 by νiν and sum up over all i ∈E.
Then we obtain

i∈E
νiν µij =

i∈E
νiν mi +

i∈E
νiν

k̸=
̸
j
pikµkj =

i∈E
νiν mi +

k̸=
̸
j
µkj

i∈E
νiν pik
=

i∈E
νiν mi +

k̸=
̸
j
νkµkj
which implies
νj
ν µjj =

i∈E
νiν mi
and thus the statement.
□
Theorem 7.12 Let Y denote a semi–Markov process with embedded Markov
renewal chain (X, T ) and characterizing matrix F. Assume that X is irre-
ducible and positive recurrent and ν = νP is the stationary distribution of its
transition matrix P. Further assume that 
i∈E νiν mi < ∞. Then the limits
πj
π := lim
t→∞P(Yt
Y = j) =
νj
ν mj

i∈E νiν mi
hold for all j ∈E, independent of the initial distribution.
Proof: Since the times of successive visits to state j form a (possibly delayed)
renewal process, the process Y is regenerative. Since all functions Fij
F
are
assumed non–lattice, the regeneration cycles of Y are not lattice either. Thus
we can apply theorem 7.2 (in the form of exercise 7.1), which yields
lim
t→∞P(Yt
Y = j) = mj
µjj
Now lemma 7.11 leads to the statement.
□
Example 7.13 This limit theorem ﬁnally sufﬁces for an application to Markov
processes. The two limit theorems (3.6) and (3.7) follow from the interpreta-
tion of a Markov process as a special semi–Markov process. For a Markov
process, the mean holding times in a state i are given by
mi =
 ∞
0

P(T1
T > t|Y0
Y = i) dt =
 ∞
0

e−λi·t dt = 1
λi

144
AN INTRODUCTION TO QUEUEING THEORY
for all i ∈E. Since λi ≥ˇλ > 0 for all i ∈E, we know that 
i∈E νiν mi < ∞.
Hence we obtain
lim
t→∞Pij
P (t) = lim
t→∞P(Yt
Y = j) =
νj
ν /λj

i∈E νiν /λi
as given in equation (3.5).
3.
Semi–regenerative Processes
Semi–Markov processes play a similar role for the analysis of a more general
class of processes that renewal processes have played for the analysis of regen-
erative processes. These more general processes are called semi–regenerative
and shall be deﬁned as follows:
Let Z = (Zt
Z : t ∈R+
0 ) denote a stochastic process with countable state space
E. Then Z is called a semi–regenerative process if there is an embedded
Markov renewal chain (X, T ) such that all Tn
T
are stopping times for Z, all
Xn
X are deterministic functions of (Zu
Z : u ≤Tn
T ), and
P(ZT
Z
n
T +t1 = j1, . . . , ZT
Z
n
T +tk = jk|Zu
Z : u ≤Tn
T , Xn
X = i)
= P(Zt
Z 1 = j1, . . . , Zt
Z k = jk|X0 = i)
(7.5)
holds for all n, k ∈N, i, j1, . . . , jk ∈E, and t1 < . . . < tk ∈R+
0 . This
condition postulates that for any prediction of the process (Zu
Z
: u ≥Tn
T )
all information of the past (Zu
Z
: u ≤Tn
T ) is contained in the state Xn
X . We
abbreviate
Kij
K (t) := P(T1
T > t, Zt
Z = j|X0 = i)
for all i, j ∈E and t ∈R+
0 .
Theorem 7.14 The transient distributions of a semi–regenerative process Z
with embedded Markov renewal sequence (X, T ) and initial distribution π are
given by P(Zt
Z = j) = 
i∈E πiPij
P (t) with
Pij
P (t) =
∞

n=0

k∈E
 t
0

Kkj(t −u) dF ∗n
ik
F
(u)
for all t > 0 and i, j ∈E.
Proof: This expression is obtained by conditioning upon the number n of
Markov renewal points until time t and the state k which is observed at the
last Markov renewal point before t.
□

Markov Renewal Theory
145
The following limit theorem is the main result of this chapter and will be ap-
plied to many classical queueing systems later on. We deﬁne the column vector
m = (mi : i ∈E) with
mi := E(T1
T |X0 = i)
for all i ∈E, and abbreviate νm := 
i∈E νiν mi.
Theorem 7.15 Let Z denote a semi–regenerative process with irreducible and
positive recurrent embedded Markov chain X. Denote the stationary distribu-
tion of X by ν and assume that νm < ∞. Then the limit
lim
t→∞P(Zt
Z = j) =
1
νm

k∈E
νk
 ∞
0

K
(t) dt
Proof: Since X is positive recurrent and νm < ∞, the process Z is regener-
ative with regeneration times being the transition times to some state i ∈E.
Then theorem 7.2 and lemma 7.11 yield
lim
t→∞P(Zt
Z = j) = 1
µii
 ∞
0

P(Zt
Z = j, τiτ > t|X0 = i) dt
= νiν
νm · Ei
 τi
τ
0

1Zt=j dt

with τiτ deﬁned as in (7.4) and Ei denoting the conditional expectation given
X0 = i. Deﬁning the stopping time σi = min{n ∈N : Xn
X = i}, we can write
τiτ = σi
n=1(Tn
T −Tn
T −1). The semi–regenerative property (7.5) yields
for all n ∈N and k ∈E. Hence we can write
lim
t→∞P(Zt
Z = j) = νiν
νm · Ei
 σi

n=1
EXn−1
 T1
T
0

1Zt=j dt

= νiν
νm · Ei
 σi

n=1

k∈E
1Xn−1=k · Ek
 T1
T
0

1Zt=j dt

= νiν
νm ·

k∈E
Ei
 σi

n=1
1Xn−1=k

· Ek
 T1
T
0

1Zt=j dt

kj
holds for all j ∈E and is independent of the initial distribution.
Ei
 Tn
T
T

n
T −1
1Zt=j dt
,,,
,,
,,
,, Zu
Z : u ≤Tn
T −1, Xn
X −1 = k

= Ek
 T1
T
0

1Zt=j dt


146
AN INTRODUCTION TO QUEUEING THEORY
By theorems 2.24 and 2.27 we get
Ei
 σi

n=1
1Xn−1=k

= νk
νiν
whence the statement follows.
□
Notes
Early papers on Markov renewal theory go back to Pyke [69, 70]. Classi-
cal textbooks on semi–Markov processes are Ross [74, 75] and C¸ inlar [25],
C
the latter containing further an extensive presentation on semi–regenerative
processes. The proof for theorem 7.15 is due to Asmussen [5]. For more ad-
vanced material on regenerative processes see Kalashnikov [44].
Exercise 7.1 A regenerative process is called a delayed regenerative process
if the respective sequence T = (Tn
T
: n ∈N) of stopping times is a delayed
renewal process. Prove theorem 7.2 for delayed regenerative processes.
Exercise 7.2 Prove theorem 7.7.
Exercise 7.3 Consider a machine that switches states between ”on” and ”off”.
First it is switched on for an amount X1 of time, then it is off for an amount
Y1
Y
of time, followed by an amount X2 of time switched on, and so forth.
Assume that the sequences (Xn
X
: n ∈N) and (Yn
Y
: n ∈N) are both iid
with distribution functions F and G for X1 and Y1
Y , respectively. Give a model
for the state of the machine in terms of a semi–Markov process and show that
for F ∗G being not lattice and E(X1 + Y1
Y ) < ∞the long–run fraction πon of
time that the machine is switched on can be expressed as
πon =
E(X1)
E(X1) + E(Y1
Y )
Such a process is called an alternating renewal process.
Exercise 7.4 For a positive recurrent Markov process with discrete state space
E, derive an expression for the mean recurrence time to a state i ∈E.

Chapter 8
SEMI–MARKOVIAN QUEUES
The term semi–Markovian queues signiﬁes the class of queues that can be
analyzed by means of an embedded semi–Markov process, i.e. by modelling
the system process of the queue as a semi–regenerative process.
1.
The GI/M/1 Queue
The ﬁrst queue that shall serve as an example for an application of the semi–
Markovian method is the GI/M/1 queue. This has an arrival stream which is
a renewal process, i.e. the inter–arrival times are iid with some common dis-
tribution function A. There is one single server with exponential service time
distribution. Its intensity, i.e. the parameter of the exponential distribution,
shall be denoted by µ > 0. The service displine is FCFS and the capacity of
the waiting room is unbounded.
The system process Q = (Qt : t ∈R+
0 ) has state space E = N0, with Qt
indicating the number of users in the system (i.e. in service or waiting) at time
t. It can be modelled by a Markov process only for the case of exponential
inter–arrival times, i.e. for A(t) = 1 −e−λt with some λ > 0, since only the
exponential distribution is memoryless (see section 4). For general distribution
functions A, we need to ﬁnd another method of analysis.
One feature that clearly distinguishes this particular queueing system GI/M/1 is
the independence of the arrival process from the rest of the system, which leads
immediately to the construction of an embedded Markov renewal sequence at
times of arrivals. This is possible since at times of arrival we know that the
new inter–arrival time has just begun and because of the memoryless service
time distribution we do not need to remember anything else than the number

148
AN INTRODUCTION TO QUEUEING THEORY
of users in the system. Thus an analysis of the system as a semi–regenerative
process seems appropriate. That this is indeed successful will be shown in the
following.
Deﬁne Tn
T
as the time of the nth arrival and Xn
X
:= QTn
T −1 as the number
of users in the system immediately before the nth arrival. Clearly, the Tn
T
are
stopping times and Xn
X
is a deterministic function of QTn
T . Assume that A is
not lattice. Further we postulate A(0) = 0 and E(A) < ∞. This implies in
particular that Tn
T
→∞almost surely as n tends to inﬁnity. The sequence
X = (Xn
X
: n ∈N0) is a Markov chain since at times of arrivals the future
of the system is determined only by the current number of users in the system,
due to the memoryless property of the service times. The same property of the
queue ensures the validity of equation (7.5) for the system process Q.
Thus the system process Q is semi–regenerative with embedded Markov re-
newal chain (X, T ), denoting T = (Tn
T
: n ∈N0) with T0
T
:= 0. For the
characterizing matrix F of (X, T ) we obtain
Fij
F (x) = P(Tn
T +1 −Tn
T ≤x, Xn
X +1 = j|Xn
X = i)
=
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
+ x
0
+
e−µt (µt)i+1−j
(i+1−j)! dA(t),
1 ≤j ≤i + 1
1 −i
k=0 Fik
F (x),
j = 0
0,
j > i + 1
for all i, j ∈E = N0. The transition probability matrix P = (pij)i,j∈N0 of X
is given by its entries
pij =
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
ai+1−j :=
+ ∞
0
+
e−µt (µt)i+1−j
(i+1−j)! dA(t),
1 ≤j ≤i + 1
bi := 1 −i
n=0 an,
j = 0
0,
j > i + 1
for all i, j ∈N0. Here the ﬁrst line describes the case that within an inter–
arrival time exactly i + 1 −j users are served such that immediately before the
next arrival there are j users in the system, given that i users have been in the
system immediately before the last arrival. The third line states that after one
inter–arrival period there can only be an increase by one user in the system.
The second line distributes the remaining probability mass to the only possible
case left.
Clearly, bn = ∞
k=n+1 ak holds for all n ∈N0. With the abbreviations an and
bn, the matrix P is strucured as
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
b0
a0
0
0
0
. . .
b1
a1
a0
0
0
. . .
b2
a2
a1
a0
0
. . .
...
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
(8.1)

Semi–Markovian Queues
149
Such a matrix is called upper Hessenberg matrix or skip–free to the right.
It is characterized by the feature that above the ﬁrst diagonal on top of the main
diagonal the matrices contain only zeros.
The function K(t) describing the behaviour of the system process between
Mar-kov renewal points is given by Kij
K (t) = P(T1
T
> t, Qt = j|X0 = i)
for i, j ∈N0. Exploiting the independence of arrival process and service we
obtain
Kij
K (t) =
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
(1 −A(t)) · e−µt (µt)i+1−j
(i+1−j)! ,
1 ≤j ≤i + 1
(1 −A(t)) · e−µt ∞
n=i+1
(µt)n
n! ,
j = 0
0,
j > i + 1
(8.2)
for all t > 0, and i, j ∈N0. The transient distributions of the system process
can now be determined according to theorem 7.14.
In order to employ theorem 7.15 for the calculation of the asymptotic distribu-
tion of the system process, we need to determine the stationary distribution ν
of X, and the vector m of the mean time between Markov renewal points. The
vector m is obtained in a straightforward manner as
mi = E(T1
T |X0 = i) = E(A)
(8.3)
independently of i ∈N0, since the arrival process does not depend on the
number of users in the system. Thus the vector m is constant.
The most difﬁcult part is to determine the stationary distribution ν of the
Markov chain X. Since a0 > 0 and bn > 0 for all n ∈N0, the transition
matrix P is clearly irreducible. Hence there is at most one stationary distribu-
tion of X. The stationary distribution ν of X can be determined by solving the
following system of equations:
ν0 =
∞

n=0
νn
ν bn
and
νk =
∞

n=k−1
νn
ν a
(8.4)
With a geometric approach, i.e. assuming νn
ν = (1 −ξ) · ξn for all n ∈N0 and
some 0 < ξ < 1, the equations for k ≥1 can be transformed to
(1 −ξ)ξk = (1 −ξ)
∞

n=k−1
ξnan−k+1
⇔
ξ =
∞

n=0
ξnan
(8.5)
n−k+1

150
AN INTRODUCTION TO QUEUEING THEORY
If some 0 < ξ < 1 satisﬁes this equation, then
ν0 =
∞

n=0
νn
ν bn = (1 −ξ)
∞

n=0
ξn
∞

k=n+1
ak = (1 −ξ)
∞

k=1
k−1

n=0
akξn
=
∞

k=1
ak(1 −ξk) = 1 −a0 −
∞

k=1
akξk = 1 −a0 −(ξ −a0) = 1 −ξ
holds, too. This means that the approach
νn
ν = (1 −ξ)ξn
for all n ∈N0
(8.6)
would yield a stationary distribution for X if the number 0 < ξ < 1 satisfying
ξ = ∞
n=0 ξnan can be determined.
To this aim, we consider the power series f(x) = ∞
n=0 anxn which is well–
deﬁned on the interval [0, 1]. Clearly, f(1) = 1 and f(0) = a0 > 0. Since
an > 0 for all n ∈N0, we obtain f′′(x) > 0 for all x which means that the
function f is strictly convex on [0, 1]. A ﬁx point ξ = ∞
n=0 ξnan geometri-
cally signiﬁes the ﬁrst coordinate of an intersection between f and the identity
function.
a
1
ξ
f(x)
1
0
Figure 8.1.
Fix point as intersection with diagonal
The above properties of f and the mean value theorem together imply that
such a ﬁx point ξ does exist if and only if the condition f′(1) > 1 is satisﬁed.

Semi–Markovian Queues
151
Because of
f′(1) =
∞

n=1
nan =
 ∞
0

e−µt
∞

n=1
n(µt)n
n!
dA(t)
=
 ∞
0

e−µt
∞

n=1
(µt)n−1
(n −1)!(µt) dA(t) = µ ·
 ∞
0

t dA(t)
this condition translates to
µ · E(A) > 1
⇔
E(A) > 1
µ
(8.7)
which simply means that the mean inter–arrival time is strictly greater than the
mean service time.
Remark 8.1 One way to calculate the number ξ is the following: We start with
ξ0 := 0 and iterate by ξn+1 := f(ξn) for all n ∈N0. Then ξ = limn→∞ξn. In
order to prove this we ﬁrst observe that the sequence (ξn : n ∈N0) is strictly
increasing. This follows by induction, as ξ1 = a0 > 0 = ξ0 and
ξn+1 =
∞

k=0
akξk
n >
∞

k=0
akξk
n−1 = ξn
by induction hypothesis, since all an are strictly positive. Again by induction
we obtain ξn < 1 for all n ∈N0, as ξ0 < 1 and
ξn =
∞

k=0
akξk
n−1 <
∞

k=0
ak = 1
for all n ∈N. Hence the sequence (ξn : n ∈N0) converges as it is increasing
and bounded. Now for the limit ξ∞we obtain
ξ∞= lim
n→∞ξn = lim
n→∞ξn+1 = lim
n→∞
∞

k=0
akξk
n =
∞

k=0
ak
 
lim
n→∞ξn
!k
=
∞

k=0
akξk
∞
which means that ξ∞is a ﬁx point for the function f. This is strictly increasing,
as f′(x) > 0 for all x > 0 due to the positivity of all an. Hence
f(x) < f(ξ) = ξ
for all
x < ξ
Since the sequence (ξn : n ∈N0) starts with ξ0 = 0, this means that ξn < ξ
for all n ∈N0. Hence ξ∞= ξ, since ξ is the only ﬁx point smaller than one.

152
AN INTRODUCTION TO QUEUEING THEORY
Now theorem 7.15 can be applied with the values for ν, m and K as determined
above. This yields for the asymptotic distribution of the GI/M/1 queue the
following results (see exercises):
πj
π := lim
t→∞P(Qt = j) = (1 −ξ)ξj−1
E(A)
 ∞
0

(1 −A(t))e−µt(1−ξ) dt
(8.8)
for all j ≥1 and
π0 := lim
t→∞P(Qt = 0) = 1 −
1
E(A)
 ∞
0

(1 −A(t))e−µt(1−ξ) dt
(8.9)
Because of
ξ =
∞

n=0
anξn =
 ∞
0

e−µt
∞

n=0
(µtξ)n
n!
dA(t) =
 ∞
0

e−µt(1−ξ) dA(t)
and integration by parts
 ∞
0

A(t)e−µt(1−ξ) dt =
−1
µ(1 −ξ)
'
A(t)e−µt(1−ξ)(∞
0
−
−1
µ(1 −ξ)
 ∞
0

e−µt(1−ξ) dA(t)
=
ξ
µ(1 −ξ)
we obtain
 ∞
0

(1 −A(t))e−µt(1−ξ) dt =
 ∞
0

e−µt(1−ξ) dt −
 ∞
0

A(t)e−µt(1−ξ) dt
=
1
µ(1 −ξ) −
ξ
µ(1 −ξ) = 1
µ
(8.10)
Hence the asymptotic distributions are given by
π0 = 1 −ρ
and
πj
π = (1 −ξ)ξj−1ρ
for j ≥1, with ρ := (µ · E(A))−1. The condition (8.7), which assures positive
recurrence of the chain X, is equivalent to the stability condition ρ < 1. This
guarantees the existence of an asymptotic distribution of the system process.
The value ρ is called the load of the queueing system.
If ρ ≥1, which means that the mean service time is not smaller than the mean
inter–arrrival time, we expect the queue to be unstable. For the system process
Q and already for its embedded Markov chain X we would in this case expect

Semi–Markovian Queues
153
that no asymptotic resp. stationary distributions exist. This will be shown in
the remainder of this section.
To this aim we will use the concept of a subinvariant measure, which is deﬁned
as follows: Let E denote a countable space and P an irreducible stochastic
matrix with state space E. A measure µ on E is called subinvariant for P if
µj ≥

i∈E
µipij
holds for all j ∈E. If there is an equality in the above relation, then µ is called
invariant for P.
Let X denote a Markov chain with transition matrix P. Deﬁne the so–called
taboo probabilities in n steps by
T P n(i, j) := P(Xn
X = j, Xk /∈/ T ∀0 < k < n|X0 = i)
(8.11)
for all i, j ∈E and the taboo set T ⊂E. If T = {α} has only one element,
we will write α instead of {α} as an index. Now we can show the following
important result for irreducible stochastic matrices:
Theorem 8.2 Let X denote an irreducible Markov chain with transition ma-
trix P and countable state space E. Choose any state α ∈E. Then the
measure µα deﬁned by
µα
j :=
∞

n=1
αP n(α, j)
is the minimal subinvariant measure for P in the sense that for any other
subinvariant measure µ with µα = 1 the relation µj ≥µα
j holds for all j ∈E.
Further µα is invariant for P if and only if P is recurrent. In this case, µα is
the only subinvariant measure for P with µα
α = 1.
Proof: By deﬁnition of µα, we obtain

i∈E
µα
i pij = µα
αpα,j +

i̸=
̸
α
∞

n=1
αP n(α, i)pij
≤αP 1(α, j) +
∞

n=2
αP n(α, j)
= µα
j
where the inequality comes from the bound µα
α = P(τα
τ
< ∞|X0 = α) ≤1,
with τα
τ
denoting the ﬁrst return time to state α. Thus µα is subinvariant and
invariant if and only if P(τα
τ
< ∞|X0 = α) = 1, i.e. if X is recurrent.

154
AN INTRODUCTION TO QUEUEING THEORY
Let µ denote any other subinvariant measure for P with µα = 1. We will
show by induction on n that µj ≥n
k=1 αP k(α, j) holds for all n ≥1. First,
subinvariance of µ yields
µj ≥

i∈E
µipij ≥µαpα,j = pα,j = αP 1(α, j)
for all j ∈E. The induction step follows from
µj ≥µαpα,j +

i̸=
̸
α
µipij ≥pα,j +

i̸=
̸
α
 n

k=1
αP k(α, i)

pij
=
n+1

k=1
αP k(α, j)
where the ﬁrst inequality is due to the subinvariance of µ and the second one
follows from the induction hypothesis. The minimality of µα follows in the
limit n →∞.
Assume that X is recurrent which implies invariance of µα and µα
α = 1. Let
µ denote another subinvariant measure for P and assume µα = 1. If µ ̸≠
µα,
then there is a state j ∈E and a number n ∈N such that µj > µα
j and
P n(j, α) > 0, due to the minimality of µα and irreducibility of P. Then we
obtain
1 = µα ≥

i∈E
µi · P n(i, α) >

i∈E
µα
i · P n(i, α) = µα
α = 1
which is a contradiction. Hence there is no other subinvariant measure µ with
µα = 1.
□
Now we will apply this result to the embedded Markov chain X immediately
before arrival times of a GI/M/1 queue. Deﬁne the sets [k] := {0, . . . , k} for
all k ∈N0. Because of the upper Hessenberg structure of the transition matrix
P, we obtain
0P n(0, k + 1) =
n−1

l=1
0P l(0, k) · [k]P n−l(k, k + 1)
(8.12)
for all k ∈N by conditioning upon the time of the last visit to state k. The self–
similarity of P yields [k]P l(k, k + 1) = 0P l(0, 1). Summing the equations in

Semi–Markovian Queues
155
(8.12) for all n ≥1 yields
∞

n=1
0P n(0, k + 1) =
∞

n=1
n−1

l=1
0P l(0, k) · [k]P n−l(k, k + 1)
=
∞

l=1
0P l(0, k)
∞

n=l+1
[k]P n−l(k, k + 1)
=
 ∞

n=1
0P n(0, k)

·
 ∞

n=1
0P n(0, 1)

Setting α = 0, we obtain for the minimal subinvariant measure
µ0
k+1 = µ0
k · µ0
1
⇔
µ0
k = µ0
0 · ξk
with ξ = µ0
1. Thus any invariant measure for P must have a geometric form.
Because of (8.5), the parameter of this geometric distribution is given by the
ﬁx point ξ = ∞
n=1 anξn. If there is no solution 0 < ξ < 1 for this equa-
tion, then there cannot exist a ﬁnite invariant measure and hence no stationary
distribution.
2.
The M/G/1 Queue
The classical counterpart of the GI/M/1 queue is the M/G/1 queue. It ad-
mits a similar method of analysis. The M/G/1 queue has a Poisson arrival
process and general iid service time distributions. One server works according
to the FCFS discipline. The waiting room is inﬁnite such that there are no lost
users. Denote the arrival intensity, which is the parameter of the Poisson arrival
process, by λ > 0. Further denote the distribution function of the service time
by B.
In the GI/M/1 queue, the exponential service time appears to be a rather spe-
cial assumption, since telephone calls, demands on server capacity etc. are cer-
tainly not memoryless. This peculiarity translates to memoryless inter–arrival
times for the M/G/1 counterpart. However, the following theorem 8.3 shows
that this property follows from a few assumptions which can often be observed
in classical ﬁelds of application, such as the classical (voice only) telephone
network.
A pure jump process A = (At : t ∈R+
0 ) with state space N0 and the property
that At ≥As for all t > s is called a counting process or an arrival process.
For an arrival process in a classical telephone network we may assume the
following properties. There is a large population of possible users which all
act independently from each others. Hence only a ﬁnite number of users can

156
AN INTRODUCTION TO QUEUEING THEORY
decide to use the system within a ﬁnite time interval. Their decision to enter
the system, i.e. to use the system capacity, is homogeneous in time.
Theorem 8.3 Let A denote an arrival process with the following properties:
(a) Only a ﬁnite number of arrivals occur within a ﬁnite time interval.
(b) The process A has indendent increments, i.e. the random variables At−As
and Av −Au are independent for all s < t ≤u < v.
(c) The process A has stationary increments, i.e. the distribution of At −As
for any s < t depends only on the distance t −s.
(d) There are only single arrivals, i.e. limh→0(At+h−At) ≤1 almost certainly
for all t ≥0.
Then A is a homogeneous Poisson process.
Proof: Let Φ(t) denote the probability that no arrivals occur in the interval
[0, t[, i.e. deﬁne
Φ(t) := P(At −A0 = 0)
for all t ≥0. By deﬁnition, 0 ≤Φ(t) ≤1 for all t ≥0. Because of assumption
(a), the function Φ does not vanish identically, i.e. Φ ̸= 0
̸
. Assumptions (b)
and (c) imply
Φ(s + t) = Φ(s) · Φ(t)
(8.13)
for all s, t > 0. If there were a time t0 > 0 with Φ(t0) = 0, then property
(8.13) would yield
0 = Φ(t0) = (Φ(t0/2))2 = (Φ(t0/4))4 = . . .
such that Φ would vanish arbitrarily near t = 0 and therefore Φ = 0 since Φ is
monotone non–increasing. Hence Φ(t) > 0 for all t ≥0. The only monotone
non–vanishing solution to (8.13) is given by
Φ(t) = e−λt
for some value λ ≥0. By assumptions (b) and (c), this means that the time
until the ﬁrst arrival is distributed exponentially with parameter λ, where λ is
independent of the number of arrivals that have already occurred. By assump-
tion (d) there is only one arrival at a time. These two conditions are exactly the
deﬁning properties of the Poisson process as given in example 6.5. The case
λ = 0 corresponds to the singular case where no arrivals occur ever.
□
Again, the system process Q = (Qt : t ∈R+
0 ) has state space E = N0
and Qt denotes the number of users in the system at time t. Similar to the
GI/M/1 queue we can construct an embedded Markov chain by considering
the number of users in the system immediately after service completions. At

Semi–Markovian Queues
157
these time instances we know that the current service (if there is one) has just
begun and need only to remember the number of users in the system in order
to predict this number immediately after the next service completion.
Deﬁne T0
T
:= 0 and Tn
T
as the time of the nth service completion. Further
deﬁne T := (Tn
T
: n ∈N0). Let Xn
X
:= QTn
T for all n ∈N0 and assume that
at the time origin there are no users in the system. The Tn
T are stopping times
for Q and by deﬁnition Xn
X
is a deterministic function of QTn
T . Assume that
0 < E(B) < ∞. This implies Tn
T
→∞for n →∞. As shown above, the
chain X = (Xn
X
: n ∈N0) is a Markov chain due to the fact that at service
completion times there is either no new service (if the system is empty) or a
new service has just begun. The same property of the queue yields condition
(7.5). Hence Q is a semi–regenerative process with embedded Markov renewal
chain (X, T ).
The transition matrix P of X is given by its entries
pij =

ak :=
+ ∞
0
+
e−λt (λt)k
k!
dB(t),
j = i −1 + k
0,
j < i −1
for i ≥1, and p0,j = aj for all j ∈N0. The ﬁrst line above describes the
fact that during one service time there need to be k = j −i + 1 arrivals in
order to observe j users in the system after the next service completion if there
were i users after the last service completion. The second line simply states
that within one service time not more than one user can leave the system. The
entries p0,j are explained by the fact that if the system is empty after a service
completion, then the next service starts only after an arrival has occurred and
hence p0,j = p1,j. The matrix P is structured as
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
a0
a1
a2
a3
. . .
a0
a1
a2
a3
. . .
0
a0
a1
a2
. . .
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
Such a matrix is called lower Hessenberg matrix or skip–free to the left. The
Markov renewal chain (X, T ) is characterized by Fij
F (t) = pij · Gij(t), with
Gij(t) = P(Tn
T +1 −Tn
T ≤t|Xn
X = i, Xn
X +1 = j)
=

B(t),
i > 0
+ t
0
+
e−λuλB(t −u) du,
i = 0

158
AN INTRODUCTION TO QUEUEING THEORY
independently of j ∈N0. The values Kij
K (t) = P(T1
T > t, Qt = j|X0 = i) are
given by
Kij
K (t) =
⎧
⎪
⎧
⎪
⎪
⎨
⎪
⎪
⎨
⎪
⎪
⎩
⎪
e−λt,
i = j = 0
+ t
0
+
e−λuλe−λ(t−u) (λ(t−u))j−1
(j−1)!
(1 −B(t −u)) du,
i = 0, j > 0
(1 −B(t)) · e−λt (λt)j(
−i
(j−i)! ,
0 < i ≤j
for all t > 0.
Since all an are strictly positive, the matrix P is clearly irreducible. Assume
that ∞
n=1 nan < 1 and set ε := 1 −∞
n=1 nan. Further deﬁne the function
f(n) := n for all n ∈N0. Because of
∞

j=0
pijf(j) =
∞

j=i−1
aj−i+1j =
∞

j=i−1
aj−i+1(j −i + 1) + i −1
=
∞

n=1
nan −1 + f(i) ≤f(i) −ε
for all i ≥1, the function f satisﬁes Foster’s criterion (see theorem 2.33) and
hence X is positive recurrent if the condition ∞
n=1 nan < 1 holds. Since
∞

n=1
nan =
∞

n=1
n
 ∞
0

e−λt (λt)n
n!
dB(t) =
 ∞
0

∞

n=1
e−λt (λt)n−1
(n −1)!(λt) dB(t)
= λ · E(B)
the above condition is equivalent to the stability condition
ρ := λ · E(B) < 1
⇔
E(B) < 1
λ
(8.14)
which simply states that the mean service time be strictly smaller than the mean
inter–arrival time.
In order to obtain the stationary distribution ν for X, we need to solve the
equation system
ν0 = ν0a0 + ν1a0
ν1 = ν0a1 + ν1a1 + ν2a0
ν2 = ν0a2 + ν1a2 + ν2a1 + ν3a0
. . .

Semi–Markovian Queues
159
For each line k, adding the ﬁrst k equations yields the recursive scheme
a0ν1 = ν0r0
a0ν2 = ν0r1 + ν1r1
(8.15)
a0ν3 = ν0r2 + ν1r2 + ν2r1
. . .
with the abbreviations rk := 1 −k
n=0 an = ∞
n=k+1 an. Deﬁne further
r := ∞
n=0 rn and note that r = ∞
n=1 nan = ρ < 1 and a0 = 1 −r0.
Adding all these equations yields
(1 −r0)
∞

n=1
νn
ν = ν0r +
∞

n=1
νn
ν (r −r0)
(8.16)
which implies
∞

n=1
νn
ν = ν0
r
1 −r
⇐⇒
1 = ν0
r + (1 −r)
1 −r
=
ν0
1 −ρ
Hence the stationary distribution of X is given by ν0 = 1−ρ and the recursive
scheme above.
Theorem 8.4 If the stability condition (8.14) holds, then the asymptotic dis-
tribution of the M/G/1 queue is given by
πj
π := lim
t→∞P(Qt = j) = νj
ν
for all j ∈N0.
Proof: By theorem 7.15 and the fact that Kij
K (t) = 0 for i > j we obtain
πj
π =
1
νm
j

i=0
νiν
 ∞
0

Kij
K (t) dt
for all j ∈E. For the mean renewal interval we get
νm = ν0
 1
λ + E(B)

+
∞

n=1
νn
ν E(B) = 1 −ρ
λ
+ E(B) = 1
λ
Thus it remains to prove that
πj
π = λ
j

i=0
νiν
 ∞
0

Kij
K (t) dt = νj
ν

160
AN INTRODUCTION TO QUEUEING THEORY
holds for all j ∈E. Abbreviate ¯B(t) := 1 −B(t) for all t ≥0. First we
observe for j > 0
 ∞
0

K0
K ,j(t) dt =
 ∞
0

 t
0

e−λuλe−λ(t−u) (λ(t −u))j−1
(j −1)!
¯B(t −u) du dt
=
 ∞
0

e−λsλ ds ·
 ∞
0

e−λt (λt)j−1
(j −1)!
¯B(t) dt
=
 ∞
0

K1,j(t) dt
For 1 ≤i ≤j it can be shown (see exercises) that
 ∞
0

Kij
K (t) dt =
 ∞
0

e−λt (λt)j−i
(j −i)!
¯B(t) dt
= 1
λ
 ∞
0

∞

k=j−i+1
e−λt (λt)k
k!
dB(t)
(8.17)
= rj−i
λ
the last equality by deﬁnition of (an)n∈N0 and rj−i. Hence we obtain for j ≥1
πj
π = ν0rj−1 +
j

i=1
νiν rj−i = νj
ν
according to (8.15). As π and ν are probability distributions and we have
shown that πj
π = νj
ν for all j ≥1, we can infer π0 = ν0 as well.
□
As we have done for the GI/M/1 queue, we want to show now that in case of
ρ ≥1 there is no stationary distribution of the embedded Markov chain X at
times of service completion. We will prove this by contradiction. Assume that
ρ ≥1 and ν be a stationary distribution. Then the recursion scheme and in
particular equation (8.16) holds, regardless of the value for ρ = r.
For the case r = 1 we obtain from this equation 0 = ν0
ν , which implies further
νn
ν = 0 for all n ≥1 because of P being irreducible. This is in contradiction to
the assumption that ν be a distribution. In the case r > 1, we get from equation
(8.16) that ∞
n=1 νn
ν
= r > 1, again a contradiction to the assumption that ν
be a distribution.
3.
The GI/M/m Queue
The last example for semi–Markovian queues is a multi–server queue with
exponential servers. Denote all the parameters as for the GI/M/1 queue, i.e.

Semi–Markovian Queues
161
the inter–arrival times are iid with distribution function A, and every server
has an exponential distribution with parameter µ > 0. Of course, instead of
one server as in section 1, we now have m servers. The service displine is
FCFS and the capacity of the waiting room is unbounded.
Regardless of the number of servers, all of them are memoryless. Hence the
times of arrivals lead to an embedded Markov renewal chain, just as in the case
m = 1. The system process Q = (Qt : t ∈R+
0 ) has state space E = N0, with
Qt indicating the number of users in the system (i.e. in service or waiting) at
time t. Deﬁne Tn
T
as the time of the nth arrival and Xn
X
:= QTn
T −1 as the
number of users in the system immediately before the nth arrival.
Clearly, the Tn
T are stopping times and Xn
X is a deterministic function of QTn
T .
Again we assume that A is not lattice, and 0 < E(A) < ∞. This implies in
particular that Tn
T
→∞almost surely as n tends to inﬁnity. The sequence
X = (Xn
X
: n ∈N0) is a Markov chain since at times of arrivals the future
of the system is determined only by the current number of users in the system,
due to the memoryless property of the service times. The same property of the
queue ensures the validity of equation (7.5) for the system process Q. Thus the
system process Q is semi–regenerative with embedded Markov renewal chain
(X, T ).
The transition probabilities pij of X are derived by the following considera-
tions. Clearly, there may be only one arrival in any interval ]Tn
T , Tn
T +1]. Hence
pij = 0 for j > i + 1.
Now consider the case j ≤i + 1 ≤m. This means that during one inter–
arrival period no user is waiting and all are served with exponential intensity
µ. Given that the inter–arrival period has length t > 0, the probability for any
user to complete service is 1 −e−µt. A transition from state i to state j for the
Markov chain X means that i + 1 −j out of i + 1 users are served during one
inter–arrival period. There are
 i+1
i+1−j

=
i+1
j

combinations to choose which
users complete their service and which do not. Hence for j ≤i + 1 ≤m we
obtain
pij =
 ∞
0

i + 1
j

(1 −e−µt)i+1−j(e−µt)j dA(t)
(8.18)
by conditioning on the length t of the inter–arrival period. Note that the inte-
grand is the binomial distribution with i + 1 degrees of freedom and parameter
e−µt evaluated at 0 ≤j ≤i + 1.
The third case is m ≤j ≤i + 1. Here, all m servers are busy during the
complete inter–arrival period. Then the number of users served in time t is dis-
tributed like a Poisson process (see example 6.5) with intensity m·µ evaluated

162
AN INTRODUCTION TO QUEUEING THEORY
at time t. Conditioning on the length of the inter–arrival period, we obtain
pij =
 ∞
0

(mµ · t)i+1−j
(i + 1 −j)! e−mµ·t dA(t)
(8.19)
for m ≤j ≤i + 1. Note that this expression coincides with formula (8.18) for
m = j = i + 1. Furthermore it depends only on the difference i + 1 −j, but
not on the values of i, j themselves.
The last case to consider is j < m < i+1. In this situation there are i+1−m
users waiting in the queue at the beginning of the inter–arrival period, while
m −j servers are idle at the end of it. This is a mixture between the second
and the third case. First the regime of the latter governs, until there are no
waiting users any more (i.e. the queue has emptied). Then the former case
applies. Thus we condition ﬁrst on the length t of the inter–arrival period and
then on the time u < t to empty the queue. The time to empty the queue has
an Erlang distribution Emµ
i
E +1−m with i + 1 −m stages and intensity mµ. After
that the number of served users has a binomial distribution Bp
m with m degrees
of freedom and parameter p := e−µ(t−u). This leads to an expression
pij =
 ∞
0

 t
0

Bp
m(m −j) dEmµ
i
E +1−m(u) dA(t)
=
 ∞
0

m
j

e−jµt
 t
0

(mµu)i−m
(i −m)! (e−µu −e−µt)m−jmµ du dA(t)
(8.20)
which the reader may verify as an exercise.
Collecting these results, we can sketch the structure of the transition matrix as
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
p00
p01
p10
p11
p12
...
...
pm−2,0
. . .
pm−2,m−1
pm−1,0
. . .
pm−1,m−1
β0
pm,0
. . .
pm,m−1
β1
β0
...
...
...
...
pm+n,0
. . .
pm+n,m−1
βn
β +1
βn
β
. . .
β0
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
abbreviating βk := pi,i+1−k in case (8.19). The non–speciﬁed entries in P
correspond to the case j > i + 1 and hence are zero. The matrix P has an
upper Hessenberg form like the respective transition matrix for the GI/M/1

Semi–Markovian Queues
163
queue. The most important part of it is the lower right–hand part containing
the entries βn
β . The other parts are boundary conditions.
In order to determine the stationary distribution of X, we ﬁrst consider the
partition E = F ∪F c with F = {0, . . . , m −2}. We then obtain for the
transition matrix P ′ of the Markov chain restricted to F c the form
P ′ =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
r0
β0
r1
β1
β0
r2
β2
β1
β0
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
with rn = 1 −n
k=0 βk. This has the same form as the transition matrix (8.1)
for the GI/M/1 queue. Deﬁne
ρ =
1
mµ · E(A)
The arguments in section 1 all apply to the matrix P ′ with ρ deﬁned as above.
In the case ρ < 1 we obtain a stationary distribution ν′ = ν′P ′ given by
ν′
n
ν = (1 −ξ)ξn
(8.21)
with ξ = ∞
n=0 ξnβn
β . For the case ρ ≥1 it follows that P ′ (and hence P) is
not positive recurrent. In the following we assume ρ < 1.
By theorem 2.30 there is a unique extension of ν′ to a stationary measure for
P, denoted by ν′′ = ν′′P. For this we have ν′′
n
ν
= ν′
n
ν −m+1 for all n ≥m −1
and
ν′′
n
ν +1 =
∞

k=0
ν′′
kpk,n+1 =
∞

k=n
ν′′
kpk,n+1
for all n = 0, . . . , m −2, which leads to
ν′′
n
ν
=
1
pn,n+1

ν′′
n
ν +1 · (1 −pn+1,n+1) −
∞

k=n+2
ν′′
kpk,n+1

(8.22)
Thus ν′′
m
ν
−2 and iteratively ν′′
m
ν
−3, . . . , ν′′
0
ν can be determined by formula (8.22).
The stationary distribution ν = νP for X is then given as ν = c · ν′′ for a
constant c > 0. This is determined by
c−1 =
∞

n=0
ν′′
n
ν
=
m−2

n=0
ν′′
n
ν + 1
(8.23)
Altogether, formulae (8.21), (8.22), and (8.23) yield the stationary distribution
ν = νP for X.

164
AN INTRODUCTION TO QUEUEING THEORY
The asymptotic distribution π of the queueing process Q can now be obtained
by theorem 7.15. For n ≥m we obtain
πn =
c
E(A)
∞

k=n−1
(1 −ξ)ξk−(m−1)
 ∞
0

(1 −A(t))e−mµt (mµt)k+1−n
(k + 1 −n)!dt
= c · (1 −ξ)
E(A)
 ∞
0

(1 −A(t))e−mµtξn−m
∞

k=n−1
(mµt · ξ)k+1−n
(k + 1 −n)! dt
= c · (1 −ξ)
E(A)
ξn−m
 ∞
0

(1 −A(t))e−mµt·(1−ξ) dt
Due to (8.10) the integral above equals (mµ)−1 and thus we get to
πn = ρ · c · (1 −ξ)ξn−m
(8.24)
for n ≥m. The asymptotic probability that all servers are busy is given by
∞

n=m
πn = ρ · c
Hence the conditional asymptotic probability that there are k users in the queue
(i.e. k + m users in the system) given that all servers are busy equals
πm+k
∞
n=m πn
= (1 −ξ)ξk
which means that the conditional asymptotic queue length distribution given
that all servers are busy is geometric.
Concerning the asymptotic probabilities πn with n = 1, . . . , m−1, we employ
the rate conservation law, for which we ﬁrst give an intuitive explanation.
The arrival process of the GI/M/m queue is a renewal process with renewal
intervals distributed by A. Blackwell’s theorem 6.14 states that asymptotically
an arrival occurs with constant rate (E(A))−1. Given that an arrival occurs,
there is an asymptotic probability νn
ν −1 of observing n users in the system. On
the other hand, out of state n ≤m −1 (which has asymptotic probability πn)
the exponential servers provide a constant rate n · µ to switch to state n −1.
Now the rate conservation law states that asymptotically
νn
ν −1 ·
1
E(A) = πn · nµ
which means that the probability ﬂow from state n −1 to state n equals the
ﬂow from n to n −1. This yields
πn =
νn
ν −1
nµ · E(A)
(8.25)

Semi–Markovian Queues
165
for n = 1, . . . , m −1. Finally we obtain π0 by normalization, i.e.
π0 = 1 −
∞

n=1
πn = 1 −ρ · c −mρ
m−1

n=1
νn
ν −1
n
This and formulae (8.25) and (8.24) collect all asymptotic probabilities.
Notes
The idea to analyze the M/G/1 queue via its embedded Markov chain has been
presented in Kendall [49]. Earlier text book presentations for the M/G/1 and
the GI/M/1 queue can be found in Cohen [28] or C¸ inlar [25]. The former con-
tains further many special queues which are analyzed via embedded Markov
chains as well. The GI/M/m queue has been examined in Kleinrock [50] and
Asmussen [5]. The latter contains further an exact presentation of the rate
conservation law.
For more examples of semi–Markovian queues see Cohen [28]. An application
of the semi–Markov method to tandem queues is given in Breuer et al. [23].
Exercise 8.1 Assume exponential service times and a Poisson arrival process
for the M/G/1 and GI/M/1 queue, respectively, and show that the results for
the asymptotic distribution coincide with the results obtained for the M/M/1
queue.
Exercise 8.2 Verify equalities (8.8) and (8.9).
Exercise 8.3 Compute mean and variance of the asymptotic number of users
in the system for the GI/M/1 queue. These may be expressed in terms of ξ.
Derive further the mean sojourn time in the system.
Exercise 8.4 Verify equality (8.17).
Exercise 8.5 For an M/G/1 queue, show that the z–transform of the number
of users which arrive during a service is given by
H(z) =
∞

n=0
anzn = B∗(λ −λz)
for all |z| < 1, where B∗denotes the LST of the service time distribution.
Show that the mean and the variance of this number are ρ and λ2µ2(B), re-
spectively, where µ2(B) denotes the second moment of the service time distri-
bution.

166
AN INTRODUCTION TO QUEUEING THEORY
Exercise 8.6 Consider an M/G/1 queue with batch arrivals. Instead of sin-
gle arrivals as in the ordinary M/G/1 queue, at every arrival instant the arrival
consists of a batch of n ∈N independent users with probability gn. Arrival
instants are distributed as a Poisson process with intensity λ > 0.
a) Deﬁne the z–transform G(z) := ∞
n=1 gnzn of the batch size distribution.
Show that the z–transform of the number of users which arrive in an interval
of length t is N∗(t, z) = e−λt(1−G(z)).
b) Let B∗(s) denote the LST of the service time distribution. Show that the
z–transform of the number of users which arrive during a service is given by
B∗(λ −λG(z)).
Exercise 8.7 Let Qd
n denote the number of users in the system after the nth
departure and Kn
K
the number of arrivals between the nth and the n + 1st
departure. Justify the relation
Dn+1 = (Dn −1)+ + Kn
K
where a+ = max(0, a). As n tends to inﬁnity, we obtain for D := limn→∞Dn
and K := limn→∞Kn
K the equality
D = D −1D>0 + K
in distribution. Take the expectation of D2 in order to derive
E(D) = ρ +
ρ2µ2(B)
2(1 −ρ)µ2
1(B)
(8.26)
where µ2
1(B) and µ2(B) denote the squared ﬁrst and the second moment of the
service time distribution. Use the results from exercise 8.5. Why is this also
the mean asymptotic number of users in the system? Equation (8.26) is known
as the Pollaczek–Khinchin mean value formula.
Exercise 8.8 Show that the asymptotic mean number of users in an M/G/1
queue is minimal for deterministic service time distributions.
Exercise 8.9 Derive expression (8.20).

Chapter 9
PHASE–TYPE DISTRIBUTIONS
The memoryless property of the exponential distribution has been substantial
for arriving at embedded Markov chains in chapter 8 when analyzing GI/M/1
and M/G/1 queues. The stationary distributions of these chains served as a
foothold for a semi–regenerative analysis.
Our goal pursued in the next two chapters is to ﬁnd generalizations beyond the
exponential distribution and/or the Poisson process that are more versatile in
their modelling capacity, but still allowing analyses of the respective queues
by means of embedded Markov chains.
1.
Motivation
In the present chapter, an extremely versatile class of distributions, the so–
called phase–type or PH distributions, will be introduced. It is possible to
approximate any distribution on the non–negative real numbers by a PH dis-
tribution, and the resulting queueing models can be analyzed almost as if we
have dealt with the exponential distribution.
As a motivation, we begin with a practical example. Consider the M/M/c/c+K
queue, which is deﬁned as follows. Arrivals are modelled by a Poisson process
with rate λ > 0. Service times are exponentially distributed with rate µ > 0.
There are c servers, and the capacity of the waiting room is K. That means that
in total there is room for c+K users in the system including the servers. If upon
an arrival the system is ﬁlled, i.e. with c + K users already in it, this arriving
user is not admitted into the system. In this case we say that the arriving user

170
AN INTRODUCTION TO QUEUEING THEORY
is lost. Queueing systems with the possibility of such an event are thus called
loss systems.
The queue described above is a simple Markov process with generator
Q =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
−λ
λ
µ
−λ −µ
λ
2µ
−λ −2µ
λ
...
...
...
cµ
−λ −cµ
λ
...
...
...
cµ
−λ −cµ
λ
cµ
−cµ
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
up to the ﬁrst loss (all non–speciﬁed entries equal zero).
From a system administrator’s point of view, the loss of a user is regarded as
a bad event, and thus the question arises naturally how the distribution of the
time up to the ﬁrst loss might be expressed. However, the above description
of the queueing process simply ignores loss events, as can be seen from the
missing λ entries in the last line of the generator.
In order to include a possible loss event into our model of the queue, we add a
new element to the state space and enlarge the generator as follows:
Q′ =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
−λ
λ
µ
−λ −µ
λ
...
...
...
cµ
−λ −cµ
λ
...
...
...
cµ
−λ −cµ
λ
cµ
−λ −cµ
λ
0
. . .
. . .
0
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
again with all non–speciﬁed entries being zero. The ﬁrst m = c+K +1 states
describe the number of users in the system, just as in the former generator Q.
But now there is the possibility to enter another state m + 1 with rate λ from
state m, obviously meaning exactly that a loss has occured. Since we want to
observe the system only until the ﬁrst loss, we choose the loss state m + 1 as
an absorbing one. Thus all entries in the last line are zero.
Now the system administrator’s question can be formulated mathematically as
the distribution of the time until the Markov process with generator Q′ enters
the absorbing state m + 1. Exactly this problem is addressed (in a general
form) by the concept of a phase–type distribution.

Phase–Type Distributions
171
2.
Deﬁnition and Examples
Deﬁnition 9.1 Let X = (Xt
X : t ≥0) denote an homogeneous Markov process
with ﬁnite state space {1, . . . , m + 1} and generator
Q =

T
η
0
0

where T is a square matrix of dimension m, η a column vector and 0 the zero
row vector of the same dimension. The initial distribution of X shall be the
row vector ˜α = (α, αm+1), with α being a row vector of dimension m. The
ﬁrst states {1, . . . , m} shall be transient, while the state m + 1 is absorbing.
Let Z := inf{t ≥0 : Xt
X = m + 1} be the random variable of the time until
absorption in state m + 1.
The distribution of Z is called phase–type distribution (or shortly PH distri-
bution) with parameters (α, T). We write Z ∼PH(α, T). The dimension m
of T is called the order of the distribution PH(α, T). The states {1, . . . , m}
are also called phases, which gives rise to the name phase–type distribution.
Let 1 denote the column vector of dimension m with all entries equal to one.
The ﬁrst observations to be derived from the above deﬁnition are
η = −T1
and
αm+1 = 1 −α1
These follow immediately from the properties that the row sums of a generator
are zero and the sum of a probability vector is one. The vector η is called
the exit vector of the PH distribution. Now the distribution function and the
density of a PH distribution are derived in
Theorem 9.2 Let Z ∼PH(α, T). Then the distribution function of Z is given
by
F(t) := P(Z ≤t) = 1 −αeT·t1
(9.1)
for all t ≥0, and the density function is
f(t) = αeT·tη
(9.2)
for all t > 0. Here, the function eT·t := exp(T · t) := ∞
n=0
tn
n!T n denotes a
matrix exponential function.
Proof: For the Markov process X with generator Q as given in deﬁnition 9.1
the equation
P(t) = exp(Q · t) =

eT·t
1 −eT·t1
0
1


172
AN INTRODUCTION TO QUEUEING THEORY
holds for the transition matrix P(t) at every time t ≥0. This implies
F(t) = ˜αeQ·tem+1 = αm+1 + α · (1 −eT·t1) = αm+1 + α1 −αeT·t1
= 1 −αeT·t1
with em+1 denoting the m+1st canonical base vector. For the density function
we obtain
f(t) = F ′(t) = −α d
dteT·t1 = −αTeT·t1 = αeT·t(−T1) = αeT·tη
which was to be proven.
□
A ﬁrst consequence is F(0) = αm+1, which is also clear from deﬁnition 9.1.
An important question to be examined is when a phase–type distribution is
non–defective, i.e. what the conditions for F(∞) = limt→∞F(t) = 1 are.
This is answered in
Theorem 9.3 Let F denote a PH(α, T) distribution function.
F is non–
defective, i.e. F(∞) = 1 for all α, if and only if T is invertible. In this case,
(−T −1)ij is the expected total time spent in state j given that the process X
started in state i.
Proof: Let Eij
E
denote the expected total time spent in state j given that the
process X started in i. Deﬁne E = (Eij
E )i,j≤m as the respective matrix of
expectations.
First we assume that F(∞) = 1 for all α, i.e. that F is non–defective. This
means that with probability one there is an absorption from any initial state i.
This implies for E to be ﬁnite, i.e.
Eij
E
< ∞
for all i, j ∈{1, . . . , m}
Conditioning on the ﬁrst state visited after i yields the relations
Eij
E
=

k̸=
̸
i
Tik
T
−Tii
T Ekj
for all i ̸≠
j
Eii
E
=
1
−Tii
T
+

k̸=
̸
i
Tik
T
−Tii
T Eki
In matrix notation, this is expressed as TE = −I, with I denoting the identity
matrix. Hence we obtain E = −T −1, which was to be proven.

Phase–Type Distributions
173
Now we assume that T is invertible. Deﬁne the vector Φ(x) = exp(Tx)1 for
all x ≥0. The numbers Φi(x) are the probabilities that the process X is in one
of the states {1, . . . , m} after time x given that the initial state was i. Hence
Φi(x) ∈[0, 1]
for all i ∈{1, . . . , m}
Further the equation
eT·t = I +
 t
0

TeT·x dx
holds as can be seen by differentiating both sides and acknowledging eT·0 = I.
Multiplying by T −1 from the left side and by 1 from the right side yields
T −1Φ(t) = T −1eT·t1 = T −11 +
 t
0

eT·x dx1
As all entries of Φ(t) are ﬁnite, we obtain for t tending to inﬁnity
lim
t→∞
 t
0

eT·x dx < ∞
in an entry–wise meaning. But the values (eT·x)ij are simply the probability
that the process X is in state j at time x given that it started in state i. Hence
we obtain
Eij
E
=
 ∞
0

(eT·x)ij dx < ∞
for all i, j ∈{1, . . . , m}
which means that all states j ∈{1, . . . , m} are transient. Thus an absorption
in state m + 1 is certain regardless of the initial distribution, which was to be
proven.
□
From now on T shall be assumed to be invertible. In order to show the versatil-
ity of the phase–type concept, we shall give a few examples below. Important
characteristics for distributions are their moments. Given a distribution func-
tion F, its nth moment (if existing) is given by
Mn
M (F) :=
 ∞
0

tndF(t)
Clearly, the ﬁrst moment is the mean of the distribution. The nth moment of
the exponential distribution with parameter λ is given by Mn
M
= n!/(λn) (see
exercises). Another important characteristic is the so–called squared coefﬁ-
cient of variation, deﬁned by
CV
C (F) := Var(F)/E(F)2

174
AN INTRODUCTION TO QUEUEING THEORY
with Var(F) denoting the variance of F. For any exponential distribution this
equals one. The values of the squared coefﬁcient of variation will explain the
names for the hypo- and hyper–exponential distributions introduced below.
Example 9.4 Erlang distribution
A well–known distribution within the family of Gamma distributions is the
so–called Erlang distribution. An Erlang distribution Eλ
n
E
with n degrees of
freedom (or stages) and parameter λ is the distribution of the sum of n expo-
nential random variables with parameter λ. It has the density function
f(t) =
λn
(n −1)!tn−1e−λt
for all t ≥0. Its interpretation as a succession of n exponential distributions
with rate λ each can be illustrated graphically as in
-











-
λ











-
λ
. . .
-
λ











-
Figure 9.1.
Erlang distribution
Here we see that an Erlang distribution can be represented as the holding time
in the transient state set {1, . . . , n} of a Markov chain with absorbing state
n + 1 where the only possible transitions occur from a state k to the next state
k + 1 (for k = 1, . . . , n), with rate λ each. In terms of our deﬁnition 9.1, we
have a PH representation
α = (1, 0, . . . , 0),
T =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
−λ
λ
...
...
−λ
λ
−λ
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
and
η =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
0
...
0
λ
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
with all non–speciﬁed entries in T being zero.
The mean of an Erlang distribution with n degrees of freedom and parameter
λ is n/λ, while its squared coefﬁcient of variation is 1/n, i.e. less than one if
n > 1 (see exercises). This explains the name hypo–exponential distribution
appearing in the next example.
Example 9.5 Generalized Erlang distribution
A slight generalization of the Erlang distribution is obtained if one admits the
exponential stages to have different parameters. Then we talk about a general-
ized Erlang (or a hypo–exponential) distribution. The representation as a PH
distribution results in the ﬁgure

Phase–Type Distributions
175
Figure 9.2.
Generalized Erlang distribution
and leads to a PH representation
α = (1, 0, . . . , 0),
T =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
−λ1
λ1
...
...
−λn−1
λn−1
−λn
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
and
η =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
0
...
0
λn
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
with all non–speciﬁed entries in T being zero. For this family of distributions,
a closed formula for the density function is already rather complex.
Example 9.6 Hyper–exponential distribution
A hyper–exponential distribution is a ﬁnite mixture of n ∈N exponential
distributions with different parameters λk (k = 1, . . . , n). Its density function
is given as
f(t) =
n

k=1
qkλke−λkt
with proportions qk > 0 satisfying n
k=1 qk = 1. A graphical representation
of this distribution is











q1












*
q2
H

H
H
H
H
H
H
H
H
H
j
Hqn

























*






*









...


H
j
H


H
j
H






-
-
-
Figure 9.3.
Hyper–exponential distribution
This leads to a PH representation by
α = (π1, . . . , πn),
T =
⎛
⎜
⎛
⎝
⎜
−λ1
...
−λn
⎞
⎟
⎞
⎠
⎟
and
η =
⎛
⎜
⎛
⎝
⎜
λ1
...
λn
⎞
⎟
⎞
⎠
⎟
-

-


-



-






-
λ1

-


-



-






-
λ2
. . .
-
λn−1

-


-



-






-

176
AN INTRODUCTION TO QUEUEING THEORY
with all non–speciﬁed entries in T being zero.
The mean of a hyper–exponential distribution is n
i=1 πi/λi, while its squared
coefﬁcient of variation is always larger than one if n > 1. This explains the
name hyper–exponential distribution.
Example 9.7 Cox distribution
A more complex example of the classical families of distributions are the Cox
distributions. These are generalized Erlang distributions with preemptive exit
options. A Coxian random variable measures the holding time within the box
depicted as
-













-
q1
@
@
@
@
R
@1
@ −q1













-
q2
@
@
@
@
R
@1
@ −q2
. . .











-
qn−1
@
@
@
@
R
@1
@ −qn−1











-
Figure 9.4.
Cox distribution
A Cox distribution can be described as a special PH distribution with parame-
ters α = (1, 0, . . . , 0) and
T =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
−λ1
q1λ1
...
...
−λn−1
qn−1λn−1
−λn
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟,
η =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
(1 −q1)λ1
...
(1 −qn−1)λn−1
λn
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
for which all non–speciﬁed entries in T are zero.
As we have seen in example 9.6, the set of transient states in a PH distribution
may fall apart into several communication classes. The deﬁnition of a phase–
type distribution leaves open the possibility of a transient communication class
which cannot be entered because the respective initial probabilities contained
in the row vector α are zero. Such states are called superﬂuous, since the
Markov process X deﬁning the PH distribution will never be in such a state. As
we will obtain the same distribution of the time until absorption if we leave out
superﬂuous states, we shall from now on (unless stated otherwise) assume that
there are no superﬂuous states in the deﬁnition of a phase–type distribution.
3.
Moments
A good means to determine the moments of a distribution is the Laplace–
Stieltjes transform (or shortly LST, see appendix). This is derived in

Phase–Type Distributions
177
Theorem 9.8 The LST of a phase–type distribution F = PH(α, T) is given
by
Φ(s) :=
 ∞
0

e−st dF(t) = αm+1 + α(s · I −T)−1η
for all s ∈C with Re(s) ≥0.
Proof: For s = 0 the statement is obvious. Be s ̸= 0
̸
and Re(s) ≥0. Integra-
tion by parts yields
 ∞
0

e−steT·t dt = −1
s
-
e−steT·t.∞
0 −
 ∞
0

e−steT·tT dt

As T is assumed to be invertible, we know from the proof of theorem 9.3 that
+ ∞
0
+
eT·t dt < ∞entry–wise, and hence limt→∞eT·t = 0, also entry–wise.
This yields
 ∞
0

e−steT·t dt = 1
s · I + 1
s ·
 ∞
0

e−steT·t dtT
which implies
 ∞
0

e−steT·t dt (s · I −T) = I
and hence
 ∞
0

e−steT·t dt = (s · I −T)−1
for all s ̸= 0
̸
with Re(s) ≥0.
This leads to
Φ(s) =
 ∞
0

e−st dF(t) = αm+1 +
 ∞
0

e−stαeT·tη dt
= αm+1 + α
 ∞
0

e−steT·t dt η = αm+1 + α(s · I −T)−1η
which is the statement.
□
Corollary 9.9 Let Z ∼PH(α, T). The moments of Z are given by
E(Zn) = (−1)n · n! · αT −n1
for all n ∈N.
Proof: Because of
dn
dsn (s · I −T)−1 = (−1)n · n! · (s · I −T)−(n+1)

178
AN INTRODUCTION TO QUEUEING THEORY
we obtain
E(Zn) = (−1)n · dn
dsn Φ(s)|s=0
= n! · α(s · I −T)−(n+1)|s=0η = n! · α(−T)−n(−T)−1(−T)1
= n! · α(−T)−n1
□
In the next chapter it will be shown in corollary 10.12 that another expression
for the mean is given by E(Z) = (πη)−1 with π(T + ηα) = 0.
4.
Closure Properties
A useful advantage of phase–type distributions is the fact that certain compo-
sitions of PH distributions result in PH distributions again. This means that
the class of PH distributions is closed under these compositions. For PH dis-
tributed random variables Z1 and Z2
Z we will show closure properties for the
compositions Z1+Z2
Z (convolution), pZ1+(1−p)Z2
Z with p ∈[0, 1] (mixture),
and min(Z1, Z2
Z ).
Theorem 9.10 Let Zi
Z ∼PH(α(i), T (i)) of order mi for i = 1, 2. Then Z =
Z1 + Z2
Z ∼PH(α, T) of order m = m1 + m2 with representation
αk =

α(1)
k ,
1 ≤k ≤m
α(1)
m1+1 · α(2)
k−m1,
m1 + 1 ≤k ≤m
and
T =

T (1)
η(1)α(2)
0
T (2)

where η(1) = −T (1)1m1 and 0 denotes a zero matrix of appropriate dimension.
Proof: By deﬁnition, Zi
Z is the random variable of the time until absorption in
a Markov process Xi
X with transient states {1, . . . , mi} and an absorbing state
which shall be denoted by ei in this proof. The transition rates of Xi
X within
the set of transient states are given by the matrix T (i) and the absorption rates
from the transient states to the absorbing state are given by the vector η(i).
Then the random variable Z = Z1 + Z2
Z is the total time duration of ﬁrst enter-
ing e1 and then e2 in the Markov process which is structured as follows:

Phase–Type Distributions
179
-
α(1)













?
η(1)
@
@
@
@
@
@
@
@
@
@
R
@
α(1)@
m1+1@
e1
-
α(2)













?
η(2)
@
@
@
@
@
@
@
@
R
@
α(2)@
m2+1@
e2
Figure 9.5.
Convolution of two PH distributions
Here the point e1 is not a state of the Markov process described above but only
an auxiliary construction aid for a better illustration. In particular there is no
holding time in e1. The only absorbing state in the Markov process constructed
above is e2.
With probability α(1)
m1+1 we enter the ﬁrst absorbing state e1 immediately, while
the vector α(1) contains the probabilities that we ﬁrst enter the set of transient
states of X1. In the latter case, the matrix T (1) and then the vector η(1) deter-
mine the time until the ﬁrst absorption in e1.
After having reached e1, the chain immediately (i.e. with no holding time in
e1) proceeds to the second stage, which is completely analogous to the ﬁrst.
With probability α(2)
m2+1 we enter the second absorbing state e2 immediately,
while the vector α(2) contains the probabilities that we ﬁrst enter the set of
transient states of X2. In the latter case, the matrix T (2) and then the vector
η(2) determine the time until the ﬁrst absorption in e2.
Thus we get to the second absorbing state e2 immediately with probability
α(1)
m1+1 ·α(2)
m2+1. There are transient states {1, . . . , m1, m1 +1, . . . , m1 +m2}.
The ﬁrst m1 of these are reached with probabilities α1, . . . , αm1, while the last
m2 of these states can only be reached via an immediate ﬁrst absorption in e1
and thus with probabilities α(1)
m1+1 · α(2)
i
for i = 1, . . . , m2. This explains the
expression for α.
In order to explain the structure of T, we observe ﬁrst that there is no path from
the second set of transient states to the ﬁrst, whence the lower left entry of T
is zero. The diagonal entries of T describe the transition rates within the two
sets of transient states, respectively, and thus are given by T (1) and T (2). The
only way to get from the ﬁrst to the second set of transient states is the path
via e1 for which we ﬁrst need the rates given in η(1) and then the probabilities

180
AN INTRODUCTION TO QUEUEING THEORY
contained in α(2). Hence the upper right entry of T.
□
Theorem 9.11 Let Zi
Z ∼PH(α(i), T (i)) of order mi for i = 1, 2, as well as
p ∈[0, 1]. Then Z = pZ1 + (1 −p)Z2
Z ∼PH(α, T) of order m = m1 + m2
with representation
α = (p · α(1), (1 −p) · α(2))
and
T =

T (1)
0
0
T (2)

where 0 denote zero matrices of appropriate dimensions.
Proof: Going along the line of reasoning of the last proof, we observe that Z
is equal to Z1 with probability p and equal to Z2
Z with probability 1−p. Hence
we obtain the following construction of a Markov process:










pα(1)

@

@
@
@
@
@
@
@
R
@
(1 −p)α(2)@
-
pα(1)
m1+1 + (1 −p)α(2)
m2+1
e

























@
R
@
R


@
R
@
R
@
R
@
R








X



X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X
X X
z
X
η(1)




























 
:

η(2)
Figure 9.6.
Mixture of two PH distributions
Here, we enter the ﬁrst set of transient states with probabilities p · α(1)
i
for
i = 1, . . . , m1 and the second set with probabilities (1 −p) · α(2)
i
for phases
i = m1 + 1, . . . , m2. This explains the expression for α.
From either of these sets we proceed with transition matrices T (i) and exit
vectors η(i), i = 1, 2, in order to reach the absorbing state e. There is no path
from one set of transient states to the other, which explains the structure of T.
The absorbing state e can be reached immediately (i.e without entering any
transient state) with probability pα(1)
m1+1 + (1 −p)α(2)
m2+1.
□
In order to formulate the next theorem, we ﬁrst need to deﬁne the so–called
Kronecker compositions of matrices. Let A = (aij) and B = (bij) denote

Phase–Type Distributions
181
n1 × m1 and n2 × m2 matrices, respectively. The Kronecker product of A
and B is deﬁned as the (n1 · n2) × (m1 · m2) matrix A ⊗B with entries
(A ⊗B)(i1,i2),(j1,j2) := ai1,j1 · bi2,j2
for all 1 ≤ik ≤nk and 1 ≤jk ≤mk, k = 1, 2. As a block matrix we can
write
A ⊗B =
⎛
⎜
⎛
⎝
⎜
a11B
. . .
a1m1B
...
...
an11B
. . .
an1m1B
⎞
⎟
⎞
⎠
⎟
If A and B are square matrices, i.e. nk = mk for k = 1, 2, then the Kronecker
sum A ⊕B of the matrices A and B is deﬁned as
A ⊕B := A ⊗I2
I + I1 ⊗B
with Ik
I denoting the nk × nk identity matrix for k = 1, 2.
Example 9.12 Let n1 = m1 = 1 and n2 = m2 = 2. If A = −λ and
B =

−µ
µ
0
−µ

,
then
A ⊕B =

−(λ + µ)
µ
0
−(λ + µ)

is an explicit expression for the Kronecker sum of A and B.
Theorem 9.13 Let Zi
Z
∼PH(α(i), T (i)) of order mi for i = 1, 2 and de-
ﬁne Z = min(Z1, Z2
Z ). Then Z ∼PH(α, T) of order m = m1 · m2 with
representation
α = α(1) ⊗α(2)
and
T = T (1) ⊕T (2)
in terms of the Kronecker compositions.
Proof: For i = 1, 2, the random variables Zi
Z are the times until absorption
in the Markov processes Xi
X = (X(i)
t
X
: t ≥0) where the initial distributions
for the transient states are α(i) and the transition rates among the transient
states are given by T (i). Thus we can determine Z if we start running X1
X and
X2
X concurrently and stop whenever the ﬁrst of the two processes enters the
absorbing state. We will show that the two–dimensional Markov process X
depicted as in the ﬁgure below has the same time until absorption as the ﬁrst
absorption of the concurrent processes X1
X and X2
X .
The state space of X shall be
E = {(i, j) : 1 ≤i ≤m1, 1 ≤j ≤m2} ∪{e}

182
AN INTRODUCTION TO QUEUEING THEORY
-
α(1) ⊗α(2) 
-

-



T (1) ⊕T (2)
?
η
HH
H
H
H
H
H
H
H
H
H
H
H
H
H
H
H
H
H
H
HH
j
H e
Figure 9.7.
Superposition of two PH distributions
where e is the absorbing state and all other states are transient. We will keep
in mind the interpretation that Xt
X = (i, j) means X(1)
t
X
= i and X(2)
t
X
= j for
all transient states i, j. The exit vector η of dimension m1 · m2 has entries
ηij = η(1)
i
+ η(2)
j
η
for all i ≤m1 and j ≤m2.
Since we start the processes X1
X
and X2
X
independently, we clearly have an
initial distribution
P(X0 = (i, j)) = P(X(1)
0
= i) · P(X(2)
0
= j) = α(1)
i
· α(2)
j
which explains the expression for α. If the process X is in state (i, j), this
means that the exponential holding times in the states i (for X1
X ) and j (for
X2
X ) are running concurrently. According to lemma 3.2 they almost certainly
do not stop at the same time instant. Thus X has only state transitions from
(i, j) to either (i, h) or (k, j). These occur with transition rates T (2)
jh
T
or T (1)
ik
T
,
respectively, if h and k are transient states. According to lemma 3.2 the holding
time in state (i, j) is exponential with parameter −(T (1)
ii
T
+T (2)
jj
T
). This explains
the structure of T. The values of η are readily veriﬁed by
ηij = −(T1m1m2)(i,j) = −
⎛
⎝
⎛
T (1)
ii
T
+ T (2)
jj
T
+

h̸=
̸
j
T (2)
jh
T
+

k̸=
̸
i
T (1)
ik
T
⎞
⎠
⎞
= −
m1

k=1
T (1)
ik
T
−
m2

h=1
T (2)
jh
T
= η(1)
i
+ η(2)
j
η
Thus we can see that Z = min(Z1, Z2
Z ) is the time until absorption in the
Markov process X.
□
Example 9.4 and theorem 9.11 already sufﬁce to prove the following powerful

Phase–Type Distributions
183
Theorem 9.14 The class of phase–type distributions is dense (in terms of
weak convergence) within the class of all distributions on R+
0 .
Proof: Let F : R+
0 →[0, 1] denote any non–negative distribution function.
Since it is bounded, monotone and right–continuous, we can approximate F
by a step function G with countably many jumps at (tn : n ∈N0), where
tn < tn+1 for all n ∈N. The error ε > 0 of approximation can be chosen
arbitrarily small such that |F(t) −G(t)| < ε holds for all t ≥0.
If t0 = 0, i.e. if there is a jump of G at zero, we can write
G = p0 · δ0 + (1 −p0) · ˜G
with p0 = G(0) and ˜G = (G−G(0))/(1−p0). The Dirac distribution function
δ0 is a phase–type distribution with m = 0 and αm+1 = 1. In view of example
9.4 and theorem 9.11, it now sufﬁces to show that we can approximate the
function ˜G by a ﬁnite mixture of Erlang distributions. First we ﬁnd a truncation
point T of ˜G such that G(T) > 1 −ε. Then there is a number N ∈N such
that tn > T for all n > N. Thus ˜G can be approximated by
H =
N−1

n=1
( ˜G(tn) −˜G(tn−1)) · δtn + (1 −˜G(tN)) · δtN
with an error bounded by ε.
For every n = 1, . . . , N we approximate the Dirac distribution δtn by a suit-
able Erlang distribution. This possible because of the following argument: The
variance of an Erlang distribution Ekλ
k
of order k with parameter k · λ is given
by (k · λ2)−1 (see exercises) and thus tends to zero as k grows larger. Since
the mean of such an Erlang distribution is 1/λ (see exercises), Chebyshev’s in-
equality tells us that the sequence (Ekλ
k
: k ∈N) converges in probability (and
hence weakly) towards δtn if we chose λ = 1/tn. This means that there is a
number K ∈N such that the distribution function Hn
H of an EK/tn
K
distribution
satisﬁes |Hn
H (t) −δtn(t)| < ε for all t ≥0.
If we pursue the above approximation method for every n = 1, . . . , N and
deﬁne
˜H =
N−1

n=1
( ˜G(tn) −˜G(tn−1)) · Hn
H + (1 −˜G(tN)) · HN
H
then we obtain an approximation bound |H −˜H| < ε. According to example
9.4 and theorem 9.11 the distribution ˜H is phase–type.
In summary, we have approximated F by p0 · δ0 + (1 −p0) · ˜H with an ap-
proximation bound of 3 · ε. This proves our statement.
□

184
AN INTRODUCTION TO QUEUEING THEORY
Notes
Phase–type distributions have been introduced in Neuts [62] as a generalization
of the Erlang and hyper–exponential distribution. A classical introduction to
phase–type distributions is given in Neuts [65]. Statistical methods for ﬁtting
PH distributions are given in Asmussen et al. [7]. Phase–type distributions
with inﬁnitely many phases are introduced in Shi et al. [78], and Shi and Liu
[79].
The name of a superﬂuous state as well as the motivating example at the be-
ginning of the section have been taken from Latouche and Ramaswami [52].
The proofs of the closure properties have been chosen to be as constructive and
illustrating as possible. More classical proofs via comparison of the Laplace
transforms can be found in Neuts [65].
Exercise 9.1 Show that the density f(t) of a phase–type distribution function
is strictly positive for all t > 0.
Exercise 9.2 Compute the Laplace–Stieltjes transform, all moments as well
as the squared coefﬁcient of variation for the exponential, the Erlang, and the
hyper–exponential distribution.
Exercise 9.3 Use the results from exercise 9.2 in order to show that the Erlang
distribution with parameter nλ and n degrees of freedom is the n–fold convo-
lution of an exponential distribution with parameter nλ. Employ this result for
a simple proof of formula (6.2).
Exercise 9.4 Consider two machines running independently at the same time.
The one has a life time which is distributed as a generalized Erlang with two
stages and parameters λ1 and λ2. The other machine’s life time has a hyper–
exponential distribution with density f(t) = p · µ1e−µ1t + (1 −p) · µ2e−µ2t.
As soon as a machine fails, it is given to repair which takes an exponentially
distributed amount of time with parameter κ. After repair it starts working
immediately. Determine the distribution of the time until both machines are
broken down.
Exercise 9.5 Consider the M/PH/k queue with Poisson input and phase–type
service time distribution for all its k servers. Derive a description for this queue
in terms of a (k + 1)–dimensional Markov process.
Exercise 9.6 Find the stationary distribution for the M/PH/1 queue.

Chapter 10
MARKOVIAN ARRIVAL PROCESSES
In this chapter we are going to generalize the concept of a Poisson process in
three steps. We shall arrive thereby at the class of so–called Batch Markovian
Arrival Processes (shortly BMAPs) that comprises a great variety of processes
and so provides much more realistic modelling tools than the class of processes
considered so far can offer. The big advantage of the very procedure of gener-
alizing basic Markovian concepts is that it essentially keeps a lot of Markovian
behaviour.
1.
The PH renewal process
The Poisson process is a renewal process with exponentially distributed re-
newal intervals (see example 6.5). In the last section we have introduced a
powerful generalization of the exponential distribution. As a ﬁrst step to go
beyond the Poisson process, it seems natural to replace the exponential dis-
tribution of the renewal intervals by a phase–type distribution. This leads us
immediately to the class of PH renewal processes. To be precise, a renewal
process N = (Nt
N : t ∈R+
0 ) with phase–type distributed renewal intervals is
called a PH renewal process.
While a concrete determination of the renewal function or the time–dependent
behaviour of an arbitrary renewal process is hard to derive, we have seen in ex-
ample 6.5 that in the case of exponential renewal intervals, i.e. for the Poisson
process, simple expressions can be derived. Since an exponential distribution
is a phase–type distribution of order m = 1, a PH renewal process clearly
generalizes the Poisson process. We want to show that the behaviour of PH
renewal processes can still be described by rather simple expressions.

186
AN INTRODUCTION TO QUEUEING THEORY
To this aim, we ﬁrst will give a Markovian description of a PH renewal process.
For a phase–type distribution PH(α, T), the remaining time Z until absorp-
tion, given that the current phase is i, can be expressed by
P(Z ≤t|X0 = i) = 1 −eieT·t1
and thus has the same form as a PH(ei, T) distribution, with ei denoting the
ith canonical row base vector. This is the conditional memoryless property
of the phase–type distribution.
If we keep track of the current phase of the PH distribution during renewal
intervals, then we obtain a Markovian description (N, J ) of the PH renewal
process N. This is derived as follows (cf. notations in deﬁnition 1). Clearly,
the state space of Y = (N, J ) is E = N0 × {1, . . . , m}, with Nt
N denoting the
number of arrivals until time t and Jt
J being the current phase at time t.
The holding times depend only on the current phase of the PH distribution, i.e.
λn,i = λi for all n ∈N0. Since N is a renewal process, the state transitions
of the embedded Markov chain X are restricted by pn,i;m,j = 0 for m < n or
m > n + 1.
Thus there are state transitions from (m, i) to (m + 1, j) or to (m, j), which
are called transitions with or without arrivals, respectively. For transitions
without arrivals, there is no renewal event and hence no absorption for the PH
distribution, which means that these transitions are described by the parameter
matrix T of the PH distribution of the renewal intervals. For transitions with
arrivals, we observe the following dynamics.
Being in phase i, there is an absorption (hence a renewal event) with rate ηi.
After that, a new PH–distributed renewal interval begins and a phase is chosen
according to the initial phase distribution α. Hence transitions with arrivals are
determined by the rate matrix A := ηα with η := −T1.
After ordering the state space E = N0 ×{1, . . . , m} lexicographically, we can
write the inﬁnitesimal generator matrix G of the Markov process Y = (N, J )
as a block matrix
G =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
T
A
T
A
T
A
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
with the non–speciﬁed blocks being zero matrices, and A := ηα.
Before analyzing the behaviour of this process, we shall generalize its struc-
ture two steps further. Then we will pursue an analysis from a more general
viewpoint.

Markovian Arrival Processes
187
2.
From PH renewal processes to MAPs
An essential feature of the PH renewal process is that immediately after an
arrival (i.e. a renewal event) the phase distribution always is α. This makes it
a real renewal process with iid renewal intervals. However, in modern com-
munication systems like the internet or other computer networks there may be
strong correlations between subsequent inter–arrival times. Thus it is a natural
idea to introduce a dependence between the subsequent renewal intervals. This
can be done without changing the block structure of the generator.
Writing down the row vectors of the matrix A, we observe
A =
⎛
⎜
⎛
⎝
⎜
η1 · α
...
ηm · α
⎞
⎟
⎞
⎠
⎟
meaning that the row entries differ only by a scalar ηi and thus the new phase
after an arrival is chosen independently of the phase immediately before that
arrival.
If we relax this restriction, we arrive at a new matrix
A′ =
⎛
⎜
⎛
⎝
⎜
η1 · α1
...
ηm · αm
⎞
⎟
⎞
⎠
⎟
with the only requirement that αi1 = 1 for all i = 1, . . . , m. Here the phase
distribution after an arrival depends on the phase immediately before that ar-
rival. However, the requirement αi1 = 1 in connection with the fact that
η = −T1 simply restates the observation that the row entries of a generator
matrix sum up to zero.
Thus there is no real restriction in choosing A′. If we denote D0 := T and
D1 := A′ as usually done in the literature, we arrive at a generator
G =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
D0
D1
D0
D1
D0
D1
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
with no restrictions on the matrices D0 and D1 except that G be a generator
matrix and D1 is non–negative. A Markov process with such a generator is
called Markovian Arrival Process (or shortly MAP). These arrival processes
play an important part in today’s queueing models.

188
AN INTRODUCTION TO QUEUEING THEORY
Differing from PH renewal processes as a special case, MAPs are not renewal
but semi–Markov processes (see section 2). The methods we will employ for
their analysis still allow a further generalization, which also is motivated by
observations of trafﬁc in modern communication networks.
3.
From MAPs to BMAPs
In a computer network it is not uncommon that a client sends various jobs
to the server at the same time. After being sent they are treated as separate
entities. For our modelling tools concerning the arrival process this means that
we observe several arrivals at the same time. These are called batch arrivals
or arrivals in batches.
It is a simple matter to include this into our concept of Markovian arrival
processes. We observe that for MAPs the matrix D0 on the main block di-
agonal contains the transition rates without arrivals. The matrix D1 on the ﬁrst
upper block diagonal contains transition rates with single arrivals. The lexi-
cographic order of the state space implies that any (positive) entry in the kth
upper block diagonal of the generator matrix G would be a transition rate for
an arrival of k jobs at the same time.
Hence a natural extension of MAPs which include the possibility of batch ar-
rivals are Markov processes with a generator of the block structure
G =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
D0
D1
D2
D3
. . .
D0
D1
D2
...
D0
D1
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
again with the non–speciﬁed blocks (i.e. all entries in the lower block diago-
nals) being zero matrices. Such processes are called Batch Markovian Ar-
rival Processes (or shortly BMAPs). A matrix Dk contains the transition rates
for a batch arrival of size k, i.e. the event that k jobs arrive at the same time. The
sequence ∆= (Dn : n ∈N0) uniquely determines the generator G and thus
the BMAP. We call it the characterizing matrix sequence or the sequence of
characterizing matrices for the BMAP.
The entry Dk;ij with k ≤1 and i, j ≤m indicates the transition rate for a
batch arrival of size k occuring in connection with a phase transition from i to
j. The entry D0;ij with i ̸≠
j ≤m indicates the transition rate for a phase
transition from i to j without an arrival. Finally, the entry D0;ii with i ≤m
indicates the negative parameter of the exponential holding time in any state
(n, i) with n ≥0 and i ≤m.

Markovian Arrival Processes
189
Clearly, MAPs and thus PH renewal processes are special cases of BMAPs.
The following examples further show the great versatility of the BMAP con-
cept.
Example 10.1 Markov–modulated Poisson process
One of the most prominent examples of a MAP is the Markov–modulated
Poisson process (or shortly: MMPP). It is a doubly stochastic Poisson process
in the following sense: First we deﬁne an underlying (or governing) Markov
process X with a ﬁnite number m of states. This is sometimes referred to as
the environment process. Denote its generator by R. Depending on the state
of X, single arrivals occur with one of the rates λ1, . . . , λm. Deﬁne the diag-
onal matrix Λ = diag(λ1, . . . , λm). Then we obtain a BMAP speciﬁcation of
the MMPP by setting D0 = R −Λ, D1 = Λ, and Dk = 0 for k ≥2. This
process is often used to model a semi–Poisson behaviour where the rates of the
Poisson arrivals depend on a changing environment.
Example 10.2 Interrupted PH renewal process
Consider a source that emits arrivals according to a PH renewal process. This
source may not be working during certain time intervals, due to failure or other
reasons of inactivity. Denote the parameters for the second PH distribution
(governing the durations of inactivity) (β, U) where U shall be of order n.
Denote the number of active states by m. If the source is in an active state
i, it may change to inactivity with rate γiγ and to another active state k with
rate Sik
S . Arrivals in state i occur with rate ηi, and the initial distribution of
active states is given by the row vector α. Deﬁne the off–diagonal entries of
the square matrix S to be the rates Sik
S
and set Sii
S
:= −γiγ −
k̸=
̸
i Sik
S
−ηi.
Then the interrupted PH renewal process has a BMAP speciﬁcation
D0 =

S
γβ
να
U

and
D1 =

ηα
0
0
0

with ν = −U1n, Dk = 0 for k ≥2. The name interrupted PH renewal
process comes from the fact that this process models the behaviour of a source
which normally emits arrivals according to a PH renewal process but may be
interrupted for PH–distributed periods of time. A classical special case of
the interrupted PH renewal process is the interrupted Poisson process (shortly:
IPP), where n = m = 1.
Example 10.3 As we have seen in the beginning of section 9 the time until
an overﬂow occurs in a Markovian queue can be modelled by a PH distribu-
tion. Likewise, the output process of Markovian queueing networks with ﬁnite
buffers can be described as a MAP.

190
AN INTRODUCTION TO QUEUEING THEORY
A useful feature of BMAPs is the closure property under superposition. Let
Ni
N = (N(i)
t
N
: t ≥0) for i = 1, 2 denote two arrival processes. Then the
process N = (Nt
N
: t ≥0) deﬁned by Nt
N
:= N(1)
t
N
+ N(2)
t
N
is called the
superposition of N1
N and N2
N . Analogously to the proof of theorem 9.13 one
can show
Theorem 10.4 If N1
N
and N2
N
are two BMAPs with characterizing matrices
(D(i)
n : n ∈N0) for i = 1, 2, then the superposition N = N1
N + N2
N is a BMAP
with characterizing matrices Dn = D(1)
n
⊕D(2)
n
for all n ∈N0.
4.
Distribution of the Number of Arrivals
The most important random variable of a BMAP is the number Nt
N of arrivals
until some time t. Since BMAPs are Markov processes, we can express all
transition probabilities in terms of the generator matrix G and thus in terms
of the characterizing matrices Dn. According to theorem 3.7, the transition
probability matrix P(t) of a BMAP Y = (N, J ) with generator G is given by
P(t) = eG·t :=
∞

n=0
tn
n!Gn
(10.1)
for all t ≥0, with Gn denoting the nth power of the matrix G. Here the entry
Pk,i
P
;n,j(t) = P(Nt
N = n, Jt
J = j|N0
N
= k, J0
J
= i) indicates the conditional
probability that until time t there have been n arrivals and the phase at time t
is j given that at time 0 there already have been k arrivals and the phase is i.
In order to ﬁnd an expression of P(t) in terms of the matrices Dn, we need to
introduce convolutions of matrix sequences. Let M = (Mn
M
: n ∈N0) and
K = (Kn
K : n ∈N0) denote two sequences of m×m matrices. The convolution
M ∗K of these sequences is deﬁned as the sequence L = (Ln : n ∈N0) of
m × m matrices with Ln := n
k=0 Mk
M Kn
K −k for all n ∈N0.
Deﬁne the convolutional powers of a matrix sequence M by the initial se-
quence M∗0 := (I, 0, 0, . . .) and recursively M∗(n+1) := M∗n ∗M for all
n ∈N0. If we denote the kth matrix of a sequence M∗n by M∗n
k , then we can
write M∗0
k
= δk0 · I for all k ∈N0, with δk0 denoting the Kronecker func-
tion and I denoting the m × m identity matrix. Further we have by deﬁnition
M∗1
k = Mk
M for all k ∈N0. With these deﬁnitions we can state
Lemma 10.5 For all n ∈N0, the nth power of the generator matrix G of a
BMAP has block entries
Gn
kl = ∆∗n
l−k

Markovian Arrival Processes
191
for all k ≤l ∈N0, and Gn
kl = 0 for all k > l ∈N0.
Proof: Fix any k, l ∈N0. Clearly, G0 is the identity matrix, which proves the
statement for n = 0. Now assume that the statement is true for some n ∈N0.
Then
Gn+1
kl
=
∞

h=0
Gn
khGhl =
l

h=k
Gn
khGhl
since Gn
kh = 0 for h < k by induction hypothesis and Ghl = 0 for h > l
as G is an upper triangular block matrix. In particular we obtain Gn+1
kl
= 0
for k > l and we can assume k ≤l. The induction hypothesis as well as the
structure of G yield Gn
kh = ∆∗n
h−k and Ghl = Dl−h. Hence we obtain
Gn+1
kl
=
l

h=k
∆∗n
h−kDl−h =
l−k

h=0
∆∗n
h Dl−k−h = ∆∗(n+1)
l−k
which completes the proof.
□
Deﬁne the convolutional exponential of a matrix sequence M by the se-
quence e∗M·t of matrices with the kth matrix being ∞
n=0
tn
n!M∗n
k
for all
k ∈N0. Further deﬁne the matrices Pkl
P (t) = (Pk,i
P
;l,j(t))i,j≤m. With these
deﬁnitions and the above lemma we arrive at
Corollary 10.6 The (k, l)th block entry of the transition probability matrix of
a BMAP with characterizing sequence ∆is given by
Pl
P −k(t) := Pkl
P (t) =

e∗∆·t
l−k :=
∞

n=0
tn
n!∆∗n
l−k
for all k ≤l ∈N0, and Pkl
P (t) = 0 for all k > l ∈N0.
It is not surprising that we obtain Pkl
P (t) = 0 for k > l as a BMAP cannot count
more arrivals in any subset of a time interval than in the time interval itself.
Furthermore, it is not surprising to see that the block entry Pkl
P (t) depends only
on the difference l −k, since future arrivals depend only on the current phase
and not on the number of arrivals observed in the past. Thus we can abbreviate
notations by Pk
P (t) := P0
P ,k(t). An immediate consequence is
Corollary 10.7 The matrix containing the probabilities that within a time in-
terval of length t there are no arrivals is given by
P0
P (t) = eD0·t

192
AN INTRODUCTION TO QUEUEING THEORY
for all t ≥0.
Deﬁne τ := min{t ≥0 : Nt
N > 0) as the stopping time until the ﬁrst arrival.
Then the expectation matrix E(τ) of τ, having dimension m × m and entries
E(τ · 1Jτ
J =j|J0
J = i), is given by
E(τ) =
 ∞
0

eD0·t dt = −D−1
0
(10.2)
For a BMAP with characterizing matrices (Dn : n ∈N0) deﬁne the matrix
D := ∞
n=0 Dn. This describes the transition rates of the marginal process
J = (Jt
J : t ≥0), which is called the phase process of the BMAP. Deﬁne
the transition probability matrix of J by P Φ(t) = (P Φ
ij
P (t))i,j≤m with entries
P Φ
ij
P (t) := P(Jt
J = j|J0
J = i). For this we obtain
Theorem 10.8 The transition probability matrix of the phase process J is
given by
P Φ(t) =
∞

k=0
Pk
P (t) = eD·t
for all t ≥0 and i, j ≤m.
Proof: The ﬁrst equation holds by deﬁnition, since the phase process is the
marginal process of (N, J ) in the second dimension. The second equation is
obtained via corollary 10.6 and Fubini’s theorem as
∞

k=0
Pk
P (t) =
∞

n=0
tn
n!
∞

k=0
∆∗n
k =
∞

n=0
tn
n!
 ∞

k=0
Dk
n
= eD·t
where the second equality is due to the relation
∞

k=0
∆∗n
k =
 ∞

k=0
Dk
n
(10.3)
which the reader may prove as an exercise.
□
5.
Expected Number of Arrivals
The expressions Pk
P (t) will help us to derive a simple representation of the
expected number E(Nt
N ) of arrivals until time t. To this aim we ﬁrst derive an
expression for the z–transform which is deﬁned as N∗
t
N (z) := ∞
n=0 Pn
P (t)zn

Markovian Arrival Processes
193
for z ∈C with |z| ≤1. The z–transform of the matrices (Dn : n ∈N0) is
deﬁned as D(z) := ∞
n=0 Dnzn. Using this, we obtain
Theorem 10.9 The z–transform of a BMAP having characterizing matrices
(Dn : n ∈N0) is given by
N∗
t
N (z) = eD(z)·t
for all z ∈C with |z| ≤1.
Proof: The deﬁnition, together with corollary 10.6, yields
N∗
t
N (z) =
∞

n=0
∞

k=0
tk
k!∆∗k
n zn =
∞

k=0
tk
k!
∞

n=0
∆∗k
n zn
Since the transform of a k–fold convolution of a sequence equals the kth power
of the transform of the sequence (see exercises), we obtain further
N∗
t
N (z) =
∞

k=0
tk
k! (D(z))k = eD(z)·t
which is the statement.
□
Let π denote the stationary probability vector of the phase process, satisfying
πD = 0. If the initial phase distribution of a BMAP is π, then we say that the
BMAP starts in phase equilibrium. Now we can state
Theorem 10.10 The expected number Eπ(Nt
N ) of arrivals until time t, given
that the BMAP starts with initial phase distribution π, is determined by
Eπ(Nt
N ) = t · π
∞

k=1
k · Dk1
Proof: The ﬁrst moment can be derived from the z–transform via
Eπ(Nt
N ) = π d
dz
∞

n=0
tn
n!(D(z))n
,,,
,,
,,
,,
z=1
1
= π
∞

n=0
tn
n!
n

h=1
Dh−1
∞

k=1
k · Dk Dn−h 1
Now the statement follows from πD = 0 and D1 = 0 which implies that the
right–hand sum over h is zero except for n = h = 1.
□

194
AN INTRODUCTION TO QUEUEING THEORY
The term λ := π ∞
k=1 kDk 1 is called the mean arrival rate of the BMAP.
For the special case of a PH renewal process, this term equals π(ηα)1 = πη.
Furthermore the stationary phase distribution π is easily determined via corol-
lary 7.2. Thus we obtain
Corollary 10.11 For a PH renewal process with parameters (α, T) starting
in phase equilibrium, the expected number Eπ(Nt
N ) of arrivals until time t is
given by
Eπ(Nt
N ) = t · πη
with η = −T1. The stationary phase distribution π is given by
π =
1
−αT −11
 ∞
0

αeT·t dt
Proof: The ﬁrst statement is a speciﬁcation of theorem 10.10. The expression
for π is veriﬁed by
πD =
1
−αT −11
 ∞
0

αeT·t dt (T + ηα)
=
1
−αT −11

α
 ∞
0

eT·tT dt +
 ∞
0

αeT·tη dt α

=
1
−αT −11 (α(−I) + α) = 0
with I denoting the identity matrix, and
 ∞
0

αeT·t dt 1 = α
 ∞
0

eT·t dt 1 = −αT −11
□
The above PH renewal process N is a delayed renewal process with initial
delay X0 ∼PH(π, T) and renewal intervals Xn
X
∼PH(α, T) for n ∈N.
The elementary renewal theorem 6.12 then states that limt→∞E(Nt
N )/t =
1/E(X1). Thus we obtain another expression for the mean of a phase–type
distributed random variable (cf. corollary 9.9).
Corollary 10.12 For a PH(α, T) distributed random variable X the expec-
tation is given by E(X) = (πη)−1.

Markovian Arrival Processes
195
Notes
The ﬁrst presentation of the PH renewal process has been given in Neuts [63].
The MAP in the present formulation has been introduced ﬁrst in Lucantoni
et al. [56] as a generalization of the PH renewal process and the Markov–
modulated Poisson process. Its further generalization to the BMAP with batch
arrivals has been introduced in Lucantoni [54]. An early algorithms for a com-
putation of the transition probabilities can be found in Neuts and Li [67]. The
calculus of matrix convolutions that leads to explicit expressions for the transi-
tion probabilities of BMAPs has been introduced in Baum [9] or Breuer [21].
The class of BMAPs is equivalent to the class of versatile Markovian point
processes (or N–processes) introduced in Neuts [64]. However, this formula-
tion is more complicated and does not yield as simple an anlysis of the respec-
tive queueing systems. For generalizations of BMAPs see Pacheco and Prabhu
[68], Baum and Kalashnikov [11, 13], or Breuer [19, 21]. A result which is
analogous to theorem 9.14 is that the class of all MAPs is dense within the
class of all marked point processes (see Asmussen and Koole [6]).
An extensive treatment on the history of the BMAP, along with many refer-
ences, can be found in Lucantoni [55]. Statistical methods for ﬁtting MMPP
and BMAP models are given in Ryden [76] and Breuer [18].
Exercise 10.1 Prove theorem 10.4.
Exercise 10.2 Prove equation (10.3) as well as
∞

n=0
∆∗k
n zn = (D(z))k
for z ∈C with |z| < 1.
Exercise 10.3 Give a model (in terms of a PH renewal process) for the number
of orders the plumber of exercise 6.4 receives.
Exercise 10.4 Show that an IPP is uniquely determined by the ﬁrst three mo-
ments of the inter–arrival time distribution.
Exercise 10.5 In a telephone network data transmission via the package voice
system works as follows. The language source is digitalized and divided into
packages, which are to be transmitted. A language source switches between
”talk spurts” and ”silent periods”. Thus a model in terms of an IPP seems
reasonable.
Measurements yield a mean inter–arrival time of 3 ms for the packages. The

196
AN INTRODUCTION TO QUEUEING THEORY
squared coefﬁcient of variation is 300. Further it is known that a silent period is
about two times as long as a talk spurt. Adjust the IPP to these measurements.
Exercise 10.6 Describe the BMAP/PH/k queue in terms of a Markov process
with (k + 2)–dimensional state space.

Chapter 11
THE GI/PH/1 QUEUE
In section 1, we have analyzed one of the classical semi–Markovian queue-
ing systems, the GI/M/1 queue. For practical applications, this model has the
disadvantage that the assumption of exponential service times is often inap-
propriate for the actual service time distribution governing the system. More
typical service times often are distributed like generalized Erlang or Cox distri-
butions, or special distributions like the lognormal or Weibull type. The former
are special cases of phase–type distributions, the latter can be approximated by
them. Thus the wish to extend the results for the GI/M/1 queue to GI/PH/1
queues is understandable.
It will turn out in the presentation of this chapter that the analysis of GI/PH/1
queues can be performed in a strikingly similar manner to the GI/M/1 analysis.
This is one of the main reasons for the success of the phase–type concept in
queueing theory.
The GI/PH/1 queue is characterized by the following features. Arrivals are
generated by a renewal process with inter–arrival times distributed by a distri-
bution function H with 0 < E(H) < ∞. The service times are iid according to
a PH(α, T) distribution of order m ∈N, with αm+1 = 0. There is one server,
and the service discipline is FCFS. The waiting room capacity is inﬁnite such
that there are no users lost.
In order to keep a complete description of the system state, it is not sufﬁcient
anymore to remember the number of users in the system only, but we need to
keep track of the current phase of the service time distribution, too. Thus we
examine the process Q = (Qt : t ≥0) with Qt = (Nt
N , Jt
J ) for all t ≥0, where
Nt
N and Jt
J denote the number of users in the system and the current phase of
service at time t, respectively. If the system is empty, i.e. Nt
N = 0, there is no

198
AN INTRODUCTION TO QUEUEING THEORY
service and we do not need to keep track of a phase of service. In this case we
set Jt
J = 0. Thus the state space of Q is E = {0, 0} ∪N × {1, . . . , m}.
1.
The Embedded Markov Chain
As for the GI/M/1 queue we observe that at times of arrivals the conditions for
a prognosis of the future of the system are less difﬁcult, since we know that the
time until the next arrival is distributed by F. This is due to the iid inter–arrival
times and the independence of the arrival process from the rest of the system.
Hence we can construct a Markov chain embedded at arrival instants.
Deﬁne Tn
T as the time of the nth arrival and Xn
X := (NT
N
n
T −1, JT
J
n
T ) for all n ∈N.
Since at times Tn
T there is always at least one user in the system (namely the one
that has just arrived), the state space of X = (Xn
X : n ∈N) is N0×{1, . . . , m}.
If we know that the phase of service currently is j, then we also know that
the time until the next service completion will be distributed by a PH(ej, T)
distribution, with ej denoting the jth canonical row base vector. Since we
further know that the time until the next arrival after Tn
T will be distributed by
F (the inter–arrival time has just begun), the chain X is Markovian.
Now we want to determine the transition matrix ˜P of X. Since the state space
of X is two–dimensional, the structure of ˜P must become more complicated
than for the analogue of the GI/M/1 queue. However, we can order the state
space of X lexicographically as
{(0, 1), . . . , (0, m), (1, 1), . . . , (1, m), (2, 1), . . .}
such that the transition matrix ˜P will have a block structure. The ﬁrst dimen-
sion of the state space shall be called the level of the process, while the second
dimension is called the phase. The chain pr2(X), with pr2 denoting the pro-
jection on the second dimension, will be called the (embedded) phase process.
The general structure of ˜P is determined by two considerations. First, between
two arrival instants the number of users in the system can increase by at most
one. Second, as long as the system is not empty, the change of the number of
users between two arrival instants Tn
T and Tn
T +1 does not depend on the number
of users in the system at time Tn
T . Thus we obtain the structure
˜P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
B0
A0
B1
A1
A0
B2
A2
A1
A0
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟
(11.1)
for the transition matrix ˜P, with the non–speciﬁed entries being zero matrices.
We see that the matrix ˜P has a block Toeplitz structure with one upper diag-
onal and special boundary entries in the ﬁrst column. In terms of levels, the

The GI/PH/1 Queue
199
matrix is skip–free to the right as in the GI/M/1 case. Because of the similar-
ity to the GI/M/1 case, such a matrix is also called a GI/M/1 type matrix.
Here, the (i, j)th entry Ak;ij of the matrix Ak indicates the probability that
between two consecutive arrivals instants Tn
T and Tn
T +1, k users are served and
the phase of service changes from i at time Tn
T to j at time Tn
T +1. The entries
Bk;ij indicate the probabilities that between two consecutive arrivals instants
Tn
T and Tn
T +1, at least k + 1 users are served and the phase of service changes
from i at time Tn
T to j at time Tn
T +1.
In order to ﬁnd expressions for the matrices Ak and Bk, we deﬁne the following
family of matrices. For every t ≥0, k ∈N0, and i, j ∈{1, . . . , m} let
Pk
P ;ij(t) denote the probability that during the interval [0, t] there are k service
completions and the phase of service at time t is j given that the phase of
service at time 0 has been i. Further deﬁne the respective m × m matrices
Pk
P (t) = (Pk
P ;ij(t))i,j≤m. Then we can write
Ak =
 ∞
0

Pk
P (t) dH(t)
and
Bk =
∞

i=k+1
Ai
for all k ∈N0. The matrices Pk
P (t) can be determined according to the ex-
pression given in corollary 10.6 by specifying ∆= (T, ηα, 0, 0, . . .), with
η = −T1. Since A := ∞
n=0 An is stochastic, we immediately obtain
Bk = 1α −
k

i=0
Ai1α
for all k ∈N0.
2.
Stationary Distribution at Arrival Instants
From now on we shall assume that the stability condition E(H) > −αT −11
for the embedded Markov chain X is satisﬁed. In the next section it is shown
that then a stationary distribution for X does exist. In this section we want to
show how this can be determined.
Denote the stationary distribution for X by x = x ˜P and write
x = (xn : n ∈N0) = (xni : n ∈N0, 1 ≤i ≤m)
with xni being the stationary probability that the chain X is in state (n, i). The
vectors xn contain the stationary probabilities of the chain X being in level n.
For the GI/M/1 queue we could derive a geometric structure νn
ν +1 = νn
ν ξ for
the stationary distribution ν of the embedded Markov chain. In the present

200
AN INTRODUCTION TO QUEUEING THEORY
case of a GI/PH/1 queue we will have a similar structure for the distribution
x, namely the relation xn+1 = xnR where now the factor R is a matrix.
To underline the analogy, such a distribution is called a matrix–geometric
distribution and R is called rate matrix.
We will ﬁrst deﬁne the matrix R and then show that x indeed is matrix–
geometric with rate matrix R. For all phases i, j ≤m and levels k ≥0,
deﬁne the taboo probabilities (cf. deﬁnition (8.11))
kP (n)
k,i;k+1,j := P(Xn
X = (k+1, j), pr1(Xh) ̸≠
k ∀1 ≤h < n|X0 = (k, i))
that the chain X enters level k + 1 in phase j after n steps and does not enter
level k before, given that it starts in level k and phase i.
The structure of ˜P is self–similar in the sense that if we delete the ﬁrst n
(block) rows and columns of ˜P for any n ∈N, we obtain the same matrix as
if we deleted only the ﬁrst row and column. This implies that the probabilities
kP (n)
k,i;k+1,j are independent of k ∈N0. Hence we can deﬁne
rij :=
∞

n=1
kP (n)
k,i;k+1,j
for all i, j ∈{1, . . . , m}, independently of k ≥0. The value rij is the expected
number of visits of the chain X to the state (k + 1, j) before returning to level
k if X is started in state (k, i). Finally, we deﬁne the matrix R = (rij)i,j≤m
and call it the rate matrix.
Theorem 11.1 The stationary distribution x = (xk : k ∈N0) of X satisﬁes
the relation
xk+1 = xkR
for all k ∈N0.
Proof: Fix any level k ∈N0. By conditioning on the last time and phase of
the last visit to level k we obtain the relation
P (n)
k
P +1,j;k+1,j = kP (n)
k+1,i;k+1,j +
m

i=1
n

r=1
P (r)
k
P +1,j;k,i · kP (n−r)
k,i;k+1,j
for all n ≥1.
If we add these equations for n = 1, . . . , N, divide both sides by N and then
let N tend to inﬁnity, the left–hand side tends to xk+1,j according to corollary

The GI/PH/1 Queue
201
2.28. Since the sum ∞
n=1 kP (n)
k+1,i;k+1,j is ﬁnite if X is positive recurrent, the
ﬁrst term on the right–hand side tends to zero. The second term equals
lim
N→∞
m

i=1
1
N
N

n=1
n

r=1
P (r)
k
P +1,j;k,i · kP (n−r)
k,i;k+1,j
=
m

i=1
lim
N→∞
1
N
N

r=1
P (r)
k
P +1,j;k,i · lim
N→∞
N−r

n=1
kP k,i;k+1,j =
m

i=1
xki · rij
by corollary 2.28 and the deﬁnition of R.
□
The matrix R contains the expected number of visits to any level l + 1 ∈N
between two consecutive visits to level l. In case of positive recurrence this
matrix is entry–wise ﬁnite by theorem 2.24. For the powers of R we obtain the
following interpretation:
Theorem 11.2 For any k ∈N, the (i, j)th entry of the matrix Rk indicates the
expected number of visits to the state (l + k, j) between two consecutive visits
to level l ∈N0, given that the chain X starts in state (l, i).
Proof: The statement holds for k = 1 by deﬁnition of R. Now we assume that
it holds for some k ∈N and want to show the induction step to k + 1. For
n ≥k + 1 the relation
lP (n)
l,i;l+k+1,j =
m

h=1
n

r=0
lP (r)
l,i;l+k,h · l+kP (n−r)
l+k,h;l+k+1,j
=
m

h=1
n

r=0
lP (r)
l,i;l+k,h · lP (n−r)
l,h;l+1,j
is obtained after conditioning on the time r and the phase h of the last visit to
level l + k before visiting level l + k + 1. For n ≤k both sides of the equation
are zero, such that we can sum over all n ≥0. The left–hand side sums up to
the desired expectation for the level k + 1. On the right–hand side we obtain
m

h=1
∞

n=0
n

r=0
lP (r)
l,i;l+k,h · lP (n−r)
l,h;l+1,j =
m

h=1
∞

r=0
lP (r)
l,i;l+k,h
∞

n=0
lP (n)
l,h;l+1,j
=
m

h=1
Rk
ihrhj
= lim
N→∞
m

i=1
1
N
N

r=1
P (r)
k
P +1,j;k,i ·
N−r

n=1
kP (n)
k,i;k+1,j
(n)

202
AN INTRODUCTION TO QUEUEING THEORY
which completes the induction step.
□
There is an interesting relation between the matrix R and its powers, which at
the same time yields a method to compute R without needing to calculate the
n–step taboo probability matrices lP (n) by which R is deﬁned. This is given
in
Theorem 11.3 If X is positive recurrent, then the matrix R is the (entry–wise)
minimal non–negative solution to the matrix equation
M =
∞

n=0
MnAn
It can be obtained as the limit R = limk→∞M(k) with M(0) := 0 and
M(k + 1) := ∞
n=0 M(k)nAn for all k ∈N0.
Proof: First we observe lP (1)
l,i;l+1,j = A0;ij. For n ≥2 we obtain
lP (n)
l,i;l+1,j =
m

h=1
∞

k=1
lP (n−1)
l,i;l+k,h · Ak;hj
by conditioning on the state (l+k, h) from which the level l is entered for time
n. If we sum up these equations for all n, we obtain R = ∞
n=0 RnAn.
For the second statement, consider the sequence M = (M(k) : k ∈N0).
Clearly, M(1) ≥M(0) and R ≥M(0) entry–wise. The fact that
M(k + 1) −M(k) =
∞

n=0
(M(k)n −M(k −1)n)An ≥0
R −M(k) =
∞

n=0
(Rn −M(k −1)n)An ≥0
yields by induction that the sequence M is entry–wise monotonically increas-
ing and M(k) ≤R for all k ∈N. Hence there is a matrix M∗= limk→∞M(k)
with M∗≤R. Thus we can use the dominated convergence theorem to verify
M∗= lim
k→∞M(k) = lim
k→∞
∞

n=0
M(k −1)nAn =
∞

n=0
lim
k→∞M(k −1)nAn
=
∞

n=0
(M∗)nAn

The GI/PH/1 Queue
203
such that M∗is indeed a solution to the matrix equation.
Every other non–negative solution M′ to the matrix equation must satisfy
M′ ≥A0 = M(1). But M′ ≥M(k) implies
M′ =
∞

n=0
(M′)nAn ≥M(k)nAn = M(k + 1)
and thus by induction M′ ≥M(k) for all k ∈N and in the limit M′ ≥M∗.
Hence M∗is minimal.
It remains to show that R ≤M∗. Deﬁne the matrices
lP (n)
r
:=
 
lP (n)
l,i;l+r,j
!
i,j≤m
for all n, r ∈N0 and remember that their deﬁnition is independent of the start
level l. The matrix lP (n)
r
contains the (phase–dependent) probabilities that the
level of the chain is raised by r after exactly n steps. Because of the structure
(11.1) it is clear that lP (n)
r
= 0 for r > n.
Further deﬁne the matrices Y (r)
s
Y
:= s
n=0 lP (n)
r
and set Ys
Y := Y (1)
s
Y
. Then we
can write R = lims→∞Ys
Y . The matrix Y (r)
s
Y
contains the (phase–dependent)
expectations of the number of visits to the level l + r within the course of s
steps beginning from taboo level l. Again the structure (11.1) yields Y (r)
s
Y
= 0
for r > s.
Because of lP (n)
1
= l−1
r=0 lP (n−1)
r
Ar we obtain
Ys
Y =
s

n=1
lP (n)
1
=
s−1

k=0
k

r=0
lP (k)
r Ar =
∞

r=0
s−1

k=0
lP (k)
r

Ar =
∞

r=0
Y (r)
s
Y −1Ar
for all s ∈N. Conditioning on the time k of the last visit to level l + r −1
yields
lP (n)
r
=
n

k=0
lP (k)
r−1 lP (n−k)
1
which implies
Y (r)
s
Y −1 =
s−1

n=0
n

k=0
lP (k)
r−1 lP (n−k)
1
≤
s−1

k=0
lP (k)
r−1
s−1

n=0
lP (n)
1
= Y (r−1)
s
Y −1
Y (1)
s
Y −1
whence Y (r)
s
Y −1 ≤Y r
s
Y −1 for all r ≥1. Thus Ys
Y satisﬁes the inequality
Ys
Y ≤
∞

n=0
Y n
s
Y −1An

204
AN INTRODUCTION TO QUEUEING THEORY
Clearly, the sequence (Ys
Y : s ∈N) is increasing and Y1
Y = A0 = M(1), while
Y2
Y ≤M(2). Now the above inequality yields Ys
Y ≤M(s) for all s ∈N, which
implies the desired bound R ≤M∗.
□
Theorem 11.4 The stationary probability vector x0 of X is determined by
x0 = x0B[R]
and
x0(I −R)−11 = 1
with B[R] = ∞
n=0 RnBn.
Proof: The stationarity of x = x ˜P yields according to the structure given in
(11.1) and theorem 11.1
x0 =
∞

n=0
xnBn = x0
∞

n=0
RnBn
which proves the ﬁrst equation. In order that x = (xk : k ∈N0) be a proba-
bility distribution, we further need
1 =
∞

k=0
xk1 = x0
∞

k=0
Rk1 = x0(I −R)−11
which is the second equation. Since ˜P is irreducible, there is at most one
stationary distribution of X. Since the two equations in the statement and the
relation in theorem 11.1 yield a solution to x = x ˜P, the vector x0 is uniquely
determined.
□
After verifying that B[R] = 1α (as an exercise), we immediately obtain
Corollary 11.5 The stationary probability vector x0 of X is explicitly given
as
x0 = (α(I −R)−11)−1α
3.
Ergodicity of the Embedded Markov Chain
In the previous section we have derived the form of the stationary distribution
in case of positive recurrence of the embedded Markov chain X. Now we
want to derive the conditions for positive recurrence, also called the ergodicity
conditions for X.
First a remark is due regarding irreducibility of X. If αi = 0 for some phase
1 ≤i ≤m, then for all matrices Bk the ith column vanishes, and X is not

The GI/PH/1 Queue
205
irreducible. However, all statements concerning existence and uniqueness of a
stationary distribution hold as if the chain X were irreducible. This is due to
remark 2.26, which was to be proven as an exercise in chapter 2.
Since in case of positive recurrence the structure of the stationary distribution
x is largely determined by R, it seems natural to search for some property of
R to yield the ergodicity condition. This can be found in the second equation
in theorem 11.4.
Theorem 11.6 The embedded Markov chain X is positive recurrent if and
only if (I −R) is invertible.
Proof: Deﬁne the vector v = ∞
n=0 Rn1, which may have inﬁnite entries.
According to theorem 11.2 the value vi indicates the expected return time to
level zero given that X0 = (0, i). By theorem 2.32 (with F being the states of
the level zero), the chain X is positive recurrent if and only if v is entry–wise
ﬁnite. This is equivalent to convergence of the series ∞
n=0 Rn = (I −R)−1.
□
Although the above stability condition is the ﬁrst that comes into mind if we
regard the structure of the stationary distribution x, it may be difﬁcult to check
this condition if there are eigenvalues of R with modulus close to one. Seen
from a more general perspective, we would expect the queue to be stable if and
only if the classical condition, namely that the mean service time be less than
the mean inter–arrival time, holds. That this is indeed the case can be shown
for the GI/PH/1 queue, too. However, to this aim we ﬁrst need to trans-
late the classical condition to an equivalent condition in terms of the system
parameters.
We have denoted the distribution function of the inter–arrival times by H and
its mean by E(H). According to corollary 9.9, the mean service time is given
by E(S) = −αT −11. Then the classical stability condition may be stated as
ρ = −αT −11
E(H)
< 1
Now deﬁne the matrix A := ∞
n=0 An, which contains the transition proba-
bilities for the phase process at times of arrivals, given that the server is busy.
Clearly A is stochastic. Note further that A is irreducible, due to the phase–
type service process without superﬂuous phases. Let π denote the stationary
vector for A, satisfying πA = π. If we deﬁne further D := T + ηα, then we
can state
Lemma 11.7 The vector π satisﬁes πD = 0.

206
AN INTRODUCTION TO QUEUEING THEORY
Proof: By deﬁnition of A and theorem 10.8 we obtain
π = π
∞

n=0
An = π
 ∞
0

∞

n=0
Pn
P (t) dH(t) = π
 ∞
0

eD·t dH(t)
=
 ∞
0

π
∞

n=0
tn
n!Dn dH(t)
Clearly this equation holds if πD = 0. Since the matrix A is irreducible and
therefore the vector π uniquely determined, the statement follows.
□
Theorem 11.8 The classical stability condition ρ < 1 is equivalent to the
condition
π
∞

k=1
k · Ak 1 > 1
Proof: The deﬁnition of the Ak yields
π
∞

k=1
k · Ak 1 =
 ∞
0

π
∞

k=1
k · Pk
P (t) 1 dH(t)
Here the expression within the integral is the expected number of service com-
pletions in time t given that the service process is started in phase distribution
π. Since the sequence of service completions under the regime of a busy server
is a PH renewal process, corollary 10.11 yields
π
∞

k=1
k · Ak 1 =
 ∞
0

Eπ(Nt
N ) dH(t) = πη · E(H)
Now the expression for π in corollary 10.11 implies
πη = −

αT −11
−1  ∞
0

αeT·tη dt = −

αT −11
−1
Thus we obtain
π
∞

k=1
k · Ak 1 = 1
ρ
(11.2)
which completes the proof.
□
The above condition postulates that under the stationary phase regime the mean
number of service completions between two arrivals is greater than one. It

The GI/PH/1 Queue
207
is as intuitive as the classical stability condition and can be veriﬁed almost
immediately once the system parameters are given. Now we will show that
this condition implies positive recurrence of the embedded Markov chain X.
We write A = ∞
n=0 An as usual, and denote by ¯a = (¯a1, . . . , ¯am) the column
vector of components ¯ai = 
j∈E
∞
n=0 nAn;ij for 1 ≤i ≤m. In order to
apply the criteria of theorem 2.33 to the chain X we ﬁrst construct a non-
negative solution v of
(I −A) v = ¯a −(1/ρ)1
(11.3)
Lemma 11.9 The system (11.3) possesses solutions of the form v + r1 with
r ∈R, where v is any ﬁnite separate solution and {r1 : r ∈R} represents
the set of all solutions of the homogeneous system. In particular, it is always
possible to ﬁnd a solution v that is non-negative.
Proof: Since A is non-negative and stochastic, A −I represents the generator
of some Markov process and, consequently, has rank m −1. The stationary
vector of that process is π (since πA = π).
An inhomogeneous system M v = d of m linear equations with m unknowns,
whose characteristic matrix M is of rank k < m, possesses a solution if and
only if the vector of the right–hand side is orthogonal to all solution vectors
w = (w1, . . . , wm) of the adjoint homogeneous system wM = 0. In our case
the rank is m −1, and any solution of the adjoint system is some multiple of
π. Checking the condition of orthogonality, we see that
π (¯a −(1/ρ)1) = π
∞

n=0
n An1 −1
ρ = 0
because of (11.2). Therefore, the system (11.3) possesses a ﬁnite solution
v = (v1, . . . , vm). The general solution is the sum of a separate solution and
a solution of the homogeneous system. Since, obviously, the homogeneous
system has as solutions all multiples of 1 = (1, 1, . . . , 1)T , the general non–
homogeneous solution is v + r · 1, for arbitrary r ∈R.
□
Theorem 11.10 The embedded Markov chain X is positive recurrent if ρ < 1.
Proof: Applying Lemma 11.9, let v = (v1, . . . , vm) be some non-negative
bounded solution of equation (11.3). Using this solution, deﬁne the ﬁnite func-
tion
f(s, j) =
)
s + vj
for
s ∈N, 1 ≤j ≤m
0
for
s = 0, 1 ≤j ≤m
,
(11.4)

208
AN INTRODUCTION TO QUEUEING THEORY
such that for r > 0,

(s,j)
˜Pr,i
P
;s,jf(s, j) −f(r, i) =

(s,j)
˜Pr,i
P
;s,j · (s −r) +

s≥1; j≤m
˜Pr,i
P
;s,j vj −vi
= −r
m

j=1
Br;ij +
m

j=1
r

n=0
An;ij · (1 −n) +
m

j=1
r

n=0
An;ijvj −vi
Since m
j=1
∞
n=0 nAn;ij < ∞, the sums m
j=1
∞
n=r nAn;ij tend to zero as
r tends to inﬁnity. Hence
r
m

j=1
Br;ij =
m

j=1
∞

n=r+1
rAn;ij →0
as
r →∞
If ρ < 1, then equation (11.3) yields
lim
r→∞
⎛
⎝
⎛

(s,j)
˜Pr,i
P
;s,j f(s, j) −f(r, i)
⎞
⎠
⎞
≤1 −
m

j=1
∞

n=0
n An;ij + ¯ai −1
ρ
= 1 −1
ρ <
0 ,
implying that there exist an r0 ∈N and an ε > 0, such that for ρ < 1
L(r, i) =

(s,j)
˜Pr,i
P
;s,j · f(s, j) −f(r, i) < −ε
for all r ≥r0, i ≤m .
Deﬁne a ﬁnite subset F ⊂N0 × E of states as
F = {(r, i) : r < r0, i ≤m} .
(11.5)
Since for r = 0 the expression m
j=1 A0;ij(1 + xj) is positive (bearing in
mind that vj ≥0
for all j), this set F is not empty. Since L(r, i) ≤−ε
for (r, i) /∈/ F, and 0 ≤f(s, j) < ∞for all s ∈N0 and 1 ≤j ≤m, the
prerequisites of theorem 2.33 are satisﬁed, i. e. the chain is positive recurrent.
□
4.
Asymptotic Distribution of the System Process
If the stability condition ρ < 1 is satisﬁed, we have seen in the previous two
sections how to derive the stationary distribution x of the embedded Markov
chain at arrival instants. Now we want to use this in order to obtain an expres-
sion for the asymptotic distribution of the system process Q = (Qt : t ≥0).
Here Qt = (n, j) states that there are n users in the system (including the

The GI/PH/1 Queue
209
server) and the phase of the service is j at time t. If there are no users in the
system, there is no service either. This is denoted by Qt = (0, 0).
To obtain the asymptotic distribution of Q is a simple matter if we conceive the
system process as a semi-regenerative process in the same way as we already
did in the analysis of the GI/M/1 queue. If T = (Tn
T
: n ∈N) denotes the
sequence of arrival instants and X = (Xn
X
: n ∈N) is deﬁned as in section 1,
then the independence of the arrival process from the rest of the system yields
G(k,i),(n,j)(t) = P(Tn
T +1 −Tn
T ≤t|Xn
X = (k, i), Xn
X +1 = (n, j)) = H(t)
independently of k, n ∈N0 and i, j ≤m. The standard assumption E(H) > 0
on the distribution function H implies that Tn
T
→∞P–almost certainly as
n →∞. Hence (X, T ) is a Markov renewal chain and Q is semi–regenerative.
In order to employ theorem 7.15 for the calculation of the asymptotic distri-
bution of the system process Q, we already have determined the stationary
distribution ν = x of X in section 2. It remains to derive the vector m of the
mean time between Markov renewal points as well as the function K(t) de-
scribing the behaviour of the system process between Markov renewal points.
The vector m is obtained in a straightforward manner as
mn,i = E(T1
T |X0 = (n, i)) = E(H)
(11.6)
independently of n ∈N0 and i ≤m, since the arrival process does not depend
on the state of the system. Thus the vector m is constant. The function K(t)
is given by the values K(k,i),(n,j)(t) = P(T1
T
> t, Qt = (n, j)|X0 = (k, i)).
Exploiting the independence of arrival process and service and abbreviating
Hc(t) := 1 −H(t), we obtain
K(k,i),(n,j)(t) =
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
Hc(t) · Pk
P +1−n;ij(t),
1 ≤n ≤k + 1
Hc(t) · ∞
h=k+1
m
j=1 Ph
P ;ij(t),
(n, j) = (0, 0)
0,
n > k + 1
for all t > 0, k, n ∈N0, and i, j ≤m.
Denote the asymptotic distribution of the system process by the block vector
y = (yn : n ∈N0). For n ∈N the blocks yn = (yni : 1 ≤i ≤m) are deﬁned
by yni = limt→∞P(Qt = (n, i)). The asymptotic probability of the system
being idle is denoted by y0 = y00 = limt→∞P(Qt = (0, 0)). Now theorem
7.15 yields
Corollary 11.11 If the stability condition ρ < 1 holds, then the asymptotic
distribution of the system process Q = (Qt : t ≥0) for the GI/PH/1 queue is

210
AN INTRODUCTION TO QUEUEING THEORY
given by
ynj =

λ · ∞
k=n−1 xk ψ(k + 1 −n) ej,
n ≥1
λ · ∞
k=0 xk
∞
h=k+1 ψ(h) 1,
n = 0
with the following notations. H denotes the inter–arrival time distribution,
and Hc(t) := 1 −H(t) its complement. λ := 1/E(H) denotes the asymptotic
arrival rate. The vector x = x ˜P contains the stationary distribution of the
embedded Markov chain X at arrival instants, and the matrices Pk
P (t) are
given as in corollary 10.6 with ∆= (T, ηα, 0, 0, . . .). Finally we denoted
ψ(k) :=
 ∞
0

Hc(t)Pk
P (t) dt
for all k ∈N0.
In the remainder of this section we shall ﬁnd simpler expressions for y. Be-
cause of theorem 11.1, we ﬁrst obtain
= λ · x0(I −R)−1
 ∞
0

Hc(t)(1 −P0
P (t)1) dt −
∞

h=1
Rhψ(h) 1

= λ · x0(I −R)−11 · E(H) −λ · x0(I −R)−1
∞

h=0
Rhψ(h) 1
Corollary 11.5 yields x0(I −R)−11 = 1. Denoting Ψ[R] := ∞
h=0 Rhψ(h)
and using λ · E(H) = 1, we can write
y0 = 1 −λ · x0(I −R)−1Ψ[R]1
(11.7)
For n ∈N the same arguments yield
yn = λ ·
∞

k=n−1
xk ψ(k + 1 −n) = λ · x0Rn−1Ψ[R]
(11.8)
Lemma 11.12 The matrix Ψ[R] is given by
Ψ[R] = (R −R 1α −I) T −1
= λ · x0(I −R)−1
∞

h=1
(I −Rh)ψ(h) 1
y0 = λ ·
∞

k=0
xk
∞

h=k+1
ψ(h) 1 = λ · x0
∞

h=1
h−1

k=0
Rkψ(h) 1

The GI/PH/1 Queue
211
Proof: Differentiating equation (10.1) for the PH(α, T) renewal process yields
P ′
0
P (t) = P0
P (t) T
and
P ′
n
P (t) = Pn
P (t) T + Pn
P −1(t) ηα
(11.9)
for all n ∈N. From theorem 11.3 we know the relation R = ∞
n=0 RnAn.
For every n ∈N0 we obtain by partial integration
An =
 ∞
0

Pn
P (t) dH(t) = −
 ∞
0

Pn
P (t) dHc(t)
= −Pn
P (t)Hc(t)|∞
t=0 +
 ∞
0

P ′
n
P (t)Hc(t) dt
= δn0I + ψ(n) T + (1 −δn0) · ψ(n −1) ηα
where δn0 denotes the Kronecker function. Multiplying by Rn and summing
up over all n ∈N0 yields
R = I + Ψ[R] T + R Ψ[R] ηα
(11.10)
Multiplying by 1 we obtain
(I −R)1 = (I −R) Ψ[R]η
By theorem 11.4, the matrix (I −R) is invertible in case of positive recurrence.
Hence Ψ[R]η = 1, which yields after substitution in (11.10)
Ψ[R] T = R −R 1α −I
The statement follows now by invertibility of T.
□
Theorem 11.13 The asymptotic distribution y of Q is given by
y0 = 1 −ρ
and
yn = λ · x0(Rn (I −1α) −Rn−1)T −1
for all n ∈N.
Proof: Substituting the expression of lemma 11.12 in equation (11.7) yields
y0 = 1 −λ · x0

−(I −R)−1R 1α −I

T −11
= 1 −λ · (α(I −R)−11)−1 
α(I −R)−1R 1 + 1

· (−αT −11)
because of corollary 11.5. Writing 1 = α(I −R)−1(I −R)1 we obtain
y0 = 1 −λ · (−αT −11) = 1 −ρ
Regarding yn with n ∈N, the statement follows immediately after substitution
of the same expression for Ψ[R] into formula (11.8).
□

212
AN INTRODUCTION TO QUEUEING THEORY
Notes
A classical treatment of the GI/PH/1 queue is given in Neuts [65], who presents
a proof for the necessity of the stability condition, too. Different proofs for the
ergodicity conditions can be found in Asmussen [5] as well as in Meyn and
Tweedie [59].
Tweedie [83] has shown how to generalize the matrix–geometric solution for
GI/M/1 - type matrices towards operator–geometric solutions for GI/M/1 - type
matrices with a general phase space.
Exercise 11.1 Verify that for the PH service time distribution being an expo-
nential one, all results coincide with the results obtained for the GI/M/1 queue.
Exercise 11.2 For corollary 11.5, show that B[R] = 1α.
Exercise 11.3 Show that the mean number of arrivals during a busy period is
given by α(I −R)−11.
Exercise 11.4 Show that the stationary mean number of users in the system
prior to arrivals is given by
¯Na
N = x0(I −R)−21 −1
Exercise 11.5 Verify equations (11.9).
Exercise 11.6 Show that the asymptotic mean number of users in the system
is given by
¯N = ρ ¯Na
N −λx0(I −R)−1T −11
with ¯Na
N as deﬁned in exercise 11.4.

Chapter 12
THE BMAP/G/1 QUEUE
Let (N, J ) denote a BMAP with characterizing matrices ∆= (Dn : n ∈N0),
each matrix Dn being of dimension m ∈N. This shall model the arrival
stream into the queue. The distribution function of the service time shall be
denoted by H and satisfy 0 < E(H) < ∞. The service discipline is FCFS.
Let Q = (Qt : t ≥0) denote the system process comprising the phase of the
arrival process. Thus, Qt = (n, i) means that there are n users in the system
at time t and the arrival process has phase Jt
J = i. The state space of Q is
E = N0 × {1, . . . , m}.
As the analysis of the GI/PH/1 queue was similar to that of the GI/M/1 queue,
we will ﬁnd many similarities between the BMAP/G/1 and the M/G/1 queueing
systems. To begin with, we will ﬁrst construct an embedded Markov chain at
the times of service completions. Deﬁne T0
T
:= 0 and Tn
T
as the time of the
nth service completion. Write T = (Tn
T
: n ∈N0). Let Xn
X
:= QTn
T
for all
n ∈N0 and assume that at time zero there are no users in the system. The Tn
T
are stopping times for Q and by deﬁnition Xn
X
is a deterministic function of
QTn
T . The assumption 0 < E(H) implies Tn
T →∞for n →∞.
At the time instances immediately after service completions we know that the
current service (if there is one) has just begun, and we need only to remember
the current system state in order to predict the system state immediately after
the next service completion. This implies that X = (Xn
X : n ∈N0) is a Markov
chain. The same property of the queue yields condition (7.5). Hence Q is
a semi–regenerative process with embedded Markov renewal chain (X, T ).
Note that this time (as opposed to the embedded chain for the M/G/1 queue)
the Markov chain is two–dimensional with state space E = N0 × {1, . . . , m}.

214
AN INTRODUCTION TO QUEUEING THEORY
1.
The Embedded Markov Chain
Deﬁne the matrices
An =
 ∞
0

Pn
P (t) dH(t)
(12.1)
and
Bn =
n+1

k=1
 ∞
0

eD0·uDk du
 ∞
0

Pn
P +1−k(t) dH(t)
(12.2)
for all n ∈N0, with Pn
P (t) denoting the transition probability matrices that the
BMAP counts n arrivals in time t (see corollary 10.6). The matrix An contains
the probabilities that within a service time n users have arrived. Hence we can
describe some of the transition probabilities for the Markov chain X by
P(X1 = (l + n, j)|X0 = (l, i)) = An+1;ij
independently of l ≥1 and for all n ≥−1 and i, j ≤m. The matrix Bn
contains the probabilities that ﬁrst a batch of 1 ≤k ≤n + 1 users arrives and
then n+1−k additional users arrive within a service time. This situation occurs
whenever a service completion leaves the queueing system empty. Therefore
we can write
P(X1 = (n, j)|X0 = (0, i)) = Bn;ij
for all n ≥0 and i, j ≤m. In summary we obtain for the transition probability
matrix of X the block structure
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
B0
B1
B2
· · ·
A0
A1
A2
· · ·
0
A0
A1
· · ·
0
0
A0
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
A matrix of this structure is said to be of M/G/1 type, which underlines the
similarity to the embedded Markov chain of the M/G/1 queue. Again we will
call the ﬁrst dimension n of a state (n, i) the level, and the second dimension i
the phase. With respect to the levels, the Markov chain X is called skip–free
to the left, since in one transition the level can be reduced only by one.
Simplifying the expression (12.2) and using deﬁnition (12.1) yields the relation
Bn = −D−1
0
n

k=0
Dk+1An−k
(12.3)
between the sequences (An) and (Bn), which reduces the computation of the
matrices Bn to a prior computation of the sequence (An).

The BMAP/G/1 Queue
215
2.
The Matrix G
Deﬁne τn
τ as the number of steps until the chain X reaches level n for the ﬁrst
time. Further deﬁne
Gk(i, j) := P(τn
τ = k, Xk = (n, j)|X0 = (n + 1, i))
for all k ≥1 and i, j ≤m. The spatial homogeneity of P implies that this
deﬁnition is independent of n ≥0. Deﬁne the matrices Gk of dimension
m × m by their entries Gk(i, j) for i, j ≤m, and G := ∞
k=1 Gk. Thus the
entry G(i, j) denotes the probability that under the condition that we start in a
level n + 1 ≥1 and in phase i, we reach the level n for the ﬁrst time in state j.
Theorem 12.1 If the Markov chain X is recurrent, then G is stochastic.
Proof: If G is not stochastic, then there is a phase i such that
∞

k=1
m

j=1
P(τn
τ = k, Xk = (n, j)|X0 = (n + 1, i)) < 1
Because the transition matrix P is skip–free to the left, this means that the
function fij
f
as deﬁned by (2.5) satisﬁes f(n+1,i),(n,j) < 1 for all n ∈N0 and
1 ≤j ≤m.
By deﬁnition of the matrices An and Bn, the Markov chain X is irreducible.
Then X is recurrent by deﬁnition if fx,x
f
= 1 for some state x ∈E. According
to equation (2.6), fx,x
f
= 1 can only hold for a state x ∈E if fx,y
f
= 1 holds for
all states x, y ∈E. Hence it follows that X is transient if G is not stochastic.
□
Theorem 12.2 The matrix G satisﬁes the ﬁxed point equation
G =
∞

n=0
AnGn
Proof: First we introduce the auxiliary matrices G[r]
k with entries
G[r]
k (i, j) = P(τn
τ = k, Xk = (n, j)|X0 = (n + r, i))
(12.4)
for all k, r ≥1 and i, j ≤m. Again, the spatial homogeneity of P implies that
this deﬁnition is independent of n ≥0. By deﬁnition G[1]
k = Gk for all k ≥1.
Because P is skip–free to the left, we have G[r]
k = 0 for k < r. Summing up

216
AN INTRODUCTION TO QUEUEING THEORY
over the number l of steps until the chain X reaches the next lower level for
the ﬁrst time, we obtain
G[r]
k =
k−1

l=1
GlG[r−1]
k−l
(12.5)
for all k, r ≥1. Further deﬁne G[r] = ∞
k=1 G[r]
k for all r ≥1. Now we obtain
G[r] =
∞

k=1
k−1

l=1
GlG[r−1]
k−l
=
∞

l=1
Gl
∞

k=l+1
G[r−1]
k−l
= GG[r−1]
which implies G[r] = Gr because of G[1] = G. Summing up over the level
reached by X after the ﬁrst step we ﬁnally obtain
G = A0 +
∞

n=1
AnG[n] =
∞

n=0
AnGn
which was to be proven.
□
Theorem 12.3 The matrix G can be computed by the recursion
G1 = A0
and
Gk =
k−1

n=1
AnG∗n
k−1
with G = (0, G1, G2, . . .) and the convolutional powers of G as deﬁned on
page 190.
Proof: The ﬁrst equality follows from the deﬁnition of G1. For the second
one, a ﬁrst passage argument yields
Gk =
k−1

n=1
AnG[n]
k−1
with G[n]
k as deﬁned in (12.4). Thus it remains to show that G[n]
k
= G∗n
k
for all
k ≥n ∈N. This relation holds by deﬁnition for n = 1 and all k ≥1. For
n + 1 the induction step follows from equation (12.5).
□
3.
Stationary Distribution at Service Completions
Using the matrix G from the previous section, we are now ready to derive a
recursive scheme for the stationary probability vector x = xP of P. We write

The BMAP/G/1 Queue
217
x = (xn : n ∈N0), with xn = (xni : 1 ≤i ≤m) containing the stationary
probabilities for level n.
Theorem 12.4 If X is positive recurrent, then the vectors xn satisfy the recur-
sion
xn =

x0 ¯Bn +
n−1

k=1
xk ¯An+1−k


I −¯A1
−1
for all n ≥1, with the deﬁnitions
¯Bn :=
∞

k=n
BkGk−n
and
¯An :=
∞

k=n
AkGk−n
for n ≥0.
Proof: For n ≥1 we consider the Markov chain X F embedded in X at times
of visits to the set F = {(k, i) : 0 ≤k ≤n, i ≤m} of states, which is
equal to levels zero through n. By the deﬁnition of G[r] and due to the relation
G[r] = Gr (see the proof of theorem 12.2), the transition probability matrix Pn
P
of X F can be written as
Pn
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
B0
B1
B2
· · ·
Bn−1
¯Bn
A0
A1
A2
· · ·
An−1
¯An
0
A0
A1
· · ·
An−2
¯An−1
...
...
...
...
...
...
0
· · ·
0
A0
A1
¯A2
0
· · ·
0
0
A0
¯A1
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
By theorem 2.29 we know that the vectors x0, . . . , xn are proportional to the
vectors y0, . . . , yn with y = (y0, . . . , yn) satisfying y = yPn
P . Hence we
obtain in particular for the nth column
xn = x0 ¯Bn +
n

k=1
xk ¯An+1−k
(12.6)
According to theorem 12.1 we know that G is stochastic, and thus
¯A11 =
∞

k=1
AkGk−11 =
∞

k=1
Ak1 = 1 −A01
Due to corollary 10.7 and deﬁnition (12.1), all row sums of the matrix A0 are
strictly positive. This implies that all row sums of ¯A1 are strictly less than one,

218
AN INTRODUCTION TO QUEUEING THEORY
and thus I −¯A1 is invertible by Gershgorin’s circle theorem (see corollary
15.11). Therefore, we can transform the relation (12.6) to
xn =

x0 ¯Bn +
n−1

k=1
xk ¯An+1−k


I −¯A1
−1
which is the statement.
□
With the above recursion it remains to determine the vector x0 in order to
obtain the stationary distribution at service completions. A simple expression
for this will be obtained as a by–product of the next section.
4.
Asymptotic Distribution of the System Process
By means of the stationary distribution x of the embedded Markov chain X
at service completions we can determine the asymptotic distribution y of the
queue’s system process via theorem 7.15. In order to apply this, we need to
obtain the product xm, with m denoting the column vector with entries
mn,i = E(T1
T |X0 = (n, i))
Theorem 12.5 The asymptotic mean time between two service completions is
given by
xm = E(H) −x0D−1
0 1
Proof: For n > 0 we have mn,i = E(H), since the service does not depend on
the phase of the arrival process. In order to determine the values m0,i, deﬁne
τ := min{t ≥0 : pr1(Qt) > 0) as the time until the ﬁrst arrival. Since the
arrival process is independent of the rest of the queue, equation (10.2) can be
applied to yield E(τ|X0 = (0, i)) = −eiD−1
0 1, with ei denoting the ith row
base vector. Hence we obtain
m0,i = E(τ|X0 = (0, i)) + E(H) = E(H) −eiD−1
0 1
Now the product xm is given by
xm =
m

i=1
 ∞

n=1
xniE(H) + x0i(E(H) −eiD−1
0 1)

= E(H) −x0D−1
0 1
which was to be proven.
□

The BMAP/G/1 Queue
219
In a stationary regime, we expect that the intensities of the ﬂow into and out of
a queueing system equal each other. The asymptotic mean arrival rate is com-
pletely determined by the parameters of the BMAP and was derived in theorem
10.10 as λ = π ∞
k=1 kDk1, with π denoting the stationary phase distribution.
The latter is determined by πD = 0, with D = ∞
k=0 Dk denoting the gen-
erator of the phase process. The intensity of the ﬂow out of the system can be
measured in terms of the mean time between two service completions, since
every user leaves the system immediately after its service is ﬁnished. Thus we
would expect that in under a stationary regime λ = 1/xm holds. This shall be
proven next in
Theorem 12.6 The asymptotic mean time between service completions equals
the inverse of the asymptotic mean arrival rate:
E(H) −x0D−1
0 1 = λ−1
Proof: Since the arrival process is independent from the rest of the queue,
λ · (E(H) −x0D−1
0 1) = λ · xm is the asymptotic mean number of arrivals
between two service completions, according to theorem 12.5. We need to show
λ·xm = 1. Using the stationary distribution x of the embedded Markov chain
X at service completion times, we recognize the following representations:
The number
M1
M =
∞

n=1
xn
∞

k=1
kAk1
indicates the mean number of arrivals between two service completions for the
case that the prior service completion does not leave the queue empty. The
number
M2
M = −x0D−1
0
∞

k=1
kDk1
signiﬁes the mean batch size of the ﬁrst arrival after the prior service comple-
tion leaves the system empty. Finally,
M3
M = −x0D−1
0
∞

k=1
Dk
∞

k=1
kAk1
represents the mean number of arrivals during the following service time. Thus
we can write M1
M + M2
M + M3
M = λ · xm and it sufﬁces to show that this sum
equals one. To this aim we take a look at the deﬁning equation x = xP.
Analogously to the argument before equation (8.16) for the M/G/1 queue, this

220
AN INTRODUCTION TO QUEUEING THEORY
can be written as
x1A0 = x0(I −B0)
x2A0 = x0(I −B0 −B1) + x1(I −A0 −A1)
x3A0 = x0(I −B0 −B1 −B2) + x1(I −A0 −A1 −A2)
+ x2(I −A0 −A1)
...
Here the nth equation is equivalent to
xnA0 = x0

B −
n−1

k=0
Bk

+
n−1

l=1
xl

A −
n−l

k=0
Ak

+ x0(I −B) +
n−1

l=1
xl(I −A)
with A = ∞
n=0 An and B = ∞
n=0 Bn. We ﬁrst multiply each equation
by 1 from the right, then we add up all equations. Employing the relations
B −n−1
k=0 Bk = ∞
k=n Bk and A −n−1
k=0 Ak = ∞
k=n Ak, we obtain
∞

n=1
xn

A −
∞

k=1
Ak

1 = x0
∞

n=1
nBn1 +
∞

n=1
xn
∞

k=1
kAk+11
Because of ∞
k=1 kAk+1 = ∞
k=1 kAk−∞
k=1 Ak and A1 = 1 this simpliﬁes
to
1 −x01 = x0
∞

n=1
nBn1 + M1
M
(12.7)
x0
∞

n=1
nBn1 = −x0D−1
0
∞

n=1
n
n+1

k=1
DkAn+1−k1
= −x0D−1
0
∞

k=1
Dk
∞

n=k−1
((n + 1 −k) + (k −1))An+1−k1
= M3
M −x0D−1
0
∞

k=1
(k −1)Dk
∞

n=0
An1
= M3
M + M2
M + x0D−1
0
∞

k=1
Dk1
= M3
M + M2
M −x01
Using Bn = −D−1
0
n+1
k=1 DkAn+1−k (see equation (12.3)), the ﬁrst term on
the right is evaluated as

The BMAP/G/1 Queue
221
where the last equality holds because of ∞
k=1 Dk = (D −D0) and D1 = 0.
This and (12.7) yield the statement.
□
Let yni := limt→∞P(Qt = (n, i)) for n ∈N0 and 1 ≤i ≤m denote the
asymptotic probabilities of the system process Q. Further deﬁne the vectors
yn := (yn1, . . . , ynm) for all n ∈N0 and the sequence y = (yn : n ∈N0).
Deﬁne the m × m matrices K[kn](t) by their entries
K[kn]
ij
K
(t) := P(T1
T > t, Qt = (n, j)|X0 = (k, i))
for all t ≥0 and k, n ∈N0. Then the asymptotic distribution y of the sys-
tem process can be expressed in terms of the stationary distribution x of the
embedded Markov chain X via theorem 7.15. As a ﬁrst result we obtain
Theorem 12.7 The asymptotic probability vector of an empty system is given
by
y0 = −λx0D−1
0
and has total mass y01 = 1 −ρ, with ρ = λ · E(H).
Proof: For n = 0 the matrices K[kn](t) are zero if k > 0, since during the
time between service completions the number of users in the system cannot
decrease. The remaining matrices K[00](t) are given by
K[00](t) = P0
P (t) = eD0·t
for all t ≥0, according to corollary 10.7. Theorem 7.15 now yields
y0 =
1
xmx0
 ∞
0

eD0·t dt = λ · x0(−D−1
0 )
which is the ﬁrst statement. The second one follows from this representation
of y0 and theorem 12.6.
□
Now we will derive a simple expression for the vector x0. To this aim we take
a look at the Markov chain X 0 embedded in X at visits to the level zero. The
state space of this chain is {(0, i) : 1 ≤i ≤m}, which is isomorphic to the
phase space {1, . . . , m} of the BMAP. From theorem 2.29 we know that x0 is
proportional to, i.e. a scalar multiplicative of, the stationary probability vector
of X 0. First we determine the transition probability matrix K for X 0, which is
of dimension m × m. From the deﬁnition of G[r] and the relation G[r] = Gr

222
AN INTRODUCTION TO QUEUEING THEORY
we see by a ﬁrst passage argument and then by equation (12.3) that
K =
∞

n=0
BnGn = −D−1
0
∞

n=0
n

k=0
Dk+1An−kGn
= −D−1
0
∞

k=0
Dk+1
∞

n=k
An−kGn−kGk = −D−1
0
∞

k=0
Dk+1Gk+1
= −D−1
0
 ∞

k=0
DkGk −D0

= I −D−1
0 D[G]
with D[G] := ∞
k=0 DkGk. In order to ﬁnd an expression for the vector
κ = κK, we ﬁrst need the following representation of the matrix G:
Lemma 12.8 The matrix G can be expressed by
G =
 ∞
0

eD[G]·tdH(t)
In particular, the invariant probability vector g = gG satisﬁes gD[G] = 0.
Proof: The matrix G contains the probabilities of phase transitions between a
service completion that does not leave the system empty and the ﬁrst consecu-
tive service completion which leaves the system with one user less than at the
beginning. During the time between these two service completions the queue
is never empty, which means that this time interval is a ﬁnite sum of (randomly
many) service times.
Phase transitions depend on the arrival process only, since this is independent
of the rest of the queue. Thus it does not matter for G which is the service dis-
cipline, as long as the time between the above mentioned service completions
remains the same.
The stated expression for G results if we regard the phase process under the
following service discipline. Whenever a new user arrives, it is immediately
admitted to the server. The current service is interrupted and the user in service
goes to the head of the queue. As soon as a service is completed, the service
of the user at the head of the queue is resumed, i.e. none of the work is lost.
Thus the time that a user spends in the server still equals exactly its service
time. The server is not idle between the above mentioned service completions
and ﬁnally, since the arrival process is independent from the service, the num-
ber of arriving users does not change under the new service discipline.
Under the new service discipline, the user that is in service at the beginning of
the time interval concerning G will also be in service when this time interval

The BMAP/G/1 Queue
223
ends, since all users arriving later will be served earlier. If there are no arrivals
during the service time of this user, then phase transitions are governed by the
rate matrix D0. If there is a ﬁrst (batch) arrival, occuring with rate matrix Dn,
then the phase upon reentering the same level again (when the ﬁrst user re-
sumes its service) will change according to the rate matrix DnG[n] = DnGn.
Thus the generator for the phase process, if we regard only the lowest level of
the ﬁrst user in service, is given by D[G] = ∞
n=0 DnGn. Since the com-
plete time that the ﬁrst user spends in the server is exactly its service time, the
expression for G follows.
The second statement gD[G] = 0 for the stationary probability vector g = gG
follows immediatlely from the obtained representation for G.
□
Remark 12.9 The service discipline that was involved in the above proof is
called LCFS (last come ﬁrst served) discipline with preemptive resume regu-
lation.
Theorem 12.10 The stationary probabilities that the chain X is in level zero
can be expressed by
x0 = −1 −ρ
λ
gD0
and the asymptotic probability vector of an empty system is given by
y0 = (1 −ρ) · g
Proof: The expression K = I −D−1
0 D[G] along with lemma 12.8 yields
that κ = c′ · (−gD0) with some constant c′. Theorem 2.29 now states that a
representation x0 = c · (−gD0) holds with some constant c. By theorem 12.7
this implies
1 −ρ = y01 = λ · c · gD0D−1
0 1 = λ · c
and thus c = (1 −ρ)/λ. This proves the ﬁrst statement. The second statement
now is a consequence of theorem 12.7.
□
Theorem 12.11 The asymptotic probability vectors yn for n ≥1 are given by
the recursion
yn =
n

k=1
(y0Dk + λxk)
 ∞
0

(1 −H(t))Pn
P −k(t) dt

224
AN INTRODUCTION TO QUEUEING THEORY
Proof: An application of theorem 7.15, along with theorems 12.5 and 12.6,
yields
yn = λ
n

k=0
xk
 ∞
0

K[kn](t) dt
(12.8)
as between service completions the number of users in the system can only
increase. For k = 0 and n ≥1 we obtain
K[0n](t) =
 t
0

eD0·u
n

l=1
Dl · (1 −H(t −u)) · Pn
P −l(t −u) du
while for k, n > 0 we have
K[kn](t) = (1 −H(t)) · Pn
P −k(t)
In both expressions the independence between arrival process and current ser-
vice is used. Employing them in (12.8) yields
yn = λ · x0
n

l=1
 ∞
t

=0

 t
u

=0
eD0·uDl · (1 −H(t −u)) · Pn
P −l(t −u) du dt
+ λ ·
n

k=1
xk
 ∞
0

(1 −H(t)) · Pn
P −k(t) dt
The integral in the ﬁrst line equals
 ∞
u

=0
eD0·uDl
 ∞
t

=

u
(1 −H(t −u))Pn
P −l(t −u) dt du
=
 ∞
u

=0
eD0·u du Dl
 ∞
t

=0

(1 −H(t))Pn
P −l(t) dt
= −D−1
0 Dl
 ∞
t

=0

(1 −H(t))Pn
P −l(t) dt
If we plug this back into the above expression for yn and use y0 = −λx0D−1
0 ,
we obtain the statement.
□
5.
Stability Conditions
As in the previous chapter on the GI/PH/1 queue we will show various stability
conditions to be equivalent. Deﬁne A = ∞
n=0 An. Then A = (aij)i,j≤m is
the transition matrix for the phase component of the embedded Markov chain
X under the condition that the queue is not empty. More exactly, we have
P(pr2(X1) = j|X0 = (n, i)) = aij

The BMAP/G/1 Queue
225
for all n ≥1. Denote the stationary probability vector of A by π = πA.
Further denote the generator of the phase process J by D = ∞
n=0 Dn. Com-
pletely analogously to the proof of lemma 11.7 one can show (as an exercise)
that the vector π satisﬁes πD = 0.
Theorem 12.12 Denote the mean service time by E(H) and the asymptotic
mean arrival rate of the BMAP by λ = π ∞
n=1 nDn1. Then
ρ = λ · E(H) = π
∞

n=1
nAn1
Proof: Simply using the deﬁnition of An yields
π
∞

n=1
nAn1 = π
∞

n=1
n
 ∞
0

Pn
P (t) dH(t)1 =
 ∞
0

Eπ(Nt
N ) dH(t)
where Eπ(Nt
N ) is the expected number of arrivals during time t if the BMAP
starts with a phase distribution π. By theorem 10.10 we have Eπ(Nt
N ) = λ · t
and hence
π
∞

n=1
nAn1 = λ ·
 ∞
0

t dH(t) = λ · E(H)
which is the statement.
□
Theorem 12.13 If the stability condition ρ < 1 holds, then the embedded
Markov chain X is positive recurrent.
Proof: As in lemma 11.9 we can ﬁnd a non–negative solution x to the equation
system
(I −A)x = ¯a −ρ1
(12.9)
with ¯ai := 
j∈E
∞
n=1 nAn;ij for i ∈E. Deﬁne the function f(s, j) = s+xj
for s ∈N0 and 1 ≤j ≤m. Then for r > 0 we obtain

(s,j)
˜P(
P r,i),(s,j)f(s, j) −f(r, i) =
m

j=1
∞

n=0
An;ij · (r −1 + n + xj) −r −xi
= r −1 +
m

j=1
∞

n=1
nAn;ij +
m

j=1
∞

n=0
An;ijxj −r −xi
=
m

j=1
∞

n=1
nAn(i, j) + ρ −¯ai −1 = ρ −1 < 0

226
AN INTRODUCTION TO QUEUEING THEORY
For the exceptional set F = {(0, i) : 1 ≤i ≤m} we obtain
which is ﬁnite by assumption. Thus the conditions of theorem 2.33 are satis-
ﬁed, which proves the statement.
□
Notes
The ﬁrst complete analysis of the BMAP/G/1 queue has appeared in a paper by
Ramaswami [71]. In this paper the BMAP was used with its older, more com-
plicated notation under the name N–process. A special case of the BMAP/G/1
queue, namely the MAP/G/1 queue without batch arrivals, has been analyzed
in Lucantoni et al. [56] using the current notation. The recursion scheme for
the stationary probability vectors at service completion times has been intro-
duced by Ramaswami [72]. An outline of Ramaswami’s analysis using the
new notations, along with some new results (namely lemma 12.8 and theorem
12.10), are presented in Lucantoni [54]. The use of a matrix convolutional cal-
culus for the determination of the matrix G has been presented in Baum [9].
A general discussion of M/G/1 type matrices and their use in queueing theory
is presented in Neuts [66], including necessary conditions for the stability of
the queue. A variant of the MAP/G/1 queue with LCFS service discipline is
analyzed in Breuer [20]. For a historical overview of the developments that led
to the BMAP and matrix–analytical methods see Lucantoni [55].
A different proof of theorem 12.6 can be found in Ramaswami [71]. In Neuts
[66] a computation of the matrix G is proposed via the ﬁxed point equation
of theorem 12.2. A more elaborate version of the proof for lemma 12.8 can
be found in Lucantoni and Neuts [57], while the idea for this proof has been
presented in an earlier form of notation by Machihara [58]. A more elementary
proof is given in Lucantoni [54]. Another recursion scheme for the asymptotic
distribution y is presented in Takine [82].
Exercise 12.1 Prove πD = 0 for the stationary distribution π = πA.
Exercise 12.2 Show the existence of a solution x to equation (12.9).
Exercise 12.3 Deﬁne the z–transforms X(z) := ∞
n=0 xnzn of the stationary
probability vector at service completions, as well as A(z) := ∞
n=0 Anzn, and

(s,j)
˜P(0
P
,i),(s,j)f(s, j) =
m

j=1
∞

n=1
nBn;ij +
m

j=1
∞

n=0
Bn;ijxj

The BMAP/G/1 Queue
227
B(z) := ∞
n=0 Bnzn for |z| ≤1.
(a) Show that
B(z) = −z−1D−1
0 (D(z) −D0)A(z)
(b) Use the result above and x = xP to show that
X(z)(zI −A(z)) = −x0D−1
0 D(z)A(z)
Exercise 12.4 Use exercise 10.2 to show that
∞

n=0
Pn
P (t)zn = eD(z)·t
Exercise 12.5 Show that D(z) is invertible for 0 ≤z < 1.
Exercise 12.6 Deﬁne ψ(n) :=
+ ∞
0
+
(1 −H(t))Pn
P (t)dt for all n ∈N0 and
Ψ(z) := ∞
n=0 ψ(n)zn. Show that
Ψ(z) = (A(z) −I)D(z)−1
Exercise 12.7 Deﬁne the z–transform Y (z) := ∞
n=0 ynzn of the asymptotic
probability vector. Show that
Y (z) =

λX(z) · (z −1) · D(z)−1,
0 ≤z < 1
π,
z = 1
Hint: Start by transforming Y (z) −y0 and use exercise 12.6.


Chapter 13
DISCRETE TIME APPROACHES
1.
Discrete Phase–Type Distributions
Analogous to the deﬁnition of PH distributions in continuous time, we will
deﬁne discrete PH distributions in terms of Markov chains with one absorbing
state. Let X denote a Markov chain with ﬁnite state space E = {0, . . . , m}
and a transition matrix structured as
P =

1
0
η
T

Denote the initial distribution of X by the row vector ˜α = (α0, α), where α is
of dimension m. The structure of P shows that state 0 is absorbing. All other
states shall be transient. Let
Z = min{n ∈N0 : Xn
X = 0}
denote the time until absorption in state 0. Deﬁne pn := P(Z = n) for all
n ∈N0. The distribution p = (pn : n ∈N0) of Z is called a discrete
phase–type distribution, or shortly discrete PH distribution. We also write
Z ∼PHd
H (α, T). The number m of transient states is called the order of p. A
transient state is called phase.
An immediate ﬁrst observation is η = 1−T1, with 1 denoting a column vector
with all entries being one. Further the deﬁnition yields
p0 = P(Z = 0) = α0 = 1 −α1
(13.1)
This explains the notation PHd
H (α, T). Knowledge of α and T is sufﬁcient to
determine ˜α and η and hence completely specify the distribution of Z. There-
fore we call the pair (α, T) the characterization of a discrete PH distribution.

230
AN INTRODUCTION TO QUEUEING THEORY
Theorem 13.1 Let Z denote a random variable which has a discrete phase–
type distribution with characterization (α, T). Then
P(Z = n) = αT n−1η
and
P(Z ≤n) = 1 −αT n1
for all n ∈N.
Proof: The structure of P leads to the observation
P n =

1
0
1 −T n1
T n

for n ∈N0, which can be veriﬁed by induction on n. Together with (13.1) this
yields the second statement. The ﬁrst one is now obtained as
P(Z = n) = P(Z ≤n) −P(Z ≤n −1) = αT n−11 −αT n1
= αT n−1(1 −T1)
which completes the proof because of η = 1 −T1.
□
By corollary 2.15 we know that invertibility of I −T is equivalent to the pos-
tulate that the states 1, . . . , m be transient. The same arguments as in the con-
tinuous case (see theorem 9.3) serve to show that
Lemma 13.2 A PHd
H (α, T) distribution is non–defective if and only if the ma-
trix I −T is invertible. Then the expected number Eij
E
of visits to state j before
absorption, given that the Markov chain X starts in state i, is Eij
E
= (I−T)−1
ij .
As already stated in the deﬁnition, we shall always assume that 1, . . . , m are
transient states, i.e. that I −T is invertible. The following examples show the
high versatility of the introduced class of distributions.
Example 13.3 Let p = (pn : n ∈N) denote a geometric distribution with
parameter q, i.e. pn = (1 −q)qn−1 for all n ∈N. Then p has a discrete PH
representation with order m = 1, α = 1, and T = q. The exit vector is given
by η = 1 −q.
Example 13.4 A generalization of the geometric distribution is the negative
binomial distribution. For parameters N ∈N, the number of successes sought,
and q ∈]0, 1[, the probability of success, a distribution p is negative binomial
if pn =
N+n−1
n

qN(1 −q)n−N for all n ≥N. The value pn is the probability
of observing n trials until the Nth success. For the special case N = 1 we
obtain the geometric distribution.

Discrete Time Approaches
231
The distribution p has a discrete PH representation with order m = N, initial
phase distribution α = e1 = (1, 0, . . . , 0), and T given by the entries
Tij
T
=
⎧
⎪
⎧
⎨
⎪
⎪
⎨
⎩
⎪
1 −q,
i = j
q,
j = i + 1 ≤N
0,
else
The exit vector is η = (0, . . . , 0, q)T .
Example 13.5 Any discrete distribution p with ﬁnite support, i.e. pn = 0
for n > m with some m ∈N, has a discrete PH representation. We write
p = (p0, . . . , pm). Then there are two possibilities for such a representation.
One is called the remaining time representation.
Here we set ˜α = p,
Ti,i
T
−1 = 1 for 1 ≤i ≤m, and Tij
T
= 0 otherwise. This implies an exit
vector η = (1, 0, . . . , 0).
The other is called the elapsed time representation. For this we set α0 = p0,
α = (1 −p0, 0, . . . , 0) and
Tij
T
=

1 −pi/(1 −i−1
k=0 pk),
j = i + 1 ≤m
0,
else
This time we have an exit vector η = (η1, . . . , ηm−1, 1) with entries deter-
mined by ηi = pi/(1 −i−1
k=0 pk).
The z–transform of a discrete phase–type distribution p is given by
p∗(z) =
∞

n=0
pnzn = α0 +
∞

n=1
αT n−1ηzn = α0 + z · α
∞

n=1
(zT)n−1η
= α0 + zα(I −zT)−1η
(13.2)
for |z| ≤1. This expression yields the factorial moments for a random variable
Z ∼PHd
H (α, T), namely
E(Z · (Z −1) · . . . · (Z −k + 1)) = k!α(I −T)−kT k−11
(13.3)
for all k ∈N. This formula is obtained by differentiating (13.2) k times with
respect to z and evaluating at z = 1. In particular, the mean time to absorption
is given by
E(Z) = α(I −T)−11
(13.4)
Another expression for this will be derived in corollary 13.6.

232
AN INTRODUCTION TO QUEUEING THEORY
2.
Like the discrete time version of phase–type distributions, we can deﬁne batch
Markovian arrival processes in discrete time, too. Let Y = (N, J ) denote a
Markov chain with state space
E = N0 × {1, . . . , m}
where m ∈N is some ﬁnite number. For a state (n, i) we call the ﬁrst dimen-
sion n the level and the second dimension i the phase. Let 0 denote the matrix
with all entries being zero. If the transition matrix of Y has a block structure
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
D0
D1
D2
D3
. . .
0
D0
D1
D2
...
0
0
D0
D1
...
...
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
and D := ∞
n=0 Dn is irreducible, then Y is called a discrete batch Markov-
ian arrival process or shortly discrete BMAP. The Markov chain determined
by the transition matrix D is called the phase process of Y.
Like the continuous time analogue, we want to use discrete BMAPs as a model
for arrival streams. Thus we always assume that D0 is strictly substochastic,
i.e. there is an index n ∈N with Dn ̸= 0
̸
. Then Y is clearly transient and does
not have a stationary distribution.
The Toeplitz structure of P implies that the transition probabilities
P(Y1
Y = (n + k, j)|Y0
Y = (n, i)) = Dk(i, j) = P(Y1
Y = (k, j)|Y0
Y = (0, i))
are homogeneous in the ﬁrst dimension of the state space. Hence the n–step
transition probabilities are determined by the values
Pk
P ;i,j(n) := P(Yn
Y = (k, j)|Y0
Y = (0, i))
Deﬁne the m × m matrices Pk
P (n) := (Pk
P ;i,j(n))i,j≤m and the sequences
P(n) := (Pk
P (n) : k ∈N0) of matrices. By deﬁnition
P(0) = (I, 0, 0, . . .)
with I denoting the m × m identity matrix. Deﬁne convolutions of matrix
sequences as in section 4. Further write ∆= (Dn : n ∈N0). Then clearly
P(1) = ∆, and we can show that
P(n) = ∆∗n
BMAPs in Discrete Time

Discrete Time Approaches
233
for all n ∈N by the induction step
Pk
P (n + 1) =
k

i=0
Pi
P (n)Dk−i =
k

i=0
∆∗n
i Dk−i = ∆∗(n+1)
k
which holds for all k ∈N0. The z–transform of P(n) is given by
P ∗
n
P (z) =
∞

k=0
Pk
P (n)zk =
∞

k=0
∆∗n
k zk =
 ∞

k=0
Dkzk
n
for all n ∈N. Hence the expectation matrix of the number of arrivals within n
time slots is
E(P(n)) = ∂
∂z P ∗
n
P (z)
,,,
,,
,,
z=1
= n · D ·
∞

k=1
kDk
Let π = πD denote the stationary distribution of the phase process. Then the
expected number of arrivals within n time slots given that Y starts with phase
distribution π is obtained as
Eπ(Nn
N ) = n · π
∞

k=1
kDk1
(13.5)
for all n ∈N.
Consider now a discrete phase–type distribution with characterization (α, T).
As usual, deﬁne the exit vector by η := 1 −T1. A special class of discrete
BMAPs arises if we set D0 := T, D1 := ηα and Dn := 0 for n ≥2. This is
called a discrete PH renewal process or shortly a PHd
H renewal process. For
the stationary phase distribution π = πD with D = Tηα, expression (13.5)
speciﬁes to Eπ(Nn
N ) = n · πη for all n ∈N. Now the same argument as for
corollary 10.12 holds. The described BMAP is a renewal process (in contin-
uous time, denoted by ˜N) with initial delay X0 ∼PHd
H (π, T) and renewal
intervals Xn
X ∼PHd
H (α, T). The elementary renewal theorem 6.12 states that
lim
t→∞
E( ˜Nt
N )
t
= lim
t→∞
E( ˜N⌊t⌋)
⌊t⌋
⌊t⌋
t
= lim
n→∞
Eπ(Nn
N )
n
=
1
E(X1)
Thus we obtain another expression for the mean of a discrete phase–type dis-
tributed random variable (cf. corollary 13.6).
Corollary 13.6 For a PHd
H (α, T) distributed random variable X the expec-
tation is given by E(X) = (πη)−1, where π = π(T + ηα) is the stationary
phase distribution.

234
AN INTRODUCTION TO QUEUEING THEORY
3.
Blockwise Skip–Free Markov Chains
In chapters 11 and 12 we have analyzed Markov chains of a blockwise Hessen-
berg structure, i.e. they were blockwise skip–free in one direction. For each of
them we have developed an own method of ﬁnding the stationary distribution.
Both methods employed matrices of central importance for the formulation of
the stationary distribution. In the former case it was an expectation matrix
called R, in the latter case a stochastic matrix called G.
For the special case that a Markov chain is blockwise skip–free in both direc-
tions, we can hope to combine both approaches and thus obtain more results.
This shall be pursued in this section. We further will see that this kind of
Markov chains can be used as the basic tool for analyzing queues in discrete
time.
An irreducible transition matrix structured as
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
B
C
D
A1
A0
A2
A1
A0
A2
A1
A0
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
with matrices B, C, D, and Ai having dimensions n × n, n × m, m × n,
and m × m, respectively, is called blockwise skip–free. This matrix deﬁnes a
Markov chain X with state space
E = {(0, i) : 1 ≤i ≤n} ∪{(n, j) : n ∈N, 1 ≤j ≤m}
with n, m ∈N. The ﬁrst dimension of a state is called level, the second phase.
The special case n = m = 1 is called a skip–free Markov chain (cf. section
2 for the continuous time analogue).
The matrix P satisﬁes the conditions of blockwise Hessenberg structure in both
directions. Hence the approaches for analyzing the embedded Markov chains
in chapters 11 and 12 apply both. The only difference to be considered are the
matrices B, C, and D at the boundary.
As in section 2 we can deﬁne a rate matrix R which satisﬁes
R = A0 + RA1 + R2A2
according to theorem 11.3. Theorem 11.1 tells us that if there is a stationary
distribution x = (xn : n ∈N0) for X, then it satisﬁes the relation
xn+1 = xnR
or
xn = x1Rn−1
(13.6)

Discrete Time Approaches
235
for all n ∈N. On the other hand we can deﬁne a matrix G as in section 2 by
G = A2 + A1G + A0G2
due to theorem 12.2. According to theorem 12.1 we know that G is stochastic
if X is recurrent.
Equation (13.6) reduces the problem of ﬁnding a stationary distribution x for
P to the determination of x0 and x1. To this aim, we consider the Markov
chain X F restricted to the subset
F = {(0, i) : 1 ≤i ≤n} ∪{(1, j) : 1 ≤j ≤m}
of the state space E. Because P is blockwise skip–free, states in level 0 do not
communicate with states in F c = E \ F. Hence transitions from F to F c and
back must go via level 1. Thus we arrive at a transition matrix
P F =

B
C
D
U

for X F , where only the lower right–hand entry remains to be determined. The
matrix U contains all probabilities to go from level 1 back to level 1 in a ﬁnite
number of steps without entering level 0.
Clearly the probabilities for one step are contained in A1, whence we obtain
U = A1 + U′. The respective probabilities for more than one step (which are
contained in U′) must consider visits to the set F c in all but the last step. The
blockwise skip–free structure of P implies that the ﬁrst step must go from level
1 to level 2, for which the transition probabilities are contained in A0. Then we
need the probabilities to go from level 2 back to level 1 in a ﬁnite number of
steps. By deﬁnition these are contained in the matrix G (see section 2). Hence
we obtain
U = A1 + A0G
and thus have determined P F completely.
Theorem 2.29 states that positive recurrence of X implies positive recurrence
of X F . Hence P F admits a stationary distribution xF = (xF
0 , xF
1 ) as the
solution of the linear equation system
(xF
0 , xF
1 ) = (xF
0 B + xF
1 D, xF
0 C + xF
1 U)
Deﬁne c := xF
0 1 + xF
1 (I −R)−11. Then theorem 2.29 yields that
x0 = c−1xF
0 ,
x1 = c−1xF
1 ,
xn+1 = x1Rn

236
AN INTRODUCTION TO QUEUEING THEORY
for all n ∈N is the stationary distribution of X. In fact, we can verify
∞

n=0
xn1 = x01 + x1
∞

n=0
Rn1 = c−1(xF
0 1 + xF
1 (I −R)−11) = 1
and theorem 2.29 states that (x0, x1) and (xF
0 , xF
1 ) differ by a constant multi-
ple only.
Deﬁne A := A0 +A1 +A2. Since P is irreducible and stochastic, so is A, and
thus there is a stationary distribution π = πA. By theorems 11.10 and 11.8 we
know that X is positive recurrent if the condition
πA11 + 2 · πA21 > 1
holds. Using the deﬁnition of A we obtain
1 = πA1 = πA01 + πA11 + πA21
which yields the equivalent condition
πA01 < πA21
(13.7)
for positive recurrence of X.
4.
The PH/PH/1 Queue in Discrete Time
As an application we shall analyze the PH/PH/1 queue in discrete time. Inter–
arrival times as well as service times are iid and have a discrete phase–type
distribution, named A and B respectively. The former has characterization
(α, T) of order n, the latter (β, S) of order m. We set α0 = β0 = 0 in order
to avoid batch arrivals and instantaneous services. Denote the exit vectors by
η = 1 −T1 and ζ = 1 −S1.
Note that by example 13.5, the discrete time GI/G/1 queue is a special case
of the PH/PH/1 queue if inter–arrival and service time distributions have ﬁnite
support. By example 13.3, the M/M/1 queue in discrete time as examined in
section 6 is a special case, too.
For any time index n ∈N0, deﬁne the random variables Nn
N as the number of
users in the system, Kn
K
as the phase for the inter–arrival time, and Jn
J
as the
phase for the service time. Then the system process Q = ((Nn
N , Kn
K , Jn
J ) : n ∈
N0) is a Markov chain with state space
E = {(0, k) : 1 ≤k ≤n} ∪{(n, k, j) : n ∈N, 1 ≤k ≤n, 1 ≤j ≤m}

Discrete Time Approaches
237
and transition matrix
P =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
⎜
B
C
D
A1
A0
A2
A1
A0
A2
A1
A0
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
⎟
where
B = T,
C = (ηα) ⊗β,
D = T ⊗ζ
A0 = (ηα) ⊗S,
A1 = T ⊗S + (ηα) ⊗(ζβ),
A2 = T ⊗(ζβ)
Here the composition ⊗represents the Kronecker product, which is deﬁned in
the pretext of theorem 9.13. The matrices B, C, D, and Ai are of dimensions
n×n, n×nm, nm×n, and nm×nm, respectively. We see that P is blockwise
skip–free which allows us to use results from the preceding section.
Deﬁne A := A0 + A1 + A2 and let π = πA denote the stationary distribution
for A. Using exercices 13.4 and 13.5, we obtain
A = (ηα) ⊗S + T ⊗S + (ηα) ⊗(ζβ) + T ⊗(ζβ)
= (ηα + T) ⊗S + (ηα + T) ⊗(ζβ)
(13.8)
= (T + ηα) ⊗(S + ζβ)
(13.9)
This yields
π = α∗⊗β∗
(13.10)
with α∗= α∗(T + ηα) and β∗= β∗(S + ζβ). Condition (13.7) for positive
recurrence of Q speciﬁes to
(α∗⊗β∗) (ηα ⊗S) 1 < (α∗⊗β∗) (T ⊗ζβ) 1
⇐⇒
α∗ηα1 · β∗S1 < α∗T1 · β∗(ζβ)1
(13.11)
⇐⇒
α∗η · β∗(1 −ζ) < α∗(1 −η) · β∗ζ
⇐⇒
α∗η < β∗ζ
By corollary 13.6 this is equivalent to
E(B) < E(A)
which is our usual condition that the mean service time is strictly smaller than
the mean inter–arrival time.

238
AN INTRODUCTION TO QUEUEING THEORY
Notes
Discrete PH distributions have been introduced in Neuts [62, 65]. For an early
application and discrete time MAPs see Alfa and Neuts [4]. A text book pre-
sentation can be found in Latouche and Ramaswami [52]. Discrete phase–type
distributions with inﬁnitely many phases are introduced in Shi and Liu [79].
The analysis of the GI/G/1 queue in discrete time is taken from Alfa and Li [3]
and Alfa [2]. An overview on further results is given in Alfa [1].
Exercise 13.1 Prove lemma 13.2.
Exercise 13.2 Verify the remaining and elapsed time representations intro-
duced in example 13.5.
Exercise 13.3 Prove formula (13.3) for the factorial moments of a discrete
phase–type distribution.
Exercise 13.4 For matrices A, B, and C of appropriate dimensions, prove the
distributive laws
A ⊗C + B ⊗C = (A + B) ⊗C
A ⊗B + A ⊗C = A ⊗(B + C)
for the Kronecker product, and verify equalities (13.8) and (13.9).
Exercise 13.5 For matrices A, B, C, and D of appropriate dimensions, prove
the associative law
(A ⊗B)(C ⊗D) = AC ⊗BD
for the Kronecker product, and verify formula (13.10) and the equivalence
(13.11).
Exercise 13.6 Analogous to an interrupted Poisson process (IPP), we deﬁne
an interrupted Bernoulli process. There are two phases (numbered 1 for ”on”
and 0 for ”off”). In every time slot there is a probability p to switch from phase
0 to phase 1 and a probability q to switch back. In phase 1 there is a probability
r of observing one arrival in a time slot. Give an exact deﬁnition in terms of a
transition matrix for a discrete MAP. Note the difference to the IPP due to the
possibility of phase change and arrival occuring in the same time slot.

Chapter 14
SPATIAL MARKOVIAN ARRIVAL PROCESSES
With respect to queueing theory we have, for several times, pointed to the
application area of telecommunication networks. In fact, during the last two
decades the analysis of complex systems of that type has become the most
signiﬁcant issue in applied queueing theory. Modern communication facilities
represent articles of daily use, and are outﬁt accessories of pedestrians, car
drivers, pilots, and nearly all people who need the contact to other people or
to data processing devices. Mobility and spatial distribution are characteristic
features of these systems. The installation of mobile communication networks
(that started in a technically useful form as early as in 19821) is accompanied in
many cases by a partition of a geographic region into cells covering the whole
area. Customers of such networks get active randomly in time, and are moving
around in and across the cells, eventually stopping their activities (vanishing
as network users) after having been serviced by the providing company.
Transferred into the language of queueing theory we are confronted thereby
with a new type of arrival process and a new species of customers, namely
processes that put their arriving elements (customers) onto certain locations,
and customers who start moving immediately after appearing at a location.
Arrival processes of that kind are characterized by a random behavior in time
and space.
In previous chapters we have stepped through various queueing models until
reaching types ”beyond the exponential”, and we saw that Markovian arrival
processes (MAPs) belong to the most versatile tools for describing the dy-
namics of modern computer networks (and related conﬁgurations). What we
1The Advanced Mobile Phone System (AMPS), developed by Bell Laboratories, USA.

240
AN INTRODUCTION TO QUEUEING THEORY
are now about to do is a generalization of these processes to spatial arrival
processes.
1.
Arrivals in Space
The Markovian arrival processes (Nt
N , Jt
J )t≥0 considered so far had state space
N0 × E, where E = {1, . . . , m} represented the phase space, Nt
N a counting
variable, and (Jt
J )t≥0 a time-homogeneous Markov process. Nothing was said
about ”where” an arrival occurs, or what kind of additional information we can
assign to the ”customers” or ”jobs” that arrive according to a MAP.
The properties of N0 that we needed for an adequate description of the counting
variable Nt
N may be seen as to be the following:
(i) We can measure (count) the ”jobs” that arrivals bring into the system
(whatever the latter is),
(ii) we can add sets of ”jobs” that arrived, i.e. the number of ”jobs” that sev-
eral (possibly not subsequent) arrival events produce is the sum of all indi-
vidual arrival sets, and
(iii) (Nt
N )t≥0 is an increment process with respect to the portions that arrivals
add to the system, i.e., for α ⊂N0 with A = 
αi∈α αi, and K ⊂J, we
have
P((Ns
N +t, Js
J +t) ∈A × K | Ns
N = n, Js
J = i) =
P((Ns
N +t, Js
J +t) ∈(A −n) × K | Ns
N = 0, Js
J = i).
This somewhat artiﬁcially looking description attains its meaning next when
we are going to generalize the concept of a MAP by adding information to the
arriving elements (e.g. jobs). In more general terms, namely, a MAP can be
regarded as a two-dimensional Markovian jump process (Nt
N , Jt
J )t≥0 with state
space U × E, where E represents the phase process, and U has the following
properties:
1 There is a σ-algebra U such that (U, U) is a measurable space with {u} ∈U
for any u ∈U.
2 (U, +) forms a semi-group with neutral element o.
3 For A ⊂U, K ⊂J, and A −u = {v ∈U : v + u ∈A},
P((Ns
N +t, Js
J +t) ∈A × K | Ns
N = u, Js
J = i) =
P((Ns
N +t, Js
J +t) ∈(A −u) × K) | Ns
N = o, Js
J = i).

Spatial Markovian Arrival Processes
241
In fact, any MAP can be regarded as a so-called Markov-additive jump process,
being deﬁned in general terms as follows. Let U denote a set with properties 1
- 3, and E ⊂N0.
Deﬁnition 14.1 A two-dimensional process (Yt
Y , Jt
J )t≥0 on a state space U ×
E is called a Markov-additive jump process if (i) (Yt
Y , Jt
J )t≥0 is a Markov
process, and (ii) for s, t ≥0, the conditional distribution of (Ys
Y +t −Ys
Y , Js
J +t),
given (Ys
Y , Js
J ), depends only on Js
J .
It is easy to see that, for any Markov-additive jump process (Yt
Y , Jt
J )t≥0, the
(phase) component (Jt
J )t≥0 forms a Markov jump process and (Yt
Y )t≥0 has
conditionally independent increments.
That is, given the states of Jt
J ν for
0 ≤ν ≤n, the random variables
Yt
Y 1 −Y0
Y , Yt
Y 2 −Yt
Y 1, . . . , Yt
Y n −Yt
Y n−1
are conditionally independent for known J0
J , Jt
J 1, . . . , Jt
J n.
Let us now consider Markovian arrival processes in which an arrival event
means the appearance of customers at speciﬁc locations, or simply the ap-
pearance of points in some space X. We may speak of localizable arrivals in
this case, and of a spatial arrival process. The term ”spatial” requires some
explanation. Being accustomed to think in terms of the Euclidian space, usu-
ally everybody takes for granted properties of the space X that have particular
mathematical signiﬁcance. Such properties are (among others)
(i) X is a metric space with metric d : X × X →R0.
(ii) Any Cauchy sequence {xn} in X is convergent.2
(iii) X contains a countable dense subset.
A space with these properties is called a Polish space. Since (ii) means com-
pleteness, and (iii) separability, a Polish space can be deﬁned more precisely as
a complete separable metric space (X, d). In such a space any compact subset
is closed, and any isolated point constitutes a closed subset. This is the type
of space we take as a basis, i.e. when speaking of localizable arrivals. Accord-
ingly, when using the notion of a spatial arrival process we shall constantly
refer to arrivals in a Polish space.
2A sequence satisfying d(xn, xm) →0 as m, n →∞.

242
AN INTRODUCTION TO QUEUEING THEORY
The appearance of (ﬁnitely many) points in X can mathematically be inter-
preted as the occurrence of some certain (ﬁnite) counting measure ν. This
is due to the fact that counting measures are the primary ingredients of point
ﬁelds. Let B(X) denote the Borel σ-algebra of (X, d), and µ a locally ﬁnite
measure on B(X), i.e. a measure with the property that for each x ∈X there
is some open vicinity U(x) such that µ(U) < ∞. Assume that µ has the
following regularity property:
µ(A) = sup{µ(K) : K ⊂A, K compact}
for any A ∈B(X).
Then µ is called a Radon measure, and if the range of µ is N0, it is called a
(Radon) counting measure. Any counting measure deﬁnes what we may call
a point ﬁeld in X. We know that arrival events in a Markovian arrival process
(MAP) occur randomly in time. In a similar way, when assuming that each
arrival speciﬁes a set of points (or, in case, a single point) in space, we should
naturally propose that these points are located randomly in the space. A spatial
MAP, hence, produces random point ﬁelds in a space X over time.
What is the precise mathematical description of a random point ﬁeld? Let V
denote the family of all counting measures, and let, for each subset S ∈B(X),
Vn(S) be the set of all measures µ with µ(S) = n. The σ-algebra V that is
generated by the family
M =
%
Vn(S) : n ∈N0, S ∈B(X)
&
(of sets of measures) deﬁnes V as a measurable space (V, V) of counting mea-
sures over the Polish space (X, d). The σ-algebra V is rich enough to allow the
distinction of single measures, i.e. every singleton {µ} in V is measurable.3
The answer to our above question is easy now: Given some probability space
(Ω, A, P), a random point ﬁeld is nothing else than a measurable mapping
F : Ω→V.
Obviously, the family V of counting measures over (X, d) forms a semi-group
with respect to addition, where the sum µ + ν of measures is deﬁned as the
measure (µ + ν)(S) = µ(S) + ν(S) for all S ∈B(X). The neutral element o
is the measure that assigns zero points to any subset S ∈B(X). We introduce
the following notation: Let A, B ∈V; then
A −ν
=
{µ ∈V : µ + ν ∈A},
µ + B
=
{µ + ν ∈V : ν ∈B},
A −B
=
{µ ∈V : µ + B ⊂A}.
3This is due to the fact that a locally ﬁnite measure µ on (X, d) is determined already by its values µ({x})
on the singletons x in X.

Spatial Markovian Arrival Processes
243
From these properties it is easy to deduce that V can well be used as the state
space of the counting variable of some Markov-additive jump process. As that
it is an arrival process for measures. More precisely, we introduce what we call
an SMAP for short.
Deﬁnition 14.2 A homogeneous Markov-additive jump process (Yt
Y , Jt
J )t≥0
with state space V × E, where (V, V) is a measurable space of counting mea-
sures over a Polish space (X, d), is called a spatial Markovian arrival process
or SMAP.
Each jump in an SMAP (Yt
Y , Jt
J )t≥0 is to be interpreted as the arrival of some
point ﬁeld in X (corresponding to a ﬁnite measure ν over X) together with some
certain phase transition i →j (i, j ∈E). Yt
Y is the random variable describing
the very point ﬁeld that is created by superposition of all those locally ﬁnite
counting measures that arrived under the SMAP up to time t.
For A ⊂V and K ⊂J, the probabilities
P(Yt
Y ∈A, Jt
J ∈K | Y0
Y = o, J0
J = i) =: pt(0, i; A × K)
deﬁne what is usually called the transition kernel of the process. Accordingly,
we call
d
dtP(Yt
Y ∈A, Jt
J ∈K | Y0
Y = o, J0
J = i) =: q(0, i; A × K)
the transition rate kernel of the SMAP (here we propose A × K ̸≠
{(o, i)}).
Using this notation we deﬁne, for each subset S ∈B(X), the subset speciﬁc
transition kernels
Pt
P (n; S) = (Pt
P (n, ij; S))i,j∈E = (pt(0, i; Vn(S) × {j}))i,j∈E
(14.1)
as well as the subset speciﬁc transition rate kernels
Dn(S) = (Dn;ij(S))i,j∈E = (q(0, i; Vn(S) × {j}))i,j∈E.
(14.2)
Now we are in the position to deﬁne a correlated family of subset speciﬁc spa-
tial BMAPs by counting, in any ﬁxed subset S ∈B(X), the points that occur
within S according to the arrivals under the SMAP (Yt
Y , Jt
J )t≥0. Since a batch
Markovian arrival process, as a Markov process, is completely determined by
its generator, it sufﬁces to specify a generator G(S) for any measurable subset
S of X. This is done by setting
G(S) =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
D0(S)
D1(S)
D2(S)
. . .
O
D0(S)
D1(S)
. . .
O
O
D0(S)
. . .
...
...
. . .
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟.
(14.3)

244
AN INTRODUCTION TO QUEUEING THEORY
The assertion that G(S) is a generator matrix with
∞

n=0

j∈E
Dn;ij(S) = 0
∀i ∈E
is justiﬁed by the following lemma.
Lemma 14.3 For every S ∈B(X), the sum ∞
n=0 Dn(S) = D forms the
generator matrix of the phase process (Jt
J )t≥0 of the SMAP (Yt
Y , Jt
J )t≥0, inde-
pendently of S.
Proof: Excluding transitions of the form (o, i) →(o, i) we have, for j ̸≠
i,
∞

n=0
Dn;ij(S)
=
∞

n=0
q((o, i), Vn(S) × {j})
=
q((o, i),
/
n∈N0
Vn(S) × {j})
=
d
dtp
 
t; (o, i),
/
n∈N0
Vn(S) × {j}
!
=
d
dtP
 
Jt
J = j | J0
J = i
!
= Dij .
On the other hand, for j = i,
∞

n=0
Dn;ii(S)
=
Dii(S) = −

j̸=
̸
i
Dij(S)
=
−

j̸=
̸
i
∞

n=0
Dn;ij(S) = −

j̸=
̸
i
Dij = Dii ,
and hence, ∞
n=0 Dn(S) = D, independently from the choice of S ∈X.
□
The BMAP (Nt
N (S), Jt
J )t≥0 with generator matrix G(S) is called a spatial
BMAP over the subset S ∈B(X), or SBMAP over S for short.
Notice, that a common BMAP as introduced in chapter 10 can be seen as an
SMAP over a single point X = {x} with time-homogeneous phase process.
If E = {1}, then (Yt
Y )t≥0 is a space-time Poisson process with general spatial
distribution. Such Poisson processes have been considered by Serfozo [77] and
Breuer [21].

Spatial Markovian Arrival Processes
245
2.
Properties of Spatial MAPs
We call an SMAP regular, if 
(n,j)=(0
̸
,i) Dn;ij(S) = −D0;ii(S) > 0, and
stable if −D0;ii(S) < ∞for any S ∈B(X), i.e. an SMAP is stable if the
total arrival rate connected with any phase transition is ﬁnite. We assume reg-
ularity and stability throughout. Let us ask for the phase depending probabil-
ities Pt
P (k; S) that k customers have arrived until time epoch t in some subset
S ∈B(X). Since, for ﬁxed S, we can proceed as in case of a common BMAP,
we immediately obtain
Pt
P (k; S) =
 
e∗∆(S)·t!
k =
∞

n=0
tn
n!∆(S)∗n
k ,
where
∆(S)∗n
k = G(S)n
0 k
for k ≥0.
As a consequence, the counting variables Nt
N (S) satisfy
P(Nt
N (S) = n, Jt
J = j | N0
N (S) = 0, J0
J = i) = Pt
P (n, ij; S).
(14.4)
We also have Pt
P (0; S) = eD0(S)·t as before, and Pt
P (n; S) =

e∗∆(S)·t
n.
Written in block matrix form, Pt
P (S) reads
Pt
P (S) =
⎛
⎜
⎛
⎜
⎜
⎜
⎜
⎝
⎜
eD0(S)·t

e∗∆(S)·t
1

e∗∆(S)·t
2
. . .
O
eD0(S)·t

e∗∆(S)·t
1
. . .
O
O
eD0(S)·t
. . .
...
...
...
...
⎞
⎟
⎞
⎟
⎟
⎟
⎟
⎠
⎟.
Justiﬁed by our result ∞
n=0 Dn(S) = D, the phase process (Jt
J : t ≥0) is the
same for any SBMAP over S ∈B(X) and plays the same role as in case of a
common BMAP. Hence, the form of the transition matrix P Φ
t
P
of (Jt
J : t ≥0)
remains unchanged as P Φ
t
P
= eD·t.
Facing this and the above statements it is obvious that nearly all properties of
a common BMAP reappear as those of an SMAP when considering only one
ﬁxed subset S ∈B(X).
There are two points, but, that have to be emphasized when dealing with an
SMAP:
1 We have to determine also joint distributions of points in different subsets
Sν, ν = 1, . . . , K, for any given family {S1, . . . , SK} ⊂B(X) in order to
fully deﬁne a spatial arrival process (and to allow realistic applications of
the theory).

246
AN INTRODUCTION TO QUEUEING THEORY
2 There is a need for recipies for deﬁning the random point ﬁelds that occur
according to arrivals under the SMAP.
Addressing the ﬁrst point, consider a family of measurable subsets S1, . . . , SK
in the Polish space X. The joint distribution of points in such a family that
accumulated due to arrivals under the SMAP can be determined as follows.
Let the sets S1, . . . , SK ∈B(X) be disjoint4, and set S = (S1, . . . , SK),
n = (n1, . . . , nK). We use the notation
Vn(S)
=
{ν ∈V : ν(Sk) = nk, 1 ≤k ≤K},
Pt
P (n, i, j; S)
=
p(t; i, Vn(S) × {j}),
D(n, i, j; S)
=
q(i, Vn(S) × {j}),
Pt
P (n; S)
=
(Pt
P (n, i, j; S))i,j∈E,
D(n; S)
=
(D(n, i, j; S))i,j∈E,
Pt
P (S)
=
{Pt
P (n; S)}n∈NK
0 ,
∆(S)
=
{D(n; S)}n∈NK
0 .
Then, due to conditional independence of increments, the Chapman-Komogorov
equations hold exactly as in the case of a common BMAP, i.e. written in con-
volutional form,
Pt
P +τ(S) = Pt
P (S) ∗Pτ
P (S).
By subtracting Pt
P (S) on both sides and forming the differential quotient, we
obtain the Chapman-Kolmogorov differential equations:
d
dtPt
P (S) = ∆(S) ∗Pt
P (S) .
Similar to the case of one-dimensional Markov processes as seen in chapter
10, the solution of these equations takes the (convolutional) exponential form:
Pt
P (S) = e∗∆(S) t,
Pt
P (n; S) =
∞

k=0
tk
k! (∆∗k(S))n .
Thus, the expressions for joint distributions of customer populations in disjoint
subsets formally resemble those of a one-dimensional BMAP. The correspond-
ing expressions for the SMAP with respect to one single subset S ∈B(X) are
obtained from the results for K = 1.
Addressing now the second point, one possible method to specify the types of
the random point ﬁelds occurring as arrivals under an SMAP is the following.
4It should be obvious that there is no loss of generality by that assumption, since intersections may be
treated as separate subsets.

Spatial Markovian Arrival Processes
247
Start with some common BMAP that is given in form of its phase process
(Jt
J : t ≥0) and its rate matrices Dn = (Dn;ij)i,j∈E. Then deﬁne a family
Φ = {φij : i, j ∈E}
of probability measures over the Polish space (X, d), such that, for any S ∈
B(X), φij(S) represents the probability that an arriving batch in coincidence
with a phase transition from i to j is located in S. To be more concrete, let
pi(n, j) denote the probability that the BMAP, upon changing its phase from i
to j, creates a batch of size n ≥0. Then we deﬁne
pi(n, j; S)
=
pi(n, j) φij(S)
for all
i, j ∈E, n ≥1,
pi(0, j; S)
=
pi(0, j) +
∞

n=1
pi(n, j; X \ S) for all j ̸≠
i.
In this notation pi(n, j; S) is the probability that a batch of size n is located
in S in coincidence with a phase transfer from i to j. Let, as usual, Nt
N be the
random number of jobs arrived until t according to the BMAP, and γiγ be the
total instantaneous transition rate when the phase is i, i.e.
γiγ =
∞

n=0

j∈E
j̸=
̸
i
Dn;ij +
∞

n=1
Dn;ii,
i ∈E.
The BMAP (Nt
N , Jt
J )t≥0 is completely described by its rate matrices, and so
would be our spatial MAP if its rate matrices were given in turn. The latter,
now, can be easily realized for each S ∈B(X) by setting
D0;ii(S)
=
−γiγ

1 −
∞

n=1
pi(n, i; X \ S)

,
D0;ij(S)
=
γiγ pi(0, j; S)
for j ̸≠
i,
Dn;ij(S)
=
γiγ pi(n, j; S)
for n ≥1.
These matrices deﬁne the generator of the SBMAP over S and thereby the
process itself. The family Φ of probability measures φij over X determines
where to locate a batch, and the size of any batch arriving under the BMAP
speciﬁes the number of points at that very location. This is for sure a somewhat
restrictive speciﬁcation of the random point ﬁelds (each by intuition may be
seen as to be a superposition of points at one location), but since the locations
themselves vary according to the measures φij the method works in practice.
Another method may consist in assigning, to each pair (i, j) ∈E × E some
positive integrable fuction ξij : X →R+, such that each S ∈B(X) contains a

248
AN INTRODUCTION TO QUEUEING THEORY
random number N(S) of points with probability
ϕij(S)n
n!
e−ϕij(S),
where
ϕij(S) =

S

ξij(x)dx.
This leads us to some simple type of a random Poisson ﬁeld with mean ϕij(S)
for any S ∈B(X). The connection to SMAP arrivals is given by setting
pi(n, j; S)
=
ϕij(S)n
n!
e−ϕij(S)
for n ≥1,
pi(0, j; S)
=
e−ϕij(S)
for j ̸≠
i,
pi(0, i; S)
=
0.
Let the total transition rate γiγ out of some state (v, i) for an SMAP be given.
Then, assuming E = {1, . . . , m},
Dn;ij(S)
=
γiγ pi(n, j; S)
for (n, j) = (0
̸
, i)
D0;ii(S)
=
−γiγ (m −e−ϕii(S)),
where γiγ is some ﬁnite positive constant for every i ∈E.
Comments on the modelling of customer motion.
It is easy to see that we can describe the movement of customers by time de-
pendent mappings of the Polish space (X, d) into itself. A customer in a mobile
communication system, for example, who is located at time t0 at a point x(t0)
when requesting a call, may move from x(t0) to y = x(t1) during a time in-
terval (t0, t1]. That way, if he is active for, say, T time units, he follows some
curve {x(s) : s ∈[t0, t0 + T]} through the landscape and then vanishes from
the system — from the point of view of the telecommunication provider — due
to call completion. In our models the ”landscape” is part of the Polish space
(X, d), and the curve relates the points x(t) to the user’s starting point x(t0)
according to some rule Υt : x(t0) →x(t0 + t) = x(t) for 0 ≤t ≤T. If we
know the parameters x(t0) (the arrival location), T (the service time duration),
and Υt (the rule for the displacement after t time units) for each user, we can
decide at any time whether or not there is an active user in the system at some
arbitrary location y. The system would be a queueing system in space and
time. For its analytical description a pecularity has to be taken into account:
In reality, the point mappings Υt : x(t0) →x(t0 + t) are resembling random
walks in most cases since human customers normally behave individually and
the curves they follow are random in general, and completely different. One

Spatial Markovian Arrival Processes
249
way to cope with this problem is to prescribe probabilities for the displace-
ment of customers with respect to their arrival locations and arrival times. A
much more simple approach is based on the assumption of deterministic mo-
tion where the (Υt : t ≥0) form a given topological group of mappings. In
fact, this restriction is not that serious as it may seem at a ﬁrst glance. On
the one side, in many practical situations one is faced with the task to model
the impact of movements that take place along streets or railway lines, such
that there are streams of uniformly moving individuals or cars or trains subject
to the same deterministic law. On the other side, the superposition of sev-
eral (ﬁnitely many) streams of that type may well mirror an average behaviour
of customers in more complex conﬁgurations. Such superpositions, although
causing additional analytical complexity, can be handled in principle without
problems. Let us shortly indicate how to determine time dependent probabili-
ties for the spatial distribution of customers (users of some facility, jobs, etc.)
in a space-time queueing system.
We assume that there is a service process deﬁned that reﬂects the treatment of
the customers up to their disappearance out of the system. Let R = (Υt : X →
X, t ≥0) be an abelian topological group with the topology O = {Υs : s ∈
O, O open in (X, d)}. Then the following holds.
1 Given any neighbourhood W of Υt ◦Υs, there are neighbourhoods U and
V such that U ◦V ⊂W.
2 For any neighbourhood V of Υ−1
t
there is some neighbourhood U with
U−1 = {Υ−1
r
: Υr ∈U} ⊂V.
We write Υt+s for Υt ◦Υs, and assume that any customer starts moving im-
mediately after arriving at location x according to the law
Υs(x) = x(s),
s ∈(0, T],
(14.5)
where T is the time he spends in the system. The set
Υs[S] = {y = Υs(x) : x ∈S},
s ≥0
is called the displacement set of S for any S ∈B(X). Similarly, the set
Υ−s[S] = {x : Υs(x) ∈S},
s ≥0
is called the source set of S with respect to Υs[S]. Note that Υ−s[S] =
(Υs)−1[S] due to proposed group property. Given a spatial Markovian arrival
process as mentioned above, we are able to compute the probability matrices
Pt
P (S) describing the phase depending numbers of arrivals up to time t for any

250
AN INTRODUCTION TO QUEUEING THEORY
subset S ∈B(X). A customer is called (S, t)-resident if, after his arrival
somewhere in X at a time u ≤t, his service (that started at u) continues to go
on beyond t, and his location at time t is in S ∈B(X). The random number
of (S, t)-resident customers observed at time u ≤t is denoted by Nu,t
N
(S),
such that Nt
N (S) = Nt,t
N
(S) represents the number of all those customers who
are located in S at time t. Assume that there is a possibility to determine the
probabilities
Qr;ij(u, t; S) = P(Nu,t
N
(S) = r, Ju
J = j | N0
N ,t(S) = 0, J0
J = i)
that deﬁne the distribution of the random variables Nt
N (S) in case that cus-
tomers do not move.5 Then, if movements are allowed and happen according to
the law (14.5), the corresponding distribution for Nt
N (S) is obtained by merely
replacing S in Qr;ij(u, t; S) by its source set Υ−(t−u)[S] and performing the
same computation.
Notes
A ﬁrst deﬁnition of a spatial batch Markovian arrival processes based on the
construction of probability mass functions in the Euclidean space traces back
to Baum [10]. Subsequent treatments and generalizations are due to Baum and
Kalashnikov [11], and Breuer [21]. In [21] a type of spatial process has been
investigated (among others) that is classiﬁed as a space-time Poisson process
with general spatial distribution. This type has been considered also by Ser-
fozo in [77]. An important application area of spatial BMAPs and correspond-
ing queueing models is the performance analysis of todays telecommunication
systems. The handling of customer motion during service in such systems has
been treated by Baum and Kalashnikov [12, 13], and Baum and Sztrik [14].
Exercise 14.1 Let the subset speciﬁc rate matrices of an SMAP be given by
Dn;ij(S)
=
γiγ pi(n, j; S)
for (n, j) = (0
̸
, i)
D0;ii(S)
=
−γiγ

1 −
∞

n=1
pi(n, i; X \ S)

,
where pi(n, j; S) is determined as the probability for the event that an arrival
under a common BMAP occurs in S together with a phase transition i →j,
pi(n, j; S) being deﬁned with means of a family
Φ = {φij : i, j ∈E}
5Techniques for the computation of the Qr;ij(u, t; S) are presented, for example, in [13].

Spatial Markovian Arrival Processes
251
of probability measures over the Polish space (X, d) as mentioned in the text
above.
The particular arrival rates λi(S) into S must be dependent upon the choice of
subset S ∈B(X). The spatial BMAP that is generated with respect to the ﬁxed
chosen subset S can be expressed, for (n, j) = (0
̸
, i), by its rates λi(S) and
routing probabilities πi(n, j; S) according to Dn;ij(S) = λi(S) · πi(n, j; S),
where πi(n, j; S) is the probability that n customers arrive in S together with
a phase transition from i to j. Show, that
λi(S)
=
γiγ

1 −
∞

n=1
pi(n, i; X \ S

,
πi(n, j; S)
=
pi(n, j; S)
1 −∞
n=1 pi(n, i; X \ S)
for (n, j) = (0
̸
, i).
Exercise 14.2 Consider the second version of an SMAP realization given in
the text above, where each point ﬁeld that occurs together with a phase transi-
tion i →j is a Poisson point ﬁeld with mean ϕij. Show that, for any S ∈B(X),
ϕij(S) = E[N(S)], where N(S) is the random variable describing the number
of points in S.


Chapter 15
APPENDIX
1.
Conditional Expectations and Probabilities
Let (Ω, A, P) denote a probability space and (S, B) a measurable space. A
random variable is a measurable mapping X : Ω→S, which means that
X−1(B) ∈A for all B ∈B. In other words, X is a random variable if and
only if X−1(B) ⊂A. In stochastic models, a random variable usually gives
information on a certain phenomenon, e.g. the number of users in a queue at
some speciﬁc time.
Consider any real–valued random variable X : (Ω, A) →(R, B), B denoting
the Borel σ–algebra on R, which is integrable or non–negative. While the
random variable X itself yields the full information, a rather small piece of
information on X is given by its expectation
E(X) :=

Ω

X dP
The conditional expectation is a concept that yields a degree of information
which lies between the full information X and its expectation E(X).
To motivate the deﬁnition, we ﬁrst observe that the distribution P X = P ◦X−1
of X is a measure on the sub–σ–algebra X−1(B) of A, i.e. in order to compute
P(X ∈B) = P X(B) =

X

−1(B)
dP
we need to evaluate the measure P on sets
A := X−1(B) ∈X−1(B) ⊂A

254
AN INTRODUCTION TO QUEUEING THEORY
On the other hand, the expectation E(X) is an evaluation of P on the set Ω=
X−1(S) only. Thus we can say that the expectation employs P only on the
trivial σ–algebra {∅, Ω}, while X itself employs P on the σ–algebra X−1(B)
generated by X.
Now we take any sub–σ–algebra C ⊂A. According to the Radon–Nikodym
theorem there is a random variable X0 : Ω→S with X−1(B) = C and

C

X0dP =

C

XdP
(15.1)
for all C ∈C. This we call the conditional expectation of X under C and
write
E(X|C) := X0
A conditional expectation is P–almost certainly uniquely determined by (15.1).
Typical special cases and examples are
Example 15.1 For C = {∅, Ω}, the conditional expectation equals the expec-
tation, i.e. E(X|C) = E(X). For any σ–algebra C with X−1(B) ⊂C we obtain
E(X|C) = X.
Example 15.2 Let I denote any index set and (Yi
Y : i ∈I) a family of random
variables. For the σ–algebra C = σ(0
i∈I Y −1
i
Y
(B)) generated by (Yi
Y : i ∈I),
we write
E(X|Yi
Y : i ∈I) := E(X|C)
By deﬁnition we obtain for a σ–algebra C ⊂A, random variables X and Y ,
and real numbers α and β
E(αX + βY |C) = αE(X|C) + βE(Y |C)
For σ–algebras C1 ⊂C2 ⊂A we obtain
E(E(X|C2)|C1) = E(E(X|C1)|C2) = E(X|C1)
(15.2)
Let C1 and C2 denote sub–σ–algebras of A, C := σ(C1 ∪C2), and X an inte-
grable random variable. If σ(X−1(B) ∪C1) and C2 are independent, then
E(X|C) = E(X|C1)
If X and Y are integrable random variables and X−1(B) ⊂C, then
E(XY |C) = X · E(Y |C)
(15.3)

Appendix
255
Conditional probabilities are special cases of conditional expectations. Deﬁne
the indicator function of a measurable set A ∈A by
1A(x) :=

1,
x ∈A
0,
x /∈/ A
Such a function is a random variable, since
1−1
A (B) = {∅, A, Ac, Ω} ⊂A
with Ac := Ω\ A denoting the complement of the set A. Let C denote a
sub–σ–algebra of A. The conditional expectation of 1A is called conditional
probability of A. We write
P(A|C) := E(1A|C)
Immediate properties of conditional probabilities are
0 ≤P(A|C) ≤1,
P(∅|C) = 0,
P(Ω|C) = 1
A1 ⊂A2 =⇒
=
P(A1|C) ≤P(A2|C)
all of which hold P–almost certainly. For a sequence (An : n ∈N) of disjoint
measurable sets, i.e. An ∈A for all n ∈N and Ai ∩Aj = ∅for i ̸≠
j, we
obtain
P
 ∞
/
n=1
An
,,,
,,
,,
,, C

=
∞

n=1
P(An|C)
P–almost certainly. Let X : (Ω, A) →(R, B) denote a non–negative or inte-
grable random variable and Y : (Ω, A) →(Ω′, A′) a random variable. Then
there is a measurable function g : (Ω′, A′) →(R, B) with
E(X|Y ) = g ◦Y
This is P Y –almost certainly determined by

A

′ g dP Y =

Y

−1(A′)
X dP
for all A′ ∈A′. Then we can deﬁne the conditional probability of X given
Y = y as g(y). We write
E(X|Y = y) := g(y)
for all y ∈Ω′.

256
AN INTRODUCTION TO QUEUEING THEORY
2.
Extension Theorems
Throughout this book, our basic stochastic tools are either sequences of ran-
dom variables (such as Markov chains or Markov renewal chains) or even un-
countable families of random variables (such as Markov processes, renewal
processes, or semi–regenerative processes). It is essential for our models that
these random variables are dependent, and in fact we deﬁne them in terms of
conditional probabilities, i.e. via their dependence structure.
It is then an immediate question whether a probability measure P exists that
satisﬁes all the postulates in the deﬁnition of a stochastic sequence or process.
This question is vital as it concerns the very existence of the tools we are using.
2.1
Stochastic chains
Let (S, B) denote a measurable space, µ a probability measure on (S, B), and
Pn
P , n ∈N, stochastic kernels on (S, B). The latter means that for every
n ∈N, Pn
P : S × B →[0, 1] is a function that satisﬁes
(K1) For every x ∈S, Pn
P (x, .) is a probability measure on (S, B).
(K2) For every A ∈B, the function Pn
P (., A) is B–measurable.
Deﬁne S∞as the set of all sequences x = (xn : n ∈N0) with xn ∈S for all
n ∈N0. A subset of S∞having the form
Cn
C
1,...,nk(A) = {x ∈S∞: (xn1, . . . , xnk) ∈A}
with k ∈N, n1 < . . . < nk ∈N0, and A ∈Bk, is called cylinder (with
coordinates n1, . . . , nk and base A). The set C of all cylinders in S∞forms an
algebra of sets. Deﬁne B∞:= σ(C) as the minimal σ–algebra containing C.
Now we can state the extension theorem for sequences of random variables,
which is proven in Gikhman and Skorokhod [37], section II.4.
Theorem 15.3 There is a probability measure P on (S∞, B∞) satisfying
P(C0
C ,...,k(A0 × . . . × Ak)) =

A

0
dµ(x0)

A

1
P1
P (x0, dx1) . . .
. . .

A

k−1
Pk
P −1(xk−2, dxk−1) Pk
P (xk−1, Ak)
(15.4)
for all k ∈N0, A0, . . . , Ak ∈B. The measure P is uniquely determined by the
system (15.4) of equations.
The ﬁrst part of the theorem above justiﬁes our deﬁnitions of Markov chains
and Markov renewal chains. The second part states in particular that a Markov
chain is uniquely determined by its initial distribution and its transition matrix.

Appendix
257
Based on this result, we may deﬁne a stochastic chain with state space S as a
sequence (Xn
X
: n ∈N0) of S–valued random variables which are distributed
according to a probability measure P on (S∞, B∞).
2.2
Stochastic processes
Let S denote a Polish (i.e. a complete separable metric) space, and B the Borel
σ–algebra on S. Deﬁne Ωas the set of all functions f : R+
0 →S. In order to
construct an appropriate σ–algebra on Ω, we again start from the cylinder sets
Ct
C 1,...,tk(A) = {f ∈Ω: (f(t1), . . . , f(tk)) ∈A}
for k ∈N, t1 < . . . < tk ∈R+
0 , and A ∈Bk. Denote the set of all cylinders in
Ωby C. Again, C forms an algebra of sets and we can deﬁne A := σ(C) as the
minimal σ–algebra containing C.
Let M = {µt1,...,tk : k ∈N, t1, . . . , tk ∈R+
0 } denote a family of probability
distributions with
(C1) For all k ∈N, t1, . . . , tk ∈R+
0 , and A ∈Bk
µt1,...,tk,tk+1(A × S) = µt1,...,tk(A)
(C2) For all k ∈N and permutations π : {1, . . . , k} →{1, . . . , k}
µπ(t1,...,tk)(π(A)) = µt1,...,tk(A)
Then the family M is called compatible.
Remark 15.4 Condition (C1) ensures that the distributions are consistent with
each other, while condition (C2) is merely notational.
The following extension theorem by Kolmogorov is proven in Gikhman and
Skorokhod [39], section 3.2.
Theorem 15.5 Let {µt1,...,tk : k ∈N, t1, . . . , tk ∈R+
0 } denote a compati-
ble family of probability measures. Then there is a probability measure P on
(Ω, A) with
P({f ∈Ω: (f(t1), . . . , f(tk)) ∈A}) = µt1,...,tk(A)
(15.5)
for all k ∈N, t1, . . . , tk ∈R+
0 , and A ∈Bk. The measure P is uniquely
determined by the system (15.5) of equations.
Based on this, we deﬁne a stochastic process with Polish state space S as a
family X = (Xt
X
: t ∈R+
0 ) of S–valued random variables which are dis-
tributed according to a probability measure P on (Ω, A). An element ω ∈Ω

258
AN INTRODUCTION TO QUEUEING THEORY
is an arbitrary function ω : R+
0 →S. It is also called a path of X. If we
want to state that the support of P consists of a special class of functions (say
right–continuous ones), then we say that X is a stochastic process with right–
continuous paths. The above family M of probability measures is called the
set of ﬁnite–dimensional marginal distributions for X.
Due to theorem 15.5 a Markov process is uniquely deﬁned by its initial dis-
tribution and the family of transition probabilities, since they determine all
ﬁnite–dimensional marginal distributions. Further our constructions of Markov
processes, renewal processes, and semi–Markov processes yield compatible
sets of ﬁnite–dimensional marginal distributions, hence by theorem 15.5 a
probability measure P for the respective process.
3.
Transforms
In several parts of the present book, it is essential to argue via transforms of
distributions. The necessary background for these shall be presented shortly in
this section. For discrete distributions on N0 we will introduce z–transforms,
while for distributions on R+
0 the Laplace–Stieltjes transform will be useful.
3.1
z–transforms
Let X denote a N0–valued random variable with distribution A = (an : n ∈
N0), i.e. P(X = n) = an for all n ∈N0. Then the power series
A∗(z) :=
∞

n=0
anzn
(15.6)
converges absolutely for z ∈C with |z| ≤1 and is analytic in this region. We
note that A∗(z) = E(zX). If A(z) is a given power series for a distribution
(an : n ∈N0), then the probabilities an can be derived as
an = 1
n!
dn
dzn A(z)
,,,
,,
,,
z=0
for all n ∈N0. Thus the mapping between discrete distributions on N0 and
the power series in (15.6) is bijective, and we may call A∗(z) the (uniquely
determined) z–transform of X (also: of the distribution A).
Example 15.6 For a Dirac distribution on k ∈N0 with
an =

1,
n = k
0,
n ̸≠
k

Appendix
259
we obtain A∗(z) = zk.
Example 15.7 Let A denote the geometric distribution with some parameter
p ∈]0, 1[, i.e.
an = (1 −p)pn
for all n ∈N0. The z–transform of A is given by
A∗(z) = (1 −p)
∞

n=0
pnzn = 1 −p
1 −pz
for all |z| ≤1.
A very useful feature is the behaviour of the z–transform with respect to the
convolution of two distributions. Let A = (an : n ∈N0) and B = (bn : n ∈
N0) denote two distributions on N0. The convolution C = A ∗B of A and B
is deﬁned as the distribution C = (cn : n ∈N0) with
cn =
n

k=0
akbn−k
for all n ∈N0. For the z–transform of C we obtain
C∗(z) =
∞

n=0
cnzn =
∞

n=0
n

k=0
akbn−kzn =
∞

n=0
akzk
∞

n=k
bn−kzn−k
= A∗(z) · B∗(z)
for all |z| ≤1.
This means that the z–transform of a convolution A ∗B equals the product
A∗(z) · B∗(z) of the z–transform of A and B. In terms of random variables
we have the following representation: Let X and Y denote two independent
N0–valued random variables. Then the z–transform of the sum X + Y equals
the product of the z–transforms of X and Y , i.e.
E

zX+Y 
= E

zX
· E

zY 
for all |z| ≤1.
3.2
Laplace–Stieltjes transforms
Let X denote an R+
0 –valued random variable with distribution function F. The
Laplace–Stieltjes transform (LST) of X (or: of F) is deﬁned by
F ∗(s) :=
 ∞
0

e−stdF(t) = E

e−sX

260
AN INTRODUCTION TO QUEUEING THEORY
for all s ∈C with Re(s) ≥0. The LST uniquely determines its underlying
distribution.
Example 15.8 Let X be exponentially distributed with parameter λ, i.e. X has
the distribution function F(t) = 1−e−λt with Lebesgue density f(t) = λe−λt.
Then
F ∗(s) =
 ∞
0

e−stλe−λt dt =
λ
s + λ
for Re(s) ≥0.
Example 15.9 For the Dirac distribution δx on x ∈R+
0 we obtain
δ∗
x(s) =
 ∞
0

e−stdF(t)
with
F(t) =

0,
t < x
1,
t ≥x
and hence
δ∗
x(s) = e−sx
for Re(s) ≥0.
Like the z–transform, the LST is very useful for dealing with convolutions.
Let X and Y denote two independent R+
0 –valued random variables. Then the
LST of the sum X + Y equals the product of the LSTs of X and Y , i.e.
E
 
e−s(X+Y )!
= E

e−sX
· E

e−sY 
for all s ∈C with Re(s) ≥0.
Notes
For more on z–transforms see e.g. Juri [43], or the collection of results in
Kleinrock [50], appendix I. For Laplace–Stieltjes transforms see chapter XIII
in Feller [35] or again Kleinrock [50], appendix I.
4.
An important theorem to ﬁnd bounds for the eigenvalues of a matrix has been
developed by Gershgorin in 1938. For ease of reference it shall be presented
in this section. Let A = (aij)i,j≤m denote a square matrix of dimension m
with entries aij ∈C. The following theorem is called Gershgorin’s circle
theorem.
Gershgorin’s Circle Theorem

Appendix
261
Theorem 15.10 All eigenvalues of the matrix A lie in the union C := 0m
i=1 Ci
C
of the circles
Ci
C =
⎧
⎨
⎧
⎩
⎨
z ∈C : |z −aii| ≤

k̸=
̸
i
|aik|
⎫
⎬
⎫
⎭
⎬
Proof: Let x(ν) denote an eigenvector to the eigenvalue λν of A, i.e. Ax(ν) =
λνx(ν). This implies
m

k=1
aikx(ν)
k
= λνx(ν)
i
(15.7)
for all i ≤m. Since an eigenvector is determined only up to a scalar multi-
plicative, we can assume without loss of generality that there is a component
x(ν)
i0 = max
1≤j≤m
,,,
,,x(ν)
j
,,,
,, = 1
of the vector x(ν). Now (15.7) yields for i = i0 the relation

k̸=
̸
i0
ai0,kx(ν)
k
= (λν −ai0,i0) x(ν)
i0 = λν −ai0,i0
which implies by the triangle inequality
|λν −ai0,i0| ≤

k̸=
̸
i0
|ai0,k| ·
,,,
,,x(ν)
k
,,,
,, ≤

k̸=
̸
i0
|ai0,k|
Since every eigenvalue satisﬁes at least one such inequality, the proof is com-
plete.
□
Corollary 15.11 If A is diagonally dominated, i.e. if
|aii| >

k̸=
̸
i
|aik|
holds for all 1 ≤i ≤m, then the matrix A is invertible.
Proof: The strict inequality of the assumption implies that aii ̸= 0
̸
for all
i ≤m. Applying theorem 15.10 yields a restriction
|λ| ≥|aii| −|aii −λ| ≥|aii| −

k̸=
̸
i
|aik| > 0
for every eigenvalue λ of A. Therefore the matrix A has no eigenvalue zero
and thus is invertible.
□


References
[1] A. S. Alfa. Discrete Time Queues and Matrix–analytic Methods. Top, 10(2):147–210,
2002.
[2] A. S. Alfa. Combined Elapsed Time and Matrix–Analytic Method for the Discrete Time
GI/G/1 and GIX/G/1 Systems. Queueing Systems, 45(1):5–25, 2003.
[3] A. S. Alfa and W. Li. Matrix-geometric analysis of the discrete time GI/G/1 system.
Stoch. Models, 17(4):541–554, 2001.
[4] A. S. Alfa and M. F. Neuts. Modelling vehicular trafﬁc using the discrete time Markovian
arrival process. Transp. Sci., 29(2):109–117, 1995.
[5] S. Asmussen. Applied Probability and Queues. New York etc.: Springer, 2003.
[6] S. Asmussen and G. Koole.
Marked point processes as limits of Markovian arrival
streams. J. Appl. Probab., 30(2):365–372, 1993.
[7] S. Asmussen, O. Nerman, and M. Olsson. Fitting phase-type distributions via the EM
algorithm. Scand. J. Stat., 23(4):419–441, 1996.
[8] F. Baskett, K. M. Chandy, R. R. Muntz, and F. G. Palacios. Open, closed, and mixed
networks of queues with different classes of customers. Journal of the ACM, 22:248–
260, 1975.
[9] D. Baum. A BMAP|G|1-analysis based on convolution calculus. J. Math. Sci., New
York, 92(4):3990–4002, 1998.
[10] D. Baum. On markovian spatial arrival processes for the performance analysis of mo-
bile communication networks. Technical Report 98–07, University of Trier, Subdept. of
Computer Science, 1998.
[11] D. Baum and V. Kalashnikov. Spatial generalization of BMAPs with ﬁnite state space.
J. Math. Sci., New York, 105(6):2504–2514, 2001.
[12] D. Baum and V. V. Kalashnikov. Stochastic models for communication networks with
moving customers. Information Processes, 1:1–23, 2001.

264
AN INTRODUCTION TO QUEUEING THEORY
[13] D. Baum and V. Kalashnikov. Spatial No–Waiting Stations with Moving Customers.
Queueing Systems, 46:231–247, 2004.
[14] D. Baum and J. Sztrik. Customer motion in queueing models: The use of tangent vector
ﬁelds. International Journal on Pure and Applied Mathematics, 2002. To appear.
[15] G. Bolch, S. Greiner, H. de Meer, and K. S. Trivedi. Queueing networks and Markov
chains. John Wiley & Sons, New York, Chichester, Weinheim, Brisbane, Singapore,
Toronto, 1998.
[16] L. Breiman. Probability. Philadelphia, PA: SIAM, 1968.
[17] L. Breuer. The Periodic BMAP/PH/c Queue. Queueing Systems, 38(1):67–76, 2001.
[18] L. Breuer. An EM Algorithm for Batch Markovian Arrival Processes and its Comparison
to a Simpler Estimation Procedure. Annals of Operations Research, 112:123–138, 2002.
[19] L. Breuer. On Markov–Additive Jump Processes. Queueing Systems, 40(1):75–91, 2002.
[20] L. Breuer. On the MAP/G/1 Queue with Lebesgue–dominated Service Time Distribution
and LCFS Preemptive Repeat Service Discipline. Stochastic Models, 18(4):589–595,
2002.
[21] L. Breuer. From Markov Jump Processes to Spatial Queues. Kluwer, Dordrecht (Nether-
lands), 2003.
[22] L. Breuer. Two Examples for Computationally Tractable Periodic Queues. International
Journal of Simulation, 3(3–4):15–24, 2003.
[23] L. Breuer, A. Dudin, V. Klimenok, and G. Tsarenkov.
A Two–Phase BMAP/G/1/N
→PH/1/M-1 System with Blocking. Automation and Remote Control, 65(1):104–115,
2004.
[24] J. P. Buzen. Computational algorithms for closed queueing networks with exponential
servers. Commun. ACM, 16:527–531, 1973.
[25] E. C¸ inlar.
C
Introduction to stochastic processes. Englewood Cliffs, N. J.: Prentice-Hall,
1975.
[26] K. M. Chandy, J. H. Howard, and D. F. Towsley. Product form and local balance in
queueing networks. Journal of the ACM, 24:250–263, 1977.
[27] K. Chung.
Markov chains with stationary transition probabilities.
Springer-Verlag,
Berlin etc., 1960.
[28] J. Cohen. The Single Server Queue. North–Holland, Amsterdam etc., 1969.
[29] H. Daduna. Queueing networks with discrete time scale. Springer, Berlin, Heidelberg,
New York, 2001.
[30] N. M. Van Dijk. Queueing Networks and Product Forms. John Wiley & Sons, Chichester,
New York, Brisbane, Toronto, Singapore, 1993.
[31] J. Doob. Stochastic processes. New York: Wiley, 1953.

265
[32] T. Engset.
Emploi du calcul des probabilites pour la d
´
etermination du nombre de
´
selecteurs dans les Bureaux T
´
el´ ephoniques Centraux.
´
Rev. gen. elect.
´
, 9:138–140, 1921.
[33] A. Erlang. Solution of Some Probability Problems of Signiﬁcance for Automatic Tele-
phone Exchanges. Electroteknikeren, 13:5–13, 1917.
[34] W. Feller. An introduction to probability theory and its applications. Vol. I. New York
etc.: John Wiley and Sons, 1950.
[35] W. Feller. An introduction to probability theory and its applications. Vol. II. 2nd ed. New
York etc.: John Wiley and Sons, 1971.
[36] E. Gelenbe and G. Pujolle. Introduction to queueing networks. John Wiley & Sons,
hichester, New York, Brisbane, Toronto, Singapore, 1987.
[37] I. Gihman and A. Skorohod. The theory of stochastic processes I. Berlin etc.: Springer,
1974.
[38] I. Gihman and A. Skorohod. The theory of stochastic processes II. Berlin etc.: Springer-
Verlag, 1975.
[39] I. Gikhman and A. Skorokhod. Introduction to the theory of random processes. Saunders,
1969.
[40] W. Gordon and G. Newell. Closed queuing systems with exponential servers. Oper. Res.,
15:254–265, 1967.
[41] P. G. Harrison and N. M. Patel. Performance Modelling of Communication Networks and
Computer Architectures. Addison-Wesley, Reading, Massachusetts, 1993.
[42] J. R. Jackson. Jobshop-like queueing systems. Management Science, 10:131–142, 1963.
[43] E. Juri. Theory and Application of the z–Transform Method. Wiley, New York, 1964.
[44] V. Kalashnikov. Topics on Regenerative Processes. CRC Press, 1994.
[45] K. Kant. Introduction to computer system performance evaluation. McGraw-Hill, Inc.,
New York, London, 1992.
[46] S. Karlin and H. M. Taylor. A ﬁrst course in stochastic processes. 2nd ed. New York etc.:
Academic Press, 1975.
[47] S. Karlin and H. M. Taylor. A second course in stochastic processes. New York etc.:
Academic Press, 1981.
[48] F. Kelly. Reversibility and Stochastic Networks. Wiley, 1979.
[49] D. Kendall. Stochastic Processes Occuring in the Theory of Queues and Their Analy-
sis by the Method of the Embedded Markov Chain. Annals of Mathematical Statistics,
24:338–354, 1953.
[50] L. Kleinrock. Queueing systems. Vol. I: Theory. New York etc.: John Wiley & Sons,
1975.
[51] Y. Lam. A Note on the Optimal Replacement Problem. Adv. Appl. Prob., 20:479–482,
1988.
References

266
AN INTRODUCTION TO QUEUEING THEORY
[52] G. Latouche and V. Ramaswami. Introduction to matrix analytic methods in stochastic
modeling. Philadelphia, PA: SIAM, 1999.
[53] J. Little. A Proof for the Queueing Formula L = λW. Operations Research, 9, 1961.
[54] D. M. Lucantoni. New results on the single server queue with a batch Markovian arrival
process. Commun. Stat., Stochastic Models, 7(1):1–46, 1991.
[55] D. M. Lucantoni. The BMAP/G/1 Queue: A Tutorial. In L. Donatiello and R. Nelson,
editor, Models and Techniques for Performance Evaluation of Computer and Communi-
cation Systems, pages 330–358. Springer, 1993.
[56] D. M. Lucantoni, K. S. Meier-Hellstern, and M. F. Neuts. A single-server queue with
server vacations and a class of non-renewal arrival processes.
Adv. Appl. Probab.,
22(3):676–705, 1990.
[57] D. M. Lucantoni and M. F. Neuts. Simpler proofs of some properties of the fundamental
period of the MAP/G/1 queue. J. Appl. Probab., 31(1):235–243, 1994.
[58] F. Machihara. A New Approach to the Fundamental Period of a Queue with Phase–type
Markov Renewal Arrivals. Stochastic Models, 6(3):551–560, 1990.
[59] S. Meyn and R. Tweedie. Markov chains and stochastic stability. Berlin: Springer-Verlag,
1993.
[60] I. Mitrani. Modelling of computer and communication systems. Cambridge University
Press, Cambridge etc., 1987.
[61] R. Nelson.
Probability, stochastic processes, and queueing theory.
New York, NY:
Springer-Verlag, 1995.
[62] M. F. Neuts. Probability distributions of phase type. In Liber Amicorum Prof. Emeritus
H. Florin, pages 173–206. Department of Mathematics, University of Louvain, Belgium,
1975.
[63] M. F. Neuts. Markov chains with applications in queueing theory, which have a matrix-
geometric invariant probability vector. Adv. Appl. Probab., 10:185–212, 1978.
[64] M. F. Neuts. A versatile Markovian point process. J. Appl. Probab., 16:764–774, 1979.
[65] M. F. Neuts. Matrix–Geometric Solutions in Stochastic Models. Johns Hopkins Univer-
sity Press, Baltimore, 1981.
[66] M. F. Neuts. Structured stochastic matrices of M/G/1 type and their applications. New
York etc.: Marcel Dekker, 1989.
[67] M. F. Neuts and J.-M. Li. An algorithm for the P(n, t) matrices of a continuous BMAP.
In Matrix-analytic methods in stochastic models (Flint, MI), volume 183 of Lecture Notes
in Pure and Appl. Math., pages 7–19. Dekker, New York, 1997.
[68] A. Pacheco and N. Prabhu. Markov-additive processes of arrivals. In J. H. Dshalalow,
editor, Advances in queueing, pages 167–194. CRC Press, Boca Raton, FL, 1995.
[69] R. Pyke. Markov Renewal Processes: Deﬁnitions and Preliminary Properties. Annals of
Mathematical Statistics, 32:1231–1242, 1961.

267
[70] R. Pyke. Markov Renewal Processes with Finitely Many States. Annals of Mathematical
Statistics, 32:1243–1259, 1961.
[71] V. Ramaswami. The N/G/1 queue and its detailed analysis. Adv. in Appl. Probab.,
12(1):222–261, 1980.
[72] V. Ramaswami. A stable recursion for the steady state vector in Markov chains of M/G/1
type. Commun. Stat., Stochastic Models, 4(1):183–188, 1988.
[73] M. Reiser and S. Lavenberg. Mean-value analysis of closed multichain queuing net-
works. J. Assoc. Comput. Mach., 27:313–332, 1980.
[74] S. Ross. Applied Probability Models with Optimization Applications. Holden–Day, San
Francisco, 1970.
[75] S. Ross. Stochastic Processes. John Wiley & Sons, New York etc., 1983.
[76] T. Ryden. An EM algorithm for estimation in Markov-modulated Poisson processes.
Comput. Stat. Data Anal., 21(4):431–447, 1996.
[77] R. Serfozo. Introduction to stochastic networks. Springer-Verlag, New York, 1999.
[78] D. Shi, J. Guo, and L. Liu. SPH–Distributions and the Rectangle–Iterative Algorithm.
In S. R. Chakravarthy and A. S. Alfa, editors, Matrix–Analytic Methods in Stochastic
Models, pages 207–224, New York etc., 1997. Marcel Dekker.
[79] D. Shi and L. Liu. Markovian Models for Non–negative Random Variables. In A. S. Alfa
and S. R. Chakravarthy, editors, Advances in Matrix Analytic Methods for Stochastic
Models, pages 403–427. Notable Publications, 1998.
[80] W. Smith.
Regenerative Stochastic Processes.
Proceedings Royal Society, Series A,
232:6–31, 1955.
[81] W. Smith. Renewal Theory and Its Ramiﬁcations. Journal of the Royal Statistical Society,
Series B, 20:243–302, 1958.
[82] T. Takine. A new recursion for the queue length distribution in the stationary BMAP/G/1
queue. Stochastic Models, 16(2):335–341, 2000.
[83] R. Tweedie. Operator-geometric stationary distributions for Markov chains, with appli-
cation to queueing models. Adv. Appl. Probab., 14:368–391, 1982.
References

Index
Absorbing, 14
Accessible, 13
Age, 122
Alternating renewal process, 144
Arrival process, 4, 153
Arrival rate, 52
Asymptotic distribution, 5
Backward process, 64
Balance equations, 45, 50
Batch arrivals, 186
Batch Markovian Arrival Process, 186
Bernoulli process, 10, 114
Birth–and–death process, 59
Blackwell’s Theorem, 119
Block Toeplitz structure, 196
Blockwise skip–free, 232
BMAP, 186
Chapman–Kolmogorov equations, 12, 40
Characterization, 227
Characterizing matrices, 186
Characterizing matrix sequence, 186
Characterizing matrix, 138
Closed network, 62
Closed, 14
Closure properties, 176
Communicate, 13
Communication class, 14
Compatible, 255
Conditional expectation, 252
Conditional memoryless property, 184
Conditional probability, 253
Convolution, 112
Convolutional exponential of a matrix sequence,
189
Convolutional power, 113
Convolutional powers of a matrix sequence, 188
Convolutions of matrix sequences, 188
Counting measure, 240
Counting process, 153
Cox distribution, 174
Cyclic closed network, 106
Cylinder, 254
Delay system, 54
Delay, 111
Delayed regenerative process, 143
Delayed renewal reward process, 132
Departure rate, 52
Detailed balance equation, 64
Directly Riemann integrable, 120
Discrete batch Markovian arrival process, 230
Discrete BMAP, 230
Discrete PH renewal process, 231
Discrete phase–type distribution, 227
Discrete random walk, 10
Doubly stochastic Poisson process, 187
Elapsed time representation, 229
Elementary Renewal Theorem, 117
Embedded Markov chain, 37
Embedded Markov renewal chain, 136
Engset distribution, 59
Environment process, 187
Ergodicity condition, 202
Erlang distribution, 172
Erlang’s delay formula, 56
Erlang’s loss formula, 57
Excess life, 122
Exit vector, 169
Expectation, 251
Failure rate, 106
Finite–dimensional marginal distributions, 44, 256
First return time, 139
First visit, 15
Forward process, 64
Foster’s criterion, 28
Generalized Erlang distribution, 172
Generator matrix, 40
Geometric process, 130
Gershgorin’s circle theorem, 258

270
AN INTRODUCTION TO QUEUEING THEORY
GI/M/1 queue, 145
GI/M/1 type matrix, 197
GI/PH/1 queue, 195
Global balance, 64
Holding time, 37
Homogeneous, 9, 136
Hyper–exponential distribution, 173
Hypo–exponential distribution, 172
Indicator function, 253
Inﬁnitesimal transition rate, 40
Initial distribution, 11, 44, 169
Insensitivity property, 84
Intensity, 38, 145
Inter–arrival time, 4
Interrupted Bernoulli process, 236
Interrupted PH renewal process, 187
Invariant, 151
IPP, 187
Irreducible, 14, 44, 137
Kendall notation, 4
Kernel, 254
Key Renewal Theorem, 120
Kolmogorov forward and backward equations, 41
Kronecker product, 179
Kronecker sum, 179
Laplace–Stieltjes transform, 174, 257
Lattice, 119
Level, 196, 212, 230, 232
Load, 31, 51, 150
Loss system, 168
Lower Hessenberg matrix, 155
LST, 174
M/G/1 queue, 153
M/G/1 type, 212
M/G/k queue, 133
M/M/c/c+K queue, 167
Machine repair problem, 59
MAP, 185
Markov chain, 9
Markov process, 37
Markov property, 9, 39
Markov routing, 77
Markov–modulated Poisson process, 187
Markov-additive jump process, 239
Markovian Arrival Process, 185
Markovian queues, 49
Matrix convolution, 138
Matrix exponential function, 169
Matrix–geometric distribution, 198
Mean arrival rate, 192
Mean recurrence time, 140
Memoryless property, 29, 38
Minimal subinvariant measure, 151
Mixed network, 62
MMPP, 187
Moment, 171
N–process, 193
Non–defective, 170
Null recurrent, 21
Open network, 62
Order, 169, 227
Ordinary renewal process, 111
Path, 256
Period, 119
PH distribution, 169
PH renewal process, 183
Phase equilibrium, 191
Phase process, 190, 196, 230
Phase, 169, 196, 212, 227, 230, 232
Phase–type distribution, 169
Poisson process, 37–38
Polish space, 239
Pollaczek–Khinchin mean value formula, 164
Positive recurrent, 21, 44
Potential matrix, 17
Probability ﬂux, 64
Product form (PF-) networks, 63
Pure jump process, 37
Queue, 4
Queueing network, 61
Queueing system, 4
Radon measure, 240
Random point ﬁeld, 240
Random variable, 251
Rate conservation law, 162
Rate matrix, 198
Rate of ﬂow, 64
Rate, 114
Recurrent, 17, 44, 137
Reducible, 14
Regeneration cycle, 133
Regeneration property, 133
Regeneration time, 133
Regenerative process, 133
Regular, 46
Reliability theory, 59, 107
Remaining time representation, 229
Renewal equation, 115
Renewal function, 114
Renewal interval, 111
Renewal process, 111
Renewal reward process, 128
Renewal time, 111
Residual life time, 122
Reversed process, 64
Reversible, 66
Reward, 128
Routing matrix, 61
Routing probabilities, 61
Semi–Markov process, 136
Semi–regenerative process, 141
Separable networks, 63
Service discipline, 4
Service times, 4

INDEX
271
Skip–free Markov chain, 232
Skip–free Markov process, 52
Skip–free to the left, 155, 212
Skip–free to the right, 147, 197
SMAP, 241
Spatial arrival process, 239
Spatial Markovian arrival process, 241
Stability condition, 32, 150, 156, 197
Stable, 243
State space, 9, 37
State transition graph, 41
State, 37
Station balance, 73
Stationary distribution, 18
Stationary increments, 125
Stationary measure, 18
Stationary renewal process, 125–126
Stationary, 18, 44
Stochastic chain, 9, 255
Stochastic ﬂow, 45
Stochastic matrix, 11
Stochastic process, 255
Stopping time, 12, 133
Strong Markov property, 12
Sub–stochastic distribution functions, 137
Subinvariant, 151
Superﬂuous, 174
Superposition, 47, 188
System capacity, 4
System process, 5
Taboo probabilities, 151, 198
Tandem queue, 106
Time until absorption, 169
Total number of visits, 16
Total reward, 128
Transient distributions, 138
Transient, 17, 44, 137
Transition matrix, 9
Transition probabilities, 39
Transition probability matrix, 40
Transition probability, 9
Transitions with arrivals, 184
Transitions without arrivals, 184
Upper Hessenberg matrix, 147
Versatile Markovian point process, 193
Version, 11, 44
Waiting time paradox, 127
Wald’s Lemma, 117
Z–transform, 256



Mathematical Summary for Digital Signal
Processing Applications with Matlab

E.S. Gopi
Mathematical Summary
for Digital Signal Processing
Applications with Matlab
123

E.S. Gopi
National Institute of Technology, Trichy
Dept. Electronics & Communication Engineering
67 Tanjore Main Road
Tiruchirappalli-620015
National Highway
India
esgopi@nitt.edu
ISBN 978-90-481-3746-6
e-ISBN 978-90-481-3747-3
DOI 10.1007/978-90-481-3747-3
Springer Dordrecht Heidelberg London New York
Library of Congress Control Number: 2009944069
c⃝Springer Science+Business Media B.V. 2010
No part of this work may be reproduced, stored in a retrieval system, or transmitted in any form or by
any means, electronic, mechanical, photocopying, microﬁlming, recording or otherwise, without written
permission from the Publisher, with the exception of any material supplied speciﬁcally for the purpose
of being entered and executed on a computer system, for exclusive use by the purchaser of the work.
Cover design: eStudio Calamar S.L.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Dedicated to my son G.V. Vasig and my wife
G. Viji

Preface
The book titled “Mathematical summary for Digital Signal Processing Applications
with Matlab” consists of Mathematics which is not usually dealt in the DSP core
subject, but used in the DSP applications. Matlab Illustrations for the selective topics
such as generation of Multivariate Gaussian distributed sample outcomes, Optimiza-
tion using Bacterial Foraging etc. are given exclusively as the separate chapter for
better understanding. The book is written in such a way that it is suitable for Non-
mathematical readers and is very much suitable for the beginners who are doing
research in Digital Signal Processing.
E.S. Gopi
vii

Acknowledgements
I am extremely happy to express my thanks to Prof. K.M.M. Prabhu, Indian
Institute of Technology Madras for his constant encouragement. I also thank the
Director Prof. M. Chidambaram, Prof. B. Venkataramani and Prof. S. Raghavan
National Institute of Technology Trichy for their support. I thank those who di-
rectly or indirectly involved in bringing up this Book. Special thanks to my parents
Mr. E. Sankarasubbu and Mrs. E.S. Meena.
ix

Contents
1
Matrices .........................................................................
1
1.1
Properties of Vectors.....................................................
2
1.2
Properties of Matrices ...................................................
3
1.3
LDU Decomposition of the Matrix .....................................
7
1.4
PLDU Decomposition of an Arbitrary Matrix ......................... 10
1.5
Vector Space and Its Properties ......................................... 12
1.6
Linear Independence, Span, Basis and the Dimension
of the Vector Space ...................................................... 12
1.6.1
Linear Independence........................................... 12
1.6.2
Span ............................................................ 13
1.6.3
Basis ............................................................ 13
1.6.4
Dimension ...................................................... 13
1.7
Four Fundamental Vector Spaces of the Matrix ........................ 13
1.7.1
Column Space .................................................. 14
1.7.2
Null Space ...................................................... 14
1.7.3
Row Space...................................................... 14
1.7.4
Left Null Space................................................. 14
1.8
Basis of the Four Fundamental Vector Spaces of the Matrix .......... 14
1.8.1
Column Space .................................................. 15
1.9
Observations on Results of the Example 1.12.......................... 20
1.9.1
Column Space .................................................. 21
1.9.2
Null Space ...................................................... 21
1.9.3
Left Column Space (Row Space) ............................. 21
1.9.4
Left Null Space................................................. 21
1.9.5
Observation..................................................... 22
1.10
Vector Representation with Different Basis ............................ 22
1.11
Linear Transformation of the Vector.................................... 24
1.11.1
Trick to Compute the Transformation Matrix ................ 25
1.12
Transformation Matrix with Different Basis ........................... 25
1.13
Orthogonality ............................................................ 26
1.13.1
Basic Deﬁnitions and Results ................................. 26
1.13.2
Orthogonal Complement ...................................... 27
1.14
System of Linear Equation .............................................. 27
xi

xii
Contents
1.15
Solutions for the System of Linear Equation [A] x D b............... 28
1.15.1
Trick to Obtain the Solution ................................... 29
1.16
Gram Schmidt Orthonormalization Procedure
for Obtaining Orthonormal Basis ....................................... 36
1.17
QR Factorization......................................................... 40
1.18
Eigen Values and Eigen Vectors ........................................ 42
1.19
Geometric Multiplicity (Versus) Algebraic Multiplicity............... 44
1.20
Diagonalization of the Matrix........................................... 47
1.21
Schur’s Lemma .......................................................... 49
1.22
Hermitian Matrices and Skew Hermitian Matrices .................... 50
1.23
Unitary Matrices ......................................................... 52
1.24
Normal Matrices ......................................................... 56
1.25
Applications of Diagonalization of the Non-deﬁcient Matrix ......... 58
1.26
Singular Value Decomposition.......................................... 60
1.27
Applications of Singular Value Decomposition ........................ 62
2
Probability ...................................................................... 67
2.1
Introduction .............................................................. 67
2.2
Axioms of Probability ................................................... 68
2.3
Class of Events or Field (F) ............................................. 68
2.4
Probability Space (S, F, P) .............................................. 68
2.5
Probability Measure ..................................................... 68
2.6
Conditional Probability.................................................. 69
2.7
Total Probability Theorem .............................................. 70
2.8
Bayes Theorem .......................................................... 70
2.9
Independence ............................................................ 70
2.10
Multiple Experiments (Combined Experiments) ....................... 71
2.11
Random Variable ........................................................ 74
2.12
Cumulative Distribution Function (cdf) of the Random
Variable ‘x’............................................................... 75
2.13
Continuous Random Variable ........................................... 76
2.14
Discrete Random Variable............................................... 76
2.15
Probability Mass Function .............................................. 76
2.16
Probability Density Function............................................ 76
2.17
Two Random Variables .................................................. 77
2.18
Conditional Distributions and Densities ................................ 79
2.19
Independent Random Variables ......................................... 79
2.20
Some Important Results on Conditional Density Function ............ 80
2.21
Transformation of Random Variables of the Type Y D g.X/ ......... 84
2.22
Transformation of Random Variables of the Type
Y1 D g1.X1; X2/; Y2 D g2.X1; X2) ................................. 85
2.23
Expectations ............................................................. 99
2.24
Indicator.................................................................. 99
2.25
Moment Generating Function ...........................................101
2.26
Characteristic Function ..................................................102

Contents
xiii
2.27
Multiple Random Variable (Random Vectors) .........................102
2.28
Gaussian Random Vector with Mean Vector X
and Covariance Matrix CX ..............................................107
2.29
Complex Random Variables.............................................118
2.30
Sequence of the Number and Its Convergence .........................119
2.31
Sequence of Functions and Its Convergence ...........................120
2.32
Sequence of Random Variable ..........................................120
2.33
Example for the Sequence of Random Variable........................122
2.34
Central Limit Theorem ..................................................122
3
Random Process................................................................123
3.1
Introduction ..............................................................123
3.2
Random Variable Xt1 ....................................................124
3.3
Strictly Stationary Random Process with Order 1 .....................124
3.4
Strictly Stationary Random Process with Order 2 .....................124
3.5
Wide Sense Stationary Random Process ...............................125
3.6
Complex Random Process ..............................................127
3.7
Properties of Real and Complex Random Process .....................127
3.8
Joint Strictly Stationary of Two Random Process......................127
3.9
Jointly Wide Sense Stationary of Two Random Process...............128
3.10
Correlation Matrix of the Random Column Vector Xt
Ys
for the Speciﬁc ‘t’ ‘s’ ....................................................128
3.11
Ergodic Process ..........................................................128
3.12
Independent Random Process ...........................................132
3.13
Uncorrelated Random Process ..........................................132
3.14
Random Process as the Input and Output of the System...............132
3.15
Power Spectral Density (PSD) ..........................................134
3.16
White Random Process (Noise) .........................................137
3.17
Gaussian Random Process ..............................................138
3.18
Cyclo Stationary Random Process ......................................139
3.19
Wide Sense Cyclo Stationary Random Process ........................139
3.20
Sampling and Reconstruction of Random Process.....................142
3.21
Band Pass Random Process .............................................144
3.22
Random Process as the Input to the Hilbert
Transformation as the System...........................................146
3.23
Two Jointly W.S.S Low Pass Random Process Obtained
Using W.S.S. Band Pass Random Process and Its Hilbert
Transformation...........................................................148
4
Linear Algebra .................................................................153
4.1
Vector Space .............................................................153
4.2
Linear Transformation...................................................154
4.3
Direct Sum ...............................................................160
4.4
Transformation Matrix ..................................................162
4.5
Similar Matrices .........................................................164

xiv
Contents
4.6
Structure Theorem .......................................................166
4.7
Properties of Eigen Space ...............................................171
4.8
Properties of Generalized Eigen Space .................................172
4.9
Nilpotent Transformation ...............................................173
4.10
Polynomial ...............................................................175
4.11
Inner Product Space .....................................................176
4.12
Orthogonal Basis ........................................................177
4.13
Riegtz Representation ...................................................179
5
Optimization....................................................................181
5.1
Constrained Optimization ...............................................181
5.2
Extension to Constrained Optimization Technique
to Higher Dimensional Space with Multiple Constraints ..............186
5.3
Positive Deﬁnite Test of the Modiﬁed Hessian Matrix
Using Eigen Value Computation ........................................189
5.4
Constrained Optimization with Complex Numbers ....................193
5.5
Dual Optimization Problem .............................................194
5.6
Kuhn-Tucker Conditions ................................................195
6
Matlab Illustrations............................................................197
6.1
Generation of Multivariate Gaussian Distributed
Sample Outcomes with the Required Mean Vector
‘MY ’ and Covariance Matrix ‘CY ’ .....................................197
6.2
Bacterial Foraging Optimization Technique............................202
6.3
Particle Swarm Optimization ...........................................208
6.4
Newton’s Iterative Method ..............................................210
6.5
Steepest Descent Algorithm.............................................214
Index .................................................................................217

Chapter 1
Matrices
One-dimensional array representation of scalars are called vector. If the elements are
arranged in row wise, it is called Row vector. In the same fashion, if the elements of
the vector are arranged in column wise, it is called column vector. Two-dimensional
array representations of scalars are called matrix. Size of the matrix is represented as
RC, where R is the number of rows and C is the number of columns of the matrix.
Scalar elements in the array can be either complex numbers .C/ or the real numbers.
.R/. The column vector is represented as X. The Row vector is represented as XT .
Example 1.1. Row Vector with the elements ﬁlled up with real numbers
Œ2:89
21:87
100
Column Vector with the elements ﬁlled up with Complex numbers.
2
664
1 C j
j
9 C 7j
0
3
775
Matrix of size 2  3 with the elements ﬁlled up with real numbers
"
2
3
6
4
1
2
#
Matrix of size 3  2 with the elements ﬁlled up with complex numbers
2
664
j
1 C j
2j
5j
0
j
3
775
E.S. Gopi, Mathematical Summary for Digital Signal Processing
Applications with Matlab, DOI 10.1007/978-90-481-3747-3 1,
c Springer Science+Business Media B.V. 2010
1

2
1 Matrices
1.1
Properties of Vectors
Scalar multiplication of the vector X is given as c X, where c  R.
Example 1.2.
2 
2
66664
1 C j
j
9 C 7j
0
3
77775
D
2
66664
2 C 2j
2j
18 C 14j
0
3
77775
Linear combinations of two vectors X1 and X2 are obtained as c1X1 C c2X2,
where c1, c2  R
Example 1.3.
2 
2
66664
1 C j
j
9 C 7j
0
3
77775
C 3 
2
66664
1  j
j
7j
0
3
77775
D
2
66664
5  j
j
18  7j
0
3
77775
Example 1.4. Graphical illustration of summation of two vectors [3 1] and [1 2] to
obtain the vector [4, 3] is given below (Fig. 1.1). (Recall the Parallelogram Rule of
addition).
Note that the ﬁrst and second elements of the vector are represented as the vari-
able X1 and X2, respectively. The variables X1 and X2 can be viewed as the random
variables.
X2
X1
(1,2)
(4,3)
(3,1)
Fig. 1.1 Graphical illustration of the summation of two vectors

1.2 Properties of Matrices
3
1.2
Properties of Matrices
(a) Matrix addition
Let the Matrix A be represented as
2
666664
a11
a12
a13
: : :
a1m
a21
a22
a33
: : :
a2m
a31
a32
a33
: : :
a3m





an1
an2
an3
: : :
anm
3
777775
Also let the Matrix B be represented as
2
666664
b11
b12
b13
: : :
b1m
b21
b22
b23
: : :
b2m
b31
b32
b33
: : :
b3m





bn1
bn2
bn3

bnm
3
777775
) A C B D
2
666664
a11 C b11
a12 C b12
a13 C b13
: : :
a1m C b1m
a21 C b21
a22 C b22
a23 C b23
: : :
a2m C b2m
a31 C b31
a32 C b32
a33 C b33
: : :
a3m C b3m





anm C bnm
anm C bnm
anm C bnm
: : :
bnm C bnm
3
777775
Note that (i,j)th element of the matrix ‘A’ is represented as aij. Matrix ‘A’ in
general is represented as A D Œaij
Let C D A C B ) Œcij D Œaij C Œbij
(b) Scalar multiplication
Let c 2 C or c 2 R
) cA D cŒaij D Œcaij
(c) Matrix multiplication
The product of the matrix ‘A’ with size np and the matrix ‘B’ with size pm
is the matrix C of size n  m. The elements of the matrix C are obtained as
follows.
C D
c11
c12
c13
: : :
c1m
c21
c22
c23
: : :
c2m
c31
c32
c33
: : :
c3m





cn1
cn2
cn3
: : :
cnm
Where cij D ai1b1j C ai2b2j C ai3b3j C : : : C aipbpj

4
1 Matrices
Example 1.5. Consider the matrix A and B of size 2  3 and 3  3, respectively
A D
"
a11
a12
a13
a21
a22
a23
#
B D
2
64
b11
b12
b13
b21
b22
b23
b31
b32
b33
3
75
Let the matrix C D AB
D
"
a11b11 C a12b21 C a13b31
a11b12 C a12b22 C a13b32
a11b13 C a12b23 C a13b33
a21b11 C a22b21 C a23b31
a21b12 C a22b22 C a23b32
a21b13 C a22b23 C a23b33
#
The matrix A can be viewed as A D Œa1 a2 a3, where
a1 D
a11
a21

a2 D
a12
a22

a3 D
a13
a23

Similarly the matrix B can be viewed as B D
2
4
b1
b2
b3
3
5, where
b1 D Œb11
b12
b13
b2 D Œb21
b22
b23
b3 D Œb31
b32
b33
So the matrix C D AB can also be represented as a1 b1 C a2 b2 C a3 b3
Also the matrix AB can be obtained as
h
b11.a1/ C b21.a2/ C b31.a3/ b12.a1/ C b22.a2/ C b32.a3/ b13.a1/ C b23.a2/ C b33.a3/
i
(d) Matrix multiplication is associative
Let A and B be the two matrices, then A .BC/ D .AB/ C.
(e) Matrix multiplication is non-commutative
Let A and B be the two matrices, then AB ¤ BA
(f) Block multiplication
Consider the matrix P as shown below
2
66666664
a11
a12
a13
b11
b12
a21
a22
a23
b21
b22
c11
c12
c13
d11
d12
c21
c22
c23
d21
d22
c31
c32
c33
d31
d32
3
77777775

1.2 Properties of Matrices
5
The above matrix ‘P’ can be viewed as the matrix with each element ﬁled up
with matrix as mentioned below. This way of representing the matrix is called
as Block matrix.
2
666664
a11
a12
a13
a21
a22
a23

b11
b12
b21
b22

2
4
c11
c12
c13
c21
c22
c33
c31
c32
c33
3
5
2
4
d11
d12
d21
d22
d31
d32
3
5
3
777775
In short the matrix P is represented as
ŒA
ŒB
ŒC
ŒD

Similarly consider the matrix Q represented as
ŒE
ŒF

Then the matrix PQ is represented as
ŒAE C BF
ŒCE C DF

This way of multiplying two Block matrices is called as Block multiplication.
(g) Transpose of the matrix
The transpose of the matrix A is the matrix B whose elements are related as
follows.
aij D bji
Note that transpose of the Block matrix
ŒA
ŒB
ŒC
ŒD

is given as
ŒAT
ŒCT
ŒBT
ŒDT

(h) Square matrix
The matrix with number of Rows is equal to the number of Columns.
(i) Identity matrix
The square matrix with all the elements is ﬁlled up with zeros except the diag-
onal elements which are ﬁlled up with all ones.

6
1 Matrices
(j) Lower triangular matrix
The square matrix with all the elements is ﬁlled up with zeros except the
elements in the diagonal and below the diagonal which are ﬁlled up at least
one non-zero elements.
In other words, Lower triangular matrix is the matrix with zeros in the up-
per triangular portion of the matrix with at least one non-zero element in the
remaining portion.
(k) Upper triangular matrix
The square matrix with all the elements is ﬁlled up with zeros except the ele-
ments in the diagonal and above the diagonal which are ﬁlled up at least one
non-zero elements.
In other words, Upper triangular matrix is the matrix with zeros in the Lower
triangular portion of the matrix with at least one non-zero element in the re-
maining portion.
(l) Diagonal matrix
The square matrix with all the elements is ﬁlled up with zeros except the di-
agonal elements which are ﬁlled up with at least one non-zero element in the
diagonal
(m) Permutation matrix
Permutation matrix is one when multiplied with the matrix interchanges the
elements of the matrix column wise or row wise. If the matrix is multiplied by
the permutation matrix, columnwise interchange of the elements of the matrix
occurs. Similarly if the permutation matrix is multiplied by some matrix, row
wise interchange of the elements of the matrix occurs.
Example 1.6. Arbitrary matrix A D
2
4
1
2
3
4
5
6
7
8
9
3
5
Arbitrary Permutation matrix P D
2
4
1
0
0
0
0
1
0
1
0
3
5
A  P D
2
4
1
3
2
4
6
5
7
9
8
3
5
Note that the elements of the second column and third column is interchanged
using the operation AP.
P  A D
2
4
1
2
3
7
8
9
4
5
6
3
5
Note that the elements of the second row and third row is interchanged using the
operation PA.
PP : : : P is always some permutation matrix. Also note that the iden-
tity matrix is the trivial permutation matrix, which when multiplied with any

1.3 LDU Decomposition of the Matrix
7
arbitrary matrix will end up with the same matrix. Also note that the inverse of
any arbitrary permutation matrix is always the permutation matrix.
(n) Inverse of the matrix
Inverse of the matrix is deﬁned only for the square matrix. The matrix A is
deﬁned as the inverse of the matrix B if AB D BA D I, where ‘I’ is the identity
matrix. If there exists the inverse matrix for the particular square matrix A, then
that matrix ‘A’ is known as the Invertible matrix. Otherwise it is called as the
non-invertible matrix.
1.3
LDU Decomposition of the Matrix
The matrix A as shown below can be represented as the product of three matrices
Lower triangular matrix (L) with all ones in the diagonal elements, Diagonal ma-
trix (D) and the upper triangular matrix (U) with all ones in the diagonal elements.
Example 1.7. LDU Decomposition of the matrix A D
2
4
1 2 3
2 3 4
3 4 6
3
5
)
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1 2
3
2 3
4
3 4
6
3
5 D
2
4
1
2
3
2
3
4
3
4
6
3
5
Note: Row operation on the Identity matrix in the LHS and the same operation
done on the RHS will not affect the equality.
R2- > R2-2R1
R3- > R3-3R1
)
2
4
1
0
0
2
1
0
3
0
1
3
5
2
4
1
2
3
2
3
4
3
4
6
3
5 D
2
4
1
2
3
0
1
2
0
2
3
3
5
)
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1
0
0
2
1
0
3
0
1
3
5
2
4
1 2
3
2 3
4
3 4
6
3
5 D
2
4
1
2
3
0
1
2
0
2
3
3
5
Again applying Row operation we get
R3- > R3-2R2
)
2
4
1 0 0
0 1 0
0 2 1
3
5
2
4
1
0
0
2
1
0
3
0
1
3
5
2
4
1 2 3
2 3 4
3 4 6
3
5 D
2
4
1
2
3
0
1
2
0
0
1
3
5

8
1 Matrices
In general, after applying the Row operation the matrix equation will in the form as
given below
ŒLnŒLn1 : : : ŒL3ŒL2ŒL1ŒA D ŒU
Where
L1; L2; L3; L4; : : : Ln are the lower triangular matrices with diagonal elements
ﬁlled up with ones.
‘A’ is the actual matrix
‘U’ is the upper triangular matrix
So the matrix A can be represented as the product of A D L11 : : : Ln21
Ln11 : : : Ln1U
In our example
L1 D
2
4
1
0
0
2
1
0
3
0
1
3
5 L2 D
2
4
1
0
0
0
1
0
0
2
1
3
5
) L1
1 D
2
4
1
0
0
2
1
0
3
0
1
3
5 L2
1 D
2
4
1
0
0
0
1
0
0
2
1
3
5
Note: In General the inverse of the matrix with the characteristics given below
can be obtained by direct observation.
Characteristics:
(a) Diagonal elements ﬁlled up ones
(b) One column below diagonal ﬁlled up atleast one non-zero elements
(c) All other elements ﬁlled up with zeros
Example 1.8. Consider the matrix Ln given below that satisﬁes all the characteris-
tics mentioned above
Ln D
1
0
0
0
0
a1
1
0
0
0
a2
0
1
0
0
a3
0
0
1
0
a4
0
0
0
1
The inverse of the matrix Ln is obtained by direct observation as
Ln1 D
1
0
0
0
0
a1
1
0
0
0
a2
0
1
0
0
a3
0
0
1
0
a4
0
0
0
1

1.3 LDU Decomposition of the Matrix
9
Consider
2
4
1
0
0
0
1
0
0
2
1
3
5
2
4
1
0
0
2
1
0
3
0
1
3
5
2
4
1
2
3
2
3
4
3
4
6
3
5 D
2
4
1
2
3
0
1
2
0
0
1
3
5
)
2
4
1
2
3
2
3
4
3
4
6
3
5 D
2
4
1
0
0
2
1
0
3
0
1
3
5
2
4
1
0
0
0
1
0
0
2
1
3
5
2
4
1
2
3
0
1
2
0
0
1
3
5
)
2
4
1
2
3
2
3
4
3
4
5
3
5 D
2
4
1
0
0
2
1
0
3
2
1
3
5
2
4
1
2
3
0
1
2
0
0
1
3
5
The matrix
2
4
1
2
3
0
1
2
0
0
1
3
5 can be represented as the product of the diagonal ma-
trix and the upper triangular matrix with all the elements in the diagonal are one as
given below.
2
4
1 2
3
0 1
2
0 0
1
3
5 D
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1
2
3
0
1
2
0
0
1
3
5
Note: In General the Upper triangular matrix with non-unity diagonal ele-
ments can be represented as the product of the diagonal matrix and the Upper
triangular matrix with ones in the diagonal as mentioned below
Example 1.9. Consider the Upper triangular matrix with non-unity diagonal ele-
ments
2
4
a b c
0 d e
0 0 f
3
5 which can be represented as the product of
2
4
a
b
c
0
d
e
0
0
f
3
5 D
2
4
a
0
0
0
d
0
0
0
f
3
5
2
6664
a
b
c
a
a
a
0
d
d
e
d
0
0
f
f
3
7775
D
2
4
a
b
c
0
d
e
0
0
f
3
5 D
2
4
a
0
0
0
d
0
0
0
f
3
5
2
4
1
b
a
c
a
0
1
e
d
0
0
1
3
5
Thus the invertible matrix A D
2
4
1
2
3
2
3
4
3
4
6
3
5 is represented as the product of Lower
triangular matrix with ones in the diagonal, diagonal matrix and the upper triangular

10
1 Matrices
matrix with ones in the diagonal as shown below
2
4
1
2
3
2
3
4
3
4
6
3
5 D
2
4
1
0
0
2
1
0
3
2
1
3
5
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1
2
3
0
1
2
0
0
1
3
5
1.4
PLDU Decomposition of an Arbitrary Matrix
In general an arbitrary matrix A can be represented as the product of the permutation
matrix (P), Lower triangular matrix with ones in the diagonal, diagonal matrix (D)
with non-zero diagonal elements and the Upper triangular matrix with all ones in
the diagonal. If the permutation matrix is the identity matrix, then the matrix A is
represented as the product of L, D, U (see Section 1.3)
Example 1.10. PLDU Decomposition of the matrix A D
2
4
1
2
3
2
4
4
3
4
6
3
5
)
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1 2
3
2 4
4
3 4
6
3
5 D
2
4
1
2
3
2
4
4
3
4
6
3
5
Note: Row operation on the Identity matrix in the LHS and the same operation
done on the RHS will not affect the equality.
R2- > R2-2R1
R3- > R3-3R1
)
2
4
1
0
0
2
1
0
3
0
1
3
5
2
4
1
2
3
2
4
4
3
4
6
3
5 D
2
4
1
2
3
0
0
2
0
2
3
3
5
Note that the matrix
2
4
1
2
3
0
0
2
0
2
3
3
5 cannot be further subjected to mere row op-
eration to obtain the upper triangular format. Hence the following technique using
permutation matrix is used.
2
4
1
0
0
0
0
1
0
1
0
3
5
2
4
1
2
3
2
4
4
3
4
6
3
5 D
2
4
1
2
3
3
4
6
2
4
4
3
5

1.4 PLDU Decomposition of an Arbitrary Matrix
11
)
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1
0
0
0
0
1
0
1
0
3
5
2
4
1
2
3
2
4
4
3
4
6
3
5 D
2
4
1
2
3
3
4
6
2
4
4
3
5
Now applying the Row operation on the identity matrix, we get
R2- > R2-3R1
R3- > R3-2R1
2
4
1
0
0
3
1
0
2
0
1
3
5
2
4
1
0
0
0
0
1
0
1
0
3
5
2
4
1
2
3
2
4
4
3
4
6
3
5 D
2
4
1
2
3
0
2
3
0
0
2
3
5
)
2
4
1
0
0
0
0
1
0
1
0
3
5
2
4
1
2
3
2
4
4
3
4
6
3
5 D
2
4
1
0
0
3
1
0
2
0
1
3
5
2
4
1
2
3
0
2
3
0
0
2
3
5
Note that the inverse of the matrix
2
4
1
0
0
3
1
0
2
0
1
3
5 is
2
4
1
0
0
3
1
0
2
0
1
3
5
)
2
4
1
2
3
2
4
4
3
4
6
3
5 D
2
4
1
0
0
0
0
1
0
1
0
3
5
2
4
1
0
0
3
1
0
2
0
1
3
5
2
4
1
2
3
0
2
3
0
0
2
3
5
Note that the inverse of the matrix
2
4
1
0
0
0
0
1
0
1
0
3
5 is
2
4
1
0
0
0
0
1
0
1
1
3
5
)
2
4
1
2
3
2
4
4
3
4
6
3
5 D
2
4
1
0
0
0
0
1
0
1
0
3
5
2
4
1
0
0
3
1
0
2
0
1
3
5
2
4
1
0
0
0
2
0
0
0
2
3
5
2
4
1
2
3
0
1
3=2
0
0
1
3
5
Thus the matrix
2
4
1
2
3
2
4
4
3
4
6
3
5 is represented as the product of the permuta-
tion matrix
2
4
1
0
0
0
0
1
0
1
0
3
5, Lower triangular matrix
2
4
1
0
0
3
1
0
2
0
1
3
5 diagonal matrix
2
4
1
0
0
0
2
0
0
0
2
3
5 and the Upper triangular matrix
2
4
1
2
3
0
1
3=2
0
0
1
3
5.

12
1 Matrices
1.5
Vector Space and Its Properties
1. Vector space V over a ﬁeld F is a set with two operation ‘C’ (addition) and ‘.’
(scalar multiplication) such that the following condition holds
x; y 2 V; then x C y 2 V
x 2 V; c 2 F; thenc:x 2 F
2. Properties of the vector space:
(a) Commutative addition
For all x; y 2 V; x C y D y C x
(b) Associatively
.x C y/ C z D x C .y C z/
(c) Additive identity
There exists an element z 2 V such that z C x D x for all x 2 V . z is called
zero vector
(d) Additive inverse
For each x 2 V , there exists y 2 V such that x C y D z
(e) There exists 1 2 F , such that 1:x D x for all x 2 V
(f) For all a; b 2 F and x 2 V a:.b:x/ D .ab/:x
(g) For all a 2 F and x; y 2 V a:.x C y/ D a:x C a:y
(h) For all a; b 2 F and x 2 V .a C b/:x D a:x C b:y
3. Subspace S of the vector space V is a subset of the V such that
x; y 2 S; then x C y 2 S
x 2 S; c 2 F; then c:x 2 S
Example 1.11. 1. Set of all real numbers R over the ﬁeld R is the vector space.
2. Set of all the vectors of the form Œx; y, where x 2 R; y 2 R, over the ﬁeld R is
the vector space which is represented in short as R2.
In general Rn is the vector space over the ﬁeld R which is the set of all the vectors
of the form Œx1 x2 x3 x4 : : : xn, where x1; x2; x3; : : : xn 2 R.
1.6
Linear Independence, Span, Basis and the Dimension
of the Vector Space
1.6.1
Linear Independence
Consider the vector space ‘V’ over the ﬁeld F. v1; v2; v3; v4 : : : ; vn 2 V are said
to be independent if the linear combinations of the above vectors [(i.e.) ˛1 v1 C

1.7 Four Fundamental Vector Spaces of the Matrix
13
˛2 v2 C ˛3 v3 C ˛4 v4 : : : C ˛n vn, where ˛1; ˛1; ˛1; : : : ˛1 2 R] is the zero vector
only when ˛1 D ˛2 D ˛3 D ˛4 D    ˛n D 0.
Suppose there exists at least one non-zero scalar ˛1; ˛2; ˛3; : : : ˛n 2 R such
that ˛1v1 C ˛2v2 C ˛3v3 C ˛4v4 : : : C ˛nvn D 0, then any one arbitrary vector
among the list v1; v2; v3; v4 : : : ; vn can be represented as the linear combinations of
the remaining vectors.
For instance if all the scalars .˛1; ˛2; ˛3; : : : ˛n 2 R/ are non-zero, then the
vector v1 is represented as the linear combinations of other vectors as shown below.
v1 D

˛2
˛1

v2 C

˛3
˛1

v3 C

˛4
˛1

v4 C   

˛n
˛1

vn
This implies that the vector v1 depends upon the other vectors v2; v3; v4 : : : ; vn.
Similarly any vector in the lists can be represented as the linear combinations of
other vectors. This implies that the vectors v1; v2; v3; v4 : : : ; vn are dependent
vectors.
1.6.2
Span
Consider the vector space ‘V’ over the ﬁeld F. Let v1; v2; v3; v4 : : : ; vn 2 V0
vectors. If all the vectors in the vector space V can be represented as the linear com-
binations of the above listed vectors, then the listed vectors are called the spanning
set of the vector space ‘V’.
1.6.3
Basis
Spanning set of the vector space ‘V’ which consists of the minimum number of
independent vectors are called the basis of the vector space ‘V’.
1.6.4
Dimension
Number of vectors in the Basis is called the dimension of the vector space ‘V’.
1.7
Four Fundamental Vector Spaces of the Matrix
Columns of the matrix can be viewed as the set of column vectors. Similarly Rows
of the matrix can be viewed as the set of Row vectors.

14
1 Matrices
1.7.1
Column Space
Column space of the matrix A of size m  n is the vector space over the ﬁeld ‘R’
which is the subspace of the vector space Rm. Any vector in the column space of the
matrix A can be obtained as the linear combinations of the columns of the matrix.
Columns of the matrix A forms the spanning set of the Column space.
1.7.2
Null Space
Null space of the matrix A of size m  n, represented as N (A), is the vector space
over the ﬁeld ‘R’ which is the subspace of the vector space Rn such that the A v D 0
for all v 2 N.A/.
1.7.3
Row Space
Row space of the matrix A of size m  n, represented as C.AT /, is the vector space
over the ﬁeld ‘R’ which is the subspace of the vector space Rn, which is basically
the Column space of the matrix AT . Therefore any vector in the Row space of the
matrix A can be obtained as linear combinations of the row vectors. Rows of the
matrix A forms the spanning set of the Row space.
1.7.4
Left Null Space
Left Null space of the matrix A of size m  n, represented as N.AT /, is the vector
space over the ﬁeld ‘R’ which is the subspace of the vector space Rm such that the
AT v D 0 for all v 2 N.AT /.
1.8
Basis of the Four Fundamental Vector Spaces of the Matrix
Example 1.12.
A D
2
4
1 2
3
4
5 5
7
12
6 7
10
16
3
5

1.8 Basis of the Four Fundamental Vector Spaces of the Matrix
15
1.8.1
Column Space
To compute the column space, we need to ﬁnd the maximum number of independent
columns of the matrix A which is the minimum spanning set (i.e.) Basis of the
column space.
Trick to ﬁnd out maximum number of Independent columns of the matrix A.
Applying the following Row Operation
R2- > R2-5R1
R3- > R3-6R1
we get
)
2
4
1 2
3
4
0 5
8
8
0 5
8
8
3
5
R3- > R3-R2
)D
2
4
1
2
3
4
0
5
8
8
0
0
0
0
3
5
)
2
4
1 2
3
4
0 5
8
8
0 0
0
0
3
5
The above is the Row Reduced Echelon Form (RREF). The Bold numbers in the
above matrix are called pivot elements and the corresponding columns are called
pivot columns.
2
4
1
2
3
4
0
5
8
8
0
0
0
0
3
5
The bold numbers mentioned in the above matrix can also be treated as the pivot
numbers and the corresponding columns are called pivot columns.
A D
2
4
1
2
3
4
0
5
8
8
0
0
0
0
3
5
Similarly the bold numbers mentioned in the above matrix can be treated as the
pivot numbers and the corresponding columns are called pivot columns.
Pivot columns are independent to each other and it is the maximum number
of independent columns of the RREF matrix. It can be shown that the corre-
sponding columns of the original matrix A are independent vectors.

16
1 Matrices
Check for independent vectors
The Linear combinations of the independent vectors is equal to zero vector only
when the scaling factors are identically zeros.
(i.e.)
˛1
2
4
1
5
6
3
5 C ˛2
2
4
2
5
7
3
5 D
2
4
0
0
0
3
5 ) ˛1 D ˛2 D 0
Rewriting the above equations in the standard Linear equation form is as shown
below.
˛1 C 2˛2 D 0- - - - - - - - - - - - - - - - - - - - - - -.1/
5˛1 C 5˛2 D 0- - - - - - - - - - - - - - - - - - - - -.2/
6˛1 C 7˛2 D 0- - - - - - - - - - - - - - - - - - - - -.3/
.2/–5.1/ and .3/–6.1/ gives the following equations
˛1 C 2˛2 D 0- - - - - - - - - - - - - - - - - - - - - - - - -.1/
5˛2 D 0- - - - - - - - - - - - - - - - - - - - - - -.2/
5˛2 D 0- - - - - - - - - - - - - - - - - - - - - - -.3/
) ˛1 D ˛2 D 0
The operation performed above is equivalent to the Row operation of the origi-
nal matrix A. Thus columns of the original matrix corresponding to the pivot
columns are independent to each other.
Hence the corresponding columns in the original matrix form the column space
of the matrix A. Thus column space of the matrix A is represented as set of vectors
˛1
2
4
1
5
6
3
5 C ˛2
2
4
2
5
7
3
5 ; where ˛1; ˛1 2 R
Also note that the dimension of the column space is 2.
Null space of the matrix A is obtained as follows.
A D
2
64
1 2
3
4
5 5
7
12
6 7
10
16
3
75
Row and the column operation of the matrix can be viewed as the matrix multipli-
cation of the particular matrix with the matrix itself as described below.

1.8 Basis of the Four Fundamental Vector Spaces of the Matrix
17
)
2
64
1
0
0
0
1
0
0
0
1
3
75
2
64
1
2 3
4
5
5 7
12
6
7 10
16
3
75 D
2
64
1
2
3
4
5
5
7
12
6
7
10 16
3
75
Applying the Row operation on the Identity matrix in the LHS and the matrix in the
RHS, we get
R2- > R2-5R1
R3- > R3-6R1
2
64
1
0 0
5
1 0
6
0 1
3
75
2
64
1
2
3
4
5
5
7
12
6
7
10
16
3
75 D
2
64
1
2
3
4
0
5
8
8
0
5
8
8
3
75
)
2
64
1
0
0
0
1
0
0
0
1
3
75
2
64
1
0 0
5
1 0
6
0 1
3
75
2
64
1
2
3
4
5
5
7
12
6
7
10
16
3
75 D
2
64
1 2
3
4
0 5
8
8
0 5
8
8
3
75
Again applying the Row operation on the Identity matrix in the LHS and the matrix
in the RHS, we get
2
64
1
0
0
0
1
0
0
1
1
3
75
2
64
1
0
0
5
1
0
6
0
1
3
75
2
64
1
2
3
4
5
5
7
12
6
7
10
16
3
75 D
2
64
1 2
3
4
0 5
8
8
0 0
0
0
3
75
Thus an arbitrary vector
2
664
x1
x2
x3
x4
3
775 if multiplied with the matrix
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5
gives the zero vector, then the same vector
2
664
x1
x2
x3
x4
3
775 when multiplied with the matrix
2
4
1
2
3
4
0
5
8
8
0
0
0
0
3
5 gives the zero vector. In other words the Null space of the
matrix
2
4
1
2
3
4
5
5
7
12
6
7
10
16
3
5 is same as that of the Null space of the matrix after per-
forming Row operation. (i.e.)
Null space of the matrix
2
4
1 2
3
4
0 5
8
8
0 0
0
0
3
5.

18
1 Matrices
Null space of the matrix
2
4
1 2
3
4
0 5
8
8
0 0
0
0
3
5 is determined as follows.
2
4
1
2
3
4
0
5
8
8
0
0
0
0
3
5
2
664
x1
x2
x3
x4
3
775 D
2
4
0
0
0
3
5
Bold numbers of the above matrix is treated as pivot columns. Remaining columns
are called free variable columns. The corresponding variables x1 and x2 are called
pivot variables and the variables x3 and x4 are called free variables.
Representing the pivot variables in terms of free variables, we get
x1 D 2 

8
5  x3  8
5  x4

 3  x3  4  x4
x1 D 16
5  x3 C 16
5  x4  3  x3  4  x4
x1 D 1
5  x3  4
5  x4
x2 D 8
5  x3  8
5  x4
x3 D x3
x4 D x4
Thus the set of vectors as shown below form the Null space of the matrix A.
2
664
x1
x2
x3
x4
3
775 D x3
2
664
1
5
 8
5
1
0
3
775 C x4
2
664
 4
5
 8
5
0
1
3
775 ; x3; x4 2 R
As shown above, the number of independent columns to represent the Null space is
two. Hence dimension of the Null space of the matrix A is given as two.
Row space and the Left Null space of the matrix A is obtained as the Column
space and the Null space of the matrix AT as shown below.
A D
2
4
1 2 3
4
5 5 7 12
6 7 10 16
3
5
AT D
2
664
1 5 6
2 5 7
3 7 10
4 12 16
3
775

1.8 Basis of the Four Fundamental Vector Spaces of the Matrix
19
Applying Row operation,
R2- > R2-2 R1
R3- > R3-3 R1
R4- > R4-4 R1
We get,
2
664
1
5
6
0
5
5
0
8
8
0
8
8
3
775
Further applying the Row operation
R3- > R3-(5/8) R2
R4- > R4-(5/8) R2
We get,
2
664
1
5
6
0
5
5
0
0
0
0
0
0
3
775
The Bold numbers in the above matrix are called pivots and the corresponding
columns are called pivot columns. The columns of the original matrix corresponding
to the pivot columns are the maximum number of independent columns of the matrix
A. Hence they are the basis of the column space of the matrix AT or the Row space
of the matrix A.
Thus the Row space of the matrix A is represented as the set as given below.
˛1
2
664
1
2
3
4
3
775 C ˛2
2
664
5
5
7
12
3
775 ; ˛1; ˛2 2 R
The dimension of the Row space of the matrix A is 2.
Similarly the Left Null space of the matrix A is obtained as the Null space of the
matrix AT
The matrix AT after subjected to Row operation is as shown below.
2
664
1
5
6
0
5
5
0
0
0
0
0
0
3
775

20
1 Matrices
Bold letters mentioned above in the matrix are the pivot elements and the
corresponding columns are the pivot columns. Other column is called free vari-
able column. Null space of the matrix AT is same as that of the Null space of the
matrix AT after subjected to Row operation. Thus the vector space of the form
2
4
x1
x2
x3
3
5 is required such that
2
664
1
5
6
0
5
5
0
0
0
0
0
0
3
775
2
4
x1
x2
x3
3
5 D
2
664
0
0
0
0
3
775 to obtain the Null
space of the matrix AT . The variables x1 and x2 are called pivot variables and the
variable x3 is called free variable.
Representing the pivot variables in terms of free variables, we get the following.
x1 C 5  x2 C 6  x3 D 0- - - - - - - - - - - - - - - - - - - - - -.1/
5  x2  5  x3 D 0- - - - - - - – - - - - - - - - - - - -.2/
Equation .2/ ) x2 D x3
Equation .1/ ) x1 D 5  x2  6  x3
) x1 D 5  x3  6  x3 D x3
Thus the Basis of the Left Null space of the matrix A, which is the Basis of the Null
space of the matrix AT is given below.
2
664
x3
x3
x3
3
775 8x3 2 R
D
2
664
1
1
1
3
775 ˛ 8 ˛ 2 R
Note that the dimension of the Left Null space of the matrix A is one.
1.9
Observations on Results of the Example 1.12
In the Example 1.12, Column space, Null space, Left Column space (Row space)
and the Left Null space of the matrix A D
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5 is obtained as the
following

1.9 Observations on Results of the Example 1.12
21
1.9.1
Column Space
˛1
2
664
1
5
6
3
775 C ˛2
2
664
2
5
7
3
775 ; where ˛1; ˛1 2 R
The dimension of the column space is two. Also note that the Column space of the
matrix A sized 3  4 is the subspace of the vector space R3.
1.9.2
Null Space
2
66664
x1
x2
x3
x4
3
77775
D x3
2
66664
1
5
 8
5
1
0
3
77775
C x4
2
66664
 4
5
 8
5
0
1
3
77775
8x3; x4 2 R
The dimension of the Null space is two. Also note that the Null pace of the matrix
A sized 3  4 is subspace of the vector space R4.
1.9.3
Left Column Space (Row Space)
˛1
2
664
1
2
3
4
3
775 C ˛2
2
664
5
5
7
12
3
775 8 ˛1; ˛2 2 R
The dimension of the Row space is two. Also note that the Row space of the matrix
A sized 3  4 is subspace of the vector space R4.
1.9.4
Left Null Space
2
4
1
1
1
3
5 ˛ 8 ˛ 2 R
The dimension of the Left Null space is one. Also note that the Left Null space of
the matrix A sized 3  4 is subspace of the vector space R3.

22
1 Matrices
1.9.5
Observation
1. The column space and the Left Null space of the matrix A are the subspaces of
the R3, where 3 is the number of rows of the matrix A.
2. The Null space and the Left column space of the matrix A are the subspaces of
the R4, where 4 is the number of columns of the matrix A.
3. Dimension of the column space of the matrix A C Dimension of the Null space
of the matrix A D 2 C 2 D 4 D Number of Columns of the matrix A.
4. Dimension of the Left Null space of the matrix AC Dimension of the Left Col-
umn space (Row space) of the matrix A D 1 C 2 D 4 D Number of Rows of the
matrix A.
5. The Column space of the matrix A and the Left Null space of the matrix A are
orthogonal to each other (i.e.) any vector a 2 C.A/ and b 2 N.AT / satisﬁes the
condition aT b D bT a D 0.
Proof. Let y 2 N.AT / ) AT y D 0¯. Taking transpose on both sides, we get
yT A D 0¯ ) yT Ax D 0¯ (Multiplying arbitrary vector on both sides).
Note that Ax 2 C.A/. Hence proved.
6. The Row of the matrix A and the Null space of the matrix A are orthogonal to
each other.
Proof. Let y 2 N.A/ ) Ay D 0¯. Taking transpose on both sides, we get
yT AT D 0¯ ) yT AT x D 0 (Multiplying arbitrary vector x on both sides). Note
that AT x 2 C.AT /. Hence proved.
In general for the matrix A of size m  n dim.C.A// C dim .N.A// D n. This
is known as Rank-Nullity Theorem.
Dimension of the Column space of the matrix A is the maximum number of inde-
pendent columns (pivot columns) of the matrix A. Let it be ‘r’. From the procedure
of the determining the Null space of the matrix A (Representing all the variables
in terms of free variables), it can be shown that dimension of the Null space of the
matrix A is equal to the number of free variable columns of the matrix A. This is
equal to n-r, where ‘n’ is the total number of columns of the matrix A and the ‘r’ is
the number of pivot columns. Thus dim.C.A// C dim.R.A// D n.
1.10
Vector Representation with Different Basis
Example 1.13.
3
4

2
41
0
3
5
2
40
1
3
5
2 R2 is the vector represented with respect to the basis
1
0
 0
1

. (i.e.) The vector
3
4

2
41
0
3
5
2
40
1
3
5
D 3
1
0

2
41
0
3
5
2
40
1
3
5
C 4
0
1

2
41
0
3
5
2
40
1
3
5

1.10 Vector Representation with Different Basis
23
Similarly
5
6

2
41
1
3
5
2
4 1
1
3
5
2 R2 is the vector represented with respect to the basis
1
1
  1
1

. (i.e.) The vector
5
6

2
41
1
3
5
2
4 1
1
3
5
D 5
1
1

2
41
1
3
5
2
4 1
1
3
5
C 6
 1
1

2
41
1
3
5
2
4 1
1
3
5
The vector
5
6

2
41
1
3
5
2
4 1
1
3
5
D 5
1
1

2
41
1
3
5
2
4 1
1
3
5
C 6
 1
1

2
41
1
3
5
2
4 1
1
3
5
Also the vector
1
1

2
41
1
3
5
2
4 1
1
3
5
is represented as the linear combinations of
1
0

and
0
1

as 1 
1
0

C 1 
0
1

Similarly the vector
 1
1

2
41
1
3
5
2
4 1
1
3
5
is represented as the linear combinations of
1
0

and
0
1

as 1 
1
0

C .1/ 
0
1

Thus the vector
5
6

2
41
1
3
5
2
4 1
1
3
5
is represented as the following
5

1 
1
0

C 1 
0
1

C 6

1 
1
0

C .1/ 
0
1

D .5  1 C 6  1/ 
1
0

C .5  1 C 6  1/ 
0
1

D
1 1
1 1
 5
6

2
41
0
3
5;
2
40
1
3
5
5
6

2
41
1
3
5
2
41 1
1 1
3
5
D
1 1
1 1
 5
6

2
41
0
3
5;
2
40
1
3
5
)
5
6

2
41
1
3
5
2
4 1
1
3
5
D
11
1

2
41
0
3
5;
2
40
1
3
5

24
1 Matrices
Similarly
5
6

2
41
0
3
5
2
40
1
3
5
D
( 1 1
1 1
1 5
6
)
2
41
1
3
5;
2
4 1
1
3
5
1.11
Linear Transformation of the Vector
T: V->U is the Linear transformation such that any vector in the vector space ‘V’
is mapped to another vector that lies in the vector space ‘U’. There exists the trans-
formation matrix to perform this operation. The vector space V can also be equal to
the vector space U.
Example 1.14. T: R2->R2 such that any vector x 2 R2 is mapped to another vector
y 2 R2 using the relation y D
1 0
0 1

x. This is the Linear operation of the
image reﬂection about the origin. Note that the vectors x and y are represented with
respect to the standard basis. Also note that the transformation matrix is with respect
to the standard basis.
R2 Vector points plotted in the 2D plane before and after transformation are given
below (Fig. 1.2).
2D Map after Transformation
2D Map before Transformation
vector [−x −y]
vector [x y]
Fig. 1.2 Illustration of the linear transformation of the vector

1.12 Transformation Matrix with Different Basis
25
1.11.1
Trick to Compute the Transformation Matrix
Identify and note down the transformed vectors for the standard basis
1
0

and
1
0

.
For the example mentioned above
1
0

and
 0
1

are the transformed vectors
corresponding to the standard basis
1
0

and
1
0

respectively. The transformation
matrix corresponding to the above transformation is obtained by representing the
transformed vectors column wise. For the above mentioned example, the transfor-
mation matrix is given as
1 0
0 1

This is same as the one given in the Example 1.14.
1.12
Transformation Matrix with Different Basis
Consider the transformation matrix with respect to the standard basis as de-
scribed below
1 0
0 1

Consider the vector
5
6

2
41
1
3
5
2
4 1
1
3
5
D
1 1
1 1
 5
6

2
41
0
3
5;
2
40
1
3
5
represented with re-
spect to the basis
1
1
  1
1

and
1
0

;
0
1

respectively.
The vector in the standard basis (i.e.)
1 1
1 1
 5
6

2
41
0
3
5;
2
40
1
3
5
is transformed into
another vector in the standard basis using the transformation matrix
1 0
0 1

as
1 0
0 1
 1 1
1 1
 5
6

2
41
0
3
5;
2
40
1
3
5
The above vector in standard basis is equivalent to the vector given below with
respect to the basis
1
1
  1
1


26
1 Matrices
( 1 1
1 1
1 1 0
0 1
 1 1
1 1
 5
6
)
2
41
1
3
5;
2
4 1
1
3
5
Note that the vector
5
6

and the transformed vector are represented with respect to
the basis
1
1

;
 1
1

The matrix
1 0
0 1

is the transformation matrix with respect to the standard
basis and the matrix
( 1 1
1 1
1 1 0
0 1
 1 1
1 1
)
2
41
1
3
5;
2
4 1
1
3
5
is the correspond-
ing transformation matrix with respect to the basis
1
1

;
 1
1

These matrices are
called similar matrices. In general similar matrix of the matrix A is obtained using
invertible matrix M as M 1AM.
1.13
Orthogonality
1.13.1
Basic Deﬁnitions and Results
Inner product: Inner product of the vectors x; y 2 Rn is deﬁned as xT y D
2
66666664
x1
x2
:
:
:
xn
3
77777775

y1y2 : : : yn
	
D x1  y1 C x2  y2 C x3  y3 : : : C xn  yn
Orthogonal vectors: Two vectors x and y are said to be orthogonal if xT y D 0.
Orthogonal basis: Basis B is said to be orthogonal if xT y D 0 8 x; y 2 B; x ¤ y
Orthonormal basis: Basis B is said to be orthonormal basis if xT y D 0 and
xT x D 1 8 x; y 2 B; x ¤ y
Example. Standard basis for the vector space Rn
A set of mutually orthogonal non-zero vectors are linearly independent. But set
of independent vectors need not be orthogonal vectors.

1.14 System of Linear Equation
27
Orthogonality of subspaces: Consider two subspaces S  Rn and T  Rn. The
two subspace S and T are said to be orthogonal if xT y D 0 8 x; 2 S and y 2
T; x ¤ y
Example. 1. Column space of the arbitrary matrix A and the Left Null space of the
matrix A are the orthogonal spaces.
2. Row space of the arbitrary matrix A and the Null space of the matrix A are
orthogonal spaces.
1.13.2
Orthogonal Complement
The Vector space ‘V’ that consists of the all the vectors that are orthogonal to the
vector space ‘S’ is called the orthogonal complement of the vector space ‘S’. It is
denoted as V D S?.
Example 1.15. 1. Column space of the arbitrary matrix A and the Left Null space
of the matrix A are the orthogonal complement.
Row space of the arbitrary matrix A and the Null space of the matrix A are the
orthogonal complement.
1.14
System of Linear Equation
System of Linear Equation can be viewed as the problem of estimating the value of
ci 0S, (i.e.) c1; c2; c3; : : : cm 2 R such that c1X1 C c2X2 C : : : cmXm D b.
where Xi D
2
66666664
x1i
x2i
x3i
:
:
xni
3
77777775
and b D
2
66666664
b1
b2
b3
:
:
bn
3
77777775
and xij; bj 2 R
The Expanded representation of the above mentioned equation is represented as
follows.
c1x11 C c2x12 C c3x13 C    cmx1m D b1
c1x21 C c2x22 C c3x23 C    cmx2m D b2
c1x31 C c2x32 C c3x33 C    cmx3m D b3
  

28
1 Matrices
  
  
c1xn1 C c2xn2 C c3xn3 C    cmxnm D bn
Thus the above equation can also be represented as
2
666664
x11
x12
x13
   x1m
x21
x22
x23
   x2m
x31
x32
x33
   x3m
  
  
  
     
xn1
xn2
xn3
   xnm
3
777775
2
666664
c1
c2
c3
  
cm
3
777775
D
2
666664
b1
b2
b3
  
bm
3
777775
D ŒXc D b
1.15
Solutions for the System of Linear Equation [A] x D b
Note:
Let the size of the matrix ‘A’ be m  n. x 2 Rn; b 2 Rm. Basis of the vector
space Rn can be obtained as the concatenation of the basis vectors in the Row space
of the matrix A with the Basis of the Null space of the matrix ‘A’.
Let fb1; b2; : : : brg be the basis of the Row space of the matrix A. Also
fbrC1; brC2; : : : bng be the basis of the Null space of the matrix A. Then the
basis of the vector space Rn is given a fb1; b2; : : : bng.
Consider any arbitrary vector ‘v’ in the vector space Rn, which can be repre-
sented as the linear combinations of the above mentioned basis vectors as given
below.
v D ˛1b1 C ˛2b2 C ˛3b3 C    ˛rbr C ˛rC1brC1 C ˛rC2brC2 C    C ˛nbn
D ˛1b1 C ˛2b2 C ˛3b3 C    ˛rbr/ C .˛rC1brC1 C ˛rC2brC2 C    C anbn/
D vr C vn
Thus any vector ‘v’ in the vector space Rn can be represented as the direct summa-
tion of the vector from the Row space ‘vr’ and the vector from the Null space ‘vn’.
Similarly any vector in the vector space Rm can be represented as the summation
of the vector from the column space and the vector from the Left Null space.
Case 1: If the vector b lies in the column space of the matrix A
The solution that exists in the vector space Rn can be represented as the direct
sum of the vector from the row space and the vector from the null space. The vector
which is obtained from the row space is unique. But any vector from the null space
can be chosen to add with the one chosen from the row space to get the solution.

1.15 Solutions for the System of Linear Equation [A] x D b
29
Thus in general, there exists inﬁnite number of solutions for the system of Linear
equation. But, if the Null space of the matrix ‘A’ is 0, then the unique solution occurs
for the equation of Linear Equation [A] x D b. In this case the unique solution is
the vector obtained from the row space
Example 1.16. Consider the Linear Equation represented in the matrix form as
shown below
2
4
1
2
3
4
5
5
7
12
6
7
10
16
3
5
2
664
x1
x2
x3
x4
3
775 D
2
4
1
2
3
3
5
Note that A D
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5 The x D
2
664
x1
x2
x3
x4
3
775 and b D
2
4
1
2
3
3
5
The above equation can be viewed as
2
4
1
5
6
3
5 x1 C
2
4
2
5
7
3
5 x2 C
2
4
3
7
10
3
5 x3 C
2
4
4
12
16
3
5 x4 D
2
4
1
2
3
3
5
From the above, we can interpret that the solution for the above linear equation
occurs only if the vector b lies in the column space of the matrix A.
1.15.1
Trick to Obtain the Solution
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5
2
664
x1
x2
x3
x4
3
775 D
2
4
1
2
3
3
5
)
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5
2
664
x1
x2
x3
x4
3
775 D
2
4
1
0
0
0
1
0
0
0
1
3
5
2
4
1
2
3
3
5
Applying the Row operation on the identity matrix on both sides will not affect the
equality

30
1 Matrices
R2- > R2-5  R1
R3- > R3-6  R1
2
64
1
0
0
5
1
0
6
0
1
3
75
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5
2
664
x1
x2
x3
x4
3
775 D
2
64
1
0
0
5
1
0
6
0
1
3
75
2
64
1
2
3
3
75
)
2
64
1
0
0
0
1
0
0
0
1
3
75
2
64
1
2
3
4
0
5
8
8
0
5
8
8
3
75
2
66664
x1
x2
x3
x4
3
77775
D
2
64
1
0
0
0
1
0
0
0
1
3
75
2
64
1
3
3
3
75
Applying again the Row operation
R3- > R3-R2
)
2
4
1
0
0
0
1
0
0
1
1
3
5
2
4
1
2
3
4
0
5
8
8
0
5
8
8
3
5
2
664
x1
x2
x3
x4
3
775 D
2
4
1
0
0
0
1
0
0
1
1
3
5
2
4
1
3
3
3
5
We get
)
2
4
1
2
3
4
0
5
8
8
0
0
0
0
3
5
2
664
x1
x2
x3
x4
3
775 D
2
4
1
3
0
3
5
The modiﬁed equation mentioned above can be used to compute the values for the
unknown variables x1; x2; x3 and x4.
To get the solution, the vector
2
4
1
3
0
3
5 should lie in the column space of the matrix
2
4
1 2
3
4
0 5 8 8
0 0
0
0
3
5. The Bold letters mentioned in the matrix are called pivot elements
and the corresponding columns are called pivot columns and the remaining columns
are called free variable columns. Hence the column space of the above matrix is
obtained as
x1
2
4
1
0
0
3
5 C x2
2
4
2
5
0
3
5

1.15 Solutions for the System of Linear Equation [A] x D b
31
Thus for some values x1 and x2
x1
2
64
1
0
0
3
75 C x2
2
64
2
5
0
3
75 D
2
64
1
3
0
3
75
) x2 D 3
5x1 D 1
5
Thus the particular solution of the equation
2
64
1
2
3
4
5
5
7
12
6
7
10 16
3
75
2
66664
x1
x2
x3
x4
3
77775
D
2
64
1
2
3
3
75 is
2
66664
x1
x2
x3
x4
3
77775
D
2
66664
1=5
3=5
0
0
3
77775
Consider the vector
2
664
n1
n2
n3
n4
3
775 in the null space of the matrix A D
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5
(i.e.)
2
64
1
2
3
4
5
5
7
12
6
7
10
16
3
75
2
66664
n1
n2
n3
n4
3
77775
D
2
64
0
0
0
3
75
Combining the two equation, we get
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5
2
664
x1 C n1
x2 C n2
x3 C n3
x4 C n4
3
775 D
2
4
1 C 0
2 C 0
3 C 0
3
5
)
2
4
1
2
3
4
5
5
7
12
6
7
10 16
3
5
2
664
x1 C n1
x2 C n2
x3 C n3
x4 C n4
3
775 D
2
4
1
2
3
3
5
)
2
664
x1 C n1
x2 C n2
x3 C n3
x4 C n4
3
775 is the solution of the Linear equation.

32
1 Matrices
where
2
664
x1
x2
x3
x4
3
775 is the particular solution and
2
664
n1
n2
n3
n4
3
775 is the particular vector in the Null
space of the matrix A
So the complete solution of Linear Equation of the form Ax D b consists of
particular solution C any vector in the null space of the matrix A.
Computation of Null space of the matrix A
The matrix A in RREF is given below
2
4
1
2
3
4
0
5
8
8
0
0
0
0
3
5
Representing pivot variables in terms of free variables, we get
x2 D 8
5x3 C

8
5

x4
x1 D 2x2  3x3  4x4
) x1 D 2

8
5x3 C

8
5

x4

 3x3  4x4
) x1 D 16
5 x3 C 16
5 x4  3x3  4x4
) x1 D 1
5x3 C 4
5 x4
The Null space of the matrix A is given as
2
6664
1
5x3  4
5x4
 8
5x3  8
5x4
x3
x4
3
7775 ) x3
2
6664
1
5
 8
5
1
0
3
7775 C x4
2
6664
 4
5
 8
5
0
1
3
7775
Thus the Complete solution of the Linear Equation is given as
2
66664
 1
5
3
5
0
0
3
77775
C x3
2
66664
1
5
 8
5
1
0
3
77775
C x4
2
66664
 4
5
 8
5
0
1
3
77775
; where x3; x4 2 R
Case 2: If the vector b does not lie in the column space of the matrix A
For any x 2 Rm; Ax always lies in the column space of the matrix A and
hence If the vector b does not lie in the column space of the matrix A, then solution
doesn’t occur.

1.15 Solutions for the System of Linear Equation [A] x D b
33
In this case the vector x is estimated such that kAx  bk is minimized. It is also
known that the solution occurs in the vector space Rm (i.e.) x 2 Rm Any vector
in the vector space Rm can be represented as the direct sum of the vector from the
column space and the vector from the Left Null space of the matrix A. The vector
bc that lies in the column space of the matrix A are to be found such that kbc  bk
is minimized.
Note:
The vector b D bc C bln:
Multiplying the matrix
ATon both sides; we get
ATb D AT.bc C bln/
D ATbc C ATbln
D ATbc C 0 ŒBecause bln lies in the left Null Space of the matrix A
) ATb D ATbc
Consider solving the equation Ax D b when b does not lie in the column space of
the matrix A.
The vector x cannot be found, because b does not lie in the column space of A.
Hence the best value for the vector x is estimated as bx such that
Abx D bc
Multiplying the matrix AT on both sides, we get
ATAbx D ATbc
) bx D .ATA/1AT bc
) bx D .ATA/1ATb
Œ* AT bc D AT b
Note that .ATA/1.ATA/ is the identity matrix and hence .ATA/1AT is the left
inverse of the matrix A.
Also note that the best estimate bx D .ATA/1AT b exists only when ATA is
invertible. Also note that ATA is invertible if and only if the columns of A are
independent (i.e.) A is invertible. This is true because Null space of A is exactly
equal to the Null space of ATA. So if N.A/ is 0, then N.ATA/ is 0 and hence if A
is invertible, ATA is invertible and vice versa.
It can also be shown that the above estimated value for x (i.e.) bx is the estimate
of the Multi variable x such that kAx  bk is minimized.
Example 1.17. Solving the Linear Equation of the form Ax D b
Where A D
2
4
1 4
2 5
3 6
3
5 b D
2
4
5
7
10
3
5

34
1 Matrices
The best estimate for the variable x lies in the vector space R2.
It is known that
dim.N.A// C dim.R.A// D 2
dim.C.A// C dim.N.AT // D 3
dim.N.A// C dim.C.A// D 2
Note that the dimension of the column space of the matrix A is 2. From the above
mentioned fact, dimension of the Null space of the matrix A is 0, the dimension of
the row space of the matrix A is 2 and the dimension of the Left Null space of the
matrix A is 1.
The vector b does not lie in the column space of A. The best estimate for the
variable x is found as bx such that Abx D bc and kbc  bk is minimized. The vector
bc lies in the column space of the matrix A, which can be represented as
bc D
2
4
1 4
2 5
3 6
3
5
c
x1
bx2

D Abx
Consider the vectors bc and b as shown below (Fig. 1.3).
From the ﬁgure, it can be realized that the distance between the vector bc and b
occur only when the vector bc is orthogonal to the vector .b  bc/. As bc lies in
the column space of A, the vector .b  bc/ orthogonal to the individual basis of the
column space of the matrix A,
)
2
4
1
2
3
3
5
T 0
@b 
2
4
1 4
2 5
3 6
3
5
c
x1
bx2
1
A D 0- - - - - - - - - - - - - - - - - - - - - - - - - - - - -.1/
Fig. 1.3 Pictorial
representation of the vectors
bc and b
0
bc
b
(b-bc)

1.15 Solutions for the System of Linear Equation [A] x D b
35
2
4
4
5
6
3
5
T 0
@b 
2
4
1 4
2 5
3 6
3
5
c
x1
bx2
1
A D 0- - - - - - - - - - - - - - - - - - - - - - - - - - - - -.2/
Combining both the equations we get
2
4
1
4
2
5
3
6
3
5
T 0
@b 
2
4
1
4
2
5
3
6
3
5
c
x1
bx2
1
A D 0
)
2
4
1 4
2 5
3 6
3
5
T
b D
2
4
1
4
2
5
3
6
3
5
T 2
4
1
4
2
5
3
6
3
5
c
x1
bx2

)
2
4
1 4
2 5
3 6
3
5
T 2
4
1
4
2
5
3
6
3
5
c
x1
bx2

D
2
4
1
4
2
5
3
6
3
5
T
b
)
c
x1
bx2

D
0
B@
2
4
1
4
2
5
3
6
3
5
T 2
4
1 4
2 5
3 6
3
5
1
CA
1 2
4
1
4
2
5
3
6
3
5
T
b
)
c
x1
bx2

D
0
B@
2
4
1
4
2
5
3
6
3
5
T 2
4
1 4
2 5
3 6
3
5
1
CA
1 2
4
1 4
2 5
3 6
3
5
T 2
4
5
7
10
3
5 D Œbx D .AT A/1AT b
which is same as the one used in the previous section.
Œbx D
1:7222
0:7778

Œbx D .ATA/1AT b
Multiplying both sides by A
AŒbx D .ATA/1AT b
AŒbx is the vector bc that lies in the column space of the matrix A.
) A.ATA/1AT b D bc
A.AT A/1 AT is called as the projection matrix, that projects the vector in
the vector space R3 into the column space of the matrix A such that kbc  bk is
minimized.

36
1 Matrices
In this example Projection matrix is
2
4
0:8333
0:3333 0:1667
0:3333
0:3333
0:3333
0:1667 0:3333
0:8333
3
5
.AT A/1.AT A/ D Identity matrix. Hence it is also clear that .AT A/1 AT is
the left inverse of the matrix A, which is usually represented as AC.
In this example Left inverse matrix of the matrix A is given as
0:9444 0:1111
0:7222
0:4444
0:1111
0:2222

0:9444 0:1111
0:7222
0:4444
0:1111
0:2222
 2
4
1
4
2
5
3
6
3
5 D
1
0
0
1

Left inverse of the matrix exists if .AT A/1 exists. Similarly if .AAT / .AAT /1 D
Identity matrix, AT .AAT /1 is the right inverse of the matrix A. Right inverse of
the matrix exists only when .AAT /1 exists.
As already mentioned, if the matrix A consists of all columns independent,
.AT A/1 exists and hence Left inverse of the matrix exists. Similarly if all the
rows of the matrix A are independent (i.e.) all the columns of the matrix AT is
independent, Right inverse of the matrix exists.
Thus solving the equation Ax D b when b is not in the column space of the
matrix A (the matrix with all the columns are independent to each other), can be
obtained by multiplying the left inverse matrix of the matrix A represented as AC
with the vector b
Œbx D .ATA/1AT b D ACb
Note: Solving the equation Ax D b, when b is not in the column space of the matrix
A and .AT A/1 doesn’t exist can be obtained using Singular Value Decomposition
(SVD) which is referred in the Section 4.25.
1.16
Gram Schmidt Orthonormalization Procedure
for Obtaining Orthonormal Basis
1. Given the set of independent vectors fv1; v2; v3; : : : vng which forms the basis of
the vector space V, an alternative orthonormal basis fa1; a2; : : : ang for the vector
space ‘V’ can be obtained using Gram-Schmidt Orthonormalization procedure
as described below (Fig. 1.4).
Steps
1. a1 D
v1
kv1k

1.16 Gram Schmidt Orthonormalization Procedure for Obtaining Orthonormal Basis
37
Fig. 1.4 Illustration
of Gram-Schmidt
orthogonalization
0
p*a1
a1
v2-p*a1
v2
Find out the vector a2 corresponding to the vector v1 that is orthogonal to the
vector a1
2. The vector that is orthogonal to the vector a1 is v2pa1 as shown in the ﬁgure,
where p  a1 is the perpendicular projection of the vector v2 on the vector a1.
The projection point p is computed using the following condition.
.v2  pa1/T a1 D
).v2/T a1 D .pa1/T a1
)p D v2T a1
a1T a1 D v2T a1Œ* a1T a1 D 1 D a1T v2
Thus the orthogonal vector q2 to the vector a1 is obtained as q2 D v2 
.a1T v2/  a1 and the corresponding orthonormal vector is given as a2 D
q2
kq2k
3. Now we are in need of the vector which is orthogonal to both a1 and a2 corre-
sponding to the vector v3
The perpendicular projection vector of the vector v3 on the column space of
the matrix consists of a1 and a2 as their columns is obtained as p1a1Cp2a2
(as shown in Fig. 1.5). The vector v3  p1  a1 C p2  a2 is perpendicular
to both the vectors a1 and a2. The projection points p1; p2 are obtained as the
perpendicular projection of the vector v3 on the vector a1 and a2 respectively
which are computed as
p1 D v3T a1
a1T a1 D v3T a1Œ* a1T a1 D 1 D a1T v3
p2 D v3T a2
a2T a2 D v3T a2Œ* a2T a2 D 1 D a2T v3

38
1 Matrices
Fig. 1.5 Illustration of
Gram-Schmidt
orthogonalization
0
a1
a2
p1*a1+p2*a2
v3
v3-p1*a1-p2*a2
Thus the vector perpendicular to both the orthonormal vectors a1 and a2 is given
as q3 D v3  a1T v3  a1  a2T v3  a2 - - - - - - - - - - - - - - - - - - - - - - - - -(3)
and the corresponding orthonormal vector which is perpendicular to both a1 and
a2 is given as
a3 D
q3
kq3k
This can also be computed using the Projection matrix as follows.
The perpendicular projection vector q30 of the vector of the vector v3 on the
column space of the matrix consists of a1 and a2 as their columns is obtained
using projection matrix as follows
Let A D Œa1 a2 (Note that a1 and a2 are the column vectors).
Projection vector q30 D A.AT A/1AT q3
D
 

a1 a2
	 a1T
a2T
 
a1 a2
	1 a1T
a2T
!
q3
D

a1 a2
	
.I/1
a1T
a2T

q3
D

a1 a2
	 a1T
a2T

q3
D .a1a1T C a2T a2/q3
Therefore perpendicular vector q3 D v3  q30 D v3  .a1a1T C a2T a2/q3 that
is same as Eq. 3 as mentioned above.

1.16 Gram Schmidt Orthonormalization Procedure for Obtaining Orthonormal Basis
39
4. Similarly the vector perpendicular to both the orthonormal vectors a1 and
a2; a3; a4; : : : an  1 corresponding to the independent vector vn is given as
qn D vn  a1T vn  a1  a2T vn  a2  a3T vn  a3
a4T vn  a4     an1T vn
an1 and the corresponding orthonormal
vector is
an D
qn
kqnk
Thus the set of orthonormal basis fa1; a2; : : : ang, of the vector space ‘V’ cor-
responding to the basis vectors fv1; v2; : : : ; vng is obtained using Gram-Schmidt
orthogonalization procedure.
Example 1.18. Consider the matrix A D
2
664
1
5
10
2
6
10
3
7
11
4
8
12
3
775 whose column vectors are
independent to each other.
Using Gram-Schmidt, the corresponding orthonormal vectors are obtained as
follows
Let v1 D
2
664
1
2
3
4
3
775 ; v2 D
2
664
5
6
7
8
3
775 and v3 D
2
664
10
10
11
12
3
775
a1 D
v1
kv1k D
2
664
0:1826
0:3651
0:5477
0:7303
3
775
q2 D v2  .a1T v2/  a1 D
2
664
2:6667
1:3333
0:0000
1:3333
3
775
a2 D
q2
kq2k D
2
664
0:1865
0:4082
0:0000
0:4082
3
775
q3 D v3  a1T v3  a1  a2T v3  a2 D
2
664
0:3000
0:4000
0:1000
0:2000
3
775

40
1 Matrices
a3 D
q3
kq3k D
2
664
0:5477
0:7303
0:1826
0:3651
3
775
Thus the set of orthonormal vectors are
2
664
0:1826
0:8165
0:5477
0:3651
0:4082
0:7303
0:5477
0:0000
0:1826
0:7303 0:4082
0:3651
3
775
1.17
QR Factorization
The matrix with independent column vectors can be represented as the product of the
matrix with orthonormal column vectors and the upper triangular matrix. Consider
the matrix A as shown below.
A D Œv1 v2 : : : vn, where v1 v2 : : : vn are the independent vectors.
From Gram Schmidt orthogonalization procedure discussed in the previous
section,
a1 D
v1
kv1k
) v1 D a1kv1k
Multiplying a1T on both sides, we get
) a1T v1 D a1T a1kv1k
) kv1k D a1T v1
From the above, we get v1 D a1.a1T v1/
Similarly
a2 D
q2
kq2k
) kq2ka2 D q2
Multiplying a2T on both sides, we get
) a2T q2 D a2T a2kq2k
) kq2k D a2T q2

1.17 QR Factorization
41
Also
q2 D v2  .a1T v2/  a1 .see previous section/
Multiplying a2T on both sides, we get
a2T q2 D a2T v2  a2T  .a1T v2/  a1
) a2T q2 D a2T v2
Also we know
kq2k D a2T q2
) a2T v2 D kq2k
Rewriting the equation for v2 we get,
v2 D q2 C .a1T v2/  a1
)v2 D kq2ka2 C .a1T v2/  a1
)v2 D .a2T v2/  a2 C .a1T v2/  a1
Similarly it can be shown that
)v3 D a3  .a3T v3/ C a2  .a2T v3/ C a1  .a1T v3/
Similarly it can be shown that
) vn D an  .anT vn/ C an1  .an1
T vn/ C an2  .an2
T vn/
Can3  .an3T vn/ C an4  .an4T vn/ C    a1  .a1T vn/
Representing the above list of equations for v1; v2; v3; : : : vn in matrix form
A D

v1 v2
v3
: : :
vn
	
D 
a1 a2 a3 : : : an
	
2
666666666664
.a1T v1/
.a1T v2/
.a1T v3/
 

.a1T vn/
0
.a2T v2/ .a2T v3/  

.a2T vn/
0
0
.a3T v3/  

.a3T vn/
0
0
0
 

.a4T vn/
0
0
0
 

.a5T vn/
0
0
0
 

.a6T vn/
: : :
: : :
: : :
 

: : :
0
0
0
 

.anT vn/
3
777777777775
Thus the matrix A is represented as the product of the matrix consists of orthonormal
vectors and the upper triangular matrix as shown above.

42
1 Matrices
Example 1.19. From the Example 1.17, QR factorization of the matrix A is given
below
2
664
1
5
10
2
6
10
3
7
11
4
8
12
3
775 D
2
664
0:1826
0:8165
0:5477
0:3651
0:4082
0:7303
0:5477
0:0000
0:1826
0:7303 0:4082
0:3651
3
775
2
4
5:4772 12:7802 20:2657
0
3:2660
7:3485
0
0
i0:5477
3
5
1.18
Eigen Values and Eigen Vectors
For the given matrix A, if there exists the non zero vector x such that Ax D x,
where  is the scaling factor, then the vector x is the Eigen vector corresponding
to the matrix A. x is in the column space of the matrix A. As  is the scalar, the
Eigen vector x lies in the column space of the matrix A.
)ŒA  Ix D 0
where I is the Identity matrix
) The Null space of the matrix ŒA  I ¤ 0
) If the matrix A is the square matrix, the matrix ŒA  I is called as singular
matrix.
) Determinant .A  I/ D 0, which is represented as jA  Ij D 0 is the
polynomial equation with variable . The equation thus obtained is called charac-
teristic equation and the solutions to the characteristic equation are called Eigen
values. Let the degree of the characteristic polynomial be ‘n’ and the correspond-
ing Eigen values are 1; 2; 3; : : : n (say). Eigen values thus obtained need not
be distinct.
For every distinct Eigen values, there exist one or more Eigen vectors that are
obtained as follows. The Eigen vectors corresponding to the Eigen value 1 are
obtained as the basis of the Null space of the matrix ŒA  1I.
Example 1.20. Consider the matrix A D
2
4
1
0
0
0
2
3
0
4
5
3
5
The Characteristic polynomial of the matrix A is given as
3 C 82  5  2 D 0

1.18 Eigen Values and Eigen Vectors
43
The Eigen values of the matrix A are given as
1 D 1
2 D 7 C
p
57
2
D 7:2749
3 D 7 
p
57
2
D 0:2749
The Eigen vectors corresponding to the distinct Eigen values are obtained as fol-
lows. Eigen vector corresponding to the Eigen value 1 D 1 is the null space of the
matrix
ŒA  1I D A D
2
4
1
0
0
0
2
3
0
4
5
3
5  1
2
4
1
0
0
0
1
0
0
0
1
3
5 D
2
4
0
0
0
0
1
3
0
4
4
3
5
The Null space of the matrix
2
4
0 0
0
0 1
3
0 4
4
3
5 is obtained as follows.
2
4
1
0
0
3
5 x; x; 2 R
Similarly the Null space of the matrix ŒA  2I is obtained as
2
4
0
0:4944
0:8693
3
5 x; x 2 R
and the Null space of the matrix ŒA  3I is obtained as
2
4
0
0:7968
0:6042
3
5 x; x 2 R
Thus the Eigen vectors are
2
4
1
0
0
3
5 ;
2
4
0
0:4944
0:8693
3
5 ;
2
4
0
0:7968
0:6042
3
5

44
1 Matrices
1.19
Geometric Multiplicity (Versus) Algebraic Multiplicity
Let the distinct Eigen values of the arbitrary matrix ‘A’ are 1; 2; 3; : : : k such
that 1 is repeated n1 times, 2 is repeated n2 times and similarly k is repeated nk
times. The sequence of numbers n1; n2; n3; : : : nk are called Algebraic multiplicity
of the corresponding Eigen values 1; 2; 3; : : : k. The dimension of the Null
space of the following matrices ŒA  1I; ŒA  2I; ŒA  3I; : : : ŒA  kI are
m1; m2; m3; : : : mk respectively. The sequence of numbers m1; m2; m3; : : : ; mk are
called Geometric multiplicity corresponding to the Eigen values 1; 2; 3; : : : k.
For any Eigen value ‘k’, mk  nk. If mk < nk, the concern matrix ‘A’ is called
deﬁcient matrix.
Note:
1. Eigen vectors corresponding to distinct Eigen values are independent.
Proof. Let 1; 2; 3 forms the Eigen values of the matrix A and the correspond-
ing Eigen vectors are e1; e2; e3 respectively.
Let suppose c1e1 C c2e2 C c3e3 D 0            .a/:
Multiply the matrix A on both sides, we get
c1Ae1 C c2Ae2 C c3Ae30:
) c1Ae1 C c2Ae2 C c3Ae3 D 0:
) c11e1 C c22e2 C c33e3 D 0          .b/:
.b/  1.a/
we get
cl.1  3/e1 C c2.2  3/e2 D 0          .c/
Multiplying the matrix A on both sides, we get
c1.1  3/A e1 C c2.2  3/Ae2 D 0
)c1.1  3/1e1 C c2.2  3/2e2
D 0        .d/
.d/  2.c/
we get
c1.1  3/.1  2 D 0
) c1 D 0 Œ* 1; 2; 3 are distinct Eigen values
Similarly it can be shown that c2 D 0 and c3 D 0
Hence proved (See Section 1.6)

1.19 Geometric Multiplicity (Versus) Algebraic Multiplicity
45
In the Example 1.19, the Eigen vectors corresponding to distinct Eigen values
1, 7.2749 and 0:2749 are
2
4
1
0
0
3
5 ;
2
4
0
0:4944
0:8693
3
5 ;
2
4
0
0:7968
0:6042
3
5. They are independent
vectors.
2. Note that Eigen vectors corresponding to the particular Eigen value is the basis
of the null space matrix of the form ŒA  1I, where 1 is the Eigen value of
the matrix A and hence they are independent vectors. Also Eigen vectors corre-
sponding to distinct Eigen values are also independent. Number of Eigen values
with or without repetition is equal to the degree of the characteristic function.
Thus if the matrix A of size m  n is not deﬁcient, Eigen vectors of the matrix A
forms the basis for the Rm.
In the Example 1.19, the matrix A of size 33 is not the deﬁcient matrix because
of the following reasons.
 Geometric multiplicity of the Eigen value 1 D Algebraic multiplicity of the
particular Eigen value 1 D 1
 Geometric multiplicity of the Eigen value 7.2749 D Algebraic multiplicity of
the particular Eigen value 7:2749 D 1
 Geometric multiplicity of the Eigen value 0:2749 D Algebraic multiplicity
of the particular Eigen value 0:2749 D 1
Thus the Eigen vectors mentioned above forms the basis of the vector space R3
3. Let Determinant
.ŒA  I D .  1/.  2/.  3/ : : : .  n/
Substituting the value for  D 0 on both sides, we get
Determinant
.ŒA/ D .0  1/.0  2/.0  3/ : : : .0  n/ D
D .1/n123 : : : n
It can also be shown that trace of the matrix D a1 C a2 C a3 C    an
D 1 C 2 C 3 C    C n by comparing the co-efﬁcient of ./n1
4. Let one of the similar matrix for the arbitrary matrix A of size n  n be V and is
obtained using the arbitrary invertible matrix M of size n  n as B D M 1AM.
It can be shown that the Eigen values of the matrices A and B are equal. Also it
can also be shown that if x is the Eigen vector of the matrix A, then M 1x is the
Eigen vector of the matrix B.
Proof. Consider the characteristic of the matrix B as jB  Ij D 0j. Substitute
B D M 1AM in the equation we get jM 1AM  Ij D 0j
)jM 1AM  M 1Mj D 0j
)jM 1.A  I/M j D 0j

46
1 Matrices
)j.A  I/j D 0j
) The Characteristic equation of the matrix A and the Characteristic equation
of the matrix B are the same and hence their Eigen values are equal. If ‘x’
is the Eigen vector of the matrix A corresponding to the Eigen value ‘’.
Ax D x. )MBM1x D x)B.M 1x/ D .M 1x/
Thus M 1x is the Eigen vector of the matrix B.
5. Computing Eigen vector for the block diagonal matrix.
Consider the matrix of the form
C D
2
66666664
1 2
3
0
0
0
4 5
6
0
0
0
7 8
9
0
0
0
0 0
0
10
11 12
0 0
0
13
14 15
0 0
0
16
17 18
3
77777775
D
A
0
0
B

; where
A D
2
4
1
2
3
4
5
6
7
8
9
3
5 B D
2
4
10
11 12
13
14 15
16
17 18
3
5
Eigen vectors of the matrix A (Arranged column wise) are given as
2
4
0:2320 0:7858
0:4082
0:5253 0:0868 0:8165
0:8187
0:6123
0:4082
3
5
Similarly Eigen vectors of the matrix B (Arranged column wise) are given as
2
4
0:4482
0:7392
0:4082
0:5689 0:0333 0:8165
0:6896
0:6727
0:4082
3
5
The Eigen vectors of the matrix C are obtained as follows. (See Eigen vectors of
A and B.)
2
66666664
0:2320 0:7858
0:4082
0
0
0
0:5253 0:0868 0:8165
0
0
0
0:8187
0:6123
0:4082
0
0
0
0
0
0
0:4482 0:7392
0:4082
0
0
0
0:5689 0:0333 0:8165
0
0
0
0:6896
0:6727
0:4082
3
77777775

1.20 Diagonalization of the Matrix
47
1.20
Diagonalization of the Matrix
If the matrix A is not the deﬁcient matrix, it can be diagonalizable as described
below.
If the matrix A of size m  n is not the deﬁcient matrix, then there exists
‘n’ Eigen vectors e1; e2; e3; e4; e5; e6; : : : en, corresponding to ‘n’ Eigen values
1 2 3 4 5 : : : n with or without repetition, that satisﬁes the following
conditions.
A e1 D 1 e1
A e2 D 2 e2
A e3 D 3 e3
: : :
A en D 1 en
The above set of equations is written in the matrix form as shown below.
2
66666664
a11
a12
a13
a14
: : :
a1n
a21
a22
a23
a24
: : :
a2n
a31
a32
a33
a34
: : :
a3n
a41
a42
a43
a44
: : :
a4n
: : :
: : :
: : :
: : :
: : :
: : :
an1
an2
an3
an4
: : :
ann
3
77777775
2
66666664
e11
e21
e31
e41
: : :
en1
e12
e22
e23
e42
: : :
en2
e13
e23
e33
e43
: : :
en3
e14
e24
e34
e44
: : :
en4
: : :
: : :
: : :
: : :
: : :
: : :
e1n
e2n
e3n
e4n
: : :
enn
3
77777775
D
2
66666664
e11
e21
e31
e41
: : :
en1
e12
e22
e23
e42
: : :
en2
e13
e23
e33
e43
: : :
en3
e14
e24
e34
e44
: : :
en4
: : :
: : :
: : :
: : :
: : :
: : :
e1n
e2n
e3n
e4n
: : :
enn
3
77777775
2
66666664
1
0
0
0
: : :
0
0
2
0
0
: : :
0
0
0
3
0
: : :
0
0
0
0
4
: : :
0
: : :
: : :
: : :
: : :
: : :
: : :
0
0
0
0
: : :
n
3
77777775
)
2
6666666666664
a11
a12
a13
a14
: : :
a1n
a21
a22
a23
a24
: : :
a2n
a31
a32
a33
a34
: : :
a3n
a41
a42
a43
a44
: : :
a4n
: : :
: : :
: : :
: : :
: : :
: : :
an1
an2
an3
an4
: : :
ann
3
7777777777775

48
1 Matrices
D
2
6666666666664
e11
e21
e31
e41
: : :
en1
e12
e22
e23
e42
: : :
en2
e13
e23
e33
e43
: : :
en3
e14
e24
e34
e44
: : :
en4
: : :
: : :
: : :
: : :
: : :
: : :
e1n
e2n
e3n
e4n
: : :
enn
3
7777777777775
2
66666664
1
0
0
0
: : :
0
0
2
0
0
: : :
0
0
0
3
0
: : :
0
0
0
0
4
: : :
0
: : :
: : :
: : :
: : :
: : :
: : :
0
0
0
0
: : :
n
3
77777775

2
66666664
e11
e21
e31
e41
: : :
en1
e12
e22
e23
e42
: : :
en2
e13
e23
e33
e43
: : :
en3
e14
e24
e34
e44
: : :
en4
: : :
: : :
: : :
: : :
: : :
: : :
e1n
e2n
e3n
e4n
: : :
enn
3
77777775
T
Where A D
a11
a12
a13
a14
: : :
a1n
a21
a22
a23
a24
: : :
a2n
a31
a32
a33
a34
: : :
a3n
a41
a42
a43
a44
: : :
a4n
: : :
: : :
: : :
: : :
: : :
: : :
an1
an2
an3
an4
: : :
ann
Eigen vectors is given as e1 D
2
6666666664
e11
e12
e13
e14
e15
: : :
e1n
3
7777777775
e2 D
2
6666666664
e21
e32
e23
e24
e25
: : :
e2n
3
7777777775
e3 D
2
6666666664
e31
e32
e33
e34
e35
: : :
e3n
3
7777777775
: : : en D
2
6666666664
en1
en2
en3
en4
en5
: : :
enn
3
7777777775
The Eigen vector matrix E D
e11
e21
e31
e41
: : :
en1
e12
e22
e23
e42
: : :
en2
e13
e23
e33
e43
: : :
en3
e14
e24
e34
e44
: : :
en4
: : :
: : :
: : :
: : :
: : :
: : :
e1n
e2n
e3n
e4n
: : :
enn

1.21 Schur’s Lemma
49
Diagonal matrix D D
2
66666664
1
0
0
0
: : :
0
0
2
0
0
: : :
0
0
0
3
0
: : :
0
0
0
0
4
: : :
0
: : :
: : :
: : :
: : :
: : :
: : :
0
0
0
0
: : :
n
3
77777775
Thus the non-deﬁcient matrix A is represented as the product of the transpose of the
Eigen vector matrix, Diagonal matrix and the Eigen vector matrix (i.e.) A D EDET .
1.21
Schur’s Lemma
For any square matrix A, there exists the Unitary matrix U such that U HAU D T ,
where T is the triangular matrix.
Example 1.21. Consider the matrix A as shown below.
A D
2
4
1
2
3
4
5
7
7
8
10
3
5
Eigen values and the corresponding Eigen vectors are as shown below.
17.1747, 1; 0:1747. The Eigen vector corresponding to the Eigen value
17.1747 is E1 D
2
4
0:2176
0:5392
0:8136
3
5.
Construct the Unitary matrix U1 with the Eigen vector E1 as the ﬁrst column and
other two columns are arbitrarily chosen.
U1 D
2
4
0:2176
1
1
0:5392
1
0:7726
0:8136 0:9302 0:2445
3
5
.U1/HA U1 D
2
4
0:2176
1
1
0:5392
1
0:7726
0:8136 0:9302 0:2445
3
5
H

2
4
1 2 3
4 5 7
7 8 10
3
5
2
4
0:2176
1
1
0:5392
1
0:7726
0:8136 0:9302 0:2445
3
5
D
2
4
17:1753 6:0233
3:6934
0
2:6023
0:9996
0
0:3201
0:4418
3
5

50
1 Matrices
Now consider the matrix B D
2:6023 0:9996
0:3201 0:4418

The Eigen values of the matrix B are 2:7414 and 0:3027. The corresponding
Eigen vector 2:7414 is
0:9905
0:1379

Form the matrix U2 D
2
4
1
0
0
0 0:9905 e1
0 0:1379 e2
3
5
The vector
e1
e2

are chosen such that the columns of the matrix
0:9905 e1
0:1379 e2

are orthogonal to each other.
Thus the matrix U2 is chosen as
2
4
1
0
0
0 0:9905
1
0 0:1379 7:1827
3
5
U20U2 D
2
4
1
0
0
0
1
0
0
0
52:5912
3
5
.U2/H.U1/HA U1U2 D
2
4
17:1753
6:4754
20:5055
0
2:7417
4:9272
0
0
15:9140
3
5
Let U H D .U2/H.U1/H, which is also the Unitary Matrix
) U HA U D
2
4
17:1753
6:4754
20:5055
0
2:7417
4:9272
0
0
15:9140
3
5
By using the procedure described above, any arbitrary matrix A, there exists the
Unitary matrix U such that U HAU D T , where T is the triangular matrix.
1.22
Hermitian Matrices and Skew Hermitian Matrices
Let AH is the conjugate transpose of the matrix A. The matrix is said to be
Hermitian matrix if AH D A and the matrix A is said to be Skew Hermitian if
AH D A.
Properties:
1. xHAx is real.
Proof. Taking the conjugate transpose of the matrix xH Ax, we get .xHAx/H D
xHAx. Hence proved.

1.22 Hermitian Matrices and Skew Hermitian Matrices
51
2. Eigen values of the Hermitian matrix are real.
Proof. Vector such that Ax D x.
Multiplying xH on both sides, we get
xH Ax D xHx:
We know xH Ax and xH x are the real numbers.
)  xH x D real number
)  is the real number:
3. Eigen vectors of the Hermitian matrix A corresponding to distinct Eigen values
are orthogonal.
Proof. Let A be the Hermitian matrix and let 1; 2 be the distinct Eigen values
of the matrix A and the corresponding Eigen vectors are e1 and e2.
Ae1 D 1 e1
Ae2 D 2 e2
Consider
.1 e1/He2 D .A e1/He2 D e1
H AHe2 D e1
HAe2 D e1
H2e2
(Note that A D AH)
) .1 e1/He2 D e1H2e2 ) e1He2.1  2/ D 0
) e1
He2 D 0 ŒBecause 1  2 ¤ 0
Hence proved.
Example 1.22. Let A D
2
4
1
i
3i
i
2
4
3i
4
5
3
5 be the Hermitian matrix (i.e.) A D AH
Eigen values are 1:3560, 0.4123 and 8.9438 (They are real) and the correspond-
ing Column wise Eigen vectors are
2
4
0:5410i
0:7601i
0:3599i
0:5728
0:6464
0:5041
0:6158
0:0666
0:7851
3
5

52
1 Matrices
Note that Eigen vectors are orthogonal to each other. (i.e.)

0:5410i 0:5728 0:6158
	
2
4
0:5410i
0:5728
0:6158
3
5 D 1:110216 l 0
Similarly it can be shown that all the Eigen vectors are orthogonal to each other.
4. Hermiitian matrix is always diagonalizable using the unitary matrix.
For all arbitrary Hermitian matrixes ‘A’, there exists the Unitary matrix U such
U HAU D D, where D is the Diagonal matrix.
Proof. From Schur’s lemma, for any arbitrary Hermitian matrix ‘A’, there exists
the Unitary matrix U such that U HAU D T . Taking Hermitian transpose on both
sides, we get
U HAHU D T H
) U HAU D T H
ŒBecause A D AH
) T D T H
) ‘T ’, in this case is the Diagonal matrix ‘D’ with all the elements in the matrix
are ﬁlled up with real numbers. Hence proved.
1.23
Unitary Matrices
Columns of the unitary matrices are orthonormal to each other. Let U be the Unitary
matrix, then U HU D I (Identity matrix)
Properties:
1. kUxk D kxk.
Proof.
Sqrt..Ux/H.Ux// D Sqrt..x/H.U /H.U /.x// D Sqrt..x/H .x// D kxk
Hence proved.
2. .Ux/H..Uy/ D .x/H .y/.
3. If U1; U2 are Unitary matrices, then U1U2 is also the Unitary matrix.
4. U is always invertible. In particular the inverse of the unitary matrix U is given
as U H.
5. Magnitude of the Eigen values of the unitary matrix is always one.
Proof. The Eigen vector x of the unitary matrix ‘U’ satisﬁes the condition
Ux D x, where  is the Eigen value of the unitary matrix

1.23 Unitary Matrices
53
Taking norm on both sides, we get
kUxk D kxk .see property 1/
kUxk D kxk D sqrt..x/H.x//
D sqrt.xH Hx/ D sqrt..H/sqrt..xHx// D kkkxk
D jjkxk
) kxk D jjkxk
) jj D 1
Hence proved.
6. Eigen vectors corresponding to distinct Eigen values are orthogonal.
Proof. Consider the Unitary matrix U such that Ux1 D 1x1 and x2 D 2x2,
where x1; x2 are Eigen vectors corresponding to the distinct Eigen values 1
and 2.
Consider .1x1/H.2x1/ D 12x1Hx1 D .Ux1/H.Ux1/ D x1Hx1
) 12x1Hx1 D x1H x1
) .12  1/x1Hx1 D 0
) x1Hx1 D 0 ŒBecause .12  1/ ¤ 0
Hence proved
Example 1.23.
A D
 0:7071
0:7071
0:7071i
0:7071i

1. Note that the columns of the matrix are orthonormal to each other.
.i:e:/ AHA D
1
0
0
1

2. The Eigen values of the matrix A are 0:9659 C 0:2588i and 0:2588  0:9659i.
Note that the magnitude of the Eigen values are 1.
3. The Eigen vectors corresponding to the distinct Eigen values are listed below.
E1 D
ˇˇˇˇ
0:8881
0:3251 C 0:3251i
ˇˇˇˇ ; E2 D
0:3251 C 0:3251i
0:8881

Note that they are orthonormal to each other.
.i:e/ E1
HE2 D
1
0
0
1


54
1 Matrices
Example 1.24. The DFT matrix is the unitary matrix.
Four-point DFT matrix is as shown below.
A D
1
2

2
664
1
1
1
1
1
w
w2
w3
1
w2
w4
w6
1
w3
w6
w9
3
775
where w D e

j2
4

Note that the columns of the matrix A are orthonormal to each other.
.i:e/A0  A D
2
664
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
3
775
1. The Eigen values of the matrix A are 1, 1, i
2. The Eigen vectors corresponding to the above mentioned Eigen values are
given as
2
664
0:8660
0:2887
0:2887
0:2887
3
775 ;
2
664
0:5
0:5
0:5
0:5
3
775 ;
2
664
0
0:7071
0
0:7071
3
775 ;
2
664
0:0231  0:0070i
0:4158  0:0035i
0:8085
0:4158  0:0035i
3
775
Characteristics of the DFT Matrices
1. The Eigen values of the DFT matrices are one among the following values 1, 1,
i, i. The magnitude of the Eigen values are always one.
2. Consider the circular convolution of the following two sequences as shown
below.
Let X D
2
664
1
2
3
4
3
775 and H D
2
664
5
6
7
8
3
775
The circular convolution performed using matrix method is as shown below.
2
664
1
4
3
2
2
1
4
3
3
2
1
4
4
3
2
1
3
775
2
664
5
6
7
8
3
775 D
2
664
66
68
66
60
3
775

1.23 Unitary Matrices
55
The circular matrix C D
2
664
1
4
3
2
2
1
4
3
3
2
1
4
4
3
2
1
3
775 is diagonalized using DFT Unitary matrix
U as shown below.
C D UDU1
)
2
664
1
4
3
2
2
1
4
3
3
2
1
4
4
3
2
1
3
775 D
2
664
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
0:5
0:5
0:5
0:5
05
0:5i
0:5
0:5i
3
775 
2
664
10
0
0
0
0
2 C 2i
0
0
0
0
2  2i
0
0
0
0
2
3
775
2
664
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
3
775
Where U D
2
664
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
0:5
0:5
0:5
0:5
05
0:5i
0:5
0:5i
3
775
D D
2
664
10
0
0
0
0 2 C 2i
0
0
0
0
2  2i 0
0
0
0
2
3
775
Let B D
2
664
5
6
7
8
3
775
Multiplying Matrix B on both sides in the above equation we get,
CB D UDU 1B
)
2
664
1
4
3
2
2
1
4
3
3
2
1
4
4
3
2
1
3
775
2
664
5
6
7
8
3
775 D
2
664
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
0:5
0:5
0:5
0:5
05
0:5i
0:5
0:5i
3
775 
2
664
10
0
0
0
0
2 C 2i
0
0
0
0
2  2i
0
0
0
0
2
3
775
2
664
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
3
775
2
664
5
6
7
8
3
775

56
1 Matrices
The product U 1B in the RHS as shown below is given as
2
664
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
3
775
2
664
5
6
7
8
3
775 D
2
664
13
1 C i
1
1  i
3
775 : This can be viewed as
the DFT of the vector ‘B’ (Except the scaling factor). The vector thus obtained is
in frequency domain. The vector thus obtained in frequency domain is scaled using
the diagonal matrix D as shown below. DU1B
)
2
664
10
0
0
0
0 2 C 2i
0
0
0
0
2  2i 0
0
0
0
2
3
775
2
664
13
1 C i
1
1  i
3
775 D
2
664
1:3X102
0:040iX102
0:02X102
0:04iX102
3
775
The obtained vector is then multiplied with the Unitary matrix U can be viewed as
the inverse DFT as shown below to obtain the circular convolved output.
2
664
0:5
0:5
0:5
0:5
0:5
0:5i
0:5
0:5i
0:5
0:5
0:5
0:5
05
0:5i
0:5
0:5i
3
775
2
664
1:3X102
0:040iX102
0:02X102
0:04iX102
3
775 D
2
664
66
68
66
60
3
775
1.24
Normal Matrices
The matrix A that satisﬁes the condition AHA D AAH is called Normal Matrix.
Hermitian matrix, Unitary matrix, Permutation matrix and circular matrices are
called Normal matrices. All the Normal matrices are diagonalizable.
Summary (see Fig.1.6 below)
1. For any matrix A of size mm, the Eigen vectors corresponding to distinct Eigen
values are independent. The Eigen vectors corresponding to the particular Eigen
value are independent, because it is the basis for the Null space of the matrix of
the form jŒAI. So in case of non-deﬁcient matrix A, the Eigen vectors forms
the basis for the vector space Rm.
2. All the Normal matrices are non-defective. (i.e.) Any Normal matrix A of size
mm can be represented as the product of UDUH, where U is the unitary matrix
and D is the Diagonal matrix. The Eigen vectors corresponding to various Eigen
values are orthogonal to each other. Hermitian Matrix, Unitary Matrix (Example
DFT Matrix), Permutation matrix and the Circular matrix are the examples for
the Normal matrices. All Normal matrices need not be invertible.

1.24 Normal Matrices
57
Fig. 1.6 Set of deﬁcient and non-deﬁcient matrices
1. Deﬁcient Matrices
2. Non-Deﬁcient Matrices
3. Invertible Matrices
4. Unitary Matrices
5. Permutation Matrices
6. Hermitian Matrices
7. Normal Matrices
3. The Eigen values of the Hermitian matrix A of size mm are real. Any arbitrary
Hermitian matrix can be represented as the product of UDUH, where U is the
unitary matrix and D is the Diagonal matrix. The Eigen vectors of the Hermitian
matrix ‘A’ forms the orthogonal basis of the vector space Rm. Hermitian matrix
need not be invertible.
4. The magnitude of the Eigen values of the Unitary matrix A of size m  m is 1.
The value can be complex. This matrix can also be represented always as UDUH.
where U is the unitary matrix and D is the Diagonal matrix. The Eigen vectors of
the Unitary matrix ‘A’ forms the orthogonal basis of the vector space Rm. Unitary
matrix is always invertible.
5. DFT matrix is the example for the Unitary matrix which is the type Normal
matrix. DFT matrix is always diagonalizable (i.e.) non-deﬁcient. The DFT matrix
A of size mm can be represented as UDUH, where U is the unitary matrix and
D is the Diagonal matrix. The Eigen values of the matrix A hold any of the
following values i, I, 1, 1. DFT matrix is the Unitary matrix and is always
invertible. The Eigen vectors of the DFT matrix ‘A’ forms the orthogonal basis
of the vector space Rm.
6. Permutation matrix is the example for the unitary matrix, which is the type of the
Normal matrix and hence diagonalizable (i.e.) non-deﬁcient. The permutation
matrix A of size m  m can be represented as UDUH. The magnitude of the
Eigen values of the permutation matrix is 1. Permutation matrix, which is the
unitary matrix, is always invertible. The Eigen vectors of the Permutation matrix
‘A’ of size m  m forms the orthogonal basis of the vector space Rm.
7. Circular matrix is the type of Normal matrix and hence diagonalizable (i.e.) non-
deﬁcient. The circular matrix A of size m  m can be represented as U H , where

58
1 Matrices
U is the unitary matrix and D is the Diagonal matrix. Circular matrix need not be
invertible matrix. The Eigen vectors of the Circular matrix ‘A’ forms the orthog-
onal basis of the vector space Rm.
1.25
Applications of Diagonalization of the Non-deﬁcient
Matrix
(a) Solving the Difference Equation of the form UkC1 D AUk, where
UkC1 D
2
666664
x1.k C 1/
x2.k C 2/
x3.k C 3/
: : :
xn.k C 1/
3
777775
Uk D
2
666664
x1.k/
x2.k/
x3.k/
: : :
xn.k/
3
777775
Where ‘A’ is the non-deﬁcient n  n matrix.
Example 1.25. Let the non-deﬁcient 2  2 matrix be A D
1 1
1 0

.
UkC1 D
x1.k C 1/
x2.k C 2/

and Uk D
x1.k/
x2.k/

Solving UkC1 D AUk is equivalent to solving the equation Uk D AkU0.
Note that the matrix A is the unitary matrix. Therefore the matrix A can be rep-
resented as A D ED EH (as given below), where E is the Eigen matrix, in which
the columns are the Eigen vectors.
1 1
1 0

D
 0:5257
0:8507
0:8507 0:5257
 0:6180
0
0
1:6180
  0:5257
0:8507
0:8507 0:5257

) ŒAk D EDEHEDEH EDEHEDEH : : : .‘k0times/
Note that EEH is the Identity matrix and hence
ŒAk D EDDD : : : .k times/EH
) ŒAk D EDkEH
Also Dk D
.0:6180/k
0
0
.1:6180/k


1.25 Applications of Diagonalization of the Non-deﬁcient Matrix
59
Thus solution to the above difference equation is given as
Uk D AkU0 D EDkEHU0
D
 0:5257 0:8507
0:8507 0:5257
 .0:6180/k
0
0
.1:6180/k
  0:5257 0:8507
0:8507 0:5257

U0
If U0 D
1
0

Uk D
 0:5257 0:8507
0:8507 0:5257
 .0:6180/k
0
0
.1:6180/k
  0:5257 0:8507
0:8507 0:5257
 1
0

D
 0:5257
0:8507
0:8507 0:5257
 .0:6180/k
0
0
.1:6180/k
  0:5257
0:8507

D
 0:5257
0:8507
0:8507 0:5257
 .0:5257/.0:6180/k
.0:8507/.1:6180/k

D
 0:5257
0:8507
0:8507 0:5257
 .0:5257/.0:6180/k
.0:8507/.1:6180/k

D

Œ.0:5257/2.0:6180/k C .0:8507/2.1:6180/k
.0:5257/.0:8507/.0:6180/k C .0:5257/.0:8507/.1:6180/k

D
 Œ.0:2764/.0:6180/k C .0:7237/.1:6180/k
Œ.0:4472/.0:6180/k C .0:4472/.1:6180/k

(b) Solving Differential Equation of the form
dU.t/
dt
D A U.t/, where U.t/ D
2
664
u1.t/
u2.t/
: : :
un.t/
3
775 and A is the Non-deﬁcient matrix.
Solution for the above differential equation is of the following form
U.t/ D eAtU.0/
Let us use the non-deﬁcient matrix A D
1 1
1 0

and U.0/ D
1
0

To compute eAt, Diagonalization of the matrix ‘A’ is used.
eA D I C A
1Š C .A/2
2Š
C .A/3
3Š
C   
) eA D I C A
1Š C EDEHEDEH
2Š
C EDEHEDEH EDEH
3Š
C   
) eA D EIEH C EDEH
1Š
C EDDEH
2Š
C EDDDEH
3Š
C   

60
1 Matrices
) eA D E

I C D
1Š C DD
2Š C DDD
3Š
C   

EH
) eA D EeDEH
) eAt D EeDtEH
For the given problem the solution of the differential equation is given as follows.
U.t/ D eAtU.0/
) U.t/ D EeDtEHU.0/
) U.t/ D
 0:5257 0:8507
0:8507 0:5257

eDt
 0:5257 0:8507
0:8507 0:5257
H 1
0

where D D
0:6180
0
0
1:6180

) eDt D e
0:6180
0
0
1:6180

t D
e0:6180t
0
0
e1:6180t

) U.t/
D
 0:5257 0:8507
0:8507 0:5257
 e0:6180t
0
0
e1:6180t
  0:5257 0:8507
0:8507 0:5257
 1
0

D
 0:5257
0:8507
0:8507 0:5257
  0:5257e0:6180t
.0:8507/e1:6180t

D
 0:5257 0:8507
0:8507 0:5257
  0:5257e0:6180t
.0:8507/e1:6180t

D
 0:5257 0:8507
0:8507 0:5257
  0:5257e0:6180t
.0:8507/e1:6180t

D
 0:2764e0:6180t C 0:7234e1:6180t
0:4472e0:6180t C 0:4472e1:6180t

1.26
Singular Value Decomposition
Consider the matrix A of size m  n. The matrix A can be represented as the prod-
uct of Hermitian transpose of the unitary matrix U1, Diagonal matrix ‘D’ and the
unitary matrix ‘U2’. A D U1DU2H.
Let the unit magnitude Eigen vector ‘vi’ corresponding to the matrix AT A
satisﬁes the condition AT A vi D i vi. Multiplying A on both sides, we get
AAT .A vi/ D i.A vi/. ) .A vi/ is the Eigen vector of the matrix AAT . The

1.26 Singular Value Decomposition
61
corresponding Eigen value is i. The magnitude of the vector Avi is obtained as
.Avi/T .Avi/ D vi T AT Avi D ivi T vi D i. The Unit magnitude Eigen vector of
the matrix AAT can be represented as Avi=pi. Let it be ui
Rewriting the above equation as Avi D pi ui
Example 1.26. Consider the matrix A D
1 2 3
4 5 6

.
AAT D
14
32
32
77

The Eigen values of the matrix AAT are 0.5973, 90.4027 and the corresponding
Eigen vectors are given as
0:9224
0:3863

;
0:3863
0:9224

Similarly the Eigen values of the matrix AT A are 0, 0.5963, 90.4027 and the
corresponding Eigen vectors are given as
2
4
0:4082
0:8165
0:4082
3
5 ;
2
4
0:8060
0:1124
0:5812
3
5 and
2
4
0:4287
0:5663
0:7039
3
5 :
The Eigen vectors of the matrix AAT are obtained as following for the corre-
sponding non-zero Eigen values.
1 2 3
4 5 6
 2
4
0:8060
0:1124
0:5812
3
5
p
0:5963
D
 0:9224
0:3863

1 2 3
4 5 6
 2
4
0:4287
0:5663
0:7039
3
5
p
90:4027
D
0:3863
0:9224

Note that it is same as the one computed directly from the basic deﬁnition. Also note
that the Eigen vectors thus obtained are orthonormal to each other as the matrix AAT
and AT A are Hermitian matrix.
Thus the matrix
1 2 3
4 5 6
 2
4
0:8060 0:4287
0:1124
0:5663
0:5812
0:7039
3
5
D
 0:9224 0:3863
0:3863 0:9224
 p
0:5963
0
0
p
90:4027


62
1 Matrices
As the Eigen vectors are orthonormal, the matrix A can be represented as follows as
follows
)
1 2 3
4 5 6

D
 0:9224 0:3863
0:3863 0:9224
 p
0:5963
0
0
p
90:4027
 2
4
0:8060 0:4287
0:1124 0:5663
0:5812
0:7039
3
5
T
D
 0:9224 0:3863
0:3863 0:9224
 p
0:5963
0
0
p
90:4027
 2
4
0:8060 0:4287
0:1124 0:5663
0:5812
0:7039
3
5
T
We can even include the Eigen vector corresponding to the zero Eigen value of
the matrix AT A as shown below.
)
1 2 3
4 5 6

D
 0:9224 0:3863
0:3863 0:9224
 2
4
p
0:5963
0
0
0
p
90:4027 0
0
0
0
3
5
2
4
0:8060 0:4287 0:4082
0:1124 0:5663
0:8165
0:5812
0:7039 0:4082
3
5
T
The above method of representing the matrix A as the product of the Unitary matrix
‘U1’, diagonal matrix ‘D’ and the Unitary matrix ‘U20T are called Singular Value
Decomposition.
1.27
Applications of Singular Value Decomposition
1. Spectral factorization representation of the matrix is obtained using Singular
Value Decomposition as given below.
Example 1.27. From the Example 1.23, we get,
1
2
3
4
5
6

:
D
 0:9224 0:3863
0:3863 0:9224
 p
0:5963
0
0
p
90:4027
 2
4
0:8060 0:4287
0:1124
0:5663
0:5812
0:7039
3
5
T
D
 0:9224 0:3863
0:3863 0:9224
 p
0:5963
0
0
p
90:4027
 0:8060 0:1124 0:5812
0:4287
0:5663
0:7039


1.27 Applications of Singular Value Decomposition
63
D
p
0:5963
 0:9224
0:3863
 
0:8060 0:1124 0:5812
	
C
p
90:4027
0:3863
0:9224
 
0:4287 0:5663 0:7039
	
The above mentioned way of representing the matrix is called spectral factor-
ization. If the Eigen values are so small, we can neglect them in the above
representation and hence data compression is achieved.
2. Computation of Pseudo inverse of the non-invertible matrix
Case 1: When the matrix AT A is invertible
Consider the case of solving the equation of the form Ax D b when ATA is
invertible.
Let the equation be
2
66664
1
0
0
0
2
0
0
0
3
0
0
0
3
77775
2
664
x1
x2
x3
3
775 D
2
66664
5
6
4
4
3
77775
:
Multiplying AT on both sides, we get
2
66664
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0
0
3
77775
2
66664
1
0
0
0
2
0
0
0
3
0
0
0
3
77775
2
664
x1
x2
x3
3
775 D
2
66664
1
0
0
0
0
2
0
0
0
0
3
0
0
0
0
0
3
77775
2
66664
5
6
4
4
3
77775
)
2
664
1
0
0
0
4
0
0
0
9
3
775
2
664
x1
x2
x3
3
775 D
2
664
5
12
12
3
775
In this case x1 D 5; x2 D 3; x3 D 4
3
Note that the solution corresponds to the projected column vector
2
664
1
0
0
0
2
0
0
0
3
0
0
0
3
775
2
4
x1
x2
x3
3
5 D
2
664
5
6
4
0
3
775

64
1 Matrices
Also note that the matrix AT A is invertible and we have already shown that the
solution vector thus obtained is such that magnitude of the error vector
2
664
5
6
4
4
3
775 
2
664
5
6
4
0
3
775 D
2
664
0
0
0
4
3
775 is minimized.
Case 2: When the matrix AT A is non-invertible
Consider the case of solving the equation of the form Ax D b, where AT A is
non-invertible.
Let the equation be
2
664
1
0
0
0
0
2
0
0
0
0
3
0
3
775
2
66664
x1
x2
x3
x4
3
77775
D
2
664
5
6
4
3
775 :
In this case A0A D
2
664
1
0 0
0
0
4 0
0
0
0 9
0
0
0 0
0
3
775 is not invertible and hence projection method
as used above cannot be used in this case.
By direct observation of the set of equations, the solution is obtained as follows
x1 D 5
x2 D 3
x2 D 4=3
The variable x4 is arbitrarily chosen as 0 to reduce the length of the vector
2
664
x1
x2
x3
x4
3
775.
Thus the solution obtained above is of shortest length.
In both the cases the pseudo inverse matrix of the diagonal matrix A which is
represented as AC is obtained as follows.
For the Case 1 AC D
1
0
0
0
0
1
2
0
0
0
0
1
3
0

1.27 Applications of Singular Value Decomposition
65
The solution for the equation Ax D b, is obtained as x D ACb
) x D
2
4
1
0
0
0
0
1
2
0
0
0
0
1
3
0
3
5
2
4
5
6
4
3
5 D
2
4
5
3
4=3
3
5
Similarly for the case 2,
AC D
1
0
0
0
1
2
0
0
0
1
3
0
0
0
The solution for the equation Ax D b, is obtained as x D ACb
) x D
2
664
1
0
0
0
1
2
0
0
0
1
3
0
0
0
3
775
2
4
5
6
4
3
5 D
2
664
5
3
4=3
0
3
775
Thus if the matrix A is the diagonal matrix the inverse of the matrix can be
obtained by inverting the non-zero diagonal elements of the matrix A with zero
elements unchanged.
If the matrix A is not the diagonal matrix, then SVD is used to represent the
matrix as A D U1DU2H and obtain the vector x such that kAxbk is minimized.
) we have to obtain the value for the vector x such that kU1DU2Hx  bk is
minimized.
) kDU2H x  U1H bk is minimized. [Multiplying Unitary matrix on both
sides of the linear equation]
Note:
Multiplying with the unitary matrix on both sides of the linear equation will
not affect the distance as described below
In general kAx  bk2 D .Ax  b/H.Ax  b/
Multiplying the Unitary matrix U on both sides we get,
kUAx  Ubk2 D .UAx  Ub/H.UAx  Ub/ D U
D xHAHUHUAx  xHAHUHUb  bHUHUb
 bHUHUb
D xHAHAx  xHAHb  bHb  bHb
D .Ax  b/H.Ax  b/ D kAx  bk2

66
1 Matrices
Let y D U2H x and the modiﬁed task is to minimize the norm kDy  U1Hbk,
where ‘D’ is the diagonal matrix and hence solution for y is obtained as
y D DCU1Hb
) U2
Hx D DCU1
Hb
) x D U2DCU1Hb
Thus for any arbitrary matrix A, we can obtain inverse (If the matrix is invert-
ible) or pseudo inverse (If the matrix is not invertible) using the technique as
mentioned above.
Example 1.28.
A D
1
2
3
4
5
6

The inverse of the matrix A is obtained as follows.
Using SVD we get,
"
1
2
3
4
5
6
#
D
"
0:9224
0:3863
0:3863
0:9224
# 2
64
p
0:5963
0
0
0
p
90:4027
0
0
0
0
3
75
2
64
0:8060
0:4287
0:4082
0:1124
0:5663
0:8165
0:5812
0:7039
0:4082
3
75
T
AC
D
"
0:9224
0:3863
0:3863
0:9224
# 2
64
1
p
0:5963
0
0
0
1
p
90:4027
0
0
0
0
3
75
2
64
0:8060
0:4287
0:4082
0:1124
0:5663
0:8165
0:5812
0:7039
0:4082
3
75
T
AC
D
"
0:9224
0:3863
0:3863
0:9224
# 2
64
1
p
0:5963
0
0
0
1
p
90:4027
0
0
0
0
3
75
2
64
0:8060
0:4287
0:4082
0:1124
0:5663
0:8165
0:5812
0:7039
0:4082
3
75
T
3. Representing the matrix A as the product of Unitary matrix and the symmetric
matrix as shown below.
Using SVD, A D U1DU2H
Inserting U2H U2 in the middle we get
A D U1U2
H U2DU2
H
Note that the matrix U1U2H is the unitary matrix and the matrix U2DU2H is the
symmetric matrix.

Chapter 2
Probability
2.1
Introduction
1. Set: It is collection of well deﬁned objects. Each object is referred as an element
2. The set B which is the subset of A (Represented as B  A), is a set whose
element are also the elements of A
3. Set of no elements are called Empty set
4. Set operations:
 A union B (Represented as A [ B or A C B) is the set that consists of the
elements which are either in A or B or in both.
 A intersection B (Represented as A \ B or AB) is the set of elements which
are in both A and B.
 A complement (Represented as NA or Ac) is the set that consists of the elements
that are not present in the set A.
5. Mutually exclusive sets (Disjoint sets): Two sets A and B are disjoint, if they
have no elements in common i.e. AB D . In general The sets A1,A2,A3,: : : An
are disjoint if Ai Aj D  for i ¤ j
6. Sample space: Set of all experimental outcomes
7. Partition: A partition of a set is the collection of mutually exclusive sets
A1, A2 : : :whose union is the sample space ‘S’
(i.e.) If A1 C A2 C A3 C : : : An D S and Ai Aj D  for i ¤ j, then Ai’s form a
partition of the set
8. Event: Subset of the sample space
Certain event: S (Sample space)
Impossible event:  (Null space)
Elementary event: One outcome of the experiment.
9. Countable inﬁnite set: The set C is countably inﬁnite when there is one-to-one
correspondence between the elements of the set C and a set of all non-negative
numbers
E.S. Gopi, Mathematical Summary for Digital Signal Processing
Applications with Matlab, DOI 10.1007/978-90-481-3747-3 2,
c Springer Science+Business Media B.V. 2010
67

68
2 Probability
2.2
Axioms of Probability
1. Probability of the event Ÿ represented as P.Ÿ/ 	 0.
2. P.S/ D 1, where ‘S’ is the sample space.
3. If A1,A2,: : : An are such that Ai Aj D  for i ¤ j then
P

X1
iD1 Ai

D
X1
iD1 P.Ai/:
Note: If sample space is having ﬁnite number or inﬁnitely countable number of
subsets, probabilities can be assigned to all the subsets of the sample space that
satisﬁes all the axioms mentioned in 2.2.
If sample space is having uncountable inﬁnite number of subsets, it is not
possible to assign the probability to all the subsets of the sample space that sat-
isﬁes all the axioms mentioned in 2.2.
2.3
Class of Events or Field (F)
Class of Events is the subset of the sample space satisfying the following properties.
1. If A  F, then NA  F.
2. If A; B 2 F, then A C B 2 F.
3. Sigma Field: If A1; A2; : : : 2 F; P1
iD1 Ai 2 F.
2.4
Probability Space (S, F, P)
The probability space (S, F, P) consists of Sample space S, Field F and the proba-
bility measure P. The probability measure maps every element of F to a number less
than 1 and greater than 0. The measured value is the probability.
2.5
Probability Measure
The probability of the event A can be measured as follows.
Method 1:
Probabilities of elementary events are assumed as equal. Let the number of out-
comes belonging to the event A is NA and the total number of outcomes is N, then
P.A/ D NA=N . The probability measured in this technique depends upon how the
set of possible outcomes are deﬁned.

2.6 Conditional Probability
69
Method 2:
Repeat the experiment N times. Let nA is the number of times event A occurs,
then P.A/ D limN!1
nA
N
2.6
Conditional Probability
Case 1 (Fig. 2.1):
Let ‘nA’ and ‘nB’ be the number of outcomes belonging to the event A and B
respectively. Also let ‘n’ be the total number of outcomes.
Probability of the event A is nA/n
Probability of the event B is nB/n
Probability of the event A given B has occurred is given as follows
P.A=B/ D nA=nB D nA
n
 nB
n D P.A/=P.B/ D P.AB/=P.B/
Case 2 (Fig. 2.2):
In this case P(A/B) D Probability of A given B has occurred is 1. This can also
be obtained using the formula obtained in the case 1. P .A=B/ D P .AB/=P.B/ D
P.B/=P.B/ D 1
Therefore P.A=B/ D P.AB/=P.B/ is considered as the common formula that
can be used in both the cases.
nA
nB
n
Fig. 2.1 Conditional probability – case 1
nB
nA
n
Fig. 2.2 Conditional Probability – case 2

70
2 Probability
2.7
Total Probability Theorem
Let A1, A2 : : : An be the partition of the sample space S.
B D BS D B.A1 C A2 C A3 C : : : An/ D A1B C A2B C A3B C : : : AnB
P.B/ D
Xn
iD1 P.AiB/
D
n
X
iD1
P
 B
Ai

P.Ai/
Total Probability Theorm W P.B/ D
n
X
iD1
P
 B
Ai

P .Ai/
Note that Ai Aj D  for i ¤ j
2.8
Bayes Theorem
P.A=B/ D P.AB/=P.B/
P.B=A/ D P.AB/=P.A/
P.B/P.A=B/ D P.B=A/P.A/
) P.B/ D P.A/P.B=A/=P.A=B/
Bayes Theorem W P.B/ D P.A/ P
B
A

P
 A
B

Let A1, A2, : : : An forms the partition of the sample space S.
In general P.Ai=B/ D
P.Ai/P. B
Ai /
Pn
iD1 P. B
Ai /P.Ai/
Note that Ai Aj D  for i ¤ j
2.9
Independence
1. A and B are independent if P.AB/ D P.A/P.B/. Also P.A=B/ D P.A/
2. Three events A, B and C are independent if it satisﬁes the following conditions
P.AB/ D P.A/P.B/
P.AC/ D P.A/P.C/
P.BC/ D P.B/P.C/
P.ABC/ D P.A/P.B/P.C/

2.10 Multiple Experiments (Combined Experiments)
71
P.AB=C/ D P.ABC/=P.C/
P.A=BC/ D P.ABC/
P.BC/ D P.A/
2.10
Multiple Experiments (Combined Experiments)
1. Consider the sample spaces S1 and S2 corresponding to the two independent
experiments.
fS1g D f1; 2; 3; 4; 5; 6g
fS2g D fh; tg
Sample space of the combined experiments is represented as S D S1XS2 D
f.a; b/; a 2 S1; b 2 S2g
2. Consider the event A D f2; 3g 2 S1 and the event B D fhg 2 S2.
The eventfAXS2g D f.2; h/.3; h/.2; t/.3; t/g
The eventfS1XBg D f.1; h/.2; h/.3; h/.4; h/.5; h/.6; h/g
fAXBg D f.2; h/.3; h/gwhich can be obtained using the following:
fAXBg D fAXS2g \ fS1XBg
P.AXB/ D P.AXS2/P.S1XB/
Some properties of the probability derived from the Axioms of the probability
1. P. NA/ D 1  P.A/
Proof. A C NA D S (A and NA are Mutually Exclusive Events)
P.S/ D P.A C NA/ D 1 ŒSecond Axiom
D P.A/ C P. NA/ ŒThird Axiom
) P.A/ D 1  P. NA/
2. P.A/  1
Proof. P.A/ D 1  P. NA/ [From proof 1]
P. NA/ 	 0 and P.A/ 	 0 ŒFirst Axiom
) P.A/  1
3. P.˚/ D 0
Proof. The Event A D A C ˚ [A and ˚ are Mutually Exclusive Events]
P.A/ D P.A/ C P.˚/ ŒThird Axiom
) P.˚/ D 0

72
2 Probability
4. fAign
iD1 is a partition of the sample space S (Fig. 2.3). For any event
B; P.B/ D Pn
iD1 P.B Ai/
Proof. From the diagram, it can be shown that any event ‘B’ can be represented
as the union of disjoint events Sn
iD1 B Ai
.i:e:/ B D [n
iD1BAi
P.B/ D P.[n
iD1B Ai/ D P.BA1 C BA2 C BA3 C BA4 C    BAn/
D P.BA1/ C P.BA2/ C : : : P.BAn/ D
Xn
iD1 P.BAi/
5. If A  B; P.A/  P.B/ (Fig. 2.4)
Proof. A D B C NBA (Note that B and NBA are disjoint events)
)P.A/ D P.B/ C P. NBA/
Also 1 	 P. NBA/ 	 0 and hence P.A/ 	 P. NBA/
6. P.ACBCC/ D P.A/CP.B/CP.C/P.AB/P.AC/ D P.BC/CP.ABC/
(Fig. 2.5)
Fig. 2.3 Partition of the
sample space S
Fig. 2.4 Venn diagram
illustrating A  B
A
B

2.10 Multiple Experiments (Combined Experiments)
73
Fig. 2.5 Venn diagram
illustrating A C B C C
A
B
C
Proof. A C B C C can be represented as the summation of three disjoint events
as given below.
A C B C C D A C NAB C .A C B/C (Disjoint Events)
) A C B C C D A C .1  A/B C NA NBC
) P.A C B C C/ D P.A/ C P..1  A/B/ C P. NA NBC/
On simpliﬁcation
A C B C C D A C .1  A/B C .1  A/.1  B/C .Disjoint Events/
) A C B C C D A C B  AB C .1  B  A C AB/C
) A C B C C D A C B  AB C C  BC  AC C ABC
) P.A C B C C/ D P.A/ C P.B/  P.AB/ C P.C/  P.BC/
P.AC/ C P.ABC/
) P.A C B C C/  P.A/ C P.B/ C P.C/
7. In general P

fAign
iD1

 Pn
iD1 P.Ai/
(It can be proved using Mathematical Induction)
8. If two events A and B are independent, NA and B are independent
Proof. Given: P.AB/ D P.A/P.B/
NAB D B  AB
P. NAB/ D P.B/  P.AB/
) P.B/  P.A/P.B/
) P.B/.1  P.A//
) P.B/P. NA/

74
2 Probability
Thus NA and B are also independent.
) NA and NB are independent (property 8)
) A and NB are independent (property 8)
9. If three events A, B and C are independent, the events A and B C C are
independent.
Proof. Given: P.ABC/ D P.A/P.B/P.C/
P.A.B C C// D P.AB C AC/ D P.AB/ C P.AC/  P.ABC/
D P.A/P.B/CP.A/P.C/P.A/P.B/P.C/
D P.A/.P.B/ C P.C/  P.BC//
D P.A/P.B C C/
) A and B C C are independent
10. If A, B and C are three events, P.AB=C/ D P.A=BC/P.B=C/.
Proof.
P.AB=C/ D P.ABC/=P.C/
D P.A=BC/P.BC/=P.C/
D P.A=BC/P.B=C/
11. If A, B and C are three events, P(ABC) D P(A/BC)P(B/C)P(C).
Proof.
P.ABC/ D P.A=BC/P.BC/
D P.A=BC/P.B=C/P.C/
2.11
Random Variable
Consider the probability space (S,F,P), then the mapping of the outcomes s 2 F to
the real line is called random variable. (i.e.) X: F > R (Fig. 2.6).
Mapping must be chosen such that every subset of the real line of the form
(1, a] should be an event in F. The subset of the real line of the form (a, b] is
called Borel set represented as B.
Let A 2 B: PX.A/ D P.fs 2 S W X.s/ 2 Ag/, where fs 2 S W X.s/ 2 Ag is called
inverse image of A. Thus the random variable maps the probability space (S,F,P) to
the Borel space (R, B, Px)

2.12 Cumulative Distribution Function (cdf) of the Random Variable ‘x’
75
Fig. 2.6 Illustration
of the random variable
(R,B,Px)
a
R
S
(S,F,P)
x
−
2.12
Cumulative Distribution Function (cdf) of the Random
Variable ‘x’
FX.˛/ D PX.X  ˛/
1. 0  FX.˛/  1 for all ˛
2. lim˛!1 FX.˛/ D 1
3. lim˛!1 FX.˛/ D 0
4. FX.˛/ is the non decreasing function. (i.e.) If ˛1 < ˛2, then FX.˛1/  FX.˛2/
5. If FX.˛0/ D 0, then FX.˛/ D 0 for all ˛ < ˛0
6. PX.a  X  b/ D FX.b/  FX.a/
7. If X and Y are two random variable such that X.s/  Y.s/ for all s 2 S, then
FX.a/ 	 FY .a/ for all ˛ (Fig. 2.7)
Proof.
FX.a/ D P.X  a/
FY .b/ D P.Y  b/
By deﬁnition
FX.a/ D FY .b/
D P.Y  a/ C P.a  Y  b/
D FY .a/ C K
The constant K ranges from 0 to 1 and it is the positive quantity and hence FX.a/ 	
FY .a/ for all values of ‘a’

76
2 Probability
b
a
s
S
Y
X
F (y)
Y
F (x)
X
Fig. 2.7 Illustration of the property 7 of the CDF
2.13
Continuous Random Variable
A random variable X is continuous if FX.˛/ is the continuous function of ˛.
2.14
Discrete Random Variable
A random variable X is discrete if and only if it maps S to a countable subset of R
(Real numbers).
2.15
Probability Mass Function
If the random variable is discrete, probability of the particular value of the random
variable can be computed using the function known as probability mass function.
Probability of the random variable X D ˛ is represented as P.X D ˛/.
2.16
Probability Density Function
If the random variable is continuous, probability of the random variable X over the
smallest range ˛ to ˛ C ˛ is computed as follows.
lim
˛!0 ŒFX.˛ C ˛/  FX.˛/

2.17 Two Random Variables
77
Deﬁne fx.˛/˛ be the probability of the random variable x over the smallest range
˛ to ˛ C ˛
) fx.˛/˛ D lim˛!0ŒFX.˛ C ˛/  FX.˛/
) fx.˛/ D lim˛!0 ŒFX.˛ C ˛/  FX.˛/ =˛
The function fx.˛/ is called probability density function.
fx.˛/
lim
˛!0 ŒFX.˛ C ˛/  FX.˛/
˛
) fx.˛/ D dFX.˛/
dx
Properties:
1. fx.˛/ 	 0 for all ˛. F [Because F is the non-decreasing function]
2. FX.˛/ D R ˛
1 fx.˛/ d˛
3.
R 1
1 fx.x/dx D 1
4. R x2
x1 fx.x/dx D FX.x2/  FX.x1/
2.17
Two Random Variables
Consider the probability space (S, F, P), then the mapping of the outcome s 2 F
to the real line is called random variable. This mapping can be done in multiple
forms and are called as Multiple random variables. In particular if the mapping is
done in two different forms, then the corresponding mapping is called two random
variables as shown in the Fig. 2.8. Two random variables are completely described
by the Joint distribution function PŒX  x; Y  y and it is usually represented as
FXY.x; y/.
Properties of joint distribution function and joint density function with two
random variables.
1. limx!1 FXY.x; y/ D P.X  1; Y  y/ D FY.y/.
2. limy!1 FXY.x; y/ D P.X  x; Y  1/ D FX.x/.
3. limY !1 FXY.x; y/ D 0
4. limX!1 FXY.x; y/ D 0
5. P.x1  X  x2; Y  1/ D FXY.x2; y/  FXY.x1; y/.
6. P.x1  X  x2; y1  Y  y2/ D FXY.x2; y2/  FXY.x2; y1/  FXY.x1; y2/ C
FXY.x1; y1/
7. 0  FXY.x; y/  1
8. It is the non-decreasing function with both ‘x’ and ‘y’

78
2 Probability
Fig. 2.8 Illustration of the
two random variables
(R,B,Px)
a
b
R
S
(S,F,P)
x
y
−
(R,B,Py)
9. The relationship between the Joint probability density function of the two ran-
dom variables ‘X’ and ‘Y’ and the corresponding joint distribution function is
given below
fXY .x; y/ D @2FXY.x; y/
@x@y
FXY.a; b/ D
Z b
1
Z a
1
fXY .x; y/dx dy
10. Probability of the event A corresponding to the random variable X which ranges
from xmin to xmax and the random variable Y which ranges from ymin to ymax
is computed using the joint density function as given below
P.A/
xmax
Z
xmin
ymax
Z
ymin
fXY.x; y/dx dy
11. The marginal probability density function of the random variable ‘X’ is given
as (Fig. 2.9)
fX.x/ D
1
Z
1
fXY .x; y/ dy
FY .y/ D
Z 1
1
fXY .x; y/ dx

2.19 Independent Random Variables
79
P(x1≤x≤x2,Y≤ ) 
x1
x2
y2
y1
x
x
x
x
y
Y
Y
x
x
y
Fy(y)
Fx(x)
P(x≤, y1≤Y≤ y2) 
Fig. 2.9 Illustration of the Marginal probability density function
2.18
Conditional Distributions and Densities
The conditional distribution of ‘x’ over the event ‘B’ represented as
FX=B.x/ D P.X  x=B/ D P.X  x; B/
P.B/
2.19
Independent Random Variables
Two events A and B are called independent if;
P.AB/ D P.A/P.B/
Then we can write, FXY.x; y/ D P.X  x; Y  y/
D P.X  x/.Y  y/
D FX.x/FY .y/
So, fXY.x; y/ D fX.x/fY .y/

80
2 Probability
2.20
Some Important Results on Conditional Density Function
1. ‘X’ is continuous, ‘Y’ is discrete, then FX=Y Dy.x/ is computed as described
below
FX=Y Dy.x/ D P.X  x=Y D y/
D P.X  x; Y D y/
P.Y D y/
P.X  x/ D
X
Y P.X  x; Y D y/
) P.X  x/ D
X
Y FX=Y Dy.x/P.Y D y/
) FX.x/ D
X
Y FX=Y Dy.x/P.Y D y/
) fX.x/ D
X
Y fX=Y Dy.x/P.Y D y/
2. ‘X’ is continuous and ‘Y’ is continuous, then FX=Y Dy.x/ is computed as follows
FX=Y Dy.x/ D P.X  x=Y D y/
D lim
y!0 P.X  x=y  Y  y C y/
D lim
y!0
P.X  x; y  Y  y C y/
P.y  Y  y C y/
D lim
y!0
P.y  Y  y C y=X  x/P.X  x/
P.y  Y  y C y/
D lim
y!0
P.y  Y  y C y=X  x/P.X  x/
P.y  Y  y C y/
D lim
y!0
.FY=Xx.y C y/  FY=Xx.y//P.X  x/
FY .y C y/  FY .y C y/
D .fY=Xx.y//P.X  x/
fY .y/
) P.X  x=Y D y/ D .fY=Xx.y/P.X  x//
fY .y/
Also P.X  x=Y D y/fY .y/ D fY=Xx.y/P.X  x/
)
Z
P.X  x=Y D y/fY .y/dy
D
Z
fY=Xx.y/P.X  x/dy
D P.X  x/

2.20 Some Important Results on Conditional Density Function
81
Also consider
P.X  x=Y D y/ D lim
y!0
P.X  x; y  Y  y C y/
P.y  Y  y C y/
) FX=Y Dy.x/ D lim
y!0
FXY .x; y C y/  FXY .x; y/
FY .y/  FY .y/
) FX=Y Dy.x/ D lim
y!0
ŒFXY .x; y C y/  FXY .x; y/ =y
ŒFY .y C y/  FY .y/=y
) FX=Y Dy.x/ D
dFXY .x;y/
dy
fY .y/
) fX=Y Dy.x/ D
@2FXY .x;y/
@x@y
fY .y/
) fX=Y Dy.x/ D fXY .x; y/
fY .y/
)
Z
fX=Y Dy.x/fY .y/dy D fX.x/
) fX.x/ D
Z
fX=Y Dy.x/fY .y/dy
Example 2.1. Let FXY.x; y/ D
1
x 2 Œ2; 1 \ y 2 Œ3; 1
1=2 x 2 Œ0; 2 \ y 2 Œ3; 1
1=2 x 2 Œ2; 1 \ y 2 Œ0; 3
1=4
x 2 Œ0; 2 \ y 2 Œ0; 3
0
Else where
Y
.11/
1/2
1
(2,3)
1/4
1/2
(0,0)
x
For the above speciﬁcation FX.x/ and FY .y/ are computed as follows (Fig. 2.10)
FX.x/ D P.X  x; Y  1/ and FY .y/ D P.X  1; Y  y/
Example 2.2. Let the event ‘B’ be X  b
Then,
FX=Xb.x/ D P.X  x=X  b/ D P.X  x; X  b/
P.X  b/
) FX=Xb.x/ D 1
for X 	 b
D FX.x/
FX.b/ for X < b
Conditional Probability density function can be obtained by differentiating condi-
tional distribution function.

82
2 Probability
2
1/2
1/2
FX(x)
FY(y)
3
X
X
Fig. 2.10 FX.x/ and FY .y/ of the Example 2.1
Fig. 2.11 Additive noise
model of the channel
transmission
X
Y
X
<
g(  .  )
N
.i:e/ fX=B.x/ D dFX=B.x/
dx
) fX=Xb.x/ D 0
for X 	 b
D fX.x/
FX.b/
for X < b
Example 2.3. Consider the signal (random variable X) which holds the values 1
or 1 with known probability. Consider that signal that is corrupted by the additive
noise signal (random variable N) to obtain the output signal (random variable Y).
The probability density function of the random variable N is known to be Gaussian
with mean D 0 (Fig. 2.11).
The corrupted signal Y is processed using the transformation function g (.) to es-
timate the value of the X. Consider the task of obtaining the transformation function
g(.) so that the probability of correct decision is maximized.
P(Correct decision) D
Z
Y
P..correct decision/=Y D y/fY .y/dy
(See Section 2.20)
Let us redeﬁne the problem as
max : P..correct decision/=Y D y// 8 y
Suppose X D 1 is sent then,
P.X D 1=Y D y/ > P.X D 1=Y D y/

2.20 Some Important Results on Conditional Density Function
83
f Y
X D1.y/
fY.y/ P.X D 1/ >
f Y
X D1.y/
fY.y/
P.X D 1/
) fY=XD1.y/
fY=XD1.y/ > P.X D 1/
P.X D 1/
We have,
FY=XD1.y/ D P.Y  y=X D 1/
D P.X C N  y=X D 1/
D P.1 C N  y/
D P.N  y  1/
D FN .y  1/
Differentiating both with respect to y,
@.FY=XD1.y/
@y
D @FN .y  1/
@y
fY=XD1.y/ D fN .y  1/
So,
fY=XD1.y/ D fN .y  1/ and;
fY=XD1.y/ D fN .y C 1/
Then,
fN .y  1/
fN .y C 1/ > P.X D 1/
P.X D 1/
If fN .n/ is a Gaussian density function with  D 0
fN .n/ D
1
p
2	 
2 e.n/=22
Then,
fN .y  1/ D
1
p
2	 
2 e .y1/2
22
fN .y C 1/ D
1
p
2	 
2 e .yC1/2
22 :
Now,
FN .y  1/
fN .y C 1/ > P.X D 1/
P.X D 1/

84
2 Probability
e .y1/2
22 e
.yC1/2
22
> P.X D 1/
P.X D 1/
e
4y
22 > P.X D 1/
P.X D 1/
4y
2
2 > lnP.X D 1/
P.X D 1/
y > 
2
2 lnP.X D 1/
P.X D 1/
Thus the transformation function g(.) is obtained as follows.
If y > 
2
2 lnP.X D 1/
P.X D 1/ Decide OX D 1; otherside decide OX D 1
2.21
Transformation of Random Variables of the Type
Y D g.X/
Consider the random variable ‘X’ which is transformed into another random vari-
able ‘Y’ using the transformation function deﬁned as Y D g.X/. The graph relating
the Y and X is as shown in the ﬁgure given below (Fig. 2.12).
Solving Y D g.X/ for the general variable ‘y1’ gives x1, x4, x5. (i.e.) g.x1/ D
g.x4/ D g.x5/ D y1. Also note that they belongs to the region where there are
piece-wise monotonically increasing or decreasing function.
y
y2
y1
x1
x3
x5 x6
x
 x4
x2
Δx1
Δx4
Δx5
Δy
Fig. 2.12 Transformation of random variables-case 1

2.22 Transformation of Random Variables of the Type Y1 D g1.X1; X2/; Y2 D g2.X1; X2) 85
From the basics of probability theory
P.y1  y  y2/ D P.x1  x  x2/ C P.x3  x  x4/ C P.x5  x  x6/
y fY .y/ D x1 fX.x/ at x D x1 C x4 fX.x/
D x4 C x5 fX.x/ at x D x5
Note that magnitude of x1; x4 and x5 are considered to compute fY .y/
Therefore; fY .y/ D fX.x1/
ˇˇˇˇ
x1
y
ˇˇˇˇ C fX.x4/
ˇˇˇˇ
x4
y
ˇˇˇˇ C fX.x5/
ˇˇˇˇ
x5
y
ˇˇˇˇ
) fY .y/ D
fX.x1/
ˇˇˇ dy
dx
ˇˇˇ at x D x1
C
fX.x4/
ˇˇˇ dy
dx
ˇˇˇ at x D x4
C
fX.x5/
ˇˇˇ dy
dx
ˇˇˇ at x D x5
In general, probability density function of y (i.e.) fY .y/ is obtained as follows.
Given fX.x/ and the transformation Y D g.X/.
1. Obtain the solutions for the equation Y D g.x/ for the general variable ‘y’, so
that x1; x2; : : : xn are obtained. Note that x1; x2; : : : xn are represented in terms
of ‘y’.
2. fY .y/ D Pn
iD1
fX .xi/
ˇˇˇ dy
dx
ˇˇˇat xDxi
3. The range of ‘Y’ can be obtained from the range of ‘X’ and the transformation
equation Y D g.X/.
2.22
Transformation of Random Variables of the Type
Y1 D g1.X1; X2/; Y2 D g2.X1; X2)
Given fX1X2.x1; x2/; Y1 D g1.X1; X2/; Y 2 D g2.X1; X2/; fY1Y 2.y1; y2/ is
obtained as follows
Also note that the functions g1(.) and g2(.) are invertible. (i.e.) There exists the
function h1(.) and h2(.) such that X1 D h1(Y1,Y2) and X2 D h2(Y1,Y2).
Consider the rectangle obtained from the intersection of lines Y1 D y1;
Y1 D y1 C y1 and Y2 D y2; Y2 D y2 C y2 in the Y1–Y2 plane. The
obtained rectangular region is represented as black shade in the Y1–Y2 plane.
Also, consider the curve ‘a’ in the X1–X2 plane which is obtained by joining the
set of points .X1 D x1; X2 D x2/ which satisﬁes the equation g1.x1; x2/ D y1.
Similarly curve ‘b’, curve ‘c’ and curve ‘d’ are obtained by joining the set of points
.X1 D x1; Y1 D y1/ which satisﬁes the equation g1.x1; x2/ D y1 C y1;
g2.x1; x2/ D y2; g2.x1; x2/ D y2 C y2 respectively. The shaded region rep-
resented in the X1–X2 plane (see Fig. 2.13)) is obtained as the intersection of
the curves ‘a’, ‘b’, ‘c’ and ‘d’. This shaded region can be approximated as the
parallelogram.

86
2 Probability
Y2
X2
y1 + Δy1
a: g1(x1,x2)=y1
b: g1(x1,x2)=y1+Δy1
c: g2(x1,x2)=y2
d: g2(x1,x2)=y2+Δy2
y1
Y1
X1
y2
a
b
d
c
y2 + Δy2
Fig. 2.13 Transformation of random variable-Case 2
From the basics of probability theory, the probability belonging to the shaded
region in the Y1–Y2 plane is equal to the probability belonging to the shaded
region in the X1–X2 plane. Therefore, fY1Y 2.y1; y2/ [Area of the rectangle]
D fX1X2.x1; x2/ [Area of the area of the parallelogram]
Consider the curve a: g1.x1; x2/ D y1. The set of points on this curve satisﬁes
the equation x1 D h1.y1; y2/ and x2 D h2.y1; y2/. Note that ‘y1’ is constant.
Hence the points on this curve ‘a’ can be rewritten as x1 D h1.y2/ and x2 D h2.y2/
for the ﬁxed Y1 D y1.
Similarly for the curve b: The invertible equations for the equation g1.x1; x2/ D
y1 C y1 can be written as x1 D h1.y2/ and x2 D h2.y2/ for the ﬁxed
Y1 D y1 C y1.
For the curve ‘c’: The invertible equations for the equation g2.x1; x2/ D y2 can
be written as x1 D h1.y1/ and x2 D h2.y1/ for the ﬁxed Y2 D y2.
For the curve ‘d’: The invertible equations for the equation g2.x1; x2/ D y2 C
y2 can be written as x1 D h1.y1/ and x2 D h1.y1/ for the ﬁxed Y2 D y2 Cy2.
Thus the rectangle with the co-ordinates mentioned and the corresponding paral-
lelogram with the co-ordinates marked is as shown in the Fig. 2.14.
Point 1 (p1) on the curve ‘b’ can be obtained as x1C (rate of change of h1 with
respect to y2)(dy2) for the X1 co-ordinate and x2C (rate of change of h2 with re-
spect to y2)dy2.
Point 2 (p2) on the curve ‘c’ can be obtained as x1C (rate of change of h1 with re-
spect to y1)(dy1) for the X1 co-ordinate and x2C (rate of change of h2 with respect
to y1)dy1.

2.22 Transformation of Random Variables of the Type Y1 D g1.X1; X2/; Y2 D g2.X1; X2) 87
(y1+dy1,y2)
(y1,y2)
(y1+dy1,y2+dy2)
curve c
p2
p1: 
x1+
x2+
dy2
dy2
h1
(x1,x2)
curve b
p1
curve a
curve d
(y1,y2+dy2)
y2
h2
y2
p2: 
x1+
x2+
dy1
dy1
h1
y2
h2
y1
Fig. 2.14 Rectangle to Parallelogram mapping for the transformation of random variable
Thus the co-ordinates for the points ‘p1’ and ‘p2’ are obtained as follows.
P1 W

x1 C dh1
dy2dy2; x2 C dh2
dy2dy2

P2 W

x1 C dh1
dy1dy1; x2 C dh2
dy1dy1

Thus the equation becomes,
fY1Y 2.y1; y2/ ŒArea of the rectangle
D fX1X2.x1; x2/ ŒArea of the Area of the parallelogram
Area of the Rectangle D dy1 dy2
Area of the parallelogram D
ˇˇˇˇˇˇˇˇ
dh1
dy2dy2
dh2
dy2dy2
dk1
dy1 dy1
dh2
dy1 dy1
ˇˇˇˇˇˇˇˇ
D
ˇˇˇˇˇˇˇˇ
dh1
dy2
dh2
dy2
dh1
dy1
dh2
dy1
ˇˇˇˇˇˇˇˇ
dy1 dy2
fY1Y 2.Y1; y2/dy1 dy2  fX1X2.x1; x2/
ˇˇˇˇˇˇˇˇ
dh1
dy2
dh2
dy2
dh1
dy1
dh2
dy1
ˇˇˇˇˇˇˇˇ
dy1 dy2
) f1 1Y 2.y1; y2/ D fX1X2 .x1; x2/
ˇˇˇˇˇˇˇˇ
dh1
dy2
dh2
dy2
dh1
dy1
dh2
dy1
ˇˇˇˇˇˇˇˇ

88
2 Probability
The matrix
ˇˇˇˇˇˇˇˇ
dh1
dy2
dh2
dy2
dh1
dy1
dh2
dy1
ˇˇˇˇˇˇˇˇ
is called as Jacobian matrix.
Also it can be shown using the same procedure
fX1X2 .x1; x2/ D fY1Y 2.y1; y2/
ˇˇˇˇˇˇˇˇ
dg1
dx2
dg2
dx2
dg1
dx1
dg2
dx1
ˇˇˇˇˇˇˇˇ
Example 2.4. The random variables X and Y are related via Y D g.X/, where g(.)
is monotonically increasing function.
FY .y/ D P.Y  y/ D P.g.X/  Y/ D P.X  g1.y// D FX.g1.y//
Let a D g1.y/
) y D g.a/
) FY .g.a// D FX.a/
Example 2.5. Let X be a random variable with uniform distribution over the interval
[4 to 4] (Fig. 2.15). Let the random variable Y D g.X/.
where g.x/ D x;
jxj  2
D 2;
X < 2
D 2;
X > 2
The density function of the random variable Y is computed as follows.
P.Y  y/ D P.g.X/y/ D P.X  g1.y//
Fig. 2.15 Y D g(X) of the
Example 2.5
-2
y = g(x)
2
2
X
-2

2.22 Transformation of Random Variables of the Type Y1 D g1.X1; X2/; Y2 D g2.X1; X2) 89
From the graph (Fig. 2.16)
FY .1/ D P.Y  1/ D P.X  1/ D 1
FY .2/ D P.Y  2/ D P.X  1/ D 1
FY .y/ D P.Y  y/ D P.X  y/ for  2  y  2
) D P.Y  2/ D 0
fY .y/ is obtained by differentiating FY .y/ with respect to y as shown in the graph
below.
Note that there are two impulses in fY .y/. One at y D 2 and another at
y D 2 ) P.Y D 2/ D 1
4 and P.Y D 2/ D 1
4
Example 2.6. Let X and Y are the two random variables such that Y D X2. Given
probability density function of the random variable X, the probability density func-
tion of the random variable Y is obtained as follows (Fig. 2.17).
From the graph, FY .y/ D P.Y  y/ D P.py  X  py/ D FX.py/ 
FX.py/
−4
−2
2
1/4
3/4
−2
2
y
1/4
1/4
1/8
y
4
x
−4
4
x
1
fX(x)
fY(y)
FX(x)
FY1(y)
1/8
Fig. 2.16 PDF and the corresponding CDF of the random variables X and Y of the Example 2.5
Fig. 2.17 Y D g(X) of the
Example 2.6
Y=X2
Y=y
X
−
y
√⎯
+
y
√⎯

90
2 Probability
Differentiating on both sides gives
fY .y/ D
1
2py

fX
py

C fX.py/
	
for y  0
(a) If X is having the following probability density function
fX.x/ D x
’e x2
2’ x 	 0
D 0; elsewhere
) fY .y/ D
1
2py
py
a e y
2˛  0

ŒNote that fx.x/ D 0 for x < 0
) fY .y/ D
 1
2˛ e y
2˛

for y 	 0
(b) If X is having the following probability density function
fX.x/ D

1
p
2˘
2 e

x2
22

for  1  x  1
) fY .y/ D
1
2py

1
p
2˘
2 e

y
22

C
1
p
2˘
2 e

y
22

) fY .y/ D
1
p
2˘y
2 e

y
22

for y0
(c) fY .y=X > 0/ is computed as follows
FY .y=X > 0/ D P.y  Y=X > 0/
D P.y  Y; X > 0/
P.X > 0/
D P

py  X  py; X > 0

P.X > 0/
D P

0 < X  py

P.X > 0/
D FX
py

 FX.0/
P.X > 0/
D FX
py

 FX.0/
Œ1  P.X  0/
D FX
py

 FX.0/
Œ1  FX.0/

2.22 Transformation of Random Variables of the Type Y1 D g1.X1; X2/; Y2 D g2.X1; X2) 91
Differentiating on both sides gives
fY .y=X > 0/ D

1
2py
 "
FX
py
Œ1  FX.0/
#
for y 	 0
Example 2.7. Let X and Y are the two independent random variables, the proba-
bility density function of the random variable Z D X C Y is computed as follows
(Fig. 2.18).
From the Graph
FZ.z/ D P.Z  z/ D P ..X C Y /  z/ D
Z 1
1
Z zy
1
fXY .x; y/dx dy
) fZ.z/ D @
R 1
1
R zy
1 fXY .x; y/dx dy
@z
Using Leibnitz integration Formula
) fZ.z/ D
Z 1
1
@.z  y/
@z
fXY .z  y; y/  @.1/
@z
fXY .1; y/
C
Z zy
1
@fXY .x; y/
@z
dx

dy
) fZ.z/ D
Z 1
1
fXY .z  y; y/ dy
Y
X+Y=z
X
z
z
Fig. 2.18 Graph depicting X C Y D z

92
2 Probability
fX(x)
fZ(z)
fY(y)
*
1/2
1/4
4
1/4
y
2
2
4
6
z
x
Fig. 2.19 Probability density function of the random variables X, Y and Z, where Z D X C Y
Note: Leibnitz integration
Differentiation of the integration with limits (Leibnitz integration)
g(x) D
Z a.x/
b.x/
f.x,y/dy
dg(x)
dx
D da.x/
dx f.x,a.x//  db.x/
dx f.x,b.x/ C
Z a.x/
b.x/
@f.x,y/
@x
dy
X and Y are independent ) fZ.z/ D
R 1
1 fX.z  y/fY .y/ dy
) fZ.z/ D fX.z/  fY .z/
Suppose if X is uniformly distributed between 0 and 2 and Y is uniformly distributed
between 0 and 4, then the probability density function of Z is computed as shown
below (Fig. 2.19).
Example 2.8. Let X and Y are the two independent random variables, the proba-
bility density function of the random variable Z D X=Y is computed as follows.
FZ.z/ D P.Z  z/ D P
X
Y

 z


2.22 Transformation of Random Variables of the Type Y1 D g1.X1; X2/; Y2 D g2.X1; X2) 93
To obtain the solution the Trick is to Compute
F Z
Y Dy.z/ D P

Z  z
Y D y

D P.Z  z; Y D y/=P.Y D y/
) FZ=Dy.z/ D P ..X= .Y D y//  z; Y D y/ =P.Y D y/
) FZ=Y Dy.z/ D P.X  zy/P.Y D y/=P.Y D y/
ŒBecause X and Y are independent
) FZ=Y Dy.z=Y D y/ D P.X  zy/ D FX.zy/
Differentiating on both sides gives,
) fZ=Y Dy.z/ D y fX.zy/
Also
P.Z  z=Y D y/ D lim
y!0
P.Z  z; y  Y  y C y/
P.y  Y  y C y/
) FZ=Y Dy.x/ D lim
y!0
FZY .z; y C y/  FZY .z; y/
FY .y C y/  FY .y/
) FZ=Y Dy.x/ D lim
y!0
ŒFZY .z; y C y/  FZY .z; y/ =y
ŒFY .y C y/  FY .y/ =y
) FZ=Y Dy.x/ D
dFZY .z; y/
dy
fY .y/
) fZ=Y Dy.x/ D
@2FZY .z; y/
@z@y
fY .y/
) fZ=Y Dy.x/ D fZY .z; y/
fY .y/
)
Z
fZ=Y Dy.x/fY .y/dy D fZ.z/
) fZ.z/ D
Z
fZ=Y Dy.z/fY .y/dy
fZ=Y Dy.z/ D y fX.zy/
) fZ.z/ D
Z
y fX.zy/fY .y/dy
Example 2.9. The random variable X and Y are independent and identically dis-
tributed random variables. Let Z D
p
X2 C Y 2 and ˆ D tan1  Y
X

. The joint
density function fZˆ.Z; ˆ/ is computed as follows.

94
2 Probability
We know,
fZˆ.z; ˆ/ D
fXY .x; y/
ˇˇˇˇˇˇˇˇ
dg1
dx
dg2
dx
dg1
dy
dg2
dy
ˇˇˇˇˇˇˇˇ
at .x1; y1/ C
fXY .x; y/
ˇˇˇˇˇˇˇˇ
dg1
dx
dg2
dx
dg1
dy
dg2
dy
ˇˇˇˇˇˇˇˇ
at .x2; y2/
C    fXY .x; y/
ˇˇˇˇˇˇˇˇ
dg1
dx
dg2
dx
dg1
dy
dg2
dy
ˇˇˇˇˇˇˇˇ
at .xn; yn/
Where Z D g1.X; Y/ and ˆ D g2.X; Y/ and (x1,y1), (x2,y2), : : : (xn,yn) are
the solutions obtained by solving the equations g1(x,y) and g2(x,y) for the constant
‘Z’ and ‘ˆ’.
In our case, g1.X; Y/ D Z D
p
X2 C Y 2 and g2.X; Y/ D ˆ D tan1  Y
X

.
Solving for X and Y for the ﬁxed Z and ˚ gives X1 D Z cos ˚ and Y1 D
Z sin ˚.x1; y1/ D .Zcos˚; Zsin˚/
dg1
dX D
X
p
X2 C Y 2 at .x1; y1/ D .Zcos˚; Zsin˚/ D Zcos˚
Z
D cos ˚
dg1
dY D
Y
p
X2 C Y 2 at .x1; y1/ D .Zcos˚; Zsin˚/ D Zsin˚
Z
D sin ˚
dg2
dX D

 Y
X2

1 C .Y=X/2 D
Y
X2 C Y 2 at .x1; y1/ D .Zcos˚; Zsin˚/
D ZSin˚
Z2
D Sin˚
Z
dg2
dY D
 1
X

1 C .Y=X/2 D
X
X2 C Y 2 at.x1; y1/ D .Zcos˚; Zsin˚/ D cos˚
Z
)
ˇˇˇˇˇˇˇˇ
dg1
dx
dg2
dx
dg1
dy
dg2
dy
ˇˇˇˇˇˇˇˇ
D
ˇˇˇˇˇˇˇ
cos˚
Sin˚
Z
sin˚
cos˚
Z
ˇˇˇˇˇˇˇ
D 1=jZj
Thus fZˆ.z; ˆ/ D
fXY.x;y/

dg1
dx
dg2
dx
dg1
dy
dg2
dy

at .x1; y1/ D .Zcos˚; Zsin˚/
) fZˆ.z; ˆ/ D jZjfXY .Zcos˚; Zsin˚/

2.22 Transformation of Random Variables of the Type Y1 D g1.X1; X2/; Y2 D g2.X1; X2) 95
Suppose if fXY .x; y/ D
1
2˘2 e
.x2Cy2/
22
, where 1  x; y  1.
) fZˆ.z; ˆ/ D
jzj
2˘
2 e
.z2/
22 ; where Z 	 0 and 0  ˆ  2˘
D 0; Otherwise
Example 2.10. The random variable X and Y are independent and identically dis-
tributed random variables. Let Z D
p
X2 C Y 2 and W D X=Y. The joint density
function
fZW.Z; W / is computed as follows.
We know,
fZW .z; w/ D
fXY .x; y/
ˇˇˇˇˇˇˇˇ
dg1
dx
dg2
dx
dg1
dy
dg2
dy
ˇˇˇˇˇˇˇˇ
at .x1; y1/ C
fXY .x; y/
ˇˇˇˇˇˇˇˇ
dg1
dx
dg2
dx
dg1
dy
dg2
dy
ˇˇˇˇˇˇˇˇ
at .x2; y2/
C    fXY .x; y/
ˇˇˇˇˇˇˇˇ
dg1
dx
dg2
dx
dg1
dy
dg2
dy
ˇˇˇˇˇˇˇˇ
at .xn; yn/
Where Z D g1.X; Y/ and W D g2.X; Y/ and (x1,y1), (x2,y2),: : :(xn,yn) are the
solutions obtained by solving the equations g1(x,y) and g2(x,y) for the constant ‘Z’
and ‘W’.
In our case, g1.X; Y/ D Z D
p
X2 C Y 2 and g2.X; Y/ D W D Y=X.
Solving for X and Y for the ﬁxed Z and W gives the following set of solutions
Z D
p
X2 C Y 2 ) Z2 D X2 C Y 2 D X2 C .XW /2 D X2.1 C W 2/
) X2 D Z2=.1 C W 2/
) X D CZ=
p
1 C W 2 or X D Z=
p
1 C W 2 and the corresponding
values for Y are
Y D CZW=
p
1 C W 2 or Y D ZW=
p
1 C W 2 respectively:
Therefore the solutions are

Z=
p
1 C W 2; CZW=
p
1 C W 2

and .Z=
p
1 C W 2;
ZW=
p
1 C W 2/
dg1
dX D
X
p
X2 C Y 2 at .x1; y1/ D

CZ=
p
1 C W 2; C ZW=
p
1 C W 2

D 1=
p
1 C W 2

96
2 Probability
dg1
dX D
X
p
X2 C Y 2 at .x2; y2/ D


Z
p
1 C W 2 ;  ZW=
p
1 C W 2

D 
1
p
1 C W 2
dg1
dY D
Y
p
X2 C Y 2 at .x1; y1/ D

CZ=
p
1 C W 2; C ZW=
p
1 C W 2

D W=
p
1 C W 2
dg1
dY D
Y
p
X2 C Y 2 at .x2; y2/ D


Z
p
1 C W 2 ;  ZW=
p
1 C W 2

D W=
p
1 C W 2
dg2
dX D Y
X2 at .x1; y1/ D

CZ=
p
1 C W 2; C ZW=
p
1 C W 2

D W
p
1 C W 2
Z
dg2
dX D Y
X2 at .x2; y2/ D


Z
p
1 C W 2 ;  ZW=
p
1 C W 2

D CW
p
1 C W 2
Z
dg2
dY D 1
X at .x1; y1/ D

Z
p
1 C W 2 ; ZW=
p
1 C W 2

D C
p
1 C W 2
Z
dg2
dY D 1
X at .x2; y2/ D


Z
p
1 C W 2 ;  ZW=
p
1 C W 2

D 
p
1 C W 2
Z
Jacobian at .x1; y1/ D
ˇˇˇˇˇ
1=
p
1 C W 2
W
p
1CW 2
Z
W=
p
1 C W 2
p
1CW 2
Z
ˇˇˇˇˇ D j.1 C W 2/j
jZj
Jacobian at .x2; y2/ D
ˇˇˇˇˇ
1=
p
1 C W 2
W=
p
1 C W 2
W
p
1CW 2
Z

p
1CW 2
Z
ˇˇˇˇˇ D j.1 C W 2/j
jZj
fZW.z; w/ D
jZj
j.1 C W 2/j
fXY

Z
p
1 C W 2 ; ZW=
p
1 C W 2

C
jzj
j.1 C W 2/jfXY


Z
p
1 C W 2 ; ZW=
p
1 C W 2

) fZW.z; w/ D
jZj
j.1 C W 2/j

fXY

Z
p
1 C W 2 ; ZW=
p
1 C W 2

CfXY

Z
p
1 C W 2 ; ZW=
p
1 C W 2


Z=
p
1 C W 2; CZW=
p
1 C W 2


2.22 Transformation of Random Variables of the Type Y1 D g1.X1; X2/; Y2 D g2.X1; X2) 97
Suppose if fXY .x; y/ D
1
2˘ e.x2Cy2/=2
) fZW .z; w/ D
jZj
j.1 C W 2/j
1
˘ e..z/2C.zw/2/=2.1CW 2/
) fZW .z; w/ D
jZj
j.1 C W 2/j
1
˘ e z2
2 for z 	 0 and  1  W  1
Given W D Y=X  1  X  1 and 1  Y  1 and hence 1  W  1
Example 2.11. Let X is a uniformly distributed random variable over the interval
[0 1]. Y is the random variable related with the random variable X as Y D g.X/.
The invertible function ‘g(.)’ is related with the distribution function FY .y/ is as
given below.
P.Y  y/ D P.g.X/  y/ D P.X  g1.y// D FX

g1.y/

Note that g(.) must be the invertible function.
Also we know
FX.x/ D x as X is uniformly distributed over the interval Œ0 1
) P.Y  y/ D FY .y/ D g1.y/
) FY .:/ D g1.:/
For instance if fY .y/ D .e
p
2jyj/=
p
2, then g (.) is obtained as follows.
For fY .y/ D e
p
2jyj
p
2
,
FY .y/ is computed as follows.
FY .y/ D
Z y
1
e
p
2jyj
p
2
dy
Case 1: If y  0
Z y
1
e
p
2y
p
2
dy
D e
p
2y
2
Case 2: If y 	 0
FY .y/ D
Z 0
1
e
p
2y
p
2
dy C
Z y
0
e
p
2y
p
2
dy

98
2 Probability
D 1
2  e
p
2y
2
C 1
2
D 1  e
p
2y
2
g1.y/ D e
p
2y
2
D x )
p
2y=2 D ln.x/
) y D
p
2 ln.x/ for y  0
Similarly for y 	 0; g1.y/ D 1  e
p
2y
2
D x
) y D ln.2.1  x//=
p
2 for y 	 0
Thus the function y D g.x/ D
p
2 ln.x/ for 0:5  x  1
D ln.2.1  x//=
p
2 for 0  x
 0:5
Example 2.12. Let Z D max.X; Y/ and W D min.X; Y/, where, X and Y are arbi-
trary random variables. The joint density function fZW.z; w/ is computed in terms
of fXY.x; y/ is as follows.
When z 	 w; P.Z  z; W  w/ D P.X  z; Y  w/ C P.X  w; Y  z/
P.X  w; Y  w/ (See Fig. 2.20 given below)
(W, Z)
(Z, Z)
(Z, W)
(W, W)
Fig. 2.20 Computation of the joint density function of the 2.12

2.24 Indicator
99
) FZW .z; w/ D FXY .z; w/ C FXY .w; z/  FXY .w; w/ for z 	 w
D 0; Otherwise
) fZW .z; w/ D fXY .z; w/ C fXY .w; z/  fXY .w; w/ for z 	 w
D 0; Otherwise
For instance if X and Y are independent and identically distributed and is uni-
formly distributed between 0 and 4, then
fZW .z; w/ D 1
25 for z 	 w; z; w D 0 to 4
D 0; Otherwise
2.23
Expectations
Expectation of the random variable ‘X’ represented as E(X) is deﬁned as follows:
E.X/ D
Z 1
1
xfX.x/
Properties:
1. E.g.X// D
R 1
1 g.x/fX.x/dx
2. EX=Y Dy D E.X=Y D y/ D R 1
1 xf X=Y Dy.x/
3. E.X/ D EY .EX=Y Dy/
4. If X > 0 then E.X/ > 0
5. E.X2 D EŒŒX  E.X/2 C EŒX2
2.24
Indicator
(a) Markov inequality (Fig. 2.21)
Consider
X=b 	 Ib (X)
) E
X
b

	 E .Ib.X//
) E
X
b

	 P.X 	 b/
) P.X 	 b/  E.X/
b

100
2 Probability
Fig. 2.21 Indicator
b
x
x/b
Ib(x)
(b) Chebyshev inequality
Consider the random variable X D Y 2
Using Markov inequality
P.Y 2 	 b/  E.Y 2/=b2
) P.jY j 	 b/  E.Y 2/=b2
) P.jY  E.Y /j 	 b/  E.jY  E.Y /j2/=b2
) P.jY  E.Y /j 	 b/  
2
y=b2
(c) Schwarz inequality
Consider E .aX  Y/2 	 0
) a2E.X2/ C E.Y 2/  2aE.XY / 	 0
) a2E.X2/  2aE.XY / C E.Y 2/ 	 0
The above equation can be viewed as the quadratic equation with variable ‘a’
The equation is valid only when 4.E.XY//2  4E.X2/E.Y 2/  0
) .E.XY //2  E.X2/E.Y 2/
(d) Chernoff bound (Fig. 2.22)
Consider eSXaS 	 Ia(x)
) E.eSXaS/ 	 E.Ia.x//
) E.eSXaS/ 	 P.X 	 b/
) P.X 	 a/  eaSE.eSX/
(e) Also it can be shown EŒXY  0:5.E.X2/ C E.Y 2// as follows
E..X  Y /2/ 	 0

2.25 Moment Generating Function
101
a
Ia(x)
esx-as
x
Fig. 2.22 Chernoff bound
) E.X2/ C E.Y 2/  2E.XY /
) EŒXY  0:5

E.X2/ C E.Y 2/

(f) Correlation co-efﬁcient
From Cauchy-Schwarz inequality .E.XY//2  E.X2/E.Y 2/
The ratio .E.XY //2
E.X2/E.Y 2/  1
Deﬁne the ratio  D
E ..X  mX/.Y  mY //
p
E.X  mX/2/E..Y  mY /2/
The ratio  is called correlation co-efﬁcient. The range of  is given as
0  jj  1
2.25
Moment Generating Function
The moment generating function is deﬁned as ˚X.s/ D E.esX/
Also it is known that E.esX/ D E

1 C .sX/2
2Š
C .sX/2
3Š
C   
Therefore differentiating ‘n-times’ the moment generating function ˚X.s/ and
equating s D 0 gives the E.Xn/.
Thus E.Xn/ D d n˚X .s/
dSn
js D 0

102
2 Probability
2.26
Characteristic Function
The characteristic function of the random variable X is given as E.ejwX/.
E.Xn/ can also be obtained using characteristic function.
2.27
Multiple Random Variable (Random Vectors)
Collections of random variables are called random vectors. The random vectors
represented as X D
X1
X2
:
:
:
:
Xn
Joint Cumulative distribution function
FX.x1; x2; x3; : : : xn/ D pr.X1  x1; X2;  x2; X3  x3; : : : Xn  xn/
Probability mass function of the random vector is represented as follows.
pr.X1 D x1; X2 D x2; X3 D x3; : : : Xn D xn/
Probability density function of teh
fX.x1; x2; x3; : : : xn/ D
@n
@x1@x2@x3 : : : @xnFX .x1; x2; x3; : : : xn/
Similarly
FX.x1; x2; x3; : : : xn/ D
Z Z Z
: : :
Z
fX .x1; x2; x3; : : : xn/ @x1@x2@x3 : : : @xn
Marginal density function
fXi.xi/ D
Z Z Z
  
Z fX .x1;x2;:::x.i1/;x.iC1/x.i/:::xn/
@x1@x2:::@x.i1/@x.iC1/;:::@xn

2.27 Multiple Random Variable (Random Vectors)
103
Joint density function
fX1X2.x1; x2/ D
Z Z Z
: : :
Z
fX
 x1; x2; : : : x.i  1/
x.i/; x.i C 1/ : : : xn

@x3@x4 : : : @xn
Conditional probability density function
Consider the random vector X is divided into two random vectors X1 and X2 as
shown below.
X D
X1
X2

Then the conditional probability of the random vector X1 over the random vector
X2 is computed as follows
fX1=X2.x1/ D fX.x/=fX2.x2/
Also note that
fX.x/ D fX1=X2.x1/fX2.x2/
In General,
fX.x/ D fX1.x1/fX2=X1.x2/fX3=X1;X2.x3/ : : : fXn=Xn1:::X1.xn/
Independence
The random variables of the random vectors X1,X2,X3,: : :Xn are independent if
FX .x1; x2; x3; : : : xn/ D FX1.x1/FX2.x2/FX3.x3/ : : : FXn.xn/
fX .x1; x2; x3; : : : xn/ D fX1.x1/fX2.x2/fX3.x3/ : : : fXn.xn/
Expectation of the random vector
E.X/ D
E.X1/
E.X2/
:
:
:
:
E.Xn/
Moment generating function of the random vector
E.eST X/

104
2 Probability
Where
X D
X1
X2
:
:
:
:
Xn
S D
s1
s2
:
:
:
:
sn
Characteristic function of the random vector
E.ejwT X/
where
X D
X1
X2
:
:
:
:
Xn
W D
w1
w2
:
:
:
:
wn
Correlation matrix of the random vector
E.X XT / D
E.X12/ E.X1X2/ E.X1X3/ : : : E.X1Xn/
E.X2X1/ E.X22/ E.X2X3/ : : : E.X2Xn/
:
:
:
: : :
:
:
:
:
: : :
:
:
:
:
: : :
:
E.XnX1/ E.XnX2/ E.XnX3/ : : : E.Xn2/
Covariance matrix of the random vector
E

.X  E.X/.X  E.X/T 
D E.X XT /  E.X/E.X/T
Note:
1. The events Xi and Xj are statistically uncorrelated if E.Xi Xj/E.Xi/E.Xj/ D 0;
i ¤ j. Also note that the co-variance matrix becomes diagonal matrix if the
elements of the random vector are uncorrelated to each other.
2. The events Xi and Xj are independent then E.XiXj/ D E.Xi/E.Xj/.
3. If the two events are statistically independent, they are uncorrelated. But the vice-
versa is not true.

2.27 Multiple Random Variable (Random Vectors)
105
Gaussian probability density function with mean ‘’ and variance ‘
2’
fX.x/ D
1
p
2…¢2 e Œx2
2¢2
FX.x/ D P.X  x/
Suppose y2 D ŒX2
¢2
FX.x/ D
Z x
1
1
p
2…¢2e Œx2
2¢2 dx
D
Z
Œx
¢
1
¢
p
2…¢2 e y2
2 dy
D
Z
Œx
¢
1
¢
p
2…¢2 e y2
2 dy
D
Z
Œx
¢
1
1
p
2…
e y2
2 dy
D FY
ŒX  
¢

Thus Gaussian distribution value at ‘x’ with mean D ; variance D 
2 can be
computed using Gaussian distribution function of y at ŒX

whose mean D 0 and
variance D 
2
Moment generating function of the Gaussian density function
ˆX.s/ D E.esx D
Z 1
1
esx
p
2…¢2 e Œx2
2¢2 dx
Consider the powers of e.
sx2¢2  x2  2 C 2x
2¢2
D 
x2 C 2  .2 C 2s¢2/x
2¢2

Adding . C s¢2/2 and subtracting . C s¢2/2 on the numerator, we get the
following
D 
x2 C 2  .2 C 2s¢2/x C . C s¢2/2  . C s¢2/2
2¢2

D 
.x  .2 C 2s¢2//2 C 2  . C s¢2/2
2¢2


106
2 Probability
)
Z 1
1
e 2.Cs¢2/2
2¢2
p
2…¢2
e
.x.2C2s¢2//
2
2¢2
dx
D ese
¢2s2
2
Because;
Z 1
1
e.x.2C2s¢2//
2
2¢2
p
2…¢2
dx D 1
EŒX D @ˆX.s/
@s
js D 0
EŒXn D @nˆX.s/
@Sn
js D 0
ˆX.s/ D ese
¢2s2
2
Consider the case when  D 0.
ˆX.s/ D e
¢2s2
2
D 1 C
¢2s2
2
1Š
C

¢2s2
2
2
2Š
C : : : : C

¢2s2
2
2m
2mŠ
C   
) @2mˆX.s/
@S2m
js D 0 D ¢2m
2mmŠ.2m/.2m  1/ : : : 1
Therefore EŒXn D ¢nŒ1:3:5:7 : : : :.n  1/ when n is even
D 0 when n is odd
Chernoff bound for Gaussian density function with zero mean and unity
variance
The Chernoff bound is given as P.X 	 a/  eaSE.eSX/ (i.e) P.X 	 a/ 
eaSˆX.s/
For Gaussian density function ˆX.s/ D ese
¢2s2
2
) P.X 	 a/  eaSese
¢2s2
2
for all s
To get the tight bound we have to ﬁnd the value of ‘s’ so that eaSese
¢2s2
2
is
minimized.
Assume  D 0 and variance ¢2 D 1 for simplicity.
Differentiating eaSe
¢2s2
2
with respect to s and equate to zero
eaSe
¢2s2
2
¢2
2 .2s/

C e ¢2s2
2
eaS.a/ D 0
s D .a=¢2/ D a

2.28 Gaussian Random Vector
107
Substituting s D a in eaSe
s2
2 we get, e
a2
2
) P.X 	 a/  e
a2
2
2.28
Gaussian Random Vector with Mean Vector X
and Covariance Matrix CX
fX.x/ D
1
.2˘/
n
2 jCj
1
2
e
1
2
 h
X  X
iT
C 1 h
X  X
i
Moment Generating function for the Gaussian random variable is given as
ˆX.S/ D eŒX TS e
1
2 ŒST CS
Properties of Gaussian random vector
1. E.X/ D
E.X1/
E.X2/
:
:
:
:
E.Xn/
D X
2. E.Xi/ D d˚X.s/
dSi
ˇˇˇˇ s D 0
3. E

Xi 2
D d 2˚X.s/
dSi2
ˇˇˇˇ s D 0
4. E.XiXj / D d 2˚X.s/
dSiSj
ˇˇˇˇ s D 0
5. In general E

Xi mXj n
D d mCn˚X.s/
dSi msj n
ˇˇˇˇ s D 0
6. The correlation matrix of the Gaussian random vector is given as
R D E.X XT / D
E.X12/ E.X1X2/ E.X1X3/ : : : E.X1Xn/
E.X2X1/ E.X22/ E.X2X3/ : : : E.X2Xn/
:
:
:
: : :
:
:
:
:
: : :
:
:
:
:
: : :
:
E.XnX1/ E.XnX2/ E.XnX3/ : : : E.Xn2/

108
2 Probability
7. The co-variance matrix is obtained as
C D E

.X  E.X/ .X  E.X/T 
D E.X XT /  E.X/E.X/T
8. Marginal density of any `-dimensional sub vector of X.`  n/ is also Gaussian
with proper mean and co-variance matrix as described below
Consider the Gaussian random vector X D
X1
X2
:
:
:
Xl
XlC1
XlC2
XlC3
:
:
:
Xn
With mean vector m D
m1
m2
:
:
:
ml
mlC1
mlC2
mlC3
:
:
:
mn
And the covariance matrix given as
c11 c12 : : : c1l : : : c1n
c21 c22 : : : c2l : : : c2n
:
:
: : : : : : :
:
:
: : : : : : :
:
:
: : : : : : :
cl1 cl2 : : : cll : : : cln

2.28 Gaussian Random Vector
109
clC1;1 clC1;2 : : : clC1;l : : : clC1;n
:
:
: : :
:
: : :
:
:
:
: : :
:
: : :
:
:
:
: : :
:
: : :
:
cn1
cn2
: : : cn;l
: : :
cnn
Let the sub vector of the above mentioned Gaussian vector be
Y D
X1
X2
:
:
:
Xl
The random vector Y is also Gaussian distributed with mean vector
my D
m1
m2
:
:
:
ml
and co-variance matrix
c11
c12
:
:
:
c1l
c21
c22
:
:
:
c2l
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
c1l
cl2
:
:
:
cll
9. If the Gaussian random vector X with mean vector M and co-variance matrix
‘CX’ is linearly transformed into the random vector Y using the transformation
matrix ‘A’ and the column vector b as
Y D AX C b
Then the random vector Y is also Gaussian distributed with mean vector
A M C b and the co-variance matrix A CXAT
Note that the co-variance matrix of the random variable Y is A CXAT irre-
spective of the distribution function. Similarly the mean vector of the random
variable Y is A M C b irrespective of the type of the distribution function.

110
2 Probability
10. Consider the random vector X is represented as row concatenation of two
random vectors X1 and X2 as shown below
X D
X1
X2

Similarly the mean vector the random vector X is represented as row concate-
nation of the mean vectors of the random vectors X1 and X2 as shown below.
M D
M1
M 2

Also let the co-variance matrix of the random vector X1 and X2 are represented
as C11 and C22 respectively. The co-variance matrix of the random vector X is
represented as
"
ŒC11
ŒC12
ŒC21
ŒC22
#
(a) The conditional density function fX2=X1Dx1.x2/ is also Gaussian with mean
vector M 2 C ŒC21ŒC111ŒX1  M1 and co-variance matrix C D ŒC22 
ŒC21ŒC111ŒC12
(b) The conditional density function fX1=X2Dx2.x1/ is also Gaussian with mean
vector M1 C ŒC12ŒC221ŒX2  M 2 and co-variance matrix C D ŒC11 
ŒC12ŒC221ŒC21
11. If the co-variance matrix of the Gaussian random vector is diagonal, the ele-
ments in the random vector X are uncorrelated and independent
12. Contours of 2D-Gaussian probability density function (Fig. 2.23)
fXY .x; y/ D
1
2˘
1
2
p
1  2 e

 x2

12 C y2

22  2xy

1
2

2.1  2/
Consider the 2D contour obtained from the equation
x2

12 C y2

22  2xy

1
2
D constant D c1
This is the contour having the same probability density value.

2.28 Gaussian Random Vector
111
rho=0 sigma1=1 sigma2=1
rho=1/2 sigma1=1 sigma2=1
rho=1/2 sigma1=1 sigma2=2
rho= −1/2 sigma1=1 sigma2=1
rho= −1/2 sigma1=1 sigma2=2
rho=0 sigma1=1 sigma2=2
rho=0 sigma1=2 sigma2=1
10
8
6
4
2
0
−2
−4
−6
−8
−10
−10
−5
0
5
10
10
8
6
4
2
0
−2
−4
−6
−8
−10
−10
−5
0
5
10
10
8
6
4
2
0
−2
−4
10
8
6
4
2
0
−2
−4
−6
−8
10
−6
−8
−10
−10
−10 −8
−6
−4
−2
0
2
4
6
8
10
−5
0
5
10
10
8
6
4
2
0
−2
−4
−6
−8
−10
−10
−5
0
5
10
10
8
6
4
2
0
−2
−4
−6
−8
−10
−10
−5
0
5
10
10
8
6
4
2
0
−2
−4
−6
−8
−10
−10
−5
0
5
10
Case 1:  D 0; 
1 D 
2
y
X
c1=2c
c1 c
Fig. 2.23 Contours of the 2D Gaussian probability density function

112
2 Probability
Note that when the contour radius increases, actual magnitude of the pdf decreases.
Thus contour c1 is having higher magnitude compared with the contour c2.
Case 2:  D 0; 
1 D 
2
y
x
1 > 2
Case 3:  D 0; 
2 D 
1
y
x
σ1 < σ2
Case 4:  < 0; 
2 D 
1. Note that the value of ‚ D 45ı
y
x
Fig. 2.23 (continued)

2.28 Gaussian Random Vector
113
Case 5:  > 0; 
2 D 
1. Note that the value of ‚ D 45ı
y
x
Case 6:  ¤ 0; 
2 ¤ 
1, Note that the value of ‚ ¤ 45ı
Fig. 2.23 (continued)
Example 2.13. Let X be a uniform random variable in [0,100]. E.X=X 	 65/ is
computed as follows.
E.X=X 	 65/ D
Z 100
0
x fX=X65.x/dx
fX=X65 is obtained as follows
FX=X65.x/ D P.X < x; X 	 65/=P.X 	 65/
D P.65  X  x/=P.X 	 65/
D .FX.x/  FX.65//=P.X 	 65/
D .FX.x/  FX.65//=P.X 	 65/

114
2 Probability
Differentiating on both sides,
) fX=X65.x/ D fX.x/=P.X 	 65/ for X 	 65
) fX=X65.x/ D fX.x/
Z 100
65
fX.x/dx
) fX=X65.x/ D fX.x/
Z 100
65
x fX.d/dx
fX.d/ D
1
100 for 0  x  100 .Uniformly distributed/
) fX=X65.x/ D fX.x/
Z 100
65
.1=100/dx
D .1=100/
 1
100

.35/
D 1
35 for X 	 65 and X  100
) E.X=X 	 65/ D
Z 100
65
x fX=X65.x/dx
D
Z 100
65
x
 1
35

dx
D
 1
35
 Z 100
65
x dx
D
 1
35

.1002  652/
2
D 165  35
35  2
D 82:5
Example 2.14. Let X be a poison random variable with probability mass function
PX.k/ D e k
kŠ
for k D 0; 1; 2; : : :
E(X) and variance(X) are computed as follows
The moment generating function
˚X.s/ D E.eSX/ D
X1
xD0 eSx ex
xŠ
D
1
X
xD0
.eS/xe
xŠ
D e

1 C .eS/
1Š
C .eS/2
2Š
C .eS/3
3Š
C   

D ee.eS/

2.28 Gaussian Random Vector
115
Differentiating on both sides with respect to ‘s’ and substitute s D 0 gives
E.X/ D e de.eS/
ds
.at s D 0/ D ee.eS/eS .at s D 0/ D 
Differentiating e e.eS/eS with respect to ‘s’ and substitute s D 0 gives
E.X2/ D e h
eSe.eS /eS C e.eS/eSi
.at s D 0/ D .2 C /
) Variance D ŒE.X2/  E.X/2 D 
Mean D 
Variance D 
Example 2.15. Consider the random variable X with probability density function as
given below.
fX.x/ D ex for x 	 0
D 0; otherwise
E.X/; fX.x=X 	 2/ and E.X=X 	 2/ are computed as shown below.
E.X/ D
Z 1
1
x fX.x/dx D
Z 1
0
x exdx D 
Z 1
0
x exdx
D 
Z 1
0
x d
"
ex

#
DD 
"
x
"
ex

#1
0

Z 1
0
ex
 dx
#
D Œ1=2 D 1=
FX.x=X 	 2/ D P.X  x=X 	 2/ D P.X  x; X 	 2/=P.X 	 2/
D P.X  x; X 	 2/=P.X 	 2/
D P.2  X  x/
P.X 	 2/
D FX.x/  FX.2/
P.X 	 2/
) fX.x=X 	 2/ D
fX.x/
P.X 	 2/ for x 	 2
D 0; otherwise
P.X 	 2/ D
Z 1
2
fX.x/dx D
Z 1
2
exdx D ex

1
2 D ex D e2
) fX.x=X 	 2/ D exe2 for x 	 2
D 0; otherwise

116
2 Probability
E.X=X 	 2/ D
Z 1
1
xfX.x=X 	 2/dx D
Z 1
2
xexe2dx D e2
Z 1
2
x d
"
ex

#
D e2
"
x
"
ex

#1
2

Z 1
2
ex
 dx
#
D e2 h
.2e2=/ C e2=2/
i
D 2 C .1=/
Example 2.16. Let X D ŒX1 X2 X3T is a three-dimensional zero-mean Gaussian
random vector with covariance matrix C given by
C D
1
3
0
3
2
3
0
3
1
The joint density function fX1X2X3.x1; x2; x3/ is given as follows
fX1X2X3.x1; x2; x3/ D fX.x/
D
1
.2˘/
n
2 jCj
1
2
e
1
2
 h
X  X
iT
C 1 h
X  X
i
where X D Œx1 x2 x3T ; jcj D 16; n D 3; X D Œ0 0 0T and
C 1 D
2
4
7
3 9
3 1 3
9 3
7
3
5 
 1
16

) fX1X2X3.x1; x2; x3/
D
1
.2˘/
3
2 j16j
1
2
e
1
2
 0
@Œx1 x2 x3
2
4
7
3 9
3 1 3
9 3
7
3
5

 1
16
 2
4
x1
x2
x3
3
5
1
A
) fX1X2X3.x1; x2; x3/
D
1
.2˘/
3
2 j16j
1
2
e
 1
32

.7 x12  x22 C 7 x32 C 6 x1 x2
C6 x2 x3  18x1 x3
If Y D X1CX2CX3, then fY .y/ is Gaussian with mean mY and covariance matrix
CY as shown below

2.28 Gaussian Random Vector
117
Y D Œ1 1 1
2
4
x1
x2
x3
3
5
Y D AX; where A D Œ1 1 1
The covariance matrix CY D A CXAT D Œ1 1 1
2
4
1
3
0
3
2
3
0
3
1
3
5
2
4
1
1
1
3
5 D 16
Mean vector mY D AmX D AX D Œ1 1 1Œ0 0 0T D 0
Thus fY .y/ D
1
p
2˘13e Œy2
216
Example 2.17. Let ŒX1 X2T be the two-dimensional zero-mean Gaussian random
vector with covariance matrix C given by C D
4 2
2 4

. The conditional density func-
tions fX1=X2.x1/ and fX2=X1.x2/ are also Gaussian distributed with the following
speciﬁcations.
(i) fX2=X1.x2/ is Gaussian with mean D m2  c21 c111.x1  m1/ and vari-
ance D c22  c21 c111c12, where Œm1 m2T is the mean vector of the
two-dimensional Gaussian random vector in general. Also C D
c11
c12
c21
c22

be the corresponding generalized co-variance matrix.
In our case m1 D 0 and m2 D 0: c11 D 4 c12 D 2 c21 D 2 c22 D 4
There fore fX2=X1.x2/ is Gaussian distributed with mean D

 2
4

x1 and
variance D 4 
 2
4

 2 D 3
fX2=X1.x2/ D
1
p
…  6
eŒx2C 1
2 x1
2
6
(ii) fX1=X2.x1/ is Gaussian with mean D m1  c12 c221.x2  m1/ and vari-
ance D c11  c12 c221c21, where Œm1 m2T is the mean vector of the
two-dimensional Gaussian random vector in general. Also C D
c11
c12
c21
c22

be the corresponding generalized co-variance matrix.
In our case m1 D 0 and m2 D 0: c11 D 4 c12 D 2 c21 D 2 c22 D 4
There fore fX2=X1.x2/ is Gaussian distributed with mean D  2
4
 x1 and vari-
ance D 4 
 2
4

 2 D 3
fX2=X1.x2/ D
1
p
…  6
eŒx2C 1
2 x1
2
6

118
2 Probability
Example 2.18. Let X1 and X2 are jointly Gaussian with zero-mean. Let the
co-variance matrix of the random vector [X1 X2] is given as C D
1


1

. Let
Y1 D X1 C X2; Y2 D X1  X2, then the co-variance matrix of the random vector
[Y1 Y2] is given as
CY D ACXAT
1
1
1
1
 1


1
 1
1
1
1

D
2.1 C /
0
0
2.1  /

This implies the random variable Y1 and Y2 are uncorrelated. As the distribu-
tion is Gaussian, this also implies that the two random variables Y1 and Y2 are
independent.
Similarly, for the case of co-variance matrix C D


12

1
2

1
21

22

and
Y1 D X1
1 C X2
2 ; Y2 D X1
1  X2
2 , the co-variance matrix CY D ACXAT is computed
as following
1=
1
1=
2
1=
1 1=
2
  
12

1
2

1
2

22
 1=
1
1=
1
1=
2
1=
2

D

1 C 
1

2 C 
2

1  
1

2  
2
 1=
1
1=
1
1=
2 1=
2

D
2.1 C /
0
0
2.1  /

This implies the random variable Y1 and Y2 are uncorrelated. As the distribu-
tion is Gaussian, this also implies that the two random variables Y1 and Y2 are
independent.
2.29
Complex Random Variables
Consider the probability space (S,F,P), then the mapping of the outcome s 2 sample
space S to the complex line is called complex random variable (Fig. 2.24).
The complex random variable ‘X’ is represented as Xr C j Xi
E.X/ D E.Xr/ C j E.Xi/
The joint pdf of ‘Xr’ and ‘Xi’ is represented as fXrXi .xr; xi/, which is the joint
density function of the random variable ‘Xr’ and ‘Xi’.
E.g.X// D E.g.Xr// C j E.g.Xi//
The complex random variable X D Xr Cj Xi is deﬁned as proper complex random
variable if it satisﬁes the following condition.

2.30 Sequence of the Number and Its Convergence
119
Fig. 2.24 Illustration of the
Complex random variables
S
(S,F,P)
x
a
(C,B,Px)
C
- `
E.X2/ D 0
E.X2/ D E ..Xr C j Xi/.Xr C j Xi// D E.Xr2  Xi2/ C 2jE.XiXr/
) E.Xr2/  E.Xi2/ D 0
.i:e/ E.Xr2/ D E.Xi2/
Also; E.Xi Xr/ D 0
(i.e.) The random variable Xr and Xi are uncorrelated
The second moment of the random variables ‘Xr’ and ‘Xi’ are zero.
It can also be shown that for the proper complex random variable, variance of the
random variables ‘Xr’ and ‘Xi’ are equal and they are uncorrelated.
E ..X  E.X//2/ is called pseudo covariance. The co-variance for the complex
random variable ‘X’ is deﬁned as E((X-E(X)) .X  E.X//H)
2.30
Sequence of the Number and Its Convergence
Let the sequence of random variables be represented as x1x2x3 : : :, then the se-
quence converges to the constant x is represented as follows
lim
n!1 xn D x
For example

1 C 1
1

;

1 C 1
2

;

1 C 1
3

; : : :

1 C 1
n

is the sequence of random
variables.

120
2 Probability
lim
n!1

1 C 1
n

D 1
Deﬁnition for convergence:
lim
n!1 xn D x
) For any 2> 0, there exists N, such that jxn  xj <2 for all n 	 N
2.31
Sequence of Functions and Its Convergence
Let the sequence of functions be represented as f 1.t/f 2.t/f 3.t/ : : : f n.t/, then the
function converges to the function f(t) is represented as follows
 Point wise convergence limn!1 fn.t/ D f .t/ for all ‘t’
 Weeker convergence limn!1 fn.t/ D f .t/ for all t except at certain discrete
time instants t1t2t3 : : : tn
 Mean square convergence
The sequence of functions converges to the function f(t) if it satisﬁes the follow-
ing condition
lim
n!1
Z 1
1
jfn.t/  f.t/j2dt D 0
This can be interpreted as the sequence of numbers and they converge to the con-
stant 0.
Example: The Fourier series representation of the periodic signal converges in
mean square sense.
(i.e.) The function fn.t/ D PkDn
kDn ckej2pikf 0t converges to the function
f(t) in mean square sense.
2.32
Sequence of Random Variable
Random variable is the function of mapping of the outcomes of the experiment
(i.e.) events’s 2 the sample space S to the real line. Hence it comes under sequence
of functions.
The sequence of functions X1.s/X2.s/X3.s/ : : : Xn.s/ converges to the func-
tion X.s/ (Another random variable) as follows
 Point wise convergence
limn!1 Xn.s/ D X.s/ for all s
 Almost sure convergence
limn!1 Xn.s/ D X.s/ for all s 2 , where  is the ﬁeld (i.e.) p./ D 1
 Mean square convergence
lim
n!1 Xn.s/ D X.s/

2.32 Sequence of Random Variable
121
If lim
n!1 E..Xn.s/  X.s//2/ D 0
Note that E..X1.s/  X.s//2/; E..X2.s/  X.s//2/; E..X3.s/  X.s//2/; : : :
E..Xn.s/X.s//2/ can be viewed as the sequence of numbers and they converge
to the constant 0.
 Convergence in probability
limn!1 Xn.s/
D
X.s/ in probability sense if the sequence of numbers
P.jXn  Xj >2/ converges to the value 0 (i.e.) limn!1 P.jXn  Xj >2/ D 0
for any 2> 0
 Convergence in distribution
limn!1 Xn.s/ D X.s/ in distribution sense if distribution function FXn.˛/ D
FX.˛/ for all values of ‘˛’ where FX.˛/ is continuous.
Properties of the different types of convergence
1. If the sequence of random variable converges in mean square sense then they will
converge in probability sense (Fig. 2.25)
P ..jXn  Xj 	2/ 	 E..Xn  X/2/
22
Proof. If the sequence of random variable converges in the mean square sense,
E..Xn  X/2/ D 0 ) P..jXn  Xj 	2/  0. Probability cannot be less than
zero and hence P..jXn  Xj 	2/ D 0 and hence proved
2. If the sequence of random variable converges in almost sure sense then they will
converge in probability sense. (Obvious from the deﬁnition)
3. If the sequence of random variable converges in probability sense then they will
converge in distribution sense and hence
Fig. 2.25 Convergence
of the Sequence of random
variables
1. Strictly convergence
2. Convergence in distribution
3. Convergence in probability
4. Convergence in mean
square sense
5. Convergence in almost sure
sense

122
2 Probability
 If the sequence of random variable converges in mean square sense then they
will converge in distribution sense.
 If the sequence of random variable converges in almost sure sense then they
will converge in distribution sense.
2.33
Example for the Sequence of Random Variable
Suppose X1 X2 : : : is the sequence of random variable such that E.Xi/ D k for all
i, E..Xi  k/2/ is ﬁnite for all i and the covariance E..Xi  k/.Xj  k// D 0 for
i ¤ j, then the sequence of random variable deﬁned as Y1Y2Y3 : : : Yn converges
to the constant ‘k’ in mean square sense.
Where Yn D  1
n
 Pn
kD1 Xi.
2.34
Central Limit Theorem
If we have independent and identically distributed random variables X1 X2 : : : with
mean ‘m’ and variance less than inﬁnity, then the sequence of random variable

1
pn
 Pn
iD1.Xi  m/ converges to the random variable X in distribution sense
such that the random variable ‘X’ is having Gaussian probability density function
with mean zero and constant variance.

Chapter 3
Random Process
3.1
Introduction
The mapping of the experimental outcomes s 2 sample space S to the set of Random
vectors is called as random process (Fig. 3.1). Individual random vector can be
treated as the signal which varies as the function of time. (i.e.) Thus the random
process can also be viewed as the mapping of the outcomes s 2 sample space S to
the set of signals as the function of time.
Example 3.1. An experiment has four equally likely outcomes 0,1,2,3 (i.e.) S D
f0; 1; 2; 3g. The random process Xt is deﬁned as Xt D cos.2  PI  s  t/ for all
s 2 S.
In the above example, the outcome of the experiment ‘0’ is mapped to the random
vector
[1 1 1 1 1 1 1 1 1 1 : : : 1]
Similarly the outcome of the experiment ‘1’, ‘2’, ‘3’ are mapped to the set of
random vectors as shown below. [Note that resolution of the variable ‘t’ is 1/1,000]
‘1’->
[1.0000 1.0000 0.9999 0.9998 0.9997 0.9995 0.9993 0.9990 0.9987 0.9984: : :
‘2’->
[1.0000 0.9999 0.9997 0.9993 0.9987 0.9980 0.9972 0.9961 0.9950 0.9936 : : :]
3-> [1.0000 0.9998 0.9993 0.9984 0.9972 0.9956 0.9936 0.9913 0.9887
0.9856 : : :]
The mapped vectors are plotted as the function of time which are shown in the
Fig. 3.1. Thus the random process can be viewed as the mapping of the outcomes of
the experiment to the set of signals as the function of time.
E.S. Gopi, Mathematical Summary for Digital Signal Processing
Applications with Matlab, DOI 10.1007/978-90-481-3747-3 3,
c Springer Science+Business Media B.V. 2010
123

124
3 Random Process
1
0
−10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1
0
−10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1
0
−10
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
2
s1=0
s2=1
s3=2
s4=3
1
00
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 3.1 Random process
3.2
Random Variable Xt1
The Random variable Xt1 is obtained by sampling across the random process
Xt at particular time instant ‘t1’. In the previous example the random variable
X0 X0:25 X0:5 are obtained by sampling the random process across the time instant
‘0’, ‘0.25’, ‘0.5’ respectively as shown in the Fig. 3.2.
The random variable X0 holds the values 0 with probability D 1
The random variable X00:25 holds the values 1 with probability D 1/4
0 with probability D 1/2
1 with probability D 1/4
The random variable X0:5 holds the values 1 with probability D 1/2
1 with probability D 1/2
3.3
Strictly Stationary Random Process with Order 1
Cumulative distribution function of the random variable Xt1C D Cumulative dis-
tribution function of the random variable Xt1 for all values of . (i.e.) FXt1.˛/ D
FXt1C.˛/ for all .
3.4
Strictly Stationary Random Process with Order 2
FXt1;Xt2.˛; ˇ/ D FXt1C; Xt2C.˛; ˇ/ for all ; t1; t2; .˛; ˇ/
If ˇ
tends to
! 1, order 1 stationarity holds

3.5 Wide Sense Stationary Random Process
125
2
0.1
0.2
0.4
0.3
0.5
0.6
0.7
0.8
0.9
1
1
0
0.1
0.2
0.4
0.3
0.5
0.6
0.7
0.8
0.9
1
1
0
−1
0.1
0.2
0.4
0.3
0.5
0.6
0.7
0.8
0.9
1
1
0
−1
0.1
0.2
0.4
0.3
0.5
0.6
0.7
0.8
0.9
1
1
0
−1
X0
X0.25
X0.5
Fig. 3.2 Random variable Xt1
In general if the random process is strictly stationary at order n, it is strictly
stationary for order n  1.
Example 3.2.(a) Xn: Functions of outcomes of tossing the coin. If the Probability
of Head is same for every toss, then the random process Xn is strictly stationary
of order 1.
(b) Example 2.1 is the non-stationary random process.
(c) Xt D A cos.2…ft C ‚/, ‘‚’ is the random variable independent of ‘t’ and
is uniformly distributed between 0 to 2…. This is strictly stationary process.
3.5
Wide Sense Stationary Random Process
The Random process Xt is said to be wide-sense stationary random process if it
satisﬁes the following conditions
 mX.t/ D E.Xt/ D constant for all time instant ‘t’.
 EŒXtXs/ D RX.t; s/ is the function of t  s.
(i.e.) EŒXtXs/ D RX.t; s/ D RX.t  s/ D RX./ for all values of ‘t’ and ‘s’.

126
3 Random Process
Example 3.3. Xt D A cos.2…ft C ‚/, ‘‚’ is uniformly distributed between 0
to 2….
mX.t/ D E.Xt/ D E.A cos.2…ft C ‚/
D A E.cos.2…ft C ‚/
(Note Expectation is computed at one particular time instant ‘t’ (i.e.) ‘t’ is ﬁxed.
so the function cos.2…ft C ‚/ is the function of the random variable ‘‚’ only.
(Represented as g(‚)).
Let the probability density function of the random variable ‘‚’ be represented as
f(‚).
Therefore
A E.cos.2…ft C ‚//
D A
Z
g.‚/f .‚/ d‚
D A
Z
cos.2…ft C ‚/ f.‚/ d‚
D A
Z 2…
0
cos.2…ft C ‚/
1
2  … d‚
D
A
2  …
Z 2…
0
cos.2…ft C ‚/ d‚
D 0
) mX.t/ is Constant
Also Autocorrelation is computed as follows
RX.s; t/ D RX.t C ; t/ D E.XtCXt/
D E.A E.cos.2…f.t C / C ‚/ A E.cos.2…ft C ‚//
D EŒA2 cos.2…f.t C / C ‚/ cos.2…ft C ‚/
D A2 E Œcos.2…f.t C / C ‚/ cos.2…ft C ‚/
D A2
2 EŒcos.2…f.2t C / C 2‚/ C cos.2…f/
D A2
2 EŒcos.2…f.2t C / C 2‚/ C A2
2 EŒcos.2…f/
Note that cos.2…f/ is constant as  and f are constant
So A2
2 EŒcos.2…f/ D A2
2 cos.2…f/

3.8 Joint Strictly Stationary of Two Random Process
127
I term
A2
2 EŒcos.2…f.2t C / C 2‚/ D 0 [Refer the steps involved in calculating
mX.t/]
) RX.s; t/ D RX.t C ; s/ D A2
2 cos.2…f/ D RX.s  t/ D RX./ is the
function of the difference u  s D .
Thus Xt D A cos.2…ft C ‚/, where ‘‚’ is the random variable which is
uniformly distributed between 0 to 2… is the wide sense stationary process.
3.6
Complex Random Process
The autocorrelation of the complex random process is given as
RX.t; s/ D E.XtX
S/:
If the Complex random process is Wide Sense Stationary process, then RX.t; s/ D
E XtX
S
 D RX.t  s/ D RX./ which is the function of difference t  s D 
3.7
Properties of Real and Complex Random Process
1. RX.0/ D E

X2
t

for real random process
RX.0/ D E.jXtj2/ for complex random process
2. RX./ D RX./ for real random process
RX./ D R
X./ for complex random process
Also Real ŒRX./ D Real ŒRX./ (Even symmetry)
Imaginary ŒRX./ D Imaginary ŒRX./ (Odd symmetry)
3. jRX./j  RX.0/ for both real and complex random process
(i.e.)
ˇˇE

XtCX

ˇˇ  E.jXtj2/ for complex random process
jE.XtCXt/j  E

X2
t

for real random process
3.8
Joint Strictly Stationary of Two Random Process
Consider two random process ‘Xt’ and ‘Yt’. They are said to be jointly strictly
stationary if it satisﬁes the following condition
FXt1;Y t2.˛; ˇ/ D FXt1C; Y t2C.˛; ˇ/ for all ; t1; t2; .˛; ˇ/:
Note that the two random process ‘Xt’ and ‘Yt’ are individually stationary.

128
3 Random Process
3.9
Jointly Wide Sense Stationary of Two Random Process
Consider two random process ‘Xt’ and ‘Yt’. They are said to be jointly Wide Sense
Stationary (W.S.S) if it satisﬁes the following condition
1. mX.t/ D E.Xt/ D constant for all time instant ‘t’
2. EŒXtXs/ D RX.t; s/ is the function of t  s.
(i.e.) EŒXtXs/ D RX.t; s/ D RX.t  s/ D RX./ for all values of ‘t’ and ‘s’
3. mY .t/ D E.Yt/ D constant for all time instant ‘t’
4. EŒYtYs/ D RY .t; s/ is the function of t  s.
(i.e.) EŒYtYs/ D RY .t; s/ D RY .t  s/ D RY ./ for all values of ‘t’ and ‘s’
5. EŒXtYs D RXY.t; s/ D RXY.t  s/ D RXY./ for all values of ‘t’ and ‘s’
Note
(a) EŒXtYs D RXY.t; s/ is called as cross correlation function.
(b) RXY.t; s/ D RYX.s; t/ for real random process
RXY./ D RYX./ for W.S.S. real random process
(c) RXY.t; s/ D R
YX.s; t/ for Complex random process RXY./ D R
YX./ for
W.S.S. Complex random process
3.10
Correlation Matrix of the Random Column Vector Xt
Ys
for the Speciﬁc ‘t’ ‘s’
E
Xt
Ys

jXt
Ysj

D
E X2
t

EŒXtYs
EŒYtXs
E

Y 2
t


3.11
Ergodic Process
Let ‘s1’, ‘s2’ : : :‘sn’ be the outcomes of the experiment. Let Xt.s1/ be the signal
as the function of time which is the map of the experiment S1. Similarly Xt.s2/ be
the signal corresponding to the experiment S2. The set of functions as the outcomes
of all the experiments forms the random process which is represented as Xt (see
Fig. 3.3). Also let Xt1 be the random variable which holds the values obtained by
collecting the values across the process at some arbitrary time instant ‘t1’.
Ensemble average of the random variable Xt1 is computed across the process and
is given by
Z 1
1
xfXt1.x/dx
where fXt1 is the probability density function of the random variable ‘Xt1’.

3.11 Ergodic Process
129
Fig. 3.3 Illustrations of Ergodic process
Time average of the arbitrary mapped signal Xt.s1/ is computed as follows
lim
T !1
 1
2T
 Z T
T
Xt.s1/ dt
In case of Wide Sense Stationary process, the ensemble average E.Xt1/ is constant..
If the above mentioned Ensemble average (constant) is equal to the Time average
computed for any arbitrary mapped signal Xt(s1) (say),the random process is called
Ergodic in mean.
(i.e.) The Random process Xt is said to be Ergodic in mean if
limT !1
 1
2T
 Z T
T
Xt.si/ dt D
Z 1
1
xfXt1.x/dx D constant for all ‘i’:
Note that Random process Xt must be the W.S.S. process if it is Ergodic process.
Similarly the random process is said be Ergodic in Auto correlation if the random
process satisﬁes for the following condition. Let Xt be the W.S.S process. Ensemble
average in auto correlation computation is given as
RX./ D E.XtCXt/ D
“
xy fXtCXt .x; y/dx dy
Time average in auto correlation computation is given as
lim
T !1
 1
2T
 Z T
T
Xt.s1/XtC.s1/ dt

130
3 Random Process
If Ensemble average is equal to the time average in auto correction computation, the
random process is said to be Ergodic in autocorrelation. (i.e.) The random process
Xt is said to be Ergodic in autocorrelation if
limT !1
 1
2T
 Z T
T
Xt.si/XtC.si/ dt
D
“
xy fXtCXt .x; y/dx dy D RX./ D function of ‘’ for all ‘i’
Example 3.4. Xt D A cos.2…ft C ‚/, ‘‚’ is uniformly distributed between 0
to 2….
One particular map corresponding to the experimental outcome S1 is given as
Xt.s1/ D A cos.2…ft C ‚.s1//
Ensemble average is given as E.Xt/ D 0 (see Example 3.3)
Time average
lim
T !1
 1
2T
 Z T
T
Xt.s1/ dt
D limT !1
 1
2T
 Z T
T
A cos.2  …  f  t C ‚.s1// dt
D limT !1
 1
2T

A sin.2  …  f  T C ‚.s1//=.2  …  f/

A sin.2  …  f  T C ‚.s1//=.2  …  f// is bounded between two constants.
Say between ‘M1’ and ‘M2’.
.i:e:/
limT !1
 1
2T

M1  limT !1
 1
2T

A sin.2  …  f  T
C‚.s1//= .2  …  f/

 limT !1
 1
2T

M 2 ) 0
 limT !1
 1
2T

A sin.2  …  f  T
C ‚.s1//=.2  …  f/

 0

3.11 Ergodic Process
131
) limT !1
 1
2T

A sin.2  …  f  T
C‚.s1//=.2  …  f /

D 0
) limT !1
 1
2T
 Z T
T
Xt.s1/ dt D 0
Thus Ensemble average D Time average D constant D 0 and hence the random
process Xt D A cos.2…ft C ‚/ (where ‘‚’ is uniformly distributed between
0 to 2…) is Ergodic in mean. In the same manner, it can be shown that Xt D
A cos.2…ft C ‚/ (where ‘‚’ is uniformly distributed between 0 to 2…) is
Ergodic in autocorrelation as shown below.
Ensemble average in auto correlation is given as RX./ D E.XtCXt/ D
A2
2 cos.2…f/ (see Example 2.3)
Time average in auto correlation is computed as
lim
T !1
 1
2T
 Z T
T
Xt.s1/XtC.s1/ dt
D limT !1
 1
2T
 Z T
T
A cos.2  …  f  .t C / C ‚.s1//
A cos.2  …  f  t C ‚.s1// dt
D limT !1
 1
2T
 Z T
T
.A2=2/Œcos.2  …  f  .2t C / C 2 ‚.s1//
C cos.2  …  f  / dt
First term
limT !1
 1
2T
 Z T
T
.A2=2/Œcos.2  …  f  .2t C / C 2 ‚.s1// dt D 0
(As described in Time average in mean)
) limT !1
 1
2T
 Z T
T
Xt.s1/XtC.s1/ dt
D limT !1
 1
2T
 Z T
T
.A2=2/Œcos.2  …  f  .2t C / C 2 ‚.s1//
C cos.2  …  f  / dt
D limT !1
 1
2T
 Z T
T

A2=2

Œcos.2  …  f  / dt
D limT !1
 1
2T

.A2=2/Œcos.2  …  f  /
Z T
T
dt

132
3 Random Process
D limT !1
 1
2T

.A2=2/Œcos.2  …  f  /.2T/
D limT !1.A2=2/Œcos.2  …  f  /
D .A2=2/Œcos.2  …  f  /
Thus Ensemble average in autocorrelation D Time average in auto correlation
D .A2=2/Œcos.2  …  f  / is the function of ‘0:
Hence the random process Xt D A cos.2…ft C ‚/ (where ‘‚’ is uniformly
distributed between (0 to 2…) is Ergodic in autocorrelation
3.12
Independent Random Process
Let Xt and Yt be two random processes. Let X D ŒXt1 Xt2 Xt3 Xt4 : : : Xtn be the
random vector obtained by sampling across the random process Xt at time instants
t1, t2, : : : tn. Similarly the random vector Y D ŒYt1 Yt2 Yt3 Yt4 : : : Ytn is obtained
by sampling across the random process Yt.
The random process Xt and Yt are independent if the random vectors X and Y
are independent random vectors.
.i:e:/ FXY ./ D FX./FY ./
3.13
Uncorrelated Random Process
Let Xt and Yt be two random processes. Let X D ŒXt1 Xt2 Xt3 Xt4 : : : Xtn be the
random vector obtained by sampling across the random process Xt at time instants
t1, t2,: : : tn. Similarly the random vector Y D ŒYt1 Yt2 Yt3 Yt4 : : : Ytn is obtained
by sampling across the random process Yt.
The random process Xt and Yt are uncorrelated if the cross-covariance matrix
computed as E ..X  mX/T .Y  mY // is the diagonal matrix.
3.14
Random Process as the Input and Output of the System
Consider the Linear time invariant system described by its impulse response h(t)
(Fig. 3.4). Let Xt be the W.S.S. random process which is given as the input to the
system h(t) and Yt be the corresponding output random process which is also W.S.S.
Then R 1
1 h./Xt d converges to the output random process in mean square
sense. (i.e.)

3.14 Random Process as the Input and Output of the System
133
Xt
Linear Time invariant
System
Yt
Fig. 3.4 Random process as the input and output of the LTI system
Yt m:s:s D
Z 1
1
h./Xtd
) E.ŒYt 
Z 1
1
h./Xtd2/ D 0
Properties
1. Mean of the output random process E.Yt/ is constant
Proof.
E.Yt/ D E
Z 1
1
h./Xtd
D
Z 1
1
h./E .Xt/ d
D
Z 1
1
h./mX d
D mX
Z 1
1
h./ d
D constant:
2. RYX .t; s/ D E.YtXs D RYX./ D h./  RX./
Proof.
RYX.t; s/ D E.YtXs D RYX./
D E
Z 1
1
h./Xtd

Xs

D
Z 1
1
h./E.XtXs/ d
D
Z 1
1
h./RX.t    s/ d
D
Z 1
1
h./RX.t  s  / d
D
Z 1
1
h./RX.  / d

134
3 Random Process
 D t  s.say/
) RYX./ D h./  RX./
3. RY ./ D RX./h./  h./
RY .t; s/ D E.YtYs D RY ./
D E

Yt
Z 1
1
h./Xsd

D
Z 1
1
h./E.YtXs/ d
D
Z 1
1
h./RYX.t  s C / d
D
Z 1
1
h./RYX. C / d
 D t  s.say/
Let • D 
) RY ./ D
Z 1
1
h.•/RYX.• C / d
D
Z 1
1
h0.•/RYX.ı C / d
Let
h0.•/ D h.•/.say/
) RY ./ D h0./  RYX./
) RY ./ D h./  RYX./
We know,
RYX./ D h./  RX./
) RY ./ D h./  h./  RX./
Note that the above mentioned properties are true for the complex random
process also.
3.15
Power Spectral Density (PSD)
The power spectral density of the W.S.S. random process Xt is deﬁned as the Fourier
transformation of its autocorrelation function RX./. (i.e.)

3.15 Power Spectral Density (PSD)
135
SX.f / D
Z 1
1
RX./ ej2˘f d
Consider the Linear time invariant system described by its impulse response h(t).
Let Xt be the W.S.S. random process which is given as the input to the system h(t)
and Yt be the corresponding output random process which is also W.S.S.
We have shown that
RY ./ D h./  h./  RX./
Taking Fourier transformation on both sides
) SY .f / D H.f /H.f /SX.f /
) SY .f / D H.f /H .f /SX.f /.Assuming h./ is the real function/
) SY .f / D jH.f /j2SX.f /
We have also shown that
RYX./ D h./  RX./
Taking Fourier transformation on both sides
) SYX.f / D H.f /SX.f /
The power spectral density SYX .f / is called as Cross power spectral density.
Properties of power spectral density
1. SX .0/ D R 1
1 RX./ d
2. RX .0/ D
R 1
1 SX.f / d D E

Xt 2
Note that mean square value is obtained from all frequencies of the spectral
density
3. SX.f / is always real for all values of ‘f’ and 	 0
SX.f / D
Z 1
1
RX./ ej2˘d
D
Z 0
1
RX./ ej2˘f d C
Z 1
0
RX./ ej2˘f d
D
Z 0
1
RX./ ej2˘d C
Z 1
0
RX./ ej2˘d
Let u D 
D
Z 1
0
RX.u/ ej2˘f udu C
Z 1
0
RX./ ej2˘f d

136
3 Random Process
D
Z 1
0
RX.u/ ej2˘f udu C
Z 1
0
RX./ ej2˘f d
D
Z 1
0
R
X.u/ ej2˘f udu C
Z 1
0
RX./ ej2˘f d
[Using the property of complex auto correlation .i:e/ RX.u/ D R
X.u/]
D
Z 1
0
R
X.u/ ej2˘f udu C
Z 1
0
RX./ ej2˘f d
Z 1
0
ŒRX./ej2˘f d
Z 1
0
RX./ ej2˘f d
which is the real value and hence power spectral density SX.f / is always
positive. (i.e.) SX.f / 	 0
4. If the W.S.S. random process Xt is real, then SX.f / D SX.f /
5. Consider the W.S.S. random process Xt and the corresponding autocorrelation
function and spectral density function are given as RX./ and SX.f / respec-
tively and SX .f / D 0 for jfj > W, then the random process Xt is said to be
band limited with Bandwidth ‘W’
6. Consider the system which is Band limited with bandwidth ‘W’ (Fig. 3.5).
Consider the band limited W.S.S. random process Xt which is given as the
input to the system. The output of the system is the random process Yt, then
E.ŒXt  Yt/2/ D 0
Suppose Wt and Vt are the responses of the system H1.f / and H2.f / to the band
limited process ‘Xt’ (Fig. 3.6)
If H1.f / D H2.f / for all jf j  W , then Wt and Vt are equal in Mean Square
Sense. (i.e.) EŒ.Wt  Yt/2 D 0
Xt
Yt
−w
w
|H(f)|
f
Fig. 3.5 Transfer function of the band limited system

3.16 White Random Process (Noise)
137
Xt
Wt
+
Vt
−
H1(f)
H2(f)
Fig. 3.6 Response of the system to the band limited process
Proof. Let Zt D .Wt  Yt/
) E

.Wt  Yt/2	
D E

Zt
2	
We know that,
SZ.f / D SX.f /jH.f /j2
where H.f / D H1.f /  H2.f /
Also E

Zt 2	
D Rz.0/
D
Z 1
1
SZ.f / df
D
Z 1
1
SX.f /jH.f /j2 df
Because H .f/ D 0 for all jf j  W and SX.f / D 0 for all jf j > W
Thus EŒ.Wt  Yt/2 D 0
3.16
White Random Process (Noise)
The random process Xt is said to be White Gaussian Random process, if Mean D
mX .t/ D E.Xt/ D 0 for all time instant ‘t’
RX.t; s/ D RX.£/•.£/No
2
SX.f / D constant:

138
3 Random Process
White noise has zero mean and inﬁnite variance, which cannot be realized in
practical situation. The white noise obtained in real time can be viewed as the ﬁl-
tered white noises through the Band limited ﬁlter whose frequency response ﬂat
over the bandwidth of interest.
3.17
Gaussian Random Process
The Random vector Xt is said to be Gaussian Random Process if the random vector
obtained by sampling across the process Xt at time instants t1, t2, t3, : : : tn (Repre-
sented as ŒXt1 Xt2 : : : Xtn) is a jointly Gaussian random vector.
Properties
1. If the input to the Linear, stable system is Gaussian random process, then output
is also Gaussian random process. Note that the system can also be Time variant
2. If Xt is Gaussian and W.S.S. it is also Strictly Stationary
3. The random process Xt is said to be White Gaussian Random process, if
 Xt W.S.S. Gaussian Random process
 Mean D mX.t/ D E.Xt/ D 0 for all time instant ‘t’
 RX.t; s/ D RX./ı./ No
2
 SX.f / D constant
Note that there can be White Non-Gaussian Random process.
Example 3.5. Gaussian Random process
1. Wiener process [Non-stationary random process]
 Mean D mX.t/ D E.Xt/ D 0 for all time instant ‘t’
 RX.t; s/ D 
2 min.t; s/ C m2ts for all t, s 	 0
2. Gauss-Markov process[Stationary random process]
 mX.t/ D 0
 RX.t; s/ D 
2eˇjtsj; 
2; ˇ > 0 for all t, s 	 0
 Let t1,t2,t3 be the three samples instance of the Gauss Markov random process
with t3 > t2 > t1, then fXt3=Xt2 Xt1 D fXt3=Xt2
In general conditional density of the random variable obtained at particular time
instant tn given the random variables obtained at set of time instants t1,t2,t3,..tn 
1 with t1 < t2 < : : : tn  1, is equal to the conditional density function of the
random variable obtained at time instant tn given the random variable obtained at
time instant tn  1.
(i.e.) Let t1,t2,t3,..tn be the three samples instance of the Gauss Markov random
process with tn > tn1 > tn2 : : : t2 > t1, then fXtn=Xtn1 Xtn2 ::: Xt1 D fXtn=Xtn1

3.19 Wide Sense Cyclo Stationary Random Process
139
3.18
Cyclo Stationary Random Process
The random process Xt is said to be strictly stationary random process
FXt1;Xt2;Xt3;Xt4:::;Xtn(˛1; ˛2; ˛3; : : : ˛n)
D FXt1C;Xt2C;Xt3C;Xt4C:::CXtnC(˛1; ˛2; ˛3; : : : ˛n)
for all £; ’1; ’2; ’3; : : : ’n
If  D n T, where n D : : :  2; 1; 0; 1; 2 : : : and T is constant, the random
process is said to be cyclo stationary random process with period ‘T’.
Example 3.6. Xt D P1
nD1 Anp.t  nT/ is the cyclo stationary random process,
where An is a discrete time strictly stationary process. p(t) is the function of ‘t’.
3.19
Wide Sense Cyclo Stationary Random Process
The random process Xt is W.S. Cyclo stationary random process if
mX.t/ D 0
RX.t; s/ D RX.t C nT; s C nT /
Example 3.7. Consider the Discrete wide sense stationary random process An that
takes the values C1 and 1 with equal probability at all time instants (Stream of
Binary data). Let the pulse used to modulate the above mentioned binary stream is
p(t) having nonzero values for 0  t  T . The random process deﬁned as Xt D
P1
nD1 Anp.t  nT/ is cyclo stationary.
Let this random process be the input to the channel input and the random process
Yt is the random process of the output of the channel (i.e.) in the receiver section.
The random process Yt is represented as follows. Yt D P1
nD1 Anp.t  nT  /,
where  be the time delay of the pulse p (t) which can be viewed as the random
variable which is uniformly distributed between 0 to T. The random process Yt is
the Wide Sense Stationary random process.
Proof.
Xt D
X1
nD1 Anp.t  nT /
mX.t/ D E.Xt/ D
X1
nD1 E.An/p.t  nT /
D
X1
nD1 kp.t  nT / which is periodic with time
period ‘T0
) mX.t C T / D mX.t/
Xt D
X1
nD1 Anp.t  nT /

140
3 Random Process
RX.t; s/ D E.XtXs/ D
X1
nD1
X1
kD1 E.AnAk/p.t  kT /p.s  nT /
D
1
X
nD1
1
X
kD1
RA.n  k/p.t  kT / p.s  nT /
Changing the limit m D n  k
D
1
X
nD1
1
X
kD1
RA .n  k/ p .t  kT / p .s  nT /
D
1
X
nD1
1
X
mD1
RA.m/ p.t  nT C mT / p .s  nT /
) RX .t C T; s C T / D
X1
nD1
X1
mD1 RA .m/ p .t C T  nT C mT /
p .s C T  nT /
D
1
X
nD1
1
X
mD1
RA .m/ p .t  nT C mT / p .s  nT / D RX .t; s/
Hence Xt D P1
nD1 Anp.t  nT/ is cyclo stationary process.
The random process Yt D P1
nD1 Anp.t nT/ [ is uniformly distributed
between 0 to T] is wide sense stationary random process
Proof. Computation of mean
Yt D
X1
nD1 Anp.t  nT  / i
E.Yt/ D EŒE.Yt= D a/
D E ŒE.Yt= D a/
ŒE.Yt= D a/
D
X1
nD1 E.An/p.t  nT  a/
D
X1
nD1 kp.t  nT  a/
D mX.t  a/ŒBecause mX.t/ D
X1
nD1 kp.t  nT /
which is periodic with time period ‘T’
) EŒE.Yt= D a/
D EŒmX.t  a/
D
 1
T
 Z T
0
mX.t  a/ da
Note that mX.t/ is periodic with time period ‘T’ .mX.t  a/ is the shifted version
of mX.t/ and hence
 1
T
 R T
0 mX.t  a/da is constant.

3.19 Wide Sense Cyclo Stationary Random Process
141
E.YtCYt/ D EŒE.YtC Yt= D a/
E.YtCYt= D a/ D
X1
nD1
X1
mD1 Anp.t C   mT  a/
Amp.t  nT  a/
D
X1
nD1
X1
mD1 E.AnAm/p.t C   mT  a/
p.t  nT  a/
D
X1
nD1
X1
mD1 RA.n  m/p.t C   mT  a/
p.t  nT  a/
Let k D n  m
D
1
X
nD1
1
X
kD1
RA.k/p.t C   nT C kT  a/
p.t  nT  a/
D
1
X
nD1
1
X
kD1
RA.k/p.t C   nT C kT  a/
p.t  nT  a/
) EŒE.YtCYt= D a/
D
 1
T
 Z T
0
X1
nD1
X1
kD1 RA.k/p.t C   nT
CkT  a/p.t  nT  a/da
D
 1
T

1
X
KD1
RA.k/
1
X
nD1
Z T
0
p.t C   nT
CkT  a/p.t  nT  a/da
Let u D t  nT  a
D
 1
T

1
X
KD1
RA.k/
1
X
nD1
Z tnT
tnT T
p.u C  C kT /p.u/du
Consider P1
nD1
R tnT
tnTT p.u C  C kT/p.u/du
Let f.u/ D p.u C  C kT/p.u/
D    C
Z tC3T
tC2T
f .u/du C
Z tC4T
tC3T
f .u/du C : : :
Z t
tT
f .u/du
C
Z tT
t2T
f .u/du C
Z t2T
tT
f .u/du C   

142
3 Random Process
D
Z 1
1
f .u/du
D
Z 1
1
p.u C  C kT /p.u/ du
D
Z 1
1
p.u C  C kT /p.u/ du
D RP . C kT/ which is the autocorrelation of the deterministic signal pulse p(t).
)
 1
T

1
X
KD1
RA.k/
1
X
nD1
Z T
0
p.t C   nT C kT  a/p.t  nT  a/da
) RY ./ D
 1
T
 X1
KD1 RA.k/RP . C kT /
Which is the function of t  s D  and hence Yt D P1
nD1 Anp.t  nT  / [‘’
is uniformly distributed between 0 to T.] is Wide Sense Stationary process.
Taking Fourier transform on both sides of the equation
RY ./ D
 1
T
 X1
KD1 RA.k/RP . C kT /
we get
SY .f / D
 1
T

1
X
KD1
RA.k/ej2˘f kT jP.f /j2
Where P(f) is the Fourier transformation of the autocorrelation function p(t).
Also note that RP .t/ D p.t/p.t/ and hence Fourier transform of RP ./ is
P.f /j2
3.20
Sampling and Reconstruction of Random Process
Consider the Band limited continuous random process Xt with bandwidth ‘W’ (i.e.)
SX.f / D 08jfj > W, which is sampled with sampling rate
 1
T

	 2W to obtain the
discrete random process XnT Then the sequence of random process Xt .N/ converges
to random process Xt in mean square sense as N> 1, where
Xt .N/ D
XnDN
nDN XnT sinc
 t
T

 n

:.i:e:/ E
ˇˇˇXt  Xt .N/ˇˇˇ
2
D 0 as N-> 1:

3.20 Sampling and Reconstruction of Random Process
143
Proof. The requirement is to show that E

jXt  Xt .N/j2
D 0 as N > 1.
E
ˇˇˇXt  Xt .N/ˇˇˇ
2
D E
 ˇˇˇˇXt 
XnDN
nDN XnT sinc
 t
T

 n
ˇˇˇˇ
2!
D E

jXtj2

XN
nDN E

XtX
nT

sinc
 t
T

 n


XN
nDN E

XntX
t

sinc
 t
T

 n

C
XN
nDN
XN
mDN E

XnT X
mT

sinc
 t
T

 n

sinc
 t
T

 m

Note
Note that if Xt is bandlimited then the corresponding autocorrelation function
(i.e.) RX.t/ bandlimited and RX.t/ can be viewed as the band limited signal and
hence using sampling theorem, RX.t/ can be reconstructed using the following
formula
RX.t/ D
X1
nD1 RX.nT //sinc
 t
T

 n

The shifted version of the signal (i.e.) RX.t  / can be reconstructed using the
formula as shown below.
RX.t  / D
X1
nD1 RX.nT  //sinc
 t
T

 n

At t D 
RX.0/ D
X1
nD1 RX.nT  t//sinc
 t
T

 n

       .1/
Similarly we can show that
RX.0/ D
X1
nD1 RX.t  nT //sinc
 t
T

 n

       .2/
Consider the four terms in the expanded form of the equation E

jXt  Xt .N/j2
(as shown above). Applying the limit N ! 1 individually on the four terms we get
the following.
First term E.jXtj2/ D RX.0/
Second term W
XN
nDN E.XtX
nT /sinc
 t
T

 n


144
3 Random Process
lim
N!1
N
X
nDN
RX.t  nT //sinc
 t
T

 n

D RX.0/
(From Eq. (2))
Third term W
XN
nDN E.XnT X
t /sinc
 t
T

 n

lim
N!1
N
X
nDN
RX.nT  t//sinc
 t
T

 n

D RX.0/
(From Eq. (1))
Fourth term
XN
nDN
XN
mDN E.XnT X
mT /sinc
 t
T

 n

sinc
 t
T

 m

D
XN
nDN
XN
mDN RX.nT  mT /sinc
 t
T

 n

sinc
 t
T

 m

Consider the term
lim
N!1
N
X
mDN
RX.nT  mT /sinc
 t
T

 n

D RX.t  mT /
(From the Reconstruction formula of sampling theorem)
lim
N!1
N
X
mDN
RX.t  mT /sinc
 t
T

 m

D RX.0/
(From Eq. (2))
Thus the
lim
N!1 E
ˇˇˇXt  X.N/
t
ˇˇˇ
2
D E
 ˇˇˇˇXt 
XnDN
nDN XnT sinc
 t
T

 n
ˇˇˇˇ
2!
RX.0/  RX.0/  RX.0/ C RX.0/ D 0
Hence proved
3.21
Band Pass Random Process
The Random process is said to be Band pass Random process if its spectral density
have band pass frequency response.

3.21 Band Pass Random Process
145
Example 3.8. The Random process Xt
	D Xt I cos.2˘fct/Xt Q sin.2˘fct/ forms
the Band pass W.S.S. Random process if the following conditions are satisﬁed.
 Xt I and Xt Q are the Low pass random process
 They are Jointly W.S.S. Process
 RXI ./ D RXQ./
 RXI XQ./ D RXQXI ./
Proof. To make the random process Xt as the Wide sense stationary process,
RX.t; u/ should be the function of  D t  u
RX.t C ; / D E.XtCX/
Let A D XtCI cos.2˘fc.t C //  XtCQ sin.2˘fc.t C //
B D Xt I cos.2˘fct/  Xt Q sin.2˘fct/
DE(AB) consists of four terms.
I term: E.XtCI cos.2˘fc.t C //Xt I cos.2˘fct//
D E

XtCIXt I 1
2 Œcos.2˘fc.2t C // C cos.2˘fc/

D RXI ./1
2 Œcos .2˘fc.2t C // C cos.2˘fc/
Second term: E

XtC I cos.2˘fc.t C //Xt Q sin.2˘fct/

D E

XtC
IXt
Q 1
2 Œsin.2˘fc.2t C //  sin.2˘fc/

D RXI XQ./1
2 Œsin.2˘fc.2t C //  sin.2˘fc/
Third term: E

XtC Q sin.2˘fc.t C //Xt I cos.2˘fct/

D E

XtCQXt I 1
2 Œsin.2˘fc.2t C // C sin.2˘fc/

D RXI XQ./1
2 Œsin.2˘fc.2t C // C sin.2˘fc/
Fourth term: E

XtCQ sin.2˘fc.t C //Xt Q sin.2˘fct/

D E

XtCQXt Q 1
2 Œ cos.2˘fc.2t C // C cos.2˘fc/

D RXQ./1
2 Œ cos.2˘fc.2t C // C cos.2˘fc/
D
RXI ./ C RXQ./
2

cos.2˘fc/ C
RXI XQ./  RXQXI ./
2

sin.2˘fc/

146
3 Random Process
C
RXI ./ C RXQ./
2

cos.2˘fc.2t C //
C
RXI XQ./  RXQXI ./
2

sin.2˘fc.2t C //
To make the above expression independent of ‘t’ First and second term is already
independent of ‘t’. To make the third and fourth terms independent of ‘t’, the fol-
lowing conditions have to be satisﬁed.
RXI ./ D RXQ./
RXI XQ./ D RXQXI ./
Hence proved.
3.22
Random Process as the Input to the Hilbert
Transformation as the System
Let W.S.S random process Xt be the input to the system (Hilbert transform) whose
frequency response is as shown below (Figs. 3.7 and 3.8).
The Hilbert transform of the random process Xt is represented as c
Xt.
Properties
1. S OX.f / D SX.f /
S OX.f / D SX.f /jH.f /j2
Fig. 3.7 Transfer function
of the Hilbert transformation
H(f)
j
−j
t
Fig. 3.8 Hilbert
transformation system
Xt
h(t)
X^
t

3.22 Random Process as the Input to the Hilbert Transformation as the System
147
jH.f /j2 D 1 for all f .see Graph/
Hence S OX.f / D SX.f /
2. SX OX.f / D S OXX.f /
We know,
R OXX./ D RX./  h./
) R OXX./ D RX./  h./
The function RX./ is the even function. ) RX./ D RX./
The impulse response of the Hilbert transformation system is given as
h.t/ D
 1
˘t

for all t
) R OXX./ D RX./  h./
) R OXX./
Also we know R OXX./ D E.1
XtCXt/ D E.Xt1
XtC/ D RX OX./
) R OXX./ D RX
OX./
From the above
) RX
OX./ D R OXX./
Taking Fourier transformation on both sides, we get
SX OX.f / D S OXX.f /
3. E.Xtc
Xt/ D 0
Proof. From property 2 we get,
RX OX./ D R OX X./
) RX OX.0/ D R OX X.0/      .1/
Also we know
E.Xtc
Xt/ D E.c
XtXt/
) RX OX.0/ D R OX X.0/      .2/
From (1) and (2) we conclude RX OX.0/ D 0
) E.Xtc
Xt/ D 0

148
3 Random Process
4. Let Zt D Xt C j c
Xt, then
E.ZtCZt / D E..XtC C j1
XtC/.XtC  j b
XtC/
D E.XtCXt/  j E.XtCc
Xt/ C jE.1
XtCXt/E.1
XtCc
Xt/
) RZ./ D RX./  jRX OX./ C jR OXX./ C R OX./
From the property 1 and property 2, RX./ D R OX./
RX OX./ D R OXX./
D RX./ C 2jR OXX./
) RZ./ D 2RX./ C 2jRX./  h./
In frequency domain (By taking Fourier Transform), we get,
SZ.f / D 2SX.f / C 2jSX.f /H.f /
) SZ.f / D 2SX.f /.1 C jH.f// .See Figure 3-7/
) SZ.f / D 4SX.f / for f > 0
D 0 for f < 0
3.23
Two Jointly W.S.S Low Pass Random Process Obtained
Using W.S.S. Band Pass Random Process and Its Hilbert
Transformation
Let Xt be the W.S.S. Band pass Random process
Deﬁne Xt
C , Xt C j c
Xt
From the property 4 of Hilbert transformation, we get
SXC.f / D 4SX.f / for f > 0
D 0 for f < 0
Deﬁne QXt
	D Xt Cej2…fct
E. QXtC£ QXt/ D RX.£/ D E

XtC£Cej2…fc.tC£/ Xt Cej2…fc.t/
D E

Xt
CXt
Cej2˘fc.£/
) RX./ D RXC./ej2˘fc./
) SX.f / D SXC.f C fc/

3.23 Two Jointly W.S.S Low Pass Random Process
149
Let Xt 	 D Xt I C j Xt Q
) Xt
I D Real

Xt
Cej2˘fc.t/
D Real


Xt C j c
Xt

ej2˘fc.t/
D Xt cos .2  ˘  fc  t/ C c
Xt sin .2  ˘  fc  t/
Similarly
Xt Q D Imaginary.Xt Cej2˘fct/
D c
Xt cos.2  ˘  fc  t/  Xt sin.2  ˘  fc  t/
In the same fashion it can be shown that
Xt D Xt I cos.2  ˘  fc  t/  Xt Q sin.2  ˘  fc  t/ (Which is of the same
form as mentioned in Example 2.11)
The random process Xt I and Xt Q satisﬁes the following conditions.
 RXI ./ D RXQ./
 RXI XQ./ D RXQXI ./
 SXI .f / D SXQ.f / D
 1
4

ŒSX.f / C SX.f /
(Low pass frequency response)
 SXI XQ.f / D

j
4

ŒSX.f /  SX.f /
(Low pass frequency response)
Thus jointly wide sense stationary low pass random process Xt I and Xt Q are gen-
erated using the Bandpass random process Xt and its Hilbert transformation c
Xt as
mentioned below
Xt I D Xt cos.2  …  fc  t/ C c
Xt sin.2  …  fc  t/
Xt Q D c
Xt cos.2  ˘  fc  t/  Xt sin.2  ˘  fc  t/
Proof. RXI .£/ D E

XtC£I 
Xt I 
We know
Xt
	 D Xt
I C j Xt
Q
) Xt I D .Xt 	 C .Xt 	//
2
) E.XtCI.Xt I// D E
.XtC£	 C .XtC£	//
2
.Xt 	 C .Xt 	//
2

D 1
4E.XtC£	Xt 	/ C 1
4E.XtC£	.Xt 	// C 1
4E..XtC£	/Xt 	/
C1
4E..XtC£	/.Xt 	//
I term: 1
4E .XtC£	Xt 	/ D 0

150
3 Random Process
D 1
4E

XtC£Cej2pifc.tC£/Xt Cej2pifc£
D 1
4E


XtC£ C j 1
XtC£

ej2pifc.tC£/ 
Xt C j c
Xt

ej2pifc£
D ej2pifc.2tC£/ 1
4E

.XtC£ C j1
XtC£/

Xt C j c
Xt

D ej2pifc.2tC£/ 1
4 .E.XtC£Xt/ C jE

1
XtC£Xt

C jE

XtC£ c
Xt

 E.1
XtC£c
Xt/
D ej2pifc.2tC£/ 1
4
RX .£/ C jRX OX .£/ C jR OXX .£/  R OX .£/
From the property of Hilbert transformation mentioned in 3.22, we get the following
RX.£/ D R OX.£/
RX OX.£/ D R OXX.£/
Hence E.XtC£	Xt 	/ D 0
II term: 1
4E.XtC£	.Xt 	// D 1
4RX.£/
III term: 1
4E..XtC£	/Xt 	/ D 1
4E.Xt 	.XtC£	// D 1
4RX.£/
IV term: 1
4E..XtC£	/.Xt 	// D 0
This can be obtained in the same fashion as that of the I term.
Thus
RXI .£/ D 1
4 ŒRX.£/ C RX.£/
Taking Fourier transformation on both sides we get
SXI .f/ D 1
4ŒSX.f/ C SX.f /
In the same fashion we can show
SXQ.f/ D 1
4ŒSX.f/ C SX.f /
and SXI XQ.f / D
j
4

ŒSX.f /  SX.f /
Example 3.9. Consider the Band pass random process Xt whose spectral density is
as shown below (Figs. 3.9 and 3.10).
RX.£/ D E.XtC£	.Xt 	// D E

XtC£C.Xt C/ej2pifc.tC£/ej2pifc.t/
D RX C .£/ej2pifc.£/
) SX.f/ D SXC.f C fc/

3.23 Two Jointly W.S.S Low Pass Random Process
151
−40
1
Sx(f)
10
−10
40
fMHz
Fig. 3.9 Spectral density of the band pass random process
Fig. 3.10 Spectral density
of the XC
t
4
SX 
+(f)
10
40
fMHZ
Fig. 3.11 Spectral density
of the XC
t
4
−5
35
fMHz
SX ~(f)
We know that
SX C .f/ D 4SX.f / for f > 0
D 0 for f < 0
Let fc be chosen as 15 MHz so that the spectral density SX.f/ looks like the fol-
lowing (Fig. 3.11).
Thus the spectral density of the XI
t and the XQ
t
are as shown below (Figs. 3.12
and 3.13).
Note that SXI .f/; SXQ.f/ and j SXI XQ.f / are having low pass characteristics.

152
3 Random Process
2
1
−35 −30
−5
5
30 35
fMHZ
Sx |(f)
Fig. 3.12 Spectral density of the XI
t which is same as that of the spectral density XQ
t
-j SX | XQ(f)
−35 −30
−5
5
30 35 
fMHZ
Fig. 3.13 Spectral density jSXI XQ.f /

Chapter 4
Linear Algebra
4.1
Vector Space
1. The set of vectors forms the vector space ‘V’ over the Field ‘F’, if the vectors
belonging to that set satisfy the following properties
(a) If v1, v2 are the elements of the vector space ‘V’, then the vector deﬁned
by v1 C v2 must be the element of the vector space ‘V’.
(b) For some scalars f‘’’ ‘“’g 2 F.
.’ C “/v D ’v C “v:
.’ “/v D ’.“v/:
(c) There exists the identity scalar represented as ‘1’ such that 1:v D v:1 D v.
(d) There exists the additive identity vector represented as ‘0’ such that for any
vector ‘v’ 2 vector space, v C 0 D 0 C v D v.
(e) There exists the additive inverse vector for every vector ‘v’ 2 vector space
which is represented as ‘v’ such that vC.v/ D 0, where 0 is the additive
identity vector which is the element of the vector space.
(f) .v1 C v2/’ D ’ v1 C ’ v2.
(g) v1 C .v2 C v3/ D .v1 C v2/ C v3.
(h) ’ v 2 V where ’ 2 Fv 2 V.
2. ‘W’ is the subspace of the vector space ‘V’ .W  V/ if the elements of the
set ‘W’ is the subset of the vector space ‘V’ and satisﬁes all the properties
mentioned in the fact 1.
3. If W1  V and W2  V, then W1 \ W2  V. But W1 [ W2  V only when
W1  W2 or W2  W1.
4. The set of vectors fv1, v2, v3, v4: : : vng are said to be linearly indepen-
dent if ’1 v1 C ’2 v2 C ’3 v3 C ’4 v4 C : : : ’n vn D 0 for some scalars
f’1; ’2; ’3 : : : ’ng, then ’1 D ’2 D ’3 D : : : ’n D 0.
5. If the set of vectors fv1,v2,v3,: : :vng are linearly dependent if any one of the
vector in the set is represented as the linear combinations of other vectors.
6. The set of vectors fv1,v2,v3,: : :vng forms the Generating set of the vector
space ‘V’ if any vector in the vector space ‘V’ can be represented as the linear
E.S. Gopi, Mathematical Summary for Digital Signal Processing
Applications with Matlab, DOI 10.1007/978-90-481-3747-3 4,
c Springer Science+Business Media B.V. 2010
153

154
4 Linear Algebra
combinations of the vectors in the Generating set. Also note that the Generating
set is the subset of the Vector space ‘V’.
7. Generating set which are linearly independent is called the basis of the vector
space ‘V’.
8. If fv1,v2,v3,: : :vng be the basis of the vector space ‘V’, then the set of .n C 1/
vectors in the vector space is always dependent.
9. The set of minimum number of the vectors which forms the Generating set of
the vector space ‘V’ is called as Minimal Generating Set.
10. The maximum number of independent vectors collected from the vector space
‘’V is called maximal independent set.
11. If fu1,u2,u3,: : :ung is the minimal generating set, then the maximal independent
set is fu1,u2,u3,: : :ung.
12. If fu1,u2,: : :ung is the maximal linear independent set then fu1,u2,u3,: : :ung is
the basis of the vector space ‘V’.
13. The number of elements of the set is called as the cardinal number of the set.
The cardinal number of the Basis set is called dimension (dim) of the vector
space ‘V’.
14. If W is the subspace of V.W  V/ then
(a) the Basis of W  Basis of V.
(b) Any basis of ‘W’ is extended to the basis of ‘V’.
(c) dim.W/  dim.V/.
15. If W1  V and W2  V then
(a) dim.W1 [ W2/ D dim.W1/ C dim.W2/  dim.W1 \ W2/.
(b) Let the basis for the subspace W1 \ W2 is fu1,u2,u3,: : :ukg.
The basis for the subspace W1 is obtained by extending the basis of the
subspace W1 \ W2 as fu1,u2,: : :uk,w1,w2,w3,: : :wmg. Similarly the basis
for the subspace W2 is obtained by extending the basis of the subspace
W1 \ W2 as fu1,u2,u3,: : :uk,v1,v2,v3,: : :vng. Note that the dimension of
the vector space W1 is k C m and the dimension of the vector space W2 is
k C n.
(c) The basis of the vector space ‘V’ is obtained as fu1,u2,: : :uk,w1,w2,
w3, : : :wm,v1,v2,v3,: : :vmg.
4.2
Linear Transformation
1. Let ‘U’ and ‘V’ be the vector spaces over the ﬁeld F.A Map T: V ! V is called
linear transformation if
(a) T.u C v/ D T.u/ C T.v/
(b) T.’ U/ D ’ T.u/
where u 2 U and v 2 V
The Linear map is graphically represented as follows (Fig. 4.1).

4.2 Linear Transformation
155
Fig. 4.1 Linear
transformation
T :U
V 
u1
u2
u3
u4
un
v1
v2
v3
v4
vm
Fig. 4.2 Injective
transformation
u1
v1
v2
vn
vn+1
vn+2
vn+3
vm
u2
un
2. The linear transformations are broadly classiﬁed as (a) ONE-ONE (Injec-
tive) transformation (b) ONTO (Surjective) transformation (c) Isomorphic
(Bijective) transformation.
(a) Injective transformation (Fig. 4.2)
A Linear map T: U ! V is said to be injective if T.u1/ D T.u2/
) u1 D u2
In this case ker.U/ D f0g. Also note that dim.U/ <D dim.V/

156
4 Linear Algebra
Fig. 4.3 Surjective
transformation
v1
u1
u2
un
0
v2
vn
vn+1
vn+2
vn+3
vm
Fig. 4.4 Isomorphic
transformation
v1
v2
v3
v4
v5
vn
u1
u2
u3
u4
u5
un
(b) Surjective transformation (Fig. 4.3)
A Linear map T: V ! U is said to be surjective if, for any w 2 W, there
exists v 2 V, such that T.v/ D w.
dim.V/ <D dim.W/. In this case Ker.V/ ¤ f0g
(c) Isomorphic transformation (Fig. 4.4)
A Linear map T: V ! U is said to be isomorphic, if the transformation is both
Injective and Surjective in nature.
dim.V/ D dim.W/
dim.Ker.V// D 0
Ker.V/ D f0g

4.2 Linear Transformation
157
T:V
Ker(V)
Im(V)
0
W
Fig. 4.5 Illustrations of the kernel and Image of the vector space
3. Let the linear transformation T: V ! W is deﬁned as the linear map from the
vector space ‘V’ to ‘W’ (Fig. 4.5).
(a) The image of the vector space ‘V’ represented as Im(V) is the set of vectors
which are linearly mapped from all the vectors in the vector space V. Note
that Im(V) is the subspace of the vector space ‘W’.
Im.V/ D fTu; for all u 2 Vg
(b) The Kernel of the vector space ‘V’ represented as Ker(V) is the set of vec-
tors in the vector space ‘V’ which are mapped to the zero vector (Additive
identity) in the vector space ‘W’. Note that the kernel of the vector space
‘V’ is the subspace of the vector space ‘V’
Ker.V/ D fu such that Tu D 0g
4. Properties of the linear transformation
(a) Two vector spaces are said to be isomorphic, if there exists the isomorphic
transformation between them.
(b) Isomorphic transformation between the vector space V and W exists only
when dim.V/ D dim.W/.
(c) The Linear transformation ‘T’ is one-one transformation if and only if the
Transformation takes independent sets into other linearly independent sets.
(d) Consider the Linear transformation T: U ! W. The transformation ‘T’ is
one-one transformation if there exists always pre-image (i.e.) if w 2 W,t
here exists u 2 U such that Tu D w.
(e) If (u1,u2,u3,: : :un) be the basis of the vector space U and (w1,w2,w3,: : :wn)
be the basis of the vector space ‘W’, then their exists the unique transfor-
mation T, such that Tu1 D w1; Tu2 D w2 : : : Tun D wn.

158
4 Linear Algebra
T:V
Ker(V)
Im(V)
0
W
Fig. 4.6 Rank-Nullity theorem
5. Rank-Nullity Theorem (Fig. 4.6)
Let the basis of ker(V) be fv1,v2,v3,: : :vng. Extend this to the basis of V as
fv1; v2; v3; : : : vn; vn C 1; vn C 2; : : : vmg:Then the the set of vectors
fT.vn C 1/; T.vn C 2/; T.vn C 3/ : : : T.vm/g forms the basis of Im.V/:
) dim.V/ D dim.Im.V// C dim.ker.V//
6. Let the basis of the vector space ‘V’ be v1,v2,v3,: : :vm and the basis
of the vector space ‘W’ be w1,w2,23,: : :wn. The set of all transforma-
tions from the vector space V to W is represented as L(V,W).L(V,W) is
the vector space with dimension dim(V)dim(W). The basis of the vector
space T(V,W) is represented as T11,T12,: : :T1n,T21,T22,T23,: : : T2n,T31,
T32,..T3n,: : :Tm1,Tm2,Tm3,: : :Tmn which satisﬁes the following condition.
Tij .vk/ D wj if i D k
D 0; otherwise
7. Let V be an n-dimensional vector space over F. Then the dual space V is an n-
dimensional vector space over F which consists of set of linear transformations
and satisﬁes the following conditions (Fig. 4.7).
(a) The Unique basis of the Dual space V is the set of transformations
fT1,T2,T3,: : :Tng
such that Ti.vj/ D •ij; where •ij D 1 if i D j
D 0; otherwise
(b) For any linear transformation T 2 V
T D
Xn
iD1 T .vi/Ti

4.2 Linear Transformation
159
1
1
0
0
0
0
0
0
V
V1
V2
T1
T2
Tn
VN
1
V*
Fig. 4.7 Illustration of the dual space
(c) For any vector v 2 V
V D
Xn
iD1 Ti.v/vi
Consider the arbitrary transformation T which is written as the linear combi-
nations of the vectors T1,T2, T3,: : :Tn as T D ’1T1 C ’2T2 C ’3T3 C
: : : ’nTn
T.v1/ D .’1T1 C ’2T2 C ’3T3 C : : : ’nTn/v1
D ’1T1.v1/ C ’2T2.v1/ C ’3T3.v1/ C : : : ’nTn.v1/
D ’1 C 0 C 0 C : : : C 0
) T.v1/ D ’1
Similarly T.v2/ D ’2; T.v3/ D ’3; : : : ; T.vn/ D ’n.
Consider the arbitrary vector V which is written as the linear combinations
of the vectors v1,v2,v3,: : :Vn as v D “1v1 C “2v2 C “3v3 C : : : C “nvn
Ti.v/ D Ti.“1v1 C “2v2 C “3v3 C : : : “ivi C : : : C “nvn/
D 0 C 0 C 0 C : : : “i C : : : 0
D “i
8. Set of all transformations acting on all the vectors in the subspace W  V to
get zeros are called Annihilator W0 (Fig. 4.8) dim.w/ C dim.w0/ D dim.V/.
Let the basis of w be fw1,w2,w3,: : :wkg. Extend the basis to the basis of V as
fw1,w2,w3,: : :wk, wk C1; : : :wng. fTkC1; TkC2; TkC3; : : : Tng is the basis
of the w0.
9. Any ‘k’ dimensional subspace is the intersection of .n  k/ subspaces with
dimension .n  1/ (Fig. 4.9).
10. Let ker.ﬁ/ D Ni. f is the linear combinations of f1,f2, f3,: : :fk if and only if
N1 \ N2 N3 : : : \ NK  N.

160
4 Linear Algebra
0
W
W0
V
V*
Fig. 4.8 Illustration of the annihilator
v
dim=k=2
dim=n=5
dim=n−1=5−1=4
Fig. 4.9 Illustration for the property 9 of the linear transformation
11. Consider the transformation from the vector space V to W as T: V ! W.
Consider the dual space for the vector space V and W be represented as V and
W respectively.
There exists the transformation denoted by Tt W W ! V such that the
transformation T and Tt satisﬁes the following conditions.
(a) Im.Tt/ D Ker.T/0 (Fig. 4.10)
(b) .Im.T//0 D Ker.Tt/ (Fig. 4.11)
4.3
Direct Sum
Let V1,V2,V3: : :Vn be the subspaces of the vector space V. The vector V is said to
have direct sum representation (i.e.) V D V1 ˚ V2 ˚ V3 ˚ : : : Vn if the following
conditions are satisﬁed

4.3 Direct Sum
161
Fig. 4.10 Illustration of the
property Im (Tt) D Ker(T)0
0
0
W
W*
Ker(T)
Im(Tt)
V
T
Tt
V*
Fig. 4.11 Illustration of the
property (Im(T)) 0 D Ker(Tt)
V
V*
0
T
Tt
0
W
Im(T)
Ker(Tt)
W*

162
4 Linear Algebra
1. Any vector v
2
V is written uniquely as the summation of the vectors
v1,v2,v3,: : :vn such that v1 2 V1; v2 2 V2; v3 2 V3; : : : vn 2 Vn.
2. Intersection of the vector space is the zero vector. \Vi D f0g.
3. If 0 is represented as the summation of the vectors belongs to V1; V2; : : : Vn,
then the vectors collected from the vector spaces V1; V2; : : : Vn are 0.
.i:e/ 0 D 0 C 0 C 0 C    0
4.4
Transformation Matrix
Consider the Isomorphic Linear transformation T from the vector space V to W.
T W V ! W. Let the basis of the vector space V and W are represented as
fv1; v2; v3; : : : vng and fw1; w2; w3; : : : wmg respectively. Linear transformation
T acting on v1 which is represented as T.v1/ 2 W can be represented as the linear
combination of w1,w2,w3..wm as follows
T.v1/ D ˛11w1 C ˛12w2 C ˛13  w3 C    ˛1mwm
T.v2/ D ˛21w1 C ˛22w2 C ˛23  w3 C    ˛2mwm
T.v3/ D ˛31w1 C ˛32w2 C ˛33  w3 C    ˛3mwm
: : :
T.vn/ D ˛n1w1 C ˛n2w2 C ˛n3  w3 C    ˛nmwm
Consider the arbitrary vector v 2 V which is represented as the linear combinations
of the basis vectors fv1; v2; v3; : : : vng as ˇ1v1Cˇ2v2Cˇ3v3C: : : ˇnvn
T .v/ D T .ˇ1  v1 C ˇ2  v2 C ˇ3  v3 C    ˇn  vn/
ˇ1 D T.v1/ C ˇ2  T .v2/ C ˇ3  T .v3/ C    ˇn  T .vn/
D ˇ1.˛11w1 C ˛12w2 C ˛13  w3 C    ˛1mwm/
Cˇ2.˛21w1 C ˛22w2 C ˛23  w3   ˛2mwm/
Cˇ3.˛31w1 C ˛32w2 C ˛33  w3 C    ˛3mwm/
C : : :
Cˇn.˛n1w1 C ˛n2w2 C ˛n3  w3 C    ˛nmwm/
D .ˇ1˛11 C ˇ2˛21 C ˇ3˛31 C : : : ˇn˛n1/w1
C.ˇ1˛12 C ˇ2˛22 C ˇ3˛32 C : : : ˇn˛n2/w2
C.ˇ1˛13 C ˇ2˛23 C ˇ3˛33 C : : : ˇn˛n3/w3
C : : :
C.ˇ1˛1m C ˇ2˛2m C ˇ3˛m3m C : : : ˇn˛nm/wm

4.4 Transformation Matrix
163
The transformed vector T(v) is represented as the linear combinations of the basis
vector .w1; w2; : : :wm/ with the coefﬁcients as mentioned above.
The scalar coefﬁcients used to represent the vector v using the basis of the vector
space V is given as .ˇ1; ˇ2; ˇ3; ˇ4; : : : ˇn/.
Similarly the scalar coefﬁcients used to represent the vector T(v) using the basis
of the vector space W is given as
..ˇ1  ˛11 C ˇ2  ˛21 C ˇ3  ˛31 C : : : ˇn  ˛n1/; .ˇ1  ˛12 C ˇ2 
˛22 C ˇ3  ˛32 C : : : ˇn  ˛n2/; .ˇ1  ˛13 C ˇ2  ˛23 C ˇ3  ˛33C : : : ˇn 
˛n3/; : : : .ˇ1  ˛1m C ˇ2  ˛2m C ˇ3  ˛m3m C : : : ˇn  ˛nm//
The scalar coefﬁcients which are used to represent the vector v in the vector space
V is related to the scalar coefﬁcients used to represent the vector T(v) in the vector
space W using the matrix as given below.
2
664
˛11
˛21
: : :
˛n1
˛12
˛22
: : :
˛n2
: : :
: : :
: : :
: : :
˛1m
˛2m
: : :
˛nm
3
775
The matrix mentioned above is called transformation matrix which is represented as
ŒT B2
B1
Note that the above transformation matrix is represented with respect to the basis
B1 in the vector space V and the basis B2 in the vector space W.
Trick to obtain the transformation matrix for the linear transformation
T: V ! W
Let fv1; v2; : : :vng and fw1; w2; : : :wng be the basis of the vector space V and W
respectively.
Obtain the transformation vector corresponding to the individual basis elements
of the vector space V. Let it be fT.v1/ T.v2/ T.v3/: : :T.Vn/g.
Represent the transformed vector as the linear combinations of the basis vectors
in the transformed domain W as follows
T.v1/ D ˛11w1 C ˛12w2 C ˛13  w3 C    ˛1mwm
T.v2/ D ˛21w1 C ˛22w2 C ˛23  w3 C    ˛2mwm
T.v3/ D ˛31w1 C ˛32w2 C ˛33  w3 C    ˛3mwm
: : :
T.vn/ D ˛n1w1 C ˛n2w2 C ˛n3  w3 C    ˛nmwm
Form the matrix with ﬁrst column ﬁlled with the scalar coefﬁcients which are used
to represent the transformed vector T(v1) (i.e.) f’11 ’12 ’13: : :’1mg. Similarly the
second column is ﬁlled up with the scalar coefﬁcients which are used to represent
the transformed vector T(v2) (i.e.) f’21 ’22 ’23 : : :’2mg.

164
4 Linear Algebra
4.5
Similar Matrices
Consider the Isomorphic Linear transformation T W V ! V.
Consider the basis B1 D fv1; v2; v3; : : :vng with respect to which the transforma-
tion matrix is represented as TB1
B1. Consider the basis B2 D fu1; u2; u3; : : :ung with
respect to which the transformation matrix is represented as TB2
B2.
The vector u1 2 V. Hence u1 can be written as the linear combinations of the
basis vectors B1 as mentioned below.
u1 D a11v1 C a21v2 C a31v3 C : : :an1vn
: : :
In general, un D a1nv1 C a2nv2 C a3nv3 C    C annvn
Consider the vector ŒxB2 D
x1
x2
x3
:::
xn
This indicates that the vector ŒxB2 is represented as the linear combinations of
the basis B2 as follows.
xB2 D x1u1 C x2u2 C x3u3 C    C xnun
) xB2 Dx1  Œa11v1 C a21v2 C a31v3 C    C an1vnC
x2  Œa12v1 C a22v2 C a32v3 C    C an2vn C   
xn  Œa1nv1 C a2nv2 C a3nv1 C    C annvn
) xB2 Dv1  Œa11x1 C a12x2 C a13x3 C    C a1nxnC
v2  Œa21x1 C a22x2 C a23x3 C    C a2nxn C   
vn  Œan1x1 C an2x2 C an3x3 C    C annxnC
Thus the vector [x] with reference to the basis B1 is represented as follows
ŒxB1 D
a11x1 C a12x2 C a13x3 C    C a1nxn
a21x1 C a22x2 C a23x3 C    C a2nxn
a31x1 C a32x2 C a33x3 C    C a3nxn
:::
an1x1 C an2x2 C an3x3 C    C annxn

4.5 Similar Matrices
165
The vector ŒxB1 can be rewritten as
ŒxB1 D
ˇˇˇˇˇˇˇˇˇˇˇˇˇ
a11
a12
a13
:
:
a1n
a21
a22 a23: :
:
a2n
a31
a32
a33
:
:
a3n
:
:
:
:
:
:
:
:
:
:
:
:
an1
an2
an3
:
:
ann
ˇˇˇˇˇˇˇˇˇˇˇˇˇ
x1
x2
x3
:
:
xn
) ŒxB1 D
ˇˇˇˇˇˇˇˇˇˇˇˇˇ
a11
a12
a13
:
:
a1n
a21 a22
a23: :
:
a2n
a31 a32
a33
:
:
a3n
:
:
:
:
:
:
:
:
:
:
:
:
an1
an2
an3
:
:
ann
ˇˇˇˇˇˇˇˇˇˇˇˇˇ
ŒxB2
) ŒxB1 D ŒMŒxB2; where
ŒM D
a11
a12
a13
:
:
a1n
a21 a22
a23: :
:
a2n
a31 a32
a33
:
:
a3n
:
:
:
:
:
:
:
:
:
:
:
:
an1
an2
an3
:
:
ann
Also as the matrix [M] is the invertible matrix
ŒxB2 D ŒM1 ŒxB1;
Consider the vector ŒxB2 is transformed to another vector using the transformation
matrix TB2
B2 as

TB2
B2 ŒxB2
	
B2 with reference to the basis B2.
Consider the vector ŒxB2 is represented with respect to the basis B1 is given as
ŒŒMŒxB2B1
The above mentioned vector with respect to the basis B1 is transformed using the
transformation matrix TB1
B1 as
˚
TB1
B1
	
ŒŒMŒxB2B1

B1
Note that the above vector is with respect to the basis B1. The obtained vector is
represented with respect to the basis B2 as follows
˚
M 1 ˚
TB1
B1
	
ŒŒM ŒxB2B1

B1

B2
(4.1)

166
4 Linear Algebra
This must be same as the transformed vector of the vector ŒxB2 obtained using the
transformation matrix TB2
B2 which is represented as
as

TB2
B2ŒxB2
	
B2
(4.2)
Comparing both the Eqs. (1) and (2)
We get TB2
B2 D ŒM1 
TB1
B1
	
ŒM
The matrices TB2
B2 and TB1
B1
	 are called as similar matrices.
In general, the two matrices A and B are said to be similar matrices, if there exists
the invertible matrix P such that B D ŒP 1 ŒA ŒP .
4.6
Structure Theorem
The transformation matrix for the particular linear transformation can be written in
different form corresponding to the different basis vectors. Structure theorem deals
with the technique for obtaining the simplest transformation matrix like diagonal
matrix, upper triangular matrix etc. such that computation of the transformation
becomes simpler and faster.
1. Consider the one-one transformation T W V ! W with dim .V/ D n and
dim .W/ D k; n < k Let fv1; v2; v3; : : :vng be the basis of the vector space V.
fT.v1/; T.v2/; T.v3/: : :T.vn/g exists in the vector space W. They are linearly in-
dependent. Extend the set fT.v1/; T.v2/; T.v3/: : :T.vn/; wnC1; wnC2; : : :wkg.
The transformation matrix with respect to the above mentioned basis will look
like below.
1
0
0
:
0
a1; n C 1
:
a1; k
0
1
0
:
0
a2; n C 1
:
a2; k
0
0
1
:
0
a3; n C 1
:
a3; k
0
0
0
:
0
a4; n C 1
:
a4; k
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
0
0
0
1
an; n C 1
:
an; k
2. Consider the onto transformation T W V
!
W with dim .V/
D
n and
dim .W/ D k; n > k. Let fv1; v2; v3; : : :vng be the basis of the vector space V.
fT.v1/; T.v2/; T.v3/: : :T.vk/g exists in the vector space W. They are linearly
independent. The transformation matrix associated with the above basis is
given as

4.6 Structure Theorem
167
1
0
0
:
0
0
:
0
0
1
0
:
0
0
:
0
0
0
1
:
0
0
:
0
0
0
0
:
0
0
:
0
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
0
0
0 1
0
:
0
3. Consider the isomorphism transformation T W V ! W with dim .V/ D n
and dim .W/ D n. Let fv1; v2; v3; : : :vng be the basis of the vector space V.
fT.v1/; T.v2/; T.v3/: : :T.vk/g exists in the vector space W. They are linearly in-
dependent. The transformation matrix associated with the above basis is given as
1
0
0
:
0
0
1
0
:
0
0
0
1
:
0
0
0
0
:
0
:
:
:
:
:
0
0
0
0
1
Note that the columns of the transformation matrix in all the three cases are
obtained using the technique described in the section Trick to obtain the trans-
formation matrix for the linear transformation T: V ! W.
4. Consider the transformation matrix T which is not of simple form.
(a) Form the polynomial by setting the equation det.T  œI/ D 0. The polyno-
mial thus obtained is called characteristic polynomial of the transforma-
tion matrix
(b) The roots of the Characteristic polynomial is called Eigen values. Let it be
œ1 œ2 œ3 œ4: : :œk Let us consider the characteristic polynomial for the trans-
formation matrix T be p.x/ D .xœ1/n1.xœ2/n2.xœ3/n3.xœ4/n4 : : :
.x  œn/nk
(c) If p(x) is the characteristic polynomial of the transformation matrix A (as
mentioned above), then p(T) is the zero matrix, where addition and multipli-
cation are performed in the usual matrix operation and the constant term ‘c’
in the polynomial p(A) is represented as cI, where ‘I’ is the identity matrix.
This is called Cayley-Hamilton theorem (i.e.).
p.T/ D .T œ1 I/n1.T œ2 I/n2.T œ3 I/n3.T œ4 I/n4 : : : .T œn I/nk D 0
(d) But there can be the polynomial q(x) with lesser degree compared to degree
of p(x) which satisﬁes the condition q.T/ D 0. This polynomial is called
minimal polynomial of the transformation matrix ‘T’ (i.e.).
q.T/ D .T œ1 I/m1.T œ2 I/m2.T œ3 I/m3.T œ4 I/m4 : : : .T œn I/mk D 0
where mk  nk8k

168
4 Linear Algebra
(e) Consider the minimal polynomial of the transformation matrix ‘T’ is of the
form in which the values of m1 D m2 D m3: : : D mk D 1.
q.T/ D .T  œ1 I/1.T  œ2 I/1.T  œ3 I/1.T  œ4 I/1 : : : .T  œn I/1 D 0
Consider .T  œ1 I/v D 0 ) T.v/ D œ1v, the transformed vector T(v) is
the scaled version of the vector v with scaling value œ1. The Vector satisfy-
ing the above form is called Eigen vector. The set of all vectors satisfying the
above equation forms the space and are called Eigen space. They are repre-
sented as Vœ1. The basis of such vector space satisfying the above mentioned
conditions are called Eigen basis of the transformation matrix T correspond-
ing to the Eigen value ‘œ1’.
Consider the transformation matrix T: V ! V. Vector space V is said to
be invariant space if for any vector v 2 V such that T.v/ 2 V.
Eigen values of the transformation matrix T are œ1; œ2; œ3; : : : œk. The
Eigen space corresponding to the Eigen values ‘œ1’; ‘œ2’; ‘œ3’; : : : ‘œ1k
are represented as Vœ1 Vœ2 Vœ3 : : : Vœk respectively. They are individually
the invariant subspace of the vector space V.
Suppose if the dim(V) D n, then dim(Vœ1) C dim(Vœ2) C dim(Vœ3) C
...dim(V^k) D n
Let The basis of the vector space Vœ1 (i.e.) the Eigen vectors be fv11 v12
v13 : : : v1ig. The basis of the vector space Vƒ2 is given as fv21 v22 v23 : : :
v2jg. Similarly the basis of the vector space Vƒk is represented as
fvk1 vk2 vk3 : : : vkmg. Then the basis of the vector space V is given
as fv11 v12 v13 : : : v1i v21 v22 v23 : : : v2j : : : vk1 vk2 vk3 : : : vkmg.
This indicate that the vector space V is obtained as the direct sum of the
vector spaces Vœ1 Vœ2 Vœ3 : : : Vœk. (i.e.)
V D Vœ1 ˚ Vœ2 ˚ Vœ3 : : : ˚ Vœk
T.v11/ D œ1 v11
T.v12/ D œ1 v12
T.v13/ D œ1 v13
: : :
T.v1i/ D œ1 v1i
T.v21/ D œ2 v21
T.v22/ D œ2 v22
T.v23/ D œ2 v23
T.v2j/ D œ2 v2j
: : :
T.vk1/ D œk vk1
T.vk2/ D œk vk2
: : :
T.vkm/ D œk vkm

4.6 Structure Theorem
169
The transformation matrix associated with the above mentioned basis is
given as follows.
œ1
0
:
0
0
0
:
:
0
0
0
œ1
:
0
0
0
:
:
0
0
0
0
:
:
:
:
:
:
0
0
0
0
:
:
:
:
:
:
0
0
0
0
:
:
:
:
:
:
0
0
0
0
:
œ1
0
0
:
:
0
0
0
0
:
0
œ2
0
:
:
0
0
0
0
:
0
0
œ2
:
:
0
0
:
:
:
:
:
:
:
:
:
0
:
:
:
:
:
:
:
:
:
0
:
:
:
:
:
:
:
:
œk
0
0
0
0
0
0
0
0
0
0
œk
The transformation matrix is the square matrix. Also the diagonal ele-
ments of the matrix is ﬁlled up with the Eigen values Œœ1 œ1 œ1 : : : œ1 ::
œ2 œ2 œ2 : : : œk œk œk
(f) Consider the minimal polynomial of the transformation matrix ‘T’
q.T/ D .T œ1 I/m1.T œ2 I/m2.T œ3 I/m3.T œ4 I/m4 : : : .T œn I/mk D 0
where mk  nk 8k
Also consider the vector satisfying the condition .T œ1 I/m1v D 0 (i.e.) the vec-
tor ‘v’ 2 ker..T œ1 I/m1/. The vector v is called as Generalized Eigen vector.
The set of all the vectors satisfying the above condition is called Generalized
Eigen space. The Generalized Eigen space corresponding to the Eigen value
‘œ1’ is represented as V.œ1/.
Consider the arbitrary vector v11 2 V.œ1/. By the deﬁnition of minimal
polynomial
.T  œ1 I/ v11 ¤ 0
) .T  œ1 I/ v11 D v12
) T.v11/ D œ1v11 C v12
Also .T  œ1 I/2v11 D .T  œ1 I/.T  œ1 I/v11 D v13
) .T  œ1 I/v12 D v13
) T.v12/ D œ1v12 C v13
Similarly
T.v13/ D œ1v13 C v14
T.v14/ D œ1v14 C v15
T.v15/ D œ1v15 C v16
T.v16/ D œ1v16 C v17

170
4 Linear Algebra
: : :
Also .T  œ1 I/m1v1 D .T  œ1 I/.T  œ1 I/m11v1 D 0
But .T  œ1 I/m11v1 D vm1
) .T  œ1 I/ vm1 D 0
) T.vm1/ D œ1 vm1
Thus the independent vectors fv1m2; v1m21; v1m22; v1m23; v1m24; v1m25
: : : v11g forms the basis for the vector space V.œ1/. They are called Jordon basis
Similarly fv2m2; v2m21; v2m22; v2m23; v2m24; v2m25 : : : v21g forms the
basis for the vector space V.œ2/ satisfying the similar conditions as de-
scribed above.
In the same fashion fvkm2; vkm21; vkm22; vkm23; vkm24; vkm25 : : : vk1g
forms the basis for the vector space V.œk/ satisfying the similar conditions as
described above.
Thus the set of independent vectors
fv1m2; v1m21; v1m22v1m23; v1m24; v1m25 : : : v11; v2m2;
v2m21; v2m22; v2m23; v2m24; v2m25 : : : v21; : : : vkm2; vkm21;
vkm22; vkm23; vkm24; vkm25 : : : vk1g forms the basis of the vector space V.
Suppose the dim(V) D n, then dim(V(œ1// C dim.V.œ2// C dim.V.œ3// C
: : : dim.V.œK// D n
This indicate that the vector space V is obtained as the direct sum of the vector
spaces V.œ1/; V.œ2/; V.œ3/; ::V.œk/ (i.e.) V D V(œ1/˚V(œ2/˚V(œ3/ : : : ˚
V(œk/
The transformation matrix with respect to the above mentioned basis is given as
œ1
1
0
0
:
0
0
0
:
:
:
0
0
0
0
0
œ1
1
0
:
0
0
0
:
:
:
0
0
0
0
0
0
œ1
1
:
0
0
0
:
:
:
0
0
0
0
0
0
0
œ1
:
0
0
0
:
:
:
0
0
0
0
0
0
0
0
:
0
0
0
:
:
:
0
0
0
0
:
:
:
:
:
:
:
:
:
:
:
0
0
0
0
:
:
:
:
:
:
:
:
:
:
:
0
0
0
0
:
:
:
:
:
:
:
:
:
:
:
0
0
0
0
0
0
0
0
:
œ2
1
0
:
:
:
0
0
0
0
0
0
0
0
:
0
œ2
1
:
:
:
:
:
:
0
0
0
0
0
:
0
0
œ2
:
:
:
:
:
:
0
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
:
:
:
:
:
:
:
:
:
:
:
œk
1
0
0
:
:
:
:
:
:
:
:
:
:
:
0
œk
1
0
0
0
0
0
:
0
0
0
:
:
:
0
0
œk
1
0
0
0
0 0
0
0
0
0
0
0
0
0
0
œk

4.7 Properties of Eigen Space
171
Note that the matrix is ﬁlled up with Eigen values in the diagonal elements.
Also note that upper off diagonal elements are ﬁlled up with one or zero.
4.7
Properties of Eigen Space
Consider the transformation matrix T: V ! V Suppose for any vector v 2 Vœ  V
such that Tv D œv for some scalar value ‘œ’, then the vector space Vœ is called
Eigen space. The scalar value ‘œ’ is called as Eigen value. The vector satisfying the
above condition is called Generalized Eigen vector. But in practice the basis of the
Generalized Eigen space are referred as Generalized Eigen vector.
1. Eigen vectors corresponding to distinct Eigen values are independent.
2. The transformation matrix A is Diagonalizable if there exists Eigen vectors
associated with the matrix A forms the basis of the vector space V.
3. If the matrix A is diagonalizable, the characteristic polynomial of the matrix A
is represented as .x  œ1/n1.x  œ2/n2.x  œ3/n3.x  œ4/n4 : : : .x  œn/nk,
Where dim.Vœk/ D nk.
4. The vector space V D Vœ1 ˚ Vœ2 ˚ Vœ3 : : : ˚ Vœk (Fig. 4.12).
5. The minimal polynomial of the diagonalizable matrix will always have the val-
ues n1 D n2 D n3 D : : : nk D 1.
6. If dim.Vœ1/ D m1 < n1, then the transformation matrix is called as deﬁcient
matrix. The value ‘m1’ is called Geometric multiplicity and ‘n1’ is called as
Algebraic multiplicity.
7. The kernel ..Aœi/n/ D Vœi, where n is the dimension of the vector space ‘V’.
8. Image ..A  œi/n/ D ˚k
jD1 Vœj, where n is the dimension of the vector space
‘V’ j ¤ i.
9. Any vector ‘v’ 2 ‘V’ can be uniquely written as v1 C v2, where v1 2
kernel..A  œi/n/ and v2 2 Image..A  œi/n/.
10. The vector space V can be written as direct sum as given below
kernel ..A  œi/n/ ˚ Image ..A  œi/n/
Fig. 4.12 Illustration of the
direct sum of the Eigen space
Vλ3
Vλ2
Vλ1
Vλk

172
4 Linear Algebra
4.8
Properties of Generalized Eigen Space
Consider the transformation matrix T: V ! V Suppose for any vector v 2 V.œ/ 
V such that .T  œI/kv D 0, where k is the minimal integer satisfying the condition,
then the vector space V.œ/ is called as Generalized Eigen space. The scalar value ‘œ’
is called as Eigen value. The vector satisfying the above condition is called Eigen
vector. But in practice the basis of the Eigen space are referred as Eigen vectors.
1. .A  œI/; .A  œI/2; .A  œI/3; : : : .A  œI/k1 are linearly independent.
2. Generalized Eigen vectors corresponding to distinct Eigen values are distinct.
3. If the minimal polynomial associated with the transformation matrix A is
represented as
.x  œ1/m1.x  œ2/m2.x  œ3/m3.x  œ4/m4 : : : .x  œn/mk
dim .V .œ 1// D dim .kernel..A  œ1/m1// D m1
dim .V .œ2// D dim .kernel..A  œ2/m2// D m2
dim .V .œ3// D dim .kernel..A  œ3/m3// D m3
dim .V .œ4// D dim .kernel..A  œ4/m4// D m4
: : :
dim .V .œk// D dim .kernel..A  œk/mk// D mk
Also the vector space V D V.œ1/ ˚ V.œ2/ ˚ V.œ3/ : : : ˚ V.œk/ (Fig. 4.13)
4. The kernel ..Aœi/n/ D V.œi/, where n is the dimension of the vector space ‘V’.
5. Image..A  œi/n/ D ˚k
jD1 V.œj/, where n is the dimension of the vector space
‘V’ and ¤ i.
6. Any vector ‘v’ 2 ‘V’ can be uniquely written as v1 C v2, where v1 2 kernel
..A  œi/n/ and v2 2 Image..A  œi/n/.
7. The vector space V can be written as direct sum as given below
kernel ..A  œi/n/ ˚ Image ..A  œi/n/
Fig. 4.13 Illustration of the
direct sum of the generalized
Eigen space
V(λ1)
V(λ2)
V(λ3)
V(λk)

4.9 Nilpotent Transformation
173
4.9
Nilpotent Transformation
1. The Matrix A is said to be Nilpotent, if 9 the minimum integer k such that
Ak D 0.
2. Consider the Linear transformation A: V ! V such that A is the Nilpotent matrix
is called Nilpotent Transformation.
3. If 0 is the only Eigen value of A, then A is Nilpotent.
4. ‘0’ is the only Eigen value of the Nilpotent matrix.
5. Maximum possible value for k is ‘n’, where ‘n’ is the dim(A).
6. If A is Nilpotent and diagonalizable, then A D 0.
7. If Vi D ker.Ai/; Vi  ViC1
Let vi 2 Vi D ker.Ai/.i:e/Aivi D 0
AiC1vi D A.Aivi/ D A.0/ D 0
) vi is the ker .AiC1/
) Vi  ViC1
Note that there can be the vector vi which is the kernel of AiC1, but not the kernel
of Ai, but not the kernel of Ai (Fig. 4.14).
0  V1  V2  V3     Vk D V
8. If Vi D ker.Ai/; AVi  Vi1
Let vi 2 Vi D ker.Ai/.i:e:/Aivi D 0
) Ai1Avi D 0
) Avi is in the kernel of Vi1
) AVi  Vi1
9. A is Nilpotent, 9 the matrix associated with A which is strictly upper triangular
(Fig. 4.15).
Fig. 4.14 Illustration
of the Nilpotent property 7
Vk=V
V2
V1
0

174
4 Linear Algebra
Fig. 4.15 Illustration
of the Nilpotent property 9
Vi
A
AVi
Vi-1
vi
Consider the basis of the V1 as fv11v12v13 : : : v1m1g
Extend the basis of V1 to the basis of V2 as
fv11v12v13 : : : v1m1 v21v22v23 : : : v2m2g
Similarly extend the basis of Vk  1 to the basis of Vk as
fv11v12v13 : : : v1m1 v21v22v23 : : : v2m2 : : : vk1vk2vk3 : : : vkmkg
The matrix corresponding to the above basis is strictly upper triangular matrix as
shown below.
Av11 D 0
Av12 D 0
Av1m1 D 0
) First m1 columns of the transformation matrix corresponding to the above basis
is completely ﬁlled up with zeros
Av21 D ‹
AV2  V21 D V1
v21 2 V2
) v21 can be written as the linear combinations of the basis of V1
fv11v12v13 : : : v1m1g. Let the coefﬁcient elements are f’11’12’13’14 : : : ’1m1g.
Similarly v2m2 can be written as the linear combinations of the basis of V1
fv11v12v13 : : : v1m1g. Let the coefﬁcient elements are represented as f’m21’m22
’m23’m24 : : : ’m2m1g.
Consider the representation of Av31.
AV3  V31 D V2
v31 2 V3
) v31 can be written as the linear combinations of the basis of V2

4.10 Polynomial
175
fv11v12v13 : : : v1m1v21v22v23 : : : v2m2g. Let the coefﬁcient elements are f“11“12
“13“14 : : : “1m2g
Similarly v3m3 can be written as the linear combinations of the basis of V2
fv11v12v13 : : : v1m1 v21v22v23 : : : v2m2g
f“m31“m32“m33“m34 : : : “m3m2g
Similarly the Transformation acting on the other basis vectors are represented as
the linear combinations of the basis vectors. Thus the transformation matrix corre-
sponding to the above basis vectors is as shown below.
0
:
0
’11
:
:
’m21
:
:
“11
:
“m31
0
:
0
:
:
:
’m22
:
:
“12
:
“m32
0
:
0
’1m1
:
:
:
:
:
“13
:
“m33
0
:
0
0
:
:
:
:
:
“14
:
“m34
0
:
0
0
0
:
:
:
:
“15
:
:
0
:
0
0
0
0
’m2m
:
:
:
:
:
0
:
0
0
0
0
0
:
:
:
:
:
0
:
0
0
0
0
0
0
:
:
:
:
0
:
0
0
0
0
0
0
0
“1m2
:
:
0
:
0
0
0
0
0
0
0
0
:
:
0
:
0
0
0
0
0
0
0
0
0
“m3m2
0
:
0
0
0
0
0
0
0
0
0
0
4.10
Polynomial
Consider the polynomial P(x) is factored and written as the product of the polyno-
mials as p.x/ D p1.x/p2.x/p3.x/p4.x/: : : pk.x/ and consider the linear transfor-
mation matrix A: V ! V. Note that p(A) is matrix obtained using the polynomial
p(x),where addition and multiplication are performed in the usual matrix operation
and the constant term ‘c’ in the polynomial p(A) is represented as cI, where ‘I’ is
the identity matrix.
1. kernel .p.A// D ˚ kernel.pi.A//.
2. The set of polynomials q1.x/q2.x/; q3.x/: : : qk.x/ are deﬁned as follows:
q1.x/ D p2.x/p3.x/: : : pk.x/
q2.x/ D p1.x/p3.x/: : : pk.x/
: : :
qk.x/ D p1.x/p2.x/: : : pk  1.x/
They are relatively prime.

176
4 Linear Algebra
3. Statement 2 implies there exists another set of vectors f1.x/; f2.x/; : : : fk.x/ such
that f1.x/q1.x/ C f2.x/q2.x/ C : : : fk.x/qk.x/ D 1.
4. There exists the polynomial acting on some vector v 2 ker.p.A//,gives the vector
vi 2 ker .pi.A//, for all i varies from 1 to k.
5. That polynomial is given as follows:
q1(A)f1(A) is the polynomial when acted on the vector v 2 ker .p.A//, gives the
vector v1 2 ker .p1.A//
6. In general, qi(A)ﬁ(A) is the polynomial when acted on the vector v 2 ker(p(A)),
gives the vector vi 2 ker.pi.A//.
Note: The polynomial properties mentioned above can be compared with the
minimal polynomial properties.
4.11
Inner Product Space
Consider the vector space V over the ﬁeld F. Inner product over the vector space V is
deﬁned as the map from VXV to F satisfying the following axioms. It is represented
as < V; V >. Note that VXV is the vector space with ﬁrst element
1. < v; v >¤ 0 where v 2 V
If < v; v >D 0 then v D 0
2. < v; w C u >D< v; w > C < v; u > where u; v 2 V
3. < cv; w > c < v; w > where c is the scalar constant.
4. < v; w >D < w; v >; where < w; v >, is called as conjugate
)< v; cw >D Nc < w; v >
The vector space V with the with the deﬁned inner product forms the Inner prod-
uct space.
Examples for the inner product space
1. Consider the vector space R2. Consider two arbitrary vectors v1 D .a1; a2/ 2
R2, and v2 D .b1; b2/ 2 R2, then the inner product deﬁned in the vector space
V as < v1; v2 >D a1b1 C a2b2. Thus the vector space R2 with the above
deﬁned inner product forms the inner product space.
2. Consider the vector space Rnxn. Consider the arbitrary vector B 2 Rnxn, the inner
product is deﬁned as < A; B > trace.AB/; B, is the conjugate of the matrix B.
Thus the vector space Rnxn for the above deﬁned inner product forms the inner
product space.
3. Consider the vector space consists of the set of all complex valued functions
deﬁned for the interval (0,1] with the inner product deﬁned as follows:
< f; g >D
Z 1
0
f .t/g.t/dt forms the inner product space:

4.12 Orthogonal Basis
177
Formation of Inner product space for the vector space V
Consider the n-dimensional vectors space V. Let the basis of the vector basis be
represented as fv1, v2, v3: : :vng.
There exists the transformation A: V ! V such that Av1 D e1; Av2 D
e2; : : : Avn D en, where ‘ei’ is the vector with n elements completely ﬁlled up
with zeros except ith element which is ﬁlled up with 1:e1; e2; e3; : : : en are called as
standard basis of the vector space Rn.
The Inner product of the vectors u, v 2 V represented as < v; u > is deﬁned as
follows.
Let v D ’1v1 C ’2v2 C ’3v3 C : : : ’nvn
u D “1v1 C “2v2 C “3v3 C : : : “nvn
< v; u >D ’1“1 C ’2“2 C ’3“3 C ’4“4 : : : C ’n“n
Norm of the vector v is denoted as follows kvk D p< v; v >
Properties of the norm
1. kc; uk D jcjkuk
2. kuk > 0
3. Cauchy-Schwarz inequality j < u; v > j  kuk kvk
4. ku C vk  kuk C kvk
5. j < v; u > j 	 Re.< v; u >/
4.12
Orthogonal Basis
1. Consider the inner product vector space V. Let u, v 2 V. The vector ‘u’ is
orthogonal to the vector ‘v’ if the inner product < u; v >D 0.
2. The set of vectors fv1; v2; : : : vkg are orthogonal set if < vi; vj >D 0, where
i ¤ j and i; j D 1; 2; ::k.
3. The set of orthogonal vectors are always linearly independent.
4. If the set of Orthogonal vectors fv1; v2; : : : vkg with kvik D 1; i D 1::k are
called as orthonormal vectors.
5. If the set of orthogonal vectors are fv1; v2; : : : vkg, then the set of orthonormal
vectors are obtained as f v1
kv1k;
v2
kv2k; : : :
vk
kvkkg.
6. If the set of orthogonal vectors forms the basis of the vector space V, they are
called orthogonal basis.
7. If fv1; v2; v3; : : : vng forms the basis of the vector space V, then there exists the
set of vectors fu1; u2; u3; : : : ung which is the orthonormal set which forms the
basis of the vector space ‘V’, which are obtained using Gram-Schmidt orthog-
onalization procedure as given below.
v1’ D
v1
kv1k D u1

178
4 Linear Algebra
v2’ D v2 < v2; u1 > u1
u2 D
v20
kv20k
v3’ D v3 < v3; u1 > u1 < v3; u2 > u2
u3 D
v30
kv30k
and so on.
8. Set of all vectors in the vector space V which are orthogonal to the set of vectors
of the subspace S  V is the vector space represented as S?.
9. If the dimension of the vector space S is k, then the dimension of the vector
space S? is n  k, where n is the dimension of the vector space ‘V’ (Fig.4.16).
10. If the basis of the space S is fw1; w2; : : : wk1; wkg and the basis of the vector
space S? is fwkC1; wkC2; : : : wn1; wng, then the basis of the vector space V
is given as fw1; w2; : : : wk1; wk; wkC1; wkC2; : : : wn1; wng.
.i:e:/ V D S ˚ S?
11. The basis are orthogonal to each other between the space S and S?.
12. Any vector v 2 V can be uniquely written as v1 C v2, where v1 2 S and
v2 2 S?
13. Any vector v 2 V can be written as the linear combinations of the
14. Orthonormal basis vectors fv1,v2,v3,: : : vng as ˛1v1 C ˛2v2 C ˛3v3 C
: : : ˛nvn, where
˛1 is computed as < v; v1 > :
Similarly ˛2 D< v; v2 >
˛3 D< v; v3 >
˛4 D< v; v4 >
In general ˛k D< v; vk >
Fig. 4.16 Illustration
of the vector space S and its
orthogonal complement S?
V
s⊥
S

4.13 Riegtz Representation
179
4.13
Riegtz Representation
Consider the Linear transformation T: V ! F, where the vector space V with
dimension ‘n’. There exists unique vector y 2 V such that T.v/ D< v; y >
Technique to obtain the vector y 2 V
1. Find the kernel(T) D W
2. Find the W?.
3. Find any orthonormal vector u 2 W?
4. Compute T.u/
5. The vector y D T.u/ u
Example 4.1. Consider the transformation
A W Rn ! R
A.Rn/ D A.Œx1 x2 x3 : : : xn/
D ˛1x1 C ˛2x2 C ˛3x3 C : : : C ˛nxn
)< .x1x2x3 : : : xn/; .˛1˛2 : : : ˛n/ >
D ˛1x1 C ˛2x2 C ˛3x3 C : : : C ˛nxn
Where y D .˛1˛2 : : : ˛n/ is the unique vector 2 Rn.

Chapter 5
Optimization
5.1
Constrained Optimization
Consider the function f .x; y/ D 2x2 C 4y C 3. The requirement is to ﬁnd out the
optimal values for ‘x’ and ‘y’ such that f(x, y) is minimized. Also it has to satisfy
the constraint that g.x; y/ D x C y C 3 D 0. Let the local extremum (maximum or
minimum) point be .x0; yo/.
The curve x Cy C3 D 0 can be viewed as the set of points .˛; 3˛/; 8 ˛ 2 R.
This is known as parametric representation of the curve. The tangent vector at the
point .x0; yo/ points towards the direction

d˛
d˛; d.3˛/
d˛

D Œ1  1. Also the gra-
dient vector of the equation g.x; y/ is obtained as
"
@.xCyC3/
@x
@.xCyC3/
@y
#
D
1
1

Note that
the gradient vector and the tangent vector is always orthogonal to each other (see
Fig. 5.1).
The solution of the above optimization problem lies on the intersection of the
curve g.x; y/ and f .x; y/ So consider the points on the curve g.x; y/ represented
in the parametric form as g.x.˛/; y.˛// D g1.˛/. Let the optimal point .x0; yo/
be represented in terms of the variable ‘˛’ as ˛0. Note that the point lies on the
curve f .x; y/. The function f .x; y/ can be represented as the function of ‘˛’ for
the points of intersection of the curves g.x; y/ and f .x; y/ as f 1.˛/
Using Taylor series,
f 1.˛ C ˛0/ D f 1.˛0/ C ˛ df 1.˛0/
d˛
C ˛2
2Š
d 2f 1.˛0/
d˛2
C : : :
) f 1.˛ C ˛0/  f 1.˛0/ D ˛ df 1.˛0/
d˛
C ˛2
2Š
d 2f 1.˛0/
d˛2
C : : :
f 1.˛0/ corresponds to local extremum and hence f 1.˛ C ˛0/  f 1.˛0/ must be
greater than 0 if the point ˛0 is local minima, or it must be lesser than 0 if the
point ˛0 is local maxima. Also ˛0 can be either positive or negative value. Thus the
condition that ˛0 belongs to the local extremum is df1.˛0/
d˛
D 0. Also note that if
E.S. Gopi, Mathematical Summary for Digital Signal Processing
Applications with Matlab, DOI 10.1007/978-90-481-3747-3 5,
c Springer Science+Business Media B.V. 2010
181

182
5 Optimization
x2
x+y+3=0
x1
[1-1]
Fig. 5.1 Illustration of the property that the gradient vector is orthogonal to the tangent vector
Fig. 5.2 Illustration of the
functional dependencies
x
y
f
α
the extremum is maxima, then d 2f 1.˛0/
d˛2
must be negative and if the extremum is
minima, then d 2f 1.’0/
d’2
is positive.
Thus the condition that the point ˛0 belongs to extremum is df 1.˛0/
d˛
D 0 (Fig. 5.2)
f 1.’0/ D f .x.’0/; y.’0//
) df1.’0/
d˛
D df.x.’0/; y.’0//
d’
D df.x.’/; y.’//
d’
at ˛ D ˛0
Using the illustration of functional dependencies as shown above, we get the
following
df.x.’/; y.’//
d˛
.at ˛ D ˛0/ D df
dx  dx
d˛ C df
dy  dy
d˛ D 0

5.1 Constrained Optimization
183
)
df
dx
df
dy

2
64
dx
d˛
dy
d˛
3
75 .at ˛ D ˛0/ D 0
The vector
" df
dx
df
dy
#
is the gradient vector of the curve f .x; y/ at the point ˛ D ˛0.
Also
" dx
d˛
dy
d˛
#
at ˛ D ˛0 is the direction of the tangent vector on the point ˛ D ˛0 of
the curve g.x; y/. [This is because of the fact that the set of points ..x.˛/; y.˛// are
the set of parametric representation of the points on the curve g.x; y/. Thus from the
above statement, the gradient vector of the curve f .x; y/ at the point .x.˛0/; y.˛0//
is orthogonal to the direction of the tangent vector drawn at the point .x.˛0/; y.˛0//
on the curve g.x; y/.
We have already shown that the gradient vector of the curve g.x; y/ at the point
.x.˛0/; y.˛0// and the direction of the tangent vector of the curve g.x; y/ at the
point .x.˛0/; y.˛0// are orthogonal to each other.
This implies that the gradient vector of the curve f .x; y/ and the gradient vector
of the curve g.x; y/ are parallel to each other and hence for some scalar ‘œ’, which
is known as Lagrange multiplier,
Gradient vector of the curve f .x; y/ C œ Gradient vector of the curve
g.x; y/ D 0.
Gradient vector of the curve f .x; y/ is represented as rf D
" df
dx
df
dy
#
. Similarly
the gradient vector of the curve g.x; y/ is represented as rg D
" dg
dx
dg
dy
#
.
) rf C œrg D 0
In our problem rf
D
4x
4

and rg
D
1
1

. There fore from above
4x
4

Cœ
1
1

D 0. Solving gives œ D 4 and x D 4=4 D 1. Also we know
x C y C 3 D 0.
) y D x  3 D 1  3 D 4:
Thus the optimal solution for computing the extremum of the function f .x; y/ D
2x2C4y C3 is obtained as .1; 4/ and the corresponding value is 2–16C3 D 11.
But to test whether the obtained extremum is maximum or minimum is decided
using the second derivative as shown below.
We have already shown that d 2f 1.˛/
d
˛2.at ˛ D ˛0/ > 0, where f 1.˛/ D
f .x.˛/; y.˛// if the extremum point ˛ D ˛0 is minima point.

184
5 Optimization
Using the functional dependencies as shown in the Fig. 5.2,
d 2f 1.˛/
d
˛2 is
computed as shown below.
df1
d˛ D @f
@x
@x
@˛ C @f
@y
@y
@˛ D p.x.˛/; y.˛//
d 2f 1.˛/
d’2
D
d

df 1
d’

d’
D d.p.x.˛/; y.˛//
d’
D @p
@x
@x
@˛ C @p
@y
@y
@˛
D @x
@˛
@
@f
@x
@x
@˛ C @f
@y
@y
@˛

@x
C @y
@˛
@
@f
@x
@x
@˛ C @f
@y
@y
@˛

@y
D @x
@˛
 @2f
@x2
 @x
@˛ C @x
@˛
@f
@x
 @2x
@x@˛

C @x
@˛
 @2f
@x@y
 @y
@˛
C@x
@˛
@f
@y
 @2y
@x@˛

C @y
@˛
 @2f
@y2
 @y
@˛ C @y
@˛
@f
@y
 @2y
@˛@y

C @y
@˛
 @2f
@y@x
 @x
@y C @y
@˛
@f
@x
 @2x
@y@˛

D
 @x
@˛
@y
@˛

2
664
 @2f
@x2

@2f
@y@x
@2f
@y@x
@2f
@y2
3
775
2
64
@x
@˛
@y
@˛
3
75 C
@f
@x
@f
@y

2
664
@2x
@˛2
@2y
@˛2
3
775
Thus to satisfy the condition d 2f 1.˛/
d’2
.at ˛ D ˛0/ > 0
 @x
@˛
@y
@˛

2
6664
 @2f
@x2

@2f
@y@x
@2f
@y@x
@2f
@y2
3
7775
2
64
@x
@˛
@y
@˛
3
75 C
@f
@x
@f
@y

2
664
@2x
@˛2
@2y
@˛2
3
775 .at ˛ D ˛0/ > 0
Note that
h
@x
@˛
@y
@˛
i
.at ˛ D ˛0/ is the direction of tangent vector drawn at the point
˛ D ˛0 on the curve g.x; y/ and hence the point lies on the tangent vector drawn
on the curve g.x; y/ at the point ˛ D ˛0.
Also œ d 2g.˛/
d’2
D 0 at.at ˛ D ˛0/.
Expanding in the similar fashion as described above, it can be shown that
œ
 @x
@˛
@y
@˛

2
664
@2g
@x2
 @2g
@y@x
@2g
@y@x
@2g
@y2
3
775
2
664
@x
@˛
@y
@˛
3
775 C œ
@g
@x
@g
@y

2
664
@2x
@˛2
@2y
@˛2
3
775 .at ˛ D ˛0/ D 0

5.1 Constrained Optimization
185
Adding both the equation, we get
 @x
@˛
@y
@˛

2
664
 @2f
@x2

@2f
@y@x
@2f
@y@x
@2f
@y2
3
775
2
664
@x
@˛
@y
@˛
3
775 C œ
 @x
@˛
@y
@˛

2
664
@2g
@x2
 @2g
@y@x
@2g
@y@x
@2g
@y2
3
775
2
64
@x
@˛
@y
@˛
3
75
C
@f
@x
@f
@y

2
664
@2x
@˛2
@2y
@˛2
3
775 C œ
@g
@x
@g
@y

2
664
@2x
@˛2
@2y
@˛2
3
775 .at ˛ D ˛0/ > 0
We have already shown that
@f
@x
@f
@y

C œ
@g
@x
@g
@y

D 0
and hence we get the following condition.
@x
@˛
@y
@˛

2
6664
@2f
@x2

C œ
@2g
@x2
 @2f
@y@x C œ @2g
@y@x
@2f
@y@x C œ @2g
@y@x
@2f
@y2 C œ@2g
@y2
3
7775
2
64
@x
@˛
@y
@˛
3
75 .at ˛ D ˛0/ > 0
In practice representing the function g.x; y/ in parametric form is not easy like
the one used in the example. But the point
h
@x
@˛
@y
@˛
i
always lies on the tangent of
the curve g.x; y/ drawn at the point .x0; y0/ and hence to conﬁrm whether the
obtained point is minima, we have to test whether the modiﬁed Hessian matrix
2
4

@2f
@x2

C œ

@2g
@x2

@2f
@y@x C œ @2g
@y@x
@2f
@y@x C œ @2g
@y@x
@2f
@y2 C œ @2g
@y2
3
5 is positive deﬁnite restricted to the points on
the tangent of the curve g.x; y/ drawn at the point .x0; y0/.
The modiﬁed Hessian matrix to be tested is given as
2
6664
d 2f
dx2 C d 2g
dx2
d 2f
dx dy C  d 2g
dx dy
d 2f
dydx C  d 2g
dx dy
d 2f
dy2 C d 2g
dx2
3
7775 with œ D 4; as found earlier
D
4 C .4/  0
0 C 0
0 C .4/  0
0 C 0

D
4
0
0
0

D A .say/
The obtained optimal solution x D 1; y D 4 (described earlier) corresponds to
the minimal point if the modiﬁed Hessian matrix (A) as shown above is the positive
deﬁnite restricted to the points on the tangent drawn on the curve g.x; y/ at the
point .1; 4/.

186
5 Optimization
The equation of the tangent drawn on the curve g.x; y/ at the point .1; 4/ is
obtained as the set of points .z1; z2/ satisfying the condition rgT Œz11 z2C4 D 0.
Note that rgT is computed at .1; 4/.
For the function g.x; y/ D xCyC3 D 0, the gradient vector at the point .1; 4/
is found as follows.
2
664
@g
@x
@g
@y
3
775
T
D
1
1

Therefore the equation of the tangent drawn at the point .1; 4/ on the curve
g.x; y/ D x C y C 3 D 0 is obtained as
1
1
T
Œz1  1 z2 C 4 D z1 C z2 C 3 D 0.
The set of points .v; 3v/8 v 2 R is the parametric representation of the above
equation that lies on the tangent drawn on the curve g.x; y/ at .1; 4/.
Also, the matrix A is said to be positive deﬁnite restricted to the points that lies
on the tangent drawn on the curve g.x; y/ at the point .1; 4/. if uT Au > 0; 8u 2
points as described above.
uT Au D Œv
 3  v
4
0
0
0
 
v
3  v

D Œ4v
0

v
3  v

D 4v2
which is greater than or equal to zero and hence the point obtained is the saddle
point.
5.2
Extension to Constrained Optimization Technique
to Higher Dimensional Space with Multiple Constraints
In general the problem discussed above can be extended to more than two variables
and more than one constraints as shown below.
Minimize f .Œx1 x2 x3 : : : xn/, subject to the constraints
f .Œx1 x2 x3 : : : xn/
g2.Œx1 x2 x3 : : : xn/ D 0 : : : gm.Œx1 x2 x3 : : : xn/ D 0:
Create the Lagrangean function
L.f; g1; g2; g3/
D f .Œx1 x2 x3 : : : xn/ C 1g1.Œx1 x2 x3 : : : xn/
C2g2.Œx1 x2 x3 : : : xn/ C : : : mgm.Œx1 x2 x3 : : : xn/
Differentiating the above equation with respect to x1 x2 x3 : : : xn; 1; 2; : : : m
and equate to zero gives the extremum point .x01 x02 x03 : : : x0n/ (say).

5.2 Extension to Constrained Optimization Technique to Higher Dimensional Space
187
The above set of equations can also be obtained using the following equation
rf C Œrg1rg2 : : : rg1m
2
664
1
2
: : :
m
3
775 D 0.
Modiﬁed Hessian matrix is used to test whether the obtained extremum point
thus obtained is minimum or not as shown below.
The Modiﬁed Hessian matrix is as shown in below
2
66666666664
d 2f
dx1
2 C 1
d 2g1
dx1
2 C 2
d 2g2
dx1
2 C 3
d 2g3
dx1
2
d 2f
dx1x2
C 1
d 2g1
dx1x2
C 2
d 2g2
dx1x2
C 3
d 2g3
dx1x2
d 2f
dx2x1
C 1
d 2g1
dx2x1
C 2
d 2g2
dx1x1
C 3
d 2g3
dx1x1
d 2f
dx2
2 C 1
d 2g1
dx2
2 C 2
d 2g2
dx2
2 C 3
d 2g3
dx2
2
: : :
: : :
d 2f
dxnx1
C 1
d 2g1
dxnx1
C 2
d 2g2
dxnx1
C 3
d 2g3
dxnx1
d 2f
dxnx2
C 1
d 2g1
dxnx2
C 2
d 2g2
dxnx2
C 3
d 2g3
dxnx2
: : :
d 2f
dx1xn
C 1
d 2g1
dx1xn
C 2
d 2g2
dx1xn
C 3
d 2g3
dx1xn
: : :
d 2f
dx2xn
C 1
d 2g1
dx2xn
C 2
d 2g2
dx3xn
C 3
d 2g3
dxnxn
: : :
: : :
: : :
d 2f
xn2 C 1
d 2g1
dxn
2 C 2
d 2g2
dxn
2 C 3
d 2g3
dxn
2
3
77777777775
If the Modiﬁed Hessian matrix as mentioned above is positive deﬁnite restricted
to the points on the tangent plane P as deﬁned below.
Tangent plane ‘P’ drawn at the optimal point .x01 x02 x03 : : : x0n/ on the surface
deﬁned by the equations g1.x1; x2; : : xn/; g2.x1; x2; : : xn/ and g3.x1; x2; : : xn/ is
deﬁned as the set of points ‘y’ satisfying the equation.
Œrg1 rg2 rg3T .computed at Œx01
x02 x03
: : : x0n/
0
BBBBBBB@
y 
2
66666664
x01
x02
x03
x04
: : :
x0n
3
77777775
1
CCCCCCCA
D 0
Example 5.1. Minimize the function f .x1; x2; x3/ D 2x12 C 4  x2 C 2  x3 C 2.
Subject to the constraint g1.x1; x2; x3/ D x1 C x2  4 D 0; g2.x1; x2; x3/ D
x1 C x3  5 D 0

188
5 Optimization
rf C Œrg1rg2 : : : rg1m
2
664
1
2
: : :
m
3
775 D 0
)
2
4
4x1
4
2
3
5 C
2
4
1
1
1
0
0
1
3
5
1
2

D 0
)
2
4
4x1 C 1 C 2 D 0
4 C 1 C 2 D 0
2 C 2 D 0
3
5
Solving the above equation gives 2 D 2; 1 D 2; x1 D 1; x2 D 3; x3 D 4
The optimum value is (1,3,4) and the corresponding function value is 24.
To test whether the obtained solution is minima or not is done using modiﬁed
Hessian matrix as given below.
2
4
4
0
0
0
0
0
0
0
0
3
5
The optimal point thus obtained is minima if the modiﬁed Hessian matrix obtained is
positive deﬁnite restricted to the points on the tangent plane of the surface deﬁned by
the equation g1.x1; x2; x3/ D x1Cx24 D 0; g2.x1; x2; x3/ D x1Cx35 D 0
The equation of the tangent as described above is obtained as follows.
Œrg1
rg2T
2
4
y1  1
y2  3
y3  4
3
5 D 0
)
1
1
0
1
0
1
 2
4
y1  1
y2  3
y3  4
3
5 D 0
)
1
1
0
1
0
1
 2
4
y1
y2
y3
3
5 D
4
5

y1 C y2 D 4
y1 C y3 D 5
Let y1 D ˛
y2 D 4  ˛ and y3 D 5  ˛

5.3 Positive Deﬁnite Test of the Modiﬁed Hessian Matrix Using Eigen Value Computation
189
Thus the tangent plane is deﬁned as the vector space that is spanned by the
column vectors as described below.
2
4
˛
4  ˛
5  ˛
3
5 ; 8˛ 2 R
To check whether the matrix
2
4
4
0
0
0
0
0
0
0
0
3
5 is negative deﬁnite restricted to the vector
of the form
2
4
˛
4  ˛
5  ˛
3
5 is tested as described below.
Œ˛
4  ˛
5  ˛
2
4
4 0
0
0 0
0
0 0
0
3
5
2
4
˛
4  ˛
5  ˛
3
5 D 4˛2 	 08˛ 2 R
Hence the obtained extremal point is the saddle point.
5.3
Positive Deﬁnite Test of the Modiﬁed Hessian Matrix
Using Eigen Value Computation
If all the Eigen values of the matrix modiﬁed Hessian matrix computed at the
extremum point are positive, the matrix is said to be positive deﬁnite matrix and
the corresponding extremum point is minima. If all the Eigen values of the matrix
modiﬁed Hessian matrix computed at the extremum point are negative, the matrix
is said to be Negative deﬁnite and the corresponding extremum point belongs to
maxima.
Proof. As Hessian matrix A D
2
4
a1
a2
a3
a2
a4
a5
a3
a5
a6
3
5 (say) is the symmetric matrix, it is
diagonalizable (i.e.) the matrix can be represented as A D EDEH. Also the Eigen
values of the matrix A are real numbers and the Eigen vectors are orthonormal to
each other. (Refer Chapter 4)

190
5 Optimization
Œx1 x2 x3
2
4
a1
a2
a3
a2
a4
a5
a3
a5
a6
3
5
2
4
x1
x2
x3
3
5
D Œx1 x2 x3
2
4
e11 e21 e31
e12 e22 e32
e13 e23 e33
3
5
2
4
œ1
0
0
0
œ2
0
0
0
œ3
3
5
2
4
e11
e12
e13
e21
e22 e23
e31
e32 e33
3
5
2
4
x1
x2
x3
3
5
D œ1.x1  e11 C x2  e12 C x3  e13/2
Cœ2.x1  e21 C x2  e22 C x3  e23/2
Cœ3.x1  e31 C x2  e32 C x3  e33/2
Thus to test whether the matrix A is positive deﬁnite, the requirement is
œ1.x1  e11 C x2  e12 C x3  e13/2 C œ2.x1  e21 C x2  e22 C x3  e23/2
Cœ3.x1  e31 C x2  e32 C x3  e33/2 > 0
This implies all the Eigen values should be greater than zero.
Example 5.2. Consider the problem of minimizing the function f .x; y/ D 2x2 C
4y C 3 subject to the constraint g.x; y/ D x C y C 3 D 0.
We have already shown that the extremal point is .1; 4/ with respect to the
xy co-ordinate system. Now shifted co-ordinate system PQ co-ordinate system is
framed such that the extremal point is (0,0) with respect to the new co-ordinate
system.
) P D X  1; Q D Y C 4
Thus the function g.x; y/ D x C y C 3 D 0 is rewritten with respect to the new
co-ordinate system as shown below.
u.p; q/ D p C 1 C q  4 C 3 D 0
) u.p; q/ D p C q D 0
The parametric representation of the above equation is represented as the set of
points of the form .ˇ; ˇ/. The equation of the tangent plane with respect to the
new co-ordinate system is the set of points y D Œy1 y2T such that ruT y D 0
) Œ1
1
y1
y2

D 0:
In other words the set of points describing the tangent plane is in the Null space of
the matrix [1
1], which is the represented as
 ˛
˛

, where ˛ 2 R.

5.3 Positive Deﬁnite Test of the Modiﬁed Hessian Matrix Using Eigen Value Computation
191
The Modiﬁed Hessian matrix is found at the point (0,0) with the new co-ordinate
system and is displayed below. The equation f .x; y/ D 2x2C4yC3 in the modiﬁed
co-ordinate system is given as
v.p; q/ D 2  .p C 1/2 C 4  .q C 5/ C 3or
D 2.p2 C 1 C 2p/ C 4q C 20 C 3a D 2p2 C 4p C 4p C 25
Modiﬁed Hessian matrix is
4
0
0
0

(Note that the value of œ D 4 which is same as
the one calculated with the previous co-ordinate system). To test whether the above
Hessian matrix is positive deﬁnite or not restricted to the tangent plane as described
above is as shown below.
Œ˛
˛
4
0
0
0
  ˛
˛

D 4˛2 	 0 and hence the extremal point obtained is the
indeterminate point.
Trick to test whether the Hessian matrix ‘H’ is positive deﬁnite restricted to the
points on the tangent plane as described above.
Any vector in the tangent plane can be represented as the linear combinations of
the basis of the Null space of the matrix ruT (see above). The Eigen basis vector
E1,E2,E3 (say) are arranged in column form to obtain Eigen matrix E. Any vector in
the tangent plane space can be represented as the linear combinations of the Eigen
vectors as described below. Thus ŒE1 E2
p1
p2

is the arbitrary point in the tangent
plane as described above.
Thus the Hessian matrix H is positive deﬁnite restricted to the points on the
tangent plane (as described above) if Œp1 p2ET HE
p1
p2

> 08p1; p2 2 R.
Thus if the Eigen values of the matrix ET HE is greater than 0, the matrix is
positive deﬁnite matrix.
In the above example, Eigen values of the matrix
"h
1
p
2  1
p
2
i 4 0
0 0
 "
1
p
2
 1
p
2
#
D
Œ2 shave to be found. The Eigen value is positive and hence the Extremum point
obtained is minimum.
Example 5.3. Consider the problem described in Example 5.1.
Minimize the function f .x1; x2; x3/ D 2x12C4x2C2x3C2. Subject to the
constraint g1.x1; x2; x3/ D x1 C x2  4 D 0; g2.x1; x2; x3/ D x1 C x3  5 D 0.
The optimal point is obtained as (1,3,4) with respect to the co-ordinate system
.x1; x2; x3/. The modiﬁed co-ordinate system .z1; z2; z3/ is obtained as z1Dx11I
z2 D x2  3I z3 D x3  4;

192
5 Optimization
The Modiﬁed equations corresponding to the functions g1; g2, and f are u1; u2
and v respectively which are displayed below
u1.z1; z2; z3/ D z1 C 1 C z2 C 3  4 D 0 D z1 C z2 D 0
u2.z1; z2; z3/ D z1 C 1 C z3 C 4  5 D 0 D z1 C z3 D 0
v.z1; z2; z3/ D 2.z1 C 1/2 C 4  .z2 C 3/ C 2  .z3 C 4/ C 2
D 2.z12 C 1 C 2z1/ C 4z2 C 12 C 2z3 C 8 C 2
D 2z12 C 4z1 C 4z2 C 2z3 C 24
The Modiﬁed Hesian matrix ‘H’ at the point (0,0,0) with respect to the modiﬁed co-
ordinate system is given below.
2
4
4
0
0
0
0
0
0
0
0
3
5. (Note that the value of œ1 D 2 and
œ2 D 2, which are same as that of the one calculated with the previous co-ordinate
system).
The Equation of the tangent plane passing through the point (0,0,0) in the new
co-ordinate system is described as the set of points (p,q,r) satisﬁes the following
condition.
Œrg1 rg2T
2
4
p
q
r
3
5 D 0
)
1
1
0
1
0
1
 2
4
p
q
r
3
5 D 0
The set of points on the tangent plane is given as the null space of the matrix
1
1
0
1
0
1

. The Eigen basis of the null space is represented as B D
2
64
1
p
3
 1
p
3
 1
p
3
3
75
The Eigen value of the matrix ET HE D Œ1
1
1
2
4
4
0
0
0
0
0
0
0
0
3
5
2
4
1
1
1
3
5 D Œ4
is 4 which is greater than zero and hence the obtained minima point is the min-
ima point.
Example 5.4. Maximize the function x1x2 C x2x3 C x1x3.subject to the constraint
x1 C x2 C x3 D 3.
The Lagrangean function is obtained as x1x2 C x2x3 C x1x3 C .x1 C x2 C
x3  3/ D 0.
The following set of equations are obtained by computing by differentiating the
Lagrangean function with respect to .x1; x2; x3; / and equate to zero.

5.4 Constrained Optimization with Complex Numbers
193
x2 C x3 C  D 0I x1 C x3 C  D 0I x1 C x2 C  D 0I x1 C x2 C x3 D 3
Solving the above equation gives  D 2; x1 D x2 D x3 D 1
The modiﬁed co-ordinate system .z1; z1; z1/ are obtained as z1 D x1  1I z2 D
x2  1I z3 D x3  1.
The modiﬁed equation corresponding to f and g is as shown below.
v.z1; z2; z3/ D .z1 C 1/.z2 C 1/ C .z2 C 1/.z3 C 1/ C .z1 C 1/.z3 C 1/
D z1z2 C z2z3 C z3z1 C z1 C z2 C z3 C 3
u.z1; z2; z3/ D z1 C 1 C z2 C 1 C z3 C 1 D z1 C z2 C z3 C 3 D 0
The Modiﬁed Hessian matrix at (0,0,0) in the new co-ordinate system is computed
as
2
4
0
1
1
1
0
1
1
1
0
3
5
The Equation of the tangent plane with the modiﬁed co-ordinate system is set of
points .p; q; r/ satisfying the condition ruT
2
4
p
q
r
3
5 D 0
This implies the set of points form the null space of the matrix Œ1 1 1
The basis of the null space of the matrix Œ1 1 1 is given as
2
4
0:5774 0:5774
0:7887 0:2113
0:2113
07887
3
5.
The
Eigen
values
of
the
matrix
2
4
0:5774 0:5774
0:7887
0:2113
0:2113
07887
3
5
T 2
4
0
1
1
1
0
1
1
1
0
3
5
2
4
0:5774 0:5774
0:7887
0:2113
0:2113
07887
3
5 D
1
0
0
1

are given as 1 and 1 which are less
than zero and hence the obtained extremum point is the maxima point.
5.4
Constrained Optimization with Complex Numbers
Consider the problem of minimizing the function f .x1; x2; x3/ where x1; x2;
x3 2 C subject to the constraint
g.x1; x2; x3/ D 0; where x1; x2; x3 2 C
The problem can be viewed as maximizing both real and imaginary part of the func-
tion. Hence the Lagrangean function is given as
f .x1; x2; x3/ C g.x1; x2; x3/

194
5 Optimization
The equation can be written as below treating the complex Lagrange multiplier
deﬁned as œ D œ1 C j œ2.
f .x1; x2; x3/ C Re
Ng.x1; x2; x3/

D 0
Differentiating the above equation with respect to x1; x2; x3 and Nœ and equate to
zero gives the extremal point for the above problem.
Note: Differentiation of the function with respect the complex number is deﬁned
as shown below.
@
@x1
D 1
2

@
@x11
C j
@
@x12

and
@
@x1 D 1
2

@
@x11
 j
@
@x12

Properties of the complex differentiation
1. @x1
@x1
D 0
2. @x1
@x1 D 1
3. @.AHx1/
@x1
D 0
4. @.AHx1/
@x1
D NA
5. @.zHA/
@Nz
D A
6. @Re.zHA/
@Nz
D 1
2A
5.5
Dual Optimization Problem
Consider the problem of minimizing the function f .x1; x2; x3/, subject to the con-
straint g1.x1; x2; x3/ D 0. Framing the Lagrange equation we get, L.X; / D
f .x1; x2; x3/ C g1.x1; x2; x3/. Differentiate the above equation with respect to
x1; x2; x3;  and equate to zero. Solve for x1; x2; x3 in terms  and substitute in
the equation L.X; / to obtain the function h./. Thus function thus obtained is
called dual optimization problem for the above mentioned constrained optimization
problem. Thus the Dual problem is as shown below.
Maximize the function h./ without constraints. (i.e.) Unconstrained optimiza-
tion problem.
Example 5.5. Consider the problem of minimizing the function f .x1; x2; x3/ D
x1x2 C x2x3 C x3x1 C x1x2x3  4 D 0. Subject to the following constraints
g1.x1; x2; x3/ D x1 C x2 C x3  3 D 0.

5.6 Kuhn-Tucker Conditions
195
Framing the Lagrange equation we get
L.X; / D x1x2 C x2x3 C x3x1 C x1x2x3  4 C .x1 C x2 C x3  3/
Differentiating the above equation with respect to x1; x2; x3 and equate to zero and
solving for x1; x2; x3 in terms of , we get
x1 D 1
2  1
2.7 C 2/
1
2 ; x2 D 1
2  1
2.7 C 2/
1
2 ; x3 D 2 C .7 C 2/1=2
Substituting the above equation in the Lagrange equation L.X; /, we get the Dual
problem Maximizing the

1
2  1
2.7 C 2/
1
2
 
1
2  1
2.7 C 2/
1
2

without any con-
straints. (i.e.)Unconstrained optimization.
5.6
Kuhn-Tucker Conditions
Consider the optimization problem as shown below.
Minimize the function f .x1; x2; x3/. Subject to the constraints that
g1.x1; x2; x3/ D 0 g2.x1; x2; x3/ D 0 h1.x1; x2; x2/  0
h2.x1; x2; x3/  0
The Lagrangean function is framed with the Lagrangean multipliers 1; 2; 1; 2
as shown below.
f .x1; x2; x3/ C 1g1.x1; x2; x3/ C 2g2.x1; x2; x3/
C1 h1.x1; x2; x3/ C 2h2.x1; x2; x3/ D 0
Differentiating the above equation with respect x1; x2; x3 and equate to zero to
obtain the three equations. Also, The Lagrange multiplier used for inequality con-
straints satisﬁes the following conditions
1 h1.x1; x2; x3/ C 2h2.x1; x2; x3/ D 0
1 	 0; 2 	 0
Along with this, the two equations g1.x1; x2; x3/ D 0 g2.x1; x2; x3/ D 0 are used
to obtain the extremal point.1
Example 5.6. Minimize the function
f .x1; x2; x3/ D x1x2 C x2x3 C x3x1 C x1x2x3  4 D 0. Subject to the
following constraints g1.x1; x2; x3/ D x1 C x2 C x3  3 D 0
h1.x1; x2; x3/ D x1  x2  x3  0; h2.x1; x2; x3/ D x1 C x2  2x3  0

196
5 Optimization
Construct the Lagrangean function
x1x2 C x2x3 C x3x1 C x1x2x3 C 1.x1 C x2 C x3  4/
C1.x1  x2  x3/ C 2.x1 C x2 C 2x3/ D 0
Differentiating with respect to x1; x2; x3 and equate to zero, the following equations
are obtained
x2 C x3 C x2x3 C 1 C 1 C 2 D 0
x1 C x3 C x1x3 C 1  1 C 2 D 0
x2 C x1 C x1x2 C 1  1  22 D 0
Also 1.x1  x2  x3/ C 2.x1 C x2  2x3/ D 0
Along with above equation g1.x1; x2; x3/ D x1 C x2 C x3  3 D 0 is used to
obtain the extremal points as given below.
Case 1: 1 D 0. The following solutions are obtained
.x1; x2; x3; 1; 1; 2/
D .1; 0; 4; 1; 0; 0/; .3; 0; 0; 3; 0; 0/; .1; 1; 1; 3; 0; 0/; .1; 5; 1; 1; 0; 0/
All the solutions are valid as 1 	 0, and 2 	 0
Case 2: 2 D 0. The following solutions are obtained
.x1; x2; x3; 1; 1; 2/ D .1; 0; 4; 1; 0; 0/.3; 0; 0; 3; 0; 0/
3
2; 3
4; 3
4; 99
32; 9
32; 0

.1; 1; 1; 3; 0; 0/.1; 5; 1; 1; 0; 0/
All the solutions obtained above are valid as 1 	 0, and 2 	 0.

Chapter 6
Matlab Illustrations
6.1
Generation of Multivariate Gaussian Distributed
Sample Outcomes with the Required Mean Vector
‘MY ’ and Covariance Matrix ‘CY ’
Consider the transfer of random variables X D R cos ‚ and Y D R sin ‚, where
‘‚’ is uniformly distributed between 0 and 2  and ‘R’ is Rayleigh density function
as described below. Also R and ‘‚’ are independent random variables.
The Rayleigh density function is given as follows
fR.r/ D r

2 e r2
22 8r 	 0
D 0; elsewhere
The corresponding distribution function is given as follows.
FR.r/ D 1  e r2
22 8r 	 0
D 0; elsewhere
The joint density function of X and Y represented as fXY.x; y/ is obtained using
Jacobian as follows.
fXY.x; y/ D 1
jJ jfR‚.r.x; y/; ‚.x; y//
The solution for the above set of equation gives R D
p
X2 C Y 2 and ‚ D
tan1  Y
X

. The Jacobian matrix at the solution is obtained as follows.
2
64
@X
@R
@X
@‚
@Y
@R
@Y
@‚
3
75 at

R D
p
X2 C Y 2; ‚ D tan1
 Y
X

is obtained as
R D
p
X2 C Y 2
E.S. Gopi, Mathematical Summary for Digital Signal Processing
Applications with Matlab, DOI 10.1007/978-90-481-3747-3 6,
c Springer Science+Business Media B.V. 2010
197

198
6 Matlab Illustrations
Thus the joint density function of
fXY.x; y/ D
1
p
X2 C Y 2 fR‚

p
x2 C y2; tan1 
y
x

D
1
p
X2 C Y 2 fR

p
x2 C y2

f‚

tan1 
y
x

D
1
p
X2 C Y 2

p
x2 C y2


2
e .x2Cy2/
22
1
2 
D
1
p
2 
2 e x2
22
1
p
2 
2 e y2
22 D fX.x/fY .y/
Note that the density functions fX.x/; fY .y/ obtained above are the independent
Gaussian density function with variance 
2 and mean zero.
Thus the steps involved in generating the sample outcomes that is Gaussian dis-
tributed with mean D 0 and variance D 
2 is summarized below.
Step 1: Generation of sample outcomes that is Rayleigh distributed from the
uniformly distributed sample outcomes
Consider the Uniformly distributed random variable be ‘U’ and Rayleigh distributed
random variable be ‘R’. Also Let R D g.U/
FR.r/ D P.R  r/ D P.g.U/  r/ D P.U  g1.r// D FU .g1.r// D g1.r/
(As U is the uniformly distributed function).
) FR.r/ D g1.r/
) 1  e r2
22 D g1.r/ D u
Solving for ‘r’, we get e r2
22 D 1  u )
r2
22 D ln

1
1u

) r D
s
2
2 ln

1
1  u

Note that the distribution function FR.r/ D 1  e r2
22 is valid only for positive
values of ‘r’. It is also noticed that the range for the u is from 0 to 1.
Thus to generate the outcomes with Rayleigh distributed, generate the out-
comes ‘u’, that is uniformly distributed over the range 0–1 and use the formula
r D
q
2
2 ln

1
1u

to obtain the Rayleigh distributed outcomes.
Step 2: Generate the sample outcomes of the uniformly distributed random variable
‘‚’ that ranges from 0 to 2 .

6.1 Generation of Multivariate Gaussian Distributed Sample Outcomes
199
4.5
x104 Probability density function of the sample outcomes of the generated data
3.5
2.5
1.5
pdf
0.5
4
3
2
1
0−6
−4
−2
0
Values
2
4
6
Fig. 6.1 Probability density function of the 10,000,000 samples generated using Matlab (Gaussian
distribution with mean D 0 and variance D 1)
Step 3: Compute X D Rcos‚ and Y D Rsin‚ to obtain the two sets of sample
outcomes that are independent Gaussian distributed with mean ‘0’ and variance 
2
(Fig. 6.1).
Steps to Generate the Multivariate (say ‘N’) Gaussian distributed sample out-
comes Repeat the procedure described above to generate ‘N’ outcomes which are
individually Gaussian distributed with mean D 0 and variance D 1. Note that they
are independent in nature (i.e.) If we compute the co-variance matrix for the above
generated ‘N’ outcomes, it is identity matrix. Let the generated Multivariate Gaus-
sian distributed outcomes be represented as the outcomes of the random vector ‘X’.
Now the requirement is to obtain the outcomes of the Gaussian random vector
‘Y’ whose mean and co-variance matrix are represented as ‘CY ’ and ‘M’ respec-
tively the generated outcomes of the random vector ‘X’.
They are obtained using transformation matrix ‘A’ as described below.
Let Y D AX C b be the transformation equation which transforms the multi-
variate random variable X to the multivariate random variable ‘Y’, where ‘A’ is the
transformation matrix and ‘b’ is the column vector.
If ‘X’ is the Multivariate Gaussian distributed, then ‘Y’ is also the Multivariate
Gaussian distributed with Co-variance matrix CY D ACX AT and the mean vector
‘AmX C b’, where mX is the mean vector of the multivariate random variable X.

200
6 Matlab Illustrations
The generated Multivariate Gaussian distributed outcomes has the Identity co-
variance matrix CX and the mean vector mX D 0,
) CY D AAT :
) Mean vector D b
Thus the requirement is to represent the required co-variance CY matrix as the
product of A and AT and hence the matrix ‘A’ is obtained. Then applying the trans-
formation Y D AX C b to obtain Multivariate Gaussian distributed outcomes with
the speciﬁed mean ‘b’ and the co-variance matrix CY . Choose ‘b D mY ’ for the
speciﬁcations mentioned above.
Representing the matrix CY as the product of AAT is obtained using Eigen de-
composition as described below.
Represent the matrix CY D EDET , where E is the Eigen column matrix, in
which the columns are the orthonormal Eigen vectors obtained from the co-variance
matrix CY and D is the Diagonal matrix with Diagonal elements ﬁlled up with the
corresponding Eigen values of the co-variance matrix CY .
CY D EDET D ED
1
2 D
1
2 ET D

ED
1
2
 
ED
1
2
T
:
) A D

ED
1
2

is obtained:
Using the obtained transformation matrix ‘A’, the outcomes of the random vector
‘X’ is transformed to the outcomes of the random vector ‘Y’ using the equation
Y D AX C mY .
gengd.m
%m-file for generating the Multivariate Gaussian
distributed
%sample outcomes with zero mean vector and Identity
matrix
%co-variance matrix
for k D 1:2:10
u D rand(1,1000000);
r D sqrt(2log(1./(1  u)));
theta D rand(1,1000000).2pi;
Z1 D r.cos(theta);
Z2 D r.sin(theta);
Xfkg D X;
Xfk C 1g D Y;
end
CX D cov(cell2mat(X’)’);

6.1 Generation of Multivariate Gaussian Distributed Sample Outcomes
201
The Co-variance matrix of the generated data X is computed as
CX D
2
6666666666666664
1:0
0
0
0
0
0
0
0
0
0
0
0:99
0
0
0
0
0
0
0
0
0
0
:99
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
:99
0
0
0
0
0
0
0
0
0
0
1:0
0
0
0
0
0
0
0
0
0
0
1:0
0
0
0
0
0
0
0
0
0
1:0
0
0
0
0
0
0
0
0
0
0
0 0:99
0
0
0
0
0
0
0
0
0
0
1:
3
7777777777777775
Note that the co-variance matrix of the generated sample outcomes is almost identity
matrix. (As expected)
Let mean vector mY D [0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0] and
CY D
2
6666666666666664
0:1
0:2
0:3
0:4
0:5
0:6
0:7
0:8
0:9
1:0
0:2
0:4
0:6
0:8
1:0
1:2
1:4
1:6
1:8
2:0
0:3
0:6
0:9
1:2
1:5
1:8
2:1
2:4
2:7
3:0
0:4
0:8
1:2
1:6
2:0
2:4
2:8
3:2
3:6
4:0
0:5
1:0
1:5
2:0
2:5
3:0
3:5
4:0
4:5
5:0
0:6
1:2
1:8
2:4
3:0
3:6
4:2
4:8
5:4
6:0
0:7
1:4
2:1
2:8
3:5
4:2
4:9
5:6
6:3
7:0
0:8
1:6
2:4
3:2
4:0
4:8
5:6
6:4
7:2
8:0
0:9
1:8
2:7
3:6
4:5
5:4
6:3
7:2
8:1
9:0
1:0
2:0
3:0
4:0
5
6:0
7:0
8:0
9:0
10:0
3
7777777777777775
%m-file for generating the Multivariate Gaussian
distributed sample
%outcomes with mean vector mY and the co-variance
matrix Co-
%variance matrix CY as displayed below
MY D 0.1:0.1:1.0;
CY D [1:1:10;2:2:20;3:3:30;4:4:40;5:5:50;6:6:60;7:7:
70;8:8:80;9:9:90;10:10:100]0.1;
%Representing the matrix CY as the product of two
matrix A and A0 as follows
%As the co-variance matrix is the symmetric matrix,
the eigen values are
%real
[P,Q] D eig(CY);
A D Psqrt(abs(Q));

202
6 Matlab Illustrations
for i D 1:2:10
r D rand(1,1000000);
sigma D 1;
R D sqrt(2sigmaˆ2log(1./(1  r)));
theta D rand(1,1000000)2pi;
u D R.cos(theta);
v D R.sin(theta);
[p1,q1] D hist(u,100)
[p2,q2] D hist(v,100)
datafig D u
datafiC 1g D v
end
X D cell2mat(data’);
Y D AX C repmat(MY’,1,1000000);
CY D cov(Y’);
The co-variance matrix computed for the generated Multivariate Gaussian distribu-
tion sample outcomes is as shown below (as expected) (Figs. 6.2 and 6.3).
CY D
2
6666666666666664
0:1
0:2
0:3
0:4
0:5
0:6
0:7
0:8
0:9
1:0
0:2
0:4
0:6
0:8
1:0
1:2
1:4
1:6
1:8
2:0
0:3
0:6
0:9
1:2
1:5
1:8
2:1
2:4
2:7
3:0
0:4
0:8
1:2
1:6
2:0
2:4
2:8
3:2
3:6
4:0
0:5
1:0
1:5
2:0
2:5
3:0
3:5
4:0
4:5
5:0
0:6
1:2
1:8
2:4
3:0
3:6
4:2
4:8
5:4
6:0
0:7
1:4
2:1
2:8
3:5
4:2
4:9
5:6
6:3
7:0
0:8
1:6
2:4
3:2
4:0
4:8
5:6
6:4
7:2
8:0
0:9
1:8
2:7
3:6
4:5
5:4
6:3
7:2
8:1
9:0
1:0
2:0
3:0
4:0
5
6:0
7:0
8:0
9:0
10:0
3
7777777777777775
6.2
Bacterial Foraging Optimization Technique
Evolutionary algorithms that are formulated from the inspiration of the natural
biological behavior are called Biologically Inspired Algorithms (BIA).Bacterial
Foraging is one of the BIA inspired from the Foraging behavior of the E-Coli Bacte-
ria. During Foraging, Bacteria tries to move towards the region where more nutrients
are available. (i.e.) Moving towards the region where the concentration of nutrients
are large. They pass through the neutral medium and they avoid poisonous sub-
stances. This biological behavior is inspired to formulate the Bacterial Foraging
Optimization technique as described below.
Consider the unconstrained optimization problem of minimizing the function
J.X/; X 2 Rm

6.2 Bacterial Foraging Optimization Technique
203
Fig. 6.2 Sample outcomes of the 10-Variate ŒX1 X2 X3 : : : X10] Gaussian distribution with mean
zero and Identity Co-variance matrix
Fig. 6.3 Sample outcomes of the 10-Variate ŒY1 Y2 Y3 : : : Y10 Gaussian distribution with mean
MY and Co-variance matrix CY
Analogy: The vector X can be viewed as the position of the Bacteria
J.X/ < 0 can be treated as the presence of Nutrients
J.X/ D 0 can be treated as the Neutral
J.X/ > 0 can be treated as the presence of Toxic substances

204
6 Matlab Illustrations
Step 1: Initialization of the population
Initialize the positions of ‘N’ (say) number of Bacteria. Let it be X1; X2; X3; : : : XN
corresponding to ‘N’ Bacterium b1; b2; b3; : : : bN . The position vector X1; X2;
X3; : : : XN is the initial population.
Step 2: Chemo taxis
It is tendency of the bacteria to move towards the sources of Nutrients. It consists of
two stages. They are the following
(a) Tumbling: It is the tendency of the bacteria to change their positions in search
of Nutrients. Let Xi
new be the next position of the ith Bacteria whose current
position is Xi. They are related as described below.
Xi
new D Xi C c'; where ' D

p
T
 2 Rm
such that each element of the vector  is in the range Œ1 1. ˚ is the unit walk
in random direction. ‘c’ is called as chemo tactic step size. The new positions
are computed for i D 1; 2; : : : N
(b) Swimming:
Bacterium will tend to keep on moving in the particular direction if it is in the
direction that is rich in nutrients.
Mathematically if J

Xi
new

< J.Xi/, then another swimming in the same direc-
tion .'/ is taken by the ith Bacteria and it can be continued upto Ns steps. After the
completion of Ns steps Bacteria goes to the step 3.
If J

Xi
new

	 J.Xi/, Bacteria comes out of the tumbling stage and goes to the
step 3.
Step 3: Reproduction
After step 2, best ‘N/2’ (50%) bacteria measured in terms of its Health are survived.
The survived Bacterium are subjected to reproduction to obtain ‘N’ Bacterium as
described below.
Health of the Bacteria is measured in terms of J.X/. If the functional value
J.X/ is less, then the corresponding Bacteria is healthier. Compute J.Xi/ for
i D 1; 2; : : : N . Arrange them in ascending order. First ‘N/2’ Bacterium and the
corresponding positions are selected Let the positions be ŒY 1; Y 2; Y 3; : : : Y N=2.
Every Bacteria is split into two Bacterium and are placed in the same posi-
tions. Thus new set of positions corresponding to ‘N’ Bacterium are given as
ŒY 1; Y 1; Y 2; Y 2; Y 3; Y 3; : : : Y N=2Y N=2 D ŒZ1; Z2; Z3; : : : ZN  (say). Go to
step 2. Repeat 2 and 3 for ﬁnite number of iterations. Then Go to step 4.
Step 4: Elimination-dispersion
In real world process, some of the bacterium (i.e.) with probability ‘Pd’ are dis-
persed to new locations. This is simulated as shown below.
Generate the random vector of size 1  N. Sort the elements of the vector in the
ascending order. Find the index corresponding to the ﬁrst NPd sorted numbers.
Choose the positions of the Bacterium corresponding to the obtained index. They
are replaced with the randomly generated positions on the optimization domain.

6.2 Bacterial Foraging Optimization Technique
205
The positions thus obtained are treated as the current best positions. Go to step
2. Repeat the steps 2–4 for the ﬁnite number of iterations.
The best value in every iteration can also be tracked and the best among them
can be declared as the optimal solution.
Social Communication
In nature there is the social communication between Bacterium such that they are
neither close together nor far away from each other. This is done by releasing the
chemical by the Bacteria. The chemical signal can be either attractant or Repellent.
If the chemical signal released by the particular Bacteria is attractant in nature, then
it attracts other Bacteria to come to its position. On the contrary if the chemical
signal released by the particular Bacteria is Repellent in nature, it doesn’t allow
other Bacteria to come to its position.
The social communication between Bacterium can be simulated using the
modiﬁed objective function to be computed for the ith position corresponding
to the ith position Bacteria as given below.
Jmod.Xi/ D J.Xi/ C Jsocial.Xi/; where
Jmod is the modiﬁed Objective function computed for the ith position Xi corre-
sponding to the ith Bacteria. J.Xi/ is the actual objective function value computed
for the ith position Xi corresponding to the ith Bacteria. social.Xi/ is the attrac-
tant cum repellent signal computed for the ith position Xi corresponding to the ith
Bacteria as displayed below.
Let dij D
ˇˇjXi  Xj j
ˇˇ2
Jsocial.Xi/ D M
0
@
N
X
jD1
eRdij 
N
X
jD1
eAdij
1
A
Note that if the ﬁrst term PN
j D1 eRdij is reduced if distance between the ith position
and others are made large and hence it acts as the repellent signal. Similarly the
second term  PN
j D1 eRdij is reduced if the distance between the ith position and
others are made small and hence it acts as the attractant signal. ‘R’ is the Repellent
factor and ‘A’ is the attractant factor.
The convergence graph obtained using Bacterial Foraging technique for mini-
mizing the function J.X/ D .x1^2  9/^2 C .x2^2  9/^2 C .x3^2  16/^2 is
shown in the ﬁgure given below (Fig. 6.4). The corresponding Matlab program is
also displayed below
After 100 iteration, the best solution obtained is [2.9025 2.9512 4.0720]
bactalgo.m
%Bacterial Foraging Technique
%Edit bactfun.m to insert the objective function to
be minimized
%TRACEERROR traces the minimum error obtained in
every iteration

206
6 Matlab Illustrations
Fig. 6.4 Illustration of the convergence of the bacterial foraging algorithm
%TRACEVAL traces the corresponding best solution in
every iteration
%jsocial regulates the social communication between
bacterium.
pos1 D rand(1,100)10;
pos2 D rand(1,100)10;
pos3 D rand(1,100)10;
vect D [pos1;pos2;pos3]’;
figure
TRACEVALUE D [];
TRACEVECTOR D [];
c D 1;
for i D 1:1:100
Jcurvalue(i) D bactfun(vect(i,:));
Jswcurvalue(i) D Jcurvalue(i) C jsocial(vect(i,:),
vect);
End
%Iteration starts
for iter D 1:1:100
for dispersal D 1:1:50
for survey D 1:1:50
for bact D 1:1:100
pos1 D rand(1,1)10;
pos2 D rand(1,1)10;
pos3 D rand(1,1)10;
phi D [pos1 pos2 pos3];
newvect D vect(bact,:) C cphi;

6.2 Bacterial Foraging Optimization Technique
207
Jcurvaluenew(bact) D bactfun(newvect);
Jswcurvaluenew(bact) D Jcurvalue(bact) C jsocial
(newvect,vect);
%m is the maximum number swimming
for m D 1:1:25
if (Jswcurvaluenew(bact) < Jswcurvalue(bact))
vect(bact,:) D newvect;
Jcurvalue(bact) D bactfun(vect(bact,:));
Jswcurvalue(bact) D Jcurvalue(bact) C jsocial
(vect(bact,:),vect);
newvect D vect(bact,:) C cphi;
Jcurvaluenew(bact) D bactfun(newvect);
Jswcurvaluenew(bact) D Jswcurvaluenew(bact) C
jsocial(newvect,vect);
else
m D 25;
end
end
end
%Reproduction of bacteria
[p,q] D sort(Jswcurvaluenew);
vect1 D vect(q(1:1:50),:);
vect D [vect1;vect1];
end
%Dispersal of bacteria with probability 0.2
[p,q] D sort(rand(1,100));
tempvect1 D vect(q(1:1:80),:);
pos1 D rand(1,20)10;
pos2 D rand(1,20)10;
pos3 D rand(1,20)10;
tempvect2 D [pos1;pos2;pos3]’;
vect D [tempvect1;tempvect2];
for i D 1:1:100
Jcurvalue(i) D bactfun(vect(i,:));
Jswcurvalue(i) D Jcurvalue(i) C jsocial(vect(i,:),
vect);
end
end
[p,q] D sort(Jswcurvalue);
temp D vect(q(1),:);
TRACEVALUE D [TRACEVALUE bactfun(temp)];
TRACEVECTOR D [TRACEVECTOR; temp];
hold on
plot(TRACEVALUE)
pause(0.2);

208
6 Matlab Illustrations
end
%Solution
for i D 1:1:100
Jcurvalue(i) D bactfun(vect(i,:));
end
[p,q] D sort(TRACEVALUE);
FINALRES D TRACEVECTOR(q(1),:);
jsocial.m
function [res] D jsocial(a,vect)
M D 1;
b D repmat(a,[100 1]);
vect D vect  b;
vect D vect.^2;
vect D sum(vect’);
Wa D 2;
Wr D 2;
res1 D sum(exp(vect(Wa)));
res2 D sum(exp(vect(Wr)));
bactfun.m
function [res] D bactfun(z);
p D z(1);
q D z(2);
r D z(3);
res D .p^2  9/^2 C .q^2  9/^2 C .r^2  16/^2;
6.3
Particle Swarm Optimization
The Particle swarm optimization is the biologically inspired algorithm inspired from
the behavior of birds on deciding the optimal path to move from the particular source
to the destination. Consider the task of movement of group of birds (say A,B,C) from
the particular source point ‘S’ to the destination ‘D’. Let ‘A’ (which is currently at
point PA) the decides to move towards the point P 0
A. Similarly the bird ‘B’ and ‘C’
decides to move towards the point P 0
B and P 0
C from PB and PC respectively. Let
the distance between the point P 0
C and the destination point ‘D’ is less compared
to the distance between the point P 0
B and the destination point ‘D’ and the distance
between the point P 0
B and the destination point ‘D’. Thus the ﬁnal decision taken
by the bird ‘B’ is the combination of the individual decision taken by bird ‘B’ and
the best global decision taken by the neighboring birds (In the current scenario, the
best global decision is the decision taken by the bird ‘C’).
Mathematically the bird moves from the current position to the next position
described as follows.
The next position moved by the bird ‘B’ is
PB C gB

P 0
C  PB

C lB

P 0
B  PB


6.3 Particle Swarm Optimization
209
Similarly the next position moved by the bird ‘A’ and ‘C’ are given below
PA C gA

P 0
C  PA

C lA

P 0
B  PA

PC C gC

P 0
C  PC

where ‘gA; gB; gC ’ are the global constants and ‘lA; lB’ are the local constants.
Consider the unconstrained optimization problem of minimizing the function
J.X/; X 2 Rm.
Analogy:
The vector ‘X’ in the above deﬁnition is treated as the current position of the bird
in the PSO algorithm. The corresponding value J.X/ is the distance between the
current position of the bird and the destination. The PSO algorithm identiﬁes the
shortest path as described above so that it reaches the destination as early as possible.
(i.e.) Identifying the optimal value of X such that J.X/ is minimized
Algorithm
Step 1: Initialize the positions of ‘N ’ number of birds. Let it be X1; X2; X3; : : : XN .
Step 2: Obtain the next positions of the ‘N’ birds using the combination of local
decision taken by the individual birds and the best decision taken by the neighboring
birds (as described earlier).
Step 3: The next positions thus obtained are treated as current positions.
Local decisions taken by the individual birds for the next move are taken as per
the procedure given below.
If the distance between current local decision taken by the ith bird position and
the destination is greater than the distance between the next position mentioned in
the step 2 and the destination, next position is treated as the local decision taken
by the ith bird position for the next movement. Otherwise the current local decision
taken by the ith bird position is considered as the local decision for the next move
also. Repeat step 2 and 3 for the ﬁnite number of iterations.
Step 4: Best position among the ‘N ’ positions of the birds in the last iteration cor-
responding to ‘N ’ birds is declared as the ﬁnal solution for minimizing the function
J.X/. Best position among the ‘N ’ positions in the ﬁnal iteration is the position
whose distance from the destination is minimum among all other positions.
(i.e.) Best positionD argi.kXiDk/; i D 1; 2; 3; : : : N
The convergence graph obtained using Particle Swarm Optimization for mini-
mizing the function J.X/ D 2.x1  4/^2 C 4.x2  3/^2 C 5.x3  6/^2 is shown
in the Fig. 6.5. The corresponding Matlab program is also displayed below.
For the problem mentioned above the vector of the form
2
4
x1
x2
x3
3
5 is treated as the
arbitrary position of the bird. PSO algorithm is performed to obtain the optimal
value of the position of the bird such that its distance from the destination is mini-
mized. The distance between the current position of the bird and the destination is
measured using the objective function J.X/ and hence the function is minimized.

210
6 Matlab Illustrations
Fig. 6.5 Illustration of the convergence of the PSO algorithm
Immediate after ﬁfth iteration, the solution obtained using PSO algorithm is given
as [3.9999 3 6] and the is corresponding
6.4
Newton’s Iterative Method
Consider the problem of minimizing the function f .x1; x2; x3/ D 2.x1  4/2 C
4.x2  3/2 C 5.x3  6/2
Expressing the above function using Taylor series we get the following.
f
0
@
2
4
x1
x2
x3
3
5 C
2
4
x1
x2
x3
3
5
1
A D f
0
@
2
4
x1
x2
x3
3
5
1
A C Df
0
@
2
4
x1
x2
x3
3
5
1
A C D2
2Š f 2
0
@
2
4
x1
x2
x3
3
5
1
A C : : :
where D D x1 @
@x1 C x2
@
@x2 C x3
@
@x3.
The above series can also be represented in the matrix form as shown below
f
0
@
2
4
x1
x2
x3
3
5 C
2
4
x1
x2
x3
3
5
1
A
D f
0
@
2
4
x1
x2
x3
3
5
1
A C Œx1 x2 3
2
666664
@f
@x1
@f
@x1
@f
@x1
3
777775

6.4 Newton’s Iterative Method
211
C Œx1 x2 3
2
66666664
@2f
@x12
@2f
@x1@x2
@2f
@x1@x3
@2f
@x2@x1
@2f
@x22
@2f
@x2@x3
@2f
@x3@x1
@2f
@x3@x2
@2f
@x32
3
77777775
2
4
x1
x2
x3
3
5
C : : :
Let
2
4
x1
x2
x3
3
5 C
2
4
x1
x2
x3
3
5 D
2
664
xnC1
1
xnC1
2
xnC1
3
3
775 and
2
4
x1
x2
x3
3
5 D
2
664
xn
1
xn
2
xn
3
3
775
Rewriting the Taylor series using the notations used above we get,
f
0
B@
2
64
xnC1
1
xnC1
2
xnC1
3
3
75
1
CA D f
0
B@
2
64
xn
1
xn
2
xn
3
3
75
1
CA C

xnC1
1
 xn
1
xnC1
2
 xn
2
xnC1
3
 xn
3
	
2
66666664
@f
@x1
@f
@x2
@f
@x3
3
77777775
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA C

xnC1
1
 xn
1
xnC1
2
 xn
2
xnC1
3
 xn
3
	
2
66666664
@2f
@x1@x1
@2f
@x1@x2
@2f
@x1@x3
@2f
@x2@x1
@2f
@x2@x2
@2f
@x2@x3
@2f
@x3@x1
@2f
@x3@x2
@2f
@x3@x3
3
77777775
0
BB@at
2
664
xn
1
xn
2
xn
3
3
775
1
CCA
2
664
xnC1
1
 xn
1
xnC1
2
 xn
2
xnC1
3
 xn
3
3
775 C : : :
If we want to ﬁnd out the roots of the above equation, we equate f
0
BB@
2
664
xnC1
1
xnC1
2
xnC1
3
3
775
1
CCA D 0
and solve for the vector
2
664
xnC1
1
xnC1
2
xnC1
3
3
775 as shown below

212
6 Matlab Illustrations
(Considering only ﬁrst two terms)
f
0
B@
2
64
xn
1
xn
2
xn
3
3
75
1
CA D 

xnC1
1
 xn
1
xnC1
2
 xn
2
xnC1
3
 xn
3
	
2
666664
@f
@x1
@f
@x2
@f
@x3
3
777775
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA
)

xnC1
1
xnC1
2
xnC1
2
	
D

xn
1 xn
2 xn
2
	
 f
0
B@
2
64
xn
1
xn
2
xn
3
3
75
1
CA
2
666664
@f
@x1
@f
@x2
@f
@x3
3
777775
1
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA
The above mentioned equation is called Newton’s method of computing the roots of
the multivariable equation f .x1; x2; x3/.
Note:
Note that
2
6664
@f
@x1
@f
@x2
@f
@x3
3
7775
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA is the column vector and hence the inverse mentioned
in the equation is the pseudo inverse (see Chapter 1 for details).
Consider the Taylor series equation as mentioned below
f
0
B@
2
64
xnC1
1
xnC1
2
xnC1
3
3
75
1
CA D f
0
B@
2
64
xn
1
xn
2
xn
3
3
75
1
CA C

xnC1
1
 xn
1
xnC1
2
 xn
2
xnC1
3
 xn
3
	
2
66666664
@f
@x1
@f
@x2
@f
@x3
3
77777775
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA C : : :

6.4 Newton’s Iterative Method
213
Differentiating with respect to the vector
2
4
x1
x2
x3
3
5 on both sides we get the following
rf
0
B@at
2
64
xnC1
1
xnC1
2
xnC1
3
3
75
1
CA D
2
666664
@f
@x1
@f
@x2
@f
@x3
3
777775
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA C
2
66666664
@2f
@x1@x1
@2f
@x1@x2
@2f
@x1@x3
@2f
@x2@x1
@2f
@x2@x2
@2f
@x2@x3
@2f
@x3@x1
@2f
@x3@x2
@2f
@x3@x3
3
77777775
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA
2
64
xnC1
1
 xn
1
xnC1
2
 xn
2
xnC1
3
 xn
3
3
75
Also to consider the point
2
64
xnC1
1
xnC1
2
xnC1
3
3
75 as the extremal point of the equation
f .x1; x2; x3/, then rf
0
B@at
2
64
xnC1
1
xnC1
2
xnC1
3
3
75
1
CA D 0.
Using the above condition and solving the expression for extremal point we get
2
64
xnC1
1
xnC1
2
xnC1
3
3
75 D
2
64
xn
1
xn
2
xn
3
3
75 
2
66666664
@2f
@x1@x1
@2f
@x1@x2
@2f
@x1@x3
@2f
@x2@x1
@2f
@x2@x2
@2f
@x2@x3
@2f
@x3@x1
@2f
@x3@x2
@2f
@x3@x3
3
77777775
1
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA
2
6666664
@f
@x1
@f
@x2
@f
@x3
3
7777775
0
B@at
2
64
xn
1
xn
2
xn
3
3
75
1
CA
Consider the problem of minimizing the function f .x1; x2; x3/ D 2.x1  4/2 C
4.x2  3/2 C 5.x3  6/2

214
6 Matlab Illustrations
Newton’s iteration equation is formulated as shown below.
2
64
xnC1
1
xnC1
2
xnC1
3
3
75 D
2
64
xn
1
xn
2
xn
3
3
75 
2
4
1=4
0
0
0
1=8
0
0
0
1=10
3
5
2
64
4

xn
1  4

8

xn
1  3

12

xn
3  6

3
75
Let us initialize the extremum vector as
2
4
0
0
0
3
5
Iterations are performed using Matlab and Error (vs) Iteration table is mentioned
below for illustration.
Iteration
1
2
3
4
5
Error
7.2
0.2880
0.0115
0.0005
0.0000
Note that Error function converges to zero immediate after reaching ﬁfth iteration.
6.5
Steepest Descent Algorithm
Consider the problem of minimizing the function f .x1; x2; x3/ D 2.x1  4/2 C
4.x2  3/2 C 5.x3  6/2
The Linear approximation of the curve f .x1; x2; x3/ can be obtained as follows
f .x1 C k 1; x2 C k 2; x3 C k 3/
D f .x1; x2; x3/ C k
 @f
@x1
@f
@x2
@f
@x3
 2
4
1
2
3
3
5
.i:e/f .X C k/ D f .X/ C krf T 
2
4
1
2
3
3
5 is the unit vector and k is some scalar constant
The maximum increase in the value of the function f .x1; x2; x3/ (i.e.) f .x1 C
k 1; x2 C k2; x3 C k3/  f .x1; x2; x3/ occurs when
h
@f
@x1
@f
@x2
@f
@x3
i
2
4
1
2
3
3
5
is maximum. We also know from Cauchy-Schwarz inequality (see Chapter 4)

6.5 Steepest Descent Algorithm
215
that maximum value of the inner product
h
@f
@x1
@f
@x2
@f
@x3
i
2
4
1
2
3
3
5 occurs only when
2
4
1
2
3
3
5 D l
2
664
@f
@x1
@f
@x2
@f
@x3
3
775, where ‘l’ is some scalar constant.
Similarly maximum decrease in the function occurs when
2
4
1
2
3
3
5 D l
2
664
@f
@x1
@f
@x2
@f
@x3
3
775
Let
2
4
xn
1
xn
2
xn
2
3
5 be the current value of the vector X used in the steepest descent iter-
ation algorithm. Then the next best value for the vector X represented as
2
64
xnC1
1
xnC1
2
xnC1
2
3
75
such that the vector is in the direction of decreasing function f (X) is given as
follows.
250
Error value (vs) Iteration
200
150
100
Error value
Iteration
50
0
0
10
20
30
40
50
60
70
80
90
100
Fig. 6.6 Illustration of the convergence of the steepest descent algorithm

216
6 Matlab Illustrations
2
64
xnC1
1
xnC1
2
xnC1
2
3
75 D
2
64
xn
1
xn
2
xn
2
3
75  l
2
666664
@f
@x1
@f
@x2
@f
@x3
3
777775
For the above problem the iterative equation is obtained as follows
2
64
xnC1
1
xnC1
2
xnC1
2
3
75 D
2
64
xn
1
xn
2
xn
2
3
75  l
2
4
4 .x1  4/
8 .x2  3/
10 .x3  6/
3
5
Let us initialize the vector D
2
4
0
0
0
3
5. Iterations are performed using Matlab and the
convergence graph is plotted for illustration (Fig. 6.6).
The learning rate is chosen as l D 0:01. After 100 iterations, function value
reaches 0.0091 and the corresponding vector obtained is
2
4
3:9325
2:9993
5:9998
3
5.

Index
A
Algebraic multiplicity, 44–46, 171
B
Bacterial foraging, 202–208
Band limited random process, 136, 137, 303
Band pass random process, 144–146, 148–152
Basis, 12–20, 22–26, 28, 34, 36–40, 42, 45,
56–58, 154, 157–159, 162–172, 174,
175, 177–179, 191–193
Bayes theorem, 70
Block multiplication, 4, 5
C
Cayley-Hamilton theorem, 167
Central limit theorem, 122
Characteristic polynomial, 42, 167, 171
Chebyshev inequality, 100
Chernoff bound, 101, 106–107
Column space, 14–22, 27–30, 32–38, 42
Complex random process, 127, 128
Complex random variable, 118–119
Conditional probability, 69, 82, 103
Constrained optimization, 181–189, 193–194
Contours, 110–112
Convergence, 119–122, 205, 206, 209, 210,
215, 216
Correlation-coefﬁcient, 101
Correlation matrix, 104, 107, 128
Co-variance matrix, 104, 107–118, 197–203
Cumulative distribution function (cdf), 75–76,
89, 102, 124
Cyclo-stationary process, 140
Cyclo stationary random process, 139–142
D
Diagonalization of the matrix, 47–49, 58–60
Direct sum, 28, 33, 160–162, 168, 170–173
Dual optimization problem, 194–195
Dual space, 158–160
E
Eigen basis, 168, 191, 192
Eigen space, 168, 169, 171–173
Eigen values, 42–47, 49–54, 57, 61–63,
167–169, 171–173, 189–193, 200, 201
Empty set, 67
Ensemble average, 128–132
Ergodic process, 128–132
Event, 67–74, 75, 78–80, 82, 104, 120
Expectations, 99, 103, 126
Extremum, 181–183, 186, 187, 189, 191, 193,
214
G
Gaussian density function, 84, 105–107, 198
Gaussian random process, 138
Generalized Eigen space, 169, 171–173
Geometric multiplicity, 44–46, 171
Gradient vector, 181–183, 186
Gram Schmidt orthogonalization procedure,
37, 38, 40
H
Hermitian matrix, 50–52, 56, 57, 61
Hessian matrix, 185, 187–193
Hilbert transformation, 146–150
217

218
Index
I
Identity matrix, 5, 7, 10, 11, 17, 29, 33, 36, 42,
52, 58, 167, 176, 199
Image, 24, 75, 157, 172
Independence, 12–13, 70–71
Independent random process, 132
Indicator, 100–101
Injective, 155, 156
Inner product space, 176–177
Invariant space, 168
Isomorphic, 155–157, 162, 164, 167
J
Jordon basis, 291
K
Kernel, 157, 172, 173, 176, 179
Kuhn-Tucker conditions, 195–196
L
LDU decomposition, 7–10
Learning rate, 216
Left Null space, 14, 18–22, 27, 28, 33, 34
Linearly independent, 13, 26, 153, 154, 157,
172, 178
Low pass random process, 145, 148–152
M
Markov inequality, 100
Matlab, 197–216
Minimal polynomial, 167–169, 171, 172, 176
Moment generating function, 102–107, 114
Multivariate Gaussian distribution, 197–202
Mutually exclusive set, 67
N
Newton’s iterative method, 210–214
Nilpotent transformation, 173–175
Non-deﬁcient matrix, 49, 56–60
Normal matrix, 56–58
Null space, 14, 16–22, 27–29, 31–34, 42, 43,
45, 56, 67, 190–193
O
Orthogonal basis, 26, 57, 177–179
Orthogonal complement, 27, 178
P
Particle swarm optimization, 208–210
Partition, 67, 70, 72
Poison random variable, 114
Positive deﬁnite matrix, 189, 191
Power spectral density (PSD), 134–137
Probability density function, 77–79, 82, 85,
89, 90, 92, 93, 102, 103, 105, 110, 111,
115, 122, 126, 128, 199
Probability space, 68, 75, 77, 118
Projection matrix, 35, 36, 38
Pseudo inverse, 63, 64, 66, 212
Q
QR factorization, 40–42
R
Random process, 123–152
Random variable, 75–80, 82, 84–100,
102–107, 109, 113–115, 118–122,
124–128, 138, 197, 198, 199
Random vector, 102–118, 123, 132, 138, 199,
200, 204
Rank-Nullity theorem, 22, 158
Rayleigh distribution, 198
Riegtz representation, 179
Row space, 14, 18–22, 27–29, 34
S
Sample space, 67, 68, 70–72, 120, 123
Schur’s lemma, 49–50, 52
Schwarz inequality, 100
Sequence of random variables, 119–122
Set, 12–16, 18, 19, 26, 36, 39, 40, 47, 57,
64, 67, 68, 75, 76, 86, 95, 123, 128,
138, 153–154, 157–159, 166, 168–170,
176–178, 181, 183, 186, 187, 190, 192,
193, 197, 199, 204
Similar matrices, 26, 45, 164–166
Singular value decomposition (SVD), 36,
60–66
Skew Hermitian matrix, 50–52
Span, 12–15, 189
Square matrix, 5–7, 42, 49, 169
Steepest-descent algorithm, 214–216
Strictly stationary random process, 124–125,
139
Subset, 12, 67, 68, 75, 76, 153
Subspace, 12, 14, 21, 22, 27, 153, 154, 157,
159, 160, 168, 178
Surjective, 155, 156

Index
219
T
Tangent vector, 181–185
Taylor series, 181, 210–212
Time average, 129–132
Total probability theorem, 70
Transformation matrix, 24–26, 109, 162–172,
174, 175, 199, 200
Transpose, 5, 22, 49, 50, 52, 60
U
Uncorrelated random process, 132
Unitary matrix, 49, 50, 52–58, 60, 62, 65, 66
V
Vector space, 12–21, 24, 26–28, 33–36, 39, 45,
56–58, 153–154, 157, 158, 160, 162,
163, 166–168, 170–172, 176–179, 189
W
White random process, 137–138
Wide sense cyclo stationary random process,
139–142
Wide sense stationary random process,
125–127


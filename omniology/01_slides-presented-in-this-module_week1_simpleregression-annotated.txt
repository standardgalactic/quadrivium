Machine	  Learning	  Specializa0on	  
Simple Regression: 
Linear regression with one input 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Recall Task:  
Predicting house prices 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
3	  
How much is my house worth? 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
I want to list 
my house 
for sale 

Machine	  Learning	  Specializa0on	  
4	  
How much is my house worth? 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
$$ ???? 

Machine	  Learning	  Specializa0on	  
5	  
Look at recent sales in my neighborhood 
•  How much did they sell for? 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Regression fundamentals:  
data, model, task 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Data 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
(x1 = sq.ft., y1 = $) 
(x2 = sq.ft., y2 = $) 
(x3 = sq.ft., y3 = $) 
(x4 = sq.ft., y4 = $) 
… 
(x5 = sq.ft., y5 = $) 
input 
output 

Machine	  Learning	  Specializa0on	  
Data 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
(x1 = sq.ft., y1 = $) 
(x2 = sq.ft., y2 = $) 
(x3 = sq.ft., y3 = $) 
(x4 = sq.ft., y4 = $) 
Input vs. Output: 
•  y is the quantity of interest 
•  assume y can be predicted from x 
input 
output 
… 
(x5 = sq.ft., y5 = $) 

Machine	  Learning	  Specializa0on	  
10	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Model –  
How we assume the world works 
Regression model: 

Machine	  Learning	  Specializa0on	  
11	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
“Essen0ally,	  all	  models	  are	  
wrong,	  but	  some	  are	  useful.”	  	  
George	  Box,	  1987.	  
Model –  
How we assume the world works 

Machine	  Learning	  Specializa0on	  
12	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Task 1–  
Which model f(x)? 
y 
sq.ft. 
price ($) 
x 
y 
sq.ft. 
price ($) 
x 
y 
sq.ft. 
price ($) 
x 
y 
sq.ft. 
price ($) 
x 

Machine	  Learning	  Specializa0on	  
13	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Task 2 – For a given model f(x), 
estimate function f(x) from data 
⌃ 

Machine	  Learning	  Specializa0on	  
14	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
x 
ŷ 
⌃f 

Machine	  Learning	  Specializa0on	  
Simple linear regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
17	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
x 
ŷ 
⌃f 

Machine	  Learning	  Specializa0on	  
18	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Simple linear regression model 
yi = w0+w1 xi + εi 
f(x) = w0+w1 x 

Machine	  Learning	  Specializa0on	  
19	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Simple linear regression model 
parameters:   
regression coeﬃcients  
 
yi = w0+w1 xi + εi 

Machine	  Learning	  Specializa0on	  
Fitting a line to data 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
21	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
ŵ 
y 
x 
ŷ 
⌃f 

Machine	  Learning	  Specializa0on	  
23	  
“Cost” of using a given line 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Residual sum of squares (RSS) 
RSS(w0,w1) =  
   ($house 1-[w0+w1sq.ft.house 1])2 
+ ($house 2-[w0+w1sq.ft.house 2])2 
+ ($house 3-[w0+w1sq.ft.house 3])2 
+ …[include all training houses] 
 

Machine	  Learning	  Specializa0on	  
24	  
“Cost” of using a given line 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Residual sum of squares (RSS) 
RSS(w0,w1) =      (yi-[w0+w1xi])2 

Machine	  Learning	  Specializa0on	  
25	  
Find “best” line 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Minimize cost over all  
possible w0,w1 
RSS(w0=1.1,w1=0.8) 
RSS(w0=0.97,w1=0.85) 
RSS(w0=0.98,w1=0.87) 
RSS(w0,w1) = 
   
      (yi-[w0+w1xi])2 

Machine	  Learning	  Specializa0on	  
The ﬁtted line: use + interpretation 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
29	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Model vs. ﬁtted line 
Estimated parameters: 
ŵ0 , ŵ1
  
f(x) = ŵ0 + ŵ1
 x 
⌃ 
yi = w0+w1 xi + εi 
 
Regression model: 

Machine	  Learning	  Specializa0on	  
30	  
yi = w0+w1 xi + εi 
 
Seller:  
Predicting your house price 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Best guess of your 
house price: 
f(x) = ŵ0 + ŵ1
 x 
ŷhouse= ŵ0
 + ŵ1
 sq.ft.house 
  
Regression model: 
⌃ 

Machine	  Learning	  Specializa0on	  
31	  
Buyer:  
Predicting size of house 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Best guess of size of 
house you can aﬀord: 
f(x) = ŵ0 + ŵ1
 x 
$in bank = ŵ0
 + ŵ1
 sq.ft.  
yi = w0+w1 xi + εi 
 
Regression model: 
⌃ 

Machine	  Learning	  Specializa0on	  
32	  
A concrete example 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
f(x) = -44850 + 280.76 x 
Predict $ of 2,640 sq.ft. house: 
-44850 + 280.76 * 2,640 
= $696,356 
Predict sq.ft. of $859,000 sale: 
(859000+44850)/ 280.76 
= 3,219 sq.ft. 
 
⌃ 

Machine	  Learning	  Specializa0on	  
34	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Interpreting the coeﬃcients 
Predicted $ 
of house with 
sq.ft.=0  
(just land) 
ŷ = ŵ0 + ŵ1
 x 
 

Machine	  Learning	  Specializa0on	  
35	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Interpreting the coeﬃcients 
ŷ = ŵ0 + ŵ1
 x 
 
1 sq. ft. 
predicted 
change in $ 

Machine	  Learning	  Specializa0on	  
36	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Interpreting the coeﬃcients 
1 sq. ft. 
predicted 
change in $ 
Warning: magnitude depends 
on units of both  
features and observations 
ŷ = ŵ0 + ŵ1
 x 
 

Machine	  Learning	  Specializa0on	  
38	  
A concrete example 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
f(x) = -$44,850 + 280.76 ($/sq.ft.) x 
Predict $ of 2,640 sq.ft. house: 
-$44,850 + 280.76 ($/sq.ft.) * 2,640 sq.ft. 
= $696,356 
Predict sq.ft. of $859,000 sale: 
($859,000+$44,850)/ 280.76 ($/sq.ft.) 
= 3,219 sq.ft. 
 
⌃ 

Machine	  Learning	  Specializa0on	  
39	  
A concrete example 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
f(x) = -$44,850 + 280.76 ($/sq.ft.) x 
But what if: 
-  House was measured in  
square meters? 
-  Price was measured in RMB?  
⌃ 

Machine	  Learning	  Specializa0on	  
Algorithms for ﬁtting the model 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
42	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
ŵ 
y 
x 
ŷ 

Machine	  Learning	  Specializa0on	  
44	  
Find “best” line 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Minimize cost over all  
possible w0,w1 
RSS(w0=1.1,w1=0.8) 
RSS(w0=0.97,w1=0.85) 
RSS(w0=0.98,w1=0.87) 
RSS(w0,w1) = 
   
      (yi-[w0+w1xi])2 

Machine	  Learning	  Specializa0on	  
45	  
Minimizing the cost 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
min         (yi-[w0+w1xi])2 
w0,w1 
RSS(w0,w1) is a function 
of 2 variables 
Minimize function 
over all possible w0,w1 

Machine	  Learning	  Specializa0on	  
An aside on optimization 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
49	  
Convex/concave functions 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
CONCAVE 
CONVEX 
NEITHER 
g(a)
g(b)

Machine	  Learning	  Specializa0on	  
50	  
Finding the max or min 
analytically 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
CONCAVE 
CONVEX 
NEITHER 
Example: 
g(w) = 5-(w-10)2 

Machine	  Learning	  Specializa0on	  
51	  
Finding the max  
via hill climbing 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
52	  
Finding the min  
via hill descent 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) - η dg  
dw w(t)	  

Machine	  Learning	  Specializa0on	  
53	  
Choosing the stepsize— 
Fixed stepsize 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
54	  
Choosing the stepsize— 
Decreasing stepsize 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Common choices: 

Machine	  Learning	  Specializa0on	  
Convergence criteria 
For convex functions, 
optimum occurs when 
 
 
In practice, stop when 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) - η dg  
dw w(t)	  

Machine	  Learning	  Specializa0on	  
56	  
Moving to multiple dimensions: 
Gradients 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
 Δ 
g(w) = 

Machine	  Learning	  Specializa0on	  
57	  
Gradient example 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
 Δ 
g(w) = 
g(w) = 5w0+10w0w1 + 2w1
2 

Machine	  Learning	  Specializa0on	  
58	  
Contour plots 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
59	  
Gradient descent 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) - η   g(w(t))  
  Δ 

Machine	  Learning	  Specializa0on	  
Finding the least squares line 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
63	  
Find “best” line 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Minimize cost over all  
possible w0,w1 
min         (yi-[w0+w1xi])2 
w0,w1 
CONVEX 

Machine	  Learning	  Specializa0on	  
64	  
Compute the gradient 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Aside:  
RSS(w0,w1) =       (yi-[w0+w1xi])2 
d
dw
N
X
i=1
gi(w) = d
dw (g1(w) + g2(w) + . . . gN(w))
= d
dwg1(w) + d
dwg2(w) + . . . d
dwgN(w)
=
N
X
i=1
d
dwgi(w)

Machine	  Learning	  Specializa0on	  
65	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Taking the derivative w.r.t. w0  
Compute the gradient 
RSS(w0,w1) =       (yi-[w0+w1xi])2 

Machine	  Learning	  Specializa0on	  
66	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Taking the derivative w.r.t. w1  
Compute the gradient 
RSS(w0,w1) =       (yi-[w0+w1xi])2 

Machine	  Learning	  Specializa0on	  
67	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
RSS(w0,w1) =       (yi-[w0+w1xi])2 
Putting it together: 
Compute the gradient 
-2     [yi – (w0+w1xi)] 
 
-2     [yi – (w0+w1xi)]xi 
 
 Δ 
RSS(w0,w1 ) = 

Machine	  Learning	  Specializa0on	  
68	  
-2     [yi – (w0+w1xi)] 
 
-2     [yi – (w0+w1xi)]xi 
 
 Δ 
RSS(w0,w1 ) = 
Approach 1: Set gradient = 0 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
69	  
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
Interpreting the gradient: 
-2     [yi – (w0+w1xi)] 
 
-2     [yi – (w0+w1xi)]xi 
 
 Δ 
RSS(w0,w1 ) = 
-2     [yi – ŷi(w0,w1)] 
 
-2     [yi – ŷi(w0,w1)]xi 
 
= 
Approach 2: Gradient descent 

Machine	  Learning	  Specializa0on	  
70	  
Approach 2: Gradient descent 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
-2     [yi – ŷi(w0,w1)] 
 
-2     [yi – ŷi(w0,w1)]xi 
 
 Δ 
RSS(w0,w1 ) = 

Machine	  Learning	  Specializa0on	  
73	  
Comparing the approaches 
•  For most ML problems,  
cannot solve gradient = 0 
•  Even if solving gradient = 0 
is feasible, gradient descent 
can be more eﬃcient 
•  Gradient descent relies on 
choosing stepsize and  
convergence criteria 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Inﬂuence of high leverage points 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
Asymmetric errors 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
79	  
Symmetric cost functions 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
Residual sum of squares (RSS) 
RSS(w0,w1) =      (yi-[w0+w1xi])2 
Assumes cost of over-
estimating sales price is same 
as under-estimating 

Machine	  Learning	  Specializa0on	  
80	  
Asymmetric cost functions 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  
square feet (sq.ft.) 
price ($) 
x 
y 
What if cost of listing house 
too high has bigger cost? 
  
Too high à no oﬀers ($=0)  
Too low à oﬀers for lower $ 
diﬀerent solution 

Machine	  Learning	  Specializa0on	  
Summary for  
simple linear regression 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  

Machine	  Learning	  Specializa0on	  
83	  
What you can do now… 
•  Describe the input (features) and output (real-valued 
predictions) of a regression model 
•  Calculate a goodness-of-ﬁt metric (e.g., RSS) 
•  Estimate model parameters to minimize RSS using 
gradient descent  
•  Interpret estimated model parameters 
•  Exploit the estimated model to form predictions 
•  Discuss the possible inﬂuence of high leverage points 
•  Describe intuitively how ﬁtted line might change 
when assuming diﬀerent goodness-of-ﬁt metrics 
©2015	  Emily	  Fox	  &	  Carlos	  Guestrin	  


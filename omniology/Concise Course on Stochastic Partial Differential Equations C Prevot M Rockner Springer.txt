Lecture Notes in Mathematics
1905
Editors:
J.-M. Morel, Cachan
F. Takens, Groningen
B. Teissier, Paris

Claudia Pr´evˆot · Michael R¨ockner
A Concise Course
on Stochastic Partial
Differential Equations
ABC

Authors
Claudia Pr´evˆot
Fakult¨at f¨ur Mathematik
Universit¨at Bielefeld
Universit¨atsstr. 25
33615 Bielefeld
Germany
e-mail: cprevot@web.de
Michael R¨ockner
Fakult¨at f¨ur Mathematik
Universit¨at Bielefeld
Universit¨atsstr. 25
33615 Bielefeld
Germany
e-mail: roeckner@math.uni-bielefeld.de
Departments of Mathematics
and Statistics
Purdue University
150 N. University St.
West Lafayette, IN 47907-2067
USA
e-mail: roeckner@math.purdue.edu
Library of Congress Control Number: 2007925694
Mathematics Subject Classiﬁcation (2000): 35-XX, 60-XX
ISSN print edition: 0075-8434
ISSN electronic edition: 1617-9692
ISBN-10 3-540-70780-8 Springer Berlin Heidelberg New York
ISBN-13 978-3-540-70780-6 Springer Berlin Heidelberg New York
DOI 10.1007/978-3-540-70781-3
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting,
reproduction on microﬁlm or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9,
1965, in its current version, and permission for use must always be obtained from Springer. Violations are
liable for prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
c⃝Springer-Verlag Berlin Heidelberg 2007
The use of general descriptive names, registered names, trademarks, etc. in this publication does not imply,
even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective laws
and regulations and therefore free for general use.
Typesetting by the authors and SPi using a Springer LATEX macro package
Cover design: WMXDesign GmbH, Heidelberg
Printed on acid-free paper
SPIN: 11982159
VA41/3100/SPi
5 4 3 2 1 0

Contents
1. Motivation, Aims and Examples
1
2. Stochastic Integral in Hilbert Spaces
5
2.1. Inﬁnite-dimensional Wiener processes
. . . . . . . . . . . . . .
5
2.2. Martingales in general Banach spaces . . . . . . . . . . . . . . .
17
2.3. The deﬁnition of the stochastic integral
. . . . . . . . . . . . .
21
2.3.1.
Scheme of the construction of the stochastic integral . .
22
2.3.2.
The construction of the stochastic integral
in detail . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.4. Properties of the stochastic integral . . . . . . . . . . . . . . . .
35
2.5. The stochastic integral for cylindrical Wiener processes
. . . .
39
2.5.1.
Cylindrical Wiener processes
. . . . . . . . . . . . . . .
39
2.5.2.
The deﬁnition of the stochastic integral
. . . . . . . . .
41
3. Stochastic Diﬀerential Equations in Finite Dimensions
43
3.1. Main result and a localization lemma . . . . . . . . . . . . . . .
43
3.2. Proof of existence and uniqueness . . . . . . . . . . . . . . . . .
49
4. A Class of Stochastic Diﬀerential Equations
55
4.1. Gelfand triples, conditions on the coeﬃcients and examples
. .
55
4.2. The main result and an Itˆo formula . . . . . . . . . . . . . . . .
73
4.3. Markov property and invariant measures . . . . . . . . . . . . .
91
A. The Bochner Integral
105
A.1. Deﬁnition of the Bochner integral . . . . . . . . . . . . . . . . . 105
A.2. Properties of the Bochner integral
. . . . . . . . . . . . . . . . 107
B. Nuclear and Hilbert–Schmidt Operators
109
C. Pseudo Inverse of Linear Operators
115
D. Some Tools from Real Martingale Theory
119
E. Weak and Strong Solutions: Yamada–Watanabe Theorem
121
E.1. The main result . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
F. Strong, Mild and Weak Solutions
133
V

VI
Contents
Bibliography
137
Index
140
Symbols
143

1. Motivation, Aims
and Examples
These lectures will concentrate on (nonlinear) stochastic partial diﬀerential
equations (SPDEs) of evolutionary type. All kinds of dynamics with stochas-
tic inﬂuence in nature or man-made complex systems can be modelled by
such equations. As we shall see from the examples, at the end of this section
the state spaces of their solutions are necessarily inﬁnite dimensional such
as spaces of (generalized) functions. In these notes the state spaces, denoted
by E, will be mostly separable Hilbert spaces, sometimes separable Banach
spaces.
There is also enormous research activity on SPDEs, where the state spaces
are not linear, but rather spaces of measures (particle systems, dynamics in
population genetics) or inﬁnite-dimensional manifolds (path or loop spaces
over Riemannian manifolds).
There are basically three approaches to analysing SPDEs: the “martingale
(or martingale measure) approach” (cf. [Wal86]), the “semigroup (or mild
solution) approach” (cf. [DPZ92], [DPZ96]) and the “variational approach”
(cf. [Roz90]). There is an enormously rich literature on all three approaches
which cannot be listed here. We refer instead to the above monographs.
The purpose of these notes is to give a concise introduction to the “vari-
ational approach”, as self-contained as possible. This approach was initiated
in pioneering work by Pardoux ([Par72],[Par75]) and further developed by
N. Krylov and B. Rozowskii in [KR79] (see also [Roz90]) for continuous mar-
tingales as integrators in the noise term and later by I. Gyongy and N. Krylov
in [GK81],[GK82],[Gy¨o82] for not necessarily continuous martingales.
These notes grew out of a two-semester graduate course given by the second-
named author at Purdue University in 2005/2006. The material has been
streamlined and could be covered in just one semester depending on the pre-
knowledge of the attending students. Prerequisites would be an advanced
course in probability theory, covering standard martingale theory, stochas-
tic processes in Rd and maybe basic stochastic integration, though the latter
is not formally required. Since graduate students in probability theory are
usually not familiar with the theory of Hilbert spaces or basic linear operator
theory, all required material from these areas is included in the notes, most
of it in the appendices. For the same reason we minimize the general theory
of martingales on Hilbert spaces, paying, however, the price that some proofs
1

2
1. Motivation, Aims and Examples
about stochastic integration on Hilbert space are a bit lengthy, since they have
to be done “by bare hands”.
In comparison with [Roz90] for simplicity we specialize to the case where
the integrator in the noise term is just a cylindrical Wiener process. But every-
thing is spelt out in a way so that it generalizes directly to continuous local
martingales. In particular, integrands are always assumed to be predictable
rather than just adapted and product measurable. The existence and unique-
ness proof (cf. Subsection 4.2) is our personal version of the one in [KR79],
[Roz90] and largely taken from [RRW06] presented there in a more general
framework. The results on invariant measures (cf. Subsection 4.3) we could
not ﬁnd in the literature for the “variational approach”. They are, however,
quite straightforward modiﬁcations of those in the “semigroup approach” in
[DPZ96]. The examples and applications in Subsection 4.1 in connection with
the stochastic porous media equation are fairly recent and are modiﬁcations
from results in [DPRLRW06] and [RRW06].
To keep these notes reasonably self-contained we also include a complete
proof of the ﬁnite-dimensional case in Chapter 3, which is based on the very
focussed and beautiful exposition in [Kry99], which uses the Euler approxi-
mation. Among other complementing topics the appendices contain a detailed
account of the Yamada–Watanabe theorem on the relation between weak and
strong solutions (cf. Appendix E).
The structure of these notes is, as we hope, obvious from the list of con-
tents. We only would like to mention here, that a substantial part consists of
a very detailed introduction to stochastic integration on Hilbert spaces (see
Chapter 2), major parts of which (as well as Appendices A–C) are taken from
the Diploma thesis of Claudia Pr´evˆot and Katja Frieler. We would like to
thank Katja Frieler at this point for her permission to do this. We also like to
thank all coauthors of those joint papers which form another component for
the basis of these notes. It was really a pleasure working with them in this
exciting area of probability. We would also like to thank Matthias Stephan
and Sven Wiesinger for the excellent typing job, as well as the participants
of the graduate course at Purdue University for spotting many misprints and
small mistakes.
Before starting with the main body of these notes we would like to give a few
examples of SPDE that appear in fundamental applications. We do this in a
very brief way, in particular, pointing out which of them can be analysed by
the tools developed in this course. We refer to the above-mentioned literature
for a more elaborate discussion of these and many more examples and their
role in the applied sciences.
Example 1.0.1 (Stochastic quantization of the free Euclidean quan-
tum ﬁeld).
dXt = (∆−m2)Xt dt + dWt
on E ⊂S′(Rd).

1. Motivation, Aims and Examples
3
• m ∈[0, ∞) denotes “mass”,
• (Wt)t⩾0 is a cylindrical Brownian motion on L2(Rd) ⊂E (the inclusion
is a Hilbert–Schmidt embedding).
Example 1.0.2 (Stochastic reaction diﬀusion equations).
dXt = [∆Xt −X3
t ] dt +

Q dWt
on E := Lp(Rd).
• Q is a trace class operator on L2(Rd), can also depend on Xt (then Q
becomes Q(Xt)),
• (Wt)t⩾0 is a cylindrical Brownian motion on L2(Rd).
Example 1.0.3 (Stochastic Burgers equation).
dXt = ∆Xt −Xt
d
dξ Xt +

Q dWt
on E := L2
[0, 1]

.
• ξ ∈[0, 1],
• Q as above,
• (Wt)t⩾0 is a cylindrical Brownian motion on L2
[0, 1]

.
Example 1.0.4 (Stochastic Navier–Stokes equation).
dXt =

ν∆sXt −⟨Xt, ∇⟩Xt

dt +

Q dWt
on E :=

x ∈L2(Λ →R2, dx)
 div x = 0

, Λ ⊂Rd, d = 2, 3, ∂Λ smooth.
• ν denotes viscosity,
• ∆s denotes the Stokes Laplacian,
• Q as above,
• (Wt)t⩾0 is a cylindrical Brownian motion on L2(Λ →Rd),
• div is taken in the sense of distributions.
Example 1.0.5 (Stochastic porous media equation).
dXt =

∆Ψ(Xt) + Φ(Xt)

dt + B(Xt) dWt
on H := dual of H1
0(Λ) (:= Sobolev space of order 1 in L2(Λ) with Dirichlet
boundary conditions).

4
1. Motivation, Aims and Examples
• Λ as above,
• Ψ, Φ : R →R “monotone”,
• B(x) : H →H Hilbert–Schmidt operator, ∀x ∈H.
The general form of these equations with state spaces consisting of functions
ξ 	→x(ξ), where ξ is a spatial variable, e.g. from a subset of Rd, looks as
follows:
dXt(ξ) = A
	
t, Xt(ξ), DξXt(ξ), D2
ξ

Xt(ξ)

dt
+ B
	
t, Xt(ξ), DξXt(ξ), D2
ξ

Xt(ξ)

dWt .
Here Dξ and D2
ξ mean ﬁrst and second total derivatives, respectively. The
stochastic term can be considered as a “perturbation by noise”. So, clearly one
motivation for studying SPDEs is to get information about the corresponding
(unperturbed) deterministic PDE by letting the noise go to zero (e.g. replace
B by ε · B and let ε →0) or to understand the diﬀerent features occurring if
one adds the noise term.
If we drop the stochastic term in these equations we get a deterministic
PDE of “evolutionary type”. Roughly speaking this means we have that the
time derivative of the desired solution (on the left) is equal to a non–linear
functional of its spatial derivatives (on the right).
Among others (see Subsection 4.1, in particular the cases, where ∆is
replaced by the p-Laplacian) the approach presented in these notes will cover
Examples 1.0.2 in case d = 3 or 4. (cf. Remark 4.1.10,2. and also [RRW06]
without restrictions on the dimension) and 1.0.5 (cf. Example 4.1.11). For
Example 1.0.1 we refer to [AR91] and for Examples 1.0.3 and 1.0.4 e.g. to
[DPZ92], [DPZ96].

2. The Stochastic Integral
in General Hilbert Spaces
(w.r.t. Brownian Motion)
This chapter is a slight modiﬁcation of Chap. 1 in [FK01].
We ﬁx two separable Hilbert spaces

U, ⟨, ⟩U

and

H, ⟨, ⟩

. The ﬁrst part
of this chapter is devoted to the construction of the stochastic Itˆo integral
 t
0
Φ(s) dW(s),
t ∈[0, T],
where W(t), t ∈[0, T], is a Wiener process on U and Φ is a process with
values that are linear but not necessarily bounded operators from U to H.
For that we ﬁrst will have to introduce the notion of the standard Wiener
process in inﬁnite dimensions. Then there will be a short section about mar-
tingales in general Hilbert spaces. These two concepts are important for the
construction of the stochastic integral which will be explained in the following
section.
In the second part of this chapter we will present the Itˆo formula and
the stochastic Fubini theorem and establish basic properties of the stochastic
integral, including the Burkholder–Davis–Gundy inequality.
Finally, we will describe how to transmit the deﬁnition of the stochastic
integral to the case that W(t), t ∈[0, T], is a cylindrical Wiener process. For
simplicity we assume that U and H are real Hilbert spaces.
2.1. Inﬁnite-dimensional Wiener processes
For a topological space X we denote its Borel σ-algebra by B(X).
Deﬁnition 2.1.1. A probability measure µ on

U, B(U)

is called Gaussian
if for all v ∈U the bounded linear mapping
v′ :U →R
deﬁned by
u 	→⟨u, v⟩U,
u ∈U,
5

6
2. Stochastic Integral in Hilbert Spaces
has a Gaussian law, i.e. for all v ∈U there exist m := m(v) ∈R and σ :=
σ(v) ∈[0, ∞[ such that, if σ(v) > 0,

µ ◦(v′)−1
(A) = µ(v′ ∈A) =
1
√
2πσ2

A
e−(x−m)2
2σ2
dx
for all A ∈B(R),
and, if σ(v) = 0,
µ ◦(v′)−1 = δm(v).
Theorem 2.1.2. A measure µ on

U, B(U)

is Gaussian if and only if
ˆµ(u) :=

U
ei⟨u,v⟩U µ(dv) = ei⟨m,u⟩U −1
2 ⟨Qu,u⟩U ,
u ∈U,
where m ∈U and Q ∈L(U) is nonnegative, symmetric, with ﬁnite trace (see
Deﬁnition B.0.3; here L(U) denotes the set of all bounded linear operators
on U).
In this case µ will be denoted by N(m, Q) where m is called mean and Q
is called covariance (operator). The measure µ is uniquely determined by m
and Q.
Furthermore, for all h, g ∈U

⟨x, h⟩U µ(dx) = ⟨m, h⟩U,
 
⟨x, h⟩U −⟨m, h⟩U

⟨x, g⟩U −⟨m, g⟩U

µ(dx) = ⟨Qh, g⟩U,

∥x −m∥2
U µ(dx) = tr Q.
Proof. (cf. [DPZ92]) Obviously, a probability measure with this Fourier trans-
form is Gaussian. Now let us conversely assume that µ is Gaussian. We need
the following:
Lemma 2.1.3. Let ν be a probability measure on (U, B(U)). Let k ∈N be
such that

U
⟨z, x⟩U
k ν(dx) < ∞
∀z ∈U.
Then there exists a constant C = C(k, ν) > 0 such that for all h1, . . . , hk ∈U

U
⟨h1, x⟩U · · · ⟨hk, x⟩U
 ν(dx) ⩽C ∥h1∥U · · · ∥hk∥U.
In particular, the symmetric k-linear form
U k ∋(h1, . . . , hk) 	→

⟨h1, x⟩U · · · ⟨hk, x⟩U ν(dx) ∈R
is continuous.

2.1. Inﬁnite-dimensional Wiener processes
7
Proof. For n ∈N deﬁne
Un :=

z ∈U


U
⟨z, x⟩U
k ν(dx) ⩽n

.
By assumption
U =
∞

n=1
Un.
Since U is a complete metric space, by the Baire category theorem, there
exists n0 ∈N such that Un0 has non-empty interior, so there exists a ball
(with centre z0 and radius r0) B(z0, r0) ⊂Un0. Hence

U
⟨z0 + y, x⟩U
k ν(dx) ⩽n0
∀y ∈B(0, r0),
therefore for all y ∈B(0, r0)

U
⟨y, x⟩U
k ν(dx) =

U
⟨z0 + y, x⟩U −⟨z0, x⟩U
k ν(dx)
⩽2k−1

U
⟨z0 + y, x⟩U
k ν(dx) + 2k−1

U
⟨z0, x⟩U
k ν(dx)
⩽2kn0.
Applying this for y := r0z, z ∈U with |z|U = 1, we obtain

U
⟨z, x⟩U
k ν(dx) ⩽2kn0r−k
0 .
Hence, if h1, . . . , hk ∈U \ {0}, then by the generalized H¨older inequality

U

 h1
|h1|U
, x

U
· · ·
 hk
|hk|U
, x

U
 ν(dx)
⩽

U

 h1
|h1|U
, x

U

k
ν(dx)
1/k
. . .

U

 hk
|hk|U
, x

U

k
ν(dx)
1/k
⩽2kn0r−k
0 ,
and the assertion follows.
Applying Lemma 2.1.3 for k = 1 and ν := µ we obtain that
U ∋h 	→

⟨h, x⟩U µ(dx) ∈R
is a continuous linear map, hence there exists m ∈U such that

U
⟨x, h⟩U µ(dx) = ⟨m, h⟩
∀h ∈H.

8
2. Stochastic Integral in Hilbert Spaces
Applying Lemma 2.1.3 for k = 2 and ν := µ we obtain that
U 2 ∋(h1, h2) 	→

⟨x, h1⟩U⟨x, h2⟩U µ(dx) −⟨m, h1⟩U⟨m, h2⟩U
is a continuous symmetric bilinear map, hence there exists a symmetric Q ∈
L(U) such that this map is equal to
U 2 ∋(h1, h2) 	→⟨Qh1, h2⟩U.
Since for all h ∈U
⟨Qh, h⟩U =

⟨x, h⟩2
U µ(dx) −

⟨x, h⟩U µ(dx)
2
⩾0,
Q is positive deﬁnite. It remains to prove that Q is trace class (i.e.
tr Q :=
∞

i=1
⟨Qei, ei⟩U < ∞
for one (hence every) orthonormal basis {ei | i ∈N} of U, cf. Appendix B).
We may assume without loss of generality that µ has mean zero, i.e. m = 0
(∈U), since the image measure of µ under the translation U ∋x 	→x −m is
again Gaussian with mean zero and the same covariance Q. Then we have for
all h ∈U and all c ∈(0, ∞)
1 −e−1
2 ⟨Qh,h⟩U =

U

1 −cos⟨h, x⟩U

µ(dx)
⩽

{|x|U⩽c}

1 −cos⟨h, x⟩U

µ(dx) + 2µ

x ∈U
 |x|U > c

⩽1
2

{|x|U⩽c}
⟨h, x⟩U
2 µ(dx) + 2µ

x ∈U
 |x|U > c

(2.1.1)
(since 1 −cos x ⩽1
2x2). Deﬁning the positive deﬁnite symmetric linear oper-
ator Qc on U by
⟨Qch1, h2⟩U :=

{|x|U⩽c}
⟨h1, x⟩U · ⟨h2, x⟩U µ(dx),
h1, h2 ∈U,
we even have that Qc is trace class because for every orthonormal basis {ek |
k ∈N} of U we have (by monotone convergence)
∞

k=1
⟨Qcek, ek⟩U =

{|x|U⩽c}
∞

k=1
⟨ek, x⟩2
U µ(dx) =

{|x|U⩽c}
|x|2
U µ(dx)
⩽c2 < ∞.

2.1. Inﬁnite-dimensional Wiener processes
9
Claim: There exists c0 ∈(0, ∞) (large enough) so that Q ⩽2 log 4 Qc0 (mean-
ing that ⟨Qh, h⟩U ⩽2 log 4⟨Qc0h, h⟩U for all h ∈U).
To prove the claim let c0 be so big that µ

x ∈U
 |x|U > c0

⩽1
8. Let
h ∈U such that ⟨Qc0h, h⟩U ⩽1. Then (2.1.1) implies
1 −e−1
2 ⟨Qh,h⟩U ⩽1
2 + 1
4 = 3
4,
hence 4 ⩾e
1
2 ⟨Qh,h⟩U , so ⟨Qh, h⟩U ⩽2 log 4. If h ∈U is arbitrary, but
⟨Qc0h, h⟩U ̸= 0, then we apply what we have just proved to h/⟨Qc0h, h⟩
1
2
U and
the claim follows for such h. If, however, ⟨Qc0h, h⟩= 0, then for all n ∈N,
⟨Qc0nh, nh⟩U = 0 ⩽1, hence by the above ⟨Qh, h⟩U ⩽n−22 log 4. Therefore,
⟨Qc0h, h⟩U = 0 and the claim is proved, also for such h.
Since Qc0 has ﬁnite trace, so has Q by the claim and the theorem is proved,
since the uniqueness part follows from the fact that the Fourier transform is
one-to-one.
The following result is then obvious.
Proposition 2.1.4. Let X be a U-valued Gaussian random variable on a
probability space (Ω, F, P), i.e. there exist m ∈U and Q ∈L(U) nonnegative,
symmetric, with ﬁnite trace such that P ◦X−1 = N(m, Q).
Then ⟨X, u⟩U is normally distributed for all u ∈U and the following state-
ments hold:
• E

⟨X, u⟩U

= ⟨m, u⟩U for all u ∈U,
• E

⟨X −m, u⟩U · ⟨X −m, v⟩U

= ⟨Qu, v⟩U for all u, v ∈U,
• E

∥X −m∥2
U

= tr Q.
The following proposition will lead to a representation of a U-valued
Gaussian random variable in terms of real-valued Gaussian random variables.
Proposition 2.1.5. If Q ∈L(U) is nonnegative, symmetric, with ﬁnite trace
then there exists an orthonormal basis ek, k ∈N, of U such that
Qek = λkek,
λk ⩾0, k ∈N,
and 0 is the only accumulation point of the sequence (λk)k∈N.
Proof. See [RS72, Theorem VI.21; Theorem VI.16 (Hilbert–Schmidt theorem)].
Proposition 2.1.6 (Representation of a Gaussian random variable).
Let m ∈U and Q ∈L(U) be nonnegative, symmetric, with tr Q < ∞. In
addition, we assume that ek, k ∈N, is an orthonormal basis of U consist-
ing of eigenvectors of Q with corresponding eigenvalues λk, k ∈N, as in
Proposition 2.1.5, numbered in decreasing order.

10
2. Stochastic Integral in Hilbert Spaces
Then a U-valued random variable X on a probability space (Ω, F, P) is
Gaussian with P ◦X−1 = N(m, Q) if and only if
X =

k∈N

λkβkek + m
(as objects in L2(Ω, F, P; U)),
where βk, k ∈N, are independent real-valued random variables with P ◦βk
−1 =
N(0, 1) for all k ∈N with λk > 0. The series converges in L2(Ω, F, P; U).
Proof.
1. Let X be a Gaussian random variable with mean m and covariance Q.
Below we set ⟨, ⟩:= ⟨, ⟩U.
Then X = 
k∈N⟨X, ek⟩ek in U where ⟨X, ek⟩is normally distributed with
mean ⟨m, ek⟩and variance λk, k ∈N, by Proposition 2.1.4. If we now deﬁne
βk :

= ⟨X,ek⟩−⟨m,ek⟩
√λk
if k ∈N with λk > 0
≡0 ∈R
else,
then we get that P ◦β−1
k
= N(0, 1) and X = 
k∈N
√λkβkek +m. To prove
the independence of βk, k ∈N, we take an arbitrary n ∈N and ak ∈R,
1 ⩽k ⩽n, and obtain that for c := −n
k=1, λk̸=0
ak
√λk ⟨m, ek⟩
n

k=1
akβk =
n

k=1,
λk̸=0
ak
√λk
⟨X, ek⟩+ c =

X,
n

k=1,
λk̸=0
ak
√λk
ek

+ c
which is normally distributed since X is a Gaussian random variable. There-
fore we have that βk, k ∈N, form a Gaussian family. Hence, to get the
independence, we only have to check that the covariance of βi and βj,
i, j ∈N, i ̸= j, with λi ̸= 0 ̸= λj, is equal to zero. But this is clear since
E(βiβj) =
1

λiλj
E

⟨X −m, ei⟩⟨X −m, ej⟩

=
1

λiλj
⟨Qei, ej⟩
=
λi

λiλj
⟨ei, ej⟩= 0
for i ̸= j.
Besides, the series n
k=1
√λkβkek, n ∈N, converges in L2(Ω, F, P; U) since
the space is complete and
E

n

k=m

λkβkek

2
=
n

k=m
λkE

|βk|2
=
n

k=m
λk.
Since 
k∈N λk = tr Q < ∞this expression becomes arbitrarily small for
m and n large enough.

2.1. Inﬁnite-dimensional Wiener processes
11
2. Let ek, k ∈N, be an orthonormal basis of U such that Qek = λkek,
k ∈N, and let βk, k ∈N, be a family of independent real-valued Gaussian
random variables with mean 0 and variance 1. Then it is clear that the
series n
k=1
√λkβkek + m, n ∈N, converges to X := 
k∈N
√λkβkek + m
in L2(Ω, F, P; U) (see part 1). Now we ﬁx u ∈U and get that
 n

k=1

λkβkek + m, u

=
n

k=1

λkβk⟨ek, u⟩+ ⟨m, u⟩
is normally distributed for all n ∈N and the sequence converges in
L2(Ω, F, P). This implies that the limit ⟨X, u⟩is also normally distributed
where
E

⟨X, u⟩

= E
	
k∈N

λkβk⟨ek, u⟩+ ⟨m, u⟩

= lim
n→∞E
	 n

k=1

λkβk⟨ek, u⟩

+ ⟨m, u⟩= ⟨m, u⟩
and concerning the covariance we obtain that
E
	
⟨X, u⟩−⟨m, u⟩

⟨X, v⟩−⟨m, v⟩

= lim
n→∞E
	 n

k=1

λkβk⟨ek, u⟩
n

k=1

λkβk⟨ek, v⟩

=

k∈N
λk⟨ek, u⟩⟨ek, v⟩=

k∈N
⟨Qek, u⟩⟨ek, v⟩
=

k∈N
⟨ek, Qu⟩⟨ek, v⟩= ⟨Qu, v⟩
for all u, v ∈U.
By part 2 of this proof we ﬁnally get the following existence result.
Corollary 2.1.7. Let Q be a nonnegative and symmetric operator in L(U)
with ﬁnite trace and let m ∈U. Then there exists a Gaussian measure µ =
N(m, Q) on

U, B(U)

.
Let us give an alternative, more direct proof of Corollary 2.1.7 without using
Proposition 2.1.6. For the proof we need the following exercise.
Exercise 2.1.8. Consider R∞with the product topology. Let B(R∞) denote
its Borel σ-algebra. Prove:
(i) B(R∞) = σ(πk | k ∈N), where πk : R∞→R denotes the projection on
the k-th coordinate.

12
2. Stochastic Integral in Hilbert Spaces
(ii) l2(R)
	
:=

(xk)k∈N ∈R∞
∞

k=1
x2
k < ∞

∈B(R∞).
(iii) B(R∞) ∩l2(R) = σ

πk |l2
 k ∈N

.
(iv) Let l2(R) be equipped with its natural norm
∥x∥l2 :=
	 ∞

k=1
x2
k

 1
2 ,
x = (xk)k∈N ∈l2(R),
and let B

l2(R)

be the corresponding Borel σ-algebra. Then:
B

l2(R)

= B(R∞) ∩l2(R).
Alternative Proof of Corollary 2.1.7. It suﬃces to construct N(0, Q), since
N(m, Q) is the image measure of N(0, Q) under translation with m. For
k ∈N consider the normal distribution N(0, λk) on R and let ν be their
product measure on

R∞, B(R∞)

, i.e.
ν =

k∈N
N(0, λk)
on

R∞, B(R∞)

.
Here λk, k ∈N, are as in Proposition 2.1.5. Since the map g : R∞→[0, ∞]
deﬁned by
g(x) :=
∞

k=1
x2
k ,
x = (xk)k∈N ∈R∞,
is B(R∞)-measurable, we may calculate

R∞g(x) ν(dx) =
∞

k=1

x2
k N(0, λk)(dxk) =
∞

k=1
λk < ∞.
Therefore, using Exercise 2.1.8(ii), we obtain ν

l2(R)

= 1. Restricting ν to
B(R∞) ∩l2(R), by Exercise 2.1.8(iv) we get a probability measure, let us
call it ˜µ, on

l2(R), B

l2(R)

. Now take the orthonormal basis {ek | k ∈N}
from Proposition 2.1.5 and consider the corresponding canonical isomorphism
I : l2(R) →U deﬁned by
I(x) =
∞

k=1
xkek,
x = (xk)k∈N ∈l2(R).
It is then easy to check that the image measure
µ := ˜µ ◦I−1
on

U, B(U)

is the desired measure, i.e. µ = N(0, Q).

2.1. Inﬁnite-dimensional Wiener processes
13
After these preparations we will give the deﬁnition of the standard Q-Wiener
process. To this end we ﬁx an element Q ∈L(U), nonnegative, symmetric and
with ﬁnite trace and a positive real number T.
Deﬁnition 2.1.9. A U-valued stochastic process W(t), t ∈[0, T], on a prob-
ability space (Ω, F, P) is called a (standard) Q-Wiener process if:
• W(0) = 0,
• W has P-a.s. continuous trajectories,
• the increments of W are independent, i.e. the random variables
W(t1), W(t2) −W(t1), . . . , W(tn) −W(tn−1)
are independent for all 0 ⩽t1 < · · · < tn ⩽T, n ∈N,
• the increments have the following Gaussian laws:
P ◦

W(t) −W(s)
−1 = N

0, (t −s)Q

for all 0 ⩽s ⩽t ⩽T.
Similarly to the existence of Gaussian measures the existence of a Q-Wiener
process in U can be reduced to the real-valued case. This is the content of the
following proposition.
Proposition 2.1.10 (Representation of the Q-Wiener process). Let ek,
k ∈N, be an orthonormal basis of U consisting of eigenvectors of Q with cor-
responding eigenvalues λk, k ∈N. Then a U-valued stochastic process W(t),
t ∈[0, T], is a Q-Wiener process if and only if
W(t) =

k∈N

λkβk(t)ek,
t ∈[0, T],
(2.1.2)
where βk, k ∈{n ∈N | λn > 0}, are independent real-valued Brownian
motions on a probability space (Ω, F, P). The series even converges in
L2
Ω, F, P; C([0, T], U)

, and thus always has a P-a.s. continuous modiﬁca-
tion. (Here the space C

[0, T], U

is equipped with the sup norm.) In particu-
lar, for any Q as above there exists a Q-Wiener process on U.
Proof.
1. Let W(t), t ∈[0, T], be a Q-Wiener process in U.
Since P ◦W(t)−1 = N(0, tQ), we see as in the proof of Proposition 2.1.6
that
W(t) =

k∈N

λkβk(t)ek,
t ∈[0, T],
with
βk(t) :

= ⟨W (t),ek⟩
√λk
if k ∈N with λk > 0
≡0
else,

14
2. Stochastic Integral in Hilbert Spaces
for all t ∈[0, T]. Furthermore, P ◦β−1
k (t) = N(0, t), k ∈N, and βk(t),
k ∈N, are independent for each t ∈[0, T].
Now we ﬁx k ∈N. First we show that βk(t), t ∈[0, T], is a Brownian
motion:
If we take an arbitrary partition 0 = t0 ⩽t1 < · · · < tn ⩽T, n ∈N, of
[0, T] we get that
βk(t1), βk(t2) −βk(t1), . . . , βk(tn) −βk(tn−1)
are independent for each k ∈N since for 1 ⩽j ⩽n
βk(tj) −βk(tj−1) =

1
√λk

W(tj) −W(tj−1), ek

if λk > 0,
0
else.
Moreover, we obtain that for the same reason P ◦

βk(t) −βk(s)
−1 =
N(0, t −s) for 0 ⩽s ⩽t ⩽T.
In addition,
t 	→
1
√λk

W(t), ek

= βk(t)
is P-a.s. continuous for all k ∈N.
Secondly, it remains to prove that βk, k ∈N, are independent.
We take k1, . . . , kn ∈N, n ∈N, ki ̸= kj if i ̸= j and an arbitrary partition
0 = t0 ⩽t1 ⩽. . . ⩽tm ⩽T, m ∈N.
Then we have to show that
σ

βk1(t1), . . . , βk1(tm)

, . . . , σ

βkn(t1), . . . , βkn(tm)

are independent.
We will prove this by induction with respect to m:
If m = 1 it is clear that βk1(t1), . . . , βkn(t1) are independent as observed
above. Thus, we now take a partition 0 = t0 ⩽t1 ⩽. . . ⩽tm+1 ⩽T and
assume that
σ

βk1(t1), . . . , βk1(tm)

, . . . , σ

βkn(t1), . . . , βkn(tm)

are independent. We note that
σ

βki(t1), . . . , βki(tm), βki(tm+1)

= σ

βki(t1), . . . , βki(tm), βki(tm+1) −βki(tm)

,
1 ⩽i ⩽n,
and that
βki(tm+1) −βki(tm) =

1
√
λki

W(tm+1) −W(tm), eki

U
if λk > 0,
0
else,

2.1. Inﬁnite-dimensional Wiener processes
15
1 ⩽i ⩽n, are independent since they are pairwise orthogonal in
L2(Ω, F, P; R) and since W(tm+1) −W(tm) is a Gaussian random vari-
able. If we take Ai,j ∈B(R), 1 ⩽i ⩽n, 1 ⩽j ⩽m + 1, then because of the
independence of σ

W(s)
 s ⩽tm

and σ

W(tm+1) −W(tm)

we get that
P
	 n
 
i=1

βki(t1) ∈Ai,1, . . . , βki(tm) ∈Ai,m,
βki(tm+1) −βki(tm) ∈Ai,m+1

=P
	 n
 
i=1
m
 
j=1

βki(tj) ∈Ai,j

!
"#
$
∈σ

W(s)
 s ⩽tm

∩
n
 
i=1

βki(tm+1) −βki(tm) ∈Ai,m+1

!
"#
$
∈σ

W(tm+1) −W(tm)


=P
	 n
 
i=1
m
 
j=1

βki(tj) ∈Ai,j

· P
	 n
 
i=1

βki(tm+1) −βki(tm) ∈Ai,m+1

=
 n

i=1
P
	 m
 
j=1

βki(tj) ∈Ai,j


·
	 n

i=1
P

βki(tm+1) −βki(tm) ∈Ai,m+1

=
n

i=1
P
	 m
 
j=1

βki(tj) ∈Ai,j

∩

βki(tm+1) −βki(tm) ∈Ai,m+1

and therefore the assertion follows.
2. If we deﬁne
W(t) :=

k∈N

λkβk(t)ek,
t ∈[0, T],
where βk, k ∈N, are independent real-valued continuous Brownian motions
then it is clear that W(t), t ∈[0, T], is well-deﬁned in L2(Ω, F, P; U). Be-
sides, it is obvious that the process W(t), t ∈[0, T], starts at zero and
that
P ◦

W(t) −W(s)
−1 = N

0, (t −s)Q

,
0 ⩽s < t ⩽T,
by Proposition 2.1.6. It is also clear that the increments are independent.
Thus
it
remains
to
show
that
the
above
series
converges
in
L2
Ω, F, P; C([0, T], U)

. To this end we set
W N(t, ω) :=
N

k=1

λkβk(t, ω)ek

16
2. Stochastic Integral in Hilbert Spaces
for all (t, ω) ∈ΩT := [0, T] × Ωand N ∈N. Then W N, N ∈N, is P-a.s.
continuous and we have that for M < N
E
	
sup
t∈[0,T ]
W N(t) −W M(t)
2
U

= E
	
sup
t∈[0,T ]
N

k=M+1
λkβ2
k(t)

⩽
N

k=M+1
λkE

sup
t∈[0,T ]
β2
k(t)

⩽c
N

k=M+1
λk
where ci = E

supt∈[0,T ] β2
1(t)

< ∞because of Doob’s maximal inequal-
ity for real-valued submartingales. As

k∈N
λk = tr Q < ∞, the assertion
follows.
Deﬁnition 2.1.11 (Normal ﬁltration). A ﬁltration Ft, t ∈[0, T], on a
probability space (Ω, F, P) is called normal if:
• F0 contains all elements A ∈F with P(A) = 0 and
• Ft = Ft+ =
 
s>t
Fs for all t ∈[0, T] .
Deﬁnition 2.1.12 (Q-Wiener process with respect to a ﬁltration).
A Q-Wiener process W(t), t ∈[0, T], is called a Q-Wiener process with respect
to a ﬁltration Ft, t ∈[0, T], if:
• W(t), t ∈[0, T], is adapted to Ft, t ∈[0, T], and
• W(t) −W(s) is independent of Fs for all 0 ⩽s ⩽t ⩽T.
In fact it is possible to show that any U-valued Q-Wiener process W(t),
t ∈[0, T], is a Q-Wiener process with respect to a normal ﬁltration:
We deﬁne
N :=

A ∈F
 P(A) = 0

,
˜Ft := σ

W(s)
 s ⩽t

and
˜
F0
t := σ( ˜Ft ∪N).
Then it is clear that
Ft :=
 
s>t
˜
F0s ,
t ∈[0, T],
(2.1.3)
is a normal ﬁltration and we get:
Proposition 2.1.13. Let W(t), t ∈[0, T], be an arbitrary U-valued Q-Wiener
process on a probability space (Ω, F, P). Then it is a Q-Wiener process with
respect to the normal ﬁltration Ft, t ∈[0, T], given by (2.1.3).

2.2. Martingales in general Banach spaces
17
Proof. It is clear that W(t), t ∈[0, T], is adapted to Ft, t ∈[0, T]. Hence we
only have to verify that W(t)−W(s) is independent of Fs, 0 ⩽s < t ⩽T. But
if we ﬁx 0 ⩽s < t ⩽T it is clear that W(t)−W(s) is independent of ˜
Fs since
σ

W(t1), W(t2), . . . , W(tn)

= σ

W(t1), W(t2) −W(t1), . . . , W(tn) −W(tn−1)

for all 0 ⩽t1 < t2 < · · · < tn ⩽s. Of course, W(t) −W(s) is then also
independent of ˜
F0s . To prove now that W(t) −W(s) is independent of Fs it
is enough to show that
P
	
W(t) −W(s) ∈A

∩B

= P

W(t) −W(s) ∈A

· P(B)
for any B ∈Fs and any closed subset A ⊂U as E := {A ⊂U | A closed}
generates B(U) and is stable under ﬁnite intersections. But we have
P
	
W(t) −W(s) ∈A

∩B

= E
	
1A ◦

W(t) −W(s)

· 1B

= lim
n→∞E
%	
1 −n dist

W(t) −W(s), A

∨0
&
1B

= lim
n→∞lim
m→∞E
%	
1 −n dist

W(t) −W(s + 1
m), A

∨0
&
1B

= lim
n→∞lim
m→∞E
	
1 −n dist

W(t) −W(s + 1
m), A

∨0

· P(B)
= P

W(t) −W(s) ∈A

· P(B),
since W(t)−W(s+ 1
m) is independent of ˜F0
s+ 1
m ⊃Fs if m is large enough.
2.2. Martingales in general Banach spaces
Analogously to the real-valued case it is possible to deﬁne the conditional
expectation of any Bochner integrable random variable with values in an
arbitrary separable Banach space

E, ∥∥

. This result is formulated in the
following proposition.
Proposition 2.2.1 (Existence of the conditional expectation). Assume
that E is a separable real Banach space. Let X be a Bochner integrable E-
valued random variable deﬁned on a probability space (Ω, F, P) and let G be a
σ-ﬁeld contained in F.

18
2. Stochastic Integral in Hilbert Spaces
Then there exists a unique, up to a set of P-probability zero, Bochner inte-
grable E-valued random variable Z, measurable with respect to G such that

A
X dP =

A
Z dP
for all A ∈G.
(2.2.1)
The random variable Z is denoted by E(X | G) and is called the conditional
expectation of X given G. Furthermore,
E(X | G)
 ⩽E

∥X∥
 G

.
Proof. (cf. [DPZ92, Proposition 1.10, p. 27]) Let us ﬁrst show uniqueness.
Since E is a separable Banach space, there exist ln ∈E∗, n ∈N, separating
the points of E. Suppose that Z1, Z2 are Bochner integrable, G-measurable
mappings from Ωto E such that

A
X dP =

A
Z1 dP =

A
Z2 dP
for all A ∈G.
Then for n ∈N by Proposition A.2.2

A

ln(Z1) −ln(Z2)

dP = 0
for all A ∈G.
Applying this with A :=

ln(Z1) > ln(Z2)

and A :=

ln(Z1) < ln(Z2)

it
follows that ln(Z1) = ln(Z2) P-a.s., so
Ω0 :=
 
n∈N

ln(Z1) = ln(Z2)

has P-measure one. Since ln, n ∈N, separate the points of E; it follows that
Z1 = Z2 on Ω0.
To show existence we ﬁrst assume that X is a simple function. So, there
exist x1, . . . , xN ∈E and pairwise disjoint sets A1, . . . , AN ∈F such that
X =
N

k=1
xk1Ak.
Deﬁne
Z :=
N

k=1
xkE(1Ak | G).
Then obviously Z is G-measurable and satisﬁes (2.2.1). Furthermore,
∥Z∥⩽
N

k=1
∥xk∥E(1Ak | G) = E
	 N

k=1
∥xk∥1Ak
 G

= E

∥X∥
 G

.
(2.2.2)

2.2. Martingales in general Banach spaces
19
Taking expectation we get
E

∥Z∥

⩽E

∥X∥

.
(2.2.3)
For general X take simple functions Xn, n ∈N, as in Lemma A.1.4 and
deﬁne Zn as above with Xn replacing X. Then by (2.2.3) for all n, m ∈N
E

∥Zn −Zm∥

⩽E

∥Xn −Xm∥

,
so Z := limn→∞Zn exists in L1(Ω, F, P; E). Therefore, for all A ∈G

A
X dP = lim
n→∞

A
Xn dP = lim
n→∞

A
Zn dP =

A
Z dP.
Clearly, Z can be chosen G-measurable, since so are the Zn. Furthermore, by
(2.2.2)
E(X | G)
 = ∥Z∥= lim
n→∞∥Zn∥⩽lim
n→∞E

∥Xn∥
 G

= E

∥X∥
 G

,
where the limits are taken in L1(P).
Later we will need the following result:
Proposition 2.2.2. Let (E1, E1) and (E2, E2) be two measurable spaces and
Ψ : E1×E2 →R a bounded measurable function. Let X1 and X2 be two random
variables on (Ω, F, P) with values in (E1, E1) and (E2, E2) respectively, and
let G ⊂F be a ﬁxed σ-ﬁeld.
Assume that X1 is G-measurable and X2 is independent of G, then
E

Ψ(X1, X2)
 G

= ˆΨ(X1)
where
ˆΨ(x1) = E

Ψ(x1, X2)

,
x1 ∈E1.
Proof. A simple exercise or see [DPZ92, Proposition 1.12, p. 29].
Remark 2.2.3. The previous proposition can be easily extended to the case
where the function Ψ is not necessarily bounded but nonnegative.
Deﬁnition 2.2.4. Let M(t), t ⩾0, be a stochastic process on (Ω, F, P) with
values in a separable Banach space E, and let Ft, t ⩾0, be a ﬁltration on
(Ω, F, P).
The process M is called an Ft-martingale, if:
• E

∥M(t)∥

< ∞for all t ⩾0,
• M(t) is Ft-measurable for all t ⩾0,
• E

M(t)
 Fs

= M(s) P-a.s. for all 0 ⩽s ⩽t < ∞.

20
2. Stochastic Integral in Hilbert Spaces
Remark 2.2.5. Let M be as above such that E(∥M(t)∥) < ∞for all t ∈
[0, T]. Then M is an Ft-martingale if and only if l(M) is an Ft-martingale
for all l ∈E∗. In particular, results like optional stopping etc. extend to
E-valued martingales.
There is the following connection to real-valued submartingales.
Proposition 2.2.6. If M(t), t ⩾0, is an E-valued Ft-martingale and p ∈
[1, ∞), then
M(t)
p, t ⩾0, is a real-valued Ft-submartingale.
Proof. Since E is separable there exist lk ∈E∗, k ∈N, such that ∥z∥=
sup lk(z) for all z ∈E. Then for s < t
E

∥Mt∥
 Fs

⩾sup
k
E

lk(Mt)
 Fs

= sup
k
lk

E(Mt | Fs)

= sup
k
lk(Ms) = ∥Ms∥.
This proves the assertion for p = 1. Then Jensen’s inequality implies the
assertion for all p ∈[1, ∞).
Theorem 2.2.7 (Maximal inequality). Let p > 1 and let E be a separable
Banach space.
If M(t), t ∈[0, T], is a right-continuous E-valued Ft-martingale, then

E
	
sup
t∈[0,T ]
M(t)
p
 1
p
⩽
p
p −1 sup
t∈[0,T ]
	
E

∥M(t)∥p
 1
p
=
p
p −1
	
E

∥M(T)∥p
 1
p .
Proof. The inequality is a consequence of the previous proposition and Doob’s
maximal inequality for real-valued submartingales.
Remark 2.2.8. We note that in the inequality in Theorem 2.2.7 the ﬁrst
norm is the standard norm on Lp
Ω, F, P; C([0, T]; E)

, whereas the second
is the standard norm on C

[0, T]; Lp(Ω, F, P; E)

. So, for right-continuous
E-valued Ft-martingales these two norms are equivalent.
Now we ﬁx 0 < T < ∞and denote by M2
T (E) the space of all E-valued
continuous, square integrable martingales M(t), t ∈[0, T]. This space will play
an important role with regard to the deﬁnition of the stochastic integral. We
will use especially the following fact.

2.3. The deﬁnition of the stochastic integral
21
Proposition 2.2.9. The space M2
T (E) equipped with the norm
∥M∥M2
T := sup
t∈[0,T ]
	
E

∥M(t)∥2
 1
2 =
	
E

∥M(T)∥2
 1
2
⩽
	
E

sup
t∈[0,T ]
∥M(t)∥2
 1
2 ⩽2 · E

∥M(T)∥2 1
2 .
is a Banach space.
Proof. By the Riesz–Fischer theorem the space L2
Ω, F, P; C

[0, T], E

is
complete. So, we only have to show that M2
T is closed. But this is obvious
since even L1(Ω, F, P; E)-limits of martingales are martingales.
Proposition 2.2.10. Let T > 0 and W(t), t ∈[0, T], be a U-valued Q-Wiener
process with respect to a normal ﬁltration Ft, t ∈[0, T], on a probability
space (Ω, F, P). Then W(t), t ∈[0, T], is a continuous square integrable Ft-
martingale, i.e. W ∈M2
T (U).
Proof. The continuity is clear by deﬁnition and for each t ∈[0, T] we have that
E

∥W(t)∥2
U

= t tr Q < ∞(see Proposition 2.1.4). Hence let 0 ⩽s ⩽t ⩽T
and A ∈Fs. Then we get by Proposition A.2.2 that

A
W(t) −W(s) dP, u

U
=

A

W(t) −W(s), u

U dP
= P(A)
 
W(t) −W(s), u

U dP = 0
for
all
u
∈
U
as
Fs
is
independent
of
W(t) −W(s)
and
E

⟨W(t) −W(s), u⟩U

= 0 for all u ∈U. Therefore,

A
W(t) dP =

A
W(s) +

W(t) −W(s)

dP
=

A
W(s) dP +

A
W(t) −W(s) dP
=

A
W(s) dP,
for all A ∈Fs.
2.3. The deﬁnition of the stochastic integral
For the whole section we ﬁx a positive real number T and a probability space
(Ω, F, P) and we deﬁne ΩT := [0, T] × Ωand PT := dx ⊗P where dx is the
Lebesgue measure.
Moreover, let Q ∈L(U) be symmetric, nonnegative and with ﬁnite trace
and we consider a Q-Wiener process W(t), t ∈[0, T], with respect to a normal
ﬁltration Ft, t ∈[0, T].

22
2. Stochastic Integral in Hilbert Spaces
2.3.1. Scheme of the construction of the stochastic
integral
Step 1:
First we consider a certain class E of elementary L(U, H)-valued
processes and deﬁne the mapping
Int :
E
→
M2
T (H) =: M2
T
Φ
	→
' t
0 Φ(s) dW(s),
t ∈[0, T].
Step 2:
We prove that there is a certain norm on E such that
Int : E →M2
T
is an isometry. Since M2
T is a Banach space this implies that Int can be
extended to the abstract completion ¯E of E. This extension remains isometric
and it is unique.
Step 3:
We give an explicit representation of ¯E.
Step 4:
We show how the deﬁnition of the stochastic integral can be ext-
ended by localization.
2.3.2. The construction of the stochastic integral
in detail
Step 1:
First we deﬁne the class E of all elementary processes as follows.
Deﬁnition 2.3.1 (Elementary process). An L = L(U, H)-valued process
Φ(t), t ∈[0, T], on (Ω, F, P) with normal ﬁltration Ft, t ∈[0, T], is said to be
elementary if there exist 0 = t0 < · · · < tk = T, k ∈N, such that
Φ(t) =
k−1

m=0
Φm1]tm,tm+1](t),
t ∈[0, T],
where:
• Φm : Ω→L(U, H) is Ftm-measurable, w.r.t. strong Borel σ-algebra on
L(U, H), 0 ⩽m ⩽k −1,
• Φm takes only a ﬁnite number of values in L(U, H), 1 ⩽m ⩽k −1.
If we deﬁne now
Int(Φ)(t) :=
 t
0
Φ(s) dW(s) :=
k−1

m=0
Φm

W(tm+1 ∧t)−W(tm ∧t)

, t ∈[0, T],
(this is obviously independent of the representation) for all Φ ∈E, we have
the following important result.

2.3. The deﬁnition of the stochastic integral
23
Proposition 2.3.2. Let Φ ∈E. Then the stochastic integral
 t
0
Φ(s) dW(s),
t ∈[0, T], deﬁned in the previous way, is a continuous square integrable mar-
tingale with respect to Ft, t ∈[0, T], i.e.
Int : E →M2
T .
Proof. Let Φ ∈E be given by
Φ(t) =
k−1

m=0
Φm1]tm,tm+1](t),
t ∈[0, T],
as in Deﬁnition 2.3.1. Then it is clear that
t 	→
 t
0
Φ(s) dW(s) =
k−1

m=0
Φm

W(tm+1 ∧t) −W(tm ∧t)

is P-a.s. continuous because of the continuity of the Wiener process and the
continuity of Φm(ω) : U →H, 0 ⩽m ⩽k −1, ω ∈Ω. In addition, we get for
each summand that
Φm

W(tm+1 ∧t) −W(tm ∧t)

⩽∥Φm∥L(U,H)
W(tm+1 ∧t) −W(tm ∧t)

U.
Since W(t), t ∈[0, T], is square integrable this implies that
 t
0
Φ(s) dW(s) is
square integrable for each t ∈[0, T].
To prove the martingale property we take 0 ⩽s ⩽t ⩽T and a set A from
Fs. If

Φm(ω)
 ω ∈Ω

:= {Lm
1 , . . . , Lm
km} we obtain by Proposition A.2.2
and the martingale property of the Wiener process (more precisely using

24
2. Stochastic Integral in Hilbert Spaces
optional stopping) that

A
k−1

m=0
Φm

W(tm+1 ∧t) −W(tm ∧t)

dP
=

0⩽m⩽k−1,
tm+1<s

A
Φm

W(tm+1 ∧s) −W(tm ∧s)

dP
+

0⩽m⩽k−1,
s⩽tm+1
km

j=1

A∩{Φm=Lm
j }
Lm
j

W(tm+1 ∧t) −W(tm ∧t)

dP
=

0⩽m⩽k−1,
tm+1<s

A
Φm

W(tm+1 ∧s) −W(tm ∧s)

dP
+

0⩽m⩽k−1,
s⩽tm+1
km

j=1
Lm
j

A∩{Φm=Lm
j }
!
"#
$
∈Fs∨tm
W(tm+1 ∧t) −W(tm ∧t) dP
=

0⩽m⩽k−1,
tm+1<s

A
Φm

W(tm+1 ∧s) −W(tm ∧s)

dP
+

0⩽m⩽k−1,
tm<s⩽tm+1
km

j=1
Lm
j

A∩{Φm=Lm
j }
W(tm+1 ∧s) −W(tm ∧s) dP
=

A
k−1

m=0
Φm

W(tm+1 ∧s) −W(tm ∧s)

dP.
Step 2:
To verify the assertion that there is a norm on E such that Int :
E →M2
T is an isometry, we have to introduce the following notion.
Deﬁnition 2.3.3 (Hilbert–Schmidt operator). Let ek, k ∈N, be an or-
thonormal basis of U. An operator A ∈L(U, H) is called Hilbert-Schmidt
if

k∈N
⟨Aek, Aek⟩< ∞.
In Appendix B we take a close look at this notion. So here we only sum-
marize the results which are important for the construction of the stochastic
integral.
The deﬁnition of a Hilbert–Schmidt operator and the number
∥A∥L2 :=
	
k∈N
∥Aek∥2
 1
2

2.3. The deﬁnition of the stochastic integral
25
are independent of the choice of the basis (see Remark B.0.6(i)). Moreover,
the space L2(U, H) of all Hilbert–Schmidt operators from U to H equipped
with the inner product
⟨A, B⟩L2 :=

k∈N
⟨Aek, Bek⟩
is a separable Hilbert space (see Proposition B.0.7). Later, we will use the
fact that ∥A∥L2(U,H) = ∥A∗∥L2(H,U), where A∗is the adjoint operator of A
(see Remark B.0.6(i)). Furthermore, compositions of Hilbert–Schmidt with
bounded linear operators are again Hilbert–Schmidt.
Besides we recall the following fact.
Proposition 2.3.4. If Q ∈L(U) is nonnegative and symmetric then there
exists exactly one element Q
1
2 ∈L(U) nonnegative and symmetric such that
Q
1
2 ◦Q
1
2 = Q.
If, in addition, tr Q < ∞we have that Q
1
2 ∈L2(U) where ∥Q
1
2 ∥2
L2 = tr Q
and of course L ◦Q
1
2 ∈L2(U, H) for all L ∈L(U, H).
Proof.
[RS72, Theorem VI.9, p. 196]
After these preparations we simply calculate the M2
T -norm of
 t
0
Φ(s) dW(s), t ∈[0, T],
and get the following result.
Proposition 2.3.5. If Φ = k−1
m=0 Φm1]tm,tm+1] is an elementary L(U, H)-
valued process then

 ·
0
Φ(s) dW(s)

2
M2
T
= E
 T
0
Φ(s)◦Q
1
2 2
L2 ds

=: ∥Φ∥2
T (“Itˆo-isometry”).
Proof. If we set ∆m := W(tm+1) −W(tm) then we get that

 ·
0
Φ(s) dW(s)

2
M2
T
= E

 T
0
Φ(s) dW(s)

2
H

= E

k−1

m=0
Φm∆m

2
H

= E
	 k−1

m=0
∥Φm∆m∥2
H

+ 2E
	

0⩽m<n⩽k−1
⟨Φm∆m, Φn∆n⟩H

.
Claim 1:
E
	 k−1

m=0
∥Φm∆m∥2
H

=
k−1

m=0
(tm+1 −tm)E

∥Φm ◦Q
1
2 ∥2
L2

=
 T
0
E
	Φ(s) ◦Q
1
2 2
L2

ds.

26
2. Stochastic Integral in Hilbert Spaces
To prove this we take an orthonormal basis fk, k ∈N, of H and get by the
Parseval identity and Levi’s monotone convergence theorem that
E

∥Φm∆m∥2
H

=

l∈N
E

⟨Φm∆m, fl⟩2
H

=

l∈N
E
	
E

⟨∆m, Φ∗
mfl⟩2
U
 Ftm

.
Taking an orthonormal basis ek, k ∈N, of U we obtain that
Φ∗
mfl =

k∈N
⟨fl, Φmek⟩Hek.
Since ⟨fl, Φmek⟩H is Ftm-measurable, this implies that Φ∗
mfl is Ftm-measurable
by Proposition A.1.3. Using the fact that σ(∆m) is independent of Ftm we
obtain by Lemma 2.2.2 that for P-a.e. ω ∈Ω
E

⟨∆m, Φ∗
mfl⟩2
U
 Ftm

(ω) = E
	
∆m, Φ∗
m(ω)fl
2
U

= (tm+1 −tm)

Q

Φ∗
m(ω)fl

, Φ∗
m(ω)fl

U,
since E

⟨∆m, u⟩2
U

= (tm+1 −tm)⟨Qu, u⟩U for all u ∈U. Thus, the symmetry
of Q
1
2 ﬁnally implies that
E

∥Φm∆m∥2
H

=

l∈N
E
	
E

⟨∆m, Φ∗
mfl⟩2
U
 Ftm

= (tm+1 −tm)

l∈N
E

⟨QΦ∗
mfl, Φ∗
mfl⟩U

= (tm+1 −tm)

l∈N
E
	Q
1
2 Φ∗
mfl
2
U

= (tm+1 −tm)E


Φm ◦Q
1
2 ∗
2
L2(H,U)

= (tm+1 −tm)E
	Φm ◦Q
1
2 2
L2(U,H)

.
Hence the ﬁrst assertion is proved and it only remains to verify the following
claim.
Claim 2:
E

⟨Φm∆m, Φn∆n⟩H

= 0 ,
0 ⩽m < n ⩽k −1.
But this can be proved in a similar way to Claim 1:
E

⟨Φm∆m, Φn∆n⟩H

= E
	
E

⟨Φ∗
nΦm∆m, ∆n⟩U
 Ftn

=

E
	
Φ∗
n(ω)Φm(ω)∆m(ω), ∆n

U

P(dω) = 0,

2.3. The deﬁnition of the stochastic integral
27
since E

⟨u, ∆n⟩U

= 0 for all u ∈U (see Proposition 2.2.2). Hence the asser-
tion follows.
Hence the right norm on E has been identiﬁed. But strictly speaking ∥∥T
is only a seminorm on E. Therefore, we have to consider equivalence classes of
elementary processes with respect to ∥∥T to get a norm on E. For simplicity
we will not change the notation but stress the following fact.
Remark 2.3.6. If two elementary processes Φ and ˜Φ belong to one equiva-
lence class with respect to ∥∥T it does not follow that they are equal PT -a.e.
because their values only have to correspond on Q
1
2 (U) PT -a.e.
Thus we ﬁnally have shown that
Int :

E, ∥∥T

→

M2
T , ∥∥M2
T

is an isometric transformation. Since E is dense in the abstract completion ¯E
of E with respect to ∥∥T it is clear that there is a unique isometric extension
of Int to ¯E.
Step 3:
To give an explicit representation of ¯E it is useful, at this moment,
to introduce the subspace U0 := Q
1
2 (U) with the inner product given by
⟨u0, v0⟩0 :=

Q−1
2 u0, Q−1
2 v0

U,
u0, v0 ∈U0, where Q−1
2 is the pseudo inverse of Q
1
2 in the case that Q is not
one-to-one. Then we get by Proposition C.0.3(i) that (U0, ⟨, ⟩0) is again a
separable Hilbert space.
The separable Hilbert space L2(U0, H) is called L0
2. By Proposition C.0.3(ii)
we know that Q
1
2 gk, k ∈N, is an orthonormal basis of

U0, ⟨, ⟩0

if gk, k ∈N,
is an orthonormal basis of

Ker Q
1
2 ⊥. This basis can be supplemented to a
basis of U by elements of Ker Q
1
2 . Thus we obtain that
∥L∥L0
2 =
L ◦Q
1
2 
L2
for each L ∈L0
2.
Deﬁne L(U, H)0 :=

T |U0
 T ∈L(U, H)

. Since Q
1
2 ∈L2(U) it is clear
that L(U, H)0 ⊂L0
2 and that the ∥∥T -norm of Φ ∈E can be written in the
following way:
∥Φ∥T =

E
 T
0
∥Φ(s)∥2
L0
2 ds
 1
2
Besides we need the following σ-ﬁeld:
PT := σ
	
]s, t] × Fs
 0 ⩽s < t ⩽T, Fs ∈Fs

∪

{0} × F0
 F0 ∈F0

= σ

Y : ΩT →R
 Y is left-continuous and adapted to
Ft, t ∈[0, T]

.

28
2. Stochastic Integral in Hilbert Spaces
Let ˜H be an arbitrary separable Hilbert space. If Y : ΩT →˜H is PT /B( ˜H)-
measurable it is called ( ˜H-)predictable.
If, for example, the process Y itself is continuous and adapted to Ft,
t ∈[0, T], then it is predictable.
So, we are now able to characterize ¯E.
Claim:
There is an explicit representation of ¯E and it is given by
N 2
W (0, T; H) :=

Φ : [0, T] × Ω→L0
2
 Φ is predictable and ∥Φ∥T < ∞

= L2 
[0, T] × Ω, PT , dt ⊗P; L0
2

.
For simplicity we also write N 2
W (0, T) or N 2
W instead of N 2
W (0, T; H).
To prove this claim we ﬁrst notice the following facts:
1. Since L(U, H)0 ⊂L0
2 and since any Φ ∈E is L0
2-predictable by construction
we have that E ⊂N 2
W .
2. Because of the completeness of L0
2 we get by Appendix A that
N 2
W = L2(ΩT , PT , PT ; L0
2)
is also complete.
Therefore N 2
W is at least a candidate for a representation of ¯E. Thus there
only remains to show that E is a dense subset of N 2
W . But this is formulated
in Proposition 2.3.8 below, which can be proved with the help of the following
lemma.
Lemma 2.3.7. There is an orthonormal basis of L0
2 consisting of elements
of L(U, H)0. This implies especially that L(U, H)0 is a dense subset of L0
2.
Proof. Since Q is symmetric, nonnegative and tr Q < ∞we know by
Lemma 2.1.5 that there exists an orthonormal basis ek, k ∈N, of U such
that Qek = λkek, λk ⩾0, k ∈N. In this case Q
1
2 ek = √λkek, k ∈N with
λk > 0, is an orthonormal basis of U0 (see Proposition C.0.3(ii)).
If fk, k ∈N, is an orthonormal basis of H then by Proposition B.0.7 we
know that
fj ⊗

λkek = fj⟨

λkek, ·⟩U0 = 1
λk
fj⟨ek, ·⟩U,
j, k ∈N, λk > 0,
form an orthonormal basis of L2
0 consisting of operators in L(U, H). But, of
course,
span

1
√λk
fj ⊗ek
 j, k ∈N with λk > 0

= L0
2.

2.3. The deﬁnition of the stochastic integral
29
Proposition 2.3.8. If Φ is a L0
2-predictable process such that ∥Φ∥T < ∞then
there exists a sequence Φn, n ∈N, of L(U, H)0-valued elementary processes
such that
∥Φ −Φn∥T −→0
as n →∞.
Proof. Step 1: If Φ ∈N 2
W there exists a sequence of simple random variables
Φn = Mn
k=1 Ln
k1An
k , An
k ∈PT and Ln
k ∈L0
2, n ∈N, such that
∥Φ −Φn∥T −→0
as n →∞.
As L0
2 is a Hilbert space this is a simple consequence of Lemma A.1.4 and
Lebesgue’s dominated convergence theorem.
Thus the assertion is reduced to the case that Φ = L1A where L ∈L0
2 and
A ∈PT .
Step 2: Let A ∈PT and L ∈L0
2. Then there exists a sequence Ln, n ∈N, in
L(U, H)0 such that
∥L1A −Ln1A∥T −→0
as n →∞.
This result is obvious by Lemma 2.3.7 and thus now we only have to consider
the case that Φ = L1A, L ∈L(U, H)0 and A ∈PT .
Step 3:
If Φ = L1A, L ∈L(U, H)0, A ∈PT , then there is a sequence Φn,
n ∈N, of elementary L(U, H)0-valued processes in the sense of Deﬁnition 2.3.1
such that
∥L1A −Φn∥T −→0
as n −→∞.
To show this it is suﬃcient to prove that for any ε > 0 there is a ﬁnite union
Λ =
N

n=1
An of pairwise disjoint predictable rectangles
An ∈

]s, t] × Fs
 0 ⩽s < t ⩽T, Fs ∈Fs

∪

{0} × F0
 F0 ∈F0

=: A
such that
PT

(A \ Λ) ∪(Λ \ A)

< ε.
For then we get that N
n=1 L1An diﬀers from an elementary process by a
function of type 1{0}×F0 with F0 ∈F0, which has ∥· ∥T -norm zero and
L1A −
N

n=1
L1An

2
T = E
 T
0
L
	
1A −
N

n=1
1An


2
L0
2
ds

⩽ε∥L∥2
L0
2.
Hence we deﬁne
K :=

i∈I
Ai
 I is ﬁnite and Ai ∈A, i ∈I

.
Then K is an algebra and any element in K can be written as a ﬁnite disjoint
union of elements in A. Now let G be the family of all A ∈PT which can
be approximated by elements of K in the above sense. Then G is a Dynkin
system and therefore PT = σ(K) = D(K) ⊂G as K ⊂G.

30
2. Stochastic Integral in Hilbert Spaces
Step 4:
Finally the so-called localization procedure provides the possibility
to extend the deﬁnition of the stochastic integral even to the linear space
NW (0, T; H) :=

Φ : ΩT →L0
2
 Φ is predictable with
P
 T
0
∥Φ(s)∥2
L0
2 ds < ∞

= 1
(
.
For simplicity we also write NW (0, T) or NW instead of NW (0, T; H) and NW
is called the class of stochastically integrable processes on [0, T].
The extension is done in the following way:
For Φ ∈NW we deﬁne
τn := inf

t ∈[0, T]

 t
0
∥Φ(s)∥2
L0
2 ds > n

∧T.
(2.3.1)
Then by the right-continuity of the ﬁltration Ft, t ∈[0, T], we get that
{τn ⩽t} =
 
m∈N

τn < t + 1
m

=
 
m∈N

q∈[0,t+ 1
m [∩Q
 q
0
∥Φ(s)∥2
L0
2 ds > n

!
"#
$
∈Fq by the real Fubini theorem
!
"#
$
∈Ft+ 1
m and decreasing in m
∈Ft.
Therefore τn, n ∈N, is an increasing sequence of stopping times with respect
to Ft, t ∈[0, T], such that
E
 T
0
∥1]0,τn](s)Φ(s)∥2
L0
2 ds

⩽n < ∞.
In addition, the processes 1]0,τn]Φ, n ∈N, are still L0
2-predictable since 1]0,τn]
is left-continuous and (Ft)-adapted or since
]0, τn] :=

(s, ω) ∈ΩT
 0 < s ⩽τn(ω)

=
	
(s, ω) ∈ΩT
 τn(ω) < s ⩽T

∪{0} × Ω

c
=
	 
q∈Q

]q, T] × {τn ⩽q}
!
"#
$
∈Fq

!
"#
$
∈PT
∪{0} × Ω

c
∈PT .
Thus we get that the stochastic integrals
 t
0
1]0,τn](s)Φ(s) dW(s),
t ∈[0, T],

2.3. The deﬁnition of the stochastic integral
31
are well-deﬁned for all n ∈N. For arbitrary t ∈[0, T] we set
 t
0
Φ(s) dW(s) :=
 t
0
1]0,τn](s)Φ(s) dW(s),
(2.3.2)
where n is an arbitrary natural number such that τn ⩾t. (Note that the
sequence τn, n ∈N, even reaches T P-a.s., in the sense that for P-a.e. ω ∈Ω
there exists n(ω) ∈N such that τn(ω) = T for all n ⩾n(ω).)
To show that this deﬁnition is consistent we have to prove that for arbitrary
natural numbers m < n and t ∈[0, T]
 t
0
1]0,τm](s)Φ(s) dW(s) =
 t
0
1]0,τn](s)Φ(s) dW(s)
P-a.s.
on {τm ⩾t} ⊂{τn ⩾t}. This result follows from the following lemma, which
implies that the process in (2.3.2) is a continuous H-valued local martingale.
Lemma 2.3.9. Assume that Φ ∈N 2
W and that τ is an Ft-stopping time such
that P(τ ⩽T) = 1. Then there exists a P-null set N ∈F independent of
t ∈[0, T] such that
 t
0
1]0,τ](s)Φ(s) dW(s) = Int

1]0,τ]Φ

(t) = Int(Φ)(τ ∧t)
=
 τ∧t
0
Φ(s) dW(s)
on N c for all t ∈[0, T].
Proof. Since both integrals which appear in the equation are P-a.s. continuous
we only have to prove that they are equal P-a.s. at any ﬁxed time t ∈[0, T].
Step 1: We ﬁrst consider the case that Φ ∈E and that τ is a simple stopping
time which means that it takes only a ﬁnite number of values.
Let 0 = t0 < t1 < · · · < tk ⩽T, k ∈N, and
Φ =
k−1

m=0
Φm1]tm,tm+1]
where Φm : Ω→L(U, H) is Ftm-measurable and only takes a ﬁnite number
of values for all 0 ⩽m ⩽k −1.
If
τ
is
a
simple
stopping
time
there
exists
n
∈
N
such
that
τ(Ω) = {a0, . . . , an} and
τ =
n

j=0
aj1Aj
where 0 ⩽aj < aj+1 ⩽T and Aj = {τ = aj} ∈Faj. In this way we get that

32
2. Stochastic Integral in Hilbert Spaces
1]τ,T ]Φ is an elementary process since
1]τ,T ](s)Φ(s) =
k−1

m=0
Φm1]tm,tm+1]∩]τ,T ](s)
=
k−1

m=0
n

j=0
1AjΦm1]tm,tm+1]∩]aj,T ](s)
=
k−1

m=0
n

j=0
1AjΦm
! "# $
Ftm∨aj -measurable
1]tm∨aj,tm+1∨aj](s)
and concerning the integral we are interested in, we obtain that
 t
0
1]0,τ](s)Φ(s) dW(s) =
 t
0
Φ(s) dW(s) −
 t
0
1]τ,T ](s)Φ(s) dW(s)
=
k−1

m=0
Φm

W(tm+1 ∧t) −W(tm ∧t)

−
k−1

m=0
n

j=0
1AjΦm
	
W

(tm+1 ∨aj) ∧t

−W

(tm ∨aj) ∧t

=
k−1

m=0
Φm

W(tm+1 ∧t) −W(tm ∧t)

−
k−1

m=0
n

j=0
1AjΦm
	
W

(tm+1 ∨τ) ∧t

−W

(tm ∨τ) ∧t

=
k−1

m=0
Φm

W(tm+1 ∧t) −W(tm ∧t)

−
k−1

m=0
Φm
	
W

(tm+1 ∨τ) ∧t

−W

(tm ∨τ) ∧t

=
k−1

m=0
Φm
	
W(tm+1 ∧t) −W(tm ∧t)
−W

(tm+1 ∨τ) ∧t

−W

(tm ∨τ) ∧t

=
k−1

m=0
Φm
	
W(tm+1 ∧τ ∧t) −W(tm ∧τ ∧t)

=
 t∧τ
0
Φ(s) dW(s).
Step 2: Now we consider the case that Φ is still an elementary process while
τ is an arbitrary stopping time with P(τ ⩽T) = 1.

2.3. The deﬁnition of the stochastic integral
33
Then there exists a sequence
τn =
2n−1

k=0
T(k + 1)2−n1]T k2−n,T (k+1)2−n] ◦τ,
n ∈N,
of simple stopping times such that τn ↓τ as n →∞and because of the
continuity of the stochastic integral we get that
 τn∧t
0
Φ(s) dW(s)
n→∞
−−−−→
 τ∧t
0
Φ(s) dW(s)
P-a.s.
Besides, we obtain (even for non-elementary processes Φ) that
1]0,τn]Φ −1]0,τ]Φ
2
T = E
 T
0
1]τ,τn](s)∥Φ(s)∥2
L0
2 ds

n→∞
−−−−→0,
which by the deﬁnition of the integral implies that
E

 t
0
1]0,τn](s)Φ(s) dW(s) −
 t
0
1]0,τ](s)Φ(s) dW(s)

2
n→∞
−−−−→0
for all t ∈[0, T]. As by Step 1
 t
0
1]0,τn](s)Φ(s) dW(s) =
 τn∧t
0
Φ(s) dW(s),
n ∈N, t ∈[0, T],
the assertion follows.
Step 3: Finally we generalize the statement to arbitrary Φ ∈N 2
W (0, T):
If Φ ∈N 2
W (0, T) then there exists a sequence of elementary processes Φn,
n ∈N, such that
∥Φn −Φ∥T
n→∞
−−−−→0 .
By the deﬁnition of the stochastic integral this means that
 ·
0
Φn(s) dW(s)
n→∞
−−−−→
 ·
0
Φ(s) dW(s)
in M2
T .
Hence it follows that there is a subsequence nk, k ∈N, and a P-null set N ∈F
independent of t ∈[0, T] such that
 t
0
Φnk(s) dW(s)
k→∞
−−−−→
 t
0
Φ(s) dW(s)
on N c
for all t ∈[0, T] and therefore we get for all t ∈[0, T] that
 τ∧t
0
Φnk(s) dW(s)
k→∞
−−−−→
 τ∧t
0
Φ(s) dW(s)
P-a.s.

34
2. Stochastic Integral in Hilbert Spaces
In addition, it is clear that
∥1]0,τ]Φn −1]0,τ]Φ∥T −→
n→∞0
which implies that for all t ∈[0, T]
E

 t
0
1]0,τ](s)Φn(s) dW(s) −
 t
0
1]0,τ](s)Φ(s) dW(s)

2
n→∞
−−−−→0.
As by Step 2
 t
0
1]0,τ](s)Φnk(s) dW(s) =
 τ∧t
0
Φnk(s) dW(s)
P-a.s.
for all k ∈N the assertion follows.
Therefore, for m < n on {τm ⩾t} ⊂{τn ⩾t}
 t
0
1]0,τn](s)Φ(s) dW(s) =
 τm∧t
0
1]0,τn](s)Φ(s) dW(s)
=
 t
0
1]0,τm](s)1]0,τn](s)Φ(s) dW(s) =
 t
0
1]0,τm](s)Φ(s) dW(s)
P-a.s.,
where we used Lemma 2.3.9 for the second equality. Hence the deﬁnition is
consistent.
Remark 2.3.10. In fact it is easy to see that the deﬁnition of the stochastic
integral does not depend on the choice of τn, n ∈N. If σn, n ∈N, is another
sequence of stopping times such that σn ↑T as n →∞and 1]0,σn]Φ ∈N 2
W
for all n ∈N we also get that
 t
0
Φ(s) dW(s) = lim
n→∞
 t
0
1]0,σn](s)Φ(s) dW(s)
P-a.s. for all t ∈[0, T].
Proof. Let t ∈[0, T]. Then we get that on the set {τm ⩾t}
 t
0
Φ(s) dW(s) =
 t
0
1]0,τm](s)Φ(s) dW(s)
= lim
n→∞
 t∧σn
0
1]0,τm](s)Φ(s) dW(s)
= lim
n→∞
 t∧τm
0
1]0,σn](s)Φ(s) dW(s)
= lim
n→∞
 t
0
1]0,σn](s)Φ(s) dW(s)
P-a.s..

2.4. Properties of the stochastic integral
35
2.4. Properties of the stochastic integral
Let T be a positive real number and W(t), t ∈[0, T], a Q-Wiener process as
described at the beginning of the previous section.
Lemma
2.4.1. Let Φ be a L0
2-valued stochastically integrable process,
( ˜H, ∥∥˜
H) a further separable Hilbert space and L ∈L(H, ˜H).
Then the process L

Φ(t)

, t ∈[0, T], is an element of NW (0, T; ˜H) and
L
 T
0
Φ(t) dW(t)

=
 T
0
L

Φ(t)

dW(t)
P-a.s.
Proof. Since Φ is a stochastically integrable process and
L

Φ(t)

L2(U0, ˜
H) ⩽∥L∥L(H, ˜
H)∥Φ(t)∥L0
2,
it is obvious that L

Φ(t)

, t ∈[0, T], is L2(U0, ˜H)-predictable and
P
 T
0
L

Φ(t)

2
L2(U0, ˜
H) dt < ∞

= 1.
Step 1:
As the ﬁrst step we consider the case that Φ is an elementary
process, i.e.
Φ(t) =
k−1

m=0
Φm1]tm,tm+1](t),
t ∈[0, T],
where 0 = t0 < t1 < · · · < tk = T, Φm : Ω→L(U, H) Ftm-measurable with
Φm(Ω)
 < ∞for 0 ⩽m ⩽k. Then
L
 T
0
Φ(t) dW(t)

= L
	 k−1

m=0
Φm

W(tm+1) −W(tm)

=
k−1

m=0
L
	
Φm

W(tm+1) −W(tm)

=
 T
0
L

Φ(t)

dW(t).
Step 2:
Now let Φ ∈N 2
W (0, T). Then there exists a sequence Φn, n ∈N, of
elementary processes with values in L(U, H)0 such that
∥Φn −Φ∥T =

E
 T
0
∥Φn(t) −Φ(t)∥2
L0
2 dt
 1
2
n→∞
−−−−→0.
Then L(Φn), n ∈N, is a sequence of elementary processes with values in
L(U, ˜H)0 and
L(Φn) −L(Φ)

T ⩽∥L∥L(H, ˜
H)∥Φn −Φ∥T
n→∞
−−−−→0.

36
2. Stochastic Integral in Hilbert Spaces
By the deﬁnition of the stochastic integral, Step 1 and the continuity of L we
get that there is a subsequence nk, k ∈N, such that
 T
0
L

Φ(t)

dW(t) = lim
k→∞
 T
0
L

Φnk(t)

dW(t)
= lim
k→∞L
 T
0
Φnk(t) dW(t)

= L

lim
k→∞
 T
0
Φnk(t) dW(t)

= L
 T
0
Φ(t) dW(t)

P-a.s.
Step 3:
Finally let Φ ∈NW (0, T).
Let τn, n ∈N, be a sequence of stopping times such that τn ↑T as n →∞
and 1]0,τn]Φ ∈N 2
W (0, T, H). Then 1]0,τn]L(Φ) ∈N 2
W (0, T, ˜H) for all n ∈N and
we obtain by Remark 2.3.10 and Step 2 (selecting a subsequence if necessary)
 T
0
L

Φ(t)

dW(t) = lim
n→∞
 T
0
1]0,τn](t)L

Φ(t)

dW(t)
= lim
n→∞L
 T
0
1]0,τn](t)Φ(t) dW(t)

= L

lim
n→∞
 T
0
1]0,τn](t)Φ(t) dW(t)

= L
 T
0
Φ(t) dW(t)

P-a.s.
Lemma 2.4.2. Let Φ ∈NW (0, T) and f an (Ft)-adapted continuous H-
valued process. Set
 T
0

f(t), Φ(t) dW(t)

:=
 T
0
˜Φf(t) dW(t)
(2.4.1)
with
˜Φf(t)(u) :=

f(t), Φ(t)u

, u ∈U0.
Then the stochastic integral in (2.4.1) is well-deﬁned as a continuous R-valued
stochastic process. More precisely, ˜Φf is a PT /B(L2(U0, R))-measurable map
from [0, T] × Ωto L2(U0, R),
∥˜Φf(t, ω)∥L2(U0,R) = ∥Φ∗(t, ω)f(t, ω)∥U0
for all (t, ω) ∈[0, T] × Ωand
 T
0
∥˜Φf(t)∥2
L2(U0,R) dt ⩽sup
t∈[0,T ]
∥f(t)∥
 T
0
∥Φ(t)∥2
L0
2 dt < ∞
P-a.e..

2.4. Properties of the stochastic integral
37
Proof. Since f is continuous, ˜Φf is clearly predictable. Let ek, k ∈N, be an
orthonormal basis of U0. Then for all (t, ω) ∈[0, T] × Ω
∥˜Φf(t, ω)∥2
L2(U0,R) =
∞

k=1
⟨f(t, ω), Φ(t, ω)ek⟩2
=
∞

k=1
⟨Φ∗(t, ω)f(t, ω), ek⟩2
U0
= ∥Φ∗(t, ω)f(t, ω)∥2
U0
⩽∥Φ∗(t, ω)∥2
L(H,U0)∥f(t, ω)∥2
H
⩽∥Φ∗(t, ω)∥2
L2(H,U0)∥f(t, ω)∥2
H
= ∥Φ(t, ω)∥2
L0
2∥f(t, ω)∥2
H,
where we used Remark B.0.6(i) in the last step. Now all assertions follow.
Lemma 2.4.3. Let Φ ∈NW (0, T) and M(t) :=
' t
0 Φ(s) dW(s), t ∈[0, T].
Deﬁne
⟨M⟩t :=
 t
0
∥Φ(s)∥2
L0
2 ds, t ∈[0, T].
Then ⟨M⟩is the unique continuous increasing (Ft)-adapted process starting
at zero such that ∥M(t)∥2 −⟨M⟩t, t ∈[0, T], is a local martingale. If Φ ∈
N 2
W (0, T), then for any sequence
Il := {0 = tl
0 < tl
1 < . . . < tl
kl = T}, l ∈N,
of partitions with
max
i (tl
i −tl
i−1) →0 as l →∞
lim
l→∞E
⎛
⎝


tl
j+1⩽t
∥M(tl
j+1) −M(tl
j)∥2 −⟨M⟩t

⎞
⎠= 0.
Proof. For n ∈N let τn be as in (2.3.1) and τ an Ft-stopping time with
P[τ ⩽T] = 1. Then by Lemma 2.3.9 for σ := τ ∧τn, t ∈[0, T]
E

 t∧σ
0
Φ(s) dW(s)

2
= E

 t
0
1]0,σ]Φ(s) dW(s)

2
= E
 t
0
∥1]0,σ]Φ(s)∥2
L0
2 ds

= E
 t∧σ
0
∥Φ(s)∥2
L0
2 ds

,

38
2. Stochastic Integral in Hilbert Spaces
and the ﬁrst assertion follows, because the uniqueness is obvious, since any
real-valued local martingale of bounded variation is constant.
To prove the second assertion we ﬁx an orthonormal basis {ei|i ∈N} of H
and note that by the theory of real-valued martingales we have for each i ∈N
lim
l→∞E
⎛
⎝


tl
j+1⩽t
⟨ei, M(tl
j+1) −M(tl
j)⟩2
H −
 t
0
∥Φ(s)∗ei∥2
U0 ds

⎞
⎠= 0, (2.4.2)
since by the ﬁrst part of the assertion and Lemmas 2.4.1 and 2.4.2
 t
0
⟨ei, Φ(s) dW(s)⟩H

t
=
 t
0
∥Φ(s)∗ei∥2
U0 ds, t ∈[0, T].
Furthermore, for all i ∈N
E
⎛
⎝


tl
j+1⩽t
⟨ei, M(tl
j+1) −M(tl
j)⟩2
H −
 t
0
∥Φ(s)∗ei∥2
U0 ds

⎞
⎠
⩽

tl
j+1⩽t
E
⎡
⎣
 tl
j+1
tl
j
⟨ei, Φ(s) dW(s)⟩H
2⎤
⎦+ E
 t
0
∥Φ(s)∗ei∥2
U0 ds

=

tl
j+1⩽t
E
 tl
j+1
tl
j
∥Φ(s)∗ei∥2
U0 ds

+ E
 t
0
∥Φ(s)∗ei∥2
U0 ds

⩽2E
 t
0
∥Φ(s)∗ei∥2
U0 ds

(2.4.3)
which is summable over i ∈N. Here we used the isometry property of Int in
the second to last step. But
E
⎛
⎝


tl
j+1⩽t
∥M(tl
j+1) −M(tl
j)∥2 −
 t
0
∥Φ(s)∥2
L0
2 ds

⎞
⎠
= E
⎛
⎝

∞

i=1
⎛
⎝
tl
j+1⩽t
⟨ei, M(tl
j+1) −M(tl
j)⟩2
H −
 t
0
∥Φ(s)∗ei∥2
U0 ds
⎞
⎠

⎞
⎠
⩽
∞

i=1
E
⎛
⎝


tl
j+1⩽t
⟨ei, M(tl
j+1) −M(tl
j)⟩2
H −
 t
0
∥Φ(s)∗ei∥2
U0 ds

⎞
⎠
where we used Remark B.0.6(i) in the second step. Hence the second assertion
follows by Lebesgue dominated convergence theorem from (2.4.2) and (2.4.3).

2.5. The stochastic integral for cylindrical Wiener processes
39
2.5. The stochastic integral for cylindrical
Wiener processes
Until now we have considered the case that W(t), t ∈[0, T], was a standard
Q-Wiener process where Q ∈L(U) was nonnegative, symmetric and with
ﬁnite trace. We could integrate processes in
NW :=

Φ : ΩT →L2(Q
1
2 (U), H) | Φ is predictable and
P
 T
0
∥Φ(s)∥2
L0
2 ds < ∞

= 1

.
In fact it is possible to extend the deﬁnition of the stochastic integral to the
case that Q is not necessarily of ﬁnite trace. To this end we ﬁrst have to
introduce the concept of cylindrical Wiener processes.
2.5.1. Cylindrical Wiener processes
Let Q ∈L(U) be nonnegative deﬁnite and symmetric. Remember that in
the case that Q is of ﬁnite trace the Q-Wiener process has the following
representation:
W(t) =

k∈N
βk(t)ek,
t ∈[0, T],
where ek, k ∈N, is an orthonormal basis of Q
1
2 (U) = U0 and βk, k ∈N, is
a family of independent real-valued Brownian motions. The series converges
in L2(Ω, F, P; U), because the inclusion U0 ⊂U deﬁnes a Hilbert–Schmidt
embedding from (U0, ⟨, ⟩0) to (U, ⟨, ⟩). In the case that Q is no longer of
ﬁnite trace one looses this convergence. Nevertheless, it is possible to deﬁne
the Wiener process.
To this end we need a further Hilbert space (U1, ⟨, ⟩1) and a Hilbert–Schmidt
embedding
J : (U0, ⟨, ⟩0) →(U1, ⟨, ⟩1).
Remark 2.5.1. (U1, ⟨, ⟩1)) and J as above always exist; e.g. choose U1 := U
and αk ∈]0, ∞[, k ∈N, such that ∞
k=1 α2
k < ∞. Deﬁne J : U0 →U by
J(u) :=
∞

k=1
αk⟨u, ek⟩0 ek,
u ∈U0.
Then J is one-to-one and Hilbert–Schmidt.
Then the process given by the following proposition is called a cylindrical
Q-Wiener process in U.

40
2. Stochastic Integral in Hilbert Spaces
Proposition 2.5.2. Let ek, k ∈N, be an orthonormal basis of U0 = Q
1
2 (U)
and βk, k ∈N, a family of independent real-valued Brownian motions. Deﬁne
Q1 := JJ∗. Then Q1 ∈L(U1), Q1 is nonnegative deﬁnite and symmetric with
ﬁnite trace and the series
W(t) =
∞

k=1
βk(t)Jek,
t ∈[0, T],
(2.5.1)
converges in M2
T (U1) and deﬁnes a Q1-Wiener process on U1. Moreover, we
have that Q
1
2
1 (U1) = J(U0) and for all u0 ∈U0
∥u0∥0 = ∥Q
−1
2
1
Ju0∥1 = ∥Ju0∥
Q
1
2
1 U1,
i.e. J : U0 →Q
1
2
1 U1 is an isometry.
Proof. Step 1: We prove that W(t), t ∈[0, T], deﬁned in (2.5.1) is a Q1-
Wiener process in U1.
If we set ξj(t) := βj(t)J(ej), j ∈N, we obtain that ξj(t), t ∈[0, T], is a
continuous U1-valued martingale with respect to
Gt := σ
 
j∈N
σ(βj(s)|s ⩽t)

,
t ∈[0, T], since
E(βj(t) | Gs) = E(βj(t) | σ(βj(u)|u ⩽s)) = βj(s)
for all 0 ⩽s < t ⩽T
as σ

σ(βj(u)|u ⩽s) ∪σ(βj(t))

is independent of
σ
 
k∈N
k̸=j
σ(βk(u)|u ⩽s)

.
Then it is clear that
Wn(t) :=
n

j=1
βj(t)J(ej),
t ∈[0, T],
is also a continuous U1-valued martingale with respect to Gt, t ∈[0, T]. In
addition, we obtain that
E
⎛
⎝sup
t∈[0,T ]
∥
m

j=n
βj(t)J(ej)∥2
1
⎞
⎠⩽4 sup
t∈[0,T ]
E
⎛
⎝∥
m

j=n
βj(t)J(ej)∥2
1
⎞
⎠
= 4T
m

j=n
∥J(ej)∥2
1,
m ⩾n ⩾1.

2.5. The stochastic integral for cylindrical Wiener processes
41
Note that ∥J∥2
L2(U0,U1) =

j∈N
∥J(ej)∥2
1 < ∞. Therefore, we get the convergence
of Wn(t), t ∈[0, T], in M2
T (U1), hence the limit W(t), t ∈[0, T], is P-a.s. con-
tinuous.
Now we want to show that P ◦(W(t) −W(s))−1 = N(0, (t −s)JJ∗). Anal-
ogously to the second part of the proof of Proposition 2.1.6 we get that
⟨W(t) −W(s), u1⟩1 is normally distributed for all 0 ⩽s < t ⩽T and u1 ∈U1.
It is easy to see that the mean is equal to zero and concerning the covariance
of ⟨W(t) −W(s), u1⟩1 and ⟨W(t) −W(s), v1⟩1, u1, v1 ∈U1, we obtain that
E(⟨W(t) −W(s), u1⟩1⟨W(t) −W(s), v1⟩1)
=

k∈N
(t −s)⟨Jek, u1⟩1⟨Jek, v1⟩1
= (t −s)

k∈N
⟨ek, J∗u1⟩0⟨ek, J∗v1⟩0
= (t −s)⟨J∗u1, J∗v1⟩0 = (t −s)⟨JJ∗u1, v1⟩1.
Thus, it only remains to show that the increments of W(t), t ∈[0, T], are
independent but this can be done in the same way as in the proof of Propo-
sition 2.1.10.
Step 2: We prove that Im Q
1
2
1 = J(U0) and that ∥u0∥0 = ∥Q
−1
2
1
Ju0∥1 for all
u0 ∈U0.
Since Q1 = JJ∗, by Corollary C.0.6 we obtain that Q
1
2
1 (U1) = J(U0) and that
∥Q
−1
2
1
u1∥1 = ∥J−1u1∥0 for all u1 ∈J(U0). We now replace u1 by J(u0),
u0 ∈U0, to get the last assertion, because J : U0 →U1 is one-to-one.
2.5.2. The deﬁnition of the stochastic integral
for cylindrical Wiener processes
We ﬁx Q ∈L(U) nonnegative, symmetric but not necessarily of ﬁnite trace.
After the preparations of the previous section we are now able to deﬁne the
stochastic integral with respect to a cylindrical Q-Wiener process W(t), t ∈
[0, T].
Basically we integrate with respect to the standard U1-valued Q1-Wiener
process given by Proposition 2.5.2. In this sense we ﬁrst get that a process
Φ(t), t ∈[0, T], is integrable with respect to W(t), t ∈[0, T], if it takes values
in L2(Q
1
2
1 (U1), H), is predictable and if
P
 T
0
∥Φ(s)∥2
L2(Q
1
2
1 (U1),H) ds < ∞

= 1.

42
2. Stochastic Integral in Hilbert Spaces
But in addition, we have by Proposition 2.5.2 that Q
1
2
1 (U1) = J(U0) and that
⟨Ju0, Jv0⟩
Q
1
2
1 (U1) = ⟨Q
−1
2
1
Ju0, Q
−1
2
1
Jv0⟩1 = ⟨u0, v0⟩0
for all u0, v0 ∈U0 (by polarization). In particular, it follows that Jek, k ∈N,
is an orthonormal basis of Q
1
2
1 (U1). Hence we get that
Φ ∈L0
2 = L2(Q
1
2 (U), H) ⇐⇒Φ ◦J−1 ∈L2(Q
1
2
1 (U1), H)
since
∥Φ∥2
L0
2 =

k∈N
⟨Φek, Φek⟩
=

k∈N
⟨Φ ◦J−1(Jek), Φ ◦J−1(Jek)⟩= ∥Φ ◦J−1∥2
L2(Q
1
2
1 (U1),H)
Now we deﬁne
 t
0
Φ(s) dW(s) :=
 t
0
Φ(s) ◦J−1 dW(s),
t ∈[0, T].
(2.5.2)
Then the class of all integrable processes is given by
NW =

Φ : ΩT →L0
2 | Φ predictable and P
	  T
0
∥Φ(s)∥2
L0
2 ds < ∞

= 1

as in the case where W(t), t ∈[0, T], is a standard Q-Wiener process in U.
Remark 2.5.3.
1. We note that the stochastic integral deﬁned in (2.5.2) is independent of
the choice of (U1, ⟨, ⟩1) and J. This follows by construction, since by
(2.5.1) for elementary processes (2.5.2) does not depend on J.
2. If Q ∈L(U) is nonnegative, symmetric and with ﬁnite trace the stan-
dard Q-Wiener process can also be considered as a cylindrical Q-Wiener
process by setting J = I : U0 →U where I is the identity map. In this
case both deﬁnitions of the stochastic integral coincide.
Finally, we note that since the stochastic integrals in this chapter all have a
standard Wiener process as integrator, we can drop the predictability
assumption on Φ ∈NW and just assume progressive measurability, i.e. Φ|[0,t]× Ω
is B([0, t])⊗Ft/B(L0
2)-measurable for all t ∈[0, T], at least if (Ω, F, P) is com-
plete (otherwise we consider its completion) (cf. [WW90, Theorem 6.3.1]).
We used the above framework so that it easily extends to more general
Hilbert-space-valued martingales as integrators replacing the standard Wiener
process. Details are left to the reader.

3. Stochastic Diﬀerential
Equations in Finite
Dimensions
This chapter is an extended version of [Kry99, Section 1].
3.1. Main result and a localization lemma
Let (Ω, F, P) be a complete probability space and Ft, t ∈[0, ∞[, a normal
ﬁltration. Let (Wt)t⩾0 be a standard Wiener process on Rd1, d1 ∈N, with
respect to Ft, t ∈[0, ∞[. So, in the terminology of the previous section U :=
Rd1, Q := I. The role of the Hilbert space H there will be taken by Rd, d ∈N.
Let M(d×d1, R) denote the set of all real d×d1-matrices. Let the following
maps σ = σ(t, x, ω), b = b(t, x, ω) be given:
σ :[0, ∞[×Rd × Ω→M(d × d1, R)
b :[0, ∞[×Rd × Ω→Rd
such that both are continuous in x ∈Rd for each ﬁxed t ∈[0, ∞[, w ∈Ω,
and progressively measurable, i.e. for each t their restriction to [0, t] × Ωis
B([0, t])⊗Ft-measurable, for each ﬁxed x ∈Rd. We note that then both σ and
b restricted to [0, t] × Rd × Ωare B([0, t]) ⊗B(Rd) ⊗Ft-measurable for every
t ∈[0, ∞[. In particular, for every x ∈Rd, t ∈[0, ∞[ both are Ft-measurable.
We also assume that the following integrability conditions hold:
 T
0
sup
|x|⩽R
{∥σ(t, x)∥2 + |b(t, x)|} dt < ∞on Ω,
(3.1.1)
for all T, R ∈[0, ∞[. Here | · | denotes the Euclidean distance on Rd and
∥σ∥2 :=
d

i=1
d1

j=1
|σij|2.
(3.1.2)
⟨, ⟩below denotes the Euclidean inner product on Rd.
43

44
3. Stochastic Diﬀerential Equations in Finite Dimensions
Theorem 3.1.1. Let b, σ be as above satisfying (3.1.1). Assume that on Ω
for all t, R ∈[0, ∞[, x, y ∈Rd, |x|, |y| ⩽R
2⟨x −y, b(t, x) −b(t, y)⟩+ ∥σ(t, x) −σ(t, y)∥2
⩽Kt(R)|x −y|2
(local weak monotonicity)
(3.1.3)
and
2⟨x, b(t, x)⟩+ ∥σ(t, x)∥2 ⩽Kt(1)(1 + |x|2),
(weak coercivity)
(3.1.4)
where for R ∈[0, ∞[, Kt(R) is an R+-valued (Ft)-adapted process satisfying
on Ωfor all R, T ∈[0, ∞[
αT (R) :=
 T
0
Kt(R) dt < ∞.
(3.1.5)
Then for any F0-measurable map X0 : Ω→Rd there exists a (up to P-
indistinguish-ability) unique solution to the stochastic diﬀerential equation
dX(t) = b(t, X(t)) dt + σ(t, X(t)) dW(t).
(3.1.6)
Here solution means that (X(t))t⩾0 is a P-a.s. continuous Rd-valued (Ft)-
adapted process such that P-a.s. for all t ∈[0, ∞[
X(t) = X0 +
 t
0
b(s, X(s)) ds +
 t
0
σ(s, X(s)) dW(s).
(3.1.7)
Furthermore, for all t ∈[0, ∞[
E(|X(t)|2e−αt(1)) ⩽E(|X0|2) + 1.
(3.1.8)
Remark 3.1.2. We note that by (3.1.1) the integrals on the right-hand side
of (3.1.7) are well-deﬁned.
For the proof of the above theorem we need two lemmas.
Lemma 3.1.3. Let Y (t), t ∈[0, ∞[, be a continuous, R+-valued, (Ft)-adapted
process on (Ω, F, P) and γ an (Ft)-stopping time, and let ε ∈(0, ∞). Set
τε := γ ∧inf{t ⩾0|Y (t) ⩾ε}
(where as usual we set inf ∅= +∞). Then
P({ sup
t∈[0,γ]
Y (t) ⩾ε}) ⩽1
εE(Y (τε)).

3.1. Main result and a localization lemma
45
Proof. We have
{ sup
t∈[0,γ]
Y (t) ⩾ε} = {Y (τε) ⩾ε}.
Hence the assertion follows by Chebyshev’s inequality.
The following general “localization lemma” will be crucial.
Lemma 3.1.4. Let n ∈N and X(n)(t), t ∈[0, ∞[, be a continuous, Rd-
valued, (Ft)-adapted process on (Ω, F, P) such that X(n)(0) = X0 for some
F0-measurable function X0 : Ω→Rd and
dX(n)(t) = b(t, X(n)(t)+p(n)(t)) dt+σ(t, X(n)(t)+p(n)(t)) dW(t), t ∈[0, ∞[
for some progressively measurable process p(n)(t), t ∈[0, ∞[. For n ∈N and
R ∈[0, ∞[ let τ (n)(R) be (Ft)-stopping times such that
(i)
|X(n)(t)| + |p(n)(t)| ⩽R
if
t ∈]0, τ (n)(R)]
P-a.e.
(ii)
lim
n→∞E
 T ∧τ (n)(R)
0
|p(n)(t)| dt = 0
for all T ∈[0, ∞[.
(iii) There exists a function r : [0, ∞[→[0, ∞[ such that limR→∞r(R) = ∞
and
lim
R→∞lim
n→∞P
	
τ (n)(R) ⩽T,
sup
t∈[0,τ (n)(R)]
|X(n)(t)| ⩽r(R)

= 0 for all T ∈[0, ∞[.
Then for every T ∈[0, ∞[ we have
sup
t∈[0,T ]
|X(n)(t) −X(m)(t)| →0
in probability as n, m →∞.
Proof. By (3.1.1) we may assume that
sup
|x|⩽R
|b(t, x)| ⩽Kt(R)
for all R, t ∈[0, ∞[.
(3.1.9)
(Otherwise, we replace Kt(R) by the maximum of Kt(R) and the integrand
in (3.1.1).) Fix R ∈[0, ∞[ and deﬁne the (Ft)-stopping times
τ(R, u) := inf{t ⩾0|αt(R) > u}, u ∈[0, ∞[.
Since t 	→αt(R) is locally bounded, we have that τ(R, u) ↑∞as u →∞.
In particular, there exists u(R) ∈[0, ∞[ such that
P({τ(R, u(R)) ⩽R}) ⩽1
R.

46
3. Stochastic Diﬀerential Equations in Finite Dimensions
Setting τ(R) := τ(R, u(R)) we have τ(R) →∞in probability as R →∞and
αt∧τ(R)(R) ⩽u(R) for all t, R ∈[0, ∞[.
Furthermore, if we replace τ (n)(R) by τ (n)(R)∧τ(R) for n ∈N, R ∈[0, ∞[,
then clearly assumptions (i) and (ii) above still hold. But
P

τ (n)(R) ∧τ(R) ⩽T,
sup
t∈[0,τ (n)(R)∧τ(R)]
|X(n)(t)| ⩽r(R)
(
⩽P

τ (n)(R) ⩽T,
sup
t∈[0,τ (n)(R)]
|X(n)(t)| ⩽r(R), τ (n)(R) ⩽τ(R)
(
+ P({τ(R) ⩽T, τ (n)(R) > τ(R)})
and limR→∞P({τ(R) ⩽T}) = 0. So, also assumption (iii) holds when τ (n)(R)
is replaced by τ (n)(R)∧τ(R). We may thus assume that τ (n)(R) ⩽τ(R), hence
αt∧τ (n)(R)(R) ⩽u(R) for all t, R ∈[0, ∞[, n ∈N.
(3.1.10)
Fix R ∈[0, ∞[ and deﬁne
λ(n)
t
(R) :=
 t
0
|p(n)(s)| Ks(R) ds,
t ∈]0, ∞[, n ∈N.
(3.1.11)
By (3.1.10) it follows that
lim
n→∞E
	
λ(n)
T ∧τ (n)(R)(R)

= 0 for all R, T ∈[0, ∞[.
(3.1.12)
Indeed, for all m, n ∈N
 T ∧τ (n)(R)
0
|p(n)(t)| Kt(R) dt
⩽m
 T ∧τ (n)(R)
0
|p(n)(t)| dt + R
 T ∧τ(R)
0
1]m,∞[(Kt(R)) Kt(R) dt.
By assumption (ii) we know that as n →∞this converges in L1(Ω, F, P) to
R
 T ∧τ(R)
0
1]m,∞[(Kt(R)) Kt(R) dt,
which in turn is dominated by R αT ∧τ(R) ⩽R u(R) and converges P-a.e.
to zero as m →∞by (3.1.5). So, (3.1.12) follows by Lebesgue’s dominated
convergence theorem. Let n, m ∈N and set
ψt(R) := exp(−2αt(R) −|X0|),
t ∈[0, ∞[.
(3.1.13)

3.1. Main result and a localization lemma
47
Then by Itˆo’s formula we have P-a.e. for all t ∈[0, ∞[
|X(n)(t) −X(m)(t)|2ψt(R)
=
 t
0
ψs(R)
1
2⟨X(n)(s) −X(m)(s), b(s, X(n)(s) + p(n)(s))
−b(s, X(m)(s) + p(m)(s))⟩
+ ∥σ(s, X(n)(s) + p(n)(s)) −σ(s, X(m)(s) + p(m)(s))∥2
−2Ks(R)|X(n)(s) −X(m)(s)|2
2
ds + M (n,m)
R
(t),
(3.1.14)
where M (n,m)
R
(t), t ∈[0, ∞[, is a continuous local (Ft)-martingale with
M (n,m)
R
(0) = 0. Writing
X(n)(s)−X(m)(s) = (X(n)(s)+p(n)(s))−(X(m)(s)+p(m)(s))−p(n)(s)+p(m)(s)
and by the weak monotonicity assumption (3.1.3), for t ∈[0, τ n(R) ∧τ m(R)]
the right-hand side of (3.1.14) is P-a.e. dominated by
 t
0
ψs(R)
1
2⟨p(m)(s) −p(n)(s), b(s, X(n)(s) + p(n)(s))
−b(s, X(m)(s) + p(m)(s))⟩
+ Ks(R)|(X(n)(s) −X(m)(s)) + (p(n)(s) −p(m)(s))|2
−2Ks(R)|X(n)(s) −X(m)(s)|2
2
ds + M (n,m)
R
(t)
⩽2
 t
0
ψs(R) Ks(R)
	
2|p(m)(s) −p(n)(s)| + |p(m)(s) −p(n)(s)|2
ds
+ M (n,m)
R
(t),
where we used (3.1.9) and assumption (i) in the last step. Since ψs(R) ⩽1
for all s ∈[0, ∞[ and since for s ∈]0, τ (n)(R) ∧τ (m)(R)]
|p(m)(s) −p(n)(s)|2 ⩽2R(|p(m)(s)| + |p(n)(s)|)
P-a.e.,
the above implies that for T ∈[0, ∞[ ﬁxed and γ(n,m)(R) := T ∧τ (n)(R) ∧
τ (m)(R) we have P-a.e. for t ∈[0, γ(n,m)(R)]
|X(n)(t) −X(m)(t)|2ψt(R) ⩽4(1 + R)(λ(n)
t
(R) + λ(m)
t
(R)) + M (n,m)
R
(t).
(3.1.15)
Hence for any (Ft)-stopping time τ ⩽γ(n,m)(R) and (Ft)-stopping times
σk ↑∞as k →∞so that M (n,m)
R
(t ∧σk), t ∈[0, ∞[, is a martingale for all

48
3. Stochastic Diﬀerential Equations in Finite Dimensions
k ∈N, we have
E(|X(n)(τ ∧σk) −X(m)(τ ∧σk)|2ψτ∧σk(R))
⩽4(1 + R)E(λ(n)
T ∧τ (n)(R)(R) + λ(m)
T ∧τ (m)(R)(R)).
First letting k →∞and applying Fatou’s lemma, and then using Lemma
3.1.3 we obtain that for every ε ∈]0, ∞[
P({
sup
t∈[0,γ(n,m)(R)]
(|X(n)(t) −X(m)(t)|2ψt(R)) > ε})
⩽4(1 + R)
ε
E(λ(n)
T ∧τ (n)(R)(R) + λ(m)
T ∧τ (m)(R)(R)).
Since [0, ∞[ ∋t 	→ψt(R)(ω) is strictly positive, independent of n, m ∈N, and
continuous, the above inequality and (3.1.12) imply that
sup
t∈[0,γ(n,m)(R)]
|X(n)(t) −X(m)(t)| →0
as n, m →∞
in P-measure. So, to prove the assertion it remains to show that given T ∈
[0, ∞[,
lim
R→∞lim
n→∞P({τ (n)(R) ⩽T}) = 0.
(3.1.16)
We ﬁrst observe that replacing Kt(R) by max(Kt(R), Kt(1)) we may assume
that
Kt(1) ⩽Kt(R) for all t ∈[0, ∞[, R ∈[1, ∞[.
(3.1.17)
Now we proceed similarly as above, but use the assumption of weak coercivity
(3.1.4) instead of the weak monotonicity (3.1.3). Let n ∈N and R ∈[1, ∞[.
Then by Itˆo’s formula P-a.e. for all t ∈[0, ∞[ we have
|X(n)(t)|2ψt(1)
=|X0|2e−|X0| +
 t
0
ψs(1)

2⟨X(n)(s), b(s, X(n)(s) + p(n)(s))⟩
+ ∥σ(s, X(n)(s) + p(n)(s))∥2 −2Ks(1)|X(n)(s)|2
ds + M (n)
R (t),
(3.1.18)
where M (n)
R (t), t
∈
[0, ∞[, is a continuous local (Ft)-martingale with
M (n)
R (0) = 0. By (3.1.4) and (3.1.9) and since ψs(1) ⩽1 for all s ∈[0, ∞[ the
second summand of the right-hand side of
(3.1.18) is P-a.e. for all

3.2. Proof of existence and uniqueness
49
t ∈[0, T ∧τ (n)(R)] dominated by
 t
0
ψs(1)

2⟨−p(n)(s), b(s, X(n)(s) + p(n)(s))⟩
+ Ks(1)|X(n)(s) + p(n)(s)|2 + Ks(1) −2Ks(1) |X(n)(s)|2
ds
⩽2
 t
0
Ks(R) |p(n)(s)|(1 + |p(n)(s)|) ds +
 t
0
e−2αs(1)Ks(1) ds
⩽2(1 + R)λ(n)
t
(R) +
 αt(1)
0
e−2s ds,
(3.1.19)
where we used (3.1.9), (3.1.17) and assumption (i).
Again localizing M (n)
R (t), t ∈[0, ∞[, from (3.1.18) and (3.1.19) we deduce
that for every (Ft)-stopping time τ ⩽T ∧τ (n)(R)
E(|X(n)(τ)|2ψτ(1)) ⩽E(|X0|2e−|X0|) + 1
2 + 2(1 + R)E(λ(n)
T ∧τ (n)(R)(R)).
Hence by Lemma 3.1.3 and (3.1.12) we obtain that for every c ∈]0, ∞[
lim
c→∞
sup
R∈[0,∞[
lim
n→∞P({
sup
t∈[0,T ∧τ (n)(R)]
(|X(n)(t)|2ψt(1)) ⩾c}) = 0.
Since [0, ∞[ ∋t 	→ψt(1) is strictly positive, independent of n ∈N and contin-
uous, and since r(R) →∞as R →∞, we conclude that
lim
R→∞lim
n→∞P({
sup
t∈[0,τ (n)(R)]
|X(n)(t)| ⩾r(R), τ (n)(R) ⩽T})
⩽lim
R→∞
sup
˜
R∈[0,∞[
lim
n→∞P({
sup
t∈[0,T ∧τ (n)( ˜
R)]
|X(n)(t)| ⩾r(R)}) = 0.
Hence (3.1.16) follows from assumption (iii).
Remark 3.1.5. In our application of Lemma 3.1.4 below, assumption (iii)
will be fulﬁlled, since the event under P will be empty for all n ∈N, R ∈[0, ∞[.
For a case where assumption (iii) is more diﬃcult to check, we refer to [Kry99,
Section 1].
3.2. Proof of existence and uniqueness
Proof of Theorem 3.1.1. The proof is based on Euler’s method. Fix n ∈N
and deﬁne the processes X(n)(t), t ∈[0, ∞[, iteratively by setting
X(n)(0) := X0

50
3. Stochastic Diﬀerential Equations in Finite Dimensions
and for k ∈N ∪{0} and t ∈
 k
n, k+1
n

by
X(n)(t)
=X(n)
k
n

+
 t
k
n
b

s, X(n)
k
n

ds +
 t
k
n
σ

s, X(n)
k
n

dW(s).
This is equivalent to
X(n)(t) = X0 +
 t
0
b(s, X(n)(κ(n, s))) ds
+
 t
0
σ(s, X(n)(κ(n, s))) dW(s), t ∈[0, ∞[,
(3.2.1)
where κ(n, t) := [tn]/n, and also to
X(n)(t) = X0 +
 t
0
b(s, X(n)(s) + p(n)(s)) ds
+
 t
0
σ(s, X(n)(s) + p(n)(s)) dW(s), t ∈[0, ∞[,
where
p(n)(t) :=X(n)(κ(n, t)) −X(n)(t)
= −
 t
κ(n,t)
b(s, X(n)(κ(n, s))) ds
−
 t
κ(n,t)
σ(s, X(n)(κ(n, s))) dW(s), t ∈[0, ∞[.
Now ﬁx R ∈[0, ∞[ and deﬁne
τ (n)(R) := inf

t ⩾0
|X(n)(t)| > R
3

and
r(R) := R
4 .
Then clearly,
|p(n)(t)| ⩽2R
3
and |X(n)(t)| ⩽R
3 if t ∈]0, τ (n)(R)].
In particular, condition (i) in Lemma 3.1.4 holds and the event in Lemma
3.1.4(iii) is empty for all n ∈N, R ∈[0, ∞[, so this condition is satisﬁed. Let

3.2. Proof of existence and uniqueness
51
ei, 1 ⩽i ⩽d, be the canonical basis of Rd and T ∈[0, ∞[. Since for t ∈[0, T]
−⟨ei, p(n)(t)⟩
=
 t
κ(n,t)
⟨ei, b(s, X(n)(κ(n, s)))⟩ds +
 t
κ(n,t)
⟨ei, σ(s, X(n)(κ(n, s))) dW(s)⟩,
it follows that for ε ∈]0, ∞[ and 1 ⩽i ⩽d, t ∈[0, ∞[
P({|⟨ei, p(n)(t)⟩| ⩾2ε, t ⩽τ (n)(R)})
⩽P
 t
κ(n,t)
sup
|x|⩽R
|b(s, x)| ds ⩾ε
(
+ P

sup
˜t∈[0,t]

 ˜t∧τ (n)(R)
0
1[κ(n,t),T ](s)
⟨ei, σ(s, X(n)(κ(n, s))) dW(s)⟩
 ⩾ε

and by Corollary D.0.2 the second summand is bounded by
3δ
ε + P
 t
κ(n,t)
sup
|x|⩽R
∥σ(t, x)∥2 ds > δ2
(
.
Altogether, letting ﬁrst n →∞and using (3.1.1), and then letting δ →0 we
obtain that for all t ∈[0, ∞[
1[0.τn(R)](t) p(n)(t) →0 as n →∞
in P-measure. Since
1[0.τn(R)](t)
pn)(t)
 ⩽2R
3 , t ∈[0, ∞[,
it follows by Lebesgue’s dominated convergence theorem and Fubini’s theorem
that condition (ii) in Lemma 3.1.4 is also fulﬁlled. Now Lemma 3.1.4 and the
fact that the space of continuous processes is complete with respect to locally
(in t ∈[0, ∞[) uniform convergence in probability imply that there exists a
continuous, (Ft)-adapted, Rd-valued process X(t), t ∈[0, ∞[, such that for all
T ∈[0, ∞[
sup
t∈[0,T ]
|X(n)(t) −X(t)| →0 in P-measure as n →∞.
(3.2.2)
To prove that X satisﬁes (3.1.6) we are going to take the limit in (3.2.1).
To this end, ﬁx T ∈[0, ∞[ and t ∈[0, T]. By (3.2.2) and because of the path

52
3. Stochastic Diﬀerential Equations in Finite Dimensions
continuity we only have to show that the right-hand side of (3.2.1) converges
in P-measure to
X0 +
 t
0
b(s, X(s)) ds +
 t
0
σ(s, X(s)) dW(s).
Since the convergence in (3.2.2) is uniform on [0, T], by equicontinuity we have
that also
sup
t∈[0,T ]
|X(n)(κ(n, t)) −X(t)| →0 in P-measure as n →∞.
Hence for Y (n)(t) := X(n)(κ(n, t)) and a subsequence (nk)k∈N
sup
t∈[0,T ]
|Y (nk)(t) −X(t)| →0 P-a.e. as k →∞.
In particular, for S(t) := supk∈N |Y (nk)(t)|
sup
t∈[0,T ]
S(t) < ∞
P-a.e..
(3.2.3)
For R ∈[0, ∞[ deﬁne the (Ft)-stopping time
τ(R) := inf{t ∈[0, T]|S(t) > R} ∧T.
By the continuity of b in x ∈Rd and by (3.1.1)
lim
k→∞
 t
0
b(s, X(nk)(κ(nk, s))) ds =
 t
0
b(s, X(s)) ds
P-a.e. on {t ⩽τ(R)}.
(3.2.4)
To handle the stochastic integrals we need another sequence of stopping times.
For R, N ∈[0, ∞[ deﬁne the (Ft)-stopping time
τN(R) := inf{t ∈[0, T]|
 t
0
sup
|x|⩽R
∥σ(s, x)∥2 ds > N} ∧τ(R).
Then by the continuity of σ in x ∈Rd, (3.1.1), and Lebesgue’s dominated
convergence theorem
lim
k→∞E
 τN(R)
0
∥σ(s, X(nk)(κ(nk, s))) −σ(s, X(s))∥2 ds

= 0,
hence
 t
0
σ(s, X(nk)(κ(nk, s))) dW(s) →
 t
0
σ(s, X(s)) dW(s)
(3.2.5)

3.2. Proof of existence and uniqueness
53
in P-measure on {t ⩽τN(R)} as k →∞. By (3.1.1) for every ω ∈Ωthere
exists N(ω) ∈[0, ∞[ such that τN(R) = τ(R) for all N ⩾N(ω), so

N∈N
{t ⩽τN(R)} = {t ⩽τ(R)}.
Therefore, (3.2.5) holds on {t ⩽τ(R)}. But by (3.2.3) for P-a.e. ω ∈Ωthere
exists R(ω) ∈[0, ∞[ such that τ(R) = T for all R ⩾R(ω). So, as above we
conclude that (3.2.4) and (3.2.5) hold P-a.e. on Ω. This completes the proof
for existence.
The uniqueness is a special case of the next proposition. So, let us prove
the ﬁnal statement. We have by Itˆo’s formula for our solution X that P-a.e.
for all t ∈[0, ∞[
|X(t)|2e−αt(1) = |X0|2 +
 t
0
e−αs(1)
2⟨X(s), b(s, X(s))⟩+ ∥σ(s, X(s))∥2
−Ks(1)|X(s)|2
ds + M(t),
where M(t), t ∈[0, ∞[, is a continuous local martingale with M(0) = 0. By
the weak coercivity assumption (3.1.4) the latter is dominated by
|X0|2 +
 αt(1)
0
e−s ds + M(t).
So, again by localizing M(t), t ∈[0, ∞[, and Fatou’s lemma we get
E(|X(t)|2e−αt(1)) ⩽E(|X0|2) + 1, t ∈[0, ∞[.
Proposition 3.2.1. Let the assumptions of Theorem 3.1.1 apart from (3.1.4)
be satisﬁed. Let X0, X(n)
0
: Ω→Rd, n ∈N, be F0-measurable such that
P −lim
n→∞X(n)
0
= X0.
Let T ∈[0, ∞[ and assume that X(t), X(n)(t), t ∈[0, T], n ∈N, be solutions
of (3.1.6) (up to time T) such that X(0) = X0 and X(n)(0) = X(n)
0
P-a.e.
for all n ∈N. Then
P −lim
n→∞sup
t∈[0,T ]
|X(n)(t) −X(t)| = 0.
(3.2.6)
Proof. By the characterization of convergence in P-measure in terms of P-a.e.
convergent subsequences (cf. e.g. [Bau01]), we may assume that X(n)
0
→X0
as n →∞P-a.e..

54
3. Stochastic Diﬀerential Equations in Finite Dimensions
Fix R ∈[0, ∞[ and deﬁne
φt(R) := exp(−αt(R) −sup
n |X(n)
0
|), t ∈[0, ∞[.
We note that since |X0| < ∞, we have φt(R) > 0 P-a.e. for all t ∈[0, ∞[.
Deﬁne
γ(n)(R) := inf{t ⩾0||X(n)(t)| + |X(t)| > R} ∧T.
Analogously to deriving (3.1.14) in the proof of Lemma 3.1.4 using the weak
monotonicity assumption (3.1.3), we obtain that P-a.e. for all t ∈[0, T] and
all n ∈N
|X(n)(t ∧γ(n)(R)) −X(t ∧γ(n)(R))|2φt∧γ(n)(R)(R)
⩽|X(n)
0
−X0|2e−supn |X(n)
0
| + m(n)
R (t),
where m(n)
R (t), t ∈[0, T], are continuous local (Ft)-martingales such that
m(n)
R (0) = 0. Hence localizing m(n)
R (t), t ∈[0, T], for any (Ft)-stopping time
τ ⩽γ(n)(R) we obtain that
E(|X(n)(τ) −X(τ)|2φτ(R)) ⩽E(|X(n)
0
−X0|2e−supn |X(n)
0
|).
(3.2.7)
Since the right-hand side of (3.2.7) converges to zero, by Lemma 3.1.3 we
conclude that
P −lim
n→∞sup
t∈[0,T ]
	
|Xn)(t ∧γ(n)(R)) −X(t ∧γ(n)(R))|2φt∧γ(n)(R)(R)

= 0.
(3.2.8)
Since P-a.e. the function [0, ∞[∋t 	→φt(R) is continuous and strictly positive,
(3.2.8) implies
P −lim
n→∞sup
t∈[0,T ]
|X(n)(t ∧γ(n)(R)) −X(t ∧γ(n)(R))| = 0.
(3.2.9)
But
P({γ(n)(R) < T})
⩽P({ sup
t∈[0,T ]
|X(n)(t ∧γ(n)(R))| + |X(t ∧γ(n)(R))|) ⩾R})
⩽P({ sup
t∈[0,T ]
|X(n)(t ∧γ(n)(R)) −X(t ∧γ(n)(R))|) ⩾1})
+ P({2 sup
t∈[0,T ]
|X(t)| ⩾R −1}).
This together with (3.2.9) implies that
lim
R→∞lim
n→∞P({γ(n)(R) < T}) = 0.
(3.2.10)
(3.2.9) and (3.2.10) imply (3.2.6).

4. A Class of Stochastic
Diﬀerential Equations in
Banach Spaces and
Applications to Stochastic
Partial Diﬀerential Equations
In this chapter we will present one speciﬁc method to solve stochastic dif-
ferential equations in inﬁnite-dimensional spaces, known as the variational
approach. The main criterion for this approach to work is that the coeﬃ-
cients satisfy certain monotonicity assumptions. As the main references for
Subsection 4.2 we mention [RRW06] and [KR79], but also one should check
the references therein.
4.1. Gelfand triples, conditions on the
coeﬃcients and examples
Let H be a separable Hilbert space with inner product ⟨, ⟩H and H∗its
dual. Let V be a Banach space, such that V ⊂H continuously and densely.
Then for its dual space V ∗it follows that H∗⊂V ∗continuously and densely.
Identifying H and H∗via the Riesz isomorphism we have that
V ⊂H ⊂V ∗
(4.1.1)
continuously and densely and if V ∗⟨, ⟩V denotes the dualization between V ∗
and V (i.e. V ∗⟨z, v⟩V := z(v) for z ∈V ∗, v ∈V ), it follows that
V ∗⟨z, v⟩V = ⟨z, v⟩H
for all z ∈H, v ∈V.
(4.1.2)
(V, H, V ∗) is called a Gelfand triple. Note that since H ⊂V ∗continuously and
densely, also V ∗is separable, hence so is V . Furthermore, B(V ) is generated
by V ∗and B(H) by H∗. We also have by Kuratowski’s theorem that V ∈
B(H), H ∈B(V ∗) and B(V ) = B(H) ∩V, B(H) = B(V ∗) ∩H.
Below we want to study stochastic diﬀerential equations on H of type
dX(t) = A(t, X(t))dt + B(t, X(t)) dW(t)
(4.1.3)
55

56
4. A Class of Stochastic Diﬀerential Equations
with W(t), t ∈[0, T] a cylindrical Q-Wiener process with Q = I on another
separable Hilbert space (U, ⟨, ⟩U) and with B taking values in L2(U, H) as
in Chapter 2, but with A taking values in the larger space V ∗.
The solution X will, however, take values in H again. In this section we give
precise conditions on A and B.
Let T ∈[0, ∞[ be ﬁxed and let (Ω, F, P) be a complete probability space with
normal ﬁltration Ft, t ∈[0, ∞[. Let
A : [0, T] × V × Ω→V ∗, B : [0, T] × V × Ω→L2(U, H)
be progressively measurable, i.e. for every t ∈[0, T], these maps restricted to
[0, t]×V ×Ωare B([0, t])⊗B(V )⊗Ft-measurable. As usual by writing A(t, v)
we mean the map ω 	→A(t, v, ω). Analogously for B(t, v). We impose the
following conditions on A and B:
(H1) (Hemicontinuity) For all u, v, x ∈V, ω ∈Ωand t ∈[0, T] the map
R ∋λ 	→V ∗⟨A(t, u + λv, ω), x⟩V
is continuous.
(H2) (Weak monotonicity) There exists c ∈R such that for all u, v ∈V
2 V ∗⟨A(·, u) −A(·, v), u −v⟩V + ∥B(·, u) −B(·, v)∥2
L2(U,H)
⩽c∥u −v∥2
H on [0, T] × Ω.
(H3) (Coercivity) There exist α ∈]1, ∞[, c1 ∈R, c2 ∈]0, ∞[ and an (Ft)-
adapted process f ∈L1([0, T] × Ω, dt ⊗P) such that for all v ∈V, t ∈
[0, T]
2 V ∗⟨A(t, v), v⟩V +∥B(t, v)∥2
L2(U,H) ⩽c1∥v∥2
H −c2∥v∥α
V + f(t)
on Ω.
(H4) (Boundedness) There exist c3 ∈[0, ∞[ and an (Ft)-adapted process
g ∈L
α
α−1 ([0, T] × Ω, dt ⊗P) such that for all v ∈V, t ∈[0, T]
∥A(t, v)∥V ∗⩽g(t) + c3∥v∥α−1
V
on Ω,
where α is as in (H3).
Remark 4.1.1.
1. By (H3) and (H4) it follows that for all v ∈V, t ∈
[0, T]
∥B(t, v)∥2
L2(U,H) ⩽c1∥v∥2
H + f(t) + 2∥v∥V g(t) + 2c3∥v∥α
V
on Ω.
2. Let ω ∈Ω, t ∈[0, T]. (H1) and (H2) imply that A(t, ·, ω) is demicontin-
uous, i.e.
un →u as n →∞(strongly) in V

4.1. Gelfand triples, conditions on the coeﬃcients and examples
57
implies
A(t, un, ω) →A(t, u, ω) as n →∞weakly in V ∗
(cf. [Zei90, Proposition 26.4])
In particular if H = Rd, d ∈N, hence V = V ∗= Rd, then (H1) and
(H2) imply that u 	→A(t, u, ω) is continuous from Rd to Rd.
Proof. Fix (t, ω) ∈[0, T] × Ωand set for u ∈V
A(u) := A(t, u, ω) −cu.
The proof will be done in four steps.
Claim 1: A is locally bounded, i.e. for all u ∈V there exists a neighborhood
U(u) such that A(U(u)) is a bounded subset of V ∗.
Proof of Claim 1. Consider ﬁrst u := 0. Suppose A(U(0)) is unbounded for
all neighborhoods U(0) of 0. Then there exist un ∈V such that
un →0 and ∥A(un)∥V ∗→∞as n →∞.
Set
an := (1 + ∥A(un)∥V ∗∥un∥V )−1.
Then by (H2) for all v ∈V
an V ∗⟨A(un), un −(±v)⟩V −an V ∗⟨A(±v), un −(±v)⟩V ⩽0,
hence
∓an V ∗⟨A(un), v⟩V ⩽−an V ∗⟨A(un), un⟩V +an V ∗⟨A(±v), un ∓v⟩V
⩽an∥A(un)∥V ∗∥un∥V + ∥A(±v)∥V ∗∥un ∓v∥V
⩽1 + ∥A(±v)∥V ∗

sup
n ∥un∥V + ∥v∥V

.
Consequently,
sup
n | V ∗⟨anA(un), v⟩V | < ∞for all v ∈V.
Therefore, by the Banach–Steinhaus theorem
N := sup
n ∥anA(un)∥V ∗< ∞,
and thus for n0 ∈N so large that ∥un∥⩽
1
2N for all n ⩾n0 we obtain
∥A(un)∥V ∗⩽a−1
n N ⩽N + 1
2∥A(un)∥V ∗,

58
4. A Class of Stochastic Diﬀerential Equations
i.e.
∥A(un)∥V ∗⩽2N for all n ⩾n0,
which is a contradiction. So, A(U(0)) is bounded for some neighborhood U(0)
of 0.
For arbitrary u ∈V we apply the above argument to the operator
Au(v) := A(u + v), v ∈V
which obviously is also hemicontinuous and weakly monotone. So, Claim 1 is
proved.
Claim 2: Let u ∈V, b ∈V ∗such that
V ∗⟨b −A(v), u −v⟩V ⩽0 for all v ∈V.
Then A(u) = b.
Proof of Claim 2. Let w ∈V, t ∈]0, ∞[ and set v := u −tw. Then
V ∗⟨b −A(u −tw), tw⟩V = V ∗⟨b −A(v), u −v⟩V ⩽0.
Dividing ﬁrst by t and then letting t →0, by (H1) we obtain
V ∗⟨b −A(u), w⟩V ⩽0 for all w ∈V.
So, replacing w by −w, w ∈V , we get
V ∗⟨b −A(u), w⟩V = 0 for all w ∈V,
hence A(u) = b.
Claim 3: (“monotonicity trick”). Let un, u ∈V, n ∈N, and b ∈V ∗such that
un →u as n →∞weakly in V,
A(un) →b as n →∞weakly in V ∗
and
lim V ∗⟨A(un), un⟩V ⩾V ∗⟨b, u⟩V .
Then A(u) = b.
Proof of Claim 3. We have for all v ∈V
V ∗⟨A(un), un⟩V −V ∗⟨A(v), un⟩V −V ∗⟨A(un) −A(v), v⟩V
= V ∗⟨A(un) −A(v), un −v⟩V ⩽0.
Letting n →∞we obtain
V ∗⟨b, u⟩V −V ∗⟨A(v), u⟩V −V ∗⟨b −A(v), v⟩V ⩽0,
so
V ∗⟨b −A(v), u −v⟩V ⩽0 for all v ∈V.
Hence Claim 2 implies that A(u) = b.

4.1. Gelfand triples, conditions on the coeﬃcients and examples
59
Claim 4: Let un, u ∈V, n ∈N, such that
un →u as n →∞(strongly) in V.
Then
A(un) →A(u) as n →∞weakly in V ∗.
Proof of Claim 4. Since {un|n ∈N} is bounded, by Claim 1 also {A(un)|n ∈N}
is bounded in V ∗. Since bounded sets in V ∗are weakly compact by the
Banach-Alaoglu theorem, there exists a subsequence (nk)k∈N and b ∈V ∗
such that A(unk) →b as k →∞weakly in V ∗. Since unk →u strongly in V
as k →∞, we get
lim
k→∞V ∗⟨A(unk), unk⟩V = V ∗⟨b, u⟩V .
Therefore, all conditions in Claim 3 are fulﬁlled and we can conclude that
A(u) = b. So, for all such subsequences their weak limit is A(u), hence
A(un) →A(u) as n →∞weakly in V ∗.
Let us now discuss the above conditions. We shall solely concentrate on A
and take B ≡0. The latter we do because of the following:
Exercise 4.1.2.
1. Suppose A, B satisfy (H2), (H3) above and ˜A is another map as A satis-
fying (H2), (H3). Then A + ˜A, B satisfy (H2),(H3). Likewise, if A and
˜A both satisfy (H1), (H4) then so does A + ˜A.
2. If A satisﬁes (H2), (H3) (with B ≡0) and for all t ∈[0, T], ω ∈Ω, the
map u 	→B(t, u, ω) is Lipschitz with Lipschitz constant independent of
t ∈[0, T], ω ∈Ωthen A, B satisfy (H2), (H3).
Below, we only look at A independent of t ∈[0, T], ω ∈Ω. From here
examples for A dependent on (t, ω) are then immediate.
Example 4.1.3. V = H = V ∗(which includes the case H = Rd)
Clearly, since for all v ∈V
2 V ∗⟨A(v), v⟩V ⩽2 V ∗⟨A(v) −A(0), v⟩V +∥A(0)∥2
V ∗+ ∥v∥2
V ,
in the present case where V = H = V ∗, (H2) implies (H3) with c1 > c2
and α := 2. Furthermore, obviously, if A is Lipschitz in u then (H1)–(H4)
are immediately satisﬁed. But for (H1)–(H3) to hold, purely local conditions
(with respect to u) on A can be suﬃcient, as the following proposition shows.
Proposition 4.1.4. Suppose A : H →H is Fr´echet diﬀerentiable such that
for some c ∈[0, ∞[ the operator DA(x) −cI (∈L(H)) is negative deﬁnite for
all x ∈H. Then A satisﬁes (H1)–(H3) (with B ≡0).

60
4. A Class of Stochastic Diﬀerential Equations
Proof. Since A is Fr´echet diﬀerentiable it is continuous, so, in particular, (H1)
holds. Furthermore, for x, y ∈H we have
A(x) −A(y) =
 1
0
d
dsA(y + s(x −y))ds
=
 1
0
DA(y + s(x −y))(x −y)ds.
Hence by assumption
⟨A(x) −A(y), x −y⟩H =
 1
0
⟨DA(y + s(x −y))(x −y), x −y⟩Hds
⩽c
 1
0
⟨x −y, x −y⟩Hds
= c∥x −y∥2
H,
and so (H2) holds and hence (H3), as shown above.
We again note that Proposition 4.1.4 shows that purely local conditions on
A can already imply (H1)–(H3), if (V = H = V ∗and) α = 2. However, the
global condition (H4) then requires that A is of at most linear growth since
α−1 = 1 if α = 2. We also note that for H = R1 the conditions in Proposition
4.1.4 just mean that A is diﬀerentiable and decreasing.
If H is a space of functions, a possible and easy choice for A would be e.g.
Au = −u3. But then we cannot choose H = L2 because A would not leave L2
invariant. This is one motivation to look at triples V ⊂H ⊂V ∗because then
we can take V = Lp and H = L2 and deﬁne A from V to V ∗= Lp/(p−1). Let
us look at this case more precisely.
Example 4.1.5 (Lp ⊂L2 ⊂Lp/(p−1) and A(u) := −u|u|p−2).
Let p ∈[2, ∞[, Λ ⊂Rd, Λ open. Let
V := Lp(Λ) := Lp(Λ, dξ),
equipped with its usual norm ∥·∥p, and
H := L2(Λ) := L2(Λ, dξ),
where dξ denotes Lebesgue measure on Λ. Then
V ∗= Lp/(p−1)(Λ).
If p > 2 we assume that
|Λ| :=

Rd IΛ(ξ) dξ < ∞.
(4.1.4)

4.1. Gelfand triples, conditions on the coeﬃcients and examples
61
Then
V ⊂H ⊂V ∗,
or concretely
Lp(Λ) ⊂L2(Λ) ⊂Lp/(p−1)(Λ)
continuously and densely. Recall that since p > 1, Lp(Λ) is reﬂexive.
Deﬁne A : V →V ∗by
Au := −u|u|p−2, u ∈V = Lp(Λ).
Indeed, A takes values in V ∗= Lp/(p−1)(Λ), since

|Au(ξ)|p/(p−1) dξ =

|u(ξ)|p dξ < ∞
for all u ∈Lp(Λ).
Claim: A satisﬁes (H1)–(H4).
Proof. Let u, v, x ∈V . Then for λ ∈R
V ∗⟨A(u + λv) −A(u), x⟩V
=

(u(ξ)|u(ξ)|p−2 −(u(ξ) + λv(ξ))|u(ξ) + λv(ξ)|p−2)x(ξ) dξ
⩽
u|u|p−2 −(u + λv)|u + λv|p−2
V ∗∥x∥V
which converges to zero as λ →0 by Lebesgue’s dominated convergence the-
orem. So, (H1) holds.
Furthermore,
V ∗⟨A(u) −A(v), u −v⟩V
=

(v(ξ)|v(ξ)|p−2 −u(ξ)|u(ξ)|p−2)(u(ξ) −v(ξ)) dξ ⩽0,
since the map s 	→s|s|p−2 is increasing on R. Thus (H2) holds, with c := 0.
We also have that
V ∗⟨A(v), v⟩V = −

|v(ξ)|p dξ = −∥v∥p
V ,
so (H3) holds with α := p. In addition,
∥A(v)∥V ∗=

|v(ξ)|p dξ
 p−1
p
= ∥v∥p−1
V
so (H4) holds with α := p as required.

62
4. A Class of Stochastic Diﬀerential Equations
Remark 4.1.6. In the example above we may take A : V := Lp(Λ) →
L
p
p−1 (Λ) = V ∗deﬁned by
A(v) := −Ψ(v), v ∈Lp(Λ),
where Ψ : R →R is a ﬁxed function satisfying properties (Ψ1)−(Ψ4) speciﬁed
in Example 4.1.11 below.
Now we turn to cases where A is given by a (possibly nonlinear) partial
diﬀerential operator. We shall start with the linear case; more concretely, A
will be given by the classical Laplace operator
∆=
d

i=1
∂2
∂ξ2
i
with initial domain given by C∞
0 (Λ). We want to take A to be an extension of
∆to a properly chosen Banach space V so that A : V →V ∗is (deﬁned on all
of V and) continuous with respect to ∥·∥V and ∥·∥V ∗. The right choice for V
is the classical Sobolev space H1,p
0 (Λ) for p ∈[2, ∞[ with Dirichlet boundary
conditions. So, as a preparation we need to introduce (ﬁrst-order) Sobolev
spaces.
Again let Λ ⊂Rd, Λ open, and let C∞
0 (Λ) denote the set of all inﬁnitely
diﬀerentiable real-valued functions on Λ with compact support. Let p ∈[1, ∞[
and for u ∈C∞
0 (Λ) deﬁne
∥u∥1,p :=

(|u(ξ)|p + |∇u(ξ)|p) dξ
1/p
.
(4.1.5)
Then deﬁne
H1,p
0 (Λ) := completion of C∞
0 (Λ) with respect to ∥·∥1,p.
(4.1.6)
At this stage H1,p
0 (Λ), called the Sobolev space of order 1 in Lp(Λ) with
Dirichlet boundary conditions, just consists of abstract objects, namly equiv-
alence classes of ∥·∥1,p-Cauchy sequences. The main point is to show that
H1,p
0 (Λ) ⊂Lp(Λ),
(4.1.7)
i.e. that the unique continuous extension
¯i : H1,p
0 (Λ) →Lp(Λ)
of the embedding
i : C∞
0 (Λ) →Lp(Λ)
is one-to-one. To this end it suﬃces (in fact it is equivalent) to show that if
un ∈C∞
0 (Λ), n ∈N, such that
un →0
in Lp(Λ)

4.1. Gelfand triples, conditions on the coeﬃcients and examples
63
and

|∇(un −um)(ξ)|p dξ →0 as n, m →∞,
then

|∇(un(ξ)|p dξ →0 as n →∞.
(4.1.8)
But by the completeness of Lp(Λ; Rd) there exists
F = (F1, . . . , Fd) ∈Lp(Λ; Rd)
such that ∇un →F as n →∞in Lp(Λ; Rd). Let v ∈C∞
0 (Λ). Then for
1 ⩽i ⩽d, integrating by parts we obtain that

v(ξ)Fi(ξ) dξ = lim
n→∞

v(ξ) ∂
∂ξi
un(ξ) dξ
= −lim
n→∞

∂
∂ξi
v(ξ)un(ξ) dξ
= 0.
Hence Fi = 0 dξ-a.e. for all 1 ⩽i ⩽d, so (4.1.8) holds.
Consider the operator
∇: C∞
0 (Λ) ⊂Lp(Λ) →Lp(Λ; Rd).
By what we have shown above, we can extend ∇to all of H1,p
0 (Λ) as follows.
Let u ∈H1,p
0 (Λ) and let un ∈C∞
0 (Λ) such that limn→∞∥u −un∥1,p = 0. In
particular, (∇un)n∈N is a Cauchy sequence in Lp(Λ; Rd), hence has a limit
there. So, deﬁne
∇u := lim
n→∞∇un
in Lp(Λ; Rd).
(4.1.9)
By what we have shown above this limit only depends on u and not on the
chosen sequence. We recall the fact that H1,p
0 (Λ) is reﬂexive for all p ∈]1, ∞[
(cf. [Zei90]).
Example 4.1.7 (H1,2
0
⊂L2 ⊂(H1,2
0 )∗, A = ∆).
Though later we shall see that to have (H3) we have to take p = 2, we shall
ﬁrst consider for p ∈[2, ∞[ and deﬁne
V := H1,p
0 (Λ), H := L2(Λ),
so
V ∗:= H1,p
0 (Λ)∗.
Again we assume (4.1.4) to hold if p > 2. Since then V ⊂Lp(Λ) ⊂H,
continuously and densely, identifying H with its dual we obtain the continuous
and dense embeddings
V ⊂H ⊂V ∗

64
4. A Class of Stochastic Diﬀerential Equations
or concretely
H1,p
0 (Λ) ⊂L2(Λ) ⊂H1,p
0 (Λ)∗.
(4.1.10)
Now we are going to extend ∆with initial domain C∞
0 (Λ) to a bounded linear
operator A : V →V ∗. First of all we can consider ∆as an operator taking
values in V ∗since
∆: C∞
0 (Λ) →C∞
0 (Λ) ⊂L2(Λ) ⊂V ∗.
Furthermore, for u, v ∈C∞
0 (Λ) again integrating by parts we obtain
| V ∗⟨∆u, v⟩V | = |⟨∆u, v⟩H|
=
−

⟨∇u(ξ), ∇v(ξ)⟩dξ

⩽

|∇u(ξ)|
p
p−1 dξ
 p−1
p 
|∇v(ξ)|p dξ
 1
p
⩽

|∇u(ξ)|
p
p−1 dξ
 p−1
p
∥v∥1,p.
Hence for all u ∈C∞
0 (Λ)
∥∆u∥V ∗⩽∥|∇u|∥
p
p−1 .
(4.1.11)
So, by (4.1.4) and since
p
p−1 ⩽2 ⩽p, we get by H¨older’s inequality
∥∆u∥V ∗⩽|Λ|
p−2
p ∥u∥1,p
for all u ∈C∞
0 (Λ),
(4.1.12)
where for p = 2 the factor on the right is just equal to 1.
So, ∆with domain C∞
0 (Λ) extends (uniquely) to a bounded linear operator
A : V →V ∗(with domain all of V ), also sometimes denoted by ∆.
Now let us check (H1)–(H4) for A.
Claim:
A(= ∆) : H1,p
0 (Λ) →
	
H1,p
0 (Λ)

∗
satisﬁes (H1),(H2),(H4) and provided p = 2, also (H3).
Proof. Since A : V →V ∗is linear, (H1) is obviously satisﬁed. Further, if
u, v ∈V then there exists un, vn ∈C∞
0 (Λ), n ∈N, such that un →u, vn →v
as n →∞in V . Hence integrating by parts we get
V ∗⟨A(u) −A(v), u −v⟩V = lim
n→∞V ∗⟨∆un −∆vn, un −vn⟩V
= lim
n→∞⟨∆(un −vn), un −vn⟩H
= lim
n→∞−

|∇(un −vn)(ξ)|2 dξ ⩽0.

4.1. Gelfand triples, conditions on the coeﬃcients and examples
65
So (H2) is satisﬁed. Furthermore,
2 V ∗⟨A(v), v⟩V = lim
n→∞2⟨∆vn, vn⟩H
= −lim
n→∞2

|∇vn(ξ)|2 dξ
= −2

|∇v(ξ)|2 dξ
= 2

∥v∥2
H −∥v∥2
1,2

.
So (H3) is satisﬁed if p = 2 with α = 2. Furthermore, (H4), with α = 2 is
clear by (4.1.12).
Remark 4.1.8. The corresponding SDE (4.1.3) then reads
dX(t) = ∆X(t) dt + B(t, X(t)) dW(t).
If B ≡0, this is just the classical heat equation. If B ̸≡0, but constant, the
solution is an Ornstein–Uhlenbeck process on H.
Example 4.1.9 (H1,p
0
⊂L2 ⊂(H1,p
0 )∗, A = p-Laplacian).
Again we take p ∈[2, ∞[, Λ ∈Rd, Λ open and bounded, and V := H1,p
0 (Λ),
H := L2(Λ), so V ∗= (H1,p
0 (Λ))∗. Deﬁne A : H1,p
0 (Λ) →H1,p
0 (Λ)∗by
A(u) := div(|∇u|p−2∇u), u ∈H1,p
0 (Λ);
more precisely, given u ∈H1,p
0 (Λ) for all v ∈H1,p
0 (Λ)
V ∗⟨A(u), v⟩V := −

|∇u(ξ)|p−2⟨∇u(ξ), ∇v(ξ)⟩dξ
for all v ∈H1,p
0 (Λ).
(4.1.13)
A is called the p-Laplacian, also denoted by ∆p. Note that ∆2 = ∆. To
show that A : V →V ∗is well-deﬁned we have to show that the right-hand
side of (4.1.13) deﬁnes a linear functional in v ∈V which is continuous with
respect to ∥·∥V = ∥·∥1,p. First we recall that by (4.1.9) ∇u ∈Lp(Λ; Rd) for
all u ∈H1,p
0 (Λ). Hence by H¨older’s inequality

|∇u(ξ)|p−1|∇v(ξ)| dξ ⩽

|∇u(ξ)|p dξ
 p−1
p 
|∇v(ξ)|p dξ
 1
p
⩽∥u∥p−1
1,p ∥v∥1,p.
Since this dominates the right-hand side of (4.1.13) for all u ∈H1,p
0 (Λ) we
have that A(u) is a well-deﬁned element of (H1,p
0 (Λ))∗and that
∥A(u)∥V ∗⩽∥u∥p−1
V
.
(4.1.14)
Now we are going to check that A satisﬁes (H1)–(H4).

66
4. A Class of Stochastic Diﬀerential Equations
(H1): Let u, v, x ∈H1,p
0 (Λ), then by (4.1.13) we have to show for λ ∈R, |λ| ⩽
1
lim
λ→0
 	
|∇(u + λv)(ξ)|p−2⟨∇(u + λv)(ξ), ∇x(ξ)⟩
−|∇u(ξ)|p−2⟨∇u(ξ), ∇x(ξ)⟩

dξ = 0.
Since obviously the integrands converge to zero as λ →0 dξ-a.e., we
only have to ﬁnd a dominating function to apply Lebesgue’s dominated
convergence theorem. But obviously, since |λ| ⩽1
|∇(u + λv)(ξ)|p−2|⟨∇(u + λv)(ξ), ∇x(ξ)⟩|
⩽2p−1 
|∇u(ξ)|p−1 + |∇v(ξ)|p−1
|∇x(ξ)|
and the right-hand side is in L1(Λ) by H¨older’s inequality as we have
seen above.
(H2): Let u, v ∈H1,p
0 (Λ). Then by (4.1.13)
−V ∗⟨A(u) −A(v), u −v⟩V
=

⟨|∇u(ξ)|p−2∇u(ξ) −|∇v(ξ)|p−2∇v(ξ), ∇u(ξ) −∇v(ξ)⟩dξ
=

(|∇u(ξ)|p + |∇v(ξ)|p −|∇u(ξ)|p−2⟨∇u(ξ), ∇v(ξ)⟩
−|∇v(ξ)|p−2⟨∇u(ξ), ∇v(ξ)⟩) dξ
⩾

(|∇u(ξ)|p + |∇v(ξ)|p −|∇u(ξ)|p−1|∇v(ξ)|
−|∇v(ξ)|p−1|∇u(ξ)|) dξ
=

(|∇u(ξ)|p−1 −|∇v(ξ)|p−1)(|∇u(ξ)| −|∇v(ξ)|) dξ
⩾0,
since the map R+ ∋s 	→sp−1 is increasing. Hence (H2) is shown with
c = 0.
(H3): Because Λ is bounded by Poincar´e’s inequality (cf. [GT83]) there exists
a constant c = c(p, d, |Λ|) ∈]0, ∞[ such that

|∇u(ξ)|p dξ ⩾c

|u(ξ)|p dξ
for all u ∈H1,p
0 (Λ).
(4.1.15)
Hence by (4.1.13) for all u ∈H1,p
0 (Λ)
V ∗⟨A(u), u⟩V = −

|∇u(ξ)|p dξ ⩽−min(1, c)
2
∥u∥p
1,p.

4.1. Gelfand triples, conditions on the coeﬃcients and examples
67
So, (H3) holds with α = p and c1 = 0. (We note that only for (H3) have
we used that Λ is bounded.)
(H4): This condition holds for A by (4.1.14) with α = p.
Before we go on to our last example which will include the case of the porous
medium equation we would like to stress the following:
Remark 4.1.10.
1. If one is given V ⊂H ⊂V ∗and A : V →V ∗(e.g.
as in the above examples) satisfying (H1)–(H4) (with B ≡0) one can
consider a “smaller” space V0, i.e. another reﬂexive separable Banach
space such that
V0 ⊂V
continuously and densely, hence (by restricting the linear functionals to
V0)
V ∗⊂V ∗
0
continuously and densely, so altogether
V0 ⊂V ⊂H ⊂V ∗⊂V ∗
0 .
Restricting A to V0 we see that A satisﬁes (H1),(H2) and (H4) with
respect to the Gelfand triple
V0 ⊂H ⊂V ∗
0 .
However, since ∥·∥V0 is up to a multiplicative constant larger than ∥·∥V ,
property (H3) might no longer hold. Therefore, e.g. if one considers a
map A which is given by a sum of the Laplacian (cf. Example 4.1.7)
and e.g. a monomial (cf. Example 4.1.5) one cannot just take any V0 ⊂
H1,2
0 (Λ) ∩Lp(Λ), since (H3) might get lost. However, if e.g. d ⩾3
and 1
d + 1
p = 1
2, then if Λ is bounded, by a Sobolev embedding theorem
(cf. [GT83, Theorems 7.10 and 7.15]), H1,2
0 (Λ) ⊂Lp(Λ) (⊂Lp′(Λ), p′ ∈
[1, p]) continuously and densely, so one can take V
:=
H1,2
0 (Λ),
H = L2(Λ) and can consider
A(u) := ∆u −u|u|p′−2, u ∈H1,2
0 (Λ).
Then (H3) holds with d := 2. So, if p′ ∈[1, 2], also (H4) holds with
d = 2. The corresponding SDE (4.1.3) then reads
dX(t) = (∆X(t) −X(t)|X(t)|p′−2) dt
(+B(t, X(t)) dW(t))
and is called a (stochastic) reaction diﬀusion equation.
In the case of the p-Laplacian, p ∈[2, ∞[, it is even easier to take
sums with monomials, since clearly H1,p
0 (Λ) ⊂Lp(Λ) continuously and
densely, so
A(u) := div(|∇u|p−2∇u) −u|u|p−2, u ∈H1,p
0 (Λ),

68
4. A Class of Stochastic Diﬀerential Equations
satisﬁes (H1)–(H4), if Λ is bounded, with respect to the Gelfand triple
H1,p
0 (Λ) ⊂L2(Λ) ⊂(H1,p
0 (Λ))∗.
But generally, taking sums of A as above requires some care and is not
always possible.
2. In all our analysis the space V ∗is only used as a tool. Eventually, since
the solutions to our SDE (4.1.3) will take values in H, V ∗will be of no
relevance. Therefore, no further information about V ∗such as its explicit
representation (e.g. as a space of Schwartz distributions) is necessary.
Example 4.1.11. [Lp ⊂(H1,2
0 )∗⊂(Lp)∗, A = porous medium operator]
As references for this example we refer e.g. to [Aro86], [DPRLRW06], [RRW06].
Let Λ ⊂Rd, Λ open and bounded, p ∈[2, ∞[ and
V := Lp(Λ), H := (H1,2
0 (Λ))∗.
Since Λ is bounded we have by Poincar´e’s inequality (4.1.15) that for some
constant c = c(2, d, |Λ|) > 0
∥u∥1,2 ⩾∥u∥H1,2
0
:=

|∇u(ξ)|2 dξ
 1
2
⩾
min(1, c)
2
 1
2
∥u∥1,2
for all u ∈H1,2
0 (Λ).
(4.1.16)
So, we can (and will do so below) consider H1,2
0 (Λ) with norm ∥·∥H1,2
0
and
corresponding scalar product
⟨u, v⟩H1,2
0
:=

⟨∇u(ξ), ∇v(ξ)⟩dξ, u, v ∈H1,2
0 (Λ).
Since H1,2
0 (Λ) ⊂L2(Λ) continuously and densely, so is
H1,2
0 (Λ) ⊂L
p
p−1 (Λ).
Hence
Lp(Λ) ≡
	
L
p
p−1 (Λ)

∗
⊂(H1,2
0 (Λ))∗= H,
continuously and densely. Now we would like to identify H with its dual
H∗= H1,2
0 (Λ) via the corresponding Riesz isomorphism R : H →H∗deﬁned
by Rx := ⟨x, ·⟩H, x ∈H. Let us calculate the latter.
Lemma 4.1.12. The map ∆: H1,2
0 (Λ) →(H1,2
0 (Λ))∗= H (deﬁned by
(4.1.13) for p = 2) is an isometric isomorphism. In particular,
⟨∆u, ∆v⟩H = ⟨u, v⟩H1,2
0
for all u, v ∈H1,2
0 (Λ).
(4.1.17)

4.1. Gelfand triples, conditions on the coeﬃcients and examples
69
Furthermore, (−∆)−1 : H →H∗= H1,2
0 (Λ) is the Riesz isomorphism for H,
i.e. for every x ∈H
⟨x, ·⟩H = H1,2
0 ⟨(−∆)−1x, ·⟩H .
(4.1.18)
Proof. Let u ∈H1,2
0 (Λ). Since by (4.1.13) for all v ∈H1,2
0 (Λ)
H⟨−∆u, v⟩H1,2
0
=

⟨∇u(ξ), ∇v(ξ)⟩dξ = ⟨u, v⟩H1,2
0 ,
(4.1.19)
it follows that −∆: H1,2
0 (Λ) →H is just the Riesz isomorphism for H1,2
0 (Λ)
and the ﬁrst part of the assertion including (4.1.17) follows. To prove the last
part, ﬁx x ∈H. Then by (4.1.17) and (4.1.19) for all y ∈H
⟨x, y⟩H = ⟨(−∆)−1x, (−∆)−1y⟩H1,2
0
= H⟨x, (−∆)−1y⟩H1,2
0
.
Now we identify H with its dual H∗by the Riesz map (−∆)−1 : H →H∗,
so H ≡H∗in this sense, hence
V = Lp(Λ) ⊂H ⊂(Lp(Λ))∗= V ∗
(4.1.20)
continuously and densely.
Lemma 4.1.13. The map
∆: H1,2
0 (Λ) →(Lp(Λ))∗
extends to a linear isometry
∆: L
p
p−1 (Λ) →(Lp(Λ))∗= V ∗
and for all u ∈L
p
p−1 (Λ), v ∈Lp(Λ)
V ∗⟨−∆u, v⟩V =
L
p
p−1 ⟨u, v⟩Lp =

u(ξ)v(ξ) dξ.
(4.1.21)
Remark 4.1.14. One can prove that this isometry is in fact surjective, hence
(Lp(Λ))∗= ∆(L
p
p−1 ) ̸= L
p
p−1 .
We shall not use this below, but it shows that the embedding (4.1.20) has to
be handled with care taking always into account that H is identiﬁed with H∗
by (−∆)−1 : H →H∗giving rise to a diﬀerent dualization between Lp(Λ) and
(Lp(Λ))∗. In particular, for all x ∈H, v ∈Lp(Λ)
(Lp)∗⟨x, v⟩Lp = ⟨x, v⟩H

̸=
L
p
p−1 ⟨x, v⟩Lp =

x(ξ)v(ξ) dξ
provided x ∈L
p
p−1

.

70
4. A Class of Stochastic Diﬀerential Equations
Proof of Lemma 4.1.13. Let u ∈H1,2
0 (Λ). Then since ∆u ∈H, by (4.1.2) and
(4.1.18) we obtain that for all v ∈V
V ∗⟨∆u, v⟩V = ⟨∆u, v⟩H = −H1,2
0 ⟨u, v⟩H = −⟨u, v⟩L2
(4.1.22)
since v ∈V ⊂L2(Λ). Therefore,
∥∆u∥V ∗⩽∥u∥
p
p−1 .
So, ∆extends to a continuous linear map
∆: L
p
p−1 (Λ) →V ∗
such that (4.1.22) holds for all u ∈L
p
p−1 (Λ), i.e. (4.1.21) is proved.
So, applying it to u ∈L
p
p−1 (Λ) and
v := −∥u∥
−q
p
q
u|u|q−2 ∈Lp(Λ),
where q :=
p
p−1, by (4.1.21) we obtain that
V ∗⟨∆u, v⟩V = ∥u∥
p
p−1
and ∥v∥p = 1, so ∥∆u∥V ∗= ∥u∥
p
p−1 and the assertion is completely proved.
Now we want to deﬁne the “porous medium operator A”. So, let Ψ : R →R
be a function having the following properties:
(Ψ1) Ψ is continuous.
(Ψ2) For all s, t ∈R
(t −s)(Ψ(t) −Ψ(s)) ⩾0.
(Ψ3) There exist p ∈[2, ∞[, a ∈]0, ∞[, c ∈[0, ∞[ such that for all s ∈R
sΨ(s) ⩾a|s|p −c.
(Ψ4) There exist c3, c4 ∈]0, ∞[ such that for all s ∈R
|Ψ(s)| ⩽c4 + c3|s|p−1,
where p is as in (Ψ3).
We note that (Ψ4) implies that
Ψ(v) ∈L
p
p−1 (Λ)
for all v ∈Lp(Λ).
(4.1.23)
Now we can deﬁne the porous medium operator A : Lp(Λ) = V →V ∗=
(Lp(Λ))∗by
A(u) := ∆Ψ(u),
u ∈Lp(Λ).
(4.1.24)
Note that by (4.1.21) and Lemma 4.1.13 the operator A is well-deﬁned. Now
let us check (H1)–(H4).

4.1. Gelfand triples, conditions on the coeﬃcients and examples
71
(H1): Let u, v, x ∈V = Lp(Λ) and λ ∈R. Then by (4.1.21)
V ∗⟨A(u + λv), x⟩V = V ∗⟨∆Ψ(u + λv), x⟩V
= −

Ψ(u(ξ) + λv(ξ))x(ξ) dξ.
(4.1.25)
By (Ψ4) for |λ| ⩽1 the integrand in the right-hand side of (4.1.25) is
bounded by
[c4 + c32p−1(|u|p−1 + |v|p−1)]|x|
which by H¨older’s inequality is in L1(Λ). So, (H1) follows by (Ψ1) and
Lebesgue’s dominated convergence theorem.
(H2): Let u, v ∈V = Lp(Λ). Then by (4.1.21)
V ∗⟨A(u) −A(v), u −v)⟩V = V ∗⟨∆(Ψ(u) −Ψ(v)), u −v⟩V
= −

[Ψ(u(ξ)) −Ψ(v(ξ))](u(ξ) −v(ξ)) dξ
⩽0,
where we used (Ψ2) in the last step.
(H3): Let v ∈Lp(Λ) = V . Then by (4.1.21) and (Ψ3)
V ∗⟨A(v), v⟩V = −

Ψ(v(ξ))v(ξ) dξ
⩽

(−a|v(ξ)|p + c) dξ.
Hence (H3) is satisﬁed with c1 := 0, c2 := 2a, α = p and f(t) = 2c|Λ|.
(H4): Let v ∈Lp(Λ) = V . Then by Lemma 4.1.13 and (Ψ4)
∥A(v)∥V ∗= ∥∆Ψ(v)∥V ∗
= ∥Ψ(v)∥
L
p
p−1
⩽c4|Λ|
p−1
p
+ c3

|v(ξ)|p dξ
 p−1
p
= c4|Λ|
p−1
p
+ c3∥v∥p−1
V
,
so (H4) holds with α = p.
Remark 4.1.15.
1. For p ∈[2, ∞[ and Ψ(s) := s|s|p−2 we have
A(v) = ∆(v|v|p−2), v ∈Lp(Λ),

72
4. A Class of Stochastic Diﬀerential Equations
which is the non-linear operator appearing in the classical porous medium
equation, i.e.
∂X(t)
∂t
= ∆(X(t)|X(t)|p−2),
X(0, ·) = X0,
whose solution describes the time evolution of the density X(t) of a
substance in a porous medium (cf. e.g. [Aro86]).
2. Let Ψ : R →R be given such that (Ψ1)–(Ψ4) are satisﬁed with some
p ∈]1, ∞[ (in (Ψ3), (Ψ4)). One can see that the above assumptions that
Λ is bounded and p ⩾2, can be avoided. But p then depends on the
dimension of the underlying space Rd. Let us assume ﬁrst that d ⩾3.
We distinguish two cases:
Case 1. |Λ| = ∞and p :=
2d
d+2, c = c4 = 0, where c, c4 are the constants in
(Ψ3) and in (Ψ4) respectively.
Case 2. |Λ| < ∞and p ∈
%
2d
d+2, ∞
%
.
By the Sobolev embedding theorem (cf. [GT83, Theorem 7.10]) we have
H1,2
0 (Λ) ⊂L
2d
d−2 (Λ)
continuously and densely, and
∥u∥2d
d−2 ⩽
2(d −1)
√
d(d −2)
∥u∥H1,2
0
for all u ∈H1,2
0 (Λ).
In Case 1 we have
2d
d−2 =
p
p−1 and in Case 2 (hence in both cases)
2d
d −2 ⩾
p
p −1
and thus
H1,2
0 (Λ) ⊂L
p
p−1 (Λ)
densely and for some c0 ∈]0, ∞[
∥u∥
p
p−1 ⩽c0∥u∥H1,2
0
for all u ∈H1,2
0 (Λ).
Now the above arguments generalize to both Cases 1 and 2, i.e. for the
Gelfand triple
V := Lp(Λ) ⊂H := (H1,2
0 (Λ))∗⊂(Lp(Λ))∗
the operator
A : Lp(Λ) =: V →V ∗= (Lp(Λ))∗
deﬁned in (4.1.24), satisﬁes (H1)–(H4).

4.2. The main result and an Itˆo formula
73
We note that in Case 1 the norm ∥·∥H1,2
0
deﬁned in (4.1.16) is in general
not equivalent to ∥·∥1,2, because the Poincare inequality does not hold.
So, H1,2
0 (Λ))∗as a dual to the normed vector spaces (H1
0(Λ), ∥·∥H1,2
0 ) is
complete. In particular, if (3 ⩽d) ⩽6, we may take p = 3
2 and
Ψ(s) := sign(s)

|s|, s ∈R.
For Λ bounded the above extends, of course, also to the case d = 1, 2
where even stronger Sobolev embeddings hold (cf. [GT83, Theorems 7.10
and 7.15]).
4.2. The main result and an Itˆo formula
Consider the general situation described at the beginning of the previous
section. So, we have a Gelfand triple
V ⊂H ⊂V ∗
and maps
A : [0, T] × V × Ω→V ∗,
B : [0, T] × V × Ω→L2(U, H)
as speciﬁed there, satisfying (H1)–(H4), and consider the stochastic diﬀerential
equation
dX(t) = A(t, X(t)) dt + B(t, X(t)) dW(t)
(4.2.1)
on H with W(t), t ∈[0, T], a cylindrical Q-Wiener process with Q := I taking
values in another separable Hilbert space (U, ⟨, ⟩U) and being deﬁned on a
complete probability space (Ω, F, P) with normal ﬁltration Ft, t ∈[0, T].
Before we formulate our main existence and uniqueness result for solutions
of (4.2.1) we have to deﬁne what we mean by “solution”.
Deﬁnition 4.2.1. A continuous H-valued (Ft)-adapted process (X(t))t∈[0,T ]
is called a solution of (4.2.1), if for its dt ⊗P-equivalence class ˆX we have
ˆX ∈Lα([0, T] × Ω, dt ⊗P; V ) ∩L2([0, T] × Ω, dt ⊗P; H) with α as in (H3)
and P-a.s.
X(t) = X(0) +
 t
0
A(s, ¯X(s))ds +
 t
0
B(s, ¯X(s)) dW(s),
t ∈[0, T],
(4.2.2)
where ¯X is any V -valued progressively measurable dt ⊗P-version of ˆX.

74
4. A Class of Stochastic Diﬀerential Equations
Remark 4.2.2.
1. The existence of the special version ¯X above follows from Exercise 4.2.3
below. Furthermore, for technical reasons in Deﬁnition 4.2.1 and
below we consider all processes initially as V ∗-valued, hence by
dt⊗
P-equivalence classes we always mean classes of V ∗-valued processes.
2. The integral with respect to ds in (4.2.2) is initially a V ∗-valued Bochner
integral which turns out to be in fact H-valued.
3. Solutions in the sense of Deﬁnition 4.2.1 are often called variational
solutions in the literature. There are various other notions of solutions
for stochastic (partial) diﬀerential equations. We recall the deﬁnition of
(probabilistically) weak and strong solutions in Appendix E below. The
notions of analytically weak and strong solutions as well as the notion
of mild solutions and their relations are recalled in Appendix F below.
Exercise 4.2.3.
1. Let BV ∗
1
denote the closed unit ball in V ∗. Since BV ∗
1
∩H ̸= ∅, it has a
countable subset {li|i ∈N}, which is dense in BV ∗
1
∩H with respect to
H-norm.
Deﬁne Θ : H →[0, ∞] by
Θ(h) := sup
i∈N
|⟨li, h⟩H|, h ∈H.
Then Θ is lower semicontinuous on H, hence B(H)-measurable. Since
V ∗⟨li, v⟩V = ⟨li, v⟩H, i ∈N, v ∈V , we have
Θ(v) = ∥v∥V
for all v ∈V,
and furthermore (by the reﬂexivity of V )
{Θ < ∞} = V.
2. Let X : [0, T] × Ω→H be any progressively measurable (i.e. B([0, t]) ⊗
Ft/B(H)-measurable for all t ∈[0, T]) dt⊗P-version of ˆX ∈Lα([0, T]×
Ω, dt ⊗P; V ), α ∈(0, ∞). Then
¯X := I{Θ◦X<∞}X
is a V -valued progressively measurable (i.e. B([0, t]) ⊗Ft/B(V )-measu-
rable) dt ⊗P-version of ˆX.
3. Both A(·, ¯X) and B(·, ¯X) are V -valued respectively L2(U, H)-valued pro-
gressively measurable processes.
Now the main result (cf. [KR79]):

4.2. The main result and an Itˆo formula
75
Theorem
4.2.4.
Let
A, B
above
satisfy
(H1)–(H4)
and
let
X0 ∈L2(Ω, F0, P; H). Then there exists a unique solution X to (4.2.1) in
the sense of Deﬁnition 4.2.1. Moreover,
E( sup
t∈[0,T ]
∥X(t)∥2
H) < ∞.
(4.2.3)
The proof of Theorem 4.2.4 strongly depends on the following Itˆo formula,
from [KR79, Theorem I.3.1], which we shall prove here ﬁrst. The presentation
of its proof and that of Theorem 4.2.4 is an extended adaptation of those in
[RRW06].
Theorem 4.2.5. Let X0 ∈L2(Ω, F0, P; H) and Y ∈L
α
α−1 ([0, T] × Ω, dt ⊗
P; V ∗), Z ∈L2([0, T] × Ω, dt ⊗P; L2(U, H)), both progressively measurable.
Deﬁne the continuous V ∗-valued process
X(t) := X0 +
 t
0
Y (s)ds +
 t
0
Z(s) dW(s), t ∈[0, T].
If for its dt ⊗P-equivalence class ˆX we have ˆX ∈Lα([0, T] × Ω, dt ⊗P, V )
with α as in (H3), then X is an H-valued continuous (Ft)-adapted process,
E

sup
t∈[0,T ]
∥X(t)∥2
H

< ∞
and the following Itˆo-formula holds for the square of its H-norm P-a.s.
∥X(t)∥2
H = ∥X0∥2
H +
 t
0
	
2 V ∗⟨Y (s), ¯X(s)⟩V +∥Z(s)∥2
L2(U,H)

ds
+ 2
 t
0
⟨X(s), Z(s) dW(s)⟩H
for all t ∈[0, T]
(4.2.4)
for any V -valued progressively measurable dt ⊗P-version ¯X of ˆX.
As in [KR79] for the proof of Theorem 4.2.5 we need the following lemma
about piecewise constant approximations based on an argument due to
[Doo53]. For abbreviation below we set
K := Lα([0, T] × Ω, dt ⊗P; V ).
(4.2.5)
Lemma 4.2.6. Let X : [0, T] × Ω→V ∗be B([0, T]) ⊗F/B(V ∗)-measurable
such that for its
dt ⊗P-equivalence class ˆX we have ˆX ∈K. Then there
exists a sequence of partitions Il := {0 = tl
0 < tl
1 < · · · < tl
kl = T} such that
Il ⊂Il+1 and δ(Il) := maxi(tl
i −tl
i−1) →0 as l →∞, X(tl
i) ∈V P-a.e. for
all l ∈N, 1 ≤i ≤kl −1, and for

76
4. A Class of Stochastic Diﬀerential Equations
¯Xl :=
kl

i=2
1[tl
i−1,tl
i[X(tl
i−1),
˜Xl :=
kl−1

i=1
1[tl
i−1,tl
i[X(tl
i),
l ∈N,
we have ¯Xl, ˜Xl are ( dt ⊗P-versions of elements) in K such that
lim
l→∞

∥ˆX −¯Xl∥K + ∥ˆX −˜Xl∥K

= 0.
Proof. For simplicity we assume that T = 1 and let ¯X : [0, 1] × Ω→V be a
dt ⊗P-version of ˆX. We extend ¯X to R × Ωby setting ¯X = 0 on [0, 1]c × Ω.
There exists Ω′ ∈F with full probability such that for every ω ∈Ω′ there
exists a sequence (fn)n∈N ⊂C(R; V ) with compact support such that

R
∥fn(s) −¯X(s, ω)∥α
V ds ≤1
2n,
n ∈N.
Thus, for every n ∈N,
lim sup
δ→0

R
∥¯X(δ + s, ω) −¯X(s, ω)∥α
V ds
≤3α−1 lim sup
δ→0

R

∥¯X(δ + s, ω) −fn(δ + s)∥α
V + ∥¯X(s, ω) −fn(s)∥α
V

ds
≤3α−1
n
, n ∈N.
Here we used that since each fn is uniformly continuous, by Lebesgue’s dom-
inated convergence theorem we have that for all n ∈N
lim
δ→0

R
∥fn(δ + s) −fn(s)∥α
V ds = 0.
Letting n →∞we obtain
lim
δ→0

R
∥¯X(δ + s, ω) −¯X(s, ω)∥α
V ds = 0,
ω ∈Ω′.
(4.2.6)
Now, given t ∈R, let [t] denote the largest integer ≤t. Let γn(t) := 2−n[2nt],
n ∈N, that is, γn(t) is the largest number of the form
k
2n , k ∈Z, below t.
Shifting the integral in (4.2.6) by t and taking δ = γn(t) −t we obtain
lim
n→∞

R
∥¯X(γn(t) + s) −¯X(t + s)∥α
V ds = 0 on Ω′.

4.2. The main result and an Itˆo formula
77
Moreover,
 1
0
∥¯X(γn(t) + s) −¯X(t + s)∥α
V ds
≤1[−2,2](t)2α−1

R

∥¯X(γn(t) + s)∥α
V + ∥¯X(t + s)∥α
V

ds
= 2α1[−2,2](t)
 1
0
∥¯X(s)∥α
V ds on Ω′.
So, by Lebesgue’s dominated convergence theorem, we obtain that
0 = lim
n→∞E

R
dt
 1
0
∥¯X(γn(t) + s) −¯X(t + s)∥α
V ds
≥lim
n→∞E
 1
0
ds
 1
0
∥¯X(γn(t −s) + s) −¯X(t)∥α
V dt.
(4.2.7)
Given s ∈[0, 1) and n ∈N, let the partition In(s) be deﬁned by
tn
0(s) := 0, tn
i (s) :=
	
s −[2ns]
2n

+ i −1
2n , 1 ≤i ≤2n, tn
2n+1(s) := 1.
Then, for t ∈[tn
i−1(s), tn
i (s)[, 1 ⩽i ⩽2n + 1, one has t −s ∈[2−n(i −[2ns] −
2), 2−n(i −[2ns] −1)[ and hence,
γn(t −s) + s =

2−n(i −[2ns] −2) + s
+ = tn
i−1(s),
1 ≤i ≤2n + 1.
Therefore, (4.2.7) implies
lim
n→∞E
 1
0
ds
 1
0
∥¯X(t) −¯Xn,s(t)∥α
V dt = 0,
where ¯Xn,s is the process deﬁned as ¯Xl for the partition In(s) but with
X(tl
i−1(s)) replaced by ¯X(tl
i−1(s)). Similarly, the same holds for ˜Xn,s in place
of ¯Xn,s by using ˜γn := γn + 2−n instead of γn, where ˜Xn,s is deﬁned as ˜Xl
for the partition In(s) but with X(tl
i(s)) replaced by ¯X(tl
i(s)). Hence, there
exist a subsequence nk →∞and a ds-zero set N1 ∈B([0, 1]) such that
lim
k→∞E
 1
0

∥¯X(t)−¯Xnk,s(t)∥α
V +∥¯X(t)−˜Xnk,s(t)∥α
V

dt = 0,
s ∈[0, 1]\N1.
Since for 1 ≤i ≤2n the maps s 	→tn
i (s) are piecewise C1-diﬀeomorphisms,
the image measures of ds under these maps are absolutely continuous with
respect to ds. Therefore, since ¯X = X ds ⊗P-a.e., there exists a ds-zero set
N2 ∈B([0, 1]) such that

78
4. A Class of Stochastic Diﬀerential Equations
¯X(tn
i (s)) = X(tn
i (s)) P-a.e. for all s ∈[0, 1] \ N2, 1 ≤i ≤2n.
Since for any s ∈[0, 1] \ (N1 ∪N2) one has E

∥¯X(tn
i (s))∥α
V

< ∞, the map
[0, 1] × Ω∋(s, ω) 	→X(tn
i (s), ω) ∈V
is once again (a
dt ⊗P-version of an element) in K. Therefore, ﬁxing
s ∈[0, 1]\(N1∪N2), the sequence of the corresponding partitions Inl(s), l ≥1,
has all properties of the assertion.
Remark 4.2.7. As follows from the above proof all the partition points tl
i, l ≥
1, 1 ≤i ≤kl −1, in the assertion of Lemma 4.2.6 can be chosen outside an a
priori given Lebesgue zero set in [0, T] instead of N2 above.
Proof of Theorem 4.2.5. Since M(t) :=
' t
0 Z(s) dW(s), t ∈[0, T], is already
a continuous martingale on H and since Y ∈K∗= Lα/(α−1)([0, T] × Ω→
V ∗; dt ⊗P) is progressively measurable,
' t
0 Y (s) ds is a continuous adapted
process on V ∗. Thus, X is a continuous adapted process on V ∗, hence
B([0, T]) ⊗F/B(V ∗)-measurable.
Claim (a):
∥X(t)∥2
H =∥X(s)∥2
H + 2
 t
s
V ∗⟨Y (r), X(t)⟩V dr + 2⟨X(s), M(t) −M(s)⟩H
+ ∥M(t) −M(s)∥2
H −∥X(t) −X(s) −M(t) + M(s)∥2
H
(4.2.8)
holds for all t > s such that X(t), X(s) ∈V.
Indeed, this follows immediately by noting that
∥M(t) −M(s)∥2
H −∥X(t) −X(s) −M(t) + M(s)∥2
H
+ 2⟨X(s), M(t) −M(s)⟩H
= 2⟨X(t) −X(s), M(t) −M(s)⟩H −∥X(t) −X(s)∥2
H
+ 2⟨X(s), M(t) −M(s)⟩H
= 2⟨X(t), M(t) −M(s)⟩H −∥X(t) −X(s)∥2
H
= 2⟨X(t), X(t) −X(s)⟩H −2
 t
s
V ∗⟨Y (r), X(t)⟩V dr
−∥X(t)∥2
H −∥X(s)∥2
H + 2⟨X(t), X(s)⟩H
= ∥X(t)∥2
H −∥X(s)∥2
H −2
 t
s
V ∗⟨Y (r), X(t)⟩V dr.

4.2. The main result and an Itˆo formula
79
Claim (b): We have
E

sup
t∈[0,T ]
∥X(t)∥2
H

< ∞.
(4.2.9)
Indeed, by (4.2.8), for any t = tl
i ∈Il \ {0, T} given in Lemma 4.2.6,
∥X(t)∥2
H −∥X0∥2
H
=
i−1

j=0
(∥X(tl
j+1)∥2
H −∥X(tl
j)∥2
H)
= 2
 t
0
V ∗⟨Y (s), ˜Xl(s)⟩V ds
+ 2
 t
0
⟨¯Xl(s), Z(s) dW(s)⟩H + 2⟨X(0),
 tl
1
0
Z(s) dW(s)⟩H
+
i−1

j=0

∥M(tl
j+1) −M(tl
j)∥2
H −∥X(tl
j+1) −X(tl
j) −M(tl
j+1) + M(tl
j)∥2
H

.
(4.2.10)
We note that since ¯Xl is pathwise bounded the stochastic integral involving
¯Xl above is well-deﬁned. By Lemma 4.2.6
E
 T
0
| V ∗⟨Y (s), ˜Xl(s)⟩V | ds

≤∥Y ∥K∗∥˜Xl∥K ≤c1
(4.2.11)
for some constant c1 > 0 independent of l. Moreover, by the Burkholder–Davis
inequality (cf. Proposition D.0.1), Lemmas 2.4.2 and 2.4.3,
E

sup
t∈[0,T ]

 t
0
⟨¯Xl(s), Z(s) dW(s)⟩H


⩽3E
1  T
0
∥Z(s)∗¯Xl(s)∥2
U ds
21/2
⩽3E
1  T
0
∥¯Xl(s)∥2
H∥Z(s)∥2
L2(U,H) ds
21/2
=3E
1  T
0
∥¯Xl(s)∥2
H d⟨M⟩s
21/2
≤1
4E

sup
kl−1≥j≥0
∥X(tl
j)∥2
H

+ 9E

⟨M⟩T

,
(4.2.12)

80
4. A Class of Stochastic Diﬀerential Equations
where ⟨M⟩t =
' t
0 ∥Z(s)∥2
L2(U,H) ds and we used that
ab ⩽1
12a2 + 3b2, a, b > 0.
Finally, by Lemma 2.4.3
E
⎛
⎝
i−1

j=0
∥M(tl
j+1) −M(tl
j)∥2
H
⎞
⎠=
i−1

j=0
E
 tl
j+1
tl
j
∥Z(s)∥2
L2(U,H) ds

= E
 tl
i
0
∥Z(s)∥2
L2(U,H) ds

= E
	
⟨M⟩tl
i

.
(4.2.13)
Combining (4.2.10)–(4.2.13), we obtain
E

sup
t∈Il\{T }
∥X(t)∥2
H

≤c2
for some constant c2 > 0 independent of l. Therefore, letting l ↑∞and setting
I := ∪l≥1Il \ {T}, with Il as in Lemma 4.2.6, we obtain
E

sup
t∈I
∥X(t)∥2
H

⩽c2,
since Il ⊂Il+1 for all l ∈N. Since for all t ∈[0, T]
N

j=1
V ∗⟨X(t), ej⟩2
V ↑∥X(t)∥2
H as N ↑∞,
where {ej
j ∈N} ⊂V is an orthonormal basis of H and as usual for x ∈V ∗\H
we set ∥x∥H := ∞, it follows that t 	→∥X(t)∥H is lower semicontinuous P-a.s.
Since I is dense in [0, T], we arrive at supt∈[0,T ] ∥X(t)∥2
H = supt∈I ∥X(t)∥2
H.
Thus, (4.2.9) holds.
Claim (c):
lim
l→∞sup
t∈[0,T ]

 t
0
⟨X(s) −¯Xl(s), Z(s) dW(s)⟩H
 = 0 in probability.
(4.2.14)
We ﬁrst note that because of (b) X is H-valued and by its continuity in V ∗the
process X is weakly continuous in H and, therefore, since B(H) is generated
by H∗, progressively measurable as an H-valued process. Hence, for any n ∈N
the process PnX(s) is continuous in H so that

4.2. The main result and an Itˆo formula
81
lim
l→∞
 T
0
∥Pn(X(s) −¯Xl(s))∥2
H d⟨M⟩s = 0,
P-a.s..
Here Pn denotes the orthogonal projection onto span{e1, . . . , en} in H. There-
fore, it suﬃces to show that for any ε > 0,
lim
n→∞sup
l∈N
P

sup
t∈[0,T ]

 t
0
⟨(1 −Pn) ¯Xl(s), Z(s) dW(s)⟩H
 > ε

= 0,
lim
n→∞P

sup
t∈[0,T ]

 t
0
⟨(1 −Pn)X(s), Z(s) dW(s)⟩H
 > ε

= 0.
(4.2.15)
For any n ∈N, δ ∈(0, 1) and N > 1 by Corollary D.0.2 we have that
P

sup
t∈[0,T ]

 t
0
⟨(1 −Pn) ¯Xl(s), Z(s) dW(s)⟩H
 > ε

≤3δ
ε + P
  T
0
∥¯Xl(s)∥2
H d⟨(1 −Pn)M⟩s > δ2

≤3δ
ε + P
	
sup
t∈[0,T ]
∥X(t)∥H > N

+ N 2
δ2 E⟨(1 −Pn)M⟩T .
By ﬁrst letting n →∞, and using Lemma 2.4.3, and then letting N →∞
and ﬁnally δ →0, we prove the ﬁrst equality in (4.2.15). Similarly, the second
equality is proved.
Claim (d): (4.2.4) holds for t ∈I.
Fix t ∈I. We may assume that t ̸= 0. In this case for each suﬃciently large l ∈
N there exists a unique 0 < i < kl such that t = tl
i. We have X(tl
j) ∈V a.s. for
all j. By Lemma 4.2.6 and (4.2.14) the sum of the ﬁrst three terms in the right-
hand side of (4.2.10) converges in probability to 2
' t
0
V ∗⟨Y (s), ¯X(s)⟩V ds +
2
' t
0⟨X(s), Z(s) dW(s)⟩H, as l →∞. Hence by Lemma 2.4.3
∥X(t)∥2
H −∥X(0)∥2
H
= 2
 t
0
V ∗⟨Y (s), ¯X(s)⟩V ds + 2
 t
0
⟨X(s), Z(s) dW(s)⟩H + ⟨M⟩t −ε0,
where
ε0 := P −lim
l→∞
i−1

j=0
∥X(tl
j+1) −X(tl
j) −M(tl
j+1) + M(tl
j)∥2
H

82
4. A Class of Stochastic Diﬀerential Equations
exists and “P −lim” denotes limit in probability. So, to prove (4.2.4) for t as
above, it suﬃces to show that ε0 = 0. Since for any ϕ ∈V ,
⟨X(tl
j+1) −X(tl
j) −M(tl
j+1) + M(tl
j), ϕ⟩H =
 tl
j+1
tl
j
V ∗⟨Y (s), ϕ⟩V ds,
letting ˜
M l and ¯
M l be deﬁned as ˜Xl and ¯Xl respectively, for M replacing X,
we obtain for every n ∈N
ε0 = P −lim
l→∞
  t
0
V ∗⟨Y (s), ˜Xl(s) −¯Xl(s) −Pn( ˜
M l(s) −¯
M l(s))⟩V ds
−⟨X(tl
1) −X(0) −M(tl
1) + M(0), PnM(0) −X(0)⟩H
−
i−1

j=0
⟨X(tl
j+1) −X(tl
j) −M(tl
j+1)
+ M(tl
j), (1 −Pn)(M(tl
j+1) −M(tl
j))⟩H

.
By the weak continuity of X in H the second term converges to zero as
l →∞. Lemma 4.2.6 implies that
' t
0 V ∗⟨Y (s), ˜Xl(s) −¯Xl(s)⟩V
ds →0 in
probability as l →∞. Moreover, since PnM(s) is a continuous process in V ,
' t
0 V ∗⟨Y (s), Pn( ˜
M l(s) −¯
M l(s))⟩V ds →0 as l →∞. Thus, by Lemma 2.4.3
ε0 ≤P- lim
l→∞
	 i−1

j=0
∥X(tl
j+1) −X(tl
j) −M(tl
j+1) + M(tl
j)∥2
H

 1
2
·
	 i−1

j=0
∥(1 −Pn)(M(tl
j+1) −M(tl
j))∥2
H

 1
2
= ε1/2
0
⟨(1 −Pn)M⟩1/2
t
,
which goes to zero as n →∞again by Lemma 2.4.3 and Lebesgue’s dominated
convergence theorem. Therefore, ε0 = 0.
Claim (e): (4.2.4) holds for all t ∈[0, T]\I.
Take Ω′ ∈F with full probability such that the limit in (4.2.14) is a pointwise
limit in Ω′ for some subsequence (denoted again by l →∞) and (4.2.4) holds
for all t ∈I on Ω′. If t /∈I, for any l ∈N there exists a unique j(l) < kl
such that t ∈]tl
j(l), tl
j(l)+1]. Letting t(l) := tl
j(l), we have t(l) ↑t as l ↑∞. By
(4.2.4) for t ∈I, for any l > m we have on Ω′ (since the above applies to

4.2. The main result and an Itˆo formula
83
X −X(t(m)) replacing X)
∥X(t(l)) −X(t(m))∥2
H
= 2
 t(l)
t(m)
V ∗⟨Y (s), ¯X(s) −X(t(m))⟩V ds
+ 2
 t(l)
t(m)
⟨X(s) −X(t(m)), Z(s) dW(s)⟩H + ⟨M⟩t(l) −⟨M⟩(t(m)
= 2
 T
0
1[t(m),t(l)](s) V ∗⟨Y (s), ¯X(s) −¯Xm(s)⟩V ds
+ 2
 t(m)
t(l)
⟨X(s) −¯Xm(s), Z(s) dW(s)⟩H + ⟨M⟩t(l) −⟨M⟩(t(m).
(4.2.16)
The second summand is dominated by
4 sup
t∈[0,T ]

 t
0
⟨X(s) −¯Xm(s), Z(s) dW(s)⟩H
 .
Thus, by the continuity of ⟨M⟩t and (4.2.14) (holding pointwise on Ω′), we
have that
lim
m→∞sup
l>m

2

 T
0
1[t(m),t(l)](s)⟨X(s) −¯Xm(s), Z(s) dW(s)⟩H

+ |⟨M⟩t(l) −⟨M⟩(t(m)|

= 0
(4.2.17)
holding on Ω′. Furthermore, by Lemma 4.2.6, selecting another subsequence
if necessary, we have for some Ω′′ ∈F with full probability and Ω′′ ⊂Ω′, that
on Ω′′
lim
m→∞
 T
0
| V ∗⟨Y (s), ¯X(s) −¯Xm(s)⟩V | ds = 0.
Since for all t /∈I
sup
l>m
 t(l)
t(m)
| V ∗⟨Y (s), ¯X(s) −¯Xm(s)⟩V | ds
≤
 T
0
| V ∗⟨Y (s), ¯X(s) −¯Xm(s)⟩V | ds,

84
4. A Class of Stochastic Diﬀerential Equations
we have that
lim
m→∞sup
l>m
 t(l)
t(m)
V ∗⟨Y (s), ¯X(s) −¯Xm(s)⟩V ds = 0
holds on Ω′′.
Combining this with (4.2.16) and (4.2.17), we conclude that
lim
m→∞sup
l≥m
∥X(t(l)) −X(t(m))∥2
H = 0
holds on Ω′′. Thus, (X(t(l)))l∈N converges in H on Ω′′. Since we know that
X(t(l)) →X(t) in V ∗, it converges to X(t) strongly in H on Ω′′. Therefore,
since (4.2.4) holds on Ω′′ for t(l), letting l →∞, we obtain (4.2.4) on Ω′′ also
for all t /∈I.
Claim (f): X is strongly continuous in H.
Since the right-hand side of (4.2.4) is on Ω′′ continuous in t ∈[0, T], so must
be its left-hand side, i.e. t 	→∥X(t)∥H is continuous on [0, T]. Therefore, the
weak continuity of X(t) in H implies its strong continuity in H.
Remark 4.2.8. In the situation of Theorem 4.2.5 we have
E(∥X(t)∥2
H)
= E(∥X0∥2
H) +
 t
0
E(2 V ∗⟨Y (s), ¯X(s)⟩V +∥Z(s)∥2
L2(U,H)) ds,
t ∈[0, T].
(4.2.18)
Proof. Let M(t), t ∈[0, T], denote the real valued local martingale in (4.2.4)
and let τl, l ∈N, be (Ft)-stopping times such that M(t ∧τl), t ∈[0, T], is a
martingale and τl ↑∞as l →∞. Then for all l ∈N, t ∈[0, T], we have
E(∥X(t ∧τl)∥2
H)
= E(∥X0∥2
H) +
 t
0
E(1[0,τl](s)[2 V ∗⟨Y (s), ¯X(s)⟩V +∥Z(s)∥2
L2(U,H)]) ds.
(4.2.19)
Using Claim (b) from the proof of Theorem 4.2.5 and the fact that the inte-
grands on the right-hand side of (4.2.19) are dt ⊗P-integrable we can apply
Lebesgue’s dominated convergence theorem to obtain the assertion.
Now we turn to the proof of Theorem 4.2.4. We ﬁrst need some prepa-
rations. Let {ei|i ∈N} ⊂V be an orthonormal basis of H and let Hn :=
span{e1, . . . , en} such that span{ei|i ∈N} is dense in V . Let Pn : V ∗→Hn
be deﬁned by
Pny :=
n

i=1
V ∗⟨y, ei⟩V ei,
y ∈V ∗.
(4.2.20)

4.2. The main result and an Itˆo formula
85
Clearly, Pn|H is just the orthogonal projection onto Hn in H. Let {gi|i ∈N}
be an orthonormal basis of U and set
W (n)(t) :=
n

i=1
⟨W(t), gi⟩Ugi =
n

i=1
Bi(t) gi.
For each ﬁnite n ∈N we consider the following stochastic equation on Hn :
dX(n)(t)
= PnA(t, X(n)(t)) dt + PnB(t, X(n)(t)) dW (n)(t),
1 ≤j ≤n,
(4.2.21)
where X(n)(0) := PnX0. It is easily seen (cf. in particular Remark 4.1.1, parts
1 and 2) that we are in the situation of Theorem 3.1.1 which implies that
(4.2.21) has a unique continuous strong solution. Let
J := L2([0, T] × Ω, dt ⊗P; L2(U, H)).
(4.2.22)
To construct the solution to (4.2.1), we need the following lemma.
Lemma 4.2.9. Under the assumptions in Theorem 4.2.4, there exists C ∈
]0, ∞[ such that
∥X(n)∥K + ∥A(·, X(n))∥K∗+ sup
t∈[0,T ]
E∥X(n)(t)∥2
H ≤C
(4.2.23)
for all n ∈N.
Proof. By the ﬁnite-dimensional Itˆo formula we have P-a.s.
∥X(n)(t)∥2
H = ∥X(n)
0
∥2
H +
 t
0

2 V ∗⟨A(s, X(n)(s)), X(n)(s)⟩V
+ ∥Z(n)(s)∥2
L2(U,H)

ds + M (n)(t), t ∈[0, T],
where Z(n)(s) := PnB(s, X(n)(s)) and
M (n)(t) := 2
 t
0
⟨X(n)(s), PnB(s, X(n)(s)) dW (n)(s)⟩H, t ∈[0, T],
is a local martingale. Let τl, l ∈N, be (Ft)-stopping times such that ∥X(n)(t∧
τl)(ω)∥V is bounded uniformly in (t, ω) ∈[0, T] × Ω, M (n)(t ∧τl), t ∈[0, T], is
a martingale for each l ∈N and τl ↑∞as l →∞. Then for all l ∈N, t ∈[0, T]
E
	
∥X(n)(t ∧τl)∥2
H

= E(∥X(n)
0
∥2
H) +
 t
0
E
	
1[0,τl](s)(2 V ∗⟨A(s, X(n)(s)), X(n)(s)⟩V
+ ∥Z(n)(s)∥2
L2(U,H))

ds.

86
4. A Class of Stochastic Diﬀerential Equations
Hence using the product rule we obtain
E(e−c1t∥X(n)(t ∧τl)∥2
H)
= E(∥X(n)
0
∥2
H) +
 t
0
E(∥X(n)(s ∧τl)∥2
H) d(e−c1s)
+
 t
0
e−c1s d(E(∥X(n)(s ∧τl)∥2
H))
= E(∥X(n)
0
∥2
H) −
 t
0
c1E(∥X(n)(s ∧τl)∥2
H)e−c1s ds
+
 t
0
e−c1sE
	
1[0,τl](s)(2 V ∗⟨A(s, X(n)(s)), X(n)(s)⟩V
+ ∥Z(n)(s)∥2
L2(U,H))

ds.
(4.2.24)
Applying (H3) we arrive at
E(e−c1t∥X(n)(t ∧τl)∥2
H) +
 t
0
c1E(∥X(n)(s ∧τl)∥2
H)e−c1s ds
+ c2
 t
0
E(1[0,τl](s)∥X(n)(s ∧τl)∥α
V )e−c1s ds
⩽E(∥X(n)
0
∥2
H) +
 t
0
c1E(∥X(n)(s)∥2
H)e−c1s ds +
 T
0
E(|f(s)|) ds.
Now taking l →∞and applying Fatou’s lemma we get
E(e−c1t∥X(n)(t)∥2
H) + c2E
 t
0
∥X(n)(s)∥α
V e−c1s ds

⩽E(∥X(n)
0
∥2
H) + E
 T
0
|f(s)| ds

for all t ∈[0, T]. Here we used that by Theorem 4.2.5 (applied to (4.2.21)) the
substracted terms are ﬁnite. Since ∥X(n)
0
∥H ⩽∥X0∥H, now the assertion fol-
lows for the ﬁrst and third summand in (4.2.23). For the remaining summand
the assertion then follows by (H4).
Proof of Theorem 4.2.4. By the reﬂexivity of K, Lemma 4.2.9 and Remark
4.1.1, part 1, we have, for a subsequence nk →∞:
(i) X(nk) →¯X weakly in K and weakly in L2([0, T] × Ω; dt ⊗P; H).
(ii) Y (nk) := A(·, X(nk)) →Y weakly in K∗.

4.2. The main result and an Itˆo formula
87
(iii) Z(nk) := PnkB(·, X(nk)) →Z weakly in J and hence
 ·
0
PnkB(s, X(nk)(s)) dW (nk)(s) →
 ·
0
Z(s) dW(s)
weakly in L∞([0, T], dt; L2(Ω, P; H)) (equipped with the supremum
norm).
Here the second part in (iii) follows since also B(·, X(nk)) ˜Pnk →Z weakly in
J, where ˜Pn is the orthogonal projection onto span{g1, · · · , gn} in U, since
 ·
0
PnkB(s, X(nk)(s)) dW (nk)(s) =
 ·
0
PnkB(s, Xnk(s)) ˜Pnk dW(s)
and since a bounded linear operator between two Banach spaces is trivially
weakly continuous. Since the approximants are progressively measurable, so
are (the dt ⊗P-versions) ¯X, Y and Z.
Thus from (4.2.21) for all v ∈3
n⩾1 Hn, ϕ ∈L∞([0, T] × Ω) by Fubini’s
theorem we get
E
 T
0
V ∗⟨¯X(t), ϕ(t)v⟩V dt

= lim
k→∞E
 T
0
V ∗⟨X(nk)(t), ϕ(t)v⟩V dt

= lim
k→∞E
  T
0
V ∗⟨X(nk)
0
, ϕ(t)v⟩V dt
+
 T
0
 t
0
V ∗⟨PnkY (nk)(s), ϕ(t)v⟩V ds dt
+
 T
0
 t
0
Z(nk)(s) dW (nk)(s), ϕ(t)v

H
dt

= lim
k→∞
1
E

⟨X(nk)
0
, v⟩H
 T
0
ϕ(t) dt

+ E
 T
0
V ∗⟨Y (nk)(s),
 T
s
ϕ(t) dt v⟩
V
ds

+
 T
0
E

ϕ(t)
 t
0
Z(nk)(s) dW (nk)(s), v

H

dt
2
= E
  T
0
V ∗⟨X0 +
 t
0
Y (s) ds +
 t
0
Z(s) dW(s), ϕ(t)v⟩
V
dt

.

88
4. A Class of Stochastic Diﬀerential Equations
Therefore, deﬁning
X(t) := X0 +
 t
0
Y (s) ds +
 t
0
Z(s) dW(s),
t ∈[0, T],
(4.2.25)
we have X = ¯X dt ⊗P-a.e.
Now Theorem 4.2.5 applies to X in (4.2.25), so X is continuous in H and
E

sup
t≤T
∥X(t)∥2
H

< ∞.
Thus, it remains to verify that
B(·, ¯X) = Z,
A(·, ¯X) = Y,
dt ⊗P-a.e..
(4.2.26)
To this end, we ﬁrst note that for any nonnegative ψ ∈L∞([0, T], dt; R) it
follows from (i) that
E
 T
0
ψ(t)∥¯X(t)∥2
H dt

= lim
k→∞E
 T
0
⟨ψ(t) ¯X(t), X(nk)(t)⟩H dt

≤

E
 T
0
ψ(t)∥¯X(t)∥2
H dt
1/2
lim inf
k→∞

E
 T
0
ψ(t)∥X(nk)(t)∥2
H dt
1/2
< ∞.
Since X = ¯X dt ⊗P-a.e., this implies
E
 T
0
ψ(t)∥X(t)∥2
H dt

≤lim inf
k→∞E
 T
0
ψ(t)∥X(nk)(t)∥2
H dt

.
(4.2.27)
By (4.2.25) using Remark 4.2.8 and the product rule we obtain that
E

e−ct∥X(t)∥2
H

−E

∥X0∥2
H

= E
 t
0
e−cs
2 V ∗⟨Y (s), ¯X(s)⟩V + ∥Z(s)∥2
L2(U,H) −c∥X(s)∥2
H

ds

.
(4.2.28)
Furthermore, for any φ ∈K ∩L2([0, T] × Ω, dt ⊗P; H) and taking l →∞in

4.2. The main result and an Itˆo formula
89
(4.2.24) with c1 replaced by c
E
	
e−ct∥X(nk)(t)∥2
H

−E
	
∥X(nk)
0
∥2
H

= E
  t
0
e−cs
2 V ∗⟨A(s, X(nk)(s)), X(nk)(s)⟩V
+ ∥PnkB(s, X(nk)(s)) ˜Pnk∥2
L2(U,H) −c∥X(nk)(s)∥2
H

ds

≤E
  t
0
e−cs
2 V ∗⟨A(s, X(nk)(s)), X(nk)(s)⟩V
+ ∥B(s, X(nk)(s))∥2
L2(U,H) −c∥X(nk)(s)∥2
H

ds

= E
  t
0
e−cs	
2 V ∗⟨A(s, X(nk)(s)) −A(s, φ(s)), X(nk)(s) −φ(s)⟩V
+ ∥B(s, X(nk)(s)) −B(s, φ(s))∥2
L2(U,H) −c∥X(nk)(s) −φ(s)∥2
H

ds
+ E
  t
0
e−cs	
2 V ∗⟨A(s, φ(s)), X(nk)(s)⟩V
+ 2 V ∗⟨A(s, X(nk)(s)) −A(s, φ(s)), φ(s)⟩V
−∥B(s, φ(s))∥2
L2(U,H) + 2⟨B(s, X(nk)(s)), B(s, φ(s))⟩L2(U,H)
−2c⟨X(nk)(s), φ(s)⟩H + c∥φ(s)∥2
H

ds

.
(4.2.29)
Note that by (H2) the ﬁrst of the two summands above is negative. Hence by
letting k →∞we conclude by (i)–(iii), Fubini’s theorem, and (4.2.27) that
for every nonnegative ψ ∈L∞([0, T], dt; R)
E
 T
0
ψ(t)(e−ct∥X(t)∥2
H −∥X0∥2
H) dt

≤E
  T
0
ψ(t)
  t
0
e−cs%
2 V ∗⟨A(s, φ(s)), ¯X(s)⟩V + 2 V ∗⟨Y (s)
−A(s, φ(s)), φ(s)⟩V −∥B(s, φ(s))∥2
L2(U,H) + 2⟨Z(s), B(s, φ(s))⟩L2(U,H)
−2c⟨X(s), φ(s)⟩H + c∥φ(s)∥2
H
&
ds

dt

.

90
4. A Class of Stochastic Diﬀerential Equations
Inserting (4.2.28) for the left-hand side and rearranging as above we arrive at
0 ≥E
  T
0
ψ(t)
  t
0
e−cs
2 V ∗⟨Y (s) −A(s, φ(s)), ¯X(s) −φ(s)⟩V
+ ∥B(s, φ(s)) −Z(s)∥2
L2(U,H) −c∥X(s) −φ(s)∥2
H

ds

dt

.
(4.2.30)
Taking φ = ¯X we obtain from (4.2.30) that Z = B(·, ¯X). Finally, ﬁrst applying
(4.2.30) to φ = ¯X −ε˜φ v for ε > 0 and ˜φ ∈L∞([0, T] × Ω, dt ⊗P; R), v ∈V ,
then dividing both sides by ε and letting ε →0, by Lebesgue’s dominated
convergence theorem, (H1) and (H4), we obtain
0 ≥E
 T
0
ψ(t)
  t
0
e−cs ˜φ(s) V ∗⟨Y (s) −A(s, ¯X(s)), v⟩V ds

dt

.
By the arbitrariness of ψ and ˜φ, we conclude that Y = A(·, ¯X). This completes
the existence proof.
The uniqueness is a consequence of the following proposition.
Proposition 4.2.10. Consider the situation of Theorem 4.2.4 and let X, Y
be two solutions. Then for c ∈R as in (H2)
E(∥X(t) −Y (t)∥2
H) ⩽ectE(∥X(0) −Y (0)∥2
H) for all t ∈[0, T].
(4.2.31)
Proof. We ﬁrst note that by our deﬁnition of solution (cf. Deﬁnition 4.2.1)
and by Remark 4.1.1, part 1 we can apply Remark 4.2.8 to X −Y and obtain
for t ∈[0, T]
E(∥X(t) −Y (t)∥2
H) = E(∥X0 −Y0∥2
H)
+
 t
0
E(2 V ∗⟨A(s, ¯X(s)) −A(s, ¯Y (s)), ¯X(s) −¯Y (s)⟩V
+ ∥B(s, X(s)) −B(s, Y (s))∥2
L2(U,H)) ds
⩽E(∥X0 −Y0∥2
H) + c
 t
0
E(∥X(s) −Y (s)∥2
H) ds,
where we used (H2) in the last step. Applying Gronwall’s lemma we obtain
the assertion.
Remark 4.2.11. Let s ∈[0, T] and Xs ∈L2(Ω, Fs, P; H). Consider the
equation
X(t) = Xs +
 t
s
A(u, ¯X(u)) du +
 t
s
B(u, ¯X(u)) dW(u), t ∈[s, T] (4.2.32)

4.3. Markov property and invariant measures
91
with underlying Wiener process W(t)−W(s), t ∈[s, T], and ﬁltration (Ft)t⩾s,
i.e. we just start our time at s. We deﬁne the notion of solution for (4.2.32)
analogously to Deﬁnition 4.2.1. Then all results above in the case s = 0 carry
over to this more general case. In particular, there exists a unique solution with
initial condition Xs denoted by X(t, s, Xs), t ∈[s, T]. Let 0 ⩽r ⩽s ⩽T.
Then for Xr ∈L2(Ω, Fr, P; H)
X(t, r, Xr) = X(t, s, X(s, r, Xr)), t ∈[s, T] P-a.e.
(4.2.33)
Indeed, we have
X(t, r, Xr) = Xr +
 t
r
A(u, ¯X(u, r, Xr)) du +
 t
r
B(u, ¯X(u, r, Xr)) dW(u)
= X(s, r, Xr) +
 t
s
A(u, ¯X(u, r, Xr)) du
+
 t
s
B(u, ¯X(u, r, Xr)) dW(u),
t ∈[s, T].
But by deﬁnition X(t, s, X(s, r, Xr)), t ∈[s, T], satisﬁes the same equation.
So, (4.2.33) follows by uniqueness. Furthermore, if for s ∈[0, T], Xs = x for
some x ∈H and A and B are independent of ω ∈Ω, then X(t, s, x) obviously
is independent of Fs for all t ∈[s, T], since so are collections of increments
of W(t), t ∈[s, T].
4.3. Markov property and invariant measures
Now we are going to prove some qualitative results about the solutions of
(4.2.1) or (4.2.32) and about their transition probabilities, i.e. about
ps,t(x, dy) := P ◦(X(t, s, x))−1(dy), 0 ⩽s ⩽t ⩽T, x ∈H.
(4.3.1)
As usual we set for B(H)-measurable F : H →R, and t ∈[s, T], x ∈H
ps,tF(x) :=

F(y)ps,t(x, dy),
provided F is ps,t(x, dy)-integrable.
Remark 4.3.1. The measures ps,t(x, dy), 0 ⩽s ⩽t ⩽T, x ∈H, could in
principle depend on the chosen Wiener process and the respective ﬁltration.
However, the construction of our solutions X(t, s, x), t ∈[s, T], suggests that
this is not the case. This can be rigorously proved in several ways. It is e.g. a
consequence of the famous Yamada–Watanabe theorem which is included in
Appendix E below in a ﬁnite-dimensional case, which immediately extends to
inﬁnite dimensions if the underlying Wiener process has covariance of ﬁnite
trace. For the case of a cylindrical Wiener process we refer to [Ond04]. In
these notes we shall use the latter as a fact referring to this remark each time
we do so.

92
4. A Class of Stochastic Diﬀerential Equations
Proposition 4.3.2. Consider the situation of Theorem 4.2.4. Let F : H →R
be Lipschitz with
Lip(F) :=
sup
x,y∈H,x̸=y
|F(x) −F(y)|
∥x −y∥H
(< ∞)
denoting its Lipschitz constant. Then for all 0 ⩽s ⩽t ⩽T
ps,t|F|(x) < ∞for all x ∈H
and for all x, y ∈H
|ps,tF(x) −ps,tF(y)| ⩽e
c
2 (t−s)Lip(F) ∥x −y∥H,
(4.3.2)
where c is as in (H2).
Proof. Clearly, for all x ∈H
|F(x)| ⩽|F(0)| + Lip(F) ∥x∥H,
and thus for all 0 ⩽s ⩽t ⩽T
ps,t|F|(x) = E(|F|(X(t, s, x)))
⩽|F(0)| + Lip(F) E(∥X(t, s, x)∥H)
⩽|F(0)| + Lip(F)

E

sup
t∈[s,T ]
∥X(t, s, x)∥2
H
1/2
< ∞.
Furthermore, for x, y ∈H by (the “started at s” analogue of) (4.2.31)
|ps,tF(x) −ps,tF(y)| ⩽E(|F(X(t, s, x)) −F(X(t, s, y)))|)
⩽Lip(F) E(∥X(t, s, x) −X(t, s, y)∥H)
⩽Lip(F) e
c
2 (t−s)∥x −y∥H.
Proposition 4.3.3. Consider the situation of Theorem 4.2.4 and, in addition,
assume that both A and B as well as f and g in (H3),(H4) respectively, are
independent of ω ∈Ω. Then any solution X(t), t ∈[r, T], of (4.2.32) (with r
replacing s) is Markov in the following sense:
for every bounded, B(H)-measurable F : H →R, and all s, t ∈[r, T], s ⩽t
E(F(X(t))|Fs)(ω) = E(F(X(t, s, X(s)(ω)))) for P-a.e. ω ∈Ω.
(4.3.3)

4.3. Markov property and invariant measures
93
Proof. Clearly, by a monotone class argument we may assume F in (4.3.3)
to be Lipschitz continuous. We ﬁrst note that by Proposition 4.3.2 for all
0 ⩽s ⩽t ⩽T the map
H ∋x 	→E(F(X(t, s, x))) = ps,tF(x)
is Lipschitz on H. So, the right-hand side of (4.3.3) is Fs-measurable. Further-
more, for any bounded Fs-measurable function Fs : Ω→R, applying (4.2.33)
we have
E(FsF(X(t))) = E(FsF(X(t, s, X(s)))).
(4.3.4)
By Lemma A.1.4 there exists a sequence of H-valued Fs-measurable simple
functions
fn : Ω→H, fn =
Nn

k=1
h(n)
k 1{fn=h(n)
k
},
Nn ∈N,
where h(n)
1 , . . . , h(n)
Nn ∈H are pairwise distinct and Ω= 3Nn
k=1{fn = h(n)
k },
such that
∥fn(ω) −X(s)(ω)∥H ↓0 as n →∞for all ω ∈Ω.
Hence again by (the “s-shifted version” of) (4.2.31) the right-hand side of
(4.3.4) is equal to
lim
n→∞E(FsF(X(t, s, fn)))
= lim
n→∞
Nn

k=1
E
	
Fs1{fn=h(n)
k
}F(X(t, s, h(n)
k ))

= lim
n→∞
Nn

k=1
E
	
Fs1{fn=h(n)
k
}

E
	
F(X(t, s, h(n)
k ))

= lim
n→∞E

Fs
Nn

k=1
1{fn=h(n)
k
}E
	
F(X(t, s, h(n)
k ))


= lim
n→∞

Fs(ω)E(F(X(t, s, fn(ω))))P(dω)
=

Fs(ω)E(F(X(t, s, X(s)(ω))))P(dω),
where we used the last part of Remark 4.2.11 for the ﬁrst and again (4.2.31)
for the last equality. Now the assertion follows.
Corollary 4.3.4. Consider the situation of Proposition 4.3.3 and let 0 ⩽r ⩽
s ⩽t ⩽T. Then
pr,sps,t = pr,t,
(4.3.5)

94
4. A Class of Stochastic Diﬀerential Equations
i.e. for F : H →R, bounded and B(H)-measurable, x ∈H,
pr,s(ps,tF)(x) = pr,tF(x).
Proof. For F : H →R as above and x ∈H by Proposition 4.3.3 we have
pr,s(ps,tF)(x) = E(ps,tF(X(s, r, x))) =

E(F(X(t, s, X(s, r, x)(ω))))P(dω)
=

E(F(X(t, r, x))|Fs)(ω)P(dω)
= E(F(X(t, r, x))) = pr,tF(x).
Now let us assume that in the situation of Theorem 4.2.4 both A and B as
well as f and g in (H3), (H4) respectively are independent of (t, ω) ∈[0, T]×Ω
(so they particularly hold for all T ∈[0, ∞[). Then again using the notation
introduced in Remark 4.2.11 for 0 ⩽s ⩽t < ∞and x ∈H we have
X(t, s, x) = X
˜
W (t −s, 0, x) P-a.e.,
(4.3.6)
where X ˜
W (t, 0, x), t ∈[0, ∞[, is the solution of
X(t) = x +
 t
0
A( ¯X(u)) du +
 t
0
B( ¯X(u)) d ˜W(u)
and ˜W := W(· + s) −W(s) with ﬁltration Fs+u, u ∈[0, ∞[, which is again
a Wiener process. To show this let us express the dependence of the solution
X(t, s, x), s ∈[t, ∞) of (4.2.32) with Xs := x on the Wiener process W by
writing XW (t, s, x) instead of X(t, s, x) and similarly, pW
s,t(s, dy) instead of
ps,t(x, dy). Then, for all 0 ⩽s ⩽t < ∞
XW ((t −s) + s, s, x)
=XW (t, s, x)
=x +
 t
s
A( ¯XW (u, s, x)) du +
 t
s
B( ¯XW (u, s, x)) dW(u)
=x +
 t−s
0
A( ¯XW (u + s, s, x)) du +
 t−s
0
B( ¯XW (u + s, s, x)) d ˜W(u),
So, by uniqueness the process XW (u+s, s, x), u ∈[0, ∞[, must P-a.e. coincide
with X ˜
W (u, 0, x), u ∈[0, ∞[. In particular, it follows by Remark 4.3.1 that
pW
s,t(x, dy) = P ◦(X
˜
W (t −s, 0, x))−1(dy) = p
˜
W
0,t−s(x, dy) = pW
0,t−s(x, dy)
(4.3.7)

4.3. Markov property and invariant measures
95
(“time homogeneity”), where we used Remark 4.3.1 for the last equality.
Deﬁning
pt := pW
0,t,
t ∈[0, ∞[,
equality (4.3.5) for r = 0 and s + t replacing t turns into
ps+t = pspt for s, t ∈[0, ∞[.
(4.3.8)
For x ∈H we deﬁne
Px := P ◦(X(·, 0, x))−1,
(4.3.9)
i.e. Px is the distribution of the solution to (4.2.1) with initial condition x ∈H,
deﬁned as a measure on C([0, ∞[, H). We equip C([0, ∞[, H) with the σ-
algebra
G := σ(πs|s ∈[0, ∞[)
and ﬁltration
Gt := σ(πs|s ∈[0, t]), t ∈[0, ∞[,
where πt(w) := w(t) for w ∈C([0, ∞[, H), t ∈[0, ∞[.
Proposition 4.3.5. Consider the situation of Theorem 4.2.4 and, in addition,
assume that both A and B as well as f and g in (H3),(H4) respectively, are
independent of (t, ω) ∈[0, T]×Ω(so they particularly hold for all T ∈[0, ∞[).
Then the following assertions hold:
1. Px, x ∈H, form a time-homogenous Markov process on C([0, ∞), H)
with respect to the ﬁltration Gt, t ∈[0, ∞[, i.e. for all s, t ∈[0, ∞[, and
all bounded, B(H)-measurable F : H →R
Ex(F(πt+s)|Gs) = Eπs(F(πt))
Px −a.e.,
(4.3.10)
where Ex and Ex(·|Gs) denote expectation, conditional expectation with
respect to Px respectively.
2. Suppose dim H < ∞. If there exist η, f ∈]0, ∞[ such that
2 V ∗⟨A(v), v⟩V +∥B(v)∥2
L2(U,H) ⩽−η∥v∥2
H + f
for all v ∈V, (4.3.11)
(“strict coercivity”) then there exists an invariant measure µ for (pt)t⩾0,
i.e. µ is a probability measure on (H, B(H)) such that

ptF dµ =

F dµ
for all t ∈[0, ∞[
(4.3.12)
and all bounded, B(H)-measurable F : H →R.
Proof.
1. The right-hand side of (4.3.10) is Gs-measurable by Proposition
4.3.2 and a monotone class argument. So, let 0 ⩽t1 < t2 < . . . < tn ⩽s

96
4. A Class of Stochastic Diﬀerential Equations
and let G : Hn →R be bounded and ⊗n
i=1B(H)-measurable. Then by
(4.3.3) and (4.3.6)
Ex(G(πt1, . . . , πtn)F(πt+s))
= E(G(X(t1, 0, x), . . . , X(tn, 0, x))F(X(t + s, 0, x))
= E(G(X(t1, 0, x), . . . , X(tn, 0, x))E(F(X(t + s, 0, x))|Fs))
=

G(X(t1, 0, x)(ω), . . . , X(tn, 0, x)(ω))
E(F(X(t + s, s, X(s, 0, x)(ω))))P(dω)
=

G(X(t1, 0, x)(ω), . . . , X(tn, 0, x)(ω))
E(F(X(t, 0, X(s, 0, x)(ω))))P(dω)
=

G(πt1(ω), . . . , πtn(ω))E(F(X(t, 0, πs(ω))))Px(dω)
=

G(πt1(ω), . . . , πtn(ω))Eπs(ω)(F(πt))Px(dω).
Since the functions G(πt1, . . . , πtn) considered above generate Fs, equal-
ity (4.3.10) follows.
2. Let δ0 be the Dirac measure in 0 ∈H considered as a measure on
(H, B(H)) and for n ∈N deﬁne the Krylov–Bogoliubov measure
µn := 1
n
 n
0
δ0pt dt,
i.e. for B(H)-measurable F : H →[0, ∞[

F dµn = 1
n
 n
0
ptF(0) dt.
Clearly, each µn is a probability measure. We ﬁrst prove that {µn|n ∈N}
is tight. By Remark 4.2.8 for any solution X to (4.2.1) applying the
product rule and using (4.3.11) we get that
E(eηt∥X(t)∥2
H) = E(∥X(0)∥2
H) + E
  t
0
eηs
2 V ∗⟨A( ¯X(s)), ¯X(s)⟩V
+ ∥B( ¯X(s))∥2
L2(U,H) + η∥¯X(s)∥2
H

ds

⩽E(∥X(0)∥2
H) + f
 t
0
eηs ds,
t ∈[0, ∞[.

4.3. Markov property and invariant measures
97
Therefore,
E(∥X(t)∥2
H) ⩽e−ηtE(∥X(0)∥2
H) + f
η ,
t ∈[0, ∞[,
(4.3.13)
which in turn implies that

∥x∥2
Hµn(dx) = 1
n
 n
0
E(∥X(t, 0, 0)∥2
H) dt ⩽f
η
for all n ∈N.
(4.3.14)
Hence by Chebychev’s inequality
sup
n∈N
µn({∥·∥2
H > R}) ⩽1
R
f
η →0 as R →∞.
(4.3.15)
Since dim H < ∞, the closed balls {∥·∥2
H ⩽R}, R ∈]0, ∞[, are compact.
Hence by Prohorov’s theorem there exists a probability measure µ and
a subsequence (µnk)k∈N such that µnk →µ weakly as k →∞.
Now let us prove that µ is invariant for (pt)t⩾0. So, let t ∈[0, ∞[ and
let F : H →R be bounded and B(H)-measurable. By a monotone class
argument we may assume that F is Lipschitz continuous. Then ptF is
bounded and (Lipschitz) continuous by Proposition 4.3.2. Hence using
(4.3.8) for the third equality below, we obtain

ptF dµ
= lim
k→∞

ptF dµnk
= lim
k→∞
1
nk
 nk
0
ps(ptF)(0) ds
= lim
k→∞
1
nk
 nk
0
ps+tF(0) ds
= lim
k→∞

F dµnk + lim
k→∞
1
nk
 nk+t
nk
psF(0) ds −lim
k→∞
1
nk
 t
0
psF(0) ds
=

F dµ,
(4.3.16)
since |psF(0)| ⩽supx∈H |F(x)|, so the second and third limits above are
equal to zero.
Remark 4.3.6. If dim H = ∞, the above proof of Proposition 4.3.5, part 2
works up to and including (4.3.15). However, since closed balls are no longer

98
4. A Class of Stochastic Diﬀerential Equations
compact, one can apply Prohorov’s theorem only on a Hilbert space H1 into
which H is compactly embedded. So, let H1 be a separable Hilbert space such
that H ⊂H1 compactly and densely (e.g. take H1 to be the completion of H
in the norm
∥x∥1 :=
4 ∞

i=1
αi⟨x, ei⟩2
H
51/2
, x ∈H,
where αi ∈]0, ∞[, ∞
i=1 αi < ∞, and {ei|i ∈N} is an orthonormal basis of
H); extending the measures µn by zero to B(H1) we obtain that {µn|n ∈N} is
tight on H1. This extension of the measures is possible, since by Kuratowski’s
theorem H ∈B(H1) and B(H1) ∩H = B(H). Hence by Prohorov’s theorem
there exists a probability measure ¯µ on (H1, B(H1)) and a subsequence (µnk)k∈N
such that µnk →¯µ weakly on H1 as k →∞. As in Exercise 4.2.3, part 1 one
constructs a lower semicontinuous function Θ : H1 →[0, ∞] such that
Θ :=

∥·∥H
on H
+∞
on H1\H.
Then (4.3.14) implies that for li, i ∈N, as in Example 4.2.3, part 1,

H1
Θ2(x)¯µ( dx) = lim
N→∞lim
M→∞

sup
i⩽N
⟨li, x⟩2
H1 ∧M ¯µ(dx)
=
sup
M,N∈N
lim
k→∞

sup
i⩽N
⟨li, x⟩2
H1 ∧Mµnk(dx)
⩽lim inf
k→∞
sup
N,M∈N

sup
i⩽N
⟨li, x⟩2
H1 ∧Mµnk(dx)
= lim inf
k→∞

H
∥x∥2
Hµnk(dx)
⩽f
η .
Hence Θ < ∞¯µ-a.e., so ¯µ(H) = 1. Therefore, µ := ¯µ

B(H) is a probability
measure on (H, B(H)).
Unfortunately, the part of the proof of Proposition 4.3.5, part 2 above, which
shows that µ is invariant, does not work. More precisely, for the ﬁrst equality
in (4.3.16) we need that ptF is continuous with respect to the same topology
with respect to which (µnk)k∈N converges weakly, i.e. the topology on H1. This
one is, however, weaker than that on H. So, unless we can construct H1 in
such a way that ptF has a continuous extension to H1, the ﬁrst equality in
(4.3.16) may not hold.
So far, we have taken a positive time s as the starting time for our SDE
(see Remark 4.2.11). In the case of coeﬃcients independent of t and ω, it
is possible and convenient to consider negative starting times also. For this

4.3. Markov property and invariant measures
99
we, however, need a Wiener process with negative time. To this end we re-
call that we can run a cylindrical Wiener process W(t), t ∈[0, ∞[ on H
(with positive time) backwards in time and get again a Wiener process.
More precisely, for ﬁxed T ∈[0, ∞[ we have that W(T −t) −W(T), t ∈
[0, T] is again a cylindrical Wiener process with respect to the ﬁltration
σ({W(T −s) −W(T)|s ∈[0, t]}), t ∈[0, T], and also with respect to the
ﬁltration σ({W(r2) −W(r1)|r1, r2 ∈[T −t, ∞[, r2 ⩽r1}), t ∈[0, T], where
the latter will be more convenient for us.
So, let A, B be independent of (t, ω) ∈[0, T]×Ωand let W (1)(t), t ∈[0, ∞[,
be another cylindrical Wiener process on (Ω, F, P) with covariance operator
Q = I, independent of W(t), t ∈[0, ∞[. Deﬁne
¯W(t) :=
 W(t),
if t ∈[0, ∞[,
W (1)(−t),
if t ∈] −∞, 0]
(4.3.17)
with ﬁltration
¯Ft :=
 
s>t
¯F◦
s ,
t ∈R,
(4.3.18)
where ¯F◦
s := σ({ ¯W(r2) −¯W(r1)|r1, r2 ∈] −∞, s], r2 ⩾r1}, N) and N :=
{A ∈F|P(A) = 0}. As in the proof of Proposition 2.1.13 one shows that if
−∞< s < t < ∞, then ¯W(t) −¯W(s) is independent of ¯Fs. Now for s ∈R
ﬁxed consider the SDE
dX(t) = A(X(t)) dt + B(X(t)) d ¯W(t), t ∈[s, ∞[.
(4.3.19)
Remark 4.3.7. Let s ∈R and Xs ∈L2(Ω, ¯Fs, P; H) and consider the integral
version of (4.3.19)
X(t) = Xs +
 t
s
A( ¯X(u)) du +
 t
s
B( ¯X(u)) d ¯W(u),
t ∈[s, ∞[,
(4.3.20)
with underlying Wiener process ¯W(t)−¯W(s), t ∈[s, ∞[ and ﬁltration ( ¯Ft)t⩾s
(cf. Remark 4.2.11). We deﬁne the notion of solution for (4.3.20) analogously
to Deﬁnition 4.2.1. Then again all results above for s = 0 (respectively for s ∈
[0, ∞[, see Remark 4.2.11) carry over to this more general case. In particular,
we have the analogue of (4.3.5), namely
pr,sps,t = pr,t
for all −∞< r ⩽s ⩽t < ∞,
(4.3.21)
where for s, t ∈R, s ⩽t, x ∈H
ps,t(x, dy) := P ◦(X(t, s, x))−1(dy),
and analogously to (4.3.7) one shows that
ps,t(x, dy) = p0,t−s(x, dy).

100
4. A Class of Stochastic Diﬀerential Equations
In particular, for t = 0 we have
p−s,0(x, dy) = p0,s(x, dy)
for all x ∈H, s ∈[0, ∞[.
(4.3.22)
Furthermore, for every s ∈R there exists a unique solution with initial con-
dition Xs denoted by X(t, s, Xs), t ∈[s, ∞[, and (4.2.33) as well as the ﬁnal
part of Remark 4.2.11 hold also in this case.
Our next main aim (cf. Theorem 4.3.9 below) is to prove the existence of
a unique invariant measure for (4.3.19) if the constant c in (H2) is strictly
negative (“strict monotonicity”). The method of the proof is an adaptation
from [DPZ96, Subsection 6.3.1]. We shall need the following:
Lemma 4.3.8. Suppose (H3), (H4) hold and that (H2) holds for c := −λ for
some λ ∈]0, ∞[. Let η ∈]0, λ[. Then there exists δη ∈]0, ∞[ such that for all
v ∈V
2 V ∗⟨A(v), v⟩V +∥B(v)∥2
L2(U,H) ⩽−η∥v∥2
H + δη.
(4.3.23)
Proof. Let v ∈V and ε ∈]0, 1[. Then using (H2) ﬁrst (with c = −λ according
to our assumption), then Remark 4.1.1, part 1 and ﬁnally (H3) we obtain
2 V ∗⟨A(v), v⟩V +∥B(v)∥2
L2(U,H)
= 2 V ∗⟨A(v) −A(0), v⟩V +2 V ∗⟨A(0), v⟩V +∥B(v) −B(0)∥2
L2(U,H)
−∥B(0)∥2
L2(U,H) + 2⟨B(v), B(0)⟩L2(U,H)
⩽−λ∥v∥2
H + 2ε∥v∥α
V + 2ε−
1
α−1 (α −1)α
−α
α−1 ∥A(0)∥
α
α−1
V ∗
+ ε−1∥B(0)∥2
L2(U,H)
+ ε∥B(v)∥2
L2(U,H)
⩽−λ∥v∥2
H + 2ε∥v∥α
V + βε
+ ε

c1∥v∥2
H + f + 2
α∥v∥α
V + 2α −1
α
g
α
α−1 + 2c3∥v∥α
V

⩽
1
−λ + εc1

1 + 2
c2
(1 + α−1 + c3)
2
∥v∥2
H + ˜βε + 2
c2
ε(1 + α−1 + c3)f
−2
c2
ε(1 + α−1 + c3)(2 V ∗⟨A(v), v⟩V +∥B(v)∥2
L2(U,H))
with βε, ˜βε ∈]0, ∞[ independent of v and where we applied Young’s inequality
in the form
ab = [(αε)−1/αa][(αε)1/αb] ⩽(αε)−1/(α−1)
α/(α −1) aα/(α−1) + εbα,
a, b ∈[0, ∞[ in the second step. Hence taking ε small enough we can ﬁnd
δη ∈]0, ∞[ such that for all v ∈V
2 V ∗⟨A(v), v⟩V +∥B(v)∥2
L2(U,H) ⩽−η∥v∥2
H + δη.

4.3. Markov property and invariant measures
101
Theorem 4.3.9. Consider the situation of Proposition 4.3.5 and, in addition,
assume that c ∈R in (H2) is strictly negative, i.e. c = −λ, λ ∈]0, ∞[ (“strict
monotonicity”). Then there exists an invariant measure µ for (pt)t⩾0 such
that

∥y∥2
Hµ(dy) < ∞.
Moreover, for F : H →R Lipschitz, x ∈H and any invariant measure µ for
(pt)t⩾0
|ptF(x) −

F dµ| ⩽e−λ
2 tLip(F)

∥x −y∥Hµ(dy)
for all t ∈[0, ∞[.
(4.3.24)
In particular, there exists exactly one invariant measure for (pt)t⩾0 with the
property that

∥y∥Hµ(dy) < ∞.
Remark 4.3.10. (4.3.24) is referred to as “exponential convergence of (pt)t⩾0
to equilibrium” (uniformly with respect to x in balls in H).
For the proof of Theorem 4.3.9 we need one lemma.
Lemma 4.3.11. Consider the situation of Theorem 4.3.9. Let t ∈R. Then
there exists ηt ∈L2(Ω, F, P; H), such that for all x ∈H
lim
s→−∞X(t, s, x) = ηt in L2(Ω, F, P; H).
Moreover, there exists C ∈[0, ∞[ such that for all s ∈] −∞, t]
E(∥X(t, s, x) −ηt∥2
H) ⩽Ceλ(s−t)(1 + ∥x∥2
H).
Proof. For s1, s2 ∈] −∞, t], s1 ⩽s2, and x ∈H
X(t, s1, x) −X(t, s2, x)
=
 t
s2
[A( ¯X(u, s1, x)) −A( ¯X(u, s2, x))] ds
+
 t
s2
[B( ¯X(u, s1, x)) −B( ¯X(u, s2, x))] d ¯W(u) + X(s2, s1, x) −x,
since
X(s2, s1, x) = x +
 s2
s1
A( ¯X(u, s1, x)) du +
 s2
s1
B( ¯X(u, s1, x)) d ¯W(u).
(4.3.25)

102
4. A Class of Stochastic Diﬀerential Equations
Since Remark 4.2.8 extends to our present case we can use the product rule
and (H2) with c = −λ to obtain
E(eλt∥X(t, s1, x) −X(t, s2, x)∥2
H) = E(eλs2∥X(s2, s1, x) −x∥2
H)
+
t

s2
eλuE
	
2 V ∗⟨A( ¯X(u, s1, x)) −A( ¯X(u, s2, x)), ¯X(u, s1, x) −¯X(u, s1, x)⟩V
+ ∥B( ¯X(u, s1, x)) −B( ¯X(u, s2, x))∥2
L2(U,H)

du
+
t

s2
eλuλE

∥X(u, s1, x) −X(u, s2, x)∥2
H

du
⩽2eλs2[E

∥X(s2, s1, x)∥2
H) + ∥x∥2
H

.
(4.3.26)
But again by Remark 4.2.8 extended to the present case, the product rule and
(4.3.23) imply
E(eηs2∥X(s2, s1, x)∥2
H)
= es1η∥x∥2
H +
 s2
s1
eηuE
	
2 V ∗⟨A( ¯X(u, s1, x)), ¯X(u, s1, x)⟩V
+ ∥B( ¯X(u, s1, x))∥2
L2(U,H)

du +
 s2
s1
eηuηE(∥X(u, s1, x)∥2
H) du
⩽es1η∥x∥2
H + δη
 s2
s1
eηu du ⩽es1η∥x∥2
H + δη
η es2η.
(4.3.27)
Combining (4.3.26) and (4.3.27) we obtain
E(∥X(t, s1, x) −X(t, s2, x)∥2
H) ⩽2
δη
η + 2∥x∥2
H

eλ(s2−t).
(4.3.28)
Letting
s2
(hence
s1)
tend
to
−∞,
it
follows
that
there
exists
ηt(x) ∈L2(Ω, F, P; H) such that
lim
s→−∞X(t, s, x) = ηt(x) in L2(Ω, F, P; H),
and letting s1 →−∞in (4.3.28) the last part of the assertion follows also,
provided we can prove that ηt(x) is independent of x ∈H. To this end let

4.3. Markov property and invariant measures
103
x, y ∈H and s ∈] −∞, t]. Then
X(t, s, x) −X(t, s, y)
=x −y +
 t
s
(A( ¯X(u, s, x)) −A( ¯X(u, s, y)) du
+
 t
s
(B( ¯X(u, s, x)) −B( ¯X(u, s, y)) d ¯W(u).
Hence by the same arguments to derive (4.3.26) we get
E(eλt∥X(t, s, x) −X(t, s, y)∥2
H) ⩽eλs∥x −y∥2
H,
so
lim
s→−∞(X(t, s, x) −X(t, s, y)) = 0 in L2(Ω, F, P; H).
Hence both assertions are completely proved.
Proof of Theorem 4.3.9. Deﬁne
µ := P ◦η−1
0
with η0 as in Lemma 4.3.11. Since η0 ∈L2(Ω, F, P; H) we have that

∥y∥2
Hµ(dy) < ∞.
Let t ∈[0, ∞[. We note that by (4.3.21) and (4.3.22) for all s ∈[0, ∞[
p−s,0p0,t = p−s,t = p0,t+s = p−(t+s),0.
(4.3.29)
Let F : H →R, F bounded and Lipschitz. Then by Proposition 4.3.2 we have
that p0,tF is (bounded and) Lipschitz. Furthermore, by Lemma 4.3.11 for all
x ∈H
p−s,0(x, dy) →µ weakly as s →∞.
Hence by (4.3.29) for all x ∈H

p0,tF dµ = lim
s→∞p−s,0(p0,tF)(x) = lim
s→∞p−(t+s),0F(x) =

F dµ.
Recalling that by deﬁnition pt = p0,t, it follows that µ is an invariant measure
for (pt)t⩾0. Furthermore, if µ is an invariant measure for (pt)t⩾0, then by
Proposition 4.3.2 for all t ∈[0, ∞[
ptF(x) −

F dµ
 =


(ptF(x) −ptF(y))µ(dy)

⩽e−λ
2 tLip(F)

∥x −y∥Hµ(dy).

A. The Bochner Integral
This chapter is a slight modiﬁcation of Chap. A in [FK01].
Let

X, ∥∥

be a Banach space, B(X) the Borel σ-ﬁeld of X and (Ω, F, µ)
a measure space with ﬁnite measure µ.
A.1. Deﬁnition of the Bochner integral
Step 1:
As ﬁrst step we want to deﬁne the integral for simple functions
which are deﬁned as follows. Set
E :=

f : Ω→X
 f =
n

k=1
xk1Ak, xk ∈X, Ak ∈F, 1 ⩽k ⩽n, n ∈N

and deﬁne a semi-norm ∥∥E on the vector space E by
∥f∥E :=

∥f∥dµ,
f ∈E.
To get that

E, ∥∥E

is a normed vector space we consider equivalence classes
with respect to ∥∥E. For simplicity we will not change the notations.
For f ∈E, f = n
k=1 xk1Ak, Ak’s pairwise disjoint (such a representation
is called normal and always exists, because f = n
k=1 xk1Ak, where f(Ω) =
{x1, . . . , xk}, xi ̸= xj, and Ak := {f = xk}) and we now deﬁne the Bochner
integral to be

f dµ :=
n

k=1
xkµ(Ak).
(Exercise: This deﬁnition is independent of representations, and hence linear.)
In this way we get a mapping
int :

E, ∥∥E

→

X, ∥∥

f
	→

f dµ
which is linear and uniformly continuous since
'
f dµ
 ⩽
'
∥f∥dµ for all
f ∈E.
Therefore we can extend the mapping int to the abstract completion of E
with respect to ∥∥E which we denote by E.
105

106
A. The Bochner Integral
Step 2:
We give an explicit representation of E.
Deﬁnition A.1.1. A function f : Ω→X is called strongly measurable if it
is F/B(X)-measurable and f(Ω) ⊂X is separable.
Deﬁnition A.1.2. Let 1 ⩽p < ∞. Then we deﬁne
Lp(Ω, F, µ; X) := Lp(µ; X)
:=

f : Ω→X
 f is strongly measurable with
respect to F, and

∥f∥p dµ < ∞

and the semi-norm
∥f∥Lp :=

∥f∥p dµ
 1
p
,
f ∈Lp(Ω, F, µ; X).
The space of all equivalence classes in Lp(Ω, F, µ; X) with respect to ∥∥Lp is
denoted by Lp(Ω, F, µ; X) := Lp(µ; X).
Claim:
L1(Ω, F, µ; X) = E.
Step 2.a:

L1(Ω, F, µ; X), ∥∥L1
is complete.
The proof is just a modiﬁcation of the proof of the Fischer–Riesz theorem
by the help of the following proposition.
Proposition A.1.3. Let (Ω, F) be a measurable space and let X be a Banach
space. Then:
(i) the set of F/B(X)-measurable functions from Ωto X is closed under
the formation of pointwise limits, and
(ii) the set of strongly measurable functions from Ωto X is closed under the
formation of pointwise limits.
Proof. Simple exercise or see [Coh80, Proposition E.1, p. 350].
Step 2.b:
E is a dense subset of L1(Ω, F, µ; X) with respect to ∥∥L1.
This can be shown by the help of the following lemma.
Lemma A.1.4. Let E be a metric space with metric d and let f : Ω→E
be strongly measurable. Then there exists a sequence fn, n ∈N, of simple E-
valued functions (i.e. fn is F/B(E)-measurable and takes only a ﬁnite number
of values) such that for arbitrary ω ∈Ωthe sequence d

fn(ω), f(ω)

, n ∈N,
is monotonely decreasing to zero.

A.2. Properties of the Bochner integral
107
Proof. [DPZ92, Lemma 1.1, p. 16] Let {ek | k ∈N} be a countable dense
subset of f(Ω). For m ∈N deﬁne
dm(ω) := min

d

f(ω), ek
  k ⩽m


= dist(f(ω), {ek, k ⩽m})

,
km(ω) := min

k ⩽m
 dm(ω) = d

f(ω), ek

,
fm(ω) := ekm(ω).
Obviously fm, m ∈N, are simple functions since they are F/B(E)-measurable
(exercise) and
fm(Ω) ⊂{e1, e2, . . . , em}.
Moreover, by the density of {ek | k ∈N}, the sequence dm(ω), m ∈N, is
monotonically decreasing to zero for arbitrary ω ∈Ω. Since d

fm(ω), f(ω)

=
dm(ω) the assertion follows.
Let now f ∈L1(µ; X). By the Lemma A.1.4 above we get the existence of
a sequence of simple functions fn, n ∈N, such that
fn(ω) −f(ω)
 ↓0
for all ω ∈Ωas n →∞.
Hence fn
n→∞
−−−−→f in ∥∥L1 by Lebesgue’s dominated convergence theorem.
A.2. Properties of the Bochner integral
Proposition A.2.1 (Bochner inequality). Let f ∈L1(Ω, F, µ; X). Then


f dµ
 ⩽

∥f∥dµ.
Proof. We know the assertion is true for f ∈E, i.e. int : E →X is linear,
continuous with ∥int f∥⩽∥f∥E for all f ∈E, so the same is true for its unique
continuous extension int : E = L1(µ; X) →X, i.e. for all f ∈L1(X, µ)


f dµ
 =
intf
 ⩽∥f∥E =

∥f∥dµ.
Proposition A.2.2. Let f ∈L1(Ω, F, µ; X). Then

L ◦f dµ = L

f dµ

holds for all L ∈L(X, Y ), where Y is another Banach space.
Proof. Simple exercise or see [Coh80, Proposition E.11, p. 356].

108
A. The Bochner Integral
Proposition A.2.3 (Fundamental theorem of calculus). Let −∞< a <
b < ∞and f ∈C1
[a, b]; X

. Then
f(t) −f(s) =
 t
s
f ′(u) du :=

'
1[s,t](u)f ′(u) du
if s ⩽t
−
'
1[t,s](u)f ′(u) du
otherwise
for all s, t ∈[a, b] where du denotes the Lebesgue measure on B(R).
Proof. Claim 1: If we set F(t) =
' t
s f ′(u) du, t ∈[a, b], we get that F ′(t) =
f ′(t) for all t ∈[a, b].
For that we have to prove that

1
h

F(t + h) −F(t)

−f ′(t)

X
h→0
−−−→0.
To this end we ﬁx t ∈[a, b] and take an arbitrary ε > 0. Since f ′ is continuous
on [a, b] there exists δ > 0 such that
f ′(u) −f ′(t)

X < ε for all u ∈[a, b]
with |u −t| < δ. Then we obtain that

1
h

F(t + h) −F(t)

−f ′(t)

X
=

1
h
 t+h
t

f ′(u) −f ′(t)

du

X
⩽1
h
 t+h
t
f ′(u) −f ′(t)

X du < ε
if t + h ∈[a, b] and |h| < δ.
Claim 2: If ˜F ∈C1
[a, b]; X

is a further function with ˜F ′ = F ′ = f ′ then
there exists a constant c ∈X such that F −˜F = c.
For all L ∈X∗= L(X, R) we deﬁne gL := L(F −˜F). Then g′
L = 0 and
therefore gL is constant. Since X∗separates the points of X by the Hahn–
Banach theorem (see [Alt92, Satz 4.2, p. 114]) this implies that F −˜F itself
is constant.

B. Nuclear and Hilbert–Schmidt
Operators
This chapter is identical to Chap. B in [FK01].
Let

U, ⟨, ⟩U

and

H, ⟨, ⟩

be two separable Hilbert spaces. The space of
all bounded linear operators from U to H is denoted by L(U, H); for simplicity
we write L(U) instead of L(U, U). If we speak of the adjoint operator of
L ∈L(U, H) we write L∗∈L(H, U). An element L ∈L(U) is called symmetric
if ⟨Lu, v⟩U = ⟨u, Lv⟩U for all u, v ∈U. In addition, L ∈L(U) is called
nonnegative if ⟨Lu, u⟩⩾0 for all u ∈U.
Deﬁnition B.0.1 (Nuclear operator). An element T ∈L(U, H) is said to
be a nuclear operator if there exists a sequence (aj)j∈N in H and a sequence
(bj)j∈N in U such that
Tx =
∞

j=1
aj⟨bj, x⟩U
for all x ∈U
and

j∈N
∥aj∥· ∥bj∥U < ∞.
The space of all nuclear operators from U to H is denoted by L1(U, H).
If U = H, T ∈L1(U, H) is nonnegative and symmetric, then T is called trace
class.
Proposition B.0.2. The space L1(U, H) endowed with the norm
∥T∥L1(U,H) := inf

j∈N
∥aj∥· ∥bj∥U
 Tx =
∞

j=1
aj⟨bj, x⟩U, x ∈U

is a Banach space.
Proof. [MV92, Corollar 16.25, p. 154].
Deﬁnition B.0.3. Let T ∈L(U) and let ek, k ∈N, be an orthonormal basis
of U. Then we deﬁne
tr T :=

k∈N
⟨Tek, ek⟩U
if the series is convergent.
109

110
B. Nuclear and Hilbert–Schmidt Operators
One has to notice that this deﬁnition could depend on the choice of the
orthonormal basis. But there is the following result concerning nuclear oper-
ators.
Remark B.0.4. If T ∈L1(U) then tr T is well-deﬁned independently of the
choice of the orthonormal basis ek, k ∈N. Moreover we have that
|tr T| ⩽∥T∥L1(U).
Proof. Let (aj)j∈N and (bj)j∈N be sequences in U such that
Tx =

j∈N
aj⟨bj, x⟩U
for all x ∈U and

j∈N
∥aj∥U · ∥bj∥U < ∞.
Then we get for any orthonormal basis ek, k ∈N, of U that
⟨Tek, ek⟩U =

j∈N
⟨ek, aj⟩U · ⟨ek, bj⟩U
and therefore

k∈N
⟨Tek, ek⟩U
 ⩽

j∈N

k∈N
⟨ek, aj⟩U · ⟨ek, bj⟩U

⩽

j∈N
	
k∈N
⟨ek, aj⟩U
2
 1
2 ·
	
k∈N
⟨ek, bj⟩U
2
 1
2
=

j∈N
∥aj∥U · ∥bj∥U < ∞.
This implies that we can exchange the summation to get that

k∈N
⟨Tek, ek⟩U =

j∈N

k∈N
⟨ek, aj⟩U · ⟨ek, bj⟩U =

j∈N
⟨aj, bj⟩U,
and the assertion follows.
Deﬁnition B.0.5 (Hilbert–Schmidt operator). A bounded linear opera-
tor T : U →H is called Hilbert–Schmidt if

k∈N
∥Tek∥2 < ∞
where ek, k ∈N, is an orthonormal basis of U.
The space of all Hilbert–Schmidt operators from U to H is denoted by
L2(U, H).

B. Nuclear and Hilbert–Schmidt Operators
111
Remark B.0.6.
(i) The deﬁnition of Hilbert–Schmidt operator and the
number
∥T∥2
L2(U,H) :=

k∈N
∥Tek∥2
does not depend on the choice of the orthonormal basis ek, k ∈N, and we
have that ∥T∥L2(U,H) = ∥T ∗∥L2(H,U). For simplicity we also write ∥T∥L2
instead of ∥T∥L2(U,H).
(ii) ∥T∥L(U,H) ⩽∥T∥L2(U,H).
(iii) Let G be another Hilbert space and S1 ∈L(H, G), S2 ∈L(G, U), T ∈
L2(U, H). Then S1T ∈L2(U, G) and TS2 ∈L2(G, H) and
∥S1T∥L2(U,G) ⩽∥S1∥L(H,G)∥T∥L2(U,H),
∥TS2∥L2(G,H) ⩽∥T∥L(U,H)∥S2∥L2(G,U).
Proof.
(i) If ek, k ∈N, is an orthonormal basis of U and fk, k ∈N, is an
orthonormal basis of H we obtain by the Parseval identity that

k∈N
∥Tek∥2 =

k∈N

j∈N
⟨Tek, fj⟩
2 =

j∈N
∥T ∗fj∥2
U
and therefore the assertion follows.
(ii) Let x ∈U and fk, k ∈N, be an orthonormal basis of H. Then we get
that
∥Tx∥2 =

k∈N
⟨Tx, fk⟩2 ⩽∥x∥2
U

k∈N
∥T ∗fk∥2
U = ∥T∥2
L2(U,H) · ∥x∥2
U.
(iii) Let ek, k ∈N be an orthonormal basis of U. Then

k∈N
∥S1Tek∥2
G ⩽∥S1∥2
L(H,G)∥T∥2
L2(U,H).
Furthermore, since (TS2)∗= S∗
2T ∗, it follows that by the above and (i)
that TS2 ∈L2(G, H) and
∥TS2∥L2(G,H) = ∥(TS2)∗∥L2(H,G)
= ∥S∗
2T ∗∥L2(H,G)
⩽∥S2∥L(G,U) · ∥T∥L2(U,H).

112
B. Nuclear and Hilbert–Schmidt Operators
Proposition B.0.7. Let S, T ∈L2(U, H) and let ek, k ∈N, be an orthonor-
mal basis of U. If we deﬁne
⟨T, S⟩L2 :=

k∈N
⟨Sek, Tek⟩
we obtain that

L2(U, H), ⟨, ⟩L2

is a separable Hilbert space.
If fk, k ∈N, is an orthonormal basis of H we get that fj ⊗ek := fj⟨ek, · ⟩U,
j, k ∈N, is an orthonormal basis of L2(U, H).
Proof. We have to prove the completeness and the separability.
1. L2(U, H) is complete:
Let Tn, n ∈N, be a Cauchy sequence in L2(U, H). Then it is clear that it is
also a Cauchy sequence in L(U, H). Because of the completeness of L(U, H)
there exists an element T ∈L(U, H) such that ∥Tn −T∥L(U,H) −→0 as
n →∞. But by the lemma of Fatou we also have for any orthonormal basis
ek, k ∈N, of U that
∥Tn −T∥2
L2 =

k∈N

(Tn −T)ek, (Tn −T)ek

=

k∈N
lim inf
m→∞
(Tn −Tm)ek
2
⩽lim inf
m→∞

k∈N
(Tn −Tm)ek
2 = lim inf
m→∞∥Tn −Tm∥2
L2 < ε
for all n ∈N big enough. Therefore the assertion follows.
2. L2(U, H) is separable:
If we deﬁne fj ⊗ek := fj⟨ek, · ⟩U, j, k ∈N, then it is clear that fj ⊗ek ∈
L2(U, H) for all j, k ∈N and for arbitrary T ∈L2(U, H) we get that
⟨fj ⊗ek, T⟩L2 =

n∈N
⟨ek, en⟩U · ⟨fj, Ten⟩= ⟨fj, Tek⟩.
Therefore it is obvious that fj ⊗ek, j, k ∈N, is an orthonormal system.
In addition, T = 0 if ⟨fj ⊗ek, T⟩L2 = 0 for all j, k ∈N, and therefore
span(fj ⊗ek | j, k ∈N) is a dense subspace of L2(U, H).
Proposition B.0.8. Let

G, ⟨, ⟩G

be a further separable Hilbert space.
If T ∈L2(U, H) and S ∈L2(H, G) then ST ∈L1(U, G) and
∥ST∥L1(U,G) ⩽∥S∥L2 · ∥T∥L2.
Proof. Let fk, k ∈N, be an orthonormal basis of H. Then we have that
STx =

k∈N
⟨Tx, fk⟩Sfk,
x ∈U

B. Nuclear and Hilbert–Schmidt Operators
113
and therefore
∥ST∥L1(U,G) ⩽

k∈N
∥T ∗fk∥U · ∥Sfk∥G
⩽
	
k∈N
∥T ∗fk∥2
U

 1
2 ·
	
k∈N
∥Sfk∥2
G

 1
2 = ∥S∥L2 · ∥T∥L2.
Remark B.0.9. Let ek, k ∈N, be an orthonormal basis of U. If T ∈L(U)
is symmetric, nonnegative with 
k∈N⟨Tek, ek⟩U < ∞then T ∈L1(U).
Proof. The result is obvious by the previous proposition and the fact that
there exists T
1
2 ∈L(U) nonnegative and symmetric such that T = T
1
2 T
1
2 (see
Proposition 2.3.4). Then T
1
2 ∈L2(U).
Proposition B.0.10. Let L ∈L(H) and B ∈L2(U, H). Then LBB∗∈
L1(H), B∗LB ∈L1(U) and we have that
tr LBB∗= tr B∗LB.
Proof. We know by Remark B.0.6 (iii) and Proposition B.0.8 that LBB∗∈
L1(H) and B∗LB ∈L1(U). Let ek, k ∈N, be an orthonormal basis of U
and let fk, k ∈N, be an orthonormal basis of H. Then the Parseval identity
implies that

k∈N

n∈N
⟨fk, Ben⟩· ⟨fk, LBen⟩

⩽

n∈N
	
k∈N
⟨fk, Ben⟩
2
 1
2 ·
	
k∈N
⟨fk, LBen⟩
2
 1
2
=

n∈N
∥Ben∥· ∥LBen∥⩽∥L∥L(H) · ∥B∥2
L2.
Therefore, it is allowed to interchange the sums to obtain that
tr LBB∗=

k∈N
⟨LBB∗fk, fk⟩=

k∈N
⟨B∗fk, B∗L∗fk⟩U
=

k∈N

n∈N
⟨B∗fk, en⟩U · ⟨B∗L∗fk, en⟩U =

n∈N

k∈N
⟨fk, Ben⟩· ⟨fk, LBen⟩
=

n∈N
⟨Ben, LBen⟩=

n∈N
⟨en, B∗LBen⟩U = tr B∗LB.

C. Pseudo Inverse of Linear
Operators
This chapter is a slight modiﬁcation of Chapter C in [FK01].
Let

U, ⟨, ⟩U

and

H, ⟨, ⟩

be two Hilbert spaces.
Deﬁnition C.0.1 (Pseudo inverse). Let T ∈L(U, H) and Ker(T) := {x ∈
U | Tx = 0}. The pseudo inverse of T is deﬁned as
T −1 :=

T |Ker(T )⊥
−1 : T

Ker(T)⊥
= T(U) →Ker(T)⊥.
(Note that T is one-to-one on Ker(T)⊥.)
Remark C.0.2.
(i) There is an equivalent way of deﬁning the pseudo in-
verse of a linear operator T ∈L(U, H). For x ∈T(U) one sets T −1x ∈U
to be the solution of minimal norm of the equation Ty = x, y ∈U.
(ii) If T ∈L(U, H) then T −1 : T(U) →Ker(T)⊥is linear and bijective.
Proposition C.0.3. Let T ∈L(U) and T −1 the pseudo inverse of T.
(i) If we deﬁne an inner product on T(U) by
⟨x, y⟩T (U) := ⟨T −1x, T −1y⟩U
for all x, y ∈T(U),
then

T(U), ⟨, ⟩T (U)

is a Hilbert space.
(ii) Let ek, k ∈N, be an orthonormal basis of (Ker T)⊥. Then Tek, k ∈N,
is an orthonormal basis of

T(U), ⟨, ⟩T (U)

.
Proof. T : (Ker T)⊥→T(U) is bijective and an isometry if (Ker T)⊥is
equipped with ⟨, ⟩U and T(U) with ⟨, ⟩T (U).
Now we want to present a result about the images of linear operators. To
this end we need the following lemma.
Lemma C.0.4. Let T ∈L(U, H). Then the set TBc(0) (=

Tu
 u ∈
U, ∥u∥U ⩽c

), c ⩾0, is convex and closed.
Proof. Since T is linear it is obvious that the set is convex.
Since a convex subset of a Hilbert space is closed (with respect to the norm)
if and only if it is weakly closed, it suﬃces to show that TBc(0) is weakly
closed. Since T : U →H is linear and continuous (with respect to the norms
115

116
C. Pseudo Inverse of Linear Operators
on U, H respectively) it is also obviously continuous with respect to the weak
topologies on U, H respectively. But by the Banach–Alaoglou theorem (see
e.g. [RS72, Theorem IV.21, p. 115]) closed balls in a Hilbert space are weakly
compact. Hence Bc(0) is weakly compact, and so is its continuous image, i.e.
TBc(0) is weakly compact, therefore weakly closed.
Proposition C.0.5. Let

U1, ⟨, ⟩1

and

U2, ⟨, ⟩2

be two Hilbert spaces.
In addition, we take T1 ∈L(U1, H) and T2 ∈L(U2, H). Then the following
statements hold.
(i) If there exists a constant c ⩾0 such that ∥T ∗
1 x∥1 ⩽c∥T ∗
2 x∥2 for all
x ∈H then

T1u
 u ∈U1, ∥u∥1 ⩽1

⊂

T2v
 v ∈U2, ∥v∥2 ⩽c

. In
particular, this implies that Im T1 ⊂Im T2.
(ii) If ∥T ∗
1 x∥1 = ∥T ∗
2 x∥2 for all x ∈H then Im T1 = Im T2 and ∥T −1
1
x∥1 =
∥T −1
2
x∥2 for all x ∈Im T1.
Proof. [DPZ92, Proposition B.1, p. 407]
(i) Assume that there exists u0 ∈U1 such that
∥u0∥1 ⩽1
and
T1u0 /∈

T2v
 v ∈U2, ∥v∥2 ⩽c

.
By Lemma C.0.4 we know that the set

T2v
 v ∈U2, ∥v∥2 ⩽c

is
closed and convex. Therefore, we get by the separation theorem (see
[Alt92, 5.11 Trennungssatz, p. 166]) there exists x ∈H, x ̸= 0, such that
1 < ⟨x, T1u0⟩
and
⟨x, T2v⟩⩽1 for all v ∈U2 with ∥v∥2 ⩽c.
Thus ∥T ∗
1 x∥1 > 1 and c∥T ∗
2 x∥2 =
sup
∥v∥2⩽c
⟨T ∗
2 x, v⟩2
 ⩽1, a contradiction.
(ii) By (i) we know that Im T1 = Im T2. It remains to verify that
∥T −1
1
x∥1 = ∥T −1
2
x∥2
for all x ∈Im T1.
If x = 0 then ∥T −1
1
0∥1 = 0 = ∥T −1
2
0∥2.
If x ∈Im T1 \ {0} then there exist u1 ∈(Ker T1)⊥and u2 ∈(Ker T2)⊥
such that x = T1u1 = T2u2. We have to show that ∥u1∥1 = ∥u2∥2.
Assume that ∥u1∥1 > ∥u2∥2 > 0. Then (i) implies that
x
∥u2∥2
= T2

u2
∥u2∥2

∈

T2v
 v ∈U2, ∥v∥2 ⩽1

=

T1u
 u ∈U1, ∥u∥1 ⩽1

.
But
x
∥u2∥2
= T1

u1
∥u2∥2

and

u1
∥u2∥2

1
> 1,

C. Pseudo Inverse of Linear Operators
117
therefore, there exists ˜u1 ∈U1, ∥˜u1∥1 ⩽1, so that for ˜u2 :=
u1
∥u2∥2 ∈
(Ker T1)⊥we have
T1˜u1 =
x
∥u2∥2
= T1˜u2 ,
i.e. ˜u1 −˜u2 ∈Ker T1.
Therefore,
0 = ⟨˜u1 −˜u2, ˜u2⟩1 = ⟨˜u1, ˜u2⟩1 −∥˜u2∥2
1
⩽∥˜u1∥1∥˜u2∥1 −∥˜u2∥2
1 =

1 −∥˜u2∥1

∥˜u2∥1.
This is a contradiction.
Corollary C.0.6. Let T ∈L(U, H) and set Q := TT ∗∈L(H). Then we
have
Im Q
1
2 = Im T
and
Q−1
2 x
 = ∥T −1x∥U for all x ∈Im T,
where Q−1
2 is the pseudo inverse of Q
1
2 .
Proof. Since by Lemma 2.3.4 Q
1
2 is symmetric we have for all x ∈H that


Q
1
2 ∗x

2
=
Q
1
2 x
2 = ⟨Qx, x⟩= ⟨TT ∗x, x⟩= ∥T ∗x∥2
U.
Therefore the assertion follows by Proposition C.0.5.

D. Some Tools from Real
Martingale Theory
We need the following Burkholder–Davis inequality for real-valued continuous
local martingales.
Proposition D.0.1. Let (Nt)t∈[0,T ] be a real-valued continuous local mar-
tingale on a probability space (Ω, E, P) with respect to a normal ﬁltration
(Ft)t∈[0,T ]. Then for all stopping times τ(⩽T)
E( sup
t∈[0,τ]
|Nt|) ⩽3E(⟨N⟩1/2
τ
).
Proof. See e.g. [KS88, Theorem 3.28].
Corollary D.0.2. Let ε, δ ∈]0, ∞[. Then for N as in Proposition D.0.1
P( sup
t∈[0,T ]
|Nt| ⩾ε) ⩽3
εE(⟨N⟩1/2
T
∧δ) + P(⟨N⟩1/2
T
> δ).
Proof. Let
τ := inf{t ⩾0| ⟨N⟩1/2
t
> δ} ∧T.
Then τ(⩽T) is an Ft-stopping time. Hence by Proposition D.0.1
P

sup
t∈[0,T ]
|Nt| ⩾ε

=P

sup
t∈[0,T ]
|Nt| ⩾ε, τ = T

+ P

sup
t∈[0,T ]
|Nt| ⩾ε, τ < T

⩽3
εE(⟨N⟩1/2
τ
) + P

sup
t∈[0,T ]
|Nt| ⩾ε, ⟨N⟩1/2
T
> δ

⩽3
εE(⟨N⟩1/2
T
∧δ) + P(⟨N⟩1/2
T
> δ).
119

E. Weak and Strong Solutions:
the Yamada-Watanabe
Theorem
Let (Ω, F, P) be a complete probability space with normal ﬁltration Ft, t ∈
[0, ∞[. Below we shall call ((Ω, F, P, (Ft)) a stochastic basis. Let d, d1 ∈N and
let M(d × d1, R) denote the set of all real d × d1-matrices equipped with the
norm (3.1.2). Let
W d := C([0, ∞[ →Rd)
(E.0.1)
and
W d
0 := {w ∈W d|w(0) = 0}.
(E.0.2)
W d is equipped with metric
ϱ(w1, w2) :=
∞

k=1
2−k( max
0⩽t⩽k |w1(t) −w2(t)| ∧1),
w1, w2 ∈W d,
(E.0.3)
which makes it a Polish space. Its Borel σ-algebra is denoted by B(W d).
Let Bt(W d) denote the σ-Algebra generated by all maps πs, 0 ⩽s ⩽t,
where πs(w) := w(s), w ∈W d. Let Ad,d1 denote the set of all B([0, ∞[) ⊗
B(W d)/B(M(d × d1, R))-measurable maps α : [0, ∞[ ×W d →M(d × d1, R)
such that for each t ∈[0, ∞[ the map
W d ∋w 	→α(t, w) ∈M(d × d1, R)
is Bt(W d)/B(M(d × d1, R))-measurable.
E.1. The main result
Fix σ ∈Ad,d1 and b ∈Ad,1 and consider the following stochastic diﬀerential
equation:
dX(t) = b(t, X) dt + σ(t, X) dW(t),
t ∈[0, ∞[.
(E.1.1)
Deﬁnition E.1.1. An Rd-valued continuous, (Ft)-adapted process X(t), t ∈
[0, ∞[, on some stochastic basis (Ω, F, P, (Ft)) is called a (weak) solution to
(E.1.1), if
121

122
E. Weak and Strong Solutions: Yamada–Watanabe Theorem
(i)
 t
0
|b(s, X)| ds < ∞
P-a.e. for all t ∈[0, ∞[.
(ii)
 t
0
∥σ(s, X)∥2 ds < ∞
P-a.e. for all t ∈[0, ∞[.
(iii) There exists an Rd1-valued standard (Ft)-Wiener process W(t), t ∈
[0, ∞[, on (Ω, F, P) such that P-a.e.
X(t) = X(0)+
 t
0
b(s, X) ds+
 t
0
σ(s, X) dW(s),
t ∈[0, ∞[. (E.1.2)
Remark E.1.2.
(i) Clearly, by the measurability assumption on elements
in Ad,d1 it follows that if X is a solution, then [0, t] × Ω∋(s, ω) 	→
σ(s, X(ω)) is B([0, t])⊗F/B(M(d×d1, R))-measurable and σ(t, X) is Ft-
measurable for t ∈[0, ∞[. Likewise for b(·, X). The (Ft)-adaptedness for
σ(·, X) and b(·, X) follows since the (Ft)-adaptiveness of X is equivalent
to the Ft/Bt(W d) measurability of X.
(ii) Below we shall brieﬂy say (X, W) in Deﬁnition E.1.1 is a (weak) solution
to (E.1.1) not always mentioning explicitly the stochastic basis, that
comes with it.
Deﬁnition E.1.3. We say that (weak) uniqueness holds for (E.1.1) if when-
ever
X
and
X′
are
two
(weak)
solutions
(with
stochastic
bases
(Ω, F, P, (Ft)), (Ω′, F′, P ′, (F′
t))
and
associated
Wiener
processes W(t),
W ′(t), t ∈[0, ∞[) such that
P ◦X(0)−1 = P ′ ◦X′(0)−1,
(as measures on (Rd, B(Rd))), then
P ◦X−1 = P ′ ◦(X′)−1
(as measures on (W d, B(W d))).
Deﬁnition E.1.4. We say that pathwise uniqueness holds for (E.1.1), if
whenever X and X′ are two (weak) solutions on the same stochastic basis
(Ω, F, P, (Ft)) and with the same (Ft)-Wiener process W(t), t ∈[0, ∞[ on
(Ω, F, P) such that X(0) = X′(0) P-a.e., then P-a.e.
X(t) = X′(t), t ∈[0, ∞[.
To deﬁne strong solutions we need to introduce the following class ˆE of
maps:

E.1. The main result
123
Let ˆE denote the set of all maps F : Rd×W d1
0
→W d such that for every prob-
ability measure µ on (Rd, B(Rd)) there exists a B(Rd) ⊗B(W d1
0 )
µ⊗P W
/B(W d)-
measurable map Fµ : Rd × W d1
0
→W d such that for µ-a.e. x ∈Rd
F(x, w) = Fµ(x, w) for P W -a.e. w ∈W d1
0 .
Here B(Rd) ⊗B(W d1
0 )
µ⊗P W
denotes the completion of B(Rd) ⊗B(W d1
0 ) with
respect
to
µ ⊗P W ,
and
P W
denotes
classical
Wiener
measure
on
(W d1
0 , B(W d1
0 )).
Let F ∈ˆE. For an F/B(Rd)-measurable map ξ : Ω→Rd on some prob-
ability space (Ω, F, P) and an Rd1-valued, standard Wiener process W(t),
t ∈[0, ∞[, on (Ω, F, P) independent of ξ, we set
F(ξ, W) := FP ◦ξ−1(ξ, W).
Deﬁnition E.1.5. A (weak) solution X to (E.1.1) on (Ω, F, P, (Ft)) and
associated Wiener process W(t), t ∈[0, ∞[, is called a strong solution if there
exists F ∈ˆE such that for x ∈Rd, w 	→F(x, w) is Bt(W d1
0 )
P W
/Bt(W d)-
measurable for every t ∈[0, ∞[ and
X = F(X(0), W)
P-a.e.,
where Bt(W d1
0 )
P W
denotes the completion with respect to P W in B(W d1
0 ).
Deﬁnition E.1.6. Equation (E.1.1) is said to have a unique strong solution,
if there exists F ∈ˆE satisfying the adaptiveness condition in Deﬁnition E.1.5
and such that:
1. For every Rd1-valued standard (Ft)-Wiener process W(t), t ∈[0, ∞[, on
a stochastic basis (Ω, F, P, (Ft)) and any F0/B(Rd)-measurable ξ : Ω→
Rd the continuous process
X := F(ξ, W)
satisﬁes (i), (ii) and (E.1.2) in Deﬁnition E.1.1, i.e. (F(ξ, W), W) is a
(weak) solution to (E.1.1), and X(0) = ξ P-a.e..
2. For any (weak) solution (X, W) to (E.1.1) we have
X = F(X(0), W) P-a.e..
Remark E.1.7. Since X(0) in the above deﬁnition is P-independent of W,
thus
P ◦(X(0), W)−1 = µ ⊗P W ,
we have that the existence of a unique strong solution for (E.1.1) implies that
also (weak) uniqueness holds.

124
E. Weak and Strong Solutions: Yamada–Watanabe Theorem
Now we can formulate the main result of this section.
Theorem E.1.8. Let σ ∈Ad,d1 and b ∈Ad,1. Then equation (E.1.1) has a
unique strong solution if and only if both of the following properties hold:
(i) For every probability measure µ on (Rd, B(Rd)) there exists a (weak)
solution (X, W) of (E.1.1) such that µ is the distribution of X(0).
(ii) Pathwise uniqueness holds for (E.1.1).
Proof. Suppose (E.1.1) has a unique strong solution. Then (ii) obviously
holds. To show (i) one only has to take the classical Wiener space
(W d1
0 , B(W d1
0 ), P W ) and consider (Rd × W d1
0 , B(Rd) ⊗B(W d1
0 )
µ⊗P W
, µ ⊗P W )
with ﬁltration
 
ε>0
σ(B(Rd) ⊗Bt+ε(W d1
0 ), N),
t ⩾0,
where N
denotes all µ ⊗P W -zero sets in B(Rd) ⊗B(W d1
0 )
µ⊗P W
. Let
ξ : Rd × W d1
0
→Rd and W : Rd × W d1
0
→W d1
0
be the canonical projec-
tions. Then X := F(ξ, W) is the desired weak solution in (i).
Now let us suppose that (i) and (ii) hold. The proof that then there exists a
unique strong solution for (E.1.1) is quite technical. We structure it through
a series of lemmas.
Lemma E.1.9. Let (Ω, F) be a measurable space such that {ω} ∈F for all
ω ∈Ωand such that
D := {(ω, ω)|ω ∈Ω} ∈F ⊗F
(which is e.g. the case if Ωis a Polish space and F its Borel σ-algebra).
Let P1, P2 be probability measures on (Ω, F) such that P1 ⊗P2(D) = 1. Then
P1 = P2 = δω0 for some ω0 ∈Ω.
Proof. Let f : Ω→[0, ∞[ be F-measurable. Then

f(ω1)P1(dω1) =

f(ω1)P1(dω1)P2(dω2)
=

1D(ω1, ω2)f(ω1)P1(dω1)P2(dω2)
=

1D(ω1, ω2)f(ω2)P1(dω1)P2(dω2) =

f(ω2)P2(dω2),
so P1 = P2. Furthermore,
1 =

1D(ω1, ω2)P1(dω1)P2(dω2) =

P1({ω2})P2(dω2),
hence 1 = P1({ω2}) for P2-a.e. ω2 ∈Ω. Therefore, P1 = δω0 for some ω0 ∈Ω.

E.1. The main result
125
Fix a probability measure µ on (Rd, B(Rd)) and let (X, W) with stochastic
basis (Ω, F, P, (Ft)) be a (weak) solution to (E.1.1) with initial distribution µ.
Deﬁne a probability measure Pµ on (Rd×W d×W d1
0 , B(Rd)⊗B(W d)⊗B(W d1
0 )),
by
Pµ := P ◦(X(0), X, W)−1.
Lemma E.1.10. There exists a family Kµ((x, w), dw1), x ∈Rd, w ∈W d1
0 , of
probability measures on (W d, B(W d)) having the following properties:
(i) For every A ∈B(W d) the map
Rd × W d1
0
∋(x, w) 	→Kµ((x, w), A)
is B(Rd) ⊗B(W d1
0 )-measurable.
(ii) For every B(Rd) ⊗B(W d) ⊗B(W d1
0 )-measurable map f : Rd × W d ×
W d1
0
→[0, ∞[ we have

f(x, w1, w)Pµ(dx dw1 dw)
=

Rd

W d1
0

W d f(x, w1, w)Kµ((x, w), dw1)P W(dw)µ(dx).
(iii) If t ∈[0, ∞[ and f : W d →[0, ∞[ is Bt(W d)-measurable, then
Rd × W d1
0
∋(x, w) 	→

f(w1)Kµ((x, w), dw1)
is B(Rd) ⊗Bt(W d1
0 )
µ⊗P W
-measurable, where B(Rd) ⊗Bt(W d1
0 )
µ⊗P W
de-
notes the completion with respect to µ ⊗P W in B(Rd) ⊗B(W d1
0 ).
Proof. Let Π : Rd ×W d ×W d1
0
→Rd ×W d1
0
be the canonical projection. Since
X(0) is F0-measurable, hence P-independent of W, it follows that
Pµ ◦Π−1 = P ◦(X(0), W)−1 = µ ⊗P W .
Hence by the existence result on regular conditional distributions (cf. e.g.
[IW81, Corollary to Theorem 3.3 on p. 15]), the existence of the family
Kµ((x, w), dw1), x ∈Rd, w ∈W d1
0 , satisfying (i) and (ii) follows.
To prove (iii) it suﬃces to show that for t ∈[0, ∞[ and for all A0 ∈B(Rd),
A1 ∈Bt(W d), A ∈Bt(W d1
0 ) and
A′ := {πr1 −πt ∈B1, . . . , πrk −πt ∈Bk},
t ⩽r1 < . . . < rk, B1, . . . , Bk ∈B(Rd1),

126
E. Weak and Strong Solutions: Yamada–Watanabe Theorem

A0

W d1
0
1A∩A′(w)Kµ((x, w), A1)P W(dw)µ(dx)
=

A0

W d1
0
1A∩A′(w)Eµ⊗P W (Kµ(·, A1)|B(Rd) ⊗Bt(W d1
0 ))P W(dw)µ(dx),
(E.1.3)
since the system of all A ∩A′, A ∈Bt(W d1
0 ), A′ as above generates B(W d1
0 ).
But by part (ii) above, the left-hand side of (E.1.3) is equal to

1A0(x)1A∩A′(w)1A1(w1)Pµ(dx dw1 dw)
=

1A0(X(0))1A1(X)1A(W)1A′(W) dP
=

1A0(X(0))1A1(X)1A(W)EP (1A′(W)|Ft) dP.
(E.1.4)
But 1A′(W) is P-independent of Ft, since W is an (Ft)-Wiener process on
(Ω, F, P), so
EP (1A′(W)|Ft) = EP (1A′(W)).
Hence the right-hand side of (E.1.4) is equal to
P W (A′)

1A0(x)1A(w)1A1(w1)Pµ(dx dw1 dw)
= P W (A′)

A0

A
Kµ((x, w), A1)P W(dw)µ(dx)
= P W (A′)

A0

A
Eµ⊗P W (Kµ(·, A1)|B(Rd) ⊗Bt(W d1
0 ))((x, w))
P W(dw)µ(dx)
=

A0

W d1
0
1A∩A′(w)Eµ⊗P W (Kµ(·, A1)|B(Rd) ⊗Bt(W d1
0 ))((x, w))
P W(dw)µ(dx),
since A′ is P W -independent of Bt(W d1
0 ).
For x ∈Rd deﬁne a measure Qx on
(Rd × W d × W d × W d1
0 , B(Rd) ⊗B(W d) ⊗B(W d) ⊗B(W d1
0 ))
by
Qx(A) :=

Rd

W d1
0

W d

W d 1A(z, w1, w2, w)
Kµ((z, w), dw1)Kµ((z, w), dw2)P W(dw)δx(dz).

E.1. The main result
127
Deﬁne the stochastic basis
˜Ω:= Rd × W d × W d × W d1
0
˜Fx := B(Rd) ⊗B(W d) ⊗B(W d) ⊗B(W d1
0 )
Qx
˜Fx
t :=
 
ε>0
σ(B(Rd) ⊗Bt+ε(W d) ⊗Bt+ε(W d) ⊗Bt+ε(W d1
0 ), Nx),
where
Nx := {N ∈˜Fx|Qx(N) = 0},
and deﬁne maps
Π0 : ˜Ω→Rd, (x, w1, w2, w) 	→x,
Πi : ˜Ω→W d, (x, w1, w2, w) 	→wi ∈W d,
i = 1, 2,
Π3 : ˜Ω→W d1
0 , (x, w1, w2, w) 	→w ∈W d1
0 .
Then, obviously,
Qx ◦Π−1
0
= δx
(E.1.5)
and
Qx ◦Π−1
3
= P W (= P ◦W −1).
(E.1.6)
Lemma E.1.11. There exists N0 ∈B(Rd) with µ(N0) = 0 such that for all
x ∈N c
0 we have that Π3 is an ( ˜Fx
t )-Wiener process on (˜Ω, ˜Fx, Qx) taking
values in Rd1.
Proof. By deﬁnition Π3 is ( ˜Fx
t )-adapted for every x ∈Rd. Furthermore, for
0 ⩽s < t, y ∈Rd, and A0 ∈B(Rd), Ai ∈Bs(W d), i = 1, 2, A3 ∈Bs(W d1
0 ),

Rd EQx(exp(i⟨y, Π3(t) −Π3(s)⟩)1A0×A1×A2×A3)µ(dx)
=

Rd

W d1
0
exp(i⟨y, w(t) −w(s)⟩)1A0(x)1A3(w)
Kµ((x, w), A1)Kµ((x, w), A2)P W(dw)µ(dx)
=

W d1
0
exp(i⟨y, w(t) −w(s)⟩)P W(dw)

Rd
Qx(A0 × A1 × A2 × A3)µ(dx),
where we used Lemma E.1.10(iii) in the last step. Now the assertion follows
by (E.1.6), a monotone class argument and the same reasoning as in the proof
of Proposition 2.1.13.

128
E. Weak and Strong Solutions: Yamada–Watanabe Theorem
Lemma E.1.12. There exists N1 ∈B(Rd), N0 ⊂N1, with µ(N1) = 0 such
that
for
all
x
∈
N c
1,
(Π1, Π3)
and
(Π2, Π3)
with
stochastic
basis
(˜Ω, ˜Fx, Qx, ( ˜Fx
t )) are (weak) solutions of (E.1.1) such that
Π1(0) = Π2(0) = x
Qx-a.e.,
therefore, Π1 = Π2 Qx-a.e.
Proof. For i = 1, 2 consider the set Ai ∈˜Fx deﬁned by
Ai :=

Πi(t) −Πi(0) =
 t
0
b(s, Πi) ds +
 t
0
σ(s, Πi) dΠ3(s)
for all t ∈[0, ∞[

∩{Πi(0) = Π0}.
Deﬁne A ∈B(Rd) ⊗B(W d) ⊗B(W d1
0 ) analogously with Πi replaced by the
canonical projection from Rd × W d × W d1
0
onto the second and Π0, Π3 by the
canonical projection onto the ﬁrst and third coordinate respectively. Then by
Lemma E.1.10 (ii) for i = 1, 2

Rd

W d1
0

W d

W d 1Ai(x, w1, w2, w)
Kµ((x, w), dw1)Kµ((x, w), dw2)P W(dw)µ(dx)
= Pµ(A) = P({(X(0), X, W) ∈A}) = 1.
(E.1.7)
Since all measures in the left-hand side of (E.1.7) are probability measures, it
follows that for µ-a.e. x ∈Rd
1 = Qx(Ai) = Qx(Ai,x),
where for i = 1, 2
Ai,x :=

Πi(t) −x =
 t
0
b(s, Πi) ds +
 t
0
σ(s, Πi) dΠ3(s), ∀t ∈[0, ∞[

!.
Hence the ﬁrst assertion follows. The second then follows by the pathwise
uniqueness assumption in condition (ii) of the theorem.
Lemma E.1.13. There exists a B(Rd) ⊗B(W d1
0 )
µ⊗P W
/B(W d)- measurable
map
Fµ : Rd × W d1
0
→W d
such that
Kµ((x, w), ·) = δFµ(x,w)
(= Dirac measure on B(W d) with mass in Fµ(x, w))

E.1. The main result
129
for
µ ⊗P W -a.e.
(x, w)
∈
Rd
× W d1
0 .
Furthermore,
Fµ
is
B(Rd) ⊗Bt(W d1
0 )
µ⊗P W
/Bt(W d)-measurable
for
all
t
∈
[0, ∞[,
where
B(Rd) ⊗Bt(W d1
0 )
µ⊗P W
denotes the completion with respect to µ ⊗P W in
B(Rd) ⊗Bt(W d1
0 ).
Proof. By Lemma E.1.12 for all x ∈N c
1, we have
1 = Qx({Π1 = Π2})
=

W d1
0

W d

W d 1D(w1, w2)Kµ((x, w), dw1)Kµ((x, w), dw2)P W(dw),
where D := {(w1, w1) ∈W d × W d|w1 ∈W d}. Hence by Lemma E.1.9 there
exists N ∈B(Rd)⊗B(W d1
0 ) such that µ⊗P W (N) = 0 and for all (x, w) ∈N c
there exists Fµ(x, w) ∈W d such that
Kµ((x, w), dw1) = δFµ(x,w)(dw1).
Set Fµ(x, w) := 0, if (x, w) ∈N. Let A ∈B(W d). Then
{Fµ ∈A} = ({Fµ ∈A} ∩N) ∪({Kµ(·, A) = 1} ∩N c)
and the measurability properties of Fµ follow from Lemma E.1.10.
Having deﬁned the mapping Fµ let us check the conditions of Deﬁnition
E.1.5 and Deﬁnition E.1.6. We start with condition 2.
Lemma E.1.14. We have
X = Fµ(X(0), W)
P-a.e..
Proof. By Lemmas E.1.10 and E.1.13 we have
P({X = Fµ(X(0), W)})
=

Rd

W d1
0

W d
1{w1=Fµ(x,w)}(x, w1, w)δFµ(x,w)(dw1)P W(dw)µ(dx)
=1.
Now let us check condition 1. Let W ′ be another Rd1-valued standard (F′
t)-
Wiener process on a stochastic basis (Ω′, F′, P ′, (F′
t)) and ξ : Ω′ →Rd an
F′
0/B(Rd)-measurable map and µ := P ′ ◦ξ−1. Let Fµ be as above and set
X′ := Fµ(ξ, W ′).

130
E. Weak and Strong Solutions: Yamada–Watanabe Theorem
Lemma E.1.15. (X′, W ′) is a (weak) solution to (E.1.1) with X′(0) = ξ
P ′-a.s..
Proof. We have
P ′({ξ = X′(0)}) = P ′({ξ = Fµ(ξ, W ′)(0)})
= µ ⊗P W ({(x, w) ∈Rd × W d1
0 |x = Fµ(x, w)})
= P({X(0) = Fµ(X(0), W)(0)}) = 1,
where we used Lemma E.1.14 in the last step.
To see that (X′, W ′) is a (weak) solution we consider the set A ∈B(Rd) ⊗
B(W d)⊗B(W d1
0 ) deﬁned in the proof of Lemma E.1.12. We have to show that
P ′({(X′(0), X′, W ′) ∈A}) = 1.
But since X′(0) = ξ is P ′-independent of W ′, we have

1A(X′(0), Fµ(X′(0), W ′), W ′) dP ′
=

Rd

W d1
0
1A(x, Fµ(x, w), w)P W(dw)µ(dx)
=

Rd

W d1
0

W d 1A(x, w1, w)δFµ(x,w)(dw1)P W(dw)µ(dx)
=

1A(x, w1, w)Pµ(dx dw1 dw)
=P({(X(0), X, W) ∈A}) = 1,
where we used E.1.10 and E.1.11 in the second to last step.
To complete the proof we still have to construct F ∈ˆE and to check the
adaptiveness conditions on the corresponding mappings Fµ. Below we shall
apply what we have obtained above now also to δx replacing µ. So, for each
x ∈Rd we have a function Fδx. Now deﬁne
F(x, w) := Fδx(x, w), x ∈Rd, w ∈W d1
0 .
(E.1.8)
The proof of Theorem E.1.8 is then completed by the following lemma.
Lemma E.1.16. Let µ be a probability measure on (Rd, B(Rd)) and Fµ :
Rd × W d1
0
→W d as constructed in Lemma E.1.13. Then for µ-a.e. x ∈Rd
F(x, ·) = Fµ(x, ·)
P W −a.e..
Furthermore, F(x, ·) is Bt(W d1
0 )
P W
/Bt(W d)-measurable for all x
∈
Rd,
t ∈[0, ∞[, where Bt(W d1
0 )
P W
denotes the completion of Bt(W d1
0 ) with respect
to P W in B(W d1
0 ).

E.1. The main result
131
Proof. Let
¯Ω:= Rd × W d × W d1
0
¯F := B(Rd) ⊗B(W d) ⊗B(W d1
0 )
and ﬁx x ∈Rd. Deﬁne a measure ¯Qx on (¯Ω, ¯F) by
¯Qx(A) :=

Rd

W d1
0

W d 1A(z, w1, w)Kµ((z, w), dw1)P W(dw)δx(dz)
with Kµ as in Lemma E.1.10. Consider the stochastic basis (¯Ω, ¯Fx, ¯Qx, ( ¯Fx
t ))
where
¯Fx := B(Rd) ⊗B(W d) ⊗B(W d1
0 )
¯
Qx,
¯Fx
t :=
 
ε>0
σ(B(Rd) ⊗Bt+ε(W d) ⊗Bt+ε(W d1
0 ), ¯
Nx),
where ¯
Nx := {N ∈¯Fx| ¯Qx(N) = 0}. As in the proof of Lemma E.1.12 one
shows that (Π, Π3) on (¯Ω, ¯Fx, ¯Qx, ( ¯Fx
t )) is a (weak) solution to (E.1.1) with
Π(0) = x ¯Qx-a.e. Here
Π0 : Rd × W d × W d1
0
→Rd, (x, w1, w) 	→x,
Π : Rd × W d × W d1
0
→W d, (x, w1, w) 	→w1,
Π3 : Rd × W d × W d1
0
→W d1
0 , (x, w1, w) 	→w.
By Lemma E.1.15 (Fδx(x, Π3), Π3) on the stochastic basis (¯Ω, ¯Fx, ¯Qx, ( ¯Fx
t ))
is a (weak) solution to (E.1.1) with
Fδx(x, Π3)(0) = x.
Hence by our pathwise uniqueness assumption (ii), it follows that
Fδx(x, Π3) = Π
¯Qx-a.s..
(E.1.9)
Hence for all A ∈B(Rd) ⊗B(W d) ⊗B(W d1
0 ) by Lemma E.1.13 and (E.1.9)

Rd

W d

W d1
0
1A(x, w1, w)δFµ(x,w)(dw1)P W(dw)µ(dx)
=

Rd
¯Qx(A)µ(dx)
=

Rd

¯Ω
1A(Π0, Fδx(x, Π3), Π3) d ¯Qxµ(dx)
=

Rd

W
d!
0
1A(x, Fδx(x, w), w)P W(dw)µ(dx)
=

Rd

W d1
0

W d 1A(x, w1, w)δFδx(x,w)(dw1)P W(dw)µ(dx),

132
E. Weak and Strong Solutions: Yamada–Watanabe Theorem
which implies the assertion.
Let x ∈Rd, t ∈[0, ∞[, A ∈Bt(W d), and deﬁne
¯Fδx := 1{x}×W d1
0 Fδx.
Then
¯Fδx = Fδx
δx ⊗P W −a.e.,
hence
{ ¯Fδx ∈A} ∈B(Rd) ⊗Bt(W d1
0 )
δx⊗P W
.
(E.1.10)
But
{ ¯Fδx ∈A} = {x} × {Fδx(x, ·) ∈A} ∪(Rd\{x}) × {0 ∈A},
so by (E.1.10) it follows that
{Fδx(x, ·) ∈A} ∈Bt(W d1
0 )
P W
.

F. Strong, Mild and Weak
Solutions
This chapter is a short version of Chapter 2 in [FK01]. We only state the
results and refer to [FK01], [DPZ92] for the proofs.
As in previous chapters let (U, ∥∥U) and (H, ∥∥) be separable Hilbert spaces.
We take Q = I and ﬁx a cylindrical Q-Wiener process W(t), t ⩾0, in U on a
probability space (Ω, F, P) with a normal ﬁltration Ft, t ⩾0. Moreover, we
ﬁx T > 0 and consider the following type of stochastic diﬀerential equations
in H:
dX(t) = [CX(t) + F(X(t))] dt + B(X(t)) dW(t),
t ∈[0, T],
X(0) = ξ,
(F.0.1)
where:
• C : D(C) →H is the inﬁnitesimal generator of a C0-semigroup S(t),
t ⩾0, of linear operators on H,
• F : H →H is B(H)/B(H)-measurable,
• B : H →L(U, H),
• ξ is a H-valued, F0-measurable random variable.
Deﬁnition F.0.1 (mild solution). An H-valued predictable process X(t),
t ∈[0, T], is called a mild solution of problem (F.0.1) if
X(t) = S(t)ξ +
 t
0
S(t −s)F(X(s)) ds
+
 t
0
S(t −s)B(X(s)) dW(s)
P-a.s.
(F.0.2)
for each t ∈[0, T]. In particular, the appearing integrals have to be well-
deﬁned.
Deﬁnition F.0.2 (analytically strong solutions). A D(C)-valued pre-
dictable process X(t), t ∈[0, T], (i.e. (s, ω) 	→X(s, ω) is PT /B(H)-
measurable) is called an analytically strong solution of problem (F.0.1) if
X(t) = ξ +
 t
0
CX(s) + F(X(s)) ds +
 t
0
B(X(s)) dW(s)
P-a.s.
(F.0.3)
133

134
F. Strong, Mild and Weak Solutions
for each t ∈[0, T]. In particular, the integrals on the right-hand side have to be
well-deﬁned, that is, CX(t), F(X(t)), t ∈[0, T], are P-a.s. Bochner integrable
and B(X) ∈NW .
Deﬁnition F.0.3 (analytically weak solution). An H-valued predictable
process X(t), t ∈[0, T], is called an analytically weak solution of problem
(F.0.1) if
⟨X(t), ζ⟩= ⟨ξ, ζ⟩+
 t
0
⟨X(s), C∗ζ⟩+ ⟨F(X(s)), ζ⟩ds
+
 t
0
⟨ζ, B(X(s))dW(s)⟩
P-a.s.
(F.0.4)
for each t ∈[0, T] and ζ ∈D(C∗). Here (C∗, D(C∗)) is the adjoint of (C, D(C))
on H.
In particular, as in Deﬁnitions F.0.2 and F.0.1, the appearing integrals have
to be well-deﬁned.
Proposition F.0.4 (analytically weak versus analytically strong so-
lutions).
(i) Every analytically strong solution of problem (F.0.1) is also an analyti-
cally weak solution.
(ii) Let X(t), t ∈[0, T], be an analytically weak solution of problem (F.0.1)
with values in D(C) such that B(X(t)) takes values in L2(U, H) for all
t ∈[0, T]. Besides we assume that
P
 T
0
∥CX(t)∥dt < ∞

= 1
P
 T
0
∥F(X(t))∥dt < ∞

= 1
P
 T
0
∥B(X(t))∥2
L2 dt < ∞

= 1.
Then the process is also an analytically strong solution.
Proposition F.0.5 (analytically weak versus mild solutions).
(i) Let X(t), t ∈[0, T], be an analytically weak solution of problem (F.0.1)
such that B(X(t)) takes values in L2(U, H) for all t ∈[0, T]. Besides

F. Strong, Mild and Weak Solutions
135
we assume that
P
 T
0
∥X(t)∥dt < ∞

= 1
P
 T
0
∥F(X(t))∥dt < ∞

= 1
P
 T
0
∥B(X(t))∥2
L2 dt < ∞

= 1.
Then the process is also a mild solution.
(ii) Let X(t), t ∈[0, T], be a mild solution of problem (F.0.1) such that the
mappings
(t, ω) 	→
 t
0
S(t −s)F(X(s, ω)) ds
(t, ω) 	→
 t
0
S(t −s)B(X(s)) dW(s)(ω)
have predictable versions. In addition, we require that
P(
 T
0
∥F(X(t))∥dt < ∞) = 1
 T
0
E(
 t
0
∥⟨S(t −s)B(X(s)), C∗ζ⟩∥2
L2(U,R) ds) dt < ∞
for all ζ ∈D(C∗).
Then the process is also an analytically weak solution.
Remark F.0.6. The precise relation of mild and analytically weak solutions
with the variational solutions from Deﬁnition 4.2.1 is obviously more diﬃcult
to describe in general. We shall concentrate just on the following quite typical
special case:
Consider the situation of Subsection 4.2, but with A and B independent of t
and ω. Assume that there exist a self-adjoint operator (C, D(C)) on H such
that −C ⩾const. > 0 and F : H →H B(H)/B(H)-measurable such that
A(x) = C(x) + F(x),
x ∈V,
and
V := D((−C)
1
2 ),
equipped with the graph norm of (−C)
1
2 . Then it is easy to see that C extends
to a continuous linear operator form V to V ∗, again denoted by C such that
for x ∈V , y ∈D(C)
V ∗⟨Cx, y⟩V = ⟨x, Cy⟩.
(F.0.5)

136
F. Strong, Mild and Weak Solutions
Now let X be a (variational) solution in the sense of Deﬁnition 4.2.1, then
it follows immediately from (F.0.5) that X is an analytically weak solution in
the sense of Deﬁnition F.0.3.

Bibliography
[Alt92]
H. W. Alt, Lineare Funktionalanalysis, Springer-Verlag, 1992.
[AR91]
S. Albeverio and M. R¨ockner, Stochastic diﬀerential equa-
tions in inﬁnite dimensions: solutions via Dirichlet forms,
Probab. Theory Related Fields 89 (1991), no. 3, 347–386. MR
MR1113223 (92k:60123)
[Aro86]
D. G. Aronson, The porous medium equation, Nonlinear dif-
fusion problems (Montecatini Terme, 1985), Lecture Notes
in Math., vol. 1224, Springer, Berlin, 1986, pp. 1–46. MR
MR877986 (88a:35130)
[Bau01]
H. Bauer, Measure and integration theory, de Gruyter Studies
in Mathematics, vol. 26, Walter de Gruyter & Co., Berlin, 2001.
[Coh80]
D. L. Cohn, Measure theory, Birkh¨auser, 1980.
[Doo53]
J. L. Doob, Stochastic processes, John Wiley & Sons Inc., New
York, 1953. MR MR0058896 (15,445b)
[DP04]
G. Da Prato, Kolmogorov equations for stochastic PDEs,
Advanced
Courses
in
Mathematics
–
CRM
Barcelona,
Birkhaeuser, Basel, 2004.
[DPRLRW06] G. Da Prato, M. R¨ockner, B. L. Rozowskii and F. Y. Wang,
Strong solutions of stochastic generalized porous media equa-
tions: existence, uniqueness, and ergodicity, Comm. Partial
Diﬀerential Equations 31 (2006), nos 1–3, 277–291. MR
MR2209754
[DPZ92]
G. Da Prato and J. Zabczyk, Stochastic equations in inﬁnite
dimensions, Cambridge University Press, 1992.
[DPZ96]
, Ergodicity for inﬁnite-dimensional systems, London
Mathematical Society Lecture Note Series, vol. 229, Cambridge
University Press, 1996.
137

138
Bibliography
[FK01]
K. Frieler and C. Knoche, Solutions of stochastic diﬀerential
equations in inﬁnite dimensional Hilbert spaces and their de-
pendence on initial data, Diploma Thesis, Bielefeld University,
BiBoS-Preprint E02-04-083, 2001.
[GK81]
I. Gy¨ongy and N. V. Krylov, On stochastic equations with re-
spect to semimartingales. I, Stochastics 4 (1980/81), no. 1, 1–
21. MR MR587426 (82j:60104)
[GK82]
, On stochastics equations with respect to semimartin-
gales.
II.
Itˆo
formula
in
Banach
spaces,
Stochastics
6
(1981/82), nos 3–4, 153–173. MR MR665398 (84m:60070a)
[GT83]
D. Gilbarg and N. S. Trudinger, Elliptic partial diﬀerential
equations of second order, second ed., Grundlehren der Math-
ematischen Wissenschaften [Fundamental Principles of Math-
ematical Sciences], vol. 224, Springer-Verlag, Berlin, 1983. MR
MR737190 (86c:35035)
[Gy¨o82]
I. Gy¨ongy, On stochastic equations with respect to semimartin-
gales. III, Stochastics 7 (1982), no. 4, 231–254. MR MR674448
(84m:60070b)
[IW81]
N. Ikeda and S. Watanabe, Stochastic diﬀerential equations
and diﬀusion processes, North-Holland Mathematical Library,
vol. 24, North-Holland Publishing Co., Amsterdam, 1981.
[KR79]
N. V. Krylov and B. L. Rozowski˘ı, Stochastic evolution equa-
tions, Current problems in mathematics, Vol. 14 (Russian),
Akad. Nauk SSSR, Vsesoyuz. Inst. Nauchn. i Tekhn. In-
formatsii, Moscow, 1979, pp. 71–147, 256. MR MR570795
(81m:60116)
[Kry99]
N.
V.
Krylov,
On
Kolmogorov’s
equations
for
ﬁnite-
dimensional diﬀusions, Stochastic PDE’s and Kolmogorov
equations in inﬁnite dimensions (Cetraro, 1998), Lecture Notes
in Math., vol. 1715, Springer, Berlin, 1999, pp. 1–63. MR
MR1731794 (2000k:60155)
[KS88]
I. Karatzas and S. E. Shreve, Brownian motion and stochastic
calculus, Graduate Texts in Mathematics, vol. 113, Springer-
Verlag, New York, 1988. MR MR917065 (89c:60096)
[MV92]
R. Meise and D. Vogt, Einf¨uhrung in die Funktionalanalysis,
Vieweg Verlag, 1992.
[Ond04]
M. Ondrej´at, Uniqueness for stochastic evolution equations in
Banach spaces, Dissertationes Math. (Rozprawy Mat.) 426
(2004), 1–63.

Bibliography
139
[Par72]
E. Pardoux, Sur des ´equations aux d´eriv´ees partielles stochas-
tiques monotones, C. R. Acad. Sci. Paris S´er. A-B 275 (1972),
A101–A103. MR MR0312572 (47 #1129)
[Par75]
, ´Equations aux d´eriv´ees partielles stochastiques de type
monotone, S´eminaire sur les ´Equations aux D´eriv´ees Partielles
(1974–1975), III, Exp. No. 2, Coll`ege de France, Paris, 1975,
p. 10. MR MR0651582 (58 #31406)
[Roz90]
B. Rozowskii, Stochastic evolution systems, Mathematics and
its Applications, no. 35, Kluwer Academic Publishers Group,
Dordrecht, 1990.
[RRW06]
J. Ren, M. R¨ockner and F. Y. Wang, Stochastic porous media
and fast diﬀusion equations, Preprint, 33 pages, 2006.
[RS72]
M. Reed and B. Simon, Methods of modern mathematical
physics, Academic Press, 1972.
[Wal86]
J. B. Walsh, An introduction to stochastic partial diﬀerential
equations, ´Ecole d’´et´e de probabilit´es de Saint-Flour, XIV—
1984, Lecture Notes in Math., vol. 1180, Springer, Berlin, 1986,
pp. 265–439. MR MR876085 (88a:60114)
[WW90]
H. Weizs¨acker and G. Winkler, Stochastic integrals: an intro-
duction, Vieweg, 1990.
[Zab98]
J. Zabczyk, Parabolic equations on Hilbert spaces, Stochas-
tic PDEs and Kolmogorov Equations in Inﬁnite Dimensions
(Giuseppe Da Prato, ed.), Lecture Notes in Mathematics,
Springer Verlag, 1998, pp. 117–213.
[Zei90]
E. Zeidler, Nonlinear functional analysis and its applications.
II/B, Springer-Verlag, New York, 1990, Nonlinear monotone
operators, Translated from the German by the author and Leo
F. Boron. MR MR1033498 (91b:47002)

Index
Bochner inequality, 107
Bochner integral, 105
boundedness, 56
Burkholder-Davis inequality, 119
coercivity, 56
conditional expectation, 17
covariance operator, 6
demicontinuity, 56
elementary process, 22
existence
- of a unique solution in ﬁnite
dimensions, 44
fundamental theorem of calculus,
108
Gaussian
- law, 6
- measure, 5
- random variable, 9
representation and existence
of a -, 9
Gelfand triple, 55
hemicontinuity, 56
Hilbert–Schmidt norm, 111
Itˆo isometry, 25
local weak monotonicity, 44
localization lemma, 45
localization procedure, 30
martingale, 19
maximal inequality, 20
normal ﬁltration, 16
operator
Hilbert–Schmidt -, 110
nonnegative -, 109
nuclear -, 109
symmetric -, 109
trace class -, 109
p-Laplacian, 65
predictable process, 28
predictable σ-ﬁeld, 27
pseudo inverse, 115
quadratic variation of the stochas-
tic integral, 37
Sobolev space, 62
solution
analytically strong -, 133
analytically weak -, 134
- in ﬁnite dimensions, 44
mild -, 133
strong -, 123
weak -, 121
square root of a linear operator, 25
stochastic basis, 121
stochastic diﬀerential equation
Burgers equation, 3
heat equation, 65
- in ﬁnite dimensions, 44
- in inﬁnite dimensions, 55
Navier–Stokes equation, 3
porous media equation, 3
141

142
Index
reaction diﬀusion equation, 3,
67
stochastic integral
- w.r.t. a standard Wiener process,
22
stochastically integrable process, 30
strong measurability, 106
trace, 109
uniqueness
pathwise -, 122
strong -, 123
weak -, 122
weak monotonicity, 56
Wiener process
- as a martingale, 21
standard -, 13
representation and existence
of a -, 13
- with respect to a ﬁltration,
16
Yamada–Watanabe
Theorem of -, 124

Symbols
N(m, Q)
Gaussian measure with mean m and covariance
Q, 6
W(t), t ∈[0, T]
standard Wiener process, 13
cylindrical Wiener process, 39
E(X|G)
conditional expectation of X given G, 18
M2
T (E)
space of all continuous E-valued, square inte-
grable martingales, 20
E
class of all L(U, H)-valued elementary processes,
22
ΩT
[0, T] × Ω, 21
dx
Lebesgue measure, 21
PT
dx|[0,T ] ⊗P, 21
PT
predictable σ-ﬁeld on ΩT , 27
'
Φ(s) dW(s)
stochastic integral w.r.t. W, 22
Lp(Ω, F, µ; X)
set of all with respect to µ p-integrable mappings
from Ωto X, 106
Lp(Ω, F, µ)
Lp(Ω, F, µ; R)
Lp
0
Lp(Ω, F0, P; H)
Lp([0, T]; H)
Lp([0, T], B([0, T]), dx; H)
Lp([0, T], dx)
Lp([0, T], B([0, T]), dx; R)
∥∥T
L2-norm on L2(ΩT , PT , PT ; L0
2), 25
N 2
W (0, T; H)
L2(ΩT , PT , PT ; L0
2), 28
N 2
W (0, T)
N 2
W (0, T; H)
N 2
W
N 2
W (0, T; H)
NW (0, T; H)
space of all stochastically integrable processes, 30
NW (0, T)
NW (0, T; H)
L(U, H)
space of all bounded and linear operators from U
to H, 109
L(U)
L(U, U)
L1(U, H)
space of all nuclear operators from U to H, 109
tr Q
trace of Q, 109
L2(U, H)
space of all Hilbert–Schmidt operators from U to
H, 110
∥∥L2
Hilbert–Schmidt norm, 111
143

144
Symbols
A∗
adjoint operator of A ∈L(U, H)
Q
1
2
square root of Q ∈L(U), 25
T −1
(pseudo) inverse of T ∈L(U, H), 115
U0
Q
1
2 (U), 27
L0
2
L2(Q
1
2 (U), H), 27
⟨u, v⟩0
⟨Q−1
2 u, Q−1
2 v⟩U, 27
L(U, H)0
{TU0 | T ∈L(U, H)}, 27
M(d × d1, R)
set of all real d × d1-matrices, 43
(V, H, V ∗)
Gelfand triple, 55
C∞
0 (Λ)
set of all inﬁnitely diﬀerentiable real-valued func-
tions on Λ with compact support, 62
∥∥1,p
norm on C∞
0 (Λ), 62
H1,p
0 (Λ)
Sobolev space, completion of C∞
0 (Λ) w.r.t. ∥∥1,p,
62
∆p
p-Laplacian, p ⩾2, 65
W d
C([0, ∞[→Rd), 121
W d
0
{w ∈W d | w(0) = 0}, 121
B(W d), Bt(W d)
121
Ad,d1
121
ˆE
122

Lecture Notes in Mathematics
For information about earlier volumes
please contact your bookseller or Springer
LNM Online archive: springerlink.com
Vol. 1715: N. V. Krylov, M. Röckner, J. Zabczyk, Stochas-
tic PDE’s and Kolmogorov Equations in Inﬁnite Dimen-
sions. Cetraro, 1998. Editor: G. Da Prato (1999)
Vol. 1716: J. Coates, R. Greenberg, K. A. Ribet, K. Ru-
bin, Arithmetic Theory of Elliptic Curves. Cetraro, 1997.
Editor: C. Viola (1999)
Vol. 1717: J. Bertoin, F. Martinelli, Y. Peres, Lectures
on Probability Theory and Statistics. Saint-Flour, 1997.
Editor: P. Bernard (1999)
Vol. 1718: A. Eberle, Uniqueness and Non-Uniqueness
of Semigroups Generated by Singular Diffusion Opera-
tors (1999)
Vol. 1719: K. R. Meyer, Periodic Solutions of the N-Body
Problem (1999)
Vol. 1720: D. Elworthy, Y. Le Jan, X-M. Li, On the Geo-
metry of Diffusion Operators and Stochastic Flows (1999)
Vol. 1721: A. Iarrobino, V. Kanev, Power Sums, Goren-
stein Algebras, and Determinantal Loci (1999)
Vol. 1722: R. McCutcheon, Elemental Methods in Ergodic
Ramsey Theory (1999)
Vol. 1723: J. P. Croisille, C. Lebeau, Diffraction by an
Immersed Elastic Wedge (1999)
Vol. 1724: V. N. Kolokoltsov, Semiclassical Analysis for
Diffusions and Stochastic Processes (2000)
Vol. 1725: D. A. Wolf-Gladrow, Lattice-Gas Cellular
Automata and Lattice Boltzmann Models (2000)
Vol. 1726: V. Mari´c, Regular Variation and Differential
Equations (2000)
Vol. 1727: P. Kravanja M. Van Barel, Computing the Zeros
of Analytic Functions (2000)
Vol. 1728: K. Gatermann Computer Algebra Methods for
Equivariant Dynamical Systems (2000)
Vol. 1729: J. Azéma, M. Émery, M. Ledoux, M. Yor (Eds.)
Séminaire de Probabilités XXXIV (2000)
Vol. 1730: S. Graf, H. Luschgy, Foundations of Quantiza-
tion for Probability Distributions (2000)
Vol. 1731: T. Hsu, Quilts: Central Extensions, Braid
Actions, and Finite Groups (2000)
Vol. 1732: K. Keller, Invariant Factors, Julia Equivalences
and the (Abstract) Mandelbrot Set (2000)
Vol. 1733: K. Ritter, Average-Case Analysis of Numerical
Problems (2000)
Vol. 1734: M. Espedal, A. Fasano, A. Mikeli´c, Filtration in
Porous Media and Industrial Applications. Cetraro 1998.
Editor: A. Fasano. 2000.
Vol. 1735: D. Yafaev, Scattering Theory: Some Old and
New Problems (2000)
Vol. 1736: B. O. Turesson, Nonlinear Potential Theory and
Weighted Sobolev Spaces (2000)
Vol. 1737: S. Wakabayashi, Classical Microlocal Analysis
in the Space of Hyperfunctions (2000)
Vol. 1738: M. Émery, A. Nemirovski, D. Voiculescu,
Lectures on Probability Theory and Statistics (2000)
Vol. 1739: R. Burkard, P. Deuﬂhard, A. Jameson, J.-L.
Lions, G. Strang, Computational Mathematics Driven
by Industrial Problems. Martina Franca, 1999. Editors:
V. Capasso, H. Engl, J. Periaux (2000)
Vol. 1740: B. Kawohl, O. Pironneau, L. Tartar, J.-P. Zole-
sio, Optimal Shape Design. Tróia, Portugal 1999. Editors:
A. Cellina, A. Ornelas (2000)
Vol. 1741: E. Lombardi, Oscillatory Integrals and Phe-
nomena Beyond all Algebraic Orders (2000)
Vol. 1742: A. Unterberger, Quantization and Non-
holomorphic Modular Forms (2000)
Vol. 1743: L. Habermann, Riemannian Metrics of Con-
stant Mass and Moduli Spaces of Conformal Structures
(2000)
Vol. 1744: M. Kunze, Non-Smooth Dynamical Systems
(2000)
Vol. 1745: V. D. Milman, G. Schechtman (Eds.), Geomet-
ric Aspects of Functional Analysis. Israel Seminar 1999-
2000 (2000)
Vol. 1746: A. Degtyarev, I. Itenberg, V. Kharlamov, Real
Enriques Surfaces (2000)
Vol. 1747: L. W. Christensen, Gorenstein Dimensions
(2000)
Vol. 1748: M. Ruzicka, Electrorheological Fluids: Model-
ing and Mathematical Theory (2001)
Vol. 1749: M. Fuchs, G. Seregin, Variational Methods
for Problems from Plasticity Theory and for Generalized
Newtonian Fluids (2001)
Vol. 1750: B. Conrad, Grothendieck Duality and Base
Change (2001)
Vol. 1751: N. J. Cutland, Loeb Measures in Practice:
Recent Advances (2001)
Vol. 1752: Y. V. Nesterenko, P. Philippon, Introduction to
Algebraic Independence Theory (2001)
Vol. 1753: A. I. Bobenko, U. Eitner, Painlevé Equations in
the Differential Geometry of Surfaces (2001)
Vol. 1754: W. Bertram, The Geometry of Jordan and Lie
Structures (2001)
Vol. 1755: J. Azéma, M. Émery, M. Ledoux, M. Yor
(Eds.), Séminaire de Probabilités XXXV (2001)
Vol. 1756: P. E. Zhidkov, Korteweg de Vries and Nonlin-
ear Schrödinger Equations: Qualitative Theory (2001)
Vol. 1757: R. R. Phelps, Lectures on Choquet’s Theorem
(2001)
Vol. 1758: N. Monod, Continuous Bounded Cohomology
of Locally Compact Groups (2001)
Vol. 1759: Y. Abe, K. Kopfermann, Toroidal Groups
(2001)
Vol. 1760: D. Filipovi´c, Consistency Problems for Heath-
Jarrow-Morton Interest Rate Models (2001)
Vol. 1761: C. Adelmann, The Decomposition of Primes in
Torsion Point Fields (2001)
Vol. 1762: S. Cerrai, Second Order PDE’s in Finite and
Inﬁnite Dimension (2001)
Vol. 1763: J.-L. Loday, A. Frabetti, F. Chapoton, F. Goi-
chot, Dialgebras and Related Operads (2001)

Vol. 1764: A. Cannas da Silva, Lectures on Symplectic
Geometry (2001)
Vol. 1765: T. Kerler, V. V. Lyubashenko, Non-Semisimple
Topological Quantum Field Theories for 3-Manifolds with
Corners (2001)
Vol. 1766: H. Hennion, L. Hervé, Limit Theorems for
Markov Chains and Stochastic Properties of Dynamical
Systems by Quasi-Compactness (2001)
Vol. 1767: J. Xiao, Holomorphic Q Classes (2001)
Vol. 1768: M. J. Pﬂaum, Analytic and Geometric Study of
Stratiﬁed Spaces (2001)
Vol. 1769: M. Alberich-Carramiñana, Geometry of the
Plane Cremona Maps (2002)
Vol.
1770:
H.
Gluesing-Luerssen,
Linear
Delay-
Differential Systems with Commensurate Delays: An
Algebraic Approach (2002)
Vol. 1771: M. Émery, M. Yor (Eds.), Séminaire de Prob-
abilités 1967-1980. A Selection in Martingale Theory
(2002)
Vol. 1772: F. Burstall, D. Ferus, K. Leschke, F. Pedit,
U. Pinkall, Conformal Geometry of Surfaces in S4 (2002)
Vol. 1773: Z. Arad, M. Muzychuk, Standard Integral
Table Algebras Generated by a Non-real Element of Small
Degree (2002)
Vol. 1774: V. Runde, Lectures on Amenability (2002)
Vol. 1775: W. H. Meeks, A. Ros, H. Rosenberg, The
Global Theory of Minimal Surfaces in Flat Spaces.
Martina Franca 1999. Editor: G. P. Pirola (2002)
Vol. 1776: K. Behrend, C. Gomez, V. Tarasov, G. Tian,
Quantum Comohology. Cetraro 1997. Editors: P. de Bar-
tolomeis, B. Dubrovin, C. Reina (2002)
Vol. 1777: E. García-Río, D. N. Kupeli, R. Vázquez-
Lorenzo,
Osserman
Manifolds
in
Semi-Riemannian
Geometry (2002)
Vol. 1778: H. Kiechle, Theory of K-Loops (2002)
Vol. 1779: I. Chueshov, Monotone Random Systems
(2002)
Vol. 1780: J. H. Bruinier, Borcherds Products on O(2,1)
and Chern Classes of Heegner Divisors (2002)
Vol. 1781: E. Bolthausen, E. Perkins, A. van der Vaart,
Lectures on Probability Theory and Statistics. Ecole d’
Eté de Probabilités de Saint-Flour XXIX-1999. Editor:
P. Bernard (2002)
Vol. 1782: C.-H. Chu, A. T.-M. Lau, Harmonic Functions
on Groups and Fourier Algebras (2002)
Vol. 1783: L. Grüne, Asymptotic Behavior of Dynamical
and Control Systems under Perturbation and Discretiza-
tion (2002)
Vol. 1784: L. H. Eliasson, S. B. Kuksin, S. Marmi, J.-C.
Yoccoz, Dynamical Systems and Small Divisors. Cetraro,
Italy 1998. Editors: S. Marmi, J.-C. Yoccoz (2002)
Vol. 1785: J. Arias de Reyna, Pointwise Convergence of
Fourier Series (2002)
Vol. 1786: S. D. Cutkosky, Monomialization of Mor-
phisms from 3-Folds to Surfaces (2002)
Vol. 1787: S. Caenepeel, G. Militaru, S. Zhu, Frobenius
and Separable Functors for Generalized Module Cate-
gories and Nonlinear Equations (2002)
Vol. 1788: A. Vasil’ev, Moduli of Families of Curves for
Conformal and Quasiconformal Mappings (2002)
Vol. 1789: Y. Sommerhäuser, Yetter-Drinfel’d Hopf alge-
bras over groups of prime order (2002)
Vol. 1790: X. Zhan, Matrix Inequalities (2002)
Vol. 1791: M. Knebusch, D. Zhang, Manis Valuations
and Prüfer Extensions I: A new Chapter in Commutative
Algebra (2002)
Vol. 1792: D. D. Ang, R. Gorenﬂo, V. K. Le, D. D. Trong,
Moment Theory and Some Inverse Problems in Potential
Theory and Heat Conduction (2002)
Vol. 1793: J. Cortés Monforte, Geometric, Control and
Numerical Aspects of Nonholonomic Systems (2002)
Vol. 1794: N. Pytheas Fogg, Substitution in Dynamics,
Arithmetics and Combinatorics. Editors: V. Berthé, S. Fer-
enczi, C. Mauduit, A. Siegel (2002)
Vol. 1795: H. Li, Filtered-Graded Transfer in Using Non-
commutative Gröbner Bases (2002)
Vol. 1796: J.M. Melenk, hp-Finite Element Methods for
Singular Perturbations (2002)
Vol. 1797: B. Schmidt, Characters and Cyclotomic Fields
in Finite Geometry (2002)
Vol. 1798: W.M. Oliva, Geometric Mechanics (2002)
Vol. 1799: H. Pajot, Analytic Capacity, Rectiﬁability,
Menger Curvature and the Cauchy Integral (2002)
Vol. 1800: O. Gabber, L. Ramero, Almost Ring Theory
(2003)
Vol. 1801: J. Azéma, M. Émery, M. Ledoux, M. Yor
(Eds.), Séminaire de Probabilités XXXVI (2003)
Vol. 1802: V. Capasso, E. Merzbach, B. G. Ivanoff,
M. Dozzi, R. Dalang, T. Mountford, Topics in Spatial
Stochastic Processes. Martina Franca, Italy 2001. Editor:
E. Merzbach (2003)
Vol. 1803: G. Dolzmann, Variational Methods for Crys-
talline Microstructure – Analysis and Computation (2003)
Vol. 1804: I. Cherednik, Ya. Markov, R. Howe, G. Lusztig,
Iwahori-Hecke Algebras and their Representation Theory.
Martina Franca, Italy 1999. Editors: V. Baldoni, D. Bar-
basch (2003)
Vol. 1805: F. Cao, Geometric Curve Evolution and Image
Processing (2003)
Vol. 1806: H. Broer, I. Hoveijn. G. Lunther, G. Vegter,
Bifurcations in Hamiltonian Systems. Computing Singu-
larities by Gröbner Bases (2003)
Vol. 1807: V. D. Milman, G. Schechtman (Eds.), Geomet-
ric Aspects of Functional Analysis. Israel Seminar 2000-
2002 (2003)
Vol. 1808: W. Schindler, Measures with Symmetry Prop-
erties (2003)
Vol. 1809: O. Steinbach, Stability Estimates for Hybrid
Coupled Domain Decomposition Methods (2003)
Vol. 1810: J. Wengenroth, Derived Functors in Functional
Analysis (2003)
Vol. 1811: J. Stevens, Deformations of Singularities
(2003)
Vol. 1812: L. Ambrosio, K. Deckelnick, G. Dziuk,
M. Mimura, V. A. Solonnikov, H. M. Soner, Mathematical
Aspects of Evolving Interfaces. Madeira, Funchal, Portu-
gal 2000. Editors: P. Colli, J. F. Rodrigues (2003)
Vol. 1813: L. Ambrosio, L. A. Caffarelli, Y. Brenier,
G. Buttazzo, C. Villani, Optimal Transportation and its
Applications. Martina Franca, Italy 2001. Editors: L. A.
Caffarelli, S. Salsa (2003)
Vol. 1814: P. Bank, F. Baudoin, H. Föllmer, L.C.G.
Rogers, M. Soner, N. Touzi, Paris-Princeton Lectures on
Mathematical Finance 2002 (2003)
Vol. 1815: A. M. Vershik (Ed.), Asymptotic Com-
binatorics with Applications to Mathematical Physics.
St. Petersburg, Russia 2001 (2003)
Vol. 1816: S. Albeverio, W. Schachermayer, M. Tala-
grand, Lectures on Probability Theory and Statistics.
Ecole d’Eté de Probabilités de Saint-Flour XXX-2000.
Editor: P. Bernard (2003)
Vol. 1817: E. Koelink, W. Van Assche (Eds.), Orthogonal
Polynomials and Special Functions. Leuven 2002 (2003)

Vol. 1818: M. Bildhauer, Convex Variational Problems
with Linear, nearly Linear and/or Anisotropic Growth
Conditions (2003)
Vol. 1819: D. Masser, Yu. V. Nesterenko, H. P. Schlick-
ewei, W. M. Schmidt, M. Waldschmidt, Diophantine
Approximation. Cetraro, Italy 2000. Editors: F. Amoroso,
U. Zannier (2003)
Vol. 1820: F. Hiai, H. Kosaki, Means of Hilbert Space
Operators (2003)
Vol. 1821: S. Teufel, Adiabatic Perturbation Theory in
Quantum Dynamics (2003)
Vol. 1822: S.-N. Chow, R. Conti, R. Johnson, J. Mallet-
Paret, R. Nussbaum, Dynamical Systems. Cetraro, Italy
2000. Editors: J. W. Macki, P. Zecca (2003)
Vol. 1823: A. M. Anile, W. Allegretto, C. Ring-
hofer, Mathematical Problems in Semiconductor Physics.
Cetraro, Italy 1998. Editor: A. M. Anile (2003)
Vol. 1824: J. A. Navarro González, J. B. Sancho de Salas,
C ∞– Differentiable Spaces (2003)
Vol. 1825: J. H. Bramble, A. Cohen, W. Dahmen, Mul-
tiscale Problems and Methods in Numerical Simulations,
Martina Franca, Italy 2001. Editor: C. Canuto (2003)
Vol. 1826: K. Dohmen, Improved Bonferroni Inequal-
ities via Abstract Tubes. Inequalities and Identities of
Inclusion-Exclusion Type. VIII, 113 p, 2003.
Vol. 1827: K. M. Pilgrim, Combinations of Complex
Dynamical Systems. IX, 118 p, 2003.
Vol. 1828: D. J. Green, Gröbner Bases and the Computa-
tion of Group Cohomology. XII, 138 p, 2003.
Vol. 1829: E. Altman, B. Gaujal, A. Hordijk, Discrete-
Event Control of Stochastic Networks: Multimodularity
and Regularity. XIV, 313 p, 2003.
Vol. 1830: M. I. Gil’, Operator Functions and Localization
of Spectra. XIV, 256 p, 2003.
Vol. 1831: A. Connes, J. Cuntz, E. Guentner, N. Hig-
son, J. E. Kaminker, Noncommutative Geometry, Martina
Franca, Italy 2002. Editors: S. Doplicher, L. Longo (2004)
Vol. 1832:
J. Azéma, M. Émery, M. Ledoux, M. Yor
(Eds.), Séminaire de Probabilités XXXVII (2003)
Vol. 1833: D.-Q. Jiang, M. Qian, M.-P. Qian, Mathemati-
cal Theory of Nonequilibrium Steady States. On the Fron-
tier of Probability and Dynamical Systems. IX, 280 p,
2004.
Vol. 1834: Yo. Yomdin, G. Comte, Tame Geometry with
Application in Smooth Analysis. VIII, 186 p, 2004.
Vol. 1835: O.T. Izhboldin, B. Kahn, N.A. Karpenko,
A. Vishik, Geometric Methods in the Algebraic Theory
of Quadratic Forms. Summer School, Lens, 2000. Editor:
J.-P. Tignol (2004)
Vol. 1836: C. Nˇastˇasescu, F. Van Oystaeyen, Methods of
Graded Rings. XIII, 304 p, 2004.
Vol. 1837: S. Tavaré, O. Zeitouni, Lectures on Probabil-
ity Theory and Statistics. Ecole d’Eté de Probabilités de
Saint-Flour XXXI-2001. Editor: J. Picard (2004)
Vol. 1838: A.J. Ganesh, N.W. O’Connell, D.J. Wischik,
Big Queues. XII, 254 p, 2004.
Vol.
1839:
R.
Gohm,
Noncommutative
Stationary
Processes. VIII, 170 p, 2004.
Vol. 1840: B. Tsirelson, W. Werner, Lectures on Probabil-
ity Theory and Statistics. Ecole d’Eté de Probabilités de
Saint-Flour XXXII-2002. Editor: J. Picard (2004)
Vol. 1841: W. Reichel, Uniqueness Theorems for Vari-
ational Problems by the Method of Transformation
Groups (2004)
Vol. 1842: T. Johnsen, A. L. Knutsen, K3 Projective Mod-
els in Scrolls (2004)
Vol. 1843: B. Jefferies, Spectral Properties of Noncom-
muting Operators (2004)
Vol. 1844: K.F. Siburg, The Principle of Least Action in
Geometry and Dynamics (2004)
Vol. 1845: Min Ho Lee, Mixed Automorphic Forms, Torus
Bundles, and Jacobi Forms (2004)
Vol. 1846: H. Ammari, H. Kang, Reconstruction of Small
Inhomogeneities from Boundary Measurements (2004)
Vol. 1847: T.R. Bielecki, T. Björk, M. Jeanblanc, M.
Rutkowski, J.A. Scheinkman, W. Xiong, Paris-Princeton
Lectures on Mathematical Finance 2003 (2004)
Vol. 1848: M. Abate, J. E. Fornaess, X. Huang, J. P. Rosay,
A. Tumanov, Real Methods in Complex and CR Geom-
etry, Martina Franca, Italy 2002. Editors: D. Zaitsev, G.
Zampieri (2004)
Vol. 1849: Martin L. Brown, Heegner Modules and Ellip-
tic Curves (2004)
Vol. 1850: V. D. Milman, G. Schechtman (Eds.), Geomet-
ric Aspects of Functional Analysis. Israel Seminar 2002-
2003 (2004)
Vol. 1851: O. Catoni, Statistical Learning Theory and
Stochastic Optimization (2004)
Vol. 1852: A.S. Kechris, B.D. Miller, Topics in Orbit
Equivalence (2004)
Vol. 1853: Ch. Favre, M. Jonsson, The Valuative Tree
(2004)
Vol. 1854: O. Saeki, Topology of Singular Fibers of Dif-
ferential Maps (2004)
Vol. 1855: G. Da Prato, P.C. Kunstmann, I. Lasiecka,
A. Lunardi, R. Schnaubelt, L. Weis, Functional Analytic
Methods for Evolution Equations. Editors: M. Iannelli,
R. Nagel, S. Piazzera (2004)
Vol. 1856: K. Back, T.R. Bielecki, C. Hipp, S. Peng,
W. Schachermayer, Stochastic Methods in Finance, Bres-
sanone/Brixen, Italy, 2003. Editors: M. Fritelli, W. Rung-
galdier (2004)
Vol. 1857: M. Émery, M. Ledoux, M. Yor (Eds.), Sémi-
naire de Probabilités XXXVIII (2005)
Vol. 1858: A.S. Cherny, H.-J. Engelbert, Singular Stochas-
tic Differential Equations (2005)
Vol. 1859: E. Letellier, Fourier Transforms of Invariant
Functions on Finite Reductive Lie Algebras (2005)
Vol. 1860: A. Borisyuk, G.B. Ermentrout, A. Friedman,
D. Terman, Tutorials in Mathematical Biosciences I.
Mathematical Neurosciences (2005)
Vol. 1861: G. Benettin, J. Henrard, S. Kuksin, Hamil-
tonian Dynamics – Theory and Applications, Cetraro,
Italy, 1999. Editor: A. Giorgilli (2005)
Vol. 1862: B. Helffer, F. Nier, Hypoelliptic Estimates and
Spectral Theory for Fokker-Planck Operators and Witten
Laplacians (2005)
Vol. 1863: H. Führ, Abstract Harmonic Analysis of Con-
tinuous Wavelet Transforms (2005)
Vol. 1864: K. Efstathiou, Metamorphoses of Hamiltonian
Systems with Symmetries (2005)
Vol. 1865: D. Applebaum, B.V. R. Bhat, J. Kustermans,
J. M. Lindsay, Quantum Independent Increment Processes
I. From Classical Probability to Quantum Stochastic Cal-
culus. Editors: M. Schürmann, U. Franz (2005)
Vol. 1866: O.E. Barndorff-Nielsen, U. Franz, R. Gohm,
B. Kümmerer, S. Thorbjønsen, Quantum Independent
Increment Processes II. Structure of Quantum Lévy
Processes, Classical Probability, and Physics. Editors: M.
Schürmann, U. Franz, (2005)
Vol. 1867: J. Sneyd (Ed.), Tutorials in Mathematical Bio-
sciences II. Mathematical Modeling of Calcium Dynamics
and Signal Transduction. (2005)

Vol. 1868: J. Jorgenson, S. Lang, Posn(R) and Eisenstein
Series. (2005)
Vol. 1869: A. Dembo, T. Funaki, Lectures on Probabil-
ity Theory and Statistics. Ecole d’Eté de Probabilités de
Saint-Flour XXXIII-2003. Editor: J. Picard (2005)
Vol. 1870: V.I. Gurariy, W. Lusky, Geometry of Müntz
Spaces and Related Questions. (2005)
Vol. 1871: P. Constantin, G. Gallavotti, A.V. Kazhikhov,
Y. Meyer, S. Ukai, Mathematical Foundation of Turbu-
lent Viscous Flows, Martina Franca, Italy, 2003. Editors:
M. Cannone, T. Miyakawa (2006)
Vol. 1872: A. Friedman (Ed.), Tutorials in Mathemati-
cal Biosciences III. Cell Cycle, Proliferation, and Cancer
(2006)
Vol. 1873: R. Mansuy, M. Yor, Random Times and En-
largements of Filtrations in a Brownian Setting (2006)
Vol. 1874: M. Yor, M. Émery (Eds.), In Memoriam Paul-
André Meyer - Séminaire de probabilités XXXIX (2006)
Vol. 1875: J. Pitman, Combinatorial Stochastic Processes.
Ecole d’Eté de Probabilités de Saint-Flour XXXII-2002.
Editor: J. Picard (2006)
Vol. 1876: H. Herrlich, Axiom of Choice (2006)
Vol. 1877: J. Steuding, Value Distributions of L-Functions
(2007)
Vol. 1878: R. Cerf, The Wulff Crystal in Ising and Percol-
ation Models, Ecole d’Eté de Probabilités de Saint-Flour
XXXIV-2004. Editor: Jean Picard (2006)
Vol. 1879: G. Slade, The Lace Expansion and its Applica-
tions, Ecole d’Eté de Probabilités de Saint-Flour XXXIV-
2004. Editor: Jean Picard (2006)
Vol. 1880: S. Attal, A. Joye, C.-A. Pillet, Open Quantum
Systems I, The Hamiltonian Approach (2006)
Vol. 1881: S. Attal, A. Joye, C.-A. Pillet, Open Quantum
Systems II, The Markovian Approach (2006)
Vol. 1882: S. Attal, A. Joye, C.-A. Pillet, Open Quantum
Systems III, Recent Developments (2006)
Vol. 1883: W. Van Assche, F. Marcellàn (Eds.), Orthogo-
nal Polynomials and Special Functions, Computation and
Application (2006)
Vol. 1884: N. Hayashi, E.I. Kaikina, P.I. Naumkin,
I.A. Shishmarev, Asymptotics for Dissipative Nonlinear
Equations (2006)
Vol. 1885: A. Telcs, The Art of Random Walks (2006)
Vol. 1886: S. Takamura, Splitting Deformations of Dege-
nerations of Complex Curves (2006)
Vol. 1887: K. Habermann, L. Habermann, Introduction to
Symplectic Dirac Operators (2006)
Vol. 1888: J. van der Hoeven, Transseries and Real Differ-
ential Algebra (2006)
Vol. 1889: G. Osipenko, Dynamical Systems, Graphs, and
Algorithms (2006)
Vol. 1890: M. Bunge, J. Funk, Singular Coverings of
Toposes (2006)
Vol.
1891:
J.B.
Friedlander,
D.R.
Heath-Brown,
H. Iwaniec, J. Kaczorowski, Analytic Number Theory,
Cetraro, Italy, 2002. Editors: A. Perelli, C. Viola (2006)
Vol. 1892: A. Baddeley, I. Bárány, R. Schneider, W. Weil,
Stochastic Geometry, Martina Franca, Italy, 2004. Editor:
W. Weil (2007)
Vol. 1893: H. Hanßmann, Local and Semi-Local Bifur-
cations in Hamiltonian Dynamical Systems, Results and
Examples (2007)
Vol. 1894: C.W. Groetsch, Stable Approximate Evaluation
of Unbounded Operators (2007)
Vol. 1895: L. Molnár, Selected Preserver Problems on
Algebraic Structures of Linear Operators and on Function
Spaces (2007)
Vol. 1896: P. Massart, Concentration Inequalities and
Model Selection, Ecole d’Eté de Probabilités de Saint-
Flour XXXIII-2003. Editor: J. Picard (2007)
Vol. 1897: R.A. Doney, Fluctuation Theory for Lévy
Processes, Ecole d’Eté de Probabilités de Saint-Flour
XXXV-2005. Editor: J. Picard (2007)
Vol. 1898: H.R. Beyer, Beyond Partial Differential Equa-
tions, On linear and Quasi-Linear Abstract Hyperbolic
Evolution Equations (2007)
Vol. 1899: Séminaire de Probabilités XL. Editors:
C. Donati-Martin, M. Émery, A. Rouault, C. Stricker
(2007)
Vol. 1900: E. Bolthausen, A. Bovier (Eds.), Spin Glasses
(2007)
Vol.
1901:
O.
Wittenberg,
Intersections
de
deux
quadriques et pinceaux de courbes de genre 1, Inter-
sections of Two Quadrics and Pencils of Curves of Genus
1 (2007)
Vol. 1902: A. Isaev, Lectures on the Automorphism
Groups of Kobayashi-Hyperbolic Manifolds (2007)
Vol. 1903: G. Kresin, V. Maz’ya, Sharp Real-Part Theo-
rems (2007)
Vol. 1904: P. Giesl, Construction of Global Lyapunov
Functions Using Radial Basis Functions (2007)
Vol. 1905: C. Prévˆot, M. Röckner, A Concise Course on
Stochastic Partial Differential Equations (2007)
Vol. 1906: T. Schuster, The Method of Approximate
Inverse: Theory and Applications (2007)
Vol. 1907: M. Rasmussen, Attractivity and Bifurcation for
Nonautonomous Dynamical Systems (2007)
Vol. 1908: T.J. Lyons, M. Caruana, T. Lévy, Differential
Equations Driven by Rough Paths, Ecole d’Eté de Proba-
bilités de Saint-Flour XXXIV-2004. (2007)
Vol.
1909:
H.
Akiyoshi,
M.
Sakuma,
M.
Wada,
Y. Yamashita, Punctured Torus Groups and 2-Bridge Knot
Groups (I) (2007)
Vol. 1910: V.D. Milman, G. Schechtman (Eds.), Geo-
metric Aspects of Functional Analysis. Israel Seminar
2004-2005 (2007)
Vol.
1911:
A.
Bressan,
D.
Serre,
M.
Williams,
K. Zumbrun, Hyperbolic Systems of Balance Laws.
Lectures given at the C.I.M.E. Summer School held in
Cetraro, Italy, July 14–21, 2003. Editor: P. Marcati (2007)
Vol. 1912: V. Berinde, Iterative Approximation of Fixed
Points (2007)
Recent Reprints and New Editions
Vol. 1618: G. Pisier, Similarity Problems and Completely
Bounded Maps. 1995 – 2nd exp. edition (2001)
Vol. 1629: J.D. Moore, Lectures on Seiberg-Witten
Invariants. 1997 – 2nd edition (2001)
Vol. 1638: P. Vanhaecke, Integrable Systems in the realm
of Algebraic Geometry. 1996 – 2nd edition (2001)
Vol. 1702: J. Ma, J. Yong, Forward-Backward Stochas-
tic Differential Equations and their Applications. 1999 –
Corr. 3rd printing (2007)
Vol. 830: J.A. Green, Polynomial Representations of
GLn, with an Appendix on Schensted Correspondence
and Littelmann Paths by K. Erdmann, J.A. Green and
M. Schocker 1980 – 2nd corr. and augmented edition
(2007)


Springer Series on Cultural Computing
Vasileios Mezaris
Claudia Niederée
Robert H. Logie    Editors 
Personal 
Multimedia 
Preservation
Remembering or Forgetting Images and 
Video

Springer Series on Cultural Computing
Editor-in-chief
Ernest Edmonds, University of Technology, Sydney, Australia
Series editors
Frieder Nake, University of Bremen, Bremen, Germany
Nick Bryan-Kinns, Queen Mary University of London, London, UK
Linda Candy, University of Technology, Sydney, Australia
David England, Liverpool John Moores University, Liverpool, UK
Andrew Hugill, De Montfort University, Leicester, UK
Shigeki Amitani, Adobe Systems Inc., Tokyo, Japan
Doug Riecken, Columbia University, New York, USA
Jonas Lowgren, Linköping University, Norrköping, Sweden
Ellen Yi-Luen Do, University of Colorado Boulder, Boulder, USA
Sam Ferguson, University of Technology Sydney, Sydney, Australia

More information about this series at http://www.springer.com/series/10481

Vasileios Mezaris
• Claudia Niederée
Robert H. Logie
Editors
Personal Multimedia
Preservation
Remembering or Forgetting Images and Video
123

Editors
Vasileios Mezaris
Centre for Research and Technology Hellas
Thermi, Thessaloniki
Greece
Claudia Niederée
University of Hannover
Hannover
Germany
Robert H. Logie
University of Edinburgh
Edinburgh
UK
ISSN 2195-9056
ISSN 2195-9064
(electronic)
Springer Series on Cultural Computing
ISBN 978-3-319-73464-4
ISBN 978-3-319-73465-1
(eBook)
https://doi.org/10.1007/978-3-319-73465-1
Library of Congress Control Number: 2017962551
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
The ease of creating digital content, especially photos and videos in nearly all life
situations, has created an explosion in the amount of personal digital content that is
continuously generated. Although declining storage device prices as well as cloud
and social media services provide short-term solutions for keeping this content,
over the years what we once stored is subject to a random form of digital forgetting,
or is just not revisited, because the mere numbers make this a tedious task, not to
mention the impact of changing life situations.
In this book, we advocate a novel forgetful approach to dealing with personal
multimedia content on the long run, which is inspired by the effectiveness of human
forgetting as a mechanism for helping us humans to stay focused on the important
things. We present the different theoretical foundations, technologies, as well as
applications and results of studies that help the reader understand the problems and
challenges associated with personal digital preservation, and the solutions that can
be developed in response to these challenges.
The book is organized into three main parts. Part I presents the necessary
Interdisciplinary Foundations, Part II covers Multimedia Preservation Theory, and
Part III discusses Multimedia Preservation in Practice.
Part I: Interdisciplinary Foundations
The ﬁrst question that is often raised when discussing the topic of digital preser-
vation is whether this is something that is really necessary or important to the
average individual. Chapter 1, “Multimedia preservation: why bother?,” attempts to
answer this very question, by explaining that just storing the content today does not
mean that this content will remain accessible and meaningful in the long run. Based
on the understanding of this fact, Chap. 1 proceeds to motivating a more intelligent
and selective approach to personal multimedia preservation. This approach intro-
duces and combines three key building blocks: (a) “managed forgetting,” for
focusing on important and useful content inspired by human forgetting and
v

remembering; (b) “contextualized remembering,” for dealing with evolution and
keeping content meaningful over time; and (c) “synergetic preservation,” for
bridging the gap between active information use and long-term information
management.
While Chap. 1 highlights problems and challenges associated with digital preser-
vation, we need to acknowledge that the preservation of information, in general, is
not a topic that ﬁrst appeared in the digital age. To the contrary, it is something that
was ﬁrst dealt with as part of evolution: humans have evolved to be very efﬁcient at
preservation of what it is necessary to preserve, and forgetting trivial or irrelevant
details when they are no longer needed. In order to allow us to draw inspiration from
nature, Chap. 2 looks at remembering and forgetting in human memory. It discusses
how the human superpowers of managed preservation and forgetting are achieved,
and how a conceptual understanding of human memory function could be used to
inspire the design of digital managed preservation and forgetting. Chapter 2 argues
that human-inspired digital forgetting is key for achieving a truly synergetic rela-
tionship between human and digital memory, and uses a study for exploring and
contrasting human management of photographic collections with managed preser-
vation and forgetting of the same photo collection by an example digital system.
Altogether, Chap. 2 highlights how understanding the human cognitive function can
help us to inspire more useful digital storage systems.
Having discussed how human memory works, Chap. 3 takes us to the opposite
side, discussing how computers can understand the digital multimedia content.
While for humans understanding what, e.g., a photo depicts is something that comes
naturally by just looking at the photo, it is not the same for computers: to them, in the
absence of specialized understanding methods, a photo is nothing more than a huge
array of bits. Thus, Chap. 3 discusses methods and algorithms that endow computers
with capabilities to understand digital content, based on the premise that under-
standing the digital content is important for subsequently supporting intelligent
preservation decisions. The methods discussed in this chapter include (a) photo/
video annotation, which refers to the problem of assigning one or more semantic
concepts to photos or video fragments; (b) photo/video quality assessment, which
refers to the automatic prediction of the aesthetic value of a photo or a video;
(c) near-duplicate detection, which aims to identify groups of very similar items in
large media collections; and (d) event-based photo clustering and summarization,
which concern the selection of the most characteristic photos of a photo collection so
as to create a storyline that conveys the gist of this collection.
Part II: Multimedia Preservation Theory
Chapter 4 goes into more depth regarding the intelligent and selective approach to
personal multimedia preservation that was sketched in Chap. 1, taking advantage
of the insights provided in Chaps. 2 and 3. Speciﬁcally, Chap. 4 focuses on a core
vi
Preface

ingredient of managed forgetting: the assessment of the importance of information
items. It introduces two key notions for describing this importance: “Memory
Buoyancy,” which, in the short-term, considers how information sinks away from
the user, and “Preservation Value,” which attempts to estimate the future impor-
tance of a digital resource in the long run. Chapter 4 then proceeds with outlining
methods for Preservation Value computation for different exemplary settings. It
also discusses managed forgetting beyond assessing the importance of information
items, that is, methods that can be used to implement managed forgetting on
top of the values for information importance. This includes methods such as
information hiding, forgetful search, summarization and aggregation, as well as
deletion.
Making informed assessments of the importance of information items and
decisions about their preservation is a big ﬁrst step, but even this does not ensure
that the preserved information will remain understandable and relevant in the long
run. For this, Chap. 5 looks into contextualization methods. Fully understanding
digital objects often requires knowing the wider context: for example, a family
photo is practically useless if you do not know who are the people portrayed. This
becomes even more important when considering the long-term preservation of
documents, as not only is human memory fallible, but over long periods the people
accessing the documents will change. Chapter 5 discusses methods for preserving
the context associated with a digital item, and for assessing how this context
evolves over time. It looks in detail at the relevant challenges and describes the
development of a conceptual framework in which context information can be
collected, preserved, evolved, and used to access and interpret documents.
A number of techniques are presented showing real examples of context in action
that ﬁt within the framework, and applying to both text documents and image
collections.
Chapter 6 takes the discussion of the Preserve-or-Forget (PoF) approach intro-
duced in this book to the system level. It proposes a reference model—the PoF
Reference Model which incorporates the techniques discussed in previous chapters
for Preservation Value assessment, contextualization, etc., while at the same time
paying special attention to the functionality which bridges between an Information
Management System and a Digital Preservation System (DPS). The design of the
PoF Reference Model was driven by the identiﬁcation of ﬁve required character-
istics: it has to be integrative, value-driven, brain-inspired, forgetful, and
evolution-aware. The proposed PoF Reference Model consists of three layers;
Chap. 6 goes on to discuss the main functional entities and the representative
workﬂows of each of them, relating them to existing standards and practices in
digital preservation. It also presents an architecture and an exemplary implemen-
tation for a system based on the PoF Reference Model.
Preface
vii

Part III: Multimedia Preservation in Practice
Based on the foundations and methods presented in the two previous parts of this
book, Chap. 7 presents the integration of preservation functionalities in a Personal
Information Management (PIM) application. In this application, the “semantiﬁca-
tion” of the user’s resources paves the way for more effective functionalities for
automated preservation, forgetting, and remembering embedded in the daily
activities of a user. The chapter also details a pilot based on this application, looking
in depth into user activities such as photo organization, and the generation of diaries
to remember past events. It investigates how forgetting functionalities can be
embedded in applications and describes how different variants for forgetting are
used in the pilot. Chapter 7 concludes with a discussion of experience of using the
pilot in daily work.
Chapter 8 continues with investigating the application of the methods presented
earlier in this book to the daily activities of users. In the ﬁrst part of this chapter, a
user study on a photo selection task is presented. Participants are asked to select
subsets of the most important pictures from their own collections. Because evalu-
ating the importance of photos to their owners is a complex process, which is often
driven by personal attachment, memories behind the content, and personal tastes
that are difﬁcult to capture automatically, this study allows us to better understand
the selection process. Then, based also on the ﬁndings of this study, the second part
of this chapter presents methods for automatically selecting important photos from
personal collections. Photo importance is modeled according to what photos users
perceive as important and would have selected, and an expectation-oriented method
for photo selection is presented, where information at both photo- and collection-
level is considered to predict the importance of photos.
Thessaloniki, Greece
Vasileios Mezaris
October 2017
Claudia Niederée
Robert H. Logie
viii
Preface

Acknowledgements
Most of the work reported throughout this book was supported by the EC’s Seventh
Framework Programme for Research, under contract FP7-600826 “ForgetIT:
Concise Preservation by combining Managed Forgetting and Contextualized
Remembering”, 2013–2016.
ix

Contents
Part I
Interdisciplinary Foundations
1
Multimedia Preservation: Why Bother? . . . . . . . . . . . . . . . . . . . . . .
3
Claudia Niederée, Vasileios Mezaris, Heiko Maus
and Robert H. Logie
2
Preserving and Forgetting in the Human Brain . . . . . . . . . . . . . . . .
9
Robert H. Logie, Maria Wolters and Elaine Niven
3
Multimedia Processing Essentials . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
Konstantinos Apostolidis, Foteini Markatopoulou, Christos Tzelepis,
Vasileios Mezaris and Ioannis Patras
Part II
Multimedia Preservation Theory
4
Preservation Value and Managed Forgetting . . . . . . . . . . . . . . . . . .
101
Claudia Niederée, Nattiya Kanhabua, Tuan Tran
and Kaweh Djafari Naini
5
Keeping Information in Context . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
Mark A. Greenwood, Nam Khanh Tran, Konstantinos Apostolidis
and Vasileios Mezaris
6
Bridging Information Management and Preservation:
A Reference Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
Francesco Gallo, Claudia Niederée and Walter Allasia
Part III
Multimedia Preservation in Practice
7
Remembering and Forgetting for Personal Preservation. . . . . . . . . .
233
Heiko Maus, Christian Jilek and Sven Schwarz
xi

8
Personal Photo Management and Preservation . . . . . . . . . . . . . . . . .
279
Andrea Ceroni
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
337
xii
Contents

Contributors
Walter Allasia EURIX Srl, Torino, Italy
Konstantinos Apostolidis Information Technologies Institute (ITI), Centre for
Research and Technology Hellas (CERTH), Thermi, Thessaloniki, Greece
Andrea Ceroni L3S Research Center, Leibniz Universität Hannover, Hanover,
Germany
Francesco Gallo EURIX Srl, Torino, Italy
Mark A. Greenwood The University of Shefﬁeld, Shefﬁeld, UK
Christian Jilek German Research Center for AI (DFKI), Kaiserslautern, Germany
Nattiya Kanhabua NTENT Inc., Barcelona, Spain
Robert H. Logie University of Edinburgh, Edinburgh, Scotland, UK
Foteini Markatopoulou Information Technologies Institute (ITI), Centre for
Research and Technology Hellas (CERTH), Thermi-Thessaloniki, Greece; Queen
Mary University of London, London, UK
Heiko Maus German Research Center for AI (DFKI), Kaiserslautern, Germany
Vasileios Mezaris Information Technologies Institute (ITI), Centre for Research
and Technology Hellas (CERTH), Thermi, Thessaloniki, Greece
Kaweh Djafari Naini L3S Research Center, Hannover, Germany
Claudia Niederée L3S Research Center, Hannover, Germany
Elaine Niven University of Dundee, Dundee, UK
Ioannis Patras Queen Mary University of London, London, UK
Sven Schwarz German Research Center for AI (DFKI), Kaiserslautern, Germany
Nam
Khanh
Tran L3S
Research
Center,
Leibniz
Universität
Hannover,
Hannover, Germany
xiii

Tuan Tran L3S Research Center, Hannover, Germany
Christos Tzelepis Information Technologies Institute (ITI), Centre for Research
and Technology Hellas (CERTH), Thermi-Thessaloniki, Greece; Queen Mary
University of London, London, UK
Maria Wolters University of Edinburgh, Edinburgh, Scotland, UK
xiv
Contributors

Part I
Interdisciplinary Foundations

Chapter 1
Multimedia Preservation: Why Bother?
Claudia Niederée, Vasileios Mezaris, Heiko Maus
and Robert H. Logie
Abstract Multimedia content and especially personal multimedia content is created
in abundance today. Short- to mid-term storage of this content is typically no problem
due to decreased storage prices and the availability of storage services. However, for
the long-term perspective, i.e., preservation, adequate technologies and best prac-
tices for keeping the content accessible and meaningful are still missing. Instead,
the breakdown of devices and changes in technologies lead to some form of random
survival and random forgetting for digital content. In this chapter, we motivate a
more intelligent and selective approach to personal multimedia preservation. This
approach introduces and combines three key building blocks: (a) “managed forget-
ting” for focusing on important and useful content inspired by human forgetting
and remembering; (b) “contextualized remembering” for dealing with evolution and
keeping content meaningful over time; and (c) “synergetic preservation” for bridging
the gap between active information use and long-term information management.
C. Niederée
L3S Research Center, Hannover, Germany
e-mail: niederee@L3S.de
V. Mezaris (B)
Information Technologies Institute (ITI)/Centre for Research
and Technology Hellas (CERTH), 6th Km. Charilaou-Thermi Road,
57001 Thermi-Thessaloniki, Greece
e-mail: bmezaris@iti.gr
H. Maus
German Research Center for AI (DFKI), Kaiserslautern, Germany
e-mail: heiko.maus@dfki.de
R. H. Logie
University of Edinburgh, Edinburgh, Scotland, UK
e-mail: rlogie@ed.ac.uk
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_1
3

4
C. Niederée et al.
1.1
Abundant Creation and Random Survival
With the advent of digital photography, taking photos and videos is nearly effortless
and requires few resources, and the widespread use of smartphones with continuously
improving camera features and storage capacities has accelerated this trend. A further
trigger for growing multimedia content creation is social media with its multifaceted
opportunities and incentives for content creation and sharing. In addition, nowadays,
taking photos and videos is tolerated nearly everywhere. Hundreds of photos are
easily taken by an individual participating in public events, such as concerts, as
well as in private events such as a holiday trip. Furthermore, photos and videos are
also very often taken of more mundane aspects of life, such as food or in support
of everyday activities such as shopping, further increasing the amount of personal
multimedia content to be dealt with. Thus, personal multimedia content is created in
abundance and with a tendency for further growth.
This raises the question of what happens to all of this personal multimedia content,
whichhastobeconsideredintheshort-term,mid-term,andthelong-termperspective.
With the decreasing price of storage media, it is not really a problem to store all of
this content; either in traditional digital storage devices such as hard disks or also on
the cloud, since storage is offered as a service by several cloud storage providers.
This situation fosters the adoption of a keep-it-all strategy for the short- to mid-term
perspective, where the majority of content created is kept “somewhere” (more or less
systematically organized).
However for the mid- to long-term perspective, just storing all of this content often
ends up as a kind of “dark archive” of photo, video, and other content collections,
which are rarely accessed (and enjoyed) again. The mere size of the collections makes
going through them as well as sorting or annotating them a tedious task.
Furthermore, for the long-term perspective, there is the risk of losing personal
content by a random form of “digital forgetting” [191]: Over decades, storage devices
such as hard disks may break down, and employed storage media become subject to
decay, loss, and accidental destruction or even theft. Moreover, even cloud storage is
vulnerable if the companies providing the service go out of business. Furthermore,
with the development and adoption of new technologies, existing formats and storage
media quickly become obsolete. These developments make random parts of personal
collections inaccessible. Just consider, for instance, how difﬁcult it would be today to
access photos stored years ago in .mos format in a ﬂoppy disk, or that even accessing
photos stored much more recently in a DVD is not that straightforward any more,
given that your new ultrabook laptop does not cater for reading this type of external
media. This leads to a random form of survival for personal multimedia content. This
weakness of digital content of being seemingly easy to keep but also easy to “lose”
has raised discussions about “digital dark ages” already in the late 90s [51, 212]. It
is possible to read a 200-year-old book, or look at 100-year-old printed photos, but
in a hundred years from now, will the technology exist to read the ﬁle and media
formats being used today? And, even if pure format readability is ensured by timely
transformation to more up-to-date formats and copying in contemporary storage

1
Multimedia Preservation: Why Bother?
5
media, more investment is still required for ensuring long-term interpretability of
content. Over the years, we might simply forget what or whom a photo shows or
why it was taken.
What is actually missing are best practices and supporting technologies for dealing
with personal multimedia content in the long run. Both the risk of dark archives and
of digital forgetting highlight the need to select, supported by automated methods,
the most important content and to invest some effort into keeping them enjoyable
and accessible not only over a lifetime but possibly also for future generations.
1.2
State of Affairs in Personal Preservation
“Digital Preservation”—i.e., secure long-term storage of content, considering time
frames of decades and longer—is a systematic approach for avoiding random digital
forgetting. It embraces tools, technologies, as well as organizational aspects.
However, while preservation of digital content is now well established in memory
institutions, such as national libraries and archives, it is still in its infancy in most
other organizations, and even more so for personal content. There are several obsta-
cles for the wider adoption of preservation technology in organizational and personal
information management: There is a considerable gap between active information
use and preservation activities. Active information use refers to dealing with infor-
mation objects for everyday private or professional activities, typically supported by
some information management environment, such as a content management system
in an organization or a desktop environment in the context of personal information
management. In addition, especially in personal information management, there is
typically little awareness for preservation. Although the need for personal preser-
vation has been recognized in theory (e.g., [271]), this has not yet propagated to
more practical settings and solutions. As a consequence, readiness for investing
considerable resources in terms of time and money for preservation is low. Finally,
establishing effective preservation and concise and usable archives still requires a
lot of manual work for selecting content that is relevant for preservation and for
keeping the archives accessible and meaningful long term, thus entailing expenses
much larger than just the storage costs. This is further aggravated by the fact that
no beneﬁts are seen for moving from more or less systematic backup to systematic
preservation.
A personal information space consists of a substantial number of information
objects connected to the person’s life such as wedding videos, travel pictures, or
graduation keepsakes. It requires serious dedication and cognitive effort to organize
all these data and keep them accessible as time passes. Moreover, these digital arti-
facts often represent past moments but are not associated with a physical memento.
Therefore, they form valuable resources for the user and future generations. If the
material is lost or corrupted due to improper conservation, it will be useless and
memories might be lost. Most users still use backups as their main form of preserva-
tion. John et al. in [186] surveyed 2600 academics and members of the digital public

6
C. Niederée et al.
about their preservation strategies. 60% of the respondents relied on backups. If data
were lost, which happened to 30% of the participants, the most common reason for
the loss (70% of all cases) was inability to ﬁnd the ﬁles again.
Furthermore, many people follow the keep-it-all strategy. Marshall [271] points
out ﬁve main reasons: (a) assessing value of resources in advance is difﬁcult, (b)
keep-it-all is currently the best practice, (c) deletion and (d) sorting out resources
are cognitively demanding exercises, and (e) archived information resources play an
important role as memory prosthesis.
There are preservation guidelines aimed at the general public that show how to go
beyond backups. For example, the Library of Congress raises awareness of personal
archiving solutions on their website and provides practical information [232, 234].
The recommended steps1 are as follows:
1. Identify what you want to save.
2. Decide what is most important to you.
3. Organize the content (descriptive ﬁle names and folders).
4. Save copies in different places.
5. Manage your archive (including migration plans).
These recommendations are very helpful but leave all steps to the user, i.e., what
to save, how to organize, where to store (hard disk and online storage), and when to
migrate. This puts a high burden on the user: various decisions need to be made and
it requires discipline in, e.g., maintaining and updating the archive.
This cognitive up-front effort is one of the reasons why the cloud storage offered
by DropBox, Microsoft OneDrive, or Google Drive is not a preservation system in
itself, but only a tool in a larger preservation strategy. Started as syncing, ﬁle sharing,
and backup solutions, those services offer structuring methods such as ﬁle folders
or (keyword-) tags, but do not comply with preservation best practices such as the
Open Archival Information System (OAIS) standard [68]. Other services, such as
Amazon Cloud, comply with OAIS, but do not support users before ingesting data
into the store. Either way, users are left on their own for large parts of the preservation
process. An overview of the preservation functionality of major cloud services can
be found in [331].
Another issue is preserving social media content. While service providers care-
fully store users’ data for in-depth analysis, they often do not provide any support
for dedicated archiving and preservation. Even when archivists are called in at a later
stage, preservation is not optimal, since it is not part of the business model [308]. As
shown, for example, in the survey reported in [189], users often post information in
social media, which might be worth preserving in a personal archive such as informa-
tion about lifestyle and data about travel, festivities/parties, and funny events. Many
services have emerged around curating social media content in a form that is easily
accessible, and would lend itself well to further preservation. At the most basic level,
Twitter allows users to download a compressed ﬁle that includes all of their tweets,
which can then be viewed in a browser. The Storify service allows users to curate
1There are some specializations depending on the media, e.g., video or social media.

1
Multimedia Preservation: Why Bother?
7
conversations on Twitter—tweets that are replies to each other are collated into a
single web page and can be annotated further [81].
Having a look at services like Flickr where people collect and share digital
moments, or services such as Twitter that focus more on real-time short messaging,
long-term preservation is not the ﬁrst thing which comes into one’s mind. Neverthe-
less, Twitter also treats concerns on archiving: they introduced a service to download
a zip archive of the user’s tweets. Furthermore, Google and Facebook have intro-
duced formal ways of handling an account in case a user is deceased, such as handing
it over to a dedicated contact person with full or partial access [57].
1.3
Preserve or Forget: Managed Forgetting and Digital
Preservation
To ease the adoption of more robust and beneﬁcial preservation practices for personal
multimedia content, we propose the introduction of a radically more adoptable and
sustainable approach, the “Preserve-or-Forget” approach to intelligent preservation
management, which combines three novel concepts:
• Inspired by the role of forgetting in the human brain, we envision a concept of
managed forgetting for systematically dealing with information that progres-
sively ceases in importance and ﬁnally becomes obsolete, as well as for redundant
information. This concept is expected to help in preservation decisions and to
create direct beneﬁts for active information use. At ﬁrst glance, forgetting seems
to contradict the idea of preservation: Preservation is about keeping things, not
about throwing them away. However, if no special actions are taken for long-term
preservation, we already face a rather random digital forgetting in the digital world
today. As discussed above, this is triggered, for example, by changing hardware,
hard disk crashes, technology evolution, and changes in life circumstances. We
aim to replace such random forgetting processes with managed forgetting, where
users are optimally supported in their explicit decisions about what to keep, and
how what is kept to be organized and preserved. In particular, we envision an idea
of gradual forgetting, where complete digital forgetting is just the extreme, and a
wide range of different forgetting actions such as summarizing are foreseen. This
draws on the principles of a highly efﬁcient process of forgetting in the human
brain for information that is trivial, redundant, or only required on a single occasion
for a short time.
• For bridging the chasm that still separates active information use from content
preservation activities, we envision the concept of synergetic preservation, which
couples information management and preservation management, making the intel-
ligent preservation integral to the content life cycle in information management.
This clearly supports easier adoption, and by enabling a rich information ﬂow from
the information context to the preservation context more intelligent and informed
preservation decisions, e.g., for preservation selection and contextualization.

8
C. Niederée et al.
• To bring preserved information back into active use in a meaningful way, even if a
long time has passed since their transition into the archive, we envision the concept
of contextualized remembering, again inspired by processes in the human brain.
The idea here is to already equip resources with rich context information when
packaging them for preservation (thus preparing them for long-term interpreta-
tion) and to gradually evolve this context information, reﬂecting the evolution in
terminology, semantics, and interpretation context, thus reaching a semantic level
of preservation.
The vision of our Preserve-or-Forget approach is a transition from pure archives to
managing and preserving concise knowledge ecosystems, coupling information man-
agement and preservation. Realizing such an ecosystem requires a concise, diversity-
aware, and evolution-aware preservation approach which includes a careful selection
of what to preserve taking into account coverage, diversity, importance and over-
lap/redundancy of information, the explicit contextualization of preserved resources
into self-contained objects to ensure long-term interpretability, as well as adequately
dealing with evolution and with information becoming obsolete.
In this book, we discuss the conceptual foundations, architectural aspects, as well
as effective methods for implementing the Preserve-or-Forget approach. Many of the
discussed methods and technologies have been developed in the European project
ForgetIT, in which the editors and authors of this book were involved.

Chapter 2
Preserving and Forgetting in the Human
Brain
Robert H. Logie, Maria Wolters and Elaine Niven
Abstract Humans have evolved to be very efﬁcient at managed preservation of what
is necessary to preserve. Humans are also extremely efﬁcient at forgetting trivial or
irrelevant details when they are no longer needed. Indeed, managed preservation and
forgetting could be viewed as a set of human ‘superpowers’ achieved through use
of a lifetime of accumulated knowledge, highly effective contextualisation, aggrega-
tion, organisation, summarisation and reconstruction of key features of experiences.
But humans are poor at preservation of large amounts of detail. Typically, memories
are partially reconstructed during the retrieval process, and this reconstruction pro-
cess can sometimes lead to false memories. Many of these strengths and limitations
of human memory are well understood by human memory researchers, although
important questions and uncertainties remain. In complete contrast, digital systems
excel in preserving large amounts of detail, and are getting better at contextualisa-
tion. But they remain rather poor at systematic forgetting of irrelevant detail. Often,
digital forgetting occurs by accident through disk crashes, incompatible upgrades
of software and hardware, lost or stolen storage devices. Even if the data are still
present and safely stored, insufﬁcient indexing and poor information retrieval may
result in those data effectively being forgotten. This chapter will provide a detailed
overview of the state of the science of human memory, based on empirical studies
and conceptual modelling. It will discuss how the human superpowers of managed
preservation and forgetting are achieved, and show how a conceptual understanding
of human memory function could be used to inspire the design of digital managed
preservation and forgetting. It will argue that human-inspired digital forgetting is key
for achieving a truly synergetic relationship between human and digital memory, and
explore how such a synergetic relationship can address aspects of the paradox that
massive investment in technology has not necessarily led to the expected increase in
R. H. Logie (B) · M. Wolters
University of Edinburgh, Edinburgh, Scotland, UK
e-mail: rlogie@ed.ac.uk
M. Wolters
e-mail: maria.wolters@ed.ac.uk
E. Niven
University of Dundee, Dundee, UK
e-mail: e.niven@dundee.ac.uk
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_2
9

10
R. H. Logie et al.
productivity (IT/productivity paradox). Next, we will describe an in-depth study of
personal, digital photograph collections that were contributed by volunteer research
participants. This study explored human management of photographic collections
and contrasted it with managed preservation and forgetting of the same photo col-
lection by an example digital system that incorporates automated conceptualisation
and forgetting. The chapter will conclude with a summary of how understanding
human cognitive function can help to inspire more useful digital storage systems
that offer reliable and usable tools to complement and support human memory rather
than attempt to replace it.
2.1
Human Memory and Forgetting
2.1.1
What Is Human Memory?
Human memory takes many forms and serves a wide range of purposes that are
essential for humans to function in everyday personal and working life. Among
the lay public, it is most widely associated with preservation and retrieval of infor-
mation about public and personal events. However, scientiﬁc study takes a much
broader view of human memory to include the acquisition, preservation and retrieval
of knowledge and skills (semantic memory and procedural memory), events and
experiences across a person’s lifetime (episodic memory), and remembering to carry
out intended actions (prospective memory). It also applies to the temporary storage
and moment-to-moment updating of information required for a focus on the current
task, an ability known as ‘working memory’. Finally, it applies to a range of control
functions that can suppress or inhibit information that is irrelevant or redundant and
that can detect or recognise whether information has been encountered previously
or is linked with previously preserved information.
2.1.2
Research Methods for Human Memory
Each individual has different experiences and different memories of those experi-
ences, as well as different knowledge and skills accumulated over their lifetime. Also,
people vary in the efﬁciency with which they can learn and retrieve new knowledge,
and encode, forget, preserve or retrieve details of events that they have experienced.
However, the study of human memory is based on the assumption that the general
principles that govern human memory organisation and function are the same across
all healthy human adults, and are the result of the effect of evolution on the human
brain. So, there are common principles across all humans for learning, forgetting,
encoding, preserving, retrieval and so on. Many of these principles also apply to
many animals. An analogy would be that the principles governing the function of

2
Preserving and Forgetting in the Human Brain
11
other aspects of physiology, such as the heart, the liver, the kidneys, the lungs, the
immune system or the endocrine system, are the same across all healthy human
adults, even if these systems differ in their efﬁciency between individuals. On this
principle, if we are collecting objective memory performance data, we need only
study the principles of functioning of human memory in a single healthy adult and
these principles should generalise to all healthy human memory.
However, experimental data are inevitably noisy and an individual who is studied
might have some underlying anomaly such as subtle undetected brain damage, or
be simply uncooperative. Therefore, in practice, multiple healthy participants are
recruited and allocated randomly to different experimental groups for the purposes
of comparison. Using this approach, the numbers in each group need not be large,
although there is usually a matching in the age range and educational level to mitigate
the variability from differences in memory efﬁciency. Data are then averaged and
analysed statistically across participants within each group to reduce the impact of
possible idiosyncrasies of any one participant. In contrast to studies that involve
subjective opinions, self-report or survey data, in experimental studies of this kind,
there is less emphasis on sampling from cross sections of the population. Therefore,
most research on human memory involves designing and running experiments with
human volunteers who are presented with material and are subsequently tested on
their memory for that material.
Experiments are designed according to theoretical, conceptual models of how a
speciﬁc aspect of human memory might function, and conclusions are drawn from
detailed analyses of the pattern of memory errors that result from different experi-
mental manipulations or different kinds of material. Some of the experiments involve
assessing the processes and accuracy of retrieval of real-life events, whereas others
involve relatively artiﬁcial materials. There is also a large amount of research of this
kind with volunteers who have suffered speciﬁc forms of brain damage, and these
studies can reveal some of the characteristics of healthy human memory as well
as the nature of memory impairments from which the patients suffer. Other experi-
ments involve exploring the patterns of brain activation while human volunteers are
completing memory tasks. The development of the theoretical, conceptual models
is driven by the patterns of results from these experiments, and converging evidence
across studies.
2.1.3
General Principles of Human Memory
The research approach described in the previous section has generated a large volume
of evidence for some general principles of memory function. Most human memory
researchers agree about these principles (for an overview see [27]), but there are
ongoing debates about the details of the conceptual models of memory and the
interpretation of patterns of results. As a result, there is currently no universally
accepted conceptual model of human memory. An example of one conceptual model
of human memory is illustrated in Fig.2.1. Some of the details of this model have

12
R. H. Logie et al.
Fig. 2.1 A Conceptual
Model of Human Memory
been developed by the ﬁrst author of this chapter (e.g. [251–253]), based on a simpler
model originally proposed in [29]. However, it has characteristics that are similar to
other contemporary conceptual models.
Insummary,thisconceptualmodelindicatesthatinformationfromauditory,visual
and other forms of perception (e.g. tactile) activates stored knowledge accumulated
over a lifetime regarding knowledge about the world and about the self (‘semantic
memory’) and preserved information about individual events (‘episodic memory’)
related to the perceived stimuli. Some of the activated knowledge is held on a tempo-
rary basis in a collection of interacting, domain-speciﬁc temporary memory systems
or components of working memory, and processed by a range of executive functions.
For example, combinations of meaning, shape and sound may be held together as
Currently Activated Knowledge. Details of recently perceived stimuli that have been
seen or heard may be held as sound-based codes in the phonological store component
or as visually based codes in the Visual Cache component. Both types of code decay
within around 2s, but the inner speech component can allow the sound-based codes
to be held for longer by mentally repeating the sounds. The Inner Scribe component
holds and can mentally rehearse sequences of movements and can allow visual codes
to be held longer by mentally rehearsing the codes held in the Visual Cache.
It should be noted that the theoretical, conceptual models of human memory such
as the one shown in Fig.2.1 are used as frameworks to generate hypotheses and
to guide the design of memory experiments. They are not formal computational
models that have clearly deﬁned characteristics for each component, or that describe
precisely what information ﬂows along the arrows between components. There are
some formal computational models of speciﬁc functions of human memory (for
example of the phonological store and inner speech shown in Fig.2.1), and these

2
Preserving and Forgetting in the Human Brain
13
are used to run simulations of the behavioural data patterns obtained from memory
experiments with human volunteers. However, these formal models are beyond the
scope of the current chapter. Figure2.1 is included here to set a context for the reader
who is unfamiliar with the approaches and style of research summarised later in this
chapter.
2.1.4
Semantic and Episodic Memory
A key distinction is between semantic and episodic memory. Semantic memory
includes knowledge acquired through life experience (e.g. language, facts about the
world, people and the self) and learned skills, sometimes referred to as procedural
memory (e.g. swimming, riding a bicycle, mathematics). Episodic memory refers
to memory for speciﬁc experiences of events that took place at a particular place
and at a particular time (e.g. a particular holiday, meeting, lecture or social event or
what you had for dinner yesterday). In [394], the authors introduced the concept of
episodic memory as a system that underlies the ‘what-when-where’ speciﬁcs of an
event, and as such is distinct from factual knowledge in semantic memory [391]).
Forgetting from episodic memory is rapid and substantial. Forgetting from seman-
tic memory is much less rapid and information is well preserved over long periods
or never lost. Semantic memory is thought to develop across the lifetime by extract-
ing features that are common across similar events, and building what are known as
schema for speciﬁc types of events, a concept ﬁrst proposed by Bartlett [34]. Details
of the occasion on which the information was ﬁrst encountered are forgotten.
For example, a restaurant schema includes tables, menus, food, conversation,
waiting staff and paying a bill, but we probably cannot recall when we ﬁrst learned
these features of restaurants. In the same way, we know that the capital of France
is Paris, but are unlikely to remember when that fact was ﬁrst encountered. This
means that the schema sets the general context for each restaurant visit, and provides
a ‘framework’ on which to build the memory for key details of speciﬁc visits to
restaurants. The features that are common to each restaurant visit need not be stored
on every occasion. The same is true of any common experience, such as a working
day, a visit to a swimming pool, a train or aeroplane journey. The framework or
context can then be used to aid retrieval of information about speciﬁc events. When
recalling a restaurant visit, we can assume that there was a menu, food, a table, a
bill, etc., and so only have to store and retrieve the key information such as who
else was at the table, and what was important about the conversation or about the
food. In summary, human memory tends to preserve generic information that is
repeated across similar experiences and events without ‘tagging’ that information
with a time and place. Human memory tends to forget details that are unique to
individual experiences or events, except when the unique features of a particular
event are particularly important for the individual.
A further role for a schema or context is in the understanding or interpretation of
presented information or events. Take, for example, the following paragraph.

14
R. H. Logie et al.
The procedure is actually quite simple. First, you arrange the items into different groups. Of
course, one pile might be sufﬁcient depending on how much there is to do. If you have to go
somewhere else due to lack of facilities, that is the next step; otherwise, you are pretty well
set. It is important not to overdo things. That is, it is better to do too few things at once than
too many. In the short run, this may not seem important but complications can easily arise. A
mistake can be expensive as well. After the procedure is completed, one arranges the material
into different groups again. They then can be put into their appropriate places. Eventually,
they will be used once more and the whole cycle will then have to be repeated [52, 134-5].
This passage is difﬁcult to understand and also is difﬁcult to remember because
there is no context. Most of the material from the passage will already have been
forgotten as you are reading this sentence. However, after the context for the passage
is given as ‘washing clothes’, then the interpretation of the text is trivial and memory
for the sequence of procedures can be generated from existing knowledge in the
schema without having to remember the exact wording. The use of the schema can
be repeated every time this kind of activity is required, and precise details of each
occasion do not have to be preserved in memory, unless, for example, an error on
one occasion is to be avoided on future occasions, or a small change to the procedure
results in a beneﬁt that should be remembered for future repetition.
However, even when a context is available, substantial forgetting of detail occurs
within minutes. For example, although the general meaning of the washing clothes
paragraph could be regenerated, the precise wording is unlikely to be remembered
accurately. Likewise, within a few seconds of reading the text of the current para-
graph, any reader will have forgotten the exact wording used but will remember
the meaning of the text. For readers who are not already familiar with the topic of
how human memory functions (i.e. have no accurate or detailed existing schema or
context), many of the detailed facts presented in this chapter will be forgotten within
an hour after it has been read, unless this material is relearned before this forgetting
occurs (e.g. [195]).
Therefore, a great deal of information concerning an event is never stored in mem-
ory. Because there is a large number of schemata and a large amount of information
accumulated in semantic memory over the lifetime of each individual, the human
memory system can select what information is necessary to set the context for the
current environment or information presented, and can inhibit or ignore information
that is irrelevant or can be assumed from the context. This aspect of human memory
is a major strength in that it avoids the distraction of information that is irrelevant or
redundant for the current task, and avoids the storage of large amounts of irrelevant or
redundant information, making it very efﬁcient for storing and retrieving key details
about an event, or retrieving key facts and skills that are required for the current task.
2.1.5
Forgetting from Episodic and Semantic Memory
The process of forgetting from semantic or episodic human memory typically refers
to the inability to retrieve information that has previously been stored, and this is often

2
Preserving and Forgetting in the Human Brain
15
Fig. 2.2 Human forgetting
over time
viewed as an unwelcome limitation. However, detailed analysis shows forgetting to
be more complex, and to be a beneﬁt to humans most of the time. As should be
clear from the previous section, a substantial amount of detail is never encoded in
memory. It is equally well established that of the details that are encoded, a substantial
amount is forgotten within a short time after the initial experience. This prevents
the memory system from being ﬁlled with information for which there is no clear
context, or that is largely irrelevant, or which is required only on a temporary basis,
and preservation is not normally required. Consequently, only information that is
important for understanding and functioning in the world tends to be preserved.
The forgetting of information that lacks context was ﬁrst subject to systematic
study by the German researcher Ebbinghaus [122] who experimented with learning
and remembering ‘nonsense material’, speciﬁcally three-letter syllables (e.g. BAZ
FUB YOX DAX LEQ VUM . . .) for which he had no established schema. Therefore,
this kind of material was selected to assess ‘pure’ episodic memory without the
support of semantic memory. In his experiments, he would spend several minutes
trying to learn sequences of these nonsense syllables, and then tested his memory by
attempting to relearn the material at different time periods after the initial learning.
Typical results from his experiments are shown in Fig.2.2. It is clear from the ﬁgure
that most of the forgetting had occurred within one hour of the learning, but the small
amount of material that was retained after one hour was retained for at least 48h.
Although the work of Ebbinghaus was important for understanding memory and
forgetting, it was unclear as to why the material was being forgotten. The research was
criticised because of the reliance on memory for nonsense material and because only
one individual was tested, namely Ebbinghaus himself. It is rare that adult humans
are required to remember material for which they have no schema or context, and
as mentioned above, most experimental studies of memory involve an investigation
of the aggregate results from groups of individuals. The issue of what might be
the main causes of forgetting has been the subject of scientiﬁc debate ever since
the time of Ebbinghaus, with the major possibilities being decay of the material
over time, or other material causing interference with the memory representation.

16
R. H. Logie et al.
In the case of decay, information is lost over time through gradual deletion from
memory of material that is rarely or never accessed and retrieved. In the case of
interference, the forgetting may arise from an inability to retrieve key details of
an event because of interference from previously stored details about similar events
(‘proactive interference’—e.g. [398], or because of interference from stored details of
similar subsequent events (‘retroactive interference’, e.g. [287]. More recent studies
have demonstrated interference-based forgetting of a ﬁrst language when trying to
learn a second language [177]), an example of retroactive interference: learning of
the new language interferes with memory for the previously learned language. Other
studies have shown that parking a car in different spaces in the same car park multiple
times (e.g. at work or near a retail centre) can make it difﬁcult to remember where
the car was parked today [97]. This is a common experience and often suggests
to people that their car has been stolen until they realise that they are looking in
the space that they used for their car yesterday or last week. This is an example
of proactive interference: multiple similar previous experiences interfere with the
ability to remember details of the most recent instance of this experience.
The Ebbinghaus forgetting function applies beyond the forgetting of context-less
material. It also applies to the forgetting of material that can be supported by context
from semantic memory. Take, for example, the results from a study published 100
years after the Ebbinghaus studies. McKenna and Glendon [288] tested memory in
people who had undertaken and successfully completed a ﬁrst aid course. At intervals
varying from 3 to 36months, they were tested on their memory for their ability to
diagnose the health problem associated with particular symptoms, their resuscitation
technique and performance as well as on a total score for the knowledge they had
retained from the course. Despite spending several days on the ﬁrst aid course, and
passing the test at the end of the course, within 3 months they had forgotten 70% of
their knowledge about diagnosis, and after 6 months they had forgotten 60% of even
their best preserved ability, namely their technique for cardiopulmonary resuscitation
(CPR). However, over the following 30 months, the rate of forgetting was very much
slower than it was during the ﬁrst 6 months. In the case of Ebbinghaus, learning took
place over a few minutes with no schema or context, and forgetting was over periods
of minutes and hours. In the McKenna and Glendon study, learning took place over
several days and involved information and skills within the schema or context of
ﬁrst aid care. In this latter case, the forgetting was over periods of months rather
than hours. So, context as well as amount of initial learning greatly slows down the
speed of forgetting. However, the shapes of the forgetting functions were remarkably
similar, even if over different time periods. Equivalent results were found in [31] for
English native speakers remembering Spanish learned at school over delays of up to
50years, and retention of information learned at University up to 30years later [88].
In both studies, there was substantial forgetting within the ﬁrst few years after leaving
the formal learning environment, but then a much slower rate of forgetting thereafter.
These well-established studies show that when memory is supported by context
or schema, then the material can be retained for periods of months or years, but even
with this support, most of the forgetting of details still occurs within a relatively

2
Preserving and Forgetting in the Human Brain
17
Fig. 2.3 Forgetting with
context support versus
forgetting without context
support
short period and the material that remains after that initial period is forgotten much
more slowly. If never ‘relearned’ from time to time, all of the information may be
forgotten. For example, the people in the study in [31] who used Spanish in their
daily lives after leaving school retained their knowledge of Spanish very much more
successfully than those who had few subsequent opportunities to practice using the
Spanish they had learned. The same was true of the material learned at university in
the study in [88]. The difference in forgetting supported by context and without such
support is illustrated in Fig.2.3.
Afurthereverydayexampleofthesupportfromcontext(alsoillustratedinFig.2.3)
is remembering the number of a hotel room for the period of staying in the hotel, or
remembering a ﬂight number. The context of being in a particular hotel is supported
by the repeated requirement to retrieve the room number when asking for the key,
going into breakfast, or returning to the room, but after leaving the hotel there is no
requirement to retrieve the room number and so it is forgotten. The same is true for
the ﬂight number which need only be retrieved while at the airport but is not retrieved
again after the travelling is complete, and so is forgotten. Paper and electronic aids
are of course extremely useful in these circumstances, and are used widely to avoid
the need to retain this kind of information in memory even on a temporary basis.
Contextandschemaworkwellinsupportingmemorymostofthetime,butbecause
much of memory retrieval involves reconstruction of details based on schema rather
than actual memory for details, the reconstruction process can result in major errors
and false memories that the individual is convinced are genuine. For example, a
witness to a crime or accident can have a false memory for details of the people
present or of the incident. These false memories can arise because people assume
‘what must have happened’ based on their schema for such events rather than what
actually happened. False memories also arise because of subsequent experiences
(retroactive interference) and can result in accusing an innocent bystander of being

18
R. H. Logie et al.
the criminal, or recognising a face as vaguely familiar and falsely remembering them
as the person who had committed the crime. For example, a psychology researcher
who is also qualiﬁed as a legal practitioner, Donald Thomson was involved in the
case of a witness identifying their attacker as someone (Thomson himself) who was
appearing on a live television programme that the witness was watching at the time of
the attack. Thomson was arrested by the police but rapidly was able to prove that he
was somewhere else at the time of the incident. The witness had falsely remembered
the face of someone on the television programme (Thomson) as the face of the person
who committed the crime (see discussion in [27, p. 344]).
False memories also arise because of the phrasing of questions to the witness: a
question such as ‘how fast was the car travelling when it smashed into the wall?’ will
generate higher estimates of speed than the question ‘how fast was the car travelling
when it hit the wall?’, even although exactly the same incident was witnessed in
each case. The memory for the speed estimate is changed by the use of the phrase
‘smashed into’, when questioning the witness [248]. There are hundreds of docu-
mented cases of innocent people being arrested and convicted of crimes on the basis
of mistaken eyewitness identiﬁcation, but for whom subsequent DNA or other evi-
dence has proved their innocence. In many cases, their innocence was established
after many years in prison, or in some cases after imposition of the death penalty.
Despite this, the legal system in many countries relies heavily on the testimony of
eyewitnesses and many members of the legal profession, including judges, seem
unaware of the fallibility of human memory for events (for reviews of eyewitness
research see [241]). So, while one of the strengths of human memory is in the use
of schemas to allow memory reconstruction and avoid overloading with redundant
or irrelevant information, this process can also generate errors with serious conse-
quences. Some of these errors could be avoided with the use of external devices that
can record events as they occur to support accuracy in subsequent recall, as in the
case of the eyewitness.
2.1.6
Forgetting Intentions
A further key everyday aspect of memory is the forming of intentions to carry out
an activity at some point in the future, and then remembering to do so. This ability
is often referred to as prospective memory, in contrast to episodic memory which
is retrospective. Failures of prospective memory can result in embarrassment or
irritation when forgetting to meet a friend or mistakenly using shaving cream on
the toothbrush instead of toothpaste. However, failures of prospective memory also
can result in serious consequences, for example if a ﬂight is missed, medicine is
not taken, forgetting to put on a parachute when skydiving, or failing to close the
entrance ramp of a car ferry when leaving the harbour. These are just few examples
of numerous real incidents that also include major aeroplane and train crashes, and
major industrial accidents, and they have been known for some time to have resulted
from this kind of human memory error [336].

2
Preserving and Forgetting in the Human Brain
19
Ironically, many such errors arise from highly practiced activities that have been
performed many hundreds if not thousands of times previously, and so are performed
somewhat automatically. A novel distraction or preoccupation with a worry such as
an upcoming exam, an important interview, a sudden technical failure in equipment
or the breakdown in a relationship, can remove the very small amount of attention
required to ensure that these highly practiced activities are performed successfully,
resulting in an error that is sometimes referred to as ‘absent mindedness’. This could
result in absent-mindedly starting to drive on the route to work on a Saturday evening
instead of to the theatre. It could also result in an experienced pilot switching off
the starboard engine on an aeroplane instead of the port engine that is on ﬁre. The
underlying type of memory error is the same, but the consequences are dramatically
different.
Absent-minded errors occur over periods of a few seconds or minutes when a novel
or threatening scenario displaces an intended action in current memory (known as
working memory). Prospective memory failures also occur over longer periods of
time when it is impractical to ‘keep the intention in mind’ or in working memory, and
other activities or thoughts intervene. For example, setting off to walk to work with
the intention of posting a letter during the 40-min journey can result in arriving at
work without the letter being posted, unless posting the letter is the only thought kept
active while walking. A cue or reminder such as passing a post box can reduce the
chance of a prospective memory failure, but remembering to keep an appointment
or the date of a wedding anniversary would not be monitored continuously over
periods of days, weeks or years. In the absence of external aids such as a calendar
or a reminder on a smart phone, it has been argued that some of the intentions are
activated automatically from time to time, thereby acting as internal memory cues
(e.g. [283, 285]). However, if there are too many intentions formed then errors are
extremely likely without some form of external paper or electronic aid. It is also likely
that an intention will be remembered at the correct time (e.g. to catch a ﬂight), but that
details required to carry out that intention (e.g. the exact ﬂight time or ﬂight number)
might require some external memory aid. An external device that could monitor
actions required in speciﬁc contexts would also reduce the likelihood of memory
errors, for example, by preventing departure of a sky diver from the aeroplane at
3000m unless the parachute is in place correctly.
2.1.7
Temporary Memory and Working Memory
Equally crucial for everyday functioning is the human ability to retain information
on a temporary basis to allow completion of a current task or to function in a novel
environment. This ability is often referred to as working memory (e.g. [28, 98]). Here,
information is held for only a few seconds and continually updated, so forgetting
of details is almost immediate. For example, in order to understand the text you
are reading now, it is important to remember the text that you have just read, and
the most recently read text is continually being updated as you progress through

20
R. H. Logie et al.
the document. Likewise, successful driving on the motorway requires rapid and
regular updating of memory for the position of nearby trafﬁc and this is continually
updated with rapidly changing trafﬁc patterns. In neither example is there normally
any requirement to retain precise details such as the exact wording and font of the
text read 10min ago, or the precise position, model and colour of the car that was
overtaking 15min ago. Those details are important at the time, but not subsequently,
and so are held for just a few seconds and then are forgotten as the contents of working
memory are updated.
Working memory is used for almost every activity while humans are awake; men-
tal arithmetic, navigating around unfamiliar environments, keeping track of current
intentions and the ﬂow of a conversation, making a meal, creative thinking, or keying
a telephone number. It is thought to have capacity for around 3 or 4 items at any one
time (e.g. [98]), but items can be grouped together so as to have 3 or 4 groups or
chunks of information. However, its capacity can also be shown to be larger when
using rehearsal or when storing different types of information. So there is capacity
for holding 7 plus or minus 2 random numbers if the numbers are mentally rehearsed
[297], and 7 random numbers can be retained at the same time as a random visual
matrix pattern [80]. Without rehearsal, items in working memory are readily replaced
by new material on a second-to-second basis.
2.2
When Human Forgetting Is Minimised
Sometimes, people remember new information and events well, to the extent that
they can recall highly complex details for a long time. There are three major factors
that have been shown to be associated with minimal forgetting:
• Expertise
• Relearning
• Vivid Memories
These are explained in more detail below.
2.2.1
Effects of Speciﬁc Expertise on Memory
Expertise in the present context refers to the accumulation of specialist and advanced
knowledge in a particular domain, typically as a result of many hundreds or thousands
of hours of learning and practice in that domain. The expert has very detailed schema
that can support memory for details of past events related to the area of expertise.
For example, expert chess players can readily remember the positions of pieces from
multiple chess game positions [113], soccer fans can remember multiple scores from
matches played between teams with which they are familiar [303]. Even residential

2
Preserving and Forgetting in the Human Brain
21
burglars show superior memory for details of houses that are related to their area of
activity [250]).
The expertise can also be very speciﬁc. For example, in [130] Ericsson trained an
individual (Falloon) to be able to repeat back random sequences of up to 80 digits.
The individual had signiﬁcant expertise in athletics and learned to group parts of the
sequences as numbers related to his area of expertise. For example, the sequence
354 would be encoded as 3min 54s or a record time for running a mile. By creating
multiple combinations of numbers and making these meaningful in the context of
athletics, his expertise allowed memory for number sequences that greatly exceeded
the typical maximum random sequence length of 7 plus or minus 2 digits that most
adults are capable of recalling [297]).
However, the expertise did not confer any general enhancement of memory for
material outside the areas of expertise: Chess experts are not better than chess novices
when remembering the position of pieces shown at random positions on the board,
rather than from a real chess game, soccer experts are no better than those lacking
an interest in the game when trying to recall random sets of scores rather than scores
from real games, burglars can remember if a house had a burglar alarm but not the
colour of the curtains in the window, and Steve Falloon could not remember any
more than 7 random letters or words. In summary, the more accumulated knowledge
that a person has about a topic, the easier it is for him/her to remember details related
to that knowledge whether that be information technology, cognitive psychology,
journalism or stamp collecting.
2.2.2
Effects of Relearning
Relearning material effectively starts the process of transferring details about a spe-
ciﬁc event (episodic memory) into knowledge and expertise about the topic of that
material (semantic memory). If the relearning occurs very soon after the initial
encounter with the material, and before most of the forgetting has occurred, this
slows down the rate of forgetting. This then allows a longer time before relearning
is required, and this expanding spacing of learning has been shown to be effective
in dramatically reducing the rate and amount of forgetting that occurs [218]). An
example is illustrated in Fig.2.4. More recently, in [195], the authors have shown
that attempting to retrieve information from memory (self-test) is even more effective
than rereading or being represented with the same material.
2.2.3
Vivid Memories
Vivid memories are associated with events that are of major personal importance,
such as one’s own wedding, start of a new job, birth of one’s child or of major public
importance such political assassinations and resignations, natural disasters, major

22
R. H. Logie et al.
Fig. 2.4 Relearning slows
forgetting
accidents, important sporting events or major human achievements. In these cases,
people report having remarkably vivid memories that include many details that are
normally forgotten. The best known report was of people remembering where they
were, who they were with, what they were doing and what the weather was like
at the time that they heard, in 1963, that the US President John F Kennedy had
been assassinated, or that Martin Luther King had been assassinated several years
later. These memories were reported as remaining vivid more than 10 years after the
original event (e.g. [55]. Similar long-term preservation of vivid memories has been
recorded for the ﬁrst moon landing in 1969, the surprise resignation of the British
Prime Minister Margaret Thatcher in 1990 [87], and the attack on the World Trade
Centrein2001[326].However,eveninthecaseofpublicevents,memoriesweremore
vivid for events that were of personal interest. The resignation of Margaret Thatcher
was recalled most vividly among UK citizens who had an interest in politics, and
the death of the UK Princess Diana in 1997 was more vividly recalled by UK than
Italian citizens, although the 2001 attack on the World Trade Centre was remembered
equally well in the UK and Italy [215]. The claimed vividness and detail associated
with these event-based memories appeared to suggest that these kinds of memories
included a great deal more detail and were much more accurate and consistent over
time than are memories for other kinds of events (for a review see [258]), including
details speciﬁcally about place, informant, activity, own affect and aftermath [198].
Several researchers have concluded that these vivid memories arise because there
is considerable media coverage at the time, for some time afterwards and on anniver-
saries of each event. Major public events are also the main topic of conversation for
people at the time and for several days, if not weeks afterwards. In sum, the memo-
ries for the events are recalled and rehearsed many times over when discussing with
friends and through media coverage, making it very likely that those memories will
be preserved. The same is true of personal memories of events that are important
for the individual. One’s own wedding will be a topic of conversation and mental

2
Preserving and Forgetting in the Human Brain
23
activity for many months in advance and for months and years after the event. So,
repeatedly recalling events leads to long-term preservation in memory.
It is worth noting that although many people experience these vivid memories,
many of the details that they recall are incorrect, and are not consistent over time,
even although people are convinced of their accuracy. A number of researchers have
argued that the main difference between vivid memories and memories for other
events is that people are much more conﬁdent about their recall of the former rather
than there being an actual difference in the amount of accurate detail remembered
(e.g. [102, 103, 378]). For example, many people in the United States conﬁdently
report a vivid memory of watching the television broadcast of the ﬁrst plane to hit
the World Trade Centre on 11 September, 2001, but that event was not recorded or
televised. Only the aftermath of the crash and the video recording of the second plane
crashing into the building were available. This major error was not only true of many
US citizens [326], but also of the US President George W Bush [155]. Similar recall
errors have been found for other vivid memories of major public events.
So, vivid memories are subject to the same process of reconstruction as are other
memories. These kinds of memories were originally referred to as ‘ﬂashbulb mem-
ories’, suggesting that they involve preservation in memory of considerably more
detail than is the case for most memories of events. However, the ﬁnding that these
memories are often not accurate has led to more widespread use of the term ‘vivid
memories’ to reﬂect the personal experience of individuals rather than the accuracy
of the preserved memory (although not all researchers agree, see [105]). That is,
they follow the same general principles of remembering and forgetting of episodic
memory, but may be subject to considerably more rehearsal, are experienced as more
vivid, often have more emotional content and are recalled with greater conﬁdence
than other memories. Moreover, they are just as error prone and therefore do not
appear to be subject to any different forms of processing or memory preservation
compared with other memories [103].
2.3
Autobiographical or Personal Memory
2.3.1
Deﬁnitions of Autobiographical Episodic and Semantic
Memory
One major focus for this book is the preservation of personal information, and thus
far we have mainly referred to this kind of information primarily in order to illus-
trate how a conceptual understanding of human memory has been developed using
results from controlled, experimental studies of memories for experienced events.
However, there is also a research literature involving both experimental research and
theoretical development of conceptual frameworks regarding how personal memo-
ries, and memories about the self are stored and accessed. Personal memories can be
understood broadly within the same set of memory systems described earlier, with

24
R. H. Logie et al.
episodic memory being the host for memories of personal events, semantic mem-
ory being the host for general information about oneself, and working memory for
holding in mind information regarding current activities.
However, when referring to personal memory, the term autobiographical memory
is often used instead of episodic memory to emphasise that the research is focused
on memory for real-life everyday personal experiences, as distinct from episodic
memory for numbers, letters, word lists or visual patterns that are used when studying
thisformofmemoryincontrolledlaboratoryexperiments.Tulving[392]extendedthe
concept of episodic memory to include: a sense of subjective time (that we mentally
revisit/travel back in time), autonoetic awareness (that we are aware of our memory
experience as different to our experience of the immediate environment) and a sense
of self (speciﬁcally, a sense of self that can exist in subjective time). He has also
argued [391, 392] that memory measures of a task completed in a laboratory should
not be considered autobiographical because they are focused on ‘what’ (remembering
content) rather than an integration of ‘what-when-where’ information. Moreover, the
speciﬁc reference to an autobiographical episodic memory (e.g. [91, 260]) refers to a
memory with personal relevance. Such an approach again requires that the memory
preserves ‘speciﬁc spatio-temporal context’, though it may be from the recent or
distant past, and acknowledges that event-speciﬁc recall will draw upon and be
inﬂuenced by more general knowledge about the world and general knowledge about
the self (history, different life periods, name, etc.) from autobiographical semantic
memory.
A distinction between autobiographical episodic memory and autobiographical
semantic memory was demonstrated in [393]. They described an individual, who,
following brain damage showed a severe impairment of memory for autobiographi-
cal events while having intact semantic memory about family and personal history,
occupation, residence and car owned. More recently, Renault et al. [337] argued
for a further distinction between personal semantics (consisting of knowledge about
oneself, self-identity and one’s experiences) and general semantic knowledge (facts
about the world).
Conway and Pleydell-Pearce [91] proposed a ‘Self-Memory System’ as a concep-
tual model of autobiographical memory. This relates development over the lifetime
of autobiographical memory knowledge to event-speciﬁc knowledge (after [415]),
through reciprocal connections with a working self. The working self refers to cur-
rent short-term goals that are embedded within a hierarchy of longer term (including
lifelong) goals, which are in turn framed and constrained by self-image and autobi-
ographical knowledge. In [392], the author argued that information represented in
episodic memory is the product of sensory information passing along a serial process-
ing chain through perceptual representation and semantic representation systems,
which may or may not result in an episodic representation. In contrast, Williams,
Conway and Baddeley maintain that episodic representations ‘originate in work-
ing memory, where they derive from mental models of online experience’ [417,
p. 40]. Such episodic representations are given conceptual context by stored autobi-
ographical knowledge, through automatic access to stored knowledge and through
operations of the working self, such as inhibiting information that is not relevant to

2
Preserving and Forgetting in the Human Brain
25
the current goal, that competes with the current goal or that is unrelated to one’s own
representation of self.
In [89], the authors have argued further that autobiographical knowledge can also
be represented as an hierarchical, nested structure comprising a semantic memory
of the self in which episodic memories can become integrated with representations
of ‘general life events’ (repeated events with common structure, property or theme),
which are themselves further represented within ‘lifetime periods’ (such as ‘when
I worked at a speciﬁc institution’, ‘when I was mother to a young child’) which
are further represented as life stories, contributing to self-images and ultimately a
‘conceptual self’. Short-term goals that an individual sets can become part of an
autobiographical record of speciﬁc memories integrated with an immediate context,
or with general life events or a lifetime history. Conway’s concept of a Self-Memory
System [85, 86, 91] allows for creation of a ‘personal history’ and ability to access
memories in varied ways, for different reasons, throughout changing personal con-
texts and personal circumstances, while still maintaining an overarching sense of self
throughout the lifetime.
Episodic memories for events that are experienced but that have limited per-
sonal relevance may or may not become integrated with autobiographical memories.
Whether they are integrated partly depends upon the richness and detail of conceptual
context available at encoding, and upon subsequent retrieval (among other inﬂuences,
see [89]). As Conway emphasises, ‘the function of episodic memories is to keep a
record of progress with short-term goals and access to most episodic memories is
lost soon after their formation’ [86, p. 2305], consistent with the broader charac-
teristics of memory identiﬁed by Ebbinghaus in 1885. Additionally, Conway [86]
has noted that within episodic memory, not all components of a memory are equally
accessible and has suggested that goal structure at the time of the experience deter-
mines the activation, and subsequent accessibility, of features within a memory (for
retrieval within that memory context). If episodic memories are to be voluntarily
recalled, they must be further integrated into autobiographical knowledge to enable
generative retrieval—an often strategic and iterative process of cue elaboration; if
this integration is lacking, episodic memories may be brought into awareness only
through direct cueing. This latter (involuntary) process would require an unlikely
encountered speciﬁcity of cue to directly access elements of an episodic memory
[89]). A similar concept was proposed by Tulving [391, 392], who suggested the
principle of encoding speciﬁcity, namely that details encoded and set in a particular
context at the time of an event will act as effective cues for prompting later recall
of details of the event. For example, if the reader is given the cue ‘washing clothes’,
that can act as a cue to retrieve the passage of text given in Sect.2.1.4 because that
context was used to encode the information in memory. The cue word ‘expensive’
is a much less effective cue for retrieving that same passage, because it was most
likely not used as part of the context or schema for encoding, even though the word
appeared in that text. If we consider the notion that each individual has a ‘schema
of the self’, then this schema can set the context for experiences and be used to help
retrieve whatever was preserved in memory from those experiences.

26
R. H. Logie et al.
As implied in the above model of Conway and colleagues [85, 86, 89, 91]), recall-
ing an autobiographical memory requires an interplay and interaction of episodic
information and semantic elements of an individual’s personal history. As noted in
Sect.2.1.5, recall of a memory is always a reconstruction from those details of an
event that have been encoded within the context of a schema that is used to make
assumptions about details that have been forgotten. It is not a veridical record of the
event. Cabeza and St Jacques [58] list the most inﬂuential factors in determining the
episodic versus semantic composition of an event during the reconstruction process
at recall: age of memories, event frequency, rehearsal and age of participants. As
may be expected, in the real-world context of a person’s life history, these factors do
not operate in isolation, rather they interact.
2.3.2
Methodologies for the Study of Autobiographical
Memory
Thesystematicstudyof autobiographical memoryis complicatedbythelargenumber
of factors that have to be considered, and the lack of experimental control over
spontaneous or even planned events that occur in everyday life. Individuals also
differ in the extent to which they retain details, even of the same event, and which
details are retained. Finally, rarely is there a full objective record of an event that can
be used to assess the accuracy of the individual’s memory for that event. This last
feature is particularly problematic given the reconstructive nature of memory.
An early approach to the study of autobiographical memory involved participants
keeping a diary or log of events. After delays of days, weeks, months or years, one
detail from a recorded event can be used as a cue for participants to recall other details
of those events. The original record can be used to check for recall accuracy (e.g.
[242]). A related approach has been to prompt participants at random intervals (e.g.
via a pager [53]) to record details of the most recent events in their daily life, and again
use these records as sources to cue speciﬁc events for later recall and for checking
accuracy. However, these methods suffer from two major limitations. First, the fact
that participants choose the events and the details of those events, and take some
time to record them, acts as a form of mental rehearsal of the events that would make
it more likely that those events will be recalled when tested at a later date. Second,
given the difﬁculties of running studies over more than a few weeks or months, the
method does not allow testing of memory for events from the more distant past. The
study in [242] took place over 6 years, but that involved a single participant (Linton
herself) and few research projects have the resources for this length of study, or for
the numbers of participants that would be required for a comprehensive study.
When there has been no speciﬁc documented record of an event, other sources
such as family and friends can be used to check recall accuracy, but they too will be
subject to memory recall errors. If it was a public event then there may be publicly
available documentation, video or audio records to verify memory accuracy. This

2
Preserving and Forgetting in the Human Brain
27
method has been used very successfully to demonstrate errors in recall of major
public events, such as errors in the recall by President George W Bush of the tragic
events of 11 September, 2001 [155]. Therefore, many researchers have focused their
investigations on memory for, or surrounding public events as described above. The
studies of memory for public events indicated that the importance of an event for
the individual, or their emotional response to an event is a major factor in the level
of detail that can be recalled. Kopple et al. [207] found that after delays of around
4 months following a major public event, the ratings of ‘emotional intensity’ and
‘personal importance’ regarding the events were the best predictors of the level of
consistency in their recall of the event. However, the fact that an event resulted in an
emotional response or was of personal importance would also lead to the individual
repeatedly thinking about or talking about the event, thereby having repeated recalls
and rehearsal of the event before the formal test of their memory within the study.
The extent of media coverage of the event may also be important in driving this form
of repeated recall.
A further approach has been to simulate natural autobiographical events by staging
them within a controlled experimental setting. This has the advantage that the same
or very similar events can be experienced by a large number of participants, thereby
removing the confounding factor that some events are more memorable than others
or include different amounts of detail (e.g. [343]). Such experiences and associated
memory will not be ‘pure’ or free from the inﬂuence of a participant’s personal,
autobiographical knowledge. For example, certain elements of a constructed expe-
rience may resonate strongly with some participants more than others because of
their individual past experiences. However, if a sufﬁciently large number of partici-
pants are included then averaging the results across participants can help mitigate the
inﬂuence of these individual inﬂuences on recall. Moreover, constructed situations
allow researchers to manipulate components of experiences, enabling generalisable
insight into the features of interest in any given experiment such as delay until recall,
nature of the event, number of times recall is repeated, effects of different memory
cues and so on.
Difﬁculties inherent in trying to characterise, measure, quantify and understand
the information that remains available and accessible from an experienced event
still persist even when many of the external characteristics of the experience are
under the control of an experimenter. Williams, Conway and Baddeley [417] made
an attempt to address some of these factors by tackling the question of event bound-
aries in memory: when does an event begin and end, and does this change over
time? The approach builds on the work of Zacks and colleagues (e.g. [213, 430,
431]), regarding boundaries of events when looking at and perceiving the world,
and conceptual features (for example, hierarchically nested goals, which can deter-
mine ﬁne or coarser grained events). In the context of Conway’s general framework
(e.g. [85, 86]), episodic memory is considered a record, or the product of, carrying
out short-term goals in the service of increasingly greater overarching goals; while
moment-to-moment goals are necessarily dynamic and short-lived, goals further up
the hierarchy are more stable and enduring.

28
R. H. Logie et al.
Williams, Conway and Baddeley [417] asked participants to freely recall their
morning commute on the day of testing, to identify how they would segment this
into ‘discrete memories’ and then to recall the same commute one week later. After
analysing the memory elements that began and ended a segment, the authors deter-
mined that actions were more likely (but not exclusively) to characterise a begin-
ning, with a fact (more likely but not exclusively) ending the segment. Other features
changed with time of recall. This so-called ‘action-fact’ structure to the memories
was replicated for three memories from a more remote time of a recent holiday,
and was taken as support of memories deﬁned around goal structure, with actions
indicating an initiation of the current short-term goal. Notably, the authors observed
that despite the general pattern of facts ending a segment, the properties with which
a segment could end were more variable than those with which they would start, and
this indicated that goals may be terminated in various ways.
Hohman, Peynircio˘glu and Beason-Held [169] tested event memory for events
from different time points in the lives of individual participants. They found that
college age and middle age adults were more likely to be ﬂexible in their attribution
of event boundaries with increasing time—that is, once a memory was recalled and
probedforfurtherinformation,themoretimethathadpassedsincetheevent,themore
likely the adults were to accept information outside of the original boundaries for the
event as belonging to the original event (therefore moving event boundaries). Overall,
older adults were more likely than college age adults to move event boundaries,
indicating an age-related component to event structure. Such results are important
in demonstrating that not only does the nature of creation of event boundaries at
encoding determine how we should approach our memories for events, but so too
does the nature of event boundaries at retrieval/reconstruction.
Using a method originally devised in [144], in which participants are asked to
provide memories in response to cue words (for example, think of a memory for an
event related to the word ‘river’ or the word ‘holiday’), and in [100], the authors
demonstrated that fewer memories are produced from early life years. Participants
are less likely to recall memories from the ﬁrst 5 years of their childhood and more
likely to recall detailed memories from the more recent past. Moreover, older auto-
biographical memories contain less detail and are more abstract than those from the
more recent past [58, 328].
An exception to the pattern of memory recall frequency is a reminiscence bump
found in participants over 40years of age who produce many memories from their
early adulthood (age 18–30years old; [140]). It has been suggested that such a period
may contain life ‘ﬁrsts’ and meaningful, life-deﬁning events (such as jobs, children
and marriage; [41]), which serve as anchors in our memory and a basis upon which
our life story (or narrative) is built. Glück and Bluck [149] observed a reminiscence
bump only for positive (not negative or neutral) memories, in which participants
indicated they had perceived themselves as having had control of their lives. This is
consistent with the idea that individuals create a positive and deﬁning life story for
themselves (see also [86]). A bias towards recall of positive life events is especially
present in older adults [272]).

2
Preserving and Forgetting in the Human Brain
29
Detailed memory for events throughout the lifespan requires a complex interplay
of a number of aspects of the memory system (see [86]) in addition to decline in
the integrity of the brain with age. Levine and colleagues [227] reviewed the history
of research showing negative effects of age on recall of episodic memories. They
demonstrated that older adults provide answers with fewer episodic details and more
semantic content than do younger adults, even when probes for episodic detail are
provided. St Jacques and Levine [373] demonstrated further that this age-related
pattern of fewer episodic details provided by older than younger adults holds true for
emotional memories, even when positive emotional memories elicit more episodic
details than neutral memories.
As noted in [343], memory for personal events is subject to the same general prin-
ciples as other types of memory—such as proactive and retroactive interference men-
tioned earlier. As people age, so the number of similar experiences increases, there-
fore the effects of interference among memories of those experiences also increase.
Meeting thousands of people over one’s lifetime, many of whom have similar ﬁrst
names, builds up a lifetime of proactive interference. This makes it increasingly dif-
ﬁcult with age to remember the name of someone just met for the ﬁrst time. It also
builds up retroactive interference, making it increasingly difﬁcult to remember the
name of someone met a few years ago because of experiencing the names of all of the
people met subsequently. This problem with names applies to memory for details of
many similar events that have been experienced over a lifetime, making it more likely
that there will be increasing reliance on reconstruction from more generic and less
speciﬁc semantic knowledge, with schema supporting recall [58]. Reconstruction
takes place even for events which appear to be unique in many aspects and which
the individual believes that they retain in a detailed, episodic format.
2.3.3
Photograph Use and Event Memory
The use of personal photographs offers an illustration of the relationship between the
conceptual understanding of human memory and use of digital or physical preserva-
tion of information. Their use for research has been widespread in both controlled
laboratory settings and in everyday settings. For example, in a laboratory-based
experiment, Koutstaal et al. [208] investigated the effect of viewing photographs
on people’s memory for events. Participants watched a video of action taking place
and their memory for the content of this video was tested. Subsequent to watching
the video, but prior to testing, participants were shown photographs that included
elements from parts of the video, or presented with verbal descriptions of these ele-
ments. Koutsaal et al. found older and younger adults remembered more from the
parts of the video that had been viewed in the photographs or verbally reviewed than
they did sections that had not been reviewed. Such an increase in memory perfor-
mance following photograph viewing after witnessing events was also replicated by
Schacter and colleagues [354].

30
R. H. Logie et al.
Despite evidence that reviewing photographs is supportive and beneﬁcial to mem-
ory [208, 354, 374], further evidence suggests that photo use may not be without
negative consequences. For example, following presentation of photographs that con-
tained images indicating items which were either present or absent in a previously
viewed video, older adult participants cued on these latter items were also more
likely to attribute them to having been present in the video [354]. More recently,
St Jacques and Schacter [374] asked participants to complete a tour of a museum,
wearing a camera which automatically took pictures during the tour. They were then
presented with photographs which were either strong cues (photographs from a natu-
ral perspective) or weak cues (photographs from an unusual perspective) to memory
for aspects of the tour. The authors then presented photos of entirely novel items,
or other items from the tour, and asked participants whether the items had been a
part of their visit. Strong cues produced a higher level of accuracy than weak cues
in detecting photographs that depicted items from the tour, but also produced an
increase in erroneous selection of photographs of objects that had not been seen on
the tour. This reinforces the general conclusion discussed earlier that memories are
reconstructed and subject to change. The study also highlights how photographs can
inﬂuence the reconstruction process.
Koutsaal and colleagues (e.g. [209]) cautioned against possible effects of selective
rehearsal when seemingly viewing photographs to help memory. Such effects were
demonstrated when participants were asked to view photographs that pertained to
elements of a series of activities in which they had been involved in the laboratory;
tasks that were carried out but were not then reviewed were more poorly recalled
than both reviewed activities and performance of a control group who carried out
the same tasks with no subsequent review (a baseline condition). That is, selectively
reviewing certain elements of associated activities (or sub-events) led to a reduction
in memory recall for activities not studied.
Results reported in [163] have also been used to advocate a need for further under-
standing of how the act of taking photographs interacts with our natural memory
abilities. Participants in this experiment were instructed, throughout an art museum
tour, to visit a list of objects and to either only look at them or to look at them and pho-
tograph them, with the relevant action for each object dictated by the experimenter.
Subsequently, the number of objects, and amount of detail, remembered between
these two conditions was compared. In a second experiment, an additional condition
was introduced where participants were instructed to zoom in to speciﬁc sections
of objects to be photographed. When required to perform the zoomed-in photogra-
phy task, participants’ memory (recognition of objects or for detail) was comparable
to the observation only (no photography) condition. This was true for the original
speciﬁc part of the object that was photographed as well as for other parts of items
that were not included in the photograph. In contrast, in both experiments, memory
was poorer for objects that participants had been instructed to photograph (with-
out zooming). While participants were told in initial instructions that they would
be asked about the appearance of the objects of art, Henkel suggested that results
indicated a reliance on outsourced memory storage that has negative effects for our
own memory. Speciﬁcally, that taking a photo could serve as a cue to ‘dismiss and

2
Preserving and Forgetting in the Human Brain
31
forget’. The cognitive effort and attention expended on a zooming task appears to
counteract this effect. However, as acknowledged by Henkel, this apparent detriment
to memory may not extend to situations in which people exert a choice over taking a
photograph, or where the photograph is driven by interest. She suggested that further
work is needed to disentangle the factors that may contribute to memory in a real-life
situation.
2.4
Remembering and Forgetting in IT Systems
This section discusses methods and technologies in IT which are related to human
remembering and forgetting. Our approach is to consider IT systems that are designed
to complement (not to replace) the processes of human remembering and forgetting.
The advent of Big Data and the development of ever more effective storage tech-
nologies may suggest that forgetting need not occur in digital systems, because there
is sufﬁcient storage capacity to keep everything. However, forgetting mechanisms
are still necessary, be it to ensure privacy [48, 187, 402], to remove or correct errors
(e.g. [279]), or to ensure that the information which was selected for preservation can
be retrieved and processed effectively without being swamped by stored irrelevant
detail that need not have been preserved. These forms of ‘managed digital forgetting’
contrast sharply with unintentional digital forgetting that occurs when storage media
and ﬁle formats become outdated and inaccessible, or when storage media are lost
or damaged.
2.4.1
Inferences from Data
When making new inferences from existing data, we always face the problem of
noise, i.e. data that look like they might be useful or applicable to the problem, but
are not. Some of this noise could arise from information that is available but is trivial
or irrelevant, and failure to exclude such information could result in misleading or
completely incorrect inferences. This is the kind of information that human memory
does not ever encode, because, as discussed in Sect.2.1.4, initial selection of infor-
mation to preserve, even on a temporary basis, is driven by context. Use of context
derived from prior inferences about the kind of information being stored can help
decisions about what data can be excluded from processing or preserving.
A second source of noise is the recording of redundant or repeated information as
separate sets of data records. Again, from Sect.2.1.4, in the case of human memory,
eachrepetitionisnotrecordedseparately,butrepetitionsaccumulatebystrengthening
the representations in memory for features that overlap between repetitions, and using
this accumulation to set a context when the same, or similar information is repeated
in the future.

32
R. H. Logie et al.
A third source of noise can arise from inconsistencies. These could perhaps be
seen as an irritation, but may be highly informative. Take for example the task of
deriving common features of the class of birds. Most birds can ﬂy, but ostriches and
penguins cannot. If we assume that all birds can ﬂy, we have a contradiction. One
approach would be to ignore the inconsistency by forgetting the information that
causes it, so that the criteria for being a bird no longer includes the ability to ﬂy.
Numerous techniques have been proposed for identifying and eliminating variables
that lead to such inconsistencies, and for reasoning with inconsistent data [126, 219].
In a system with managed digital forgetting, we can say that internally consistent
information should be given a high priority for long-term preservation, or has a
high preservation value. However, if we consider that the functioning of human
memory categorisation can be based on a match with the majority of criteria rather
than an exact match with all criteria, this allows for exceptions. A human memory
schema for a bird would contain the key criteria that are true of all birds (e.g. a warm
blooded animal that lays eggs, has feathers, and modiﬁcation of the forelimbs to
form wings) plus the extremely useful information that most, but not all members of
this category can ﬂy. A similar problem emerges when we consider instance-based
learning algorithms, where new examples are classiﬁed by retrieving similar items
from a database and inferring the class of the new item from the class of these similar
items. One approach for items that are irregular and that lead to misclassiﬁcation is
to eliminate them from the database, that is to forget [54].
However, using irregular items to reﬁne the criteria in the database could lead
to much greater accuracy in categorisation, so penguins and ostriches would still be
categorisedcorrectly,andtheinformationaboutthemajorityofthecategorymembers
is not lost. So too in natural language processing, where irregular spellings and word
forms abound, particularly in English, it is important to remember these irregular
forms [107]. Having a computational taxonomy learn from instances of exceptions,
through a process of taxonomic revision can further enhance effectiveness (e.g. [7,
8, 216, 370]). Therefore, inconsistencies should reﬁne the context rather than be
forgotten.
2.4.2
User Models and Companions
IT systems that are personalised to particular users often rely on detailed user models,
whichconsistofgeneralinformationabouttheuser,suchasgenderandage,butwhich
can also cover the history of interactions between system and user. Other information
about the user such as the languages they speak and at what level of proﬁciency, or
other particular skills and experience that they have might also be useful. Displaying
a human-like memory for information about the user is key to appearing intelligent
and responsive [236, 338], and so may maximise utility. This complements the notion
of a schema for the self, discussed in Sect.2.3. User models are often fully or partially
inferred from data about the user’s behaviour, and some longitudinal models can take
account of changes in user preferences over time. These data can come from sources

2
Preserving and Forgetting in the Human Brain
33
of information outside the actual application, such as linked social media accounts,
or web browsing history, as documented by cookies.
When it comes to maintaining a human–IT interaction history, not everything
should be stored. For example, Vargas et al. [402] make a case for ensuring that
privacy and conﬁdentiality are preserved. Barua et al. [35] propose to give users a
degree of control over the information that is preserved and the information that is
forgotten, which is key to addressing privacy issues. For example, the system might
store information about ‘work-related self’ such as periods when the individual had
particular sets of responsibilities or was at a particular stage in career. There could
also be storage of work-related relationships with colleagues to support communi-
cation and networking. The representations of these relationships could be dynamic,
with their prominence or Memory Buoyancy driven by the frequency and recency
of interactions. The user would make a personal choice about whether equivalent
information about personal relationships and life periods would be included.
Agents that can simulate an autobiographical memory are often designed to recall
signiﬁcant experiences in great detail, equivalent to episodic speciﬁc knowledge,
and compress everyday routines into scripts that simulate schema within semantic
memory [166]. Robot companions should be able to remember both their own expe-
riences and those of the people over whom they watch. These experiences should be
easy to retrieve [238], with constraints on the level of detail set by the context at the
time of retrieval.
The implementation of such digital memories is based on computational models
of the neurobiology of memory, such as Adaptive Resonance Theory [64]. Memo-
ries are stored in a network. Accessing a particular item of knowledge or experience
activates related memories through a spreading activation framework that simulates
relevant interactions between groups of neurons in the brain. In actual robot compan-
ion systems, this autobiographical component is integrated into a memory model that
covers a store for incoming perceptual information through sensors, working memory
and long-term memory [165]. Throughout these systems, in particular when dealing
with the details of sensory input, forgetting is key to keeping memory requirements in
check [375]. Forgetting can be implemented as removal of transient traces of incom-
ing information, through decay with the passage of time [237], and through gener-
alisation [403]. Forgetting could also be implemented as developing a schema for
common features of repeated events, removing the requirement to store the repeated
details of those events.
2.4.3
Reminder Systems and Decision Support
The classic example for technology that supports human memory is reminder sys-
tems, designed to support prospective memory by giving people cues that help them
remember to do things (c.f. Sect.2.1.6). They can also cover warnings or alerts, and
can support complex decision-making. For example, computerised prescription order

34
R. H. Logie et al.
entry systems routinely warn prescribers about potentially problematic drug/drug
interactions (e.g. [344]).
Most of the extensive work on reminder and decision support systems focuses on
safetycritical workplaces, suchas aviationor medicine(e.g. [125, 332]), whileothers
have addressed solutions for the home (e.g. [286]). Both reminders and key infor-
mation to support decisions need to be intelligible and acceptable. Being intelligible
means that people can perceive the reminder or other information and understand the
message. Acceptability means that people are willing to attend to the reminder or to
use the information when they make decisions, and that the information is relevant
for the user. People often dismiss or disregard reminders because they have received
too many irrelevant reminders in the past or because the reminder irritates them. This
is ‘alert fatigue’, and it is one of the main reasons why reminder or alert systems
fail, as Thimbleby [381] has discussed. Similar problems arise when professional
healthcare staff are provided with information about patients in their care that is
in too complex a format or is simply not relevant to their speciﬁc healthcare role
(e.g. [249, 400]).
2.4.4
Augmented Memory for Personal Information
Another way in which technology can support human memory is by storing personal
information so it can be recalled later. Here, we brieﬂy review the literature on
a particular type of personal data that we discussed in Sect.2.3.2, namely, digital
photos.
In [206], the authors studied what people do with their photos after these have
been taken. They term these activities ‘photo work’ and they encompass reviewing,
downloading, organising, ﬁling and editing. In [10], the authors investigated photo
work on camera phones and found a similar rich set of activities related to managing
and manipulating photos. Kirk, Sellen, Rother and Wood [206] suggested that as
part of photo work, people tend to browse rather than search photos. Browsing is a
less goal-directed activity with room for serendipity and following new associations.
Interfaces for browsing use a range of different design metaphors that are inspired by
ways in which users arrange and search through information, such as a timeline [159],
locations, [257] or a choice of contexts including data from friends [399]. At the time
of writing, in 2017, Google and Apple organise their users’ photos by location and
time. Google automatically detects trips and invites users to revisit highlights of a
day, while Apple highlights common locations in the photo timeline.
When it comes to preserving photos for later browsing, a key problem is preserv-
ing not just the photo itself, but also its meaning. People often forget details such as
where or why a photo was taken or the names of people depicted. Such information
is particularly difﬁcult to recover if the person who decided to preserve the photo can
no longer remember the event or has died. Many automatic analysis techniques have
been developed for extracting content information from photos, such as automatic
detection of people and faces, or automatic grouping of photos into events [94].

2
Preserving and Forgetting in the Human Brain
35
The techniques developed within the ForgetIT project are summarised in Chap.3.
Here, we focus on solutions that are inspired by the way people interact with photos.
For example, Frohlich and Fennell [142] suggest several ways of embedding digital
photos in a rich context that will preserve information about their meaning, such as
linking them to other physical memorabilia or augmenting them with audio narra-
tions. For more information about how this issue was approached within ForgetIT,
see Chap.7.
An important aspect of photo context is the reason why a photo was taken. In a
2004 study of early camera phones [205], Kindberg and colleagues found that peo-
ple took photos for both affective and functional reasons, and that photos could be
either for oneself or for sharing with others. Affective reasons included documenting
a shared experience, communicating with absent friends or family, and individual
reﬂection and reminiscence. Photos taken for functional reasons contained informa-
tion to support a task that was either personal or was shared with others, who might
or might not be at the same location. Annotating photos with detailed tags that can
then be stored as metadata would be ideal for retrieval and contextualisation, but it is
a very burdensome activity that requires well-designed user interfaces to keep users
engaged and motivated [194]).
Another form of annotating photos are interactions, where people share reac-
tions, comments, and information. Platforms such as Flickr or Facebook offer this
facility routinely, but largely for typed interactions. For example, Vennelakanti et al.
[404] proposed a system, Pixene, that can capture conversations about photos and
record when each photo had been mentioned during a conversation. Cosley et al. [96]
describe a system called Pensieve, which uses social media content as memory cues.
The Pensieve sends users random snippets of the social media content that they have
generated over the years, with the aim to encourage users to reminisce and reﬂect on
past experiences. Facebook now has similar functionality, ‘On This Day’ memories,
that retrieve a post from past years and display it again in the user’s timeline, with
the option of sharing it again.
2.4.5
Digital Heirlooms
Researchers in the ﬁeld of digital heritage and personal digital archives have investi-
gated how people use both physical and digital objects as mementos of the past. Van
den Hoven and Eggen [172] set out design principles for augmented memory systems
inspired by autobiographical memory research. They recommended that augmented
digital memories should rely heavily on memory cues that allow people to recon-
struct memories. The memories held may not need to be related to speciﬁc episodes,
they could also refer to lifetime periods or general life events as outlined in [89],
where speciﬁc information has been forgotten. Digital memory systems should also
consider different reasons for retrieving autobiographical memories, be it to support
one’s identity, to inform decision-making or to share experiences with others.

36
R. H. Logie et al.
From the user interface design point of view, augmented digital memory sys-
tems should be capable of integrating ‘souvenirs’, that is, auditory, visual or tactile
materials that are particularly good at evoking memories in users. As Kaye et al.
[199] have shown, when it comes to personal archives, the physical and the digital
can be impossible to separate. Indeed, as Petrelli, van den Hoven and Whittaker
[324] found, when asked to construct a time capsule of their life, people preferred
a few well chosen, but barely annotated physical objects to a richly annotated large
set of digital recordings. Barthel and colleagues [33] report a solution for integrating
physical objects into digital networks that relies on a tagging system. Objects are
labelled using two-dimensional QR codes or RFID tags that link stories to objects.
Compared to physical objects, digital mementos may be seen as less valuable, they
are accessed less frequently, because sharing them can be cumbersome, and they are
mostly limited to photos and videos [325].
Banks, Kirk and Sellen [32] reanalysed both their own data and other published
studies to determine themes around the interaction with digital mementos. Four main
themes emerged: how the mementos relate to people, how the mementos connect to
memory, qualities of the objects serving as memento and the kind of record that a
memento represents. The ‘people’ theme revolves around access. Are the mementos
for oneself, to be shared with others, or can they be made public? This is an impor-
tant distinction [240]. Were they constructed speciﬁcally to be preserved for future
generations? What are others expected to do with these mementos?
Mementos can be a record of many different things. They can be intended to
capture a place, such as the house of a deceased relative before it was sold or to
document a timeline, such as sets of school photos through the years, or a lifelog.
They can also encompass material from different modalities, such as the smell of a
favourite cologne or the sound of a voice singing. Objects that serve as mementos
may also have speciﬁc qualities. They may show signs of wear and tear, they may
have been crafted exquisitely well, or they may have been handmade with more love
than skill. The main connection to memory is through narrative. Mementos serve as
cues to speciﬁc episodic knowledge in autobiographical memory that can be retrieved
and recounted, and they are also linked to such knowledge about the people who used
to own that item. As digital means of communication become widespread, it may be
tempting to assume that there will be fewer physical mementos, and that people’s
practices of remembering will focus more on the digital. However, this needs to be
empirically tested.
2.4.6
Reminiscence Therapy
Reminiscence therapy uses artefacts related to people’s past to stimulate memories.
It is particularly popular for people with dementia, as memories that are important
for the individual and that will have been retrieved many times during their lifetime
are usually less vulnerable to the disease. This arises because each retrieval can
strengthen the memory trace, even if memory for some of the details changes with

2
Preserving and Forgetting in the Human Brain
37
each retrieval. Reminiscence therapy can take place both in person and remotely
through teleconferencing [214].
Kalnikaite and Whittaker [190] designed a reminiscence system called Memory-
Lane that is based around pictures of mementos. Users can write or record stories
about these mementos and store them in a home, save them in a photo frame (for
people), or assign them to places on a map. One of the most sophisticated reminis-
cence systems is CIRCA [153]. The CIRCA system is designed to evoke memories
in older users and to stimulate verbal or non-verbal communication about the mate-
rial that is presented. Materials include songs, video, pictures, animation, text and
Virtual Reality reproductions of environments such as a 1930s pub. They cover life
experiences and events that were common to the older generation. The interface also
uses metaphors that are familiar to older users. For example, songs can be played on
a virtual record player, or they can come from an old radio.
An alternative to systems that are built around speciﬁc mementos of the past are
systems that are built on ongoing lifelogging. The most popular example is Sense-
Cam, developed by Microsoft. SenseCam is a camera worn on a lanyard around the
neck that automatically takes photos at set intervals. SenseCam can be paused for
up to 7 min, taken off or turned towards the user, if there is a need for more privacy
[202]. A variety of published case studies, such as [167], have shown that Sense-
Cam can help people with memory impairments recall what has happened to them.
Through reviewing SenseCam images, these patients were able to recall the events
associated with those images far better and more vividly. For people without memory
impairments, SenseCam does not appear to be more efﬁcient in stimulating recall of
episodes than other techniques, such as social reminiscing [359]). However, Sellen
and colleagues [360] found that SenseCam helped healthy users activate background
knowledge about other things that happened on the same day that the SenseCam
images were taken.
2.5
A Sample Experimental Study on Personal Digital
Photograph Use
Thus far, this chapter has focused on discussing the current understanding of human
memory and forgetting, along with a description of the research methodologies that
are used and the scientiﬁc evidence that has led to our current understanding. There
are many remaining unanswered questions about the details of how human mem-
ory is organised and functions, and these questions are major drivers for a large
international scientiﬁc effort and a large volume of published research (for a review
see [27]). We have also discussed some of the research that has been carried out
on use of digital memory systems and their relationship with human memory. The
current book is focused on personal multimedia preservation, and so in the ﬁnal
main section of this chapter, we describe an experimental study that drew on the
understanding and research methodologies of human memory to investigate the

38
R. H. Logie et al.
relationship between human memory and one of the most ubiquitous forms of per-
sonal multimedia, namely digital photographs. These offer an ideal example use case
for ForgetIT in that every individual with a camera on their smart phone stores very
large numbers of photographs simply because the camera function is extremely easy
to use, the smart phone is nearly always readily available, and for most devices there
is capacity for many hundreds if not thousands of photographs.
This clearly illustrates the major challenge of how to organise the very large
volumes of photographs that accumulate, and how to ensure that photographs that
the individual might want to preserve are stored securely long term and that they can
easily be retrieved. Some automatic sorting can occur, for example by date and time
when the photograph was taken, and for some devices, photographs can be uploaded
automatically to cloud services.
As discussed in Chap.1, the problem of increasing volumes of photographs
remains. It is very easy to look at printed photographs that were taken over a hundred
years ago and kept in a family album. However, it is very challenging to look at
photographs that might have been stored on a ﬂoppy disk 20years ago. Complete
collections of photographs could have been lost when switching to a new phone,
a new operating system, a new computer, new software that cannot read older ﬁle
formats, or if the digital device is lost or stolen. Selecting from the many thousands
of digital photographs, a subset that an individual feels that they would want to pre-
serve would allow for low-cost long-term storage solutions. Indeed the popularity of
printed ‘photobook’ services suggests that many individuals may still see printing on
paper as a reliable method for preserving a small selection of important photographs.
With the above context in mind, our study focused on asking people to take digital
photographsofbuildings,peopleandstreettheatreeventsduringamajorinternational
arts festival. After a range of delays from one hour through one week and one month
and one year, they were asked to decide which photographs they would like to keep
and which they would be happy to delete. They were also asked what they could
remember of the events that they photographed. Here, we will brieﬂy summarise
the methodological approach, the study and the results. Full reports of the study are
available in [1, 2, 418, 419].
2.5.1
An Experimental Study on Personal Digital
Photograph Use
In order to design a system that will be adopted by potential users, it is crucial to
identify how such potential users currently use digital storage for personal informa-
tion, and how that use impacts on their biological memory for the original source
information, such as an experienced event. In this section, we report on an experi-
mental study in which objective data were collected for this use case scenario. The
study took place during the annual Edinburgh Fringe Arts Festival, a large street
fair (musicians, acrobats, street theatre, stand-up comedy) that takes place in the

2
Preserving and Forgetting in the Human Brain
39
Table 2.1 Demographic details of participant groups
Group
1
2
3
4
Number of
participants
20
18
18
18
Age (years)
31.65 ± 5.52
(19–68)
38.61 ± 20.41
(19–71)
33.94 ± 17.54
(20–72)
35.17 ± 15.02
(19–59)
Males
3
8
5
10
centre of Edinburgh, UK over 4 weeks in August each year. In this study, partici-
pants spent an hour at the street fair and were tasked with taking snapshots every
3 min to document this experience. Through using the Edinburgh Festival Fringe
as a setting for studies centred around photograph use to support memory for an
event, we were able to take advantage of a great deal of activity happening in a fairly
well-deﬁned area: approximately 400m along one street. This provided a substantial
amount of potential information that participants might want to remember, and also
provided a rich opportunity and variety of contexts with which to link their pho-
tographs upon subsequent reviewing. More speciﬁcally, this enabled investigation of
a series of research questions about photograph use when experiencing and mentally
revisiting an event.
Here, we focus on how we investigated participant engagement with photographs
of the events that they experienced during the one hour visit to the street fair: which
photographs they chose to keep, and their methods for organising photographs into
meaningful groups. These elements were investigated for changes over time.
For testing, participants were split into four pseudorandomly selected groups. This
involved random allocation of people to groups, but this was constrained by dates
upon which participants were able to commit to returning for a second test session.
Table2.1 shows the demographic details for each group. Immediately after their
hour of festival experience ﬁnished (hereafter referred to as Time 1), all participants
returned to be interviewed: they orally recalled their experiences in as much detail
as possible. In order to assess changes in the preservation value of each photograph,
these participants returned to the laboratory a second time (hereafter referred to
as Time 2) after one day (Group 1), or after one week (Group 2), or after one
month (Group 3), and were again asked recall their experiences and decide which
photographstheywouldliketokeepandwhichtheywouldliketodelete.Thisallowed
us to investigate changes over time (from Time 1 to Time 2). Participants in Group
4 were not asked to review their photographs or to make any keep/delete decisions
immediately after taking the photographs, but were asked to return after one month
to do so. For illustration purposes in this chapter, we will present a summary of the
photograph keep/delete decisions over time. An analysis of the keep/delete decision
results is available in [418], while a full report of the analysis of the oral recall is
available in [1].

40
R. H. Logie et al.
Fig. 2.5 Proportion of photos that participants choose to keep at Time 1 and Time 2, broken down
by group (1-one day, 2-one week, 3-one month). Standard error bars are shown
2.5.1.1
Photograph Data
Participants took a mean of 19 photographs (range 12–20) during the one hour set
for doing so. The results for Groups 1, 2 and 3 are illustrated in Fig.2.5. At Time
1, on average, 28% of photographs were selected for deletion (median: 26%, range
0–58%, SD 15%). At Time 2 (a day, week, or month later), these participants deleted
on average 33% of their photographs (median: 30%, range: 0–70%, SD 17%). This
average difference between Time 1 and Time 2 corresponds to one photograph and
is statistically signiﬁcant (t(55) = 3.3352, p < 0.001, 95% CI for median).
When further probing the keep or delete decisions, it was observed that seven
participants (13%) decided to keep exactly the same photographs when interviewed
at both Time 1 and Time 2, while seven other participants (13%) chose to delete
four or more photographs at Time 2 that they had previously chosen to keep at Time
1. Overall, 32 participants (59%) did not choose to keep any photographs at Time
2 that they had selected to delete at Time 1, while 37% decided to keep one–three
additional photographs at Time 2 compared to Time 1, and two outliers added seven
and nine photographs at Time 2 compared to those they had opted to keep at Time 1.
Of note, the percentage of photographs chosen for deletion at Time 2 did not differ
signiﬁcantly between the different groups of participants, indicating that whether the
retest occurred a day, a week, or a month later made no difference to the keep/delete
decisions. That is, there was a similar pattern of results for all three groups.
Participants in Group 4 (that is, those who ﬁrst saw and interacted with their
photographs a month after the initial experience) deleted on average 33% of their
photographs(median:31%,range:0%-78%,SD19%)atTime2.Thisnumberiscom-
parable to that observed in Groups 1–3 at Time 2, indicating that viewing photographs

2
Preserving and Forgetting in the Human Brain
41
immediately after an experience does not produce a strong inﬂuence on deletion deci-
sions made one month later.
In order to better understand the decision process of participants, following their
keep/delete decisions participants were probed with further questions. Of those pho-
tographs that they opted to keep, participants were asked for ﬁve, randomly chosen
pictures to provide an explanation for why they wanted to keep that particular photo-
graph. Likewise, among the photographs that were chosen for deletion, each partic-
ipant was asked for the motivation behind each of their decisions. Six categories of
reasons for keeping photographs were derived from all provided explanations. Like-
wise, six reasons for deletion were derived. Results are summarised in Table2.2. This
method enabled participants to provide more than one reason (category) per expla-
nation or per photograph, for example, ‘Enjoyed their music’. Not a good photo
composition but it is a reminder of the ‘atmosphere’ (see also Chap.8).
The reasons that people provided as motivation behind keep and delete deci-
sions for their small photograph collection from one hour of experiences were in
part highly predictable, while in part also very subjective. Therefore, it appears
that criteria that mimic or simulate these judgments could potentially be computed
(semi-)automatically for some photographs. However, for photographs associated
with highly subjective value judgments, any automated digital system would be
required to include a learning algorithm for the preferences of a given user. This
would most likely require substantial user input, in order to either support or simu-
late user decision-making.
Figure2.6 shows the percentage of deletion explanations containing a justiﬁca-
tion/reason per category, collapsed across participants in Groups 1–3. For example,
from all of the Time 1 deletion decisions for these groups, 39.5% referenced poor
photo quality/aesthetics. Additionally, 42% mentioned that they evoked a negative
reaction. Across Time 1 and Time 2 (collapsed across groups), it is evident that there
are slight but not drastic changes. The largest differences emerged in ‘aesthetic’ and
‘random’ deletion reasons; speciﬁcally, participants’ aesthetic judgments appeared
to feature more in decision-making at Time 2 than they did at Time 1, whereas
they were less likely to suggest ‘random’ for deletion at Time 2, than they were at
Time 1.
It is important to note that our participants’ decisions to keep or delete photographs
were often explicitly made in relation to other photos, for example, ‘(I have) Another
one that’s better’ or ‘Was walking out of frame; have other better shots of her’.
Moreover, this factor is also evident when looking more closely at descriptions that
did not explicitly state a reason for deletion that could be identiﬁed as surplus, for
example, the description ‘Not a good angle, things included are okay but e.g., too
much of chairs would rather reshoot’ does not preclude the possibility that this non-
optimal photograph would be kept if other photographs of the event did not exist.
That is, sometimes a non-optimal photograph is better than no photograph. Therefore,
these data suggest that to best support keep/delete decisions, interfaces for digital
storage should make it easy for users to browse sets of related pictures. Related
pictures may refer to, among other properties: pictures taken at a certain time, of
a certain location, of particular buildings or event pictures that represent visually

42
R. H. Logie et al.
Table 2.2 Deﬁnitions and relevant examples for each category of reason produced by thematic
analysis of participants keep and delete decision explanations
Decision
Category of reason
Deﬁnition of category
Example quotea
Keep
Typical
Photo as something typical
of Festi-
val/Edinburgh/Scotland
etc.
‘Shows Scotland’
Aesthetics
The composition, quality
or staging (or aesthetics
otherwise) of the
photograph.
‘Came out well’
Reaction
Something the person
found funny/
interesting/scary, etc.
‘It shocked me the way his
eyes are painted different
and unusual’
Sharing
Would be used to tell
stories about the person’s
experience of street fair.
‘The man whose mask kept
falling off! ...would tell
people the story when
showing them the photo’
Subject matter
Speciﬁc act, item or view
etc.
‘It’s a Storm Trooper!’
Personal
Personal relevance to
person, their life, or time at
street fair.
‘Start of Journey. Not
successful photo, but start
of journey’
Delete
Aesthetics
Bad quality (such as,
blurry, ﬁngers blocking
view)
‘It’s fuzzy’
Surplus
Content was already
covered (better) in another
picture
‘I’ve already got a picture
of them’
Random
Pictures were just taken
because the task alarm
went off, indicating a
picture should be taken
‘buzzer shot’
No shows
Photograph did not capture
content participant had
intended to photograph
‘Wanted a picture of the
street but here (there is a)
bus and no view’
Untypical
Photograph included
intended subject matter,
but on viewing did not
represent the
city/country/festival
‘The photo could also be
Christmas, not necessarily
Fringe’
Reaction
Photograph evoked
negative feelings or
connections, or were
uninteresting
‘Didn’t like act’
aNotethatexamplequotesmaydemonstratemorethantheexamplecategorytheyarelistedalongside

2
Preserving and Forgetting in the Human Brain
43
Fig. 2.6 Percentage of different explanations for delete decisions, observed at Time 1 (day of
testing) and Time 2 (day, week, month later), collapsed across Groups 1-3
similar views, despite originating in different places. It is important to offer users
ﬂexibility in the views they can deﬁne.
Finally, a sample of the photographs from the study was processed by an auto-
mated system (see Chap.3) that incorporates feature detection and automated sug-
gestions for deletion of, for example, duplicates, or poor quality photographs. We
then compared the results from the automated system with the human decisions for
these same photographs. There was a reasonable match between categorisation by
humans who took the photographs, and the automated categorisation, although there
were some errors (e.g. detecting trees when no trees were depicted). The automated
system generated a selection of photographs for preservation and for deletion that
were similar to the human selection, but selected some photographs for deletion
that the human participant had chosen to keep because, for example, it was the only
photograph of an event that they had enjoyed, or it reminded them of a previous
experience. These discrepancies between the automated and human categorisations
and selection emphasise the need for semi-automated systems, with the digital tech-
nologies generating suggestions but with the humans having the ﬁnal decision. While
automatic image annotation and managed preservation and forgetting can support
the process, ﬂexible tagging facilities are also required.
These results also indicate that strict binary keep/delete judgments might not be
realistic for managed preservation and forgetting with digital storage of photographs.
Instead, participants appeared to use three categories, ‘deﬁnitely keep’, ‘potentially
keep’ and ‘deﬁnitely delete’. Photos that should deﬁnitely be kept are also far more

44
R. H. Logie et al.
likely to be chosen as landmarks. This is reminiscent of the way in which Google
creates stories from sets of photos, where highlights of a photo collection associated
with a certain time and location are assembled into a slide show. If this ﬁnding
is conﬁrmed in more formal studies, it suggests innovative ways for eliciting data
on personal preservation preferences, such as editing automatically assembled slide
shows, or creating scrap books that combine photos with videos, comments and other
documents.
2.6
Key Features of Human Memory for Designing Digital
Preservation
Human memory is a complex, multifaceted collection of interacting and overlapping
systems that continues to be the subject of an abundance of psychological studies.
Discoveries are made daily, and therefore our understanding of memory is ﬂexible,
and will change as new evidence accumulates. Nevertheless, there are central and
discernible features of memory upon which many researchers agree. On the basis
of the literature reviewed above, and the example small study of engagement with
photographs as reminders of an event, it is recommended that the following key
features of human memory are considered when designing digital preservation tools:
1. Human memory is often considered to be ‘episodic’, referring to details about spe-
ciﬁc events, or ‘semantic’, referring to knowledge accumulated over the lifetime.
Within semantic memory are domains of knowledge or schema that group key
features that are repeatedly experienced in speciﬁc scenarios. There are schema
that are similar across people, such as for a restaurant, the ofﬁce, the home, the
swimming pool, a holiday and so on. Each individual also has a schema of the
‘self’, including knowledge of life periods, name, date of birth, relationships, etc.
2. Preservation of information about speciﬁc events in human memory is heavily
inﬂuenced by contextualising the information about those events within schema.
Recall from memory involves reconstruction of details of events based on schema,
rather than retrieval of a veridical record.
3. The level of detail for preservation in human memory is determined by what
is required to set the context and to preserve the key features of an event. This
means that many details of an event will not be stored. For example, the fact that
a particular restaurant was visited on holiday and who else was at the meal may
be preserved in memory, but the precise colours and design of the walls and ﬂoor
coverings, the food eaten and layout of the food on the plate will not be stored
in memory or will be forgotten rapidly, unless these were key features of the
experience.
4. Forgetting of details of events is extremely rapid and substantial, with most details
ofanygiveneventforgottenwithinminutesorhours.Theamountforgottenandthe
rate of forgetting is largely determined by the length of time spent on experiencing
the event or actively attempting to preserve in memory details of that event, and

2
Preserving and Forgetting in the Human Brain
45
the links with a context or existing schema. Most of the information that remains
after the initial period of forgetting is preserved in memory long term.
5. Some details of an event are required only on a temporary basis in order to
complete a particular task, and are unlikely ever to be required again in the future.
These kinds of details may therefore be retained for only a few seconds, or in
some cases for less than a second, and not preserved. Examples could be the
positions, colours and models of cars seen brieﬂy while travelling on a motorway,
or retaining a telephone number only long enough to successfully press the keys
in the correct order on the telephone.
6. Some details are required for periods of hours or days but are forgotten completely
after they serve no purpose and are therefore not repeatedly recalled or supported
by a different context. Examples here might be a ﬂight number after a particular
journey is complete, or the number of the room occupied in a hotel after returning
home or moving to a different hotel.
7. Memories for events can be changed as a result of interference from other experi-
ences or information, such as viewing photographs subsequent to the event, or as
a result of the process of reconstructing memories from schema or context. This
can result in ‘false memories’ or ‘memory illusions’ with the individual being
unaware of the error and convinced of the accuracy of their retrieval.
8. Memories for major public or personal events that are experienced as being vivid
and highly detailed are preserved in memory largely because of rehearsal and
multiple retrievals of those details at the time of the event and subsequently. Many
of the details that are personally experienced as vivid may be false recollections
as a result of many memories being reconstructions and rationalisations based on
schema and context.
9. Design of digital storage systems for personal use should:
• incorporate, but not necessarily duplicate, elements of categorisation, contex-
tualisation, and summarisation analogous to those found in human memory.
• incorporatealgorithmstoestimatepreservationvalueofinformationasadriver
for (a) storing all details, (b) storing summaries with options for reconstruction
of details (c) selecting to delete.
• incorporate algorithms to assess what information may be needed for a current
task, but can be summarised or deleted after the task is complete or after some
speciﬁed period.
• incorporate succession protocols to ensure that information with high, long-
term preservation value continues to be available indeﬁnitely with changing
software and operating systems, digital hardware and changing ﬁle formats
and storage media.
• incorporate sufﬁcient redundancy to ensure preservation following failure of
hardware, storage media, software or corporate and organisational failure of
storage services.
• incorporate the fundamental principle that digital storage should complement
and support human memory, not attempt to replace it.

Chapter 3
Multimedia Processing Essentials
Konstantinos Apostolidis, Foteini Markatopoulou, Christos Tzelepis,
Vasileios Mezaris and Ioannis Patras
Abstract As multimedia applications have become part of our life, preservation
and long-term access to the multimedia elements that are continuously produced is
amajorconsideration,bothformanyorganizationsthatgenerateorcollectandneedto
maintain digital content, and for individuals. In this chapter, we focus primarily on the
following multimedia analysis and organization techniques that provide the basis for
multimedia preservation: (a) photo/video annotation, which refers to the problem of
assigningoneormoresemanticconceptstophotosorvideofragments,(b)photo/video
qualityassessment,whichreferstotheautomaticpredictionofaphoto’sorvideo’saes-
theticvalue,(c)near-duplicatedetection,whichaimstoidentifygroupsofverysimilar
itemsinlargemediacollections,and(d)event-basedphotoclusteringandsummariza-
tion,whichconcerntheselectionofthemostcharacteristicphotosofaphotocollection
so as to create a storyline that conveys the gist of this collection.
3.1
Introduction
Either for organizational or personal use, the amount of media items produced every
day is huge. In the case of organizations, taking into account the information located
in their web sites, and in the documents and media produced and used internally, we
K. Apostolidis · F. Markatopoulou · C. Tzelepis · V. Mezaris (B)
Information Technologies Institute (ITI)/Centre for Research
and Technology Hellas (CERTH), 6th Km. Charilaou-Thermi Road,
57001 Thermi-Thessaloniki, Greece
e-mail: bmezaris@iti.gr
K. Apostolidis
e-mail: kapost@iti.gr
F. Markatopoulou
e-mail: markatopoulou@iti.gr
C. Tzelepis
e-mail: tzelepis@iti.gr
F. Markatopoulou · C. Tzelepis · I. Patras
Queen Mary University of London, Mile end Campus, London E1 4NS, UK
e-mail: i.patras@qmul.ac.uk
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_3
47

48
K. Apostolidis et al.
realize that the amount of data that is created continuously grows at an increasing
pace. Similarly, for individuals, digital cameras, mobile phone cameras, webcams,
etc. are frequently used, giving them the opportunity to capture events important for
them, like weddings, birthday parties, a trip, as well as everyday situations like a
coffee break with friends, a walk to the beach, etc. These constitute a considerable
volume of data, nevertheless, not all of them are important and should be preserved
along time. Many documents become outdated at some point, and often photos exist
that are just duplicates or of poor quality. In order to support human-memory-inspired
approaches for organizing, retrieving, processing, and ﬁnally preserving these media
items efﬁciently and effectively, multimedia content analysis and condensation meth-
ods are essential for evaluating the preservation value of each photo or video. Such
methods should be able to answer questions such as:
• What does a photo “A” show? What are the high-level concepts describing it?
Which items in a photo/video collection depict the same concepts as photo “A”?
• What is the aesthetic value of a photo/video? How can we assess this in an auto-
matic way (without the need for user interaction)? Which items in a photo/video
collection are characterized by poor aesthetic quality?
• Are there any items in a photo/video collection that are very similar (or nearly
identical) to photo “A”? And, more generally, are there any items in a photo/video
collection that are very similar (or nearly identical) with each other?
• How to present a large photo collection concerning an event to the user in a compact
way?
Inthischapter,wepresentmultimediaprocessingtechniquesthatwillhelpaPreserve-
or-Forget system (PoF, see Chap.6) answer such questions, and/or give clues about
which items can be “forgotten” and which should be “preserved” as input to in a
preservation value estimation method (see Chap.4).
We focus primarily on the following multimedia analysis and organization tasks:
(a) photo/video annotation, (b) photo/video quality assessment, (c) near-duplicate
detection, and (d) event-based photo clustering and summarization. Photo/video
annotation refers to the problem of assigning one or more semantic concepts to
photos or video fragments. We focus on deep learning techniques that have shown
excellent results during the last years. We show how to take advantage of state-of-the-
art pretrained deep convolutional neural networks (DCNNs) in smaller collections
by ﬁne-tuning their parameters, and we also present a method that improves the
photo/video annotation accuracy by performing multi-task learning in combination
with DCNN-based features. Photo/video aesthetic quality assessment refers to the
automatic prediction of the aesthetic value of a photo/video. To this end, we inves-
tigate comprehensive image- and motion-based features, as well as a sophisticated
learning method that takes the input features’ uncertainty into consideration. Near-
duplicate detection and retrieval aim to identify very similar media items in large
collections. We present various approaches for near-duplicate detection and retrieval
based on the use of different types of features extracted from photos. On top of
all the presented approaches, we propose a graph-based analysis of a photo/video
collection to support both near-duplicate detection and near-duplicate retrieval sce-

3
Multimedia Processing Essentials
49
narios. Event-based photo clustering and summarization consist in segmenting an
event-centered photo collection to groups, each depicting a distinct part of the event.
This is an essential preprocessing step for being able to summarize an event-centered
photo collection by selecting the most characteristic photos of each subevent, thus
creating a storyline that conveys the gist of the collection. For all of the above tech-
niques,wereviewthestateoftheart,proposeanownapproach,andperformextensive
experiments and comparisons.
3.2
Photo and Video Annotation
Photo/video annotation, aka concept detection, refers to the task of assigning seman-
tic labels to photos or video fragments (e.g., shots) based on a predeﬁned list of
labels [369]. Photos/videos are commonly annotated with concept labels (aka key-
words or tags) that refer to objects (e.g., “car” and “chair”), activities (e.g., “running”
and “dancing”), scenes (e.g., “hills” and “beach”), etc. Automatically understanding
the content of photos/videos is a challenging and intensively investigated problem.
Typically, a two-step label detection procedure for photo annotation is followed: (a)
Extraction of features; referring to interest point detection, local feature extraction
and feature encoding, or DCNN-based (deep convolutional neural network) features.
(b) Classiﬁcation; referring to the process of building classiﬁers that learn the asso-
ciations between the extracted features and semantic labels. A similar process can be
followed for video annotation. In this case, video sequences are ﬁrst segmented into
one or more video fragments; one or more keyframes are selected for each fragment,
and these keyframes can subsequently be treated as still images.
In this section, we discuss how deep learning can be applied to photo/video anno-
tation, focusing mainly on the optimal training of DCNNs on small-sized datasets
using transfer learning, and on how improved concept detection accuracy can be
achieved by employing multitask learning (MTL) techniques in combination with
DCNN-based features. A recent trend in video annotation is to learn features directly
from the raw keyframe pixels using DCNNs. A DCNN consists of several layers that
are placed sequentially the one after the other. Three main types of layers can be dis-
tinguished: convolutional, pooling, and fully-connected. In a typical architecture, the
most-bottom layers are convolutional layers, after each convolution layer, there may
be a pooling layer, and ﬁnally, after several convolutional and pooling layers one or
more fully-connected layers follow. A fully-connected layer takes all neurons in the
previous layer and connects it to every single neuron it has. Fully-connected layers
can be visualized as d-dimensional vectors. Essentially, the convolutional layers are
providing a meaningful, low-dimensional, and somewhat invariant feature space, and
the fully-connected layer is learning a (possibly nonlinear) function in that space.
DCNN training requires the learning of millions of parameters, which means that a
small-sized training set could easily over-ﬁt the DCNN on the training data. Transfer-
ring a pretrained network in a new dataset by ﬁne-tuning its parameters is a common
strategy that can take advantage of the bottom generic layers and adjust the top layers

50
K. Apostolidis et al.
to the target dataset and the new target concepts [74, 147, 428]. Most DCNN-based
approaches use the pretrained or ﬁne-tuned DCNN as a standalone classiﬁer, i.e.,
they perform the ﬁnal class label prediction directly, using for example, a softmax
layer [210, 363]. Other approaches use the DCNN as a feature generator, i.e., they
train classiﬁers with features extracted from one or more layers of the DCNN [266].
Speciﬁcally, for each target concept one supervised learning classiﬁer (e.g., SVM) is
trained to solve a binary classiﬁcation problem (i.e, decide on the presence or absence
of the concept) [369]. Independently, training concept detectors (i.e., using SVMs)
is a single-task learning (STL) process, where each task involves recognizing one
concept. STL ignores the fact that groups of concepts can be related. Some recent
methods use MTL in order to build many concept detectors together at the same time,
which can improve concept detection accuracy. In the remainder of this section, we
survey the literature on DCNN transfer learning and MTL for video/photo annotation
(Sect.3.2.1), and then we present in more detail:
• Three ﬁne-tuning strategies that can be used for transferring a pretrained DCNN
to a new target domain (Sect.3.2.2.1).
• A lifelong MTL algorithm for photo/video concept detection (Sect.3.2.2.2).
3.2.1
Related Work
Concept detection is a multi-label classiﬁcation problem (one photo/keyframe may
be annotated with more than one semantic concepts), that can be treated as multi-
ple independent binary classiﬁcation problems, where for each concept a model can
be learned to distinguish photos/keyframes that the concept appears in from those
that the concept does not appear in. Given feature-based photo/keyframe represen-
tations and the ground-truth annotations for each photo/keyframe (i.e., the concepts
that are present), any supervised machine learning algorithm that solves classiﬁ-
cation problems can be used in order to learn the relations between the low-level
photo/keyframe representations and the high-level semantic concepts. The most com-
monly used machine learning algorithms are support vector machines (SVM) and
logistic regression (LR). Chi-square kernels, that were originally considered to be
optimal for use in SVMs [182, 433] are now often replaced by Histogram Intersec-
tion kernels [261] or even Linear SVMs. Other machine learning algorithms have
also been used, such as Random Forest and Lazy style algorithms (e.g., knn); how-
ever, achieving lower performance or presenting higher computational complexity.
A recent trend in video annotation is to learn features directly from the raw keyframe
pixels using DCNNs. DCNNs consist of many layers of feature extractors, which
makes them have a more sophisticated structure than simply using a single hand-
crafted representation or combinations of a few such. DCNNs can be used either as
feature generators, as described in the beginning of this section, or as standalone clas-
siﬁers, i.e., unlabeled keyframes are forward propagated by a DCNN that performs

3
Multimedia Processing Essentials
51
the ﬁnal class label prediction directly, using typically a softmax or a hinge loss
layer [210, 363].
The small number of labeled training examples is a common problem, especially
invideodatasets,makingitdifﬁculttotrainadeepnetworkfromscratchwithoutover-
ﬁtting its parameters on the training set [368]. For this reason, it is common to use
transfer learning, i.e., to take a network that has been trained on a large-scale source
dataset (e.g., ImageNet [345]) and ﬁne-tune its parameters for the target dataset.
Fine-tuning is a process where the weights of a pretrained DCNN are used as the
starting point for a new target training set and they are modiﬁed in order to adapt the
pretrained DCNN to the new target dataset. Different DCNN-based transfer learning
approaches have been successfully applied in many datasets. The most straightfor-
ward approach replaces the classiﬁcation layer of a pretrained DCNN with a new
output layer that corresponds to the categories that should be learned with respect
to the target dataset, [74, 147, 428]. Generalizing this approach, the weights of the
ﬁrst K network layers can remain frozen, i.e., they are copied from the pretrained
DCNN and kept unchanged, and the rest of the layers (be it just the last one or more
than one) are learned from scratch [60, 316]. Alternatively, the copies of the ﬁrst K
layers could be allowed to adapt to the target dataset with a low learning rate. For
example, [428] investigates which layers of Alexnet [210] are generic, i.e., can be
directly transferred to a target domain, and which layers are dataset-speciﬁc. Further-
more, experiments in [428] show that ﬁne-tuning the transferred layers of a network
work better than freezing them. Other studies extend the pretrained network by one
or more fully-connected layers, which seems to improve the above transfer learning
strategies [60, 316, 329, 368]. In Sect.3.2.2.1 we present the most commonly used
ﬁne-tuning strategies and we compare them in order to ﬁnd the most suitable one for
photo/video annotation.
Given a pretrained or a ﬁne-tuned DCNN, more elaborate methods that promote
the sharing of information between concepts, such as multitask learning (MTL) meth-
ods instead of STL ones (e.g., training SVM classiﬁers on DCNN-based features),
have been proposed in order to improve concept detection accuracy. Speciﬁcally, a
pretrained or ﬁne-tuned DCNN is applied on a ground-truth annotated video/image
dataset in order to extract DCNN-based features; then, a MTL classiﬁer can be trained
on these features in order to learn task models for all of the concepts together at the
same time by exploiting the relations across the different concept models. Many
MTL methods have been proposed that exploit task relatedness towards improving
the overall concept detection accuracy. The main difference of MTL methods is the
way they deﬁne task relatedness. Some methods identify shared features between
different tasks and use regularization to model task relatedness [20, 305, 313]. Other
methods identify a shared subspace over the task parameters [21, 112, 132]. The
methods above make the strong assumption that all tasks are related; some newer
methods consider the fact that some tasks may be unrelated. For example, the clus-
tered MTL algorithm (CMTL) [438] uses a clustering approach to assign to the
same cluster parameters of tasks that lie nearby in terms of their L2 distance. Adap-
tive MTL (AMTL) [376] decomposes the task parameters into a low-rank structure
that captures task relations, and a group-sparse structure that detects outlier tasks.

52
K. Apostolidis et al.
The GO-MTL algorithm [211] uses a dictionary-based method that allows two tasks
from different groups to overlap by having one or more basis in common. In general,
MTL methods have high-computational cost, because the shared representation of
all tasks is learned at once. Online or lifelong MTL, in contrast, can learn consecutive
tasks that arrive in a sequence. The ELLA algorithm [121] is the online version of
GO-MTL [211], presenting similar performance but being three orders of magnitude
faster in training. In Sect.3.2.2.2 we show how ELLA can be used as the starting point
for devising an online MTL algorithm suitable for image/video concept detection.
3.2.2
Method
3.2.2.1
DCNN-Based Features and Fine-Tuning Strategies
Let Dso denote a pretrained DCNN, trained on Tso categories using a source dataset,
and Dta denote the target DCNN, ﬁne-tuned on Tta categories of a different target
dataset. In this section, we present three ﬁne-tuning strategies (Fig.3.1) that can be
used for the problem of visual annotation, in order to effectively ﬁne-tune DCNNs
Dso that were trained on a large visual dataset for a new target video/image dataset.
These three ﬁne-tuning strategies are as follows:
• FT1-def: Default ﬁne-tuning strategy: This is the typical strategy that modiﬁes the
last fully-connected layer of Dso to produce the desired number of outputs Tta, by
replacing the last fully-connected layer with a new Tta-dimensional classiﬁcation
fully-connected layer.
• FT2-re: Re-initialization strategy: In this scenario, similar to FT1-def, the last
fully-connectedlayer is replacedbyanew Tta-dimensional classiﬁcationlayer. The
weights of the last N layers, preceding the classiﬁcation layer, are also reinitialized
(i.e., reset and learned from scratch).
• FT3-ex: Extension strategy: Similar to the previous two strategies, the last
fully-connected layer is replaced by a new Tta-dimensional classiﬁcation fully-
connected layer. Subsequently, the network is extended with E fully-connected
layers of size L that are placed on the bottom of the modiﬁed classiﬁcation layer.
These additional layers are initialized and trained from scratch during ﬁne-tuning,
at the same rate as the modiﬁed classiﬁcation layer. One example of a modiﬁed
network, after the insertion of one extension layer for two popular DCNN archi-
tectures, is presented in Fig.3.2. Regarding the GoogLeNet architecture in this
ﬁgure, which has two additional auxiliary classiﬁers, an extension layer is also
inserted in each of them.
Each ﬁne-tuned network Dta can be used in two different ways to annotate new
test keyframes/images with semantic concepts: (a) Direct classiﬁcation: Each test
keyframe/image is forward propagated by Dta and the network’s output is used as
the ﬁnal class distribution assigned to the keyframe/image. (b) Dta is used as feature
generator: The training set is forward propagated by the network and the features

3
Multimedia Processing Essentials
53
Fig. 3.1 Fine-tuning strategies outline
extracted from one or more layers of Dta are used as feature vectors to subsequently
train one supervised classiﬁer (e.g., Logistic Regression) per concept. Then, each test
keyframe/image is ﬁrst described by the DCNN-based features and subsequently,
these features serve as input to the trained classiﬁers.
3.2.2.2
ELLA with Label Constraints: ELLA_LC
In this section, we present a MTL method that uses a trained DCNN (i.e., pretrained or
ﬁne-tunedfromasourceDCNNusinganyofthethreeﬁne-tuningstrategiespresented
in Sect.3.2.2.1) as a feature generator in order to learn concept detectors. Starting by
formulating our problem, a video concept detection system needs to learn a number of
supervised learning tasks, one for each target concept. Each task t is associated with
one concept detector (task model) C(t) : Rd →{±1}, and the training set available

54
K. Apostolidis et al.
Fig. 3.2 A simpliﬁed illustration of the CaffeNet [210] (left) and GoogLeNet [377] (right) archi-
tectures used after insertion of one extension layer. Each of the inception layers of GoogLeNet
consists of six convolution layers and one pooling layer. The ﬁgure also presents the direct output
of each network and the output of the last three layers that were used as features w.r.t. the FT3-ex
strategy. Similarly, the corresponding layers were used for the FT1-def and FT2-re strategies
for this concept (x(t)
i , y(t)
i )nt
i=1, where x(t)
i
∈Rd, a feature vector extracted for the
ith keyframe and y(t)
i
∈{±1} the ground-truth annotation for the ith keyframe and
the tth task/concept. A concept detector can be deﬁned as C(t)(x(t)) = C(x(t); w(t)),
where w(t) ∈Rd is the task parameter vector. A concept detection system should
be easily extended with new tasks even if the total number of tasks/concepts Tmax
is not available from the start. Furthermore, task parameters of related concepts
may share similar knowledge, but also concept correlations obtained by the ground-
truth annotation may provide another source of information regarding the relations

3
Multimedia Processing Essentials
55
between tasks. Considering all the above, in [267] we introduced the ELLA_LC
algorithm (Efﬁcient Lifelong Learning Algorithm with Label Constraint), which is
presented in the rest of this section.
ELLA_LC is an extension of the ELLA algorithm proposed by Eaton and
Ruvolo [121]. Speciﬁcally, we add a new label-based constraint on ELLA’s model in
order to incorporate statistical information of pairwise correlations between concepts
that we can acquire from the ground-truth annotation. ELLA_LC uses a knowledge
shared basis L ∈Rd×k for all task models, where the columns of L correspond to the
parameter vectors of k latent tasks. We model the parameter vector w(t) of observed
task t as w(t) = Ls(t), where s(t) ∈Rk is a task-speciﬁc weight vector that contains
the coefﬁcients of the linear combination. Each linear combination is assumed to be
sparse in L; in this way, we assume that there exist a small number of latent basis
tasks and the task parameter vector of every observed task w(t) is a linear combination
of them. The overlap in the sparsity patterns of any two tasks controls the amount of
sharing between them. ELLA_LC builds a concept detection system that (a) updates
the shared basis L when a new concept arrives without building all the previous task
models from scratch; and (b) incorporates the label correlations of the new concept
with all of the previously learned concepts in order to improve the learning of the
task-speciﬁc weight vector regarding the new concept and the shared basis L. The
above problem can be formulated by the following objective function:
min
(L,s(t))
1
T
T

t=1

1
nt
nt

i=1
L

C(x(t)
i ; Ls(t)), y(t)
i

+ μ
s(t)
1
(3.1)
+ β
⎛
⎜⎜⎝
T

t′=1
t′̸=t
1
T -1|rt,t′|
L(s(t) −sign(rt,t′)s(t′))

2
⎞
⎟⎟⎠
⎫
⎪⎪⎬
⎪⎪⎭
+ λ ∥L∥2
F ,
where L refers to the loss function that is used for learning the task-speciﬁc param-
eter vector w(t); and w(t) is obtained by building a classiﬁer only using the training
data available for task t: min(w(t))
1
nt
nt
i=1 L

C(x(t)
i ; w(t), y(t)
i )

. T is the number
of tasks that have been learned so far. rt,t′ is the φ-correlation coefﬁcient between
the concepts learned in tasks t and t′, calculated from the ground-truth annotation of
the training set.
This is similar to the objective function of ELLA [121] with the addition of the
extra term: |rt,t′|
L(s(t) −sign(rt,t′)s(t′))
2 that incorporates the label correlations.
Speciﬁcally, we aim to model highly correlated concepts (positive or negative). We
assume that if two concepts are positively correlated, then the underlying task param-
eters should be similar; on the other hand, if two concepts are negatively correlated,
then the task parameters should be opposite. The similarity measure suitable to cap-
ture both positive and negative correlations that was selected in this study is the
φ-correlation coefﬁcient. To model this assumption, we use the above constraint in
the objective function of Eq.(3.1). The larger the correlation between two concepts

56
K. Apostolidis et al.
Algorithm 1 ELLA_LC(k,d,λ,μ,β)
while existMoreTrainingData() do
T ←T +1, t = T
1. (w(t), H(t)) ←base_learner(X(t)
new, y(t)
new)
2. s(t) ←Eq.(3.3)
3. A ←A +

(s(t)s(t)⊤) ⊗H(t)
+β
T
t′=1
t′̸=t
1
T −1|rt,t′|[φt,t′φ⊤
t,t′ ⊗I]

4. b ←b + vec(s(t)⊤⊗(w(t)⊤H(t)))
5. L ←mat(( 1
T A + λI)−1 1
T b)
end while
(positive or negative), the higher the imposed constraint. On the other hand, if two
concepts are not correlated, using the above function will not impose any constraint.
This constraint is applicable to linear classiﬁers, where positive correlated concepts
are forced to return similar responses, while negative correlated concepts are forced
to return opposite responses.
Equation(3.1) is not jointly convex in L and s(t), so ELLA [121] approximates
them using two simpliﬁcations that we also follow here: (a) to eliminate the explicit
dependence on all previous training data through the inner summation, we approxi-
mate Eq.(3.1) using the second-order Taylor expansion of
1
nt
nt

i=1
L

C

x(t)
i ; w(t)
, y(t)
i

around w(t). (b) We compute each s(t) only when training data for task t are available
and do not update it when new tasks arrive. These give the following objective
function that approximates Eq.(3.1):
min
(L,s(t))
1
T
T

t=1
w(t) −Ls(t)2
H(t) + μ
s(t)
1
(3.2)
+ β
⎛
⎜⎜⎝
T

t′=1
t′̸=t
1
T −1|rt,t′|
L(s(t) −sign(rt,t′)s(t′))

2
⎞
⎟⎟⎠
⎫
⎪⎪⎬
⎪⎪⎭
+ λ ∥L∥2
F ,
where H(t) is the Hessian of the loss function L evaluated on w(t). To optimize this
objective function, consequently to update our model, we perform three steps: (a)
we compute the φ-correlation coefﬁcient of the concept learned in task t with all the
previously learned concepts; (b) we compute the task-speciﬁc weight vector s(t) and
(c) we update the shared basis L. Below we describe each of these steps in more
details.

3
Multimedia Processing Essentials
57
To compute s(t) we solve Eq.(3.2) for s(t), when the task t arrives (i.e., t = T ):
min
s(t)
w(t) −Ls(t)2
H(t) + μ
s(t)
1
+ β
⎛
⎜⎜⎝
t
t′=1
t′̸=t
1
t−1|rt,t′|
L(s(t) −sign(rt,t′)s(t′))

2
⎞
⎟⎟⎠
⎫
⎪⎪⎬
⎪⎪⎭
(3.3)
By expanding the above we arrive at the following problem that can be solved using
quadratic programming:
min
s(t)
⎧
⎪⎪⎨
⎪⎪⎩
s(t)⊤
⎡
⎢⎢⎣L⊤H(t)L + β
⎛
⎜⎜⎝
t
t′=1
t′̸=t
1
t−1|rt,t′|L⊤L
⎞
⎟⎟⎠
⎤
⎥⎥⎦s(t)
(3.4)
−2
⎡
⎢⎢⎣w(t)⊤H(t)L + β
2
⎛
⎜⎜⎝
t
t′=1
t′̸=t
1
t−1rt,t′s(t′)⊤L⊤L
⎞
⎟⎟⎠−μ
2 I
⎤
⎥⎥⎦s(t)
⎫
⎪⎪⎬
⎪⎪⎭
where I denotes the identity matrix of order k × d. Similarly, to update L, we solve
Eq.(3.2) for L, which equals to:
min
L
1
T
T

t=1
w(t) −Ls(t)2
H
+ β
⎛
⎜⎜⎝
T

t′=1
t′̸=t
1
T −1|rt,t′|
Lφt,t′
2
⎞
⎟⎟⎠
⎫
⎪⎪⎬
⎪⎪⎭
+ λ ∥L∥2
F ,
(3.5)
where φt,t′ = s(t) −sign(rt,t′)s(t′). Then, we null the gradient of Eq. (3.5) and solve
for L. This gives a column-wise vectorization of L as A−1b where:
A = 1
T
T

t=1

(s(t)s(t)⊤) ⊗H(t)
+ β
⎛
⎜⎜⎝
T

t′=1
t′̸=t
1
T −1|rt,t′|[φt,t′φ⊤
t,t′ ⊗I]
⎞
⎟⎟⎠
⎫
⎪⎪⎬
⎪⎪⎭
+ λI, and
b = 1
T
T

t=1
vec(s(t)⊤⊗(w(t)⊤H(t)))
(3.6)

58
K. Apostolidis et al.
where ⊗is the Kronecker product symbol.
Algorithm 1 summarizes the steps that ELLA_LC performs for updating the above
parameters and learning consecutive tasks. In each iteration, ELLA_LC receives
training data (X(t)
new, y(t)
new) for a task t. If this is the ﬁrst time that task t appears, then
ELLA_LC computes the model parameters w(t) and Hessian H(t) from only this
training data for task t using a base learner (Algorithm 1: Step 1). This step depends
on the base learning algorithm; Eaton and Ruvolo [121] provide details for learning
linear and logistic regression models using ELLA. In [267] we also show how the
hinge loss can be used instead. We compute the φ-correlation coefﬁcient of the
concept learned in task t with all the previously learned concepts. Subsequently, the
correlation information and the shared basis L are used to compute the task-speciﬁc
weight vector s(t) (Algorithm 1: Step 2). Finally, ELLA_LC updates the basis L
to incorporate new knowledge via an incremental update that considers Eq. (3.6)
(Algorithm 1: Steps 3–5). If additional training data for a previously learned task is
provided, the algorithm can be extended, similarly to [121], in order to concatenate
the new data with the past data and then update the vector s(t) and the shared basis L.
3.2.3
Experimental Study
3.2.3.1
Dataset and Experimental Setup
The TRECVID SIN task 2013 [6] dataset and the PASCAL VOC-2012 [131] dataset
were utilized to train and evaluate the concept detection methods presented in the
previous sections. The TRECVID SIN dataset consists of low-resolution videos,
segmented into video shots; each shot is represented by one keyframe. The dataset
is divided into a training and a test set (approx. 600 and 200h, respectively). The
training set is partially annotated with 346 semantic concepts. The test set is evaluated
on 38 concepts, i.e., a subset of the 346 concepts. The PASCAL VOC-2012 [131]
dataset consists of photos annotated with one object class label of the 20 available
object classes. PASCAL VOC-2012 is divided into training, validation, and test sets
(consisting of 5717, 5823, and 10991 photos, respectively). We used the training set to
train the compared methods, and evaluated them on the validation set. We did not use
the original test set because ground-truth annotations are not publicly available for it
at the time of writing this chapter. The photo/video indexing problem was examined;
that is, given a concept, we measure how well the top retrieved photos/video shots for
this concept truly relate to it. Figure3.3 shows (a) an example regarding the concept-
based image annotation problem, i.e., a photo is automatically annotated with the
concepts that are depicted in it; (b) an example regarding the concept-based image
retrieval problem, i.e., the top photos that are retrieved for concept “river”.
A ﬁrst set of experiments was developed in order to compare the three ﬁne-
tuning strategies presented in Sect.3.2.2.1. Speciﬁcally, in all cases, we replaced
the classiﬁcation fully-connected (FC) layer of the utilized pretrained network, with

3
Multimedia Processing Essentials
59
Fig. 3.3 Example of a the concept-based image annotation and b the concept-based image retrieval
problem
a 345-dimensional FC classiﬁcation layer for the 345 concepts of the TRECVID SIN
dataset,orwitha20-dimensionalclassiﬁcationlayerforthe20objectcategoriesofthe
PASCAL VOC-2012 dataset. We examined two values for parameter N of the FT2-
re strategy; we refer to each conﬁguration as FT2-re1 (for N = 1) and FT2-re2 (for
N = 2). The FT3-ex strategy was examined for two settings of network extensions
E ∈{1, 2}:i.e.,extendingthenetworkbyoneortwoFClayers,respectively,followed
by ReLU (Rectiﬁed Linear Units) and Dropout layers. Seven values where exam-
ined for the size of each extension layer: L ∈{64, 128, 256, 512, 1024, 2048, 4096}.
We refer to these conﬁgurations as FT3-exE-L. The new layers’ learning rate and
momentum was set to 0.01 and 5e−4, whereas the mini-batch size was restricted to
128 (based on the capabilities of the employed PC and GPU).
A second set of experiments was developed to evaluate the MTL approach pre-
sented in Sect.3.2.2.2. We instantiated ELLA_LC with two base classiﬁers: LR and
LSVM. We performed comparisons with the following methods: (a) STL using (i)
LR, (ii) LSVM, and (iii) kernel SVM with radial kernel (KSVM); (b) The label
powerset (LP) multi-label learning algorithm, that has been used in [268] to model
the label relations; (c) AMTL [376], and (d) CMTL [438], two batch MTL methods,

60
K. Apostolidis et al.
Table 3.1 MXinfAP (%) for GoogLeNet-5k-345-SIN. The best result per column is underlined.
The globally best result is bold and underlined
conf/layer
Final classiﬁer
Middle classiﬁer
First classiﬁer
Direct
(a)
Last
(b)
2nd last
(c)
3rd last
(d)
Fused
(e)
Direct
(f)
Fused
(g)
Direct
(h)
Fused
(i)
FT1-def
22.45
29.60
29.80
–
30.58
23.08
29.41
21.25
26.00
FT2-re1
20.88
28.44
28.43
–
29.58
22.51
28.55
20.37
25.16
FT2-re2
19.08
27.21
27.17
–
28.02
21.73
28.44
20.07
25.74
FT3-ex1-64
25.48
28.86
26.86
29.22
29.62
23.30
28.37
20.20
24.47
FT3-ex1-128 25.52
29.75
28.66
29.57
30.60
23.98
28.82
20.87
25.38
FT3-ex1-256 24.79
30.16
28.99
30.26
31.11
23.62
29.56
21.06
26.32
FT3-ex1-512 24.28
30.86
29.26
29.68
31.47
23.54
29.86
20.71
26.32
FT3-ex1-
1024
24.03
31.02
28.78
29.35
31.55
23.43
29.90
20.53
26.57
FT3-ex1-
2048
23.37
31.02
27.24
29.37
31.02
23.29
29.94
20.56
26.61
FT3-ex1-
4096
23.07
30.91
28.98
29.61
31.57
22.85
29.64
20.82
26.26
FT3-ex2-64
16.44
17.51
19.62
19.95
20.09
11.43
15.12
10.65
13.33
FT3-ex2-128 23.87
26.19
26.73
26.05
27.02
18.70
23.64
14.87
19.95
FT3-ex2-256 24.46
28.94
28.69
28.68
29.57
22.68
26.98
18.75
23.10
FT3-ex2-512 23.95
29.44
29.07
28.94
30.14
22.72
28.22
20.20
24.79
FT3-ex2-
1024
23.41
30.03
28.80
29.54
30.63
22.79
29.10
19.74
25.68
FT3-ex2-
2048
23.38
30.74
28.98
28.21
30.61
22.29
29.34
19.57
26.23
FT3-ex2-
4096
23.07
31.21
28.94
27.98
30.93
22.11
29.40
19.64
26.11
(e) ELLA [121], an online MTL approach. We selected all the parameter values for
these methods based on the training data alone. The value of k was set to 38, regular-
ization parameters λ and μ in Eq. (3.1) were kept ﬁxed at e−10 and e−3, respectively,
for all the online MTL methods, in all experiments. These parameters are expected to
depend on the dimensionality of the feature space and the number of examples, and
according to preliminary experiments seem to work well for the employed features.
The parameter value of β for ELLA_LC was selected from the set {e−2, e−1, 1, e1,
e2}. The ordering of the tasks was ﬁxed and the same for all the online MTL algo-
rithms, and each task was presented sequentially. AMTL and CMTL learned each
new task from a single batch of data that contained all training instances of that task.

3
Multimedia Processing Essentials
61
Table 3.2 MAP % for GoogleNet-5k-VOC. The best result per column is underlined. The globally
best result per subtable is bold and underlined
conf/layer
Final classiﬁer
Middle classiﬁer
First classiﬁer
Direct
(a)
Last
(b)
2nd last
(c)
3rd last
(d)
Fused
(e)
Direct
(f)
Fused
(g)
Direct
(h)
Fused
(i)
FT1-def
82.39
86.75
86.74
–
88.01
81.10
84.25
78.96
79.06
FT2-re1
80.50
85.21
86.91
–
87.44
79.58
82.76
77.78
77.23
FT2-re2
77.73
78.81
83.13
–
83.11
75.28
77.34
71.99
69.65
FT3-ex1-64
79.74
82.86
86.41
86.26
86.92
76.36
82.72
72.32
77.51
FT3-ex1-128 80.47
85.50
88.26
86.56
88.12
78.57
84.12
74.01
78.76
FT3-ex1-256 81.43
85.81
88.33
86.73
88.36
79.31
84.48
75.29
79.12
FT3-ex1-512 81.65
85.91
87.84
86.90
88.33
79.99
84.76
76.25
79.69
FT3-ex1-
1024
82.30
86.48
87.01
86.89
88.20
80.68
84.56
77.32
79.32
FT3-ex1-
2048
82.51
86.93
86.80
86.96
88.23
81.15
84.51
77.97
79.62
FT3-ex1-
4096
82.39
87.20
86.37
87.05
88.13
81.52
84.45
78.43
79.65
FT3-ex2-64
43.85
45.11
53.99
51.67
52.81
39.10
47.22
32.42
38.72
FT3-ex2-128 75.89
70.96
82.85
83.34
82.51
63.27
72.34
54.45
63.64
FT3-ex2-256 78.94
80.30
86.44
86.43
86.01
69.19
77.67
65.31
72.75
FT3-ex2-512 80.47
82.83
87.56
87.00
87.38
75.17
81.44
66.50
74.38
FT3-ex2-
1024
81.47
84.54
86.81
86.53
87.58
76.99
82.85
71.09
76.74
FT3-ex2-
2048
82.11
85.49
86.90
86.28
87.76
78.15
83.24
73.55
77.69
FT3-ex2-
4096
80.50
83.83
85.82
84.71
86.64
77.49
81.79
74.66
78.21
3.2.3.2
Experimental Results
Tables 3.1 and 3.2 present the results of the three ﬁne-tuning strategies of Sect.3.2.2.1
for the TRECVID SIN and PASCAL VOC dataset, respectively. For each dataset,
we ﬁne-tuned the GoogLeNet-5k, which refers to a DCNN that we trained according
to the 22-layer GoogLeNet architecture on the ImageNet “fall 2011” dataset for
5055 categories. Speciﬁcally, GoogLeNet-5k was ﬁne-tuned on the 345 TRECVID
SIN concepts (i.e., all the available TRECVID SIN concepts, except for one which
was discarded because only ﬁve positive samples are provided for it). We refer to
it as GoogLeNet-5k-345-SIN. In addition, GoogLeNet-5k was ﬁne-tuned on the
positive examples of the PASCAL VOC-2012 training set. This network is labeled
as GoogLeNet-5k-VOC. For the TRECVID SIN dataset, we analyze our results in
terms of mean extended inferred average precision (MXinfAP) [427], which is an

62
K. Apostolidis et al.
approximation of the mean average precision (MAP) suitable for the partial ground-
truth that accompanies the TRECVID dataset [6]. Table 3.1 presents the MXinfAP
of the GoogLeNet-5k-345-SIN and Table 3.2 presents the results in terms of a MAP
of the GoogLeNet-5k-VOC.
For each pair of utilized network and ﬁne-tuning strategy we evaluate: First, the
direct output of the network (Tables 3.1 and 3.2: col. (a)). Second, logistic regression
(LR) classiﬁers trained on DCNN-based features. Speciﬁcally, the output of each of
the three last layers of each ﬁne-tuned network was used as a feature to train one
LR model per concept (Tables 3.1 and 3.2: col. (b)–(d)). Furthermore, we present
results for the late-fused output (arithmetic mean) of the LR classiﬁers built using
the last three layers (Tables 3.1 and 3.2: col. (e)). Evaluations are also reported for
the two auxiliary classiﬁers (Tables 3.1 and 3.2: col. (f)–(i)). The details of the two
GoogLeNet architectures and the extracted features are also illustrated in Fig.3.2.
Based on the results reported in the aforementioned tables, we reach the following
conclusions:
• For both datasets, the FT3-ex strategy almost always outperforms the other two
ﬁne-tuning strategies (FT1-def, FT2-re) for speciﬁc (L, E) values.
• With respect to the direct output, FT3-ex1-64 and FT3-ex1-128 constitute the top-
two methods for the TRECVID SIN dataset. On the other hand, FT3-ex1-2048
and FT3-ex1-4096 are the top-two methods for the PASCAL VOC-2012 dataset.
That is, the FT3-ex strategy with one extension layer is always the best solution,
but the optimal dimension of the extension layer varies, depending on the target
domain dataset.
• The highest concept detection accuracy for each network is always reached when
LR classiﬁers are trained on features extracted from the last and the second last
fully-connected layer using the FT3-ex strategy for TRECVID SIN and PASCAL
VOC-2012 dataset (when examining each DCNN layer separately, i.e., excluding
the fusion of features reported in column (e) of Tables 3.1 and 3.2), respectively.
That is, features extracted from the top layers are more accurate than layers posi-
tioned lower in the network, but the optimal layer varies, depending on the target
domain dataset.
• DCNN-based features signiﬁcantly outperform the direct output alternative in
the majority of cases. However, in a few cases, the direct network output works
comparably well. The choice between the two approaches should be based on
the application that the DCNN will be used, e.g., real-time applications’ time and
memory limitations would probably render using DCNNs as feature extractors in
conjunction with additional learning (LR or SVMs) prohibitive. Furthermore, we
observe that the features extracted from the ﬁnal classiﬁer of GoogLeNet-based
networks outperform the other two auxiliary classiﬁers, in most cases.
• Finally, it is better to combine features extracted from many layers; speciﬁcally,
performing late fusion on the output of the LR classiﬁers trained with each one of
the last three fully-connected layers almost always outperforms using a single such
classiﬁer irrespective of the employed network (Tables 3.1 and 3.2: col. (e)). The
above conclusion was also reached for the auxiliary classiﬁers of GoogLeNet-

3
Multimedia Processing Essentials
63
based networks, but for space-limitations, we only present the fused output for
each of these auxiliary classiﬁers (Tables 3.1 and 3.2: col. (g), (i)).
The interested reader can refer to [329] for an extensive experimental comparison
with more DCNN architectures, different subsets of concepts and different DCNN
and ﬁne-tuning learning parameters.
Table 3.3 presents the results with respect to the MTL method presented in
Sect.3.2.2.2. In this case, we used features from four different DCNNs that were
trained on ImageNet data [345]: (a) The 8-layer CaffeNet [210], (b) the 16-layer
deep ConvNet [363], (c) the 22-layer GoogLeNet [377], (iv) the GoogLeNet-5k that
was used for the ﬁne-tuning experiments above. We will refer to these networks as
CaffeNet1K, ConvNet1K, GNET1k, GNET5k, respectively. We applied each of these
networks on the TRECVID keyframes and we used as a feature the network’s direct
output, that corresponds to the ﬁnal class label prediction for 1000 ImageNet cate-
gories for CaffeNet1k, ConvNet1k, and GNET1k, and 5055 categories for GNET5k.
All the feature vectors were ﬁnally reduced to 400 dimensions using principal com-
ponents analysis (PCA), as this was shown to improve the performance for all the
methods in our experiments by around 2% (in terms of MXinfAP). To improve the
results, we also experimented with ﬁne-tuning (FT) the above methods on various
subsets of the 346 TRECVID SIN concepts using the extension strategy presented
in Sect.3.2.2.1 and evaluated above. This resulted in 5 FT networks that differ in the
number and dimension of the extension layers and in the number of output categories.
Subsequently, we again applied these FT networks on the TRECVID keyframes. To
train our base classiﬁers, for each concept, a training set was assembled that included
all positive annotated training examples for the given concept, and negatives to a
maximum of 15:1 ratio.
Table 3.3 presents the results of our experiments in terms of MXinfAP. ELLA_QP
is an intermediate version of the ELLA_LC that solves the objective function of
ELLA [121] with respect to s(t) using quadratic programming (QP), instead of solv-
ing the Lasso problem [121], but does not use the label constraint of ELLA_LC.
Starting from the upper part of Table 3.3, which refers to features extracted from
ImageNet DCNN networks, we can see that the ELLA_QP and ELLA_LC perform
better than the STL alternatives both when LR and when LSVM is used as the base
learner. In addition, solving Eq. (3.2) with QP (ELLA_QP) outperforms the orig-
inal ELLA [121]. Adding also the label constraint (ELLA_LC) further improves
the ELLA_QP method for all of the feature types. The ELLA_LC with LSVM is
the best-performing method, reaching a MXinfAP of 26.10%. Similar conclusions
can be reached if we look in the lower part of Table 3.3, where features extracted
from the FT networks are used. Furthermore, we observe that ﬁne-tuning is a proce-
dure that signiﬁcantly improves the retrieval accuracy of all the compared methods,
with ELLA_LC reaching once again the highest performance (MXinfAP equal to
32.10%). To investigate the statistical signiﬁcance of the difference of each method
from the best-performing method we used a paired t-test as suggested by [43]; in
Table 3.3, the absence of * suggests statistical signiﬁcance. We found that differ-
ences between the ELLA_LC and all the other methods are signiﬁcant (at 5% sig-

64
K. Apostolidis et al.
Table 3.3 MXinfAP for 38 concepts using different features or combinations of them: (a) using networks that have been trained on ImageNet data, (b) using
ImageNet networks that have been ﬁne-tuned on the TRECVID SIN 2013 training set. After the “-” symbol we indicate the number of concepts for which the
network has been ﬁne-tuned. For 4×DCNN and 5×DCNN FT the direct output of the four ImageNet and ﬁve FT networks, respectively, was fused, in terms of
arithmetic mean, and served as input to the learning algorithms. The * symbol indicates that the difference in MXinfAP between the denoted method and the
best-performing method in the same row of the table is not statistically signiﬁcant
R#
Features
Direct
output
Single-task learning
Joint concept learning
Proposed multi-task learning
LR
LSVM KSVM LP
[268]
AMTL
[376]
CMTL
[438]
ELLA
[121]
ELLA_QP
LR
ELLA_QP
LSVM
ELLA_LC
LR
ELLA_LC
LSVM
(a) Using the output of ImageNet-based networks as features
1
CaffeNet1k
-
13.00
14.20
12.81
11.77
12.90
11.56
13.14
13.99
16.27*
14.28
16.36
2
ConvNet1k
-
17.58
19.29
15.62
1608
17.58
16.09
17.88
18.45
21.02*
18.94
21.10
3
GNET1k
-
16.10
17.73
14.17
15.00
16.34
14.43
15.79
17.07
19.86*
17.48
19.98
4
GNET5k
-
20.89
22.68
20.73
20.54
21.01
19.99
15.65
21.88
24.05*
22.16
24.14
5
4×DCNN
-
21.77
24.29
22.64
19.58
22.96
21.42
21.17
23.66
25.97*
24.18
26.10
(b) Using the output of networks ﬁne-tuned on different subsets of the TRECVID SIN 2013 training set as features
6
CaffeNet1k-345
20.29
22.21
24.16
23.00
21.29
24.22
24.03
16.63
23.09
25.47*
23.51
25.88
7
GNET1k-60
19.77
24.51
24.30
23.07
25.06* 22.56
22.25
23.71
24.56
26.05
24.51
25.90*
8
GNET1k-60
19.90
24.71
24.78
22.90
25.20* 23.87
22.87
24.57
24.69
26.24
24.52
26.24
9
GNET1k-323
23.97
26.67
28.65
27.79
27.22
28.67
28.09
25.75
27.56
29.86
28.19
30.23
10
GNET5k-323
22.78
27.13
29.32
28.53
28.21
29.47
29.27
27.15
28.61
30.80*
28.90
31.01
11
5×DCNN FT
25.35
28.56
30.60
29.93
30.27
30.94
30.15
28.19
29.89
31.82*
30.32
32.10

3
Multimedia Processing Essentials
65
Fig. 3.4 Change in XinfAP for each task between the iteration that the task was ﬁrst learned and
the last iteration, (where all tasks had been learned), divided by the position of the task in the task
sequence
niﬁcance level) except for two runs based on LP (Table 3.3: R7-8) and all but one
of the ELLA_QP LSVM runs (Table 3.3: R9). We note that although the differences
between ELLA_QP and ELLA_LC are in most cases not signiﬁcant, ELLA_LC
exhibits better results consistently across the different runs (R1-11, except R7,8).
ELLA_LC updates the value of s(t) only when it receives new training data for task
t. This online characteristic of ELLA makes it faster than the batch MTL methods. At
the same time, the update of L beneﬁts the previously learned tasks by introducing
new knowledge acquired from the last learned task. To assess whether the latter
occurred, in Fig.3.4, for each task we computed the change in XinfAP from when
the task was ﬁrst learned and the task’s XinfAP in the last iteration (where all tasks had
been learned. We normalize this quantity by the task’s position in the task sequence.
We can see that reverse transfer occurred, i.e., a positive change in accuracy for a
task indicates this, mainly for the tasks that were learned early. As the pool of tasks
increases, early tasks get new knowledge from many more tasks, which explains why
the beneﬁt is bigger for them.
3.3
Photo and Video Aesthetic Quality Assessment
Photo and video aesthetic quality assessment are two emerging and challenging ﬁelds
of research. In the relevant literature, there are a few methods that examine the re-
ranking, search, and retrieval of photos based on their aesthetic quality, including
[193, 228, 307]. Aesthetic assessment methods are in great need for enhancing multi-
media content management in various applications, such as personal photo collection
management [362], food photo aesthetics assessment [231], and online fashion shop-
ping photo assessment [409]. In the video domain, the automatic assessment of each
video’s aesthetic value could further improve the users’ experience in multimedia
content distribution channels, since videos could be retrieved or recommended by
also taking their aesthetic quality into account.
In this chapter, we present our recent developments on a comprehensive and effec-
tive photo aesthetic assessment method based on the basic photographic rules [276],

66
K. Apostolidis et al.
and an extension of this method to the video domain [395]. The latter, besides exploit-
ing the information derived from both low- and high-level analysis of video layout,
leading to a photo- and motion-based video representation scheme, also uses a kernel
SVM extension, the KSVM-iGSU [397]. This classiﬁer takes the inherent uncer-
tainty of the input representation into consideration, in order to classify the videos
and retrieve those of high-aesthetic value.
3.3.1
Related Work
The aesthetic quality assessment of photographs is a challenging ﬁeld of research
where a variety of methods have been proposed over the past few years. Many
of the proposed methods detect the high aesthetic value photos by exploiting the
information derived from low-level visual features, such as brightness, texture, and
color distribution [110, 259, 382]. These methods are mainly based on the low-
level feature extraction. High-level features refer to photo layout analysis. In [115],
the photo aesthetic value is predicted based on the composition attributes, such as
salient objects and the rule of thirds; content attributes, which express the concepts
and objects depicted in the photo; and sky illumination attributes, which distinguish
between photos taken in clear, cloudy, or sunset sky. The spatial distribution of edges,
the color, and hue distribution, as well as the blurriness, brightness, and contrast
are used in [200] in order to identify high-quality professional photos. In addition,
in [246] the authors focus on the design of color and spatial features which are
not computationally expensive. In [264], it is shown that generic image descriptors
can sometimes outperform the aesthetic assessment methods that are based on the
features speciﬁcally designed for this task. Web-based applications exploiting some
of the above techniques have also appeared in [111, 247, 424]. Finally, in the relevant
literature, there are also several works, which assess the aesthetic quality of a photo
based on the features extracted from speciﬁc regions of the photo or for speciﬁc
content. For example, in [379] the authors present a set of regional features that
measure the clarity and the complexity of the subject areas and a set of global features
that capture the color and spatial composition of photos using the hue wheel and the
Hough transform.
Each of the above techniques uses low-level visual features and often some high-
level features that allow the method to take into account up to three of the basic
rules of photography. In this chapter, we present our recent developments [276] on
the use of ﬁve representative feature vectors that correspond to ﬁve basic rules of
photography, along with the pattern feature, which can effectively capture patterns
and symmetry in natural photos. We assess the photo aesthetic quality using these
ﬁve comprehensive feature vectors which can be used individually to capture the
simplicity, the colorfulness, the sharpness, the existence of a pattern, and the layout
composition of the photo. We also concatenate them into one vector, serving as
the overall aesthetic quality assessment feature in our method, and we use it in

3
Multimedia Processing Essentials
67
combination with an SVM classiﬁer to estimate whether a given photo exhibits high
aesthetic quality or not.
Concerning the extension of the problem to the video domain, only a few methods
for video aesthetic quality assessment have been proposed so far. The ﬁrst methods in
this domain tried to estimate the video aesthetic value by extracting mostly low-level
features from video frames. For instance, in [302], a set of low-level features, such
as sharpness, colorfulness, luminance,s and “blockiness” quality, and certain motion
features, are extracted. Then, an SVM using the radial basis function (RBF) kernel
is trained for assessing the aesthetic quality of videos. In [259], the authors treat the
video as a sequence of still photos to which they apply a set of visual-based features
together with two additional motion-based features, i.e., the length of subject region
motion and motion stability, so as to distinguish professional videos from amateurish
ones. They also applied a set of learning approaches, such as kernel SVM, Bayesian
classiﬁcation, and Gentle AdaBoost.
A more elaborate method that introduces a set of features ranging from low- and
mid-level attributes to high-level style descriptors, combined with a kernel SVM
learning stage, is presented in [411]. In [423], an RBF SVM is applied to a set of
“semantically independent” features, such as camera motion and stabilization, and
frame composition, along with a set of “semantically dependent” features, such as
motion direction entropy, color saturation, and lightness. The semantic dependency
of a feature refers to whether this feature relates or not to the semantic content of
each frame. Moreover, in [42], low- and high-level visual and motion features are
extracted at cell-, frame-, and shot-level and a low rank late fusion (LRLF) scheme is
used for fusing the scores produced by a set of SVMs, each of which was trained with
one speciﬁc aesthetic feature. More motion features are introduced in [426], where
the authors evaluate the effectiveness of motion space, motion direction entropy and
handshaking(i.e.,camerastabilization)onVAQassessmenttasks.Theyalsousenaive
Bayesian, SVM, and AdaBoost classiﬁcation techniques. Finally, in [311], a variety
of aesthetic-related features for video are designed, such as visual continuity and
shot length, and their performance in retrieving professional videos in conjunction
with a kernel SVM classiﬁer is examined.
The video aesthetic assessment techniques in the literature are typically evalu-
ated on different datasets, including the NHK “Where is beauty?” dataset [154], the
Telefonica dataset [426], the dataset of [311], etc. The NHK dataset consists of 1000
professionally produced video segments that last about 1 min each. The Telefonica
dataset consists of 160 short consumer videos (each being 11 to 60s long), for which
the mean opinion scores (MOS) in terms of aesthetic quality are provided. The dataset
of [311] includes 1000 professionally generated videos, and 1000 amateur videos.
Most of these datasets are not publicly available. In this chapter, we experiment with
a new dataset for the problem of video aesthetic quality assessment, which was ﬁrst
presented in [395] and is publicly available.1
1http://mklab.iti.gr/project/certh-iti-vaq700-dataset.

68
K. Apostolidis et al.
3.3.2
Method
3.3.2.1
Photo Aesthetic Quality Assessment
In photography there is a number of generally accepted rules or guidelines, which,
although cannot guarantee a perfect photo, if applied during the capturing process,
can usually enhance the aesthetic appeal of the photo. In our approach [276] we try
to describe each photo according to such a set of photographic rules. Therefore, we
select ﬁve representative photographic rules under which we extract the appropriate
image features in order to classify the photos as aesthetically pleasing or not. Below,
we brieﬂy present the rules in accordance with which we design our features.
1. Simplicity: In photography, the notion of simplicity ideally means the capturing
of a single subject on a neutral background, avoiding distractions. By adopting
this technique, the photographer highlights the subject, leading the eye directly
on the desired point. The simplicity of photo composition plays a crucial role
in aesthetic assessment and it depends on two basic rules, the color difference
between the main subject and the background, and the spatial arrangement of
the main subject. We start by making the assumption that the main subject is the
least-uniform portion of the photo, and based on this, we identify our main subject
by edge detection (using the photo gradient magnitude and orientation), followed
by the application of simple morphological operations on a binarized edge mask,
so as to result in a limited number of connected regions. Subsequently, in order
to capture color differences, we (a) compute the HSV histogram of the main
subject and background separately, and we estimate the mean value, the standard
deviation, the kurtosis, the skewness and the Kullback–Leibler divergence of both
distributions for each color component, and (b) considering two color regions of
the photo, the main subject and the background, we estimate a 5-bin histogram of
the color-difference formula, CIE 1976, which is presented in [289]. In parallel,
in order to capture the spatial arrangement of the main subject, we compute for
it the percentage of its pixels that appear in each of the nine patches of the rule
of thirds grid.
2. Colorfulness: Capturing photographs with vibrant colors and intense contrast are
among the most powerful ways to arouse viewers’ attention. There are several
photographic techniques involving the usage of color such as the complemen-
tary colors rule, the neighboring colors rule or the photo-shooting of a colorful
object on an otherwise monochromatic background, as described in [39, 293].
To evaluate the colorfulness of photos we design a comprehensive color feature
vector. At ﬁrst, we perform k-means clustering in order to detect three basic
color regions on photo. Subsequently, we exploit the RGB and HSV color spaces,
as well as an emotion-based color space consisting of activity, weight and heat
dimensions, which is presented in [371]. For each component of the three afore-
mentioned color spaces we estimate a 5-bin histogram; these histograms are then
concatenated into one color vector, capturing the color distribution on the three
corresponding regions. Then, we aim to ﬁnd which color characterizes each of

3
Multimedia Processing Essentials
69
these three regions, and for this, we use the aforementioned color vector which is
mapped onto a color description matrix of ten dominant colors. In order to deﬁne
the importance of each area we also store the percentage of pixels that each color
region occupies. For each of the nine patches of the rule of thirds grid we addi-
tionally estimate a 5-bin histogram for each color component of the HSV color
space and the Correlated Color Temperature (CCT), which is proposed in [282].
Finally, we also extract a feature to measure contrast and darkness.
3. Sharpness: This is arguably one of the most important factors that affect the
quality of photos. A clear, sharp subject automatically attracts the attention of
the eye to the right point, while a blurred subject usually has an adverse aesthetic
effect on photos. To this end, we perform the no-reference photo blur detection
scheme described in [275] so as to extract features that can describe the blurriness
of the photo, resulting in a 500-element sharpness feature vector. In addition, in
order to detect more complicated forms of blur, such as motion blur, which is
difﬁcult to be captured on the power spectrum of Fourier transform, we use the
Haar wavelet transform.
4. Pattern: The world is full of patterns, symmetry and textures. Photographers seem
to appreciate this kind of beauty that surrounds us, as patterns and symmetry
exude a sense of harmony and rhythm which makes the photographs appealing.
In order to extract the appropriate information to describe the aforementioned
characteristic of photos, we divide the photo in multiple half-photo parts. For
every pair of half-photo patches we detect SURF interest points and subsequently,
we perform keypoint matching, capturing the similarities of these patches. In
addition, our aim is to examine the presence of intense edges and if these edges
are distributed in the same manner in both half-photo patches, for each of the four
pairs. In order to achieve this, we estimate the mean value, the standard deviation
and the Kullbak–Leibler divergence after applying the Haar wavelet transform
on both half-photo patches of each pair.
5. Composition is also an important aesthetic factor, since the appropriate arrange-
ment of the objects on the photographic canvas can add balance and harmony,
while a poor composition can distract the viewer’s eye out of the intended point of
interest. In the presented approach we examine three composition rules: the “ﬁll
the frame” (Fig.3.5a), the rule of thirds (Fig.3.5b) and the “landscape” composi-
tion (Fig.3.5c), as presented in [171, 220]. To achieve this, we use the objectness
measure, presented in [9], in order to detect eight objects on each photo. Then,
our aim is to estimate the relative position of each object with respect to the center
of each of the shaded photo patches, that is marked with a cross in Fig.3.5. For
assessing the “ﬁll the frame” composition, we compute the euclidean distance
and the angle of the object’s center with respect to the center of the shaded patch
depicted in Fig.3.5a, as well as the spatial overlap between the shaded patch
and the detected object’s area (as an example, the latter area is marked with a
dashed line in Fig.3.5a). For the rule of thirds and the “landscape” composition
we similarly estimate the euclidean distance, the angle and the spatial overlap of
the object with respect to the shaded patches presented in Fig.3.5b, c. For the

70
K. Apostolidis et al.
Fig. 3.5 Composition rules: a “ﬁll the frame”, b rule of thirds, c “landscape”
“landscape” composition, we additionally estimate the color distribution on the
three patches of the “landscape” grid (Fig.3.5c), based on the HSV color format.
Having computed the aforementioned feature vectors, we concatenate them in
one ﬁnal, comprehensive, 1323-dimensional vector. The overall procedure of the
presented aesthetic quality assessment method is presented in Fig.3.6. The code for
our method is publicly available.2
3.3.2.2
Video Aesthetic Quality Assessment
For the problem of video aesthetic quality assessment, since video is typically treated
as a sequence of still photos that gives the impression of motion, both the visual and
motion modalities need to be exploited in order to effectively evaluate its aesthetic
quality. That is, each video is described according to a set of rules borrowed from
photography (as in the case of photo aesthetic quality assessment discussed above)
and cinematography. Initially, each video is divided into its shots using the shot
detection method of [16]. Then, for each video, we estimate the mean duration of its
shots, and, considering that the shot transitions can be either abrupt or gradual, we
estimate for each of these transition types their duration as a percentage of the whole
video’s duration. This procedure results in a 3-element video-level vector.
Subsequently, one keyframe per second is extracted from the original raw video
sequence (irrespective of shot boundaries), and photo- and motion-based features are
extracted for each one of them. Photo-based features include the simplicity, color-
fulness, sharpness, pattern, and overall aesthetic quality values, which are extracted
based on the still-photo aesthetic quality assessment method proposed in [276] and
is discussed in detail above. Motion-based features, adopted from [426], include:
(a) a measure of similarity between successive frames (cross-correlation between
these frames), (b) a measure of the diversity of motion directions (motion direction
entropy), (c) a measure of the stability of the camera during the capturing process
(handshaking), and (d) a measure which can distinguish the difference between three
categories of shots: focused shots, panorama shots, and static shots (shooting type).
The above result in a 44-element keyframe-level feature vector. Concatenating with
it the video-level feature vector, we end up with a 47-element vector as the ﬁnal
representation for each keyframe.
2The CERTH photo aesthetic method, http://mklab.iti.gr/project/IAQ.

3
Multimedia Processing Essentials
71
Fig. 3.6 Overview of the presented method
A challenge in the video aesthetic quality assessment problem, similarly to many
video classiﬁcation tasks, is that video representation techniques usually introduce
uncertainty in the input that is subsequently fed to the classiﬁers. Thus, uncertainty
needs to be taken into consideration during classiﬁer training. The kernel SVM with
Isotropic Gaussian Sample Uncertainty (KSVM-iGSU), proposed in [397], is an
extension of the standard kernel SVM that exploits the uncertainty of input data in
order to achieve better classiﬁcation results. The overall method of the presented
video aesthetic quality assessment method, originally proposed in [395], is shown in
Fig.3.7.
The uncertainty of the ith input example is modeled as an isotropic Gaussian
distribution with given mean vector xi ∈Rn and an isotropic covariance matrix, i.e.,
a scalar multiple of the identity matrix, σ 2
i In ∈Sn
++, where n denotes the dimen-
sionality of the input feature space, Sn
++ denotes the convex cone of all symmetric
positive deﬁnite n × n matrices with entries in R, and In denotes the identity matrix
of order n. The optimization problem of KSVM-iGSU can be cast as a variational

72
K. Apostolidis et al.
Fig. 3.7 Video aesthetic quality assessment approach of [395]
calculus problem of ﬁnding the function f that minimizes the functional [ f ], i.e.,
min f ∈H [ f ], where the functional [ f ] is given by
[ f ] = 1
2λ∥f ∥2
H + 1
ℓ
ℓ

i=1
⎡
⎣yi −f (xi) −b
2
⎛
⎝erf
⎛
⎝yi −f (xi) −b

2σ 2
i ∥f ∥2
H
⎞
⎠+ yi
⎞
⎠
+

σ 2
i ∥f ∥2
H
√
2π
exp

(yi −f (xi) −b)2
2σ 2
i ∥f ∥2
H
 ⎤
⎦,
(3.7)
where λ is a regularization parameter and f belongs to a Reproducing Kernel Hilbert
Space (RKHS), H, with associated kernel function k. Using a generalized semi-
parametric version [356] of the representer theorem [204], it can be shown that the
minimizer of the above functional admits a solution of the form
f (x) =
ℓ

i=1
αik(x, xi) −b,
(3.8)
where b ∈R, αi ∈R, i = 1, . . . , ℓ.
WedeﬁnethekernelmatrixK asthesymmetricpositivedeﬁniteℓ× ℓmatrixgiven
as K =
!
k(xi, x j)
"ℓ
i, j=1. If we set α = (α1, . . . , αℓ)⊤and let ki denote the ith column
of the kernel matrix K, the objective function of KSVM-iGSU, JH : Rℓ× R →R,
can be rewritten as follows
JH(α, b) = 1
2λα⊤Kα + 1
ℓ
ℓ

i=1
⎡
⎣yi −k⊤
i α −b
2
⎛
⎝erf
⎛
⎝yi −k⊤
i α −b

2σ 2
i α⊤Kα
⎞
⎠+ yi
⎞
⎠
+

σ 2
i α⊤Kα
√
2π
exp

−(yi −k⊤
i α −b)2
2σ 2
i α⊤Kα
 ⎤
⎦,
(3.9)
where the sum above expresses the total loss. We (jointly) minimize the above
convex objective function with respect to α, b using the Limited-memory BFGS
(L-BFGS) algorithm [243]. L-BFGS is a quasi-Newton optimization algorithm that
approximates the Broyden–Fletcher–Goldfarb–Shanno (BFGS) [56] algorithm using
a limited amount of computer memory. Since JH is a convex function on Rℓ× R

3
Multimedia Processing Essentials
73
(see [396, 397]), L-BFGS leads to a global optimal solution; that is, at a pair (α, b)
such that the decision function given in the form of (3.8) minimizes the functional
(3.7).
3.3.3
Experimental Results
3.3.3.1
Photo Aesthetic Quality Assessment Experimental Results
Most of the photo aesthetic assessment techniques evaluate the performance of their
features on publicly available datasets which consist of photos acquired from online
photography communities. There are hundreds of amateur and professional photos
uploaded and rated by the community members. These photos are appropriate for
aesthetic assessment tasks due to their content and the diversity of the ratings sub-
mitted by both amateurs and professionals. The former tent to judge the photos based
on the emotion while the latter pay more attention to technical details.
To
evaluate
the
presented
approach
we
use
three
different
datasets,
CUHKPQ [379], CUHK [379] and AVA [306]. The CUHKPQ dataset is divided
into seven thematic categories, and each photo has been classiﬁed by members of
the photography community as a high-aesthetic or low-aesthetic quality photo. Sim-
ilarly to the methodology of [246], we randomly partitioned ten times the photos of
each category, assigning half of them as training, and the rest as testing set. After-
wards, we train a single SVM classiﬁer for each individual category and the average
results are reported. In addition, the contributors of [379] collected 12,000 photos
from all categories (this subset is denoted as the CUHK dataset), on which we also
perform experiments following the same experimental design. On the other side, the
AVA dataset contains photos derived from a photo contest community3 which are
associated with their mean aesthetic scores. This dataset has already been divided in
training and testing set by the authors of [306], so for both of these sets, following
the experimental design of [200], we choose the 10% and 20% top ranked photos as
high-aesthetic ones and the 10% and 20% bottom ranked as low-aesthetic photos,
respectively. In this way, the photos with score in the middle of the score range are
excluded as in [200], since they could introduce noise in the classiﬁcation process.
In all datasets, the performance of our aesthetic assessment approach is evaluated
calculating the classiﬁcation accuracy and the Area Under the ROC Curve (AUC)
measures [158].
At ﬁrst, we test the performance of the presented method on each of the seven
categories of the CUHKPQ dataset [379] and we compare our features with pre-
viously presented methods. Moreover, in order to demonstrate that our results do
not depend a lot on the photo subject, we further evaluate our method over all
categories. For comparison we use the aesthetic assessment method proposed by Lo
et al. in [246], for which the corresponding code has been released, the region-based
3www.dpchallenge.com.

74
K. Apostolidis et al.
and global aesthetic features proposed by Tang et al. in [379], the subject-oriented
method proposed by Luo et al. in [259] and the high-level attributes proposed by
Ke et al. in [200]. The corresponding results of the last three methods are taken
from [379]. The performance evaluation using the AUC measure is shown in
Table 3.4. As can be seen, our method achieves the best results, both for each of
the different subject categories and overall, followed by the algorithm of Tang et al.
and in some categories by the method of Lo et al. Furthermore, our individual fea-
tures present promising performance in comparison to similar features of the other
methods.
We continue with experiments on the CUHK dataset [379], consisting of 6000
high-aesthetic and 6000 low-aesthetic photos. In this case, we compare with the
related methods [110, 200, 246, 264, 315], whose corresponding results were pre-
sented in [264]. The feature dimensionality and the accuracy of these methods are
presented in Table 3.5, where we can see that our method outperforms most of the
others. The generic photo features with Fisher Vector encoding and spatial pyrami-
dal decomposition of [264] provide the highest accuracy, but these features exhibit
extremely high dimensionality and, consequently, also excessive computational com-
plexity, when it comes to using them for training an SVM classiﬁer. Also, all results
of [264] are obtained using a more elaborate SVM with a hinge loss using the primal
formulation and a stochastic gradient descent algorithm [50], instead of the simpler
linear SVM that is used in all other methods and our own method. At this point,
we also implemented the most efﬁcient generic descriptor of [264], the FV-Color-
SP, and tested it with a linear SVM, to demonstrate that the combination of our
photographic-rule-based features with generic descriptors can boost the overall per-
formance. For the FV-Color-SP feature vector, we employ dimensionality reduction
to 4000 elements, for computational efﬁciency purposes. It is clear from the exper-
imental results shown in Table 3.5 that our features can indeed improve the results
obtained using generic low-level features, when combined with the latter.
Finally, the performance of our method in comparison with the algorithm pro-
vided by Lo et al. on the “overall” category of the AVA [306] dataset is presented
in Table 3.6. The corresponding results demonstrate that our method achieves good
performance on another challenging dataset and that its results are consistently better
than the results of [246]. In addition, in this dataset, which includes mostly profes-
sional photos, we can see that the pattern feature is one of the most effective individual
features.
3.3.3.2
Video Aesthetic Quality Assessment Experimental Results
For the problem of video aesthetic quality assessment, we experiment with the pub-
licly available4 dataset presented in [395], i.e., the CERTH-ITI-VAQ700 dataset.
This dataset includes 350 videos that are rated as being of high-aesthetic quality and
another 350 as being of low aesthetic quality. Indicative frames of such videos are
4http://mklab.iti.gr/project/certh-iti-vaq700-dataset.

3
Multimedia Processing Essentials
75
Table 3.4 Experimental results (AUC %) on the CUHKPQ dataset
Features
Animal
Architecture Human
Landscape
Night
Plant
Static
Overall
Lo et al. [246]
Concatenated
91.60
84.14
92.09
90.65
86.33
93.30
89.72
89.74
Tang et al. [379]
Dark channel
83.93
88.69
79.87
85.75
70.62
78.58
83.35
81.89
Complexity
combined
82.12
72.19
78.15
75.16
72.84
89.72
74.91
78.17
Hue composition
78.61
83.76
79.09
89.36
72.14
83.16
83.67
81.65
Scene composition
70.03
67.81
79.23
69.79
74.77
59.66
70.57
70.56
Concatenated
87.12
90.04
96.31
92.73
83.09
91.47
88.90
90.44
Luo et al. [259]
Clarity contrast
80.74
53.48
66.67
53.79
62.97
74.39
73.09
67.38
Lighting
75.51
64.60
76.12
62.26
53.11
77.52
74.30
70.32
Geometry
composition
74.25
58.06
68.28
43.93
60.75
73.08
59.20
63.93
Simplicity
64.78
55.82
77.52
74.54
69.18
74.50
78.49
68.65
Color combination
80.52
71.94
65.13
72.80
58.73
78.46
75.13
72.44
Concatenated
87.12
90.04
96.31
92.73
83.09
91.47
88.90
77.92
Ke et al. [200]
Blur
75.66
79.81
73.81
77.85
66.65
79.63
76.62
75.92
Brightness
69.93
81.38
78.01
78.48
72.44
73.37
69.76
74.64
Hue count
62.60
70.82
70.27
59.64
55.37
69.20
55.11
63.53
Concatenated
77.51
85.26
79.08
81.70
73.21
80.93
78.29
79.44
Proposed method
Simplicity
85.47
81.55
89.24
87.87
78.39
89.61
83.23
82.06
Colorfulness
82.15
86.61
91.12
91.82
79.47
90.84
86.05
84.07
Sharpness
94.51
87.21
96.15
86.74
90.42
92.48
90.66
90.85
Pattern
89.66
83.12
86.14
85.62
89.54
89.50
81.84
84.34
Composition
88.31
88.59
88.45
92.10
89.32
90.63
87.78
86.10
Concatenated
95.42
92.08
97.31
94.10
94.26
96.03
94.07
94.60

76
K. Apostolidis et al.
Table 3.5 Classiﬁcation accuracy (%) on the CUHK dataset
Feature dimensionality
Accuracy
Datta et al. [110]
56
75.85
Ke et al. [200]
7
76.53
Lo et al. [246]
24
77.25
GIST [315]
512
67.96
BOV-SIFT-SP [264]
8192
81.36
BOV-Color-SP [264]
8192
82.13
FV-Color-SP [264]
2,62,144
89.90
Our features
1323
82.41
FV-Color-SP
4000
78.16
FV-Color-SP+Our
5323
85.02
Table 3.6 Overall classiﬁcation accuracy on the AVA dataset
Features
Accuracy (%) at 10%
Accuracy (%) at 20%
Lo et al. [246]
Concatenated
66.60
62.14
Proposed method
Simplicity
63.61
60.05
Colorfulness
65.03
63.48
Sharpness
75.94
72.42
Pattern
76.92
71.80
Composition
72.64
69.83
Concatenated
77.08
74.35
shown in Fig.3.8. We ﬁrst randomly split the dataset into a training subset (50%) and
an evaluation subset (50%), each maintaining a positive–negative ratio of 1:1. That
is, each of the training and evaluation subsets includes 175 positive (high aesthetic)
and 175 negative (low aesthetic) video examples. As discussed above, for video
representation, 1 keyframe per second was extracted at regular time intervals from
each video, and each keyframe was represented using our photo- and motion-based
features.
The aforementioned keyframe-level video representations can be seen as obser-
vations of the input Gaussian distributions that describe the training videos. That is,
let X be a set of ℓannotated random vectors representing the video-level feature
vectors. We assume that each random vector is distributed normally; i.e., for the
random vector representing the ith video, Xi, we have Xi ∼N (xi, 
i). Also, for
each random vector Xi, a number, Ni, of observations, {x(t)
i
∈Rn : t = 1, . . . , Ni}
are available; these are the keyframe-level feature vectors that have been computed.
Then, the mean vector and the covariance matrix of Xi are computed respectively as
follows
xi =
Ni
t=1 x(t)
i
Ni
,
i =
Ni
t=1(x(t)
i
−xi)(x(t)
i
−xi)⊤
Ni −1
(3.10)

3
Multimedia Processing Essentials
77
Fig. 3.8 Indicative examples of videos of high (a, b, c) and low (d, e, f) aesthetic value, available
in the CERTH-ITI-VAQ700 dataset
Now, due to the assumption for isotropic covariance matrices, we approximate the
above covariance matrices as multiples of the identity matrix, i.e., #
i = σ 2
i In. As
discussed in [395], it sufﬁces to set σ 2
i equal to the mean value of the elements of
the main diagonal of i.
Since the problem of video aesthetic quality assessment can be naturally seen as a
retrieval application, where a user queries for videos of high-aesthetic quality within
a dataset, for assessing the performance of our method we use retrieval-oriented
evaluation measures: the average precision (AP) [340], as well as the precision at
depth n (where n ∈{5, 10, 15, 20}) and the accuracy, which are measures that are
typically used in the VAQ assessment literature [259, 276, 311, 423, 426].
The presented KSVM-iGSU-based method [395] is tested and compared to the
standard kernel SVM (KSVM). KSVM is the state-of-the-art classiﬁer for the prob-
lem of VAQ assessment [276, 302, 311, 423, 426]. For both KSVM-iGSU and
KSVM, the radial basis function (RBF) kernel was used. Training parameters C,
γ were obtained via a 3-fold cross-validation procedure (grid search) with C being
searchedintherange{2−4, 2−3, . . . , 26, 27}andγ intherange{2−7, 2−6, . . . , 23, 24}.
Each of the above experiments was repeated 10 times using different random train-
ing/evaluation subsets, similarly to [276].
Table 3.7 shows the average performance of KSVM-iGSU compared to the
standard KSVM in terms of precision for the top-n retrieved videos, where n =

78
K. Apostolidis et al.
Table 3.7 Performance of our method (using KSVM-iGSU) compared to the standard KSVM
in terms of precision at top-n retrieved videos (n = 5, 10, 15, 20), accuracy (AC), and average
precision (AP) using CERTH-ITI-VAQ700 dataset
KSVM (as in [311, 423, 426])
KSVM-iGSU (Proposed)
P@5
64.00
82.00
P@10
63.00
82.00
P@15
61.33
83.33
P@20
61.50
81.50
AC
67.87
68.14
AP
61.67
69.97
Fig. 3.9 Precision-recall
curves for our VAQ
assessment method
(KSVM-iGSU) compared to
the state-of-the-art KSVM
approach using
CERTH-ITI-VAQ700 dataset
0
20
40
60
80
100
50
60
70
80
90
Recall (%)
Precision (%)
CERTH−ITI−VAQ700
KSVM−iGSU
KSVM
5, 10, 15, 20. We see that our VAQ assessment method leads to considerably better
results in terms of retrieval precision. Furthermore, the mean values of accuracy (AC)
and average precision (AP) are also reported for 10 repetitions of the experiment. We
see that, in terms of accuracy, our method slightly outperforms the state-of-the-art
KSVM (68.14 over 67.87, respectively), but in terms of average precision, which is a
more meaningful measure for retrieval tasks, our method reaches 69.97 as compared
to KSVM’s 61.67, leading to a 13.45% relative boost. Finally, Fig.3.9 shows the
recall-precision curves of our method (KSVM-iGSU) and standard KSVM.
3.4
Near-Duplicate Detection
Near-duplicates are deﬁned as sets of media items which show the same event or
object of interest (e.g., a monument) under slightly different conditions (e.g., illu-
mination, camera parameters) or viewpoints (e.g., view angle, partial obstruction of
object of interest). Two media items can also be deﬁned as near-duplicates if one
of them was created by postprocessing the other. A wide range of postprocessing
operations were studied in the relevant literature (indicatively see [420]), such as
storage of photos using a different compression format or with different encoding

3
Multimedia Processing Essentials
79
parameters, application of photometric variations (color and lighting changes) or
other editing operations (e.g., caption, logo, and border insertion, or spatial cropping
so as to remove such distinguishing marks). For a representative set of near-duplicate
item examples, see Fig.3.10.
In the literature, methods that deal with the identiﬁcation of near-duplicates in
mediacollections arecategorizedtonear-duplicateretrieval andnear-duplicatedetec-
tion [244]. Near-duplicate retrieval refers to the situation, where a collection is estab-
lished, the user inputs a query and the system matches the query with items in the
collection,returningarankedlist,withdescendingorderbysimilarity.Near-duplicate
detection aims at identifying any near-duplicates within a given collection (not driven
by a query).
Near-duplicate retrieval and detection present certain challenges; (a) Different
captureconditions canresult inphotos of substantiallydifferent visual content despite
depicting the exact same object of interest. Human vision is able to quickly identify
a pair of near-duplicate photos, but it is not a trivial task for a single computer vision
method to compensate for all possible variations and distortions that may appear in
near-duplicate photos. (b) It is difﬁcult to make reliable assumptions about which
semantically similar photos should be considered as near-duplicates [406, 432].
(c) Most real-world applications demand detection in huge volumes of data; for a
solution to be useful, it must be both fast and accurate.
3.4.1
Related Work
Most commonly, a photo near-duplicate retrieval or detection framework represents
photos by extracting feature vectors and creates an index on these vectors to allow
faster access. The near-duplicates of a photo are then retrieved by assessing the visual
similarity between the query photo and the stored data with a help of a nearest neigh-
bor search applied on the indexed descriptor vectors. The visual features that are
usually extracted can be categorized in three groups: (a) global handcrafted features,
(b) local handcrafted features and (c) features extracted with the use of DCNNs. Most
near-duplicate detection approaches (such as [230]) employ sets of local descriptors
(e.g., SIFT, SURF) or binary local descriptors (e.g., ORB and BRISK) which can
produce a ﬁne-grained but also a quite complex representation of the visual content.
In order to create a coarser representation (ideally one that uses a single vector with
relatively few elements) of the photo, an encoding method, such as Bag-of-Words
[101], Fischer Vectors [323] or the most recent VLAD [180], must be applied to the
aforementioned local descriptors. These techniques quantize the space of descriptor
vectors and assign them to one or several words of a visual vocabulary that typi-
cally contains thousands of elements. Such methods are computationally demanding
(feature extraction, encoding, matching of high-dimensional feature vectors), and the
resulting representation vectors for each photo are quite large. This size is prohibitive
for large-scale search applications due to memory and search efﬁciency constraints
[372]. To address these issues, hashing techniques (e.g., Locality-Sensitive Hash-

80
K. Apostolidis et al.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 3.10 Indicative examples of near-duplicates: a, b a pair of near-duplicate photos with pho-
tometric variation due to camera ﬂash use (from the California-ND dataset), c, d: a pair of near-
duplicate photos depicting the same object from a different viewpoint (from the Holidays dataset),
e, f a partial near-duplicate pair of photos (from the Oxford Buildings dataset), and g, h a pair
of near-duplicate photos resulting from the use of extreme JPEG compression settings (from the
Copydays dataset)

3
Multimedia Processing Essentials
81
ing [15], Spectral Hashing [414]) which improve the speed of search, are usually
employed [120, 422]. Dimensionality reduction methods may also be utilized for
shortening the length of the descriptor vectors in a near-duplicate retrieval or detec-
tion framework (such as the methods described in [178, 401]). Yet, these methods
add an extra stage in the whole process and favor either speed or memory efﬁciency,
but rarely both.
Especially in scenarios where time and space efﬁciency is a key factor, relying on
high-dimensional feature vectors renders the use of such methods prohibitive. There
are approaches found in the literature which are based on the global descriptors (e.g.,
color and texture), which directly extract a single ﬁxed-size vector to represent the
visual content of the whole photo under consideration. Such methods are compu-
tationally less expensive compared to the local descriptor-based ones, due to the
faster feature extraction and the omission of constructing a visual vocabulary, but
offer a coarser description of photos and therefore are not robust to certain photo-
transformations (e.g., afﬁne transformations, as discussed in [134]). Differently to
the approaches discussed above, and driven by the success of DCNNs in large-
scale image classiﬁcation and concept detection tasks (as discussed in Sect.3.2.2.1),
DCNN-based visual representations are also used in image retrieval tasks, exhibiting
state-of-the-art performance [309] on the standard benchmark datasets.
In the following, three different approaches to near-duplicate detection are pre-
sented; the ﬁrst one using a fast global descriptor to assess the visual similarity of
photos,whichisdestinedforfastnear-duplicateretrievalanddetectioninlargecollec-
tions of media items but only considering a speciﬁc set of photo alterations; a second
one using local descriptors, which is robust to a wider variety of photo alterations
and suitable for partial near-duplicate detection; and ﬁnally, a third one that uses
DCNNs as feature generators (as discussed in Sect.3.2), aiming to come up with a
higher level description of the photos’ content. Most related to the global descriptor-
based algorithm, in terms of feature complexity, are the approaches described in
[174, 226], which represent visual content with the help of global descriptors (e.g.,
color histograms, color moments, GIST). The local descriptor-based approach is
similar to [372], while the presented DCNN-based approach is based on the method
of [309]. All of the three presented approaches function in a ofﬂine/online fashion.
That is, the main part of the methods (the ofﬂine processing of the media collection,
which is agnostic of the queries that may be submitted) aims at indexing the collec-
tions’ items and detecting the prior existence of any near-duplicates among them.
The processing in ofﬂine mode is carried out only once and always precedes that
of the online mode. During the online mode, any query to the preindexed collection
can be answered swiftly utilizing the index constructed during the ofﬂine process-
ing. Thus, the discussed approaches can serve both in near-duplicate retrieval and
near-duplicate detection scenarios, i.e., the ofﬂine mode sufﬁces for near-duplicate
detection, while the online mode can answer near-duplicate retrieval queries.
Looking beyond still photos, a common framework of methods for near-duplicate
retrieval of videos can be summarized in four stages [244]: (a) extracting features
from videos, (b) constructing a compact and robust signature from the features for
each video, (c) constructing an index on the signatures for fast similarity search and,

82
K. Apostolidis et al.
(d) comparing the signatures to retrieve near-duplicate videos. In [436], the features
extracted are categorized in global features (color histograms, ordinal signatures),
and local features (keypoint-based approaches). More recently, the authors of [244]
used a ﬁner categorization of features, namely video-level global signatures, frame-
level global signatures, frame-level local signatures, spatiotemporal signature, and
visual-temporal network signatures. Generally, global signatures are fast to compute
and compact but they lack robustness to geometric and photometric transformations
(similar to the case of photos). On the other hand, local signatures are computation-
ally costly, especially frame-level local signatures, where descriptor keypoints are
matched and their geometric information is also used to verify the scene similarity.
Such signatures are usually extracted only on representative frames of shots, or the
computations take place last in a coarse-to-ﬁne framework.
3.4.2
Method
Aiming to cover several diverse cases where near-duplicate detection methods are
applied, and as discussed in the previous section, we describe in this section three
approaches: (a) a global descriptor-based approach, which is very light computation-
ally wise. This approach is suitable for very large collections of photos, but can only
detect a speciﬁc set of photo alterations; (b) a local descriptor-based approach, that
can detect more complex near-duplicates (e.g., partial duplicates) at the expense of
being more computationally and memory expensive; (c) a DCNN-based approach,
that assesses the content in a high-level manner, being able to discover semantically
similar photos that may have similar but also somewhat different visual content.
3.4.2.1
Global Descriptor-Based Approach
For the global descriptor-based near-duplicate photo retrieval and detection (denoted
DCT-NDD in the sequel) we consider a set of photo alterations that include: (a)
changes in illumination, (b) noise addition, (c) differences in JPEG compression,
(d) scaling operations, (e) overlay of text labels, and (f) moderate cropping. Our
aim is the development of a method that ensures robustness to the aforementioned
photo alterations while being efﬁcient in terms of processing space and time, thus
enabling the swift analysis of large content collections. This method utilizes the
well-known Discrete Cosine Transform (DCT), which has been used and exhibited
good performance in a variety of different computer vision tasks, such as ﬁngerprint
matching [364] and digital watermarking [203].
The processing of the media collection during the ofﬂine mode starts with extract-
ing the feature representation of the photos. The following process is performed on
each photo of the collection. The algorithm initially resizes it to m × m pixels and
represents it as a sum of cosine functions oscillating at different frequencies via
a two-dimensional DCT. The outcome of this process is an m × m matrix where

3
Multimedia Processing Essentials
83
the top-left element corresponds to the DC (zero-frequency) coefﬁcient and every
other element moving from left to right and from top to bottom corresponds to an
increase in the horizontal and vertical frequency by a half cycle, respectively. Then,
we keep only the top-left r × r area (r < m) of the computed matrix, discarding
high-frequency coefﬁcients and thus removing information that contributes to the
details of the photo. Consequently, the extracted r × r submatrix is reshaped and the
DC-coefﬁcient is removed in order to form a vector with length r2 −1. Finally, the
dimensionality of the resulting vector is reduced from r2 −1 to d using Principal
Component Analysis, so at the end of this process each photo of the collection is
represented by a d-element feature vector.
For initially detecting the existence of near-duplicates among the items of the
collection (i.e., during the ofﬂine mode and still prior to the receipt and processing of
any query photo), the computed feature vectors are stored in a matrix with dimensions
n × d, denoted X in the sequel (where the jth row of this matrix contains the feature
vector of the jth item). For the rest of this subsection we apply the following rule
for math notations: a superscript to a matrix symbol denotes the speciﬁc row of this
matrix (e.g., Xj denotes the jth row of matrix X), while a subscript to a matrix symbol
denotes the speciﬁc column of this matrix (e.g., Xi denotes the ith column of matrix
X). Consequently, X j
i denotes the item of the X matrix in the jth row and the ith
column. Matrices and vectors symbols are in bold font. The similarity of each pair of
media items of the collection is stored in a matrix with dimensions n × n, denoted S.
The element S j
l of this matrix captures the similarity between the jth and lth media
items and is computed as follows:
S j
l =
X j ⊤Xl
∥X j∥∥Xl∥
(3.11)
By introducing an L2-normalization of all feature vectors prior to the above calcu-
lations (so that ∥X j∥= 1, ∀j ∈1 . . . n), Eq. (3.11) becomes:
S j
l = X j ⊤Xl =
d

i=1
X j
i · Xl
i
(3.12)
In this way, the cosine similarity is transformed to the dot product of the given vectors.
Despitethiscomputationbeingveryfast,anexhaustivenearestneighborsearchonthe
available data (i.e., computing s j
l ∀j,l ∈1 . . . n) would be highly time-demanding,
therefore, we perform an accelerated search via pruning the number of pairwise
similarities that need to be assessed. In particular, during the media collection’s
indexing process:
• We compute and store in a row-vector R ∈Rn the maximum value among all
elements of each photo feature vector (i.e., R j = imax(X j
i )).
• We compute and store in a row-vector R′ ∈{1 . . . n}n the order indicators for the
elements of R if the corresponding R j values were sorted in descending order

84
K. Apostolidis et al.
(i.e., R′
j = 1 shows that R j is the largest value in R, R′
l = 2 shows that Rl is the
second-largest value in R, etc.).
• We compute and store in a row-vector N ∈{1 . . . d}n the number of positive ele-
ments contained in each feature vector.
• We compute and store in a row-vector C ∈Rd the maximum value of each DCT-
coefﬁcient for all feature vectors, and in a row-vector C′ ∈{1 . . . d}d the order indi-
cators for the elements of C if the corresponding values were sorted in descending
order.
The pruning of similarity calculations is performed based on the following rules:
• By utilizing the dot product upper-bound equation from [11],
S j
l = X j ⊤Xl ≤min(N j, Nl) · R j · Rl
(3.13)
we calculate the dot product of two feature vectors, only if the following Eq. (3.14)
is satisﬁed, with t being a predeﬁned threshold value:
min(N j, Nl) · R j · Rl > t
(3.14)
• For a given value of j, the order in which Eq. (3.14) is evaluated for the different
Xl feature vectors is the one speciﬁed by R′ (i.e., starting from the photos with the
highest Rl values). Once we ﬁnd a value of l for which Eq. (3.14) is not satisﬁed,
we stop the procedure and omit the similarity estimation for the rest of the media
items.
• During the calculation of the dot product between the DCT coefﬁcients of X j and
Xl feature vectors, the order in which DCT coefﬁcients are selected to be multiplied
and accumulated is speciﬁed by C′. Consequently, we check if the accumulated
product of the ﬁrst q DCT coefﬁcients (with q < d) is above a predeﬁned threshold
t′, as:
q

i=1
X j
i · Xl
i > t′
(3.15)
If Eq. (3.15) is not satisﬁed, we do not apply any further similarity calculations
for feature vectors X j and Xl and we set S j
l = 0.
Finally, we determine the items that satisfy the condition X j · Xl > t and keep only
the top-k of them for constructing a sparse similarity matrix.
The formed sparse similarity matrix S is treated as the weight matrix of a graph
representing the similarities between all media items in the collection, and is used
for detecting the strongly connected components (SCCs) of this graph using the
established Tarjan method [380]. Each SCC is a subgraph in which every node is
connected to every other node in the subgraph, i.e., a group of near-duplicates. The
ﬁnal output of this ofﬂine processing of the media collection is a graph such as the
example one shown on the left side of Fig.3.11.
Having completed the ofﬂine processing of the given collection, we can now efﬁ-
ciently respond to any request for detecting if this collection contains near-duplicates

3
Multimedia Processing Essentials
85
Fig. 3.11 Time-efﬁcient
near-duplicate detection
based on the strongly
connected components
(SCCs) of the similarity
graph. SCCs are denoted
here with dashed ellipses
of any new (previously unseen) query photo during the online mode. Given the query
photo, the described feature extraction process is performed. The most similar item
in the collection for the given photo is detected by computing the cosine similarity of
the latter to all the items in the collection; the set of rules for pruning the number of
required similarity calculations that was introduced above is again used for speeding
up the process. Finally, all items of the collection that belong to the same SCC as
the detected nearest neighbor (based on the previously applied SCC detection), if
any, are also retrieved as near-duplicates of the query photo. The latter is illustrated
in Fig.3.11, where the query photo is matched with media item #3, but media items
#1 and #2 are also known to be near-duplicates of the query photo since they are
included in the same SCC of the constructed graph. Thus, media items #1, #2 and
#3 are all returned as detected near-duplicates of the query photo in this example.
This approach restricts the number of pairwise similarities that need to be assessed
in both the ofﬂine and online modes, and reduces the space complexity of the ofﬂine
procedure.
3.4.2.2
Local Descriptor-Based Approach
In this section, we present the local descriptor-based approach for near-duplicate
retrieval and detection (denoted SIFT-NDD in the sequel). Due to the heavy compu-
tational complexity of local descriptors and/or keypoint matching for each possible
pair of photos in a collection, we follow a coarse to ﬁne scheme, where candidate
near-duplicate photos are ﬁrst selected and then further reﬁned in two stages: (a)
the descriptor matching reﬁnement stage and (b) the keypoints matching reﬁnement
stage.
The pipeline of the SIFT-NDD approach is as follows: SIFT keypoints [255] are
detected in each photo of the collection. The detected keypoints are sorted based on
theirresponsemetricandtherespectiveSIFTdescriptorsarecomputedjustonthetop-
n selected keypoints. All descriptors of the collection are clustered to k clusters, using
the k-means method. The k centers of the clusters form a visual vocabulary, which in
turn enables the descriptor-based representation of each photo using VLAD encoding
[180]. A component-wise mass normalization and square-rooting is performed on

86
K. Apostolidis et al.
the VLAD vectors to address the problem of “burstiness” (as discussed in [19]). A
randomized KD forest index is constructed on VLAD vectors so as to compute the
k-nearest neighbors of each photo VLAD vector, which are considered as potential
near-duplicates of each photos.
In the ﬁrst reﬁnement stage, for each potential near-duplicate pair we proceed to
match the SIFT descriptors by checking for each descriptor if the ratio of distance
from the closest neighbor to the distance of the second closest is greater than 0.7 (as
suggested by Lowe in [255]). We discard the near-duplicate pairs for which the ratio
of matched descriptors to the total extracted descriptors is lower than an empirically
set threshold. During the second reﬁnement stage, the remaining near-duplicate can-
didates of the ﬁrst stage are further trimmed by checking the geometric consistency
of SIFT keypoints using the Geometric Coding method of [439]. Optionally, we
can further check (a) the total photo area to matched area ratio, to exclude partial
duplicates, (b) the query matched area to neighbor matched area ratio, to exclude
scaled duplicates. Finally, we proceed to construct the graph of similarities between
all media items in the collection, as discussed in Sect.3.4.2.2, to conclude the ofﬂine
mode processing and be able to respond to queries during the online mode.
3.4.2.3
DCNN-Based Descriptor Approach
Deep convolutional neural networks (DCNNs) exhibit hierarchical learning, a natu-
ral progression from low-level to high-level structure [223]: the most-bottom layers
detect the presence of characteristics such as edges at particular location and orien-
tation, the middle layers detect motifs formed by particular arrangements of edges,
and the most-top layers detect high-order combinations that have a certain semantic
meaning (e.g., complex motifs of edges that constitute a face).
To identify similar photos in a collection using a DCNN-based approach (denoted
DCNN-NDD in the sequel), we adopt the method of [309]. Speciﬁcally, we selected
the GoogLeNet pretrained ImageNet network [377] that was also used as a source
DCNN in Sect.3.2 for concept-based annotation. The structure of the network is illus-
trated in Fig.3.2 (excluding the extension layers), i.e., GoogLeNet is a 22-layer net-
work consisting of nine inception layers that are connected sequentially. GoogLeNet
takes as input a 224 × 224 photo which is then forward propagated through the net-
work. In order to perform near-duplicate detection we select a speciﬁc GoogLeNet
convolutional layer and use its responses (which are L2-normalized) to describe the
different input image regions. This results in a collection for all regions of all pho-
tos, and all ﬁlters of the chosen layer. Then, we perform k-means clustering in this
collection in order to obtain a vocabulary of k words. Then, each ﬁlter response
vector is assigned to its nearest visual word, enabling the subsequent encoding of all
the ﬁlter responses from all regions into one feature vector using VLAD encoding
[180]. Finally, the VLAD descriptors are normalized by intra-normalization [19].
Taking into consideration the evaluation carried out in [309], we choose to extract
the features from the “inception3a/output” layer as this layer scored the best results in
various datasets. Furthermore, differently from [309] we choose to construct a visual

3
Multimedia Processing Essentials
87
vocabulary with k = 256 words, and reduce the dimensionality of the convolutional
responses features from 256 to 192 using PCA prior to VLAD encoding, to reduce
the size of the resulting VLAD vectors. Finally, we proceed to construct the graph of
similarities between all media items in the collection, as discussed in Sect.3.4.2.2,
similarly to what was done in the previous two approaches (DCT-NDD, SIFT-NDD).
3.4.2.4
Near-Duplicate Detection for Video Collections
In this section, we present a near-duplicate technique for collections of videos. The
video near-duplicate retrieval (denoted VNDR in the sequel) method can detect
near-duplicate videos under various transformations/distortions, and videos which
are fractions of others in a large corpus. The VNDR method has three main stages:
(a) feature extraction, (b) coarse similarity assessment and (c) ﬁne similarity assess-
ment and temporal alignment. In the ﬁrst stage, we select 2 frames per second and
we follow the same procedure described in Sect.3.4.2.3 to extract a DCNN-based
feature vector from the “loss3/classiﬁer” layer for each selected frame. The row vec-
tors of each analyzed frame are horizontally concatenated to create the signature
matrix of each video. The height of the signature matrix is equal to the length of the
feature vector, while the width is variable and proportional to the videos length. In
the second stage, for each video we compute the mean value of each prediction class
of GoogLeNet by averaging the rows of the signature matrix, producing a signature
vector for the video. This signature vector can roughly indicate the presence of cer-
tain concepts throughout the video. Constructing an index on the signature vectors
using Randomized KD forests, and performing a kNN search we can quickly dis-
cover potential near-duplicate videos and exclude non-near-duplicates videos from
the more computationally expensive third stage. In the third stage, we employ a 2-
dimensional normalized cross-correlation method on signature matrices of a pair of
near-duplicate candidate videos to compute a similarity time-series. If the maximum
value of this time-series is above an empirically set threshold, the videos are con-
sidered as near-duplicates and the position of this maximum value in the time-series
denotes the temporal position in which the pair of videos contain a near-duplicate
segment.
3.4.3
Experimental Results
We conducted a series of experiments, to test the accuracy and efﬁciency of the devel-
oped global, local and DCNN-based descriptor approaches for photo near-duplicate
detection, as well as the near-duplicate detection approach for video collections. All
these techniques tackle the problem of near-duplicate detection, yet, as discussed
in their respective sections, each of them deals with certain cases of near-duplicate
detection and has special merits, and thus, to fairly evaluate each method, we used
the most appropriate datasets, i.e., for the global descriptor-based method we use

88
K. Apostolidis et al.
datasets that consider the speciﬁc set of alterations discussed in Sect.3.4.2.1, adding
a large number of distractor photos, while for the local descriptor-based and DCNN-
based methods we use datasets that contain partial duplicates and more complex
cases of near-duplicates.
The DCT-NDD algorithm’s performance was compared against state-of-the-art
methods of the literature, using the mean average precision (MAP) metric. The
employed datasets are listed below:
• The Copydays dataset [118] contains 157 photos and considers three different
alterations (cropping, JPEG compression and “strong” attacks such as printing,
hand-painting and scanning).
• The Copydays1 dataset is a variation of the Copydays dataset, described in [226].
• The Feature Detector Evaluation Sequences dataset [296] contains the photo
sequences Leuven, UBC, Bikes, Grafﬁti, Wall, Bark, Boat, where in each sequence
differentalterationswereapplied(i.e.,illumination,JPEGcompression,blur,view-
point changes, rotation).
• The Copydays Overlay dataset was created by the authors after resizing the 157
photos of the Copydays dataset to 1000 pixels height, and overlaying a text banner
at the top-left, top-right, center, bottom-left, and bottom-right areas of these photos,
using a 72pt font size.
• The Flickr100k dataset [327] consists of 1,00,071 photos collected from Flickr by
searching popular Flickr tags. This dataset was used for adding distractors to the
aforementioned datasets.
Table 3.8 reports the MAP scores of the evaluated approaches on the considered
datasets. Column 1 lists the alterations found in each dataset or subset thereof, col-
umn 2 speciﬁes the corresponding datasets, column 3 lists the applied methods, and
columns 4 and 5 report the MAP score without or with using additional distractor
photos, respectively. According to these results, the DCT-NDD approach performs
the best in the cases of noise addition, scaling and JPEG compression. This perfor-
mance is slightly reduced, although still remaining high, when distractor photos are
included in the dataset. Concerning the case where the original photos were postpro-
cessed by applying illumination changes, the DCT-NDD technique is the best among
three considered literature approaches on the Leuven dataset (also achieving MAP
score equal to 1.00 when no distractor photos were used), while it performs slightly
worse than the method of [226] on the Copydays1 dataset. Similar ﬁndings about the
algorithm’s performance are observed for the blur alteration, where the MAP score
of the DCT-NDD method is the highest among all compared techniques on the Bikes
dataset, and close to the best one on the Copydays1 dataset. Finally, our algorithm’s
performance, when photo cropping is applied, varies for the Copydays (Crop) and
Copydays1 datasets. This variation is explained by the fact that the ﬁrst dataset is
created by gradually cropping the original photos, removing from 10 up to 80% of
their surface (which is extreme variation). Under such conditions, methods based
on the local features and geometric constraints (such as [440]) are more capable of
identifying small cropped parts of the original photos. In contrast, the Copydays1
dataset is constructed by cropping the original photos’ surface by up to 25% (i.e., it

3
Multimedia Processing Essentials
89
Table 3.8 Performance of the evaluated methods for the considered photo alterations
Photo alterations
Dataset
Method
MAP (%)
MAP (%)
(+100k
distractors)
Photometric - illumination change
Leuven
[138] (SIFT) 35
[138] (CNN) 62
[138]
(U.CNN)
58
DCT-NDD
100
91
Copydays1
[226]
96
DCT-NDD
86
74
Noise addition
Copydays1
[226]
94
DCT-NDD
100
91
JPEG compression
Copydays
(JPEG)
[440]
91
DCT-NDD
100
91
UBC
[138] (SIFT) 55
[138] (CNN) 65
[138]
(U.CNN)
70
DCT-NDD
100
91
Copydays1
[226]
99
DCT-NDD
100
91
Scale
Copydays1
[226]
96
DCT-NDD
100
91
Blur
Copydays1
[226]
93
DCT-NDD
85
79
Bikes
[138] (SIFT) 30
[138] (CNN) 55
[138]
(U.CNN)
61
DCT-NDD
83
75
Crop
Copydays
(Crop)
[440]
93
DCT-NDD
43
32
Copydays1
[226]
89
DCT-NDD
86
72
Overlay
Copydays
Overlay
DCT-NDD
94
91

90
K. Apostolidis et al.
does not apply extreme cropping), in which case the DCT-NDD approach performs
competitively to the examined literature approaches. Last but not least, the developed
DCT-NDD algorithm exhibits robustness to the insertion of overlaying text labels in
the original photos (Copydays Overlay dataset).
In the next set of experiments, aiming to assess the SIFT-NDD and DCNN-NDD
methods, the following datasets were used.
• The California-ND dataset [184] contains 701 photos taken directly from a real
user’s personal photo collections. As a result, it includes many challenging non-
identical near-duplicate cases without resorting to the use of artiﬁcial photo trans-
formations. In order to deal with the subjectivity and ambiguity that near-duplicate
cases exhibit, the dataset is annotated by 10 different subjects, including the pho-
tographer. These annotations can be combined into a nonbinary ground-truth,
representing the probability that a pair of photos is considered a near-duplicate.
• The Holidays dataset [179] is a set of photos contains personal holidays photos.
The photos were taken to test the robustness to various attacks: rotations, viewpoint
and illumination changes, blurring, etc. The dataset includes a very large variety
of scene types (natural, man-made, water and ﬁre effects, etc.), and contains 500
photo groups, each of which represents a distinct scene or object. The ﬁrst photo
of each group is the query photo and the near-duplicates of it are the other photos
of the group.
• The University of Kentucky benchmark dataset (UKBench, [265]) contains 10200
photos of (2550 groups with four photos each), at size 640 × 480. The photos are
rotated and blurred.
• The Oxford Buildings dataset [327] consists of 5062 photos collected from Flickr
by searching for particular Oxford landmarks. The collection has been manually
annotated to generate a comprehensive ground-truth for 11 different landmarks,
each represented by 5 possible queries. This gives a set of 55 queries over which
an object retrieval system can be evaluated. The queries are a cropped portion of
a photo in the collection, therefore, this dataset is oriented toward partial near-
duplicate photo retrieval.
Table 3.9 reports the MAP scores of the evaluated approaches on the selected
photo datasets. Column 1 speciﬁes the tested method, column 2, 3, 4, and 5 list the
achieved mean average precision score (MAP) of each tested method on California-
ND, Holidays, UKBench, and Oxford Building datasets respectively. According to
these results, the SIFT-NDD method performs better than the DCNN-NDD method
on UKBench and Oxford Buildings datasets which are oriented toward partial near-
duplicate photo retrieval. This due to the use of local descriptors that incorporate
spatial information into the feature vector that describes the photo. Inversely, the
DCNN-NDD method performs better than the SIFT-NDD method on the California-
ND and Holidays datasets.
Table 3.10 reports the mean extraction time per photo for all the near-duplicate
retrieval and detection methods discussed here. We tested the SIFT-NDD standard
approach (SIFT-NDD/serial) and an approach running the feature extraction in par-
allel (SIFT-NDD/parallel) using the Intel OpenMP library. For the DCNN-NDD

3
Multimedia Processing Essentials
91
Table 3.9 Evaluation of SIFT-NDD and DCNN-NDD approaches against near-duplicate detection
methods from the literature
Dataset
California-ND
MAP (%)
Holidays MAP
(%)
UKBench MAP
(%)
Oxford buildings
MAP (%)
SIFT-NDD
76.3
81.6
84.3
79.3
DCNN-NDD
93.2
91.2
73.2
63.9
[434]
85.2
–
–
–
[421]
–
89.9
–
–
[23]
–
–
95.0
–
[410]
–
–
–
82.80
Table 3.10 Feature extraction times for each of the proposed near-duplicate detection approaches.
For the local descriptor-based approach (SIFT-NDD/serial) the feature extraction time for a par-
allel implementation is also presented (SIFT-NDD/parallel), while for the DCNN-based approach
(DCNN-NDD/CPU), we also present the feature extraction times when using a GPU (DCNN-
NDD/GPU) and when using a GPU and the NVIDIA CUDA Deep Neural Network library (DCNN-
NDD/GPU+cudNN)
Approach
Mean feature extraction time per photo (ms)
DCT-NDD
0.08
SIFT-NDD/serial
405
SIFT-NDD/parallel
58
DCNN-NDD/CPU
124
DCNN-NDD/GPU
28
DCNN-NDD/GPU+cudNN
14
method, we test the performance using the Caffe deep learning framework running on
CPU, GPU, and ﬁnally utilizing the NVIDIA cuDNN library, which provides highly
tuned implementations for standard routines and can offer an up to 2.2x speed-up
on most NVIDIA GPU cards. The DCNN-NDD on GPU (DCNN-NDD/GPU) and
DCNN-NDD on GPU using cuDNN (DCNN-NDD/GPU+CUDNN) mean feature
extraction times are calculated using a batch size equal to 24. All experiments dis-
cussed in this section are executed on a Windows 7 64-bit PC with Intel Core (TM)
i7-4790K CPU @ 3.50GHz, 16GB RAM, and an NVIDIA Geforce GTX 1060 GPU.
According to these results, the DCT-NDD which is suited for application where spe-
ciﬁc photo alterations are considered (as discussed in Sect.3.4.2.3) is the faster one
by a large margin. As expected, we can also see how the use of a GPU can drastically
reduce the feature extraction time for DCNN-based methods.
To evaluate the video near-duplicate retrieval method (VNDR) we used the
CC_WEB_VIDEO dataset [420], which consists of 13,129 videos related to 24
distinct text queries. Each text query was issued to YouTube, Google Video,
and Yahoo! Video. The videos gathered include exact- and near-duplicate videos
under various photometric variations (color/lighting change), editing changes (logo

92
K. Apostolidis et al.
Table 3.11 Evaluation of video near-duplicate retrieval methods
Method
MAP (%)
VNDR
91.3
SIG_CH of [420]
89.2
HIRACH of [420]
95.2
insertion, borders addition around frames, superposition of text), and content mod-
iﬁcations (unrelated frames’ addition with different content). We conducted experi-
ments comparing the proposed method against the method of [420], using the mean
average precision (MAP). Table 3.11 reports the results. We observe that the proposed
method performs better than the SIG_CH approach of [420]. The HIRACH approach
of [420] performs the best, however, this technique is based on the local handcrafted
features, therefore, it presents higher computational complexity compared to using
global handcrafted features (as discussed in Sect.3.4.1) or DCNN-based features,
which are used in VNDR.
3.5
Event-Based Photo Clustering and Summarization
Clustering algorithms partition a collection of items into groups called clusters, such
that similar items are assigned into the same group. Using clustering algorithms
large collections can be handled in such a way that items can be organized, and
subsequently summarized by selecting a representative item from each different
cluster to construct a summary. Regarding photo collections, a clustering algorithm
may segment the photos of the collection to visually similar clusters or semantically
similar clusters. In this section, we deal with clustering algorithms that group together
photos with close capture times.
3.5.1
Related Work
There is a variety of works in the literature that deal with the management of
photo collections (such as [437]), but very few deal with the explicit problem of
event-based photo clustering. In an early work, the authors of [145] propose the use
of capture date/time (timestamps) found in photos’ metadata, in order to cluster a
photo collection to events. They perform an analysis on consumer media capture
behavior to show that the temporal distribution of photos is not well described by
a Poisson model and propose a fractal process, to perform the temporal clustering.
However, this method only makes use of the timestamps, not taking into considera-
tion the visual content of the photos. In [254], various algorithms for the management

3
Multimedia Processing Essentials
93
of photo collections, and among them, an automated event clustering method, are
proposed. The clustering technique is based on the timestamps as well as color infor-
mation. In [94] several variants of an automatic unsupervised algorithm to partition
a collection of digital photographs are presented, based either on temporal similarity
alone, or temporal and content-based similarity. The authors quantify inter-photo
similarity at multiple temporal scales to identify likely event clusters, and the ﬁnal
clusters are determined according to three clustering goodness criteria. The use of
multiple temporal scales and multiple clustering criteria helps to convey meaning-
ful results, yet, the computational complexity is O(N 2). In [291], a probabilistic
approach is presented that fuses multimodal data for event-based home photo clus-
tering, which include time, visual content as well as camera settings. The clusters
are discovered through the Expectation–Maximization algorithm.
3.5.2
Method
The event-based clustering method presented here concerns photo collections about
a speciﬁc event. The event depicted in such a collection typically consists of several
subevents, namely groups of photos that constitute a distinct action within a bounded
time and space, and the discovery of these subevents’ bounds is the objective of
the method proposed here. Furthermore, selecting a representative photo from each
detected subevent, we can construct a storyline of photos that summarizes the whole
collection. To give an example, suppose that a person attends a public event such as
some competitions of the Olympic games of London 2012 and takes a number of
photos. The individual sports competitions (e.g., ﬁrst day of men’s tennis, women’s
weightlifting ﬁnal, etc.) can be considered as sub-events of the entire London 2012
summer Olympic event. Our event-based photo collection clustering method would
detect all photos of each sport competition (subevents). Selecting the most appropri-
ate photo of each subevent, it would construct a summary of this Olympics event.
The main stages of the photo collection event-based clustering and summariza-
tion method are: (a) feature extraction from photos, (b) employing an event-based
clustering algorithm to cluster the photos using the extracted features, (c) selection
of representative photos for summarizing the whole photo collection. The type of
features used to describe the visual content of photos in the ﬁrst stage can vary: a fast
global descriptor (such HSV color histograms, or the features used in Sect.3.4.2.1)
or a more robust aggregation of local descriptors (extracted similarly to the approach
described in Sect.3.4.2.2) could be used. We choose to extract DCNN-based features
using again the Caffe deep learning framework [210] and the 22-layer GoogLeNet
DCNN pre-trained model [377]. The GoogLeNet model, besides the main classi-
ﬁer, uses two auxiliary classiﬁers, with softmax loss as the classiﬁer, predicting the
same 1000 classes as the main classiﬁer (see Fig.3.2). For the speciﬁc problem of
event-based photo collection clustering and summarization, we choose to extract the
“loss1/classiﬁer” and “loss3/classiﬁer” layers due to the semantic nature of these
layers output, resulting in two 1000-dimensional vectors. The 2000-dimensional

94
K. Apostolidis et al.
concatenated feature vector is used to represent each photo. We then perform PCA
to reduce its dimensionality to 300.
During the second stage, the employed event-based clustering algorithm used can
be one of the following:
• Cascaded Event-based Clustering approach (CEC): consists of three stages: (a)
time-based splitting, (b) geolocation-based splitting, and (c) merging. In the ﬁrst
stage, we sort the photos based on their capture time and the temporal distance
of each photo to the next one is computed. The timeline of the temporally sorted
photos is split where consecutive photos have temporal distance above the mean of
all temporal distances. During the second stage, geolocation information is used
to further split clusters of photos, considering the case of concurrent subevents
at different locations. We compute all photos pairwise capture location distances
using the haversine distance [341]. Using k-means, we cluster the pairwise capture
location distances into two clusters. The cluster with the lowest mean of distances
(m1) presumably signiﬁes photos captured in the same subevent while the cluster
with the highest mean of distances presumably corresponds to photos captured
in different subevents. We check the pairwise capture location distances for the
photos within each cluster of the ﬁrst stage, and photos with distance more than m1,
are moved to a new cluster. In the third stage, clusters are merged using time and
geolocation information. If the temporal distance of two clusters (i.e., the temporal
distance between the last photo of one cluster and the ﬁrst photo of the next
cluster) is smaller than the mean of the intracluster temporal distances then these
two clusters are merged. We also merge temporally neighboring clusters whose
geolocation difference is less than m1. For the clusters that do not have geolocation
information, the merging is continued by considering the concept scores similarity,
essentially checking if pairwise distances of photos of two clusters are below an
empirically set threshold. If none of the photos in the collection has geolocation
information, then the second stage of geolocation-based splitting is omitted.
• Event-based clustering utilizing the Potential-based Hierarchical Agglomerative
algorithm (PHA): We normalize the capture times information of photos in the
collection in the [0 . . . 1] range. We concatenate the features extracted during
the ﬁrst stage with the normalized capture time of photos, if this information
is available, or with a zero-values attribute for the photos with no capture time
information available. We then construct a similarity matrix of the augmented
feature vectors. We weight the similarity matrix with the inverse of the photos
physical locations distance, so that similarity is increased for photos with close
capture locations. We ﬁnally apply the clustering method of [256] on the weighted
similarity matrix.
• Baseline method: In the conducted experiments for event-based clustering (see
Sect.3.5.3), we included an additional method, denoted as “baseline”, in which
we consider only temporal metadata for the clustering to subevents (similarly to
[145]). This is included for showcasing the merits of the aforementioned CEC and
PHA methods.

3
Multimedia Processing Essentials
95
Table 3.12 Evaluation of event-based clustering methods for photo collections
Vancouver
London
FI (%)
JI (%)
FI (%)
JI (%)
Baseline
19.2
24.3
28.1
42.6
CEC
24.6
31.1
34.3
51.6
PHA
24.3
30.7
31.2
45.5
Finally, to summarize the photo collection we select a photo from each cluster.
The photo to be selected can be the one that is the most similar to rest of the photos
in the same cluster; this computation is fast, considering that we already have a sim-
ilarity matrix of all photos in the collection constructed in the second stage of the
method. Alternatively, the summarization algorithm may rely on aesthetics assess-
ment: the aesthetic quality of all photos in each cluster can be evaluated, as discussed
in Sect.3.3, and the highest scoring one can be selected as the representative of a
speciﬁc cluster. Additionally, a summarization algorithm may rely on near-duplicate
detection (Sect.3.4): the presence of numerous near-duplicates about an object/scene
of interest in a cluster may signify the importance of this speciﬁc object/scene to the
user, and thus, one of these near-duplicates can be selected to represent the cluster.
Note here, that such a summarization method solely concerns event-centered photo
collections; for a discussion on the automatic selection of important photos from
personal collections, based on the insights emerging from a user study, see Chap.8.
3.5.3
Experimental Results
The aforementioned approaches of photo collection event-based clustering are eval-
uated on the datasets of the MediaEval 2014 SEM [82] task:
• The Vancouver dataset, which consists of 1351 photos and captures various
subevents of the Vancouver 2010 Winter Olympic Games
• The London dataset, which consists of 2124 photos and captures various subevents
of the London 2012 Olympic Games.
For the evaluation of the methods against the ground-truth event-based clustering,
we used two measures: (a) the F-measure (F1), and (b) the Jaccard Index (JI), which
is deﬁned as the number of unique elements common to both sets divided by the total
number of unique elements in both sets.
Table 3.12 reports the F-measure and Jaccard Index scores of the presented meth-
ods on the Vancouver and London datasets. According to these results, the CEC
method performed marginally better than PHA on the Vancouver dataset, while on
the London one the CEC method performed the best by a large margin. The baseline
method performed poorly in both datasets, since it exploits only the temporal meta-
data of the photos. An indicative example is shown in Fig.3.12, where a subset of the

96
K. Apostolidis et al.
(a)
(b)
(c)
Fig. 3.12 a A subset of the photo collection of the London 2012 Olympic games (London dataset),
captured by multiple users and depicting concurrent subevents. b The result of applying the baseline
event-based clustering method. c The result of applying the PHA or CEC method (their results were
identical for this speciﬁc subset of the London dataset)
London dataset is used. In the top row, the photos of this subset are displayed, sorted
according to their capture time. Note that the photos of this dataset are captured by
multiple users and therefore they depict concurrent subevents. In the middle row, the
result of applying the baseline event-based clustering method is depicted, while the
bottom row shows the result of applying either the PHA or the CEC methods (the
results were identical for this speciﬁc subset of the London dataset). We notice that
the baseline method included both boxing and men’s tennis in the same subevent
(subevent-2 of the middle row), since these were concurrent subevents (same applies
for the case of fencing and rowing being included in the same subevent-4 of the mid-
dle row). Both of the proposed event-based clustering techniques managed to assign
the photos of these sport events to different subevents (subevent-2 and subevent-3
of the bottom row for boxing and men’s tennis, respectively, and subevent-5 and
subevent-6 of the bottom row for fencing and rowing, respectively). As a ﬁnal note,
and taking into consideration that the CEC method relies heavily on the capture
time of photos, while the PHA method uses this just as auxiliary information, we
conclude that the PHA method is ideal for the scenarios where temporal information
is partially or fully unavailable.
3.6
Conclusions
In this chapter, we surveyed the literature and presented our developed methods
that can be used for photo/video annotation, photo/video quality assessment, near-
duplicate detection, and event-based photo clustering and summarization. These

3
Multimedia Processing Essentials
97
methods are valuable tools in implementing a digital preservation and forgetting
strategy.
For the problem of photo/video annotation, we presented a deep learning two-step
process that can be used both for photo and video shot annotation and we focused
on important directions for improving concept detection accuracy. Experiments per-
formed on two datasets show that the method of increasing the depth of a pretrained
network with one fully-connected layer and ﬁne-tuning the rest of the layers on the
target dataset can improve the network’s concept detection accuracy, compared to
other ﬁne-tuning approaches. Using layers’ responses as feature vectors for a learning
model can lead to additional gains, compared to using the direct network’s output.
Using more elaborate methods (such as the presented ELLA_LC model instead of
simple models) can further improve concept detection accuracy, reaching a 32.10%
of MXinfAP when evaluated on the TRECVID SIN 2013 dataset. This number is
among the state-of-the-art results for this speciﬁc dataset and, although seemingly
low (since it is much lower than 100%), a qualitative analysis shows how good results
it represents. For example, considering the concept chair that reaches an infAP of
32.84%, we observe in the resulting ranked list of concept-based retrieval results
that among the top-20 retrieved keyframes all of them are positive; among the top-50
retrieved keyframes 46 are positive; and among the top-100 ones 86 of them are
positive samples.
We also discussed the literature about the problems of photo/video aesthetic qual-
ity assessment and presented our developed methods. We presented a comprehen-
sive photo aesthetic assessment method, which exploits ﬁve basic photographic rules
(“simplicity,” “colorfulness,” “sharpness,” “pattern,” and “composition”) to extract
rule-speciﬁc low- and high-level visual features. The experimental results of this
approach, and of the combination of these features with previously proposed generic
ones, demonstrated the importance of taking basic photographic rules into account
when designing an aesthetic quality assessment method. Regarding the problem of
video aesthetic quality assessment, we presented a method that combines a compre-
hensive set of video features with a new learning approach, which takes the video
representation’s uncertainty into consideration. Experimental results of our approach
on a dataset constructed by us, demonstrated considerable performance improvement
in comparison to the state-of-the-art learning methods used for video aesthetic qual-
ity assessment. Applying these techniques, we can assess the aesthetic quality in
an automatic way, with no user interaction, and help the detection of items of poor
quality which are more likely to be archived or “forgotten” (i.e., deleted) in a PoF
framework.
Furthermore, we reviewed the most commonly used approaches in the literature
for near-duplicate detection and retrieval. We presented three approaches based on
the different types of features. The experimental results demonstrated the merits
of each tested approach. More speciﬁcally, a global descriptor-based approach is
the fastest but is more suited when a speciﬁc set of photo alterations is considered.
A local descriptor-based approach performed the best when employed for partial
near-duplicate detection. A DCNN-based approach presented very good overall per-

98
K. Apostolidis et al.
formance. We also described and evaluated a video near-duplicate retrieval method,
which is able to detect segments of a near-duplicate video in another.
Finally, we presented and compared two event-based clustering methods that
partition the photo collection to different subevents under the assumption that a photo
collection concerns a single event. Taking into consideration the subevent clusters
created and various other results, such as the aesthetic quality of the photos/videos or
the presence of near-duplicates in each cluster, we are able to construct a summary
for the whole collection. Near-duplicate detection and summarization algorithms can
assist the condensation of large photo/video collections.
The methods presented in this chapter are used in subsequent chapters in this book.
For instance, in Chap.4 they are used for automatically predicting the long-term
importance of a photo; in Chap.5 they are exploited in a pre-processing step for the
contextualization of photo collections; in Chap.7 they are integrated in the Semantic
Desktop infrastructure; and, in Chap.8 they are used in a method for preservation-
centered automatic photo selection.

Part II
Multimedia Preservation Theory

Chapter 4
Preservation Value and Managed Forgetting
Claudia Niederée, Nattiya Kanhabua, Tuan Tran
and Kaweh Djafari Naini
Abstract In the cognitive processes of humans, forgetting is a very effective way
for focusing on the important things, while unstressing things, which are (currently)
less important. The translation of forgetting into the digital world is, thus, a promis-
ing approach for better dealing with the increasing problem of information overload.
Information overload is not only caused by the mere volume of information, it is also
triggered by the fact that all information is seemingly on the same level of impor-
tance. In the ideal case, a perfect dynamic assessment of importance could restrict
the information space strictly to the information currently needed, thus dramatically
reducing information overload. The role of a digital memory including digital for-
getting is to support, not to replace or to hinder human memory. Therefore, a useful
approach for “managed forgetting”—a controlled form of digital forgetting—in a
digital memory has to be carefully designed, such that it complements human mem-
ory. A core ingredient of managed forgetting is the assessment of the importance
of information items. Furthermore, forgetting actions are required that go beyond
the binary decision between keep and delete. For the short-term perspective, man-
aged forgetting replaces the binary decision on importance by a gradually changing
value: Information sinks away from the user with a decreasing value, which we call
“Memory Buoyancy”. The transition from short-term value to long-term importance
brings a variety of new challenges. When we look into the “Preservation Value” of
an information item, we have to estimate the future importance of a resource. This
challenging task is further complicated by the facts that (a) preservation looks into
very long time frames (e.g., decades rather than months) and (b) the importance of
information items may change over time. The Preservation Value provides the basis
C. Niederée (B) · T. Tran · K. D. Naini
L3S Research Center, Hannover, Germany
e-mail: niederee@L3S.de
T. Tran
e-mail: ttran@L3S.de
K. D. Naini
e-mail: naini@L3S.de
N. Kanhabua
NTENT Inc., Barcelona, Spain
e-mail: nkanhabua@ntent.com
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_4
101

102
C. Niederée et al.
for making preservation decisions, e.g., how much to invest for ensuring that a media
item such as a photo will survive the next years or decades. We, furthermore, investi-
gate into methods for information value assessment in support of managed forgetting.
For this purpose, we analyze existing methods for information value assessment and
discuss their usefulness in the context of computing the Preservation Value. We also
outline methods for Preservation Value computation for different exemplary settings.
This will also point to the more in-depth discussion of computing Preservation Value
in the semantic desktop and photo preservation as it is discussed in later chapters
of the book. Finally, we discuss managed forgetting beyond assessing the impor-
tance of information items. We study a portfolio of forgetting methods, i.e., methods
that can be used to implement managed forgetting on top of the values for informa-
tion importance. This includes methods such as information hiding, forgetful search,
summarization and aggregation as well as deletion.
4.1
Effective Information Management with Managed
Forgetting
The idea of “managed forgetting” is to systematically deal with information that
progressively decreases in importance, in order to enable the user to focus on the
things that are important. At ﬁrst glance, digital forms of forgetting seem to contradict
theideaofpreservation,whichisaboutkeepingthings,notaboutthrowingthemaway.
However, if no special actions are taken for long-term preservation, we already face
a rather random digital forgetting process in the digital world today in the private
as well as in the professional information management context as has already been
discussed in Chap.1 of this book. Furthermore, from a more global perspective, there
is a growing understanding that a systematic support for digital forgetting [278] has
to be considered as an alternative to the dominating “keep-it-all” paradigm and the
often random survival of information, e.g., in the public space of the web.
We suggest to replace such random forgetting processes with managed forgetting.
Here, users are supported in their explicit decisions about what information to keep,
and how such information should be organized and preserved. In particular, we
envision an idea of “gradual forgetting”, where complete digital forgetting is just
the extreme and a wide range of forgetting actions including different levels of
condensation is foreseen. This concept is expected to help in preservation decisions.
It is also expected to create immediate beneﬁts for active information use by helping
to keep the active information spaces more focused. The aim is to strike a balance
between preservation and managed forgetting, also taking into account constraints
for digital forgetting (e.g., legal regulations).
To support managed forgetting, it is essential to evaluate the information resources
with respect to their importance. This embraces current importance as well as
“preservation-worthiness” in the long term. Such assessment methods need to be
ﬂexible enough to cope with the inherent heterogeneity and temporal dynamics of

4
Preservation Value and Managed Forgetting
103
resources over time. For coping with short-term and long-term importance, we deﬁne
two complementing parameters to assess information values of resources for man-
aged forgetting.
The ﬁrst parameter, “Memory Buoyancy” (MB), is inspired by the metaphor of
information objects sinking down in the digital memory with decreasing importance,
usage, etc., increasing their distance from the user. This type of information value
is highly associated to short-term interests [416]. MB is inﬂuenced by a variety of
factors such as usage of a resource or importance of the resource’s topic in the current
context.
The second information assessment value is “Preservation Value” (PV). It is used
for making preservation decisions, i.e., deciding on whether the resource under con-
sideration should be preserved or how much should be invested in preserving it. The
PV will use an overlapping but different set of parameters for its computation, com-
pared to MB. PV is more related to long-term interests and to more objective types
of assessment, e.g., diversity and coverage.
Beyond information value assessment, managed forgetting also requires meth-
ods that act upon those values. We call this actions “forgetting actions”. A wide
variety of actions is possible here. For MB, i.e., short-term importance, the target
of such actions is mainly to support the user in keeping focused, to make dealing
with information spaces more delightful, and to reduce information overload. This
implies methods such as information hiding (based on MB threshold) and forgetful
search which punishes search results with low MB value. For PV, i.e., long-term
value, actions related to long-term management and preservation are important. This
includes preservation decisions such as what to preserve and how much to invest into
preservation. In addition, this includes actions such as summarization, aggregation,
and elimination of near duplicates as more reﬁned forgetting actions.
Finally, deletion or suggestion for deletion is also a possible forgetting action both
for short-term as well as for long-term importance.
4.2
Short-Term Value and Memory Buoyancy
MB indicates the short-term importance of a digital object reﬂecting some type
of “closeness” to current human memory processes and current activities. MB is
inspired by the metaphor of information objects sinking down in the digital memory
with decreasing importance, usage, etc., increasing their distance from the user.
Our discussions of the topic in this book are based on the following deﬁnition of
MB:
Deﬁnition 4.1 Memory Buoyancy (MB) is a value attached to a resource reﬂecting
the expected beneﬁt from bringing this resource digitally close to the user in the
current context. Here, bringing the resource digitally close to the user means reducing
the cognitive effort of the user for ﬁnding and accessing the resource.

104
C. Niederée et al.
This idea embraces the concept of resource importance as well as the idea of com-
plementing human memory. Following the idea of complementing human memory,
MB is related in a nontrivial way to the interaction with resources and to the degree to
which memories and an image of a digital object fade in human brain over time, either
as a result of interference in human memory, or because of other factors intervening
in the remembering or recall process. On the one hand, it is important to identify the
objects that are currently important or will be important in the near future, i.e., which
have a high probability to be (re-)accessed. On the other hand, for complementing
human memory, it is also important to consider both (a) how easy is it to ﬁnd the
identiﬁed important resources (access effort) and (b) how probable is it that the actor
needs to rehearse the information from the resource (vs. he still remembers the con-
tent and, thus, will not re-access). Only when considering both aspects, beneﬁts can
be created for the user by bringing the resource closer to him or her. For identifying
important resources as well as for assessing re-access effort, it is important to not
just consider a resource and the action on this resource in isolation, but to understand
the interaction with related resources.
Based on the systematic review of existing practices in human-assisted digital
mementos, as well as on cognitive aspects of complementing human memory dis-
cussed thus far, we propose a general framework as a basis for incorporating MB
and short-term managed forgetting into digital memories with a special focus on per-
sonal information management (PIM) [106]. In PIM, an ever-growing information
space tends to clutter with information which is irrelevant in given situations and thus
is in danger to become unmanageable and useless both for technical reasons (e.g.,
resource limits on smart phones) and for cognitive reasons (information overload).
Therefore, managed forgetting can play an important role in accessing and managing
personal information space.
Following the above considerations, we developed an initial conceptual frame-
work for managed forgetting and contextualized remembering in digital systems.
While we are aware of the importance of contextualization in the framework, we
focus on the aspects of contexts that are relevant and useful for managed forgetting.
Our proposed conceptual framework is inspired by an integrated and contextual per-
spective on Information Retrieval (IR) presented in [175]. As illustrated in Fig.4.1,
the conceptual framework consists of three main parts:
• A virtual “information space” which is composed of human and digital memory;
• A “context” which deﬁnes a social, societal, and organizational environment into
which the information space is embedded;
• A “human actor” accessing and interacting with the information space.
We decided to conceptually split the human actor from his/her memory and to
rather consider the human and the digital memory as an integral system. There are
two reasons for this: (a) we are especially interested in what we can learn from human
memory for creating effectively accessible digital memories (time-aware information
access) and (b) the boundaries between human and digital memory are in ﬂux due
to an increased delegation to digital memory.

4
Preservation Value and Managed Forgetting
105
Fig. 4.1 Conceptual framework for managed forgetting and contextualized remembering in digital
systems with special focus on personal information management
In this framework, we introduce three main concepts for describing central pro-
cesses in this framework: “resources”, “interactions”, and “human actors”.
Resource is the abstraction of information artifacts in the information space. A
resource can refer to data objects stored in the digital systems, but it can also refer to
the image of such objects as encoded and stored in the human brain, or as shared by
human actors in three overlapping spheres of digital memory—personal information
management (organizing resources on local devices such as mobiles or personal
computers), the social web (online storage and sharing still of personal information),
and the web in more general (see Fig.4.1).
Interaction refers to actions within the information space that resemble or are
relevant to human memory processes (i.e., information encoding, storing and recall-
ing [13]). We consider three main types of interaction: (a) delegation and digestion,
(b) forgetting and contextualization, and (c) access and remembering. By “delega-
tion”, information is moved into outer spheres (e.g., from human memory to things
that can be looked up, or from personal information management to the social web).
This happens increasingly, triggered by improved access strategies and increased
availability of information via the web (e.g., via smart phones). An example is look-
ing up deﬁnitions instead of learning them. “Digestion” is the counterpart to dele-
gation at the boundary between human and digital memory. Closely related to this,
the human actor conceptually has two choices, when he needs information: he can
try to ﬁnd it in his memory (“Remember”), where forgetting and contextualization
are important processes; or he tries to ﬁnd them in the digital space (“Access”). Of
course, using digital resources again implies cognitive processes, before they can
be used. Managed forgetting now comes in as an additional process in the digital

106
C. Niederée et al.
memory. It helps to bring resources, which are important to the user closer to the user.
Following the above discussion about complementing human memory, the ultimate
goal of managed forgetting is to bring resources closer to the user, that are expected
to create a beneﬁt to him in the current situation. This would not enclose resources
with information, which is useful or important but already memorized by the user.
Human actor can “access” his/her information space through access interfaces
and such “direct interactions” can be, for instance, search, lookup or exploration
of data objects. There also exist “indirect interactions” between actors and external
contexts, for example, relevance of resources as background information, topic and
general importance of topic, and external constraints, which also play an important
role in the proposed framework. “Remembering” can be (roughly) considered as
counterpart of “access” for human memory. It is typically triggered by external
events, which are experienced by the human actor.
The key difference between our proposed model and the previous work [175] is to
explicitly introduce “managed forgetting” into the framework. Managed forgetting
creates immediate beneﬁt by focusing on relevant content in a time-aware fashion,
“forgetting” (unfocusing) things that are no longer important.
Managed forgetting for the short-term perspective is based on assessing the value
of a resource in the current context. This type of information value is highly associ-
ated to short-term interests [416], which is inﬂuenced by a variety of factors that can
be roughly grouped in the following categories: usage parameters (such as frequency
and recency of use, user ratings, recurrent pattern), type and provenance parameters
(information object type, source/creator), context parameters (such as relevance of
resources as background information, general importance of topic, external con-
straints), and temporal parameters (age, lifetime speciﬁcations). In order to support
managed forgetting, various factors inﬂuencing MB need to be investigated as well
as approaches for learning most effective factor combinations.
Furthermore, approaches for enabling the user to explicitly and implicitly inﬂu-
ence the values for MB (and for PV) have to be considered, e.g., explicit expiry dates
and lifetime speciﬁcations or tagging objects as non-forgettable.
4.3
Preservation Value and Its Drivers
In contrast to MB, which has been discussed in the previous section, PV is related
to long-term information management considering time frames of decades. PV is
crucial to preservation decisions, since it aims to estimate the expected long-term
beneﬁt of a resource.
In this section, we discuss PV in more detail, present a working deﬁnition for PV,
and discuss a set of dimensions for drivers of PV as a foundation for a systematic way
of determining PV. Furthermore, we relate PV assessment and managed forgetting
to the ﬁeld of content appraisal known from research and practical work in the area
of preservation.

4
Preservation Value and Managed Forgetting
107
4.3.1
Preservation Value
In particular, PV is used to determine the amount of investment to be made for
ensuring the long-term survival and understandability of the resource under consid-
eration. This can refer to preservation decisions such as how many copies to keep
or to investment in semantic preservation, which also takes into account context
evolution aspects. In the extreme case, PV can also be used to make keep or delete
decisions.
Our discussions of the topic in this book are based on the following deﬁnition of
PV:
Deﬁnition 4.2 Preservation Value (PV) is a value attached to a resource reﬂecting
the beneﬁt expected from the long-term survival of it.
The computation of PV, obviously, is a challenging task. The deﬁnition already
highlights one of the major challenges in assessing PV: it is linked to the expected
future value of a resource. This encompasses predicting the future value of a resource.
Since neither future value nor future uses of a resource can be fully known at a given
point in time, it is clear that PV can only be an estimate of the expected future value.
Overall, automatic computation of PV is a very novel ﬁeld. Such assessments are
typically done by humans.
4.3.2
Preservation Value Dimensions
In addition, PV is inﬂuenced by a variety of partially setting-speciﬁc factors. There-
fore, it is not expected that there will be one single method, which can compute PV
of resources for all possible settings and types of content, even if we just restrict
to personal and organizational preservation. For example, the decision, if I want to
keep a photo or an email on the long run, is inﬂuenced by other factors. It has to be
noted here again that preservation refers to very long time frames.
However, we have identiﬁed eight high-level dimensions that can be used to
categorize the evidences used for computing PV. They provide a basis for developing
a more systematic approach to PV assessment. The list of dimensions has been
compiled based on content selection work from literature [408, 418], own studies in
content selection for preservation [69, 70], and a study of work on appraisal in the
archival domain [93, 355]. In the following, we describe those dimensions together
with examples for illustrating the concept of each dimension:
Content Type This dimension refers to the type of the content to be assessed. Types
might be considered on several levels ranging from image versus text formats to
more semantic types, e.g., distinguishing a holiday photo from a food picture.
Investment In a wide sense, this dimension refers to the investment, which has been
made into the resource and its improvement/change. For a photo collection, such

108
C. Niederée et al.
investment might be the annotation of photos, the improvement of photos in photo
software, or the creation of multiple photos of the same scene.
Gravity This dimension refers to the relationship or closeness of a resource to impor-
tant events, processes, and structures in the domain under consideration. For per-
sonal photos this might be the closeness to an important event such as a wedding
or an important life situations such as the ﬁrst years of one’s child.
Time Although the age of the content and time-related properties in more general are
less important for long-term information management than for the decision about
short-term interests, temporal aspects still play an important role for assessing
Preservation Value. For social web content, for example, there is a trend to be
more selective, when the content gets older.
Social Graph This dimension describes the relationship of the resource to the rel-
evant social graph, i.e., the persons related to the resource, their roles, and rela-
tionships. This might refer to the creators and editors of a resource as well as to
persons related to the content of the resource.
Popularity This dimension refers to the usage and perception of the resource. For
the case of social web content, this might refer to shared and liked content.
Coverage and Diversity This dimension refers to factors, which consider the
resource in relationship to other resources in the same collection. This includes,
for instance, diversity or coverage of sub-events, which are also used in making
preservation decisions and, thus, inﬂuence PV. It can, for example, be taken into
account by trying to cover all the sub-events of a holiday, when selecting the most
important photos to preserve from a holiday photo collection.
Quality This dimension refers to the quality of the resource. Obvious examples for
content quality is photo quality assessing, e.g., if the photo is blurred or exhibits
good contrast. More advanced quality aspects are, for example, photo composition
and aesthetics.
From the consideration of the dimensions, a better understanding of the PV itself as
well as of the importance of those dimensions for the computation of PV is expected.
They also help in abstracting from the individual case.
4.3.3
Preservation Value and Appraisal
In this book, we are mainly focusing on preservation in the personal setting. However,
so far little work exists in the ﬁeld of personal preservation (see also Sect.2.4.5). We,
therefore, in this section, also look into preservation practices from the organizational
setting, because we can learn from organizational practices for personal preservation.
We are especially interested in the idea of “appraisal”, which is linked to our concept
of “managed forgetting”. There is a general understanding in the preservation com-
munity that (digital) preservation is also a selection process [93, 160, 222], since
it is neither useful nor possible to preserve everything [222]. This introduces the
task of appraisal into the preservation process [93, 355]. Formally, appraisal is the

4
Preservation Value and Managed Forgetting
109
process of determining the value of records, which are estimated based on their legal
or ﬁnancial meaning, or other factors. These values will assist archivists in deciding
which record is worth to be preserved and how long they should be retained.
In assessing the value of a resource, two types of value are considered: primary
value and secondary value [355]. Roughly speaking, primary value refers to the value
that a resource has for the organization that created it. It is typically related to ongoing
business value. The secondary value can target a wider audience beyond the creating
organization. It refers to a broader historical and societal value of a resource. This,
for example, embraces the ability of a resource to reﬂect historical developments and
situations.
Functional appraisal and macro-appraisal [93], as alternative appraisal methods
suggest not to start the appraisal process by looking into individual resources. They
suggest to look into an organization’s purpose and processes (functions), and then
identify which records reﬂect those functions best. For breaking this down, ﬁrst some
broad functions can be identiﬁed for an organization and each function can be further
divided into a set of sub-functions.
In the following, we will link the Preservation Value dimensions proposed above
to appraisal methods and criteria used in archives. An obvious link can be made from
the approach of functional appraisal and macro-appraisal [93] to the dimensions of
gravity and the social graph. As in the case of gravity, macro-appraisal looks
into the importance of information items for the institution, looking into the struc-
ture and processes of the institution. Functional appraisal looks at “the functions
carried out by the record creator”1 and uses this information in appraisal instead
of content criteria, which is related to the dimension of the social graph, which
has a broader scope enabling its application in personal and organizational settings.
Another aspect, which is linked to the dimension of gravity is the aspect of long-term
historical importance, which is often used as a criteria for assessing secondary value
in appraisal.
For the dimension of investment, our approach refers to the preparation of
resource and the corresponding improvement. However, archivists would also take
the legal risks or administration efforts into account. The dimension of popularity
is linked to the idea of “Social Signiﬁcance” as named as appraisal criteria in [160].
Popularity can be seen as the measurable part of social signiﬁcance, but does not fully
cover this aspect. The dimension coverage and diversity is not directly related to an
appraisal criterion. However, it is implicitly linked to the idea of the macro-appraisal
approach, which by its strategy aims to cover the big picture of the content to be
preserved.
The dimension time is important in both appraisal and in our approach. For
example, the need of a reappraisal process [92] is emphasized and there is a general
understanding that in the digital age preservation decisions have to be taken in a
timely fashion [160].
1http://www.paradigm.ac.uk/workbook/appraisal/appraisal-approaches.html.

110
C. Niederée et al.
For the dimension content type, we focus on the semantic type of a content object
rather than on its format as a decision criterion for preservation. This is again linked
to a functional approach to appraisal.
4.4
Methods for Information Value Assessment
4.4.1
Existing Methods for Information Value Assessment
An important ﬁeld embracing the distinction between important and less important
information is Information Retrieval (IR). In this ﬁeld, importance or more precisely
relevance of an information resource for the information need of a user is underlying
the selection and ranking process of IR components. In addition to the pure content
of the documents, which are core in classical IR approaches [262], further factors
are used in modern IR approaches for information value assessment. This especially
includes the link structure of the information space, which is exploited in the popular
PageRank algorithm [318]. Additional factors are also used for information value
assessment in IR such as diversity [4] of the top results aiming for coverage and
complementarity of the search results.
A more socially oriented form of information value assessment is used in recom-
mender systems or collaborative ﬁltering [151]. In this proactive form of information
ﬁltering, the assessment of the value of a new item for a user is learned from the
behavior of “similar” users for this item. This can for example be perceived interest
(e.g., user bought this item) or ratings given for this item.
Other more efﬁciency-driven methods of information value assessment are mem-
ory caching and index pruning. In index pruning [61, 76], the value of information
in the index is assessed based on its potential to contribute to relevant results. Here,
the assessment is used to decide about cutting off parts of the index, which are not
very productive aiming for smaller and more efﬁcient indices. In memory caching,
information value assessment is used to dynamically decide, which information items
remain in a limited fast memory structure, the cache, and which are replaced by more
promising items. Caching memory plays an important role for efﬁcient web search
by caching frequent results [30, 133]. Index pruning and web caching can also be
combined [366].
Memory caching is just one of an entire class of methods, which uses usage
patterns for analyzing the importance/value of information. In [416], for example,
usage and activity patterns are used to predict users short-term interest or intent while
in [22], usage patterns are used to predict navigation behavior.
Another method used in information value assessment is spreading activation, a
concept adopted from cognitive science. Similar to the aforementioned PageRank
algorithm, resources are not considered in isolation, but rather as parts of (semantic)
networks. Along the paths created by such networks, activation can be propagated
(spread), also raising the importance of resources related to an activated (e.g., used

4
Preservation Value and Managed Forgetting
111
or accessed) resource. Spreading activation is, for example, used in the context of
search [99, 342] and in the context of Personal Information Management [196] for
improving results.
4.4.2
Exemplary Methods in Support of Managed Forgetting
Managed forgetting is a very novel ﬁeld, where no established methods exist yet.
In addition, the computation of information value, which is in the core of managed
forgetting, depends upon the respective setting and upon the type of content consid-
ered. In the following sections, we present four case studies and the related methods
for computing information value in support of managed forgetting. We start from
the computation of MB in the context of desktop data, followed by PV computation
also for semantically enriched desktop data, which is discussed in more detail in
Chap.7. In addition, we look into PV computation for personal photo collections as
an important type of personal content, which is discussed in more detail in Chap.8
of this book. Finally, we investigate the retention preferences in social media, which
addresses a mid-term information value. We thus illustrate information value assess-
ment for a variety of content types and for different time frames (from short-term
importance to long-term value).
4.4.3
Computing Memory Buoyancy with Semantic
Information
Taking up the idea of complementing human memory discussed above, the presented
method is inspired by a re-access model of human effort in (re-)ﬁnding a resource for
accessing it. The effort of re-accessing a resource depends on a variety of factors and
is related to MB. For example, if a human actor accesses a photo every day, it will take
him or her nearly no time to recall the location of the photo in his computer. In line
with existing approaches, such models must take into account the usage activities of
information resources in the past, and devise a salient way to predict the accessibility
probability of the item in the future [77, 300].
Actually, the task of computing a MB value for resources is framed as a ranking
problem. The ranking process consists of two steps. In the ﬁrst step, we mine the
activity history and devise a MB scoring function based on the recency and frequency
of access, so that more recently and frequently accessed documents get higher MB
scores. This step embraces a time-decay component for taking into consideration that
the effect of having accessed a document fades over time. This is actually true for
human brain re-access models as well as for models, which predict access based on
previous document interactions. Different time-decay models have been considered
for this purpose including polynomial decay and the Weinbul distribution.

112
C. Niederée et al.
In the second step, a propagation method is used to propagate the activation of
documents along selected connections based on a semantic network deﬁned for the
considered information space. From experiments, we have seen that a simple prop-
agation model does not perform very well, since it fails to distinguish the different
contributions of individual types of resource relationships to MB propagation. There-
fore, the revised approach introduces a machine learning framework that can learn the
contributions of individual relationship types and can combine them automatically.
4.4.3.1
Activity-Based Memory Buoyancy
In order to compute the scores for MB, we use the access times of the document
from the access history. We estimate the score of a document through the distances
of previous access time points and the time of interest. For this purpose, we start from
an activity-based MB scoring function. This is a function that takes as input the time
t and document d, and based on the access sequence Td,t computes an MB Score as
an output. If no other evidence (i.e., access of document) is present, the function is
purely driven by the decay effect. An access at time ti resets the score to its original
value. In Table4.1, we present different activity-based scoring functions studied in
this work, each corresponding to one decay function. Each of these functions only
considers the most recent access at time td, and can be used as a basic recency-based
method.
Frequency In [12], Anderson et al. suggest that the frequency of interactions also
plays an important role in the human recall of a resource, as by the relearning effect.
Hence, for each of the functions in Table4.1, we introduce a “frequency”-based
variant, which aggregates the effect of decays in different time points:
v f (d, t) =

ti∈W
vr(d, ti)
(4.1)
where vr(d, ti) can be any of the recency-based functions in Table4.1. The sequence
W ⊆Td,t represents the time window in which all time points are taken into con-
Table 4.1 List of activity-based ranking functions
Method name
Function
Parameters
Most recently used
MRU(d, t) =
1
t−td+1
None
Polynomial decay
PD(d, t) =
1
(t−td)α+1
α: Decay rate
Ebbinghaus curve
Ebb(d, t) = e
(td −t)
S
S: Relative memory strength
Weibull distribution
Wei(d, t) = e−α(t−td )s
s
s: Forgetting steepness, α: volume of what
can be remembered

4
Preservation Value and Managed Forgetting
113
sideration for the ranking. For instance, if W = Td,t and vr = M RU, we have
the well-established most frequently used method in cache replacement policies. If
W = Td,t and vr = P D, we have the decay ranking model in [320]. The Frequency
algorithm used in Mozilla Firefox [84], on the other hand, constructs W from only
the last ten items of Td,t, in order to avoid the convolution of too old accesses into
the current rank. In this work, we follow this idea, and only aggregate from the last
ten time points of accesses for each document.
4.4.3.2
Memory Buoyancy Propagation
Before we can discuss our propagation learning framework, we introduce some
formal notations. A semantic information space is a collection of documents or
resources and is denoted as D. A document or a resource d can be of different types
(photos, ofﬁce documents, folders, web pages, etc.) and has different attributes (e.g.,
titles, authors, and creation time). Given any two documents d1 and d2, there can
exist multiple relations with different semantics. For instance, d1 and d2 are both
created by the same author, d1 is the containing folder of the ﬁle d2. Relations can be
associated with some scores indicating the strength of their relation, for instance, the
cosine score for content similarity. Let R denote a set of all semantic relations. For
each pair (di, d j), we have an |R|-dimensional vector Xi j = (xi j1, xi j2, . . . , xi j|R|)T ,
where xi jk ≥0 represents the score of the k-th relation between di and d j, xi jk = 0 if
di and d j are not connected by the relation. Usually, the number of relations is small
compared to the number of all documents in the information space. The collection
of relation scores X = {Xi j} forms the weights of edges in a multi-graph, where
nodes are all documents in D, and each edge corresponds to a semantic relation.
With these notations, the problem can be formalized as follows. Given a collection
of documents D, a set of relation scores X, time of interest t, and an activity history
Lt corresponding to a user u, or to a group of users U, identify documents with
highest importance with respect to u’s or U’s task and interest at time t.
Propagation Process In this new version of propagation model, we treat the process
that the user ﬁnds the important documents as a Markov process, when she recalls
and searches for important documents via the related resources. For each pair of
connected documents (di, d j), we deﬁne the transition probability from document
di to d j as
pi j(w) =


k wkxi jk

l

k wkxilk
if Xi j ̸= ∅and Ld j,t ̸= ∅
0
otherwise,
(4.2)
where w is the weighting vector for the semantic relations in R. The condition
Ld j,t ̸= ∅ensures that the propagation has no effect on the documents that have
not been created before the time t, i.e., no propagation to the future. Similarly, the
indices l’s run only over the documents dl with Ldl,t ̸= ∅. Consequently, we have

j pi j = 1 for all documents di. In practice, to avoid rank sink when performing

114
C. Niederée et al.
the propagation, if a document has no relation, we assume a dummy edge from it to
all other documents with zero probability.
Next, we describe our propagation framework. Let P be the transition matrix of
documents in D. We follow the PageRank model to deﬁne the propagation as an
iterative process
s(n+1) = λPT s(n) + (1 −λ)v,
(4.3)
where s(n) = (s(d1, t), s(d2, t), . . . , s(dm, t)) is the vector of documents’ MB values
at iteration n, (m is the number of documents appearing in Lt), v is the vector of
values obtained by an activity-based scoring method, and λ is the damping factor.
Learning Framework: The learning aim is to identify the weights w1, . . . , w|R| of
the semantic relations which results in the best prediction of document rankings.
In this work, we propose to exploit the activity history to learn the optimal w. In
particular, we simulate the navigation of the user at each time point t′ in the past,
and compare the computed ranks of the documents with the ranks based on the
frequency of access in the time point t′ + 1. The idea is to learn w in a way, which
minimizes the number of mis-ranked pairs (d1, d2), i.e., it minimizes the situations
where s(d1, t′) > s(d2, t′), although document d1 has been accessed less than d2
until time point t′ + 1.
Formally, we deﬁne the label yi j = s(di, t′) −s(d j, t′) and the ground-truth ˆy,
ˆyi j = −1 if di has less access than d j at t′ + 1 and ˆyi j = 1 otherwise. We learn w by
solving the following optimization problem:
min
w F(w) = ∥w∥2 + θ

(di,d j)∈A
h(yi j),
(4.4)
where A is the training data, θ is the regularization parameter that controls the
complexity of the model (i.e., ∥w∥2), while minimizing the mis-ranked pairs in A via
the loss function h. In this work, we apply the simple hinge loss function: h(y) =
max(0, 1 −ˆy.y). Equation4.4 then can be solved using the well-known supervised
PageRank framework [24] employing the gradient-descent-based learning paradigm
for efﬁciency. More details can be found in our paper at [390].
4.4.3.3
Evaluation Results
The methods for computing MB described above have been evaluated in experiments,
in order to assess their effectiveness. The following baselines have been used in the
evaluation:
Recency–Frequency: These baselines use values of the activity-based scoring func-
tions to provide the ﬁnal ranking without propagation. This includes the two recency-
based methods MRU and Ebb, and their frequency-based variants, denoted as FMRU
and FEbb (details in [390]).

4
Preservation Value and Managed Forgetting
115
PageRank: This baseline ranks the documents by their authority scores, estimated
in a graph of documents relations. The scores of documents are initialized equally.
It can be thought of as the propagation method without the activity-based rankings
and without a differentiation based on relationship types. In our case, we adapt the
PageRank algorithm by aggregating all relations between two documents into one
single relation, with the weighting score obtained by averaging out all the individual
relation weights.
SUPRA: Papadakis et al. [320] proposed combining the activity-based ranking results
with a one-step propagation in a layered framework. The relations are constructed
simply by identifying documents accessed in the same sessions. In our scenarios, we
deﬁne the “sessions” to be one unit time step, which is one hour.
The results of the evaluations are reported in detail in [390] and relevant ﬁndings
are summarized in what follows.
The ﬁrst experiment aims to evaluate how well the system performs in the revisit
prediction task, i.e., predicting the likelihood that a document will be accessed by the
user in the subsequent time point. This is the well-established task in research on web
recommendation [84], personal ﬁle retrieval [139], etc. We evaluate the correlation
between the predicted rank of a document at a time point t and the real document
accesses at the time point t + 1.
Inspired by [197], we have employed the following evaluation metrics:
• Success at 1 (S@1): It quantiﬁes the fraction of time points t (from all time points
of study) at which the ﬁrst-ranked documents according to a ranking method are
truly accessed at t+1. This resembles the Precision at 1 (P@1) metric in traditional
IR tasks.
• Success at 10 (S@10): It quantiﬁes the fraction of documents truly accessed in
the next time point, from all documents ranked at top 10, averaging over all time
points of study in the micro-average manner (i.e., per-document average).
• Average Ranking Position (ARP): This metric starts from the subsequent doc-
ument access backwards. It computes the average ranking position of accessed
documents as produced by a ranking method. The lower the value, the better the
performance of the corresponding ranking system.
From the results of the experiments with the revisit prediction task, some inter-
esting insights can be gained. Among the ranking methods, PageRank has the worst
predictive performance. This is because it ignores the recency and frequency signals
of the documents. Another interesting observation is that for activity-based ranking
methods, adding frequency into the ranking function did not really help in revisit
prediction: FMRU performs worse than MRU and FEbb performs worse than Ebb in
all metrics, although the differences are not signiﬁcant. At the ﬁrst glance, this con-
tradicts somewhat to previous ﬁndings on the inﬂuence of frequency in document
ranking [322]. However, after a deeper analysis, we believe that the cause stems
from the fact that a revisiting action typically involves very recent documents, as
also argued in [197]. Aggregating recency scores over a time span (10-day window
as in our case) can introduce some documents belonging to different tasks and thus

116
C. Niederée et al.
bring more noise to the ranking results. One direction for future work is thus to design
a more ﬂexible time window size which adapts to the user’s task.
Compared to the sole activity-based ranking methods, adding propagation shows
clear improvements in prediction, starting from the baseline SUPRA. Bringing
semantic relations into the propagation improves even further, producing signiﬁ-
cantly higher performance for all cases of temporal priors. Our best performing
method, propagation with polynomial decay prior, improves the results by 60% as
compared to SUPRA. In addition, in contrast to the observed trend in the activity-
based ranking, here the combination of frequency and recency with the propagation
actually produces better results than the combination between only recency and the
propagation.
We also evaluate the effectiveness of our proposed system with respect to the
user perception and appreciation. We do this by simulating the way users re-access
and reassess the documents in their collections. For this purpose, we performed an
experiment with a desktop data set of selected users. Actually, the Semantic Desktop,
as discussed in more detail in Chap.7 was used for this purpose.
In the dataset Person, each assessor chose 4 weeks to evaluate. For the dataset
Collaboration, two assessors are asked to choose 3 weeks per each, all are related
to joint events they participated in. The activity history is constructed according
to the activities of to this pair of users. The ranking methods are conﬁgured to
provide the ranks of document with respect to the same time step of the user’s
evaluations. The same trend as in the prediction task can be observed here: The
activity-basedrankingmethodsperformbetterthanPageRankbutworsethanSUPRA
andourpropagationvariants.Similarly,thefrequency-basedfunctionsperformworse
than the recency ones as isolated methods, but improve the results when combined
with the propagation. All propagation methods except the MRU prior-based give
higher results than SUPRA. In addition, the performance of all methods in the user-
perceived study is slightly higher than in the prediction task. This suggests that many
of the suggested documents, although not accessed subsequently, are still deemed
“important” by the user.
4.4.4
Computing Preservation Value for Desktop Data
For computing PV for desktop data, the Semantic Desktop [273] has been used—a
desktop system, which relies on semantic annotation of content and an ontology
representing the conceptual information model of the user. The Semantic Desktop
is a powerful approach to support organizational as well as personal knowledge
management. In computing PV, evidences are used along the PV dimensions of con-
tent, type, investment, gravity, social graph, popularity, coverage, and quality. For
example, number of annotations and text related to a resource are used to measure
investment, the closeness of resources and the importance of some resources (also
dependent on their type) are used to consider gravity as well as content type, and

4
Preservation Value and Managed Forgetting
117
image quality is used for the quality dimension. This method for preservation com-
putation is discussed in more detail in Chap.7.
4.4.5
Computing Preservation Value for Photo Selection
When considering preservation, photos are an important type of content to be pre-
served for the long run. For photo selection, a user study on this topic has been per-
formed for understanding user expectations with respect to photo collection preser-
vation. An important ﬁnding from the user study is that the objective quality of photos
is rated as the second least important selection criterion, after the sharing intent. This
shows that quality and aesthetics, although being important and used for “general
purpose” photo selection, is not considered very important in the case of selecting
images for preservation. In contrast, criteria more related to reminiscence, such as
event coverage, typical image, and “the picture evokes (positive) memories” are all
rated high, with highest ratings for memory evocation.
Exploiting modern multimedia analysis methods as they are described in Chap.3
of this book and using the results of the user study, an automatic method for predicting
long-termimportanceofphotoswithinacollectionhasbeendevelopedandevaluated.
It relies on a number of the aforementioned PV dimensions including the dimensions
coverage (in the form of event coverage and the consideration of near duplicates),
investment (e.g., when multiple photos are taken of the same situation), and quality
(looking into photo quality). The user study and its results as well as the developed
photo selection method are discussed in more detail in Chap.8 of this book.
4.4.6
Investigating Retention in Social Media
In this case study, we looked into the idea of PV or the wish for retention in a social
media context. Preservation of human generated content in social media is another
important scenario for preservation of personal content. In our study, we took into
account the different dimensions of PV by using not only the content itself but also
different types of features, e.g., for the popularity of content and the social graph
and the relationship of the persons relevant to the content. In this study, we analyzed
relevant features for content retention. In addition, we also investigated a method for
the automatic recognition of memorable posts based on an adequate training set and
a machine learning approach.
4.4.6.1
User Study on Social Media Content Retention
In order to better understand human expectations and build a ground-truth of mem-
orable social network posts, we set up a user study on top of the Facebook platform.

118
C. Niederée et al.
The main goal of this evaluation was to collect participants’ opinions regarding
retention preferences for their own Facebook posts, shares, and updates of Facebook
posts from different time periods. For facilitating the participation, we prepared an
intuitive evaluation in a form of Facebook apps. We took extra care regarding the
participants’ privacy and to comply with Facebook’s Platform Policies.2 A summary
of the user study can be found in [117].
In the study, each participant had to judge their own posts on a 5-point Likert scale
answering the following question: Which posts do you think are relevant and worth
keeping for the future needs? Once a post is rated using a 5 points scale starting from
0 (irrelevant) to 4 (extremely relevant), it fades out providing more space for posts
to scroll up.
We asked participants to judge about 100–200 of their posts. It is important to note
that we are not judging participant’s memory skills, but instead we are collecting their
personal opinions. Due to that, we presented participants’ posts in a chronological
order starting from the latest. In this case, for more active users, 100 posts may date
back to just a few days, reaching up to months for less active ones.
In order to not only get ratings for the most recent posts shown to users (as it
can be biased in displaying posts), we picked starting time points differently among
participants in our user study. We deﬁned three distinct groups of participants with
respect to the recency of the posts they evaluated. Each participant was randomly
assigned to one of the groups at the beginning of evaluation (according to his/her
Facebook id). Participants in Group 1 (recent) were assigned to evaluate posts starting
from the most recent ones (February 2014) in their timelines. Participants in Group
2 (mid-term) were assigned to evaluate posts starting from January 2011 and back to
this date. Finally, Group 3 (long-term) received posts from January 2009 and before
(if available). This results in rated posts spanning from 2007 to 2014.
The study was performed between the second week of November 2013 and the
third week of February 2014. We had 41 participants, 24 males, and 17 females, with
age ranging from 23 to 39 years. In total, there are 8494 evaluated posts. Additionally,
once the users provided us authorization to access their data, we were able to collect
general numbers that help us to depict the general use of Facebook social network.
From 2008 to 2014, on the average, there are about 30 to 100 posts annotated per
year and user. For 2007, only three participants evaluated posts, thus we discarded
posts from this year for most of the analysis.
Facebook deﬁnes seven types of posts, namely, link, checkin, offer, photo, ques-
tion, swf, and video. This basically describes the type of content that is attached to
a post. In the dataset, we found the following distribution among these categories:
42.5% of evaluated posts consists of status updates, followed by shared links (33%),
photos (19%), and videos (4%). We disregarded swf, offer, and checkin types, which
do not have sufﬁcient occurrences to be signiﬁcant. We also investigated the distri-
bution of different content types over years. Our observation is that there is a clear
increase in the use of videos and photos over time. Several factors help us to explain
this change in behavior. First, the catch up of broadband connection allowed users
2https://developers.facebook.com/policy/.

4
Preservation Value and Managed Forgetting
119
Fig. 4.2 Average ratings of
social media content by
content type
to quickly upload large amounts of data (photos and videos). Second, the dissemina-
tion of smart phones with embedded cameras played an important role. Nowadays,
anyone can quickly take a snapshot and upload it on the web. Statistics from photo
sharing website Flickr3 show that the most used cameras are, by far, embedded
smart phone cameras.4 The rate of links and status information changes over years,
however, there is no clear trend seen.
In addition, the second characterizing ﬁeld (so-called status_type) deﬁnes prede-
ﬁned actions taken by the user. For example, the type ‘photo’ can have status_ type
‘added_photos’, ‘tagged_in_photo’, or ‘shared_story’. Due to space limitation, we
left out the analysis of these types, nevertheless, we use these features in our experi-
ments. The distribution of ratings in the full dataset shows a clear dominance (57%)
of irrelevant posts (with rating 0). This is followed by 16% of posts with rating 1;
13% with rating 2; 8% with rating 3 and 6% of top-rated posts (with rating 4). This
results in an average rating of 0.92 with standard deviation 1.58 and variance 1.26.
When looking into the individual content types (see Fig.4.2), we found out that
photos have the highest average rating of 1.94 followed by videos with an average
rating of 1.27. The average ratings of status updates (0.71) and links (0.54) are much
lower suggesting a dominating role of photos and videos for memorable posts.
Besides other features relevant for identifying memorable posts, we are interested
in the role of time in deciding about content retention, i.e., whether older content on
the average is rated lower than more recent content. For this purpose, we analyzed
the dependency between content ratings and the age of content.
In Fig.4.3, the dash line shows the average rating for the different years of content
creation.
The image shows a clear trend where participants in the evaluation assigned
higher ratings to more recent posts. This is in line with the idea of a decay function
underlying the content retention model. The decrease in the average rating with
growing age of content is especially steep in the ﬁrst year (2013/2014). However,
we also see an increase in the rating values in 2008, where we leave this for further
investigation. In Fig.4.3 (solid lines), we see the development of the average ratings
for individual content types (videos have only been included starting in 2010, because
3http://www.ﬂickr.com.
4http://www.ﬂickr.com/cameras.

120
C. Niederée et al.
Fig. 4.3 Observed average ratings for increasing content age (the dash line), and the average ratings
for individual content types over increasing content age (solid lines)
Fig. 4.4 Distribution of the ratings per content types over years
the number of videos in the dataset before 2010 is very small). Once more, we
observe an increase of ratings for the most recent content. However, we also see
very high average ratings for older photos (older than 5 years). Thus, we assume that
seeing these older (already forgotten) photos again caused some positive surprise,
which resulted in higher ratings. This perception would support the idea of creating
Facebook summaries for reminiscence. However, this still would require further
investigation, since unfortunately only a rather small number of photos was available
for rating in 2008 and thus the observation is only based on a rather small data set.
For analyzing the temporal behavior in more detail, Fig.4.4 shows the distribution
(ratio) of ratings 0–4 among the content categories over years (content age).

4
Preservation Value and Managed Forgetting
121
Besides the dominance of content rated as irrelevant (blue line, rating 0), we
can see a clear decrease of content rated as irrelevant for the more recent content.
Accordingly, there is an increase of content rated relevant, which is shown in the
green line as an aggregation of content rated from 1 to 4. An increase of positively
rated posts can especially be seen for content rated as very relevant (red line, rating
4), which increases its rating by a factor of 4 from 2010 to 2014. Figure4.4 also
supports the idea of a decay function (forgetting) when assessing the memorability
of older posts.
On Facebook, users are able to comment or like a particular post. Comments are
a very common feature on the web. Any user (with a adequate permission) is able
to express her opinion on a subject. Like is a particular feature of Facebook where
users vote to show their appreciation on a given post. Both actions have a limited
audience that is imposed by the author of the original post. In the most common case
(default setup), a user’s post is visible for her network of friends, and those are the
ones that are able to comment and like the post. Our ﬁrst analysis of results already
suggests that the average number of comments and average number of likes for the
evaluated posts have an impact on the rating of the post: the higher the number of
likes and number of comments, the higher the rating.
From these statistics, one can deduce ﬁrst ideas for determining features that have
a high impact in the identiﬁcation of memorable posts. Roughly speaking, recent
photos with high number of likes and high number of comments seem to be the best
evidence.
User Feedback: To further complement our user studies, we asked the participants
to ﬁll a short survey in order to give us qualitative feedback to understand people’s
Facebook habits. All participants stated that the main use of Facebook is for commu-
nication, 83% claimed using Facebook in private life matters and 50% for business.
Additionally, 58% of the participants claimed to use Facebook as a personal archive.
Regarding this preservation aspect, half of the participants claimed to have deleted
(at least once) older posts. Almost 74% of the participants also stated that they have
removed their tag (once or more) associated to a particular picture. Interestingly,
when asked if they “care” about their proﬁles and timelines, only 50% answered yes,
contradicting with the fact that they sometimes “clean” their proﬁles.
To summarize this section, we performed a user study in order to collect a ground-
truth for memorable posts. The primary data analyzes show that most of the posts
(around 60%) are considered expendable and, temporal aspects have signiﬁcant
impact on memorability perception. In this light, in the following sections, we present
a series of experiments in order to empirically uncover the best features that identify
memorable items.

122
C. Niederée et al.
4.4.6.2
Features for Retention
The annotated dataset provides the basis for building models for ranking memorable
items. Feature selection experiments allow to identify a compact core feature set for
the task at hand.
For capturing factors that might inﬂuence retention decisions of users, we com-
piled a broad set of 139 features. They can be categorized in ﬁve groups: temporal,
social interactions, content-based, privacy, and network. The inclusion of temporal
features is inspired by the idea that retention preferences are inﬂuenced by a decay
function as it was also conﬁrmed by the data analysis in the previous section.
For temporal features, we consider the temporal aspect of the post in terms of
creation date, age, and lifetime. While “age” is the time between the evaluation and
creation date, “lifetime” is measuring the active time of a post starting at the time it
was created to the last update. We also use variants of the age feature, which use the
time of the last update and the time of the last commenting, respectively, instead of
the creation time.
The social features capture core signals of social interaction in a social web appli-
cation, covering the features that are typically used in Facebook analysis: number
of likes (No.Likes), number of comments (No.Comments), and number of shares
(No.Shares).
The next group of features are content-based features. We use the type of the
post and some speciﬁc meta data about the post. This is based on the content-based
features offered by Facebook and includes, for example, features such as status type,
type, hasLink, hasIcon, app type, etc. We map each categorical feature (like status
type) to multiple binary features. To respect user privacy and the privacy policies, the
only text-based feature in our set is the length of text included in posts and comments.
In other words, we do not utilize the textual content of posts.
The privacy features are based on the privacy options offered by Facebook, which
are used to restrict the access to a post to a particular set of user.
Furthermore, for each post, we compute standard network measures, such as,
cluster coefﬁcient, density, diameter, path length, and modularity. The network fea-
tures are extracted from the network of all users involved in a post, and also on the
networks of likes and comments.
We also use a personalized normalization (Pers.) for the social and network fea-
tures based on the average values of the collections of individual users. This better
adapts the features to the individual characteristics and behavior of individual users.
Including the normalized version of the features, we use 5 temporal, 6 social features,
47 content-based, 13 privacy, and 68 network features. The number of network fea-
tures is relatively high since we analyze the typical network features for the different
types of social graphs that are implied by Facebook interactions separately (e.g.,
likes, messages, and comments), and also include a normalized version of each of
these features.

4
Preservation Value and Managed Forgetting
123
Fig. 4.5 Effectiveness of ranking
4.4.6.3
Learning Models for Ranking with Feature Selection
Based on the candidate features, our goal here is ranking a user’s posts to identify
the most memorable ones as it is a crucial stage for various interesting applications,
such as constructing targeted summaries. To this end, we adopt strategies from the
web search domain, where machine-learned rankers are heavily investigated and, as
far as we know, incorporated into commercial search engines [245]. If we make an
analogy, a user in our case corresponds to a query in the search setup, and user’s
posts correspond to the documents retrieved for the query. During the training stage,
for a given user u, we construct an m-dimensional feature vector F for each post of
this user, and augment the vector with the label r assigned to this post (obviously,
labels are the ratings collected in the user study, after a mapping to a range 0–4 as
the posts rated as 1 are not at all considered memorable). For the testing, we feed
vectors in the form of < u, F > to the learnt model for each user in the test set; and
the model outputs a ranked list of posts for each user. We evaluate the success of the
ranking using a typical metric from the literature, namely, Normalized Discounted
Cumulative Gain (NDCG), which is a rank sensitive metric that takes into account
graded labels. We report NDCG scores at the cutoff values of {5, 10, 15, 20}.
In the experiments, we employ a well-known algorithm, namely RankSVM, from
learning-to-rank literature [185]. Instead of single data instances, RankSVM consid-
ers pairs of instances (posts of a user, in our case) while building a model. We apply
leave-one-out cross validation.
Figure4.5 reveals the performance of RankSVM for ranking posts using all the
proposed features. As a baseline, we also train a model using three basic social
features, namely, the number of likes, comments, and shares. We choose the latter
features for the baseline ranker as they are the most intuitive popularity signals
in social web and very likely to be involved in practical applications, such as the

124
C. Niederée et al.
Facebook’s post ranking applications. The results show that our proposed features
are actually very useful, and using these features for training a ranker yields relative
effectiveness improvement of up to 26% over the baseline ranker trained with the
basic social features (i.e., comparing the ﬁrst and last bars for each metric in Fig.4.5).
The next question we address is: Can we identify a subset of the proposed fea-
tures that has the highest impact in ranking memorable posts? While feature selection
methods are widely applied for various classiﬁcation tasks, only a few works have
investigated their performance in a learning-to-rank framework [108, 116, 146].
Here, we adopt two ﬁltering-type approaches: In the ﬁrst approach, so-called TOP,
we use each feature on its own to rank the posts of the users, and then choose top-N
features with highest effectiveness in terms of the NDCG@10 scores [108]. Sec-
ond, we adopt the so-called GAS (Greedy search Algorithm of Feature Selection)
introduced by Geng et al. [146]. In GAS, we again compute each feature’s isolated
effectiveness, but additionally, we also compute pairwise feature similarity, i.e., to
what extent the top-10 rankings generated by two different features correlate. To
compute the similarity of two ranked lists, we use Kendall’s Tau metric. Then, the
feature selection proceeds in a greedy manner as follows: In each iteration, ﬁrst
the feature with the highest effectiveness score is selected. Next, all other features’
effectiveness scores are discounted with their similarity to the already selected fea-
ture. The algorithm stops when it reaches the required number of features, N. We
experiment for all possible values of N, from 1 to 138 (as N = 139 is the case with
all features), and evaluate the performance. Figure4.5 also demonstrates that feature
selection with TOP strategy (for the best performing value of N, which is found to
be 41) cannot outperform models trained with all available features. However, using
GAS strategy and with only 30 features we can achieve the same effectiveness as
using all 139 features (i.e., compare the third and last bars for each metric in Fig.4.5).
For this latter case, we analyze the features selected by GAS in each fold to identify
the most promising features for the task of ranking posts. As the absolute value of
the weights assigned to features by the RankSVM model (built using a linear kernel)
can reﬂect the importance of features [72], we average these absolute values for the
features appearing in the learnt model in each fold. Table4.2 shows the features with
the highest average scores in the models built after feature selection with GAS.
We see that the most discriminative features cover all four feature categories
except the privacy. Regarding top-10 features in the list, content-based and social
categories, with 4 and 3 features respectively, are the most inﬂuential ones. We also
observe that features that are adapted to the behavior pattern of individual users such
as pers.no.likes play an important role for ranking memorable posts. Interestingly,
the feature content type is outperformed by the feature status type. Our explanation
is that status type can be considered as a reﬁned form of content type, which captures
what has been done with the content (e.g., “added photo”). This more ﬁne grained
information has proven to be more important for the ranking task than the information
on the content type alone. Network features computed from the graphs in the social
network have less impact for the characterization of memorable posts. And ﬁnally,
the temporal feature age is also among the top-10 features, a ﬁnding in line with our
earlier claims on the relationship of the time and human retention preferences.

4
Preservation Value and Managed Forgetting
125
Table 4.2 Best features
selected by GAS—core social
features (baseline) denoted
by∗
Category
Feature
Weight
Content-base Pers.Length Description
1.676
Social
No.Shares*
1.190
Social
Pers.No.Shares
0.471
Social
No.Likes*
0.453
Network
Overlap.No.Friends (all)
0.392
Content-base Status_Type
0.329
Temporal
age (created time)
0.327
Content-base Pers.Length.Story
0.326
Network
Cluster Coefﬁcient
0.320
Content-base Length Message
0.269
Social
Pers.No.Likes
0.260
Content-base APP_Type
0.259
Content-base hasMessage
0.200
Content-base Length Story
0.199
Content-base hasDescription
0.167
Content-base hasStory
0.151
Content-base Type
0.140
Content-base Length Comments
0.140
Content-base Pers.Length Message
0.115
Temporal
CreatedTime
0.112
Network
Density (all)
0.091
Social
Pers.No.Comments
0.065
Content-base Length Description
0.063
Network
Overlap.No.Friends (likes)
0.059
Network
Path length (like)
0.023
Network
Overlap.No.Friends (tagged)
0.009
Social
No.Comments*
0.006
The results summarized in Fig.4.5 are encouraging, because, even in a relatively
small setup with 41 users in total, our approach considerably improves the NDCG@5
and NDCG@15 scores in comparison to using a single ranking model (i.e., compare
the ﬁrst and last bars in the ﬁgure for each metric). Finally, we also experimented
with feature selection in this setup, where we applied Top-N and GAS strategies.
In Fig.4.5, we only report the results for the better performing GAS strategy. It

126
C. Niederée et al.
turns out that feature selection using GAS strategy is outperforming the basic social
features signiﬁcantly, denoted with ⋆in Fig.4.5, as well as the set of all features for
NDCG@10, NDCG@15, and NDCG@20.
4.5
Forgetting Actions—Leveraging Information Value
The raw information value, as it has been discussed in the previous sections is only
part of the managed forgetting approach, although a very central one. It describes the
short or long-term value of an information object. In this section, we investigate how
information value—PV as well as MB—can be used as part of the managed forgetting
approach. There are different ways of leveraging those types of information values.
First of all those values can be made accessible to users for supporting users’
decisions (see Sect.4.5.1). This is especially helpful if it is combined with an ade-
quate form of visualization or categorization. Second, such values can be used by
other information management and information access methods such as search for
reﬂecting the forgetful approach, such as implementing a forgetful version of search
(see Sect.4.5.2). Finally, those information values can be used for supporting and/or
partially automating decision making. This is especially relevant in the context of
preservation decisions (see Sect.4.5.3).
4.5.1
Informing the User
The value coming from information value assessment can be directly used to inform
the user about the current value or importance of an object. Such numeric values are,
however, difﬁcult to interpret. Therefore, a translation into categories using adequate
thresholds might be helpful and ease decision-making based on the information
values MB and PV.
Fig. 4.6 Visualization
options/logical categories for
Preservation Value (left) and
Memory Buoyancy (right)

4
Preservation Value and Managed Forgetting
127
This is especially important for decisions with respect to preservation. Information
objects can for example be grouped into the categories such as gold, silver, bronze,
wood, and ash based on their PV as shown in Fig.4.6. Similar category sets can be
imagined for MB values, where “temperature” might serve as an adequate metaphor
for short-term importance. A possible category set for short-term importance, thus,
could be hot, warm, lukewarm, and cold. Such human-readable labels make it easier
for the user to interpret the respective information value.
Those categories can also be used in visualization of the respective information
objects. This may include visual highlighting of important items, e.g., framing in
strong colors, marking, etc. At the opposite site of the range, less important items
can be visually unstressed up to hiding information objects, which are currently not
important. This leads directly to the use of information value in information access
methods as it is discussed in the next subsection.
4.5.2
Forgetful Methods
Information value can also be incorporated into other methods used for informa-
tion access and information management. The aforementioned visualizations, which
support highlighting and/or hiding selected information objects, are already a ﬁrst
example incorporating information values, e.g., in a ﬁle management system.
More demanding ways of incorporating information value are possible, when
combining forgetting and search into what we call forgetful search. Here, the rele-
vance assessment of an information objects based on its content and the query can
be combined with the current importance of the object according to the information
value assessment.
The easiest way of combination is by use of a threshold. Only information objects
with a information value above a speciﬁc information value (the threshold) are incor-
porated into the search result, i.e., information objects below the threshold although
relevant to the queries are hidden in the search or excluded from the search result.
More advanced forms of forgetful search can be achieved by ﬂexibly combining the
impact of relevance and information value in a mixed formula.
Other forms of forgetful methods refer to different forms of reduction and aggre-
gation. Less important information items might be summarized or condensed in other
ways (e.g., for images). Furthermore, redundancies in the form of duplicates or near
duplicates can be reduced by just keeping one or a smaller number of the (near)
duplicates. This aspect has already been raised in the context of the PV dimensions
and is further elaborated for the photo section method presented in Chap.8.

128
C. Niederée et al.
4.5.3
Supporting Preservation Decisions
The most obvious way of using the PV is to make decisions about preservation.
A threshold can be set and only resources with a PV above this threshold will be
considered for preservation (i.e., sent to the preservation system). Such a process can
be automated or the resources above the threshold can be presented to the user for
ﬁnal decision.
One possible time point for preservation decisions, is when the respective resource
goes out of active use (decreased MB for a speciﬁed time period). Before this point in
time, it might be expensive to keep track of the changes of the resource in the active
system and to synchronize them with the resource copy in the preservation store.
Waiting until long after the active life of the resource will put resources with high
PV at risk. Other scheduling options would be a regular (e.g., monthly) computation
of PV with preservation decisions or a manually triggered PV computation (e.g., for
a collection of photos or the resources of a project, which has been completed).
The Preservation Value cannot only be used for binary preservation decisions:
preserve or not preserve. It has to be noted again that “not preserve” does not mean
“delete”. It just means the resource is not sent to the preservation system. It might still
resideintheactivesystemoranytypeofbackupsystem.ThePVoracategoryinferred
from this value can also be provided to the preservation system as an indication of
the effort to be invested for the respective resource. As mentioned above, human-
readable categories such as “gold”, “silver”, “bronze”, etc., can help the user in
getting an intuitive understanding of PV. The PV (or PV categories) can be used by
the preservation system to select the adequate preservation levels (e.g., how much
redundancy, how much effort to put into conversions to new formats etc.). In addition,
the PV can also be used for deciding about other preservation options such as just
preserving a summary of a collection of images with low PV or storing more context
information for a resource with high PV (see discussion on redundancy above). Such
actions on top of the PV can be build into a preservation system for achieving more
intelligent, value-aware preservation systems.
4.6
Conclusions
Inthischapter,wehaveintroducedtheconceptofmanagedforgettingasanalternative
to the currently dominating keep-it-all-(and-live-with-random-survival) strategy. We
have presented its opportunities and challenges as well as ﬁrst steps towards its
implementation.
Information value assessment, which is in the core of managed forgetting, has been
considered from a short-term and a long-term perspective resulting Memory Buoy-
ancy (MB) as an information value linked to current importance of a resource and
Preservation Value (PV) related to expected long-term beneﬁt, respectively. Assess-
ing information value especially with a long-term perspective is a challenging and

4
Preservation Value and Managed Forgetting
129
setting-dependent task, since it implies anticipating future uses of a resource. In the
chapter, we also presented ﬁrst methods for computing MB and PV for different
settings such as social media content and personal desktop data with further methods
beingdiscussedinChaps.7and8.Thediscussionofinformationvalueassessmenthas
been complemented by the presentation of various forgetting actions, i.e., methods
for leveraging the computed information value for achieving a full implementation
of managed forgetting in support of more effective information management and
preservation solutions.
It is, however, clear that further research is still required for a full realization of
the novel approach of managed forgetting. This includes for example methods for
information value assessment for further settings, the implementation and evaluation
of further forgetting actions, and an improved understanding of the interaction and
possible synergies between human forgetting and digital forgetting.

Chapter 5
Keeping Information in Context
Mark A. Greenwood, Nam Khanh Tran, Konstantinos Apostolidis
and Vasileios Mezaris
Abstract Without context, words have no meaning, and the same is true for docu-
ments, in that often a wider context is required to fully interpret the information they
contain. For example, a family photo is practically useless if you do not know who
the people portrayed in it are, and likewise, a document that refers to the president of
the US is of little use without knowing who held the job at the time the document was
written. This becomes even more important when considering the long-term preser-
vation of documents, as not only is human memory fallible, but over long periods
the people accessing the documents will change (e.g. photos passed down through
generations), as will their understanding and knowledge of the world. While pre-
serving the context associated with a document is an important ﬁrst step in ensuring
information remains useful over long periods of time, we also need to consider how
information evolves. Over any signiﬁcant time period, the meaning of information
changes. This evolution can range from changes in the meaning of individual words
to more general terms or concepts, such as who holds a speciﬁc position in an orga-
nization. In this chapter, we look in detail at all of these challenges and describe
the development of a conceptual framework in which context information can be
collected, preserved, evolved and used to access and interpret documents. A number
of techniques are presented showing real examples of context in action that ﬁt within
the framework, and applying to both text documents and image collections.
M. A. Greenwood (B)
The University of Shefﬁeld, Western Bank, Shefﬁeld S10 2TN, UK
e-mail: m.a.greenwood@shefﬁeld.ac.uk
N. K. Tran
L3S Research Center, Leibniz Universität Hannover, Appelstrasse 9a,
30167 Hannover, Germany
e-mail: ntran@l3s.de
K. Apostolidis · V. Mezaris
Information Technologies Institute (ITI), Centre for Research
and Technology Hellas (CERTH), 57001 Thermi, Greece
e-mail: kapost@iti.gr
V. Mezaris
e-mail: bmezaris@iti.gr
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_5
131

132
M. A. Greenwood et al.
5.1
What Is Context?
You shall know a word by the company it keeps.
J. R. Firth, 1957
In its simplest form, context is external information which is either required or aids
in understanding an ‘item’. This extra information may just be a few words to help
disambiguate a single word [137] (is a bulb a light bulb or the root of a plant; without
context you cannot be sure) or it might be large quantities of extra information
(understanding a document on geopolitics, for example, may require a lot of extra
background information). Time also plays an important factor in what context is
needed. Terms that are unambiguous in today’s world can easily fall out of use or
their use change over time. A good example of this is the word computer which used
to refer to a person employed to do computations, a meaning which many people
today are unaware of. This means that while many concepts might not need to be
explained now, their meaning would need to be captured as context information and
preserved so that they could be unambiguously interpreted in the future.
Broadly speaking, there are two main types of knowledge, world and personal,
that can provide context for a document. World knowledge is usually the same for
everyone and describes the world in which we live. This kind of knowledge is already
collected and encoded in a number of successful projects, such as DBpedia and
Wikidata. Barring a large-scale digital dark age [212], this general knowledge will
continue to be accessible even in the distant future. Personal knowledge, however,
is often never written down, and once lost cannot be recreated, so it is exceedingly
important that this information be captured and archived if personal documents are
to be preserved and kept useful over long periods of time. Personal knowledge does
not, of course, have to refer simply to an individual. Organizations also have a
world view and internal domain knowledge that is unique to them. When preserving
company documents, capturing this organizational memory is just as important as
capturing world knowledge and the personal knowledge of the person who authored
the document.
The rest of this section gives an overview of different types of context and shows
how it might be gathered and used through an illustrative example, before the more
technical aspects are discussed later in the chapter.
5.1.1
An Illustrated Example
Rather than focusing on the technical details, we shall start by looking at an example
illustrating the main concepts, which paved the way for the development of many
of the components documented in the rest of this chapter. This revolves around a
diary entry and a photo covering a single event that occurred during a meeting held
in Luleå, Sweden.
If we assume that this diary entry is the ﬁrst document (text or image) that we
are preserving, then we will have no existing personal store of world knowledge

5
Keeping Information in Context
133
Jörgen had offered to take Elaine, Maria, and Robert out to Gammelstad this morning before the
meeting started as they are looking at running a memory study there over the summer. When Jörgen
arrived to pick them up I decided to be cheeky and ask if there was room in the car for one more.
There was so I got to do the touristy thing of looking around while everyone else did some actual
work. A large snow pile made an excellent back drop for Robert to take a group photo which
hopefully I’ll get a copy of at some point. It was certainly an interesting place to look around and
you can understand why it is a UNESCO world heritage site.
Diary Entry 5.1 File metadata list the author as ‘Mark A. Greenwood’ and the creation data as
the 12th of February 2014
to call upon. The ﬁrst stage of contextualization will therefore be to link the doc-
ument to a source of world knowledge, which for this example we will assume is
Wikipedia (implementations of this idea are more likely to use DBpedia, but for a
handworked example, Wikipedia makes more sense). In this instance, that would
involve generating links to the pages for Gammelstad1 and UNESCO,2 as they are
the two famous entities within the text. This leaves ﬁve entity mentions (Jörgen,
Elaine, Maria, Robert and I) which, if not well known, must fall within the user’s
personal knowledge.
With no pre-existing store of personal knowledge available to act as context, the
only information we have is the ﬁle metadata, which allows us to map the ﬁrst person
pronoun I to the document’s author Mark A. Greenwood. This leaves us with four
unknown people for which the system would need to prompt the user for additional
data. At a minimum, this additional data should probably consist of a person’s full
name and their relationship to the user (i.e. Robert Logie is a collaborator on the
ForgetIT project and works at The University of Edinburgh). Alternatively, such
information could be available from the user’s own information sources, such as
(electronic) address book, email account or calendar.
Having gathered this information, the next step would be to store the current
personal knowledge in an archive. The diary entry could then be contextualized
by storing not only the entry itself but also links to both the world and personal
knowledge.
In addition to allowing us to store extra context information alongside the diary
entry, this has also started the process of building up a repository of personal knowl-
edge which can in turn be used to contextualize new documents more accurately and
with less user intervention. It would be beneﬁcial if this linking process were part
of an interactive feedback loop [150] so that users could see the beneﬁt of existing
data being used to enhance their output (e.g. links to Gammelstad and UNESCO
appearing magically) as this would encourage them to provide personal data when
prompted.
Having now fully contextualized the diary entry, let us turn our attention to the
task of contextualizing and preserving Photo 5.1. The ﬁrst thing to note is that the
photo and the diary entry clearly act as context for one another. If both were archived
1http://en.wikipedia.org/wiki/Gammelstad_Church_Town.
2http://en.wikipedia.org/wiki/UNESCO.

134
M. A. Greenwood et al.
Photo. 5.1 Metadata associated with this photo show that it was taken at 7:48:33 UTC on the 12th
of February 2014 at N65◦38′42.75′′ E22◦1′38.573′′ at a magnetic bearing of 171.5◦. Copyright c⃝
Robert Logie
at the same time, this context would be obvious, but as we are assuming that the photo
is being preserved at a later date than the diary entry, then this link needs to be made
clear. This highlights the fact that one important source of contextual information
is the documents which have already been archived. Previously archived documents
can act as context in two ways:
• An item being preserved can refer to previously preserved items. In this case, a
link between Photo 5.1 and Diary Entry 5.1 could be explicitly recorded to provide
contextual information that explains both the occasion of the photo and its content
(i.e. the people and location).
• The processing of each new item adds to what we know about the individual user’s
personal knowledge. In this example, contextualizing the diary entry will have led
to four previously unknown people being added to the user’s personal knowledge.
It is likely that the explicit linking of the diary and photo would be a manual action
taken by the user. It may, however, be possible to suggest the relationship to the user
based on the associated metadata and context information generated when the diary
entry was preserved. First, the photo and the diary were both created, according to
the metadata, on the same day (the 12th of February 2014), which at least suggests
a common context. Furthermore, the GPS information can be used to search for

5
Keeping Information in Context
135
Photo. 5.2 A wider context? Copyright c⃝Mark A. Greenwood
Wikipedia pages describing nearby places which would link the photo to the same
page describing Gammelstad3 as used to provide world knowledge for the diary entry.
Combining the GPS data with magnetic bearing (if available) and the focal length
of the shot, a list of places that may be visible can also be extracted and proposed
to the user. An example of place information for Photo 5.1 (based on the GPS info
listed in the caption) is given below:
Visible Name
Type(s)
Distance
No
Luleå V
Sublocality level 1 13626m
Sublocality
Political
No
Äldreboende Ingridshem
Establishment
290m
No
Snickare Anders Viklund i Luleå AB Establishment
225m
No
Öhemsvgen
Bus station
225m
Transit station
Establishment
The GPS metadata associated with the photo could also be used to select other
photos which could act as context. In this example, Photo 5.2 has almost identical
GPS metadata, but clearly shows a wider view than the original photo and would
help to provide a larger visual context.
In a similar way to textual documents, images can be contextualized based on their
content as well as their associated metadata. In this example, face detection would
highlight the four people in the photo and could be used to prompt the user to identify
the people. If the link to the diary entry had already been formed, then the names of
the people associated with it could even be suggested. Also, face clustering can detect
similar (already named) faces in existing archived images and suggest them to the
user. As all the personal knowledge needed for contextualizing the photo was already
gathered and stored for the diary entry, the previously preserved personal knowledge
can be referenced to avoid duplication of information within the preservation system.
3https://en.wikipedia.org/w/api.php?action=query&list=geosearch&gscoord=65.6452|22.0274.

136
M. A. Greenwood et al.
Fig. 5.1 A general overview of contextualization
5.1.2
A Generalised Approach to Contextualization
While the aforementioned example shows how context information can be gathered
and used in order to aid in preserving documents in an unambiguous form, a slightly
more abstract description of how we view the process of contextualization will help
set the scene for the more technical details which make up the rest of this chapter.
Our view of the process of contextualizing a document can be summarized as in
Fig.5.1. We have an ‘information piece’ (a document or image) which needs contex-
tualizing. This item contains a number of ‘context hooks’ which refer to information
possibly recorded elsewhere, or about which more information should be captured
as part of the process of contextualization. As we will see later in this chapter, these
hooks could be named entities or phrases in a text document, or concepts visible in an
image. As well as the document being processed, there is a store of information that
can be used as a ‘contextualization source’ which contains information, referred to
as ‘context units’, which can provide information about one or more ‘context hooks’.
The job of any ‘contextualization method’ is therefore to link a ‘context hook’ to one
or more ‘context units’ and to preserve this association along with the ‘information
piece’.
5.2
Techniques for Contextualization
As with any complex task, there are many ways to approach solving the contextual-
ization problem. Approaches will clearly differ depending on the type of document
being processed: text versus images, long documents versus short documents, etc.
It is also possible, of course, to have multiple competing and/or complementary
approaches that can be tested on the same set of documents. Put simply, there is no
single solution to contextualization. The rest of this section is devoted to a number
of different approaches to contextualization that aim to cover a broad range of docu-
ment types. The ﬁrst section focuses on text documents, followed by approaches for
contextualizing image collections.

5
Keeping Information in Context
137
5.2.1
Contextualizing Text
As with many techniques that process textual information, the genres, writing styles
and lengths of the documents play an important part in the effectiveness of a given
contextualization technique. Speciﬁcally, we found that document length plays a
highlysigniﬁcantrole,whichisunderstandablegiventhatalongerdocumentcontains
more information and therefore is likely to be more self-contained. We have therefore
developed separate techniques for processing long versus short documents [71, 384,
385, 387].
5.2.1.1
Temporal Contextualization of Short Texts
With the continued growth and adoption of Twitter and other microblogging plat-
forms for the dissemination of news, there has been a parallel increase in Natural
Language Processing (NLP) research aimed speciﬁcally at this style of short texts.
This work ranges from those approaches which aim for a deep understanding of the
information within a tweet (or similar sized posts on any other microblogging plat-
form), often using semantic annotation to link entities within a tweet to a knowledge
base [152, 157, 292], to work on adopting underlying techniques, such as Part-of-
Speech tagging and tokenization, to the style of language usually seen in these short
posts [45, 317, 339].
Much of this work focuses on processing each tweet in isolation, but tweets usually
form part of a larger conversation, either through replies or the use of hashtags. The
occurrence of the same hashtag in a large body of tweets from a single time period
(usually referred to as a trending hashtag) clearly suggests a connection that can be
used, at least in part, as clues to context, but to date, this data has largely been left
unexplored.
In this section, we study the problem of annotating trending hashtags on Twitter
using entities derived from Wikipedia. Instead of establishing a static semantic con-
nection between hashtags and entities, we are interested in dynamically linking the
hashtags to entities that are closest to the underlying events during the time period
the hashtag is trending; hence, we allow the context to evolve over time. For instance,
while ‘#sochi’ refers to a city in Russia, during February 2014, the hashtag was used
to report the 2014 Winter Olympics (cf. Fig.5.2); therefore, it should be linked more
to Wikipedia pages related to the event than to the location.
Compared with traditional domains of text (e.g. news articles), associating hash-
tags with known entities poses additional challenges. Hashtags’ surface forms (i.e.
the sequence of characters) are very ad hoc, as they are chosen not in favour of the
text quality but to be short and memorable. In addition, the rapid evolution of the
semantics of hashtags (e.g. in the case of ‘#sochi’) makes them more ambiguous.
Furthermore, a hashtag can encode multiple topics at one time period. For example,
in March 2014, ‘#oscar’ refers to the 86th Academy Awards, but at the same time
also to the Trial of Oscar Pistorius. Sometimes, it is difﬁcult even for humans to

138
M. A. Greenwood et al.
understand a trending hashtag without knowledge about what is happening with the
entities in the real world (such as the trial of Oscar Pistorius). Automatic processing
of hashtags can also be complicated by the use of multi-word expressions, which can
be difﬁcult to interpret [280]. A famous example, #nowthatcherisdead, saw many
people on Twitter misinterpreting the death of Margaret Thatcher as the death of the
pop star Cher.
In this work, we describe a novel solution to these challenges by exploiting tem-
poral knowledge about entity dynamics derived from Wikipedia. We hypothesize
that a trending hashtag indicates an increase in public attention to certain entities
which can also be observed on Wikipedia. For example, in Fig.5.2, we can identify
2014 Winter Olympics as a prominent entity for ‘#sochi’ during February 2014, by
observing the change of user attention to the entity, for instance, via the page view
statistics of Wikipedia articles. We exploit both Wikipedia edits and page views for
annotation. We also propose a novel learning method, inspired by the information
spreading nature of social media, to suggest the optimal annotations without the need
for human labelling.
For this work, we have chosen to study hashtags at the daily level, i.e. we only
consider the creation day of the tweet (from its timestamp). A hashtag is classiﬁed
as ‘trending’ at a point in time if the number of tweets it occurs in is signiﬁcantly
higher than on other days [221, 225]. Each trending hashtag can be associated with
one or more ‘burst time periods’ surrounding the trending time points, where the
user’s interest in the underlying topic remains stronger than during other periods.
The problem is formally deﬁned as follows: Given a trending hashtag h and the
burst time period T (h) of h, identify the top-k most prominent entities e for h.
Fig. 5.2 Example of trending hashtag annotation. During the 2014 Winter Olympics, the hashtag
‘#sochi’ had a different meaning

5
Keeping Information in Context
139
It is worth noting that not all trending hashtags are mappable to Wikipedia entities,
as the coverage of topics in Wikipedia is much lower than on Twitter, a limitation
common to all systems which rely on Wikipedia, and we therefore focus on precision
over recall. Our approach breaks down into three main components. The ﬁrst step
is to identify all entity candidates using the text of the tweets associated with a
hashtag. In the second step, we compute a number of similarity scores between each
candidate and the hashtag, based on different types of contexts. Finally, we learn an
unsupervised, uniﬁed ranking function for each hashtag-entity pair and choose the
top-k entities with the highest scores.
The most obvious resource to identify candidate entities for a hashtag is via its
tweets. We follow common approaches that use a lexicon to match each textual
phrase in a tweet to a potential entity set [135, 361]. Our lexicon is constructed from
Wikipedia page titles, hyperlink anchors, redirects and disambiguation pages. We
generate textual phrases by generating all n-grams (n ≤5) from the input tweets
within T (h). We apply the longest match heuristic [292]: we start with the longest
n-grams and stop as soon as an entity set is found; otherwise, we continue with the
constituent, smaller n-grams.
While lexicon-based linking works well for single tweets, applying it on the hash-
tag level has subtle implications. Processing a huge amount of text, especially during
a hashtag trending time period, incurs expensive computational costs. Therefore, to
guarantee a good recall in this step while still maintaining a feasible computation,
we apply entity linking only on a random sample of the complete tweet set, T (h).
For each candidate entity e, we also include all entities whose Wikipedia article is
linked with the article of e by an outgoing or incoming link.
To rank the entity by prominence, we measure the similarity between each can-
didate entity and the hashtag. We evaluate three types of similarities. The ﬁrst type,
mention similarity, relies on the explicit mentions of entities in tweets and assumes
that entities directly linked from more prominent anchors are more relevant to the
hashtag. It is estimated using both statistics from Wikipedia and tweet phrases and
turns out to be surprisingly effective in practice.
For entities that are not directly linked to mentions (the mention similarity is
zero), we exploit external resources instead, such as web pages linked from tweets,
or entities’ home pages. By exploiting the content of entities from these external
sources, we can complement the explicit similarity metrics based on mentions. We
refer to this as context similarity.
The ﬁrst two metrics rely on the textual representation and are degraded by the
linguistic difference between the two platforms. To overcome this drawback, we
incorporate the temporal dynamics of hashtags and entities, which serve as a proxy
to the change of user interests towards the underlying topics [79]. We employ the
correlation between the time series of hashtag adoption and the entity view as the
third similarity measure (temporal similarity).
While each similarity measure captures one aspect of the prominence of an entity,
we unify the scores to obtain a global ranking function. In this work, we combine
the individual similarities using the following linear function:

140
M. A. Greenwood et al.
f (e, h) = αfm(e, h) + β fc(e, h) + γ ft(e, h)
(5.1)
where α, β, andγ are model weights and fm, fc, and ft are the similarity measures
derived from mentions, context and temporal information between the entity e and
the hashtag h. We further constrain α + β + γ = 1 so that the ranking scores of
entities are normalized between 0 and 1. The Inﬂuence Prominence Learning (IPL)
algorithm we have developed, which automatically learns the parameters without the
need of human-labelled data, is explained in detail in [387].
Experimental Setup
Unfortunately, there is no standard benchmark dataset available for this problem.
Available datasets on microblog annotation (such as the Micro-posts challenge [36])
often skip global information, and hence, we cannot infer the social statistics of
hashtags. Therefore, we created our own dataset using the Twitter API to collect a
sample of 500,551,041 tweets from January to April 2014. We removed hashtags that
were adopted by fewer than 500 users, had no letters or had characters repeated more
than four times (e.g. ‘#oooommgg’). We identiﬁed trending hashtags by computing
the daily time series of hashtag tweet counts, and removing those for which the
time-series variance score was less than 900. To identify the hashtag burst time
period T (h), we computed the outlier fraction [225] for each hashtag h and day t:
pt(h) =
|nt−nb|
max (nb,nmin), where nt is the number of tweets containing h, nb is the median
value of nt over all points in a 2-month time window centred on t and nmin = 10 is
the threshold to ﬁlter low activity hashtags. The hashtag was skipped if its highest
outlier fraction score was less than 15. Finally, we deﬁne the burst time period of
a trending hashtag as the time window of size w, centred at day t0 with the highest
pt0(h).
For the Wikipedia datasets, we processed a snapshot from 3rd May 2014, which
covers all events in the Twitter dataset, using Hedera [386], a scalable tool for pro-
cessing the Wikipedia revision history dataset based on the Map-Reduce paradigm.
In addition, we make use of the Wikipedia page view dataset, which details how
many times a Wikipedia article was requested on an hourly level. We processed the
dataset for the 4 months of our study and used Hedera to accumulate all view counts
of redirects to the relevant articles. A statistical break down of this dataset is given
in Table5.1.
Table 5.1 Statistics of the
evaluation dataset
Total tweets
500,551,041
Trending hashtags
2,444
Test hashtags
30
Test tweets
352,394
Distinct mentions
145,941
Test (entity, hashtag) pairs
6,965
Candidates per hashtag (avg.)
50
Extended candidates (avg.)
182

5
Keeping Information in Context
141
From the trending hashtags, we sample 30 distinct hashtags for evaluation. Since
our study focuses on trending hashtags that are mappable to entities in Wikipedia,
the sampling must cover a sufﬁcient number of ‘popular’ topics that are reﬂected
in Wikipedia, and at the same time rare topics in the long tail. To do this, we apply
several heuristics in the sampling. First, we only consider hashtags where the lexicon-
based linking results in at least 20 different entities. Second, we randomly choose
hashtags to cover different types of topics (long-running events, breaking events,
memes, etc.). Instead of inspecting all hashtags in our corpus, we follow [225] and
calculate the fraction of tweets published before, during and after the peak. The
hashtags are then clustered in this 3-dimensional vector space. Each cluster suggests
a group of hashtags with a distinct semantics [225]. We then pick hashtags randomly
from each cluster, resulting in a total of 200 hashtags. From this rough sample, three
inspectors carefully checked the tweets and chose 30 hashtags where the meanings
were clear.
The parameters for the linear function to combine the similarity scores are set as
follows: we initialize the similarity weights to 1
3, the damping factor to τ = 0.85,
and the weight for the language model to λ = 0.9. The learning rate μ is empirically
ﬁxed to μ = 0.003.
As results of a system in isolation are fairly meaningless, we present our approach
in comparison with a number of other entity annotation methods. Our ﬁrst group of
baselines includes Wikiminer [298], an entity linking system for general text, and
TagMe [136] which is for short text. For each method, we use the default parameter
settings, apply them for the individual tweets and take the average of the annotation
conﬁdence scores as the prominence ranking function. The second group of baselines
includes systems speciﬁcally designed for microblogs. For the content-based meth-
ods, we compare against Meij’s system [292], which uses a supervised method to
rank entities with respect to tweets. We train the model using the same training data as
in the original paper. For the graph-based method, we compare against KAURI [361],
a method that uses user interest propagation to optimize the entity linking scores.
To tune the parameters, we picked four hashtags from different clusters, randomly
sampled 50 tweets for each, and manually annotated the tweets. We also compare
three variants of our method, using only local functions for entity ranking (referred
to as M, C and T for ‘mention’, ‘context’ and ‘time’, respectively).
In total, there are 6,965 entity-hashtag pairs returned by all systems. We employed
ﬁve volunteers to evaluate the pairs in the range from 0 to 2, where 0 means the entity
is noisy or obviously unrelated, 2 means the entity is strongly tied to the topic of the
hashtag and 1 means that although the entity and hashtag might share some common
contexts, they are not involved in a direct relationship (for instance, the entity is too
general a concept, such as ice hockey). The annotators were advised to use search
engines, the Twitter search box, or Wikipedia archives whenever applicable to get
more background on the stories. Inter-annotator agreement under Fleiss score is
0.625.

142
M. A. Greenwood et al.
Table 5.2 Experimental results on the sampled trending hashtags
TagMe
Wikiminer
Meij
Kauri
M
C
T
IPL
P@5
0.284
0.253
0.500
0.305
0.453
0.263
0.474
0.642
P@15
0.253
0.147
0.670
0.319
0.312
0.245
0.378
0.495
MAP
0.148
0.096
0.375
0.162
0.211
0.140
0.291
0.439
Results and Discussion
Table5.2 shows the performance comparison of all these approaches using the stan-
dard metrics for a ranking system (precision at 5 and 15 and MAP at 15). In general,
all baselines perform worse than reported in the literature, conﬁrming the higher
complexity of the hashtag annotation task as compared with traditional tasks. Inter-
estingly, using our local similarities already produces better results than TagMe and
Wikiminer. The model fm derived from mentions signiﬁcantly outperforms both the
baselines in all metrics. Combining the similarities improves the performance even
more signiﬁcantly; all signiﬁcance tests are done against both TagMe and Wikiminer,
with a p-value < 0.01. Compared to the baselines, IPL improves the performance
by 17–28%. The time similarity achieves the highest result compared with other
content-based mention and context similarities. This supports our assumption that
lexical matching is not always the best strategy to link entities in tweets. The time-
series-based metric incurs lower cost than others, yet it produces a considerably
good performance. Context similarity based on Wikipedia edits does not yield much
improvement. This can be explained in two ways. First, information in Wikipedia
is largely biased to popular entities, so that it fails to capture many entities in the
long tail. Second, language models are dependent on direct word representations,
which are different between Twitter and Wikipedia. This is another advantage of
non-content measures such as ft.
For the second group of baselines (Kauri and Meij), we also observe the reduction
in precision, especially for Kauri. This is because the method relies on the coher-
ence of user interests within a group of tweets to be able to perform well, which
does not hold in the context of hashtags. One surprising result is that Meij performs
better than IPL in terms of P@15. However, it performs worse in terms of MAP
and P@5, suggesting that most of the correctly identiﬁed entities are ranked lower
in the list. This is reasonable, as Meij attempts to optimize (with human supervi-
sion effort) the semantic agreement between entities and information found in the
tweets, instead of ranking their prominence as in our work. To investigate this case
further, we re-examined the hashtags and divided them by their semantics, according
to whether the hashtags are spurious trends of memes inside social media (endoge-
nous, e.g. ‘#stopasian2014’), or whether they reﬂect external events (exogenous, e.g.
‘#mh370’).
The performance of the methods in terms of MAP scores is shown in Fig.5.3. It
can be clearly seen that entity linking methods perform well in the endogenous group,
but then deteriorate in the exogenous group. The explanation is that for endogenous

5
Keeping Information in Context
143
hashtags, the topical consonance between tweets is very low; thus, we can barely
annotate further than identifying just individual concepts. In this case, topical anno-
tation is trumped by conceptual annotation. However, whenever the hashtag evolves
into a meaningful topic, a deeper annotation method will produce a signiﬁcant
improvement, as seen in Fig.5.3.
Finally, we study the impact of the burst time period on the annotation quality.
For this, we expand the window size w and examine how different methods perform.
The result is depicted in Fig.5.4. It is obvious that within the window of 2 months
(where the hashtag time series is constructed and a trending time is identiﬁed), our
method is stable and always outperforms the baselines by a large margin. Even when
the trending hashtag has been saturated, hence introducing more noise, our method
is still able to identify the prominent entities with high quality.
In summary, we have shown that our approach performs well at the task of linking
hashtags to entities described in Wikipedia. This linkage allows us to provide extra
context to a tweet containing a hashtag by providing one or more links to Wikipedia.
This is especially useful with short texts such as tweets, where often all useful context
is excluded due to the space constraints. Added to this, our approach also takes into
account the evolution of concepts, ensuring that the right meaning is used, thereby
further reducing ambiguity and providing a more useful context.
Fig. 5.3 Performance of the
methods for different types
of trending hashtags
Fig. 5.4 IPL compared to
other baselines on different
sizes of the burst time
window T

144
M. A. Greenwood et al.
Fig. 5.5 Camel advertisement (left) and contextualization information taken from Wikipedia (right)
5.2.1.2
Time-Aware Re-contextualization
In the previous section, we described an approach to contextualizing very short
documents. In this section, we turn our attention to longer documents, speciﬁcally
focusing on news articles as these contain many entities that can be linked to an
external source of world knowledge.
Reading a current news article about your own country is typically straightforward
as your own world knowledge allows you to unambiguously understand the text.
Things are different if you read an article from say the 60s or the 70s as can be
found in news archives such as the New York Times Archive.4 In this section, we
are especially interested in time-aware re-contextualization, where explicit context
information is required to bridge the gap between our current understanding of the
world and the situation at the time of content creation. This includes changes in
background knowledge, the societal and political situation, language, technology or
simply the passage of time leading readers to forget.
The importance of time-aware re-contextualization is well illustrated by the adver-
tisement poster from the 1950s in Fig.5.5. From today’s perspective, it is more than
surprising that doctors would be recommending smoking. It can, however, be under-
stood from the context information at the right side of Fig.5.5, which has been
extracted from the Wikipedia article on tobacco advertising.
Dealing with content from former times is not restricted to expert users such as
journalists, historians or researchers. With the growing age of the web, general web
users are increasingly confronted with content which requires time relevant context
to be properly interpreted.
Basicformsofcontextualizationhavealreadybeensuggestedinearlierworkssuch
as [168, 295, 298]. The Wikify! system [295], for example, enables an automated
linkage of concept mentions with Wikipedia pages. Pure linkage to the Wikipedia
article is, however, not sufﬁcient for the re-contextualization task we are targeting.
First, Wikipedia pages on popular concepts, events and entities tend to contain large
amounts of content, while the concrete aspect of the text to be contextualized might
be covered only marginally or not at all, and relevant information might be distributed
4http://catalog.ldc.upenn.edu/LDC2008T19.

5
Keeping Information in Context
145
Fig. 5.6 Overview of our approach to time-aware re-contextualization
over various articles. Furthermore, the crucial temporal aspect is also missing in pure
linking approaches.
Time-aware re-contextualization, that is, the association of an information piece i
(such as a phrase in a text) with additional context units ci for easing its understanding
is a challenging task. Several sub-goals of the information search process have to be
combined with each other: (1) ci has to be relevant for i, (2) ci has to complement
the information already available in i and the surrounding document, (3) ci has to
consider the time of creation (or reference) of i and (4) the set of collected context
information for i should be concise to avoid overloading the user.
The work presented in this section automates the process of time-aware
re-contextualization and provides advanced approaches for retrieval and ranking
of contextualization candidates by taking into consideration complementarity. In
more detail, we follow a two-step process. In the ﬁrst step, we identify contextual-
ization candidates based on contextualization hooks, i.e. the parts of document that
require contextualization, such as entity or concept mentions. For this purpose, we
explore and analyse different methods for formulating (generating) queries, which
are used for retrieving adequate contextualization candidates from an underlying
knowledge source. In the second step, we rank the candidates. Similarly to diversiﬁ-
cation approaches (e.g. [435]), this requires balancing two goals: high content-based
and temporal relevance for the text to be contextualized, on the one hand, and com-
plementarity for providing information that cannot already be found in the text, on the
other hand. For our contextualization approach, we use Wikipedia as the knowledge
source (because of its worldwide topical and temporal coverage).
In the general contextualization model underlying our approach, we distinguish
the information piece d to be contextualized and the contextualization source, where
the information for the contextualization comes from. Within d a context hook h is an
aspect or part of d that requires further information for its time-aware interpretation.

146
M. A. Greenwood et al.
The contextualization source is organized into contextualization units cu. In our
approach, we have pre-processed a Wikipedia dump as the contextualization source
resulting in annotated and indexed Wikipedia paragraphs as contextualization units
(see Fig.5.6). For information items d to be contextualized, we use articles from the
New York Times Archive5 with manually annotated contextualization hooks, i.e. we
assume that a reader has marked the places he/she ﬁnds difﬁcult to understand.
Starting from the context hooks, the next process is to retrieve a ranked list
of contextualization units from the contextualization source. In time-aware
re-contextualization, the time gap between the creation and reading time of d imposes
additional challenges. In our approach, the contextualization process consists of two
main steps: (1) formulating queries that are able to retrieve contextualization units,
which are good candidates for contextualization; (2) retrieving and ranking the can-
didates from the contextualization source using the queries from step (1). For step
(1), we explore document-based and hook-based query formulation methods and
present a procedure that selects good queries based on recall-oriented query perfor-
mance prediction. For step (2), we employ a retrieval method based on language
modelling and re-rank the retrieved contextualization candidates based on a variety
of features and a learning to rank approach for ensuring complementarity. These
steps are described in detail below.
The goal of the query formulation phase consists of generating a set of queries
Qd for a given document d to retrieve contextualization candidates as input for the
re-ranking phase. We explore two families of query formulation methods: one using
the document to be contextualized itself as a ‘generator’ of queries and the other using
contextualization hooks as generators. Since some of these methods can generate
more than one query from an input document, we also discuss two procedures to
merge the ranked result lists.
The ﬁrst family of query formulation methods exploits the document content
and structure. Similarly to [388], we use three methods to formulate queries from
documents: title, lead and title + lead. Title formulates a query consisting of the
document title, which is indicative of the main topic of the article. Lead uses the lead
paragraph of a document, typically representing a concise summary of the article and
including its main actors. Title + lead is a combination of the previous two methods
which formulates a query consisting of both the title and the lead paragraph of the
document. All the queries are pre-processed by tokenization, stop-word removal and
stemming. We did not investigate further information extraction approaches for query
formulation, since it has already been shown that these methods perform better than
more complex information extraction techniques, e.g. key phrase extraction [388].
Documents in our model are assumed to contain a set of hooks explicitly repre-
senting the information needs of the reader or, more precisely, what requires contex-
tualization to be understood and interpreted. The analysis done in [71] showed that
context hooks are not only entity or concept mentions but can also be general terms
and short phrases. We consider two basic hook-based query formulation methods:
all_hooks and each_hook. All_hooks includes all the hooks for a document in a single
5http://catalog.ldc.upenn.edu/LDC2008T19.

5
Keeping Information in Context
147
query, representing a tailored perspective of the user’s combined information needs
for the document. Each_hook queries each hook separately, focusing on speciﬁc
information about single actors, aspects, or sub-topics of the document. The queries
generated by these methods are augmented with the title of the document, under the
assumption that it is a good representative of the document’s topic. We also exper-
imented with more advanced methods based on identifying hook relationships, for
instance, considering their co-occurrence in a document collection. However, since
these approaches did not perform better than the all_hooks method described before,
we will not discuss them further.
Different methods based on ranking and selection of query terms from an initial
query might be employed [37, 224, 277], considering the entire set of hooks for
a document as the initial query. We explore an adaptive method which formulates
queries based on the characteristics of the input document and hooks. Our approach
consists of predicting the performances of candidate queries representing subsets of
hooks for a given document, ranking them according to the predicted performance,
and selecting the top-m to be performed for the document. The value of m is identiﬁed
through experiments. In contrast to previous works in query performance prediction,
the prediction model is trained on recall performances instead of precision. Further-
more, we deﬁne novel features for query performance prediction that explicitly take
the temporal dimension into account. Finally, our method assesses performances of
subsets of query terms (hooks) and can generate more than one query (subsets of
hooks).
Given a document d and the set of its hooks Hd, we compute its power set P(Hd)
and create a candidate query for each set of hooks p ∈P (Hd). Again, candidate
queries are augmented with the title of the document. We measure the performance of
each candidate query in terms of its recall because, as already explained, at retrieval
phaseweareinterestedinretrievingasmanycontextualizationcandidatesaspossible.
In this work, we predict query performance with a regression model learned via
Support Vector Regression (SVR) [119]. In this model, each learning sample s =

fq,rq

consists in a feature vector fq describing query q (as well as the document it
refers to) and its recall rq, i.e. the label to be predicted. Note that different numbers of
top l results can be used to compute the recall. The feature set that we use to represent
queries and the document it belongs to are described in the rest of this section. It is
composed of novel temporal features for query performance prediction, along with
more standard features [62, 162, 304].
We compute a family of linguistic features [304] for a query by considering its
text and the document it refers to. This results in a set of features both at query and
document level: the length of the query, in words; the number of duplicate terms
in the query; the number of entities (people, locations, organization and artefacts)
in the query; the number of nouns in the query; the number of verbs in the query;
the number of hooks in the query; the length of the document’s title; the length of
the document’s lead paragraph; the number of entities in the document (title and
lead paragraph); the number of nouns in the documents; the number of verbs in the
document; the number of hooks for the document; and the number of duplicates in
the document.

148
M. A. Greenwood et al.
Thedocumentfrequency ofahookh representsthepercentageofcontextualization
units in the corpus containing h, and it is computed as
d f (h) = log Nh
N
(5.2)
where Nh is the number of contextualization units in the corpus containing h and N
is the size of the corpus. At document level, we compute the document frequency
for every hook of the document the query belongs to, i.e. d f (h) ∀h ∈Hd, and then
we derive aggregate statistics like average, standard deviation, maximum value and
minimum value. Similarly, at query level, we compute d f (h) for every hook in the
query, and we derive the same aggregate statistics as before. In the following, we will
refer to average, standard deviation, maximum value and minimum value simply as
aggregate statistics.
In order to restrict the popularity of a term to a particular time period T =
[t0 −w; t0 + w], we compute Eq.5.2 only for those contextualization units hav-
ing at least one temporal reference contained in T . This can be done efﬁciently
since contextualization units in our corpus have been annotated with the temporal
references mentioned in them. The time period we are interested in is centred around
the publication date of the document, i.e. t0 = pd, and the parameter w determines the
width of the interval. After experimenting different values of w, we set w = 2 years
for our dataset.
The scope of a query has been deﬁned in [162] as the percentage of documents
(contextualization units in our case) in the corpus that contain at least one query term.
Besides the scope of the query itself, we also compute the scope of the document
title and the scope of the document hooks Hd when queried together.
We deﬁne the temporal scope of a query as the percentage of contextualization
units in the corpus that contains at least one query term and at least one temporal
expression within a given time period. The time period that we consider is the same as
the one considered for the computation of temporal document frequency, i.e. a period
centred around the publication date of the document and with temporal window equal
to 2w. Again, we experimented different values of w and we set w = 2 years.
For a given query q, we retrieve the top-k contextualization units and we com-
pute aggregated statistics of their relevance scores given by the underlying retrieval
model. The value of k has been empirically set to 100 after experimenting different
candidate values. We also computed relevance features at document level, using both
document’s title and document’s hooks as queries.
For a given query q generated from a document d and every retrieved contex-
tualization unit c in its top-k result set (again, k = 100), we compute the temporal
similarity between q and c, and we derive aggregated statistics over the elements in
the result set. Temporal similarity between time points t1 and t2 is computed through
the time-decay function [192]:
T SU(t1, t2) = αλ |t1−t2|
μ
(5.3)

5
Keeping Information in Context
149
where α and λ are constants, 0 < α < 1 and λ > 0, and μ is a unit of time dis-
tance. The temporal similarity between a query q and a result c is computed as
maxt∈Tc{T SU(t, pd)}, where Tc is the set of temporal references mentioned in c,
and pd is the publication date of the document q refers to. This can be done efﬁ-
ciently since temporal references mentioned in contextualization units have been
extracted and stored at indexing time.
We also computed temporal similarity features at document level, using both
document’s title and document’s hooks as queries. The computation of the features
is the same as the one described above.
We observed that changing the function parameters did not affect the correlation
capabilities of the feature, and we set λ = 0.25, α = 0.5, and μ = 2 years in our
experiments.
We now describe the methods used in addressing the second part of the re-
contextualization process: retrieving and re-ranking context. For the retrieval step,
given the queries generated from the different methods for each document described
above, we use a retrieval model based on language modelling to create a ranked list
of contextualization candidates. Later, learning to select relevant context items is
applied to this ranked list.
For the retrieval step, we use query-likelihood language modelling [330] to deter-
mine the similarity of a query with the context. In particular, given a queryq generated
by using one of the query formulation methods for the document d, we compute the
likelihood of generating the query q from a language model estimated from a context
c with the assumption that query terms are independent.
P(c|q) ∝P(c)

w∈q
P(w|c)n(w,q)
(5.4)
where w is a query term in q, n(w, q) the term frequency of w in q, and P(w|c) is
the probability of w estimated using Dirichlet smoothing:
P(w|c) = n(w, c) + μP(w)
μ + 
w′ n(w′, c)
(5.5)
where μ is the smoothing parameter, P(w) is the probability of w in the collection.
To combine the rankings produced by each query of a document, we exploited
two combining methods, namely, round-robin, which chooses one result from each
ranked list, skipping any result if it has occurred before, and CombSUM, which sums
up a result’s scores from all ranked lists where it was retrieved. In the experiment,
we observed that round-robin method achieves better performance than CombSUM
especially in terms of recall, which is also reported in [388]. Therefore, we decided
to use round-robin method for combining different ranked lists.
Once we have obtained a ranked list of contextualization candidates for each doc-
ument, we turn to context selection (re-ranking) where we need to decide which of
the context items are most viable. Our ranking algorithm needs to balance two goals,
high topical and temporal relevance for the document and complementarity for pro-
viding additional information. In this work, we use supervised machine learning that

150
M. A. Greenwood et al.
takes as input a set of labelled examples (context to document mappings) and various
complementarity features of these examples similar to diversity features [435].
The ﬁrst class that we employ is topic diversity, which aims to compare the
dissimilarity between document d and context c on a higher level by representing
them using topics. We use Latent Dirichlet Allocation (LDA) [44] to model a set of
implicit topics distribution of the document and context. We deﬁne this feature as
follows:
R1(c, d) =




m
	
k=1
(p(zk|d) −p(zk|c))2
where m is the number of topics, and zk is the topic index.
We also considered text difference as a feature: in this case, we represent the
document and context as a set of words. The novelty of context c is measured by
the number of new words in the smoothed set representation of c. If a word w
occurred frequently in context c but less frequently in document d, it is likely that
new information not covered by d is covered by c. For computation, document
and context are represented by a set of informative words (removing stop words,
stemming) denoted by Set(d) and Set(c), respectively. We compute this feature as
follows:
R2(c, d) = ∥Set(c) ∩Set(d)∥
The way of computing entity difference is similar to the one for text difference,
with the difference that document and context are represented by a set of entities.
The feature is denoted as R3(c, d).
Anchor texts can be regarded as a short summary (i.e. a few words) of the target
document and capture what the document is about. This feature can be computed
similarly as text and entity features and is denoted as R4(c, d). We extract anchor
texts using WikiMiner [298] with a conﬁdence threshold γ .
The next feature we use is distribution similarity, which is denoted as R5(c, d).
R5(c, d) = −K L(θc, θd) = −
	
wi
P(wi|θc) log P(wi|θd)
P(wi|θc)
where θd and θc are the language models for document d and context c, respec-
tively, and are multinomial distributions. We compute θd (and similarly for θc) using
maximum likelihood estimation (MLE) given as
P(wi|d) =
t f (wi, d)

w j t f (w j, d)
The problem with using MLE is that if a word never occurs in document d, its
probability will be zero (P(wi|d) = 0). Thus, a word in context c but not in document
d will make K L(θc|θd) = ∞. In order to solve this problem, we make use of Dirichlet
smoothing method.

5
Keeping Information in Context
151
Pλ(wi|d) =
t f (wi, d) + λp(wi)

w j(t f (w j, d) + λp(w j))
There are several ways to compute geometric distance measure, such as Manhattan
distance and Cosine distance. We leverage Cosine distance because of its robustness
to document length.
R6(c, d) = cos(c, d) =
n
k=1 wk(c)wk(d)
∥d∥∥c∥
In our experiment, we used each unique word as one dimension and the tf.idf score
as the weight of each dimension.
In order to retrieve high topical and temporal relevant contextualization candidates
for the document, we also consider the relevance and temporal features. For the
former, we exploit the retrieval scores of context returned by our retrieval model.
For the later, we apply temporal similarity measurement, i.e. TSU which has been
described previously.
Experimental Setup
In our experiments, we used the New York Times Annotated Corpus, which contains
1.8 million documents from January 1987 to June 2007, as the document collection
to be contextualized and Wikipedia as the contextualization source. We obtained a
copy of Wikipedia as of 4 February 2013 and considered paragraphs as contextual-
ization units. In this particular snapshot, we have 4,414,920 proper articles that con-
tain 25,708,539 paragraphs. For each paragraph, we used Stanford CoreNLP [263]
for tokenization, entity annotation and temporal expression extraction. In addition,
anchor texts found in the paragraph hyperlinks are also extracted. We used Apache
Solr6 to index the annotated paragraphs.
In order to obtain a ground-truth dataset (both for training and evaluation), we
ultimately picked a set of 51 articles that spanned a wide range of topics (business,
technology, education, science, politics and sports) focusing on older articles (29
articles published in 1987, 2 articles in 1988, 6 articles in 1990, 7 articles in 1991
and 7 articles in 1992) and recruited six human annotators to manually annotate those
articles.
The annotators were presented with an annotation interface in which they can
evaluate article/context pairs (relevant or non-relevant). The annotation guidelines
speciﬁed that the annotators should assign relevance to the context that provides
additional information which complements the information in the article and which
helps to understand the article. For each article, we retrieved up to 20 candidate
contexts with each query formulation method and then removed duplicates.
6https://lucene.apache.org/solr/.

152
M. A. Greenwood et al.
In total, our annotated dataset consists of 9,464 article/context pairs, where the
annotators evaluated 26.9 relevant contexts per article on average. To foster further
research on this challenging task, our ground-truth dataset is publicly available.7
We averaged the pairwise kappa values of all possible combinations of annotators
that had overlapping candidates they had annotated and we obtained a reasonable
agreement of κc = 0.37 given the high complexity of this contextualization task.
The parameters are set as follows. For query performance prediction, the regres-
sion model was built using the Support Vector Regression implementation of Lib-
SVM.8 In particular, we trained an n-SVR model with Gaussian Kernel through ten-
fold cross validation. The open parameters were tuned via grid search to C = 3, γ =
0.5 and ν = 0.75. Linguistic features were extracted using Stanford CoreNLP [263].
For re-ranking context, we performed ﬁvefold cross validation at document level.
We conducted experiments using several machine learning algorithms to conﬁrm
the robustness of our approach, i.e. it does not depend on any speciﬁc algorithm.
Speciﬁcally, we employed Random Forests (RF), RankBoost (RB) and AdaRank that
are implemented in RankLib.9 In order to compute topic-based features, we employed
the topic modelling tool, Mallet10 by specifying the number of topics to 100, for this
task. In addition, we set the conﬁdence threshold to γ = 0.3 for extracting anchor
texts using WikiMiner. For smoothing, we set μ = 2000 and λp(wi) = 0.5.
For the evaluation metrics, we consider precision at rank 1, 3, 10 (P@1, P@3,
P@10, respectively), recall, and Mean Average Precision (MAP). These measures
provide a short summary of the quality of the retrieved context. In our experiment, a
context is considered relevant if it is marked as relevant by an annotator; otherwise,
we consider it as non-relevant. We used the top-20 returned context suggestions
for evaluation because we do not expect that readers would consider more than 20
contextualization units. Statistical signiﬁcance was performed using a two-tailed
paired t-test and is marked as ▲and △for a signiﬁcant improvement (with p < 0.01
and p < 0.05, respectively), and signiﬁcant decrease with ▼and ▽(for p < 0.01 and
p < 0.05, respectively).
As a comparison to our proposed approach, we also investigated three competitive
baselines. Proposed by Milne and Witten [298] our M&W baseline represents the
state of the art in automatic linking approaches. We use the algorithm and best
performing settings as described in [298]. In order to apply this method to our task,
we consider all paragraphs of all linked pages as a candidate set. A standard query-
likelihood language model, LM, is used for the initial retrieval which provides the
top retrieved documents as a candidate set for the contextualization task. Since our
aim is to contextualize old articles, the temporal dimension is important and so for
our ﬁnal baseline. For our ﬁnal baseline, Time-aware Language Model (LM-T), we
selected a state-of-the-art time-aware ranking method, which has previously been
shown to be very effective at answering temporal queries. It assumes the textual and
7http://www.l3s.de/~ntran/contextualization/.
8http://www.csie.ntu.edu.tw/~cjlin/libsvm/.
9http://sourceforge.net/p/lemur/wiki/RankLib/.
10http://mallet.cs.umass.edu/topics.php.

5
Keeping Information in Context
153
temporal part of the document d are generated independently from the corresponding
parts of the context c, yielding
P(d|c) = P(dtext|ctext) × P(dtime|ctime)
(5.6)
where dtime is the document’s publication date, ctime is the set of temporal expressions
in the context c.
The ﬁrst factor P(dtext|ctext) can be computed by Eqs.5.4 and 5.5. The second
factor in Eq.5.6 is estimated, based on a simpliﬁed variant of [38], as
P(dtime|ctime) =
1
| ctime |
	
t∈ctime
P(dtime|t)
(5.7)
If the document has zero probability of being generated from the context, Jelinek-
Mercer smoothing is employed, and we estimate probability of generating the doc-
ument’s publication date from context c as
P(dtime|ctime) = (1 −λ)
1
| Ctime |
	
t∈Ctime
P(dtime|t)
+ λ
1
| ctime |
	
t∈ctime
P(dtime|t)
(5.8)
where λ ∈[0, 1] is a tunable mixing parameter which is set to λ = 0.5 in our exper-
iment (changing this parameter does not affect our results), and Ctime refers to the
temporal part of the context collection treated as a single context and P(dtime|t) is
estimated by using time-decay function, i.e. TSU computed as in Eq.5.3.
We evaluate and compare the performances of the different query formulation
methods focusing on recall metric. The results reported in the rest of this section are
averaged over the 51 documents in our dataset.
In order to fairly evaluate and compare the recall capabilities of the different
methods, which can generate different numbers of queries, and we allow each method
to retrieve the same number of results k. The round-robin method is used to create a
single result set of k elements from different ranked lists.
The query formulation methods are based on predicting the performance (recall
in our case) of candidate queries, ranking them according to the prediction, and then
using the top-m queries to retrieve results. Thus, the quality of the query performance
prediction itself has to be evaluated before assessing and comparing the performance
of the whole query formulation method.
A regression model has been trained via 10-fold cross validation, and the results
reported hereafter have been averaged over the 10 folds. The correlation coefﬁcient
is equal to 0.973, the root mean squared error equal to 0.056 and the mean absolute
error equal to 0.037. The low error values and high correlation value, if compared

154
M. A. Greenwood et al.
Fig. 5.7 Recall curves of document-based and hook-based methods
with the performance in predicting query precision reported in previous works (e.g.
[63, 334]), show that the recall of queries in our task can be predicted quite accurately
using the proposed features.
In order to analyse which are the most important features in our model, we iden-
tiﬁed the top-10 features according to their absolute correlation coefﬁcient. These
features include the following: max query relevance, number of hooks in document,
min document’s hooks df, max document’s hooks temporal df, document’s hooks
scope, avg query temporal similarity, document’s title temporal scope, std query
relevance, avg document’s title temporal similarity and std query temporal similar-
ity. The presence of temporal document frequency, temporal similarity and temporal
scope shows that the temporal features that we identiﬁed play an important role in
the model. We can also note that both query-level and document-level features are
important, since the set is made of four features from the former and six features
from the latter. Finally, there is only one linguistic feature in the set, namely, the
number of hooks in the document, conﬁrming that this class of features alone does
not correlate well with query performance [62].
We now compare recall values for the document-based methods (title, lead and
title + lead), the basic hook-based methods (each_hook, all_hooks), as well as
the method based on query performance prediction, hereafter called qpp. For the
latter method, we report the performances achieved when using prediction mod-
els trained with different labels: we experimented with different l values, namely,
l = 50, 100, 200, for the computation of the recall at l to be used as label.
These three methods will be called qpp_r@50, qpp_r@100 and qpp_r@200,
respectively, in the rest of the experiments. Note that each qpp method considered
here uses the top-2 queries, according to their predicted performances, to retrieve the

5
Keeping Information in Context
155
results. The choice of selecting m = 2 queries will be explained and motivated later
in this section.
The recall curves of the different methods, for different values of top-k results,
are shown in Fig.5.7. The curves of title and lead are the lowest ones, while their
combination (title + lead) becomes comparable with each_hook. Querying using
all the hooks of a document together, i.e. all_hooks, exposes higher recall values
than all the aforementioned methods, showing that performing hook-based queries
does lead to better performances in terms of recall with respect to document-based
methods. The difference in performances between each_hook and all_hooks is due
to the fact that querying all the hooks together prefers contextualization candidates
that contain many hooks. These are potentially more relevant, as they refer to different
aspects (hooks) of the same document.
Regarding the qpp methods, for k > 20 −30, the recall values achieved are
between 3 and 7% higher than the ones obtained by all_hooks. For larger values
of k, e.g. k > 400, the difference between the qpp methods and all_hooks reduces
because the prediction models used by the qpp methods have been optimized for
lower values of k (recall that l = 50, 100, 200).
This means that, if the number of k results to be retrieved for the re-ranking phase
is known and ﬁxed in advance, this information can be exploited early in the training
of the query performance prediction model by setting l = k, leading to higher recall
values for that particular k.
Another comparative analysis between qpp methods and all_hooks can be done
by categorizing the documents according to their difﬁculty, which we deﬁne in terms
of the amount of relevant context that can be retrieved for a given document. This
means that difﬁcult documents are those for which little relevant context can be
retrieved, before the re-ranking phase. We categorize documents as ‘easy’ or ‘hard’
with respect to the all_hooks method, since it represents a baseline in this compar-
ative analysis with qpp methods.
The splitting of the documents into easy and hard was performed by considering
the recall at k = 200 achieved by all_hooks for the different documents. Since the
recall values associated to the different documents exhibited a uniform distribution,
we split the document set into two equal parts, one representing easy documents and
the other representing hard documents.
Table5.3 shows the performances of qpp_r@50, qpp_r@100 and qpp_r@200
compared to the ones of all_hooks for the different categories of difﬁculty. The
comparison between each qpp method and all_hooks is done considering the recall
at the k value used to train the prediction model (i.e. k = l,l = 50, 100, 200). Besides
qpp_r@50, qpp_r@100 and qpp_r@200 are on average better than all_hooks both
for easy and hard documents, their improvements are greater for hard documents.
In case of qpp_r@100, for instance, the relative improvement with respect to the
recall value achieved by all_hooks is 5.6% for easy documents and 18.3% for hard
documents. We believe that the capability of getting higher recall improvements for
documents whose relevant context units are difﬁcult to retrieve is a considerable
characteristic for the qpp methods.

156
M. A. Greenwood et al.
Table 5.3 Recall of all_hooks and qpp methods over different classes of documents (based on their
retrieval difﬁculty)
R@50
R@100
R@200
qpp
all_hooks
qpp
all_hooks
qpp
all_hooks
Easy
0.6208
0.5666
0.7361
0.6969
0.7951
0.7686
Hard
0.3837
0.3094
0.4606
0.3892
0.5391
0.4550
Fig. 5.8 Recall values of qpp_r@50, qpp_r@100 and qpp_r@200 by varying the number of
top-m queries
As a conclusion, in this section, we showed that exploiting hooks in query formu-
lation is more effective, in terms of recall, than document-based query formulation
methods. Moreover, we showed that learning to select candidate hook-based queries
can be better, again in terms of recall, than the basic hook-based query formulation
methods.
The number of top-ranked queries that qpp methods perform is an open parameter,
which we tuned via an empirical analysis observing the recall performances when
selecting different numbers of top-m ranked queries. Recall that, for sake of fair
comparison, we allow each method to pick the same number of results k from the
result lists retrieved by the queries that it generated for a given document. This
means that increasing the number of queries to be selected and performed does not
necessarily lead to higher recall.
Figure5.8 shows the recall values achieved by qpp_r@50 (computed at top-50
results), qpp_r@100 (computed at top-100 results) and qpp_r@200 (computed at
top-200 results) for different numbers of top-m queries selected. A common trend
over the different curves can be observed: they stay quite stable for small values of
m, exhibiting a little peak for m = 2, and then they decrease for increasing values

5
Keeping Information in Context
157
Table 5.4 Retrieval performance of document-based and hook-based query models. The signif-
icance test is compared with Row 1 (within the ﬁrst group) and Row 3 (for the second and third
groups)
P@1
P@3
P@10
MAP
Recall
Document-based query models
title
0.2156
0.1895
0.1745
0.2446
0.1211
lead
0.4902▲
0.4641▲
0.3333▲
0.4908▲
0.2603▲
title + lead
0.5294▲
0.4705▲
0.3901▲
0.5161▲
0.2723▲
Basic hook-based query models
each_hook
0.3333
0.3464
0.2745
0.4003
0.1969
all_hooks
0.5490
0.5098
0.4137
0.5640
0.2979
Query performance prediction model
qpp_r@100
0.5882
0.5490▲
0.4529▲
0.5802▲
0.3097▲
Table 5.5 Retrieval performance of all_hooks and qpp_@100 on a set of difﬁcult documents
P@1
P@3
P@10
MAP
Recall
all_hooks
0.5000
0.3462
0.2885
0.4487
0.2217
qpp_r@100
0.5000
0.4743△
0.3730△
0.5048△
0.2357
of m. After observing this behaviour, we decided to ﬁx the number of performed
queries to m = 2.
Results and Discussion
We report the retrieval performance of different query formulation methods and
analyse the effectiveness of our context ranking methods trained using different
machine learning algorithms. First, we investigated the performance of the standard,
well-known Wikiﬁcation technique, i.e. the M&W method, in retrieving contextu-
alization candidates. Our experiment considered all paragraphs of all linked pages
as candidates. The results obtained using the M&W method achieve the low recall
value of 0.2290, thus indicating that current semantic linking approaches are not
appropriate for the contextualization task.
Table5.4 shows the results of the different query formulation methods. The ﬁrst
group (top) reports results for candidate retrieval based on document-based query
models in which the best performing model is title + lead that uses content from
the article’s title and lead paragraph. Table5.4 shows that the qpp_r@100 model is
the best performing among the hook-based query models and signiﬁcantly improves
over title + lead on all metrics.
Similar to the previous experiment, Table5.5 reports the results of all_hooks
and qpp_@100 retrieval baselines on a subset of difﬁcult documents (here recall is
computed on top-20 candidates). On this subset, qpp_r@100 also shows signiﬁcant
improvement over all_hooks in terms of precision. In short, the results on different
query formulation methods indicate that using hook-based approaches outperform

158
M. A. Greenwood et al.
Table 5.6 Retrieval performance of different machine-learned ranking methods compared to the
best performing retrieval baselines
P@1
P@3
P@10
MAP
Recall
title + lead
LM
0.5294
0.4705
0.3901
0.5161
0.2723
RF
0.7672▲
0.5757△
0.4909▲
0.6170▲
0.3522▲
RB
0.6036
0.5945△
0.4694▲
0.5945
0.3417▲
Adabank
0.6254
0.5406
0.4143
0.5457
0.3249
all_hooks
LM
0.5490
0.5098
0.4137
0.5640
0.2979
RF
0.8272▲
0.6630▲
0.5014 ▲
0.6427△
0.3611▲
RB
0.7855▲
0.6593▲
0.5009▲
0.6475△
0.3637▲
AdaRank
0.6472
0.5836
0.4687
0.6034
0.3372△
qpp_r@100
LM
0.5882
0.5490
0.4529
0.5802
0.3097
RF
0.8054▲
0.6993▲
0.5140▲
0.6498▲
0.3951▲
RB
0.7218
0.6915▲
0.5300▲
0.6632▲
0.3792▲
AdaRank
0.6072
0.6139
0.4895
0.6109
0.3479▲
the document-based approach that based on merely article internal structure. Using
the query performance prediction method obtains the highest performance on all
metrics, followed by all_hooks.
We now present the results of our re-ranking approach when using a set of innova-
tive complementarity features to further improve performances of the context ranking
step, especially in terms of precision. We select title + lead for the document-based
approach and all_hooks, qpp_r@100 for the hook-based approach.
Theﬁrst(top)groupinTable5.6showstheresultswhenapplyingmachinelearning
to the title + lead retrieval baseline. All three algorithms are able to improve precision
at rank k, MAP and recall. RF and RB obtain signiﬁcant improvement where RF
achieves the highest score on most metrics, except precision at rank 3 where RB
is the best. The second (middle) group reports the results of all_hooks retrieval
baseline, augmented by the re-ranking step. In this case, RF and RB are again able to
signiﬁcantly improve over all_hooks on all metrics while AdaRank is also performing
signiﬁcantly better than all_hooks in terms of recall. Among three algorithms, RF
achieves the highest results, except for recall. Similarly, all three machine learning
algorithms perform signiﬁcantly better than the qpp_@100 retrieval baseline. Again,
in this case, RF obtain the highest performances, closely followed by RB.
In order to compare our approach to time-aware language model which takes into
account temporal information, we use the queries derived from query performance
prediction method, i.e. qpp_@100 that obtain the highest results among our query
formulation methods. Table5.7 shows that using time-aware language models is not
efﬁcient in our case. This is possibly because lots of relevant context (paragraphs in

5
Keeping Information in Context
159
Table 5.7 Retrieval performance of our proposed ranking method and the state-of-the-art time-
aware language modelling approach. The signiﬁcance test is compared against LM-T
qpp_@100
P@1
P@3
P@10
MAP
Recall
LM-T
0.5882
0.4967
0.4176
0.5446
0.2796
LM
0.5882
0.5490
0.4529
0.5802
0.3097△
RF
0.8054△
0.6993▲
0.5140▲
0.6498▲
0.3951▲
News article - Maj. Gen. Richard V. Secord, a main organizer of the Iran arms sales and
the contra supply operation, testiﬁed today that he had been told that President Reagan had
been informed that proceeds from the sales to Iran had been diverted to the Nicaraguan
rebels.
Context - Speaking of the Iran-Contra affair, a Reagan administration scandal that involved
the diverting of funds being shipped to Iran to the contras in Nicaragua, Reagan says,
“None of the arms we’d shipped to Iran had gone to the terrorists who had kidnapped our
citizens.” Of the scandal, Reagan writes, “ and, I presume, knew how deeply I felt about
the need for the contras’ survival as a democratic resistance force in Nicaragua. Perhaps
that knowledge... led them to support the contras secretly and saw no reason to report this
to me.” He also says of himself, “As president, I was at the helm, so I am the one who is
ultimately responsible.” Also, Reagan discusses his political rivalry and personal friendship
with former Speaker of the House Tip O’Neill.
Fig. 5.9 Example of contextualization candidate for a given document with no explicit temporal
information
our case) do not have any temporal information as shown in Fig.5.9. Consequently,
these candidates are ranked low (e.g. higher than 20) in the ranked list returned by
LM-T. This result indicates that purely using the time dimension in context retrieval
is not sufﬁcient in the contextualization task. It also conﬁrms the importance of
complementarity that is used in our re-ranking step.
These results link nicely back to the general model of contextualization we pre-
sented towards the beginning of this chapter (see page xxx), in so far as they rely on
extracting ‘context hooks’ from a document (referred to as an ‘information piece’ in
our model) and linking them to ‘context units’ to provide a state-of-the-art time-aware
contextualization approach.
5.2.2
Contextualizing Photo Collections
In this section, we discuss the application of contextualization on photo collections.
A previously processed set of personal photo collections or photos from the web
can be considered as the pool of ‘context units’, while photos of this pool that are
semantically similar to photos of a new collection can be considered as ‘context
hooks’ (see Sect.5.1.2). The additional information provided, about the situation in

160
M. A. Greenwood et al.
which the photos of the user collection were captured, results in a more complete view
of the event, and ultimately helps the formation of well-preserved semantic memories
of an event (as discussed in Chap.2). In the following, we discuss in more detail two
characteristic methods for photo collection contextualization: (a) a personal photo
collection contextualization method, and (b) a photo collection contextualization
method that acquires ‘world knowledge’, taking advantage of information found on
the web.
An approach for a personal photo collection contextualization is to enrich an
event-centred user collection by retrieving additional content (i.e. photos covering
different aspects of the same event) from other resources. This can be applied in
cases where an individual attends either public events (e.g. sports competitions,
concerts) or personal events (e.g. a professional meeting, a birthday party and a
family trip) and captures photos, and also has access to photo collections of different
users that attended the same event. To give a more speciﬁc example, suppose that a
person attends a public event such as the London 2012 Olympic Games and takes
photos. At the end of the event, what remains to the user are the memories and
photos. A large number of people attended the same event capturing the sports, and
more speciﬁcally the moments which they considered interesting. The information
collected by individuals is diverse, covering the overall event. By combining this
information and sharing it, we can provide a more holistic description of the event.
The event depicted in a photo collection to be contextualized will typically consist
of several sub-events, i.e. groups of photos that constitute a distinct action within a
bounded time and space. For the Olympic Games, the individual sports competitions
(e.g. ﬁrst day of men’s tennis, women’s weightlifting ﬁnal) can be considered as sub-
events of the entire London 2012 summer Olympic event (see Fig.3.12 in Chap.3 for
an example). We argue that such sub-events related to the overall photo collection
can play a major role in the contextualization process.
In light of the above considerations, it is no surprise that photo collection contex-
tualization relies on techniques for event-based clustering, multi-user galleries tem-
poral synchronization, and sub-event matching. A variety of such collection analysis
methods has been proposed in the literature for tackling the problems of organizing
and visualizing personal photo collections (e.g. [75, 254]), selecting representa-
tive photos out of a photo collection [321], event-based clustering (e.g. [17], also
discussed in Sect.3.5) and temporal synchronization of multi-user collections (e.g.
[347]). However, very few works deal explicitly with photo collection contextualiza-
tion. One example is the method of [383], which compares events at collection level
instead of taking into account individual photo features. Given an event described
by a set of photos, this method can retrieve photos from the same event captured by
different users, utilizing visual, temporal and geolocation information (although the
authors do not test the latter in their experiments).
An alternative approach for photo collection contextualization is to use the web as
a pool of ‘context units’, discarding this way the restrictive need for photo collections
coming from different known users that attended the speciﬁc event. Both of the
aforementioned methods are discussed in the following sections.

5
Keeping Information in Context
161
Fig. 5.10 Overview of the personal photo collection contextualization method
5.2.2.1
Personal Photo Collection Contextualization
Wepresentapersonalphotocollectioncontextualizationmethod(originallyproposed
in [18]) that enriches the content of a user collection with additional photos extracted
from collections of different users who attended the same event. This method adopts
the notions of a single ‘seed’ collection (the user collection to be contextualized) and
‘archive’ collections (the collections of other users from which contextual informa-
tion will be drawn). ‘Seed’ and ‘archive’ collections are assumed to belong to the
same event; thus, this particular method does not deal with recognizing the event or
deciding whether two collections depict the same event.
Figure5.10 shows the framework of this method, which consists of two stages:
the pre-processing stage of the user collection and the actual contextualization stage.
During the ﬁrst stage, the photos of the ‘seed’ collection are analysed and the collec-
tion is split into sub-events using a clustering method (such as the method in [17]).
During the actual contextualization stage, similar and different sub-events of the
event are detected within the ‘archive’ collection and matched to ‘seed’ collection
sub-events, while a selected representative subset of their photos is used for enriching
and contextualizing the ‘seed’ collection. The ‘archive’ collections are considered to

162
M. A. Greenwood et al.
Fig. 5.11 An example of ‘seed’ and ‘archive’ collections sub-events. The symbols A to F denote
different sub-events of the overall event
be already analysed and organized in sub-events since this stage is performed itera-
tively as each new collection of photos is added to the ‘archive’ collection. Thus, as
depicted in Fig.5.10, the event-based clustering is executed only when a new ‘seed’
collection is submitted.
An example of the event-based clustering and matching stages of the contextu-
alization procedure is depicted in Fig.5.11, during which all collections (circles)
are split into sub-events (circle sectors). During the sub-event matching stage, the
sub-events of the ‘archive’ collections are matched to those of the ‘seed’ collection
(A and C sub-events are common between ‘seed’ and ‘archive’ collections, while
B, D, E and F sub-events are unique in the ‘archive’ collection). Depending on the
user needs, the photos that are used for contextualizing the ‘seed’ collection can be
selected from both similar sub-events (A and C sub-events) and different sub-events
(B, D, E and F sub-events of the ‘archive’ collections) with the percentage of photos
taken from similar and different sub-events being user-deﬁned.
Photo Analysis and Event-Based Clustering
We stress the importance of using multiple features for representing a photo, to
support the later stages of the event-based clustering and sub-event matching, partic-
ularly in the case of events with sub-events in multiple locations and/or temporally
overlapping. To this end, the following photo visual content features and metadata
are employed:
1. time information, i.e. the capture date and time of the photo;
2. geolocation, i.e. the GPS coordinates of the location at which the photo was
captured;
3. colour information, extracted using HSV colourspace histograms; and

5
Keeping Information in Context
163
4. concept detection scores, extracted by applying a concept detection algorithm
[269] resulting in a 346-dimensional vector, where each value is a score indicating
the probability of a particular concept being depicted in the photo.
Although time information can often be easily extracted from the EXIF metadata of
the photos, there are cases where it is not available (e.g. photos are captured using
an old camera, and no metadata are saved) or not directly exploitable (e.g. when
photo capture devices of different users are not synchronized or are set to different
time zones). In the latter case, and in order to render time information usable, a
multi-user time synchronization method must be used (such as [347]) to estimate the
temporal offset of each of the ‘archive’ collections to the ‘seed’ collection. Addi-
tionally, geolocation information might be missing from some photos, since not all
photo capture devices are able to include this type of information in a photo’s EXIF
metadata. Therefore, we cannot rely on such geolocation information, but only use
it as auxiliary information when available.
Sub-event Matching
Let S be the ‘seed’ collection that is clustered in NS sub-events Si, i = 1, . . . , NS.
Each sub-cluster Si contains NSi photos si, j, j = 1, . . . , NSi . For each photo si, j, we
can use the capture time si, j
T information, the GPS location si, j
G PS, the HSV histogram
si, j
H SV and the concept detection scores si, j
SC. Similarly, A j
k, j = 1, . . . , NAk is the jth
sub-event of the ‘archive’ collection of user k denoting the corresponding photo
information as ai, j
k,{H SV/G PS/SC/T }. To match a ‘seed’ collection sub-event to sub-
events of the ‘archive’ collections, four sub-event distance measures are introduced.
1. Temporal distance (time): this baseline approach utilizes the time information of
the photos. The distance between a ‘seed’ sub-event and an ‘archive’ one is the
minimum pairwise temporal distance of their photos, and is given by
DT (Si, A j
k) =
⎧
⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎩
if sub-events
min
m,n {|(si,m
T
−a j,n
k,T |)}, are not temporally
overlapping
0,
otherwise
where 1 ≤m ≤NSi and 1 ≤n ≤NA j
k.
2. Colourinformationdistance(HSV):inthisapproach,sub-eventdistanceisdeﬁned
as the minimum pairwise distance between the HSV colour histograms of the sub-
events photos:
DH(Si, A j
k) = min
m,n {dcos(si,m
H SV , a j,n
k,H SV )}
where dcos is the cosine distance.
3. Semantic distance (scores): in this case, we use the minimum pairwise distance
between vectors of concept detection scores:

164
M. A. Greenwood et al.
DSC(Si, A j
k) = min
m,n {dcd(si,m
SC , a j,n
k,SC)}
where dcd is a distance used for concept detection scores, given by Eq.6 in [294].
4. Signature distance (signature): while the aforementioned approaches employ
pairwise photo-based distances, an approach that calculates the distances among
entire sub-events without calculating pairwise photo distances can be adopted, in
which each sub-event is described by a single vector, namely, its signature. To
construct the signature vector of a sub-event, we calculate the mean and variance
of vectors of concept detection scores. We then keep the u concept indices with
the highest mean, and ﬁnally, we sort them in ascending order of their variances.
The signature vector is representative of the entire sub-event, since it contains
concepts that attained a high score (top u), while at the same time their position
in the vector shows the signiﬁcance of the speciﬁc concept (low variance).
The indices of the concept c of sub-events Si and A j
k are denoted as Si
SI Gc and
A j
k,SI Gc, respectively. If the concept c does not belong to the top u concept indices,
then its value is set to 0. The distance of two sub-event signatures equals the sum
of the distances DC of the signatures for all concepts.
DSI G(Si, A j
k) =
NC
	
c=1
DC(Si
SI Gc, A j
k,SI Gc)
(5.9)
If a concept c belongs in both signatures, then distance DC equals the W(p) · p
where p = |(Si
SI Gc −Ai
k,SI Gc)|. As a result, concept indices with low variance
have more impact on the distance calculation. The intuition of this distance mea-
sure is that two sub-events are close if they contain more common concepts having
high score and same positions. If a concept index exists only in one signature,
then DC is set to W(i)(u + 1), where i is the index of concept c in this signature.
Finally, if a concept index does not exist in any of the signatures, then DC is set
to W(p + 1) · (u + 1). Therefore, the distance measure based on the signatures
of sub-events is deﬁned as
DC(Si, A j
k) =
⎧
⎨
⎩
W(p)· p,
if Si
SI Gc >0 and A j
k,SI Gc >0
W(p)·(u+1),
if Si
SI Gc >0 xor A j
k,SI Gc >0
W(p + 1)·(u+1),
otherwise
The time measure can be combined with other sub-event distance measures. The
weighted sum of time and HSV, scores or signatures (DT,x = w · DT +
(1 −w) · D{x}) where x ∈{H SV, SC, SIG}) is tested in later experiments. Fur-
thermore, geolocation information is employed only to further reﬁne the matched
sub-events. A tGE O distance threshold is calculated that denotes if two sub-events are
considered spatially close or not. For the estimation of tGE O we cluster all pairwise
‘archive’ photo spatial distances using the k-means clustering algorithm, setting the
number of clusters k = 2. Threshold tGE O is set equal to the value of the lowest

5
Keeping Information in Context
165
cluster centre. Pairs of photos with capture location distance larger than tGE O are
excluded from the matching procedure. Finally, we select a subset of ‘archive’ sub-
events whose distance to a ‘seed’ sub-event is below the corresponding thresholds
(tT , tT,x, tx), where x ∈{H SV, SC, SIG}.
Seed Collection Enrichment
Having matched each ‘seed’ user sub-event to an ‘archive’ sub-event, we select the
photos of the matched ‘archive’ sub-events that are either similar or dissimilar to the
‘seed’ user collection photos, depending on the user needs. To achieve this, two user-
controlled parameters, a and b, are established. Parameter a (a > 0) controls the total
numberofphotosthatwillbeusedforcontextualizingthe‘seed’collection.Parameter
b (0 ≤b ≤1) speciﬁes what percentage of these photos should belong to sub-events
that are also contained in the ‘seed’ collection. We select the a · b · N ‘archive’ sub-
event photos (where N = NS
i=1 NSi is the total number of ‘seed’ collection photos)
that are most similar to the ‘seed’ collection sub-events, and from each ‘archive’
sub-event, we pick one photo which is the most dissimilar to the photos already
contained in this sub-event of the ‘seed’ collection. The set of the a · b · N photos is
the set of photos that contextualizes the sub-events of the event that were originally
contained in the ‘seed’ collection. The remaining a · (1 −b) · N photos that enrich
the ‘seed’ collection are collected from the most dissimilar sub-events. In the case
of b ≈0, the user requests the contextualization of their collection to be performed
by considering only different sub-events, and the selection is based on a measure
of photo signiﬁcance. In [109], a method that detects signiﬁcant events in personal
photo collections is presented. More speciﬁcally, the number of photos captured by
a user within a temporal window is modelled as a time series, which is subsequently
analysed to detect potential signiﬁcant events. We follow a simpler approach in which
the signiﬁcance of each photo is calculated according to the temporal distance from
the rest of the sub-event’s photos. The signiﬁcance SG of a photo a j,l
k which belongs
in sub-event A j
k is given by
SG(a j,l
k ) =
NA j
k
	
p=1
exp

−g|a j,l
k,T −a j,p
k,T |

By tuning parameter g, the exponential can become signiﬁcant only for photos that
are temporally close, while for the rest, the exponential is negligible. As a result,
the above sum depends also on the number of temporally close photos to a j,l
k . Given
a sub-event, the signiﬁcance of all photos is initially calculated, and then, the one
having the highest signiﬁcance is selected. If the ‘archive’ sub-event is dissimilar
to the ‘seed’ collection sub-events, then this photo is used as the representative one
for the ‘seed’ collection enrichment. If the ‘archive’ sub-event is similar to one of
the ‘seed’ collection sub-events and the selected photo is temporally far from the
‘seed’ collection photos, the photo is selected as the representative one for the ‘seed’
collection. Otherwise, the photo with the second highest signiﬁcance is examined.
This procedure is continued until a temporally distant photo is detected.

166
M. A. Greenwood et al.
Evaluation of Personal Photo Collection Contextualization Method
We used the two datasets11 of the MediaEval 2014 SEM task [83] for evaluating
the personal photo collection contextualization method proposed here. The goals of
this task are as follows: the temporal alignment of a series of photo galleries that
relate to the same event but have been collected by different users; the alignment
of the photos on a common timeline; and the detection of the sub-events attended
by the users. The Vancouver dataset consists of 1351 photos capturing various sub-
events of the Vancouver 2010 Winter Olympic Games, split into 35 user collections.
The London dataset consists of 2124 photos capturing various sub-events of the
London 2012 Olympic Games, split into 37 user collections. We used the Vancouver
dataset to estimate the threshold values (tT , tT,x, tx, where x ∈{H SV, SC, SIG}) for
each sub-event distance calculation approach and used their estimated values on the
London dataset, which are tT = 0.10, tT,H SV = 0.27, tT,SC = 1.05, tT,SI G = 0.69,
tH SV = 0.05, tSC = 0.8 and tSI G = 0.27.
In the experiments conducted on the London dataset, we consider the collection
of the ﬁrst user (user1) as the ‘seed’ collection, while the remaining 36 user collec-
tions form the ‘archive’ collections which are pre-processed. For the pre-processing
method of clustering to sub-events, we used the ground truth of MediaEval 2014
SEM task [83] in order to test the actual contextualization part of our method. We
also performed a set of experiments considering a subset of user collections (speciﬁ-
cally those which contain at least two sub-events) as ‘seed’ collections and averaged
the results. Finally, it is worth noting that geolocation information is available for a
small portion of photos in this dataset.
We evaluate the results of our approaches on sub-event matching using the estab-
lished precision (P), recall (R) and F-measure (F1) measures. Table5.8 shows the
results for each matching approach, using the ﬁrst user of the London dataset as
‘seed’ and the rest of the users as the ‘archive’ collections, since the ﬁrst user has
the most extended collection of sub-events.
We observe in Table5.8 that the baseline method does not perform well, due to
the London dataset containing temporally overlapping events. Combining time infor-
mation with the other features, a signiﬁcant boost is achieved (compare time+HSV,
time+scores and time+signatures to HSV, scores and signatures approaches, respec-
tively). Speciﬁcally, time+HSV approach achieved the highest F1 measure. Fur-
thermore, in the case where the time information is not available, the scores-based
approach can distinguish sub-events better than the other proposed approaches.
These observations using the ﬁrst user as the ‘seed’ collection are conﬁrmed for
the case where we use the union of several user collections as the ‘seed’ collection,
in Table5.9. The selection of user collection to be used as the ‘seed’ was based on
the number of sub-events included to be at least 5.
We numerically evaluate the impact of using these photos for contextualization by
examining the sub-events that they contain. Speciﬁcally, we establish and use three
evaluation measures:
11The datasets are publicly available at http://mmlab.disi.unitn.it/MediaEvalSEM2014/.

5
Keeping Information in Context
167
Table 5.8 Different sub-event matching methods evaluation using user1 collection as the ‘seed’
Method
P
R
F1
Time
0.466
0.872
0.607
Time + HSV
0.894
0.764
0.824
Time + scores
0.765
0.750
0.757
Time + signatures
0.677
0.880
0.765
HSV
0.840
0.568
0.677
Scores
0.659
0.763
0.707
Signatures
0.773
0.425
0.548
Table 5.9 Different sub-event matching methods evaluation using multiple users’ collections as
the ‘seed’ and averaging the results
Method
P
R
F1
Time
0.399
0.872
0.536
Time + HSV
0.702
0.724
0.702
Time + scores
0.685
0.681
0.666
Time + signatures
0.587
0.675
0.611
HSV
0.567
0.447
0.462
Scores
0.624
0.549
0.525
Signatures
0.544
0.323
0.392
• Percentage of Similar (PoS): out of the photos of the ‘archive’ collections that
were selected on the basis of representing similar sub-events; we measure the
percentage that truly belongs to such sub-events. This measure ranges from 0 to
1, where 1 is the optimal.
• Percentage of Dissimilar (PoD): out of the photos of the ‘archive’ collections that
were selected on the basis of representing dissimilar sub-events; we measure the
percentage that truly belong to such sub-events. This measure ranges from 0 to 1,
where 1 is the optimal.
• Cluster Recall (CR): here we measure the coverage increase after contextualiza-
tion. The initial coverage of the ‘seed’ collection is calculated based on the number
of different sub-events of the total number of sub-events contained in the overall
event. By contextualizing the collection, we attempt to include more sub-events
into the ‘seed’ collection and increase the coverage.
Figure5.12 illustrates the values of these measures when varying the values of
parameters a and b, while indicative results for selected values of a and b are shown
in Table5.10. As far as the CR measure is concerned, the user1 ‘seed’ collection
consists of 46 sub-events out of the 238 total sub-events before contextualization,
which is equal to 0.1932. As parameter a increases, meaning that the user selected
to increase the number of photos that are used for contextualizing the ‘seed’ collec-
tion, the CR measure also increases and reaches almost 0.3. This indicates that the

168
M. A. Greenwood et al.
Fig. 5.12 Evaluation results using the three deﬁned evaluation measures, for different a and b
parameters a percentage of similar, b percentage of dissimilar and c cluster recall
contextualized ‘seed’ collection offers a broader coverage of the event, in comparison
to the information contained in the ‘seed’ collection prior to contextualization.
An example of the contextualization results, for both similar and dissimilar sub-
events, is shown in Fig.5.13. These sub-events were selected by applying the per-
sonal photo collection contextualization method on the London dataset. This example
shows how the ‘seed’ collection is contextualized with different photos. In Fig.5.13a,
the ‘seed’ collection photos are illustrated, grouped in sub-events using ground truth.
It seems that this collection contains photos from a part of the opening and award cer-
emonies, and the rowing (coxless pair, eight, single scull, quad scull), weightlifting,
soccer, marathon, long jump, track, wrestling, tennis, beach volley and judo compe-
titions. Figure5.13b shows the photos of similar sub-events that were chosen from
the ‘archive’ collections for contextualization. Finally, Fig.5.13c shows the photos
that were selected from the sub-events not present in the ‘seed’ collection. These
include photos from the taekwondo, cycling, fencing, basketball and horse riding
competitions, as well as different parts of the opening and award ceremonies.

5
Keeping Information in Context
169
Table 5.10 Percentage of Similar (PoS), Percentage of Dissimilar (PoD) and Cluster Recall (CR)
clustering evaluation measures for different a and b parameters
Method
a = 0.5
a = 1
b = 0.2
b = 0.5
b = 0.8
b = 0.2
b = 0.5
b = 0.8
PoS
0.8
0.76
0.57
0.83
0.58
0.46
PoD
0.82
0.84
0.90
0.72
0.81
0.88
CR
0.22
0.26
0.27
0.25
0.28
0.29
Fig. 5.13 a ‘seed’ collection sub-events. b Photos added through contextualization that belong to
sub-events already contained in the ‘seed’ collection. c Photos added through contextualization that
belong to different sub-events of the same event

170
M. A. Greenwood et al.
5.2.2.2
Photo Collection Contextualization Using Web Data
The personal photo contextualization method discussed in Sect.5.2.2.1 has two lim-
itations: (a) The collection to be contextualized must be about a single event that
several other users have also attended and created their own photo collections for,
and(b)theotherusersthatattendedtheeventmusthavetheircollections(the‘archive’
collections) submitted and pre-processed by the method, i.e. they must have been
temporally aligned using a time synchronization algorithm (e.g. [347]) and split
into sub-events using an event-based clustering algorithm (see Sect.3.5) prior to
the actual contextualization. The photo contextualization method presented in this
section draws information using the web as a ‘world knowledge’ pool, thus over-
coming the aforementioned limitations. The need for other photo collections from
different users attending the same event is substituted with the input of a short textual
description of the event.
An overview of this method is illustrated in Fig.5.14. This method takes as input
a photo collection that has been captured by a user attending an event and a textual
description of the event that contains the name, location and time of the event. Web
photos that are related to the event are retrieved and downloaded, their visual content
is analysed and a selection of them is provided to the user as additional contextual
content.
First, the user’s photo collection is fed into an image analysis component. Using
the Caffe framework [181] and the 22-layer GoogLeNet Deep Convolutional Neural
Network (DCNN) pre-trained model [377], the loss1/classiﬁer and loss3/classiﬁer
layers’ output is extracted (resulting in two 1000-dimensional vectors). The L*a*b*
colourspace colour histogram of each photo is also calculated, using 8 bins for the
L* plane, 32 bins for the a* plane and 32 bins for the b* plane (resulting in a
72-dimensional vector). The 2072-dimensional concatenated feature vector of the
DCNN layers output and the L*a*b* histogram are used to represent each photo in
the user collection. The textual description that must be provided by the user consists
of the name of the event (e.g. Olympic Games), the location (e.g. London) where the
event took place and the date that the event took place (e.g. 2012). This information
is then used to create a set of queries which are sent to web search engines, and a
pool of data is collected, from which the contextual information will be selected.
The following three web queries are performed:
1. A query to the Google text search engine; the P ﬁrst web pages and the Y ﬁrst
YouTube links of the search results are kept. Moreover, the photos contained in
the returned web pages are also collected. The query to the Google text search
engine is formed by combining the textual description provided by the user, using
AND (&) (i.e. event & location & time).
2. A query is sent to the Bing search engine; the B top-ranked photos are collected.
The query is generated similarly to the previous one.
3. A query is sent to the Flickr search engine; the F top-ranked photos are collected.
In the case of Flickr, the Flickr API allows us to add constraints to the queries.
Thus, apart from the initial query (i.e. event & location & time), we also send

5
Keeping Information in Context
171
Fig. 5.14 Overview of the photo collection contextualization method that uses web data
two additional queries; one limiting the location of the place where the photos are
taken (assuming that they contain location coordinates) and the other limiting the
time that the photos have been taken (assuming that this information is available
at the photo’s metadata). In case of limiting the location, the query is formulated
using the event and location terms (i.e. event & location), while for limiting the
time, only event and time terms are used (event & time).
The number of returned items for each query (P, Y, B, F thresholds) is selected
empirically. The links of web pages and YouTube videos are directly returned to the
user as contextual information. The set of photos collected in the above steps is used
as the pool of data from which the contextualization information is retrieved. This

172
M. A. Greenwood et al.
pool, denoted as web photos from now on, serves a similar purpose to the archived
collection set of the method discussed in Sect.5.2.2.1. Speciﬁcally, these web photos
are fed into the image analysis component, and the same features as in the case of
the user-supplied photo collection are extracted.
To perform the actual contextualization step, we construct a similarity matrix
W between the Nw web photos and Nu user photos, so that Wi, j denotes the sim-
ilarity between the ith photo of the web photos to the jth photo of the user data
(i ∈[1 . . . Nw], j ∈[1 . . . Nu]). Four thresholds t1, t2, t3 and t4 are established, with
t1, t2, t3, t4 ∈[0 . . . 1] and t4 < t3 < t2 < t1. All pairs of web photos and user photos
are examined:
• An (i, j) pair with t2 < Wi, j < t1 is considered contextually similar. These photos
are similar to those of the user photo collection but not identical and are used to
describe in more detail a sub-event that the user has also captured.
• An (i, j) pair with t4 < Wi, j < t3 is considered contextually dissimilar. These pho-
tos are from the same event, but quite different from those of the user photo col-
lection, and are used to describe the event from a different aspect to the user.
• An (i, j) pair with Wi, j > t1 is considered as a pair of duplicates, and these photos
are discarded.
• An(i, j)pairwith Wi, j < t4 isconsideredirrelevant,andthesephotosarediscarded.
• An (i, j) pair in the interval t2 < Wi, j < t3 is omitted to get more accurate results,
since it is not clear if these photos should be considered contextually similar or
dissimilar to the user photo collection.
In order to make sure that no identical or near-duplicate photos are selected from
web photos, a similarity matrix of all photos in the web photos collection is con-
structed, using the Euclidean distances of their features. This similarity matrix is
considered as the weight matrix of a graph, and subsequently, the strongly con-
nected components of the graph are found. Each strongly connected component is
a sub-graph in which every node is connected to every other node in the sub-graph.
Therefore, a strongly connected component is a group of very similar photos in the
web photos collection. Each time a photo from the web photos is selected for con-
textualization, the strong component of the web photos graph in which this photo
belongs is marked as used, and no other photos are selected from it.
The values of internal parameters t1, t2, t3 and t4 are set empirically to, respectively,
0.95, 0.89, 0.76 and 0.70. The implemented method requires only two user-controlled
inputparametersa(a > 0)andb(0 < b < 1).Similartothepersonalphotocollection
contextualization method presented in Sect.5.2.2.1, a controls the number of web
photos that will be used for contextualizing the user collection, while b speciﬁes the
percentage of the latter photos that should belong to the contextually similar set; the
rest of the web photos that will be used for contextualizing the user collection are
drawn from the contextually dissimilar set.
Evaluation of Photo Collection Contextualization Using Web Data Method
The method is tested again using the London dataset of the MediaEval SEM task
[83]. We randomly selected 129 photos covering all sub-events of the dataset and

5
Keeping Information in Context
173
Fig. 5.15 a User collection. b Contextually similar web photos. c Contextually dissimilar web
photos. d Irrelevant web photos
created the event name of the textual description as Olympic Games, the location as
London and the year as 2012. 588 photos were downloaded from the web and served
as the web photos collection.
Due to lack of ground-truth data for the photo collection gathered from the web,
we evaluate the results of this contextualization method visually. A subset of the
user collection is shown in Fig.5.15a, and the retrieved web photos collection is
shown in Figs.5.15b–d. Figure5.15b contains the contextually similar photos and
we observe that these depict sub-events that are contained in the user collection,
but from different aspects. On the other side, Fig.5.15c contains the contextually
dissimilar photos, depicting sub-events that are not contained in the user collection.
Finally, the rest of the photos retrieved from the web, in Fig.5.15(d), are the photos
that were not used for contextualization.
Additionally, the following web page links are returned by the web queries and
provided as contextual information:
• http://www.olympic.org/london-2012-summer-olympics
• https://en.wikipedia.org/wiki/2012_Summer_Olympics
• https://en.wikipedia.org/wiki/London_Olympics
• http://www.miniclip.com/games/london-2012-olympic-games/en/
In conclusion, we can see how the resulting contextualized collection includes
complementary information for the events that were attended by the user that sub-
mitted the original collection which should help to aid the better understanding of
those events. It also includes additional information about events the user did not
attend, in order to give a more complete view of the whole Olympics. Both of these

174
M. A. Greenwood et al.
aspects ﬁt perfectly with the general model of contextualization detailed at the begin-
ning of this chapter.
5.3
Context Evolution
The idea behind context evolution is to give some sense of how the world (your own
personal sphere as well as the world in general) changes over time. Such changes
clearly affect our ability to understand previously archived documents, as our current
world view may well be very different to that when the document was originally
produced. For the purpose of modelling context evolution, context can be structured
or unstructured. While both forms evolve over time, we believe that this happens in
different ways, and we therefore need differing approaches to modelling this.
Structured data, by their very nature, are curated resources. This curation may
be an active process where an ontology or data source is knowingly updated and
maintained, or an unintended side effect, such as adding a new contact to an email
client, which provides structured data that could be used for contextualization. In
both cases, changes in the data are handled by an existing process. This applies
equally to large sources of world knowledge (such as the fairly frequent updated
releases of DBpedia) and to personal data. In structured resources, the evolution of
context information is often already handled as new data supplements, rather than
replaces existing information.
In contrast, unstructured context is entirely lacking in information relating to how
it might have evolved over time. For example, this context may take the form of topics
relevant to preserved documents, which give a sense of what a document might be
talking about, but as they may not be linked to any other information, it is not clear
exactly what they refer to and how that meaning might change from one document to
another. Thus, extra information that can be extracted about the terms and how they
evolve would be beneﬁcial. This may involve explicitly determining the evolution of
a term/topic or it might revolve more around the way the information is presented,
allowing users to easily infer the evolution aspects.
Imagine that the data contain a simple statement: ‘the situation in Iran is obviously
worrying and the Foreign Ofﬁce is keeping a close watch on events’. It is likely that
both ‘Iran’ and ‘Foreign Ofﬁce’ would be linked to a structured source of context
information (world and organizational in turn) and while useful, neither gives a sense
of what the ‘situation in Iran’ might refer to. The date of the document will, however,
give some sense of what this might be; for the late 1970s to early 1980s, it would
most likely be referring to the revolution, while since the late 1990s, it is likely to
refer to their nuclear research programme, and it may well refer to human rights
issues regardless of the time period. We therefore need to know how the world has
evolved to set the document in the correct context.
In the rest of this section, we detail a number of approaches to context evolution
that are able to process either structured or unstructured context.

5
Keeping Information in Context
175
5.3.1
Structured Context
In most cases, structured knowledge which could be viewed or used as context
is produced and maintained by some form of manual process. This process may
be speciﬁcally aimed at producing structured knowledge, such as the construction
and maintenance of an ontology, or just a side effect of using an application. For
example, adding the contact details for a new employee to the address book in an
email client has the side effect of producing structured knowledge that could be used
for contextualization. Regardless of the approach, the result is the same; the issues
are how the data evolve over time, how we can capture this information and how the
changes can be reported or explained to the user.
Our approach to modelling the evolution of context revolves around being able
to track how structured information changes over time. We take a similar approach
to [124] and adopt the idea of Valid Time: a set of points or intervals of time in
which a single fact is known to be true. Ideally, this would involve having accurate
timestamps associated with every piece of information, but this is not always feasible.
For example, an ontology does not allow properties on relations, making it impossible
to timestamp the addition of a relation between two existing entities. In the worst-
case scenario, there may only be periodic updates of the data with only a single
timestamp on the entire knowledge repository. In such an extreme case, the changes
would need to be determined via ﬁnding all differences, and each change could not be
individually assigned an accurate Valid Time. For the purpose of further discussion,
we will assume that some intermediate situation holds in which the evolving data,
but not necessarily all data, have a Valid Time associated with it.
We have previously assumed that information useful for contextualization will be
stored as an ontology, and so the ﬁrst step in handling the evolution of this information
is to specify how information on Valid Time should be encoded within it. Since
relations within an ontology cannot have properties, in a standard ontology it is only
possible to associate timestamps with instances. Fortunately, this only really affects
datatype properties (i.e. those that lead to a value such as a string or number rather
than an object property that leads to another instance in the ontology), and it is always
possibly to rewrite a datatype property as an object property if required.
With the focus now on recording Valid Time for object properties, it is a simple
case of adding new datatype properties to each instance that records the period for
which they are valid. This could use the standard xsd:datetime format to allow
timestamps to be accurate to the minute (i.e. 2014-05-23T10:20:13+05:30), or could
be a simpler integer representation of a date if accuracy only to the day is required
(i.e. 20140523).
Let us assume that we are working with an organizational ontology that includes
people and the jobs they hold. Stored as triples, this may give us information such
as the following (using an oo preﬁx to stand for organizational ontology):
<oo: Employee1 >
<rdfs :label >
" Joe
Bloggs "
<oo: Employee1 >
<oo: jobTitle >
" Cleaner "

176
M. A. Greenwood et al.
While simple, this approach does not allow for changes in job title over time
to be recorded. Switching from a datatype property to an object property allows
us to extend the data with Valid Time information, stored as an integer in the form
yyyyMMdd, as follows:
<oo: Employee1 >
<rdfs :label >
" Joe
Bloggs "
<oo: Employee1 >
<oo: jobTitle >
<oo:Job1 >
<oo:Job1 >
<rdfs :label >
" Cleaner "
<oo:Job1 >
<oo: fromdate >
20080305
<oo:Job1 >
<oo: todate >
99991231
This extended ontology now shows that the employee was hired as a cleaner on
the 3rd of May 2008, while the far future date, in the todate property, shows that
they are still working in that role at the time of the ontology creation. Now let us
assume that after just over 2 years, Joe is promoted to a supervisory role, and that
his change in job title is added to the ontology.
<oo: Employee1 >
<rdfs :label >
" Joe
Bloggs "
<oo: Employee1 >
<oo: jobTitle >
<oo:Job1 >
<oo:Job1 >
<rdfs :label >
" Cleaner "
<oo:Job1 >
<oo: fromdate >
20080305
<oo:Job1 >
<oo: todate >
20100607
<oo: Employee1 >
<oo: jobTitle >
<oo:Job2 >
<oo:Job2 >
<rdfs :label >
" Cleaning
Supervisor "
<oo:Job2 >
<oo: fromdate >
20100607
<oo:Job2 >
<oo: todate >
99991231
This approach also allows multiple overlapping pieces of information. For exam-
ple, let us assume that cleaning was not a full-time job, and so after working for the
company for a year, up until the time he was promoted to supervisor, Joe was also
employed as a porter.
<oo: Employee1 >
<rdfs :label >
" Joe
Bloggs "
<oo: Employee1 >
<oo: jobTitle >
<oo:Job1 >
<oo:Job1 >
<rdfs :label >
" Cleaner "
<oo:Job1 >
<oo: fromdate >
20080305
<oo:Job1 >
<oo: todate >
20100606
<oo: Employee1 >
<oo: jobTitle >
<oo:Job2 >
<oo:Job2 >
<rdfs :label >
" Cleaning
Supervisor "
<oo:Job2 >
<oo: fromdate >
20100607
<oo:Job2 >
<oo: todate >
99991231
<oo: Employee1 >
<oo: jobTitle >
<oo:Job3 >
<oo:Job3 >
<rdfs :label >
" Porter "
<oo:Job3 >
<oo: fromdate >
20090305
<oo:Job3 >
<oo: todate >
20100606
Notice how each change to the ontology is now non-destructive. The only changes
made are additions or setting the todate data property to a valid date stamp rather

5
Keeping Information in Context
177
than the far future ﬁller value. This means that no information is ever lost, and
so a single current snapshot of the ontology contains all the information related to
documents written at any time during the organization’s history (or at least since they
adopted this approach to data storage).
One potential problem with using such an ontology for contextualization is that it
now includes information which was true but is no longer so. For example, in the last
ontology snippet given above, we can see that information on Joe Bloggs being both
a cleaner and porter is present, although at the current time he is only employed as a
cleaning supervisor. That means that we need to be careful to restrict the parts of the
ontology used for contextualizing a document to only those which are valid at the
time the document was created. Fortunately, it is easy to add a ﬁlter to any SPARQL
query to extract speciﬁc elements of an ontology. The following query extracts a list
of employees and their job titles on the 1st April 2009.
select
?e
?j
where
{
?e
<oo: jobTitle >
?j
.
?j
<oo: fromdate >
?f
.
?j
<oo: todate >
?t
FILTER
(?f
<=
20090401
&&
?t
>=
20090401)
}
Using our example ontology, this query would return Joe Bloggs employed as a
cleaner. If we are only interested in the current status, then we can utilize the far
future date to select just those items that are currently valid (avoiding the ﬁlter, in
this case, should increase the performance):
select
?e
?j
where
{
?e
<oo: jobTitle >
?j
.
?j
<oo: fromdate >
?f
.
?j
<oo: todate >
" 99991231 "^^< http :// www.w3. org
/2001/ XMLSchema #int >
}
With this ability to determine the state of the ontology at any point in time, it is
possible to track changes over time. A timeline showing the jobs held by a given
person over a given time period is now easy to produce. The problem is that, even in
short documents, a lot of context information may be present, and if a long time has
elapsedsincethedocumentwasoriginallywritten,thenitscontextcouldhaveevolved
signiﬁcantly. While it may be possible to display all the changes to the user in some
fashion, it is likely to simply lead to information overload. The solution, although not
ideal, is to manually weight domain-speciﬁc properties in order to focus on showing
the evolution of the most important concepts and relationships within the ontology.
While this approach to handling context evolution within a structured knowledge
source is quite easy to adopt and use, it should be noted that having information and
it being useful are two separate concepts. As with many of the techniques detailed in
this chapter, a good user interface would be required to surface this information in a
usable fashion. User applications are, however, outside the scope of this chapter.

178
M. A. Greenwood et al.
5.3.2
Unstructured Context
As explained earlier, unstructured context is entirely lacking in information relating
to how it might have evolved over time. In this section, we focus on the evolution
of topics, illustrated via the following problem: Given a topic or entity of interest q,
such as Iran or Germany, determine the context evolution of that entity over a given
period of time. The problem contains two subtasks: (i) identify the context that the
entity is involved in at a speciﬁc time, and (ii) determine the evolution of the context
over time.
To tackle this problem, we propose an approach which consists of three main
steps: Document Gathering that collects relevant documents for each topic (where a
topic is a ‘context hook’ as deﬁned on page 136) used as a query q, Context Extraction
extracts associated topics from those documents and Context Evolution determines
which topics are relevant at a particular point in time.
The ﬁrst step is to gather relevant documents from a time-stamped collection
for a given query. The time-stamped corpus contains a set of articles with speciﬁc
publication dates; e.g. New York Time articles published from 1987 to 2007, and
UK Parliament debates spanning from 1935 to present. In order to obtain relevant
documents, we use a query-likelihood language model that determines the similarity
of the query q with documents from the corpus (as described in Sect.5.2.1.2). The
likelihood of the query q from a language model estimated from a document d with
the assumption that query terms are independent is calculated as follows:
P(d|q) ∝P(d)

w∈q
P(w|d)n(w,q)
where w is a query term in q, n(w, q) is the term frequency of w and q, and P(w|d)
is the probability of w estimated using Dirichlet smoothing:
P(w|d) = n(w, c) + μP(w)
μ + 
w′ n(w′, c)
where μ is the smoothing parameter, and P(w) is the probability of each term w in
the collection.
In order to extract context in which a given entity is involved, we consider topics
mentioned in documents as context and make use of Latent Dirichlet Allocation
(LDA) to accomplish this task. LDA is a generative model of documents. Assuming
that we have a corpus D containing a set of documents, and each document d is
represented by a bag of words. LDA assumes that every word in a document is
generated by ﬁrst picking a latent topic and then sampling from the distribution
of words conditioned on the selected topic. Probability distributions obtained after
training an LDA model include P(w|z), the probability word w given topic z, and
P(z|d), the probability of topic z given document d.

5
Keeping Information in Context
179
When attempting to determine context evolution, the ﬁrst task is to determine
the signiﬁcant years for a given entity. This can be estimated in a straightforward
way, by counting how many times each year has been mentioned in the documents
in the time-stamped corpus. In addition, we want to see the reason why a particular
year has been mentioned frequently and how the context that the entity involved in
changes during the time. For this purpose, we estimate the topic distribution of each
signiﬁcant year. Let P(z|d) be the topic distribution of each document returned by
LDA, and let Dy be the set of documents mentioning y as their publication dates.
We can estimate the topic distribution of each year, i.e. P(z|y) using the following
equation:
P(z|y) =
1
|Dy|
	
d∈Dy
P(z|d)
Using the probability P(z|y), we can determine a set of topics that the query entity
is heavily associated with for a given year y. This information can be represented as
context evolution timelines.
In order to determine the evolution of a speciﬁc topic over time, we can compute
the probability P(y|z) as follows:
P(y|z) = P(z|y) × P(y)
P(z)
P(y|z) ∝P(z|y) × P(y)
where P(z|y) is computed as described above, and P(y) is estimated as
P(y) =

d∈D;yd==y 1
|D|
where |D| is a total number of documents in the collection.
To illustrate the effectiveness of the proposed approach, we qualitatively analyse
the results using several entity queries on the UK Parliament Debate corpus.12 Let
us consider ‘Iran’ as the entity of interest. Our method ﬁrst identiﬁes a list of most
important years, based on how frequently the entity was discussed in each year. In this
example, 1991, 2010 and 2014 are determined signiﬁcant years for the entity ‘Iran’ in
the corpus. For each signiﬁcant year, a set of related contexts is then extracted, where
each context is represented by a list of correlated words, as shown in Table5.11.
The proposed method can also analyse the evolution of extracted contexts over
time. For example, the ‘iraq war saddam’ topic was mostly discussed in 1991 but
was frequently mentioned again in 1999. It reﬂects the conﬂict between Iran and Iraq
during such years. Similarly, the topic of Russia’s relation with Iran occurred in the
debate corpus in 1980, but received high attention again in 2014.
These results clearly show that the set of topics related to a given ‘context hook’
changes over time, and that visualizing those changing topics can give an insight into
how the meaning of the context hook has changed over that time period.
12http://parser.theyworkforyou.com/hansard.html.

180
M. A. Greenwood et al.
Table 5.11 Example of
signiﬁcant years and related
topics for the entity Iran
Year
Topics
1991
Iraq war saddam hussein
House debate government statement
Israel middle east peace
2010
Afghanistan pakistan security force
Iran russia sanction foreign
European europe union minister
2014
Syria minister government libya
Iran russia sanction foreign
Afghanistan pakistan security force
5.4
Practical Uses for Context Information
While the main aim of contextualization is to provide a context in which previously
archived documents can be interpreted, this does not prohibit the information from
being utilized to provide an immediate beneﬁt to those adopting contextualization
within their applications and frameworks. One place where context can play an
important role is within a search system, by allowing related information to be used
to answer search queries. Adoption of the approaches we have outlined can thus have
an immediate as well as long-term beneﬁt. In fact, such an approach often allows
relevant documents that would never be found by a conventional text search engine
to be retrieved and highly ranked. In the rest of this section, we describe one such
search system, GATE Mímir, which can make use of context information to provide
a rich search experience.
GATE Mímir13 [104] is an open source, multi-paradigm information management
index and repository which can be used to index and search over text, annotations
and context information. It allows queries that arbitrarily mix full-text, structural,
linguistic and context queries, and that can scale to gigabytes of text. To support
such queries, a GATE Mímir index allows combinations of text (to enable full-
text search); annotations (linguistic metadata associated with the text, e.g. ‘Person’,
and including wildcards, standard binary operators, etc.); and context data (world
and personal knowledge stored in an ontology, which allows the system to perform
generalisations based on knowledge that a city is located in a particular country).
It is worth noting that in our example, the related data (organizational or personal
depending on the application) are actually used both during the annotation of the
documents and at search time, although the two uses differ substantially. The data
are used to both contextualize the documents (i.e. to annotate them with instance
data) and then again at search time where related information can be utilized to
fulﬁl the search query. The main advantage of this approach over existing text search
engines is that there are interesting questions which can only be answered using
13http://gate.ac.uk/family/mimir.html.

5
Keeping Information in Context
181
the additional data. The combination of text, annotations and context data allow, for
example, us to ﬁnd documents using queries such as:
{ Person
sparql
=
" SELECT
? inst
WHERE
{? inst
: birthPlace
<http :// dbpedia . org/ resource /
Sheffield >}"
}
[0..4]
root : say }}
This query basically says: ﬁnd all the places at which a Person annotation, who
is mentioned in DBpedia as being born in Shefﬁeld, occurs within at most ﬁve words
of the verb ‘to say’. It is important to note that few, if any, of the documents which
would match this query mention place of birth; that information would be present in
the ontology used for contextualization.
While we have shown just a single complex query that includes context, this
has demonstrated how powerful combining information in this way can be. This
approach allows us to provide an immediate beneﬁt to users adopting our approaches
to contextualization as part of a preservation strategy, rather than the focus being
purely on how the data can be used in some distant future to look back at old data, a
situation that might, initially at least, seem less interesting to potential adopters. For
many more examples of how useful this approach, see [281].
5.5
Summary and Discussion
We started this discussion of contextualization with the quote You shall know a word
by the company it keeps [137], and throughout the chapter have introduced a number
of concepts, ideas and approaches which embody this notion, while extending it
beyond a single word to encompass textual documents and images.
The techniques we have described are based on our general approach to contex-
tualization (detailed on page 136) in which a ‘contextualization method’ attempts to
associate extra information, referred to as ‘context units’, with speciﬁc elements of
a document, which we call ‘context hooks’. While we have not detailed approaches
for all document types, we have presented algorithms for contextualizing short social
media posts, longer text documents (such as newspaper articles) and two approaches
for contextualizing image collections, which together cover a large proportion of the
documents encountered on a daily basis. Context information, as with any other type
of information, is only useful if it is kept up to date, and for this, we have introduced
two approaches to context evolution: one that handles structured context information,
and one which attempts to determine the evolution of unstructured information.
Taken together, these approaches generate a potentially vast trove of context infor-
mation that can be preserved alongside documents to enable them to be unambigu-
ously interpreted at a later date. The information can, however, have an immediate
use in providing data for search and other document navigation tasks. While such
tools are primarily UI related, and so outside the scope of this chapter, we have
presented one possible search use case to illustrate this point more clearly.

182
M. A. Greenwood et al.
To conclude, information is deﬁned by the context, both world and personal, in
whichitwasﬁrstcreated.Thatcontextneedstobecaptured,storedandpresentedback
to users if we stand any chance of ensuring the information stays fully understandable
over long periods of time. The techniques described in this chapter are important steps
in this direction and should allow others to take us further down this road.

Chapter 6
Bridging Information Management
and Preservation: A Reference Model
Francesco Gallo, Claudia Niederée and Walter Allasia
Abstract The idea of the Preserve-or-Forget (PoF) approach introduced in this book
is to follow a forgetful, focused approach to digital preservation, which is inspired
by human forgetting and remembering. Its goal is to ease the adoption of preserva-
tion technology especially in the personal and organizational context and to ensure
that important content is kept safe, useful, and understandable in the long run. For
this purpose, it stresses the smooth interaction between information management
and preservation management. Leveraging the PoF approach, in this chapter we
introduce a reference model, which will be referred to in the following as PoF Ref-
erence Model. The model pays special attention to the functionality which bridges
between Information Management System (Active System) and Digital Preservation
System (DPS), such as the selection of content for preservation and the transfer of
content between the systems. The model aims to encapsulate the core ideas of the
PoF approach, which considers Active System and DPS as a joint ecosystem into a
re-usable model, and is inspired by the core principles of this approach: synergetic
preservation, managed forgetting, and contextualized remembering. The design of
the PoF Reference Model was driven by the identiﬁcation of ﬁve required character-
istics for such a reference model: it has to be integrative, value-driven, brain-inspired,
forgetful, and evolution-aware. The PoF Reference Model consists of a functional
part (Functional Model) and of an associated Information Model. The Functional
Model is made up of three layers: Core Layer, Remember and Forget Layer, and
Evolution Layer. For each layer, we discuss the main functional entities and the rep-
resentative workﬂows, also relating them to existing standards and practices in digital
preservation. The functionality required to mediate between the Active System and
the DPS has been encapsulated into the PoF Middleware, which has been designed
F. Gallo (B) · W. Allasia
EURIX Srl, Torino, Italy
e-mail: gallo@eurixgroup.com
W. Allasia
e-mail: allasia@eurixgroup.com
C. Niederée
L3S Research Center, Hannover, Germany
e-mail: niederee@L3S.de
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_6
183

184
F. Gallo et al.
and implemented as part of the ForgetIT project. The Information Model describes
the preservation entities and their relationships, also discussing the interoperability
with existing digital preservation standards.
6.1
Introduction
In this chapter, we present a model and an implementation for the novel approach
to intelligent preservation management propagated in this book, which is inspired
by the three principles: synergetic preservation, managed forgetting, and contextual-
ized remembering— the PoF approach, for short. The model discussed here will be
referred to in the following as PoF Reference Model. We follow a forgetful, focused
approach to digital preservation, which is inspired by human forgetting and remem-
bering. Its goal is to ease the adoption of preservation technology especially in the
personal and organizational context and to ensure that important content is kept safe,
useful, and understandable in the long run.
The PoF Reference Model leverages the ideas of the approach to a conceptual
level: required conceptual functionalities, core processes, and relevant concepts are
collected in a systematic way and are related to each other. The model aims to
encapsulate the core ideas of this approach into a re-usable model. Similar to the
OAIS Reference Model [67], the model described here deﬁnes the terminology and
concepts for the approach, which can be used as a basis for the implementation of
the approach as well as for the further discussion and development of a forgetful
approach to preservation.
The PoF Reference Model targets personal as well as organizational preserva-
tion settings, which are not covered by legal regulations such as deposit laws. The
focus is not on supporting memory institutions, although parts of the approach (e.g.,
contextualization) might be applicable in this area as well. The selected settings can
especially beneﬁt from the PoF approach, since (a) there is a big gap to preservation
adoption in those ﬁelds, (b) there are explicit preservation choices to be made, and
(c) there is a need for automation of the processes, in order to reduce the amount of
investment required for preservation.
Since our approach stresses the smooth interaction between the so far mainly
separated worlds of information management and preservation management, the PoF
Reference Model pays special attention to the functionality which bridges between
the Information Management System (which we call Active System) and the Digital
Preservation System (DPS). This includes the selection of content for preservation,
the transfer of content between the systems, contextualization for easing long-term
interpretation, processing during preservation time in reaction to changes, as well
as access to the joint information space populated both by preserved content and
content in active use.
In order to support a smooth transition between the Active System and the DPS, in
the PoF Reference Model we are considering active information use and preservation
as part of a joint ecosystem, stressing the smooth transitions and the synergetic

6
Bridging Information Management …
185
interactions rather than the system borders. This is also a core distinction from the
OAIS model, which is restricted to the DPS.
It is worth noting that the PoF Reference Model does not intend to replace the
OAIS model and other preservation standards. As mentioned above, it has a different
focus than OAIS and can interact with OAIS compatible approaches.
In the following, we discuss a functional and an information part of the PoF
Reference Model, which are referred to in the following as Functional Model and
Information Model, respectively.
In this chapter, we also present a possible mapping of this joint ecosystem onto
three separate systems, i.e., the Active System (in the form of adapters and/or system
extensions), the DPS (typically OAIS based), and a middleware component, named
PoF Middleware, which couples both systems in a ﬂexible way. The PoF Middleware
is the core part of the exemplary implementation of the PoF approach.
6.2
Foundations of the PoF Reference Model
The goal of the PoF Reference Model is to ease the use of preservation systems.
In a nutshell, it should be easy to get things preserved, the right things should
be preserved and the things should stay useful in the long term. Thus, we have to
consider the full cycle of getting things into the DPS, managing them there for a
considerable time and bringing them back into use.
This raises three questions: (1) how do we get content easily from the Active
System into the DPS? (2) which content should be put into the DPS? (3) how can
we keep the content useful in the long run and bring it back in a reasonable form,
when needed? Here, it has to be considered that we speak of time frames of several
decades in the context of preservation.
For answering question (1) it is important that the gap, which actually exists
between an Active System and a DPS, has to be bridged. Thus, the PoF Reference
Model should support an integrative approach. For question (2) on which content to
select, it makes sense to follow the idea of appraisal typically followed in an archive,
i.e., having a decision process for which content to keep. Since we want to ease the
use of DPS, it is desirable that this process is automated, performing information
value assessment for identifying important content to be preserved: this opts for
a value-driven approach. Finally, question (3) implies that evolution, which takes
place while the content is in the archival system, has to be carefully considered, in
order to keep it accessible and useful in the long term. Thus, the approach taken by
the model should be evolution-aware. In addition to these three characteristics, as
already discussed in the previous chapters, our approach takes inspiration from the
effectiveness of humans in remembering important things and forgetting irrelevant
things. Therefore, our model is also supposed to be brain-inspired and forgetful in
its preservation decisions.
Thus, ﬁve main characteristics have been identiﬁed for the proposed PoF Refer-
ence Model, supporting the PoF approach for a sustainable and smooth transition

186
F. Gallo et al.
between information and preservation management. Those characteristics and their
implications will be discussed in more detail in the following sections.
6.2.1
Integrative: Bridging the Gap Between Information
and Preservation Management
It is the aim of the PoF approach to preservation to create a smooth transition between
active information use and preservation of information, which so far are some quite
separate worlds. For this purpose, the PoF Reference Model should be integrative,
bringing the Active System and the DPS closer together.
However, due to the inherent long-term perspective of preservation-related solu-
tions it is not the aim to build a strongly integrated, monolithic system. In the long
run, it has to be foreseen that the used Active System and the employed DPS will
both change [3]. The idea is rather a ﬂexible integration, which enables smooth bidi-
rectional transition of information between the Active System and the DPS and, at
the same time, is also prepared for major changes in the overall environment [319].
A core part of integration is to enable the smooth transition of content to be
preserved into the DPS and the meaningful reactivation of content back into the
Active System after a (possibly very long) period in the DPS. An integrative solution
should also embrace the idea of a joint information space, where the information
in the DPS stays conceptually accessible, e.g., visible in search results, even if the
content is only available in the DPS.
One part of achieving a smooth transition is to introduce a middleware system
(the so-called PoF Middleware) to act as a pre-ingest system (or pre-access) and
prepare information packages for delivery in either way. With highly automated pro-
cedures for preparation of these packages according to agreements or requirements,
preservation technology becomes easier to use and the quality of the information
packages becomes more consistent, which alleviate the burden of information pack-
age handling on both sides (see e.g. [164]). In addition, an integrative system should
also support the decisions on what to preserve for easing the integration of preser-
vation into the information management workﬂows, ﬁnally aiming for an integrated
information and preservation management workﬂow.
6.2.2
Value-Driven: Acting upon Short-Term and Long-Term
Information Value
As already discussed in Chap.4 of this book in more detail, one of the core ideas
of the PoF approach is to deviate from the general keep it all model, which makes
the implicit assumption that all information has the same value with respect to being
kept or for being important, in more general. The value of information is multifaceted

6
Bridging Information Management …
187
and can be considered from different perspectives. We distinguish short-term value,
i.e., value for current activities, from the long-term value of a resource, e.g., for an
organization. For the combined information and preservation management system,
both short-term value and long-term value of information have to be considered.
The short-term value refers to the value of content for the current focus of activ-
ity. Identifying the short-term value of a document is of high interest for creating
immediate beneﬁt in information management, e.g., by de-cluttering the desktop in
Personal Information Management (see Chap.7). For a more in depth discussion on
short-term value and the related concept of Memory Buoyancy (MB) see Chap.4.
The long-term value of a resource is obviously relevant in the context of preserva-
tion. It refers to the value a resource has in the long term. Such long-term value can be
used to decide about the investment to be made into preservation for the respective
resource [14]. For a more in-depth discussion on long-term value and the related
concept of Preservation Value (PV) see Chap.4.
6.2.3
Brain-Inspired: What Can We Learn from Human
Forgetting and Remembering?
The model discussed in this chapter is inspired by a joint perspective on Human and
Digital Memory as described in [310]. On the Human Memory side, three main types
of memory are distinguished in such joint perspective, depicted in Fig.6.1: Working
Memory, Episodic Memory and Semantic Memory. For a discussion of the memory
types see also Sects.2.1.3 and 2.1.4. Together with the currently activated Episodic
and Semantic Memory, the Verbal Short-Term Memory (things just heard) and the
Visual Short-Term Memory (things just seen) form the Working Memory, which
frames the current situation. Knowledge is activated on demand from the Semantic
and Episodic Memory according to current needs via the so-called Executive Func-
tions. Perception (depicted on top of Fig.6.1) is one driver for such activation. It
is worth noting that perceived signals do not directly become part of the Verbal or
Visual Short-Term Memory, which are constantly updated, but they are rather ﬁltered
and interpreted by things already in the memory for making sense of the perceived
signals.
Similarly, as shown in Fig.6.1, we also foresee a Working Memory within the
Digital Memory in our model. This is composed of the digital resources currently
relevant, e.g., used or important for current tasks or activities. In an automated Digital
WorkingMemorysignalsfromresourceusage,patternofusage,andchangeaswellas
relationships between resources will be used to update the Digital Working Memory.
For this purpose, in our model, we introduce Managed Forgetting functions which
control the transitions between the different parts of the Digital Memory (see also
Chap.4).
Together, the Working Memory and the Digital Working Memory form the Virtual
Working Memory. Clearly, there is an inﬂuence between both of them. In the ideal

188
F. Gallo et al.
Fig. 6.1 Joint perspective on forgetful, interacting Human and Digital Memory (see also [310])
case, the Digital Working Memory would show to the user just all the information
that the user needs in the current situation, but does not have in the Working Memory.
Note that it is also possible that there is an inﬂuence of the way the Digital Memory
works on the Human (Working) Memory (joint system perspective). For example,
with the easy storage of phone numbers in mobile phones, humans no longer have
to remember those numbers.
Managed Forgetting functions are also used to identify content that is of long-term
value (see Sect.6.2.2) and should, therefore, be preserved. In Fig.6.1, we distinguish
(a) Information Management for Re-use, as it is, e.g., done on a desktop computer or
a server and (b) the system for Archival and Preservation (on top of Fig.6.1). When
content is transferred to Archival and Preservation, it makes sense to add context
information to it (contextualization, see also Chap.5). This prepares the content for
re-contextualization which is required when preserved content is brought back at a
(much) later time. The idea of re-contextualization as an active situation-dependent
process of bringing back things “stored” in the memory is again inspired from Human
Memory: when we as humans remember things this is also a re-construction process,
which depends upon the current situation.
Episodic Memory is a detailed storage of events and is typically subject to
fast forgetting as well as blurring between the memory of similar events due to

6
Bridging Information Management …
189
interference. Here, Digital Memory complementing Human Memory, e.g., via pho-
tos, can serve as a reminder of things that are forgotten, but that one might want to
remember or refresh at a later point in time. For this purpose, it is crucial to select
the adequate content to preserve.
Semantic Memory is a more conceptual part of memory, which stresses on pat-
terns, abstractions, and lessons learned and is directly connected to Perception (see
top of Fig.6.1). Here, the strongest interaction between Human Memory and Dig-
ital Memory is that the organization of digital resources does or should reﬂect the
conceptualization of the world of the user, which is linked to the Semantic Memory.
In Chap.7 of this book, the Semantic Desktop approach is described as an exam-
ple of more explicit modeling of the Conceptualization of the World with a richer
annotation of resources. This knowledge about resources is used in the Information
Management for Re-use and in the Archival and Preservation of the joint perspective
outlined in Fig.6.1.
6.2.4
Forgetful: Focus on the Important Things
As an alternative to the dominant keep it all approach, the PoF approach opts for
conscious decisions about what is important and thus should be kept (and preserved).
Since preservation comes at a cost, it is important to make such conscious decisions
about what to preserve or how much to invest in the preservation of which part of
the information space (see for example [49, 201, 335]). For this need, a forgetful
approach based on Managed Forgetting is a good ﬁt, as discussed in detail in Chap.4.
A forgetful approach is based on Information Value assessment, i.e., computing
and predicting the value of information resources (see e.g. [390]). Effective infor-
mation value assessment, especially for long-term information value, is a complex
task involving a variety of parameters and heuristics. Based on such value, preser-
vation decisions can be taken. On a high level, these decisions could include choice
of preservation provider and/or service as well as decisions about redundancy and
transformations.
6.2.5
Evolution-Aware: Embracing the Long-Term
Perspective
Since we are targeting integrated information and preservation management systems,
we are operating in a long-lived context, covering a time perspective of several
decades. Even things that are considered relatively stable in the current setting of
an information system—such as the type or class of content management system
in use—will change over time [3]. For sustainable operation, it is important to be
prepared for such changes. It is one of the core ideas of the PoF approach to keep

190
F. Gallo et al.
the important information accessible and usable even in case of large changes in
the setting and context of operation. For incorporating evolution-awareness into our
model, several types of evolution with different impacts on the model itself have to
be considered:
1. Changes in conceptual model of the Active System: This could be due to, for
example, changes in the organizational ontology underlying the content structur-
ing as well as processes described in the content. This creates a semantic gap
between the archived content (relying on the old implicit or explicit ontology)
and the active content (structured by new ontology). This gap has to be bridged,
at latest when preserved content is brought back into the active environment, in
order to enable correct interpretation of the re-activated content (see Chap.5);
2. ActiveSystemevolutionandexchange(Migration): TheusedActiveSystemmight
be subject to major changes or might even be completely replaced by another
system, if we look at time frames of several decades. In spite of such changes the
content should stay accessible and usable [3];
3. DPS evolution or exchange: In the same way, the chosen DPS might evolve or
could be exchanged over time. This implies the migration of content into a new
DPS [173]. In the ideal case, this should have as little impact on the Active System
as possible [3];
4. Change in best practices and technology: Formats as well as employed technolo-
gies might become obsolete over time. This requires the identiﬁcation of such
changes as well as adequate actions to react to those changes, such as format
transformations.
The last item in the list above is a classical issue of any DPS. It is, therefore, not
covered in too much detail here, since we focus on the things that go beyond current
best practices in digital preservation.
6.3
PoF Functional Model
In the following we describe the functional part of the PoF Reference Model, which
we refer to as PoF Functional Model or Functional Model for short, while the infor-
mation part of the model (named Information Model) is described in Sect.6.4.
The Functional Model is concerned with the main workﬂows of the PoF approach
and the functionalities required for realizing those workﬂows. Special focus is given
to the novel concepts introduced, namely the aspect of using Managed Forgetting in
support of preservation, the role of contextualization and the impact of evolution. The
PoF Functional Model is complemented by the PoF Information model presented in
Sect.6.4.
As in the case of the OAIS model, the functional view of the PoF Reference Model
considers the main functional entities of the proposed reference model. Furthermore,
we also describe the main workﬂows in the model and how the functional entities

6
Bridging Information Management …
191
contribute to those workﬂows. Again the stress is on the parts which connect the two
types of systems, Active System and DPS, with each other.
6.3.1
Layers, Workﬂows, and Functional Entities
The proposed PoF Functional Model is made up of three layers, which also represent
different levels of compliance with the PoF approach:
• The Core Layer considers the basic functionalities required for connecting the
Active System and the DPS;
• Building upon this layer, the Remember and Forget Layer introduces the brain-
inspired and forgetful aspects into the PoF Reference Model implementing more
advanced functionalities related to preservation preparation and re-activation;
• Finally, the Evolution Layer is responsible for all types of functionalities dealing
with long-term change and evolution.
The different workﬂows and functional entities in the PoF Functional Model are
associated with the three model layers above and are summarized in Table6.1, where
for each layer we list the associated workﬂows and the related functional entities.
The description of the layers, workﬂows, and functional entities is provided in the
following. Some workﬂows are stepwise reﬁned in the more complex layers.
Table 6.1 PoF Functional Model: layers, workﬂows, and functional entities
Model layers
Workﬂows
Functional entities
Core layer
Preservation preparation (basic)
Re-activation (basic)
ID management
Exchange support
Remember and
forget layer
Preservation preparation
Re-activation
Content value assessment
Managed forgetting and appraisal
De-contextualization
Contextualization
Preservation contract management
Re-contextualization
Search and navigation
Metadata management
Evolution layer
Situation change
Content value re-assessment
Setting change
Evolution monitoring
System change (active system change
and preservation system change)
Context evolution management
Context-aware preservation
management

192
F. Gallo et al.
Fig. 6.2 High-level functional view of the PoF Reference Model
An overview of the PoF Functional Model components (layers, workﬂows, func-
tional entities) is depicted in Fig.6.2: within each layer box the relevant entities and
workﬂows are shown. In the following sections, we provide a more detailed repre-
sentation of each workﬂow with the steps associated to each process and the involved
entities. It is worth noting that Fig.6.2 already makes some assumptions about the
functionalities implemented in the Active System and, especially, the DPS: those
functionalities, which are part of one of the respective systems, are not explicitly
listed in the PoF Reference Model.
For our purpose, we assume a OAIS compliant DPS implementing functionalities
such as Ingest, Data Management, Archival Storage and Access of preserved content:
in Fig.6.2 we omitted all OAIS-compliant entities but the Preservation Planning, due
to its relevance in the context of the Evolution Layer, as discussed in the following.
Additional details about OAIS functional entities can be found in [67]. From OAIS
model, we also take the concept of Information Package [67]: the Submission Infor-
mation Package(SIP) and the Dissemination Information Package (DIP) in the PoF
approach are enriched with context information to support synergetic preservation,
while for what concerns the implementation of the Archival Information Package
(AIP) the PoF approach leverages the speciﬁc solution adopted for the DPS.
The three layers are used in the following to describe the functional view of the
PoF Reference Model in more detail.

6
Bridging Information Management …
193
6.3.2
Core Layer
The Core Layer embraces basic forms of two workﬂows connecting the Active Sys-
tem and the DPS, the Preservation Preparation workﬂow and the Re-activation
workﬂow (see Fig.6.2). In support of these two core workﬂows in their basic form, the
Core Layer includes two types of functionality for managing resource identiﬁcation
and information exchange:
• ID Management is mainly responsible for mapping between the identiﬁer (ID)
of the resources in the Active System and the ID used in the DPS for identifying
(and locating) the respective content resources. Since several versions of the same
resource can be put in the DPS, the ID Management functionality also has to take
care of resource versions and their mappings to archive IDs.
• Exchange Support is responsible for enabling the exchange of content and meta-
data between the Active System and the DPS. It adapts and maintains protocols for
this purpose. The Exchange Support functionality handles both outgoing informa-
tion and incoming packages that should be put back into active use. This includes
basic functional activities for preparation of Information Packages, such as auto-
matic identiﬁcation and extraction of technical metadata providing input to the
enrichment phase, and the creation of packages according to a given contract,
the so-called preservation contract described in the following (see Table6.3). The
Exchange Support functionality can be considered as a client-side communication
adapter and can, for example, be implemented in the form of a repository and
related basic processes.
6.3.2.1
Basic Preservation Preparation
The Preservation Preparation workﬂow prepares content received from the Active
System for preservation and transfers it to the DPS. For the pure transfer it relies
on the functionality of the Exchange Support. On an abstract level, this workﬂow—
in its basic form— consists of ﬁve steps (see Fig.6.3): (1) select the content to be
archived, (2) provide the content to the archival process, (3) enrich the content with
Fig. 6.3 Preservation Preparation Workﬂow (basic) in the Core Layer

194
F. Gallo et al.
context for preservation, (4) package the content according to the expectations of the
DPS, and (5) transfer the content into the DPS. In terms of preservation terminology,
the Preservation Preparation workﬂow can be considered as a pre-ingest workﬂow,
which leads into the ingestion functionality of the DPS. This interaction is discussed
in more detail in Sect.6.5. Furthermore, it is worth noting that the enrich functionality
is available on the Core Layer only in its basic form (e.g., to add technical information
such as ﬁle format to the content to be archived). More advanced enrich functionality
is discussed for the Remember and Forget Layer.
6.3.2.2
Basic Re-activation
The Re-activation workﬂow takes care of enabling the Active System to retrieve and
re-activate content, which has been transferred to the DPS. Again on an abstract level,
this workﬂow—in its basic form—consists of ﬁve steps (see Fig.6.4): (1) request
the content to be retrieved from the DPS (here via its identiﬁer), (2) search requested
content thus translating the request into archival ID(s) (in the basic workﬂow just
involving the ID Management functionality), (3) fetch the respective content from
the DPS, (4) prepare the content for delivery to the Active System, and (5) deliver
content to the Active System.
In the Core Layer, the DPS is involved mainly in the fetch activity, which makes
use of the Access functionality offered by the DPS to retrieve the content. As in the
case of the Preservation Preparation workﬂow, some of the steps in the Re-activation
workﬂow are only included in a very basic form on the Core Layer and are extended
with more advanced functionalities on the Remember and Forget Layer.
Fig. 6.4 Re-activation workﬂow (basic) in the Core Layer

6
Bridging Information Management …
195
6.3.3
Remember and Forget Layer
The Remember and Forget Layer introduces brain-inspired functionality into the
PoF Reference Model, which targets the concepts of managed forgetting and con-
textualized remembering. For this purpose, the Remember and Forget Layer extends
the two workﬂows Preservation Preparation and Re-activation from the Core
Layer with further, more advanced functionalities related to contextualization and
re-contextualization, to value assessment and to content access, which are described
in the following. All of those listed functionalities create additional metadata, which
have to be managed in a systematic way. Therefore, the Remember and Forget Layer
also contains a functional entity for managing metadata. The workﬂows mentioned
above deﬁne activities that contain rules, constraints, and settings, that should be
agreed upon and that could be deﬁned in a Preservation Contract. This contract is
properly handled by a functional entity for contract management, which potentially
relates to many, if not all, entities and is not drawn in the ﬁgures to avoid clutter.
6.3.3.1
Preservation Preparation
The Preservation Preparation workﬂow at the level of the Remember and Forget
layer still consists of the same ﬁve steps deﬁned for the Core Layer (select, provide,
enrich, package, and transfer), as depicted in Fig.6.5, but now uses two additional
functionalities for the assessment of content value and for Managed Forgetting in
the phase of selecting content for preservation, as described below:
• Content Value Assessment is based on the idea that understanding the value of
content is in the core of content appraisal for preservation and managed forgetting.
Content value assessment aims to determine the value of a resource. This value may
change over time and there are different value dimensions, which reﬂect the value
considering different purposes or perspectives and which may inﬂuence each other.
Fig. 6.5 Preservation Preparation workﬂow in the Remember and Forget Layer

196
F. Gallo et al.
There is, for example, a value dimension reﬂecting current importance, the MB,
and a dimension reﬂecting the long-term importance or relevance of a resource, the
PV. For assessing content value, the Content Value Assessment component takes
evidence from the Active System, e.g., about information use, content creation, and
further knowledge about the role of resources in the Active System. Content value
can be used as a basis for making preservation decisions, e.g., whether a resource
should be preserved or not, or how much should be invested in the preservation of
a resource. Content value can also be used in the Active System, e.g., for especially
highlighting resources with high content value.
• Managed Forgetting and Appraisal takes into account that with the dramatic
growth of the amount of content, nowadays it becomes more and more important
to make conscious decisions about preservation. Clear decisions on what to put
into the DPS and explicit content appraisal have always been part of the processes
of an archive [160], although not always as much in personal archiving [270]. The
component for Managed Forgetting and Appraisal aims to help in automating such
decisions, a need that has been identiﬁed earlier [161], for both personal archiving
as well as organizational settings. This is encapsulated in the concept of Man-
aged Forgetting, which uses the results of Content Value Assessment for deciding
about preservation and forgetting actions. The effects of Managed Forgetting and
Appraisal is not restricted to the preservation functionality only, it can also be used
in the Active System for improved information access.
Furthermore, the steps of the Preservation Preparation are extended with two
functionalities related to contextualization:
• De-contextualization refers to the extraction of an object from its Active Sys-
tem context in preparation of packaging it for archiving. Decoupling the object
under preservation from its Active System context is nontrivial, since it has to be
decided how much of its current context has to be taken for its future contextu-
alization and where a cutting can and should be made. De-contextualization and
Contextualization (see below) are conceptually closely related.
• Contextualization consists of providing sufﬁcient additional information for the
content to be preserved, in order to allow archived items to be fully and cor-
rectly interpreted at some undeﬁned future date. This functionality is responsi-
ble for deﬁning and assigning the appropriate context to content to be archived.
Contextualization can leverage other processes (e.g., similarity analysis, concept
detection) to explicate context. Contextualization provides the basis for the man-
agement of context evolution over time (see Evolution Layer in Sect.6.3.4) and
Re-contextualization (see Re-activation workﬂow in Fig.6.6).
The Preservation Preparation workﬂow is linked to the Pre-ingest functionality
as it is typically described for OAIS-compliant preservation systems, e.g., in the
PAIMAS [66] model. In order to facilitate easy (seamless) ingestion into the DPS
and make sure that the Information Packages contain metadata needed for both the
DPS as well as for access, Pre-ingest aids the Active Systems as well as the DPS
systems adhering to standard protocols and metadata, as deﬁned in the Preservation

6
Bridging Information Management …
197
Fig. 6.6 Re-activation workﬂow in the Remember and Forget Layer
Contract. This also means that the Pre-ingest function puts up some requirements
on the Active Systems to follow certain protocols (which can/should be domain
speciﬁc). Our Preservation Preparation workﬂow, from the perspective of the DPS,
serves as such a Pre-ingest function.
6.3.3.2
Re-activation
For the Re-activation workﬂow, two additional functionalities are used, with respect
to the Core Layer, for the search and future access to the preserved content:
• Re-contextualization has the purpose of supporting the interpretation of a content
object at the time of access (which might be a considerable time after archival).
Re-contextualization occurs when a document is retrieved from the DPS at some
future date. Once retrieved from the DPS and before it is put back into active use,
the context information, which has been provided by the Contextualization func-
tionality, stored together with the content object and possibly updated or extended
over time, is retrieved. This context information is used for Re-contextualization,
i.e., to relate the content object to the current usage context. Re-contextualization
can also include the re-construction or extension of context information for content
archived with no or insufﬁcient original context.
• Search and Navigation is responsible to enable ﬁnding things that have been
preserved. Various types of content search and navigation can be supported here.
This includes searchinthemetadata, full-text searchinthecontent (or moregeneral
content-based search also including non-textual content), search in the context
information and in other types of annotation, exploratory search for understanding
the archive content, etc. For our integrative and forgetful approach it is crucial (a)
to manage the interaction between the search in the Active System and the search
in the DPS and (b) to understand how the forgetful approach and search support

198
F. Gallo et al.
interact. Since we are following an integrative approach, it makes sense for (a)
to consider the information in the Active System and the DPS as a type of a
joint virtual information space, which are both considered for search in the Active
System. However, it might still make sense to differentiate the two types of content
taking into account the cost that might be attached to accessing content from the
DPS. Archived content might, for example, only be considered on demand or if
nothing can be found in the Active System. Furthermore, content stemming from
the preservation store might be marked in result lists. For aspect (b), the inﬂuence
of the forgetful approach, the results of Content Value Assessment (namely MB
and PV) can be considered in result ranking (or even indexing): this would prefer
results with higher content value balancing content value and relevance as it is for
example done in diversiﬁcation approaches. Furthermore, this includes adequate
ﬁltering and ranking approaches for handling versioned archived content. Situation
search can also be used here. However, since it is especially relevant for Active
System Change it is discussed in Sect.6.3.4.
6.3.3.3
Metadata and Preservation Contract
In both workﬂows of the Remember and ForgetLayer, metadata are generated and
used for different purposes. This metadata is taken care of by different functional
entities also responsible for managing the Preservation Contract, the agreement con-
cerning the preservation rules and strategy among all actors. The functional entities
mentioned above are described in the following:
• Metadata Management is responsible for the different kinds of metadata that are
created and exchanged by the aforementioned functional entities. This includes a
variety of different types of metadata such as current and past values for MB and
PV, information on the context of a resource, information extracted from resources
for further processing as well as indexing information for Search and Navigation.
Some of the metadata, which is collected here just remains in this middleware for
supporting its operation. Other parts of the metadata such as the PV and the context
information will be stored as part of the archived object in the DPS. The Metadata
Management functional entity interacts with the respective components of the
DPS by (a) providing input for enriching the metadata in the DPS for improved
preservation management, (b) incorporating information coming from the DPS
(e.g., for the joint indexing) and (c) by - as mentioned before - storing some of the
metadata as part of the resources to be preserved.
• Preservation Contract Management is involved before transferring any digital
items or collections from Active System to DPS, when a submission agreement has
to be established between the participating actors, preferably following a standard
approach, e.g., as described in PAIMAS [66]. In ForgetIT, we extended this to also
include aspects of relevance to the re-activation of content and, therefore, labeled it
PreservationContract tosignifythatitdoesnotonlyconcernthesubmissionphase.
The Preservation Contract should contain accurate information about the type of

6
Bridging Information Management …
199
package content, structure, and metadata. It should also include requirements for
security and privacy mechanisms at transfer and storage. Furthermore, it should
be stated in the agreement if there is a need or requirement for migration at ingest.
Other examples are a speciﬁcation to what extent metadata should be obtained and
generated during the Pre-ingest process, if there is a speciﬁc demand on storage, or
rules regarding management of objects in the DPS including different preservation
levels as forgetting options. There is also room for agreements about e.g. privacy
requirements and exploitation rights.
6.3.4
Evolution Layer
A preservation framework per deﬁnition exists in a long-lived environment aimed
to survive at least decades. Clearly, most of the involved parts widely ranging from
formats, technology and systems to semantics, and relevant real-world situations
(represented by the application) will not remain stable (i.e., unchanged) over such
long period of times. This implies that adequately dealing with changes is a core
property that is required from such a framework.
The purpose of the Evolution Layer is to adequately deal with the upcoming
changes, such that the preserved content remains accessible and understandable.
Clearly this means different things for different types of changes. Other types of
actions are, for example, required in case a media format gets out of use as compared
to the situation that an organization is re-organized.
The Evolution Layer, therefore, groups the changes considered in three classes,
each handled by its own type of change workﬂow. Thus, the Evolution Layer contains
three new workﬂows: the Situation Change workﬂow is responsible for monitoring
semantic changes in the Active System and propagating them into the DPS, in order to
keep the preserved content understandable; the Setting Change workﬂow deals with
changes in practices, formats and technology in the environment of the preservation
(setting); ﬁnally, the System Change workﬂow is responsible for situations, where
one of the involved systems changes.
In support of the aforementioned workﬂows, the Evolution Layer includes addi-
tional functionalities which are described in the following.
6.3.4.1
Situation Change
Active Systems such as the Semantic Desktop (see Chap.7) reﬂect real-world pro-
cesses and situations and thus the content and structures in such systems are subject to
change, which we capture under the notion of Situation Change. Since the preserved
content coexists with such changes, this raises the question, if such changes have
implications for the preserved content. Even if the content is not directly affected,
changes such as re-structuring in an organization or change in life situation of an

200
F. Gallo et al.
Fig. 6.7 Situation Change workﬂow in the Evolution Layer
individual might have implications for the interpretation and contextualization of
preserved content.
For this purpose, a Situation Change workﬂow has been deﬁned. This workﬂow
consists of four steps (as depicted in Fig.6.7): (1) change monitoring, (2) change
assessment (assessment of detected changes), (3) change notiﬁcation (notiﬁcation
of involved components such as the DPS on relevant changes), and (4) change propa-
gation, which performs different types of actions depending on the observed change
and the chosen change propagation strategy.
Change monitoring is responsible for detecting changes in the content and the
content structuring of the Active System. This is typically best realized by an exten-
sion of the Active System. An important and more demanding step in the workﬂow
is change assessment. The idea here is that changes are analysed with respect to their
potential impact on the preserved content. It does not make sense to consider every
small change that happens in the Active System. Rather Situation Change is only
interested in the larger high-impact changes. One way of dealing with this idea is
to distinguish more and less important concepts, instances, and relationships in the
content structures of the Active System and to propagate this importance level to
the changes affecting such entities. Alternatively or in addition, it is also possible to
give different impact levels to different types of change operation, e.g., the deletion
of a department will have higher impact than a smaller change of name. Change
assessment is the basis for ﬁltering out irrelevant changes and for performing change
notiﬁcation, i.e., informing relevant system components about the change. This will
lead to change propagation, which can take different forms depending on the type of
the change and the selected change strategy. A change might, for example, imply that
the preserved context information is modiﬁed or extended, in order to capture the
change and to be able to reﬂect it, when re-contextualization is performed. Another
action that can be triggered by a change is the re-assessment of the content value:
due to a change, e.g., a change in employment, an entity such as the former employer

6
Bridging Information Management …
201
might cease in importance, which would lead to a decreased PV of content items
related to such an entity.
For monitoring, assessing the changes and deciding about the consequences, in
support to the Situation Change workﬂow, the following functionalities have been
introduced in the Evolution Layer:
• Evolution Monitoring is required for the change monitoring step in the Situation
Change workﬂow. This is mainly performed in an extension of the Active Sys-
tem, because this is the main place, where evolution in the conceptual model of
the application becomes visible. Such changes might, for example, be changes in
the ontology (implicitly) underlying the organization of the information triggered,
e.g., by a major re-organization. Evolution Monitoring has to observe changes in
the explicit representations of the conceptual model (such as taxonomies, infor-
mation structures) in the Active System as well as more implicit signals of context
evolution (e.g., newly upcoming topics, tags getting out of use, department sites
no longer updated).
• Context EvolutionManagement is responsiblefor keepingup-to-datethecontext
information which has been stored along with the archived content. This may
include the storage of further context information, in case of larger changes in the
Active System (e.g., major re-structuring of an organization). This also includes
keeping track of such larger changes, which have happened in the Active System
on a conceptual level. Such a change history can be used in Re-contextualization
for making the content understandable in the changed new context.
• Content Value Re-assessment here serves a very similar purpose as the function-
ality Content Value Assessment described for the Remember and Forget layer. It
revisits the value assessments originally provided by the Content Value Assess-
ment based on observed evolution in context. It is considered separately in our
model, since the resources that it works on are now already preserved, which has
implications on the computation of the value (e.g., the role of usage data) as well
as on where this functionality is performed. One option, which has been followed
in the ForgetIT project, is to map it to in storage computation.
6.3.4.2
Setting Change
Although the DPS should have its own support for typical OAIS functionalities, such
as Preservation Planning and Administration [67], the many-to-many relationship in
the PoF Reference Model and the long-term perspective of those functions mean that
at least parts of those functionalities need to be shared and communicated over several
systems. The reference architecture of the model, described in Sect.6.5, includes a
middleware component, namely the PoF Middleware, which acts as a man-in-the-
middle between Active Systems and DPS and therefore has a suitable position for
capturing these bidirectional interactions. The OAIS Preservation Planning function
could, for example, beneﬁt from a component responsible for the technology watch
and residing in the middleware, thereby gathering and aggregating information about,

202
F. Gallo et al.
Fig. 6.8 Setting Change workﬂow in the Evolution Layer
e.g., usage of ﬁle formats in all systems connected to the same middleware. Another
example would be when Preservation Planning in a DPS declare a ﬁle format as
obsolete, this information could then be shared with the PoF Middleware which
propagates this to Active System and DPS.
Based on such evidence, the PoF Middleware provides additional possibilities
to summarize and analyze the usage and, e.g., storage quotas over several systems,
thereby giving an overview of the holdings for a particular customer. Since a customer
might use several Active Systems, as well as preserved content in several archives,
such an overview is beneﬁcial in locating objects that need preservation actions.
The Setting Change workﬂow consists of four different phases with two different
starting points (as depicted in Fig.6.8), involving the Context-aware Preserva-
tion Management (CaPM) entity, described below: (1) activity monitoring, which
logs the bidirectional communication between the Active System and DPS includ-
ing process activities, systems in action, and digital objects passing through; (2)
change assessment, that detects and propagates change in usage; (3) change esti-
mation, which suggests suitable change recommendations based on rules deﬁned
in Preservation Contract including, e.g., PV, and use statistics, and (4) change rec-
ommendation, which propagates recommended actions to DPS, which could be of
different types, such as transformation of content or change of physical and logical
content structure.
The Evolution Layer includes the CaPM functional entity, with three support-
ing functional entities, namely: Activity Logger, Technology Watch, and Analyser.
Together they support the workﬂow described above:
• Context-aware Preservation Management (CaPM) functional entity external-
izes and extends parts of OAIS Preservation Planning and OAIS Administration
by keeping track of Active Systems as well as the (several) DPS that are involved.
The main idea here is that it should be easy to seamlessly put back preserved
information into active use in either the same system as it originates from, but
even more importantly into other information systems (of the same type). Even
the same system might have evolved to newer versions with different standards,

6
Bridging Information Management …
203
and maybe even a different information structure or ontology. This must also be
tracked by the CaPM functionality.
• CaPM Activity Logger provides support for monitoring of activities in PoF work-
ﬂows. A typical example is to keep track of the execution of tasks executed by
components in the workﬂows. This data will then serve as one input to the change
assessment.
• CaPM Technology Watch gathers data on objects handled by the PoF workﬂows.
This includes informationonActiveSystemandDPS, as well as technical metadata
on the object, e.g., ﬁle format. This information combined with input from CaPM
Activity Logger forms the basis for a change assessment, which, if needed, triggers
a change estimation.
• CaPM Analyser supports the change estimation process, based on an initial
assessment of the need for a setting change, by aggregating and processing data
provided from earlier functions and should provide a graphical user interface for
human interpretation and intervention. The CaPM Analyser may calculate a pre-
ferred setting change based on thresholds and rules, provided by, e.g., a Preser-
vation Contract, but it can be interactively overridden by a human operator. The
decision, be it automatic or manual, will then become a change recommendation
that is handed over to the DPS to be implemented.
6.3.4.3
System Change
Within the Evolution Layer, System Change refers to the case, where one of the
participating systems is replaced. This might be the Active System (e.g., because a
new type of Active System is adopted for the same task) or the DPS (e.g., because a
preservation provider goes out of business or the user wants to change the provider).
Of course, it is also possible that the middleware is replaced with a middleware
solution based on a different approach or technology. However, this case is out of
the scope of this reference model.
The two cases, change of Active System and change of DPS have very different
implications and are, therefore, considered separately.
Active System Change
The most interesting aspect for the change of Active System is how the content
preserved with Active System A can still be accessed, if the system no longer exists
or is no longer in use. This might mean to access the preserved content through a
new Active System B or to access it via a lightweight access infrastructure such as a
Web Interface. For covering those cases, we have identiﬁed a modiﬁed Re-activation
workﬂow (see Fig.6.9).
A prerequisite for accessing the content of Active System A is some type of an
identiﬁer (and credentials) for getting access to the content that has been connected
to a user via Active System A. After authentication (1), the owner or another user on
behalf of the owner can access the content. For not overwhelming the user with the

204
F. Gallo et al.
Fig. 6.9 Re-activation workﬂow for Active System Change
full content collection, which might have a considerable size, a high-level and user-
friendly structuring principle is required. For this purpose, the concept of Situations
has been introduced into the model (see Sect.6.4.1 for details). Therefore, after
authentication, the user can perform a situation search (2), which returns to the user
a list of situations, for which there is content in the archive (or the list of situations
fulﬁlling the user query). Search can be either offered as an integrated part of the
new Active System B or separately (e.g., via a Web Interface). For each of the
situations also a short description in the form of a proﬁle is given (see Situation
Proﬁle in Sect.6.4.1). The situation selection step (3) enables the user to select
relevant situations, which subsequently during the fetch step (4) can be retrieved by
the system via the mapping from situation to content objects provided by the ID
Management functional entity. During the prepare step (5) as in the normal case
of Re-activation, the content is prepared applying, e.g., Re-contextualization and
made available via the Exchange Support. At this point, the transform step (6) might
be necessary to transform the content and the context into a format that can be
digested by the Active System B before the deliver to new system step (7) takes
place. Alternatively, if no content transformation is required, the provide for access
step (6) can be performed based on the content as it is provided by Exchange Support,
e.g., for browsing the content.
Preservation System Change
A change of DPS can happen for various reasons including technology change,
preservation provider change, etc. In each case, it will become necessary to get
the preserved content out of the DPS to be discarded and to import it into a new

6
Bridging Information Management …
205
Fig. 6.10 Simple migration workﬂow for Preservation System Change
DPS. In addition to dealing with the content, it is also necessary to deal with the
Preservation Contract associated with the content, as can be seen in the workﬂow
shown in Fig.6.10.
The workﬂow shows a high-level Preservation System Change process. During
the (1) identify content step, all the content that belongs to the system instance or
the user that wants to change the DPS is identiﬁed. In case of a technology change,
this must be done for all the users or a bulk export function can be used; then the
(2) export content step requires that all the content is fetched and exported together
with the necessary metadata, e.g., linking content to situations or linking content to
context. Also, since the middleware has information on what metadata and the format
of metadata that is required from the new DPS it is possible to request the export to be
in a suitable format; in addition, during the (3) migrate contract step, the Preservation
Contract is migrated to meet the service offer, possibilities and practices of the new
DPS; the migration might have implications that require re-negotiations with the user,
therefore it is expected that contract migration is no fully automated process; it may
be then necessary to (4) transform content (and metadata) in order to comply with
the newly negotiated contract, depending on what was possible to achieve already
during the export content phase. This would also lead to an update of the provenance
metadata for the objects, since they have been handled by different actors during this
process. Subsequently, content is imported, possibly after re-packaging according to
the new Preservation Contract: this takes place during the (5) import content step,
when the ID Management functionality is used to link the new archival IDs with the
IDs known to the Active System and possibly internal IDs of the Middleware.
Under certain circumstances, a change of DPS might just concern a subset of the
digital objects in the current DPS. In that case, it could be useful to utilize the CaPM
Analyser and CaPM Technology Watch to support decision making on whether to
make that change or not, based on, e.g., the number of objects concerned or the
volume of those objects.

206
F. Gallo et al.
6.4
Information Model
In this section, we describe the information model as part of the PoF Reference
Model, which bridges the gap between the Active System and the DPS.
In order to meet the requirements of a variety of systems on the site of the Active
System as well as on the site of the DPS, the information model has been designed
to be ﬂexible, extensible, and inter-operable.
The requirement for the PoF Information Model is twofold: (a) it should reﬂect
the perspective of the user, thus enabling the Active System to interact with the PoF
compliant system and (b) it should reﬂect the perspective of the PoF Middleware
in order to support all the needed information for storing content in the archive
and for retrieving it from the archive. Therefore, we present two perspectives of
the information model, the user perspective (see Fig.6.11) and the PoF Middleware
perspective (see Fig.6.12).
Fig. 6.11 Information Model from user perspective
Fig. 6.12 Information Model from middleware perspective (user perspective in yellow)

6
Bridging Information Management …
207
Three core elements have been identiﬁed for the PoF Information Model:
• Content, obviously, representing the content to be preserved.
• Context provides additional information helping in the interpretation of the con-
tent, as required when considering the long-term perspective.
• Situation provides a novel, high- level structuring concept for archival content.
Due to its central role in the model, the concept of Situations and its meaning for
preservation is discussed in more detail in the next section, before the information
model as a whole is presented.
6.4.1
Situations as Units of Preservation
Users as well as organizations have to be able to incrementally store things and
retrieve units of content from the archive even after long time periods. This should
also be possible in the case, when the original Active System no longer exists. To this
end, we have chosen a situation-driven approach: typically, content is created for (or
in) a situation such as a holiday trip for personal context or is created in a project for
organizational context and is also associated with this situation in memory, e.g., my
photos of my trip to Paris. Therefore, we have decided to introduce Situations as an
important high-level structuring concept into the Reference Model.
According to our deﬁnition, a Situation represents an event, a life situation, or
an experience from the perspective of a user or an organization. For enabling a high
ﬂexibility, which ﬁts many preservation settings a rather wide deﬁnition is chosen
here. Examples for situations are: a holiday trip or a life event such a wedding in the
personal context, a project or a project meeting in the organizational context.
Inthearchive,thecontentisassociatedwiththesituation(orratherarepresentation
of it). This does not mean that all resources such as photos belonging to the same
situation have to be archived at the same time or have to be stored at the same place
in the archive. The purpose of introducing situations is to provide the user with a
high-level notion for perceiving the structure of the potentially large and growing
amount of content that is put into the archive over the time. Archive content will
be associated with situations and can be accessed in terms of situations. In some
way, situations are similar to the concept of collections as they are used in digital
archives. However, we see an important difference. It is the purpose of the archive to
enable the memorization of situations by storing content that enables remembering
relevant aspects of the situation. The preserved content thus illustrates the preserved
situations.
Similar to an event, a situation is associated with a time span, which describes the
temporal dimension of the situation and with location information. Due to the wide
deﬁnition of situation the considered time frame can vary widely. This is driven by
the user’s understanding of situation granularity. A Situation can be very short, but
also can span several years. Nested Situations are also possible, allowing situations to
be part of larger ones. Table6.2 summarizes the core attributes of a Situation Proﬁle,

208
F. Gallo et al.
Table 6.2 Core attributes of the Situation Proﬁle
Attribute
Description
Id
Unique ID for the situation
Title
Name of the situation
Type
Type of situation
Time
Time range related to the situations
Locations
Location or locations related to the situation
Persons
Important persons related to the situation
Further entities
Important entities that are not persons, such as pets, buildings or
organizations
Memory cues
Information that is expected to be useful in memorizing and retrieving the
situation
Description
Optional short description of the situation
PV
Preservation value of the situation
which is the representation of a situation in the PoF Information Model. It provides
a core set of metadata for describing a situation.
In order to enable interoperability among different Active Systems (if we need
to migrate from one to another or in case of changes), an agreement upon or stan-
dardization of situation proﬁles would be desirable. This would ease the access to
situations, which have been archive on behalf of one Active System and are supposed
to be brought back in the context of another Active System.
An approach similar to the one used for Dublin Core (DC) [413] can be adopted,
by identifying a core set of elements to be used as descriptive metadata associated to
a given situation. Such attributes could be standardized, as done by the Dublin Core
Metadata Initiative (DCMI), and further speciﬁed in order to cover a wide variety of
situations. Subsequently, further attributes can be agreed upon for speciﬁc types of
situations such as holidays, projects, etc., resulting in a more speciﬁc set of attributes
comparable to the qualiﬁed DC elements.
Completing this section describing the Situations, we introduce some aspects
related to the human memory behavior. The notion of the situations is related to
concepts of cognitive science in two ways. First of all, the situation can be related to
episodic memory. Following the current understanding of human memory behavior
(see for example [25, 26]) and more precisely focusing the attention on the Episodic
Memory (as discussed in [90]), our brain and human archival system have to store
“Situations” with the associated and related contents. A Situation in some aspects
can be compared to a Episodic Memory Item (EMI) (rather than being translated to
a rigid OAIS-like Information Package). An EMI is:
• complex, because it is made up of several parts and concepts (or even sub-parts);
• heterogeneous, because its parts are of different kinds: we, for example, have
associations, contexts, concepts, and images;
• dynamic, because an EMI can change over the time due to the changes of associated
resources;

6
Bridging Information Management …
209
• unstructured, because even if some parts can be structured more or less rigidly
(i.e., by concept hierarchies and semantics), many other parts can be completely
unstructured such as images, contextual information (feelings, moods or environ-
mental cues) and generally not classiﬁed information;
• loosely coupled with other EMIs.
Situations in our PoF Information Model can be considered in a similar way, in
order to support and complement human memory with a digital memory. A second
aspect of Situations that is inspired by cognitive processes is the inclusion of memory
cues into the Situation Proﬁles.
6.4.2
Core Information Model: The User Perspective
From the point of view of the user or of the Active System, we can summarize the
Information Model as shown in Fig.6.11.
As stated above in the previous section, the three classes Content, Context,
and Situation are the core elements of the Information Model. Both, Context and
Content are Preservation Entities, i.e., they are elements, that the user can provide
to the archival system for preservation. Content can be an individual content Item
or a Collection, which will again contain content objects.
As another child of Content, the Information Model contains the concept Con-
densation, which is an object created from a content object such as a collection or
an content item by condensation operations such as text summarization. Information
reduction is an important concept in the forgetting environment and is one of the pos-
sible forgetting actions resulting in condensed objects that can replace or augment
and improve the original projects.
Information on Context, the second core concept of the PoF Information Model, is
kept in the archive for helping in the interpretation of content objects. Context can be
provided in many different forms and for content on all levels of granularity, including
individual items, collections as well as situations. In addition, we distinguish Local
Context and WorldContext dependinguponthescopeoftherespectiveinformation:
this distinguishes information that is only known in the local scope and information
considered as world knowledge.
The third core element, Situation, is also a child of Content. This provides a
high ﬂexibility allowing nesting of situations as well as for nested collections to
be assigned to a Situation. A Situation is represented by a Situation Proﬁle, an
associated Proﬁle storing the properties of the Situation (details of the Situation
Proﬁle are described in Table6.2). As shown in Fig.6.11, the Situations in Reference
Model are linked by the association representedBy to the Situation Proﬁle. As
explained above, Situation and Situation Proﬁle are beyond the OAIS approach.
A further important concept in the Information Model is ownership of the pre-
served content. This is modeled by introducing the concept of an Actor, which can be
a Person or a System Instance of the Active System. For modeling ownership, the

210
F. Gallo et al.
Actor is linked to the Content element via the relationship owned by. This enables
for ﬂexible assignment of ownership.
Another important concept of the user’s perspective of the information model,
is the relationship to a preservation provider. This is modeled in the Information
model by the associative class of a Preservation Relationship, which is attached
(associated) to both the Preservation Entity and the Preservation Contract. Such a
relationship is deﬁned by a Preservation Contract, in which the preservation actions
for the respective Preservation Entities are deﬁned (see also [66]).
In addition, Preservation Entities are also associated with a PV, which reﬂects the
expected beneﬁt of preserving the respective entity and acts as a parameter for the
preservation processes (mainly assessments and preservation planning activities).
All the information related to Content (such as title, type, etc.) is represented by
Content Information which can be considered as a child of an Information Object
according to OAIS.
The individual Model Entities are detailed in Table6.3.
6.4.3
Extended Information Model: Middleware Perspective
In the previous section, we have described the PoF Middleware from the point of view
of the User. Here, we describe the point of view of the Middleware. The Middleware
has to manage the mapping between the Information Model as seen by the user
(or organization) and the Information Model of the underlying DPS (or systems).
Figure6.12 shows the Extended information model, which reﬂects the perspective of
the Middleware. For better comparison, the elements (classes) coming from the User
Perspective have been highlighted (yellow background color) and can be considered
as core elements of the model.
A core aspect of the Middleware perspective is shown at the right corner of the
ﬁgure, the ID Mapping Table. Since the preservation entities as they are seen by the
Active System are not mapped 1:1 to the DPS, it is necessary that the Middleware
manages a mapping between the Preservation Entities and the Archival Resources
in the respective repositories. For maximal ﬂexibility an ID mapping is used for this
purpose. The class Preservation Entity Identiﬁer, which is a type of Reference, is
theidentiﬁerofapreservedentity.Thiscanbeidentiﬁersofdifferentkinds.Therefore,
there are at least three children, the CMIS ID is the identiﬁer used for the resource in
the Active System, PoF ID is an identiﬁer used for internal purposes and Repository
ID is the ID of the Resource in the Archive. All these IDs are related to each other
by the ID Mapping Table that can be considered as their aggregator (represented
by the black diamond in the diagram). This mapping table is also use to link archival
resource to Situations.
A class speciﬁcally useful for the Middleware and not for the User, is the Archival
Resource, a parent class of Preservation Entity. Actually the User has to deal
only with Preservation Entity, while the Middleware has to store into archives the
related resource as top level. Hence a further abstraction layer was needed in order

6
Bridging Information Management …
211
Table 6.3 Information Model entities for user and for middleware perspective (see Fig.6.12)
Entity Name
Description
User Perspective
Situation
A Situation is an event, a life situation, or an experience from the
perspective of a user or an organization. It is introduced as a user-oriented
structuring principle for the preserved content. A user can store, retrieve
and access content in terms of situations. Situations are illustrated by
content objects, typically collections of preserved content items. An
example is the situation “My Holidays in Crete” illustrated by a selected
set of photos from this trip with high PV
Situation proﬁle
Situation Proﬁle is a metadata record describing a situation. In its core, it
consist of a set of attribute value pairs as described in Table6.2. The
information in the situation proﬁle is kept simple and system independent,
in order to ease ﬁnding and recognizing situations, even if the Active
System no longer exists
Content
Content is an abstract concept for referring to different types of content
that can be provided for preservation and can be part of content
collections. This can be individual items (Item), collections of other
content items (Collection) as well as condensed information objects
(Condensation). Furthermore, also Situations are considered as Content
Context
Context is extra information, which is intended to help in the
interpretation understanding the preserved content objects, especially,
when re-accessed after a long time. Context can be collected in different
ways, e.g., provided by the user, extracted or collected from external
sources, and it can come in different forms, e.g., structured semantic
information, extra images, text, links to external knowledge basis, etc
Local context
Local Context is context information associated with a content object
based on local knowledge. In contrast to world knowledge, this is
information, which is (mainly) only known in the environment of the
content object (e.g., within the organization, or by a person and his/her
family and friends
World context
World Context is context information associated with a content object
based on “world knowledge”. It is assumed that such world knowledge is
generally known (not only in the local environment) and, thus, can be
retrieved from external sources at the time of preservation, but also
retrospectively at the time of later access. This property makes local
context more important for preservation than world context
Collection
A Collection is a content object consisting of a set of content objects.
Those contained content objects can either be individual content items
such as an image or they can be again collections, thus allowing nested
collections
Condensation
A Condensation is a content object, which has been obtained by a
condensation operation from one or a set of other content objects. This can
for example a text summary obtained by summarizing a text document or
a set of text documents. Another example is a low-resolution image
obtained by a transformation from a high-resolution image or a reduced
image collection obtained by using a near-duplicate detection method.
Condensation plays an important role in creating additional forgetting
options beyond keep-or-delete
(continued)

212
F. Gallo et al.
Table 6.3 (continued)
Entity Name
Description
User Perspective
Item
An Item is an individual content object, such as an image, a text ﬁle, or a
video. An item may also consist of groups of sub-items and/or components
that are bound to relevant Descriptors. The Item descriptor contains
information about the Item. An Item that contains no sub-items can be
considered a whole. An Item that does contain sub-items can be considered
a compilation. Items may also contain annotations to their sub-parts [176]
Preservation entity
A Preservation Entity is an object to be preserved coming from the
Active System. This concept provides an umbrella over the two core
concepts Context and Context in the PoF Information Model capturing
them both as preservable objects. Furthermore, it links such objects to be
preserved with preservation-related information: It links such object to a
preservation relationship, which deﬁnes the way the respective object is
preserved, and to a PV, which act as a parameter to the preservation process
PV
Preservation Value is a value reﬂecting the expected beneﬁt from
keeping the related Preservation in the long run. Here numerical values as
well as more user-friendly preservation categories such as gold, silver,
bronze, etc., are possible. PV can be used both for deciding if something is
preserved or not and for actually deciding about preservation options, e.g.,
the redundancy level
Preservation
rela-
tionship
A Preservation Relationship is a relationship between an Actor and a
preservation service provider, who takes care of the preservation actions.
The Preservation actions and constraints are deﬁned in a Preservation
contract. The actions taken for a Preservation entity are governed by this
Preservation Relationship and the associated Contract
Preservation contract A Preservation Contract is an agreed upon contract between a
preservation provider and a preservation client, which deﬁnes rules for
digital preservation actions and communications to be undertaken within
the middleware as well as within the DPS (see [66])
Actor
An Actor is an entity that takes an active role as a subject with respect to
content to be preserved. This can for example be as the owner of a content
object or as a preservation contract holder. An Actor is not restricted to be
a person. It can also be a system instance acting on behalf of a person or
an organization in the interaction with a preservation middleware.
Indirectly, this will typically boil down to a person or a role in an
organization, which, for example, has the ownership of the content
Person
A Person is a real person or a role within an organization that can be taken
by several persons at the same time or over time. In the Information
Model, persons come into play as owners of content objects and as actors
involved in preservation contracts.
System instance
A System Instance is an instance of the Active System, which interacts
with the Preservation Middleware on behalf of a user or a group of users.
In this role, it might act as an intermediary between the user(s) and the
DPS. However, since the Active System may go out of operation, it is
important to deﬁne processes on how to deal with relationships such as
content ownership in this case
(continued)

6
Bridging Information Management …
213
Table 6.3 (continued)
Entity Name
Description
User Perspective
Information object
Information Object is deﬁned according to OAIS speciﬁcation and
provides a set of attributes deﬁning the semantics (meaning) of a content.
In our model, it can be considered as the abstract element for descriptors
Content information Content Information extends the Information Object and represents all
the information related to a Preservation Entity, it may contain editorial
information (such as title, author, editor, series, year, ID code, keywords,
etc.), technical (such as format, length, size, etc.) and other descriptive
attributes.
Middleware perspective
Archival resource
An Archival Resource is a resource as it is archived in a DPS
Component
A Component is the binding of a digital resource to a set of Descriptors,
i.e., the information concerning all or part of the speciﬁc resource
instance. A Component itself is not an Item; Components are building
blocks of Items [176]
File
A File is a Component materialized as a unit recognized by a computer
system, subsystem, or application [176]
BitStream
A BitStream is a Component recorded as contiguous or non-contiguous
data within a File. If metadata are speciﬁc to streams or tracks (e.g., audio
and video tracks of a ﬁle), Bitstream shall be used and descriptors shall be
added on Bitstream level [176]
Rights
Rights represent information concerning legal, regulatory or contractual
provisions that affect ownership, control, access, or use of resources
insofar as they impact the long-term preservation (e.g., intellectual
property, copyrights, privacy, etc.). Actions or events in the preservation of
resources need to respect such rights [176]
Exploitation rights
Exploitation Rights represent information speciﬁcally related to the
ownership and commercial exploitation of the digital resource
ID mapping table
The ID Mapping Table provides a mapping among the different types of
entities based on their IDs. The user only needs to know the ID generated
in the user application and the DPS only makes use of a repository ID. The
middleware bridges the gap between Active System and DPS by linking
such identiﬁers, also using a PoF identiﬁer for internal purposes. Note that
the mappings can also be 1:N mappings. The mapping table can be
implemented in different ways
Preservation entity
identiﬁer
A Preservation Entity Identiﬁer represents the different identiﬁers
associated to a given Preservation Entity in the different systems or for the
different perspectives: the user perspectives requires a Active System
identiﬁer (CMIS ID), the middleware perspective makes use of a PoF ID,
while a Repository ID represents the content identiﬁer in the DPS. The
way these identiﬁers are implemented and managed depends on the
particular implementation. For the Active System identiﬁer, we assume an
ID based on the CMIS standard
(continued)

214
F. Gallo et al.
Table 6.3 (continued)
Entity Name
Description
User Perspective
Repository
A Repository is a system used to store digital content and its associated
metadata, providing methods to import, update, search and access content,
exposing standard and application-independent interfaces. A digital
repository can be part of a Content Management System used by a user
application or be included in a Digital Preservation solution, where it is
extended with long term preservation functionalities
CMIS repository
A CMIS Repository exposes interfaces based on the OASIS CMIS
standard, to enable interoperability among different content management
systems and to provide functionalities to import, update, search and access
the content using CMIS standard
Usage rights
Usage Rights represent information related to the usage of the digital
resource. They deﬁne if preservation actions can be undertaken by DPSs
Descriptor
A Descriptor associates information with the enclosing entity. This
information may be a Component (e.g. image) or a textual statement [176]
Quality
Quality provides information related to the description of the technical
condition of preserved items and resources. This information can at least
partly be automatically extracted from content with specialized tools but
often requires manual revision and validation. This manual work causes
considerable costs, which is an additional reason for preserving it. Quality
information includes digital defects (such as audio and visual) and
characteristics, their collocation in time and space and their severity.
Additionally, structural information and technical metadata of resources in
relation to relevant standards are considered. It is worth to highlight that it
is needed to preserve as wells the description of the hardware devices,
tools, and agents used for extracting and reviewing that quality
information [176]
Provenance
Provenance documents the chronology of events regarding the creation,
modiﬁcation, ownership, and custody of a resource, such as who produced
it and who has had custody since its origination; it provides information on
the history of the multimedia content (including processing history) [176]
Reference
Reference provides information that is used for identifying the digital
resources. It provides one or more identiﬁers, or systems of identiﬁers, by
which the resources may be uniquely and persistently identiﬁed.
Reference information supports the linkage of identical or related
resources that might be stored in separate repositories. These repositories
may use different mechanisms for identifying resources (e.g., using
different standards for representing local identiﬁers) [176].
(continued)

6
Bridging Information Management …
215
Table 6.3 (continued)
Entity Name
Description
User Perspective
Authenticity
Authenticity provides information to enable any User to verify that an
object is correctly identiﬁed and free from (intentional or accidental)
corruption (i.e., capable of delivering its original message). Authenticity
encompasses identity and integrity (as well as Provenance and Reference).
Identity comprises all those attributes necessary to determine what a thing
is (e.g., the original recording of a Work). Integrity asserts that none of
those essential attributes have changed, i.e., there are no signiﬁcant
differences neither in the same resource over time nor between two
resources thought to be copies of the same asset. While identical copies
are authentic, authenticity does not require complete equivalence. Thus, a
digital version of an analog original may be an authentic copy of the Work
if it can be shown that the differences between the two versions are not
signiﬁcant, e.g., all of the content is present and is structured the same
way, and all important elements or attributes, such as title, creator,
performer, remain the same [176]
Integrity
Integrity represents the state of a Digital Item indicating the fact of being
complete and unaltered. It can be proven by verifying the presence of all
required parts in an unaltered (i.e., not modiﬁed) state [176]
Fixity
Fixity encompasses the information ensuring that resources (as described
by their properties) are not altered in an undocumented manner. This
information is also used to verify the integrity of Digital Items. Thus, if
the ﬁxity information for an Item changes over time, the Item has changed
[176]
to associate it to a speciﬁc child of Information Object, the Repository. In our PoF
Middleware we have to manage CMIS repositories, i.e., every repository exporting
a CMIS interface [312].
Furthermore, the Middleware Perspective in Fig.6.12 introduces the so-called
Detachment Elements [47] that are low- level elements such as File and Bitstream
(children of Component, aggregated by Item) as well as the detailed metadata infor-
mation represented by Descriptors (that are all children of Information Object).
These Detachments have been designed in order to be compliant to the most widely
adopted preservation metadata standards, as described in Sect.6.7.
Furthermore, the OAIS:PDI information package has been added to the Middle-
ware Perspective, which embodies the Preservation Description Information accord-
ing to the OAIS model. They are mostly Descriptors needed for expressing digital
preservation information. We have identiﬁed a Descriptor stereotyped as Metadata
that inherits from the more generic Information Object. The latter is inherited by
many low level descriptive information elements. Among others we have reported
here the Provenance, the Reference, the Authenticity, Integrity, and Fixity. As
described in the Fig.6.12, in order to assess the Authenticity we need to make use
of the other descriptors, expressed by the uses dependency dashed arrows: Fixity is
needed for guaranteeing the Integrity which is needed for assuring the Authenticity

216
F. Gallo et al.
that needs also Provenance information [148] and related resources and references.
Together with the OAIS Preservation Description Information we have added some
other Elements coming from other standards and best practices in digital preservation
(see Sect.6.7).
The OAIS Descriptor expressing the Rights has been improved: the Descriptor
OAIS Rights has quite general purposes. The experience matured in professional
environments (e.g., broadcasting) where archived digital contents have strict rules
for their government has induced us to split the Rights Descriptor into two main
elements: Exploitation Rights and Usage Rights. Without this distinction, it may
happen that many preservation actions can be blocked and/or forbidden: there are
many cases where the lack of usage rights are practically freezing the DPS, leading
to the impossibility to preserve and curate the stored contents. Even if the DPS has a
copy of the contents (and is ideally able to preserve it), if it is not sure about the exact
rights and permission on their modiﬁcations and changes it cannot undertake any
preservation action. To this end we need to clearly state as preservation description
information (PDI) this difference and clear it.
We split exploitation and usage. Separating the Rights about Exploitation (even-
tually described into Contracts) from the Usage allows the DPS to execute the
needed preservation actions without infringing the copyright laws. As good prac-
tice in digital preservation, during the submission and ingestion of digital contents
into preservation storage systems, it must be explicitly granted the usage rights,
especially if the digital archive has not the ownership of the contents and cannot
commercially exploit them. We have expressed the rights to perform these actions
with the hasRights association between Actor and the generic Rights. As already
discussed above, an Actor can be a Person as well as a System Instance.
A Descriptor not planned in the OAIS but vitally important especially for media
contents, is the Quality (represented as child of Descriptor as well) and implement-
ing all the digital properties related to the quality of a digital item. An image can
have, for instance, a quality property expressing the blurriness level and a video can
have a property for the lossy or lossless compression adopted (e.g., YUV 4:2:2).
The complete list of Elements introduced in the Information Model is provided in
Table6.3. It is worth noticing that due to the close correspondence of some elements
to the MP-AF standard those elements are also described according to [176].
6.5
Mapping to PoF Architecture
In the following, we describe the architecture of an integrated framework, which we
refer to in the following as the PoF Framework, based on the core concepts of the
model described above. We describe how the model can be mapped to the architecture
of the framework and propose a reference implementation.

6
Bridging Information Management …
217
6.5.1
PoF Framework Architecture
The architecture of the PoF Framework is made up of three layers: Active System,
PoF Middleware, and DPS, as depicted in Fig.6.13.
The Active System represents user applications, while the PoF Middleware is
intended to enable seamless transition from Active System to the DPS (and vice
versa) for the synergetic preservation, and to provide the necessary functionality for
supporting managed forgetting and contextualized remembering. The PoF Middle-
ware provides also the integration framework for all components implementing the
functional entities of the model.
The DPS is composed by two sub-systems: the Digital Repository and the
Preservation-awareStorage,whichincludesalsoaCloudStorageService.TheDigital
Repository is an OAIS-compliant system which implements the core OAIS function-
alities such as Ingest, Access, Administration and Data Management and supports
Information Packages. As already mentioned before, in the PoF Reference Model
a few OAIS functionalities have been extended, so the Digital Preservation System
and the Information Packages must also implement additional features such as those
related to contextualization. The Digital Repository is complemented by a dedi-
cated storage system, the Preservation-aware Storage, which implements the OAIS
Archival Storage functionality but is also used to extend some OAIS functionali-
ties such as Preservation Planning and therefore the storage system also includes
such preservation-aware functionalities. In the designed architecture, taking into
account recent developments in the most popular open-source digital preservation
systems, we also included a cloud-based storage to support long-term preservation.
The DPS, therefore, provides both content management and typical archive func-
tionalities required for the synergetic preservation.
TherepresentationofthePoFarchitectureinFig.6.13containsasyntheticdescrip-
tion and additional details for each component, which cannot be discussed here in
detail. It is important to stress again the role of the OAIS Reference Model [67] in
the overall architecture: since OAIS nowadays is the most recognizable conceptual-
ization of a DPS, it was considered as one of the building blocks of PoF approach.
However, the model described in Sects.6.3 and 6.4 complements and supersedes this
initial approach towards the OAIS model.
6.5.2
Relationship with PoF Architecture
The mapping between each functional entity in the PoF Functional Model and the
architectural components of the PoF Middleware is reported in Table6.4. A detailed
description of the purpose of each component can be found in Fig.6.13. It is worth
noting that for some components the mapping with model entities is not one-to-one,
because more than one component can participate in the implementation of a given
functional entity of the model.

218
F. Gallo et al.
Fig. 6.13 Architecture diagram of the PoF Framework

6
Bridging Information Management …
219
Table 6.4 Mapping between PoF Reference Model functional entities and the PoF Middleware
components
Functional entity
Model layers
PoF middleware components
ID Management
Core, remember and forget
ID Manager
Exchange support
Core, remember and forget
Collector, Archiver,
Metadata Repository
Content value assessment
Remember and forget
Forgettor
Managed forgetting and
appraisal
Remember and forget
Forgettor
De-contextualization
Remember and forget
Contextualizer
Contextualization
Remember and forget
Contextualizer,
Extractor,
Condensator
Preservation contract
management
Remember and forget
Context-aware
Preservation Manager
Re-contextualization
Remember and forget
Contextualizer,
Archiver
Search and navigation
Remember and forget
Navigator
Metadata management
Remember and forget
Forgettor, Extractor,
Condensator,
Contextualizer,
Metadata Repository,
Collector, Archiver
Content value re-assessment
Remember and forget
Forgettor,
Contextualizer
Context-aware preservation
management
Evolution
Context-aware
Preservation Manager
Evolution monitoring
Evolution
Context-aware
Preservation Manager
Context evolution management Evolution
Context-aware
Preservation Manager,
Contextualizer
The PoF Functional Model described in Sect.6.3 includes several workﬂows span-
ning across the three model layers. The different steps of such workﬂows involve
different PoF Framework components, mainly for what concerns the PoF Middle-
ware. The activation of the different components in each workﬂow can be obtained
from Table6.4.
Compared to Fig.6.13, the Scheduler component is not explicitly mentioned
in Table6.4, because it mainly provides process management functionalities sup-
porting the different workﬂows across the three layers described before. As already
mentioned in the description of the PoF Functional Model layers, other preservation-
related functionalities, which are typically supported by a DPS, also beneﬁt from PoF
Middleware components. For example, in terms of OAIS entities, two functionalities

220
F. Gallo et al.
which are relevant for the Evolution Layer, such as Preservation Planning and
Administration, are partially supported by the Context-aware Preservation Manager.
Another typical DPS-related functionality, such as the Pre-ingest, is supported by
the Archiver (and a dedicated workﬂow for Preservation Preparation).
It is worth noting that some components are involved only in one of the layers,
e.g., the Remember and Forget Layer or the Evolution Layer (see Table6.4) and
that Fig.6.8 about the Setting Change workﬂow includes also the Active System
and DPS components, while the others involve only the components within the PoF
Middleware.
6.5.3
PoF Reference Implementation
An implementation perspective of the Information Model described in Sect.6.4 has
been provided by the ForgetIT project.
The main challenges in implementing the PoF Reference Model are associated
to (a) the integration of the components within the PoF Middleware, (b) the inte-
gration of the Active Systems and of the DPS with the PoF Middleware, and (c)
the implementation of some speciﬁc components which are crucial for the new PoF
approach to digital preservation described in this model. Examples of such compo-
nents are the Forgettor, the Contextualizer, and the Context-aware
Preservation Manager, just to name a few.
In the following, we describe the reference implementation of the PoF Framework,
providing high-level details about the technologies used for such implementation
within the ForgetIT project.
The communication among the components and the business logic for the different
workﬂows have been implemented in the PoF Middleware using a Message-Oriented
Middleware (MOM) approach [73], powered by a rule-based engine which acti-
vates the different components according to speciﬁc Enterprise Integration Patterns
(EIP) [170], based on best practices in the ﬁeld of Enterprise Application Integration
(EAI). The choice of the most suitable solution was performed according to ForgetIT
requirements and also taking into account the possible future adoption of the PoF
Framework.
One suitable solution adopted in ForgetIT for implementing the PoF Middleware
was the Apache ServiceMix1 suite, which provides both the message broker and rule-
engine components, is released under an open source license and is supported by an
active community of developers and contributors, being included also in other prod-
ucts. Anyway, the adoption of any middleware technology requires some effort by
software architects and engineers before it can be used in a effective way. The integra-
tion of the Active System and the DPS should be based on standard technologies and
robust APIs, to enable the integration with different preservation solutions and user
1Apache ServiceMix - http://servicemix.apache.org.

6
Bridging Information Management …
221
applications. The PoF Middleware implemented in ForgetIT exposes REST APIs for
integrating with the Active System and the DPS, the communication protocols were
JSON and CMIS [312]. CMIS was used to access content in the Active Systems and
also to fetch it after re-activation. By employing CMIS, a standard representation of
the user content (including relationships, hierarchies and metadata) is established,
which eases later re-use, transformation and migration. For the implementation of the
two DPS components we used DSpace2 for the Digital Repository and OpenStack
Swift3 for the implementation of the Preservation-aware Cloud Storage.
In the previous section, we described how the different components of the PoF
Framework are related to the PoF Reference Model functional entities. The ﬁnal
release of the PoF Framework developed by ForgetIT was fully compliant to the
model described here. The PoF Framework prototype ﬁrst focused on two priority
workﬂows for basic synergetic preservation and managed forgetting support, then
it was further improved according to the model and provided an implementation
of the Preservation Preparation and Re-activation workﬂow, integrating all compo-
nents and ﬁnally implemented the PoF Information Model with an improved support
for the functional model workﬂows. This approach could be considered a viable
implementation schedule for building a PoF reference implementation from scratch.
The main challenge in implementing the PoF Framework components is related
to the novelty of the approach and to the lack of similar technologies or applica-
tions. Examples of such components are the Context-aware Preservation
Manager (synergetic preservation), the Forgettor (managed forgetting), and
the Contextualizer (contextualized remembering), just to name a few which
are closely related to the core PoF principles. The design and development of such
components should be based on a iterative approach, for example as one of those
provided by Agile software development methodologies such as SCRUM.4 Based
on one or more digital preservation use cases (for example, personal preservation
or organizational preservation), when new requirements are captured they can be
converted into speciﬁc digital preservation scenarios associated to one or more use
cases. Such scenarios, describing the relevant interactions among the users (human
or software components) and the PoF Framework, can drive the identiﬁcation of the
main processes and the design of the components.
6.6
Information Management Systems Extensions
The goal of the PoF approach is to keep the impact of introducing preservation
into the information management workﬂow as small as possible. Besides the long-
term preservation of selected content—which is the aim—the approach also has
2DSpace - http://dspace.org.
3OpenStack Swift - http://swift.openstack.org.
4SCRUM Alliance - https://www.scrumalliance.org.

222
F. Gallo et al.
the potential to introduce other more immediate beneﬁts into active information
management. Both for the basic functionality of supporting preservation as well as
for leveraging the beneﬁt enabled by the approach, some extensions are required in
the Active System.
These extensions are discussed in more detail in Sect.6.6.1, while Sect.6.6.2
elaborates on the beneﬁt that can be created in the Active System. Finally, Sect.6.6.3
focuses on preservation strategies, discussing different interactions of Active System
and DPS.
6.6.1
Extensions of the Active System
Extensions to the Active System are required, where it has to interact with a DPS
(possibly via a middleware as in the case of PoF architecture) and where information
has to be provided for the targeted intelligent preservation processes. This includes
the collection of evidence for information value assessment and the collection of
information in support of contextualization.
6.6.1.1
Supporting Information Exchanges
A core functionality, which needs to be enabled for synergetic preservation is infor-
mation exchange between the Active System and the DPS.
Information to be exchanged includes the content to be preserved as well as
metadata and context information describing this content. Furthermore, it has to be
possible to bring content from the DPS back into the Active System (see Re-activation
in Sect.6.3.2). Thus, bidirectional information exchange has to be enabled.
Bidirectional exchange can be enabled for example by a repository used by both
sides for making content available to the respective other system (plus possibly a
notiﬁcation channel). We mention here two different approaches adopted in For-
getIT to exchange information with the PoF Middleware in order to show that dif-
ferent strategies are possible with the actual implementation of the PoF architecture.
As a ﬁrst example, we consider TYPO3 CMS, the Active System chosen in the
project to implement the organizational preservation use case, empowered with a
standard-based repository supporting the content exchange standard CMIS [312],
which enabled the exchange between TYPO3 CMS and the PoF Middleware using a
CMIS repository as intermediate. Besides these asynchronous channels, more syn-
chronized forms of information exchange are also possible, such as direct service
calls. This was done in the second approach, where we considered as a second
example the PIMO Server (see Chap.7), the Active System chosen in the project to
implement the personal preservation use case, where we made use of direct service
calls between PoF Middleware and the Active System. There, CMIS was used as an
exchange format for content objects which enabled the PoF Middleware to retrieve
content directly from the PIMO Server. Nevertheless, both approaches use the PoF

6
Bridging Information Management …
223
interfaces for communicating with the PoF Middleware such as registering content,
PV updates, restore requests, and so on.
6.6.1.2
Information Value Evidence
The Managed Forgetting and Appraisal function described in Sect.6.3.3 heavily
depends on the idea of Content Value Assessment discussed before. This is true
for assessing short-term importance as it is done for computing MB as well as for
assessing the long-term value, namely PV.
For substantial Content Value Assessment, evidence have to be collected from
the Active System. For the short-term importance, these are for example information
about the usage pattern of a resource as well as information about the relationship
between resources. In order to provide such evidence, speciﬁc interfaces as well
as protocols are required deﬁning which evidence are provided, in which format
and in which frequency. Furthermore, methods for collecting such evidence have to
be implemented in the Active System. In the opposite direction, the Active System
might also proﬁt from the computed content value information and use it for advanced
functionalities and generate short-term beneﬁts (see Sect.6.6.2). The Active System
can perform such assessments on its own; in such case the exchange is reduced to
the calculated values such as the PV categories.
6.6.1.3
Supporting Contextualization
Context information added to the preserved content is meant to ease the interpreta-
tion of such content in case of re-activation. Information contextualization has been
discussed in Chap.5.
Context information can also be gained in different ways, since (a) it can be
provided by the Active System at the time content is sent to the DPS, (b) the PoF
Middleware can automatically extract information from the provided content and
other sources such as e.g. a domain-speciﬁc ontology or external knowledge sources
(e.g., Wikipedia), and (c) it can be a mix of the previous ones.
If this is possible in the considered Active System, harvesting context information
which is already explicated in the Active System (option (a) above) looks more
promising. In this way, a richer and more quality-controlled form of context can be
provided, with respect to what can be automatically extracted in the preservation
process.
For example, the approach discussed in Chap.7 is based on content already anno-
tated using an ontology, i.e., the PIMO. This annotation, obviously, is a good source
of context information for preservation. The Semantic Editor described in Chap.7
allows annotation of textual content during writing which supports early contextual-
ization by a user. The annotation vocabulary consists of above-mentioned personal
as well as external knowledge.

224
F. Gallo et al.
However, option (a) also puts higher requirements on the Active System: (1)
explicated context has to be available (or it has to be explicated for this purpose) and
(2) the Active System has to be extended with a functionality that is able to attach
context information to the content information sent for preservation.
6.6.2
Creating Beneﬁt in the Active System
Investment in preservation is typically paying back in the long term only. In order
to foster the adoption of preservation technology, it is one of the goals of the PoF
approach to also create short-term beneﬁt in the Active Systems—as a kind of positive
side-effect of introducing preservation technology. This section summarizes some
ideas on how such side-effects can be created in the Active System based on the PoF
approach.
6.6.2.1
Forgetful Information Presentation
The functionality for Managed Forgetting and Appraisal and the related Content
Value Assessment (see Sect.6.3.3) can be used to distinguish between content which
is of current importance from other content (see discussion about MB in Chap.4).
The values of MB can be used in the Active System to bring the content currently
important closer to the user, e.g., by showing it on a PC desktop or in special lists,
having it on mobile devices, or by preferring search results with high MB, the so-
called forgetful search. The core idea is to ease access to the things that are currently
important: this is related to one of the ﬁve characteristics of the PoF Reference
Model (brain-inspired), as discussed in Sect.6.2.3 where we described the Digital
Working Memory. This would be very similar to the Human Working Memory, thus
helping the user to focus on current activities. Examples for implementations of such
forgetful information presentation can be found in Chap.7.
6.6.2.2
Forgetful and Archive-Aware Search
Search is one of the core content access methods. Search and Navigation functionality
(see Sect.6.3.3) in the Active System can be affected in two ways by the introduction
of preservation technology, as described in the following. The most obvious way is to
smoothly integrate the archived content into the Active System search functionality,
i.e., archive-aware search. This idea has already been discussed in more detail in
Sect.6.3.3, within the Active System appropriate extensions in support of archive-
aware search have to be implemented. As a second way of modifying search in order
to beneﬁt from the PoF approach, forgetful search (see above) can be introduced.
The idea here is to take MB into account in the ranking function, thus preferring
resources of relevance for the current task in the search result list.

6
Bridging Information Management …
225
6.6.2.3
Creating Awareness for Content Value
On a more conceptual level, the idea of Content Value Assessment as it is used for
computing MB and PV in Managed Forgetting can also be used to raise the awareness
for the value of content assets. An improved understanding of the value of content,
which is based on a variety of factors such as investment, usage, popularity, etc.
can become an important building block for next-generation CMS applications. This
idea is closely related to organizational preservation and was investigated in ForgetIT,
while in the context of personal preservation using the Content Value Assessment
also for evolutionary knowledge management can be considered.
6.6.2.4
Content Value Assessment in the Active System
The functional entity Content Value Assessment is responsible for the assessment of
resources in the PoF Framework as pointed out in Sect.6.3.3. Considering its role
in the PoF Framework, the Semantic Desktop described in Chap.7 is an example of
Active System which is capable of computing the PV for the preservation decision as
well as the MB for Managed Forgetting within the Active System itself. In the light
of the beneﬁts discussed above, the Semantic Desktop embeds the Content Value
Assessment functional entity into the Active System. In contrast to that, another
Active System implementation could not calculate the values by itself but rather
deliver the evidence to the PoF Framework where the computation will take place.
Choosing either of the two approaches is also a design decision, which could
be made because of the beneﬁcial usage of both MB and PV in the Active System
infrastructure, as in the case of Semantic Desktop. The rich semantic model of the
PIMO (see Chap.7) and the usage statistics of the Semantic Desktop allow for a
comprehensive view on the resources with respect to MB and PV. Furthermore, the
nature of the PIM application scenario implies a lot of access, usage, and changes to
resources and the PIMO resulting in a lot of trafﬁc as well as content assessment in
the PIMO as a knowledge base. Therefore, both values are computed in the Semantic
Desktop and stored directly in the PIMO to be easily accessed by its components
and thus, making them an integral part of the PIMO. To enable the PoF Middleware
to make decisions based on the PV, the values are reported and updated in certain
time intervals to the PoF Middleware.
6.6.3
Preservation Strategies
When preservation is introduced into the content management life-cycle of an Active
System,avarietyofdecisionshavetobetakenwhendeﬁningthepreservationstrategy
to be used. This includes decisions about when to preserve and about the granularity
of preservation. Furthermore, the interaction between resource versioning and the
preservation of a resource has to be deﬁned.

226
F. Gallo et al.
Preservation actions can be triggered in different ways. They can for example be
activated by the content management life-cycle: resources might be considered for
preservation, when they go out of active use (low MB) or already upon creation or
import into the system (e.g., for very valuable resources). Furthermore, preservation
can be scheduled, for example, by queuing all resources above a predeﬁned PV
threshold for preservation on a regular basis. Finally, it is of course also possible to
manually trigger preservation actions for individual resources or resource collections.
The choice of strategy is also dependent on the type of resources considered as well
as on the level of control the user wants (or needs) over the preservation process. This
could include enabling identiﬁcation and removal of duplicate objects, or removal of
objects withpoor quality, as well as if thereshouldbe(or has tobe) anytransformation
of objects already at ingest. Decision has to be made on which of the options are best
suited for an Active System under consideration. The chosen options inﬂuence the
way the preservation process is integrated into the Active System beyond enabling
the transfer of content to be archived.
Besides deciding when to preserve, it is also necessary to decide what to preserve.
This can be considered along two related dimensions. First, it is possible to either
preserve individual resources or entire collections of resources (or other types of
complex objects such as sets of related resources) as one unit of archival. Second,
resources can be preserved in isolation or together with context, which describes
them. This second point is closely related to the work on contextualization discussed
in Chap.5 and the results affect the deﬁnition of the archival objects, the basic units
of the PoF Information Model (see Sect.6.4).
The choices with respect to granularity of preservation have consequences for the
transfer protocols between Active System and DPS. In addition, it might require
methods for selecting (extracting/collecting) relevant context information for a
resource to be preserved.
Onefurther aspect of granularityis thepossibilitytousedifferent serviceproviders
for different types of objects. Image objects could for example go to a specialized
image preservation provider, while regular documents go to a general preservation
service, and moving pictures (and sound) to a media archive. This is supported by
the Preservation Contract, Exchange Support, and ID Management entities running
in the PoF Middleware, which gives the Active System one single point of contact
for interacting with several DPS.
Aninterestingfurtheraspectofthepreservationstrategyistothinkaboutthemodel
of co-existence between the copy of the resource in the archive and the resource in
the Active System (if the strategy allows for such a co-existence). This has some
implications, when the resource is changed in the Active System, after a copy has
been archived. Typically, newer versions will not overwrite the archived version.
Rather, the changed version will be archived as a new version. There are, however,
decisionstobemadeabout,ifandwhentheupdatedversionisput(automatically)into
the archive. In any event, there should be a possibility to deﬁne how to handle copies
in the DPS and if/when the version in the archive should be overwritten/updated, or
versioned.

6
Bridging Information Management …
227
Last but not least, the preservation strategy is also to be considered for a service
provider when deﬁning the business model for a preservation service. The strategy
will inﬂuence service costs including trafﬁc and storage as well as quality of service
offered to the customer.
6.7
Digital Preservation Standards
Several digital preservation standards have been developed so far. In the previous
sections, we already mentioned the relationships with a few standards, such as OAIS
and PAIMAS. In order to better position the model in the context of digital preserva-
tion, we mention here a few examples among the most popular digital preservation
models, starting from OAIS Reference Model, but also discussing other standard
metadata schema conceived for digital preservation applications:
• Open Archival Information System (OAIS) [67], according to the deﬁnition,
is a model for describing an archive, consisting of an organization of people and
systems, that has accepted the responsibility to preserve information and make it
available for a Designated Community. The reference model introduced by the
Consultative Committee for Space Data Systems (CCSDS) is now an ISO stan-
dard (ISO 14721:2012). Due to the relevance of OAIS in the digital preservation
community, including the available platforms claiming compliance to this model,
it has been considered as one of the building blocks of PoF approach. The structure
of the PoF Reference Model is similar to the one adopted in OAIS, with functional
and information models. The PoF Reference Model has a broader scope, since it
considers active information use and preservation as part of such a joint ecosys-
tem, which stresses the smooth transitions and the synergetic interactions rather
than the system borders. This is a core distinction from the OAIS model, which is
mainly related the DPS. Hence the PoF approach has a different focus and can be
integrated with any OAIS-compliant approach because it extends the preservation
outside the archive, bridging the gap with the information system.
• Metadata Encoding and Transmission Standard (METS) [233] is a standard
for encoding descriptive, administrative, and structural metadata regarding objects
within a digital library and is maintained by the Library of Congress. Its modu-
lar structure allows the integration with virtually any other metadata schema to
represent speciﬁc information. The METS document acts as a wrapper of several
sections, each with a speciﬁc set of metadata for the different purposes of digital
preservation. Each section can embed any metadata schema, including custom
ones. For example METS can embed Dublin Core elements [413] or PREMIS
elements (see below) for descriptive metadata. The most popular digital preser-
vation platforms and also solutions for digital repositories typically use METS
for internal and external metadata representation and also as an exchange format.
Concerning the relationship with the PoF model, due to its ﬂexibility, METS can

228
F. Gallo et al.
be used to represent archived information in the DPS but also as an exchange
format when ingesting or accessing content from the PoF Middleware.
• Preservation Metadata Implementation Strategies (PREMIS) [235] refers to a
popular metadata standard from Library of Congress, supported by several digital
preservation standards. The PREMIS Data Dictionary and its supporting docu-
mentation is a comprehensive, practical resource for implementing preservation
metadata in digital archiving systems. The Data Dictionary is built on a data
model that deﬁnes ﬁve entities: Intellectual Entities, Objects, Events, Rights, and
Agents. The Data Dictionary deﬁnes preservation metadata supporting viability,
renderability, understandability, authenticity, and identity of digital objects in a
preservation context. It represents the information necessary to preserve digital
materials over the long term and makes no assumptions made about preservation
technologies, strategies, metadata storage, and management. PREMIS can be used
standalone or to represent preservation metadata within the Administrative section
of a METS container. The relationship between PREMIS and the PoF Model lies
in the representation of core elements of the Information Model, such as Collec-
tions and Items even at the level of Files and Bitstreams, but also to represent some
features of the Preservation Contract.
• Provenance Ontology (PROV-O) [407] is a standard from World Wide Web
Consortium (W3C). PROV-O is a lightweight ontology that can be adopted in a
wide range of applications. The PROV Ontology classes and properties can be used
to represent provenance information (for example, according to OAIS guidelines),
and can also be specialized for modeling application-speciﬁc provenance details
in a variety of domains or used to create domain-speciﬁc provenance ontologies.
PROV-O Agents can be used to represent actors in Preservation Contracts and to
enrich Collections and Items in the DPS with provenance metadata, as part of the
preservation tasks.
• Multimedia Application Preservation Format (MP-AF) [176] is a standard
deﬁned by ISO IEC SC29 WG11 Moving Picture Expert Group (MPEG). The
MP-AF (standard ISO/IEC 23000-15) deﬁnes the Multimedia Preservation
Description Information (MPDI), extending the concept of OAIS Preservation
DescriptionInformation(PDI),providingmetadataaddressingthespeciﬁcrequire-
ments for preserving multimedia content. MP-AF deﬁnes a metadata format that
enables users to effectively exchange information (metadata) related to multimedia
preservation operations and their outcomes. Typical examples include the descrip-
tion of integrity checking and related results, content migration from one system
to another, replication of sub-parts or entire contents, content quality evaluation
and related quality report, relationships between the source and output of any
transformation process, etc. The model has been harmonized with MPEG-21 Dig-
ital Item Declaration and the schema reuses considerable parts of existing MPEG
technologies, most notably MPEG-21 and MPEG-7. The relationship with the PoF
model is mainly related to the representation of Collections and Items, but also
for archiving information in the DPS concerning the digital rights associated to
the preserved resources and the quality information generated automatically by

6
Bridging Information Management …
229
tools for quality assessment, which is an essential part of the digital preservation
life-cycle.
• Conceptual Data Model (CCDM) [123] is a standard maintained by the European
Broadcasting Union (EBU). The EBU Class CCDM is an ontology deﬁning a
basic set of classes and properties as a common vocabulary to describe business
objects, e.g. programmes, articles and other types of content, and their relations
in the business processes of media enterprises. Examples are programmes in their
different phases of creation from commissioning to delivery, their associated rights
or publication events, etc. Properties for describing each of the objects can be found
in another EBU standard, EBUCore, or can be customized. The CCDM has been
purposefully designed as a minimum and ﬂexible set of classes for a wide range
of broadcasting applications, including archives, exchange and media service-
oriented production, semantic web, and linked data. The CCDM speciﬁcation
combines several aspects from existing models and speciﬁcations into a common
framework. It has been built over several EBU attempts to represents broadcasting
as a simple logical model. It has beneﬁted from EBU work in metadata modeling
(P-META and EBUCore) and semantic web developments. The connection with
the PoF model mainly lies in a alternative way of representing Collections and
Items in scenarios typical of video broadcasting.
The list provided above is not intended to be exhaustive and is mainly based on
the direct experience of the authors in digital preservation research.
Based on the ideas discussed in the previous sections, a detailed model could be
derived to represent core information model entities and the representation of such
entitiesshouldbeeitherbasedonexistingmetadatastandardssuchasthosementioned
above or based on custom elements in such a way that guarantees interoperability
with the adopted technologies, platforms and tools.
6.8
Conclusions
In this chapter, we discussed a model for digital preservation based on the PoF
approach. Five main characteristics concerning the foundations of the model were
discussed. The resulting model is composed by a functional and a information part.
The main entities and workﬂows of the functional model were described, including
three different layers (Core, Remember and Forget, Evolution) which represent the
different levels of compliance to the model which can be adopted within an orga-
nization or in a preservation process. The information model introduced three main
preservation entities (Content, Context, and Situation) and their relationships, also
compared to existing digital preservation standards. A reference implementation of
the PoF model with an emerging architecture integrating information and preserva-
tion systems was described. In this chapter we also discussed the impact of the PoF
approach on the information management systems and how to take advantages from
the adoption of synergetic preservation.

Part III
Multimedia Preservation in Practice

Chapter 7
Remembering and Forgetting for Personal
Preservation
Heiko Maus, Christian Jilek and Sven Schwarz
Abstract Chapter 1 detailed the need for preservation especially for our personal
life. Even so, it is not part of most users’ regular practice. Preservation challenges
users with a manual effort and a somewhat stern discipline to continuously perform
the required steps. Being aware of these challenges, the chapter will present the novel
approach realized in ForgetIT to embed support for personal preservation in users’
daily activities of Personal Information Management (PIM), reducing the effort and
users’ cognitive burden for preservation. In this approach, the previously introduced
concepts of remembering and forgetting play a major role for a successful realiza-
tion and for providing beneﬁts to users. It is a method rooted in artiﬁcial intelligence
research that allows us to derive and represent a user’s mental model and making
data machine understandable. This “semantiﬁcation” of the user’s resources paves
the way for more effective functionalities for automated preservation, forgetting, and
remembering embedded in the user’s daily activities and applications. The chapter
will detail a pilot based on this ForgetIT approach realizing remembering and forget-
ting for preservation. We further investigate how activities from PIM provide context
to these. Regarding remembering, we present applications allowing a semantic photo
organization and providing a self-generating diary to remember past events. Consid-
ering forgetting, we investigate how forgetting functionalities can be embedded in
applications and present how different variants for forgetting are used in the pilot.
Finally, a comprehensive approach for personal preservation is presented utilizing
the PoF Framework introduced in Chap.6 and overcoming the obstacles for personal
preservation. The chapter concludes with a discussion of experience of using the
pilot in daily work of our research group.
H. Maus (B) · C. Jilek · S. Schwarz
German Research Center for AI (DFKI), Kaiserslautern, Germany
e-mail: Heiko.Maus@dfki.de
C. Jilek
e-mail: Christian.Jilek@dfki.de
S. Schwarz
e-mail: Sven.Schwarz@dfki.de
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_7
233

234
H. Maus et al.
7.1
Motivation for Personal Preservation
When considering preservation for personal, non-professional, or home usage, we
have to contend with a vast increase in ways to create digital artifacts (computers,
smartphones, tablets, digital cameras, etc.) as well as an ever-increasing amount
of storage for this digital material. These days, users’ personal information space
consists of a substantial number of information objects connected to the person’s life
such as wedding videos, travel photos, or graduation keepsakes. It requires serious
dedication and cognitive effort to organize all this data and keep it accessible as time
passes. Moreover, these digital artifacts often represent past moments but are not
associated with a physical memento. Therefore, they form a valuable resource for
the user and future generations. If the material is lost or corrupted due to improper
conservation, it will be useless.
As already discussed in Sect.1.2, most users still use backups as their main form of
preservation (see [186]). Existing recommendations for personal preservation such
as the ones of the Library of Congress [232, 234] as well as preliminary technologies
such as ﬁrst cloud storage services (see [331] for an overview) leave most steps for
personal preservation to the user, i.e., what to save, how to organize, where to store
(hard disk or online storage), and when to migrate. All this is a lot of effort for users,
various decisions need to be made and it requires discipline in, e.g., maintaining and
updating the archive.
Creating a structure for preservation in particular is one of the major problems.
Everypersonwhoaddsmaterialhastofollowthisstructureeverytimefurthermaterial
is added, and people who want to search for ﬁles need to be aware of this structure.
After a long period of time, someone else such as descendants need to be able to
interpret the structure.
Considering this current state of personal preservation, the main obstacles we see
so far are:
• Users are not aware of personal preservation of digital content. There is a huge
gap between current practices, such as backup by copying material to a different
hard disk, and a proper Preservation Strategy.
• When starting with personal preservation, the user faces high up-front costs in
terms of time, effort, and resources, and there are very few tools to help users
prepare material for preservation and interact with an archiving service. The best-
practice recommended by Marshall (see Sect.1.2) is not supported comprehen-
sively.
• There is no personal preservation service for the majority of end users which
supports the whole preservation process. Cloud storage alone is not preservation.
• The vast increase in digital content with relevance to a person’s life poses
challenges to PIM as well as preservation.
• Designing and organizing an archive so that its structure can be understood in
several decades from now is cognitively challenging for users.
While some users are concerned about preservation, it is not part of most users’
regular practice. Preservation requires manual effort and to actually do it, the users

7
Remembering and Forgetting for Personal Preservation
235
need to think about it which poses a cognitive burden on them. Therefore, the
approach for Personal Preservation presented in this chapter is to embed it into
the user’s activities in the personal information space. This comprises collecting
material to be preserved and evidence for preservation values, as well as triggers for
preservation while keeping user involvement minimal.
But how can this be achieved? User’s Personal Information Management (PIM;
see [188])—i.e., all activities to organize one’s life and information space—is a
major reason to use computers in daily life (apart from recreation with gaming,
for example). That means, the interaction with computers and information objects
leaves electronic footprints of resources which could be leveraged for supporting
preservation activities of users.
Therefore, by concentrating on the users’ PIM, we can cover various life events
together with associated digital material, usage of the digital material, and derive
evidences for preservation values. For example, we can detect whether a ﬁle is only
relevant for a certain time frame (such as time tables for a trip) or has emotional
relevance (e.g., a photo showing the user’s children).
Furthermore, it is a chance to derive the user’s mental model when dealing with the
content, and thus, get a means to describe the preserved material from a user’s point
of view with less effort. Therefore, providing an ecosystem for PIM will allow to
collect resources including relevant context information from their usage to identify
candidates to preserve and automate preservation. Ideally, activities for PIM and
applications in that area such as photo organization, should be enabled to provide
input for preservation as resources as well as context provider.
As part of Artiﬁcial Intelligence, research in the Semantic Desktop (see, e.g.,
[40, 156, 349]) ﬁeld addresses how to use semantic technologies and knowledge
representation to make user’s data and activities machine understandable and how
to embed this in user applications as well as their devices. Section7.2.1 will give
insights on the Semantic Desktop approach.
Although some of the required ingredients can be seen in some operating systems
such as Microsoft’s ofﬁce suite or Apple’s iCloud ecosystem with documents, con-
tacts, calendars, or even tags, a knowledge representation as envisioned in the Seman-
tic Desktop approach with its knowledge representation is not available. Therefore,
the presented work for personal preservation also presents potentials for future oper-
ating systems and application ecosystems to apply semantic technologies and knowl-
edge representation.
By using the Semantic Desktop approach we can:
• use the Personal Information Model (PIMO) as knowledge representation to
represent a user’s mental model over time. The PIMO [351] provides a basic
ontology of concepts that a person uses for their desktop and PIM. The PIMO is
modeled as a semantic graph of interconnected concepts and information objects.
Extensions adapt the ontology to speciﬁc domains or tasks.
• provide an ecosystem of applications and plug-ins which access the PIMO for
vocabulary and knowledge representation. The ecosystem outlined in [273] serves
as an implementation of the Semantic Desktop approach and can be extended to

236
H. Maus et al.
support the goals of Managed Forgetting, Contextual Remembering, and Syner-
getic Preservation.
• providemeanstocontinuouslyupdateauser’sPIMOandadapttonewsituations.
The PIMO is capable of long-term use [348, 352] and the oldest PIMO still in use
as knowledge base at the DFKI has been evolving steadily for more than 10 years.
• provide context for information objects such as ﬁles, webpages, or emails by
using the PIMO. The PIMO provides the knowledge representation layer both for
users and for semantic services [350].
• provide a means to understand—together with observing user actions, access,
creation, and deletion of information objects—the context of the user [357] and
provide services such as context-aware task management [274].
Up until now, research focused on users in a professional setting, e.g., in PIM for
research [352] or business administration [217]. This work indicates that people can
build up and use rich PIMOs from and in their everyday work in the ofﬁce. Concepts
range from professional topics such as projects, contacts, and meetings to personal
concepts such as interests and friends.
7.2
Semantic Desktop Approach to Personal Preservation
In the following, the Semantic Desktop approach is introduced together with the
concept behind the PIMO. We will detail how this contributes to the PIM of users
and present several PIMO-enabled services and how they contribute to Personal
Preservation.
7.2.1
Semantic Desktop in a Nutshell
The Semantic Desktop approach was motivated by the observation that users, espe-
cially knowledge workers, would beneﬁt from a support of their personal knowledge
management (for details on history—here, most notably the EU Integrated Project
Nepomuk (The Social Semantic Desktop; Grant No FP6-027705) was a predecessor
for the Semantic Desktop work in ForgetIT—and motivation see [40, 114, 349]) as
the modern working environment places high requirements on knowledge workers:
they are confronted with various applications, are involved in several projects and
processes, work in changing teams, are on the road with a mobile ofﬁce, and ﬁnally,
face an ever-increasing ﬂow of information. The result is a knowledge space —a
metaphor for the distribution of information and knowledge known by the user, con-
tained in different documents, applications, and in the structuring of folders on the
ﬁle system, as well as their interconnections which need to be handled by the user.

7
Remembering and Forgetting for Personal Preservation
237
These knowledge spaces are complex, dynamic, distributed over several applications,
and use different vocabulary (i.e., set of names for folders or tags, for example). In
this situation, it is hard to handle the complexity of the resulting personal knowledge
space.
Therefore, the approach of the Semantic Desktop addresses this challenge. It fol-
lows the strategy to embed the mental model—the representation of the world in
the user’s cognitive system as the user understands it—of the knowledge worker in
their daily work by means of a “Personal Information Model” (“PIMO”, [351]). The
representation of the user’s mental model in the PIMO consists of concepts (called
“things”; We keep this naming convention from the Semantic Desktop area as it is
one step closer to an end user when explaining that all their things are represented in
the PIMO instead of talking about concepts.) such as speciﬁc topics, projects, per-
sons, tasks, etc.), associations between them (persons are member of projects, a task
has topic “Semantic Desktop”, etc.), and ﬁnally, associated resources (documents,
emails, web pages, photos, etc.). Such resources are semantically represented in the
PIMO (the Semantic Desktop community denotes this as “rebirth”: the resource is
introduced with its semantic representation into the PIMO).
The PIMO serves as an easy to understand conceptualization of the knowledge
worker’s mental model, which can be used as a common vocabulary across different
applications. Therefore, the PIMO provides the means required for a multi-criterial
document classiﬁcation considering the user’s subjective view. Figure7.1 shows an
example of a representation of resources in the real world within the PIMO. The
person Peter Stainer, his holiday trip as an event in a calendar, and the photo on his
computer are resources in the real world.
These resources are represented in the PIMO as things having different types
(in knowledge representation, these are instances of classes such as “Peter Stainer”
is an instance of the class “Person”). The relations between the things can also be
Fig. 7.1 The layers of the PIMO and their connection to the real world

238
H. Maus et al.
expressed, for instance, that Peter attended the event and a photo has been taken on
the trip. The classes and relations that are available for usage are deﬁned in the PIMO
ontology: a model of classes, their hierarchy, and allowed relations between them
(as shown in the upper layer of Fig.7.1).
The resulting graph connects various resources, e.g., from the ﬁle system with
items of a calendar application, with notes written by the user, with web pages about
the holiday. The resources can further be associated with topics, locations, tasks, etc.
In various applications, this graph then allows to ﬁnd and access resources or things,
to annotate, and to relate them.
The PIMO uses the semantic power of the formal representation of the PIMO as
an ontology [351], thus, introducing a knowledge representation layer on the user’s
computer and beyond. Besides enabling to annotate and interconnect resources over
application borders, further semantic services are available which make use of the
semantic representation of the user’s mental model in the PIMO. Figure7.2 depicts
the Semantic Desktop ecosystem and components.
Therefore,theSemanticDesktopsupportsauser’sPIM(andevenPersonalKnowl-
edge Management) across various applications, embracing various resources, and
connecting them in the PIMO, providing contextual information for value-added
services. This support for a user’s PIM is detailed in the next section.
Fig. 7.2 The Semantic Desktop (SD) ecosystem architecture: integrating and leveraging external
data sources by means of semantic technologies to provide value-added knowledge services. Plug-
ins and endpoints embed the PIMO into applications and reaching out to user daily activities

7
Remembering and Forgetting for Personal Preservation
239
7.2.2
Supporting Personal Information Management
This section provides an overview of selected features of the Semantic Desktop
ecosystem for PIM and their contribution to the goals of personal preservation. The
Semantic Desktop ecosystem consists of various applications and plug-ins for stan-
dard programs that provide support for the users in every day life, and thus, in various
scenarios identiﬁed for personal preservation. Furthermore, the resources, concepts,
and the interaction with them provide evidences for services in the Preserve-or-Forget
(PoF) Framework in order to calculate Memory Buoyancy (MB) and Preservation
Value (PV). Furthermore, the ability of the PoF Framework to get insights on the
user’s relation to resources helps in the reduction of effort for actually preserving
resources whenever this is embedded and supported by PIM applications.
Therefore, the infrastructure consists of components which support users’ PIM.
The support addresses the following areas and functionalities:
• Annotation support: With its contained concepts (such as topics, persons, events,
projects, etc.), PIMO provides a vocabulary to users for annotating (or tagging
respectively) resources of different types. By providing access to this vocabu-
lary as a set of tags for third-party applications, tagging with one vocabulary
across different applications is enabled. This surpasses what is common today,
where each application or service has its own set of tags. The following appli-
cations for PIM activities are covered: web browser and email client (with the
add-on FireTag for Mozilla Firefox and Thunderbird, also MS Internet Explorer),
ﬁle system (Semantic File Explorer for Windows), and annotation embedded in
special-purpose applications such as photo organization or a text editor.
• File organization: With the SemanticFileExplorer (SFE)—a PIMO-enabled side-
bar embedded in the Microsoft Windows File Explorer (see Fig.7.10) —it is pos-
sible to annotate, access, ﬁlter, and ﬁnd ﬁles using the PIMO embedded in a user’s
normal desktop environment. The PIMOCloud—a service similar to DropBox or
Google Drive, but embedded in the PIMO, semantically representing each ﬁle as
a resource in the PIMO—extends this to cloud storage with ﬁle synchronization
across different devices, versioning, and sharing. Embedded in the SFE are vari-
ous convenience methods such as browsing a ﬁle in the PIMO or creating a photo
collection from a folder.
• Task management: The Semantic Desktop ecosystem supports task management
by introducing tasks as dedicated concepts with special-purpose applications such
as the Task Management application in PIMO5 (PIMO5 is a web-based user inter-
face for the Semantic Desktop ecosystem using HTML5; see Fig.7.5). This app
supports reminders, due dates, notes, annotations of concepts and resources (across
all connected applications and plug-ins such as tagging an email with a task), etc.
Besides, it is also possible to connect and synchronize with third-party task man-
agement tools.
• Calendar: Like tasks, events are also speciﬁc concepts in the Semantic Desk-
top. This allows to manage events and reminders within the PIMO as well as to
organize and associate them with everything else represented there, such as topics,

240
H. Maus et al.
persons, resources, or notes. By offering a CalDAV (a standard internet protocol for
accessing scheduling information on a server including events, alarms, and tasks)
endpoint, events can be synced and managed in all third-party calendar tools and
devices that support CalDAV such as Mozilla Thunderbird (with Lightning calen-
dar) or on iOS (e.g., iPhone) without losing the power of the richer representation
of events in the PIMO.
• Contacts: In analogy to the “calendar”, the Semantic Desktop also offers a Card-
DAV (a standard internet protocol for accessing address book information on a
server) endpoint, thus allowing to access and exchange contacts in the PIMO, i.e.,
persons in the PIMO will be available via CardDAV and the Semantic Desktop
can extract contact information from CardDAV-supporting address books, respec-
tively.
• Writing: Taking notes, writing descriptive texts or whole documents is supported
by a semantic text editor (see Sect.7.2.3) which is embedded throughout the infras-
tructure. This supports all situations in the Semantic Desktop ecosystem where text
has to be written such as notes, descriptions of photos (see Fig.7.4), or task notes.
• Mobile: The infrastructure of the PIMO allows to have mobile access to the
PIMO—to the concepts as well as to resources via PIMOCloud—either by an
HTML client on a mobile device or by the endpoints mentioned above such as
calendar or contacts.
• Collaboration: Users have their own PIMO with private concepts and resources
which can be shared with other users (on the same PIMO server). This supports
teams in an organization as well as a family setting. The infrastructure allows to
share and reuse concepts as well as any kind of resources from photos to notes,
calendar events to tasks. As a default, things created by users are initially pri-
vate, only visible to the creator. Even annotations of such a private resource to
public ones stay private. Thus, users are in control of what to share with others.
Furthermore, users can subscribe to interesting things in a PIMO such as topics
and get informed about activities of other users. A chat infrastructure allows to
chat with users about resources such as appointments or webpages which are then
also available in the PIMO. First step toward talking to the PIMO via a chatbot
are undertaken such as searching the PIMO for things or let the chatbot explain
a thing contained in the PIMO such as a person or an organization. This social
aspect of the PIMO fosters its usage and the beneﬁts of actually introducing mate-
rial. Likewise, sharing of resources and interaction with shared resources of other
users provide further evidences for PV.
The open nature of the Semantic Desktop ecosystem—i.e., providing access and
semantic services on the user’s PIMO but leaving the actual usage in plug-ins or
special-purpose apps open to developers —allows a multitude of special-purpose
applications or plug-ins to be developed. For instance, tagging in the email client as
shown in Fig.7.3.

7
Remembering and Forgetting for Personal Preservation
241
Fig. 7.3 PIM with PIMO: FireTag, an add-on for Mozilla Thunderbird, embedding the PIMO into
the email client: by using semantic text analysis, relevant concepts from the PIMO are proposed.
Furthermore, anything from the PIMO can be annotated such as tasks or events. Such annotated
emails are then available in the PIMO
7.2.3
PIMO-Enabled Services
As previously explained, at the heart of the Semantic Desktop is the PIMO, which
models the personal knowledge of the user. Having access to the PIMO and the
mental model of the user represented there, various services can be provided. Since
it is very useful for personal preservation, we present the semantic text editor “Seed”
(short for “Semantic editor”).
Seed provides an interface for texts in the PIMO such as notes and is an extensible
knowledge-supported natural language text composition tool. It builds upon state-of-
the-art technology in the ﬁelds of NLP (Natural Language Processing), Linked Open
Data, and Semantic Web technologies to provide a user-friendly way of interacting
with complex knowledge systems (see [127–129]).
Semantic annotation of texts plays an important role in enabling early contextu-
alization of written texts. These annotations embedded in the text allow to ﬁnd texts
more easily via semantic associations. Moreover, they provide context for preserva-
tion in terms of the user’s mental model in the PIMO at the time of creation without
requiring much additional effort.
Seed is integrated in multiple GUIs of the PIMO where texts appear such as in
notes, tasks, or events. It helps in exploring, modifying, and creating semantically
annotated textual content. It reduces prerequisite domain knowledge and the accom-
panying cognitive load on users. Figure7.4 shows a sample text being composed in
Seed. Seed’s main features are:

242
H. Maus et al.
Fig. 7.4 Text being composed using seed (highlights in green: annotated entities from the PIMO).
Users are able to view information on entities including information from external sources (here,
Taksim Square entry from DBpedia—originating from Wikipedia—as well as the gray highlight
for the Beyolu district which gives details on that entity from Wikipedia); such information can also
be imported into the PIMO as was the case here
• Annotate as you type: Users can naturally compose text in a WYSIWYG (What
you see is what you get: The ﬁnal appearance of the text is directly shown to the
user, e.g., headlines are shown in bold) fashion, while Seed analyzes the content
in real-time and annotates mentions of PIMO entities in a non-obtrusive way.
Furthermore, wrong or unwanted annotations can be rejected.
• Proactive information delivery: Seed not only annotates text, it also presents
information about the entities annotated in an interactive way allowing the user to
discover further knowledge about things mentioned in the text.
• Access to public knowledge: Seed also discovers entities mentioned in texts which
are not yet represented in the PIMO. It links them to publicly available Linked
Open Data sources like DBpedia (which is a semantic representation of Wikipedia)
and proposes them to the user (gray highlights in Fig.7.4). Then, users are able to
import them into the PIMO along with their metadata, facilitating the growth of
the user’s personal knowledge model.
• Collaboration: Texts created and annotated in Seed can immediately be reused by
others. This allows for many collaborative editing scenarios that make use of the
semantic annotations. For example, one user can create and annotate a text, which
is later accessed by another user who revises it and modiﬁes shared annotations
as well as the content of the text.
• Immediate contextualization: By annotating the text with entities from the user’s
PIMO, Seed saves the context with the text allowing it to be better retrieved,
recalled, preserved in context, and ultimately understood. This immediate con-
textualization reduces the effort during preservation time as well as providing the
subjective view of the user.
Seed is an example for the openness of the PIMO infrastructure and available as
a service in the application layer shown in Fig.7.2.

7
Remembering and Forgetting for Personal Preservation
243
7.2.4
Contribution of the Semantic Desktop Approach to
Personal Preservation
In this chapter, we will extend PIM with preservation and forgetting and use with the
PIMOapersonalvocabularyclosetotheuserssubjectiveview.Applyingpreservation
support as part of PIM will ease the burden for users to do this manually. Moreover,
using the PoF Framework introduced in Chap.6 allows for seamlessly embedding
preservation in the Semantic Desktop ecosystem.
Second, introducing Managed Forgetting to PIM will provide a whole new way
for users in this area. The forgetting will also reduce the user’s cognitive burden for
past activities and information in PIM but still allow access if needed. Similar to our
brain will retrieve details of our past when remembering and getting associations
(see Chap.2), the Semantic Desktop provides such means.
Therefore, the contribution of the Semantic Desktop approach for preservation,
forgetting, and remembering is as follows.
7.2.4.1
Preservation
The Semantic Desktop ecosystem (applications, plug-ins, mobile apps) allows us
to connect the PIMO to the user’s information objects through annotating photos
and web pages, organizing documents and emails, and managing tasks as well as
reminders. Information objects are connected by reusing concepts such as contacts,
which are part of the PIMO, for annotating images and writing emails. The resulting
personal information space tightly links resources and concepts. Evidence for PVs
and context for preserving an information object can be derived from this information
and formalized using the PIMO knowledge representation.
Importantly, the continuously evolving PIMO not only covers information objects
in current use but also objects which have already been stored in the archive for later
use and are therefore no longer directly accessible for the user.
7.2.4.2
Forgetting
Metadata about and observations on information objects in the Semantic Desktop
ecosystem that is held together by the PIMO provide evidences also for assessing
topicalandlong-termrelevance.Therefore,investigatingactivitiesandchangesauser
does in the semantic network of the PIMO gives insights on relevance of information
objects. For instance, while topics of previous projects might still be relevant to the
user, most of the associated resources, such as meetings, notes, and presentations,
might no longer be of interest. Here, the PIMO and the Semantic Desktop ecosystem
provide insights to identify concepts and resources to be forgotten.

244
H. Maus et al.
7.2.4.3
Remembering
Like the human brain, the PIMO is still capable to retrieve things which seem to be
forgotten. Similar to humans, who can remember things or situations by starting with
a cue and then follow associations, PIMO can provide paths through the semantic
graph that start from a particular node. For example, starting from a project (ForgetIT)
we can follow a path to an associated event (the kick-off meeting in Hannover), then
to a photo (the group in front of the town hall), and to a person (the professor
from Edinburgh). At each node along the path, the links from the node to other
concepts provide the context required to remember. Thus, the PIMO contributes to
contextualized remembering.
7.2.4.4
Proof-of-Concept
As a result of the ForgetIT project, the presented approach built on top of the Semantic
Desktop shows the potential for personal preservation if provided as part of the user’s
activities for PIM. Furthermore, it shows how the concepts of Managed Forgetting,
Contextual Remembering, and Synergetic Preservation presented in this book can
be used for an end user and how they could be part of applications and future oper-
ating systems. Therefore, the presented approach realizes the personal preservation
application scenario by connecting to the PoF Framework as well as using compo-
nents presented in this book so far to realize some of its functionality. Most notably,
these are multimedia processing as explained in Chap.3, Memory Buoyancy calcu-
lations and initial preservation policy (Chap.4), contextualization and condensation
(Chap.5), interfacing with preservation workﬂows, functionality, communication,
and the PoF Middleware (Chap.6).
The goal of the pilot is also to provide applications and services which support
needs or desires of users. In the PoF Framework, they are part of the Active System
(see Fig.6.2) to provide beneﬁts for the users and rely on PoF components and
features. In turn, they again contribute to the goals for personal preservation, e.g., by
allowing to collect and organize photos which enable preservation possibilities.
Two such applications will be presented in the following sections: PIMO Photo
Organization and the PIMO Diary.
7.2.5
Organizing Photos with the PIMO
The user survey in Sect.2.5 and [419] showed that users would appreciate support
in organizing their photos. This motivated us to introduce a dedicated application
allowing users to organize their photo collections as part of the personal preservation
scenario. The resulting PIMO Photo Organization app is an example of how the
PIMO and technologies presented in this book can be embedded in user’s daily life
and be combined to provide semantically enriched resources for preservation.

7
Remembering and Forgetting for Personal Preservation
245
The idea for the photo organization in the Semantic Desktop, is to provide users
with an application that could be part of their usual PIM for the family. Because
the application is part of the PoF Framework, evidences can be derived contributing
to personal preservation goals without additional effort required by the user. Such
evidences can be photos marked as favorites, the event a photo belongs to, further
annotations and descriptions of a photo, as well as further material connected to the
collection which could provide even more contextual information on the event or on
the photo itself.
The challenge such an application faces is how to motivate people to use and
contribute information when organizing photos. The approach taken is ﬁrst to provide
a better experience than organizing photos simply by naming folders on the computer
(or even keeping them on the camera).
Photo organization software for home users such as Apple’s Photos already pro-
vides easy organization by events (here, an event is simply a set of photos with a
label, with no connection to or interpretation as a calendar event), geo-location, and
persons (via face detection). We extend this by allowing to annotate with PIMO con-
cepts and resources, (semantically interpreted) texts, condensation of events, etc. The
PIMO introduces the concept of a life situation which can contain photo collections
and allows to differ in its view depending on the speciﬁc type of life situation such
as birth, wedding, or trip.
The application (see Fig.7.5) allows users to organize their photos by providing
the user’s mental model (as represented in the PIMO) as well as details of activities
and events (such as a trip) from other sources in the Semantic Desktop (events,
documents, notes, etc.). Moreover, by using the mental model represented in the
PIMO, we reduce effort for the user for organizing photos. And vice versa, that part
of the mental model which was explicated in the photo collection, can be reused later
in other parts of the Semantic Desktop ecosystem.
Furthermore, annotations added to one photo are aggregated for the photo collec-
tion. Combined they comprise an aggregated description of the collection (as shown
in Fig.7.6). And vice versa, annotations to the photo collection are propagated to the
contained photos. This helps in ﬁnding photos by using annotations of the collection
which were not separately added to a photo by the user. Again, this reduces effort
for contextualizing each and every photo.
The resulting life situation with its photos is embedded in the PIMO, thus enabling
services like preserving, forgetting, reminiscence, cloud storage in the PIMO, and
mobile access, contextualizing and condensation, or associative retrieval of photos
via PIMO’s faceted search.
Pleasenote:Byusingjustthisapplicationforphotoorganizationpurposes,without
all the other Semantic Desktop applications, we are already able to get hold of photo
collections, can enrich these with contextual information, and enable the selection
for preservation by the PoF Middleware. This is in contrast to the Photo Preservation
Application presented in the following Chap.8 which focuses on supporting users
during the selection process for preservation.

246
H. Maus et al.
Fig. 7.5 Photo collection in the PIMO: collection from an Istanbul trip with entities such as persons,
topics, locations
By integrating part of the image services as presented in Chap.3, the Semantic
Desktop ecosystem and especially the use case of photo organization beneﬁts from
three functionalities: image quality assessment, near duplicate detection, and visual
concept detection (see Fig.7.7).

7
Remembering and Forgetting for Personal Preservation
247
Fig. 7.6 Photo collections in PIMO: the entry page shows the favorite images in a thumbnail gallery,
all annotations of this collection, and a descriptive text. This overview serves as a condensed view
(favorite photos and PIMO concepts) of the whole photo collection, thus giving an impression on
the collection w/o requiring to browse through all photos
7.2.6
Contextual Remembering with PIMO Diary
Using the Semantic Desktop regularly leads to a PIMO enriched with plenty of
semantically annotated information, e.g., documents, web pages, emails, calendar
events, etc. Sorting, mentally connecting and abstracting from parts of these things
in order to remember what actually happened in a given period of time is typically a
difﬁcult and time-consuming task.
The PIMO Diary realizes Contextual Remembering by enabling a user to generate
a personal (or group) diary based on these information items from the PIMO. To
support contextual remembering and at the same time to prevent the diary from being
a confusing, large, sequential collection of material, we need to identify semantic
relationships among possibly several thousands of individual information items and
create suitable abstractions from them. If a user, for example, looks back on the last
decade, they should not be overwhelmed with a view showing plenty of individual
events, but compact statements like project names, stages of life, life situations, etc.
Examples for those are terms like “school years”, “studies”, “wedding”, or the name

248
H. Maus et al.
Fig. 7.7 Visual concept detection: detected concepts are automatically annotated as suggested
topics. The detail view also shows the conﬁdence of each concept. Clicking the magnifying glass
next to a concept starts a semantic search for images and photo collections that were also annotated
with it
of a place where a vacation or longer stay abroad has been spent. The user literally
zoomsout ofanoverwhelmingmassofdetails.Ifdesired,theseabstractionscaneasily
be resolved by selecting a subperiod of time for concretization (zooming in), e.g., a
year of a decade or a month of a year. Concretizations (half-years, quarters, months,
weeks, days) can be performed until the user reaches the actual basic material, which
are the concrete information items like notes, photos, documents, etc. Clicking any
of these items directly opens them in the appropriate app, e.g., notes are opened in
the note app.
The system applies a combination of merging and ﬁltering by clustering related
or very similar things to diary entries and evaluating their importance for the user.
The former aspect fosters a high diversity within the diary, making it interesting
and fun to read, whereas the latter aspect is a necessity induced by the fact that the
number of diary entries to be generated is usually limited. It is also compliant to
ForgetIT’s basic idea of forgetting less important, situational short-term information
in favor of more important items selected for long-term preservation. In the example
screenshot (Fig.7.8), we see that there are 1420 information items available for the
selected month. By using 502 of these items, ten diary entries were generated and
displayed. The other 918 items were disregarded in the current diary view, which

7
Remembering and Forgetting for Personal Preservation
249
Fig. 7.8 PIMO Diary showing the year 2012 of a ForgetIT member using DFKI’s PIMO: most
relevant clusters: a project ﬁnished in 2012 (ADiWa) and the proposal preparation of the ForgetIT
project both containing tasks and documents from the user activities at that time
leads to a data coverage of 35%. Nevertheless, diaries covering 100% of the available
data can be generated if desired (in these cases, the feature of sorting out items by
their importance is turned off).
As depicted in Fig.7.8, a typical diary entry consists of a date (or time interval), a
headline, and the most prominent things and keywords gathered from all information
items that were clustered to form this entry. On an entry’s right-hand side there
are its most prominent annotations revealing more of its contextual background.
Additionally, if a photo is associated with the entry (or more precisely: with one or
more of its items), it is displayed on its left-hand side.
In the lower right of Fig.7.8, another feature called the “concept context” (or
“context” for short) is illustrated. It is intended to provide a quick overview of those
things of a user’s life (reﬂected by their PIMO) that concerned them the most in a
given time period. For example, “DFKI GmbH”, the “ForgetIT” project, “PIMO” and
the topic of “context” are among the most prominent things in the depicted sample
year.
Users also have the possibility to incorporate shared data of their family, friends,
or colleagues—represented by a group information model (GIMO)—into their own

250
H. Maus et al.
personal diary turning it into a group diary. As a consequence, a friend’s shared
photo collection may appear as a separate entry in a user’s own diary, or some of
their entries may be complemented by additional information items coming from
other people’s PIMOs.
7.3
Managed Forgetting in the PIMO
Managed Forgetting was introduced in Chap.4. In the following, we address how this
is realized in the PIMO as well as how to restore forgotten resources. The Semantic
Desktop, with its knowledge base, faces problems of information overﬂow if each
and every event, tasks, note, web page, etc. would be kept and treated with the same
importance as currently required information or important ones from the past.
We tackle this problem by following the example of the human forgetting as
presented in Chap.2 which prevents humans from problems caused by too much
information: a substantial amount of information is forgotten after a mere short time;
the forgetting process continues over time, reducing more and more details. If not
relearned, information will be hard to recall or eventually not be available anymore.
Trying to remember will help in retrieving some details, however, other information
might be lost forever.
AnotherpromisingapproachforManagedForgettingistheanalogyofalighthouse
which identiﬁes an area from far away without actually seeing the patch of land.
Getting closer, more and more details get visible. Transferred to our semantic network
in the PIMO, we introduce “lighthouse concepts” which will aggregate a complex,
detailed network of concepts, content, and relations under a single node in the PIMO.
The lighthouse concept represents a cluster of information that could be forgotten
as long as the lighthouse is still there. If getting older, even the lighthouse concepts
will fade away from the user’s direct view.
These views on forgetting are applied in the concept and calculation of the MB.
The Semantic Desktop provides two types of evidences for the calculation: First, user
interaction events with resources such as “create”, “view”, or “modify” are analyzed.
That means, the Semantic Desktop delivers a user action event containing the type
of user action, a timestamp, the active user, and the URI of the respective thing. This
information serves as evidence that the user actually did something with the resource
and is therefore used in the calculation.) Second, the semantic representation of a
resource in the PIMO and its connections to other resources are taken into account.
The latter is used, e.g., for the spreading algorithm in the MB calculation enabling
it to decide where to spread and which weights to apply to other relevant resources
which are connected. This enables the MB to be available for every resource in the
PIMO and also ensures that values are always updated if the user interacts with
resources or as time elapses (such as decay). Figure7.9 shows an internal view of a
MB graph of several resources.

7
Remembering and Forgetting for Personal Preservation
251
Fig. 7.9 A real-life example of a Memory Buoyancy graph from the DFKI PIMO: a ForgetIT work
package task and two subtasks. We see ﬁnal phase of the project, where the task T9.5 was active
and T9.4 was revisited due to deliverables (in December) and ﬁnal review in April
Currently realized forgetting strategies in the PIMO are:
• Hiding: Hiding resources to be forgotten in the user interface (in PIMO5, an
HTML-based user interface for the PIMO): Depending on a threshold (which
is higher on a mobile device than on a desktop computer), resources below the
threshold are not shown immediately to the user but can be retrieved after explicit
requests (e.g., via button “show forgotten”). See Fig.7.11 for an example.
• Moving: Moving ﬁles from the desktop to the cloud and to the archive: desktop
ﬁles to be forgotten can be removed from the ﬁle system and moved to the cloud
(if not already there). The ﬁles are still accessible on demand from the cloud. This
is done via the PIMOCloud Service (available as desktop and server version). A
further escalation strategy is then to move cloud ﬁles to the archive and leaving
only a condensed version (which could simply be the PIMO representation only
containing a reference to the archive) (see Sect.7.3.2).
• Condensing: Condensing several information items to one information object
representing original non-relevant/too detailed/to-be-forgotten items: the PIMO
Diary (see Sect.7.2.6, [183]) uses condensation to generate on demand condensed
representations for activities in the PIMO within a speciﬁc period of time. This
condensed representation is presented to the user. First steps toward identifying
lighthouseconcepts representingawholecluster is introducedbycontext elements.
Links to the contained material are kept. If this material is then forgotten (e.g.,
moved to the archive), only the condensation is kept in the PIMO.
• Synchronizing: Synchronizing material to a device based on a relevancy assess-
ment: the PIMOCloud Service on a device is capable of syncing ﬁles to a device if
their relevancy (i.e., MB) is above a certain threshold (which again can be different
depending on the device type and available space). It is also capable of removing
ﬁles from the device again, if the MB drops below a threshold—meaning it should
be forgotten (on the device).

252
H. Maus et al.
7.3.1
Design Principles for Implementing a Managed
Forgetting Approach in the Semantic Desktop
The MB calculation in the PIMO follows these design principles: Every thing (i.e.,
the semantic representation of a resource or concept) in the PIMO has an individual
MB value for a user.
• The MB value is updated every time the thing is stimulated.
• The strength of a stimulation depends on the user interaction (e.g., viewing, modi-
fying, annotating, etc.), the thing itself, and its connections in the semantic network
of the PIMO.
• MB calculation has to cope with activity bursts as well as erratic accesses.
– MB values are normalized (between 0.0 and 1.0).
– Single access of an item should not directly lead to a MB of 1.0.
– Multiple accesses in quick succession (every minute) are treated reluctantly.
– Over day, multiple accesses will saturate against 1.0.
Realize forgetting by orienting on insights human brain activity on the mental
model as described in Chap.2.
• MB drops for things that are not stimulated (ﬁrst steep decline then a long-tail of
slow decline).
• MB increases for things that are stimulated.
• MB decreases slower for things that are repeatedly stimulated over time (learning
effect).
• Associations to accessed things are stimulated as well.
– Especially “lighthouse concepts” allow an orientation for users in a set of
resources; allow such lighthouse concepts, let their MB inherit stimulations
of connected things.
– We took an iterative approach: use the expected ones from the PIMO and further
domain ontologies (such as projects or life situations).
– Start to identify such concepts in the PIMO from user evidences (see, e.g., the
PIMO Diary).
Apply forgetting reasonably and remember important stuff in context to keep the
user’s trust—ensure MB actually helps rather than merely imitates human behavior
(mimicking “human forgetting as failure” as well as “remembering it all” is no help).
• Apply rules and heuristics to deal with requirements of various domains. Utilize
heuristics derived from PIM to predict individual MB such as
– upcoming events should stimulate connected things (such as locations, persons,
material, topics),
– ﬁnished items shall decrease faster (tasks, events) unless other indicators speak
against this, or

7
Remembering and Forgetting for Personal Preservation
253
– times with low user interaction (thus sparse or no stimulations such as weekends
or holidays) should not lead to massive decay in MB.
• Applications shall use MB including various thresholds to the requirements of the
individual use case.
– For instance, a 3rd-Party calendar which is not capable of dealing with MB
should have access to all the events otherwise users might lose trust in interacting
with the PIMO. This requires that each thing has its own MB and PIMO Server
APIs allowing to set individual MB thresholds.
Utilize the information from the PIMO’s semantic network. First, we make use
of the type of a concept as short- and long-term importance of information items
also can be derived from their type. For instance, emails are faster forgotten than
persons. Therefore, individual decay curves are used for classes in the PIMO and
domain ontologies. Furthermore, as with humans, mentioning things also stimulate
associations in their semantic neighborhood:
• As a basis, spreading activation on the semantic network of the PIMO is used for
MB calculation.
• Heuristics for the type and number of relations connecting things inﬂuence the
spreading algorithm.
We considered also technical requirements as forgetting is just one among many
dimensions in the PIMO. Over time, the PIMO can amount to a huge data collection
but still needs a quick response time for MB values. As it is used in daily work, a
mere research approach is not feasible. Therefore, the following design decisions to
embed MB calculation in the current PIMO Server version were used:
• MB is time-dependent which poses a challenge for calculation as MB values
change permanently just as time passes by (i.e., decay).
– Conceptually splitting MB value into two parts: a static (time-independent) and
a dynamic (time-dependent) part.
– Database stores most recently calculated MB value and calculation timestamp.
– Decay happens at retrieval time: value stored in database is “decayed” according
to time difference between retrieval time and last calculation time.
• MB values must always be up-to-date.
– Use an online calculation (instead of ofﬂine-like overnight calculation).
– Accessing PIMO things must trigger recalculation of MB involving their seman-
tic neighborhood to also identify hot regions.
• Keep high performance despite additional MB calculation.
– Requesting MB values is done very often, therefore, retrieval must be fast (pro-
cessing high-volume access event streams is not feasible at runtime).

254
H. Maus et al.
– Minimize database updates: updating MB values for all things in the PIMO is
not feasible.
– Incremental updates: only recalculate the most minimal set of PIMO things.
These principles are applied in the MB calculation in the Semantic Desktop. In the
following sections, two exemplary forgetting services implemented in the Semantic
Desktop are presented.
7.3.2
Managed Forgetting in the File System
With the help of the Semantic Desktop ecosystem being embedded in a computer’s
ﬁle system, the forgetting functionality is available on the user’s desktop ﬁle system.
It is now possible that ﬁles are removed from the computer if the PoF Framework
decides to do so (based on low MB). Depending on the user’s chosen policy, these
ﬁles are either suggested as candidates to be forgotten (awaiting conﬁrmation, see
Fig.7.10) or they are removed automatically in the background.
The Semantic Desktop checks at regular time intervals if there exist local ﬁles
whose MB dropped under a certain threshold which would indicate that these should
be forgotten. This interval can be individually conﬁgured. Reasonable time frames
Fig. 7.10 Forgetting of ﬁles: the PoF Framework has decided to forget several ﬁles on the user’s
computer. They are presented in a dialog window that also allows modiﬁcations to the list before
the forgetting is ﬁnally executed

7
Remembering and Forgetting for Personal Preservation
255
range from a daily basis—although no quick changes are to be expected—to a week.
Identiﬁed ﬁles are proposed to the user to be forgotten, i.e., removed from the com-
puter.
The current implementation uses a preservation policy which removes the ﬁle
from the computer (after user conﬁrmation, see Fig.7.10), but leaves the ﬁle in the
PIMOCloud untouched. The preservation decision is left to the PoF Middleware.
That means that details pertaining to what happens in the case of removal from the
computer w.r.t. preservation will be clariﬁed in a preservation policy. For instance,
the ﬁle in the PIMOCloud may ﬁrst be preserved and then deleted in the PIMOCloud,
or the ﬁle will be preserved and kept in the PIMOCloud until the MB drops below
an even lower threshold. As long as the cloud ﬁle is available, it can be restored on
any computer or accessed on a mobile device. If the cloud ﬁle is also removed, only
restoring it from the archive will be possible (if actually preserved there).
This strategy keeps high- and medium-buoyant resources on the desktop and
gradually removes resources with low MB from the desktop.
Although the ﬁles are forgotten, their semantic representation (i.e., the thing) in
the PIMO is still there. However, due to the low MB it will not show up in normal
browsing (i.e., the user will not stumble upon it easily; see also Sect.7.3.3). At what
time such things shall be removed from the PIMO is subject to the user’s policy of
trashing things as well as condensing things into a landmark.
Therefore, users are also able to ﬁnd, inspect, and restore once forgotten ﬁles. By
using PIMO’s associative search, forgotten things can be retrieved. Users may then
inspect them and ﬁnally decide to restore them on their local computer.
7.3.3
Managed Forgetting in the User Interface
A further beneﬁt of having MB calculations is the possibility to actively hide things
from the user when browsing the PIMO if the MB is low. Not showing each and
every resource in the PIMO, but only those of current and medium-term relevancy
(as expressed by a high MB) prevents the users from an information overload.
The PIMO5 user interface applies hiding of information if the MB of a thing is
below a certain threshold (during browsing and during search). This threshold is set
differently whether users browse on a desktop computer or a smartphone and can
also be adjusted by them.
Figure7.11 shows an example of a thing’s buoyancy over a period of time: whereas
in the beginning all connected things are listed directly, after time progresses, things
with lower MB are hidden from immediate view. Forgotten things are shown only if
the user issues an explicit request (“show forgotten”).

256
H. Maus et al.
Fig. 7.11 Hiding forgotten things: an example of viewing a workshop in the PIMO at different
times: during the workshop (left), after eight months (middle) and after two years (right). Things
are hidden when falling below the MB threshold (which is at 0.5 as we see on the right image).
Here, documents, emails, and tasks are hidden, while still active things such as project members
and the project itself are kept, which results in a condensed view for the user
7.4
Supporting Preservation with the Semantic Desktop
The following sections detail how preservation is supported in the Semantic Desktop.
First, Sect.7.4.1 details how the realization of the Preservation Value (PV) Assess-
ment in the Semantic Desktop allows to embed a detailed and personalized Preser-
vation Strategy into the Semantic Desktop. This Preservation Strategy is based on
indicators from the Semantic Desktop and the PIMO speciﬁed as policies and rules.
These policies and rules are classiﬁed along the dimensions for assessment identi-
ﬁed by in Sect.4.3.2. Users can use predeﬁned Preservation Strategies as well as
customize their own. This is embedded in a user interface to deﬁne the Preserva-
tion Service Contract with a service provider including Preservation Levels which is

7
Remembering and Forgetting for Personal Preservation
257
presented in Sect.7.4.2. The components and examples shown here are taken from the
Personal Preservation pilot implemented in the ForgetIT project using the Semantic
Desktop ecosystem and the DFKI’s PIMO.
7.4.1
Preservation Value Assessment in the Semantic Desktop
Assessing a PV was introduced in Sect.4.3. In the following, we detail how the
Semantic Desktop realizes such a PV Assessment of resources. This will allow for
Synergetic Preservation by the PoF Middleware connected to the Semantic Desktop.
In the PoF Framework this is done in the functional entity called “Content Value
Assessment” (see also the Preservation Preparation Workﬂow (see Fig.6.5) in the
PoF Reference Model). This entity assesses resources w.r.t. MB and PV. In this
Section, we focus on the PV which reﬂects the long-term importance or relevance
of a resource as deﬁned in Sect.4.3. This PV is then used by the PoF Middleware
as a basis for making preservation decisions, e.g., if a resource should be preserved
or how much should be invested in its preservation, i.e., which “Preservation Level”
should be used (see Sect.7.4.2.1).
To allow an assessment of resources, ﬁrst, it must be clariﬁed what aspects in the
assessment could possibly contribute to the long-term importance and then consider
individual preferences of the user in a so-called “Preservation Strategy” of the PoF
Reference Model as introduced in Sect.6.6.3.
The following sections explain how a personal Preservation Strategy is supported.
To ease the usability for the user, such a Preservation Strategy can be deﬁned along
speciﬁc dimensions for assessment which will be detailed in Sect.7.4.1.1.
The approach for personal preservation allows to deﬁne Preservation Strategies
in two ways: presets based on dedicated user proﬁles (Sect.7.4.1.2) or a combination
of presets and a more detailed customization using policies and rules (Sect.7.4.1.3).
7.4.1.1
Preservation Strategy
The Preservation Strategy will be based on a set of policies and rules which support
an assessment of the PV. For this assessment six dimensions where identiﬁed, which
are—from the point of view of the Semantic Desktop—relevant for the Personal
Preservation Scenario. As identiﬁed in Sect.4.3.2 these are investment, gravity, social
graph, popularity, coverage, and quality.
Thesearethedimensionsidentiﬁedforcollectingevidencesfortheassessmentand
ﬁnally the calculation of the PV. Which evidences are used and how they contribute
(weighting) to the calculation of the PV is deﬁned by a Preservation Strategy.
The following sections detail the two realized approaches.

258
H. Maus et al.
7.4.1.2
Personal Preservation Strategy Based on Personas
To help users who are new to a personal preservation service, it would be useful to
have a predeﬁned set of policies and rules deﬁned for the six dimensions mentioned
above, so that they do not need to care about deﬁning a rather complex Preservation
Strategy.
The survey conducted on personal preservation of photos (see Sect.2.5) iden-
tiﬁed four personas representing attitudes toward personal Preservation Strategies
(reported in [419]). The personas are deﬁned along two key preservation dimensions
of “Loss”—the user is worried about losing important photos—and “Generations”—
importance to the user of preserving important photos for future generations.
For each dimension, two habits could be clustered. Regarding “Loss”, a group took
less precautions against losing their photos (therefore named “Forget”) than the
“Safety in Redundancy” group which tended to have redundant copies of their photos
on different storage units. Regarding “Generations”, one group put much effort in
organizing their photos (therefore named “Curators”), e.g., by assigning keywords.
In contrast, the “Filing First” group almost exclusively relies on ﬁles and folders.
Using these dimensions, the four personas identiﬁed are “Safe Curator”, “Safe Filer”,
“File and Forget”, and “File and Forget Curator”.
To achieve the aforementioned ease of use, we deﬁned one Preservation Strategy
for each persona. We assigned different weights to policies and rules to match the
persona’s preferences and assumed behavior. The different personas tend to score
differently on the six dimensions of PV Assessment presented in Sect.4.3.2:
• Investment: The Curator’s investment is on purpose, thus evidences for investment
in this group should be ranked higher, whereas in the File and Forget group, the
investment might be sporadic.
• Gravity: This is assumed to be more helpful to Safe Filers as Curators are supposed
to decide on importance on their own.
• Social Graph: The relevance of content to social relations is important for all
personas. Social aspects are important for every persona.
• Popularity: Filers are purposeful in accessing resources valuable to them and
ratings might be rare. In contrast, Curators are expected to access more resources
especially for curation purposes which does not directly indicate popularity of the
resource, thus this evidence should be weighted less.
• Coverage: For both Curators and Safe Filers coverage is important; to the Curator
more than to the Safe Filer.
• Quality: For Filers the quality (expressed by image quality) is more important
whereas for Curators the automatically calculated quality assessment is not that
important as they will rely on their manual assessment.
To reﬂect these persona perspectives in the PV Assessment, corresponding proﬁles
were created for the PV calculation (which can be set in the Semantic Desktop as
shown in Fig.7.12). The proﬁles mirror the basic distinction between “Curators” and
“Filers” by mainly basing the preservation suggestions on the investment spent by
the former, and an emphasis on popularity and material quality by the latter. For the

7
Remembering and Forgetting for Personal Preservation
259
Fig. 7.12 Setting a more detailed Preservation Strategy in PIMO5: allowing to easily select/deselect
those policies and rules which matter the most to the user. Selecting a persona loads a predeﬁned
set of choices. Changes are possible and saved as a personal Preservation Strategy
safety-conscious persona proﬁles, the algorithm tries to ensure a certain coverage of
the different subsets (e.g., photo collections) of the material to be preserved.
The next section will describe more comprehensive strategies for including more
resources as well as more complex rules.
7.4.1.3
Customization for Preservation Strategies
In the previous section, the Preservation Strategy has been based on personas deﬁning
strategies in which the user is not able to inﬂuence details but can only change
the persona itself. To allow a more ﬁne-grained Preservation Strategy setting for
the Semantic Desktop along the preservation policy and rules, the following more
detailed strategy setting has been realized as an extension of the aforementioned
persona-only approach.

260
H. Maus et al.
For theapplicationscenarioof PIM, several evidences or indicators wereidentiﬁed
in the Semantic Desktop along the aforementioned six dimensions which could help
in the assessment of the expected long-term beneﬁt of a resource. Now, the Semantic
Desktop allows to adapt these to one’s preferences without requiring the user to
be an expert in policy or rule management. Figure7.12 shows the PIMO5 interface
allowing to set and change the Preservation Strategy applied to a user’s resources as
introduced above. Here, a more detailed choice of indicators (e.g., allowing further
evidences from the Semantic Desktop usage as policies or rules) to be used for
calculating the PV of the user’s resources are shown. Each indicator is described
in human readable terms explaining the consequences if applied. Each indicator is
internally used in a calculation or a rule which can be switched on or off.
Users are able to choose a persona which describes them the best (or it rather
could be inferred by preference questions during conﬁguration time and is then set
automatically). Selecting a persona will load a default proﬁle and set the respective
check mark to those indicators which best ﬁt the respective persona (including a
predeﬁned weighting). Once changed and saved, the settings are used for calculating
the PV for each of the user’s resources. This is done periodically in the Semantic
Desktop. This customized Preservation Strategy is part of a “Preservation Service
Contract” which is explained in the following Section.
7.4.1.4
Investigating the Results of the Chosen Preservation Strategy
Once a user has set the Preservation Strategy and pressed the “Save setting and
recalculate” button (as shown in Fig.7.12), a new PV calculation is started using this
new strategy. The “Show preservation overview” button provides the user with an
overview of the current material from the PIMO which would be preserved (if the
PoF Framework decides to start preserving) according to the selected Preservation
Strategy. This view shows all material from the PIMO such as documents, concepts,
projects, or persons.
For easier inspection, the view lists those images which would be preserved under
their respective photo collection (in the PIMO the class Life Situation). Currently, the
list starts with an overview of the resources with the highest PVs ﬁrst and allowing
the user to request more items as shown in Fig.7.13. Each thing has a so-called badge
indicating the PV Category. Clicking on a thing’s badge shows an explanation for
the PV decision as shown in Figs.7.14 and 7.15.
7.4.1.5
Preservation Embedded in the File System
The previous sections have shown how the Semantic Desktop is connected to the PoF
Framework and can start preservation activity automatically after certain time frames.
To bring preservation even closer to users, the Semantic Desktop ecosystem allows
to preserve single items at any time by triggering PoF Framework’s preservation
workﬂow on manual request.

7
Remembering and Forgetting for Personal Preservation
261
Fig. 7.13 Preservation overview: a dedicated preservation view has been integrated in PIMO5 to
check the Preservation Strategy setting. It allows to browse through all resources and concepts to be
preserved. The screenshot shows an individual (and partly anonymized) view on the DFKI PIMO
with a safe curator proﬁle
As a use case, the Semantic Desktop’s plug-in for the Microsoft Windows File
Explorer has been extended to enable users to manually preserve resources. To do
this, the user could choose a ﬁle through the ﬁle explorer, as shown in Fig.7.16, and
select “Preserve” in the ﬁle’s context menu. Preservation of the ﬁle is issued without
requiring any further interaction by the user with the PoF Middleware. Technically,
the manual triggering of “Preserve” on one ﬁle sets its PV to maximum. Depending
on the preservation policy, the PoF will then decide on the schedule to preserve.

262
H. Maus et al.
(a) Scrolling down all the photos . . .
(b) . ..to the media items.
Fig. 7.14 Preservation overview: choosing the File and Forget Preservation Strategy generates this
list which contains mainly photos as media items
7.4.2
Preservation Service Contract
In addition to the Preservation Strategy, another detail to be considered for per-
sonal preservation is which service infrastructure is used for preservation. In other
words, who provides the actual service for preservation to the user and what are
the conditions? Preservation could be offered as a service by a provider using an
instance of the PoF Framework (for the services) and a subset of the Semantic Desk-
top ecosystem—most prominently the PIMO Photo Organization—as applications
for customers. Assuming such a service is established, a contract between the user
and the company would state the conditions of the service and its costs. Moreover,

7
Remembering and Forgetting for Personal Preservation
263
Fig. 7.15 A short explanation of the decision for PV using the Preservation Strategy settings
Fig. 7.16 Preserving embedded in the MS windows Explorer context menu of a ﬁle: issuing manual
preservation

264
H. Maus et al.
Fig. 7.17 Preservation
Service Contract: it can be
set in the PIMO5 options.
Here with an exemplary
choice of a provider
such a contract would also detail how the service would be used, e.g., the amount
of space to be consumed, preservation levels, how formats are migrated, etc. As a
proof-of-concept for such a contract, PIMO5 has been extended with basic options
for selecting and changing the conditions of a user’s service contract with a preser-
vation service provider. This is shown in Fig.7.17 under the tab “Contract”. The tab
“Strategy” then leads to the Preservation Strategy settings as discussed above. In the
following, the details of this basic service contract preference selection are detailed.
7.4.2.1
Preservation Level Package
With regards to preservation, a huge amount of options and parameters have to be
decided upon. But complex decisions on preservation parameters might discourage
normal customers to choose a preservation service. Considering the business of a
telecom provider, for example, pre-conﬁgured contracts are usually offered contain-
ing several bundles (ﬂat rates for text messages, landline, broadband, mobile data,
and options for roaming) making it (more or less) easier for customers to decide on
suitable contracts.
As the goal is to reduce the effort for Personal Preservation, offering such bundles
for preservation options as in telecom contracts makes sense for the average customer.
By additionally providing more detailed options, this still allows advanced users to
adapt their contract to their personal preferences. Finally, the bundling of preservation

7
Remembering and Forgetting for Personal Preservation
265
Fig. 7.18 Preservation Level
Packages: in PIMO5 the user
is able to select Preservation
Level Packages for each
Preservation Value Category
services with normal telecom contracts holds the potential of bringing preservation
to a larger audience.
Offering a preservation service with bundles of preservation functionalities and
options as packages would allow customers to choose for the respective Preservation
Value Category (gold, silver, bronze), between several preservation levels (offered
as “Preservation Level Package”) as well as a “do not preserve” for a category as
shown in Fig.7.18. Such a package contains a reasonable default set of preservation
functionalities and options for a certain price. These packages could range from a
basic package—which could be free with a mobile contract—up to packages with
extended functionalities or security with additional costs. Furthermore, some options
might be offered to the customer such as allowing storage worldwide (as in the
package “Premium Preservation WorldWide”) or limit storage to certain countries.
Furthermore, we also assume that for a contract bundle, the PV Categories come
with a preset for the Preservation Levels as well as for the Preservation Strategy to
accomplish an uncomplicated contract activation.
7.4.2.2
Alternative Contact Person
To cope with the case that a user is deceased, a contract has to provide possibilities to
identify alternative contacts which are then able to take over the archive. Therefore,
the preservation contract tab allows to enter a contact from the PIMO which shall
be contacted in the case a user passed away. Figure7.17 shows the section where a
contact can be selected via a dropdown box listing all persons in the PIMO. This
contact is actually a pimo:Person which can either be imported from an address
book together with address details or simply created in the Semantic Desktop. This
is realized as a proof-of-concept. It could be extended with more persons, with
priorities,situations(death,unabletoact,etc.)inwhichtocontact,withacommitment
of the selected user, e.g., by exchanging emails, etc.

266
H. Maus et al.
7.4.2.3
Preservation Broker Contract
The terms of this Preservation Service Contract are transferred in a so-called “Preser-
vation Broker Contract” to the PoF Middleware. This is done via the SD/PoF Adapter
(in the PoF Framework, this adapter is part of the Active System and communicates
with the PoF; see Figs.6.2 and 6.13). The contract is used in the “Managed Forgetting
and Appraisal” step (see Fig.6.5) to decide on the preservation actions.
7.4.3
Preservation Preparation Workﬂow
This section details the connection of the Semantic Desktop as an Active System with
the PoF Middleware to enable preservation by relying upon the user’s Preservation
Strategy and Preservation Value explained in the previous sections. This section
describes the preservation steps along the PoF Framework workﬂow “Preservation
Preparation Workﬂow” as deﬁned in the PoF Functional Model in Sect.6.3.
The workﬂow is depicted in Fig.7.19 with its steps and functional entities involved
in these steps (which are explained in the Functional Model; see Fig.6.2). The addi-
tional numbers in the Figure are aligned with the following subsection numbers to
explain the steps with involved functional entities.
7.4.3.1
Content Value Assessment
Before the workﬂow starts, the functional entity “Content Value Assessment” (CVA)
is responsible for the assessment of resources in the PoF Framework. As already
pointed out in Sect.7.4.1.1 this step provides the Preservation Value of a resource.
Considering the role of the functional entity CVA in the PoF Framework, the
Semantic Desktop as Active System is an example of the situation where the Active
System is capable of providing the PV for the preservation decision (as well as the
Fig. 7.19 The Preservation Preparation Workﬂow (adapted from Fig.6.5)

7
Remembering and Forgetting for Personal Preservation
267
MB for Managed Forgetting in the Active System) and thus, the functional entity
CVA is part of the Active System.
In contrast to that, the implementation chosen for the application scenario using
Typo3 in Sect.6.6.1 is an example of the situation where the Active System does not
calculate the values by itself but rather delivers the evidences to the PoF Framework
where the computation will take place.
This design decision was made because of the beneﬁcial usage of both MB and
PV in the Semantic Desktop ecosystem. The rich semantic model of the PIMO and
the usage statistics of the Semantic Desktop allow for a comprehensive view on the
resources wrt. MB and PV. Furthermore, the nature of the PIM application scenario
implies a lot of access, usage, and changes to resources and the PIMO resulting
in a lot of trafﬁc as well as content assessment in the PIMO as a knowledge base.
Therefore, both values are computed in the Semantic Desktop and stored directly in
the PIMO to be easily accessed by its components and thus, making them an integral
part of the PIMO.
Therefore, to enable the PoF Middleware to make decisions based on the PV in
the “Select” step, the values are reported and updated in certain time intervals to
the PoF Middleware by the SD/PoF Adapter (see Fig.6.13). The update contains the
resource’s URI, its Preservation Value, and last modiﬁcation date of the resource.
Adding the last modiﬁcation date allows the PoF Middleware to decide if the resource
might need to be sent to the archive again if the resource changed since the last
preservation.
7.4.3.2
Select
The “Select” step uses the functional entity “Managed Forgetting and Appraisal” to
make conscious decisions about preservation of resources of the Active System. To
accomplish this, the results of the “Content Value Assessment” are used for deciding
about preservation actions.
The “Forgettor” component (see Fig.6.13) selects the set of resources to be pre-
served based on the selected Preservation Value Categories set in the user’s Preser-
vation Strategy. This information is part of the “Preservation Broker Contract” intro-
duced in previous Sect.7.4.2, set in the Preservation Service Contract, communicated
to the PoF Middleware and managed there for each user.
7.4.3.3
Provide
The step “Provide” uses the functional entity “De-Contextualization” to extract a
resource from its Active System context in preparation of packaging it for archiving.
The PoF Middleware retrieves resources via the “Collector” using the CMIS [312]
(see also Sect.6.6.1) interface embedded in the SD/PoF Adapter. For the PIMO, this
means that a thing and its grounding occurrence (i.e., the semantic representation
and the actual physical ﬁle) is separated: the CMIS interface hands over the resource

268
H. Maus et al.
to be preserved as a cmis:Item and the PIMO’s model information about its thing
will be part of the context information handed over in the forgetit:context
attribute of the cmis:Item. This attribute is then available for the modules in the
PoF Middleware, especially the “Contextualizer” in the next step.
Technically, the context information export is an excerpt from the PIMO semantic
graph describing the resource in the PIMO and its connection to other things such
as topics for a document or persons attending an event. The format used for the
exported excerpt is RDF/S using the PIMO Ontology RDF Schema and Turtle1 (the
“Terse RDF Triple Language” is a compact textual syntax for representing RDF) as
exchange format.
Furthermore, this interface was enhanced by handling collections of resources
and using the additional context delivered by the SD/PoF Adapter. Now every con-
cept in the PIMO can be preserved separately, i.e., the handling was extended to all
PIMO classes not only those representing (ﬁle) resources such as pimo:Media
and pimo:LifeSituation. Now, it is also possible to preserve a, e.g.,
pimo:Project such as “ForgetIT” in the DFKI PIMO, although it might not
have a physical ﬁle attached.
7.4.3.4
Enrich
In the “Enrich” step the functional entity “Contextualization” shall provide additional
information for the content to be preserved in order to allow archived items to be
fully and correctly interpreted at some future date (see Sect.6.4.2). All resources in
the submitted collection are handed over to the “Contextualizer” which runs three
different components:
First, the world knowledge contextualization—as proposed in Chap.5—
processes each textual resource in the submitted collection. This component cre-
ates a “World Context” by applying an entity recognition to the text of a resource
(e.g., a document or email) using DBPedia2 as source to disambiguate entities. Each
entity found in the text is added as semantic annotation (i.e., as URI) to the World
Context. This World Context is then stored as additional context information to the
metadata of the respective resource.
Second, the visual concept detection—as proposed in Sect.5.2.2 for contextual-
izing images—adds visual concepts detected in images as additional context. Third,
the personal knowledge contextualization takes the context information provided
in the previous step by the Semantic Desktop in the forgetit:context attribute
as separate context. In terms of the PoF Reference Model, this context information
generated from personal knowledge represented in the PIMO is stored in the so-
called “Local Context”. The context information delivered by the Semantic Desktop
satisﬁes the following context dimensions:
1http://www.w3.org/TR/turtle/.
2https://dbpedia.org.

7
Remembering and Forgetting for Personal Preservation
269
• Time: The excerpt contains the thing’s creation time, last modiﬁcation, and in
addition any time information associated with certain things such as events (either
a point in time or a time period).
• Location: If locations are associated with a resource, e.g., usually for events or
also for the subclass pimo:LifeSituation.
• Topic: Any kind of topics identiﬁed for the resource. For the PIMO, this includes
manually annotated topics (e.g., with the property pimo:hasTopic), suggested
ones with pimo:hasSuggestedTopic (e.g., suggested by the entity recogni-
tion using GATE in FireTag or SemanticFileExplorer, or using the ForgetIT image
services), or inferred concepts with pimo:hasInferredTopic (e.g., because
the photo collection as a whole has this topic, or the super task was annotated with
it).
• Entity Space: This is comprised by the remaining relations a resource has
in the PIMO not already covered above. These are various properties such as
rdfs:partOf or pimo:isFundedBy.
• Document Space: This consists of other documents or subclasses such as web
pages or emails related to the resource.
7.4.3.5
Package
In this step the functional entity “Archiver” creates from the resource(s) collected
from the Semantic Desktop the content and metadata to create a “Submission Infor-
mation Package” (SIP). This is then handed over to the “Transfer” step.
7.4.3.6
Transfer
The “Transfer” step then submits the SIP to the “Digital Preservation System”
(DPS)(see Fig.6.13) which stores it as an “Archival Information Package” (AIP).
In the case of the pilot implementation on the ForgetIT testbed, the DPS is composed
of DSpace and Openstack Swift. For the DFKI PIMO pilot it is DSpace only (see
Fig.7.20).
7.4.3.7
Preservation Finished
Once the preservation is ﬁnished, the PoF Middleware notiﬁes the Active System
of the outcome. Notifying the user of the outcome is twofold: ﬁrst, the user gets
a notiﬁcation once a collection has been preserved in the PIMO5 home screen as
shown in Fig.7.21. Second, several places in the Semantic Desktop show if a thing
is preserved such as in the thing view in Fig.7.22 or 7.23.

270
H. Maus et al.
Fig. 7.20 Preserved document in DSpace: the ForgetIT proposal preserved from the DFKI PIMO
7.5
Toward Using Personal Information Management to
Support Personal Preservation
The presented approach for Personal Preservation has three phases:
• In the Preparation phase the tasks are connecting to a service provider, choosing
a preservation contract, and installing the Semantic Desktop ecosystem.

7
Remembering and Forgetting for Personal Preservation
271
Fig. 7.21 Notiﬁcation of the successful preservation on the PIMO5 home screen
Fig. 7.22 Showing the preservation state of a photo in the PIMO5 image detail view

272
H. Maus et al.
Fig. 7.23 Showing the preservation state of a photo collection
• In the Usage phase, the Semantic Desktop and connected applications are used
for PIM activities. In this phase, resources are continuously preserved (usually
without user intervention, but manual preservation is also possible).
• Finally, if the need arises, Access to Preserved Material is granted either directly
via the Semantic Desktop ecosystem or by the preservation service provider alone.
7.5.1
Overcoming the Obstacles of Personal Preservation
In Sect.7.1, we assessed the current state of Personal Preservation and identiﬁed ﬁve
main obstacles which should be addressed in the Personal Preservation approach.
In the following, those obstacles are revisited (and stated in the beginning of the
respective section) and we discuss how these obstacles are now overcome by the
presented solution.
7.5.1.1
Awareness
Users are not aware of personal preservation of digital content. There is a huge gap
between current practices, such as backup by copying material to a different hard
disk, and a proper Preservation Strategy.
The Personal Preservation Pilot as well as the application in Chap.8 show how
a preservation service can be realized by using the PoF Framework. Although both
use the PoF Middleware, the respective approach is different.
First, the photo preservation application of Chap.8 is a single application extended
with preservation abilities by connecting to the PoF Middleware. Thus, it shows how
“normal” applications can be extended with a preservation functionality and beneﬁt
from services provided by the PoF Middleware.
Second, the Personal Preservation Pilot approach shows how a complete infras-
tructure such as the Semantic Desktop as well as various applications can be extended
with preservation functionality and by their usage, provide valuable resources and
context for preservation. Moreover, they show how an application ecosystem can con-
tribute to realize Synergetic Preservation allows to asses the beneﬁts of embedding
preservation in users’ PIM.

7
Remembering and Forgetting for Personal Preservation
273
After choosing a Preservation Strategy, the Synergetic Preservation is started with
no additional effort for the user apart from occasional checks if the resource selection
is sufﬁcient (which would be optimized for product-like services). Furthermore,
although setting up a preservation service is easy, the prerequisite of using a Semantic
Desktop-like infrastructure still holds. Semantic Desktop applications such as the
SemanticFileExplorer or the PIMO Photo Organization app, show that user resources
can be covered to some extent with less effort and could be realized as stand-alone
applications.
Nevertheless, the Semantic Desktop ecosystem is also a step toward embedding
Synergetic Preservation into an operating system. Helpful capabilities such as PIM,
photo collections, or tagging, are already prominently introduced, e.g., in Apple’s
macOS operating system.
7.5.1.2
High Up-Front Costs
When starting with personal preservation, the user faces high up-front costs in terms
of time, effort, and resources, and there are very few tools to help users prepare
material for preservation and interact with an archiving service. The best-practice
recommended by Marshall is not supported comprehensively.
The high up-front costs in making personal resources suitable for preservation are
reduced by embedding the required functionality for preservation into user’s daily
PIM activities or at least providing applications supporting selection and connection
to a preservation service as shown in Chap.8. The evaluation with the PIMO Photo
Organization app showed that early contextualization is well accepted by users if the
beneﬁts are immediate (such as the textual explanation of the context of a photo or
automated search capabilities). Furthermore, offering a personal preservation service
is a potential business model for, e.g., a telecom provider as additional service for
their customers. If that would be accomplished, the high up-front costs would be
reduced for ﬁnding and connecting to a preservation service for end users.
Concerning the availability of tools, the two application scenarios show that
extending an Active System with preservation capabilities is possible and the con-
nection to the PoF Framework by implementing the interfaces is manageable. Thus,
both possibilities—embedding preservation services in an infrastructure (such as the
Semantic Desktop) or extending a single application with preservation capabilities—
are now open to enhance tools with preservation functionality which could foster the
availability in future.
7.5.1.3
No Personal Preservation Services
Thereisnopersonalpreservationserviceforthemajorityofenduserswhichsupports
the whole preservation process. Cloud storage alone is not preservation.

274
H. Maus et al.
As mentioned before, services for personal preservation could be offered by a
service provider. The business domain of a telecom provider nowadays covers lots
of activities (e.g., mobile internet and access at home) and resources of users (e.g.,
photos, emails, or ﬁles on a cloud storage) with potential to extend it even further
(calendar, services for friends and family). In this light, extending their services with
preservation capability ﬁts well in the existing business models.
The digitalisation of various daily activities (e.g., calendar, chat, photos) includ-
ing resources held in internet platforms such as social media, lead to challenges of
how to access, inherit, and preserve the material after a user’s death. Recent lawsuits
concerning access to material in internet platforms (such as material in Facebook or
resources connected to the Apple account) shed a light on the increasing need for
services embracing these sources. However, this will include legal issues to be cov-
ered when extending the preservation service. Furthermore, we can predict that the
public awareness will increase for such services also including all digital resources.
This would be a good market entry for a personal preservation service.
Finally, apart from telecom providers, with the PoF Framework various ﬂavors of
Digital Preservation Systems can be supported. Therefore, we see the possibility also
for a take-up by specialized service providers offering PoF Middleware as a service.
7.5.1.4
Increasing Amount of Digital Content
The vast increase in digital content with relevance to a person’s life poses challenges
to PIM as well as preservation.
The variety of applications in the Semantic Desktop ecosystem and its openness
to integrate further applications, plug-ins, and information sources allows to include
various digital content in the user’s PIM. The solution presented in this chapter has
shown that applications could support users in handling large amounts of digital data.
This is done with the help of the technologies for analyzing digital media presented in
the ﬁrst part of the book. For instance, visual concept detection allows to search large
photo collections without any effort for manual annotation. The research presented
in Chaps.3 and 5 show further technologies to manage large media content such as
photo and video analysis.
7.5.1.5
Understanding for Future Generations
Designing and organizing an archive so that its structure can be understood in
several decades from now is cognitively challenging for users.
The PIMO—as a semantic representation of the mental model of a user—allows
to structure existing digital content of the user’s PIM activities. Further extensions
such as the life situations (“LifeSituation”, an advanced photo collection) can cover
more specialized situations and involved material.

7
Remembering and Forgetting for Personal Preservation
275
Applications in the Semantic Desktop ecosystem take care of structuring this
information for the user (using the PIMO) without requiring them to model this
by themselves. The PIMO Photo Organization app is an example for this approach.
Users organize their photo collections, add texts and maybe also annotations. The app
stores this in a machine understandable semantic structure which covers the photo
collections. It includes information from the user as well as extracted (e.g., EXIF
data) and enriched metadata from components such as visual concept detection.
These structures allow a machine understandable access to the digital content as
well as a human understandable presentation. They are used in the Semantic Desk-
top for providing added value such as semantic search or reusing content for other
services such as the PIMO Diary. Moreover, the combination of personal knowledge
in the PIMO, early contextualization with PIMO services as well as world knowl-
edge (e.g., entity recognition and annotation in texts using PIMO as well as external
knowledge sources such as DBpedia in the semantic text editor) provides an even
richer context for the digital content.
This rich semantic structure as well as the enriched digital content builds the
basis for an understanding of the structure for future generations. The structure is
transferred to the PoF Middleware in the “Local Context” for each and every resource
covering semantic information of the resource itself as well as relations to other
resources and concepts. Technically, the PIMO is represented using RDF (Resource
Description Framework), standardized by the W3C. The “Local Context” handed
over to the PoF Middleware is a formalized textual representation. This ensures
readability and accessibility for long-term preservation.
Finally, the PoF Framework retrieves the content and adds its own contextualiza-
tion for interpretation by future generations.
To summarize, the Semantic Desktop approach and the PIMO are the means to
reduce the effort for users to generate structures. The combination of the “Local
Context”—generated from the semantic information in the PIMO—and the addi-
tional contextualization in the PoF Middleware provides the means for the under-
standing of the content for future generations.
7.6
Conclusions
This chapter presented how PIM activities can be used to derive context to realize
Personal Preservation and thus reduce the manual effort for users. This context is
enriched with information derived from the user’s mental model during usage of
the resources by using semantic technologies and the PIMO. A pilot for Personal
Preservation built on top of the Semantic Desktop ecosystem showed the potential of
embedding semantic technologies in the operating system as well as realizing user
applications for preservation, forgetting, and remembering in context.
The work presented here allowed us to deploy and use the functionalities described
in DFKI’s own installation of the PIMO (which is in use since 2012). This allowed us
to gain insights on the applicability of the pilot which will be discussed the following.

276
H. Maus et al.
7.6.1
Managed Forgetting
By extending the DFKI PIMO Server with a Memory Buoyancy calculation based
on the work presented in [390] and Chap.3 as well as the forgetting extensions in the
PIMO5 user interface, we already use part of the Managed Forgetting functionality
in daily work (as of now, over 3 years).
We can see beneﬁts for coping with the information overload originating from
daily usage of the PIMO as lots of things are created with decreasing relevance on
the long run such as tasks, events, web pages, or notes. Such once relevant things
are now forgotten over time and are hidden from direct view during browsing and
search. Furthermore, due to the PIMO’s semantic network, although not accessed
directly, things do not drop that much in MB due to their relation to high buoyant
concepts preventing them from being forgotten.
While we still need more ﬁne tuning of the algorithm and experience with the
MB for the DFKI PIMO, we see that Managed Forgetting already provides beneﬁts
for the knowledge work and still offers more potential. On the other hand, we also
see further research required considering user behavior and their expectations. For
instance, how to predict and cope with the circumstances in the user interface that
sometimes users want to access exactly those things long forgotten.
We see lots of potential in the Managed Forgetting approach to tackle challenges
of information overload in information systems especially when dealing with an
evolutionary approach as supported by the Semantic Desktop.
7.6.2
Contextual Remembering
The PIMO already allowed to see resources in their past context by inspecting explicit
relations to other things. With the introduction of the PIMO Diary, we could show
the potential for Contextual Remembering by using the rich material in the PIMO
including semantic representation as well as content (textual or visual). Personal
insights from the diary give an interesting timeline. We found that the PIMO Diary
is also enjoyable due to its capabilities to identify and present clustered activities
of the user’s “electronic footprints” in the PIMO. Although focused on the personal
application scenario, by “playing around” with it in the DFKI PIMO we see lots
of potential for usage in Knowledge Management applications. For instance, it can
serve as a kind of reporting tool giving an overview of one’s own activities in a period
of time or it can be used as a starting point for explorative search, especially for new
colleagues to get an overview of a group’s activities.

7
Remembering and Forgetting for Personal Preservation
277
7.6.3
Synergetic Preservation
In the pilot, a Preservation Strategy has been introduced initially as a predeﬁned set
of rules and policies. Taking the advantage of actually having a live PIMO running at
the DFKI as a real data set for an organizational application scenario, we introduced
a more ﬁne-grained Preservation Strategy with several heuristics and rules which
could be applied to the DFKI scenario.
Especially the possibility of changing the strategy, recalculating, and inspect-
ing the results provides interesting insight into the data set. Lots of material is
accumulated over the years in this group PIMO. Besides the photo collections, par-
ticularly interesting from organizational point of view are other items proposed for
preservation such as documents, presentations, notes, or topics. Nevertheless, there
are still resources assumed not to be worthwhile to be preserved.
Enhancing the pilot with group functionality (in the personal use case these are
family and friends) also provides a group or a team at work with a means for organi-
zational preservation on a group level. Considering the sharing activity in the DFKI
PIMO, we see that nearly 47% of the things are public for the group. Thus, the PIMO
contains private material as well as material considered relevant for the group. Thus,
at least the shared resources would be subject to a preservation for the group.
With lots of professional businesses based on knowledge work such as product
development, research, or consulting, companies get the opportunity to acquire and
preserve valuable knowledge items from team interaction without requiring effort
to document and preserve manually. This is also a contribution from the Semantic
Desktop approach to an organizational application scenario.
In this PV Assessment we also see a ﬁrst step toward Information Value Assess-
ment for Knowledge Management purposes using new evidences for calculating
content value provided by the Semantic Desktop ecosystem and the PIMO.
To conclude, the Semantic Desktop approach provides lots of potential to embed
personal preservation in personal life. Some of the activities of PIM supported by
dedicated applications or recent operating systems could already now realize per-
sonal preservation if connected to a preservation service as introduced with the PoF
Framework. Further research in the Semantic Desktop area will contribute to the per-
sonal preservation scenario as well as to organizational preservation as the experience
with the pilot at the DFKI shows.

Chapter 8
Personal Photo Management
and Preservation
Andrea Ceroni
Abstract Thanks to the spread of digital photography and available devices, taking
photographs has become effortless and tolerated nearly everywhere. This makes
people easily ending up with hundreds or thousands of photos, for example, when
returning from a holiday trip or taking part in ceremonies, concerts, and other events.
Furthermore, photos are also taken of more mundane motives, such as food and
aspects of everyday life, further increasing the number of photos to be dealt with. The
decreased prices of storage devices make dumping the whole set of photos common
and affordable. However, this practice frequently makes the stored collections a kind
of dark archives, which are rarely accessed and enjoyed again in the future. The
big size of the collections makes revisiting them time demanding. This suggests to
identify, with the support of automated methods, the sets of most important photos
within the whole collections and to invest some preservation effort for keeping them
accessibleovertime.Evaluatingtheimportanceofphotostotheirownersisacomplex
process, which is often driven by personal attachment, memories behind the content,
and personal tastes that are difﬁcult to capture automatically. Therefore, to better
understand the selection process for photo preservation and future revisiting, the ﬁrst
part of this chapter presents a user study on a photo selection task where participants
selected subsets of most important pictures from their own collections. In the second
part of this chapter, we present methods to automatically select important photos
from personal collections, in light of the insights emerged from the user study. We
model a notion of photo importance driven by user expectations, which represents
what photos users perceive as important and would have selected. We present an
expectation-oriented method for photo selection, where information at both photo
and collection levels is considered to predict the importance of photos.
A. Ceroni (B)
L3S Research Center, Leibniz Universität Hannover, Hanover, Germany
e-mail: ceroni@L3S.de
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1_8
279

280
A. Ceroni
8.1
Introduction
Photos are excellent means for keeping and refreshing memories; they can illustrate
situations we have gone through and serve as memory cues [284, 285] to bring back
reminiscences of experiences, events, and people from our past. In the recent years,
we have been witnessing a huge increase in the production of photographs, mostly
due to the wide spread of digital devices such as cameras, smartphones, and tablets.
People easily take hundreds or even thousands of photos during relatively short and
memorable events, e.g., vacations, ceremonies, concerts, or depicting more mundane
aspects of everyday life [301], like shopping, eating, working, and free time. These
numbers can amount to Terabytes of data over years. Even, considering only those
uploaded in social media like Flickr, Facebook, Instagram, and Snapchat, a study
conducted few years ago [290] estimates that 500 million photos (most of them from
personal collections) are uploaded to the Internet every day. In addition, this number
is expected to double every year.
This scenario points out the signiﬁcance of properly dealing with such increasing
volumes of pictures. Due to decreasing storage prices and offers of cloud storage
services, e.g., by Microsoft or Google, it is not a problem to store personal photos
somewhere. As a matter of fact, directly dumping photo collections spending little
or even no time in activities like pruning, editing, sorting, or naming has turned
out to be a popular procedure [206]. This comes at a price: storage devices tend to
become a kind of “dark archives” [222] of photo collections, which means that the
stored pictures, although still available, are rarely accessed and revisited again in
the future. The big size of the stored collections makes going through them such a
tedious activity to prevent the viewers from accessing them at all. As an additional
challenge, already discussed in Chap.1, there is the risk of losing photos by a random
form of “digital forgetting” [191]: over decades storage devices break down, and for-
mats and storage media become obsolete, making random parts of photo collections
inaccessible (Digital Obsolescence [367]). One example is how difﬁcult it would be
today to access photos stored years ago in .mos format in a ﬂoppy disk.
Both the threats of personal dark archives and digital forgetting raise the following
question: how can photos be kept enjoyable and serve their original purpose as mem-
ory cues, where large photo collections tend to get dumped on hard disks and other
types of storage? We propose a transition from dumped contents to more selective
personal digital memories, supported by automatic methods for information value
assessment, to support long-term personal data management. Regarding photo col-
lections, this means identifying the most important photos from an entire collection
and investing some effort to keep them accessible and enjoyable on the long run.
Having a reduced subset of important photos would make the revisiting easier and
more pleasant for the user. However, understanding the importance of pictures to
their owner for preservation and revisiting purposes is a complex process due to the
presence of hidden factors, which are hard to model and capture automatically. These
can be, for instance, memories, context, relationships to whom is in the picture, or
simply personal tastes.

8
Personal Photo Management and Preservation
281
Therefore, the ﬁrst part of this chapter summarizes a user study for a photo selec-
tion task where participants were asked to provide their personal photo collections
and to select the subsets of photos that they would want to preserve and revisit again
in the future. The study involved 35 participants, each one contributing at least one
personal collection containing some hundreds of photos. The goal of this part is to
better understand the human selection process for photo preservation and revisit-
ing, identifying insights, patterns, and challenges that can shape the development of
automatic selection approaches. Moreover, the gathered data will be employed for
the development and evaluation of automatic selection methods. The user study was
complemented by a survey, which we asked the participants to ﬁll after completing
the photo selection task.
In the second part of this chapter, we present and compare methods to auto-
matically select important photos from personal collections for the sake of preser-
vation and revisiting, inspired by the insights emerged from the user study. Many
approaches to photo selection for summarization are centered around the concept
of coverage, aiming at creating summaries that resemble the original collection as
much as possible (see Sect.8.2.2 for an overview). However, we believe that the
complex decision-making behind the selection of photos from personal collections,
characterized by personal attachment due to memories, might reduce the importance
of coverage. Therefore, we model a notion of photo importance driven by user expec-
tations, which represents what photos users perceive as important and would have
selected. We present an expectation-oriented selection method, where information at
both photo and collection levels (incorporating a relaxed notion of coverage) is con-
sidered to predict the importance of photos. We also investigate the role of coverage
further by combining the expectation-oriented selection with an explicit modeling
of coverage in different ways, showing that coverage plays only a secondary role in
this task.
Our approach is an attempt to estimate Preservation Value (PV), previously intro-
duced in Chap.4, considering personal photos as speciﬁc information items. In fact,
the notion of importance assigned to each photo by our model reﬂects what should be
kept for future preservation, because the selection decisions collected during our user
study and exploited for the development of our model have been taken exactly for that
purpose. Furthermore, the input information extracted from photo collections covers
different PV dimensions, compatibly with the characteristics of our scenario (e.g.,
the popularity dimension is not addressed as it refers to sharing and liking behav-
iors that are not very prominent in a personal scenario, where the data is rather kept
private). The PV dimensions that we take into account are discussed in Sect.8.4.2.1
and their roles within the selection process are summarized in Sect.8.4.5.6.
The rest of the chapter is structured as follows. In Sect.8.2, we outline related
works and current approaches to photo selection. Section8.3 describes the user study
while the selection methods are presented and compared in Sect.8.4. Finally, in
Sect.8.5 we summarize and conclude the chapter.

282
A. Ceroni
8.2
Related Work
The discussion of previous works relevant to the topic of personal photo selection and
preservationisorganizedintotwoparts.Theﬁrstonementionspreviousempiricaland
rather qualitative user studies, while the second one reviews automatic approaches
to the photo selection and summarization tasks.
8.2.1
User Studies and Surveys
A considerable research effort has been dedicated to investigate issues related to
photo management and preservation from a human–computer interaction perspective
[95, 206, 408, 418, 419]. Kirk et al. [206] introduced the notion of “photowork” as
the set of activities performed with digital photos after capturing them and before
any end usage like sharing or revisiting. One of their ﬁndings was that people spend
little time in activities like reviewing, pruning, editing, and sorting, because these are
cumbersome and time-consuming procedures. This fact clearly supports the topic and
objective of this chapter. In the context of preservation of public photos, a qualitative
study assessing their value for representing social history is reported in [95]. This
study is mostly limited in (a) not considering personal photos and (b) the small
number of photos considered. The evaluators were asked to rate ﬁve images, selected
from Flickr, considering their worthiness for long-term preservation. Interestingly,
the participants expressed a clear inclination to preserve all the pictures irrespective
of their actual value. The authors hypothesized two possible reasons for this, namely
the difﬁculty of anticipating a future information need and the effort required for
organizing and pruning increasing amounts of data. In any case, they recognized this
as a problem and pointed to the need of methodologies for information appraisal and
selection.
Wolters et al. [418] investigated which photos from an event people tend to delete
over time. In this study, described in Chap.2, the participants took photos during a
common event and then they were asked for deletion decisions at different points
in time. While this work is certainly related to our study, which drew inspiration
from it especially regarding the formulation of the survey, there are nevertheless
some differences. Despite preservation (“keep”) and “delete” decisions are related,
we explicitly asked our evaluators to make selection decisions for the purposes of
preservation and revisiting of images, rather than for deletion. Moreover, in our study,
the users were asked to make joint selection decisions (i.e., select a subcollection)
instead of making decisions for each individual picture in isolation. This is potentially
a key difference, since selecting one photo might affect the decisions for other similar
photos. Finally, instead of taking pictures of a common event explicitly for the study,
we work with personal real-world collections belonging to diversiﬁed events. A
subsequent work by Wolters et al. [419] presented a large-scale survey of 72 young
people and students, with the goal of supporting the design of personal and mobile

8
Personal Photo Management and Preservation
283
preservation systems. The main message of the study is coherent with what emerged
from our survey: a large part of the participants acknowledge the importance of
preserving photos for future generations. Interestingly, however, only a small fraction
of them carries out practices to support photo management and preservation. Another
user study has been presented in [408], where participants wearing eye-tracking
devices were asked to select subsets of photos from two collections depicting two
social events. This work focuses more on the selection process than on preservation
matters. The survey on the aspects driving the selection process shares with our
experiment both similarities (e.g., most of the highly rated aspects were subjective)
and differences (e.g., quality was highly rated there).
8.2.2
Photo Selection and Summarization
Automated photo selection has already been studied in various other contexts, such
as photo summarization [229, 358, 365, 389, 412], identiﬁcation of appealing photos
based on quality and aesthetics [228, 425], selection of representative photos [78,
408], and the creation of photo books from social media content [333]. We consider
the task of selecting important photos from personal collections (e.g., for revisiting
or preservation), which meet user expectations.
The work of Wang et al. [412] is probably the most related to ours, as their model
of image importance does not explicitly include coverage and diversity aspects.
They introduce the notion of “event-speciﬁc image importance”, meaning that the
importance of photos for selection purposes depends on the category of the event
they belong to. The main assumption is that, within a photo collection depicting a
certain type of event, the set of images commonly perceived as important by most
people can be identiﬁed based on the event type. There are, however, substantial
differences regarding the task deﬁnition and the way the ground truth was built.
First, the ground truth was not gathered considering photo selection, since ratings
were assigned by the evaluators to each image in isolation without explicitly deciding
what subset of the collection should be kept. Second, individuals different than the
collection owner rated the importance of images, potentially ignoring any personal
attachment due to memories or hidden context. Image importance has been also
considered in [228, 425]; nevertheless, it was based on quality and aesthetic criteria.
Instead, we explicitly consider selections preferences and expectations of users for
both training our model and an evaluation criterion. Walber et al. [408] also consider
human judgments to evaluate selections, but the users have to wear eye trackers when
using the system to make automatic selections because gaze information is used as
features in the model.
Different photo selection and summarization works consider coverage by identi-
fying clusters of images based on time and visual content [78, 229, 333]. Differently,
our approach does not impose such a strict notion of coverage but rather considers
clusters and other global information together with image-level information, learn-
ing their different impacts on a single model. The works in [299, 358, 365, 389]

284
A. Ceroni
are closer to ours, as they consider coverage in a relaxed way as part of a multi-
goal optimization, but they still consider coverage as a key component. Moreover,
[358, 365] do not consider user assessments in their evaluation and make partial
use of manually created text to associate semantic descriptors to images, while our
method does not require any manual input, once the models for both feature extrac-
tion and importance estimation have been learned. Image collection summarization
is performed in [299, 389] by applying structured prediction methods for learning
weighted mixtures of submodular functions. The attention is drawn to two aspects
that good summaries should exhibit, namely ﬁdelity (coverage) and diversity, which
are represented as a set of nonnegative submodular functions and combined together
in a single weighted submodular scoring function. There are two main differences
with respect to the work presented in this chapter. First, their goal is purely summa-
rization, aiming at optimizing coverage and diversity of output summaries, without
considering whether they contain the most valuable pictures. This is strengthened
by the utilization of the recall-based V-ROUGE metric (a criterion for summary
evaluation inspired by the ROUGE metric [239], used for document summarization)
within the loss function. Second, the way the ground truth has been collected is
heavily oriented toward coverage: the evaluators, not the owners of the collections,
were explicitly told to produce reference summaries that summarize the original col-
lections in the best possible way, and those exhibiting low coverage were discarded.
Conversely, we asked the collection’s owners to select the most important photos
according to their memories and perceptions, without any mention to coverage or
diversity.
Besides [358, 365], other works in the literature rely on external knowledge to
accomplish the task of image summarization [59, 346, 429]. Camargo et al. [59]
combine textual and visual contents of a collection in the same latent semantic
space, using convex nonnegative matrix factorization, to generate multimodal image
collection summaries. Domain-speciﬁc ontologies are required as further input in
[346]. They provide the knowledge about the concepts in a domain and are used
to derive a set of ontology-based features for measuring the semantic similarity
between images. Finally, [429] jointly leverages image content and associated tags
and encodes the selection of images in two vectors, for the visual and textual domains,
respectively, whose nonzero elements represent the images to be included in the
summary. The optimization process makes use of a similarity-inducing regularizer
imposed on the two vectors to encourage the summary images to be representative
in both visual and textual domains.
Summarizing, our approach is different from all the previous works under at
least one of the following aspects: (a) our notion of photo importance is based on
selection decisions made by people on their own photo collections; (b) we do not
estimatephotoimportanceusingsingleindicators(e.g.,quality,presenceoffaces,and
representativeness of the cluster a photo belongs to), but we rather learn the impact of
such aspects through a single prediction model; (c) we use selection decision made
by the collection owners themselves as ground truth for evaluation; and (d) we do
not rely on any kind of photo tagging or descriptive annotation provided manually.

8
Personal Photo Management and Preservation
285
8.3
User Study
As a preliminary step toward the development of automatic methods, we describe
a user study conducted on a photo selection task, whose objective is the gathering
of insights, challenges, and behaviors exhibited by humans when selecting personal
photos for preservation and revisiting purposes [69]. Using their own photo collec-
tions depicting personal events, participants were asked to select a subset of photos
that they would like to stay accessible and enjoyable in the future. Such data, i.e.,
the whole collections along with the selections done by the users, will be used for
the training and evaluation of the selection methods described in Sect.8.4. Upon
completion of the task, the participants were also asked to ﬁll a survey about it,
which is described and analyzed in Sect.8.3.2. This study is closely related to the
one described in Chap.2 and has been partially inspired by it. However, as already
elaborated during the survey of the literature in Sect.8.2.1, there are some impor-
tant differences. First, we asked the participants to jointly select a subcollection for
the sake of preservation and revisiting instead of making “keep” or “delete” deci-
sions for each image in isolation. Second, our study involves personal collections
spontaneously taken and belonging to diversiﬁed events rather than photos of a com-
mon event taken explicitly for the study. This section is mainly organized into three
parts: Sect.8.3.1 elaborates on the setup of the study, Sect.8.3.2 reports the insights
learned from the study and the survey, and in Sect.8.3.3 we show a comparison
between event-based clustering and human selections.
8.3.1
Task Setup
The setup of the performed photo selection task involves the gathering of both par-
ticipants and their photo collections, instructions on how the task should be accom-
plished, and, of course, the development of a software application to perform the
selection in a comfortable way.
The experiment involved 35 users (28.6% females and 71.4% males) with 15
nationalities: 25.7% of the participants came from Greece, 17.1% from Germany,
11.4% from Italy, 11.4% from China, 5.7% from Vietnam, and the rest from Ethiopia,
Turkey, Kosovo, Iran, UK, Thailand, Sweden, Brazil, Albania, and Georgia. Regard-
ing their ages, 60.0% of the participants are between 20 and 30years, 25.7% between
30 and 40, 11.4% between 40 and 50, and 2.9% between 50 and 60.
Previous works mostly consider either public photo collections, for instance, avail-
able on social media like Facebook and Flickr [59, 333, 346, 412], or pictures from
a shared event in which all the evaluators took part [408]. One difﬁculty we see
with using public collections of photos from different people, even if they attended
the same event, is that according to the different experiences of the individuals in
the event they might also have a different level of appreciation for the same photo,
thus inﬂuencing their decisions. In contrast, we use personal photo collections. For

286
A. Ceroni
instance, these can be photos from business trips, vacations, ceremonies, or other per-
sonal events the evaluator participated in. This means that each collection is not just a
bunch of pictures, which might exhibit different degrees of quality and aesthetics, but
there are experiences, sub-events, and memories that might inﬂuence the selection
behavior. We decided to focus on such personal collections because we wanted to
observe the personal photo selection decisions in a setting that is as realistic as pos-
sible. In total, 39 collections were used in the experiment (four users evaluated two
collections), resulting in 8,528 photos. The size of the collections ranges between
100 and 625 pictures, with an average size of 219 and a standard deviation of 128.7.
These collection sizes also emphasize the need for automated selection support, since
manually browsing for photo selection becomes time-consuming. We asked users for
further information about their collections, such as the main topic of the collection,
whether they were previously pruned (e.g., by discarding low-quality photos) and
when they were taken. Overall, 51% of the collections represent vacations, 30%
business trips, and 19% other events like music festivals and graduation ceremonies.
In addition, 23% of the collections were already pruned before the evaluation. The
time when the collections were taken spans from 2007 to 2014 (64% in 2013–2014,
17% in 2011–2012, the rest in 2007–2010).
Since our task of selecting photos for preservation is not an everyday task for
the users, it was important to ﬁnd a good metaphor for supporting the task. After
discussinganumberofoptionswithcognitiveexperts,wedecidedtousethemetaphor
of a “magic digital vault”, which incorporates the ideas of protection, durability, and a
sort of advanced technologies to keep things accessible in the long term. Therefore,
the task consisted in selecting a subset of valuable photos to be put in the magic
digital vault, which would protect them against loss and would ensure that they
remain readable and accessible over the next decades.
To perform the photo selection task, we developed a desktop application, which
enabled the participants to import their own collections and to select photos in a
comfortable way. It is depicted in Fig.8.1, where the photos contained in the imported
collection are displayed in the bottom panel, while the ones selected are shown in
the top panel. Note that faces appearing in Fig.8.1 have been blurred for the sake
of privacy (only for inclusion in this book). The photos are selected and deselected
by double-clicking on them, and they can be enlarged to inspect them better and
appreciate their quality, although no explicit reference to the quality aspect was
made in our instructions to the users. The photos in the collection were shown in the
same order in which they were taken, since this makes the browsing, remembering,
and selection easier and more realistic for the users. Nevertheless, we also made a
preliminary evaluation where the photos were shufﬂed before being presented. This
resulted in higher evaluation time and a higher mental effort for the selection process,
because it made picking from a set of related photos very difﬁcult. We veriﬁed that
keeping the original order did not introduce any signiﬁcant bias in the selection
toward the early photos in the collection. This could have been a risk, since users
might lose attention or even complete the selection without going through the entire
collection.

8
Personal Photo Management and Preservation
287
Fig. 8.1 GUI used by participants to browse the collections and select the photos to preserve
Before starting the evaluation, the users were personally introduced to the photo
selection task as well as to the application that they were asked to use. Further remarks
and clariﬁcations about both the task and the usage of the application were given,
where needed. However, no guidelines were given about the criteria to use for selec-
tion, in order not to inﬂuence the selection process. After the users imported their
collections, the application asked them to select 20% of photos from them for preser-
vation and revisiting purposes. This selection percentage (20%) has been empirically
identiﬁed as a reasonable amount of representative photos, after a discussion with a
subset of users before the study. We also checked the adequacy of this chosen amount
with the users in the survey by asking them whether they would have selected more
photos if they could: 45% of them answered yes, the rest no. This balance means that
20% was a meaningful threshold, neither being too low (the majority of the users
would have answered “yes” in this case) or too high.
8.3.2
Survey and Discussion
After the photo selection step, the users were asked to ﬁll a survey that can be
conceptually split into two parts. The ﬁrst group of questions refers to the scenario

288
A. Ceroni
Fig. 8.2 Survey results with respect to preservation scenario, preservation target group, and preser-
vation as a service
of photo selection process for personal preservation, while the second one looks into
the criteria that were considered during the selection.
Regarding the ﬁrst group of questions, the users were asked to provide information
about (a) which scenario they had in mind when selecting the photos; (b) for whom
they are preserving the photos; and (c) whether they would be ready to pay, and for
how many years, if preservation was a paid service. The answers to each question
were posed as multiple choices and are reported in Fig.8.2. The answers to questions
(a) and (b) reveal that the long-term preservation process is centered around the owner
of the photos: more than 70% of the evaluators said that they thought about own future
reminiscence when they selected the photos, and almost 80% indicated themselves
as a main consumer of the preservation outcome. Looking at the preservation as a
valuable service to be paid (question (c)), the evaluators were mostly split into two
groups: either being ready to pay for many decades (39%) or needing ﬂexibility to
make new preservation decisions every 2–5 years (36%). In both cases, these answers
highlight a clear need for preservation of personal photo collections.
In the second group of questions, we suggested different photo selection criteria
and asked the users to rate how much each criterion was considered during the selec-
tion. The suggested criteria, which are in line with the insights on “keep” and “delete”
decisions in [418], were rated via star ratings on a scale between 1 and 5 (5 stars mean
very important, 1 means not important at all). The criteria along with statistics about
their ratings are reported as box plots in Fig.8.3. Note that medians are represented as
horizontal bold bars, while sample mean is indicated with a bold cross. For the sake
of clarity, we grouped the criteria into three classes: “content-based criteria” refer
to objective and subjective measures for individual photos such as quality, typicality
(i.e., how suitable it is for serving as an iconic summary of the event), the presence of
important people in them, whether photos are generally important, and the evocation

8
Personal Photo Management and Preservation
289
Fig. 8.3 Boxplots of the different selection criteria
of memories; “collection-based criteria”—here represented by coverage of events—
consider a photo in the context of its collection; “purpose-based criteria”, indicating
the importance of different selection goals (in our case, sharing and preservation).
An important ﬁnding of this evaluation is that the objective quality of photos is
rated as the second least important selection criterion, after the sharing intent. This
shows that quality and aesthetics, although being important and used for general-
purpose photo selection [228], are not considered very important in case of selecting
photos for preservation. In contrast, criteria more related to reminiscence, such as
event coverage, typical image, and “the picture evokes (positive) memories” are all
rated high, with highest ratings for memory evocation. The remaining two crite-
ria “picture is important to me” and picture “shows somebody important” refer to
the personal relationship to the picture and are also both rated high. These results
anticipate that the task of predicting photos to be selected for long-term preservation
is likely to be difﬁcult, since many of the criteria that are rated high, e.g., mem-
ory evocation, personal importance and “typical image”, are difﬁcult to assess for a
machine, because they contain a high level of subjectivity. Another complicating fact
is that there is no single dominant selection criterion, but a combination of highly
rated criteria. In these ratings, we can observe differences with respect to the ones
given to the partially overlapping set of criteria reported in [408], where photos on
shared events were used and the selection was not directly related to preservation and
reminiscence. In that work, much higher ratings are given to criteria such as quality,
whereas event coverage and importance of depicted persons are rated relatively low
(although with high variance). Interestingly, photos that capture a memory are also
rated high in this case.

290
A. Ceroni
8.3.3
Image Clustering and Human Selections
We analyze the applicability of current selection and summarization approaches
to the scenario of personal photo selection for preservation, highlighting possible
issues that they might face in this situation. The main uncertainties in applying
state-of-the-art methods to our task are (a) that they are developed with other photo
selection scenarios in mind and (b) that they often do not compare the performances
of their output with selections done by users. They, for example, identify subsets of
photos that provide comprehensive summaries of the initial collections [333, 365,
389], without checking if the summary meets the user expectations, or they consider
judgmentsbasedonmoreobjectivecriteriasuchasaesthetics[228, 425].Sinceawide
part of the state-of-the-art methods for photo selection and summarization considers
clustering and/or coverage for generating selections and summaries (as discussed in
Sect.8.2.2), we clustered photos by applying the event-based clustering technique
described in Chap.3 (Sect.3.5) and compared the clustering results with the human
selections. This analysis is corroborated by the fact that the event coverage criterion,
representable through clustering, has been identiﬁed as important during our study
(Sect.8.3.2).
In our opinion, one of the main risks of applying clustering to emulate human
selections for long-term preservation is that not all the clusters might be important
for the users. There might be photos from a sub-event that the user either simply
does not like or considers less important than others. We supported this hypothesis
by counting the number of human-selected photos in each cluster identiﬁed in our
collections. As to be expected, only for a few clusters (7.3%) all the photos within the
same cluster were selected. However, for a considerable part of the clusters (43%),
no photos were selected at all. Given these statistics, the selection done by any pure
coverage-based method that picks an equal number of photos from each cluster will
contain at least 43% of them that would not have been selected by the user. Another
statistics worth to be mentioned refers to the possibility for cluster-based selections
of picking centroids as representative photos. From our collections, it resulted that
only the 26% of the centroids was actually selected by the users. This reveals that
information about how much a photo is representative of a wider group is only one
of the aspects considered by the users when selecting photos.
Finally, making the assumption that bigger clusters might be more important for
the users (as indicated by the users’ choice to take more photos that capture that part
of the event), we consider the size of the clusters with respect to the number of user-
selected photos that they contain. Figure8.4 shows the correlation between relative
size of clusters (x-axis) and the percentage of selected photos in them (y-axis). It is
possible to observe that the selections done by the users result in many clusters with
few selected photos in each, which is coherent with the notion of coverage. However,
what is more interesting is that the size of the cluster seems to be only marginally
correlated with the importance of the cluster (i.e., the number of selected photos it
contains). This is potentially another limitation for all those methods that select an
amount of photos from each cluster proportionally to its size.

8
Personal Photo Management and Preservation
291
Fig. 8.4 Amount of selected photos in clusters (with respect to the size of selection) versus relative
size of clusters
8.4
Photo Selection
We present in this section an automatic method [70] to identify, within big personal
collections, those photos that are most important to the user, in order to invest more
effort for keeping them accessible and enjoyable in the future. The availability of
such a method alleviates the problems of “digital forgetting” and “dark archives”,
discussed in Sect.8.1, which affect the archival of images and their access, respec-
tively. From one side, preservation effort could be invested only on those photos that
are worth to be preserved for the owner. From the other side, having a reduced subset
of important photos would make the revisiting and enjoying easier and pleasant for
the user. Moreover, to foster adoption, such automated selection method has to keep
the level of user investment low. We do not rely on any additional user investment
such as photo annotation with text [333, 358, 365] or eye-tracking information [408],
because we believe it is exactly the reluctance of further investment that lets large
photo collections unattended on our hard disks. To alleviate errors in automatically
generated selections as well as accommodate user preferences, our approach can be
regarded as a semi-automatized procedure, where the user can interact with it and
modify the suggested selections.
When developing methods for semi-automatic photo selection, it is important to
consider human expectations and practices. Photo selection is a complex and par-
tially subjective process, where the selection decision taken for a given image both
affects the decisions for other photos and depends on the ones already selected. For
this reason, many state-of-the-art methods for photo selection and summarization
are driven by the aspect of coverage, which means attempting to create summaries
that resemble the content of the original collection as much as possible. Some of

292
A. Ceroni
them perform a two-step process of ﬁrst clustering the photo collection (for reﬂect-
ing sub-events in the collection) and subsequently picking the most representative
photos from the clusters [229, 333]. Others [358, 365, 389] consider coverage as
part of a multi-goal optimization, along with the concepts of quality and diversity
within the summary. While coverage surely plays an important role for many photo
selection tasks (see e.g., [408]), we believe that the complex decision-making behind
the selection of photos from personal collections, characterized by subjectivity and
personal attachment possibly due to memories, might reduce the importance of cov-
erage. For instance, considering photos taken during a trip, the user might want to
discard the ones depicting boring or joyless moments.
Therefore, we model a multifaceted notion of photo importance driven by user
expectations, which represents what photos users perceive as important and would
have selected. User expectations have been acquired during the study described in
Sect.8.3, where participants have been asked to provide their own photo collections
and to select those most important to them for preservation and revisiting. We present
an expectation-oriented method for photo selection, where information at both photo
and collection levels is considered to predict the importance of photos (Sect.8.4.2).
This information consists of (a) concept detection, to capture the semantic content
of images beyond aesthetic and quality indicators; (b) face detection, reﬂecting the
importance of the presence of people within photos; (c) near-duplicate detection,
to take the redundancy of many pictures of the same scene as a signal of impor-
tance, and to eliminate very similar ones; (d) quality assessment, since good quality
photos might be preferred in case of comparable photos. This is complemented by
(e) temporal event clustering and, more generally, collection-level information, to
reﬂect the role of coverage in photo selection. The impact of the different features is
learned through a single model to predict the importance of each photo. Information
regarding the selections performed by the users from their own collections is explic-
itly used to train the selection model, so that the predicted importance of photos
represents what the user would have selected. For sake of comparison, in Sect.8.4.3,
we investigate how the expectation-oriented selection can be combined with more
explicit ways of modeling coverage, showing that coverage plays only a secondary
role in this task.
Before delving into the details of the selection method, a general consideration on
the comparison between the features considered in the model and the user study pre-
sented in Sect.8.3 has to be done. The aspects that resulted to be important from the
userstudy,e.g.,evocationofpositivememories,imagetypicality,andpersonalimpor-
tance of photos, are highly subjective and not directly recognizable by a machine,
especially when only relying on the visual content without any other contextual infor-
mation. Given these challenges and constraints imposed by the task, our attempt to
address the insights emerged from the study is threefold: (a) we model event cover-
age, which resulted to be an important aspect in the user study, through clustering and
the hybrid selection methods described in Sect.8.4.3; (b) we employ concept detec-
tion to model more semantic and abstract aspects; and (c) we also include image
quality, although perceived as not very important within the user study, for the sake
of comparison with the other features.

8
Personal Photo Management and Preservation
293
8.4.1
Overview
The problem that we tackle in this chapter can be formalized as follows.
Deﬁnition 1 Let a photo collection P be a set of N photos, where P = {p1, p2, . . . ,
pN}.Thephotoselectionproblemistoselectasubset S ofsizeθ (S ⊂P and|S| = θ),
which is as close as possible to the subset S* that the user would select as the photos
most important to her, i.e., S meets user expectations.
We represent each photo collection as a set C = {P, CL, N D}, where P is the
set of original photos, and CL and N D are sets of clusters and near-duplicate photos
identiﬁed in the collection, respectively. A cluster cl ∈CL contains a set of photos
Pcl grouped together with respect to a deﬁned notion of similarity, whereas a near-
duplicate set nd ∈N D is a set of highly similar photos Pnd. Each photo p ∈P is
modeled as a set of features p = {q, c, F, t}, where q ∈Rnq is the quality vector
of the photo, c ∈Rnc is the concept vector of the photo, F is the set of faces f
appearing in the photo, and t is its timestamp. Each face f = { fl, fs} is described
by its location fl and relative size fs in the photo. For each photo p, we will estimate
the importance value I using the extracted features.
Figure8.5 depicts the overview of our approach to photo selection. Given a photo
collection, we extract information from the photos it contains by applying different
image processing techniques described in Chap.3 and in [70], such as concept detec-
tion, image quality assessment, face detection, event clustering, and near-duplicate
detection. Our main approach is named expectation-oriented selection (Sect.8.4.2),
which learns to generate selections by taking into account user selections from per-
sonal collections as training data. Furthermore, we present three different hybrid
selection methods (coverage-driven, ﬁltered expectation-oriented, and optimization-
driven), with the goal of investigating whether our method can be improved by
combining it with state-of-the-art methods that explicitly consider coverage. The
hybrid selection methods will be discussed in detail in Sect.8.4.3.
Fig. 8.5 Approach overview of automatic photo selection

294
A. Ceroni
8.4.2
Expectation-Oriented Selection
The photo selection model presented in this section aims at meeting human expec-
tations when selecting photos that are most important to the user from a collection,
for revisiting or preservation purposes. We believe that selecting photos that are
important to a user from personal collections is a different task than generating
comprehensive summaries: the set of images important to the user might not be a
proportioned subsample of the original collection. For this reason, we do not impose
a strict notion of coverage but rather consider clusters and other global information
as a set of features, along with photo-level features, learning their different impacts
on a single selection model by mean of supervised machine learning.
While we do not employ deep learning techniques, they can be used for either
computing new features or replacing the computation of the current ones, still leaving
the rest of the approach intact. For instance, image representations learned by convo-
lutional neural networks (e.g., GoogLeNet [377]) can be used for concept detection
as shown in Sect.3.2. This is indeed one of our goals for the next future.
A key characteristic of our features is that they do not require any manual annota-
tion(e.g.,tags,textualdescriptions,andﬁlenames)orexternalknowledge,differently
from other works [333, 346, 358, 365] that make partial use of manually created
text associated to photos. This means that the user does not have to invest time and
effort in preparing the photos before feeding them into our system.
8.4.2.1
Features
Four groups of features have been designed to be used in the photo selection task,
based on the information extracted from photos as presented in Chap.3 and [70]. The
correspondence between these features and the PV dimensions deﬁned in Chap.4 is
made explicit in their descriptions.
Quality-Based Features
They consist of the ﬁve quality measures, namely, blur, contrast, darkness, noise,
and their fused value (weighted pooling using Minkowski metric), which have been
extracted as in [70], following the procedure presented in [275]. They are all numeric
features whose values are between 0 and 1, where 0 represents the best quality and
1 the worst. The assumption behind using this information is that users might tend
to select good quality photos, although their impact seems to be less important in
subjective selections of humans as emerged from previous work [408] and from our
user study (Sect.8.3). Nevertheless, quality might probably play a role in case of
near-duplicate images with different qualities: the user would pick the best one in
these cases. This family of features corresponds to the quality PV dimension deﬁned
in Chap.4.

8
Personal Photo Management and Preservation
295
Face-Based Features
The presence and position of faces in a picture might be an indicator of importance
and might inﬂuence the selection. Some people might prefer photos depicting persons
instead of landscapes, others might like group photos more than single portraits. We
capture this by considering, for each photo, the number of faces within it as well
as their positions and relative sizes. Faces have been detected through the approach
presented in [70], which combines several face detectors (all incorporating the Haar-
like-feature-based detector by Viola and Jones [405]) to maximize the number of
detected faces. Then, each photo is divided into nine quadrants and the number of
faces and their size in each quadrant are computed, resulting in 19 features: two for
number and size of faces in each quadrant, plus an aggregated one representing the
total number of faces in the photo. These features, to some extent, can be associated
to the social graph PV dimension deﬁned in Chap.4, because the presence of people
in a picture could indicate relationships between the appearing people and the owner
of the photo. The notion of who is in a picture, for instance, obtainable via face
clustering and tagging, would provide more precise and complex relationships among
the owners and the people appearing in their collections. However, this would also
introduce additional effort for the user, who would have to manually make the system
aware of the kind of relationship with respect to any unknown person found in a new
collection.
Concept-Based Features
High-level and semantic information has been thoroughly investigated in the past
years within the scope of digital summarization (e.g., [358, 365]). The semantic con-
tent of photos, which we model in terms of concepts appearing in them, is expected
to be a better indicator than low-level visual features, because it is closer to what a
picture encapsulates. We consider the 346 concepts deﬁned as part of the TRECVID
2013 benchmarking activity [6] and previously mentioned in Chap.3, Sect.3.2.3.1.
The concept set includes both abstract concepts, such as “joy”, “entertainment”,
“greeting”, and more concrete concepts, such as “animal”, “building”, and “moun-
tain”. We trained a Support Vector Machine (SVM) as concept detector for each
of them, using the TRECVID 2013 dataset (described in Chap.3, Sect.3.2.3.1) as
training corpus. We used SIFT, SURF, and ORB local descriptors and their color
variants [5] for visual feature extraction. Then, PCA was applied on each descriptor
for reducing their dimensionality to 80, and VLAD encoding [19] was applied to
calculate the ﬁnal image representation. The applied methodology is described in [5]
in more detail. Having such detectors available, we associate to each photo a vector
of 346 elements, one for each concept, where the ith value represents the probability
for the ith concept to appear in the photo. The correspondence between this class
of features and the PV dimensions is not strict and depends on which concepts are
included in the concept space. Concepts might be related to the gravity dimension,
in case they represent aspects related to the events in the collection, or to the social
graph dimension, in case they represent appearance of people, groups, or crowds.

296
A. Ceroni
Collection-Based Features
All the previously mentioned features are extracted from photos in isolation. How-
ever, when users have to identify a subset of important photos, instead of just making
decisions for each photo separately, the characteristics of the collection a photo
belongs to might inﬂuence the overall selection of the subset. For the same rea-
sons, but moving to a ﬁner granularity, it might be worth considering information
about the cluster a photo belongs to. This family of features is a representative of
the coverage PV dimension. For each photo, we consider collection-based features
to describe the collection and, if any, the cluster and near-duplicate set the photo
belongs to. Regarding the whole collection, we consider its size, the number of
clusters and near-duplicate sets in the collection, the number of not near-duplicate
photos, the size of the clusters (avg, std, min, and max) in the collection, the size
of near-duplicate sets (avg, std, min, and max) in the collection, the quality of the
collection (avg and std), and the number of faces in the collection (avg, std, max, and
min). Regarding clusters, we ﬁrst perform event-based clustering by applying the
method described in Chap.3, Sect.3.5. Then, given the cluster a given photo belongs
to, we compute its size, its quality (avg, std, max, and min), and the number of faces
within it (avg). Finally, since the redundancy introduced by shooting many pictures
of the same scene can be evidence of its importance for the user, we also extract
features regarding whether the given photo has near-duplicates or not, as well as how
many they are. Near-duplicates are detected by mean of the methodology described
in Sect.3.4. Shooting many similar photos of the same scene can be regarded as a
form of “investment”, because the user puts effort in replicating a scene to ensure its
availability and quality.
8.4.2.2
Importance Prediction and Ranking
Given a set of photos pi, their vectors fpi containing the features presented above,
and their selection labels lpi (i.e., “selected” or “not selected”) available for train-
ing, a prediction model represented by an SVM is trained to predict the selection
probabilities of new unseen photos, i.e., their importance (see Sect.8.4.4 for details
regarding the training process). Figure8.6 shows how the importance prediction and
ranking of photos is performed for new unseen collections. First, feature vectors fp
are constructed based on the information extracted from the collections as described
before and the importance of each unseen photo p is computed as
Ip = M

fp

,
(8.1)
which is the probability of the photo to be selected by the user. Second, once the
importance of each photo in the collection has been predicted, they are ranked based
on these values and the top-k are ﬁnally selected. The parameter k represents the
requested size of the selection and has to be speciﬁed in advance. The choice of k
will be discussed during our evaluation (Sect.8.4.4).

8
Personal Photo Management and Preservation
297
Fig. 8.6 Workﬂow of the importance prediction and photo selection
8.4.3
Hybrid Selection
In Sect.8.4.2, we have presented an expectation-oriented photo selection model to
explicitly meet selection decisions of humans via Machine Learning. As the evalu-
ation will show (Sect.8.4.5), our expectation-oriented selection clearly outperforms
state-of-the-art methods for photo selection based on explicit modeling of coverage.
However, given the wide exploitation of the concept of coverage in many state-of-
the-art methods, we want to better understand its role in photo selection, in order to
see if and in which way our method can be improved by combining it with explicit
consideration of coverage. The notion of coverage resulted to be highly important
from our user study (Sect.8.3) as well, which is another motivation for further inves-
tigating its potential contributions and limitations. It is interesting to note that, despite
the participants declared coverage as highly important, the selections that they made
in the study exhibited a poor degree of coverage.
We propose and investigate three ways of combining our importance prediction
modelwithcoverage-orientedphotoselectionmethods,denotedas“hybridselection”
methods and described hereafter. The coverage PV dimension, although kept into
account within the expectation-oriented selection via the collection-based features
(Sect.8.4.2.1), becomes more dominant in this new family of selection methods.
While our discussion is centered around the role of coverage, it is worth mentioning
that also the diversity PV dimension is considered within one of the hybrid methods
(described in Sect.8.4.3.3).
8.4.3.1
Coverage-Driven Selection
The coverage-driven selection is based on the widely used two-step process of ﬁrst
clustering and subsequently picking photos from the clusters. First, for a given col-
lection C, a set of clusters CLC is computed as described in Chap.3 (Sect.3.5) and
the importance I (p) of each photo p ∈PC is computed according to our importance
prediction model (Eq.(8.1)). Given the clusters CLC, we use the importance I (p) for

298
A. Ceroni
each photo p ∈PC to pick an equal number of top-ranked photos from each cluster
in order to produce the selection S of required size k.
Cluster Visiting
When picking photos from each cluster, there are different possible ways of iterating
over them until the requested size of the selection is reached. After experimenting a
number of alternatives, we identiﬁed a round-robin strategy with a greedy selection
at each round as the best performing one. The pseudocode is listed in Algorithm 1.
Given an initial set of candidate clusters CLcand, the greedy strategy in each step
selects the cluster cl∗containing the photo p∗with the highest importance, according
to the prediction model M. The photo p∗is added to the selection S and removed
from its cluster cl∗. The cluster cl∗is then removed from the set of candidate clusters
for this iteration, and the greedy strategy is repeated until the candidate set is empty.
Once it is, all the not empty clusters are considered available again and a new iteration
of the cluster visiting starts. This procedure continues until the requested selection
size k is reached. We also experimented with a regression model to predict the number
of photos to select from each cluster, but it did not lead to satisfactory results.
Cluster Filtering
The intuition behind cluster ﬁltering is that not all the clusters identiﬁed in a collection
are equally important to the user. For instance, considering photos taken during a
trip, there might be some of them depicting exciting moments along with other more
boring situations, which the user might want to discard. We tackle this issue by
proposing a cluster ﬁltering method to automatically predict the clusters that are not
important for the user, in order to ignore them when picking photos from each cluster.
We train a classiﬁer (SVM) to detect and ﬁlter out clusters which are not important
Algorithm 1 Coverage-driven Selection (Greedy)
Input: clusters CL, size k, prediction model M
Output: selection S
Set S = ∅
while |S| < k do
Set CLcand = CL
while |CLcand| > 0 do
{cl∗, p∗} = get_most_important_cluster (CLcand, M)
S = S ∪{p∗}
Pcl∗= Pcl∗−{p∗}
CLcand = CLcand −{cl∗}
if |cl∗| = 0 then
CL = CL −{cl∗}
end if
end while
end while
return S

8
Personal Photo Management and Preservation
299
to the user. First, each cluster is described with the following features: size, quality
vector (avg and std), average concept vector, number of faces (avg, std, min, and
max), number of near-duplicate sets and near-duplicate photos in it, near-duplicate
sets size (avg, std, min, and max), photo time (avg, std, min, and max), and photo
importance (avg, std, min, and max). The label associated to a cluster is “good” if
it contains at least one selected photo, “bad” otherwise. Given a training set made
of clusters ci, their corresponding feature vectors fci, and their classes lci, an SVM
is trained and the learned model N is used to predict the class L = N

fcnew

of new
unseen clusters cnew. Details regarding the training process are reported in Sect.8.4.4.
Given the clusters CLC in a collection and a classiﬁer trained on a different portion
of the dataset, applying cluster ﬁltering removes from CLC all those clusters that are
classiﬁed as bad by the classiﬁer. The iteration and picking phase are then performed
only with the remaining “good” clusters.
8.4.3.2
Filtered Expectation-Oriented Selection
The coverage-driven selection is characterized by two steps: ﬁrst clusters are iden-
tiﬁed and handled by possibly ﬁltering and sorting them, and then photos in each
cluster are ranked based on their predicted importance. Differently, within the ﬁl-
tered expectation-oriented selection, we give priority to importance prediction. The
photos in a collection are ﬁrst ranked based on the predicted importance and then
cluster ﬁltering is applied. The result is a ranked list of photos, where those belong-
ing to clusters classiﬁed as “bad” have been removed. Note that the second phase
of this paradigm, which contains cluster ﬁltering in our case, can incorporate any
other computation that exploits cluster information. The way photos are selected
after applying cluster ﬁltering is the same as the one described in Sect.8.4.2.2: the
selection S of size k is created by choosing the top-k photos in the list.
8.4.3.3
Optimization-Driven Selection
Besides applying clustering, another way of explicitly incorporating coverage into a
photo selection process is to consider it as part of a multi-goal optimization problem.
This has been done in [365] to generate representative summaries from personal
photo collections, with the objective of having concise subcollections that resemble
the original one as much as possible. In more detail, in this work quality, coverage,
and diversity are jointly optimized and the optimal summary S∗of a requested size
k is deﬁned as
S∗= arg max
S⊂PC
F (Qual (S) , Div (S) , Cov (S, PC)),
(8.2)
where Qual (S) determines the interestingness of the summary S and it aggregates
the “interest” values of the individual photos within S, Div (S) is an aggregated

300
A. Ceroni
measure of the diversity of the summary measured as Div (S) = minpi,p j∈S,i̸= j Dist
(pi, p j), and Cov (S, PC) denotes the number of photos in the original collection C
that are represented by the photos in the summary S with respect to a concept space.
We incorporate our expectation-oriented selection within this framework, creating
the optimization-driven selection, by computing the Qual (·) function in Eq.(8.2)
based on the importance prediction model (Eq.(8.1)), that is,
Qual (S) =

p∈S
M (p)
(8.3)
Since parts of the concepts in [365] are discrete categorical attributes, associated
to photos using textual information and external knowledge bases not available in
our task, we binarized the elements of our automatically detected concept vector
(which includes the probability that a given concept appears in the photo) by using
a threshold τ such that ci = 1 i f ci > τ, and ci = 0 otherwise. The threshold has
been empirically identiﬁed as τ = 0.4 as the value that led to the most meaningful
binary results. The rest of the calculation of the Div (·) and Cov (·) functions in
Eq.(8.2) is performed as in the original work. In more detail, the distance between
two photos, used to measure the diversity within a summary, is computed based on
Exif features, time, and concept vectors (as in the original work, however we use the
automatically extracted concepts), while the coverage of a summary is calculated
based on the number of photos in the original collection that are represented by the
ones within the summary in a concept space (considering binarized concepts vectors
when needed).
Regarding the resolution of Eq.(8.2), which is an NP-Hard problem, we experi-
mented with the different approaches presented in [365] and the best performing one
consisted in combining quality, diversity, and coverage in a linear way:
S∗= arg max
S⊂PC

α · Qual (S) + β · Div (S) + γ · Cov (S, PC)

(8.4)
and performing a greedy optimization, which has proved performance guarantees
(please refer to [365] for further details). We will discuss the values used for the
α, β, γ parameters in the experimental analysis.
8.4.4
Experimental Setup
8.4.4.1
Dataset
For our experiments, we use personal photo collections with importance judgments
given by the owners of the collections as dataset. These can be photos from business
trips, vacations, ceremonies, or other personal events a person participated in. We
decided to focus on personal collections because we wanted to observe the personal

8
Personal Photo Management and Preservation
301
photo selection decisions in a setting that is as realistic as possible. This gives us a
ground truth for assessing user expectations.
Given the unavailability of such a dataset of real-world personal collections, with
selections done by the owners based on their perceived importance, we considered
the data collected during the user study previously described in Sect.8.3. As a short
reminder, participants were asked to provide their personal photo collections and to
select the 20% that they perceive as the most important for revisiting or preservation
purposes. The selection percentage (20%) was empirically identiﬁed as a reasonable
amount of representative photos, after discussing this matter with a subset of par-
ticipants before the study. In order to make the evaluation results more statistically
signiﬁcant, we expanded the originally collected dataset (Sect.8.3.1) by repeating
the same evaluation procedure with other participants and photo collections. Such
extended dataset consists in 18,147 photos organized in 91 collections and belonging
to 42 users. The collection sizes range between 100 and 625 photos, with an average
of 199.4 (SD = 101.03).
Near-duplicateshavebeendetectedandﬁlteredbyconsideringthecentroidofeach
set as representative photo, as done in [78]. For sets containing two photos, the one
with better quality is chosen as representative. Similarly to [353], each representative
is marked as selected if at least one photo in its set has been marked as selected, and
marked as not selected otherwise.
8.4.4.2
Evaluation Metrics
Since the overall goal of our work is emulating the human behaviors in selecting the
subsets of photos from a personal collection, we compare the automatic selections
generated by our methods with the ones done by the users.
The selection methods presented in this chapter can generate a selection S of size
k from the original collection, where k can assume different values. We evaluate the
different methods considering the precision P@k of the selection S of size k that they
produce, computed as the ratio between number of photos in S that were originally
selectedbytheuserandthesizeof S.Sincethecollectionsinourdatasethavehighsize
variability (from 100 to 625 photos), absolute values of k, although traditionally used
in information retrieval tasks, would result in selecting very different relative portions
of the collections depending on their sizes. This makes the impact of the selection
different among collections. We, therefore, decided to express k as a percentage of the
collection size, instead of an absolute value. In particular, we compute the precision
for k = 5, 10, 15, 20%, which are indicated as P@5, P@10, P@15, and P@20%,
respectively. We concentrate the discussion on P@20%, because our ground truth
was gathered by asking users to select the 20% of their collections. We will also give
comments about the recall of the selections generated by the different methods.
The 91 collections in our dataset have been split by 10-fold cross validation (used
for training and evaluating the classiﬁers) and all the values reported in the rest of this
section are averaged over the test sets of each split. Statistical signiﬁcance tests were
performed using a two-tailed paired t test, and signiﬁcant improvements are marked

302
A. Ceroni
as ▲and △(with p < 0.01 and p < 0.05, respectively). If not stated otherwise, the
signiﬁcance outcome reported in the tables always refers to the comparisons with
both the baselines described in Sect.8.4.4.4.
8.4.4.3
Parameter Settings
The classiﬁers employed in this chapter for importance prediction and cluster ﬁl-
tering, built using the SVM implementation of LibSVM,1 have radial basis func-
tions as Kernels and their hyper-parameters are the following. The ones of the SVM
used within the expectation-oriented method for importance prediction are C = 1.0,
γ = 1.0, while the SVM used for cluster ﬁltering has parameters C = 3.0, γ = 1.5.
All of them were tuned by grid search and 10-fold cross validation.
8.4.4.4
Baselines
We compare our method with two baselines, one based on clustering and one repre-
senting the optimization framework presented in [365].
Clustering
SimilarlytowhatwasdescribedatthebeginningofSect.8.4.3.1,foragivencollection
C, a set of clusters CLC is computed. The selection is built by iterating the clusters,
temporally sorted, in a round-robin fashion and picking at each round the most
importantphotofromthecurrentcluster(untiltherequestedselectionsizeisreached).
Instead of using our expectation-based model, the importance of each photo p ∈PC
is modeled as
I (p) = α ·
qp
 + (1 −α) · dim

Fp

(8.5)
which is a weighted sum of the quality vector of the photo and the number of faces
in it. This notion of image importance covers different works in the literature, for
instance [229, 333]. We experimented with different values of the parameter α,
identifying the best value as α = 0.3, which gives more importance to the number of
faces in the photos. We report the performances obtained with this parameter value
in our evaluation.
Summary Optimization
We implemented the approach presented in [365] as another baseline, where sum-
maries are generated by optimizing quality, coverage, and diversity as in Eq.(8.2).
It differs from the hybrid method described in Sect.8.4.3.3 in how photo impor-
tance is modeled, as here the expectation-oriented model is not considered. Instead,
the quality of summaries is computed by summing the “interest” of photos in it,
deﬁned as a measure dependent on photo quality and presence of portraits, groups,
1http://www.csie.ntu.edu.tw/~cjlin/libsvm/.

8
Personal Photo Management and Preservation
303
and panoramas. We computed the interest of photos as in the original work, using the
concepts “face”, “3 or more people”, and “landscape” available in our concept set
to represent portraits, groups, and panoramas, respectively. Also, the diversity and
coverage of summaries are computed coherently with their original computation, as
already described in Sect.8.4.3.3. Assigning equal weights to the α, β, γ parameters
gave us the best results, and thus we will report the performances for only this setup
in the following evaluation, denoting it SummOpt.
8.4.5
Results
The discussion of the results is organized as follows. First, we show the performances
of our expectation-oriented selection with respect to the baselines, discussing the
impact of different features subsets in the selection (Sect.8.4.5.1). We also analyze
the correlation between single features and selections in Sect.8.4.5.2. Second, we
present the results of the hybrid selection methods and we compare them both with the
baselines (Sect.8.4.5.3) and with the expectation-oriented selection (Sect.8.4.5.4).
Third, we make a general comparison of the methods based on recall performances
(Sect.8.4.5.5).
Besides providing the numeric performances, we discuss and compare the dom-
inant criteria behind each method, and we map such analysis to the PV dimensions
introduced in Chap.4. These considerations are summarized in Sect.8.4.5.6.
8.4.5.1
Expectation-Oriented Selection
This section presents the evaluation of our expectation-oriented selection with respect
to the two baselines deﬁned in Sect.8.4.4.4. Different importance prediction models
have been trained by using the subsets of the features described in Sect.8.4.2.1, so
that the impact of different groups of features on the precision can be analyzed. Since
each group is linked to part of the PV dimensions (Chap.4), our analysis provides
insights about the importance of the dimensions in the context of personal photo
selection for preservation.
To reduce the dimensionality of the concepts features, consisting of 346 numerical
values, we performed feature selection based on Information Gain [141] and we kept
the top-160 features for training. The amount of 160 features has been empirically
identiﬁed as a compromise between simplicity and expressiveness of the model.
We did not apply any feature selection on quality and faces features because their
dimensionality is small.
The results for different selection sizes (k) are listed in Table8.1. The two baselines
exhibit comparable performances, with SummOpt performing slightly better for all
considered values of k (5, 10, 15, and 20%). Regarding our model, quality features
are the ones that perform weakest individually, which has already been observed
for other photo selection tasks [408]. This corroborates the idea that low-quality

304
A. Ceroni
Table 8.1 Precision of the expectation-oriented selection, distinguishing different sets of features
P@5%
P@10%
P@15%
P@20%
Baselines
Clustering
0.3741
0.3600
0.3436
0.3358
SummOpt
0.3858
0.3843
0.3687
0.3478
Expectation-oriented selection
Quality
0.3431
0.3261
0.3204
0.3168
Faces
0.4506▲
0.3968▲
0.3836△
0.3747△
Concepts
0.5464▲
0.4599▲
0.4257▲
0.4117▲
Photo-level
0.5482▲
0.4760▲
0.4434▲
0.4266▲
All (Expo)
0.7124▲
0.5500▲
0.4895▲
0.4652▲
photos might be kept anyway because they contain and recall memories and events
important to the user. Faces features alone already show better performances than the
baselines: the presence, number, and position of people in photos, largely used as one
selection criterion in other works, is indeed a meaningful indicator of importance.
The performance achieved when only using concepts features is better than the ones
of quality and faces: they are able to capture the semantic content of the photos,
going beyond their superﬁcial aesthetic and quality. Examples of concepts with a
high importance in the model are “person”, “joy”, “entertainment”, and “crowd”.
The model trained with the combination of all the aforementioned features, denoted
by photo-level because the features are extracted from each picture in isolation,
slightly improves the performance of using concept features alone. This indicates
that leveraging quality and faces features in addition to semantic measures, such as
concepts, can ameliorate the overall performance.
If we include global features for each photo representing information about the
collection, the cluster, and the near-duplicate set the photo belongs to, we get a
comprehensive set of features, which we call all. Similarly to the case of concepts
features, we performed feature selection based on Information Gain on the whole
set of all features and we retained the top-200 features for training. Again, the set
size has been empirically identiﬁed as a compromise between simplicity and expres-
siveness of the model. The precision of the selection for this global model further
increases for every selection size: this suggests that decisions for single photos are
not taken in isolation but they are also driven by considering general characteristics of
the collection the photo belongs to, e.g., number of photos, clusters, average quality
of photos in the collection and in the same cluster, and how many duplicates for the
photo there are. This is a point of distinction with respect to state-of-the-art methods
(represented by the two baselines), because our selection approach does not strictly
handle collection-level information by imposing clustering (Clustering) or optimiz-
ing measures like coverage and diversity along with photo importance only based
on quality and presence of people (SummOpt). It rather takes this global information
in consideration in a ﬂexible way through a set of features, whose impact on the

8
Personal Photo Management and Preservation
305
selection is learned from user selections and expectations. The expectation-oriented
model using all the available features (named Expo in the rest of the evaluation)
leads to a relative improvement of 38.5 and 33.75% over Clustering and SummOpt,
respectively, considering P@20%, and even higher improvements when considering
smaller values of k (90.4 and 84.6% for P@5%).
The P@20% metric is of primary importance because we asked users to select
exactly 20% of their collections during the data acquisition. However, another point
of discussion is the trend of precision performances over different values of k: all the
models reach higher precision values for smaller selection sizes. This can be due to
the presence of a limited number of selected photos that are relatively easy to identify
for the methods, which give them highest selection probability.
Summarizing, modeling different promising aspects in terms of features and ﬂex-
ibly combining them through Machine Learning leads (except when using quality
information alone) to consistent and statistically signiﬁcant improvements over state-
of-the-art summarization and selection methods.
8.4.5.2
Feature Analysis
For sake of completeness, in Table8.2 we report the top-30 features ranked based
on the Information Gain with respect to the class (i.e., user selections). Despite the
presence of similar and redundant features, the table still provides an overview of the
features that are correlated to the class the most. The symbol † for features related to
Table 8.2 Top-30 features ranked by information gain with respect to the class
Info gain
Feature name
Info gain
Feature name
0.10836
ND of photos
0.01561
Avg aggr. quality in collection
0.02569
Images without ND in collection
0.01538
Std ND set size
0.02258
Min darkness in cluster†
0.01523
Min ND set size
0.02251
Std aggr. quality in collection
0.01469
Std faces in collection
0.02240
Norm of concepts in collection
0.01440
Concept “person”
0.02189
Count of faces in photo
0.01414
Count of faces in cluster†
0.02177
Avg size of ND sets in collection
0.01321
Std aggr. quality in cluster†
0.02144
Avg contrast in cluster†
0.01306
Concept “dresses”
0.02009
Max cluster size in collection
0.01291
Concept “joy”
0.01863
Avg contrast in collection
0.01273
Avg blur in cluster†
0.01760
Count of central faces in photo
0.01147
Avg blur in collection
0.01732
Avg count of faces in collection
0.00952
Concept “two people”
0.01610
Min clusters size
0.00889
Concept “entertainment”
0.01609
ND sets in collection
0.00873
Contrast of photo
0.01565
Size of central faces in photo
0.00826
Concept “girl”

306
A. Ceroni
clusters means that the cluster containing the input photo is considered. For instance,
given an input photo, the feature “Min darkness in cluster” represents the minimum
darkness over all the images within the cluster the input photo belongs to. The ﬁrst-
ranked feature, whose Information Gain value is signiﬁcantly higher than the ones
of the other features, represents the number of near-duplicates that the input photo
has. This reveals that the redundancy introduced by taking many shoots of the same
scene is a strong signal of importance for that scene. Besides this feature, the other
ones in the table have much smaller and similar Information Gain values. Many other
high-ranked features are computed considering global information from clusters and
collections, which conﬁrms what already discussed: the decisions taken for single
photos implicitly take into account general characteristics of the collection the photo
belongs to. Features computed based on faces are also important, namely the total
number of faces in the picture and the number and size of faces in the center of
the photo. Quality is mostly considered in relation to collections and clusters (i.e.,
quality statistics with respect to the whole collection or a given cluster). A relatively
low number of features represent concepts, which is somewhat counterintuitive if
compared with the selection results of the concepts features reported in Table8.1.
Nevertheless, the high-performance values, if compared to those of quality and faces
features, might be due to the combination of many concept features, although they
are not all top ranked.
8.4.5.3
Hybrid Selection
This section discusses the precision of the hybrid selection methods presented in
Sect.8.4.3 with respect to the baselines, along with a comparative analysis between
the different hybrid selection methods. The results are listed in Table8.3, where they
have been split based on the three different classes of hybrid selection described in
Sect.8.4.3. For coverage-driven selection, we report results of different combina-
tions: basic refers to the coverage-driven selection which only uses our importance
prediction model deﬁned in Sect.8.4.2.2 as photo importance measure, picking pho-
tos in a round-robin fashion from clusters temporally ordered; the term ﬁltered means
the use of cluster ﬁltering, while the presence of the term greedy indicates the use of
the greedy visiting strategy. The ﬁltered expectation-oriented selection is denoted as
F-Expo.
For the optimization-driven method, we experimented with the different optimiza-
tion methods described in [365] after introducing our importance prediction model
in place of the original importance measure used in that work (Qual (·)). We found
out that the best performing method was still the greedy optimization of a linear
cost functional combining importance, diversity, and coverage (Eq.(8.4)) but with
a parameter combination that gives more importance to the quality of the photos
(0.6 Qual, 0.3 Cov, 0.1 Div). We consider the results of this setup in the following
evaluation. This difference in weights with respect to the SummOpt baseline already
anticipates that our expectation-based measure of importance has a bigger impact on

8
Personal Photo Management and Preservation
307
Table 8.3 Precision of the hybrid selection methods
P@5%
P@10%
P@15%
P@20%
Baselines
Clustering
0.3741
0.3600
0.3436
0.3358
SummOpt
0.3858
0.3843
0.3687
0.3478
Coverage-driven selection
Basic
0.4732▲
0.4113▲
0.3902△
0.3809△
Filtered
0.5351▲
0.4617▲
0.4325▲
0.4170▲
Filtered+greedy
0.6271▲
0.4835▲
0.4391▲
0.4262▲
F-Expo
0.7065▲
0.5502▲
0.4863▲
0.4600▲
SummOpt++
0.7115▲
0.5533▲
0.4937▲
0.4708▲
Expo
0.7124▲
0.5500▲
0.4895▲
0.4652▲
Filtering with oracle
Greedy+oracle
0.6499▲
0.5107▲
0.4665▲
0.4484▲
F-Expo+oracle
0.7150▲
0.5606▲
0.4982▲
0.4753▲
the performances than the native quality measure deﬁned in [365]. The method will
be referred as SummOpt++.
The results in Table8.3 show that all hybrid methods outperform the baselines,
with statistical signiﬁcance, showing that the inclusion of the importance prediction
model to assess photo importance has a strong impact compared to the baselines
methods, which model photo importance with simple functions of quality and people
occurrence. Similarly to the performances of the expectation-oriented models, both
the absolute precision values and the improvements with respect to the baselines
increase for decreasing k.
Focusing on the coverage-driven selection, the results in Table8.3 also show
that cluster ﬁltering increments the precision of the basic approach of an amount
between 9.48% (P@20%) and 13.1% (P@10%). The greedy visiting strategy leads
to improvements as well. Statistical signiﬁcance tests revealed that the improvements
introduced by ﬁltered and ﬁltered+greedy are statistically signiﬁcant.
Comparing the results of the different hybrid selection methods, F-Expo and Sum-
mOpt++ achieve better precision performances than the coverage-driven methods,
and a t test conﬁrms that these improvements are statistically signiﬁcant. This shows
that the measure of photo importance modeled by our importance prediction has a
bigger impact on the precision of the selection than coverage, and those methods that
strictly model it through clustering (coverage-driven selection) get a smaller bene-
ﬁt when incorporating the expectation-oriented model. On the other side, methods
that either give priority to expectations (F-Expo) or consider expectations, coverage,
and global information in a ﬂexible way via optimization (SummOpt++) can better
exploit the expectation-oriented model.

308
A. Ceroni
8.4.5.4
Expectation Versus Hybrid Analysis
In this section, we make a comparative analysis between the expectation-oriented
selection model exploiting all the available features (Expo) and the hybrid selection
models. Considering Table8.3, we can observe that the performances of Expo are
better or comparable with the ones of the hybrid selection models. In particular,
the improvements of Expo with respect to the coverage-driven methods are statisti-
cally signiﬁcant. The only improvements over Expo (which anyway are not statisti-
cally signiﬁcant) are obtained when considering methods that prioritize expectations
(F-Expo) or possess a relaxed consideration of coverage and global information in
general (SummOpt++). These results further support our assumption that, for the
photo selection task involving personal data, a strong consideration of coverage
overstresses this aspect as a selection criterion. Instead, the users might not follow a
strict idea of coverage when making selections, generating selections that are not as
proportioned samples of the original collections as purely coverage-based methods
would suggest. Only for the methods with a more ﬂexible consideration of coverage,
the performances are similar to the pure expectation-oriented method.
Cluster ﬁltering is an attempt to eliminate clusters uninteresting to the user, and
in order to further alleviate this aspect we conducted experiments considering only
important clusters, i.e., those ones containing at least one selected photo. This is
done by assuming to have a perfect classiﬁer, i.e., an “oracle”, to ﬁlter out not impor-
tant clusters and to focus the hybrid selection strategies only on the important ones.
Although getting improvements compared to ﬁltered+greedy and F-Expo, the per-
formances when using such Oracle, reported in the bottom part of Table8.3, did not
lead to consistent and statistically signiﬁcant improvements with respect to Expo.
Greedy+oracle does not beat Expo, while F-Expo+oracle only introduces a limited
and not statistically signiﬁcant improvement. These results show that the aspect that
mostly drives user selections and expectations is the personal perception of impor-
tance, although this can produce unbalanced selections which are not representative
of the original collection. Another problem related to clustering, even considering
the important ones, might be the decision of how many photos to pick from each of
them.
8.4.5.5
Recall-Based Analysis
We make a comparative analysis of the different methods based on recall. The moti-
vation of considering recall is that a user might accept to increase the size of the
automatically created selection in order to include more important photos than the
ones included when remaining strict to the ideal size of 20% (considered during
the user study). In Table8.4, we show the recalls of the best performing methods
from each selection class, computed for different selection sizes. Note that R@20%
always coincides with P@20%, since users were asked to select 20% of their col-
lections. The results are coherent with the analysis already done for the precision:
both the expectation-based model and the hybrid selection methods outperform the

8
Personal Photo Management and Preservation
309
Table 8.4 Recall of different selection methods
R@20%
R@30%
R@50%
R@75%
Clustering
0.3358
0.4555
0.7000
0.9231
SummOpt
0.3478
0.4354
0.6884
0.9253
Expo
0.4652▲
0.5310▲
0.7356△
0.9310
Filtered+greedy
0.4262▲
0.5129△
0.7232
0.9231
Filtered Expo
0.4600▲
0.5361▲
0.7433△
0.9275
SummOpt++
0.4708▲
0.5408▲
0.7405△
0.9315
baselines, and the former is overall better than or comparable to the latter class.
Only methods that prioritize user selections (ﬁltered expectation-based) or consider
expectations, coverage, and global information in a ﬂexible way via optimization
(optimization-driven selection) can reach slightly higher recall values than one of
the expectation-based models. In the future, this consideration could be the starting
point for a photo selection method that maximizes recall, or at least considers it in
the learning model along with precision-based criteria.
8.4.5.6
Features and Preservation Value Dimensions
We now summarize the main insights obtained from this work, linking the results
of the photo selection methods to the high-level dimensions of the PV introduced in
Chap.4.
From the results reported in Sect.8.4.5.1, quality features are the ones that perform
worst, revealing that the quality PV dimension is not of primary importance for
preservation in personal scenarios. As an example, one might want to keep a photo
because it evokes memories of the time when we took the photo, despite its low
quality. The faces class of features alone, although performing better than quality
features, was not as a good indicator as expected from its common usage in photo
summarization and selection. The introduction of face clustering and tagging, to
know who is actually appearing in the photos and what the relationship with the
collection owner is, might probably help make the social graph dimension more
important. However, this would also require a certain investment of the user in tagging
and annotating, as well as the awareness of social relationships, which are both not
assumed to be available in the considered scenario.
Since a wide part of the state-of-the-art methods for photo selection and sum-
marization [78, 333, 358, 365] considers clustering and, more generally, coverage
as primary criterion for generating selections and summaries, we applied selection
methods based on temporal clustering and on summary optimization to our scenario.
These high expectations on the coverage dimension were not conﬁrmed by the exper-
imental results. We observed that emphasizing coverage, either strictly by selecting
photos fairly from each cluster or more ﬂexibly via summary optimization, did not

310
A. Ceroni
yield signiﬁcant improvements over the pure expectation-oriented selection, which
incorporates global information in a more relaxed way through a set of features. The
only positive result related to coverage is the high correlation between the presence
of near-duplicates and selection decisions (Table8.2), which shows that people tend
to shoot many similar photos of what they like the most and is most important to
them. However, this fact is more related to the concepts of redundancy and invest-
ment than coverage. In our opinion, one of the main pitfalls of stressing coverage to
emulate human selections from personal collections for preservation is that not all
the clusters are usually equally important for the users. There might be photos from
a sub-event that the user either simply does not like or considers less important than
others. The optimal parameter values identiﬁed for the optimization-driven selection
(Sect.8.4.3.3), jointly considering importance, coverage, and diversity, showed that
also the diversity dimension had a low impact on the selection. While being widely
considered for photo summarization, diversity resulted to have only a marginal role
in emulating user selections for preservation.
8.4.6
Personalization
Although the expectation-oriented method presented in Sect.8.4.2 has been proved
to be more effective in meeting user expectations than state-of-the-art approaches
based on coverage, it applies the learned selection model for any user and collec-
tion. Nevertheless, the photo selection process (especially for personal data) can be
highly subjective and the factors that drive the selection can vary from individual
to individual [314, 353, 425]. General selection models, although capable of repre-
senting common selection patterns (e.g., photos depicting people might be usually
appreciated), might be improved by considering the preferences of each single user
separately and derive personalized models for them. Some users might be particularly
interested in photos depicting many people, while others might prefer pictures with
landscapes or buildings. Besides variations in the set of appreciated concepts, also
selection aspects that are ignored by some people might become more important for
others. It is therefore worth spending some thoughts on how personalized selection
models that adapt to the preferences of different users could be developed. To this
aim, we have performed a preliminary study in [143] to investigate how personalized
models can be derived from the photo selection approach described in Sect.8.4.2,
denoted by “general model” hereafter. We highlight the applied methodology and its
ﬁndings in the rest of this section.
8.4.6.1
Methodology
A recurrent matter in Machine Learning is continuously managing new data, so that
the existing model can be updated to accommodate new information and to adapt
to it. Two common approaches for updating the model to new incoming data are

8
Personal Photo Management and Preservation
311
Online Learning [46], where the model is updated only considering the new data, and
Incremental Learning [65], where the model update considers the old training data
along with the incoming data. We considered the latter strategy and retrain the model
each time new data (i.e., selection decisions) was provided by the user because, in our
scenario, the updated model has to be aware of the entire data available, not just of
the most recent one. Although efﬁcient and effective incremental versions of off-line
learning algorithms exist (e.g., [65]), we performed the model update by including
the new data in the training set and retrain the model from scratch. We implemented
such more straightforward but functionally equivalent approach because our scenario
does not impose strict temporal constraints for the model update, thus making the
efﬁciency beneﬁt of incremental versions of secondary importance. The time taken
by a user to produce a new collection (e.g., after a trip or vacation) can be considered
sufﬁcient to retrain the model with the whole available data. Should the temporal
constraints of the envisioned scenario become stricter, the incremental version of the
employed algorithm could be plugged in without changing the functionalities of the
whole application.
The personalization workﬂow is summarized in Fig.8.7, which emulates the appli-
cation of the personalized model in real-world settings. The personalized photo selec-
tion models, one for each given user, are built by re-training the model every time that
a new collection is imported and the automatic selection done by the current selec-
tion model is revised by the user. The annotated photo collections available to train
the general model are ﬁrst preprocessed through image processing techniques and
features are extracted from them, in the same way as described before in this chapter.
Fig. 8.7 Overview of the personalization of the general photo selection model

312
A. Ceroni
For each new collection provided by the user, a ﬁrst selection is made by the trained
general model as described in Sect.8.4.2.2 and the selected photos are displayed
to the user, who gives feedback revising the automatically generated selection. The
training dataset is then expanded by adding the feedback data and the general model
is retrained with the updated training dataset. Iterating this process, it is expected
that the gap between user expectations and model’s selections gets lower, due to the
adaptation of the model toward the selection preferences of the user. This workﬂow
represents the envisioned behavior once the whole system has been ﬁnalized and
released to the end user. However, in order to easily repeat evaluations when design-
ing and implementing the model, we collected the data from each user once for all,
i.e., users evaluated all the collections from scratch without revising any automat-
ically generated selection. Although we are aware that the selections done by the
user starting from an automatically generated selection might differ from those done
when selecting photos from scratch, repeating the evaluation multiple times when
designing the system would have been unfeasible for the users. Moreover, acquir-
ing evaluations done from scratch is unbiased from the initial selection proposed
automatically.
Usually, the adaptation of a system within the initial rounds of user interactions
is affected by the so-called “cold-start problem”: there is not enough (or even not
at all) training data to let the model adapt to the user. This holds in our scenario as
well, where the selection model might not make proper predictions due to the lack of
annotated collections in the initial training set. We considered two ways of building
the initial training set. One consists in using one annotated collection of the given
user as initial training set. The other is based on using annotated collections from
other users to train the initial selection model, hopefully boosting the adaptation of
the model to a given user when a limited amount of personal training data is available.
The latter approach is based on the assumption that, despite the subjectivity of the
task, common selection patterns exist and could be captured through a sample of
selections done by other users.
8.4.6.2
Findings
We used the dataset already described in Sect.8.4.4 as basis of our experiments. In
order to assess personalization performances, we consider users who contributed at
least ﬁve collections as test users. Among the overall 91 photo collections, there are
11 users who provided at least 5 collections (10 users contributed 5 collections, 1
user contributed 6 collections) which result in 56 collections totally. Afterward, the
original dataset is split into two parts: one part contains 35 collections from 31 users,
whereby each user provided at most 2 collections, which is named general dataset;
another part contains 56 collections from 11 users, whereby each user provided at
least 5 collections, which is called personalized dataset.
Given the aforementioned general distinction between general and personalized
dataset, we evaluate the performances of the model update over different rounds of
adaptation. The personalized dataset is split based on users where each one owns 5

8
Personal Photo Management and Preservation
313
collections (one user owns 6). At each iteration k, for each user with N collections, k
collections are added to the initial training set to learn the personalized model of the
user, and N −k collections are used for testing. We considered three ways of building
training sets. In the stand-alone procedure, the initial model is trained with one
randomcollectionoftheuser,andthemodelupdateisincrementallydoneconsidering
the remaining collections of the same user. In this case, we are considering each
test user in isolation, ignoring any data from others. The collaborative strategy ﬁlls
the initial training set with all the collections within the general dataset. This case
represents the situation where, in the absence of large amount of annotated personal
data for training, annotated collections of other users are used to alleviate the cold-
start problem. The user-agnostic method, similarly to the collaborative case, uses
the general dataset as initial training set. However, at each iteration, instead of
including collections of the user under consideration, we add randomly selected
collections from the other test users. This case is motivated by the assumption that
if one collection, which is not from the user that we are considering, is included in
the training set at each iteration, then the adaptation performances should be smaller
than including collections that are from the user that we are considering. This would
highlight the importance of incorporating selection information of the user in the
training set when making selections for new collections of the same user.
We observed from the experiments that the precision of both stand-alone and
collaborative increases at each iteration, i.e., with the increase of the number of user’s
collections considered for training the model. This suggests that having a selection
model partially aware of the user preferences (by exploiting a certain amount of the
selection behavior in the training phase) can improve the precision of new unseen
collections of the same user. The precision of collaborative was higher than the one
of stand-alone, especially at the ﬁrst iterations, showing that the selection data from
other users can alleviate the cold-start problem. We also measured the relative gain
obtained by each strategy between any two consecutive iterations. The gain of stand-
alone at each iteration resulted to be higher than the one of collaborative, because
the initial model was weaker (due to the limited training set) and the inclusion of new
training collections had a higher impact on the learning. Comparing user-agnostic
and collaborative, the former exhibited an almost null or even negative gain over
iterations, while the latter led to a bigger and increasing performance gain at every
iteration. This demonstrated that the increase of performance at each iteration was
due to the inclusion of a new collection of the same user in the training set and not
simply caused by expanding the training set at each iteration, since in this case the
gain of user-agnostic should have been higher as well.
Asaconclusion,thisevaluationledtopromisingresults,showingthat(a)including
new annotated collections for the same user when training the model can beneﬁt the
selections on new unseen collections of the same user, and (b) exploiting annotated
collections from other users as initial training data can boost the system performances
in cold-start scenarios. It is important to clarify that the standard deviation observed
in these experiments was relatively high. This can be due to a mixture of aspects,
such as a limited size of test set (both in terms of users and iterations) and intrinsic
changes of difﬁculty among collections of the same user. For this reason, although

314
A. Ceroni
a promising user adaptation emerged from this study, the inclusion of an extended
amount of users, collections, and iterations would help make the results more evident
and statistically signiﬁcant.
8.5
Conclusions
In this chapter, we considered the problem of keeping personal photo collections
enjoyable over time. Given the explosion in the production of digital photos within
the recent years and the common practice of merely dumping such data on cheap
storage devices or using storage services, the stored photo collections are rarely
accessed and revisited afterward. To some extent, their content tends to be forgotten
because the big collection size makes their revisiting a fatiguing process.
As a remedy, we proposed a selective approach to long-term data management that
aims at identifying what is most important to the user and investing in the longevity
and enrichment of this content, in order to make the future revisiting more enjoyable
and less tedious. The development of such automated method was preceded by a
user study, to lay the foundations of the task and better understand its challenges.
The user study was centered around a photo selection task where 35 participants
contributed their own collections, from which they selected the photos most impor-
tant to them, namely the ones that they would like to preserve for future revisiting.
One important outcome was that many hidden and subjective criteria (memory evo-
cation, personal importance, and photo typicality) were rated high, anticipating the
difﬁculty of automatizing the selection task. Moreover, the more objective criterion
of photo quality was rated as less important. Another aspect emerged as important
was coverage, which means that the set of selected pictures should fairly represent
the content of the original collection. Although this was stated by the participants,
their selections exhibited a poor degree of coverage.
Afterward, we presented an expectation-oriented method for photo selection
exploiting an extensive set of photo- and collection-level features, to estimate the
long-term photo importance based on user expectations. The evidence of user expec-
tations has been derived from the personal data provided during the user study and
has been used to train the selection model. The goal of this method is supporting
users in selecting the most important photos for creating an enjoyable subcollection
of a personal collection for preservation and revisiting purposes. Since a wide part
of state-of-the-art methods is driven by the concept of coverage, which resulted to
be highly rated in our user study as well, we also investigated how to combine the
expectation-oriented selection with more explicit modelings of coverage. Experi-
ments with real-world photo collections showed that (a) our method outperforms
such state-of-the-art works when considering human selections as evaluation crite-
rion and (b) comparable results to our method can be achieved only when coverage
is not considered as a primary selection aspect.

References
1. Foundations of forgetting and remembering ﬁnal report. Deliverable 2.4 for eu fp7: Project
forgetit-concisepreservationbycombiningmanagedforgettingandcontextualizationremem-
bering. https://www.forgetit-project.eu/ﬁleadmin/fm-dam/deliverables/ForgetIT_WP2_D2.
4.new.pdf. Accessed 10 July 2017
2. Foundations of forgetting and remembering intermediate report. Deliverable 2.3 for eu
fp:7 project forgetit - concise preservation by combining managed forgetting and con-
textualizationremembering.https://www.forgetit-project.eu/ﬁleadmin/fm-dam/deliverables/
ForgetIT_WP2_D2.3.pdf. Accessed 10 July 2017
3. Afrasiabi Rad, P., Nilsson, J., Päivärinta, T.: Administration of digital preservation services in
the cloud over time: design issues and challenges for organizations. In: Endicott-Popovsky B.
(ed.) The Proceedings of the 2nd International Conference on Cloud Security Management,
Proceedings of the International Conference on Cloud Security Management. Academic Con-
ferences and Publishing International Ltd (2014)
4. Agrawal, R., Gollapudi, S., Halverson, A., Ieong, S.: Diversifying search results. In: Proceed-
ings of the Second ACM International Conference on Web Search and Data Mining, WSDM
’09, pp. 5–14. ACM, New York, NY, USA (2009). http://doi.acm.org/10.1145/1498759.
1498766
5. Markatopoulou et al., F.: A study on the use of a binary local descriptor and color extensions of
local descriptors for video concept detection. In: MultiMedia Modeling Conference. LNCS,
vol. 8935, pp. 282–293. Springer, Berlin (2015)
6. Over et al., P.: Trecvid 2013 – an overview of the goals, tasks, data, evaluation mechanisms
and metrics. In: Proceedings of TRECVID 2013. NIST, USA (2013)
7. Alberdi, E.: Accommodating surprise in taxonomic tasks: a psychological and computational
investigation. Ph.D. thesis, University of Aberdeen (1996)
8. Alberdi, E., Sleeman, D.H.: Retax: a step in the automation of taxonomic revision. Artif.
Intell. 91(2), 257–279 (1997)
9. Alexe, B., Deselaers, T., Ferrari, V.: Measuring the objectness of image windows. IEEE Trans.
Pattern Anal. Mach. Intell. 34(11), 2189–2202 (2012)
10. Ames, M., Eckles, D., Naaman, M., Spasojevic, M., Van House, N.: Requirements for mobile
photoware. Pers. Ubiquitous Comput. 14(2), 95–109 (2010)
11. Anastasiu, D.C., Karypis, G.: L2ap: fast cosine similarity search with preﬁx l-2 norm bounds.
In: 30th International Conference on Data Engineering (ICDE), pp. 784–795. IEEE (2014)
12. Anderson, J.R., Schooler, L.J.: Reﬂections of the environment in memory. Psychol. Sci. 6(2),
396–408 (1991)
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1
315

316
References
13. Anderson, M.C., Baddeley, A., Eysenck, M., Anderson, M.: Retrieval. Memory pp. 163–189
(2009)
14. Andersson, I., Lindqvist, G., Randers, F.: Preservation services planning: a decision sup-
port framework. In: The Proceedings of the 2nd International Conference on Cloud Security
Management (2014)
15. Andoni, A.: Locality-sensitive hashing scheme based on p-stable distributions. Nearest neigh-
bor methods in learning and vision: Theory and practice (2006)
16. Apostolidis, E., Mezaris, V.: Fast shot segmentation combining global and local visual descrip-
tors. In: Proceedings of International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pp. 6583–6587. IEEE (2014)
17. Apostolidis, K., Papagiannopoulou, C., Mezaris, V.: CERTH at MediaEval 2014 synchroniza-
tion of multi-user event media task. In: Proceedings of the MediaEval Workshop (2014)
18. Apostolidis, K., Solachidis, V., Papadopoulou, O., Mezaris, V.: Photo collection contextual-
ization. In: 2015 IEEE International Conference on Multimedia Expo Workshops (ICMEW),
pp. 1–6 (2015). https://doi.org/10.1109/ICMEW.2015.7169761
19. Arandjelovic, R., Zisserman, A.: All about VLAD. In: Proceedings of the Conference on
Computer Vision and Pattern Recognition, pp. 1578–1585. IEEE (2013)
20. Argyriou, A., Evgeniou, T., Pontil, M.: Multi-task feature learning. In: Advances in Neural
Information Processing Systems (NIPS 2007). MIT Press, Cambridge (2007)
21. Argyriou, A., Evgeniou, T., Pontil, M.: Convex multi-task feature learning. Mach. Learn.
73(3), 243–272 (2008)
22. Awad, M., Khan, L., Thuraisingham, B.: Predicting www surﬁng using multiple evidence
combination. VLDB J. 17(3), 401–417 (2008). https://doi.org/10.1007/s00778-006-0014-1
23. Babenko, A., Lempitsky, V.: Aggregating local deep features for image retrieval. In: Pro-
ceedings of the International Conference on Computer Vision (ICCV), pp. 1269–1277. IEEE
(2015)
24. Backstrom, L., Leskovec, J.: Supervised random walks: predicting and recommending links in
social networks. In: Proceedings of the Fourth ACM International Conference on Web Search
and Data Mining, WSDM ’11, pp. 635–644. ACM, New York, NY, USA (2011). https://doi.
org/10.1145/1935826.1935914
25. Baddeley, A.: Essentials of Human Memory, pp. 139–144. Psychology Press, Abingdon
(1999)
26. Baddeley, A., Eysenck, M., Anderson, M.: Memory. Psychology Press, Abingdon (2009)
27. Baddeley, A., Eysenck, M., Anderson, M.: Memory. Taylor & Francis, New York (2015).
https://books.google.gr/books?id=nxNWBQAAQBAJ
28. Baddeley, A., Logie, R.: Working memory: the multiple-component model. In: Miyake, A.,
Shah, P. (eds.) Models of Working Memory: Mechanisms of Active Maintenance and Exec-
utive Control, pp. 28–61 (1999)
29. Baddeley, A.D., Hitch, G.: Working memory. Psychol. Learn. Motiv. 8, 47–89 (1974)
30. Baeza-Yates, R., Gionis, A., Junqueira, F., Murdock, V., Plachouras, V., Silvestri, F.: The
impact of caching on search engines. In: Proceedings of the 30th Annual International ACM
SIGIR Conference on Research and Development in Information Retrieval, SIGIR ’07, pp.
183–190. ACM, New York, NY, USA (2007). https://doi.org/10.1145/1277741.1277775
31. Bahrick, H.P.: Semantic memory content in permastore: ﬁfty years of memory for spanish
learned in school. J. Exp. Psychol. Gen. 113(1), 1 (1984)
32. Banks, R., Kirk, D., Sellen, A.: A design perspective on three technology heirlooms. Human-
Comput. Interact. 27(1–2), 63–91 (2012)
33. Barthel, R., Mackley, K.L., Hudson-Smith, A., Karpovich, A., de Jode, M., Speed, C.: An
internet of old things as an augmented memory system. Pers. Ubiquitous Comput. 17(2),
321–333 (2013)
34. Bartlett, F.C., Burt, C.: Remembering: a study in experimental and social psychology. B. J.
Educ. Psychol. 3(2), 187–192 (1933)
35. Barua, D., Kay, J., Kummerfeld, B., Paris, C.: Theoretical foundations for user-controlled for-
getting in scrutable long term user models. In: Proceedings of the 23rd Australian Computer-
Human Interaction Conference, pp. 40–49. ACM (2011)

References
317
36. Basave, A.E.C., Rizzo, G., Varga, A., Rowe, M., Stankovic, M., Dadzie, A.: Making sense of
microposts (#microposts2014) named entity extraction and linking challenge. In: 4th Work-
shop on Making Sense of Microposts (2014)
37. Bendersky, M., Croft, W.B.: Discovering key concepts in verbose queries. In: Proceedings
of the 31st Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’08, pp. 491–498 (2008)
38. Berberich, K., Bedathur, S., Alonso, O., Weikum, G.: A language modeling approach for
temporal information needs. In: Proceedings of the 32 European Conference on Advances in
Information Retrieval, ECIR’2010, pp. 13–25 (2010)
39. Berdan, R.: Composition and the elements of visual design. Photo composition articles (2004)
40. Bernardi, A., Grimnes, G.A., Groza, T., Scerri, S.: The NEPOMUK semantic desktop. In:
Warren, P., Davies, J., Simperl, E. (eds.) Context and Semantics for Knowledge Management
- Technologies for Personal Productivity. The NEPOMUK Semantic Desktop, pp. 255–274.
Springer, Berlin (2011)
41. Berntsen, D., Rubin, D.C.: Cultural life scripts structure recall from autobiographical memory.
Mem. Cognit. 32(3), 427–442 (2004)
42. Bhattacharya, S., Nojavanasghari, B., Chen, T., Liu, D., Chang, S.F., Shah, M.: Towards a
comprehensive computational model foraesthetic assessment of videos. In: Proceedings of
the 21st International Conference on Multimedia, pp. 361–364. ACM (2013)
43. Blanken, H.M., de Vries, A.P., Blok, H.E., Feng, L.: Multimedia Retrieval. Springer, Berlin
(2005)
44. Blei, D.M., Ng, A.Y., Jordan, M.I.: Latent dirichlet allocation. J. Mach. Learn. Res. (2003)
45. Bontcheva,K.,Derczynski,L.,Funk,A.,Greenwood,M.A.,Maynard,D.,Aswani,N.:TwitIE:
an open-source information extraction pipeline for microblog text. In: Proceedings of the
International Conference on Recent Advances in Natural Language Processing. Association
for Computational Linguistics (2013)
46. Bordes, A., Ertekin, S., Weston, J., Bottou, L.: Fast kernel classiﬁers with online and active
learning. J. Mach. Learn. Res. 6(Sep), 1579–1619 (2005)
47. Borghoff, U., Rdig, P., Scheffczyk, J.: Long-term preservation of digital documents. In: Uwe,
M.B., Rodig, P., Schmitz, L., Jan S. Principles and Practices, Online-Ausg. edn. Springer,
Berlin (2006). https://doi.org/10.1007/978-3-540-33640-2
48. Bos, E.: We can make forgetting impossible, but should we? Interactions 2(3), 11–14 (1995)
49. Bote, J., Fernandez-Feijoo, B., Ruiz, S.: The cost of digital preservation: a methodological
analysis. Proc. Technol. 5, 103–111 (2012)
50. Bottou, L.: Large-scale machine learning with stochastic gradient descent. In: Proceedings of
COMPSTAT, pp. 177–186. Springer (2010)
51. Brand, S.: Escaping the digital dark age. Libr. J. 124(2), 4669 (1999)
52. Bransford, J.: Human Cognition: Learning, Understanding, and Remembering. Thomson
Brooks/Cole (1979)
53. Brewer, W.F.: Qualitative Analysis of the Recalls of Randomly Sampled Autobiographical
Events. Cambridge University Press, Cambridge (1988)
54. Brighton, H., Mellish, C.: Advances in instance selection for instance-based learning algo-
rithms. Data Min. Knowl. Discov. 6(2), 153–172 (2002)
55. Brown, R., Kulik, J.: Flashbulb memories. Cognition 5(1), 73–99 (1977)
56. Broyden, C.G.: The convergence of a class of double rank minimization algorithms 1. general
considerations. J. Appl. Math. 6(1), 76–90 (1970)
57. Brubaker,J.R.,Callison-Burch,V.:Legacycontact:Designingandimplementingpost-mortem
stewardship at facebook. In: Proceedings of the 2016 CHI Conference on Human Factors in
Computing Systems, CHI ’16, pp. 2908–2919. ACM, New York, NY, USA (2016). https://
doi.org/10.1145/2858036.2858254
58. Cabeza, R., St Jacques, P.: Functional neuroimaging of autobiographical memory. Trends
Cognit. Sci. 11(5), 219–227 (2007)
59. Camargo, J.E., González, F.A.: Multimodal latent topic analysis for image collection sum-
marization. Inf. Sci. 328, 270–287 (2016)

318
References
60. Campos, V., Salvador, A.: Giro-i Nieto, X., Jou, B.: Diving deep into sentiment: Understanding
ﬁne-tuned CNNs for visual sentiment prediction. In: Proceedings of the 1st International
Workshop on Affect and Sentiment in Multimedia. ASM ’15, pp. 57–62. ACM, New York,
NY, USA (2015)
61. Carmel, D., Cohen, D., Fagin, R., Farchi, E., Herscovici, M., Maarek, Y.S., Soffer, A.: Static
index pruning for information retrieval systems. In: Proceedings of the 24th Annual Inter-
national ACM SIGIR Conference on Research and Development in Information Retrieval,
SIGIR ’01, pp. 43–50. ACM, New York, NY, USA (2001). https://doi.org/10.1145/383952.
383958
62. Carmel,D.,Kurland,O.:Queryperformancepredictionforir.In:Proceedingsofthe35thInter-
national ACM SIGIR Conference on Research and Development in Information Retrieval,
SIGIR ’12, pp. 1196–1197 (2012)
63. Carmel, D., Yom-Tov, E.: Estimating the query difﬁculty for information retrieval. In: Pro-
ceedings of the 33rd International ACM SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’10, pp. 911–911 (2010)
64. Carpenter, G.A., Grossberg, S.: Adaptive Resonance Theory. Springer, Berlin (2017)
65. Cauwenberghs, G., Poggio, T.: Incremental and decremental support vector machine learn-
ing. In: Proceedings of the 13th International Conference on Neural Information Processing
Systems, NIPS’00, pp. 388–394 (2000)
66. CCSDS: Producer-Archive Interface Methodology Abstract Standard (PAIMAS) - Recom-
mended Practice, CCSDS 651.0-M-1 (Magenta Book) Issue 1. Equivalent to ISO 20652:2006
(2004). http://public.ccsds.org/publications/archive/651x0m1.pdf. Accessed Sep 2017
67. CCSDS: Reference Model for an Open Archival Information System (OAIS) - Recommended
Practice, CCSDS 650.0-M-2 (Magenta Book) Issue 2. Equivalent to ISO 14721:2012 (2012).
http://public.ccsds.org/publications/archive/650x0m2.pdf. Accessed Sep. 2017
68. CCSDS Secretariat, S.C., Ofﬁce, N.: Reference Model for an Open Archival Information
System (OAIS). Space Operations Mission Directorate (2009)
69. Ceroni, A., Solachidis, V., Fu, M., Kanhabua, N., Papadopoulou, O., Niederée, C., Mezaris, V.:
Investigating human behaviors in selecting personal photos to preserve memories. In: Proceed-
ings of the 2015 IEEE International Conference on Multimedia and Expo (ICME) Workshop
on Human Memory-Inspired Multimedia Organization and Preservation (HMMP’15) (2015)
70. Ceroni,A.,Solachidis,V.,Niederée,C.,Papadopoulou,O.,Kanhabua,N.,Mezaris,V.:Tokeep
or not to keep: an expectation–oriented photo selection method for personal photo collections.
In: Proceedings of the ACM International Conference on Multimedia Retrieval (ICMR’2015)
(2015)
71. Ceroni, A., Tran, N.K., Kanhabua, N., Niederée, C.: Bridging temporal context gaps using
time-aware re-contextualization. In: Proceedings of the 37th International ACM SIGIR Con-
ference on Research and Development in Information Retrieval, SIGIR ’14, pp. 1127–1130
(2014)
72. Chang,Y.W.,Lin,C.J.:FeaturerankingusinglinearSVM.In:ProceedingsofWCCICausation
and Prediction Challenge, pp. 53–64 (2008)
73. Chappell, D.: Enterprise Service Bus. O’Reilly Media, Inc., California (2004)
74. Chatﬁeld, K., Simonyan, K., Vedaldi, A., Zisserman, A.: Return of the devil in the details:
delving deep into convolutional nets. In: British Machine Vision Conference (2014)
75. Chen, J., Hibino, S.: Reminiscing view: Event-based browsing of consumer’s photo and video-
clip collections. In: Tenth IEEE International Symposium on Multimedia, pp. 23–30 (2008).
https://doi.org/10.1109/ISM.2008.104
76. Chen, R.C., Lee, C.J., Croft, W.B.: On divergence measures and static index pruning. In:
Proceedings of the 2015 International Conference on The Theory of Information Retrieval,
ICTIR ’15, pp. 151–160. ACM, New York, NY, USA (2015). https://doi.org/10.1145/
2808194.2809472
77. Chen, Y.: Information valuation for information lifecycle management. In: Proceedings of
International Conference on Autonomic Computing (2005)

References
319
78. Chu, W.T., Lin, C.H.: Automatic selection of representative photo and smart thumbnailing
using near-duplicate detection. In: Proceedings of the 16th ACM International Conference on
Multimedia, MM ’08, pp. 829–832. ACM (2008)
79. Ciglan, M., Nørvåg, K.: Wikipop: personalized event detection system based on Wikipedia
pageviewstatistics.In:Proceedingsofthe19thACMInternationalConferenceonInformation
and Knowledge Management, CIKM ’10, pp. 1931–1932 (2010)
80. Cocchini, G., Logie, R.H., Della Sala, S., MacPherson, S.E., Baddeley, A.D.: Concurrent
performance of two memory tasks: evidence for domain-speciﬁc working memory systems.
Mem. Cognit. 30(7), 1086–1095 (2002)
81. Cohen, J., Mihailidis, P.: Storify and news curation: teaching and learning about digital sto-
rytelling. Second Ann. Soc. Media Technol. Conf. Workshop 1, 27–31 (2012)
82. Conci, N., De Natale, F.G., Mezaris, V., Matton, M.: Synchronization of multi-user event
media (SEM) at mediaeval 2014: task description, datasets, and evaluation. In: MediaEval
(2014)
83. Conci, N., Natale, F.D., Mezaris, V.: Synchronization of multi-user event media (SEM) at
mediaeval 2014: task description, datasets, and evaluation. In: MediaEval 2014 Workshop,
Barcelona, Spain (2014)
84. Connor, M., Spitzer, S.: The places freqency algorithm. https://developer.mozilla.org/en-US/
docs/Mozilla/Tech/Places/Frecency_algorithm (2015). Accessed 31 Aug 2015
85. Conway, M.A.: Memory and the self. J. Mem. Lang. 53(4), 594–628 (2005)
86. Conway, M.A.: Episodic memories. Neuropsychologia 47(11), 2305–2313 (2009)
87. Conway, M.A., Anderson, S.J., Larsen, S.F., Donnelly, C.M., McDaniel, M.A., McClelland,
A.G., Rawles, R.E., Logie, R.H.: The formation of ﬂashbulb memories. Mem. Cognit. 22(3),
326–343 (1994)
88. Conway, M.A., Cohen, G., Stanhope, N.: Very long-term memory for knowledge acquired at
school and university. Appl. Cognit. Psychol. 6(6), 467–482 (1992)
89. Conway, M.A., Loveday, C.: Accessing autobiographical memories. Act of Remembering:
Toward an Understanding of How We Recall the Past, pp. 56–70 (2010)
90. Conway, M.A., Loveday, C.: Accessing Autobiographical Memories, the Act of Remember-
ing: Toward an Understanding of How We Recall the Past, pp. 56–70. Wiley Blackwell, New
Jersey (2010)
91. Conway, M.A., Pleydell-Pearce, C.W.: The construction of autobiographical memories in the
self-memory system. Psychol. Rev. 107(2), 261 (2000)
92. Conway,P.:Overview:RationaleforDigitizationandPreservation,Chap.2,pp.5–20.NEDCC
(2000)
93. Cook, T.: Macroappraisal in theory and practice: origins, characteristics, and implementation
in Canada, 19502000. Arch. Sci. 5(2–4), 101–161 (2005). http://dx.doi.org/10.1007/s10502-
005-9010-2
94. Cooper, M., Foote, J., Girgensohn, A., Wilcox, L.: Temporal event clustering for digital photo
collections. ACM Trans. Multimed. Comput. Commun. Appl. (TOMM) 1(3), 269–288 (2005)
95. Copeland, A.: The use of personal value estimations to select images for preservation in public
library digital community collections. Future Internet 6(2) (2014)
96. Cosley, D., Sosik, V.S., Schultz, J., Peesapati, S.T., Lee, S.: Experiences with designing tools
for everyday reminiscing. Hum. Comput. Interact. 27(1–2), 175–198 (2012)
97. da Costa Pinto, A.a.n., Baddeley, A.D.: Where did you park your car? Analysis of a naturalistic
long-term recency effect. Eur. J. Cognit. Psychol. 3(3), 297–313 (1991)
98. Cowan, N.: Working Memory Capacity. Psychology Press (2012)
99. Crestani, F., Lee, P.L.: Searching the web by constrained spreading activation. Inf. Process.
Manag. 36(4), 585–605 (2000). https://doi.org/10.1016/S0306-4573(99)00073-4. http://
www.sciencedirect.com/science/article/pii/S0306457399000734
100. Crovitz, H.F., Schiffman, H.: Frequency of episodic memories as a function of their age. Bull.
Psychon. Soc. 4(5), 517–518 (1974)
101. Csurka, G., Dance, C., Fan, L., Willamowski, J., Bray, C.: Visual categorization with bags of
keypoints. In: ECCV Workshop on Statistical Learning in Computer Vision, 1–22, pp. 1–2.
Prague (2004)

320
References
102. Cubelli, R., Della Sala, S.: Flashbulb memories: special but not iconic. Cortex 44(7), 908–909
(2008)
103. Cubelli, R., Della Sala, S.: Flashbulb memories: sorry, no ﬂash!. Cortex 1(49), 356–357 (2013)
104. Cunningham, H., Tablan, V., Roberts, I., Greenwood, M.A., Aswani, N.: Information
extraction and semantic annotation for multi-paradigm information management. In: Lupu,
M., Mayer, K., Tait, J., Trippe, A.J. (eds.) Current Challenges in Patent Information Retrieval.
The Information Retrieval Series, vol. 29, pp. 307–327. Springer, Berlin (2011). https://doi.
org/10.1007/978-3-642-19231-9_15
105. Curci, A., Conway, M.A.: Playing the ﬂashbulb memory game: a comment on cubelli and
della sala. Cortex 49(1), 352–355 (2013)
106. Cutrell, E., Dumais, S.T., Teevan, J.: Searching to eliminate personal information manage-
ment. Commun. ACM 49(1), 58–64 (2006)
107. Daelemans, W., Van Den Bosch, A., Zavrel, J.: Forgetting exceptions is harmful in language
learning. Mach. Learn. 34(1–3), 11–41 (1999)
108. Dang, V., Croft, W.B.: Feature selection for document ranking using best ﬁrst search and
coordinate ascent. In: Proceedings of SIGIR’10 Workshop on Feature Generation and Selec-
tion for Information Retrieval (2010)
109. Das, M., Loui, A.: Detecting signiﬁcant events in personal image collections. In: IEEE Inter-
national Conference on Semantic Computing, 2009. ICSC ’09, pp. 116–123 (2009)
110. Datta, R., Li, J., Wang, J.Z.: Studying aesthetics in photographic images using a computational
approach. US Patent App. 13/542,326, 2012
111. Datta, R., Wang, J.Z.: ACQUINE: aesthetic quality inference engine-real-time automatic
rating of photo aesthetics. In: Proceedings of the International Conference on Multimedia
information retrieval, pp. 421–424. ACM (2010)
112. Daumé III, H.: Bayesian multitask learning with latent hierarchies. In: Proceedings of the
25th Conference on Uncertainty in Artiﬁcial Intelligence (UAI ’09), pp. 135–142. AUAI
Press, Arlington, Virginia, US (2009)
113. De Groot, A.D.: Thought and Choice in Chess, vol. 4. Walter de Gruyter GmbH & Co KG
(1978)
114. Dengel, A. (ed.): Semantische Technologien. Spektrum (2012)
115. Dhar, S., Ordonez, V., Berg, T.L.: High level describable attributes for predicting aesthetics and
interestingness. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 1657–1664 (2011)
116. Djafari Naini, K., Altingovde, I.S.: Exploiting result diversiﬁcation methods for feature selec-
tion in learning to rank. In: de Rijke, M., Kenter, T., de Vries, A.P., Zhai, C., de Jong, F., Radin-
sky, K. Hofmann, K. (eds.) Advances in Information Retrieval: 36th European Conference
on IR Research, ECIR 2014, Amsterdam, The Netherlands, April 13–16, 2014. Proceedings,
pp. 455–461. Springer International Publishing, Cham (2014). https://doi.org/10.1007/978-
3-319-06028-6_41
117. Djafari Naini, K., Kawase, R., Kanhabua, N., Niederee, C.: Characterizing high-impact fea-
tures for content retention in social web applications. In: Proceedings of the 23rd International
Conference on World Wide Web, WWW ’14 Companion, pp. 559–560. ACM, New York,
NY, USA (2014). https://doi.org/10.1145/2567948.2576954
118. Douze,M.,Jégou,H.,Sandhawalia,H.,Amsaleg,L.,Schmid,C.:Evaluationofgistdescriptors
for web-scale image search. In: Proceedings of the International Conference on Image and
Video Retrieval, p. 19. ACM (2009)
119. Drucker, H., Burges, C.J.C., Kaufman, L., Smola, A.J., Vapnik, V.: Support vector regression
machines. In: Mozer, M.C., Jordan, M.I., Petsche T. (eds.) Advances in Neural Information
Processing Systems, vol. 9, pp. 155–161 (1997)
120. Duan, L.Y., Lin, J., Wang, Z., Huang, T., Gao, W.: Weighted component hashing of binary
aggregated descriptors for fast visual search. IEEE Trans. Multimed. 17(6), 828–842 (2015)
121. Eaton, E., Ruvolo, P.L.: Ella: An efﬁcient lifelong learning algorithm. In: Dasgupta, S.,
Mcallester D. (eds.) Proceedings of the 30th International Conference on Machine Learn-
ing (ICML-13), vol. 28, pp. 507–515. JMLR Workshop (2013)

References
321
122. Ebbinghaus, H.: Memory: A Contribution to Experimental Psychology (1964)
123. EBU: Class Conceptual Data Model (CCDM) - TECH 3351. https://tech.ebu.ch/metadata
(2016). Accessed Sep 2017
124. Eder, J., Koncilia, C.: Modelling changes in ontologies. In: Proceedings of the On
The Move - Federated Conferences (OTM 2004) (2004)
125. Edworthy, J., Hellier, E.: Fewer but better auditory alarms will improve patient safety. In:
BMJ Quality and Safety 14(3), 212–215 (2005). https://doi.org/10.1136/qshc.2004.013052.
http://qualitysafety.bmj.com/content/14/3/212
126. Eiter, T., Wang, K.: Semantic forgetting in answer set programming. Artif. Intell. 172(14),
1644–1672 (2008)
127. Eldesouky, B., Bakry, M., Maus, H., Dengel, A.: Supporting early contextualization of textual
content in digital documents on the web. In: Proceedings of the 13th International Conference
on Document Analysis and Recognition. Nancy, France (2015)
128. Eldesouky, B., Bakry, M., Maus, H., Dengel, A.: Seed, an end-user text composition tool
for the semantic web. In: The Semantic Web - ISWC 2016, 15th International Semantic
Web Conference, Proceedings. International Semantic Web Conference (ISWC-2016), 15th,
October 17–21, Kobe, Japan. Springer (2016)
129. Eldesouky, B., Maus, H., Schwarz, S., Dengel, A.: Seed, a natural language interface to knowl-
edge bases. In: Yamamoto, S. (ed.) Human Interface and the Management of Information.
Information and Knowledge Design, LNCS, vol. 9172, pp. 280–290. Springer (2015)
130. Ericcson, K.A., Chase, W.G., Faloon, S.: Acquisition of a memory skill. Science 208(4448),
1181–1182 (1980)
131. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The PASCAL
Visual Object Classes Challenge 2012 (VOC2012) Results
132. Evgeniou, T., Pontil, M.: Regularized multi–task learning. In: Proceedings of the 10th ACM
SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD ’04),
pp. 109–117. ACM, NY, USA (2004)
133. Fagni, T., Perego, R., Silvestri, F., Orlando, S.: Boosting the performance of web search
engines: caching and prefetching query results by exploiting historical usage data. ACM
Trans. Inf. Syst. 24(1), 51–78 (2006). https://doi.org/10.1145/1125857.1125859
134. Faidi, T., Chaieb, F., Ghorbel, F.: A new multi-resolution afﬁne invariant planar contour
descriptor. In: International Conference on Image Analysis and Processing, pp. 494–505.
Springer (2015)
135. Fang, Y., Chang, M.W.: Entity linking on microblogs with spatial and temporal signals. Trans.
Assoc. Comput. Linguist. 2, 259–272 (2014)
136. Ferragina, P., Scaiella, U.: Fast and accurate annotation of short texts with wikipedia pages.
IEEE Softw. 70–75 (2012)
137. Firth, J.R.: A Synopsis of Linguistic Theory, 1930–1955. Special Volume of The Philological
Society (1957)
138. Fischer, P., Dosovitskiy, A., Brox, T.: Descriptor Matching with Convolutional Neural Net-
works: a Comparison to Sift (2014). arXiv:1405.5769
139. Fitchett, S., Cockburn, A.: Accessrank: Predicting what users will do next. In: Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems, CHI ’12, pp. 2239–2242.
ACM, New York, NY, USA (2012). https://doi.org/10.1145/2207676.2208380
140. Fitzgerald, J.M., Lawrence, R.: Autobiographical memory across the life-span. J. Gerontol.
39(6), 692–698 (1984)
141. Forman, G.: An extensive empirical study of feature selection metrics for text classiﬁcation.
J. Mach. Learn. Res. 3, 1289–1305 (2003)
142. Frohlich, D., Fennell, J.: Sound, paper and memorabilia: resources for a simpler digital pho-
tography. Personal Ubiquitous Comput. 11(2), 107–116 (2007)
143. Fu, M., Ceroni, A., Solachidis, V., Niederée, C., Papadopoulou, O., Kanhabua, N., Mezaris,
V.: Learning personalized expectation-oriented photo selection models for personal photo
collections. In: IEEE International Conference on Multimedia and Expo Workshops, ICME
Workshops, pp. 1–6 (2015)

322
References
144. Galton, F.: Psychometric experiments. Brain 2(2), 149–162 (1879)
145. Gargi, U.: Consumer media capture: Time-based analysis and event clustering. HP-Labs Tech
Report (2003)
146. Geng, X., Liu, T.Y., Qin, T., Li, H.: Feature selection for ranking. In: Proceedings of SIGIR’07,
pp. 407–414 (2007)
147. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accurate object
detection and semantic segmentation. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2014)
148. Gladney, H.M.: Preserving Digital Information. Springer, Berlin (2007). https://doi.org/10.
1007/978-3-540-37887-7
149. Glück, J., Bluck, S.: Looking back across the life span: a life story account of the reminiscence
bump. Mem. Cognit. 35(8), 1928–1939 (2007)
150. Goetz, T.: Harnessing the Power of Feedback Loops. WIRED Magazine (2011). http://www.
wired.com/magazine/2011/06/ff_feedbackloop/
151. Goldberg, D., Nichols, D., Oki, B.M., Terry, D.: Using collaborative ﬁltering to weave an
information tapestry. Commun. ACM 35(12), 61–70 (1992). https://doi.org/10.1145/138859.
138867
152. Gorrell, G., Petrak, J., Bontcheva, K.: Using @Twitter conventions to improve #lod-based
named entity disambiguation. In: The Semantic Web. Latest Advances and New Domains,
pp. 171–186. Springer, Berlin (2015)
153. Gowans, G., Campbell, J., Alm, N., Dye, R., Astell, A., Ellis, M.: Designing a multimedia con-
versation aid for reminiscence therapy in dementia care environments. In: CHI’04 Extended
Abstracts on Human Factors in Computing Systems, pp. 825–836. ACM (2004)
154. Grand Challenge at ACM Multimedia Conference (MM’13): NHK Where is beauty
(2013). http://acmmm13.org/subm-issions/call-for-multimedia-grand-challenge-solutions/
task-where-is-beauty/
155. Greenberg, D.L.: President bush’s false [ﬂashbulb] memory of 9/11/01. Appl. Cognit. Psychol.
18(3), 363–370 (2004)
156. Grimnes, G.A., Adrian, B., Schwarz, S., Maus, H., Schumacher, K., Sauermann, L.: Semantic
desktop for the end-user. i-com 8(3), 25–32 (2009)
157. Guo, S., Chang, M.W., Kıcıman, E.: To link or not to link? a study on end-to-end tweet entity
linking. NAACL-HLT 2013, 1020–1030 (2013)
158. Hanley, J.A., McNeil, B.J.: The meaning and use of the area under a receiver operating
characteristic (roc) curve. Radiology 143(1), 29–36 (1982)
159. Harada, S., Naaman, M., Song, Y.J., Wang, Q., Paepcke, A.: Lost in memories: interacting
with photo collections on PDAS. In: Proceedings of the 4th ACM/IEEE-CS Joint Conference
on Digital Libraries, pp. 325–333. ACM (2004)
160. Harvey, R.: Appraisal and Selection, Chapter 1, pp. 6–39. Digital Curation Center (2007)
161. Harvey, R., Thompson, D.: Automating the appraisal of digital materials. Libr. Hi Tech 28(2),
313–322 (2010). https://doi.org/10.1108/07378831011047703
162. He, B., Ounis, I.: Inferring query performance using pre-retrieval predictors. In: Proceedings
of the Symposium on String Processing and Information Retrieval (2004)
163. Henkel, L.A.: Point-and-shoot memories: the inﬂuence of taking photos on memory for a
museum tour. Psychol. Sci. 25(2), 396–402 (2014)
164. Heutelbeck, D., Brunsmann, J., Wilkes, W., Hundsdörfer, A.: Motivations and challenges
for digital preservation in design and engineering InDP 2009. In: Proceedings of the First
International Workshop on Innovation in Digital Preservation in Conjunction with JCDL
2009. Austin TX, USA (2009)
165. Ho, W.C., Dautenhahn, K., Lim, M.Y., Vargas, P.A., Aylett, R., Enz, S.: An initial memory
model for virtual and robot companions supporting migration and long-term interaction. In:
The 18th IEEE International Symposium on Robot and Human Interactive Communication,
2009. RO-MAN 2009, pp. 277–284. IEEE (2009)
166. Ho, W.C., Dautenhahn, K., Nehaniv, C.L.: Computational memory architectures for autobio-
graphic agents interacting in a complex virtual environment: a working model. Connect. Sci.
20(1), 21–65 (2008)

References
323
167. Hodges,S.,Berry,E.,Wood,K.:Sensecam:awearablecamerathatstimulatesandrehabilitates
autobiographical memory. Memory 19(7), 685–696 (2011)
168. Hoffart, J., Yosef, M.A., Bordino, I., Fürstenau, H., Pinkal, M., Spaniol, M., Taneva, B.,
Thater, S., Weikum, G.: Robust disambiguation of named entities in text. In: Proceedings
of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,
pp. 782–792 (2011)
169. Hohman, T.J., Peynircio˘glu, Z.F., Beason-Held, L.L.: Flexibility of event boundaries in auto-
biographical memory. Memory 21(2), 249–260 (2013)
170. Hohpe, G., Woolf, B.: Enterprise Integration Patterns: Designing, Building, and Deploying
Messaging Solutions. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA
(2003)
171. Hook, E.: Where to Position that Horizon? (Published in Digital Photography School, May,
2013). http://goo.gl/vU1g46
172. van den Hoven, E., Eggen, B.: Informing augmented memory system design through autobi-
ographical memory theory. Personal Ubiquitous Comput. 12(6), 433–443 (2008)
173. Hutaˇr, J.: Archives New Zealand Migration from Fedora Commons to the Rosetta Digital
Preservation System. iPRES2013 Proceedings (2013)
174. Hyun, J., Kim, H.K., Oh, W.G.: Study on performance of mpeg-7 visual descriptors for
deformable object retrieval. In: 21st Korea-Japan Joint Workshop on Frontiers of Computer
Vision (FCV), pp. 1–5. IEEE (2015)
175. Ingwersen, P., Järvelin, K.: The Turn: Integration of Information Seeking and Retrieval in
Context (The Information Retrieval Series). Springer, New York Inc., Secaucus, NJ, USA
(2005)
176. ISO/IEC: FDIS 23000-15 - Information technology – Multimedia application format (MPEG-
A) – Part 15: Multimedia preservation application. http://www.iso.org/iso/catalogue_detail.
htm?csnumber=66430 (2016). Accessed Sep 2017
177. Isurin, L., Mcdonald, J.L.: Retroactive interference from translation equivalents: implications
for ﬁrst language forgetting. Mem. Cognit. 29(2), 312–319 (2001)
178. Jégou, H., Chum, O.: Negative evidences and co-occurences in image retrieval: the beneﬁt of
pca and whitening. Comput. Vis. ECCV 2012, 774–787 (2012)
179. Jegou, H., Douze, M., Schmid, C.: Hamming embedding and weak geometric consistency for
large scale image search. Comput. Vis. ECCV 304–317 (2008)
180. Jégou, H., Douze, M., Schmid, C., Pérez, P.: Aggregating local descriptors into a compact
image representation. In: Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 3304–3311. IEEE (2010)
181. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,
Darrell, T.: Caffe: Convolutional architecture for fast feature embedding. In: Proceedings of
the 22nd ACM International Conference on Multimedia, pp. 675–678. ACM (2014)
182. Jiang, Y.G., Ngo, C.W., Yang, J.: Towards optimal bag-of-features for object categorization
and semantic video retrieval. In: Proceedings of the 6th ACM International Conference on
Image and Video Retrieval, CIVR ’07, pp. 494–501. ACM, NY, USA (2007)
183. Jilek, C., Maus, H., Schwarz, S., Dengel, A.: Diary generation from personal information
models to support contextual remembering and reminiscence. In: IEEE International Con-
ference on Multimedia and Expo Workshops, ICME Workshops 2015, Turin, Italy, pp. 1–6
(2015)
184. Jinda-Apiraksa, A., Vonikakis, V., Winkler, S.: California-ND: an annotated dataset for near-
duplicate detection in personal photo collections. In: 5th International Workshop on Quality
of Multimedia Experience (QoMEX), pp. 142–147. IEEE (2013)
185. Joachims, T.: Optimizing search engines using clickthrough data. In: Proceedings of KDD’02,
pp. 133–142 (2002)
186. John, J.L., Rowlands, I., Williams, P., Dean, K.: Digital lives: Personal digital archives for
the 21st century – an initial synthesis. Research paper Beta Version 0.2, British Library
(2010). http://britishlibrary.typepad.co.uk/ﬁles/digital-lives-synthesis02-1.pdf. Accessed 16
Oct 2016

324
References
187. John Walker, S.: Big Data: A Revolution that Will Transform How We Live, Work, and Think
(2014)
188. Jones, W.: Keeping Found Things Found: The Study and Practice of Personal Information
Management. Morgan Kaufmann (2007)
189. Julius, M.: Themen in Sozialen Netzwerken (topics in social networks). blog (2013). http://
multimedia-memories.tumblr.com/post/45185122672/. Accessed 16 Oct 2017
190. Kalnikait˙e, V., Whittaker, S.: A saunter down memory lane: digital reﬂection on personal
mementos. Int. J. Hum. Comput. Stud. 69(5), 298–310 (2011)
191. Kanhabua, N., Niederée, C., Siberski, W.: Towards concise preservation by managed forget-
ting: Research issues and case study. In: Proceedings of the 10th International Conference on
Preservation of Digital Objects (iPres) (2013)
192. Kanhabua, N., Nørvåg, K.: Determining time of queries for re-ranking search results. In:
Proceedings of the 14th European Conference on Research and Advanced Technology for
Digital Libraries, ECDL’10, pp. 261–272 (2010)
193. Kao, S.H., Day, W.Y., Cheng, P.J.: An aesthetic-based approach to re-ranking web images.
In: Information Retrieval Technology, pp. 610–623. Springer, Berlin (2010)
194. Kärkkäinen, T., Kaakinen, A., Vainio, T., Väänänen-Vainio-Mattila, K.: Design guidelines
for managing metadata for personal content on mobile devices. In: Proceedings of the 7th
International Conference on Mobile and Ubiquitous Multimedia, pp. 162–167. ACM (2008)
195. Karpicke, J.D., Roediger, H.L.: Repeated retrieval during learning is the key to long-term
retention. J. Mem. Lang. 57(2), 151–162 (2007)
196. Katifori, A., Vassilakis, C., Dix, A.: Ontologies and the brain: Using spreading activation
through ontologies to support personal interaction. Cognit. Syst. Res. 11(1), 25 – 41 (2010).
https://doi.org/10.1016/j.cogsys.2009.02.001. http://www.sciencedirect.com/science/article/
pii/S1389041709000138. Brain Informatics
197. Kawase, R., Papadakis, G., Herder, E., Nejdl, W.: Beyond the usual suspects: context-aware
revisitation support. In: ACM Hypertext, pp. 27–36 (2011). https://doi.org/10.1145/1995966.
1995974
198. Kaya Kızılöz, B., Tekcan, A.I.: Canonical categories in ﬂashbulb memories. Appl. Cognit.
Psychol. 27(3), 352–359 (2013)
199. Kaye, J.J., Vertesi, J., Avery, S., Dafoe, A., David, S., Onaga, L., Rosero, I., Pinch, T.: To have
and to hold: exploring the personal archive. In: Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, CHI ’06, pp. 275–284. ACM, New York, NY, USA
(2006). https://doi.org/10.1145/1124772.1124814
200. Ke, Y., Tang, X., Jing, F.: The design of high-level features for photo quality assessment. In:
IEEE Computer Society Conference on Computer Vision and Pattern Recognition, vol. 1, pp.
419–426 (2006)
201. Kejser, U.B., Nielsen, A.B., Thirifays, A.: Cost model for digital preservation: cost of digital
migration. IJDC 6(1), 255–267 (2011)
202. Kelly, P., Marshall, S.J., Badland, H., Kerr, J., Oliver, M., Doherty, A.R., Foster, C.: An ethical
framework for automated, wearable cameras in health behavior research. Am. J. Prev. Med.
44(3), 314–319 (2013)
203. Kesireddi, R.C., Raj, G.: Embedding, extracting and matching of ﬁngerprint images using
digital watermarking. In: Proceedings of the International Conference on Advanced Research
in Computer Science Engineering and Technology (ICARCSET), ICARCSET ’15. ACM,
New York, NY, USA (2015)
204. Kimeldorf, G., Wahba, G.: Some results on Tchebychefﬁan spline functions. J. Math. Anal.
Appl. 33(1), 82–95 (1971)
205. Kindberg, T., Spasojevic, M., Fleck, R., Sellen, A.: How and why people use camera phones.
HP Laboratories Technical Report HPL-2004–216 (2004)
206. Kirk, D., Sellen, A., Rother, C., Wood, K.: Understanding photowork. In: Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems, pp. 761–770. ACM (2006)
207. Koppel, J., Brown, A.D., Stone, C.B., Coman, A., Hirst, W.: Remembering president barack
obama’s inauguration and the landing of us airways ﬂight 1549: a comparison of the predictors
of autobiographical and event memory. Memory 21(7), 798–806 (2013)

References
325
208. Koutstaal, W., Schacter, D.L., Johnson, M.K., Angell, K.E., Gross, M.S.: Post-event review
in older and younger adults: improving memory accessibility of complex everyday events.
Psychol. Aging 13(2), 277 (1998)
209. Koutstaal, W., Schacter, D.L., Johnson, M.K., Galluccio, L.: Facilitation and impairment of
event memory produced by photograph review. Mem. Cognit. 27(3), 478–493 (1999)
210. Krizhevsky, A., Ilya, S., Hinton, G.: ImageNet classiﬁcation with deep convolutional neural
networks. In: Advances in Neural Information Processing Systems (NIPS 2012), pp. 1097–
1105. Curran Associates, Inc., New York (2012)
211. Kumar, A., Daume, H.: Learning task grouping and overlap in multi-task learning. In: Lang-
ford, J., Pineau, J. (eds.) Proceedings of the 29th International Conference on Machine Learn-
ing (ICML-12), pp. 1383–1390. ACM, NY, USA (2012)
212. Kuny, T.: A digital dark ages? Challenges in the preservation of electronic information. In:
63RD IFLA Council and General Conference, Workshop on Audiovisual and Multimedia
joint with Preservation and Conservation, Information Technology, Library Buildings and
Equipment, and the PAC Core Programme (1997)
213. Kurby, C.A., Zacks, J.M.: Segmentation in the perception and memory of events. Trends
Cognit. Sci. 12(2), 72–79 (2008)
214. Kuwahara, N., Abe, S., Yasuda, K., Kuwabara, K.: Networked reminiscence therapy for
individuals with dementia by using photo and video sharing. In: Proceedings of the 8th
international ACM SIGACCESS Conference on Computers and Accessibility, pp. 125–132.
ACM (2006)
215. Kvavilashvili, L., Mirani, J., Schlagman, S., Kornbrot, D.E.: Comparing ﬂashbulb memories
of september 11 and the death of princess diana: effects of time delays and nationality. Appl.
Cognit. Psychol. 17(9), 1017–1031 (2003)
216. Lakatos, I.: Proofs and refutations (i). Br. J. Philos. Sci. 14(53), 1–25 (1963)
217. Lampasona, C., Rostanin, O., Maus, H.: Seamless integration of order processing in ms
outlook using smartofﬁce: an empirical evaluation. In: ESEM ’12 Proceedings of the ACM-
IEEE International Symposium on Empirical Software Engineering and Measurement. ACM
Press (2012). https://doi.org/10.1145/2372251.2372281
218. Landauer, T.K., Bjork, R.A.: Optimum rehearsal patterns and name learning. Pract. Asp.
Mem. 1, 625–632 (1978)
219. Lang, J., Marquis, P.: Reasoning under inconsistency: a forgetting-based approach. Artif.
Intell. 174(12–13), 799–823 (2010)
220. Lantz, B.: Photography Composition Tips and Techniques (Published in Macro Photography,
2013). http://goo.gl/fsx20y
221. Lappas, T., Arai, B., Platakis, M., Kotsakos, D., Gunopulos, D.: On burstiness-aware search
for document sequences. In: Proceedings of the 15th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, KDD ’09, pp. 477–486 (2009)
222. Lavoie, B., Dempsey, L.: Thirteen ways of looking at...digital preservation. D-Lib Mag.
10(7/8) (2004). http://dblp.uni-trier.de/db/journals/dlib/dlib10.html
223. LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444 (2015)
224. Lee, C.J., Chen, R.C., Kao, S.H., Cheng, P.J.: A term dependency-based approach for query
terms ranking. In: Proceedings of the 18th ACM Conference on Information and Knowledge
Management, CIKM ’09, pp. 1267–1276 (2009)
225. Lehmann, J., Gonçalves, B., Ramasco, J.J., Cattuto, C.: Dynamical classes of collective atten-
tion in twitter. In: Proceedings of the 21st International Conference on World Wide Web,
WWW ’12, pp. 251–260 (2012)
226. Lei, Y., Zheng, L., Huang, J.: Geometric invariant features in the radon transform domain for
near-duplicate image detection. Pattern Recognit. Lett. 47(11), 3630–3640 (2014)
227. Levine, B., Svoboda, E., Hay, J.F., Winocur, G., Moscovitch, M.: Aging and autobiographical
memory: dissociating episodic from semantic retrieval. Psychol. Aging 17(4), 677 (2002)
228. Li,C.,Loui,A.C.,Chen,T.:Towardsaesthetics:aphotoqualityassessmentandphotoselection
system. In: Proceedings of the International Conference on Multimedia, pp. 827–830. ACM
(2010)

326
References
229. Li, J., Lim, J.H., Tian, Q.: Automatic summarization for personal digital photos. In: Fourth
International Conference on Information, Communications and Signal Processing, 2003 and
the Fourth Paciﬁc Rim Conference on Multimedia. Proceedings of the 2003 Joint, vol. 3, pp.
1536–1540 vol.3 (2003)
230. Li, X., Larson, M., Hanjalic, A.: Pairwise geometric matching for large-scale object retrieval.
In: Proceedings of the Conference on Computer Vision and Pattern Recognition, pp. 5153–
5161. IEEE (2015)
231. Li, Y., Sheopuri, A.: Applying image analysis to assess food aesthetics and uniqueness. In:
Proceedings of International Conference on Image Processing (ICIP), pp. 311–314. IEEE
(2015)
232. Library of Congress: Personal archiving: preserving your digital memories. http://www.
digitalpreservation.gov/personalarchiving. Accessed 16 Oct 2017
233. Library of Congress, M.E.B.: Metadata encoding and transmission standard, Version 1.11.
http://www.loc.gov/standards/mets (2015). Accessed Sep 2017
234. Library of Congress: Preserving digital memories. In: 7th International Conference on
Preservation of Digital Objects (iPRES 2010) (2010). http://www.digitalpreservation.gov/
personalarchiving/documents/NDIIP_PA_poster.pdf Accessed 16 Oct 2017
235. Library of Congress, P.E.C.: PREMIS data dictionary for preservation metadata, Version 3.0.
http://www.loc.gov/standards/premis/v3/index.html (2015). Accessed Sep 2017
236. Lim, M.Y.: Memory Models for Intelligent Social Companions (2012)
237. Lim, M.Y., Aylett, R., Ho, W.C., Enz, S., Vargas, P.: A socially-aware memory for companion
agents. In: International Workshop on Intelligent Virtual Agents, pp. 20–26. Springer (2009)
238. Lim, M.Y., Aylett, R., Vargas, P.A., Ho, W.C., Dias, J.: Human-like memory retrieval mecha-
nisms for social companions. In: The 10th International Conference on Autonomous Agents
and Multiagent Systems, vol. 3, pp. 1117–1118. International Foundation for Autonomous
Agents and Multiagent Systems (2011)
239. Lin, C.Y.: Rouge: a package for automatic evaluation of summaries. In: Text Summarization
Branches Out: Proceedings of the ACL-04 Workshop, vol. 8 (2004)
240. Lindley,S.E.:Beforeiforget:frompersonalmemorytofamilyhistory.Hum.Comput.Interact.
27(1–2), 13–36 (2012)
241. Lindsay, R.C.: Handbook Of Eyewitness Psychology 2 Volume Set. Psychology Press (2014)
242. Linton, M.: Memory for Real-world Events. Explorations in Cognition, pp. 376–404 (1975)
243. Liu, D.C., Nocedal, J.: On the limited memory BFGS method for large scale optimization.
Math. Progr. 45(1–3), 503–528 (1989)
244. Liu, J., Huang, Z., Cai, H., Shen, H.T., Ngo, C.W., Wang, W.: Near-duplicate video retrieval:
current research and future trends. Comput. Surv. (CSUR) 45(4), 44 (2013)
245. Liu, T.Y.: Learning to rank for information retrieval. Found. Trends Inf. Retr. 3(3), 225–331
(2009)
246. Lo, K.Y., Liu, K.H., Chen, C.S.: Assessment of photo aesthetics with efﬁciency. In: 21st
International Conference on Pattern Recognition (ICPR), pp. 2186–2189. IEEE (2012)
247. Lo, K.Y., Liu, K.H., Chen, C.S.: Intelligent photographing interface with on-device aesthetic
quality assessment. In: Computer Vision-ACCV 2012 Workshops, pp. 533–544. Springer
(2013)
248. Loftus, E.F., Palmer, J.C.: Reconstruction of automobile destruction: an example of the inter-
action between language and memory. J. Verb. Learn. Verb. Behav. 13(5), 585–589 (1974)
249. Logie, R., Hunter, J., McIntosh, N., Gilhooly, K., Alberdi, E., Reiss, J.: Medical cognition and
computer support in the intensive care unit: a cognitive engineering approach. In: Engineering
Psychology and Cognitive Ergonomics: Integration of Theory and Application, pp. 167–174
(1997)
250. Logie, R., Wright, R., Decker, S.: Recognition memory performance and residential burglary.
Appl. Cognit. Psychol. 6(2), 109–123 (1992)
251. Logie, R.H.: Spatial and visual working memory: a mental workspace. Psychol. Learn. Mot.
42, 37–78 (2003)

References
327
252. Logie, R.H.: The functional organization and capacity limits of working memory. Current
Directions Psychol. Sci. 20(4), 240–245 (2011). https://doi.org/10.1177/0963721411415340
253. Logie, R.H.: Visuo-spatial Working Memory. Psychology Press (2014)
254. Loui, A.C., Savakis, A.: Automated event clustering and quality screening of consumer pic-
tures for digital albuming. IEEE Trans. Multimed. 5(3), 390–402 (2003)
255. Lowe, D.G.: Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis.
60(2), 91–110 (2004)
256. Lu, Y., Wan, Y.: PHA: a fast potential-based hierarchical agglomerative clustering method.
Pattern Recognit. Lett. 46(5), 1227–1239 (2013)
257. Lucero, A., Boberg, M., Uusitalo, S.: Image space: capturing, sharing and contextualizing
personal pictures in a simple and playful way. In: Proceedings of the International Conference
on Advances in Computer Entertainment Technology, ACE ’09, pp. 215–222. ACM, New
York, NY, USA (2009). https://doi.org/10.1145/1690388.1690424
258. Luminet, O., Curci, A.: Flashbulb Memories: New Issues and New Perspectives. Psychology
Press (2008)
259. Luo, Y., Tang, X.: Photo and video quality evaluation: focusing on the subject. In: Computer
Vision–ECCV, pp. 386–399. Springer, Berlin (2008)
260. Maguire, E.A.: Neuroimaging studies of autobiographical event memory. Philos. Trans. R.
Soc. Lond. B Biol. Sci. 356(1413), 1441–1451 (2001)
261. Maji, S., Berg, A., Malik, J.: Classiﬁcation using intersection kernel support vector machines
is efﬁcient. In: IEEE Conference on Computer Vision and Pattern Recognition, 2008. CVPR
2008, pp. 1–8 (2008)
262. Manning, C.D., Raghavan, P., Schütze, H.: Introduction to Information Retrieval. Cambridge
University Press, New York, NY, USA (2008)
263. Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S.J., McClosky, D.: The Stanford
CoreNLP natural language processing toolkit. In: ACL ’14 (2014)
264. Marchesotti, L., Perronnin, F., Larlus, D., Csurka, G.: Assessing the aesthetic quality of
photographs using generic image descriptors. In: IEEE International Conference on Computer
Vision (ICCV), pp. 1784–1791 (2011)
265. Marée, R., Geurts, P., Wehenkel, L.: Content-based image retrieval by indexing random sub-
windows with randomized trees. In: Asian Conference on Computer Vision, pp. 611–620.
Springer (2007)
266. Markatopoulou, F., Mezaris, V., Patras, I.: Cascade of classiﬁers based on binary, non-binary
and deep convolutional network descriptors for video concept detection. In: Proceedings of
the IEEE International Conference on Image Processing (ICIP 2015), pp. 1786–1790 (2015)
267. Markatopoulou, F., Mezaris, V., Patras, I.: Online multi-task learning for semantic concept
detection in video. In: IEEE International Conference on Image Processing (ICIP), pp. 186–
190 (2016)
268. Markatopoulou, F., Mezaris, V., Pittaras, N., Patras, I.: Local features and a two-layer stacking
architecture for semantic concept detection in video. IEEE Trans. Emerg. Top. Comput. 3(2),
193–204 (2015)
269. Markatopoulou, F., Pittaras, N., Papadopoulou, O., Mezaris, V., Patras, I.: A Study on the Use
of a Binary Local Descriptor and Color Extensions of Local Descriptors for Video Concept
Detection, pp. 282–293. Springer International Publishing, Cham (2015)
270. Marshall, C.C.: Challenges and Opportunities for Personal Digital Archiving. Digital: Per-
sonal Collections in the Digital Era, pp. 90–114 (2011)
271. Marshall, C.C.: I, Digital: Personal Collections in the Digital Era. Society of American
Archivists (2011)
272. Mather, M., Carstensen, L.L.: Aging and motivated cognition: the positivity effect in attention
and memory. Trends Cognit. Sci. 9(10), 496–502 (2005)
273. Maus, H., Schwarz, S., Dengel, A.: Weaving personal knowledge spaces into ofﬁce applica-
tions. In: Integration of Practice-Oriented Knowledge Technology: Trends and Prospectives,
pp. 71–82. Springer, Berlin (2013)

328
References
274. Maus, H., Schwarz, S., Haas, J., Dengel, A.: CONTASK: context-sensitive task assistance in
the semantic desktop. In: Filipe, J., Cordeiro, J. (eds.) Enterprise Information Systems. 12th
International Conference, ICEIS 2010, Funchal-Madeira, Portugal, June 8–12, 2010, Revised
Selected Papers. Lecture Notes in Business Information Processing, vol. 73, pp. 177–192.
Springer (2011). https://doi.org/10.1007/978-3-642-19802-1_13
275. Mavridaki, E., Mezaris, V.: No-reference blur assessment in natural images using fourier trans-
form and spatial pyramids. In: Proceedings of International Conference on Image Processing
(ICIP). IEEE (2014)
276. Mavridaki, E., Mezaris, V.: A comprehensive aesthetic quality assessment method for natural
images using basic rules of photography. In: 2015 IEEE International Conference on Image
Processing (ICIP), pp. 887–891. IEEE (2015)
277. Maxwell, K.T., Croft, W.B.: Compact query term selection using topically related text. In:
Proceedings of the 36th International ACM SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ’13, pp. 583–592 (2013)
278. Mayer-Schönberger, V.: Delete - The Virtue of Forgetting in the Digital Age. Morgan Kauf-
mann Publishers, Massachusetts (2009)
279. Mayer-Schönberger, V.: Delete: The Virtue of Forgetting in the Digital Age. Princeton Uni-
versity Press, Princeton (2011)
280. Maynard, D., Greenwood, M.A.: Who cares about sarcastic tweets? Investigating the impact
of sarcasm on sentiment analysis. In: Proceedings of LREC 2014. Reykjavik, Iceland (2014)
281. Maynard, D., Roberts, I., Greenwood, M.A., Rout, D., Bontcheva, K.: A Framework for
Real-time Semantic Social Media Analysis. Web Semantics: Science, Services and Agents
on the World Wide Web (2017). https://doi.org/10.1016/j.websem.2017.05.002. http://www.
sciencedirect.com/science/article/pii/S1570826817300240
282. McCamy, C.S.: Correlated color temperature as an explicit function of chromaticity coordi-
nates. Color Res. Appl. 17(2), 142–144 (1992)
283. McDaniel, M.A., Einstein, G.O.: Strategic and automatic processes in prospective memory
retrieval: a multiprocess framework. Appl. Cognit. Psychol. 14(7) (2000)
284. McDaniel, M.A., Einstein, G.O.: Strategic and automatic processes in prospective memory
retrieval: a multiprocess framework. Appl. Cognit. Psychol. 14, 127–144 (2000)
285. McDaniel, M.A., Einstein, G.O.: Prospective Memory: An Overview and Synthesis of an
Emerging Field. SAGE Publications, Inc., California (2007)
286. McGee-Lennon, M.R., Wolters, M.K., Brewster, S.: User-centred multimodal reminders for
assistive living. In: Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, pp. 2105–2114. ACM (2011)
287. McGeoch, J.A., McDonald, W.T.: Meaningful relation and retroactive inhibition. Am. J. Psy-
chol. 43(4), 579–588 (1931)
288. McKENNA, S.P., Glendon, A.: Occupational ﬁrst aid training: decay in cardiopulmonary
resuscitation (CPR) skills. J. Occup. Organ. Psychol. 58(2), 109–117 (1985)
289. McLaren, K.: Xiiithe development of the cie 1976 (l* a* b*) uniform colour space and colour-
difference formula. J. Soc. Dyers Colour. 92(9), 338–341 (1976)
290. Meeker, M., Wu, L.: Internet trends. Technical report, Kleiner Perkins Cauﬁeld and Byers
(2013)
291. Mei, T., Wang, B., Hua, X.S., Zhou, H.Q., Li, S.: Probabilistic multimodality fusion for event
based home photo clustering. In: 2006 IEEE International Conference on Multimedia and
Expo, pp. 1757–1760. IEEE (2006)
292. Meij, E., Weerkamp, W., de Rijke, M.: Adding semantics to microblog posts. In: Proceedings
of the Fifth ACM International Conference on Web Search and Data Mining, pp. 563–572
(2012)
293. Meyer, J.: Emotional Images: How to add Feeling with Muted Tones and Harmonious Color
(Published in Digital Camera World, March 28, 2013). https://goo.gl/7uV9iz
294. Mezaris, V., Sidiropoulos, P., Dimou, A., Kompatsiaris, I.: On the use of visual soft semantics
for video temporal decomposition to scenes. In: 2010 IEEE Fourth International Conference
on Semantic Computing (ICSC), pp. 141–148 (2010). https://doi.org/10.1109/ICSC.2010.23

References
329
295. Mihalcea, R., Csomai, A.: Wikify!: Linking documents to encyclopedic knowledge. In: Pro-
ceedings of the Sixteenth ACM Conference on Conference on Information and Knowledge
Management, CIKM ’07, pp. 233–242 (2007)
296. Mikolajczyk, K., Tuytelaars, T., Schmid, C., Zisserman, A., Matas, J., Schaffalitzky, F., Kadir,
T., Van Gool, L.: A comparison of afﬁne region detectors. Int. J. Comput. Vis. 65(1–2), 43–72
(2005)
297. Miller, G.A.: The magical number seven, plus or minus two: some limits on our capacity for
processing information. Psychol. Rev. 63(2), 81 (1956)
298. Milne, D., Witten, I.H.: Learning to link with wikipedia. In: Proceedings of the 17th ACM
Conference on Information and Knowledge Management, CIKM ’08, pp. 509–518 (2008)
299. Mirzasoleiman, B., Badanidiyuru, A., Karbasi, A.: Fast constrained submodular maximiza-
tion: Personalized data summarization. In: Proceedings of the 33nd International Conference
on Machine Learning, ICML, pp. 1358–1367 (2016)
300. Mitra, S., Winslett, M., Hsu, W.W.: Query-based partitioning of documents and indexes for
information lifecycle management. In: Proceedings of the 2008 ACM SIGMOD international
conference on Management of data, SIGMOD ’08, pp. 623–636 (2008). https://doi.org/10.
1145/1376616.1376680
301. Mols, I., Hoven, E.v.d., Eggen, B.: Making memories: a cultural probe study into the remem-
bering of everyday life. In: Proceedings of the 8th Nordic Conference on Human-Computer
Interaction: Fun, Fast, Foundational, NordiCHI ’14, pp. 256–265. ACM (2014)
302. Moorthy, A.K., Obrador, P., Oliver, N.: Towards computational models of the visual aesthetic
appeal of consumer videos. In: Proceedings of the 11th European Conference on Computer
Vision (ECCV), Heraklion, Crete, Greece, 2010, pp. 1–14. Springer (2010)
303. Morris, P.E., Tweedy, M., Gruneberg, M.M.: Interest, knowledge and the memorizing of
soccer scores. Br. J. Psychol. 76(4), 415–425 (1985)
304. Mothe, J., Tanguy, L.: Linguistic features to predict query difﬁculty. In: ACM SIGIR 2005
Workshop on Predicting Query Difﬁculty - Methods and Applications (2005)
305. Mousavi, H., Srinivas, U., Monga, V., Suo, Y., Dao, M., Tran, T.: Multi-task image clas-
siﬁcation via collaborative, hierarchical spike-and-slab priors. In: Proceedings of the IEEE
International Conference on Image Processing (ICIP 2014), pp. 4236–4240 (2014)
306. Murray, N., Marchesotti, L., Perronnin, F.: AVA: a large-scale database for aesthetic visual
analysis. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.
2408–2415 (2012)
307. Murray, N., Marchesotti, L., Perronnin, F., Meylan, F.: Learning to rank images using semantic
and aesthetic labels. In: BMVC, pp. 1–10 (2012)
308. Nathan, L., Shaffer, E.: Preserving social media: opening a multi-disciplinary dialogue. In:
Proceedings of the The Memory of the World in the Digital Age: Digitization and Preservation.
Vancouver, BC, 26–28 September 2012. UNESCO (2013)
309. Ng, J., Yang, F., Davis, L.: Exploiting local features from deep networks for image retrieval.
In: Proceedings of the Conference on Computer Vision and Pattern Recognition Workshops,
pp. 53–61. IEEE (2015)
310. Niederee, C., Kanhabua, N., Gallo, F., Logie, R.H.: Forgetful digital memory: towards brain-
inspired long-term data and information management. SIGMOD Rec. 44(2), 41–46 (2015).
https://doi.org/10.1145/2814710.2814718
311. Niu, Y., Liu, F.: What makes a professional video? a computational aesthetics approach. IEEE
Trans. Circuits Syst. Video Technol. 22(7), 1037–1049 (2012)
312. OASIS: Content Management Interoperability Services (CMIS) Version 1.1. OASIS Standard.
https://docs.oasis-open.org/cmis/CMIS/v1.1/CMIS-v1.1.html (2013). Accessed Sep 2017
313. Obozinski, G., Taskar, B.: Multi-task feature selection. In: Proceedings of the 23rd Interna-
tional Conference on Machine Learning (ICML 2006). Workshop of Structural Knowledge
Transfer for Machine Learning. Pittsburgh, Pennsylvania (2006)
314. Obrador, P., De Oliveira, R., Oliver, N.: Supporting personal photo storytelling for social
albums. In: Proceedings of the 18th ACM International Conference on Multimedia, pp. 561–
570. ACM (2010)

330
References
315. Oliva, A., Torralba, A.: Modeling the shape of the scene: a holistic representation of the spatial
envelope. Int. J. Comput. Vis. 42(3), 145–175 (2001)
316. Oquab, M., Bottou, L., Laptev, I., Sivic, J.: Learning and transferring mid-level image repre-
sentations using convolutional neural networks. In: CVPR (2014)
317. Owoputi, O., O’Connor, B., Dyer, C., Gimpel, K., Schneider, N., Smith, N.A.: Improved
part-of-speech tagging for online conversational text with word clusters. NAACL-HLT 2013,
380–390 (2013)
318. Page, L., Brin, S., Motwani, R., Winograd, T.: The pagerank citation ranking: Bringing order
to the web. In: Proceedings of the 7th International World Wide Web Conference, pp. 161–172.
Brisbane, Australia (1998). https://citeseer.nj.nec.com/page98pagerank.html
319. Päivärinta, T., Westerlund, P., Nilsson, J.: Integrating Contemporary Content Management
and Long-term Digital Preservation: a Design Problem. Springer Verlag, Luleå University of
Technology, Information Systems (2015)
320. Papadakis, G., Kawase, R., Herder, E., Niederée, C.: A layered approach to revisitation pre-
diction. In: Proceedings of the 11th International Conference on Web Engineering, ICWE’11,
pp. 258–273. Springer, Berlin (2011). http://dl.acm.org/citation.cfm?id=2027776.2027796
321. Papagiannopoulou, C., Mezaris, V.: Concept-based image clustering and summarization of
event-related image collections. In: Proceedings of the 1st ACM International Workshop on
Human Centered Event Understanding from Multimedia, pp. 23–28. ACM (2014)
322. Peetz, M.H., de Rijke, M.: Cognitive temporal document priors. In: Proceedings of the 35th
European Conference on Advances in Information Retrieval, ECIR’13, pp. 318–330. Springer,
Berlin (2013). https://doi.org/10.1007/978-3-642-36973-5_27
323. Perronnin, F., Sánchez, J., Mensink, T.: Improving the ﬁsher kernel for large-scale image
classiﬁcation. In: Computer Vision ECCV, pp. 143–156 (2010)
324. Petrelli, D., van den Hoven, E., Whittaker, S.: Making history: intentional capture of future
memories. In: Proceedings of the SIGCHI Conference on Human Factors in Computing
Systems, CHI ’09, pp. 1723–1732. ACM, New York, NY, USA (2009). https://doi.org/10.
1145/1518701.1518966
325. Petrelli, D., Whittaker, S.: Family memories in the home: contrasting physical and digital
mementos. Personal Ubiquitous Comput. 14(2), 153–169 (2010)
326. Pezdek, K.: Event memory and autobiographical memory for the events of September 11,
2001. Appl. Cognit. Psychol. 17(9), 1033–1045 (2003)
327. Philbin, J., Chum, O., Isard, M., Sivic, J., Zisserman, A.: Object retrieval with large vocabu-
laries and fast spatial matching. In: Conference on Computer Vision and Pattern Recognition
(CVPR), pp. 1–8. IEEE (2007)
328. Piolino, P., Desgranges, B., Eustache, F.: Episodic autobiographical memories over the course
of time: cognitive, neuropsychological and neuroimaging ﬁndings. Neuropsychologia 47(11),
2314–2329 (2009)
329. Pittaras, N., Markatopoulou, F., Mezaris, V., et al.: Comparison of Fine-Tuning and Extension
Strategies for Deep Convolutional Neural Networks, pp. 102–114. Springer, Cham (2017)
330. Ponte, J.M., Croft, W.B.: A language modeling approach to information retrieval. In: Proceed-
ings of the 21st Annual International ACM SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ’98, pp. 275–281 (1998)
331. POWRR: Tool-grid (2013). http://digitalpowrr.niu.edu/tool-grid/. Accessed 16 Oct 2017
332. Pritchett,A.R.:Aviationautomation:Generalperspectivesandspeciﬁcguidanceforthedesign
of modes and alerts. Rev. Hum. Factors Ergon. 5(1), 82–113 (2009). https://doi.org/10.1518/
155723409X448026
333. Rabbath, M., Sandhaus, P., Boll, S.: Automatic creation of photo books from stories in social
media. In: Proceedings of Second ACM SIGMM Workshop on Social Media, WSM ’10, pp.
15–20. ACM (2010)
334. Raiber, F., Kurland, O.: Query-performance prediction: setting the expectations straight. In:
Proceedings of the 37th International ACM SIGIR Conference on Research and Development
in Information Retrieval, SIGIR ’14, pp. 13–22 (2014)

References
331
335. Rauch, J., Braud, M., Edelstein, O., Rabinovici-Cohen, S., Nagin, K., Marberg, J., Voets,
D., Sanya, I., Badawy, M., Shebab, E., Randers, F., Droppert, J., Klecha, M.: Ensure: long
term digital preservation of health care, clinical trial and ﬁnancial data. In: Borbinha, J., Nel-
son, M., Knight, S. (eds.) iPRES 2013: Proceedings of the 10th International Conference on
Preservation of Digital Objects (2013)
336. Reason, J.: Human Error. Cambridge University Press, Cambridge (1990)
337. Renoult, L., Davidson, P.S., Palombo, D.J., Moscovitch, M., Levine, B.: Personal semantics:
at the crossroads of semantic and episodic memory. Trends Cognit. Sci. 16(11), 550–558
(2012)
338. Richards, D., Bransky, K.: Forgetmenot: what and how users expect intelligent virtual agents to
recall and forget personal conversational content. Int. J. Hum. Comput. Stud. 72(5), 460–476
(2014)
339. Ritter, A., Clark, S., Mausam, Etzioni, O.: Named entity recognition in tweets: an experi-
mental study. In: Proceedings of the Conference on Empirical Methods in Natural Language
Processing, pp. 1524–1534 (2011)
340. Robertson, S.: A new interpretation of average precision. In: Proceedings of the 31st
Annual International ACM SIGIR Conference on Research and Development in Informa-
tion Retrieval, pp. 689–690. ACM (2008)
341. Robusto, C.C.: The cosine-haversine formula. Am. Math. Mon. 64(1), 38–40 (1957)
342. Rocha, C., Schwabe, D., Aragao, M.P.: A hybrid approach for searching in the semantic web.
In: Proceedings of the 13th International Conference on World Wide Web, WWW ’04, pp.
374–383. ACM, New York, NY, USA (2004). https://doi.org/10.1145/988672.988723
343. Roediger, H.L., Marsh, E.J.: Episodic and Autobiographical Memory. Handbook of Psychol-
ogy (2003)
344. Russ, A.L., Zillich, A.J., McManus, M.S., Doebbeling, B.N., Saleem, J.J.: Prescribers inter-
actions with medication alerts at the point of prescribing: a multi-method, in situ investigation
of the human-computer interaction. Int. J. Med. Inf. 81(4), 232–243 (2012)
345. Russakovsky, O., Deng, J., et al., H.S.: ImageNet large scale visual recognition challenge. Int.
J. Comput. Vis. (IJCV) 115(3), 211–252 (2015). https://doi.org/10.1007/s11263-015-0816-
y
346. Samani,Z.R.,Moghaddam,M.E.: Aknowledge-basedsemantic approachforimage collection
summarization. Multimed. Tools Appl. 1–23 (2016)
347. Sansone, E., Apostoldis, K., Conci, N., Boato, G., Mezaris, V., Natale, F.G.B.D.: Automatic
synchronization of multi-user photo galleries. IEEE Trans. Multimed. (2017). https://doi.org/
10.1109/TMM.2017.2655446
348. Sauermann, L.: The Gnowsis Semantic Desktop Approach to Personal Information Manage-
ment. Ph.D. thesis, Fachbereich Informatik der Universitt Kaiserslautern (2009). http://www.
dfki.uni-kl.de/~sauermann/papers/Sauermann2009phd.pdf
349. Sauermann, L., Bernardi, A., Dengel, A.: Overview and outlook on the semantic desktop.
In:Decker,S.,Park,J.,Quan,D.,Sauermann,L.(eds.)ProceedingsoftheFirstSemanticDesk-
top Workshop at the ISWC Conference 2005 (2005). http://www.dfki.uni-kl.de/~sauermann/
papers/Sauermann+2005d.pdf
350. Sauermann, L., Dengel, A., van Elst, L., Lauer, A., Maus, H., Schwarz, S.: Personalization
in the EPOS project. In: Proceedings of the Semantic Web Personalization Workshop at the
ESWC 2006 (2006)
351. Sauermann, L., van Elst, L., Dengel, A.: PIMO – A Framework for Representing Personal
Information Models. In: Pellegrini, T., Schaffert, S. (eds.) I-SEMANTICS Conference 5–7
September 2007, Graz, Austria, J.UCS, pp. 270–277. Know-Center, Austria (2007)
352. Sauermann, L., Heim, D.: Evaluating long-term use of the gnowsis semantic desktop for PIM.
In: The Semantic Web. 7th International Semantic Web Conference (ISWC-2008), 7th Inter-
national Semantic Web Conference, October 26–30, Karlsruhe, Germany. LNCS, vol. 5318,
pp. 467–482. Springer (2008). http://www.springerlink.com/content/c7u686mq27751373/
353. Savakis, A.E., Etz, S.P., Loui, A.C.: Evaluation of image appeal in consumer photography.
In: Electronic Imaging, pp. 111–120. International Society for Optics and Photonics (2000)

332
References
354. Schacter, D.L., Koutstaal, W., Johnson, M.K., Gross, M.S., Angell, K.E.: False recollection
induced by photographs: a comparison of older and younger adults. Psychol. Aging 12(2),
203 (1997)
355. Schellenberg, T.R.: The appraisal of modern records. Bull. Natl. Arch. 8, 46 (1956)
356. Schölkopf, B., Herbrich, R., Smola, A.J.: A generalized representer theorem. In: Proceeding
of 14th Annual Conference on Computational Learning Theory, pp. 416–426 (2001)
357. Schwarz, S.: Context-Awareness and Context-Sensitive Interfaces for Knowledge Work Sup-
port. Ph.D thesis, University of Kaiserslautern, Fachbereich Informatik (2010). http://www.
dr.hut-verlag.de/978-3-86853-388-0.html
358. Seah, B.S., Bhowmick, S.S., Sun, A.: Prism: concept-preserving social image search results
summarization.In:Proceedingsofthe37thInternationalACMSIGIRConferenceonResearch
and Development in Information Retrieval, SIGIR ’14, pp. 737–746. ACM (2014)
359. Seamon, J.G., Moskowitz, T.N., Swan, A.E., Zhong, B., Golembeski, A., Liong, C., Narzikul,
A.C., Sosan, O.A.: Sensecam reminiscence and action recall in memory-unimpaired people.
Memory 22(7), 861–866 (2014)
360. Sellen, A.J., Fogg, A., Aitken, M., Hodges, S., Rother, C., Wood, K.: Do life-logging technolo-
gies support memory for the past?: an experimental study using senseCam. In: Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems, pp. 81–90. ACM (2007)
361. Shen, W., Wang, J., Luo, P., Wang, M.: Linking named entities in tweets with knowledge
base via user interest modeling. In: Proceedings of the 19th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining, KDD ’13, pp. 68–76 (2013)
362. Simond, F., Arvanitopoulos Darginis, N., Süsstrunk, S.: Image aesthetics depends on context.
In: Proceedings of International Conference on Image Processing (ICIP). IEEE (2015)
363. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recog-
nition. In: CoRR (2014). arXiv:abs/1409.1556
364. Singh, V., Elamvazuthi, I.: Fingerprint matching algorithm for poor quality images. J. Eng.
1(1) (2015)
365. Sinha, P., Mehrotra, S., Jain, R.: Summarization of personal photologs using multidimensional
content and context. In: Proceedings of the 1st ACM International Conference on Multimedia
Retrieval, ICMR ’11, pp. 4:1–4:8. ACM (2011)
366. Skobeltsyn, G., Junqueira, F., Plachouras, V., Baeza-Yates, R.: Resin: A combination of results
caching and index pruning for high-performance web search engines. In: Proceedings of
the 31st Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, SIGIR ’08, pp. 131–138. ACM, New York, NY, USA (2008). https://
doi.org/10.1145/1390334.1390359
367. Smit, E., Van Der Hoeven, J., Giaretta, D.: Avoiding a digital dark age for data: why publishers
should care about digital preservation. Learn. Publ. 24(1), 35–49 (2011)
368. Snoek, C., Fontijne, D., van de Sande, K.E., Stokman, H.e.a.: Qualcomm research and uni-
versity of Amsterdam at TRECVID 2015: recognizing concepts, objects, and events in video.
In: Proceedings of TRECVID 2015. NIST, USA (2015)
369. Snoek, C.G.M., Worring, M.: Concept-based video retrieval. Found. Trends Inf. Retr. 2(4),
215–322 (2009)
370. Sokal, R.R.: Classiﬁcation: purposes, principles, progress, prospects. Science 185(4157),
1115–1123 (1974)
371. Solli, M., Lenz, R.: Color emotions for image classiﬁcation and retrieval. In: Conference
on Colour in Graphics, Imaging, and Vision, vol. 2008, pp. 367–371. Society for Imaging
Science and Technology (2008)
372. Spyromitros-Xiouﬁs, E., Papadopoulos, S., Kompatsiaris, I.Y., Tsoumakas, G., Vlahavas, I.:
A comprehensive study over vlad and product quantization in large-scale image retrieval.
IEEE Trans. Multimed. 16(6), 1713–1728 (2014)
373. St. Jacques, P.L., Levine, B.: Ageing and autobiographical memory for emotional and neutral
events. Memory 15(2), 129–144 (2007)
374. St. Jacques, P.L., Schacter, D.L.: Modifying memory: selectively enhancing and updating
personal memories for a museum tour by reactivating them. Psychol. Sci. 24(4), 537–543
(2013)

References
333
375. Subagdja, B., Wang, W., Tan, A.H., Tan, Y.S., Teow, L.N.: Memory formation, consolidation,
and forgetting in learning agents. In: Proceedings of the 11th International Conference on
AutonomousAgentsandMultiagent Systems,vol.2,pp.1007–1014.International Foundation
for Autonomous Agents and Multiagent Systems (2012)
376. Sun,G.,Chen,Y.,Liu,X.,Wu,E.:Adaptivemulti-tasklearningforﬁne-grainedcategorization.
In: Proceedings of the IEEE International Conference on Image Processing (ICIP 2015), pp.
996–1000 (2015)
377. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,
Rabinovich, A.: Going deeper with convolutions. In: Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1–9 (2015)
378. Talarico, J.M., Rubin, D.C.: Conﬁdence, not consistency, characterizes ﬂashbulb memories.
Psychol. Sci. 14(5), 455–461 (2003)
379. Tang, X., Luo, W., Wang, X.: Content-based photo quality assessment. IEEE Trans. Multimed.
(TMM) 15, 1930–1943 (2013)
380. Tarjan, R.: Depth-ﬁrst search and linear graph algorithms. SIAM J. Comput. 1(2), 146–160
(1972)
381. Thimbleby, H.: Improving safety in medical devices and systems. In: 2013 IEEE International
Conference on Healthcare Informatics (ICHI), pp. 1–13. IEEE (2013)
382. Tong, H., Li, M., Zhang, H.J., He, J., Zhang, C.: Classiﬁcation of digital photos taken by
photographersorhomeusers.In:AdvancesinMultimediaInformationProcessing-PCM2004,
pp. 198–205. Springer, Berlin (2005)
383. Trad, M.R., Joly, A., Boujemaa, N.: Large scale visual-based event matching. In: Proceedings
of the 1st ACM International Conference on Multimedia Retrieval, ICMR ’11, pp. 53:1–53:7.
ACM, New York, NY, USA (2011). https://doi.org/10.1145/1991996.1992049
384. Tran, N.K., Ceroni, A., Kanhabua, N., Niederée, C.: Back to the past: supporting interpreta-
tions of forgotten stories by time-aware re-contextualization. In: Proceedings of the Eighth
ACM International Conference on Web Search and Data Mining, WSDM ’15, pp. 339–348
(2015)
385. Tran, N.K., Ceroni, A., Kanhabua, N., Niederée, C.: Time-travel translator: automatically
contextualizing news articles. In: Proceedings of the 24th International Conference on World
Wide Web, WWW ’15 Companion, pp. 247–250 (2015)
386. Tran, T., Nguyen, T.N.: Hedera: Scalable indexing and exploring entities in Wikipedia revision
history. In: Proceedings of the 2014 International Conference on Posters and Demonstrations
Track, ISWC-PD’14, vol. 1272, pp. 297–300 (2014)
387. Tran, T., Tran, N.K., Teka Hadgu, A., Jschke, R.: Semantic annotation for microblog topics
using Wikipedia temporal information. In: Proceedings of the 2015 Conference on Empirical
Methods in Natural Language Processing (EMNLP) (2015)
388. Tsagkias, M., de Rijke, M., Weerkamp, W.: Linking online news and social media. In: Pro-
ceedings of the Fourth ACM International Conference on Web Search and Data Mining,
WSDM ’11, pp. 565–574 (2011)
389. Tschiatschek, S., Iyer, R.K., Wei, H., Bilmes, J.A.: Learning mixtures of submodular functions
for image collection summarization. In: Advances in Neural Information Processing Systems,
pp. 1413–1421 (2014)
390. Tuan, T., Schwarz, S., Niederée, C., Maus, H., Kanhabua, N.: The forgotten needle in my
collections: task-aware ranking of documents in semantic information space. In: Proceedings
of the 1st ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR-
16), March 13–17, Chapel Hill, North Carolina, USA. ACM, ACM Press (2016)
391. Tulving, E.: Elements of Episodic Memory (1985)
392. Tulving, E.: Episodic memory: from mind to brain. Ann. Rev. Psychol. 53(1), 1–25 (2002)
393. Tulving, E., Schacter, D.L., McLachlan, D.R., Moscovitch, M.: Priming of semantic autobi-
ographical knowledge: a case study of retrograde amnesia. Brain Cognit. 8(1), 3–20 (1988)
394. Tulving, E., et al.: Episodic and semantic memory. Organ. Mem. 1, 381–403 (1972)
395. Tzelepis, C., Mavridaki, E., Mezaris, V., Patras, I.: Video aesthetic quality assessment using
kernel support vector machine with isotropic gaussian sample uncertainty (KSVM-IGSU).

334
References
In: 2016 IEEE International Conference on Image Processing (ICIP), pp. 2410–2414. IEEE
(2016)
396. Tzelepis, C., Mezaris, V., Patras, I.: Linear Maximum Margin Classiﬁer for Learning from
Uncertain Data (2015). arXiv:1504.03892
397. Tzelepis, C., Mezaris, V., Patras, I.: Video event detection using kernel support vector machine
with isotropic Gaussian sample uncertainty (KSVM-IGSU). In: International Conference on
Multimedia Modeling, pp. 3–15. Springer (2016)
398. Underwood, B.J.: Interference and forgetting. Psychol. Rev. 64(1), 49 (1957)
399. Vaittinen, T., Kärkkäinen, T., Roimela, K.: Experience explorer: context-based browsing of
personal media. In: Symposium on Human Interface, pp. 111–120. Springer (2011)
400. Van Der Meulen, M., Logie, R.H., Freer, Y., Sykes, C., McIntosh, N., Hunter, J.: When a
graph is poorer than 100 words: a comparison of computerised natural language generation,
human generated descriptions and graphical displays in neonatal intensive care. Appl. Cognit.
Psychol. 24(1), 77–89 (2010)
401. Van Opdenbosch, D., Steinbach, E.: AVLAD: Optimizing the VLAD image signature for
speciﬁc feature descriptors. In: 2016 IEEE International Symposium on Multimedia (ISM),
pp. 545–550. IEEE (2016)
402. Vargas,P.A.,Fernaeus,Y.,Lim,M.Y.,Enz,S.,Ho,W.C.,Jacobsson,M.,Ayllet,R.:Advocating
an ethical memory model for artiﬁcial companions from a Human-centred perspective. AI
Soc. 26(4), 329–337 (2011)
403. Vargas, P.A., Freitas, A.A., Lim, M., Enz, S., Ho, W.C., Aylett, R.: Forgetting and generalisa-
tion in memory modelling for robot companions: a data mining approach. In: Proceedings of
the Human Memory for Artiﬁcial Agents Symposium (AISB 2010), De Montfort University,
Leicester, UK (2010)
404. Vennelakanti, R., Madhvanath, S., Subramanian, A., Sowndararajan, A., David, A., Dey, P.:
Pixene: creating memories while sharing photos. In: Proceedings of the 14th ACM Interna-
tional Conference on Multimodal Interaction, pp. 59–60. ACM (2012)
405. Viola, P., Jones, M.J.: Robust real-time face detection. Int. J. Comput. Vis. (IJCV) 57 (2004)
406. Vliegendhart, R., Larson, M., Pouwelse, J.A.: Discovering user perceptions of semantic sim-
ilarity in near-duplicate multimedia ﬁles. In: CrowdSearch, pp. 54–58. Citeseer (2012)
407. W3C: PROV-O: The PROV Ontology - Technical report (2013). http://www.w3.org/TR/prov-
o/. Accessed Sep 2017
408. Walber, T., Scherp, A., Staab, S.: Smart photo selection: interpret gaze as personal interest.
In: Proceedings of CHI ’14 (2014)
409. Wang, J., Allebach, J.: Automatic assessment of online fashion shopping photo aesthetic
quality. In: Proceedings of International Conference on Image Processing (ICIP), pp. 2915–
2919. IEEE (2015)
410. Wang,L.,Wang,H.,Zhu,F.:Twinfeatureandsimilaritymaximalmatchingforimageretrieval.
In: Proceedings of the 5th International Conference on Multimedia Retrieval, pp. 59–66. ACM
(2015)
411. Wang, Y., Dai, Q., Feng, R., Jiang, Y.G.: Beauty is here: Evaluating aesthetics in videos using
multimodal features and free training data. In: Proceedings of the 21st ACM International
Conference on Multimedia, pp. 369–372. ACM (2013)
412. Wang, Y., Lin, Z., Shen, X., Mech, R., Miller, G., Cottrell, G.W.: Event-speciﬁc image impor-
tance. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 4810–4819 (2016)
413. Weibel, S.: The dublin core: a simple content description model for electronic resources. Bull.
Am. Soc. Inf. Sci. Technol. 24(1), 9–11 (1997). https://doi.org/10.1002/bult.70.
414. Weiss, Y., Torralba, A., Fergus, R.: Spectral hashing. In: Advances in Neural Information
Processing Systems, pp. 1753–1760 (2009)
415. Wheeler, M.A., Stuss, D.T., Tulving, E.: Toward a theory of episodic memory: the frontal
lobes and autonoetic consciousness. Psychol. Bull. 121(3), 331 (1997)
416. White, R.W., Bennett, P.N., Dumais, S.T.: Predicting short-term interests using activity-based
search context. In: Proceedings of the 19th ACM International Conference on Information

References
335
and Knowledge Management, CIKM ’10, pp. 1009–1018 (2010). https://doi.org/10.1145/
1871437.1871565
417. Williams, H., Conway, M., Baddeley, A.: The boundaries of Episodic Memories, pp. 589–616.
Oxford University Press, Oxford (2008)
418. Wolters, M.K., Niven, E., Logie, R.H.: The art of deleting snapshots. In: CHI’14 Extended
Abstracts on Human Factors in Computing Systems, pp. 2521–2526. ACM (2014)
419. Wolters, M.K., Niven, E., Runardotter, M., Gallo, F., Maus, H., Logie, R.H.: Personal photo
preservation for the smartphone generation. In: Proceedings of the 33rd Annual ACM Con-
ference Extended Abstracts on Human Factors in Computing Systems, pp. 1549–1554. ACM
(2015)
420. Wu,X.,Hauptmann,A.G.,Ngo,C.W.:Practicaleliminationofnear-duplicatesfromwebvideo
search. In: Proceedings of the 15th International Conference on Multimedia, pp. 218–227.
ACM (2007)
421. Xie, L., Hong, R., Zhang, B., Tian, Q.: Image classiﬁcation and retrieval are one. In: Proceed-
ings of the 5th International Conference on Multimedia Retrieval, pp. 3–10. ACM (2015)
422. Yan, L., Zou, F., Guo, R., Gao, L., Zhou, K., Wang, C.: Feature aggregating hashing for image
copy detection. World Wide Web 19(2), 217–229 (2016)
423. Yang, C.Y., Yeh, H.H., Chen, C.S.: Video aesthetic quality assessment by combining seman-
tically independent and dependent features. In: Proceedings of International Conference on
Acoustics, Speech and Signal Processing (ICASSP), pp. 1165–1168. IEEE (2011)
424. Yao, L., Suryanarayan, P., Qiao, M., Wang, J.Z., Li, J.: Oscar: on-site composition and aes-
thetics feedback through exemplars for photographers. Int. J. Comput. Vis. 96(3), 353–383
(2012)
425. Yeh, C.H., Ho, Y.C., Barsky, B.A., Ouhyoung, M.: Personalized photograph ranking and
selection system. In: Proceedings of the 18th ACM International Conference on Multimedia,
MM ’10, pp. 211–220. ACM (2010)
426. Yeh, H.H., Yang, C.Y., Lee, M.S., Chen, C.S.: Video aesthetic quality assessment by temporal
integration of photo-and motion-based features. IEEE Trans. Multimed. 15(8), 1944–1957
(2013)
427. Yilmaz, E., Kanoulas, E., Aslam, J.A.: A simple and efﬁcient sampling method for estimating
AP and NDCG. In: 31st ACM SIGIR International Conference on Research and Development
in Information Retrieval, pp. 603–610. ACM, USA (2008)
428. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in deep neural
networks? In: CoRR (2014). arXiv:abs/1411.1792
429. Yu, H., Deng, Z.H., Yang, Y., Xiong, T.: A joint optimization model for image summarization
based on image content and tags. In: Proceedings of the Twenty-Eighth AAAI Conference on
Artiﬁcial Intelligence, AAAI’14, pp. 215–221. AAAI Press (2014)
430. Zacks, J.M., Speer, N.K., Swallow, K.M., Braver, T.S., Reynolds, J.R.: Event perception: a
mind-brain perspective. Psychol. Bull. 133(2), 273 (2007)
431. Zacks, J.M., Swallow, K.M.: Event segmentation. Current Direct. Psychol. Sci. 16(2), 80–84
(2007)
432. Zagoruyko, S., Komodakis, N.: Learning to compare image patches via convolutional neural
networks. In: Conference on Computer Vision and Pattern Recognition (CVPR). IEEE (2015)
433. Zhang, J., Marszalek, M., Lazebnik, S., Schmid, C.: Local features and kernels for classi-
ﬁcation of texture and object categories: a comprehensive study. Int. J. Comput. Vis. 73(2),
213–238 (2007)
434. Zhang, W., Ji, J., Zhu, J., Li, J., Xu, H., Zhang, B.: BitHash: an efﬁcient bitwise locality
sensitive hashing method with applications. Knowl. Based Syst. 97, 40–47 (2016)
435. Zhang, Y., Callan, J., Minka, T.: Novelty and redundancy detection in adaptive ﬁltering.
In: Proceedings of the 25th Annual International ACM SIGIR Conference on Research and
Development in Information Retrieval, SIGIR ’02, pp. 81–88 (2002)
436. Zhao, W.L., Tan, S., Ngo, C.W.: Large-scale near-duplicate web video search: challenge and
opportunity. In: International Conference on Multimedia and Expo (ICME), pp. 1624–1627.
IEEE (2009)

336
References
437. Zhao, Y., Hong, R., Jiang, J.: Visual summarization of image collections by fast ransac.
Neurocomputing 172, 48–52 (2016)
438. Zhou, J., Chen, J., Ye, J.: Clustered multi-task learning via alternating structure optimization.
In: Advances in Neural Information Processing Systems (NIPS 2011) (2011)
439. Zhou, W., Li, H., Lu, Y., Tian, Q.: Sift match veriﬁcation by geometric coding for large-scale
partial-duplicate web image search. Trans. Multimed. Comput. Commun. Appl. (TOMM)
9(1), 4 (2013)
440. Zhou, W., Li, H., Lu, Y., Tian, Q.: SIFT match veriﬁcation by geometric coding for large-scale
partial-duplicate web image search. Trans. Multimed. Comput. Commun. Appl. (TOMM)
9(1), 4 (2013)

Index
A
Activation propagation, 113
Active system, 183, 186, 190, 192, 197, 203,
222
Activity based methods, 112
Aesthetic quality assessment for photos, 65,
68
Aesthetic quality assessment for videos, 65,
70
Alert fatigue, 34
Appraisal, 108
C
CCDM, 229
Cinematography rules, 70
Cluster ﬁltering, 298
Clustering, 92, 290, 297
Clustering (event-based), 93
Cluster visiting, 298
CMIS, 215
Cold-start, 312
Collection, 211
Collection-based features, 296
Colorfulness visual feature, 68
Complementing human memory, 104
Composition visual feature, 69
Concept annotation of photo, 49
Concept annotation of video, 49
Concept correlations, 54
Concepts, 295
Condensation, 211
Content, 207, 211
Content Value Assessment, 195
Content Value Re-assessment, 201
Context, 207, 211
Context-aware Preservation Management,
202
Context Evolution Management, 201
Context hook, 136, 145, 159, 178, 179, 181
Contextualization, 196, 223
Contextualization method, 136, 181
Contextualization source, 136, 145
Contextualized remembering, 8
Context units, 136, 145, 159, 181
Coverage, 108, 281, 283, 290, 292, 297, 299,
309
cuDNN, 91
D
Dark archives, 280, 291
DBpedia, 132, 174, 181
DCNN, 49, 86
DCNN extension strategy, 52
DCNN ﬁne-tuning strategies, 52
DCNN re-initialization strategy, 52
DCNN transfer learning, 49
DCT, 82
De-contextualization, 196
Decay function, 112
Delegation, 105
Digestion, 105
Digital forgetting, 4, 102, 280, 291
Digital memory, 104
Digital Preservation System, 183, 186, 190,
192, 196, 197, 201
Digital vault, 286
Diversity, 108, 299
Dublin Core, 208
© Springer International Publishing AG 2018
V. Mezaris et al. (eds.), Personal Multimedia Preservation, Springer Series
on Cultural Computing, https://doi.org/10.1007/978-3-319-73465-1
337

338
Index
E
ELLA_LC, 55
Episodic memory, 13, 15, 18, 23–25, 27
Evolution layer, 199
Evolution monitoring, 201
Exchange support, 193
Expectation-oriented, 281
Expectation-oriented selection, 294
Expectations, 291, 294
F
Faces, 295
Feature extraction, 49
Features, 294, 303, 305
Flashbulb memories, 23
Forgetful, 189
Forgetful methods, 127
Forgetful search, 127
ForgetIT, 233
Forgetting actions, 103, 126
G
GATE Mímir, 180
H
Hedera, 140
I
ID Management, 193
Importance, 296
Incremental learning, 311
Information gain, 303–305
Information management, 184
Information piece, 136, 145, 159
Information value, 280
Information value assessment, 110
Integrative, 186
Item, 212
J
Joint ecosystem, 184
K
K-means, 68, 86, 94
Keep-it-all, 102
Knowledge base, 137
KSVM-iGSU, 71
M
Machine learning, 294, 305, 310
Managed forgetting, 7, 102, 243, 276
Managed forgetting and appraisal, 196
Managed forgetting in the PIMO, 250
Memories, 280
Memory Buoyancy (MB), 33, 103, 187, 196,
198
Memory Buoyancy in the PIMO, 252
Memory cues, 280
Metadata management, 198
METS, 227
MP-AF, 228
Multi-task learning, 49
N
Near-duplicate, 78
Near-duplicate detection, 79
Near-duplicate retrieval, 79
O
OAIS, 184, 190, 192, 201, 209, 213, 215,
217, 227
Ofﬂine processing, 81
Online learning, 311
Online processing, 81
Optimization, 299
P
PAIMAS, 196, 198, 227
PASCAL VOC, 58
Pattern visual feature, 69
Personal information management, 104,
239, 270
Personal Information Model (PIMO), 235
Personal knowledge, 132
Personal Preservation, 277
Personal Preservation, Obstacles, 234, 272
Personal Preservation, Personas, 258
Personalization, 310
Photo alterations, 82
Photo collections, 280
Photographic Rules, 68
Photo importance, 281, 292
Photo selection, 281, 291
Photo work, 34, 282
PIM, 104
PIMO Diary, 247
PIMO Photo Organization, 244
PoF Framework, 216, 225, 239, 244, 257,
266

Index
339
PoF Functional Model, 190, 192
PoF Information Model, 190, 206
PoF Middleware, 186, 201
PoF Reference Model, 183–185
Pre-ingest, 196
PREMIS, 228
Preservation, 280, 282, 288, 291, 309
Preservation contract, 193, 198, 212
Preservation contract management, 198
Preservation decision, 103, 128
Preservation entity, 212
Preservation management, 184
Preservation preparation, 193, 195, 196
Preservation Preparation Workﬂow, 266
Preservation Service Contract, 262
Preservation strategy, 257
Preservation Value (PV), 103, 107, 187, 196,
198, 212, 257, 281
Preservation Value, dimensions, 107, 258
Preserve-or-Forget approach, 7
Proactive interference, 16, 29
Prospective memory, 10, 18, 19, 33
PROV-O, 228
Q
Quality, 294, 299
R
Re-activation, 193, 195
Re-contextualization, 197
Remembering, 244, 247, 276
Revisiting, 280
S
Schema, 13–18, 20, 25, 26, 29, 32, 33, 44,
45
Search and Navigation, 197
Self Memory System, 25
Semantic annotation, 137
Semantic Desktop, 239
Semantic Desktop, approach, 235, 236
Semantic Desktop for Personal Preservation,
243, 256
Semantic Editor (Seed), 241
Semantic memory, 13–16, 24, 25, 33, 44
Semantic preservation, 107
Setting change, 199, 202
Sharpness visual feature, 69
Simplicity visual feature, 68
Single-task learning, 50
Situation, 207, 211
Situation change, 199
Situation proﬁle, 207, 211
Social graph, 295
Social media, 117
Subevent, 93
Summarization, 95, 283, 309
Support Vector Machine, 295
SVM, 73, 295, 298, 302
Synergetic preservation, 7
System change, 199, 203
T
TagMe, 141
TRECVID SIN task, 58
U
User investment, 291
V
VLAD, 86
W
Wikipedia, 133, 137, 144
Working memory, 12, 19, 20, 24, 33
World knowledge, 132, 144, 160, 170, 174


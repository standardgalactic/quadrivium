S I X T H
E D I T I O N
Linear Algebra
and Its Applications
David C. Lay
University of Maryland–College Park
Steven R. Lay
Lee University
Judi J. McDonald
Washington State University

Copyright © 2021, 2016, 2012 by Pearson Education, Inc. All Rights Reserved. Printed in the
United States of America. This publication is protected by copyright, and permission should be
obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or
transmission in any form or by any means, electronic, mechanical, photocopying, recording, or
otherwise. For information regarding permissions, request forms and the appropriate contacts
within the Pearson Education Global Rights & Permissions department, please visit
www.pearsoned.com/permissions/.
Photo credit: Page 1, Kim Kulish Corbis
PEARSON, ALWAYS LEARNING, and MYLAB are an exclusive trademarks in the U.S. and/or
other countries owned by Pearson Education, Inc. or its aﬃliates.
Unless otherwise indicated herein, any third-party trademarks that may appear in this work are the
property of their respective owners and any references to third-party trademarks, logos or other
trade dress are for demonstrative or descriptive purposes only. Such references are not intended to
imply any sponsorship, endorsement, authorization, or promotion of Pearson’s products by the
owners of such marks, or any relationship between the owner and Pearson Education, Inc. or its
aﬃliates, authors, licensees or distributors.
This work is solely for the use of instructors and administrators for the purpose of teaching courses
and assessing student learning. Unauthorized dissemination, publication or sale of the work, in
whole or in part (including posting on the internet) will destroy the integrity of the work and is
strictly prohibited.
ISBN-10: 0-13-588280-X
ISBN-13: 978-0-13588280-1

Contents
Chapter 10
Finite-State Markov Chains
C-1
INTRODUCTORY EXAMPLE: Googling Markov Chains
C-1
10.1
Introduction and Examples
C-2
10.2
The Steady-State Vector and Google’s PageRank
C-13
10.3
Communication Classes
C-25
10.4
Classiﬁcation of States and Periodicity
C-33
10.5
The Fundamental Matrix
C-42
10.6
Markov Chains and Baseball Statistics
C-54
Appendixes
1
Proof of Theorem 1
C-65
2
Probability
C-69
Chapter 10: Answers to Exercises
C-79
iii

10 Finite-State
Markov Chains
.Introductory Example
GOOGLING MARKOV CHAINS
Google means many things: it is an internet search engine,
the company that produces the search engine, and a
verb meaning to search on the Internet for a piece of
information. Although it may seem hard to believe, there
was a time before people could “google” to ﬁnd the
capital of Botswana, or a recipe for deviled eggs, or other
vitally important matters. Users of the internet depend
on trustworthy search engines—the amount of available
information is so vast that the searcher relies on the search
engine not only to ﬁnd those webpages that contain the
terms of the search, but also to return ﬁrst those webpages
most likely to be relevant to the search. Early search engines
had no good way of determining which pages were more
likely to be relevant. Searchers had to check the returned
pages one by one, which was a tedious and frustrating
process. This situation improved markedly in 1998, when
search engines began to use the information contained in
the hyperlinked structure of the World Wide Web to help to
rank pages. Foremost among this new generation of search
engines was Google, a project of two computer science
graduate students at Stanford University: Sergey Brin and
Lawrence Page.
Brin and Page reasoned that a webpage was important
if it had hyperlinks to it from other important pages. They
used the idea of the random surfer: a web surfer moving
from webpage to webpage merely by choosing at random
which hyperlink to follow. The motion of the surfer among
the webpages can be modeled using Markov chains, which
were introduced in Section 5.9. The pages that this random
surfer visits more often ought to be more important, and
thus more relevant, if their content matches the terms of a
search. Although Brin and Page did not know it at the time,
they were attempting to ﬁnd the steady-state vector for a
particular Markov chain whose transition matrix modeled
the hyperlinked structure of the web. After some important
modiﬁcations of this impressively large matrix (detailed in
Section 10.2), a steady-state vector can be found, and its
entries can be interpreted as the amount of time a random
surfer will spend at each webpage. The calculation of this
steady-state vector is the basis for Google’s PageRank
algorithm.
So the next time you google the capital of Botswana,
know that you are using the results of this chapter to ﬁnd
just the right webpage.
Even though the number of webpages is huge, it is still ﬁnite. When the link structure
of the World Wide Web is modeled by a Markov chain, each webpage is a state of the
Markov chain. This chapter continues the study of Markov chains begun in Section 5.9,
C-1

C-2
CHAPTER 10
Finite-State Markov Chains
focusing on those Markov chains with a ﬁnite number of states. Section 10.1 introduces
useful terminology and develops some examples of Markov chains: signal transmission
models, diﬀusion models from physics, and random walks on various sets. Random
walks on directed graphs will have particular application to the PageRank algorithm.
Section 10.2 deﬁnes the steady-state vector for a Markov chain. Although every Markov
chain has a steady-state vector, not every Markov chain converges to a steady-state
vector. When the Markov chain converges to a steady-state vector, that vector can be
interpreted as telling the amount of time the chain will spend in each state. This in-
terpretation is necessary for the PageRank algorithm, so the conditions under which a
Markov chain converges to a steady-state vector will be developed. The model for the
link structure of the World Wide Web will then be modiﬁed to meet these conditions,
forming what is called the Google matrix. Sections 10.3 and 10.4 discuss Markov chains
that do not converge to steady-state vectors. These Markov chains can be used to model
situations in which the chain eventually becomes conﬁned to one state or a set of states.
Section 10.5 introduces the fundamental matrix. This matrix can be used to calculate
the expected number of steps it takes the chain to move from one state to another, as
well as the probability that the chain ends up conﬁned to a particular state. In Section
10.6, the fundamental matrix is applied to a model for run production in baseball: the
number of batters in a half inning and the state in which the half inning ends will be of
vital importance in calculating the expected number of runs scored.
..10.1 Introduction and Examples
Recall from Section 5.9 that a Markov chain is a mathematical model for movement
between states. A process starts in one of these states and moves from state to state. The
moves between states are called steps or transitions. The terms “chain” and “process”
are used interchangeably, so the chain can be said to move between states and to be “at
a state” or “in a state” after a certain number of steps.
The state of the chain at any given step is not known; what is known is the probability
that the chain moves from state j to state i in one step. This probability is called a
transition probability for the Markov chain. The transition probabilities are placed in
a matrix called the transition matrix P for the chain by entering the probability of a
transition from state j to state i at the .i; j /-entry of P . So if there were m states named
1, 2; : : : m, the transition matrix would be the m  m matrix
P D
From:
1
j
m
2
6664
:::
#
pij
  
!
3
7775
To:
1
i
m
The probabilities that the chain is in each of the possible states after n steps are
listed in a state vector xn. If there are m possible states, the state vector would be
xn D
2
6666664
a1
:::
aj
:::
am
3
7777775
   Probability that the chain is at state j after n steps

10.1
Introduction and Examples
C-3
State vectors are probability vectors since their entries must sum to 1. The state vector
x0 is called the initial probability vector.
Notice that the j th column of P is a probability vector—its entries list the proba-
bilities of a move from state j to the states of the Markov chain. The transition matrix
is thus a stochastic matrix since all of its columns are probability vectors.
The state vectors for the chain are related by the equation
xnC1 D P xn
(1)
for n D 1; 2; : : :. Notice that Equation (1) may be used to show that
xn D P nx0
(2)
Thus any state vector xn may be computed from the initial probability vector x0 and an
appropriate power of the transition matrix P .
This chapter concerns itself with Markov chains with a ﬁnite number of states—that
is, those chains for which the transition matrix P is of ﬁnite size. To use a ﬁnite-state
Markov chain to model a process, the process must have the following properties, which
are implied by Equations (1) and (2).
1. Since the values in the vector xnC1 depend only on the transition matrix P and on
xn, the state of the chain before time n must have no eﬀect on its state at time n C 1
and beyond.
2. Since the transition matrix P does not change with time, the probability of a transition
from one state to another must not depend on how many steps the chain has taken.
Even with these restrictions, Markov chains may be used to model an amazing variety
of processes. Here is a sampling.
Signal Transmission
Consider the problem of transmitting a signal along a telephone line or by radio waves.
Each piece of data must pass through a multistage process to be transmitted, and at each
stage there is a probability that a transmission error will cause the data to be corrupted.
Assume that the probability of an error in transmission is not aﬀected by transmission
errors in the past and does not depend on time, and that the number of possible pieces
of data is ﬁnite. The transmission process may then be modeled by a Markov chain. The
object of interest is the probability that a piece of data goes through the entire multistage
process without error. Here is an example of such a model.
EXAMPLE 1
Suppose that each bit of data is either a 0 or a 1, and at each stage there
is a probability p that the bit will pass through the stage unchanged. Thus the probability
is 1   p that the bit will be transposed. The transmission process is modeled by a Markov
chain, with states 0 and 1 and transition matrix
P D
From:
0
1

p
1   p
1   p
p

To:
0
1
It is often easier to visualize the action of a Markov chain by representing its transition
probabilities graphically, as in Figure 1. The points are the states of the chain, and the
arrows represent the transitions.
Suppose that p D :99. Find the probability that the signal 0 will still be a 0 after a
two-stage transmission process.

C-4
CHAPTER 10
Finite-State Markov Chains
1 2 p
p
p
1 2 p 
1
0
FIGURE 1 Transition diagram for signal
transmission.
SOLUTION
Since the signal begins as 0, the probability that the chain begins at 0 is
100%, or 1; that is, the initial probability vector is x0 D
 1
0

. To ﬁnd the probability of
a two-step transition, compute
x2 D P 2x0 D
 :99
:01
:01
:99
2 1
0

D
 :9802
:0198
:0198
:9802
 1
0

D
 :9802
:0198

The probability that the signal 0 will still be a 0 after the two-stage process is thus
:9802. Notice that this is not the same as the probability that the 0 is transmitted without
error; that probability would be .:99/2 D :9801. Our analysis includes the very small
probability that the 0 is erroneously changed to 1 in the ﬁrst step, then back to 0 in the
second step of transmission.
Diffusion
Consider two compartments ﬁlled with diﬀerent gases that are separated only by a mem-
brane that allows molecules of each gas to pass from one container to the other. The two
gases will then diﬀuse into each other over time, so that each container will contain
some mixture of the gases. The major question of interest is what mixture of gases is in
each container at a time after the containers are joined. A famous mathematical model
for this process was described originally by the physicists Paul and Tatyana Ehrenfest.
Since their preferred term for “container” was urn, the model is called the Ehrenfest
urn model for diﬀusion.
Label the two urns A and B, and place k molecules of gas in each urn. At each time
step, select one of the 2k molecules at random and move it from its urn to the other urn,
and keep track of the number of molecules in urn A. This process can be modeled by
a ﬁnite-state Markov chain: the number of molecules in urn A after n C 1 time steps
depends only on the number in urn A after n time steps, the transition probabilities do
not change with time, and the number of states is ﬁnite.
EXAMPLE 2
For this example, let k D 3. Then the two urns contain a total of 6
molecules, and the possible states for the Markov chain are 0, 1, 2, 3, 4, 5, and 6. Notice
ﬁrst that if there are 0 molecules in urn A at time n, then there must be 1 molecule in
urn A at time n C 1, and if there are 6 molecules in urn A at time n, then there must be
5 molecules in urn A at time n C 1. In terms of the transition matrix P , this means that
the columns in P corresponding to states 0 and 6 are
p0 D
2
666666664
0
1
0
0
0
0
0
3
777777775
and
p6 D
2
666666664
0
0
0
0
0
1
0
3
777777775

10.1
Introduction and Examples
C-5
If there are i molecules in urn A at time n, with 0 < i < 6, then there must be either
i   1 or i C 1 molecules in urn A at time n C 1. In order for a transition from i to i   1
molecules to occur, one of the i molecules in urn A must be selected to move; this event
happens with probability i=6. Likewise, a transition from i to i C 1 molecules occurs
when one of the 6   i molecules in urn B is selected, and this occurs with probability
.6   i/=6. Allowing i to range from 1 to 5 creates the columns of P corresponding to
these states, and the transition matrix for the Ehrenfest urn model with k D 3 is thus
P D
0
1
2
3
4
5
6
2
666666664
0
1
0
0
0
0
0
1=6
0
5=6
0
0
0
0
0
1=3
0
2=3
0
0
0
0
0
1=2
0
1=2
0
0
0
0
0
2=3
0
1=3
0
0
0
0
0
5=6
0
1=6
0
0
0
0
0
1
0
3
777777775
0
1
2
3
4
5
6
Figure 2 shows a transition diagram of this Markov chain. Other models for diﬀusion
will be considered in the Exercises for this section.
1
1
0
1
5
6
2
3
4
5
6
5
6
2
3
2
3
1
2
1
2
1
3
1
3
1
6
1
6
FIGURE 2 Transition diagram of the Ehrenfest urn model.
Random Walks on f1, : : : , ng
Molecular motion has long been an important issue in physics. Einstein and others inves-
tigated Brownian motion, which is a mathematical model for the motion of a molecule
exposed to collisions with other molecules. The analysis of Brownian motion turns out to
be quite complicated, but a discrete version of Brownian motion called a random walk
provides an introduction to this important model. Think of the states f1; 2; : : : ; ng as
lying on a line. Place a molecule at a point that is not on the end of the line. At each step
the molecule moves left one unit with probability p and right one unit with probability
1   p. See Figure 3. The molecule thus “walks randomly” along the line. If p D 1=2,
the walk is called simple, or unbiased. If p ¤ 1=2, the walk is said to be biased.
...
...
p
p
p
p
1 2 p 
1 2 p 
1 2 p 
1 2 p 
k 2 2
k 2 1
k
k 1 1
 
k 1 2
FIGURE 3 A graphical representation of a random walk.
The molecule must move to either the left or the right at the states 2; : : : ; n   1, but
it cannot do this at the endpoints 1 and n. The molecule’s possible movements at the
endpoints 1 and n must be speciﬁed. One possibility is to have the molecule stay at an
endpoint forever once it reaches either end of the line. This is called a random walk
with absorbing boundaries, and the endpoints 1 and n are called absorbing states.
Another possibility is to have the molecule bounce back one unit when an endpoint is
reached. This is called a random walk with reﬂecting boundaries.

C-6
CHAPTER 10
Finite-State Markov Chains
EXAMPLE 3
A random walk on f1; 2; 3; 4; 5g with absorbing boundaries has a tran-
sition matrix of
P D
1
2
3
4
5
2
6666664
1
0
0
0
0
p
0
1   p
0
0
0
p
0
1   p
0
0
0
p
0
1   p
0
0
0
0
1
3
7777775
1
2
3
4
5
since the molecule at state 1 has probability 1 of staying at state 1, and a molecule at state
5 has probability 1 of staying at state 5. A random walk on f1; 2; 3; 4; 5g with reﬂecting
boundaries has a transition matrix of
P D
1
2
3
4
5
2
6666664
0
1
0
0
0
p
0
1   p
0
0
0
p
0
1   p
0
0
0
p
0
1   p
0
0
0
1
0
3
7777775
1
2
3
4
5
since the molecule at state 1 has probability 1 of moving to state 2, and a molecule at
state 5 has probability 1 of moving to state 4.
In addition to their use in physics, random walks also occur in problems related to
gambling and its more socially acceptable variants: the stock market and the insurance
industry.
EXAMPLE 4
Consider a very simple casino game. A gambler (who still has some
money left with which to gamble) ﬂips a fair coin and calls heads or tails. If the gambler
is correct, he wins a dollar; if he is wrong, he loses a dollar. Suppose that the gambler
will quit the game when he has either won n dollars or lost all of his money.
Suppose that n D 7 and the gambler starts with $4. Notice that the gambler’s win-
nings move either up or down $1 for each coin ﬂip, and once the gambler’s winnings
reach 0 or 7, they do not change any more since the gambler has quit the game. Thus
the gambler’s winnings may be modeled by a random walk on f0; 1; 2; 3; 4; 5; 6; 7g with
absorbing boundaries. Since a move up or down is equally likely in this case, p D 1=2
and the walk is simple.
Random Walks on Graphs
It is useful to perform random walks on geometrical objects other than the one-dimen-
sional line. For example, a graph is a collection of points and lines connecting some of
the points. The points of a graph are called vertices, and the lines connecting the vertices
are called the edges. In Figure 4, the vertices are labeled with the numbers 1 through 7.
To deﬁne a simple random walk on a graph, allow the chain to move from vertex
to vertex on the graph. At each step, the chain is equally likely to move along any of
the edges attached to the vertex. For example, if the molecule is at state 5 in Figure 4, it
has probability 1=2 of moving to state 2 and probability 1=2 of moving to state 6. This
Markov chain is called a simple random walk on a graph.
1
3
4
2
5
6
7
FIGURE 4
A graph with seven vertices.

10.1
Introduction and Examples
C-7
EXAMPLE 5
The simple random walk on the graph in Figure 4 has transition matrix
P D
1
2
3
4
5
6
7
2
666666666664
0
1=2
1=2
0
0
0
0
1=3
0
1=3
0
1=3
0
0
1=4
1=4
0
1=4
0
1=4
0
0
0
1
0
0
0
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
0
0
0
0
0
1
0
3
777777777775
1
2
3
4
5
6
7
Find the probability that the chain in Figure 4 moves from state 6 to state 2 in exactly
three steps.
SOLUTION
Compute
x3 D P 3x0 D P 3
2
666666664
0
0
0
0
0
1
0
3
777777775
D
2
666666664
:0833
:0417
:4028
0
:2778
0
:1944
3
777777775
Thus the probability of moving from state 6 to state 2 in exactly three steps is :0417.
Sometimes interpreting a random process as a random walk on a graph can be useful.
EXAMPLE 6
Suppose a mouse runs through the ﬁve-room maze at left in Figure 5.
The mouse moves to a diﬀerent room at each time step. When the mouse is in a particular
room, it is equally likely to choose any of the doors out of the room. Note that a Markov
chain can model the motion of the mouse. Find the probability that a mouse starting in
room 3 returns to that room in exactly ﬁve steps.
1
2
3
5
4
1
2
3
5
4
FIGURE 5 Five-room maze with overlaid
graph.
SOLUTION
A graph is overlaid on the maze, as shown at right in Figure 5. Notice
that the motion of the mouse is identical to a simple random walk on the graph, so the
transition matrix is
P D
1
2
3
4
5
2
6666664
0
1=2
1=2
0
0
1=3
0
1=3
1=3
0
1=4
1=4
0
1=4
1=4
0
1=3
1=3
0
1=3
0
0
1=2
1=2
0
3
7777775
1
2
3
4
5

C-8
CHAPTER 10
Finite-State Markov Chains
and
x5 D P 5x0 D P 5
2
66664
0
0
1
0
0
3
77775
D
2
66664
:1507
:2143
:2701
:2143
:1507
3
77775
Thus the probability of a return to room 3 in exactly ﬁve steps is :2701.
Another interesting object on which to walk randomly is a directed graph. A di-
rected graph is a graph in which the vertices are joined not by lines but by arrows. See
Figure 6.
To perform a simple random walk on a directed graph, allow the chain to move
from vertex to vertex on the graph but only in the directions allowed by the arrows. At
each step the walker is equally likely to move away from its current state along any of
the arrows pointing away from the vertex. For example, if the molecule is at state 6 in
Figure 6, it has probability 1=3 of moving to state 3, state 5, or state 7.
The PageRank algorithm that Google uses to rank the importance of pages on the
World Wide Web (see Introductory Example page C-1) begins with a simple random
1
3
4
2
5
6
7
FIGURE 6
A directed graph with seven
vertices.
walk on a directed graph. The Web is modeled as a directed graph in which the vertices
are the pages and an arrow is drawn from page j to page i if there is a hyperlink from
page j to page i. A person surfs randomly in the following way: when the surfer gets to
a page, he or she chooses a link from the page so that it is equally probable to choose any
of the possible “outlinks.” The surfer then follows the link to arrive at another page. The
person surﬁng in this way is performing a simple random walk on the directed graph
that is the World Wide Web.
EXAMPLE 7
Consider a set of seven pages hyperlinked by the directed graph in
Figure 6. If the random surfer starts at page 5, ﬁnd the probability that the surfer will be
at page 3 after four clicks.
SOLUTION
The transition matrix for the simple random walk on the directed graph is
P D
1
2
3
4
5
6
7
2
666666666664
0
0
1
0
0
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
0
0
0
0
1
0
0
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
0
0
0
0
0
0
1
3
777777777775
1
2
3
4
5
6
7
Notice that there are no arrows coming from either state 4 or state 7 in Figure 6. If the
surfer clicks on a link to either of these pages, there is no link to click on next. (Using
the “Back” key is not allowed: the state of the chain before time n must have no eﬀect
on its state at time n C 1 and beyond.) For this reason, the transition probabilities p44
and p77 are set equal to 1—the chain must stay at state 4 or state 7 forever once it enters
either of these states. Computing x4 gives

10.1
Introduction and Examples
C-9
x4 D P 4x0 D
2
666666664
:1319
:0833
:0880
:1389
:2199
:0833
:2546
3
777777775
so the probability of being at page 3 after exactly four clicks is :0880.
States 4 and 7 are absorbing states for the Markov chain in the previous example.
In technical terms, they are called dangling nodes and are quite common on the Web;
data pages in particular usually have no links leading from them. Dangling nodes will
appear in the next section, where the PageRank algorithm will be explained.
As noted in Section 5.9, the most interesting questions about Markov chains concern
their long-term behavior—that is, the behavior of xn as n increases. This study will
occupy a large portion of this chapter. The foremost issues in our study will be whether
the sequence of vectors fxng is converging to some limiting vector as n increases, and
how to interpret this limiting vector if it exists. Convergence to a limiting vector will be
addressed in the next section.
..
Practice Problems
.
1. Fill in the missing entries in the stochastic matrix.
P D
2
4
:1

:2

:3
:3
:6
:2

3
5
2. In the signal transmission model in Example 1, suppose that p D :97. Find the
probability that the signal “1” will be a “0” after a three-stage transmission process.
10.1 Exercises
In Exercises 1 and 2, determine whether P is a stochastic matrix.
If P is not a stochastic matrix, explain why not.
1. a. P D
 :3
:4
:7
:6

b. P D
 :3
:7
:4
:6

2. a. P D
 1
:5
0
:5

b. P D
 :2
1:1
:8
 :1

In Exercises 3 and 4, compute x3 in two ways: by computing x1
and x2, and by computing P 3.
3. P D
 :6
:5
:4
:5

, x0 D
 1
0

4. P D
 :3
:8
:7
:2

, x0 D
 :5
:5

In Exercises 5 and 6, the transition matrix P for a Markov chain
with states 0 and 1 is given. Assume that in each case the chain
starts in state 0 at time n D 0. Find the probability that the chain
will be in state 1 at time n.
5. P D
 1=3
3=4
2=3
1=4

, n D 3
6. P D
 :4
:2
:6
:8

, n D 5
In Exercises 7 and 8, the transition matrix P for a Markov chain
with states 0, 1, and 2 is given. Assume that in each case the chain
starts in state 0 at time n D 0. Find the probability that the chain
will be in state 1 at time n.
7. P D
2
4
1=3
1=4
1=2
1=3
1=2
1=4
1=3
1=4
1=4
3
5, n D 2
8. P D
2
4
:1
:2
:4
:6
:3
:4
:3
:5
:2
3
5, n D 3
9. Consider a pair of Ehrenfest urns labeled A and B. There are
currently 3 molecules in urn A and 1 in urn B. What is the
probability that the exact same situation will apply after
a. 4 selections?
b. 5 selections?

C-10
CHAPTER 10
Finite-State Markov Chains
10. Consider a pair of Ehrenfest urns labeled A and B. There are
currently no molecules in urn A and 5 in urn B. What is the
probability that the exact same situation will apply after
a. 4 selections?
b. 5 selections?
11. Consider an unbiased random walk on the set f1; 2; 3; 4g.
What is the probability of moving from 2 to 3 in exactly 3
steps if the walk has
a. reﬂecting boundaries?
b. absorbing boundaries?
12. Consider a biased random walk on the set f1; 2; 3; 4g with
probability p D :2 of moving to the left. What is the proba-
bility of moving from 2 to 3 in exactly 3 steps if the walk has
a. reﬂecting boundaries?
b. absorbing boundaries?
In Exercises 13 and 14, ﬁnd the transition matrix for the simple
random walk on the given graph.
13. 1
2
4
3
5
14.
1
2
3
4
In Exercises 15 and 16, ﬁnd the transition matrix for the simple
random walk on the given directed graph.
15.
1
2
3
4
16.
1
4
2
5
3
In Exercises 17 and 18, suppose a mouse wanders through the
given maze. The mouse must move into a diﬀerent room at each
time step and is equally likely to leave the room through any of the
available doorways.
17. The mouse is placed in room 2 of the maze shown.
a. Construct a transition matrix and an initial probability
vector for the mouse’s travels.
b. What are the probabilities that the mouse will be in each
of the rooms after 3 moves?
3
4
5
1
2
18. The mouse is placed in room 3 of the maze shown below.
a. Construct a transition matrix and an initial probability
vector for the mouse’s travels.
b. What are the probabilities that the mouse will be in each
of the rooms after 4 moves?
1
2
3
5
4
In Exercises 19 and 20, suppose a mouse wanders through the
given maze, some of whose doors are “one-way”: they are just
large enough for the mouse to squeeze through in only one direc-
tion. The mouse still must move into a diﬀerent room at each time
step if possible. When faced with accessible openings into two or
more rooms, the mouse chooses them with equal probability.
19. The mouse is placed in room 1 of the following maze.
a. Construct a transition matrix and an initial probability
vector for the mouse’s travels.
b. What are the probabilities that the mouse will be in each
of the rooms after 4 moves?
1
3
2
4
6
5
20. The mouse is placed in room 1 of the maze shown.
a. Construct a transition matrix and an initial probability
vector for the mouse’s travels.
b. What are the probabilities that the mouse will be in each
of the rooms after 3 moves?
3
4
5
1
2
In Exercises 21–26, mark each statement True or False. Justify
each answer.
21. (T/F) The columns of a transition matrix for a Markov chain
must sum to 1.
22. (T/F) The rows of a transition matrix for a Markov chain must
sum to 1.
23. (T/F) The transition matrix P may change over time.
24. (T/F) If fxng is a Markov chain, then xnC1 must depend only
on the transition matrix and xn.
25. (T/F) The .i; j /-entry in a transition matrix P gives the
probability of a move from state j to state i.
26. (T/F) The .i; j /-entry in P 3 gives the probability of a move
from state i to state j in exactly three time steps.

10.1
Introduction and Examples
C-11
27. The weather in Charlotte, North Carolina, can be classiﬁed
as sunny, cloudy, or rainy on a given day. Climate data from
2003 reveal that
 If a day is sunny, then the next day will be sunny with
probability .65, cloudy with probability .1, and rainy
with probability .25.
 If a day is cloudy, then the next day will be sunny with
probability .25, cloudy with probability .25, and rainy
with probability .5.
 If a day is rainy, then the next day will be sunny with
probability .25, cloudy with probability .15, and rainy
with probability .60.
Suppose it is cloudy on Monday. Use a Markov chain to ﬁnd
the probabilities of the diﬀerent kinds of weather on Friday.
28. Suppose that whether it rains in Charlotte tomorrow depends
on the weather conditions for today and yesterday. Climate
data from 2003 show that
 If it rained yesterday and today, then it will rain
tomorrow with probability .58.
 If it rained yesterday but not today, then it will rain
tomorrow with probability .29.
 If it rained today but not yesterday, then it will rain
tomorrow with probability .47.
 If it did not rain yesterday or today, then it will rain
tomorrow with probability .31.
Even though the weather depends on the last two days in this
case, we can create a Markov chain model using the states
1
it rained yesterday and today
2
it rained yesterday but not today
3
it rained today but not yesterday
4
it did not rain yesterday or today
So, for example, the probability of a transition from state 1 to
state 1 is :58, and the transition from state 1 to state 3 is 0.
a. Complete the creation of the transition matrix for this
Markov chain.
b. If it rains on Tuesday and is clear on Wednesday, what is
the probability of no rain on the next weekend?
29. Consider a set of four webpages hyperlinked by the directed
graph in Exercise 15. If a random surfer starts at page 1, what
is the probability that the surfer will be at each of the pages
after 3 clicks?
30. Consider a set of ﬁve webpages hyperlinked by the directed
graph in Exercise 16. If a random surfer starts at page 2, what
is the probability that the surfer will be at each of the pages
after 4 clicks?
31. Consider a model for signal transmission in which data is sent
as two-bit bytes. Then there are four possible bytes, 00, 01,
10, and 11, which are the states of the Markov chain. At each
stage there is a probability p that each bit will pass through
the stage unchanged.
a. Construct the transition matrix for the model.
b. Suppose that p D :99. Find the probability that the signal
“01” will still be “01” after a three-stage transmission.
32. Consider a model for signal transmission in which data is
sent as three-bit bytes. Construct the transition matrix for the
model.
33. Another version of the Ehrenfest model for diﬀusion starts
with k molecules of gas in each urn. One of the 2k molecules
is picked at random just as in the Ehrenfest model in the
text. The chosen molecule is then moved to the other urn
with a ﬁxed probability p and is placed back in its urn with
probability 1   p. (Note that the Ehrenfest model in the text
is this model with p D 1.)
a. Let k D 3. Find the transition matrix for this model.
b. Let k D 3 and p D 1=2. If there are currently no
molecules in urn A, what is the probability that there will
be 3 molecules in urn A after 5 selections?
34. Another model for diﬀusion is called the Bernoulli-Laplace
model. Two urns (urn A and urn B) contain a total of 2k
molecules. In this case, k of the molecules are of one type
(called type I molecules) and k are of another type (type II
molecules). In addition, k molecules must be in each urn at
all times. At each time step, a pair of molecules is selected,
one from urn A and one from urn B, and these molecules
change urns. Let the Markov chain model the number of type
I molecules in urn A (which is also the number of type II
molecules in urn B).
a. Suppose that there are j type I molecules in urn A with
0 < j < k. Explain why the probability of a transition to
j   1 type I molecules in urn A is .j=k/2, and why the
probability of a transition to j C 1 type I molecules in
urn A is ..k   j /=k/2.
b. Let k D 5. Use the result in part (a) to set up the transition
matrix for the Markov chain that models the number of
type I molecules in urn A.
c. Let k D 5 and begin with all type I molecules in urn A.
What is the distribution of type I molecules after 3 time
steps?
35. To win a game in tennis, one player must score four points
and must also score at least two points more than his or
her opponent. Thus if the two players have scored an equal
number of points (four or more), which is called “deuce” in
tennis jargon, one player must then score two points in a row
to win the game. Suppose that players A and B are playing
a game of tennis that is at deuce. If A wins the next point
it is called “advantage A,” while if B wins the point it is
“advantage B.” If the game is at advantage A and player A
wins the next point, then player A wins the game. If player B
wins the point at advantage A, the game is back at deuce.
a. Suppose the probability of player A winning any point is
p. Model the progress of a tennis game starting at deuce
using a Markov chain with the following ﬁve states.

C-12
CHAPTER 10
Finite-State Markov Chains
1
deuce
2
advantage A
3
advantage B
4
A wins the game
5
B wins the game
Find the transition matrix for this Markov chain.
b. Let p D :6. Find the probability that the game will be at
“advantage B” after three points starting at deuce.
36. Volleyball uses two diﬀerent scoring systems in which a team
must win by at least two points. In both systems, a rally begins
with a serve by one of the teams and ends when the ball goes
out of play or touches the ﬂoor or a player commits a fault.
The team that wins the rally gets to serve for the next rally.
Games are played to 15, 25, or 30 points.
a. In rally point scoring, the team that wins a rally is awarded
a point no matter which team served for the rally. Assume
that team A has probability p of winning a rally for
which it serves, and that team B has probability q of
winning a rally for which it serves. Model the progress of a
volleyball game using a Markov chain with the following
six states.
1
tied – A serving
2
tied – B serving
3
A ahead by 1 point – A serving
4
B ahead by 1 point – B serving
5
A wins the game
6
B wins the game
Find the transition matrix for this Markov chain.
b. Suppose that team A and team B are tied 15–15 in a 15-
point game and that team A is serving. Let p D q D :6.
Find the probability that the game will not be ﬁnished after
three rallies.
c. In side out scoring, the team that wins a rally is awarded a
point only when it served for the rally. Assume that team
A has probability p of winning a rally for which it serves,
and that team B has probability q of winning a rally for
which it serves. Model the progress of a volleyball game
using a Markov chain with the following eight states.
1
tied – A serving
2
tied – B serving
3
A ahead by 1 point – A serving
4
A ahead by 1 point – B serving
5
B ahead by 1 point – A serving
6
B ahead by 1 point – B serving
7
A wins the game
8
B wins the game
Find the transition matrix for this Markov chain.
d. Suppose that team A and team B are tied 15–15 in a 15-
point game and that team A is serving. Let p D q D :6.
Find the probability that the game will not be ﬁnished after
three rallies.
37. Suppose that P is a stochastic matrix all of whose entries are
greater than or equal to p. Show that all of the entries in P n
are greater than or equal to p for n D 1; 2; : : :.
..
Solutions to Practice Problems
.
1. Since a stochastic matrix must have columns that sum to 1,
P D
2
4
:1
:5
:2
:3
:3
:3
:6
:2
:5
3
5
2. The transition matrix for the model is
P D
 :97
:03
:03
:97

Since the signal begins as “1,” the initial probability vector is
x0 D
 0
1

To ﬁnd the probability of a three-step transition, compute
x2 D P 3x0 D
 :9153
:0847
:0847
:9153
 0
1

D
 :0847
:9153

The probability of a change to “0” is thus :0847.

10.2
The Steady-State Vector and Google’s PageRank
C-13
..10.2 The Steady-State Vector and Google’s PageRank
As was seen in Section 5.9, the most interesting aspect of a Markov chain is its long-
range behavior: the behavior of xn as n increases without bound. In many cases, the
sequence of vectors fxng is converging to a vector that is called the steady-state vector
for the Markov chain. This section will review how to compute the steady-state vector
of a Markov chain, explain how to interpret this vector if it exists, and oﬀer an expanded
version of Theorem 11 in Section 5.9, which describes the circumstances under which
fxng converges to a steady-state vector. This theorem will be applied to the Markov
chain model used for the World Wide Web in the previous section and will show how
the PageRank method for ordering the importance of webpages is derived.
Steady-State Vectors
In many cases, the Markov chain xn and the matrix P n change very little for large values
of n.
EXAMPLE 1
To begin, recall Example 3 in Section 5.9. That example concerned
a Markov chain with transition matrix P D
2
4
:5
:2
:3
:3
:8
:3
:2
0
:4
3
5 and initial probability
vector x0 D
2
4
1
0
0
3
5. The vectors xn were seen to be converging to the vector q D
2
4
:3
:6
:1
3
5.
This result may be written as lim
n!1 xn D q. Increasing powers of the transition matrix P
may also be computed, as follows:
P 2 D
2
4
:3700
:2600
:3300
:4500
:7000
:4500
:1800
:0400
:2200
3
5
P 3 D
2
4
:3290
:2820
:3210
:5250
:6500
:5250
:1460
:0680
:1540
3
5
P 4 D
2
4
:3133
:2914
:3117
:5625
:6250
:5625
:1242
:0836
:1258
3
5
P 5 D
2
4
:3064
:2958
:3061
:5813
:6125
:5813
:1123
:0917
:1127
3
5
P 10 D
2
4
:3002
:2999
:3002
:5994
:6004
:5994
:1004
:0997
:1004
3
5
P 15 D
2
4
:3000
:3000
:3000
:6000
:6000
:6000
:1000
:1000
:1000
3
5
so the sequence of matrices fP ng also seems to be converging to a matrix as n increases,
and this matrix has the unusual property that all of its columns equal q. The example
also showed that P q D q. This equation forms the deﬁnition of the steady-state vector
and is a straightforward way to calculate it.
DEFINITION
If P is a stochastic matrix, then a steady-state vector (or equilibrium vector or
invariant probability vector) for P is a probability vector q such that
P q D q
Exercises 40 and 41 will show that every stochastic matrix P has a steady-state
vector q. Notice that 1 must be an eigenvalue of any stochastic matrix, and the steady-
state vector is a probability vector that also an eigenvector of P associated with the
eigenvalue 1.

C-14
CHAPTER 10
Finite-State Markov Chains
Although the deﬁnition of the steady-state vector makes the calculation of q
straightforward, it has the major drawback that there are Markov chains that have a
steady-state vector q but for which lim
n!1 xn ¤ q: the deﬁnition is not suﬃcient for xn
to converge. Examples 3 to 5 will show diﬀerent ways in which xn can fail to converge.
Later in this section, the conditions under which lim
n!1 xn D q will be restated. For now,
consider what q means when lim
n!1 xn D q, as it does in the previous example. When
lim
n!1 xn D q, there are two ways to interpret this vector.
 Since xn is approximately equal to q for large n, the entries in q approximate the
probability that the chain is in each state after n time steps. Thus in the previous
example, no matter what the value of the initial probability vector is, after many
steps the probability that the chain will be in state 1 is approximately q1 D :3.
Likewise, the probability that the chain will be in state 2 in the distant future is
approximately q2 D :6, and the probability that the chain will be in state 3 in
the distant future is approximately q3 D :1. So the entries in q give long-run
probabilities.
 When N is large, q approximates xn for almost all values of n  N . Thus the
entries in q approximate the proportion of time steps that the chain spends in
each state. In the previous example the chain will end up spending :3 of the time
steps in state 1, :6 of the time steps in state 2, and :1 of the time steps in state 3.
So the entries in q give the proportion of the time steps spent in each state, which
are called the occupation times for each state.
EXAMPLE 2
For an application of computing q, consider the mouse-in-the-maze
example (Example 6, Section 10.1). In this example, the position of a mouse in a ﬁve-
room maze is modeled by a Markov chain with states f1; 2; 3; 4; 5g and transition matrix
P D
1
2
3
4
5
2
66664
0
1=2
1=2
0
0
1=3
0
1=3
1=3
0
1=4
1=4
0
1=4
1=4
0
1=3
1=3
0
1=3
0
0
1=2
1=2
0
3
77775
1
2
3
4
5
The steady-state vector may be computed by solving the system P q D q, which is
equivalent to the homogeneous system .P   I/q D 0. Row reduction yields
2
66664
 1
1=3
1=4
0
0
0
1=2
 1
1=4
1=3
0
0
1=2
1=3
 1
1=3
1=2
0
0
1=3
1=4
 1
1=2
0
0
0
1=4
1=3
 1
0
3
77775

2
66664
1
0
0
0
 1
0
0
1
0
0  3=2
0
0
0
1
0
 2
0
0
0
0
1  3=2
0
0
0
0
0
0
0
3
77775
so a general solution is
q5
2
66664
1
3=2
2
3=2
1
3
77775

10.2
The Steady-State Vector and Google’s PageRank
C-15
Letting q5 be the reciprocal of the sum of the entries in the vector results in the steady-
state vector
q D 1
7
2
66664
1
3=2
2
3=2
1
3
77775
D
2
66664
1=7
3=14
2=7
3=14
1=7
3
77775

2
66664
:142857
:214286
:285714
:214286
:142857
3
77775
There are again two interpretations for q: long-run probabilities and occupation times.
After many moves, the probability that the mouse will be in room 1 at a given time is
approximately 1/7 no matter where the mouse began its journey. Put another way, the
mouse is expected to be in room 1 for 1/7 (or about 14.3%) of the time.
Again notice that taking high powers of the transition matrix P gives matrices whose
columns are converging to q; for example,
P 10 D
2
66664
:144169
:141561
:142613
:144153
:142034
:212342
:216649
:214286
:211922
:216230
:285226
:285714
:286203
:285714
:285226
:216230
:211922
:214286
:216649
:212342
:142034
:144153
:142613
:141561
:144169
3
77775
The columns of P 10 are very nearly equal to each other, and each column is also nearly
equal to q.
Interpreting the Steady-State Vector
As noted previously, every stochastic matrix will have a steady-state vector, but in some
cases steady-state vectors cannot be interpreted as vectors of long-run probabilities or
of occupation times. The following examples show some diﬃculties.
EXAMPLE 3
Consider an unbiased random walk on f1; 2; 3; 4; 5g with absorbing
boundaries. The transition matrix is
P D
1
2
3
4
5
2
66664
1
0
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
0
1
3
77775
1
2
3
4
5
Notice that only two long-term possibilities exist for this chain: it must end up in state 1
or state 5. Thus the probability that the chain is in state 2, 3, or 4 becomes smaller and
smaller as n increases, as P n illustrates:
P 20 D
2
66664
1
:74951
:49951
:24951
0
0
:00049
0
:00049
0
0
0
:00098
0
0
0
:00049
0
:00049
0
0
:24951
:49951
:74951
1
3
77775
P 30 D
2
66664
1
:749985
:499985
:249985
0
0
:000015
0
:000015
0
0
0
:000030
0
0
0
:000015
0
:000015
0
0
:249985
:499985
:749985
1
3
77775

C-16
CHAPTER 10
Finite-State Markov Chains
It seems that P n converges to the matrix
2
66664
1
:75
:5
:25
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
:25
:5
:75
1
3
77775
as n increases. But the columns of this matrix are not equal; the probability of ending
up either at 1 or at 5 depends on where the chain begins. Although the chain has steady-
state vectors, they cannot be interpreted as in Example 1. Exercise 35 conﬁrms that if
0  q  1 the vector
2
66664
q
0
0
0
1   q
3
77775
is a steady-state vector for P . This matrix then has an inﬁnite number of possible steady-
state vectors, which shows in another way that xn cannot be expected to have convergent
behavior that does not depend on x0.
EXAMPLE 4
Consider an unbiased random walk on f1; 2; 3; 4; 5g with reﬂecting
boundaries. The transition matrix is
P D
1
2
3
4
5
2
66664
0
1
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1
0
3
77775
1
2
3
4
5
If the chain xn starts at state 1, notice that it can return to 1 only when n is even, while the
chain can be at state 2 only when n is odd. In fact, the chain must be at an even-numbered
site when n is odd and at an odd-numbered site when n is even. If the chain were to start at
state 2, however, this situation would be reversed: the chain must be at an odd-numbered
site when n is odd and at an even-numbered site when n is even. Therefore, P n cannot
converge to a unique matrix since P n looks very diﬀerent depending on whether n is
even or odd, as shown:
P 20 D
2
66664
:2505
0
:2500
0
:2495
0
:5005
0
:4995
0
:5000
0
:5000
0
:5000
0
:4995
0
:5005
0
:2495
0
:2500
0
:2505
3
77775
P 21 D
2
66664
0
:2502
0
:2498
0
:5005
0
:5000
0
:4995
0
:5000
0
:5000
0
:4995
0
:5000
0
:5005
0
:2498
0
:2502
0
3
77775
Even though P n does not converge to a unique matrix, P does have a steady-state vector.

10.2
The Steady-State Vector and Google’s PageRank
C-17
In fact,
2
66664
1=8
1=4
1=4
1=4
1=8
3
77775
is a steady-state vector for P (see Exercise 36). This vector can be interpreted as giving
long-run probabilities and occupation times in a sense that will be made precise in
Section 10.4.
EXAMPLE 5
Consider a Markov chain on f1; 2; 3; 4; 5g with transition matrix
P D
1
2
3
4
5
2
66664
1=4
1=4
1=2
0
0
1=3
1=3
1=3
0
0
1=2
1=4
1=4
0
0
0
0
0
1=3
2=3
0
0
0
3=4
1=4
3
77775
1
2
3
4
5
If this Markov chain begins at state 1, 2, or 3, then it must always be at one of those
states. Likewise if the chain starts at state 4 or 5, then it must always be at one of those
states. The chain splits into two separate chains, each with its own steady-state vector.
In this case P n converges to a matrix whose columns are not equal. The vectors
2
66664
4=11
3=11
4=11
0
0
3
77775
and
2
66664
0
0
0
9=17
8=17
3
77775
both satisfy the deﬁnition of steady-state vector (Exercise 37). The ﬁrst vector gives the
limiting probabilities if the chain starts at state 1, 2, or 3, and the second does the same
for states 4 and 5.
Regular Matrices
Examples 1 and 2 show that in some cases a Markov chain xn with transition matrix P
has a steady-state vector q for which
lim
n!1 P n D
 q
q
  
q 
In these cases, q can be interpreted as a vector of long-run probabilities or occupation
times for the chain. These probabilities or occupation times do not depend on the initial
probability vector; that is, for any probability vector x0,
lim
n!1 P nx0 D lim
n!1 xn D q
Notice also that q is the only probability vector that is also an eigenvector of P associated
with the eigenvalue 1.
Examples 3, 4, and 5 do not have such a steady-state vector q. In Examples 3 and
5, the steady-state vector is not unique; in all three examples the matrix P n does not
converge to a matrix with equal columns as n increases. The goal is then to ﬁnd some
property of the transition matrix P that leads to these diﬀerent behaviors, and to show
that this property causes the diﬀerences in behavior.

C-18
CHAPTER 10
Finite-State Markov Chains
A little calculation shows that in Examples 3, 4, and 5, every matrix of the form P k
has some zero entries. In Examples 1 and 2, however, some power of P has all positive
entries. As was mentioned in Section 5.9, this is exactly the property that is needed.
DEFINITION
A stochastic matrix P is regular if some power P k contains only strictly positive
entries.
Since the matrix P k contains the probabilities of a k-step move from one state to
another, a Markov chain with a regular transition matrix has the property that, for some
k, it is possible to move from any state to any other in exactly k steps. The following
theorem expands on the content of Theorem 11 in Section 5.9. One idea must be deﬁned
before the theorem is presented. The limit of a sequence of m  n matrices is the m  n
matrix (if one exists) whose .i; j /-entry is the limit of the .i; j /-entries in the sequence
of matrices. With that understanding, here is the theorem.
THEOREM 1
If P is a regular m  m transition matrix with m  2, then the following state-
ments are all true.
a. There is a stochastic matrix … such that lim
n!1 P n D ….
b. Each column of … is the same probability vector q.
c. For any initial probability vector x0, lim
n!1 P nx0 D q.
d. The vector q is the unique probability vector that is an eigenvector of P
associated with the eigenvalue 1.
e. All eigenvalues  of P other than 1 have jj < 1.
A proof of Theorem 1 is given in Appendix 1. Theorem 1 is a special case of the
Perron-Frobenius Theorem, which is used in applications of linear algebra to economics,
graph theory, and systems analysis. Theorem 1 shows that a Markov chain with a regular
transition matrix has the properties found in Examples 1 and 2. For example, since the
transition matrix P in Example 1 is regular, Theorem 1 justiﬁes the conclusion that P n
converges to a stochastic matrix all of whose columns equal q D
2
4
:3
:6
:1
3
5, as numerical
evidence seemed to indicate.
PageRank and the Google Matrix
In Section 10.1, the notion of a simple random walk on a graph was deﬁned. The World
Wide Web can be modeled as a directed graph, with the vertices representing the web-
pages and the arrows representing the links between webpages. Let P be the huge
transition matrix for this Markov chain. If the matrix P were regular, then Theorem
1 would show that there is a steady-state vector q for the chain, and that the entries in q
can be interpreted as occupation times for each state. In terms of the model, the entries
in q would tell what fraction of the random surfer’s time was spent at each webpage. The
founders of Google, Sergey Brin and Lawrence Page, reasoned that “important” pages
had links coming from other “important” pages. Thus the random surfer would spend
more time at more important pages and less time at less important pages. But the amount
of time spent at each page is just the occupation time for each state in the Markov chain.

10.2
The Steady-State Vector and Google’s PageRank
C-19
This observation is the basis for PageRank, which is the model that Google uses to
rank the importance of all webpages it catalogs:
The importance of a webpage may be measured by the relative size of the
corresponding entry in the steady-state vector q for an appropriately chosen
Markov chain.
Unfortunately, a simple random walk on the directed graph model for the Web is
not the appropriate Markov chain, because the matrix P is not regular. Thus Theorem
1 will not apply. For example, consider the seven-page Web modeled in Section 10.1
using the directed graph in Figure 1. The transition matrix is
1
3
4
2
5
6
7
FIGURE 1
A seven-page Web.
P D
1
2
3
4
5
6
7
2
666666666664
0
0
1
0
0
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
0
0
0
0
1
0
0
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
0
0
0
0
0
0
1
3
777777777775
1
2
3
4
5
6
7
Pages 4 and 7 are dangling nodes, and so are absorbing states for the chain. Just as
in Example 3, the presence of absorbing states implies that the state vectors xn do not
approach a unique limit as n ! 1. To handle dangling nodes, an adjustment is made
to P :
ADJUSTMENT 1: If the surfer reaches a dangling node, the surfer will pick any page
in the Web with equal probability and will move to that page. In terms of the transition
matrix P , if state j is an absorbing state, replace column j of P with the vector
2
6664
1=n
1=n
:::
1=n
3
7775
where n is the number of rows (and columns) in P .
In the seven-page example, the transition matrix is now
P D
1
2
3
4
5
6
7
2
666666666664
0
0
1
0
0
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
0
1=7
1=7
1=7
1=7
1=7
1=7
1=7
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
1=7
1=7
1=7
1=7
1=7
1=7
1=7
3
777777777775
1
2
3
4
5
6
7
Yet even this adjustment is not suﬃcient to ensure that the transition matrix is
regular: while dangling nodes are no longer possible, it is still possible to have “cycles”

C-20
CHAPTER 10
Finite-State Markov Chains
of pages. If page j linked only to page i and page i linked only to page j, a random
surfer entering either page would be condemned to spend eternity linking from page i
to page j and back again. Thus the columns of P k
 corresponding to these pages would
always have zeros in them, and the transition matrix P would not be regular. Another
adjustment is needed.
ADJUSTMENT 2: Let p be a number between 0 and 1. Assume the surfer is now at
page j. With probability p the surfer will pick from among all possible links from page
j with equal probability and will move to that page. With probability 1   p, the surfer
will pick any page in the Web with equal probability and will move to that page. In terms
of the transition matrix P, the new transition matrix will be
G D pP C .1   p/K
where K is an n  n matrix all of whose columns are1
2
6664
1=n
1=n
:::
1=n
3
7775
The matrix G is called the Google matrix, and G is now a regular matrix since all entries
in G1 D G are positive. Although any value of p between 0 and 1 is allowed, Google is
said to use a value of p D :85 for their PageRank calculations. In the seven-page Web
example, the Google matrix is thus
G D :85
2
666666664
0
1=2
0
1=7
0
0
1=7
0
0
1=3
1=7
1=2
0
1=7
1
0
0
1=7
0
1=3
1=7
0
0
1=3
1=7
0
0
1=7
0
1=2
0
1=7
0
1=3
1=7
0
0
1=3
1=7
1=2
0
1=7
0
0
0
1=7
0
1=3
1=7
3
777777775
C :15
2
666666664
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
1=7
3
777777775
D
2
666666664
:021429
:446429
:021429
:142857
:021429
:021429
:142857
:021429
:021429
:304762
:142857
:446429
:021429
:142857
:871429
:021429
:021429
:142857
:021429
:304762
:142857
:021429
:021429
:304762
:142857
:021429
:021429
:142857
:021429
:446429
:021429
:142857
:021429
:304762
:142857
:021429
:021429
:304762
:142857
:446429
:021429
:142857
:021429
:021429
:021429
:142857
:021429
:304762
:142857
3
777777775
1 PageRank really uses a K that has all its columns equal to a probability vector v, which could be linked to
an individual searcher or group of searchers. This modiﬁcation also makes it easier to police the Web for
websites attempting to generate Web traﬃc. See Google’s PageRank and Beyond: The Science of Search
Engine Rankings by Amy N. Langville and Carl D. Meyer (Princeton: Princeton University Press, 2006) for
more information.

10.2
The Steady-State Vector and Google’s PageRank
C-21
It is now possible to ﬁnd the steady-state vector by the methods of this section:
q D
2
666666664
:116293
:168567
:191263
:098844
:164054
:168567
:092413
3
777777775
so the most important page according to PageRank is page 3, which has the largest entry
in q. The complete ranking is 3, 2 and 6, 5, 1, 4, and 7.
..
.. Numerical Note
.
The computation of q is not a trivial task, since the Google matrix has more than 8
billion rows and columns. Google uses a version of the power method introduced
in Section 5.8 to compute q. While the power method was used in that section to
estimate the eigenvalues of a matrix, it can also be used to provide estimates for
eigenvectors. Since q is an eigenvector of G corresponding to the eigenvalue 1,
the power method applies. It turns out that only between 50 and 100 iterations of
the method are needed to get the vector q to the accuracy that Google needs for
its rankings. It still takes days for Google to compute a new q, which it does every
month.
..
Practice Problem
.
1. Consider the Markov chain on f1; 2; 3g with transition matrix
P D
2
4
1=2
0
1=2
1=2
1=2
0
0
1=2
1=2
3
5
a. Show that P is a regular matrix.
b. Find the steady-state vector for this Markov chain.
c. What fraction of the time does this chain spend in state 2? Explain your answer.
10.2 Exercises
In Exercises 1 and 2, consider a Markov chain on f1; 2g with the
given transition matrix P . In each exercise, use two methods to
ﬁnd the probability that, in the long run, the chain is in state 1.
First, raise P to a high power. Then directly compute the steady-
state vector.
1. P D
 :2
:4
:8
:6

2. P D
 1=4
2=3
3=4
1=3

In Exercises 3 and 4, consider a Markov chain on f1; 2; 3g with
the given transition matrix P . In each exercise, use two methods
to ﬁnd the probability that, in the long run, the chain is in state 1.
First, raise P to a high power. Then directly compute the steady-
state vector.
3. P D
2
4
1=3
1=4
0
1=3
1=2
1
1=3
1=4
0
3
5
4. P D
2
4
:1
:2
:3
:2
:3
:4
:7
:5
:3
3
5
In Exercises 5 and 6, ﬁnd the matrix to which P n converges as n
increases.
5. P D
 1=4
2=3
3=4
1=3

6. P D
2
4
1=4
3=5
0
1=4
0
1=3
1=2
2=5
2=3
3
5

C-22
CHAPTER 10
Finite-State Markov Chains
In Exercises 7 and 8, determine whether the given matrix is
regular. Explain your answer.
7. P D
2
4
1=3
0
1=2
1=3
1=2
1=2
1=3
1=2
0
3
5
8. P D
2
664
1=2
0
1=3
0
0
2=5
0
3=7
1=2
0
2=3
0
0
3=5
0
4=7
3
775
9. Consider a pair of Ehrenfest urns with a total of 4 molecules
divided between them.
a. Find the transition matrix for the Markov chain that mod-
els the number of molecules in urn A, and show that this
matrix is not regular.
b. Assuming that the steady-state vector may be interpreted
as occupation times for this Markov chain, in what state
will this chain spend the most steps?
10. Consider a pair of Ehrenfest urns with a total of 5 molecules
divided between them.
a. Find the transition matrix for the Markov chain that mod-
els the number of molecules in urn A, and show that this
matrix is not regular.
b. Assuming that the steady-state vector may be interpreted
as occupation times for this Markov chain, in what state
will this chain spend the most steps?
11. Consider an unbiased random walk with reﬂecting bound-
aries on f1; 2; 3; 4g.
a. Find the transition matrix for the Markov chain and show
that this matrix is not regular.
b. Assuming that the steady-state vector may be interpreted
as occupation times for this Markov chain, in what state
will this chain spend the most steps?
12. Consider a biased random walk with reﬂecting boundaries on
f1; 2; 3; 4g with probability p D :2 of moving to the left.
a. Find the transition matrix for the Markov chain and show
that this matrix is not regular.
b. Assuming that the steady-state vector may be interpreted
as occupation times for this Markov chain, in what state
will this chain spend the most steps?
In Exercises 13 and 14, consider a simple random walk on the
given graph. In the long run, what fraction of the time will the
walk be at each of the various states?
13. 1
2
4
3
5
14.
1
2
3
4
In Exercises 15 and 16, consider a simple random walk on the
given directed graph. In the long run, what fraction of the time
will the walk be at each of the various states?
15.
1
2
3
4
16.
1
4
2
5
3
17. Consider the mouse in the following maze from Section 10.1,
Exercise 17.
3
4
5
1
2
The mouse must move into a diﬀerent room at each time step
and is equally likely to leave the room through any of the
available doorways. If you go away from the maze for a while,
what is the probability that the mouse will be in room 3 when
you return?
18. Consider the mouse in the following maze from Section 10.1,
Exercise 18.
1
2
3
5
4
What fraction of the time does it spend in room 3?
19. Consider the mouse in the following maze, which includes
“one-way” doors, from Section 10.1, Exercise 19.
1
3
2
4
6
5
Show that
q D
2
6666664
0
0
0
0
0
1
3
7777775
is a steady-state vector for the associated Markov chain, and
interpret this result in terms of the mouse’s travels through
the maze.

10.2
The Steady-State Vector and Google’s PageRank
C-23
20. Consider the mouse in the following maze, which includes
“one-way” doors.
3
4
5
1
2
What fraction of the time does the mouse spend in each of the
rooms in the maze?
In Exercises 21–26, mark each statement True or False. Justify
each answer.
21. (T/F) Every stochastic matrix has a steady-state vector.
22. (T/F) Every stochastic matrix is regular.
23. (T/F) If its transition matrix is regular, then the steady-state
vector gives information on long-run probabilities of the
Markov chain.
24. (T/F) If P is a regular stochastic matrix, then P n approaches
a matrix with equal columns as n increases.
25. (T/F) If  D 1 is an eigenvalue of a matrix P , then P is
regular.
26. (T/F) If lim
n!1 xn D q, then the entries in q may be interpreted
as occupation times.
27. Suppose that the weather in Charlotte is modeled using the
Markov chain in Section 10.1, Exercise 27. Over the course of
a year, about how many days in Charlotte are sunny, cloudy,
and rainy according to the model?
28. Suppose that the weather in Charlotte is modeled using the
Markov chain in Section 10.1, Exercise 28. Over the course of
a year, about how many days in Charlotte are rainy according
to the model?
In Exercises 29 and 30, consider a set of webpages hyperlinked by
the given directed graph. Find the Google matrix for each graph
and compute the PageRank of each page in the set.
29.
1
2
5
3
4
30.
1
4
2
5
6
3
31. A genetic trait is often governed by a pair of genes, one inher-
ited from each parent. The genes may be of two types, often
labeled A and a. An individual then may have three diﬀerent
pairs: AA, Aa (which is the same as aA), or aa. In many cases
the AA and Aa individuals cannot be otherwise distinguished;
in these cases gene A is dominant and gene a is recessive.
Likewise, an AA individual is called dominant and an aa
individual is called recessive. An Aa individual is called a
hybrid.
a. Show that if a dominant individual is mated with a hybrid,
the probability of an oﬀspring being dominant is 1=2 and
the probability of an oﬀspring being a hybrid is 1=2.
b. Show that if a recessive individual is mated with a hybrid,
the probability of an oﬀspring being recessive is 1=2 and
the probability of an oﬀspring being a hybrid is 1=2.
c. Show that if a hybrid individual is mated with another
hybrid, the probability of an oﬀspring being dominant is
1=4, the probability of an oﬀspring being recessive is 1=4,
and the probability of an oﬀspring being a hybrid is 1=2.
32. Consider beginning with an individual of known type and
mating it with a hybrid, then mating an oﬀspring of this
mating with a hybrid, and so on. At each step, an oﬀspring is
mated with a hybrid. The type of the oﬀspring can be modeled
by a Markov chain with states AA, Aa, and aa.
a. Find the transition matrix for this Markov chain.
b. If the mating process in Exercise 31 is continued for an
extended period of time, what percent of the oﬀspring will
be of each type?
33. Consider the variation of the Ehrenfest urn model of diﬀusion
studied in Section 10.1, Exercise 33, where one of the 2k
molecules is chosen at random and is then moved between
the urns with a ﬁxed probability p.
a. Let k D 3 and suppose that p D 1=2. Show that the tran-
sition matrix for the Markov chain that models the number
of molecules in urn A is regular.
b. Let k D 3 and suppose that p D 1=2. In what state will
this chain spend the most steps, and what fraction of the
steps will the chain spend at this state?
c. Does the answer to part (b) change if a diﬀerent value of
p with 0 < p < 1 is used?
34. Consider the Bernoulli-Laplace diﬀusion model studied in
Section 10.1, Exercise 34.
a. Let k D 5 and show that the transition matrix for the
Markov chain that models the number of type I molecules
in urn A is regular.
b. Let k D 5. In what state will this chain spend the most
steps, and what fraction of the steps will the chain spend
at this state?
35. Let 0  q  1. Show that
2
66664
q
0
0
0
1   q
3
77775
is a steady-state vector
for the Markov chain in Example 3.

C-24
CHAPTER 10
Finite-State Markov Chains
36. Consider the Markov chain in Example 4.
a. Show that
2
66664
1=8
1=4
1=4
1=4
1=8
3
77775
is a steady-state vector for this Markov
chain.
b. Compute the average of the entries in P 20 and P 21 given
in Example 4. What do you ﬁnd?
37. Show that
2
66664
4=11
3=11
4=11
0
0
3
77775
and
2
66664
0
0
0
9=17
8=17
3
77775
are steady-state vectors
for the Markov chain in Example 5. If the chain is equally
likely to begin in each of the states, what is the probability of
being in state 1 after many steps?
38. Let 0  p, q  1, and deﬁne
P D

p
1   q
1   p
q

a. Show that 1 and p C q   1 are eigenvalues of P .
b. By Theorem 1, for what values of p and q will P fail to
be regular?
c. Find a steady-state vector for P .
39. Let 0  p, q  1, and deﬁne
P D
2
4
p
q
1   p   q
q
1   p   q
p
1   p   q
p
q
3
5
a. For what values of p and q is P a regular stochastic
matrix?
b. Given that P is regular, ﬁnd a steady-state vector for P .
40. Let A be an m  m stochastic matrix, let x be in Rm, and let
y D Ax. Show that
jy1j C    C jymj  jx1j C    C jxmj
with equality holding if and only if all of the nonzero entries
in x have the same sign.
41. Show that every stochastic matrix has a steady-state vector
using the following steps.
a. Let P be a stochastic matrix. By Theorem 10 in Section
5.9,  D 1 is an eigenvalue for P . Let v be an eigenvector
of P associated with  D 1. Use Exercise 40 to conclude
that the nonzero entries in v must have the same sign.
b. Show how to produce a steady-state vector for P from v.
42. Consider a simple random walk on a ﬁnite connected graph.
(A graph is connected if it is possible to move from any vertex
of the graph to any other along the edges of the graph.)
a. Explain why this Markov chain must have a regular tran-
sition matrix.
b. Use the results of Exercises 13 and 14 to hypothesize
a formula for the steady-state vector for such a Markov
chain.
43. By Theorem 1(e), all eigenvalues  of a regular matrix other
than 1 have the property that jj < 1; that is, the eigen-
value 1 is a strictly dominant eigenvalue. Suppose that P
is an n  n regular matrix with eigenvalues 1 D 1, : : : ,
n ordered so that j1j > j2j  j3j  : : :  jnj. Suppose
that x0 D c1q C c2v2 C    C cnvn is a linear combination of
eigenvectors of P .
a. Use Equation (2) in Section 5.8 to derive an expression
for xk D P kx0.
b. Use the result of part (a) to derive an expression for
xk   c1q, and explain how the value of j2j aﬀects the
speed with which fxkg converges to c1q.
..
Solution to Practice Problem
.
1. a. Since
P 2 D
2
4
1=4
1=4
1=2
1=2
1=4
1=4
1=4
1=2
1=4
3
5
P is regular by the deﬁnition with k D 2.
b. Solve the equation P q D q, which may be rewritten as .P   I/q D 0. Since
P   I D
2
4
 1=2
0
1=2
1=2  1=2
0
0
1=2  1=2
3
5
and row reducing the augmented matrix gives
2
4
 1=2
0
1=2
0
1=2  1=2
0
0
0
1=2  1=2
0
3
5 
2
4
1
0  1
0
0
1  1
0
0
0
0
0
3
5

10.3
Communication Classes
C-25
..
the general solution is q3
2
4
1
1
1
3
5. Since q must be a probability vector, set q3 D
1=.1 C 1 C 1/ D 1=3 and compute that
q D 1
3
2
4
1
1
1
3
5 D
2
4
1=3
1=3
1=3
3
5
c. The chain will spend 1=3 of its time in state 2 since the entry in q corresponding
to state 2 is 1=3, and we can interpret the entries as occupation times.
..10.3 Communication Classes
Section 10.2 showed that if the transition matrix for a Markov chain is regular, then xn
converges to a unique steady-state vector for any choice of initial probability vector.
That is, lim
n!1 xn D q, where q is the unique steady-state vector for the Markov chain.
Examples 3, 4, and 5 in Section 10.2 illustrated that, even though every Markov chain
has a steady-state vector, not every Markov chain has the property that lim
n!1 xn D q.
The goal of the next two sections is to study these examples further, and to show that
Examples 3, 4, and 5 in Section 10.2 describe all the ways in which Markov chains fail
to converge to a steady-state vector. The ﬁrst step is to study which states of the Markov
chain can be reached from other states of the chain.
Communicating States
Suppose that state j and state i are two states of a Markov chain. If state j can be reached
from state i in a ﬁnite number of steps and state i can be reached from state j in a ﬁnite
number of steps, then states j and i are said to communicate. If P is the transition
matrix for the chain, then the entries in P k give the probabilities of going from one state
to another in k steps:
P k D
From:
1
j
m
2
6664
:::
#
pij
  
!
3
7775
To:
1
i
m
and powers of P can be used to make the following deﬁnition.
DEFINITION
Let i and j be two states of a Markov chain with transition matrix P . Then state
i communicates with state j if there exist nonnegative integers m and n such that
the .j; i/-entry of P m and the .i; j /-entry of P n are both strictly positive. That is,
state i communicates with state j if it is possible to go from state i to state j in m
steps and from state j to state i in n steps.

C-26
CHAPTER 10
Finite-State Markov Chains
This deﬁnition implies three properties that will allow the states of a Markov chain
to be placed into groups called communication classes. First, the deﬁnition allows the
integers m and n to be zero, in which case the .i; i/-entry of P 0 D I is 1, which is
positive. This ensures that every state communicates with itself. Because both .i; j / and
.j; i/ are included in the deﬁnition, it follows that if state i communicates with state
j then state j communicates with state i. Finally, you will show in Exercise 40 that if
state i communicates with state j and state j communicates with state k, then state i
communicates with state k. These three properties are called, respectively, the reﬂexive,
symmetric, and transitive properties:
a. (Reﬂexive Property) Each state communicates with itself.
b. (Symmetric Property) If state i communicates with state j , then state j communi-
cates with state i.
c. (Transitive Property) If state i communicates with state j , and state j communicates
with state k, then state i communicates with state k.
A relation with these three properties is called an equivalence relation. The communi-
cation relation is an equivalence relation on the state space for the Markov chain. Using
the properties listed above simpliﬁes determining which states communicate.
EXAMPLE 1
Consider an unbiased random walk with absorbing boundaries on
f1; 2; 3; 4; 5g. Find which states communicate.
SOLUTION
The transition matrix P is given below, and Figure 1 shows the transition
diagram for this Markov chain.
P D
1
2
3
4
5
2
6666664
1
0
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
0
1
3
7777775
1
2
3
4
5
1
1
2
3
1
4
5
1
2
1
2
1
2
1
2
1
2
1
2
FIGURE 1 Unbiased random walk with absorbing
boundaries.
First note that, by the reﬂexive property, each state communicates with itself. It is clear
from the diagram that states 2, 3, and 4 communicate with each other. The same conclu-
sion may be reached using the deﬁnition by ﬁnding that the .2; 3/-, .3; 2/-, .3; 4/-, and
.4; 3/-entries in P are positive, thus states 2 and 3 communicate, as do states 3 and 4.
States 2 and 4 must also communicate by the transitive property. Now consider state 1
and state 5. If the chain starts in state 1, it cannot move to any state other than itself. Thus
it is not possible to go from state 1 to any other state in any number of steps, and state
1 does not communicate with any other state. Likewise, state 5 does not communicate
with any other state. In summary,

10.3
Communication Classes
C-27
State 1 communicates with state 1.
State 2 communicates with state 2, state 3, and state 4.
State 3 communicates with state 2, state 3, and state 4.
State 4 communicates with state 2, state 3, and state 4.
State 5 communicates with state 5.
Notice that even though states 1 and 5 do not communicate with states 2, 3, and 4, it is
possible to go from these states to either state 1 or state 5 in a ﬁnite number of steps: this
is clear from the diagram, or by conﬁrming that the appropriate entries in P , P 2, or P 3
are positive.
In Example 1, the state space f1; 2; 3; 4; 5g can now be divided into the classes f1g,
f2; 3; 4g, and f5g. The states in each of these classes communicate only with the other
members of their class. This division of the state space occurs because the communica-
tion relation is an equivalence relation. The communication relation partitions the state
space into communication classes. Each state in a Markov chain communicates only
with the members of its communication class. For the Markov chain in Example 1, the
communication classes are f1g, f2; 3; 4g, and f5g.
EXAMPLE 2
Consider an unbiased random walk with reﬂecting boundaries on
f1; 2; 3; 4; 5g. Find the communication classes for this Markov chain.
SOLUTION
The transition matrix P for this chain, as well as P 2, P 3, and P 4, is shown
below.
P D
1
2
3
4
5
2
6666664
0
1
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1
0
3
7777775
1
2
3
4
5
P 2 D
1
2
3
4
5
2
6666664
1=2
0
1=2
0
0
0
3=4
0
1=4
0
1=4
0
1=2
0
1=4
0
1=4
0
3=4
0
0
0
1=2
0
1=2
3
7777775
1
2
3
4
5
P 3 D
1
2
3
4
5
2
6666664
0
3=4
0
1=4
0
3=8
0
1=2
0
1=8
0
1=2
0
1=2
0
1=8
0
1=2
0
3=8
0
1=4
0
3=4
0
3
7777775
1
2
3
4
5
P 4 D
1
2
3
4
5
2
6666664
3=8
0
1=2
0
1=8
0
5=8
0
3=8
0
1=4
0
1=2
0
1=4
0
3=8
0
5=8
0
1=8
0
1=2
0
3=8
3
7777775
1
2
3
4
5
The transition diagram for this Markov chain is given in Figure 2.
1
1
1
2
3
4
5
1
2
1
2
1
2
1
2
1
2
1
2
FIGURE 2 Unbiased random walk with
reﬂecting boundaries.
Notice that the .i; j /-entry in at least one of these matrices is positive for any choice
of i and j . Thus every state is reachable from any other state in four steps or fewer,

C-28
CHAPTER 10
Finite-State Markov Chains
and every state communicates with every state. There is only one communication class:
f1; 2; 3; 4; 5g.
EXAMPLE 3
Consider the Markov chain given in Example 5 in Section 10.2. Find
the communication classes for this Markov chain.
SOLUTION
The transition matrix for this Markov chain is
P D
1
2
3
4
5
2
6666664
1=4
1=4
1=2
0
0
1=3
1=3
1=3
0
0
1=2
1=4
1=4
0
0
0
0
0
1=3
2=3
0
0
0
3=4
1=4
3
7777775
1
2
3
4
5
and a transition diagram is shown in Figure 3.
2
3
4
5
1
1
2
1
2
1
3
1
3
1
3
1
3
1
4
1
4
1
4
3
4
1
4
1
4
2
3
FIGURE 3 Transition diagram for Example 3.
It is impossible to move from any of the states 1, 2, or 3 to either of the states 4 or 5, so
these states must be in separate communication classes. In addition, state 1, state 2, and
state 3 communicate; state 4 and state 5 also communicate. Thus the communication
classes for this Markov chain are f1; 2; 3g and f4; 5g.
The Markov chains in Examples 1 and 3 have more than one communication class,
while the Markov chain in Example 2 has only one communication class. This distinction
leads to the following deﬁnitions.
DEFINITION
A Markov chain with only one communication class is irreducible. A Markov
chain with more than one communication class is reducible.
Thus the Markov chains in Examples 1 and 3 are reducible, while the Markov chain in
Example 2 is irreducible. Irreducible Markov chains and regular transition matrices are
connected by the following theorem.
THEOREM 2
If a Markov chain has a regular transition matrix, then it is irreducible.
PROOF
Suppose that P is a regular transition matrix for a Markov chain. Then, by
deﬁnition, there is a k such that P k is a positive matrix. That is, for any states i and
j , the .i; j/- and .j; i/-elements in P k are strictly positive. Thus there is a positive
probability of moving from i to j and from j to i in exactly k steps, and so i and j

10.3
Communication Classes
C-29
communicate with each other. Since i and j were arbitrary states and must be in the
same communication class, there can be only one communication class for the chain, so
the Markov chain must be irreducible.
Example 2 shows that the converse of Theorem 2 is not true, because the Markov chain
in this example is irreducible, but its transition matrix is not regular.
EXAMPLE 4
Consider the Markov chain whose transition diagram is given in
Figure 4. Determine whether this Markov chain is reducible or irreducible.
.8
.6
5
1
1
.2
1
.6
3
2
.3
.1
4
.4
FIGURE 4 Transition diagram for
Example 4.
SOLUTION
The diagram shows that states 1 and 2 communicate, as do states 4 and 5.
Notice that states 1 and 2 cannot communicate with states 3, 4, or 5 since the probability
of moving from state 2 to state 3 is 0. Likewise states 4 and 5 cannot communicate with
states 1, 2, or 3 since the probability of moving from state 4 to state 3 is 0. Finally, state
3 cannot communicate with any state other than itself since it is impossible to return to
state 3 from any other state. Thus the communication classes for this Markov chain are
f1; 2g, f3g, and f4; 5g. Since there is more than one communication class, this Markov
chain is reducible.
Mean Return Times
Let q be the steady-state vector for an irreducible Markov chain. It can be shown using
advanced methods in probability theory that the entries in q may be interpreted as
occupation times; that is, qi is the fraction of time steps that the chain will spend at state i.
For example, consider a Markov chain on f1; 2; 3g with steady-state vector q D
2
4
:2
:5
:3
3
5.
In the long run, the chain will spend about half of its steps in state 2. If the chain is
currently in state 2, it should take about two (1=:5) steps to return to state 2. Likewise,
since the chain spends about 1=5 of its time in state 1, it should visit state 1 once every
ﬁve steps.
Given a Markov chain and states i and j , a quantity of considerable interest is the
number of steps nij that it will take for the system to ﬁrst visit state i given that it starts in
state j . The value of nij cannot be known—it could be any positive integer depending
on how the Markov chain evolves. Such a quantity is known as a random variable.

C-30
CHAPTER 10
Finite-State Markov Chains
Since nij is unknowable, the expected value of nij is studied instead. The expected
value of a random variable functions as a type of average value of the random variable.
The following deﬁnition will be used in subsequent sections.
DEFINITION
The expected value of a random variable X that takes on the values x1; x2; : : : is
EŒX D x1P.X D x1/ C x2P.X D x2/ C    D
1
X
kD1
xkP.X D xk/
(1)
where P.X D xk/ denotes the probability that the random variable X equals the
value xk.
Now let tii D EŒnii be the expected value of nii, which is the expected number
of steps it will take for the system to return to state i given that it starts in state
i. Unfortunately, Equation (1) will not be helpful at this point. Instead, proceeding
intuitively, the system should spend one step in state i for each tii steps on average.
It seems reasonable to say that the system will, over the long run, spend about 1=tii of
the time in state i. But that quantity is qi, so the expected number of time steps needed to
return, or mean return time to a state i, is the reciprocal of qi. This informal argument
may be made rigorous using methods from probability theory; see Appendix 2 for a
complete proof.
THEOREM 3
Consider an irreducible Markov chain with a ﬁnite state space, let nij be the
number of steps until the chain ﬁrst visits state i given that the chain starts in
state j , and let tii D EŒnii. Then
tii D 1
qi
(2)
where qi is the entry in the steady-state vector q corresponding to state i.
The previous example matches Equation 2: t11 D 1=:2 D 5, t22 D 1=:5 D 2, and t33 D
1=:3 D 10=3. Recall that the mean return time is an expected value, so the fact that
t33 is not an integer ought not be troubling. Section 10.5 will include a discussion of
tij D EŒnij , where i ¤ j .
..
Practice Problem
.
1. Consider the Markov chain on f1; 2; 3; 4g with transition matrix
P D
2
664
1=4
1=3
1=2
0
0
1=3
0
1=3
3=4
0
1=2
1=3
0
1=3
0
1=3
3
775
Determine the communication classes for this chain.

10.3
Communication Classes
C-31
10.3 Exercises
In Exercises 1–6, consider a Markov chain with state space
f1; 2; : : : ; ng and the given transition matrix. Find the communica-
tion classes for each Markov chain, and state whether the Markov
chain is reducible or irreducible.
1.
2
4
1=4
0
1=3
1=2
1
1=3
1=4
0
1=3
3
5
2.
2
4
1=4
1=2
1=3
1=2
1=2
1=3
1=4
0
1=3
3
5
3.
2
4
1
1=2
1=2
0
1=2
0
0
0
1=2
3
5
4.
2
66664
0
0
0
1
1
1=3
0
0
0
0
2=3
0
0
0
0
0
1=4
2=3
0
0
0
3=4
1=3
0
0
3
77775
5.
2
6666664
0
0
:4
0
:8
0
0
0
0
:7
0
:5
:3
0
0
0
:2
0
0
:1
0
0
0
:5
:7
0
:6
0
0
0
0
:9
0
:3
0
0
3
7777775
6.
2
666666664
0
1=3
0
2=3
1=2
0
0
1=2
0
1=2
0
0
1=3
0
0
2=3
0
1=3
0
0
2=5
1=2
0
1=2
0
0
0
0
0
0
0
0
0
0
3=5
0
0
0
0
1=2
0
0
0
0
0
0
0
2=3
0
3
777777775
7. Consider the mouse in the following maze from Section 10.1,
Exercise 19.
1
3
2
4
6
5
Find the communication classes for the Markov chain that
models the mouse’s travels through this maze. Is this Markov
chain reducible or irreducible?
8. Consider the mouse in the following maze from Section 10.1,
Exercise 20.
3
4
5
1
2
Find the communication classes for the Markov chain that
models the mouse’s travels through this maze. Is this Markov
chain reducible or irreducible?
In Exercises 9 and 10, consider the set of webpages hyperlinked
by the given directed graph. Find the communication classes for
the Markov chain that models a random surfer’s progress through
this set of webpages. Use the transition matrix derived from the
graph itself instead of the Google matrix.
9.
1
2
5
3
4
10.
1
4
2
5
6
3
11. Consider an unbiased random walk with reﬂecting bound-
aries on f1; 2; 3; 4g. Find the communication classes for this
Markov chain and determine whether it is reducible or irre-
ducible.
12. Consider an unbiased random walk with absorbing bound-
aries on f1; 2; 3; 4g. Find the communication classes for this
Markov chain and determine whether it is reducible or irre-
ducible.
In Exercises 13 and 14, consider a simple random walk on the
given graph. Show that the Markov chain is irreducible and calcu-
late the mean return times for each state.
13. 1
2
4
3
5
14.
1
2
3
4
In Exercises 15 and 16, consider a simple random walk on the
given directed graph. Show that the Markov chain is irreducible
and calculate the mean return times for each state.
15.
1
2
3
4
16.
1
4
2
5
3

C-32
CHAPTER 10
Finite-State Markov Chains
17. Consider the mouse in the following maze from Section 10.1,
Exercise 17.
3
4
5
1
2
If the mouse starts in room 3, how long on average will it take
the mouse to return to room 3?
18. Consider the mouse in the following maze from Section 10.1,
Exercise 18.
1
2
3
5
4
If the mouse starts in room 2, how long on average will it take
the mouse to return to room 2?
In Exercises 19 and 20, consider the mouse in the following maze
from Section 10.2, Exercise 20.
3
4
5
1
2
19. If the mouse starts in room 1, how long on average will it take
the mouse to return to room 1?
20. If the mouse starts in room 4, how long on average will it take
the mouse to return to room 4?
In Exercises 21–26, mark each statement True or False. Justify
each answer.
21. (T/F) If it is possible to go from state i to state j in n steps,
where n  0, then states i and j communicate with each
other.
22. (T/F) An irreducible Markov chain must have a regular tran-
sition matrix.
23. (T/F) If a Markov chain is reducible, then it cannot have a
regular transition matrix.
24. (T/F) If the .i; j /- and .j; i/-entries in P k are positive for
some k, then the states i and j communicate with each other.
25. (T/F) The entries in the steady-state vector are the mean
return times for each state.
26. (T/F) If state i communicates with state j and state j
communicates with state k, then state i communicates with
state k.
27. Suppose that the weather in Charlotte is modeled using the
Markov chain in Section 10.1, Exercise 27. About how many
days elapse in Charlotte between rainy days?
28. Suppose that the weather in Charlotte is modeled using the
Markov chain in Section 10.1, Exercise 28. About how many
days elapse in Charlotte between consecutive rainy days?
29. The following set of webpages hyperlinked by the directed
graph was studied in Section 10.2, Exercise 29.
1
2
5
3
4
Consider randomly surﬁng on this set of webpages using the
Google matrix as the transition matrix.
a. Show that this Markov chain is irreducible.
b. Suppose the surfer starts at page 1. How many mouse
clicks on average must the surfer make to get back to
page 1?
30. The following set of webpages hyperlinked by the directed
graph was studied in Section 10.2, Exercise 30.
1
4
2
5
6
3
Repeat Exercise 29 for this set of webpages.
31. Consider the pair of Ehrenfest urns studied in Section 10.2,
Exercise 9. Suppose that there are now 2 molecules in urn
A. How many steps on average will be needed until there are
again 2 molecules in urn A?
32. Consider the pair of Ehrenfest urns studied in Section 10.2,
Exercise 10. Suppose that urn A is now empty. How many
steps on average will be needed until urn A is again empty?
33. A variation of the Ehrenfest model of diﬀusion was studied
in Section 10.2, Exercise 33. Consider this model with k D 3
and p D 1=2 and suppose that there are now 3 molecules in
urn A. How many draws on average will be needed until there
are again 3 molecules in urn A?
34. Consider the Bernoulli-Laplace model of diﬀusion studied
in Section 10.2, Exercise 34. Let k D 5. Suppose that all of
the type I molecules are now in urn A. How many draws on
average will be needed until all of the type I molecules are
again in urn A?
35. A Markov chain model for scoring a tennis game was studied
in Section 10.1, Exercise 35. What are the communication
classes for this Markov chain?

10.4
Classiﬁcation of States and Periodicity
C-33
36. A Markov chain model for the rally point method for scoring
a volleyball game was studied in Section 10.1, Exercise 36.
What are the communication classes for this Markov chain?
In Exercises 37 and 38, consider the Markov chain on
f1; 2; 3; 4; 5g with transition matrix
P D
2
66664
0
0
0
1=2
1
1=3
0
0
0
0
2=3
0
0
0
0
0
2=5
1=5
1=2
0
0
3=5
4=5
0
0
3
77775
37. Show that this Markov chain is irreducible.
38. Suppose the chain starts in state 1. What is the expected
number of steps until it is in state 1 again?
39. How does the presence of dangling nodes in a set of hy-
perlinked webpages aﬀect the communication classes of the
associated Markov chain?
40. Show that the communication relation is transitive. Hint:
Show that the .i; k/-entry of P nCm must be greater than or
equal to the product of the .i; j /-entry of P m and the .j; k/-
entry of P n.
..
Solution to Practice Problem
.
1. First note that states 1 and 3 communicate with each other, as do states 2 and 4.
However, there is no way to proceed from either state 1 or state 3 to either state 2
or state 4, so the communication classes are f1; 3g and f2; 4g.
.10.4 Classiﬁcation of States and Periodicity
The communication classes of a Markov chain have important properties that help
determine whether the state vectors converge to a unique steady-state vector. These
properties are studied in this section, and it will be shown that Examples 3, 4, and 5
in Section 10.2 are examples of all the ways that the state vectors of a Markov chain can
fail to converge to a unique steady-state vector.
Recurrent and Transient States
One way to describe the communication classes is to determine whether it is possible
for the Markov chain to leave the class once it has entered it.
DEFINITION
Let C be a communication class of states for a Markov chain, and let j be a state in
C. If there is a state i not in C and k > 0 such that the .i; j /-entry in P k is positive,
then the class C is called a transient class and each state in C is a transient state.
If a communication class is not transient, it is called a recurrent class and each
state in the class is a recurrent state.
Suppose that C is a transient class. Notice that once the system moves from C to
another communication class D, the system can never return to C. This is true because
D cannot contain a state i from which it is possible to move to a state in C. If D did
contain such a state i, then the transitive property of the communication relation would
imply that every state in C communicates with every state in D. This is impossible.
EXAMPLE 1
Consider the Markov chain on f1; 2; 3; 4; 5g studied in Example 4 in
Section 10.3. Its transition diagram is given in Figure 1. Determine whether each of the
communication classes is transient or recurrent.
SOLUTION
The communication classes were found to be f1; 2g, f3g, and f4; 5g. First
consider class f3g. There is a positive probability of a transition from state 3 to state 2,

C-34
CHAPTER 10
Finite-State Markov Chains
.8
.6
5
1
1
.2
1
.6
3
2
.3
.1
4
.4
FIGURE 1 Transition diagram for
Example 1.
so applying the deﬁnition with k D 1 shows that class f3g is a transient class. Now
consider class f1; 2g. The probability of a one-step transition from either state 1 or state
2 to any of states 3, 4, or 5 is zero, and this is also true for any number of steps. If the
system starts in state 1 or 2, it will always stay in state 1 or 2. Class f1; 2g is thus a
recurrent class. A similar argument shows that class f4; 5g is also a recurrent class.
EXAMPLE 2
Consider the random walk with reﬂecting boundaries studied in Exam-
ple 2 in Section 10.3. Determine whether each of the communication classes is transient
or recurrent.
SOLUTION
This Markov chain is irreducible: the single communication class for
this chain is f1; 2; 3; 4; 5g. By the deﬁnition, this class cannot be transient. Thus the
communication class must be recurrent.
The result of the preceding example may be generalized to any irreducible Markov
chain.
Remark:
All states of an irreducible Markov chain are recurrent.
Suppose that a reducible Markov chain has two transient classes C1 and C2 and no
recurrent classes. Since C1 is transient, there must be a state in C2 that can be reached
from a state in C1. Since C2 is transient, there must be a state in C1 that can be reached
from C2. Thus all states in C1 and C2 communicate, which is impossible. Thus the
Markov chain must have at least one recurrent class. This argument can be generalized
to refer to any reducible Markov chain with any number of transient classes, which along
with the previous remark proves the following.
Remark:
Every Markov chain must have at least one recurrent class.
EXAMPLE 3
Consider the Markov chain studied in Example 3 in Section 10.3.
Determine whether each of the communication classes is transient or recurrent.
SOLUTION
The transition matrix for this Markov chain is
P D
1
2
3
4
5
2
6666664
1=4
1=4
1=2
0
0
1=3
1=3
1=3
0
0
1=2
1=4
1=4
0
0
0
0
0
1=3
2=3
0
0
0
3=4
1=4
3
7777775
1
2
3
4
5

10.4
Classiﬁcation of States and Periodicity
C-35
and the two communication classes are f1; 2; 3g and f4; 5g. The matrix P may be written
as the partitioned matrix P D
 P1
O
O
P2

, where
P1 D
1
2
3
2
64
1=4
1=4
1=2
1=3
1=3
1=3
1=2
1=4
1=4
3
75
1
2
3
and
P2 D
4
5
"
1=3
2=3
3=4
1=4
#
4
5
and O is an appropriately sized zero matrix. Using block multiplication,
P k D
"
P k
1
O
O
P k
2
#
for all k > 0. Thus if state j is in one class and state i is in the other class, the .i; j/-
and .j; i/-entries of P k are zero for all k > 0. Thus both classes of this Markov chain
must be recurrent.
EXAMPLE 4
Consider altering the previous example slightly to get a Markov chain
with transition matrix
P D
1
2
3
4
5
2
6666664
1=4
1=4
1=2
0
0
1=3
1=3
1=3
0
0
1=2
1=4
1=4
0
0
0
0
0
1=3
2=3
0
0
1=4
1=2
1=4
3
7777775
1
2
3
4
5
and the transition diagram given in Figure 2. Determine whether each of the communi-
cation classes is transient or recurrent.
2
3
4
5
1
1
3
1
3
1
4
1
4
1
4
1
4
1
4
1
4
1
3
1
3
1
2
1
2
1
2
2
3
FIGURE 2 Transition diagram for Example 4.
SOLUTION
The communication classes are still f1; 2; 3g and f4; 5g. Now the .3; 5/-
entry is not zero, so f4; 5g is a transient class. The chain must have at least one recurrent
class, so f1; 2; 3g must be that recurrent class. This result may also be proven using
partitioned matrices. Let P D
P1
S
O
Q

, where P1 is as in the previous example,
Q D
4
5
"
1=3
2=3
1=2
1=4
#
4
5
and
S D
4
5
2
64
0
0
0
0
0
1=4
3
75
1
2
3

C-36
CHAPTER 10
Finite-State Markov Chains
The submatrix P1 is a transition matrix in its own right: it describes transitions
within the recurrent class f1; 2; 3g. Matrix S records the probabilities of transitions
from the transient class f4; 5g into the recurrent class f1; 2; 3g. Matrix Q records the
probabilities of transitions within the transient class f4; 5g. Block multiplication now
gives
P k D
"
P k
1
Sk
O
Qk
#
for some nonzero matrix Sk. Since the lower left block is O for all matrices P k, it
is impossible to leave class f1; 2; 3g after entering it, and class f1; 2; 3g is a recurrent
class.
In Examples 3 and 4, the states were ordered so that the members of each class were
grouped together. In Example 4, the recurrent classes were listed ﬁrst followed by the
transient classes. This ordering was convenient, as it allowed for the use of partitioned
matrices to determine the recurrent and transient classes. It is also possible to use block
multiplication to compute powers of the transition matrix P if the states are reordered
in the manner done in Examples 3 and 4: the states in each communication class are
consecutive, and if there are any transient classes, the recurrent classes are listed ﬁrst,
followed by the transient classes. A transition matrix with its states thus reordered is
said to be in canonical form. To see how this reordering works, consider the following
example.
EXAMPLE 5
The Markov chain in Example 1 has transition matrix
P D
1
2
3
4
5
2
66664
:8
:2
0
0
0
1
0
0
0
0
0
:3
:6
:1
0
0
0
0
0
1
0
0
0
:4
:6
3
77775
1
2
3
4
5
and its communication classes are f1; 2g, f3g, and f4; 5g. To place the matrix in canonical
form, reorder the classes f1; 2g, f4; 5g, and f3g; that is, rearrange the states in the order
1, 2, 4, 5, 3. To perform this rearrangement, ﬁrst rearrange the columns, which produces
the matrix
1
2
3
4
5
2
66664
:8
:2
0
0
0
1
0
0
0
0
0
:3
:6
:1
0
0
0
0
0
1
0
0
0
:4
:6
3
77775
1
2
3
4
5
rearrange
     !
columns
1
2
4
5
3
2
66664
:8
:2
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
:4
:6
0
:3
:6
:1
0
3
77775
1
2
3
4
5
Now rearranging the rows produces the transition matrix in canonical form:
1
2
4
5
3
2
66664
:8
:2
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
:4
:6
0
:3
:6
:1
0
3
77775
1
2
3
4
5
rearrange
     !
rows
1
2
4
5
3
2
66664
:8
:2
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
:4
:6
0
0
:3
:1
0
:6
3
77775
1
2
4
5
3

10.4
Classiﬁcation of States and Periodicity
C-37
The transition matrix may be divided as follows:
P D
1
2
4
5
3
2
66664
:8
:2
0
0
0
1
0
0
0
0
0
0
0
1
0
0
0
:4
:6
0
0
:3
:1
0
:6
3
77775
1
2
4
5
3
D
 P1
S
O
Q

In general, suppose that P is the transition matrix for a reducible Markov chain with
r recurrent classes and one or more transient classes. A canonical form of P is
P D
2
6664
P1
  
O
:::
:::
:::
S
O
  
Pr
O
Q
3
7775
Here Pi is the transition matrix for the ith recurrent class, O is an appropriately sized zero
matrix, Q records transitions within the transient classes, and S contains the probabilities
of transitions from the transient classes to the recurrent classes. Since P is a partitioned
matrix, it is relatively easy to take powers of it using block multiplication:
P k D
2
6664
P k
1
  
O
:::
:::
:::
Sk
O
  
P k
r
O
Qk
3
7775
for some matrix Sk. The matrices Q, S, and Sk help to answer questions about the long-
term behavior of the Markov chain that are addressed in Section 10.5.
Periodicity
A ﬁnal way of classifying states is to examine at what times it is possible for the system
to return to the state in which it begins. Consider the following simple example.
EXAMPLE 6
A Markov chain on f1; 2; 3g has transition matrix
P D
1
2
3
2
4
0
1
0
0
0
1
1
0
0
3
5
1
2
3
The transition diagram, which is shown in Figure 3, is quite straightforward. The system
1
2
1
1
3
1
FIGURE 3
Transition diagram for Example 6.
must return to its starting point in three steps and every time the number of steps is a
multiple of three.
EXAMPLE 7
A Markov chain on f1; 2; 3; 4g has transition matrix
P D
1
2
3
4
2
66664
0
1
0
0
0
0
1=2
1=2
1
0
0
0
0
0
1
0
3
77775
1
2
3
4

C-38
CHAPTER 10
Finite-State Markov Chains
and the transition diagram shown in Figure 4. If the system starts in state 1, 2, or 3, the
1
2
1
1
3
4
1
1
2
1
2
FIGURE 4
Transition diagram for Example 7.
system may return to its starting point in three steps or in four steps, and may return
every time the number of steps is 3a C 4b for some non-negative integers a and b. It
can be shown that every positive integer greater than 5 may be written in that form, so if
the system starts in state 1, 2, or 3, it may also return to its starting point at any number
of steps greater than 5. If the system starts in state 4, the system may return to its starting
point in four steps or in seven steps, and a similar argument shows that the system may
also return to its starting point at any number of steps greater than 17.
EXAMPLE 8
The unbiased random walk on f1; 2; 3; 4; 5g with reﬂecting boundaries
has the transition diagram shown in Figure 5. From this diagram, one can see that it will
always take an even number of steps for the system to return to the state in which it
started.
1
1
1
2
3
4
5
1
2
1
2
1
2
1
2
1
2
1
2
FIGURE 5 Unbiased random walk with
reﬂecting boundaries.
In Examples 6 and 8, the time steps at which the system may return to its initial site are
multiples of a number d: d D 3 for Example 6, d D 2 for Example 8. This number d
is called the period of the state, and is deﬁned as follows.
DEFINITION
The period d of a state i of a Markov chain is the greatest common divisor of all
time steps n such that the probability that the Markov chain that started at i visits i
at time step n is strictly positive.
Using a careful analysis of the set of states visited by the Markov chain, it may be
shown that the period of each state in a given communication class is the same, so the
period is a property of communication classes. See Appendix 2 for a proof of this fact,
which leads to the following deﬁnition.
DEFINITION
The period of a communication class C is the period of each state in C. If a
Markov chain is irreducible, then the period of the chain is the period of its single
communication class. If the period of every communication class (and thus every
state) is d D 1, then the Markov chain is aperiodic.
The reason that the greatest common divisor appears in the deﬁnition is to allow a
period to be assigned to all states of all Markov chains. In Example 7, the system may
return to its starting state after any suﬃciently large number of steps, so the period of
each state is d D 1. That is, the Markov chain in Example 7 is aperiodic. Notice that this
chain does not exhibit periodic behavior, so the term aperiodic is quite apt. Using the
deﬁnition conﬁrms that the period of the Markov chain in Example 6 is d D 3, while
the period of the Markov chain in Example 8 is d D 2. The next theorem describes the
transition matrix of an irreducible and aperiodic Markov chain.

10.4
Classiﬁcation of States and Periodicity
C-39
THEOREM 4
Let P be the transition matrix for an irreducible, aperiodic Markov chain. Then P
is a regular matrix.
PROOF
Let P be an n  n transition matrix for an irreducible, aperiodic Markov chain.
To show that P is regular, a number k must be found for which every entry in P k is
strictly positive. Let 1  i; j  n. Since the Markov chain is irreducible, there must
be a number a that depends on i and j such that the .i; j /-element in P a is strictly
positive. Since the Markov chain is aperiodic, there is a number b that depends on j
such that the .j; j /-element in P m is strictly positive for all m  b. Now note that since
P aCm D P aP m, the .i; j /-element in P aCm must be greater than or equal to the product
of the .i; j /-element in P a and the .j; j /-element in P m. Thus the .i; j/-element in
P aCm must be strictly positive for all m  b. Now let k be the maximum over all pairs
.i; j/ of the quantity a C b. This maximum exists because the state space is ﬁnite, and
the .i; j/-element of P k must be strictly positive for all pairs .i; j /. Thus every entry of
P k is strictly positive, and P is a regular matrix.
So, if P is the transition matrix for an irreducible, aperiodic Markov chain, then P
must be regular and Theorem 1 must apply to P . Thus there is a steady-state vector q
for which
lim
n!1 P nx0 D q
for any choice of initial probability vector x0. What can be said about the steady-state
vector q if an irreducible Markov chain has period d > 1? The following result is proven
in more advanced texts in probability theory.
THEOREM 5
Let P be the transition matrix for an irreducible Markov chain with period d > 1,
and let q be the steady-state vector for the Markov chain. Then, for any initial
probability vector x0,
lim
n!1
1
d
 P nC1 C    C P nCd
x0 D q
Theorem 5 says that in the case of an irreducible Markov chain with period d > 1, the
vector q is the limit of the average of the probability vectors P nC1x0, P nC2x0, : : : ,
P nCdx0. When a Markov chain is irreducible with period d > 1, the vector q may still
be interpreted as a vector of occupation times.
EXAMPLE 9
The period of the irreducible Markov chain in Example 8 is d D 2, so
the Markov chain has period d > 1. Let n be an even integer. Taking high powers of the
transition matrix P shows that
P n  !
1
2
3
4
5
2
6666664
1=4
0
1=2
0
1=4
0
1=2
0
1=2
0
1=4
0
1=2
0
1=4
0
1=2
0
1=2
0
1=4
0
1=2
0
1=4
3
7777775
1
2
3
4
5

C-40
CHAPTER 10
Finite-State Markov Chains
and
P nC1  !
1
2
3
4
5
2
6666664
0
1=2
0
1=2
0
1=4
0
1=2
0
1=4
0
1=2
0
1=2
0
1=4
0
1=2
0
1=4
0
1=2
0
1=2
0
3
7777775
1
2
3
4
5
So for any initial probability vector x0,
lim
n!1
1
2
 P n C P nC1
x0 D
2
66664
1=8
1=8
1=8
1=8
1=8
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=8
1=8
1=8
1=8
1=8
3
77775
x0 D
2
66664
1=8
1=4
1=4
1=4
1=8
3
77775
But this vector was the steady-state vector for this Markov chain calculated in Exercise
36 in Section 10.2. Theorem 5 is thus conﬁrmed in this case.
The steady-state vector for a reducible Markov chain will be discussed in detail in
the next section.
.
Practice Problem
.
1. Consider the Markov chain on f1; 2; 3; 4g with transition matrix
P D
2
664
1=4
1=3
1=2
0
0
1=3
0
1=3
3=4
0
1=2
1=3
0
1=3
0
1=3
3
775
Identify the communication classes of the chain as either recurrent or transient, and
reorder the states to produce a matrix in canonical form.
10.4 Exercises
In Exercises 1–6, consider a Markov chain with state space with
f1; 2; : : : ; ng and the given transition matrix. Identify the commu-
nication classes for each Markov chain as recurrent or transient,
and ﬁnd the period of each communication class.
1.
2
4
1=4
0
1=3
1=2
1
1=3
1=4
0
1=3
3
5
2.
2
4
1=4
1=2
1=3
1=2
1=2
1=3
1=4
0
1=3
3
5
3.
2
4
1
1=2
1=2
0
1=2
0
0
0
1=2
3
5
4.
2
66664
0
0
0
1
1
1=3
0
0
0
0
2=3
0
0
0
0
0
1=4
2=3
0
0
0
3=4
1=3
0
0
3
77775
5.
2
6666664
0
0
:4
0
:8
0
0
0
0
:7
0
:5
:3
0
0
0
:2
0
0
:1
0
0
0
:5
:7
0
:6
0
0
0
0
:9
0
:3
0
0
3
7777775
6.
2
666666664
0
1=3
0
2=3
1=2
0
0
1=2
0
1=2
0
0
1=3
0
0
2=3
0
1=3
0
0
2=5
1=2
0
1=2
0
0
0
0
0
0
0
0
0
0
3=5
0
0
0
0
1=2
0
0
0
0
0
0
0
2=3
0
3
777777775
In Exercises 7–10, consider a simple random walk on the given
directed graph. Identify the communication classes of this Markov
chain as recurrent or transient, and ﬁnd the period of each commu-
nication class.

10.4
Classiﬁcation of States and Periodicity
C-41
7.
1
2
4
3
8.
1
4
2
5
3
9.
1
2
5
3
4
10.
1
4
2
5
6
3
11. Reorder the states in the Markov chain in Exercise 1 to
produce a transition matrix in canonical form.
12. Reorder the states in the Markov chain in Exercise 2 to
produce a transition matrix in canonical form.
13. Reorder the states in the Markov chain in Exercise 3 to
produce a transition matrix in canonical form.
14. Reorder the states in the Markov chain in Exercise 4 to
produce a transition matrix in canonical form.
15. Reorder the states in the Markov chain in Exercise 5 to
produce a transition matrix in canonical form.
16. Reorder the states in the Markov chain in Exercise 6 to
produce a transition matrix in canonical form.
17. Find the transition matrix for the Markov chain in Exercise 9
and reorder the states to produce a transition matrix in canon-
ical form.
18. Find the transition matrix for the Markov chain in Exercise 10
and reorder the states to produce a transition matrix in canon-
ical form.
19. Consider the mouse in the following maze from Section 10.1,
Exercise 19.
1
3
2
4
6
5
a. Identify the communication classes of this Markov chain
as recurrent or transient.
b. Find the period of each communication class.
c. Find the transition matrix for the Markov chain and
reorder the states to produce a transition matrix in
canonical form.
20. Consider the mouse in the following maze from Section 10.1,
Exercise 20.
3
4
5
1
2
a. Identify the communication classes of this Markov chain
as recurrent or transient.
b. Find the period of each communication class.
c. Find the transition matrix for the Markov chain and re-
order the states to produce a transition matrix in canonical
form.
In Exercises 21–26, mark each statement True or False. Justify
each answer.
21. (T/F) If two states i and j are both recurrent, then they must
belong to the same communication class.
22. (T/F) If state i is recurrent and state i communicates with
state j , then state j is also recurrent.
23. (T/F) All of the states in an irreducible Markov chain are
recurrent.
24. (T/F) If two states of a Markov chain have diﬀerent periods,
then the Markov chain is reducible.
25. (T/F) Every Markov chain must have at least one transient
class.
26. (T/F) Every Markov chain must have exactly one recurrent
class.
27. Conﬁrm Theorem 5 for the Markov chain in Exercise 7 by
taking powers of the transition matrix (see Example 9).
28. Conﬁrm Theorem 5 for the Markov chain in Exercise 8 by
taking high powers of the transition matrix (see Example 9).
29. Consider the Markov chain on f1; 2; 3g with transition matrix
P D
2
4
0
1=2
0
1
0
1
0
1=2
0
3
5
a. Explain why this Markov chain is irreducible and has
period 2.
b. Find a steady-state vector q for this Markov chain.
c. Find an invertible matrix A and a diagonal matrix D such
that P D ADA 1. (See Section 5.3.)
d. Use the result from part (c) to show that P n may be
written as
2
4
1=4
1=4
1=4
1=2
1=2
1=2
1=4
1=4
1=4
3
5
C . 1/n
2
4
1=4
 1=4
1=4
 1=2
1=2
 1=2
1=4
 1=4
1=4
3
5
e. Use the result from part (d) to conﬁrm Theorem 5 for P .

C-42
CHAPTER 10
Finite-State Markov Chains
30. Follow the plan of Exercise 29 to conﬁrm Theorem 5 for the
Markov chain with transition matrix
P D
2
4
0
p
0
1
0
1
0
1   p
0
3
5
where 0 < p < 1.
31. Conﬁrm Theorem 5 for the Markov chain in Example 6.
32. Matrix multiplication can be used to ﬁnd the canonical form
of a transition matrix. Consider the matrix P in Example 5
and the matrix
E D
2
66664
1
0
0
0
0
0
1
0
0
0
0
0
0
1
0
0
0
0
0
1
0
0
1
0
0
3
77775
Notice that the rows of E are the rows of the identity matrix
in the order 1, 2, 4, 5, 3.
a. Compute EP and explain what has happened to the
matrix P .
b. Compute PET and explain what has happened to the
matrix P .
c. Compute EPET and explain what has happened to the
matrix P .
33. Let A be an n  n matrix and let E be an n  n matrix
resulting from permuting the rows of In, the n  n identity
matrix. The matrix E is called a permutation matrix.
a. Show that EA is the matrix A with its rows permuted in
exactly the same order that the rows of In were permuted
to form E. Hint: Any permutation of rows can be written
as a sequence of swaps of pairs of rows.
b. Apply the result of part (a) to AT to show that AET is the
matrix A with its columns permuted in exactly the same
order that the rows of In were permuted to form E.
c. Explain why EAET is the matrix A with its rows and
columns permuted in exactly the same order that the rows
of In were permuted to form E.
d. In the process of ﬁnding the canonical form of a transition
matrix, does it matter whether the rows of the matrix or
the columns of the matrix are permuted ﬁrst? Why or why
not?
..
Solution to Practice Problem
.
1. First note that states 1 and 3 communicate with each other, as do states 2 and 4.
However, there is no way to proceed from either state 1 or state 3 to either state
2 or state 4, so the communication classes are f1; 3g and f2; 4g. Since the chain
stays in class f1; 3g after it enters this class, class f1; 3g is recurrent. Likewise,
there is a positive probability of leaving class f2; 4g at any time, so class f2; 4g
is transient. One ordering of the states that produces a canonical form is 1, 3, 2, 4:
the corresponding transition matrix is
P
rearrange
     !
columns
1
3
2
4
2
66664
1=4
0
3=4
0
1=2
0
1=2
0
1=3
1=3
0
1=3
0
1=3
1=3
1=3
3
77775
1
2
3
4
rearrange
     !
rows
1
3
2
4
2
66664
1=4
3=4
0
0
1=2
1=2
0
0
1=3
0
1=3
1=3
0
1=3
1=3
1=3
3
77775
1
3
2
4
..10.5 The Fundamental Matrix
The return time for a state in an irreducible Markov chain was deﬁned in Section 10.3
to be the expected number of steps needed for the system to return to its starting state.
This section studies the expected number of steps needed for a system to pass from
one state to another state, which is called a transit time. Another quantity of interest
is the probability that the system visits one state before it visits another. It is perhaps
surprising that discussing these issues for irreducible Markov chains begins by working
with reducible Markov chains, particularly those with transient states.

10.5
The Fundamental Matrix
C-43
The Fundamental Matrix and Transient States
The ﬁrst goal is to compute the expected number of visits the system makes to a state i
given that the system starts in state j , where j is a transient state. Suppose that a Markov
chain has at least one transient state. Its transition matrix may be written in canonical
form as
P D
 R
S
O
Q

Since at least one state is transient, S has at least one nonzero entry. In order for P to be
a stochastic matrix, at least one of the columns of Q must sum to less than 1. The matrix
Q is called a substochastic matrix. It can be shown that
lim
k!1 Qk D O
for any substochastic matrix Q. This fact implies that if the system is started in a transient
class, it must eventually make a transition to a recurrent class and never visit any state
outside that recurrent class again. The system is thus eventually absorbed by some
recurrent class.
Now let j and i be transient states, and suppose that the Markov chain starts at state
j . Let vij be the number of visits the system makes to state i before the absorption into
a recurrent class. The goal is to calculate EŒvij , which is the expected value of vij . To
do so, a special kind of random variable called an indicator random variable is useful.
An indicator random variable I is a random variable that is 1 if an event happens and
is 0 if the event does not happen. Symbolically,
I D
(
0
if the event does not happen
1
if the event happens
The expected value of an indicator random variable may be easily calculated:
EŒI D 0  P.I D 0/ C 1  P.I D 1/ D P.I D 1/ D P.event happens/
(1)
Returning to the discussion of the number of visits to state i starting at state j , let Ik be
the indicator random variable for the event “the system visits state i at step k.” Then
vij D I0 C I1 C I2 C : : : D
1
X
kD0
Ik
since a visit to state i at a particular time will cause 1 to be added to the running total of
visits kept in vij . Using Equation (1), the expected value of vij is
EŒvij  D E
" 1
X
kD0
Ik
#
D
1
X
kD0
EŒIk D
1
X
kD0
P.Ik D 1/ D
1
X
kD0
P.visit to i at step k/
But P.visit to i at step k/ is just the .i; j /-entry in the matrix Qk, so
EŒvij  D
1
X
kD0
.Qk/ij
Thus the expected number of times that the system visits state i starting at state j is the
.i; j/-entry in the matrix
I C Q C Q2 C Q3 C : : : D
1
X
kD0
Qk

C-44
CHAPTER 10
Finite-State Markov Chains
Using the argument given in Section 2.6,
I C Q C Q2 C Q3 C : : : D .I   Q/ 1
The matrix .I   Q/ 1 is called the fundamental matrix of the Markov chain and is
denoted by M. The interpretation of the entries in M is given in the following theorem.
THEOREM 6
Let j and i be transient states of a Markov chain, and let Q be that portion of the
transition matrix that governs movement between transient states.
a. When the chain starts at a transient state j , the .i; j /-entry of M D .I   Q/ 1
is the expected number of visits to the transient state i before absorption into a
recurrent class.
b. When the chain starts at a transient state j , the sum of the entries in column j
of M D .I   Q/ 1 is the expected number of time steps until absorption.
An alternative proof of Theorem 6 is given in Appendix 2.
EXAMPLE 1
Consider an unbiased random walk on f1; 2; 3; 4; 5g with absorbing
boundaries. If the system starts in state 3, ﬁnd the expected number of visits to state 2
before absorption. Also ﬁnd the expected number of steps until absorption starting at
states 2, 3, and 4.
SOLUTION
Placing the states in the order 1, 5, 2, 3, 4 produces a transition matrix in
canonical form:
1
2
3
4
5
2
6666664
1
0
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
0
1
3
7777775
1
2
3
4
5
rearrange
     !
columns
1
5
2
3
4
2
6666664
1
0
0
0
0
0
0
0
0
1
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
3
7777775
1
2
3
4
5
rearrange
     !
rows
1
5
2
3
4
2
6666664
1
0
0
0
0
0
1
0
0
0
1=2
0
0
1=2
0
0
0
1=2
0
1=2
0
1=2
0
1=2
0
3
7777775
1
5
2
3
4
The matrix Q and the fundamental matrix M D .I   Q/ 1 are
Q D
2
3
4
2
64
0
1=2
0
1=2
0
1=2
0
1=2
0
3
75
2
3
4
and
M D
2
3
4
2
64
3=2
1
1=2
1
2
1
1=2
1
3=2
3
75
2
3
4
Starting at state 3, the expected number of visits to state 2 until absorption is the entry
of M whose row corresponds to state 2 and whose column corresponds to state 3. This
value is 1, so the chain will visit state 2 once on the average before being absorbed. The
sum of the column of M corresponding to state 2 (or state 4) is 3, so the expected number
of steps until absorption is three if starting at state 2 (or state 4). Likewise, the expected
number of steps until absorption starting at state 3 is four.

10.5
The Fundamental Matrix
C-45
Transit Times
Consider the problem of calculating the expected number of steps tji needed to travel
from state j to state i in an irreducible Markov chain. If the states i and j are the same
state, the value tjj is the expected return time to state j found in Section 10.4. The value
tji will be called the transit time (or mean ﬁrst passage time) from state j to state i.
Surprisingly, the insight into transient states provided by Theorem 6 is exactly what is
needed to calculate tji.
Finding the transit time of a Markov chain from state j to state i begins by changing
the transition matrix P for the chain. First reorder the states so that state i comes ﬁrst.
The new matrix has the form
 pii
S
X
Q

for some matrices S, X, and Q. Next change the ﬁrst column of the matrix from
 pii
X

to
 1
O

, where O is a zero vector of appropriate size. In terms of probabilities, it is
now impossible to leave state i after entering it. State i is now an absorbing state for the
Markov chain, and the transition matrix now has the form
 1
S
O
Q

The expected number of steps tji that it takes to reach state i after starting at state j may
be calculated using Theorem 6(b): it will be the sum of the column of M corresponding
to state j .
EXAMPLE 2
Consider an unbiased random walk on f1; 2; 3; 4; 5g with reﬂecting
boundaries. Find the expected number of steps tj4 required to get to state 4 starting at
any state j ¤ 4 of the chain.
SOLUTION
The transition matrix for this Markov chain is
P D
2
6666664
0
1=2
0
0
0
1
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1
0
0
0
1=2
0
3
7777775
First reorder the states to list state 4 ﬁrst, then convert state 4 to an absorbing state.
1
2
3
4
5
2
6666664
0
1
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1
0
3
7777775
1
2
3
4
5
rearrange
     !
columns
4
1
2
3
5
2
6666664
0
0
1=2
0
1=2
0
1
0
0
0
1=2
0
1=2
0
0
0
1=2
0
1=2
0
0
0
0
1
0
3
7777775
1
2
3
4
5
rearrange
     !
rows
4
1
2
3
5
2
6666664
0
0
0
1=2
1=2
0
0
1
0
0
0
1=2
0
1=2
0
1=2
0
1=2
0
0
1
0
0
0
0
3
7777775
4
1
2
3
5

C-46
CHAPTER 10
Finite-State Markov Chains
convert
     !
state 4
4
1
2
3
5
2
6666664
1
0
0
0
0
0
0
1
0
0
0
1=2
0
1=2
0
1=2
0
1=2
0
0
1
0
0
0
0
3
7777775
4
1
2
3
5
The matrix Q and the fundamental matrix M D .I   Q/ 1 are
Q D
1
2
3
5
2
66664
0
1
0
0
1=2
0
1=2
0
0
1=2
0
0
0
0
0
0
3
77775
1
2
3
5
and
M D
1
2
3
5
2
66664
3
4
2
0
2
4
2
0
1
2
2
0
0
0
0
1
3
77775
1
2
3
5
Summing the columns of M gives t14 D 9, t24 D 8, t34 D 5, and t54 D 1.
Absorption Probabilities
Suppose that a Markov chain has more than one recurrent class and at least one transient
state j . If the chain starts at state j , then the chain will eventually be absorbed into one of
the recurrent classes; the probability that the chain is absorbed into a particular recurrent
class is called the absorption probability for that recurrent class. The fundamental
matrix is used in calculating the absorption probabilities.
To calculate the absorption probabilities, begin by changing the transition matrix for
the Markov chain. First write all recurrent classes as single states i with pii D 1; that is,
each recurrent class coalesces into an absorbing state. (Exercises 41 and 42 explore the
information that the absorption probabilities give for recurrent classes with more than
one state.) A canonical form for this altered transition matrix is
P D
 I
S
O
Q

where the identity matrix describes the lack of movement between the absorbing states.
Let j be a transient state and let i be an absorbing state for the changed Markov
chain; to ﬁnd the probability that the chain starting at j is eventually absorbed by i,
consider the .i; j/-entry in the matrix P k. This entry is the probability that a system that
starts at state j is at state i after k steps. Since i is an absorbing state, in order for the
system to be at state i, the system must have been absorbed by state i at some step at or
before the kth step. Thus the probability that the system has been absorbed by state i at
or before the kth step is just the .i; j /-entry in the matrix P k, and the probability that the
chain starting at j is eventually absorbed by i is the .i; j /-entry in lim
k!1 P k. Computing
P k using rules for multiplying partitioned matrices (see Section 2.4) gives
P 2 D
 I
S C SQ
O
Q2

;
P 3 D
 I
S C SQ C SQ2
O
Q3

and it may be proved by induction (Exercise 43) that
P k D
 I
Sk
O
Qk

where
Sk D S C SQ C SQ2 C : : : C SQk 1
D S.I C Q C Q2 C : : : C Qk 1/

10.5
The Fundamental Matrix
C-47
Since j is a transient state and i is an absorbing state, only the entries in Sk need be
considered. The probability that the chain starting at j is eventually absorbed by i may
thus be found by investigating the matrix
A D lim
k!1 Sk D lim
k!1 S.I C Q C Q2 C : : : C Qk 1/
D S.I C Q C Q2 C : : :/ D SM
where M is the fundamental matrix for the Markov chain with coalesced recurrent
classes. The .i; j /-entry in A is the probability that the chain starting at j is eventually
absorbed by i. The following theorem summarizes these ideas; an alternative proof is
given in Appendix 2.
THEOREM 7
Suppose that the recurrent classes of a Markov chain are all absorbing states. Let j
be a transient state and let i be an absorbing state of this chain. Then the probability
that the Markov chain starting at state j is eventually absorbed by state i is the
.i; j/-element of the matrix A D SM, where M is the fundamental matrix of the
Markov chain and S is that portion of the transition matrix that governs movement
from transient states to absorbing states.
EXAMPLE 3
Consider the unbiased random walk on f1; 2; 3; 4; 5g with absorbing
boundaries studied in Example 1. Find the probability that the chain is absorbed into
state 1 given that the chain starts at state 4.
SOLUTION
Placing the states in the order f1; 5; 2; 3; 4g gives the canonical form of
the transition matrix:
1
5
2
3
4
2
6666664
1
0
0
0
0
0
1
0
0
0
1=2
0
0
1=2
0
0
0
1=2
0
1=2
0
1=2
0
1=2
0
3
7777775
1
5
2
3
4
The matrix Q and the fundamental matrix M D .I   Q/ 1 are
Q D
2
3
4
2
64
0
1=2
0
1=2
0
1=2
0
1=2
0
3
75
2
3
4
and
M D
2
3
4
2
64
3=2
1
1=2
1
2
1
1=2
1
3=2
3
75
2
3
4
so
A D SM D
"
1=2
0
0
0
0
1=2
# 2
64
3=2
1
1=2
1
2
1
1=2
1
3=2
3
75 D
2
3
4
"
3=4
1=4
1=2
1=2
1=4
3=4
#
1
5
The columns of A correspond to the transient states 2, 3, and 4 in that order, while the
rows correspond to the absorbing states 1 and 5. The probability that the chain that started
at state 4 is absorbed at state 1 is 1=4.
Absorption probabilities may be used to compute the probability that a system
modeled by an irreducible Markov chain visits one state before another.

C-48
CHAPTER 10
Finite-State Markov Chains
EXAMPLE 4
Consider a simple random walk on the graph in Figure 1. What is the
probability that a walker starting at state 1 visits state 4 before visiting state 7?
SOLUTION
Changing state 4 and state 7 to absorbing states and then computing the
absorption probabilities starting at state 1 will answer this question. Begin by reordering
the states as 4, 7, 1, 2, 3, 5, 6 and rewrite states 4 and 7 as absorbing states:
1
3
4
2
5
6
7
FIGURE 1
The graph for Example 4.
1
2
3
4
5
6
7
2
666666666664
0
1=2
1=2
0
0
0
0
1=3
0
1=3
0
1=3
0
0
1=4
1=4
0
1=4
0
1=4
0
0
0
1
0
0
0
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
0
0
0
0
0
1
0
3
777777777775
1
2
3
4
5
6
7
rearrange
     !
columns
4
7
1
2
3
5
6
2
666666666664
0
0
1
0
0
0
0
0
0
0
0
0
1
0
0
1=2
1=2
0
0
0
0
1=3
0
1=3
0
1=3
0
0
1=4
1=4
0
1=4
0
1=4
0
0
1=2
0
0
0
1=2
0
0
0
1=3
0
1=3
0
1=3
3
777777777775
1
2
3
4
5
6
7
rearrange
     !
rows
4
7
1
2
3
5
6
2
666666666664
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
1=2
1=2
0
0
0
0
1=3
0
1=3
1=3
0
1=4
0
1=4
1=4
0
0
1=4
0
0
0
1=2
0
0
1=2
0
1=3
0
0
1=3
1=3
0
3
777777777775
4
7
1
2
3
5
6
convert
        !
states 4 and 7
4
7
1
2
3
5
6
2
666666666664
1
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1=2
1=2
0
0
0
0
1=3
0
1=3
1=3
0
1=4
0
1=4
1=4
0
0
1=4
0
0
0
1=2
0
0
1=2
0
1=3
0
0
1=3
1=3
0
3
777777777775
4
7
1
2
3
5
6
The resulting transition matrix is
 I
S
O
Q

, with
S D
1
2
3
5
6
"
0
0
0
0
1=4
0
0
0
0
1=3
#
4
7

10.5
The Fundamental Matrix
C-49
and
Q D
1
2
3
5
6
2
6666664
0
1=2
1=2
0
0
1=3
0
1=3
1=3
0
1=4
1=4
0
0
1=4
0
1=2
0
0
1=2
0
0
1=3
1=3
0
3
7777775
1
2
3
5
6
so
M D .I   Q/ 1 D
1
2
3
5
6
2
6666664
12=5
12=5
12=5
6=5
6=5
8=5
31=10
34=15
22=15
13=10
6=5
17=10
38=15
14=15
11=10
6=5
11=5
28=15
34=15
8=5
4=5
13=10
22=15
16=15
19=10
3
7777775
1
2
3
5
6
and
A D SM D
1
2
3
5
6
"
3=5
2=5
17=30
13=30
19=30
11=30
7=15
8=15
11=30
19=30
#
4
7
Since the ﬁrst column of A corresponds to state 1 and the rows correspond to states 4
and 7, respectively, the probability of visiting 4 before visiting 7 is 3=5.
A mathematical model that uses Theorems 6 and 7 appears in Section 10.6.
.
Practice Problems
.
1. Consider a Markov chain on f1; 2; 3; 4g with transition matrix
P D
2
66664
1
1=2
0
0
0
1=6
1=2
0
0
1=3
1=6
0
0
0
1=3
1
3
77775
a. If the Markov chain starts at state 2, ﬁnd the expected number of steps before
the chain is absorbed.
b. If the Markov chain starts at state 2, ﬁnd the probability that the chain is absorbed
at state 1.
2. Consider a Markov chain on f1; 2; 3; 4g with transition matrix
P D
2
66664
2=3
1=2
0
0
1=3
1=6
1=2
0
0
1=3
1=6
1=2
0
0
1=3
1=2
3
77775
a. If the Markov chain starts at state 2, ﬁnd the expected number of steps required
to reach state 4.
b. If the Markov chain starts at state 2, ﬁnd the probability that state 1 is reached
before state 4.

C-50
CHAPTER 10
Finite-State Markov Chains
10.5 Exercises
In Exercises 1–3, ﬁnd the fundamental matrix of the Markov chain
with the given transition matrix. Assume that the state space in
each case is f1; 2; : : : ; ng. If reordering of states is necessary, list
the order in which the states have been reordered.
1.
2
66664
1
0
1=6
0
0
1
0
1=3
0
0
1=3
2=3
0
0
1=2
0
3
77775
2.
2
6666664
1
0
0
1=4
1=5
0
1
0
1=8
1=10
0
0
1
1=8
1=5
0
0
0
1=4
3=10
0
0
0
1=4
1=5
3
7777775
3.
2
6666664
1=5
0
1=10
0
1=5
1=5
1
1=5
0
1=5
1=5
0
1=5
0
1=4
1=5
0
1=4
1
1=10
1=5
0
1=4
0
1=4
3
7777775
In Exercises 4–6, ﬁnd the matrix A D limn!1 Sn for the Markov
chain with the given transition matrix. Assume that the state space
in each case is f1; 2; : : : ; ng. If reordering of states is necessary,
list the order in which the states have been reordered.
4.
2
66664
1
0
1=6
0
0
1
0
1=3
0
0
1=3
2=3
0
0
1=2
0
3
77775
5.
2
6666664
1
0
0
1=4
1=5
0
1
0
1=8
1=10
0
0
1
1=8
1=5
0
0
0
1=4
3=10
0
0
0
1=4
1=5
3
7777775
6.
2
6666664
1=5
0
1=10
0
1=5
1=5
1
1=5
0
1=5
1=5
0
1=5
0
1=4
1=5
0
1=4
1
1=10
1=5
0
1=4
0
1=4
3
7777775
7. Suppose that the Markov chain in Exercise 1 starts at state 3.
How many visits will the chain make to state 4 on average
before absorption?
8. Suppose that the Markov chain in Exercise 2 starts at state
4. How many steps will the chain take on average before
absorption?
9. Suppose that the Markov chain in Exercise 3 starts at state 1.
How many steps will the chain take on average before absorp-
tion?
10. Suppose that the Markov chain in Exercise 4 starts at state 3.
What is the probability that the chain is absorbed at state 1?
11. Suppose that the Markov chain in Exercise 5 starts at state 4.
Find the probabilities that the chain is absorbed at states 1, 2,
and 3.
12. Suppose that the Markov chain in Exercise 6 starts at state 5.
Find the probabilities that the chain is absorbed at states 2
and 4.
13. Consider a simple random walk on the following graph.
1
2
4
3
5
a. Suppose that the walker begins in state 5. What is the
expected number of visits to state 2 before the walker
visits state 1?
b. Suppose again that the walker begins in state 5. What
is the expected number of steps until the walker reaches
state 1?
c. Now suppose that the walker starts in state 1. What is the
probability that the walker reaches state 5 before reaching
state 2?
14. Consider a simple random walk on the following graph.
1
2
3
4
a. Suppose that the walker begins in state 3. What is the
expected number of visits to state 2 before the walker
visits state 1?
b. Suppose again that the walker begins in state 3. What
is the expected number of steps until the walker reaches
state 1?
c. Now suppose that the walker starts in state 1. What is the
probability that the walker reaches state 3 before reaching
state 2?

10.5
The Fundamental Matrix
C-51
15. Consider a simple random walk on the following directed
graph. Suppose that the walker starts at state 1.
1
2
3
4
a. How many visits to state 2 does the walker expect to make
before visiting state 3?
b. How many steps does the walker expect to take before
visiting state 3?
16. Consider a simple random walk on the following directed
graph. Suppose that the walker starts at state 4.
1
4
2
5
3
a. How many visits to state 3 does the walker expect to make
before visiting state 2?
b. How many steps does the walker expect to take before
visiting state 2?
17. Consider the mouse in the following maze from Section 10.1,
Exercise 17.
3
4
5
1
2
If the mouse starts in room 2, what is the probability that the
mouse visits room 3 before visiting room 4?
18. Consider the mouse in the following maze from Section 10.1,
Exercise 18.
1
2
3
5
4
If the mouse starts in room 1, what is the probability that the
mouse visits room 3 before visiting room 4?
19. Consider the mouse in the following maze from Section 10.1,
Exercise 19.
1
3
2
4
6
5
If the mouse starts in room 1, how many steps on average will
it take the mouse to get to room 6?
20. Consider the mouse in the following maze from Section 10.1,
Exercise 20.
3
4
5
1
2
If the mouse starts in room 1, how many steps on average will
it take the mouse to get to room 5?
In Exercises 21–26, mark each statement True or False. Justify
each answer.
21. (T/F) The .i; j /-element in the fundamental matrix M is the
expected number of visits to the transient state j prior to
absorption, starting at the transient state i.
22. (T/F) The sum of the column j of the fundamental matrix M
is the expected number of time steps until absorption.
23. (T/F) The .j; i/-element in the fundamental matrix gives
the expected number of visits to state i prior to absorption,
starting at state j .
24. (T/F) Transit times may be computed directly from the entries
in the transition matrix.
25. (T/F) The probability that the Markov chain starting at state
i is eventually absorbed by state j is the .j; i/-element of the
matrix A D SM, where M is the fundamental matrix of the
Markov chain and S is that portion of the transition matrix
that governs movement from transient states to absorbing
states.
26. (T/F) If A is an m  m substochastic matrix, then the entries
in An approach 0 as n increases.
27. Suppose that the weather in Charlotte is modeled using the
Markov chain in Section 10.1, Exercise 27. If it is sunny
today, what is the probability that the weather will be cloudy
before it is rainy?
28. Suppose that the weather in Charlotte is modeled using the
Markov chain in Section 10.1, Exercise 28. If it rained yester-
day and today, how many days on average will it take before
there are two consecutive days with no rain?
29. Consider a set of webpages hyperlinked by the given directed
graph that was studied in Section 10.2, Exercise 29.

C-52
CHAPTER 10
Finite-State Markov Chains
1
2
5
3
4
If a random surfer starts on page 1, how many mouse clicks
on average will the surfer make before becoming stuck at a
dangling node?
30. Consider a set of webpages hyperlinked by the given directed
graph that was studied in Section 10.2, Exercise 30.
1
4
2
5
6
3
If a random surfer starts on page 3, what is the probability
that the surfer will eventually become stuck on page 1, which
is a dangling node?
Exercises 31–34 concern the Markov chain model for scoring a
tennis match described in Section 10.1, Exercise 35. Suppose that
players A and B are playing a tennis match, that the probability that
player A wins any point is p D :6, and that the game is currently
at “deuce.”
31. How many more points will the tennis game be expected to
last?
32. Find the probability that player A wins the game.
33. Repeat Exercise 31 if the game is
a. currently at “advantage A.”
b. currently at “advantage B.”
34. Repeat Exercise 32 if the game is
a. currently at “advantage A.”
b. currently at “advantage B.”
Exercises 35–40 concern the two Markov chain models for scoring
volleyball games described in Section 10.1, Exercise 36. Suppose
that teams A and B are playing a 15-point volleyball game that
is tied 15-15 with team A serving. Suppose that the probability
p that team A wins any rally for which it serves is p D :7, and
the probability q that team B wins any rally for which it serves is
q D :6.
35. Suppose that rally point scoring is being used. How many
more rallies will the volleyball game be expected to last?
36. Suppose that rally point scoring is being used. Find the
probability that team A wins the game.
37. Suppose that side out scoring is being used. How many more
rallies will the volleyball game be expected to last?
38. Suppose that side out scoring is being used. Find the proba-
bility that team A wins the game.
39. Rally point scoring was introduced to make volleyball
matches take less time. Considering the results of Exercises
35 and 37, does using rally point scoring really lead to fewer
rallies being played?
40. Since p D :7 and q D :6, it seems that team A is the dominant
team. Does it really matter which scoring system is chosen?
Should the manager of each team have a preference?
41. Consider a Markov chain on f1; 2; 3; 4; 5g with transition
matrix
P D
2
6666664
1=4
1=2
1=3
0
1=4
3=4
1=2
0
1=3
1=4
0
0
0
1=3
0
0
0
1=3
0
0
0
0
1=3
1=3
1=2
3
7777775
Find lim
n!1 P n by the following steps.
a. What are the recurrent and transient classes for this chain?
b. Find the limiting matrix for each recurrent class.
c. Determine the long-range probabilities for the Markov
chain starting from each transient state.
d. Use the results of parts (b) and (c) to ﬁnd lim
n!1 P n.
e. Conﬁrm your answer in part (d) by taking P to a high
power.
42. Consider a Markov chain on f1; 2; 3; 4; 5; 6g with transition
matrix
P D
2
6666666664
1=3
1=2
0
0
1=2
0
2=3
1=2
0
0
0
0
0
0
1=4
2=3
0
1=2
0
0
3=4
1=3
0
0
0
0
0
0
1=4
1=4
0
0
0
0
1=4
1=4
3
7777777775
Find lim
n!1 P n by the following steps.
a. What are the recurrent and transient classes for this chain?
b. Find the limiting matrix for each recurrent class.
c. Find the absorption probabilities from each transient state
into each recurrent class.
d. Use the results of parts (b) and (c) to ﬁnd lim
n!1 P n.
e. Conﬁrm your answer in part (d) by taking P to a high
power.
43. Show that if P D
 I
S
O
Q

, then P k D
 I
Sk
O
Qk

,
where
Sk D S C SQ C SQ2 C : : : C SQk 1
D S.I C Q C Q2 C : : : C Qk 1/:

10.5
The Fundamental Matrix
C-53
..
Solutions to Practice Problems
.
1. a. Since states 1 and 4 are absorbing states, reordering the states as f1; 4; 2; 3g
produces the canonical form
P D
1
4
2
3
2
66664
1
0
0
0
0
1
0
0
1=2
0
1=6
1=3
0
1=3
1=2
1=6
3
77775
1
4
2
3
So
Q D
2
3
"
1=6
1=3
1=2
1=6
#
2
3
and
M D
2
3
"
30=19
12=19
18=19
30=19
#
2
3
The expected number of steps needed when starting at state 2 before the chain
is absorbed is the sum of the entries in the column of M corresponding to state
2, which is
30
19 C 12
19 D 42
19
b. Using the canonical form of the transition matrix, we see that
S D
2
3
"
1=2
0
0
1=3
#
1
4
and
A D SM D
2
3
"
15=19
4=19
9=19
10=19
#
1
4
The probability that the chain is absorbed at state 1 given that the Markov chain
starts at state 2 is the entry in A whose row corresponds to state 1 and whose
column corresponds to state 2; this entry is 15=19.
2. a. Reorder the states as f4; 1; 2; 3g and make state 4 into an absorbing state to
produce the canonical form
P D
4
1
2
3
2
66664
1
0
0
0
0
2=3
1=3
0
0
1=2
1=6
1=3
1=3
0
1=2
1=6
3
77775
4
1
2
3
So
Q D
1
2
3
2
64
2=3
1=3
0
1=2
1=6
1=3
0
1=2
1=6
3
75
1
2
3
and
M D
1
2
3
2
64
14:25
7:50
3:00
11:25
7:50
3:00
6:75
4:50
3:00
3
75
1
2
3
The expected number of steps required to reach state 4, starting at state 2, is the
sum of the entries in the column of M corresponding to state 2, which is
11:25 C 7:50 C 3:00 D 21:75

C-54
CHAPTER 10
Finite-State Markov Chains
.
Solutions to Practice Problems (Continued)
.
b. Make states 1 and 4 into absorbing states and reorder the states as f1; 4; 2; 3g to
produce the canonical form
P D
1
4
2
3
2
66664
1
0
0
0
0
1
0
0
1=2
0
1=6
1=3
0
1=3
1=2
1=6
3
77775
1
4
2
3
So
Q D
2
3
"
1=6
1=3
1=2
1=6
#
2
3 ;
M D
2
3
"
30=19
12=19
18=19
30=19
#
2
3 ;
S D
2
3
"
1=2
0
0
1=3
#
1
4 ;
and
A D SM D
2
3
"
15=19
4=19
9=19
10=19
#
1
4
Thus the probability that, starting at state 2, state 1 is reached before state 4 is
the entry in A whose row corresponds to state 1 and whose column corresponds
to state 2; this entry is 15=19.
..10.6 Markov Chains and Baseball Statistics
Markov chains are used to model a wide variety of systems. The examples and exercises
in this chapter have shown how Markov chains may be used to model various situations.
The ﬁnal example to be explored is a model for how runners proceed around the bases
in baseball. This model leads to useful measures of expected run production both for a
team and for individual players.
Baseball Modeled by a Markov Chain
Many baseball fans carefully study the statistics of their favorite teams. The teams
themselves use baseball statistics for individual players to determine strategy during
games, and to make hiring decisions.1 This section shows how a Markov chain is used to
predict the number of earned runs a team will score and to compare the oﬀensive abilities
of diﬀerent players. Some exercises suggest how to use Markov chains to investigate
matters of baseball strategy, such as deciding whether to attempt a sacriﬁce or a steal.
The Markov chain in this section provides a way to analyze how runs are scored
during one half-inning of a baseball game. The states of the chain are the various
conﬁgurations of runners on bases and the number of outs. See Table 1.
The ﬁrst state in the left column of Table 1 (“no bases occupied, 0 outs”) is the initial
state of the chain, when the baseball half-inning begins (that is, when one team becomes
the team “at bat”). The four states in the far right column describe the various ways the
half-inning can end (when the third out occurs and the teams trade places). Physically,
1 The use of statistical analysis in baseball is called sabermetrics as a tribute to SABR, the Society for
American Baseball Research. An overview of sabermetrics can be found at
http://en.wikipedia.org/wiki/Sabermetrics.

10.6
Markov Chains and Baseball Statistics
C-55
TABLE 1
The 28 States of a Baseball Markov Chain
Bases Occupied
Outs
State
Left on Base
Outs
State
None
0
0:0
0
3
0:3
First
0
1:0
1
3
1:3
Second
0
2:0
2
3
2:3
Third
0
3:0
3
3
3:3
First and Second
0
12:0
First and Third
0
13:0
Second and Third
0
23:0
First, Second, and Third
0
123:0
None
1
0:1
First
1
1:1
Second
1
2:1
Third
1
3:1
First and Second
1
12:1
First and Third
1
13:1
Second and Third
1
23:1
First, Second, and Third
1
123:1
None
2
0:2
First
2
1:2
Second
2
2:2
Third
2
3:2
First and Second
2
12:2
First and Third
2
13:2
Second and Third
2
23:2
First, Second, and Third
2
123:2
the half-inning is completed when the third out occurs. Mathematically, the Markov
chain continues in one of the four “ﬁnal” states. (The model only applies to a game in
which each half-inning is completed.) So, each of these four states is an absorbing state
of the chain. The other 24 states are transient states, because whenever an out is made,
the states with fewer outs can never occur again.
The Markov chain moves from state to state because of the actions of the batters.
The transition probabilities of the chain are the probabilities of possible outcomes of a
batter’s action. For a Markov chain, the transition probabilities must remain the same
from batter to batter, so the model does not allow for variations among batters. This
assumption means that each batter for a team hits as an “average batter” for the team.2
The model also assumes that only the batter determines how the runners move
around the bases. This means that stolen bases, wild pitches, and passed balls are
not considered. Also, errors by the players in the ﬁeld are not allowed, so the model
only calculates earned runs—runs that are scored without the beneﬁt of ﬁelding errors.
Finally, the model considers only seven possible outcomes at the plate: a single (arriving
safely at ﬁrst base and stopping there), a double (arriving safely at second base), a triple
2 This unrealistic assumption can be overcome by using a more complicated model that uses diﬀerent
transition matrices for each batter. Nevertheless, the model presented here can lead to useful information
about the team. Later in the section, the model will be used to evaluate individual players.

C-56
CHAPTER 10
Finite-State Markov Chains
(arriving safely at third base), a home run, a walk (advancing to ﬁrst base without hitting
the ball), a hit batsman (a pitched ball hits the batter, and the batter advances to ﬁrst
base), and an “out.” Thus, the model allows no double or triple plays, no sacriﬁces, and
no sacriﬁce ﬂies. However, Markov chain models can be constructed that include some
of these excluded events.3
Constructing the Transition Matrix
The 28  28 transition matrix for the Markov chain has the canonical form
P D
 I4
S
O
Q

(1)
where I4 is the 4  4 identity matrix (because the only recurrent states are the four
absorbing states, one of which is entered when the third out occurs), S is a 4  24 matrix,
and Q is a 24  24 substochastic matrix. The columns of S and Q correspond to the
transient states, in the order shown in Table 1. The entries in S describe the transitions
from the 24 transient states (with 0, 1, or 2 outs) to the absorbing states (with 3 outs).
Note that the only way to enter an absorbing state is to come from a state with 2 outs.
Let pO denote the probability that the batter makes an out. Then S may be written in
block form, with three 4  8 blocks, as
S D
 O
O
X 
where
X D
0:2
1:2
2:2
3:2
12:2
13:2
23:2
123:2
2
664
pO
0
0
0
0
pO
0
0
0
pO
0
0
0
pO
0
0
0
0
pO
0
0
0
pO
0
0
0
pO
0
0
0
0
pO
3
775
0:3
1:3
2:3
3:3
(2)
The matrix X describes the transitions from the transient states with 2 outs to the
absorbing states with 3 outs. (For example, columns 2, 3, and 4 of X list the probabilities
that the batter makes the third out when one runner is on one of the three bases.) The
substochastic matrix Q has the following block form, with 8  8 blocks,
Q D
0
1
2
2
4
A
B
O
O
A
B
O
O
A
3
5
0
1
2
(3)
The labels on the rows and columns of Q represent the number of outs. The four zero
blocks in Q reﬂect the facts that the number of outs cannot go from 1 to 0, from 2 to 0
or 1, or from 0 directly to 2 in one step. The matrix A describes how the various base
conﬁgurations change when the number of outs does not change.
The entries in A and B depend on how the batter’s action at the plate aﬀects any
runners that may already be on base. The Markov chain model presented here makes the
assumptions shown in Table 2. The exercises consider some alternative assumptions.
The entries in the 8  8 matrices A and B are constructed from the probabilities of
the six batting events in Table 2. Denote these probabilities by pW , p1, p2, p3, pH, and
pO, respectively. The notation pO was introduced earlier during the construction of the
matrix S.
3 Other models use “play-by-play” data. The number of transitions between states are counted and scaled to
produce a transition matrix. For these models it does not matter how the runners advance, merely that they do.

10.6
Markov Chains and Baseball Statistics
C-57
TABLE 2
Assumptions about Advancing Runners
Batting Event
Outcome
Walk or Hit Batsman
The batter advances to ﬁrst base. A runner on ﬁrst base advances to
second base. A runner on second base advances to third base only
if ﬁrst base was also occupied. A runner on third base scores only
if ﬁrst base and second base were also occupied.
Single
The batter advances to ﬁrst base. A runner on ﬁrst base advances to
second base. A runner on third base scores. A runner on second
base advances to third base half of the time and scores half of the
time.
Double
The batter advances to second base. A runner on ﬁrst base advances
to third base. A runner on second base scores. A runner on third
base scores.
Triple
The batter advances to third base. A runner on ﬁrst base scores. A
runner on second base scores. A runner on third base scores.
Home Run
The batter scores. A runner on ﬁrst base scores. A runner on second
base scores. A runner on third base scores.
Out
No runners advance. The number of outs increases by one.
The 8  8 matrix B involves the change of state when the number of outs increases.
In this case, the conﬁguration of runners on the bases does not change (see Table 2). So
B D pOI
where I is the 8  8 identity matrix.4
Matrix A concerns the situations in which the batter does not make an out and
either succeeds in reaching one of the bases or hits a home run. The construction of
A is discussed in Example 1 and in the exercises. The labels on the rows and columns
of A correspond to the states in Table 2. Here k is the ﬁxed number of outs: either 0, 1,
or 2.
A D
0:k
1:k
2:k
3:k
12:k
13:k
23:k
123:k
2
66666666664
pH
pW C p1
p2
p3
0
0
0
0
pH
0
0
p3
pW C p1
0
p2
0
pH
:5p1
p2
p3
pW
:5p1
0
0
pH
p1
p2
p3
0
pW
0
0
pH
0
0
p3
:5p1
0
p2
pW C :5p1
pH
0
0
p3
p1
0
p2
pW
pH
:5p1
p2
p3
0
:5p1
0
pW
pH
0
0
p3
:5p1
0
p2
pW C :5p1
3
77777777775
0:k
1:k
2:k
3:k
12:k
13:k
23:k
123:k
The analysis in Example 1 requires two facts from probability theory. If an event can
occur in two mutually exclusive ways, with probabilities p1 and p2, then the probability
of the event is p1 C p2. The probability that two independent events both occur is the
product of the separate probabilities for each event.
4 A batter can make an out in three ways—by striking out, by hitting a ﬂy ball that is caught, or by hitting
a ground ball that is thrown to ﬁrst base before the batter arrives. When the second or third case occurs, a
runner on a base sometimes can advance one base, but may also make an out and be removed from the bases.
Table 2 excludes these possibilities.

C-58
CHAPTER 10
Finite-State Markov Chains
EXAMPLE 1
a. Justify the transition probabilities for the initial state “no bases occupied.”
b. Justify the transition probabilities for the initial state “second base occupied.”
SOLUTION
a. For the ﬁrst column of A, the batter either advances to one of the bases or hits a
home run. So the probability that the bases remain unoccupied is pH. The batter
advances to ﬁrst base when the batter either walks (or is hit by a pitch) or hits a single.
Since the desired outcome can be reached in two diﬀerent ways, the probability of
success is the sum of the two probabilities—namely, pW C p1. The probabilities of
the batter advancing to second base or third base are, respectively, p2 and p3. All
other outcomes are impossible, because there can be at most one runner on base after
one batter when the starting state has no runners on base.
b. This concerns the third column of A. The initial state is 2:k (a runner on second base,
k outs). For entry .1; 3/ of A, the probability of a transition “to state 0:k” is required.
Suppose that only second base is occupied and the batter does not make an out. Only
a home run will empty the bases, so the .1; 3/-entry is pH.
Entry .2; 3/: (“to state 1:k”) To leave a player only on ﬁrst base, the batter must
get to ﬁrst base and the player on second base must reach home plate successfully.5
From Table 2, the probability of reaching home plate successfully from second base
is :5. Now, assume that these two events are independent, because only the actions
of the batter (and Table 2) inﬂuence the outcome. In this case, the probability of both
events happening at the same time is the product of these two probabilities, so the
.2; 3/-entry is :5p1.
Entry .3; 3/: (“to state 2:k”) To leave a player only on second base, the batter
must reach second base (a “double”) and the runner on second base must score. The
second condition, however, is automatically satisﬁed because of the assumption in
Table 2. So the probability of success in this case is p2. This is the .3; 3/-entry.
Entry .4; 3/: (“to state 3:k”) By an argument similar to that for the .3; 3/-entry,
the .4; 3/-entry is p3.
Entry .5; 3/: (“to state 12:k”) To leave players on ﬁrst base and second base,
the batter must get to ﬁrst base and the player on second base must remain there.
However, from Table 2, if the batter hits a single, the runner on second base will at
least get to third base. So, the only way for the desired outcome to occur is for the
batter to get a walk or be hit by a pitch. The .5; 3/-entry is thus pW .
Entry .6; 3/: (“to state 13:k”) This concerns the batter getting to ﬁrst base and
the runner on second base advancing to third base. This can happen only if the batter
hits a single, with probability p1, and the runner on second base stops at third base,
which happens with probability :5 (by Table 2). Since both events are required, the
.6; 3/-entry is the product :5p1.
Entry .7; 3/: (“to state 23:k”) To leave players on second base and third base, the
batter must hit a double and the runner on second base must advance only to third
base. Table 2 rules this out—when the batter hits a double, the runner on second base
scores. Thus the .7; 3/-entry is zero.
Entry .8; 3/: The starting state has just one runner on base. The next state cannot
have three runners on base, so the .8; 3/-entry is zero.
5 The only other way to make the player on second base “disappear” would be for the player to be tagged out,
but the model does not permit outs for runners on the bases.

10.6
Markov Chains and Baseball Statistics
C-59
EXAMPLE 2
Batting statistics are often displayed as in Table 3. Use the data in
Table 3 to obtain the transition probabilities for the 2002 Atlanta Braves.
TABLE 3
Atlanta Braves Batting Statistics---2002 Season
Walks
Hit Batsman
Singles
Doubles
Triples
Home Runs
Outs
558
54
959
280
25
164
4067
SOLUTION
The sum of the entries in Table 3 is 6107. This is the total number of times
that Atlanta Braves players came to bat during the 2002 baseball season. From the ﬁrst
two columns, there are 612 walks or hit batsmen. So, pW D 612=6107 D :1002. Of the
6107 times a player came to bat, a player hit a single 959 times, so p1 D 959=6107 D
:1570. Similar calculations provide p2 D :0458, p3 D :0041, pH D :0269, and pO D
:660. These values are placed in the matrices to produce the transition matrix for the
Markov chain.
Applying the Model
Now that the data for the stochastic matrix is available, Theorems 6 and 7 from Section
10.5 can provide information about how many earned runs to expect from the Atlanta
Braves during a typical game. The goal is to calculate how many earned runs the Braves
will score on average in each half-inning. First, observe that since three batters must
make an out to ﬁnish one half-inning, the number of runs scored in that half-inning is
given by
Œ# of runs D Œ# of batters   Œ# of runners left on base   3
(4)
If R is the number of runs scored in the half-inning, B is the number of batters, and L
is the number of runners left on base, Equation (4) becomes
R D B   L   3
(5)
The quantity of interest is EŒR, the expected number of earned runs scored. Properties
of expected value indicate that
EŒR D EŒB   EŒL   3
(6)
Each batter moves the Markov chain ahead one step. So, the expected number of batters
in a half-inning EŒB is the expected number of steps to absorption (at the third out)
when the chain begins at the initial state “0 bases occupied, 0 outs.” This initial state
corresponds to the ﬁfth column of the transition matrix
P D
 I4
S
O
Q

In baseball terms, Theorem 6 shows that
The expected number of players that bat in one half-inning is the sum of the entries
in column 1 of the fundamental matrix M D .I   Q/ 1.
Thus EŒB may be computed. The other quantity needed in Equation (6) is EŒL,
the expected number of runners left on base in a typical half-inning. This is given by the
following sum:
EŒL D 0  P.L D 0/ C 1  P.L D 1/ C 2  P.L D 2/ C 3  P.L D 3/
(7)

C-60
CHAPTER 10
Finite-State Markov Chains
Theorem 7 can provide this information because the recurrent classes for the chain are
just the four absorbing states (at the end of the half-inning). The probabilities needed
in Equation (7) are the probabilities of absorption into the four ﬁnal states of the half-
inning given that the initial state of the system is “0 bases occupied, 0 outs.” So the
desired probabilities are in column 1 of the matrix SM, where M is the fundamental
matrix of the chain and S D
 O
O
X 
as in Equation (2). The probabilities can
be used to calculate EŒL using Equation (7), and thus to ﬁnd EŒR.
EXAMPLE 3
When the Atlanta Braves data from Example 2 is used to construct the
transition matrix (not shown here), it turns out that the sum of the ﬁrst column of the
fundamental matrix M is 4:5048, and the ﬁrst column of the matrix SM is
2
664
:3520
:3309
:2365
:0805
3
775
Compute the number of earned runs the Braves can expect to score per inning based on
their performance in 2002. How many earned runs does the model predict for the entire
season, if the Braves play 1443 2
3 innings, as they did in 2002?
SOLUTION
The ﬁrst column of SM shows that, for example, the probability that the
Braves left no runners on base is :3520. The expected number of runners left on base is
EŒL D 0.:3520/ C 1.:3309/ C 2.:2365/ C 3.:0805/ D 1:0454
The expected number of batters is EŒB D 4:5048, the sum of the ﬁrst column of M:
From Equation (6), the expected number of earned runs EŒR is
EŒR D EŒB   EŒL   3 D 4:5048   1:0454   3 D :4594
The Markov chain model predicts that the Braves should average :4594 earned run per
inning. In 1443 2
3 innings, the total number of earned runs expected is
:4594  1443:67 D 663:22
The actual number of earned runs for the Braves in 2002 was 636, so the model’s error
is 27:22 runs, or about 4:3%.
Mathematical models are used by some Major League teams to compare the
oﬀensive proﬁles of single players. To analyze a player using the Markov chain model,
use the player’s batting statistics instead of a team’s statistics. Compute the expected
number of earned runs that a team of such players would score in an inning. This number
is generally multiplied by 9 to yield what has been termed an “oﬀensive earned run
average.”
EXAMPLE 4
Table 4 shows the career batting statistics for Jose Oquendo, who
played for the New York Mets and St. Louis Cardinals in the 1980s and 1990s. Compute
his oﬀensive earned run average.
TABLE 4
Jose Oquendo Batting Statistics
Walks
Hit Batsman
Singles
Doubles
Triples
Home Runs
Outs
448
5
679
104
24
14
2381

10.6
Markov Chains and Baseball Statistics
C-61
SOLUTION
Construct the transition matrix from this data as described in Example 2,
and then compute M and SM: The sum of the ﬁrst column of M is 4:6052, so a team
entirely composed of Jose Oquendos would come to bat an average of 4:6052 times per
inning. That is, EŒB D 4:6052. The ﬁrst column of SM is
2
664
:2844
:3161
:2725
:1270
3
775
so the expected number of runners left on base is
EŒL D 0.:2844/ C 1.:3161/ C 2.:2725/ C 3.:1270/ D 1:2421
From Equation (6), the expected number of earned runs is
EŒR D EŒB   EŒL   3 D 4:6052   1:2421   3 D :3631
The oﬀensive earned run average for Jose Oquendo is :3631  9 D 3:2679. This com-
pares with an oﬀensive earned run average of about 10 for teams composed of the
greatest hitters in baseball history. See the Exercises.
.
Practice Problems
.
1. Let A be the matrix just before Example 1. Explain why entry .3; 6/ is zero.
2. Explain why entry .6; 3/ of A is :5p1:
10.6 Exercises
In Exercises 1–6, justify the transition probabilities for the given
initial states. See Example 1.
1. First base occupied
2. Third base occupied
3. First and second bases occupied
4. First and third bases occupied
5. Second and third bases occupied
6. First, second, and third bases occupied
7. Major League batting statistics for the 2006 season are shown
in Table 5. Compute the transition probabilities for this data
as was done in Example 2, and ﬁnd the matrix A for this data.
8. Find the complete transition matrix for the model using the
Major League data in Table 5.
9. It can be shown that the sum of the ﬁrst column of M for the
2006 Major League data is 4:53933, and that the ﬁrst column
of SM for the 2006 Major League data is
2
664
:34973
:33414
:23820
:07793
3
775
Find the expected number of earned runs per inning in a
Major League game in 2006.
10. The number of innings batted in the Major Leagues in the
2006 season was 43,257, and the number of earned runs
scored was 21,722. What is the total number of earned runs
scored for the season predicted by the model, and how does
it compare with the actual number of earned runs scored?
TABLE 5
Major League Batting Statistics---2006 Season
Walks
Hit Batsman
Singles
Doubles
Triples
Home Runs
Outs
15;847
1817
29;600
9135
952
5386
122;268

C-62
CHAPTER 10
Finite-State Markov Chains
TABLE 6
Batting Statistics for Leading Batters
Name
Walks
Hit Batsman
Singles
Doubles
Triples
Home Runs
Outs
Barry Bonds
2558
106
1495
601
77
762
6912
Babe Ruth
2062
43
1517
506
136
714
5526
Ted Williams
2021
39
1537
525
71
521
5052
11. Batting statistics for three of the greatest batters in Major
League history are shown in Table 6. Compute the transition
probabilities for this data for each player.
12. The sums of the ﬁrst columns of M for the player data in
Table 6 and the ﬁrst columns of SM for the player data in
Table 6 given in Table 7. Find and compare the oﬀensive
earned run averages of these players. Which batter does the
model say was the best of these three?
TABLE 7
Model Information for
Batting Statistics
Sum of First
First Column
Columns of M
of SM
Barry Bonds
5.43012
2
664
:281776
:292658
:258525
:167041
3
775
Babe Ruth
5.70250
2
664
:268150
:295908
:268120
:167822
3
775
Ted Williams
5.79929
2
664
:233655
:276714
:290207
:199425
3
775
13. Consider the second columns of the matrices M and SM,
which correspond to the “Runner on ﬁrst, none out” state.
a. What information does the sum of the second column of
M give?
b. What value can you calculate using the second column of
SM?
c. What would the calculation of expected runs scored using
the data from the second columns mean?
Exercises 14–18 show how the model for run production in the
text can be used to determine baseball strategy. Suppose that you
are managing a baseball team and have access to the matrices M
and SM for your team.
14. The sum of the column of M corresponding to the “Runner
on ﬁrst, none out” state is 4:53933, and the column of SM
corresponding to the “Runner on ﬁrst, none out” state is
2
664
:06107
:35881
:41638
:16374
3
775
Your team now has a runner on ﬁrst and no outs. How many
earned runs do you expect your team to score this inning?
15. The sum of the column of M corresponding to the “Runner
on second, none out” state is 4:53933, and the column of SM
corresponding to the “Runner on second, none out” state is
2
664
:06107
:47084
:34791
:12018
3
775
How many earned runs do you expect your team to score if
there is a runner on second and no outs?
16. The sum of the column of M corresponding to the “Bases
empty, one out” state is 3:02622, and the column of SM
corresponding to the “Bases empty, one out” state is
2
664
:48513
:31279
:16060
:04148
3
775
How many earned runs do you expect your team to score if
the bases are empty and there is one out?
17. Suppose that a runner for your team is on ﬁrst base with no
outs. You have to decide whether to tell the baserunner to
attempt to steal second base. If the steal is successful, there
will be a runner on second base and no outs. If the runner
is caught stealing, the bases will be empty and there will be
one out. Suppose further that the baserunner has a probability
of p D :8 of stealing successfully. Does attempting a steal in
this circumstance increase or decrease the number of earned
runs your team will score this inning?
18. In the previous exercise, let p be the probability that the
baserunner steals second base successfully. For which values
of p would you as manager call for an attempted steal?

10.6
Markov Chains and Baseball Statistics
C-63
..
Solutions to Practice Problems
.
1. For entry .3; 6/ of A, the probability of a transition from state 13:k to state 2:k is
required. Suppose that ﬁrst and third bases are occupied and that the batter does not
make an out. To leave a player on second base, the batter must hit a double and the
players on ﬁrst and third base must both reach home plate successfully. This cannot
happen according to the model, so the .3; 6/-entry is 0.
2. For entry .6; 3/ of A, the probability of a transition from state 2:k to state 13:k is
required. Suppose that only second base is occupied and that the batter does not
make an out. To leave players on ﬁrst base and on third base, the batter must get to
ﬁrst base, and the player on second base must advance to third. The desired outcome
occurs when the batter hits a single, but the runner from second base will then stop
at third base with probability .5. The .6; 3/-entry is thus :5p1.

Appendix 1
Proof of Theorem 1
Here is a restatement of Theorem 1, which will be proven in this appendix:
THEOREM 1
If P is a regular m  m transition matrix with m  2, then the following state-
ments are all true.
a. There is a stochastic matrix … such that lim
n!1 P n D ….
b. Each column of … is the same probability vector q.
c. For any initial probability vector x0, lim
n!1 P nx0 D q.
d. The vector q is the unique probability vector that is an eigenvector of P
associated with the eigenvalue 1.
e. All eigenvalues  of P other than 1 have jj < 1.
The proof of Theorem 1 requires creation of an order relation for vectors, and begins
with the consideration of matrices whose entries are strictly positive or nonnegative.
DEFINITION
If x and y are in Rm, then
a. x > y if xi > yi for i D 1; 2; : : : ; m.
b. x < y if xi < yi for i D 1; 2; : : : ; m.
c. x  y if xi  yi for i D 1; 2; : : : ; m.
d. x  y if xi  yi for i D 1; 2; : : : ; m.
DEFINITION
An m  n matrix A is positive if all its entries are positive. An m  n matrix A is
nonnegative if it has no negative entries.
Notice that all stochastic matrices are nonnegative. The row-vector rule (Section 1.3)
shows that multiplication of vectors by a positive matrix preserves order.
If A is a positive matrix and x > y; then Ax > Ay:
(1)
If A is a positive matrix and x  y; then Ax  Ay:
(2)
C-65

C-66
APPENDIX 1
Proof of Theorem 1
In addition, multiplication by nonnegative matrices “almost” preserves order in the
following sense.
If A is a nonnegative matrix and x  y; then Ax  Ay:
(3)
The ﬁrst step toward proving Theorem 1 is a lemma that shows how the transpose
of a stochastic matrix acts on a vector.
LEMMA 1
Let P be an m  m stochastic matrix, and let  be the smallest entry in P . Let a
be in Rm; let Ma be the largest entry in a, and let ma be the smallest entry in a.
Likewise, let b D P T a, let Mb be the largest entry in b, and let mb be the smallest
entry in b. Then ma  mb  Mb  Ma and
Mb   mb  .1   2/.Ma   ma/
PROOF
Create a new vector c from a by replacing every entry of a by Ma except for
one occurrence of ma. Suppose that this single ma entry lies in the ith row of c. Then
c  a. If the columns of P T are labeled q1, q2, : : : , qm, we have
P T c D
m
X
kD1
ckqk
D
m
X
kD1
Maqk   Maqi C maqi
Since P is a stochastic matrix, each row of P T sums to 1. If we let u be the vector in
Rm consisting of all 1’s, then
m
X
kD1
Maqk D Ma
m
X
kD1
qk D Mau, and
m
X
kD1
Maqk   Maqi C maqi D Mau   .Ma   ma/qi
Since each entry in P (and thus P T ) is greater than or equal to , qi  u, and
Mau   .Ma   ma/qi  Mau   .Ma   ma/u D .Ma   .Ma   ma//u
So
P T c  .Ma   .Ma   ma//u
But since a  c and P T is nonnegative, Equation (3) gives
b D P T a  P T c  .Ma   .Ma   ma//u
Thus each entry in b is less than or equal to Ma   .Ma   ma/. In particular,
Mb  Ma   .Ma   ma/
(4)
So Mb  Ma. If we now examine the vector  a, we ﬁnd that the largest entry in  a
is  ma, the smallest is  Ma, and similar results hold for  b D P T. a/. Applying
Equation (4) to this situation gives
 mb   ma   . ma C Ma/
(5)

APPENDIX 1
Proof of Theorem 1
C-67
so mb  ma. Adding Equations (4) and (5) together gives
Mb   mb  Ma   ma   2.Ma   ma/
D .1   2/.Ma   ma/
Proof of Theorem 1
First assume that the transition matrix P is a positive stochastic
matrix. As above, let  > 0 be the smallest entry in P . Consider the vector ej where
1  j  m. Let Mn and mn be the largest and smallest entries in the vector .P T /nej .
Since .P T /nej D P T .P T /n 1ej , Lemma 1 gives
Mn   mn  .1   2/.Mn 1   mn 1/
(6)
Hence, by induction, it may be shown that
Mn   mn  .1   2/n.M0   m0/ D .1   2/n
Since m  2, 0 <   1=2. Thus 0  1   2 < 1, and lim
n!1.Mn   mn/ D 0. Therefore
the entries in the vector .P T /nej approach the same value, say qj , as n increases. Notice
that since the entries in P T are between 0 and 1, the entries in .P T /nej must also be
between 0 and 1, and so qj must also lie between 0 and 1. Now .P T /nej is the j th
column of .P T /n, which is the j th row of P n. Therefore P n approaches a matrix all
of whose rows are constant vectors, which is another way of saying the columns of P n
approach the same vector q:
lim
n!1 P n D … D
 q
q
  
q 
D
2
6664
q1
q1
  
q1
q2
q2
  
q2
:::
:::
:::
:::
qm
qm
  
qm
3
7775
So Theorem 1(a) is true if P is a positive matrix. Suppose now that P is regular but
not positive; since P is regular, there is a power P k of P that is positive. We need to
show that lim
n!1.Mn   mn/ D 0; the remainder of the proof follows exactly as above. No
matter the value of n, there is always a multiple of k, say rk, with rk < n  r.k C 1/.
By the proof above, lim
r!1.Mrk   mrk/ D 0. But Equation (6) applies equally well to
nonnegative matrices, so 0  Mn   mn  Mrk   mrk, and lim
n!1 Mn   mn D 0, proving
part (a) of Theorem 1.
To prove part (b), it suﬃces to show that q is a probability vector. To see this, note
that since .P T /n has row sums equal to 1 for any n, .P T /nu D u. Since lim
n!1.P T /n D
…T , it must be the case that …T u D u. Thus the rows of …T , and so also the columns
of …, must sum to 1 and q is a probability vector.
The proof of part (c) follows from the deﬁnition of matrix multiplication and the
fact that P n approaches … by part (a). Let x0 be any probability vector. Then
lim
n!1 P nx0 D lim
n!1 P n.x1e1 C : : : C xmem/
D x1. lim
n!1 P ne1/ C : : : C xm. lim
n!1 P nem/
D x1.…e1/ C : : : C xm.…em/ D x1q C : : : C xmq
D .x1 C : : : C xm/q D q
since the entries in x0 sum to 1.

C-68
APPENDIX 1
Proof of Theorem 1
To prove part (d), we calculate P …. First note that lim
n!1 P nC1 D …. But since
P nC1 D PP n, and lim
n!1 P n D …, lim
n!1 P nC1 D P …. Thus P … D …, and any column
of this matrix equation gives P q D q. Thus q is a probability vector that is also an
eigenvector for P associated with the eigenvalue  D 1. To show that this vector is
unique, let v be any eigenvector for P associated with the eigenvalue  D 1, which
is also a probability vector. Then P v D v, and P nv D v for any n. But by part (c),
lim
n!1 P nv D q, which can happen only if v D q. Thus q is unique. Note that this part of
the proof has also shown that the eigenspace associated with the eigenvalue  D 1 has
dimension 1.
To prove part (e), let  ¤ 1 be an eigenvalue of P , and let x be an associated
eigenvector. Assume that
m
X
kD1
xk ¤ 0. Since any nonzero scalar multiple of x will also
be an eigenvector associated with , we may scale the eigenvector x by the reciprocal
of
m
X
kD1
xk to form the eigenvector w. Notice that the sum of the entries in w is 1. Then
P w D w, so P nw D nw for any n. By the proof of part (c), lim
n!1 P nw D q since the
entries in w sum to 1. Thus
lim
n!1 nw D q
(7)
Notice that Equation (7) can be true only if  D 1. If jj  1 and  ¤ 1, the left side of
Equation (7) diverges; if jj < 1, the left side of Equation (7) must converge to 0 ¤ q.
This contradicts our assumption, so it must be the case that
m
X
kD1
wk D 0. By part (a),
lim
n!1 P nw D …w. Since
…w D
 q
q
  
q 
w
D w1q C w2q C    C wmq
D .w1 C w2 C    C wm/q D 0q D 0
then lim
n!1 P nw D 0. Since P nw D nw and w ¤ 0, lim
n!1 n D 0, and jj < 1.

Appendix 2
Probability
The purpose of this appendix is to provide some information from probability theory
that can be used to develop a formal deﬁnition of a Markov chain and to prove some
results from Chapter 10.
Probability
DEFINITION
For each event E of the sample space S, the probability of E (denoted P.E/) is
a number that has the following three properties:
a. 0  P.E/  1
b. P.S/ D 1
c. For any sequence of mutually exclusive events E1; E2; : : : ,
P
 1
[
nD1
En
!
D
1
X
nD1
P.En/
Properties of Probability
1. P.;/ D 0
2. P.Ec/ D 1   P.E/
3. P.E [ F / D P.E/ C P.F /   P.E \ F /
4. If E and F are mutually exclusive events, P.E [ F / D P.E/ C P.F /
DEFINITION
The conditional probability of E given F (denoted P.EjF /), is the probability
that E occurs given that F has occurred, is
P.EjF / D P.E \ F /
P.F /
C-69

C-70
APPENDIX 2
Probability
Law of Total Probability
Let F1; F2; : : : be a sequence of mutually exclusive events for which
1
[
nD1
Fn D S
Then for any event E in the sample space S,
P.E/ D
1
X
nD1
P.EjFn/P.Fn/
Random Variables and Expectation
DEFINITION
A random variable is a real-valued function deﬁned on the sample space S. A
discrete random variable is a random variable that takes on at most a countable
number of possible values.
Only discrete random variables will be considered in this text; random variables that take
on an uncountably inﬁnite set of values are considered in advanced courses in probability
theory. In Section 10.3, the expected value of a discrete random variable was deﬁned.
The expected value of a discrete random variable may also be deﬁned using a function
called the probability mass function.
DEFINITION
The probability mass function p of a discrete random variable X is the real-valued
function deﬁned by p.a/ D P.X D a/.
DEFINITION
The expected value of a discrete random variable X is
EŒX D
X
x
xp.x/
where the sum is taken over all x with p.x/ > 0.
Notice that if the random variable takes on the values x1; x2; : : : with positive probability,
then the expected value of the random variable is
X
x
xp.x/ D x1P.X D x1/ C x2P.X D x2/ C   
that matches the deﬁnition of expected value given in Section 10.3. Using the deﬁnition
above, it is straightforward to show that expected value has the following properties.

APPENDIX 2
Probability
C-71
Properties of Expected Value
For any real constant k and any discrete random variables X and Y ,
1. EŒkX D kEŒX
2. EŒX C k D EŒX C k
3. EŒX C Y  D EŒX C EŒY 
4. If f is a real-valued function, then f .X/ is a discrete random variable, and
EŒf .X/ D P
x f .x/p.x/, where the sum is taken over all x with p.x/ > 0.
Just as probabilities can be aﬀected by whether an event occurs, so can expected values.
DEFINITION
Let X be a discrete random variable and let F be an event in the sample space S.
Then the conditional expected value of X given F is
EŒXjF  D
X
x
xP.X D xjF /
where the sum is taken over all x with p.x/ > 0.
There is a law of total probability for expected value that will be used to prove a result
from Chapter 10. Its statement and its proof follow.
Law of Total Probability for Expected Value
Let F1; F2; : : : be a sequence of mutually exclusive events for which
1
[
nD1
Fn D S
Then, for any discrete random variable X,
EŒX D
1
X
nD1
EŒXjFnP.Fn/
PROOF
Let F1; F2; : : : be a sequence of mutually exclusive events for which S1
nD1Fn D
S, and let X be a discrete random variable. Then, using the deﬁnition of expected value
and the law of total probability,
EŒX D
X
x
xp.x/ D
X
x
xP.X D x/
D
X
x
x
1
X
nD1
P.X D xjFn/P.Fn/
D
1
X
nD1
P.Fn/
X
x
xP.X D xjFn/
D
1
X
nD1
EŒXjFnP.Fn/

C-72
APPENDIX 2
Probability
Markov Chains
In Section 5.9, a Markov chain was deﬁned as a sequence of vectors. In order to un-
derstand Markov chains from a probabilistic standpoint, it is better to deﬁne a Markov
chain as a sequence of random variables. To begin, consider any collection of random
variables. This is called a stochastic process.
DEFINITION
A stochastic process fXn W n 2 T g is a collection of random variables.
Notes:
1. The set T is called the index set for the stochastic process. The only set T that
need be considered for this appendix is T D f0; 1; 2; 3; : : :g, so the stochastic process
can be described as the sequence of random variables fX0; X1; X2; : : :g. When T D
f0; 1; 2; 3; : : :g, the index is often identiﬁed with time and the stochastic process is
called a discrete-time stochastic process. The random variable Xk is understood to
be the stochastic process at time k.
2. It is assumed that the random variables in a stochastic process have a common range.
This range is called the state space for the stochastic process. The state spaces in
Chapter 10 are all ﬁnite, so the random variables Xk are all discrete random variables.
If Xk D i, we will say that i is the state of the process at time k, or that the process
is in state i at time (or step) k.
3. Notice that a stochastic process can be used to model movement between the states
in the state space. For some element ! in the sample space S, the sequence fX0.!/,
X1.!/; : : :g will be a sequence of states in the state space—a sequence that will
potentially be diﬀerent for each element in S. Usually the dependence on the sample
space is ignored and the stochastic process is treated as a sequence of states, and the
process is said to move (or transition) between those states as time proceeds.
4. Since a stochastic process is a sequence of random variables, the actual state that the
process occupies at any given time cannot be known. The goal therefore is to ﬁnd the
probability that the process is in a particular state at a particular time. This amounts
to ﬁnding the probability mass function of each random variable Xk in the sequence
that is the stochastic process.
5. When a discrete-time stochastic process has a ﬁnite state space, the probability mass
function of each random variable Xk can be expressed as a probability vector xk.
These probability vectors were used to deﬁne a Markov chain in Section 5.9.
In order for a discrete-time stochastic process fX0; X1; X2; : : :g to be a Markov chain,
the state of the process at time n C 1 can depend only on the state of the process at
time n. This is in contrast with a more general stochastic process, whose state at time
n could depend on the entire history of the process. In terms of conditional probability,
this property is
P.XnC1 D ijX0 D j0; X1 D j1; : : : ; Xn D j/ D P.XnC1 D ijXn D j /
The probability on the right side of this equation is called the transition probability from
state j to state i. In general, this transition probability can change depending on the
time n. This is not the case for Markov chains considered in Chapter 10: the transition
probabilities do not change with time, so the transition probability from state j to state
i is
P.XnC1 D ijXn D j/ D pij

APPENDIX 2
Probability
C-73
A Markov chain with constant transition probabilities is called a time-homogeneous
Markov chain. Thus its deﬁnition is as follows.
DEFINITION
A time-homogeneous Markov chain is a discrete-time stochastic process whose
transition probabilities satisfy
P.XnC1 D ijX0 D j0; X1 D j1; : : : ; Xn D j/ D P.XnC1 D ijXn D j/ D pij
for all times n and for all states i and j .
Using this deﬁnition, it is clear that, if the number of states is ﬁnite, then a transition
matrix can be constructed that has the properties assumed in Section 10.1.
Proofs of Theorems
Mean Return Times
Theorem 3 in Section 10.3 connected the steady-state vector for a Markov chain with
the mean return time to a state of the chain. Here is a statement of this theorem and a
proof that relies on the law of total probability for expected value.
THEOREM 3
Let Xn, n D 1; 2; : : : be an irreducible Markov chain with ﬁnite state space S. Let
nij be the number of steps until the chain ﬁrst visits state i given that the chain
starts in state j , and let tii D EŒnii. Then
tii D 1
qi
where qi is the entry in the steady-state vector q corresponding to state i.
PROOF
To ﬁnd an expression for tii, ﬁrst produce an equation involving tij by con-
sidering the ﬁrst step of the chain X1. There are two possibilities: either X1 D i or
X1 D k ¤ i. If X1 D i, then it took exactly one step to visit state i and
EŒnij jX1 D i D 1
If X1 D k ¤ i, the chain will take one step to reach state k, and then the expected number
of steps the chain will make to ﬁrst visit state i will be EŒnik D tik. Thus
EŒnij jX1 D k ¤ i D 1 C tik
By the law of total probability for expected value,
tij D EŒnij 
D
X
k2S
EŒnij jX1 D kP.X1 D k/
D EŒnij jX1 D iP.X1 D i/ C
X
k¤i
EŒnij jX1 D kP.X1 D k/

C-74
APPENDIX 2
Probability
D 1  pij C
X
k¤i
.1 C tik/pkj
D pij C
X
k¤i
pkj C
X
k¤i
tikpkj
D 1 C
X
k¤i
tikpkj
D 1 C
X
k2S
tikpkj   tiipij
Let T be the matrix whose .i; j /-element is tij , and let D be the diagonal matrix whose
diagonal entries are tii. Then the ﬁnal equality above may be written as
Tij D 1 C .TP /ij   .DP /ij
(1)
If U is an appropriately sized matrix of ones, Equation (1) can be written in matrix form
as
T D U C TP   DP D U C .T   D/P
(2)
Multiplying each side of Equation (2) by the steady-state vector q and recalling that
P q D q gives
T q D U q C .T   D/P q D U q C .T   D/q D U q C T q   Dq
so
U q D Dq
(3)
Consider the entries in each of the vectors in Equation (3). Since U is a matrix of all 1’s,
U q D
2
6664
1
1
  
1
1
1
  
1
:::
:::
:::
:::
1
1
  
1
3
7775
2
6664
q1
q2
:::
qn
3
7775 D
2
6664
Pn
kD1 qk
Pn
kD1 qk
:::
Pn
kD1 qk
3
7775 D
2
6664
1
1
:::
1
3
7775
since q is a probability vector. Likewise,
Dq D
2
6664
t11
0
  
0
0
t22
  
0
:::
:::
:::
:::
0
0
  
tnn
3
7775
2
6664
q1
q2
:::
qn
3
7775 D
2
6664
t11q1
t22q2
:::
tnnqn
3
7775
Equating corresponding entries in U q and Dq gives tiiqi D 1, or
tii D 1
qi
Periodicity as a Class Property
In Section 10.4 it was stated that if two states belong to the same communication class,
then their periods must be equal. A proof of this result follows.

APPENDIX 2
Probability
C-75
THEOREM
Let i and j be two states of a Markov chain that are in the same communication
class. Then the periods of i and j are equal.
PROOF
Suppose that i and j are in the same communication class for the Markov chain
X, that state i has period di, and that state j has period dj . To simplify the exposition
of the proof, the notation .ar/ij will be used to refer to the .i; j /-entry in the matrix Ar.
Since i and j are in the same communication class, there exist positive integers m and
n such that .pm/ji > 0 and .pn/ij > 0. Let k be a positive integer such that .pk/jj > 0.
In fact, .plk/jj > 0 for all integers l > 1. Now .pnClkCm/ii > .pn/ij .plk/jj .pm/ji > 0
for all integers l > 1, since a loop from state i to state i in n C lk C m steps may be
created in many ways, but one way is to proceed from state i to state j in n steps, then to
loop from state j to state j l times using a loop of k steps each time, and then to return
to state i in m steps. Since di is the period of state i, di must divide n C lk C m for
all integers l > 1. So di divides n C k C m and n C 2k C m, and so divides .n C 2k C
m/   .n C k C m/ D k. Thus di is a common divisor of the set of all time steps k such
that .pk/jj > 0. Since dj is the greatest common divisor of the set of all time steps k
such that .pk/jj > 0, di  dj . A similar argument shows that di  dj , so di D dj .
The Fundamental Matrix
In Section 10.5, the number of visits vij to a transient state i that a Markov chain makes
starting at the transient state j was studied. Speciﬁcally, the expected value EŒvij 
was computed, and the fundamental matrix was deﬁned as the matrix whose .i; j/-
element is mij D EŒvij . The following theorem restates Theorem 6 in Section 10.5
in an equivalent form and provides a proof that relies on the law of total probability for
expected value.
THEOREM 6
Let j and i be transient states of a Markov chain, and let Q be that portion of
the transition matrix that governs movement between transient states. Let vij be
the number of visits that the chain will make to state i given that the chain starts
in state j , and let mij D EŒvij . Then the matrix M whose .i; j /-element is mij
satisﬁes the equation
M D .I   Q/ 1
PROOF
We produce an equation involving mij by conditioning on the ﬁrst step of the
chain X1. We consider two cases: i ¤ j and i D j . First assume that i ¤ j and suppose
that X1 D k. Then we see that
EŒvij jX1 D k D EŒvik
(4)
if i ¤ j . Now assume that i D j . Then the previous analysis is valid, but we must add
one visit to i since the chain was at state i at time 0. Thus
EŒviijX1 D k D 1 C EŒvik
(5)
We may combine Equations (4) and (5) by introducing the following symbol, called the
Kronecker delta:
ıij D
(
1
if i D j
0
if i ¤ j

C-76
APPENDIX 2
Probability
Notice that ıij is the .i; j /-element in the identity matrix I. We can write Equations (4)
and (5) as
EŒvij jX1 D k D ıij C EŒvik
Thus, by the law of total probability for expected value,
mij D EŒvij 
D
X
k2S
EŒvij jX1 D kP.X1 D k/
D
X
k2S
.ıij C EŒvik/P.X1 D k/
D ıij
X
k2S
P.X1 D k/ C
X
k2S
EŒvikP.X1 D k/
D ıij C
X
k2S
EŒvikP.X1 D k/
Now note that if k is a recurrent state, then EŒvik D 0. Thus we only need to sum over
transient states of the chain:
mij D ıij C
X
k transient
EŒvikP.X1 D k/
D ıij C
X
k transient
mikqkj
since j and k are transient states and Q is deﬁned in the statement of the theorem. We
may write the ﬁnal equality above as
mij D Iij C .MQ/ij
or in matrix form as
M D I C MQ
(6)
We may rewrite Equation (6) as
M   MQ D M.I   Q/ D I
so .I   Q/ is invertible by the Invertible Matrix Theorem, and M D .I   Q/ 1.
Absorption Probabilities
In Section 10.5, the probability that the chain was absorbed into a particular absorbing
state was studied. The Markov chain was assumed to have only transient and absorbing
states, j is a transient state, and i is an absorbing state of the chain. The probability aij
that the chain is absorbed at state i given that the chain starts at state j was calculated,
and it was shown that the matrix A whose .i; j /-element is aij satisﬁes A D SM, where
M is the fundamental matrix and S is that portion of the transition matrix that governs
movement from transient states to absorbing states. The following theorem restates this
result, which was presented as Theorem 7 in Section 10.5. An alternative proof of this
result is given that relies on the law of total probability.

APPENDIX 2
Probability
C-77
THEOREM 7
Consider a Markov chain with ﬁnite state space whose states are either absorbing
or transient. Suppose that j is a transient state and that i is an absorbing state of
the chain, and let aij be the probability that the chain is absorbed at state i given
that the chain starts in state j . Let A be the matrix whose .i; j /-element is aij .
Then A D SM, where S and M are as deﬁned above.
PROOF
We again consider the ﬁrst step of the chain X1. Let X1 D k. There are three
possibilities: k could be a transient state, k could be i, and k could be an absorbing state
unequal to i. If k is transient, then
P.absorption at ijX1 D k/ D aik
If k D i, then
P.absorption at ijX1 D k/ D 1
while if k is an absorbing state other than i,
P.absorption at ijX1 D k/ D 0
By the law of total probability,
aij D P.absorption at i/
D
X
k
P.absorption at ijX1 D k/P.X1 D k/
D 1  P.X1 D i/ C
X
k transient
P.absorption at ijX1 D k/P.X1 D k/
D pij C
X
k transient
aikpkj
Since j is transient and i is absorbing, pij D sij . Since in the ﬁnal sum j and k are both
transient, pkj D qkj . Thus the ﬁnal equality may be written as
aij D sij C
X
k transient
aikqkj
D sij C .AQ/ij
or, in matrix form, as
A D S C AQ
This equation may be solved for A to ﬁnd that A D SM.

Chapter 10:
Answers to Selected Exercises
Chapter 10
Section 10.1, page C-9
1. a. Stochastic.
b. Not stochastic. Columns do not sum to 1.
3. x3 D
 :556
:444

5. 109=216
7. 13=36
9. a. :53125
b. 0
11. a. 5=8
b. 1=8
13. P D
2
66664
0
1=3
0
1=2
1=2
1=3
0
1=2
0
1=2
0
1=3
0
1=2
0
1=3
0
1=2
0
0
1=3
1=3
0
0
0
3
77775
15. P D
2
664
0
0
1
0
1=3
0
0
1=2
1=3
0
0
1=2
1=3
1
0
0
3
775
17. a. P D
2
66664
0
1=3
1=4
1=3
0
1=3
0
1=4
0
1=3
1=3
1=3
0
1=3
1=3
1=3
0
1=4
0
1=3
0
1=3
1=4
1=3
0
3
77775
, x0 D
2
66664
0
1
0
0
0
3
77775
b. x3 D
2
66664
:25926
:11111
:25926
:11111
:25926
3
77775
19. a. P D
2
6666664
0
1=3
0
0
0
0
1=2
0
1=2
0
1=3
0
0
1=3
0
0
0
0
1=2
0
0
0
1=3
0
0
1=3
0
1
0
0
0
0
1=2
0
1=3
1
3
7777775
, x0 D
2
6666664
1
0
0
0
0
0
3
7777775
b. x4 D
2
6666664
:12963
0
:12963
0
:43518
:30556
3
7777775
21. True.
22. False. The columns of a transition matrix for a Markov
chain must sum to 1.
23. False. The transition matrix P cannot change over time.
24. True.
25. True.
26. False. The .i; j /-entry in matrix P 3 gives the probability of
a move from state j to state i in exactly three moves.
27. Sunny with probability :406, cloudy with probability
:145375, rainy with probability :448625.
29. x3 D
2
664
1=6
5=18
5=18
5=18
3
775
C-79

C-80
Answers to Exercises
31. a. P D
2
664
p2
p.1   p/
p.1   p/
.1   p/2
p.1   p/
p2
.1   p/2
p.1   p/
p.1   p/
.1   p/2
p2
p.1   p/
.1   p/2
p.1   p/
p.1   p/
p2
3
775
b. :94206
33. a. P D
2
666666664
1   p
p=6
0
0
0
0
0
p
1   p
p=3
0
0
0
0
0
5p=6
1   p
p=2
0
0
0
0
0
2p=3
1   p
2p=3
0
0
0
0
0
p=2
1   p
5p=6
0
0
0
0
0
p=3
1   p
p
0
0
0
0
0
p=6
1   p
3
777777775
b. :19290
35. a. P D
2
66664
0
1   p
p
0
0
p
0
0
0
0
1   p
0
0
0
0
0
p
0
1
0
0
0
1   p
0
1
3
77775
b. :192
37. Suppose that P is an m  m stochastic matrix all of whose entries are greater than or equal to p. The proof proceeds by induction.
Notice that the statement to be proven is thus true for n D 1. Assume the statement is true for n, and let B D P n. Then, since
P nC1 D BP , the .i; j /-entry in P nC1 is
m
X
kD1
bikpkj . Since bik  p by the induction hypothesis,
m
X
kD1
bikpkj  p
m
X
kD1
pkj . But
m
X
kD1
pkj D 1 since P is a stochastic matrix, so all of the entries in P nC1 are greater than or equal to p.
Section 10.2, page C-21
1. P 10 D
 :33333
:33333
:66667
:66667

q D
 :33333
:66667

The probability is :33333.
3. P 20 D
2
4
:21429
:21429
:21429
:57143
:57143
:57143
:21429
:21429
:21429
3
5
q D
2
4
3=14
4=7
3=14
3
5 
2
4
:21429
:57143
:21429
3
5
The probability is :21429.
5.
 8=17
8=17
9=17
9=17

7. P is regular since all entries in P 2 are positive.
9. a. The transition matrix is
P D
2
66664
0
1=4
0
0
0
1
0
1=2
0
0
0
3=4
0
3=4
0
0
0
1=2
0
1
0
0
0
1=4
0
3
77775
The .0; 0/-entry in P k will be zero if k is odd while the
.0; 1/-entry in P k will be zero if k is even. Thus P is
not regular.
b. Compute that
q D
2
66664
1=16
1=4
3=8
1=4
1=16
3
77775
so the chain will spend the most steps in state 2, which
corresponds to both urns containing 2 molecules.
11. a. The transition matrix is
P D
2
664
0
1=2
0
0
1
0
1=2
0
0
1=2
0
1
0
0
1=2
0
3
775
The .1; 1/-entry in P k will be zero if k is odd while the
.1; 2/-entry in P k will be zero if k is even. Thus P is
not regular.
b. Compute that
q D
2
664
1=6
1=3
1=3
1=6
3
775
so the chain will spend the most steps in states 2 and 3.
13. q D
2
664
1=4
1=4
1=4
1=4
3
775
15. q D
2
664
3=13
3=13
3=13
4=13
3
775

Section 10.2
C-81
17. Since q D
2
66664
:1875
:1875
:25
:1875
:1875
3
77775
, the probability is :25.
19. Since
P q D
2
6666664
0
1=3
0
0
0
0
1=2
0
1=2
0
1=3
0
0
1=3
0
0
0
0
1=2
0
0
0
1=3
0
0
1=3
0
1
0
0
0
0
1=2
0
1=3
1
3
7777775
2
6666664
0
0
0
0
0
1
3
7777775
D
2
6666664
0
0
0
0
0
1
3
7777775
;
q is a steady-state vector for the Markov chain. Room 6 is
an absorbing state for the chain—once the mouse moves
into room 6 it will stay there forever.
21. True.
22. False. See Examples 4 and 5.
23. True.
24. True.
25. False. See Examples 4 and 5.
26. True.
27. To the nearest day, 152 are sunny, 52 are cloudy, and 161
are rainy.
29. The Google matrix and its steady-state vector are
G D
2
66664
:03
:03
:88
:03
:2
:313333
:03
:03
:455
:2
:313333
:03
:03
:455
:2
:313333
:455
:03
:03
:2
:03
:455
:03
:03
:2
3
77775
;
q D
2
66664
:231535
:208517
:208517
:208517
:142915
3
77775
and the PageRanks are 1; 2, 3, and 4 (tied); 5.
31. a. If a dominant (AA) individual is mated with a hybrid
(Aa), then the dominant individual will always
contribute an A. One half of the time the hybrid will also
contribute an A, leading to a dominant oﬀspring. The
other half of the time, the hybrid will contribute an a,
yielding a hybrid oﬀspring.
b. If a recessive (aa) individual is mated with a hybrid
(Aa), then the recessive individual will always
contribute an a. One half of the time the hybrid will also
contribute an a, leading to a recessive oﬀspring. The
other half of the time, the hybrid will contribute an A,
yielding a hybrid oﬀspring.
c. If a hybrid (Aa) is mated with another hybrid (Aa), then
a dominant oﬀspring will result when both hybrids
contribute an A, which happens .1=2/.1=2/ D 1=4 of
the time. Likewise a recessive oﬀspring will result when
both hybrids contribute an a, which also happens
.1=2/.1=2/ D 1=4 of the time. Finally, in all other
cases, a hybrid oﬀspring will be produced, which
happens 1   1=4   1=4 D 1=2 of the time.
33. a. Conﬁrm that all entries in P 6 are strictly positive.
b. Compute that
q D
2
666666664
1=64
3=32
15=64
5=16
15=64
3=32
1=64
3
777777775
so the chain spends the most steps in state 3, which corresponds to both urns containing 3 molecules. The fraction of steps the
chain spends there is 5=16.
c. Compute that
P q D
2
666666664
1   p
p=6
0
0
0
0
0
p
1   p
p=3
0
0
0
0
0
5p=6
1   p
p=2
0
0
0
0
0
2p=3
1   p
2p=3
0
0
0
0
0
p=2
1   p
5p=6
0
0
0
0
0
p=3
1   p
p
0
0
0
0
0
p=6
1   p
3
777777775
2
666666664
1=64
3=32
15=64
5=16
15=64
3=32
1=64
3
777777775
D
2
666666664
1=64
3=32
15=64
5=16
15=64
3=32
1=64
3
777777775
D q
so the result of part (b) does not depend on the value of p.

C-82
Answers to Exercises
35. Compute that
2
66664
1
1=2
0
0
0
0
0
1=2
0
0
0
1=2
0
1=2
0
0
0
1=2
0
0
0
0
0
1=2
1
3
77775
2
66664
q
0
0
0
1   q
3
77775
D
2
66664
q
0
0
0
1   q
3
77775
37. Compute that
2
66664
1=4
1=3
1=2
0
0
1=4
1=3
1=4
0
0
1=2
1=3
1=4
0
0
0
0
0
1=3
3=4
0
0
0
2=3
1=4
3
77775
2
66664
4=11
3=11
4=11
0
0
3
77775
D
2
66664
4=11
3=11
4=11
0
0
3
77775
and
2
66664
1=4
1=3
1=2
0
0
1=4
1=3
1=4
0
0
1=2
1=3
1=4
0
0
0
0
0
1=3
3=4
0
0
0
2=3
1=4
3
77775
2
66664
0
0
0
9=17
8=17
3
77775
D
2
66664
0
0
0
9=17
8=17
3
77775
If the chain is equally likely to begin in each of the states,
then it begins in state 1, 2, or 3 with probability 3=5, and in
state 4 or 5 with probability 2=5. Since
3
5
2
66664
4=11
3=11
4=11
0
0
3
77775
C 2
5
2
66664
0
0
0
9=17
8=17
3
77775
D
2
66664
12=55
9=55
12=55
18=85
16=85
3
77775
the probability of the chain being in state 1 after many steps
is 12=55.
39. a. The matrix P will be a stochastic matrix if p C q  1. It
will be a regular stochastic matrix if in addition p ¤ 1
and q ¤ 1.
b. A steady-state vector for P is
2
4
1=3
1=3
1=3
3
5
41. a. Let v be an eigenvector of P associated with  D 1. Let
P v D y. Then, by Exercise 40,
jy1j C : : : C jymj  jv1j C : : : C jvmj
But P v D v, so y D v and
jy1j C : : : C jymj D jv1j C : : : C jvmj
Since equality holds, each nonzero entry in v must have
the same sign by Exercise 40.
b. By part (a), each nonzero entry in v must have the same
sign. Since v is an eigenvector, v ¤ 0 and so must have
at least one nonzero entry. Thus the sum of the entries in
v will not be zero, so one may deﬁne
1
v1 C    C vm
v
This vector will also be an eigenvector of P associated
with  D 1, each entry in this vector will be
nonnegative, and the sum of the entries in this vector
will be 1. It is thus a steady-state vector for P .
43. a. Since x0 D c1q C c2v2 C    C cnvn and 1 D 1,
Equation (2) indicates that
xk D P kx0 D c1q C c2k
2v2 C    C cnk
nvn
b. By part (a),
xk   c1q D c2k
2v2 C    C cnk
nvn
and xk ! c1q since jij < 1. Since j2j is the largest
magnitude eigenvalue remaining, the c2k
2v2 will be the
largest of the error terms and will thus govern how
quickly fxkg converges to c1q.
Section 10.3, page C-31
1. f1; 3g, f2g; reducible
3. f1g, f2g, f3g; reducible
5. f1; 3; 5g, f2; 4; 6g; reducible
7. f1; 2; 3; 4; 5g, f6g; reducible
9. f1; 2; 3; 4g, f5g
11. f1; 2; 3; 4g; irreducible
13. Every state is reachable from every other state in two steps
or fewer, so the Markov chain is irreducible. The return
times are
State 1: 4
State 2: 4
State 3: 6
State 4: 6
State 5: 6
15. Every state is reachable from every other state in three steps
or fewer, so the Markov chain is irreducible. The return
times are
State 1: 13/3
State 2: 13/3
State 3: 13/3
State 4: 13/4
17. 4 steps.
19. 15=2 steps.
21. False. It must also be possible to go from state j to state i in
a ﬁnite number of steps for states i and j to communicate
with each other.
22. False. See Example 2.
23. True.
24. True.
25. False. The reciprocals of the entries in the steady-state
vector are the return times for each state.
26. True.
27. 2:27368 days.

Section 10.4
C-83
29. a. Since each entry in G is positive, the Markov chain is
irreducible.
b. 4:31901 mouse clicks.
31. 8=3 steps.
33. 16=5 D 3:2 draws.
35. fdeuce, advantage A, advantage Bg, fA wins the gameg, fB
wins the gameg
37. Every state is reachable from every other state in three steps
or fewer, so the Markov chain is irreducible.
39. Each dangling node forms a separate communication class
for the Markov chain.
Section 10.4, page C-40
1. The communication classes are f1; 3g and f2g. Class f1; 3g
is transient while class f2g is recurrent. All classes have
period 1.
3. The communication classes are f1g, f2g, and f3g. Class f1g
is recurrent while classes f2g and f3g are transient. All
classes have period 1.
5. The communication classes are f1; 3; 5g and f2; 4; 6g. Both
classes are recurrent and have period 2.
7. The communication class is f1; 2; 3; 4g, which must be
recurrent. The class has period 4.
9. The communication classes are f1; 2; 3; 4g, which is
transient, and f5g, which is recurrent. Both classes have
period 1.
11. Ordering the states 2; 1; 3 gives the matrix
2
4
1
1=2
1=3
0
1=4
1=3
0
1=4
1=3
3
5
13. The matrix is already in canonical form.
15. Ordering the states 1; 3; 5; 2; 4; 6 gives the matrix
2
6666664
0
:4
:8
0
0
0
:3
0
:2
0
0
0
:7
:6
0
0
0
0
0
0
0
0
:7
:5
0
0
0
:1
0
:5
0
0
0
:9
:3
0
3
7777775
17. The original transition matrix is
2
66664
1=3
0
1
0
0
1=3
0
0
1=2
0
1=3
0
0
1=2
0
0
1=2
0
0
0
0
1=2
0
0
1
3
77775
Ordering the states 5; 1; 2; 3; 4 gives the matrix
2
66664
1
0
1=2
0
0
0
1=3
0
1
0
0
1=3
0
0
1=2
0
1=3
0
0
1=2
0
0
1=2
0
0
3
77775
19. a. The communication classes are f1; 2; 3; 4; 5g and f6g.
Class f1; 2; 3; 4; 5g is transient while class f6g is
recurrent.
b. Both classes have period 1.
c. The original transition matrix is
2
6666664
0
1=3
0
0
0
0
1=2
0
1=2
0
1=3
0
0
1=3
0
0
0
0
1=2
0
0
0
1=3
0
0
1=3
0
1
0
0
0
0
1=2
0
1=3
1
3
7777775
Ordering the states 6; 1; 2; 3; 4; 5 gives the matrix
2
6666664
1
0
0
1=2
0
1=3
0
0
1=3
0
0
0
0
1=2
0
1=2
0
1=3
0
0
1=3
0
0
0
0
1=2
0
0
0
1=3
0
0
1=3
0
1
0
3
7777775
21. False. A Markov chain can have more than one recurrent
class.
22. True.
23. True.
24. True.
25. False. Every Markov chain must have a recurrent class.
26. False. A Markov chain can have more than one recurrent
class.
27. It is easy to compute that q D
2
664
1=4
1=4
1=4
1=4
3
775 for the matrix P .
We further ﬁnd that
P 2 D
2
664
0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0
3
775;
P 3 D
2
664
0
1
0
0
0
0
1
0
0
0
0
1
1
0
0
0
3
775;
and
P 4 D
2
664
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
3
775 D I
by direct computation. Thus P 5 D P , P 6 D P 2, P 7 D P 3,
P 8 D P 4 D I, and so on. So no matter the value of n, one
of the four matrices P nC1, P nC2, P nC3, and P nC4 will be
P , one will be P 2, one will be P 3, and one will be P 4 D I.
Therefore

C-84
Answers to Exercises
lim
n!1
1
4

P nC1 C P nC2 C P nC3 C P nC4
D 1
4
 P C P 2 C P 3 C P 4
D 1
4
0
BB@
2
664
0
0
0
1
1
0
0
0
0
1
0
0
0
0
1
0
3
775 C
2
664
0
0
1
0
0
0
0
1
1
0
0
0
0
1
0
0
3
775
C
2
664
0
1
0
0
0
0
1
0
0
0
0
1
1
0
0
0
3
775 C
2
664
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1
3
775
1
CCA
D
2
664
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
1=4
3
775
as promised in Theorem 5.
29. a. It is possible to go from any state to any other state in any even number of steps, so the Markov chain is irreducible with
period 2.
b. q D
2
4
1=4
1=2
1=4
3
5
c. One choice is D D
2
4
1
0
0
0
 1
0
0
0
0
3
5, A D
2
4
1
1
 1
2
 2
0
1
1
1
3
5
d. Compute that
P n D ADnA 1 D
2
4
1
1
 1
2
 2
0
1
1
1
3
5
2
4
1
0
0
0
. 1/n
0
0
0
0
3
5
2
4
1=4
1=4
1=4
1=4
 1=4
1=4
 1=2
0
1=2
3
5
D 1
4
2
4
1 C . 1/n
1   . 1/n
1 C . 1/n
2   2. 1/n
2 C 2. 1/n
2   2. 1/n
1 C . 1/n
1   . 1/n
1 C . 1/n
3
5
D
2
4
1=4
1=4
1=4
1=2
1=2
1=2
1=4
1=4
1=4
3
5 C . 1/n
2
4
1=4
 1=4
1=4
 1=2
1=2
 1=2
1=4
 1=4
1=4
3
5
e. The second terms in the expressions for P n and P nC1 will cancel each other when added, so
.1=2/.P n C P nC1/ D
2
4
1=4
1=4
1=4
1=2
1=2
1=2
1=4
1=4
1=4
3
5
as promised in Theorem 5.
31. It is easy to compute that q D
2
4
1=3
1=3
1=3
3
5 for the matrix P .
We further ﬁnd that P 2 D
2
4
0
1
0
0
0
1
1
0
0
3
5 and
P 3 D
2
4
1
0
0
0
1
0
0
0
1
3
5 D I by direct computation. Thus
P 4 D P , P 5 D P 2, P 6 D P 3 D I, and so on. So no
matter the value of n, one of the three matrices P nC1,
P nC2, and P nC3 will be P , one will be P 2, and one will be
P 3 D I. Therefore
lim
n!1
1
3

P nC1 C P nC2 C P nC3
D 1
3
 P C P 2 C P 3
D 1
3
0
@
2
4
0
0
1
1
0
0
0
1
0
3
5 C
2
4
0
1
0
0
0
1
1
0
0
3
5
C
2
4
1
0
0
0
1
0
0
0
1
3
5
1
A
D
2
4
1=3
1=3
1=3
1=3
1=3
1=3
1=3
1=3
1=3
3
5
as promised in Theorem 5.

Section 10.5
C-85
33. a. Since any permutation of rows may be written as a
sequence of row swaps, the permutation of rows may be
performed by multiplying A on the left by a sequence of
elementary matrices E1, : : : , Ek. Set E D Ek    E1,
then EA will be the matrix A with its rows permuted in
exactly the same order in which the rows of In were
permuted to form E.
b. By part (a), EAT will be the matrix AT with its rows
permuted in exactly the same order in which the rows of
In were permuted to form E. Thus .EAT /T will be the
matrix A with its columns permuted in exactly the same
order in which the rows of In were permuted to form E,
and since .EAT /T D .AT /T ET D AET , the result
follows.
c. The matrix EAET is .EA/ET . By part (a), EA is the
matrix A with its rows permuted in exactly the same
order in which the rows of In were permuted to form E.
Applying part (b) to EA, .EA/ET is the matrix EA
with its columns permuted in exactly the same order in
which the rows of In were permuted to form E. Thus
EAET is the matrix A with its rows and columns
permuted in exactly the same order in which the rows of
In were permuted to form E.
d. Since matrix multiplication is associative,
.EA/ET D E.AET / and it does not matter whether the
rows of matrix A or the columns of matrix A are
permuted ﬁrst.
Section 10.5, page C-50
1.
 3
2
3=2
2

3. Using reordering 2, 4, 1, 3, 5:
2
4
1075=736
125=368
185=368
25=46
35=23
15=23
105=184
55=92
155=92
3
5
5.
2
4
10=21
3=7
5=21
3=14
2=7
5=14
3
5
7. 3=2
9. 1895=736
11. At state 1: 10=21; at state 2: 5=21; at state 3: 2=7
13. a. 9=11
b. 29=11
c. 3=7
15. a. 1
b. 10=3
17. 5=7
19. 38=5
21. False. The .i; j /-element in the fundamental matrix M is
the expected number of visits to the transient state i prior to
absorption, starting at the transient state j .
22. True.
23. False. See Theorem 6.
24. False. See the discussion prior to Example 2.
25. True.
26. True.
27. 2=7
29. 19=2
31. 3:84615
33. Advantage A: 2:53846
Advantage B: 3:30769
35. 3:01105
37. 3:92932
39. From the results of Exercises 35 and 37, using rally point
scoring led to 3:92932   3:01105 D :91827 fewer rallies
being played.
41. a. f1; 2g is a recurrent class; f3; 4; 5g is a transient class.
b. The limiting matrix for f1; 2g is
 2=5
2=5
3=5
3=5

.
c. Since there is only one recurrent class, the probability that the chain is absorbed into f1; 2g is 1. Thus if the chain is started in
any transient state, the probability of being at state 1 after many time steps is 2=5, the probability of being at state 2 after many
time steps is 3=5, and the probability of being at state 3, 4, or 5 after many time steps is 0.
d. Since the ith column of lim
n!1 P n gives the long-range probabilities for the chain started at state i,
lim
n!1 P n D
2
66664
2=5
2=5
2=5
2=5
2=5
3=5
3=5
3=5
3=5
3=5
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
3
77775
e. P 50 
2
66664
:4
:4
:4
:4
:4
:6
:6
:6
:6
:6
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
3
77775

C-86
Answers to Exercises
43. The result is trivially true if n D 1. Assume that the result is
true for n D k; that is, P k D
 I
Sk
O
Qk

, where
Sk D S
 I C Q C Q2 C    C Qk 1
. Then
P kC1 D P kP
D
 I
Sk
O
Qk
  I
S
O
Q

D
 I
S C SkQ
O
QkC1

But
S C SkQ D S C S.I C Q C Q2 C    C Qk 1/Q
D S C S.Q C Q2 C    C Qk/
D S.I C Q C Q2 C    C Qk/
D SkC1
and the result is proven by induction.
Section 10.6, page C-61
1. This concerns the second column of A. The initial state is
1:k (a runner on ﬁrst base, k outs). For entry .1; 2/ of A, the
probability of a transition “to state 0:k” is required. Suppose
that only ﬁrst base is occupied and the batter does not make
an out. Only a home run will empty the bases, so the
.1; 2/-entry is pH.
Entry .2; 2/: (“to state 1:k”) To leave a player on ﬁrst
base, the batter must get to ﬁrst base and the player on ﬁrst
base must reach home plate successfully. This cannot
happen according to the model, so the .2; 2/-entry is 0.
Entry .3; 2/: (“to state 2:k”) To leave a player on
second base, the batter must get to second base and the
player on ﬁrst base must reach home plate successfully.
This cannot happen according to the model, so the
.3; 2/-entry is 0.
Entry .4; 2/: (“to state 3:k”) To leave a player on third
base, the batter must get to third base and the player on ﬁrst
base must reach home plate successfully. This can happen
only if the batter hits a triple, so the .4; 2/-entry is p3.
Entry .5; 2/: (“to state 12:k”) To leave players on ﬁrst
base and second base, the batter must get to ﬁrst base and
the player on ﬁrst base must advance to second. The desired
outcome occurs when the batter either hits a single, gets a
walk, or is hit by a pitch. The .5; 2/-entry is thus pW C p1.
Entry .6; 2/: (“to state 13:k”) This concerns the batter
getting to ﬁrst base and the runner on ﬁrst base advancing to
third base. This cannot happen according to the model, so
the .6; 2/-entry is 0.
Entry .7; 2/: (“to state 23:k”) To leave players on
second base and third base, the batter must hit a double and
the runner on ﬁrst base must advance only to third base.
Thus the .7; 2/-entry is p2.
Entry .8; 2/: (“to state 123:k”) The starting state has
just one runner on base. The next state cannot have three
runners on base, so the .8; 2/-entry is 0.
3. This concerns the ﬁfth column of A. The initial state is 12:k
(runners on ﬁrst base and second base, k outs). For entry
.1; 5/ of A, the probability of a transition “to state 0:k” is
required. Suppose that ﬁrst and second bases are occupied
and the batter does not make an out. Only a home run will
empty the bases, so the .1; 5/-entry is pH.
Entry .2; 5/: (“to state 1:k”) To leave a player on ﬁrst
base, the batter must get to ﬁrst base and both players on
base must reach home plate successfully. This cannot
happen according to the model, so the .2; 5/-entry is 0.
Entry .3; 5/: (“to state 2:k”) To leave a player on
second base, the batter must get to second base and both
players on base must reach home plate successfully. This
cannot happen according to the model, so the .3; 5/-entry
is 0.
Entry .4; 5/: (“to state 3:k”) To leave a player on third
base, the batter must get to third base and the players on
base must reach home plate successfully. This can happen
only if the batter hits a triple, so the .4; 5/-entry is p3.
Entry .5; 5/: (“to state 12:k”) To leave players on ﬁrst
base and second base, the batter must get to ﬁrst base, the
player on ﬁrst base must advance to second base, and the
player on second base must reach home plate successfully.
The desired outcome occurs when the batter hits a single,
but the runner from second will then reach home with
probability :5. The .5; 5/-entry is thus :5p1.
Entry .6; 5/: (“to state 13:k”) This concerns the batter
getting to ﬁrst base and the runner on ﬁrst base advancing to
third base. This cannot happen according to the model, so
the .6; 5/-entry is 0.
Entry .7; 5/: (“to state 23:k”) To leave players on
second base and third base, the batter must hit a double, in
which case the runner on ﬁrst base must advance to third
base and the runner on second base must reach home. Thus
the .7; 5/-entry is p2.
Entry .8; 5/: (“to state 123:k”) To leave runners on
ﬁrst, second, and third bases, the batter must reach ﬁrst and
the two runners must each advance one base. This happens
when the batter is walked, is hit by a pitch, or hits a single
but the runner on second base does not reach home. Thus
the .8; 5/-entry is pW C :5p1.
5. This concerns the seventh column of A. The initial state is
23:k (runners on second and third bases, k outs). For entry
.1; 7/ of A, the probability of a transition “to state 0:k” is
required. Suppose that second and third bases are occupied
and the batter does not make an out. Only a home run will
empty the bases, so the .1; 7/-entry is pH.
Entry .2; 7/: (“to state 1:k”) To leave a player on ﬁrst
base, the batter must get to ﬁrst base and the players on
second base and third base must reach home plate
successfully. The desired outcome occurs when the batter
hits a single, but the runner from second will then reach
home with probability :5. Thus the .2; 7/-entry is :5p1.
Entry .3; 7/: (“to state 2:k”) To leave a player on
second base, the batter must reach second base (a “double”)
and the runners on second and third bases must score. The

Section 10.6
C-87
second condition, however, is automatically satisﬁed
because of the assumption in Table 2. So the probability of
success in this case is p2. This is the .3; 7/-entry.
Entry .4; 7/: (“to state 3:k”) By an argument similar to
that for the .3; 6/-entry, the .4; 7/-entry is p3.
Entry .5; 7/: (“to state 12:k”) To leave players on ﬁrst
base and second base, the batter must get to ﬁrst base and
the player on second base must remain there while the
runner on third base reaches home. This is impossible, so
the .5; 7/-entry is 0.
Entry .6; 7/: (“to state 13:k”) This concerns the batter
getting to ﬁrst base and the runner on second base advancing
to third base while the runner on third base reaches home.
This can happen only if the batter hits a single, with
probability p1, and the runner on second base stops at third
base, which happens with probability :5 (by Table 2). Since
both events are required, the .6; 7/-entry is the product :5p1.
Entry .7; 7/: (“to state 23:k”) To leave players on
second base and third base, the batter must hit a double and
the runner on second base must advance only to third base.
This cannot happen, so the .7; 7/-entry is 0.
Entry .8; 7/: (“to state 123:k”) To leave runners on
ﬁrst, second, and third bases, the batter must reach ﬁrst base
and the two runners must each fail to advance one base.
This happens when the batter is walked or is hit by a pitch.
Thus the .8; 7/-entry is pW .
7. pW D :0954785, p1 D :159996, p2 D :049377, p3 D :00514581, pH D :0291127, pO D :66089. Thus
A D
0:k
1:k
2:k
3:k
12:k
13:k
23:k
123:k
2
66666666664
:0291127
:255474
:049377
:00514581
0
0
0
0
:0291127
0
0
:00514581
:255474
0
:049377
0
:0291127
:0799978
:049377
:00514581
:0954785
:0799978
0
0
:0291127
:159996
:049377
:00514581
0
:0954785
0
0
:0291127
0
0
:00514581
:0799978
0
:049377
:175476
:0291127
0
0
:00514581
:159996
0
:049377
:0954785
:0291127
:0799978
:049377
:00514581
0
:0799978
0
:0954785
:0291127
0
0
:00514581
:0799978
0
:049377
:175476
3
77777777775
0:k
1:k
2:k
3:k
12:k
13:k
23:k
123:k
9. The sum of the ﬁrst column of M shows that
EŒB D 4:53933. The ﬁrst column of SM allows EŒL to
be computed:
EŒL D 0.:34973/ C 1.:33414/ C 2.:23820/ C 3.:07793/
D 1:04433
Thus
EŒR D EŒB   EŒL   3
D 4:53933   1:04433   3 D :495
11. Bonds: pW D :212933, p1 D :119495, p2 D :0480377,
p3 D :00615458, pH D :0609064, pO D :552474.
Ruth: pW D :2004, p1 D :144421, p2 D :0481721,
p3 D :0129474, pH D :0679741, pO D :526085.
Williams: pW D :210936, p1 D :157383, p2 D :0537579,
p3 D :00727012, pH D :0533484, pO D :517305.
13. a. The sum of the second column of M will tell the
expected number of batters that will come to the plate
starting with a runner on ﬁrst and none out.
b. The second column of SM will give the probabilities of
leaving 0, 1, 2, or 3 runners on base starting with a
runner on ﬁrst and none out. Thus the expected number
of runners left on base starting with a runner on ﬁrst and
none out could be calculated.
c. The expected number of runs scored using the second
column data will give the expected number of runs
scored starting with a runner on ﬁrst and none out.
15. The sum of the column of M is 4:53933. One batter has
already reached base, so EŒB D 1 C 4:53933 D 5:53933.
The column of SM allows EŒL to be computed:
EŒL D 0.:06107/ C 1.:47084/ C 2.:34791/ C 3.:12108/
D 1:52990
Thus
EŒR D EŒB   EŒL   3
D 5:53933   1:52990   3 D 1:00943
17. If the baserunner does not attempt a steal, you expect to
score :85654 runs by Exercise 14. If the runner attempts a
steal and succeeds, you expect to score 1:00943 runs by
Exercise 15. If the runner attempts a steal and does not
succeed, you expect to score :26779 runs by Exercise 16.
Thus the expected number of runs scored if a steal is
attempted is 1:00943.:8/ C :26779.:2/ D :861102.
Attempting a steal thus increases the expected number of
runs scored.


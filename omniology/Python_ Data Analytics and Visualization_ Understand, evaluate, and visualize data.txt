
Python: Data Analytics and 
Visualization
Understand, evaluate, and visualize data
A course in three modules
BIRMINGHAM - MUMBAI

Python: Data Analytics and Visualization
Copyright © 2017 Packt Publishing
All rights reserved. No part of this course may be reproduced, stored in a retrieval 
system, or transmitted in any form or by any means, without the prior written 
permission of the publisher, except in the case of brief quotations embedded in 
critical articles or reviews.
Every effort has been made in the preparation of this course to ensure the accuracy 
of the information presented. However, the information contained in this course 
is sold without warranty, either express or implied. Neither the authors, nor Packt 
Publishing, and its dealers and distributors will be held liable for any damages 
caused or alleged to be caused directly or indirectly by this course.
Packt Publishing has endeavored to provide trademark information about all of the 
companies and products mentioned in this course by the appropriate use of capitals. 
However, Packt Publishing cannot guarantee the accuracy of this information.
Published on: March 2017
Production reference: 1220317
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham B32PB, UK.
ISBN: 978-1-78829-009-8
www.packtpub.com

Credits
Authors
Phuong Vo.T.H 
Martin Czygan 
Ashish Kumar 
Kirthi Raman
Reviewers
Dong Chao
Hai Minh Nguyen
Kenneth Emeka Odoh
Matt Hollingsworth
Julian Quick
Hang (Harvey) Yu
Content Development Editor
Deepti Thore
Graphics
Tania Dutta
Production Coordinator
Aparna Bhagat


[ i ]
Preface
The world generates data at an increasing pace. Consumers, sensors, or scientific 
experiments emit data points every day. In finance, business, administration and the 
natural or social sciences, working with data can make up a significant part of the 
job. Being able to efficiently work with small or large datasets has become a valuable 
skill. Python started as a general purpose language. Around ten years ago, in 2006, 
the first version of NumPy was released, which made Python a first class language 
for numerical computing and laid the foundation for a prospering development, 
which led to what we today call the PyData ecosystem: A growing set of high-
performance libraries to be used in the sciences, finance, business or anywhere else 
you want to work efficiently with datasets. Python is not only about data analysis. 
The list of industrial-strength libraries for many general computing tasks is long, 
which makes working with data in Python even more compelling. 
Social media and the Internet of Things have resulted in an avalanche of data. The 
data is powerful but not in its raw form; it needs to be processed and modeled and 
Python is one of the most robust tools we have out there to do so. It has an array of 
packages for predictive modeling and a suite of IDEs to choose from. Learning to 
predict who would win, lose, buy, lie, or die with Python is an indispensable skill 
set to have in this data age. This course is your guide to get started with Predictive 
Analytics using Python as the tool.
Data visualization is intended to provide information clearly and help the viewer 
understand them qualitatively. The well-known expression that a picture is worth 
a thousand words may be rephrased as “a picture tells a story as well as a large 
collection of words”. Visualization is, therefore, a very precious tool that helps the  
viewer understand a concept quickly. We are currently faced with a plethora of 
data containing many insights that hold the key to success in the modern day. It is 
important to find the data, clean it, and use the right tool to visualize it. This course 
explains several different ways to visualize data using Python packages, along with 
very useful examples in many different areas such as numerical computing, financial 
models, statistical and machine learning, and genetics and networks.

Preface
[ ii ]
What this learning path covers
Module 1, Getting Started with Python Data Analysis starts with an introduction 
to data analysis and process, overview of libraries and its uses. Further you’ll dive 
right into the core of the PyData ecosystem by introducing the NumPy package for 
high-performance computing. We will also deal with a prominent and popular data 
analysis library for Python called Pandas and understand the data through graphical 
representation. Moving further you will see how to work with time-oriented data 
in Pandas. You will then learn to interact with three main categories: text formats, 
binary formats and databases and work on some application examples. In the end 
you will see the working of different scikit-learn modules.
Module 2, Learning Predictive Analytics with Python, talks about aspects, scope, and 
applications of predictive modeling. Data cleaning takes about 80% of the modelling 
time and hence we will understand its importance and methods. You will see how 
to subset, aggregate, sample, merge, append and concatenate a dataset. Further 
you will get acquainted with the basic statistics needed to make sense of the model 
parameters resulting from the predictive models. You will also understand the 
mathematics behind linear and logistic regression along with clustering. You will  
also deal with Decision trees and related classification algorithms. In the end you 
will be learning about the best practices adopted in the field of predictive modelling 
to get the optimum results.
 Module 3, Mastering Python Data Visualization, expounds that data visualization 
should actually be referred to as “the visualization of information for knowledge 
inference”. You will see how to use Anaconda from Continuum Analytics and learn 
interactive plotting methods. You will deal with stock quotes, regression analysis, 
the Monte Carlo algorithm, and simulation methods with examples. Further you 
will get acquainted with statistical methods such as linear and nonlinear regression 
and clustering and classification methods using numpy, scipy, matplotlib, and 
scikit-learn. You will use specific libraries such as graph-tool, NetworkX, matplotlib, 
scipy, and numpy. In the end we will see simulation methods and examples of signal 
processing to show several visualization methods.
What you need for this learning path
You will need a Python programming environment installed on your system. The 
first module uses a recent Python 2, but many examples will work with Python 3 as 
well.b The versions of the libraries used in the first module are: NumPy 1.9.2, Pandas 
0.16.2, matplotlib 1.4.3, tables 3.2.2, pymongo 3.0.3, redis 2.10.3, and scikit-learn 
0.16.1. As these packages are all hosted on PyPI, the Python package index,  
they can be easily installed with pip. To install NumPy, you would write: 

Preface
[ iii ]
$ pip install numpy If you are not using them already, we suggest you take a 
look at virtual environments for managing isolating Python environment on your 
computer. For Python 2, there are two packages of interest there: virtualenv and 
virtualenvwrapper. Since Python 3.3, there is a tool in the standard library called 
pyvenv (https://docs.python.org/3/library/venv.html), which serves the 
same purpose. Most libraries will have an attribute for the version, so if you already 
have a library installed, you can quickly check its version: 
>>> import redis
>>> redis.__version__
‘2.10.3’).
While all the examples in second module can be run interactively in a Python shell. 
We used IPython 4.0.0 with Python 2.7.10.
For the third module, you need Python 2.7.6 or a later version installed on your 
operating system. For the examples in this module, Mac OS X 10.10.5’s Python 
default version (2.7.6) has been used. Install the prepackaged scientific Python 
distributions, such as Anaconda from Continuum or Enthought Python Distribution 
if possible
Who this learning path is for
This course  is for Python Developers who are willing to get into data analysis and 
wish to visualize their analyzed data in a more efficient and insightful manner.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about 
this course—what you liked or disliked. Reader feedback is important for us as it 
helps us develop titles that you will really get the most out of.
To send us general feedback, simply e-mail feedback@packtpub.com, and mention 
the course’s title in the subject of your message.
If there is a topic that you have expertise in and you are interested in either writing 
or contributing to a book, see our author guide at www.packtpub.com/authors.

Preface
[ iv ]
Customer support
Now that you are the proud owner of a Packt course, we have a number of things to 
help you to get the most from your purchase.
Downloading the example code
You can download the example code files for this course from your account at 
http://www.packtpub.com. If you purchased this course elsewhere, you can visit 
http://www.packtpub.com/support and register to have the files e-mailed directly 
to you.
You can download the code files by following these steps:
1.	 Log in or register to our website using your e-mail address and password.
2.	 Hover the mouse pointer on the SUPPORT tab at the top.
3.	 Click on Code Downloads & Errata.
4.	 Enter the name of the course in the Search box.
5.	 Select the course for which you’re looking to download the code files.
6.	 Choose from the drop-down menu where you purchased this course from.
7.	 Click on Code Download.
You can also download the code files by clicking on the Code Files button on the 
course’s webpage at the Packt Publishing website. This page can be accessed by 
entering the course’s name in the Search box. Please note that you need to be  
logged in to your Packt account.
Once the file is downloaded, please make sure that you unzip or extract the folder 
using the latest version of:
•	
WinRAR / 7-Zip for Windows
•	
Zipeg / iZip / UnRarX for Mac
•	
7-Zip / PeaZip for Linux
The code bundle for the course is also hosted on GitHub at https://github.com/
PacktPublishing/Python-Data-Analytics-and-Visualization. We also have 
other code bundles from our rich catalog of books, videos, and courses available at 
https://github.com/PacktPublishing/. Check them out!

Preface
[ v ]
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes 
do happen. If you find a mistake in one of our courses—maybe a mistake in the text 
or the code—we would be grateful if you could report this to us. By doing so, you 
can save other readers from frustration and help us improve subsequent versions 
of this course. If you find any errata, please report them by visiting http://www.
packtpub.com/submit-errata, selecting your course, clicking on the Errata 
Submission Form link, and entering the details of your errata. Once your errata are 
verified, your submission will be accepted and the errata will be uploaded to our 
website or added to any list of existing errata under the Errata section of that title.
To view the previously submitted errata, go to https://www.packtpub.com/books/
content/support and enter the name of the course in the search field. The required 
information will appear under the Errata section.
Piracy
Piracy of copyrighted material on the Internet is an ongoing problem across all 
media. At Packt, we take the protection of our copyright and licenses very seriously. 
If you come across any illegal copies of our works in any form on the Internet, please 
provide us with the location address or website name immediately so that we can 
pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected  
pirated material.
We appreciate your help in protecting our authors and our ability to bring you 
valuable content.
Questions
If you have a problem with any aspect of this course, you can contact us at 
questions@packtpub.com, and we will do our best to address the problem.


i
Module 1: Getting Started with Python Data Analysis
Chapter 1: Introducing Data Analysis and Libraries	
1
Data analysis and processing	
2
An overview of the libraries in data analysis	
5
Python libraries in data analysis	
7
Summary	
9
Chapter 2: NumPy Arrays and Vectorized Computation	
11
NumPy arrays	
12
Array functions	
19
Data processing using arrays	
21
Linear algebra with NumPy	
24
NumPy random numbers	
25
Summary	
28
Chapter 3: Data Analysis with Pandas	
31
An overview of the Pandas package	
31
The Pandas data structure	
32
The essential basic functionality	
38
Indexing and selecting data	
46
Computational tools	
47
Working with missing data	
49
Advanced uses of Pandas for data analysis	
52
Summary	
56
Chapter 4: Data Visualization	
59
The matplotlib API primer	
60
Exploring plot types	
68
Legends and annotations	
73
Plotting functions with Pandas	
76

ii
Table of Contents
Additional Python data visualization tools	
78
Summary	
81
Chapter 5: Time Series	
83
Time series primer	
83
Working with date and time objects	
84
Resampling time series	
92
Downsampling time series data	
92
Upsampling time series data	
95
Time zone handling	
97
Timedeltas	
98
Time series plotting	
99
Summary	
103
Chapter 6: Interacting with Databases	
105
Interacting with data in text format	
105
Interacting with data in binary format	
111
Interacting with data in MongoDB	
113
Interacting with data in Redis	
118
Summary	
122
Chapter 7: Data Analysis Application Examples	
125
Data munging	
126
Data aggregation	
139
Grouping data	
142
Summary	
144
Chapter 8: Machine Learning Models with scikit-learn	
145
An overview of machine learning models	
145
The scikit-learn modules for different models	
146
Data representation in scikit-learn	
148
Supervised learning – classification and regression	
150
Unsupervised learning – clustering and dimensionality reduction	
156
Measuring prediction performance	
160
Summary	
162
Module 2: Learning Predictive Analytics with Python
Chapter 1: Getting Started with Predictive Modelling	
167
Introducing predictive modelling	
167
Applications and examples of predictive modelling	
174

iii
Table of Contents
Python and its packages – download and installation	
177
Python and its packages for predictive modelling	
182
IDEs for Python	
184
Summary	
187
Chapter 2: Data Cleaning	
189
Reading the data – variations and examples	
190
Various methods of importing data in Python	
191
Basics – summary, dimensions, and structure	
202
Handling missing values	
204
Creating dummy variables	
211
Visualizing a dataset by basic plotting	
212
Summary	
217
Chapter 3: Data Wrangling	
219
Subsetting a dataset	
220
Generating random numbers and their usage	
228
Grouping the data – aggregation, filtering, and transformation	
246
Random sampling – splitting a dataset in training and testing datasets	
257
Concatenating and appending data	
260
Merging/joining datasets	
268
Summary	
280
Chapter 4: Statistical Concepts for Predictive Modelling	
283
Random sampling and the central limit theorem	
284
Hypothesis testing	
285
Chi-square tests	
293
Correlation	
298
Summary	
305
Chapter 5: Linear Regression with Python	
307
Understanding the maths behind linear regression	
309
Making sense of result parameters	
319
Implementing linear regression with Python	
322
Model validation	
334
Handling other issues in linear regression	
339
Summary	
360
Chapter 6: Logistic Regression with Python	
363
Linear regression versus logistic regression	
364
Understanding the math behind logistic regression	
365
Implementing logistic regression with Python	
382

iv
Table of Contents
Model validation and evaluation	
394
Model validation	
398
Summary	
405
Chapter 7: Clustering with Python	
407
Introduction to clustering – what, why, and how?	
408
Mathematics behind clustering	
411
Implementing clustering using Python	
424
Fine-tuning the clustering	
431
Summary	
435
Chapter 8: Trees and Random Forests with Python	
437
Introducing decision trees	
438
Understanding the mathematics behind decision trees	
441
Implementing a decision tree with scikit-learn	
453
Understanding and implementing regression trees	
459
Understanding and implementing random forests	
464
Summary	
468
Chapter 9: Best Practices for Predictive Modelling	
471
Best practices for coding	
472
Best practices for data handling	
478
Best practices for algorithms	
479
Best practices for statistics	
480
Best practices for business contexts	
481
Summary	
482
Appendix: A List of Links	
483
Module 3: Mastering Python Data Visualization
Chapter 1: A Conceptual Framework for Data Visualization	
487
Data, information, knowledge, and insight	
488
The transformation of data	
491
Data visualization history	
497
How does visualization help decision-making?	
501
Visualization plots	
507
Summary	
525
Chapter 2: Data Analysis and Visualization	
527
Why does visualization require planning?	
528
The Ebola example	
529
A sports example	
535

v
Table of Contents
Creating interesting stories with data	
548
Perception and presentation methods	
558
Some best practices for visualization	
561
Visualization tools in Python	
568
Interactive visualization	
571
Summary	
576
Chapter 3: Getting Started with the Python IDE	
577
The IDE tools in Python	
578
Visualization plots with Anaconda	
595
Interactive visualization packages	
602
Summary	
605
Chapter 4: Numerical Computing and Interactive Plotting	
607
NumPy, SciPy, and MKL functions	
608
Scalar selection	
624
Slicing	
625
Array indexing	
626
Other data structures	
629
Visualization using matplotlib	
641
The visualization example in sports	
659
Summary	
663
Chapter 5: Financial and Statistical Models	
665
The deterministic model	
666
The stochastic model	
677
The threshold model	
707
An overview of statistical and machine learning	
711
Creating animated and interactive plots	
717
Summary	
722
Chapter 6: Statistical and Machine Learning	
723
k-nearest neighbors	
747
Logistic regression	
751
Support vector machines	
755
Principal component analysis	
757
k-means clustering	
762
Summary	
766
Chapter 7: Bioinformatics, Genetics, and Network Models	
767
Directed graphs and multigraphs	
768
The clustering coefficient of graphs	
780
Analysis of social networks	
784

vi
Table of Contents
The planar graph test	
786
The directed acyclic graph test	
788
Maximum flow and minimum cut	
790
A genetic programming example	
792
Stochastic block models	
794
Summary	
799
Chapter 8: Advanced Visualization	
801
Computer simulation	
802
Summary	
822
Appendix: Go Forth and Explore Visualization	
823
Bibliography	
831

Module 1
Getting Started with Python Data Analysis
Learn to use powerful Python libraries for effective data processing and analysis


[ 1 ]
Introducing Data Analysis 
and Libraries
Data is raw information that can exist in any form, usable or not. We can easily get 
data everywhere in our lives; for example, the price of gold on the day of writing 
was $ 1.158 per ounce. This does not have any meaning, except describing the price 
of gold. This also shows that data is useful based on context.
With the relational data connection, information appears and allows us to expand 
our knowledge beyond the range of our senses. When we possess gold price data 
gathered over time, one piece of information we might have is that the price has 
continuously risen from $1.152 to $1.158 over three days. This could be used by 
someone who tracks gold prices.
Knowledge helps people to create value in their lives and work. This value is 
based on information that is organized, synthesized, or summarized to enhance 
comprehension, awareness, or understanding. It represents a state or potential for 
action and decisions. When the price of gold continuously increases for three days, it 
will likely decrease on the next day; this is useful knowledge. 

Introducing Data Analysis and Libraries
[ 2 ]
The following figure illustrates the steps from data to knowledge; we call this 
process, the data analysis process and we will introduce it in the next section:
Data
Collecting
Summarizing
organizing
Gold price today is 1158$
Gold price has risen
for three days
Gold price will slightly
decrease on next day
Knowledge
Information
Decision making
Synthesising
Analysing
In this chapter, we will cover the following topics:
•	
Data analysis and process
•	
An overview of libraries in data analysis using different programming 
languages
•	
Common Python data analysis libraries
Data analysis and processing
Data is getting bigger and more diverse every day. Therefore, analyzing and 
processing data to advance human knowledge or to create value is a big challenge. 
To tackle these challenges, you will need domain knowledge and a variety of skills, 
drawing from areas such as computer science, artificial intelligence (AI) and 
machine learning (ML), statistics and mathematics, and knowledge domain, as 
shown in the following figure:

Chapter 1
[ 3 ]
Computer
Science
Artificial
Intelligent &
Machine
Learning
Knowledge
Domain
Statistics &
Mathematics
Data Analysis
Math
....
Data expertise
....
Algorithms
....
Programming
....
Let's go through data analysis and its domain knowledge:
•	
Computer science: We need this knowledge to provide abstractions for 
efficient data processing. Basic Python programming experience is required 
to follow the next chapters. We will introduce Python libraries used in data 
analysis.
•	
Artificial intelligence and machine learning: If computer science knowledge 
helps us to program data analysis tools, artificial intelligence and machine 
learning help us to model the data and learn from it in order to build smart 
products.
•	
Statistics and mathematics: We cannot extract useful information from raw 
data if we do not use statistical techniques or mathematical functions.
•	
Knowledge domain: Besides technology and general techniques, it is 
important to have an insight into the specific domain. What do the data fields 
mean? What data do we need to collect? Based on the expertise, we explore 
and analyze raw data by applying the above techniques, step by step.

Introducing Data Analysis and Libraries
[ 4 ]
Data analysis is a process composed of the following steps:
•	
Data requirements: We have to define what kind of data will be collected 
based on the requirements or problem analysis. For example, if we want to 
detect a user's behavior while reading news on the internet, we should be 
aware of visited article links, dates and times, article categories, and the time 
the user spends on different pages.
•	
Data collection: Data can be collected from a variety of sources: mobile, 
personal computer, camera, or recording devices. It may also be obtained in 
different ways: communication, events, and interactions between person and 
person, person and device, or device and device. Data appears whenever and 
wherever in the world. The problem is how we can find and gather it to solve 
our problem? This is the mission of this step.
•	
Data processing: Data that is initially obtained must be processed or 
organized for analysis. This process is performance-sensitive. How fast can 
we create, insert, update, or query data? When building a real product that 
has to process big data, we should consider this step carefully. What kind of 
database should we use to store data? What kind of data structure, such as 
analysis, statistics, or visualization, is suitable for our purposes?
•	
Data cleaning: After being processed and organized, the data may still 
contain duplicates or errors. Therefore, we need a cleaning step to reduce 
those situations and increase the quality of the results in the following 
steps. Common tasks include record matching, deduplication, and column 
segmentation. Depending on the type of data, we can apply several types of 
data cleaning. For example, a user's history of visits to a news website might 
contain a lot of duplicate rows, because the user might have refreshed certain 
pages many times. For our specific issue, these rows might not carry any 
meaning when we explore the user's behavior so we should remove them 
before saving it to our database. Another situation we may encounter is click 
fraud on news—someone just wants to improve their website ranking or 
sabotage awebsite. In this case, the data will not help us to explore a user's 
behavior. We can use thresholds to check whether a visit page event comes 
from a real person or from malicious software.
•	
Exploratory data analysis: Now, we can start to analyze data through a 
variety of techniques referred to as exploratory data analysis. We may detect 
additional problems in data cleaning or discover requests for further data. 
Therefore, these steps may be iterative and repeated throughout the whole 
data analysis process. Data visualization techniques are also used to examine 
the data in graphs or charts. Visualization often facilitates understanding of 
data sets, especially if they are large or high-dimensional.

Chapter 1
[ 5 ]
•	
Modelling and algorithms: A lot of mathematical formulas and algorithms 
may be applied to detect or predict useful knowledge from the raw data. For 
example, we can use similarity measures to cluster users who have exhibited 
similar news-reading behavior and recommend articles of interest to them 
next time. Alternatively, we can detect users' genders based on their news 
reading behavior by applying classification models such as the Support 
Vector Machine (SVM) or linear regression. Depending on the problem, we 
may use different algorithms to get an acceptable result. It can take a lot of 
time to evaluate the accuracy of the algorithms and choose the best one to 
implement for a certain product.
•	
Data product: The goal of this step is to build data products that receive data 
input and generate output according to the problem requirements. We will 
apply computer science knowledge to implement our selected algorithms as 
well as manage the data storage.
An overview of the libraries in data 
analysis
There are numerous data analysis libraries that help us to process and analyze data. 
They use different programming languages, and have different advantages and 
disadvantages of solving various data analysis problems. Now, we will introduce 
some common libraries that may be useful for you. They should give you an 
overview of the libraries in the field. However, the rest of this book focuses on 
Python-based libraries.
Some of the libraries that use the Java language for data analysis are as follows:
•	
Weka: This is the library that I became familiar with the first time I learned 
about data analysis. It has a graphical user interface that allows you to run 
experiments on a small dataset. This is great if you want to get a feel for what 
is possible in the data processing space. However, if you build a complex 
product, I think it is not the best choice, because of its performance, sketchy 
API design, non-optimal algorithms, and little documentation (http://www.
cs.waikato.ac.nz/ml/weka/).

Introducing Data Analysis and Libraries
[ 6 ]
•	
Mallet: This is another Java library that is used for statistical natural 
language processing, document classification, clustering, topic modeling, 
information extraction, and other machine-learning applications on text. 
There is an add-on package for Mallet, called GRMM, that contains support 
for inference in general, graphical models, and training of Conditional 
random fields (CRF) with arbitrary graphical structures. In my experience, 
the library performance and the algorithms are better than Weka. However, 
its only focus is on text-processing problems. The reference page is at 
http://mallet.cs.umass.edu/.
•	
Mahout: This is Apache's machine-learning framework built on top of 
Hadoop; its goal is to build a scalable machine-learning library. It looks 
promising, but comes with all the baggage and overheads of Hadoop.  
The homepage is at http://mahout.apache.org/.
•	
Spark: This is a relatively new Apache project, supposedly up to a hundred 
times faster than Hadoop. It is also a scalable library that consists of common 
machine-learning algorithms and utilities. Development can be done in 
Python as well as in any JVM language. The reference page is at  
https://spark.apache.org/docs/1.5.0/mllib-guide.html.
Here are a few libraries that are implemented in C++:
•	
Vowpal Wabbit: This library is a fast, out-of-core learning system sponsored 
by Microsoft Research and, previously, Yahoo! Research. It has been 
used to learn a tera-feature (1012) dataset on 1,000 nodes in one hour. 
More information can be found in the publication at http://arxiv.org/
abs/1110.4198.
•	
MultiBoost: This package is a multiclass, multi label, and multitask 
classification boosting software implemented in C++. If you use 
this software, you should refer to the paper published in 2012 in the 
JournalMachine Learning Research, MultiBoost: A Multi-purpose Boosting 
Package, D.Benbouzid, R. Busa-Fekete, N. Casagrande, F.-D. Collin, and B. Kégl.
•	
MLpack: This is also a C++ machine-learning library, developed by the 
Fundamental Algorithmic and Statistical Tools Laboratory (FASTLab) 
at Georgia Tech. It focusses on scalability, speed, and ease-of-use, and was 
presented at the BigLearning workshop of NIPS 2011. Its homepage is at 
http://www.mlpack.org/about.html.
•	
Caffe: The last C++ library we want to mention is Caffe. This is a deep 
learning framework made with expression, speed, and modularity in mind. 
It is developed by the Berkeley Vision and Learning Center (BVLC) and 
community contributors. You can find more information about it at  
http://caffe.berkeleyvision.org/.

Chapter 1
[ 7 ]
Other libraries for data processing and analysis are as follows:
•	
Statsmodels: This is a great Python library for statistical modeling and is 
mainly used for predictive and exploratory analysis.
•	
Modular toolkit for data processing (MDP): This is a collection of 
supervised and unsupervised learning algorithms and other data processing 
units that can be combined into data processing sequences and more complex 
feed-forward network architectures (http://mdp-toolkit.sourceforge.
net/index.html).
•	
Orange: This is an open source data visualization and analysis for novices 
and experts. It is packed with features for data analysis and has add-ons  
for bioinformatics and text mining. It contains an implementation of  
self-organizing maps, which sets it apart from the other projects as well 
(http://orange.biolab.si/).
•	
Mirador: This is a tool for the visual exploration of complex datasets, 
supporting Mac and Windows. It enables users to discover correlation patterns 
and derive new hypotheses from data (http://orange.biolab.si/).
•	
RapidMiner: This is another GUI-based tool for data mining, machine 
learning, and predictive analysis (https://rapidminer.com/).
•	
Theano: This bridges the gap between Python and lower-level languages. 
Theano gives very significant performance gains, particularly for large 
matrix operations, and is, therefore, a good choice for deep learning models. 
However, it is not easy to debug because of the additional compilation layer.
•	
Natural language processing toolkit (NLTK): This is written in Python with 
very unique and salient features.
Here, I could not list all libraries for data analysis. However, I think the above 
libraries are enough to take a lot of your time to learn and build data analysis 
applications. I hope you will enjoy them after reading this book.
Python libraries in data analysis
Python is a multi-platform, general-purpose programming language that can run 
on Windows, Linux/Unix, and Mac OS X, and has been ported to Java and .NET 
virtual machines as well. It has a powerful standard library. In addition, it has 
many libraries for data analysis: Pylearn2, Hebel, Pybrain, Pattern, MontePython, 
and MILK. In this book, we will cover some common Python data analysis libraries 
such as Numpy, Pandas, Matplotlib, PyMongo, and scikit-learn. Now, to help you 
get started, I will briefly present an overview of each library for those who are less 
familiar with the scientific Python stack.

Introducing Data Analysis and Libraries
[ 8 ]
NumPy
One of the fundamental packages used for scientific computing in Python is Numpy. 
Among other things, it contains the following:
•	
A powerful N-dimensional array object
•	
Sophisticated (broadcasting) functions for performing array computations
•	
Tools for integrating C/C++ and Fortran code
•	
Useful linear algebra operations, Fourier transformations, and random 
number capabilities
Besides this, it can also be used as an efficient multidimensional container of  
generic data. Arbitrary data types can be defined and integrated with a wide  
variety of databases.
Pandas
Pandas is a Python package that supports rich data structures and functions for 
analyzing data, and is developed by the PyData Development Team. It is focused on 
the improvement of Python's data libraries. Pandas consists of the following things:
•	
A set of labeled array data structures; the primary of which are Series, 
DataFrame, and Panel
•	
Index objects enabling both simple axis indexing and multilevel/hierarchical 
axis indexing
•	
An intergraded group by engine for aggregating and transforming datasets
•	
Date range generation and custom date offsets
•	
Input/output tools that load and save data from flat files or PyTables/HDF5 
format
•	
Optimal memory versions of the standard data structures
•	
Moving window statistics and static and moving window linear/panel 
regression
Due to these features, Pandas is an ideal tool for systems that need complex  
data structures or high-performance time series functions such as financial data 
analysis applications.

Chapter 1
[ 9 ]
Matplotlib
Matplotlib is the single most used Python package for 2D-graphics. It provides  
both a very quick way to visualize data from Python and publication-quality  
figures in many formats: line plots, contour plots, scatter plots, and Basemap plots. 
It comes with a set of default settings, but allows customization of all kinds of 
properties. However, we can easily create our chart with the defaults of almost  
every property in Matplotlib.
PyMongo
MongoDB is a type of NoSQL database. It is highly scalable, robust, and perfect to 
work with JavaScript-based web applications, because we can store data as JSON 
documents and use flexible schemas.
PyMongo is a Python distribution containing tools for working with MongoDB. 
Many tools have also been written for working with PyMongo to add more features 
such as MongoKit, Humongolus, MongoAlchemy, and Ming.
The scikit-learn library
The scikit-learn is an open source machine-learning library using the Python 
programming language. It supports various machine learning models, such as 
classification, regression, and clustering algorithms, interoperated with the Python 
numerical and scientific libraries NumPy and SciPy. The latest scikit-learn version is 
0.16.1, published in April 2015.
Summary
In this chapter, we presented three main points. Firstly, we figured out the 
relationship between raw data, information and knowledge. Due to its contribution 
to our lives, we continued to discuss an overview of data analysis and processing 
steps in the second section. Finally, we introduced a few common supported libraries 
that are useful for practical data analysis applications. Among those, in the next 
chapters, we will focus on Python libraries in data analysis.

Introducing Data Analysis and Libraries
[ 10 ]
Practice exercise
The following table describes users' rankings on Snow White movies:
UserID
Sex
Location 
Ranking
A
Male
Philips
4
B
Male
VN
2
C
Male 
Canada
1
D
Male
Canada
2
E
Female
VN
5
F
Female
NY
4
Exercise 1: What information can we find in this table? What kind of knowledge can 
we derive from it?
Exercise 2: Based on the data analysis process in this chapter, try to define the data 
requirements and analysis steps needed to predict whether user B likes Maleficent 
movies or not.

[ 11 ]
NumPy Arrays and 
Vectorized Computation
NumPy is the fundamental package supported for presenting and computing data 
with high performance in Python. It provides some interesting features as follows:
•	
Extension package to Python for multidimensional arrays (ndarrays), 
various derived objects (such as masked arrays), matrices providing 
vectorization operations, and broadcasting capabilities. Vectorization can 
significantly increase the performance of array computations by taking 
advantage of Single Instruction Multiple Data (SIMD) instruction sets in 
modern CPUs.
•	
Fast and convenient operations on arrays of data, including mathematical 
manipulation, basic statistical operations, sorting, selecting, linear algebra, 
random number generation, discrete Fourier transforms, and so on.
•	
Efficiency tools that are closer to hardware because of integrating  
C/C++/Fortran code.
NumPy is a good starting package for you to get familiar with arrays and  
array-oriented computing in data analysis. Also, it is the basic step to learn  
other, more effective tools such as Pandas, which we will see in the next chapter.  
We will be using NumPy version 1.9.1.

NumPy Arrays and Vectorized Computation
[ 12 ]
NumPy arrays
An array can be used to contain values of a data object in an experiment or 
simulation step, pixels of an image, or a signal recorded by a measurement device.  
For example, the latitude of the Eiffel Tower, Paris is 48.858598 and the longitude  
is 2.294495. It can be presented in a NumPy array object as p:
>>> import numpy as np
>>> p = np.array([48.858598, 2.294495])
>>> p
Output: array([48.858598, 2.294495])
This is a manual construction of an array using the np.array function. The standard 
convention to import NumPy is as follows:
>>> import numpy as np
You can, of course, put from numpy import * in your code to avoid having to write 
np. However, you should be careful with this habit because of the potential code 
conflicts (further information on code conventions can be found in the Python Style 
Guide, also known as PEP8, at https://www.python.org/dev/peps/pep-0008/).
There are two requirements of a NumPy array: a fixed size at creation and a uniform, 
fixed data type, with a fixed size in memory. The following functions help you to get 
information on the p matrix:
>>> p.ndim    # getting dimension of array p
1
>>> p.shape   # getting size of each array dimension
(2,)
>>> len(p)    # getting dimension length of array p
2
>>> p.dtype    # getting data type of array p
dtype('float64')
Data types
There are five basic numerical types including Booleans (bool), integers (int), 
unsigned integers (uint), floating point (float), and complex. They indicate how 
many bits are needed to represent elements of an array in memory. Besides that, 
NumPy also has some types, such as intc and intp, that have different bit sizes 
depending on the platform.

Chapter 2
[ 13 ]
See the following table for a listing of NumPy's supported data types:
Type
Type 
code
Description
Range of value
bool
Boolean stored as a byte
True/False
intc
Similar to C int (int32 or int 
64)
intp
Integer used for indexing 
(same as C size_t)
int8, uint8
i1, u1
Signed and unsigned 8-bit 
integer types
int8: (-128 to 127)
uint8: (0 to 255)
int16, 
uint16
i2, u2
Signed and unsigned 16-bit 
integer types 
int16: (-32768 to 32767)
uint16: (0 to 65535)
int32, 
uint32
I4, u4
Signed and unsigned 32-bit 
integer types
int32: (-2147483648 to 
2147483647
uint32: (0 to 4294967295)
int64, 
uinit64
i8, u8
Signed and unsigned 64-bit 
integer types
Int64: (-9223372036854775808 
to 9223372036854775807)
uint64: (0 to 
18446744073709551615)
float16
f2
Half precision float: sign bit, 
5 bits exponent, and 10b bits 
mantissa
float32
f4 / f
Single precision float: sign 
bit, 8 bits exponent, and 23 
bits mantissa
float64
f8 / d
Double precision float: sign 
bit, 11 bits exponent, and 52 
bits mantissa
complex64, 
complex128, 
complex256
c8, 
c16, 
c32
Complex numbers 
represented by two 32-bit, 
64-bit, and 128-bit floats
object
0
Python object type
string_
S
Fixed-length string type
Declare a string dtype with 
length 10, using S10
unicode_
U
Fixed-length Unicode type
Similar to string_ example, we 
have 'U10'

NumPy Arrays and Vectorized Computation
[ 14 ]
We can easily convert or cast an array from one dtype to another using the astype 
method:
>>> a = np.array([1, 2, 3, 4])
>>> a.dtype
dtype('int64')
>>> float_b = a.astype(np.float64)
>>> float_b.dtype
dtype('float64')
The astype function will create a new array with a copy of 
data from an old array, even though the new dtype is similar 
to the old one.
Array creation
There are various functions provided to create an array object. They are very useful 
for us to create and store data in a multidimensional array in different situations.
Now, in the following table we will summarize some of NumPy's common functions 
and their use by examples for array creation:
Function
Description
Example
empty, 
empty_like
Create a new array 
of the given shape 
and type, without 
initializing elements
>>> np.empty([3,2], dtype=np.float64)
array([[0., 0.], [0., 0.], [0., 0.]])
>>> a = np.array([[1, 2], [4, 3]])
>>> np.empty_like(a)
array([[0, 0], [0, 0]])
eye, 
identity
Create a NxN 
identity matrix with 
ones on the diagonal 
and zero elsewhere
>>> np.eye(2, dtype=np.int)
array([[1, 0], [0, 1]])
ones, ones_
like
Create a new array 
with the given shape 
and type, filled with 
1s for all elements
>>> np.ones(5)
array([1., 1., 1., 1., 1.])
>>> np.ones(4, dtype=np.int)
array([1, 1, 1, 1])
>>> x = np.array([[0,1,2], [3,4,5]])
>>> np.ones_like(x)
array([[1, 1, 1],[1, 1, 1]])

Chapter 2
[ 15 ]
Function
Description
Example
zeros, 
zeros_like
This is similar to 
ones, ones_like, 
but initializing 
elements with 0s 
instead
>>> np.zeros(5)
array([0., 0., 0., 0-, 0.])
>>> np.zeros(4, dtype=np.int)
array([0, 0, 0, 0])
>>> x = np.array([[0, 1, 2], [3, 4, 
5]])
>>> np.zeros_like(x)
array([[0, 0, 0],[0, 0, 0]])
arange
Create an array with 
even spaced values 
in a given interval
>>> np.arange(2, 5)
array([2, 3, 4])
>>> np.arange(4, 12, 5)
array([4, 9])
full, full_
like
Create a new array 
with the given shape 
and type, filled with 
a selected value
>>> np.full((2,2), 3, dtype=np.int)
array([[3, 3], [3, 3]])
>>> x = np.ones(3)
>>> np.full_like(x, 2)
array([2., 2., 2.])
array
Create an array from 
the existing data
>>> np.array([[1.1, 2.2, 3.3], [4.4, 
5.5, 6.6]])
array([1.1, 2.2, 3.3], [4.4, 5.5, 
6.6]])
asarray
Convert the input to 
an array
>>> a = [3.14, 2.46]
>>> np.asarray(a)
array([3.14, 2.46])
copy
Return an array copy 
of the given object
>>> a = np.array([[1, 2], [3, 4]])
>>> np.copy(a)
array([[1, 2], [3, 4]])
fromstring
Create 1-D array 
from a string or text 
>>> np.fromstring('3.14 2.17', 
dtype=np.float, sep=' ')
array([3.14, 2.17])

NumPy Arrays and Vectorized Computation
[ 16 ]
Indexing and slicing
As with other Python sequence types, such as lists, it is very easy to access and 
assign a value of each array's element:
>>> a = np.arange(7)
>>> a
array([0, 1, 2, 3, 4, 5, 6])
>>> a[1], a [4], a[-1]
(1, 4, 6)
In Python, array indices start at 0. This is in contrast to Fortran or 
Matlab, where indices begin at 1.
As another example, if our array is multidimensional, we need tuples of integers to 
index an item:
>>> a = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
>>> a[0, 2]      # first row, third column
3
>>> a[0, 2] = 10
>>> a
array([[1, 2, 10], [4, 5, 6], [7, 8, 9]])
>>> b = a[2]
>>> b
array([7, 8, 9])
>>> c = a[:2]
>>> c
array([[1, 2, 10], [4, 5, 6]])
We call b and c as array slices, which are views on the original one. It means that the 
data is not copied to b or c, and whenever we modify their values, it will be reflected 
in the array a as well:
>>> b[-1] = 11
>>> a
array([[1, 2, 10], [4, 5, 6], [7, 8, 11]])
We use a colon (:) character to take the entire axis when we 
omit the index number.

Chapter 2
[ 17 ]
Fancy indexing
Besides indexing with slices, NumPy also supports indexing with Boolean or integer 
arrays (masks). This method is called fancy indexing. It creates copies, not views.
First, we take a look at an example of indexing with a Boolean mask array:
>>> a = np.array([3, 5, 1, 10])
>>> b = (a % 5 == 0)
>>> b
array([False, True, False, True], dtype=bool)
>>> c = np.array([[0, 1], [2, 3], [4, 5], [6, 7]])
>>> c[b]
array([[2, 3], [6, 7]])
The second example is an illustration of using integer masks on arrays:
>>> a = np.array([[1, 2, 3, 4], 
 [5, 6, 7, 8], 
 [9, 10, 11, 12],
 [13, 14, 15, 16]])
>>> a[[2, 1]]
array([[9, 10, 11, 12], [5, 6, 7, 8]])
>>> a[[-2, -1]]          # select rows from the end
array([[ 9, 10, 11, 12], [13, 14, 15, 16]])
>>> a[[2, 3], [0, 1]]    # take elements at (2, 0) and (3, 1)
array([9, 14])
The mask array must have the same length as the axis that 
it's indexing.
Downloading the example code
You can download the example code files for all Packt books 
you have purchased from your account at http://www.
packtpub.com. If you purchased this book elsewhere, you can 
visit http://www.packtpub.com/support and register to 
have the files e-mailed directly to you.

NumPy Arrays and Vectorized Computation
[ 18 ]
Numerical operations on arrays
We are getting familiar with creating and accessing ndarrays. Now, we continue to 
the next step, applying some mathematical operations to array data without writing 
any for loops, of course, with higher performance.
Scalar operations will propagate the value to each element of the array:
>>> a = np.ones(4)
>>> a * 2
array([2., 2., 2., 2.])
>>> a + 3
array([4., 4., 4., 4.])
All arithmetic operations between arrays apply the operation element wise:
>>> a = np.ones([2, 4])
>>> a * a
array([[1., 1., 1., 1.], [1., 1., 1., 1.]])
>>> a + a
array([[2., 2., 2., 2.], [2., 2., 2., 2.]])
Also, here are some examples of comparisons and logical operations:
>>> a = np.array([1, 2, 3, 4])
>>> b = np.array([1, 1, 5, 3])
>>> a == b
array([True, False, False, False], dtype=bool)
>>> np.array_equal(a, b)      # array-wise comparison
False
>>> c = np.array([1, 0])
>>> d = np.array([1, 1])
>>> np.logical_and(c, d)      # logical operations
array([True, False])

Chapter 2
[ 19 ]
Array functions
Many helpful array functions are supported in NumPy for analyzing data. We will 
list some part of them that are common in use. Firstly, the transposing function 
is another kind of reshaping form that returns a view on the original data array 
without copying anything:
>>> a = np.array([[0, 5, 10], [20, 25, 30]])
>>> a.reshape(3, 2)
array([[0, 5], [10, 20], [25, 30]])
>>> a.T
array([[0, 20], [5, 25], [10, 30]])
In general, we have the swapaxes method that takes a pair of axis numbers and 
returns a view on the data, without making a copy:
>>> a = np.array([[[0, 1, 2], [3, 4, 5]], 
 [[6, 7, 8], [9, 10, 11]]])
>>> a.swapaxes(1, 2)
array([[[0, 3],
    [1, 4],
    [2, 5]],
   [[6, 9],
    [7, 10],
    [8, 11]]])
The transposing function is used to do matrix computations; for example, computing 
the inner matrix product XT.X using np.dot:
>>> a = np.array([[1, 2, 3],[4,5,6]])
>>> np.dot(a.T, a)
array([[17, 22, 27],
   [22, 29, 36],
   [27, 36, 45]])

NumPy Arrays and Vectorized Computation
[ 20 ]
Sorting data in an array is also an important demand in processing data. Let's take a 
look at some sorting functions and their use:
>>> a = np.array ([[6, 34, 1, 6], [0, 5, 2, -1]])
>>> np.sort(a)     # sort along the last axis
array([[1, 6, 6, 34], [-1, 0, 2, 5]])
>>> np.sort(a, axis=0)    # sort along the first axis
array([[0, 5, 1, -1], [6, 34, 2, 6]])
>>> b = np.argsort(a)    # fancy indexing of sorted array
>>> b
array([[2, 0, 3, 1], [3, 0, 2, 1]])
>>> a[0][b[0]]
array([1, 6, 6, 34])
>>> np.argmax(a)    # get index of maximum element
1
See the following table for a listing of array functions:
Function
Description
Example
sin, cos, tan, 
cosh, sinh, tanh, 
arcos, arctan, 
deg2rad
Trigonometric 
and hyperbolic 
functions
>>> a = np.array([0.,30., 45.])
>>> np.sin(a * np.pi / 180)
array([0., 0.5, 0.7071678])
around, round, 
rint, fix, floor, 
ceil, trunc
Rounding elements 
of an array to the 
given or nearest 
number
>>> a = np.array([0.34, 1.65])
>>> np.round(a)
array([0., 2.])
sqrt, square, exp, 
expm1, exp2, log, 
log10, log1p, 
logaddexp
Computing the 
exponents and 
logarithms of an 
array
>>> np.exp(np.array([2.25, 
3.16]))
array([9.4877, 23.5705])

Chapter 2
[ 21 ]
Function
Description
Example
add, negative, 
multiply, devide, 
power, substract, 
mod, modf, 
remainder
Set of arithmetic 
functions on arrays
>>> a = np.arange(6)
>>> x1 = a.reshape(2,3)
>>> x2 = np.arange(3)
>>> np.multiply(x1, x2)
array([[0,1,4],[0,4,10]])
greater, 
greater_equal, 
less, less_equal, 
equal, not_equal
Perform 
elementwise 
comparison: >, >=, 
<, <=, ==, !=
>>> np.greater(x1, x2)
array([[False, False, False], 
[True, True, True]], dtype = 
bool)
Data processing using arrays
With the NumPy package, we can easily solve many kinds of data processing 
tasks without writing complex loops. It is very helpful for us to control our code 
as well as the performance of the program. In this part, we want to introduce some 
mathematical and statistical functions.
See the following table for a listing of mathematical and statistical functions:
Function
Description
Example
sum
Calculate the sum 
of all the elements 
in an array or 
along the axis
>>> a = np.array([[2,4], [3,5]])
>>> np.sum(a, axis=0)
array([5, 9])
prod
Compute the 
product of array 
elements over the 
given axis
>>> np.prod(a, axis=1)
array([8, 15])
diff
Calculate the 
discrete difference 
along the given 
axis
>>> np.diff(a, axis=0)
array([[1,1]])
gradient
Return the 
gradient of an 
array
>>> np.gradient(a)
[array([[1., 1.], [1., 1.]]), 
array([[2., 2.], [2., 2.]])]
cross
Return the cross 
product of two 
arrays
>>> b = np.array([[1,2], [3,4]])
>>> np.cross(a,b)
array([0, -3])

NumPy Arrays and Vectorized Computation
[ 22 ]
Function
Description
Example
std, var
Return standard 
deviation and 
variance of arrays 
>>> np.std(a)
1.1180339
>>> np.var(a)
1.25
mean
Calculate 
arithmetic mean 
of an array
>>> np.mean(a)
3.5
where
Return elements, 
either from x or 
y, that satisfy a 
condition
>>> np.where([[True, True], [False, 
True]], [[1,2],[3,4]], [[5,6],[7,8]])
array([[1,2], [7, 4]])
unique
Return the sorted 
unique values in 
an array
>>> id = np.array(['a', 'b', 'c', 
'c', 'd'])
>>> np.unique(id)
array(['a', 'b', 'c', 'd'], 
dtype='|S1')
intersect1d
Compute the 
sorted and 
common elements 
in two arrays
>>> a = np.array(['a', 'b', 'a', 'c', 
'd', 'c'])
>>> b = np.array(['a', 'xyz', 'klm', 
'd'])
>>> np.intersect1d(a,b)
array(['a', 'd'], dtype='|S3')
Loading and saving data
We can also save and load data to and from a disk, either in text or binary format,  
by using different supported functions in NumPy package.
Saving an array
Arrays are saved by default in an uncompressed raw binary format, with the file 
extension .npy by the np.save function:
>>> a = np.array([[0, 1, 2], [3, 4, 5]])
>>> np.save('test1.npy', a)

Chapter 2
[ 23 ]
The library automatically assigns the .npy extension, if we omit it.
If we want to store several arrays into a single file in an uncompressed .npz format, 
we can use the np.savez function, as shown in the following example:
>>> a = np.arange(4)
>>> b = np.arange(7)
>>> np.savez('test2.npz', arr0=a, arr1=b)
The .npz file is a zipped archive of files named after the variables they contain. 
When we load an .npz file, we get back a dictionary-like object that can be queried 
for its lists of arrays:
>>> dic = np.load('test2.npz')
>>> dic['arr0']
array([0, 1, 2, 3])
Another way to save array data into a file is using the np.savetxt function that 
allows us to set format properties in the output file:
>>> x = np.arange(4)
>>> # e.g., set comma as separator between elements
>>> np.savetxt('test3.out', x, delimiter=',')
Loading an array
We have two common functions such as np.load and np.loadtxt, which 
correspond to the saving functions, for loading an array:
>>> np.load('test1.npy')
array([[0, 1, 2], [3, 4, 5]])
>>> np.loadtxt('test3.out', delimiter=',')
array([0., 1., 2., 3.])
Similar to the np.savetxt function, the np.loadtxt function also has a lot of options 
for loading an array from a text file.

NumPy Arrays and Vectorized Computation
[ 24 ]
Linear algebra with NumPy
Linear algebra is a branch of mathematics concerned with vector spaces and the 
mappings between those spaces. NumPy has a package called linalg that supports 
powerful linear algebra functions. We can use these functions to find eigenvalues 
and eigenvectors or to perform singular value decomposition:
>>> A = np.array([[1, 4, 6],
    [5, 2, 2],
    [-1, 6, 8]])
>>> w, v = np.linalg.eig(A)
>>> w                           # eigenvalues
array([-0.111 + 1.5756j, -0.111 – 1.5756j, 11.222+0.j])
>>> v                           # eigenvector
array([[-0.0981 + 0.2726j, -0.0981 – 0.2726j, 0.5764+0.j],
    [0.7683+0.j, 0.7683-0.j, 0.4591+0.j],
    [-0.5656 – 0.0762j, -0.5656 + 0.00763j, 0.6759+0.j]])
The function is implemented using the geev Lapack routines that compute the 
eigenvalues and eigenvectors of general square matrices.
Another common problem is solving linear systems such as Ax = b with A as a 
matrix and x and b as vectors. The problem can be solved easily using the  
numpy.linalg.solve function:
>>> A = np.array([[1, 4, 6], [5, 2, 2], [-1, 6, 8]])
>>> b = np.array([[1], [2], [3]])
>>> x = np.linalg.solve(A, b)
>>> x
array([[-1.77635e-16], [2.5], [-1.5]])
The following table will summarise some commonly used functions in the numpy.
linalg package:
Function
Description
Example
dot
Calculate the dot 
product of two arrays
>>> a = np.array([[1, 0],[0, 1]])
>>> b = np.array( [[4, 1],[2, 2]])
>>> np.dot(a,b)
array([[4, 1],[2, 2]])

Chapter 2
[ 25 ]
Function
Description
Example
inner, outer
Calculate the inner and 
outer product of two 
arrays
>>> a = np.array([1, 1, 1])
>>> b = np.array([3, 5, 1])
>>> np.inner(a,b)
9
linalg.norm
Find a matrix or vector 
norm
>>> a = np.arange(3)
>>> np.linalg.norm(a)
2.23606
linalg.det
Compute the 
determinant of an array
>>> a = np.array([[1,2],[3,4]])
>>> np.linalg.det(a)
-2.0
linalg.inv
Compute the inverse of 
a matrix
>>> a = np.array([[1,2],[3,4]])
>>> np.linalg.inv(a)
array([[-2., 1.],[1.5, -0.5]])
linalg.qr
Calculate the QR 
decomposition
>>> a = np.array([[1,2],[3,4]])
>>> np.linalg.qr(a)
(array([[0.316, 0.948], [0.948, 
0.316]]), array([[ 3.162, 4.427], 
[ 0., 0.632]]))
linalg.cond
Compute the condition 
number of a matrix
>>> a = np.array([[1,3],[2,4]])
>>> np.linalg.cond(a)
14.933034
trace
Compute the sum of the 
diagonal element
>>> np.trace(np.arange(6)).
reshape(2,3))
4
NumPy random numbers
An important part of any simulation is the ability to generate random numbers.  
For this purpose, NumPy provides various routines in the submodule random.  
It uses a particular algorithm, called the Mersenne Twister, to generate 
pseudorandom numbers.

NumPy Arrays and Vectorized Computation
[ 26 ]
First, we need to define a seed that makes the random numbers predictable.  
When the value is reset, the same numbers will appear every time. If we do not 
assign the seed, NumPy automatically selects a random seed value based on the 
system's random number generator device or on the clock:
>>> np.random.seed(20)
An array of random numbers in the [0.0, 1.0] interval can be generated as 
follows:
>>> np.random.rand(5)
array([0.5881308, 0.89771373, 0.89153073, 0.81583748, 
         0.03588959])
>>> np.random.rand(5)
array([0.69175758, 0.37868094, 0.51851095, 0.65795147,  
       0.19385022])
>>> np.random.seed(20)    # reset seed number
>>> np.random.rand(5)
array([0.5881308, 0.89771373, 0.89153073, 0.81583748,  
       0.03588959])
If we want to generate random integers in the half-open interval [min, max],  
we can user the randint(min, max, length) function:
>>> np.random.randint(10, 20, 5)
array([17, 12, 10, 16, 18])
NumPy also provides for many other distributions, including the Beta,  
bionomial, chi-square, Dirichlet, exponential, F, Gamma, geometric, or Gumbel. 

Chapter 2
[ 27 ]
The following table will list some distribution functions and give examples for 
generating random numbers:
Function
Description
Example
binomial
Draw samples from a 
binomial distribution 
(n: number of trials, p: 
probability)
>>> n, p = 100, 0.2
>>> np.random.binomial(n, p, 3)
array([17, 14, 23])
dirichlet
Draw samples using a 
Dirichlet distribution
>>> np.random.
dirichlet(alpha=(2,3), size=3)
array([[0.519, 0.480], [0.639, 
0.36],
 [0.838, 0.161]])
poisson
Draw samples from a 
Poisson distribution
>>> np.random.poisson(lam=2, size= 
2)
array([4,1])
normal
Draw samples using 
a normal Gaussian 
distribution
>>> np.random.normal
(loc=2.5, scale=0.3, size=3)
array([2.4436, 2.849, 2.741)
uniform
Draw samples using a 
uniform distribution
>>> np.random.uniform(
low=0.5, high=2.5, size=3)
array([1.38, 1.04, 2.19[)
We can also use the random number generation to shuffle items in a list. Sometimes 
this is useful when we want to sort a list in a random order:
>>> a = np.arange(10)
>>> np.random.shuffle(a)
>>> a
array([7, 6, 3, 1, 4, 2, 5, 0, 9, 8])

NumPy Arrays and Vectorized Computation
[ 28 ]
The following figure shows two distributions, binomial and poisson , side by side 
with various parameters (the visualization was created with matplotlib, which will 
be covered in Chapter 4, Data Visualization):
Summary
In this chapter, we covered a lot of information related to the NumPy package, 
especially commonly used functions that are very helpful to process and analyze 
data in ndarray. Firstly, we learned the properties and data type of ndarray in the 
NumPy package. Secondly, we focused on how to create and manipulate an ndarray 
in different ways, such as conversion from other structures, reading an array from 
disk, or just generating a new array with given values. Thirdly, we studied how  
to access and control the value of each element in ndarray by using indexing  
and slicing.

Chapter 2
[ 29 ]
Then, we are getting familiar with some common functions and operations  
on ndarray.
And finally, we continue with some advance functions that are related to  
statistic, linear algebra and sampling data. Those functions play important  
role in data analysis.
However, while NumPy by itself does not provide very much high-level data 
analytical functionality, having an understanding of it will help you use tools such  
as Pandas much more effectively. This tool will be discussed in the next chapter.
Practice exercises
Exercise 1: Using an array creation function, let's try to create arrays variable in the 
following situations:
•	
Create ndarray from the existing data
•	
Initialize ndarray which elements are filled with ones, zeros, or a  
given interval
•	
Loading and saving data from a file to an ndarray
Exercise 2: What is the difference between np.dot(a, b) and (a*b)?
Exercise 3: Consider the vector [1, 2, 3, 4, 5] building a new vector with four 
consecutive zeros interleaved between each value.
Exercise 4: Taking the data example file chapter2-data.txt, which includes 
information on a system log, solves the following tasks:
•	
Try to build an ndarray from the data file
•	
Statistic frequency of each device type in the built matrix
•	
List unique OS that appears in the data log
•	
Sort user by provinceID and count the number of users in each province


[ 31 ]
Data Analysis with Pandas
In this chapter, we will explore another data analysis library called Pandas.  
The goal of this chapter is to give you some basic knowledge and concrete  
examples for getting started with Pandas.
An overview of the Pandas package
Pandas is a Python package that supports fast, flexible, and expressive data 
structures, as well as computing functions for data analysis. The following  
are some prominent features that Pandas supports:
•	
Data structure with labeled axes. This makes the program clean and clear 
and avoids common errors from misaligned data.
•	
Flexible handling of missing data.
•	
Intelligent label-based slicing, fancy indexing, and subset creation of  
large datasets.
•	
Powerful arithmetic operations and statistical computations on a custom  
axis via axis label.
•	
Robust input and output support for loading or saving data from and to  
files, databases, or HDF5 format.
Related to Pandas installation, we recommend an easy way, that is to install it as 
a part of Anaconda, a cross-platform distribution for data analysis and scientific 
computing. You can refer to the reference at http://docs.continuum.io/
anaconda/ to download and install the library.

Data Analysis with Pandas
[ 32 ]
After installation, we can use it like other Python packages. Firstly, we have to 
import the following packages at the beginning of the program:
>>> import pandas as pd
>>> import numpy as np
The Pandas data structure
Let's first get acquainted with two of Pandas' primary data structures: the Series  
and the DataFrame. They can handle the majority of use cases in finance, statistic, 
social science, and many areas of engineering.
Series
A Series is a one-dimensional object similar to an array, list, or column in table.  
Each item in a Series is assigned to an entry in an index:
>>> s1 = pd.Series(np.random.rand(4),
                   index=['a', 'b', 'c', 'd'])
>>> s1
a    0.6122
b    0.98096
c    0.3350
d    0.7221
dtype: float64
By default, if no index is passed, it will be created to have values ranging from 0 to 
N-1, where N is the length of the Series:
>>> s2 = pd.Series(np.random.rand(4))
>>> s2
0    0.6913
1    0.8487
2    0.8627
3    0.7286
dtype: float64

Chapter 3
[ 33 ]
We can access the value of a Series by using the index:
>>> s1['c']
0.3350
>>>s1['c'] = 3.14
>>> s1['c', 'a', 'b']
c    3.14
a    0.6122
b    0.98096
This accessing method is similar to a Python dictionary. Therefore, Pandas also 
allows us to initialize a Series object directly from a Python dictionary:
>>> s3 = pd.Series({'001': 'Nam', '002': 'Mary',
                    '003': 'Peter'})
>>> s3
001    Nam
002    Mary
003    Peter
dtype: object
Sometimes, we want to filter or rename the index of a Series created from a Python 
dictionary. At such times, we can pass the selected index list directly to the initial 
function, similarly to the process in the above example. Only elements that exist in 
the index list will be in the Series object. Conversely, indexes that are missing in the 
dictionary are initialized to default NaN values by Pandas:
>>> s4 = pd.Series({'001': 'Nam', '002': 'Mary',
                    '003': 'Peter'}, index=[
                    '002', '001', '024', '065'])
>>> s4
002    Mary
001    Nam
024    NaN
065    NaN
dtype:   object
ect

Data Analysis with Pandas
[ 34 ]
The library also supports functions that detect missing data:
>>> pd.isnull(s4)
002    False
001    False
024    True
065    True
dtype: bool
Similarly, we can also initialize a Series from a scalar value:
>>> s5 = pd.Series(2.71, index=['x', 'y'])
>>> s5
x    2.71
y    2.71
dtype: float64
A Series object can be initialized with NumPy objects as well, such as ndarray. 
Moreover, Pandas can automatically align data indexed in different ways in 
arithmetic operations:
>>> s6 = pd.Series(np.array([2.71, 3.14]), index=['z', 'y'])
>>> s6
z    2.71
y    3.14
dtype: float64
>>> s5 + s6
x    NaN
y    5.85
z    NaN
dtype: float64
The DataFrame
The DataFrame is a tabular data structure comprising a set of ordered columns and 
rows. It can be thought of as a group of Series objects that share an index (the column 
names). There are a number of ways to initialize a DataFrame object. Firstly, let's take 
a look at the common example of creating DataFrame from a dictionary of lists:
>>> data = {'Year': [2000, 2005, 2010, 2014],
         'Median_Age': [24.2, 26.4, 28.5, 30.3],

Chapter 3
[ 35 ]
         'Density': [244, 256, 268, 279]}
>>> df1 = pd.DataFrame(data)
>>> df1
    Density    Median_Age    Year
0  244        24.2        2000
1  256        26.4        2005
2  268        28.5        2010
3  279        30.3        2014
By default, the DataFrame constructor will order the column alphabetically. We can 
edit the default order by passing the column's attribute to the initializing function:
>>> df2 = pd.DataFrame(data, columns=['Year', 'Density', 
                                      'Median_Age'])
>>> df2
    Year    Density    Median_Age
0    2000    244        24.2
1    2005    256        26.4
2    2010    268        28.5
3    2014    279        30.3
>>> df2.index
Int64Index([0, 1, 2, 3], dtype='int64')
We can provide the index labels of a DataFrame similar to a Series:
>>> df3 = pd.DataFrame(data, columns=['Year', 'Density',  
                   'Median_Age'], index=['a', 'b', 'c', 'd'])
>>> df3.index
Index([u'a', u'b', u'c', u'd'], dtype='object')
We can construct a DataFrame out of nested lists as well:
>>> df4 = pd.DataFrame([
    ['Peter', 16, 'pupil', 'TN', 'M', None],
    ['Mary', 21, 'student', 'SG', 'F', None],
    ['Nam', 22, 'student', 'HN', 'M', None],
    ['Mai', 31, 'nurse', 'SG', 'F', None],
    ['John', 28, 'laywer', 'SG', 'M', None]],
columns=['name', 'age', 'career', 'province', 'sex', 'award'])

Data Analysis with Pandas
[ 36 ]
Columns can be accessed by column name as a Series can, either by dictionary-like 
notation or as an attribute, if the column name is a syntactically valid attribute name:
>>> df4.name    # or df4['name'] 
0    Peter
1    Mary
2    Nam
3    Mai
4    John
Name: name, dtype: object
To modify or append a new column to the created DataFrame, we specify the 
column name and the value we want to assign:
>>> df4['award'] = None
>>> df4
    name age   career province  sex award
0  Peter  16    pupil       TN    M  None
1    Mary  21  student       SG    F  None
2    Nam   22  student       HN  M  None
3    Mai    31    nurse        SG    F    None
4    John    28    lawer        SG    M    None
Using a couple of methods, rows can be retrieved by position or name:
>>> df4.ix[1]
name           Mary
age              21
career      student
province         SG
sex               F
award          None
Name: 1, dtype: object
A DataFrame object can also be created from different data structures such as a list 
of dictionaries, a dictionary of Series, or a record array. The method to initialize a 
DataFrame object is similar to the examples above.

Chapter 3
[ 37 ]
Another common case is to provide a DataFrame with data from a location such as 
a text file. In this situation, we use the read_csv function that expects the column 
separator to be a comma, by default. However, we can change that by using the sep 
parameter:
# person.csv file
name,age,career,province,sex
Peter,16,pupil,TN,M
Mary,21,student,SG,F
Nam,22,student,HN,M
Mai,31,nurse,SG,F
John,28,lawer,SG,M
# loading person.cvs into a DataFrame
>>> df4 = pd.read_csv('person.csv')
>>> df4
     name   age   career   province  sex
0    Peter    16    pupil       TN        M
1    Mary     21    student     SG       F
2    Nam      22    student     HN       M
3    Mai      31    nurse       SG       F
4    John     28    laywer      SG       M
While reading a data file, we sometimes want to skip a line or an invalid value. 
As for Pandas 0.16.2, read_csv supports over 50 parameters for controlling the 
loading process. Some common useful parameters are as follows:
•	
sep: This is a delimiter between columns. The default is comma symbol.
•	
dtype: This is a data type for data or columns.
•	
header: This sets row numbers to use as the column names.
•	
skiprows: This skips line numbers to skip at the start of the file.
•	
error_bad_lines: This shows invalid lines (too many fields) that will, by 
default, cause an exception, such that no DataFrame will be returned. If we 
set the value of this parameter as false, the bad lines will be skipped.
Moreover, Pandas also has support for reading and writing a DataFrame directly 
from or to a database such as the read_frame or write_frame function within the 
Pandas module. We will come back to these methods later in this chapter.

Data Analysis with Pandas
[ 38 ]
The essential basic functionality
Pandas supports many essential functionalities that are useful to manipulate Pandas 
data structures. In this book, we will focus on the most important features regarding 
exploration and analysis.
Reindexing and altering labels
Reindex is a critical method in the Pandas data structures. It confirms whether  
the new or modified data satisfies a given set of labels along a particular axis of 
Pandas object.
First, let's view a reindex example on a Series object:
>>> s2.reindex([0, 2, 'b', 3])
0    0.6913
2    0.8627
b    NaN
3    0.7286
dtype: float64
When reindexed labels do not exist in the data object, a default value of NaN will be 
automatically assigned to the position; this holds true for the DataFrame case as well:
>>> df1.reindex(index=[0, 2, 'b', 3],
        columns=['Density', 'Year', 'Median_Age','C'])
   Density  Year  Median_Age        C
0      244  2000        24.2      NaN
2      268  2010        28.5      NaN
b      NaN   NaN         NaN      NaN
3      279  2014        30.3      NaN

Chapter 3
[ 39 ]
We can change the NaN value in the missing index case to a custom value by setting 
the fill_value parameter. Let us take a look at the arguments that the reindex 
function supports, as shown in the following table: 
Argument
Description
index
This is the new labels/index to conform to.
method
This is the method to use for filling holes in a reindexed object. 
The default setting is unfill gaps.
pad/ffill: fill values forward
backfill/bfill: fill values backward
nearest: use the nearest value to fill the gap
copy
This return a new object. The default setting is true.
level
The matches index values on the passed multiple index level.
fill_value
This is the value to use for missing values. The default setting is 
NaN.
limit
This is the maximum size gap to fill in forward or backward 
method.
Head and tail
In common data analysis situations, our data structure objects contain many columns 
and a large number of rows. Therefore, we cannot view or load all information of 
the objects. Pandas supports functions that allow us to inspect a small sample. By 
default, the functions return five elements, but we can set a custom number as well. 
The following example shows how to display the first five and the last three rows of 
a longer Series:
>>> s7 = pd.Series(np.random.rand(10000))
>>> s7.head()
0    0.631059
1    0.766085
2    0.066891
3    0.867591
4    0.339678

Data Analysis with Pandas
[ 40 ]
dtype: float64
>>> s7.tail(3)
9997    0.412178
9998    0.800711
9999    0.438344
dtype: float64
We can also use these functions for DataFrame objects in the same way.
Binary operations
Firstly, we will consider arithmetic operations between objects. In different indexes 
objects case, the expected result will be the union of the index pairs. We will not 
explain this again because we had an example about it in the above section (s5 + 
s6). This time, we will show another example with a DataFrame:
>>> df5 = pd.DataFrame(np.arange(9).reshape(3,3),0
                       columns=['a','b','c'])
>>> df5
   a  b  c
0  0  1  2
1  3  4  5
2  6  7  8
>>> df6 = pd.DataFrame(np.arange(8).reshape(2,4), 
                      columns=['a','b','c','d'])
>>> df6
   a  b  c  d
0  0  1  2  3
1  4  5  6  7
>>> df5 + df6
    a   b   c   d
0   0   2   4 NaN
1   7   9  11 NaN
2   NaN NaN NaN NaN

Chapter 3
[ 41 ]
The mechanisms for returning the result between two kinds of data structure are 
similar. A problem that we need to consider is the missing data between objects. In 
this case, if we want to fill with a fixed value, such as 0, we can use the arithmetic 
functions such as add, sub, div, and mul, and the function's supported parameters 
such as fill_value:
>>> df7 = df5.add(df6, fill_value=0)
>>> df7
   a  b   c   d
0  0  2   4   3
1  7  9  11   7
2  6  7   8   NaN
Next, we will discuss comparison operations between data objects. We have some 
supported functions such as equal (eq), not equal (ne), greater than (gt), less than 
(lt), less equal (le), and greater equal (ge). Here is an example:
>>> df5.eq(df6)
       a      b      c      d
0   True   True   True  False
1  False  False  False  False
2  False  False  False  False
Functional statistics
The supported statistics method of a library is really important in data analysis. To 
get inside a big data object, we need to know some summarized information such 
as mean, sum, or quantile. Pandas supports a large number of methods to compute 
them. Let's consider a simple example of calculating the sum information of df5, 
which is a DataFrame object:
>>> df5.sum()
a     9
b    12
c    15
dtype: int64

Data Analysis with Pandas
[ 42 ]
When we do not specify which axis we want to calculate sum information, by default, 
the function will calculate on index axis, which is axis 0:
•	
Series: We do not need to specify the axis.
•	
DataFrame: Columns (axis = 1) or index (axis = 0). The default setting is 
axis 0.
We also have the skipna parameter that allows us to decide whether to exclude 
missing data or not. By default, it is set as true:
>>> df7.sum(skipna=False)
a    13
b    18
c    23
d   NaN
dtype: float64
Another function that we want to consider is describe(). It is very convenient for 
us to summarize most of the statistical information of a data structure such as the 
Series and DataFrame, as well:
>>> df5.describe()
         a    b    c
count  3.0  3.0  3.0
mean   3.0  4.0  5.0
std    3.0  3.0  3.0
min    0.0  1.0  2.0
25%    1.5  2.5  3.5
50%    3.0  4.0  5.0
75%    4.5  5.5  6.5
max    6.0  7.0  8.0
We can specify percentiles to include or exclude in the output by using the 
percentiles parameter; for example, consider the following:
>>> df5.describe(percentiles=[0.5, 0.8])
         a    b    c
count  3.0  3.0  3.0
mean   3.0  4.0  5.0
std    3.0  3.0  3.0

Chapter 3
[ 43 ]
min    0.0  1.0  2.0
50%    3.0  4.0  5.0
80%    4.8  5.8  6.8
max    6.0  7.0  8.0
Here, we have a summary table for common supported statistics functions  
in Pandas:
Function
Description
idxmin(axis), 
idxmax(axis)
This compute the index labels with the minimum 
or maximum corresponding values.
value_counts()
This compute the frequency of unique values.
count()
This return the number of non-null values in a 
data object.
mean(), median(), 
min(), max()
This return mean, median, minimum, and 
maximum values of an axis in a data object.
std(), var(), sem()
These return the standard deviation, variance, 
and standard error of mean.
abs()
This gets the absolute value of a data object.
Function application
Pandas supports function application that allows us to apply some functions 
supported in other packages such as NumPy or our own functions on data structure 
objects. Here, we illustrate two examples of these cases, firstly, using apply to 
execute the std() function, which is the standard deviation calculating function of 
the NumPy package:
>>> df5.apply(np.std, axis=1)    # default: axis=0
0    0.816497
1    0.816497
2    0.816497
dtype: float64

Data Analysis with Pandas
[ 44 ]
Secondly, if we want to apply a formula to a data object, we can also useapply 
function by following these steps:
1.	 Define the function or formula that you want to apply on a data object.
2.	 Call the defined function or formula via apply. In this step, we also need  
to figure out the axis that we want to apply the calculation to:
>>> f = lambda x: x.max() – x.min()    # step 1
>>> df5.apply(f, axis=1)               # step 2
0    2
1    2
2    2
dtype: int64
>>> def sigmoid(x):
    return 1/(1 + np.exp(x))
>>> df5.apply(sigmoid)
     a           b         c
0  0.500000  0.268941  0.119203
1  0.047426  0.017986  0.006693
2  0.002473  0.000911  0.000335
Sorting
There are two kinds of sorting method that we are interested in: sorting by row or 
column index and sorting by data value.
Firstly, we will consider methods for sorting by row and column index. In this case, 
we have the sort_index () function. We also have axis parameter to set whether 
the function should sort by row or column. The ascending option with the true or 
false value will allow us to sort data in ascending or descending order. The default 
setting for this option is true:
>>> df7 = pd.DataFrame(np.arange(12).reshape(3,4),  
                       columns=['b', 'd', 'a', 'c'],
                       index=['x', 'y', 'z'])
>>> df7
   b  d   a   c
x  0  1   2   3
y  4  5   6   7
z  8  9  10  11

Chapter 3
[ 45 ]
>>> df7.sort_index(axis=1)
    a  b   c  d
x   2  0   3  1
y   6  4   7  5
z  10  8  11  9
Series has a method order that sorts by value. For NaN values in the object, we can 
also have a special treatment via the na_position option:
>>> s4.order(na_position='first')
024     NaN
065     NaN
002    Mary
001     Nam
dtype: object
>>> s4
002    Mary
001     Nam
024     NaN
065     NaN
dtype: object
Besides that, Series also has the sort() function that sorts data by value. However, 
the function will not return a copy of the sorted data:
>>> s4.sort(na_position='first')
>>> s4
024     NaN
065     NaN
002    Mary
001     Nam
dtype: object

Data Analysis with Pandas
[ 46 ]
If we want to apply sort function to a DataFrame object, we need to figure out which 
columns or rows will be sorted:
>>> df7.sort(['b', 'd'], ascending=False)
   b  d   a   c
z  8  9  10  11
y  4  5   6   7
x  0  1   2   3
If we do not want to automatically save the sorting result to the current data object, 
we can change the setting of the inplace parameter to False.
Indexing and selecting data
In this section, we will focus on how to get, set, or slice subsets of Pandas data 
structure objects. As we learned in previous sections, Series or DataFrame objects 
have axis labeling information. This information can be used to identify items that 
we want to select or assign a new value to in the object:
>>> s4[['024', '002']]    # selecting data of Series object
024     NaN
002    Mary
dtype: object
>>> s4[['024', '002']] = 'unknown' # assigning data
>>> s4
024    unknown
065        NaN
002    unknown
001        Nam
dtype: object
If the data object is a DataFrame structure, we can also proceed in a similar way:
>>> df5[['b', 'c']]
   b  c
0  1  2
1  4  5
2  7  8

Chapter 3
[ 47 ]
For label indexing on the rows of DataFrame, we use the ix function that enables us 
to select a set of rows and columns in the object. There are two parameters that we 
need to specify: the row and column labels that we want to get. By default, if we do 
not specify the selected column names, the function will return selected rows with all 
columns in the object:
>>> df5.ix[0]
a    0
b    1
c    2
Name: 0, dtype: int64
>>> df5.ix[0, 1:3]
b    1
c    2
Name: 0, dtype: int64
Moreover, we have many ways to select and edit data contained in a Pandas object. 
We summarize these functions in the following table:
Method
Description
icol, irow
This selects a single row or column by integer location.
get_value, set_value
This selects or sets a single value of a data object by row 
or column label.
xs
This selects a single column or row as a Series by label.
Pandas data objects may contain duplicate indices. In this case, 
when we get or set a data value via index label, it will affect all 
rows or columns that have the same selected index name.
Computational tools
Let's start with correlation and covariance computation between two data objects. 
Both the Series and DataFrame have a cov method. On a DataFrame object, this 
method will compute the covariance between the Series inside the object:
>>> s1 = pd.Series(np.random.rand(3))
>>> s1

Data Analysis with Pandas
[ 48 ]
0    0.460324
1    0.993279
2    0.032957
dtype: float64
>>> s2 = pd.Series(np.random.rand(3))
>>> s2
0    0.777509
1    0.573716
2    0.664212
dtype: float64
>>> s1.cov(s2)
-0.024516360159045424
>>> df8 = pd.DataFrame(np.random.rand(12).reshape(4,3),  
                       columns=['a','b','c'])
>>> df8
          a         b         c
0  0.200049  0.070034  0.978615
1  0.293063  0.609812  0.788773
2  0.853431  0.243656  0.978057
0.985584  0.500765  0.481180
>>> df8.cov()
          a         b         c
a  0.155307  0.021273 -0.048449
b  0.021273  0.059925 -0.040029
c -0.048449 -0.040029  0.055067
Usage of the correlation method is similar to the covariance method. It computes the 
correlation between Series inside a data object in case the data object is a DataFrame. 
However, we need to specify which method will be used to compute the correlations. 
The available methods are pearson, kendall, and spearman. By default, the function 
applies the spearman method:
>>> df8.corr(method = 'spearman')
     a    b    c
a  1.0  0.4 -0.8
b  0.4  1.0 -0.8
c -0.8 -0.8  1.0

Chapter 3
[ 49 ]
We also have the corrwith function that supports calculating correlations between 
Series that have the same label contained in different DataFrame objects:
>>> df9 = pd.DataFrame(np.arange(8).reshape(4,2), 
                       columns=['a', 'b'])
>>> df9
   a  b
0  0  1
1  2  3
2  4  5
3  6  7
>>> df8.corrwith(df9)
a    0.955567
b    0.488370
c         NaN
dtype: float64
Working with missing data
In this section, we will discuss missing, NaN, or null values, in Pandas data 
structures. It is a very common situation to arrive with missing data in an object.  
One such case that creates missing data is reindexing:
>>> df8 = pd.DataFrame(np.arange(12).reshape(4,3),  
                       columns=['a', 'b', 'c'])
   a   b   c
0  0   1   2
1  3   4   5
2  6   7   8
3  9  10  11
>>> df9 = df8.reindex(columns = ['a', 'b', 'c', 'd'])
   a   b   c   d
0  0   1   2 NaN
1  3   4   5 NaN
2  6   7   8 NaN
4  9  10  11 NaN

Data Analysis with Pandas
[ 50 ]
>>> df10 = df8.reindex([3, 2, 'a', 0])
    a   b   c
3   9  10  11
2   6   7   8
a NaN NaN NaN
0   0   1   2
To manipulate missing values, we can use the isnull() or notnull() functions to 
detect the missing values in a Series object, as well as in a DataFrame object:
>>> df10.isnull()
       a      b      c
3  False  False  False
2  False  False  False
a   True   True   True
0  False  False  False
On a Series, we can drop all null data and index values by using the dropna 
function:
>>> s4 = pd.Series({'001': 'Nam', '002': 'Mary',
                    '003': 'Peter'},
                    index=['002', '001', '024', '065'])
>>> s4
002    Mary
001     Nam
024     NaN
065     NaN
dtype: object
>>> s4.dropna()    # dropping all null value of Series object
002    Mary
001     Nam
dtype: object

Chapter 3
[ 51 ]
With a DataFrame object, it is a little bit more complex than with Series. We can tell 
which rows or columns we want to drop and also if all entries must be null or a 
single null value is enough. By default, the function will drop any row containing a 
missing value:
>>> df9.dropna()    # all rows will be dropped
Empty DataFrame
Columns: [a, b, c, d]
Index: []
>>> df9.dropna(axis=1)
   a   b   c
0  0   1   2
1  3   4   5
2  6   7   8
3  9  10  11
Another way to control missing values is to use the supported parameters of 
functions that we introduced in the previous section. They are also very useful to 
solve this problem. In our experience, we should assign a fixed value in missing 
cases when we create data objects. This will make our objects cleaner in later 
processing steps. For example, consider the following:
>>> df11 = df8.reindex([3, 2, 'a', 0], fill_value = 0)
>>> df11
   a   b   c
3  9  10  11
2  6   7   8
a  0   0   0
0  0   1   2
We can alse use the fillna function to fill a custom value in missing values:
>>> df9.fillna(-1)
   a   b   c  d
0  0   1   2 -1
1  3   4   5 -1
2  6   7   8 -1
3  9  10  11 -1

Data Analysis with Pandas
[ 52 ]
Advanced uses of Pandas for data 
analysis
In this section we will consider some advanced Pandas use cases.
Hierarchical indexing
Hierarchical indexing provides us with a way to work with higher dimensional  
data in a lower dimension by structuring the data object into multiple index  
levels on an axis:
>>> s8 = pd.Series(np.random.rand(8), index=[['a','a','b','b','c','c', 
'd','d'], [0, 1, 0, 1, 0,1, 0, 1, ]])
>>> s8
a  0    0.721652
   1    0.297784
b  0    0.271995
   1    0.125342
c  0    0.444074
   1    0.948363
d  0    0.197565
   1    0.883776
dtype: float64
In the preceding example, we have a Series object that has two index levels.  
The object can be rearranged into a DataFrame using the unstack function.  
In an inverse situation, the stack function can be used:
>>> s8.unstack()
          0         1
a  0.549211  0.420874
b  0.051516  0.715021
c  0.503072  0.720772
d  0.373037  0.207026

Chapter 3
[ 53 ]
We can also create a DataFrame to have a hierarchical index in both axes:
>>> df = pd.DataFrame(np.random.rand(12).reshape(4,3),
                      index=[['a', 'a', 'b', 'b'],
                               [0, 1, 0, 1]],
                      columns=[['x', 'x', 'y'], [0, 1, 0]])
>>> df
            x                   y
            0         1         0
a 0  0.636893  0.729521  0.747230
  1  0.749002  0.323388  0.259496
b 0  0.214046  0.926961  0.679686
0.013258  0.416101  0.626927
>>> df.index
MultiIndex(levels=[['a', 'b'], [0, 1]],
           labels=[[0, 0, 1, 1], [0, 1, 0, 1]])
>>> df.columns
MultiIndex(levels=[['x', 'y'], [0, 1]],
           labels=[[0, 0, 1], [0, 1, 0]])
The methods for getting or setting values or subsets of the data objects with multiple 
index levels are similar to those of the nonhierarchical case:
>>> df['x']
            0         1
a 0  0.636893  0.729521
  1  0.749002  0.323388
b 0  0.214046  0.926961
0.013258  0.416101
>>> df[[0]]
            x
            0
a 0  0.636893
  1  0.749002
b 0  0.214046
0.013258

Data Analysis with Pandas
[ 54 ]
>>> df.ix['a', 'x']
          0         1
0  0.636893  0.729521
0.749002  0.323388
>>> df.ix['a','x'].ix[1]
0    0.749002
1    0.323388
Name: 1, dtype: float64
After grouping data into multiple index levels, we can also use most of the 
descriptive and statistics functions that have a level option, which can be used to 
specify the level we want to process:
>>> df.std(level=1)
          x                   y
          0         1         0
0  0.298998  0.139611  0.047761
0.520250  0.065558  0.259813
>>> df.std(level=0)
          x                   y
          0         1         0
a  0.079273  0.287180  0.344880
b  0.141979  0.361232  0.037306
The Panel data
The Panel is another data structure for three-dimensional data in Pandas.  
However, it is less frequently used than the Series or the DataFrame. You can think 
of a Panel as a table of DataFrame objects. We can create a Panel object from a 3D 
ndarray or a dictionary of DataFrame objects:
# create a Panel from 3D ndarray
>>> panel = pd.Panel(np.random.rand(2, 4, 5),
                     items = ['item1', 'item2'])
>>> panel
<class 'pandas.core.panel.Panel'>
Dimensions: 2 (items) x 4 (major_axis) x 5 (minor_axis)
Items axis: item1 to item2
Major_axis axis: 0 to 3

Chapter 3
[ 55 ]
Minor_axis axis: 0 to 4
>>> df1 = pd.DataFrame(np.arange(12).reshape(4, 3), 
                       columns=['a','b','c'])
>>> df1
   a   b   c
0  0   1   2
1  3   4   5
2  6   7   8
9  10  11
>>> df2 = pd.DataFrame(np.arange(9).reshape(3, 3), 
                       columns=['a','b','c'])
>>> df2
   a  b  c
0  0  1  2
1  3  4  5
6  7  8
# create another Panel from a dict of DataFrame objects
>>> panel2 = pd.Panel({'item1': df1, 'item2': df2})
>>> panel2
<class 'pandas.core.panel.Panel'>
Dimensions: 2 (items) x 4 (major_axis) x 3 (minor_axis)
Items axis: item1 to item2
Major_axis axis: 0 to 3
Minor_axis axis: a to c
Each item in a Panel is a DataFrame. We can select an item, by item name:
>>> panel2['item1']
   a   b   c
0  0   1   2
1  3   4   5
2  6   7   8
3  9  10  11

Data Analysis with Pandas
[ 56 ]
Alternatively, if we want to select data via an axis or data position, we can use the ix 
method, like on Series or DataFrame:
>>> panel2.ix[:, 1:3, ['b', 'c']]
<class 'pandas.core.panel.Panel'>
Dimensions: 2 (items) x 3 (major_axis) x 2 (minor_axis)
Items axis: item1 to item2
Major_axis axis: 1 to 3
Minor_axis axis: b to c
>>> panel2.ix[:, 2, :]
   item1  item2
a      6      6
b      7      7
c      8      8
Summary
We have finished covering the basics of the Pandas data analysis library. Whenever 
you learn about a library for data analysis, you need to consider the three parts that 
we explained in this chapter. Data structures: we have two common data object types 
in the Pandas library; Series and DataFrames. Method to access and manipulate 
data objects: Pandas supports many way to select, set or slice subsets of data object. 
However, the general mechanism is using index labels or the positions of items 
to identify values. Functions and utilities: They are the most important part of a 
powerful library. In this chapter, we covered all common supported functions of 
Pandas which allow us compute statistics on data easily. The library also has a lot 
of other useful functions and utilities that we could not explain in this chapter. We 
encourage you to start your own research, if you want to expand your experience 
with Pandas. It helps us to process large data in an optimized way. You will see 
more of Pandas in action later in this book.
Until now, we learned about two popular Python libraries: NumPy and Pandas. 
Pandas is built on NumPy, and as a result it allows for a bit more convenient 
interaction with data. However, in some situations, we can flexibly combine  
both of them to accomplish our goals.

Chapter 3
[ 57 ]
Practice exercises
The link https://www.census.gov/2010census/csv/pop_change.csv contains an 
US census dataset. It has 23 columns and one row for each US state, as well as a few 
rows for macro regions such as North, South, and West.
•	
Get this dataset into a Pandas DataFrame. Hint: just skip those rows that do 
not seem helpful, such as comments or description.
•	
While the dataset contains change metrics for each decade, we are interested 
in the population change during the second half of the twentieth century, 
that is between, 1950 and 2000. Which region has seen the biggest and the 
smallest population growth in this time span? Also, which US state?
Advanced open-ended exercise:
•	
Find more census data on the internet; not just on the US but on the world's 
countries. Try to find GDP data for the same time as well. Try to align this 
data to explore patterns. How are GDP and population growth related? Are 
there any special cases. such as countries with high GDP but low population 
growth or countries with the opposite history?


[ 59 ]
Data Visualization
Data visualization is concerned with the presentation of data in a pictorial or graphical 
form. It is one of the most important tasks in data analysis, since it enables us to see 
analytical results, detect outliers, and make decisions for model building. There are 
many Python libraries for visualization, of which matplotlib, seaborn, bokeh, and 
ggplot are among the most popular. However, in this chapter, we mainly focus on the 
matplotlib library that is used by many people in many different contexts.
Matplotlib produces publication-quality figures in a variety of formats, and 
interactive environments across Python platforms. Another advantage is that Pandas 
comes equipped with useful wrappers around several matplotlib plotting routines, 
allowing for quick and handy plotting of Series and DataFrame objects.
The IPython package started as an alternative to the standard interactive Python 
shell, but has since evolved into an indispensable tool for data exploration, 
visualization, and rapid prototyping. It is possible to use the graphical capabilities 
offered by matplotlib from IPython through various options, of which the simplest to 
get started with is the pylab flag:
$ ipython --pylab
This flag will preload matplotlib and numpy for interactive use with the default 
matplotlib backend. IPython can run in various environments: in a terminal, as a Qt 
application, or inside a browser. These options are worth exploring, since IPython 
has enjoyed adoption for many use cases, such as prototyping, interactive slides for 
more engaging conference talks or lectures, and as a tool for sharing research.

Data Visualization
[ 60 ]
The matplotlib API primer
The easiest way to get started with plotting using matplotlib is often by using the 
MATLAB API that is supported by the package:
>>> import matplotlib.pyplot as plt
>>> from numpy import *
>>> x = linspace(0, 3, 6)
>>> x
array([0., 0.6, 1.2, 1.8, 2.4, 3.])
>>> y = power(x,2)
>>> y
array([0., 0.36, 1.44, 3.24, 5.76, 9.])
>>> figure()
>>> plot(x, y, 'r')
>>> xlabel('x')
>>> ylabel('y')
>>> title('Data visualization in MATLAB-like API')
>>> plt.show()
The output for the preceding command is as follows:

Chapter 4
[ 61 ]
However, star imports should not be used unless there is a good reason for doing so. 
In the case of matplotlib, we can use the canonical import:
>>> import matplotlib.pyplot as plt
The preceding example could then be written as follows:
>>> plt.plot(x, y)
>>> plt.xlabel('x')
>>> plt.ylabel('y')
>>> plt.title('Data visualization using Pyplot of Matplotlib')
>>> plt.show()
The output for the preceding command is as follows:

Data Visualization
[ 62 ]
If we only provide a single argument to the plot function, it will automatically use 
it as the y values and generate the x values from 0 to N-1, where N is equal to the 
number of values:
>>> plt.plot(y)
>>> plt.xlabel('x')
>>> plt.ylabel('y')
>>> plt.title('Plot y value without given x values')
>>> plt.show()
The output for the preceding command is as follows:
By default, the range of the axes is constrained by the range of the input x and y data. 
If we want to specify the viewport of the axes, we can use the axis() method to 
set custom ranges. For example, in the previous visualization, we could increase the 
range of the x axis from [0, 5] to [0, 6], and that of the y axis from [0, 9] to [0, 
10], by writing the following command:
>>> plt.axis([0, 6, 0, 12])

Chapter 4
[ 63 ]
Line properties
The default line format when we plot data in matplotlib is a solid blue line, which 
is abbreviated as b-. To change this setting, we only need to add the symbol code, 
which includes letters as color string and symbols as line style string, to the plot 
function. Let us consider a plot of several lines with different format styles:
>>> plt.plot(x*2, 'g^', x*3, 'rs', x**x, 'y-')
>>> plt.axis([0, 6, 0, 30])
>>> plt.show()
The output for the preceding command is as follows:
There are many line styles and attributes, such as color, line width, and dash style, 
that we can choose from to control the appearance of our plots. The following 
example illustrates several ways to set line properties:
>>> line = plt.plot(y, color='red', linewidth=2.0)
>>> line.set_linestyle('--')
>>> plt.setp(line, marker='o')
>>> plt.show()

Data Visualization
[ 64 ]
The output for the preceding command is as follows:
The following table lists some common properties of the line2d plotting:
Property
Value type
Description
color or c
Any matplotlib color
This sets the color of the line in the 
figure
dashes
On/off
This sets the sequence of ink in the 
points
data
np.array xdata, 
np.array ydata
This sets the data used for 
visualization
linestyle or ls
[ '-' | '—' |'-.' | ':' 
| ...]
This sets the line style in the figure
linewidth or lw
Float value in points
This sets the width of line in the 
figure
marker
Any symbol
This sets the style at data points in 
the figure

Chapter 4
[ 65 ]
Figures and subplots
By default, all plotting commands apply to the current figure and axes. In some 
situations, we want to visualize data in multiple figures and axes to compare 
different plots or to use the space on a page more efficiently. There are two steps 
required before we can plot the data. Firstly, we have to define which figure we want 
to plot. Secondly, we need to figure out the position of our subplot in the figure:
>>> plt.figure('a')    # define a figure, named 'a'
>>> plt.subplot(221)    # the first position of 4 subplots in 2x2 figure
>>> plt.plot(y+y, 'r--')
>>> plt.subplot(222)    # the second position of 4 subplots
>>> plt.plot(y*3, 'ko')
>>> plt.subplot(223)    # the third position of 4 subplots
>>> plt.plot(y*y, 'b^')
>>> plt.subplot(224)
>>> plt.show()
The output for the preceding command is as follows:

Data Visualization
[ 66 ]
In this case, we currently have the figure a. If we want to modify any subplot in 
figure a, we first call the command to select the figure and subplot, and then execute 
the function to modify the subplot. Here, for example, we change the title of the 
second plot of our four-plot figure:
>>> plt.figure('a')
>>> plt.subplot(222)
>>> plt.title('visualization of y*3')
>>> plt.show()
The output for the preceding command is as follows:
Integer subplot specification must be a three-digit number if we are 
not using commas to separate indices. So, plt.subplot(221) is 
equal to the plt.subplot(2,2,1) command.
There is a convenience method, plt.subplots(), to creating a figure that  
contains a given number of subplots. As inthe previous example, we can use the 
plt.subplots(2,2) command to create a 2x2 figure that consists of four subplots.

Chapter 4
[ 67 ]
We can also create the axes manually, instead of rectangular grid, by using the plt.
axes([left, bottom, width, height]) command, where all input parameters 
are in the fractional [0, 1] coordinates:
>>> plt.figure('b')    # create another figure, named 'b'
>>> ax1 = plt.axes([0.05, 0.1, 0.4, 0.32])
>>> ax2 = plt.axes([0.52, 0.1, 0.4, 0.32])
>>> ax3 = plt.axes([0.05, 0.53, 0.87, 0.44])
>>> plt.show()
The output for the preceding command is as follows:
However, when you manually create axes, it takes more time to balance coordinates 
and sizes between subplots to arrive at a well-proportioned figure.

Data Visualization
[ 68 ]
Exploring plot types
We have looked at how to create simple line plots so far. The matplotlib library 
supports many more plot types that are useful for data visualization. However,  
our goal is to provide the basic knowledge that will help you to understand and use 
the library for visualizing data in the most common situations. Therefore, we will 
only focus on four kinds of plot types: scatter plots, bar plots, contour plots, and 
histograms.
Scatter plots
A scatter plot is used to visualize the relationship between variables measured in 
the same dataset. It is easy to plot a simple scatter plot, using the plt.scatter() 
function, that requires numeric columns for both the x and y axis:

Chapter 4
[ 69 ]
Let's take a look at the command for the preceding output:
>>> X = np.random.normal(0, 1, 1000)
>>> Y = np.random.normal(0, 1, 1000)
>>> plt.scatter(X, Y, c = ['b', 'g', 'k', 'r', 'c'])
>>> plt.show()
Bar plots
A bar plot is used to present grouped data with rectangular bars, which can be either 
vertical or horizontal, with the lengths of the bars corresponding to their values. 
We use the plt.bar() command to visualize a vertical bar, and the plt.barh() 
command for the other:

Data Visualization
[ 70 ]
The command for the preceding output is as follows:
>>> X = np.arange(5)
>>> Y = 3.14 + 2.71 * np.random.rand(5)
>>> plt.subplots(2)
>>> # the first subplot
>>> plt.subplot(211)
>>> plt.bar(X, Y, align='center', alpha=0.4, color='y')
>>> plt.xlabel('x')
>>> plt.ylabel('y')
>>> plt.title('bar plot in vertical')
>>> # the second subplot
>>> plt.subplot(212)
>>> plt.barh(X, Y, align='center', alpha=0.4, color='c')
>>> plt.xlabel('x')
>>> plt.ylabel('y')
>>> plt.title('bar plot in horizontal')
>>> plt.show()
Contour plots
We use contour plots to present the relationship between three numeric variables 
in two dimensions. Two variables are drawn along the x and y axes, and the third 
variable, z, is used for contour levels that are plotted as curves in different colors:
>>> x = np.linspace(-1, 1, 255)
>>> y = np.linspace(-2, 2, 300)
>>> z = np.sin(y[:, np.newaxis]) * np.cos(x)
>>> plt.contour(x, y, z, 255, linewidth=2)
>>> plt.show()

Chapter 4
[ 71 ]
Let's take a look at the contour plot in the following image:
If we want to draw contour lines and filled contours, we can use the 
plt.contourf() method instead of plt.contour(). In contrast to 
MATLAB, matplotlib's contourf() will not draw the polygon edges.

Data Visualization
[ 72 ]
Histogram plots
A histogram represents the distribution of numerical data graphically. Usually, the 
range of values is partitioned into bins of equal size, with the height of each bin 
corresponding to the frequency of values within that bin:
The command for the preceding output is as follows:
>>> mu, sigma = 100, 25
>>> fig, (ax0, ax1) = plt.subplots(ncols=2)
>>> x = mu + sigma * np.random.randn(1000)
>>> ax0.hist(x,20, normed=1, histtype='stepfilled', 
               facecolor='g', alpha=0.75)
>>> ax0.set_title('Stepfilled histogram')
>>> ax1.hist(x, bins=[100,150, 165, 170, 195] normed=1, 
             histtype='bar', rwidth=0.8)
>>> ax1.set_title('uniquel bins histogram')
>>> # automatically adjust subplot parameters to give specified padding
>>> plt.tight_layout()
>>> plt.show()

Chapter 4
[ 73 ]
Legends and annotations
Legends are an important element that is used to identify the plot elements in a 
figure. The easiest way to show a legend inside a figure is to use the label argument 
of the plot function, and show the labels by calling the plt.legend() method:
>>> x = np.linspace(0, 1, 20) 
>>> y1 = np.sin(x)
>>> y2 = np.cos(x)
>>> y3 = np.tan(x)
>>> plt.plot(x, y1, 'c', label='y=sin(x)')
>>> plt.plot(x, y2, 'y', label='y=cos(x)')
>>> plt.plot(x, y3, 'r', label='y=tan(x)')
>>> plt.lengend(loc='upper left')
>>> plt.show()
The output for the preceding command as follows:

Data Visualization
[ 74 ]
The loc argument in the legend command is used to figure out the position of the 
label box. There are several valid location options: lower left, right, upper left, 
lower center, upper right, center, lower right, upper right, center right, 
best, upper center, and center left. The default position setting is upper right. 
However, when we set an invalid location option that does not exist in the above list, 
the function automatically falls back to the best option.
If we want to split the legend into multiple boxes in a figure, we can manually set 
our expected labels for plot lines, as shown in the following image:
The output for the preceding command is as follows:
>>> p1 = plt.plot(x, y1, 'c', label='y=sin(x)')
>>> p2 = plt.plot(x, y2, 'y', label='y=cos(x)')
>>> p3 = plt.plot(x, y3, 'r', label='y=tan(x)')
>>> lsin = plt.legend(handles=p1, loc='lower right')
>>> lcos = plt.legend(handles=p2, loc='upper left')
>>> ltan = plt.legend(handles=p3, loc='upper right')
>>> # with above code, only 'y=tan(x)' legend appears in the figure
>>> # fix: add lsin, lcos as separate artists to the axes

Chapter 4
[ 75 ]
>>> plt.gca().add_artist(lsin)
>>> plt.gca().add_artist(lcos)
>>> # automatically adjust subplot parameters to specified padding
>>> plt.tight_layout()
>>> plt.show()
The other element in a figure that we want to introduce is the annotations which 
can consist of text, arrows, or other shapes to explain parts of the figure in detail, 
or to emphasize some special data points. There are different methods for showing 
annotations, such as text, arrow, and annotation.
•	
The text method draws text at the given coordinates (x, y) on the plot; 
optionally with custom properties. There are some common arguments in the 
function: x, y, label text, and font-related properties that can be passed in via 
fontdict, such as family, fontsize, and style.
•	
The annotate method can draw both text and arrows arranged 
appropriately. Arguments of this function are s (label text), xy (the position 
of element to annotation), xytext (the position of the label s), xycoords (the 
string that indicates what type of coordinate xy is), and arrowprops (the 
dictionary of line properties for the arrow that connects the annotation).
Here is a simple example to illustrate the annotate and text functions:
>>> x = np.linspace(-2.4, 0.4, 20)
>>> y = x*x + 2*x + 1
>>> plt.plot(x, y, 'c', linewidth=2.0)
>>> plt.text(-1.5, 1.8, 'y=x^2 + 2*x + 1',
             fontsize=14, style='italic')
>>> plt.annotate('minima point', xy=(-1, 0),
                 xytext=(-1, 0.3),
                 horizontalalignment='center', 
                 verticalalignment='top', 
                 arrowprops=dict(arrowstyle='->',    
                 connectionstyle='arc3'))
>>> plt.show()

Data Visualization
[ 76 ]
The output for the preceding command is as follows:
Plotting functions with Pandas
We have covered most of the important components in a plot figure using 
matplotlib. In this section, we will introduce another powerful plotting method for 
directly creating standard visualization from Pandas data objects that are often used 
to manipulate data.
For Series or DataFrame objects in Pandas, most plotting types are supported,  
such as line, bar, box, histogram, and scatter plots, and pie charts. To select a plot 
type, we use the kind argument of the plot function. With no kind of plot specified, 
the plot function will generate a line style visualization by default , as in the 
following example:
>>> s = pd.Series(np.random.normal(10, 8, 20))
>>> s.plot(style='ko—', alpha=0.4, label='Series plotting')
>>> plt.legend()
>>> plt.show()

Chapter 4
[ 77 ]
The output for the preceding command is as follows:
Another example will visualize the data of a DataFrame object consisting of  
multiple columns:
>>> data = {'Median_Age': [24.2, 26.4, 28.5, 30.3],
         'Density': [244, 256, 268, 279]}
>>> index_label = ['2000', '2005', '2010', '2014'];
>>> df1 = pd.DataFrame(data, index=index_label)
>>> df1.plot(kind='bar', subplots=True, sharex=True)
>>> plt.tight_layout();
>>> plt.show()

Data Visualization
[ 78 ]
The output for the preceding command is as follows:
The plot method of the DataFrame has a number of options that allow us to handle the 
plotting of the columns. For example, in the above DataFrame visualization, we chose 
to plot the columns in separate subplots. The following table lists more options:
Argument
Value
Description
subplots
True/False
The plots each data column in a separate 
subplot
logy
True/False
The gets a log-scale y axis
secondary_y
True/False
The plots data on a secondary y axis
sharex, sharey
True/False
The shares the same x or y axis, linking 
sticks and limits
Additional Python data visualization 
tools
Besides matplotlib, there are other powerful data visualization toolkits based on 
Python. While we cannot dive deeper into these libraries, we would like to at least 
briefly introduce them in this session.

Chapter 4
[ 79 ]
Bokeh
Bokeh is a project by Peter Wang, Hugo Shi, and others at Continuum Analytics. It 
aims to provide elegant and engaging visualizations in the style of D3.js. The library 
can quickly and easily create interactive plots, dashboards, and data applications. 
Here are a few differences between matplotlib and Bokeh:
•	
Bokeh achieves cross-platform ubiquity through IPython's new model of in-
browser client-side rendering
•	
Bokeh uses a syntax familiar to R and ggplot users, while matplotlib is more 
familiar to Matlab users
•	
Bokeh has a coherent vision to build a ggplot-inspired in-browser interactive 
visualization tool, while Matplotlib has a coherent vision of focusing on 2D 
cross-platform graphics.
The basic steps for creating plots with Bokeh are as follows:
•	
Prepare some data in a list, series, and Dataframe
•	
Tell Bokeh where you want to generate the output
•	
Call figure() to create a plot with some overall options, similar to the 
matplotlib options discussed earlier
•	
Add renderers for your data, with visual customizations such as colors, 
legends, and width
•	
Ask Bokeh to show() or save() the results
MayaVi
MayaVi is a library for interactive scientific data visualization and 3D plotting, built 
on top of the award-winning visualization toolkit (VTK), which is a traits-based 
wrapper for the open-source visualization library. It offers the following:
•	
The possibility to interact with the data and object in the visualization 
through dialogs.
•	
An interface in Python for scripting. MayaVi can work with Numpy 
and scipy for 3D plotting out of the box and can be used within IPython 
notebooks, which is similar to matplotlib.
•	
An abstraction over VTK that offers a simpler programming model.

Data Visualization
[ 80 ]
Let's view an illustration made entirely using MayaVi based on VTK examples and 
their provided data:

Chapter 4
[ 81 ]
Summary
We finished covering most of the basics, such as functions, arguments, and 
properties for data visualization, based on the matplotlib library. We hope that, 
through the examples, you will be able to understand and apply them to your own 
problems. In general, to visualize data, we need to consider five steps- that is, getting 
data into suitable Python or Pandas data structures, such as lists, dictionaries, Series, 
or DataFrames. We explained in the previous chapters, how to accomplish this 
step. The second step is defining plots and subplots for the data object in question. 
We discussed this in the figures and subplots session. The third step is selecting a 
plot style and its attributes to show in the subplots such as: line, bar, histogram, 
scatter plot, line style, and color. The fourth step is adding extra components 
to the subplots, like legends, annotations and text. The fifth step is displaying or 
saving the results.
By now, you can do quite a few things with a dataset; for example, manipulation, 
cleaning, exploration, and visualization based on Python libraries such as Numpy, 
Pandas, and matplotlib. You can now combine this knowledge and practice with 
these libraries to get more and more familiar with Python data analysis.
Practice exercises:
•	
Name two real or fictional datasets and explain which kind of plot would 
best fit the data: line plots, bar charts, scatter plots, contour plots, or 
histograms. Name one or two applications, where each of the plot type 
is common (for example, histograms are often used in image editing 
applications).
•	
We only focused on the most common plot types of matplotlib.  
After a bit of research, can you name a few more plot types that are  
available in matplotlib?
•	
Take one Pandas data structure from Chapter 3, Data Analysis with Pandas  
and plot the data in a suitable way. Then, save it as a PNG image to the disk.


[ 83 ]
Time Series
Time series typically consist of a sequence of data points coming from measurements 
taken over time. This kind of data is very common and occurs in a multitude  
of fields.
A business executive is interested in stock prices, prices of goods and services or 
monthly sales figures. A meteorologist takes temperature measurements several times 
a day and also keeps records of precipitation, humidity, wind direction and force. 
A neurologist can use electroencephalography to measure electrical activity of the 
brain along the scalp. A sociologist can use campaign contribution data to learn about 
political parties and their supporters and use these insights as an argumentation aid. 
More examples for time series data can be enumerated almost endlessly.
Time series primer
In general, time series serve two purposes. First, they help us to learn about the 
underlying process that generated the data. On the other hand, we would like to be 
able to forecast future values of the same or related series using existing data. When 
we measure temperature, precipitation or wind, we would like to learn more about 
more complex things, such as weather or the climate of a region and how various 
factors interact. At the same time, we might be interested in weather forecasting.
In this chapter we will explore the time series capabilities of Pandas. Apart from 
its powerful core data structures – the series and the DataFrame – Pandas comes 
with helper functions for dealing with time related data. With its extensive built-in 
optimizations, Pandas is capable of handling large time series with millions of data 
points with ease.
We will gradually approach time series, starting with the basic building blocks of 
date and time objects.

Time Series
[ 84 ]
Working with date and time objects
Python supports date and time handling in the date time and time modules from  
the standard library:
>>> import datetime
>>> datetime.datetime(2000, 1, 1)
datetime.datetime(2000, 1, 1, 0, 0)
Sometimes, dates are given or expected as strings, so a conversion from or to 
strings is necessary, which is realized by two functions: strptime and strftime, 
respectively:
>>> datetime.datetime.strptime("2000/1/1", "%Y/%m/%d")
datetime.datetime(2000, 1, 1, 0, 0)
>>> datetime.datetime(2000, 1, 1, 0, 0).strftime("%Y%m%d")
'20000101'
Real-world data usually comes in all kinds of shapes and it would be great if we did 
not need to remember the exact date format specifies for parsing. Thankfully, Pandas 
abstracts away a lot of the friction, when dealing with strings representing dates or 
time. One of these helper functions is to_datetime:
>>> import pandas as pd
>>> import numpy as np
>>> pd.to_datetime("4th of July")
Timestamp('2015-07-04 
>>> pd.to_datetime("13.01.2000")
Timestamp('2000-01-13 00:00:00')
>>> pd.to_datetime("7/8/2000")
Timestamp('2000-07-08 00:00:00')
The last can refer to August 7th or July 8th, depending on the region. To 
disambiguate this case, to_datetime can be passed a keyword argument dayfirst:
>>> pd.to_datetime("7/8/2000", dayfirst=True)
Timestamp('2000-08-07 00:00:00')
Timestamp objects can be seen as Pandas' version of datetime objects and indeed, 
the Timestamp class is a subclass of datetime:
>>> issubclass(pd.Timestamp, datetime.datetime)
True

Chapter 5
[ 85 ]
Which means they can be used interchangeably in many cases:
>>> ts = pd.to_datetime(946684800000000000)
>>> ts.year, ts.month, ts.day, ts.weekday()
(2000, 1, 1, 5)
Timestamp objects are an important part of time series capabilities of Pandas, since 
timestamps are the building block of DateTimeIndex objects:
>>> index = [pd.Timestamp("2000-01-01"),
             pd.Timestamp("2000-01-02"),
             pd.Timestamp("2000-01-03")]
>>> ts = pd.Series(np.random.randn(len(index)), index=index)
>>> ts
2000-01-01    0.731897
2000-01-02    0.761540
2000-01-03   -1.316866
dtype: float64
>>> ts.indexDatetime
Index(['2000-01-01', '2000-01-02', '2000-01-03'],
dtype='datetime64[ns]', freq=None, tz=None)
There are a few things to note here: We create a list of timestamp objects and pass 
it to the series constructor as index. This list of timestamps gets converted into a 
DatetimeIndex on the fly. If we had passed only the date strings, we would not  
get a DatetimeIndex, just an index:
>>> ts = pd.Series(np.random.randn(len(index)), index=[
              "2000-01-01", "2000-01-02", "2000-01-03"])
>>> ts.index
Index([u'2000-01-01', u'2000-01-02', u'2000-01-03'], dtype='object')
However, the to_datetime function is flexible enough to be of help, if all we have  
is a list of date strings:
>>> index = pd.to_datetime(["2000-01-01", "2000-01-02", "2000-01-03"])
>>> ts = pd.Series(np.random.randn(len(index)), index=index)
>>> ts.index
DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03'], 
dtype='datetime64[ns]', freq=None, tz=None))
Another thing to note is that while we have a DatetimeIndex, the freq and tz 
attributes are both None. We will learn about the utility of both attributes later  
in this chapter.

Time Series
[ 86 ]
With to_datetime we are able to convert a variety of strings and even lists of strings 
into timestamp or DatetimeIndex objects. Sometimes we are not explicitly given all 
the information about a series and we have to generate sequences of time stamps of 
fixed intervals ourselves.
Pandas offer another great utility function for this task: date_range.
The date_range function helps to generate a fixed frequency datetime index 
between start and end dates. It is also possible to specify either the start or end date 
and the number of timestamps to generate.
The frequency can be specified by the freq parameter, which supports a number of 
offsets. You can use typical time intervals like hours, minutes, and seconds:
>>> pd.date_range(start="2000-01-01", periods=3, freq='H')
DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 01:00:00', 
  '2000-01-01 02:00:00'], dtype='datetime64[ns]', freq='H', tz=None)
>>> pd.date_range(start="2000-01-01", periods=3, freq='T')
DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 00:01:00', 
  '2000-01-01 00:02:00'], dtype='datetime64[ns]', freq='T', tz=None) 
>>> pd.date_range(start="2000-01-01", periods=3, freq='S')
DatetimeIndex(['2000-01-01 00:00:00', '2000-01-01 00:00:01', 
  '2000-01-01 00:00:02'], dtype='datetime64[ns]', freq='S', tz=None)
The freq attribute allows us to specify a multitude of options. Pandas has been 
used successfully in finance and economics, not least because it is really simple to 
work with business dates as well. As an example, to get an index with the first three 
business days of the millennium, the B offset alias can be used:
>>> pd.date_range(start="2000-01-01", periods=3, freq='B')
DatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05'], 
dtype='datetime64[ns]', freq='B', tz=None)
The following table shows the available offset aliases and can be also be looked up 
in the Pandas documentation on time series under http://pandas.pydata.org/
pandas-docs/stable/timeseries.html#offset-aliases:
Alias
Description
B
Business day frequency
C
Custom business day frequency
D
Calendar day frequency
W
Weekly frequency

Chapter 5
[ 87 ]
Alias
Description
M
Month end frequency
BM
Business month end frequency
CBM
Custom business month end frequency
MS
Month start frequency
BMS
Business month start frequency
CBMS
Custom business month start frequency
Q
Quarter end frequency
BQ
Business quarter frequency
QS
Quarter start frequency
BQS
Business quarter start frequency
A
Year end frequency
BA
Business year end frequency
AS
Year start frequency
BAS
Business year start frequency
BH
Business hour frequency
H
Hourly frequency
T
Minutely frequency
S
Secondly frequency
L
Milliseconds
U
Microseconds
N
Nanoseconds
Moreover, The offset aliases can be used in combination as well. Here, we are 
generating a datetime index with five elements, each one day, one hour, one minute 
and one second apart:
>>> pd.date_range(start="2000-01-01", periods=5, freq='1D1h1min10s')
DatetimeIndex(['2000-01-01 00:00:00', '2000-01-02 01:01:10', 
               '2000-01-03 02:02:20', '2000-01-04 03:03:30', 
               '2000-01-05 04:04:40'], 
              dtype='datetime64[ns]', freq='90070S', tz=None)

Time Series
[ 88 ]
If we want to index data every 12 hours of our business time, which by default starts 
at 9 AM and ends at 5 PM, we would simply prefix the BH alias:
>>> pd.date_range(start="2000-01-01", periods=5, freq='12BH')
DatetimeIndex(['2000-01-03 09:00:00', '2000-01-04 13:00:00', 
               '2000-01-06 09:00:00', '2000-01-07 13:00:00', 
               '2000-01-11 09:00:00'], 
              dtype='datetime64[ns]', freq='12BH', tz=None)
A custom definition of what a business hour means is also possible:
>>> ts.index
DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03'], 
dtype='datetime64[ns]', freq=None, tz=None)
We can use this custom business hour to build indexes as well:
>>> pd.date_range(start="2000-01-01", periods=5, freq=12 * bh)
DatetimeIndex(['2000-01-03 07:00:00', '2000-01-03 19:00:00', 
               '2000-01-04 07:00:00', '2000-01-04 19:00:00', 
               '2000-01-05 07:00:00', '2000-01-05 19:00:00', 
               '2000-01-06 07:00:00'], 
              dtype='datetime64[ns]', freq='12BH', tz=None)
Some frequencies allow us to specify an anchoring suffix, which allows us to express 
intervals, such as every Friday or every second Tuesday of the month:
>>> pd.date_range(start="2000-01-01", periods=5, freq='W-FRI')
DatetimeIndex(['2000-01-07', '2000-01-14', '2000-01-21', '2000-01-28', 
'2000-02-04'], dtype='datetime64[ns]', freq='W-FRI', tz=None)
>>> pd.date_range(start="2000-01-01", periods=5, freq='WOM-2TUE') 
DatetimeIndex(['2000-01-11', '2000-02-08', '2000-03-14', '2000-04-11', 
'2000-05-09'], dtype='datetime64[ns]', freq='WOM-2TUE', tz=None)
Finally, we can merge various indexes of different frequencies. The possibilities  
are endless. We only show one example, where we combine two indexes – each over 
a decade – one pointing to every first business day of a year and one to the last day 
of February:
>>> s = pd.date_range(start="2000-01-01", periods=10, freq='BAS-JAN')
>>> t = pd.date_range(start="2000-01-01", periods=10, freq='A-FEB')
>>> s.union(t)

Chapter 5
[ 89 ]
DatetimeIndex(['2000-01-03', '2000-02-29', '2001-01-01', '2001-02-28', 
               '2002-01-01', '2002-02-28', '2003-01-01', '2003-02-28', 
               '2004-01-01', '2004-02-29', '2005-01-03', '2005-02-28', 
               '2006-01-02', '2006-02-28', '2007-01-01', '2007-02-28', 
               '2008-01-01', '2008-02-29', '2009-01-01', '2009-02-28'], 
              dtype='datetime64[ns]', freq=None, tz=None)
We see, that 2000 and 2005 did not start on a weekday and that 2000, 2004, and 2008 
were the leap years.
We have seen two powerful functions so far, to_datetime and date_range. Now 
we want to dive into time series by first showing how you can create and plot time 
series data with only a few lines. In the rest of this section, we will show various 
ways to access and slice time series data.
It is easy to get started with time series data in Pandas. A random walk can be 
created and plotted in a few lines:
>>> index = pd.date_range(start='2000-01-01', periods=200, freq='B')
>>> ts = pd.Series(np.random.randn(len(index)), index=index)
>>> walk = ts.cumsum()
>>> walk.plot()
A possible output of this plot is show in the following figure:

Time Series
[ 90 ]
Just as with usual series objects, you can select parts and slice the index:
>>> ts.head()
2000-01-03    1.464142
2000-01-04    0.103077
2000-01-05    0.762656
2000-01-06    1.157041
2000-01-07   -0.427284
Freq: B, dtype: float64
>>> ts[0]
1.4641415817112928 
>>> ts[1:3]
2000-01-04    0.103077
2000-01-05    0.762656
We can use date strings as keys, even though our series has a DatetimeIndex:
>>> ts['2000-01-03']
1.4641415817112928
Even though the DatetimeIndex is made of timestamp objects, we can use datetime 
objects as keys as well:
>>> ts[datetime.datetime(2000, 1, 3)]
1.4641415817112928
Access is similar to lookup in dictionaries or lists, but more powerful. We can, for 
example, slice with strings or even mixed objects:
>>> ts['2000-01-03':'2000-01-05']
2000-01-03    1.464142
2000-01-04    0.103077
2000-01-05    0.762656
Freq: B, dtype: float64
>>> ts['2000-01-03':datetime.datetime(2000, 1, 5)]
2000-01-03    1.464142
2000-01-04    0.103077
2000-01-05    0.762656
Freq: B, dtype: float64
>>> ts['2000-01-03':datetime.date(2000, 1, 5)]
2000-01-03   -0.807669
2000-01-04    0.029802
2000-01-05   -0.434855
Freq: B, dtype: float64    

Chapter 5
[ 91 ]
It is even possible to use partial strings to select groups of entries. If we are only 
interested in February, we could simply write:
>>> ts['2000-02']
2000-02-01    0.277544
2000-02-02   -0.844352
2000-02-03   -1.900688
2000-02-04   -0.120010
2000-02-07   -0.465916
2000-02-08   -0.575722
2000-02-09    0.426153
2000-02-10    0.720124
2000-02-11    0.213050
2000-02-14   -0.604096
2000-02-15   -1.275345
2000-02-16   -0.708486
2000-02-17   -0.262574
2000-02-18    1.898234
2000-02-21    0.772746
2000-02-22    1.142317
2000-02-23   -1.461767
2000-02-24   -2.746059
2000-02-25   -0.608201
2000-02-28    0.513832
2000-02-29   -0.132000
To see all entries from March until May, including:
>>> ts['2000-03':'2000-05']
2000-03-01    0.528070
2000-03-02    0.200661
                    ...
2000-05-30    1.206963
2000-05-31    0.230351
Freq: B, dtype: float64    
Time series can be shifted forward or backward in time. The index stays in place,  
the values move:
>>> small_ts = ts['2000-02-01':'2000-02-05']
>>> small_ts
2000-02-01    0.277544

Time Series
[ 92 ]
2000-02-02   -0.844352
2000-02-03   -1.900688
2000-02-04   -0.120010
Freq: B, dtype: float64
>>> small_ts.shift(2)
2000-02-01         NaN
2000-02-02         NaN
2000-02-03    0.277544
2000-02-04   -0.844352
Freq: B, dtype: float64
To shift backwards in time, we simply use negative values:
>>> small_ts.shift(-2)
2000-02-01   -1.900688
2000-02-02   -0.120010
2000-02-03         NaN
2000-02-04         NaN
Freq: B, dtype: float64
Resampling time series
Resampling describes the process of frequency conversion over time series data. It is 
a helpful technique in various circumstances as it fosters understanding by grouping 
together and aggregating data. It is possible to create a new time series from daily 
temperature data that shows the average temperature per week or month. On the 
other hand, real-world data may not be taken in uniform intervals and it is required 
to map observations into uniform intervals or to fill in missing values for certain 
points in time. These are two of the main use directions of resampling: binning 
and aggregation, and filling in missing data. Downsampling and upsampling 
occur in other fields as well, such as digital signal processing. There, the process of 
downsampling is often called decimation and performs a reduction of the sample 
rate. The inverse process is called interpolation, where the sample rate is increased. 
We will look at both directions from a data analysis angle.
Downsampling time series data
Downsampling reduces the number of samples in the data. During this reduction, 
we are able to apply aggregations over data points. Let's imagine a busy airport  
with thousands of people passing through every hour. The airport administration 
has installed a visitor counter in the main area, to get an impression of exactly how 
busy their airport is.

Chapter 5
[ 93 ]
They are receiving data from the counter device every minute. Here are the 
hypothetical measurements for a day, beginning at 08:00, ending 600 minutes  
later at 18:00:
>>> rng = pd.date_range('4/29/2015 8:00', periods=600, freq='T')
>>> ts = pd.Series(np.random.randint(0, 100, len(rng)), index=rng)
>>> ts.head()
2015-04-29 08:00:00     9
2015-04-29 08:01:00    60
2015-04-29 08:02:00    65
2015-04-29 08:03:00    25
2015-04-29 08:04:00    19
To get a better picture of the day, we can downsample this time series to larger 
intervals, for example, 10 minutes. We can choose an aggregation function as well. 
The default aggregation is to take all the values and calculate the mean:
>>> ts.resample('10min').head()
2015-04-29 08:00:00    49.1
2015-04-29 08:10:00    56.0
2015-04-29 08:20:00    42.0
2015-04-29 08:30:00    51.9
2015-04-29 08:40:00    59.0
Freq: 10T, dtype: float64
In our airport example, we are also interested in the sum of the values, that is, the 
combined number of visitors for a given time frame. We can choose the aggregation 
function by passing a function or a function name to the how parameter works:
>>> ts.resample('10min', how='sum').head()
2015-04-29 08:00:00    442
2015-04-29 08:10:00    409
2015-04-29 08:20:00    532
2015-04-29 08:30:00    433
2015-04-29 08:40:00    470
Freq: 10T, dtype: int64
Or we can reduce the sampling interval even more by resampling to an  
hourly interval:
>>> ts.resample('1h', how='sum').head()
2015-04-29 08:00:00    2745
2015-04-29 09:00:00    2897
2015-04-29 10:00:00    3088

Time Series
[ 94 ]
2015-04-29 11:00:00    2616
2015-04-29 12:00:00    2691
Freq: H, dtype: int64
We can ask for other things as well. For example, what was the maximum number  
of people that passed through our airport within one hour:
>>> ts.resample('1h', how='max').head()
2015-04-29 08:00:00    97
2015-04-29 09:00:00    98
2015-04-29 10:00:00    99
2015-04-29 11:00:00    98
2015-04-29 12:00:00    99
Freq: H, dtype: int64
Or we can define a custom function if we are interested in more unusual metrics.  
For example, we could be interested in selecting a random sample for each hour:
>>> import random
>>> ts.resample('1h', how=lambda m: random.choice(m)).head() 
2015-04-29 08:00:00    28
2015-04-29 09:00:00    14
2015-04-29 10:00:00    68
2015-04-29 11:00:00    31
2015-04-29 12:00:00     5    
If you specify a function by string, Pandas uses highly optimized versions.
The built-in functions that can be used as argument to how are: sum, mean, std, sem, 
max, min, median, first, last, ohlc. The ohlc metric is popular in finance. It stands 
for open-high-low-close. An OHLC chart is a typical way to illustrate movements in 
the price of a financial instrument over time.
While in our airport this metric might not be that valuable, we can compute it 
nonetheless:
>>> ts.resample('1h', how='ohlc').head()
                     open  high  low  close
2015-04-29 08:00:00     9    97    0     14
2015-04-29 09:00:00    68    98    3     12
2015-04-29 10:00:00    71    99    1      1
2015-04-29 11:00:00    59    98    0      4
2015-04-29 12:00:00    56    99    3     55

Chapter 5
[ 95 ]
Upsampling time series data
In upsampling, the frequency of the time series is increased. As a result, we have 
more sample points than data points. One of the main questions is how to account 
for the entries in the series where we have no measurement.
Let's start with hourly data for a single day:
>>> rng = pd.date_range('4/29/2015 8:00', periods=10, freq='H')
>>> ts = pd.Series(np.random.randint(0, 100, len(rng)), index=rng)
>>> ts.head()
2015-04-29 08:00:00    30
2015-04-29 09:00:00    27
2015-04-29 10:00:00    54
2015-04-29 11:00:00     9
2015-04-29 12:00:00    48
Freq: H, dtype: int64
If we upsample to data points taken every 15 minutes, our time series will be 
extended with NaN values:
>>> ts.resample('15min')
>>> ts.head()
2015-04-29 08:00:00    30
2015-04-29 08:15:00   NaN
2015-04-29 08:30:00   NaN
2015-04-29 08:45:00   NaN
2015-04-29 09:00:00    27
There are various ways to deal with missing values, which can be controlled by  
the fill_method keyword argument to resample. Values can be filled either  
forward or backward:
>>> ts.resample('15min', fill_method='ffill').head()
2015-04-29 08:00:00    30
2015-04-29 08:15:00    30
2015-04-29 08:30:00    30
2015-04-29 08:45:00    30
2015-04-29 09:00:00    27
Freq: 15T, dtype: int64
>>> ts.resample('15min', fill_method='bfill').head()
2015-04-29 08:00:00    30
2015-04-29 08:15:00    27
2015-04-29 08:30:00    27

Time Series
[ 96 ]
2015-04-29 08:45:00    27
2015-04-29 09:00:00    27
With the limit parameter, it is possible to control the number of missing values  
to be filled:
>>> ts.resample('15min', fill_method='ffill', limit=2).head()
2015-04-29 08:00:00    30
2015-04-29 08:15:00    30
2015-04-29 08:30:00    30
2015-04-29 08:45:00   NaN
2015-04-29 09:00:00    27
Freq: 15T, dtype: float64
If you want to adjust the labels during resampling, you can use the loffset 
keyword argument:
>>> ts.resample('15min', fill_method='ffill', limit=2, loffset='5min').
head()
2015-04-29 08:05:00    30
2015-04-29 08:20:00    30
2015-04-29 08:35:00    30
2015-04-29 08:50:00   NaN
2015-04-29 09:05:00    27
Freq: 15T, dtype: float64
There is another way to fill in missing values. We could employ an algorithm to 
construct new data points that would somehow fit the existing points, for some 
definition of somehow. This process is called interpolation.
We can ask Pandas to interpolate a time series for us:
>>> tsx = ts.resample('15min')
>>> tsx.interpolate().head()
2015-04-29 08:00:00    30.00
2015-04-29 08:15:00    29.25
2015-04-29 08:30:00    28.50
2015-04-29 08:45:00    27.75
2015-04-29 09:00:00    27.00
Freq: 15T, dtype: float64
We saw the default interpolate method – a linear interpolation – in action.  
Pandas assumes a linear relationship between two existing points.

Chapter 5
[ 97 ]
Pandas supports over a dozen interpolation functions, some of which require 
the scipy library to be installed. We will not cover interpolation methods in this 
chapter, but we encourage you to explore the various methods yourself. The right 
interpolation method will depend on the requirements of your application.
Time zone handling
While, by default, Pandas objects are time zone unaware, many real-world 
applications will make use of time zones. As with working with time in general,  
time zones are no trivial matter: do you know which countries have daylight 
saving time and do you know when the time zone is switched in those countries? 
Thankfully, Pandas builds on the time zone capabilities of two popular and proven 
utility libraries for time and date handling: pytz and dateutil:
>>> t = pd.Timestamp('2000-01-01')
>>> t.tz is None
True
To supply time zone information, you can use the tz keyword argument:
>>> t = pd.Timestamp('2000-01-01', tz='Europe/Berlin')
>>> t.tz
<DstTzInfo 'Europe/Berlin' CET+1:00:00 STD>
This works for ranges as well:
>>> rng = pd.date_range('1/1/2000 00:00', periods=10, freq='D', 
tz='Europe/London')
>>> rng
DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04', 
               '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08', 
               '2000-01-09', '2000-01-10'], 
              dtype='datetime64[ns]', freq='D', tz='Europe/London')
Time zone objects can also be constructed beforehand:
>>> import pytz
>>> tz = pytz.timezone('Europe/London')
>>> rng = pd.date_range('1/1/2000 00:00', periods=10, freq='D', tz=tz)
>>> rng
DatetimeIndex(['2000-01-01', '2000-01-02', '2000-01-03', '2000-01-04', 
               '2000-01-05', '2000-01-06', '2000-01-07', '2000-01-08', 
               '2000-01-09', '2000-01-10'], 
              dtype='datetime64[ns]', freq='D', tz='Europe/London')

Time Series
[ 98 ]
Sometimes, you will already have a time zone unaware time series object that you 
would like to make time zone aware. The tz_localize function helps to switch 
between time zone aware and time zone unaware objects:
>>> rng = pd.date_range('1/1/2000 00:00', periods=10, freq='D')
>>> ts = pd.Series(np.random.randn(len(rng)), rng)
>>> ts.index.tz is None
True
>>> ts_utc = ts.tz_localize('UTC')
>>> ts_utc.index.tz
<UTC>
To move a time zone aware object to other time zones, you can use the tz_convert 
method:
>>> ts_utc.tz_convert('Europe/Berlin').index.tz
<DstTzInfo 'Europe/Berlin' LMT+0:53:00 STD>
Finally, to detach any time zone information from an object, it is possible to pass 
None to either tz_convert or tz_localize:
>>> ts_utc.tz_convert(None).index.tz is None
True
>>> ts_utc.tz_localize(None).index.tz is None
True
Timedeltas
Along with the powerful timestamp object, which acts as a building block for the 
DatetimeIndex, there is another useful data structure, which has been introduced  
in Pandas 0.15 – the Timedelta. The Timedelta can serve as a basis for indices as well, 
in this case a TimedeltaIndex.
Timedeltas are differences in times, expressed in difference units. The Timedelta 
class in Pandas is a subclass of datetime.timedelta from the Python standard 
library. As with other Pandas data structures, the Timedelta can be constructed from 
a variety of inputs:
>>> pd.Timedelta('1 days')
Timedelta('1 days 00:00:00')
>>> pd.Timedelta('-1 days 2 min 10s 3us')

Chapter 5
[ 99 ]
Timedelta('-2 days +23:57:49.999997')
>>> pd.Timedelta(days=1,seconds=1)
Timedelta('1 days 00:00:01')
As you would expect, Timedeltas allow basic arithmetic:
>>> pd.Timedelta(days=1) + pd.Timedelta(seconds=1)
Timedelta('1 days 00:00:01')
Similar to to_datetime, there is a to_timedelta function that can parse strings or 
lists of strings into Timedelta structures or TimedeltaIndices:
>>> pd.to_timedelta('20.1s')
Timedelta('0 days 00:00:20.100000')
Instead of absolute dates, we could create an index of timedeltas. Imagine 
measurements from a volcano, for example. We might want to take measurements 
but index it from a given date, for example the date of the last eruption. We could 
create a timedelta index that has the last seven days as entries:
>>> pd.to_timedelta(np.arange(7), unit='D')
TimedeltaIndex(['0 days', '1 days', '2 days', '3 days', '4 days', '5 
days', '6 days'], dtype='timedelta64[ns]', freq=None)
We could then work with time series data, indexed from the last eruption. If we  
had measurements for many eruptions (from possibly multiple volcanos), we would 
have an index that would make comparisons and analysis of this data easier. For 
example, we could ask whether there is a typical pattern that occurs between the 
third day and the fifth day after an eruption. This question would not be impossible 
to answer with a DatetimeIndex, but a TimedeltaIndex makes this kind of 
exploration much more convenient.
Time series plotting
Pandas comes with great support for plotting, and this holds true for time series  
data as well.
As a first example, let's take some monthly data and plot it:
>>> rng = pd.date_range(start='2000', periods=120, freq='MS')
>>> ts = pd.Series(np.random.randint(-10, 10, size=len(rng)), rng).
cumsum()
>>> ts.head()
2000-01-01    -4
2000-02-01    -6

Time Series
[ 100 ]
2000-03-01   -16
2000-04-01   -26
2000-05-01   -24
Freq: MS, dtype: int64
Since matplotlib is used under the hood, we can pass a familiar parameter to plot, 
such as c for color, or title for the chart title:
>>> ts.plot(c='k', title='Example time series')
>>> plt.show()
The following figure shows an example time series plot:
We can overlay an aggregate plot over 2 and 5 years:
>>> ts.resample('2A').plot(c='0.75', ls='--')
>>> ts.resample('5A').plot(c='0.25', ls='-.')

Chapter 5
[ 101 ]
The following figure shows the resampled 2-year plot:
The following figure shows the resample 5-year plot:

Time Series
[ 102 ]
We can pass the kind of chart to the plot method as well. The return value of the 
plot method is an AxesSubplot, which allows us to customize many aspects of the 
plot. Here we are setting the label values on the X axis to the year values from our 
time series:
>>> plt.clf()
>>> tsx = ts.resample('1A')
>>> ax = tsx.plot(kind='bar', color='k')
>>> ax.set_xticklabels(tsx.index.year)
Let's imagine we have four time series that we would like to plot simultaneously.  
We generate a matrix of 1000 × 4 random values and treat each column as a 
separated time series:
>>> plt.clf()
>>> ts = pd.Series(np.random.randn(1000), index=pd.date_range('1/1/2000', 
periods=1000))
>>> df = pd.DataFrame(np.random.randn(1000, 4), index=ts.index, 
columns=['A', 'B', 'C', 'D'])

Chapter 5
[ 103 ]
>>> df = df.cumsum() 
>>> df.plot(color=['k', '0.75', '0.5', '0.25'], ls='--')
Summary
In this chapter we showed how you can work with time series in Pandas. We 
introduced two index types, the DatetimeIndex and the TimedeltaIndex and 
explored their building blocks in depth. Pandas comes with versatile helper 
functions that take much of the pain out of parsing dates of various formats or 
generating fixed frequency sequences. Resampling data can help get a more 
condensed picture of the data, or it can help align various datasets of different 
frequencies to one another. One of the explicit goals of Pandas is to make it easy to 
work with missing data, which is also relevant in the context of upsampling.
Finally, we showed how time series can be visualized. Since matplotlib and Pandas 
are natural companions, we discovered that we can reuse our previous knowledge 
about matplotlib for time series data as well.
In the next chapter, we will explore ways to load and store data in text files  
and databases.

Time Series
[ 104 ]
Practice examples
Exercise 1: Find one or two real-world examples for data sets, which could – in a 
sensible way – be assigned to the following groups:
•	
Fixed frequency data
•	
Variable frequency data
•	
Data where frequency is usually measured in seconds
•	
Data where frequency is measured in nanoseconds
•	
Data, where a TimedeltaIndex would be preferable
Create various fixed frequency ranges:
•	
Every minute between 1 AM and 2 AM on 2000-01-01
•	
Every two hours for a whole week starting 2000-01-01
•	
An entry for every Saturday and Sunday during the year 2000
•	
An entry for every Monday of a month, if it was a business day,  
for the years 2000, 2001 and 2002

[ 105 ]
Interacting with Databases
Data analysis starts with data. It is therefore beneficial to work with data storage 
systems that are simple to set up, operate and where the data access does not become 
a problem in itself. In short, we would like to have database systems that are easy to 
embed into our data analysis processes and workflows. In this book, we focus mostly 
on the Python side of the database interaction, and we will learn how to get data into 
and out of Pandas data structures.
There are numerous ways to store data. In this chapter, we are going to learn to 
interact with three main categories: text formats, binary formats and databases. We 
will focus on two storage solutions, MongoDB and Redis. MongoDB is a document-
oriented database, which is easy to start with, since we can store JSON documents 
and do not need to define a schema upfront. Redis is a popular in-memory data 
structure store on top of which many applications can be built. It is possible to use 
Redis as a fast key-value store, but Redis supports lists, sets, hashes, bit arrays and 
even advanced data structures such as HyperLogLog out of the box as well.
Interacting with data in text format
Text is a great medium and it's a simple way to exchange information.  
The following statement is taken from a quote attributed to Doug McIlroy:  
Write programs to handle text streams, because that is the universal interface.
In this section we will start reading and writing data from and to text files.
Reading data from text format
Normally, the raw data logs of a system are stored in multiple text files, which can 
accumulate a large amount of information over time. Thankfully, it is simple to 
interact with these kinds of files in Python.

Interacting with Databases
[ 106 ]
Pandas supports a number of functions for reading data from a text file into a 
DataFrame object. The most simple one is the read_csv() function. Let's start with a 
small example file:
$ cat example_data/ex_06-01.txt
Name,age,major_id,sex,hometown
Nam,7,1,male,hcm
Mai,11,1,female,hcm
Lan,25,3,female,hn
Hung,42,3,male,tn
Nghia,26,3,male,dn
Vinh,39,3,male,vl
Hong,28,4,female,dn
The cat is the Unix shell command that can be used to print the 
content of a file to the screen.
In the above example file, each column is separated by comma and the first row is 
a header row, containing column names. To read the data file into the DataFrame 
object, we type the following command:
>>> df_ex1 = pd.read_csv('example_data/ex_06-01.txt')
>>> df_ex1
    Name  age  major_id     sex hometown
0    Nam    7         1    male      hcm
1    Mai   11         1  female      hcm
2    Lan   25         3  female       hn
3   Hung   42         3    male       tn
4  Nghia   26         3    male       dn
5   Vinh   39         3    male       vl
6   Hong   28         4  female       dn
We see that the read_csv function uses a comma as the default delimiter between 
columns in the text file and the first row is automatically used as a header for the 
columns. If we want to change this setting, we can use the sep parameter to change 
the separated symbol and set header=None in case the example file does not have a 
caption row. 

Chapter 6
[ 107 ]
See the below example:
$ cat example_data/ex_06-02.txt
Nam     7       1       male    hcm
Mai     11      1       female  hcm
Lan     25      3       female  hn
Hung    42      3       male    tn
Nghia   26      3       male    dn
Vinh    39      3       male    vl
Hong    28      4       female  dn
>>> df_ex2 = pd.read_csv('example_data/ex_06-02.txt',
                         sep = '\t', header=None)
>>> df_ex2
       0   1  2       3    4
0    Nam   7  1    male  hcm
1    Mai  11  1  female  hcm
2    Lan  25  3  female   hn
3   Hung  42  3    male   tn
4  Nghia  26  3    male   dn
5   Vinh  39  3    male   vl
6   Hong  28  4  female   dn
We can also set a specific row as the caption row by using the header that's equal to  
the index of the selected row. Similarly, when we want to use any column in the  
data file as the column index of DataFrame, we set index_col to the name or index 
of the column. We again use the second data file example_data/ex_06-02.txt to 
illustrate this:
>>> df_ex3 = pd.read_csv('example_data/ex_06-02.txt',
                         sep = '\t', header=None,
                         index_col=0)
>>> df_ex3
        1  2       3    4
0
Nam     7  1    male  hcm
Mai    11  1  female  hcm
Lan    25  3  female   hn

Interacting with Databases
[ 108 ]
Hung   42  3    male   tn
Nghia  26  3    male   dn
Vinh   39  3    male   vl
Hong   28  4  female   dn
Apart from those parameters, we still have a lot of useful ones that can help us load 
data files into Pandas objects more effectively. The following table shows some 
common parameters:
Parameter
Value
Description
dtype
Type name or dictionary of 
type of columns
Sets the data type for data 
or columns. By default it 
will try to infer the most 
appropriate data type.
skiprows
List-like or integer
The number of lines to skip 
(starting from 0).
na_values
List-like or dict, default None
Values to recognize as NA/
NaN. If a dict is passed, this 
can be set on a per-column 
basis.
true_values
List
A list of values to be 
converted to Boolean True as 
well.
false_values
List
A list of values to be 
converted to Boolean False 
as well.
keep_default_na
Bool, default True
If the na_values parameter 
is present and keep_
default_na is False, 
the default NaN values are 
ignored, otherwise they are 
appended to
thousands
Str, default None
The thousands separator
nrows
Int, default None
Limits the number of rows to 
read from the file.
error_bad_lines
Boolean, default True
If set to True, a DataFrame 
is returned, even if an error 
occurred during parsing.

Chapter 6
[ 109 ]
Besides the read_csv() function, we also have some other parsing functions  
in Pandas:
Function
Description
read_table
Read the general delimited file into DataFrame
read_fwf
Read a table of fixed-width formatted lines into DataFrame
read_clipboard
Read text from the clipboard and pass to read_table. It is 
useful for converting tables from web pages
In some situations, we cannot automatically parse data files from the disk using 
these functions. In that case, we can also open files and iterate through the reader, 
supported by the CSV module in the standard library:
$ cat example_data/ex_06-03.txt
Nam     7       1       male    hcm
Mai     11      1       female  hcm
Lan     25      3       female  hn
Hung    42      3       male    tn      single
Nghia   26      3       male    dn      single
Vinh    39      3       male    vl
Hong    28      4       female  dn
>>> import csv
>>> f = open('data/ex_06-03.txt')
>>> r = csv.reader(f, delimiter='\t')
>>> for line in r:
>>>    print(line)
['Nam', '7', '1', 'male', 'hcm']
['Mai', '11', '1', 'female', 'hcm']
['Lan', '25', '3', 'female', 'hn']
['Hung', '42', '3', 'male', 'tn', 'single']
['Nghia', '26', '3', 'male', 'dn', 'single']
['Vinh', '39', '3', 'male', 'vl']
['Hong', '28', '4', 'female', 'dn']

Interacting with Databases
[ 110 ]
Writing data to text format
We saw how to load data from a text file into a Pandas data structure. Now, we 
will learn how to export data from the data object of a program to a text file. 
Corresponding to the read_csv() function, we also have the to_csv() function, 
supported by Pandas. Let's see an example below:
>>> df_ex3.to_csv('example_data/ex_06-02.out', sep = ';')
The result will look like this:
$ cat example_data/ex_06-02.out
0;1;2;3;4
Nam;7;1;male;hcm
Mai;11;1;female;hcm
Lan;25;3;female;hn
Hung;42;3;male;tn
Nghia;26;3;male;dn
Vinh;39;3;male;vl
Hong;28;4;female;dn
If we want to skip the header line or index column when writing out data into a disk 
file, we can set a False value to the header and index parameters:
>>> import sys
>>> df_ex3.to_csv(sys.stdout, sep='\t',
                  header=False, index=False)
7       1       male    hcm
11      1       female  hcm
25      3       female  hn
42      3       male    tn
26      3       male    dn
39      3       male    vl
28      4       female  dn
We can also write a subset of the columns of the DataFrame to the file by specifying 
them in the columns parameter:
>>> df_ex3.to_csv(sys.stdout, columns=[3,1,4],
                  header=False, sep='\t')

Chapter 6
[ 111 ]
Nam     male    7       hcm
Mai     female  11      hcm
Lan     female  25      hn
Hung    male    42      tn
Nghia   male    26      dn
Vinh    male    39      vl
Hong    female  28      dn
With series objects, we can use the same function to write data into text files, with 
mostly the same parameters as above.
Interacting with data in binary format
We can read and write binary serialization of Python objects with the pickle module, 
which can be found in the standard library. Object serialization can be useful, if you 
work with objects that take a long time to create, like some machine learning models. 
By pickling such objects, subsequent access to this model can be made faster. It also 
allows you to distribute Python objects in a standardized way.
Pandas includes support for pickling out of the box. The relevant methods are the  
read_pickle() and to_pickle() functions to read and write data from and to 
files easily. Those methods will write data to disk in the pickle format, which is a 
convenient short-term storage format:
>>> df_ex3.to_pickle('example_data/ex_06-03.out')
>>> pd.read_pickle('example_data/ex_06-03.out')
        1  2       3    4
0
Nam     7  1    male  hcm
Mai    11  1  female  hcm
Lan    25  3  female   hn
Hung   42  3    male   tn
Nghia  26  3    male   dn
Vinh   39  3    male   vl
Hong   28  4  female   dn

Interacting with Databases
[ 112 ]
HDF5
HDF5 is not a database, but a data model and file format. It is suited for write-one, 
read-many datasets. An HDF5 file includes two kinds of objects: data sets, which 
are array-like collections of data, and groups, which are folder-like containers what 
hold data sets and other groups. There are some interfaces for interacting with HDF5 
format in Python, such as h5py which uses familiar NumPy and Python constructs, 
such as dictionaries and NumPy array syntax. With h5py, we have high-level 
interface to the HDF5 API which helps us to get started. However, in this book, we 
will introduce another library for this kind of format called PyTables, which works 
well with Pandas objects:
>>> store = pd.HDFStore('hdf5_store.h5')
>>> store
<class 'pandas.io.pytables.HDFStore'>
File path: hdf5_store.h5
Empty
We created an empty HDF5 file, named hdf5_store.h5. Now, we can write data to 
the file just like adding key-value pairs to a dict:
>>> store['ex3'] = df_ex3
>>> store['name'] = df_ex2[0]
>>> store['hometown'] = df_ex3[4]
>>> store
<class 'pandas.io.pytables.HDFStore'>
File path: hdf5_store.h5
/ex3                  frame        (shape->[7,4])
/hometown             series       (shape->[1])
/name                 series       (shape->[1])
Objects stored in the HDF5 file can be retrieved by specifying the object keys:
>>> store['name']
0      Nam
1      Mai
2      Lan
3     Hung
4    Nghia
5     Vinh
6     Hong
Name: 0, dtype: object

Chapter 6
[ 113 ]
Once we have finished interacting with the HDF5 file, we close it to release the file 
handle:
>>> store.close()
>>> store
<class 'pandas.io.pytables.HDFStore'>
File path: hdf5_store.h5
File is CLOSED
There are other supported functions that are useful for working with the HDF5 
format. You should explore ,in more detail, two libraries – pytables and h5py – if 
you need to work with huge quantities of data.
Interacting with data in MongoDB
Many applications require more robust storage systems then text files, which is why 
many applications use databases to store data. There are many kinds of databases, 
but there are two broad categories: relational databases, which support a standard 
declarative language called SQL, and so called NoSQL databases, which are often 
able to work without a predefined schema and where a data instance is more 
properly described as a document, rather as a row.
MongoDB is a kind of NoSQL database that stores data as documents, which are 
grouped together in collections. Documents are expressed as JSON objects. It is 
fast and scalable in storing, and also flexible in querying, data. To use MongoDB 
in Python, we need to import the pymongo package and open a connection to the 
database by passing a hostname and port. We suppose that we have a MongoDB 
instance, running on the default host (localhost) and port (27017):
>>> import pymongo
>>> conn = pymongo.MongoClient(host='localhost', port=27017)
If we do not put any parameters into the pymongo.MongoClient() function, it will 
automatically use the default host and port.
In the next step, we will interact with databases inside the MongoDB instance.  
We can list all databases that are available in the instance:
>>> conn.database_names()
['local']
>>> lc = conn.local
>>> lc
Database(MongoClient('localhost', 27017), 'local')

Interacting with Databases
[ 114 ]
The above snippet says that our MongoDB instance only has one database, named 
'local'. If the databases and collections we point to do not exist, MongoDB will create 
them as necessary:
>>> db = conn.db
>>> db
Database(MongoClient('localhost', 27017), 'db')
Each database contains groups of documents, called collections. We can understand 
them as tables in a relational database. To list all existing collections in a database, 
we use collection_names() function:
>>> lc.collection_names()
['startup_log', 'system.indexes']
>>> db.collection_names()
[]
Our db database does not have any collections yet. Let's create a collection, named 
person, and insert data from a DataFrame object to it:
>>> collection = db.person
>>> collection
Collection(Database(MongoClient('localhost', 27017), 'db'), 'person')
>>> # insert df_ex2 DataFrame into created collection
>>> import json
>>> records = json.load(df_ex2.T.to_json()).values()
>>> records
dict_values([{'2': 3, '3': 'male', '1': 39, '4': 'vl', '0': 'Vinh'}, 
{'2': 3, '3': 'male', '1': 26, '4': 'dn', '0': 'Nghia'}, {'2': 4, '3': 
'female', '1': 28, '4': 'dn', '0': 'Hong'}, {'2': 3, '3': 'female', '1': 
25, '4': 'hn', '0': 'Lan'}, {'2': 3, '3': 'male', '1': 42, '4': 'tn', 
'0': 'Hung'}, {'2': 1, '3':'male', '1': 7, '4': 'hcm', '0': 'Nam'}, {'2': 
1, '3': 'female', '1': 11, '4': 'hcm', '0': 'Mai'}])
>>> collection.insert(records)
[ObjectId('557da218f21c761d7c176a40'),
 ObjectId('557da218f21c761d7c176a41'),
 ObjectId('557da218f21c761d7c176a42'),
 ObjectId('557da218f21c761d7c176a43'),
 ObjectId('557da218f21c761d7c176a44'),
 ObjectId('557da218f21c761d7c176a45'),
 ObjectId('557da218f21c761d7c176a46')]

Chapter 6
[ 115 ]
The df_ex2 is transposed and converted to a JSON string before loading into a 
dictionary. The insert() function receives our created dictionary from df_ex2  
and saves it to the collection.
If we want to list all data inside the collection, we can execute the following 
commands:
>>> for cur in collection.find():
>>>     print(cur)
{'4': 'vl', '2': 3, '3': 'male', '1': 39, '_id': 
ObjectId('557da218f21c761d7c176
a40'), '0': 'Vinh'}
{'4': 'dn', '2': 3, '3': 'male', '1': 26, '_id': 
ObjectId('557da218f21c761d7c176
a41'), '0': 'Nghia'}
{'4': 'dn', '2': 4, '3': 'female', '1': 28, '_id': 
ObjectId('557da218f21c761d7c1
76a42'), '0': 'Hong'}
{'4': 'hn', '2': 3, '3': 'female', '1': 25, '_id': 
ObjectId('557da218f21c761d7c1
76a43'), '0': 'Lan'}
{'4': 'tn', '2': 3, '3': 'male', '1': 42, '_id': 
ObjectId('557da218f21c761d7c176
a44'), '0': 'Hung'}
{'4': 'hcm', '2': 1, '3': 'male', '1': 7, '_id': 
ObjectId('557da218f21c761d7c176
a45'), '0': 'Nam'}
{'4': 'hcm', '2': 1, '3': 'female', '1': 11, '_id': 
ObjectId('557da218f21c761d7c
176a46'), '0': 'Mai'}
If we want to query data from the created collection with some conditions, we can 
use the find() function and pass in a dictionary describing the documents we want 
to retrieve. The returned result is a cursor type, which supports the iterator protocol:
>>> cur = collection.find({'3' : 'male'})
>>> type(cur)
pymongo.cursor.Cursor
>>> result = pd.DataFrame(list(cur))

Interacting with Databases
[ 116 ]
>>> result
       0   1  2     3    4                       _id
0   Vinh  39  3  male   vl  557da218f21c761d7c176a40
1  Nghia  26  3  male   dn  557da218f21c761d7c176a41
2   Hung  42  3  male   tn  557da218f21c761d7c176a44
3    Nam   7  1  male  hcm  557da218f21c761d7c176a45
Sometimes, we want to delete data in MongdoDB. All we need to do is to pass a 
query to the remove() method on the collection:
>>> # before removing data
>>> pd.DataFrame(list(collection.find()))
       0   1  2       3    4                       _id
0   Vinh  39  3    male   vl  557da218f21c761d7c176a40
1  Nghia  26  3    male   dn  557da218f21c761d7c176a41
2   Hong  28  4  female   dn  557da218f21c761d7c176a42
3    Lan  25  3  female   hn  557da218f21c761d7c176a43
4   Hung  42  3    male   tn  557da218f21c761d7c176a44
5    Nam   7  1    male  hcm  557da218f21c761d7c176a45
6    Mai  11  1  female  hcm  557da218f21c761d7c176a46
>>> # after removing records which have '2' column as 1 and '3' column as 
'male'
>>> collection.remove({'2': 1, '3': 'male'})
{'n': 1, 'ok': 1}
>>> cur_all = collection.find();
>>> pd.DataFrame(list(cur_all))
       0   1  2       3    4                       _id
0   Vinh  39  3    male   vl  557da218f21c761d7c176a40
1  Nghia  26  3    male   dn  557da218f21c761d7c176a41
2   Hong  28  4  female   dn  557da218f21c761d7c176a42
3    Lan  25  3  female   hn  557da218f21c761d7c176a43
4   Hung  42  3    male   tn  557da218f21c761d7c176a44
5    Mai  11  1  female  hcm  557da218f21c761d7c176a46

Chapter 6
[ 117 ]
We learned step by step how to insert, query and delete data in a collection. Now, we 
will show how to update existing data in a collection in MongoDB:
>>> doc = collection.find_one({'1' : 42})
>>> doc['4'] = 'hcm'
>>> collection.save(doc)
ObjectId('557da218f21c761d7c176a44')
>>> pd.DataFrame(list(collection.find()))
       0   1  2       3    4                       _id
0   Vinh  39  3    male   vl  557da218f21c761d7c176a40
1  Nghia  26  3    male   dn  557da218f21c761d7c176a41
2   Hong  28  4  female   dn  557da218f21c761d7c176a42
3    Lan  25  3  female   hn  557da218f21c761d7c176a43
4   Hung  42  3    male  hcm  557da218f21c761d7c176a44
5    Mai  11  1  female  hcm  557da218f21c761d7c176a46
The following table shows methods that provide shortcuts to manipulate documents 
in MongoDB:
Update Method
Description
inc()
Increment a numeric field
set()
Set certain fields to new values
unset()
Remove a field from the document
push()
Append a value onto an array in the document
pushAll()
Append several values onto an array in the document
addToSet()
Add a value to an array, only if it does not exist
pop()
Remove the last value of an array
pull()
Remove all occurrences of a value from an array
pullAll()
Remove all occurrences of any set of values from an array
rename()
Rename a field
bit()
Update a value by bitwise operation

Interacting with Databases
[ 118 ]
Interacting with data in Redis
Redis is an advanced kind of key-value store where the values can be of different 
types: string, list, set, sorted set or hash. Redis stores data in memory like 
memcached but it can be persisted on disk, unlike memcached, which has no such 
option. Redis supports fast reads and writes, in the order of 100,000 set or get 
operations per second.
To interact with Redis, we need to install the Redis-py module to Python, which is 
available on pypi and can be installed with pip:
$ pip install redis
Now, we can connect to Redis via the host and port of the DB server. We assume 
that we have already installed a Redis server, which is running with the default host 
(localhost) and port (6379) parameters:
>>> import redis
>>> r = redis.StrictRedis(host='127.0.0.1', port=6379)
>>> r
StrictRedis<ConnectionPool<Connection<host=localhost,port=6379,db=0>>>
As a first step to storing data in Redis, we need to define which kind of data structure 
is suitable for our requirements. In this section, we will introduce four commonly 
used data structures in Redis: simple value, list, set and ordered set. Though data is 
stored into Redis in many different data structures, each value must be associated 
with a key.
The simple value
This is the most basic kind of value in Redis. For every key in Redis, we also have a 
value that can have a data type, such as string, integer or double. Let's start with an 
example for setting and getting data to and from Redis:
>>> r.set('gender:An', 'male')
True
>>> r.get('gender:An')
b'male'
In this example we want to store the gender info of a person, named An into Redis. 
Our key is gender:An and our value is male. Both of them are a type of string.

Chapter 6
[ 119 ]
The set() function receives two parameters: the key and the value. The first 
parameter is the key and the second parameter is value. If we want to update the 
value of this key, we just call the function again and change the value of the second 
parameter. Redis automatically updates it.
The get() function will retrieve the value of our key, which is passed as the 
parameter. In this case, we want to get gender information of the key gender:An.
In the second example, we show you another kind of value type, an integer:
>>> r.set('visited_time:An', 12)
True
>>> r.get('visited_time:An')
b'12'
>>> r.incr('visited_time:An', 1)
13
>>> r.get('visited_time:An')
b'13'
We saw a new function, incr(), which used to increment the value of key by a 
given amount. If our key does not exist, RedisDB will create the key with the given 
increment as the value.
List
We have a few methods for interacting with list values in Redis. The following 
example uses rpush() and lrange() functions to put and get list data to and from  
the DB:
>>> r.rpush('name_list', 'Tom')
1L
>>> r.rpush('name_list', 'John')
2L
>>> r.rpush('name_list', 'Mary')
3L
>>> r.rpush('name_list', 'Jan')
4L
>>> r.lrange('name_list', 0, -1)
[b'Tom', b'John', b'Mary', b'Jan']
>>> r.llen('name_list')
4
>>> r.lindex('name_list', 1)
b'John'

Interacting with Databases
[ 120 ]
Besides the rpush() and lrange() functions we used in the example, we also 
want to introduce two others functions. First, the llen() function is used to get the 
length of our list in the Redis for a given key. The lindex() function is another way 
to retrieve an item of the list. We need to pass two parameters into the function: a 
key and an index of item in the list. The following table lists some other powerful 
functions in processing list data structure with Redis:
Function
Description
rpushx(name, value)
Push value onto the tail of the list name if name exists
rpop(name)
Remove and return the last item of the list name
lset(name, index, value)
Set item at the index position of the list name to  
input value
lpushx(name,value)
Push value on the head of the list name if name exists
lpop(name)
Remove and return the first item of the list name
Set
This data structure is also similar to the list type. However, in contrast to a list, we 
cannot store duplicate values in our set:
>>> r.sadd('country', 'USA')
1
>>> r.sadd('country', 'Italy')
1
>>> r.sadd('country', 'Singapore')
1
>>> r.sadd('country', 'Singapore')
0
>>> r.smembers('country')
{b'Italy', b'Singapore', b'USA'}
>>> r.srem('country', 'Singapore')
1
>>> r.smembers('country')
{b'Italy', b'USA'}

Chapter 6
[ 121 ]
Corresponding to the list data structure, we also have a number of functions to get, 
set, update or delete items in the set. They are listed in the supported functions for 
set data structure, in the following table:
Function
Description
sadd(name, values)
Add value(s) to the set with key name
scard(name)
Return the number of element in the set with key name
smembers(name)
Return all members of the set with key name
srem(name, values)
Remove value(s) from the set with key name
Ordered set
The ordered set data structure takes an extra attribute when we add data to a set 
called score. An ordered set will use the score to determine the order of the elements 
in the set:
>>> r.zadd('person:A', 10, 'sub:Math')
1
>>> r.zadd('person:A', 7, 'sub:Bio')
1
>>> r.zadd('person:A', 8, 'sub:Chem')
1
>>> r.zrange('person:A', 0, -1)
[b'sub:Bio', b'sub:Chem', b'sub:Math']
>>> r.zrange('person:A', 0, -1, withscores=True)
[(b'sub:Bio', 7.0), (b'sub:Chem', 8.0), (b'sub:Math', 10.0)]
By using the zrange(name, start, end) function, we can get a range of values 
from the sorted set between the start and end score sorted in ascending order 
by default. If we want to change the way method of sorting, we can set the desc 
parameter to True. The withscore parameter is used in case we want to get the 
scores along with the return values. The return type is a list of (value, score) pairs as 
you can see in the above example. 

Interacting with Databases
[ 122 ]
See the below table for more functions available on ordered sets:
Function
Description
zcard(name)
Return the number of elements in the sorted set with 
key name
zincrby(name, value, 
amount=1)
Increment the score of value in the sorted set with 
key name by amount
zrangebyscore(name, min, 
max, withscores=False, 
start=None, num=None)
Return a range of values from the sorted set with key 
name with a score between min and max.
If withscores is true, return the scores along with 
the values.
If start and num are given, return a slice of the range
zrank(name, value)
Return a 0-based value indicating the rank of value 
in the sorted set with key name
zrem(name, values)
Remove member value(s) from the sorted set with 
key name
Summary
We finished covering the basics of interacting with data in different commonly  
used storage mechanisms from the simple ones, such as text files, over more 
structured ones, such as HDF5, to more sophisticated data storage systems, such 
as MongoDB and Redis. The most suitable type of storage will depend on your use 
case. The choice of the data storage layer technology plays an important role in the 
overall design of data processing systems. Sometimes, we need to combine various 
database systems to store our data, such as complexity of the data, performance of 
the system or computation requirements.

Chapter 6
[ 123 ]
Practice exercise
•	
Take a data set of your choice and design storage options for it. Consider 
text files, HDF5, a document database, and a data structure store as possible 
persistent options. Also evaluate how difficult (by some metric, for examples, 
how many lines of code) it would be to update or delete a specific item. 
Which storage type is the easiest to set up? Which storage type supports the 
most flexible queries?
•	
In Chapter 3, Data Analysis with Pandas we saw that it is possible to create 
hierarchical indices with Pandas. As an example, assume that you have data 
on each city with more than 1 million inhabitants and that we have a two 
level index, so we can address individual cities, but also whole countries. 
How would you represent this hierarchical relationship with the various 
storage options presented in this chapter: text files, HDF5, MongoDB, and 
Redis? What do you believe would be most convenient to work with in the 
long run?


[ 125 ]
Data Analysis Application 
Examples
In this chapter, we want to get you acquainted with typical data preparation tasks 
and analysis techniques, because being fluent in preparing, grouping, and reshaping 
data is an important building block for successful data analysis.
While preparing data seems like a mundane task – and often it is – it is a step we 
cannot skip, although we can strive to simplify it by using tools such as Pandas.
Why is preparation necessary at all? Because most useful data will come from the 
real world and will have deficiencies, contain errors or will be fragmentary.
There are more reasons why data preparation is useful: it gets you in close contact 
with the raw material. Knowing your input helps you to spot potential errors early 
and build confidence in your results.
Here are a few data preparation scenarios:
•	
A client hands you three files, each containing time series data about a single 
geological phenomenon, but the observed data is recorded on different 
intervals and uses different separators
•	
A machine learning algorithm can only work with numeric data, but your 
input only contains text labels
•	
You are handed the raw logs of a web server of an up and coming service 
and your task is to make suggestions on a growth strategy, based on existing 
visitor behavior

Data Analysis Application Examples
[ 126 ]
Data munging
The arsenal of tools for data munging is huge, and while we will focus on Python we 
want to mention some useful tools as well. If they are available on your system and 
you expect to work a lot with data, they are worth learning.
One group of tools belongs to the UNIX tradition, which emphasizes text  
processing and as a consequence has, over the last four decades, developed many 
high-performance and battle-tested tools for dealing with text. Some common tools 
are: sed, grep, awk, sort, uniq, tr, cut, tail, and head. They do very elementary 
things, such as filtering out lines (grep) or columns (cut) from files, replacing text 
(sed, tr) or displaying only parts of files (head, tail).
We want to demonstrate the power of these tools with a single example only.
Imagine you are handed the log files of a web server and you are interested in the 
distribution of the IP addresses.
Each line of the log file contains an entry in the common log server format  
(you can download this data set from http://ita.ee.lbl.gov/html/contrib/
EPA- HTTP.html):
$ cat epa-html.txt
wpbfl2-45.gate.net [29:23:56:12] "GET /Access/ HTTP/1.0" 200 2376ebaca.
icsi.net [30:00:22:20] "GET /Info.html HTTP/1.0" 200 884
For instance, we want to know how often certain users have visited our site.
We are interested in the first column only, since this is where the IP address or 
hostname can be found. After that, we need to count the number of occurrences of 
each host and finally display the results in a friendly way.
The sort | uniq -c stanza is our workhorse here: it sorts the data first and uniq -c 
will save the number of occurrences along with the value. The sort -nr | head 
-15 is our formatting part; we sort numerically (-n) and in reverse (-r), and keep 
only the top 15 entries.

Chapter 7
[ 127 ]
Putting it all together with pipes:
$ cut -d ' ' -f 1 epa-http.txt | sort | uniq -c | sort -nr | head -15
294 sandy.rtptok1.epa.gov
292 e659229.boeing.com
266 wicdgserv.wic.epa.gov
263 keyhole.es.dupont.com
248 dwilson.pr.mcs.net
176 oea4.r8stw56.epa.gov
174 macip26.nacion.co.cr
172 dcimsd23.dcimsd.epa.gov
167 www-b1.proxy.aol.com
158 piweba3y.prodigy.com
152 wictrn13.dcwictrn.epa.gov
151 nntp1.reach.com
151 inetg1.arco.com
149 canto04.nmsu.edu
146 weisman.metrokc.gov
With one command, we get to convert a sequential server log into an ordered list of 
the most common hosts that visited our site. We also see that we do not seem to have 
large differences in the number of visits among our top users.
There are more little helpful tools of which the following are just a tiny selection:
•	
csvkit: This is the suite of utilities for working with CSV, the king of tabular 
file formats
•	
jq: This is a lightweight and flexible command-line JSON processor
•	
xmlstarlet: This is a tool that supports XML queries with XPath, among 
other things
•	
q: This runs SQL on text files
Where the UNIX command line ends, lightweight languages take over. You might  
be able to get an impression from text only, but your colleagues might appreciate 
visual representations, such as charts or pretty graphs, generated by matplotlib, 
much more.
Python and its data tools ecosystem are much more versatile than the command line, 
but for first explorations and simple operations the effectiveness of the command 
line is often unbeatable.

Data Analysis Application Examples
[ 128 ]
Cleaning data
Most real-world data will have some defects and therefore will need to go through 
a cleaning step first. We start with a small file. Although this file contains only four 
rows, it will allow us to demonstrate the process up to a cleaned data set:
$ cat small.csv
22,6.1
41,5.7
  18,5.3*
29,NA
Note that this file has a few issues. The lines that contain values are all  
comma-separated, but we have missing (NA) and probably unclean (5.3*) values.  
We can load this file into a data frame, nevertheless:
>>> import pandas as pd
>>> df = pd.read_csv("small.csv")
>>> df
   22   6.1
0  41   5.7
1  18  5.3*
2  29   NaN
Pandas used the first row as header, but this is not what we want:
>>> df = pd.read_csv("small.csv", header=None)
>>> df
    0     1
0  22   6.1
1  41   5.7
2  18  5.3*
3  29   NaN
This is better, but instead of numeric values, we would like to supply our own 
column names:
>>> df = pd.read_csv("small.csv", names=["age", "height"])
>>> df
   age height
0   22    6.1
1   41    5.7
2   18   5.3*
3   29    NaN

Chapter 7
[ 129 ]
The age column looks good, since Pandas already inferred the intended type, but the 
height cannot be parsed into numeric values yet:
>>> df.age.dtype
dtype('int64')
>>> df.height.dtype
dtype('O')
If we try to coerce the height column into float values, Pandas will report an 
exception:
>>> df.height.astype('float')
ValueError: invalid literal for float(): 5.3*
We could use whatever value is parseable as a float and throw away the rest with the 
convert_objects method:
>>> df.height.convert_objects(convert_numeric=True)
0    6.1
1    5.7
2    NaN
3    NaN
Name: height, dtype: float64
If we know in advance the undesirable characters in our data set, we can 
augment the read_csv method with a custom converter function:
>>> remove_stars = lambda s: s.replace("*", "")
>>> df = pd.read_csv("small.csv", names=["age", "height"],
                     converters={"height": remove_stars})
>>> df
   age height
0   22    6.1
1   41    5.7
2   18    5.3
3   29     NA

Data Analysis Application Examples
[ 130 ]
Now we can finally make the height column a bit more useful. We can assign it the 
updated version, which has the favored type:
>>> df.height = df.height.convert_objects(convert_numeric=True)
>>> df
   age  height
0   22     6.1
1   41     5.7
2   18     5.3
3   29     NaN
If we wanted to only keep the complete entries, we could drop any row that contains 
undefined values:
>>> df.dropna()
   age  height
0   22     6.1
1   41     5.7
2   18     5.3
We could use a default height, maybe a fixed value:
>>> df.fillna(5.0)
   age  height
0   22     6.1
1   41     5.7
2   18     5.3
3   29     5.0
On the other hand, we could also use the average of the existing values:
>>> df.fillna(df.height.mean())
   age  height
0   22     6.1
1   41     5.7
2   18     5.3
3   29     5.7
The last three data frames are complete and correct, depending on your definition 
of correct when dealing with missing values. Especially, the columns have the 
requested types and are ready for further analysis. Which of the data frames is best 
suited will depend on the task at hand.

Chapter 7
[ 131 ]
Filtering
Even if we have clean and probably correct data, we might want to use only parts 
of it or we might want to check for outliers. An outlier is an observation point that 
is distant from other observations because of variability or measurement errors. In 
both cases, we want to reduce the number of elements in our data set to make it more 
relevant for further processing.
In this example, we will try to find potential outliers. We will use the Europe Brent 
Crude Oil Spot Price as recorded by the U.S. Energy Information Administration. 
The raw Excel data is available from http://www.eia.gov/dnav/pet/hist_xls/
rbrted.xls (it can be found in the second worksheet). We cleaned the data slightly 
(the cleaning process is part of an exercise at the end of this chapter) and will work 
with the following data frame, containing 7160 entries, ranging from 1987 to 2015:
>>> df.head()
        date  price
0 1987-05-20  18.63
1 1987-05-21  18.45
2 1987-05-22  18.55
3 1987-05-25  18.60
4 1987-05-26  18.63
>>> df.tail()
           date  price
7155 2015-08-04  49.08
7156 2015-08-05  49.04
7157 2015-08-06  47.80
7158 2015-08-07  47.54
7159 2015-08-10  48.30
While many people know about oil prices – be it from the news or the filling station 
– let us forget anything we know about it for a minute. We could first ask for the 
extremes:
>>> df[df.price==df.price.min()]
           date  price
2937 1998-12-10    9.1
>>> df[df.price==df.price.max()]
           date   price
5373 2008-07-03  143.95

Data Analysis Application Examples
[ 132 ]
Another way to find potential outliers would be to ask for values that deviate most 
from the mean. We can use the np.abs function to calculate the deviation from the 
mean first:
>>> np.abs(df.price - df.price.mean())
0       26.17137  1       26.35137  7157     2.99863
7158     2.73863  7159     3.49863
We can now compare this deviation from a multiple – we choose 2.5 – of the 
standard deviation:
>>> import numpy as np
>>> df[np.abs(df.price - df.price.mean()) > 2.5 * df.price.std()]
       date   price 
5354 2008-06-06  132.81
5355 2008-06-09  134.43
5356 2008-06-10  135.24
5357 2008-06-11  134.52
5358 2008-06-12  132.11
5359 2008-06-13  134.29
5360 2008-06-16  133.90
5361 2008-06-17  131.27
5363 2008-06-19  131.84
5364 2008-06-20  134.28
5365 2008-06-23  134.54
5366 2008-06-24  135.37
5367 2008-06-25  131.59
5368 2008-06-26  136.82
5369 2008-06-27  139.38
5370 2008-06-30  138.40
5371 2008-07-01  140.67
5372 2008-07-02  141.24
5373 2008-07-03  143.95
5374 2008-07-07  139.62
5375 2008-07-08  134.15
5376 2008-07-09  133.91
5377 2008-07-10  135.81
5378 2008-07-11  143.68
5379 2008-07-14  142.43
5380 2008-07-15  136.02
5381 2008-07-16  133.31
5382 2008-07-17  134.16

Chapter 7
[ 133 ]
We see that those few days in summer 2008 must have been special. Sure enough, it 
is not difficult to find articles and essays with titles like Causes and Consequences of the 
Oil Shock of 2007–08. We have discovered a trace to these events solely by looking at 
the data.
We could ask the above question for each decade separately. We first make our data 
frame look more like a time series:
>>> df.index = df.date
>>> del df["date"]
>>> df.head()
            price 
date
1987-05-20  18.63  1987-05-21  18.45
1987-05-22  18.55  1987-05-25  18.60
1987-05-26  18.63
We could filter out the eighties:
>>> decade = df["1980":"1989"]
>>> decade[np.abs(decade.price - decade.price.mean()) > 2.5 * decade.
price.std()]
            price
date
1988-10-03  11.60  1988-10-04  11.65  1988-10-05  11.20  1988-10-06  
11.30  1988-10-07  11.35
We observe that within the data available (1987–1989), the fall of 1988 exhibits a 
slight spike in the oil prices. Similarly, during the nineties, we see that we have a 
larger deviation, in the fall of 1990:
>>> decade = df["1990":"1999"]
>>> decade[np.abs(decade.price - decade.price.mean()) > 5 * decade.price.
std()]
            price 
date
1990-09-24  40.75  1990-09-26  40.85  1990-09-27  41.45  1990-09-28  
41.00  1990-10-09  40.90  1990-10-10  40.20  1990-10-11  41.15
There are many more use cases for filtering data. Space and time are typical units: 
you might want to filter census data by state or city, or economical data by quarter. 
The possibilities are endless and will be driven by your project.

Data Analysis Application Examples
[ 134 ]
Merging data
The situation is common: you have multiple data sources, but in order to make 
statements about the content, you would rather combine them. Fortunately, Pandas' 
concatenation and merge functions abstract away most of the pain, when combining, 
joining, or aligning data. It does so in a highly optimized manner as well.
In a case where two data frames have a similar shape, it might be useful to just 
append one after the other. Maybe A and B are products and one data frame contains 
the number of items sold per product in a store:
>>> df1 = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})
>>> df1
   A  B
0  1  4
1  2  5
2  3  6
>>> df2 = pd.DataFrame({'A': [4, 5, 6], 'B': [7, 8, 9]})
>>> df2
   A  B
0  4  7
1  5  8
2  6  9
>>> df1.append(df2)
   A  B
0  1  4
1  2  5
2  3  6
0  4  7
1  5  8
2  6  9
Sometimes, we won't care about the indices of the originating data frames:
>>> df1.append(df2, ignore_index=True)
   A  B
0  1  4
1  2  5
2  3  6
3  4  7
4  5  8
5  6  9

Chapter 7
[ 135 ]
A more flexible way to combine objects is offered by the pd.concat function, which 
takes an arbitrary number of series, data frames, or panels as input. The default 
behavior resembles an append:
>>> pd.concat([df1, df2])
   A  B
0  1  4
1  2  5
2  3  6
0  4  7
1  5  8
2  6  9
The default concat operation appends both frames along the rows – or index, which 
corresponds to axis 0. To concatenate along the columns, we can pass in the axis 
keyword argument:
>>> pd.concat([df1, df2], axis=1)
   A  B  A  B
0  1  4  4  7
1  2  5  5  8
2  3  6  6  9
We can add keys to create a hierarchical index.
>>> pd.concat([df1, df2], keys=['UK', 'DE'])
      A  B
UK 0  1  4
   1  2  5
   2  3  6
DE 0  4  7
   1  5  8
   2  6  9
This can be useful if you want to refer back to parts of the data frame later. We use 
the ix indexer:
>>> df3 = pd.concat([df1, df2], keys=['UK', 'DE'])
>>> df3.ix["UK"]
   A  B
0  1  4
1  2  5
2  3  6

Data Analysis Application Examples
[ 136 ]
Data frames resemble database tables. It is therefore not surprising that Pandas 
implements SQL-like join operations on them. What is positively surprising is that 
these operations are highly optimized and extremely fast:
>>> import numpy as np
>>> df1 = pd.DataFrame({'key': ['A', 'B', 'C', 'D'],
                        'value': range(4)})
>>> df1
  key  value
0   A      0
1   B      1
2   C      2
3   D      3
>>> df2 = pd.DataFrame({'key': ['B', 'D', 'D', 'E'], 
                        'value': range(10, 14)})
>>> df2
  key  value
0   B     10
1   D     11
2   D     12
3   E     13
If we merge on key, we get an inner join. This creates a new data frame by combining 
the column values of the original data frames based upon the join predicate, here the 
key attribute is used:
>>> df1.merge(df2, on='key')
  key  value_x  value_y
0   B        1       10
1   D        3       11
2   D        3       12
A left, right and full join can be specified by the how parameter:
>>> df1.merge(df2, on='key', how='left')
  key  value_x  value_y
0   A        0      NaN
1   B        1       10
2   C        2      NaN
3   D        3       11
4   D        3       12

Chapter 7
[ 137 ]
>>> df1.merge(df2, on='key', how='right')
  key  value_x  value_y
0   B        1       10
1   D        3       11
2   D        3       12
3   E      NaN       13
>>> df1.merge(df2, on='key', how='outer')
  key  value_x  value_y
0   A        0      NaN
1   B        1       10
2   C        2      NaN
3   D        3       11
4   D        3       12
5   E      NaN       13
The merge methods can be specified with the how parameter. The following table 
shows the methods in comparison with SQL:
Merge Method
SQL Join Name
Description
left
LEFT OUTER JOIN
Use keys from the left frame only.
right
RIGHT OUTER JOIN
Use keys from the right frame only.
outer
FULL OUTER JOIN
Use a union of keys from both frames.
inner
INNER JOIN
Use an intersection of keys from both frames.
Reshaping data
We saw how to combine data frames but sometimes we have all the right data in a 
single data structure, but the format is impractical for certain tasks. We start again 
with some artificial weather data:
>>> df
          date    city  value
0   2000-01-03  London      6
1   2000-01-04  London      3
2   2000-01-05  London      4
3   2000-01-03  Mexico      3
4   2000-01-04  Mexico      9
5   2000-01-05  Mexico      8
6   2000-01-03  Mumbai     12
7   2000-01-04  Mumbai      9

Data Analysis Application Examples
[ 138 ]
8   2000-01-05  Mumbai      8
9   2000-01-03   Tokyo      5
10  2000-01-04   Tokyo      5
11  2000-01-05   Tokyo      6
f we want to calculate the maximum temperature per city, we could just group the 
data by city and then take the max function:
>>> df.groupby('city').max()
              date  value
city
London  2000-01-05      6
Mexico  2000-01-05      9
Mumbai  2000-01-05     12
Tokyo   2000-01-05      6
However, if we have to bring our data into form every time, we could be a little more 
effective, by creating a reshaped data frame first, having the dates as an index and 
the cities as columns.
We can create such a data frame with the pivot function. The arguments are the 
index (we use date), the columns (we use the cities), and the values (which are stored 
in the value column of the original data frame):
>>> pv = df.pivot("date", "city", "value")
>>> pv
city          London  Mexico  Mumbai  Tokyo 
date
2000-01-03       6       3      12      5
2000-01-04       3       9       9      5
2000-01-05       4       8       8      6
We can use max function on this new data frame directly:
>>> pv.max()
city
London     6
Mexico     9
Mumbai    12
Tokyo      6
dtype: int64

Chapter 7
[ 139 ]
With a more suitable shape, other operations become easier as well. For example,  
to find the maximum temperature per day, we can simply provide an additional  
axis argument:
>>> pv.max(axis=1)
date
2000-01-03    12
2000-01-04     9
2000-01-05     8
dtype: int64
Data aggregation
As a final topic, we will look at ways to get a condensed view of data with 
aggregations. Pandas comes with a lot of aggregation functions built-in. We already 
saw the describe function in Chapter 3, Data Analysis with Pandas. This works 
on parts of the data as well. We start with some artificial data again, containing 
measurements about the number of sunshine hours per city and date:
>>> df.head()
   country     city        date  hours
0  Germany  Hamburg  2015-06-01      8
1  Germany  Hamburg  2015-06-02     10
2  Germany  Hamburg  2015-06-03      9
3  Germany  Hamburg  2015-06-04      7
4  Germany  Hamburg  2015-06-05      3
To view a summary per city, we use the describe function on the grouped data set:
>>> df.groupby("city").describe()
                      hours
city
Berlin     count  10.000000
           mean    6.000000
           std     3.741657
           min     0.000000
           25%     4.000000
           50%     6.000000
           75%     9.750000
           max    10.000000

Data Analysis Application Examples
[ 140 ]
Birmingham count  10.000000
           mean    5.100000
           std     2.078995
           min     2.000000
           25%     4.000000
           50%     5.500000
           75%     6.750000
           max     8.000000
On certain data sets, it can be useful to group by more than one attribute.  
We can get an overview about the sunny hours per country and date by passing  
in two column names:
>>> df.groupby(["country", "date"]).describe()
                         hours 
country date
France  2015-06-01 count  5.000000
                   mean   6.200000
                   std    1.095445
                   min    5.000000
                   25%    5.000000
                   50%    7.000000
                   75%    7.000000
                   max    7.000000
        2015-06-02 count  5.000000
                   mean   3.600000
                   std    3.577709
                   min    0.000000
                   25%    0.000000
                   50%    4.000000
                   75%    6.000000
                   max    8.000000
UK      2015-06-07 std    3.872983
                   min    0.000000
                   25%    2.000000
                   50%    6.000000
                   75%    8.000000
                   max    9.000000

Chapter 7
[ 141 ]
We can compute single statistics as well:
>>> df.groupby("city").mean()
            hours
city 
Berlin        6.0
Birmingham    5.1
Bordeax       4.7
Edinburgh     7.5
Frankfurt     5.8
Glasgow       4.8
Hamburg       5.5
Leipzig       5.0
London        4.8
Lyon          5.0
Manchester    5.2
Marseille     6.2
Munich        6.6
Nice          3.9
Paris         6.3
Finally, we can define any function to be applied on the groups with the agg method. 
The above could have been written in terms of agg like this:
>>> df.groupby("city").agg(np.mean)
hours
city
Berlin        6.0
Birmingham    5.1
Bordeax       4.7
Edinburgh     7.5
Frankfurt     5.8
Glasgow       4.8
...
But arbitrary functions are possible. As a last example, we define a custom function, 
which takes an input of a series object and computes the difference between the 
smallest and the largest element:
>>> df.groupby("city").agg(lambda s: abs(min(s) - max(s)))
        hours

Data Analysis Application Examples
[ 142 ]
city
Berlin         10
Birmingham      6
Bordeax        10
Edinburgh       8
Frankfurt       9
Glasgow        10
Hamburg        10
Leipzig         9
London         10
Lyon            8
Manchester     10
Marseille      10
Munich          9
Nice           10
Paris           9
Grouping data
One typical workflow during data exploration looks as follows: 
•	
You find a criterion that you want to use to group your data. Maybe you 
have GDP data for every country along with the continent and you would 
like to ask questions about the continents. These questions usually lead to 
some function applications- you might want to compute the mean GDP per 
continent. Finally, you want to store this data for further processing in a new 
data structure.
•	
We use a simpler example here. Imagine some fictional weather data about 
the number of sunny hours per day and city:
>>> df
          date    city  value
0   2000-01-03  London      6
1   2000-01-04  London      3
2   2000-01-05  London      4
3   2000-01-03  Mexico      3
4   2000-01-04  Mexico      9
5   2000-01-05  Mexico      8
6   2000-01-03  Mumbai     12
7   2000-01-04  Mumbai      9
8   2000-01-05  Mumbai      8

Chapter 7
[ 143 ]
9   2000-01-03   Tokyo      5
10  2000-01-04   Tokyo      5
11  2000-01-05   Tokyo      6
The groups attributes return a dictionary containing the unique groups and 
the corresponding values as axis labels:
>>> df.groupby("city").groups
{'London': [0, 1, 2],
'Mexico': [3, 4, 5],
'Mumbai': [6, 7, 8],
'Tokyo': [9, 10, 11]}
•	
Although the result of a groupby is a GroupBy object, not a DataFrame, we 
can use the usual indexing notation to refer to columns:
>>> grouped = df.groupby(["city", "value"])
>>> grouped["value"].max()
city
London     6
Mexico     9
Mumbai    12
Tokyo      6
Name: value, dtype: int64
>>> grouped["value"].sum()
city
London    13
Mexico    20
Mumbai    29
Tokyo     16
Name: value, dtype: int64
•	
We see that, according to our data set, Mumbai seems to be a sunny city. An 
alternative – and more verbose – way to achieve the above would be:
>>> df['value'].groupby(df['city']).sum()
city
London    13
Mexico    20
Mumbai    29
Tokyo     16
Name: value, dtype: int64

Data Analysis Application Examples
[ 144 ]
Summary
In this chapter we have looked at ways to manipulate data frames, from cleaning 
and filtering, to grouping, aggregation, and reshaping. Pandas makes a lot of the 
common operations very easy and more complex operations, such as pivoting 
or grouping by multiple attributes, can often be expressed as one-liners as well. 
Cleaning and preparing data is an essential part of data exploration and analysis.
The next chapter explains a brief of machine learning algorithms that is applying 
data analysis result to make decisions or build helpful products.
Practice exercises
Exercise 1: Cleaning: In the section about filtering, we used the Europe Brent Crude 
Oil Spot Price, which can be found as an Excel document on the internet. Take this 
Excel spreadsheet and try to convert it into a CSV document that is ready to be 
imported with Pandas.
Hint: There are many ways to do this. We used a small tool called xls2csv.py and 
we were able to load the resulting CSV file with a helper method:
import datetime
import pandas as pd
def convert_date(s):
    parts = s.replace("(", "").replace(")", "").split(",") 
    if len(parts) < 6: 
        return datetime.date(1970, 1, 1) 
    return datetime.datetime(*[int(p) for p in parts]) 
df = pd.read_csv("RBRTEd.csv", sep=',', names=["date", "price"], 
converters={"date": convert_date}).dropna()
Take a data set that is important for your work – or if you do not have any at hand, 
a data set that interests you and that is available online. Ask one or two questions 
about the data in advance. Then use cleaning, filtering, grouping, and plotting 
techniques to answer your question.

[ 145 ]
Machine Learning Models 
with scikit-learn
In the previous chapter, we saw how to perform data munging, data aggregation, and 
grouping. In this chapter, we will see the working of different scikit-learn modules for 
different models in brief, data representation in scikit-learn, understand supervised 
and unsupervised learning using an example, and measure prediction performance.
An overview of machine learning models
Machine learning is a subfield of artificial intelligence that explores how machines 
can learn from data to analyze structures, help with decisions, and make predictions. 
In 1959, Arthur Samuel defined machine learning as the, "Field of study that gives 
computers the ability to learn without being explicitly programmed."
A wide range of applications employ machine learning methods, such as spam 
filtering, optical character recognition, computer vision, speech recognition, credit 
approval, search engines, and recommendation systems.
One important driver for machine learning is the fact that data is generated at an 
increasing pace across all sectors; be it web traffic, texts or images, and sensor data 
or scientific datasets. The larger amounts of data give rise to many new challenges 
in storage and processing systems. On the other hand, many learning algorithms 
will yield better results with more data to learn from. The field has received a lot of 
attention in recent years due to significant performance increases in various hard 
tasks, such as speech recognition or object detection in images. Understanding large 
amounts of data without the help of intelligent algorithms seems unpromising.

Machine Learning Models with scikit-learn
[ 146 ]
A learning problem typically uses a set of samples (usually denoted with an  
N or n) to build a model, which is then validated and used to predict the  
properties of unseen data.
Each sample might consist of single or multiple values. In the context of machine 
learning, the properties of data are called features.
Machine learning can be arranged by the nature of the input data:
•	
Supervised learning 
•	
Unsupervised learning
In supervised learning, the input data (typically denoted with x) is associated  
with a target label (y), whereas in unsupervised learning, we only have unlabeled 
input data.
Supervised learning can be further broken down into the following problems:
•	
Classification problems
•	
Regression problems
Classification problems have a fixed set of target labels, classes, or categories, while 
regression problems have one or more continuous output variables. Classifying 
e-mail messages as spam or not spam is a classification task with two target labels. 
Predicting house prices—given the data about houses, such as size, age, and nitric 
oxides concentration—is a regression task, since the price is continuous.
Unsupervised learning deals with datasets that do not carry labels. A typical case 
is clustering or automatic classification. The goal is to group similar items together. 
What similarity means will depend on the context, and there are many similarity 
metrics that can be employed in such a task.
The scikit-learn modules for different 
models
The scikit-learn library is organized into submodules. Each submodule  
contains algorithms and helper methods for a certain class of machine  
learning models and approaches. 

Chapter 8
[ 147 ]
Here is a sample of those submodules, including some example models:
Submodule
Description
Example models
cluster
This is the unsupervised clustering
KMeans and Ward
decomposition
This is the dimensionality reduction
PCA and NMF
ensemble
This involves ensemble-based 
methods
AdaBoostClassifier,
AdaBoostRegressor,
RandomForestClassifier,
RandomForestRegressor
lda
This stands for latent discriminant 
analysis
LDA
linear_model
This is the generalized linear model
LinearRegression, 
LogisticRegression, 
Lasso and Perceptron
mixture
This is the mixture model
GMM and VBGMM
naive_bayes
This involves supervised learning 
based on Bayes' theorem
BaseNB and BernoulliNB, 
GaussianNB
neighbors
These are k-nearest neighbors
KNeighborsClassifier, 
KNeighborsRegressor,
LSHForest
neural_
network
This involves models based on neural 
networks
BernoulliRBM
tree
decision trees
DecisionTreeClassifier, 
DecisionTreeRegressor
While these approaches are diverse, a scikit-learn library abstracts away a lot of 
differences by exposing a regular interface to most of these algorithms. All of the 
example algorithms listed in the table implement a fit method, and most of them 
implement predict as well. These methods represent two phases in machine learning. 
First, the model is trained on the existing data with the fit method. Once trained, it 
is possible to use the model to predict the class or value of unseen data with predict. 
We will see both the methods at work in the next sections.
The scikit-learn library is part of the PyData ecosystem. Its codebase has seen steady 
growth over the past six years, and with over hundred contributors, it is one of the 
most active and popular among the scikit toolkits.

Machine Learning Models with scikit-learn
[ 148 ]
Data representation in scikit-learn
In contrast to the heterogeneous domains and applications of machine learning, the 
data representation in scikit-learn is less diverse, and the basic format that many 
algorithms expect is straightforward—a matrix of samples and features.
The underlying data structure is a numpy and the ndarray. Each row in the matrix 
corresponds to one sample and each column to the value of one feature.
There is something like Hello World in the world of machine learning datasets as 
well; for example, the Iris dataset whose origins date back to 1936. With the standard 
installation of scikit-learn, you already have access to a couple of datasets, including 
Iris that consists of 150 samples, each consisting of four measurements taken from 
three different Iris flower species:
>>> import numpy as np
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
The dataset is packaged as a bunch, which is only a thin wrapper around a 
dictionary:
>>> type(iris)
sklearn.datasets.base.Bunch
>>> iris.keys()
['target_names', 'data', 'target', 'DESCR', 'feature_names']
Under the data key, we can find the matrix of samples and features, and can confirm 
its shape:
>>> type(iris.data)
numpy.ndarray
>>> iris.data.shape
(150, 4)
Each entry in the data matrix has been labeled, and these labels can be looked up in 
the target attribute:
>>> type(iris.target)
numpy.ndarray
>>> iris.target.shape
(150,)

Chapter 8
[ 149 ]
>>> iris.target[:10]
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
>>> np.unique(iris.target)
array([0, 1, 2])
The target names are encoded. We can look up the corresponding names in the 
target_names attribute:
>>> iris.target_names
>>> array(['setosa', 'versicolor', 'virginica'], dtype='|S10')
This is the basic anatomy of many datasets, such as example data, target values,  
and target names.
What are the features of a single entry in this dataset?:
>>> iris.data[0]
array([ 5.1,  3.5,  1.4,  0.2])
The four features are the measurements taken of real flowers: their sepal length and 
width, and petal length and width. Three different species have been examined: the 
Iris-Setosa, Iris-Versicolour, and Iris-Virginica.
Machine learning tries to answer the following question: can we predict the species 
of the flower, given only the measurements of its sepal and petal length?
In the next section, we will see how to answer this question with scikit-learn.
Besides the data about flowers, there are a few other datasets included in the scikit-
learn distribution, as follows:
•	
The Boston House Prices dataset (506 samples and 13 attributes)
•	
The Optical Recognition of Handwritten Digits dataset (5620 samples and 64 
attributes)
•	
The Iris Plants Database (150 samples and 4 attributes)
•	
The Linnerud dataset (30 samples and 3 attributes)
A few datasets are not included, but they can easily be fetched on demand (as these 
are usually a bit bigger). Among these datasets, you can find a real estate dataset and 
a news corpus:
>>> ds = datasets.fetch_california_housing()
downloading Cal. housing from http://lib.stat.cmu.edu/modules.php?op=...
>>> ds.data.shape

Machine Learning Models with scikit-learn
[ 150 ]
(20640, 8)
>>> ds = datasets.fetch_20newsgroups()
>>> len(ds.data)
11314
>>> ds.data[0][:50]
u"From: lerxst@wam.umd.edu (where's my thing)\nSubjec"
>>> sum([len([w for w in sample.split()]) for sample in ds.data])
3252437
These datasets are a great way to get started with the scikit-learn library, and they 
will also help you to test your own algorithms. Finally, scikit-learn includes functions 
(prefixed with datasets.make_) to create artificial datasets as well.
If you work with your own datasets, you will have to bring them in a shape that 
scikit-learn expects, which can be a task of its own. Tools such as Pandas make this 
task much easier, and Pandas DataFrames can be exported to numpy.ndarray easily 
with the as_matrix() method on DataFrame.
Supervised learning – classification and 
regression
In this section, we will show short examples for both classification and regression.
Classification problems are pervasive: document categorization, fraud detection, 
market segmentation in business intelligence, and protein function prediction in 
bioinformatics.
While it might be possible for hand-craft rules to assign a category or label to new 
data, it is faster to use algorithms to learn and generalize from the existing data.
We will continue with the Iris dataset. Before we apply a learning algorithm, we 
want to get an intuition of the data by looking at some values and plots.

Chapter 8
[ 151 ]
All measurements share the same dimension, which helps to visualize the variance 
in various boxplots:

Machine Learning Models with scikit-learn
[ 152 ]
We see that the petal length (the third feature) exhibits the biggest variance, which 
could indicate the importance of this feature during classification. It is also insightful 
to plot the data points in two dimensions, using one feature for each axis. Also, 
indeed, our previous observation reinforced that the petal length might be a good 
indicator to tell apart the various species. The Iris setosa also seems to be more easily 
separable than the other two species:
From the visualizations, we get an intuition of the solution to our problem. We will 
use a supervised method called a Support Vector Machine (SVM) to learn about a 
classifier for the Iris data. The API separates models and data, therefore, the first step 
is to instantiate the model. In this case, we pass an optional keyword parameter to be 
able to query the model for probabilities later:
>>> from sklearn.svm import SVC
>>> clf = SVC(probability=True)
The next step is to fit the model according to our training data:
>>> clf.fit(iris.data, iris.target)
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,

Chapter 8
[ 153 ]
    degree=3, gamma=0.0, kernel='rbf', max_iter=-1,
    probability=True, random_state=None, shrinking=True,
    tol=0.001, verbose=False)
With this one line, we have trained our first machine learning model on a dataset. 
This model can now be used to predict the species of unknown data. If given some 
measurement that we have never seen before, we can use the predict method on  
the model:
>>> unseen = [6.0, 2.0, 3.0, 2.0]
>>> clf.predict(unseen)
array([1])
>>> iris.target_names[clf.predict(unseen)]
array(['versicolor'],
      dtype='|S10')
We see that the classifier has given the versicolor label to the measurement.  
If we visualize the unknown point in our plots, we see that this seems like a  
sensible prediction:

Machine Learning Models with scikit-learn
[ 154 ]
In fact, the classifier is relatively sure about this label, which we can inquire into by 
using the predict_proba method on the classifier:
>>> clf.predict_proba(unseen)
array([[ 0.03314121,  0.90920125,  0.05765754]])
Our example consisted of four features, but many problems deal with higher-
dimensional datasets and many algorithms work fine on these datasets as well.
We want to show another algorithm for supervised learning problems: linear 
regression. In linear regression, we try to predict one or more continuous output 
variables, called regress ands, given a D-dimensional input vector. Regression means 
that the output is continuous. It is called linear since the output will be modeled with 
a linear function of the parameters.
We first create a sample dataset as follows:
>>> import matplotlib.pyplot as plt
>>> X = [[1], [2], [3], [4], [5], [6], [7], [8]]
>>> y = [1, 2.5, 3.5, 4.8, 3.9, 5.5, 7, 8]
>>> plt.scatter(X, y, c='0.25')
>>> plt.show()
Given this data, we want to learn a linear function that approximates the data and 
minimizes the prediction error, which is defined as the sum of squares between the 
observed and predicted responses:
>>> from sklearn.linear_model import LinearRegression
>>> clf = LinearRegression()
>>> clf.fit(X, y)
Many models will learn parameters during training. These parameters are marked 
with a single underscore at the end of the attribute name. In this model, the coef_ 
attribute will hold the estimated coefficients for the linear regression problem:
>>> clf.coef_
array([ 0.91190476])
We can plot the prediction over our data as well:
>>> plt.plot(X, clf.predict(X), '--', color='0.10', linewidth=1)

Chapter 8
[ 155 ]
The output of the plot is as follows:
The above graph is a simple example with artificial data, but linear regression has a 
wide range of applications. If given the characteristics of real estate objects, we can 
learn to predict prices. If given the features of the galaxies, such as size, color, or 
brightness, it is possible to predict their distance. If given the data about household 
income and education level of parents, we can say something about the grades of 
their children.
There are numerous applications of linear regression everywhere, where one or more 
independent variables might be connected to one or more dependent variables.

Machine Learning Models with scikit-learn
[ 156 ]
Unsupervised learning – clustering and 
dimensionality reduction
A lot of existing data is not labeled. It is still possible to learn from data without labels 
with unsupervised models. A typical task during exploratory data analysis is to find 
related items or clusters. We can imagine the Iris dataset, but without the labels:
While the task seems much harder without labels, one group of measurements  
(in the lower-left) seems to stand apart. The goal of clustering algorithms is to 
identify these groups.

Chapter 8
[ 157 ]
We will use K-Means clustering on the Iris dataset (without the labels). This 
algorithm expects the number of clusters to be specified in advance, which can be a 
disadvantage. K-Means will try to partition the dataset into groups, by minimizing 
the within-cluster sum of squares.
For example, we instantiate the KMeans model with n_clusters equal to 3:
>>> from sklearn.cluster import KMeans
>>> km = KMeans(n_clusters=3)
Similar to supervised algorithms, we can use the fit methods to train the model, but 
we only pass the data and not target labels:
>>> km.fit(iris.data)
KMeans(copy_x=True, init='k-means++', max_iter=300, n_clusters=3, 
n_init=10, n_jobs=1, precompute_distances='auto', random_state=None, 
tol=0.0001, verbose=0)
We already saw attributes ending with an underscore. In this case, the algorithm 
assigned a label to the training data, which can be inspected with the labels_ 
attribute:
>>> km.labels_
array([1, 1, 1, 1, 1, 1, ..., 0, 2, 0, 0, 2], dtype=int32)
We can already compare the result of these algorithms with our known target labels:
>>> iris.target 
array([0, 0, 0, 0, 0, 0, ..., 2, 2, 2, 2, 2])
We quickly relabel the result to simplify the prediction error calculation:
>>> tr = {1: 0, 2: 1, 0: 2}
>>> predicted_labels = np.array([tr[i] for i in km.labels_])
>>> sum([p == t for (p, t) in zip(predicted_labels, iris.target)])
134

Machine Learning Models with scikit-learn
[ 158 ]
From 150 samples, K-Mean assigned the correct label to 134 samples, which is an 
accuracy of about 90 percent. The following plot shows the points of the algorithm 
predicted correctly in grey and the mislabeled points in red:
As another example for an unsupervised algorithm, we will take a look at Principal 
Component Analysis (PCA). The PCA aims to find the directions of the maximum 
variance in high-dimensional data. One goal is to reduce the number of dimensions 
by projecting a higher-dimensional space onto a lower-dimensional subspace while 
keeping most of the information.

Chapter 8
[ 159 ]
The problem appears in various fields. You have collected many samples and each 
sample consists of hundreds or thousands of features. Not all the properties of the 
phenomenon at hand will be equally important. In our Iris dataset, we saw that the 
petal length alone seemed to be a good discriminator of the various species. PCA 
aims to find principal components that explain most of the variation in the data. If 
we sort our components accordingly (technically, we sort the eigenvectors of the 
covariance matrix by eigenvalue), we can keep the ones that explain most of the data 
and ignore the remaining ones, thereby reducing the dimensionality of the data.
It is simple to run PCA with scikit-learn. We will not go into the implementation 
details, but instead try to give you an intuition of PCA by running it on the Iris 
dataset, in order to give you yet another angle.
The process is similar to the ones we implemented so far. First, we instantiate our 
model; this time, the PCA from the decomposition submodule. We also import a 
standardization method, called StandardScaler, that will remove the mean from 
our data and scale to the unit variance. This step is a common requirement for many 
machine learning algorithms:
>>> from sklearn.decomposition import PCA
>>> from sklearn.preprocessing import StandardScaler
First, we instantiate our model with a parameter (which specifies the number of 
dimensions to reduce to), standardize our input, and run the fit_transform 
function that will take care of the mechanics of PCA:
>>> pca = PCA(n_components=2)
>>> X = StandardScaler().fit_transform(iris.data)
>>> Y = pca.fit_transform(X)
The result is a dimensionality reduction in the Iris dataset from four (sepal and petal 
width and length) to two dimensions. It is important to note that this projection 
is not onto the two existing dimensions, so our new dataset does not consist of, 
for example, only petal length and width. Instead, the two new dimensions will 
represent a mixture of the existing features.

Machine Learning Models with scikit-learn
[ 160 ]
The following scatter plot shows the transformed dataset; from a glance at the plot,  
it looks like we still kept the essence of our dataset, even though we halved the 
number of dimensions:
Dimensionality reduction is just one way to deal with high-dimensional datasets, 
which are sometimes effected by the so called curse of dimensionality.
Measuring prediction performance
We have already seen that the machine learning process consists of the  
following steps:
•	
Model selection: We first select a suitable model for our data. Do we have 
labels? How many samples are available? Is the data separable? How many 
dimensions do we have? As this step is nontrivial, the choice will depend on 
the actual problem. As of Fall 2015, the scikit-learn documentation contains a 
much appreciated flowchart called choosing the right estimator. It is short, but 
very informative and worth taking a closer look at.

Chapter 8
[ 161 ]
•	
Training: We have to bring the model and data together, and this usually 
happens in the fit methods of the models in scikit-learn.
•	
Application: Once we have trained our model, we are able to make 
predictions about the unseen data.
So far, we omitted an important step that takes place between the training and 
application: the model testing and validation. In this step, we want to evaluate how 
well our model has learned.
One goal of learning, and machine learning in particular, is generalization. The 
question of whether a limited set of observations is enough to make statements 
about any possible observation is a deeper theoretical question, which is answered in 
dedicated resources on machine learning.
Whether or not a model generalizes well can also be tested. However, it is 
important that the training and the test input are separate. The situation where a 
model performs well on a training input but fails on an unseen test input is called 
overfitting, and this is not uncommon.
The basic approach is to split the available data into a training and test set, and  
scikit-learn helps to create this split with the train_test_split function.
We go back to the Iris dataset and perform SVC again. This time we will evaluate  
the performance of the algorithm on a training set. We set aside 40 percent of the 
data for testing:
>>> from sklearn.cross_validation import train_test_split
>>> X_train, X_test, y_train, y_test = train_test_split( 
  iris.data, iris.target, test_size=0.4, random_state=0)
>>> clf = SVC()
>>> clf.fit(X_train, y_train)
The score function returns the mean accuracy of the given data and labels.  
We pass the test set for evaluation:
>>> clf.score(X_test, y_test)
 0.94999999999999996

Machine Learning Models with scikit-learn
[ 162 ]
The model seems to perform well, with about 94 percent accuracy on unseen 
data. We can now start to tweak model parameters (also called hyper parameters) 
to increase prediction performance. This cycle would bring back the problem of 
overfitting. One solution is to split the input data into three sets: one for training, 
validation, and testing. The iterative model of hyper-parameters tuning would take 
place between the training and the validation set, while the final evaluation would 
be done on the test set. Splitting the dataset into three reduces the number of samples 
we can learn from as well.
Cross-validation (CV) is a technique that does not need a validation set, but still 
counteracts overfitting. The dataset is split into k parts (called folds). For each fold, 
the model is trained on k-1 folds and tested on the remaining folds. The accuracy is 
taken as the average over the folds.
We will show a five-fold cross-validation on the Iris dataset, using SVC again:
>>> from sklearn.cross_validation import cross_val_score
>>> clf = SVC()
>>> scores = cross_val_score(clf, iris.data, iris.target, cv=5)
>>> scores
array([ 0.96666667,  1.    ,  0.96666667,  0.96666667,  1.    ])
>>> scores.mean()
0.98000000000000009
There are various strategies implemented by different classes to split the dataset 
for cross-validation: KFold, StratifiedKFold, LeaveOneOut, LeavePOut, 
LeaveOneLabelOut, LeavePLableOut, ShuffleSplit, StratifiedShuffleSplit, 
and PredefinedSplit.
Model verification is an important step and it is necessary for the development of 
robust machine learning solutions.
Summary
In this chapter, we took a whirlwind tour through one of the most popular  
Python machine learning libraries: scikit-learn. We saw what kind of data this  
library expects. Real-world data will seldom be ready to be fed into an estimator 
right away. With powerful libraries, such as Numpy and, especially, Pandas, 
you already saw how data can be retrieved, combined, and brought into shape. 
Visualization libraries, such as matplotlib, help along the way to get an intuition of 
the datasets, problems, and solutions.

Chapter 8
[ 163 ]
During this chapter, we looked at a canonical dataset, the Iris dataset. We also looked 
at it from various angles: as a problem in supervised and unsupervised learning and 
as an example for model verification.
In total, we have looked at four different algorithms: the Support Vector Machine, 
Linear Regression, K-Means clustering, and Principal Component Analysis. Each 
of these alone is worth exploring, and we barely scratched the surface, although we 
were able to implement all the algorithms with only a few lines of Python.
There are numerous ways in which you can take your knowledge of the data analysis 
process further. Hundreds of books have been published on machine learning, 
so we only want to highlight a few here: Building Machine Learning Systems with 
Python by Richert and Coelho, will go much deeper into scikit-learn as we couldn't 
in this chapter. Learning from Data by Abu-Mostafa, Magdon-Ismail, and Lin, is a great 
resource for a solid theoretical foundation of learning problems in general.
The most interesting applications will be found in your own field. However, if you 
would like to get some inspiration, we recommend that you look at the www.kaggle.
com website that runs predictive modeling and analytics competitions, which are 
both fun and insightful.
Practice exercises
Are the following problems supervised or unsupervised? Regression or  
classification problems?:
•	
Recognizing coins inside a vending machine
•	
Recognizing handwritten digits
•	
If given a number of facts about people and economy, we want to estimate 
consumer spending
•	
If given the data about geography, politics, and historical events, we want to 
predict when and where a human right violation will eventually take place
•	
If given the sounds of whales and their species, we want to label yet 
unlabeled whale sound recordings
Look up one of the first machine learning models and algorithms: the perceptron. 
Try the perceptron on the Iris dataset and estimate the accuracy of the model. How 
does the perceptron compare to the SVC from this chapter?


Module 2
Learning Predictive Analytics with Python
Gain practical insights into predictive modelling by implementing  
Predictive Analytics algorithms on public datasets with Python


[ 167 ]
Getting Started with 
Predictive Modelling
Predictive modelling is an art; its a science of unearthing the story impregnated 
into silos of data. This chapter introduces the scope and application of predictive 
modelling and shows a glimpse of what could be achieved with it, by giving some 
real-life examples.
In this chapter, we will cover the following topics in detail:
•	
Introducing predictive modelling
•	
Applications and examples of predictive modelling
•	
Installing and downloading Python and its packages
•	
Working with different IDEs for Python
Introducing predictive modelling
Did you know that Facebook users around the world share 2,460,000 pieces of 
content every minute of the day? Did you know that 72-hours worth of new video 
content is uploaded on YouTube in the same time and, brace yourself, did you know 
that everyday around 2.5 exabytes (10^18) of data is created by us humans? To give 
you a perspective on how much data that is, you will need a million 1 TB (1000 GB) 
hard disk drives every day to store that much data. In a year, we will outgrow the US 
population and will be north of five times the UK population and this estimation is 
by assuming the fact that the rate of the data generation will remain the same, which 
in all likelihoods will not be the case.

Getting Started with Predictive Modelling
[ 168 ]
The breakneck speed at which the social media and Internet of Things have grown 
is reflected in the huge silos of data humans generate. The data about where we 
live, where we come from, what we like, what we buy, how much money we spend, 
where we travel, and so on. Whenever we interact with a social media or Internet 
of Things website, we leave a trail, which these websites gleefully log as their data. 
Every time you buy a book at Amazon, receive a payment through PayPal, write a 
review on Yelp, post a photo on Instagram, do a check-in on Facebook, apart from 
making business for these websites, you are creating data for them.
Harvard Business Review (HBR) says "Data is the new oil" and that "Data Scientist 
is the sexiest job of the 21st century". So, why is the data so important and how  
can we realize the full potential of it? There are broadly two ways in which the  
data is used:
•	
Retrospective analytics: This approach helps us analyze history and glean 
out insights from the data. It allows us to learn from mistakes and adopt 
best practices. These insights and learnings become the torchbearer for the 
purpose of devising better strategy. Not surprisingly, many experts have 
been claiming that data is the new middle manager.
•	
Predictive analytics: This approach unleashes the might of data. In short, 
this approach allows us to predict the future. Data science algorithms take 
historical data and spit out a statistical model, which can predict who will 
buy, cheat, lie, or die in the future.
Let us evaluate the comparisons made with oil in detail:
•	
Data is as abundant as oil used to be, once upon a time, but in contrast to 
oil, data is a non-depleting resource. In fact, one can argue that it is reusable, 
in the sense that, each dataset can be used in more than one way and also 
multiple number of times.
•	
It doesn't take years to create data, as it takes for oil.
•	
Oil in its crude form is worth nothing. It needs to be refined through a 
comprehensive process to make it usable. There are various grades of this 
process to suit various needs; it's the same with data. The data sitting in silos 
is worthless; it needs to be cleaned, manipulated, and modelled to make use 
of it. Just as we need refineries and people who can operate those refineries, 
we need tools that can handle data and people who can operate those tools. 
Some of the tools for the preceding tasks are Python, R, SAS, and so on, and 
the people who operate these tools are called data scientists.

Chapter 1
[ 169 ]
A more detailed comparison of oil and data is provided in the following table:
Data
Oil
It's a non-depleting resource and also 
reusable.
It's a depleting resource and non-reusable.
Data collection requires some 
infrastructure or system in place. Once 
the system is in place, the data generation 
happens seamlessly.
Drilling oil requires a lot of infrastructure. 
Once the infrastructure is in place, one can 
keep drawing the oil until the stock dries up.
It needs to be cleaned and modelled.
It needs to be cleaned and processed.
The time taken to generate data varies 
from fractions of second to months and 
years.
It takes decades to generate.
The worth and marketability of different 
kinds of data is different.
The worth of crude oil is same everywhere. 
However, the price and marketability of 
different end products of refinement is 
different.
The time horizon for monetization of data 
is smaller after getting the data.
The time horizon for monetizing oil is longer 
than that for data.
Scope of predictive modelling
Predictive modelling is an ensemble of statistical algorithms coded in a statistical tool, 
which when applied on historical data, outputs a mathematical function (or equation). It 
can in-turn be used to predict outcomes based on some inputs (on which the model 
operates) from the future to drive a goal in business context or enable better decision 
making in general.
To understand what predictive modelling entails, let us focus on the phrases 
highlighted previously.
Ensemble of statistical algorithms
Statistics are important to understand data. It tells volumes about the data. How is 
the data distributed? Is it centered with little variance or does it varies widely? Are 
two of the variables dependent on or independent of each other? Statistics helps 
us answer these questions. This book will expect a basic understanding of basic 
statistical terms, such as mean, variance, co-variance, and correlation. Advanced 
terms, such as hypothesis testing, Chi-Square tests, p-values, and so on will be 
explained as and when required. Statistics are the cog in the wheel called model.

Getting Started with Predictive Modelling
[ 170 ]
Algorithms, on the other hand, are the blueprints of a model. They are responsible 
for creating mathematical equations from the historical data. They analyze the data, 
quantify the relationship between the variables, and convert it into a mathematical 
equation. There is a variety of them: Linear Regression, Logistic Regression, 
Clustering, Decision Trees, Time-Series Modelling, Naïve Bayes Classifiers, Natural 
Language Processing, and so on. These models can be classified under two classes:
•	
Supervised algorithms: These are the algorithms wherein the historical 
data has an output variable in addition to the input variables. The model 
makes use of the output variables from historical data, apart from the input 
variables. The examples of such algorithms include Linear Regression, 
Logistic Regression, Decision Trees, and so on.
•	
Un-supervised algorithms: These algorithms work without an  
output variable in the historical data. The example of such  
algorithms includes clustering.
The selection of a particular algorithm for a model depends majorly on the kind 
of data available. The focus of this book would be to explain methods of handling 
various kinds of data and illustrating the implementation of some of these models.
Statistical tools
There are a many statistical tools available today, which are laced with inbuilt 
methods to run basic statistical chores. The arrival of open-source robust tools like R 
and Python has made them extremely popular, both in industry and academia alike. 
Apart from that, Python's packages are well documented; hence, debugging is easier.
Python has a number of libraries, especially for running the statistical, cleaning, 
and modelling chores. It has emerged as the first among equals when it comes to 
choosing the tool for the purpose of implementing preventive modelling. As the  
title suggests, Python will be the choice for this book, as well.
Historical data
Our machinery (model) is built and operated on this oil called data. In general,  
a model is built on the historical data and works on future data. Additionally,  
a predictive model can be used to fill missing values in historical data by 
interpolating the model over sparse historical data. In many cases, during modelling 
stages, future data is not available. Hence, it is a common practice to divide the 
historical data into training (to act as historical data) and testing (to act as future 
data) through sampling.

Chapter 1
[ 171 ]
As discussed earlier, the data might or might not have an output variable. However, 
one thing that it promises to be is messy. It needs to undergo a lot of cleaning and 
manipulation before it can become of any use for a modelling process.
Mathematical function
Most of the data science algorithms have underlying mathematics behind them. In 
many of the algorithms, such as regression, a mathematical equation (of a certain 
type) is assumed and the parameters of the equations are derived by fitting the data 
to the equation.
For example, the goal of linear regression is to fit a linear model to a dataset and find 
the equation parameters of the following equation:
0
1
1
2
2
.
.
.......
.
n
n
Y
X
X
X
α
β
β
β
=
+
+
+
+
The purpose of modelling is to find the best values for the coefficients. Once  
these values are known, the previous equation is good to predict the output.  
The equation above, which can also be thought of as a linear function of Xi's  
(or the input variables), is the linear regression model.
Another example is of logistic regression. There also we have a mathematical 
equation or a function of input variables, with some differences. The defining 
equation for logistic regression is as follows:
(
* )
1
1
1
a b x
a b x
a b x
e
P
e
e
+ ∗
+ ∗
−
+
=
=
+
+
Here, the goal is to estimate the values of a and b by fitting the data to this equation. 
Any supervised algorithm will have an equation or function similar to that of the 
model above. For unsupervised algorithms, an underlying mathematical function  
or criterion (which can be formulated as a function or equation) serves the purpose. 
The mathematical equation or function is the backbone of a model.
Business context
All the effort that goes into predictive analytics and all its worth, which accrues to 
data, is because it solves a business problem. A business problem can be anything 
and it will become more evident in the following examples:
•	
Tricking the users of the product/service to buy more from you by increasing 
the click through rates of the online ads

Getting Started with Predictive Modelling
[ 172 ]
•	
Predicting the probable crime scenes in order to prevent them by aggregating 
an invincible lineup for a sports league
•	
Predicting the failure rates and associated costs of machinery components
•	
Managing the churn rate of the customers
The predictive analytics is being used in an array of industries to solve business 
problems. Some of these industries are, as follows:
•	
Banking
•	
Social media
•	
Retail
•	
Transport
•	
Healthcare
•	
Policing
•	
Education
•	
Travel and logistics
•	
E-commerce
•	
Human resource
By what quantum did the proposed solution make life better for the business, is all 
that matters. That is the reason; predictive analytics is becoming an indispensable 
practice for management consulting.
In short, predictive analytics sits at the sweet spot where statistics, algorithm, 
technology and business sense intersect. Think about it, a mathematician, a 
programmer, and a business person rolled in one.
Knowledge matrix for predictive modelling
As discussed earlier, predictive modelling is an interdisciplinary field sitting at the 
interface and requiring knowledge of four disciplines, such as Statistics, Algorithms, 
Tools, Techniques, and Business Sense. Each of these disciplines is equally 
indispensable to perform a successful task of predictive modelling.
These four disciplines of predictive modelling carry equal weights and can be better 
represented as a knowledge matrix; it is a symmetric 2 x 2 matrix containing four 
equal-sized squares, each representing a discipline.

Chapter 1
[ 173 ]
Fig. 1.1: Knowledge matrix: four disciplines of predictive modelling
Task matrix for predictive modelling
The tasks involved in predictive modelling follows the Pareto principle. Around 80% 
of the effort in the modelling process goes towards data cleaning and wrangling, 
while only 20% of the time and effort goes into implementing the model and getting 
the prediction. However, the meaty part of the modelling that is rich with almost 
80% of results and insights is undoubtedly the implementation of the model. This 
information can be better represented as a matrix, which can be called a task matrix 
that will look something similar to the following figure:
Fig. 1.2: Task matrix: split of time spent on data cleaning and modelling and their final contribution to the 
model

Getting Started with Predictive Modelling
[ 174 ]
Many of the data cleaning and exploration chores can be automated because  
they are alike most of the times, irrespective of the data. The part that needs a  
lot of human thinking is the implementation of a model, which is what makes  
the bulk of this book.
Applications and examples of predictive 
modelling
In the introductory section, data has been compared with oil. While oil has been 
the primary source of energy for the last couple of centuries and the legends of 
OPEC, Petrodollars, and Gulf Wars have set the context for the oil as a begrudged 
resource; the might of data needs to be demonstrated here to set the premise for the 
comparison. Let us glance through some examples of predictive analytics to marvel 
at the might of data.
LinkedIn's "People also viewed" feature
If you are a frequent LinkedIn user, you might be familiar with LinkedIn's  
"People also viewed" feature.
What it does?
Let's say you have searched for some person who works at a particular organization 
and LinkedIn throws up a list of search results. You click on one of them and you 
land up on their profile. In the middle-right section of the screen, you will find a 
panel titled "People Also Viewed"; it is essentially a list of people who either work at 
the same organization as the person whose profile you are currently viewing or the 
people who have the same designation and belong to same industry.
Isn't it cool? You might have searched for these people separately if not for this 
feature. This feature increases the efficacy of your search results and saves your time.
How is it done?
Are you wondering how LinkedIn does it? The rough blueprint is as follows:
•	
LinkedIn leverages the search history data to do this. The model underneath 
this feature plunges into a treasure trove of search history data and looks at 
what people have searched next after finding the correct person they were 
searching for.

Chapter 1
[ 175 ]
•	
This event of searching for a particular second person after searching for a 
particular first person has some probability. This will be calculated using 
all the data for such searches. The profiles with the highest probability of 
being searched (based on the historical data) are shown in the "People Also 
Viewed" section.
•	
This probability comes under the ambit of a broad set of rules called 
Association Rules. These are very widely used in Retail Analytics where we 
are interested to know what a group of products will sell together. In other 
words, what is the probability of buying a particular second product given 
that the consumer has already bought the first product?
Correct targeting of online ads
If you browse the Internet, which I am sure you must be doing frequently, you must 
have encountered online ads, both on the websites and smartphone apps. Just like 
the ads in the newspaper or TV, there is a publisher and an advertiser for online ads 
too. The publisher in this case is the website or the app where the ad will be shown 
while the advertiser is the company/organization that is posting that ad.
The ultimate goal of an online ad is to be clicked on. Each instance of an ad display 
is called an impression. The number of clicks per impression is called Click Through 
Rate and is the single most important metric that the advertisers are interested in. 
The problem statement is to determine the list of publishers where the advertiser 
should publish its ads so that the Click Through Rate is the maximum.
How is it done?
•	
The historical data in this case will consist of information about people who 
visited a certain website/app and whether they clicked the published ad or 
not. Some or a combination of classification models, such as Decision Trees, 
and Support Vector Machines are used in such cases to determine whether a 
visitor will click on the ad or not, given the visitor's profile information.
•	
One problem with standard classification algorithms in such cases is that the 
Click Through Rates are very small numbers, of the order of less than 1%. 
The resulting dataset that is used for classification has a very sparse positive 
outcome. The data needs to be downsampled to enrich the data with positive 
outcomes before modelling.
The logistical regression is one of the most standard classifiers for situations with 
binary outcomes. In banking, whether a person will default on his loan or not can be 
predicted using logistical regression given his credit history.

Getting Started with Predictive Modelling
[ 176 ]
Santa Cruz predictive policing
Based on the historical data consisting of the area and time window of the occurrence 
of a crime, a model was developed to predict the place and time where the next 
crime might take place.
How is it done?
•	
A decision tree model was created using the historical data. The prediction of 
the model will foretell whether a crime will occur in an area on a given date 
and time in the future.
•	
The model is consistently recalibrated every day to include the crimes that 
happened during that day.
The good news is that the police are using such techniques to predict the crime 
scenes in advance so that they can prevent it from happening. The bad news is that 
certain terrorist organizations are using such techniques to target the locations that 
will cause the maximum damage with minimal efforts from their side. The good 
news again is that this strategic behavior of terrorists has been studied in detail and 
is being used to form counter-terrorist policies.
Determining the activity of a smartphone user 
using accelerometer data
The accelerometer in a smartphone measures the acceleration over a period of time 
as the user indulges in various activities. The acceleration is measured over the three 
axes, X, Y, and Z. This acceleration data can then be used to determine whether the 
user is sleeping, walking, running, jogging, and so on.
How is it done?
•	
The acceleration data is clustered based on the acceleration values in the 
three directions. The values of the similar activities cluster together.
•	
The clustering performs well in such cases if the columns contributing the 
maximum to the separation of activities are also included while calculating 
the distance matrix for clustering. Such columns can be found out using a 
technique called Singular Value Decomposition.

Chapter 1
[ 177 ]
Sport and fantasy leagues
Moneyball, anyone? Yes, the movie. The movie where a statistician turns the fortunes 
of a poorly performing baseball team, Oak A, by developing an algorithm to select 
players who were cheap to buy but had a lot of latent potential to perform.
How was it done?
•	
Bill James, using historical data, concluded that the older metrics used to rate 
a player, such as stolen balls, runs batted in, and batting average were not 
very useful indicators of a player's performance in a given match. He rather 
relied on metrics like on-base percentage and sluggish percentage to be a 
better predictor of a player's performance.
•	
The chief statistician behind the algorithms, Bill James, compiled the data 
for performance of all the baseball league players and sorted them for these 
metrics. Surprisingly, the players who had high values for these statistics also 
came at cheaper prices.
This way, they gathered an unbeatable team that didn't have individual stars who 
came at hefty prices but as a team were an indomitable force. Since then, these 
algorithms and their variations have been used in a variety of real and fantasy 
leagues to select players. The variants of these algorithms are also being used by 
Venture Capitalists to optimize and automate their due diligence to select the 
prospective start-ups to fund.
Python and its packages – download and 
installation
There are various ways in which one can access and install Python and its packages. 
Here we will discuss a couple of them.
Anaconda
Anaconda is a popular Python distribution consisting of more than 195 popular 
Python packages. Installing Anaconda automatically installs many of the packages 
discussed in the preceding section, but they can be accessed only through an 
IDE called Spyder (more on this later in this chapter), which itself is installed on 
Anaconda installation. Anaconda also installs IPython Notebook and when you click 
on the IPython Notebook icon, it opens a browser tab and a Command Prompt.

Getting Started with Predictive Modelling
[ 178 ]
Anaconda can be downloaded and installed from the following web 
address: http://continuum.io/downloads
Download the suitable installer and double click on the .exe file and it will install 
Anaconda. Two of the features that you must check after the installation are:
•	
IPython Notebook
•	
Spyder IDE
Search for them in the "Start" icon's search, if it doesn't appear in the list of programs 
and files by default. We will be using IPython Notebook extensively and the codes in 
this book will work the best when run in IPython Notebook.
IPython Notebook can be opened by clicking on the icon. Alternatively, you can 
use the Command Prompt to open IPython Notebook. Just navigate to the directory 
where you have installed Anaconda and then write ipython notebook, as shown in 
the following screenshot:
Fig. 1.3: Opening IPython Notebook
On the system used for this book, Anaconda was installed in the C:\
Users\ashish directory. One can open a new Notebook in IPython by 
clicking on the New Notebook button on the dashboard, which opens up. 
In this book, we have used IPython Notebook extensively.
Standalone Python
You can download a Python version that is stable and is compatible to the OS on 
your system. The most stable version of Python is 2.7.0. So, installing this version is 
highly recommended. You can download it from https://www.python.org/ and 
install it.

Chapter 1
[ 179 ]
There are some Python packages that you need to install on your machine before 
you start predictive analytics and modelling. This section consists of a demo of 
installation of one such library and a brief description of all such libraries.
Installing a Python package
There are several ways to install a Python package. The easiest and the most effective 
is the one using pip. As you might be aware, pip is a package management system 
that is used to install and manage software packages written in Python. To be able to 
use it to install other packages, pip needs to be installed first.
Installing pip
The following steps demonstrate how to install pip. Follow closely!
1.	 Navigate to the webpage shown in the following screenshot. The URL 
address is https://pypi.python.org/pypi/pip:
Downloading pip from the Python's official website

Getting Started with Predictive Modelling
[ 180 ]
2.	 Download the pip-7.0.3.tar.gz file and unzip in the folder where Python 
is installed. If you have Python v2.7.0 installed, this folder should be C:\
Python27:
Unzipping the .zar file for pip in the correct folder
3.	 On unzipping the previously mentioned file, a folder called pip-7.0.3 is 
created. Opening that folder will take you to the screen similar to the one in 
the preceding screenshot.
4.	 Open the CMD on your computer and change the current directory to the 
current directory in the preceding screenshot that is C:\Python27\pip-
7.0.3 using the following command:
cd C:\Python27\pip-7.0.3.
5.	 The result of the preceding command is shown in the following screenshot:
Navigating to the directory where pip is installed
6.	 Now, the current directory is set to the directory where setup file for pip 
(setup.py) resides. Write the following command to install pip:
python setup.py install

Chapter 1
[ 181 ]
7.	 The result of the preceding command is shown in the following screenshot:
Installing pip using a command line
Once pip is installed, it is very easy to install all the required Python packages to  
get started.
Installing Python packages with pip
The following are the steps to install Python packages using pip, which we just 
installed in the preceding section:
1.	 Change the current directory in the command prompt to the directory where 
the Python v2.7.0 is installed that is: C:\Python27.
2.	 Write the following command to install the package:
pip install package-name
3.	 For example, to install pandas, you can proceed as follows:
Installing a Python package using a command line and pip
4.	 Finally, to confirm that the package has installed successfully, write the 
following command:
python  -c "import pandas"
5.	 The result of the preceding command is shown in the following screenshot:
Checking whether the package has installed correctly or not

Getting Started with Predictive Modelling
[ 182 ]
If this doesn't throw up an error, then the package has been installed successfully.
Python and its packages for predictive 
modelling
In this section, we will discuss some commonly used packages for predictive 
modelling.
pandas: The most important and versatile package that is used widely in data science 
domains is pandas and it is no wonder that you can see import pandas at the 
beginning of any data science code snippet, in this book, and anywhere in general. 
Among other things, the pandas package facilitates:
•	
The reading of a dataset in a usable format (data frame in case of Python)
•	
Calculating basic statistics
•	
Running basic operations like sub-setting a dataset, merging/concatenating 
two datasets, handling missing data, and so on
The various methods in pandas will be explained in this book as and when we  
use them.
To get an overview, navigate to the official page of pandas here: 
http://pandas.pydata.org/index.html
NumPy: NumPy, in many ways, is a MATLAB equivalent in the Python 
environment. It has powerful methods to do mathematical calculations and 
simulations. The following are some of its features:
•	
A powerful and widely used a N-d array element
•	
An ensemble of powerful mathematical functions used in linear algebra, 
Fourier transforms, and random number generation
•	
A combination of random number generators and an N-d array elements 
is used to generate dummy datasets to demonstrate various procedures, a 
practice we will follow extensively, in this book
To get an overview, navigate to official page of NumPy at 
http://www.NumPy.org/

Chapter 1
[ 183 ]
matplotlib: matplotlib is a Python library that easily generates high-quality 2-D 
plots. Again, it is very similar to MATLAB.
•	
It can be used to plot all kind of common plots, such as histograms, stacked 
and unstacked bar charts, scatterplots, heat diagrams, box plots, power 
spectra, error charts, and so on
•	
It can be used to edit and manipulate all the plot properties such as title, axes 
properties, color, scale, and so on
To get an overview, navigate to the official page of matplotlib at: 
http://matplotlib.org
IPython: IPython provides an environment for interactive computing.
It provides a browser-based notebook that is an IDE-cum-development environment 
to support codes, rich media, inline plots, and model summary. These notebooks and 
their content can be saved and used later to demonstrate the result as it is or to save 
the codes separately and execute them. It has emerged as a powerful tool for web 
based tutorials as the code and the results flow smoothly one after the other in this 
environment. At many places in this book, we will be using this environment.
To get an overview, navigate to the official page of IPython here 
http://ipython.org/
Scikit-learn: scikit-learn is the mainstay of any predictive modelling in Python. 
It is a robust collection of all the data science algorithms and methods to implement 
them. Some of the features of scikit-learn are as follows:
•	
It is built entirely on Python packages like pandas, NumPy, and matplotlib
•	
It is very simple and efficient to use
•	
It has methods to implement most of the predictive modelling techniques, 
such as linear regression, logistic regression, clustering, and Decision Trees
•	
It gives a very concise method to predict the outcome based on the model 
and measure the accuracy of the outcomes
To get an overview, navigate to the official page of scikit-learn 
here: http://scikit-learn.org/stable/index.html
Python packages, other than these, if used in this book, will be situation based and 
can be installed using the method described earlier in this section.

Getting Started with Predictive Modelling
[ 184 ]
IDEs for Python
The IDE or the Integrated Development Environment is a software that provides 
the source-code editor cum debugger for the purpose of writing code. Using these 
software, one can write, test, and debug a code snippet before adding the snippet in 
the production version of the code.
IDLE: IDLE is the default Integrated Development Environment for Python  
that comes with the default implementation of Python. It comes with the  
following features:
•	
Multi-window text-editor with auto-completion, smart-indent, syntax,  
and keyword highlighting
•	
Python shell with syntax highlighting
IDLE is widely popular as an IDE for beginners; it is simple to use and works well 
for simple tasks. Some of the issues with IDLE are bad output reporting, absence of 
line numbering options, and so on. As a result, advanced practitioners move on to 
better IDEs.
IPython Notebook: IPython Notebook is a powerful computational environment 
where code, execution, results, and media can co-exist in one single document.  
There are two components of this computing environment:
•	
IPython Notebook: Web applications containing code, executions, plots,  
and results are stored in different cells; they can be saved and edited as  
and when required
•	
Notebook: It is a plain text document meant to record and distribute the 
result of a computational analysis
The IPython documents are stored with an extension .ipynb in the directory where 
it is installed on the computer.
Some of the features of IPython Notebook are as follows:
•	
Inline figure rendering of the matplotlib plots that can be saved in multiple 
formats(JPEG, PNG).
•	
Standard Python syntax in the notebook can be saved as a Python script.
•	
The notebooks can be saved as HTML files and .ipynb files. These notebooks 
can be viewed in browsers and this has been developed as a popular tool for 
illustrated blogging in Python. A notebook in IPython looks as shown in the 
following screenshot:

Chapter 1
[ 185 ]
An Ipython Notebook
Spyder: Spyder is a powerful scientific computing and development environment for 
Python. It has the following features:
•	
Advanced editing, auto-completion, debugging, and interactive testing
•	
Python kernel and code editor with line numbering in the same screen
•	
Preinstalled scientific packages like NumPy, pandas, scikit-learn, matplotlib, 
and so on.

Getting Started with Predictive Modelling
[ 186 ]
•	
In some ways, Spyder is very similar to RStudio environment where text 
editing and interactive testing go hand in hand:
The interface of Spyder IDE
In this book, IPython Notebook and Spyder have been used extensively. IDLE 
has been used from time to time and some people use other environments, such 
as Pycharm. Readers of this book are free to use such editors if they are more 
comfortable with them. However, they should make sure that all the required 
packages are working fine in those environments.

Chapter 1
[ 187 ]
Summary
The following are some of the takeaways from this chapter:
•	
Social media and Internet of Things have resulted in an avalanche of data.
•	
Data is powerful but not in its raw form. The data needs to be processed  
and modelled.
•	
Organizations across the world and across the domains are using data to 
solve critical business problems. The knowledge of statistical algorithms, 
statisticals tool, business context, and handling of historical data is vital to 
solve these problems using predictive modelling.
•	
Python is a robust tool to handle, process, and model data. It has an array of 
packages for predictive modelling and a suite of IDEs to choose from.
Let us enter the battlefield where Python is our weapon. We will start using it from 
the next chapter. In the next chapter, we will learn how to read data in various cases 
and do a basic processing.


[ 189 ]
Data Cleaning
Without any further ado, lets kick-start the engine and start our foray into the world 
of predictive analytics. However, you need to remember that our fuel is data. In 
order to do any predictive analysis, one needs to access and import data for the 
engine to rev up.
I assume that you have already installed Python and the required packages with an 
IDE of your choice. Predictive analytics, like any other art, is best learnt when tried 
hands-on and practiced as frequently as possible. The book will be of the best use if 
you open a Python IDE of your choice and practice the explained concepts on your 
own. So, if you haven't installed Python and its packages yet, now is the time. If not 
all the packages, at-least pandas should be installed, which are the mainstay of the 
things that we will learn in this chapter.
After reading this chapter, you should be familiar with the following topics:
•	
Handling various kind of data importing scenarios that is importing various 
kind of datasets (.csv, .txt), different kind of delimiters (comma, tab, pipe), 
and different methods (read_csv, read_table)
•	
Getting basic information, such as dimensions, column names,  
and statistics summary
•	
Getting basic data cleaning done that is removing NAs and blank spaces, 
imputing values to missing data points, changing a variable type, and so on
•	
Creating dummy variables in various scenarios to aid modelling
•	
Generating simple plots like scatter plots, bar charts, histograms, box plots, 
and so on
From now on, we will be using a lot of publicly available datasets to illustrate 
concepts and examples. All the used datasets have been stored in a Google Drive 
folder, which can be accessed from this link: https://goo.gl/zjS4C6.

Data Cleaning
[ 190 ]
This folder is called "Datasets for Predictive Modelling with 
Python". This folder has a subfolder dedicated to each chapter 
of the book. Each subfolder contains the datasets that were 
used in the chapter.
The paths for the dataset used in this book are paths on my 
local computer. You can download the datasets from these 
subfolders to your local computer before using them. Better 
still, you can download the entire folder, at once and save it 
somewhere on your local computer.
Reading the data – variations and 
examples
Before we delve deeper into the realm of data, let us familiarize ourselves with a few 
terms that will appear frequently from now on.
Data frames
A data frame is one of the most common data structures available in Python. Data 
frames are very similar to the tables in a spreadsheet or a SQL table. In Python 
vocabulary, it can also be thought of as a dictionary of series objects (in terms of 
structure). A data frame, like a spreadsheet, has index labels (analogous to rows)  
and column labels (analogous to columns). It is the most commonly used pandas 
object and is a 2D structure with columns of different or same types. Most of the 
standard operations, such as aggregation, filtering, pivoting, and so on which can 
be applied on a spreadsheet or the SQL table can be applied to data frames using 
methods in pandas.
The following screenshot is an illustrative picture of a data frame. We will learn more 
about working with them as we progress in the chapter:

Chapter 2
[ 191 ]
Fig. 2.1 A data frame
Delimiters
A delimiter is a special character that separates various columns of a dataset from 
one another. The most common (one can go to the extent of saying that it is a default 
delimiter) delimiter is a comma (,). A .csv file is called so because it has comma 
separated values. However, a dataset can have any special character as its delimiter 
and one needs to know how to juggle and manage them in order to do an exhaustive 
and exploratory analysis and build a robust predictive model. Later in this chapter, 
we will learn how to do that.
Various methods of importing data in 
Python
pandas is the Python library/package of choice to import, wrangle, and manipulate 
datasets. The datasets come in various forms; the most frequent being in the .csv 
format. The delimiter (a special character that separates the values in a dataset) in a 
CSV file is a comma. Now we will look at the various methods in which you can read 
a dataset in Python.

Data Cleaning
[ 192 ]
Case 1 – reading a dataset using the read_csv 
method
Open an IPython Notebook by typing ipython notebook in the command line.
Download the Titanic dataset from the shared Google Drive folder (any of .xls or 
.xlsx would do). Save this file in a CSV format and we are good to go. This is a very 
popular dataset that contains information about the passengers travelling on the 
famous ship Titanic on the fateful sail that saw it sinking. If you wish to know more 
about this dataset, you can go to the Google Drive folder and look for it.
A common practice is to share a variable description file with the dataset describing 
the context and significance of each variable. Since this is the first dataset we are 
encountering in this book, here is the data description of this dataset to get a feel of 
how data description files actually look like:
VARIABLE DESCRIPTIONS:
pclass          Passenger Class
                (1 = 1st; 2 = 2nd; 3 = 3rd)
survival        Survival
                (0 = No; 1 = Yes)
name            Name
sex             Sex
age             Age
sibsp           Number of Siblings/Spouses Aboard
parch           Number of Parents/Children Aboard
ticket          Ticket Number
fare            Passenger Fare
cabin           Cabin
embarked        Port of Embarkation
                (C = Cherbourg; Q = Queenstown; S = 
Southampton)
boat            Lifeboat
body            Body Identification Number
home.dest       Home/Destination
The following code snippet is enough to import the dataset and get you started:
 import pandas as pd
 data = pd.read_csv('E:/Personal/Learning/Datasets/Book/titanic3.csv')

Chapter 2
[ 193 ]
The read_csv method
The name of the method doesn't unveil its full might. It is a kind of misnomer in 
the sense that it makes us think that it can be used to read only CSV files, which is 
not the case. Various kinds of files, including .txt files having delimiters of various 
kinds can be read using this method.
Let's learn a little bit more about the various arguments of this method in order to 
assess its true potential. Although the read_csv method has close to 30 arguments, 
the ones listed in the next section are the ones that are most commonly used.
The general form of a read_csv statement is something similar to:
pd.read_csv(filepath, sep=', ', dtype=None, header=None, 
skiprows=None, index_col=None, skip_blank_lines=TRUE, na_filter=TRUE)
Now, let us understand the significance and usage of each of these arguments one  
by one:
•	
filepath: filepath is the complete address of the dataset or file that you 
are trying to read. The complete address includes the address of the directory 
in which the file is stored and the full name of the file with its extension. 
Remember to use a forward slash (/) in the directory address. Later in this 
chapter, we will see that the filepath can be a URL as well.
•	
sep: sep allows us to specify the delimiter for the dataset to read. By default, 
the method assumes that the delimiter is a comma (,). The various other 
delimiters that are commonly used are blank spaces ( ), tab (|), and are called 
space delimiter or tab demilited datasets. This argument of the method also 
takes regular expressions as a value.
•	
dtype: Sometimes certain columns of the dataset need to be formatted to 
some other type, in order to apply certain operations successfully. One 
example is the date variables. Very often, they have a string type which 
needs to be converted to date type before we can use them to apply date-
related operations. The dtype argument is to specify the data type of the 
columns of the dataset. Suppose, two columns a and b, of the dataset need to 
be formatted to the types int32 and float64; it can be achieved by passing 
{'a':np.float64, 'b':np.int32} as the value of dtype. If not specified, it 
will leave the columns in the same format as originally found.
•	
header: The value of a header argument can be an integer or a list. 
Most of the times, datasets have a header containing the column names. 
The header argument is used to specify which row to be used as the header. 
By default, the first row is the header and it can be represented as header 
=0. If one doesn't specify the header argument, it is as good as specifying 
header=0. If one specifies header=None, the method will read the data 
without the header containing the column names.

Data Cleaning
[ 194 ]
•	
names: The column names of a dataset can be passed off as a list using this 
argument. This argument will take lists or arrays as its values. This 
argument is very helpful in cases where there are many columns and the 
column names are available as a list separately. We can pass the list of 
column names as a value of this argument and the column names in the list 
will be applied.
•	
skiprows: The value of a skiprows argument can be an integer or a list. 
Using this argument, one can skip a certain number of rows specified as the 
value of this argument in the read data, for example skiprows=10 will read 
in the data from the 11th row and the rows before that will be ignored.
•	
index_col: The value of an index_col argument can be an integer or a 
sequence. By default, no row labels will be applied. This argument allows 
one to use a column, as the row labels for the rows in a dataset.
•	
skip_blank_lines: The value of a skip_blank_lines argument takes 
Boolean values only. If its value is specified as True, the blank lines are 
skipped rather than interpreting them as NaN (not allowed/missing values; 
we shall discuss them in detail soon) values. By default, its value is set  
to False.
•	
na_filter: The value of a na-filter argument takes Boolean values only. 
It detects the markers for missing values (empty strings and NA values) 
and removes them if set to False. It can make a significant difference while 
importing large datasets.
Use cases of the read_csv method
The read_csv method can be put to a variety of uses. Let us look at some such  
use cases.
Passing the directory address and filename as variables
Sometimes it is easier and viable to pass the directory address and filename as 
variables to avoid hard-coding. More importantly so, when one doesn't want to 
hardcode the full address of the file and intend to use this full address many times. 
Let us see how we can do so while importing a dataset.
import pandas as pd
path = 'E:/Personal/Learning/Datasets/Book'
filename = 'titanic3.csv'
fullpath = path+'/'+filename
data = pd.read_csv(fullpath)

Chapter 2
[ 195 ]
For such cases, alternatively, one can use the following snippet that uses the  
path.join method in an os package:
import pandas as pd
import os
path = 'E:/Personal/Learning/Datasets/Book'
filename = 'titanic3.csv'
fullpath = os.path.join(path,filename)
data = pd.read_csv(fullpath)
One advantage of using the latter method is that it trims the lagging or leading white 
spaces, if any, and gives the correct filename.
Reading a .txt dataset with a comma delimiter
Download the Customer Churn Model.txt dataset from the Google Drive folder 
and save it on your local drive. To read this dataset, the following code snippet  
will do:
import pandas as pd
data = read_csv('E:/Personal/Learning/Datasets/Book/Customer Churn 
Model.txt')
As you can see, although it's a text file, it can be read easily using the read_csv 
method without even specifying any other argument of the method.
Specifying the column names of a dataset from a list
We just read the Customer Churn Model.txt file in the last segment with the 
default column names. But, what if we want to rename some or all of the column 
names? Or, what if the column names are not there already and we want to assign 
names to columns from a list (let's say, available in a CSV file).
Look for a CSV file called Customer Churn Columns.csv in the Google Drive and 
download it. I have put English alphabets as placeholders for the column names in 
this file. We shall use this file to create a list of column names to be passed on to the 
dataset. You can change the names in the CSV files, if you like, and see how they are 
incorporated as column names.
The following code snippet will give the name of the column names of the dataset 
we just read:
import pandas as pd
data = pd.read_csv('E:/Personal/Learning/Datasets/Book/Customer Churn 
Model.txt')
data.columns.values

Data Cleaning
[ 196 ]
If you run it on one of the IDEs, you should get the following screenshot as  
the output:
Fig. 2.2: The column names in the Customer Churn Model.txt dataset
This basically lists all the column names of the dataset. Let us now go ahead and 
change the column names to the names we have in the Customer Churn Columns.
csv file.
data_columns = pd.read_csv('E:/Personal/Learning/Predictive Modeling 
Book/Book Datasets/Customer Churn Columns.csv')
data_column_list = data_columns['Column_Names'].tolist()
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Customer Churn Model.txt',header=None,names=data_column_list)
data.columns.values
The output after running this snippet should look like the following screenshot (if 
you haven't made any changes to the values in the Customer Churn Columns.csv 
file):
Fig. 2.3: The column names in the Customer Churn Columnsl.txt dataset which have been passed  
to the data frame data
The key steps in this process are:
•	
Sub-setting the particular column (containing the column names) and 
converting it to a list—done in the second line
•	
Passing the header=None and names=name of the list containing  
the column names(data_column_list in this case) in the  
read_csv method
If some of the terms, such as sub-setting don't make sense now, just remember that  
it is an act of selecting a combination of particular rows or columns of the dataset. 
We will discuss this in detail in the next chapter.

Chapter 2
[ 197 ]
Case 2 – reading a dataset using the open 
method of Python
pandas is a very robust and comprehensive library to read, explore, and manipulate 
a dataset. But, it might not give an optimal performance with very big datasets as it 
reads the entire dataset, all at once, and blocks the majority of computer memory. 
Instead, you can try one of the Python's file handling methods—open. One can read 
the dataset line by line or in chunks by running a for loop over the rows and delete 
the chunks from the memory, once they have been processed. Let us look at some of 
the use case examples of the open method.
Reading a dataset line by line
As you might be aware that while reading a file using the open method, we can 
specify to use a particular mode that is read, write, and so on. By default, the method 
opens a file in the read-mode. This method can be useful while reading a big dataset, 
as this method reads data line-by-line (not at once, unlike what pandas does). You 
can read datasets into chunks using this method.
Let us now go ahead and open a file using the open method and count the number of 
rows and columns in the dataset:
data=open('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Customer Churn Model.txt','r')
cols=data.next().strip().split(',')
no_cols=len(data.next().strip().split(','))
A couple of points about this snippet:
•	
'r' has been explicitly mentioned and hence the file will be opened in the 
read mode. To open it in the write mode, one needs to pass 'w' in place of 
'r'.
•	
The next method navigates the computer memory to the line next to the 
header. The strip method is used to remove all the trailing and leading 
blank spaces from the line. The split method breaks down a line into 
chunks separated by the argument provided to the split method. In this 
case, it is ','.

Data Cleaning
[ 198 ]
Finding the number of the rows is a bit tedious, but here lies the key trick to reading 
a huge file in chunks:
counter=0
main_dict={}
for col in cols:
    main_dict[col]=[]
Basically, we are doing the following two tasks in the preceding code snippet:
•	
Defining a counter variable that will increment its value by 1 on passing 
each line and hence will count the number of rows/lines at the end of  
the loop
•	
Defining a dictionary called main_dict with column names as the keys and 
the values in the columns as the values of the dictionary
Now, we are all set to run a for loop over the lines in the dataset to determine the 
number of rows in the dataset:
for line in data:
    values = line.strip().split(',')
    for i in range(len(cols)):
        main_dict[cols[i]].append(values[i])
    counter += 1
print "The dataset has %d rows and %d columns" % (counter,no_cols)
The explanation of the code-snippet is as follows:
1.	 Running a for loop over the lines in the dataset and splitting the lines in the 
values by ','. These values are nothing but the values contained in each 
column for that line (row).
2.	 Running a second for loop over the columns for each line and appending 
the column values to the main_dict dictionary, which we defined in the 
previous step. So, for each key of the main_dict dictionary, all the column 
values are appended together. Each key of the main_dict becomes the 
column name of the dataset, while the values of each key in the dictionary 
are the values in each column.
3.	 Printing the number of rows and columns of the dataset that are contained in 
counter and no_cols respectively.

Chapter 2
[ 199 ]
The main_dict dictionary, in a way, contains all the information in the dataset; 
hence, it can be converted to a data frame, as we have read already in this chapter 
that a dictionary can be converted to a data frame using the DataFrame method in 
pandas. Let us do that:
import pandas as pd
df=pd.DataFrame(main_dict)
print df.head(5)
This process can be repeated after a certain number of lines, say 10000 lines, for a 
large file; it can be read in and processed in chunks.
Changing the delimiter of a dataset
Earlier in this chapter, we said that juggling and managing delimiters is a great skill 
to master. Let us see one example of how we can change the delimiter of a dataset.
The Customer Churn Model.txt has comma (',') as a delimiter. It looks something 
similar to the following screenshot:
Fig. 2.4: A chunk of Customer Churn Model.txt dataset with default delimiter comma (',')
Note that, any special character can be a delimiter. Let us change the delimiter to a 
'slash t' ('/t'):
infile='E:/Personal/Learning/Datasets/Book/Customer Churn Model.txt'
outfile='E:/Personal/Learning/Datasets/Book/Tab Customer Churn Model.
txt'
with open(infile) as infile1:
  with open(outfile,'w') as outfile1:
    for line in infile1:
      fields=line.split(',')
      outfile1.write('/t'.join(fields))

Data Cleaning
[ 200 ]
This code snippet will generate a file called Tab Customer Churn Model.txt in 
the specified directory. The file will have a '/t' delimiter and will look something 
similar to the following screenshot:
Fig. 2.5: A chunk of Tab Customer Churn Model.txt with changed delimiter ('/t')
The code snippet can be explained as follows:
1.	 Creating two variables called infile and outfile. The infile variable is 
the one whose delimiter we wish to change and outfile is the one in which 
we will write the results after changing the delimiter.
2.	 The infile is opened in the read mode, while outfile is opened in the 
write mode.
3.	 The lines in the infile are split based on the existing delimiter that is ',' 
and the chunks are called fields. Each line will have several fields (equal to 
the number of columns).
4.	 The lines in the outfile are created by joining the fields of each line 
separated by the new delimiter of our choice that is '/t'.
5.	 The file is written into the directory specified in the definition of the outfile.
To demonstrate this, the read_csv method, as described earlier, can be used to read 
datasets that have a delimiter other than a comma, we will try to read the dataset 
with a '/t' delimiter, we just created:
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Tab Customer Churn Model.txt',sep='/t')
Case 3 – reading data from a URL
Several times, we need to read the data directly from a web URL. This URL 
might contain the data written in it or might contain a file which has the data. For 
example, navigate to this website, http://winterolympicsmedals.com/ which 
lists the medals won by various countries in different sports during the Winter 
Olympics. Now type the following address in the URL address bar: http://
winterolympicsmedals.com/medals.csv.

Chapter 2
[ 201 ]
A CSV file will be downloaded automatically. If you choose to download it 
manually, saving it and then specifying the directory path for the read_csv method 
is a time consuming process. Instead, Python allows us to read such files directly 
from the URL. Apart from the significant saving in time, it is also beneficial to loop 
over the files when there are many such files to be downloaded and read in.
A simple read_csv statement is required to read the data directly from the URL:
import pandas as pd
medal_data=pd.read_csv('http://winterolympicsmedals.com/medals.csv')
Alternatively, to work with URLs to get data, one can use a couple of Python 
packages, which we have not used till now, that is csv and urllib. The readers 
can go to the documentation of the packages to learn more about these packages. It 
is sufficient to know that csv provides a range of methods to handle the CSV files, 
while urllib is used to navigate and access information from the URL. Here is how 
it can be done:
import csv
import urllib2
url='http://archive.ics.uci.edu/ml/machine-learning-databases/iris/
iris.data'
response=urllib2.urlopen(url)
cr=csv.reader(response)
for rows in cr:
  print rows
The working of the preceding code snippet can be explained in the following  
two points:
1.	 The urlopen method of the urllib2 library creates a response that can be 
read in using the reader method of the csv library.
2.	 This instance is an iterator and can be iterated over its rows.
The csv module is very helpful in dealing with CSV files. It can be used to read the 
dataset row by row, or in other words, iterate over the dataset among other things.  
It can be used to write to CSV files as well.
Case 4 – miscellaneous cases
Apart from the standard cases described previously, there are certain less frequent 
cases of data file handling that might need to be taken care of. Let's have a look at 
two of them.

Data Cleaning
[ 202 ]
Reading from an .xls or .xlsx file
Go to the Google Drive and look for .xls and .xlsx versions of the Titanic dataset. 
They will be named titanic3.xls and titanic3.xlsx. Download both of them 
and save them on your computer. The ability to read Excel files with all its sheets is a 
very powerful technique available in pandas. It is done using a read_excel method, 
as shown in the following code:
import pandas as pd
data=pd.read_excel('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/titanic3.xls','titanic3')
import pandas as pd
data=pd.read_excel('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/titanic3.xlsx','titanic3')
It works with both, .xls and .xlsx files. The second argument of the read_excel 
method is the sheet name that you want to read in.
Another available method to read a delimited data is read_table. The read_table 
is exactly similar to read_csv with certain default arguments for its definition. In 
some sense, read_table is a more generic form of read_csv.
Writing to a CSV or Excel file
A data frame can be written in a CSV or an Excel file using a to_csv or to_excel 
method in pandas. Let's go back to the df data frame that we created in Case 2 – 
reading a dataset using the open method of Python. This data frame can be exported to a 
directory in a CSV file, as shown in the following code:
df.to_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Customer Churn Model.csv'
Or to an Excel file, as follows:
df.to_excel('E:/Personal/Learning/Predictive Modeling Book/Book
Datasets/Customer Churn Model.xlsx'
Basics – summary, dimensions, and 
structure
After reading in the data, there are certain tasks that need to be performed to get the 
touch and feel of the data:
•	
To check whether the data has read in correctly or not
•	
To determine how the data looks; its shape and size

Chapter 2
[ 203 ]
•	
To summarize and visualize the data
•	
To get the column names and summary statistics of numerical variables
Let us go back to the example of the Titanic dataset and import it again. The head() 
method is used to look at the first first few rows of the data, as shown:
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Datasets/Book/titanic3.csv')
data.head()
The result will look similar to the following screenshot:
Fig. 2.6: Thumbnail view of the Titanic dataset obtained using the head() method
In the head() method, one can also specify the number of rows they want to see.  
For example, head(10) will show the first 10 rows.
The next attribute of the dataset that concerns us is its dimension, that is the  
number of rows and columns present in the dataset. This can be obtained by  
typing data.shape.
The result obtained is (1310,14), indicating that the dataset has 1310 rows  
and 14 columns.
As discussed earlier, the column names of a data frame can be listed using data.
column.values, which gives the following output as the result:
Fig. 2.7: Column names of the the Titanic dataset
Another important thing to do while glancing at the data is to create summary 
statistics for the numerical variables. This can be done by:
data.describe()

Data Cleaning
[ 204 ]
We get the following result:
Fig. 2.8: Summary statistics for the numerical variables in the Titanic dataset
Knowing the type each column belongs to is the key to determine their behavior 
under some numerical or manipulation operation. Hence, it is of critical importance 
to know the type of each column. This can be done as follows:
data.dtypes
We get the following result from the preceding code snippet:
Fig. 2.9: Variable types of the columns in the Titanic dataset
Handling missing values
Checking for missing values and handling them properly is an important step in the 
data preparation process, if they are left untreated they can:
•	
Lead to the behavior between the variables not being analyzed correctly
•	
Lead to incorrect interpretation and inference from the data

Chapter 2
[ 205 ]
To see how; move up a few pages to see how the describe method is explained. 
Look at the output table; why are the counts for many of the variables different 
from each other? There are 1310 rows in the dataset, as we saw earlier in the section. 
Why is it then that the count is 1046 for age, 1309 for pclass, and 121 for body. This 
is because the dataset doesn't have a value for 264 (1310-1046) entries in the age 
column, 1 (1310-1309) entry in the pclass column, and 1189 (1310-121) entries in 
the body column. In other words, these many entries have missing values in their 
respective columns. If a column has a count value less than the number of rows in 
the dataset, it is most certainly because the column contains missing values.
Checking for missing values
There are a multitude of in-built methods to check for missing values. Let's go 
through some of them. Suppose you wish to find the entries that have missing values 
in a column of a data frame. It can be done as follows for the body column of the 
data data frame:
pd.isnull(data['body'])
This will give a series indicating True in the cells with missing values and False for 
non-missing values. Just the opposite can be done as follows:
pd.notnull(data['body'])
The result will look something similar to the following screenshot:
Fig. 2.10: The notnull method gives False for missing values and True for non-missing values

Data Cleaning
[ 206 ]
The number of entries with missing values can be counted for a particular column 
to verify whether our calculation earlier about the number of missing entries was 
correct or not. This can be done as follows:
pd.isnull(data['body']).values.ravel().sum()
The result we get is 1189. This is the same number of missing entries from the body 
column as we have calculated in the preceding paragraph. In the preceding one-
liner, the values (True/False; 1/0 in binary) have been stripped off the series and 
have been converted into a row (using the ravel method) to be able to sum them 
up. The sum of 1/0 values (1 for missing values and 0 for non-missing) gives the 
number of total missing values.
The opposite of isnull is notnull. This should give us 121 as the result:
pd.notnull(data['body']).values.ravel().sum()
Before we dig deeper into how to handle missing data, let's see what constitutes the 
missing data and how missing values are generated and propagated.
What constitutes missing data?
Nan is the default keyword for a missing value in Python. None is also considered as 
a missing value by the isnull and notnull functions.
How missing values are generated and propagated
There are various ways in which a missing values are incorporated in the datatset:
•	
Data extraction: While extracting data from a database, the missing values 
can be incorporated in the dataset due to various incompatibilities between 
the database server and the extraction process. In this case, the value is 
actually not missing but is being shown as missing because of the various 
incompatibilities. This can be corrected by optimizing the extraction process.
•	
Data collection: It might be the case that at the time of collection, certain 
data points are not available or not applicable and hence can't be entered 
into the database. Such entries become missing values and can't be obtained 
by changing the data extraction process because they are actually missing. 
For example, in case of a survey in a village, many people might not want 
to share their annual income; this becomes a missing value. Some datasets 
might have missing values because of the way they are collected. A time 
series data will have data starting from the relevant time and before that  
time it will have missing values.

Chapter 2
[ 207 ]
Any numerical operator on a missing value propagates the missing value to the 
resultant variable. For example, while summing the entries in two columns, if one of 
them has a missing value in one of the entries, the resultant sum variable will also 
have a missing value.
Treating missing values
There are basically two approaches to handle missing values: deletion and 
imputation. Deletion means deleting the entire row with one or more missing  
entries. Imputation means replacing the missing entries with some values based  
on the context of the data.
Deletion
One can either delete a complete row or column. One can specify when to delete 
an entire row or column (when any of the entries are missing in a row or all of the 
entries are missing in a row). For our dataset, we can write something, as shown:
data.dropna(axis=0,how='all')
The statement when executed will drop all the rows (axis=0 means rows, axis=1 
means columns) in which all the columns have missing values (the how parameter 
is set to all). One can drop a row even if a single column has a missing value. One 
needs to specify the how method as 'any' to do that:
data.dropna(axis=0,how='any')
Imputation
Imputation is the method of adding/replacing missing values with some other 
values such as 0, a string, or mean of non-missing values of that variable. There are 
several ways to impute a missing value and the choice of the best method depends 
on the context of the data.
One method is to fill the missing values in the entire dataset with some number or 
character variable. Thus, it can be done as follows:
data.fillna(0)
This will replace the missing values anywhere in the dataset with the value 0.  
One can impute a character variable as well:
data.fillna('missing')

Data Cleaning
[ 208 ]
The preceding statement will impute a missing string in place of NaN, None, blanks, 
and so on. Another way is to replace the missing values in a particular column only 
is as shown below.
If you select the body column of the data by typing data['body'], the result will be 
something similar to the following screenshot:
Fig. 2.11: The values in the body column of the Titanic dataset without imputation for missing values
One can impute zeros to the missing values using the following statement:
data['body'].fillna(0)
But after imputing 0 to the missing values, we get something similar to the  
following screenshot:
Fig. 2.12: The values in the body column of the Titanic dataset after imputing 0 for missing values

Chapter 2
[ 209 ]
A common imputation is with the mean or median value of that column. This 
basically means that the missing values are assumed to have the same values as 
the mean value of that column (excluding missing values, of course), which makes 
perfect sense. Let us see how we can do that using the fillna method. Let us have a 
look at the age column of the dataset:
data['age']
Fig. 2.13: The values in the age column of the Titanic dataset without imputation for missing values
As shown in the preceding screenshot, some of the entries in the age column have 
missing values. Let us see how we can impute them with mean values:
data['age'].fillna(data['age'].mean())
The output looks something similar to the following screenshot:
Fig. 2.14: The values in the age column of the Titanic dataset after imputing mean for missing values

Data Cleaning
[ 210 ]
As you can see, all the NaN values have been replaced with 29.881135, which is the 
mean of the age column.
One can use any function in place of mean, the most commonly used functions 
are median or some defined calculation using lambda. Apart from that, there are 
two very important methods in fillna to impute the missing values: ffill and 
backfill. As the name suggests, ffill replaces the missing values with the nearest 
preceding non-missing value while the backfill replaces the missing value with the 
nearest succeeding non-missing value. It will be clearer with the following example:
data['age'].fillna(method='ffill')
Fig. 2.15: The result of using ffill method of imputation on the age column of the Titanic dataset
As it can be seen, the missing value in row number 1297 is replaced with the value in 
row number 1296.
With the backfill statement, something similar happens:
data['age'].fillna(method='backfill')
Fig. 2.16: The result of using backfill method of imputation

Chapter 2
[ 211 ]
As it can be seen, the missing value in row number 1297 is replaced with the value in 
row number 1298.
Creating dummy variables
Creating dummy variables is a method to create separate variable for each  
category of a categorical variable., Although, the categorical variable contains  
plenty of information and might show a causal relationship with output variable,  
it can't be used in the predictive models like linear and logistic regression without 
any processing.
In our dataset, sex is a categorical variable with two categories that are male and 
female. We can create two dummy variables out of this, as follows:
dummy_sex=pd.get_dummies(data['sex'],prefix='sex')
The result of this statement is, as follows:
Fig. 2.17: Dummy variable for the sex variable in the Titanic dataset
This process is called dummifying, the variable creates two new variables that take 
either 1 or 0 value depending on what the sex of the passenger was. If the sex was 
female, sex_female would be 1 and sex_male would be 0. If the sex was male, 
sex_male would be 1 and sex_female would be 0. In general, all but one dummy 
variable in a row will have a 0 value. The variable derived from the value (for that 
row) in the original column will have a value of 1.
These two new variables can be joined to the source data frame, so that they can be 
used in the models. The method to that is illustrated, as follows:
column_name=data.columns.values.tolist()
column_name.remove('sex')
data[column_name].join(dummy_sex)

Data Cleaning
[ 212 ]
The column names are converted to a list and the sex is removed from the list before 
joining these two dummy variables to the dataset, as it will not make sense to have a 
sex variable with these two dummy variables.
Visualizing a dataset by basic plotting
Plots are a great way to visualize a dataset and gauge possible relationships between 
the columns of a dataset. There are various kinds of plots that can be drawn. For 
example, a scatter plot, histogram, box-plot, and so on.
Let's import the Customer Churn Model dataset and try some basic plots:
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Customer Churn Model.txt')
While plotting any kind of plot, it helps to keep these things in mind:
•	
If you are using IPython Notebook, write % matplotlib inline in the input 
cell and run it before plotting to see the output plot inline (in the output cell).
•	
To save a plot in your local directory as a file, you can use the savefig 
method. Let's go back to the example where we plotted four scatter plots in a 
2x2 panel. The name of this image is specified in the beginning of the snippet, 
as a figure parameter of the plot. To save this image one can write the 
following code:
figure.savefig('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Scatter Plots.jpeg')
As you can see, while saving the file, one can specify the local directory to save the 
file and the name of the image and the format in which to save the image (jpeg in 
this case).
Scatter plots
We suspect the Day Mins and Day Charge to be highly correlated, as the calls are 
generally charged based on their duration. To confirm or validate our hypothesis, we 
can draw a scatter plot between Day Mins and Day Charge. To draw this scatter plot, 
we write something similar to the following code:
data.plot(kind='scatter',x='Day Mins',y='Day Charge')

Chapter 2
[ 213 ]
The output looks similar to the following figure where the points lie on a straight  
line confirming our suspicion that they are (linearly) related. As we will see later in 
the chapter on linear regression, such a situation will give a perfect linear fit for the 
two variables:
Fig. 2.18: Scatter plot of Day Charge versus Day Mins
The same is the case when we plot Night Mins and Night Charge against one 
another. However, when we plot Night Calls with Night Charge or Day Calls  
with Day Charge, we don't get to see much of a relationship.
Using the matplotlib library, we can get good quality plots and with a lot of 
flexibility. Let us see how we can plot multiple plots (in different panels) in the  
same image:
import matplotlib.pyplot as plt
figure,axs = plt.subplots(2, 2,sharey=True,sharex=True)
data.plot(kind='scatter',x='Day Mins',y='Day Charge',ax=axs[0][0])
data.plot(kind='scatter',x='Night Mins',y='Night Charge',ax=axs[0][1])
data.plot(kind='scatter',x='Day Calls',y='Day Charge',ax=axs[1][0])
data.plot(kind='scatter',x='Night Calls',y='Night Charge',ax=axs[1]
[1])
Downloading the example code
You can download the example code files for all Packt books you have 
purchased from your account at http://www.packtpub.com. If you 
purchased this book elsewhere, you can visit http://www.packtpub.
com/support and register to have the files e-mailed directly to you.

Data Cleaning
[ 214 ]
Here, we are plotting four graphs in one image in a 2x2 panel using the subplots 
method of the matplotlib library. As you can see in the preceding snippet, we 
have defined the panel to be 2x2 and set sharex and sharey parameters to be True. 
For each plot, we specify their location by passing appropriate values for the ax 
parameter in the plot method. The result looks similar to the following screenshot:
Fig. 2.19: Four plots in a 2x2 panel using the subplots method
Histograms
Plotting histograms is a great way to visualize the distribution of a numerical 
variable. Plotting a histogram is a method to understand the most frequent ranges 
(or bins as they are called) in which the variable lies. One can also check whether the 
variable is normally distributed or skewed on one side.
Let's plot a histogram for the Day Calls variable. We can do so by writing the 
following code:
import matplotlib.pyplot as plt
plt.hist(data['Day Calls'],bins=8)
plt.xlabel('Day Calls Value')
plt.ylabel('Frequency')
plt.title('Frequency of Day Calls')
The first line of the snippet is of prime importance. There we specify the variable 
for which we have to plot the histogram and the number of bins or ranges we want. 
The bins parameters can be passed as a fixed number or as a list of numbers to 
be passed as bin-edges. Suppose, a numerical variable has a minimum value of 1 
and a maximum value of 1000. While plotting histogram for this variable, one can 
either specify bins=10 or 20, or one can specify bins=[0,100,200,300,…1000] or 
[0,50,100,150,200,…..,1000].

Chapter 2
[ 215 ]
The output of the preceding code snippet appears similar to the following snapshot:
Fig. 2.20: Histogram of the Day Calls variable
Boxplots
Boxplots are another way to understand the distribution of a numerical variable.  
It specifies something called quartiles.
If the numbers in a distribution with 100 numbers are arranged in an 
increasing order; the 1st quartile will occupy the 25th position, the 3rd 
quartile will occupy the 75th position, and so on. The median will be the 
average of the 50th and 51st terms. (I hope you brush up on some of the 
statistics you have read till now because we are going to use a lot of it, but 
here is a small refresher). Median is the middle term when the numbers 
in the distribution are arranged in the increasing order. Mode is the one 
that occurs with the maximum frequency, while mean is the sum of all the 
numbers divided by their total count.
Plotting a boxplot in Python is easy. We need to write this to plot a boxplot for  
Day Calls:
import matplotlib.pyplot as plt
plt.boxplot(data['Day Calls'])
plt.ylabel('Day Calls')
plt.title('Box Plot of Day Calls')

Data Cleaning
[ 216 ]
The output looks similar to the following snapshot:
Fig. 2.21: Box Plot for the Day Calls variable
The blue box is of prime importance. The lower-horizontal edge of the box specifies 
the 1st quartile, while the upper-horizontal edge specifies the 3rd quartile. The 
horizontal line in the red specifies the median value. The difference in the 1st and 
3rd quartile values is called the Inter Quartile Range or IQR. The lower and upper 
horizontal edges in black specify the minimum and maximum values respectively.
The boxplots are important plots because of the following reasons:
•	
Boxplots are potent tools to spot outliers in a distribution. Any value that is 
1.5*IQR below the 1st quartile and is 1.5*IQR above the 3rd quartile can be 
classified as an outlier.
•	
For a categorical variable, boxplots are a great way to visualize and compare 
the distribution of each category at one go.
There are a variety of other types of plots that can be drawn depending on the 
problem at hand. We will learn about them as and when needed. For exploratory 
analysis, these three types are enough to provide us enough evidence to further 
or discard our initial hypotheses. These three types can have multiple variations 
and together with the power of looping and panel-wise plotting, we can make the 
plotting; hence, the data exploration process is very efficient.

Chapter 2
[ 217 ]
Summary
The main learning outcomes of this chapter are summarized as follows:
•	
Various methods and variations in importing a dataset using pandas: 
read_csv and its variations, reading a dataset using open method in Python, 
reading a file in chunks using the open method, reading directly from a URL, 
specifying the column names from a list, changing the delimiter of a dataset, 
and so on.
•	
Basic exploratory analysis of data: observing a thumbnail of data, shape, 
column names, column types, and summary statistics for numerical variables
•	
Handling missing values: The reason for incorporation of missing values, 
why it is important to treat them properly, how to treat them properly by 
deletion and imputation, and various methods of imputing data.
•	
Creating dummy variables: creating dummy variables for categorical 
variables to be used in the predictive models.
•	
Basic plotting: scatter plotting, histograms and boxplots; their meaning and 
relevance; and how they are plotted.
This chapter is a head start into our journey to explore our data and wrangle it to 
make it modelling-worthy. The next chapter will go deeper in this pursuit whereby 
we will learn to aggregate values for categorical variables, sub-set the dataset, merge 
two datasets, generate random numbers, and sample a dataset.
Cleaning, as we have seen in the last chapter takes about 80% of the modelling time, 
so it's of critical importance and the methods we are learning will come in handy in 
the pursuit of that goal.


[ 219 ]
Data Wrangling
I assume that by now you are at ease with importing datasets from various sources 
and exploring the look and feel of the data. Handling missing values, creating 
dummy variables and plots are some tasks that an analyst (predictive modeller) does 
with almost all the datasets to make them model-worthy. So, for an aspiring analyst 
it will be better to master these tasks, as well.
Next in the line of items to master in order to juggle data like a pro is data wrangling. 
Put simply, it is just a fancy word for the slicing and dicing of data. If you compare 
the entire predictive modelling process to a complex operation/surgery to be 
performed on a patient, then the preliminary analysis with a stethoscope and 
diagnostic checks on the patient is the data cleaning and exploration process,  
zeroing down on the ailing area and deciding which body part to operate on is  
data wrangling, and performing the surgery/operation is the modelling process.
Surgery/operation
Predictive modelling
Diagnostic checks/asking questions to fill 
missing pieces of information/discarding 
trivial information
Data exploration/Data cleaning
Zeroing down on specific body part/sourcing 
required pieces like blood, catheter
Data wrangling
Operating the area
Modelling the data

Data Wrangling
[ 220 ]
A surgeon can vouch for the fact that zeroing down on a specific body part is the 
most critical piece of the puzzle to crack down before one gets to the root of the 
ailment. The same is the case with data wrangling. The data is not always at one 
place or in one table, maybe the information you need for your model is scattered 
across different datasets. What does one do in such cases? One doesn't always 
need the entire data. Many a times, one needs only a column or a few rows or a 
combination of a few rows and columns. How to do all this jugglery? This is the crux 
of this chapter. Apart from this, the chapter tries to provide the reader with all the 
props needed in their tryst with predictive modelling.
At the end of the chapter, the reader should be comfortable with the  
following functions:
•	
Sub-set a dataset: Slicing and dicing data, selecting few rows and columns 
based on certain conditions that is similar to filtering in Excel
•	
Generating random numbers: Generating random numbers is an important 
tool while performing simulations and creating dummy data frames
•	
Aggregating data: A technique that helps to group the data by categories in 
the categorical variable
•	
Sampling data: This is very important before venturing into the actual 
modelling; dividing a dataset between training and testing data is essential
•	
Merging/appending/concatenating datasets: This is the solution of the 
problem that arises when the data required for the purpose of modelling is 
scattered over different datasets
We will be using a variety of public datasets in this chapter. Another good  
way of demonstrating these concepts is to use dummy datasets created using 
random numbers. In fact, random numbers are used heavily for this purpose.  
We will be using a mix of both public datasets and dummy datasets, created  
using random numbers.
Let us now kick-start the chapter by learning about subsetting a dataset. As it 
unfolds, one will realize how ubiquitous and indispensable this is.
Subsetting a dataset
As discussed in the introductory section, the task of subsetting a dataset can entail 
a lot of things. Let us look at them one by one. In order to demonstrate it, let us first 
import the Customer Churn Model dataset, which we used in the last chapter:
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Customer Churn Model.txt')

Chapter 3
[ 221 ]
Selecting columns
Very frequently, an analyst might come across situations wherein only a handful of 
columns among a vast number of columns are useful and are required in the model. 
It then becomes important, to select particular columns. Let us see how to do that.
If one wishes to select the Account Length variable of the data frame we just 
imported, one can simply write:
account_length=data['Account Length']
account_length.head()
The square bracket ([ ]) syntax is used to subset a column of a data frame. One just 
needs to type the appropriate column name in the square brackets. Selecting one 
column returns a Series object (an object similar to a data frame) consisting of the 
values of the selected column. The output of the preceding snippet is as follows:
Fig. 3.1: First few entries of the Account Length column
The fact that this process returns a series can be confirmed by typing type(account_
length), this will return something similar to the following output, as a result:
Selecting multiple columns can be accomplished in a similar fashion. One just needs 
to add an extra square bracket to indicate that it is a list of column names that they 
are selecting and not just one column.
If one wants to select Account Length, VMail Message, and Day Calls, one can 
write the code, as follows:
subdata = data[['Account Length','VMail Message','Day Calls']]
subdata.head()

Data Wrangling
[ 222 ]
The output of the preceding snippet should be similar to the following screenshot:
Fig. 3.2: First few entries of the Account Length and VMail Message columns
Unlike in the case of selecting a single column, selecting multiple columns throws up 
a data frame, as the result:
type(subdata)
One can also create a list of required columns and pass the list name as the parameter 
inside the square bracket to subset a data frame. The following code snippet will give 
the same result, as shown in Fig. 3.3, in the next section:
wanted_columns=['Account Length','VMail Message','Day Calls']
subdata=data[wanted_columns]
subdata.head()
In some cases, one might want to delete or remove certain columns from the dataset 
before they proceed to modelling. The same approach, as taken in the preceding 
section, can be taken in such cases.
This approach of subsetting columns from data frames works fine when the list of 
columns is relatively small (3-5 columns). After this, the time consumed in typing 
column names warrants some more efficient methods to do this. The trick is to 
manually create a list to complement (a list not containing the elements that are 
present in the other set) the bigger list and create the bigger list using looping. The 
complement list of a big table will always be small; hence, we need to make the 
method a tad bit efficient. 

Chapter 3
[ 223 ]
Let us have a look at the following code snippet to observe how to implement this:
wanted=['Account Length','VMail Message','Day Calls']
column_list=data.columns.values.tolist()
sublist=[x for x in column_list if x not in wanted]
subdata=data[sublist]
subdata.head()
The sublist as expected contains all the column names except the ones listed in the 
wanted list, as shown in the following screenshot:
Fig. 3.3: Column names of the subdata data frame
In the third line of the preceding code snippet, a list comprehension has been used. 
It is a convenient method to run for loops over lists and get lists as output. Many of 
you, who have experience with Python, will know of this. For others, it is not rocket 
science; just a better way to run for loops.
Selecting rows
Selecting rows is similar to selecting columns, in the sense that the same square 
bracket is used, but instead of column names the row number or indices are used. 
Let us see some examples to know how to select a particular number of rows from a 
data frame:
•	
If one wants to select the first 50 rows of the data frame, one can just write:
data[1:50]
•	
It is important to note that one needs to pass a range of numbers to subset  
a data frame over rows. To select 50 rows starting from 25th column, we  
will write:
data[25:75]
•	
If the lower limit is not mentioned, it denotes that the upper limit is the 
starting row of the data, which is row 1 in most cases. Thus, data[:50] is 
similar to data[1:50].
In the same way, if the upper limit is not mentioned, it is assumed to be the last 
row of the dataset. To select all the rows except the first 50 rows, we will write 
data[51:].

Data Wrangling
[ 224 ]
A variety of permutations and combinations can be performed on these rules to fetch 
the row that one needs.
Another important way to subset a data frame by rows is conditional or Boolean 
subsetting. In this method, one filters the rows that satisfy certain conditions. The 
condition can be either an inequality or a comparison written inside the square 
bracket. Let us see a few examples of how one can go about implementing them:
•	
Suppose, one wants to filter the rows that have clocked Total Mins to be 
greater than 500. This can be done as follows:
data1=data[data['Total Mins']>500]
data1.shape
•	
The newly created data frame, after filtering, has 2720 rows compared to 3333 
in the unfiltered data frame. Clearly, the balance rows have been filtered by 
the condition.
•	
Let us have a look at another example, where we provide equality as a 
condition. Let us filter the rows for which the state is VA:
data1=data[data['State']=='VA']
data1.shape
•	
This data frame contains only 77 rows, while the rest get filtered.
•	
One can combine multiple conditions, as well, using AND (&) and OR (|) 
operators. To filter all the rows in the state VA that have Total Mins greater 
than 500, we can write:
data1=data[(data['Total Mins']>500) & (data['State']=='VA')]
data1.shape
•	
This data frame contains only 64 rows; it's lesser than the previous data 
frame. It also has two conditions, both of which must be satisfied to get 
filtered. The AND operator has a subtractive effect.
•	
To filter all the rows that are either in state VA or have Total Mins greater 
than 500, we can write the following code:
data1=data[(data['Total Mins']>500) | (data['State']=='VA')]
data1.shape
•	
This data frame has 2733 rows, which is greater than 2720 rows obtained 
with just one filter of Total Mins being greater than 500. The OR operator  
has an additive affect.

Chapter 3
[ 225 ]
Selecting a combination of rows and columns
This is the most used form of subsetting a dataset. Earlier in this chapter we selected 
three columns of this dataset and called the sub-setted data frame a subdata. What if 
we wish to look at specific rows of that sub-setted data frame? How can we do that? 
We just need another square bracket adjacent to the one already existing.
Let's say, we need to look at the first 50 rows of that sub-setted data frame. We can 
write a snippet, as shown:
subdata_first_50=data[['Account Length','VMail Message','Day Calls']]
[1:50]
subdata_first_50
We can use the already created subdata data frame and subset it for the first 50 rows 
by typing:
subdata[1:50] or subdata[:50]
Alternatively, one can subset the columns using the list name as explained earlier 
and then subset for rows.
Another effective (but a little unstable, as its behavior changes based on the version 
of Python installed) method to select both rows and columns together is the .ix 
method. Let's see how to use this method.
Basically, in the .ix method, we can provide row and column indices (in a lay  
man's term, row and column numbers) inside the square bracket. The syntax  
can be summarized, as follows:
•	
The data frame name is appended with ix
•	
Inside the square bracket, specify the row number (range) and column 
number (range) in that order
Now, let's have a look at a few examples:
•	
Selecting the first 100 rows of the first 5 columns:
data.ix[1:100,1:6]

Data Wrangling
[ 226 ]
The output looks similar to the following screenshot:
Fig. 3.4: First 100 rows of the first 5 columns
•	
Selecting all rows from the first five columns:
data.ix[:,1:6] 
•	
Selecting first 100 rows from all the columns:
data.ix[1:100,:]
The row and column numbers/name can be passed off as a list, as well. Let's have a 
look at how it can be done:
•	
Selecting the first 100 rows from the 2nd, 5th, and 7th columns:
data.ix[1:100,[2,5,7]]
The output looks similar to the following screenshot:
Fig. 3.5: First 100 rows of the 2nd, 5th and 7th columns
•	
Selecting the 1st, 2nd and 5th rows from the 2nd, 5th and 7th columns:
data.ix[[1,2,5],[2,5,7]]

Chapter 3
[ 227 ]
The output looks similar to the following screenshot:
Fig. 3.6: 1st, 2nd and 5th rows of the 2nd, 5th and 7th columns
Instead of row and column indices or numbers, we can also write corresponding 
column names, as shown in the following example:
data.ix[[1,2,5],['Area Code','VMail Plan','Day Mins']]
Creating new columns
Many times during the analysis, we are required to create a new column based on 
some calculation or modification of the existing columns containing a constant value 
to be used in the modelling. Hence, the knowledge of creating new columns becomes 
an indispensable tool to learn. Let's see how to do that.
Suppose, in the Customer Churn Model dataset, we want to calculate the total 
minutes spent during the day, evening, and night. This requires summing up the 3 
columns, which are Day Mins, Eve Mins, and Night Mins. It can be done, as shown 
in the following snippet:
data['Total Mins']=data['Day Mins']+data['Eve Mins']+data['Night 
Mins']
data['Total Mins'].head() 
The output of the snippet is, as follows:
Fig. 3.7: First few entries of the new Total Mins column

Data Wrangling
[ 228 ]
Generating random numbers and their 
usage
Random numbers are just like any other number in their property except for the 
fact that they assume a different value every time the call statement to generate 
a random number is executed. Random number generating methods use certain 
algorithms to generate different numbers every time, which are beyond the scope 
of this book. However, after a finitely large period, they might start generating the 
already generated numbers. In that sense, these numbers are not truly random and 
are sometimes called pseudo-random numbers.
In spite of them actually being pseudo-random, these numbers can be assumed to 
be random for all practical purposes. These numbers are of critical importance to 
predictive analysts because of the following points:
•	
They allow analysts to perform simulations for probabilistic  
multicase scenarios
•	
They can be used to generate dummy data frames or columns of a data frame 
that are needed in the analysis
•	
They can be used for the random sampling of data
Various methods for generating random 
numbers
The method used to deal with random number is called random and is found in 
the numpy library. Let's have a look at the different methods of generating random 
numbers and their usage.
Let's start by generating a random integer between 1 and 100. This can be done,  
as follows:
import numpy as np
np.random.randint(1,100)
If you run the preceding snippet, it will generate a random number between 1 and 
100. When I ran it, it gave me 43 as the result. It might give you something else.
To generate a random number between 0 and 1, we can write something similar to 
the following code:
import numpy as np
np.random.random()

Chapter 3
[ 229 ]
These methods allow us to generate one random number at a time. What if we 
wanted to generate a list of numbers, all lying within a given interval and generated 
randomly. Let's define a function that can generate a list of n random numbers lying 
between a and b.
All one needs to do is define a function, wherein an empty list is created and the 
randomly generated numbers are appended to the list. The recipe to do that is  
shown in the following code snippet:
def randint_range(n,a,b):
    x=[]
    for i in range(n):
        x.append(np.random.randint(a,b))
    return x
After defining this function we can generate, let's say, 10 numbers lying between 2 
and 1000, as shown:
rand_int_gen(10,2,1000)
On the first run, it gives something similar to the following output:
Fig. 3.8: 10 random integers between 2 and 1000
The randrange method is an important method to generate random numbers and 
is in a way an extension to the randint method, as it provides a step argument in 
addition to the start and stop argument in the case of randint function.
To generate three random numbers between 0 and 100, which are all multiples of 5, 
we can write:
import random
for i in range(3):
    print random.randrange(0,100,5) 
You should get something similar to the following screenshot, as a result (the actual 
numbers might change):

Data Wrangling
[ 230 ]
Another related useful method is shuffle, which shuffles a list or an array in 
random order. It doesn't generate a random number, per se, but nevertheless it is 
very useful. Lets see how it works. Lets generate a list of consecutive 100 integers 
and then shuffle the list:
a=range(100)
np.random.shuffle(a)
The list looks similar to the following screenshot before and after the shuffle:
The choice method is another important technique that might come in very handy 
in various scenarios including creating simulations, depending upon selecting a 
random item from a list of items. The choice method is used to pick an item at 
random from a given list of items.
To see an example of how this method works, let's go back to the data frame that we 
have been using all along in this chapter. Let's import that data again and get the list 
of column names, using the following code snippet:
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Customer Churn Model.txt')
column_list=data.columns.values.tolist()
To select one column name from the list, at random, we can write it similar to the 
following example:
np.random.choice(column_list)
This should result in one column name being chosen at random from the list of the 
column names. I got Day Calls for my run. Of course, one can loop over the choice 
method to get multiple items, as we did for the randint method.

Chapter 3
[ 231 ]
Seeding a random number
At the onset of this section on random numbers, we discussed how random numbers 
change their values on every execution of their call statement. They repeat their 
values but only after a very large period. Sometimes, we need to generate a set 
of random numbers that retain their value. This can be achieved by seeding the 
generation of random numbers. Basically, the particular instance of generating a 
random number is given a seed (sort of a key), which when used can regenerate the 
same set of random numbers. Let's see this with an example:
np.random.seed(1)
for i in range(5):
    print np.random.random()
In the first line, we set the seed as 1 and then generated 5 random numbers.  
The output looks something similar to this:
Fig. 3.9: Five random numbers generated through random method with seed 1
If one removes the seed and then generates random numbers, one will get different 
random numbers. Let's have a look:
for i in range(5):
    print np.random.random()
By running the preceding code snippet, one indeed gets different random numbers, 
as shown in the following output screenshot:
Fig. 3.10: Five random number generated through random method without seed 1

Data Wrangling
[ 232 ]
However, if one brings back the seed used to generate random numbers, we can get 
back the same numbers. If we try running the following snippet, we will have to 
regenerate the numbers, as shown in the first case:
np.random.seed(1)
for i in range(5):
    print np.random.random()
Generating random numbers following 
probability distributions
If you have taken a probability class in your school or college, you might have heard 
of probability distributions. There are two concepts that you might want to refresh.
Probability density function
For a random variable, it is just the count of times that the random variable attains a 
particular value x or the number of times that the value of the random variable falls 
in a given range (bins). This gives the probability of attaining a particular value by 
the random variable. Histograms plot this number/probability on the y axis and it 
can be identified as the y axis value of a distribution plot/histogram:
PDF = Prob(X=x)
Cumulative density function
For a random variable, it is defined as the probability that the random variable is less 
than or equal to a given value x. It is the total probability that the random variable is 
less than or equal to a given value. For a given point on the x axis, it is calculated as 
the area enclosed by the frequency distribution curve between by values less than x. 

Chapter 3
[ 233 ]
Mathematically, it is defined as follows:
CDF(x) = Prob(X<=x)
Value of Random
Variable
Frequency
CDF
X
PDF
Fig. 3.11: CDF is the area enclosed by the curve till that value of random variable. PDF is the frequency/
probability of that particular value of random variable.
There are various kinds of probability distributions that frequently occur, including 
the normal (famously known as the Bell Curve), uniform, poisson, binomial, 
multinomial distributions, and so on.
Many of the analyses require generating random numbers that follow a particular 
probability distribution. One can generate random numbers in such a fashion using 
the same random method of the numpy library.
Let's see how one can generate two of the most commonly used distributions,  
which are normal and uniform distributions.
Uniform distribution
A uniform distribution is defined by its endpoints—the start and stop points.  
Each of the points lying in between these endpoints are supposed to occur with  
the same (uniform) probability and hence the name of the distribution. 

Data Wrangling
[ 234 ]
If the start and stop points are a and b, each point between a and b would occur with 
a frequency of 1/(b-a):
Value of Random
Variable
1/(b-a)
a
b
Frequency
Fig. 3.12: In a uniform distribution, all the random variables occur with the same  
(uniform) frequency/probability
As the uniform distribution is defined by its start and stop points, it is essential 
to know these points while generating random numbers following a uniform 
distribution. Thus, these points are taken as input parameters for the uniform 
function that is used to generate a random number following a uniform distribution. 
The other parameter of this function is the number of random numbers that one 
wants to generate.
To generate 100 random numbers lying between 1 and 100, one can write  
the following:
import numpy as np
randnum=np.random.uniform(1,100,100)
To check whether it indeed follows the uniform distribution, let's plot a histogram of 
these numbers and see whether they occur with the same probability or not. This can 
be done using the following code snippet:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
a=np.random.uniform(1,100,100)
b=range(1,101)
plt.hist(a)

Chapter 3
[ 235 ]
The output that we get is not what we expected. It doesn't have the same probability 
for all the numbers, as seen in the following output:
Fig. 3.13: Histogram of 100 random numbers between 1 and 100 following uniform distribution
The reason for this is that 100 is a very small number, given the range (1-100), 
to showcase the property of the uniform distribution. We should try generating 
more random numbers and then see the results. Try generating around a million 
(1,000,000) numbers by changing the parameter in the uniform function, and then 
see the results of the preceding code snippet. 

Data Wrangling
[ 236 ]
It should look something like the following:
Fig. 3.14: The kind of plot expected for uniform distribution, all the numbers occur with  
the same frequency/probability
If you observe the preceding plot properly, each bin that contains 10 numbers occurs 
roughly with a frequency of 100,000 (and hence a probability of 100000/1000000=1/10). 
This means that each number occurs with a probability of 1/10*1/10=1/100, which is 
equal to the probability that we would have expected from a set of numbers following 
the uniform distribution between 1 and 100 (1/(100-1)=1/99).
Normal distribution
Normal distribution is the most common form of probability distribution arising 
from everyday real-life situations. Thus, the exam score distribution of students in 
a class would roughly follow the normal distribution as would the heights of the 
students in the class. An interesting behavior of all the probability distributions 
is that they tend to follow/align to a normal distribution as the sample size of the 
numbers increase. In a sense, one can say that a normal distribution is the most 
ubiquitous and versatile probability distribution around.
The parameters that define a normal distribution are the mean and standard 
deviation. A normal distribution with a 0 mean and 1 standard deviation is called 
a standard normal distribution. The randn function of the random method is used 
to generate random numbers following a normal distribution. It returns random 
numbers following a standard normal distribution.

Chapter 3
[ 237 ]
To generate 100 such numbers, one simply writes the following:
import numpy as np
a=np.random.randn(100)
To take a look at how random these values actually are, let's plot them against a list 
of integers:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
a=np.random.randn(100)
b=range(1,101)
plt.plot(b,a)
The output looks something like the following image. The numbers are  
visibly random.
Fig. 3.15: A plot of 100 random numbers following normal distribution

Data Wrangling
[ 238 ]
One can pass a list defining the shape of the expected array. If one passes, let's say, 
(2,4) as the input, one would get a 2 x 4 array of numbers following a standard 
normal distribution:
import numpy as np
a=np.random.randn(2,4)
If no numbers are specified, it generates a single random number from the standard 
normal distribution.
To get numbers following normal distributions (with mean and standard deviation 
other than 0 and 1, let's say, mean 1.5 and standard deviation 2.5), one can write 
something like the following:
import numpy as np
a=2.5*np.random.randn(100)+1.5
The preceding calculation holds because the standard normal distribution S is 
created from a normal distribution X, with mean μ and standard deviation σ,  
using the following formula:
(
) /
S
X
µ
σ
=
−
Let's generate enough random numbers following a standard normal distribution 
and plot them to see whether they follow the shape of a standard normal distribution 
(a bell curve). This can be done using the following code snippet:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
a=np.random.randn(100000)
b=range(1,101)
plt.hist(a)

Chapter 3
[ 239 ]
The output would look something like this, which roughly looks like a bell curve  
(if one joins the top points of all the bins to form a curvilinear line):
Fig. 3.16: Histogram of 100000 random numbers following standard normal distribution.
Using the Monte-Carlo simulation to find the 
value of pi
Till now, we have been learning about various ways to generate random numbers. 
Let's now see an application of random numbers. In this section, we will use random 
numbers to run something called Monte-Carlo simulations to calculate the value  
of pi. These simulations are based on repeated random sampling or the generation  
of numbers.

Data Wrangling
[ 240 ]
Geometry and mathematics behind the calculation 
of pi
Consider a circle of radius r unit circumscribed inside a square of side 2r units such 
that the circle's diameter and the square's sides have the same dimensions:
2r
r
Fig. 3.17: A circle of radius r circumscribed in a square of side 2r
What is the probability that a point chosen at random would lie inside the circle? 
This probability would be given by the following formulae:
(
)
(
)
/
/ 2
2
/ 4
Prob point lying insidethecircle
Areaof circle Areaof square
p
pi r r
r
r
p
pi
=
=
∗∗
∗
=
Thus, we find out that the probability of a point lying inside the circle is pi/4. The 
purpose of the simulation is to calculate this probability and use this to estimate the 
value of pi. The following are the steps to be implemented to run this simulation:
1.	 Generate points with both x and y coordinates lying between 0 and 1.
2.	 Calculate x*x + y*y. If it is less than 1, it lies inside the circle. If it is greater 
than 1, it lies outside the circle.

Chapter 3
[ 241 ]
3.	 Calculate the total number of points that lie inside the circle. Divide it by the 
total number of points generated to get the probability of a point lying inside 
the circle.
4.	 Use this probability to calculate the value of pi.
5.	 Repeat the process for a sufficient number of times, say, 1,000 times and 
generate 1,000 different values of pi.
6.	 Take an average of all the 1,000 values of pi to arrive at the final value of pi.
Let's see how one can implement these steps in Python. The following code snippet 
would do just this:
pi_avg=0
pi_value_list=[]
for i in range(100):
    value=0
    x=np.random.uniform(0,1,1000).tolist()
    y=np.random.uniform(0,1,1000).tolist()
    for j in range(1000):
            z=np.sqrt(x[j]*x[j]+y[j]*y[j])
            if z<=1:
                value+=1
    float_value=float(value)
    pi_value=float_value*4/1000
    pi_value_list.append(pi_value)
    pi_avg+=pi_value
    
pi=pi_avg/100
print pi
ind=range(1,101)
fig=plt.plot(ind,pi_value_list)
fig

Data Wrangling
[ 242 ]
The preceding snippet generates 1,000 random points to calculate the probability  
of a point lying inside the circle and then repeats this process 100 times to get at the 
final averaged value of pi. These 100 values of pi have been plotted and they look  
as follows:
Fig. 3.18: Values of pi over 100 simulations of 1000 points each
The final averaged value of pi comes out to be 3.14584 in this run. As we increase 
the number of runs, the accuracy increases. One can easily wrap the preceding 
snippet in a function and pass the number of runs as an input for the easy 
comparison of pi values as an increasing number of runs are passed to this  
function. The following code snippet is a function to do just this:
def pi_run(nums,loops):
    pi_avg=0
    pi_value_list=[]
    for i in range(loops):
        value=0
        x=np.random.uniform(0,1,nums).tolist()
        y=np.random.uniform(0,1,nums).tolist()
        for j in range(nums):
            z=np.sqrt(x[j]*x[j]+y[j]*y[j])
            if z<=1:

Chapter 3
[ 243 ]
                value+=1
        float_value=float(value)
        pi_value=float_value*4/nums
        pi_value_list.append(pi_value)
        pi_avg+=pi_value
    
    pi=pi_avg/loops
    ind=range(1,loops+1)
    fig=plt.plot(ind,pi_value_list)
    return (pi,fig)
To call this function, write pi_run(1000,100), and it should give you a similar 
result as was given previously with the hardcoded numbers. This function would 
return both the averaged value of pi as well as the plot.
Generating a dummy data frame
One very important use of generating random numbers is to create a dummy  
data frame, which will be used extensively in this book to illustrate concepts  
and examples.
The basic concept of this is that the array/list of random numbers generated through 
the various methods described in the previous sections can be passed as the columns 
of a data frame. The column names and their descriptions are passed as the keys and 
values of a dictionary.
Let's see an example where a dummy data frame contains two columns, A and B, 
which have 10 random numbers following a standard normal distribution and 
normal distribution, respectively.
To create such a data frame, one can run the following code snippet:
import pandas as pd
d=pd.DataFrame({'A':np.random.randn(10),'B':2.5*np.random.
randn(10)+1.5})
d

Data Wrangling
[ 244 ]
The following screenshot is the output of the code:
Fig. 3.19: A dummy data frame containing 2 columns – one having numbers following standard  
normal distribution, the second having random numbers following normal distribution with mean 1.5 and 
standard deviation 2.5
Categorical/string variables can also be passed as a list to be part of a dummy  
data frame. Let's go back to our example of the Customer Churn Model data and 
use the column names as the list to be passed. This can be done as described in the 
following snippet:
import pandas as pd
data = pd.read_csv('E:/Personal/Learning/Datasets/Book/Customer Churn 
Model.txt') 
column_list=data.columns.values.tolist()
a=len(column_list)
d=pd.DataFrame({'Column_Name':column_list,'A':np.random.
randn(a),'B':2.5*np.random.randn(a)+1.5})
d

Chapter 3
[ 245 ]
The output of the preceding snippet is as follows:
Fig. 3.20: Another dummy data frame. Similar to the one above but with one extra column which has column  
names of the data data frame
The index can also be passed as one of the parameters of this function. By default,  
it gives a range of numbers starting from 0 as the index. If we want something  
else as the index, we can specify it in the index parameter as shown in the  
following example:
import pandas as pd
d=pd.DataFrame({'A':np.random.randn(10),'B':2.5*np.random.randn(10)+1.
5},index=range(10,20))
d
The output of the preceding code looks like the following:
Fig. 3.21: Passing indices to the dummy data frame

Data Wrangling
[ 246 ]
Grouping the data – aggregation, 
filtering, and transformation
In this section, you will learn how to aggregate data over categorical variables. 
This is a very common practice when the data consists of categorical variables. This 
analysis enables us to conduct a category-wise analysis and take further decisions 
regarding the modelling.
To illustrate the concepts of grouping and aggregating data better, let's create a 
simple dummy data frame that has a rich mix of both numerical and categorical 
variables. Let's use whatever we have explored till now about random numbers to 
create this data frame, as shown in the following snippet:
import numpy as np
import pandas as pd
a=['Male','Female']
b=['Rich','Poor','Middle Class']
gender=[]
seb=[]
for i in range(1,101):
    gender.append(np.random.choice(a))
    seb.append(np.random.choice(b))
height=30*np.random.randn(100)+155
weight=20*np.random.randn(100)+60
age=10*np.random.randn(100)+35
income=1500*np.random.randn(100)+15000
df=pd.DataFrame({'Gender':gender,'Height':height,'Weight':weight,'Age'
:age,'Income':income,'Socio-Eco':seb})
df.head()
The output data frame df looks something as follows:
Fig. 3.22: The resulting dummy data frame df containing 6 columns

Chapter 3
[ 247 ]
As we can see from the preceding code snippet, the shape of the data frame  
is 100x6.
Grouping can be done over a categorical variable using the groupby function. The 
column name of the categorical variable needs to be specified for this. Suppose that 
we wish to group the data frame based on the Gender variable. This can be done by 
writing the following:
df.groupby('Gender')
If you run the preceding snippet on your IDE, you will get the following output 
indicating that a groupby object has been created:
Fig. 3.23: Prompt showing that the groupby object has been created
The groupby function doesn't split the original data frame into several groups, 
instead it creates a groupby object that has two attributes, which are name and group.
These attributes can be accessed by following the name of the groupby object 
with '.', followed by the name of the attribute. For example, to access the group 
attribute, one can write the following:
grouped = df.groupby('Gender')
grouped.groups
The following is the output:
Fig. 3.24: Two groups based on gender

Data Wrangling
[ 248 ]
The numbers indicate the row numbers that belong to that particular group.
One important feature of these attributes is that they are iterable, and the same 
operation can be applied to each group just by looping. This comes in very handy 
when the number of groups are large and one needs results of the operation 
separately for each group.
Let's perform a simple operation to illustrate this. Let's try to print the name and 
groups in the groupby object that we just created. This can be done as follows:
grouped=df.groupby('Gender')
for names,groups in grouped:
    print names
    print groups
This prints the name of the group followed by the entire data for this group.  
The output looks something like the following:
Fig. 3.25.1: Name and the data in the group with gender female
Here is the second group as a part of the output:
Fig. 3.25.2: Name and the data in the group with gender male

Chapter 3
[ 249 ]
A single group can be selected by writing the following:
grouped.get_group('Female')
This would generate only the first of the two groups, as shown in the  
preceding screenshot.
A data frame can be grouped over more than one categorical variable as well. As in 
this case, the data frame can be grouped over both Gender and Soci-Eco by writing 
something like the following:
grouped=df.groupby(['Gender','Socio-Eco'])
This should create six groups from a combination of two categories of Gender and 
three categories of the Socio-Eco variable. This can be checked by checking the 
length of the groupby object as follows:
len(grouped)
It indeed returns six. To look at how these groups look, let's run the same iteration 
on the group attributes as we did earlier:
grouped=df.groupby(['Gender','Socio-Eco'])
for names,groups in grouped:
    print names
    print groups
The code gives six groups' names and their entire data as the output. There would be 
six of such groups in total.
The first group looks like the following:
Fig. 3.26.1: Name and the data in the group with gender female and Socio_Eco Middle Class

Data Wrangling
[ 250 ]
The second group looks like the following:
Fig. 3.26.2: Name and the data in the group with gender female and Socio_Eco Middle Class
Aggregation
There are various aggregations that are possible on a data frame, such as sum, mean, 
describe, size, and so on. The aggregation basically means applying a function to 
all the groups all at once and getting a result from that particular group.
Let's see the sum function. We just need to write the following code snippet to see 
how it works:
grouped=df.groupby(['Gender','Socio-Eco'])
grouped.sum()
We gets the following table as the result:
Fig. 3.27: Sum of each column for different groups
To get the number of rows in each group (or calculate the size of each group), we can 
write something similar to the following code snippet:
grouped=df.groupby(['Gender','Socio-Eco'])
grouped.size()

Chapter 3
[ 251 ]
This results in a table, as shown in the following screenshot:
Fig. 3.28: Size of each group
One can use the describe function to get the summary statistics for each group 
separately. The syntax is exactly the same as it is for the earlier two functions:
grouped=df.groupby(['Gender','Socio-Eco'])
grouped.describe()
This output looks similar to the following table:
Fig. 3.29: All the summary statistics of each column for different groups

Data Wrangling
[ 252 ]
The groupby objects behave similar to an individual data frame, in the sense that one 
can select columns from these groupby objects just as we do from the data frames:
grouped=df.groupby(['Gender','Socio-Eco'])
grouped_income=grouped['Income']
One can apply different functions to different columns. The aggregate method used 
to do this is shown in the following snippet. With the following snippet, one can 
calculate sum of Income, mean of Age, and standard deviation of Height, as shown:
grouped=df.groupby(['Gender','Socio-Eco'])
grouped.aggregate({'Income':np.sum,'Age':np.mean,'Height':np.std})
The output of the preceding snippet looks similar to the following table:
Fig. 3.30: Selected summary statistics of selected columns for different groups
We can also define a function using the lambda method of defining a calculation in 
Python. Suppose you don't want the mean of age but the ratio of mean and standard 
deviation for height. You can define the formula for this ratio using the lambda 
method, illustrated as follows:
grouped=df.groupby(['Gender','Socio-Eco'])
grouped.aggregate({'Age':np.mean,'Height':lambda x:np.mean(x)/
np.std(x)})

Chapter 3
[ 253 ]
Rather than applying different functions to different columns, one can apply several 
functions to all the columns at the same time, as shown:
grouped.aggregate([np.sum, np.mean, np.std])
The output of the code snippet contains the result of all the three functions applied 
on all the columns of the groupby object, as seen in the following screenshot:
Fig. 3.31: More than one selected summary statistics of selected columns for different groups
Filtering
One important operation that can be applied on the groupby objects is filter. We 
can filter elements based on the properties of groups. Suppose we want to choose 
elements from the Age column that are a part of the group wherein the sum of Age is 
greater than 700. This filtering can be done by writing the following snippet:
grouped['Age'].filter(lambda x:x.sum()>700)

Data Wrangling
[ 254 ]
The output contains the row numbers that are part of the group where the sum of 
Age is greater than 700. The output is, as follows:
Fig. 3.32: The rows left after filtering it for elements, which are part of groups, where the sum of  
ages is greater than 700
Transformation
One can use the transform method to mathematically transform all the elements in 
a numerical column. Suppose, we wish to calculate the standard normal values for 
all the elements in the numerical columns of our data frame; this can be done in a 
manner as shown:
zscore = lambda x: (x - x.mean()) / x.std()
grouped.transform(zscore)

Chapter 3
[ 255 ]
The output contains standard normal values for all the numerical columns in the 
data frame, as shown in the following screenshot:
Fig. 3.33: Result of applying a lambda defined function on the columns of groups
The transform method comes in handy in a lot of situations. For example, it can be 
used to fill the missing values with the mean of the non-missing values, as shown:
f = lambda x: x.fillna(x.mean()
grouped.transform(f)
Miscellaneous operations
In many situations, one needs to select the nth row of each group of a groupby object, 
most often the first and the last row. This can be easily done once the groupby object 
is created. Let's see how:
•	
The first row of each group can be selected by writing the following  
code snippet:
grouped.head(1)
•	
While the last row of each group can be selected by writing the following 
code snippet:
grouped.tail(1)

Data Wrangling
[ 256 ]
The result of the former, is as shown:
Fig. 3.34: First few rows of the grouped element
In general, we can use the nth function to get the nth row from a group,  
as illustrated:
grouped=df.groupby('Gender')
grouped.nth(1)
This gives the following result:
Fig. 3.35: First rows of each group
One can use any number (of course, less than the number of rows in each group)  
as the argument for the nth function.
It is always a good practice to sort the data frame for the relevant columns  
before creating the groupby object from the data frame. Suppose, you want to  
look at the youngest male and female members of this data frame. 

Chapter 3
[ 257 ]
This can be done by sorting the data frame, creating a groupby object, and then 
taking the first element of each group:
df1=df.sort(['Age','Income'])
grouped=df1.groupby('Gender')
grouped.head(1)
The output has two rows containing the details of the two youngest members from 
the two groups:
Fig. 3.36: Sorting by the age column before grouping by gender and then selecting the first row from each group 
can give you the oldest/youngest guy in the group
The oldest members can be identified in the same way by typing grouped.tail(1).
Random sampling – splitting a dataset in 
training and testing datasets
Splitting the dataset in training and testing the datasets is one operation every 
predictive modeller has to perform before applying the model, irrespective of the 
kind of data in hand or the predictive model being applied. Generally, a dataset is 
split into training and testing datasets. The following is a description of the two types 
of datasets:
•	
The training dataset is the one on which the model is built. This is the one 
on which the calculations are performed and the model equations and 
parameters are created.
•	
The testing dataset is used to check the accuracy of the model. The model 
equations and parameters are used to calculate the output based on the 
inputs from the testing datasets. These outputs are used to compare the 
model efficiency in the light of the actuals present in the testing dataset.

Data Wrangling
[ 258 ]
This will become clearer from the following image:
Training data
Testing data
Model = M = f(X1, X2, X3)
Actual
output
Model
output
Compare Y and M 
X1
X2
X3
Y
X1
X2
X3
Y
M
Fig. 3.37: Concept of sampling: Training and Testing data
Generally, the training and testing datasets are split in the ratio of 75:25 or 80:20. 
There are various ways to split the data into two halves. The crudest way that comes 
to mind is taking the first 75/80 percent rows as the training dataset and the rest as 
the testing dataset, or taking the first 25/20 percent rows as the testing and the rest 
as the training dataset. However, the problem with this approach is that it might 
bias the two datasets for a variety of reasons. The earlier rows might come from a 
different source or were observed during different scenarios. These situations might 
bias the model results from the two datasets. The rows should be chosen to avoid 
this bias. The most effective way to do that is to select the rows at random. Let us  
see a few methods to divide a dataset into training and testing datasets.
One way is to create as many standard normal random numbers, as there are  
rows in the dataset and then filter them for being smaller than a certain value.  
This filter condition is then used to partition the data in two parts. Let us see  
how it can be done.

Chapter 3
[ 259 ]
Method 1 – using the Customer Churn Model
Let us use the same Customer Churn Model data that we have been using 
frequently. Let us go ahead and import it, as shown:
import pandas as pd
data = pd.read_csv('E:/Personal/Learning/Datasets/Book/Customer Churn 
Model.txt')
len(data)
There are 3333 rows in the dataset. Next, we will generate random numbers and 
create a filter on which to partition the data:
a=np.random.randn(len(data))
check=a<0.8
training=data[check]
testing=data[~check]
The rows where the value of the random number is less than 0.8 becomes a part of 
the training variable, while the one with a value greater than 0.8 becomes a part of 
the testing dataset.
Let us check the lengths of the two datasets to see in what ratio the dataset has been 
divided. A 75:25 split between training and testing datasets would be ideal:
len(training)
len(testing)
The length of training dataset is 2635 while that of the testing dataset is 698; thus, 
resulting in a split very close to 75:25.
Method 2 – using sklearn
Very soon we will be introduced to a very powerful Python library used extensively 
for the purpose of modelling, scikit-learn or sklearn. This sklearn library has 
inbuilt methods to split a dataset in a training and testing dataset. Let's have a look at 
the procedure:
from sklearn.cross_validation import train_test_split
train, test = train_test_split(data, test_size = 0.2)
The test size specifies the size of the testing dataset: 0.2 means that 20 percent of the 
rows of the dataset should go to testing and the remaining 80 percent to training. 
If we check the length of these two (train and test), we can confirm that the split is 
indeed 80-20 percent.

Data Wrangling
[ 260 ]
Method 3 – using the shuffle function
Another method involves using the shuffle function in the random method.  
The data is read in line by line, which are shuffled randomly and then assigned  
to training and testing datasets in designated proportions, as shown:
import numpy as np
with open('E:/Personal/Learning/Datasets/Book/Customer Churn Model.
txt','rb') as f:
    data=f.read().split('\n')
np.random.shuffle(data)
train_data = data[:3*len(data)/4]
test_data = data[len(data)/4:]
In some cases, mostly during data science competitions like Kaggle, we would be 
provided with separate training and testing datasets to start with.
Concatenating and appending data
All the required information to build a model doesn't always come from a single 
table or data source. In many cases, two datasets need to be joined/merged to get 
more information (read new column/variable). Sometimes, small datasets need to be 
appended together to make a big dataset which contains the complete picture. Thus, 
merging and appending are important components of an analyst's armor.
Let's learn each of these methods one by one. For illustrating these methods, we 
will be using a lot of new interesting datasets. The one we are going to use first is a 
dataset about the mineral contents of wine; we will have separate datasets for red 
and white wine. Each sample represents a different sample of red or white wine.
Let us import this dataset and have a look at it. The delimiter for this dataset is ;  
(a semi-colon), which needs to be taken care of:
import pandas as pd
data1=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Merge and Join/winequality-red.csv',sep=';')
data1.head()

Chapter 3
[ 261 ]
The output of this input snippet is similar to the following screenshot:
Fig. 3.38: First few entries of the wine quality-red dataset
The column names are as follows:
data1.columns.values
Fig. 3.39: Column names of the wine quality-red dataset
The size of the dataset is can be found out using the following snippet:
data1.shape
The output is 1599x12 implying that the dataset has 1599 rows.
Let us import the second dataset which is very similar to the preceding dataset 
except that the data points are collected for white wine:
import pandas as pd
data2=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Merge and Join/winequality-white.csv',sep=';')
data2.head()
The output of this input snippet looks similar to the following screenshot:
Fig. 3.40: First few entries of the winequality-white dataset

Data Wrangling
[ 262 ]
As we can see, this dataset looks very similar to the preceding dataset. Let us  
confirm this by getting the column names for this dataset. They should be the  
same as the preceding array of column names:
Fig. 3.41: Column names of the winequality-white dataset
The size of the dataset is, as follows:
data2.shape
4898x12, this means that the dataset has 4898 rows.
So, we can see that the data1 and data2 are very similar (in terms of column names 
and column types) except the row numbers in the two datasets. These are ideal 
circumstances to append two datasets along the horizontal axis (axis=0).
In Python, the horizontal axis is denoted by axis=0 and the 
vertical axis is denoted by axis=1.
Let us append these two datasets along axis=0. This can be done using the concat 
method of pandas library. After appending the datasets, the row numbers of the final 
dataset should be the same as the row numbers of both the datasets.
This can be accomplished as follows:
wine_total=pd.concat([data1,data2],axis=0)
Let us check the number of rows of the appended dataset wine_total:
wine_total.shape
The output is 6497x12. It indicates that the final appended dataset has 6497 
(6497=1599+4898) rows. One can see that the row numbers in the appended  
dataset is the sum of row numbers of the individual datasets.

Chapter 3
[ 263 ]
Let us have a look at the final dataset just to ensure everything looks fine. While 
appending over axis=0, the two datasets are just stacked over one another. In this 
case, data1 will be stacked over data2 dataset. So, the first few rows of the final 
dataset wine_total will look similar to the first few rows of the first dataset data1. 
Let us check that:
wine_total.head()
The output looks similar to the following screenshot:
Fig. 3.42: First few entries of the final dataset obtained from appending data1 and data2
The preceding output is the same as the first few rows of the data1.
This concat method can be used to scramble data that is taken a few rows from here 
and there and stacking them over one another. The concat method takes more than 
two datasets also as an argument. The datasets are stacked over one another in order 
of appearance. If the datasets are data1, data2, and data3, in that order, then data1 
will be stacked over data2 which will be stacked over data3.
Let us look at an example of such scrambling. We will use the data1 dataset (coming 
from winequality-red.csv) and take 50 rows from head, middle, and tail to create 
three different data frames. These data frames will be then stacked over one another 
to create a final dataset:
data1_head=data1.head(50)
data1_middle=data1[500:550]
data1_tail=data.tail(50)
wine_scramble=pd.concat([data1_middle,data1_head,data1_tail],axis=0)
wine_scramble
The output dataset will contain 150 rows, as confirmed by the following snippet:
wine_scramble.shape
This returns 150x12 as the output.

Data Wrangling
[ 264 ]
The output dataset wine_scramble looks similar to the following screenshot:
Fig. 3.43: First few rows of the scrambled data frame with rows from the data1_middle at the top
Since, the order of the appended dataset is data1_middle, data1_head, data1_tail, 
the rows contained in the data1_middle come at the top followed by the data1_
head and data1_tail rows.
If you change the order of the stacking, the view of the appended dataset will 
change. Let's try that:
data1_head=data1.head(50)
data1_middle=data1[500:550]
data1_tail=data.tail(50)
wine_scramble=pd.concat([data1_head,data1_middle,data1_tail],axis=0)
wine_scramble
The output looks similar to the following screenshot, wherein, as expected the rows 
in the data1_head appear at the top:
Fig. 3.44: First few rows of the scrambled data frame with rows from the data1_head at the top
Let's see another scenario where the concat function comes as a savior. Suppose, 
you have to deal with the kind of data that comes in several files containing similar 
data. One example of such a scenario is the daily weather data of a city where the 
same metrics are tracked everyday but the data for each day is stored in a separate 
file. Many a time, when we do analysis of such files, we are required to append them 
into a single consolidated file before running any analyses or models.

Chapter 3
[ 265 ]
I have curated some data that has these kind of properties to illustrate the method of 
consolidation into a single file. Navigate to the Chapter 3 folder in the Google Drive 
and then to the lotofdata folder. You will see 332 CSV files, each named with its 
serial number. Each CSV file contains pollutant measure levels at different points of 
time in a single day. Each CSV file represents a day worth of pollutant measure data.
Let us go ahead and import the first file and have a look at it. Looking at one file 
will be enough, as the others will be very similar to this (exactly similar except the 
number of rows and the data points):
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Merge and Join/lotofdata/001.csv')
data.head()
The one feature of this data is that it is very sparse and it contains a lot of missing 
values that would be visible once we look at the output of the preceding code 
snippet. The sparseness doesn't affect the analyses in this case because the dataset 
has sufficient rows with non-missing values. Even 100 rows with non-missing values 
should give us a good picture of the pollutant levels for a day. However, for the 
same reason, appending such a dataset becomes all the more important so that we 
have a significant amount of data with the non-missing values for our analyses.
Let us look at the first CSV file of the lot and the output of the preceding snippet:
Fig. 3.45: First few entries of the first file out of 332 CSV files
The size of the dataset is 1461x4 indicating that there are 1,461 rows and four 
columns. The name of the columns are Date, sulfate, nitrate, and ID. ID would be 
1 for the first dataset, 2 for the 2nd, and so on. The number of rows in the other CSV 
files should be in the same range while the number of rows with non-missing values 
might vary.

Data Wrangling
[ 266 ]
Let us now move towards our goal of this discussion that is to demonstrate how to 
consolidate such small and similar files in a single file. To be able to do so, one needs 
to do the following in that sequence:
1.	 Import the first file.
2.	 Loop through all the files.
3.	 Import them one by one.
4.	 Append them to the first file.
5.	 Repeat the loop.
Let us now look at the code snippet, which will achieve this:
import pandas as pd
filepath='E:/Personal/Learning/Predictive Modeling Book/Book Datasets/
Merge and Join/lotofdata'
data_final=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/
Book Datasets/Merge and Join/lotofdata/001.csv')
for i in range(1,333):
    if i<10:
        filename='0'+'0'+str(i)+'.csv'
    if 10<=i<100:
        filename='0'+str(i)+'.csv'
    if i>=100:
        filename=str(i)+'.csv'
        
    file=filepath+'/'+filename
    data=pd.read_csv(file)
    
    data_final=pd.concat([data_final,data],axis=0)
In the code snippet, the read_csv is taking a file variable that consists of filepath 
and filename variables. The if condition takes care of the changing filenames (three 
conditions arise – first, when filename contains a single non-zero digit; second, when 
the filename contains two non-zero digits, and third, when the filename contains all 
the three non-zero digits).
The first file is imported and named as data_final. The subsequent files are 
imported and appended to data_final. The for loop runs over all the 332 files 
wherein the importing and appending of the files occur.
The size of the data_final data frame is 773548x4 rows, indicating that it has 
7,73,548 rows because it contains the rows from all the 332 files.

Chapter 3
[ 267 ]
If one looks at the last rows of the data_final data frame, one can confirm that all 
the files have been appended if the ID column contains 332 as value. This means that 
the last few rows come from the 332nd file.
Fig. 3.46: Last few entries of the data_final data frame. They have ID as 332 indicating that  
they come from the 332nd CSV file.
The ID column indeed contains 332 observations, confirming that all the 332 files 
have been successfully appended.
Another way to confirm whether all the rows from all the files have been successfully 
appended or not, one can sum up the row numbers of each file and compare them 
to the row numbers of the final appended data frame. They should be equal if they 
have been appended successfully.
Let us check. We can use the same code as the preceding one, for this process with 
some minor tweaks. Let us see how:
import pandas as pd
filepath='E:/Personal/Learning/Predictive Modeling Book/Book Datasets/
Merge and Join/lotofdata'
data_final=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/
Book Datasets/Merge and Join/lotofdata/001.csv')
data_final_size=len(data_final)
for i in range(1,333):
    if i<10:
        filename='0'+'0'+str(i)+'.csv'
    if 10<=i<100:
        filename='0'+str(i)+'.csv'
    if i>=100:
        filename=str(i)+'.csv'
        
    file=filepath+'/'+filename
    data=pd.read_csv(file)

Data Wrangling
[ 268 ]
    data_final_size+=len(data)
    
    
    data_final=pd.concat([data_final,data],axis=0)
print data_final_size
Here, we are summing-up the row numbers of all the files (in the line highlighted) 
and the summed-up number is printed in the last line. The output is 773,548; it 
confirms that the final data frame has the same number of rows as the sum of  
rows in all the files.
Merging/joining datasets
Merging or joining is a mission critical step for predictive modelling and, more often 
than not, while working on actual problems, an analyst will be required to do it. 
The readers who are familiar with relational databases know how there are multiple 
tables connected by a common key column across which the required columns are 
scattered. There can be instances where two tables are joined by more than one key 
column. The merges and joins in Python are very similar to a table merge/join in a 
relational database except that it doesn't happen in a database but rather on the local 
computer and that these are not tables, rather data frames in pandas. For people 
familiar with Excel, you can find similarity with the VLOOKUP function in the sense 
that both are used to get an extra column of information from a sheet/table joined by 
a key column.
There are various ways in which two tables/data frames can be merged/joined. The 
most commonly used ones are Inner Join, Left Join, Right Join, and so on. We will go 
in to detail and understand what each of these mean. But before that, let's go ahead 
and perform a simple merge to get a feel of how it is done.
We will be using a different dataset to illustrate the concept of merge and join. 
These datasets can be found in the Google Drive folder in Merge and the Join/
Medals folder. The main dataset Medals.csv contains details of medals won by 
individual players at different Olympic events. The two subsidiary datasets contain 
details of the nationality and sports of the individual player. What if we want to 
see the nationality or sport played by the player together with all the other medal 
information for each player? The answer is to merge both the datasets and to get the 
relevant columns. In data science parlance, merging, joining, and mapping are used 
synonymously; although, there are minor technical differences.

Chapter 3
[ 269 ]
Let us import all of them and have a cursory look at them:
import pandas as pd
data_main=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/
Book Datasets/Merge and Join/Medals/Medals.csv')
data_main.head()
The Medals.csv looks similar to the following screenshot:
Fig. 3.47: First few entries of the Medals dataset
As we can see, this is the information about the Olympic Year in which the  
medals were won, details of how many Gold, Silver, and Bronze medals were won 
and the Age of the player. There are 8,618 rows in the dataset. One more thing one 
might be interested to know about this dataset is how many unique athletes are there 
in the dataset, which will come in handy later when we learn and apply different 
kinds of joins:
a=data_main['Athlete'].unique().tolist()
len(a)
The output of this snippet is 6956, which means that there are many athletes for 
whom we have records in the datasets. The other entries come because many  
athletes may have participated in more than one Olympics.
Let us now import the Athelete Country Map.csv and have a look at it:
country_map=pd.read_csv('E:/Personal/Learning/Predictive Modeling 
Book/Book Datasets/Merge and Join/Medals/Athelete_Country_Map.csv')
country_map.head()

Data Wrangling
[ 270 ]
The output data frame looks similar to the following screenshot, with two columns: 
Athlete and Country:
Fig. 3.48: First few entries of the Athelete_Country_Map dataset
There are 6,970 rows in this dataset. If you try to find out the unique number of 
athletes in this data frame, it will still be 6,956. The 14 extra rows come from the fact 
that some players have played for two countries in different Olympics and have won 
medals. Search for Aleksandar Ciric and you will find that he has played for both 
Serbia and Serbia and Montenegro.
(Disclaimer: This might not be the actual case and this might be an 
issue with the mapping file, which can be taken care of by removing 
duplicate values, as we would show later in this chapter).
You can do this by using the following code snippet:
country_map[country_map['Athlete']=='Aleksandar Ciric']
Fig. 3.49: Subsetting the country_map data frame for Aleksandar Ciric
Let us finally import the Athelete Sports Map.csv and have a look at it:
sports_map=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/
Book Datasets/Merge and Join/Medals/Athelete_Sports_Map.csv')
sports_map.head()

Chapter 3
[ 271 ]
The sports_map data frame looks as shown in the following screenshot:
Fig. 3.50: First few entries of the Athelete_Sports_Map dataset
There are 6,975 rows in this dataset because, yes you guessed it right, there are very 
few athletes in this mapping data frame who have played more than one game and 
have won medals. Watch out for athletes, such as Chen Jing, Richard Thompson 
and Matt Ryan who have played more than one game.
This can be done by writing a code, such as the following snippet:
sports_map[(sports_map['Athlete']=='Chen Jing') | (sports_
map['Athlete']=='Richard Thompson') | (sports_map['Athlete']=='Matt 
Ryan')]
The output looks similar to the following screenshot:
Fig. 3.51: Subsetting the sports_map data frame for athletes Richard Thompson and Matt Ryan

Data Wrangling
[ 272 ]
Let's now merge the data_main and country_map data frames to get the country for 
all the athletes. There is a merge method in pandas, which facilitates this:
import pandas as pd
merged=pd.merge(left=data_main,right=country_map,left_
on='Athlete',right_on='Athlete')
merged.head()
The output looks, as follows. It has a country column as expected:
Fig. 3.52: First few entries of the merged data frame. It has a country column.
The length of the merged data frame is 8,657, which is more than the total number 
of rows (8,618) in the data_main data frame. This is because when we join these two 
data frames without any specified conditions, an inner join is performed wherein the 
join happens based on the common key-values present in both the data frames. Also, 
we saw that some athletes have played for two countries and the entries for such 
athletes will be duplicated for such athletes. If you look at Aleksandar Ciric in the 
merged data frame, you will find something similar to this:
merged[merged['Athlete']=='Aleksandar Ciric']
Fig. 3.53 Subsetting the merged data frame for athlete Aleksandar Ciric
The problem is not with the type of join but with the kind of mapping file we have. 
This mapping file is one-many and hence the number increases because for each key 
multiple rows are created in such a case.

Chapter 3
[ 273 ]
To rectify this issue, one can remove the duplicate entries from the country_map 
data frame and then perform the merge with data_main. Let's do that. This can be 
done using the drop_duplicates method, as shown:
country_map_dp=country_map.drop_duplicates(subset='Athlete')
The length of the country_map_dp is 6,956 rows, which is the same as the number of 
unique athletes. Let us now merge this with data_main.
merged_dp=pd.merge(left=data_main,right=country_map_dp,left_
on='Athlete',right_on='Athlete')
len(merged_dp)
The number of rows in the merged_dp is indeed 8,618, which is the actual number of 
rows in the data_main.
The next step is to merge sports_map with the merged_dp to get the country and 
sports along with other details in the same data frame.
We have seen similar issue of increase in the number of rows for sports_map, as was 
the case for country_map data frame. To take care of that, let's remove the duplicates 
from the sports_map before merging it with merged_dp:
sports_map_dp=sports_map.drop_duplicates(subset='Athlete')
len(sports_map_dp)
The length of the sports_map_dp is 6,956, which is the same as the number of rows 
in the data_main data frame, as expected.
The next step is to merge this with the merge_pd data frame to get the sports played 
by the athlete in the final merged table:
merged_final=pd.merge(left=merged_dp,right=sports_map_dp,left_
on='Athlete',right_on='Athlete')
merged_final.head()
Fig. 3.54: First few entries of the merged_final dataset. The duplicates from country_map were  
deleted before the merge

Data Wrangling
[ 274 ]
As we can see, the Sport column is present in the merged_final data frame after the 
merge. The merged_final data frame has 8,618 rows as expected.
Let us now look at various kinds of merge/joins that we can apply to two data 
frames. Although you would come across many kinds of joins in different texts, it is 
sufficient to know the concept behind the three of them—Inner Join, Left Join, and 
Right Join. If you consider the two tables/data frames as sets, then these joins can be 
well represented by Venn Diagrams.
Inner Join
The characteristics of the Inner Join are as follows:
•	
Returns a data frame containing rows, which have a matching value in both 
the original data frames being merged.
•	
The number of rows will be equal to the minimum of the row numbers of the 
two data frames. If data frame A containing 100 rows is being merged with 
data frame B having 80 rows, the merged data frame will have 80 rows.
•	
The Inner Join can be thought of as an intersection of two sets, as illustrated 
in the following figure:
Inner Join
A
B
Fig. 3.55: Inner Join illustrated via a Venn diagram
Left Join
The characteristics of the Left Join are, as follows:
•	
Returns a data frame containing rows, which contains all the rows from  
the left data frame irrespective of whether it has a match in the right data 
frame or not.

Chapter 3
[ 275 ]
•	
In the final data frame, the rows with no matches in the right data frame will 
return NAs in the columns coming from right data frame.
•	
The number of rows will be equal to the number of rows in the left data 
frame. If data frame A containing 100 rows is being merged with data frame 
B having 80 rows, the merged data frame would have 100 rows.
•	
The Left Join can be thought of as the set containing the entire left data frame, 
as illustrated in the following figure:
Left Join
A
B
Fig. 3.56: Left Join illustrated via a Venn Diagram
Right Join
The characteristics of the Right Join are as follows:
•	
Returns a data frame containing rows, which contains all the rows from the 
right data frame irrespective of whether it has a match in the left data frame 
or not.
•	
In the final data frame, the rows with no matches in the left data frame will 
return NAs in the columns coming from left data frame.
•	
The number of rows will be equal to the number of rows in the left data 
frame. If data frame A containing 100 rows is being merged with data frame 
B having 80 rows, the merged data frame will have 80 rows

Data Wrangling
[ 276 ]
•	
The Right Join can be thought of as the set containing the entire right data 
frame, as illustrated in the following figure:
Right Join
A
B
Fig. 3.57: Right Join illustrated via a Venn diagram
The comparison between join type and set operation is summarized in the  
following table:
Join type
Set operation
Inner Join
Intersection
Left Join
Set A (left data frame)
Right Join
Set B (right data frame)
Outer Join
Union
Let us see some examples of how different kinds of mappings actually work. For 
that, a little data preparation is needed. Currently, both our mapping files contain 
matching entries for all the rows in the actual data frame data_main. So, we can't 
see the effects of different kind of merges. Let's create a country and sports mapping 
file which doesn't have the information for some of the athletes and let's see how 
it reflects in the merged table. This can be done by creating a new data frame that 
doesn't have country/sports information for some of the athletes, as shown in the 
following code:
country_map_dlt=country_map_dp[(country_map_dp['Athlete']<>'Michael 
Phelps') & (country_map_dp['Athlete']<>'Natalie Coughlin') & (country_
map_dp['Athlete']<>'Chen Jing')
                    & (country_map_dp['Athlete']<>'Richard Thompson') 
& (country_map_dp['Athlete']<>'Matt Ryan')]
len(country_map_dlt)

Chapter 3
[ 277 ]
Using this snippet, we have created a country_map_dlt data frame that doesn't have 
country mapping for five athletes, that is Michael Phelps, Natalie Coughlin, Chen 
Jing, Richard Thompson, and Matt Ryan. The length of this data frame is 6,951; it is 
five less than the actual mapping file, indicating that the information for five athletes 
has been removed.
Let's do the same for sports_map as well as the data_main data frame using the 
following snippets:
sports_map_dlt=sports_map_dp[(sports_map_dp['Athlete']<>'Michael 
Phelps') & (sports_map_dp['Athlete']<>'Natalie Coughlin') & (sports_
map_dp['Athlete']<>'Chen Jing')
                    & (sports_map_dp['Athlete']<>'Richard Thompson') & 
(sports_map_dp['Athlete']<>'Matt Ryan')]
len(sports_map_dlt)
data_main_dlt=data_main[(data_main['Athlete']<>'Michael 
Phelps') & (data_main['Athlete']<>'Natalie Coughlin') & (data_
main['Athlete']<>'Chen Jing')
                    & (data_main['Athlete']<>'Richard Thompson') & 
(data_main['Athlete']<>'Matt Ryan')]
len(data_main_dlt)
The length of data_main_dlt becomes 8,605 because the data_main contains 
multiple rows for an athlete.
An example of the Inner Join
One example of Inner join would be to merge data_main data frame with  
country_map_dlt. This can be done using the following snippet:
merged_inner=pd.merge(left=data_main,right=country_map_
dlt,how='inner',left_on='Athlete',right_on='Athlete')
len(merged_inner)
This merge should give us information for the athletes who are present in  
both the data frames. As the country_map_dlt doesn't contain information about 
five athletes present in data_main, these five athletes wouldn't be a part of the 
merged table.
The length of the merged_inner comes out to be 8,605 (similar to data_main_dlt) 
indicating that it doesn't contain information about those five athletes.

Data Wrangling
[ 278 ]
An example of the Left Join
One example of Left Join would be to merge data_main data frame with  
country_map_dlt. This can be done using the following snippet:
merged_left=pd.merge(left=data_main,right=country_map_
dlt,how='left',left_on='Athlete',right_on='Athlete')
len(merged_left)
This merge should give us the information about all the athletes that are present in 
the left data frame (data_main) even if they aren't present in the right data frame 
(country_map_dlt). So, the merged_left data frame should contain 8,618 rows 
(similar to the data_main) even if the country_map_dlt doesn't contain information 
about five athletes present in data_main. These five athletes will have a NaN value 
in the Country column.
The length of merged_left indeed comes out to be 8,618. Let's check the merged_
left for an athlete whose information is not present in the country_map_dlt. It 
should contain NaN for the Country column:
merged_left_slt=merged_left[merged_left['Athlete']=='Michael Phelps']
merged_left_slt
The output is similar to the following screenshot. It indeed contains NaN for Michael 
Phelps' Country because it doesn't have a mapping in country_map_dlt:
Fig. 3.58: Merged_left data frame sub-setted for Michael Phelps contains NaN values, as expected
An example of the Right Join
One example of Right Join will be to merge data frame data_main with  
country_map_dlt. This can be done using the following snippet:
merged_right=pd.merge(left=data_main_dlt,right=country_map_
dp,how='right',left_on='Athlete',right_on='Athlete')
len(merged_right)

Chapter 3
[ 279 ]
This should contain the NaN values for the columns coming from data_main_dlt, in 
the rows where there is no athlete information in data_main_dlt.
As shown in the following table:
Fig. 3.59: merged_right data frame sub-setted for Michael Phelps contains NaN values, as expected
There will be one row created for each athlete who is not there in the data_main_dlt 
but is present in the country_map_dp. Hence, there will be five extra rows, one for 
each deleted athlete. The number of rows in the merged_right is thus equal to 8,610.
There are other joins like Outer Joins, which can be illustrated as the Union of two 
data frames. The Outer join would contain rows from both the data frames, even if 
they are not present in the other. It will contain NaN for the columns which it can't get 
values for. It can be easily performed setting the how parameter of the merge method 
to outer.
Summary of Joins in terms of their length
The effect of these joins can be more effectively explained if we summarize the 
number of samples present in the data frames that were used for merging and in the 
resultant data frames.
The first table provides the number of samples present in the data frames that were 
used for merging. All these data frames have been defined earlier in this section of 
the chapter:
Data frame
Length (# rows)
data_main
8618
data_main_dlt
8605
country_map_dp
6956
country_map_dlt
6951

Data Wrangling
[ 280 ]
This table provides the number of samples present in the merged data frames:
Merged data frame
Components
Length
merged_inner
data_main with 
country_map_dlt
8605
merged_left
data_main with 
country_map_dlt
8618
merged_right
data_main_dlt with 
country_ma_dp
8610
Summary
Quite a long chapter! Isn't it? But, this chapter will form the core of anything you 
learn and implement in data-science. Let us wrap-up the chapter by summarizing the 
key takeaways from the chapter:
•	
Data can be sub-setted in a variety of ways: by selecting a column, selecting 
few rows, selecting a combination of rows and columns; using .ix method 
and [ ] method, and creating new columns.
•	
Random numbers can be generated in a number of ways. There are many 
methods like randint(), raandarrange() in the random library of numpy. 
There are also methods like shuffle and choice to randomly select an 
element out of a list. Randn() and uniform() are used to generate random 
numbers following normal and uniform probability distributions. Random 
numbers can be used to run simulations and generate dummy data frames.
•	
The groupby() method creates a groupby element on which aggregate, 
transform, and filter operations can be applied. This is a good method to 
summarize data for each categorical variable at once.
•	
A data must be split between training and testing datasets before a modelling 
is performed. The training dataset is the one on which the model equations 
are developed. The testing dataset is used to test the performance of the 
model comparing the actual result (present in testing dataset) to the model 
output. There are various ways to perform this split. One can use choice  
and shuffle. Scikit-learn has a readymade method for this.

Chapter 3
[ 281 ]
•	
Two datasets can be merged just like two tables in a relational database. 
There are various kind of joins—Inner, Left, Right, Outer, and so on. These 
joins can be understood better if the datasets are assumed analogous to sets. 
Inner Join is then Intersection, Outer Join is Union, and Left and Right joins 
are entire left and right data frame.
Wrangling data and bringing it in the form you desire is a big challenge before 
one proceeds to modelling. But, once done, it opens up a plethora of insights and 
information to be discovered using predictive models. As Bob Marley said, "If it is 
easy, it won't be amazing; if it is amazing, it won't be easy."


[ 283 ]
Statistical Concepts for 
Predictive Modelling
There are a few statistical concepts, such as hypothesis testing, p-values,  
normal distribution, correlation, and so on without which grasping the concepts  
and interpreting the results of predictive models becomes very difficult. Thus, 
it is very critical to understand these concepts, before we delve into the realm of 
predictive modelling.
In this chapter, we will be going through and learning these statistical concepts  
so that we can use them in the upcoming chapters. This chapter will cover the 
following topics:
•	
Random sampling and central limit theorem: Understanding the concept 
of random sampling through an example and illustrating the central limit 
theorem's application through an example. These two concepts form the 
backbone of hypothesis testing.
•	
Hypothesis testing: Understanding the meaning of the terms, such as null 
hypothesis, alternate hypothesis, confidence intervals, p-value, significance 
level, and so on. A step-by-step guide to implement a hypothesis test, 
followed by an example.
•	
Chi-square testing: Calculation of chi-square statistic. A description of usage 
of chi-square tests with a couple of examples.
•	
Correlation: The meaning and significance of correlations between two 
variables, the meaning and significance of correlation coefficients and 
calculating and visualizing the correlation between variables of a dataset.

Statistical Concepts for Predictive Modelling
[ 284 ]
Random sampling and the central limit 
theorem
Let's try to understand these two important statistical concepts using an example. 
Suppose one wants to find the average age of one state of India, lets say Tamil Nadu. 
Now, the safest and brute-force way of doing this will be to gather age information 
from each citizen of Tamil Nadu and calculate the average for all these ages. But, 
going to each citizen and asking their age or asking them to tell their age by some 
method will take a lot of infrastructure and time. It is such a humongous task that 
census, which attempts to do just that, happens once a decade and what will happen 
if you decided to do so in a non-census year?
The statisticians face such issues all the time. The answer lies in random sampling. 
Random sampling means that you take a group of 1000 individuals (or 10000, 
depending on your capacity, obviously the more the merrier) and calculate the 
average for this group. You call this A1. Getting to this is easier as 1000 or 10000 
is within your reach. Then you select a second group of 1000 or 10000 people and 
calculate their average. You call this A2. You do this 100 times or 1000 times and call 
them A3, A4,…, A100 or A3, A4,…, A1000.
Then according to the most fundamental theorem in statistics called the central  
limit theorem:
•	
The average of A1, A2,…, A100 will be a good estimator of the average age 
of the residents of Tamil Nadu. If Am is the estimated average age of the 
residents of Tamil Nadu, then it is given by:
1
2
100 /100
Am
A
A
A
=
+
+
+
……
•	
If the number of such samples is sufficiently large, then the distribution of 
these averages will roughly follow a normal distribution. In other words,  
A1, A2,…, A100 will be normally distributed.
Now, the thing is that we are no more interested in finding the exact value of the 
average age, but we are settling for an estimator of the same. In such a case, we will 
have to make do with defining a range of values in which the actual value might lie. 
Since we have assumed a normal distribution for the average age values of these 
groups, we can apply all the properties of a normal distribution to quantify the 
chances of this average age being greater or lesser than a certain number.

Chapter 4
[ 285 ]
Hypothesis testing
The concept we just discussed in the preceding section is used for a very important 
technique in statistics, called hypothesis testing. In hypothesis testing, we assume a 
hypothesis (generally related to the value of the estimator) called null hypothesis and 
try to see whether it holds true or not by applying the rules of a normal distribution. 
We have another hypothesis called alternate hypothesis.
Null versus alternate hypothesis
There is a catch in deciding what will be the null hypothesis and what will be the 
alternate hypothesis. The null hypothesis is the initial premise or something that we 
assume to be true as yet. The alternate hypothesis is something we aren't sure about 
and are proposing as an alternate premise (almost often contradictory to the null 
hypothesis) which might or might not be true.
So, when someone is doing a quantitative research to calibrate the value of an 
estimator, the known value of the parameter is taken as the null hypothesis while 
the new found value (from the research) is taken as the alternate hypothesis. In 
our case of finding the mean age of Tamil Nadu, we can say that based on the rich 
demographic dividend of India, a researcher can claim that the mean age should be 
less than 35. This can serve as the null hypothesis. If a new agency claims otherwise 
(that it is greater than 35), then it can be termed as the alternate hypothesis.
Z-statistic and t-statistic
Assume that the value of the parameter assumed in the null hypothesis is Ao. Take 
a random sample of 100 or 1000 people or occurrences of the event and calculate 
the mean of the parameter, such as mean age, mean delivery time for pizza, mean 
income, and so on. We can call it Am. According to the central limit theorem,  
the distribution of population means that random samples will follow a  
normal distribution.
The Z-statistic is calculated to convert a normally distributed variable (the 
distribution of population mean of age) to a standard normal distribution. This 
is because the probability values for a variable following the standard normal 
distribution can be obtained from a precalculated table. The Z-statistic is given  
by the following formula:
(
) / (
/
)
Z
Am
Ao
n
σ
=
−

Statistical Concepts for Predictive Modelling
[ 286 ]
In the preceding formula, the σ stands for the standard deviation of the population/
occurrences of events and n is the number of people in the sample.
Now, there can be two cases that can arise:
•	
Z- test (normal distribution): The researcher knows the standard deviation 
for the parameter from his/her past experience. A good example of this is the 
case of pizza delivery time; you will know the standard deviation from past 
experiences:
(
) / (
/
)
Z
Am
Ao
n
σ
=
−
Ao (from the null hypothesis) and n are known. Am is calculated from the 
random sample. This kind of test is done when the standard deviation is 
known and is called the z-test because the distribution follows the normal 
distribution and the standard-normal value obtained from the preceding 
formula is called the Z-value.
•	
t-test (Student-t distribution): The researcher doesn't know the standard 
deviation of the population. This might happen because there is no such data 
present from the historical experience or the number of people/event is very 
small to assume a normal distribution; hence, the estimation of mean and 
standard deviation by the formula described earlier. An example of such a 
case is a student's marks in an exam, age of a population, and so on. In this 
case, the mean and standard deviation become unknown and the expression 
assumes a distribution other than normal distribution and is called a 
Student-t distribution. The standard value in this case is called t-value  
and the test is called t-test.
Standard distribution can also be estimated once the mean is estimated, if 
the number of samples is large enough. Let us call the estimated standard 
distribution S; then the S is estimated as follows:
2
(
) / (
1)
S
Ai
Ao
n
= ∑
−
−
The t-statistic is calculated as follows:
(
) / (
/
)
t
Am
Ao
S
n
=
−

Chapter 4
[ 287 ]
The difference between the two cases, as you can see, is the distribution they follow. 
The first one follows a normal distribution and calculates a Z-value. The second 
one follows a Student-t distribution and calculates a t-value. These statistics that is 
Z-statistics and t-statistics are the parameters that help us test our hypothesis.
Confidence intervals, significance levels, and 
p-values
Let us go back a little in the last chapter and remind ourselves about the cumulative 
probability distribution.
P2
P1
P2
Z1
Z2
P1
Fig. 4.1: A typical normal distribution with p-values
Let us have a look to the preceding figure, it shows a standard normal distribution. 
Suppose, Z1 and Z2 are two Z-statistics corresponding to two values of random 
variable and p1 and p2 are areas enclosed by the distribution curve to the right of 
those values. In other words, p1 is the probability that the random variable will take 
a value lesser than or equal to Z1 and p2 is the probability that the random variable 
will take a value greater than Z2.
If we represent the random variable by X, then we can write:
(
1)
1
(
2)
2
P X
Z
p
P X
Z
p
<
=
<
=

Statistical Concepts for Predictive Modelling
[ 288 ]
Also, since the sum of all the exclusive probabilities is always 1, we can write:
(
1)
1
1
(
2)
1
2
P X
Z
p
P X
Z
p
>
= −
>
= −
For well-defined distributions, such as the normal distribution, one can define an 
interval in which the value of the random variable will lie with a confidence level 
(read probability). This interval is called the confidence interval. For example, for a 
normal distribution with mean μ and standard deviation σ, the value of the random 
variable will lie in the interval [μ-3σ, μ+3σ] with 99% probability. For any estimator 
(essentially a random variable) that follows a normal distribution, one can define 
a confidence interval if we decide on the confidence (or probability) level. One can 
think of confidence intervals as thresholds of the accepted values to hold a null 
hypothesis as true. If the value of the estimator (random variable) lies in this range,  
it will be statistically correct to say that the null hypothesis is correct.
To define a confidence interval, one needs to define a confidence (or probability 
level). This probability needs to be defined by the researcher depending on the 
context. Lets call this p. Instead of defining this probability p, one generally defines 
(1-p) that is called level of significance. Let us represent it by ß. This represents the 
probability that the null hypothesis won't be true. This is defined by the user for  
each test and is usually of the order of 0.01-0.1.
An important concept to learn here is the probability value or just a p-value of a 
statistic. It is the probability that the random variable assumes, it's a value greater 
than the Z-value or t-value:
(
)
p
value
P X
Z
−
=
>

Chapter 4
[ 289 ]
β
P
Z
β
P
Fig. 4.2: A typical normal distribution with p-values and significance level
Now, this Z-value and the p-value has been obtained assuming that the null 
hypothesis is true. So, for the null hypothesis to be accepted, the Z-value has to lie 
outside the area enclosed by ß. In other words, for the null hypothesis to be true, the 
p-value has to be greater than the significance level, as shown in the preceding figure.
To summarize:
•	
Accept the null hypothesis and reject the alternate hypothesis if p-value>ß
•	
Accept the alternate hypothesis and reject the null hypothesis if p-value<ß
Different kinds of hypothesis test
Due to the symmetry and nature of the normal distribution, there are three kinds of 
possible hypothesis tests:
•	
Left-tailed
•	
Right-tailed
•	
Two-tailed

Statistical Concepts for Predictive Modelling
[ 290 ]
Left-tailed: This is the case when the alternate hypothesis is a "less-than" type.  
The hypothesis testing is done on the left tail of the distribution and hence the  
name. In this case, for:
•	
Accepting a null hypothesis and rejecting an alternate hypothesis the 
p-value>ß or Z>Zß
•	
Accepting an alternate hypothesis and rejecting a null hypothesis the 
p-value<ß or Z<Zß
  Z < Zβ
  β > P - value
Reject Null Hypothesis
  Z > Zβ
  β < P - value
Accept Null Hypothesis
P-value
β
Z
value
Zβ
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
β
P-value
Zβ
Z
value
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
xx
x x
x
x x
x x
x
x
x
x x
Fig. 4.3: Left-tailed hypothesis testing
Right-tailed: This is the case when the alternate hypothesis is of greater than type.  
The hypothesis testing is done on the right tail of the distribution, hence the name.  
In this case, for:
•	
Accepting a null hypothesis and rejecting an alternate hypothesis the 
p-value>ß or Z<Zß

Chapter 4
[ 291 ]
•	
Accepting an alternate hypothesis and rejecting a null hypothesis the 
p-value<ß or Z>Zß
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
P-value
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
x
xx
x x
x
x x
x x
x
x
x
x x
  Z - value > Zβ
  P - value < β 
Reject Null Hypothesis
  Z - value < Zβ
  P - value > β
Accept Null Hypothesis
β
Z
value
Zβ
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x
x x
x
x
x
x
x
x
x
x
x
β
P-value
Zβ
Z
value
Fig. 4.4: Right-tailed hypothesis testing
Two-tailed: This is the case when the alternate hypothesis has an inequality—less 
than or more than is not mentioned. It is just an OR operation over both kind of tests. 
If either of the left- or right-tailed tests reject the null hypothesis, then it is rejected. 
The hypothesis testing is done on both the tails of the distribution; hence, the name.
A step-by-step guide to do a hypothesis test
So how does one accept one hypothesis and reject the other? There has to be a  
logical way to do this. Let us summarize and put to use whatever we have learned 
till now in this section, to make a step-by-step plan to do a hypothesis test. Here is  
a step-by-step guide to do a hypothesis test:
1.	 Define your null and alternate hypotheses. The null hypothesis is something 
that is already stated and is assumed to be true, call it Ho. Also, assume that 
the value of the parameter in the null hypothesis is Ao.

Statistical Concepts for Predictive Modelling
[ 292 ]
2.	 Take a random sample of 100 or 1000 people/occurrences of events and 
calculate the value of estimator (for example, mean of the parameter that is 
mean age, mean delivery time for pizza, mean income, and so on). You can 
call it Am.
3.	 Calculate the standard normal value or Z-value as it is called using  
this formula:
(
) / (
/
)
Z
Am
Ao
n
σ
=
−
In the preceding formula, σ is the standard deviation of the population or 
occurrences of events and n is the number of people in the sample.
The probability associated with the Z-value calculated in step 3 is compared with  
the significance level of the test to determine whether null hypothesis will be 
accepted or rejected.
An example of a hypothesis test
Let us see an example of hypothesis testing now. A famous pizza place claims  
that their mean delivery time is 20 minutes with a standard deviation of 3 minutes. 
An independent market researcher claims that they are deflating the numbers for 
market gains and the mean delivery time is actually more. For this, he selected a 
random sample of 64 deliveries over a week and found that the mean is 21.2 minutes. 
Is his claim justified or the pizza place is correct in their claim? Assume a significance 
level of 5%.
First things first, let us define a null and alternate hypothesis:
:
20(
)
:
20(
)
3,
64
24,
0.05
Ho Do
What the pizza guyclaims
Ha Do
what researcher claims
n
and Dm
σ
β
=
>
=
=
=
=
Let us calculate the Z-value:
(
) (
)
21.2
20 / 3/
64
3.2
Z =
−
=

Chapter 4
[ 293 ]
When we see the standard normal table for this Z-value, we find out that this value 
has an area of .9993 to the left of it; hence, the area to the right is 1-.99931, which is 
less than 0.05.
Hence, p-value<ß. Thus, the null hypothesis is rejected. This can be summarized in 
the following figure:
P - value < B
Null Hypothesis Rejected 
=0.05  Z=3.2
             P-value
β
=1-.9993
=0.0007
Fig. 4.5: Null Hypothesis is rejected because p-value<significance level
Hence, the researcher's claim that the mean delivery time is more than 20 minutes is 
statistically correct.
Chi-square tests
The chi-square test is a statistical test commonly used to compare observed data with 
the expected data assuming that the data follows a certain hypothesis. In a sense, this 
is also a hypothesis test. You assume one hypothesis, which your data will follow 
and calculate the expected data according to that hypothesis. You already have the 
observed data. You calculate the deviation between the observed and expected data 
using the statistics defined in the following formula:
2
( )
(
) /
chi
squarevalue g
O
E
E
−
= ∑
−
Where O is the observed value and E is the expected value while the summation is 
over all the data points.

Statistical Concepts for Predictive Modelling
[ 294 ]
The chi-square test can be used to do the following things:
•	
Show a causal relationship or independence between one input and output 
variable. We assume that they are independent and calculate the expected 
values. Then we calculate the chi-square value. If the null hypothesis 
is rejected, it suggests a relationship between the two variables. The 
relationship is not just by chance but statistically proven.
•	
Check whether the observed data is coming from a fair/unbiased source. 
If the observed data is more skewed towards one extreme, compared to the 
expected data, then it is not coming from a fair source. But, if it is very close 
to the expected value then it is.
•	
Check whether a data is too good to be true. As, it is a random experiment 
and we don't expect the values to toe the assumed hypothesis. If they do toe 
the assumed hypothesis, then the data has probably been tampered to make 
it look good and is too good to be true.
Let us create a hypothetical experiment where a coin is tossed 10 times. How many 
times do you expect it to turn heads or tails? Five, right? Now, what if we do this 
experiment 1000 times and record the scores (number of heads and tails). Suppose 
we observed heads 553 times and a tails in the rest of the trials:
:
0.5
:
0.5
Ho The proportionof head and tail is
Ha The proportionis not
Head
Tail
Observed
553
447
Expected
1000*0.5=500
1000*0.5=500
Let us calculate the chi-square value:
2
2
(553
500)
(447
500)
/ 500
11.236
g


=
−
+
−
=



Chapter 4
[ 295 ]
This chi-square value is compared to the value on a chi-square distribution for a 
given degree of freedom and a given significance level. The degrees of freedom is  
the number of categories -1. In this case, it is 2-1=1. Let us assume a significance  
level of 0.05.
The chi-square distribution looks a little different than the normal distribution.  
It also has a peak but has a much longer tail than the normal distribution and is  
only on one side. As the degree of freedom increases, they start looking similar  
to a normal distribution:
Fig. 4.6: Chi-square distribution with different degrees of freedom
When we look at the chi-square distribution table for a degree of freedom 1 and a 
significance level of 0.05, we get a value of 3.841. At a significance level of 0.01, we 
get 6.635. In both the cases, the chi-square statistic is greater than the value from the 
chi-square distribution, meaning that the chi-square statistic lies on the right of the 
value from the distribution table. 

Statistical Concepts for Predictive Modelling
[ 296 ]
Hence, the null hypothesis is rejected. That means that the coin is not fair.
Significance Chi square
level        value
=0.05   =11.236
Value
=3.841
Value at significance level < Chi square value
Null hypothesis is rejected
Fig. 4.7: Null hypothesis is rejected because the value of the chi-square statistic at the significance level is less 
than the value of the chi-square statistic
Let us look at another example where we want to prove that the gender of a student 
and the subjects they choose are independent.
Suppose, in a group of students, the following table represents the number of boys 
and girls who have taken Maths, Arts, and Commerce, as their main subjects.
The observed number of boys and girls in each subject is as shown in the  
following table:
Maths 
Arts
Commerce
Total
Boys 
68
52
90
200
Girls
28
37
35
100
Total
96
89
125
300

Chapter 4
[ 297 ]
If the choice of the subjects is irrespective of the gender, then the expected number of 
boys and girls taking different subjects is, as follows:
Maths 
Arts
Commerce
Total
Boys 
(200/300)*96=64
(200/300)*89=59.3
(200/300)*125=83.3
200
Girls
(100/300)*96=32
(100/300)*89=29.7
(100/300)*125=41.7
100
Total
96
89
125
300
The deviation element is calculated for each cell using the (O-E)2/E formula:
Maths
Arts
Commerce
Boys 
(68-64)2/64
(52-59.3)2/59.3
(90-83.3)2/83.3
Girls
(28-32)2/32
(37-29.7)2/29.7
(35-41.7)2/41.7
On calculating and summing up all the values, the chi-square value comes out to  
be 5.05. The degree of freedom is the number of categories-1, which amounts to 
[(3x2)-1=5]. Let us assume a significance level of 0.05.
Looking at the chi-square distribution, one can find out that for a 5-degree freedom 
chi-square distribution, the value of the chi-square statistic at a significance level of 
0.05 is 11.07.
The calculated chi-square statistic < chi-square statistic (at significance level=0.05).
Since, the chi-square statistic lies on the left of the value at the significance level,  
the null hypothesis can't be rejected. Hence, the choice of subjects is independent  
of the gender.

Statistical Concepts for Predictive Modelling
[ 298 ]
Correlation
Another statistical idea which is very basic and important while finding a relation 
between two variables is called correlation. In a way, one can say that the concept of 
correlation is the premise of predictive modelling, in the sense that the correlation is 
the factor relying on which we say that we can predict outcomes.
A good correlation between two variables suggests that there is a sort of dependence 
between them. If one is changed, the change will be reflected in the other as well. 
One can say that a good correlation certifies a mathematical relation between two 
variables and due to this mathematical relationship, we might be able to predict 
outcomes. This mathematical relation can be anything. If x and y are two variables, 
which are correlated, then one can write:
( )
Y
f x
=
If f is a linear function, then a and b are linearly correlated. If f is an exponential 
function, then a and b are exponentially correlated:
:
Linear correlation y
ax
b
=
+
( )
:
exp
Exponential correlation y
a
b
=
+
The degree of correlation between the two variables x and y is quantified by the 
following equation:
( )
(
) (
)
(
)
(
)
(
)
2
2
x
xm
y
ym
correlationcoefficient h
x
xm
y
ym
∗
∑
−
∗
−
=
∑
−
∑
−
Where xm and ym are mean values of x and y
A few points to note about the correlation coefficient are as follows:
•	
The value of the correlation coefficient can range from -1 to 1, that is -1<h<1.
•	
A positive correlation coefficient means that there is a direct relationship 
between the two variables; if one variable increases, the other variable will 
also increase and if one decreases the other will decrease as well.

Chapter 4
[ 299 ]
•	
A positive correlation coefficient means that there is an inverse relationship 
between the two variables; if one variable increases, the other variable will 
decrease and if one decreases the other will increase.
•	
The more the value of the correlation coefficient, the stronger the relation 
between the two variables.
Although, a strong correlation suggests that there is some kind of a relationship  
that can be leveraged to predict one based on the other; it doesn't imply that its 
relation with the other variable is the only factor explaining this, there can be several 
others. Hence, the most often used quote related to correlation is, "Correlation doesn't 
imply causation."
Let us try to understand this concept better by looking at a dataset and trying to 
find the correlation between the variables. The dataset that we will be looking at 
is a very popular dataset about various costs incurred on advertising by different 
mediums and the sales for a particular product. We will be using it later to explore 
the concepts of linear regression. Let us import the dataset and calculate the  
correlation coefficients:
import pandas as pd
advert=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Linear Regression/Advertising.csv')
advert.head()
Fig. 4.8: Dummy dataset

Statistical Concepts for Predictive Modelling
[ 300 ]
Let us try to find out the correlation between the advertisement costs on TV and the 
resultant sales. The following code will do the job:
import numpy as np
advert['corrn']=(advert['TV']-np.mean(advert['TV']))*(advert['Sales']-
np.mean(advert['Sales']))
advert['corrd1']=(advert['TV']-np.mean(advert['TV']))**2
advert['corrd2']=(advert['Sales']-np.mean(advert['Sales']))**2
corrcoeffn=advert.sum()['corrn']
corrcoeffd1=advert.sum()['corrd1']
corrcoeffd2=advert.sum()['corrd2']
corrcoeffd=np.sqrt(corrcoeffd1*corrcoeffd2)
corrcoeff=corrcoeffn/corrcoeffd
corrcoeff
In this code snippet, the formula written above has been converted to code. The 
value of the correlation coefficient comes out to be 0.78 indicating that there is a 
descent in positive correlation between TV-advertisement costs and sales; it implies 
that if the TV-advertisement cost is increased, as a result sales will increase.
Let us convert the preceding calculation to a function, so that we can calculate all the 
pairs of correlation coefficients very fast just by replacing the variable names. One 
can do that using the following snippet wherein a function is defined to parameterize 
the name of the data frame and the column names for which the correlation 
coefficient is to be calculated:
def corrcoeff(df,var1,var2):
    df['corrn']=(df[var1]-np.mean(df[var1]))*(df[var2]-np.
mean(df[var2]))
    df['corrd1']=(df[var1]-np.mean(df[var1]))**2
    df['corrd2']=(df[var2]-np.mean(df[var2]))**2
    corrcoeffn=df.sum()['corrn']
    corrcoeffd1=df.sum()['corrd1']
    corrcoeffd2=df.sum()['corrd2']
    corrcoeffd=np.sqrt(corrcoeffd1*corrcoeffd2)
    corrcoeff=corrcoeffn/corrcoeffd
    return corrcoeff

Chapter 4
[ 301 ]
This function can be used to calculate correlation coefficient for any two variables of 
any data frame.
For example, to calculate the correlation between TV and Sales columns of the 
advert data frame, we can write it as follows:
&
(
,'
','
')
0.78
&
(
,'
','
')
0.57
TV
Sales
corrcoeff advert TV
Sales
Radio
Sales
corrcoeff advert Radio
Sales
We can summarize the pair-wise correlation coefficients between the variables in the 
following table:
TV
Radio
Newspaper
Sales
TV
1
0.05
0.06
0.78
Radio
0.05
1
0.35
0.57
Newspaper
0.06
0.35
1
0.23
Sales
0.78
0.57
0.23
1
This table is called Correlation Matrix. As you can see, it is a symmetric matrix 
because the correlation between TV and Sales will be the same as that between 
Sales and TV. Along the diagonal, all the entries are 1 because, by definition, the 
correlation of a variable with itself will always be 1. As can be seen, the strongest 
correlation can be found between TV advertisement cost and sales.
Let us see the nature of this correlation by plotting TV and Sales variables of the 
advert data frame. We can do this using the following code snippet:
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(advert['TV'],advert['Sales'],'ro')
plt.title('TV vs Sales')

Statistical Concepts for Predictive Modelling
[ 302 ]
The result is similar to the following plot:
Fig. 4.9: Scatter plot of TV vs Sales
Looking at this plot, we can see that the points are more or less compact and not 
scattered far away and as the TV advertisement cost increases, the sales also increase. 
This is the characteristic of two variables that are positively correlated. This is 
supported by a strong correlation coefficient of 0.78.
Let us plot the variables and see how they are distributed to corroborate their 
correlation coefficient. For Radio and Sales, this can be plotted as follows:
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(advert['Radio '],advert['Sales'],'ro')
plt.title('Radio vs Sales')

Chapter 4
[ 303 ]
The plot we get is as shown in the following figure:
Fig. 4.10: Scatter plot of Radio vs Sales
For Radio and Sales, the points are a little more scattered than TV versus Sales and 
this is corroborated by the fact that the correlation coefficient for this pair (0.57) is 
less than that for TV and Sales (0.78).
For plotting Newspaper vs Sales data, we can write something similar to the 
following code:
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(advert['Newspaper'],advert['Sales'],'ro')
plt.title('Newspaper vs Sales')

Statistical Concepts for Predictive Modelling
[ 304 ]
The output plot looks similar to the following figure:
Fig. 4.11: Scatter plot of Newspaper vs Sales
For Newspaper and Sales, the points are way more scattered than in the case of TV 
and Sales and Radio and Sales. This is further strengthened by a small correlation 
coefficient of 0.23 between Newspaper and Sales, compared to 0.78 between TV and 
Sales, and 0.57 between Radio and Sales.

Chapter 4
[ 305 ]
Summary
In this chapter, we skimmed through the basic concepts of statistics. Here is a brief 
summary of the concepts we learned:
•	
Hypothesis testing is used to test the statistical significance of a hypothesis. 
The one which already exists or is assumed to be true is a null hypothesis, 
the one which someone is not sure about or is being proposed as an alternate 
premise is an alternate hypothesis.
•	
One needs to calculate a statistic and the associated p-value to conduct t 
he test.
•	
Hypothesis testing (p-values) is used to test the significance of the estimates 
of the coefficients calculated by the model.
•	
The chi-square test is used to test the causal relationship between a  
predictor and an input variable. It can also be used to check whether  
the data is fair or fake.
•	
The correlation coefficient can range from -1 to 1. The closer it is to the 
extremes, the stronger is the relationship between the two variables.
Linear regression is part of the family of algorithms called supervised algorithms as 
the dataset on which they are built has an output variable. In a sense, one can say 
that this output variable governs or supervises the development of the model and 
hence the name. More on this is covered in the next chapter.


[ 307 ]
Linear Regression  
with Python
If you have mastered the content of the last two chapters, implementing  
predictive models will be a cake walk. Remember the 80-20% split between the  
data cleaning + wrangling and modelling? Then what is the need of dedicating  
a full chapter to illustrate the model? The reason is not about running a predictive 
model; it is about understanding the mathematics (algorithms) that goes behind 
the ready-made methods which we will be using to implement these algorithms. 
It is about interpreting the swathe of results these models spew after the model 
implementation and making sense of them in the context. Thus, it is of utmost 
importance to understand the mathematics behind the algorithms and the result 
parameters of these models.
With this chapter onwards, we will deal with one predictive modelling algorithm in 
each chapter. In this chapter, we will discuss a technique called linear regression. It 
is the most basic and generic technique to create a predictive model out of a historical 
dataset with an output variable.
The agenda of this chapter is to thoroughly understand the mathematics behind 
linear regression and the results generated by it by illustrating its implementation  
on various datasets. The broad agenda of this chapter is, as follows:
•	
The maths behind the linear regression: How does the model work? How 
is the equation of the model created based on the dataset? What are the 
assumptions for this calculation?

Linear Regression with Python
[ 308 ]
•	
Implementing linear regression with Python: There are a couple of ready-
made methods to implement linear regression in Python. Instead of using 
these ready-made methods, one can write one's own Python code snippet for 
the entire calculation with custom inputs. However, as linear regression is a 
regularly used algorithm, the use of ready-made methods is quite common. 
Its implementation from scratch is generally used to illustrate the maths 
behind the algorithm.
•	
Making sense of result parameters: There will be tons of result parameters, 
such as slope, co-efficient, p-values, and so on. It is very important to 
understand what each parameter means and the range their values lie in,  
for the model to be an efficient model.
•	
Model validation: Any predictive model needs to be validated. One common 
method of validating is splitting the available dataset into training and 
testing datasets, as discussed in the previous chapter. The training dataset 
is used to develop the model while the testing is used to compare the result 
predicted by the model to the actual values.
•	
Handling issues related to linear regression: Issues, such as multi-
collinearity, handling categorical variables, non-linear relationships, and so 
on come up while implementing a linear regression; these need to be taken 
care of to ensure an efficient model.
Before we kick-start the chapter, let's discuss what a model means and entails. A 
mathematical/statistical/predictive model is nothing but a mathematical equation 
consisting of input variables yielding an output when values of the input variables 
are provided. For example, let us, for a moment, assume that the price (P) of a house 
is linearly dependent upon its size (S), amenities (A), and availability of transport (T). 
The equation will look like this:
1
2
3
P
a
S
a
A
a
T
=
∗
+
∗
+
∗
This is called the model and the variables a1, a2, and a3 are called the variable 
coefficients. The variable P is the predicted output while the S, A, and T are input 
variables. Here, S, A, and T are known but a1, a2, and a3 are not. These parameters 
are estimated using the historical input and output data. Once, the value of these 
parameters is found, the equation (model) becomes ready for testing. Now, S, A, and 
T can be numerical, binary, categorical, and so on; while P can also be numerical, 
binary, or categorical and it is this need to tackle various types of variables that gives 
rise to a large number of models.

Chapter 5
[ 309 ]
Understanding the maths behind linear 
regression
Let us assume that we have a hypothetical dataset containing information about the 
costs of several houses and their sizes (in square feet):
Size (square feet) X
Cost (lakh INR) Y
1500
45
1200
38
1700
48
800
27
There are two kinds of variables in a model:
•	
The input or predictor variable, the one which helps predict the value of 
output variable
•	
The output variable, the one which is predicted
In this case, cost is the output variable and the size is the input variable. The output 
and the input variables are generally referred as Y and X respectively.
In the case of linear regression, we assume that Y (Cost) is a linear function of X (Size) 
and to estimate Y, we write:
Cos
eY
X or
t
Size
α
β
α
β
=
+
∗
=
+
∗
Where Ye is the estimated or predicted value of Y based on our linear equation.
The purpose of linear regression is to find statistically significant values of a and ß, 
which minimize the difference between Y and Ye. If we are able to determine the 
values of these two parameters satisfying these conditions, then we will have an 
equation which we can predict the values of Y, given the value of X.
So, to summarize, linear regression (like any other supervised algorithm) requires 
historical data with one output variable and one or more than one input variables to 
make a model/equation, using which output variables can be calculated/predicted 
if the input variable is present. In the preceding case, if we find the value of a=2 and 
ß=.3, then the equation will be:
2
.03
eY
X
=
+

Linear Regression with Python
[ 310 ]
Using this equation, we can find the cost of a home of any size. For a 900 square feet 
house, the cost will be:
2
900 .03
29
eY
units
=
+
∗
=
The next question that we can ask is how do we estimate a and ß. We use a method 
called least square sum of the difference between Y and Ye. The difference between 
the Y and Ye can be represented as e:
(
)
e
e
Y
Y
=
−
Thus, the objective is to minimize 
(
)
(
)
2
2
(
)
Y
Ye
Y
X
α
β
∑
−
= ∑
−
+
∗
; the summation is 
over all the data points.
We can also minimize: 
2
2
2
2
1
2
e
e
e
en
∑
=
+
+……
, where n is the number of data points.
Using calculus, we can show that the values of the unknown parameters are  
as follows:
(
)(
)
(
)
2
/
Xi
Xm
Yi
Ym
Xi
Xm
Ym
Xm
β
α
β
= ∑
−
−
∑
−
=
−
∗
where Xm – mean of X values and Ym-mean of Y values
If you are interested to know how these formulae come up, you can go through 
the following information box, which describes the derivation. The steps for this 
derivation can be summarized, as follows:
•	
Take partial derivatives of e2 with respect to all the variable coefficients and 
equate them to 0 (at maxima or minima, the derivative of a function is 0). 
This will give us as many equations, as there are variables:
(
)
(
)
(
)
(
)
1
1
ˆ
ˆ,
ˆ
ˆ
2
0
ˆ
ˆ
ˆ,
ˆ
ˆ
2
0
ˆ
n
i
i
i
n
i
i
i
i
S
y
x
S
y
x
x
α β
α
β
α
α β
α
β
β
=
=
∂
= −
−
−
∗
=
∂
∂
= −
−
−
∗
=
∂
∑
∑
where S= (Y-Ye), Y-actual value of Y, Ye-estimated/predicted value of Y= a+ ß*X

Chapter 5
[ 311 ]
•	
Solve these equations to get the values of the variable coefficients:
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)
(
)(
)
1
2
1
2
1
2
1
1
1
1
1
1
1
1
ˆ
ˆ
:
0
ˆ
ˆ
0
ˆ
ˆ
0
ˆ
ˆ
ˆ
ˆ
0
0
ˆ
ˆ
0
ˆ
ˆ
n
i
i
i
n
i
i
i
i
i
n
i
i
i
i
i
n
n
i
i
i
i
i
i
i
i
i
i
n
n
n
i
i
i
i
i
i
i
n
i
i
n
i
i
i
i
i
yi
x
x
y x
x
x
y x
y
x x
x
y x
yx
xx
x
y
y
x
x
x
y
y
x
x
y
y
x
x
y
y
x
x
y
y
x
x
α
β
α
β
β
β
β
β
β
β
β
β
β
β
=
=
=
=
=
=
=
=
=
=
=
∏
−
−
∗
=
⇒
−
−
=
⇒
−
−
−
=
⇒
−
+
−
=
⇒
−
+
−
=
−
+
−
=
⇒
−
= −
−
−
⇒
=
−
−
−
⇒
=
∑
∑
∑
∑
∑
∑
∑
∑
∑
∑
(
)
(
)
( )
1
2
1
,
'
'
n
n
i
i
Cov x y
X X
X y
Var x
x
x
−
=
=
=
−
∑
∑
Almost all the statistical tools have ready-made programs to calculate the coefficients 
a and ß. However, it is still very important to understand how they are calculated 
behind the curtain.
Linear regression using simulated data
For the purpose of linear regression, we write that 
eY
X
α
β
=
+
∗
; whereas Y will 
rarely be perfectly linear and would have an error component or residual and we 
write Y
X
K
α
β
=
+
∗
+
.
In the above example, K is the error component or residual. It is a random variable 
and is assumed to be normally distributed.

Linear Regression with Python
[ 312 ]
Fitting a linear regression model and checking its 
efficacy
Let us simulate the data for the X and Y variables and try to look at how the 
predicted values (Ye) differ from the actual value (Y).
For X, we generate 100 normally distributed random numbers with mean 1.5 and 
standard deviation 2.5 (you can take any other number of your choice and try). 
For predicted value(Ye),we assume an intercept of 2 and a slope of .3 and we write 
Ye =2+.3*x. Later, we will calculate the values of a and ß using the preceding data 
and see how that changes the efficacy of the model. For the actual value, we add a 
residual term (res) that is nothing but a random variable distributed normally with 
mean 0 and a standard deviation of .5.
The following is the code snippet to generate these numbers and convert these three 
columns in a data frame:
import pandas as pd
import numpy as np
x=2.5*np.random.randn(100)+1.5
res=.5*np.random.randn(100)+0
ypred=2+.3*x
yact=2+.3*x+res
xlist=x.tolist()
ypredlist=ypred.tolist()
yactlist=yact.tolist()
df=pd.DataFrame({'Input_Variable(X)':xlist,'Predicted_Output(ypred)':y
predlist,'Actual_Output(yact)':yactlist})
df.head()
The resultant data frame df output looks similar to the following table:
Fig. 5.1: Dummy dataset

Chapter 5
[ 313 ]
Let us now plot both, the actual output (yact) and predicted output (ypred) against 
the input variable (x) for the sake of comparing yact and ypred and see what the 
difference between them is. This ultimately answers the bigger question, as to how 
accurately the proposed equation has been able to predict the value of the output:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline
x=2.5*np.random.randn(100)+1.5
res=.5*np.random.randn(100)+0
ypred=2+.3*x
yact=2+.3*x+res
plt.plot(x,ypred)
plt.plot(x,yerr,'ro')
plt.title('Actual vs Predicted')
The output of the snippet looks similar to the following screenshot. The red dots are 
the actual values (yact) while the blue line is the predicted value (ypred):
Fig. 5.2: Plot of Actual vs Predicted values from the dummy dataset

Linear Regression with Python
[ 314 ]
Let us add a line representing the mean of the actual values for a better perspective 
of the comparison. The line in green represents the mean of the actual values:
Fig. 5.3: Plot of Actual vs. Predicted values from the dummy dataset with mean actual value
This could be achieved by the following code snippet that is obtained by a little 
tweaking of the preceding code snippet:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
%matplotlib inline
x=2.5*np.random.randn(100)+1.5
res=.5*np.random.randn(100)+0
ypred=2+.3*x
yact=2+.3*x+res
ymean=np.mean(yact)
yavg=[ymean for i in range(1,len(xlist)+1)]
plt.plot(x,ypred)
plt.plot(x,yact,'ro')
plt.plot(x,yavg)
plt.title('Actual vs Predicted')

Chapter 5
[ 315 ]
Now, the question to be asked is why we chose to plot the mean value of the yact. 
This is because in the case when we don't have any predictor model, our best bet  
is to go with the mean value of the observed value and say that this will be the 
predicted value.
Another point to think about is how to judge the efficacy of our model. If you pass 
any data containing two variables—one input and one output, the statistical program 
will generate some values of alpha and beta. But, how do we understand that these 
values are giving us a good model?
Fig. 5.4: Actual vs. Predicted vs. Fitted (Regressed) line from the dummy dataset (a picture to always keep in 
mind whenever you think of R2)
In case, there is no model and the total variability is explained as the Total Sum of 
Squares or SST:
(
)
2
SST
yact
yavg
= ∑
−
Now, this total error is composed of two terms—one which is the difference between 
the regression value and the mean value, this is the difference which the model seeks 
to explain and is called Regression Sum of Squares or SSR:
2
(
)
SSR
ypred
yavg
= ∑
−

Linear Regression with Python
[ 316 ]
The unexplained random term, let us call it, Difference Sum of Squares or SSD is:
2
(
)
SSD
yact
ypred
= ∑
−
As you can see, in the preceding figure or you can guess intuitively that:
SST
SSR
SSD
=
+
Where, SSR is the difference explained by the model; SSD is the difference not 
explained by the model and is random; SST is the total error.
The more the share of SSR in SST, the better the model is. This share is quantified by 
something called R2 or R-squared or coefficient of determination
2
/
R
SSR SST
=
Since SST>SSR, the value of R2 can range from 0 to 1. The closer it is to 1, the better 
the model. A model with R2=0.9 will be compared to a model with R2=0.6, all the 
other factors remaining the same. That being said, a good R2 alone doesn't mean 
that the model is a very efficient one. There are many other factors that we need to 
analyze before we come to that conclusion. But, R2 is a pretty good indicator that a 
linear regression model will be effective.
Let us see what the value of R2 is for the dataset that we created in the preceding 
section. When we perform a linear regression, the R2 value will be calculated 
automatically. Nevertheless, it is great to have an understanding of how it  
is calculated.
In the following code snippet, SSR and SST have been calculated according to the 
formulae described in the preceding section and have been divided to calculate R2:
df['SSR']=(df['Predicted_Output(ypred)']-ymean)**2
df['SST']=(df['Actual_Output(yact)']-ymean)**2
SSR=df.sum()['SSR']
SST=df.sum()['SST']
SSR/SST
The value of R2 comes out to be 0.57, suggesting that ypred provides a decent 
prediction of the yact. In this case, we have randomly assumed some values for a 
and ß. a=2, ß=.3. This might or might not be the best values of a, ß. We have seen 
earlier that a least sum of square methods is used to find an optimum value for a, ß. 
Let us use these formulae to calculate a+ß*X and see if there is an improvement in R2 
or not. Hopefully, there will be.

Chapter 5
[ 317 ]
Finding the optimum value of variable coefficients
Let us go back to the data frame df that we created a few pages back. The Input_
Variable(X)column is the predictor variable using which the (a, ß) model will be 
derived. The Actual_Output(yact) variable, as the name suggests, is the actual 
output variable. Using these two variables, we will calculate the values of a and ß, 
according to the formulae described previously.
One thing to be cautious about while working with random numbers is that they 
might not produce the same result, most of the times. It is very likely that you will 
get a number different than what is mentioned in this text and it is alright as long as 
you grasp the underlying concept.
To calculate the coefficients, we will create a few more columns in the df data frame 
that is already defined, as we did while calculating the value of R2. Just to reiterate, 
here are the formulas again:
(
)(
)
(
)
2
/
Xi
Xm
Yi
Ym
Xi
Xm
β = ∑
−
−
∑
−
Ym
Xm
α
β
=
−
∗
We write the following code snippet to calculate these values:
xmean=np.mean(df['Input_Variable(X)'])
ymean=np.mean(df['Actual_Output(yact)'])
df['beta']=(df['Input_Variable(X)']-xmean)*(df['Actual_Output(yact)']-
ymean)
df['xvar']=(df['Input_Variable(X)']-xmean)**2
betan=df.sum()['beta']
betad=df.sum()['xvar']
beta=betan/betad
alpha=ymean-(betan/betad)*xmean
beta,alpha
If you go through the code carefully, you will find out that betan and betad are 
the numerators and denominators of the formula to calculate beta. Once, beta is 
calculated, getting alpha is a cakewalk. The snippet outputs the value of alpha and 
beta. For my run, I got the following values:
1.982,
0.318
α
β
=
=

Linear Regression with Python
[ 318 ]
As we can see, the values are a little different from what we had assumed earlier, 
that is a=2 and β=.3. Let us see how the value of R2 changes if we use the values 
predicted by the model consisting of these parameters. The equation for the model 
can be written as:
mod
1.98
0.31
y
el
x
=
+
∗
Let us create a new column in our df data frame to accommodate the values 
generated by this equation and call this ymodel. To do that we write the following 
code snippet:
df['ymodel']=beta*df['Input_Variable(X)']+alpha
Let us calculate the value of R2 based on this column and see whether it has 
improved or not. To calculate that, we can reuse the code snippet we wrote earlier  
by replacing the Predicted_Output(ypred) variable with ymodel variable:
df['SSR']=(df['ymodel']-ymean)**2
df['SST']=(df['Actual_Output(yact)']-ymean)**2
SSR=df.sum()['SSR']
SST=df.sum()['SST']
SSR/SST
The value of new R2 comes out to be 0.667, a decent improvement from the value of 
0.57 when we assumed the values for a and β.
Let us also plot this new result derived from the model equation against the actual 
and our earlier assumed model, just to get a better visual understanding. We will 
introduce one more line to represent the model equation we just created:
%matplotlib inline
plt.plot(x,ypred)
plt.plot(x,df['ymodel'])
plt.plot(x,yact,'ro')
plt.plot(x,yavg)
plt.title('Actual vs Predicted vs Model')
The graph looks similar to the following figure. As we can see, the ymodel and ypred 
are more or less overlapping, as the values of a and β are not very different.

Chapter 5
[ 319 ]
Legend: Blue line –ypred, green line – ymodel, red line – ymean, red dots -x
Fig. 5.5: Actual vs Predicted vs Fitted line from the dummy dataset where model coefficients have been 
calculated rather than assumed
Making sense of result parameters
Apart from the R2 statistic, there are other statistics and parameters that one needs to 
look at in order to do the following:
1.	 Select some variables and discard others for the model.
2.	 Assess the relationship between the predictor and output variable and check 
whether a predictor variable is significant in the model or not.
3.	 Calculate the error in the values predicted by the selected model.
Let us now see some of the statistics which helps to address the issues  
discussed earlier.
p-values
One thing to realize here is that the calculation of a and β are estimates and not the 
exact calculations. Whether their values are significant or not need to be tested using 
a hypothesis test.

Linear Regression with Python
[ 320 ]
The hypothesis tests whether the value of β is non-zero or not; in other words whether 
there is a sufficient correlation between X and yact. If there is, the β will be non-zero.
In the equation, y=a+β*x, if we put β=0, there will be no relation between y and x. 
Hence the hypothesis test is defined, as shown in the following:
:
0
:
0
Null hypothesis
Ho
Alternate Hypothesis
Ha
β
β
−
=
−
◊
So, whenever a regression task is performed and β is calculated, there will be 
an accompanying t-statistic and a p-value corresponding to this hypothesis test, 
calculated automatically by the program. Our task is to assume a significance level 
of our choice and compare this with the p-value. It will be a two-tailed test and if the 
p-value is less than the chosen significance level, then the null hypothesis that β=0  
is rejected.
If p-value for the t-statistic is less than the significance level, then the null-hypothesis 
is rejected and β is taken to be significant and non-zero. The values of p-value larger 
than the significance level demonstrate that β is not very significant in explaining the 
relationship between the two variables. As we see in the case of multiple regression 
(multiple input variables/predictors), this fact can be used to weed out unwanted 
columns from the model. The higher the p-value, the less significant they are to the 
model and the less significant ones can be weeded out first.
F-statistics
When one moves from a simple linear regression to a multiple regression, there will 
be multiple βs and each of them will be an estimate. In such a case, apart from testing 
the significance of the individual variables in the model by checking the p-values 
associated with their estimation, it is also required to check whether, as a group all 
the predictors are significant or not. This can be done using the following hypothesis:
1
2
3
:
0
:
0
n
i
Null hypothesis
Ho
Alternate Hypothesis
Ha Oneof the
is not equal to
β
β
β
β
β
−
=
=
=
=
=
−
……
The statistic that is used to test this hypothesis is called the F-statistic and is defined 
as follows:
(
)
(
)
/
/
1
SST
SSD
p
F
statistic
SSD
n
p
−
−
=
−
−

Chapter 5
[ 321 ]
Where SST and SSD have been defined earlier as:
(
)
(
)
2
2
SST
yact
yavg
SSD
yact
ypred
= ∑
−
= ∑
−
where n=number of rows in the dataset; p- number of predictor variables in the model
The F-statistics follows the F-distribution. There will be a p-value that is associated 
with this F-statistic. If this p-value is small enough (smaller than the significance 
level chosen), the null hypothesis can be rejected.
The significance of F-statistic is as follows:
•	
p-values are about individual relationships between one predictor and one 
outcome variable. In case of more than one predictor variable, one predictor's 
relationship with the output might get changed due to the presence of other 
variables. The F-statistics provides us with a way to look at the partial change 
in the associated p-value because of the addition of a new variable.
•	
When the number of predictors in the model is very large and all the βi 
are very close to 0, the individual p-values associated with the predictors 
might give very small values. In such a case, if we rely solely on individual 
p-values, we might incorrectly conclude that there is a relationship between 
the predictors and the outcome, when it is not there actually. In such cases, 
we should look at the p-value associated with the F-statistic.
Residual Standard Error
Another concept to learn is the concept of Residual Standard Error (RSE). It is 
defined as:
(
)
2
1
1
mod
2
n
i
RSE
yact
y
el
n
=
=
∗∑
−
−
 and 
(
)
2
1
mod
n
i
SSD
yact
y
el
=
= ∑
−
So, RSE can be written as 
1
2
RSE
SSD
n
=
∗
−
 for a simple linear regression model.
Where n=number of data points. In general, 
1
1
RSE
SSD
n
p
=
∗
−
−
 where p=number of 
predictor variables in the model.

Linear Regression with Python
[ 322 ]
The RSE is an estimate of the standard deviation of the error term (res). This is the 
error that is inevitable even if the model coefficients are known correctly. This may 
be the case because the model lacks something else, or may be another variable in  
the model (we have just looked at one variable regression till now, but in most of  
the practical scenarios we have to deal with multiple regression, where there  
would be more than one input variable. In multiple regressions, values of the RSE 
generally go down, as we add more variables that are more significant predictors of 
the output variable).
The RSE for a model can be calculated using the following code snippet. Here, we are 
calculating the RSE for the data frame we have used for the model, df:
df['RSE']=(df['Actual_Output(yact)']-df['ymodel'])**2
RSEd=df.sum()['RSE']
RSE=np.sqrt(RSEd)/98
RSE
The value of the RSE comes out to be 0.46 in this case. As you might have guessed, 
the smaller the RSE, the better the model is. Again, the benchmark to compare this 
error is the mean of the actual values, yact. As we have seen earlier, this value is 
ymean=2.53. So, we will observe an error of 0.46 over 2.53 that amounts to around an 
18% error.
Implementing linear regression with 
Python
Let's now go ahead and try to make a simple linear regression model and see what 
are the issues that we face and how can they be resolved to make the model more 
robust. We will use the advertising data that we used earlier for illustrating  
the correlation.
The following two methods implement linear regression in Python:
•	
The ols method and the statsmodel.formula.api library
•	
The scikit-learn package
Let's implement a simple linear regression using the first method and then build 
upon a multiple-linear regression model. We will then also look at how the second 
method is used to do the same.

Chapter 5
[ 323 ]
Linear regression using the statsmodel library
Let's first import the Advertising data, as shown:
import pandas as pd
advert=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Linear Regression/Advertising.csv')
advert.head()
To reiterate, this dataset contains data about the advertising budget spent on TV, 
Radio, and Newspapers, for a particular product and the resulting sales. We will 
expect a positive correlation between such advertising costs and sales. We have 
already seen that there is a good correlation between TV advertising costs and sales. 
Let's see whether it is present or not. If yes, how does the relationship look like and 
to do that we write the following code:
import statsmodels.formula.api as smf
model1=smf.ols(formula='Sales~TV',data=advert).fit()
model1.params
In this code snippet, we have assumed a linear relationship between advertising 
costs on TV and sales. We have also created a best fit using the least sum of square 
method. This snippet will output the values for model parameters that is a and β. 
The following is the output:
In the notation that we have been using, a is the intercept and β is the slope. Thus:
7.03
0.047
a
and β
=
=
The equation for the model will be:
7.032
0.047
Sales
TV
=
+
∗
The equation implies that an increase of 100 units in advertising costs will increase 
the sale by four units.

Linear Regression with Python
[ 324 ]
If you remember, we learnt that the values of these parameters are estimates and 
there will be a p-value associated to these. If the p-values are very small, then it 
can be accepted that these parameters have a non-zero value and are statistically 
significant in the model. Let's have a look at the p-values for these parameters:
model1.pvalues
As it can be seen, the p-values are very small; hence, the parameters are significant.
Let's also check another important indicator of the model efficacy and that is R2. As 
we saw earlier, there is a ready-made method for doing this. This can be done by 
typing the following code line:
model1.rsquared
The value comes out to be 0.61.
If we want the entire important model parameters at one go, we can take a look at 
the model summary by writing this snippet:
model1.summary()
The result is as follows:
Fig. 5.6: Model 1 Summary

Chapter 5
[ 325 ]
As we can see, the F-statistic for this model is very high and the associated p-value is 
negligible, suggesting that the parameter estimates for this model were all significant 
and non-zero.
Let's now predict the values of sales based on the equation we just derived. This can 
be done using the following snippet:
sales_pred=model1.predict(pd.DataFrame(advert['TV']))
sales_pred
This equation basically calculates the predicted sales value for each row based on the 
model equation using TV costs. One can plot sales_pred against the TV advertising 
costs to find the line of best fit. Let's do that:
import matplotlib.pyplot as plt
%matplotlib inline
advert.plot(kind='scatter', x='TV', y='Sales')
plt.plot(pd.DataFrame(advert['TV']),sales_pred,c='red',linewidth=2)
We get the following plot as the output. The red line is the line of best fit (obtained 
from the model). The blue dots represent the actual data present:
Fig. 5.7: Line of best fit (obtained from the model) and the scatter plot of actual data

Linear Regression with Python
[ 326 ]
Now, let's calculate the RSE term for our prediction using the following code snippet:
advert['sales_pred']=0.047537*advert['TV']+7.03
advert['RSE']=(advert['Sales']-advert['sales_pred'])**2
RSEd=advert.sum()['RSE']
RSE=np.sqrt(RSEd/198)
salesmean=np.mean(advert['Sales'])
error=RSE/salesmean
RSE,salesmean,error
The output consists of three numbers, first of which is RSE=3.25, second is 
salesmean (mean of actual sales) = 14.02 and error is their ratio, which is equal 
to 0.23. Thus, on an average this model will have 23%, even if the coefficients are 
correctly predicted. This is a significant amount of errors and we would like to bring 
it down by some means. Also, the R2 value of 0.61 can be improved upon. One thing 
we can try is to add more columns in the model, as predictors and see whether it 
improves the result or not.
Multiple linear regression
When linear regression involves more than one predictor variable, then it is called 
multiple linear regression. The nature of the model remains the same (linear),  
except that there might be separate slope (β) coefficients associated with each  
of the predictor variables. The model will be represented, as follows:
1
1
2
2
3
3
mod
n
n
y
el
X
X
X
X
α
β
β
β
β
=
+
+
+
+
……
Each βi will be estimated using the same least sum of squares method; hence, would 
have a p-value associated with the estimation. The smaller the p-value associated 
with a variable, the more the significance of that variable to the model. The variables 
with large p-values should be eliminated from the model as they aren't good 
predictors of the output variable.
While the multiple regression gives us with the possibility of using more variables 
as predictors; hence, it increases the efficiency of the model. It also increases the 
complexity of the process of model building, as the selection of the variables to be 
kept and discarded in the model becomes tedious.
With this simple dataset of three predictor variables, there can be seven possible 
models. They are as follows:
•	
Model 1: Sales~TV
•	
Model 2: Sales~Newspaper
•	
Model 3: Sales~Radio

Chapter 5
[ 327 ]
•	
Model 4: Sales~TV+Radio
•	
Model 5: Sales~TV+Newspaper
•	
Model 6: Sales~Newspaper+Radio
•	
Model 7: Sales~TV+Radio+Newspaper
For a model with p possible predictor variables, there can be 2p-1 possible models; 
hence, as the number of predictors increases, the selection becomes tedious.
It would have been a tedious task to choose from so many possible models. 
Thankfully, there are a few guidelines to filter some of these and then navigate 
towards the most efficient one. The following are the guidelines:
•	
Keep the variables with low p-values and eliminate the ones with high 
p-values
•	
Inclusion of a variable to the model should ideally increase the value of R2 
(although it is not a very reliable indicator of the same and looking at the 
adjusted R2 is preferred. The concept of adjusted R2 and why it is a better 
indicator than R2 will be explained later in this chapter).
Based on these guidelines, there are two kinds of approaches to select the predictor 
variables to go in the final model:
•	
Forward selection: In this approach, we start with a null model without any 
predictor and then start adding predictor variables one by one. The variable 
whose addition results into a model with the lowest residual sum of squares 
will be added first to the model. If the p-value for the variable is small 
enough and the value of the adjusted R2 goes up; the predictor variable is 
included in the model. Otherwise, it is not included in the model.
•	
Backward selection: In this approach, one starts with a model that has all the 
possible predictor variables in the model and discards some of them. If the 
p-value of a predictor variable is large and the value of the adjusted R2 goes 
up, the predictor variable is discarded from the model. Otherwise, it remains 
a part of the model.
Many of the statistical programs, including the Python, give us an option to  
select from the two preceding approaches while implementing a linear  
regression. The statistical program then implements the linear regression  
using the selected approach.
For now, let us manually add a few variables and see how it changes the model 
parameters and efficacy, so that we can get a better glimpse of what goes on behind 
the curtain when these approaches are implemented by the statistical program.

Linear Regression with Python
[ 328 ]
We have already seen one model assuming a linear relationship between sales and 
TV advertising costs. We can ignore the other models consisting of single variables 
(that is newspaper and radio, as they have a small correlation compared to TV). Let 
us now try to add more variables to the model we already have and see how the 
parameters and efficacy change.
Let us try adding the newspaper variable to the model using the following  
code snippet:
import statsmodels.formula.api as smf
model2=smf.ols(formula='Sales~TV+Newspaper',data=advert).fit()
model2.params
The following are the results:
Fig. 5.8: Model 2 coefficients and p-values
The p-values for the coefficients are very small, suggesting that all the estimates are 
significant. The equation for this model will be:
5.77
0.046
0.04
Sales
TV
Newspaper
=
+
∗
+
∗
The values of R2 and adjusted R2 are 0.646 and 0.642, which is just a minor 
improvement from the value obtained in the earlier model.
The values can be predicted using the following snippet:
sales_pred=model2.predict(advert[['TV','Newspaper']])
sales_pred
To calculate the RSE, we modify the snippet a little:
import numpy as np
advert['sales_pred']=5.77 + 0.046*advert['TV'] + 
0.04*advert['Newspaper']
advert['RSE']=(advert['Sales']-advert['sales_pred'])**2
RSEd=advert.sum()['RSE']
RSE=np.sqrt(RSEd/197)
salesmean=np.mean(advert['Sales'])
error=RSE/salesmean
RSE,salesmean,error

Chapter 5
[ 329 ]
The value of RSE comes out to be 3.12 (22%), not very different from the model with 
only TV. The number 197 comes from the (n-p-1) term in the formula for RSE, where 
n=200, p=2 for the current model. The following table is the model summary:
Fig. 5.9: Model 2 Summary
Although as the F-statistic decreases, the associated p-value also decreases. But, it 
is just a marginal improvement to the model, as we can see in the adj. R2 value. So, 
adding newspaper didn't improve the model significantly.
Let's try adding radio to the model instead of the newspaper. Radio had the second 
best correlation with the Sales variable in the correlation matrix we created earlier 
in the chapter. Thus, one expects some significant improvement in the model upon 
its addition to the model. Let's see if that happens or not:
import statsmodels.formula.api as smf
model3=smf.ols(formula='Sales~TV+Radio',data=advert).fit()
model3.params
The output parameters and the associated p-values of this model are, as follows:
Fig. 5.10: Model 2 coefficients and p-values
The model can be represented as the following:
2.92
0.045
0.18
Sales
TV
Radio
=
+
∗
+
∗

Linear Regression with Python
[ 330 ]
The values can be predicted based on the preceding model using the  
following snippet:
sales_pred=model3.predict(advert[['TV','Radio']])
sales_pred
The model summary looks something similar to the following screenshot:
Fig. 5.11: Model 3 summary
One thing to observe here is that the R2 value has improved considerably due to the 
addition of radio to the model. Also, the F-statistic has increased significantly from 
the last model indicating a very efficient model.
The RSE can be calculated using the same method described previously. The value 
for this model comes out to be 1.71 (around 12%),which is much better than the 23% 
and 22% in the previous model.
Thus, we can conclude that radio is a great addition to the model and TV and radio 
advertising costs have been able to describe the sales very well and this model itself 
is a very efficient model. But, can we improve it a bit further by combining all three 
predictor variables?
The last thing that we should try is, all the predictor variables together by using the 
following code:
import statsmodels.formula.api as smf
model4=smf.ols(formula='Sales~TV+Radio+Newspaper',data=advert).fit()
model4.params

Chapter 5
[ 331 ]
The estimates of the coefficients and the associated p-values for this model will be  
as follows:
Fig. 5.12: Model 4 coefficients and p-values
The p-values for the coefficients are very small, suggesting that all the estimates are 
significant. The equation for this model will be:
2.93
0.045
0.18
0.01
Sales
TV
Radio
Newspaper
=
+
∗
+
∗
−
∗
The values of sales can be predicted using the following snippet:
sales_pred=model4.predict(advert[['TV','Radio','Newspaper']])
sales_pred
The summary of the model is shown in the following table:
Fig. 5.13: Model 4 summary
The most striking feature of this model is that the estimate of the coefficients is 
very similar to that in the previous model. The intercept, coefficient for TV, and the 
coefficient for Radio are more or less the same. The values of R2 and adj-R2 are also 
similar to the previous model.

Linear Regression with Python
[ 332 ]
The value of RSE can be calculated in a similar way, as before. The value comes out 
to 2.57 (18%), which is more than the previous model.
Other things to note about this model are the following:
•	
There is a small negative coefficient for the newspaper. When we considered 
only TV and newspaper in the model, the coefficient of the newspaper was 
significantly positive. Something affected the coefficient of the newspaper 
when it became a part of the model in presence of TV and radio.
•	
For this model, the F-statistic has decreased considerably to 570.3 from 
859.6 in the previous model. This suggests that the partial benefit of adding 
newspaper to the model containing TV and radio is negative.
•	
The value of RSE increases on addition of newspaper to the model.
All these point in the direction that the model actually became a little less efficient on 
addition of newspaper to the previous model. What is the reason?
Multi-collinearity
Multi-collinearity is the reason for the sub-optimal performance of the model when 
newspaper was added to the final model. Multi-collinearity alludes to the correlation 
between the predictor variables of the model.
These are some of the signs of a common problem encountered during a regression 
called multi-collinearity. Go back a few pages to the correlation matrix that we 
created for this dataset and you will find that there is a significant correlation of 0.35 
between radio and newspaper. This means that the expense on Newspaper is related 
to that on the Radio. This relationship between the predictor variable increases the 
variability of the co-efficient estimates of the related predictor variables.
The t-statistic for these coefficients is calculated by dividing the mean value by the 
variability (or error). As this error goes up, the value of t-statistic goes down and 
the value of p-value increases. Thus, the chances that the null hypothesis for the 
hypothesis test associated with the F-statistic will be accepted are increased. This 
decreases the significance of the variable in the model.
(
)
(
)
0 /
m
m
t
statistic
SE
β
β
−
=
−
Where βm=mean of estimates of β, SE(βm)=variability in the estimate of β.
Thus collinearity is an issue that needs to be taken care of. For highly correlated 
predicted variables, we need to do a deep-dive with these variables and see whose 
inclusion in the model makes the model more efficient.

Chapter 5
[ 333 ]
It is a good practice to identify the pair of predictor variables with high correlation, 
using the correlation matrix and check the pairs of multi-collinearity effect on  
the model. The culprit variables should be removed from the model. The VIF  
is a method to tackle this issue.
Variance Inflation Factor
A fool-proof way to detect this menace called multi-collinearity is a statistic called 
Variance Inflation Factor (VIF). It is a method to quantify the rise in the variability 
of the coefficient estimate of a particular variable because of high correlation between 
two or more than two predictor variables. The VIF needs to be calculated for each 
of the variables and if the value is very high for a particular variable, then that 
predictor needs to be eliminated from the model. Some statistical processes calculate 
VIF when fed with the option to do so. The following process goes under the hood 
for calculation of VIF:
1.	 Write Xi as a linear function of other predictor variables:
1
1
2
2
1
1
1
1
i
i
i
i
i
n
n
X
a X
a X
a
X
a
X
a X
−
−
+
+
=
+
+
+
+
+
+
……
……
……
2.	 Calculate the coefficient of determination for this model and call it Ri
2.  
The VIF for Xi is given by:
2
1
1
VIF
Ri
= −
3.	 If the VIF=1, then the variables are not correlated. If 1<VIF<5, then the 
variables are moderately correlated with other predictor variables and can 
still be part of the model. If VIF>5, then variables are highly correlated and 
need to be eliminated from the model.
Let us write a short snippet to calculate the VIF to understand the calculation better:
model=smf.ols(formula='Newspaper~TV+Radio',data=advert).fit()
rsquared=model.rsquared 
VIF=1/(1-rsquared)
VIF
This will give a VIF for the Newspaper. By changing the formula in the snippet,  
we can calculate the VIF for the other variables. The following are the values:
Newspaper
Radio
TV
1.14
1.14
1.04

Linear Regression with Python
[ 334 ]
The newspaper and TV have almost the same VIF, indicating that they are correlated 
to just one another and not the TV.
In this case, radio and newspaper are correlated. However, the model with TV and 
radio, as predictor variables, are far superior to the model with TV and newspaper 
as the predictor variables. The model with the all the three variables as predictors 
doesn't improve the model much. In fact, it increases the variability and the 
F-statistic. It will be a decent choice to drop the newspaper variable from the  
model and pick model 3 as the best candidate for the final model:
Model validation
Any predictive model needs to be validated to see how it is performing on different 
sets of data, whether the accuracy of the model is constant over all the sources of 
similar data or not. This checks the problem of over-fitting, wherein the model fits 
very well on one set of data but doesn't fit that well on another dataset. One common 
method is to validate a model train-test split of the dataset. Another method is k-fold 
cross validation, about which we will learn more in the later chapter.
Training and testing data split
Ideally, this step should be done right at the onset of the modelling process so that 
there are no sampling biases in the model; in other words, the model should perform 
well even for a dataset that has the same predictor variables, but their means and 
variances are very different from what the model has been built upon. This can 
happen because the dataset on which the model is built (training) and the one on 
which it is applied (testing) can come from different sources. A more robust way to 
do this is a process called the k-fold cross validation, about which we will read in 
detail in a little while.
Let's see how we can split the available dataset in the training and testing dataset 
and apply the model to the testing dataset to get other results:
import numpy as np
a=np.random.randn(len(advert))
check=a<0.8
training=advert[check]
testing=advert[~check]
The ratio of split between training and testing datasets is 80:20; in other words,  
160 rows of the advert dataset will be in training and 40 rows in testing.

Chapter 5
[ 335 ]
Let's create a model on training the data and test the model performance on the 
testing data. Let us create the only model that works best (we have found it already), 
the one with TV and radio variables, as predictor variables:
import statsmodels.formula.api as smf
model5=smf.ols(formula='Sales~TV+Radio',data=training).fit()
model5.summary()
Fig. 5.14: Model 5 coefficients and p-values
Most of the model parameters, such as intercept, coefficient estimates, and R2 are 
very similar. The difference in F-statistics can be attributed to a smaller dataset.  
The smaller the dataset, the larger the value of SSD and the smaller the value of 
the (n-p-1) term in F-statistic formula; both contribute towards the decrease in the 
F-statistic value.
The model can be written, as follows:
2.86
0.04
0.17
Sales
TV
Radio
+
∗
+
∗
∼

Linear Regression with Python
[ 336 ]
Let us now predict the sales values for the testing dataset:
sales_pred=model5.predict(training[['TV','Radio']])
sales_pred
The value of RSE for this prediction on the testing dataset can be calculated using the 
following snippet:
import numpy as np
testing['sales_pred']=2.86 + 0.04*testing['TV'] + 
0.17*testing['Radio']
testing['RSE']=(testing['Sales']-testing['sales_pred'])**2
RSEd=testing.sum()['RSE']
RSE=np.sqrt(RSEd/51)
salesmean=np.mean(testing['Sales'])
error=RSE/salesmean
RSE,salesmean,error
The value of RSE comes out to be 2.54 over a sales mean (in the testing data) of 14.80 
amounting to an error of 17%.
We can see that the model doesn't generalize very well on the testing dataset, as the 
RSE for the same model is different in the two cases. It implies some degree of over 
fitting when we tried to build the model based on the entire dataset. The RSE with 
the training-testing split, albeit a bit more, is more reliable and replicable.
Summary of models
We have tried four models previously. Let us summarize the major results from each 
of the models, at one place:
Name 
Definition
R2/Adj-R2
F-statistic
 F-statistic 
(p-value)
RSE
Model 1
Sales ~ TV
0.612/0.610
312.1
1.47e-42
3.25 (23%)
Model 2
Sales ~ TV+Newspaper
0.646/0.642
179.6
3.95e-45
3.12(22%)
Model 3
Sales ~ TV+Radio
0.897/0.896
859.6
4.83e-98
1.71(12%)
Model 4
Sales ~ TV+Radio+Newspaper
0.897/0.896
570.3
1.58e-96
1.80(13%)
Guide for selection of variables

Chapter 5
[ 337 ]
To summarize, for a good linear model, the predictor variables should be chosen 
based on the following criteria:
•	
R2: R2 will always increase when you add a new predictor variable to the 
model. However, it is not a very reliable check of the increased efficiency of 
the model. Rather, for an efficient model, we should check the adjusted-R2. 
This should increase on adding a new predictor variable.
•	
p-values: The lower the p-value for the estimate of the predictor variable,  
the better it is to add the predictor variable to the model.
•	
F-statistic: The value of the F-statistic for the model should increase after 
the addition of a new predictor variable for a predictor variable to be an 
efficient addition to the model. The increase in the F-statistic is a proxy to 
the improvement in the model brought upon solely by the addition of that 
particular variable. Alternatively, the p-value associated with the F-statistic 
should decrease on the addition of a new predictor variable.
•	
RSE: The value of RSE for the new model should decrease on the addition of 
the new predictor variable.
•	
VIF: To take care of the issues arising due to multi-collinearity one needs to 
eliminate the variables with large values of VIF.
Linear regression with scikit-learn
Let's now re-implement the linear regression model using the scikit-learn 
package. This method is more elegant as it has more in-built methods to perform 
the regular processes associated with regression. For example, you might remember 
from the last chapter that there is a separate method for splitting the dataset into 
training and testing datasets:
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import train_test_split
feature_cols = ['TV', 'Radio']
X = advert[feature_cols]
Y = advert['Sales']
trainX,testX,trainY,testY = train_test_split(X,Y, test_size = 0.2)
lm = LinearRegression()
lm.fit(trainX, trainY)
We split the advert dataset into train and test dataset and built the model on TV and 
radio variables from the test dataset. The following are the parameters of the model:
print lm.intercept_
print lm.coef_

Linear Regression with Python
[ 338 ]
The result is as follows: Intercept – 2.918, TV coefficient – 0.04, Radio  
coefficient – 0.186
A better way to look at the coefficients is to use the zip method to write the variable 
name and coefficient together. The required snippet and the output are mentioned in 
the following code:
zip(feature_cols, lm.coef_)
[('TV', 0.045706061219705982), ('Radio', 0.18667738715568111)]
The value of R2 is calculated by typing the following code:
lm.score(trainX, trainY)
The value comes out to be around 0.89, very close to the value obtained by the 
method used earlier.
The model can be used to predict the value of sales using TV and radio variables 
from the test dataset, as follows:
lm.predict(testX)
Feature selection with scikit-learn
As stated before, many of the statistical tools and packages have in-built methods to 
conduct a variable selection process (forward selection and backward selection). If 
it is done manually, it will consume a lot of time and selecting the most important 
variables will be a tedious task compromising the efficiency of the model.
One advantage of using the scikit-learn package for regression in Python is 
that it has this particular method for feature selection. This works more or less like 
backward selection (not exactly) and is called Recursive Feature Elimination (RFE). 
One can specify the number of variables they want in the final model.
The model is first run with all the variables and certain weights are assigned to all 
the variables. In the subsequent iterations, the variables with the smallest weights  
are pruned from the list of variables till the desired number of variables is left.
Let us see how one can do a feature selection in scikit-learn:
from sklearn.feature_selection import RFE
from sklearn.svm import SVR
feature_cols = ['TV', 'Radio','Newspaper']
X = advert[feature_cols]
Y = advert['Sales']

Chapter 5
[ 339 ]
estimator = SVR(kernel="linear")
selector = RFE(estimator,2,step=1)
selector = selector.fit(X, Y)
We use the methods named RFE and SVR in-built in scikit-learn. We indicate that 
we want to estimate a linear model and the number of desired variables in the model 
to be two.
To get the list of selected variables, one can write the following code snippet:
selector.support_
It results in an array mentioning whether the variables in X have been selected for 
the model or not. True means that the variable has been selected, while False means 
otherwise. In this case, the result is as follows:
Fig. 5.15: Result of feature selection process
In our case, X consists of three variables: TV, radio, and newspaper. The preceding 
array suggests that TV and radio have been selected for the model, while the 
newspaper hasn't been selected. This concurs with the variable selection we had 
done manually.
This method also returns a ranking, as described in the following example:
selector.ranking_
Fig. 5.16: Result of feature selection process
All the selected variables will have a ranking of 1 while the subsequent ones will be 
ranked in descending order of their significance. A variable with rank 2 will be more 
significant to the model than the one with a rank of 3 and so on.
Handling other issues in linear 
regression
So far in this chapter, we have learnt:
•	
How to implement a linear regression model using two methods
•	
How to measure the efficiency of the model using model parameters

Linear Regression with Python
[ 340 ]
However, there are other issues that need to be taken care of while dealing with 
data sources of different types. Let's go through them one by one. We will be using a 
different (simulated) dataset to illustrate these issues. Let's import it and have a look 
at it:
import pandas as pd
df=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Linear Regression/Ecom Expense.csv')
df.head()
We should get the following output:
Fig. 5.17: Ecom Expense dataset
The preceding screenshot is a simulated dataset from any-commerce website. It 
captures the information about several transactions done on the website. A brief 
description of the column names of the dataset is, as follows:
•	
Transaction ID: Transaction ID for the transaction
•	
Age: Age of the customer
•	
Items: Number of items in the shopping cart (purchased)
•	
Monthly Income: Monthly disposable income of the customer
•	
Transaction Time: Total time spent on the website during the transaction
•	
Record: How many times the customer has shopped with the website in  
the past
•	
Gender: Gender of the customer
•	
City Tier: Tier of the city
•	
Total Spend: Total amount spent in the transaction
The output variable is the Total Spend variable. The others are potential predictor 
variables and we suspect that the Total Spend is linearly related to all these 
predictor variables.

Chapter 5
[ 341 ]
Handling categorical variables
Until now, we have assumed that the predictor variables can only be quantitative or 
numerical, but we know from real-life experiences that most of the times the dataset 
contains a categorical or qualitative variable and many of the times these variables 
will have a significant impact on the value of the output. However, the question is 
how to process these variables, so as to use them in the model?
We can't assign them values, such as 0, 1, 2, and so on, and then use them in the 
model, as it will give undue weightage to the categories because of the numbers 
assigned to them. Most of the time it might give a wrong result and will change,  
as the number assigned to a particular category changes.
In the data frame we just imported, Gender and City Tier are the categorical 
variables. In Chapter 2, Data Cleaning, we learnt how to create dummy variables from 
a categorical variable. That was exactly for this purpose. Let's see how it works and 
why it is required.
A linear regression is of the form:
1
1
2
2
3
3
mod
n
n
y
el
X
X
X
X
d
α
β
β
β
β
=
+
+
+
+
+
……
Where one or more of the Xi's can be categorical. Let's say Xm is that variable. For 
such a variable, we can define another dummy variable (if it has only two categories 
as in the case of Gender), such that:
1,
0,
Xg
if customer is male
if customer is female
=
=
The model then becomes:
1
1
2
2
3
3
mod
1 1
2
2
3
3
,
mod
,
n
n
y
el
X
X
X
g
nXn
d if customer is male
y
el
X
X
X
X
d if customer is female
α
β
β
β
β
β
α
β
β
β
β
=
+
+
+
+
+
+
=
+
+
+
+
+
……
……
……
If there are three levels in the categorical variable, then one needs to define two 
variables as compared to 1 when there were two levels in the categorical variable. 
For example, City Tier variable has three levels in our dataset.
For this, we can define two variables, such that:
1
2
1,
1
2,
2
0,
1
0,
2
t
t
X
if CityisTier
X
if CityisTier
if Cityis notTier
if Cityis notTier
=
=
=

Linear Regression with Python
[ 342 ]
The model then becomes:
1
1
2
2
3
3
1
1
1
1
2
2
3
3
2
2
1
1
2
2
3
3
mod
.
,
1
mod
.
,
2
mod
.
,
3
t
t
n
n
t
t
n
n
n
n
y
el
X
X
X
X
X
d if customer is fromtier city
y
el
X
X
X
X
X
d if customer is fromtier city
y
el
X
X
X
X
d if customer is fromtier city
α
β
β
β
β
β
α
β
β
β
β
β
α
β
β
β
β
=
+
+
+
+
+
+
+
=
+
+
+
+
+
+
+
=
+
+
+
+
+
+
…
……
…
……
…
Note that one doesn't have to create the third variable. This is 
because of the nature in which these variables are defined. If a 
customer doesn't belong to tier 1 or tier 2 city, then he will certainly 
belong to a tier 3 city. Hence, no one variable is required for one of 
the levels. In general, for categorical variables having n levels, one 
should create (n-1) dummy variables.
The process of creating dummy variables has been already enumerated in  
Chapter 2, Data Cleaning. Let's now create the dummy variables for both our 
categorical variables and then add them to our data frame, as shown:
dummy_gender=pd.get_dummies(df['Gender'],prefix='Sex')
dummy_city_tier=pd.get_dummies(df['City Tier'],prefix='City')
Let's see how they look and whether they satisfy the conditions we have defined 
earlier or not. This is how the dummy_city_tier looks:
Fig. 5.18: City Tier dummy variables

Chapter 5
[ 343 ]
The dummy_gender looks similar to the following table:
Fig. 5.19: Gender dummy variables
Now, we have these dummy variables created but they are not a part of the main 
data frame yet. Let's attach these new variables to the main data frame so that they 
can be used in the model:
column_name=df.columns.values.tolist()
df1=df[column_name].join(dummy_gender)
column_name1=df1.columns.values.tolist()
df2=df1[column_name1].join(dummy_city_tier)
df2
Fig. 5.20: Ecom Expense dataset with dummy variables
There are five new columns in the data frame, two from the Gender dummy 
variables and three from the City Tier dummy variables.
If you compare it with the entire dataset, the City_Tier_1 has value 1 if the City_Tier 
has value Tier 1, the City_Tier_2 has value 1 if the City_Tier has value Tier 2 and 
the City_Tier_3 has value 1 if the City_Tier has value Tier 3. All the other dummy 
variables in that particular row will have values 0. This is what we wanted.

Linear Regression with Python
[ 344 ]
Let's see how to include these dummy variables in the model and how to assess  
their coefficients.
For the preceding dataset, let's assume a linear relationship between the output 
variable Total Spend and the predictor variables: Monthly Income and 
Transaction Time, and both set of dummy variables:
from sklearn.linear_model import LinearRegression
feature_cols = ['Monthly Income','Transaction Time','City_Tier 
1','City_Tier 2','City_Tier 3','Sex_Female','Sex_Male']
X = df2[feature_cols]
Y = df2['Total Spend']
lm = LinearRegression()
lm.fit(X,Y)
The model parameters can be found out, as follows:
print lm.intercept_
print lm.coef_
zip(feature_cols, lm.coef_)
The following is the output we get:
Fig. 5.21: Coefficients of the model
The R2 for this model can be found out by writing the following:
lm.score(X,Y)
The value comes out to be 0.19, which might be because we haven't used the other 
variables and the output might be related to them as well. We need to fine-tune the 
model by suitably transforming some of the variables and adding them to the model. 
For example, if you add Record variable to the model, the R2 jumps to 0.91 (try that 
on your own). It is a good dataset to play with.
The model can be written, as follows:
Total_Spend=3655.72 + 0.12*Transaction Time + 0.15*Monthly Income + 119*City_Tier 
1-16*City_Tier 2 - 102*City_Tier 3-94*Sex_Female+94*Sex_Male

Chapter 5
[ 345 ]
The RSE can be calculated, as follows:
import numpy as np
df2['total_spend_pred']=3720.72940769 + 0.12*df2['Transaction 
Time']+0.15*df2['Monthly Income']+119*df2['City_Tier 1']-16*df2['City_
Tier 2']
-102*df2['City_Tier 3']-94*df2['Sex_Female']+94*df2['Sex_Male']
df2['RSE']=(df2['Total Spend']-df2['total_spend_pred'])**2
RSEd=df2.sum()['RSE']
RSE=np.sqrt(RSEd/2354)
salesmean=np.mean(df2['Total Spend'])
error=RSE/salesmean
RSE,salesmean,error
The RSE comes out to be 2519 over a Total Spend mean of 6163, amounting to an 
error of around 40%, suggesting that there is a scope for improvement in the model.
However, the purpose of this section is to illustrate how the dummy variables are 
used in building a model and assessed in the final model.
As we can see, there are different coefficients for different dummy variables. For City 
Tier, City_Tier_1 has 119, City_Tier_2 has -16 and City_Tier_3 has -102. This means 
that on an average, everything else being same, a customer from a Tier 1 city will 
spend more than someone from Tier 2 and Tier 3 city. Someone from a Tier 2 city 
will spend less than someone from Tier 3. If we take City_Tier_1 as the baseline, the 
Total Spend is lesser by 135 units for a customer from Tier 2 city, while it is lesser by 
222 units for a customer from Tier 3 city.
For different Gender and City Tier, the model will be reduced to following for 
different cases:
Gender
City Tier
Model
Male
1
Total_Spend=3655.72 + 0.12*Transaction Time + 
0.15*Monthly Income + 119*City_Tier 1 +94*Sex_Male
Male
2
Total_Spend=3655.72 + 0.12*Transaction Time + 
0.15*Monthly Income -16*City_Tier 2 +94*Sex_Male
Male
3
Total_Spend=3655.72 + 0.12*Transaction Time + 
0.15*Monthly Income - 102*City_Tier 3 +94*Sex_Male
Female
1
Total_Spend=3655.72 + 0.12*Transaction Time + 
0.15*Monthly Income + 119*City_Tier 1 - 94*Sex_Female
Female
2
Total_Spend=3655.72 + 0.12*Transaction Time + 
0.15*Monthly Income -16*City_Tier 2 - 94*Sex_Female

Linear Regression with Python
[ 346 ]
One of the three dummy variables can be converted to baseline by masking it. 
Remember, we said earlier that only (n-1) dummy variables are needed for a 
categorical variable with n levels. However, here you are seeing three dummy 
variable for City Tier (three levels) and two dummy variables for Gender (two levels). 
This is just because it is easier to understand this way. There is only n-1 variables 
required for n-level categorical variables.
We can use (n-1) variables by masking one of the variables from the list of dummy 
variables going into the model. This masked variable will then become the baseline 
for the coefficients associated with these dummy variables.
Let's do that and see how the coefficients change:
dummy_gender=pd.get_dummies(df['Gender'],prefix='Sex').iloc[:, 1:]
dummy_city_tier=pd.get_dummies(df['City Tier'],prefix='City').iloc[:, 
1:]
column_name=df.columns.values.tolist()
df3=df[column_name].join(dummy_gender)
column_name1=df3.columns.values.tolist()
df4=df3[column_name1].join(dummy_city_tier)
df4
It is the same process of converting categorical variables to dummy variables, but  
we are masking the first variable from the resulting list using the iloc method  
of subsetting.
The resulting data frame has one dummy variable for Gender and two for City Tier 
and is similar to the following screenshot:
Fig. 5.22: Ecom expense dataset with only (n-1) dummy variables
Let's now use these variables into the model and see how the coefficients change:
from sklearn.linear_model import LinearRegression
feature_cols = ['Monthly Income','Transaction Time','City_Tier 
2','City_Tier 3','Sex_Male']
X = df2[feature_cols]
Y = df2['Total Spend']
lm = LinearRegression()
lm.fit(X,Y)

Chapter 5
[ 347 ]
The variables and their coefficients can be obtained in the same way as earlier.  
They are as follows:
print lm.intercept_
print lm.coef_
zip(feature_cols, lm.coef_)
Fig. 5.23: Coefficients of the model
As one might observe, the coefficients of City_Tier_2 and City_Tier_3 variables 
along with that of the Sex_Male variables have changed while that of all the others 
remain the same. The change in the coefficient doesn't change the model as such,  
but just the account for the absence of the baseline dummy variable. The new 
coefficient for City_Tier_2 is -136, which can be thought of as its coefficient when  
the City_Tier_1 has a coefficient of 0 (we saw earlier it has a coefficient of 119):
Variable
Coefficient earlier
Coefficient later
City_Tier_1
120
0
City_Tier_2
-16
-136 (-16-120)
City_Tier_3
-102
-222(-102-120)
Sex_Male
94
188 (94-(-94))
Sex_Female
-94
0
Transforming a variable to fit non-linear 
relations
Sometimes the output variable doesn't have a direct linear relationship with the 
predictor variable. They have a non-linear relationship. These relationships could 
be simple functions like quadratic, exponential, logarithm, or complex ones such as 
polynomials. In such cases, transforming the variable comes in very handy.

Linear Regression with Python
[ 348 ]
The following is a rough guideline about how to go about it:
•	
Plot a scatter plot of the output variable with each of the predictor  
variables. This can be thought of as a scatter plot matrix similar to  
the correlation matrix.
•	
If the scatter plot assumes more or less a linear shape for a predictor variable 
then it is linearly related to the output variable.
•	
If the scatter plot assumes a characteristic shape of any of the non-linear 
shapes for a predictor variable then transform that particular variable by 
applying that function.
Let's illustrate this with one example. We will use the Auto.csv dataset for this. 
This dataset contains information about miles per gallon (mpg) and horsepower for 
a number of car models and much more. The mpg is the predictor variable and is 
considered to be highly dependent on the horsepower of a car model.
Let's import the dataset and have a look at it before proceeding further:
import pandas as pd
data = pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Linear Regression/Auto.csv')
data.head()
This is how the dataset looks:
Fig. 5.24: Auto dataset
It has 406 rows and 9 columns. Some of the variables have NA values and it makes 
sense to drop the NA values before using them.
Now, let's plot a scatter plot between the horsepower and the mpg variables to see 
whether they exhibit a linear shape or some non-linear shape:
import matplotlib.pyplot as plt
%matplotlib inline
data['mpg']=data['mpg'].dropna()

Chapter 5
[ 349 ]
data['horsepower']=data['horsepower'].dropna()
plt.plot(data['horsepower'],data['mpg'],'ro')
plt.xlabel('Horsepower')
plt.ylabel('MPG (Miles Per Gallon)')
As can be seen in the output, the relationship doesn't seem to have a linear shape but 
rather assumes a non-linear shape; it is most probably an exponential or quadratic 
kind of relationship.
However, for the sake of comparison, let's try and fit a linear model for the 
relationship between mpg and horsepower first and then compare it with the  
scatter plot.
We are assuming that the model is:
0
1.
mpg
c
a horsepower
=
+
While it looks like that the model is something similar to:
2
0
1.
mpg
c
a horsepower
=
+
2
0
1
2
.
.
mpg
c
a horsepower
a horsepower
=
+
+
Fig. 5.25: Scatterplot of MPG vs Horsepower

Linear Regression with Python
[ 350 ]
The following code snippet will fit a linear model between horsepower and mpg 
variables. The NA values need to be dropped from the variables before they can 
be used in the model. Also simultaneously, let us create a model assuming a linear 
relationship between mpg and square of horsepower:
import numpy as np
from sklearn.linear_model import LinearRegression
X=data['horsepower'].fillna(data['horsepower'].mean())
Y=data['mpg'].fillna(data['mpg'].mean())
lm=LinearRegression()
lm.fit(X[:,np.newaxis],Y)
The linear regression method by default requires that X be an array of two 
dimensions. Using np.newaxis, we are creating a new dimension for it to  
function properly.
The line of best fit can be plotted by the following snippet:
import matplotlib.pyplot as plt
%matplotlib inline
plt.plot(data['horsepower'],data['mpg'],'ro')
plt.plot(X,lm.predict(X[:,np.newaxis]),color='blue')
The plot looks similar to the following graph. The blue line is the line of the best fit:
Fig. 5.26:The line of best fit (from the linear model) and the scatterplot
The R2 for this model can be obtained using the following snippet:
lm1.score(X[:,np.newaxis],Y)

Chapter 5
[ 351 ]
The value comes out to be 0.605.
Let's now calculate the RSE for this model in a different manner:
RSEd=(Y-lm.predict(X[:,np.newaxis]))**2
RSE=np.sqrt(np.sum(RSEd)/389)
ymean=np.mean(Y)
error=RSE/ymean
RSE,error
Here, we are using the predict method to calculate the predicted value from the 
model instead of writing them explicitly.
The value of RSE for this model comes out to be 5.14, which over a mean value of 
23.51 gives an error of 21%.
If the model is of the form mpg = co+a1.horsepower2, then it can be fitted after 
transforming the horsepower variable, as shown in the following snippet:
import numpy as np
from sklearn.linear_model import LinearRegression
X=data['horsepower'].fillna(data['horsepower'].
mean())*data['horsepower'].fillna(data['horsepower'].mean())
Y=data['mpg'].fillna(data['mpg'].mean())
lm=LinearRegression()
lm.fit(X[:,np.newaxis],Y)
The R2 value for this model comes out to be around 0.51 and there is a scope of 
improvement in this model. The RSE can be calculated in the same manner,  
as shown in the preceding section using the following code snippet:
type(lm.predict(X[:,np.newaxis]))
RSEd=(Y-lm.predict(X[:,np.newaxis]))**2
RSE=np.sqrt(np.sum(RSEd)/390)
ymean=np.mean(Y)
error=RSE/ymean
RSE,error,ymean
The value of RSE for this model comes out to be 10.51, which over a mean value  
of 23.51 gives an error of 45%. The RSE increased when we transformed the  
variable exponentially.
Thus, we need to look at some other method to fit this seemingly non-linear data. 
What about polynomial fits that are, as follows:
2
0
1
2
:
.
.
Model
mpg
c
a horsepower
a horsepower
=
+
+

Linear Regression with Python
[ 352 ]
This can be fitted using the PolynomialFeatures method in the scikit-learn 
library. In this model, we are assuming a polynomial relationship between mpg  
and horsepower:
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model
X=data['horsepower'].fillna(data['horsepower'].mean())
Y=data['mpg'].fillna(data['mpg'].mean())
poly = PolynomialFeatures(degree=2)
X_ = poly.fit_transform(X[:,np.newaxis])
clf = linear_model.LinearRegression()
clf.fit(X_, Y)
The PolynomialFeatures method used in this method automatically generates the 
powers (up to the specified degree) of the X variable using its transform feature. 
In this case, the R2 value comes out to be 0.688. The R2 value increased considerably 
when we introduced the polynomial regression.
The coefficients for this model come out to be, as follows:
print clf.intercept_
print clf.coef_
Fig. 5.27: Model coefficients
The model can be written as:
2
55.02
0.43
0.001
mpg
horsepower
horsepower
=
−
∗
+
∗
Let us increase the degree and see whether it increases the R2 further or not. One just 
needs to change the degree from 2 to 5.
from sklearn.preprocessing import PolynomialFeatures
from sklearn import linear_model
X=data['horsepower'].fillna(data['horsepower'].mean())
Y=data['mpg'].fillna(data['mpg'].mean())
poly = PolynomialFeatures(degree=5)
X_ = poly.fit_transform(X[:,np.newaxis])
clf = linear_model.LinearRegression()
clf.fit(X_, Y)

Chapter 5
[ 353 ]
The model in this case will be:
2
3
4
5
0
1
2
3
4
5
.
.
.
.
.
mpg
c
a horsepower
a horsepower
a horsepower
a horsepower
a horsepower
=
+
+
+
+
+
The R2 for this model comes out to be 0.70. After that degree, an increase in degree 
doesn't improve the value of R2.
The model coefficients can be found, as follows:
print clf.intercept_
print clf.coef_
Fig. 5.28: Model coefficients
2
3
4
5
40.69
4 .
7.54
6.19
2.32
3.14
mpg
horsepower
horsepower
horsepower
horsepower
horsepower
= −
+ ∗
+
∗
−
∗
−
∗
+
The reader can try the models of higher degrees as well and see how the coefficients 
change and whether it improves the model further (minimizing the error). The 
reader can also try to plot the results after the polynomial fit to see how it has 
improved the results.
Handling outliers
Outliers are the points in the dataset that are way out of the league of the other 
points. If a scatterplot of the concerned variable is drawn, the outliers can be easily 
identified, as they lay significantly away from the other data points.
The outliers need to be removed or properly treated before using the dataset for 
modelling. The outliers can distort the model and reduce its efficacy even if they are 
less in number, compared to the size of the dataset. As low as 1% outlier data is also 
capable enough to distort the model. It is actually not the number of outlier points 
but the degree to which it is different from an average point that determines the 
degree of distortion.

Linear Regression with Python
[ 354 ]
Let us look at a scatterplot of a dataset that has outliers:
Fig. 5.29: A dataset with outliers. Outliers in the encircled region
As we can see, the points in the encircled region lie away from where the majority of 
the points lie. These points lying in the encircled regions are outliers.
Let's now see how it affects the modelling process. To illustrate that, let's look at 
the result of a linear regression model built upon the dataset with outliers. We will 
then compare this result to the results of a linear regression model derived from the 
dataset from which the outliers have been removed.
The best fit line for the model developed from the dataset with outliers looks similar 
to the following screenshot:

Chapter 5
[ 355 ]
Fig. 5.30: Best fit line for the linear regression model developed over dataset with outliers
The model summary for this model is, as follows:
Fig. 5.31: Summary of the linear regression model developed over dataset with outliers

Linear Regression with Python
[ 356 ]
Let's now remove the outliers from the data and run the same linear regression 
model to check whether there is an improvement in the model or not.
The data without the outliers looks similar to the following screenshot:
Fig. 5.32: The dataset after removing outliers
The best fit line for the model developed from the dataset without outliers looks  
as follows:
Fig. 5.33: Best fit line for the linear regression model developed over dataset without outliers

Chapter 5
[ 357 ]
The model summary for this model is as follows:
Fig. 5.34: Summary of the linear regression model developed over the dataset without outliers
As we can see, the model has improved significantly especially in the terms of R2 and 
F-statistic and the associated p-values. The model coefficients have also changed.
The following table is a comparison between the two models in terms of  
the parameters:
Parameter
Model w/o outliers
Model with outlier
R2
0.739
0.253
F-statistic
560.7
70.61
Coefficient 
2.004
2.289
Intercept
0.3241
0.279
RSE
19.5%
41.4%
As it can be seen in the preceding comparison table, the model without outliers is 
better than the model with outliers in all the aspects. Thus, it is essentials to check for 
outliers in variables of the dataset and remove them from the dataset before using it 
for modelling.

Linear Regression with Python
[ 358 ]
The following are the ways in which one can identify outliers in the dataset:
•	
Plotting a scatter plot of the concerned variable.
•	
Boxplots are potent tools to spot outliers in a distribution. Any value 1.5*IQR 
below the 1st quartile and 1.5*IQR above the 1st quartile can be classified as 
an outlier. The difference in the 1st and 3rd quartile values is called the Inter 
Quartile Range (IQR).
•	
Another method is to calculate the error (the difference between the actual 
value and the value predicted from the model) and set a cut-off for the error. 
Anything outside this cut-off will be an outlier.
Other considerations and assumptions for 
linear regression
There are certain assumptions and considerations that need to be taken into account 
before finalizing on the model. Here are some of these.
Residual plots: The residual is the difference between the actual value and the 
predicted value of the output variable. A plot of the residuals plotted against the 
predictor variable should be randomly (normally with mean zero and constant 
variance) distributed and shouldn't have an identifiable shape. If the residual follows 
a characteristic curve, then it means that these errors can be predicted, which means 
something is wrong with the model and there is a scope for improvement. The error 
term for a fair estimate should be random and that's why if the residual plot shows a 
characteristic pattern there is a reason to improvise upon the model.
Ideally, the points in a residual plot should be:
•	
Symmetrically distributed or tending to cluster towards the middle of  
the plot
•	
There are no clear patterns in the plot
The non-ideal residual plots having characteristic shapes are observed because of 
one of the following reasons:
•	
Non-linear relationship
•	
Presence of outliers
•	
Very large Y-axis datapoint

Chapter 5
[ 359 ]
This can be taken care of by transforming the variable or removing the outlier:
Fig. 5.35: What residual plot should look like vs. what residual plot shouldn't look like
Non-constant variance of error terms: The error term associated with the model is 
assumed to have a constant variance and several calculations, standard errors, and 
confidence intervals. Hypothesis tests rely upon this assumption.

Linear Regression with Python
[ 360 ]
This problem is called heteroscedasticity and can be identified by a funnel shaped 
pattern in the residual plot. Transforming the output variable using a concave 
function, such as sqrt(Y) or log(Y) generally solves the problem.
High leverage points: In contrast to outliers, which have high values for the output 
variables, the high leverage points have a very high value of predictor variables. 
Such a value can distort the model. For a model with single predictor, it is easy to 
identify this issue. It can be done in the same way, as in the case of outliers.
It is difficult to do so in case of multiple regressions, where there are more than 
one predictor variable. In this case, a variable can have a value which brings a 
considerable change in the output variable compared to the same change in another 
variable; such points are called high leverage points. These need to be removed 
and sometimes, their removal increases the efficiency of the model more than the 
removal of the outlier.
To identify high leverage points in such cases, one calculates something called 
leverage statistics, which is defined as:
(
)
(
)
2
2
1
'
xi
xm
Leverage
n
x i
xm
−
=
+
∑
−
Where:
•	
xi: This is the value of the ith row of predictor variable x
•	
xm: This is the mean of the predictor variable x
The denominator is summed over all the variables for that particular row.
The rows with high values of leverage statistics are ruled out of the dataset before 
kicking off the modelling process.
Summary
This chapter marks the beginning of the introduction to the algorithms, which are the 
backbone of predictive modelling. These algorithms are converted into mathematical 
equations based on the historical data. These equations are the predictive models.
In this chapter, we discussed the simplest and the most widely used predictive 
modelling technique called linear regression.

Chapter 5
[ 361 ]
Here is a list of things that we learned in this chapter:
•	
Linear regression assumes a linear relationship between an output variable 
and one or more predictor variables. The one with a single predictor variable 
is called a simple linear regression while the one with multiple variables is 
called multiple linear regression.
•	
The coefficients of the linear relationship (model) are estimated using the 
least sum of squares method.
•	
In Python, statsmodel.api and scikit-learn are the two methods to 
implement Python.
•	
The coefficient of determination, R2, is a good way to gauge the efficiency of 
the model in explaining the error between the predicted value and the actual 
value. The more the value of R2, the lesser the error and the better the model.
•	
The model parameters, such as p-values associated with the estimates of the 
co-efficients, F-statistic, and RSE should be analyzed to further assess the 
efficiency of the model.
•	
Multi-collinearity is an issue that arises when two of the input variables in a 
multiple regression model are highly correlated. This increases the variability 
of the coefficient estimates of the correlated variables. Variance Inflation 
Factor or VIF statistic can be used to select variables getting affected due to 
multi-collinearity. Variables with a very high VIF should be removed from 
the model.
•	
A dataset can be broken into training and testing data before starting the 
modelling process, in order to validate the model. K-fold cross validation 
(about which we will learn more later) is another popular method.
•	
scikit-learn has inbuilt methods for variable selection which will take a lot 
of time, if done manually.
•	
Categorical variables can be included in the model by converting them into 
dummy variables.
•	
Some variables might need to be transformed before they are fit into a linear 
function. Sometimes, a variable might exhibit polynomial relationship with 
its predictor variable.
The linear regression is the simplest of all the predictive models. But after going 
through this chapter, we should be able to appreciate the complexities involved in 
the process. There can be multiple variations and the fine-tuning of the model is an 
elaborate process.

Linear Regression with Python
[ 362 ]
However, there's nothing to be worried about. We have gathered all the armor we 
need to implement a linear regression and understanding the model coefficients and 
parameters. The variations and kind of data shouldn't deter us in our endeavor. We 
need to fine tune the model using the methods discussed in this chapter.

[ 363 ]
Logistic Regression  
with Python
In the previous chapter, we learned about linear regression. We saw that linear 
regression is one of the most basic models that assumes that there is a linear 
relationship between a predictor variable and an output variable.
In this chapter, we will be discussing the details of logistic regression. We will be 
covering the following topics in this chapter:
•	
Math behind logistic regression: Logistic regression relies on concepts 
such as conditional probability and odds ratio. In this chapter, we will 
understand what they mean and how they are applied. We will also see 
how the odds ratio is transformed to establish a linear relationship with the 
predictor variable. We will analyze the final logistic regression equation and 
understand the meaning of each term and coefficient.
•	
Implementing logistic regression with Python: Similar to what we did in 
the last chapter, we will take a dataset and implement a logistic regression 
model on it to understand the various nuances of logistic regression. We will 
use both the statsmodel.api and scikit-learn modules for doing this.
•	
Model validation: The model, once developed, needs to be validated to 
assess the accuracy and efficiency of the model. We will see how a k-fold cross 
validation works to validate a model. We will also understand concepts such 
as Sensitivity, Specificity, and the Receiver Operating Characteristic (ROC) 
curve and see how they are used for model validation.

Logistic Regression with Python
[ 364 ]
Linear regression versus logistic 
regression
One thing to note about the linear regression model is that the output variable is 
always a continuous variable. In other words, linear regression is a good choice 
when one needs to predict continuous numbers. However, what if the output 
variable is a discrete number. What if we want to classify our records in two or more 
categories? Can we still extend the assumptions of a linear relationship and try to 
classify the records?
As it happens, there is a separate regression model that takes care of a situation 
where the output variable is a binary or categorical variable rather than a continuous 
variable. This model is called logistic regression. In other words, logistic regression 
is a variation of linear regression where the output variable is a binary or categorical 
variable. The two regressions are similar in the sense that they both assume a linear 
relationship between the predictor and output variables. However, as we will see 
soon, the output variable needs to undergo some transformation in the case of 
logistic regression.
A few scenarios where logistic regression can be applied are as follows:
•	
To predict whether a random customer will buy a particular product 
or not, given his details such as income, gender, shopping history, and 
advertisement history
•	
To predict whether a team will win or lose a match, given the match and 
team details such as weather, form of players, stadium, and hours spent  
in training
Note how the output variable in both the cases is a binary or  
categorical variable.
The following table contains a comparison of the two models:
Linear regression
Logistic regression
Predictor variables
Continuous numeric/categorical
Continuous numeric/categorical
Output variables
Continuous numeric
Categorical 
Relationship
Linear
Linear (with some 
transformations)

Chapter 6
[ 365 ]
Before we delve into implementing and assessing the model, it is of critical 
importance to understand the mathematics that makes the foundation of the 
algorithm. Let us try to understand some mathematical concepts that make the 
backbone of the logistic regression model.
Understanding the math behind logistic 
regression
Imagine a situation where we have a dataset from a supermarket store about the 
gender of the customer and whether that person bought a particular product or 
not. We are interested in finding the chances of a customer buying that particular 
product, given their gender. What comes to mind when someone poses this question 
to you? Probability anyone? Odds of success?
What is the probability of a customer buying a product, given he is a male? What is 
the probability of a customer buying that product, given she is a female? If we know 
the answers to these questions, we can make a leap towards predicting the chances 
of a customer buying a product, given their gender.
Let us look at such a dataset. To do so, we write the following code snippet:
import pandas as pd
df=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Logistic Regression/Gender Purchase.csv')
df.head()
Fig. 6.1: Gender and Purchase dataset
The first column mentions the gender of the customer and the second column 
mentions whether that particular customer bought the product or not. There are a 
total of 511 rows in the dataset, as can be found out by typing df.shape.

Logistic Regression with Python
[ 366 ]
Contingency tables
A contingency table is basically a representation of the frequency of observations 
falling under various categories of two or more variables. It comes in a matrix 
form and essentially contains the frequency of occurrences for the combination of 
categories of two or more variables.
Let us create a contingency table for the dataset we just imported and get a sense 
of how such a table actually looks. Creating a contingency table in Python is very 
simple and can be done in a single line using the crosstab method in the pandas 
library. One can actually create a crosstab object and use it for other purposes  
such as adding row/column sums. Here, we are creating a contingency table for  
the Gender and Purchase variables:
contingency_table=pd.crosstab(df['Gender'],df['Purchase'])
contingency_table
Fig. 6.2: Contingency table between Gender and Purchase variables
The interpretation of this table is simple. It implies that there are 106 females who 
didn't purchase that product, while 159 bought it. Similarly, 125 males didn't buy 
that particular product, but 121 did. Let us now find the total number of males and 
females for the purpose of calculating probabilities. The sum can be found out by 
simple manual addition. However, for the purpose of demonstrating how to do it in 
Python, let us do it using a code snippet:
contingency_table.sum(axis=1)
contingency_table.sum(axis=0)
Fig. 6.3: Totals across the rows and columns of the contingency table

Chapter 6
[ 367 ]
Rather than calculating numbers, one can calculate the proportions as well. In our 
case, we will try to calculate the proportion of males and females who purchased 
and who didn't purchase a particular product. The whole calculation can be done 
programmatically also using the following code snippet:
contingency_table.astype('float').div(contingency_table.
sum(axis=1),axis=0)
The result of the snippet looks as follows:
Fig. 6.4: The contingency table for Gender and Purchase in percentage format
Creating a contingency table is a first step towards exploring the data that has a 
binary outcome variable and categorical predictor variable.
Conditional probability
Remember the questions we asked at the beginning of this section? What is 
the probability of a customer buying a product, given he is a male? What is the 
probability of a customer buying that product, given she is a female? These  
questions are the reasons behind something called conditional probability.
Conditional probability basically defines the probability of a certain event 
happening, given that a certain related event is true or has already happened.  
Look at the questions above. They perfectly fit the description for conditional 
probability. The conditional probability of a purchase, given the customer is  
male, is denoted as follows:
)
\
(
Probability Purchase Male
It is calculated by the following formula:
(
/
)
Total number of purchasesby males
Probability Purchase Male
Total number of malesinthe group
=

Logistic Regression with Python
[ 368 ]
Now, when we know the required numbers and formulae, let us calculate the 
probabilities we were interested in before:
(
/
)
121/ 246
0.49
(
/
)
125 / 246
1 0.49
0.51
(
/
)
159 / 265
0.60
(
/
)
106 / 265
1 0.60
0.40
Probability Purchase Male
Probability Not Purchase Male
Probability Purchase Female
Probability Not Purchase Male
=
=
=
= −
=
=
=
=
= −
=
This concept comes in very handy in understanding many of the predictive models, 
as it is the building block of many of them.
Odds ratio
This is the correct time to introduce a very important concept called odds ratio.  
The odds ratio is a ratio of odds of success (purchase in this case) for each group 
(male and female in this case).
Odds of success for a group are defined as the ratio of probability of successes 
(purchases) to the probability of failures (non-purchases). In our case, the odds  
of the purchase for the group of males and females can be defined as follows:
/ (1
)
/ (1
)
m
m
f
f
Oddsof purchaseby males
P
P
Oddsof purchaseby females
P
P
=
−
=
−
Here, Pm=probability of purchase by males and Pf=probability of purchase by females.
For the preceding contingency table given:
121/ 246,
159 / 265
(121/ 246) / (125 / 246)
121/125
/ (1
)
(159 / 265) / (106 / 265)
159 /106
m
P
Pf
Oddsof purchaseby males
Oddsof purchaseby females
Pf
Pf
=
=
=
=
=
−
=
=

Chapter 6
[ 369 ]
As it is obvious from the calculations above, the odds of the success for a particular 
group can easily be written as follows:
/
/
sm
fm
sf
ff
Oddsof success for males
N
N
Oddsof success for females
N
N
=
=
Here, Ns=number of successes in that group and Nf=number of failures in that group.
A few points to be noted about the odds of an event are as follows:
•	
If the odds of success for a group is more than 1, then it is more likely for that 
group to be successful. The higher the odds, the better the chances of success.
•	
If the odds of success is less than 1, then then it is more likely to get a failure. 
The lower the odds, the higher the chances of failure.
•	
The odds can range from 0 to infinity.
In our case, the odds of success is greater than 1 for females and less than 1 for males. 
Thus, we can conclude that females have a better chance of success (purchase), in this 
case, than males.
One better way to determine which group has better odds of success is by calculating 
odds ratios for each group. The odds ratio is defined as follows:
1/
2
/ (1
)
/ (1
)
/
/
Odds ratio Odd of successinGroup
Oddsof successin group
Pm
Pm
Odds ratio
Pf
Pf
Nsm Nfm
Odds ratio
Nsf
Nff
=
−
=
−
=
In the preceding example we have seen:
(
)
/
(121/125) / (159 /106)
0.64
(
)
/
(159 /106) / (121/125)
1.54
Odds ratio for males
Oddsof success for males Oddsof success for females
Odds ratio for males
Oddsof Purchase for female Oddsof purchase by males
=
=
=
=
=
=
 

Logistic Regression with Python
[ 370 ]
Actually,
(
1)
1/
(
2)
Odds ratio for Group
Odds ratio for Group
=
There are a couple of important things to note about the odds ratio:
•	
The more the odds ratio, the more the chances of success from that group.  
In our case, the female group has an odds ratio of 1.54, which means that it  
is more probable to get success (purchase) from a female customer than a 
male customer.
•	
If the odds ratio=1, then there is no association between the two variables. If 
odds ratio>1, then the event is more likely to happen in Group 1. If the odds 
ratio<1, then the event is more likely to happen in Group 2.
•	
Also, the odds ratio for one group can be obtained by taking the reciprocal of 
the odds ratio of the other group.
Moving on to logistic regression from linear 
regression
If you remember, the equation for a simple linear regression model was as follows:
Y
a
b
X
=
+ ∗
In this case, Y was a continuous variable whose value can range from –infinity to 
+infinity. The X was either a continuous or a dummy categorical variable and hence 
it also ranged from –infinity to +infinity. So, the ranges of variables on both the sides 
of the equation matched.
However, when we move to logistic regression, the Y variable can take only discrete 
values, 0 or 1, as the outcome variable is a binary variable. However, predicting 0 
or 1 using an equation similar to linear regression is not possible. What if we try 
to predict the probabilities associated with the two events rather than the binary 
outcomes? Predicting the probabilities will be feasible as their range spans  
from 0 to 1.

Chapter 6
[ 371 ]
Earlier, we calculated the conditional probability of a customer purchasing a 
particular product, given he is male or female. These are the probabilities we are 
thinking of predicting. In the case demonstrated above, there was only one predictor 
variable, so it was very easy. However, as the number of predictor variables increase, 
these conditional probabilities will become more and more difficult to calculate. 
However, anyways, predicting probability is a better choice than predicting 0 or 1. 
Hence, for logistic regression, something like the following suits better:
P
a
b
X
=
+ ∗
Here P=conditional probability of success/failure given the X variable
Even with this new equation, the problem of non-matching ranges on both the sides 
of the equation persists. The P ranges from 0 to 1, while X ranges from –infinity to 
+infinity. What if we replace the P with odds, that is, P/1-P. We have seen earlier that 
the odds can range from 0 to +infinity. So, the proposed equation becomes:
/1
P
P
a
b
X
−
=
+ ∗
Where P=conditional probability of success/failure given the X variable
Still the LHS of the equation ranges from 0 to +infinity, while the RHS ranges 
from –infinity to +infinity. How to get rid of this? We need to transform one side of 
the equation so that the ranges on both the sides match. What if we take a natural 
logarithm of the odds (LHS of the equation)? Then, the range on the LHS also 
becomes –infinity to +infinity.
So, the final equation becomes as follows:
log(
/1
)
P
P
a
b
X
−
=
+ ∗
 
Here, P=conditional probability of success/failure given the X variable.
The term loge(Odds) is called logit.
The base of the logarithm is e (e=2.73) as we have taken a 
natural logarithm.

Logistic Regression with Python
[ 372 ]
The transformation can be better understood if we look at the plot of a logarithmic 
function. For a base greater than 1, the plot of a logarithmic function is shown  
as follows:
Fig. 6.5: The plot of a logarithmic curve for a base>1
The summary of the ranges of odds and the corresponding ranges of the loge(Odds) 
can be summarized as follows:
Range of Odds
Range of loge(Odds)
0 to 1
-infinity to 0
1 to +infinity
0 to +infinity
The evolution of the transformations that lead from linear to logistic regression can 
be summarized as follows. The range on the RHS of the equation is always –infinity 
to +infinity while we transform the LHS to match it:
Transformation (LHS)
Range of LHS
Range of LHS
Y
Y= 0 or Y= 1
Infinity<X<+infinity
P (Probability)
0<P<1
Infinity<X<+infinity
P/1-P (Odds)
0<P/1-P<+infinity
Infinity<X<+infinity
log(P/1-P)
-infinity<log(P/1-P)<+infinity
Infinity<X<+infinity
The final equation we have for logistic regression is as follows:
(
)
log(
/1
)
1
1
1
1
a b X
a b X
a b X
a b X
P
P
a
b
X
P
e
P
e
P
e
e
+ ∗
+ ∗
+ ∗
−
+ ∗
−
=
+ ∗
=
−
=
=
+
+

Chapter 6
[ 373 ]
The final equation can be used to calculate the probability, given the value of X, a, 
and b:
•	
If a+b*X is very small, then P approaches 0
•	
If a+b*X is very large, then P approaches 1
•	
If a+b*X is ), then P=0.5
For a multiple logistic regression, the equation can be written as follows:
1
1
2
2
3
3
1
1
2
2
3
3
(
)
log(
/1
)
1
1
n
n
a b
X
b
X
b
X
bn Xn
P
P
a
b
X
b
X
b
X
b
X
P
e−
+
∗
+
∗
+
∗
+
+
∗
−
=
+
∗
+
∗
+
∗
+
+
∗
= +
…
…………………
If we replace (X1, X2, X3,…,Xn) with Xi' and (b1, b2, b3,----,bn) with bi', the equation 
can be rewritten as follows:
(
)
1
1
1
a b Xi
a b Xi
a b Xi
e
P
e
e
′
+ ∗
′
′
+ ∗
−
+ ∗
=
=
+
+
Estimation using the Maximum Likelihood 
Method
The variables a and bi are estimated using the Maximum Likelihood Method (MLE).
For a multivariate data having multiple variables and n observations, the likelihood 
(L) or the joint probability is defined as follows:
(
)
(
)
(
)
1
3
2
1,
1
1,
0
1
1
1
n
n
n
n
i
y
i
y
L
P
P
P
P
P
Pi
Pi
=
=
=
=
=
∗
∗
∗
−
∗
−
=
∗
−
∏
∏
…
…
Here, Pi is the probability associated with the ith observation. When the outcome 
variable is positive (or 1), we take Pi for multiplying; when it is negative (or 0), we 
take (1-Pi) for multiplying in the likelihood function:
As we have seen already, the defining equation for logistic regression is as follows: 
(
)
1
1
1
a b Xi
a b Xi
a b Xi
e
Pi
e
e
+ ∗
+ ∗
−
+ ∗
=
=
+
+

Logistic Regression with Python
[ 374 ]
Also, assume that the output variable is Y1, Y2, Y3,…,Yn, all of which can take the 
value 0 or 1. For one predictor variable, the best estimate to predict the probability of 
the success is the mean value of all Yi's:
[ ]
i
m
i
E Y
Y
P
=
=
Here, E[Yi] = Ym is the mean of Yi's.
Hence, the equation above can be rewritten as follows:
(
)
(
)
(
)
1
1
1
1
1
/
1
1
a b Xi
i
m
a b Xi
a b X
a b Xi
n
i
a b Xi
a b Xi
e
P
Y
e
e
e
Yi
n
e
e
+ ∗
+ ∗
−
+ ∗
+ ∗
=
+ ∗
−
+ ∗
=
=
=
+
+
= ∑
=
=
+
+
Thus, the equation for Likelihood becomes as follows:
1
1
1
1
1
1
a b Xi
n
a b Xi
n
n
i
i
i
a b Xi
a b Xi
e
Yi
e
Yi
L
e
e
′
′
+ ∗
+ ∗
=
=
=
′
′
+ ∗
+ ∗



∑
∑
= ∏
∗
−



+
+



Taking log on both the sides:
(
)
1
1
1
1
1
a b Xi
a b Xi
n
i
a b Xi
a b Xi
e
e
logL
Yi log
Yi
log
e
e
+ ∗
+ ∗
=
+ ∗
+ ∗






= ∑
∗
+
−
∗
−






+
+






To estimate the MLE of bi's, we equate the derivative of logL to 0:
(log ) /
0
U
d
L
dXi
=
=
d(logL)/dXi is the partial derivative of logL with respect to each Xi (variable).
This equation is called the Score function and there would be as many such 
equations as there are variables in the dataset.
Another related calculation is that of the Fisher Information and it is given as the 
second derivative of logL:
/
Fischer Information
I
dU dL
=
=

Chapter 6
[ 375 ]
The Fisher Information is useful because its inverse gives the variance associated 
with the estimate of the bi. In the case of multiple variables, we will get a covariance 
matrix. It is also used to decide whether the values found with the help of setting 
the derivative to 0 were maxima or minima. These many equations will be difficult 
to solve analytically and there are numerical methods such as Newton-Raphson 
methods to do the same.
Let us go a little deeper into the mathematics behind calculating the coefficients 
using the maximum likelihood estimate. The following are the steps in this process:
1.	 Define a function that calculates the probability for each observation, given 
the data (predictor variables).
2.	 Define a likelihood function that multiplies Pi's and (1-Pi)'s for each 
observation, depending on whether the outcome is 1 or 0. Calculate the 
likelihood.
3.	 Use the Newton-Raphson method to calculate the roots. In the  
Newton-Raphson method, one starts with a value of the root and  
updates it using the following equation for a number of times until  
it stops improving:
(
)
(
)
1
\ '
Xn
Xn
f
X
f
X
+ =
−
Likelihood function:
(
)
( )
( )
(
)
1,
1
1,
0
|
1
i
i
N
N
i
i
i
y
i
y
L X P
P x
P x
=
=
=
=
=
−
∏
∏
Log likelihood function:
(
)
( )
( )
(
)
1,
1
0,
0
|
log
1
i
i
N
N
i
i
i
y
i
y
L X P
P x
log
P x
=
=
=
=
=
+
−
∑
∑

Logistic Regression with Python
[ 376 ]
The derivative of the Log likelihood function is as follows:
(
)
(
)
(
)
(
) (
)
1
1
0
1
1
1
0
1
1
1
1
1
1
1
1
0
i
i
i
i
N
N
i
i
i
i
b
i
i
i i
i
i
i
y
y
N
N
i
i
i
i
i
i
y
y
N
i
i
i
i
i
i
N
i
i
i
i
i
P
p
P
p
L
x
x
P
P
p
x
Px
y
P
y
P x
y x
Px
=
=
=
=
=
=
=
=
=
=
−
−
∇
=
−
−
=
−
−
=
−
−
−




=
−
=
∑
∑
∑
∑
∑
∑
The second derivative of the Log likelihood function is as follows:
(
)
1
1
1
b
N
i
b
i
i
N
T
i
i
i
i
T
H
L
b
x
P
x P
P x
XWX
=
=
∂
=
∇
∂
= −
∇
= −
−
=
∑
∑
Partial derivatives have been taken with respect to the variable coefficients.
According to the Newton-Raphson method:
(
)
(
)
\
\
1
'
(
)
(
)
Xn
Xn
f
X
f
X
X
f X
f X
+ =
−
∆
=

Chapter 6
[ 377 ]
In this case, Newton-Raphson method translates to the following:
(
)
(
)
(
)
T
f X
first derivationof log likelihood function
X Y
P
f X
XWX
=
=
−
=
 Here, W is a diagonal matrix containing the product of probabilities in the diagonal.
(
)
1
,
(
)
T
Hence
X
XWX
X Y
P
−
∆
=
−
Here, we are solving for variable coefficients, that is, a and b. According to the 
Newton-Raphson method, if we calculate the multiplication above enough number 
of times, starting with an initial approximate value of variable coefficients (we 
assume 0 in this case), we will get the optimum values of coefficients. To help 
better understand the calculation behind the logistic regression, let us implement 
the mathematics behind the logistic regression using Python code. Let us build the 
logistic regression model from scratch.
Building the logistic regression model from scratch
The following are the steps to implement the mathematics behind the logistic 
regression. Before we start using the in-built methods in Python as a black box to 
implement logistic regression, let us create a code that can do all the computation 
and throw up coefficients and likelihoods as the results just the same way as a built 
in method in Python would. Each of the step has been defined as a separate Python 
function:
# Step 1: defining the likelihood function
def likelihood(y,pi):
    import numpy as np
    ll=1
    ll_in=range(1,len(y)+1)
    for i in range(len(y)):
        ll_in[i]=np.where(y[i]==1,pi[i],(1-pi[i]))
        ll=ll*ll_in[i]
    return ll
# Step 2: calculating probability for each observation
def logitprob(X,beta):
    import numpy as np
    rows=np.shape(X)[0]
    cols=np.shape(X)[1]
    pi=range(1,rows+1)

Logistic Regression with Python
[ 378 ]
    expon=range(1,rows+1)
    for i in range(rows):
        expon[i]=0
        for j in range(cols):
            ex=X[i][j]*beta[j]
            expon[i]=ex+expon[i]
        with np.errstate(divide='ignore', invalid='ignore'):
            pi[i]=np.exp(expon[i])/(1+np.exp(expon[i]))
    return pi
# Step 3: Calculate the W diagonal matrix
def findW(pi):
    import numpy as np
    W=np.zeros(len(pi)*len(pi)).reshape(len(pi),len(pi))
    for i in range(len(pi)):
        print i
        W[i,i]=pi[i]*(1-pi[i])
        W[i,i].astype(float)
    return W
# Step 4: defining the logistic function
def logistic(X,Y,limit):
    import numpy as np
    from numpy import linalg
    nrow=np.shape(X)[0]
    bias=np.ones(nrow).reshape(nrow,1)
    X_new=np.append(X,bias,axis=1)
    ncol=np.shape(X_new)[1]
    beta=np.zeros(ncol).reshape(ncol,1)
    root_diff=np.array(range(1,ncol+1)).reshape(ncol,1)
    iter_i=10000
    while(iter_i>limit):
        print iter_i, limit
        pi=logitprob(X_new,beta)
        print pi
        W=findW(pi)
        print W
        print X_new
        print (Y-np.transpose(pi))
        print np.array((linalg.inv(np.matrix(np.transpose(X_new))*np.
matrix(W)*np.matrix(X_new)))*(np.transpose(np.matrix(X_new))*np.
matrix(Y-np.transpose(pi)).transpose()))
        print beta
        print type(np.matrix(np.transpose(Y-np.transpose(pi)))) 
        print np.matrix(Y-np.transpose(pi)).transpose().shape
        print np.matrix(np.transpose(X_new)).shape

Chapter 6
[ 379 ]
        root_diff=np.array((linalg.inv(np.matrix(np.transpose(X_
new))*np.matrix(W)*np.matrix(X_new)))*(np.transpose(np.matrix(X_
new))*np.matrix(Y-np.transpose(pi)).transpose()))
        beta=beta+root_diff
        iter_i=np.sum(root_diff*root_diff)
        ll=likelihood(Y,pi)
        print beta
        print beta.shape
    return beta
# Testing the model
import numpy as np
X=np.array(range(10)).reshape(10,1)
Y=[0,0,0,0,1,0,1,0,1,1]
bias=np.ones(10).reshape(10,1)
X_new=np.append(X,bias,axis=1)
# Running logistic Regression using our function
a=logistic(X,Y,0.000000001)
ll=likelihood(Y,logitprob(X,a))
Coefficient of X = 0.66 , Intercept = -3.69
# From stasmodel.api
import statsmodels.api as sm
logit_model=sm.Logit(Y,X_new)
result=logit_model.fit()
print result.summary()
Coefficient of X = 0.66, Intercept = -3.69
Isn't this cool?! We have been able to match the exact values for the variable 
coefficient and intercept. Run these codes in your Python IDE one by one and see 
what each snippet throws up as an output. Each of them is a separate function so 
you will have to give inputs to make them run. Compare it with the calculations 
performed above and see how the steps in the calculations have been implemented.
In Python, scikit-learn performs these calculations under the hood and throws up 
the estimates for the coefficients when asked to run a logistic regression.
Making sense of logistic regression 
parameters
As with the linear regression, there are various parameters that are thrown up  
by a logistic regression model, which can be assessed for variable selection and 
model accuracy.

Logistic Regression with Python
[ 380 ]
Wald test
As in the case of linear regression, here also, we are estimating the values of the 
coefficients. There is a hypothesis test associated with each estimation. Here, we  
test the significance of the coefficients bi's:
:
:
0
:
:
0
Null Hypothesis Ho bi
Alternate Hypothesis Ha bi
=
<>
The Wald statistic is defined as follows:
(
0) /
(
)
im
Wald statistic
b
sd bi
=
−
Here, bim=mean of bi and sd(bi)=standard error in estimation of bi.
The standard error comes from the Fisher Information covariance matrix. The Wald 
statistic assumes a standard normal distribution and, hence, we can perform a z-test 
over it. As we will see in the output of a logistic regression, there will be a p-value 
associated with the estimation of each bi. This p-value comes from this z-test. The 
smaller the p-value, the more the significance of that variable.
Likelihood Ratio Test statistic
The Likelihood Ratio Test statistic is the ratio of the (null) hypothesized value of the 
parameter to the MSE (alternate) values of the parameters.
The definition of the hypothesis test is the same as above:
:
0
:
0
Null Hypothesis bi
Alternate Hypothesis bi
=
<>
LR statistic is given by:
(
)
(
)
2log(
/
)
2log(
/
)
\
\
\
2
\
(
LR
valueof bi given Ho valueof bi given Ha
bi Ho bi Ha
log bi Ho
log bi Ha
= −
= −
= −
−





Chapter 6
[ 381 ]
To calculate the value of bi|Ho and bi|Ha, we need to fit two different models and 
calculate the values of bi for each model.
If the proposed model is
(
)
1
1
2
2
3
3
/1
n
n
log P
P
a
b
X
b
X
b
X
b
X
−
=
+
∗
+
∗
+
∗
+
+
∗
……………
If we are testing LR statistic for b1, then Ho would give rise to the following model:
(
)
2
2
3
3
:
/1
n
n
Ho log P
P
a
b
X
b
X
b
X
−
=
+
∗
+
∗
+
+
∗
………………
Also, Ha would give rise to the following model:
(
)
1
1
2
2
3
3
:
/1
n
n
Ha log P
P
a
b
X
b
X
b
X
b
X
−
=
+
∗
+
∗
+
∗
+
+
∗
………………
The significance of the values of bi's is defined by the value of the LR statistic. The 
LR statistic follows a chi-square distribution with degrees of freedom equal to the 
difference in the degrees of freedom in two cases. If the p-value associated with this 
statistic is very small, then the alternate hypothesis is true, that is, the value of bi is 
significant and non-zero.
Both the models need to be fit and the MLE value of bi is calculated for bi from both 
the models. Then, a log of the ratio of the two values of bi gives the LR statistic.
Chi-square test
For large datasets (large n), a LR test reduces to a chi-square test with a degree of 
freedom equal to the number of parameters being estimated. This is the reason 
pairwise chi-square tests are often performed between the predictor and the outcome 
variable, in order to decide whether they are independent of each other or have some 
association. This is sometimes used for variable selection for the model. The variables 
for which there is an association between them and the outcome variable are better 
predictors of the outcome variable. If the null hypothesis for a predictor variable is 
rejected, then more often than not, it should be made a part of the model.

Logistic Regression with Python
[ 382 ]
Implementing logistic regression with 
Python
We have understood the mathematics that goes behind the logistic regression 
algorithm. Now, let's take one dataset and implement a logistic regression model 
from scratch. The dataset we will be working with is from the marketing department 
of a bank and has data about whether the customers subscribed to a term deposit, 
given some information about the customer and how the bank has engaged and 
reached out to the customers to sell the term deposit.
Let us import the dataset and start exploring it:
import pandas as pd
bank=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Logistic Regression/bank.csv',sep=';')
bank.head()
The dataset looks as follows:
Fig. 6.6: A glimpse of the bank dataset
There are 4119 records and 21 columns. The column names are as follows:
bank.columns.values
Fig. 6.7: The columns of the bank dataset
The details of each column are mentioned in the Data Dictionary file present in the 
Logistic Regression folder of the Google Drive folder. The type of the column 
can be found out as follows:
bank.dtypes

Chapter 6
[ 383 ]
Fig. 6.8: The column types of the bank dataset
Processing the data
The y column is the outcome variable recording yes and no. yes for customers who 
bought the term deposit and no for those who didn't. Let us start by converting yes-
no to 0-1 so that they can be used in modelling. This can be done as follows:
bank['y']=(bank['y']=='yes').astype(int)
The preceding code snippet converts yes to 1 and no to 0. The astype method 
converts the True/False to integer (0/1).
The education column of the dataset has many categories and we need to reduce the 
categories for a better modelling. The education column has the following categories:
bank['education'].unique()
Fig. 6.9: The categories of the education column in the bank dataset
The basic category has been repeated three times probably to capture 4, 6, and 9 
years of education. Let us club these three together and call them basic. Also, let us 
modify the categories so that they look better:
import numpy as np
bank['education']=np.where(bank['education'] =='basic.9y', 'Basic', 
bank['education'])
bank['education']=np.where(bank['education'] =='basic.6y', 'Basic', 
bank['education'])
bank['education']=np.where(bank['education'] =='basic.4y', 'Basic', 
bank['education'])

Logistic Regression with Python
[ 384 ]
bank['education']=np.where(bank['education'] =='university.degree', 
'University Degree', bank['education'])
bank['education']=np.where(bank['education'] =='professional.course', 
'Professional Course', bank['education'])
bank['education']=np.where(bank['education'] =='high.school', 'High 
School', bank['education'])
bank['education']=np.where(bank['education'] =='illiterate', 
'Illiterate', bank['education'])
bank['education']=np.where(bank['education'] =='unknown', 'Unknown', 
bank['education'])
After the change, this is how the categories look:
Fig. 6.10: The column types of the bank dataset
Data exploration
First of all, let us find out the number of people who purchased the term deposit and 
those who didn't:
bank['y'].value_counts()
Fig. 6.11: Total number of yes's and no's in the bank dataset
There are 3668 no's and 451 yes's in the outcome variables.
As you might have observed, there are many numerical variables in the dataset.  
Let us get a sense of the numbers across the two classes such as yes or no:
bank.groupby('y').mean()
Fig. 6.12: The mean of the numerical variables for yes's and no's

Chapter 6
[ 385 ]
A few points to note from the preceding output are as follows:
•	
The average age of customers who bought the term deposit is higher than 
that of the customers who didn't.
•	
The pdays (days since the customer was last contacted) is understandably 
lower for the customers who bought it. The lower the pdays, the better the 
memory of the last call and hence the better chances of a sale.
•	
Surprisingly, campaigns (number of contacts or calls made during the 
current campaign) are lower for customers who bought the term deposit.
We can calculate categorical means for other categorical variables such as education 
and marital status to get a more detailed sense of our data. The categorical means for 
education looks as follows:
bank.groupby('education').mean()
Fig. 6.13: The mean of the numerical variables for different categories of education
Data visualization
Let us visualize our data to get a much clearer picture of the data and significant 
variables. Let us start with a histogram of education with separate bars for customers 
who bought the term deposit and the customers who didn't.

Logistic Regression with Python
[ 386 ]
The tabular data for Education Level and whether they purchased the deposit or not 
would look like as follows:
Fig. 6.14: Tabular data for Education Level and Purchase
The same data can be plotted as a bar chart using the code snippet as follows:
       %matplotlib inline
pd.crosstab(bank.education,bank.y).plot(kind='bar')
plt.title('Purchase Frequency for Education Level')
plt.xlabel('Education')
plt.ylabel('Frequency of Purchase')
Fig. 6.15: Bar chart for Education Level and Frequency of Purchase

Chapter 6
[ 387 ]
As is evident in the preceding plot, the frequency of purchase of the deposit depends 
a great deal on the Education Level. Thus, the Education Level can be a good 
predictor of the outcome variable.
Let us draw a stacked bar chart of the marital status and the purchase of term 
deposit. Basically, the chart will represent the proportion of the customers who 
bought the customers from each marital status. It looks as follows:
table=pd.crosstab(bank.marital,bank.y)
table.div(table.sum(1).astype(float), axis=0).plot(kind='bar', 
stacked=True)
plt.title('Stacked Bar Chart of Marital Status vs Purchase')
plt.xlabel('Marital Status')
plt.ylabel('Proportion of Customers')
Fig. 6.16: The bar chart for the Marital Status and Proportion of Customers
The frequency of the purchase of the deposit is more or less the same for each marital 
status; hence, it might not be very helpful in predicting the outcome.
Let us plot the bar chart for the Frequency of Purchase against each day of the 
week to see whether this can be a good predictor of the outcome:
%matplotlib inline
import matplotlib.pyplot as plt
pd.crosstab(bank.day_of_week,bank.y).plot(kind='bar')
plt.title('Purchase Frequency for Day of Week')
plt.xlabel('Day of Week')
plt.ylabel('Frequency of Purchase')

Logistic Regression with Python
[ 388 ]
The plot (the frequency of the positive outcomes) varies depending on the month of 
the year; hence, it might be a good predictor of the outcome:
Fig. 6.17: The bar chart for Month of the Year and Frequency of Purchase
The Histogram of Age variable looks as follows, suggesting that the most of the 
customers of the bank in this dataset are in the age range of 30-40:
import matplotlib.pyplot as plt
bank.age.hist()
plt.title('Histogram of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
Fig. 6.18 The bar chart for customer's Age and Frequency of Purchase

Chapter 6
[ 389 ]
Another bar chart of Poutcome and the frequency of purchase shows that the 
Poutcome might be an important predictor of the outcome:
Fig. 6.19: The bar chart for Poutcome and Frequency of Purchase
Several other charts can be plotted to gauge which variables are more significant and 
which ones are not in order to predict the outcome variable.
Creating dummy variables for categorical 
variables
There are many categorical variables in the dataset and they need to be converted 
to dummy variables before they can be used for modelling. We know the process 
of converting a categorical variable into a dummy variable. However, since there 
are many categorical variables, it would be time-efficient to automate the process 
using a for loop. The following code snippet will create dummy variables for each 
categorical variables and join these dummy variables to the bank data frame:
cat_vars=['job','marital','education','default','housing','loan','cont
act','month','day_of_week','poutcome']
for var in cat_vars:
    cat_list='var'+'_'+var
    cat_list = pd.get_dummies(bank[var], prefix=var)
    bank1=bank.join(cat_list)
    bank=bank1

Logistic Regression with Python
[ 390 ]
The actual categorical variable needs to be removed once the dummy variables have 
been created. We have done something like this earlier in this book:
cat_vars=['job','marital','education','default','housing','loan','cont
act','month','day_of_week','poutcome']
bank_vars=bank.columns.values.tolist()
to_keep=[i for i in bank_vars if i not in cat_vars]
Let us subset the bank data frame to keep only the columns present in the  
to_keep list:
bank_final=bank[to_keep]
bank_final.columns.values
Fig. 6.20: Column names after creating dummy variables for categorical variables
The outcome variable is y and all the other variables are predictor variables.  
The X predictor and the Y outcome variable can be created using the code  
snippet as follows:
bank_final_vars=bank_final.columns.values.tolist()
Y=['y']
X=[i for i in bank_final_vars if i not in Y ]

Chapter 6
[ 391 ]
Feature selection
Before implementing the model, let us perform a feature selection to decide the 
significant variables that can predict the outcome with great accuracy. We have the 
freedom to select as many as variables as we can. Let us select 12 columns. This can 
be done as follows, which is similar to that done in the chapter on linear regression:
from sklearn import datasets
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
rfe = RFE(model, 12)
rfe = rfe.fit(bank_final[X],bank_final[Y] )
print(rfe.support_)
print(rfe.ranking_)
The output of the preceding code snippet are two arrays. One contains the support 
and the other contains the ranking. The columns that have True in the support array 
are selected for the model, or the columns that have the value 1 in the rank array are 
selected for the model. If we want to include more (than 12) columns in the model, 
we can select the columns with the rank 2 onwards:
Fig. 6.21: The outcome of feature selection process. The columns with True/1 in the respective positions should 
be selected for the final model
The columns that are selected using this method are as follows:
'previous', 'euribor3m', 'job_entrepreneur', 'job_self-employed', 
'poutcome_success', 'poutcome_failure', 'month_oct', 'month_
may','month_mar', 'month_jun', 'month_jul', 'month_dec'

Logistic Regression with Python
[ 392 ]
Next, we will try to fit a logistic regression model using the preceding selected 
variables as predictor variables, with the y as the outcome variable:
cols=['previous', 'euribor3m', 'job_entrepreneur', 'job_self-
employed', 'poutcome_success', 'poutcome_failure', 'month_oct', 
'month_may',
    'month_mar', 'month_jun', 'month_jul', 'month_dec'] 
X=bank_final[cols]
Y=bank_final['y']
Implementing the model
Let's first use the stasmodel.api method to run the logistic regression model as 
shown in the following code snippet:
import statsmodels.api as sm
logit_model=sm.Logit(Y,X)
result=logit_model.fit()
print result.summary()
Fig. 6.22: The summary of the logistic regression model with selected variables

Chapter 6
[ 393 ]
One advantage of this method is that p-values are calculated automatically in the 
result summary. The scikit-learn method doesn't have this facility, but is more 
powerful for calculation-intensive tasks such as prediction, calculating scores, and 
advanced functions such as feature selection. The statsmodel.api method can be 
used while exploring and fine-tuning the model, while the scikit-learn method 
can be used in the final model used to predict the outcome.
The summary of the model looks as shown in the preceding screenshot. For 
each variable, the coefficient value has been estimated, and corresponding to 
each estimation, there is a std error and p-value. This p-value corresponds to the 
hypothesis testing of the Wald statistics, and the lower the p-value, the more the 
significance of the variable in the model. For most of the variables in this model,  
the p-values are very less and, hence, most of them are significant to the model.
We will be using the scikit-learn method to fit the model as is shown in the 
following code snippet:
from sklearn import linear_model
clf = linear_model.LogisticRegression()
clf.fit(X, Y)
The accuracy of this model can be calculated as follows:
clf.score(X,Y)
The value comes out to be .902. The mean value of the outcome is .11, meaning that 
the outcome is positive (1) around 11% of the time and negative around 89% of the 
time. So, even by predicting 0 for all the rows, one could have achieved an accuracy 
rate of 89%. Our model takes this accuracy to 90.2. For a little bit of enhancement, 
maybe, we can try reducing the number of columns, training-testing split, and cross 
validation to increase this score.

Logistic Regression with Python
[ 394 ]
For this method, one can get the value of the coefficients using the following  
code snippet:
import numpy as np
pd.DataFrame(zip(X.columns, np.transpose(clf.coef_)))
Fig. 6.23: Coefficients for the variables in the model
The variable coefficients indicate the change in the log (odds) for a unit change in 
the variable. The coefficient for the previous variable is 0.37. This implies that, if the 
previous variable increases by 1, the log(odds) will increase by 0.37 and, hence, the 
probability of the purchase will change accordingly.
Model validation and evaluation
The preceding logistic regression model is built on the entire data. Let us now split 
the data into training and testing sets, build the model using the training set, and 
then check the accuracy using the testing set. The ultimate goal is to see whether it 
improves the accuracy of the prediction or not:
from sklearn.cross_validation import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_
size=0.3, random_state=0)
The preceding code snippet creates testing and training datasets for a predictor 
and also outcome variables. Let us now build a logistic regression model over the 
training set:
from sklearn import linear_model
from sklearn import metrics
clf1 = linear_model.LogisticRegression()
clf1.fit(X_train, Y_train)

Chapter 6
[ 395 ]
The preceding code snippet creates the model. If you remember the equation behind 
the model, you will know that the model predicts probabilities and not the classes 
(binary output, that is, 0 or 1). One needs to select a threshold over these probabilities 
to classify them into two categories. Something of this sort: if the probability is less 
than the threshold, then it is a 0 outcome, and if it is greater than the threshold, then 
it is a 1 outcome.
Let us see how we can get those probabilities and classifications:
probs = clf1.predict_proba(X_test)
This gives the probability of a negative and positive outcome for each row of  
the data:
Fig. 6.24: Predicted probability values for each observation
The second column provides the probability of a positive outcome (purchase of a 
deposit outcome in our case). By default, if this probability is more than 0.5, then the 
observation is categorized as a positive outcome, and as a negative outcome if it is 
less than that.
The default outcome for predicting the class can be found out using the following 
code snippet:
predicted = clf1.predict(X_test)
The output is an array consisting of 0 and 1. In the default case, it categorizes 
probabilities less than 0.5 as 0, and more than that as 1. One can use different  
cutoffs for this as well. One can change it to 0.15 or 0.20 depending upon the 
situation. In our case, we have seen that only 10% of the customers buy the product; 
hence, probability=0.10 can be a good threshold. If an observation has a probability 
of more than 0.10, we can classify it as a positive outcome (a customer will buy the 
product). An observation with the probability less than 0.10 will be classified as a 
negative outcome.

Logistic Regression with Python
[ 396 ]
The changing of threshold values can be done using the following code snippet:
import pandas as pd
import numpy as np
prob=probs[:,1]
prob_df=pd.DataFrame(prob)
prob_df['predict']=np.where(prob_df[0]>=0.10,1,0)
prob_df.head() 
The number of positive and negative responses will change with the threshold 
values. The percentage of positive outcomes with three different threshold 
probabilities are mentioned as follows:
Threshold
% Positive outcome
0.10
28%
0.15
18%
0.05
65%
Let us check the accuracy of this model using the following code snippet:
print metrics.accuracy_score(Y_test, predicted)
This model has the same accuracy of 90.21% as the previous model.
Cross validation
Cross validation is performed on a dataset while predicting to check how well the 
model will generalize its results on an independent dataset.
Cross validation is required to deal with an issue common with the predictive 
models. The models are developed based on one set of data, and most of the model 
parameters are calculated using the criterion of the most optimal fit with the data on 
which the model is being built. This leads to a problem called overfitting, wherein 
the model fits the given (training) data very well, but doesn't reproduce the good 
fitting with some other (testing) dataset. This problem is more severe in the case of 
datasets with less observations.
Splitting up a dataset in the training and testing dataset is the simplest way to do 
cross validation. This is called a holdout method, wherein the training set and testing 
set are randomly chosen.

Chapter 6
[ 397 ]
The most popular way to perform a cross validation is using something called as 
k-fold cross validation. It is done as follows:
1.	 Divide the data set into k partitions.
2.	 One partition is used as the test set, while the other k-1 partitions together 
are used as the training set.
3.	 The process in (2) is repeated k times with a different partition as the testing 
dataset and the rest of them as the training dataset in each iteration.
4.	 For each iteration, the model accuracy is calculated and averaged out over 
the iterations. The averaged value is the value of the model accuracy.
5.	 If the accuracy of the model doesn't vary much and the average accuracy 
remains closer to the accuracy numbers calculated for the model before,  
then it can be confirmed that the model generalizes well.
Each observation gets to be part of the testing dataset exactly once, while each row 
becomes part of the training dataset exactly k-1 times. One advantage of this method 
is that each of the observations get to be part of either testing or training dataset 
at least once; hence, it leads to a better generalization. K=10 is generally the norm, 
but can be changed according to the situation. In scikit-learn, there is a separate 
method to perform cross validation which can be done very easily:
from sklearn.cross_validation import cross_val_score
scores = cross_val_score(linear_model.LogisticRegression(), X, Y, 
scoring='accuracy', cv=8)
print scores
print scores.mean()
The preceding code snippet basically runs an 8-fold cross validation method and 
calculates the accuracy for each of the iterations. The average accuracy is also 
printed:
Fig. 6.25: The accuracy for each run (fold) of the model during cross validation
The average accuracy remains very close to the accuracy we have observed before; 
hence, we can conclude that the model generalizes well.

Logistic Regression with Python
[ 398 ]
Model validation
Once the model has been built and evaluated, the next step is to validate the model. 
In the case of logistic regression models or classification models in general, we 
basically validate the model by comparing the actual class with the predicted class. 
There are various ways to do this, but the most famous and widely used is the 
Receiver Operating Characteristic (ROC) curve.
The ROC curve
An ROC curve is a graphical tool to understand the performance of a classification 
model. For a logistic regression model, a prediction can either be positive or 
negative. Also, this prediction can either be correct or incorrect.
There are four categories in which the predictions of a logistic regression model  
can fall:
Actual/predicted
Positive
Negative
Positive
True Positive (TP):
•	 Correct positive 
prediction
•	 Actually positive and 
prediction is also positive
True Negative (TN):
•	 Correct negative 
prediction
•	 Actually negative 
and prediction is also 
negative
Negative
False Positive (FP):
•	 Incorrect positive 
prediction
•	 Actually negative and 
prediction is positive
False Negative (FN):
•	 Incorrect negative 
prediction
•	 Actually positive and 
prediction is negative
So, True Positives are the ones that are actually positive, and the model has also 
predicted a positive outcome for them. False Positives are false successes. These are 
actually failures, but the model is predicting them as successes. False Negatives are 
actually successes, but the model predicts them as failures.
Let us state some totals in terms of these categories:
•	
The total number of actual positive = TP+FN
•	
The total number of actual negative = TN+FP
•	
The total number of correct predictions = TP+TN
•	
The total number of incorrect predictions = FP+FN

Chapter 6
[ 399 ]
Aware of these terms, we can now understand the terms that are the constituents of a 
ROC curve. These terms are as follows:
Sensitivity (True Positive Rate): This is the proportion of the positive outcomes that 
are identified as such (as positives) by the model:
/ (
)
Sensitivity
TP
TP
FN
=
+
Specificity (True Negative Rate): This is the proportion of the negative outcomes 
that are identified as such (as negatives) by the model:
/ (
)
Specificity
TN
TN
FP
=
+
Sensitivity wards off against False Positive, while the Specificity does the  
same against False Negative. A perfect model will be 100% sensitive and  
also 100% specific.
An ROC curve is a plot of True Positive Rate vs False Positive Rate where False 
Positive Rate=FP/(TN+FP) =1-Specificity.
As we saw earlier, the number of positive and negative outcomes change as we 
change the threshold of probability values to classify a probability value as a positive 
or negative outcome. Thus, the Sensitivity and Specificity will change as well.
An ROC curve has the following important properties:
•	
Any increase in Sensitivity will decrease the Specificity
•	
The closer the curve is to the left and upper border of the quadrant, the better 
the model prediction
•	
The closer the curve is to the diagonal line, the worse the model prediction is
•	
The larger the area under the curve, the better the prediction
The following are the steps in plotting an ROC curve:
1.	 Define several probability thresholds and calculate Sensitivity and 
1-Specificity for each threshold.
2.	 Plot Sensitivity and 1-Specificity points obtained in this way.
Let us plot the ROC curve for the model we built earlier in this chapter by following 
the steps described above. Later, we will see how to do it using the built-in methods 
in scikit-learn.

Logistic Regression with Python
[ 400 ]
This is the model that we ran and calculated the probabilities for each observation:
from sklearn.cross_validation import train_test_split
from sklearn import linear_model
from sklearn import metrics
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_
size=0.3, random_state=0)
clf1 = linear_model.LogisticRegression()
clf1.fit(X_train, Y_train)
probs = clf1.predict_proba(X_test)
Each probability value is then compared to a threshold probability, and categorized 
as 1 (positive outcome) if it is greater than threshold probability and 0 if less than 
threshold probability. It can be done using the following code snippet (we have 
chosen a threshold probability of 0.05 in this case):
prob=probs[:,1]
prob_df=pd.DataFrame(prob)
prob_df['predict']=np.where(prob_df[0]>=0.05,1,0)
prob_df['actual']=Y_test
prob_df.head()
The resulting data frame looks as follows:
Fig. 6.26: Predicted and actual outcomes for the bank dataset
Confusion matrix
The result of how many correct and incorrect predictions were made can be 
summarized using what is called a confusion matrix. A confusion matrix is just 
a tabular representation to state the number of TPs, TNs, FPs, and FNs. Once we 
have a data frame in such a format, we can calculate the confusion matrix using the 
crosstab statement as follows:
confusion_matrix=pd.crosstab(prob_df['actual'],prob_df['predict'])
confusion_matrix

Chapter 6
[ 401 ]
The confusion matrix in this case is as follows:
At p=0.05:
Predict
actual
0
1
0
1
413
15
701
107
TN
413
FN
15
FP
701
TP
107
Sensitivity
1-Specificity
=107/
(107+15)
=0.87
=701/
(413+701)
=0.62
Fig. 6.27.1: Confusion matrix at p=0.05
At p=0.10:
Predict
actual
0
1
0
1
847
46
267
76
TN
847
FN
46
FP
267
TP
76
Sensitivity
1-Specificity
=76/
(76+46)
=0.62
=267/
(267+847)
=0.23
Fig. 6.27.2: Confusion matrix at p=0.10
The Sensitivity and Specificity are calculated at various other probability threshold 
levels and then the Sensitivity and (1-Specificity) are plotted against each other. The 
Sensitivity and (1-Specificity) or FPR at different threshold probability values are 
summarized as follows:
Threshold p
Sensitivity
(1-Specificity)
0.05
0.87
0.62
0.10
0.62
0.23
0.07
0.67
0.27
0.12
0.59
0.17
0.20
0.50
0.12
0.25
0.41
0.07
0.04
0.95
0.76

Logistic Regression with Python
[ 402 ]
As one can observe, as the threshold of the probability increases, both the Sensitivity 
and the FPR (1-Specificity) decreases. Now, we have the Sensitivity and Specificity 
at different threshold probabilities. We can make use of this data to plot our ROC 
curve. A diagonal (y=x) line is a good benchmark for an ROC curve. If the ROC  
curve lies above the diagonal line, then the model is considered a better predictor 
than a random guess (represented by a diagonal line). An ROC curve lying below  
the diagonal line indicates that the model is a worse predictor compared to a  
random guess.
Let us plot our ROC curve and also the diagonal line and see whether the ROC  
curve lies above the diagonal line or below. This can be done using the following 
code snippet:
import matplotlib.pyplot as plt
%matplotlib inline
Sensitivity=[1,0.95,0.87,0.62,0.67,0.59,0.5,0.41,0]
FPR=[1,0.76,0.62,0.23,0.27,0.17,0.12,0.07,0]
plt.plot(FPR,Sensitivity,marker='o',linestyle='--',color='r')
x=[i*0.01 for i in range(100)]
y=[i*0.01 for i in range(100)]
plt.plot(x,y)
plt.xlabel('(1-Specificity)')
plt.ylabel('Sensitivity')
plt.title('ROC Curve')
The ROC curve looks as follows:
Fig. 6.28: The ROC curve drawn without using the scikit-learn methods

Chapter 6
[ 403 ]
The curve in red is the ROC curve, while the blue line is the benchmark diagonal 
line. The ROC curve lies above the diagonal line and, hence, the model is a better 
predictor than a random guess. However, there can be many ROC curves lying 
above the diagonal line. How to determine one ROC curve is better than the other? 
This is determined by calculating the area enclosed under the ROC curves. The more 
the area enclosed by the ROC curve, the better it is. The area under the curve can lie 
between 0 and 1. The closer it is to 1, the better it is.
Let us now see how we can draw an ROC curve and calculate the area under 
the curve using the methods built in scikit-learn. This can be done using the 
following code snippet. Make sure you install the ggplot package before running 
this snippet:
from sklearn import metrics
from ggplot import *
prob = clf1.predict_proba(X_test)[:,1]
fpr, sensitivity, _ = metrics.roc_curve(Y_test, prob)
df = pd.DataFrame(dict(fpr=fpr, sensitivity=sensitivity))
ggplot(df, aes(x='fpr', y='sensitivity')) +\
    geom_line() +\
    geom_abline(linetype='dashed')
Fig. 6.29: The ROC curve drawn using the scikit-learn methods

Logistic Regression with Python
[ 404 ]
The area under the curve can be found out as follows:
auc = metrics.auc(fpr,sensitivity)
auc
The area under the curve comes out to be 0.76, which is pretty good. The area under 
the curve can be plotted using the following code snippet:
ggplot(df, aes(x='fpr', ymin=0, ymax='sensitivity')) +\
    geom_area(alpha=0.2) +\
    geom_line(aes(y='sensitivity')) +\
    ggtitle("ROC Curve w/ AUC=%s" % str(auc))
The plot looks as follows:
Fig. 6.30: The area under the ROC curve drawn using the scikit-learn methods

Chapter 6
[ 405 ]
Summary
A logistic regression is a versatile technique used widely in the cases where the 
variable to be predicted is a binary (or categorical) variable. This chapter dives deep 
into the math behind the logistics regression and the process to implement it using 
the scikit-learn and statsmodel api modules. It is important to understand 
the math behind the algorithm so that the model is not used as a black box without 
knowing what is going on behind the hood. To recap, the following are the main 
takeaways from the chapter:
•	
Linear regression wouldn't be an appropriate model to predict binary 
variables as the predictor variables can range from -infinity to +infinity, 
while the binary variable would be 0 or 1.
•	
The odds of a certain event happening is the probability of that event 
happening divided by the probability of that event not happening. The 
higher the odds, the higher are the chances of the event happening. The  
odds can range from 0 to infinity.
•	
The final equation for the logistic regression is:
(
/1
)
1
1
2
2
3
3
log P
P
a
b
X
b
X
b
X
bn
Xn
−
=
+
∗
+
∗
+
∗
+
+
∗
……………
•	
The variable coefficients are calculated using the maximum Log-likelihood 
estimate. The roots of the equation are often calculated using the Newton-
Raphson method.
•	
Each coefficient estimate has a Wald statistic and p-value associated to it.  
The smaller the p-value, the more significant the variable coefficient is to  
the model.
•	
The model can be validated using the k-fold cross validation technique, 
wherein the logistic regression model is run k-times using the testing and 
training data derived from the overall dataset.
•	
The model predicts the probability for each observation. A threshold 
probability value is defined to categorize the probability values as  
0 (failures) and 1 (successes).
•	
Sensitivity measures what proportion of successes were actually identified 
as successes, while Specificity measures what proportion of failures were 
actually identified as failures.

Logistic Regression with Python
[ 406 ]
•	
An ROC curve is a plot of Sensitivity vs (1-Specificity). A diagonal (y=x) line 
is a good benchmark for the ROC curve. If the curve lies above the diagonal 
line, the model is better than a random guess. If the curve lies below, then the 
model is worse than a random guess.
It will do wonders for your understanding of logistic regression if you take a dataset 
and try implementing a logistic regression model on it. In the next chapter, we will 
learn about an unsupervised algorithm called Clustering or Segmentation that is 
used widely in marketing and natural sciences.

[ 407 ]
Clustering with Python
In the previous two chapters, we discussed and understood two important 
algorithms used in predictive analytics, namely, linear regression and logistic 
regression. Both of them are very widely used. They are supervised algorithms. If 
you stress your memory a tad bit and have thoroughly read the previous chapters 
of the book, you would remember that a supervised algorithm is one where 
the historical value of an output variable is known from the data. A supervised 
algorithm uses this value to train and build the model to forecast the value of an 
output variable for a dataset in future. An unsupervised algorithm, on the other 
hand, doesn't have the luxury or constraints (different perspectives of looking at it) 
of the output variable. It uses the values of the predictor variables instead to build  
a model.
Clustering—the algorithm that we are going to discuss in this chapter—is an 
unsupervised algorithm. Clustering or segmentation, as the name suggests, 
categorizes entries in clusters or segments in which the entries are more similar to 
each other than the entries outside the cluster. The properties of such clusters are 
then identified and treated separately. Once the clusters are defined, one can identify 
the properties of the cluster and define plans or strategy separately for each cluster. 
This results in efficient strategizing and planning for each cluster.
The broad focus of this chapter will be clustering and segmentation and by the end 
of this chapter, you would be able to learn the following:
•	
Math behind the clustering algorithms: This section will talk about 
the various kinds of measures of similarity or dissimilarity between 
observations. The similarity or dissimilarity is measured in something  
called distances. We will look at different types of distances and create 
distance metrics.

Clustering with Python
[ 408 ]
•	
Different types of clustering algorithms: This section has information about 
two kinds of clustering algorithms, namely, hierarchical clustering and 
k-means clustering. The details of the two algorithms will be illustrated  
using tables and code simulations.
•	
Implementing clustering using Python: This section will deal with 
implementing k-means clustering algorithm on a dataset from scratch, 
analyzing and making sense of the output, generating plots showing the 
clusters, and making contextual sense of the clusters.
•	
Fine-tuning the clustering: In this section, we will cover topics such as 
finding the optimum number of clusters and calculating a few statistics to 
check the efficiency of the clustering we performed.
Introduction to clustering – what, why,  
and how?
Now let us discuss the various aspects of clustering in greater detail.
What is clustering?
Clustering basically means the following:
•	
Creating a group with a high similarity among the members of clusters
•	
Creating a group with a significant distinction or dissimilarity between the 
members of two different clusters
The clustering algorithms work on calculating the similarity or dissimilarity between 
the observations to group them in clusters.
How is clustering used?
Let us look at the plot of Monthly Income and Monthly Expense for a group of 
400 people. As one can see, there are visible clusters of people whose earnings and 
expenses are different from people from other clusters, but are very similar to the 
people in the cluster they belong to:

Chapter 7
[ 409 ]
Fig. 7.1: Illustration of clustering plotting Monthly Income vs Monthly Expense
In the preceding plot, the visible clusters of the people can be identified based on 
their income and expense levels, as follows:
•	
1 (low income, low expense): The cluster marked as 1 has low income and 
low expense levels
•	
2 (medium income, less expense): The cluster marked as 2 has a medium 
level of income, but spend less—only a little higher than the people in the 
cluster 1 with low income
•	
3 (medium income, medium or high expense): The cluster marked as 3 also 
has medium levels of income, almost the same range as cluster 2, but they 
spend more than cluster 2
•	
4 (high income, high expense): The cluster marked as 4 has a high level of 
income and a high level of expense
This analysis can be very helpful if, let's say, an organization is trying to target 
potential customers for their different range of products. Once the clusters are 
known, the organization can target different clusters for different ranges of their 
products. Maybe, they can target the cluster 4 to sell their premium products and 
cluster 1 and 2 to sell their low-end products. This results in higher conversion rates 
for the advertisement campaigns.

Clustering with Python
[ 410 ]
This was one of the illustrations of how clustering can be advantageous. This was a 
very simple case with just the two attributes of the potential customers, and we were 
able to plot it on a 2D graph and look at the clusters. However, this is not the case 
for most of the time. We need to define some generalized metric for the similarity or 
dissimilarity of the observations. Also, we will discuss this in detail later in  
this chapter.
Some of the properties of a good cluster can be listed as follows:
•	
Clusters should be identifiable and significant in size so that they can be 
classified as one.
•	
Points within a cluster should be compactly placed within themselves and 
there should be minimum overlap with points in the other clusters.
•	
Clusters should make business sense. The observations in the same cluster 
should exhibit similar properties when it comes to the business context.
Why do we do clustering?
Clustering can have a variety of applications. The following are some of the cases 
where clustering is used:
•	
Clustering and segmentation are the bread and butter of the marketing 
professionals. The advent of digital marketing has made clustering 
indispensable. The goal here is to find customers who think, behave, and 
make decisions on similar lines and reach out to them and persuade them in 
a fashion tailor-made for them. Think of Facebook and sponsored posts. How 
do they use the demography, age group, and your preferences data to show 
you the most relevant posts?
•	
Remember those taxonomy charts in Biology books from high school? Well, 
that is one of the most widely used applications of clustering—a particular 
type called hierarchical clustering. The clustering, in this case, happens on 
the basis of the similarity between sequences of amino acids between the two 
genus/species.
•	
Clustering is used in seismology to find the expected epicenter of 
earthquakes and identify earthquake-prone zones.
•	
Clustering is also used to impute values to the missing elements in a dataset. 
Remember that we imputed the missing values with the mean of the rest of 
the observations. To start with, some forms of clustering require calculating 
or assuming the centroid of the clusters. These can be used to impute  
missing values.

Chapter 7
[ 411 ]
•	
Clustering is used in urban planning to group together houses according to 
their geography, value, and amenities. It can also be used to identify a spot 
for public amenities such as public transport stops, mobile signal towers,  
and so on.
Mathematics behind clustering
Earlier in this chapter, we discussed how a measure of similarity or dissimilarity is 
needed for the purpose of clustering observations. In this section, we will see what 
those measures are and how they are used.
Distances between two observations
If we consider each observation as a point in an n-dimensional space, where n is 
the number of columns in the dataset, one can calculate the mathematical distance 
between the points. The lesser the distance, the more similar they are. The points that 
are less distant to each other will be clubbed together.
Now, there are many ways of calculating distances and different algorithms use 
different methods of calculating distance. Let us see the different methods with a few 
examples. Let us consider a sample dataset of 10 observations with three variables, 
each to illustrate the distance better. The following dataset contains percentage 
marks obtained by 10 students in English, Maths, and Science:
Student
English
Maths
Science
1
0.12
0.49
0.21
2
0.21
0.81
0.79
3
0.73
0.30
0.99
4
0.55
0.03
0.17
5
0.15
0.83
0.25
6
0.24
0.37
0.63
7
0.20
0.82
0.85
8
0.17
0.92
0.45
9
0.26
0.16
0.31
10
0.15
0.47
0.23
Table 7.1: Percentage marks obtained by 10 students in English, Maths, and Science

Clustering with Python
[ 412 ]
Euclidean distance
This is the most commonly used definition of the distance. Let us denote each 
observation as Xi. Then, the Euclidean distance between the two points, Xi and Xj, 
for a dataset having n-columns, is defined as follows:
(
)
(
)
(
)
2
2
2
.
.1
.1
.2
.2
.
.
i j
i
j
i
j
i n
j n
D
X
X
X
X
X
X
=
−
+
−
+
+
−
……
For our dataset, the Euclidean distance between the student 1 and 2 is calculated  
as follows:
(
)
(
)
(
)
2
2
2
1.2
0.12
0.21
0.49
0.81
0.21 0.79
0.67
S
=
−
+
−
+
+
−
=
……
Thus, the Euclidian distance between the student 1 and 2 comes out to be 0.67.
Manhattan distance
Manhattan distance is defined as follows:
(
)
.
.1
.1
.2
.2
.
.
|
|
|
|
|
|
i j
i
j
i
j
i n
j n
D
X
X
X
X
X
X
=
−
+
−
+
+
−
……
Minkowski distance
Minkowski distance is defined as follows:
(
)
(
)
(
)
(
)
.
.1
.1
.2
.2
.
.
|
|
|
|
|
|
1/
P
P
P
i j
i
j
i
j
i n
j n
D
X
X
X
X
X
X
p
=
−
+
−
+
+
−
……
where p>=1
The distance matrix
Once we have defined the distance between the two points, we can calculate 
the distance between any two points and store them as a matrix. This matrix 
representing the distance between any two points (observations) in the dataset is 
called a distance matrix. A distance matrix has certain properties as follows:
•	
All the diagonal elements in a distance matrix have a value 0 as they 
represent the distance of the point from itself, that is, Di,i=0 for all i's.

Chapter 7
[ 413 ]
•	
For a dataset with n observations, we get a nxn distance matrix.
•	
A distance matrix is a symmetric matrix as the distance between the two 
points is the same, irrespective of the order of the points. The distance 
between point 1 and point 2 is the same as the difference between point 2  
and point 1. This implies Di,j = Dj,I for all i's and j's.
For our dataset containing information about the marks of 10 students, it would 
be a 10x10 matrix. If we decide to calculate the matrix for Euclidean distances, the 
distance matrix will look as follows:
 
1
2
3
4
5
6
7
8
9
10
1
0.00
0.67
1.01
0.63
0.33
0.45
0.72
0.49
0.37
0.04
2
0.67
0.00
0.76
1.06
0.55
0.47
0.06
0.36
0.81
0.66
3
1.01
0.76
0.00
0.88
1.08
0.62
0.76
1.00
0.84
0.97
4
0.63
1.06
0.88
0.00
0.89
0.65
1.10
1.01
0.35
0.60
5
0.33
0.55
1.08
0.89
0.00
0.60
0.60
0.23
0.68
0.35
6
0.45
0.47
0.62
0.65
0.60
0.00
0.50
0.58
0.38
0.41
7
0.72
0.06
0.76
1.10
0.60
0.50
0.00
0.41
0.85
0.71
8
0.49
0.36
1.00
1.01
0.23
0.58
0.41
0.00
0.78
0.50
9
0.37
0.81
0.84
0.35
0.68
0.38
0.85
0.78
0.00
0.34
10
0.04
0.66
0.97
0.60
0.35
0.41
0.71
0.50
0.34
0.00
Table 7.2: Distance matrix for marks information for 10 students
Each element of this matrix has been calculated in the same way as we calculated 
S1,2, only that the different rows are used for calculation. One can observe that 
these two properties stated above are satisfied. The diagonal elements are all 0. 
Interpreting a distance matrix is simple. The value in the 2nd row and the 1st column 
gives the distance between the 2nd and the 1st student (or the 1st and the 2nd 
student), and so on. So, D1,3=1.01, D2,5 = 0.55, D6,8 = 0.58, and so on.
Normalizing the distances
These distances can sometimes be misleading if the variables are not in the same 
numerical range. What happens in such cases is that the variables having larger 
numerical values start influencing the distance more than the variables having 
smaller numerical values. This gives undue importance to the variables with  
large numerical values and subdues the importance of the ones with small  
numerical values.

Clustering with Python
[ 414 ]
The following dataset has the data of the Monthly Income, Monthly Expense, and 
Education Level for 10 customers. Look at the 1 and 9 customers. The difference in 
their monthly income is 1758. The difference in the monthly expense is 7707. Also, 
the difference in the education level is just 3. The contribution of the education 
level in distance becomes insignificant compared to the contribution of the other 
two variables even though, actually, the difference of 3 in the education level might 
matter more. This creates a problem as the distance is not a reliable measure of 
similarity anymore and can give rise to incorrect clustering.
Let us look at the following datasheet:
Fig. 7.2: Monthly Income, Monthly Expense and Education Levels
The solution to this problem is normalizing the values of the variables to bring them 
in the same numerical range. The normalization entails the numerical transformation 
of each value so that they come in the same numerical range. The following method 
is used to normalize values. This method uses the min and max values and the 
transformation is defined as follows:
(
) (
)
/
i
i
min
max
min
Z
X
X
X
- X
=
−
Note that Xi is the value of the variables, Xmin is the minimum value of the  
variable, Xmax is the maximum value of the variable, and Zi is the normalized  
value of the variable.
The preceding dataset shows us how to normalize using the preceding equation;  
the datasheet looks as shown:

Chapter 7
[ 415 ]
Fig. 7.3: Monthly Income, Monthly Expense, and Education Levels after normalization
Linkage methods
While clustering the observations using the distances, one needs to link two or more 
points or clusters to form a new cluster or grow an already existing cluster. The 
distance between two clusters can be defined in many ways; for example, it can be 
the minimum distance between two points in two clusters, the maximum distance 
between two such points, or the distance between the centroids of the two clusters. 
The two clusters having the minimum distance between each other are clubbed 
together. Corresponding to each definition of the distance between the two  
clusters, there is a linkage method. Some of these linkage methods are shown  
in the following sections.
Single linkage
•	
The distance between two clusters is the minimum distance between a point 
in cluster 1 and cluster 2
•	
Two clusters having the smallest distance between them are combined  
as follows:
(
)
(
)
(
)
,
,
d Cm Cn
min d i j
=
Here, Cm and Cn are two clusters; i and j are are points in the m and n clusters.

Clustering with Python
[ 416 ]
Compete linkage
•	
The distance between two clusters is the maximum distance between a point 
in cluster 1 and cluster 2
•	
Two clusters having the smallest distance between them are combined  
as follows:
(
)
(
)
(
)
,
,
d Cm Cn
max d i j
=
Here, Cm and Cn are two clusters; i and j are points in the m and n clusters.
Average linkage
•	
The distance between two clusters is the average distance between a point in 
cluster 1 and cluster 2
•	
Two clusters having the smallest distance between them are combined  
as follows:
(
)
(
)
(
)
,
,
d Cm Cn
avg d i j
=
Here, Cm and Cn are two clusters; i and j are points in the m and n clusters.
Centroid linkage
•	
The distance between two clusters is the distance between the centroid 
(mean) of all the points in cluster 1 and the centroid (mean) of all the points 
in cluster 2
•	
Two clusters having the smallest distance between them are combined  
as follows:
(
)
(
)
(
)
(
)
,
,
d Cm Cn
d centroid Cm
centroid Cn
=
Here, Cm and Cn are two clusters.
Ward's method
A cluster that minimizes the increase in the combined error sum of the square 
of ANOVA is joined to an already existing cluster to form a new joined cluster. 
ANOVA is a statistical method to check whether there is more variation within the 
cluster or in the overall dataset. The smallest increase in the ANOVA error term 
shows that the elements of the newly joined clusters are more similar to each other 
than elements in other clusters.

Chapter 7
[ 417 ]
Hierarchical clustering
Hierarchical clustering is an agglomerative method of clustering, wherein we start 
with each point as a separate cluster and then agglomerate them in a single cluster 
based on the similarity between observations.
For a dataset with N observations and NxN distance matrix, a hierarchical cluster can 
be created using the following steps:
1.	 Start with each observation as a cluster so that you have N clusters to  
start with.
2.	 Find the smallest distance in the distance matrix. Join the two observations 
having the smallest distance to form a cluster.
3.	 Recompute the distances between all the old clusters and the new clusters.  
If one follows a single linkage method, the distance between two clusters is 
the minimum distance between two points on the two clusters.
4.	 Repeat the steps 2 and 3 until you are left with a single cluster of all  
the N observations.
Let us take the distance matrix we created earlier in this chapter and follow the 
above steps to create a hierarchical cluster. The distance matrix, as we have seen 
before, looks like this:
Fig. 7.4: Distance matrix for the students' marks

Clustering with Python
[ 418 ]
Iteration 1:
Fig. 7.5: The first iteration of clustering
Iteration 2:
Fig. 7.6: The second iteration of clustering
Iteration 3:
Fig. 7.7: The third iteration of clustering

Chapter 7
[ 419 ]
Iteration 4:
Fig. 7.8: The fourth iteration of clustering
Next, iterations can be performed in a similar manner. Here we used a single linkage 
method. If we use a different linkage method, the clustering would take a different 
shape. The hierarchical clusters can be best understood using a hierarchical tree 
depicting when and where the two or more points/clusters joined to form a  
bigger cluster.
For the distance matrix used above and the single method of linkage, one would get 
the following tree structure:
Fig. 7.9: A tree using a single linkage method

Clustering with Python
[ 420 ]
While using Ward's method, one gets a tree as shown in the following figure:
Fig. 7.10: A tree using Ward's linkage method
The tree formed using a single linkage can be interpreted as follows:
•	
The #1 and #10 observations are joined to form a cluster at first, followed by 
a cluster formed by #2 and #7.
•	
The #5 and #8 observations form a different cluster that later joins the cluster 
formed by #1 and #10.
•	
This bigger cluster containing #1, #10, #5, and #8 is joined by the cluster 
containing #2 and #7, and so on
•	
The observations that joined earlier to form a cluster are more similar to each 
other. So, (#1, #10), (#2, #7), and (#5, #8) are similar to each other.
Hierarchical clusters produce efficient and analyzable results when the number of 
observations in the dataset is relatively less. As the number of observations increase, 
the trees resulting from the hierarchical clusters become messier and more difficult to 
analyze. In such cases, it is better to try another method of clustering.
K-means clustering
K-means clustering is another unsupervised algorithm to cluster a number of 
observations. It is different from the hierarchical cluster in the sense that here the 
number of desired clusters and the centroid of the clusters need to be defined prior 
to the model formation. The centroid of the clusters keep updating based on the 
observations assigned to that cluster. The output consists of an array containing the 
cluster number to which each observation belongs to.

Chapter 7
[ 421 ]
A step-by-step detail of the k-means clustering algorithm is as follows:
1.	 Decide the number of clusters and assign the cluster centroid for each cluster. 
The number of clusters can be decided based on the business context. The 
cluster centroids can be passed manually (based on the business context or 
some prior information), or the randomly chosen observations can serve as 
the cluster centroids to start with.
2.	 Calculate the distance of each observation from each cluster. Assign the 
observation to the cluster from which its distance is the least. The distance  
of an observation from a cluster is defined.
3.	 Recalculate the cluster centroid using the mean of the observations in the 
cluster. The following formula defines the update of the cluster centroids:
1
1
ci
m
vi
Xm
ci
=
=
∑
Here, ci=number of observations in the clusters, and Xm=observation vector 
whose length is equal to the number of columns in the observation. A cluster 
centroid is also as long as the observation vector or the number of columns in 
the observation.
4.	 Repeat the steps starting from step 2.
5.	 Stop if none of the observations were reassigned from one cluster to another.
None of the observations are being reassigned; that means that all the 
observations are already in the correct clusters and their distance to a  
cluster centroid can't be reduced further.
The goal of this algorithm is to attain a configuration of cluster centers and cluster 
observation so that the overall J squared error function or J-score is minimized:
(
)
2
1
1
c
ci
i
j
J
Xj
Vi
=
=
=
−
∑∑
Here, c=number of clusters, ci=number of points in the cluster, and Vi=centroid of the ith cluster.
The J squared error function can be understood as the sum of the squared distance of 
points from their respective cluster centroids. A smaller value of J squared function 
implies tightly packed and homogeneous clusters. This also implies that most of the 
points have been placed in the right clusters.

Clustering with Python
[ 422 ]
Let us try the k-means clustering algorithm for clustering some random numbers 
between 0 and 1. The Python library and Scipy have some inbuilt methods to perform 
the algorithm and return a list defining which observation belongs to which cluster:
1.	 Define a set of observations consisting of random numbers ranging from  
0 to 1. In this case, we have defined an observation set of 30x3:
Import numpy as np
obs=np.random.random(90).reshape(30,3)
obs
The output of the code looks as follows:
Fig. 7.11: A 30x3 array of random numbers between 0 and 1
2.	 Decide that we want two clusters (no hard and fast rule, you can try  
with three clusters also). Select two observations at random to make  
them cluster centroids:
c1=np.random.choice(range(len(obs)))
c2=np.random.choice(range(len(obs)))
clust_cen=np.vstack([obs[c1],obs[c2]])
clust_cen
The output of the code looks as follows:
Fig. 7.12: Selecting two rows (out of 30) at random to be initial cluster centers
The two rows in the clust_cen array correspond to the two  
cluster centroids.
3.	 With the number of clusters and cluster centroids defined, one is ready to 
implement the k-means clustering. This can be done using the cluster 
method of Scipy:
from scipy.cluster.vq import vq
vq(obs,clust_cen)

Chapter 7
[ 423 ]
Fig. 7.13: Cluster label and distance from cluster centers for each observation
The first array gives us the information as to which cluster the observation 
belongs to. The first observation belongs to cluster c2, the second observation 
belongs to c1, the third belongs to c2, the fourth to c1, and so on.
The second array gives the distance of the observation from the final cluster 
centroid. Hence, the first observation is at a distance of 0.33 units from the 
centroid of the cluster c2, the second observation is at a distance of 0.43 from 
the centroid of the cluster c1, and so on.
4.	 Find the cluster centroid for the two clusters. This is done using the kmeans 
method in Scipy:
from scipy.cluster.vq import kmeans
kmeans(obs,clust_cen)
The output of the code looks as follows:
Fig. 7.14: The final cluster centers and the value of the squared error function or J-score
The two rows in the array correspond to the two final cluster centroids. The 
centroid of the first cluster is at (0.524, 0.837, 0.676). The number at the end is 
the value of the squared error function or J-score, which we seek to minimize. 
Its value comes out to be 0.35.
K-means also works if one provides just the number of required clusters and 
not the cluster centroids. If only the required number of clusters is provided, 
then the method will randomly select that many observations at random 
from the observation set to become a cluster centroid. Thus, we could have 
also written the following:
from scipy.cluster.vq import kmeans
kmeans(obs,2)

Clustering with Python
[ 424 ]
Implementing clustering using Python
Now, as we understand the mathematics behind the k-means clustering  
better, let us implement it on a dataset and see how to glean insights from the 
performed clustering.
The dataset we will be using for this is about wine. Each observation represents a 
separate sample of wine and has information about the chemical composition of that 
wine. Some wine connoisseur painstakingly analyzed various samples of wine to 
create this dataset. Each column of the dataset has information about the composition 
of one chemical. There is one column called quality as well, which is based on the 
ratings given by the professional wine testers.
The prices of wines are generally decided by the ratings given by the professional 
testers. However, this can be very subjective and certainly there is a scope for a 
more logical process to wine prices. One approach is to cluster them based on their 
chemical compositions and quality and then price the similar clusters together based 
on the desirable components present in the wine clusters.
Importing and exploring the dataset
Let us import and have a look at this dataset:
import pandas as pd
df=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/Book 
Datasets/Clustering/wine.csv',sep=';')
df.head()
The output looks as follows:
Fig. 7.15: The first few observations of the wine dataset
As one can observe, it has 12 columns as follows:
Fig. 7.16: The column names of the wine dataset

Chapter 7
[ 425 ]
There are 1599 observations in this dataset.
Let us focus on the quality variable for a while and plot a histogram to see the 
number of wine samples in each quality type:
import matplotlib.pyplot as plt
% matplotlib inline
plt.hist(df['quality'])
The code shows the following output:
Fig. 7.17: The histogram of wine quality. The majority of samples have been rated 6 or 7 for quality
As it is evident from the plot, more than 75% of the samples were assigned the 
quality of 5 and 6. Also, let's look at the mean of the various chemical compositions 
across samples for the different groups of the wine quality:
df.groupby('quality').mean()
The code shows the following output:
Fig. 7.18: The mean values of all the numerical columns for each value of quality

Clustering with Python
[ 426 ]
Some observations based on this table are as follows:
•	
The lesser the volatile acidity and chlorides, the higher the wine quality
•	
The more the sulphates and citric acid content, the higher the wine quality
•	
The density and pH don't vary much across the wine quality
Next, let's proceed with clustering these observations using k-means.
Normalizing the values in the dataset
As discussed above, normalizing the values is important to get the clustering right. 
This can be achieved by applying the following formula to each value in the dataset:
(
) (
)
/
i
i
min
max
min
Z
X
X
X
X
=
−
−
To normalize our dataset, we write the following code snippet:
df_norm = (df - df.min()) / (df.max() - df.min())
df_norm.head() 
This results in a data frame with normalized values for entire data frame as follows:
Fig. 7.19 Normalized wine dataset
Hierarchical clustering using scikit-learn
Hierarchical clustering or agglomerative clustering can be implemented using the 
AgglomerativeClustering method in scikit-learn's cluster library as shown in 
the following code. It returns a label for each row denoting which cluster that row 
belongs to. The number of clusters needs to be defined in advance. We have used the 
ward method of linkage:
from sklearn.cluster import AgglomerativeClustering
ward = AgglomerativeClustering(n_clusters=6, linkage='ward').fit(df_
norm)
md=pd.Series(ward.labels_)

Chapter 7
[ 427 ]
We can plot a histogram of cluster labels to get a sense of how many rows belong to a 
particular cluster:
import matplotlib.pyplot as plt
% matplotlib inline
plt.hist(md)
plt.title('Histogram of Cluster Label')
plt.xlabel('Cluster')
plt.ylabel('Frequency')
The plot looks as follows. The observations are more uniformly distributed across the 
cluster except Cluster 2 that has more observations than the others:
Fig. 7.20: The histogram of Cluster Labels. Samples are more-or-less uniformly distributed across clusters
It also outputs the children for each non-leaf node. This would be an array with the 
shape (number of non-leaf nodes, 2) as there would be two immediate children for 
any non-leaf node:
ward.children_
The code shows the following output:
Fig. 7.21: The child array containing two child elements for each non-leaf node

Clustering with Python
[ 428 ]
K-Means clustering using scikit-learn
Let us randomly choose 6 as the required number of clusters for now as there  
were that many groups of quality in the dataset. Then, to cluster the observations, 
one needs to write the following code snippet:
from sklearn.cluster import KMeans
from sklearn import datasets
model=KMeans(n_clusters=6)
model.fit(df_norm)
The preceding snippet fits the k-means clustering model to the wine dataset. To 
know which observation belongs to which of the clusters, one can call the labels_ 
parameter of the model. It returns an array depicting the cluster the row belongs to:
model.labels_
The output of the code is as follows:
Fig. 7.22: Cluster labels for each row
For better observation, let us make this array part of the data frame so that we can 
look at the cluster each row belongs to, in the same data frame:
md=pd.Series(model.labels_)
df_norm['clust']=md
df_norm.head()
The output of the code shows the following datasheet:
Fig. 7.23: The wine dataset with a clust column depicting the cluster the row belongs to
The last column clust of the data frame denotes the cluster to which that particular 
observation belongs. The 1st, 2nd, 3rd, and 5th observations belong to the 3rd cluster 
(counting starts from 0), while the 4th observation belongs to the 2nd cluster.
The final cluster's centroids for each cluster can be found out as follows:
model.cluster_centers_

Chapter 7
[ 429 ]
Note that each cluster centroid would have 12 coordinates as there are 12 variables in 
the dataset.
The dataset is as follows:
Fig. 7.24: Cluster centroids for each of the six clusters
The J-score can be thought of as the sum of the squared distance between points and 
cluster centroid for each point and cluster. For an efficient cluster, the J-score should 
be as low as possible. The value of the J-score can be found as follows:
model.inertia_
The value comes out to be 186.56.
Let us plot a histogram for the clust variable to get an idea of the number of 
observations in each cluster:
import matplotlib.pyplot as plt
% matplotlib inline
plt.hist(df_norm['clust'])
plt.title('Histogram of Clusters')
plt.xlabel('Cluster')
plt.ylabel('Frequency')

Clustering with Python
[ 430 ]
The code shows the following output:
Fig. 7.25: The histogram of cluster labels
As can be observed, the number of wine samples is more uniformly (or rather 
normally) distributed in this case when compared to the distribution based on the 
wine quality. This is an improvement from the classification based on the wine 
quality as it provides us with better segregated and identifiable clusters.
Interpreting the cluster
This clustering can be used to price the wine samples in the same cluster similarly 
and target the customers who prefer the particular ingredient of wine by marketing 
them as a different brand having that ingredient as its specialty.
Let us calculate the mean of the composition for each cluster and each component.  
If you observe the output table, it is exactly similar to the six cluster centroids  
observed above. This is because the cluster centroids are nothing but the mean  
of the coordinates of all the observations in a particular cluster:
df_norm.groupby('clust').mean()

Chapter 7
[ 431 ]
Fig. 7.26: The mean of all the numerical columns for different clusters
The wine quality and taste mainly depends on the quantity of acid, alcohol, and 
sugar. A few examples of how the information on clustering can be used for efficient 
marketing and pricing are as follows:
•	
People from cooler regions prefer wines with higher volatile acid content.  
So, clusters 2 and 5 can be marketed in cooler (temperature-wise) markets.
•	
Some people might prefer wine with higher alcohol content, and the wine 
samples from clusters 3 and 5 can be marketed to them.
•	
Some connoisseurs trust others' judgment more and they might like to go 
with professional wine testers' judgments. These kinds of people should  
be sold the wine samples from clusters 3 and 5 as they have high  
mean quality.
More information from the wine industry can be combined with this result to form a 
better marketing and pricing strategy.
Fine-tuning the clustering
Deciding the optimum value of K is one of the tough parts while performing a 
k-means clustering. There are a few methods that can be used to do this.
The elbow method
We earlier discussed that a good cluster is defined by the compactness between  
the observations of that cluster. The compactness is quantified by something called 
intra-cluster distance. The intra-cluster distance for a cluster is essentially the sum of 
pair-wise distances between all possible pairs of points in that cluster.

Clustering with Python
[ 432 ]
If we denote intra-cluster distance by W, then for a cluster k intra-cluster, the distance 
can be denoted by:
(
)
(
)
2
1
1
2
1
2
ck
ck
i
j
ck
i
Wk
Xi
Xj
Wk
Nk
Xi
Mk
=
=
=
=
−
=
−
∑∑
∑
Generally, the normalized intra-cluster distance is used, which is given by:
1
1
2
ck
i
Wk
Wk
Nk
=
′ =
∗
∑
Here Xi and Xj are points in the cluster, Mk is the centroid of the cluster, Nk is the 
number of points in the centroid, and K is the number of clusters.
Wk' is actually a measure of the variance between the points in the same cluster. 
Since it is normalized, its value would range from 0 to 1. As one increases the 
number of clusters, the value of Wk' increases marginally until a certain point post 
of this marginal increase stops. At this point, we get an elbow in the curve and this 
gives us the correct number of the cluster as shown in the following graph:
Fig. 7.27: Elbow method to find the correct K

Chapter 7
[ 433 ]
As shown in the preceding plot for a hypothetical dataset, the percentage variance 
explained by the clusters peaks at k=4, post which the marginal increase stops. This 
encircled point in the preceding graph is called the elbow that gives the name to the 
method. The correct number of clusters in this case is then 4.
Silhouette Coefficient
Silhouette Coefficient for an observation in the dataset quantifies how tightly the 
point is bound to the other points in the same cluster and how loosely the point 
is bound to points in the nearest neighbor cluster. The Silhouette Coefficient is 
calculated using the mean intra-cluster distance (b) and the mean nearest-cluster 
distance (a) for each sample:
( )
(
)
(
)
/
,
Silhouettecoefficient S
b
a
max a b
=
−
Let us have a look at the following example to understand the concept of the 
Silhouette Coefficient better:
Fig. 7.28: Illustrating a silhouette coefficient
Let us look at the preceding situation for the point (observation) marked X.  
It has two nearby clusters, A and B; A being relatively closer to X than B. The  
mean intra-cluster distance of X from the points in A is denoted by a. The mean 
intra-cluster distance of X from the points in the next nearest cluster (B in this case) is 
denoted by b. The intra-cluster distance is simply defined as the sum of the distances 
of the point marked X from all the points in a given cluster.

Clustering with Python
[ 434 ]
The Silhouette Coefficient, S, can be rewritten as follows:
(
)
1
/
,
0,
( / ) 1,
a b
if a
b
S
if a
b
b a
if a
b
−
<
=
=
−
>
The value of the Silhouette Coefficient ranges from -1 to 1. A value close to -1 means 
that a is (very) large than b implying that the point is more similar to the neighboring 
cluster (B) than the current cluster (A) and is wrongly placed in the current cluster. 
A value close to 1 means that a is (very) smaller than b and hence a point is placed in 
the correct cluster.
Overall the silhouette coefficient of the entire dataset is a mean of the silhouette 
coefficients of each sample. The value of the Silhouette Coefficient is affected by the 
number of clusters. The Silhouette Coefficients can be used to decide the optimum 
number of clusters as follows:
1.	 Start with two clusters. Calculate the mean silhouette coefficient for  
each cluster.
2.	 Calculate the mean silhouette coefficients of the entire dataset (average over 
all the clusters).
3.	 Check whether the mean silhouette coefficient of any of the clusters is less 
than the overall mean silhouette coefficient of the dataset. If this is the case, 
the number of clusters is suboptimal or a bad pick. If it is not the case, then it 
is a potential candidate for being the optimum number of clusters.
4.	 Repeat the steps 1 to 3 for different numbers of clusters (untill n=6 to 10 or a 
suitable number derived from the context or elbow method). 
5.	 Decide on one of the potential candidates identified in the steps 3 and 4 to be 
the optimum number of clusters.
The elbow method and silhouette coefficients can be used to fine-tune the clustering 
once it has been run assuming some arbitrary number of clusters. The actual 
clustering can then be performed once the optimum number of clusters is known. 
The results from these methods coupled with the business context should give an 
idea about the number of clusters to be used.

Chapter 7
[ 435 ]
Summary
In this chapter, we learned the following:
•	
Clustering is an unsupervised predictive algorithm to club similar data 
points together and segregate the dissimilar points from each other. This 
algorithm finds the usage in marketing, taxonomy, seismology, public policy, 
and data mining.
•	
The distance between two observations is one of the criteria on which the 
observations can be clustered together.
•	
The distance between all the points in a dataset is best represented by an nxn 
symmetric matrix called a distance matrix.
•	
Hierarchical clustering is an agglomerative mode of clustering wherein 
we start with n clusters (equal to the number of points in the dataset) that 
are agglomerated into a lesser number of cluster based on the linkages 
developed over distance matrix.
•	
K-means clustering algorithm is a widely used mode of clustering wherein 
the number of clusters need to be stated in advance before performing the 
clustering. K-means clustering method outputs a label for each row of data 
depicting the cluster it belongs to. It also outputs the cluster centers. K-means 
method is easier to analyze and make sense of.
•	
Deciding the number of clusters (k) for the k-means clustering is an 
important task. The elbow method and silhouette coefficient method are 
some of the methods that can help us to decide the optimum number of k.
In the current chapter, we dealt with an unsupervised algorithm that is very widely 
used. Next, we will learn about a classification supervised algorithm. It is called a 
decision tree. It is a great set of algorithms to classify and predict data.


[ 437 ]
Trees and Random Forests 
with Python
Clustering, discussed in the last chapter, is an unsupervised algorithm. It is now time 
to switch back to a supervised algorithm. Classification is a class of problems that 
surfaces quite frequently in predictive modelling and in various forms. Accordingly, 
to deal with all of them, a family of classification algorithms is used.
A decision tree is a supervised classification algorithm that is used when the target 
variable is a discrete or categorical variable (having two or more than two classes) 
and the predictor variables are either categorical or numerical variables. A decision 
tree can be thought of as a set of if-then rules for a classification problem where the 
target variables are discrete or categorical variables. The if-then rules are represented 
as a tree.
A decision tree is used when the decision is based on multiple-staged criteria and 
variables. A decision tree is very effective as a decision making tool as it has a 
pictorial output that is easier to understand and implement compared to the output 
of the other predictive models.
Decision trees and related classification algorithms will be the theme running 
throughout this chapter. At the end of this chapter, the reader will be able to get a 
basic understanding of the concept of a decision tree, the mathematics behind it, the 
implementation of decision trees and Python, and the efficiency of the classification 
performed through the model.

Trees and Random Forests with Python
[ 438 ]
We will cover the following topics in depth:
•	
Introducing decision trees
•	
Understanding the mathematics behind decision trees
•	
Implementing a decision tree
•	
Understanding and implementing regression trees
•	
Understanding and implementing random forests
Introducing decision trees
A tree is a data structure that might be used to state certain decision rules because 
it can be represented in such a way as to pictorially illustrate these rules. A tree has 
three basic elements: nodes, branches, and leaves. Nodes are the points from where 
one or more branches come out. A node from where no branch originates is a leaf. A 
typical tree looks as follows:
Fig. 8.1: A representation of a decision tree with its basic elements—node, branches, and leaves

Chapter 8
[ 439 ]
A tree, specifically a decision tree, starts with a root node, proceeds to the decision 
nodes, and ultimately to the terminal nodes where the decision rules are made. All 
nodes, except the terminal node, represent one variable and the branches represent 
the different categories (values) of that variable. The terminal node represents the 
final decision or value for that route.
A decision tree
To understand what decision trees look like and how to make sense of them, let us 
consider an example. Consider a situation where one wants to predict whether a 
particular Place will get a Bumper, Moderate, or a Meagre Harvest of a crop based 
on information about the Rainfall, Terrain, availability of groundwater, and usage of 
fertilizers. Consider the following dataset for that situation:
Fig. 8.2 A simulated dataset containing information about the Harvest type and conditions such as the Rainfall, 
Terrain, Usage of Fertilizers, and Availability of Groundwater

Trees and Random Forests with Python
[ 440 ]
For a while, let us forget how these trees are constructed and focus on interpreting  
a tree and how it becomes handy in classification problems. For illustrative  
purposes, let us assume that the final decision tree looks as follows (we will  
learn the mathematics behind it later):
Fig. 8.3: A representational decision tree based on the above dataset
There are multiple decisions one can make using this decision tree:
•	
If Terrain is Plain and Rainfall is High, then the harvest will be Bumper
•	
If Terrain is Plain, Rainfall is Low, and Groundwater is Yes (present), then 
the harvest will be Moderate
•	
If Terrain is Plain, Rainfall is Low, and Groundwater is No (present), then the 
harvest will be Meagre
•	
If Terrain is Hills and Rainfall is High, then the harvest will be Moderate
•	
If Terrain is Hills and Rainfall is Low, then the harvest will be Meagre
•	
If Terrain is Plateau, Groundwater is Yes, and Fertilizer is Yes (present),  
then the harvest will be Bumper
•	
If Terrain is Plateau, Groundwater is Yes, and Fertilizer is No (present),  
then the harvest will be Moderate

Chapter 8
[ 441 ]
•	
If Terrain is Plateau, Groundwater is No, and Fertilizer is Yes (present),  
then the harvest will be Meagre
•	
If Terrain is Plateau, Groundwater is No, and Fertilizer is No (present),  
then the harvest will be Meagre
A decision tree, or the decisions implied by it, can also be represented as disjunction 
statements. For example, the conditions for the harvest being Bumper can be written 
as a combination of AND and OR operators (or disjunctions), as follows:
(
)
'
'
'
'
'
'
(
'
'
'
'
'
')
Harvest
Bumper
Terrain
Plains
Rainfall
High
Terrain
Plateau
Groundwater
Yes
Fertilizers
Yes
==
==
==
==
==
==
∩
∪
∩
∩
In the same way, the conditions for a Moderate harvest can be summarized using 
disjunctions as follows:
(
)
(
)
'
'
'
'
'
'
'
'
(
'
'
'
'
(
'
'
'
'
'
')
Harvest
Moderate
Terrain
Plains
Rainfall
Low
Groundwater
Yes
Terrain
Hills
Rainfall
High
Terrain
Plateau
Groundwater
Yes
Fertilizers
No
==
==
==
==
==
==
==
==
==
∩
∩
∪
∩
∪
∩
∩
In the preceding example, we have used only categorical variables as the predictor 
variables. However, there is no such restriction. The numerical variables can very 
well be used as predictor variables. However, the numerical variables are not the 
most preferred variables for a decision tree, as they lose some information while  
they are categorized into groups to be used in a decision tree algorithm.
A decision tree is advantageous because it is easier to understand and doesn't require 
a lot of data cleaning in the sense that it is not influenced by missing values and 
outliers that much.
Understanding the mathematics behind 
decision trees
The main goal in a decision tree algorithm is to identify a variable and classification 
on which one can give a more homogeneous distribution with reference to the target 
variable. The homogeneous distribution means that similar values of the target 
variable are grouped together so that a concrete decision can be made.

Trees and Random Forests with Python
[ 442 ]
Homogeneity
In the preceding example, the first goal would be to find a parameter (out of four: 
Terrain, Rainfall, Groundwater, and Fertilizers) that results in a better homogeneous 
distribution of the target variable within those categories.
Without any parameter, the count of harvest type looks as follows:
Bumper 
Moderate
Meagre
4
9
7
Let us calculate, for each parameter, how the split on that parameter affects the 
homogeneity of the target variable split:
Fig. 8.4: Splitting the predictor and the target variables into categories to see their effect  
on the homogeneity of the dataset
If one observes carefully, the classification done on the Rainfall parameter is more 
homogeneous than that done on the Terrain parameter. Homogeneity means 
more similar things together or, in other words, fewer dissimilar things together. 
Homogeneity can be understood as follows: for Low rainfall, the harvest is 
distributed as 0%, 71%, and 29% across Bumper, Meagre, and Moderate, and it 
is a more homogeneous classification than 0% (Bumper), 50% (Meagre), and 50% 
(Moderate) for hilly terrains. This is because Low rainfall was able to group more 
of the Meagre rainfall (71%) together than the classes in the Terrain parameter. 
For the Fertilizers and Groundwater parameters, the highest homogeneity that 
can be achieved is 67%. Thus, the rainfall is the most suited to classify the target 
variable; that is, the Harvest type. This can be refined more as we add more variables 
(parameters) for the classification.

Chapter 8
[ 443 ]
To understand it even better, let us look at the following pictures. Which one is more 
homogeneous? Which one can represent a node where a concrete decision (about the 
type of dot: bold or normal) can be made? Definitely C, because it has only one type 
of dot. B is the next best choice as, in this case, the decision of the dot type is skewed 
more towards the bold dots (compared to A) than the normal dots.
Making these classifications more and more homogeneous, so that they can be 
identified as concrete decisions, is the ultimate goal in decision tree algorithms. 
Identifying the variable that results in the best homogeneous classification can be 
done in many ways. There are multiple algorithms available to do this. Let us take a 
look at some of them.
Entropy
The entropy technique takes cues from information theory. The premise is that 
more homogeneous or pure nodes require less information to be represented. One 
example of information can be an encrypted signal that has to be transmitted. If the 
constituents of the message are more homogeneous, it will consume fewer bits. The 
configuration that requires less information is always preferred.
The impurity or heterogeneity of a node can be measured using something called 
entropy. One interpretation of entropy (from information theory) is the minimum 
number of bits required to encode the classification of an arbitrary member of the set. 
The change (reduction) in non-desirable entropy can be measured with information 
gain. In the case of a decision tree, the nodes whose addition results in information 
gain should be added to the configuration.
Entropy is defined as follows:
( )
2
i
i
Entropy S
p log p
= ∑−

Trees and Random Forests with Python
[ 444 ]
Here, the summation is over different categories of the target variable. Pi is the 
proportion (over the total number of observations) of the ith particular category of the 
target variable.
Entropy ranges from 0 to log2c, c being the number of categories present in the target 
variable. The entropy will be 0 when the observations are perfectly homogeneous, 
while it will be log2c when the observations are perfectly heterogeneous. The entropy 
reaches the maximum value (log2c) when all the pi's are equal. The entropy reaches 
the minimum value (0) when one pi is equal to 1 and all the others are 0. If pi=1, the 
receiver of the information will know with certainty what classification an arbitrary 
member should belong to, so that there will be no information required and, thus, 
the entropy will be 0.
In the preceding example, we have three categories of target variables; hence, the 
entropy can range from 0 to log23 (1.58). We have three categories of the target 
variable, namely, Bumper, Meagre, and Moderate with proportions 4/20, 9/20,  
and 7/20. Thus, the entropy can be calculated as follows:
2
2
2
((4 / 20)
(4 / 20)
(9 / 20)
(9 / 20)
(7 / 20)
(7 / 20)
( 0.46
0.51 0.53)
1.5
S
Log
Log
Log
= −
∗
+
∗
+
∗
= −−
−
−
=
When the target variables are perfectly homogeneously distributed, the entropy will 
be 0. Suppose that all the observations had Bumper, Meagre, or Moderate as a target 
variable, then the entropy would be as follows:
2
2
2
2
((20 / 20)
(20 / 20)
(0)
(0 / 20)
(0)
(0 / 20)
0(
1
0)
S
log
log
log
remember log
= −
∗
+
∗
+
∗
=
=
On the other hand, if the classification is perfectly heterogeneous (that is, when each 
category of the target variable has equal number of observations) then the entropy 
reaches its maximum value.
A plot of entropy versus pi takes up the shape of an inverted U with a maxima in the 
middle. A plot of entropy versus the pi for a binary classification looks as follows:

Chapter 8
[ 445 ]
Fig. 8.5: A plot of entropy versus the proportions
As you can see in the preceding picture, the entropy increases with an increase 
in pi, reaches a maximum at the point where all the pi's are equal, and then starts 
decreasing as the pi increases after that.
Information gain
The entropy (S=1.5) we calculated is the entropy of the system at the beginning. This 
reduces as we introduce variables as nodes in the system. This reduction reflects as 
an Information Gain. Let us see how information gains are calculated for different 
variables. The information gain for a particular variable V is defined as follows:
( , )
(
/
).
(
)
v
c
c
informationGain S V
S
V
V Entropy V
=
−∑
Sv is the total entropy for the node variable V, c stands for the categories in the 
node variable, Vc is the number of total observation with the category c of the node 
variable, V is the total number of observations, and Entropy(Vc) is the entropy of the 
system having observations with the category c of the node variable. Summation is 
over the categories of the variable.
Let us calculate the information gain based on using the Terrain variable as a node.

Trees and Random Forests with Python
[ 446 ]
For this, let us calculate the entropy for different categories of the Terrain variable. 
Entropy is always calculated keeping the target variable in mind. For each category 
of the particular variable, we need to count how many target variables of each kind 
are present: 
2
2
2
2
2
2
2
,1
,3
((2 / 6)
(2 / 6)
(1/ 6)
(1/ 6)
(3/ 6)
(3/ 6)
1.45
2
,3
,3
((2 / 8)
(2 / 8)
(3/ 8)
(3/ 8)
(3/ 8)
(3/ 8
plains
Plateau
Terrain
Plains
Bumper
Meagre
Moderate
S
Log
Log
Log
Terrain
Plateau
Bumper
Meagre
Moderate
S
Log
Log
Log
=
→
= −
∗
+
∗
+
∗
=
=
→
= −
∗
+
∗
+
∗
2
2
2
)
1.56
2
,1
,3
((0 / 6)
(0 / 6)
(3/ 6)
(3/ 6)
(3/ 6)
(3/ 6)
1
Hills
Terrain
Hills
Bumper
Meagre
Moderate
S
Log
Log
Log
=
=
→
= −
∗
+
∗
+
∗
=
Now, we can calculate the information gain as follows:
(
)
(
)
(
)
(
)
(
)
( , )
1.5
6 / 20
8 / 20
1.5
6 / 20
1.45
8 / 20
1.56
6 / 20
1
1.5 1.36
0.14
Plain
Plateau
Hills
InformationGain S V
S
S
S
=
−
∗
+
∗
+∗




=
−
∗
+
∗
+
∗
=
−
=




The preceding calculation can be summarized in a tree-branch as follows:
Plains
Terrain
Hills
Plateau
[2B,1M,3Mo]
S
=-((2/6)*log2(2/6)
Plains
+(1/6)*log2(1/6)
+(3/6)*log2(3/6)=1.45
[0B,3M,3Mo]
S
=-((0/6)*log2(0/6)
Hills
+(3/6)*log2(3/6)
+(3/6)*log2(3/6)=1
[2B,3M,3Mo]
S
=-((2/6)*log2(2/6)
Plateau
+(3/8)*log2(3/8)
+(3/8)*log2(3/8)=1.56
Fig. 8.6: The calculation of information gain for the Terrain variable

Chapter 8
[ 447 ]
Similarly, the information gain for the Rainfall variable can be calculated as 0.42, that 
for the Fertilizers variable can be calculated as 0.36, and that for Groundwater can be 
calculated as 0.16:
Parameter
Information Gain
Terrain
0.14
Rainfall
0.42
Fertilizers
0.36
Groundwater
0.16
The variable that provides the maximum information gain is chosen to be the node. 
In this case, the Rainfall variable is chosen as it results in the maximum information 
gain of 0.42.
ID3 algorithm to create a decision tree
The tree keeps growing by selecting the next variable that results in the maximum 
information gain as a subnode branching out from this node (Rainfall in this case). 
A node should be chosen separately for each branch of the previous node. The 
variables already used for the split still qualify to be considered for being a node. It 
is a recursive process that stops when the node is totally homogeneous (pure), or it 
reaches the maximum possible depth (the number of observations in the dataset) of 
the tree.
This algorithm using entropy and information gain is called ID3 algorithm, and can 
be summarized as follows:
1.	 Calculate the initial entropy of the system based on the target variable.
2.	 Calculate the information gains for each candidate variable for a  
node. Select the variable that provides the maximum information gain as a 
decision node.
3.	 Repeat step 2 for each branch (value) of the node (variable) identified in  
step 2. The newly identified node is termed as leaf node.
4.	 Check whether the leaf node classifies the entire data perfectly. If not, repeat 
the steps from step 2 onwards. If yes, stop.

Trees and Random Forests with Python
[ 448 ]
Apart from the ID3 algorithm involving entropy and information gain, there 
are other algorithms that can be used to decide which variable should be chosen 
as a subnode. Some of them are Gini index method, Chi-Square Automatic 
Interaction Detector (CHAID) method, and Reduction in Variance method. All 
these algorithms are useful and might give better results than the others in specific 
scenarios. A summary of all these methods is available in the following table:
Method
Properties
Gini index
This can be used only if the target variable is a binary variable. 
Classification and Regression Trees (CART), a famous 
classification technique, uses Gini index method.
CHAID
This uses the chi-square statistic to find the statistical significance 
of the difference between a parent node and a subnode. It can 
handle more than two categories of the target variable.
Reduction in Variance
This is used to handle a continuous numerical variable as the 
target variable in decision tree creation. It calculates the variance 
at each node and takes a weighted average of the variances to 
decide the subnode.
To give an idea about the nitty-gritty of these methods, a couple of them such as Gini 
index and Reduction in Variance have been explained in detail.
Gini index
Gini method works only when the target variable is a binary variable. This method is 
used by the famous CART algorithm.
Gini is defined as the sum of the square of proportions of categories of the target 
variable for the particular category of the node variable. Suppose the node variable, 
for which we are trying to look for a subnode, has two categories C1 and C2. C1 has 
2 yes's and 8 no's (out of the total 10 observations with C1 as the value of the node 
variable), while C2 has 6 yes's and 4 no's (the total of 8 yes's and 12 no's). Gini for each 
category is calculated as follows:
2
2
1
2
2
2
(0.2)
(0.8)
0.68
(0.6)
(0.4)
0.52
C
C
Gini
Gini
=
+
=
=
+
=
Then, Gini index for a variable is calculated by taking a weighted average over  
the categories:
(
)
(
)
10 / 20
0.68
10 / 20
0.52
0.60
Giniindex =
∗
+
∗
=

Chapter 8
[ 449 ]
Gini index is calculated for all the candidate variables. A variable with a higher Gini 
index is selected to create the subnode.
Reduction in Variance
In the Reduction in Variance method, the target variable is supposed to be a 
numerical variable. Let's consider the same example that we considered for 
understanding the Gini index method. To convert the target variable (yes and no) to a 
numerical variable, let's transform yes to 1 and no to 0 (resulting in 8 1's and 12 0's).
(
)
(
)
(
)
(
)
(
)
(
)
2
2
2
2
8 1 12 0 / 20
0.4
8
1 0.4
12
0
0.4
/ 20
0.24
1
1 2
8 0 /10
0.2
1
2
1 0.2
8
0
0.2
/10
0.16
Mean for root node
Variance for root node
Mean for categoryC of variable
Variance for categoryC of variable
Mean for categoryC
=
∗+
∗
=


=
∗
−
+
∗
−
=


=
∗+ ∗
=


=
∗
−
+ ∗
−
=


(
)
(
)
(
)
(
)
(
)
2
2
2
1 6
4 0 /10
0.6
2
6
1 0.6
4
0
0.6
/10
0.24
10 / 20
0.16
10 / 20
0.24
0.20
of variable
Variance for categoryC of variable
Weighted variance for thevariable
=
∗+ ∗
=


=
∗
−
+ ∗
−
=


=
∗
+
∗
=
The weighed variance is calculated for each variable. The variable with the smallest 
weighted variance is chosen to be the node.
Pruning a tree
As we have seen earlier, a decision tree keeps growing by adding subnodes to the 
parent node until the data is perfectly classified or totally homogeneous. An extreme 
case is that of a decision tree that has as many nodes as there are observations in the 
dataset. This happens rarely, but a more common phenomenon is over-fitting, where 
the tree over-learns a given training dataset, but doesn't perform that well with other 
similar datasets. A small tree will lose out on accuracy as it might miss out on some 
important variables, decreasing its accuracy.

Trees and Random Forests with Python
[ 450 ]
A common strategy to overcome this situation is to first allow the tree to grow until 
the nodes have the minimum number of instances under them and then prune the 
tree to remove the branches or nodes that don't provide a lot of classification power 
to the tree. This procedure is called pruning a decision tree and can be done in both a 
bottom-up and top-down fashion. The following are a few methods that are used to 
do this:
•	
Reduced error pruning: It is a naïve top-down approach for pruning a tree. 
Starting from the terminal nodes or the leaves, each node is replaced with 
its most popular category. If the accuracy of the tree is not affected, the 
replacements are put into effect.
•	
Cost complexity pruning: This method generates a set of trees, T0, T1,…,Tn, 
where T0 is the unpruned tree and Tn is just the root node. The Ti tree is 
created by replacing a subtree of the Ti-1 tree with one of the leaf nodes 
(selecting the leaf node is done using either of the algorithms explained 
above). The subtree to be replaced is chosen as follows:
1.	 Define an error rate for a decision tree T over a dataset D as err(T,D).
2.	 The number of terminal nodes or leaves in a decision tree T is given 
by leaves (T). A decision tree T from which a subtree t has been 
pruned is denoted by prune(T,t). 
3.	 Define a function M, where M=[err(prune(T,t),D)-err(T,D)]/
[|leaves(T)|-|leaves(prune(T,t))|].
4.	 Calculate M for all the candidate subtrees.
5.	 The subtree that minimizes the M is chosen for removal.
Handling a continuous numerical variable
Earlier in the discussion, we mentioned that continuous numerical variables can 
also be used as a predictor variable while creating a decision tree. However, the 
algorithms we have discussed to choose a subnode and grow the tree, all require a 
categorical variable. How do we then use continuous numerical variables to create a 
decision tree?
The answer is by defining thresholds for the continuous numerical variable, based 
on which the variable will be categorized in several classes dynamically. How do we 
define such a threshold for a numerical variable? To answer these questions, let us 
suppose that the harvest dataset used earlier has the information about Temperature, 
as well. Let us look at the first 10 observations for such a dataset (showing only the 
Temperature and Harvest variables):

Chapter 8
[ 451 ]
Temp(0C)
35
27
12
51
46
38
4
22
29
17
Harvest
Bumper
Moderate
Meagre
Meagre
Meagre
Bumper
Meagre
Moderate
Moderate
Meagre
To find the appropriate thresholds that can be used to divide Temperature  
into categories for the sake of creating a decision tree, the following steps can  
be performed:
1.	 Sort the dataset based on Temperature (the numerical variable) in ascending 
order. The sorted dataset will look as shown in the following table:
Temp(0C)
4
12
17
22
27
29
35
38
35
46
51
Harvest
Meagre
Meagre
Meagre
Moderate
Moderate
Moderate
Bumper
Bumper
Bumper
Meagre
Meagre
2.	 Mark the Temperature ranges where the Harvest type transitions from 
one category to another. For example, in the 17-22 range, the Harvest type 
changes from Meagre to Moderate. Similarly, in the 29-35 range, the Harvest 
type transitions from Moderate to Bumper, and in the 35-46 range, Harvest 
transitions from Bumper to Meagre.
3.	 Corresponding to each transition in Harvest, there will be one threshold. In 
this case, there are three transitions so there will be three thresholds. Let us 
call these c1, c2, and c3.
4.	 The thresholds of Temperature for these transitions are nothing but the 
average of the Temperature range over which the transition occurs. Thus, 
c1=(17+22)/2=19.5, c2=(29+35)/2=32, and c3=(35+46)/2=40.5.
5.	 Corresponding to each of the thresholds, there will be a separate category of 
Temperature. Thus, the 3 categories of the Temperature will be as follows: 
Temperature>19.5, Temperature>32, and Temperature>40.5.
This method works because it has already been proved that such ranges where the 
target variable transitions from one category to an other provide the thresholds for 
continuous numerical variables that maximize the information gain.
Handling a missing value of an attribute
One of the advantages of using a decision tree is that it can handle missing values of 
an attribute. In many situations in real life where decision trees are used, one faces 
a situation where the data points of an attribute are missing. For example, consider 
a situation where one wants to predict a patient condition based on a number of 
laboratory results. What if some of the laboratory results of some of the tests are not 
available for some of the patients? How do we handle such a situation?

Trees and Random Forests with Python
[ 452 ]
Let us consider the first 10 observations of the dataset we used above, albeit with a 
few missing values:
Place
Rainfall
Terrain
Fertilizers
Groundwater
Harvest
P1
High
Plains
Yes
Yes
Bumper
P2
Low
No
Yes
Meagre
P3
Low
Plateau
No
Yes
Moderate
P4
High
Plateau
No
Yes
Moderate
P5
High
Plains
Yes
No
Bumper
P6
Low
Hills
No
No
Meagre
P7
Low
Plateau
No
No
Meagre
P8
Mild
Plateau
No
No
Meagre
P9
High
Hills
Yes
Yes
Moderate
P10
Mild
 
Yes
Yes
Bumper
Some of the approaches that can be used to handle the missing values while creating 
a decision tree are as follows:
•	
Assign the most common (highest frequency) category of that variable to 
the missing value. In the preceding case, the Terrain variable has a missing 
value. The most common category of the Terrain variable is Plateau. So, the 
missing value will be replaced by Plateau.
•	
Assign the most common (highest frequency) category of that variable 
among all the observations that have the same class of the target variable 
to the missing value. For the first blank Harvest class is Meagre. The most 
common class for the Harvest==Meagre is Plateau. So, we will replace the 
blank with Plateau. For the second blank, the Harvest type is Bumper. The 
most common class for the Harvest==Bumper is Plain. So, we will replace the 
blank with Plain.
Once the missing values have been replaced, the usual decision tree algorithms can 
be applied steadfastly.

Chapter 8
[ 453 ]
Implementing a decision tree with  
scikit-learn
Now, when we are sufficiently aware of the mathematics behind decision trees, let us 
implement a simple decision tree using the methods in scikit-learn. The dataset 
we will be using for this is a commonly available dataset called the iris dataset 
that has information about flower species and their petal and sepal dimensions. 
The purpose of this exercise will be to create a classifier that can classify a flower as 
belonging to a certain species based on the flower petal and sepal dimensions.
To do this, let's first import the dataset and have a look at it:
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/My 
Work/Chapter 7/iris.csv')
data.head()
The datasheet looks as follows:
Fig. 8.7: The first few observations of the iris dataset
Sepal-length, Sepal-width, Petal-length, and Petal-width are the dimensions of the 
flower while the Species denotes the class the flower belongs to. There are actually 
three classes of species here that can be looked at as follows:
data['Species'].unique()

Trees and Random Forests with Python
[ 454 ]
The output will be three categories of the species as follows:
Fig. 8.8: The categories of the species in the iris dataset
The purpose of this exercise will be to classify the flowers as belonging to one of the 
three species based on the dimensions. Let us see how we can do this.
Let us first get the predictors and the target variables separated:
colnames=data.columns.values.tolist()
predictors=colnames[:4]
target=colnames[4]
The first four columns of the dataset are termed predictors and the last one, that is, 
species is termed as the target variable.
Next, let's split the dataset into training and testing data:
Import numpy as np
data['is_train'] = np.random.uniform(0, 1, len(data) <= .75
train, test = data[data['is_train']==True], data[data['is_
train']==False]
In the first line, we are basically creating as many uniformly distributed random 
numbers between 0 and 1 as there are observations in the dataset. If the random 
number is less than or equal to .75, that observation goes to the training dataset; 
otherwise the observation goes to the testing dataset.
We have everything ready to create a decision tree now. As we have seen earlier, 
there are several methods to create nodes and subnodes. This method can be 
specified while invoking the DecisionTreeClassifier method of the  
sklearn library:
from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier(criterion='entropy',min_samples_split=20, 
random_state=99)
dt.fit(train[predictors], train[target])

Chapter 8
[ 455 ]
The min_samples_split specifies the minimum number of observations required to 
split a node into a subnode. By default, it is set to 2, which can be troublesome and 
can lead to over-fitting as a tree in such case can keep growing until it can find at 
least two observations. In this case, we have specified it to be 20. Our decision tree is 
now ready. Let us now test the result of our decision tree by using it for prediction 
over the testing dataset:
preds=dt.predict(test[predictors])
pd.crosstab(test['Species'],preds,rownames=['Actual'],colnames=['Pred
ictions'])
In the first line of the preceding code snippet, the decision tree is used to predict the 
class (species) for the flowers in the test dataset using the flower dimensions. The 
second line creates a table comparing the Actual species and the Predicted species. 
The table looks as follows:
Fig. 8.9: Comparing the Actual and Predicted categories
This table can be interpreted as follows: all the actual setosas were actually  
classified correctly as setosas. Out of the total 13 versicolors, 11 were classified 
correctly and 2 were classified wrongly as virginicas. Out of the total 12 virginicas,  
11 were classified correctly while 1 was classified wrongly as versicolor. This 
accuracy rate is pretty good.
Visualizing the tree
In scikit-learn, there are the following four steps to visualize a tree:
1.	 Creating a .dot file from the Decision Tree Classifier model that is fit for  
the data.

Trees and Random Forests with Python
[ 456 ]
2.	 In Python, this can be done using the export_graphviz module in the 
sklearn package. A .dot file contains information necessary to draw a 
tree. This information includes the entropy value (or Gini) at that node, the 
number of observations in that node, the condition referring to that node, 
and the node number pointing to another node number denoting which 
node is connected next to which one. For example, 2->3 and 3->4 means that 
node 2 is connected to 3, 3 is connected to 4, and so on. You can specify the 
directory name where you want to create the .dot file:
from sklearn.tree import export_graphviz
with open('E:/Personal/Learning/Predictive Modeling Book/My Work/
Chapter 7/dtree2.dot', 'w') as dotfile:
    export_graphviz(dt, out_file = dotfile, feature_names = 
predictors)
dotfile.close()
3.	 Take a look at the .dot file after it is created to have a better idea. It looks  
as follows:
Fig. 8.10: Information inside a .dot file
4.	 Rendering a .dot file into a tree:
This can be done using the system module of the os package that is used to 
run the cmd commands from within Python. This is done as follows:
from os import system
system("dot -Tpng /E:/Personal/Learning/Predictive Modeling Book/
My Work/Chapter 7/dtree2.dot -o /E:/Personal/Learning/Predictive 
Modeling Book/My Work/Chapter 7/dtree2.png")

Chapter 8
[ 457 ]
Fig. 8.11: The decision tree
This is how the tree looks like. The left arrow from the node ascribes to True and 
the right arrow to False for the condition given in the node. Each node has several 
important pieces of information such as the entropy at that node (remember, the less,  
the better), the number of samples (observations) at that node, and the number of 
samples in each species flower (under the heading value).
The tree is read as follows:
•	
If Petal Length<=2.45, then the flower species is setosa.
•	
If Petal Length>2.45, then check Petal Width. If Petal Length<=4.95, then the 
species is versicolor. If Petal Length > 4.95, then there is 1 versicolor and 2 
virginica, and further classification is not possible.
•	
If Petal Length>2.45, then check Petal Width. If Petal Length<=4.85, then there 
is 1 versicolor and 2 virginica, and further classification is not possible. If 
Petal Length>4.85, then the species is virginica.
Some other observations from the tree are as follows:
•	
The maximum depth (the number of levels) of the tree is 3. In 3 leaves, 
the tree has been able to identify categories, which will make the dataset 
homogeneous.
•	
Sepal dimensions don't seem to be playing any role in the tree formation or, 
in other words, in the classification of these flowers into one of the species.

Trees and Random Forests with Python
[ 458 ]
•	
There is a terminal node at the 1st level itself. If Petal Length<=2.45, one gets 
only the setosa species of flowers.
•	
The value in each node denotes the number of observations belonging  
to the three species (setosa, versicolor, and virginica in that order) at that  
node. Thus, the terminal node in the 1st level has 34 setosas, 0 versicolors, 
and 0 virginicas.
Cross-validating and pruning the decision 
tree
The tree might have grown very complex even after putting the min_samples_split 
of 20. There is a parameter of DecisionTreeClassifier that can be used to check 
the maximum depth to which the tree grows. This is called max_depth. Let us use 
this parameter and also the cross validation accuracy score to get an optimum depth 
of the tree. We are actually pruning the tree to get to an optimum depth where it 
neither overfits nor underfits the dataset.
We will do cross validation over the entire dataset. If you remember, cross validation 
splits the dataset into training and testing sets on its own and does this a number of 
times to generalize the results of the model.
Let us cross validate our decision tree:
X=data[predictors]
Y=data[target]
dt1.fit(X,Y)
dt1 = DecisionTreeClassifier(criterion='entropy',max_depth=5, min_
samples_split=20, random_state=99)
In these lines, we just assigned predictor variables to X and the target variable to 
Y. We have created a new decision tree that is very similar to the tree we created 
previously, except that it has an additional parameter, namely, max_depth=5.
The next step is to import the cross validation methods in sklearn and perform the 
cross validation:
from sklearn.cross_validation import KFold
crossvalidation = KFold(n=X.shape[0], n_folds=10, shuffle=True, 
random_state=1)
from sklearn.cross_validation import cross_val_score
score = np.mean(cross_val_score(dt1, X, Y, scoring='accuracy', 
cv=crossvalidation, n_jobs=1))
score

Chapter 8
[ 459 ]
We have chosen to do a 10-fold cross validation, and the score is the mean of the 
accuracy score obtained from each fold. The score in this comes out to be 0.933. This 
score signifies the accuracy of the classification.
If we vary the max_depth from 1 to 10, this is how the mean accuracy score varies:
As you can observe, for max_depth => 4, the score remains almost constant. The 
maximum score is obtained when max_depth = 3. Hence, we will choose to grow 
our tree to only three levels from the root node.
Let us now do a feature importance test to determine which of the variables in the 
preceding dataset are actually important for the model. This can be easily done  
as follows:
dt1.feature_importances_
Fig. 8.12: Feature importance scores of the variables in the iris dataset
The higher the values, the higher the feature importance. Hence, we conclude that 
the Petal width and Petal length are important features (in ascending order of 
importance) to predict the flower species using this dataset.
Understanding and implementing 
regression trees
An algorithm very similar to decision trees is regression tree. The difference  
between the two is that the target variable in the case of a regression tree is a 
continuous numerical variable, unlike decision trees where the target variable  
is a categorical variable.

Trees and Random Forests with Python
[ 460 ]
Regression tree algorithm
Regression trees are particularly useful when there are multiple features in the 
training dataset that interact in complicated and non-linear ways. In such cases, a 
simple linear regression or even the linear regression with some tweaks will not be 
feasible or produces a very complex model that will be of little use. An alternative to 
non-linear regression is to partition the dataset into smaller nodes/local partitions 
where the interactions are more manageable. We keep partitioning until the point 
where the non-linear interactions are non-existent or the observations in that 
partition/node are very similar to each other. This is called recursive partition.
A regression tree is similar to a decision tree because the algorithm behind them 
is more or less the same; that is, the node is split into subnodes based on certain 
criteria. The criterion of a split, in this case, is the maximum Reduction in Variance; 
as we discussed earlier, this approach is used when the target variable is a 
continuous numerical variable. The nodes are partitioned based on this criterion 
unless met by a stopping criterion. This process is called recursive partitioning. One 
of the common stopping criteria is the one we described above for the decision tree. 
The depth (level of nodes) after which the accuracy of the model stops improving is 
generally the stopping point for a regression tree. Also, the predictor variables that 
are continuous numerical variables are categorized into classes using the approach 
described earlier.
Once a leaf (terminal) node is decided, a local model is fit for all the observations 
falling under that node. The local model is nothing but the average of the output 
values of all the observations falling under that leaf node. If the observations (x1,y1), 
(x2,y2), (x3,y3),……, and (xn,yn) fall under the leaf node l, then the output value, y, for 
this node is given by:
A stepwise summary of the regression tree algorithm is as follows:
1.	 Start with a single node, that is, all the observations, calculate the mean,  
and then the variance of the target variable.
2.	 Calculate the reduction in variance caused by each of the variables that are 
potential candidates for being the next node, using the approach described 
earlier in this chapter. Choose the variable that provides the maximum 
reduction in the variance as the node.
3.	 For each leaf node, check whether the maximum reduction in the variance 
provided by any of the variables is less than a set threshold, or the number 
of observations in a given node is less than a set threshold. If one of these 
criterions is satisfied, stop. If not, repeat step 2.

Chapter 8
[ 461 ]
Some advantages of using the regression tree are as follows:
•	
It can take care of non-linear and complicated relations between  
predictor and target variables. Non-linear models often become difficult  
to comprehend, while the regression trees are simple to implement  
and understand.
•	
Even if some of the attributes of an observation or an entire observation is 
missing, the observation might not be able to reach a leaf node, but we can 
still get an output value for that observation by averaging the output values 
available at the terminal subnode of the observation.
•	
Regression trees are also very useful for feature selection; that is, selecting 
the variables that are important to make a prediction. The variables that are a 
part of the tree are important variables to make a prediction.
Implementing a regression tree using Python
Let us see an implementation of the regression trees in Python on a commonly used 
dataset called Boston. This dataset has information about housing and median 
prices in Boston. Most of the predictor variables are continuous numerical variables. 
The target variable, the median price of the house, is also a continuous numerical 
variable. The purpose of fitting a regression tree is to predict these prices:
Let us take a look at the dataset and then see what the variables mean:
import pandas as pd
data=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/My 
Work/Chapter 7/Boston.csv')
data.head()
Fig. 8.13: A look at the Boston dataset

Trees and Random Forests with Python
[ 462 ]
Here is a brief data dictionary describing the meaning of all the columns in  
the dataset:
•	
CRIM: This is the per-capita crime rate by town
•	
ZN: This is the proportion of residential land zoned for lots 
over 25,000 sq.ft
•	
INDUS: This is the proportion of non-retail business acres per 
town
•	
CHAS: The is the Charles River dummy variable (1 if the tract 
bounds river; 0 otherwise)
•	
NOX: This is the nitric oxide concentration (parts per 10 
million)
•	
RM: This is the average number of rooms per dwelling
•	
AGE: This is the proportion of owner-occupied units built 
prior to 1940
•	
DIS: This is the weighted distance to five Boston employment 
centers
•	
RAD: This is the index of accessibility to radial highways
•	
TAX: This is the full-value property tax rate per $10,000
•	
PTRATIO: This is the pupil-teacher ratio by town
•	
B: 1000(Bk - 0.63)^2: Here, Bk is the proportion of blacks by 
town
•	
LSTAT: This is the % of lower status of the population
•	
MEDV: This is the median value of owner-occupied homes in 
$1000s
Let us perform the required preprocessing before we build the regression tree model. 
In the following code snippet, we just assign the first 13 variables of the preceding 
dataset as predictor variables and the last one (MEDV) as the target variable:
colnames=data.columns.values.tolist()
predictors=colnames[:13]
target=colnames[13]
X=data[predictors]
Y=data[target]
Let us now build the regression tree model:
from sklearn.tree import DecisionTreeRegressor
regression_tree = DecisionTreeRegressor(min_samples_split=30,min_
samples_leaf=10,random_state=0)
regression_tree.fit(X,Y)

Chapter 8
[ 463 ]
The min_samples_split specifies the minimum number of observations required 
in a node for it to be qualified for a split. The min_samples_leaf specifies the 
minimum number of observations required to classify a node as a leaf.
Let us now use the model to make some predictions on the same dataset and see how 
close they are to the actual value of the target variable:
reg_tree_pred=regression_tree.predict(data[predictors])
data['pred']=reg_tree_pred
cols=['pred','medv']
data[cols]
Fig. 8.14: Comparing the actual and predicted values of the target variable
One point to observe here is that many of the observations have the same predicted 
values. This was expected because, if you remember, the output of a regression 
model is nothing but an average of the output of all the observations falling under a 
particular node. Thus, all the observations falling under the same node will have the 
same predicted output value.
Let us now cross-validate our model and see how accurate the cross-validated  
model is:
from sklearn.cross_validation import KFold
from sklearn.cross_validation import cross_val_score
import numpy as np
crossvalidation = KFold(n=X.shape[0], n_folds=10,shuffle=True, random_
state=1)
score = np.mean(cross_val_score(regression_tree, X, Y,scoring='mean_
squared_error', cv=crossvalidation,n_jobs=1))
score

Trees and Random Forests with Python
[ 464 ]
In this case, the score we are interested in looking at is the mean squared error. A 
mean of the score in this comes out to be 20.10. The cross_val_predict module can 
be used, just like the cross_val_score module, to predict the values of the output 
variable from the cross-validated model.
Let us now have a look at the other outputs of the regression tree that can be useful. 
One important attribute of a regression tree is the feature importance of various 
variables. The importance of a feature is measured by the total reduction it has 
brought to the variance. The feature importance for the variables of a regression  
tree can be calculated as follows:
regression_tree.feature_importances_
Fig. 8.15: Feature importance scores for regression tree on the Boston dataset
The higher the value of the feature importance for a variable, the more important 
it is. In this case, the three most important variables are age, lstat, and rm, in 
ascending order of importance.
A regression tree can be drawn in the same way as the decision tree to understand 
the results and predictions better.
Understanding and implementing random 
forests
Random forests is a predictive algorithm falling under the ambit of ensemble 
learning algorithms. Ensemble learning algorithms consist of a combination of 
various independent models (similar or different) to solve a particular prediction 
problem. The final result is calculated based on the results from all these 
independent models, which is better than the results of any of the  
independent models.
There are two kinds of ensemble algorithm, as follows:
•	
Averaging methods: Several similar independent models are created (in 
the case of decision trees, it can mean trees with different depths or trees 
involving a certain variable and not involving the others, and so on.) and the 
final prediction is given by the average of the predictions of all the models.

Chapter 8
[ 465 ]
•	
Boosting methods: The goal here is to reduce the bias of the combined 
estimator by sequentially building it from the base estimators. A powerful 
model is created using several weak models.
Random forest, as the name implies, is a collection of classifier or regression trees. A 
random forest algorithm creates trees at random and then averages the predictions 
(random forest is an averaging method of ensemble learning) of these trees.
Random forest is an easy-to-use algorithm for both classification and regression 
prediction problems and doesn't come with all the prerequisites that other 
algorithms have. Random forest is sometimes called the leatherman of all algorithms 
because one can use it to model any kind of dataset and find a decent result.
Random forest doesn't need a cross-validation. Instead, it uses something called 
Bagging. Suppose we want n observations in our training dataset T. Also, let's say 
there are m variables in the dataset. We decide to grow S trees in our forest. Each tree 
will be grown from a separate training dataset. So, there will be S training datasets. 
The training datasets are created by sampling n observations randomly with a 
replacement (n times). So, each dataset can have duplicate observations as well, and 
some of the observations might be missing from all the S training datasets. These 
datasets are called bootstrap samples or simply bags. The observations that are not 
part of a bag are out of the bag observation for that bag or sample.
The random forest algorithm
The following is a stepwise algorithm for a random forest:
1.	 Take a random sample of size n with a replacement.
2.	 Take a random sample of the predictor variables without a replacement.
3.	 Construct a regression tree using the predictors chosen in the random sample 
in step 2. Let it grow as much as it can. Do not prune the tree.
4.	 Pass the outside of the bag observations for this bootstrap sample through 
the current tree. Store the value or class assigned to each observation through 
this process.
5.	 Repeat steps 1 to 4 for a large number of times or the number of times 
specified (this is basically the number of trees one wants in the forest).
6.	 The final predicted value for an observation is the average of the predicted 
values for that observation over all the trees. In the case of a classifier, the 
final class will be decided by a majority of votes; that is, the class that gets 
predicted by the maximum number of trees gets to be the final prediction for 
that observation.

Trees and Random Forests with Python
[ 466 ]
Implementing a random forest using Python
Let us fit a random forest on the same dataset and see whether there is some 
improvement in the error rate of the prediction:
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor(n_jobs=2,oob_score=True,n_estimators=10)
rf.fit(X,Y)
The parameters in RandomForestRegressor have their significance. The n_jobs is 
used to specify the parallelization of the computing and signifies the number of jobs 
running parallel for both fit and predict. The oob_score is a binary variable. Setting 
it to True means that the model has done an out-of-the-box sampling to make the 
predictions. The n_estimators specifies the number of trees our random forest will 
have. It has been chosen to be 10 just for illustrative purposes. One can try a higher 
number and see whether it improves the error rate or not.
The predicted values can be obtained using the oob_prediction attribute of the 
random forest:
rf.oob_prediction_
Let us now make the predictions a part of the data frame and have a 
look at it. 
data['rf_pred']=rf.oob_prediction_
cols=['rf_pred','medv']
data[cols].head()
The output of the preceding code snippet looks as follows:
Fig. 8.16: Comparing the actual and predicted values of the target variable

Chapter 8
[ 467 ]
The next step is to calculate a mean squared error for the prediction. For a regression 
tree, we specified the cross-validation scoring method to be a mean squared error; 
hence, we were able to obtain a mean squared error for the regression tree from 
the cross-validation score. In the case of random forest, as we noted earlier, a cross 
validation is not needed. So, to calculate a mean squared error, we can use the oob 
predicted values and the actual values as follows:
data['rf_pred']=rf.oob_prediction_
data['err']=(data['rf_pred']-data['medv'])**2
sum(data['err'])/506
The mean squared error comes out to be 16.823, which is less than 20.10 obtained 
from the regression tree with cross-validation.
Another attribute of the random forest regressor is oob_score, which is similar to 
the coefficient of the determination (or R2) used in the linear regression.
The oob_score for a random forest can be obtained by writing the following one 
liner: rf.oob_score_
The oob_score for this random forest comes out at 0.83.
Why do random forests work?
Random forests do a better job of making predictions because they average the 
outputs from an ensemble of trees. This maximizes the variance reduction. Also, 
taking a random sample of the predictors to create a tree makes the tree independent 
of the other trees (as they are not necessarily using the same predictors, even if using 
similar datasets).
Random forest is one of the algorithms where all the variables of a dataset are 
optimally utilized. In most machine learning algorithms, we select a bunch of 
variables that are the most important for an optimal prediction. However, in the case 
of random forest, because of the random selection of the variables and also because 
the final outputs in a tree are calculated at the local partitions where some of the 
variables that are not important globally might become significant, each variable is 
utilized to its full potential. Thus, the entire data is more optimally used. This helps 
in reducing the bias arising out of dependence on only a few of the predictors.

Trees and Random Forests with Python
[ 468 ]
Important parameters for random forests
The following are some of the important parameters for random forests that help in 
fine-tuning the results of the random forest models:
•	
Node size: The trees in random forests can have very few observations in 
their leaf node, unlike the decision or regression trees. The trees in a random 
forest are allowed to grow without pruning. The goal is to reduce the bias as 
much as possible. This can be specified by the min_samples_leaf parameter 
of the RandomForestRegressor.
•	
Number of trees: The number of trees in a random forest is generally set to a 
large number around 500. It also depends on the number of observations and 
columns in the dataset. This can be specified by the n_estimators parameter 
of the RandomForestRegressor.
•	
Number of predictors sampled: This is an important tuning parameter 
determining how the tree grows independently and unbiased. Generally, it 
should range between 2 to 5.
Summary
In this chapter on the decision trees, we first tried to understand the structure and 
the meaning of a decision tree. This was followed by a discussion on the mathematics 
behind creating a decision tree. Apart from implementing a decision tree in Python, 
the chapter also discussed the mathematics of related algorithms such as regression 
trees and random forests. Here is a brief summary of the chapter:
•	
A decision tree is a classification algorithm used when the predictor variables 
are either categorical or continuous numerical variables.
•	
Splitting a node into subnodes so that one gets a more homogeneous 
distribution (similar observations together), is the primary goal while  
making a tree.
•	
There are various methods to decide which variable should be used to split 
the node. These methods include information gain, Gini, and maximum 
reduction in variance methods.

Chapter 8
[ 469 ]
•	
The method of building a regression tree is very similar to a decision tree. 
However, the target variable in the case of a regression tree is a continuous 
numerical variable, unlike the decision tree where it is a categorical variable.
•	
Random forest is an algorithm coming under the ambit of ensemble methods. 
In ensemble methods, a lot of models are fitted over the same dataset. The 
final prediction is a combination (average or maximum votes) of the outputs 
from these models.
•	
In the case of random forests, the models are all decision or regression trees. 
A random forest is more accurate than a single decision or a regression tree 
because the averaging of outputs maximizes the variance reduction.
In the next and final chapter, we will go through some best practices in predictive 
modeling to get optimal results.


[ 471 ]
Best Practices for  
Predictive Modelling
As we have seen in all the chapters on the modelling techniques, a predictive model 
is nothing but a set of mathematical equations derived using a few lines of codes. In 
essence, this code together with a slide-deck highlighting the high-level results from 
the model constitute a project. However, the user of our solution is more interested 
in finding a solution for the problem he is facing in the business context. It is the 
responsibility of the analyst or the data scientist to offer the solution in a way that is 
user-friendly and maximizes output or insights.
There are some general guidelines that can be followed for the optimum results in a 
predictive modelling project. As predictive modelling comprises a mix of computer 
science techniques, algorithms, statistics, and business context capabilities, the 
best practices in the predictive modelling are a total of the best practices in the 
aforementioned individual fields.
In this chapter, we will be learning about the best practices adopted in the field of 
predictive modelling to get the optimum results. The major headings under which  
all the best practices in the field of predictive analytics/modelling can be clubbed are 
as follows:
•	
Code: This makes the code legible, reproducible, elegant, and parametric.
•	
Data handling: This makes sure the data is read correctly without any loss. 
Also, it makes preliminary guesses about the data.
•	
Algorithms: This explains the math underlying the selected algorithm in a 
lucid manner and illustrates how the selected algorithm fits the best for the 
problem in the business context. In brief, it answers as to why this is the most 
suited algorithm for the given problem.

Best Practices for Predictive Modeling
[ 472 ]
•	
Statistics: This applies the statistical tests relevant to the business context  
and interprets their result; it interprets the statistical output parameters  
of the algorithms or models and documents their implications in the  
business context.
•	
Business context/communication: This clearly states the key insights 
unearthed from the analysis, the improvement or change the model has 
brought and what are its implications in the business context, and the key 
action items for the business.
The following are some of the best practices or conventional wisdom amassed over 
the decade-long existence of predictive modelling.
Best practices for coding
When one uses Python for predictive modelling, one needs to write small snippets 
of code. To ensure that one gets the maximum out of their code snippets and that the 
work is reproducible, one should be aware of and aspire to follow the best practices 
in coding. Some of the best practices for coding are as follows.
Commenting the codes
There is a tradeoff between the elegance and understandability of a code snippet. As 
a code snippet becomes more elegant, its understandability by a new user (other than 
the author of the snippet) decreases. Some of the users are interested only in the end 
results, but most of the users like to understand what is going on behind the hood 
and want to have a good understanding of the code.
For the code snippet to be understandable by a new person or the user of the code, 
it is a common practice to comment on the important lines, if not all the lines, and 
write the headings for the major chunks of the code. Some of the properties of a 
comment are as follows:
•	
The code should be succinct, brief, and preferably a one-liner.
•	
The comment should be part of the code, but shouldn't be executable unlike 
the other parts of the code. In Python, a line can be commented by appending 
a hash # in front of the line.
Some of the reasons to prefer a commented code are as follows:
•	
Commenting can also be used for testing the code and trying small 
modifications in a considerably large code snippet.
•	
Transferring the understanding of the code to a new person is an integral 
part of the process of knowledge transfers in the project management.

Chapter 9
[ 473 ]
The following is an example of a well-commented code snippet clearly stating the 
objective of the code in the header and the purpose of each line in the comments. 
This code has already been used in the Chapter 3, Data Wrangling. You can revisit this 
for more context and then try to understand the code with comments. Most likely, it 
would be easier to understand with the comments:
# appending 333 similar datasets to form a bigger dataset
import pandas as pd          # importing pandas library
filepath='E:/Personal/Learning/Predictive Modeling Book/Book Datasets/
Merge and Join/lotofdata' # defining filepath variable as the folder
# which has all the small datasets
data_final=pd.read_csv('E:/Personal/Learning/Predictive Modeling Book/
Book Datasets/Merge and Join/lotofdata/001.csv') # initialising the
# data-final data frame with the first dataset of the lot
data_final_size=len(data_final)    # initializing the data_final_size 
variable which counts the number of rows in the data_final data frame
for i in range(1,333):             # defining a loop over all the 333 
files
    if i<10:
        filename='0'+'0'+str(i)+'.csv' # the files are named as 001.
csv, 101.csv etc. Accordingly, 3 conditions arise for the filename
        # variable. i<10 requires appending 2 zeros at the beginning.
    if 10<=i<100:
        filename='0'+str(i)+'.csv' # i<100 requires appending 1 zeros 
at the beginning.
    if i>=100:
        filename=str(i)+'.csv'     # i>=100 requires appending no 
zeros at the beginning.
    file=filepath+'/'+filename     # defining the file variable by 
appending filepath and filename variable. file variable contains a new
    # file in every iteration
    data=pd.read_csv(file)         # file is read as data frame called 
data
    data_final_size+=len(data)     # data_final_size variable is 
updated by adding the length of the currently read file
    data_final=pd.concat([data_final,data],axis=0)  # concatanating/
appending data to the data_final data frame on the axis=0 i.e. on rows
print data_final_size                               # printing the 
final_data_size variable containing the number of rows in the final
# data frame

Best Practices for Predictive Modeling
[ 474 ]
The same code looks as follows in the IPython Notebook:
Fig. 9.1: An example of a well-commented code
Defining functions for substantial individual 
tasks
Any code implements a set of tasks. Many of these talks are an important part of the 
overall task at hand, but can be segregated from the main code. These tasks can be 
defined separately as functions parameterizing all the possible inputs required for 
the particular task. These functions can then be used with the particular inputs, and 
the output can be used in the implementation of the main task.
The functions are useful because of a variety of reasons, as follows:
•	
Functions are also useful when the same task needs to be performed a large 
number of times with just a minor change in the inputs.
•	
Defining separate functions makes the code legible and easier to understand 
and follow. In the absence of a function, the code becomes cluttered and 
difficult to follow.
•	
If the task performed by the function is a calculation, transformation,  
or aggregation, then it can be easily applied across columns using the  
apply method.
•	
Debugging also becomes difficult if they are not present. These functions 
can be tested on their own. If they work fine, then we know that the error is 
somewhere else.
Let us now see a few examples of a function defined to implement small tasks.

Chapter 9
[ 475 ]
Example 1
This function takes a positive integer greater than 2 as an input and creates a 
Fibonacci sequence with as many members in the sequence as the positive integer:
  def fib(n):
  a,b=1,2
  count=3
  fib_list=[1,2]
  while(count<=n):
    a,b=b,a+b
    count=count+1
    fib_list.append(b)
  return fib_list
Example 2
This function calculates the distance of a particular place (defined by latitude and 
longitude) from a list of possible stations and finds the station that is the closest to 
the given place. This function can take an array consisting of latitude and longitude 
and calculate the distances from possible stations for each location. It can also be 
applied to a column of data consisting of latitude and longitude in each row to find 
the closest station for each location (defined by latitude and longitude):
def closest_station(lat, longi,stations):
    loc = np.array([lat, longi])
    deltas = stations - loc[None, :]
    dist2 = (deltas**2).sum(1)
    return np.argmin(dist2)
An example of the list of stations can be a list of two possible stations containing the 
latitude and longitudes for both the stations as follows:
    stations = np.array([[41.995, -87.933],
                         [41.786, -87.752]])
Example 3
A function can work without any input as well. These functions will perform 
some task, but will not necessarily return an output. They perform some sort of 
transformation or manipulation. Functions can also be defined to implement several 
repetitive tasks at once; for example, applying the same conversion to all the columns 
of a dataset.

Best Practices for Predictive Modeling
[ 476 ]
One example of defining such a function is to define a function to convert several 
columns of a dataset at once to a desired data type. This is a very common and 
widely used data preparation step used in almost all the predictive modelling 
projects because the data type of some of the columns need to be changed to  
facilitate a particular operation or calculation in the business context.
In the following example, a hypothetical dataset called datafile is opened as a 
dictionary so that it can be read line by line and sub settled easily over columns. The 
dataset has columns Date, Latitude, Longitude, NumA1, and NumA2 that need to be 
converted to date, float, float, int, and int data types, respectively. A dictionary 
consisting of the column names and the required data type of the column name is 
defined. Each column is then converted to the required data type and the resultant 
line is appended to the final dataset called data:
def load_data():
    data = []
    for line in csv.DictReader(open("..../datafile.csv")):
        for name, converter in {"Date" : date,
                                "Latitude" : float, "Longitude" : 
float,
                                "NumA1" : int, "NumA2" : int}.items():
            line[name] = converter(line[name])
        data.append(line)
    return data
As one can see, these small tasks are significant on their own so that they are 
identified as a separate subtask, but in the larger picture, they are a part of the  
larger task and can be used later for further analyses.
Avoid hard-coding of variables as much as 
possible
One of the most essential guidelines to follow while writing a legible and an  
easy-to-debug code is to create variables and avoid hard-coding as much as possible.
Some of the benefits of avoiding hard-coding can be listed as follows:
•	
Hard-coding makes it difficult to spot the error and debug the code. If a 
variable is created, one needs to check for the error at just one place, that is, 
the place where the variable has been defined. If not, one has to go through 
the entire code, spot the places where the hard-coding has been used and 
check for error at all those places.

Chapter 9
[ 477 ]
•	
Also, once defined, a variable can be used at multiple places in the code. 
Making a change in the script also becomes easier as the variable-related 
change needs to be done only once if the variable is defined.
The following code is one example of defining a variable and avoiding hard-coding. 
Here, we are defining a variable for a particular directory path and a couple of files. 
We can use these variables for subsequent usage such as reading one of the files. If 
one of the inputs, such as the directory path, needs to be changed, it can be done by 
making a change only at one place, that is, where it is defined:
import pandas as pd
import os
filepath='E:\Personal\Learning\Predictive Modeling Book\Book Datasets\
Interesting Datasets'
filename1='chopstick-effectiveness.csv'
filename2='pigeon-racing.csv'
df1=pd.read_csv(os.path.join(filepath,filename1))
df2=pd.read_csv(os.path.join(filepath,filename2))
df1.head()
df2.head()
Some parameters, which need to be changed regularly, should always be defined as 
a variable. Some codes need to be run repeatedly with a change only in one of the 
variables. In such cases also, defining a variable comes in very handy.
Version control
While developing the code, the changes and improvements are suggested  
phase-wise and not all at once. It is not possible to write in one sitting, a perfectly 
working code with no scope for improvement. However, the intermediate code 
might be used for demonstrative (the proof of concept or POC; before a project starts 
officially, evidence is needed to prove that the concept can be put into reality) and 
testing purposes. Hence, there is a need to follow a version control.
It essentially means saving a copy of the old code, making a copy of it, renaming it, 
and making changes to the new copy. The new copy is the new version of the code. 
This new copy can be released as the latest production version once it is tested after 
making the changes and has started running without error. Until then, the latest but 
one version of the code should be used as the production version. Version control 
can be done manually or by using version controlling tools such as GitHub and  
so on.

Best Practices for Predictive Modeling
[ 478 ]
Using standard libraries, methods, and 
formulas
As far as possible, try to use a function or method, if it already exists, to perform a 
particular task in the production version of the code. For better understanding of 
how the method works, one can deconstruct the method and try building it up from 
scratch (as we have done for the logistic regression algorithm in this book) on their 
own, but this should be a part of the exploratory work. In the production version, 
already existing methods should be used.
For example, to calculate correlation, one should use the already existing formula 
and not reinvent the wheel from scratch. Another example is the groupby 
functionality in pandas to split the dataset into groups based on the different 
categories of a categorical variable. This saves time and also increases the elegance of 
the code snippet. There are an ample number of libraries to choose from to perform 
tasks in Python. One should choose a library that performs well and is stable over a 
range of IDEs, interpreters, and OSs.
Best practices for data handling
Data cleaning and manipulation constitutes the framework of any analytics project. 
To ensure that this important step is executed efficiently, the following best practices 
should be executed:
•	
After importing the dataset, one should ensure that the dataset (all the 
variables and rows) has been read correctly. This means reading all the 
variables in their correct or required format. Sometimes, due to some 
limitation on the data or the IDE side, some variables are read wrongly  
and they need to be formatted to the correct format.
•	
For example, if a variable reports some numerical ID (let's say 10-digits 
long), many a times it would be read and displayed in a scientific notation. 
However, this would be wrong as it is an ID and shouldn't be displayed 
in a scientific notation. Sometimes, a variable containing long strings are 
truncated. These issues should be taken care of before performing any 
operation on the data.
•	
After every data manipulation step such as transposing a dataset, creating 
and joining dummy variables to the dataset, merging two datasets, creating 
a new variable, or changing the format type of a variable, one should look 
at the resultant dataset to see whether the manipulation has taken place 
correctly or not.

Chapter 9
[ 479 ]
•	
As far as possible, data shouldn't be deleted from a dataset. This should be 
kept in mind while dealing with missing values. If some of the values in a 
row are missing, imputing values should be the preferred choice. Deleting 
the entire row should be avoided.
•	
Basic plots, namely, histograms and scatter plots should be created for all the 
numerical variables to see the general outlook and behavior of that variable. 
This helps in spotting some obvious trends, outliers, potential modifications, 
and so on. The pair-wise scatter plot of all the numerical variables can also 
be tried if the number of variables is manageable. This plot is called scatter 
plot matrix and is very useful to spot relationships, if any, among any two 
variables. Category-wise histograms are also used to get a good sense of 
distribution of a variable over different categories.
Best practices for algorithms
The choice of which algorithm to deploy to answer a business question depends on 
a variety of parameters, and there is no one good answer. The choice of algorithm 
generally depends on the nature of the predictor and output variables; also, the 
overarching nature of the business problem at hand—whether it is a numerical 
prediction, classification, or an aggregation problem. Based on these preliminary 
criteria, one can shortlist a few existing methods to apply on the dataset.
Each method will have its own pros and cons, and the final decision should be taken 
keeping in mind the business context. The decision for the best-suited algorithm is 
usually taken based on the following two requirements:
•	
Sometimes, the user of the result is interested only in the accuracy of the 
results. In such cases, the choice of the algorithm is done based on the 
accuracy of the algorithms. All the qualifying models are run and the one 
with the maximum accuracy is finalized.
•	
At other times, the user is interested in knowing the details of the algorithms 
as well. In such cases, the complexity of the algorithm also becomes a 
concern. The selected algorithms shouldn't be too complex to explain  
to the user and should also be decently accurate.

Best Practices for Predictive Modeling
[ 480 ]
The following table summarizes the algorithms that should be chosen depending 
upon the type of predictor and outcome variables and the question needed to be 
answered in the business context:
Type of variables
Business contexts/
questions
Algorithm/Model
A continuous numerical 
variable as an output 
variable; a mix of categorical 
and numerical variables as 
predictor variables.
To answer quantifiable 
questions such as how 
many, how much, and  
so on.
Linear regression, polynomial 
regression, and regression 
tree.
A binary or categorical 
variable as an output 
variable; a mix of categorical 
and numerical variables as 
predictor variables.
Classification problems. 
To answer questions with 
yes/no, fail/success, and 
0/1 answers.
Logistic regression.
No output variable; a mix of 
categorical and numerical 
variables as predictor 
variables.
Grouping/aggregation 
and targeted marketing. To 
answer what data points 
are similar to each other? 
How many such groups 
can be created? These 
groups are earlier  
non-existent. 
Clustering and segmentation.
A categorical or numerical 
variable as an output 
variable; a mix of categorical 
and numerical variables as 
predictor variables.
Classification problems. 
Classifying data points into 
already existing groups. 
Decision Trees, k-Nearest 
Neighbor, Bayes' Classifier, 
Support Vector Machines, and 
so on.
Best practices for statistics
Statistics are an integral part of any predictive modelling assignment. Statistics are 
important because they help us gauge the efficiency of a model. Each predictive 
model generates a set of statistics, which suggests how good the model is and how 
the model can be fine-tuned to perform better. The following is a summary of the 
most widely reported statistics and their desired values for the predictive models 
described in this book:

Chapter 9
[ 481 ]
Algorithms
Statistics/Parameter
The desired value of statistics
Linear regression 
R2, p-values, F-statistic,  
and Adj. R2
High Adj. R2, low F-statistic, 
and low p-value
Logistic regression
Sensitivity, specificity, Area 
Under the Curve (AUC), and 
KS statistic
High AUC (proximity to 1)
Clustering 
Intra-cluster distance and 
silhouette coefficient
High intra-cluster distance 
and high silhouette coefficient 
(proximity to 1)
Decision trees 
(classification)
AUC and KS statistics
High AUC (proximity to 1)
While reporting the results of a predictive model, the value of these statistics and 
its meaning in the business context should be stated explicitly. A brief and lucid 
explanation of the relevance and significance of the statistic is appreciated. Report 
the best values (most optimum value attainable) of these statistics. The model should 
be fine-tuned based on the value of these statistics until the point that they can't be 
further improved.
Apart from these statistics, there are various statistical tests that can be performed 
over the dataset to test certain hypothesis about the data before fitting any predictive 
model to it. These tests include Z-test, t-test, chi-square test, ANOVA, and so on. 
If such tests have been performed, the results (value and significance) and their 
implications should be clearly stated.
Best practices for business contexts
This is the meatiest part of the report created for a predictive modeling project. 
Some users of the report will navigate directly to this section as they are primarily 
interested in the overall effect of the project. Thus, it is imperative to mention the 
highlights and most important findings of the project in this section. This is different 
from reporting the statistics, which is in a way the raw output of the predictive 
model. In this section, we will focus on the following:
•	
Findings and insights of the analyses
•	
Major problems identified
•	
Major results from the model
•	
The accuracy or efficiency of the model
•	
Action steps for the user to solve the business problem, and so on

Best Practices for Predictive Modeling
[ 482 ]
If it is a customer segmentation problem, mention the names and characteristics 
of the segments identified along with the statistical summary for each segment. 
Recommend a plan to maximize sales and revenue (or whatever the business 
objective might be) for each of the segments.
If it is a regression/prediction/forecasting problem, mention the accuracy of the 
results along with a summary of the results. For example, the expected number of 
house sales in the coming year is around t (say 900K), according to the model. The 
accuracy of the model is a% (say 98.5%).
Don't write in paragraphs. Write in bullet points. Add relevant plots and graphs to 
summarize the results.
Tables are a great way to summarize a lot of information in a small space. Use a lot of 
them. Screenshots are also a great way to show results as they are quite widely used. 
Assumptions, if any, should be clearly stated.
Summary
What are the do's and don'ts of a predictive modelling project? This chapter dealt 
with these pressing questions and listed a number of best practices to make a 
predictive modelling project successful. Following are the important points:
•	
Codes should be well-commented, modular, version-controlled, generalized, 
and not have hard-coded values.
•	
Data should be observed carefully after every import and manipulation  
in order to check for any errors that might creep in while performing  
these operations.
•	
The choice of the algorithm is guided by the nature of the predictor  
and outcome variable. The ultimate selection of the algorithm depends  
upon whether the user prioritizes accuracy or the understandability of  
the algorithm.
•	
While reporting the results of a predictive model, the most optimum value  
of the important statistics and their relevance should be clearly stated.
•	
Main business questions should be clearly answered. Major finding should 
be reported clearly. Some actionable recommendations for the findings 
should be given. All the assumptions should be stated.
As a practitioner of any discipline, one should strive to follow the best practices, to 
get the best result and impact. The same stands true for predictive modelling as well.

[ 483 ]
A List of Links
The following is a list of links that have been referenced throughout the chapters  
in this book:
•	
Link to the datasets used in the book:
°°
https://goo.gl/zjS4C6
•	
Info on predictive modelling:
°°
http://aci.info/2014/07/12/the-data-explosion-in-2014-
minute-by-minute-infographic/
°°
http://www.bbc.com/news/business-26383058
•	
UCI Machine Learning Library of datasets: 
°°
http://archive.ics.uci.edu/ml/
•	
Scikit-learn official website:
°°
http://scikit-learn.org/stable/
•	
Pandas documentation:
°°
http://pandas.pydata.org/pandas-docs/stable/tutorials.
html
•	
An Introduction to Statistical Learning:
°°
http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20
Printing.pdf
•	
Analytics Vidhya blog:
°°
http://www.analyticsvidhya.com/
•	
Kaggle blog:
°°
http://blog.kaggle.com/

A List of Links
[ 484 ]
•	
Hierarchical clustering PPT, San Jose State University:
°°
http://goo.gl/p4kSxv
•	
Decision tree learning material, Carnegie Mellon University:
°°
http://www.cs.cmu.edu/afs/cs/project/theo-20/www/mlbook/
ch3.pdf
•	
Biostatistics Course BIOST 515, Washington University:
°°
http://courses.washington.edu/b515/l13.pdf
°°
http://courses.washington.edu/b515/l12.pdf
•	
Scipy documentation:
°°
http://www.scipy.org/docs.html
•	
Matplotlib documentation:
°°
http://matplotlib.org/

Module 3
Mastering Python Data Visualization
Generate effective results in a variety of visually appealing charts  
using the plotting packages in Python


[ 487 ]
A Conceptual Framework for 
Data Visualization
The existence of the Internet and social media in modern times has led to an 
abundance of data, and data sizes are growing beyond imagination. How and  
when did this begin?
A decade ago, a new way of doing business evolved: of corporations collecting, 
combining, and crunching large amount of data from sources throughout the 
enterprise. Their goal was to use a high volume of data to improve the decision-
making process. Around that same time, corporations like Amazon, Yahoo, and 
Google, which handled large amounts of data, made significant headway. Those 
milestones led to the creation of several technologies supporting big data. We will not 
get into details about big data, but will try exploring why many organizations have 
changed their ways to use similar ideas for better decision-making.
How exactly are these large amount of data used for making better decisions? We will 
get to that eventually, but first let us try to understand the difference between data, 
information, and knowledge, and how they are all related to data visualization. One 
may wonder, why are we talking about data, information, and knowledge. There is a 
storyline that connects how we start, what we start with, how all these things benefit 
the business, and the role of visualization. We will determine the required conceptual 
framework for data visualization by briefly reviewing the steps involved.

A Conceptual Framework for Data Visualization
[ 488 ]
In this chapter, we will cover the following topics:
•	
The difference between data, information, knowledge, and insight
•	
The transformation of information into knowledge, and further, to insight
•	
Collecting, processing, and organizing data
•	
The history of data visualization
•	
How does visualizing data help decision-making?
•	
Visualization plots
Data, information, knowledge, and insight
The terms data, information, and knowledge are used extensively in the context of 
computer science. There are many definitions of these terms, often conflicting and 
inconsistent. Before we dive into these definitions, we will understand how these 
terms are related to visualization. The primary objective of data visualization is to 
gain insight (hidden truth) into the data or information. The whole discussion about 
data, knowledge, and insight in this book is within the context of computer science, 
and not psychology or cognitive science. For the cognitive context, one may refer to 
https://www.ucsf.edu/news/2014/05/114321/converting-data-knowledge-
insight-and-action.
Data
The term data implies a premise from which one may draw conclusions. Though 
data and information appear to be interrelated in a certain context, data actually 
refers to discrete, objective facts in a digital form. Data are the basic building blocks 
that, when organized and arranged in different ways, lead to information that is 
useful in answering some questions about the business.

Chapter 1
[ 489 ]
Data can be something very simple, yet voluminous and unorganized. This discrete 
data cannot be used to make decisions on its own because it has no meaning and, 
more importantly, because there is no structure or relationship between them. The 
process by which data is collected, transmitted, and stored varies widely with the 
types of data and storage methods. Data comes in many forms; some notable forms 
are listed as follows:
•	
CSV files
•	
Database tables
•	
Document formats (Excel, PDF, Word, and so on)
•	
HTML files
•	
JSON files
•	
Text files
•	
XML files
Information
Information is processed data presented as an answer to a business question. Data 
becomes information when we add a relationship or an association. The association 
is accomplished by providing a context or background to the data. The background 
is helpful because it allows us to answer questions about the data.
For example, let us assume that the data given for a basketball player includes height, 
weight, position, college, date of birth, draft pick, draft round, NBA-debut, and 
recruiting rank. The answer to the question, "Who is the first draft pick with a height of 
more than six feet and plays on the point guard position?" is also the information.
Similarly, each player's score is one piece of data. The answer to the question "Who 
has the highest point per game this year and what is his score" is "LeBron James, 
27.47", which is also information.

A Conceptual Framework for Data Visualization
[ 490 ]
Knowledge
Knowledge emerges when humans interpret and organize information and use that 
to drive decision-making. Knowledge is the data, information, and the skills acquired 
through experience. Knowledge comprises the ability to make the appropriate 
decision as well as the skills to execute it.
The essential ingredient—connecting the data—allows us to understand the relative 
importance of each piece of information. By comparing results from the past and 
by recognizing patterns, we don't have to build a solution to a problem from 
scratch. The following diagram summarizes the concepts of data, information, and 
knowledge:
Data
Information
Model
Classify
Structure
Cluster
New data
Can this be
associated
here?
Does it
belong to
any known
cluster?
Does it belong to
this structure or
similar structure?
Does it fit
in this
model?
Knowledge
Knowledge changes in an incremental way, particularly when information is 
rearranged or reorganized or when some computing algorithm changes. Knowledge 
is like an arrow pointing to the results of an algorithm that is dependent on past 
information that comes from data. In many instances, knowledge is also gained by 
visually interacting with the results. Insight on the other hand, opens the way to  
the future.

Chapter 1
[ 491 ]
Data analysis and insight
Before we dive into the definition of insight and how it relates to business, let us see 
how the idea of capturing insight ever began. For over a decade, organizations have 
been struggling to make sense of all the data and information they have, particularly 
with the exploding data size. They all realized the importance of data analysis (also 
known as data analytics or analytics) in order to arrive at an optimal or realistic 
business decision based on existing data and information.
Analytics hinges upon mathematical algorithms to determine the relationships 
between the data that can yield insight. One simple way to understand insight is by 
considering an analogy: when data does not have a structure and proper alignment 
with the business, it gives a clearer and deeper understanding by converting the 
data to a more structured form and aligning it more closely to the business goals. 
Insight is that "eureka" moment when there is a breakthrough result that comes out. 
One should not get confused between the terms Analytics and Business Intelligence. 
Analytics has predictive capabilities while Business Intelligence provides results 
based on the analysis of historical data.
Analytics is usually applicable to a broader spectrum of data and, for this reason, 
it is very common that data collaboration happens internally and/or externally. In 
some business paradigms, the collaboration only happens internally in an extensive 
collection of a dataset, but in most other cases, an external connection helps in 
connecting the dots or completing the puzzle. Two of the most common sources of 
external data connection are social media and consumer base.
Later in this chapter, we refer to real-life business stories that achieved some 
remarkable results by applying analytics to gain insight and drive business value, 
improve decision-making, and understand their customers better.
The transformation of data
By now we know what data is, but now the question is: what is the purpose of 
collecting data? Data is useful for describing a physical or social phenomenon and to 
further answer questions about that phenomenon. For this reason, it is important to 
ensure that the data is not faulty, inaccurate, or incomplete; otherwise, the responses 
based on that data will also not be accurate or complete.
There are different categories of data, some of which are past performance data, 
experimental data, and benchmark data. Past performance data and experimental data 
are pretty self-explanatory. Benchmark data, on the other hand, is data that compares 
the characteristics of two different items or products to a standard measure. Data gets 
transformed into information, is processed further, and is then used for answering 
questions. It is apparent, therefore, that our next step is to achieve that transformation.

A Conceptual Framework for Data Visualization
[ 492 ]
Transforming data into information
Data is collected and stored in several different forms depending on the content and 
its significance. For instance, if the data is about playoff basketball games, then it 
will be in a text and video format. Another example is the temperature recordings 
from all the cities of a country, collected and made accessible via different formats. 
The transformation from data to information involves collection, processing, and 
organization of data as shown in the following diagram:
Collect
Process
Organize
The collected data needs some processing and organizing, which later may or may 
not have a structure, model, or a pattern. However, this process at least gives us an 
organized way of finding answers to questions about the data. The process could be 
a simple sorting based on the total points scored by basketball players or a sorting 
based on the names of the city and state.
The transformation from data to information could also be a little more than 
just sorting such as statistical modeling or a computational algorithm. It is this 
transformation from data to information that is really important and enables the data 
to be queried, accessed, and manipulated. In some cases, when there is a vast and 
divergent amount of data, the transformation may involve processing methods such as 
filtering, aggregating, applying correlation, scaling and normalizing, and classifying.
Data collection
Data collection is a time-consuming process. So, businesses are looking for better 
ways to automate data capture. However, manual data collection is still prevalent 
for many processes. Data collection by automatic processes in modern times uses 
input devices such as sensors. For instance, underwater coral reefs are monitored 
via sensors; agriculture is another area where sensors are used in monitoring soil 
properties, controlling irrigation, and fertilization methods.

Chapter 1
[ 493 ]
Another way to collect data automatically is by scanning documents and log files, 
which is a form of server-side data collection. Manual processes include data 
collection via web-based methods that get stored in the database, which can then be 
transformed into information. Nowadays, web-based collaborative environments are 
benefiting from improved communication and sharing of data.
Traditional visualization and visual analytic tools are typically designed for a single 
user interacting with a visualization application on a single machine. Extending 
these tools to include support for collaboration has clearly come a long way towards 
increasing the scope and applicability of visualizations in the real world.
Data preprocessing
Today, data is highly susceptible to noise and inconsistency due to its size and 
likely origin from multiple, heterogeneous sources and types. There are several 
data preprocessing techniques such as data cleaning, data integration, data reduction, 
and data transformation. Data cleaning can be applied to remove noise and correct 
inconsistencies in the data. Data integration merges and combines the data from 
multiple sources into a coherent format, mostly known as data warehouse. 
Data reduction can reduce data size by, for instance, merging, aggregating, and 
eliminating the redundant features. Data transformations may be applied where data 
is scaled to fall within a smaller range, thus improving the accuracy and efficiency 
in processing and visualizing them. The transformation cycle of data is shown in the 
following diagram:
Extract
Data
Verify
Data
Normalize
Data
Rebuild
Missing Data
Remove
Inconsistent Data

A Conceptual Framework for Data Visualization
[ 494 ]
Anomaly detection is the identification of unusual data that might not fall into an 
expected behavior or pattern in the collected data. Anomalies are also known as 
outliers or noise; for example in signal data, a particular signal that is unusual is 
considered noise, and in transaction data, an outlier is a fraudulent transaction. 
Accurate data collection is essential for maintaining the integrity of data. As much 
as the down side of anomalies, on the flip side, there is also a significant importance 
of outliers—specifically in cases where one would want to find fraudulent insurance 
claims, for instance.
Data processing
Data processing is a significant step in the transformation process. It is imperative 
that the focus be on data quality. Some processing steps that help in preparing data 
for analyzing and understanding it better are dependency modeling and clustering. 
There are other processing techniques, but we will limit our discussion here with the 
two most popular processing methods.
Dependency modeling is the fundamental principle of modeling data to determine 
the nature and structure of the representation. This process searches for relationships 
between the data elements; for example, a department store might gather data on the 
purchasing habits of its customers. This process helps the department store deduce 
the information about frequent purchases.
Clustering is the task of discovering groups in the data that have, in some way or 
another, a "similar pattern", without using known structures in the data.
Organizing data
Database management systems allow users to store data in a structured format. 
However, the databases are too large to fit into memory. There are two ways of 
structuring data:
•	
Storing large data in disks in a structured format like tables, trees, or graphs
•	
Storing data in memory using data structure formats for faster access
A data structure comprises a set of different formats for structuring data to be able 
to store and access it. The general data structure types are arrays, files, tables, trees, 
lists, maps, and so on. Any data structure is designed to organize the data to suit a 
specific purpose so that it can be stored, accessed, and manipulated at runtime.  
A data structure may be selected or designed to store data for the purpose of 
working on it with various algorithms for faster access.
Data that is collected, processed, and organized to be stored efficiently is much easier 
to understand, which leads to information that can be better understood.

Chapter 1
[ 495 ]
Getting datasets
For readers who do not have access to organizational data, there are plenty of 
resources on the Internet with rich datasets from several different sources, such as:
•	
http://grouplens.org (from the University of Minnesota)
•	
http://ichart.finance.yahoo.com/table.csv?s=YHOO&c=1962
•	
http://datawrangling.com/some-datasets-available-on-the-web
•	
http://weather-warehouse.com (weather data)
•	
http://www.bjs.gov/developer/ncvs/ (Bureau of Justice Statistics)
•	
http://census.ire.org/data/bulkdata.html (census data)
•	
http://ww.pro-football-reference.com (football reference)
•	
http://www.basketball-reference.com (basketball reference)
•	
http://www.baseball-reference.com (baseball reference)
•	
http://archive.ics.uci.edu/ml/datasets.html (machine learning)
•	
http://www.pewresearch.org/data/download-datasets/
•	
http://archive.ics.uci.edu/ml/datasets/Heart+Disease (heart 
disease)
Transforming information into knowledge
Information is quantifiable and measurable, it has a shape, and can be accessed, 
generated, stored, distributed, searched for, compressed and duplicated. It is 
quantifiable by the volume or amount of information.
Information transforms into knowledge by the application of discrete algorithms, 
and knowledge is expected to be more qualitative than information. In some problem 
domains, knowledge continues to go through an evolving cycle. This evolution 
happens particularly when the data changes in real time.
Knowledge is like the recipe that lets you make bread out of the information, in this 
case, the ingredients of flour and yeast. Another way to look at knowledge is as the 
combination of data and information, to which experience and expert opinion is added 
to aid decision making. Knowledge is not merely a result of filtering or algorithms.
What are the steps involved in this transformation, and how does the change 
happen? Naturally, it cannot happen by itself. Though the word information is 
subject to different interpretations based on the definition, we will explore it further 
within the context of computing.

A Conceptual Framework for Data Visualization
[ 496 ]
A simple analogy to illustrate the difference between information and knowledge: 
course materials for a particular course provide you the necessary information about 
the concepts, and the teacher later helps the students to understand the concepts 
through discussions. This helps the students in gaining knowledge about the course. 
By a similar process, something needs to be done to transform information into 
knowledge. The following diagram shows the transformation from information  
to knowledge:
Aggregate Information
Discrete Algorithm
As illustrated in the figure, information when aggregated and run through some 
discrete algorithms, gets transformed into knowledge. The information needs to be 
aggregated to get broader knowledge. The knowledge obtained by this transformation 
helps in answering questions about the data or information such as which quarter did 
the company have maximum revenue from sales? How much has advertising driven 
the sales? Or, how many new products have been released this year?
Transforming knowledge into insight
In the traditional system, information is processed, and then analyzed to generate 
reports. Ever since the Internet came into existence, processed information is  
already and always available, and social media has emerged as a new way of 
conducting business.
Organizations have been using external data to gain insights via data analysis. For 
example, the measure of user sentiments from tweets by consumers via Twitter is 
used to follow the opinions about product brands. In some cases, there is a higher 
percentage of users giving a positive message on social media about a new product, 
say an iPhone or a tablet computer. The analytical tool can provide numerical evidence 
of that sentiment, and this is where data visualization plays a significant role.

Chapter 1
[ 497 ]
Another example to illustrate this transformation, Netflix announced a competition 
in 2009 for the best collaborative filtering algorithm to predict user ratings for films, 
based on previous ratings. The winner of that competition used the pragmatic 
theory and achieved a 10.05 percent improvement in predicting user ratings, which 
increased the business value for Netflix.
Internal
External
Data Collaboration
Insight
Analytics
Transforming knowledge into insight is achieved using collaboration and analytics 
as shown in the preceding diagram. Insight implies seeing the solution and realizing 
what needs to be done. Achieving data and information is easy and organizations have 
known methods to achieve that, but getting insight is very hard. Achieving insight 
requires new and creative thinking and the ability to connect the dots. In addition 
to applying creative thinking, data analysis and data visualization play a big role in 
achieving insight. Data visualization is considered both an art and a science.
Data visualization history
Visualization has its roots in a long historical tradition of representing information 
using primitive paintings and maps on walls, tables of numbers, and paintings on 
clay. However, they were not known as visualization or data visualization. Data 
visualization is a new term; it expresses the idea that it involves more than just 
representing data in a graphical form. The information behind the data should 
be revealed in an intuitive representation using good display; the graphic should 
inherently aid viewers in seeing the structure of data.

A Conceptual Framework for Data Visualization
[ 498 ]
Visualization before computers
In early Babylonian times, pictures were drawn on clay and in the later periods 
were rendered on papyrus. The goal of those paintings and maps was to provide 
the viewer with a qualitative understanding of the information. We also know 
that understanding pictures are our natural instincts as a visual presentation 
of information is perceived with greater ease. This section includes only partial 
details about the history of visualization. For elaborate details and examples, we 
recommend two interesting resources:
•	
Data visualization (http://euclid.psych.yorku.ca/datavis/)
•	
The work of Edward Tufte and Graphics Press (www.edwardtufte.com/tufte)
Minard's Russian campaign (1812)
Charles Minard was a civil engineer working in Paris. He summarized the War of 
1812—Napoleon's march on Moscow—in a figurative map. This map is a simple 
picture, which is both a visual timeline and a geographic map depicting the size and 
direction of the army, temperature, and the landmarks and locations. Prof. Edward 
Tufte famously described this picture as possibly being the best statistical graphic  
ever drawn.

Chapter 1
[ 499 ]
The wedge starts with being thick on the left-hand side, and we see the army begin 
the campaign at the Polish border with 422,000 men. The wedge becomes narrower 
as it gets deeper into Russia and the temperature gets lower. This visualization 
manages to condense a number of different numeric and geographic facts into one 
image: when the army gets reduced, the reason for the reduction, and subsequently, 
their retreat.
The Cholera epidemics in London (1831-1855)
In October 1831, the first case of Asiatic cholera occurred in Great Britain, and over 
52,000 people died in the epidemic. Subsequently, in 1848-1849 and 1853-1854, more 
cholera epidemics produced large death tolls.
In 1855, Dr. John Snow produced a map showing the deaths due to cholera clustered 
around the Broad Street pump in London. This map by Dr. John Snow was a 
landmark graphic discovery, but unfortunately, it was devised at the end of that 
period. His map showed the location of each of the deceased, and that provided 
an insight for his conclusion that the source of outbreak could be localized to 
contaminated water from a pump on Broad Street. Around that time, the use of 
graphs became important in economic and state planning.
Statistical graphics (1850-1915)
By the mid 18th century, a rapid growth of visualization had been established 
throughout Europe. In 1863, one page of Galton's multivariate weather chart of 
Europe showed barometric pressure, wind direction, rain, and temperature for 
the month of December 1861 (source: The life, letters and labors of Francis Galton, 
Cambridge University Press).
During this period, statistical graphics became mainstream and there were many 
textbooks written on the same. These textbooks contained detailed descriptions of 
the graphic method, discussing frequencies, and the effects of the choice of scales 
and baselines on the visual estimation of differences and ratios. They also contained 
historical diagrams in which two or more time series could be shown on a single 
chart for comparative views of their histories.

A Conceptual Framework for Data Visualization
[ 500 ]
Later developments in data visualization
In the year 1962, John W. Tukey issued a call for the recognition of data analysis as a 
legitimate branch of statistics; shortly afterwards, he began the invention of a wide 
variety of new, simple, and effective graphic displays under the rubric Exploratory 
Data Analysis (EDA), which was followed by Exploratory Spatial Data Analysis 
(ESDA). Tukey later wrote a book titled Exploratory Data Analysis in 1977. There are 
a number of tools that are useful for EDA with graphical techniques, which are listed 
as follows:
•	
Box-and-whisker plot (box plot)
•	
Histogram
•	
Multivari chart (from candlestick charts)
•	
Run-sequence plot
•	
Pareto chart (named after Vilfredo Pareto)
•	
Scatter plot
•	
Multidimensional scaling
•	
Targeted projection pursuit
Visualization in scientific computing is emerging as an important computer-based 
field, with the goal to improve the understanding of data and to make quick real-time 
decisions. Today, the ability of medical doctors to diagnose ailments is dependent 
upon vision. For example, in hip-replacement surgeries, custom hips can now be 
fabricated before surgical procedures. Accurate measurements can be made prior to 
surgery using non-invasive 3D imaging thereby reducing the number of post-operative 
body rejections from 30 percent to a mere 5 percent (source: http://bonesmart.org/
hip/hip-implants-specialized-and-custom-fitted-options/).
Visualization of the human brain structure and function in 3D is a research 
frontier of far-reaching importance. Few advances have transformed the fields of 
neuroscience and brain-imaging technology, like the ability to see inside and read the 
brain of a living human. For continued progress in brain research, it will be necessary 
to integrate structural and functional information at many levels of abstraction.
The rate at which the hardware performance power has been on the rise tells us that 
we are already able to analyze DNA sequences and visually represent them. The 
future advances in computing promises a much brighter progress in the fields of 
medicine and other scientific areas.

Chapter 1
[ 501 ]
How does visualization help  
decision-making?
There is a variety of ways to represent data visually. However, there are only a few 
ways in which one can portray the data in a manner that allows one to see something 
visually and observe new patterns. Data visualization is not as easy as it seems; it 
is an art and requires a great deal of practice and experience. (Just like painting a 
picture—one cannot be a master painter from day one, it takes a lot of practice.)
Human perception plays an important role in the field of data visualization. A pair of 
healthy human eyes has a total field view of approximately 200 degrees horizontally 
(about 120 degrees of which are shared by both the eyes). About one quarter of the 
human brain is involved in visual processing, which is more than any other sense. 
Among the three senses of hearing, seeing, and smelling, human vision has the 
maximum sense—measured to be sixty per cent (http://contemplatingmadness.
tumblr.com/post/27478393311/10-limits-to-human-perception-and-how-
they-shape).
Effective visualization helps us in analyzing and understanding data. Author 
Stephen Few described the following eight types of quantitative messages (via 
visualization) that may help us with understanding or communicating from a set 
of data (source: https://www.perceptualedge.com/articles/ie/the_right_
graph.pdf):
•	
Time-series
•	
Ranking
•	
Part-to-whole
•	
Deviation
•	
Frequency distribution
•	
Correlation
•	
Nominal comparison
•	
Geographic or geospatial
Scientists have mapped the human genome, and this is one of the reasons why we 
are faced with the challenges of transforming knowledge into a visual representation 
for better understanding. In other words, we may have to find new ways to visually 
present the human genome so that it is not difficult for a common person to 
understand.

A Conceptual Framework for Data Visualization
[ 502 ]
Where does visualization fit in?
It is important to note that data visualization is not scientific visualization. Scientific 
visualization deals with the data that has an inherent physical structure, such as 
air molecules flowing over an aircraft wing. Information visualization, on the other 
hand, deals with abstract data, and helps in solving problems involving large 
datasets. One of the challenges is to ensure that the data is clean and subsequently, to 
reduce the dimensions so that unnecessary information is discarded.
Visualization can be used wherever we see increased knowledge or value of 
data. That can be determined by doing more data analysis and running through 
algorithms. The data analysis might vary from the simplest form to a more 
complicated one.
Sometimes, there is value in looking at data beyond the mean, median, or total, 
because these measurements only measure things that may seem obvious. 
Sometimes, aggregates or values around a region hide the interesting details that 
need special focus. One classic example is the "Anscombe's quartet" which comprises 
of four datasets that have nearly identical simple statistical properties yet appear 
very different when graphed. For more on this, one can refer to the link, https://
en.wikipedia.org/wiki/Anscombe%27s_quartet.
Visualize
Insights
Transform
Analyze
Mostly, datasets that lend themselves well to visualization can take different forms, 
but some paint a clearer picture to understand than others. In some cases, it is 
mandatory to analyze them several times to get a much better understanding of the 
visualization as shown in the preceding diagram.

Chapter 1
[ 503 ]
A good visualization is not just a static picture that one can look at, like an exhibit 
in a museum. It is something that allows us to drill down and find more about the 
change in data. For example, view first, zoom and filter, change the values of some 
scale of display, and view the results in an incremental way, as described in http://
www.mat.ucsb.edu/~g.legrady/academic/courses/11w259/schneiderman.pdf 
by Ben Shneiderman. Sometimes, it is much harder to display everything on a single 
display and on a single scale, and only by experience, one can better understand 
these visualization methods. Summarizing further, visualization is useful in both 
organizing and making sense out of data, particularly when it is in abundance.
Interactive visualization is emerging as a new form of communication, which allows 
users to analyze the information in order to construct their own, new understanding 
of the data.
Data visualization today
While many areas of computing aim to replace human judgment with automation, 
visualization systems are unique and are explicitly designed not to replace humans. 
In fact, they are designed to keep the humans actively involved in the whole process; 
why is that?
Data Visualization is an art, driven by data and yet created by humans with the help 
of various computing tools. An artist paints a picture using tools and materials like 
brushes, and colors. Similarly, another artist tries to create data visualization with 
the help of computing tools. Visualization can be aesthetically pleasing and helps in 
making things clear; sometimes, it may lack one or both of those qualities depending 
on the users who create it.
Today, there are over thirty different visual representations of data, each having a 
reason to represent data in that specific way. As the visualization methods progress, 
we have much more than just bar graphs and pie charts. Despite the many benefits of 
data visualization, they are undermined due to a lack of understanding and, in some 
cases, due to cluttering together of things on a dashboard that becomes  
too cumbersome.
There are many ways to present data, but only a handful of those make sense in 
most cases; this will be explained in detail in later sections of this chapter. Before 
that discussion, let us take a look at a list of some important things that make a good 
visualization.

A Conceptual Framework for Data Visualization
[ 504 ]
What is a good visualization?
Good visualization helps the users to explore and understand data, providing 
value and deep insights. It is effective, visually appealing, scalable, and is easy to 
understand (good visualization does not have to be too complicated). Visualization is 
a central tool in finding patterns and trends in the data by carrying out research and 
analysis, using whichever one can answer questions about the data.
The main principle behind an effective visualization is to identify the main point that 
you want to make, recognize the level and background of your audience, accurately 
represent the data, and then create a clear presentation that conveys the message to 
that audience.
Example: The following representations have been created with a small sample data 
source that shows the percentage of women and men conferred with degrees in ten 
different disciplines for the years from 1970-2012 (womens-undergrad-degrees.csv 
and mens-undergrad-degrees.csv from http://www.knapdata.com/python/):

Chapter 1
[ 505 ]
The full data source available at http://nces.ed.gov/programs/digest/d11/
tables/dt11_290.asp maintains the complete set of data.
One simple way is to represent them on one scale, although there is no relationship 
between the numbers between the different disciplines. Let us analyze and see if this 
representation makes sense, and if it doesn't, then what else do we need? Are there 
any other representations?
For one thing, all the data about the different disciplines is displayed on one screen, 
which is an excellent comparison. However, if we need to get the information for 
the year 2000, there is no straightforward way. Unless there is an interactive mode 
of display that is similar to a financial stock chart, there is no easy way to determine 
the information about the degrees conferred in multiple disciplines for the year 2000. 
Another confusing part of these plots is that the percentage doesn't add up to a sum 
of 100 percent. On the other hand, the percentage of conferred degrees within one 
discipline for men and women add up to 100 percent; for instance, the percentage of 
degrees conferred in the Health Professions discipline for men and women are 15.2 
percent and 84.8 percent respectively.
Can we represent these through other visualization methods? One can create bubble 
charts for each year, have an interactive visualization with year selection, and also 
have a play button that transitions the bubbles for each year.
This visualization better suits the data that we are looking at. We can also use the 
same slider with the original plot and make it interactive by highlighting the data 
for the selected year. It is a good habit to visualize the data in several different ways 
to see if some display makes more sense than the other. We may have to scale the 
values on a logarithmic scale if there is a very large range of numerical values (for 
example, from 20 to 200,000).
One can write a program in Python to accomplish this bubble chart. Other alternate 
languages are JavaScript using D3.js and R using R-Studio. It is left for the reader to 
explore other visualization options.

A Conceptual Framework for Data Visualization
[ 506 ]
Google Motion Chart can be used for visualization to represent this interactive 
chart at developers.google.com/chart/interactive/docs/gallery/
motionchart?csw=1#Example where it shows a working example that is similar to 
this bubble chart. The bubble chart shown here is for only three years, but you can 
create another one for all the years.
Data visualization is a process that has to be used after data analysis. We also 
noticed earlier that data transformation, data analysis, and data visualization are 
done several times; why is that so? We all know the famous quote, Knowledge is 
having the right answer, Intelligence is asking the right question. Data analysis helps us 
to understand the data better and therefore be in a position to respond to questions 
about the data. However, when the data is represented visually in several different 
ways, some new questions emerge, and this is one of the reasons why there is a 
repeated process of analysis and visualization.
Visualization of data is one of the primary tools for data exploration, and almost 
always precedes or inspires data analysis. There are many tools to display data 
visually, but there are fewer tools to do the analysis. Programming languages 
like Julia, R, and Python have ranked higher for performing data analysis, but 
for visualization, JavaScript based D3.js has a much greater potential to generate 
interactive data visualization.

Chapter 1
[ 507 ]
Between R and Python, R is a more difficult language to learn. Python, on the other 
hand, is much easier. This is also debated on Quora; one may check the validity 
of this on the internet (https://www.quora.com/Which-is-better-for-data-
analysis-R-or-Python). Today there are numerous tools in Python for statistical 
modeling and data analysis, and therefore, it is an attractive choice for data science.
Visualization plots
One of the reasons why we perform visualization is to confirm our knowledge of 
data. However, if the data is not well understood, you may not frame the right 
questions about the data.
When creating visualizations, the first step is to be clear on the question to be 
answered. In other words, how is visualization going to help? There is another 
challenge that follows this—knowing the right plotting method. Some visualization 
methods are as follows:
•	
Bar graph and pie chart
•	
Box plot
•	
Bubble chart
•	
Histogram
•	
Kernel Density Estimation (KDE) plot
•	
Line and surface plot
•	
Network graph plot
•	
Scatter plot
•	
Tree map
•	
Violin plot
In the course of identifying the message that the visualization should convey, it 
makes sense to look at the following questions:
•	
How many variables are we dealing with, and what are we trying to plot?
•	
What do the x axis and y axis refer to? (For 3D, z axis as well.)
•	
Are the data sizes normalized and does the size of data points mean anything?
•	
Are we using the right choices of colors?
•	
For time series data, are we trying to identify a trend or a correlation?

A Conceptual Framework for Data Visualization
[ 508 ]
If there are too many variables, it makes sense to draw multiple instances of the same 
plot on different subsets of data. This technique is called lattice or trellis plotting. It 
allows a viewer to quickly extract a large amount of information about complex data.
Consider a subset of student data that has an unusual mixture of information about 
(gender, sleep, tv, exercise, computer, gpa) and (height, momheight, 
dadheight). The units for computer, tv, sleep, and exercise are hours, height is 
in inches and gpa is measured on a scale of 4.0.
tv
computer
height
dadheight
exercise
gpa
13.0
20.0
15.0
8.0
2.5
2.0
4.0
8.0
1.0
8.0
3.5
11.0
10.0
1.0
10.0
10.0
7.0
15.0
20.0
10.0
14.0
28.0
10.0
15.0
25.0
9.0
20.0
14.0
84.0
11.0
sleep
3.50
9.00
6.00
6.00
5.00
9.00
8.50
7.00
8.00
4.50
8.00
5.00
8.00
9.00
5.00
66.0
72.0
68.0
68.0
64.0
68.5
69.0
66.0
70.0
67.0
68.0
68.0
68.0
61.0
65.0
momheight
66.0
64.0
62.0
59.0
65.0
60.0
66.0
63.0
68.0
63.0
62.0
64.0
61.0
62.0
62.0
71.0
65.0
74.0
70.0
70.0
68.0
76.0
70.0
71.0
66.0
64.0
69.0
72.0
62.0
66.0
10.0
2.0
3.0
6.0
6.5
2.0
3.0
4.5
3.0
6.0
8.0
0.0
10.0
3.0
5.0
4.000
2.300
2.600
2.800
2.620
2.200
3.780
3.200
3.310
3.390
3.000
2.500
2.800
2.340
2.000
gender
Female
Female
Female
Female
Female
Female
Male
Male
Male
Male
Male
Male
Male
Male
Male
The preceding data is an example that has more variables than usual, and therefore, 
it makes sense to do a trellis plot to visualize and see the relationship between these 
variables.
One of the reasons we perform visualization is to confirm our knowledge of data.  
However, if the data is not well understood, one may not frame the right  
questions about it.

Chapter 1
[ 509 ]
Since there are only two genders in the data, there are 10 combinations of variables 
that can be possible (sleep, tv), (sleep, exercise), (sleep, computer), (sleep, gpa), 
(tv, exercise), (tv, computer), (tv, gpa), (exercise, computer), (exercise, gpa), 
and (computer, gpa) for the first set of variables; another two, (height, momheight) 
and (height, dadheight) for the second set. Following are all the combinations 
except (sleep, tv), (tv, exercise).

A Conceptual Framework for Data Visualization
[ 510 ]
Our goal is to find what combination of variables can be used to make some sense out 
of this data, or to see if any of these variables have any meaningful impact. Since the 
data is about students, gpa may be a key variable that drives the relevance of the other 
variables. The preceding image depicts scatter plots that show that a greater number 
of female students have a higher gpa than the male students and a greater number of 
male students spend more time on computer and get a similar gpa range of values. 
Although all scatter plots are being shown here, the intent is to find out which data 
plays a more significant role, and what sense can we make out of this data.
A greater number of blue dots high up (for gpa on the y axis) shows that there are 
more female students with a higher gpa (this data was collected from UCSD).
The data can be downloaded from http://www.knapdata.com/python/ucdavis.
csv.

Chapter 1
[ 511 ]
One can use the seaborn package and display a scatter plot with very few lines 
of code, and the following example shows a scatter plot of gpa along the x - axis 
compared with the time spent on computer by students:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
students = pd.read_csv("/Users/kvenkatr/Downloads/ucdavis.csv")
g = sns.FacetGrid(students, hue="gender", palette="Set1", size=6)
g.map(plt.scatter, "gpa", "computer", s=250, linewidth=0.65,
  edgecolor="white")
g.add_legend()
Downloading the example code
You can download the example code files for all Packt books you 
have purchased from your account at http://www.packtpub.
com. If you purchased this book elsewhere, you can visit http://
www.packtpub.com/support and register to have the files 
e-mailed directly to you.
These plots were generated using the matplotlib, pandas, and seaborn library 
packages. Seaborn is a statistical data visualization library based on matplotlib, 
created by Michael Waskom from Stanford University. Further details about these 
libraries will be discussed in the following chapters.
There are many useful classes in the Seaborn library. In particular, the FacetGrid 
class comes in handy when we need to visualize the distribution of a variable or the 
relationship between multiple variables separately within subsets of data. FacetGrid 
can be drawn with up to three dimensions, that is, row, column and hue. These 
library packages and their functions will be described in later chapters.
When creating visualizations, the first step is to be clear on the question to be 
answered. In other words, how is visualization going to help? The other challenge is 
choosing the right plotting method.

A Conceptual Framework for Data Visualization
[ 512 ]
Bar graphs and pie charts
When do we choose bar graphs and pie charts? They are the oldest visualization 
methods and pie chart is best used to compare the parts of a whole. However, bar 
graphs can compare things between different groups to show patterns.
Bar graphs, histograms, and pie charts help us compare different data samples, 
categorize them, and determine the distribution of data values across that sample. Bar 
graphs come in several different styles varying from single, multiple, and stacked.
Bar graphs
Bar graphs are especially effective when you have numerical data that splits nicely 
into different categories, so you can quickly see trends within your data.
Bar graphs are useful when comparing data across categories. Some notable 
examples include the following:
•	
Volume of jeans in different sizes
•	
World population change in the past two decades
•	
Percent of spending by department
In addition to this, consider the following:
•	
Add color to bars for more impact: Showing revenue performance  
with bars is informative, but adding color to reveal the profits adds  
visual insight. However, if there are too many bars, colors might make  
the graph look clumsy.
•	
Include multiple bar charts on a dashboard: This helps the viewer to 
quickly compare related information instead of flipping through a bunch of 
spreadsheets or slides to answer a question.
•	
Put bars on both sides of an axis: Plotting both positive and negative data 
points along a continuous axis is an effective way to spot trends.
•	
Use stacked bars or side-by-side bars: Displaying related data on top of 
or next to each other gives depth to your analysis and addresses multiple 
questions at once.
These plots can be achieved with fewer than 12 lines of Python code, and more 
examples will be discussed in the later chapters.

Chapter 1
[ 513 ]
With bar graphs, each column represents a group defined by a specific category; with 
histograms, each column represents a group defined by a quantitative variable. With 
bar graphs, the x axis does not have a low-end or a high-end value, because the labels 
on the x axis are categorical and not quantitative. On the other hand, in a histogram, 
there is going to be a range of values. The following bar graph shows the statistics of 
Oscar winners and nominees in the US from 2000-2009:
The following Python code uses matplotlib to display bar graphs for a small data 
sample from the movies (This may not necessarily be a real example, but gives an 
idea of plotting and comparing):
[5]: import numpy as np
     import matplotlib.pyplot as plt
     N = 7
     winnersplot = (142.6, 125.3, 62.0, 81.0, 145.6, 319.4, 178.1)
     ind = np.arange(N)  # the x locations for the groups

A Conceptual Framework for Data Visualization
[ 514 ]
     width = 0.35        # the width of the bars
     fig, ax = plt.subplots()
     winners = ax.bar(ind, winnersplot, width, color='#ffad00')
     nomineesplot = (109.4, 94.8, 60.7, 44.6, 116.9, 262.5, 102.0)
     nominees = ax.bar(ind+width, nomineesplot, width,
       color='#9b3c38')
     # add some text for labels, title and axes ticks
     ax.set_xticks(ind+width)
     ax.set_xticklabels( ('Best Picture', 'Director', 'Best  
     Actor',
       'Best Actress','Editing', 'Visual Effects',  
       'Cinematography'))
     ax.legend( (winners[0], nominees[0]), ('Academy Award  
     Winners',  
       'Academy Award Nominees') )
     def autolabel(rects):
       # attach some text labels
       for rect in rects:
         height = rect.get_height()
         hcap = "$"+str(height)+"M"
         ax.text(rect.get_x()+rect.get_width()/2., height, hcap,
           ha='center', va='bottom', rotation="vertical")
     autolabel(winners)
     autolabel(nominees)
     plt.show()
Pie charts
When it comes to pie charts, one should really consider answering the questions, "Do 
the parts make up a meaningful whole?" and "Do you have sufficient real-estate to 
represent them using a circular view?". There are critics who come crashing down 
on pie charts, and one of the main reasons, for that is that when there are numerous 
categories, it becomes very hard to get the proportions and compare those categories 
to gain any insight. (Source: https://www.quora.com/How-and-why-are-pie-
charts-considered-evil-by-data-visualization-experts).

Chapter 1
[ 515 ]
Pie charts are useful for showing proportions on a single space or across a map. 
Some notable examples include the following:
•	
Response categories from a survey
•	
Top five company market shares in a specific technology (in this case, one 
can quickly know which companies have a major share in the market)
In addition to this, consider the following:
•	
Limit pie wedges to eight: If there are more than eight proportions to 
represent, consider a bar graph. Due to limited real - estate, it is difficult to 
meaningfully represent and interpret the pieces.
•	
Overlay pie charts on maps: Pie charts can be much easier to spread across  
a map and highlight geographical trends. (The wedges should be limited 
here too.)
Consider the following code for a simple pie-chart to compare how the intake of 
admissions among several disciplines are distributed:
[6]: import matplotlib.pyplot as plt
     labels = 'Computer Science', 'Foreign Languages', 
       'Analytical Chemistry', 'Education', 'Humanities', 
       'Physics', 'Biology', 'Math and Statistics', 'Engineering'
     sizes = [21, 4, 7, 7, 8, 9, 10, 15, 19]
     colors = ['yellowgreen', 'gold', 'lightskyblue',  
     'lightcoral',
       'red', 'purple', '#f280de', 'orange', 'green']
     explode = (0,0,0,0,0,0,0,0,0.1)
     plt.pie(sizes, explode=explode, labels=labels, 
       autopct='%1.1f%%', colors=colors)
     plt.axis('equal')
     plt.show()

A Conceptual Framework for Data Visualization
[ 516 ]
The following pie chart example shows the university admission intake in some 
chosen top-study areas:
Box plots
Box plots are also known as box-and-whisker plots. This is a standardized way of 
displaying the distribution of data based on the five number summaries: minimum, 
first quartile, median, third quartile, and maximum. The following diagram shows 
how a box plot can be read:
This whisker shows
the lowest value
The width of the box shows
the interquartile range
This whisker shows
the highest value
lower quartile
median
upper quartile

Chapter 1
[ 517 ]
A box plot is a quick way of examining one or more sets of data graphically, and 
they take up less space to define five summaries at a time. One example that we can 
think of for this usage is: if the same exam is given to two or more classes, then a box 
plot can tell when the most students in one class did better than most students in the 
other class. Another example is that if there are more people who eat burgers, the 
median is going to be higher or the top whisker could be longer than the bottom one. 
In such a case, it gives one a good overview of the data distribution.
Before we try to understand when to use box plots, here is a definition that one needs 
to understand. An outlier in a collection of data values is an observation that lies at 
an abnormal distance from other values.
Box plots are most useful in showing the distribution of a set of data. Some notable 
examples are as follows:
•	
Identifying outliers in the data
•	
Determining how the data is skewed towards either end
In addition to this, consider the following:
•	
Hide the points within the box: focus on the outliers
•	
Compare across distributions: Box plots are good for comparing quickly 
with distributions between data set
Scatter plots and bubble charts
A scatter plot is a type of visualization method for displaying two variables. The 
pattern of their intersecting points can graphically show the relationship patterns. 
A scatter plot is a visualization of the relationship between two variables measured 
on the same set of individuals. On the other hand, a Bubble chart displays three 
dimensions of data. Each entity with its triplet (a,b,c) of associated data is plotted as a 
disk that expresses two of those three variables through the xy location and the third 
shows the quantity measured for significance.
Scatter plots
The data is usually displayed as a collection of points, and is often used to plot 
various kinds of correlations. For instance, a positive correlation is noticed when  
the increase in the value of one set of data increases the other value as well. The 
student record data shown earlier has various scatter plots that show the correlations 
among them.

A Conceptual Framework for Data Visualization
[ 518 ]
In the following example, we compare the heights of students with the height of their 
mother to determine if there is any positive correlation. The data can be downloaded 
from http://www.knapdata.com/python/ucdavis.csv.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
students = pd.read_csv("/Users/Macbook/python/data/ucdavis.csv")
g = sns.FacetGrid(students, palette="Set1", size=7)
g.map(plt.scatter, "momheight", "height", s=140, linewidth=.7, 
edgecolor="#ffad40", color="#ff8000")
g.set_axis_labels("Mothers Height", "Students Height")
We demonstrate this example using the seaborn package, but one can also 
accomplish this using only matplotlib, which will be shown in the following 
section. The scatterplot map for the preceding code is depicted as follows:

Chapter 1
[ 519 ]
Scatter plots are most useful for investigating the relationship between two different 
variables. Some notable examples are as follows:
•	
The likelihood of having skin cancer at different ages in males versus females
•	
The correlation between the IQ test score and GPA
In addition to this, consider the following:
•	
Add a trend line or line of best-fit (if the relation is linear): Adding a trend 
line can show the correlation among the data values
•	
Use informative mark types: Informative mark types should be used if the 
story to be revealed is about data that can be visually enhanced with relevant 
shapes and colors
Bubble charts
The following example shows how one can use color map as a third dimension that 
may indicate the volume of sales or any appropriate indicator that drives the profit:
 [7]: import numpy as np
     import pandas as pd
     import seaborn as sns
     import matplotlib.pyplot as plt
     sns.set(style="whitegrid")
     mov =  
     pd.read_csv("/Users/MacBook/python/data/2014_gross.csv")
     x=mov.ProductionCost
     y=mov.WorldGross
     z=mov.WorldGross
     cm = plt.cm.get_cmap('RdYlBu')
     fig, ax = plt.subplots(figsize=(12,10))
     sc = ax.scatter(x,y,s=z*3, c=z,cmap=cm, linewidth=0.2,  
     alpha=0.5)
     ax.grid()
     fig.colorbar(sc)
     ax.set_xlabel('Production Cost', fontsize=14)
     ax.set_ylabel('Gross Profits', fontsize=14)
     plt.show()
...

A Conceptual Framework for Data Visualization
[ 520 ]
The following scatter plot is the result of the example using color map:

Chapter 1
[ 521 ]
Bubble charts are extremely useful for comparing relationships between data in three 
numeric-data dimensions: the x axis data, the y axis data, and the data represented by 
the bubble size. Bubble charts are like XY scatter plots, except that each point on the 
scatter plot has an additional data value associated with it that is represented by the 
size of the circle or "bubble" centered on the XY point. Another example of a bubble 
chart is shown here (without the python code, to demonstrate a different style):
In the preceding display, the bubble chart shows the Life Expectancy versus Gross 
Domestic Product per Capita around different continents.
Bubble charts are most useful for showing the concentration of data along two axes 
with a third data element being the significance value measured. Some notable 
examples are as follows:
•	
The production cost of movies and gross profit made, and the significance 
measured along a heated scale as shown in the example

A Conceptual Framework for Data Visualization
[ 522 ]
In addition to this, consider the following:
•	
Adding color and shape significance: By varying the size and color,  
the data points can be transformed into a visualization that clearly  
answers some questions
•	
Make it interactive: If there are too many data points, bubble charts could 
get cluttered, so group them on the time axis or categories, and visualize 
them interactively
KDE plots
Kernel Density Estimation (KDE) is a non-parametric way to estimate the 
probability density function and its average across the observed data points to create 
a smooth approximation. They are closely related to histograms, but sometimes can 
be endowed with smoothness or continuity by a concept called kernel.
The kernel of a Probability Density Function (PDF) is the form of the PDF in which 
any factors that are not functions of any of the variables in the domain are omitted. 
We will focus only on the visualization aspect of it; for more theory, one may refer to 
books on statistics.
There are several different Python libraries that can be used to accomplish a KDE 
plot at various depths and levels including matplotlib, Scipy, scikit-learn, and 
seaborn. Following are two examples of KDE Plots. There will be more examples in 
later chapters, wherever necessary to demonstrate various other ways of displaying 
KDE plots.

Chapter 1
[ 523 ]
In the following example, we use a random dataset of size 250 and the seaborn 
package to show the distribution plot in a few simple lines:
One can display simple distribution of a data plot using seaborn, which is 
demonstrated here using a random sample generated using numpy.random:
from numpy.random import randn
import matplotlib as mpl
import seaborn as sns
import matplotlib.pyplot as plt
sns.set_palette("hls")
mpl.rc("figure", figsize=(10,6))
data = randn(250)
plt.title("KDE Demonstration using Seaborn and Matplotlib", 
fontsize=20)
sns.distplot(data, color='#ff8000')

A Conceptual Framework for Data Visualization
[ 524 ]
In the second example, we are demonstrating the probability density function using 
SciPy and NumPy. First we use norm() from SciPy to create normal distribution 
samples and later, use hstack() from NumPy to stack them horizontally and apply 
gaussian_kde() from SciPy.

Chapter 1
[ 525 ]
The preceding plot is the result of a KDE plot using SciPy and NumPy, which is 
shown as follows:
from scipy.stats.kde import gaussian_kde
from scipy.stats import norm
from numpy import linspace, hstack
from pylab import plot, show, hist
sample1 = norm.rvs(loc=-1.0, scale=1, size=320)
sample2 = norm.rvs(loc=2.0, scale=0.6, size=320)
sample = hstack([sample1, sample2])
probDensityFun = gaussian_kde(sample)
plt.title("KDE Demonstration using Scipy and Numpy", fontsize=20)
x = linspace(-5,5,200)
plot(x, probDensityFun(x), 'r')
hist(sample, normed=1, alpha=0.45, color='purple')
show()
The other visualization methods such as the line and surface plot, network graph 
plot, tree maps, heat maps, radar or spider chart, and the violin plot will be discussed 
in the next few chapters.
Summary
The examples shown so far are just to give you an idea of how one should think 
and plan before making a presentation. The most important stage is the data 
familiarization and preparation process for visualization. Whether one can get the 
data first or shape the desired story is mainly influenced by exactly what outcome is 
attempted. It is like the "chicken and the egg" situation—does data come first or the 
focus? Initially, it may not be clear what data one may need, but in most cases, after a 
few iterations, things will be clear as long as there are no errors in the data.
Transform the quality of data by doing some cleanup or reducing the dimensions 
(if required), and fill gaps if any. Unless the data is good, the efforts that one may 
put into presenting it visually will be wasted. After a reasonable understanding of 
the data is achieved, it makes sense to determine what kind of visualization may be 
appropriate. In some cases, it would be better to display it in several different ways 
to see the story clearly.


[ 527 ]
Data Analysis and 
Visualization
Most visualization stories begin with some question that is oriented towards a 
topic where the data is being either explored or collected. The question contains the 
premise to the story and leads us to the point at which the data takes an expedition 
over the storyline. Such data expeditions that start with a question, for example, 
How many Ebola deaths were reported in the year 2014? are implemented by a group of 
people by collaborating with each other. The role of data communicators should be 
to create an information experience that transforms how their audiences think about 
their story.
The key parts of the story relate to the process of placing the visualization in a 
meaningful context. The context provides knowledge that answers questions  
such as the following:
•	
Is there sufficient data?
•	
Is there a time frame within which this data exists?
•	
Which associable events around the globe will influence this data?
To reiterate, it is important to understand the data, and identify the context of  
the question to which an answer is being attempted. Sometimes, one may start 
digging into the data before even finalizing the question, and in such a case, on 
gaining a refined understanding of the data, you may have an improved or clearer 
version of the question.

Data Analysis and Visualization
[ 528 ]
The process starts with the input data, assuming that one has ways to acquire, 
parse, and gather the required information from some source. There are some 
situations where it is better to visualize this collected information to eliminate noise, 
while in some other cases, one may filter and analyze the data before applying 
the visualization methods. In this chapter, we will learn the different ways of data 
exploration to further use them for visualization methods. We will also go through 
some interesting data stories and the related concepts in the following sequence:
•	
Acquiring, parsing, and filtering data to detect outliers and abnormalities, 
data mining and refining, visual representation, and interaction
•	
Creating interesting stories with data
•	
Perception, presentation methods, and best practice for visualization
•	
Interactive visualization—exploring event listeners and layouts
Why does visualization require planning?
The whole process of visualization involves people with different skill sets 
and domain expertise. Data wranglers diligently collect data and analyze it. 
Mathematicians and statisticians understand the visual design principles and 
communicate their data using those principles. Designers or artists (in some cases, 
frontend developers) have the skills necessary for visualization, while business 
analysts look out for things like customer behavioral patterns, outliers, or a sudden 
unusual trend. However, it always starts with either acquiring or gathering data, and 
with the following steps:
•	
Acquire or gather data from an external source, a website, or from a file  
on a disk
•	
Parse and filter data using programming methods to parse, clean, and 
reduce the data
•	
Analyze and refine to remove noise and unnecessary dimensions and  
find patterns
•	
Represent and interact to present the data in ways that are more accessible 
and understandable
How much of this process is followed varies with different problems, and in some 
cases, there is more analysis done than filtering of data. As discussed in the previous 
chapter, in some instances, the analysis and visualization is done iteratively. In other 
words, the distribution of these steps is not always predictable and consistent.

Chapter 2
[ 529 ]
The Ebola example
To illustrate the steps mentioned in the previous section and how they lead to an 
understandable visualization, let us consider the question we had earlier, that is, 
How many Ebola deaths were reported in the year 2014? This particular question leads to 
very specific data, which is usually maintained by the World Health Organization 
(http://www.who.int/en/) or Humanitarian Data Exchange (https://hdx.
rwlabs.org). The original source of this data is the World Health Organization 
(WHO), but the Humanitarian Data Exchange (HDX) is the contributor. Please 
note, however, that we shall have all the data, along with the Python source code 
examples for this book, available at a single place.
The data contains information about the spread of the Ebola disease in Guinea, 
Liberia, Mali, Nigeria, Senegal, Sierra Leone, Spain, United Kingdom, and the  
United States of America.
The contributor URL for this information is https://data.hdx.rwlabs.org/
dataset/ebola-cases-2014/.
The contents of the data file in the Comma Separated Value (CSV) format include 
the indicator, country name, date, and the number of deaths or the number of 
infections depending upon what the indicator says. There are 36 different indicators, 
and the top 10 are listed as follows (others can be viewed in Appendix, Go Forth and 
Explore Visualization):
•	
Number of probable Ebola cases in the last 7 days
•	
Number of probable Ebola deaths in the last 21 days
•	
Number of suspected Ebola cases in the last 21 days
•	
Number of suspected Ebola cases in the last 7 days
•	
Number of suspected Ebola deaths in the last 21 days
•	
Proportion of confirmed Ebola cases that are from the last 21 days
•	
Proportion of confirmed Ebola cases that are from the last 7 days
•	
Proportion of confirmed Ebola deaths that are from the last 21 days
•	
Proportion of suspected Ebola cases that are from the last 7 days
•	
Proportion of suspected Ebola deaths that are from the last 21 days
At this point, after looking at the list of indicators, the single question that we 
had initially, that is, How many Ebola deaths were reported in the year 2014? could 
be changed to multiple sets of questions. For simplicity, we stay focused on that 
single question and see how we can further analyze the data and come up with a 
visualization method. First, let us take a look at the ways to read the data file.

Data Analysis and Visualization
[ 530 ]
In any programming language, there is more than one way to read a file, and one of 
the options is to use the pandas library for Python, which has high performance and 
uses data structures and data analysis tools. The other option is to use the csv library 
to read the data file in the CSV format. What is the difference between them? They 
both can do the job. In the older version of pandas there were issues with memory 
maps for large data (that is, if the data file in the CSV format was very large), but 
now that has been optimized. So let's start with the following code:
[1]:  with open('("/Users/kvenkatr/python/ebola.csv ', 'rt') as f: 
        filtereddata = [row for row in csv.reader(f) if row[3] != 
"0.0" and 
        row[3] != "0" and "deaths" in row[0]]
[2]:    len(filtereddata)
Out[2]: 1194
The preceding filter can also be performed using pandas, as follows:
import pandas as pd
eboladata = pd.read_csv("/Users/kvenkatr/python/ebola.csv")
filtered = eboladata[eboladata["value"]>0]
filtered = filtered[filtered["Indicator"].str.contains("deaths")]
len(filtered)
The data can be downloaded from http://www.knapdata.com/python/ebola.csv. 
The next step is to open the data file with the read text (rt) format. It reads each row 
and further filters the rows with zero number of deaths as the indicator string has 
the word deaths in it. This is a very straightforward filter applied to ignore the data 
with no reported cases or deaths. Printing only the top five rows of the filtered data 
shows the following:
[3]:  filtereddata[:5]
Out[3]: 
[['Cumulative number of confirmed Ebola deaths', 
'Guinea','2014-08-29', '287.0'],
 ['Cumulative number of probable Ebola deaths','Guinea','2014-08-29',
  '141.0'],
 ['Cumulative number of suspected Ebola deaths','Guinea','2014-08-29',
  '2.0'],
 ['Cumulative number of confirmed, probable and suspected Ebola 
deaths',
  'Guinea','2014-08-29','430.0'],
 ['Cumulative number of confirmed Ebola deaths',
  'Liberia','2014-08-29','225.0']]

Chapter 2
[ 531 ]
If all the data about the reported cases of Ebola in each country are to be separated, 
how do we further filter this? We can sort them on the country column. There are 
four columns in this data file, indicator, country, date, and number value, as 
shown in the following code:
[4]:  import operator
      sorteddata = sort(filtereddata, key=operator.itemgetter(1))
[5]:  sorteddata[:5]
Out[5]: 
[['Cumulative number of confirmed Ebola deaths', 
'Guinea','2014-08-29', '287.0'],
 ['Cumulative number of probable Ebola deaths','Guinea','2014-08-29',
  '141.0'],
 ['Cumulative number of suspected Ebola deaths','Guinea','2014-08-29',
  '2.0'],
 ['Cumulative number of confirmed, probable and suspected Ebola 
deaths',
  'Guinea','2014-08-29','430.0'],
 ['Number of confirmed Ebola deaths in the last 21 days', 'Guinea',
  '2014-08-29','8.0']]
After looking at the data so far, there are two indicators that appear to be of interest 
in the context in which we started this data expedition:
•	
Cumulative number of confirmed Ebola deaths
•	
Cumulative number of confirmed, probable, and suspected Ebola deaths
By applying visualization several times, we also notice that among the several 
countries, Guinea, Liberia, and Sierra Leone had more confirmed deaths than the 
others. We will now see how the reported deaths in these three countries could  
be plotted:
import matplotlib.pyplot as plt  
import csv 
import operator 
import datetime as dt  
with open('/Users/kvenkatr/python/ebola.csv', 'rt') as f: 
  filtereddata = [row for row in csv.reader(f) if row[3] != "0.0" and 
  row[3] != "0" and "deaths" in row[0]] 
sorteddata = sorted(filtereddata, key=operator.itemgetter(1))  
guineadata  = [row for row in sorteddata if row[1] == "Guinea" and 

Data Analysis and Visualization
[ 532 ]
  row[0] == "Cumulative number of confirmed Ebola deaths"] 
sierradata  = [row for row in sorteddata if row[1] == "Sierra Leone" 
and 
  row[0] == "Cumulative number of confirmed Ebola deaths"] 
liberiadata = [row for row in sorteddata if row[1] == "Liberia" and 
  row[0] == "Cumulative number of confirmed Ebola deaths"] 
g_x = [dt.datetime.strptime(row[2], '%Y-%m-%d').date() for 
  row in guineadata] 
g_y = [row[3] for row in guineadata] 
s_x = [dt.datetime.strptime(row[2], '%Y-%m-%d').date() for 
  row in sierradata] 
s_y = [row[3] for row in sierradata] 
l_x = [dt.datetime.strptime(row[2], '%Y-%m-%d').date() for
  row in liberiadata] 
l_y = [row[3] for row in liberiadata] 
plt.figure(figsize=(10,10))
plt.plot(g_x,g_y, color='red', linewidth=2, label="Guinea") 
plt.plot(s_x,s_y, color='orange', linewidth=2, label="Sierra Leone") 
plt.plot(l_x,l_y, color='blue', linewidth=2, label="Liberia")
plt.xlabel('Date', fontsize=18) 
plt.ylabel('Number of Ebola Deaths', fontsize=18) 
plt.title("Confirmed Ebola Deaths", fontsize=20) 
plt.legend(loc=2) 
 
plt.show()

Chapter 2
[ 533 ]
The result would look like the following image:
We can construct a similar plot for the other indicator, that is, cumulative number of 
confirmed, probable, and suspected Ebola deaths. (This may not be the best way to do so, 
but we could include the data from more countries and plot a similar result.)
import matplotlib.pyplot as plt  
import csv 
import operator 
import datetime as dt  
with open('/Users/kvenkatr/python/ebola.csv', 'rt') as f: 
  filtereddata = [row for row in csv.reader(f) if row[3] != "0.0" and 
  row[3] != "0" and "deaths" in row[0]] 
sorteddata = sorted(filtereddata, key=operator.itemgetter(1))  
guineadata  = [row for row in sorteddata if row[1] == "Guinea" and 

Data Analysis and Visualization
[ 534 ]
  row[0] == "Cumulative number of confirmed, probable and suspected 
Ebola deaths"] 
sierradata  = [row for row in sorteddata if row[1] == "Sierra Leone" 
and 
  row[0] == " Cumulative number of confirmed, probable and suspected 
Ebola deaths "] 
liberiadata = [row for row in sorteddata if row[1] == "Liberia" and 
  row[0] == " Cumulative number of confirmed, probable and suspected 
Ebola deaths "] 
g_x = [dt.datetime.strptime(row[2], '%Y-%m-%d').date() for 
  row in guineadata] 
g_y = [row[3] for row in guineadata] 
s_x = [dt.datetime.strptime(row[2], '%Y-%m-%d').date() for 
  row in sierradata] 
s_y = [row[3] for row in sierradata] 
l_x = [dt.datetime.strptime(row[2], '%Y-%m-%d').date() for
  row in liberiadata] 
l_y = [row[3] for row in liberiadata] 
plt.figure(figsize=(10,10))
plt.plot(g_x,g_y, color='red', linewidth=2, label="Guinea") 
plt.plot(s_x,s_y, color='orange', linewidth=2, label="Sierra Leone") 
plt.plot(l_x,l_y, color='blue', linewidth=2, label="Liberia")
plt.xlabel('Date', fontsize=18) 
plt.ylabel('Number of Ebola Deaths', fontsize=18) 
plt.title("Probable and Suspected Ebola Deaths", fontsize=20) 
plt.legend(loc=2) 
 
plt.show()

Chapter 2
[ 535 ]
The plot should look like this:
A sports example
To illustrate another example, and how a specific visualization method works 
better than another, let us consider a different question: What are the top five record 
touchdowns by quarterbacks in American Football as of Feb 2015? The original source of 
data for this are the Len Dawson NFL and AFL Statistics. (Data source: http://www.
pro-football-reference.com/players/D/DawsLe00.htm.)

Data Analysis and Visualization
[ 536 ]
The data contains information about the top 22 quarterbacks: Peyton Manning, Brett 
Favre, Dan Marino, Drew Brees, Tom Brady, Frank Tarkenton, John Elway, Warren 
Moon, John Unitas, Vinny Testaverda, Joe Montana, Dave Krieg, Eli Manning, Sonny 
Jurgensen, Dan Fouts, Philip Rivers, Ben Roethlisberger, Drew Bledsoe, Boomer 
Esiason, John Hadle, Tittle, and Tony Romo:
Before we think of a visualization method, a little bit of analysis needs to be done. 
These quarterbacks had played in different time periods. For example, Brett Favre 
played from 1991 to 2010, and Dan Marino played from 1983 to 1999. The challenge 
is that if we use a bar graph or a bubble chart, they will show the results in only  
one dimension.
The first step is to parse the CSV file, and we have several options here. We can 
either use the pandas read_csv function or the csv module, which has some 
convenient functions such as DictReader:
import csv 
import matplotlib.pyplot as plt
# csv has Name, Year, Age, Cmp, Att, Yds, TD, Teams

Chapter 2
[ 537 ]
with open('/Users/MacBook/java/qb_data.csv') as csvfile: 
   reader = csv.DictReader(csvfile) 
   for row in reader:
     name  = row['Name']
     tds = row['TD']
The quarterback data was downloaded from the source listed previously in this 
section; the filtered data is also available at http://www.knapdata.com/python/qb_
data.csv. The csv module includes classes for working with rows as dictionaries 
so that the fields can be named. The DictReader and DictWriter classes translate 
the rows to dictionaries instead of lists. Keys for the dictionary can be passed in or 
inferred from the first row in the input (where the row contains headers). Reading 
the contents of the CSV file is achieved via DictReader, where the column input 
values are treated as strings:
#ways to call DictReader
# if fieldnames are Name, Year, Age, Cmp, Att, Yds, TD, Teams
fieldnames = ['Name', 'Year', 'Age', 'Cmp', 'Att', 'Yds', 'TD', 
'Teams'] 
reader = csv.DictReader(csvfile, fieldNames=fieldnames)
# If csv file has first row as Name, Year, Cmp, Att, Yds, TD, Teams
#   we don't need to define fieldnames, the reader automatically 
recognizes
#   them. 
In order to convert some of the values to a number, we may need a function 
that converts and returns a numeric value. We have also added functions like 
getcolors() and num() in prepare.py, which can be used in future examples:
# num(s) and getcolors() functions 
def num(s):
  try:
    return int(s)
  except ValueError:
    return 0  
def getcolors():
  colors = [(31, 119, 180), (255,0,0), (0,255,0), (148, 103, 189), 
(140, 86, 75), (218, 73, 174), (127, 127, 127), (140,140,26), (23, 
190, 207), (65,200,100), (200, 65,100), (125,255,32), (32,32,198), 
(255,191,201), (172,191,201), (0,128,0), (244,130,150), (255, 
127, 14), (128,128,0), (10,10,10), (44, 160, 44), (214, 39, 40), 
(206,206,216)]

Data Analysis and Visualization
[ 538 ]
  for i in range(len(colors)):
    r, g, b = colors[i]
    colors[i] = (r / 255. , g / 255. , b / 255.)
  return colors
Visually representing the results
Based on the field names in the input data, for every quarterback, their touchdown 
statistics or passing-yards statistics can be plotted on a timeline. Now that we know 
what to plot, the next step is to figure out how to plot it.
Simple X-Y plots with the fields (year, touchdown) or (touchdown, year) should be 
a good start. However, there are 252 quarterbacks so far in this input data file, and a 
majority of them are not relevant. Therefore, showing them all with different colors 
would not make sense. (Why? Do we have 252 different colors?) We can attempt to 
plot the top 7 or top 10 results, as seen in the following image:

Chapter 2
[ 539 ]
The following Python program demonstrates how one can use matplotlib to 
display the top 10 quarterbacks in terms of the number of touchdowns and the plot 
produced by this program is shown in the preceding image:
import csv
import matplotlib.pyplot as plt
# The following functions can be in separate file 
#  (If it does, you need to import) 
def num(s):
  try:
    return int(s)
  except ValueError:
    return 0  
def getcolors():
  colors = [(31, 119, 180), (255,0,0), (0,255,0), (148, 103, 189), 
(140, 86, 75), (218, 73, 174), (127, 127, 127), (140,140,26), (23, 
190, 207), (65,200,100), (200, 65,100), (125,255,32), (32,32,198), 
(255,191,201), (172,191,201), (0,128,0), (244,130,150), (255, 
127, 14), (128,128,0), (10,10,10), (44, 160, 44), (214, 39, 40), 
(206,206,216)]
  for i in range(len(colors)):
    r, g, b = colors[i]
    colors[i] = (r / 255. , g / 255. , b / 255.)
  return colors
def getQbNames():
  qbnames = ['Peyton Manning']
  name=''
  i=0
  with open('/Users/MacBook/java/qb_data.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
      if ( row['Name'] != name and qbnames[i] != row['Name']):
        qbnames.append(row['Name'])
        i = i+1
  return qbnames
def readQbdata():
  resultdata = []
  with open('/Users/MacBook/java/qb_data.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    resultdata = [row for row in reader]

Data Analysis and Visualization
[ 540 ]
  return resultdata
fdata=[]
prevysum=0
#    -- functions End -- 
qbnames = getQbNames()
fdata = readQbdata()
i=0
rank=0
prevysum=0
lastyr=0
highrank=244
colorsdata = getcolors() 
fig = plt.figure(figsize=(15,13))
ax=fig.add_subplot(111,axisbg='white')
# limits for TD
plt.ylim(10, 744) 
plt.xlim(1940, 2021)
colindex=0
lastage=20
for qbn in qbnames:
  x=[]
  y=[]
  prevysum=0
  for row in fdata: 
    if ( row['Name'] == qbn and row['Year'] != 'Career'):
      yrval = num(row['Year'])
      lastage = num(row['Age'])
      prevysum += num(row['TD'])
      lastyr = yrval
      x += [yrval]
      y += [prevysum]
  if ( rank > highrank):
    plt.plot(x,y, color=colorsdata[colindex], label=qbn, 
linewidth=2.5)
    plt.legend(loc=0, prop={'size':10}) 

Chapter 2
[ 541 ]
    colindex = (colindex+1)%22
    plt.text(lastyr-1, prevysum+2, qbn+"("+str(prevysum)+"):" 
+str(lastage), fontsize=9)
  else:
    plt.plot(x,y, color=colorsdata[22], linewidth=1.5) 
    rank = rank +1 
plt.xlabel('Year', fontsize=18) 
plt.ylabel('Cumulative Touch Downs', fontsize=18) 
plt.title("Cumulative Touch Downs by Quarter Backs", fontsize=20)
plt.show() 
When the plot (X,Y) is switched to (Y,X), there is enough room to display the 
quarterback names. In the preceding code snippet, we might have to make the 
following change:

Data Analysis and Visualization
[ 542 ]
If we flip the x and y axes, then there is more room to display the name of the 
quarterback and the total touchdown score, as shown in the preceding plot. In order 
to accomplish this, one may have to switch x and y, and have the label text properly 
positioned according to the new x and y axes.
plt.xlim(10, 744)  
plt.ylim(1940, 2021)
# remaining code all un-changed except
y += [num(row['Year'])]
x += [prevysum]
# Don't forget to switch the x,y co-ordinates of text display
plt.text(prevysum+2, lastyr-1, qbn+"("+str(prevysum)+"):" 
str(lastage), fontsize=9)
At first glance, we can only make out the quarterbacks who are leading in the 
number of touchdown scores in their career (as of the 2014-2015 football season). 
Based on this visualization, you can further try to analyze and understand what 
else can we infer from this data. The answer to this is based on the answers of the 
following questions:
•	
Which quarterback has played the longest in their career?
•	
Are there any quarterbacks today who can surpass Peyton Manning's 
touchdown records?
Among the fields that we read from the input file, Age happens to be one of the field 
values that we have. There are many ways to experiment with the starting value of 
Age that can be used to plot the Age versus Touchdown statistics. To answer the first 
question, we have to keep track of Age instead of Year. The following snippet can  
be either used in a separate function (if one has to use it often), or included in the 
main script:
maxage = 30
with open('/Users/MacBook/java/qb_data.csv') as csvfile:
  reader = csv.DictReader(csvfile)
    for row in reader:
      if ( num(row['Age']) > maxage ):
        maxage = num(row['Age']) 
print maxage 

Chapter 2
[ 543 ]
Running the preceding block of code shows 44 as the maximum age of a quarterback 
(when actively played in the league, and there were three such quarterbacks: Warren 
Moon, Vinny Testaverde, and Steve DeBerg. Technically, George Blanda played until 
he was 48 (which is the maximum age as a player), but he started as quarterback and 
was also a kicker for some years).
In order to answer the other question, we plot the touchdown statistics against the 
quarterback age, as follows:
import csv
import matplotlib.pyplot as plt
# The following functions can be in a separate file
#    -- functions Begin -- 
def num(s):
  try:
    return int(s)
  except ValueError:
    return 0  
def getcolors():
  colors = [(31, 119, 180), (255,0,0), (0,255,0), (148, 103, 189), 
(140, 86, 75), (218, 73, 174), (127, 127, 127), (140,140,26), (23, 
190, 207), (65,200,100), (200, 65,100), (125,255,32), (32,32,198), 
(255,191,201), (172,191,201), (0,128,0), (244,130,150), (255, 
127, 14), (128,128,0), (10,10,10), (44, 160, 44), (214, 39, 40), 
(206,206,216)]
  for i in range(len(colors)):
    r, g, b = colors[i]
    colors[i] = (r / 255. , g / 255. , b / 255.)
  return colors
def getQbNames():
  qbnames = ['Peyton Manning']
  name=''
  i=0
  with open('/Users/MacBook/java/qb_data.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
      if ( row['Name'] != name and qbnames[i] != row['Name']):
        qbnames.append(row['Name'])

Data Analysis and Visualization
[ 544 ]
        i = i+1
  return qbnames
def readQbdata():
  resultdata = []
  with open('/Users/MacBook/java/qb_data.csv') as csvfile:
    reader = csv.DictReader(csvfile)
    resultdata = [row for row in reader]
  return resultdata
fdata=[]
prevysum=0
#    -- functions End -- 
qbnames = getQbNames()
fdata = readQbdata()
i=0
rank=0
prevysum=0
lastyr=0
highrank=244
colorsdata = getcolors() 
fig = plt.figure(figsize=(15,13))
ax=fig.add_subplot(111,axisbg='white')
# limits for TD
plt.ylim(10, 744)
#change xlimit to have age ranges 
plt.xlim(20, 50)
colindex=0
lastage=20
for qbn in qbnames:
  x=[]
  y=[]
  prevysum=0
  for row in fdata: 
    if ( row['Name'] == qbn and row['Year'] != 'Career'):

Chapter 2
[ 545 ]
      yrval = num(row['Year'])
      lastage = num(row['Age'])
      prevysum += num(row['TD'])
      lastyr = yrval
      x += [lastage]
      y += [prevysum]
  if ( rank > highrank):
    if ( lastage == 44):
      plt.plot(x,y, color='red', label=qbn, linewidth=3.5)
    else:
      plt.plot(x,y, color=colorsdata[colindex], label=qbn, 
linewidth=2.5)
      plt.legend(loc=0, prop={'size':10}) 
    colindex = (colindex+1)%22
    plt.text(lastage-1, prevysum+2, qbn+"("+str(prevysum)+"):" 
+str(lastage), fontsize=9)
  else:
    if ( lastage == 44):
      plt.plot(x,y, color='red', label=qbn, linewidth=3.5)
      plt.text(lastage-1, prevysum+2, qbn+"("+str(prevysum)+"):" 
+str(lastage), fontsize=9)
    else:         
      plt.plot(x,y, color=colorsdata[22], linewidth=1.5) 
    rank = rank +1 
plt.xlabel('Age', fontsize=18) 
plt.ylabel('Number of Touch Downs', fontsize=18) 
plt.title("Touch Downs by Quarter Backs by Age", fontsize=20)
plt.show() 

Data Analysis and Visualization
[ 546 ]
When you take a look at the plotting results, only two quarterback results are 
comparable to Peyton Manning at the age of 35, which are Drew Brees and Tom 
Brady. However, given the current age of Tom Brady and his accomplishments so 
far, it appears that only Drew Brees has a better probability of surpassing Peyton 
Manning's touchdown records.

Chapter 2
[ 547 ]
This conclusion is shown in the following image with a simpler plot for data based 
on the age of 35. Comparing the top four quarterback results—Peyton Manning, 
Tom Brady, Drew Brees, and Brett Favre—we see that Drew Brees's achievement at 
the age of 35 is comparable to that of Peyton at the same age. Although the write-up 
by NY Times with the title Why Peyton Manning's record will be hard to beat concludes 
differently, the following plot, at least, is inclined towards the possibility that Drew 
Brees might beat the record:

Data Analysis and Visualization
[ 548 ]
Creating interesting stories with data
Data visualization regularly promotes its ability to reveal stories with data, and in 
some cases, reveal the not so trivial stories visually. In the recent past, journalists 
have been integrating visualizations more into their narratives, often helping us 
better understand their stories. In the commercial world, there are few that grasp 
the ways in which data can be associated with a meaningful story that appeals both 
emotionally and intelligently to the audience. As Rudyard Kipling wrote, If history 
were taught in the form of stories, it would never be forgotten; a similar thought applies 
to data. We should, therefore, understand that data would be understood and 
remembered better if presented in the right way.
Why are stories so important?
There are many tools and methods of visualization that we have today: bar and pie 
charts, tables, line graphs, bubble charts, scatter plots, and so on—the list is long. 
However, with these tools, the focus is on data exploration, and not on aiding a 
narrative. While there are examples of visualizations that do help tell stories, they are 
rare. This is mainly because finding the story is significantly harder than crunching the 
numbers. There are reader-driven narratives and author-driven narratives as well.
An author-driven narrative has data and visualization that are chosen by the author 
and presented to the public reader. A reader-driven narrative, on the other hand, 
provides tools and methods for the reader to play with the data, which gives the 
reader more flexibility and choices to analyze and understand the visualization.
Reader-driven narratives
In 2010, researchers at Stanford University studied and reviewed the emerging 
importance of storytelling and suggested some design strategies for narrative 
visualization. According to their study, a purely author-driven approach has a strict 
linear path through the visualization, relies on messaging, and has no interactivity, 
whereas a reader-driven approach has no prescribed ordering of images, no 
messaging, and has a  high degree of interactivity. An example of the author-driven 
approach is a slideshow presentation. The seven narratives of visualization listed by 
that study include magazine style, annotated chart, partitioned poster, flow chart, 
comic strip, slideshow, and a film/video/animation.

Chapter 2
[ 549 ]
Gapminder
A classic example of a reader-driven narrative combined with a data-driven one 
is Gapminder World (http://gapminder.org/world). It has a collection of over 
600 data indicators in international economy, environment, health, technology, and 
much more. It provides tools that students can use to study real-world issues and 
discover patterns, trends, and correlations. This was developed by the Trendalyzer 
software that was originally developed by Hans Rosling's foundation in Sweden, and 
later acquired by Google in March 2007.
The information visualization technique used by Gapminder is an interactive 
bubble chart with the default set to five variables: X, Y, bubble size, color, and a 
time variable that is controlled by a slider. This sliding control and the selection of 
what goes along the X and Y axes makes it interactive. However, creating a story, 
even with a tool like this, is not necessarily easy. Storytelling is a craft and can be an 
effective knowledge-sharing technique, because it conveys the context and emotional 
content more effectively than most other modes of communication.

Data Analysis and Visualization
[ 550 ]
The most attractive storytellers grasp the importance of understanding the audience. 
They might tell the same story to a child and an adult, but the articulation would be 
different. Similarly, a data-driven or reader-driven story should be adjusted based on 
who is listening or studying it. For example, to an executive, statistics are likely the 
key, but a business intelligence manager would most likely be interested in methods 
and techniques.
There are many JavaScript frameworks that are available today for creating 
interactive visualization, and the most popular one is D3.js. Using Python, there are 
only a few ways today in which one can create an interactive visualization (without 
using Flash). One way is by generating the data in the JSON format that D3.js can 
use to plot, and the second option is to use Plotly (http://www.plot.ly). We will go 
into more detail about Plotly in the concluding section of this chapter.
The State of the Union address
Twitter has created a visualization from the tweets during President Obama's speech 
that graphs the tweets by location and topic. This visualization is interesting because 
it captures a lot of details in one place. Scroll through the speech to see how Twitter 
reacted; it is posted at http://twitter.github.io/interactive/sotu2015/#p1.

Chapter 2
[ 551 ]
Mortality rate in the USA
The mortality rate in the USA fell by about 17 percent from 1968 to 2010, years for 
which we have detailed data (from http://www.who.int/healthinfo/mortality_
data/en/). Almost all of this improvement can be attributed to improved survival 
prospects for men. It looks like progress stopped in the mid 1990s, but one of the 
reasons may be that the population has aged a lot since then. One may read a 
complete description of this from Bloomberg, but here we attempt to display two 
visualizations:
•	
Mortality rate during the period 1968-2010 among men, women,  
and combined
•	
Mortality rate for seven age groups to show some interesting results

Data Analysis and Visualization
[ 552 ]
The code for this example is as follows:
import csv
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(15,13))
plt.ylim(740,1128)
plt.xlim(1965,2011)
# Data from http://www.who.int/healthinfo/mortality_data/en/
with open('/Users/MacBook/Downloads/mortality1.csv') as csvfile:
  mortdata = [row for row in csv.DictReader(csvfile)]
x=[]
males_y=[]
females_y=[]
every_y=[]
yrval=1968
for row in mortdata:
  x += [yrval]
  males_y += [row['Males']]
  females_y += [row['Females']]
  every_y += [row['Everyone']]
  yrval = yrval + 1
plt.plot(x, males_y, color='#1a61c3', label='Males', linewidth=1.8)
plt.plot(x, females_y, color='#bc108d', label='Females', 
linewidth=1.8)
plt.plot(x, every_y, color='#747e8a', label='Everyone', linewidth=1.8)
plt.legend(loc=0, prop={'size':10})
plt.show()

Chapter 2
[ 553 ]
The mortality rates were measured per 100,000 people. By dividing the population 
into separate age cohorts, the improvements in life expectancy are shown to have been 
ongoing, particularly showing most progress for the age group below 25. What exactly 
happened to the population falling under the age group of 25-44 (shown in red)? The 
narrative on Bloomberg lays out the reason very well by connecting another fact that 
the number of deaths caused by AIDS had an effect on that age group during that time.
AIDS killed more than 40,000 Americans a year and 75 percent of them were in  
the age group of 25-44. Therefore, the unusual results are seen during that window  
of time.
import csv
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(15,13))
plt.ylim(35,102)

Data Analysis and Visualization
[ 554 ]
plt.xlim(1965,2015)
colorsdata = ['#168cf8', '#ff0000', '#009f00', '#1d437c', '#eb912b', 
'#8663ec', '#38762b']
labeldata = ['Below 25', '25-44', '45-54', '55-64', '65-74', '75-84', 
'Over 85']
# using reader() instead of DictReader() so that we could loop to 
# build y-values in list
with open('/Users/MacBook/Downloads/mortality2.csv') as csvfile:
  mortdata = [row for row in csv.reader(csvfile)]
x=[]
for row in mortdata:
  yrval = int(row[0])
  if ( yrval == 1969 ):
    y = [[row[1]],[row[2]],[row[3]],[row[4]],[row[5]],[row[6]],[r
ow[7]]]
  else:
   for col in range(0,7):
     y[col] += [row[col+1]]
  x += [yrval]
for col in range(0,7):
  if ( col == 1 ):
    plt.plot(x, y[col], color=colorsdata[col], label=labeldata[col], 
linewidth=3.8)
  else:
    plt.plot(x, y[col], color=colorsdata[col], label=labeldata[col], 
linewidth=2)
plt.legend(loc=0, prop={'size':10})
plt.show()
The difference between csv.reader() and csv.DictReader() is that when the 
input CSV file has fieldnames (or column names), DictReader() uses the fieldnames 
as keys and the actual value in that column as the data value. In the preceding 
example, we have used reader(), because it is convenient when there is looping 
involved ( y[col] = [row[col+1]] ). Moreover, with reader(), if the column 
names exist in the CSV file, that first row should be ignored.
We have also made filtered data for both these examples available as mortality1.
csv and mortality2.csv at http://www.knapdata.com/python.

Chapter 2
[ 555 ]
For mortdata[:4], the result would be different in each of these methods of  
reading. In other words, the result of mortdata[:4] when we use reader()  
will be as follows:
[['1969', '100', '99.92', '97.51', '97.47', '97.54', '97.65', 
'96.04'],  ['1970', '98.63', '97.78', '97.16', '97.32', '96.2', 
'96.51', '83.4'],  ['1971', '93.53', '95.26', '94.52', '94.89', 
'93.53', '93.73', '89.63'],  ['1972', '88.86', '92.45', '94.58', 
'95.14', '94.55', '94.1', '89.51']] 
With DictReader(), assuming that the CSV file has fieldnames, the four rows will 
be displayed as follows:
[{'25-44': '99.92', '45-54': '97.51', '55-64': '97.47', '65-74': 
'97.54', '75-84': '97.65', 'Below 25': '100', 'Over 85': '96.04', 
'Year': '1969'},
 {'25-44': '97.78', '45-54': '97.16', '55-64': '97.32', '65-74': 
'96.2', '75-84': '96.51', 'Below 25': '98.63', 'Over 85': '83.4', 
'Year': '1970'},
 {'25-44': '95.26', '45-54': '94.52', '55-64': '94.89', '65-74': 
'93.53', '75-84': '93.73', 'Below 25': '93.53', 'Over 85': '89.63', 
'Year': '1971'},
 {'25-44': '92.45', '45-54': '94.58', '55-64': '95.14', '65-74': 
'94.55', '75-84': '94.1', 'Below 25': '88.86', 'Over 85': '89.51', 
'Year': '1972'}]
A few other example narratives
There are numerous examples that one can explore, visualize, interact and play with. 
Some notable ones are as follows:
•	
How the recession reshaped the economy in 255 charts (NY Times): This 
narrative shows how, in five years since the end of the Great Recession, the 
economy regained the lost nine million jobs, highlighting which industries 
recovered faster than others. (Source: http://tinyurl.com/nwdp3pp.)
•	
Washington Wizards shooting stars of 2013 (Washington Post): This 
interactive graphic was created a few years ago based on the performance of 
the Washington Wizards in 2013, trying to analyze and see how the signing 
of Paul Pierce could bring much improved shooting from the mid-range. 
(Source: http://www.washingtonpost.com/wp-srv/special/sports/
wizards-shooting-stars/.)

Data Analysis and Visualization
[ 556 ]
Author-driven narratives
The New York Times produces some of the world's best data visualization, 
multimedia, and interactive stories. Their ambition for these projects has always been 
to meet the journalistic standards at a very prestigious level and to create genuinely 
new kinds of experiences for the readers. The storytelling culture among them is one 
of the sources of energy behind this work.
For example, there is a combination of data and author-driven narrative titled The 
Geography of Chaos in Yemen. On March 26, 2015, Saudi Arabian jets struck targets 
in Yemen in a drive against the Houthi rebel group. Yemen plays an important role 
for the key players such as Saudi Arabia, Iran, and the United States. The Houthis' 
influence has grown over the past years, which was captured visually by the authors 
at the NY Times.

Chapter 2
[ 557 ]
Yemen is home to one of Al Qaeda's most active branches in the Arabian Peninsula. 
Since 2009, the United States has carried out at least 100 airstrikes in Yemen. In addition 
to Al Qaeda's occupation, the Islamic State also has activities in that region, and 
recently, they claimed responsibility for the bombings at two Shiite mosques in Sana 
that killed more than 135 people. The following visualization comes from The Bureau 
of Investigative Journalism, American Enterprise Institute's Critical Threat Project:
Another good example is the visualization of the Atlantic's past by David McCandless, 
which shows what the oceans were like before over-fishing. It is hard to imagine the 
damage that over-fishing is wreaking on the oceans. The effects are invisible, hidden 
in the ocean. The following image shows the biomass of the popularly-eaten fish in 
the North Atlantic Ocean in 1900 and 2000. The popularly-eaten fish include tuna, cod, 
haddock, hake, halibut, herring, mackerel, pollock, salmon, sea trout, striped bass, 
sturgeon, and turbot, many of which are now vulnerable or endangered.

Data Analysis and Visualization
[ 558 ]
Dr. Villy Christensen and his colleagues at the University of British Columbia used 
ecosystem models, underwater terrain maps, fish-catch records, and statistical 
analysis to render the biomass of the Atlantic fish at various points in this century.
Perception and presentation methods
In the past, data size and variety did not impose much of a challenge; therefore, 
perceiving and analyzing data was straightforward. Today there are large quantities 
of data in innumerable fields, and visualization can provide valuable assistance 
to humans for perceiving and interacting with visualization of the data. Human 
factors contribute significantly to the whole visualization process in order to better 
understand data and aid in decision-making tasks.
Visualization techniques can be categorized into two areas:
•	
Scientific visualization: This involves scientific data with an inherent 
physical entity
•	
Information visualization: This involves abstract data (spatial or non-spatial)

Chapter 2
[ 559 ]
Most visualization systems are designed so that humans and computers can 
cooperate, each performing the following tasks:
•	
Visually representing data to enhance data analysis
•	
Visually displaying models, interpretations of data, ideas, hypotheses,  
and insight
•	
Helping users to improve their models by finding either supporting or 
contradictory evidence for their hypotheses
•	
Helping users to organize and share their ideas
New insights into visual perception are arising from work in various disciplines 
besides information visualization, such as human factors and human-computer 
interaction. One of the great strengths of data visualization is our ability to process 
visual information much more rapidly than verbal information. Psychologists 
studied perceptual organization during the 1920s in Germany, and the first group of 
them was the Gestalt Theorists.
The Gestalt principles of perception
The word Gestalt means "organized whole" or, in other words, when parts identified 
individually have different characteristics to the whole. For example, for describing 
a tree, you can say that it has different parts such as the trunk, leaves, branches, fruit 
(in some cases). However, when we look at an entire tree, we are not conscious of the 
parts, but aware of the whole object—in this case, the tree.
The principles of Gestalt perception are as follows:
•	
Proximity: Objects that are close together or connected to each other are 
perceived as a group, reducing the need to process smaller objects separately.

Data Analysis and Visualization
[ 560 ]
•	
Similarity: Objects that share similar attributes, color, or shape are perceived 
as a group.
•	
Common fate: When both the principles of proximity and similarity are in 
place, a movement takes place. Then they appear to change grouping.
•	
Good continuation: Some things are important as a whole, which means 
if there are interruptions, then it changes the perceptive reading. In the 
following image, we perceive the two crossed lines instead of four lines 
meeting at the center:
•	
Closure: Even if a part of the border of a shape is missing, we still tend to see 
the shape as completely enclosed by the border and ignore the gaps.

Chapter 2
[ 561 ]
It is very useful to know these principles for creating any visualization method.
Let's elaborate this further with an example. Proximity refers to the visual approach 
of grouping shapes together if they appear similar to each other. Such a group is 
usually perceived as a single unit. For instance, the following image shows how one 
can distinguish proximity:
Some best practices for visualization
The first important step one can take to make a great visualization is to know what is 
the goal behind the effort. How does one know if the visualization has a purpose? It 
is also very important to know who the audience is and how this will help them.
Once the answers to these questions are known, and the purpose of visualization is 
well understood, the next challenge is to choose the right method to present it. The 
most commonly-used types of visualization could further be categorized according 
to the following:
•	
Comparison and ranking
•	
Correlation
•	
Distribution
•	
Location-specific or geodata
•	
Part-to-whole relationships
•	
Trends over time

Data Analysis and Visualization
[ 562 ]
Comparison and ranking
Comparing and ranking can be done in more than one way, but the traditional way 
is by using bar charts. A bar chart is believed to encode quantitative values as length 
on the same baseline. However, it is not always the best way to display comparison 
and rankings. For instance, to display the top 12 countries in Africa by GDP, the 
following presentation is a creative way to visualize (courtesy: Stats Legend, Andrew 
Gelman and Antony Unwin):
Correlation
A simple correlation analysis is a great place to start for identifying the relationships 
between measures, although correlation doesn't guarantee a relationship. To confirm 
that the relationship truly exists, a statistical methodology is often required. The 
following is an example to build a simple scatter plot to detect the correlations 
between two factors, say gpa and tv or gpa and exercise among the students from 
a university:

Chapter 2
[ 563 ]
However, we can use other ways in order to display the correlation matrix. For 
instance, one can use scatter plots, heat maps, or some specific example to show 
the influence network amongst stocks in the S&P 100. (The following two plots are 
taken from Statistical Tools for High Throughput Analysis at http://www.sthda.com.) 
To emphasize further, a correlation matrix involves data in a matrix form. The data 
is correlated by using a scaled color map, as shown in the following examples. For 
more details, we suggest you to refer to the site, http://www.sthda.com.
A correlation matrix is used to investigate the dependence between multiple 
variables at the same time. The result is a table containing the correlation coefficients 
between each variable and the others. Heat maps originated in 2D display of the 
values in a data matrix. There are many different color schemes that can be used to 
illustrate the heat map, with perceptual advantages and disadvantages for each.

Data Analysis and Visualization
[ 564 ]
Distribution
A distribution analysis shows how the quantitative values are distributed across 
their range, and is therefore, extremely useful in data analysis. For example, compare 
the grade distribution of homework the midterm, the final exam, and the total 
course grade of a class of students. In this example, we will discuss two of the most 
commonly used chart types for this purpose. One is a histogram (as shown in the 
following image), and the other is a box plot or box-and-whisker plot.

Chapter 2
[ 565 ]
The shape of a histogram depends strongly on the specified bin size
and locations. The box-and-whisker plots are excellent for displaying multiple 
distributions. They pack all the data points—in this case, grades per student—into  
a box-and-whisker display. Now you can easily identify the low values, the  
25th-percentile values, the medians, the 75th-percentiles, and the maximum values 
across all categories—all at the same time.
One of the many ways to conveniently plot these in Python is by using Plotly, 
which is an online analytics and visualization tool. Plotly provides online graphing, 
analytics, and statistics tools as well as scientific plotting libraries for Python, R, 
Julia, and JavaScript. For examples of histograms and box-and-whisker plots, refer to 
https://plot.ly/python/histograms-and-box-plots-tutorial.

Data Analysis and Visualization
[ 566 ]
Location-specific or geodata
Maps are the best way to display data that is location-specific. Maps are best used 
when paired with another chart that details what the map is displaying (such as a 
bar chart sorted from greatest to least, line chart showing the trends, and so on).  
For example, the following map shows the intensity of an earthquake compared 
across continents:

Chapter 2
[ 567 ]
Part-to-whole relationships
Pie charts are known to be common for displaying part-to-whole relationships, 
but there are other ways to do it. Grouped bar charts are good for comparing 
each element in the categories with the others, and for comparing elements across 
categories. However, grouping makes it harder to tell the difference between the 
total of each group. This is where the stacked column charts come in.
The stacked column charts are great for showing the total because they visually 
aggregate all the categories in a group. The downside is that it becomes harder to 
compare the sizes of the individual categories. Stacking also indicates a part-to-whole 
relationship.

Data Analysis and Visualization
[ 568 ]
Trends over time
One of the most frequently used visualization methods to analyze data is to display 
a trend over a period of time. In the following example, the investment in wearables 
startups from 2009-2015 has been plotted. It shows that the investment in wearables 
has been on the rise for a few years; activity shot through the roof in 2014, with 61 
completed deals totaling $427 million, when compared to 43 deals worth only $166 
million in 2013 (just a year earlier).
With this observation, it will be interesting to see how the marketplace evolves over 
the coming years.
Visualization tools in Python
Analyzing and visualizing data requires several software tools: a text editor to write 
the code (preferably syntax highlighted), Python and additional libraries to run and 
test the code, and perhaps a tool to present the results. There are two categories of 
software tools: general-purpose software tools and specific software components.

Chapter 2
[ 569 ]
Development tools
The general-purpose software tool is an integrated development environment 
(IDE), which is an application that has all the productivity tools within one package. 
These IDEs are usually very convenient from the standpoint of handling the Python 
libraries. More details about these IDE tools will be discussed in the following 
chapter. In this chapter, we'll limit our discussion to a brief introduction to Canopy 
from Enthought and Anaconda from Continuum Analytics.
The specific software component are Python plotting libraries such as Bokeh, 
IPython, matplotlib, NetworkX, SciPy and NumPy, Scikit-learn, and Seaborn. 
Both the IDEs have a very convenient way to handle the adding, removing, and 
updating to later versions of these plotting libraries.
Canopy from Enthought
Enthought Canopy has a free version that is released under the BSD-style license, 
and comes with GraphCanvas, SciMath, and Chaco as plotting tools, among 
several other libraries. It has an advanced text editor, integrated IPython console, 
graphics package manager, and online documentation links. The Canopy analysis 
environment streamlines data analysis, visualization, algorithm design, and 
application development for scientists, engineers, and analysts.

Data Analysis and Visualization
[ 570 ]
Anaconda from Continuum Analytics
Anaconda IDE is based on the conda application. Conda is an application for finding 
and installing software packages. A conda package is a binary tarball containing 
system-level libraries, Python modules, executable programs, or other components. 
Conda keeps track of the dependencies between packages and platform specifics, 
making it simple to create working environments from different sets of packages.
Anaconda has sypder-app, a scientific Python development environment, which  
has an IPython viewer as well. In addition to this, IPython can be launched as a GUI 
or a web-based notebook. The most convenient thing is that you can install Python in 
the home directory without touching the system-installed Python. Not all packages 
are ready to work with Python 3 as yet; therefore, it is better to use Python 2 with 
these IDEs.
IPython (http://ipython.scipy.org/) provides an enhanced, interactive Python 
shell, and is highly recommended mostly because data analysis and visualization 
is interactive in nature. IPython is supported on most platforms. Some additional 
features that come with IPython are as follows:
•	
Tab completion: This involves completion of variables, functions, methods, 
attributes, and filenames. Tab completion is achieved with the GNU Readline 
library (http://tiswww.case.edu/php/chet/readline/rltop.html) and 
is highly addictive. It is very hard to go back to a regular command-line 
interface after you are exposed to GNU Readline.
•	
Command history capabilities: This issues the command history for a full 
account of the previously used commands.

Chapter 2
[ 571 ]
Interactive visualization
For a visualization to be considered interactive, it must satisfy two criteria:
•	
Human input: The control of some aspect of the visual representation of 
information must be available to humans
•	
Response time: The changes made by humans must be incorporated into the 
visualization in a timely manner
When large amounts of data must be processed to create a visualization, this 
becomes very hard, sometimes impossible, even with the current technology; 
therefore, "interactive visualization" is usually applied to systems that provide 
feedback to the users within several seconds of input. Many interactive visualization 
systems support a metaphor of navigation, analogous to navigation through the 
physical world.
The benefit of interaction is that people can explore a larger information space 
in a shorter time, which can be understood through one platform. However, a 
disadvantage to this interaction is that it requires a lot of time to exhaustively check 
every possibility to test the visualization system. Moreover, designing systems to 
guarantee immediate response to user actions can require significant algorithmic 
attention.
Any visualization method needs a good plan of layout. Some layout methods 
automatically lead to symmetrical drawings; alternatively, some drawing methods 
start by finding symmetries in the data. Interactive visualization is implemented 
using event listeners, and to some, this is well understood as common knowledge, 
but in any case, the following section describes what it is all about.
Event listeners
An event listener is a process that is used when a mouse is either moved or clicked.  
Technically, there are many kinds of events, but purely for interactive visualization, 
you need to only know what happens when the user navigates through the 
visualization with the mouse. The latency of interaction, that is, the time it takes for 
the system to respond to the mouse action, matters immensely.

Data Analysis and Visualization
[ 572 ]
The most obvious principle is that the user should indeed have some sort of 
confirmation that the action has completed, rather than being left dangling 
wondering whether the action is still in progress. Thus, feedback such as 
highlighting a selected item is a good way to confirm that the desired operation 
has completed successfully. Visual feedback should typically take place within the 
immediate response latency class of around one second. The following is an example 
of a JavaScript event listener in Google Charts:
chart = new google.visualization.PieChart(document.getElementById( 
'chart_div'));
google.visualization.events.addListener(chart, 'select', 
selectHandler);
chart.draw(data, options);
function selectHandler() {
  var selectedItem = chart.getSelection()[0];
  var value = data.getValue(selectedItem.row, 0);
  alert('The user selected ' + value);
}
Another principle is that if an action takes significantly longer than a user would 
naturally expect, some kind of progress indicator should be shown to the user. It is 
much easier to write event listeners in JavaScript, but in order to create an interactive 
visualization using plotting methods written in Python, one should use Plotly.
There is another module, graph-tool (https://graph-tool.skewed.de), that can 
be harnessed to perform animations in a straightforward manner. It uses GTK+ to 
display animations in an interactive window as well as off-screen to a file. The idea is 
to easily generate visualizations, which can be used in presentations and embedded 
in websites.
Layouts
In order to display data visually and efficiently, it is very important to know the 
layout methods. Aesthetics is one of the criteria that measures the strength and 
weakness of the layout algorithm. In order to make the layout results more readable, 
the structure needs to have either an hierarchy or a symmetry, if possible; one critical 
factor is the utilization of space.

Chapter 2
[ 573 ]
A good layout is essential for untangling and understanding any graphic. Generally, 
each layout is uniquely suited to different kinds of data visualization in order to be 
best understood. A few notable layout methods are as follows:
•	
Circular layout
•	
Radial layout
•	
Balloon layout
Circular layout
Tables are natural containers for data. Whenever information is presented, chances 
are very high that it is presented by means of a table. In many cases, however, when 
this information is complex (and the table, therefore, is large), a tabular presentation 
is difficult to parse visually and the patterns in the tabulated data remain opaque.
In other words, a useful container is not always a useful way to present data. The 
table presents individual data very well, but their inter-relationship and the patterns 
that they compose are hardly visible. A circular layout can use several different 
combinations (qualitative and quantitative) to be displayed in a single visualization 
as shown in the following image:

Data Analysis and Visualization
[ 574 ]
For instance, it is intuitive to display a complex relationship within a limited space as 
shown in the preceding image.
The preceding image shows an example of a complex hierarchical relationship 
displayed in a circular layout.
Radial layout
Sunburst visualization is a radial space-filling visualization technique for displaying 
tree-like structures (as shown in the preceding image). There are other space-filling 
visualization methods that use other visual encodings for describing hierarchies. For 
example, the treemap is a space-filling visualization that uses "containment" to show 
"parent-child" relationships. There are a couple of subtle changes that can improve 
the way the information is communicated by this visualization.
Since the length of each orbit increases with the radius, there tends to be more room 
for the nodes. A radial tree will spread the larger number of nodes over a larger area 
as the levels increase.

Chapter 2
[ 575 ]
Balloon layout
There are different variations to a balloon layout, and one may even view  
these as bubbles. However, if we use different colors and sizes of the balloons  
(or circles/bubbles), a lot more can be displayed in this visualization, as shown  
in the following image:

Data Analysis and Visualization
[ 576 ]
Summary
The principles of visualization methods are useful to follow for creating an effective 
story. The narratives explained in this chapter give an idea of aesthetics and the vast 
variation of approaches.
The goal of data visualization is to communicate information clearly and efficiently 
to the users, via the visual display method selected. Effective visualization helps 
in analyzing and reasoning about data and evidence. It makes complex data more 
accessible, understandable, and usable. Users may have particular analytical tasks, 
such as making comparisons or understanding causality, and the design principle of 
the graphic follows the task.
Tables are generally used where users will look up a specific measure of a variable, 
while charts of various types are used to show patterns or relationships in the data 
for one or more variables.
Data visualization is both an art and a science and it is like solving a mathematical 
problem. There is no one right way to solve it. Similarly, there is no one right way 
to create a visualization method. There are many tools out there for visualization, 
and we know a few tools that support Python. In the following chapter, more details 
about these tools will be discussed.

[ 577 ]
Getting Started with  
the Python IDE
Python is a widely used programming language that has been around for over 20 
years. Among many other things, this language is quite popular for its simplicity 
and dynamic typing. Type(datum) dynamically determines the type of the data object. 
It has a syntax that allows programmers to write a very few lines of code. Python 
supports multiple programming paradigms that include functional, object-oriented, 
and procedural styles.
Python interpreters are available on almost every operating system that is in use. 
Its built-in data structures combined with dynamic binding make it very attractive 
to use as a high performance language to connect the existing manipulative 
components quickly. Even in distributed applications, Python is used as a glue in 
conjunction with Hive (NoSQL) to accomplish something very quick and efficient. 
Python, which is powerful and popular in the software development community, 
needs an interactive environment to create, edit, test, debug, and run programs.
An integrated development environment (IDE) is a software application that 
provides a comprehensive and powerful set of tools to build applications for target 
systems that run Windows, Linux, or Mac OS operating systems. These tools provide 
a single and consistent integrated environment and are designed to maximize 
productivity. There are many choices of IDE for Python programming. The details 
will be discussed in the following section of this chapter. In addition, we will discuss 
the following topics:
•	
The IDE tools in Python
•	
The installation guide — instructions to download and install tools
•	
The conda command-line interface (CLI) and Spyder

Getting Started with the Python IDE
[ 578 ]
•	
The data visualization tools in the IDE tools that are specific to libraries that 
are useful for visualization
•	
Interactive visualization packages
•	
Some plotting examples using the IDE tools
The IDE tools in Python
Analyzing and visualizing data requires several software tools: a text editor to write 
code (preferably the syntax highlight), additional tools and libraries to run and test 
the code, and perhaps another set of tools to present the results. There are many 
advantages of an IDE. Some notable ones are as follows:
•	
The syntax highlight (showing errors or warnings right away)
•	
Stepping through code in the debug mode
•	
The interactive console
•	
Integration with the interactive graphic notebook (such as IPython)
Python 3.x versus Python 2.7
Python 3.x is not backward compatible with the 2.x version. This is why Python 2.7 
is still being used. In this book, we will use Python 2.7 and try not to focus on Python 
3.x. This issue is beyond the scope of this book, and we recommend that you search 
for information about how to write code that works with different versions. Some 
IDE tools have specific instructions to use both these versions. In some cases, the 
code may have to be written a little differently.
Types of interactive tools
Before discussing further about the Python IDEs, it is important to consider the 
different ways available to display interactive data visualization. There are many 
options to create interactive data visualization, but here, we will consider only two 
popular tools to accomplish this:
•	
IPython
•	
Plotly

Chapter 3
[ 579 ]
IPython
In the year 2001, Fernando Perez began working on IPython, an enhanced 
interactive Python shell with improvements, such as history caching, profiles, object 
information, and session logging. Originally focused on the interactive computing in 
Python, it later included Julia, R, Ruby, and so on. Some features—such as automatic 
parenthesizing and tab completion—are small timesavers and very productive in 
terms of usability. In standard Python, to do tab completion, you have to import a 
few modules, whereas IPython offers tab completion by default.
IPython provides the following rich set of tools for Python scripting:
•	
Terminal commands and Qt-based tools that are convenient
•	
An interactive environment that is purely a web-based notebook with the 
same core features as the standalone notebook; it also supports code, text, 
mathematical expressions, and inline plots
•	
A convenient interactive data visualization; this capability has been the 
reason for many IDEs having integrated support for IPython
•	
Easy-to-use and high-performance tools for multiprocess computing
The four most helpful commands for IPython with a brief description:
Command
Description
?
This specifies the introduction and overview of IPython's 
features
%quickref
This denotes quick reference
--help-all
This specifies Python's help
%who/%whos
This gives information about identifiers
The IPython notebook is a web-based interactive computational environment. Here, 
you can merge code, mathematics, and plotting into a single document.

Getting Started with the Python IDE
[ 580 ]
IPython (http://ipython.scipy.org/) provides an enhanced interactive Python 
shell and is highly recommended mostly because data analysis and visualization are 
interactive in nature. IPython is supported on most platforms. Some added features 
that come with IPython are:
•	
Tab completion: This involves the completion of variables, functions, 
methods, attributes, and filenames. Tab completion is achieved using the 
GNU Readline library (http://tiswww.case.edu/php/chet/readline/
rltop.html). It is very hard to go back to a regular command-line interface 
after you have been exposed to GNU Readline.
•	
Command history capabilities: This issues the command history for a full 
account of the previously used commands.
An example that was run on IPython is shown in the following screenshot. To learn 
more about IPython and the IPython notebook, refer to http://nbviewer.ipython.
org.
Plotly
Plotly is an online analytics and data visualization tool that provides online 
graphing, analytics, and statistical tools for better collaboration. This tool was built 
using Python with a user interface that uses JavaScript and a visualization library 
that uses D3.js, HTML, and CSS. Plotly includes the scientific graphic libraries for 
many languages, such as Arduino, Julia, MATLAB, Perl, Python, and R. For an 
example source of Plotly, refer to https://plot.ly/~etpinard/84/fig-31a-hans-
roslings-bubble-chart-for-the-year-2007/.

Chapter 3
[ 581 ]
The following is the infamous example of bubble chart that shows GDP per capita 
around the globe.
Plotly provides a convenient way to convert plots from matplotlib to Plotly, as 
shown in the following code (assuming that you have a Plotly account and signed in 
with your credentials):
import plotly.plotly as py
import matplotlib.pyplot as plt
#auto sign-in with credentials or use py.sign_in()
mpl_fig_obj = plt.figure()
#code for creating matplotlib plot
py.plot_mpl(mpl_fig_obj)
Types of Python IDE
The following are some of the popular Python IDEs that are available today:
•	
PyCharm: This specifies the user interface based on Java Swing
•	
PyDev: This denotes the user interface based on SWT (works on Eclipse)
•	
Interactive Editor for Python (IEP)
•	
Canopy from Enthought: This is based on PyQt
•	
The Anaconda distribution of Spyder from Continuum Analytics: This is 
also based on PyQt

Getting Started with the Python IDE
[ 582 ]
PyCharm
PyCharm is one of the few popular IDEs that has great features, and the community 
version is free. The PyCharm 4.0.6 community edition is the current version that is 
available for free download at https://www.jetbrains.com/pycharm/download. 
They have shortcuts reference cards available for Mac, Linux, and Windows. 
Dr. Pedro Kroger had written an elaborate description on PyCharm at http://
pedrokroger.net/getting-started-pycharm-python-ide/. You can refer to this 
link for more details. Among many interesting features, the code wizard and the 
NumPy array viewer are shown in the following screenshot:
Polar projection can be done quickly, as shown in the preceding screenshot, and the 
creation of an array of random samples is shown in the following screenshot:

Chapter 3
[ 583 ]
A similar random sample is created in a different IDE (such as Spyder); here is  
an example:
rand_4 = np.random.random_sample((2,2,2,2))-1  
array([[[[-0.6565232 , -0.2920045 ],
[-0.45976502, -0.70469325]],
[[-0.80218558, -0.77538009],
[-0.34687551, -0.42498698]]],
[[[-0.60869175, -0.9553122 ], 
[-0.05888953, -0.70585856]], 
[[-0.69856656, -0.21664848],
[-0.29017137, -0.61972867]]]])
PyDev
PyDev is a plugin for the Eclipse IDE. In other words, rather than creating a new 
IDE, a plugin for Eclipse was sufficient to make use of other default functionalities 
that a regular IDE may have. PyDev supports code refactoring, graphical debugging, 
interactive console, code analysis, and code folding.

Getting Started with the Python IDE
[ 584 ]
You can install PyDev as a plugin for Eclipse or install LiClipse, an advanced Eclipse 
distribution. LiClipse adds support not only for Python, but also for languages such 
as CoffeeScript, JavaScript, Django templates, and so on.
PyDev comes preinstalled in LiClipse, but it requires Java 7 to be installed first. For 
the complete installation steps, you can refer to http://pydev.org/manual_101_
install.html.
Interactive Editor for Python (IEP)
IEP is another Python IDE that has similar tools available in other IDEs, but appears 
similar to any tool that you may have used on Microsoft Windows.
IEP is a cross-platform Python IDE aimed at interactivity and introspection, which 
makes it very suitable for scientific computing. Its practical design is aimed at 
simplicity and efficiency.
IEP consists of two main components, the editor and the shell, and uses a set of 
pluggable tools to help the programmer in various ways. Some example tools 
are source structure, project manager, interactive help, and workspace. Some key 
features are as follows:
•	
Code introspection like in any modern IDE
•	
Either run the Python script from the command line or interactively via a file 
or the IPython interface
•	
Shells run as a background process
•	
Multiple shells can use different Python versions (from v2.4 to 3.x)

Chapter 3
[ 585 ]
The following screenshot shows how you can use two different versions of Python in 
the same IDE:
Some people do not consider IEP as an IDE tool, but it serves the purpose of 
developing the programs of Python, editing them, and running them. It supports 
multiple Python shells simultaneously. Therefore, it is a very productive tool for 
someone who wants to program using more than one GUI toolkit, such as PySide, 
PyQt4, GTK, and Tk interactively.
IEP is written in (pure) Python 3 and uses the Qt GUI toolkit, but it can be used  
to execute code on any Python version available. You can download IEP from 
http://www.iep-project.org/downloads.html.

Getting Started with the Python IDE
[ 586 ]
Canopy from Enthought
Enthought Canopy has a free version that is released under the BSD-style license, 
which comes with GraphCanvas, SciMath, and Chaco as plotting tools, among 
several other libraries. Like all the IDEs, it has a text editor. It also has the IPython 
console that is quite useful to be able to run and visualize results. In addition, it comes 
with a graphics package manager as well. When Canopy is launched, it gives an 
option with an Editor, Package Manager, and Doc Browser to choose from. One may 
also attempt to use their training materials, as shown in the following screenshot:
Besides other development code, Canopy has the IPython notebook integrated and 
convenient functions that you can use to create data visualization. Like most IDEs, 
this has an editor, a file browser, and the IPython console. In addition, there is a 
status display that shows the current editing status. These components of Canopy 
IDE mainly perform the following:
•	
The file browser: With this, you can read or write Python programs from the 
hard drive
•	
The Python code editor: This specifies a syntax-highlighted code editor with 
additional features specifically meant for Python code
•	
The Python pane: This is an integrated IPython (interactive Python) prompt 
that can be used to run the Python program interactively, rather than from  
a file

Chapter 3
[ 587 ]
•	
The editor status bar: This can be used to display the line number, the 
column number, the file type, and the file path
The following screenshot shows the number highlighted. This represents the 
components of IDEs described before this. The file browser and Python panes can 
be dragged and dropped onto the different positions in a code editor window or 
outside the borders.  When a pane is dragged, the location where it could dock is 
highlighted in blue, as shown in the following screenshot:
The documentation is organized via a browser called Canopy Documentation Browser, 
which is accessible from the Help menu. This includes the links to documentation for 
some commonly used Python packages.
One significant feature of Documentation Browser is that it provides easy access to the 
sample code presented in the documentation. When a user right-clicks on a sample 
code box, a display to the context menu is shown. Further, you can select the Copy 
code option to copy the contents of the code block into Canopy's copy-and-paste 
buffer to be used in an editor.

Getting Started with the Python IDE
[ 588 ]
Canopy comes in several different products for individuals, and the free version is 
called Canopy Express with approximately 100 core packages. This free version is a 
useful tool for easy Python development for scientific and analytic computing. You 
can download this at https://store.enthought.com/downloads/ after selecting 
the target operating system as one of Windows, Linux, or Mac OS.
One of the challenges in the Python development environment is that managing the 
packages of many different libraries and tools can be a very time-consuming and 
daunting task. This is how their Documentation Browser looks like.

Chapter 3
[ 589 ]
Canopy has a package manager that can be used to discover the Python packages 
available with Canopy and decide which additional ones to install and which ones 
to remove. There is a convenient search interface to find and install any available 
packages and to revert to the previous states of packages.
Canopy uses a Python capability to determine the Python packages that are 
available. When Canopy starts, it looks for packages first in the virtual environment 
and displays them, as shown in the following screenshot:

Getting Started with the Python IDE
[ 590 ]
The numbered highlighted areas of the IDE are:
1.	 The navigation panel: This is similar to any IDE; the navigation has a  
tree-list kind of structure to select the components of the package manager.
2.	 The main view area: Once the selection on the left-hand side changes, the 
right-hand side panel will display the item selected, along with the associated 
package listings (as shown in the preceding screenshot), the specific package 
information with a button titled More Info, and so on.
3.	 The search bar: This is similar to any search functionality and helps to 
quickly search the names and descriptions of the packages. For example, 
the typing machine filters the list down to eleven packages (the number of 
matches may vary depending on the operating system).
4.	 The subscription status and help: This is where the link to subscription and 
the name of the account currently in use will be displayed.
5.	 The status bar: For every navigation that the user makes, the status  
bar will show the details about the current state of results based on  
the navigational changes.
Anaconda from Continuum Analytics
Anaconda is one of the most popular IDEs that is being used by the community. It 
comes with a compiled long list of packages that are already integrated. This IDE is 
based on the core component called conda (which is explained in detail later), and 
you may either install or update the Python packages using conda or pip.
Anaconda is a free collection of powerful packages for Python that enables large-
scale data management, analysis, and visualization for business intelligence, 
scientific analysis, engineering, machine learning, and more.
Anaconda has a Scientific PYthon Development EnviRonment (Spyder), which has 
an IPython viewer as well. In addition, IPython can be launched as a GUI or a web-
based notebook. The most convenient aspect is that you can install Python in a home 
directory and not touch the system installed Python. Not all packages are yet ready 
to work with Python 3; therefore, it is better to use Python 2 with these IDEs. The 
Anaconda IDE has two important components and is based on the conda package 
manager. The two components are conda and spyder.

Chapter 3
[ 591 ]
The following screenshot appears when Anaconda is launched. This gives users 
several options that include the IPython console, the IPython notebook, the Spyder 
IDE, and glueviz:
An overview of Spyder
Spyder is a Python development environment that comes with the following 
components:
•	
The Python code editor: This comes with a separate browser for functions, 
and the class editor comes with a support for Pylint code analysis. Code 
completion has become a norm today and is convenient on all the IDEs, so it 
supports this too.
•	
The interactive console: The Python language is most suited for interactive 
work; therefore, it is imperative that consoles have all the necessary tools that 
support instant evaluation of the code written in the editor.
•	
Exploring variables: Exploring variables during any interactive execution 
helps in the overall productivity. Editing the variables is also possible, such 
as a dictionary and sometimes arrays.

Getting Started with the Python IDE
[ 592 ]
The code editor and the IPython console are shown in the following screenshot:
An overview of conda
Conda is a command-line tool used for managing environments and the packages 
of Python, rather than using pip. There are ways to query and search the packages, 
create new environments if necessary, and install and update the Python packages 
in the existing conda environments. This command-line tool also keeps track of 
dependencies between packages and platform specifics, helping you to create 
working environments from the different combination of packages. To check which 
version of conda is running, you can enter the following code (in my environment, it 
shows the 3.10.1 version):
Conda –v
3.10.1
A conda environment is a filesystem directory that contains a specific collection of 
conda packages. As a concrete example, you may want to have one environment that 
provides NumPy 1.7 and another environment that provides NumPy 1.6 for legacy 
testing; conda makes this kind of mixing and matching easy. To begin using an 
environment, simply set the PATH variable to point to its bin directory.

Chapter 3
[ 593 ]
Let's take a look at an example of how to install a package called SciPy with conda. 
Assuming that you have installed Anaconda correctly and conda is available in the 
running path, you may have to enter the following code to install SciPy:
$ conda install scipy
Fetching package metadata: ....
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    flask-0.10.1               |           py27_1         129 KB
    itsdangerous-0.23          |           py27_0          16 KB
    jinja2-2.7.1               |           py27_0         307 KB
    markupsafe-0.18            |           py27_0          19 KB
    werkzeug-0.9.3             |           py27_0         385 KB
The following packages will be linked:
    package                    |            build
    ---------------------------|-----------------
    flask-0.10.1               |           py27_1
    itsdangerous-0.23          |           py27_0
    jinja2-2.7.1               |           py27_0
    markupsafe-0.18            |           py27_0
    python-2.7.5               |                2
    readline-6.2               |                1
    sqlite-3.7.13              |                1
    tk-8.5.13                  |                1
    werkzeug-0.9.3             |           py27_0
    zlib-1.2.7                 |                1
Proceed ([y]/n)? 

Getting Started with the Python IDE
[ 594 ]
You should note that any dependencies on the package that is being tried to install 
would be recognized, downloaded, and linked automatically. If any Python package 
needs to be installed or updated, you will have to use the following code:
conda install <package name>  or conda update <package name> 
Here is an example of package update from the command line using conda (to 
update matplotlib):
conda update matplotlib
Fetching package metadata: ....
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    freetype-2.5.2             |                0         691 KB
    conda-env-2.1.4            |           py27_0          15 KB
    numpy-1.9.2                |           py27_0         2.9 MB
    pyparsing-2.0.3            |           py27_0          63 KB
    pytz-2015.2                |           py27_0         175 KB
    setuptools-15.0            |           py27_0         436 KB
    conda-3.10.1               |           py27_0         164 KB
    python-dateutil-2.4.2      |           py27_0         219 KB
    matplotlib-1.4.3           |       np19py27_1        40.9 MB
    ------------------------------------------------------------
                                           Total:        45.5 MB
The following NEW packages will be INSTALLED:
    python-dateutil: 2.4.2-py27_0    
The following packages will be UPDATED:
    conda:           3.10.0-py27_0    --> 3.10.1-py27_0   

Chapter 3
[ 595 ]
    conda-env:       2.1.3-py27_0     --> 2.1.4-py27_0    
    freetype:        2.4.10-1         --> 2.5.2-0         
    matplotlib:      1.4.2-np19py27_0 --> 1.4.3-np19py27_1
    numpy:           1.9.1-py27_0     --> 1.9.2-py27_0    
    pyparsing:       2.0.1-py27_0     --> 2.0.3-py27_0    
    pytz:            2014.9-py27_0    --> 2015.2-py27_0   
    setuptools:      14.3-py27_0      --> 15.0-py27_0     
Proceed ([y]/n)?
In order to check the packages that are installed using Anaconda, navigate to the 
command line and enter the following command to quickly display the list of all the 
packages installed in the default environment:
conda list 
In addition, you can always install a package with the usual means, for example, pip 
install, or from the source using a setup.py file. Although conda is the preferred 
packaging tool, there is nothing special about Anaconda that prevents the usage of a 
standard Python packaging tool (such as pip).
IPython is not required, but it is highly recommended. IPython should be installed 
after Python, GNU Readline, and PyReadline are installed. Anaconda and Canopy 
do these things by default. There are Python packages that are used in all the 
examples in this book for a good reason. In the following section, we have updated 
this list.
Visualization plots with Anaconda
From getting data, manipulating and processing data to visualizing and 
communicating the research results, Python and Anaconda support a variety of 
processes in the scientific data workflow. Python can be used in a wide variety of 
applications (even beyond scientific computing); users can adopt this language 
quickly and don't need to learn new software or programming languages. Python's 
open source availability enhances the research results and enables users to connect 
with a large community of scientists and engineers around the world.

Getting Started with the Python IDE
[ 596 ]
The following are some of the common plotting libraries that you can use  
with Anaconda:
•	
matplotlib: This is one of the most popular plotting libraries for Python. 
Coupled with NumPy and SciPy, this is one of the major driving forces in 
the scientific Python community. IPython has a pylab mode, which was 
specifically designed to perform interactive plotting using matplotlib.
•	
Plotly: This is a collaborative plotting and analytics platform that works on a 
browser. It supports interactive graphs using IPython notebooks. Graphs are 
interactive and can be styled by modifying the code and viewing the results 
interactively. Any plotting code that is generated using matplotlib can be 
easily exported to a Plotly version.
•	
Veusz: This is a GPL-scientific plotting package written in Python and PyQt. 
Veusz can also be embedded in other Python programs.
•	
Mayavi: This is a three-dimensional plotting package that is fully scriptable 
from Python and is similar to a simple pylab and MATLAB-like interface for 
plotting arrays.
•	
NetworkX: This is a Python language software package for the creation, 
manipulation, and study of the structure, dynamics, and functions of 
complex networks.
•	
pygooglechart: This is a powerful package that enables you to create 
visualization methods and allows you to interface with the Google Chart API.
The surface-3D plot
Three-dimensional plots are generated from the data defined as Z as a function of 
(X,Y). This is mathematically denoted as Z=f(X,Y). In our example here, we will plot 
Z=sin(sqrt(X2+Y2)), and this is essentially similar to a two-dimensional parabola. The 
following steps need to be followed for our plot:
1.	 First, generate the X and Y grid with the following code:
import numpy as np
X = np.arange(-4, 4, 0.25) 
Y = np.arange(-4, 4, 0.25) 
X, Y = np.meshgrid(X, Y)
Generate the Z data:
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)

Chapter 3
[ 597 ]
Plotting a simple three-dimensional surface sin(sqrt(X**2+Y**2)) using the 
mpl_toolkits package is shown here; the blow and the plot diagram is 
represented using a color bar:
2.	 Then, plot the surface, as shown in the following code:
from mpl_toolkits.mplot3d import Axes3d
from matplotlib import cm
from matplotlib.ticker import LinearLocator, FormatStrFormatter
import matplotlib.pyplot as plt
import numpy as np
fig = plt.figure(figsize=(12,9))
ax = fig.gca(projection='3d')
X = np.arange(-4, 4, 0.25)
Y = np.arange(-4, 4, 0.25)
X, Y = np.meshgrid(X, Y)
R = np.sqrt(X**2 + Y**2)
Z = np.sin(R)

Getting Started with the Python IDE
[ 598 ]
surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.
coolwarm, linewidth=0, antialiased=False)
ax.set_zlim(-1.01, 1.01)
ax.zaxis.set_major_locator(LinearLocator(10))
ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))
fig.colorbar(surf, shrink=0.6, aspect=6)
plt.show()
In order to make this three-dimensional plot work, you have to make sure that 
matplotlib and NumPy are installed. The default package in Anaconda comes with 
these installed.
The square map plot
With the comparison and ranking example that we discussed in the previous chapter 
to display the top 12 countries in Africa by GDP using the squarify algorithm (with 
matplotlib), you can obtain a plot that looks similar to a tree map, as shown in the 
following code:
# Squarified Treemap Layout : source file (squarify.py)
# Implements algorithm from Bruls, Huizing, van Wijk, "Squarified 
Treemaps"
# squarify was created by Uri Laserson 
# primarily intended to support d3.js 
def normalize_sizes(sizes, dx, dy):
  total_size = sum(sizes)
  total_area = dx * dy
  sizes = map(float, sizes)
  sizes = map(lambda size: size * total_area / total_size, sizes)
  return sizes
def pad_rectangle(rect):
  if rect['dx'] > 2:
    rect['x'] += 1
    rect['dx'] -= 2
  if rect['dy'] > 2:
    rect ['y'] += 1
    rect['dy'] -= 2
def layoutrow(sizes, x, y, dx, dy):
  covered_area = sum(sizes)

Chapter 3
[ 599 ]
  width = covered_area / dy
  rects = []
  for size in sizes:  
    rects.append({'x': x, 'y': y, 'dx': width, 'dy': size / width})
    y += size / width
  return rects
def layoutcol(sizes, x, y, dx, dy):
  covered_area = sum(sizes)
  height = covered_area / dx
  rects = []
  for size in sizes:
    rects.append({'x': x, 'y': y, 'dx': size / height, 'dy': height})
    x += size / height
  return rects
def layout(sizes, x, y, dx, dy):
  return layoutrow(sizes, x, y, dx, dy) if dx >= dy else 
layoutcol(sizes, x, y, dx, dy)
def leftoverrow(sizes, x, y, dx, dy):
  covered_area = sum(sizes)
  width = covered_area / dy
  leftover_x = x + width
  leftover_y = y
  leftover_dx = dx - width
  leftover_dy = dy
  return (leftover_x, leftover_y, leftover_dx, leftover_dy)
def leftovercol(sizes, x, y, dx, dy):
  covered_area = sum(sizes)
  height = covered_area / dx
  leftover_x = x
  leftover_y = y + height
  leftover_dx = dx
  leftover_dy = dy - height
  return (leftover_x, leftover_y, leftover_dx, leftover_dy)
def leftover(sizes, x, y, dx, dy):
  return leftoverrow(sizes, x, y, dx, dy) if dx >= dy else 
leftovercol(sizes, x, y, dx, dy)
def worst_ratio(sizes, x, y, dx, dy):

Getting Started with the Python IDE
[ 600 ]
  return max([max(rect['dx'] / rect['dy'], rect['dy'] / rect['dx']) 
for rect in layout(sizes, x, y, dx, dy)])
def squarify(sizes, x, y, dx, dy):
  sizes = map(float, sizes)
  if len(sizes) == 0:
    return []
  if len(sizes) == 1:
    return layout(sizes, x, y, dx, dy)
  # figure out where 'split' should be
  i = 1
  while i < len(sizes) and worst_ratio(sizes[:i], x, y, dx, dy) >= 
worst_ratio(sizes[:(i+1)], x, y, dx, dy):
    i += 1
  current = sizes[:i]
  remaining = sizes[i:]
  (leftover_x, leftover_y, leftover_dx, leftover_dy) = 
leftover(current, x, y, dx, dy)
  return layout(current, x, y, dx, dy) + \
squarify(remaining, leftover_x, leftover_y, leftover_dx, leftover_dy)
def padded_squarify(sizes, x, y, dx, dy):
  rects = squarify(sizes, x, y, dx, dy)
  for rect in rects:
    pad_rectangle(rect)
  return rects
The squarify function displayed in the preceding code can be used to display the top 
12 countries by GDP in Africa, as shown in the following code:
import matplotlib.pyplot as plt
import matplotlib.cm
import random
import squarify
x = 0.
y = 0.
width = 950.
height = 733.
norm_x=1000
norm_y=1000
fig = plt.figure(figsize=(15,13))

Chapter 3
[ 601 ]
ax=fig.add_subplot(111,axisbg='white')
initvalues = [285.4,188.4,173,140.6,91.4,75.5,62.3,39.6,29.4,28.5, 
26.2, 22.2]
values = initvalues
labels = ["South Africa", "Egypt", "Nigeria", "Algeria", "Morocco",
"Angola", "Libya", "Tunisia", "Kenya", "Ethiopia", "Ghana", "Cameron"]
colors = [(214,27,31),(229,109,0),(109,178,2),(50,155,18), 
(41,127,214),(27,70,163),(72,17,121),(209,0,89), 
(148,0,26),(223,44,13), (195,215,0)] 
# Scale the RGB values to the [0, 1] range, which is the format 
matplotlib accepts. 
for i in range(len(colors)): 
  r, g, b = colors[i] 
  colors[i] = (r / 255., g / 255., b / 255.) 
# values must be sorted descending (and positive, obviously)
values.sort(reverse=True)
# the sum of the values must equal the total area to be laid out
# i.e., sum(values) == width * height
values = squarify.normalize_sizes(values, width, height)
# padded rectangles will probably visualize better for certain cases
rects = squarify.padded_squarify(values, x, y, width, height)
cmap = matplotlib.cm.get_cmap()
color = [cmap(random.random()) for i in range(len(values))]
x = [rect['x'] for rect in rects]
y = [rect['y'] for rect in rects]
dx = [rect['dx'] for rect in rects]
dy = [rect['dy'] for rect in rects]
ax.bar(x, dy, width=dx, bottom=y, color=colors, label=labels)
va = 'center'
idx=1
for l, r, v in zip(labels, rects, initvalues):
  x, y, dx, dy = r['x'], r['y'], r['dx'], r['dy']
  ax.text(x + dx / 2, y + dy / 2+10, str(idx)+"--> "+l, va=va,
     ha='center', color='white', fontsize=14)

Getting Started with the Python IDE
[ 602 ]
  ax.text(x + dx / 2, y + dy / 2-12, "($"+str(v)+"b)", va=va,
     ha='center', color='white', fontsize=12)
  idx = idx+1
ax.set_xlim(0, norm_x)
ax.set_ylim(0, norm_y)
plt.show()
Interactive visualization packages
A few years ago, there were not many interactive tools besides IPython. In order 
to understand how you can make any visualization interactive, it makes sense to 
compare it with an existing tool (such as D3.js). One of the reasons why D3.js is very 
powerful is that a JavaScript-based plotting framework can make the plots to be 
presented on the Web. Moreover, it comes with all the event-driven functions that 
can be configured easily.

Chapter 3
[ 603 ]
There are two visualization libraries called Bokeh and VisPy that are popular among 
a few that are available today. There is another tool called Wakari. This is mainly 
used for data analytics with a resemblance to IPython in terms of how you can 
create a browser-based visualization. The Ashiba project was another tool that was 
developed by Clayton Davis at Continuum, but since the focus of Continuum shifted 
to Bokeh and Wakari, there is very little work that has been done on Ashiba in the 
past few years.
Bokeh
Bokeh is an interactive visual library that is developed in Python and is targeted to 
work via web browsers. Where does the name Bokeh come from? This is a Japanese 
word that describes the blurring or the parts of an image that are out of focus. The 
goal was to develop a library that closely resembles the aesthetics of D3.js; the choice 
of the name Bokeh seemed to match. Bokeh writes to the HTML5 Canvas library 
and therefore guarantees to work on browsers that support HTML5. This is useful 
because you would want to compare the JavaScript-based plots with Python.
We will not elaborate much about this tool. You can read and explore more about 
this at http://bokeh.pydata.org. However, what is important is to know the 
dependencies of the Bokeh library. Before installing the Bokeh library, it is required 
that jsonschema be installed, as follows:
conda install jsonschema
Fetching package metadata: ....
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    jsonschema-2.4.0           |           py27_0          51 KB
The following NEW packages will be INSTALLED:
    jsonschema: 2.4.0-py27_0
Proceed ([y]/n)?

Getting Started with the Python IDE
[ 604 ]
The examples of interactive visualization using Bokeh, pandas, SciPy, matplotlib, 
and ggplot can be found at http://nbviewer.ipython.org/gist/fonnesbeck/
ad091b81bffda28fd657.
VisPy
VisPy is a visualization library for 2D or 3D plotting that is interactive and has 
high performance. You can take advantage of the OpenGL knowledge to create 
visualization quickly. It also has methods that do not necessarily require a deep 
understanding of OpenGL. For more information, you can read the documentation at 
vispy.org.
In order to install the VisPy library, one may attempt the conda install vispy 
command, but it most likely responds with the binstar search –t conda vispy 
suggestion. The following code is one of those in the list:
conda install --channel https://conda.binstar.org/asmeurer vispy
With this command, you will obtain the following the response:
Fetching package metadata: ......
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    numpy-1.8.2                |           py27_0         2.9 MB
    vispy-0.3.0                |       np18py27_0         679 KB
    ------------------------------------------------------------
                                           Total:         3.6 MB
The following NEW packages will be INSTALLED:
    vispy: 0.3.0-np18py27_0
The following packages will be DOWNGRADED:
    numpy: 1.9.2-py27_0 --> 1.8.2-py27_0    
Proceed ([y]/n)?

Chapter 3
[ 605 ]
There are many examples in the gallery collection of VisPy. One particular example 
of the display of points that uses the vispy.gloo command and GLSL shading code 
can be viewed at http://vispy.org/gloo.html?highlight=gloo#module-vispy.
gloo.
Summary
There is a good set of tools and packages for Python developers that are available 
today. Python has a large standard library. This is commonly cited as one of Python's 
greatest strengths. It has modules to create the graphical user interfaces, connecting 
to relational databases, pseudorandom number generators, arithmetic with arbitrary 
precision decimals, manipulating regular expressions. In addition, there are high-
performance packages to plot 2D and 3D graphics, machine learning and statistical 
algorithms, and so on.
We have seen that the IDE tools (such as Canopy and Anaconda) have leveraged 
the efficient development work from a computation and visualization standpoint, 
among many other areas. There are many effective ways to produce visualization 
methods using these tools. In the following few chapters, interesting examples will 
be shown with these tools and packages.


[ 607 ]
Numerical Computing and 
Interactive Plotting
The field of high-performance numerical computing lies at the crossroads of 
a number of disciplines and skill sets. In order to be successful at using high-
performance computing today, it requires knowledge and skills of programming, 
data science, and applied mathematics. In addition to these, efficient implementation 
of the computational problems requires some understanding of processing and 
storage devices.
The role of computing in science has evolved to a different level in recent years. 
Programming languages (such as R and MATLAB) were common in academic 
research and scientific computing. Today, Python plays a big role in scientific 
computing for a good reason. The Python community has put together many efficient 
tools and packages that is being used not only by the research community, but also 
successful commercial organizations such as Yahoo, Google, Facebook, and Amazon.
There are two popular packages that are widely used in scientific computing. They 
are Numerical Python Package (NumPy) and Scientific Python Package (SciPy). 
NumPy is popular for efficient arrays and in particular the ease of indexing. In the 
following sections, we will discuss the following topics:
•	
NumPy, SciPy, and MKL functions
•	
Numerical indexing and logical indexing
•	
Data structures—stacks, queues, tuples, sets, tries, and dictionaries
•	
Visualizing plots using matplotlib, and so on
•	
Optimization and interpolation using NumPy and SciPy with examples
•	
Integrating Cython with NumPy and advantages of Cython

Numerical Computing and Interactive Plotting
[ 608 ]
NumPy, SciPy, and MKL functions
Almost all scientific and numerical computing requires the representation of data in 
the form of vectors and matrices, and NumPy handles all these in terms of arrays.
NumPy and SciPy are computational modules of Python that provide convenient 
mathematical and numerical methods in precompiled, fast functions. The NumPy 
package provides basic routines to manipulate large arrays and matrices of numeric 
data. The SciPy package extends NumPy with a collection of useful algorithms 
with applied mathematical techniques. In NumPy, ndarray is an array object that 
represents a multidimensional, homogeneous array of items that have a known size.
NumPy
NumPy not only uses array objects, but also linear algebraic functions that can be 
conveniently used for computations. It provides a fast implementation of arrays and 
associated array functionalities. Using an array object, one can perform operations 
that include matrix multiplication, transposition of vectors and matrices, solve 
systems of equations, perform vector multiplication and normalization, and so on.
NumPy universal functions
A universal function (ufunc) is a function that operates on ndarrays by each element, 
supporting type casting and several other standard features. In other words, ufunc 
is a vectorized wrapper for a function that takes scalar inputs and produces scalar 
outputs. Many built-in functions are implemented in the compiled C code, which 
makes it faster.
NumPy universal functions are faster than Python functions because looping is 
performed in compiled code. Also, since arrays are typed, their type is known before 
any sort of computation occurs.
A simple example of ufunc operating on each element is shown here:
import numpy as np
x = np.random.random(5)
print x
print x + 1   # add 1 to each element of x
[ 0.62229809  0.18010463  0.28126201  0.30701477  0.39013144] 
[ 1.62229809  1.18010463  1.28126201  1.30701477  1.39013144]
Other examples are np.add and np.subtract.

Chapter 4
[ 609 ]
NumPy's ndarray is similar to the lists in Python, but it is rather strict in storing 
only a homogeneous type of object. In other words, with a Python list, one can mix 
the element types, such as the first element as a number, the second element as a 
list, and the next element as another list (or dictionary). The performance in terms of 
operating the elements of ndarray is significantly faster for a large size array, which 
will be demonstrated here. The example here demonstrates that it is faster because we 
will measure the running time. However, for readers who are curious about NumPy 
implementations in C, there is a documentation on the same available at http://
docs.scipy.org/doc/numpy/reference/internals.code-explanations.html.
import numpy as np
arr = np.arange(10000000)
listarr = arr.tolist()
def scalar_multiple(alist, scalar):
    for i, val in enumerate(alist):
        alist[i] = val * scalar
    return alist
# Using IPython's magic timeit command
timeit arr * 2.4
10 loops, best of 3: 31.7 ms per loop
# above result shows 31.7 ms (not seconds)
timeit scalar_multiple(listarr, 2.4)
1 loops, best of 3: 1.39 s per loop
# above result shows 1.39 seconds (not ms)
In the preceding code, each array element occupies 4 bytes. Therefore, a million 
integer arrays occupy approximately 44 MB of memory, and the list uses 711 MB 
of memory. However, arrays are slower for small collection sizes, but for large 
collection sizes, they use less memory space and are significantly faster than lists.
NumPy comes with many useful functions that are broadly categorized as 
trigonometric functions, arithmetic functions, exponent and logarithmic functions, 
and miscellaneous functions. Among many miscellaneous functions, convolve() for 
linear convolution and interp() for linear interpolation are popular. In addition, 
for most experimental work that involve equally spaced data, the linspace() and 
random.rand() functions are among a few that are used widely.

Numerical Computing and Interactive Plotting
[ 610 ]
Shape and reshape manipulation
Changing the shape of an existing array can be more efficient than creating a new 
array from the old data with a new shape. In the first example, reshape happens in 
memory (the array is not stored in a variable), whereas in the following code, the 
array is first stored in a variable a and then a is reshaped:
import numpy as np
np.dandom.rand(2,4)
array([[ 0.96432148,  0.63192759,  0.12976726,  0.56131001], 
    [    0.27086909,  0.92865208,  0.27762891,  0.40429701]])
np.random.rand(8).reshape(2,4)
array([[ 0.39698544,  0.88843637,  0.66260474,  0.61106802], 
       [ 0.97622822,  0.47652548,  0.56163488,  0.43602828]]) 
In the preceding example, after creating 8 values, they are reshaped into a valid 
dimension of choice, as shown in the following code:
#another example
a = np.array([[11,12,13,14,15,16],[17,18,19,20,21,22]])
print a
[[11, 12, 13, 14, 15, 16], [17, 18, 19, 20, 21, 22]]
# the following shows shape is used to know the dimensions
a.shape
(2,6)
#Now change the shape of the array
a.shape=(3,4)
print a
[[11 12 13]  [14 15 16]  [17 18 19]  [20 21 22]]
xrange is used instead of range because it is faster for loops and avoids the storage 
of the list of integers; it just generates them one by one. The opposite of shape and 
reshape is ravel(), as shown in the following code:
#ravel example
a = np.array([[11,12,13,14,15,16],[17,18,19,20,21,22]])
a.ravel()
array([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22])

Chapter 4
[ 611 ]
An example of interpolation
Here is an example of interpolation using interp():
n=30 
# create n values of x from 0 to 2*pi 
x = np.linspace(0,2*np.pi,n) 
y = np.zeros(n) 
#for range of x values, evaluate y values
for i in xrange(n):    
   y[i] = np.sin(x[i])
The image displayed in the following picture is the result of a simple sine curve 
interpolation:
The following code shows the plotting curves with and without interpolation:
import numpy as np
import matplotlib.pyplot as plt
# create n values of x from 0 to 2*pi 
x = np.linspace(0, 8*np.pi, 100)
y = np.sin(x/2)
#interpolate new y-values 
yinterp = np.interp(x, x, y)

Numerical Computing and Interactive Plotting
[ 612 ]
#plot x,y values using circle marker (line style)
plt.plot(x, y, 'o')  
#plot interpolated curve using dash x marker
plt.plot(xvals, yinterp, '-x')  
plt.show()
Vectorizing functions
Vectorizing functions via vectorize() in NumPy and SciPy can be very efficient. 
Vectorize has the capability to convert a function that takes scalars as arguments to  
a function that takes arrays as arguments by applying the same rule element-wise. 
We will demonstrate this here with two examples.
The first example uses a function that takes three scalar arguments to produce a 
vectorized function that takes three array arguments, as shown in the following code:
import numpy as np
def addition(x, y, z):
    return x + y + z
def addpoly():
    i = np.random.randint(25)
    poly1 = np.arange(i, i+10)
    i = np.random.randint(25) 
    poly2 = np.arange(i, i+10)
    poly3 = np.arange(10, 20)
    print poly1
    print poly2
    print poly3
    print '-' * 32
    vecf = np.vectorize(addition)
    print vecf(poly1,poly2,poly3)
addpoly()
[ 4  5  6  7  8  9 10 11 12 13]
[13 14 15 16 17 18 19 20 21 22]
[10 11 12 13 14 15 16 17 18 19]
--------------------------------
[27 30 33 36 39 42 45 48 51 54]
Note that arrange is an array-valued version of the built-in Python range function.

Chapter 4
[ 613 ]
The second example uses a function that takes one scalar argument to produce a 
vectorized function that takes an array argument, as shown in the following code:
import numpy as np
def posquare(x):
  if x >= 0: return x**2
  else: return -x
i = np.random.randint(25)
poly1 = np.arange(i,i+10)
print poly1
vecfunc = vectorize(posquare, otypes=[float]) 
vecfunc(poly1)
[14 15 16 17 18 19 20 21 22 23]
array([ 196., 225., 256., 289., 324., 361., 400., 441., 484., 529.])
There is yet another example that is interesting to study with the help of an example 
code. This example shows three ways to increment the array elements by a constant 
and measure the running time to determine which method is faster:
import numpy as np
from time import time
def incrembyone(x):
    return x + 1
dataarray=np.linspace(1,5,1000000)
t1=time()
lendata = len(dataarray)
print "Len = "+str(lendata)
print dataarray[1:7]
for i in range(lendata):
    dataarray[i]+=1
print " time for loop (No vectorization)->" + str(time() - t1)
t2=time()
vecincr = np.vectorize(incrembyone) #1 
vecincr(dataarray) #2          
print " time for vectorized version-1:" + str(time() - t2)
t3 = time()

Numerical Computing and Interactive Plotting
[ 614 ]
# This way to increment array elements with one line
# is pretty powerful, accomplishes same thing as #1 and #2
dataarray+=1  # how does this achieve the results
print dataarray[1:7]
print " time for vectorized version-2:" + str(time() - t3)
Len = 1000000
 [ 1.000004 1.000008 1.000012 1.000016 1.00002 1.000024]  
time for loop (No vectorization)->0.473765850067  
time for vectorized version-1:0.221153974533 # half the time
 
[ 3.000004 3.000008 3.000012 3.000016 3.00002 3.000024]  
time for vectorized version-2:0.00192213058472 # in fraction time
Besides the vectorizing techniques, there is another simple coding practice that could 
make programs more efficient. If there are prefix notations that are being used in 
loops, it is best practice to create a local alias and use this alias in the loop. One such 
example is shown here:
fastsin = math.sin
x = range(1000000)
for i in x:
    x[i] = fastsin(x[i])
Summary of NumPy linear algebra
The following is a list of some well-known functions that NumPy offers in  
linear algebra:
Name
Description
dot(a,b)
This is a dot product of two arrays
linalg.norm(x)
This is a matrix or vector norm
linalg.cond(x)
This specifies the condition number
linalg.solve(A,b)
This solves linear system Ax=b
linalg.inv(A)
This represents an inverse of A
linalg.pinv(A)
This specifies a pseudo-inverse of A
linalg.eig(A)
These are eigenvalues/vectors of square A
linalg.eigvals(A)
These are eigenvalues of general A
linalg.svd(A)
This is a singular value decomposition

Chapter 4
[ 615 ]
SciPy
NumPy has already many convenient functions that can be used in computation. 
Then, why do we need SciPy? SciPy is an extension of NumPy for mathematics, 
science, and engineering that has many packages available for linear algebra, 
integration, interpolation, fast Fourier transforms, large matrix manipulation, 
statistical computation, and so on. The following table shows a brief description  
of these packages:
Subpackage
Brief description of functionalities
scipy.cluster
This specifies the functions for clustering, including vector 
quantization and k-means.
scipy.fftpack
This denotes the functions of fast Fourier transform.
scipy.
integrate
This specifies the functions for performing numerical integration 
using trapezoidal, Simpson's, Romberg, and other methods. It also 
specifies methods for integration of ordinary differential equations. 
One can perform single, double, and triple integrations on a function 
object with the functions quad, dblquad, and tplquad.
scipy.
interpolate
This denotes the functions and classes for interpolation objects with 
discrete numeric data and linear and spline interpolation.
scipy.linalg
This is a wrapper for the package linalg in NumPy. All the 
functionalities from NumPy is part of scipy.linalg, along with 
several other functions.
scipy.
optimize
This denotes the maximization and minimization functions that 
include Neider-Mead Simplex, Powell's, conjugate gradient BFGS, 
least squares, constrained optimizers, simulated annealing, Newton's 
method, bisection method, Broyden Anderson, and line search.
scipy.sparse
This specifies the functions that can work with large sparse matrices.
scipy.special
This has special functions for computational physics, such as elliptic, 
bessel, gamma, beta, hypergeometric, parabolic, cylinder, mathieu, 
and spheroidal wave.
In addition to the preceding listed subpackages, SciPy also has a scipy.io package 
that has functions to load a matrix called spio.loadmat(), save a matrix called 
spio.savemat(), and read images via scio.imread(). When there is a need to 
develop computational programs in Python, it is good practice to check the SciPy 
documentation to see whether it contains the functions that already accomplish the 
intended task.
Let's take a look at an example using scipy.polyId():
import scipy as sp
# function that multiplies two polynomials

Numerical Computing and Interactive Plotting
[ 616 ]
def multiplyPoly():  
    #cubic1 has coefficients 3, 4, 5 and 5 
    cubic1 = sp.poly1d([3, 4, 5, 5])  
 
    #cubic2 has coefficients 4, 1, -3 and 3
    cubic2 = sp.poly1d([4, 1, -3, 3]) 
    print cubic1   
    print cubic2 
    print '-' * 36
    #print results of polynomial multiplication
    print cubic1 * cubic2
multiplyPoly()  # produces the following result
   3     2
3 x + 4 x + 5 x + 5
   3     2
4 x + 1 x - 3 x + 3
------------------------------------
    6      5      4      3     2
12 x + 19 x + 15 x + 22 x + 2 x + 15  
The result matches with the multiplication done in the traditional term-by-term 
method, as follows:
As such, polynomial representation can be used for integration, differentiation, and 
other computational physics. These capabilities along with many more functions in 
NumPy, SciPy, and other package extensions clearly indicate that Python is another 
alternative to MATLAB and is therefore used in some academic environments.
There are many different kinds of interpolation that SciPy offers. The following 
example uses interpolate.splev, which uses B-spline and its derivatives and 
interpolate.splprep for the B-spline representation of two-dimensional curves 
(N-dimensional in general):
import numpy as np 
import matplotlib.pyplot as plt
import scipy as sp

Chapter 4
[ 617 ]
t = np.arange(0, 2.5, .1)
x = np.sin(2*np.pi*t)
y = np.cos(2*np.pi*t)
tcktuples,uarray = sp.interpolate.splprep([x,y], s=0)
unew = np.arange(0, 1.01, 0.01)
splinevalues = sp.interpolate.splev(unew, tcktuples)
plt.figure(figsize=(10,10))
plt.plot(x, y, 'x', splinevalues[0], splinevalues[1], 
np.sin(2*np.pi*unew), np.cos(2*np.pi*unew), x, y, 'b')
plt.legend(['Linear', 'Cubic Spline', 'True'])
plt.axis([-1.25, 1.25, -1.25, 1.25])
plt.title('Parametric Spline Interpolation Curve')
plt.show()
The following diagram is the result of this spline interpolation using SciPy and NumPy:

Numerical Computing and Interactive Plotting
[ 618 ]
Let's take a look at an example in numerical integration and solve linear equations using 
some of the SciPy functions (such as Simpson's and Romberg) and compare these with 
the NumPy function trapezoidal. We know that when a function such as f(x) = 9 – x2 is 
integrated from -3 to 3, we expect 36 units, as shown in the following diagram:
The preceding plot shows the 9-x2 function (which is symmetric along the Y axis). 
Mathematically, the integration from -3 to 3 is twice that of the integration from 0 to 
3. How do we numerically integrate using SciPy? The following code shows one way 
to perform it using the trapezoidal method from NumPy:
import numpy as np
from scipy.integrate import simps, romberg
a = -3.0; b = 3.0;
N = 10  
x = np.linspace(a, b, N)
y = 9-x*x
yromb = lambda x: (9-x*x)
t = np.trapz(y, x)
s = simps(y, x)
r = romberg(yromb, a, b)
#actual integral value
aiv = (9*b-(b*b*b)/3.0) - (9*a-(a*a*a)/3.0)
print 'trapezoidal = {0} ({1:%} error)'.format(t, (t - aiv)/aiv)
print 'simpsons = {0} ({1:%} error)'.format(s, (s - aiv)/aiv)
print 'romberg  = {0} ({1:%} error)'.format(r, (r - aiv)/aiv)

Chapter 4
[ 619 ]
print 'actual value = {0}'.format(aiv)
trapezoidal = 35.5555555556 (-1.234568% error)
simpsons = 35.950617284 (-0.137174% error)
romberg  = 36.0 (0.000000% error)
actual value = 36.0
An example of linear equations
Let's try to solve a set of linear equations in three variables (x, y, and z) as follows:
•	
x + 2y – z = 2
•	
2x – 3y + 2z = 2
•	
3x + y – z = 2
NumPy offers a convenient method np.linalg.solve() to solve linear equations. 
However, the inputs are expected in vector form. The following program shows how 
one can solve linear equations.
import numpy as np
# Matrix A has coefficients of x,y and z
A = np.array([[1, 2, -1],
              [2, -3, 2],
              [3, 1, -1]])
#constant vector 
b = np.array([2, 2, 2])
#Solve these equations by calling linalg.solve
v = np.linalg.solve(A, b)
# v is the vector that has solutions
print "The solution vector is "
print v
# Reconstruct Av to see if it produces identical values 
print np.dot(A,v) == b
The solution vector is
[ 1.  2.  3.]
[ True  True  True]
Note that np.dot(A,v) is a matrix multiplication (not A*v). The solution vector  
v = [1,2,3] is the correct expected result.

Numerical Computing and Interactive Plotting
[ 620 ]
The vectorized numerical derivative
Now as the last example in this section, we will take a look at the vectorized 
numeric derivatives that NumPy offers. We do know that the derivative is 
 by applying the quotient rule of differentiation. 
However, by applying the vectorized methods in Python to compute the derivatives 
without loop, we will see the following code:
import numpy as np 
import matplotlib.pyplot as plt
x = np.linspace(-np.pi/2, np.pi/2, 44)
y = 1/(1+np.cos(x)*np.cos(x))
dy_actual = np.sin(2*x)/(1+np.cos(x)*np.cos(x))**2
fig = plt.figure(figsize=(10,10))
ax=fig.add_subplot(111,axisbg='white')
# we need to specify the size of dy ahead because diff returns 
dy = np.zeros(y.shape, np.float) #we know it will be this size
dy[0:-1] = np.diff(y) / np.diff(x)
dy[-1] = (y[-1] - y[-2]) / (x[-1] - x[-2])
plt.plot(x,y, linewidth=3, color='b', label='actual function')
plt.plot(x,dy_actual,label='actual derivative', linewidth=2, 
color='r')
plt.plot(x,dy,label='forward diff', linewidth=2, color='g')
plt.legend(loc='upper center')
plt.show()
In the following example, we can see how you can plot the actual function, its 
derivative, and the forward difference in the same plot. The actual derivative is 
plugged into dy_actual, and the forward difference is calculated using diff()  
from NumPy.

Chapter 4
[ 621 ]
The following plot diagram is the result of this program:

Numerical Computing and Interactive Plotting
[ 622 ]
MKL functions
The MKL functions from Intel provide high-performance routines on vectors and 
matrices. In addition, they include FFT functions and vector statistical functions. 
These functions have been enhanced and optimized to work efficiently on Intel 
processors. For Anaconda users, Continuum has packaged these FFT functions 
into binary versions of Python libraries for MKL optimizations. However MKL 
optimizations are available as an add-on as part of the Anaconda Accelerate package. 
The graph here shows the difference in slowness without MKL:
The preceding graph has been taken from  
https://store.continuum.io/cshop/mkl-optimizations/.

Chapter 4
[ 623 ]
For larger array inputs, MKL offers a significant improvement over performance, as 
shown in the following screenshot:
The preceding image has been taken from https://software.intel.com/en-us/
articles/numpyscipy-with-intel-mkl.
The performance of Python
Python programmers often try to rewrite their innermost loops in C and call the 
compiled C functions from Python for performance reasons. There are many projects 
aimed at making this kind of optimization easier, such as Cython. However, it 
would be preferable to make their existing Python code faster without depending on 
another programming language.

Numerical Computing and Interactive Plotting
[ 624 ]
There are few other options available to improve the performance of the 
computationally intensive programs in Python:
•	
Use Numbapro: This is a Python compiler from Continuum Analytics that 
can compile the Python code for execution on CUDA-capable GPUs or 
multicore CPUs. This compiled code runs the native compiled code and is 
several times faster than the interpreted code. Numbapro works by enabling 
compilation at runtime (this is just-in-time or the JIT compilation). With 
Numbapro, it is possible to write standard Python functions and run them on 
a CUDA-capable GPU. Numbapro is designed for array-oriented computing 
tasks, such as the widely used NumPy library. Numbapro is an enhanced 
version of Numba and is part of Anaconda Accelerate, a commercially 
licensed product from Continuum Analytics.
•	
Use Scipy.weave: This is a module that lets you insert snippets of the C code 
and seamlessly transports the arrays of NumPy into the C layer. It also has 
some efficient macros.
•	
Use multicore approach: The multiprocessing package of Python 2.6 or 
higher provides a relatively simple mechanism to create a subprocess. Even 
desktop computers these days have multicore processors; it makes sense to 
put all the processors to work. This is much simpler than using threading.
•	
Use process pool called pool: This is another class in the multiprocessing 
package. With pool, you can define the number of worker processes to be 
created in the pool and then pass an iterable object containing the parameters 
for each process.
•	
Use Python in a distributed computing package (such as Disco): This is a 
lightweight, open source framework for distributed computing based on the 
MapReduce paradigm (http://discoproject.org). Other similar packages 
are Hadoop Streaming, mrjob, dumbo, hadoopy, and pydoop.
Scalar selection
Scalar selection is the simplest method to select elements from an array and 
is implemented using [rowindex] for one-dimensional arrays, [rowindex, 
columnindex] for two-dimensional arrays, and so on. The following is a simple code 
that shows an array element reference:
import numpy as np
x = np.array([[2.0,4,5,6], [1,3,5,9]])
x[1,2]
5.0

Chapter 4
[ 625 ]
A pure scalar selection always returns a single element and not an array. The data 
type of the selected element matches the data type of the array used in the selection. 
Scalar selection can also be used to assign a value to an array element, as shown in 
the following code:
x[1,2] = 8
x
array([[2, 4, 5, 6],[1, 3, 8, 9]])
Slicing
Arrays can be sliced just like lists and tuples. Array slicing is identical to list slicing, 
except that the syntax is simpler. Arrays are sliced using the [ : , :, ... :] 
syntax, where the number of dimensions of the arrays determine the size of the slice, 
except that these dimensions for which slices are omitted, all elements are selected. 
For example, if b is a three-dimensional array, b[0:2] is the same as b[0:2,:,:]. 
There are shorthand notations for slicing. Some common ones are:
•	
: and: are the same as 0:n:1, where n is the length of the array
•	
m: and m:n: are the same as m:n:1, where n is the length of the array
•	
:n: is the same as 0:n:1
•	
::d: is the same as 0:n:d, where n is the length of the array
All these slicing methods have been referenced with the usage of arrays. This can 
also be applicable to lists. Slicing of one-dimensional arrays is identical to slicing 
a simple list (as one-dimensional arrays can be seen equivalent to a list), and the 
returned type of all the slicing operations matches the array being sliced. The 
following is a simple mechanism that shows array slices:
x = array([5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20])
# interpret like this – default start but end index is 2
y = x[:2]
array([5, 6])
# interpretation – default start and end, but steps of 2
y = x[::2]
array([5,7,9,11,13,15,17,19])

Numerical Computing and Interactive Plotting
[ 626 ]
NumPy attempts to convert data type automatically if an element with one data type is 
inserted into an array with a different data type. For example, if an array has an integer 
data type, place a float into the array results in the float being truncated and store it as 
an integer. This can be dangerous; therefore in such cases, arrays should be initialized 
to contain floats unless a considered decision is taken to use a different data type for 
a good reason. This example shows that even if one element is float and the rest is 
integer, it is assumed to be the float type for the benefit of making it work properly:
a = [1.0, 2,3,6,7]
b = array(a)
b.dtype
dtype('float64')
Slice using flat
Data in matrices are stored in a row-major order, which means elements are indexed 
first by counting along the rows and then down the columns. For example, in the 
following matrix, there are three rows and three columns; the elements are read in 
the order 4,5,6,7,8,9,1,2,3 (for each row, column-wise):
Linear slicing assigns an index to each element of the array in the order of the 
elements read. In two-dimensional arrays or lists, linear slicing works by first 
counting across the rows and then down the columns. In order to use linear slicing, 
you have to use the flat function, as shown in the following code:
a=array([[4,5,6],[7,8,9],[1,2,3]])
b = a.flat[:]
print b
[4, 5, 6, 7, 8, 9, 1, 2, 3]
Array indexing
Elements from NumPy arrays can be selected using four methods: scalar selection, 
slicing, numerical indexing, and logical (or Boolean) indexing. Scalar selection and slicing 
are the basic methods to access elements in an array, which has already been discussed 
here. Numerical indexing and logical indexing are closely related and allows more 
flexible selection. Numerical indexing uses lists or arrays of locations to select elements, 
whereas logical indexing uses arrays that contain Boolean values to select elements.

Chapter 4
[ 627 ]
Numerical indexing
Numerical indexing is an alternative to slice notation. The idea in numerical indexing 
is to use coordinates to select elements. This is similar to slicing. Arrays created using 
numerical indexing create copies of data, whereas slices are only views of data, and 
not copies. For performance sake, slicing should be used. Slices are similar to one-
dimensional arrays, but the shape of the slice is determined by the slice inputs.
Numerical indexing in one-dimensional arrays uses the numerical index values 
as locations in the array (0-based indexing) and returns an array with the same 
dimensions as the numerical index.
Note that the numerical index can be either a list or a NumPy array and must contain 
integer data, as shown in the following code:
a = 10 * arange(4.0)
array([0.,10.,20.,30.])
a[[1]]  # arrays index is list with first element 
array([ 10.])
a[[0,3,2]] # arrays index are 0-th, 3-rd and 2-nd
array([  0.,  30.,  20.])
sel = array([3,1,4,2,3,3])  # array with repetition
a[sel]
array([ 30.  10.   0.  20.  30.  30.])
sel = array([4,1],[3,2]])
a[sel]
array([[ 30.,10.], [ 0.,20.]])
These examples show that the numerical indices determine the element location, and 
the shape of the numerical index array determines the shape of the output.
Similar to slicing, numerical indexing can be combined using the flat function to 
select elements from an array using the row-major ordering of the array. The behavior 
of numerical indexing with flat is identical to that of using numerical indexing on a 
flattened version of the underlying array. A few examples are shown here:
a = 10 * arange(8.0)
array([  0.,  10.,  20.,  30.,  40., 50., 60., 70.])
a.flat[[3,4,1]]
array([ 30., 40., 10.])
a.flat[[[3,4,7],[1,5,3]]]
array([[ 30., 40., 70.], [ 10., 50., 30.]])

Numerical Computing and Interactive Plotting
[ 628 ]
Logical indexing
Logical indexing is different from slicing and numeric indexing; it rather uses logical 
indices to select elements, rows, or columns. Logical indices act as light switches and 
are either true or false. Pure logical indexing uses a logical indexing array with the 
same size as the array being used for selection and always returns a one-dimensional 
array, as shown in the following code:
x = arange(-4,5)
x < 0 
array([True, True, True, True, False, False, False, False, False], 
dtype=bool)
x[x>0]
array([1, 2, 3, 4])
x[abs(x) >= 2]  
array([-4, -3, -2,  2,  3,  4])
#Even for 2-dimension it still does the same
x = reshape(arange(-8, 8), (4,4))
x
array([[-8, -7, -6, -5], [-4, -3, -2, -1], [ 0,  1,  2,  3], [ 4,  5,  
6,  7]])
x[x<0]
array([-8, -7, -6, -5, -4, -3, -2, -1])
Here is another example to demonstrate logical indexing:
from math import isnan
a = [[3, 4, float('NaN')], [5, 9, 8], [3, 3, 2], [9, -1, 
float('NaN')]]
list2 = [3, 4, 5, 6]
list1_valid = [elem for elem in list1 if not any([isnan(element) for 
element in elem])]
list1_valid
[[3, 7, 8], [1, 1, 1]] 
list2_valid = [list2[index] for index, elem in enumerate(list1) if not 
any([isnan(element) for element in elem])]
list2_valid
 [4, 5]

Chapter 4
[ 629 ]
Other data structures
Python has data structures such as stacks, lists, sets, sequences, tuples, lists, heaps, 
arrays, dictionaries, and deque. We have already discussed lists while attempting 
to understand arrays. Tuples are typically more memory efficient than lists because 
they are immutable.
Stacks
The list method is very convenient to be used as a stack, which is known to be 
an abstract data type with the principle of operation last-in, first-out. The known 
operations include adding of an item at the top of the stack using append(), 
extracting of the item from the top of the stack using pop(), and removing of the 
item using remove(item-value), as shown in the following code:
stack = [5, 6, 8]
stack.append(6)
stack.append(8)
stack
[5, 6, 8, 6, 8]
stack.remove(8)
stack
[5, 6, 6, 8]
stack.pop()
8
stack.remove(8)
Traceback (most recent call last): 
File "<ipython-input-339-61d6322e3bb8>", line 1, in <module>     
stack.remove(8)
  ValueError: list.remove(x): x not in list
The pop() function is most efficient (constant-time) because all the other elements 
remain in their location. However, the parameterized version, pop(k), removes the 
element that is at the k < n index of a list, shifting all the subsequent elements to 
fill the gap that results from the removal. The efficiency of this operation is linear 
because the amount of shifting depends on the choice of index k, as illustrated in the 
following image:

Numerical Computing and Interactive Plotting
[ 630 ]
Tuples
A tuple is a sequence of immutable objects that look similar to lists. Tuples are 
heterogeneous data structures, which means that their elements have different 
meanings, whereas lists are a homogeneous sequence of elements. Tuples have 
structure, and lists have order. Some examples of tuples are days of the week, course 
names, and grading scales, as shown in the following code:
#days of the week
weekdays = ("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", 
"Friday", "Saturday")
#course names
courses = ("Chemistry", "Physics", "Mathematics", "Digital Logic", 
"Circuit Theory")
#grades
grades = ("A+", "A", "B+", "B", "C+", "C", "I")
Tuples have immutable objects. This means that you cannot change or remove them 
from tuple. However, the tuple can be deleted completely, for example, "del grades" 
will delete this tuple. After this, if an attempt is made to use that tuple, an error will 
occur. The following are the built-in tuple functions:
•	
cmp(tup1, tup2): This function can be used to compare the elements of two 
tuples
•	
len(tuple): This function can be used to get the total length of the tuple
•	
max(tuple): This function can be used to determine the maximum value in 
the tuple
•	
min(tuple): This function can be used to determine the minimum value in 
the tuple
•	
tuple(lista): This function can be used to convert lista to tuple
Python has a max() function that behaves as expected for numerical values. 
However, if we pass a list of strings, max() returns the item that is the longest.
weekdays = ("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", 
"Friday", "Saturday")
print max(weekdays) 
Wednesday  
Similarly min() has the same behavior for strings.
print min(weekdays) 
Friday

Chapter 4
[ 631 ]
When we need to find how many elements are in an array or list, len() is a 
convenient method that does the job.
len(weekdays)
7
Sets
Sets are similar to lists, but are different in two aspects. Firstly, they are an unordered 
collection as compared to lists (which are ordered by location or index). Secondly, 
they do not have duplicates (if you know the mathematical definition of sets). The 
notation used for a set is shown in the following command:
setoftrees = { 'Basswood', 'Red Pine', 'Chestnut', 'Gray Birch', 
'Black Cherry'} 
newtree = 'Tulip Tree' 
if newtree not in setoftrees:  setoftrees.add(newtree)
Now with this command, you can see what is on setoftrees:
setoftrees  # typing this shows list of elements shown below
{'Basswood', 'Black Cherry', 'Chestnut', 'Gray Birch', 'Red Pine', 
'Tulip Tree'}
Then, build charsinmath and charsinchem using the appropriate spelling, as shown 
in the following code
#example of set of operation on letters
charsinmath = set('mathematics')
charsinchem = set('chem')
Now, let's try to see what the values are in these sets:
Charsinmath # typing this shows letters in charsinmath
{'a', 'c', 'e', 'h', 'i', 'm', 's', 't'}
charsinchem # typing this shows letters in charsinchem
{'c', 'e', 'h', 'm'}
In order to find the set difference, we need to display charsinmath – charsinchem 
as follows:
# take away letters from charsinchem from charsinmath
charsinmath - charsinchem
{'a', 'i', 's', 't'}

Numerical Computing and Interactive Plotting
[ 632 ]
Queues
Just like stacks, it is possible to use a list as a queue. However, the difference is that 
elements can be added or removed from the end of the list or from the beginning of the 
list. Although adding and removing from the end of a list is efficient, doing the same 
from the beginning is not efficient because in this case, elements have to be shifted.
Fortunately, Python has deque in its collections package that efficiently implements 
the adding and removing of elements from both ends using append(), pop(), 
appendleft(), and popleft(), as shown in the following code:
from collections import deque
queue = deque(["Daniel", "Sid", "Mathew",  "Michael"]) 
queue.append("Dirk")      # Dirk arrives 
queue.append("Monte")   # Monte arrives queue
queue
deque(['Daniel', 'Sid', 'Mathew', 'Michael', 'Dirk', 'Monte'])
queue.popleft()  
'Daniel'  
queue.pop() 
'Monte'
queue.appendleft('William')
queue
deque(['William', 'Sid', 'Mathew', 'Michael', 'Dirk'])
queue.append('Lastone')
queue
deque(['William', 'Sid', 'Mathew', 'Michael', 'Dirk', 'Lastone'])
Dictionaries
Dictionaries are a collection of unordered data values that are composed of a  
key/value pair, which has the unique advantage of accessing a value based on the 
key as an index. The question is that if the key is a string, then how does the indexing 
work? The key has to be hashable: a hash function is applied on the key to extract 
the location where the value is stored. In other words, the hash function takes a key 
value and returns an integer. Dictionaries then use these integers (or hash values) to 
store and retrieve the value. Some examples are shown here:

Chapter 4
[ 633 ]
#example 1: Top 10 GDP of Africa
gdp_dict = { 'South Africa': 285.4, 'Egypt': 188.4, 'Nigeria': 173, 
'Algeria': 140.6, 'Morocco': 91.4, 'Angola': 75.5, 'Libya': 62.3, 
'Tunisia': 39.6, 'Kenya': 29.4, 'Ethiopia': 28.5, 'Ghana': 26.2, 
'Cameron': 22.2}
gdp_dict['Angola']
75.5
#example 2: English to Spanish for numbers one to ten
english2spanish = { 'one' : 'uno', 'two' : 'dos', 'three': 'tres', 
'four': 'cuatro', 'five': 'cinvo', 'six': 'seis', 'seven': 'seite',  
'eight': 'ocho', 'nine': 'nueve', 'ten': 'diez'}
english2spanish['four'] 
'cuatro'
The keys should be immutable to have a predictable hash value; otherwise, the hash 
value change will result in a different location. Also, unpredictable things could 
occur. The default dictionary does not keep the values in the order they is inserted; 
therefore, by iterating after the insertion, the order of the key/value pair is arbitrary.
Python's collections package has an equivalent OrderedDict() function that keeps 
the order of pairs in the inserted order. One additional difference between the 
default dictionary and the ordered dictionary is that in the former, equality always 
returns true if they have an identical set of key/value pairs (not necessarily in the 
same order), and in the latter, equality returns true only when they have an identical 
set of key/value pairs and when they are in the same order. The following example 
demonstrates this:
# using default dictionary
dict = {}
dict['cat-ds1'] = 'Introduction to Data Structures'
dict['cat-ds2'] = 'Advanced Data Structures'
dict['cat-la1'] = 'Python Programming'
dict['cat-la2'] = 'Advanced Python Programming'
dict['cat-pda'] = 'Python for Data Analysis'
dict['cat-ps1'] = 'Data Science in Python'
dict['cat-ps2'] = 'Doing Data Science'
for key, val in dict.items():  print key,val
cat-ps1 Data Science in Python 
cat-ps2 Doing Data Science 

Numerical Computing and Interactive Plotting
[ 634 ]
cat-pda Python for Data Analysis 
cat-la2 Advanced Python Programming 
cat-la1 Python Programming 
cat-ds1 Introduction to Data Structures 
cat-ds2 Advanced Data Structures
#using OrderedDict (inserting data the same way as before)
odict = OrderedDict()
odict['cat-ds1'] = 'Introduction to Data Structures'
odict['cat-ds2'] = 'Advanced Data Structures'
odict['cat-la1'] = 'Python Programming'
odict['cat-la2'] = 'Advanced Python Programming'
odict['cat-pda'] = 'Python for Data Analysis'
odict['cat-ps1'] = 'Data Science in Python'
odict['cat-ps2'] = 'Doing Data Science'
for key, val in odict.items():  print key,val
cat-ds1 Introduction to Data Structures 
cat-ds2 Advanced Data Structures 
cat-la1 Python Programming 
cat-la2 Advanced Python Programming 
cat-pda Python for Data Analysis 
cat-ps1 Data Science in Python 
cat-ps2 Doing Data Science
If you have to implement something similar to this, it is computationally better to 
use the ISBN as the key, rather than the catalog number as in a library. However, 
there may be old books that do not have an ISBN; therefore, an equivalent unique 
key/value has to be used to maintain consistency with other new books that have an 
ISBN number. A hash value is usually a number, and with a numeric key, the hash 
function might be much easier compared to an alphanumeric key.
Dictionaries for matrix representation
Usually, there are many examples where you can apply dictionaries when there is 
a key/value association. For example, state abbreviations and names; either one 
could be the key and the other the value, but it would be more efficient to have the 
abbreviation as the key. Other examples are word and word count or city names and 
population. One interesting area of computation where dictionaries could really be 
efficient is the representation of a sparse matrix.

Chapter 4
[ 635 ]
Sparse matrices
Let's examine the space utilization of a matrix; for a 100 x 100 matrix represented 
using a list, each element occupies 4 bytes; therefore, the matrix would need 40,000 
bytes, which is approximately 40 KB of space. However, among these 40,000 bytes, 
if only 100 of them have a nonzero value and the others are all zero, then the space 
is wasted. Now, let's consider a smaller matrix for the simplicity of discussion, as 
shown in the following image:
This matrix has approximately 20 percent of nonzero values; therefore, finding an 
alternative way to represent the nonzero elements of the matrix would be a good 
start. There are seven values of 1, five values of 2 and 3 each, and one value of 4, 6, 
and 7. This could be represented as follows:
A = {1: [(2,2),(6,6), (0,7),(1,8),(7,8),(3,9),(8,9)],
  2: [(5,2),(8,2),(6,3),(0,4),(0,9)],
  3: [(5,0),(8,0),(9,1),(1,3),(5,8)],
  4:[(1,1)], 6:[(2,0)], 7:[(2,5)]} 
However, this representation makes it harder to access the (i,j)th value of A. There 
is a better way to represent this sparse matrix using dictionary, as shown in the 
following code:
def getElement(row, col):
    if (row,col) in A.keys():
       r = A[row,col]
    else:
       r = 0
    return r
A={(0,4): 2, (0,7): 1, (1,1): 4, (1,3):3, (1,8): 1, (2,0): 6, (0,9): 
2, (2,2):1, (2,5): 7, (3,9): 1, (5,0): 3, (5,2): 2, (5,8): 3, (6,3): 
2, (6,6):1, (7,8): 1, (8,0): 3, (8,2): 2, (8,9): 1, (9,1): 3}

Numerical Computing and Interactive Plotting
[ 636 ]
print getElement(1,3)
3
print getElement(1,2)
0
To access an element at (1, 3) of the matrix A, we could use A[(1, 3)], but if the 
key does not exist, it will throw an exception. In order to get the nonzero value 
using the key and return 0 if the key does not exist, we can use a function called 
getElement(), as shown in the preceding code.
Visualizing sparseness
We can visually see how sparse the matrix is with the help of SquareBox diagrams. 
The following image shows the sparseDisplay() function. This uses square boxes 
for each matrix entry that attempts to view the display. The black color represents 
sparseness, whereas the green color represents nonzero elements:
The following code demonstrates how one can display sparseness:
import numpy as np
import matplotlib.pyplot as plt
"""
  SquareBox diagrams are useful for visualizing values of a 2D array,
  Where black color representing sparse areas.  
"""
def sparseDisplay(nonzero, squaresize, ax=None):
    ax = ax if ax is not None else plt.gca()
    ax.patch.set_facecolor('black')
    ax.set_aspect('equal', 'box')
    for row in range(0,squaresize):
      for col in range(0,squaresize):

Chapter 4
[ 637 ]
        if (row,col) in nonzero.keys():
           el = nonzero[(row,col)]
           if el == 0:  color='black' 
           else:  color = '#008000'
           rect = plt.Rectangle([col,row], 1, 1, 
                   facecolor=color, edgecolor=color)
           ax.add_patch(rect)
    ax.autoscale_view()
    ax.invert_yaxis()
if __name__ == '__main__': 
    nonzero={(0,4): 2, (0,7): 1, (1,1): 4, (1,3): 3, (1,8): 1, 
(2,0): 6, (0,9): 2, (2,2): 1, (2,5): 7, (3,9): 1, (5,0): 3, 
(5,2): 2, (5,8): 3, (6,3): 2, (6,6): 1, (7,8): 1, (8,0): 3, (8,2): 2, 
(8,9): 1, (9,1): 3}
    
    plt.figure(figsize=(4,4))
    sparseDisplay(nonzero, 10)
    plt.show()
This is only a quick example to display the sparse matrix. Imagine that you have a 30 
x 30 matrix with only a few nonzero values, then the display would look somewhat 
similar to the following image. The saving in this case is 97 percent as far as space 
utilization is concerned. In other words, the larger the matrix, the lesser the space 
utilized, as shown in the following image:
Having found a way to store the sparse matrix using dictionary, you may have to 
remember that there is no need to reinvent the wheel. Moreover, it makes sense 
to consider the possibility of storing the sparse matrix to understand the power of 
dictionary. However, what is really recommended is to take a look at the SciPy and 
pandas package for the sparse matrix. There may be further opportunities in this 
book to use these approaches in some examples.

Numerical Computing and Interactive Plotting
[ 638 ]
Dictionaries for memoization
Memoization is an optimization technique in computational science that enables one 
to store intermediate results, which otherwise could be expensive. Not every problem 
needs memoization, but when there is a pattern of computing the same values by 
calling the function, it is often useful to use this approach. One example where this 
approach can be used is in the computation of the Fibonacci function using the 
dictionary to store the already computed value, so next time, you can just search for 
the value, rather than recompute it again, as shown in the following code:
fibvalues = {0: 0, 1: 1, 2:1, 3:2, 4:3, 5:5}
def fibonacci(n):
    if n not in fibvalues: 
        sumvalue = fibonacci(n-1) + fibonacci(n-2)
        fibvalues[n] = sumvalue  
    return fibvalues[n]
fibonacci(40)
102334155
print sorted(fibvalues.values())
[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 
1597, 2584, 4181, 6765, 10946, 17711, 28657, 46368, 75025, 121393, 
196418, 317811, 514229, 832040, 1346269, 2178309, 3524578, 5702887, 
9227465, 14930352, 24157817, 39088169, 63245986, 102334155]
#regular fibonacci without using dictionary
def fib(n):
   if n <= 1 : return 1
   sumval = fib(n-1)+fib(n-2)
   return sumval
The dictionary of fibvalues is very useful to prevent the recomputation of the 
values of Fibonacci, but fibcalled is used here only to demonstrate that by using 
dictionary, there cannot be more than one call to fibonacci() for a particular value 
of n. By comparing the ratio of the running times for fib() (without using dictionary 
to store the computed value) and fibonacci(), we can see that when plotted, it 
looks similar to the following screenshot:

Chapter 4
[ 639 ]
from time import time
for nval in range(16,27):
  fibvalues = {0: 0, 1: 1, 2:1, 3:2, 4:3, 5:5}
  t3 = time()
  fibonacci(nval)
  diftime1 = time()-t3
  t2 = time()
  fib(nval)
  diftime2 = time()-t2
  print "The ratio of time-2/time-1 :"+str(diftime2/diftime1)
Tries
Trie (pronounced trie or trai) is a data structure that has different names (digital tree, 
radix tree, or prefix tree). Tries are very efficient for search, insert, and delete functions. 
This data structure is very optimal for storage. For example, when the words add, 
also, algebra, assoc, all, to, trie, tree, tea, and ten are stored in the trie, it will look similar 
to the following diagram:

Numerical Computing and Interactive Plotting
[ 640 ]
The characters are shown in uppercase just for clarity purposes in the preceding 
diagram, whereas in real storage, the characters are stored as they appear in words. 
In the implementation of trie, it makes sense to store the word count. The search 
functionality is very efficient and in particular when the pattern does not match, the 
results are even quicker. In other words, if the search is for are, then the failure is 
determined at the level when the letter r is not found.
One of the popular functionalities is longest prefix matching. In other words, if we 
were to find all the words in the dictionary that have the longest prefix match with a 
particular search string: base (for example). The results could be base, based, baseline, 
or basement, or even more words if they are found in the dictionary of words.
Python has many different implementations: suffix_tree, pytire, trie, datrie, 
and so on. There is a nice comparison study done by J. F. Sebastian that can be 
accessed at https://github.com/zed/trie-benchmark.
Most search engines have an implementation of trie called inverted index. This is the 
central component where space optimization is very important. Moreover, searching 
for this kind of structure is very efficient to find the relevance between  
the search string and the documents. Another interesting application of trie is IP 
routing, where the ability to contain large ranges of values is particularly suitable.  
It also saves space.
A simple implementation in Python (not necessarily the most efficient) is shown in 
the following code:
_end = '_end_'
# to search if a word is in trie
def in_trie(trie, word):
     current_dict = trie
     for letter in word:
         if letter in current_dict:
             current_dict = current_dict[letter]
         else:
             return False
     else:
         if _end in current_dict:
             return True
         else:
             return False
#create trie stored with words
def create_trie(*words):
    root = dict()

Chapter 4
[ 641 ]
    for word in words:
        current_dict = root
        for letter in word:
            current_dict = current_dict.setdefault(letter, {})
        current_dict = current_dict.setdefault(_end, _end)
    return root
def insert_word(trie, word):
    if in_trie(trie, word): return
    current_dict = trie
    for letter in word:
            current_dict = current_dict.setdefault(letter, {})
    current_dict = current_dict.setdefault(_end, _end)
def remove_word(trie, word):
    current_dict = trie
    for letter in word:
        current_dict = current_dict.get(letter, None)
        if current_dict is None:
            # the trie doesn't contain this word.
            break
    else:
        del current_dict[_end]
dict = create_trie('foo', 'bar', 'baz', 'barz', 'bar')
print dict
print in_trie(dict, 'bar')
print in_trie(dict, 'bars')
insert_word(dict, 'bars')
print dict
print in_trie(dict, 'bars')
Visualization using matplotlib
matplotlib has been a popular plotting package besides a few other that are 
available today. The capability of matplotlib is now being realized by the Python 
community. John Hunter, the creator and project leader of this package summed it 
up as matplotlib tries to make easy things easy and hard things possible. You can generate 
very high-quality, publication-ready graphs with very little effort. In this section, we 
will pick a few interesting examples to illustrate the power of matplotlib.

Numerical Computing and Interactive Plotting
[ 642 ]
Word clouds
Word clouds give greater prominence to words that appear more frequently in any 
given text. They are also called tag clouds or weighted words. You can tweak word 
clouds with different fonts, layouts, and color schemes. The significance of a word's 
strength in terms of the number of occurrences visually maps to the size of their 
appearance. In other words, the word that appears the largest in visualization is the 
one that has appeared the most in the text.
Beyond the obvious map to their occurrences, word clouds have several useful 
applications for social media and marketing. Some of the applications are as follows:
•	
Businesses can get to know their customers and how they view their 
products. Some organizations have used a very creative method of asking 
their fans or followers to post words about what they think of their brand, 
taking all these words into a word cloud to better understand the most 
common impressions of their product brand.
•	
Finding ways to learn about their competitors by identifying a brand whose 
online presence is popular. Creating a word cloud from their content to 
better understand what words and themes hook the product target market.
In order to create a word cloud, you can write the Python code or use something 
that already exists. Andreas Mueller from the NYU Center for Data Science created 
a pretty simple and easy-to-use word cloud in Python. It can be installed with the 
instructions given in the next section.
Installing word clouds
For faster installation, you can just use pip with sudo access, as shown in the 
following code:
sudo pip install git+git://github.com/amueller/word_cloud.git
Alternatively, you can obtain the package via wget on Linux or curl on Mac OS with 
the following code:
wget https://github.com/amueller/word_cloud/archive/master.zip
unzip master.zip
rm master.zip 
cd word_cloud-master 
sudo pip install -r requirements.txt

Chapter 4
[ 643 ]
For the Anaconda IDE, you will have to install it using conda with the following 
three steps:
#step-1 command
conda install wordcloud
Fetching package metadata: ....
Error: No packages found in current osx-64 channels matching: wordcloud
You can search for this package on Binstar with
# This only means one has to search the source location
binstar search -t conda wordcloud
Run 'binstar show <USER/PACKAGE>' to get more details:
Packages:
                          Name | Access       | Package Types   | 
     ------------------------- | ------------ | --------------- |
             derickl/wordcloud | public       | conda           |
Found 1 packages
# step-2 command
binstar show derickl/wordcloud
Using binstar api site https://api.binstar.org
Name:    wordcloud
Summary:
Access:  public
Package Types:  conda
Versions:
   + 1.0
To install this package with conda run:
conda install --channel https://conda.binstar.org/derickl wordcloud
# step-3 command
conda install --channel https://conda.binstar.org/derickl wordcloud

Numerical Computing and Interactive Plotting
[ 644 ]
Fetching package metadata: ......
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    cython-0.22                |           py27_0         2.2 MB
    django-1.8                 |           py27_0         3.2 MB
    pillow-2.8.1               |           py27_1         454 KB
    image-1.3.4                |           py27_0          24 KB
    setuptools-15.1            |           py27_1         435 KB
    wordcloud-1.0              |       np19py27_1          58 KB
    conda-3.11.0               |           py27_0         167 KB
    ------------------------------------------------------------
                                           Total:         6.5 MB
The following NEW packages will be INSTALLED:
    django:     1.8-py27_0
    image:      1.3.4-py27_0
    pillow:     2.8.1-py27_1
    wordcloud:  1.0-np19py27_1
The following packages will be UPDATED:
    conda:      3.10.1-py27_0 --> 3.11.0-py27_0
    cython:     0.21-py27_0   --> 0.22-py27_0
    setuptools: 15.0-py27_0   --> 15.1-py27_1
The following packages will be DOWNGRADED:
    libtiff:    4.0.3-0       --> 4.0.2-1
Proceed ([y]/n)? y

Chapter 4
[ 645 ]
Input for word clouds
In this section, there will be two sources where you can extract words to construct 
word clouds. The first example shows how to extract text from the web feeds of 
some known websites and how to extract the words from its description. The second 
example shows how to extract text from tweets with the help of search keywords. 
The two examples will need the feedparser package and the tweepy package, and 
by following similar steps (as mentioned for other packages previously), you can 
easily install them.
Our approach will be to collect words from both these examples and use them as the 
input for a common word cloud program.
Web feeds
There are well grouped and structured RSS or atom feeds in most of the news and 
technology service websites today. Although our aim is to restrict the context to 
technology alone, we can determine a handful of feed lists, as shown in the following 
code. In order to be able to parse these feeds, the parser() method of feedparser 
comes in handy. Word cloud has its own stopwords list, but in addition to this, we 
can also use one while collecting the data, as shown here (stopwords here is not 
complete, but you can gather more from any known resource on the Internet):
import feedparser
from os import path
import re
d = path.dirname(__file__)
mystopwords = [  'test', 'quot', 'nbsp']
feedlist = ['http://www.techcrunch.com/rssfeeds/',
'http://www.computerweekly.com/rss',
'http://feeds.twit.tv/tnt.xml',
'https://www.apple.com/pr/feeds/pr.rss',
'https://news.google.com/?output=rss'
'http://www.forbes.com/technology/feed/'                  'http://rss.
nytimes.com/services/xml/rss/nyt/Technology.xml',         'http://www.
nytimes.com/roomfordebate/topics/technology.rss',
'http://feeds.webservice.techradar.com/us/rss/reviews'            
'http://feeds.webservice.techradar.com/us/rss/news/software',
'http://feeds.webservice.techradar.com/us/rss',
'http://www.cnet.com/rss/',
'http://feeds.feedburner.com/ibm-big-data-hub?format=xml',
'http://feeds.feedburner.com/ResearchDiscussions-DataScien
ceCentral?format=xml',        'http://feeds.feedburner.com/
BdnDailyPressReleasesDiscussions-BigDataNews?format=xml',

Numerical Computing and Interactive Plotting
[ 646 ]
'http://http://feeds.feedburner.com/ibm-big-data-hub-
galleries?format=xml',          'http://http://feeds.feedburner.com/
PlanetBigData?format=xml',
'http://rss.cnn.com/rss/cnn_tech.rss',
'http://news.yahoo.com/rss/tech',
'http://slashdot.org/slashdot.rdf',
'http://bbc.com/news/technology/']          
def extractPlainText(ht):
    plaintxt=''
    s=0
    for char in ht:
        if char == '<': s = 1
        elif char == '>': 
            s = 0
            plaintxt += ' '
        elif s == 0: plaintxt += char
    return plaintxt
    
def separatewords(text):
    splitter = re.compile('\\W*')
    return [s.lower() for s in splitter.split(text) if len(s) > 3]
    
def combineWordsFromFeed(filename):
    with open(filename, 'w') as wfile:
      for feed in feedlist:
        print "Parsing " + feed
        fp = feedparser.parse(feed)
        for e in fp.entries:
          txt = e.title.encode('utf8') + 
               extractPlainText(e.description.encode('utf8'))
          words = separatewords(txt)
          
          for word in words:
            if word.isdigit() == False and word not in mystopwords:
               wfile.write(word)
               wfile.write(" ")
          wfile.write("\n")
    wfile.close()
    return
combineWordsFromFeed("wordcloudInput_FromFeeds.txt")

Chapter 4
[ 647 ]
The Twitter text
In order to access the Twitter API, you will need the access token and consumer 
credentials that consist of four parameters: access_token, access_token_secret, 
consumer_key, and consumer_secret. In order to obtain these keys, you will have 
to use a Twitter account. The steps involved in obtaining these keys are available on 
the Twitter website. The steps involved are:
1.	 Log in to the Twitter account.
2.	 Navigate to developer.twitter.com and use Manage My Apps to follow 
through and obtain the parameters mentioned before.
Assuming that these parameters are ready, with the tweepy package, you can access 
tweets via Python. The following code displays a simple custom stream listener. 
Here, as the tweets are streamed, there is a listener that listens to the status and 
writes the state to a file. This can be used later to create word clouds.
The stream uses a filter to narrow the Twitter text that is focused on the Python 
program, data visualization, big data, machine learning, and statistics. The tweepy stream 
provides the tweets that are extracted. This can run forever because there is unlimited 
data available out there. How do we set it to stop? The accessing speed may be slower 
than you would expect, and for the purposes of creating a word cloud, you would 
imagine that extracting a certain number of tweets is probably sufficient. We therefore 
set a limit and called it MAX_TWEETS to be 50, as shown in the following code:
import tweepy
import json
import sys
import codecs
counter = 0
MAX_TWEETS = 500
#Variables that contains the user credentials to access Twitter API 
access_token = "Access Token"
access_token_secret = "Access Secret"
consumer_key = "Consumer Key"
consumer_secret = "Consumer Secret"
fp = codecs.open("filtered_tweets.txt", "w", "utf-8")
class CustomStreamListener(tweepy.StreamListener):
     def on_status(self, status):
        global counter

Numerical Computing and Interactive Plotting
[ 648 ]
        fp.write(status.text)
        print "Tweet-count:" +str(counter)
        counter += 1
        if counter >= MAX_TWEETS: sys.exit()
    def on_error(self, status):
        print status
if __name__ == '__main__':
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)
    streaming_api = tweepy.streaming.Stream(auth,
               CustomStreamListener(), timeout=60)
    streaming_api.filter(track=['python program', 'statistics', 
             'data visualization', 'big data', 'machine learning'])
Using any bag of words, you can write fewer than 20 lines of the Python code to 
generate word clouds. A word cloud generates an image, and using matplotlib.
pyplot, you can use imshow() to display the word cloud image. The following word 
cloud can be used with any input file of words:
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from os import path
d = path.dirname("__file__")
text = open(path.join(d, 'filtered_tweets.txt')).read()
wordcloud = WordCloud(
    font_path='/Users/MacBook/kirthi/RemachineScript.ttf',
    stopwords=STOPWORDS,
    background_color='#222222',
    width=1000,
    height=800).generate(text)
# Open a plot of the generated image.
plt.figure(figsize=(13,13))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()

Chapter 4
[ 649 ]
The required font file can be downloaded from any of a number of sites  
(one specific resource for this font is available at http://www.dafont.com/
remachine-script.font). Wherever the font file is located, you will have to use this 
exact path set to font_path. For using the data from feeds, there is only one line that 
changes, as shown in the following code:
text = open(path.join(d, 'wordcloudInput_fromFeeds.txt')).read()

Numerical Computing and Interactive Plotting
[ 650 ]
Using the similar idea of extracting text from tweets to create word clouds, you could 
extract text within the context of mobile phone vendors with keywords, such as iPhone, 
Samsung Galaxy, Amazon Fire, LG Optimus, Nokia Lumia, and so on, to determine the 
sentiments of consumers. In this case, you may need an additional set of information, 
that is, the positive and negative sentiment values associated with words.
There are a few approaches that you can follow in a sentiment analysis on tweets in a 
restricted context. First, a very naïve approach would be to just associate weights to 
words that correspond to a positive sentiment as wp and a negative sentiment as wn, 
applying the following notation p(+) as the probability of a positive sentiment and 
p(-) for a negative sentiment:
The second approach would be to use a natural language processing tool and apply 
trained classifiers to obtain better results. TextBlob is a text processing package that 
also has sentiment analysis (http://textblob.readthedocs.org/en/dev).
TextBlob builds a text classification system and creates a training set in the JSON 
format. Later, using this training and the Naïve Bayes classifier, it performs the 
sentiment analysis. We will attempt to use this tool in later chapters to demonstrate 
our working examples.
Plotting the stock price chart
The two biggest stock exchanges in the U.S. are the New York Stock Exchange (NYSE), 
founded in 1792 and the NASDAQ founded in 1971. Today, most stock market trades 
are executed electronically. Even the stocks themselves are almost always held in the 
electronic form, not as physical certificates. There are numerous other websites that also 
provide real-time stock price data, apart from NASDAQ and NYSE.
Obtaining data
One of the websites to obtain data is Yahoo, which provides data via the API, for 
example, to obtain the stock price (low, high, open, close, and volume) of Amazon, 
the URL is http://chartapi.finance.yahoo.com/instrument/1.0/amzn/
chartdata;type=quote;range=3y/csv. Depending on the plotting method you 
select, there is some data conversion that is required. For instance, the data obtained 
from this resource includes date in a format that does not have any format, as shown 
in the following code:

Chapter 4
[ 651 ]
uri:/instrument/1.0/amzn/chartdata;type=quote;range=3y/csv
ticker:amzn
Company-Name:Amazon.com, Inc.
Exchange-Name:NMS
unit:DAY
timestamp:
first-trade:19970516
last-trade:20150430
currency:USD
previous_close_price:231.9000
Date:20120501,20150430
labels:20120501,20120702,20121001,20130102,20130401,20130701,20131001, 
20140102,20140401,20140701,20141001,20150102,20150401
values:Date,close,high,low,open,volume
close:208.2200,445.1000
high:211.2300,452.6500
low:206.3700,439.0000
open:207.4000,443.8600
volume:984400,23856100
20120501,230.0400,232.9700,228.4000,229.4000,6754900
20120502,230.2500,231.4400,227.4000,227.8200,4593400
20120503,229.4500,232.5300,228.0300,229.7400,4055500
...
...
20150429,429.3700,434.2400,426.0300,426.7500,3613300
20150430,421.7800,431.7500,419.2400,427.1100,3609700
We will discuss three approaches in creating the plots. Each one has its own 
advantages and limitations.
In the first approach, with the matplotlib.cbook package and the pylab package, 
you can create a plot with the following lines of code:
from pylab import plotfile show, gca 
import matplotlib.cbook as cbook  
fname = cbook.get_sample_data('/Users/MacBook/stocks/amzn.csv', 
asfileobj=False) 
plotfile(fname, ('date', 'high', 'low', 'close'), subplots=False) 
show()

Numerical Computing and Interactive Plotting
[ 652 ]
This will create a plot similar to the one shown in the following screenshot:
There is one additional programming effort that is required before attempting to plot 
using this approach. The date values have to be formatted to represent 20150430 as 
%d-%b-%Y. With this approach, the plot can also be split into two, one showing the 
stock price and the other showing the volume, as shown in the following code:
from pylab import plotfile show, gca 
import matplotlib.cbook as cbook  
fname = cbook.get_sample_data('/Users/MacBook/stocks/amzn.csv', 
asfileobj=False) 
plotfile(fname, (0,1,5), plotfuncs={f:'bar'}) 
show()

Chapter 4
[ 653 ]
The second approach is to use the subpackages of matplotlib.mlab and 
matplotlib.finance. This has convenient methods to fetch the stock data from 
http://ichart.finance.yahoo.com/table.csv?s=GOOG&a=04&b=12&c=2014&d=0
6&e=20&f=2015&g=d, and to just show a sample, here is a code snippet:
ticker='GOOG'
import matplotlib.finance as finance
import matplotlib.mlab as mlab
import datetime
startdate = datetime.date(2014,4,12)
today = enddate = datetime.date.today()
fh = finance.fetch_historical_yahoo(ticker, startdate, enddate)   
r = mlab.csv2rec(fh); fh.close()
r.sort()
print r[:2]
[ (datetime.date(2014, 4, 14), 538.25, 544.09998, 529.56, 532.52002, 
2568000, 532.52002)  (datetime.date(2014, 4, 15), 536.82001, 
538.45001, 518.46002, 536.44, 3844500, 536.44)]
When you attempt to plot the stock price comparison, it does not make sense to 
display the volume information because for each stock ticker, the volumes are 
different. Also, it becomes too cluttered to view the stock chart.

Numerical Computing and Interactive Plotting
[ 654 ]
matplotlib already has a working example to plot the stock chart, which is 
elaborate enough and includes Relative Strength Indicator (RSI) and Moving 
Average Convergence/Divergence (MACD), and is available at http://
matplotlib.org/examples/pylab_examples/finance_work2.html. For details 
on RSI and MACD, you can find many resources online, but there is one interesting 
explanation at http://easyforextrading.co/how-to-trade/indicators/.
In an attempt to use the existing code, modify it, and make it work for multiple 
charts, a function called plotTicker() was created. This helps in plotting each ticker 
within the same axis, as shown in the following code:
import datetime
import numpy as np
import matplotlib.finance as finance
import matplotlib.dates as mdates
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
startdate = datetime.date(2014,4,12)
today = enddate = datetime.date.today()
plt.rc('axes', grid=True)
plt.rc('grid', color='0.75', linestyle='-', linewidth=0.5)
rect = [0.4, 0.5, 0.8, 0.5]
fig = plt.figure(facecolor='white', figsize=(12,11))
axescolor = '#f6f6f6' # the axes background color
ax = fig.add_axes(rect, axisbg=axescolor)
ax.set_ylim(10,800)
def plotTicker(ticker, startdate, enddate, fillcolor):
  """
     matplotlib.finance has fetch_historical_yahoo() which fetches 
     stock price data the url where it gets the data from is 
     http://ichart.yahoo.com/table.csv stores in a numpy record 
     array with fields: 
      date, open, high, low, close, volume, adj_close
  """
  fh = finance.fetch_historical_yahoo(ticker, startdate, enddate) 
  r = mlab.csv2rec(fh); 
  fh.close()
  r.sort()

Chapter 4
[ 655 ]
  ### plot the relative strength indicator
  ### adjusted close removes the impacts of splits and dividends
  prices = r.adj_close
  ### plot the price and volume data
  
  ax.plot(r.date, prices, color=fillcolor, lw=2, label=ticker)
  ax.legend(loc='top right', shadow=True, fancybox=True)
  # set the labels rotation and alignment 
  for label in ax.get_xticklabels():
    # To display date label slanting at 30 degrees
    label.set_rotation(30)
    label.set_horizontalalignment('right')
  ax.fmt_xdata = mdates.DateFormatter('%Y-%m-%d')
#plot the tickers now
plotTicker('BIDU', startdate, enddate, 'red')
plotTicker('GOOG', startdate, enddate, '#1066ee')
plotTicker('AMZN', startdate, enddate, '#506612')
plt.show()
When you use this to compare the stock prices of Bidu, Google, and Amazon, the 
plot would look similar to the following screenshot:

Numerical Computing and Interactive Plotting
[ 656 ]
Use the following code to compare the stock prices of Twitter, Facebook,  
and LinkedIn:
plotTicker('TWTR', startdate, enddate, '#c72020')
plotTicker('LNKD', startdate, enddate, '#103474')
plotTicker('FB', startdate, enddate, '#506612')
Now, you can add the volume plot as well. For a single ticker plot with volume, use 
the following code:
import datetime
import matplotlib.finance as finance
import matplotlib.dates as mdates
import matplotlib.mlab as mlab
import matplotlib.pyplot as plt
startdate = datetime.date(2013,3,1)
today = enddate = datetime.date.today()
rect = [0.1, 0.3, 0.8, 0.4]   
fig = plt.figure(facecolor='white', figsize=(10,9))  
ax = fig.add_axes(rect, axisbg='#f6f6f6')

Chapter 4
[ 657 ]
def plotSingleTickerWithVolume(ticker, startdate, enddate):
    
    global ax
    fh = finance.fetch_historical_yahoo(ticker, startdate, enddate)
    
    # a numpy record array with fields: 
    #     date, open, high, low, close, volume, adj_close
    r = mlab.csv2rec(fh); 
    fh.close()
    r.sort()
    
    plt.rc('axes', grid=True)
    plt.rc('grid', color='0.78', linestyle='-', linewidth=0.5)
    
    axt = ax.twinx()
    prices = r.adj_close
    fcolor = 'darkgoldenrod'
    ax.plot(r.date, prices, color=r'#1066ee', lw=2, label=ticker)
    ax.fill_between(r.date, prices, 0, prices, facecolor='#BBD7E5')
    ax.set_ylim(0.5*prices.max())
    ax.legend(loc='upper right', shadow=True, fancybox=True)
    
    volume = (r.close*r.volume)/1e6  # dollar volume in millions
    vmax = volume.max()
   
    axt.fill_between(r.date, volume, 0, label='Volume', 
                 facecolor=fcolor, edgecolor=fcolor)
    axt.set_ylim(0, 5*vmax)
    axt.set_yticks([])
    
    for axis in ax, axt:  
        for label in axis.get_xticklabels():
            label.set_rotation(30)
            label.set_horizontalalignment('right')
    
        axis.fmt_xdata = mdates.DateFormatter('%Y-%m-%d')
plotSingleTickerWithVolume ('MSFT', startdate, enddate)
plt.show()

Numerical Computing and Interactive Plotting
[ 658 ]
With the single ticker plot along with volume and the preceding changes in the 
earlier code, the plot will look similar to the following screenshot:
You may also have the option of using the third approach: using the blockspring 
package. In order to install blockspring, you have to use the following pip command:
pip install blockspring
Blockspring's approach is to generate the HTML code. It autogenerates data for the 
plots in the JavaScript format. When this is integrated with D3.js, it provides a very 
nice interactive plot. Amazingly, there are only two lines of code:
import blockspring 
import json  
print blockspring.runParsed("stock-price-comparison", 
   { "tickers": "FB, LNKD, TWTR", 
   "start_date": "2014-01-01", "end_date": "2015-01-01" }).params
Depending on the operating system, when this code is run, it generates the HTML 
code in a default area.

Chapter 4
[ 659 ]
The visualization example in sports
Let's consider a different example here to illustrate the various different approaches 
to visualizing data. Instead of choosing a computational problem, we will restrict 
ourselves to a simple set of data, and show how many different analyses can be done 
that ultimately result in visualizations, to help in clarifying these analyses.
There are several major league sports in North American sports, and we will 
compare four of them: The National Football League (NFL), Major League Baseball 
(MLB), National Basketball Association (NBA), and National Hockey League. NFL 
has a combined team value of 9.13 billion dollars and a total revenue of 9.58 billion 
dollars. We will select this sport with the following data of team values and their 
championships (only part of the data is shown here):

Numerical Computing and Interactive Plotting
[ 660 ]
The team value is one significant factor in comparing different teams, but 
championships also have a value. A simple plot of this data with years completed 
along the x axis, the number of championships along the y axis, and the bubble size 
representing the number of championship per year average would give us something 
similar to the following image:
However, unless you can make it interactive by displaying the labels or details, 
the preceding plot may not be very useful. The preceding plot is possible with 
matplotlib, as shown in the following code:
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(15,10), facecolor='w')
def plotCircle(x,y,radius,color, alphaval):
  circle = plt.Circle((x, y), radius=radius, fc=color,\
   alpha=alphaval)
  fig.gca().add_patch(circle)
  nofcircle = plt.Circle((x, y), radius=radius, ec=color, \
   fill=False)
  fig.gca().add_patch(nofcircle)
x = [55,83,90,13,55,82,96,55,69,19,55,95,62,96,82,30,22,39, \
  54,50,69,56,58,55,55,47,55,20,86,78,56]
y = [5,3,4,0,1,0,1,3,5,2,2,0,2,4,6,0,0,1,0,0,0,0,1,1,0,0,3,0, \
  0,1,0]
r = [23,17,15,13,13,12,12,11,11,10,10,10,10,10,9,9,9,8,8,8,8, \
    8,8,8,7,7,7,7,6,6,6]
for i in range(0,len(x)):
  plotCircle(x[i],y[i],r[i],'b', 0.1)
plt.axis('scaled')
plt.show()

Chapter 4
[ 661 ]
You can even use this numeric data to convert into a format that JavaScript can 
understand (JSON format) so that when integrated with an SVG map, it is possible to 
display the valuation on the map, as shown in the following screenshot:
The preceding map with bubbles would be better if there were associated labels 
displayed. However, due to the lack of space in certain regions of the map, it would 
make much more sense to add an interactive implementation to this map and have 
the information displayed via navigation.
You can refer to the original data source at http://tinyurl.com/oyxk72r.
An alternate source is available at http://www.knapdata.com/python/nfl_
franch.html.
There are several other visualization methods you could apply, apart from the  
plain bubble chart and the bubble chart on maps. One of the visual formats that  
will look cluttered when displaying the statistics of 32 teams would be a pie chart  
or a bar chart.

Numerical Computing and Interactive Plotting
[ 662 ]
It not only looks cluttered, the labels are hardly readable. The whole point in 
showing this pie chart is to illustrate that in this sort of data, one has to seek alternate 
methods of visualization, as shown in the following image:
If we combine a set of teams within a certain range of their team value, then by 
reducing them, we may be able to show them in a more organized fashion, as shown 
in the following image:

Chapter 4
[ 663 ]
The preceding image is one alternative to display the value of teams by segregating 
them into groups, for example, denote 2300 million dollars for $2300,000,000, which 
means 2300 million dollars. This way, the data labels are readable.
Summary
During the last several decades, computing has emerged as a very important 
part of many fields. In fact, the curriculum of computer science in many schools, 
such as Stanford, UC-Berkeley, MIT, Princeton, Harvard, Caltech, and so on, has 
been revised to accommodate interdisciplinary courses because of this change. In 
most scientific disciplines, computational work is an important complement to 
experiments and theory. Moreover, a vast majority of experimental and theoretical 
papers involve some numerical calculations, simulations, or computer modeling.
Python has come a long way, and today the community of Python has grown to the 
extent that there are sources and tools to help write minimal code to accomplish 
almost everything that one may need in computing very efficiently. We could only 
pick a few working examples in this chapter, but in the following chapters, we will 
take a look at more examples.


[ 665 ]
Financial and  
Statistical Models
Financial and economic models primarily help in the simplification and abstraction 
of data and make extensive use of probability and statistics. It's always important 
to take a look at the data; the first step in data analysis should be plotting the data. 
Problems such as bad data, outliers, and missing data can often be detected by 
visualizing data. Bad data should be corrected whenever possible or otherwise 
discarded. However, in some unusual cases, such as in a stock market, outliers are 
good data and should be retained. To summarize, it is important to detect the bad 
data and outliers and to understand them so that appropriate action can be taken. 
The choice of data variables plays an important role in these models.
The selection of variables is important because the nature of a model will often 
determine the facts that are being looked at. For instance, in order to measure 
inflation, a model of behavior is required so that you can understand the real 
changes in price, and the changes in price that directly connect to inflation.
There are many interesting models and their applications that we can discuss, but to 
stay within the scope of this book, we will select some examples. In some cases, such 
as Monte Carlo, we will also select some application in sports. In the later sections, 
we will discuss the following topics:
•	
Monte Carlo simulation—examples applicable in many areas
•	
Price models with examples
•	
Understanding volatility measures with examples
•	
The threshold model—Shelling's model of segregation
•	
Bayesian regression methods with plotting options

Financial and Statistical Models
[ 666 ]
•	
Geometric Brownian, diffusion-based simulation, and portfolio valuation
•	
Profiling and creating real-time interactive plots
•	
Statistical and machine learning overview
Computational finance is a field in computer science and deals with the data and 
algorithms that arise in financial modeling. For some readers, the contents of this 
chapter may be well understood, but for others, looking at these concepts will be 
useful for learning some new insights that may likely be useful in their lives or be 
applicable in their areas of interests.
Before you learn about the applications of Monte Carlo simulation methods, let's  
take a look at a very simple example of investment and gross returns over a period  
of time.
The deterministic model
The ultimate goal of investment is to make a profit, and the revenue from investing 
or loss depends on both the change in prices and the number of assets being held. 
Investors are usually interested in revenues that are highly relative to the size of 
the initial investments. Returns measure this mainly because returns are an asset. 
For example, a stock, a bond, or a portfolio of stocks and bonds are by definition 
expressed as changes, and price is expressed as a fraction of the initial price. Let's 
take a look at the gross returns example.
Gross returns
Let's assume that Pt is the investment amount at time t. A simple gross return is 
expressed as follows: 
1
1
1
t
t
t
P
R
P
+
+
= +
Here, Pt+1 is the returned investment value and the return is Rt+1. For example, if Pt = 
10 and Pt+1 = 10.6, then Rt+1 = 0.06 = 6%. Returns are scale-free, meaning that they do 
not depend on the units, but returns are dependent on the units of t (hour, day, and 
so on). In other words, if t is measured in years, then, as stated more precisely, this 
net return is 6 percent per year.

Chapter 5
[ 667 ]
The gross return over the most recent k years is the product of k single year gross 
returns (from t-k to t), as shown here:
( )
(
)(
)
(
)
1
1
1
2
1
1
1
1
1
1
t
t
t k
t
t
t k
t
t
t k
t
t
t k
P
R k
P
P
P
P
P
P
P
R
R
R
−
−
−+
−
−
−
−
−+
+
=





= 









=
+
+
+
…
…
This is an example of a deterministic model, except that there is one caveat that we 
did not mention: you have to incorporate the inflation rate in the equation per year. 
If we include this in the preceding equation by assuming Ft is the inflation that 
corresponds to the return Rt, we will get the following equation:
( )
(
)
(
)
(
)
(
)
(
)
(
)
1
1
1
1
1
1
1
1
1
1
1
t
t
t k
t
t
t
t k
R
R
R
R k
F
F
F
−
−+
−
−+
+
+
+
+
=
+
+
+
…
If we assume Ft = 0, then the previous equation would be applicable. Assume that we 
do not include inflation and ask this question: "with an initial investment of $10,000 
in 2010 and a return rate of 6%, after how many years will my investment double?"
Let's try to find the answer with the Python program. In this program, we also have 
to add a straight line that is almost similar to y = 2x and see where it intersects the 
curve of the return values plotted on the y axis with the year running on the x axis. 
First, we will plot without the line to determine whether the invested value is almost 
doubled in 12 years. Then, we will calculate the slope of the line m = 10,000/12 = 
833.33. Therefore, we included this slope value of 833.33 in the program to display 
both the return values and the straight line. The following code compares the return 
value overlap with the straight line:
import matplotlib.pyplot as plt
principle_value=10000  #invested amount
grossReturn = 1.06     # Rt
return_amt = []
x = []
y = [10000]
year=2010
return_amt.append(principle_value)
x.append(year)

Financial and Statistical Models
[ 668 ]
for i in range(1,15):
  return_amt.append(return_amt[i-1] * grossReturn)
  print "Year-",i," Returned:",return_amt[i]
  
  year += 1
  x.append(year)
  y.append(833.33*(year-2010)+principle_value)
# set the grid to appear
plt.grid()
# plot the return values curve
plt.plot(x,return_amt, color='r')
plt.plot(x,y, color='b')
Year- 1 Returned: 10600.0
Year- 2 Returned: 11236.0
Year- 3 Returned: 11910.16
Year- 4 Returned: 12624.7696
Year- 5 Returned: 13382.255776
Year- 6 Returned: 14185.1911226
Year- 7 Returned: 15036.3025899
Year- 8 Returned: 15938.4807453
Year- 9 Returned: 16894.78959
Year- 10 Returned: 17908.4769654
Year- 11 Returned: 18982.9855834
Year- 12 Returned: 20121.9647184
Year- 13 Returned: 21329.2826015
Year- 14 Returned: 22609.0395575
After looking at the plot, you would wonder whether there is a way to find out how 
much money the banks that provide mortgage loans make. We'll leave this to you.

Chapter 5
[ 669 ]
An interesting fact is that the curve intersects the line before 2022. At this point, 
the return value is exactly $20,000. However, in 2022, the return value will be 
approximately $20,121. Having looked at the gross returns, is it similar in stocks? 
Many stocks, especially of mature companies, pay dividends that must be accounted 
for in the equation.

Financial and Statistical Models
[ 670 ]
If a dividend (or interest) Dt is paid prior to time t, then the gross return at time t is 
defined as follows:
1
1
t
t
t
t
P
D
R
P−
+
+
=
Another example is a mortgage loan, where a certain amount of loan is 
borrowed from a financial institution at an interest rate. Here, for the purposes of 
understanding the nature of the business, we will select a loan amount of $350,000 at 
an interest rate of 5 percent on a 30-year term. This is a typical example of American 
Mortgage Loan  
(the loan amount and interest rate varies depending on the credit history and the 
market rate of interest of a loan seeker).
A simple interest calculation is known to be P (1 + rt), where P is the principal 
amount, r is the interest rate, and t is the term, so the total amount accrued at the  
end of 30 years is:
5
5
350,000
1
30
350,000
875,000
100
2


×
+
×
=
×
=




It turns out that by the end of 30 years, you would have paid more than twice the 
loan amount (we have not taken the real estate taxes into account in this calculation):
from decimal import Decimal
import matplotlib.pyplot as plt
colors = [(31, 119, 180),(174, 199, 232),(255,128,0),(255, 15, 14),
      (44, 160, 44),(152, 223, 138),(214, 39, 40),(255,173, 61),
      (148, 103, 189),(197, 176, 213),(140, 86, 75),(196, 156, 148),
      (227, 119, 194),(247, 182, 210),(127, 127, 127),
      (199, 199, 199),(188, 189, 34), (219, 219, 141), 
      (23, 190, 207), (158, 218, 229)]
# Scale the RGB values to the [0, 1] range, which is the format 
matplotlib accepts.
for i in range(len(colors)):
    r, g, b = colors[i]
    colors[i] = (r / 255., g / 255., b / 255.)
def printHeaders(term, extra):
    # Print headers
    print "\nExtra-Payment: $"+str(extra)+" Term:"+str(term)+" years"
    print "---------------------------------------------------------"
    print 'Pmt no'.rjust(6), ' ', 'Beg. bal.'.ljust(13), ' ',

Chapter 5
[ 671 ]
    print 'Payment'.ljust(9), ' ', 'Principal'.ljust(9), ' ',
    print 'Interest'.ljust(9), ' ', 'End. bal.'.ljust(13)
    print ''.rjust(6, '-'), ' ', ''.ljust(13, '-'), ' ',
    print ''.rjust(9, '-'), ' ', ''.ljust(9, '-'), ' ',
    print ''.rjust(9, '-'), ' ', ''.ljust(13, '-'), ' '
def amortization_table(principal, rate, term, extrapayment, 
printData=False):
    xarr=[]
    begarr = []
    original_loan = principal
    money_saved=0
    total_payment=0
    payment = pmt(principal, rate, term)
    begBal = principal
    # Print data
    num=1
    endBal=1
    if printData == True: printHeaders(term, extrapayment)
    while  (num < term + 1) and (endBal >0):
        interest = round(begBal * (rate / (12 * 100.0)), 2)
        applied = extrapayment+round(payment - interest, 2)
        endBal = round(begBal - applied, 2)
        if (num-1)%12 == 0 or (endBal < applied+extrapayment):
          begarr.append(begBal)
          xarr.append(num/12)
          if printData == True:
              print '{0:3d}'.format(num).center(6), ' ',
              print '{0:,.2f}'.format(begBal).rjust(13), ' ',
              print '{0:,.2f}'.format(payment).rjust(9), ' ',
              print '{0:,.2f}'.format(applied).rjust(9), ' ',
              print '{0:,.2f}'.format(interest).rjust(9), ' ',
              print '{0:,.2f}'.format(endBal).rjust(13)
        total_payment += applied+extrapayment
        num +=1
        begBal = endBal
    if extrapayment > 0 :
      money_saved = abs(original_loan - total_payment)
      print '\nTotal Payment:','{0:,.2f}'.format(total_payment).
rjust(13)
      print '  Money Saved:','{0:,.2f}'.format(money_saved).rjust(13)
    return xarr, begarr, '{0:,.2f}'.format(money_saved)
def pmt(principal, rate, term):

Financial and Statistical Models
[ 672 ]
    ratePerTwelve = rate / (12 * 100.0)
    result = principal * (ratePerTwelve / (1 - (1 + ratePerTwelve) ** 
(-term)))
    # Convert to decimal and round off to two decimal
    # places.
    result = Decimal(result)
    result = round(result, 2)
    return result
plt.figure(figsize=(18, 14))
#amortization_table(150000, 4, 180, 500)
i=0
markers = ['o','s','D','^','v','*','p','s','D','o','s','D','^','v','*
','p','s','D']
markersize=[8,8,8,12,8,8,8,12,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8]
for extra in range(100,1700,100):
  xv, bv, saved = amortization_table(450000, 5, 360, extra, False)
  if extra == 0:
    plt.plot(xv, bv, color=colors[i], lw=2.2, label='Principal only', 
marker=markers[i], markersize=markersize[i])
  else:
    plt.plot(xv, bv, color=colors[i], lw=2.2, label="Principal 
plus\$"+str(extra)+str("/month, Saved:\$")+saved, marker=markers[i], 
markersize=markersize[i])
  i +=1
plt.grid(True)
plt.xlabel('Years', fontsize=18)
plt.ylabel('Mortgage Balance', fontsize=18)
plt.title("Mortgage Loan For $350,000 With Additional Payment Chart", 
fontsize=20)
plt.legend()
plt.show()
When this program is run, you would get the amortized schedule for every 12 
months for all the cases of extra payment, starting from $100 to $1600. Here is just 
one of those cases:
Extra-Payment: $800 Term: 30 years 
-----------------------------------------------------------------Pmt 
no   Beg. bal.   Payment   Principal   Interest    End. bal.     -----
-   ---------   -------   ---------   ---------   ------
  1      350,000.00  1,878.88* 1,220.55  1,458.33   348,779.45   
 13      335,013.07  1,878.88  1,282.99  1,395.89   333,730.08   

Chapter 5
[ 673 ]
 25      319,259.40  1,878.88  1,348.63  1,330.25   317,910.77   
 37      302,699.75  1,878.88  1,417.63  1,261.25   301,282.12   
 49      285,292.85  1,878.88  1,490.16  1,188.72   283,802.69   
 61      266,995.41  1,878.88  1,566.40  1,112.48   265,429.01   
 73      247,761.81  1,878.88  1,646.54  1,032.34   246,115.27   
 85      227,544.19  1,878.88  1,730.78    948.10   225,813.41   
 97      206,292.20  1,878.88  1,819.33    859.55   204,472.87  
109      183,952.92  1,878.88  1,912.41    766.47   182,040.51  
121      160,470.74  1,878.88  2,010.25    668.63   158,460.49  
133      135,787.15  1,878.88  2,113.10    565.78   133,674.05  
145      109,840.70  1,878.88  2,221.21    457.67   107,619.49  
157       82,566.78  1,878.88  2,334.85    344.03    80,231.93  
169       53,897.49  1,878.88  2,454.31    224.57    51,443.18  
181       23,761.41  1,878.88  2,579.87     99.01    21,181.54  
188        5,474.98  1,878.88  2,656.07     22.81     2,818.91  
189        2,818.91  1,878.88  2,667.13     11.75       151.78  
* $1878.88 includes $1078.88 plus $800 extra payment towards principal
Total Payment: $504,526.47 Money Saved: $154,526.47
Approximately after 15 years 10 months, one can pay off in half the 
time.
The Python code results in the following plot that compares the additional savings 
with principal savings on a mortgage payment:
 

Financial and Statistical Models
[ 674 ]
The preceding plot shows the mortgage balance dropping earlier than the 30 years 
by paying an additional amount against the principal amount.
The monthly payment for a fixed rate mortgage is the amount paid by the borrower 
every month to ensure that the loan is paid in full with interest at the end of its term. 
The monthly payment depends on the interest rate (r) as a fraction, the number of 
monthly payments (N), which is called the loan's term, and the amount borrowed 
(P), which is called the loan's principal; when you rearrange the formula for the 
present value of an ordinary annuity, we get the formula for the monthly payment. 
However, every month, if an extra amount is paid along with the fixed monthly 
payment, then the loan amount can be paid off in a much shorter time.
In the following chart, we have attempted to use the money saved from the program 
and plot that money against the additional amount in the range of $500 to $1300. If 
we see carefully, with an additional amount of $800, you can save almost half the 
loan amount and pay off the loan in half the term.

Chapter 5
[ 675 ]
The preceding plot shows the savings for three different loan amounts, where the 
additional contribution is shown along the x axis and savings in thousands is shown 
along the y axis. The following code uses a bubble chart that also visually shows 
savings  toward additional amount toward the principal on a mortgage loan:
import matplotlib.pyplot as plt
# set the savings value from previous example
yvals1 = [101000,111000,121000,131000,138000, 
143000,148000,153000,158000]
yvals2 = [130000,142000,155000,160000,170000, 
180000,190000,194000,200000]
yvals3 = [125000,139000,157000,171000,183000, 
194000,205000,212000,220000]
xvals = ['500','600','700', '800', '900','1000','1100','1200','1300']
#initialize bubbles that will be scaled 
bubble1 = []
bubble2 = []
bubble3 = []
# scale it on something that can be displayed
# It should be scaled to 1000, but display will be too big 
# so we choose to scale by 5% (divide these by 20 again to relate 
# to real values)
for i in range(0,9): 
  bubble1.append(yvals1[i]/20) 
  bubble2.append(yvals2[i]/20) 
  bubble3.append(yvals3[i]/20) 
#plot yvalues with scaled by bubble sizes
#If bubbles are not scaled, they don't fit well
fig, ax = plt.subplots(figsize=(10,12))
plt1 = ax.scatter(xvals,yvals1, c='#d82730', s=bubble1, alpha=0.5)
plt2 = ax.scatter(xvals,yvals2, c='#2077b4', s=bubble2, alpha=0.5)
plt3 = ax.scatter(xvals,yvals3, c='#ff8010', s=bubble3, alpha=0.5)
#Set the labels and title 
ax.set_xlabel('Extra Dollar Amount', fontsize=16)
ax.set_ylabel('Savings', fontsize=16)
ax.set_title('Mortgage Savings (Paying Extra Every Month)', 
        fontsize=20)
#set x and y limits
ax.set_xlim(400,1450)

Financial and Statistical Models
[ 676 ]
ax.set_ylim(90000,230000)
ax.grid(True)
ax.legend((plt1, plt2, plt3), ('$250,000 Loan', '$350,000 Loan',
     '$450,000 Loan'), scatterpoints=1, loc='upper left', 
      markerscale=0.17, fontsize=10, ncol=1)
fig.tight_layout()
plt.show()
By creating a scatter plot, it is much easier to view which loan category would offer 
more savings compared to others, but to keep it simple, we will compare only three 
loan amounts: $250,000, $350,000, and $450,000.
The following plot is the result of a scatter plot that demonstrates the savings by 
paying extra every month:

Chapter 5
[ 677 ]
The stochastic model
We have discussed the deterministic model, where a single outcome with 
quantitative input values has no randomness. The word stochastic is derived from 
the Greek word called Stochastikos. It means skillful at guessing or chance. The 
antonym of this is "certain", "deterministic", or "sure". A stochastic model predicts a 
set of possible outcomes weighted by their likelihoods or probabilities. For instance, 
a coin when flipped in the air will "surely" land on earth eventually, but whether it 
lands heads or tails is "random".
Monte Carlo simulation
Monte Carlo simulation, which is also viewed as a probability simulation, is a 
technique used to understand the impact of risk and uncertainty in any forecasting 
model. The Monte Carlo method was invented in 1940 by Stanislaw Ulam when 
he was working on several nuclear weapon projects at the Los Alamos National 
Laboratory. Today, with computers, you can generate random numbers and run 
simulations pretty fast, but he amazingly found this method many years ago when 
computing was really hard.
In a forecasting model or any model that plans ahead for the future, there are 
assumptions made. These may be assumptions about the investment return on a 
portfolio, or how long it will take to complete a certain task. As these are future 
projections, the best thing to do is estimate the expected value.
What exactly is Monte Carlo simulation?
Monte Carlo simulation is a method to iteratively evaluate a deterministic model 
with sets of random numbers as inputs. This method is often used when the model 
is complex, nonlinear, or involves more than just a couple of uncertain parameters. 
A simulation can typically involve more than 100,000 or even a million evaluations 
of the model. Let's take a look at the difference between a deterministic model 
and a stochastic model. A deterministic model will have actual inputs that are 
deterministic to produce consistent results, as shown in the following image:
Model
f(x , x , x )
1
2
3
x1
x2
x3
Consistent result
Let's see how a probabilistic model is different from the deterministic model.

Financial and Statistical Models
[ 678 ]
Stochastic models have inputs that are probabilistic and come from a probabilistic 
density function, producing a result that is also probabilistic. Here is how a 
stochastic model looks:
Model
f(x , x , x )
1
2
3
random x1
random x2
random x3
Most likely result
Now, how do we describe the preceding diagram in words?
First, create a model. Let's say that you have determined three random inputs: x1, 
x2, and x3, determined a method: f(x1, x2 , x3), and generated a set of 10,000 random 
values of inputs (in some cases, it could be less or more). Evaluate the model for 
these inputs, repeat it for these 10,000 random inputs, and record these as yi, where i 
runs from 1 to 10,000. Analyze the results and pick one that is most likely.
For instance, if we were to find an answer to the question, that is, "what is the 
probability that the Los Angeles Clippers will win the seventh game?" With some 
random inputs in the context of basketball that make reasonable sense to the 
question, you can find an answer by running Monte Carlo simulation and obtain an 
answer: there is a 45 percent chance that they will win. Well, actually they lost.
Monte Carlo simulation depends heavily on random number generators; therefore, it 
makes sense to figure out what is the fastest and efficient way to perform Monte Carlo 
simulation? Hans Petter Langtangen has performed an outstanding task that shows 
that Monte Carlo simulation can be made much more efficient by porting the code 
to Cython at http://hplgit.github.io/teamods/MC_cython/sphinx/main_MC_
cython.html, where he also compares it with pure C implementation.
Let's consider several examples to illustrate Monte Carlo simulation. The first 
example shows an inventory problem. Later, we will discuss an example in sports 
(Monte Carlo simulation is applicable to many sports analytics.
An inventory problem in Monte Carlo simulation
A fruit retail salesman sells some fruit and places an order for Y units everyday. 
Each unit that is sold gives a profit of 60 cents and units not sold at the end of the 
day are thrown out at a loss of 40 cents per unit. The demand, D, on any given day 
is uniformly distributed on [80, 140]. How many units should the retailer order to 
maximize the expected profit?

Chapter 5
[ 679 ]
Let's denote the profit as P. When you attempt to create equations based on the 
preceding problem description, s denotes the number of units sold, whereas d 
denotes the demand, as shown in the following equation:
(
)
0.6
0.6
0.4
s
if d
s
P
d
s
d
if s
d
≥

= 
−
−
>

Using this representation of profit, the following Python program shows  
maximum profit:
import numpy as np
from math import log
import matplotlib.pyplot as plt
x=[]
y=[]
#Equation that defines Profit
def generateProfit(d):
   global s
   if d >= s: 
     return 0.6*s
   else:
     return 0.6*d - 0.4*(s-d)
# Although y comes from uniform distribution in [80,140]
# we are running simulation for d in [20,305]
maxprofit=0
for s in range (20, 305):
  # Run a simulation for n = 1000 
  # Even if we run for n = 10,000 the result would
  # be almost the same
  for i in range(1,1000):
     # generate a random value of d 
     d = np.random.randint(10,high=200)

Financial and Statistical Models
[ 680 ]
     # for this random value of d, find profit and
     # update maxprofit
     profit = generateProfit(d)
     if profit > maxprofit:
        maxprofit = profit
  
  #store the value of s to be plotted along X axis 
  x.append(s)
  #store the value of maxprofit plotted along Y axis 
  y.append(log(maxprofit)) # plotted on log scale
plt.plot(x,y)
print "Max Profit:",maxprofit
# Will display this
Max Profit: 119.4
The following plot shows that the profit increases when the number of units are sold, 
but when the demand is met, the maximum profit stays constant:

Chapter 5
[ 681 ]
The preceding plot is shown on the log scale, which means that the maximum 
profit was 119.4 from a simulation run for n=1000. Now, let's try to take a look at 
an analytical solution and see how close the simulation result is to the one from the 
analytical method.
As the demand (D) is uniformly distributed in [80,140], the expected profit is derived 
from the following integral:
(
)
(
)
(
)
140
80
140
80
2
2
2
2
0.6
0.4
0.6
60
60
0.4
0.6
60
60
8
160
140
100
600
15
3
7
8
160
5
100
600
15
3
5
29
160
600
15
3
29
29 60
0
116
60
15
15
s
s
s
s
x
s
x
s
Profit =
dx
dx
x
s
s dx
dx
s
s
s
s
s
s
s
s
s
s
s
s
−
−
+
−
=
+
=
−
+
+
−
=
−
+
+
−
= −
+
−
×
−
+
=
⇒
=
=
∫
∫
∫
∫
The answer using the analytical method is 116, and Monte Carlo simulation also 
produces somewhere around this figure: 119. Sometimes, it produces 118 or 116. This 
depends on the number of trials.
Let's consider another simple example and seek an answer to the question: "in a 
classroom full of 30 students, what is the probability that more than one person will 
have the same birthday?" We will assume that it is not a leap year and instead of using 
the month and day, we will just use the days in the calendar year, that is, 365 days. The 
logic is pretty simple. The following code shows how one can calculate the probability 
of more than one person having the same birthday in a room of 30 students:
import numpy as np
numstudents = 30
numTrials = 10000
numWithSameBday = 0
for trial in range(numTrials):
    year = [0]*365
    for i in range(numstudents):
        newBDay = np.random.randint(365)
        year[newBDay] = year[newBDay] + 1
    haveSameBday = False

Financial and Statistical Models
[ 682 ]
    for num in year:
        if num > 1:
           haveSameBday = True
    if haveSameBday == True:
        numWithSameBday = numWithSameBday + 1
prob = float(numWithSameBday) / float(numTrials)
print("The probability of a shared birthday in a class of ", 
numstudents, " is ", prob) 
('The probability of a shared birthday in a class of ', 30, ' is ', 
0.7055)
In other words, there is a 70 percent chance that two people have the same birthday 
in a class of 30 students. The following example illustrates how Monte Carlo 
simulation can be applied to know the likelihood of winning the game in a situation 
that continues to occur more often today than in the past.
Monte Carlo simulation in basketball
Let's consider an example in a basketball game that is being addressed at the Khan 
Academy using JavaScript. The question was, "when down by three and left with 
only 30 seconds, is it better to attempt a hard 3-point shot or an easy 2-point shot and 
get another possession?"(asked by Lebron James).
Most readers may understand the basketball game, but we will only highlight some 
of the important rules. Each team has a 24-second possession of the ball. Within this 
time, if they score (even in less time), the opposing team gets possession for the next 
24 seconds. However, as there is only 30 seconds left, if a player can make a quick 
3-point shot in probably less than 10 seconds, then there is about 20 seconds left for 
the opposing team. Basketball, similar to any other sport, is very competitive, and 
since the player's goal is to reduce the trailing points, it is in their best interest to get 
a 3-point score. Let's try to write a Python program to answer the question.
Before showing the simulation program, let's take a look at some of the parameters 
that are involved in this problem domain. In order to determine whether shooting a 
3-point shot is better in terms of the probability of winning, the following statistics of 
the player and also the opposing team is very important. The threePtPercent and 
twoPtPercent of the player not only determines his strength, but also determines 
the opposing team's percentage of scoring a 2-point labeled oppTwoPtPercent, 
and the opposing team's strength in a free throw percentage which is labeled 
oppFtPercent.

Chapter 5
[ 683 ]
There are other combinations too, but to keep it simple, we will stop here. The higher 
the opposing team's free throw percentage, the more our answer is inclined towards 
making a 3-point shot. You can lower the value of oppFtPercent and see what is 
being discussed here. For this example, we will show snippets in different bits and 
pieces, and you can put them together in whichever way you are comfortable and 
use them to run. First, we will need the NumPy package because here, we intend to 
use the random number generator from this package, and matplotlib is required to 
plot, as shown in the following code:
import numpy as np
import matplotlib.pyplot as plt
In many examples, we will use colors that are standard colors used in tableau, and 
you may want to put this in a separate file. The following array of color codes can be 
used in any visualization plot:
colors = [(31, 119, 180), (174, 199, 232), (255, 127,  14),
   (255, 187, 120), (44, 160, 44), (214,  39,  40), (148,103,189),
   (152, 223, 138), (255,152,150), (197, 176, 213), (140, 86, 75),
   (196, 156, 148), (227,119,194), (247, 182, 210), (127,127,127),
   (199, 199, 199),(188,189, 34),(219, 219, 141), (23, 190,207),
   (158, 218, 229),(217,217,217)]
# Scale RGB values to the [0, 1] range, format matplotlib accepts.
for i in range(len(colors)):
  r, g, b = colors[i]
  colors[i] = (r / 255., g / 255., b / 255.)
Let's take a look at the three-point attempt. If threePtPercent is larger than 
the random number and there is more probability of an overtime, then a win is 
guaranteed. Take a look at the following code:
def attemptThree():
  if np.random.randint(0, high=100) < threePtPercent:
    if np.random.randint(0, high=100) < overtimePercent:
      return True #We won!!
  return False #We either missed the 3 or lost in OT
The logic for the two-point attempt is a little bit involved because it is all about 
how much time is left and who has the possession of the ball. Assuming that on 
an average, it takes only 5 seconds to attempt a two-point shot and the two-point 
scoring percent labeled twoPtPercent of the player is pretty high, then they score a  
two-point shot, which will be deducted from the value in the pointsDown variable. 
The following function is for a two-point scoring attempt:
def attemptTwo():
  havePossession = True
  pointsDown = 3

Financial and Statistical Models
[ 684 ]
  timeLeft = 30
  while (timeLeft > 0):
    #What to do if we have possession
    if (havePossession):
      #If we are down by 3 or more, we take the
      #2 quickly.  If we are down by 2 or less
      #We run down the clock first
      if (pointsDown >= 3):
        timeLeft -= timeToShoot2
      else:
        timeLeft = 0
      #Do we make the shot?
      if (np.random.randint(0, high=100) < twoPtPercent):
        pointsDown -= 2
        havePossession = False
    else:
      #Does the opponent team rebound?
      #If so, we lose possession.
      #This doesn't really matter when we run
      #the clock down
      if (np.random.randint(0, high=100) >= offenseReboundPercent):
        havePossession = False
      else:   #cases where we don't have possession
        if (pointsDown > 0):  #foul to get back possession
          #takes time to foul
          timeLeft -= timeToFoul
          #opponent takes 2 free throws
          if (np.random.randint(0, high=100) < oppFtPercent):
            pointsDown += 1
          if (np.random.randint(0, high=100) < oppFtPercent):
            pointsDown += 1
            havePossession = True
        else:
          if (np.random.randint(0, high=100) >= ftReboundPercent):
            #you were able to rebound the missed ft
            havePossession = True
          else:  
            #tied or up so don't want to foul; 
            #assume opponent to run out clock and take
            if (np.random.randint(0, high=100) < oppTwoPtPercent):

Chapter 5
[ 685 ]
              pointsDown += 2 #They made the 2
            timeLeft = 0
  if (pointsDown > 0):
    return False
  else:
    if (pointsDown < 0):
      return True
    else:
      if (np.random.randint(0, high=100) < overtimePercent):
        return True
      else:
        return False
For the sake of comparison, we will choose five players who have either a good 
3-point average or a 2-point average or both, as shown in the following code:
plt.figure(figsize=(14,14))
names=['Lebron James', 'Kyrie Irving', 'Steph Curry', 
       'Kyle Krover', 'Dirk Nowitzki']
threePercents = [35.4,46.8,44.3,49.2, 38.0]
twoPercents = [53.6,49.1,52.8, 47.0,48.6]
colind=0
for i in range(5):  # can be run individually as well
  x=[]
  y1=[]
  y2=[]
  trials = 400 #Number of trials to run for simulation
  threePtPercent = threePercents[i] # % chance of making 3-pt shot
  twoPtPercent = twoPercents[i] # % chance of making a 2-pt shot
  oppTwoPtPercent = 40 #Opponent % chance making 2-pter
  oppFtPercent = 70 #Opponent's FT %
  timeToShoot2 = 5 #How many seconds elapse to shoot a 2
  timeToFoul = 5 #How many seconds elapse to foul opponent
  offenseReboundPercent = 25 #% of regular offense rebound
  ftReboundPercent = 15 #% of offense rebound after missed FT
  overtimePercent = 50 #% chance of winning in overtime
  winsTakingThree = 0
  lossTakingThree = 0
  winsTakingTwo = 0
  lossTakingTwo = 0

Financial and Statistical Models
[ 686 ]
  curTrial = 1
  while curTrial < trials:
    #run a trial take the 3
    if (attemptThree()):
      winsTakingThree += 1
    else:
      lossTakingThree += 1
      #run a trial taking a 2
      if attemptTwo() == True :
        winsTakingTwo += 1
      else:
        lossTakingTwo += 1
      x.append(curTrial)
      y1.append(winsTakingThree)
      y2.append(winsTakingTwo)
      curTrial += 1
  plt.plot(x,y1, color=colors[colind], label=names[i]+" Wins Taking 
Three Point", linewidth=2)
  plt.plot(x,y2, color=colors[20], label=names[i]+" Wins Taking Two 
Point", linewidth=1.2)
  colind += 2
legend = plt.legend(loc='upper left', shadow=True,)
for legobj in legend.legendHandles:
    legobj.set_linewidth(2.6)
plt.show()
This was run for individual players by setting the range 1 and only including that 
player's name and statistics. In all the cases, as the opponent team's 2-point percent 
was high (70 percent), for all the players, Monte Carlo simulation resulted in 
suggesting wins by taking a 3-point score. Let's take a look at the results when one of 
them is plotted individually and all together.

Chapter 5
[ 687 ]
We have picked players with a reasonably good 3-point percentage from the latest 
statistics available at http://www.basketball-reference.com/teams/. The 
statistics is current as of May 12, 2015. In all the cases, taking an attempt to score 3 
points has a better chance of winning. If the opposing team has a lower average of 
free point throws, then the result will be different.
The following two plots shows the results for an individual player and five chosen 
players from the NBA League (2015):

Financial and Statistical Models
[ 688 ]
The preceding screenshot shows the three-point and two-point attempts by Lebron. 
The following plot shows the attempts of the other four players for comparison:
The volatility plot
We have seen many useful Python packages so far. Time and again, we have seen the 
use of matplotlib, but here, we will show the use of pandas with a very few lines of 
code so that you can achieve financial plots quickly. Standard deviation is a statistical 
term that measures the amount of variation or dispersion around an average. This is 
also a measure of volatility.
By definition, dispersion is the difference between the actual value and the average 
value. For plotting volatility along with the closed values, this example illustrates 
how you can see from a given start date, how a particular stock (such as IBM) is 
performing, and take a look at the volatility with the following code:
import pandas.io.data as stockdata
import numpy as np

Chapter 5
[ 689 ]
r,g,b=(31,  119, 180)
colornow=(r/255.,g/255.,b/255.)
ibmquotes = stockdata.DataReader(name='IBM', data_source='yahoo', 
start='2005-10-1')
ibmquotes['Volatility'] = np.log(ibmquotes['Close']/
    ibmquotes['Close'].shift(1))
ibmquotes[['Close', 'Volatility']].plot(figsize=(12,10), \
    subplots=True, color=colornow) 
The following screenshot is a result of the volatility plot:

Financial and Statistical Models
[ 690 ]
Now, let's see how our volatility is measured. Volatility is the measure of variation 
of price, which can have various peaks. Moreover, the exponential function lets us 
plug in time and gives us growth; logarithm (inverse of exponential) lets us plug in 
growth and gives us the time measure. The following snippet shows logarithm plot 
of measuring volatility:
%time
ibmquotes['VolatilityTest'] = 0.0
for I in range(1, len(ibmquotes)):
  ibmquotes['VolatilityTest'] = 
    np.log(ibmquotes['Close'][i]/ibmquotes['Close'][i-1])
If we time this, the preceding snippet will take the following:
CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns Wall time: 5.01 
µs
To break down and show how we did is using %time and assigning volatility measure 
using the ratio of close value against the change in close value, as shown here:
%time
ibmquotes['Volatility'] = np.log(ibmquotes['Close']/ 
ibmquotes['Close'].shift(1))
If we time this, the preceding snippet will take the following:
CPU times: user 2 µs, sys: 3 µs, total: 5 µs Wall time: 5.01 µs.
The higher the variance in their values, the more volatile it turns out. Before we 
attempt to plot some of the implied volatility of volatility against the exercise price, 
let's take a look at the VSTOXX data. This data can be downloaded from http://
www.stoxx.com or http://www.eurexchange.com/advanced-services/. Sample 
rows of the VSTOXX data is shown here:
            V2TX   V6I1   V6I2   V6I3   V6I4   V6I5   V6I6   V6I7   
V6I8 
  Date                                                                     
2015-05-18  21.01  21.01  21.04    NaN  21.12  21.16  21.34  21.75 
21.84 
2015-05-19  20.05  20.06  20.15  17.95  20.27  20.53  20.83  21.38 
21.50 
2015-05-20  19.57  19.57  19.82  20.05  20.22  20.40  20.63  21.25 
21.44 
2015-05-21  19.53  19.49  19.95  20.14  20.39  20.65  20.94  21.38 
21.55
2015-05-22  19.63  19.55  20.07  20.31  20.59  20.83  21.09  21.59 
21.73

Chapter 5
[ 691 ]
This data file consists of Euro Stoxx volatility indices, which can all be plotted via 
one simple filter mechanism of dates between Dec 31, 2011 to May 1, 2015. The 
following code can be used to plot the VSTOXX data:
import pandas as pd
url = 'http://www.stoxx.com/download/historical_values/h_vstoxx.txt'
vstoxx_index = pd.read_csv(url, index_col=0, header=2,
                           parse_dates=True, dayfirst=True,
                           sep=',')
vstoxx_short = vstoxx_index[('2011/12/31' < vstoxx_index.index)
                            & (vstoxx_index.index < '2015/5/1')]
# to plot all together
vstoxx_short.plot(figsize=(15,14))
When the preceding code is run, it creates a plot that compares Euro Stoxx  
volatility index:

Financial and Statistical Models
[ 692 ]
The preceding plot shows all the indices plot together, but if they are to be plotted on 
separate subplots, you may have to set the following subplots to True:
# to plot in subplots separately
vstoxx_short.plot(subplots=True, grid=True, color='r',
     figsize=(20,20), linewidth=2)

Chapter 5
[ 693 ]
Implied volatilities
The Black–Scholes–Merton model is a statistical model of a financial market. From 
this model, one can find an estimate of the price of European style options. This 
formula is widely used, and many empirical tests have shown that the Black–Scholes 
price is "fairly close" to the observed prices. Fischer Black and Myron Scholes 
first published this model in their 1973 paper, The Pricing of Options and Corporate 
Liabilities. The main idea behind the model is to hedge the option by buying and 
selling the underlying asset in just the right way.
In this model, the price of a European call option on a nondividend paying stock  
is as follows:
(
)
(
)
( )
1
2
2
1
2
2
rT
o
o
o
o
C
S N d
Xe
N d
S
ln
r
T
X
T
d
T
S
ln
r
T
X
T
d
T
N d is standard Normal Distribution
σ
σ
σ
σ
−
=
−



+
+








=



+
−








=
(
)
o
where
S
thestock price
T
timetoexpiration
X
exercise priceor strike price
r
risk freeinterest rate
= standard deviation of log returns volatility
σ
=
=
=
=
For a given European call option, Cg, the implied volatility is calculated from the 
preceding equation (the standard deviation of log returns). The partial derivative 
of the option pricing formula with respect to the volatility is called Vega. This is a 
number that tells in what direction and to what extent the option price will move 
if there is a positive 1 percent change in the volatility and only in the volatility, as 
shown in the following equation:
(
)
1
o
o
C
Vega
S N
d
T
σ
∂
′
=
=
∂

Financial and Statistical Models
[ 694 ]
The volatility model (such as BSM) forecasts the volatility and what the financial uses 
of this model entail, forecasting the characters of the future returns. Such forecasts 
are used in risk management and hedging, market timing, portfolio selection, and 
many other financial activities. American Call or Put Option provides you the right 
to exercise at any time, but for European Call or Put Option, one can exercise only on 
the expiration date.
There is no closed form solution to Black–Scholes–Merton (BSM), but with the 
Newton's method (also known as the Newton-Raphson method), one can obtain an 
approximate value by iteration. Whenever an iterative method is involved, there is 
a certain amount of threshold that goes in to determine the terminating condition 
of the iteration. Let's take a look at the Python code to find the values by iteration 
(Newton's method) and plot them:
(
)
(
)
1
1
1
1
1
1
n
n
n
n
n
n
n
n
n
n
n
n
n
C
C
C
C
C
C
C
C
vega
σ
σ
σ
σ
σ
σ
σ
σ
σ
σ
∗
+
+
∗
+
+
∗
+
+
∂


−
= −

∂
−






−


⇒
−
= −
∂




∂




−
⇒
=
−



from math import log, sqrt, exp
from scipy import stats
import pandas as pd
import matplotlib.pyplot as plt
colors = [(31, 119, 180), (174, 199, 232), (255,128,0), 
 (255, 15, 14), (44, 160, 44), (152, 223, 138), (214, 39, 40), 
 (255, 152, 150),(148, 103, 189), (197, 176, 213), (140, 86, 75),
(196, 156, 148),(227, 119, 194), (247, 182, 210), (127, 127, 127),
(199, 199, 199),(188, 189, 34), (219, 219, 141), (23, 190, 207),
(158, 218, 229)]
# Scale the RGB values to the [0, 1] range, which is the format 
matplotlib accepts.
for i in range(len(colors)):
  r, g, b = colors[i]
  colors[i] = (r / 255., g / 255., b / 255.)
def black_scholes_merton(S, r, sigma, X, T):

Chapter 5
[ 695 ]
  S = float(S) # convert to float
  logsoverx = log (S/X)
  halfsigmasquare = 0.5 * sigma ** 2
  sigmasqrtT = sigma * sqrt(T)
  d1 = logsoverx + ((r + halfsigmasquare) * T) / sigmasqrtT
  d2 = logsoverx + ((r - halfsigmasquare) * T) / sigmasqrtT
  # stats.norm.cdf —> cumulative distribution function
  value = (S * stats.norm.cdf(d1, 0.0, 1.0) – 
     X * exp(-r * T) *   stats.norm.cdf(d2, 0.0, 1.0))
  return value
def vega(S, r, sigma, X, T):
  S = float(S)
  logsoverx = log (S/X)
  halfsigmasquare = 0.5 * sigma ** 2
  sigmasqrtT = sigma * sqrt(T) 
  d1 = logsoverx + ((r + halfsigmasquare) * T) / sigmasqrtT
  vega = S * stats.norm.cdf(d1, 0.0, 1.0) * sqrt(T)
  return vega 
def impliedVolatility(S, r, sigma_est, X, T, Cstar, it):
  for i in range(it):
    numer = (black_scholes_merton(S, r, sigma_est, X, T) - Cstar)
    denom = vega(S,r, sigma_est, X, T)
    sigma_est -= numer/denom
  return sigma_est
We have these functions ready to be used, which can either be used in a separate file 
and imported, or to run this only once, be embedded in code altogether. The input 
file is obtained from stoxx.com in a file called vstoxx_data.h5, as shown in the 
following code:
h5 = pd.HDFStore('myData/vstoxx_data_31032014.h5', 'r')
futures_data = h5['futures_data'] # VSTOXX futures data
options_data = h5['options_data'] # VSTOXX call option data
h5.close()
options_data['IMP_VOL'] = 0.0

Financial and Statistical Models
[ 696 ]
V0 = 17.6639  # the closing value of the index
r=0.04        # risk free interest rate
sigma_est=2
tol = 0.5     # tolerance level for moneyness
Now, let's run the iteration with the options_data and futures_data values form:
for option in options_data.index:
  # iterating over all option quotes
  futureval = futures_data[futures_data['MATURITY'] == 
      options_data.loc[option]['MATURITY']]['PRICE'].values[0]
  # picking the right futures value 
  if (futureval * (1 - tol) < options_data.loc[option]['STRIKE'] 
    < futureval * (1 + tol)):
    impliedVol = impliedVolatility(V0,r,sigma_est,
            options_data.loc[option]['STRIKE'], 
            options_data.loc[option]['TTM'], 
            options_data.loc[option]['PRICE'],  #Cn
            it=100)                             #iterations 
    options_data['IMP_VOL'].loc[option] = impliedVol
  
plot_data = options_data[options_data['IMP_VOL'] > 0]
maturities = sorted(set(options_data['MATURITY']))
plt.figure(figsize=(15, 10))
i=0
for maturity in maturities:
  data = plot_data[options_data.MATURITY == maturity]
  # select data for this maturity
  plot_args = {'lw':3, 'markersize': 9}
  plt.plot(data['STRIKE'], data['IMP_VOL'], label=maturity.date(),
       marker='o', color=colors[i], **plot_args)
  i += 1
plt.grid(True)
plt.xlabel('Strike rate $X$', fontsize=18)
plt.ylabel(r'Implied volatility of $\sigma$', fontsize=18)
plt.title('Short Maturity Window (Volatility Smile)', fontsize=22)
plt.legend()
plt.show()

Chapter 5
[ 697 ]
The following plot is the result of running the preceding program that demonstrates 
implied volatility against the strike rate with data downloaded from http://
vstoxx.com. Alternatively, this can be downloaded at http://knapdata.com/
python/vstoxx_data_31032014.h5. The following plot shows implied volatility 
against the strike rate of Euro Stoxx:
The portfolio valuation
The common sense of portfolio valuation of an entity is to estimate its current 
worth. Valuations are usually applicable on a financial asset or liability and can 
be performed on stocks, options, business enterprises, or intangible assets. For the 
purposes of understanding valuation and their visualization methods, we will pick 
mutual funds and plot them, compare them, and find the correlation.
Let's assume that we value all the portfolios denominated in a single currency.  
This simplifies the aggregation of values in a portfolio significantly.
We will pick three funds from the Vanguard, such as Vanguard US Total (vus.to), 
Vanguard Canadian Capped (vre.to), and Vanguard Emerging Markets (vee.to). 
The following code shows the comparison of three Vanguard funds.
import pandas as pd   #gets numpy as pd.np

Financial and Statistical Models
[ 698 ]
from pandas.io.data import get_data_yahoo
import matplotlib.pyplot as plt
# get data
data = get_data_yahoo(["vus.to","vre.to","vee.to"], 
  start = '2014-01-01')['Adj Close']
data.plot(figsize=(10,10), lw=2) 
plt.show()
There is also another alternative to obtain data using get_data_yahoo() from 
pandas.io.data, as shown in the following screenshot:
Besides plotting them, one may also get the correlation matrix after converting prices 
to log returns in order to scale the values, as shown in the following code:
#convert prices to log returns
retn=data.apply(pd.np.log).diff()
# make corr matrix

Chapter 5
[ 699 ]
retn.corr()
#make scatterplot to show correlation
pd.tools.plotting.scatter_matrix(retn, figsize=(10,10))
plt.show()
# some more stats
retn.skew()
retn.kurt()
# Output
vee.to    0.533157 
vre.to    3.717143 
vus.to    0.906644 
dtype: float64
The correlation plot is shown in the following image. This was obtained using  
the scatter_matrix function from pandas after applying the skew() and  
kurt() correlation:

Financial and Statistical Models
[ 700 ]
The simulation model
A model is a representation of a construction and system's functions. A model is 
similar to the system it represents and is easier to understand. A simulation of a 
system is a working model of the system. The model is usually reconfigurable to 
allow frequent experimentation. The operation of the model can be useful to study 
the model. Simulation is useful before an existing system is built to reduce the 
possibility of failure in order to meet specifications.
When is a particular system suitable for the simulation model? In general, whenever 
there is a need to model and analyze randomness in a system, simulation is the tool 
of choice.
Geometric Brownian simulation
Brownian motion is an example of a random walk, which is widely used to model 
physical processes, such as diffusion and biological processes and social and 
financial processes (such as the dynamics of a stock market).
Brownian motion is a sophisticated method. This is based on a process in plants 
discovered by R. Brown in 1827. It has a range of applications, including modeling 
noise in images, generating fractals, the growth of crystals, and the stock market 
simulation. For the purposes of the relevance of the contents here, we will pick the 
latter, that is, the stock market simulation.
M.F.M Osborne studied the logarithms of common stock prices and the value of 
money and showed that they have an ensemble of impact in statistical equilibrium. 
Using statistics and the prices of stock choice at random times, he was able to 
derive a distribution function that closely resembles the distribution for a particle in 
Brownian motion.
Definition of geometric Brownian motion:
A stochastic process (St) is said to follow a geometric Brownian motion if it satisfies 
the following stochastic differential equation:
t
t
t
t
t
t
t
dS
uS dt
S dW
dS
udt
dW
S
σ
σ
=
+
=
+

Chapter 5
[ 701 ]
Integrating both sides and applying the initial condition: St = So, the solution to the 
preceding equation can be arrived at as follows:
2
2
t
u
t
W
t
o
S
S exp
σ
σ






−
+










=
Using the preceding derivation, we can plug in the values to obtain the following 
Brownian motion:
import matplotlib.pyplot as plt
import numpy as np
'''
Geometric Brownian Motion with drift!
u=drift factor 
sigma: volatility 
T: time span
dt: length of steps
S0: Stock Price in t=0
W: Brownian Motion with Drift N[0,1] 
'''
rect = [0.1, 5.0, 0.1, 0.1]
fig = plt.figure(figsize=(10,10))
T = 2
mu = 0.1
sigma = 0.04
S0 = 20
dt = 0.01
N = round(T/dt)
t = np.linspace(0, T, N)
# Standare normal distrib
W = np.random.standard_normal(size = N) 
W = np.cumsum(W)*np.sqrt(dt) 
X = (mu-0.5*sigma**2)*t + sigma*W
#Brownian Motion 
S = S0*np.exp(X) 
plt.plot(t, S, lw=2)
plt.xlabel("Time t", fontsize=16)

Financial and Statistical Models
[ 702 ]
plt.ylabel("S", fontsize=16)
plt.title("Geometric Brownian Motion (Simulation)",
  fontsize=18)
plt.show()
The result of the Brownian motion simulation is shown in the following screenshot:
The simulating stock prices using Brownian motion is also shown in the  
following code:
import pylab, random
class Stock(object):
    def __init__(self, price, distribution):
        self.price = price
        self.history = [price]
        self.distribution = distribution
        self.lastChange = 0

Chapter 5
[ 703 ]
    def setPrice(self, price):
        self.price = price
        self.history.append(price)
    def getPrice(self):
        return self.price
    def walkIt(self, marketBias, mo):
        oldPrice = self.price
        baseMove = self.distribution() + marketBias
        self.price = self.price * (1.0 + baseMove)
        if mo:
            self.price = self.price + random.gauss(.5, .5)*self.
lastChange
        if self.price < 0.01:
            self.price = 0.0
        self.history.append(self.price)
        self.lastChange = oldPrice - self.price
    def plotIt(self, figNum):
        pylab.figure(figNum)
        pylab.plot(self.history)
        pylab.title('Closing Price Simulation Run-' + str(figNum))
        pylab.xlabel('Day')
        pylab.ylabel('Price')
def testStockSimulation():
    def runSimulation(stocks, fig, mo):
        mean = 0.0
        for s in stocks:
            for d in range(numDays):
                s.walkIt(bias, mo)
            s.plotIt(fig)
            mean += s.getPrice()
        mean = mean/float(numStocks)
        pylab.axhline(mean)
    pylab.figure(figsize=(12,12))
    numStocks = 20
    numDays = 400
    stocks = []
    bias = 0.0
    mo = False
    startvalues = [100,500,200,300,100,100,100,200,200, 300,300,400,50
0,00,300,100,100,100,200,200,300]
    for i in range(numStocks):
        volatility = random.uniform(0,0.2)

Financial and Statistical Models
[ 704 ]
        d1 = lambda: random.uniform(-volatility, volatility)
        stocks.append(Stock(startvalues[i], d1))
    runSimulation(stocks, 1, mo)
testStockSimulation()
pylab.show()
The results of the closing price simulation using random data from the uniform 
distribution is shown in the following screenshot:
The diffusion-based simulation
Stochastic models provide a more detailed understanding of the reaction diffusion 
processes. Such a description is often necessary for the modeling of biological systems. 
There are a variety of simulation models that have been studied, and to restrict 
ourselves within the context of this chapter, we will consider the square-root diffusion.

Chapter 5
[ 705 ]
The square-root diffusion, popularized for finance by Cox, Ingersoll, and Ross (1985) 
is used to model mean reverting quantities (such as interest rates and volatility). The 
stochastic differential equation of this process is as follows:
(
)
|
Driftpart
Diffusion
t
t
t
t
dx
k
x dt
x dW
θ
σ
=
−
+

The values of xt have chi-squared distribution, but in the discrete version, they can be 
approximated by normal distribution. By discrete version, we mean applying Euler's 
numerical method of approximation using the iterative approach, as shown in the 
following equation:
(
)
(
)
(
)
max
,0
max
,0
new
new
t
s
s
s
t
t
t
s
s
t
t
x
x
k
x
t
x
tw
x
x
where x
x
and x
x
θ
σ
+
+
+
+
+
=
+
−
∆+
∆
=
=
=
import numpy as np
import matplotlib.pyplot as plt
import numpy.random as npr
S0 = 100 # initial value
r = 0.05 
sigma = 0.25 
T = 2.0 
x0=0
k=1.8
theta=0.24
i = 100000
M = 50
dt = T / M
def srd_euler():
  xh = np.zeros((M + 1, i))
  x1 = np.zeros_like(xh)
  xh[0] = x0
  x1[0] = x0
  for t in range(1, M + 1):
   xh[t] = (xh[t - 1]
    + k * (theta - np.maximum(xh[t - 1], 0)) * dt
    + sigma * np.sqrt(np.maximum(xh[t - 1], 0)) * np.sqrt(dt) 
    * npr.standard_normal(i))
  x1 = np.maximum(xh, 0)
  return x1

Financial and Statistical Models
[ 706 ]
x1 = srd_euler()
plt.figure(figsize=(10,6))
plt.hist(x1[-1], bins=30, color='#98DE2f', alpha=0.85)
plt.xlabel('value')
plt.ylabel('frequency')
plt.grid(False)
plt.figure(figsize=(12,10))
plt.plot(x1[:, :10], lw=2.2)
plt.title("Square-Root Diffusion - Simulation")
plt.xlabel('Time', fontsize=16)
plt.ylabel('Index Level', fontsize=16)
#plt.grid(True)
plt.show()

Chapter 5
[ 707 ]
The threshold model
A threshold model is any model where some threshold value(s) is/are used to 
distinguish the ranges of values, where the behavior predicted by the model 
converges in some important way. Schelling attempted to model the dynamics of 
segregation, which was motivated when individual interactions by constructing two 
simulation models.
Schelling's Segregation Model
Schelling's Segregation Model (SSM) was first developed by Thomas C. Schelling. 
This model is one of the first constructive models of a system that is capable of  
self-organization.
Schelling experimented by placing pennies and dimes on a chessboard and moving 
them around according to various rules. In his experiment, he used a board 
analogous to a city, a board square to a domicile, and squares to a neighborhood.  
The pennies and dimes (visually different as well) could represent smokers, 
nonsmokers, male, female, executives, nonexecutives, students, or teachers for  
two groups.
The simulation rules specify the termination condition as none of the agents moved from 
their current position because they are happy, which means that agents will move if they 
are not happy.
The Schelling Model is used to simulate the segregation of a classroom, where the 
model shows that segregated patterns can occur even for weak preferences on 
neighboring classmates.
Suppose we have three types of student categories based on their number one 
priority: sports, advanced proficiency academics, and regular, each with type 0, 1,  
and 2 respectively.

Financial and Statistical Models
[ 708 ]
For the purpose of illustration here, we will assume that there are 250 students of 
each type in a high school. Each agent represents a student. These agents all live on 
a single unit square (this can be visualized as a high school building). The position 
of an agent is just a point (x, y), where 0 < x ,y <1. An agent is happy if half or more 
of her 12 nearest neighbors are of the same type (nearest in terms of Euclidean 
distance). The initial position of each agent is an independent draw from a bivariate 
uniform distribution, as shown in the following code:
from random import uniform, seed
from math import sqrt
import matplotlib.pyplot as plt
num = 250             # These many agents of a particular type
numNeighbors = 12     # Number of agents regarded as neighbors
requireSameType = 8   # At least this many neighbors to be same type
seed(10)  # for reproducible random numbers
class StudentAgent:
    def __init__(self, type):
        #Students of different type will be shown in colors
        self.type = type
        self.show_position()
    def show_position(self):
        # position changed by using uniform(x,y)
        self.position = uniform(0, 1), uniform(0, 1)
    def get_distance(self, other):
        #returns euclidean distance between self and other agent.
        a = (self.position[0] - other.position[0])**2
        b = (self.position[1] - other.position[1])**2
        return sqrt(a + b)
    def happy(self, agents):
        "returns True if reqd number of neighbors are the same type."
        distances = []
        for agent in agents:
            if self != agent:
                distance = self.get_distance(agent)
                distances.append((distance, agent))
        distances.sort()

Chapter 5
[ 709 ]
        neighbors = [agent for d, agent in distances[:numNeighbors]]
        numSameType = sum(self.type == agent.type 
            for agent in neighbors)
        return numSameType >= requireSameType
    def update(self, agents):
        "If not happy, randomly choose new positions until happy."
        while not self.happy(agents):
            self.show_position()
def plot_distribution(agents, cycle_num):
    
    x1,y1 = [],[]
    x2,y2 = [],[]
    x3,y3 = [],[]
    
    for agent in agents:
        x, y = agent.position
        if agent.type == 0:
            x1.append(x); y1.append(y)
        elif agent.type == 1:
            x2.append(x); y2.append(y)        
        else:
            x3.append(x); y3.append(y)
    fig, ax = plt.subplots(figsize=(10,10))
    plot_args = {'markersize' : 8, 'alpha' : 0.65, 'markersize': 14}
    ax.set_axis_bgcolor('#ffffff')
    ax.plot(x1, y1, 'o', markerfacecolor='#1b62a5',  **plot_args)
    ax.plot(x2, y2, 'o', markerfacecolor='#279321', **plot_args)
    ax.plot(x3, y3, 'D', markerfacecolor='#fd6610', **plot_args)
    ax.set_title('Iteration {}'.format(cycle_num))
    plt.show()
agents = [StudentAgent(0) for i in range(num)]
agents.extend(StudentAgent(1) for i in range(num))
agents.extend(StudentAgent(2) for i in range(num))
count = 1
terminate=False
while terminate == False:
    plot_distribution(agents, count)
    count += 1
    no_one_moved = True

Financial and Statistical Models
[ 710 ]
    for agent in agents:
        old_position = agent.position
        agent.update(agents)
        if agent.position != old_position:
            no_one_moved = False
    if no_one_moved:
        terminate=True

Chapter 5
[ 711 ]
An overview of statistical and  
machine learning
The field of Artificial Intelligence (AI) is not new, and if we remember thirty years 
ago when we studied AI, except for robotics, there was very little understanding of 
the future this field held back then. Now, especially in the last decade, there has been 
a considerable growth of interest in Artificial Intelligence and machine learning. In 
the broadest sense, these fields aim to 'discover and learn something useful' about 
the environment. The gathered information leads to the discovery of new algorithms, 
which then leads to the question, "how to process high-dimensional data and deal 
with uncertainty"?
Machine learning aims to generate classifying expressions that are simple enough 
to follow by humans. They must mimic human reasoning sufficiently to provide 
insights into the decision process. Similar to statistical approaches, background 
knowledge may be exploited in the development phase. Statistical learning plays a 
key role in many areas of science, and the science of learning plays a key role in the 
fields of statistics, data mining, and artificial intelligence, which intersect with areas 
of engineering and other disciplines.
The difference between statistical and machine learning is that statistics emphasizes 
inference, whereas machine learning emphasizes prediction. When one applies 
statistics, the general approach is to infer the process by which data was generated. 
For machine learning, one would want to know how to predict the future 
characteristics of the data with respect to some variable. There is a lot of overlap 
between statistical learning and machine learning, and often one side of the experts 
argues one way versus the other. Let's leave this debate to the experts and select 
a few areas to discuss in this chapter. Later in the following chapter, there will be 
elaborate examples of machine learning. Here are some of the algorithms:
•	
Regression or forecasting
•	
Linear and quadratic discriminant analysis
•	
Classification
•	
Nearest neighbor
•	
Naïve Bayes
•	
Support vector machines
•	
Decision trees
•	
Clustering

Financial and Statistical Models
[ 712 ]
The algorithms of machine learning are broadly categorized as supervised learning, 
unsupervised learning, reinforced learning, and deep learning. The supervised 
learning method of classification is where the test data is labeled, and like a teacher, 
it gives the classes supervision. Unsupervised learning does not have any labeled 
training data, whereas supervised learning has completely labeled training data. 
Semisupervised learning falls between supervised and unsupervised learning. This 
also makes use of the unlabeled data for training.
As the context of this book is data visualization, we will only discuss a few 
algorithms in the following sections.
K-nearest neighbors
The first machine learning algorithm that we will look at is k-nearest neighbors  
(k-NN). k-NN does not build the model from the training data. It compares a new 
piece of data without a label to every piece of existing data. Then, take the most 
similar pieces of data (the nearest neighbors) and view their labels. Now, look at 
the top k most similar pieces of data from the known dataset (k is an integer and is 
usually less than 20). The following code demonstrates  k-nearest neighbors plot:
from numpy import random,argsort,sqrt
from pylab import plot,show
import matplotlib.pyplot as plt
def knn_search(x, data, K):
  """ k nearest neighbors """
  ndata = data.shape[1]
  K = K if K < ndata else ndata
  # euclidean distances from the other points
  sqd = sqrt(((data - x[:,:ndata])**2).sum(axis=0))
  idx = argsort(sqd) # sorting
  # return the indexes of K nearest neighbors
  return idx[:K]
data = random.rand(2,200) # random dataset
x = random.rand(2,1) # query point
neig_idx = knn_search(x,data,10)
plt.figure(figsize=(12,12))
# plotting the data and the input point
plot(data[0,:],data[1,:],'o,  x[0,0],x[1,0],'o', color='#9a88a1', 

Chapter 5
[ 713 ]
   markersize=20)
# highlighting the neighbors
plot(data[0,neig_idx],data[1,neig_idx],'o', 
  markerfacecolor='#BBE4B4',markersize=22,markeredgewidth=1)
show()
The approach to k-Nearest Neighbors is as follows:
•	
Collecting data using any method
•	
Preparing numeric values that are needed for a distance calculation
•	
Analyzing with any appropriate method
•	
Training none (there is no training involved)
•	
Testing to calculate the error rate
•	
The application takes some action on the calculated k-nearest neighbor 
search and identifies the top k nearest neighbors of a query
In order to test out a classifier, you can start with some known data so that you can 
hide the answer from the classifier and ask the classifier for its best guess.

Financial and Statistical Models
[ 714 ]
Generalized linear models
Regression is a statistical process to estimate the relationships among variables. More 
specifically, regression helps you understand how the typical value of the dependent 
variable changes when any one of the independent variables is varied.
Linear regression is the oldest type of regression that can apply interpolation, but it 
is not suitable for predictive analytics. This kind of regression is sensitive to outliers 
and cross-correlations.
Bayesian regression is a kind of penalized estimator and is more flexible and stable 
than traditional linear regression. It assumes that you have some prior knowledge 
about the regression coefficients, and statistical analysis is applicable in the context of 
the Bayesian inference.
We will discuss a set of methods in which the target value (y) is expected to be a linear 
combination of some input variables (x1, x2, and … xn). In other words, representing the 
target values using notations is as follows:
(
)
1 1
2
2
0
ˆ
ˆ
,
n
o
n
n
o
i
i
i
Predicted value yis givenby
y w x
w
w x
w x
w x
w
w x
=
=
+
+
+
+
=
+∑
…
Now, let's take a look at the Bayesian linear regression model. A logical question one 
may ask is "why Bayesian?" The answer being:
•	
Bayesian models are more ﬂexible
•	
The Bayesian model is more accurate in small samples (may depend on priors)
•	
Bayesian models can incorporate prior information
Bayesian linear regression
First, let's take a look at a graphical model for linear regression. In this model, let's 
say we are given data values—D = ((x1, y1), (x2, y2), … (xn, yn)) —and our goal is to 
model this data and come up with a function, as shown in the following equation:
( )
( )
(
)
( )
(
)
2
0
2
0,
,
T
T
i
i
f x
w
x
w
N
I
Y
N w
x
φ
σ
φ
σ
=
∼
∼

Chapter 5
[ 715 ]
Here, w is a weight vector and each Yi is normally distributed, as shown in the 
preceding equation. Yi are random variables, and with a new variable x to condition 
each of the random variable Yi = yi from the data, we can predict the corresponding y 
for the new variable x, as shown in the following code:
import numpy as np
import matplotlib.pyplot as plt
from scipy import stats
from sklearn.linear_model import BayesianRidge
from sklearn.linear_model import LinearRegression
np.random.seed(0)
n_samples, n_features = 200, 200
X = np.random.randn(n_samples, n_features)  # Gaussian data
# Create weights with a precision of 4.
theta = 4.
w = np.zeros(n_features)
# Only keep 8 weights of interest
relevant_features = np.random.randint(0, n_features, 8)
for i in relevant_features:
    w[i] = stats.norm.rvs(loc=0, scale=1. / np.sqrt(theta))
alpha_ = 50.
noise = stats.norm.rvs(loc=0, scale=1. / np.sqrt(alpha_), size=n_
samples)
y = np.dot(X, w) + noise
# Fit the Bayesian Ridge Regression 
clf = BayesianRidge(compute_score=True)
clf.fit(X, y)
# Plot weights and estimated and histogram of weights
plt.figure(figsize=(11,10))
plt.title("Weights of the model", fontsize=18)
plt.plot(clf.coef_, 'b-', label="Bayesian Ridge estimate")
plt.plot(w, 'g-', label="Training Set Accuracy")
plt.xlabel("Features", fontsize=16)
plt.ylabel("Values of the weights", fontsize=16)
plt.legend(loc="best", prop=dict(size=12))
plt.figure(figsize=(11,10))

Financial and Statistical Models
[ 716 ]
plt.title("Histogram of the weights", fontsize=18)
plt.hist(clf.coef_, bins=n_features, log=True)
plt.plot(clf.coef_[relevant_features], 5 * np.ones(len(relevant_
features)),
         'ro', label="Relevant features")
plt.ylabel("Features", fontsize=16)
plt.xlabel("Values of the weights", fontsize=16)
plt.legend(loc="lower left")
plt.show()
The following two plots are the results of the program:

Chapter 5
[ 717 ]
Creating animated and interactive plots
There are a few tools for interactive plots that one may choose from, such as Bokeh, 
Plotly, and VisPy.
Bokeh allows you to plot matplotlib objects via JavaScript, which enables the 
interactive part easily. For instance, if one needs a map plot that is interactive, Bokeh 
can be used. Bokeh uses JavaScript and enables D3.js style plots and targets the 
visualization via modern web browsers. Bokeh delivers good performance over a 
large dataset. You can easily install bokeh either via conda or pip, as shown in the 
following code:
conda install bokeh
   OR
pip install bokeh
import collections 

Financial and Statistical Models
[ 718 ]
from bokeh.sampledata import us_counties, unemployment
from bokeh.plotting import figure, show, output_file
from bokeh.models import HoverTool
county_coordinate_xs=[
us_counties.data[code]['lons'] for code in us_counties.data
if us_counties.data[code]['state'] == 'ca'
]
county_coordinate_ys=[
us_counties.data[code]['lats'] for code in us_counties.data
if us_counties.data[code]['state'] == 'ca'
]
colors = ["#e6f2ff", "#cce5ff", "#99cbff", "#b2d8ff", "#73abe5", 
"#5985b2"]
county_colors = []
for county_id in us_counties.data:
  if us_counties.data[county_id]['state'] != 'ca':
    continue
  try:
    rate = unemployment.data[county_id]
    idx = min(int(rate/2), 5)
    county_colors.append(colors[idx])
  except KeyError:
    county_colors.append("black")
output_file("california.html", title="california.py example")
TOOLS="pan,wheel_zoom,box_zoom,reset,hover,save"
p = figure(title="California Unemployment 2009", width=1000, 
height=1000, tools=TOOLS)
p.patches(county_coordinate_xs, county_coordinate_ys,
fill_color=county_colors, fill_alpha=0.7,
line_color="white", line_width=0.5)
mouse_hover = p.select(dict(type=HoverTool))
mouse_hover.point_policy = "follow_mouse"
mouse_hover.tooltips = collections.OrderedDict([
("index", "$index"), ("(x,y)", "($x, $y)"),
("fill color", "$color[hex, swatch]:fill_color"),
])
show(p)

Chapter 5
[ 719 ]
In order to view the results, you may have to use a browser to open  
California.html:
Plotly is another option that allows interactive plots, but requires one to be online 
and have a Plotly account. The plots using Plotly look very nice and is interactive. 
The following code shows how one can create interactive plots using plotly:
from pylab import * 
import plotly
#py = plotly.plotly('me', 'mykey')
def to_plotly(ax=None):
    if ax is None:
        ax = gca()
    

Financial and Statistical Models
[ 720 ]
    lines = []
    for line in ax.get_lines():
        lines.append({'x': line.get_xdata(),
                      'y': line.get_ydata(),
                      'name': line.get_label(),
                      })
       
    layout = {'title':ax.get_title(),
              'xaxis':{'title':ax.get_xlabel()},
              'yaxis':{'title':ax.get_ylabel()}
              }
    filename = ax.get_title()  if ax.get_title() != '' else 'Untitled'
    print filename
    close('all')
    #return lines, layout
    return py.iplot(lines,layout=layout, filename = filename)    
plot(rand(100), label = 'trace1')
plot(rand(100)+1, label = 'trace2')
title('Title')
xlabel('X label')
ylabel('Y label ')
response = to_plotly()
response

Chapter 5
[ 721 ]
VisPy is another high performance interactive tool built using Python and  
OpenGL; therefore, it delivers the power of modern GPU's. It is fairly new, and as 
it matures, it leaves users with another good visualization library to choose from. 
The following example shows that using vispy one can create an image that can be 
zoomed interactively:
import sys
from vispy import scene
from vispy import app
import numpy as np
canvas = scene.SceneCanvas(keys='interactive')
canvas.size = 800, 800
canvas.show()
# Set up a viewbox to display the image with interactive pan/zoom
view = canvas.central_widget.add_view()
# Create the image
img_data = np.random.normal(size=(100, 100, 3), loc=128,
                            scale=40).astype(np.ubyte)
image = scene.visuals.Image(img_data, parent=view.scene)
# Set 2D camera (the camera will scale to the contents in the scene)
view.camera = scene.PanZoomCamera(aspect=1)
if __name__ == '__main__' and sys.flags.interactive == 0:
    app.run()

Financial and Statistical Models
[ 722 ]
The preceding image shows the plot that appears the first time, but as we move the 
mouse and zoom in on it, it appears as follows:
Summary
This chapter discussed typical financial examples and looked at machine learning 
towards the end. A brief introduction to the deterministic model using gross profit 
analysis and savings in mortgage payments was discussed. 
Using real-world data in the form of options, the implied volatilities of European 
call options on the VSTOXX volatility index was also discussed. We also looked at 
Monte Carlo simulation. Using different implementation approaches, we showed 
simulation methods using the Monte Carlo method, the inventory problem, and a 
basketball situation.
Further, you learned simulation models (such as geometric Brownian and the 
diffusion-based simulation) with the example of the stock market model. The  
chapter also focused on how diffusion can be used to show drift and volatility.
We also looked at Bayesian linear regression and interactive plotting methods 
that one can choose from. Then, we discussed the k-nearest neighbors algorithm, 
instance-based learning performance, and the machine learning algorithm. This 
example was just touched to generate an interest about the subject and give you an 
idea about these algorithms. However, in the following chapter, we will look at more 
interesting statistical and machine learning algorithms.

[ 723 ]
Statistical and Machine 
Learning
Machine learning enables you to create and use computer algorithms, learn from 
these algorithms, correct them, and improve them to draw any new patterns that 
were unknown in the past. You can also extract insights from these new patterns 
that were found from the data. For instance, one may be interested in teaching a 
computer how to recognize ZIP codes value in an image. Another example is if we 
have a specific task, such as to determine spam messages, then instead of writing a 
program to solve this directly, in this paradigm, you can seek methods to learn and 
become better at getting accurate results using a computer.
Machine learning has become a significant part of artificial intelligence in recent 
years. With the power of computing, it is very likely that we will be able to build 
intelligent systems using machine learning methods. With the power of computing 
that we have today, these tasks have become far simpler than they were two decades 
ago. The primary goal of machine learning is to develop algorithms that have 
promising value in the real world. Besides time and space efficiency, the amount of 
data that is required by these learning algorithms also plays a challenging role. As 
machine learning algorithms are driven by data, you can see why there are so many 
different algorithms already today in this subject area. In the following sections of 
this chapter, we will discuss the following topics with examples:
•	
Classification methods—decision tree and linear and k-nearest neighbors
•	
Naïve Bayes, linear regression, and logistic regression
•	
Support vector machines
•	
Tree-based regression and unsupervised learning
•	
Principal component analysis
•	
Clustering based on similarity
•	
Measuring performance for classification

Statistical and Machine Learning
[ 724 ]
Classification methods
Machine learning algorithms are useful in many real-world applications, for 
example, if someone is interested in making accurate predictions about the climate or 
in the diagnosis of a disease. The learning is usually based on some known behavior 
or observations. This means that machine learning is about learning to improve on 
something in the future based on the experience or observations of the past.
Machine learning algorithms are broadly categorized as supervised learning, 
unsupervised learning, reinforced learning, and deep learning. The supervised 
learning method of classification (where the test data is labeled) is similar to a 
teacher who supervises different classes. Supervised learning relies on the algorithm 
to learn from data when we specify a target variable. Building an accurate classifier 
requires the following features:
•	
A good set of training examples
•	
A reasonably good performance on the training set
•	
A classifier method that is closely related to prior expectations

Chapter 6
[ 725 ]
A binary classifier takes the data items and places them in one of the two classes 
(for higher dimensions, the data items are placed in k classes). The examples of a 
binary classifier determines whether a person's results can be diagnosed with the 
possibility of being positive on some disease or negative. The classifier algorithm 
is probabilistic. With some margin of error, someone can be diagnosed as either 
positive or negative. In any of these algorithms, there is a general approach to 
accomplish this, which goes in the following order:
•	
Collecting data from a reliable source.
•	
Preparing or reorganizing data with a specific structure. For a binary 
classifier, a distance calculation is required.
•	
Analyzing data with any appropriate method.
•	
Training (this is not applicable for a binary classifier).
•	
Testing (calculating the error rate).
In this chapter, the discussion will be to focus on what tools are available to visualize 
the input and results, but there is not much focus on the machine learning concepts. 
For greater depth on this subject, you can refer to the appropriate material. Let's 
take a look at an example and gradually walk through to see the various options to 
choose from.
Understanding linear regression
A simple scenario would be where one would like to predict whether a student 
is likely to be accepted into a college undergraduate program (such as Princeton 
University) based on the data of the GPA score and the SAT score with sample  
data as follows:

Statistical and Machine Learning
[ 726 ]
In order to be able to consider the acceptance versus some score that is a combination 
of the SAT score and the GPA score, just for the purposes of illustrating an example 
here (note that this does not resemble the actual admissions process), we will attempt 
to figure out the line of separation. As the SAT scores vary from 2100 to 2390 along 
the x axis, we can try five values from y=2490 – 2*i*2000. In the following example, 
we have 2150 instead of 2000. GPA along the y axis has extreme values as 3.3 and 5.0; 
therefore, we use the incremental values starting with 3.3 using 3.3+0.2i from one 
extreme and 5.0-0.2i from the other extreme (with a step size of 0.2).
As a first attempt to see how the data visually looks, we will attempt to explore it 
with matplotlib and numpy. Using the SAT and GPA scores in the x and y axes 
and applying the scatter plot, we will attempt to find the line of separation in the 
following example:
import matplotlib.pyplot as plt
import matplotlib as mpl
import numpy as np
mpl.rcParams['axes.facecolor']= '#f8f8f8' 
mpl.rcParams['grid.color'] = '#303030' 
mpl.rcParams['grid.color']= '#303030' 
mpl.rcParams['lines.linestyle'] = '--'
#SAT Score 
x=[2400,2350,2400,2290,2100,2380,2300,2280,2210,2390]
#High school GPA
y=[4.4,4.5,4.2,4.3,4.0,4.1,3.9,4.0,4.3,4.5]
a = '#6D0000'
r = '#00006F' 
#Acceptance or rejections core 
z=[a,a,a,r,r,a,r,r,a,a]
plt.figure(figsize=(11,11))
plt.scatter(x,y,c=z,s=600)
# To see where the separation lies
for i in range(1,5):
  X_plot = np.linspace(2490-i*2,2150+i*2,20)
  Y_plot = np.linspace(3.3+i*0.2,5-0.2*i,20)
  plt.plot(X_plot,Y_plot, c='gray')
plt.grid(True) 
plt.xlabel('SAT Score', fontsize=18) 

Chapter 6
[ 727 ]
plt.ylabel('GPA', fontsize=18) 
plt.title("Acceptance in College", fontsize=20) 
plt.legend()
plt.show()
In the preceding code, we will not perform any regression or classification. This is 
just an attempt to understand how the data visually looks. You can also draw several 
lines of separation to get an intuitive understanding of how linear regression works.
You can see that there is not enough data to apply an accurate way to predict with 
the test data. However, if we attempt to get more data and use some well-known 
packages to apply machine learning algorithms, we can get a better understanding of 
the results. For instance, adding extracurricular activities (such as sports and music).

Statistical and Machine Learning
[ 728 ]
Linear regression
The main goal of using linear regression is to predict a numeric target value. One way 
to do this is to write an equation for the target value with respect to the inputs. For 
example, assume that we are trying to forecast the acceptance rate of a fully rounded 
student who participates in sports and music, but belongs to a low-income family.
One possible equation is acceptance = 0.0015*income + 0.49*(participation_score); this is 
a regression equation. This uses a simple linear regression to predict a quantitative 
response with a single feature. It takes the following form:
0
1
0
1
where yis theresponse
feature
intercept
is thecoefficient for x
y
x
x
β
β
β
β
=
+
=
=
=
Together, β0 and β1 are called the model coefficients. To create our model, you must 
learn the values of these coefficients. Once you've learned these coefficients, you can 
use the model to predict the acceptance rate reasonably.
These coefficients are estimated using the least squares criteria, which means that we 
will find the separating line mathematically and minimize the sum of squared residuals. 
The following is a portion of the data that is used in the following example:

Chapter 6
[ 729 ]
The following Python code shows how one can attempt scatter plots to determine the 
correlation between variables:
from matplotlib import pyplot as pplt
import pandas as pds
import statsmodels.formula.api as sfapi
df = pds.read_csv('/Users/myhomedir/sports.csv', index_col=0)
fig, axs = plt.subplots(1, 3, sharey=True)
df.plot(kind='scatter', x='sports', y='acceptance', ax=axs[0], 
figsize=(16, 8))
df.plot(kind='scatter', x='music', y='acceptance', ax=axs[1])
df.plot(kind='scatter', x='academic', y='acceptance', ax=axs[2])
# create a fitted model in one line
lmodel = sfapi.ols(formula='acceptance ~ music', data=df).fit()
X_new = pd.DataFrame({'music': [df.music.min(), df.music.max()]})
predictions = lmodel.predict(X_new)
df.plot(kind='scatter', x='music', y='acceptance', figsize=(12,12), 
s=50)
plt.title("Linear Regression - Fitting Music vs Acceptance Rate", 
fontsize=20)
plt.xlabel("Music", fontsize=16)
plt.ylabel("Acceptance", fontsize=16)
# then, plot the least squares line

Statistical and Machine Learning
[ 730 ]
As shown in the preceding image, the blue dots are the observed values of (x,y), the 
line that crosses diagonally is the least square fit based on the (x,y) values, and the 
orange lines are the residuals, which are the distances between the observed values 
and the least squares line.
Using statsmodels, pandas, and matplotlib (as shown in the preceding image), 
we can assume that there is some sort of scoring based on how a university rates its 
students' contribution to academics, sports, and music.
To test a classifier, we can start with some known data and not knowing the answer, 
we will seek the answer from the classifier for its best guess. In addition, we can add 
the number of times the classifier was wrong and divide it by the total number of 
tests conducted to get the error rate.

Chapter 6
[ 731 ]
The following is a plot of linear regression derived from the previous Python code.
There are numerous other Python libraries that one can use for linear regression, and 
scikit-learn, seaborn, statsmodels, and mlpy are some of the notable and popular 
libraries among them. There are numerous examples already on the Web that explains 
linear regression with these packages. For details on the scikit-learn package, refer 
to http://scikit-learn.org/stable/modules/generated/sklearn.linear_
model.LinearRegression.html.

Statistical and Machine Learning
[ 732 ]
There is another interesting machine learning model called decision tree learning, 
which can sometimes be referred to as classification tree. Another similar model is 
regression tree. Here, we will see the differences between them and whether one 
makes sense over the other.
Decision tree
Classification trees are used to separate the data into classes belonging to the 
response variable. The response variable usually has two classes: Yes or No (1 or 0) 
and sunny or rain. If the target variable has more than two categories, then C4.5 can 
be applicable. C4.5 improves the ID3 algorithm for the continuous attributes, the 
discrete attributes, and the post construction process.
Similar to most learning algorithms, the classification tree algorithm analyzes a 
training set and then builds a classifier based on that training so that with new data in 
the future, it can classify the training as well as the new data correctly. A test example 
is an input object, and the algorithm must predict an output value. Classification trees 
are used when the response or target variable is categorical in nature.
On the contrary, regression trees are needed when the response variable is 
continuous and not discrete. For example, the predicted price of a product. A 
regression tree is built through binary partitioning. This is an iterative process that 
splits the data into partitions or branches and then continues splitting each partition 
into smaller groups as the method moves up each partition or branch. In other 
words, regression trees are applicable when the problem involves prediction as 
opposed to classification. For more details on this, we recommend you refer to books 
on classification and regression trees.
When the relationship between predictors and response is linear, a standard 
regression tree is more appropriate, and when the relationship between predictors 
and response is nonlinear, then C4.5 should be used. Furthermore, to summarize, 
when the response variable has only two categories, the classification tree algorithm 
should be used.
An example
For a decision tree algorithm to play tennis or golf, one can easily sort down the 
decision process by asking a question, that is, is it raining out there or is it sunny? 
and draw the decision diagram branching out at every question based on the 
answers. The playing nature of the games are almost the same—tennis versus  
golf—and in any sporting event, if it is windy and raining, chances are that there is 
not going to be a game.

Chapter 6
[ 733 ]
For tennis, if the outlook is sunny, but the humidity is high, then it is recommended 
to not play. Similarly, if it is raining and windy, then the whole dynamics of the 
tennis game will be pretty bad. Therefore, chances are that it is no fun playing  
tennis under these conditions as well. The following diagram shows all the  
possible conditions:
Outlook
Wind
Humidity
Sunny
Rain
Overcast
Yes
Strong
Weak
No
Yes
High
Normal
No
Yes
We can also add discrete attributes (such as temperature); for what range of 
temperatures does it not make sense to play tennis? Probably, if the temperature is 
greater than 70 degrees Fahrenheit, that is, if the temperature is hot. We can write the 
rules combining all these as follows:
If (Outlook = Sunny) and (Humidity = High) then play=No
If (Outlook = Rain) and (Wind = Strong) then play=No
If (Outlook = Sunny) and (Humidity = Normal) or
  (Outlook = Overcast) or (Outlook=Rain and Wind=Weak) then play=Yes
With the following training set, we can run the algorithm to select the next best classifier:
Outlook
Temperature
Humidity
Wind
Play?
Sunny
Hot
High
Weak
No
Sunny
Hot
High
Strong
No
Overcast
Hot
High
Weak
Yes
Overcast
Cool
Normal
Strong
Yes
Sunny
Mild
High
Weak
No
Sunny
Cool
Normal
Weak
Yes
Rain
Mild
High
Weak
Yes
Rain
Cool
Normal
Weak
Yes
Rain
Cool
Normal
Strong
No
Rain
Mild
Normal
Weak
Yes

Statistical and Machine Learning
[ 734 ]
Outlook
Temperature
Humidity
Wind
Play?
Sunny
Mild
Normal
Strong
Yes
Overcast
Mild
High
Strong
Yes
Overcast
Hot
Normal
Weak
Yes
Rain
Mild
High
Strong
No
The top down induction of decision trees (ID3) is a method that follows these rules:
•	
Iterate over leaf nodes until stopping condition:
1.	 Identify the best decision attribute for the next node in the traversal.
2.	 Assign that best node from step 1 as the decision attribute.
3.	 For each value of those best nodes, create new descendants of  
those nodes.
4.	 Sort the training data into leaf nodes.
5.	 Stopping condition for iteration:
If the training data is classified within the threshold
One clear distinction between a linear regression and a decision tree algorithm is that 
the decision boundaries are parallel to the axes, for example, if we have two features 
(x1 and x2), then it can only create rules, such as x1 >=5.2, x2 >= 7.2. The advantage the 
decision tree algorithm has is that it is robust to errors, which means that the training 
set could have errors. Also, it doesn't affect the algorithm much.
Using the sklearn package from scikit-learn (scikit-learn.org) and the 
following code, we can plot the decision tree classifier:
from sklearn.externals.six import StringIO
from sklearn import tree
import pydot 
# Four columns from the table above with values
# 1st col - 1 for Sunny, 2 for Overcast, and 3 for Rainy
# 2nd col - 1 for Hot, 2 for Mild, 3 for Cool
# 3rd col – 1 for High and 2 for Normal
# 4th col – 0 for Weak and 1 for Strong
X=[[1,1,1,0],[1,1,1,1],[2,1,1,0],[2,3,2,1],[1,2,1,0],[1,3,2,0],\
[3,2,1,0],[3,3,2,0],[3,3,2,1],[3,2,2,0],[1,2,2,1],[2,2,1,1],\
[2,1,2,0],[3,2,1,0]]  

Chapter 6
[ 735 ]
# 1 for Play and 0 for Don't Play
Y=[0,0,1,1,0,1,1,1,0,1,1,1,1,0] 
clf = tree.DecisionTreeClassifier()
clf = clf.fit(X, Y)
dot_data = StringIO() 
tree.export_graphviz(clf, out_file=dot_data) 
graph = pydot.graph_from_dot_data(dot_data.getvalue()) 
graph.write_pdf("game.pdf")
Use the export functionality of sklearn to be able to convert the tree diagram in the 
form of a graph that looks similar to the following diagram:
In order to create your own tree structure, there is an option of using plotting 
methods from matplotlib. In order to display a tree-like diagram, matplotlib has 
annotation that allows you to create a tree-shaped structure with labels, as shown in 
the following code:
import matplotlib.pyplot as plt
#create nodes here
branchNode = dict(boxstyle="sawtooth", fc="0.8")
leafNode = dict(boxstyle="round4", fc="0.8")
startNode = dict(boxstyle="sawtooth", fc="0.9")

Statistical and Machine Learning
[ 736 ]
def createPlot():
    fig = plt.figure(1, facecolor='white')
    fig.clf()
    createPlot.ax1 = plt.subplot(111, frameon=False) #ticks for demo 
purposes
    plotNode('from here', (0.3,0.8), (0.3, 0.8), startNode)
    plotNode('a decision node', (0.5, 0.1), (0.3, 0.8), branchNode)
    plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)
    plt.show()
...
This is usually an idea of how you can create a tree structure from scratch using 
matplotlib. Basically the preceding example shows the creation of three nodes and 
connecting them to form a small tree. The results of this code are shown as follows:

Chapter 6
[ 737 ]
The Bayes theorem
In order to understand the Bayes theorem first, before we attempt to take a look 
at the Naïve Bayes classification method, we should consider this example. Let's 
assume that among all the people in the U universe, the set of people who have 
breast cancer is set A, and set B is the set of people who had a screening test and were 
unfortunately diagnosed with the result positive for breast cancer. This is shown as 
the overlap region A∩B in the following diagram:
There are two unusual areas that need focus: B – A∩B or people without breast 
cancer and with a positive test on diagnosis and the event A – A∩B or people with 
breast cancer and with a negative test on diagnosis. Now, let's attempt to answer 
whether we know that the test is positive for a randomly selected person. Then, 
what is the probability that the person has breast cancer? This visually translates to 
whether we know that a person is visible in the B area, then what is the probability 
that the same person appears in A∩B? Mathematically, this translates to what is 
probability (A given B). Conditional probability equation is shown here:
(
)
(
)
(
)
( )
|
|
A
B
A
B
U
P A B
B
B
U
P A
B
P A B
P B
∩
∩
=
=
∩
=

Statistical and Machine Learning
[ 738 ]
Similarly, if we know that a randomly selected person has cancer, what is the 
probability that the diagnosis test came out positive? This translates to probability  
(B given A), as shown in the following code:
(
)
(
)
(
)
( )
(
)
(
) ( )
(
) ( )
(
)
(
) ( )
( )
|
|
|
|
|
|
A
B
A
B
U
P B A
A
B
U
P A
B
P B A
P A
P A
B
P B A P A
P A B P B
P B A P A
P A B
P B
∩
∩
=
=
∩
=
⇒
∩
=
=
=
Thus, we derive at the Bayes theorem, where A and B are events with P (B) nonzero.
The Naïve Bayes classifier
The Naive Bayes classifier technique is based on the Bayesian theorem and is 
appropriate when the dimensionality of the input is high. Although it appears to be 
very simple, it is technically better performed than the other classification methods.
(More information is available at http://scikit-learn.org/stable/modules/
naive_bayes.html and http://sebastianraschka.com/Articles/2014_naive_
bayes_1.html).
Let's take a look at the following example that shows objects in red and blue. As 
indicated, the objects shown in red represent the set of people who have breast 
cancer, and the objects shown in blue represent the set of people diagnosed positive 
for breast cancer. Our task is to be able to label any new data, which in this case is 
a new person as they emerge that is based on the existing structure or category of 
objects and identify the group or class that the new data or person belongs to.
In Bayesian, the prior probability is more inclined to be close to the pattern or 
behavior of how the objects are currently characterized. This is mainly due to the fact 
that the word prior is synonymous to previous experience here; therefore, if there 
is a greater percentage of red than blue objects, then this gives us an advantage in 
expecting that the predicted outcome should be higher for it to be red.
The method here is a combination of Naïve Bayes and the k-nearest neighbor 
algorithm. For a pure Naïve Bayes classification, we will discuss another example 
using TextBlob (http://textblob.readthedocs.org/en/dev/).

Chapter 6
[ 739 ]
The following image visually shows a new person as unclassified yet:
Using the prior probability of red and blue, you can calculate the posterior 
probability of x being red or blue, as shown in the following code:
13
21
8
21
in thevicinity
1
13
in thevicinity
3
8
prior probabilityof red
prior probabilityof blue
Number of red
likelihood of x given red
Total number of reds
Number of blue
likelihood of x given blue
Total number of blue
posterior prob
=
=
=
=
=
=
1
13
1
13
21
21
3
8
3
1
8
21
21
7
ability of x being red
posterior probabilityof x being blue
=
×
=
=
×
=
=
The new person is most likely to be classified as one who is diagnosed positive with 
breast cancer.

Statistical and Machine Learning
[ 740 ]
The Naïve Bayes classifier using TextBlob
TextBlob is an interesting library that has a collection of tools for text processing 
purposes. It comes with the API for natural language processing (NLP) tasks,  
such as classification, noun phrase extraction, part-of-speech tagging, and 
sentiment analysis.
There are a few steps involved to make sure that one can use TextBlob. Any library 
that works with NLP needs some corpora; therefore, the following sequence of 
installation and configuration needs to be done before attempting to use this 
interesting library:
•	
Installing TextBlob (either via conda or pip)
•	
Downloading corpora
Installing TextBlob
Using binstar search -t conda textblob, one can find where to install  
it for anaconda users. More details can be found in Appendix, Go Forth and  
Explore Visualization.
Downloading corpora
The following command will let one download corpora:
$ python -m textblob.download_corpora
[nltk_data] Downloading package brown to
[nltk_data]     /Users/administrator/nltk_data...
[nltk_data]   Unzipping corpora/brown.zip.
[nltk_data] Downloading package punkt to
[nltk_data]     /Users/administrator/nltk_data...
[nltk_data]   Unzipping tokenizers/punkt.zip.
[nltk_data] Downloading package wordnet to
[nltk_data]     /Users/administrator/nltk_data...
[nltk_data]   Unzipping corpora/wordnet.zip.
[nltk_data] Downloading package conll2000 to
[nltk_data]     /Users/administrator/nltk_data...
[nltk_data]   Unzipping corpora/conll2000.zip.
[nltk_data] Downloading package maxent_treebank_pos_tagger to
[nltk_data]     /Users/administrator/nltk_data...
[nltk_data]   Unzipping taggers/maxent_treebank_pos_tagger.zip.
[nltk_data] Downloading package movie_reviews to
[nltk_data]     /Users/administrator/nltk_data...
[nltk_data]   Unzipping corpora/movie_reviews.zip.
Finished.

Chapter 6
[ 741 ]
The Naïve Bayes classifier using TextBlob
TextBlob makes it easy to create custom text classifiers. In order to understand this 
better, one may need to do some experimentation with their training and test data.  
In the TextBlob 0.6.0 version, the following classifiers are available:
•	
BaseClassifier
•	
DecisionTreeClassifier
•	
MaxEntClassifier
•	
NLTKClassifier *
•	
NaiveBayesClassifier
•	
PositiveNaiveBayesClassifier
The classifier marked with * is the abstract class that wraps around the  
nltk.classify module.
For sentiment analysis, one can use the Naive Bayes classifier and train the system 
with this classifier and textblob.en.sentiments.PatternAnalyzer. A simple 
example is as follows:
from textblob.classifiers import NaiveBayesClassifier
from textblob.blob import TextBlob
from textblob.classifiers import NaiveBayesClassifier
from textblob.blob import TextBlob
train = [('I like this new tv show.', 'pos'),
  # similar train sentences with sentiments goes here]
test = [ ('I do not enjoy my job', 'neg'),
  # similar test sentences with sentiments goes here]
]
cl = NaiveBayesClassifier(train)
cl.classify("The new movie was amazing.") # shows if pos or neg
cl.update(test)
# Classify a TextBlob
blob = TextBlob("The food was good. But the service was horrible. "
                "My father was not pleased.", classifier=cl)
print(blob)
print(blob.classify())

Statistical and Machine Learning
[ 742 ]
for sentence in blob.sentences:
    print(sentence)
    print(sentence.classify())
Here is the result that will be displayed when the preceding code is run:
pos
neg
The food was good.
pos
But the service was horrible.
neg
My father was not pleased.
pos
One can read the training data from a file either in the text format or the JSON 
format. The sample data in the JSON file is shown here:
[
  {"text": "mission impossible three is awesome btw","label": "pos"},
  {"text": "brokeback mountain was beautiful","label":"pos"},
  {"text": " da vinci code is awesome so far","label":"pos"},
  {"text": "10 things i hate about you + a knight's tale * brokeback 
mountain","label":"neg"},
  {"text": "mission impossible 3 is amazing","label":"pos"},
    {"text": "harry potter = gorgeous","label":"pos"},  
    {"text": "i love brokeback mountain too: ]","label":"pos"},
]
from textblob.classifiers import NaiveBayesClassifier
from textblob.blob import TextBlob
from nltk.corpus import stopwords
stop = stopwords.words('english')
pos_dict={}
neg_dict={}
with open('/Users/administrator/json_train.json', 'r') as fp: 
     cl = NaiveBayesClassifier(fp, format="json")
print "Done Training"
rp = open('/Users/administrator/test_data.txt','r')
res_writer = open('/Users/administrator/results.txt','w')
for line in rp:
    linelen = len(line)

Chapter 6
[ 743 ]
    line = line[0:linelen-1]
    sentvalue = cl.classify(line)
    blob = TextBlob(line)
    sentence = blob.sentences[0]
    for word, pos in sentence.tags:
       if (word not in stop) and (len(word)>3 \
            and sentvalue == 'pos'): 
         if pos == 'NN' or pos == 'V':  
           pos_dict[word.lower()] = word.lower()
       if (word not in stop) and (len(word)>3 \
            and sentvalue == 'neg'): 
         if pos == 'NN' or pos == 'V':  
           neg_dict[word.lower()] = word.lower()
    res_writer.write(line+" => sentiment "+sentvalue+"\n")
    #print(cl.classify(line))
print "Lengths of positive and negative sentiments",len(pos_dict), 
len(neg_dict)  
Lengths of positive and negative sentiments 203 128 
We can add more training data from the corpus and evaluate the accuracy of the 
classifier with the following code:
test=[
("mission impossible three is awesome btw",'pos'),
("brokeback mountain was beautiful",'pos'),
("that and the da vinci code is awesome so far",'pos'),
("10 things i hate about you =",'neg'),
("brokeback mountain is a spectacularly beautiful movie",'pos'),
("mission impossible 3 is amazing",'pos'),
("the actor who plays harry potter sucks",'neg'),
("harry potter = gorgeous",'pos'),
('The beer was good.', 'pos'),
('I do not enjoy my job', 'neg'),
("I ain't feeling very good today.", 'pos'),
("I feel amazing!", 'pos'),
('Gary is a friend of mine.', 'pos'),
("I can't believe I'm doing this.", 'pos'),
("i went to see brokeback mountain, which is beautiful(",'pos'),
("and i love brokeback mountain too: ]",'pos')
]
print("Accuracy: {0}".format(cl.accuracy(test)))

Statistical and Machine Learning
[ 744 ]
from nltk.corpus import movie_reviews
reviews = [(list(movie_reviews.words(fileid)), category)
for category in movie_reviews.categories()
for fileid in movie_reviews.fileids(category)]
new_train, new_test = reviews[0:100], reviews[101:200]
cl.update(new_train)
accuracy = cl.accuracy(test + new_test)
print("Accuracy: {0}".format(accuracy))
# Show 5 most informative features
cl.show_informative_features(4)
The output would be as follows:
Accuracy: 0.973913043478 
Most Informative Features        
contains(awesome) = True         pos : neg    =     51.9 : 1.0 
contains(with) = True            neg : pos    =     49.1 : 1.0 
contains(for) = True             neg : pos    =     48.6 : 1.0 
contains(on) = True              neg : pos    =     45.2 : 1.0 
First, the training set had 250 samples with an accuracy of 0.813 and later it added 
another 100 samples from movie reviews. The accuracy went up to 0.974. We 
therefore attempted to use different test samples and plotted the sample size versus 
accuracy, as shown in the following graph:

Chapter 6
[ 745 ]
Viewing positive sentiments using word 
clouds
Word clouds give greater prominence to words that appear more frequently in any 
given text. They are also called tag clouds or weighted words. The significance of a 
word's strength in terms of its number of occurrences visually maps to the size of its 
appearance. In other words, the word that appears the largest in visualization is the 
one that has appeared the most in the text.
Beyond showing the occurrences of the words in shapes and colors, word clouds 
have several useful applications for social media and marketing as follows:
•	
Businesses could get to know their customers and how they view their 
products. Some organizations have used a very creative way of asking their 
fans or followers to post words about what that they think of their brand, 
taking all these words to a word cloud to understand what the most common 
impressions of their product brand are.
•	
Finding ways to know competitors by identifying a brand whose online 
presence is popular. Creating a word cloud from their content to better 
understand what words and themes hook the product target market.
In order to create a word cloud, one can write the Python code or use something  
that already exists. Andreas Mueller from NYU Center for Data Science  
created a word cloud in Python. This is pretty simple and easy to use.  
The RemachineScript.ttf font file can be downloaded from  
http://www.fonts101.com/fonts/view/Script/63827/Remachine_Script.
STOPWORDS consist of extremely common words, for example a, an, the, is, was, 
at, in, and many more. The following code creates a word cloud using a list of 
STOPWORDS in order to ignore them:
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
from os import path
d = path.dirname("__file__")
text = open(path.join(d, '/Users/MacBook/kirthi/results.txt')).read()
wordcloud = WordCloud(
    font_path='/Users/MacBook/kirthi/RemachineScript.ttf',
    stopwords=STOPWORDS,
    background_color='#222222',
    width=1000,
    height=800).generate(text)

Statistical and Machine Learning
[ 746 ]
In order to plot this, first set the figure size and use imshow() that will display the 
word cloud as an image.
# Open a plot of the generated image.
plt.figure(figsize=(13,13))
plt.imshow(wordcloud)
plt.axis("off")
plt.show()
To summarize, we will first extract the sentiments from the TextBlob example and 
assume that the extracted results are in results.txt. Then, we will use these words 
to visualize data as a word cloud with the matplotlib package.
The results of wordcloud are shown in the following image:

Chapter 6
[ 747 ]
k-nearest neighbors
The k-nearest neighbor (k-NN) classification is one of the easiest classification 
methods to understand (particularly when there is little or no prior knowledge 
about the distribution of the data). The k-nearest neighbor classification has a way 
to store all the known cases and classify new cases based on a similarity measure 
(for example, the Euclidean distance function). The k-NN algorithm is popular in its 
statistical estimation and pattern recognition because of its simplicity.
For 1-nearest neighbor (1-NN), the label of one particular point is set to be the 
nearest training point. When you extend this for a higher value of k, the label of a 
test point is the one that is measured by the k nearest training points. The k-NN 
algorithm is considered to be a lazy learning algorithm because the optimization is 
done locally, and the computations are delayed until classification.
There are advantages and disadvantages of this method. The advantages are high 
accuracy, insensitive to outliers, and no assumptions about data. The disadvantages 
of k-NN is that it is computationally expensive and requires a lot of memory.
One of the following distance metrics could be used:
(
)
(
)
2
1
1
1
1
k
i
i
i
k
i
i
i
k
q
q
i
i
i
Euclidean Distance
x
y
Manhattan Distance
x
y
Minkowski Distance
x
y
=
=
=
−
−


−




∑
∑
∑
Let's consider an example where we are given a big basket of fruits with apples, 
bananas, and pears only. We will assume that the apples were red apples, not green. 
There is one characteristic that will distinguish these fruits from one another: color. 
Apples are red, bananas are yellow, and pears are green. These fruits can also be 
characterized by the weight of each. The following assumptions are made for the 
purpose of illustrating this example:
The shape characteristic is categorized as follows:
•	
For an apple, the shape value lies between 1 and 3, whereas the weight lies 
between 6 and 7 ounces
•	
For a pear, the shape value lies between 2 and 4, whereas the weight lies 
between 5 and 6 ounces
•	
For a banana, the shape value lies between 3 and 5, whereas the weight lies 
between 7 and 9 ounces

Statistical and Machine Learning
[ 748 ]
We have the data about the fruits in a basket as follows:
If we have an unlabeled fruit with a known weight and a color category, then 
applying the k-nearest neighbor method (with any distance formula) will most likely 
find the nearest k neighbors (if they are green, red, or yellow, the unlabeled fruit is 
most likely a pear, apple, or banana respectively). The following code demonstrates 
k-nearest neighbor algorithm using the shape and weight of fruits:
import csv
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
count=0
x=[]
y=[]
z=[]
with open('/Users/myhome/fruits_data.csv', 'r') as csvf:
  reader = csv.reader(csvf, delimiter=',')
  for row in reader:
    if count > 0:
      x.append(row[0])

Chapter 6
[ 749 ]
      y.append(row[1])
      if ( row[2] == 'Apple' ): z.append('r')
      elif ( row[2] == 'Pear' ): z.append('g')
      else: z.append('y')
    count += 1
plt.figure(figsize=(11,11))
recs=[]
classes=['Apples', 'Pear', 'Bananas']
class_colours = ['r','g','y']
plt.title("Apples, Bananas and Pear by Weight and Shape", fontsize=18)
plt.xlabel("Shape category number", fontsize=14)
plt.ylabel("Weight in ounces", fontsize=14)
plt.scatter(x,y,s=600,c=z)

Statistical and Machine Learning
[ 750 ]
Let's pick four unlabeled fruits with their x and y values as A(3.5,6.2), B(2.75,6.2),  
C(2.9, 7.6), and D(2.4, 7.2) with the following code:
from math import pow, sqrt
dist=[]
def determineFruit(xv, yv, threshold_radius):
  for i in range(1,len(x)):
    xdif=pow(float(x[i])-xv, 2)
    ydif=pow(float(y[i])-yv, 2)
    sqrtdist = sqrt(xdif+ydif))
    if ( xdif < threshold_radius and 
         ydif < thresholdradius and sqrtdist < threshold_radius):
      dist.append(sqrtdist)
    else:
      dist.append(99)
  pear_count=0
  apple_count=0
  banana_count=0
  for i in range(1,len(dist)):
      if dist[i] < threshold_radius:
        if z[i] == 'g': pear_count += 1
        if z[i] == 'r': apple_count += 1
        if z[i] == 'y': banana_count += 1
  if ( apple_count >= pear_count and apple_count >= banana_count ):
    return "apple"
  elif ( pear_count >= apple_count and pear_count >= banana_count):
    return "pear"
  elif ( banana_count >= apple_count and banana_count >= pear_count):
    return "banana"
dist=[]
determine = determineFruit(3.5,6.2, 1)
print determine
'pear'

Chapter 6
[ 751 ]
Logistic regression
As we have seen earlier, one problem with linear regression is that it tends to 
underfit the data. This gives us the lowest mean-squared error for unbiased 
estimators. With the underfit model, we will not get the best predictions. There are 
some ways to reduce this mean-squared error by adding some bias to our estimator.
Logistic regression is one of the ways to fit models for data that have true or false 
responses. Linear regression cannot predict all the probabilities directly, but logistic 
regression can. In addition, the predicted probabilities can be calibrated better when 
compared to the results from Naive Bayes.
For this discussion, by keeping our focus on the binary response, we can set the 
value of 1 to true and 0 to false. The logistic regression model assumes that the 
input variables can be scaled by the inverse log function; therefore, another way to 
take a look at this is that the log of the observed y value can be expressed as a linear 
combination of the n input variables of x, as shown in the following equation:
( )
( )
( )
( )
( )
0
log1
1
1
1
1
n
j
j
j
z
z
z
z
P x
b x
z
P x
P x
e
P x
e
P x
e
e
=
−
=
=
−
=
−
⇒
=
=
+
+
∑
As the inverse of a logarithmic function is an exponential function, the expression 
on the right-hand side appears to be a version of a sigmoid of the linear combination 
of the variables of x. This means that the denominator can never be 1 (unless z is 0). 
The value of P(x) is therefore strictly greater than 0 and less than 1, as shown in the 
following code:
import matplotlib.pyplot as plt
import matplotlib
import random, math
import numpy as np
import scipy, scipy.stats
import pandas as pd
x = np.linspace(-10,10,100)
y1 = 1.0 / (1.0+np.exp(-x))
y2 = 1.0 / (1.0+np.exp(-x/2))
y3 = 1.0 / (1.0+np.exp(-x/10))
plt.title("Sigmoid Functions vs LineSpace")
plt.plot(x,y1,'r-',lw=2)

Statistical and Machine Learning
[ 752 ]
plt.plot(x,y2,'g-',lw=2)
plt.plot(x,y3,'b-',lw=2)
plt.xlabel("x")
plt.ylabel("y")
plt.show()
The following image shows a standard sigmoid function:
The following is an example showing probability of happy and sad.
(
)
(
)
(
)
1
1
1
1
z
z
z
e
P happy
e
P sad
P happy
e
= +
= −
= +
Kaggle hosts all the machine learning competitions. It usually provides the training 
and test data. A while ago, predicting the survivors of the Titanic was contested on 
Kaggle based on the real data. The titanic_train.csv and titanic_test.csv files 
are for training and testing purposes respectively. Using the linear_model package 
from scikit-learn, which includes logistic regression, we can see that the following 
code is a modified version of the author's version who won the contest:
Import numpy as np
import pandas as pd
import sklearn.linear_model as lm
import sklearn.cross_validation as cv
import matplotlib.pyplot as plt

Chapter 6
[ 753 ]
train = pd.read_csv('/Users/myhome/titanic_train.csv')
test = pd.read_csv('/Users/myhome/titanic_test.csv')
train[train.columns[[2,4,5,1]]].head()
data = train[['Sex', 'Age', 'Pclass', 'Survived']].copy()
data['Sex'] = data['Sex'] == 'female'
data = data.dropna()
data_np = data.astype(np.int32).values
X = data_np[:,:-1]
y = data_np[:,-1]
female = X[:,0] == 1
survived = y == 1
# This vector contains the age of the passengers.
age = X[:,1]
# We compute a few histograms.
bins_ = np.arange(0, 121, 5)
S = {'male': np.histogram(age[survived & ~female], 
                          bins=bins_)[0],
     'female': np.histogram(age[survived & female], 
                            bins=bins_)[0]}
D = {'male': np.histogram(age[~survived & ~female], 
                          bins=bins_)[0],
     'female': np.histogram(age[~survived & female], 
                            bins=bins_)[0]}
bins = bins_[:-1]
plt.figure(figsize=(15,8))
for i, sex, color in zip((0, 1),('male', 'female'), ('#3345d0', 
'#cc3dc0')):
    plt.subplot(121 + i)
    plt.bar(bins, S[sex], bottom=D[sex], color=color,
            width=5, label='Survived')
    plt.bar(bins, D[sex], color='#aaaaff', width=5, label='Died', 
alpha=0.4)
    plt.xlim(0, 80)
    plt.grid(None)
    plt.title(sex + " Survived")
    plt.xlabel("Age (years)")
    plt.legend()
(X_train, X_test, y_train, y_test) = cv.train_test_split(X, y, test_

Statistical and Machine Learning
[ 754 ]
size=.05)
print X_train, y_train
# Logistic Regression from linear_model
logreg = lm.LogisticRegression();
logreg.fit(X_train, y_train)
y_predicted = logreg.predict(X_test)
plt.figure(figsize=(15,8));
plt.imshow(np.vstack((y_test, y_predicted)),
           interpolation='none', cmap='bone');
plt.xticks([]); plt.yticks([]);
plt.title(("Actual and predicted survival outcomes on the test set"))
The following is a linear regression plot showing male and female survivors  
of Titanic:

Chapter 6
[ 755 ]
We have seen that scikit-learn has a good collection of functions for machine 
learning. They also come with a few standard datasets, for example, the iris dataset 
and the digits dataset for the classification and the Boston house prices the dataset 
for regression. Machine learning is about learning the properties of data and 
applying these properties to the new dataset.
Support vector machines
Support vector machines (SVM) are supervised learning methods that can be 
applied to regression or classification. These learning methods are an extension of 
nonlinear models, which empirically offers good performance and is successful in 
many applications, such as bioinformatics, text, image recognition, and so on. These 
methods are computationally inexpensive and easy to implement, but are prone to 
underfitting and may have low accuracy.
Let's understand the goal of SVM. The goal here is to map or find a pattern between 
x and y, where we want to perform the mapping from X → Y (x ϵ X and y ϵ Y ).  
Here, x can be an object, whereas y can be a label. Another simple example is that X 
is an n-dimensional real value space, whereas y is a set of -1, 1.
A classic example of SVM is that when two pictures of a tiger and a human being are 
given, X becomes the set of pixel images, whereas Y becomes the label that answers 
the question, that is, "is this a tiger or a human being?" when an unknown picture is 
given. Here is another example of the character recognition problem:

Statistical and Machine Learning
[ 756 ]
There are already many examples of SVM on the Internet, but here, we will show 
how you can use scikit-learn (sklearn) to apply the visualization methods on 
various machine learning algorithms that include SVM. In sklearn, among many 
other things, the sklearn.svm package includes the following SVR models:
import numpy as np
from sklearn.svm import SVR
import matplotlib.pyplot as plt
X = np.sort(5 * np.random.rand(40, 1), axis=0)
y = (np.cos(X)+np.sin(X)).ravel()
y[::5] += 3 * (0.5 - np.random.rand(8))
svr_rbfmodel = SVR(kernel='rbf', C=1e3, gamma=0.1)
svr_linear = SVR(kernel='linear', C=1e3)
svr_polynom = SVR(kernel='poly', C=1e3, degree=2)
y_rbfmodel = svr_rbfmodel.fit(X, y).predict(X)
y_linear = svr_linear.fit(X, y).predict(X)
y_polynom = svr_polynom.fit(X, y).predict(X)
plt.figure(figsize=(11,11))
plt.scatter(X, y, c='k', label='data')
plt.hold('on')
plt.plot(X, y_rbfmodel, c='g', label='RBF model')
plt.plot(X, y_linear, c='r', label='Linear model')
plt.plot(X, y_polynom, c='b', label='Polynomial model')
plt.xlabel('data')
plt.ylabel('target')
plt.title('Support Vector Regression')
plt.legend()
plt.show()

Chapter 6
[ 757 ]
Principal component analysis
Principal component analysis (PCA) transforms the attributes of unlabeled data 
using a simple rearrangement and transformation with rotation. Looking at the data 
that does not have any significance, you can find ways to reduce dimensions this 
way. For instance, when a particular dataset looks similar to an ellipse when run at 
a particular angle to the axes, while in another transformed representation moves 
along the x axis and clearly has signs of no variation along the y axis, then it may be 
possible to ignore that.
k-means clustering is appropriate to cluster unlabeled data. Sometimes, one can use 
PCA to project data to a much lower dimension and then apply other methods, such 
as k-means, to a smaller and reduced data space.

Statistical and Machine Learning
[ 758 ]
However, it is very important to perform dimension reduction carefully because 
any dimension reduction may lead to the loss of information, and it is crucial that 
the algorithm preserves the useful part of the data while discarding the noise. Here, 
we will motivate PCA from at least two perspectives and explain why preserving 
maximal variability makes sense:
•	
Correlation and redundancy
•	
Visualization
Suppose that we did collect data about students on a campus that involves details 
about gender, height, weight, tv time, sports time, study time, GPA, and so on. While 
performing the survey about these students using these dimensions, we figured that 
the height and weight correlation yields an interesting theory (usually, the taller the 
student, the more weight due to the bone weight and vice versa). This may probably 
not be the case in a bigger set of population (more weight does not necessarily mean 
taller). The correlation can also be visualized as follows:

Chapter 6
[ 759 ]
import matplotlib.pyplot as plt
import csv
gender=[]
x=[]
y=[]
with open('/Users/kvenkatr/height_weight.csv', 'r') as csvf:
  reader = csv.reader(csvf, delimiter=',')
  count=0
  for row in reader:
    if count > 0:
        if row[0] == "f": gender.append(0)
        else:  gender.append(1)
        height = float(row[1])
        weight = float(row[2])
        x.append(height)
        y.append(weight)
    count += 1
plt.figure(figsize=(11,11))
plt.scatter(y,x,c=gender,s=300)
plt.grid(True)
plt.xlabel('Weight', fontsize=18)
plt.ylabel('Height', fontsize=18)
plt.title("Height vs Weight (College Students)", fontsize=20)
plt.legend()
plt.show()

Statistical and Machine Learning
[ 760 ]
Using sklearn again with the preprocessing, datasets, and decomposition 
packages, you can write a simple visualization code as follows:
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
data = load_iris()
X = data.data
# convert features in column 1 from cm to inches
X[:,0] /= 2.54
# convert features in column 2 from cm to meters
X[:,1] /= 100
from sklearn.decomposition import PCA
def scikit_pca(X):
    # Standardize
    X_std = StandardScaler().fit_transform(X)
    # PCA
    sklearn_pca = PCA(n_components=2)
    X_transf = sklearn_pca.fit_transform(X_std)
    # Plot the data
    plt.figure(figsize=(11,11))
    plt.scatter(X_transf[:,0], X_transf[:,1], s=600, color='#8383c4', 
alpha=0.56)
    plt.title('PCA via scikit-learn (using SVD)', fontsize=20)
    plt.xlabel('Petal Width', fontsize=15)
    plt.ylabel('Sepal Length', fontsize=15)
    plt.show()
scikit_pca(X)

Chapter 6
[ 761 ]
This plot shows PCA using the scikit-learn package:
Installing scikit-learn
The following command will help the installation of the scikit-learn package:
$ conda install scikit-learn
Fetching package metadata: ....
Solving package specifications: .
Package plan for installation in environment /Users/myhomedir/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    nose-1.3.7                 |           py27_0         194 KB
    setuptools-18.0.1          |           py27_0         341 KB

Statistical and Machine Learning
[ 762 ]
    pip-7.1.0                  |           py27_0         1.4 MB
    scikit-learn-0.16.1        |       np19py27_0         3.3 MB
    ------------------------------------------------------------
                                           Total:         5.2 MB
The following packages will be UPDATED:
    nose:         1.3.4-py27_1      --> 1.3.7-py27_0     
    pip:          7.0.3-py27_0      --> 7.1.0-py27_0     
    scikit-learn: 0.15.2-np19py27_0 --> 0.16.1-np19py27_0
    setuptools:   17.1.1-py27_0     --> 18.0.1-py27_0    
Proceed ([y]/n)? y
Fetching packages ...
For anaconda, as the CLI is all via conda, one can install it using conda. For other ways, 
by default, one would always attempt to use pip install. However, in any case, you 
should check the documentation for installation. As all the scikit-learn packages are 
pretty popular and have been around for a while, not much has changed. Now, in the 
following section, we will explore k-means clustering to conclude this chapter.
k-means clustering
k-means clustering originated from signal processing and is a popular method in 
data mining. The main intent of k-means clustering is to find some m points of a 
dataset that can best represent the center of some m-regions in the dataset.
k-means clustering is also known as partition clustering. This means that one needs 
to specify the number of clusters before any clustering process is started. You can 
define an objective function that uses the sum of Euclidean distance between a data 
point and its nearest cluster centroid. One can follow a systematic procedure to 
minimize this objective function iteratively by finding a brand new set of cluster 
centers that can lower the value of the objective function iteratively.
k-means clustering is a popular method in cluster analysis. It does not require any 
assumptions. This means that when a dataset is given and a predetermined number 
of clusters is labeled as k and when you apply the k-means algorithm, it minimizes 
the sum-squared error of the distance.

Chapter 6
[ 763 ]
The algorithm is pretty simple to understand as follows:
•	
Given is a set of n points (x,y) and a set of k centroids
•	
For each (x,y), find the centroid that is closest to that point (which determines 
the cluster this (x,y) belong to
•	
In each cluster, find the median and set this as the centroid of that cluster and 
repeat this process
Let's take a look at a simple example (this can be applied to a large collection of points) 
using k-means from the sklearn.cluster package. This example shows that with 
minimal code, you can accomplish k-means clustering using the scikit-learn library:
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import csv
x=[]
y=[]
with open('/Users/myhomedir/cluster_input.csv', 'r') as csvf:
  reader = csv.reader(csvf, delimiter=',')
    for row in reader:
      x.append(float(row[0]))
      y.append(float(row[1]))
data=[]
for i in range(0,120):
  data.append([x[i],y[i]])
plt.figure(figsize=(10,10))
plt.xlim(0,12)
plt.ylim(0,12)
plt.xlabel("X values",fontsize=14)
plt.ylabel("Y values", fontsize=14)
plt.title("Before Clustering ", fontsize=20)
plt.plot(x, y, 'k.', color='#0080ff', markersize=35, alpha=0.6)
kmeans = KMeans(init='k-means++', n_clusters=3, n_init=10)
kmeans.fit(data)

Statistical and Machine Learning
[ 764 ]
plt.figure(figsize=(10,10))
plt.xlabel("X values",fontsize=14)
plt.ylabel("Y values", fontsize=14)
plt.title("After K-Means Clustering (from scikit-learn)", fontsize=20)
plt.plot(x, y, 'k.', color='#ffaaaa', markersize=45, alpha=0.6)
# Plot the centroids as a blue X
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1], marker='x', s=200,
  linewidths=3, color='b', zorder=10)
plt.show()
Plotting the data before clustering looks like this:

Chapter 6
[ 765 ]
In this example, if we set k=5 for five clusters, then this cluster remains the same, 
but the other two clusters get split into two to obtain five clusters, as shown in the 
following diagram:

Statistical and Machine Learning
[ 766 ]
Summary
This chapter illustrates popular machine learning algorithms with examples. A 
brief introduction to linear and logistic regression was discussed. Using the college 
acceptance criteria for linear regression and the Titanic survivors for logistic 
regression, this chapter also illustrated how you can use the statsmodels.formula.
api, pandas, and sklearn.linear_model packages for these regression methods. In 
both these examples, matplotlib has been used for visualization methods.
You learned about decision trees. Using the sports example (golf and tennis), we 
looked at the decision tree using the sklearn and pydot packages. Further, we 
discussed Bayes theorem and the Naïve Bayes classifier. Using the TextBlob package 
and the movie reviews data from the nltk corpora, we looked at the example of a 
word cloud visually using the wordcloud package.
You learned about the k-nearest neighbors algorithm. Here, we looked at an  
example that classified fruits based on their weight and shape, visually separating  
them by their color.
We also looked at the illustration of SVM in its simplest form with an example  
of how to generate data from the sklearn.svm package and plotted the results  
using the matplotlib library. You learned about PCA, how to determine the 
redundancy, and eliminate some of the variables. We used the iris example with the 
sklearn.preprocesing library to see how to visualize results. Finally, we looked 
at k-means clustering with an example of random points using sklearn.cluster 
as it is the simplest way you can achieve clustering (with minimal code). In the next 
chapter, we will discuss various examples of bioinformatics, genetics, and network.

[ 767 ]
Bioinformatics, Genetics,  
and Network Models
Scientific applications have multiple black boxes, and what goes inside these boxes 
is complex and often thought of as magical. However, they all follow a systematic 
set of protocols. These protocols are well known in the research community. For 
instance, network models are widely used to represent complex structured data, 
such as protein networks, molecular genetics, and chemical structures. Another 
interesting field in the research community is bioinformatics. This is a growing field 
that has lately generated a considerable amount of breakthrough in research.
In the field of biology, there are many different complex structures, such as DNA 
sequences, protein structures, and so on. In order to compare, let's take a look at some 
of the unknown elements within these structures. It is helpful to have a model that will 
visually display them. Similarly, in any application of the graph theory or networks, it 
is essentially beneficial to be able to visualize the complex graph structure.
Later in this chapter, we will discuss some interesting examples, such as social 
networks, directed graph examples in real life, data structures appropriate for these 
problems, and network analysis. For the purposes of demonstrating examples, here 
we will use specific libraries, such as metaseq, NetworkX, matplotlib, Biopython, and 
ETE toolkit, covering the following topics:
•	
Directed graphs and multigraphs
•	
The clustering coefficient of graphs
•	
Analysis of social networks
•	
The planar graph test and the directed acyclic graph test
•	
Maximum flow and minimum cut
•	
A genetic programming example
•	
Stochastic block models and random graphs

Bioinformatics, Genetics, and Network Models
[ 768 ]
Directed graphs and multigraphs
First, we will review directed graphs and multigraphs. Later, we will figure out the 
options in Python to generate them. Also, we will take a look at an example where 
you may require directed graphs. Before we conceptually describe graphs and 
directed graphs, let's take a look at the different ways to understand when you can 
use graphs and directed graphs.
Computers that are connected to each other within a university campus area can be 
considered a connected graph, where each computer in this connection is viewed as a 
node or a vertex. The connected path is an edge, and in some cases, if there is only a 
one-way connection, then it is a directed graph. For instance, a very restricted federal 
network will not allow any connection from outside to go in, but will probably not 
restrict the other way around. The following are simple graphs showing distances 
between places:
In the preceding examples, the graph with city labels A through F is a directed 
graph, and the other one on the right-hand side is an undirected graph. In the 
directed graph, if the arrow points both ways, there is a way to go both ways, 
whereas in the undirected graph, both ways are assumed. If we were to represent 
these graphs using some data structure, what would that be? Also, if we were to plot 
these kinds of graphs, which libraries do we use and how do we accomplish it?

Chapter 7
[ 769 ]
Storing graph data
Graph data is usually represented as an adjacency matrix, unless it is sparse. An 
adjacency matrix is a matrix that has V2 rows, assuming that the graph has a V  
vertex or a node. For example, for the two graphs shown in the preceding figure,  
the adjacency matrix looks similar to the following tables:
A
B
C
D
E
F
A
0
25
26
B
0
85
5
10
C
26
85
0
10
D
0
11
E
9
0
88
F
11
88
0
Chicago
Boston
New 
York
Wash 
DC
Miami
Dallas
Chicago
0
1613
1145
Boston
1613
0
338
725
New York
338
0
383
2145
Wash DC
1145
725
383
0
1709
2113
Miami
2145
1709
0
2161
Dallas
2113
2161
0
For undirected graphs, by symmetry, it is enough to use half the storage (no need to 
store all the information from A to B and B to A). The blank entries show that there is 
not enough data about the distance. If the matrix is sparse, where most of the entries 
are not filled, then you can store it as a list of lists. Fortunately, there are convenient 
methods in scipy to deal with sparse matrices. The following code is only for the 
first graph shown in the preceding figure:
import scipy.sparse as sparse
matrixA = sparse.lil_matrix((6,6))
matrixA = sparse.lil_matrix( [[0,25,26,0,0,0], [0,0,85,5,10,0],
   [26,85,0,0,0,10], [0,0,0,0,0,11],[0,0,0,9,0,88],[0,0,0,11,88,0]])
print matrixA   
(0, 1)  25   
(0, 2)  26   

Bioinformatics, Genetics, and Network Models
[ 770 ]
(1, 2)  85   
(1, 3)  5   
(1, 4)  10   
(2, 0)  26   
(2, 1)  85   
(2, 5)  10
(3, 5)  11   
(4, 3)  9   
(4, 5)  88   
(5, 3)  11   
(5, 4)  88
Displaying graphs
The preceding example only shows how to represent the graph using the scipy 
library (the scipy.sparse package in particular). However, in the following section, 
we will see how to display these graphs. Although there are numerous Python 
packages that you can choose from to display graphs, the top three popular choices 
among these are NetworkX, igraph (from igraph.org), and graph-tool. Let's take a 
look at an example of graph display using these three packages.
igraph
Originally, igraph was intended for R users, but later, the Python version was 
added. For smaller graphs, you can add the vertices and edges and display them 
very easily, but in most cases, graphs are not small; therefore, igraph offers 
functions that reads the data of a graph from files conveniently and displays it.
Currently, igraph offers several formats, such as dimacs, dl, edgelist, graml, 
graphdb, gml, lgl, ncol, and pajek. GraphML is an XML-based file format and can 
be used for large graphs, and the NCOL graph format is suited for large graphs with 
a weighted edge list. The LGL graph format can also be used for a large graph layout 
with weighted edges. Most others use a simple textual format. Only the DL file 
format is fully supported by igraph, and for all others, igraph only supports partial 
file formats.
Similar to many other Python packages, the good part about igraph is that it offers 
very convenient ways to configure and display graphs and stores them in the SVG 
format so that they can be embedded in an HTML file.

Chapter 7
[ 771 ]
Let's take a look at one example that involves the pajek format (for more details on 
pajek, you can refer to http://vlado.fmf.uni-lj.si/pub/networks/pajek/). 
There are many other parameters. A few among these are labelcolor, vertexsize, 
and radius for some vertex shapes. We will see two examples here. The first 
example has assigned labels and edges for a small graph, whereas the second 
example reads the data of a graph from a file and displays it. The following example 
shows a labeled graph using the igraph package:
from igraph import *
vertices = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J"]
edges = [(0,1),(1,2),(2,3),(3,4),(4,5),(5,6),(6,7),(7,1),
         (1,8),  (8,2),(2,4),(4,9),(9,5),(5,7),(7,0)]
graphStyle = { 'vertex_size': 20}
g = Graph(vertex_attrs={"label": vertices}, edges=edges, 
directed=True)
g.write_svg("simple_star.svg", width=500, height=300, **graphStyle)
There are 10 vertices in the star graph that forms five triangles and a pentagon. 
Also, there are 15 edges because the five triangles complete the set of edges. It is a 
very simple graph, where each edge is defined by the associated vertex numbers 
that starts from zero. The following labeled graph plot is the result of the preceding 
Python example:

Bioinformatics, Genetics, and Network Models
[ 772 ]
This second example illustrates not only how to read the graph data from a file, but also 
how to save the plot in the SVG format so that you can embed the SVG data in HTML:
from igraph import read  
g=read("ragusa.net",format="pajek")  
g.vs["color"]="#3d679d" 
g.es["color"]="red" 
graphStyle={ 'vertex_size': 12, 'margin': 6} 
#graphStyle["layout"]=g.layout("fr")  # optional
g.write_svg("ragusa_graph.svg", width=600, height=600,**graphStyle)
The pajek format file is read using the read function from igraph. When you set up 
the edge and the vertex color, you can generate the SVG format of the graph. There 
are several different layouts that igraph offers that you can experiment with. The 
following plot shows a graph that was created using the igraph package by reading 
the graph data from a file:

Chapter 7
[ 773 ]
The graph data in the pajek format was obtained from the pajek networks website 
(http://vlado.fmf.uni-lj.si/pub/networks/pajek/data/gphs.htm) from a 
file named Rgausa16.net. Once a data file from here is downloaded, you can use it 
in a similar way and display the graph, as shown in the preceding image. If we use 
the tinamatr.net data and set the circular layout, then the graph would appear in a 
circular layout, as shown in the following code:
graphStyle["layout"]=g.layout("circle")
NetworkX
One of the reasons this Python package is called NetworkX is because it is a library 
for network and graph analysis. From finding the shortest path from a source node 
or vertex to the destination node or vertex, finding the degree distribution to figure 
the nodes that are similar to the junction, and finding the clustering coefficient of a 
graph, there are several ways to perform a graph analysis.
The study of graphs has been around for a while and is applicable in neurobiology, 
chemistry, social network analysis, page ranks, and many more such interesting 
areas today. Social networks are assortative truly in the sense of joining similar 
affiliated members, and biological networks are the opposite. In other words, the 
friendship between Facebook users or the academicians (who are coauthors) can be 
visualized easily via graphs. Python packages offer users many options. Often, users 
choose several of these to combine the best of their individual functionalities.
NetworkX offers graph building and analysis capabilities. You can read and write 
network data in standard and nonstandard data formats, generate graph networks, 
analyze their structure, and build several models. The following Python code shows 
how one can create a directed graph by just using matplotlib:
import matplotlib.pyplot as plt
import pylab
from pylab import rcParams
import networkx as nx
import numpy as np
# set the graph display size as 10 by 10 inches
rcParams['figure.figsize'] = 10, 10
G = nx.DiGraph()
# Add the edges and weights
G.add_edges_from([('K', 'I'),('R','T'),('V','T')], weight=3)
G.add_edges_from([('T','K'),('T','H'),('I','T'),('T','H')], weight=4)
G.add_edges_from([('I','R'),('H','N')], weight=5)
G.add_edges_from([('R','N')], weight=6)

Bioinformatics, Genetics, and Network Models
[ 774 ]
# these values to determine node colors
val_map = {'K': 1.5, 'I': 0.9, 'R': 0.6, 'T': 0.2}
values = [val_map.get(node, 1.0) for node in G.nodes()]
edge_labels=dict([((u,v,),d['weight'])
                 for u,v,d in G.edges(data=True)])
#set edge colors
red_edges = [('R','T'),('T','K')]
edge_colors = ['green' if not edge in red_edges else 'red' for edge in 
G.edges()]
pos=nx.spring_layout(G)
nx.draw_networkx_edges(G,pos,width=2.0,alpha=0.65)
nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)
nx.draw(G,pos, node_color = values, node_size=1500,
 edge_color=edge_colors, edge_cmap=plt.cm.Reds)
pylab.show()
The following diagram illustrates how you can use NetworkX to configure the edge 
weights and the visual aesthetics of a graph. Among several approaches of displaying 
a directed graph, NetworkX took a different approach by showing a thick bar at the 
end, rather than using an arrow symbol that determines the direction of a graph.

Chapter 7
[ 775 ]
When there is a scientific study that involves a collection of elements that represent 
things or people, the association between them is better represented in the form 
of graphs, where these elements are vertices or nodes. In most of these cases, the 
centrality visually identifies nodes that are significantly important. Python packages 
(such as NetworkX) have many useful functions for graph analysis that includes 
finding cliques in the graph. For smaller graphs, it is easier to visually inspect 
intricate details, but for larger graphs, one would want to recognize a pattern of 
behavior, such as isolated cluster groups.
Typically, the labels for nodes and edges depend on what you are trying to display 
as a graph. For instance, protein interaction can be displayed as a graph. A more 
complex example will be a sequence space graph, where a graph node represents 
a protein sequence, whereas an edge represents a single DNA mutation. It would 
be easier for scientists to zoom into these images to see patterns, as shown in 
the following image. This example does not use Python and uses interactive 
programming to zoom and view the intricate details.
The preceding image has been taken from  
http://publications.csail.mit.edu/.

Bioinformatics, Genetics, and Network Models
[ 776 ]
Sometimes, you would want to highlight different routes on a map. For instance, if 
a road map is being displayed and you have to display the routes that the Olympic 
cycling team is going to follow this year on this map, you can do something similar 
to the following code:
import networkx as nx
from pylab import rcParams
# set the graph display size as 10 by 10 inches
rcParams['figure.figsize'] = 10, 10
def genRouteEdges(r):
    return [(r[n],r[n+1]) for n in range(len(r)-1)]
G=nx.Graph(name="python")
graph_routes = [[11,3,4,1,2], [5,6,3,0,1], [2,0,1,3,11,5]]
edges = []
for r in graph_routes:
    route_edges = genRouteEdges(r)
    G.add_nodes_from(r)
    G.add_edges_from(route_edges)
    edges.append(route_edges)
print("Graph has %d nodes with %d edges" %(G.number_of_nodes(),    
G.number_of_edges()))
pos = nx.spring_layout(G)
nx.draw_networkx_nodes(G,pos=pos)
nx.draw_networkx_labels(G,pos=pos)
colors = ['#00bb00', '#4e86cc', 'y']
linewidths = [22,14,10]
for ctr, edgelist in enumerate(edges):
    nx.draw_networkx_edges(G,pos=pos,edgelist=edgelist,
      edge_color = colors[ctr], width=linewidths[ctr])
Using convenient methods from NetworkX for a specific route, you can easily highlight 
the routes with different colors and line widths, as shown in the following image:

Chapter 7
[ 777 ]
As shown in the preceding image, by controlling the highlights of routes, you can 
recognize different routes on a map.
Additionally, from the shortest path to degree distribution to clustering coefficients, 
NetworkX offers a variety of ways to perform a graph analysis. One simple way to 
see the shortest path is shown in the following code:
import networkx as nx
g = nx.Graph()
g.add_edge('m','i',weight=0.1)
g.add_edge('i','a',weight=1.5)
g.add_edge('m','a',weight=1.0)
g.add_edge('a','e',weight=0.75)
g.add_edge('e','h',weight=1.5) 
g.add_edge('a','h',weight=2.2)
print nx.shortest_path(g,'i','h')
nx.draw(g)
#printed shortest path as result 
['i', 'a', 'h']

Bioinformatics, Genetics, and Network Models
[ 778 ]
One more example using NetworkX (particularly reading the data in the  
GML format) is the "coappearance of characters in the Les Miserables novel",  
which we downloaded from the datasets available from gephi.org at  
https://gephi.org/datasets/lesmiserables.gml.zip.
The preceding plot is the result of the program that reads the association of 
characters from Les Miserables and creates a network diagram, as shown in the 
following code:
import networkx as nx
from pylab import rcParams
rcParams['figure.figsize'] = 12, 12
G = nx.read_gml('/Users/kvenkatr/Downloads/lesmiserables.gml', 
relabel=True)
G8= G.copy()
dn = nx.degree(G8)
for n in G8.nodes():
  if dn[n] <= 8:
    G8.remove_node(n)
pos= nx.spring_layout(G8)

Chapter 7
[ 779 ]
nx.draw(G8, node_size=10, edge_color='b', alpha=0.45, font_size=9, 
pos=pos)
labels = nx.draw_networkx_labels(G8, pos=pos)
Graph-tool
Among the three packages, igraph, networkx, and graph-tool, the graph-tool 
package is the hardest to install, especially on a Mac OS. Graph-tool has  
many convenient functions and is also considered very efficient in terms of 
centrality-related algorithms. This includes k-core, PageRank, minimum spanning 
tree, and the single source shortest path. The comparison table is available at 
https://graph-tool.skewed.de/performance. The module that includes 
centrality-related algorithms as mentioned earlier is graph_tool.centrality.
import graph_tool.all as gtool
gr = gtool.collection.data["polblogs"]
gr = gtool.GraphView(gr, vfilt=gtool.label_largest_component(gr))
cness = gtool.closeness(gr)
gtool.graph_draw(gr, pos=gr.vp["pos"], vertex_fill_color=cness,
               vertex_size=gtool.prop_to_size(cness, mi=5, ma=15),
               vorder=cness, vcmap=matplotlib.cm.gist_heat,
               output="political_closeness.pdf")

Bioinformatics, Genetics, and Network Models
[ 780 ]
The prefix centra in the "centrality" word truly means that some entity (in this 
context, this would be a node or vertex) is central. Also, many other entities are 
connected to the central entity. So, we can ask a reasonable question, that is, what 
are the characteristics that makes a vertex important? In the graph_tool centrality 
module, there are nine centrality-related algorithms that are offered, and PageRank is 
one among these, in addition to closeness.
PageRank
The graph_tool.centrality	.pagerank() function generates the PageRank of the 
v vertex. Most people who know Google's PageRank understand how the measure 
works. In a nutshell, it is a way to measure how important web page A is (in terms of 
how many outside web sites B are depending on web page A and also on how many 
web pages A depends on – in graph theory they are called in-degree and out-degree). 
In addition to these, Google applies many other external factors to rank a web page. 
In the preceding example, if we replace the line that finds closeness by PageRank  
as follows:
pagerank = gtool.pagerank(gr)
This should generate a graph with the emphasis on PageRank. In addition to the 
centrality measure, there is one other factor called the clustering coefficient of a graph.
The clustering coefficient of graphs
The clustering coefficient of a node or a vertex in a graph depends on how close the 
neighbors are so that they form a clique (or a small complete graph), as shown in the 
following diagram:

Chapter 7
[ 781 ]
There is a well known formula to cluster coefficients, which looks pretty heavy 
with mathematical symbols. However, to put it in simple words, take a look at the 
following equation:
(
)
(
)
2
1
_
i
b
b
linkstothenodei
C
n
n
wheren
bisthenumber of neighborstonodei
×
=
−
This involves keeping track of the links at every vertex and calculating the clustering 
index at every vertex, where the neighbor of a node in the most obvious sense is a node 
that is only one link away from that node. Clustering index calculation is shown here:
(
)
(
)
2
1
_
i
b
b
linkstothenodei
C
n
n
wheren
bisthenumber of neighborstonodei
×
=
−

Bioinformatics, Genetics, and Network Models
[ 782 ]
The following code illustrates how you can show the characters of the Les Miserables 
novel and how each character is associated or connected to other characters:
import networkx as nx
from pylab import rcParams
rcParams['figure.figsize'] = 12, 12
G = nx.read_gml('/Users/kvenkatr/Downloads/lesmiserables.gml', 
relabel=True)
G8= G.copy()
dn = nx.degree(G8)
for n in G8.nodes():
  if dn[n] <= 8:
    G8.remove_node(n)
pos= nx.spring_layout(G8)
nx.draw(G8, node_size=10, edge_color='b', alpha=0.45, font_size=9, 
pos=pos)
labels = nx.draw_networkx_labels(G8, pos=pos)
def valuegetter(*values):
    if len(values) == 1:
        item = values[0]
        def g(obj):
            return obj[item]
    else:
        def g(obj):
            return tuple(obj[item] for item in values)
    return g
def clustering_coefficient(G,vertex):
    neighbors = G[vertex].keys()
    if len(neighbors) == 1: return -1.0
    links = 0
    for node in neighbors:
        for u in neighbors:
            if u in G[node]: links += 1
    ccoeff=2.0*links/(len(neighbors)*(len(neighbors)-1))
    return links, len(neighbors),ccoeff 
def calculate_centrality(G):
    degc = nx.degree_centrality(G)
    nx.set_node_attributes(G,'degree_cent', degc)

Chapter 7
[ 783 ]
    degc_sorted = sorted(degc.items(), key=valuegetter(1), 
reverse=True)
    for key, value in degc_sorted[0:10]:
        print "Degree Centrality:", key, value
    return G, degc
print "Valjean", clustering_coefficient(G8,"Valjean")
print "Marius", clustering_coefficient(G8,"Marius")
print "Gavroche", clustering_coefficient(G8,"Gavroche")
print "Babet", clustering_coefficient(G8,"Babet")
print "Eponine", clustering_coefficient(G8,"Eponine")
print "Courfeyrac", clustering_coefficient(G8,"Courfeyrac")
print "Comeferre", clustering_coefficient(G8,"Combeferre")
calculate_centrality(G8)
There are two results of the preceding code; the first part is the textual output that 
gets printed, whereas the second part is the network diagram that gets plotted, as 
shown in the following code and diagram:
#Text Results printed
Valjean (82, 14, 0.9010989010989011)
Marius (94, 14, 1.032967032967033)
Gavroche (142, 17, 1.0441176470588236)
Babet (60, 9, 1.6666666666666667)
Eponine (36, 9, 1.0)
Courfeyrac (106, 12, 1.606060606060606)
Comeferre (102, 11, 1.8545454545454545)
Degree Centrality: Gavroche 0.708333333333
Degree Centrality: Valjean 0.583333333333
Degree Centrality: Enjolras 0.583333333333
Degree Centrality: Marius 0.583333333333
Degree Centrality: Courfeyrac 0.5
Degree Centrality: Bossuet 0.5
Degree Centrality: Thenardier 0.5
Degree Centrality: Joly 0.458333333333
Degree Centrality: Javert 0.458333333333
Degree Centrality: Feuilly 0.458333333333

Bioinformatics, Genetics, and Network Models
[ 784 ]
The graph results are shown here below.
Clearly among all, so far we have found that Comeferre happens to have a larger 
clustering coefficient (0.927). Often, when we plot a large graph in two dimensions, it 
is not easy to visually see the clustering coefficient.
Analysis of social networks
Accessing data from social networks, such as LinkedIn, Facebook, or Twitter, 
used to be much simpler and easier several years ago. Now, most of the APIs have 
restrictions. Also, the accessing methods are a little bit more involved. First, one 
has to get authentication (which used to be the case even earlier) and then use 
methods that access either friends or connections. We have only chosen Twitter for 
demonstrating the analysis of social network data here, but you can also find other 
social media data in a similar way.

Chapter 7
[ 785 ]
In order to access Twitter data, as we noticed from the previous chapters (when we 
discussed word clouds), you have to get authentication keys to access their APIs.  
There are four keys: CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN_KEYS, and 
ACCESS_TOKEN_SECRET. Once these credentials are verified via Python successfully, 
you can call GetFriends() and GetFollowers() to get the list of friends and 
followers. There are many packages available in Python to access Twitter data. So, it is 
very confusing which ones to use. We have used tweepy in past examples. Here, in the 
following code, we will use Python-Twitter because it has convenient modules to get 
data, summarize it, store it in cPickle, and then visualize it.
import cPickle
import os
import twitter  # https://github.com/ianozsvald/python-twitter
# Usage:
# $ # setup CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN_KEY, ACCESS_
TOKEN_SECRET
# as environment variables
# $ python get_data.py  # downloads friend and follower data to ./data
# Errors seen at runtime:
# raise URLError(err)
# urllib2.URLError: <urlopen error [Errno 104] Connection reset by 
peer>
DATA_DIR = "data"  # storage directory for friend/follower data
# list of screen names that we'll want to analyze
screen_names = [ 'KirthiRaman', 'Lebron' ]
def get_filenames(screen_name):
    """Build the friends and followers filenames"""
    return os.path.join(DATA_DIR, "%s.friends.pickle" % (screen_
name)), os.path.join(DATA_DIR, "%s.followers.pickle" % (screen_name))
if __name__ == "__main__":
    # deliberately stripped my keys
    t = twitter.Api(consumer_key='k7atkBNgoGrioMS...',
                  consumer_secret='eBOx1ikHMkFc...',
                  access_token_key='8959...',
                  access_token_secret='O7it0...');
    print t.VerifyCredentials()
    for screen_name in screen_names:

Bioinformatics, Genetics, and Network Models
[ 786 ]
        fr_filename, fo_filename = get_filenames(screen_name)
        print "Checking for:", fr_filename, fo_filename
        if not os.path.exists(fr_filename):
            print "Getting friends for", screen_name
            fr = t.GetFriends(screen_name=screen_name)
            cPickle.dump(fr, open(fr_filename, "w"), protocol=2)
        if not os.path.exists(fo_filename):
            print "Getting followers for", screen_name
            fo = t.GetFollowers(screen_name=screen_name)
            cPickle.dump(fo, open(fo_filename, "w"), protocol=2)
The friends and followers information is dumped in cPickle. By running the following 
commands (as explained in https://github.com/ianozsvald/python-twitter), 
you can run the following code:
python get_data.py
python summarise_data.py
python draw_network.py
The planar graph test
Planar graphs are graphs that can be drawn on a plane without any intersecting 
edges. In order to draw them, you have to start from a vertex, draw from edge to 
edge, and keep track of the faces as the drawing continues. According to Kuratowski, 
a graph is planar if it does not contain a subgraph that is part of the complete graph 
on five vertices.

Chapter 7
[ 787 ]
The following is a simple example of a planar graph:
Euler's formula connects a number of vertices, edges, and faces. According to Euler's 
formula, if a finite and connected planar graph is drawn in the plane without any 
intersecting edge, and if v represents the number of vertices, e represents the number 
of edges, and f represents the number of faces, then v − e + f = 2.
Besides Mayavi, NetworkX, and planarity, you can use the gamera package to 
create and display graphs. However, gamera is only available on Windows. We have 
a simple example here that uses planarity and NetworkX:
import planarity 
import networkx as nx 
# complete graph of 8 nodes, K8 
G8=nx.complete_graph(8) 
# K8 is not planar 
print(planarity.is_planar(G8)) 
# Will display false because G8 is not planar subgraph 
K=planarity.kuratowski_subgraph(G8) 
# Will display the edges
print(K.edges())
#Will display the graph
nx.draw(G8)
False
[(0, 4), (0, 5), (0, 7), (2, 4), (2, 5), (2, 7), (3, 5), (3, 6), (3, 
7), (4, 6)]

Bioinformatics, Genetics, and Network Models
[ 788 ]
This example illustrates that the following complete graph of eight nodes is not planar:
The preceding diagram shows that a planar graph with only eight nodes could look 
messy, so a graph with more nodes will look more complex.
The directed acyclic graph test
Let's take a look at what a directed acyclic graph (DAG) is first. A directed acyclic 
graph is a graph that is directed, which means that the edges from a given vertex A 
to B will be directed in a particular direction (A->B or B->A) and is acyclic. Acyclic 
graphs are those graphs that are not cyclic, which also means that there is no cycle 
(they don't go around in cycle).
What is a good example of a DAG? A tree or even a trie. We all know what they 
are because it was discussed in one of the previous chapters of this book. A good 
example of using trie is to store the words of dictionaries and have a spell check 
algorithm. We will not go further into details about this, but in the context of 
visualization and to check whether a graph is acyclic or not, we will determine the 
Python packages that offer methods to test whether a graph is acyclic or not.

Chapter 7
[ 789 ]
NetworkX has a convenient function called is_directed_acyclic_graph (Graph). 
Here is an example of a graph that is acyclic; using this function, we will test to see 
whether it returns true:
import matplotlib.pyplot as plt
import pylab
from pylab import rcParams
import networkx as nx
import numpy as np
# set the graph display size as 10 by 10 inches
rcParams['figure.figsize'] = 10, 10
G = nx.DiGraph()
# Add the edges and weights
G.add_edges_from([('K', 'I'),('R','T'),('V','T')], weight=3)
G.add_edges_from([('T','K'),('T','H'),('T','H')], weight=4)
# these values to determine node colors
val_map = {'K': 1.5, 'I': 0.9, 'R': 0.6, 'T': 0.2}
values = [val_map.get(node, 1.0) for node in G.nodes()]
edge_labels=dict([((u,v,),d['weight'])
                 for u,v,d in G.edges(data=True)])
#set edge colors
red_edges = [('R','T'),('T','K')]
edge_colors = ['green' if not edge in red_edges else 'red' for edge in 
G.edges()]
pos=nx.spring_layout(G)
nx.draw_networkx_edges(G,pos,width=2.0,alpha=0.65)
nx.draw_networkx_edge_labels(G,pos,edge_labels=edge_labels)
nx.draw(G,pos, node_color = values, node_size=1500,
 edge_color=edge_colors, edge_cmap=plt.cm.Reds)
pylab.show()
nx.is_directed_acyclic_graph(G) 
True

Bioinformatics, Genetics, and Network Models
[ 790 ]
The acyclic graph from this example is displayed in the following diagram:
Maximum flow and minimum cut
A flow network is a directed graph from a source to a destination with capacities 
assigned along each edge. Just as we can model a street map as a directed graph 
in order to find the shortest path from one place to another, we can also interpret a 
directed graph as a "flow network". Some examples of flow networks are liquid flowing 
through pipes, current passing through electrical networks, and data transferring 
through communication networks. The following is an example graph flow diagram:
The edges of the G graph are expected to have a capacity that indicates how much 
flow the edge can support. If this capacity is not present, then it is assumed to have 
infinite capacity. The maximum flow of the flow network G here is 4.

Chapter 7
[ 791 ]
In the NetworkX package, the maximum_flow_value(Graph, from, to) function 
evaluates the maximum flow of a graph, as shown in the following code:
import networkx as nx
G = nx.DiGraph()
G.add_edge('p','y', capacity=5.0)
G.add_edge('p','s', capacity=4.0)
G.add_edge('y','t', capacity=3.0)
G.add_edge('s','h', capacity=5.0)
G.add_edge('s','o', capacity=4.0)
flow_value = nx.maximum_flow_value(G, 'p', 'o')
print "Flow value", flow_value
nx.draw(G, node_color='#a0cbe2')
Flow value 4.0
The graph from the preceding code is being tested for maximum_flow_value, and the 
display of this graph is shown in the following diagram:

Bioinformatics, Genetics, and Network Models
[ 792 ]
A genetic programming example
CnvKit is also available, but it is a CLI and not easy to use. In addition to this, PyCogent, 
which was developed by researchers at NCBI from the National Institutes of Health 
(NIH), is a useful tool. However, they are not easy to use. We will use a package called 
Bio (https://github.com/biopython/biopython/tree/master/Bio) and libraries 
from Python programming for biology.
In general, every experiment, research project, or study has sequence as the key object 
that is used in bioinformatics. As a mathematician, my visual thought of a sequence 
relates to a string with certain patterns (such as ATAGCATATGCT). To begin with, here 
is a simple example that shows a sequence, GC ratio, and codons:
from Bio.Seq import Seq 
from Bio.Alphabet import IUPAC 
from Bio.SeqUtils import GC 
def DNACodons(seq):
    end = len(seq) - (len(seq) % 3) – 1
    codons = [seq[i:i+3] for i in range(0, end, 3)]     
    return codons DNACodons(my_seq)
my_seq = Seq('GGTCGATGGGCCTAGCAGCATATCTGAGC', IUPAC.unambiguous_dna) 
print "GC Result==>", GC(my_seq)  
DNACodons(my_seq)
[Seq('GGT', IUPACUnambiguousDNA()),
 Seq('CGA', IUPACUnambiguousDNA()), 
 Seq('TGG', IUPACUnambiguousDNA()), 
 Seq('GCC', IUPACUnambiguousDNA()), 
 Seq('TAG', IUPACUnambiguousDNA()), 
 Seq('CAG', IUPACUnambiguousDNA()), 
 Seq('CAT', IUPACUnambiguousDNA()), 
 Seq('ATC', IUPACUnambiguousDNA()), 
 Seq('TGA', IUPACUnambiguousDNA())]
GC Result==> 58.6206896552

Chapter 7
[ 793 ]
Let's consider two molecular structures, collect certain atoms, and try to plot 
their positions with their Phi and Psi angles. The allowed molecular structures 
are DNA, RNA, and protein. Using the Modelling and Maths modules from the 
PythonForBiology library, we will attempt to plot these structures side by side:
The two plots uses data from two files: testTransform.pdb and 1A12.pub. This 
contains the regulator of chromosome condensation (RCC1) of humans, as shown in 
the following code:
# bio_1.py
#
import matplotlib.pyplot as plt
from phipsi import getPhiPsi
from Modelling import getStructuresFromFile
def genPhiPsi(fileName):
  struc = getStructuresFromFile(fileName)[0]

Bioinformatics, Genetics, and Network Models
[ 794 ]
  phiList = []
  psiList = []
  for chain in struc.chains:
    for residue in chain.residues[1:-1]:
      phi, psi = getPhiPsi(residue)
      phiList.append(phi)
      psiList.append(psi)
  return phiList, psiList
if __name__ == '__main__':
  phiList = []
  psiList = []
  phiList, psiList = genPhiPsi('examples/testTransform.pdb')
  phiList2 = []
  psiList2 = []
  phiList2, psiList2 = genPhiPsi('examples/1A12.pdb')
  plt.figure(figsize=(12,9))
  f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(12,9))
  ax1.scatter(phiList, psiList, s=90, alpha=0.65)
  ax1.axis([-160,160,-180,180])
  ax1.set_title('Ramachandran Plot for Two Structures')
  ax2.scatter(phiList2, psiList2, s=60, alpha=0.65, color='r')
  plt.show()
The library used in this example will be available with the code examples in a file 
called PythonForBiology.zip. You can extract it and run this code via a command 
line, assuming that you have numpy and matplotlib installed.
Stochastic block models
In the previous chapters, we have already discussed stochastic models using the 
Monte Carlo simulation. So far, we have been discussing graphs and networks, so 
purely from that context, a community structure can also be viewed as a graph. 
In such graphs, nodes often cluster together as densely connected subgraphs. In 
general, the probability of an edge between two such nodes is a function of the 
cluster to which the node belongs.

Chapter 7
[ 795 ]
A popular choice for such a network partition is the stochastic block model. A simple 
definition of a stochastic block model is characterized by a scalar n. This represents 
the number of groups or the number of clusters and a matrix that shows the nodes 
and their connections. For a more rigorous mathematical definition, you can refer to 
a statistics book.
Among a few Python packages that support stochastic models, PyMC is one  
that offers Markov Chain Monte Carlo (MCMC) and three building blocks  
for probability models, such as stochastic, deterministic, and potential. In  
addition to PyMC, there is another interesting package called StochPy for  
Stochastic Modeling. The SSA module in particular offers convenient methods 
(http://stochpy.sourceforge.net/examples.html). The first example uses pymc 
with a normal distribution to display a composite plot and another with an MCMC 
model, as shown in the following code:
import pymc as mc
from pylab import rcParams
# set the graph display size as 10 by 10 inches
rcParams['figure.figsize'] = 12, 12
z = -1.
#instead of 0 and 1, some unknown mu and std goes here:
X = mc.Normal( "x", 0, 1, value = -3. ) 
#Here below, one can place unknowns here in place of 1, 0.4
@mc.potential
def Y(x=X, z=z): 
  return mc.lognormal_like( z-x, 1, 0.4,  )
mcmc = mc.MCMC([X])
mcmc.sample(10000,500)
mc.Matplot.plot(mcmc)

Bioinformatics, Genetics, and Network Models
[ 796 ]
The example shown here is to illustrate how you can display a complex model in 
very few lines of code:
There are examples in PyMC for disaster_model, and with MCMC and 50,000 
simple iterations, the model display appears as follows:
from pymc.examples import disaster_model
from pymc import MCMC
from pylab import hist, show, rcParams
rcParams['figure.figsize'] = 10, 10
M = MCMC(disaster_model)
M.sample(iter=65536, burn=8000, thin=16)
hist(M.trace('late_mean')[:], color='#b02a2a')
show()

Chapter 7
[ 797 ]
If we were to show the histogram plot of mean values from the model, this is one 
option of using PyMC:
The following code uses the stochpy timeseries trajectory data for simulation:
import stochpy as stp
smod = stp.SSA()
from pylab import rcParams
# set the graph display size as 10 by 10 inches
rcParams['figure.figsize'] = 12, 12
smod.Model('dsmts-003-04.xml.psc')
smod.DoStochSim(end=35,mode='time',trajectories=2000)
smod.GetRegularGrid()
smod.PlotAverageSpeciesTimeSeries()

Bioinformatics, Genetics, and Network Models
[ 798 ]
StochPy has several convenient methods to simulate stochastic models and display 
the results, as shown in the following image:

Chapter 7
[ 799 ]
Summary
This chapter illustrates the examples of networks and bioinformatics and the choice 
of Python packages to be able to plot the results. We looked at a brief introduction to 
graphs and multigraphs and used the sparse matrix and distance graphs to illustrate 
how you can store and display graphs with several different packages, such as 
NetworkX, igraph (from igraph.org), and graph-tool.
The clustering coefficient and centrality of graphs demonstrates how you can 
compute clustering coefficients so that they are able to know how significant a node 
or vertex is in the graph. We also looked at the analysis of social network data with 
an illustration of Twitter friends and followers visually, using the Python-Twitter 
package and the NetworkX library.
You also learned about genetic programming samples with a demonstration of how 
you can see codons in a DNA sequence and how to compute GC ratio with the bio 
package. In addition to this, we demonstrated how to display the structures of DNA, 
RNA, or protein.
The planar graph test, the acyclic graph test, and maximum flow using the NetworkX 
package, along with a very few lines of code of how to test all of these was discussed. 
In addition, you can plot stochastic block models with several choices, such as 
PyMC or StochPy. In the next chapter, we will conclude with advanced visualization 
methods that you can choose from.


[ 801 ]
Advanced Visualization
Visualization methods have transformed from the traditional bar and pie graphs 
several decades ago to much more creative forms lately. Designing visualization 
is not as straightforward as picking one from the many choices that a particular 
tool offers. The right visualization conveys the right message, and the wrong 
visualization may distort, confuse, or convey the wrong message.
Computers and storage devices within them are useful in not only storing large 
chunks of data using data structures, but also to use the power of computing 
via algorithms. According to Michael Bostock, the creator of D3.js and a leading 
visualization expert, we should visualize the algorithm and not just the data that 
feeds into it. An algorithm is the core engine behind any process or computational 
model; therefore, this algorithm has become an important use case for visualization.
Visualizing algorithms has only been recognized in the recent past few years, and 
one interesting place to explore this concept is visualgo.net, where they have 
some advanced algorithms to teach data structures and algorithms. Visualgo 
contains algorithms that can be found in Dr. Steven Halim's book titled Competitive 
Programming. Another similar interesting visualization methods have been made 
available by Prof. David Galles from the University of San Francisco  
(https://www.cs.usfca.edu/~galles/visualization/). There are other such 
contributions to teach algorithms and data.

Advanced Visualization
[ 802 ]
We discussed many different areas, including numerical computing, financial 
models, statistical and machine learning, and network models. Later in this  
chapter, we will discuss some new and creative ideas about visualization and  
some simulation and signal processing examples. In addition, we will cover  
the following topics:
•	
Computer simulation, signal processing, and animation examples
•	
Some interesting visualization methods using HTML5
•	
How is Julia different from Python?—advantages and disadvantages
•	
Why is D3.js the most popular visualization tool when compared with 
Python
•	
Tools to create dashboards
Computer simulation
A computer simulation is a discipline that gained popularity for more than several 
decades. It is a computer program that attempts to simulate an abstract model. 
The models of computer simulation can assist in the creation of complex systems 
as a way to understand and evaluate hidden or unknown scenarios. Some notable 
examples of computer simulation modeling are weather forecasting and aircraft 
simulators used for training pilots.
Computer simulations have become a very productive part of mathematical 
modeling of systems in diverse fields, such as physics, chemistry, biology, 
economics, engineering, psychology, and social science.
Here are the benefits of simulation models:
•	
Gaining a better understanding of an algorithm or process that is being 
studied
•	
Identifying the problem areas in the processes and algorithm
•	
Evaluating the impact of changes in anything that relates to the algorithmic 
model
The types of simulation models are as follows:
•	
Discrete models: In this, changes to the system occur only at specific times
•	
Continuous models: In this, the state of the system changes continuously 
over a period of time
•	
Mixed models: This contains both discrete and continuous elements

Chapter 8
[ 803 ]
In order to conduct a simulation, it is common to use random probabilistic inputs 
because it is unlikely that you would have real data before any such simulation 
experiment is performed. It is therefore common that simulation experiments involve 
random numbers whether it is done for a deterministic model or not.
To begin with, let's consider several options to generate random numbers in Python 
and illustrate one or more examples in simulation.
Python's random package
Python provides a package called random that has several convenient functions that 
can be used for the following:
•	
To generate random real numbers between 0.0 and 1.0, or between specific 
start and end values
•	
To generate random integers between specific ranges of numbers
•	
To get a list of random values from a list of numbers or letters
import random
print random.random() # between 0.0 and 1.0
print random.uniform(2.54, 12.2) # between 2.54 and 12.2
print random.randint(5,10)  # random integer between 5 and 10
print random.randrange(25)  # random number between 0 and 25
#  random numbers from the range of 5 to 500 with step 5
print random.randrange(5,500,5) 
# three random number from the list 
print random.sample([13,15,29,31,43,46,66,89,90,94], 3) 
# Random choice from a list
random.choice([1, 2, 3, 5, 9])
SciPy's random functions
NumPy and SciPy are Python modules that consist of mathematical and numerical 
routines. The Numeric Python (NumPy) package provides basic routines to 
manipulate large arrays and matrices of numeric data. The scipy package extends 
NumPy with algorithms and mathematical techniques.

Advanced Visualization
[ 804 ]
NumPy has a built-in pseudorandom number generator. The numbers are 
pseudorandom, which means that they are generated deterministically from a single 
seed number. Using the same seed number, you can generate the same set of random 
numbers, as shown in the following code:
Import numpy as np
np.random.seed(65536)
A different random sequence can be generated by not providing the seed value. 
NumPy automatically selects a random seed (based on the time) that is different 
every time a program is run with the following code:
np.random.seed()
An array of five random numbers in the interval [0.0, 1.0] can be generated as 
follows:
import numpy as np
np.random.rand(5)
#generates the following
array([ 0.2611664,  0.7176011,  0.1489994,  0.3872102,  0.4273531])
The rand function can be used to generate random two-dimensional arrays as well, 
as shown in the following code:
np.random.rand(2,4) 
array([
[0.83239852, 0.51848638, 0.01260612, 0.71026089],        
[0.20578852, 0.02212809, 0.68800472, 0.57239013]])
To generate random integers, you can use randint (min, max), where min and max 
define the range of numbers, in which the random integer has to be drawn, as shown 
in the following code:
np.random.randint(4,18) 
Use the following code to draw the discrete Poisson distribution with λ = 8.0:
np.random.poisson(8.0)
To draw from a continuous normal (Gaussian) distribution with the mean as μ = 1.25 
and the standard deviation as σ = 3.0, use the following code:
np.random.normal(2.5, 3.0)
#for mean 0 and variance 1
np.random.mormal()

Chapter 8
[ 805 ]
Simulation examples
In the first example, we will select geometric Brownian motion, which is also 
known as exponential Brownian motion, to model the stock price behavior with the 
Stochastic Differential Equation (SDE):
t
t
t
t
dS
S dt
S dW
µ
σ
=
+
In the preceding equation, Wt is Brownian motion, μ the percentage drift, and σ is 
the percentage volatility. The following code shows Brownian motion plot:
from numpy.random import standard_normal
from numpy import zeros, sqrt
import matplotlib.pyplot as plt
S_init = 20.222
T =1
tstep =0.0002
sigma = 0.4
mu = 1
NumSimulation=6
colors = [ (214,27,31), (148,103,189), (229,109,0), (41,127,214), 
(227,119,194),(44,160,44),(227,119,194), (72,17,121), (196,156,148)]  
# Scale the RGB values to the [0, 1] range.
for i in range(len(colors)):  
    r, g, b = colors[i]  
    colors[i] = (r / 255., g / 255., b / 255.)
plt.figure(figsize=(12,12))
Steps=round(T/tstep); #Steps in years
S = zeros([NumSimulation, Steps], dtype=float)
x = range(0, int(Steps), 1)
for j in range(0, NumSimulation, 1):
    S[j,0]= S_init
    for i in x[:-1]:
       S[j,i+1]=S[j,i]+S[j,i]*(mu-0.5*pow(sigma,2))*tstep+ \
          sigma*S[j,i]*sqrt(tstep)*standard_normal()

Advanced Visualization
[ 806 ]
    plt.plot(x, S[j], linewidth=2., color=colors[j])
plt.title('%d Simulation using %d Steps, \n$\sigma$=%.6f $\mu$=%.6f 
$S_0$=%.6f ' % (int(NumSimulation), int(Steps), sigma, mu, S_init), 
          fontsize=18)
plt.xlabel('steps', fontsize=16)
plt.grid(True)
plt.ylabel('stock price', fontsize=16)
plt.ylim(0,90)
plt.show()
The following plot shows the results of six simulations using Brownian motion:

Chapter 8
[ 807 ]
Another simulation example here demonstrates how you can apply the Hodrick–
Prescott filter to get a smoothed curve representation of the stock price data that falls 
under the class of time series data:
Here, we will use the finance subpackage in matplotlib to generate the stock price 
data for a range of dates with the start date as May 2012 and the end date as Dec 
2014. Using the hold method of matplotlib, you can show the smoothed curve 
together with the stock price plot, as shown in the following code:
from matplotlib import finance
import matplotlib.pyplot as plt
import statsmodels.api as sm
titleStr='Stock price of FB from May. 2012 to Dec. 2014'

Advanced Visualization
[ 808 ]
plt.figure(figsize=(11,10))
dt1 = datetime.datetime(2012, 05, 01)
dt2 = datetime.datetime(2014, 12, 01)
sp=finance.quotes_historical_yahoo('FB',dt1,dt2,asobject=None)
plt.title(titleStr, fontsize=16) 
plt.xlabel("Days", fontsize=14) 
plt.ylabel("Stock Price", fontsize=14)
xfilter = sm.tsa.filters.hpfilter(sp[:,2], lamb=100000)[1]
plt.plot(sp[:,2])
plt.hold(True)
plt.plot(xfilter,linewidth=5.)
In addition to these examples, you can simulate a queue system or any process that is 
event-based. For instance, you can simulate a neural network, and one such package 
that helps to model one quickly is available at http://briansimulator.org. Take a 
look at their demo programs for more details.
Signal processing
There are many examples in signal processing that you could think of, but we will 
choose one specific example that involves convolution. A convolution of two signals 
is a way to combine them to produce a filtered third signal. In a real-life situation, 
signal convolutions are applied to smoothen images. To a great extent, convolution 
is also applied to calculate signal interference. For more details, you can refer to a 
book on microwave measurements, but we will attempt to show you some simple 
examples.
Let's consider three simple examples here. The first example illustrates the 
convoluted signal of a digital signal and simulates the analog signal using hamming, 
as shown in the following code:
import matplotlib.pyplot as plt
from numpy import concatenate, zeros, ones, hamming, convolve
digital = concatenate ( (zeros(20), ones(25), zeros(20)))
norm_hamming = hamming(80)/sum(hamming(80))
res = convolve(digital, norm_hamming)
plt.figure(figsize=(10,10))
plt.ylim(0, 0.6)
plt.plot(res, color='r', linewidth=2)

Chapter 8
[ 809 ]
plt.hold(True)
plt.plot(data, color='b', linewidth=3)
plt.hold(True)
plt.plot(norm_hamming, color='g', linewidth=4)
plt.show()
In this example, we will use concatenate and zeros and ones from numpy to 
produce digital signals, hamming to produce analog signals, and convolve to apply 
convolutions.
If we plot all the three signals, that is, digital signals, analog hammings, and 
convolved result signals (res), the resulting signal will be shifted as expected, as 
shown in the following graph:

Advanced Visualization
[ 810 ]
In another example, we will use a random signal, that is, random_data and apply 
fast Fourier transform (FFT) as follows:
import matplotlib.pyplot as plt
from scipy import randn
from numpy import fft
plt.figure(figsize=(10,10))
random_data = randn(500)
res = fft.fft(random_data)
plt.plot(res, color='b')
plt.hold(True)
plt.plot(random_data, color='r')
plt.show()
Using randn from scipy to generate random signal data and fft from numpy that 
performs fast Fourier transform, the result that comes out of the transform is plotted 
in blue and the original random signal is plotted in red using matplotlib, as shown 
in the following image:

Chapter 8
[ 811 ]
In the third example, a simple illustration of how to create an inverted image using 
the scipy package is shown. Before we get to the actual Python code and the results, 
let's try to analyze how an inverted image will help in visualizing data.
It is debated that in certain cases, inverted colors create less strain on our vision 
and is comfortable to look at. Surprisingly, if we place the original image and the 
inverted image side by side, inverted images will help in visualizing certain areas 
that may otherwise be difficult in the original image, if not for all images at least 
in certain cases. The following code shows how you can convert an image to an 
inverted image using scipy.misc.pilutil.Image():
import scipy.misc as scm 
from scipy.misc.pilutil import Image  
# open original image 
orig_image = Image.open('/Users/kvenkatr/Desktop/filter.jpg')
# extract image data into array
image1 = scm.fromimage(orig_image)
# invert array values 
inv_image = 255 - image1
# using inverted array values, convert image 
inverted_image = scm.toimage(inv_image) 
#save inverted image
inverted_image.save('/Users/kvenkatr/Desktop/filter_invert.jpg').
The inverted image result is shown along with the original image here:

Advanced Visualization
[ 812 ]
Similarly, other filtering mechanisms can be applied to any image using some of the 
following functions:
convolve()         Multidimensional convolution.
correlate()        Multi-dimensional correlation.
gaussian_filter()  Multidimensional Gaussian filter
A full list of functions is shown at http://tinyurl.com/3xubv9p.
Animation
You can accomplish animation in Python using matplotlib, but the results are 
saved in a file in the MP4 format that can be used to be replayed later. The basic 
setup for the animation is as follows:
import numpy as np 
import matplotlib.pyplot as plt 
from matplotlib import animation  
# Set up the figure, axis, and the plot element to be animated 
fig = plt.figure() 
ax = plt.axes(xlim=(0, 3.2), ylim=(-2.14, 2.14)) 
line, = ax.plot([], [], lw=2)
Make sure that the animation package is imported from matplotlib, sets the axes, 
and prepares the necessary plotting variables (this is just an empty line) as follows:
# initialization function: plot the background of each frame
def init():
    line.set_data([], [])
    return line,
The initialization of plotting needs to be performed before starting any animation 
because it creates a base frame, as shown in the following code:
# animation function.  This is called sequentially
def animate(i):
    x = np.linspace(0, 2, 1000)
    xval = 2 * np.pi * (x - 0.01 * i)
    y = np.cos(xval) # Here we are trying to animate cos function
    line.set_data(x, y)
    return line,

Chapter 8
[ 813 ]
Here is the animate function that takes the frame number as the input, defines the 
changed x and y values, and sets the plotting variables:
anim = animation.FuncAnimation(fig, animate, init_func=init,\
            frames=200, interval=20, blit=True)
anim.save('basic_animation.mp4', fps=30)
plt.show()
The actual animation object is created via FuncAnimation and passes the init() 
and animate() functions, along with the number of frames, frames per second 
(fps), and time interval parameters. The blit=True parameter tells you that only the 
changed part of the display needs to be redrawn (otherwise, one may see flickers).
Before you attempt to perform an animation, you have to make sure that mencoder or 
ffmpeg is installed; otherwise, running this program without ffmpeg or mencoder will 
result in the following error: ValueError: Cannot save animation: no writers 
are available. Please install mencoder or ffmpeg to save animations.. 
The following image shows an animation of trigonometric curves, such as sin or cos:
You can embed this MP4 file in an HTML for display and press the play button in the 
bottom-left corner to see the animation.
There is an interesting demonstration of a double pendulum animation by Jake 
Vanderplas at  
https://jakevdp.github.io/blog/2012/08/18/matplotlib-animation-tutorial/ 
and a dynamic image animation at  
http://matplotlib.org/examples/animation/dynamic_image2.html.

Advanced Visualization
[ 814 ]
In this book, so far we have discussed visualization methods that involve how to 
plot in Python or create external formats (such as MP4). One of the reasons why 
JavaScript-based visualization methods are popular is because you can present them 
on the Web and also associate some event-driven animation to them. Support Vector 
Graphics (SVG) is gaining popularity for many reasons, and one among them is the 
ability to scale to any size without losing details.
Visualization methods using HTML5
A simple illustration of SVG to display circles using feGaussianBlur is shown in the 
following code:
  <svg width="230" height="120" xmlns="http://www.w3.org/2000/svg" 
xmlns:xlink="http://www.w3.org/1999/xlink">
    <filter id="blurMe">
       <feGaussianBlur in="SourceGraphic" stdDeviation="5" />
    </filter>
    <circle cx="60"  cy="80" r="60" fill="#E90000" />
    <circle cx="190" cy="80" r="60" fill="#E90000"
      filter="url(#blurMe)" />
    <circle cx="360"  cy="80" r="60" fill="#4E9B01" />
    <circle cx="490" cy="80" r="60" fill="#4E9B01"
      filter="url(#blurMe)" />
    <circle cx="660"  cy="80" r="60" fill="#0080FF" />
    <circle cx="790" cy="80" r="60" fill="#0080FF"
      filter="url(#blurMe)" />
  </svg>
The first two circles are drawn with the radius as 60 and are filled with the same 
color, but the second circle uses the blurring filter. Similarly, adjacent circles in green 
and blue also follow the same behavior (for a colored effect, refer to  
http://knapdata.com/dash/html/svg_circle.html), as shown in the  
following image:

Chapter 8
[ 815 ]
How can we use this blurring concept when the data presentation needs parts-of-
whole in visualization, but does not combine to become a whole. What does this 
mean? Let's consider two examples. In the first example, we'll consider a class of 
students enrolled in foreign languages (in some cases, more than one language). If 
we were to represent the distribution as follows, how would we do it?
You can generate the SVG format via the Python program, as show in the following 
code:
import os
display_prog = 'more' # Command to execute to display images.
svcount=1
class Scene:
    def __init__(self,name="svg",height=400,width=1200):
        self.name = name
        self.items = []
        self.height = height
        self.width = width
        return
    def add(self,item): self.items.append(item)
    def strarray(self):
        var = [ "<html>\n<body>\n<svg height=\"%d\" width=\"%d\" >\n" 
% (self.height,self.width),
               "  <g id=\"setttings\">\n",
               "    <filter id=\"dropshadow\" height=\"160%\">\n",
               "     <feGaussianBlur in=\"SourceAlpha\" 
stdDeviation=\"5\"></feGaussianBlur>\n",
               "       <feOffset dx=\"0\" dy=\"3\" 
result=\"offsetblur\"></feOffset>\n",
               "       <feMerge>\n",
               "          <feMergeNode></feMergeNode>\n",

Advanced Visualization
[ 816 ]
               "          <feMergeNode in=\"SourceGraphic\"></
feMergeNode>\n",
               "       </feMerg>\n",
               "    </filter>\n"]
        for item in self.items: var += item.strarray()            
        var += [" </g>\n</svg>\n</body>\n</html>"]
        return var
    def write_svg(self,filename=None):
        if filename:
            self.svgname = filename
        else:
            self.svgname = self.name + ".html"
        file = open(self.svgname,'w')
        file.writelines(self.strarray())
        file.close()
        return
    def display(self,prog=display_prog):
        os.system("%s %s" % (prog,self.svgname))
        return        
def colorstr(rgb): return "#%x%x%x" % (rgb[0]/16,rgb[1]/16,rgb[2]/16)
class Text:
    def __init__(self, x,y,txt, color, isItbig, isBold):
        self.x = x
        self.y = y
        self.txt = txt
        self.color = color
        self.isItbig = isItbig 
        self.isBold = isBold
    def strarray(self):
        if ( self.isItbig == True ):
          if ( self.isBold == True ):
            retval = [" <text y=\"%d\" x=\"%d\" style=\"font-
size:18px;font-weight:bold;fill:%s\">%s</text>\n" %(self.y, self.x, 
self.color,self.txt) ]
          else:
            retval = [" <text y=\"%d\" x=\"%d\" style=\"font-
size:18px;fill:%s\">%s</text>\n" %(self.y, self.x, self.color,self.
txt) ]
        else:
          if ( self.isBold == True ):

Chapter 8
[ 817 ]
            retval = [" <text y=\"%d\" x=\"%d\" style=\"fill:%s;font-
weight:bold;\">%s</text>\n" %(self.y, self.x, self.color,self.txt) ]
          else:
            retval = [" <text y=\"%d\" x=\"%d\" style=\"fill:%s\">%s</
text>\n" %(self.y, self.x, self.color,self.txt) ]
        return retval
class Circle:
    def __init__(self,center,radius,color, perc):
        self.center = center #xy tuple
        self.radius = radius #xy tuple
        self.color = color   #rgb tuple in range(0,256)
        self.perc = perc
        return
    def strarray(self):
        global svcount
        diam = self.radius+self.radius
        fillamt = self.center[1]-self.radius - 6 + (100.0 - self.
perc)*1.9
        xpos = self.center[0] - self.radius
        retval = ["  <circle cx=\"%d\" cy=\"%d\" r=\"%d\"\n" %\
                (self.center[0],self.center[1],self.radius),
                "    style=\"stroke: %s;stroke-width:2;fill:white;filt
er:url(#dropshadow)\"  />\n" % colorstr(self.color),
               "  <circle clip-path=\"url(#dataseg-%d)\" fill=\"%s\" 
cx=\"%d\" cy=\"%d\" r=\"%d\"\n" %\
                (svcount, colorstr(self.color),self.center[0],self.
center[1],self.radius),
                "    style=\"stroke:rgb(0,0,0);stroke-width:0;z-
index:10000;\"  />\n",
               "<clipPath id=\"dataseg-%d\"> <rect height=\"%d\" 
width=\"%d\" y=\"%d\" x=\"%d\"></rect>" %(svcount,diam, 
diam,fillamt,xpos),
               "</clipPath>\n"
                ]
        svcount += 1
        return retval
def languageDistribution():
    scene = Scene('test')
    scene.add(Circle((140,146),100,(0,128,0),54))
    scene.add(Circle((370,146),100,(232,33,50),42))
    scene.add(Circle((600,146),100,(32,119,180),65))

Advanced Visualization
[ 818 ]
    scene.add(Circle((830,146),100,(255,128,0),27))
    scene.add(Text(120,176,"English", "white", False, True))
    scene.add(Text(120,196,"Speaking", "#e2e2e2", False, False))
    scene.add(Text(340,202,"German", "black", False, True))
    scene.add(Text(576,182,"Spanish", "white", False, True))
    scene.add(Text(804,198,"Japanese", "black", False, True))
    scene.add(Text(120,88,"54%", "black", True, True))
    scene.add(Text(350,88,"42%", "black", True, True))
    scene.add(Text(585,88,"65%", "black", True, True))
    scene.add(Text(815,88,"27%", "black", True, True))
    scene.write_svg()
    scene.display()
    return
if __name__ == '__main__': languageDistribution()
The preceding example gives an idea to create custom svg methods for visualization. 
There are many other svg writers in Python today, but none of them have 
demonstrated the methods to display the one that we have shown here. There are 
also many different ways to create custom visualization methods in other languages, 
such as Julia. This has been around for almost three years now and is considered 
suitable for numerical and scientific computing.
How is Julia different from Python?
Julia is a dynamic programming language. However, it is comparable to C in terms 
of performance because Julia is a low-level virtual machine-based just-in-time 
compiler (JIT compiler). As we all know, in Python, in order to combine C and 
Python, you may have to use Cython.
Some notable advantages of Julia are as follows:
•	
Performance comparable to C
•	
The built-in package manager
•	
Has lisp-like macros
•	
Can call Python functions using the PyCall package
•	
Can call C functions directly
•	
Designed for distributed computing
•	
User-defined types are as fast as built-ins

Chapter 8
[ 819 ]
The only disadvantage is that you have to learn a new language, although there are 
some similarities with C and Python.
D3.js (where D3 in short means DDD, which stands for document-driven data) is 
one among the competing frameworks in Python for visualization.
D3.js for visualization
D3.js is a JavaScript library for presenting data on the Web and helps in displaying 
data, using HTML, SVG, and CSS.
D3.js attaches data to Document Object Model (DOM) elements; therefore, you can 
use CSS3, HTML, and SVG to showcase their data. Furthermore, as JavaScript has 
event listeners, you can make the data interactive.
Mike Bostock created D3.js during his PhD work at the Stanford Visualization 
Group. First, Mike worked with the Stanford Visualization Group to produce 
Protivis, which then eventually became D3. Mike Bostock, Vadim Ogievetsky, and 
Jeffrey Heer produced a paper titled D3: Data-Driven Documents, which can be 
accessed at http://vis.stanford.edu/papers/d3.
In practice, the underlying principle of D3.js is to use the CSS style selector to select 
from DOM nodes and then use the jQuery style to manipulate them. Here is an 
example:
d3.selectAll("p")            // select all <p> elements
  .style("color", "#FF8000") // set style "color" to value "#FF8000"
  .attr("class", "tin")      // set attribute "class" to value "tin"
  .attr("x", 20);            // set attribute "x" to 20px

Advanced Visualization
[ 820 ]
One of the many advantages of D3 is that by simply accessing a mechanism of DOM, 
you can create a stunning representation of data. Another advantage is that by fully 
using the power of JavaScript combined with the power of computing today, you 
can easily add the navigational behavior quickly. There is a large collection of such 
visualizations available at http://bost.ocks.org/mike/. One example of D3 
visualization plot is shown here:
There are many visualization examples that you can produce, and among the 
examples in the gallery  
(http://christopheviau.com/d3list/gallery.html#visualizationType=lollipop), 
my favorite is the one that tells the story about different aggregations using multiple 
series and multiple axes, which can be viewed at http://tinyurl.com/p988v2u 
(also shown in the preceding image).
Dashboards
Python has many advantages compared to D3. When you combine these two, you 
can use the best of both. For instance, Python offers some very good options of 
packages for numerical and scientific computing. For this reason, it has been very 
popular to academia.

Chapter 8
[ 821 ]
There are very few interesting data visualization and collaboration tools that 
have emerged lately, and one such tool is Plotly (https://plot.ly). The Python 
dashboard collection can be accessed at https://plot.ly/python/dashboard/.  
As this is fairly new, we have not had a chance to explore further to see what  
one can do. Splunk offers an SDK to create Python-based dashboards at  
http://dev.splunk.com/view/SP-CAAADSR, and Pyxley is a collection of packages 
that combine the power of Python and JavaScript to create web-based dashboards. 
One of the examples from Splunk Dashboard is shown here:
One of the examples of Plotly is shown in the preceding image. It demonstrates 
how you can generate a visualization that looks pretty, is easy to understand, and is 
navigable at http://tinyurl.com/pwmg5zr.

Advanced Visualization
[ 822 ]
Summary
This chapter illustrates additional topics that were not covered in the previous 
chapters, such as signal processing and animation using Python. In addition, we 
also compared Python with D3.js and Julia and determined their strengths. Several 
examples of signal processing were discussed. We also looked at the convolution of 
analog and digital signal spectrums using numpy and matplotlib.
We also looked at an example of animation and demonstrated how you can generate 
the MP4 format animation via Python. We also compared Julia with Python and 
listed a few advantages of Julia over Python and compared them to see  
the differences.
Further, we showcased the strengths of D3.js, highlighting the difference between 
this JavaScript-based visualization tool and Python. Finally, we discussed the options 
available for dashboards and listed a few options to create Python-based dashboards.

[ 823 ]
Go Forth and Explore 
Visualization
Python has been around since 1991 and has gained popularity among the community 
of scientists and engineers. Among many libraries, numpy, scipy, and matplotlib 
have been widely used in scientific computing. Sage covers the areas of algebra, 
combinatorics, numerical mathematics, number theory, and calculus using an easy 
browser interface via IPython. Another popular package called pandas can be used 
to store and process complex datasets.
There are multiple tools to run and edit Python programs, and one among them 
is Anaconda from Continuum. One of the advantages of Anaconda is that it does 
not cost anything and comes inbuilt with most necessary packages. The underlying 
command-line tool for managing environments and Python packages is conda, and 
the editor is Spyder.
In the past, installing Spyder was complicated because it involved downloading 
and installing it in a multistep process. Installation in the recent versions has been 
very straightforward, and one can download and install all the components together 
automatically in one step.

Go Forth and Explore Visualization
[ 824 ]
An overview of conda
Conda is a command line-tool that is responsible for managing environments and 
Python packages, rather than using pip. There are ways to query and search the 
packages, create new environments if necessary, and install and update Python 
packages into the existing conda environments. This command-line tool also keeps 
track of dependencies between packages and platform specifics, helping you to 
create working environments from different combinations of packages. To check the 
version of conda that is running, you can enter conda --version in Python and it 
will show, for example, conda 3.18.2 as the version.
A conda environment is a filesystem directory that contains a specific collection of 
conda packages. To begin using an environment, simply set the PATH variable to 
point it to its bin directory.
Here is an example of the package installation from the command line using conda:
$ conda install scipy
Fetching package metadata: ....
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                |       build
    ---------------------|-----------------
    flask-0.10.1         |       py27_1         129 KB
    itsdangerous-0.23    |       py27_0          16 KB
    jinja2-2.7.1         |       py27_0         307 KB
    markupsafe-0.18      |       py27_0          19 KB
    werkzeug-0.9.3       |       py27_0         385 KB
The following packages will be linked:
    package              |            build
    ---------------------|-----------------
    flask-0.10.1         |       py27_1

Appendix
[ 825 ]
    itsdangerous-0.23    |       py27_0
    jinja2-2.7.1         |       py27_0
    markupsafe-0.18      |       py27_0
    python-2.7.5         |       2
    readline-6.2         |       1
    sqlite-3.7.13        |       1
    tk-8.5.13            |       1
    werkzeug-0.9.3       |       py27_0
    zlib-1.2.7           |       1
Proceed ([y]/n)? 
Any dependencies on the package that we are installing will be recognized, 
downloaded, and linked automatically.
Here is an example of package update from the command line using conda:
$ conda update matplotlib
Fetching package metadata: ....
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    freetype-2.5.2             |                0         691 KB
    conda-env-2.1.4            |           py27_0          15 KB
    numpy-1.9.2                |           py27_0         2.9 MB
    pyparsing-2.0.3            |           py27_0          63 KB
    pytz-2015.2                |           py27_0         175 KB
    setuptools-15.0            |           py27_0         436 KB
    conda-3.10.1               |           py27_0         164 KB
    python-dateutil-2.4.2      |           py27_0         219 KB
    matplotlib-1.4.3           |       np19py27_1        40.9 MB

Go Forth and Explore Visualization
[ 826 ]
    ------------------------------------------------------------
                                           Total:        45.5 MB
The following NEW packages will be INSTALLED:
    python-dateutil: 2.4.2-py27_0    
The following packages will be UPDATED:
    conda:             3.10.0-py27_0    --> 3.10.1-py27_0   
    conda-env:       2.1.3-py27_0     --> 2.1.4-py27_0    
    freetype:   2.4.10-1         --> 2.5.2-0         
    matplotlib:     1.4.2-np19py27_0 --> 1.4.3-np19py27_1
    numpy:          1.9.1-py27_0     --> 1.9.2-py27_0    
    pyparsing:      2.0.1-py27_0     --> 2.0.3-py27_0    
    pytz:           2014.9-py27_0    --> 2015.2-py27_0   
    setuptools:     14.3-py27_0      --> 15.0-py27_0     
Proceed ([y]/n)?
In some cases, there are more steps involved in installing a package via conda. For 
instance, to install wordcloud, you will have to perform the steps given in this code:
#step-1 command
conda install wordcloud
Fetching package metadata: ....
Error: No packages found in current osx-64 channels matching: wordcloud
You can search for this package on Binstar with
# This only means one has to search the source location
binstar search -t conda wordcloud
Run 'binstar show <USER/PACKAGE>' to get more details:
Packages:
                          Name | Access       | Package Types   | 

Appendix
[ 827 ]
     ------------------------- | ------------ | --------------- |
             derickl/wordcloud | public       | conda           |
Found 1 packages
# step-2 command
binstar show derickl/wordcloud
Using binstar api site https://api.binstar.org
Name:    wordcloud
Summary:
Access:  public
Package Types:  conda
Versions:
   + 1.0
To install this package with conda run:
conda install --channel https://conda.binstar.org/derickl wordcloud
# step-3 command
conda install --channel https://conda.binstar.org/derickl wordcloud
Fetching package metadata: ......
Solving package specifications: .
Package plan for installation in environment /Users/MacBook/anaconda:
The following packages will be downloaded:
    package                    |            build
    ---------------------------|-----------------
    cython-0.22                |           py27_0         2.2 MB
    django-1.8                 |           py27_0         3.2 MB
    pillow-2.8.1               |           py27_1         454 KB
    image-1.3.4                |           py27_0          24 KB
    setuptools-15.1            |           py27_1         435 KB
    wordcloud-1.0              |       np19py27_1          58 KB

Go Forth and Explore Visualization
[ 828 ]
    conda-3.11.0               |           py27_0         167 KB
    ------------------------------------------------------------
                                           Total:         6.5 MB
The following NEW packages will be INSTALLED:
    django:     1.8-py27_0
    image:      1.3.4-py27_0
    pillow:     2.8.1-py27_1
    wordcloud:  1.0-np19py27_1
The following packages will be UPDATED:
    conda:      3.10.1-py27_0 --> 3.11.0-py27_0
    cython:     0.21-py27_0   --> 0.22-py27_0
    setuptools: 15.0-py27_0   --> 15.1-py27_1
Finally, the following packages will be downgraded:
    libtiff:    4.0.3-0       --> 4.0.2-1
Proceed ([y]/n)? y
Anaconda is a free Python distribution for scientific computing. This distribution 
comes with Python 2.x or Python 3.x and 100+ cross-platform tested and optimized 
Python packages. Anaconda can also create custom environments that mix and 
match different Python versions.
Packages installed with Anaconda
The following command will display a list of all the packages in the Anaconda 
environment:
conda list 
The featured packages in Anaconda are Astropy, Cython, h5py, IPython, LLVM, 
LLVMpy, matplotlib, Mayavi, NetworkX, NLTK, Numexpr, Numba, numpy, pandas, 
Pytables, scikit-image, scikit-learn, scipy, Spyder, Qt/PySide, and VTK.

Appendix
[ 829 ]
In order to check the packages that are installed with Anaconda, navigate to the 
command line and enter the conda list command to quickly display a list of all 
the packages installed in the default environment. Alternatively, you can also check 
Continuum Analytics for details on the list of packages available in the current and 
latest release.
In addition, you can always install a package with the usual means, for example, 
using the pip install command or from the source using a setup.py file. 
Although conda is the preferred packaging tool, there is nothing special about 
Anaconda that prevents the usage of standard Python packaging tools.
IPython is not required, but it is highly recommended. IPython should 
be installed after Python, GNU Readline, and PyReadline are installed. 
Anaconda and Canopy does these things by default. There are Python 
packages that are used in all the examples in this book for a good reason. 
In the following section, we have updated the list.
Packages websites
Here is a list of Python packages that we have mentioned in this book with their 
respective websites, where you can find the most up-to-date information:
•	
IPython: This is a rich architecture for interactive computing  
(http://ipython.org)
•	
NumPy: This is used for high performance and vectorized computations on 
multidimensional arrays (http://www.numpy.org)
•	
SciPy: This is used for advanced numerical algorithms  
(http://www.scipy.org)
•	
matplotlib: This is used to plot and perform an interactive visualization 
(http://matplotlib.org)
•	
matplotlib-basemap: This is a mapping toolbox for matplotlib  
(http://matplotlib.org/basemap/)
•	
Seaborn: This is used to represent statistical data visualization for 
matplotlib (http://stanford.edu/~mwaskom/software/seaborn)
•	
Scikit: This is used for machine learning purposes in Python  
(http://scikit-learn.org/stable)
•	
NetworkX: This is used to handle graphs (http://networkx.lanl.gov)
•	
Pandas: This is used to deal with any kind of tabular data  
(http://pandas.pydata.org)

Go Forth and Explore Visualization
[ 830 ]
•	
Python Imaging Library (PIL): This is used for image processing algorithms 
(http://www.pythonware.com/products/pil)
•	
PySide: This acts as a wrapper around Qt for graphical user interfaces 
(GUIs) (http://qt-project.org/wiki/PySide)
•	
PyQt: This is similar to PySide, but with a different license  
(http://www.riverbankcomputing.co.uk/software/pyqt/intro)
•	
Cython: This is used to leverage C code in Python (http://cython.org)
About matplotlib
The matplotlib package comes with many convenient methods to create 
visualization charts and graphs. Only a handful of these have been explored in this 
book. You will have to explore matplotlib further from the following sources:
•	
http://www.labri.fr/perso/nrougier/teaching/matplotlib/
•	
http://matplotlib.org/Matplotlib.pdf
One should also refer to other packages listed in the previous section, which are 
libraries that make plotting more attractive.

Chapter No.
[ 831 ]
Bibliography
This course is packaged keeping your journey in mind. It includes content from the 
following Packt products:
•	
Getting Started with Python Data Analysis, Phuong Vo.T.H &Martin Czygan
•	
Learning Predictive Analytics with Python, Ashish Kumar
•	
Mastering Python Data Visualization, Kirthi Raman


[ 833 ]
Index
Symbol
1-nearest neighbor (1-NN)  747
A
advanced Pandas use cases
for data analysis  52
hierarchical indexing  52-54
panel data  54-56
algorithms
about  471
best practices  479
Anaconda
about  177
packages, installed  828, 829
Anaconda distribution of Spyder from  
Continuum Analytics  581
Anaconda from Continuum Analytics  590
analytics  491
animation  812-814
annotations  73-75
ANOVA  481
Anscombe's quartet
reference  502
applications and examples, predictive  
modelling
about  174
online ads, correct targeting  175
People also viewed feature, LinkedIn  174
Santa Cruz predictive policing  176
smartphone user activity, determining  176
sport and fantasy leagues  177
array creation  14
array functions  19, 20
array indexing
about  626
logical indexing  628
numerical indexing  627
Artificial Intelligence (AI)  711
author-driven narratives  556-558
B
Bagging  465
balloon layout  575
bar graphs  512, 513
bar plot  69
Bayesian linear regression  714-716
Bayes theorem  737, 738
Bell Curve  233
Berkeley Vision and Learning Center 
(BVLC)  6
best practices
for algorithms  479
for business context  481, 482
for coding  472
for data handling  478, 479
for statistics  480, 481
best practices, for coding
about  472
codes, commenting  472, 473
examples, of functions  474-476
formulas  478
functions, defining for substantial 
 individual tasks  474
hard-coding of variables, avoiding  476, 477
methods  478
standard libraries  478
version control  477

[ 834 ]
Bio package
reference  792
Bokeh
about  79, 603
differences, with matplotlib  79
plots, creating with  79
box-and-whisker plot  564, 565
boxplots
about  215, 516, 517, 564
plotting  215, 216
bubble charts  519-521
business context
best practices  481, 482
C
Caffe
about  6
reference  6
Canopy Express  588
Canopy from Enthought  581
chi-square test
about  293, 381, 481
usage  294-297
circular layout  573, 574
classification methods  724, 725
clustering
about  407, 408, 494
cases  410, 411
using  408-410
clustering, fine-tuning
about  431
elbow method  431-433
Silhouette Coefficient  433, 434
clustering, implementing with Python
about  424
cluster, interpreting  430
dataset, exporting  425
dataset, importing  424, 425
hierarchical clustering, using  
scikit-learn  426, 427
k-means clustering, using  
scikit-learn  428, 429
values in dataset, normalizing  426
code  471
coding
best practices  472
cognitive context
reference  488
command-line interface (CLI)  577
Comma Separated Value (CSV)  529
computational tools  47-49
computer simulation
about  802, 803
animation  812-814
benefits  802
dashboards  820, 821
examples  805-808
Julia  818-820
Python's, random package  803
SciPy's random  803, 804
signal, processing  808-812
types  802
visualization methods, HTML5  
used  814-818
conda  592-595, 824-828
contingency table
about  366
creating  366, 367
contour plot  70
correlation  298-304
correlation coefficient  298, 563
Correlation Matrix  301
cross-validation (CV)  162
csvkit tool  127
Cumulative Density Function  232, 233
Customer Churn Model
using  259
Cython
about  830
reference  678
D
D3.js
about  819
for visualization  819-821
dashboards  820
data
about  1, 488
appending  260-267
concatenating  260-267
dimensions  203
grouping  142, 143

[ 835 ]
indexing  46
reading  190
selecting  46, 47
structure  203, 204
summary  202
versus oil  169
data aggregation  139-141
data analysis
about  2, 491
algorithms  5
artificial intelligence  3
computer science  3
data cleaning  4
data collection  4
data processing  4
data product  5
data requirements  4
domain knowledge  3
exploratory data analysis  4
knowledge domain  3
libraries  5
machine learning  3
mathematics  3
modelling  5
process  2
Python libraries  7
statistics  3
steps  4, 5
data analytics  491
data collection  206, 493
data extraction  206
DataFrame  36, 37
data grouping
about  246
aggregation  250-252
filtering  253
illustration  246-250
miscellaneous operations  255-257
transformation  254, 255
data handling
about  471
best practices  478, 479
data importing, in Python
about  191
dataset, reading from URL  200, 201
dataset, reading with open method  197
dataset, reading with read_csv method  192
miscellaneous cases  201
data, in binary format
HDF5  112, 113
interacting with  111
data, in MongoDB
interacting with  113-117
data, in Redis
interacting with  118
list  119
ordered set  121
set  120, 121
simple value  118
data, in text format
interacting with  105
reading  105-109
writing  110
data munging
about  126, 127
data, cleaning  128, 130
data, merging  134-137
data, reshaping  137, 138
filtering  131-133
data preprocessing  493, 494
data processing  494
data processing, using arrays
about  21
data, loading  23
data, saving  22
dataset
columns, selecting  221, 222
combination of rows and columns,  
selecting  225-227
merging/joining  268-274
obtaining  495
new columns, creating  227
rows, selecting  223, 224
sub-setting  220
visualizing, by basic plotting  212
dataset, reading with open method
about  197
delimiter, changing  199, 200
reading line by line  197, 199
data structure, Pandas
about  32
DataFrame  34-37
Series  32, 33

[ 836 ]
data structures
dictionaries  632-634
queues  632
sets  631
stacks  629
tries  639, 640
tuples  630
data transformation
about  491, 492
data collection  492
data, organizing  494
data preprocessing  493, 494
data processing  494
datasets, getting  495
data types  12-14
data visualization
about  503
before computers  498, 499
developments  500
history  497
reference  498
date and time objects
working with  84-91
decision tree
about  437-440, 732
example  732, 734
mathematics  441
using  440, 441
decision tree, implementing with  
scikit-learn
about  453-455
decision tree, cross-validating  458, 459
decision tree, pruning  458, 459
tree, visualizing  455-457
delimiter  191
deterministic model
about  666
gross return  666-676
dictionaries
about  632-634
for matrix representation  634
memoization  638
sparse matrices  635
diffusion-based simulation  704, 705
directed acyclic graph test  788-790
directed graphs  768
Disco
reference  624
distance matrix  412, 413
distances, between two observations
Euclidean distance  412
Manhattan distance  412
Minkowski distance  412
Document Object Model (DOM)  
elements  819
dummy data frame
generating  243-245
dummy variables
creating  211
E
Ebola example
about  529-535
reference  530
economic model  665
elbow method  431
equal (eq) function  41
essential functionalities
about  38
binary operations  40, 41
functional statistics  41-43
function application  43
head and tail  39
labels, altering  38
labels, reindexing  38
sorting  44, 45
Euclidean distance  412
event listeners  571, 572
F
fancy indexing  17
fast Fourier transform (FFT)  810
financial model  665
flow network
maximum flow  790, 791
font file
reference  649
frames per second (fps)  813
F-statistics
about  320, 321
significance  321

[ 837 ]
functions
plotting, with Pandas  76-78
Fundamental Algorithmic and Statistical 
Tools Laboratory (FASTLab)  6
G
Gapminder  549, 550
genetic programming
example  792-794
geometric Brownian simulation  700-704
Gestalt perception
principles  559-561
good visualization  504-506
graph data
storing  769
graphical user interfaces (GUIs)  830
graphs
clustering coefficient  780-784
displaying  770
igraph  770-773
NetworkX  773-778
graph-tool
about  779, 780
PageRank  780
reference  779
greater equal (ge) function  41
greater than (gt) function  41
guidelines, for selecting predictor variables
F-statistic  337
p-values  337
R2  337
RSE  337
VIF  337
H
Harvard Business Review (HBR)  168
heteroscedasticity  360
hierarchical clustering  417-420
histogram plot  72
histograms
about  214, 564
plotting  214
Humanitarian Data Exchange (HDX)  529
hypothesis testing
about  285
confidence intervals  287, 288
example  292, 293
null hypothesis, versus alternate  
hypothesis  285
p-values  289
significance levels  287, 288
step-by-step guide  291
t-statistic  286
types  289
z-statistic  285
hypothesis tests
left-tailed  290
right-tailed  290
two-tailed  291
I
IDEs, for Python
about  184
IDLE  184
IPython Notebook  184
Spyder  185
IDE tools
about  578
interactive tools, types  578
Python 3.x versus Python 2.7  578
IDLE
about  184
features  184
igraph  770-773
information
about  489
transforming, to insight  496, 497
transforming, to knowledge  495, 496
information visualization  558
Inner Join
about  274
characteristics  274
example  277
Integrated Development Environment  
(IDE)  569
interactive tools
about  578
IPython  579, 580
Plotly  580, 581
Interactive visualization packages  602, 603
interpolation  92
Inter Quartile Range(IQR)  358

[ 838 ]
intra-cluster distance  431
IPython
about  183, 579, 829
references  183, 570
IPython Notebook
about  184
features  184
Iris-Setosa  149
Iris-Versicolour  149
Iris-Virginica  149
issues handling, in linear regression
about  339
categorical variables, handling  341-347
outliers, handling  353-357
variable, transforming to fit non-linear 
 relations  347-353
J
JIT (just-in-time) compilation  624
joins
summarizing  279
jq tool  127
Julia  818, 819
K
Kernel Density Estimation (KDE)  522-525
k-means clustering  420-423, 762-765
k-nearest neighbors (kNN)  712, 713
knowledge matrix, predictive  
modelling  172
L
layouts
balloon layout  575
circular layout  573, 574
radial layout  574
Left Join
about  274
characteristics  274
example  278
left-tailed test  290
legends  73-75
less equal (le) function  41
less than (lt) function  41
libraries, for data processing
Mirador  7
Modular toolkit for data processing 
 (MDP)  7
Natural language processing toolkit 
(NLTK)  7
Orange  7
RapidMiner  7
Statsmodels  7
Theano  7
libraries, implemented in C++
Caffe  6
MLpack  6
MultiBoost  6
Vowpal Wabbit  6
libraries, in data analysis
Mahout  6
Mallet  6
overview  5
Spark  6
Weka  5
Likelihood Ratio Test statistic  380
linear algebra
about  24
with NumPy  24, 25
linear models  714
linear regression
about  307, 725-731
assumptions  358-360
considerations  358-360
issues, handling  339, 340
versus logistic regression  364
linear regression, implementing with 
 Python
about  322
multi-collinearity  332, 333
multiple linear regression  326-332
statsmodel library, using  323-326
Variance Inflation Factor (VIF)  333
linkage methods
about  415
average linkage  416
centroid linkage  416
compete linkage  416
single linkage  415
Ward's method  416

[ 839 ]
logical indexing  628
logistic regression
about  751-755
math  365
scenarios  364
logistic regression model
cross validation  396, 397
evaluation  395, 396
validation  394-396
logistic regression parameters
about  379
chi-square test  381
Likelihood Ratio Test statistic  380
Wald test  380
logistic regression, with Python
data exploration  384
data, processing  383
data visualization  385-389
dummy variables, creating for categorical 
variables  389, 390
feature selection  391
implementing  382
model, implementing  392, 393
M
machine learning (ML)  2, 711-723
machine learning models
defining  145, 146
supervised learning  146
unsupervised learning  146
Mahout
about  6
reference  6
Mallet
about  6
reference  6
Manhattan distance  412
math, behind logistic regression
about  365
conditional probability  367, 368
contingency tables  366, 367
estimation, using Maximum Likelihood 
Method  373-376
logistic regression model, building from 
scratch  377-379
moving to logistic regression  370-372
odds ratio  368-370
mathematics, behind clustering
about  411
distance matrix  412, 413
distances, between two observations  411
distances, normalizing  413
hierarchical clustering  417-420
k-Means clustering  420-423
linkage methods  415
mathematics, decision tree
continuous numerical variable,  
handling  450, 451
entropy  443-445
Gini index  448
homogeneity  442, 443
ID3 algorithm  447, 448
information gain  445-447
missing value of attribute,  
handling  451, 452
Reduction in Variance  449
tree, pruning  449, 450
maths, behind linear regression
about  309-311
linear regression model efficacy,  
checking  312-316
linear regression model, fitting  312
optimum value of variable coefficients, 
finding  317, 318
simulated data, using  311
matplotlib
about  829, 183
reference  183, 484
sources  830
Matplotlib API Primer
about  60-62
figures  65
line properties  63, 64
subplots  65-67
matplotlib-basemap  829
Mayavi  596
miles per gallon (mpg)  348
Minkowski distance  412
Mirador
about  7
reference  7

[ 840 ]
miscellaneous cases, data reading
CSV or Excel file, writing to  202
reading, from .xls or .xlsx file  202
missing data  49-51
missing values
about  206
checking for  205, 206
deletion  207
generating  206, 207
handling  204
imputation  207-210
propagating  207
treating  207
MKL functions  622, 623
MLpack
about  6
reference  6
model validation
about  334, 398
data split, testing  334-336
data split, training  334-336
feature selection, with scikit-learn  338, 339
guidelines, for selecting variables  337
linear regression with scikit-learn  337, 338
models, summarizing  336
Modular toolkit for data processing (MDP)
about  7
reference  7
Monte Carlo simulation
about  677
for finding value of pi  239
implied volatilities  693-697
in basketball  682-688
inventory problem  678-682
reference  678
volatility plot  688-692
MultiBoost  6
multi-collinearity  332
multigraphs  768
N
Naïve Bayes classifier
about  738, 739
TextBlob, installing  740
TextBlob used  740-744
natural language processing  
(NLP) tasks  740
Natural language processing toolkit 
 (NLTK)  7
NetworkX  596, 829
normal distribution  236-239
not equal (ne) function  41
null hypothesis
versus alternate hypothesis  285
numerical indexing  627
NumPy
about  8, 11, 182, 608, 829
functions, vectorizing  612-614
interpolation, example  611
linear algebra, summary  614
linear algebra with  24
random numbers  25-28
reference  182
reshape manipulation  610
shape manipulation  610
universal functions  608, 609
NumPy arrays
about  12
array creation  14
data type  12, 14
fancy indexing  17
indexing  16
numerical operations on arrays  18
slicing  16
O
Orange
about  7
reference  7
outliers
about  353
handling  353-357
overfitting  161
P
pajek format
reference  771
Pandas
about  8, 829
data structure  32

[ 841 ]
package overview  31
parsing functions  109
reference  483
parameters, random forest
node size  468
number of predictors sampled  468
number of trees  468
PEP8
about  12
reference  12
perception and presentation methods
about  558, 559
Gestalt principles  559-561
pie charts  512-515
pip
installing  179-181
planar graph test  786-788
Plotly  580, 581, 596
plots
animated and interactive plots,  
creating  717-722
plot types
bar plot  69
contour plot  70
exploring  68
histogram plot  72
scatter plot  68
portfolio valuation  697-699
positive sentiments
viewing, word clouds used  745
prediction performance
measuring  160-162
predictive analytics  168
predictive modelling
about  167
applications and examples  174
business context  171, 172
historical data  170
knowledge matrix  172
mathematical function  171
scope  169
statistical algorithms  169
statistical tools  170
task matrix  173, 174
predictor variables
about  326
backward selection approach  327
forward selection approach  327
Principal Component Analysis (PCA)
about  158, 757-760
scikit-learn, installing  762
Probability Density Function (PDF)  232, 522
probability distributions
about  232
Cumulative Density Function  232
p-values  319, 320
PyCharm  581-583
PyDev  581-584
pygooglechart  596
PyMongo  9
PyQt  830
PySide  830
Python
about  577, 823
IDE tools  578
packages  829
performance  623
Python 3.x
versus Python 2.7  578
Python data visualization tools
about  78
Bokeh  79
MayaVi  79, 80
Python IDE, types
about  581
Anaconda from Continuum  
Analytics  590, 591
Canopy, from Enthought  586-589
Interactive Editor for Python (IEP)  584, 585
Python Imaging Library (PIL)  830
Python libraries, in data analysis
about  7
Matplotlib  9
NumPy  8
Pandas  8
PyMongo  9
scikit-learn library  9

[ 842 ]
Python packages
about  177
Anaconda  177
installing  179
installing, with pip  181, 182
Standalone Python  178
Python packages, for predictive modelling
about  182
IPython  183
matplotlib  183
NumPy  182
pandas  182
scikit-learn  183
Q
q tool  127
queues  632
R
radial layout  574
random forest
about  464, 465
features  467
implementing, using Python  466, 467
parameters  468
random numbers
about  228
generating  228
generating, following probability  
distributions  232
methods, for generating  228-230
seeding  231
usage  228
random sampling
about  257, 258
and central limit theorem  284
Customer Churn Model, using  259
dataset, splitting  257, 258
dataset, testing  257, 258
shuffle function, using  260
sklearn, using  259
RapidMiner
about  7
reference  7
read_csv method
about  192, 193
dtype  193
filepath  193
header  193
index_col  194
na-filter  194
names  194
sep  193
skip_blank_lines  194
skiprows  194
use cases  194
reader-driven narratives
about  548
example narratives  555
Gapminder  549, 550
union address, state  550
USA, mortality rate  551-554
Receiver Operating Characteristic (ROC) 
curve  398
Recursive Feature Elimination (RFE)  338
regression tree algorithm  460
regression trees
about  459
advantages  461
implementing, with Python  461-464
Relative Strength Indicator (RSI)
reference  654
Residual Standard Error (RSE)  321, 322
result parameters
about  319
F-statistics  320, 321
p-values  319, 320
Residual Standard Error (RSE)  321, 322
retrospective analytics  168
Right Join
about  275
characteristics  275-277
example  278, 279
right-tailed test  290
ROC curve
about  398-400
confusion matrix  400-404

[ 843 ]
S
Scalar selection  624
scatter plot
about  68, 212, 517-519
plotting  213
reference  518
Schelling Segregation Model (SSM)  707
Scientific PYthon Development  
EnviRonment (Spyder)
about  185, 590
components  591, 592
features  185
scientific visualization  558
scikit-learn
about  183
features  183
installing  762
reference  183, 731
scikit-learn modules
data representation, defining  148-150
defining, for different models  146, 147
SciPy
about  608, 615-618, 829
linear equations, example  619
packages  615
reference  484
vectorized numerical derivative  620
Seaborn  829
Sensitivity (True Positive Rate)  399
series  32, 33
sets  631
shuffle function
using  260
signal processing  808
Silhouette Coefficient  433, 434
Single Instruction Multiple Data (SIMD)  11
sklearn
using  259
slicing
about  625, 626
flat used  626
social networks
analysis  784-786
Spark
about  6
reference  6
sparse matrices
visualize sparseness  636, 637
Specificity (True Negative Rate)  399
sports example
about  535-537
reference  535
results, visually representing  538-542, 547
square map plot  598-600
SSA module
reference  795
stacks  629
standalone Python  178
statistical algorithms, predictive modelling
about  169
supervised algorithms  170
un-supervised algorithms  170
statistical learning  711, 712
statistics
about  472
best practices  480, 481
statistics functions  41-43
Statsmodels  7
Stochastic block models  794-797
Stochastic Differential Equation (SDE)  805
stochastic model
about  677
diffusion-based simulation  704, 705
geometric Brownian simulation  700-704
Monte Carlo simulation  677
portfolio valuation  697-699
simulation model  700
stories
author-driven narratives  548, 556, 557
creating, with data  548
reader-driven narratives  548
supervised learning
about  150-155
classification  150-155
classification problems  146
regression  150-155
regression problems  146
Support Vector Machine (SVM)  152, 755

[ 844 ]
surface-3D plot  596-598
sypder-app  570
T
tab completion
reference  570
task matrix, predictive modelling  173, 174
TextBlob
references  650, 738
text method  75
Theano  7
threshold model  707
Timedeltas  98
time series
plotting  99-102
resampling  92
time series data
downsampling  92-94
unsampling  95, 96
time series primer  83
time zone handling  97
tries  639, 640
t-statistic  287
t-test (Student-t distribution)  286, 481
tuples  630, 631
Twitter text  647-650
two-tailed test  291
U
uniform distribution  233-236
unsupervised learning
clustering  156-160
defining  156-160
dimensionality reduction  156-160
use cases, read_csv method
.txt dataset, reading with comma 
 delimiter  195
about  194
dataset column names, specifying from 
 list  195, 196
directory address and filename, passing as 
variables  194, 195
V
value of pi
calculating  240-242
Variance Inflation Factor (VIF)  333
Veusz  596
VisPy
about  603-605
reference  605
visualization
about  502, 503
benefits  501
example  659-662
information visualization  558
matplotlib used  641
planning, need for  528
plots  507-511
references  501, 511
scientific visualization  558
visualization, best practices
about  561
comparison and ranking  562
correlation  562, 563
distribution  564, 565
location-specific or geodata  566
part to whole  567
trends over time  568
visualization, interactive
about  571
event listeners  571, 572
layouts  572
visualization plots, with Anaconda
about  595, 596
square map plot  598-600
surface-3D plot  596-598
visualization toolkit (VTK)  79
visualization tools, in Python
about  568
Anaconda, from continuum analytics  570
Canopy, from Enthought  569
development tools  569
Vowpal Wabbit
about  6
URL  6
VSTOXX data
references  690, 697

[ 845 ]
W
Wakari  603
Wald test  380
web feeds  645
Weka
about  5
URL  5
word clouds
about  642
data, obtaining  650-658
input for  645
installing  642
stock price chart, plotting  650
Twitter text  647-650
used, for viewing positive sentiments  745
web feeds  645
X
xmlstarlet tool  127
Z
Z-statistic  285
Z- test (normal distribution)  286


Thank you for buying  
Python: Data Analytics and 
Visualization
About Packt Publishing
Packt, pronounced 'packed', published its first book, Mastering phpMyAdmin for Effective 
MySQL Management, in April 2004, and subsequently continued to specialize in publishing 
highly focused books on specific technologies and solutions.
Our books and publications share the experiences of your fellow IT professionals in adapting 
and customizing today's systems, applications, and frameworks. Our solution-based books 
give you the knowledge and power to customize the software and technologies you're using 
to get the job done. Packt books are more specific and less general than the IT books you have 
seen in the past. Our unique business model allows us to bring you more focused information, 
giving you more of what you need to know, and less of what you don't.
Packt is a modern yet unique publishing company that focuses on producing quality,  
cutting-edge books for communities of developers, administrators, and newbies alike.  
For more information, please visit our website at www.packtpub.com.
Writing for Packt
We welcome all inquiries from people who are interested in authoring. Book proposals should 
be sent to author@packtpub.com. If your book idea is still at an early stage and you would 
like to discuss it first before writing a formal book proposal, then please contact us; one of our 
commissioning editors will get in touch with you. 
We're not just looking for published authors; if you have strong technical skills but no writing 
experience, our experienced editors can help you develop a writing career, or simply get some 
additional reward for your expertise.
www.packtpub.com


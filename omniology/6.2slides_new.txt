1 
Modeling Networks of Neurons 
Feedforward 
Recurrent 
Image Source: Dayan & Abbott textbook  
2 
Modeling Networks: Spiking versus Firing Rate 
F Option 1: Model networks using Spiking neurons  
Advantages: Model computation and learning based on: 
Spike Timing 
Spike Correlations/Synchrony between neurons 
Disadvantages: Computationally expensive 
F Option 2: Use neurons with firing-rate outputs (real 
valued outputs) 
Advantages: Greater efficiency, scales well to large networks 
Disadvantages: Ignores spike timing issues 
F Question: How are these two approaches related? 

3 
Recall: Linear Filter Model of a Synapse 
 
 
 
 
 
 




d
t
K
g
t
t
K
g
t
g
t
b
b
t
t
i
b
b
i
)
(
)
(
)
(
)
(
max
,
max
,









Synaptic conductance at b:  
b(t) = i δ(t-ti)   (ti are the input spike times, δ = delta function) 
Filter for 
synapse b = 
)
(t
K
Synapse b 
Input Spike Train  b(t) 
4 
From a Single Synapse to Multiple Synapses 
wN 
Spike trains  1(t) 







N
b
t
b
b
s
d
t
K
w
t
I
1
)
(
)
(
)
(







N
b
b
s
t
I
t
I
1
)
(
)
(
Total synaptic current 
w1 
N(t) 
Synaptic weights 

5 
From Spiking to Firing Rate Model 
Firing rate ub(t) 
Spike train b(t) 














N
b
t
b
b
N
b
t
b
b
s
d
u
t
K
w
d
t
K
w
t
I
1
1
)
(
)
(
)
(
)
(
)
(







Firing rate u1(t) 
uN(t) 
wN 
Spike trains  1(t) 
w1 
N(t) 
Synaptic weights 
Total 
synaptic 
current 
6 
Simplifying the Input Current Equation 
Suppose synaptic filter K is exponential: 
 
Differentiating                                                        w.r.t. time t, 
 
 
we get 
 






b
t
b
b
s
d
u
t
K
w
t
I



)
(
)
(
)
(
u
w







s
b
b
b
s
s
s
I
u
w
I
dt
dI

s
t
s
e
t
K



1
)
(
Firing rate u1(t) 
uN(t) 
wN 
w1 
Synaptic weights 
Weight vector w 
Input vector u 

7 
Firing-Rate-Based Network Model 
))
(
(
t
I
F
v
dt
dv
s
r




u
w 



s
s
s
I
dt
dI

Output firing rate 
changes like this: 
Input current 
changes like this: 
8 
What if there are multiple output neurons? 
)
W
(
u
v
v
F
dt
d




)
(
u
w 



F
v
dt
dv

u
w 

sI
(Assuming relatively fast 
synapses,                   at each t) 
Vector v 
Matrix W 
Vector u 
Scalar v 
Vector w 
Vector u 
Single Output 
Input Vector 
Output Vector 
Input Vector 

9 
Feedforward versus Recurrent Networks 
)
M
W
(
v
u
v
v




F
dt
d

For feedforward networks, M = matrix of zeros 
Output       Decay 
    Input     Feedback 
Feedforward 
Recurrent 
Image Source: Dayan & Abbott textbook  
10 
Example: Linear Feedforward Network 
u
v
v
W



dt
d

Dynamics: 
 
Steady State  
(set dv/dt to 0): 
u
v
W

ss










































1
2
2
2
1
   
          
1
0
0
0
1
1
1
0
0
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
0
0
1
W
u
What is vss? 

11 
Linear Feedforward Network 






























































0
1
0
0
1
0
1
2
2
2
1
 
1
0
0
0
1
1
1
0
0
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
0
0
1
Wu
vss
What is the network doing? 
12 
Network is performing Linear Filtering for 
Edge Detection 









































0
1
0
0
1
0
 
Output
    
1
2
2
2
1
Input 
W)
 
in
 
 versions
shifted
 
(and
0
0
1
1
0
 
Filter 
Input 
Output 

13 
Example of Edge Detection in a 2D Image 
Image from http://www.alexandria.nu/ai/blog/entry.asp?E=51 
14 
Edge detectors in the brain 
Examples of 
receptive 
fields in 
primary 
visual cortex 
(V1) 
Retina 
Lateral 
Geniculate 
Nucleus (LGN) 
Primary 
Visual Cortex 
(V1) 
+ 
+ 
+ 

15 
The Brain can do Calculus! 
)
(
)1
(
ion
approximat
 
Discrete
)
(
)
(
lim
0
x
f
x
f
h
x
f
h
x
f
dx
df
h










)1
(
)
(
2
)1
(
)1
(
)
(
)
(
)1
(
approx.
 
Disc.
)
(
)
(
lim
0
2
2

















x
f
x
f
x
f
x
f
x
f
x
f
x
f
h
x
f
h
x
f
dx
f
d
h


0
0
1
1
0



0
1
2
1
0

+ 
+ 
+ 
V1 neurons are basically computing derivatives! 
16 
Next Lecture: Recurrent Networks 
v
u
v
v
M
W 



dt
d

Output        Decay     Input      Feedback 



ROS Robotics By Example
Bring life to your robot using ROS robotic applications
Carol Fairchild
Dr. Thomas L. Harman
BIRMINGHAM - MUMBAI

ROS Robotics By Example
Copyright © 2016 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval 
system, or transmitted in any form or by any means, without the prior written 
permission of the publisher, except in the case of brief quotations embedded in 
critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy 
of the information presented. However, the information contained in this book is 
sold without warranty, either express or implied. Neither the authors, nor Packt 
Publishing, and its dealers and distributors will be held liable for any damages 
caused or alleged to be caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the 
companies and products mentioned in this book by the appropriate use of capitals. 
However, Packt Publishing cannot guarantee the accuracy of this information.
First published: June 2016
Production reference: 1240616
Published by Packt Publishing Ltd.
Livery Place
35 Livery Street
Birmingham B3 2PB, UK.
ISBN  978-1-78217-519-3
www.packtpub.com

Credits
Authors
Carol Fairchild
Dr. Thomas L. Harman
Reviewer
Shweta Gupte
Commissioning Editor
Dipika Gaonkar
Acquisition Editors
Vivek Anantharaman
Prachi Bisht
Content Development Editor
Mehvash Fatima
Technical Editor
Devesh Chugh
Copy Editors
Roshni Banerjee
Gladson Monteiro
Rashmi Sawant
Project Coordinator
Kinjal Bari
Proofreader
Safis Editing
Indexer
Hemangini Bari
Graphics
Kirk D'Penha
Production Coordinator
Shantanu N. Zagade
Cover Work
Shantanu N. Zagade

About the Authors
Carol Fairchild is the owner and principal engineer of Fairchild Robotics, a 
robotics development and integration company. She is a researcher at Baxter's Lab at 
the University of Houston–Clear Lake (UHCL) and a member of the adjunct faculty. 
Her research involves the use of Baxter for expanded applications. Ms. Fairchild has 
been involved in many aspects of robotics from her earliest days of building her 
first robot, a Heathkit Hero. She has an MS in computer engineering from UHCL 
and a BS in engineering technology from Texas A&M. Ms. Fairchild has taught 
middle-school robotics, coached FLL, and volunteered for FIRST Robotics 
in Houston.
Dr. Thomas L. Harman is the chair of the engineering division at UHCL. 
His research interests are control systems and applications of robotics and 
microprocessors. Several of his research papers with colleagues involve robotic and 
laser applications in medicine. In 2005, he was selected as the UHCL Distinguished 
Professor. He has been a judge and safety advisor for the FIRST robotic contests in 
Houston. Dr. Harman has authored or coauthored 18 books on subjects including 
microprocessors, MATLAB and Simulink applications, and the National Electrical 
Code. His laboratory at UHCL has a Baxter two-armed robot and several TurtleBots 
as well as other robots.

About the Reviewer
Shweta Gupte is an embedded systems engineer with 4 years of experience in 
the robotics field. She has worked with robots both at the software level and at the 
low-level hardware interface. She has experience in working with embedded system 
design, RTOS, microcontroller programming and low-level robot interfaces , motion 
planning, and computer vision.
Shweta has experience programming in C, embedded C, Java, MATLAB, and 
microcontroller assembly language. She has worked with low-level controllers, 
including the Renesas RX62, Renesas QSK, Arduino, PIC, and Rasberry Pi.
Currently, she is working at Mathworks to develop MATLAB and Simulink products 
for robotics and embedded targets, including developing automated test beds and 
hardware testing. She has a master's degree in electrical engineering and a bachelor's 
degree in electronics and telecommunication. Her interests include playing with low-
cost microcontroller-based robots and quadrotors. She likes to spend her spare time 
rock climbing and fostering rescued dogs.

www.PacktPub.com
eBooks, discount offers, and more
Did you know that Packt offers eBook versions of every book published, with PDF 
and ePub files available? You can upgrade to the eBook version at www.PacktPub.
com and as a print book customer, you are entitled to a discount on the eBook copy. 
Get in touch with us at customercare@packtpub.com for more details.
At www.PacktPub.com, you can also read a collection of free technical articles, sign 
up for a range of free newsletters and receive exclusive discounts and offers on Packt 
books and eBooks.
TM
https://www2.packtpub.com/books/subscription/packtlib
Do you need instant solutions to your IT questions? PacktLib is Packt's online digital 
book library. Here, you can search, access, and read Packt's entire library of books.
Why subscribe?
•	
Fully searchable across every book published by Packt
•	
Copy and paste, print, and bookmark content
•	
On demand and accessible via a web browser

[ i ]
Table of Contents
Preface	
xi
Chapter 1: Getting Started with ROS	
1
What does ROS do and what are the benefits of learning ROS?	
2
Who controls ROS?	
3
Which robots are using ROS?	
3
Installing and launching ROS	
5
Configuring your Ubuntu repositories	
5
Setting up your sources.list	
6
Setting up your keys	
6
Installing ROS Indigo	
6
Initialize rosdep	
7
Environment setup	
8
Getting rosinstall	
8
Troubleshooting – examining your ROS environment	
8
Creating a catkin workspace	
9
ROS packages and manifest	
10
ROS manifest	
11
Exploring the ROS packages	
11
rospack find packages	
12
rospack list	
13
ROS nodes and ROS Master	
13
ROS nodes	
14
Nodes can publish and nodes can subscribe	
14
ROS Master	
15
Invoking the ROS Master using roscore	
15
ROS commands to determine the nodes and topics	
17

Table of Contents
[ ii ]
Turtlesim, the first ROS robot simulation	
18
Starting turtlesim nodes	
19
rosrun command	
19
Turtlesim nodes	
21
Turtlesim topics and messages	
23
rostopic list	
24
rostopic type	
24
rosmsg list	
24
rosmsg show	
25
rostopic echo	
25
Parameter Server of turtlesim	
26
rosparam help	
27
rosparam list for /turtlesim node	
27
Change parameters for the color of the turtle's background	
28
ROS services to move turtle	
29
rosservice call	
30
ROS commands summary	
31
Summary	
32
Chapter 2: Creating Your First Two-Wheeled ROS Robot 
(in Simulation)	
33
Rviz	
34
Installing and launching rviz	
34
Using rviz	
36
Displays panel	
36
Views and Time panels	
37
Toolbar	
38
Main window menu bar	
38
Creating and building a ROS package	
39
Building a differential drive robot URDF	
40
Creating a robot chassis	
41
Using roslaunch	
42
Adding wheels	
45
Adding a caster	
48
Adding color	
49
Adding collisions	
51
Moving the wheels	
52
A word about tf and robot_state_publisher	
54
Adding physical properties	
54
Trying URDF tools	
56
check_urdf	
56
urdf_to_graphiz	
56

Table of Contents
[ iii ]
Gazebo	
57
Installing and launching Gazebo	
58
Using roslaunch with Gazebo	
59
Using Gazebo	
60
Environment toolbar	
60
World and Insert panels	
61
Joints panel	
62
Main window menu bar	
62
Simulation panel	
63
Modifications to the robot URDF	
63
Adding the Gazebo tag	
63
Specifying color in Gazebo	
64
A word about <visual> and <collision> elements in Gazebo	
64
Verifying a Gazebo model	
64
Viewing the URDF in Gazebo	
65
Tweaking your model	
67
Moving your model around	
67
Other ROS simulation environments	
69
Summary	
69
Chapter 3: Driving Around with TurtleBot	
71
Introducing TurtleBot	
72
Loading TurtleBot simulator software	
74
Launching TurtleBot simulator in Gazebo	
75
Problems and troubleshooting	
77
ROS commands and Gazebo	
78
Keyboard teleoperation of TurtleBot in simulation	
80
Setting up to control a real TurtleBot	
82
TurtleBot standalone test	
83
Networking the netbook and remote computer	
84
Types of networks	
85
Network addresses	
85
Remote computer network setup	
87
Netbook network setup	
88
Secure Shell (SSH) connection	
88
Summary of network setup	
89
Troubleshooting your network connection	
89
Testing the TurtleBot system	
90
TurtleBot hardware specifications	
91
TurtleBot dashboard	
92

Table of Contents
[ iv ]
Move the real TurtleBot	
93
Using keyboard teleoperation to move TurtleBot	
94
Using ROS commands to move TurtleBot around	
95
Writing your first Python script to control TurtleBot	
97
Introducing rqt tools	
100
rqt_graph	
101
rqt message publisher and topic monitor	
104
TurtleBot's odometry	
106
Odom for the simulated TurtleBot	
107
Real TurtleBot's odometry display in rviz	
110
TurtleBot automatic docking	
111
Summary	
113
Chapter 4: Navigating the World with TurtleBot	
115
3D vision systems for TurtleBot	
116
How these 3D vision sensors work	
116
Comparison of 3D sensors	
117
Microsoft Kinect	
117
ASUS	
119
Obstacle avoidance drawbacks	
122
Configuring TurtleBot and installing the 3D sensor software	
122
Kinect	
122
ASUS and PrimeSense	
124
Camera software structure	
124
Defining terms	
125
Testing the 3D sensor in standalone mode	
125
Running ROS nodes for visualization	
126
Visual data using Image Viewer	
126
Visual data using rviz	
128
Navigating with TurtleBot	
132
Mapping a room with TurtleBot	
133
Defining terms	
133
Building a map	
134
How does TurtleBot accomplish this mapping task?	
137
Autonomous navigation with TurtleBot	
138
Defining terms	
139
Driving without steering TurtleBot	
140
rviz control	
141
How does TurtleBot accomplish this navigation task?	
144
rqt_reconfigure	
146
Exploring ROS navigation further	
148
Summary	
149

Table of Contents
[ v ]
Chapter 5: Creating Your First Robot Arm (in Simulation)	
151
Features of Xacro	
152
Expanding Xacro	
153
Building an articulated robot arm URDF using Xacro	
154
Using the Xacro property tag	
154
Using roslaunch for rrbot	
157
Using Xacro include and macro tags	
159
Adding mesh to the robot arm	
163
Controlling an articulated robot arm in Gazebo	
169
Adding Gazebo-specific elements	
170
Fixing the robot arm to the world	
171
Viewing the robot arm in Gazebo	
172
Adding controls to the Xacro	
173
Defining transmission elements for joints	
174
Adding a Gazebo ROS control plugin	
175
Creating a YAML configuration file	
175
Creating a control launch file	
176
Controlling your robot arm with the ROS command line	
178
Controlling your robot arm with rqt	
180
Trying more things in rqt	
182
Summary	
182
Chapter 6: Wobbling Robot Arms Using Joint Control	
183
Introducing Baxter	
184
Baxter, the research robot	
186
Baxter Simulator	
187
Baxter's arms	
188
Baxter's pitch joints	
190
Baxter's roll joints	
190
Baxter's coordinate frame	
192
Control modes for Baxter's arms	
193
Baxter's grippers	
194
Baxter's arm sensors	
195
Loading the Baxter software	
195
Installing Baxter SDK software	
196
Installing Baxter Simulator	
197
Configuring the Baxter shell	
199
Installing MoveIt	
200
Launching Baxter Simulator in Gazebo	
201
Bringing Baxter Simulator to life	
201
Warm-up exercises	
206

Table of Contents
[ vi ]
Flexing Baxter's arms	
208
Tucking and untucking	
208
Wobbling arms	
211
Controlling arms and grippers with a keyboard	
212
Controlling arms and grippers with a joystick	
214
Controlling arms with a Python script	
217
Recording and replaying arm movements	
219
Tuning Baxter's arm controllers	
220
Baxter's arms and forward kinematics	
222
Joints and joint state publisher	
222
Understanding tf	
227
A program to move Baxter's arms to a zero angle position	
227
Rviz tf frames	
229
Viewing a tf tree of robot elements	
231
Introducing MoveIt	
231
Planning a move of Baxter's arms with MoveIt	
233
Adding objects to a scene	
234
Position of objects	
236
Planning a move to avoid obstacles with MoveIt	
236
Configuring a real Baxter setup	
238
Controlling a real Baxter	
241
Commanding joint position waypoints	
241
Commanding joint torque springs	
242
Demonstrating joint velocity	
243
Additional examples	
244
Visual servoing and grasping	
244
Inverse kinematics	
245
Summary	
249
Chapter 7: Making a Robot Fly	
251
Introducing quadrotors	
252
Why are quadrotors so popular?	
253
Defining roll, pitch, and yaw	
254
How do quadrotors fly?	
255
Components of a quadrotor	
258
Adding sensors	
258
Quadrotor communications	
259
Understanding quadrotor sensors	
259
Inertial measurement unit	
260
Quadrotor condition sensors	
260

Table of Contents
[ vii ]
Preparing to fly your quadrotor	
260
Testing your quadrotor	
261
Pre-flight checklist	
262
Precautions when flying your quadrotor	
262
Following the rules and regulations	
263
Using ROS with UAVs	
264
Introducing Hector Quadrotor	
264
Loading Hector Quadrotor	
266
Launching Hector Quadrotor in Gazebo	
267
Flying Hector outdoors	
267
Flying Hector indoors	
272
Introducing Crazyflie 2.0	
275
Controlling Crazyflie without ROS	
277
Communicating using Crazyradio PA	
278
Loading Crazyflie ROS software	
279
Setting up udev rules for Crazyradio	
281
Pre-flight check	
282
Flying Crazyflie with teleop	
283
Details of teleop_xbox360.launch	
285
Flying with a motion capture system	
288
Flying multiple Crazyflies	
288
Introducing Bebop	
289
Loading bebop_autonomy software	
290
Preparing to fly Bebop	
292
Testing Bebop communications	
292
Flying Bebop using commands	
292
Take off	
293
Landing	
294
Summary	
294
Chapter 8: Controlling Your Robots with External Devices 	
295
Creating a custom ROS game controller interface	
296
Testing a game controller	
297
Alternative test of a game controller	
299
Using the joy ROS package	
300
Controlling Turtlesim with a custom game controller interface	
300
Creating a custom ROS Android device interface	
307
Playing with Turtlebot Remocon	
307
Troubleshooting TurtleBot Remocon	
312

Table of Contents
[ viii ]
Custom control of an ROS robot using an Android device	
313
Installing Android Studio and Tools	
313
Installing an ROS-Android development environment	
314
Defining terms	
315
Introducing ROS-Android development	
316
Creating ROS nodes on Arduino or Raspberry Pi	
317
Using Arduino	
318
Installing the Arduino IDE software	
318
Installing the ROS Arduino software	
319
Ultrasonic sensor control using ROS and Arduino	
324
Other applications using ROS and Arduino	
332
Using Raspberry Pi	
333
Installing ROS on the Raspberry Pi	
333
Summary	
334
Chapter 9: Flying a Mission with Crazyflie	
335
Mission components	
337
Kinect for Windows v2	
337
Crazyflie operation	
338
Mission software structure	
338
OpenCV and ROS	
340
Loading software for the mission	
341
Installing libfreenect2	
342
Installing iai_kinect2	
345
Using the iai_kinect2 metapackage	
348
kinect2_bridge and kinect2_viewer	
348
kinect2_calibration	
349
Setting up the mission	
354
Detecting Crazyflie and a target	
355
Identifying markers in a color image	
355
Detecting and viewing markers with OpenCV	
358
Using Kinect and OpenCV	
360
How to track Crazyflie	
362
How to control Crazyflie	
365
Crazyflie control states	
365
Using ROS services to control takeoff and land	
366
Using PID control for hover and flight	
368
Using an observer node	
370

Table of Contents
[ ix ]
Flying Crazyflie	
371
Hovering in place	
371
What makes hover work?	
372
Flying to a stationary target	
374
What makes target detection work?	
375
Learned lessons	
376
Logging messages with rosout and rospy	
377
Summary	
378
Chapter 10: Extending Your ROS Abilities	
379
Controlling a robot with your voice	
379
Sphinx	
380
rospeex	
382
Enabling a robot to speak	
383
Enabling a robot to recognize faces	
384
Face detection with a cascade classifier	
384
Using ROS package face_detector	
386
Face recognition with OpenCV	
387
Using ROS package cob_people_detection	
387
Using ROS package face_recognition	
388
Summary	
389
Index	
391


[ xi ]
Preface
Learning ROS and working with ROS robots such as Baxter and TurtleBot is the 
beginning of a big adventure. The features and benefits of ROS are substantial but 
the learning curve is steep. Through trial and error, we have foraged a path through 
many of the ROS applications. In this book, we hope to present to you the best of our 
knowledge of ROS and provide you with detailed step-by-step instructions for your 
journey. Our approach centers on using the ROS robots that are featured—TurtleBot, 
Baxter, Crazyflie, and Bebop as well as simulated robots—Turtlesim and Hector.
This book provides introductory information as well as advanced applications 
featuring these ROS robots. The chapters begin with the basics of setting up 
your computer and loading ROS and the packages for ROS robots and tools. 
Straightforward instructions are provided with troubleshooting steps when the 
desired results are not achieved. The building blocks of ROS are described first in the 
simulation, Turtlesim, and then on each of the featured robots. Starting with basic 
ROS commands, the ROS packages, nodes, topics, and messages are explored to gain 
an overall knowledge of these ROS robotic systems. Technical information on these 
example robots is provided to describe the robot's full capabilities.
ROS encompasses a full spectrum of software concepts, implementation, and tools 
that attempt to provide a homogeneous view of the complex systems and software 
integration required in robotics. Extensive libraries of sensor and actuator drivers 
and interfaces are already in place as well as the latest and most efficient algorithms. 
What ROS doesn't provide directly is imported from other prevailing open source 
projects such as OpenCV. ROS also possesses a spectrum of time-saving tools 
to control, monitor, and debug robot applications—rqt, rviz, Gazebo, dynamic 
reconfigure, and MoveIt to name a few.

Preface
[ xii ]
In the pages that follow, each of these areas will be incrementally introduced to 
you as part of the robot examples. With TurtleBot, the subjects of navigation and 
mapping are explored. Using Baxter, joint control and path planning are described 
for your understanding. Simple Python scripts are included to provide examples of 
implementing ROS elements for many of these robots. These robots are all available 
in simulation to accomplish the exercises in this book. Furthermore, instructions are 
provided for you to build and control your own robot models in simulation.
The power of ROS, the variety of robots using ROS, and the diversity and support of 
the widespread ROS community make this adventure worthwhile. Extensive online 
tutorials, wiki instructions, forums, tips, and tricks are available for ROS. So dive into 
the pages of this book to begin your adventure with ROS robotics!
What this book covers
Chapter 1, Getting Started with ROS, explains the advantages of learning ROS and 
highlights the spectrum of robots currently using ROS. Instructions to install and 
launch ROS on a computer running a Ubuntu operating system are provided. An 
overview of the ROS architecture is given and its components are described. The 
Turtlesim simulation is introduced and used to provide a deeper understanding of 
how the components of ROS work and ROS commands.
Chapter 2, Creating Your First Two-Wheeled ROS Robot (in Simulation), introduces you 
to the ROS simulation environment of Gazebo. We will lead you through the steps to 
create your first robot simulation (a two-wheeled differential-drive base) and teach 
the structure of the Universal Robotic Description Format. The use of the ROS tools, 
rviz and Gazebo, are detailed to enable you to display your robot and interact 
with it.
Chapter 3, Driving Around with TurtleBot, introduces you to a real ROS robot, 
TurtleBot. This mobile base robot can be used in the simulation environment of 
Gazebo if you do not own one. ROS commands and Python scripts are used to 
control the TurtleBot through a variety of methods. The ROS tool, rqt, is introduced 
and subsets of its plugins are used to control TurtleBot and monitor its sensor data.
Chapter 4, Navigating the World with TurtleBot, explores visual sensors and the ability 
for a robot to map its environment. The 3D sensor options for TurtleBot's vision 
system are described and their setup and operation using ROS enables TurtleBot to 
navigate autonomously. The knowledge of Simultaneous Localization and Mapping 
techniques is applied in combination with TurtleBot's navigation stack to move 
about in the mapped environment.

Preface
[ xiii ]
Chapter 5, Creating Your First Robot Arm (in Simulation), provides a gentle introduction 
to the complexity of robotic arms. A simulated robot arm is designed and built using 
the macro language of Xacro. Controllers for the arm are created to operate the 
arm in Gazebo. Through developing the controllers for this arm, an insight into the 
mechanics and physics of a simple robot arm is offered.
Chapter 6, Wobbling Robot Arms Using Joint Control, takes a deeper look at the 
intricacies of controlling robotic arms. Our second ROS robot, the "state-of-the-art" 
Baxter robot, is introduced. Baxter has two 7 degree-of-freedom arms and a number 
of other sensors. Baxter Simulator is available as open source software to use for the 
instructions in this chapter. Examples are provided for the control of Baxter's arms 
using position, velocity, and torque modes with control for both forward and inverse 
kinematics. The ROS tool, MoveIt, is introduced for motion planning in simulation 
and execution on either a real or simulated Baxter.
Chapter 7, Making a Robot Fly, describes a small but growing area of ROS robotics—
unmanned air vehicles. This chapter focuses on quadrotors, and an understanding 
of quadrotor hardware and flight control is provided. Instructions to download 
and control the simulated quadrotor Hector are supplied. With skills from flying a 
simulated quadrotor, you can move on to control a real Bitcraze Crazyflie or Parrot 
Bebop. Quadrotor control is via teleoperation or ROS topic/message commands.
Chapter 8, Controlling Your Robots with External Devices, presents a number of 
peripheral devices you can use to control a ROS robot. Joystick controllers, controller 
boards (Arduino and Raspberry Pi), and mobile devices all have ROS interfaces that 
can be integrated with your robot to provide external control.
Chapter 9, Flying a Mission with Crazyflie, incorporates many of the ROS components 
and concepts presented in this book into a challenging mission of autonomous flight. 
The mission involves the Crazyflie quadrotor flying to a "remote" target all mapped 
through a Kinect 3D sensor. This mission uses ROS message communication and 
co-ordinate transforms to employ the Kinect's view of the quadrotor and target to 
orchestrate the flight. Flight control software for the Crazyflie using PID control is 
described and provided as part of the mission software.
Chapter 10, Extending Your ROS Abilities, provides a few last ROS capabilities to 
stimulate the further learning of advanced ROS robotic applications. The ability to 
control a robot with voice commands, the ability for a robot to detect and recognize 
faces as well as the ability for a robot to speak are all presented.

Preface
[ xiv ]
What you need for this book
The format of this book is intended for you to follow along and perform the 
instructions as the information is provided. You will need a computer with Ubuntu 
14.04 (Trusty Tahr) installed. Other Ubuntu versions and Linux distributions 
may work as well as Mac OS X, Android, and Windows but documentation for 
these versions will need to reference the ROS wiki (http://wiki.ros.org/
Distributions).
The version of ROS that this book was written around is Indigo Igloo, which is the 
current release recommended for stability. Its end of life is targeted for April 2019. 
Other versions of ROS may be used but are untested.
All the software used in this book is open source and freely available for download 
and use. Instructions to download the software are found in the chapter where the 
software is introduced. In Chapter 1, Getting Started with ROS, instructions are given 
to download and set up the ROS software environment.
Our preferred method to download software is the use of Debian packages. Where 
no Debian packages exist, we refer to downloading the software from repositories 
such as GitHub.
Gazebo simulation performs intensive graphics processing and the use of a dedicated 
graphics card is advised but not required.
Peripheral devices such as 3D sensors, Xbox, or PS3 controllers, Arduino or 
Raspberry Pi controller boards, and Android mobile devices are optional equipment.
Who this book is for
If you are a robotics developer, whether a hobbyist, researcher, or professional, and 
are interested in learning about ROS through a hands-on approach, then this book is 
for you. You are encouraged to have a working knowledge of GNU/Linux systems 
and Python.
Conventions
In this book, you will find a number of text styles that distinguish between different 
kinds of information. Here are some examples of these styles and an explanation of 
their meaning.
Code words in text, directory names, filenames, file extensions, and pathnames are 
shown as follows: "The terminal commands rostopic and rosnode have a number 
of options".

Preface
[ xv ]
A block of code is set as follows:
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  <link name="base_link">
    <visual>
      <origin xyz="0 0 0" rpy="0 0 0" />
      <geometry>
          <box size="0.5 0.5 0.25"/>
      </geometry>
    </visual>
  </link>
</robot>
To avoid repeating previous code blocks but provide placement of new code 
blocks, previous code left for reference is abbreviated and new code is highlighted 
as follows:
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  <link name="base_link">
  …
  </link>
  <!-- Right Wheel -->
  <link name="right_wheel">
Any command-line input is written as follows:
$ rosrun turtlesim turtlesim_node
Output from command is written as:
[ INFO] [1427212356.117628994]: Starting turtlesim with node name /turtlesim
New terms and important words are shown in bold.
Words that you see on the screen, for example, in menus or dialog boxes, appear in 
the text like this: "By clicking the Add button on the Displays panel."

Preface
[ xvi ]
URL references are shown as: http://www.ros.org/about-ros/
Warnings or important notes appear in a box like this.
Tips and tricks appear like this.
Reader feedback
Feedback from our readers is always welcome. Let us know what you think about 
this book—what you liked or disliked. Reader feedback is important for us as it helps 
us develop titles that you will really find useful and enjoyable.
To send us general feedback, simply e-mail feedback@packtpub.com, and mention 
the book's title in the subject of your message.
If there is a topic that you have expertise in and you are interested in either writing 
or contributing to a book, see our author guide at www.packtpub.com/authors.
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to 
help you to get the most from your purchase.
Downloading the example code
You can download the example code files for this book from your account at 
http://www.packtpub.com. If you purchased this book elsewhere, you can visit 
http://www.packtpub.com/support and register to have the files e-mailed directly 
to you.
You can download the code files by following these steps:
1.	 Log in or register to our website using your e-mail address and password.
2.	 Hover the mouse pointer on the SUPPORT tab at the top.
3.	 Click on Code Downloads & Errata.
4.	 Enter the name of the book in the Search box.

Preface
[ xvii ]
5.	 Select the book for which you're looking to download the code files.
6.	 Choose from the drop-down menu where you purchased this book from.
7.	 Click on Code Download.
Once the file is downloaded, please make sure that you unzip or extract the folder 
using the latest version of:
•	
WinRAR / 7-Zip for Windows
•	
Zipeg / iZip / UnRarX for Mac
•	
7-Zip / PeaZip for Linux
The code bundle for the book is also hosted on GitHub at https://github.com/
PacktPublishing/ROS-Robotics-By-Example. We also have other code bundles 
from our rich catalog of books and videos available at https://github.com/
PacktPublishing/. Check them out!
Downloading the color images of this book
We also provide you with a PDF file that has color images of the screenshots/
diagrams used in this book. The color images will help you better understand the 
changes in the output. You can download this file from http://www.packtpub.com/
sites/default/files/downloads/ROSRoboticsByExample_ColorImages.pdf.
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes 
do happen. If you find a mistake in one of our books—maybe a mistake in the text or 
the code—we would be grateful if you could report this to us. By doing so, you can 
save other readers from frustration and help us improve subsequent versions of this 
book. If you find any errata, please report them by visiting http://www.packtpub.
com/submit-errata, selecting your book, clicking on the Errata Submission Form 
link, and entering the details of your errata. Once your errata are verified, your 
submission will be accepted and the errata will be uploaded to our website or 
added to any list of existing errata under the Errata section of that title.
To view the previously submitted errata, go to https://www.packtpub.com/books/
content/support and enter the name of the book in the search field. The required 
information will appear under the Errata section.

Preface
[ xviii ]
Piracy
Piracy of copyrighted material on the Internet is an ongoing problem across all 
media. At Packt, we take the protection of our copyright and licenses very seriously. 
If you come across any illegal copies of our works in any form on the Internet, please 
provide us with the location address or website name immediately so that we can 
pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected 
pirated material.
We appreciate your help in protecting our authors and our ability to bring you 
valuable content.
Questions
If you have a problem with any aspect of this book, you can contact us at 
questions@packtpub.com, and we will do our best to address the problem.

[ 1 ]
Getting Started with ROS
In this chapter, we will introduce the Robot Operating System (ROS), which is 
a collection of software packages to aid researchers and developers using robotic 
systems. After we discuss the instructions to install ROS on your computer 
system using the Ubuntu operating system, the ROS architecture and many of its 
components are discussed. This will aid you in understanding the use of ROS to 
develop software for robotic applications.
ROS will be introduced in terms of its elements and their functions. 
An understanding of the ROS vocabulary is necessary to become proficient 
in using ROS to create programs for the control of real or simulated robots 
as well as devices, such as cameras.
To make the discussion more concrete, the turtlesim simulator will be presented 
with various examples of the ROS command usage. This simulator is part of ROS 
and it provides an excellent introduction to the capabilities of ROS.
In this chapter, we will cover the following topics:
•	
What ROS is and which robots use ROS
•	
How to install and launch ROS on your computer
•	
How to navigate the ROS directories
•	
An introduction to ROS packages, nodes, and topics
•	
Examples of useful ROS commands
•	
How to use ROS commands with the turtlesim simulator

Getting Started with ROS
[ 2 ]
What does ROS do and what are the 
benefits of learning ROS?
ROS is sometimes called a meta operating system because it performs many 
functions of an operating system, but it requires a computer's operating system such 
as Linux. One of its main purposes is to provide communication between the user, 
the computer's operating system, and equipment external to the computer. This 
equipment can include sensors, cameras, as well as robots. As with any operating 
system, the benefit of ROS is the hardware abstraction and its ability to control a 
robot without the user having to know all of the details of the robot.
For example, to move a robot's arms, a ROS command is issued, or scripts in Python 
or C++ written by the robot designers cause the robot to respond as commanded. 
The scripts can, in turn, call various control programs that cause the actual motion 
of the robot's arms. It is also possible to design and simulate your own robot using 
ROS. These subjects and many others will be considered in this book.
In this book, you will learn a set of concepts, software, and tools that apply to an 
ever-increasing and diverse army of robots. For example, the navigation software of 
one mobile robot can be used, with a few changes, to work in another mobile robot. 
The flight navigation of an aerial robot is similar to that of the ground robot and so 
on. All across the broad spectrum of robotics, system interfaces are standardized or 
upgraded to support increased complexity. There are readily available libraries for 
commonly used robotics functions. ROS not only applies to the central processing 
of robotics but also to sensors and other subsystems. ROS hardware abstraction 
combined with low-level device control speeds the upgrade toward the 
latest technology.
ROS is an open source robotic software system that can be used without 
licensing fees by universities, government agencies, and commercial companies. 
The advantages of open source software is that the source code for the system 
is available and can be modified according to a user's needs. More importantly 
for some users, the software can be used in a commercial product as long as the 
appropriate licenses are cited. The software can be improved and modules can be 
added by users and companies.
ROS is used by many thousands of users worldwide and knowledge can be shared 
between users. The users range from hobbyists to professional developers of 
commercial robots. In addition to the large group of ROS researchers, there 
is a ROS-Industrial group dedicated to applying ROS software to robots 
for manufacturing.

Chapter 1
[ 3 ]
Who controls ROS?
A ROS distribution is a set of ROS software packages that can be downloaded 
to your computer. These packages are supported by the Open Source Robotics 
Foundation (OSRF), a nonprofit organization. The distributions are updated 
periodically and given different names by the ROS organization. More details 
about the ROS organization are available at:
http://www.ros.org/about-ros/
This book is written using Ubuntu 14.04 as the operating system and ROS Indigo 
as the version of the ROS distribution. Always make sure that you check for any 
updates for the Ubuntu or ROS versions you are using.
Which robots are using ROS?
There is a long list of robots on the ROS wiki website http://wiki.ros.org/
Robots, which use ROS. For example, we are using four different robots in this 
book to provide you with an experience of a wide range of ROS capabilities. 
These robots are as follows:
•	
TurtleBot, a mobile robot
•	
Baxter, a friendly two-armed robot
•	
Crazyflie and Bebop, flying robots
The images of these robots are in the following figures:
TurtleBot 2

Getting Started with ROS
[ 4 ]
Baxter in the authors' laboratory
Of course, not everyone has the opportunity to use real robots such as Baxter. 
However, there is good news! Using the ROS Gazebo software, you can simulate 
Baxter as well as many other robots whose models are provided for Gazebo. We will 
simulate TurtleBot using Gazebo and actually design our own mobile robot in the 
upcoming chapters of this book.
Bebop and Crazyflie

Chapter 1
[ 5 ]
Installing and launching ROS
For this book, we assume the reader has a computer with Ubuntu Saucy 13.10 or 
Trusty 14.04 installed. The examples in this book have been developed using ROS 
Indigo and this version of ROS is only supported by these two versions of Ubuntu. 
The instructions for ROS installation provided in this section are for installing 
Debian (binary) packages. This method is the most efficient and preferred way to 
install ROS.
If you wish to install the ROS Indigo source code and build the software, refer 
to instructions at http://wiki.ros.org/indigo/Installation/Source. The 
instructions presented here to install ROS Indigo with Debian packages can also 
be found at http://wiki.ros.org/indigo/Installation/Ubuntu.
If you have any problems installing ROS, refer to this site and the ROS forum at 
http://answers.ros.org.
Configuring your Ubuntu repositories
To begin, configure the Ubuntu repositories to allow restricted, universe, and 
multiverse. Click on the Ubuntu Software Center icon in the launch menu on the 
left side of your desktop. From the Software Center's top menu bar, navigate to 
Edit, then to the drop-down menu, and select Software Sources. On the Software & 
Updates screen, select checkboxes to match the following screenshot:
Ubuntu Software Center Screen

Getting Started with ROS
[ 6 ]
Setting up your sources.list
Open a terminal window to set up the sources.list file on your computer to accept 
software from the ROS software repository at http://packages.ros.org which is 
the authorized site for the ROS software.
At the $ command prompt, type the following command as one long command:
$ sudo sh -c 'echo "deb http://packages.ros.org/ros/ubuntu
$(lsb_release -sc) main" > /etc/apt/sources.list.d/ros-latest.list'
This step allows the operating system to know where to download programs that 
need to be installed on your system. When updates are made to ROS Indigo, your 
operating system will be made aware of these updates.
Setting up your keys
Keys confirm the origin of the code and verify that unauthorized modifications to the 
code have not been made without the knowledge of the owner. A repository and the 
keys of that repository are added the operating system's trusted software list. Type 
the following command:
$ sudo apt-key adv --keyserver hkp://pool.sks-keyservers.net:80 --
recv-key 0xB01FA116
Installing ROS Indigo
Before you begin with the installation, the current system software must be up to 
date to avoid problems with libraries and wrong versions of software. To make sure 
your Debian package index is up-to-date, type the following command:
$ sudo apt-get update

Chapter 1
[ 7 ]
Warning: If you are using Ubuntu Trusty14.04.2 and experience 
dependency issues during the ROS installation, you may have to install 
some additional system dependencies.
 Do not install these packages if you are using 14.04, it will destroy your 
X server:
$ sudo apt-get install xserver-xorg-dev-lts-utopic mesa-
common-dev-lts-utopic libxatracker-dev-lts-utopic 
libopenvg1-mesa-dev-lts-utopic libgles2-mesa-dev-lts-
utopic libgles1-mesa-dev-lts-utopic libgl1-mesa-dev-lts-
utopic libgbm-dev-lts-utopic libegl1-mesa-dev-lts-utopic
Alternatively, try installing just this to fix dependency issues:
$ sudo apt-get install libgl1-mesa-dev-lts-utopic
For more information on this issue, refer to the following sites:
http://answers.ros.org/question/203610/ubuntu-14042-
unmet-dependencies/
https://bugs.launchpad.net/ubuntu/+source/mesa-lts-
utopic/+bug/1424059
Install the desktop-full configuration of ROS. Desktop-full includes ROS, rqt, 
rviz, robot-generic libraries, 2D/3D simulators, navigation, and 2D/3D perception. 
In this book, we will be using rqt and rviz for visualization and also the Gazebo 3D 
simulator, as well as the ROS navigation and perception packages. To install, type 
the following command:
$ sudo apt-get install ros-indigo-desktop-full
ROS Indigo is installed on your computer system when the installation is complete!
Initialize rosdep
The ROS system may depend on software packages that are not loaded initially. 
These software packages external to ROS are provided by the operating system. The 
ROS environment command rosdep is used to download and install these external 
packages. Type the following command:
$ sudo rosdep init
$ rosdep update

Getting Started with ROS
[ 8 ]
Environment setup
Your terminal session must now be made aware of these ROS files so that it knows 
what to do when you attempt to execute ROS command-line commands. Running 
this script will set up the ROS environment variables:
$ source /opt/ros/indigo/setup.bash
Alternately, it is convenient if the ROS environment variables are automatically 
added to your terminal session every time a new shell is launched. If you are using 
bash for your terminal shell, do this by typing the following command:
$ echo "source /opt/ros/indigo/setup.bash" >> ~/.bashrc
$ source ~/.bashrc
Now when a new terminal session is launched, the bash shell is automatically aware 
of the ROS environment variables.
Getting rosinstall
The rosinstall command is a command-line tool in ROS that allows you to 
download ROS packages with one command.
To install this tool on Ubuntu, type the following command:
$ sudo apt-get install python-rosinstall
Troubleshooting – examining your ROS 
environment
The ROS environment is set up through a number of variables that tell the 
system where to find ROS packages. Two main variables are ROS_ROOT and 
ROS_PACKAGE_PATH that enable ROS to locate packages in the filesystem.
To check whether the ROS environment variables are set correctly, use the export 
command in the following form that lists the ROS environment variables:
$ export | grep ROS

Chapter 1
[ 9 ]
The output of the preceding command is as follows:
declare -x ROSLISP_PACKAGE_DIRECTORIES=""
declare -x ROS_DISTRO="indigo"
declare -x ROS_ETC_DIR="/opt/ros/indigo/etc/ros"
declare -x ROS_MASTER_URI="http://localhost:11311"
declare -x ROS_PACKAGE_PATH="/opt/ros/indigo/share:/opt/ros/indigo/stacks"
declare -x ROS_ROOT="/opt/ros/indigo/share/ros"
If the variables are not set correctly, you will need to source your setup.bash file 
as described in the Environment setup section of this chapter. Check whether 
the ROS_DISTRO= "indigo" and the ROS_PACKAGE_PATH variables are correct, 
as shown previously.
The tutorial that discusses the ROS environment can be found at http://wiki.ros.
org/ROS/Tutorials/InstallingandConfiguringROSEnvironment.
Creating a catkin workspace
The next step is to create a catkin workspace. A catkin workspace is a directory 
(folder) in which you can create or modify existing catkin packages. The catkin 
structure simplifies the build and installation process for your ROS packages. The 
ROS wiki website is http://wiki.ros.org/catkin/Tutorials/create_a_
workspace.
A catkin workspace can contain up to three or more different subdirectories 
(/build, /devel, and /src) which each serve a different role in the software 
development process.
We will label our catkin workspace catkin_ws. To create the catkin workspace, type 
the following commands:
$ mkdir –p ~/catkin_ws/src
$ cd ~/catkin_ws/src
$ catkin_init_workspace

Getting Started with ROS
[ 10 ]
Even though the workspace is empty (there are no packages in the src folder, just 
a single CMakeLists.txt link), you can still build the workspace by typing the 
following command:
$ cd ~/catkin_ws/
$ catkin_make
The catkin_make command creates the catkin workspaces. If you view your current 
directory contents, you should now have the build and devel folders. Inside the 
devel folder there are now several setup.*sh files. We will source the setup.bash 
file to overlay this workspace on top of your ROS environment:
$ source ~/catkin_ws/devel/setup.bash
Remember to add this source command to your .bashrc file by typing the 
following command:
$ echo "source ~/catkin_ws/devel/setup.bash" >> ~/.bashrc
To make sure your workspace is properly overlaid by the setup script, make sure the 
ROS_PACKAGE_PATH environment variable includes the directory you're in by typing  
the following command:
$ echo $ROS_PACKAGE_PATH
The output of the preceding command should be as follows:
    /home/<username>/catkin_ws/src:/opt/ros/indigo/share:/opt/ros/indigo/stacks
Here, <username> is the name you chose as user when Ubuntu was installed.
ROS packages and manifest
The ROS software is divided into packages that can contain various types of 
programs, images, data, and even tutorials. The specific contents depend on the 
application for the package. The site http://wiki.ros.org/Packages discusses 
ROS packages.
A package can contain programs written in Python or C++ to control a robot or 
another device. For the turtlesim simulator package for example, the package 
contains the executable code used to change the background color or move a turtle 
around on the screen. This package also contains images of a turtle for display and 
files used to create the simulator.

Chapter 1
[ 11 ]
There is another class of packages in ROS called metapackages that are specialized 
packages that only contain a package.xml manifest. Their purpose is to reference 
one or more related packages, which are loosely grouped together.
ROS manifest
Each package contains a manifest named package.xml that describes the package 
in the Extensible Markup Language (XML) format. In addition to providing a 
minimal specification describing the package, the manifest defines properties about 
the package such as the package name, version numbers, authors, maintainers, and 
dependencies on other packages.
Exploring the ROS packages
Occasionally, we would like to find packages that we wish to use and display the 
files involved. This section introduces several useful ROS commands:
•	
rospack used for information about a package
•	
roscd used to navigate the ROS directories
•	
rosls used to list directories and files in a package directory
The rospack command can be used to list ROS packages, locate packages by name, 
and determine if a package depends on another package, among other uses. For 
more information use the following command with the help or -h option in 
the form:
$ rospack help | less
There are many options for this command, so piping with the less 
option shows one screen at a time. For many commands, using the power 
of Linux to make the output readable is necessary. Use the Q key to quit 
(exit) the mode.
We will use the turtlesim package for the examples here. To change directories to 
the location of turtlesim, use the following command:
$ roscd turtlesim
This yields the location to one of the author's laptops as follows:
harman@Laptop-M1210:/opt/ros/indigo/share/turtlesim$

Getting Started with ROS
[ 12 ]
On your computer, the $ command prompt will be preceded by the information 
about your computer. Generally, that information for our computers will be deleted 
in our examples using ROS commands. Once you are in the turtlesim directory, 
the standard Linux commands can be used with the subdirectories or files, or 
the ROS commands can be used. To determine the directories and files in the 
turtlesim directory but without changing to the turtlesim directory, use the 
following command:
$ rosls turtlesim
Here is the result from the home directory of the author's laptop with ROS installed:
cmake  images  msg  package.xml  srv
To see the filenames of the images loaded with turtlesim, specify the images 
directory in the package as follows:
$ rosls turtlesim/images
The output of the preceding command is as follows:
box-turtle.png            fuerte.png       hydro.svg       palette.png          turtle.png
diamondback.png     groovy.png     indigo.png     robot-turtle.png
electric.png                 hydro.png       indigo.svg      sea-turtle.png
There are various turtle images that can be used. The rosls turtlesim command 
will also work to show the contents of the turtlesim subdirectories: /msg for 
messages and /srv for services. These files will be discussed later. To see the 
manifest, type the following command:
$ roscd turtlesim
$ cat package.xml
This will also show the dependencies such as roscpp for C++ programs.
rospack find packages
The rospack find <package name> command returns the path to the package 
named <package name>. For example, type the following command:
$ rospack find turtlesim
The preceding command displays the path to the turtlesim directory.

Chapter 1
[ 13 ]
rospack list
Execute the following command:
$ rospack list
This lists the ROS package names and their directories on the computer. In the case 
of the laptop mentioned earlier, there are 225 ROS packages listed!
If you really want to see all the ROS packages and their locations, use the 
following command form:
$ rospack list | less
This form allows paging of the long list of names and directories for the 
packages. Press Q to quit.
Alternatively, this is the form of the rospack command:
$ rospack list-names
This lists only the names of the packages without the directories. After 
such a long list, it is a good idea to open a new terminal window or clear 
the window with the clear command.
This is the form of the rospack command:
$ rospack list-names | grep turtle
This lists the four packages with turtle in the name.
More information on commands that are useful to navigate the ROS filesystem 
is available at the ROS website http://wiki.ros.org/ROS/Tutorials/
NavigatingTheFilesystem.
ROS nodes and ROS Master
One of the primary purposes of ROS is to facilitate communication between the ROS 
modules called nodes. These nodes represent the executable code and the code can 
reside entirely on one computer, or nodes can be distributed between computers or 
between computers and robots. The advantage of this distributed structure is that 
each node can control one aspect of a system. One node can capture and output 
camera images, and another node can control a robot's manipulator in response to 
the camera view. We will see that the turtlesim simulator has two nodes that are 
created when turtlesim is executed.

Getting Started with ROS
[ 14 ]
The ROS site http://www.ros.org/core-components/ describes the 
communication and robot-specific features of ROS. Here, we will explore some of the 
main components of a ROS system including ROS nodes and the ROS Master. It is 
important for you to understand the ROS nodes and Master, so they will be defined 
later through discussions and examples.
ROS nodes
Basically, nodes are processes that perform some action. The nodes themselves are 
really software modules but with the capability to register with the ROS Master 
node and communicate with other nodes in the system. The ROS design idea is that 
each node is an independent module that interacts with other nodes using the ROS 
communication capability.
The nodes can be created in various ways. From a terminal window a node can be 
created directly by typing a command after the command prompt, as shown in the 
examples to follow. Alternatively, nodes can be created as part of a program written 
in Python or C++. In this book, we will use both the ROS commands in a terminal 
window and Python programs to create nodes.
Nodes can publish and nodes can subscribe
One of the strengths of ROS is that a particular task, such as controlling a wheeled 
mobile robot, can be separated into a series of simpler tasks. The tasks can include 
the perception of the environment using a camera or laser scanner, map making, 
planning a route, monitoring the battery level of the robot's battery, and controlling 
the motors driving the wheels of the robot. Each of these actions might consist of a 
ROS node or series of nodes to accomplish the specific tasks.
A node can independently execute code to perform its task but can communicate 
with other nodes by sending or receiving messages. The messages can consist of 
data, or commands, or other information necessary for the application.
Some nodes provide information for other nodes, as a camera feed would do, for 
example. Such a node is said to publish information that can be received by other 
nodes. The information in ROS is called a topic, which will be explained later. 
Continuing with the camera example, the camera node can publish the image 
on the camera/image_raw topic.
Image data from the camera/image_raw topic can be used by a node that shows 
the image on the computer screen. The node that receives the information is said to 
subscribe to the topic being published, in this case camera/image_raw.

Chapter 1
[ 15 ]
In some cases, a node can both publish and subscribe to one or more topics. 
A number of examples later in the book will demonstrate this functionality.
ROS Master
The ROS nodes are typically small and independent programs that can run 
concurrently on several systems. Communication is established between the nodes 
by the ROS Master. The ROS Master provides naming and registration services 
to the nodes in the ROS system. It tracks publishers and subscribers to the topics. 
The role of the Master is to enable individual ROS nodes to locate one another. 
The most often used protocol for connection is the standard Transmission Control 
Protocol/Internet Protocol (TCP/IP) or Internet Protocol called TCPROS in ROS. 
Once these nodes are able to locate one another, they can communicate with each 
other peer-to-peer.
One responsibility of the Master is to keep track of nodes when new nodes are 
executed and come into the system. Thus, the Master provides a dynamic allocation 
of connections. The nodes cannot communicate however, until the Master notifies the 
nodes of each others' existence. A simple example is shown at: http://wiki.ros.
org/Master.
Invoking the ROS Master using roscore
roscore is a collection of nodes and programs that you must have running for ROS 
nodes to communicate. After it is launched, roscore will start the following:
•	
A ROS Master
•	
A ROS Parameter Server
•	
A rosout logging node
The roscore command starts ROS and creates the Master so that nodes can register 
with the Master. You can view the ROS tutorial for roscore at http://wiki.ros.
org/roscore.
Issue the following command to start the Master in a new terminal window and 
observe the output:
$ roscore

Getting Started with ROS
[ 16 ]
The output of the preceding command is as follows:
... logging to /home/harman/.ros/log/94248b4a-3f05-11e5-b5ce-
00197d37ddd2/roslaunch-Laptop-M1210-2322.log
Checking log directory for disk usage. This may take awhile.
Press Ctrl-C to interrupt
Done checking log file disk usage. Usage is <1GB.
started roslaunch server http://Laptop-M1210:46614/
ros_comm version 1.11.13
SUMMARY
========
PARAMETERS
 * /rosdistro: indigo
 * /rosversion: 1.11.13
NODES
auto-starting new master
process[master]: started with pid [2334]
ROS_MASTER_URI=http://Laptop-M1210:11311/
setting /run_id to 94248b4a-3f05-11e5-b5ce-00197d37ddd2
process[rosout-1]: started with pid [2347]
started core service [/rosout]
In the preceding screen output, you will see information about the computer, 
parameters that list the name (indigo) and version number of the ROS distribution, 
and other information. The Master is defined by its Uniform Resource Identifier 
(URI). This identifies the location of the Master; in this case, it is running on the 
laptop and is used to execute the roscore command.

Chapter 1
[ 17 ]
Parameter Server
The Parameter Server is a shared dictionary of parameters that nodes store and 
retrieve at runtime. The Parameter Server runs inside the Master and parameters 
are globally viewable so that nodes can access the parameters.
In the preceding screen output from the roscore command, the parameters 
associated with the Master are as follows:
* /rosdistro: indigo
* /rosversion: 1.11.13
Indigo is the ROS distribution release that we are using. As Indigo is changed or 
packages are added, numbered versions such as 1.11.13 are released. Issuing 
the roscore command is a way to determine the release of ROS running on 
your computer.
Whenever ROS is executing, it is possible to list the nodes that are active and the 
topics that are used for communication. We will explore the information in the 
roscore output in more detail by invoking useful ROS terminal commands.
ROS commands to determine the nodes and 
topics
Three commands used extensively in ROS are as follows:
•	
roscore to start the Master and allow nodes to communicate
•	
rosnode list to list the active nodes
•	
rostopic list to list the topics associated with ROS nodes
After the roscore command is executed, the terminal window used to execute 
roscore must remain active, but it can be minimized. In another terminal window, 
the rosnode list command will cause a list of the ROS nodes that are active to be 
displayed on the screen. After the command for roscore is executed, only one node 
rosout will be listed as an active node if you type the following command:
$ rosnode list
The output of the preceding command is as follows:
/rosout

Getting Started with ROS
[ 18 ]
In the second terminal window,  list the active topics by typing:
$ rostopic list
The output of the preceding command is as follows:
/rosout
/rosout_agg
Notice that the /rosout node and the /rosout topic have the same designation. In 
ROS terms, the rosout node subscribes to the /rosout topic. All the active nodes 
publish their debug messages to the /rosout topic. We will not be concerned with 
these messages here; however, they can be useful to debug a program. For some 
explanation, refer to the ROS wiki at http://wiki.ros.org/rosout.
The rosout node is connected to every other active node in the system. The 
/rosout_agg topic receives messages also, but just from the rosout node so it does 
not have to connect to all of the nodes and thus saves time at system startup.
The rostopic and rosnode terminal commands have a number of options that will 
be demonstrated by various examples in this book.
Most of the ROS commands have help screens that are usually helpful. 
Type the following command for the command options:
$ rosnode -h
For more detailed usage, use the subcommand name, for example:
$ rosnode list –h
will list the subcommands and the options for the rosnode command.
There are a number of other important ROS terminal commands that you should 
know. They are introduced and explained using the turtlesim simulator in the 
upcoming section.
Turtlesim, the first ROS robot simulation
A simple way to learn the basics of ROS is to use the turtlesim simulator that is part 
of the ROS installation. The simulation consists of a graphical window that shows 
a turtle-shaped robot. The background color for the turtle's world can be changed 
using the Parameter Server. The turtle can be moved around on the screen by ROS 
commands or using the keyboard.

Chapter 1
[ 19 ]
Turtlesim is a ROS package, and the basic concepts of package management were 
presented in the Exploring the ROS packages section, as discussed earlier. We suggest 
that you refer to this section before continuing.
We will illustrate a number of ROS commands that explore the nodes, topics, 
messages, and services used by the turtle simulator. We have already covered the 
roscore, rosnode, and rostopic commands. These commands will be used with 
turtlesim also.
Other important ROS terminal commands that will be covered in this section are 
as follows:
•	
rosrun that finds and starts a requested node in a package
•	
rosmsg that shows information about messages
•	
rosservice that displays runtime information about nodes and can pass 
data between nodes in a request/response mode
•	
rosparam that is used to get and set parameters (data) used by nodes
Starting turtlesim nodes
To start turtlesim with ROS commands, we need to open two separate terminal 
windows. First, issue the following command in the first window if the Master 
is not already running:
$ roscore
Wait for the Master to complete startup. You can minimize this window but do not 
close it because the Master must run to allow the nodes to communicate.
The result on your screen will resemble the output discussed previously in the 
Invoking the ROS Master using roscore section, where roscore was described.
rosrun command
To display the turtle on the screen, use the rosrun command. It takes the arguments 
[package name] [executable name] , and in this case, turtlesim as the package 
and turtlesim_node as the executable program.
In the second terminal window, issue the following command:
$ rosrun turtlesim turtlesim_node

Getting Started with ROS
[ 20 ]
You will see an output similar to this:
[ INFO] [1427212356.117628994]: Starting turtlesim with node name 
/turtlesim
[ INFO] [1427212356.121407419]: Spawning turtle [turtle1] at 
x=[5.544445], y=[5.544445], theta=[0.000000]
Wait for the display screen to appear with the image of a turtle at the center, as 
shown in the turtlesim screen in the following screenshot. The terminal window can 
be minimized, but keep the turtle display screen in view. The turtle is called turtle1 
since this is the first and only turtle in our display.
After you have started turtlesim by executing the rosrun command, you will see  
information about the turtle's position on the screen. The /turtlesim node creates 
the screen image and the turtle. Here, the turtle is in the center at about x = 5.5, y 
= 5.5 with no rotation since angle theta is zero. The origin (0, 0) is at the lower-left 
corner of the screen:
Turtlesim screen
Let's study the properties of the nodes, topics, services, and messages available with 
the turtlesim package in another terminal window. Thus, at this point, you will 
have three windows active but the first two can be minimized or dragged off to the 
side or the bottom. They should not be closed.

Chapter 1
[ 21 ]
Turtlesim nodes
In the third window, issue the rosnode command to determine information about 
any node. First, list the active nodes, using the following command:
$ rosnode list
The output is as follows:
/rosout
/turtlesim
We will concentrate on the /turtlesim node. Note the difference in notation 
between the /turtlesim node and the turtlesim package.
To see the publications, subscriptions, and services of the turtlesim node, type the 
following command:
$ rosnode info /turtlesim
The output of the preceding command is as follows:
---------------------------------------------------------------------
Node [/turtlesim]
Publications:
 * /turtle1/color_sensor [turtlesim/Color]
 * /rosout [rosgraph_msgs/Log]
 * /turtle1/pose [turtlesim/Pose]
Subscriptions:
 * /turtle1/cmd_vel [unknown type]
Services:
 * /turtle1/teleport_absolute
 * /turtlesim/get_loggers
 * /turtlesim/set_logger_level
 * /reset
 * /spawn

Getting Started with ROS
[ 22 ]
 * /clear
 * /turtle1/set_pen
 * /turtle1/teleport_relative
 * /kill
contacting node http://Laptop-M1210:38993/ ...
Pid: 2364
Connections:
 * topic: /rosout
    * to: /rosout
    * direction: outbound
    * transport: TCPROS
The following diagram represents a graphical illustration of the relationship of the 
turtlesim node in elliptical shapes and topics in rectangular boxes:
/turtlesim node
The graph was created using the rqt_graph command as described in the 
Introducing RQT tools section in Chapter 3, Driving Around with TurtleBot.

Chapter 1
[ 23 ]
We read the output of the rosnode info command and the graph of the turtlesim 
node and topics in the preceding diagram as follows (ignoring the logging services 
and the /rosout and /rosout_agg nodes):
•	
The /turtlesim node publishes to two topics. These topics control the 
color of the turtle's screen and the position of the turtle on the screen when 
messages are sent from /turtlesim:
°°
/turtle1/color_sensor with the message type [turtlesim/
Color]
°°
/turtle1/pose with the message type [turtlesim/Pose]
•	
The /turtlesim node subscribes to the turtle1/cmd_vel topic of an 
unknown type. The type is unknown because another node that publishes 
on this topic has not yet been executed.
•	
There are a number of services associated with the /turtlesim node. The 
services can be used to move the turtle around (teleport), clear the screen, kill 
the nodes, and perform other functions. The services will be explained later 
in this section.
Turtlesim topics and messages
A ROS message is a strictly typed data structure. In fact, the message type is not 
associated with the message contents. For example, one message type is string, 
which is just text. Another type of message is uint8, which means that it is an 
unsigned 8-bit integer. These are part of the std_msg package or standard messages. 
The command form rosmsg list lists the type of messages on your system; it is a 
long list! There are packages for messages of the type control, type geometry, and 
type navigation, among others to control robot actions. There are sensor message 
types used with laser scanners, cameras, and joysticks to name just a few of the 
sensors or input devices possible with ROS. Turtlesim uses several of the message 
types to control its simulated robot—the turtle.
For these exercises, keep the roscore and /turtlesim windows active. Open other 
terminal windows as needed. We will concentrate on the turtle1/color_sensor 
topic first. You will be typing in the third window.
If the screen gets too cluttered, remember the $ clear command.
Use the $ history command to see the commands you have used.

Getting Started with ROS
[ 24 ]
rostopic list
For the /turtlesim node, the topics are listed using the following command:
$ rostopic list
The output of the preceding command is as follows:
/rosout
/rosout_agg
/turtle1/cmd_vel
/turtle1/color_sensor
/turtle1/pose
rostopic type
The topic type can be determined for each topic by typing the following command:
$ rostopic type /turtle1/color_sensor
turtlesim/Color
The message type in the case of the /turtle1/color_sensor topic is 
turtlesim/Color. This is the format of ROS message type names:
[package name]/[message type]
If a node publishes a message, we can determine the message type and read 
the message.
rosmsg list
The turtlesim package has two message types that are found with the following 
command:
$ rosmsg list | grep turtlesim
The output is as follows:
turtlesim/Color
turtlesim/Pose

Chapter 1
[ 25 ]
The rosmsg list command displays all of the messages. Adding | grep 
turtlesim shows all messages of the turtlesim package. There are only two in the 
turtlesim package. From the message type, we can find the format of the message. 
Make sure that you note that Color in the message type starts with a capital letter.
rosmsg show
The rosmsg show <message type> command displays the fields in a ROS message 
type. For the turtlesim/Color type, the fields are integers:
$ rosmsg show turtlesim/Color
Output of the preceding command is as follows:
uint8 r
uint8 g
uint8 b
The format of values designating colors red (r), green (g), and blue (b) is unsigned 
8-bit integer.
To understand the message, it is necessary to find the message type. For example, 
turtlesim/Color is a message type for turtlesim that has three elements that define 
the color of the background. For example, the red color in the background is defined 
by uint8 r. This indicates that if we wish to modify the red value, an 8-bit unsigned 
integer is needed. The amount of red in the background is in the range of 0-255.
In general, the formats of numerical data include integers of 8, 16, 32, or 64 bits, 
floating point numbers, and other formats.
rostopic echo
To determine the color mixture of red, green, and blue in the background of our 
turtle, use the rostopic echo [topic name] command in the form, as follows:
$ rostopic echo /turtle1/color_sensor
Output of the preceding command is as follows:
r: 69
g: 86
b: 255
---

Getting Started with ROS
[ 26 ]
Press Ctrl + C to stop the output.
The website describing the RGB Color Codes Chart and the meaning of the 
numerical color values can be found at http://www.rapidtables.com/web/color/
RGB_Color.htm.
The chart explains how to mix the red, green, and blue (rgb) color values to achieve 
any desired color. An example, which is discussed later, will show you how to 
change the background color for the turtle. The color values are parameters that can 
be changed.
A simple table can clarify the relationship between the topics and the messages for 
the /turtlesim node:
Topics and Messages for /turtlesim node
Topics name
Topic type
Message format
Message
$ rostopic 
list
$ rostopic type
[topic name]
$ rosmsg show
[topic type]
$ rostopic echo
[topic name]
/turtle1
/color_sensor
turtlesim/Color
uint8 r
uint8 g
uint8 b
r: 69
g: 86
b: 255
/turtle1/pose
turtlesim/Pose
float32 x
float32 y
float32 theta
float32 
linear_velocity
float32 
angular_velocity
x: 5.54444456
y: 5.54444456
theta: 0.0
linear_velocity: 
0.0
angular_velocity: 
0.0
The table of Topics and Messages shows the topics, types, message formats, and 
data values for the two topics of the /turtlesim node that we explored. The 
commands to determine the information are also shown in the table.
Parameter Server of turtlesim
The Parameter Server maintains a dictionary of the parameters that are used to 
configure the screen of turtlesim. We will see that these parameters are used to 
define the color of the background for the turtle. Thus, the turtlesim node can read 
and write parameters held by the Parameter Server.

Chapter 1
[ 27 ]
rosparam help
Use the help option to determine the form of the rosparam command:
$ rosparam help
Output of the preceding command is as follows:
rosparam is a command-line tool for getting, setting, and deleting 
parameters from the ROS Parameter Server.   Commands:
  rosparam set       set parameter
  rosparam get       get parameter
  rosparam list      list parameter names
<Edited>
rosparam list for /turtlesim node
To list the parameters for the /turtlesim node, we will use the following command:
$ rosparam list
Output of the preceding code is as follows:
/background_r
/background_g
/background_b
/rosdistro
/roslaunch/uris/host_d125_43873__51759
/rosversion
/run_id
Note that the last four parameters were created by invoking the Master with the 
roscore command, as discussed previously. Also, the list defines the characteristics 
of the parameter but not the data value.

Getting Started with ROS
[ 28 ]
Change parameters for the color of the turtle's 
background
To change the color parameters for turtlesim, let's change the turtle's background 
to red. To do this, make the blue and green data values equal to zero and saturate 
red = 255 using the rosparam set command. Note that the clear option from 
rosservice must be executed before the screen changes color.
rosparam get
The default turtle screen is blue. You can use rosparam get / to show the data 
contents of the entire Parameter Server:
$ rosparam get /
Output of the preceding command is as follows:
background_b: 255
background_g: 86
background_r: 69
rosdistro: 'indigo
roslaunch:
  uris: {host_d125_43873__60512: 'http://D125-43873:60512/'}
rosversion: '1.11.13
run_id: 2429b792-d23c-11e4-b9ee-3417ebbca982
rosparam set
You can change the colors of the turtle's screen to a full red background using the 
rosparam set command:
$ rosparam set background_b 0
$ rosparam set background_g 0
$ rosparam set background_r 255
$ rosservice call /clear
You will see a red background on the turtle screen. To check the numerical results, 
use the rosparam get / command.

Chapter 1
[ 29 ]
ROS services to move turtle
Another capability of nodes is to provide what in ROS terms is called a service. This 
feature is used when a node requests information from another node. Thus, there is a 
two-way communication between the nodes.
You can check the turtle's pose using the /turtle1/pose topic and the message 
type, /turtlesim/Pose. Carefully note the different notations and meanings. 
To determine the the message type, run the following command:
$ rostopic type /turtle1/pose
The output is as follows:
turtlesim/Pose
$ rosmsg show turtlesim/Pose
The output is as follows:
float32 x
float32 y
float32 theta
float32 linear_velocity
float32 angular_velocity
We can find the turtle's position, orientation in angle (theta), and its velocity using 
the rostopic echo command:
$ rostopic echo /turtle1/pose
The output is as follows:
x: 5.544444561
y: 5.544444561
theta: 0.0
linear_velocity: 0.0
angular_velocity: 0.0

Getting Started with ROS
[ 30 ]
This command outputs the result continuously and is stopped by pressing the Ctrl 
+C keys. The result will show that the turtle is at the center of its screen with no 
rotation at angle zero and no movement since the velocities are zero.
rosservice call
The turtle can be moved using the rosservice teleport option. The format of the 
command is rosservice call <service name> <service arguments>. The 
arguments here will be the turtle's position and orientation as x, y, and theta. The 
turtle is moved to position [1,1] with theta=0 by running the following command:
$ rosservice call /turtle1/teleport_absolute 1 1 0
The result can be seen in the following screenshot:
turtle after an absolute move
The relative teleport option moves the turtle with respect to its present position. 
The arguments are [linear distance , angle]. Here, the rotation angle is zero. The 
command for relative movement is as follows:
$ rosservice call /turtle1/teleport_relative 1 0
Your turtle should now move to x=2 and y=1.

Chapter 1
[ 31 ]
ROS commands summary
If you are communicating with ROS via the terminal window, it is possible to issue 
commands to ROS to explore or control nodes in a package from the command 
prompt, as listed in the following table:
Command
Action
Example usage and subcommand 
examples
roscore
This starts the Master
$ roscore
rosrun
This runs an executable 
program and creates nodes
$ rosrun [package name] 
[executable name]
rosnode
This shows information about 
nodes and lists the active 
nodes
$ rosnode info [node name]
$ rosnode <subcommand>
Subcommand: list
rostopic
This shows information about 
ROS topics
$ rostopic <subcommand> <topic 
name>
Subcommands: echo, info, and type
rosmsg
This shows information about 
the message types
$ rosmsg <subcommand> [package 
name]/ [message type]
Subcommands: show, type, and list
rosservice
This displays the runtime 
information about various 
services and allows the 
display of messages being 
sent to a topic
$ rosservice <subcommand> 
[service name]
Subcommands: args, call, find, 
info, list, and type
rosparam
This is used to get and set 
parameters (data) used by 
nodes
$ rosparam <subcommand> 
[parameter]
Subcommands: get, set, list, and 
delete
The website (http://wiki.ros.org/ROS/CommandLineTools) describes many ROS 
commands. The table lists some important ones. However, these examples only 
cover a few of the possible variations of the commands.

Getting Started with ROS
[ 32 ]
Summary
In this chapter, you first learned how to install and launch ROS. We discussed the 
ROS architecture and ROS packages, nodes, topics, messages, and services. To apply 
the knowledge, the turtlesim simulator was used to illustrate many ROS commands.
In Chapter 2, Creating Your First Two-Wheeled ROS Robot (in Simulation), we will show 
you how to build a robot model that ROS uses to display the robot and allows you 
to control it in a simulation. The chapter introduces the visualization tool called rviz 
to display the robot and the simulation tool Gazebo that includes the physics of the 
robot as you move it around in a simulated environment.

[ 33 ]
Creating Your First 
Two-Wheeled ROS Robot 
(in Simulation)
Your first robot will be created in simulation so that even if you do not have a 
robot to learn ROS on, you will be able to follow along and do the exercises in this 
book. We will build a simple two-wheeled robot named dd_robot (dd is short for 
a differential drive). We will build a Unified Robot Description Format (URDF) 
file for the robot that will describe the main components of our robot and enable 
it to be visualized and controlled by ROS tools, such as rviz and Gazebo. Rviz is a 
visualization tool in which we will view our dd_robot URDF file as we build it in 
increments. When the visual model is complete, we will modify the URDF file for use 
in the Gazebo simulator. In Gazebo, we can view the effects of physics on our model 
as we move our model around the 3D environment.
In this chapter, we will cover the following topics:
•	
An introduction to rviz, installation instructions, and instructions for use
•	
How to create and build a ROS package
•	
An incremental approach to develop a URDF file and visualize it in rviz
•	
URDF tools to verify the URDF file
•	
An introduction to Gazebo, installation instructions, and instructions for use
•	
Modifications necessary to visualize the URDF file in Gazebo
•	
Tools to verify your Gazebo URDF/Simulation Description 
Format (SDF) file
•	
A simple way to control a robot in Gazebo

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 34 ]
We begin by learning about rviz.
Rviz
Rviz, abbreviation for ROS visualization, is a powerful 3D visualization tool for ROS. 
It allows the user to view the simulated robot model, log sensor information from 
the robot's sensors, and replay the logged sensor information. By visualizing what 
the robot is seeing, thinking, and doing, the user can debug a robot application from 
sensor inputs to planned (or unplanned) actions.
Rviz displays 3D sensor data from stereo cameras, lasers, Kinects, and other 3D 
devices in the form of point clouds or depth images. 2D sensor data from webcams, 
RGB cameras, and 2D laser rangefinders can be viewed in rviz as image data.
If an actual robot is communicating with a workstation that is running rviz, rviz 
will display the robot's current configuration on the virtual robot model. ROS topics 
will be displayed as live representations based on the sensor data published by any 
cameras, infrared sensors, and laser scanners that are part of the robot's system. This 
can be useful to develop and debug robot systems and controllers. Rviz provides 
a configurable Graphical User Interface (GUI) to allow the user to display only 
information that is pertinent to the present task.
In this chapter, we will use rviz to visualize our progress in creating a two-wheeled 
robot model. Rviz will use the URDF file that we create for our robot and display 
the visual representation.
We will begin by checking whether rviz has been downloaded and installed on 
your system.
Installing and launching rviz
To run rviz, you require a powerful graphics card and the appropriate drivers need 
to be installed on your computer.
If you have trouble with running rviz, refer to http://wiki.
ros.org/rviz/Troubleshooting or search the ROS forum at 
http://answers.ros.org/questions/.
Rviz should have been installed on your computer as part of the ros-indigo-desktop-
full installation, as described in the Installing and launching ROS section in Chapter 1, 
Getting Started with ROS.

Chapter 2
[ 35 ]
To test whether rviz has been installed correctly, open a terminal window and start 
the ROS Master by typing the following command:
$ roscore
Next, open a second terminal window and type the following command:
$ rosrun rviz rviz
This will display an environment similar to the following screenshot:
rviz main screen
If the $ rosrun rviz rviz command generates a warning message, make sure 
that you have source ~/catkin_ws/devel/setup.bash in your .bashrc file, or 
this command is entered at the terminal window prompt. The .bashrc file resides 
in your home directory but cannot be seen unless you use the $ ls –la command 
option to list the directory and files. This option shows the hidden files that are 
preceded by a dot (.).
If rviz has not been installed, then install it from the Debian repository using the 
following command:
$ sudo apt-get install ros-indigo-rviz

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 36 ]
If you wish to install rviz from a source, refer to the rviz user guide 
at http://wiki.ros.org/rviz/UserGuide. This guide is also 
a good reference to learn additional features of rviz that are not 
covered in this book.
Using rviz
The central window on the rviz main screen provides the world view of a 3D 
environment. Typically, only the grid is displayed in the center window or the 
window is blank.
The main screen is divided into four main display areas: the central window, the 
Displays panel to the left, the Views panel to the right, and the Time panel at the 
bottom. Across the top of these display areas are the toolbar and the main screen 
menu bar. Each of these areas of the rviz main screen is described in the following 
sections. This overview is provided so that you can gain familiarity with the 
rviz GUI.
Displays panel
On the left panel of the rviz main screen is the Displays panel, where the user can 
add, remove, or rename the visualization elements in the 3D environment.
By clicking on the Add button on the Displays panel, the Add menu appears, as 
shown in the following screenshot. This menu displays the visualization elements 
that can be added to the environment, such as a camera image, point cloud, robot 
state, and so on. A brief description of each item is provided at the bottom of the 
window when that item is highlighted. A unique display name can be entered for 
the item to be added to the environment. For further details on the display types, 
go to http://wiki.ros.org/rviz/DisplayTypes. This site also identifies the ROS 
messages that provides the data for the display:

Chapter 2
[ 37 ]
Rviz 'Add' Display menu
Clicking on the triangle symbol on the left side of a panel item will expand or hide 
the details of the item.
The Displays panel also shows access to Global Options, such as Background Color 
and Frame Rate. The options under the Grid element allow the user to tailor the grid 
lines by changing the number of grid cells, line width, line color, and so on.
Views and Time panels
The Views panel on the right side of the rviz main screen and the Time panel at the 
bottom are not important at this point, and so we have removed them from the rest 
of the screenshots in this chapter. We will work in the orbit view, which is the default 
camera view for rviz. In this view, the orbital camera will rotate around a focal point 
visualized as a small yellow disc in the 3D world view.

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 38 ]
Mouse control
To move around the 3D world in the orbit view, use the mouse and keyboard 
as follows:
•	
Left mouse button: Click and drag to rotate around the focal point.
•	
Middle mouse button (if available): Click and drag to move the focal point 
in the plane formed by the camera's up and right vectors. (The Shift key and 
left mouse button combination also invokes this mode.)
•	
Right mouse button: Click and drag to zoom in/out of the focal point. 
Dragging up zooms in and down zooms out. (The scroll wheel also invokes 
this mode.)
Toolbar
The toolbar at the top of the rviz main screen provides the following functionalities:
•	
Interact: This shows interactive markers when present.
•	
Move camera (default mode): This is a 3D view that responds to the 
mouse/keyboard, as described for the Views panel.
•	
Select: This allows items to be selected by a mouse click or drag box 
selection. The selected item will have a wireframe box placed around it.
•	
Focus camera: A mouse click on a specific spot in the 3D view becomes the 
focal point of the camera.
•	
2D Nav Goal and 2D Pose Estimate: These selections will be discussed in 
Chapter 4, Navigating the World with TurtleBot.
Main window menu bar
The top-most main window menu bar provides options under the basic File, Panels, 
and Help headings, as shown here:
•	
File: Open Config, Save Config, Save Config As, Recent Configs, Save 
Image, and Quit
•	
Panels: Add Panel and Delete Panel
Other options for panels are: Tools, Displays, Selection, Tool Properties, 
Views, and Time
•	
Help: Show Help panel, Open rviz wiki in browser, and About
These selections allow the user to customize the panels to be displayed for rviz. 
This custom configuration of rviz can be saved and reused.

Chapter 2
[ 39 ]
In this chapter, we use rviz to visualize the construction of our two-wheeled robot 
model in 3D. We will show you how to use rviz to visualize odometry data for 
navigation purposes in Chapter 3, Driving Around with TurtleBot.
For more in-depth tutorials on rviz, go to http://wiki.ros.org/rviz/Tutorials.
At this point, rviz can be exited by navigating to File | Quit. In the next section, 
we create and build a ROS package to hold our URDF code and launch files.
Creating and building a ROS package
Before we begin to design and build our robot model in simulation, we should create 
our first ROS package. In Chapter 1, Getting Started with ROS, we created a ROS 
catkin workspace under /home/<username>/catkin_ws. The structure of a catkin 
workspace looks like this:
catkin_ws/                     -- WORKSPACE
   build/                          -- BUILD SPACE
   devel/                          -- DEVEL SPACE
   src/                               -- SOURCE SPACE
   CMakeLists.txt          -- 'Toplevel' CMake file, provided by catkin
Make sure that you have source ~/catkin_ws/devel/setup.bash 
in your .bashrc file, or this command is entered at the terminal window 
prompt.
We begin by moving to your catkin workspace source directory:
$ cd ~/catkin_ws/src
Now, let's create our first ROS package, ros_robotics:
$ catkin_create_pkg ros_robotics
This command will create a /ros_robotics directory under the ~/catkin_ws/
src directory. The /ros_robotics directory will contain a package.xml file and a 
CMakeLists.txt file. These files contain information generated from the $ catkin_
create_pkg command execution.

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 40 ]
The catkin_create_pkg syntax
catkin_create_pkg requires a unique package name and, optionally, 
a list of dependencies for the package. The command format to create it is 
as follows:
$ catkin_create_pkg <package_name> [depend1] [depend2] 
[depend3]
[depend1], [depend2], and [depend3] specify software packages 
that are required to be present for this software package to be made.
We will not identify any dependencies for our ros_robotics package at this point.
Next, build the packages in the catkin workspace:
$ cd ~/catkin_ws
$ catkin_make
After the workspace has been built to include the ros_robotics package, 
the ~/catkin_ws/devel subdirectory will have a structure similar to the 
structure under the /opt/ros/indigo directory.
Building a differential drive robot URDF
URDF is an XML format specifically defined to represent robot models down to their 
component level. These URDF files can become long and cumbersome on complex 
robot systems. Xacro (XML Macros) is an XML macro language created to make 
these robot description files easier to read and maintain. Xacro helps you reduce 
the duplication of information within the file.
For our first robot model, we will build a URDF file for a two-wheeled differential 
drive robot. The model will be created incrementally, and we will view the results 
at each step in rviz. When our simple two-wheeled robot is complete, we will add 
Gazebo formatting and view the model in Gazebo. In Chapter 5, Creating Your First 
Robot Arm (in Simulation), we will expand our knowledge of URDF files and build a 
simple robot arm model using the Xacro notation.
Downloading the ros_robotics code
You can download the example code files and other support 
material for this book from www.PacktPub.com.

Chapter 2
[ 41 ]
If you download the ros_robotics package from the Packt website, replace the 
entire ~/catkin_ws/src/ros_robotics directory with the downloaded package. 
Instead, if you plan to enter the code from this book, begin by creating a /urdf 
directory under your ros_robotics package directory:
$ cd ~/catkin_ws/src/ros_robotics
$ mkdir urdf
$ cd urdf
Creating a robot chassis
Two basic URDF components are used to define a tree structure that describes a 
robot model. The link component describes a rigid body by its physical properties 
(dimensions, position of its origin, color, and so on). Links are connected together by 
joint components. Joint components describe the kinematic and dynamic properties 
of the connection (that is, links connected, types of joint, axis of rotation, amount 
of friction and damping, and so on). The URDF description is a set of these link 
elements and a set of the joint elements connecting the links together.
The first component of our robot is a simple chassis box. The downloaded 
dd_robot.urdf file contains the code for this exercise. Alternately, you 
can enter the code portion using your favorite editor and save the file as 
dd_robot.urdf to your ~/catkin_ws/src/ros_robotics/urdf directory:
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  <link name="base_link">
    <visual>
      <origin xyz="0 0 0" rpy="0 0 0" />
      <geometry>
          <box size="0.5 0.5 0.25"/>
      </geometry>
    </visual>
  </link>
</robot>
XML comments are bracketed by <!--       and        -->.

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 42 ]
This XML code defines a robot labeled dd_robot that has one link (also known as 
part) whose visual component is a box 0.5 meters long, 0.5 meters wide, and 0.25 
meters tall. The box is centered at the origin (0, 0, 0) of the environment with no 
rotation in the roll, pitch, or yaw (rpy) axes. The link has been labeled base_link, 
and our model will use this box as the link on which our other links are defined. 
(A base_link link should be identified as the URDF root link to create the beginning 
of the robot's kinematic chain.) This XML is quite a bit of code for a simple box, but it 
will soon become even more complicated.
Using roslaunch
Roslaunch is a ROS tool that makes it easy to launch multiple ROS nodes as well 
as set parameters on the ROS Parameter Server. Roslaunch configuration files 
are written in XML and typically end in a .launch extension. In a distributed 
environment, the .launch files also indicate the processor the nodes should run on.
The roslaunch syntax is as follows:
$ roslaunch <package_name> <file.launch>
To use roslaunch for our URDF file, you will need to use one of the following ways:
•	
Download the ddrobot_rviz.launch file from the ros_robotics/launch 
directory from this book's website
•	
Create a launch directory under the ros_robotics package and create the 
ddrobot_rviz.launch file from the following XML code:
<launch>
  <!-- values passed by command line input -->
  <arg name="model" />
  <arg name="gui" default="False" />
  <!-- set these parameters on Parameter Server -->
  <param name="robot_description" 
    textfile="$(find ros_robotics)/urdf/$(arg model)" />
  <param name="use_gui" value="$(arg gui)"/>
  <!-- Start 3 nodes: joint_state_publisher, 
    robot_state_publisher and rviz -->
    <node name="joint_state_publisher" 
    pkg="joint_state_publisher" 
    type="joint_state_publisher" />
    <node name="robot_state_publisher" 
    pkg="robot_state_publisher"

Chapter 2
[ 43 ]
    type="state_publisher" />
    <node name="rviz" pkg="rviz" type="rviz"
    args="-d $(find ros_robotics)/urdf.rviz" 
    required="true" />
</launch>
This roslaunch file performs the following:
•	
Loads the model specified in the command line into the Parameter Server.
•	
Starts nodes that publish the JointState and transforms (discussed later in 
this chapter).
•	
Starts rviz with a configuration file (urdf.rviz). It is important to use the 
urdf.rviz file that came with the book example code or save your own 
urdf.rviz file from rviz to be used with this launch file.
Type the following command to see the robot model in rviz:
$ roslaunch ros_robotics ddrobot_rviz.launch model:=dd_robot.urdf
At this point, your rviz screen should resemble one of the following two screenshots. 
Look carefully for a visual representation of your dd_robot box in the main screen 
and examine it under the Displays panel to decide how to proceed. Does your rviz 
screen look like the following screenshot:
Rviz screen without urdf.rviz file

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 44 ]
If your rviz screen looks like the the preceding screenshot with no box on the main 
screen and no Robot Model or TF under the Displays panel, then perform the 
following three steps in any order:
•	
Select the Add button under the Displays panel and add RobotModel
•	
Select the Add button under the Displays panel and add TF
•	
Select the field next to Fixed Frame (under Global Options), which in the 
preceding screenshot says map, and type in base_link
(The preceding screenshot shows the Add menu with the RobotModel selection 
highlighted.) When all the three steps are completed, your rviz screen will look 
similar to the following screenshot.
When you go to File | Quit to close your rviz session, you will be asked 
whether you want to save the configuration to a urdf.rviz file and it 
is recommended that you do. If you do not, you will have to perform the 
previous three steps each time to see your RobotModel and TF frames.
For the users who copied the urdf.rviz file from the book example code, the rviz 
screen will come up and and look like this:
dd_robot.urdf in rviz

Chapter 2
[ 45 ]
Things to note:
•	
The fixed frame is a transform frame where the center (origin) of the grid 
is located.
•	
In URDF, the <origin> tag defines the reference frame of the visual element 
with respect to the reference frame of the link. In dd_robot.urdf, the visual 
element (the box) has its origin at the center of its geometry by default. Half 
of the box is above the grid plane and half is below.
•	
The rviz display configuration has been changed to remove the View and 
Time displays. This configuration is defined in the urdf.rviz file that comes 
with the book's example code (refer to the .launch file commands).
•	
The RobotModel and TF displays have been added under the Displays 
panel. Under RobotModel, notice the following:
°°
Robot description: robot_description is the name of the ROS 
parameter where the URDF file is stored on the Parameter Server. 
The description of the links and joints and how they are connected is 
stored here.
Adding wheels
Now, let's add shapes and links for wheels on our robot. When we add link elements 
to the URDF file, we must add joints to describe the relationship between the links. 
Joint elements define whether the joint is flexible (movable) or inflexible (fixed). For 
flexible joints, the URDF describes the kinematics and dynamics of the joint as well 
as its safety limits. In URDF, there are six possible joint types, which are as follows:
•	
Fixed: This is not really a joint because it cannot move. All degrees of 
freedom are locked. This type of joint does not require the axis, calibration, 
dynamics, limits, or safety controller.
•	
Revolute: This joint rotates around one axis and has a range specified by the 
upper and lower limits.
•	
Continuous: This is a continuous hinge joint that rotates around the axis and 
has no upper and lower limits.
•	
Prismatic: This is a sliding joint that slides along the axis and has a limited 
range specified by the upper and lower limits.
•	
Floating: This joint allows motion for all six degrees of freedom.
•	
Planar: This joint allows motion in a plane perpendicular to the axis.

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 46 ]
For our robot wheels, we require continuous joints, which means that they can 
respond to any rotation angle from negative infinity to positive infinity. They are 
modeled like this so that they can rotate in both directions forever.
The downloaded dd_robot2.urdf file contains the XML code for this exercise. 
Alternately, you can enter the new code portion to your previous URDF file to create 
the two wheels (lines from the previous code have been left in or omitted and new 
code has been highlighted):
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  <link name="base_link">
  …
  </link>
  <!-- Right Wheel -->
  <link name="right_wheel">
    <visual>
      <origin xyz="0 0 0" rpy="1.570795 0 0" />
      <geometry>
          <cylinder length="0.1" radius="0.2" />
      </geometry>
    </visual>
  </link>
  <joint name="joint_right_wheel" type="continuous">
    <parent link="base_link"/>
    <child link="right_wheel"/>
    <origin xyz="0 -0.30 0" rpy="0 0 0" /> 
    <axis xyz="0 1 0" />
  </joint>
  <!-- Left Wheel -->
  <link name="left_wheel">
    <visual>
      <origin xyz="0 0 0" rpy="1.570795 0 0" />
      <geometry>
          <cylinder length="0.1" radius="0.2" />
      </geometry>
    </visual>
  </link>
  <joint name="joint_left_wheel" type="continuous">
    <parent link="base_link"/>
    <child link="left_wheel"/>
    <origin xyz="0 0.30 0" rpy="0 0 0" /> 
    <axis xyz="0 1 0" />
  </joint>
</robot>

Chapter 2
[ 47 ]
Run your rviz roslaunch command:
$ roslaunch ros_robotics ddrobot_rviz.launch model:=dd_robot2.urdf
Rviz should come up and look like this:
dd_robot2.urdf in rviz
Things to note in the URDF:
•	
Each wheel is defined visually as a cylinder of radius 0.2 meters and length 
of 0.1 meters. The wheel's visual origin defines where the center of the visual 
element should be, relative to its origin. Each wheel's origin is at (0, 0, 0) and 
is rotated by 1.560795 radians (= pi/2 = 90 degrees) about the x axis.
•	
The joint is defined in terms of a parent and a child. The URDF file is 
ultimately a tree structure with one root link. The base_link link 
is our robot's root link with the wheel's position dependent on the 
base_link's position.

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 48 ]
•	
The wheel joint is defined in terms of the parent's reference frame. Therefore, 
the wheel's joint origin is 0.30 meters in the x direction for the left wheel and 
-0.30 meters for the right wheel.
•	
The axis of rotation is specified by an xyz triplet, indicating that the wheel's 
joint axis of rotation is around the y axis.
•	
These <joint> elements define the complete kinematic model of our robot.
Adding a caster
In the next step, we will add a caster to the front of our robot in order to keep 
the robot chassis balanced. The castor will only be a visual element added to the 
chassis and not a joint. The caster will slide along the ground plane as the robot's 
wheels move.
The downloaded dd_robot3.urdf file contains the XML code for this exercise. 
Alternately, you can enter the new code portion to your previous URDF file(new 
code has been highlighted):
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  …
    <!-- Caster -->
    <visual name="caster">
      <origin xyz="0.2 0 -0.125" rpy="0 0 0" />
      <geometry>
        <sphere radius="0.05" />
      </geometry>
    </visual>
  </link>
  <!-- Right Wheel -->
  …
  <!-- Left Wheel -->
  …
</robot>
Run your rviz roslaunch command:
$ roslaunch ros_robotics ddrobot_rviz.launch model:=dd_robot3.urdf

Chapter 2
[ 49 ]
Rviz should come up and look like this:
dd_robot3.urdf in rviz
Things to note in the URDF file:
•	
The caster is defined visually as a sphere with a radius of 0.05 meters. The 
center of the caster is 0.2 meters in the x direction and -0.125 meters in the z 
direction with respect to the origin of the base_link.
Adding color
A completely red robot has parts that are not distinctive enough; we will add some 
color to our model.
The downloaded dd_robot4.urdf file contains the XML code for this exercise. 
Alternately, you can enter the new code portions to your previous URDF file 
(new code has been highlighted):
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  ...
      <material name="blue">
        <color rgba="0 0.5 1 1"/>

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 50 ]
      </material>
    </visual>
    <!-- Caster -->
    …
  <!-- Right Wheel -->
  …
      <material name="black">
        <color rgba="0.05 0.05 0.05 1"/>
      </material>
     </visual>
 …
  <!-- Left Wheel -->
  …
      <material name="black"/>
    </visual>
  …
</robot>
Run your rviz roslaunch command: 
$ roslaunch ros_robotics ddrobot_rviz.launch model:=dd_robot4.urdf
Rviz should look like the following screenshot:
dd_robot4.urdf in rviz

Chapter 2
[ 51 ]
Things to note in the URDF file:
•	
The <material> tag can define <color> in terms of red/green/blue/alpha, 
each in the range of [0,1]. Alpha is the transparency level of the color. An 
alpha value of 1 is opaque and 0 is transparent. Once specified and labeled 
with a name, the material name can be reused without specifying the color 
values. (For example, note that the left wheel does not have a <color rgba> 
tag because it has been defined in the right wheel visual link.)
•	
Although the book may show this picture in shades of gray, the chassis of the 
robot is now blue and the wheels are black.
Adding collisions
Next, we will add the <collision> properties to each of our <link> elements. Even 
though we have defined the visual properties of the elements, Gazebo's collision 
detection engine uses the collision property to identify the boundaries of the object. 
If an object has complex visual properties (such as a mesh), a simplified collision 
property should be defined in order to improve the collision detection performance.
The downloaded dd_robot5.urdf file contains the XML code for this exercise. 
Alternately, you can enter the new code portions to your previous URDF file 
(new code has been highlighted):
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  …
    <!-- Base collision -->
    <collision>
      <origin xyz="0 0 0" rpy="0 0 0" />
      <geometry>
          <box size="0.5 0.5 0.25"/>
      </geometry>
    </collision>
    <!-- Caster -->
    …
    <!-- Caster collision -->
    <collision>
      <origin xyz="0.2 0 -0.125" rpy="0 0 0" />
      <geometry>
        <sphere radius="0.05" />
      </geometry>
    </collision>

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 52 ]
  </link>
  <!-- Right Wheel -->
  …
    <!-- Right Wheel collision -->
    <collision>
      <origin xyz="0 0 0" rpy="1.570795 0 0" />
      <geometry>
          <cylinder length="0.1" radius="0.2" />
      </geometry>
    </collision>
  …
  <!-- Left Wheel -->
  …
    <!-- Left Wheel collision -->
    <collision>
      <origin xyz="0 0 0" rpy="1.570795 0 0" />
      <geometry>
          <cylinder length="0.1" radius="0.2" />
      </geometry>
    </collision>
</robot>
Adding the <collision> property does not change the visual model of the robot, 
and the rviz display will look the same as in the previous screenshot.
Moving the wheels
Now that we have the right and left wheel joints defined and we can see them 
clearly, we will bring up the GUI pop-up screen to control these joints. In the 
ddrobot_rviz.launch file, we start three ROS nodes: joint_state_publisher, 
robot_state_publisher, and rviz. The joint_state_publisher node finds all of 
the non-fixed joints and publishes a JointState message with all those joints defined. 
So far, the values in the JointState message have been constant, keeping the wheels 
from rotating. We bring up a GUI interface in rviz to change the value of each 
JointState and watch the wheels rotate.
Add the gui field to the rviz roslaunch command:
$ roslaunch ros_robotics ddrobot_rviz.launch model:=dd_robot5.urdf 
gui:=True

Chapter 2
[ 53 ]
Rviz should look like the following screenshot:
dd_robot5.urdf in rviz
Things to note:
•	
The joint positions in the window are sliders. The wheel joints are defined as 
continuous but this GUI limits each slider's value from –Pi to +Pi. Play with 
the sliders and see how the wheels move.
•	
The Randomize button will select a random value for both the joints.
•	
The Center button will move both the joints to the zero position. 
(Visually, the blue dot on both the wheels should be at the top).

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 54 ]
A word about tf and robot_state_publisher
A robotic system is made up of a collection of 3D coordinate frames for every 
component in the system. In our dd_robot model, there is a base coordinate frame 
and a frame for each wheel that relates back to the base coordinate frame. The 
model's coordinate frames are also related to the world coordinate frame of the 
3D environment. The tf package is the central ROS package used to relate the 
coordinate frames of our robot to the 3D simulated environment (or a real 
robot to its real environment).
The robot_state_publisher node subscribes to the JointState message and 
publishes the state of the robot to the tf transform library. The tf transform library 
maintains the relationships between the coordinate frames of each component 
in the system over time. The robot_state_publisher node receives the robot's 
joint angles as inputs and computes and publishes the 3D poses of the robot links. 
Internally, the robot_state_publisher node uses a kinematic tree model of the 
robot built from its URDF. Once the robot's state gets published, it is available to 
all components in the system that also use tf.
Adding physical properties
With the additional physical properties of mass and inertia, our robot will be ready 
to be launched in the Gazebo simulator. These properties are needed by Gazebo's 
physics engine. Specifically, every <link> element that is being simulated needs an 
<inertial> tag.
The two sub-elements of the inertial element we will use are as follows:
•	
<mass>: This is the weight defined in kilograms.
•	
<inertia>: This frame is a 3 x 3 rotational inertia matrix. Because 
this matrix is symmetrical, it can be represented by only six elements. 
The six highlighted elements are the six element <inertia> values. 
The other three values are not used.
ixx
ixy
ixz
ixy
iyy
iyz
ixz
iyz
izz
Using Wikipedia's list of moment of inertia tensors (https://en.wikipedia.org/
wiki/List_of_moments_of_inertia), which provides the equations for the inertia 
of simple geometric primitives, such as a cylinder, box, and sphere. We use these 
equations to compute the inertia values for the model's chassis, caster, and wheels.

Chapter 2
[ 55 ]
Do not use inertia elements of zero (or almost zero) because real-time controllers can 
cause the robot model to collapse without warning, and all links will appear with 
their origins coinciding with the world origin.
The downloaded dd_robot6.urdf file contains the XML code for this exercise. 
Alternately, you can enter the new code portions in your previous URDF file 
(new code has been highlighted):
<?xml version='1.0'?>
<robot name="dd_robot">
  <!-- Base Link -->
  …
    <inertial>
      <mass value="5"/>
      <inertia ixx="0.13" ixy="0.0" ixz="0.0" 
               iyy="0.21" iyz="0.0" izz="0.13"/>
    </inertial>
    <!-- Caster -->
    …
    <inertial>
      <mass value="0.5"/>
      <inertia ixx="0.0001" ixy="0.0" ixz="0.0" 
               iyy="0.0001" iyz="0.0" izz="0.0001"/>
    </inertial>
  </link>
  <!-- Right Wheel -->
  …
    <inertial>
      <mass value="0.5"/>
      <inertia ixx="0.01"  ixy="0.0" ixz="0.0" 
               iyy="0.005" iyz="0.0" izz="0.005"/>
    </inertial>
  …
  <!-- Left Wheel -->
  …
    <inertial>
      <mass value="0.5"/>
      <inertia ixx="0.01" ixy="0.0" ixz="0.0" 
               iyy="0.005" iyz="0.0" izz="0.005"/>
    </inertial>
    …
</robot>

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 56 ]
Adding the <inertial> property does not change the visual model of the robot, 
and the rviz display will look the same as the preceding screenshot.
Trying URDF tools
ROS provides command-line tools that can help verify and visualize information 
about your URDF. We will try out these tools on our robot URDF but first the tools 
must be installed on your computer system. Type the following command:
$ sudo apt-get install liburdfdom-tools
check_urdf
check_urdf attempts to parse a URDF file description and either prints a description 
of the resulting kinematic chain or an error message:
$ check_urdf dd_robot6.urdf
The output of the preceding command is as follows:
robot name is: dd_robot
---------- Successfully Parsed XML ---------------
root Link: base_link has 2 child(ren)
 child(1): left_wheel
child(2): right_wheel
urdf_to_graphiz
The urdf_to_graphiz tool creates a graphviz diagram of a URDF file and a diagram 
in the .pdf format. Graphviz is an open source graph visualization software. 
To execute urdf_to_graphiz, type:
$urdf_to_graphiz dd_robot6.urdf
The output is as follows:
Created file dd_robot.gv
Created file dd_robot.pdf

Chapter 2
[ 57 ]
The dd_robot.pdf file is shown as follows:
dd_robot.pdf
Now that we have a working URDF model of our two-wheeled robot, we are 
ready to launch it into Gazebo and move it around. First, we must make some 
modifications to our URDF file to add simulation-specific tags so that it properly 
works in Gazebo. Gazebo uses SDF, which is similar to URDF, but by adding 
specific Gazebo information, we can convert our dd_robot model file into an 
SDF-type format.
Gazebo
Gazebo is a free and open source robot simulation environment developed by 
Willow Garage. As a multifunctional tool for ROS robot developers, Gazebo 
supports the following:
•	
Designing of robot models
•	
Rapid prototyping and testing of algorithms
•	
Regression testing using realistic scenarios
•	
Simulation of indoor and outdoor environments
•	
Simulation of sensor data for laser range finders, 2D/3D cameras, 
kinect-style sensors, contact sensors, force-torque, and more
•	
Advanced 3D objects and environments utilizing Object-Oriented Graphics 
Rendering Engine (OGRE)
•	
Several high-performance physics engines (Open Dynamics Engine (ODE), 
Bullet, Simbody, and Dynamic Animation and Robotics Toolkit (DART)) to 
model the real-world dynamics

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 58 ]
In this section, we will load our two-wheeled robot URDF into Gazebo to visualize 
it in a 3D environment. Gazebo allows you to take control of some aspects of our 
model without an external control program. In the later chapters, we will be using 
simulated versions of robots in Gazebo to control joints, visualize sensor data, and 
test control algorithms.
Installing and launching Gazebo
To run Gazebo requires a powerful graphics card and the appropriate drivers be 
installed on your computer.
If you have trouble with running Gazebo, refer to http://answers.
gazebosim.org/questions/ or search the ROS forum at http://
answers.ros.org/questions/.
Gazebo should have been installed on your computer as part of the ros-indigo-
desktop-full installation, as described in the Installing and launching ROS section in 
Chapter 1, Getting Started with ROS. Gazebo 2 is the default version of Gazebo for 
ROS-Indigo/Ubuntu-Trusty and is the version recommended for the exercises 
in this book.
To test whether Gazebo has been installed correctly, open a terminal window and 
type the following command:
$ gazebo
This should display an environment similar to the following screenshot:
Gazebo main screen

Chapter 2
[ 59 ]
If Gazebo has not been installed, refer to the ROS-Indigo installation instructions 
at http://wiki.ros.org/indigo/Installation/Ubuntu or the general Gazebo 
installation instructions at http://gazebosim.org/tutorials?cat=install. 
Make sure that you install the Gazebo 2 version.
The $ gazebo command runs two different executables: the Gazebo server and the 
Gazebo client. The Gazebo server gzserver will execute the simulation process, 
including the physics update loop and sensor data generation. This process is the 
core of Gazebo and can be used independently of any graphical interface. The 
Gazebo client gzclient command runs the Gazebo GUI. This GUI provides 
a nice visualization of simulation and handy controls for an assortment of 
simulation properties.
Tutorials for Gazebo can be found at http://gazebosim.org/tutorials.
Use the Ctrl + C keys to kill the terminal window process 
after you have closed the Gazebo window.
Important commands: If at any time, your command 
generates a warning or error command, type $ rosnode 
list to determine whether there are any active nodes still 
lingering after you have attempted to shut down Gazebo. 
If there are still active nodes, use the $ rosnode kill 
command to list the active nodes. Next, select the number 
of the ROS nodes that you wish to kill.
Using roslaunch with Gazebo
Roslaunch is a standard method used to start Gazebo with world files and robot 
URDF models. To perform a basic test of Gazebo, an empty Gazebo world can be 
brought up for the following command:
$ roslaunch gazebo_ros empty_world.launch
This test will verify that Gazebo has been installed properly. If you wish to try 
other demo worlds, including the gazebo_ros package, try substituting one of the 
following with empty_world.launch in the previous command:
•	
willowgarage_world.launch
•	
mud_world.launch
•	
shapes_world.launch
•	
rubble_world.launch

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 60 ]
For the exercises in this chapter, we created our own world, ddrobot.world. This 
world consists of a ground plane and two construction cones for you to drive the 
robot around. You will find this file in the ros_robotics package under ros_
robotics/worlds. We will launch our dd_robot model into this world using the 
ddrobot_gazebo.launch launch file found in the ros_robotics/launch directory.
Using Gazebo
The Gazebo GUI is similar to rviz in many ways. The central window provides the 
view for Gazebo's 3D world environment. The grid is typically configured to be the 
ground plane of the environment on which all the models are held due to gravity in 
the environment.
Gazebo also has the same cursor/mouse control as rviz, described in the Using rviz 
section. For Gazebo keyboard shortcuts, visit http://gazebosim.org/hotkeys.
Double-clicking on a spot in the environment will cause the display to be zoomed in 
to that spot.
For Gazebo, the standard units of measurement are in terms 
of meters and kilograms (like rviz).
The main screen of Gazebo is divided into four main display areas: the central 
window, the World and Insert panels to the left, the Joints panel to the right, and 
the Simulation panel at the bottom. Across the top of these display areas are the 
Environment toolbar and the main screen menu bar. Each of the display areas of 
Gazebo will be described in the following sections. This overview should provide 
basic familiarity with the Gazebo GUI.
Environment toolbar
The toolbar at the top of the Gazebo environment display provides the 
following functionalities:
•	
Selection Mode (Esc): This selects a model in the environment when the 
cursor is clicked on it or a drag box is wrapped around it. The World panel 
displays the model's properties. When selected, a white outline 3D box is 
drawn around the model. A yellow disc is placed where the cursor is clicked 
and becomes the focal point when the mouse controls are used to move 
around the scene. (Hitting the Esc key will also activate this mode.)

Chapter 2
[ 61 ]
•	
Translation Mode (T): This selects a model in the environment when 
the cursor is clicked on it or a drag box is wrapped around it. A 3D axis 
(red-green-blue) is drawn and centered on the model. Use the cursor and 
left mouse button to move the model anywhere in the x, y, and z plane. 
(Hitting the T key will also activate this mode.)
•	
Rotation Mode (R): This selects a model in the environment when the cursor 
is clicked on it or a drag box is wrapped around it. A 3D sphere (red-green-
blue) is drawn and centered around the model. Use the cursor and left mouse 
button to rotate the model in the roll, pitch, or yaw directions using one of 
the rings. (Hitting the R key will also activate this mode.)
•	
Scale Mode (S): This selects a model in the environment when the cursor is 
clicked on it or a drag box is wrapped around it. A 3D axis (red-green-blue) 
with square endpoints is drawn and centered on the model. Scaling of a 
model is currently limited to only simple shapes. (A warning message will be 
displayed in the terminal window if the user attempts to scale other models.)
•	
The next set of icons is used to create simple shapes in Gazebo: Box, Sphere, 
and Cylinders. Click on the icon and place the image shape anywhere in the 
3D environment. The scale mode can then be used to resize the object.
•	
The next set of icons is used for lighting: Point Light, Spot Light, and 
Directional Light. Explore these if you wish to change the lighting and 
shadows in your environment.
•	
The last icon of the camera will take a Screenshot and save it to your 
/home/<username>/.gazebo/pictures directory.
World and Insert panels
The World panel to the left of the 3D environment provides access to all of the 
environment elements. These environment elements are Scene, Physics, Models, 
and Lights. By clicking on any of these labels, a properties panel will appear below 
the World panel with a properties list specific for that element.
The Scene selections allow the user to alter the ambient environment, the 
background and the shadows. The Physics selections check whether the physics 
engine is enabled. If the physics engine is enabled, the user can control the real-
time update rate, gravity, and constraints under the Physics tab as well as other 
properties. The Models list will display all models active in the environment. When 
the $ gazebo command is used, the only active model will be the ground_plane 
model. By clicking on the ground_plane label, the properties displayed will be the 
model name, a checkbox to make the model static, the model's pose (x, y, z, roll, 
pitch, and yaw) in the environment, and its link information. 

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 62 ]
These details are lengthy so you can explore them at your leisure. The last 
element, Lights, displays all the light sources for the environment. For our default 
environment, the sun is the only source. The properties of the sun are its pose, 
diffuse, specular, range, and attenuation. Our primary interest for this book 
will be the Models.
The Insert tab is behind the World panel. The Insert panel accesses two locations 
to allow the user to select from a number of predefined models to be added to 
the environment. The first location, /home/<username>/.gazebo/models, is the 
user's repository of Gazebo models that they have selected from the main Gazebo 
repository. This repository is the second selection available at http://gazebosim.
org/models/.
Joints panel
The panels to the right and left of the center display can be revealed or hidden using 
the three tiny rectangles in the black vertical strip. On the right side, the user can 
click and drag these three tiny rectangles to reveal the Joints panel. Under the Joints 
panel, there are three tabs labeled: Force, Position, and Velocity. There is also a 
Reset button to return your model to its original state (if possible).
To use these controls, the model must be selected to reveal the available model joints. 
For the joint control, we have the following values:
•	
Force values are in Newton meters
•	
Position values are in radians or degrees (make a selection from the 
drop-down window), P Gain (for proportional gain), I Gain (for integral 
gain), and D Gain (for derivative gain)
•	
Velocity values are in m/s (for meters per second), P Gain (for proportional 
gain), I Gain (for integral gain), and D Gain (for derivative gain)
The slider bar at the bottom of the Joints panel will help you 
see the information to the right of the display.
Main window menu bar
The top-most main window menu bar provides options under the basic File, Edit, 
View, Window, and Help headings as shown here:
•	
File: Save World, Save World As, and Quit
•	
Edit: Reset Model Poses, Reset World, and Building Editor

Chapter 2
[ 63 ]
•	
View: Grid, Transparent, Wireframe, Collisions, Joints, Center of 
Mass/Inertia, Contacts, Reset Camera, Full Screen, and Orbital 
View Control
•	
Window: Topic Visualization and Log Data
•	
Help: About
These selections can be very useful. For example, if you wish to check the center 
of mass for your URDF in Gazebo, click on the View heading and select both the 
Wireframe and the Center of Mass options.
Simulation panel
At the bottom of the environment display is a handy tool used to run simulation 
scripts. It is useful when recording and playing back simulation runs.
Before we can load our dd_robot model into Gazebo, we must make a few 
modifications to the URDF file.
Modifications to the robot URDF
Gazebo expects the robot model file to be in SDF format. SDF is similar to the URDF, 
using some of the same XML descriptive tags. With the following modifications, 
Gazebo will automatically convert the URDF code into an SDF robot description. 
The following sections will describe the steps to be taken.
Adding the Gazebo tag
The <gazebo> tag must be added to the URDF to specify additional elements needed 
for simulation in Gazebo. This tag allows for identifying elements found in the SDF 
format that are not found in the URDF format. If a <gazebo> tag is used without a 
reference="" property, it is assumed that the <gazebo> elements refer to the whole 
robot model. The reference parameter usually refers to a specific robot link.
Other <gazebo> elements for both the links and joints can be applied to your robot 
but are not described in this book because of the extensive list and explanations of 
how they applied to the physics of Gazebo.

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 64 ]
Refer to the Gazebo tutorial at http://gazebosim.org/
tutorials/?tut=ros_urdf for a list of these elements and their usage.
Specifying color in Gazebo
The method of specifying link colors in rviz does not work in Gazebo since Gazebo 
has adopted OGRE's material scripts for coloring and texturing links. Therefore, a 
Gazebo <material> tag must be specified for each link. These tags can be placed in 
the model file just before the ending </robot> tag:
  <gazebo reference="base_link">
    <material>Gazebo/Blue</material>
  </gazebo>
  <gazebo reference="right_wheel">
    <material>Gazebo/Black</material>
  </gazebo>
  <gazebo reference="left_wheel">
    <material>Gazebo/Black</material>
  </gazebo>
A word about <visual> and <collision> elements in 
Gazebo
Gazebo will not use <visual> elements the same as <collision> elements if you 
do not explicitly specify them for each link. Instead, Gazebo will treat your link as 
invisible to laser scanners and collision checking. If your model ends up partially 
embedded in Gazebo's ground plane, you should check your <collision> elements.
Verifying a Gazebo model
The dd_robot URDF has been updated with the <gazebo> tags and 
<material> elements, as described earlier, and is stored in the downloaded file, 
dd_robot.gazebo. The .gazebo extension is used by the author to signify that this 
file is ready for use in Gazebo. An easy tool exists to check whether your URDF can 
be properly converted into an SDF. Simply run the following command:
$ gzsdf print dd_robot.gazebo
or
$ gzsdf print $(rospack find ros_robotics)/urdf/dd_robot.gazebo

Chapter 2
[ 65 ]
This command outputs the entire SDF to the screen so you may wish to redirect the 
output to a file. The output will show you the SDF that has been generated from 
your input URDF as well as any warnings about the missing information required to 
generate the SDF.
Viewing the URDF in Gazebo
Viewing the dd_robot model in Gazebo requires a launch file obtained or created by 
one of the following ways:
•	
Using the downloaded ddrobot_gazebo.launch file from the 
ros_robotics/launch directory from the book's website
•	
Creating the ddrobot_gazebo.launch file from the following XML code:
<launch>
  <!-- We resume the logic in gazebo_ros package 
empty_world.launch,  
         changing only the name of the world to be launched -->
  <include file="$(find gazebo_ros)/launch/empty_world.launch">
    <arg name="world_name" 
            value="$(find ros_robotics)/worlds/ddrobot.world"/>
    
    <arg name="paused" default="false"/>
    <arg name="use_sim_time" default="true"/>
    <arg name="gui" default="true"/>
    <arg name="headless" default="false"/>
    <arg name="debug" default="false"/>
  </include>
  <!-- Spawn dd_robot into Gazebo -->
  <node name="spawn_urdf" pkg="gazebo_ros"
        type="spawn_model" output="screen"
        args="-file $(find ros_robotics)/urdf/dd_robot.gazebo 
              -urdf -model ddrobot" />
</launch>
This launch file inherits most of the necessary functionality from 
empty_world.launch from the gazebo_ros package. The only parameter that is 
changed is the world_name parameter by substituting the ddrobot.world world 
file. In addition to this, our URDF-based dd_robot model is launched into Gazebo 
using the ROS spawn_model service from the gazebo_ros ROS node. If you plan to 
reuse this code or share it, it is recommended that you add the dependency to your 
package.xml file for the ros_robotics package. The following statement should be 
added under dependencies:
<exec_depend>gazebo_ros</exec_depend>

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 66 ]
The ddrobot.world world file contains a ground plane and two construction cones. 
This file can be found in the ros_robotics/worlds directory on the book's website, 
or you can create the ddrobot.world file from the following code:
<?xml version="1.0" ?>
<sdf version="1.4">
  <world name="default">
    <include>
      <uri>model://ground_plane</uri>
    </include>
    <include>
      <uri>model://sun</uri>
    </include>
    <include>
      <uri>model://construction_cone</uri>
      <name>construction_cone</name>
      <pose>-3.0 0 0 0 0 0</pose>
    </include>
    <include>
      <uri>model://construction_cone</uri>
      <name>construction_cone</name>
      <pose>3.0 0 0 0 0 0</pose>
    </include>
  </world>
</sdf>
The ddrobot_gazebo.launch file should be found in the /launch 
directory and ddrobot.world should be found in the /worlds 
directory of the ros_robotics ROS package.
Now we are ready to launch our dd_robot model in Gazebo by typing the 
following command:
$ roslaunch ros_robotics ddrobot_gazebo.launch
This command will launch both the Gazebo server and GUI client with the dd_robot 
model and world automatically launched inside the Gazebo environment. Gazebo 
will look similar to the following screenshot:

Chapter 2
[ 67 ]
dd_robot.gazebo in Gazebo
Tweaking your model
If your robot model behaves unexpectedly within Gazebo, it is likely because your 
model URDF needs further tuning to accurately represent its physics in Gazebo. 
Refer to the SDF user guide at http://sdformat.org/spec for more information on 
various properties available in Gazebo, which are also available in the URDF via the 
<gazebo> tag.
Moving your model around
To understand the physics of Gazebo, it is important to play with your model in 
Gazebo. Use the Selection, Translation, and Rotation modes on the Environment 
toolbar to move your model to different positions, and then watch how the gravity 
model works. If you are brave, you can even manipulate the environment to 
experiment with the relationship of the elements. For example, remove the 
ground plane and see what happens.

Creating Your First Two-Wheeled ROS Robot (in Simulation)
[ 68 ]
Simple joint control of our dd_robot model is possible by using the Joints panel, 
which is to the right of the center environment. In selection mode, click on the 
ddrobot model and the model will be highlighted with a white outline box. The 
joint_left_wheel and joint_right_wheel will appear under the tabbed sections with 
a value of 0.000 for each of the input windows. We will experiment by changing the 
values of the left and right wheel joints to see the dd_robot model move around on 
the ground plane. Values for Force and Position cause the robot to move; Velocity 
causes the robot to collapse. The following screenshot shows our dd_robot ready to 
be controlled via the Joints panel:
dd_robot.gazebo in Gazebo with the Joints panel
A greater control of our model can be achieved by adding transmission blocks to the 
URDF for the model joints. Gazebo plugins are also needed to simulate controllers 
that publish ROS messages for motor commands. A discussion of these advanced 
topics will be delayed until Chapter 5, Creating Your First Robot Arm (in Simulation) 
when the reader has a better understanding of ROS messages for control of mobile 
robots. Chapter 5, Creating Your First Robot Arm (in Simulation) will walk you through 
the construction of a URDF/SDF for a robot arm with a joint control implemented 
via Gazebo plugins. The implementation of transmission blocks and plugins for our 
dd_robot model is left as an exercise on completion of Chapter 5, Creating Your First 
Robot Arm (in Simulation).

Chapter 2
[ 69 ]
Other ROS simulation environments
Gazebo is only one simulator that can interface to ROS and ROS models. A list 
of other simulators, both open source and commercial, is provided along with a 
website reference:
•	
MATLAB with Simulink is a commercially available, multi-domain 
simulation and modeling design package for dynamic systems. 
It provides support for ROS through its Robotics System Toolbox 
(http://www.mathworks.com/hardware-support/robot-operating-
system.html).
•	
Stage is an open source 2D simulator for mobile robots and sensors 
(http://playerstage.sourceforge.net/doc/Stage-3.2.1/).
•	
Virtual Robot Experimentation Platform (V-REP) is a commercially 
available robot simulator with an integrated development environment. 
Developed by Coppelia Robotics, V-REP lends itself to many robotic 
applications (http://www.coppeliarobotics.com/).
Summary
Your first robot design has been a simple two-wheeled differential drive model 
defined in URDF. There are many other properties that can be defined in the 
URDF file, and you are free to extend the dd_robot model. This introductory 
exercise was provided so that the elements of simulation can be understood by 
the reader. In Chapter 3, Driving Around with TurtleBot, we will use a simulated 
and a real TurtleBot to explore a variety of ROS control methods for mobile robot 
navigation. The rqt toolset will be introduced and used to monitor and control the 
TurtleBot's movements.
In Chapter 5, Creating Your First Robot Arm (in Simulation), we will extend our 
understanding of URDF by learning more about Xacro. We will build a Xacro 
file to define a robot model for a robot arm.


[ 71 ]
Driving Around with TurtleBot
It is time for a real ROS robot! A robot called TurtleBot will be discussed and 
described both in simulation and as the real robot. In this chapter, you will learn how 
to move TurtleBot as a simulated robot and as the real robot. Even if you do not have 
a real TurtleBot, the examples in this chapter will teach you the techniques to control 
a mobile robot.
After introducing the TurtleBot, we will cover the following topics:
•	
Loading the TurtleBot simulation software and using Gazebo with TurtleBot
•	
Setting up your system to control a real TurtleBot from its own netbook 
computer or wirelessly from a remote computer
•	
Controlling the movement of the TurtleBot with ROS terminal commands or 
using the keyboard for control in teleoperation
•	
Creating a Python script which, when executed, moves the TurtleBot
•	
Using rqt tools to provide a GUI that aids the user in analyzing robot 
programs and also monitoring and controlling the robot
•	
Exploring an environment using TurtleBot's odometry data
•	
Executing the automatic docking program of TurtleBot

Driving Around with TurtleBot
[ 72 ]
Introducing TurtleBot
TurtleBot is a mobile robot that can be purchased as a kit or fully assembled. Several 
companies in North America and around the world sell TurtleBots. The TurtleBot2 
model is shown in the following image:
Turtlebot 2
A list of manufacturers can be found at: http://www.turtlebot.com/
manufacturers/
The main items that comprise the TurtleBot 2 model from bottom to top in the 
preceding image are as follows:
•	
A mobile base that also serves as support for the upper stages of the robot
•	
A netbook resting on a module plate
•	
Another module plate used to hold items
•	
A vision sensor with a color camera and 3D depth sensor
•	
The top most module plate used to hold items

Chapter 3
[ 73 ]
We will discuss the main items briefly here and provide more details for the base and 
the netbook later in this chapter. Overall, the TurtleBot model stands about 420 mm 
(16.5 inches) high and the base is approximately 355 mm (14 inches) in diameter.
The particular base in the authors' TurtleBot is a Kobuki mobile platform produced 
by the Yujin Robot company. TurtleBot rests on the floor on two wheels and a 
caster. The base is configured as a differential drive base, which means that when 
the TurtleBot is moving, the rotational velocity of the wheels can be controlled 
independently. So, for example, TurtleBot can move back and forth in a straight 
line when the wheels are driven in the same direction, clockwise (CW) or 
counterclockwise (CCW), with the same rotational velocity. If the wheels turn at 
different rotational velocities, TurtleBot can make turns as the velocity of the wheels 
is controlled. More details are available at: http://kobuki.yujinrobot.com/home-
en/about/.
A model of a differential drive robot was built in the Building a differential drive robot 
URDF section in Chapter 2, Creating Your First Two-Wheeled ROS Robot (in Simulation). 
The base has a battery pack and various power connections for accessories, including 
a USB connection and power plug for the netbook. TurtleBot also comes with a 
separate docking station for charging the Kobuki base.
The netbook is essentially a laptop computer but is lightweight with a small screen 
compared to many laptops. The netbook purchased with TurtleBot has Ubuntu and 
ROS packages installed. For a remote control, the netbook is connected via Wi-Fi to 
a network and a remote computer. There are USB ports used to plug in the vision 
sensor or other accessories. The setup of the network is described in the Networking 
the netbook and remote computer section of this chapter.
The vision sensor, as shown in the preceding image of TurtleBot2, is an Xbox 360 
Kinect sensor manufactured by Microsoft. Originally designed for video games, 
the Kinect sensor is a popular vision and depth sensor for robotics.
The ROS wiki has a series of tutorials that cover TurtleBot, including the 
Gazebo simulator, at http://wiki.ros.org/Robots/TurtleBot.

Driving Around with TurtleBot
[ 74 ]
Loading TurtleBot simulator software
This section deals with loading software packages for the TurtleBot simulator. The 
physical TurtleBot is not involved because these software packages are loaded on 
your laptop or desktop computer. It is assumed that Ubuntu 14.04 and ROS Indigo 
software are installed on the computer that you will use for the simulation. This 
installation is described in the Installing and launching ROS section in Chapter 1, 
Getting Started with ROS.
The procedure to load the TurtleBot software on your computer is documented 
in the tutorial at: http://wiki.ros.org/turtlebot/Tutorials/indigo/PC%20
Installation
The installation described on the website will apply to the real TurtleBot netbook 
also, but we will only consider the simulation here. In a terminal window, type the 
following command:
$ sudo apt-get install ros-indigo-turtlebot ros-indigo-turtlebot-apps 
ros-indigo-turtlebot-interactions ros-indigo-turtlebot-simulator ros-
indigo-kobuki-ftdi ros-indigo-rocon-remocon ros-indigo-rocon-qt-
library ros-indigo-ar-track-alvar-msgs
A large number of ROS packages are loaded by the sudo apt-get command. 
The groups are as follows:
•	
The TurtleBot software has ROS packages to simulate the TurtleBot and 
control the real TurtleBot. The TurtleBot simulator download includes the 
turtlebot_gazebo package.
•	
The Kobuki software consists of ROS packages used to drive or simulate the 
mobile base.
•	
The rocon (robots in concert) software is used for multirobot applications.
•	
The alvar software is used to track markers guiding a robot.
To view the TurtleBot packages downloaded in each category, type the 
following command:
$ rospack list | grep turtlebot
To see the packages that apply to the base, type the following command:
$ rospack list | grep kobuki
The other software can be viewed in a similar way.

Chapter 3
[ 75 ]
Launching TurtleBot simulator in Gazebo
The simulator package Gazebo was introduced in Chapter 2, Creating Your First 
Two-Wheeled ROS Robot (in Simulation). If you run the examples there using the 
differential drive robot, dd_robot, you should have a good understanding of 
Gazebo, including how to load models and worlds and manipulate the environment.
To run the simulator, you need to install the TurtleBot software, as described in the 
previous section. If you view the tutorials of TurtleBot Gazebo from the ROS wiki, 
make sure that you select Indigo since that should be your distribution of ROS.
To start the simulation, open a new terminal window and type the 
following command:
$ roslaunch turtlebot_gazebo turtlebot_world.launch
If all goes well, you will see a screenshot similar to this one:
TurtleBot simulated in Gazebo

Driving Around with TurtleBot
[ 76 ]
If you do not see Gazebo start, refer to the following Problems and troubleshooting 
section. If that is the case, there are a few issues that may help you if you are having 
problems with the simulation and the use of Gazebo. TurtleBot is in the approximate 
center of the world view, as seen from an overhead camera.
Right-click on the scene over TurtleBot, and from the menu that appears, select 
View and Joints to see the x, y, and z axes used by the TurtleBot simulator. Of 
course, TurtleBot will only move along its own x axis and rotate about its z axis. The 
following screenshot shows the result of zooming in on the TurtleBot and displaying 
the axes from choosing Joints:
TurtleBot simulated with joint axes shown
If you have modified the scene, you can take a picture of it by left-clicking on 
the camera icon on the toolbar. You can save the world using the Edit tab on the 
menu bar.
Roll the cursor over the symbols on the Gazebo toolbar to obtain their 
meaning.

Chapter 3
[ 77 ]
Problems and troubleshooting
The authors have tried their best to present the material in a clear manner so that you 
can follow along and achieve the same results. However, computers may differ in 
their abilities to run simulations that rely heavily on graphics, as Gazebo does.
We have run the code on relatively new laptops, older laptops, and on a powerful 
workstation. On a laptop, the response to commands from the keyboard may be 
slow, sometimes painfully so! Be patient: if the software is working, the TurtleBot 
will respond if commanded to move.
Some serious problems that may occur are as follows:
•	
On some older laptops, the hardware accelerator will not allow Gazebo to 
run, but this can be fixed by adding the following statement to .bashrc file, 
which disables the hardware accelerator:
export LIBGL_ALWAYS_SOFTWARE=1
•	
You may need to execute the roslaunch command several times if Gazebo 
fails to come up on the first or second try
•	
Sometimes, it is necessary to close all the windows and start over if the 
system does not respond
•	
If you execute the roscore or roslaunch command, and you have changed 
your ROS Master address using an export command to be linked to the 
TurtleBot network, as described later in this chapter, you may receive an 
error message similar to this:
ERROR: unable to contact ROS master at [http://<IP Address>:11311]
The traceback for the exception was written to the log file
It probably means that the ROS Master address is incorrect for your 
local machine. Usually, the problem can be fixed by issuing the 
following command:
$ export ROS_MASTER_URI=http://localhost:11311
This returns the ROS Master control to your local computer to run the 
simulator. You must run this export command in each new terminal 
window that is opened.
Check the results for these environment variables with the 
following command:
$ env | grep ROS
Make sure that the ROS_MASTER_URI variable points to the proper location.

Driving Around with TurtleBot
[ 78 ]
For more information on computer and network addresses, refer to the Networking 
the netbook and remote computer section in this chapter.
ROS commands and Gazebo
In the left-hand side pane of the Gazebo window, the list of models will appear 
when you click on Models. Notice, particularly, the mobile_base link. You can find 
the position and orientation of the base with the rosservice command. In a new 
terminal window, type the following command:
$ rosservice call gazebo/get_model_state '{model_name: mobile_base}'
The output of the preceding command is as follows:
pose:
  position:
    x: 0.00161336508139
    y: 0.0091790167961
    z: -0.00113098620237
  orientation:
    x: -5.20108036968e-05
    y: -0.00399736084462
    z: -0.0191615228716
    w: 0.999808408868
twist:
  linear:
    x: 9.00012388429e-06
    y: 6.54279879125e-05
    z: -1.4365465304e-05
  angular:
    x: -0.000449167550145
    y: 0.000197996689198
    z: -0.000470014447946
success: True
status_message: GetModelState: got properties

Chapter 3
[ 79 ]
Looking at the position and orientation, we can see that the TurtleBot base is 
approximately at the center (x=0, y=0, z=0) of the grid as you can see by zooming 
out in the world view. Since so many decimal places are shown, it appears that the 
TurtleBot is off center.
However, if you notice, the first two decimal places in the position are zeros, and you 
can see that the values are very small, near zero. The orientation is also near zero and 
is represented in a special notation called a quaternion.
To see the complete list of services, type the following command:
$ rosservice list
You can also use the rosnode list or rosmsg list ROS commands, as was shown 
in Chapter 1, Getting Started with ROS, to list the nodes or messages.
With ROS commands, you can move the TurtleBot as we did with the turtle in 
Turtlesim in Chapter 1, Getting Started with ROS. First, find the topic that will control 
the mobile_base link since that is the name given in Gazebo's left panel:
$ rostopic list | grep mobile_base
The output is as follows:
/mobile_base/commands/motor_power
/mobile_base/commands/reset_odometry
/mobile_base/commands/velocity
/mobile_base/events/bumper
/mobile_base/events/cliff
/mobile_base/sensors/bumper_pointcloud
/mobile_base/sensors/core
/mobile_base/sensors/imu_data
/mobile_base_nodelet_manager/bond
Now you can find the message type published by the rostopic /mobile_base/
commands/velocity that moves the base by typing the following command:
$ rostopic type /mobile_base/commands/velocity

Driving Around with TurtleBot
[ 80 ]
The output is as follows:
geometry_msgs/Twist
From the previously shown screen printout of the rosservice command to call 
gazebo/get_model_state, you can see that the twist is a six-dimensional value 
although all six need not be specified. The values represent velocities, which in the 
case of the TurtleBot represent the linear velocity along its forward x axis and the 
angular velocity about the vertical z axis. A reference is available at
https://en.wikipedia.org/wiki/Screw_theory.
If you drive the turtle with a command, the possible motions are linear along its x 
direction and angular rotation about the z axis since the TurtleBot moves on the xy 
plane and cannot fly. To drive it forward, run the following command:
$ rostopic pub -r 10 /mobile_base/commands/velocity 
\geometry_msgs/Twist '{linear: {x: 0.2}}'
Notice that the TurtleBot moves forward slowly until you stop it or it drives off the 
screen or it hits one of the objects in the environment. To stop its motion, press Ctrl 
+ C. To bring the TurtleBot back, change the value of x to x: -0.2 in the rostopic 
command and execute it.
If things go wrong, click on the Edit menu at the top of the 
Gazebo window and select Reset to reset the model pose.
There are many other features of Gazebo that can be explored, and you are 
encouraged to try various selections on the menu bar (File, Edit, View, Window, and 
Help). Also, you can open the rightmost third panel and change the values of Force, 
Position, or Velocity for the TurtleBot simulator.
Keyboard teleoperation of TurtleBot in 
simulation
A command to launch the teleop mode using the keyboard keys to move TurtleBot 
on the screen is as follows:
$ roslaunch turtlebot_teleop keyboard_teleop.launch

Chapter 3
[ 81 ]
This command allows keyboard keys to maneuver the TurtleBot on the screen. 
The keys to command the motion are as follows:
Control Your Turtlebot!
---------------------------
Moving around:
   u    i    o
   j    k    l
   m    ,    .
q/z : increase/decrease max speeds by 10%
w/x : increase/decrease only linear speed by 10%
e/c : increase/decrease only angular speed by 10%
space key, k : force stop
anything else : stop smoothly
CTRL-C to quit
currently:  speed 0.2  turn 1
Think of the letter k as the center of TurtleBot looking down on it. Start with the 
letter i to move the TurtleBot straight ahead along its x axis and try the other keys.
Remember to click on the window in which you executed the roslaunch command 
to move TurtleBot. This is termed focusing on the window.
The reference for TurtleBot teleoperation is found at:
http://wiki.ros.org/turtlebot_gazebo/Tutorials/indigo/Explore%20the%2
0Gazebo%20world
For now, we leave Gazebo and concentrate on installing software to control the real 
TurtleBot. However, even if you do not have access to a real TurtleBot, many of the 
commands and scripts that will be presented can also be used with the simulated 
TurtleBot. In fact, ideally, the Gazebo simulation should reflect the motion of the 
real TurtleBot in its environment.
For example, we later present a Python script that moves the real TurtleBot forward. 
You can use the command to run the script with Gazebo also.

Driving Around with TurtleBot
[ 82 ]
Setting up to control a real TurtleBot
The TurtleBot system consists of the TurtleBot base and its netbook that rides along 
with the TurtleBot and a separate remote computer that is used to control the robot. 
The netbook and computer communicate wirelessly once a network connection is 
established. This section describes the setup of the system, including the network.
A brief overview of the steps to set up and test the TurtleBot are as follows:
1.	 Set up the netbook with Ubuntu 14.04 and ROS Indigo, and then load the 
TurtleBot software packages.
2.	 Set up the remote computer with similar software.
3.	 Test the TurtleBot in the standalone mode to assure proper operation.
4.	 Create the network of computers, being careful to define the TurtleBot 
netbook as the ROS Master to the remote computer.
5.	 Test the TurtleBot by communicating with commands wirelessly from the 
remote computer to the netbook of the TurtleBot.
The ROS wiki describes the installation of the ROS software for the TurtleBot 
netbook at:
http://wiki.ros.org/turtlebot/Tutorials/indigo/Turtlebot%20
Installation
With Ubuntu installed and the ros-indigo-desktop-full installation, the packages for 
the TurtleBot are installed with the following command:
$ sudo apt-get install ros-indigo-turtlebot ros-indigo-turtlebot-apps 
ros-indigo-turtlebot-interactions ros-indigo-turtlebot-simulator ros-
indigo-kobuki-ftdi ros-indigo-rocon-remocon ros-indigo-rocon-qt-
library ros-indigo-ar-track-alvar-msgs
To link the Kobuki base to the device folder of Ubuntu, visit:
http://wiki.ros.org/turtlebot/Tutorials/indigo/
Kobuki%20Base
To set up your netbook battery monitor for the TurtleBot, visit:
http://wiki.ros.org/turtlebot/Tutorials/indigo/
Netbook%20Battery%20Setup
There are many tutorials that cover TurtleBot. There is a website devoted 
to the TurtleBot at http://learn.turtlebot.com/, with many 
interesting tutorials that cover the various aspects of the TurtleBot with 
details of setup, testing, and applications.

Chapter 3
[ 83 ]
TurtleBot standalone test
Before we make an attempt to network the TurtleBot to a remote computer, it is wise 
to test the TurtleBot in the standalone mode to determine whether the software has 
been installed properly. Now disconnect the netbook from any networks. Once the 
TurtleBot and its netbook are powered up, you can test software by opening a new 
terminal window on the netbook and executing the following command:
$ roscore
This should respond with a screen output that ends with the following message:
started core service [/rosout]
If there are no errors indicated in the screen output the netbook is set up correctly 
with ROS.
After this, press Ctrl + C and close this terminal window. Open a new one to move 
the TurtleBot around, as was done previously in simulation. On the netbook, 
initialize the TurtleBot by typing the roslaunch command in the new window:
$ roslaunch turtlebot_bringup minimal.launch
Quite a bit of information is shown on the screen as the minimal launch proceeds, 
but most of this output is not of any concern now. The ROS Master is the netbook 
indicated by the following lines:
auto-starting new master
ROS_MASTER_URI=http://localhost:11311
This will launch roscore and initialize the TurtleBot for control when the movement 
commands are issued. The importance of the ROS_MASTER_URI variable will be 
explained when networking the TurtleBot is discussed.
To move the TurtleBot, open a new terminal window, and launch teleoperation by 
typing the following command:
$ roslaunch turtlebot_teleop keyboard_teleop.launch

Driving Around with TurtleBot
[ 84 ]
Among other things, you will see a diagram of the control keys used to control the 
robot on the screen:
Control Your Turtlebot!
---------------------------
Moving around:
   u    i    o
   j    k    l
   m    ,    .
These are the same keys as discussed previously for the TurtleBot simulator. If all 
goes well, the TurtleBot will move forward, backward, or turn according to the key 
pressed on the netbook.
Tutorials devoted to the TurtleBot can be found at:
http://learn.turtlebot.com/
Of course, controlling the TurtleBot from its netbook is not very satisfying. 
It is done only to see that the TurtleBot software is set up correctly. In the next 
section, we describe the setup of a network so that the robot can be controlled 
by a remote computer.
In particular, as discussed in the Using keyboard teleoperation to move TurtleBot section 
of this chapter, the keyboard keys of a remote computer are used to move TurtleBot 
after it is connected to a network.
Networking the netbook and remote 
computer
ROS has the ability to allow multiple computers to communicate and share nodes, 
topics, and services. In the case of TurtleBot, the netbook has limited capabilities for 
graphics applications, such as rviz. It is better to run rviz and other visualization 
software on a desktop computer or a powerful laptop, both of which will be called 
a remote computer here to distinguish it from the netbook that rides along with 
the TurtleBot.

Chapter 3
[ 85 ]
The approach is to designate one computer in the network to run the ROS Master 
identified by the ROS_MASTER_URI variable and launch the roscore process from 
that computer. The choice is to set up TurtleBot's netbook as the Master since many 
applications of the TurtleBot require autonomous motion without the intervention of 
the remote computer.
Any other remote computer on the network will have its own IP address as the ROS_
IP address but its ROS_MASTER_URI variable will be TurtletBot's netbook IP address.
Types of networks
Networks between computers can be set up in various ways. To link to the TurtleBot 
from a remote computer, there are several common ways to network wirelessly:
•	
Use a network with a server computer that allows access to the Internet 
with Wi-Fi access, as might be found in a university or any other 
large organization.
•	
Use a router that allows local communication via Wi-Fi between the netbook 
and the remote computer. This is commonly used when setting up a private 
network to connect devices to each other wirelessly and to the Internet.
The network system in an organization may have security limitations that cover 
computers that can access their network. It is best to check any such requirements. 
Also, many such networks have network addresses assigned by a server using 
Dynamic Host Configuration Protocol (DHCP), which means that the IP address of 
a computer connected to a network can change if the computer is disconnected from 
the network and then reconnected. If the IP address changes, it is important to assign 
the ROS Master address as the new IP address of the TurtleBot's netbook connected 
to the wireless network.
Network addresses
A network identifies each computer on the network in one of the several ways, but 
each computer connected to the network has a unique identity. If the computers 
communicate through the Internet, you can refer to any Internet-connected machine 
by its Internet Protocol (IP)address, which is a four-part number string (such as 
192.168.11.xxx) in which the first part identifies the specific network to 
which the machine is connected. Another way to refer to the computer is by its 
hostname, which is usually a text string that consists of the machine name and the 
domain name.

Driving Around with TurtleBot
[ 86 ]
You can determine the hostname of your computer with the hostname command 
and the username using the whoami command in the forms:
$ hostname
$ whoami
In a ROS network, the Master is designated by a URI used to identify the name of the 
Master on a network. For example, the ROS_MASTER_URI variable for the TurtleBot in 
the authors' laboratory has the following address:
ROS_MASTER_URI=http://192.168.11.123:11311
The IP address in this case is 192.168.11.123. The IP address of a computer on the 
network can be determined by the following Ubuntu command:
$ ifconfig
This command will list the communication properties of the computer. The screen 
output will typically show an Ethernet connection (eth0) if any, a local loopback 
address (lo), and the wireless IP address (wlan), which is designated as inet addr. 
The digits 11311 represent the port used by the ROS Master for communication on 
the computer.
The description of the ROS networking requirements can be viewed on the 
following websites:
•	
http://wiki.ros.org/ROS/NetworkSetup
•	
http://wiki.ros.org/ROS/Tutorials/MultipleMachines
There must be a connection between the machines. Using the IP addresses of the 
machines to identify the machines is sufficient. Only one machine in the network 
can be the Master.
ROS_IP and ROS_HOSTNAME are environment variables that set the declared network 
address of a ROS node. The convention is to use ROS_IP if you are specifying an IP 
address, and ROS_HOSTNAME if you are specifying a hostname. The ROS_HOSTNAME 
variable takes precedence over the ROS_IP variable.
The ROS_MASTER_URI, ROS_IP, and ROS_HOSTNAME variables are described in the 
tutorial at: http://wiki.ros.org/ROS/EnvironmentVariables

Chapter 3
[ 87 ]
In the case of TurtleBot, the ROS Master resides on the netbook and the netbook's 
IP address must be indicated to the remote computer. On the remote computer, the 
ROS_MASTER_URI variable must be set to the address of the netbook so that its nodes 
can register with the Master. Once that is done, the nodes can communicate with the 
Master and other nodes wirelessly.
Remote computer network setup
To link the remote computer and the TurtleBot's netbook, make sure that both the 
computers communicate on the same network. This may involve changing the 
network choice of the computers if there are several networks available.
For your setup of the remote computer, determine the IP addresses of the netbook 
and your remote computer using the ifconfig command. Your commands will use 
your specific addresses, and you will use the following commands:
$ export ROS_MASTER_URI=http://<IP address of TurtleBot>:11311
$ export ROS_IP=<IP address of remote computer>
We recommend that you add these commands to the .bashrc file so that the 
TurtleBot is the ROS Master every time you open a new window.
To be more specific, on our remote computer, we edited our .bashrc file and added 
the following commands to create these environment variables for the TurtleBot:
export ROS_MASTER_URI=http://192.168.11.123:11311
export ROS_IP=192.168.11.120
The ROS Master address points to the TurtleBot netbook, and the ROS_IP variable 
is the IP address of our remote computer's wireless card used in this example. 
The examples just shown using the network addresses were taken from the actual 
computers used in the authors' laboratory to run TurtleBot. Of course, your 
addresses will be different.
To check the variables on the remote computer, type the following command to 
check the IP addresses of the ROS Master and the remote computer:
$ env | grep ROS

Driving Around with TurtleBot
[ 88 ]
Netbook network setup
The netbook setup instructions are found at the ROS wiki location at:
http://wiki.ros.org/turtlebot/Tutorials/indigo/Network%20
Configuration
To set up the netbook addresses, you can type the following command at the 
netbook terminal window:
$ echo export ROS_MASTER_URI=http://<IP address of TurtleBot>:11311 
>> ~/.bashrc
$ echo export ROS_IP=<IP address of TurtleBot> >> ~/.bashrc
where <IP address of TurtleBot> is replaced with the IP address of the TurtleBot 
netbook, which is normally called the IP address of the TurtleBot. This sets the 
TurtleBot as the Master.
Secure Shell (SSH) connection
The Secure Shell (SSH) will be used to allow remote login to the TurtleBot's netbook 
from the remote computer. Check the SSH status with the following command:
$ sudo service ssh status
If the SSH service is not present, install it according to the instructions available at: 
http://learn.turtlebot.com/ in the Setting Up Networking section.
For the authors' TurtleBot netbook, our username is turtlebot. Your username can 
be found by running the following command:
$ whoami
To communicate with the TurtleBot, on the remote computer, type the ssh command 
in the following form and enter your TurtleBot password when prompted:
$ ssh <username>@<IP address of TurtleBot>

Chapter 3
[ 89 ]
Summary of network setup
In summary, to set up the communication between the TurtleBot and the remote 
computer to control the robot, check the following on both the computers:
•	
TurtleBot's netbook hosts the ROS Master with
ROS_MASTER_URI= http://<IP address of TurtleBot>:11311
and
ROS_IP=http://<IP address of TurtleBot>
•	
The remote computer has
ROS_MASTER_URI = http://<IP address of TurtleBot>:11311
and
ROS_IP==http://<IP address of remote computer>
The addresses here are assumed to be the addresses of the TurtleBot netbook and the 
remote computer on a wireless network.
Troubleshooting your network connection
Many problems in networking ROS occur because the IP addresses of the netbook 
and the remote computer are not set correctly. Perform the following steps:
1.	 Check the computers' network settings.
2.	 Make sure that the network is working by communicating with the server or 
router for the network.
3.	 Use the ifconfig and env | grep ROS commands to check whether the 
network addresses are set correctly.
4.	 If your network has DHCP, see if the assigned IP addresses have changed.
Some information about networks that may be helpful can be 
found at the following sites:
http://compnetworking.about.com/cs/
protocolsdhcp/g/bldef_dhcp.htm
http://compnetworking.about.com/od/
homenetworking/a/routernetworks.htm

Driving Around with TurtleBot
[ 90 ]
Testing the TurtleBot system
To test the system and the communication, perform the following steps:
1.	 Make sure that the TurtleBot base battery and the netbook battery are 
charged.
2.	 Plug in the netbook to the base and power up the base.
3.	 Turn on the netbook and log on using the netbook's password and then 
connect to your network.
4.	 Give the TurtleBot room to move without obstacles in the way.
5.	 Log on to the remote computer and start communicating with the TurtleBot 
through your network.
This procedure is used to command the robot from the remote computer by typing 
the ssh command at the remote computer terminal and entering the TurtleBot 
password. The first example in Using keyboard teleoperation to move TurtleBot section 
will allow you to control the TurtleBot using several keyboard keys.
To start communication with the TurtleBot from the remote computer, type the ssh 
command and enter the TurtleBot password when prompted:
$ ssh <username>@<IP address of TurtleBot>
(The output is deleted for brevity.)
As described earlier, our TurtleBot IP address is 192.168.11.123.
The window prompt will change to the window prompt of the TurtleBot 
netbook. Our netbook prompt is turtlebot@turtlebot-0428:~$ 
and has been left in the following command lines to identify where the 
commands are issued.
After the response, you can send commands to the TurtleBot by typing the 
following command:
turtlebot@turtlebot-0428:~$ roslaunch turtlebot_bringup 
minimal.launch
The output is as follows:
.
.
Checking log directory for disk usage. This may take awhile

Chapter 3
[ 91 ]
After a long list of parameters and nodes, you will see the ROS Master address. In 
our system, the output is as follows:
auto-starting new master
process[master]: started with pid [23426]
ROS_MASTER_URI=http://192.168.11.123:11311
This line of the output shows that the TurtleBot is the ROS Master. It is followed by a 
list of the processes running and other information.
To view the nodes that are active after the minimal launch, use the command:
$ rosnode list
TurtleBot hardware specifications
Before driving the real TurtleBot around, it would be useful to understand the 
capabilities of the robot in terms of its possible speed, turning capability, carrying 
capacity, and other such properties. With this information, you can plan the motion 
and speed of TurtleBot and design interesting applications. The specifications here 
are taken from the information provided for the Kobuki base by the Yujin 
Robot company. Their website for general information can be found at: 
http://kobuki.yujinrobot.com/home-en/documentation/get-started/
For specific information about the base, check out the specifications at: 
http://kobuki.yujinrobot.com/home-en/about/specifications/
The base has a rechargeable battery that powers the motors turning the wheels. 
The netbook has its own battery but is not charged when the TurtleBot is moving 
on its own. There are a number of sensors in the base.
In the previous examples of teleoperation, the TurtleBot linear speed in the forward 
or backward direction was 0.2 meters/second or 20 cm/second. That is a bit over 
1 foot per second and is probably fast enough for a robot moving in a room with 
obstacles in its way. The turning rate was 1 radian/second. Since there are 2π (6.28) 
radians in a circle, the TurtleBot will rotate completely around in about 6 seconds 
or so.
According to the manufacturer, Yujin Robot, the maximum values are as follows:
•	
The maximum linear speed is 70 cm/second (27.5 inches/second)
•	
The maximum angular velocity is 180 degrees/second or π radians/second
•	
The payload is 5 kg (11 pounds) on a hard floor

Driving Around with TurtleBot
[ 92 ]
Review the other functional and hardware specifications to familiarize yourself with 
TurtleBot and its capabilities and limitations. In our laboratory for safety reasons, 
we run the TurtleBot at a relatively slow speed compared to its maximum speed.
TurtleBot dashboard
In this section, it is assumed that you have established communication with 
the TurtleBot and can send commands to start minimal launch. First, view the 
TurtleBot's dashboard, which indicates its condition, as described on the site at: 
http://wiki.ros.org/turtlebot_dashboard.
In a new terminal window on the remote computer, type the following command to 
bring up the dashboard:
$ roslaunch turtlebot_dashboard turtlebot_dashboard.launch
The output is as follows:
... logging to /home/tlharmanphd/.ros/log/2dfad508-6602-11e5-8e83-
6c71d9a711bd/roslaunch-D125-43873-4550.log
Checking log directory for disk usage. This may take awhile.
Press Ctrl + C to interrupt.
The dashboard indicates the status of various systems of the TurtleBot:
•	
A diagnostic indicator, /rosout messages, and motor control (OFF/ON) in 
the upper-left corner of the screen
•	
Controls for two colored LEDs on the base that can be turned on or off
•	
Battery monitor indicators for the netbook and the Kobuki base
•	
Status of the power system, the motors, and the sensors
In the following Turtlebot (Kobuki) dashboard screenshot, we have clicked on 
the diagnostic icon in the far upper-left corner of the screen to bring up the status 
messages on the dashboard:

Chapter 3
[ 93 ]
Kobuki dashboard
If your netbook battery monitor does not work, check the directions in the Setting up 
to control a real TurtleBot section to select a proper battery for monitoring.
Move the real TurtleBot
There are a number of ways to move the TurtleBot using ROS. In this section, 
we present the following three methods:
•	
Using the keyboard
•	
Using ROS terminal window commands
•	
Using a Python script

Driving Around with TurtleBot
[ 94 ]
Using keyboard teleoperation to move 
TurtleBot
In a new terminal window, launch the TurtleBot keyboard teleop program on the 
remote computer:
$ roslaunch turtlebot_teleop keyboard_teleop.launch
The output is as follows:
... logging to /home/turtlebot/.ros/log/b662ab4a-c22e-11e5-b730-
6c71d9a711bd/roslaunch-turtlebot-0428-26633.log
Checking log directory for disk usage. This may take awhile.
Press Ctrl-C to interrupt
Done checking log file disk usage. Usage is <1GB.
started roslaunch server http://192.168.11.123:58861/
SUMMARY
========
PARAMETERS
 * /rosdistro: indigo
 * /rosversion: 1.11.16
 * /turtlebot_teleop_keyboard/scale_angular: 1.5
 * /turtlebot_teleop_keyboard/scale_linear: 0.5
NODES
  /
    turtlebot_teleop_keyboard (turtlebot_teleop/turtlebot_teleop_key)
ROS_MASTER_URI=http://192.168.11.123:11311
core service [/rosout] found
process[turtlebot_teleop_keyboard-1]: started with pid [26653]
Control Your Turtlebot!

Chapter 3
[ 95 ]
---------------------------
Moving around:
   u    i    o
   j    k    l
   m    ,    .
q/z : increase/decrease max speeds by 10%
w/x : increase/decrease only linear speed by 10%
e/c : increase/decrease only angular speed by 10%
space key, k : force stop
anything else : stop smoothly
CTRL-C to quit
currently:  speed 0.2  turn 1
We have left the entire output from the launch because it is useful to know the 
parameters and nodes involved when a package is launched. Make sure that the 
TurtleBot base and the netbook batteries are sufficiently charged so you can move 
the TurtleBot around to become familiar with its capabilities for straight-line motion 
and rotation. Now try the i key to move the TurtleBot forward or the , key to drive 
backward. The speed is 0.2 meters/second.
Using ROS commands to move TurtleBot 
around
There are a number of ways to control the TurtleBot movement other than using the 
keyboard. There are several ROS commands that are useful to move and monitor the 
TurtleBot in motion:
•	
rostopic pub is used to publish commands to move the TurtleBot
•	
rostopic echo is used to display the messages sent
After the TurtleBot has been brought up with the minimal launch command, the 
rostopic pub command can be used to move and turn the TurtleBot. To move the 
TurtleBot forward, issue this command from the remote computer:
$ rostopic pub -r 10 /mobile_base/commands/velocity 
\geometry_msgs/Twist '{linear: {x: 0.2}}'

Driving Around with TurtleBot
[ 96 ]
TurtleBot should move forward continuously at 0.2 meters/second until you press 
Ctrl + C while the focus is on the active window.
This command publishes (pub) the /mobile_base/commands/velocity topic at 
the rate of 10 times per second. The –r variable indicates that the rate is repeated. 
To send the message once, use -1 instead of –r.
To move the TurtleBot backward, issue the following command:
$ rostopic pub -r 20 /mobile_base/commands/velocity 
\geometry_msgs/Twist '{linear: {x: -0.2}}'
Always press Ctrl + C to stop TurtleBot.
To cause the robot to turn in a circle requires some forward velocity and angular 
velocity, which the following command shows:
$ rostopic pub -r 10 /mobile_base/commands/velocity 
\geometry_msgs/Twist '{linear: {x: 0.2}, angular: {x: 0, y: 0, z: 
1.0}}'
The linear speed is 0.2 meters/second and the rotation is 1.0 radian 
(about 57 degrees) per second.
To view the messages sent, type the following command in a separate 
terminal window:
$ rostopic echo /mobile_base/commands/velocity
The output is as follows:
  linear:
  x: 0.2
  y: 0.0
  z: 0.0
angular:
  x: 0.0
  y: 0.0
  z: 1.0
---

Chapter 3
[ 97 ]
The message repeats the linear velocity and angular rotation values being sent 10 
times a second. Use Ctrl + C to stop the display.
Writing your first Python script to control 
TurtleBot
We will present a simple Python script to move the TurtleBot in this section. The 
basic approach to create a script begins with a design. The design should detail the 
activity to be accomplished. For example, a script can command TurtleBot to move 
straight ahead, make several turns, and then stop. The next step is to determine 
the commands for TurtleBot to accomplish the tasks. Finally, a script is written and 
tested to see whether TurtleBot responds in the expected way. The remote computer 
will execute the Python script and TurtleBot will move as directed if the script is 
correctly written.
In terms of the TurtleBot commands that will be used, we can summarize the process 
as follows:
•	
Design the program outlining the activities of TurtleBot when the script 
is executed
•	
Determine the nodes, topics, and messages to be sent (published) or received 
(subscribed) from the TurtleBot during the activity
•	
Study the ROS Python tutorials and examples to determine the way to 
write Python statements that send or receive messages between the remote 
computer and the TurtleBot
There is a great deal of documentation describing ROS Python scripts. The statement 
structure is fixed for many operations. The site http://wiki.ros.org/rospy briefly 
describes rospy, which is called the ROS client library for Python. The purpose is 
to allow statements written in Python language to interface with ROS topics 
and services.
The site http://wiki.ros.org/rospy_tutorials contains a list of tutorials. At the 
top of the tutorial page, there is a choice of distributions of ROS, and Indigo is chosen 
for our discussions. A specific tutorial that describes many Python statements that 
are used in a typical script can be found at: http://wiki.ros.org/ROS/Tutorials/
WritingPublisherSubscriber(python)

Driving Around with TurtleBot
[ 98 ]
To find the nodes that are active after the keyboard_teleop.launch file is launched, 
type this command:
$ rosnode list
The output is as follows:
/app_manager
/bumper2pointcloud
/capability_server
/capability_server_nodelet_manager
/cmd_vel_mux
/diagnostic_aggregator
/interactions
/master
/mobile_base
/mobile_base_nodelet_manager
/robot_state_publisher
/rosout
/turtlebot_laptop_battery
/turtlebot_teleop_keyboard
/zeroconf/zeroconf
The nodes are described in the Kobuki tutorial that can be found at: 
http://wiki.ros.org/kobuki/Tutorials/Kobuki's%20Control%20System
According to the site, the mobile_base node listens for commands, such as velocity, 
and publishes sensor information. The cmd_vel_mux variable serves to multiplex 
commands to assure that only one velocity command at a time is relayed to the 
mobile base.
In the previous example, we used the rostopic pub command to publish the linear 
and angular geometry_msgs/Twist data in order to move the TurtleBot. The Python 
script that follows will accomplish essentially the same thing. The script will send a 
Twist message on the cmd_vel_mux/input/navi topic.

Chapter 3
[ 99 ]
A Python script will be created to move the TurtleBot forward in a simple example. 
If you are not very familiar with Python, it may be best to study and execute the 
example script and then refer to the ROS tutorials. The procedure to create an 
executable script on the remote computer is as follows:
1.	 Write the script with the required format for a ROS Python script using an 
ordinary text editor.
2.	 Give the script a name in the <name>.py format and save the script.
We have called our script ControlTurtleBot.py and saved it in our home directory.
To make the script executable, execute the Ubuntu command:
$ chmod +x ControlTurtleBot.py
Make sure that the TurtleBot is ready by running the minimal launch. Then, in a new 
terminal window, type this command:
$ python ControlTurtleBot.py
In our example, Ctrl + C is used to stop the TurtleBot. The comments in the script 
explain the statements. The tutorials listed previously give further details of Python 
scripts written using the ROS conventions:
#!/usr/bin/env python
# Execute as a python script  
# Set linear and angular values of TurtleBot's speed and turning.
import rospy      # Needed to create a ROS node
from geometry_msgs.msg import Twist    # Message that moves base
class ControlTurtleBot():
  def __init__(self):
    # ControlTurtleBot is the name of the node sent to the master
    rospy.init_node('ControlTurtleBot', anonymous=False)
    # Message to screen
    rospy.loginfo(" Press CTRL+c to stop TurtleBot")
    # Keys CNTL + c will stop script   
    rospy.on_shutdown(self.shutdown)
        
    # Publisher will send Twist message on topic 
    # cmd_vel_mux/input/navi
    
    self.cmd_vel = rospy.Publisher('cmd_vel_mux/input/navi', 
    Twist, queue_size=10)
   

Driving Around with TurtleBot
[ 100 ]
    # TurtleBot will receive the message 10 times per second. 
    rate = rospy.Rate(10);   
    # 10 Hz is fine as long as the processing does not exceed
    #   1/10 second.
    # Twist is geometry_msgs for linear and angular velocity
    move_cmd = Twist()
    # Linear speed in x in meters/second is + (forward) or 
    # - (backwards)
    move_cmd.linear.x = 0.3	
    # Modify this value to change speed
    # Turn at 0 radians/s
    move_cmd.angular.z = 0	
    # Modify this value to cause rotation rad/s
    # Loop and TurtleBot will move until you type CNTL+c
    while not rospy.is_shutdown():
    # publish Twist values to TurtleBot node /cmd_vel_mux
    self.cmd_vel.publish(move_cmd)
    # wait for 0.1 seconds (10 HZ) and publish again
    rate.sleep()
    
    
  def shutdown(self):
    # You can stop turtlebot by publishing an empty Twist message 
    rospy.loginfo("Stopping TurtleBot")
    self.cmd_vel.publish(Twist())
    # Give TurtleBot time to stop
    rospy.sleep(1)
 
if __name__ == '__main__':
  try:
    ControlTurtleBot()
  except:
    rospy.loginfo("End of the trip for TurtleBot")
Introducing rqt tools
The rqt tools (ROS Qt GUI toolkit) that are part of ROS allow graphical 
representations of ROS nodes, topics, messages, and other information. The ROS 
wiki lists many of the possible tools that are added to the rqt screen as plugins: 
http://wiki.ros.org/rqt/Plugins.

Chapter 3
[ 101 ]
The ROS tutorial on topics also describes some of the features of the rqt tools at: 
http://wiki.ros.org/ROS/Tutorials/UnderstandingTopics
rqt_graph
One of the common uses of rqt is to view the nodes and topics that are active. Bring 
the TurtleBot up with minimal launch as previously described. Then, on the remote 
computer, issue the following command:
$ rqt_graph
Select the top-left box, Nodes/Topics (all). The following screenshot of rqt_graph 
shows the nodes that are active and the connections between the publishers and 
subscribers that deal with moving the base of the TurtleBot. Pass the cursor over 
the various items to see the nodes and topics and see how they communicate:
rqt_graph after minimal launch of TurtleBot

Driving Around with TurtleBot
[ 102 ]
On the menu bar at the top of the screen, keep the dead sinks, leaf topics, and debug 
topics hidden to simplify the graph. Take a look at the preceding graph; the names, 
such as master and mobile_base are called namespaces to identify the items. Ellipses 
(ovals) represent nodes while arrows represent connections through topics. The 
names in the rectangles represent topics.
For example, the /mobile_base_nodelet_manager node publishes on the 
/joint_states topic.
After the minimal launch, the keyboard_teleop.launch command is issued in a 
separate terminal window, as described earlier. When the following command 
is issued:
$ roslaunch turtlebot_teleop keyboard_teleop.launch
One of the screen outputs shows the name of the node that the turtlebot_teleop 
package is using as follows:
NODES
  /    turtlebot_teleop_keyboard (turtlebot_teleop/turtlebot_teleop_key)
The following screenshot portion shows selected nodes and topics after the launch 
of keyboard_teleop.launch. As shown in the following screenshot, a new 
turtlebot_teleop_keyboard node has appeared, publishing on the 
/cmd_vel_mux/input/teleop topic:
rqt_graph after TurtleBot teleoperation

Chapter 3
[ 103 ]
To list all the active nodes, the rosnode list command, on the remote computer, 
can be issued. For details on a particular node, such as /turtlebot_teleop_
keyboard, type the following command:
$ rosnode info /turtlebot_teleop_keyboard
The output is as follows:
--------------------------------------------------------------------------------
Node [/turtlebot_teleop_keyboard]
Publications:
 * /rosout [rosgraph_msgs/Log]
 * /cmd_vel_mux/input/teleop [geometry_msgs/Twist]
Subscriptions: None
Services:
 * /turtlebot_teleop_keyboard/set_logger_level
 * /turtlebot_teleop_keyboard/get_loggers
contacting node http://192.168.11.120:46784/ ...
Pid: 12745
Connections:
 * topic: /cmd_vel_mux/input/teleop
    * to: /mobile_base_nodelet_manager
    * direction: outbound
    * transport: TCPROS
 * topic: /rosout
    * to: /rosout
    * direction: outbound
    * transport: TCPROS

Driving Around with TurtleBot
[ 104 ]
From the keyboard, the messages of the geometry_msgs/Twist type are sent when 
you press a key that moves TurtleBot as indicated by the node's publications.
rqt message publisher and topic monitor
There are a number of variations of the rqt command with options. The simplest 
command is as follows:
$ rqt
This brings up a display screen, as shown in the following screenshot. In the menu 
bar, there are drop-down menu items that allow you to make choices to perform the 
following steps:
1.	 Select the Plugins tab that will be displayed; in our screenshot, the Message 
Publisher and the Topic Monitor options were chosen from the Plugins tab.
2.	 Select the topics or other information for your plugins.
3.	 Rearrange the screen to suit your preferences if you choose more than 
one plugin.
The rqt command and the drop-down menu selections are shown in the 
following screenshot:
rqt command initial screen with plugin selections

Chapter 3
[ 105 ]
For the following screenshot of rqt, the selections are made in the following order:
1.	 Issue the rqt command.
2.	 From the Plugins tab, select Message Publisher under the Topics tab.
3.	 From the Plugins tab, select Topic Monitor under the Topics tab.
4.	 Choose to publish the Twist message to /cmd_vel_mux and see the 
message monitored.
5.	 Rearrange the plugins on the screen for convenient viewing:
Two rqt plugins to publish and monitor messages
After you make the selections, there will be two plugins on the screen. 
You can rearrange them by clicking on the undocking symbol () in the 
upper-right corner of the plugins screen and dragging the window of the 
Topic Monitor below that of the Message Publisher.

Driving Around with TurtleBot
[ 106 ]
Specifically, in the Topic entry box, type /cmd_vel_mux/input/teleop and click on 
the + button to add the topic. Left-click to check the topic's checkbox and right-click 
to expand the parameters of the message to see the angular and linear parameters of 
the Twist message. Note that the rate variable has been set to 10.00 from its original 
value of 1.00 so that the TurtleBot can move smoothly. When you click on the linear 
x or angular z variable, you can change the parameter under the column titled 
expression. Changing the values from 0.0 will cause the TurtleBot to move 
because the message will be published.
The result shows the Message Publisher and the Topic Monitor with the /cmd_vel_
mux/input/teleop topic selected. From the geometry_msgs package, the Twist 
message will be sent to the TurtleBot to move the robot.
From the screenshot showing the drop-down menu, it is clear that there are many 
options associated with the rqt tools. View the tutorials and try various options 
to experience the power of the rqt tools to allow you to control and monitor your 
robot's activities. One use is for debugging your scripts if the TurtleBot does not 
respond as expected, because you can monitor the messages sent to the TurtleBot.
TurtleBot's odometry
In this section, we explore the TurtleBot's odometry. The general definition of 
odometry is the use of data from motion sensors to estimate change in position over 
time. Odometry is used by the TurtleBot to estimate its position and orientation 
relative to a starting location given in terms of an x and y position and an orientation 
around the z (upward) axis. The topic is /odom and the command to view the form of 
the /odom message is as follows:
$ rostopic echo /odom
When you execute this echo command, the output will be updated continuously on 
the screen. However, we wish to display TurtleBot's motion using rviz. When the 
odometry option is chosen in rviz, the TurtleBot's position and orientation will be 
displayed with arrows that are generated as TurtleBot moves.

Chapter 3
[ 107 ]
Odom for the simulated TurtleBot
The simulated TurtleBot will be used to demonstrate the odometry display possible 
in rviz.
To run Gazebo on your remote computer, you must reassign 
the ROS Master if it is assigned to the TurtleBot in the 
.bashrc file. In each terminal window you open, type the 
following command:
$ export ROS_MASTER_URI=http://localhost:11311
The commands executed on the remote computer to start Gazebo for simulation and 
rviz for visualization are as follows:
$ roslaunch turtlebot_gazebo turtlebot_world.launch
In another terminal window, run this command:
$ roslaunch turtlebot_rviz_launchers view_robot.launch
Gazebo includes the physics of the robot and rviz allows a variety of visualization 
options. In particular, it is useful to show the pose of the robot as indicated by arrows 
that point in the direction of motion of the TurtleBot on the screen.
In rviz, it is necessary to choose several options to show the TurtleBot's 
odometry arrows on the screen. As shown in the following screenshot, 
we choose the following:
1.	 Under Global Options on the left side panel for Fixed Frame, 
change base_link or base_footprint to odom.
2.	 Click on Add, and select the By topic tab shown.
3.	 Choose Odometry and click on OK.

Driving Around with TurtleBot
[ 108 ]
4.	 On the left side panel, click on the small arrow to the left of Odometry to 
show the various options. The topic is odom and the screen will keep 100 
arrows that point to the direction of the simulated TurtleBot as it moves:
Selection of the odom topic in rviz showing a list of topics
5.	 Once these selections are made, the simulated TurtleBot will appear on 
the screen with an arrow pointing in its forward direction, as shown in the 
following screenshot:
Rviz showing odom arrow for initial position of simulated TurtleBot

Chapter 3
[ 109 ]
To track the motion of the simulated TurtleBot on the screen and display the arrows, 
we issue a movement command. Once the two screens are up for Gazebo and 
rviz, any commands to move the robot are possible, including the execution of a 
Python script. For example, in a third terminal window, issue one of the following 
commands to make the TurtleBot move in a circle on the screen:
$ rostopic pub -r 10 /cmd_vel_mux/input/teleop \geometry_msgs/Twist 
'{linear: {x: 0.1, y: 0, z: 0}, angular: {x: 0, y: 0, z: -0.5}}'
$ rostopic pub -r 10 /mobile_base/commands/velocity 
\geometry_msgs/Twist '{linear: {x: 0.1, y: 0, z: 0}, angular: {x: 0, 
y: 0, z: -0.5}}'
The result is the same in terms of the movement of the robot in our examples, but 
the /mobile_base/commands/velocity topic is used to control the mobile base as 
explained in the Kobuki tutorial at: http://wiki.ros.org/kobuki/Tutorials/
Kobuki's%20Control%20System.
The /cmd_vel_mux variable is used to multiplex velocity commands from different 
sources, such as the keyboard or a Python script. Either commands make the 
TurtleBot move in a circle with the result shown in the following screenshot:
Simulated TurtleBot moving in a circle with the direction shown in rviz

Driving Around with TurtleBot
[ 110 ]
Real TurtleBot's odometry display in rviz
The commands used in simulation can be used with the physical TurtleBot. 
After bringing up the real TurtleBot with the minimal launch, start rviz on 
the remote computer:
$ roslaunch turtlebot_rviz_launchers view_robot.launch
TurtleBot will appear in rviz, as shown in the following screenshot:
TurtleBot on rviz bringup
Then, set up rviz with odom for Fixed Frame and navigate to Add | By topic | 
Odometry, as was done with the simulated TurtleBot.
Run the following command to move TurtleBot in a circle:
$ rostopic pub -r 10 /mobile_base/commands/velocity 
\geometry_msgs/Twist '{linear: {x: 0.1, y: 0, z: 0}, angular: {x: 0, 
y: 0, z: -0.5}}'
Stop the TurtleBot by pressing Ctrl + C with focus on the window in which you 
executed the command to move the robot.

Chapter 3
[ 111 ]
In the following screenshot, TurtleBot's turning was stopped by pressing Ctrl + C, 
and the Python script was executed that drives TurtleBot straight forward until the 
Ctrl + C keys are pressed again.
The command is as follows:
$ python ControlTurtleBot.py
TurtleBot's path after the Twist message and running of the Python script
TurtleBot automatic docking
The TurtleBot has the capability of finding its docking station and moving to that 
station for recharging as described in the tutorial available at: http://wiki.ros.
org/kobuki/Tutorials/Testing%20Automatic%20Docking.
According to the tutorial, the TurtleBot must be placed in line-of-sight of the docking 
station since the robot homes on the station using an infrared beam. The docking 
station will show a solid red light when it is powered up. If the TurtleBot finds the 
station and docks properly, the red light will turn to blinking green when charging 
and solid green when TurtleBot's battery is fully charged.

Driving Around with TurtleBot
[ 112 ]
Make sure that the minimal launch is active and the TurtleBot is within the line-of-
sight to the docking station. On the remote computer, type the following command:
$ roslaunch kobuki_auto_docking minimal.launch
Then, in another terminal window, type the following command to cause the 
TurtleBot to start the search for the docking station:
$roslaunch kobuki_auto_docking activate.launch
The following screenshot shows the TurtleBot rotating to find the IR signal and then 
heading toward the dock:
TurtleBot docking
In the preceding screenshot, the distance that the TurtleBot moved was about 
2 meters (about 6 feet) to find the docking station and start recharging the base 
battery. The screen output as TurtleBot was completing the docking is as follows:
Feedback: [DockDrive: DOCKED_IN]:
Feedback: [DockDrive: DOCKED_IN]:
Result - [ActionServer: SUCCEEDED]: Arrived on docking station successfully.
[dock_drive_client_py-1] process has finished cleanly

Chapter 3
[ 113 ]
log file: /home/tlharmanphd/.ros/log/789b3ecc-6bca-11e5-b156-6c71d9a711bd/dock_
drive_client_py-1*.log
all processes on machine have died, roslaunch will exit
shutting down processing monitor...
... shutting down processing monitor complete
done
Summary
This chapter introduced the TurtleBot robot and described how to load the necessary 
software for TurtleBot. The Gazebo simulator was used to show the capability of 
ROS to control TurtleBot.
To control a real TurtleBot and allow it to roam autonomously, it is desirable to 
set up a wireless communication between a remote computer and the TurtleBot's 
netbook. The explanation given in this chapter will allow you to set up the network 
and remotely control TurtleBot.
The various methods to control TurtleBot were presented. Teleoperation from the 
remote computer is one of the common methods used to control the robot's motion. 
A Python script was shown, which, when executed, will make the TurtleBot move 
in a straight line. This chapter also covered the use of rqt tools to send commands to 
TurtleBot and monitor them.
An important aspect of this chapter is that the TurtleBot can be controlled in 
simulation or in a real environment with the same commands and scripts. This use of 
a simulator can save much time in planning, testing, and debugging the applications 
for TurtleBot before the real robot is turned loose.
Odometry for the TurtleBot was described for the simulated TurtleBot and the real 
TurtleBot using rviz for visualizing the robot's motion. Finally, the auto-docking 
feature of TurtleBot was demonstrated.
The next chapter explains TurtleBot's use of the vision sensor. The chapter shows 
in detail how to create a map for TurtleBot and enable it to autonomously navigate 
around its environment.


[ 115 ]
Navigating the 
World with TurtleBot
In the previous chapter, the TurtleBot robot was described as a two-wheeled 
differential drive robot developed by Willow Garage. The setup of the TurtleBot 
hardware, netbook, network system, and remote computer were explained, so the 
user could set up and operate his own TurtleBot. Then, the TurtleBot was driven 
around using the keyboard control, command-line control, and a Python script.
In this chapter, we will expand TurtleBot's capabilities by giving the robot vision. 
The chapter begins by describing 3D vision systems and how they are used to map 
obstacles within the camera's field of view. The three types of 3D sensors typically 
used for TurtleBot are shown and described, detailing their specifications.
Setting up the 3D sensor for use on TurtleBot is described and the configuration is 
tested in a standalone mode. To visualize the sensor data coming from TurtleBot, 
two ROS tools are utilized: Image Viewer and rviz. Then, an important aspect of 
TurtleBot is described and realized: navigation. TurtleBot will be driven around and 
the vision system will be used to build a map of the environment. The map is loaded 
into rviz and used to give the user point and click control of TurtleBot so that it can 
autonomously navigate to a location selected on the map.
In this chapter, you will learn the following topics:
•	
How 3D vision sensors work
•	
The difference between the three primary 3D sensors for TurtleBot
•	
Information on TurtleBot environmental variables and the ROS software 
required for the sensors
•	
ROS tools for the rgb and depth camera output

Navigating the World with TurtleBot
[ 116 ]
•	
How to use TurtleBot to map a room using Simultaneous Localization and 
Mapping (SLAM)
•	
How to operate TurtleBot in autonomous navigation mode by adaptive 
monte carlo localization (amcl)
3D vision systems for TurtleBot
TurtleBot's capability is greatly enhanced by the addition of a 3D vision sensor. The 
function of 3D sensors is to map the environment around the robot by discovering 
nearby objects that are either stationary or moving. The mapping function must 
be accomplished in real time so that the robot can move around the environment, 
evaluate its path choices, and avoid obstacles. For autonomous vehicles, such as 
Google's self-driving cars, 3D mapping is accomplished by a high-cost LIDAR 
system that uses laser radar to illuminate its environment and analyze the reflected 
light. For our TurtleBot, we will present a number of low cost but effective options. 
These standard 3D sensors for TurtleBot include Kinect sensors, ASUS Xtion sensors, 
and Carmine sensors.
How these 3D vision sensors work
The 3D vision systems that we describe for TurtleBot have a common infrared 
technology to sense depth. This technology was developed by PrimeSense, an 
Israeli 3D sensing company and originally licensed to Microsoft in 2010 for the 
Kinect motion sensor used in the Xbox 360 gaming system. The depth camera uses 
an infrared projector to transmit beams that are reflected back to a monochrome 
Complementary Metal–Oxide–Semiconductor (CMOS) sensor that continuously 
captures image data. This data is converted into depth information, indicating the 
distance that each infrared beam has traveled. Data in x, y, and z distance is captured 
for each point measured from the sensor axis reference frame.
For a quick explanation of how 3D sensors work, the video How 
the Kinect Depth Sensor Works in 2 Minutes is worth watching at 
https://www.youtube.com/watch?v=uq9SEJxZiUg.

Chapter 4
[ 117 ]
This 3D sensor technology is primarily for use indoors and does not typically work 
well outdoors. Infrared from the sunlight has a negative effect on the quality of 
readings from the depth camera. Objects that are shiny or curved also present a 
challenge for the depth camera.
Comparison of 3D sensors
Currently, three manufacturers produce 3D vision sensors that have been integrated 
with the TurtleBot. Microsoft Kinect, ASUS Xtion, and PrimeSense Carmine have all 
been integrated with camera drivers that provide a ROS interface. The ROS packages 
that handle the processing for these 3D sensors will be described in an upcoming 
section, but first, a comparison of the three products is provided.
Microsoft Kinect
Kinect was developed by Microsoft as a motion sensing device for video games, but 
it works well as a mapping tool for TurtleBot. Kinect is equipped with a rgb camera, 
a depth camera, an array of microphones, and a tilt motor.
The rgb camera acquires 2D color images in the same way in which our smart 
phones or webcams acquire color video images. The Kinect microphones can be 
used to capture sound data and a 3-axis accelerometer can be used to find the 
orientation of the Kinect. These features hold the promise of exciting applications 
for TurtleBot, but unfortunately, this book will not delve into the use of these Kinect 
sensor capabilities.
Kinect is connected to the TurtleBot netbook through a USB 2.0 port (USB 3.0 for 
Kinect v2). Software development on Kinect can be done using the Kinect Software 
Develoment Kit (SDK), freenect, and OpenSource Computer Vision (OpenCV). 
The Kinect SDK was created by Microsoft to develop Kinect apps, but unfortunately, 
it only runs on Windows. OpenCV is an open source library of hundreds of 
computer vision algorithms and provides support for mostly 2D image processing. 
3D depth sensors, such as the Kinect, ASUS, and PrimeSense are supported in the 
VideoCapture class of OpenCV. Freenect packages and libraries are open source 
ROS software that provides support for Microsoft Kinect. More details on freenect 
will be provided in an upcoming section titled Configuring TurtleBot and installing 
3D sensor software.

Navigating the World with TurtleBot
[ 118 ]
Microsoft has developed three versions of Kinect to date: Kinect for Xbox 360, Kinect 
for Xbox One, and Kinect for Windows v2. The following table presents images of 
their variations and the subsequent table shows their specifications:
Microsoft Kinect versions
Microsoft Kinect version specifications:
Spec
Kinect 360
Kinect One
Kinect for 
Windows v2
Release date
November 2010
November 2013
July 2014
Horizontal field of view 
(degrees)
57
57
70
Vertical field of view 
(degrees)
43
43
60
Color camera data
640 x 480 32-bit @
30 fps
640 x 480 @ 30 fps
1920 x 1080 @
30 fps

Chapter 4
[ 119 ]
Spec
Kinect 360
Kinect One
Kinect for 
Windows v2
Depth camera data
320 x 240 16-bit @
30 fps
320 x 240 @ 30 fps
512 x 424 @ 30 fps
Depth range (meters)
1.2 – 3.5
0.5 – 4.5
0.5 – 4.5
Audio
16-bit @ 16 kHz
4 microphones
Dimensions
28 x 6.5 x 6.5 cm
25 x 6.5 x 6.5 cm
25 x 6.5.x 7.5 cm
Additional information
Motorized tilt base 
range ± 27 degrees; 
USB 2.0
Manual tilt base; 
USB 2.0
No tilt base; 
USB 3.0 only
Requires external power
fps: frames per second
ASUS
ASUS Xtion, Xtion PRO, and PRO LIVE are also 3D vision sensors designed 
for motion sensing applications. The technology is similar to the Kinect, 
using an infrared projector and a monochrome CMOS receptor to capture 
the depth information.
The ASUS sensor is connected to the TurtleBot netbook through a USB 2.0 port and 
no other external power is required. Applications for the ASUS Xtion PRO can be 
developed using the ASUS development solution software, OpenNi, and OpenCV 
for the PRO LIVE rgb camera. OpenNi packages and libraries are open source 
software that provides support for ASUS and PrimeSense 3D sensors. More details 
on OpenNi will be provided in the following Configuring TurtleBot and installing 3D 
sensor software section.

Navigating the World with TurtleBot
[ 120 ]
The following figure presents images of the ASUS sensor variations and the 
subsequent table shows their specifications:
ASUS Xtion and PRO versions
ASUS Xtion and PRO version specifications:
Spec
Xtion
Xtion PRO
Xtion PRO LIVE
Horizontal field of view 
(degrees)
58
58
58
Vertical field of view 
(degrees)
45
45
45
Color camera data 
none
none
1280 x 1024
Depth camera data
unspecified
640 x 480 @ 30 fps
320 x 240 @ 60 fps
640 x 480 @ 30 fps
320 x 240 @ 60 fps
Depth range (meters)
0.8 – 3.5
0.8 – 3.5
0.8 – 3.5
Audio
none
none
2 microphones
Dimensions
18 x 3.5 x 5 cm
18 x 3.5 x 5 cm
18 x 3.5 x 5 cm

Chapter 4
[ 121 ]
Spec
Xtion
Xtion PRO
Xtion PRO LIVE
Additional information
USB 2.0
USB 2.0
USB 2.0/ 3.0
No additional power required—powered through USB
PrimeSense Carmine
PrimeSense was the original developer of the 3D vision sensing technology using 
near-infrared light. They also developed the NiTE software that allows developers 
to analyze people, track their motions, and develop user interfaces based on gesture 
control. PrimeSense offered its own sensors, Carmine 1.08 and 1.09, to the market 
before the company was bought by Apple in November 2013. The Carmine sensor 
is shown in the following image. ROS OpenNi packages and libraries also support 
the PrimeSense Carmine sensors. More details on OpenNi will be provided in an 
upcoming section titled Configuring TurtleBot and installing 3D sensor software:
PrimeSense Carmine
PrimeSense has two versions of the Carmine sensor: 1.08 and the short range 1.09. 
The preceding image shows how the sensors look and the subsequent table shows 
their specifications:
Spec
Carmine 1.08
Carmine 1.09
Horizontal field of view (degrees) 57.5
57.5
Vertical field of view (degrees)
45
45
Color camera data
640 x 480 @ 30 Hz
640 x 480 @ 60 Hz
Depth camera data
640 x 480 @ 60 Hz
640 x 480 @ 60 Hz
Depth range (meters)
0.8 – 3.5
0.35 – 1.4
Audio
2 microphones
2 microphones
Dimensions
18 x 2.5 x 3.5 cm
18 x 3.5 x 5 cm
Additional information
USB 2.0 / 3.0
USB 2.0 / 3.0
No additional power required—powered through USB

Navigating the World with TurtleBot
[ 122 ]
TurtleBot uses 3D sensing for autonomous navigation and obstacle avoidance, 
as described later in this chapter. Other applications that these 3D sensors are 
used in include 3D motion capture, skeleton tracking, face recognition, and 
voice recognition.
Obstacle avoidance drawbacks
There are a few drawbacks that you need to know about when using the infrared 3D 
sensor technology for obstacle avoidance. These sensors have a narrow imaging area 
of about 58 degrees horizontal and 43 degrees vertical. It can also not detect anything 
within the first 0.5 meters (~20 inches). Highly reflective surfaces, such as metals, 
glass, or mirrors cannot be detected by the 3D vision sensors.
Configuring TurtleBot and installing the 
3D sensor software
There are minor but important environmental variables and software that are needed 
for the TurtleBot based on your selection of 3D sensors. We have attached a Kinect 
Xbox 360 sensor to our TurtleBot, but we will provide instructions to configure each 
of the 3D sensors mentioned in this chapter. These environmental variables are used 
by the ROS launch files to launch the correct camera drivers. In ROS Indigo, the 
Kinect and ASUS sensors are supported by different camera drivers, as described in 
the following sections.
Kinect
The environmental variables for the Kinect sensors are as follows:
export KINECT_DRIVER=freenect
export TURTLEBOT_3D_SENSOR=kinect
These variables should be added to the ~/.bashrc files of both the TurtleBot and the 
remote computer.

Chapter 4
[ 123 ]
Kinect also requires a special driver for the camera to be downloaded from GitHub. 
Type the following commands in a terminal window on the TurtleBot netbook:
$ mkdir ~/kinectdriver
$ cd ~/kinectdriver
$ git clone https://github.com/avin2/SensorKinect
$ cd SensorKinect/Bin/
$ tar xvjf SensorKinect093-Bin-Linux-x64-v5.1.2.1.tar.bz2
$ cd Sensor-Bin-Linux-x64-v5.1.2.1/
$ sudo ./install.sh
If your netbook has an 86-bit processor, change the fifth and sixth commands, 
as follows:
$ tar xvjf SensorKinect093-Bin-Linux-x86-v5.1.2.1.tar.bz2
$ cd Sensor-Bin-Linux-x86-v5.1.2.1/
Libfreenect is an open source library that provides an interface for Microsoft Kinect 
to be used with Linux, Windows, and Mac. ROS packages for Kinect 360 and Kinect 
One can be installed with the following commands:
$ sudo apt-get install libfreenect-dev
$ sudo apt-get install ros-indigo-freenect*
This command installs the following packages:
•	
  freeglut3-dev
•	
  ros-indigo-depth-image-proc
•	
  ros-indigo-eigen-conversions
•	
  ros-indigo-freenect-camera
•	
  ros-indigo-freenect-launch
•	
  ros-indigo-freenect-stack
•	
  ros-indigo-image-proc
•	
  ros-indigo-libfreenect
•	
  ros-indigo-rgbd-launch
Kinect for Windows v2 requires a different camera driver named libfreenect2 and 
the iai_kinect2 software toolkit. The installation of this software is described in 
Chapter 9, Flying a Mission with Crazyflie.

Navigating the World with TurtleBot
[ 124 ]
For the latest information on the ROS freenect software, check the ROS 
wiki at http://wiki.ros.org/freenect_launch. Maintainers of 
the freenect software utilize as much of the OpenNi software as possible 
to preserve compatibility.
ASUS and PrimeSense
The TurtleBot software for ROS Indigo is configured to working with the ASUS Xtion 
PRO as the default configuration. It is possible to add the following environmental 
variable:
export TURTLEBOT_3D_SENSOR=asus_xtion_pro
although (at this time) it is not necessary.
The openni2_camera ROS package supports the ASUS Xtion, Xtion PRO, and the 
PrimeSense 1.08 and 1.09 cameras. The openni2_camera package does not support 
any Kinect devices. This package provides drivers for the cameras to publish raw 
rgb, depth, and infrared image streams.
ROS packages for OpenNi2 can be installed with the following command:
$ sudo apt-get install ros-indigo-openni2*
For the latest information on the ROS OpenNi2 software, check the ROS 
wiki at http://wiki.ros.org/openni2_launch.
Camera software structure
The freenect_camera and openni_camera packages are ROS nodelet packages 
used to streamline the processing of the enormous quantity of image data. Initially, 
a nodelet manager is launched and then nodelets are added to the manager. 
The default 3D sensor data type for the freenect/openni nodelet processing is 
depth_image. The camera driver software publishes the depth_image message 
streams. These messages can be converted to point cloud data types to make them 
more usable for Point Cloud Library (PCL) algorithms. Basic navigation operations 
on TurtleBot use depth_images for faster processing. Launching nodelets to 
handle the conversion of raw depth, rgb, and IR data streams to the depth_image, 
disparity_image, and registered_point_cloud messages is the method of 
handling all the conversions in one process. Nodelets allow multiple algorithms to 
be running in a single process without creating multiple copies of the data when 
messages are passed between processes.

Chapter 4
[ 125 ]
The depthimage_to_laserscan package uses the depth_image data to create 
sensor_msgs/LaserScan in order to utilize more processing power to generate 
maps. For more complex applications, converting depth_images to the point cloud 
format offers the advantage of using the PCL algorithms.
Defining terms
The important terms that are used in configuring TurtleBot are as follows:
Depth cloud: Depth cloud is another name for the depth_image produced by the 3D 
sensor, such as the Kinect, ASUS, and PrimeSense depth cameras.
Point cloud: A point cloud is a set of points with x, y, and z coordinates that 
represent the surface of an object.
Registered DepthCloud and Registered PointCloud: These terms are used by ROS 
for special DepthCloud or PointCloud data colored by the rgb image data. These 
data streams are available when the depth_registration option is selected 
(set to true).
Testing the 3D sensor in standalone 
mode
Before we make an attempt to control the TurtleBot from a remote computer, it is 
wise to test the TurtleBot in standalone mode. TurtleBot will be powered on and 
we will use its netbook to check whether the robot is operational on its own.
To prepare the TurtleBot, perform the following steps:
1.	 Plug in the power to the 3D sensor via the TurtleBot base connection 
(Kinect only).
2.	 Plug in the power to the netbook via the TurtleBot base connection.
3.	 Power on the netbook and establish the network connection on the netbook. 
This should be the network used for TurtleBot's ROS_MASTER_URI IP address.
4.	 Power on the TurtleBot base.
5.	 Plug in the 3D sensor to the netbook through a USB 2.0 port (Kinect for 
Windows v2 uses the USB 3.0 port).
Ensure that ROS environment variables are configured correctly on the netbook. 
Refer to the Netbook network setup section in Chapter 3, Driving Around with TurtleBot, 
and the Configuring TurtleBot and installing 3D sensor software section.

Navigating the World with TurtleBot
[ 126 ]
To test the operation of the TurtleBot 3D sensor in standalone mode, perform the 
following steps on the netbook:
1.	 On the TurtleBot netbook, bring up a terminal window and run the TurtleBot 
minimal launch:
$ roslaunch turtlebot_bringup minimal.launch
2.	 Open another terminal window and start the camera nodelets for Kinect:
$ roslaunch freenect_launch freenect.launch
If you are using an ASUS or Carmine sensor, start the camera nodelets using 
the following command:
$ roslaunch openni2_launch openni2.launch
If these commands run on TurtleBot with no errors, you are ready to proceed with 
running 3D visualizations from the remote computer. If you receive errors, such as 
No devices connected…, make sure that the correct camera drivers are installed, as 
described in the Configuring TurtleBot and installing 3D sensor software section. Also, 
make sure that the TurtleBot base is powered on.
Running ROS nodes for visualization
Viewing images on the remote computer is the next step to setting up the TurtleBot. 
Two ROS tools can be used to visualize the rgb and depth camera images. Image 
Viewer and rviz are used in the following sections to view the image streams 
published by the Kinect sensor.
Visual data using Image Viewer
A ROS node can allow us to view images that come from the rgb camera on Kinect. 
The camera_nodelet_manager node implements a basic camera capture program 
using OpenCV to handle publishing ROS image messages as a topic. This node 
publishes the camera images in the /camera namespace.
Three terminal windows will be required to launch the base and camera nodes on 
TurtleBot and launch the Image Viewer node on the remote computer. The steps 
are as follows:
1.	 Terminal Window 1: Minimal launch of TurtleBot:
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch turtlebot_bringup minimal.launch

Chapter 4
[ 127 ]
2.	 Terminal Window 2: Launch freenect camera:
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch freenect_launch freenect.launch
This freenect.launch file starts the camera_nodelet_manager node, 
which prepares to publish both the rgb and depth stream data. When the 
node is running, we can check the topics by executing the rostopic list 
command. The topic list shows the /camera namespace with multiple depth, 
depth_registered, ir, rectify_color, rectify_mono, and rgb topics.
3.	 To view the image messages, open a third terminal window and type the 
following command to bring up the Image Viewer:
$ rosrun image_view image_view image:=/camera/rgb/image_color
This command creates the /image_view node that opens a window, 
subscribes to the /camera/rgb/image_color topic, and displays the image 
messages. These image messages are published over the network from the 
TurtleBot to the remote computer (a workstation or laptop). If you want to 
save an image frame, you can right-click on the window and save the current 
image to your directory.
If you are using an ASUS sensor and openni2_launch, the 
/camera/rgb/image_color topic does not exist. Instead, 
use the /camera/rgb/image_raw topic.
An image view of a rgb image

Navigating the World with TurtleBot
[ 128 ]
4.	 To view depth camera images, press Ctrl + C keys to end the previous 
image_view process. Then, type the following command in the third 
terminal window:
$ rosrun image_view image_view image:=/camera/depth/image
A pop-up window for Image Viewer will appear on your screen:
An image view of a depth image
To close the Image Viewer and other windows, press Ctrl + C keys in each 
terminal window.
Visual data using rviz
To visualize the 3D sensor data from the TurtleBot using rviz, begin by launching the 
TurtleBot minimal launch software. Next, a second terminal window will be opened 
to start the launch software for the 3D sensor.
1.	 Terminal Window 1: Minimal launch of TurtleBot:
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch turtlebot_bringup minimal.launch
2.	 Terminal Window 2: Launch 3D sensor software:
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch turtlebot_bringup 3dsensor.launch

Chapter 4
[ 129 ]
The 3dsensor.launch file within the turtlebot_bringup package 
configures itself based on the TURTLEBOT_3D_SENSOR environment variable 
set by the user. Using this variable, it includes a custom Kinect or ASUS Xtion 
PRO launch.xml file that contains all of the unique camera and processing 
parameters set for that particular 3D sensor. The 3dsensor.launch file turns 
on all the sensor processing modules as the default. These modules include 
the following:
°°
rgb_processing
°°
ir_processing
°°
depth_processing
°°
depth_registered_processing
°°
disparity_processing
°°
disparity_registered_processing
°°
scan_processing
It is typically not desirable to generate so much sensor data for an 
application. The 3dsensor.launch file allows users to set arguments 
to minimize the amount of sensor data generated. Typically, TurtleBot 
applications only turn on the sensor data needed in order to minimize the 
amount of processing performed. This is done by setting these roslaunch 
arguments to false when data is not needed.
When the 3dsensor.launch file is executed, the turtlebot_bringup 
package launches a /camera_nodelet_manager node with multiple 
nodelets. Nodelets were described in the Camera software structure 
section. The following is a list of nodelets that are started:
NODES
  /camera/
    camera_nodelet_manager (nodelet/nodelet)
    debayer (nodelet/nodelet)
    depth_metric (nodelet/nodelet)
    depth_metric_rect (nodelet/nodelet)
    depth_points (nodelet/nodelet)
    depth_rectify_depth (nodelet/nodelet)
    depth_registered_rectify_depth (nodelet/nodelet)
    disparity_depth (nodelet/nodelet)
    disparity_registered_hw (nodelet/nodelet)

Navigating the World with TurtleBot
[ 130 ]
    disparity_registered_sw (nodelet/nodelet)
    driver (nodelet/nodelet)
    points_xyzrgb_hw_registered (nodelet/nodelet)
    points_xyzrgb_sw_registered (nodelet/nodelet)
    rectify_color (nodelet/nodelet)
    rectify_ir (nodelet/nodelet)
    rectify_mono (nodelet/nodelet)
    register_depth_rgb (nodelet/nodelet)
  /
    depthimage_to_laserscan (nodelet/nodelet)
Next, rviz is launched to allow us to see the various forms of visualization 
data. A third terminal window will be opened for the following command:
3.	 Terminal Window 3: View sensor data on rviz:
$ roslaunch turtlebot_rviz_launchers view_robot.launch
The turtlebot_rviz_launchers package provides the view_robot.launch 
file for bringing up rviz and is configured to visualize the TurtleBot and its 
sensor output.
Within rviz, the 3D sensor data can be displayed in many formats. If images 
are not visible in the environment window, set the Fixed Frame (under 
Global Options) on the Displays panel to /camera_link. Try checking 
the box for the Registered PointCloud and rotating the TurtleBot's screen 
environment in order to see what the Kinect is sensing. Then wait. Patience is 
required because displaying a point cloud involves a lot of processing power.
The following screenshot shows the rviz display of a Registered PointCloud 
image in our lab:

Chapter 4
[ 131 ]
 
A Registered PointCloud image
On the rviz Displays panel, the following sensors can be added and checked 
for display in the environment window:
°°
DepthCloud
°°
Registered DepthCloud
°°
Image
°°
LaserScan
°°
PointCloud
°°
Registered PointCloud

Navigating the World with TurtleBot
[ 132 ]
The following table describes the different types of image sensor displays available 
in rviz and the message types that they display:
Sensor name
Description
Messages used
Camera
This creates a new rendering window 
from the perspective of a camera and 
overlays the image on top of it.
sensor_msgs/Image,
sensor_msgs/CameraInfo
DepthCloud,
Registered 
DepthCloud
This displays point clouds based on 
depth maps.
sensor_msgs/Image
Image
This creates a new rendering window 
with an image. Unlike the camera 
display, this display does not use 
camera information.
sensor_msgs/Image
LaserScan
This shows data from a laser scan 
with different options for rendering 
modes, accumulation, and so on.
sensor_msgs/LaserScan
Map
This displays an occupancy grid on 
the ground plane.
nav_msgs/OccupancyGrid
PointCloud,
PointCloud2,
and Registered 
PointCloud
This shows data from a point cloud 
with different options for rendering 
modes, accumulation, and so on.
sensor_msgs/PointCloud,
sensor_msgs/PointCloud2
Navigating with TurtleBot
Launch files for TurtleBot will create ROS nodes either remotely on the TurtleBot 
netbook (via SSH to TurtleBot) or locally on the remote computer. As a general rule, 
the launch files (and nodes) that handle the GUI and visualization processing should 
run on the remote computer while the minimal launch and camera drivers should 
run on the TurtleBot netbook. Note that we will specify when to SSH to TurtleBot for 
a ROS command or omit the SSH for using a ROS command on the remote computer.

Chapter 4
[ 133 ]
Mapping a room with TurtleBot
TurtleBot can autonomously drive around its environment if a map is made of the 
environment. The 3D sensor is used to create a 2D map of the room as the TurtleBot 
is driven around either by a joystick, keyboard, or any other method of teleoperation.
Since we are using the Kobuki base, calibration of the gyro inside the base is 
not necessary. If you are using the Create base, make sure that you perform the 
gyro calibration procedure in the TurtleBot ROS wiki at http://wiki.ros.org/
turtlebot_calibration/Tutorials/Calibrate%20Odometry%20and%20Gyro 
before you begin with the mapping operation.
Defining terms
The core terms that are used in TurtleBot navigation are as follows:
Odometry: Data gathered from moving sensors is used to estimate the change in a 
robot's position over time. This data is used to estimate the current position of the 
robot relative to its starting location.
Map: For TurtleBot, a map is a 2D representation of an environment encoded with 
occupancy data.
Occupancy Grid Map (OGM): An OGM is a map generated from the 3D sensor 
measurement data and the known pose of the robot. The environment is divided 
into an evenly-spaced grid in which the presence of obstacles is identified as a 
probabilistic value in each cell on the grid.
Localization: Localization determines the present position of the robot with respect 
to a known map. The robot uses features in the map to determine where its current 
position is on the map.

Navigating the World with TurtleBot
[ 134 ]
Building a map
The following steps are fairly complex and will require the use of four or five 
terminal windows. Be conscious of which commands are on TurtleBot (requiring ssh 
from the remote computer) and those that are on the remote computer (not requiring 
ssh). In each terminal window, enter the commands following the $ prompt:
1.	 Terminal window 1: Minimal launch of TurtleBot
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch turtlebot_bringup minimal.launch
These commands are the now familiar process of setting the many 
arguments and parameters and launching nodes for the TurtleBot 
mobile base functionality.
2.	 Terminal window 2: Launch the gmapping operation as follows:
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch turtlebot_navigation gmapping_demo.launch
Look for the following text on your window:
odom received!
The gmapping_demo launch file launches the 3dsensor.launch file, 
specifying turning off the rgb_processing, depth_registration, and 
depth_processing modules. This leaves the modules for ir_processing, 
disparity_processing, disparity_registered_processing, 
and scan_processing. The .xml files for gmapping.launch and 
move_base.launch are also invoked. gmapping.launch.xml launches 
the slam_gmapping node and sets multiple parameters in the .xml file. 
move_base.launch.xml launches the move_base node and also starts the 
nodes for velocity_smoother and safety_controller. A more complete 
description of this processing is provided in the following How does TurtleBot 
accomplish this mapping task? section.
3.	 Terminal window 3: View navigation on rviz by running the 
following command:
$ roslaunch turtlebot_rviz_launchers view_navigation.launch
Rviz should come up in the TopDownOrtho view identified in the 
Views panel on the right side of the screen. This environment shows 
a map that is the initial OGM, which shows occupied space, free space, 
and unknown space.

Chapter 4
[ 135 ]
If a map is not displayed, make sure that the following display checkboxes 
have been selected on the Displays panel (on the left side):
°°
Grid
°°
RobotModel
°°
LaserScan
°°
Bumper Hit
°°
Map
°°
Global Map
°°
Local Map
°°
Amcl Particle Swarm
°°
Full Plan
Your rviz screen should display results similar to the following screenshot:
An initial gmapping screen in rviz
4.	 Terminal window 4: Keyboard control of TurtleBot:
$ roslaunch turtlebot_teleop keyboard_teleop.launch

Navigating the World with TurtleBot
[ 136 ]
Here, the keyboard navigation command is used, but the joystick teleop 
or interactive marker navigation can be used instead.
At this point, the operator should use keyboard commands to navigate TurtleBot 
completely around the environment. A representation of the map is built and can be 
viewed in rviz as TurtleBot's 3D sensor detects objects within its range.
The following screenshot shows a map of our lab that TurtleBot produced on rviz:
TurtleBot mapping a room
Notice that light gray areas are clear, unoccupied space, dark gray areas are 
unexplored areas, black indicates a solid border, such as a wall, and colored spots 
are obstacles in the room. The area of the brightest color is TurtleBot's local map 
(the area the sensor is currently detecting).
When a complete map of the environment appears on rviz, the map should be saved. 
Without killing any of the prior processes, open another terminal window and type 
the following commands:
$ ssh <username>@<Turtlbot's IP Address>
$ rosrun map_server map_saver -f  /home/<TurtleBot's username>/my_map
If you do not know the TurtleBot's username, after ssh'ing to TurtleBot, use the pwd 
command to find it.

Chapter 4
[ 137 ]
The process creates two files: my_map.yaml and my_map.pgm and places them in your 
TurtleBot netbook home directory. The path and filename can be changed as you 
desire, but files should be saved on the TurtleBot.
The .yaml file contains configuration information of the map and the path and name 
of the .pgm image file. The .pgm file is in portable gray map format and contains the 
image of the OGM.
The map configuration information includes the following:
•	
The absolute pathname to the .pgm image file
•	
The map resolution in meters per pixel
•	
Coordinates (x, y, and yaw) of the origin on the lower-left corner of the grid
•	
A flag to reverse the white pixel=free and black pixel=occupied 
semantics of the map color space
•	
The lowest threshold value at which pixels will be considered 
completely occupied
•	
The highest threshold value at which pixels will be considered 
completely free
In the next section, we will examine TurtleBot's mapping process from a more in-
depth ROS perspective.
How does TurtleBot accomplish this mapping task?
TurtleBot builds maps using the ROS gmapping package. The gmapping package is 
based on OpenSlam's Gmapping (http://openslam.org/gmapping.html), which is 
a highly efficient Rao-Blackwellized particle filter algorithm. This approach is based 
on a laser scan-based SLAM implementation. Although a laser scanner would work 
the best for SLAM, the Kinect will provide a simulated laser scan for the TurtleBot. 
The ROS gmapping package contains the slam_gmapping node that takes the 
incoming laser scan stream and transforms it to the odometry tf reference frame.
The gmapping process is implemented by a set of parameters within the 
gmapping_demo.launch file in the turtlebot_navigation package. This launch 
file initiates the 3dsensor.launch file from the turtlebot_bringup package to 
handle the processing of the 3D sensor. Some of the sensor processing modules 
are turned off to minimize processing for this task.

Navigating the World with TurtleBot
[ 138 ]
The slam_gmapping node subscribes to the sensor_msgs/LaserScan messages from 
the camera_nodelet_manager node and the tf/tfMessage messages containing 
the odometry frames. The following diagram from rqt_graph shows the /tf and 
/tf_static topics (with tf/tfMessage messages) and the /scan topic (with 
sensor_msgs/LaserScan messages) being subscribed to by the slam_gmapping 
node. The slam_gmapping node combines this data to create an OGM of the 
environment. As the robot is driven around the room, the slam_gmapping node 
publishes the /map topic to update the OGM with an estimate of TurtleBot's 
location and the surrounding environment based on data from the laser scan.
slam_gmapping node
When the operator issues the command to save the map, the map_saver node of the 
map_server package gets activated. The map_saver node provides a ROS service to 
take the OGM data and saves it to a pair of files (the .pgm and .yaml files described 
in the previous section). Each cell of the OGM records its occupancy state as a color 
for the corresponding pixel. Free space is identified as white with a value of 0 and 
occupied space is identified as black with a value of 100. A special value of -1 is used 
for unknown (unmapped) space. The threshold values within the .yaml file make 
the pixel values between 0 and 100 categorized as occupied, free, or in-between.
Autonomous navigation with TurtleBot
ROS has implemented the concept of a Navigation Stack. ROS stacks are a collection 
of packages that provide a useful functionality, in this case navigation. Packages in 
the Navigation Stack handle the processing of odometry data and sensor streams 
into velocity commands for the robot base. As a differential drive base, TurtleBot 
takes advantage of the ROS Navigation Stack to perform tasks, such as autonomous 
navigation and obstacle avoidance. Therefore, understanding TurtleBot's navigation 
processes will provide the knowledge base for many other ROS mobile robots as well 
as a basic understanding of navigation for aerial and underwater robots.

Chapter 4
[ 139 ]
In this section, we will use the map that we created in the Mapping a room with 
TurtleBot section. As an alternative, you can use a bitmap image of a map of the 
environment, but you will need to build the .yaml file by hand. Values for map 
resolution, coordinates of the origin, and the threshold values will need to be 
selected. With the environment map loaded, we will command TurtleBot to move 
from its present location to a given location on the map defined as its goal.
At this point, understand that:
•	
TurtleBot is publishing odometry data and accepting velocity commands
•	
Kinect is publishing 3D sensor data (fake laser scan data)
•	
The tf library is maintaining the transformations between base_link, 
odom frame, and the depth sensor frame of Kinect
•	
Our map (my_map) will identify the environment locations that have obstacles
Defining terms
The following are the core terms used for autonomous navigation with TurtleBot:
Amcl: The amcl algorithm works to figure out where the robot would need to 
be on the map in order for its laser scans to make sense. Each possible location 
is represented by a particle. Particles with laser scans that do not match well are 
removed, resulting in a group of particles representing the location of the robot 
in the map. The amcl node uses the particle positions to compute and publish the 
transform from map to base_link.
Global navigation: These processes perform path planning for a robot to reach a 
goal on the map.
Local navigation: These processes perform path planning for a robot to create paths 
to nearby locations on a map and avoid obstacles.
Global costmap: This costmap keeps information for global navigation. Global 
costmap parameters control the global navigation behavior. These parameters are 
stored in global_costmap_params.yaml. Parameters common to global and local 
costmaps are stored in costmap_common_params.yaml.
Local costmap: This costmap keeps information for local navigation. Local 
costmap parameters control the local navigation behavior and are stored in 
local_costmap_params.yaml.

Navigating the World with TurtleBot
[ 140 ]
Driving without steering TurtleBot
To navigate the environment, TurtleBot needs a map, a localization module, and 
a path planning module. TurtleBot can safely and autonomously navigate the 
environment if the map completely and accurately defines the environment.
Before we begin with the steps for autonomous navigation, check the location of 
your .yaml and .pgm map files created in the previous section.
As in the previous section, be conscious of which commands are on TurtleBot 
(requiring ssh from the remote computer) and those that are on the remote computer 
(not requiring ssh). In each terminal window, enter the commands following 
the $ prompt:
1.	 Terminal Window 1: Minimal launch of TurtleBot:
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch turtlebot_bringup minimal.launch
2.	 Terminal Window 2: Launch amcl operation:
$ ssh <username>@<TurtleBot's IP Address>
$ roslaunch turtlebot_navigation amcl_demo.launch map_file:=/ 
home/<TurtleBot's username>/my_map.yaml
Look for the following text on your window:
odom received!
The amcl_demo launch file launches the 3dsensor.launch file, 
specifying to turning off the rgb_processing, depth_registration, and 
depth_processing modules. This leaves the modules for ir_processing, 
disparity_processing, disparity_registered_processing, and 
scan_processing. The map_server node is launched to read the map data 
from the file. The .xml files for amcl.launch and move_base.launch are 
also invoked. amcl.launch.xml launches the amcl node and processing 
sets multiple parameters in the .xml file. move_base.launch.xml launches 
the move_base node and also starts the nodes for velocity_smoother and 
safety_controller. A more complete description of this processing is 
provided in the following How does TurtleBot accomplish this navigation 
task? section.

Chapter 4
[ 141 ]
3.	 Terminal Window 3: View navigation on rviz:
$ roslaunch turtlebot_rviz_launchers view_navigation.launch
This command launches the rviz node and rviz will come up in the 
TopDownOrtho view. Your rviz screen should display results similar 
to the following screenshot:
An initial amcl screen in rviz
rviz control
When amcl_demo loads the map of the environment, TurtleBot does not know its 
current location on the map. It needs a little help. Locate TurtleBot's position in 
the rviz environment and let TurtleBot know this location by performing the 
following steps:
1.	 Click on the 2D Pose Estimate button on the tool toolbar at the top of the 
main screen.
2.	 Click on the location on the map where TurtleBot is located and drag the 
mouse in the direction TurtleBot is facing.

Navigating the World with TurtleBot
[ 142 ]
A giant green arrow will appear to help you align the direction of TurtleBot's 
orientation, as shown in the following screenshot:
TurtleBot 2D Pose Estimate
When the mouse button is released, a collection of small arrows will appear around 
TurtleBot to show the direction. If the location and/or orientation are not correct, 
these steps can be repeated.
The previous steps seed TurtleBot's localization, so it has some idea where it is on 
the environment map. To improve the accuracy of the localization, it is best to drive 
TurtleBot around a bit so that the estimate of its current position converges when 
comparing data from the map with TurtleBot's current sensor streams. Use one 
of the teleoperation methods previously discussed. Be careful driving around the 
environment because there is no obstacle avoidance software running at this point. 
TurtleBot can be driven into obstacles even though they appear on its map.

Chapter 4
[ 143 ]
Next, we can command TurtleBot to a new location and orientation in the room by 
identifying a goal:
1.	 Click on the 2D Nav Goal button on the tool toolbar at the top of the 
main screen.
2.	 Click on the location on the map where you want TurtleBot to go and drag 
the mouse in the direction TurtleBot should be facing when it is finished.
Warning: Try to avoid navigating near obstacles that have low 
protrusions that will not be detected by the 3D sensor. In our lab, the 
extensions at the base of the Baxter robot cannot be seen by the TurtleBot.
The following screenshot shows setting the navigation goal for our TurtleBot:
TurtleBot 2D Nav Goal

Navigating the World with TurtleBot
[ 144 ]
The following screenshot shows our TurtleBot accomplishing the goal:
TurtleBot reaches its goal
TurtleBot can also perform obstacle avoidance during autonomous navigation. 
While TurtleBot is on its way to a goal, step in front of it (at least 0.5 meters (1.6 feet) 
in front of the Kinect) and see that TurtleBot will move around you. Objects can be 
moved around or doors can be opened or closed to alter the environment. TurtleBot 
can also respond to the teleoperation control during this autonomous navigation.
In the next section, we will examine TurtleBot's autonomous navigation process from 
a more in-depth ROS perspective.
How does TurtleBot accomplish this navigation 
task?
At the highest level of processing, ROS navigation acquires odometry data from the 
robot base, 3D sensor data, and a goal robot pose. To accomplish the autonomous 
navigation task, safe velocity commands are sent to the robot to move it to the 
goal location.

Chapter 4
[ 145 ]
TurtleBot's navigation package, turtlebot_navigation, contains a collection 
of launch and YAML configuration files to launch nodes with the flexibility of 
modifying process parameters on-the-fly. The following diagram shows an 
overview of the navigation process:
The ROS navigation process
When the amcl node is launched, it begins providing localization information 
on the robot based on the current 3D sensor scans (sensor_msgs/LaserScan), 
tf transforms (tf/tfMessage), and the OGM (nav_msgs/OccupancyGrid). 
When a 2D Pose Estimate is input by the operator, an initialpose message 
(geometry_msgs/PoseWithCovaianceStamped) resets the localization parameter 
and reinitializes the amcl particle filter. As laser scans are read, amcl resolves the 
data to the odometry frame. The amcl node provides TurtleBot's estimated position 
in the map (geometry_msgs/PoseWithCovarianceStamped), a particle cloud 
(geometry_msgs/PoseArray), and the tf transforms for odom (tf/tfMessage).
The main component of the TurtleBot navigation is the move_base node. This node 
performs the task of commanding the TurtleBot to make an attempt to reach the 
goal location. This task is set as a preemptable action based on its implementation 
as a ROS action and TurtleBot's progress toward the goal is provided as feedback. 
The move_base node uses a global and a local planner to accomplish the task. Two 
costmaps, global_costmap and local_costmap, are also maintained for the planners 
by the move_base node.

Navigating the World with TurtleBot
[ 146 ]
The behavior of the move_base node relies on the following YAML files:
•	
costmap_common_params.yaml
•	
local_costmap_params.yaml
•	
global_costmap_params.yaml
•	
dwa_local_planner_params.yaml
•	
move_base_params.yaml
•	
global_planner_params.yaml
•	
navfn_global_planner_params.yaml
The global planner and costmap are used to create long-term plans over the entire 
environment, such as path planning for the robot to get to its goal. The local planner 
and costmap are primarily used for interim goals and obstacle avoidance.
The move_base node receives the goal information as a pose with 
position and orientation of the robot in relation to its reference frame. 
A move_base_msg/MoveBaseActionGoal message is used to specify the 
goal. The global planner will calculate a route from the robot's starting location 
to the goal taking into account data from the map. The 3D sensor will publish 
sensor_msgs/LaserScan with information on obstacles in the world to be avoided. 
The local planner will send navigation commands for TurtleBot to steer around 
objects even if they are not on the map. Navigation velocity commands are generated 
by the move_base node as geometry_msgs/Twist messages. TurtleBot's base will 
use the cmd_vel.linear.x, cmd_vel.linear.y, and cmd_vel.angular.z velocities 
for the base motors.
Goal tolerance is a parameter set by the user to specify the acceptable limit for 
achieving the goal pose. The move_base node will attempt certain recovery behaviors 
if TurtleBot is stuck and cannot proceed. These recovery behaviors include clearing 
out the supplied map and using sensor data by rotating in place.
rqt_reconfigure
The many parameters involved in TurtleBot navigation can be tweaked on-the-fly 
by using the rqt_reconfigure tool. This tool was previously named Dynamic 
Reconfigure and this name still appears on the screen. To activate this rqt plugin, 
use the following command:
$ rosrun rqt_reconfigure rqt_reconfigure

Chapter 4
[ 147 ]
Nodes that have been programmed using the rqt_reconfigure API will be visible 
on the rqt_reconfigure GUI. On the GUI, nodes can be selected and a window with 
the nodes' parameters will appear with the current values and range limits. Sliders 
and input boxes allow the user to enter new values that will dynamically overwrite the 
current values. The following screenshot shows configuration parameters that can be 
changed for the /camera/depth, /camera/depth_registered, and /camera/driver:
rqt_reconfigure camera parameters

Navigating the World with TurtleBot
[ 148 ]
The parameters for the move_base node control can be accessed through 
rqt_reconfigure. These parameters are set by the move_base_params.yaml file 
mentioned in the previous section. This screen identifies the base_global_planner 
and the base_local_planner as well as how often to update the planning process 
(planner_frequency) and so on. These parameters allow the operator to tweak the 
performance of the software during an operation.
rqt_reconfigure move_base parameters
Exploring ROS navigation further
The ROS wiki provides extensive information on all aspects of setting up and 
configuring the navigation parameters. The following links are provided to 
enhance your understanding:
•	
http://wiki.ros.org/navigation
•	
http://wiki.ros.org/navigation/Tutorials/RobotSetup
•	
http://wiki.ros.org/navigation/Tutorials/Navigation%20Tuning%20
Guide
The following book is worth reading to gain an understanding on amcl and 
robotic navigation:
Probabilistic Robotics by Thrum, Burgard, and Fox by MIT Press

Chapter 4
[ 149 ]
Summary
TurtleBot comes with its own 3D vision system that is a low-cost laser scanner. The 
Kinect, ASUS, or PrimeSense devices can be mounted on the TurtleBot base and 
provide a 3D depth view of the environment. This chapter provides a comparison of 
these three types of sensors and identifies the software that is needed to operate them 
as ROS components. We check their operation by testing the sensor on TurtleBot in 
standalone mode. To use the devices, we can utilize Image Viewer or rviz to view 
image streams from the rgb or depth cameras.
The primary objective is for TurtleBot to see its surroundings and be able to 
autonomously navigate through them. First, TurtleBot is driven around in 
teleoperation mode to create a map of the environment. The map provides the room 
boundaries and obstacles so that TurtleBot's navigation algorithm, amcl, can plan a 
path through the environment from its start location to a user-defined goal.
In the next chapter, we will return to the ROS simulation world and create a robot 
arm. The development of a URDF for a robotic arm and control of it in simulation 
will then prepare us to examine the robotic arms of Baxter in Chapter 6, Wobbling 
Robot Arms Using Joint Control. Using Baxter's robot arms, we will explore the 
complexities of multiple joint control and the mathematics of kinematic solutions 
for positioning multiple joints.


[ 151 ]
Creating Your First Robot 
Arm (in Simulation)
In this chapter, you will begin to understand the control of robot arms with ROS. We 
begin with a simple three-link, two-joint articulated robotic arm in simulation. The 
simulated robot arm, rrbot, has two revolute joints that will help you to understand, 
without the complexities of more joints. We will use the URDF elements described 
in Chapter 2, Creating Your First Two-Wheeled Robot (in Simulation) and incorporate 
the advantages of Xacro to make our code more modular and efficient. We will also 
include a mesh design for our gripper, and add control elements for the arm and 
gripper to our URDF. Next, we will show various ways to control the robot arm 
in Gazebo.
In this chapter, you will learn:
•	
The advantages of using Xacro in a URDF
•	
How to design a three-link, two-joint robotic arm using Xacro and mesh files
•	
How to control the arm in Gazebo using ROS commands and rqt
We begin by expanding your 3D modeling skills in order to create a robot arm URDF 
using Xacro. First, the advantages of Xacro will be described.

Creating Your First Robot Arm (in Simulation)
[ 152 ]
Features of Xacro
Xacro is the XML macro language for ROS. Xacro provides a set of macro operations 
to replace some repetitive statements with shorter, concise macros that will expand 
into full XML statements when processed. Xacro can be used with any XML 
document, but is most useful with long, complex URDF files. Xacro allows you to 
create shorter and more readable XML files for the robot URDF. Xacro provides 
advantages in many different areas:
•	
Property and property blocks: If repeated information is used in a URDF/
SDF file, the <property> tag can be used to specify these constant values in 
a central location. These are typically parameters that can be changed later. 
Properties are usually identified at the beginning of the file, but they can be 
found anywhere in the XML file at any level. It does not matter whether the 
property declaration is before or after its use.
Here is an example of how to implement a property:
°°
Define as <xacro:property name="my_name" value ="Robby" />
°°
Use as "${my_name}", which stands for the value of my_name and can 
be used to substitute text or values into an attribute
•	
Simple math: Math expressions can be constructed using the four basic 
operations: +, -, / , and *. Unary minus and parenthesis can also be used. 
The expression must be enclosed in the ${} construct. Numeric values are 
floating point numbers.
•	
Macros: This is the main feature of Xacro. When creating a macro, a simple 
<xacro> tag can expand into a statement or sequence of statements in the 
URDF/SDF file. Macros are extremely useful when statements are repeated 
or reused with modifications defined by parameters.
•	
Use of rospack commands: Xacro supports the use of rospack commands, 
just as roslaunch does for substitution arguments (args) (http://wiki.
ros.org/roslaunch/XML). Rospack commands enclosed within $() will be 
resolved during Xacro processing. For example, $(find ros_robotics) 
will find the relative pathname for the ros_robotics package. The $(arg 
var1) argument will be resolved to a value passed by an Xacro statement or 
the command line. Arguments passed via the command line must use the 
myvar:=true flag.
•	
Combining multiple Xacro files: Other Xacro files can be included in the 
main URDF file to allow you to modularize the URDF file into component 
files. The tag is as follows:
<xacro:include filename="path to filename/filename" />

Chapter 5
[ 153 ]
•	
Other features of Xacro can be found at http://wiki.ros.org/xacro. 
Additional features are part of the ROS Jade software release.
These features will be used in the URDF file for rrbot throughout this chapter. 
The order in which Xacro processes all these features is as follows:
1.	 Includes
2.	 Properties and property blocks
3.	 Macro definitions
4.	 Instantiation of macros
5.	 Expression evaluation
Expanding Xacro
In order to create the URDF file from Xacro files, the Xacro file must contain an 
XML namespace declaration using the xmlns attribute with the Xacro prefix and 
corresponding URI. Here is the XML namespace (xmlns) attribute for our rrbot 
robot arm:
<robot name="rrbot" 
xmlns:xacro="http://www.ros.org/wiki/xacro">
This declaration is vital for the file to parse properly. This statement appears as the 
second line in the main Xacro file, following the XML version reference.
To generate a URDF file, the xacro program (from the xacro package) expands all 
the macros and outputs a resulting URDF file. For example:
$ rosrun xacro xacro rrbot.xacro > rrbot.urdf
This command will pull together all of the xacro include files, expand the xacro 
macros in rrbot.xacro, and output the result to rrbot.urdf. This step is not 
necessary for running in rviz or Gazebo, but it can be a handy tool when used to 
examine the full URDF. The URDF XML file is generated with a heading comment 
warning that the file is autogenerated and editing of the file is not recommended.
The most common way to generate the URDF is in a launch file. The following line 
of code can be added to a launch file to create the most current robot_description 
from the Xacro file:
<param name="robot_description"
    command="$(find xacro)/xacro.py
      '$(find rrbot_description)/urdf/rrbot.xacro'" />

Creating Your First Robot Arm (in Simulation)
[ 154 ]
For complex robots, generating the URDF file at launch time will require a bit more 
time to process. The advantages are that the URDF is up to date and does not require 
a lot of memory to be stored.
Building an articulated robot arm URDF 
using Xacro
In the next few sections, our rrbot URDF will be created and incrementally built to 
incorporate the advantages of each of the Xacro features we discussed for our robot.
Using the Xacro property tag
For the first iteration of our rrbot robot arm, we will build a URDF file that defines 
three links with the <visual>, <collision>, and <inertial> tags, and two joints 
with the <parent>, <child>, <origin>, and <axis> tags. This is a very similar 
format to the dd_robot URDF file that you are familiar with from Chapter 2, Creating 
Your First Two-Wheeled ROS Robot (in Simulation). The differences for the Xacro 
format are listed here and explained in more detail after the code is presented:
•	
Addition of the XML namespace declaration on the second line
•	
Use of the Xacro <property> tag to define constant values
•	
Addition of property names instead of values within the <box> and 
<origin> tags
•	
Simple math (along with property names) to calculate link <origin> z values
•	
Joints for this arm are revolute and have the additional tags of <dynamics> 
and <limit>
The rrbot.xacro file can be downloaded from the Packt website, or you can enter 
the following code into your favorite editor:
<?xml version="1.0">
<!-- Revolute-Revolute Manipulator -->
<robot xmlns:xacro="http://www.ros.org/wiki/xacro" name="rrbot">
  <!-- Constants for robot dimensions -->
  <xacro:property name="width" value="0.1" />
  <xacro:property name="height1" value="2" />
  <xacro:property name="height2" value="1" />
  <xacro:property name="height3" value="1" />

Chapter 5
[ 155 ]
  <xacro:property name="axle_offset" value="0.05" />
  <xacro:property name="damp" value="0.7" />
  <!-- Base Link -->
  <link name="base_link">
    <visual>
      <origin xyz="0 0 ${height1/2}" rpy="0 0 0" />
      <geometry>
        <box size="${width} ${width} ${height1}" />
      </geometry>
    </visual>
    <collision>
      <origin xyz="0 0 ${height1/2}" rpy="0 0 0" />
      <geometry>
        <box size="${width} ${width} ${height1}" />
      </geometry>
    </collision>
    <inertial>
      <origin xyz="0 0 ${height1/2}" rpy="0 0 0" />
      <mass value="1" />
      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" 
izz="1.0" />
    </inertial>
  </link>
  <!-- Joint between Base Link and Middle Link -->
  <joint name="joint_base_mid" type="revolute">
    <parent link="base_link" />
    <child link="mid_link" />
    <origin xyz="0 ${width} ${height1 - axle_offset}" rpy="0 0  0" 
/>
    <axis xyz="0 1 0" />
    <dynamics damping="${damp}" />
    <limit effort="100.0" velocity="0.5" lower="-3.14" 
upper="3.14" />
  </joint>
  <!-- Middle Link -->
  <link name="mid_link">
    <visual>
      <origin xyz="0 0 ${height2/2 - axle_offset}" rpy="0 0 0" />
      <geometry>
        <box size="${width} ${width} ${height2}" />
      </geometry>

Creating Your First Robot Arm (in Simulation)
[ 156 ]
    </visual>
    <collision>
      <origin xyz="0 0 ${height2/2 - axle_offset}" rpy="0 0 0" />
      <geometry>
        <box size="${width} ${width} ${height2}" />
      </geometry>
    </collision>
    <inertial>
      <origin xyz="0 0 ${height2/2 - axle_offset}" rpy="0 0 0" />
      <mass value="1" />
      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" 
izz="1.0" />
    </inertial>
  </link>
  <!-- Joint between Middle Link and Top Link -->
  <joint name="joint_mid_top" type="revolute">
    <parent link="mid_link" />
    <child link="top_link" />
    <origin xyz="0 ${width} ${height2 - axle_offset*2}" rpy="0 0  
0" />
    <axis xyz="0 1 0" />
    <dynamics damping="${damp}" />
    <limit effort="100.0" velocity="0.5" lower="-3.14" 
upper="3.14" />
  </joint>
  <!-- Top Link -->
  <link name="top_link">
    <visual>
      <origin xyz="0 0 ${height3/2 - axle_offset}" rpy="0 0 0" />
      <geometry>
        <box size="${width} ${width} ${height3}" />
      </geometry>
    </visual>
    <collision>
      <origin xyz="0 0 ${height3/2 - axle_offset}" rpy="0 0 0" />
      <geometry>
        <box size="${width} ${width} ${height3}" />
      </geometry>
    </collision>
    <inertial>
      <origin xyz="0 0 ${height3/2 - axle_offset}" rpy="0 0 0" />
      <mass value="1" />
      <inertia ixx="1.0" ixy="0.0" ixz="0.0" iyy="1.0" iyz="0.0" 
izz="1.0" />
    </inertial>
  </link>
</robot>

Chapter 5
[ 157 ]
This file and all .xacro files should be saved in the /urdf directory of your 
ros_robotics package.
This XML code defines a robot arm, labeled rrbot, which has three links that are 
0.1 meters deep and wide. The base_link is 2 meters tall, and the mid_link and 
top_link are both 1 meter tall. The origin of the base_link is at (0, 0, 1) in order 
for the arm to be above the ground plane in rviz. The reference frame of the box is 
1 meter in the z direction above the reference frame of the link. This base_link is 
identified as the URDF root link and is the beginning of the kinematic chain for the 
arm. The collision elements of each match their visual elements. Each inertial element 
indicates that each link weighs 1 kilogram and has the same basic inertia matrix 
values. (We will utilize this duplication of the inertial element when we improve 
the code for rrbot later in this chapter.)
In rrbot.xacro, the <property> elements are used for constants for the dimensions 
of the links, the offset for the axis of rotation, and for the damping coefficient. These 
values are declared at the beginning of the XML code, making it easy to change the 
size of the links, the limits of the joints' rotation, or the damping characteristics of 
the arm.
Two new tags have been added to the <dynamics damping> and <limit> joint 
elements. The dynamics damping coefficient is set to 0.7 Nms/rad. This damping 
is the amount of opposing force to any joint velocity, used to slow down the moving 
arms to a rest position. Using the <property> element, this damping value can be 
changed in one location. You are encouraged to change the value when you run 
the rrbot URDF/SDF with Gazebo and see the changes made to the pendulum 
swinging motion. The <limit> tag is required for revolute joints. The limit effort is 
set to 1000.0 N-m for the maximum joint effort that can be commanded. The joint 
velocity is limited to a magnitude of 0.5 rad/s, with a damping effort applied when 
the joint is commanded beyond this limit. The upper and lower limits of the joint are 
set to pi radians and -pi radians, respectively.
Using roslaunch for rrbot
Modifications made to the launch file in rviz are necessary, but you will notice 
similarities to the ddrobot_rviz.launch file in Chapter 2, Creating Your First 
Two-Wheeled Robot (in Simulation). Either download the rrbot_rviz.launch file 
from the ros_robotics/launch directory on this book's website, or create the 
rrbot_rviz.launch file from the following code:
<launch>
  <!-- set parameter on Parameter Server -->
  <arg name="model" />
  <param name="robot_description"

Creating Your First Robot Arm (in Simulation)
[ 158 ]
    command="$(find xacro)/xacro.py
            '$(find ros_robotics)/urdf/$(arg model)'" />
  <!-- send joint values from gui -->
  <node name="joint_state_publisher" pkg="joint_state_publisher"
        type="joint_state_publisher">
    <param name="use_gui" value="TRUE"/>
  </node>
  <!-- use joint positions to update tf -->
  <node name="robot_state_publisher" pkg="robot_state_publisher"
        type="state_publisher"/>
  <!-- visualize robot model in 3D -->
  <node name="rviz" pkg="rviz" type="rviz" 
        args="-d $(find ros_robotics)/urdf.rviz" required="true" 
/>
</launch>
The main difference in this rviz launch file is executing xacro.py with the argument 
passed as the model parameter. This process will generate the robot_description 
parameter to be loaded on the Parameter Server. Make sure that you place this 
rrbot_rviz.launch file in the /launch directory of your ros_robotics directory.
If you have added the ROS_MASTER_URI and ROS_HOSTNAME or ROS_IP 
environment variables to your .bashrc file, you will need to comment 
out these lines with a # as the first character of the line. Add the following 
two commands to your .bashrc file to be able to execute the ROS 
commands used in this chapter:
export ROS_MASTER_URI=http://localhost:11311//
export ROS_HOSTNAME=localhost
Next, run your rviz roslaunch command:
$ roslaunch ros_robotics rrbot_rviz.launch model:=rrbot.xacro

Chapter 5
[ 159 ]
The rviz screen will look similar to the following screenshot:
rrbot.xacro in rviz
In the preceding screenshot, notice the pop-up window with two joint control sliders 
for joint_base_mid and joint_mid_top. The center (0.00) position for both the 
joints puts the arm in a vertical position. You are encouraged to play with the sliders 
and the Randomize and Center buttons to understand the controls for the arm joints.
Using Xacro include and macro tags
In the following increment of the rrbot Xacro file, we will use an Xacro <include> 
tag to specify colors for each link of the robot arm. The materials.xacro file must 
be in the ros_robotics/urdf directory. To create the materials.xacro file, use the 
following code:
<?xml version="1.0"?>
<robot>
  <material name="black">

Creating Your First Robot Arm (in Simulation)
[ 160 ]
    <color rgba="0.0 0.0 0.0 1.0"/>
  </material>
  <material name="blue">
    <color rgba="0.0 0.0 0.8 1.0"/>
  </material>
  <material name="green">
    <color rgba="0.0 1.0 0.0 1.0"/>
  </material>
  <material name="grey">
    <color rgba="0.2 0.2 0.2 1.0"/>
  </material>
  <material name="orange">
    <color rgba="${255/255} ${108/255} ${10/255} 1.0"/>
  </material>
  <material name="brown">
    <color rgba="${222/255} ${207/255} ${195/255} 1.0"/>
  </material>
  <material name="red">
    <color rgba="0.8 0.0 0.0 1.0"/>
  </material>
  <material name="white">
    <color rgba="1.0 1.0 1.0 1.0"/>
  </material>
</robot>
Color values can be modified to your preferences for colors and textures. Within 
each <visual> element of the arm link, the following code should be added with 
an appropriate color:
<material name="any color defined in materials.xacro"/>

Chapter 5
[ 161 ]
An Xacro <macro> block is also added to the rrbot Xacro file to replace the duplicate 
inertial elements in each link. The macro for <inertial> is as follows:
  <xacro:macro name="default_inertial" params="z_value i_value 
mass">
    <inertial>
      <origin xyz="0 0 ${z_value}" rpy="0 0 0"/>
      <mass value="${mass}" />
      <inertia ixx="${i_value}" ixy="0.0" ixz="0.0"
        iyy="${i_value}" iyz="0.0"
        izz="${i_value}" />
      </inertial>
  </xacro:macro>
Within each <link> of the arm, the entire <inertial> block is replaced with the 
following code:
  <xacro:default_inertial z_value="${computation for <origin> in 
  z-axis}" i_value="1.0" mass="1"/>
Study the following code to understand the computation for <origin> in z 
axis. This code is in the rrbot2.xacro file available for download on the book's 
website, or can be entered from the following lines: (lines from the previous code 
have been left in or omitted and new code has been highlighted):
<?xml version="1.0"?>
<!-- Revolute-Revolute Manipulator -->
…
  <!-- Import Rviz colors -->
  <xacro:include 
           filename="$(find ros_robotics)/urdf/materials.xacro" />
  <!-- Default Inertial -->
  <xacro:macro name="default_inertial" params="z_value i_value mass">
    <inertial>
      <origin xyz="0 0 ${z_value}" rpy="0 0 0"/>
      <mass value="${mass}" />
      <inertia ixx="${i_value}" ixy="0.0" ixz="0.0"
               iyy="${i_value}" iyz="0.0"

Creating Your First Robot Arm (in Simulation)
[ 162 ]
               izz="${i_value}" />
      </inertial>
  </xacro:macro>
  <!-- Base Link -->
    …
    </collision>
    <xacro:default_inertial z_value="${height1/2}"
                            i_value="1.0" mass="1"/>
  </link>
  …
  <!-- Middle Link -->
    …
    </collision>
    <xacro:default_inertial z_value="${height2/2 - axle_offset}"
    i_value="1.0" mass="1"/>
  </link>
  …
  <!-- Top Link -->
    …
    </collision>
    <xacro:default_inertial z_value="${height3/2 - axle_offset}"
      i_value="1.0" mass="1"/>  
  </link>
</robot>
Next, run the rviz roslaunch command:
$ roslaunch ros_robotics rrbot_rviz.launch model:=rrbot2.xacro

Chapter 5
[ 163 ]
The rviz screen will look similar to the following screenshot:
rrbot2.xacro in rviz
Although the book may show the screenshot in shades of gray, the base_link of the 
arm is now red, the mid_link is green, and the top_link is blue. On your screen, 
you will see the colors specified in your rrbot2.xacro file. The arm is shown in a 
random pose.
Adding mesh to the robot arm
A mesh is a collection of polygon surfaces that provides a more realistic shape for 
an object in 3D. Although adding a mesh to the URDF is not specific to Xacro, we 
include the exercise here to give you the experience of using meshes and to append a 
more realistic gripper for our robot arm.

Creating Your First Robot Arm (in Simulation)
[ 164 ]
For the next upgrade to our robot arm, we will add a composite mesh image of a 
gripper to the top_link of the arm. To make our code design modular, we will 
create the gripper code in a separate file and use an Xacro <include> statement in 
the main rrbot3.xacro file:
<xacro:include filename="$(find ros_robotics)/urdf/gripper.xacro" />
Using this <include> statement, the gripper.xacro file must be in the ros_
robotics/urdf directory.
The gripper is defined as four links with the <visual>, <collision>, and 
<inertial> tags, and four joints with the <parent>, <child>, <origin>, and 
<axis> tags. The four links are identified as the left_gripper, left_tip, right_
gripper, and right_tip. The links utilize mesh files from the PR2 robot for their 
<visual> and <geometry> definitions. The PR2 robot is another famous Willow 
Garage robot, now mainly used for academic research. The PR2_description Xacro 
files are part of the ros-indigo-desktop-full installation described in the Installing 
and launching ROS section in Chapter 1, Getting Started with ROS. The files used for 
the gripper are found in the /opt/ros/indigo/share/pr2_description/meshes/
gripper_v0 directory. The l_finger.dae and l_finger_tip.dae files should be 
copied to a /meshes directory under your ros_robotics package directory, or they 
can be downloaded from the example code on the book's website.
Code to add the mesh file to the left_gripper link:
  <link name="left_gripper">
    <visual>
      <origin rpy="0 0 0" xyz="0 0 0"/>
      <geometry>
        <mesh filename="package://ros_robotics/meshes/l_finger.dae"/>
      </geometry>
    </visual>
The other links follow the same format with the left_tip and right_tip links, 
utilizing the l_finger_tip.dae file. The .dae file is a Digital Asset Exchange file 
in the COLLADA format, representing a 3D image. These images can be created in 
Photoshop, SketchUp, AutoCAD, Blender, and other graphics software.
You can use the same geometry or meshes for both the <collision> and <visual> 
elements, although for performance improvements, we strongly suggest that you 
have simplified models/meshes for your collision geometry. A good open source 
tool used to simplify meshes is Blender. There are many closed source tools, such as 
Maya and 3DS Max, which can also simplify meshes. In the case of our robot arm, we 
specify a simple 0.1 meter cube to be the <collision> and <geometry> for each of 
these links.

Chapter 5
[ 165 ]
Two types of joints are used for our gripper. The left_gripper and right_gripper 
links are connected to the top_link of our robot arm using a revolute joint to restrict 
the range of movement of the joint. A <limit> tag is required for a revolute joint 
to define the <effort>, <velocity>, and <lower> and <upper> limits of the range. 
The effort limit is set to 30 Nm, the velocity limit is 0.5 rad/s, and the range is from 
-0.548 to 0.0 radians for the left_gripper_joint and 0.0 to 0.548 radians for the 
right_gripper_joint.
A fixed joint is specified between the left_gripper and the left_tip and also 
between the right_gripper and the right_tip. There is no movement between 
these links.
This following code is provided for gripper.xacro, or it can be downloaded from 
the Packt website for this book:
<?xml version="1.0"?>
<robot xmlns:xacro="http://www.ros.org/wiki/xacro">
  <!-- Gripper -->
  <joint name="left_gripper_joint" type="revolute">
    <parent link="top_link"/>
    <child link="left_gripper"/>
    <origin xyz="0 0 ${height2 - axle_offset}" rpy="0 -1.57 0"/>
    <axis xyz="0 0 1"/>
    <limit effort="30.0" lower="-0.548" upper="0.0" 
velocity="0.1"/>
  </joint>
  <link name="left_gripper">
    <visual>
      <origin rpy="0 0 0" xyz="0 0 0"/>
      <geometry>
        <mesh filename="package://ros_robotics/meshes/l_finger.dae"/>
      </geometry>
    </visual>
    <collision>
      <geometry>
        <box size="0.1 0.1 0.1"/>
      </geometry>
    </collision>
    <xacro:default_inertial z_value="0" i_value="0.000001" 
mass="1"/>
  </link>

Creating Your First Robot Arm (in Simulation)
[ 166 ]
  <joint name="left_tip_joint" type="fixed">
    <parent link="left_gripper"/>
    <child link="left_tip"/>
  </joint>
  <link name="left_tip">
    <visual>
      <origin rpy="0 0 0" xyz="0.09137 0.00495 0"/>
      <geometry>
        <mesh 
          filename="package://ros_robotics/meshes/l_finger_tip.dae"/>
      </geometry>
    </visual>
    <collision>
      <geometry>
        <box size="0.1 0.1 0.1"/>
      </geometry>
    </collision>
    <xacro:default_inertial z_value="0" i_value="1e-6" mass="1e-
5"/>
  </link>
  <joint name="right_gripper_joint" type="revolute">
    <parent link="top_link"/>
    <child link="right_gripper"/>
    <origin xyz="0 0 ${height2 - axle_offset}" rpy="0 -1.57 0"/>
    <axis xyz="0 0 -1"/>
    <limit effort="30.0" lower="0.0" upper="0.548" 
velocity="0.5"/>
  </joint>
  <link name="right_gripper">
    <visual>
      <origin rpy="3.1415 0 0" xyz="0 0 0"/>
      <geometry>
        <mesh filename="package://ros_robotics/meshes/l_finger.dae"/>
      </geometry>
    </visual>

Chapter 5
[ 167 ]
    <collision>
      <geometry>
        <box size="0.1 .1 .1"/>
      </geometry>
    </collision>
    <xacro:default_inertial z_value="0" i_value="1e-6" mass="1"/>
  </link>
  <joint name="right_tip_joint" type="fixed">
    <parent link="right_gripper"/>
    <child link="right_tip"/>
  </joint>
  <link name="right_tip">
    <visual>
      <origin rpy="-3.1415 0 0" xyz="0.09137 0.00495 0"/>
      <geometry>
        <mesh 
          filename="package://ros_robotics/meshes/l_finger_tip.dae"/>
      </geometry>
    </visual>
    <collision>
      <geometry>
        <box size="0.1 .1 .1"/>
      </geometry>
    </collision>
    <xacro:default_inertial z_value="0" i_value="1e-6" mass="1e-
5"/>
  </link>
</robot>
Next, run the rviz roslaunch command:
$ roslaunch ros_robotics rrbot_rviz.launch model:=rrbot3.xacro

Creating Your First Robot Arm (in Simulation)
[ 168 ]
A close-up view of the gripper should look similar to the following screenshot:
rrbot gripper in rviz
The controls for the arm and gripper are accessible via the four joint control sliders, 
as shown in the following screenshot. Controls for left_gripper_joint and 
right_gripper_joint have been added to the arm joints. The robot arm is 
shown in a random pose:

Chapter 5
[ 169 ]
rrbot3.xacro in rviz
With our robot arm built using Xacro, we are ready to add modifications to the Xacro 
file so that it can be recognized as an SDF by Gazebo. Then, we will add transmission 
and control plugins to enable our robot arm to be controlled by ROS commands from 
the command line or rqt.
Controlling an articulated robot arm in 
Gazebo
The Modifications to the robot URDF section in Chapter 2, Creating Your First 
Two-Wheeled Robot (in Simulation) describes changes that need to be made to 
the URDF model so that Gazebo recognizes it as an SDF. The next section 
identifies the changes needed for our robot arm, rrbot.

Creating Your First Robot Arm (in Simulation)
[ 170 ]
Adding Gazebo-specific elements
Specific elements unique to the Gazebo simulation environment are grouped in the 
following areas:
•	
The <material> tags are used to specify the Gazebo color or texture for 
each link
•	
The <mu1> and <mu2> tags are used to define friction coefficients for the 
contact surfaces for four of the robot's links
•	
Plugin to control the revolute joints of rrbot (included here but described in 
the Adding a Gazebo ROS control plugin section)
These specific Gazebo XML elements needed for simulation are split into a separate 
file, labeled rrbot.gazebo, and an Xacro <include> statement is used in the main 
rrbot4.xacro file:
<xacro:include filename="$(find ros_robotics)/urdf/rrbot.gazebo" />
Using this <include> statement, the rrbot.gazebo file must be located in the 
ros_robotics/urdf directory.
To create the rrbot.gazebo file, use the following code:
<?xml version="1.0"?>
<robot>
  <!-- Base Link -->
  <gazebo reference="base_link">
    <material>Gazebo/Red</material>
  </gazebo>
  <!-- Middle Link -->
  <gazebo reference="mid_link">
    <mu1>0.2</mu1>
    <mu2>0.2</mu2>
    <material>Gazebo/Green</material>
  </gazebo>
  <!-- Top Link -->
  <gazebo reference="top_link">
    <mu1>0.2</mu1>
    <mu2>0.2</mu2>
    <material>Gazebo/Blue</material>

Chapter 5
[ 171 ]
  </gazebo>
  <!-- Gripper Elements -->
  <gazebo reference="left_gripper">
    <mu1>0.2</mu1>
    <mu2>0.2</mu2>
  </gazebo>
    
  <gazebo reference="right_gripper">
    <mu1>0.2</mu1>
    <mu2>0.2</mu2>
  </gazebo>
 
  <gazebo reference="left_tip" />
  <gazebo reference="right_tip" />
  <!-- ros_control plugin -->
  <gazebo>
    <plugin name="gazebo_ros_control" 
            filename="libgazebo_ros_control.so">
      <robotNamespace>/rrbot</robotNamespace> 
      <robotSimType>
        gazebo_ros_control/DefaultRobotHWSim
      </robotSimType>
    </plugin>
  </gazebo>
</robot>
Fixing the robot arm to the world
Since Gazebo simulates the physics of the real world, a robot arm, as we have 
defined it, will not stand up for long as the force of gravity will cause it to topple. 
Therefore, we need to attach our robot arm model to Gazebo's world frame. In the 
rrbot4.xacro version of our code, a world link has been added and a joint fixing 
our robot's base_link to the world link:
  <link name="world"/>
  <joint name="fixed" type="fixed">
    <parent link="world"/>
    <child link="base_link"/>
  </joint>

Creating Your First Robot Arm (in Simulation)
[ 172 ]
With the base_link of the arm fixed to the world link, the mid_link and 
top_link will still succumb to the force of gravity. Although the robot arm appears 
to be standing straight up when the arm is launched in Gazebo, you will see 
that these top two links of the arm fall. The arm will swing and slow due to the 
<dynamics> element defined for the joint, until it comes to a complete stop. We 
encourage you to play with the <damping> value in order to understand its property 
in relation to the Gazebo simulation.
Viewing the robot arm in Gazebo
Before we continue to add control elements to our URDF, we need to launch the 
rrbot4.xacro file in Gazebo. This launch file is similar to the ddrobot_gazebo.
launch file in Chapter 2, Creating Your First Two-Wheeled Robot (in Simulation). This 
launch file can be downloaded from the book's website, or created as rrbot_gazebo.
launch from the following XML code:
<launch>
  <!-- We resume the logic in gazebo_ros package 
empty_world.launch, 
         changing only the name of the world to be launched -->
  <include file="$(find gazebo_ros)/launch/empty_world.launch">
    <arg name="world_name" 
         value="$(find ros_robotics)/worlds/rrbot.world"/>
   
    <arg name="paused" default="false"/>
    <arg name="use_sim_time" default="true"/>
    <arg name="gui" default="true"/>
    <arg name="headless" default="false"/>
    <arg name="debug" default="false"/>
  </include>
  <!-- Load the URDF into the ROS Parameter Server -->
  <param name="robot_description"
    command="$(find xacro)/xacro.py 
      '$(find ros_robotics)/urdf/rrbot4.xacro'" />
  <!-- Spawn rrbot into Gazebo -->
  <node name="spawn_urdf" pkg="gazebo_ros"
    type="spawn_model" respawn="false" output="screen"
      args="-param robot_description -urdf -model rrbot" />
</launch>

Chapter 5
[ 173 ]
The rrbot.world file from the book's website can be downloaded and used, or you 
can use the ddrobot.world file created in Chapter 2, Creating Your First Two-Wheeled 
Robot (in Simulation). You can even just omit the argument for world_name in the 
include statement and use the empty_world from the gazebo_ros package.
The command to launch the robot arm in Gazebo is as follows:
$ roslaunch ros_robotics rrbot_gazebo.launch
The Gazebo screen should look similar to the following screenshot after the top two 
links of the arm have fallen and slowed to a stop:
rrbot4.xacro in Gazebo
After verifying the model in Gazebo, additional control elements should be added to 
the robot arm URDF.
Adding controls to the Xacro
The following are the steps used to set up controls for this robot arm in the 
Gazebo simulation:
1.	 Define transmission elements for joints in the rrbot and gripper Xacro files.
2.	 Add a gazebo_ros_control plugin to our file of Gazebo-specific elements.
3.	 Create a YAML configuration file for control parameters.
4.	 Create a control launch file to launch the robot joint controllers.

Creating Your First Robot Arm (in Simulation)
[ 174 ]
To begin with, we need to install four packages: gazebo_ros_pkgs, gazebo_ros_
control, ros_control, and ros_controllers. The Gazebo_ros_pkgs metapackage 
is a set of ROS packages (a metapackage) that provides the interface and control for a 
robot in Gazebo. The Ros_control and ros_controllers packages provide generic 
controllers for ROS robots. For ROS Indigo, the Debian packages can be installed 
with the following command:
$ sudo apt-get install ros-indigo-gazebo-ros-pkgs ros-indigo-gazebo-
ros-control ros-indigo-ros-control ros-indigo-ros-controllers
The gazebo_ros_control package integrates the ros_contol controller software 
with the Gazebo simulator. Gazebo_ros_control instantiates the ros_control 
(metapackage) controller_manager (package) to provide simulation of the robot's 
controllers. The controller manager will be used by our control launch file to spawn 
controllers for the joint_state_controller and controllers for all four of the robot 
arm's revolute joints.
More details on the ROS controllers and the ros_control packages can be found at 
http://wiki.ros.org/ros_control.
Defining transmission elements for joints
Specific elements must be added to the URDF/SDF in order for a model to be 
controlled in the Gazebo simulation environment. The <transmission> element 
is used to define the relationship between the robot joint and the actuator. These 
elements are supposed to encapsulate the details of the mechanical coupling, with 
specific gear ratios and parallel linkages defined. As you will note, we have a simple 
mechanical joint and do not require complex transmission element definitions.
The rrbot robot arm requires a <transmission> element for each of the 
rrbot revolute joints (joint_base_mid, joint_mid_top, left_gripper, and 
right_gripper). Each <transmission> element has a unique <name> and is 
associated with one of the <joint names> for the revolute joints. The <type> 
is transmission_interface/SimpleTransmission. In the <actuator> tag, 
the <hardwareInterface> is EffortJointInterface, since it is the only type 
implemented for Gazebo at the present time. Each of the four <transmission> 
element will look similar to the following code:
  <transmission name="transmission1">
    <type>transmission_interface/SimpleTransmission</type>
    <joint name="joint_base_mid">
      <hardwareInterface>EffortJointInterface</hardwareInterface>
    </joint>
    <actuator name="motor1">
      <hardwareInterface>EffortJointInterface</hardwareInterface>
      <mechanicalReduction>1</mechanicalReduction>

Chapter 5
[ 175 ]
    </actuator>
  </transmission>
This code should be duplicated and added once each for the joint_base_mid and 
joint_mid_top in the rrbot4.xacro file. It should also be duplicated and added 
once each for the left_gripper and right_gripper in the gripper.xacro file.
More details on the <transmission> elements can be found at http://wiki.ros.
org/urdf/XML/Transmission.
Adding a Gazebo ROS control plugin
The gazebo_ros_control plugin defined previously in the Adding Gazebo-specific 
elements section is as follows:
  <!-- ros_control plugin -->
  <gazebo>
    <plugin name="gazebo_ros_control" 
            filename="libgazebo_ros_control.so">
      <robotNamespace>/rrbot</robotNamespace>
      <robotSimType>
        gazebo_ros_control/DefaultRobotHWSim
      </robotSimType>
    </plugin>
  </gazebo>
This plugin will parse the <transmission> elements in the rrbot4.xacro and 
gripper.xacro files and load the identified hardware interfaces and controller 
managers. The preceding control plugin is a simple default plugin that should 
already be in the rrbot.gazebo file.
Creating a YAML configuration file
YAML is a markup language commonly used for ROS parameters. It is convenient 
to use YAML-encoded files to set ROS parameters on the Parameter Server. For 
rrbot, a YAML file is created to hold the joint controller configurations, and this 
YAML file is loaded via the control launch file. The controller type is defined for 
the joint_state_controller, as well as for all the four rrbot joint controllers. 
The four rrbot controllers also have proportional-integral-derivative (PID) gains 
defined. These PID gains have been tuned to the control of the rrbot arm and 
gripper. The rrbot_control.yaml file contains the following code:
rrbot:
  # Publish all joint states 
  joint_state_controller:
    type: joint_state_controller/JointStateController

Creating Your First Robot Arm (in Simulation)
[ 176 ]
    publish_rate: 50  
  
  # Position Controllers 
  joint_base_mid_position_controller:
    type: effort_controllers/JointPositionController
    joint: joint_base_mid
    pid: {p: 100.0, i: 0.01, d: 10.0}
  joint_mid_top_position_controller:
    type: effort_controllers/JointPositionController
    joint: joint_mid_top
    pid: {p: 100.0, i: 0.01, d: 10.0}
  left_gripper_joint_position_controller:
    type: effort_controllers/JointPositionController
    joint: left_gripper_joint
    pid: {p: 1.0, i: 0.00, d: 0.0}
  right_gripper_joint_position_controller:
    type: effort_controllers/JointPositionController
    joint: right_gripper_joint
    pid: {p: 1.0, i: 0.00, d: 0.0}
The rrbot_control.yaml file should be saved to a /config directory under the 
ros_robotics package directory.
Creating a control launch file
The best way to initiate control of our rrbot robot arm is to create a launch file 
to load the parameters into the Parameter Server and start all the ros_control 
controllers. The rosparam statement loads the controller settings to the Parameter 
Server from the YAML configuration file. Next, the control_spawner node creates 
the five controllers for rrbot using the controller_manager package. Another node 
is started for the robot_state_publisher. This rrbot_control.launch control 
file is shown as follows, and is stored in the /launch directory of the ros_robotics 
package directory:
<launch>
  <!-- Load joint controller configurations from YAML file to
            parameter server -->
  <rosparam file="$(find ros_robotics)/config/rrbot_control.yaml"
            command="load"/>
  <!-- load the controllers -->

Chapter 5
[ 177 ]
  <node name="control_spawner" pkg="controller_manager"
    type="spawner" respawn="false"
    output="screen" ns="/rrbot" args="joint_state_controller
    joint_base_mid_position_controller
    joint_mid_top_position_controller
    left_gripper_joint_position_controller
    right_gripper_joint_position_controller"/>
  <!-- convert joint states to TF transforms for rviz, etc -->
  <node name="robot_state_publisher" pkg="robot_state_publisher"
    type="robot_state_publisher" respawn="false" output="screen">
    <remap from="/joint_states" to="/rrbot/joint_states" />
  </node>
</launch>
After the nodes are started, the joint_state_controller begins publishing all 
the (non-fixed) joint states of rrbot on the JointState topic. The robot_state_
publisher subscribes to the JointState messages and publishes the robot's 
transforms to the tf transform library. Each of the other joint position controllers 
manages the control for its particular revolute joint.
To start the rrbot simulation, launch rrbot in Gazebo using the following 
command:
$ roslaunch ros_robotics rrbot_gazebo.launch
When rrbot is visible in the Gazebo window, open a second terminal window and 
launch the controllers using the following command:
$ roslaunch ros_robotics rrbot_control.launch
In the previously created control launch file, both the controller_manager and 
robot_state_publisher packages are used. If you plan to reuse this code or share 
it, it is recommended that you add these dependencies to your package.xml file 
for the ros_robotics package. The following statements should be added under 
the dependencies:
<exec_depend>controller_manager</exec_depend>
<exec_depend>robot_state_publisher</exec_depend>

Creating Your First Robot Arm (in Simulation)
[ 178 ]
Controlling your robot arm with the ROS 
command line
Now, we are able to send commands via a third terminal window to control our 
rrbot robot arm. The rostopic pub command is used to publish our command 
data to a specific joint position controller.
This command will move the top link of the arm to a 1.57-radian (90-degree) 
position, relative to the middle link:
$ rostopic pub -1 /rrbot/joint_mid_top_position_controller/command 
std_msgs/Float64 "data: 1.57"
On the terminal, the following screen message is displayed:
publishing and latching message for 3.0 seconds
On the Gazebo screen, the rrbot arm should look similar to the following 
screenshot. The view has been rotated to make the arm more visible to the reader:
rrbot joint_mid_top at 1.57 radians

Chapter 5
[ 179 ]
To open the gripper, two commands can be sent consecutively to move the right 
gripper and then the left gripper -0.5 radians from its center position:
$ rostopic pub -1 
/rrbot/right_gripper_joint_position_controller/command 
std_msgs/Float64 "data: -0.5"; rostopic pub -1 
/rrbot/left_gripper_joint_position_controller/command 
std_msgs/Float64 "data: -0.5"
On the Gazebo screen, the rrbot gripper should look similar to the following 
screenshot. Joint axes and rotation are shown in the Gazebo view:
rrbot gripper open 1.0 radians

Creating Your First Robot Arm (in Simulation)
[ 180 ]
Controlling your robot arm with rqt
Another tool that we can use to control our rrbot arm is rqt, the ROS plugin-based 
user interface described in the Introducing RQT tools section of Chapter 3, Driving 
Around with TurtleBot. The command to start rqt is as follows:
$ rosrun rqt_gui rqt_gui
Under the Plugins menu on the rqt main window menu bar, select the Topic 
| Message Publisher plugin. From the Topic drop-down box, at the top of the 
Message Publisher plugin, select the command for the particular controller that 
you want to publish to and add it to the Message Publisher main screen. The green 
plus sign button in the top-right corner of the window will add the command to the 
main screen. In the following screenshot, the /rrbot/joint_base_mid_position_
controller/command and the /rrbot/joint_mid_top_position_controller/
command topics have been added to the Message Publisher main screen:
rqt Message Publisher screen for rrbot
To change the position of the rrbot arm, select and change the expression field of one 
of the topics. In the following screenshot, the value of the /rrbot/joint_base_mid_
position_controller has been changed to -1.57 radians. In addition to this, the 
plugin for the Topic Monitor has been displayed so that the change in the published 
state of the /rrbot/joint_base_mid_position_controller can be verified:

Chapter 5
[ 181 ]
rqt Message Publisher and Topic Monitor
The following screenshot shows both rqt and Gazebo with the rrbot arm positioned 
with its joint_base_mid at -1.57 radians. The joint_mid_top is set at 0.0 radians:
rrbot arm controlled via rqt

Creating Your First Robot Arm (in Simulation)
[ 182 ]
Trying more things in rqt
The following suggestions are additional starting points for using rqt with your 
rrbot arm:
•	
Message Publisher expression values can also be equations, such as sin(i/10), 
where i is an rqt variable for time. Inserting this equation into one of the joint 
commands will make the joint vary sinusoidally with respect to time.
•	
The rqt Plot plugin is found under Plugins | Visualization | Plot. Choose to 
plot the joint_xxx_position_controller/command/data to the screen or 
even the joint_xxx_position_controler/state/error. The error plot will 
visually display how well the PID control eliminates the error on the joint.
•	
The rqt Dynamic Reconfigure plugin is found under Plugins | 
Configuration | Dynamic Reconfigure. Click on Expand All to see all the 
options and select PID for any of the joint controllers. A pop-up window will 
allow you to dynamically change the p_gain, i_gain, d_gain, i_clamp_min, 
and i_clamp_max to tune that controller's performance.
It is also possible to move the robot arm with a Python script, but this is left as an 
exercise for the reader.
Summary
This chapter developed an understanding of how robot arms are modeled and 
controlled. We created a URDF/SDF for a simple three-link, two-joint robot arm to 
be used in a Gazebo simulation. Xacro was used to make the URDF/SDF modular 
and efficient. Mesh files were incorporated into the gripper design to give it a more 
realistic look. Control plugins and transmission elements were added to the 
robot model code to enable control of the robot arm in simulation. Then, ROS 
commands were used via the command line and rqt tools to control the arm's 
position in Gazebo.
In Chapter 6, Wobbling Robot Arms Using Joint Controls, we will extend our control 
of robot arms to the 7-DOF dual-armed Baxter. We will learn multiple ways to 
command and control Baxter's arms using both forward kinematic and inverse 
kinematic methods.

[ 183 ]
Wobbling Robot Arms Using 
Joint Control
Mobile robots are good at getting from one location to another without running into 
things around them. To be even more useful, a robot arm can grasp and manipulate 
objects in its environment. This chapter features a leading-edge robot that uses its 
two arms to perform tasks from manufacturing to human assistance and more. The 
Baxter robot by Rethink Robotics is a collaborative robot that safely works alongside 
humans without the need for safety precautions. The authors thank Rethink Robotics 
for their recent open source release of Baxter Simulator so that the simulated version 
of Baxter could be included in this book for those who cannot afford to purchase a 
real Baxter.
In this chapter, you will be introduced to Baxter in both real and simulated form. 
The software for Baxter Simulator will be installed and executed to bring up the 
Gazebo environment with a Baxter model in it. Baxter's arms will be controlled using 
a variety of methods: keyboard, joystick, and Python script. Demonstrations of the 
different types of joint controls for Baxter's arms will be provided.
A more in-depth look at tf, ROS transform reference frames, is included in this 
chapter. These reference frames are critical for maintaining the complex kinematic 
equations that are required for Baxter's arm joints. Another ROS tool, MoveIt, will be 
introduced and used to manipulate Baxter's arms. MoveIt provides a framework for 
motion planning for both of Baxter's arms, an individual arm, or a subset of joints in 
an arm.

Wobbling Robot Arms Using Joint Control
[ 184 ]
A section on the real Baxter is included and describes the configuration of Baxter 
with a workstation computer. This setup is the standard for what is referred to as 
the research Baxter. In the section Introducing Baxter, the different versions of Baxter 
will be clarified. All commands and controls described for Baxter Simulator will also 
apply to the real Baxter. The use of MoveIt to plan Baxter's arm movements to avoid 
obstacles will be presented.
In this chapter, you will learn about the following topics:
•	
Baxter and the robot's hardware
•	
Loading and using Baxter Simulator with Gazebo
•	
Using MoveIt to create trajectories for Baxter's arms
•	
Controlling the real Baxter with applications
Introducing Baxter
Baxter is a two-armed robot created by Rethink Robotics to be a collaborative worker 
for the manufacturing industry. Each of Baxter's arms has seven degrees of freedom 
(DOF) and a series of joint actuators that make Baxter unique as a manufacturing 
robot. Baxter's joints are composed of series elastic actuators (SEAs) that have a 
spring between the motor/gearing and the output of the actuator. This springiness 
makes Baxter's arms compliant and capable of detecting external forces such as 
contact with a human. This advantage makes Baxter safe to work alongside people 
without a safety cage. The SEAs also provide greater flexibility for control using the 
torque deflection as feedback for the control system.

Chapter 6
[ 185 ]
Baxter, as shown in the following figure, sports sensors that enable Baxter to perform 
many tasks:
•	
360-degree sonar sensor at the top of Baxter's head
•	
1024 x 600 pixel screen face with built-in camera
•	
Camera, infrared sensor, and accelerometer on the cuff at the end of each arm
•	
Gripper mount that can easily mount a variety of end-effectors
•	
Navigator buttons with a scroll wheel dial on each forearm and torso side
Baxter on pedestal

Wobbling Robot Arms Using Joint Control
[ 186 ]
The manufacturing version of Baxter is programmed by moving the arms to desired 
locations and interacting with navigator buttons on the arm or torso to remember the 
positions. Gripper control is achieved by activation of buttons located on each cuff. 
An indented spot on Baxter's cuff places the arm in Zero Force Gravity (Zero-G) 
mode to allow the arm's joints to be moved effortlessly into position. Teaching Baxter 
different arm positions and trajectories can be collected into a sequence and stored as 
a type of program. The display screen face is used as a GUI for the worker to build 
and store these programs. No special programming language or mathematics is 
required, only arm manipulation, button presses, and a scroll wheel dial located 
with the navigator buttons.
Position to activate Zero-G mode
More information on Baxter's technical specifications can be found at the 
following websites:
•	
http://www.rethinkrobotics.com/baxter/tech-specs/
•	
http://sdk.rethinkrobotics.com/wiki/Hardware_Specifications
•	
http://www.active8robots.com/wp-content/uploads/Baxter-
Hardware-Specification-Architecture-Datasheet.pdf
Information on the manufacturing version of Baxter can be found at: 
http://www.rethinkrobotics.com/baxter/.
Baxter, the research robot
A second version of Baxter was introduced by Rethink the year following the release 
of the manufacturing version of Baxter. This second Baxter version is for use by 
academic and research organizations. The hardware for the research version of 
Baxter is identical to the manufacturing version. However, the software for the 
two versions is not the same.

Chapter 6
[ 187 ]
The Baxter research robot is configured with an SDK that runs on a remote computer 
workstation and allows researchers to develop custom software for Baxter. The SDK 
provides an open source ROS application programming interface (API) to directly 
run ROS commands and scripts to operate Baxter. Baxter runs as the ROS Master 
and any remote workstation (on Baxter's network) launches ROS nodes to connect to 
Baxter and control the joints and sensors.
An alternative arrangement of configuring the SDK directly on the physical Baxter is 
possible, but this scenario will not be covered in this book.
For information on the Baxter on-robot workspace setup and code execution, visit 
http://sdk.rethinkrobotics.com/wiki/SSH.
Researchers have been able to develop applications with Baxter in numerous areas. 
Rethink Robotics hosts a web page to link many of the accomplishments.
For videos, visit http://sdk.rethinkrobotics.com/wiki/Customer_Videos.
For research papers, visit http://sdk.rethinkrobotics.com/wiki/Published_
Work.
We recommend you explore these websites after completing this chapter.
Baxter Simulator
Baxter Simulator has been developed by Rethink Robotics to provide a comparable 
simulation experience for controlling Baxter using Gazebo. The simulation software 
for Baxter is contained in the ROS metapackage baxter_simulator. Baxter's URDF 
is used to create the simulated Baxter model and an emulation of the hardware 
interfaces to the research Baxter is provided by the ROS package baxter_sim_
hardware. This package allows the models of the position and velocity controllers 
to be modified using the ROS rqt tool. Arm and head controllers are found in the 
package baxter_sim_controllers. These controller plugins for Gazebo are for the 
arm position, velocity, and torque control and the position control for the head and 
the electric grippers. Interfaces are also simulated for Baxter's components:
•	
The head sonar ring
•	
The infrared sensors on each cuff
•	
The cameras on each cuff and head
•	
The navigator lights and buttons
•	
The shoulder buttons
•	
The head screen display (xdisplay)

Wobbling Robot Arms Using Joint Control
[ 188 ]
Baxter Simulator can also be used with the ROS tools rviz and MoveIt. Details on 
rviz can be found in the Rviz section of Chapter 2, Creating Your First Two-Wheeled 
ROS Robot (in Simulation); details on MoveIt will be provided later in this chapter. 
Further details on Baxter Simulator will be supplied as we install the software and 
learn to control the simulated Baxter in Gazebo.
For details on the Baxter Simulator ROS packages and API, refer to the following 
Rethink websites:
•	
http://sdk.rethinkrobotics.com/wiki/Simulator_Architecture
•	
http://sdk.rethinkrobotics.com/wiki/API_
Reference#tab.3DSimulator_API
•	
https://github.com/RethinkRobotics/sdk-docs/wiki/Baxter-
simulator-API
Baxter's arms
Baxter has seven rotary joints as shown in the following figure. Each arm is often 
referred to as a 7-DOF arm, since motion of the arm is controlled by seven actuators 
(motors) that are capable of independent rotation.
Baxter's 7-DOF arms are described on the Rethink Robotics site at 
http://sdk.rethinkrobotics.com/wiki/Arms.
Baxter's arm joints

Chapter 6
[ 189 ]
The arm joints are named in the following manner:
•	
S0: Shoulder Roll
•	
S1: Shoulder Pitch
•	
E0: Elbow Roll
•	
E1: Elbow Pitch
•	
W0: Wrist Roll
•	
W1: Wrist Pitch
•	
W2: Wrist Roll
The designation of the joints as S0, S1, E0, E1, W0, W1, and W2 enables us to define 
and even monitor each of the angles for these joints with respect to the coordinates 
of the joints. The angles are measured in ROS in radians. As there are 2π radians in 
a complete circle, one radian is 360/(2π) or about 57.3 degrees. A 90-degree angle is 
π/4 or about 0.7854 radians. These conversion values are given because it is often 
required to move the joints to a 90-degree position or other angle defined in radians 
and it is usual for us to think in terms of degrees of rotation.
The joints of the arms are connected by links of various lengths. Although all the 
joints are rotary joints, there is a distinction between bend joints and twist joints. The 
bend joints, also called pitch joints, are S1, E1, and W1. They pitch up and down on 
the arm and rotate about their axis perpendicular to the joint. The twist or roll joints 
S0, E0, W0, and W2 rotate about an axis that extends along their centerline.
Information on maximum joint speeds, joint flexure stiffness, peak torque, and other 
detailed arm specifications can be found at: http://sdk.rethinkrobotics.com/
wiki/Hardware_Specifications.

Wobbling Robot Arms Using Joint Control
[ 190 ]
Baxter's pitch joints
Three of the arm joints, S1, E1, and W1, are defined as pitch joints and are labeled in 
the following figure for positive and negative direction:
Baxter's pitch joints
The following table shows Baxter's pitch joint limits and range of motion measured 
in degrees and radians:
Joint
Min limit
Max limit
Range
Min limit
Max limit
Range
in degrees
in radians
S1
-123
+60
183
-2.147
+1.047
3.194
E1
-3
+150
153
-0.052
+2.618
2.67
W1
-90
+120
210
-1.571
+2.094
3.665
Baxter's roll joints
Four of Baxter's arm joints, S0, E0, W0, and W2, are defined as roll joints and are 
labeled in the following figure for positive and negative direction.

Chapter 6
[ 191 ]
Baxter's roll joints
The following table shows Baxter's roll joint limits and range of motion measured in 
degrees and radians:
Joint
Min limit
Max limit
Range
Min limit
Max limit
Range
in degrees
in radians
E0
-173.5
+173.5
347
-3.028
+3.028
6.056
S0
-141
+51
192
-2.461
+0.890
3.351
W0
-175.25
+175.25
350.5
-3.059
+3.059
6.118
W2
-175.25
+175.25
350.5
-3.059
+3.059
6.118

Wobbling Robot Arms Using Joint Control
[ 192 ]
Baxter's coordinate frame
Before we can discuss the details of the arm positions and orientations, it is necessary 
to define a base coordinate system from which other positions are measured. 
The following figure shows Baxter's reference coordinate system:
Baxter's base coordinate system
Standing behind Baxter, the positive x axis is forward along the centerline of Baxter, 
the positive y axis is to the left from the center line, and the positive z axis is up 
vertically. The z axis center of Baxter's base coordinate system is at the base of 
Baxter's torso. This is the z=0 position. The position x=0, y=0 is back from Baxter's 
front plate, along the center line along the vertical axis.
An important use of this base coordinate system is to define the position of the 
grippers at the end of Baxter's arms in terms of the distance in x, y, and z from the 
base origin considered (0, 0, 0). This is useful since Baxter on the pedestal has a base 
coordinate system that does not move during operations. The coordinates of each 
joint and the gripper will change as Baxter performs various tasks.

Chapter 6
[ 193 ]
Cartesian positions are defined in meters in ROS as defined in REP-103, titled 
Standard Units of Measure and Coordinate Conventions (http://www.ros.org/reps/
rep-0103.html).
Another measure for any three-dimensional object is its orientation or angles 
with respect to a given coordinate system, usually a coordinate system centered 
at the object itself. We will discuss the orientation of Baxter's grippers when the 
transformation of coordinate systems, tf, is introduced.
Baxter's outstretched arms in the previous figure represent the joint angles of zero 
degrees for all the joints. The various conventions for measuring the distance and 
rotation of Baxter's grippers will be presented later.
Control modes for Baxter's arms
There are four modes of controlling Baxter's arms: joint position, joint velocity, 
joint torque, and (the most recent mode) raw joint position control. Please note the 
descriptions and the important differences between these four joint control modes:
•	
Joint position control: This mode is the most fundamental control mode 
and is the primary method for controlling Baxter's arms. The angle of each 
of Baxter's joints is specified in the message to the motor controllers. This 
message contains seven values, one value for each of the seven joints. The 
motor controller processes the message, checking for collisions in the URDF 
model between the arms and also with the torso. If a potential collision is 
detected, the collision-avoidance model plans offsets to the commanded path 
to avoid the impact.
•	
Joint velocity control: This mode is for the advanced control of Baxter's arms. 
Joint velocities are specified for the joints to simultaneously achieve. The 
joint command message will contain seven velocity values for the controllers 
to achieve. Collision avoidance and detection is applied. If the commanded 
velocity takes the joint to a position outside its limits, no joints will move. 
This mode is dangerous and its use is not advised.
•	
Joint torque control: This mode is also for the advanced control of Baxter's 
arms. Joint torques are specified for the joints to simultaneously achieve. The 
joint command message will contain seven torque values for the controllers 
to achieve. Collision avoidance and detection is not applied. This mode is 
dangerous and its use is not advised.

Wobbling Robot Arms Using Joint Control
[ 194 ]
•	
Raw joint position control: This mode provides a more direct position 
control, leaving the execution of the joint commands primarily for the 
controllers. Collision avoidance and detection is not implemented and 
motor velocity limits are not monitored. This mode is dangerous and 
its use is not advised.
For an in-depth description of the joint control modes, refer to Rethink's wiki at 
http://sdk.rethinkrobotics.com/wiki/Arm_Control_Modes.
Baxter's grippers
Rethink provides two options of grippers for Baxter: electric gripper and suction 
gripper. The electric gripper, as shown on the left in the following figure, has two 
fingers with removable inserts to allow different configurations of the gripping 
surface. Force control of the grippers allows them to pick up rigid and semi-rigid 
objects. The electric grippers can grasp an object from the inside or the outside. The 
gripper can open fully to 144 mm (approximately 5.6 inches) to grasp an object, 
though the fingers have various configurations within this grasping range.
The suction gripper supports the attachment of a single vacuum cup or a 
multi-cup vacuum manifold. The figure shows a single suction gripper 
(on the right). The gripper is powered by an external air supply line. 
This gripper works well on smooth, non-porous, or flat objects:
Baxter's electric and suction grippers

Chapter 6
[ 195 ]
Baxter's arm sensors
Each of Baxter's arms has a number of sensors on the cuff at the end of the arm. 
An integrated camera is mounted on the cuff and pointed towards objects that the 
gripper could potentially pick up. The camera has a frame rate of 30 frames per 
second and a maximum resolution of 1280 x 800 pixels. An Infrared sensor pointed in 
the same direction can detect distances from 4 to 40 cm (1.5–15 inches). The following 
figure shows the position of the cuff camera and the Infrared sensor. Each cuff also 
contains a 3-axis accelerometer. Additional information on these sensors can be 
found at the following websites:
•	
http://sdk.rethinkrobotics.com/wiki/Hardware_Specifications
•	
https://github.com/RethinkRobotics/sdk-docs/wiki/API-
Reference
Baxter's cuff camera and Infrared sensor
Loading the Baxter software
This section describes loading the software packages for the SDK, Baxter Simulator, 
and MoveIt. It is assumed that Ubuntu 14.04 and ROS Indigo software are installed 
on the computer to be used for the Baxter Simulator and software development. The 
steps for installing Ubuntu and ROS are described in Chapter 1, Getting Started with 
ROS, in the Installing and launching ROS section.

Wobbling Robot Arms Using Joint Control
[ 196 ]
Installing Baxter SDK software
The installation of Baxter Simulator requires that the SDK already be downloaded 
and installed into an ROS catkin workspace on the workstation computer. 
Instructions for the installation are presented here and can be found on the Rethink 
website at http://sdk.rethinkrobotics.com/wiki/Workstation_Setup.
In Chapter 1, Getting Started with ROS, we created the catkin workspace catkin_ws 
and used this workspace in Chapter 2, Creating Your First Two-Wheeled ROS Robot (in 
Simulation) to create the ROS package ros_robotics. For the Baxter SDK packages, 
we will create another catkin workspace, baxter_ws, to contain the Rethink ROS 
packages and be the development space for the new software we wish to create. If 
you wish to use the catkin_ws workspace for your Baxter software, skip to the step 
where the Baxter SDK dependencies are installed. (Afterwards, remember to replace 
the baxter_ws name in each of the command lines with catkin_ws.)
At the system level, the Baxter software can also be installed for all 
users of the workstation computer. Administrator privileges are 
necessary. We have installed our Baxter workspace under the /opt 
directory using the following instructions by replacing ~/baxter_ws 
with /opt/baxter_ws.
For a user-level installation, create the Baxter catkin workspace baxter_ws by typing 
these commands:
$ mkdir -p ~/baxter_ws/src
$ cd ~/baxter_ws/src
$ catkin_init_workspace
Build and install the Baxter workspace:
$ cd ~/baxter_ws
$ catkin_make
Next, source the setup.bash file within the Baxter workspace to overlay this 
workspace on top of the ROS environment for the workstation:
$ source ~/baxter_ws/devel/setup.bash
Remember to add this source command to your .bashrc file:
$ echo "source ~/baxter_ws/devel/setup.bash" >> ~/.bashrc

Chapter 6
[ 197 ]
Make sure the ROS_PACKAGE_PATH environment variable includes the path you just 
sourced by typing:
$ echo $ROS_PACKAGE_PATH
The path /home/<username>/baxter_ws/src should be displayed as one of the paths 
on the screen.
Now that the Baxter catkin workspace has been created, the Baxter SDK 
dependencies are installed by typing the following commands:
$ sudo apt-get update
$ sudo apt-get install git-core python-argparse python-wstool python-
vcstools python-rosdep ros-indigo-control-msgs ros-indigo-joystick-
drivers
Next, the ROS wstool workspace tool is used to check out all the required Baxter 
SDK packages from the GitHub repository and place them in the Baxter workspace 
source directory:
$ cd ~/baxter_ws/src
$ wstool init
$ wstool merge 
https://raw.githubusercontent.com/RethinkRobotics/baxter/master/baxte
r_sdk.rosinstall
$ wstool update
Then, the workspace is built and installed:
$ cd ~/baxter_ws
$ catkin_make
$ catkin_make install
These instructions install the latest version of the Baxter SDK source, which is 
version 1.2 at the time of writing. In the next section, installation instructions 
for the Baxter Simulator software packages will be described.
Installing Baxter Simulator
Since Rethink has made the packages for Baxter Simulator open source, owning 
a real Baxter robot is no longer necessary for gaining access to the GitHub files. 
The instructions presented here for loading the Baxter Simulator software on 
your computer can also be found at http://sdk.rethinkrobotics.com/wiki/
Simulator_Installation.

Wobbling Robot Arms Using Joint Control
[ 198 ]
To ensure that you have the supporting ROS packages required for Baxter Simulator, 
we recommend that you execute the following two commands in preparation for 
loading the Baxter Simulator packages:
$ sudo apt-get update
$ sudo apt-get install gazebo2 ros-indigo-qt-build ros-indigo-driver-
common ros-indigo-gazebo-ros-control ros-indigo-gazebo-ros-pkgs ros-
indigo-ros-control ros-indigo-control-toolbox ros-indigo-realtime-
tools ros-indigo-ros-controllers ros-indigo-xacro ros-indigo-tf-
conversions ros-indigo-kdl-parser
A large number of ROS packages are loaded by the sudo apt-get command. 
The packages are as follows:
•	
Gazebo2: This is the correct version of Gazebo to work with ROS Indigo.
•	
qt_build: This is necessary for building qt and rqt applications such as 
rqt_reconfigure.
•	
driver_common: This package provides support for keyboard and joystick 
drivers as well as drivers for output devices such as cameras and displays.
•	
gazebo_ros_control, ros_control, control_toolbox, and ros_
controllers: These provide the simulated control software as well as the 
real-time control software for Baxter Simulator.
•	
gazebo_ros_pkgs: This provides the interface between Gazebo and ROS 
enabling robots to be simulated in the Gazebo environment.
•	
realtime_tools: This provides a real-time publisher that publishes ROS 
messages to a topic from a real-time thread.
•	
xacro: This package is used with Baxter's URDF to generate the robot_
description parameter. Xacro is described in some detail in Chapter 5, 
Creating Your First Robot Arm (in Simulation).
•	
tf_conversions: This package and kdl_parser work together to 
support the tf transforms of Baxter. The tf_conversion package provides 
conversions for the user to obtain the data type they require from the 
transform library. KDL stands for Kinematics and Dynamics Library. The 
kdl_parser package provides the tools to construct the full KDL tree from 
the URDF. If you are not familiar with URDF, Chapter 2, Creating Your First 
Two-Wheeled ROS Robot (in Simulation) and Chapter 5, Creating Your First 
Robot Arm (in Simulation) will provide an understanding of how the URDF 
describes the robot, its kinematics structure and its dynamic movement.

Chapter 6
[ 199 ]
The ROS wstool workspace tool is used to checkout all the required Baxter 
Simulator packages from the GitHub repository and place them in the Baxter 
workspace source directory:
$ cd ~/baxter_ws/src
$ wstool init 
$ wstool merge https://raw.githubusercontent.com/RethinkRobotics/baxter_
simulator/master/baxter_simulator.rosinstall
$ wstool update
Then, the workspace is built and installed:
$ cd ~/baxter_ws
$ catkin_make
$ catkin_make install
Configuring the Baxter shell
The Baxter SDK requires the script file baxter.sh to establish the connections 
between the Baxter robot and the workstation computer. This connection will 
depend on how your network is set up. Further details on the network connection 
to a real Baxter will be discussed in the Configuring a real Baxter setup section.
Baxter Simulator additionally uses the file baxter.sh to establish a simulation mode 
where the ROS environment variables are setup to identify the host workstation 
computer. The script file baxter.sh contains a special hook for Baxter Simulator. 
First, the file must be copied to the baxter_ws directory and the file permissions 
changed to grant execution privileges to all users:
$ cp ~/baxter_ws/src/baxter/baxter.sh ~/baxter_ws
$ chmod +x baxter.sh
Next, open the baxter.sh script in your favorite editor and find the your_ip 
parameter (around line 26). Change the your_ip value to be the IP address of 
your workstation computer:
your_ip="192.168.XXX.XXX"
If the IP address of your computer is unknown, use the ifconfig command:
$ ifconfig
The screen results will contain the inet_addr field for the IP address of the 
workstation computer.

Wobbling Robot Arms Using Joint Control
[ 200 ]
Alternatively, if you wish to use the hostname, comment out the line for your_ip 
and uncomment the line for your_hostname. To use the real hostname of your 
workstation computer, use the following command:
$ hostname
Then, add this to the your_hostname parameter.
These parameters will assign either the ROS_IP or ROS_HOSTNAME environment 
variable. If both are present, the ROS_HOSTNAME variable takes precedence.
Also, notice near line 30 that ros_version should be set to indigo. 
Then, save and close the baxter.sh script.
Installing MoveIt
MoveIt is an important ROS tool for path planning and can be used with Baxter 
Simulator or a real Baxter. The installation of MoveIt is described here and the 
operation of MoveIt is detailed later in the chapter. Instructions for the installation 
can also be found on the Rethink website at http://sdk.rethinkrobotics.com/
wiki/MoveIt_Tutorial.
The MoveIt software should be loaded into the source (src) directory of the catkin 
workplace baxter_ws created earlier in the chapter. The commands are as follows:
$ cd ~/baxter_ws/src
$ git clone https://github.com/ros-planning/moveit_robots.git
$ sudo apt-get update
$ sudo apt-get install ros-indigo-moveit-full
Then, the new additions to the workspace are incorporated with the 
catkin_make command:
$ cd ~/baxter_ws
$ catkin_make
To verify that all the Baxter SDK, simulator, and MoveIt packages were downloaded 
and installed, type the following command:
$ ls ~/baxter_ws/src
The output should be as follows:
baxter                      baxter_examples      baxter_simulator    CMakeLists.txt
baxter_common    baxter_interface       baxter_tools             moveit_robots

Chapter 6
[ 201 ]
Launching Baxter Simulator in Gazebo
Before launching Baxter Simulator in Gazebo, it is important to check the ROS 
environment variables. To start up Baxter Simulator, use the following commands 
to get to your Baxter catkin workspace and run your baxter.sh script with the sim 
parameter:
$ cd ~/baxter_ws
$ ./baxter.sh sim
At this point, check your ROS environment with the following command:
$ env | grep ROS
Within the output screen text, look for the following result:
ROS_MASTER_URI=http://localhost:11311
ROS_IP= <your workstation's IP address>
or
ROS_HOSTNAME=<your workstation's hostname>
The ROS_HOSTNAME field need not be present.
If there are issues with Baxter's hardware, software, or network, refer to the general 
Baxter troubleshooting website at http://sdk.rethinkrobotics.com/wiki/
Troubleshooting.
The baxter.sh script should run without errors and the ROS environment 
variables should be correct. The next section covers the first experience of 
running Baxter Simulator.
Bringing Baxter Simulator to life
To start Baxter Simulator, go to the baxter_ws workspace and run the Baxter shell 
script with the sim parameter specified:
$ cd ~/baxter_ws
$ ./baxter.sh sim
The command prompt should return with the tag [baxter - http://
localhost:11311] appended to the beginning of the prompt. You are 
now talking to the simulated Baxter!

Wobbling Robot Arms Using Joint Control
[ 202 ]
Next, call roslaunch command to start the simulation with controllers:
$ roslaunch baxter_gazebo baxter_world.launch
The following lines are some of the results you will see on the screen while Baxter 
Simulator starts. Look for the last three lines to be assured that the simulation has 
started correctly:
NODES
  /
    base_to_world (tf2_ros/static_transform_publisher)
    baxter_emulator (baxter_sim_hardware/baxter_emulator)
    baxter_sim_io (baxter_sim_io/baxter_sim_io)
    baxter_sim_kinematics_left (baxter_sim_kinematics/kinematics)
    baxter_sim_kinematics_right (baxter_sim_kinematics/kinematics)
    gazebo (gazebo_ros/gzserver)
    gazebo_gui (gazebo_ros/gzclient)
    robot_state_publisher (robot_state_publisher/robot_state_publisher)
    urdf_spawner (gazebo_ros/spawn_model)
  /robot/
    controller_spawner (controller_manager/spawner)
    controller_spawner_stopped (controller_manager/spawner)
    left_gripper_controller_spawner_stopped (controller_manager/spawner)
    right_gripper_controller_spawner_stopped    (controller_manager/spawner)
[ INFO] [1459630925.769891432, 34.821000000]: Simulator is loaded and 
started successfully
[ INFO] [1459630925.773241168, 34.826000000]: Robot is disabled
[ INFO] [1459630925.773390870, 34.826000000]: Gravity compensation was 
turned off

Chapter 6
[ 203 ]
The following screenshot should appear with Baxter in a disabled state:
Baxter's initial state in Gazebo
If Gazebo and Baxter Simulator fail to appear or there are red error messages in your 
terminal window, refer to the Gazebo Troubleshooting page provided by Rethink 
Robotics at http://sdk.rethinkrobotics.com/wiki/Gazebo_Troubleshooting.
For an introduction to using Gazebo, refer to the Using Gazebo section in Chapter 2, 
Creating Your First Two-Wheeled ROS Robot (in Simulation). In this section, the various 
Gazebo display panels, menus, and toolbars are explained. Gazebo uses the same 
cursor/mouse control as rviz and these mouse/cursor actions are described in the 
rviz: Mouse control section of Chapter 2, Creating Your First Two-Wheeled ROS Robot 
(in Simulation).
In the previous screenshot, the left World panel shows the Models element open 
to reveal the two models in the environment: ground_plane and baxter. Under the 
baxter model, all of Baxter's links are listed and you are welcome to select the links 
to explore the details about each link. The screenshot also shows the smaller display 
window that contains Baxter's IO. Baxter's four navigators, located one on each side 
of the back torso (near the shoulders) and one on each arm, are also shown. The 
oval-shaped navigators have three push buttons, one of which is a scroll wheel. 
Baxter's cuff buttons are also shown in this window. There are two buttons and 
one touch sensor on each cuff.

Wobbling Robot Arms Using Joint Control
[ 204 ]
The terminal window in which the roslaunch command was performed will 
be unable to run additional commands, so a second terminal window should be 
opened. In this window, go to the baxter_ws workspace and run the baxter.sh 
script with the sim parameter:
$ cd ~/baxter_ws
$ ./baxter.sh sim
For each additional terminal window opened, go to the baxter_ws 
workspace and run the baxter.sh script with the sim parameter.
Baxter (in simulation) is initially in a disabled state. To confirm this, use the 
enable_robot script from the baxter_tools package using the following command:
$ rosrun baxter_tools enable_robot.py -s
The screen should display the following output:
ready: False
enabled: False
stopped: False
error: False
estop_button: 0
estop_source: 0
To enable Baxter, use the same enable_robot script with the -e option:
$ rosrun baxter_tools enable_robot.py -e
The output is similar to the following:
[INFO] [WallTime: 1444937074.277686] [736.081000] Robot Enabled
Confirm Baxter is enabled by using the following command:
$ rosrun baxter_tools enable_robot.py -s

Chapter 6
[ 205 ]
The output should as follows:
ready: False
enabled: True
stopped: False
error: False
estop_button: 0
estop_source: 0
Always enable Baxter Simulator before attempting to control 
any of the motors.
At this point, a cheat sheet for use with Baxter Simulator is provided for you to use 
with the example programs that follow. The commands for launching, enabling, 
and untucking are provided here for your reference:
Baxter Simulator cheat sheet
To launch Baxter Simulator in Gazebo, use the following commands:
$ cd ~/baxter_ws
$ ./baxter.sh sim
$ roslaunch baxter_gazebo baxter_world.launch
For subsequent terminal windows, use the following commands:
$ cd ~/baxter_ws
$ ./baxter.sh sim
To enable the robot, use the following command:
$ rosrun baxter_tools enable_robot.py –e
To enable and set the arms in a known position, use the following 
command:
$ rosrun baxter_tools tuck_arms.py -u
With Baxter enabled, the next section describes some of Baxter's example scripts that 
use the head display screen.

Wobbling Robot Arms Using Joint Control
[ 206 ]
Warm-up exercises
Rethink Robotics has provided a collection of example scripts to demonstrate 
Baxter's interfaces and features. These example programs are contained in the 
package baxter_examples and work primarily with a real Baxter and the SDK. 
A portion of these example programs also work with Baxter Simulator.
The baxter_examples are Python programs that access Baxter's hardware and 
functionality through the baxter_interface package. The baxter_examples 
programs are written to demonstrate how to use the Baxter interfaces. The 
baxter_interface package is a repository of Python APIs to use for interacting 
with the Baxter Research Robot. The repository contains a set of classes that are ROS 
wrappers for communication to and control of Baxter's hardware and functionality. 
These Python classes are built on top of the ROS API layer.
This section and the following sections present SDK example programs that can be 
used with Baxter Simulator. To find additional information on the SDK example 
programs implemented in Baxter Simulator, visit the following websites:
•	
https://github.com/RethinkRobotics/sdk-docs/wiki/Baxter-
simulator-API
•	
http://sdk.rethinkrobotics.com/wiki/API_
Reference#tab.3DSimulator_API
The first example program will display an image on Baxter's (simulated) head 
display screen using the following command:
$ rosrun baxter_examples xdisplay_image.py --file=`rospack find 
baxter_examples`/share/images/baxterworking.png
Your screen should look similar to the following screenshot:

Chapter 6
[ 207 ]
Baxter after xdisplay_image.py
The program xdisplay_image.py locates the image baxterworking.png in the 
specified location under the baxter_examples package. This image data is published 
as a sensor_msgs/Image ROS message. The display image must be a .png or .jpg 
file with display resolution 1024 x 600 pixels or smaller. Smaller images will appear 
in the top-left corner of Baxter's display screen.
A second baxter_examples program will cause Baxter Simulator to nod Baxter's 
head up and down then turn from side to side:
$ rosrun baxter_examples head_wobbler.py
The simulated Baxter should randomly wobble its head until Ctrl + C is pressed. 
The movement demonstrates both the head pan motion (side to side) and 
head nod motion (up and down) interfaces. This program shows the use of the 
baxter_interface Head class (head.py). The command_nod function is called 
first to trigger an up-down motion of the head. A specific angle for the nod motion 
cannot be commanded. The pan motion is achieved with several calls to the 
set_pan function with random angles provided as the parameter.
Another baxter_examples program also moves Baxter's head through a set of head 
positions and velocities. The Head Action Client Example demonstrates the use of 
the Head Action Server. This example is similar to the head wobble just performed 
but provides a good example of an action server and client interaction. If you wish 
to try the Head Action Client Example, access the instructions and explanations at 
http://sdk.rethinkrobotics.com/wiki/Head_Action_Client_Example.

Wobbling Robot Arms Using Joint Control
[ 208 ]
The next section will demonstrate some example programs for Baxter's arms.
Flexing Baxter's arms
The focus of the following sections will be on Baxter's arms. The section on Bringing 
Baxter Simulator to life should be completed before starting these sections. Baxter 
Simulator should be launched in Gazebo and the robot should be enabled and 
arms untucked.
The following example programs use the baxter_interface Limb class (limb.py) 
to create instances for each arm. The function joint_names is used to return an array 
of all the joints in the limb.
Commands for the joint control modes are via ROS messages within the baxter_
core_msgs package. To move the arm, a JointCommand message must be published 
on the topic robot/limb/<left/right>/joint_command. Within the JointCommand 
message, a mode field indicates the control mode to the Joint Controller Boards as 
POSITION_MODE, VELOCITY_MODE, TORQUE_MODE, or RAW_POSITON_MODE.
In the sections to follow, various methods of controlling Baxter's arm movement will 
be demonstrated. After several example arm programs are presented, a Python script 
to command Baxter's arms to a home position will be shown.
Tucking and untucking
As the first step to control Baxter's arms, the tuck and untuck commands are used 
to bring Baxter's arms to a known configuration. The tuck operation is used to place 
the real Baxter in the position referred to as the shipping pose. The following figure 
shows Baxter in simulation in a tucked position. This pose is Baxter's most compact 
state and is useful for storing Baxter during inactive periods. Remember that Baxter's 
arm control programs cannot be run from this state. The arms must be untucked for 
the joints to be commanded to a specific position.
During the tuck and untuck movements, Baxter collision avoidance aspect is 
disabled. Collision avoidance for Baxter Simulator is modeled as part of the URDF. 
Each of Baxter's links is tagged with a collision block that is slightly larger than 
the visual element. For further details on the URDF, collision blocks, and visual 
element, refer to Chapter 2, Creating Your First Two-Wheeled ROS Robot (in Simulation). 
Typically, when the pose of the arm places the collision blocks into contact with 
each other, the collision model detects the contact and stops the movement to 
avoid collision between the actual parts.

Chapter 6
[ 209 ]
To command Baxter to the tuck position, the following command is used:
$ rosrun baxter_tools tuck_arms.py -t
Here is the output on the screen:
[INFO] [WallTime: 1445965397.509736] [0.000000] Tucking arms
[INFO] [WallTime: 1445965397.615134] [83.930000] Moving head to neutral 
position
[INFO] [WallTime: 1445965397.615655] [83.931000] Tucking: One or more 
arms not Tucked.
[INFO] [WallTime: 1445965397.615829] [83.931000] Moving to neutral start 
position with collision on.
[INFO] [WallTime: 1445965399.012163] [85.320000] Tucking: Tucking with 
collision avoidance off.
[INFO] [WallTime: 1445965399.612480] [85.920000] Finished tuck
This screenshot shows the simulated Baxter in the tucked position:
Baxter tucked

Wobbling Robot Arms Using Joint Control
[ 210 ]
To command Baxter to the untuck position, use the following command:
$ rosrun baxter_tools tuck_arms.py –u
The output should be as follows:
[INFO] [WallTime: 1444940090.850322] [0.000000] Untucking arms
[INFO] [WallTime: 1444940090.955454] [3742.494000] Moving head to neutral 
position
[INFO] [WallTime: 1444940090.955729] [3742.494000] Untucking: Arms 
already Untucked; Moving to neutral position.
[INFO] [WallTime: 1444940092.864302] [3744.397000] Finished tuck
The following screenshot shows the simulated Baxter in the untucked position:
Baxter untucked

Chapter 6
[ 211 ]
To explore Baxter's tuck and untuck operations further, refer to the Rethink wiki 
Tuck Arms Tool information at http://sdk.rethinkrobotics.com/wiki/Tuck_
Arms_Tool.
Wobbling arms
The next example program provides a demonstration of controlling Baxter's arms 
using joint velocity control. The subject matter of joint control modes for Baxter's 
arms has been described previously in the Baxter's arms section. In simulation, 
the joint velocity wobble can be observed by typing the command:
$ rosrun baxter_examples joint_velocity_wobbler.py
The output should be as follows:
Initializing node...
Getting robot state...
Enabling robot...
[INFO] [WallTime: 1445967379.527851] [2059.870000] Robot Enabled
Moving to neutral pose...
Wobbling. Press Ctrl-C to stop...
^C
Exiting example...
Moving to neutral pose...

Wobbling Robot Arms Using Joint Control
[ 212 ]
The program will begin by moving Baxter's arms to a preset neutral starting position. 
Next, random velocity commands are sent to each arm to create a sinusoidal motion 
across both limbs. The following screenshot shows Baxter's neutral starting position:
Baxter's neutral position
To explore Baxter's arms' joint_velocity_wobbler operation in more detail, 
refer to the Rethink wiki Wobbler Example information at http://sdk.
rethinkrobotics.com/wiki/Wobbler_Example.
Controlling arms and grippers with a keyboard
Baxter's arms can also be controlled with keyboard keystrokes. The keystrokes are 
used to control the positions of the joints, with each keyboard key mapped to either 
increase or decrease the angle of one of Baxter's 14 arm joints. Keys on the right 
side of the keyboard are mapped to Baxter's left arm and keys on the left side of 
the keyboard are mapped to Baxter's right arm.
This example demonstrates another of Baxter's arm control modes: joint 
position control.
To start the keyboard joint position control example, use the following command:
$ rosrun baxter_examples joint_position_keyboard.py

Chapter 6
[ 213 ]
This should be the output on the screen:
Initializing node...
Getting robot state...
Enabling robot...
[INFO] [WallTime: 1444939449.128589] [3103.020000] Robot Enabled
Controlling joints. Press ? for help, Esc to quit.
key bindings:
  Esc: Quit
  ?: Help
/: left: gripper calibrate
,: left: gripper close
m: left: gripper open
y: left_e0 decrease
o: left_e0 increase
u: left_e1 decrease
i: left_e1 increase
6: left_s0 decrease
9: left_s0 increase
7: left_s1 decrease
8: left_s1 increase
h: left_w0 decrease
l: left_w0 increase
j: left_w1 decrease
k: left_w1 increase
n: left_w2 decrease
.: left_w2 increase
b: right: gripper calibrate
c: right: gripper close
x: right: gripper open
q: right_e0 decrease
r: right_e0 increase
w: right_e1 decrease
e: right_e1 increase
1: right_s0 decrease
4: right_s0 increase
2: right_s1 decrease
3: right_s1 increase
a: right_w0 decrease
f: right_w0 increase
s: right_w1 decrease
d: right_w1 increase
z: right_w2 decrease
v: right_w2 increase

Wobbling Robot Arms Using Joint Control
[ 214 ]
The output has been modified to aid ease of use.
Controlling arms and grippers with a joystick
This example program uses a joystick to control Baxter's arms. The joint_
position_joystick program uses the ROS drivers from the joy package to 
interface with a generic Linux joystick. Joysticks with a USB interface are supported 
by the joy package. The joy package creates a joy_node to generate a joy_msg 
containing the various button push and joystick move events.
The first step is to check for the joystick driver package joy using the 
following command:
$ rospack find joy
If the ROS package is on the computer, the screen should display this:
/opt/ros/indigo/share/joy
If it is not, then an error message is displayed:
[rospack] Error: stack/package joy not found
If the joy package is not present, install it with the following command:
$ sudo apt-get install ros-indigo-joystick-drivers
For a PS3 joystick controller, you will need the ps3joy package. 
Instructions can be found at http://wiki.ros.org/ps3joy/Tutorials/
PairingJoystickAndBluetoothDongle.
Next, type the command to start the joint_position_joystick program using one 
of the joystick types (xbox, logitech, or ps3):
$ roslaunch baxter_examples joint_position_joystick.launch 
joystick:=<joystick_type>
We used the Xbox controller joystick in our example; the output is as follows:
...
NODES
  /
    joy_node (joy/joy_node)

Chapter 6
[ 215 ]
    rsdk_joint_position_joystick (baxter_examples/joint_position_joystick.py)
...
[INFO] [WallTime: 1444940605.062164] [4255.152000] Robot Enabled
Press Ctrl-C to quit.
rightTrigger: left gripper close
rightTrigger: left gripper open
leftTrigger: right gripper close
leftTrigger: right gripper open
leftStickHorz: right inc right_s0
leftStickHorz: right dec right_s0
rightStickHorz: left inc left_s0
rightStickHorz: left dec left_s0
leftStickVert: right inc right_s1
leftStickVert: right dec right_s1
rightStickVert: left inc left_s1
rightStickVert: left dec left_s1
rightBumper: left: cycle joint
leftBumper: right: cycle joint
btnRight: left calibrate
btnLeft: right calibrate
function1: help
function2: help
Press Ctrl-C to stop.

Wobbling Robot Arms Using Joint Control
[ 216 ]
The preceding output shows the Xbox joystick buttons and knobs to move Baxter's 
joints. The joystick controls two joints at a time on each of Baxter's two arms by using 
the Left Stick and the Right Stick (see following diagram). The up-down (vertical) 
control of the stick controls increasing and decreasing one of the joint angles. The 
side-to-side (horizontal) control increases and decreases another joint angle. The 
Left Bumper and Right Bumper cycle the joystick control through all of Baxter's 
arm joints in the order: S0-S1-E0-E1-W0-W1-W2. For example, initially the Left Stick 
control will be in command of the (right arm) S0 joint using horizontal direction and 
the S1 joint using vertical direction. When the Left Bumper is pressed, the Left Stick 
horizontal control will command the S1 joint and the vertical control will command 
the E0 joint. Cycling the joints continues in a continuous loop where the S0 joint will 
be selected next after the W2 joint.
The joystick mapping of joystick left = robot right allows the operator ease 
of use while the operator is positioned facing Baxter.
The following diagram and table describe the mapping of the Xbox joystick controls:
Xbox joystick controls

Chapter 6
[ 217 ]
Buttons
Action for RIGHT 
Arm
Buttons
Action for LEFT Arm
Back
Help
Ctrl+C or Ctrl+Z 
Quit
Left Button (X)
gripper calibrate
Right Button (B)
gripper calibrate
Top Button (Y)
none
Bottom Button (A)
none
Left Trigger [PRESS]
gripper close
Right Trigger[PRESS]
gripper close
Left Trigger 
[RELEASE]
gripper open
Right 
Trigger[RELEASE]
gripper open
Left Bumper
cycle joints
Right Bumper
cycle joints
Stick Axes
Action
Left Stick Horizontal right: increase/decrease <current joint 1> (S0)
Left Stick Vertical
right: increase/decrease <current joint 2> (S1)
Right Stick 
Horizontal
left: increase/decrease <current joint 1> (S0)
Right Stick Vertical
left: increase/decrease <current joint 2> (S1)
Controlling arms with a Python script
In this section, we will create a simple Python script to command Baxter's arms to a 
specific pose. The following script commands Baxter's arms to a home position similar 
to the tuck position. Comments have been placed throughout the code to provide 
information on the process. Further explanation of the Python code operation is 
given following the script:
#!/usr/bin/env python
"""
Script to return Baxter's arms to a "home" position
"""
# rospy - ROS Python API
import rospy
# baxter_interface - Baxter Python API
import baxter_interface

Wobbling Robot Arms Using Joint Control
[ 218 ]
# initialize our ROS node, registering it with the Master
rospy.init_node('Home_Arms')
# create instances of baxter_interface's Limb class
limb_right = baxter_interface.Limb('right')
limb_left = baxter_interface.Limb('left')
# store the home position of the arms
home_right = {'right_s0': 0.08, 'right_s1': -1.00, 'right_w0': -
0.67, 'right_w1': 1.03, 'right_w2': 0.50, 'right_e0': 1.18, 
'right_e1': 1.94}
home_left = {'left_s0': -0.08, 'left_s1': -1.00, 'left_w0': 0.67, 
'left_w1': 1.03, 'left_w2': -0.50, 'left_e0': -1.18, 'left_e1': 
1.94}
# move both arms to home position
limb_right.move_to_joint_positions(home_right)
limb_left.move_to_joint_positions(home_left)
quit()
This code can be placed in a file home_arms.py and executed after it is made 
executable using the Ubuntu chmod +x command with this terminal command:
$ python home_arms.py
In this script, the ROS-Python interface package rospy is used to create ROS 
components from Python code. The rospy client API provides software routines for 
initializing the ROS node, Home_Arms, to invoke the process. The baxter_interface 
package provides the API for interacting with Baxter. In the script, we instantiate 
instances of the limb class for both the right and left arms. A Python dictionary 
is used to assign joint angles to specific joints for both the right arm and left arm. 
These joint angle dictionaries are passed to the move_to_joint_positions method 
to command the respective arm to the provided position. The move_to_joint_
positions method is also a part of the baxter_interface package.

Chapter 6
[ 219 ]
Recording and replaying arm movements
Another capability provided by the baxter_examples programs is the ability to 
record and playback arm positions. A recorder program captures the time and 
joint positions to an external file. The filename armRoutine is used in the following 
command lines, but you may substitute your own filename instead. After the 
command for the recorder program is executed, the operator should move Baxter's 
arms by hand or by using the keyboard, joystick, ROS commands, or a script. When 
the operator wishes to end the recording, Ctrl + C or Ctrl + Z must be pressed to stop 
the recording. The playback program can be executed with the external file passed 
as a parameter. The playback program will run through the arm positions in the file 
once and then exit. The following instructions show the commands and the order 
of operation:
$ rosrun baxter_examples joint_recorder.py -f armRoutine
The output should be as follows:
Initializing node...
Getting robot state...
Enabling robot...
[INFO] [WallTime: 1444942596.926847] [6239.995000] Robot Enabled
Recording. Press Ctrl-C to stop.
At this time, you should use the joystick, keyboard, Python script, and/or commands 
to move Baxter's arms. Press Ctrl + C when you are finished moving Baxter's arms. 
Next, execute the following command to playback the file:
$ rosrun baxter_examples joint_position_file_playback.py -f 
armRoutine
The output on the screen should be similar to the following:
Initializing node...
Getting robot state...
Enabling robot...
[INFO] [WallTime: 1444943103.913642] [6745.321000] Robot Enabled

Wobbling Robot Arms Using Joint Control
[ 220 ]
Playing back: armRoutine
Moving to start position...
 Record 10462 of 10462, loop 1 of 1
Exiting example...
If the file armRoutine is brought up in an editor, you should see that it contains data 
similar to the following:
time,left_s0,left_s1,left_e0,left_e1,left_w0,left_w1,left_w2,left_gripper,right_
s0,right_s1,right_e0,right_e1,right_w0,right_w1,right_w2,right_gripper
0.221000,-0.0799704928309,-1.0000362209,-0.745950772448,-0.0499208630966,-
1.6948350728,1.03001017593,-0.500000660376,0.0,-1.04466483018,-
0.129655442605,1.5342459042,1.94952695585,-
0.909650985497,1.03000093981,0.825985250377,0.0
...
As shown, the first line contains the labels for the data on each of the subsequent 
rows. As the first label indicates, the first field contains the timestamp. The 
subsequent fields hold the joint positions for each of the left and right arm joints 
and grippers.
Tuning Baxter's arm controllers
The velocity controllers and the position controllers for Baxter's arm joints can be 
tweaked using an rqt dashboard GUI. Baxter Simulator should be running velocity 
or position joint control mode before using one of the following command to 
launch rqt. For position control mode, use the following command:
$ roslaunch baxter_sim_hardware baxter_sdk_position_rqt.launch
For velocity control mode, you can use the following command:
$ roslaunch baxter_sim_hardware baxter_sdk_velocity_rqt.launch
The following figure shows the rqt Dynamic Configure plugin position controller 
GUI where the PID values can be modified for Baxter Simulator:

Chapter 6
[ 221 ]
 
rqt GUI control for Baxter's positon controllers
Be careful changing these values!
If you experiment with Baxter's control system, you should record the 
initial values and reset the PID values to the originals after changing them.

Wobbling Robot Arms Using Joint Control
[ 222 ]
Baxter's arms and forward kinematics
Considering Baxter's arms up to the wrist cuff, each arm has seven values that define 
the rotation angle of each joint. Since the link lengths and joint angles are known, it is 
possible to calculate the position and orientation of the gripper attached to the wrist. 
This approach to calculating the pose of the gripper given the configuration of the 
arm is called forward kinematic analysis.
Fortunately, ROS has programs that allow calculation and publishing of the joint 
angles given a particular position and orientation of the gripper. The particular topic 
for Baxter is /robot/joint_states.
Joints and joint state publisher
Baxter has seven joints in each of its two arms and two more joints in its head. 
The topic /robot/joint_states publishes the current joint states of the head pan 
(side-to-side) joint and the 14 arm joints. These joint states show position, velocity, 
and effort values for each of these joints. Joint position values are in radians, velocity 
values are in radians per second, and torque values are in Newton meters. The robot 
state publisher internally has a kinematic model of the robot. So, given the joint 
positions of the robot, the robot state publisher can compute and broadcast the 3D 
pose of each link in the robot.
For the examples in this section, it is assumed that Baxter Simulator is running, 
baxter_world is launched from baxter_gazebo, and the simulated robot is enabled:
$ cd baxter_ws
$ ./baxter.sh sim
$ roslaunch baxter_gazebo baxter_world.launch
In a second terminal, type:
$ cd ros_ws
$ ./baxter.sh sim
$ rosrun baxter_tools enable_robot.py -e
Baxter's arms will be placed in the home position using the Python script presented 
before using the command:
$ python home_arms.py

Chapter 6
[ 223 ]
The joint states will be displayed with the screen output edited to show the arm 
positions as angles of rotation in radians. To view the joint states, type:
$ rostopic echo /robot/joint_states
Here's the output on the screen:
header:
  seq: 42448
  stamp:
    secs: 850
    nsecs: 553000000
  frame_id: ''
name: ['head_pan', 'l_gripper_l_finger_joint', 'l_gripper_r_finger_joint', 'left_e0', 
'left_e1', 'left_s0', 'left_s1', 'left_w0', 'left_w1', 'left_w2', 'r_gripper_l_finger_joint', 
'r_gripper_r_finger_joint', 'right_e0', 'right_e1', 'right_s0', 'right_s1', 'right_w0', 
'right_w1', 'right_w2']
position: [9.642012118504795e-06 (Head),
Left: -9.409649977892339e-08, -0.02083311343765363, -1.171334885477055, 
1.9312641121225074, -0.07941855421008803, -0.9965989736590268, 
0.6650922280384437, 1.0314330310192892, -0.49634000104265397,
Right: 0.020833000098799456, 2.9266103072174966e-10, 1.1714460516466971, 
1.9313701087550257, 0.07941788278369621, -0.9966421178258322, -
0.6651529936706897, 1.0314155121179436, 0.49638770883940264]

Wobbling Robot Arms Using Joint Control
[ 224 ]
velocity: [8.463358573117045e-09, 2.2845555643152853e-05, 2.766005018018799e-
05, 6.96516608889685e-08, -1.4347584964474649e-07, 5.379243329637427e-08, 
-3.07783563763457e-08, -5.9625446169838476e-06, -2.765075210928186e-06, 
4.37915209815064e-06, -1.9330586583769175e-08, -3.396963606705046e-08, 
-4.1024914575147146e-07, -6.470964538079114e-07, 1.2464164369422782e-07, 
-3.489373517131325e-08, 1.3838850846575283e-06, 1.1659521943505596e-06, 
-3.293066091641411e-06]
effort: [0.0, 0.0, 0.0, -0.12553439407980704, -0.16093410986695034, 
1.538268268319598e-06, -0.1584186302672208, 0.0026223415490989055, 
-0.007023475006633362, -0.0002595722218323715, 0.0, 0.0, 0.12551329635801522, 
-0.16096013901023554, -1.4389475655463002e-05, -0.1583874287014453, 
-0.0026439994199378702, -0.0070054474078151685, 0.00024931690616014635]
Compare the radian values from home_arms.py and the result of rostopic echo of 
joint states, but watch the order of listing of the joints:
# store the home position of the arms
home_right = {'right_s0': 0.08, 'right_s1': -1.00, 'right_w0': -0.67, 
'right_w1': 1.03, 'right_w2': 0.50, 'right_e0': 1.18, 
'right_e1': 1.94}
home_left = {'left_s0': -0.08, 'left_s1': -1.00, 'left_w0': 0.67, 
'left_w1': 1.03, 'left_w2': -0.50, 'left_e0': -1.18, 'left_e1': 
1.94}
The velocity and effort (torque) terms are essentially zero since Baxter's arms are 
not moving. Rounding off the arm joint position values to two places shows that the 
angular positions of the arm joints are as set in the Python script.
We find the type of messages for joint states are from sensor_msgs using 
the command:
$ rostopic type /robot/joint_states
The output is as follows:
sensor_msgs/JointState
To show the home_arms pose for Baxter in Gazebo, follow these steps:
1.	 Go to World | Models | baxter | left_s0.
2.	 Pull the Property window into view by clicking and dragging the three small 
ticks above this panel.

Chapter 6
[ 225 ]
The figure should look like this:
Baxter home position
Choose pose and look at angle_0 -0.0799983484275408. Rounded off, this is 
the left_s0: -0.08 selected in home_arms.py. You can view other information by 
selecting another joint or link of Baxter from the World panel.
Another command shows the position and orientation of the end of the left arm:
$ rostopic echo /robot/limb/left/endpoint_state
The output should be similar to the following:
---
header:
  seq: 62403
  stamp:
    secs: 1249
    nsecs: 653000000
  frame_id: ''
pose:
  position:

Wobbling Robot Arms Using Joint Control
[ 226 ]
    x: 0.582326339279
    y: 0.191017651504
    z: 0.111128161508
  orientation:
    x: 0.131168552915
    y: 0.991040351028
    z: 0.0117205349262
    w: 0.0222814367168
(Other output deleted.)
Yet another way to see the values is to start rqt and select Topics as Plugins and 
Topic Monitor, as shown in the screenshot:
Topic Monitor in rqt for endpoint states

Chapter 6
[ 227 ]
The left arm's endpoint x, y, and z position agrees with the output from the rostopic 
echo command for the left endpoint_state. The right arm endpoint has the same 
x and z positions, but a negative value for y. This indicates that it is to the right of 
Baxter's vertical center line.
Understanding tf
Tf is a transform system used to keep track of the relation between different 
coordinate frames in ROS. The relationship between the coordinate frames is 
maintained in a tree structure which can be viewed. In Baxter's example, the robot 
has many coordinate frames that can be referenced to Baxter's base frame.
Tutorials about tf are given on the ROS wiki at http://wiki.ros.org/tf/
Tutorials.
To demonstrate the use of tf, the following Baxter examples will be provided:
•	
Show tf in rviz after Baxter's arms are moved to a position such that the 
angles of the joints are zero
•	
Show various coordinate frames for Baxter's elements, such as cameras 
or grippers
A program to move Baxter's arms to a zero angle 
position
With Baxter Simulator running (Gazebo), executing the Python script home_arms_
zero.py will move Baxter's arms to a position with all the joint angles zero. 
The following code simply sets the joint angles to zero:
#!/usr/bin/env python   # home_arms_zero.py 10/29/2015
#   
"""
Script to return Baxter's arms to a "home zero" position
"""
# rospy - ROS Python API
import rospy
# baxter_interface - Baxter Python API
import baxter_interface
# initialize our ROS node, registering it with the Master
rospy.init_node('Home_Arms')
# create instances of baxter_interface's Limb class
limb_right = baxter_interface.Limb('right')

Wobbling Robot Arms Using Joint Control
[ 228 ]
limb_left = baxter_interface.Limb('left')
# store the home position of the arms
home_zero_right = {'right_s0': 0.0, 'right_s1': 0.00, 'right_w0': 
0.00, 'right_w1': 0.00, 'right_w2': 0.00, 'right_e0': 0.00, 
'right_e1': 0.00}
home_zero_left = {'left_s0': 0.0, 'left_s1': 0.00, 'left_w0': 
0.00, 'left_w1': 0.00, 'left_w2': 0.00, 'left_e0': 0.00, 
'left_e1': 0.00}
# move both arms to home position
limb_right.move_to_joint_positions(home_zero_right)
limb_left.move_to_joint_positions(home_zero_left)
quit()
Make the Python script executable:
$ chmod +x home_arms_zero.py
Then, run the script:
$ python home_arms_zero.py
The position of the arms can be visualized in Gazebo and the values position, 
velocity, and effort can be displayed. In the following Gazebo window, Baxter 
has arms outstretched at an angle from the torso:
Baxter's joints at zero degrees

Chapter 6
[ 229 ]
The results of the joint states without the velocity and effort are as follows:
---
header:
  seq: 120710
  stamp:
    secs: 2415
    nsecs: 793000000
  frame_id: ''
name: ['head_pan', 'l_gripper_l_finger_joint', 'l_gripper_r_finger_joint', 
'left_e0', 'left_e1', 'left_s0', 'left_s1', 'left_w0', 'left_w1', 'left_w2', 
'r_gripper_l_finger_joint', 'r_gripper_r_finger_joint', 'right_e0', 'right_e1', 
'right_s0', 'right_s1', 'right_w0', 'right_w1', 'right_w2']
position: [2.1480795875383762e-05,
0.02083300010459807, 7.094235804419552e-09,
 -0.0052020885142498585, 0.008648349108503872, -0.0003526224733487737, -
0.004363080957646481, 0.0029469234535000055, 0.004783709772852696, -
0.0022098995590349446,
-4.685055459408831e-10, -0.02083300002921974, 0.005137618938708677, 
0.008541712202397633, 0.0003482148331919177, -0.004308001456631239, -
0.0029103069740452625, 0.004726431947482013, 0.002182588672263286]
Within numerical error tolerance, the values are zero for the arm joint angles.
Rviz tf frames
With Gazebo running and Baxter's arms in zero angles pose, start rviz:
$ rosrun rviz rviz
Now select the parameters for rviz:
1.	 Select the field next to Fixed Frame (under Global Options) and select base.
2.	 Select the Add button under the Displays panel and add Robot Model and 
you will see Baxter appear.

Wobbling Robot Arms Using Joint Control
[ 230 ]
3.	 Select the Add button under the Displays panel and add TF and see all the 
frames, which are too complicated to use.
4.	 Arrange the windows to see the left panel and the figure. Close the Views on 
the right panel.
°°
Expand TF in the Displays panel by clicking on the triangle symbol 
at the left
°°
Under TF, expand Frames by clicking on the left triangle
°°
Uncheck the checkbox next to All enabled
5.	 Now, check left_gripper to display the axes.
The rviz display looks like similar to the following screenshot:
tf transform base and left gripper
You will see the left gripper axes in color on your screen: x is down (red), y is to the 
right (green), and z is forward (blue) in the preceding screenshot.
Now you can choose various elements of Baxter to see the tf coordinate axes.

Chapter 6
[ 231 ]
Viewing a tf tree of robot elements
The program view_frames can generate a PDF file with a graphical representation 
of the complete tf tree. To try the program, Baxter Simulator or the real Baxter 
should be communicating to the terminal window. To run view_frames, use the 
following command:
$ rosrun tf view_frames
In the current working folder, you should now have a file called frames.pdf. 
Open the file with the following command:
$ evince frames.pdf
More information about the tf frames can be found at http://wiki.ros.org/tf/
Tutorials/Introduction%20to%20tf.
Introducing MoveIt
One of the difficult problems in robotics is to define a path for the motion of a robot's 
arms to grasp an object, especially when obstacles may obstruct the most obvious 
path of motion. Fortunately, a ROS package called MoveIt allows us to plan and 
execute a complicated trajectory.
A video created at Rethink Robotics shows how to use MoveIt to plan the motion 
of Baxter's arms and then have MoveIt actually cause a real or simulated Baxter 
to execute the motion. To see the video, go to https://www.youtube.com/
watch?feature=player_detailpage&v=1Zdkwym42P4.
A tutorial is available at the Rethink wiki site at http://sdk.rethinkrobotics.
com/wiki/MoveIt_Tutorial.
First, start the Baxter simulator in Gazebo:
$ cd baxter_ws
$ ./baxter.sh sim
$ roslaunch baxter_gazebo baxter_world.launch
In a second terminal window, untuck Baxter's arms and start the Python script that 
starts the joint_trajectory_action_server:
$ cd baxter_ws
$ ./baxter.sh sim
$ rosrun baxter_tools tuck_arms.py -u
$ rosrun baxter_interface joint_trajectory_action_server.py

Wobbling Robot Arms Using Joint Control
[ 232 ]
The output on the screen should be as follows:
Initializing node...
Initializing joint trajectory action server...
Running. Ctrl-c to quit
In a third terminal, start MoveIt and wait for the response:
$ cd baxter_ws
$ ./baxter.sh sim
$ roslaunch baxter_moveit_config baxter_grippers.launch
The output should end as follows:
. . .
All is well! Everyone is happy! You can start planning now!
Most of the screen text has been deleted here, but wait for the happy ending! 
Looking at the Gazebo window and the MoveIt window, you'll see that Baxter 
looks the same in terms of the positions of the arms:
MoveIt startup

Chapter 6
[ 233 ]
In the figure, which is the rviz screen, the Displays and Motion Planning windows 
are shown on the left with the Context tab information showing. On the right, you 
can see the simulated Baxter in the starting position of MoveIt with arms untucked.
You can select any one of the Displays categories and modify the parameters. For 
example, the figure shows Baxter with a lightened Background Color chosen under 
Global Options.
Under the Motion Planning panel, the tabs Context/Planning/Manipulation/
Scene Objects/Stored Scenes/Stored States/Status are defined in the 
following table:
Tab
Uses
Context
Publish Current Scene and Save scene to a database
Planning
Set the start state, the goal state, and plan and execute moves 
of Baxter's arms
Manipulation
Object detection and manipulation
Scene Objects
Import or export scenes such as pillars or tabletops from a 
disk file
Stored Scenes
Stored scenes on a database
Stored States
Store and load robot states
Status
Status
Planning a move of Baxter's arms with MoveIt
Click on the Planning tab under Motion Planning. On the Planning panel, look 
for the Query field and the heading Select Start State. Click on Select Start State 
to reveal a menu box and set it to <current>. Then, click on the Update button.
•	
Use arrows and rings to move Baxter's simulated arms to the desired 
positions. The desired goal positions should appear in orange in the 
simulation window.
•	
Under the Commands area, choose the Plan button to see the trajectory of 
Baxter's arms in MoveIt.
•	
Choose the Execute button to see Baxter's arms move to the goal positions.

Wobbling Robot Arms Using Joint Control
[ 234 ]
•	
You should see red arms move from the start state to the final (goal) states:
Baxter's arms and goal state for the arms
Next, have the arms move back to original start position to perform another move. 
To do this, under the Query field, click on Select Goal State to reveal the menu box 
and set it to <same as start>. Then, click on the Update button:
Query field to return arms to start positions
Adding objects to a scene
Select the Scene Objects tab from the Motion Planning frame. If you have scenes in 
your computer's directories, you can import them using the Scene Geometry field by 
selecting Import From Text. Alternatively, use the following scene we created in the 
file PillarTable.scene. Make sure that this file is saved as a text file.

Chapter 6
[ 235 ]
(noname)+
* pillar
1
box
0.308 0.13056 0.6528
0.7 -0.01 0.03
0.0108439 0.706876 0.0103685 0.707178
0 0 0 0
* tabletop
1
box
0.7 1.3 0.02
0.7 0.04 -0.13
0 0 0 1
0.705882 0.705882 0.705882 1
.
The first line of dimensions under box in the file represents the height, width, and 
length of the pillar in meters. The next line defines the position from Baxter's origin. 
The third line is the pose of the pillar as a quaternion. The following screenshot 
shows the results of importing the scene elements:
Baxter with tabletop and pillar

Wobbling Robot Arms Using Joint Control
[ 236 ]
To manipulate the objects by moving or rotating, select the object name (not the 
checkbox), and the arrows and rings should appear. Change position with the green 
and blue arrows and rotate with the ring. Moving the Scale slider will change the 
size of the object. Move the mouse to rotate the object and roll the mouse wheel, if 
you have one, to zoom the object's size. You can save the scene (Export as Text) after 
you finish manipulating it.
Next, select the Context tab from the Motion Planning panel and click on the 
Publish Current Scene button under the Planning Library field. This step is 
important and tells MoveIt to plan around the obstacles in the environment!
Position of objects
When pillar is selected under the Current Scene Objects (select the word, not the 
checkbox), values for its Position (XYZ) and Rotation (RPY) appear under Manage 
Pose and Scale. The position of x, y, and z of the centroid of the pillar is shown with 
respect to Baxter's origin. Baxter's x axis extends outward toward the viewer. The 
positive y axis is to the right in the view and the z axis is upward. Note that the roll, 
pitch, and yaw are about the x, y, and z axes, respectively.
Planning a move to avoid obstacles with 
MoveIt
In our example, the left arm is going to move to the other side of the obstacle. MoveIt 
will plan the trajectory so that Baxter's arm will not hit the pillar.
Return to the Planning tab on the Motion Planning panel. First, move Baxter's right 
arm away from the pillar. We can now drag our interactive markers for Baxter's left 
arm to move the goal state to a location on the opposite side of the pillar. Each time 
you click on the Plan button, a different arm trajectory path is shown on virtual 
Baxter. Each path avoids collision with the pillar.
Caution!
We move Baxter's other arm (right arm, in this case) out of the way to 
avoid any possible collisions. This is necessary if the MoveIt trajectories 
are used on the real Baxter. Move the right arm with the markers and 
choose Plan and Execute.

Chapter 6
[ 237 ]
The following screenshot shows Baxter prior to moving the left arm around the 
pillar. Notice that the right arm is moved out of the way:
Baxter's right arm moved aside
Caution!
Using MoveIt with the real Baxter, the arms sometimes move to odd 
positions. If so, move them apart and restart MoveIt.

Wobbling Robot Arms Using Joint Control
[ 238 ]
Now, click on Execute to see Baxter's arm avoid the obstacle and move to the goal 
position on the other side of the pillar. The following screenshot shows Baxter's left 
arm has moved around the pillar to the other side:
Baxter's simulated arm moved to other side of the obstacle
Configuring a real Baxter setup
In the Installing Baxter SDK software section, we loaded the workstation 
computer SDK software used to control Baxter. This control can be through the 
baxter_example programs described previously, or through Python scripts or the 
command line. To control the real Baxter, the baxter.sh script is used to set up the 
environmental variables for configuring the network for Baxter. Ensure that you 
have completed the Configuring Baxter shell section before continuing.
The Baxter Research Robot can be configured to communicate to a development 
workstation computer over various network configurations. An Ethernet network 
is established between Baxter and the workstation computer for bi-directional 
communication. The following figure shows this Ethernet connection and the 
location of various software components on Baxter and on the workstation computer. 
For full descriptions of the various network configurations for Baxter and the 
workstation, refer to the Rethink wiki at http://sdk.rethinkrobotics.com/wiki/
Networking.

Chapter 6
[ 239 ]
Baxter's network setup
To communicate with Baxter, the baxter.sh script must be edited (again) to modify 
the baxter_hostname variable. The default for baxter_hostname is the robot's serial 
number located on the back of the robot next to the power button. An alternative 
method is to assign a new robot hostname using the Field Service Menu (FSM) 
accessed by plugging in a USB keyboard into the back of Baxter. Refer to the Rethink 
wiki for more details (http://sdk.rethinkrobotics.com/wiki/Field_Service_
Menu_(FSM)).

Wobbling Robot Arms Using Joint Control
[ 240 ]
Using Baxter's serial number or assigned hostname within the baxter.sh script, find 
and edit the following line:
baxter_hostname="baxter_hostname.local"
Next, verify that either the your_ip variables OR the your_hostname variable is 
set to specify the IP address or hostname of your workstation computer. The IP 
address should be the one assigned for the Ethernet connection. (When using the 
command $ ifconfig, this would be the IP address associated with eth0 'inet 
addr' field.) The hostname must be resolvable by Baxter to identify the workstation 
computer hostname.
If there is any doubt, use the ping <IP address> or ping <hostname> commands 
to verify that the network communication is working.
Find and modify one of the following variables inside the baxter.sh script:
your_ip="192.168.XXX.XXX"
OR
your_hostname="my_computer.local"
Do not use both of these variables; then your_hostname will take precedence. The 
unused variable should be commented out with a # symbol. Save your changes for 
baxter.sh.
To verify the ROS environment setup for Baxter, it is wise to run the baxter.sh 
script and verify the ROS variables. To do so, use the following commands:
$ cd ~/baxter_ws
$ ./baxter.sh
$ env | grep ROS
Check whether these important fields are set with the correct information:
•	
ROS_MASTER_URI: This should now contain Baxter's hostname
•	
ROS_IP: This should contain the workstation computer's IP address
OR
•	
ROS_HOSTNAME: If not using IP address, this field should contain the 
workstation computer's hostname
Again, a cheat sheet for use with the real Baxter is provided for you to use with the 
example programs that follow. The commands for communicating, enabling, and 
untucking are provided here for your reference:

Chapter 6
[ 241 ]
Real Baxter cheat sheet
To communicate with the real Baxter, use the following commands:
$ cd ~/baxter_ws
$ ./baxter.sh
For subsequent terminal windows, use the following commands:
$ cd ~/baxter_ws
$ ./baxter.sh
Be sure that Baxter is enabled and untucked for the examples using the 
real Baxter:
$ rosrun baxter_tools enable_robot.py –e
$ rosrun baxter_tools tuck_arms.py -u
If there are issues with Baxter's hardware, software, or network, refer to the general 
Baxter troubleshooting website at http://sdk.rethinkrobotics.com/wiki/
Troubleshooting.
If there are problems with the workstation computer setup, refer to the Rethink wiki 
site at http://sdk.rethinkrobotics.com/wiki/Workstation_Setup.
Controlling a real Baxter
The baxter_examples programs described in the subsections within the Launching 
Baxter Simulator in Gazebo section also work on a real Baxter robot. Some additional 
arm control programs that work on a real Baxter but not on Baxter Simulator are 
described in the following sections.
Commanding joint position waypoints
This program is another example of joint position control for Baxter's arms. Baxter's 
arm is moved using the Zero-G mode to freely configure the arm's joints to a desired 
position. When the desired position is attained, the corresponding navigator button 
on the arm is pressed to record the waypoint position. This baxter_examples 
program is executed with the following command specifying either right or left for 
the arm to be moved:
$ rosrun baxter_examples joint_position_waypoints.py -l <right or 
left>

Wobbling Robot Arms Using Joint Control
[ 242 ]
The output should be as follows:
...
Press Navigator 'OK/Wheel' button to record a new joint position waypoint.
Press Navigator 'Rethink' button when finished recording waypoints to begin 
playback.
…
On the navigator, the center button (scroll wheel) is the control used to record 
all seven joint angles of the specified arm's current position. Waypoints can be 
recorded repeatedly until the lower button (button with Rethink icon) is pressed. 
This Rethink button activates the playback mode when the arm will begin going 
back to the waypoint positions in the order that they were recorded. This playback 
mode will continue to loop through the waypoints until the Ctrl + C or Ctrl + Z key 
combination is pressed. Parameters for speed and accuracy can be passed with 
the joint_position_waypoints.py command. Refer to Rethink's wiki site at 
http://sdk.rethinkrobotics.com/wiki/Joint_Position_Waypoints_Example.
Commanding joint torque springs
This baxter_examples program provides an example of Baxter's joint torque 
control. This program moves the arms to a neutral position then applies joint torques 
at 1000 Hz to create an illusion of virtual springs. The program calculates and 
applies linear torques to any offset from the arm's starting position. When the arm is 
moved, these joint torques will return the arm to the start position. Depending on the 
stiffness and damping applied to the joints, oscillation of the joints will occur.
This joint torque springs program is executed with the following command 
specifying right or left for the arm to be manipulated:
$ rosrun baxter_examples joint_torque_springs.py -l <right or left>
The joint torques are configurable using the rqt reconfigure tool. To adjust the torque 
settings, type the following command in a new terminal:
$ rosrun rqt_reconfigure rqt_reconfigure
The following screenshot shows the rqt_reconfigure screen for joint_torque_
springs.py for the left arm:

Chapter 6
[ 243 ]
rqt reconfigure joint torque springs
Select rsdk_joint_torque_springs from the left panel to view the control 
menu. The spring stiffness and damping coefficient can be varied for each joint 
of the arm specified.
Demonstrating joint velocity
Rethink provides a simple baxter_examples program to demonstrate the joint 
velocity control mode for Baxter's arms. This program begins by moving the arms to 
a neutral position. The joint velocity puppet program simply mirrors the movement 
of Baxter's arm when the other arm is moved in Zero-G mode. This baxter_
examples program is executed with the following command specifying either right 
or left for the arm to be moved:
$ rosrun baxter_examples joint_velocity_puppet.py -l <right or left>
A parameter for amplitude can be passed with this command to change the velocity 
applied to the puppet response. For more information on this command, refer to 
Rethink's wiki site at http://sdk.rethinkrobotics.com/wiki/Puppet_Example.

Wobbling Robot Arms Using Joint Control
[ 244 ]
Additional examples
The baxter_examples programs also include programs for gripper control, camera 
control, and analog and digital input/output control. Refer to the Rethink wiki 
Baxter examples program site to get details on these programs: http://sdk.
rethinkrobotics.com/wiki/Examples.
In addition, Rethink offers a series of video tutorials that provide information on 
everything from setting up Baxter to running the example programs. Referring 
to these videos may provide some help if you have problems with executing the 
example programs (http://sdk.rethinkrobotics.com/wiki/Video_Tutorials).
Visual servoing and grasping
One of the greatest features of a real Baxter is the capability to detect and grasp an 
object. This capability is called visual servoing control. Baxter's cuff camera and 
gripper combination makes this a determined objective.
Baxter's cuff camera provides 2D camera images that can be processed by computer 
vision software such as OpenCV. OpenCV provides a vast library of functions for 
processing real-time image data. Computer vision algorithms for thresholding, shape 
recognition, feature detection, edge detection, and many more are useful for 2D 
(and 3D) perception.
An example of visual servoing from the Rethink website is available at http://sdk.
rethinkrobotics.com/wiki/Worked_Example_Visual_Servoing#Code.
This is a basic implementation linking object detection with the autonomous 
movement of the arm to grasp the object. This project is a good example of the 
technique described previously. Unfortunately, this example works with ROS 
Hydro and uses OpenCV functions that have been depreciated.
Only using Baxter's 2D cameras limits the accuracy of grasping objects. The depth 
of objects in the entire scene is hard to determine. Typically, these programs such 
as the one previously mentioned require a setup phase in which an Infrared sensor 
measurement to the table surface is required. An alternative is to use an external 3D 
camera such as the Kinect, ASUS, or Primesense to detect the depth of objects and 
match that information with the RGB camera data. This requires calibrating the two 
image data streams. The Open Source Robotics Foundation has demo software for 
both 2D perception and manipulation and 3D perception at https://github.com/
osrf/baxter_demos.
The calculation of inverse kinematics to move the gripper to the desired location is 
also crucial to this process.

Chapter 6
[ 245 ]
Inverse kinematics
Using forward kinematics, we can determine the position of the gripper at any time. 
The inverse kinematic problem is to place the gripper at a desired location and 
orientation. This requires the calculation of the joint angles then sending Baxter the 
seven joint angles and commanding the arm to move.
Rethink Robotics provides an Inverse Kinematic (IK) example that sets a specific 
endpoint position and orientation and solves for the required joint angles. The 
example and the Python script are described in these websites:
•	
http://sdk.rethinkrobotics.com/wiki/IK_Service_Example
•	
http://sdk.rethinkrobotics.com/wiki/IK_Service_-
_Code_Walkthrough
To run the example to find the joint angles of the left limb (arm) for the fixed position 
and orientation in the Python script, type:
$ rosrun baxter_examples ik_service_client.py -l left
The endpoint position and orientation of the right arm can be found using the same 
command but with the option right. See the code to find the specific pose assigned 
for these examples.
To demonstrate the IK service example using the real Baxter's left arm, we will 
perform the following steps:
1.	 Power up Baxter and untuck both arms.
2.	 Record the endpoint state in position and orientation of the left arm.
3.	 Move Baxter's left arm to an arbitrary position.
4.	 Modify the ik_service_client.py script by entering the position and 
orientation from the untucked left arm position and save the file under a 
different name. This is the home position for Baxter's left arm.
5.	 Execute the script to get the joint angles of the left arm.
6.	 Type the angles into a modified home_arms.py script and execute it.
7.	 Record the new endpoint positions and orientations and compare to the 
original values recorded in step 2.
First, execute the script to move Baxter's arms to the untucked position:
$ cd baxter_ws
$ ./baxter.sh
$ rosrun baxter_tools tuck_arms.py -u

Wobbling Robot Arms Using Joint Control
[ 246 ]
Then, display left arm endpoint pose positions and orientation and record the values:
$ rostopic echo /robot/limb/left/endpoint_state
Our output is as follows:
pose:
  position:
    x: 0.584753440252
    y: 0.195382237943
    z: 0.101405569316
  orientation:
    x: 0.131891460542
    y: 0.990928450502
    z: 0.0117922898402
    w: 0.0229432020794
The gripper of Baxter should be out about 0.6 meters in x, 0.19 meters to the left of 
the vertical centerline in y, and about 0.1 meters up from the base in z. Next, by 
hand move Baxter's arms arbitrarily so that you can test the IK server routine.
To use the IK service with the endpoints of the untucked position and get angles for 
endpoints of the left limb, put the x, y, z values and the Quaternion into the script 
by editing the script ik_service_client.py with the values shown here or use the 
values you obtained:
poses = {
  'left': PoseStamped(
    header=hdr,
    pose=Pose(
      position=Point(
        x: 0.584753440252,
        y: 0.195382237943,
        z: 0.101405569316,
      ),
      orientation=Quaternion(
        x: 0.131891460542,
        y: 0.990928450502,

Chapter 6
[ 247 ]
        z: 0.0117922898402,
        w: 0.0229432020794,
      ),
    ),
  ),
}
After editing ik_service_client.py, you should rename the file. Our new file was 
named ik_home_arms_ch6RealBaxter.py. To made it executable, type:
$ chmod +x ik_home_arms_ch6RealBaxter.py
To run the example to find the joint angles of the left arm that would move Baxter's 
arm to the untucked position, type:
$ python ik_home_arms_ch6RealBaxter.py -l left
The output should be similar to the following:
SUCCESS - Valid Joint Solution Found from Seed Type: Current Joint Angles
IK Joint Solution:
{'left_w0': 0.59208994893573, 'left_w1': 0.991464695921505, 'left_w2': -
0.44186139221262843, 'left_e0': -1.09990187443917, 'left_e1': 
1.9511256097791254, 'left_s0': -0.12534096010690407, 'left_s1': -
1.063859702348023}
------------------
Your results will probably be different but the end position of Baxter's 
arm should be the same as in this Python example that moves the arm to 
the home (untucked) position.
Use the resulting angles to move Baxter arms using the edited Python script 
home_arms.py. Change the values of the left arm joints and save the file with 
a new name. We used the filename MoveLeftArmToHome.py. Make the file 
executable using the command:
$ chmod +x MoveLeftArmToHome.py

Wobbling Robot Arms Using Joint Control
[ 248 ]
Execute the new script and watch Baxter's left arm return to the untucked position, 
if all goes well:
$ python MoveLeftArmToHome.py -l left
Finally, display left arm endpoint pose positions and orientation and record the 
values to compare to the original values in position and orientation:
$ rostopic echo /robot/limb/left/endpoint_state
After Baxter's arm moved, our values were fairly close to the originals:
header:
  seq: 405035
  stamp:
    secs: 1459992006
    nsecs: 87484528
  frame_id: ''
pose:
  position:
    x: 0.581561073269
    y: 0.192636180771
    z: 0.10043051263
  orientation:
    x: 0.134391972572
    y: 0.990591354182
    z: 0.0121901390617
    w: 0.0227808524492

Chapter 6
[ 249 ]
Summary
In this chapter, we described a real and popular robot called Baxter, manufactured 
by Rethink Robotics Corporation. Many of the details of the robot can be discovered 
using Baxter Simulator that displays the simulated Baxter using the Gazebo 
program. Since Baxter has two movable arms, much of the chapter text 
describes the arms and control of the arms.
The chapter started with a description of Baxter in both the research and the 
manufacturing versions. Baxter's arms and sensors and the control modes for 
the arms were described.
After downloading the Baxter Simulator software, the simulator was used to 
demonstrate various examples of control of Baxter with Python scripts supplied by 
Rethink Robotics. Baxter can be controlled by ROS commands, by the keyboard, 
or with a joystick. We have also included several Python scripts that will make the 
control of Baxter easier if joint angles are specified for movement of the arms.
The ROS frame transform package tf was used to show the relationship between the 
coordinate frames of Baxter's base and other elements of the arms. The view in rviz 
displays these frames.
MoveIt is another package that works with simulated Baxter as well as with the 
real robot. MoveIt was explained and the method of planning and executing arm 
trajectories even with obstacles in the path was explained.
Finally, we were introduced to the real Baxter by explaining the setup procedure 
for communication between a workstation and Baxter. Various examples showing 
control of the real Baxter were also discussed.
In the next chapter, a ROS view of quadrotors will be described using both simulated 
and real air vehicles. The simulated air vehicle is a generalized representation of a 
quadrotor and is great for learning to control the craft before you decide to try flying 
a real one.


[ 251 ]
Making a Robot Fly
Today, flying vehicles are very popular. Even in their primary configuration, 
controlled by a radio controller, some flying vehicles can be considered robots that 
respond to their environment to stay in the air. Flying robots that have altitude 
sensors can hover in place. If they have Global Positioning System (GPS) sensors, 
they know where they are and can fly to a specific location. As more sensors are 
added, their capabilities increase.
As you will see in this chapter, there is some commonality between the mobile 
robots and flying robots in their command and control. ROS utilizes this 
commonality in the structure of the nodes, topics, messages, and services of 
these two categories of robots. The topic cmd_vel which was used for Turtlesim 
and TurtleBot earlier in this book is again used for the simulated and real quadrotors 
presented in this chapter. For sensors and devices that are common between mobile 
robots and flying robots, ROS takes advantage of this commonality through its 
standard topic and message interfaces. Just as with other robots, ROS standard topics 
and messages communicate information from the on-board sensors about the state of 
the quadrotor and its environment.
This chapter begins by providing an overview of how a quadrotor works to stay 
in the air. The basic steps of learning to fly a quadrotor are discussed so that you 
can quickly gain expertise in flying. Three ROS quadrotors are introduced: one 
simulated quadrotor, Hector, that exists only in Gazebo and two real quadrotors, a 
tiny Crazyflie and the Bebop. The Hector and Crazyflie quadrotors will be controlled 
through a common Xbox 360 controller interface. The advanced capabilities for 
SLAM and autonomous navigation will be explored. References are also given for 
controlling multiple quadrotors using ROS.

Making a Robot Fly
[ 252 ]
We will cover the following topics in this chapter:
•	
How quadrotors fly
•	
Capability of sensors that are used in quadrotors
•	
Techniques and rules of flying quadrotors
•	
Examples of quadrotors using ROS
•	
Flying the simulated Hector Quadrotor
•	
Flying real quadrotors; Crazyflie and Bebop
Introducing quadrotors
Quadrotors, sometimes called quadcopters, are part of a broad category of robots 
called Unmanned Aerial Vehicles (UAVs) that have four motors and propellers to 
provide lift for the craft. In this chapter, we will introduce some of these flying robots 
that are controlled by ROS. The chapter will consider both simulations and flying the 
real thing.
The following figure shows the Crazyflie quadrotor that will be discussed later in 
this chapter:
Crazyflie quadrotor

Chapter 7
[ 253 ]
In the image of Crazyflie, notice the four propellers or rotors that act to lift the craft 
vertically for takeoff and keep it in flight at some altitude when flying. First, such 
crafts are classified as rotocraft because the lift is generated by the rotors shown in 
the figure, rather than the wings of an airplane. Second, they are not helicopters, 
because the main propeller and the tail rotor control the flight of a helicopter. The 
tail rotor of a helicopter keeps the craft from rotating itself due to the rotation of the 
main horizontal propeller.
With a quadrotor, all the flight maneuvers are made by varying the speed of one or 
more propellers. The propellers are called fixed-pitch propellers, since their angle 
with respect to the quadrotor body cannot be changed. There are more sophisticated 
quadrotors available with propellers that have variable pitch, but they will not be 
considered in this chapter. In helicopters, the main propeller is a variable-pitch 
propeller to control the craft's direction of flight.
For a general discussion of quadrotors, visit https://en.wikipedia.org/wiki/
Quadcopter.
Why are quadrotors so popular?
Quadrotors are a popular option for hobbyists and researchers. They have attractive 
characteristics, primarily:
•	
Relatively low cost compared to other aerial crafts
•	
Not too difficult to fly due to electronic stabilization during flight
•	
Depending on the size and type, some quadrotors can be flown indoors as 
well as outdoors
•	
With the addition of a camera, the quadrotor is excellent for outdoor 
aerial photography

Making a Robot Fly
[ 254 ]
Defining roll, pitch, and yaw
As in any field of robotics, there is a vocabulary which is important for speaking 
precisely about the operations and tasks involved. A flying craft is said to have six 
DOF because the position of the craft can be located in space (x, y, and z coordinates) 
and in orientation with respect to three axes. This position is usually defined in terms 
of the position of a fixed point usually on the ground such as the point of takeoff. The 
figure of the plane here shows how the orientation of the plane is defined in terms of 
angles around the coordinate axes of the airplane itself. The same definition would 
apply to the quadrotor:
Roll, pitch, and yaw for an airplane
Pitch is the movement of the nose, up or down in forward flight. Roll is rotation 
around the longitudinal axis that runs along the length of the aircraft. Yaw is the 
movement of the nose to left or right which is rotation about the vertical axis. The 
amount of rotation is typically stated in degrees.
You can imagine two coordinate systems involved in flight. The 
fixed system usually defined at a ground point and the system 
belonging to the flying craft. The relationship between these 
coordinate systems becomes important for flight maneuvers and 
when making a flight plan.

Chapter 7
[ 255 ]
The important difference between airplane flight control and quadrotor control 
is that in a plane, turns and other maneuvers are controlled by the movement of 
flight surfaces such as ailerons to control roll and a rudder to control yaw. Pitch 
is controlled by the elevator. For fixed-pitch quadrotors, only the speed of the 
propellers can be controlled. This control determines the direction and orientation 
of the quadrotor as well as the speed over the ground.
Turning an aircraft to change its heading is called banking, because this requires the 
aircraft to roll to achieve an angle of bank. When the plane returns to level flight on 
a new heading, the plane would have no pitch, roll, or yaw with respect to its own 
coordinate system during straight and level flight. A quadrotor craft is different in 
that it can turn on its own yaw axis without banking. It can even fly backwards!
How do quadrotors fly?
The quadrotor has two pairs of counter-rotating propellers. When hovering, the 
propellers rotate at the same speed and provide the lift to keep the craft in the air, 
overcoming the pull of gravity. They are counter-rotating to negate the torque that 
would cause the body of the quadrotor to rotate in the opposite direction if only one 
propeller turned. Since the propellers cannot change pitch, the lift vector is always 
in a direction perpendicular to the plane of the rotors. The gravity vector is always 
perpendicular to the ground.
According to our previous discussion of the airplane flying, we expect to control the 
pitch, roll, and yaw of the quadrotor to control banking and the heading as well as 
the forward speed. A throttle control varies the speed of rotation of the propellers. 
For example, in level flight, if the rotational speed of the propellers is increased, the 
craft will rise. Thus, what is often called a throttle on the user's flight controller is 
really an altitude control for a hovering quadrotor. In forward or backward motion, 
the throttle does control speed over the ground.

Making a Robot Fly
[ 256 ]
To move the quadrotor forward, the vehicle must tilt in the forward direction, which 
causes the front of the craft to pitch down. This is done by increasing the rotational 
speed of the rear pair of propellers. As shown in the figure, the lift vector now has 
a component in the forward direction, so the craft moves forward. Thus, the lift 
vector overcomes the downward force of gravity and the drag force caused by the 
air resistance as the quadrotor moves through the air. A component of lift is in the 
forward direction and causes the craft to fly forward. However, the lift component 
opposing gravity is slightly reduced. The amount of lift increase to keep the craft 
level is determined by the flight control software for the quadrotor:
Quadrotor flying forward
The next figure shows a top view of a quadrotor in flight with the forward propeller 
1 rotating clockwise (CW) and propeller 4 rotating counterclockwise (CCW). To 
achieve forward motion, increasing the speed of motor 2 (rotating CW) and motor 3 
(rotating CCW) with respect to motors 1 and 4 will cause the craft to pitch down and 
fly forward:

Chapter 7
[ 257 ]
Quadrotor propellers
If you want to roll the quadrotor, the speed of the propellers on one or other of the 
lateral sides has to be increased, such as motors 2 and 4. To yaw the craft, the speed 
of the two motors across from each other diagonally is decreased, and the other two 
motors are speeded up. This imparts angular torque to the aircraft, which makes it 
turn. An example of this would be to increase the speed of motors 3 and 4.
If you own a quadrotor, notice the pitch (tilt) of the propeller blades on the propellers 
that rotate CW and those that rotate CCW. The upward tilt of the blades causes lift 
by the propeller rotating, with the upward edge cutting into the air.
Unfortunately, determining and setting the rotational speeds of the four propellers 
for such control in flight is quite difficult. Fortunately in practice, the calculations for 
the speed of the propellers are carried out by a microcontroller and the appropriate 
software when commands are given to maneuver the quadrotor. The results of the 
calculations by the microcontroller are output to an electronic motor control unit 
that adjusts the speed of the individual propellers.
The algorithm that converts commands from the ground-based control device to the 
motor controller is typically a PID controller. For those interested in the math, the 
following website explains PID for quadrotors: http://blog.oscarliang.net/
quadcopter-pid-explained-tuning/.

Making a Robot Fly
[ 258 ]
In manual flight, the operator commands various maneuvers such as takeoff, 
forward flight, and landing using a flight-control unit with joystick-like controls. 
Many of the quadrotors also allow control by smart phones, tablets such as iPads, 
and similar devices. The software interfaces for many quadrotors are provided by 
the manufacturer and are unique to that quadrotor. We have chosen quadrotors that 
can be controlled by ROS to illustrate the use of ROS in flying them.
The basic control of the quadrotor from the ground is to command its altitude by 
causing it to rise or descend and control its direction and attitude. The attitude of a 
flying craft represents its pitch and roll or bank with respect to the Earth's horizon. 
The electronic motor control unit of the craft makes the necessary changes or 
corrections to the speed of the propellers to achieve the result commanded.
Components of a quadrotor
The main components for the flight of a quadrotor craft are the frame or body, 
the motors, and the propellers. The body holds on-board flight and motor control 
circuitry, communications circuitry, and a battery. In flight, the quadrotor reports 
its condition and other flight information to the ground-based control device 
using telemetry.
For the quadrotor, telemetry is the wireless transmission of various parameters such 
as the battery condition and the position and orientation of the craft. On the craft, 
there is a set of measuring units called sensors that measure the parameters that are 
encoded and transmitted to the ground-based control device.
Adding sensors
One important application of quadrotors is for aerial photography. Some models 
have the camera built directly into the body. Other models have cameras attached 
under the body. Usually, the camera is controlled from the ground to take pictures 
or videos.
In addition, some quadrotors may have a GPS receiver on board. The position of the 
craft over the earth is available and some quadrotors can be guided through a series 
of GPS waypoints to traverse a selected course.
For more information about the GPS system, visit http://www.gps.gov/systems/
gps/.
There are many models of quadrotors on the market. For a comparison, visit 
http://quadcopterhq.com/best-quadcopters/.

Chapter 7
[ 259 ]
Since manufacturers are producing new quadrotors in finished or kit form, 
you should check websites that review the latest crafts if you are in the market 
to buy one.
Quadrotor communications
There are a number of communication methods that allow quadrotors to be 
controlled for autonomous or manually controlled flight. Here are a few examples:
•	
GPS provides position data for your quadrotor if it has a GPS receiver. Using 
maps for GPS that can be downloaded, you can plot a course for the craft and 
it will fly that course autonomously.
•	
Wi-Fi communication can allow manual control of the quadrotor by smart 
phones and tablets. After downloading the manufacturer's software from 
a website, a screen appears with an image of flight controls that mimic 
joysticks that you can use to fly and control the quadrotor. Data from the 
quadrotor can also be received on these devices. If the drone has a camera, 
the camera view can be seen on these devices. Some quadrotors come with 
their own controllers that usually include joystick-type controls to fly 
the craft.
•	
Bluetooth connection provides another method for transmitting information 
to and from the quadrotor. The range of the signal is limited to 10 meters (32 
feet) for mobile devices.
•	
Some quadrotors may use Radio Frequency (RF) signals to communicate 
with the craft. Radio-controlled crafts such as model airplanes have been 
available for many years. These signals allow for a much longer range 
of communication.
Understanding quadrotor sensors
The on-board flight controller circuitry receives information from sensors that 
provide data about the craft in flight. Some of the possible sensors that determine 
the attitude, altitude, and direction of flight include:
•	
A gyroscope that determines the attitude of the craft including its pitch and 
roll. This indicates the rotational motion of the craft.
•	
An accelerometer that determines the rate of change of velocity of the craft 
with respect to the three axes.

Making a Robot Fly
[ 260 ]
•	
An altimeter or barometer that determines the altitude of the craft above 
ground. At low altitudes, a down-looking sonar sensor may be used to 
determine altitudes up to several meters or more.
•	
A magnetometer that serves as a compass to indicate the craft's direction by 
using the Earth's magnetic field as a reference.
The accelerometer and magnetometer need calibration to initialize their readings to 
the conditions where the flights will take place. For each quadrotor it is therefore 
important to follow the manufacturer's instructions carefully to setup the craft 
before the flights.
Inertial measurement unit
The inertial measurement unit (IMU) is a combined gyroscope and accelerometer. 
This unit will indicate the complete information about the flight characteristics of the 
quadrotor. Typically, the unit will measure the acceleration and orientation of the 
flying craft in all three dimensions.
These sensors allow indoor and outdoor flight. However, all the sensors previously 
mentioned suffer from slight errors that may accumulate during flight, so caution is 
necessary while flying in confined spaces.
Quadrotor condition sensors
Many quadrotors have sensors that will indicate information about their condition, 
including the motor temperature and the percentage of battery charge. This 
information is relayed by telemetry to the ground-based control device.
ROS, with its message passing capability, is ideally suited for such communication 
of sensor messages between the quadrotor and the ground-based control device. 
Various types of ROS sensor messages are listed at http://wiki.ros.org/sensor_
msgs.
Preparing to fly your quadrotor
Some quadrotors can be dangerous if flown carelessly. Depending on the size, 
weight, and power of the quadrotor, collisions with property, people, or pets can 
cause serious damage. At the very least, crashes of your quadrotor can damage 
the craft and end your flying career until you purchase a new one or repair the 
damaged one.

Chapter 7
[ 261 ]
Although this book is not about flying quadrotors, we believe that some discussion 
of good flying practice is necessary. The discussion will be particularly helpful to 
new pilots.
Since this chapter is not about flying the quadrotor but how ROS is used to control 
the craft, we will refer you to various websites on the Internet. Searching for How 
to fly a quadrotor or How to fly a quadcopter will yield over one million hits. Better to 
refine the search and specify the type of quadrotor you wish to fly.
Many websites present articles on flying quadrotors. For example, various tutorials 
are available at the following websites:
•	
http://uavcoach.com/
•	
http://uavcoach.com/how-to-fly-a-quadcopter-guide/
There are also many YouTube videos showing quadrotor or quadcopter 
flying techniques.
Some of the things that should be considered before and during flight are as follows:
•	
Testing your quadrotor
•	
Preflight checklist
•	
Safety and dangers
•	
Rules and regulations
Testing your quadrotor
When your new quadrotor first arrives, it is natural to wish to begin flying 
immediately. Our suggestion is to be patient and take time to familiarize yourself 
with the quadrotor and its flight controller. We found that removing the propellers to 
test the quadrotor indoors was a good way to understand the craft and its controller 
without any danger of crashes.
Also, some practice on a simulator such as Hector (described later) will help you 
understand the flight controls. Remember that the controls will be reversed for 
direction and pitch control when the quadrotor is flying towards you, as compared 
to control when the craft is flying away from you. A little time using the simulator 
will improve your flying ability.

Making a Robot Fly
[ 262 ]
Pre-flight checklist
Any good pilot follows a checklist before flight. Some of the basics are as follows:
•	
Check that the quadrotor is not damaged and its battery is charged.
•	
Make sure that the flight controller is disarmed so the quadrotor cannot take 
off until you are ready.
•	
Make sure the area for flight is clear of obstacles and people.
•	
When flying the quadrotor, always be aware of the surroundings and keep 
the quadrotor is in sight. Flying over people or their private property without 
permission is typically illegal in most countries.
•	
When flying in a public area, inform the police or the appropriate authorities 
that flights will take place. Be sure to keep the craft well away from 
buildings, trees, and people.
•	
If flying using GPS, be sure the GPS satellite signals are locked on before 
flying. It could take several minutes for the on-board GPS receiver to get the 
signals from at least four satellites.
Precautions when flying your quadrotor
When you are learning, start your flights outdoors in a light wind or no wind 
condition. A high wind can cause the quadrotor to fly out of control. Remember 
that if the battery fails, the quadrotor will not glide but will fall straight down. Keep 
aware of the battery percentage charge and bring the quadrotor to its landing point 
when the battery charge is low, below 20 percent to be safe.
Use caution when flying quadrotors
Motors can fail and propellers can break due to a hard landing. 
Communication between the ground-based control device and the 
quadrotor can be interrupted or lost. If the motors or propellers are 
damaged, controlled flight may be impossible. If the battery drains in 
flight, the quadrotor will fall to the ground unless it has a fail-safe mode 
that returns the craft home when the battery is low or communication 
with the quadrotor is lost.

Chapter 7
[ 263 ]
Following the rules and regulations
Quadrotors are considered drones and it is likely that these unmanned aircraft 
systems will be regulated heavily in the future. In the United States, the Federal 
Aviation Administration (FAA) regulates flights and requires registration of some 
craft including quadrotors based on weight of the craft. Around the world, the 
International Civil Aviation Organization (ICAO) works with many countries to 
regulate flights.
The FAA has issued guidelines for flying craft in the general category of Model 
Aircraft with the following guidelines quoted from the website: https://www.faa.
gov/uas/model_aircraft.
•	
Fly below 400 feet and remain clear of surrounding obstacles
•	
Keep the aircraft within visual line of sight at all times
•	
Remain well clear of, and do not interfere with, manned aircraft operations
•	
Don't fly within 5 miles of an airport unless you contact the airport and 
control tower before flying
•	
Don't fly near people or stadiums
•	
Don't fly an aircraft that weighs more than 55 lbs (24.9 kg)
•	
Don't be careless or reckless with your unmanned aircraft – you could be 
fined for endangering people or other aircraft
The ICAO website is at http://www.icao.int/about-icao.
You can also read the book Building Multicopter Video Drones, Ty Audronis, Packt 
Publishing (www.PacktPub.com). The book contains many useful suggestions and 
safety tips for flying quadrotors.
Be aware that requirements might change and probably will, so keep up with the 
latest flying regulations for your quadrotor.

Making a Robot Fly
[ 264 ]
Using ROS with UAVs
The ROS wiki currently contains a growing list of ROS UAVs. These UAVs are 
as follows:
•	
AscTec Pelican and Hummingbird Quadrotors
•	
Berkeley's STARMAC
•	
Bitcraze Crazyflie
•	
DJI Matrice 100 Onboard SDK ROS support
•	
Erle-copter
•	
ETH sFly
•	
Lily Camera/Quadrotor
•	
Parrot AR.Drone
•	
Parrot Bebop
•	
Penn's AscTec Hummingbird Quadrotors
•	
PIXHAWK MAVs
•	
Skybotix CoaX Helicopter
View the list at http://wiki.ros.org/Robots#UAVs in the future for additions to 
this list and the website http://www.ros.org/news/robots/uavs/ to get the latest 
ROS UAV news.
The preceding list contains quadrotors except for the Skybotix Helicopter. A number 
of universities have adopted using the AscTec Hummingbird as their ROS UAV of 
choice. For this book, we present a simulator called Hector Quadrotor and two real 
quadrotors that use ROS: Crazyflie and Bebop.
Introducing Hector Quadrotor
The hardest part of learning about flying robots is the constant crashing. From 
learning flight control for the first time, to testing new hardware or flight algorithms; 
the resulting failures can have a huge cost in terms of broken hardware components. 
To answer this difficulty, a simulated air vehicle designed and developed for ROS 
is ideal.

Chapter 7
[ 265 ]
A simulated quadrotor UAV for the ROS Gazebo environment has been developed 
by the Team Hector Darmstadt of Technische Universität Darmstadt. This quadrotor, 
called Hector Quadrotor, is enclosed in the hector_quadrotor metapackage. This 
metapackage contains the URDF description for the quadrotor UAV, its flight 
controllers, and launch files for running the quadrotor simulation in Gazebo.
Advanced use of the Hector Quadrotor simulation allows the user to record sensor 
data such as Lidar, depth camera, and so on. The quadrotor simulation can also be 
used to test flight algorithms and control approaches in simulation.
The hector_quadrotor metapackage contains the following key packages:
•	
hector_quadrotor_description: This package provides a URDF model 
of the Hector Quadrotor UAV and the quadrotor configured with various 
sensors. Several URDF quadrotor models exist in this package, each 
configured with specific sensors and controllers.
•	
hector_quadrotor_gazebo: This package contains launch files for executing 
Gazebo and spawning one or more Hector Quadrotors.
•	
hector_quadrotor_gazebo_plugins: This package contains three UAV 
specific plugins:
°°
The simple controller gazebo_quadrotor_simple_controller 
subscribes to a geometry_msgs/Twist topic and calculates the 
required forces and torques
°°
A sensor plugin gazebo_ros_baro simulates a barometric altimeter
°°
The plugin gazebo_quadrotor_propulsion simulates the 
propulsion, aerodynamics, and drag from messages containing motor 
voltages and wind vector input
•	
hector_gazebo_plugins: This package contains generic sensor plugins not 
specific to UAVs such as IMU, magnetic field, GPS, and sonar data.
•	
hector_quadrotor_teleop: This package provides a node and launch files 
for controlling a quadrotor using a joystick or gamepad.
•	
hector_quadrotor_demo: This package provides sample launch files that 
run the Gazebo quadrotor simulation and hector_slam for indoor and 
outdoor scenarios.
The entire list of packages for the hector_quadrotor metapackage is given in the 
next section.

Making a Robot Fly
[ 266 ]
Loading Hector Quadrotor
The repository for the hector_quadrotor software can be found at: 
https://github.com/tu-darmstadt-ros-pkg/hector_quadrotor.
The commands provided here will install the binary packages of hector_quadrotor 
into the ROS package repository on your computer. If you wish to install the source 
files, instructions can be found at http://wiki.ros.org/hector_quadrotor/
Tutorials/Quadrotor%20outdoor%20flight%20demo.
For the binary packages, type the following commands to install the ROS Indigo 
version of Hector Quadrotor:
$ sudo apt-get update
$ sudo apt-get install ros-indigo-hector-quadrotor-demo
A large number of ROS packages are downloaded and installed in the 
hector_quadrotor_demo download with the main hector_quadrotor 
packages, providing functionality that should now be somewhat familiar. 
This installation downloads the following packages:
•	
hector_gazebo_worlds
•	
hector_geotiff
•	
hector_map_tools
•	
hector_mapping
•	
hector_nav_msgs
•	
hector_pose_estimation
•	
hector_pose_estimation_core
•	
hector_quadrotor_controller
•	
hector_quadrotor_controller_gazebo
•	
hector_quadrotor_demo
•	
hector_quadrotor_description
•	
hector_quadrotor_gazebo
•	
hector_quadrotor_gazebo_plugins
•	
hector_quadrotor_model
•	
hector_quadrotor_pose_estimation
•	
hector_quadrotor_teleop
•	
hector_sensors_description

Chapter 7
[ 267 ]
•	
hector_sensors_gazebo
•	
hector_trajectory_server
•	
hector_uav_msgs
•	
message_to_tf
A number of these packages will be discussed as the Hector Quadrotor simulations 
are described in the next section.
Launching Hector Quadrotor in Gazebo
Two demonstration tutorials are available to provide simulated applications of the 
Hector Quadrotor for both outdoor and indoor environments. These simulations are 
described in the next sections.
Before you begin the Hector Quadrotor simulations, check your ROS Master using 
the following command in your terminal window:
$ echo $ROS_MASTER_URI
If this variable is set to localhost or the IP address of your computer, no action is 
needed. If not, type the following command:
$ export ROS_MASTER_URI=http://localhost:11311
The preceding command can also be added to your .bashrc file. Delete or comment 
out (with a #) any other commands setting the ROS_MASTER_URI variable.
Flying Hector outdoors
The quadrotor outdoor flight demo software is included as part of the hector_
quadrotor metapackage. Start the simulation by typing the following command:
$ roslaunch hector_quadrotor_demo outdoor_flight_gazebo.launch
This launch file loads a rolling landscape environment into the Gazebo simulation 
and spawns a model of the Hector Quadrotor configured with a Hokuyo UTM-30LX 
sensor. A rviz node is also started and configured specifically for the quadrotor 
outdoor flight. A large number of flight positions and control parameters are 
initialized and loaded into the Parameter Server.

Making a Robot Fly
[ 268 ]
Note that the quadrotor propulsion model parameters for the quadrotor_
propulsion plugin and quadrotor drag model parameters for the quadrotor_
aerodynamics plugin are displayed. Then, look for the message:
Physics dynamic reconfigure ready.
The following screenshots show both the Gazebo and rviz display windows when 
the Hector outdoor flight simulation is launched. The view from the onboard camera 
can be seen in the lower-left corner of the rviz window. If you do not see the camera 
image on your rviz screen, be sure that Camera has been added to your Displays 
panel on the left and the checkbox has been checked. If you would like to pilot the 
quadrotor using the camera, it is best to uncheck the checkboxes for tf and 
robot_model because the visualizations sometimes block the view:
Hector Quadrotor outdoor gazebo view

Chapter 7
[ 269 ]
Hector Quadrotor outdoor rviz view
The quadrotor appears on the ground in the simulation and it is ready for takeoff. 
Its forward direction is marked by a red mark on its leading motor mount. To fly 
the quadrotor, you have to launch the joystick controller software for the Xbox 360 
controller. In a second terminal window, launch the joystick controller software with 
a launch file from the hector_quadrotor_teleop package:
$ roslaunch hector_quadrotor_teleop xbox_controller.launch
This launch file launches the joy_node to process all joystick input from the left 
stick and right stick on the Xbox 360 controller as shown in the following figure. The 
message published by joy_node contains the current state of the joystick axes and 
buttons. The quadrotor_teleop node subscribes to these messages and publishes 
messages on the topic cmd_vel. These messages provide the velocity and direction 
for the quadrotor flight.

Making a Robot Fly
[ 270 ]
Several joystick controllers are currently supported by the ROS joy package, 
including PS3 and Logitech devices. For this launch, the joystick device is accessed as 
/dev/input/js0 and is initialized with a deadzone value of 0.050000. Parameters to 
set the joystick axes are as follows:
* /quadrotor_teleop/x_axis: 5
* /quadrotor_teleop/y_axis: 4
* /quadrotor_teleop/yaw_axis: 1
* /quadrotor_teleop/z_axis: 2
These parameters map to the left stick and the right stick controls on the Xbox 360 
controller shown in the following diagram. The directions of these sticks control are 
as follows:
•	
Left stick:
°°
Forward (Up) is ascend
°°
Backward (Down) is descend
°°
Right is rotate clockwise
°°
Left is rotate counter-clockwise
•	
Right stick:
°°
Forward (Up) is fly forward
°°
Backward (Down) is fly backward
°°
Right is fly right
°°
Left is fly left
Xbox 360 joystick controls for Hector

Chapter 7
[ 271 ]
Now, use the joystick to fly around the simulated outdoor environment! The pilot's 
view can be seen in the Camera image view on the bottom left of the rviz screen.
As you fly around in Gazebo, keep an eye on the Gazebo launch terminal window. 
The screen will display messages as follows depending on your flying ability:
[ INFO] [1447358765.938240016, 617.860000000]: Engaging motors!
[ WARN] [1447358778.282568898, 629.410000000]: Shutting down motors due 
to flip over!
When Hector flips over, you will need to relaunch the simulation.
Within ROS, a clearer understanding of the interactions between the active nodes 
and topics can be obtained using the rqt_graph tool. The following diagram depicts 
all currently active nodes (except debug nodes) enclosed in oval shapes. These nodes 
publish to the topics enclosed in rectangles that are pointed to by arrows:
ROS nodes and topics for Hector Quadrotor outdoor flight demo
The command rostopic list will provide a long list of topics currently 
being published. Other command line tools like rosnode, rosmsg, rosparam, 
and rosservice will help gather specific information about Hector 
Quadrotor's operation.

Making a Robot Fly
[ 272 ]
To understand the orientation of the quadrotor on the screen, use the Gazebo GUI 
to show the vehicle's tf reference frame. Select quadrotor in the World panel on the 
left, and then select the Translation mode on the top Environment toolbar (looks like 
crossed double-headed arrows). This selection will bring up the red-green-blue axis for 
the x-y-z of the tf frame. In the following figure, the x axis is pointing to the left, the y 
axis is pointing to the right (toward the reader), and the z axis is pointing up:
Hector Quadrotor tf reference frame
A YouTube video of hector_quadrotor outdoor scenario demo shows the 
hector_quadrotor in Gazebo operated with a gamepad controller.
You can find the video at: https://www.youtube.com/watch?v=9CGIcc0jeuI.
Flying Hector indoors
The quadrotor indoor SLAM demo software is included as part of the hector_
quadrotor metapackage. To launch the simulation, type the following command:
$ roslaunch hector_quadrotor_demo indoor_slam_gazebo.launch

Chapter 7
[ 273 ]
The following screenshots show both the rviz and Gazebo display windows when 
the Hector indoor simulation is launched:
Hector Quadrotor indoor rviz and Gazebo views
If you do not see this image for Gazebo, roll your mouse wheel to zoom out of the 
image. Then you will need to rotate the scene to a top-down view in order to find the 
quadrotor (Shift + right-click mouse button).
The environment was the offices at Willow Garage and Hector starts out on the floor 
of one of the interior rooms. Just like in the outdoor demo, the xbox_controller.
launch file from the hector_quadrotor_teleop package should be executed:
$ roslaunch hector_quadrotor_teleop xbox_controller.launch
If the quadrotor becomes embedded in the wall, waiting a few seconds should 
release it and it should (hopefully) end up in an upright position ready to fly again. 
If you lose sight of it, zoom out from the Gazebo screen and look from a top-down 
view. Remember that the Gazebo physics engine is applying minor environment 
conditions as well. This can create some drifting out of its position.

Making a Robot Fly
[ 274 ]
The rqt graph of the active nodes and topics during the Hector indoor SLAM demo 
is shown in the following figure. As Hector is flown around the office environment, 
the hector_mapping node will be performing SLAM and be creating a map of 
the environment:
ROS nodes and topics for Hector Quadrotor indoor SLAM demo
The following screenshot shows Hector Quadrotor mapping an interior room of 
Willow Garage:
Hector mapping indoors using SLAM
The 3D robot trajectory is tracked by the hector_trajectory_server node and can 
be shown in rviz. The map along with the trajectory information can be saved to a 
GeoTiff file with the following command:
$ rostopic pub syscommand std_msgs/String "savegeotiff"
The savegeotiff map can be found in the hector_geotiff/map directory.
A YouTube video of hector_quadrotor stack indoor SLAM demo shows the 
hector_quadrotor in Gazebo operated with a gamepad controller.
https://www.youtube.com/watch?v=IJbJbcZVY28.

Chapter 7
[ 275 ]
Now, let's take a look at real quadrotors. In this chapter, we evaluated the entire 
spectrum of quadrotors which interface with ROS that were available at the time. 
At the bottom of the price range, the Crazyflie was an easy pick due to its small size 
and the advantage of flying it indoors. The small motors cause the propellers to 
spin at high RPM, but the propellers are soft and compliant. Because of the vehicle's 
low weight, damage to property, people, or the vehicle itself is usually minimal. In 
addition, replacement parts are inexpensive.
Introducing Crazyflie 2.0
Crazyflie is a quadrotor that is classified as a micro air vehicle (MAV), as it only 
weighs 27 grams and can fit in your hand. It was developed and manufactured 
by Bitcraze AB and comes as a kit—ready to assemble with no soldering required. 
The designers of Crazyflie wanted to build a small flying electronic board that 
had minimal mechanical parts. They provide it and support it as an open source 
development platform, encouraging others to contribute to both Crazyflie hardware 
and software development. For this book, the Crazyflie 2.0 model was used for 
testing purposes, but the Crazyflie ROS software indicates that it is compatible with 
both Crazyflie 1.0 and 2.0 models embedded with stock firmware. The Crazyflie 
2.0 model is shown in the following figure and is the model referred to in the rest 
of this chapter:
Crazyflie 2.0

Making a Robot Fly
[ 276 ]
The size of the assembled Crazyflie is 92 mm (3.6 inches) square motor to motor and 
29 mm (1.1 inches) high.
The Crazyflie 2.0 system design is based on dual microcontroller architecture, as 
shown in the following diagram. The STM32F405 Microcontroller Unit (MCU) 
controls the flight of the Crazyflie through power to the motor driver. The STM32 
also reads data from the 10-DOF IMU which includes a three-axis accelerometer, 
three-axis gyroscope, three-axis magnetometer and a high precision pressure sensor. 
It can control telemetry to other components through the expansion port. The other 
MCU is a Nordic Semiconductor nRF51822. The main tasks of this MCU are to 
handle the radio communication and the power management for the Crazyflie. Both 
Bluetooth Low Energy (BLE) and Compressed Real-Time Protocol (CRTP) are 
supported by the nRF51. The CRTP mode is compatible with radio communication 
provided by the Crazyradio. The nRF51 also handles logic from the on/off button 
and power to the other components in the system:
The Crazyflie is charged through an on-board μUSB port and requires a 40-minute 
charge cycle for the stock battery. Flight time is up to 7 minutes.
The Crazyflie 2.0 is priced under $ 200 and is available at www.seeedstudio.com. Its 
local distributors are listed at the website www.bitcraze.io/distributors.

Chapter 7
[ 277 ]
Bitcraze provides excellent documentation to take you from unpacking your 
Crazyflie to getting it in the air. Instructions for assembling Crazyflie and getting it 
ready to fly can be found at https://www.bitcraze.io/getting-started-with-
the-crazyflie-2-0/.
For any problems that arise, refer to the support forum at https://forum.
bitcraze.io/.
Controlling Crazyflie without ROS
Crazyflie can be controlled through a number of host devices over BLE or a 
Crazyradio communication channel. A BLE link to Crazyflie limits the flying 
range to 20 meters. The best communications link, Crazyradio PA, extends the 
communication range to 1000 meters. Either communication method is supported 
by the Crazyflie Python client (PC) software available for computers running 
Linux, Windows, or MAC OS. Bitcraze has also configured its own Virtual Machine 
(VM) that imports into the Oracle VirtualBox to make it easy to start on Crazyflie 
development projects. For more information on the Bitcraze VM, visit https://
wiki.bitcraze.io/projects:virtualmachine:index.
For the latest release of the Crazyflie PC client, visit https://github.com/
bitcraze/crazyflie-clients-python/releases.
Instructions for installing the Crazyflie PC client can be found at https://github.
com/bitcraze/crazyflie-clients-python.
The Crazyflie PC client can be used to upgrade and flash the Crazyflie firmware over 
the Crazyradio link. Refer to the following website for instructions: https://wiki.
bitcraze.io/projects:crazyflie2:upgrading:index.
Application software also exists for controlling the Crazyflie from an Android OS 
(4.4 or newer) or iOS (7.1 or newer) device. Refer to the appropriate app store for the 
Crazyflie software:
For Android users: https://play.google.com/store/apps/details?id=se.
bitcraze.crazyfliecontrol2.
For iOS users: https://itunes.apple.com/us/app/crazyflie-2.0/
id946151480?mt=8.

Making a Robot Fly
[ 278 ]
For Raspberry Pi enthusiasts, check out the client version of software for Raspbian at 
https://wiki.bitcraze.io/projects:crazyflie:binaries:raspberrypi#down
load.
For user input control, Crazyflie can use any gamepad with a minimum of four 
analog axes. The primary joystick controllers are the Xbox 360 USB controller, the PS 
3 USB controller and the PS4 USB controller. Other controllers can be configured to 
work with the Crazyflie by modifying the Crazyflie client software.
Communicating using Crazyradio PA
In order to use Crazyflie 2.0 with ROS, a Crazyradio or Crazyradio PA is necessary 
to provide wireless radio communication to the Crazyflie. The Crazyradio resides 
on a small circuit board mounted on a USB dongle to interface with a computer, 
tablet, or smart phone. It has radio amplification to 20 dBm power output and a 
line-of-sight (LOS) range of greater than 1 kilometer. The design is based on the 
nRF24LU1+ chip from Nordic Semiconductor that operates at the 2.4 GHz band of 
radio communications. It provides 125 radio channels and offers 2 Megabits per 
second (Mbps), 1Mbps, and 250Kbps communication data rates. Firmware for the 
Crazyradio is open source and is upgradeable through a bootloader that comes 
embedded with the hardware.
The following figure shows the Crazyradio PA with its antenna extended. Additional 
information on the Crazyradio PA can be found at https://www.bitcraze.io/
crazyradio-pa.
For the latest stable release of the Crazyradio firmware, go to https://github.com/
bitcraze/crazyradio-firmware/releases.
Crazyradio PA
The next sections will focus on using ROS to command and control the flight of the 
Crazyflie quadrotor. A ROS metapackage for the Crazyflie has been developed by 
Wolfgang Hoenig with current information found at the GitHub site: https://
github.com/whoenig/crazyflie_ros.
We thank Mr. Hoenig for his generous support in helping us prepare this section.
This software supports both the Crazyflie 1.0 and the Crazyflie 2.0 quadrotors 
embedded with stock firmware.

Chapter 7
[ 279 ]
Loading Crazyflie ROS software
The Crazyflie ROS software for Crazyflie can be added to the catkin workspace 
catkin_ws created in Chapter 1, Getting Started with ROS, and then used in Chapter 
2, Creating Your First Two-Wheeled ROS Robot (in Simulation), for our package 
ros_robotics. However, we provide instructions here to install the crazyflie 
metapackage into a catkin workspace of its own identified as crazyflie_ws. This 
workspace will provide you with the chance to examine the existing Crazyflie ROS 
packages and be a development space for new software you might wish to create. 
If you wish to use the catkin_ws workspace for your Crazyflie ROS software, skip 
to the step where the command sudo apt-get update is issued. (Afterwards, 
remember to replace the crazyflie_ws name in each of the command lines with 
catkin_ws.)
To create the Crazyflie workspace crazyflie_ws, type the following commands:
$ mkdir -p ~/crazyflie_ws/src
$ cd ~/crazyflie_ws/src
$ catkin_init_workspace
Build and install the Crazyflie workspace:
$ cd ~/crazyflie_ws
$ catkin_make
Next, source the setup.bash file within the Crazyflie workspace to overlay this 
workspace on top of the ROS environment for the workstation:
$ source ~/crazyflie_ws/devel/setup.bash
Remember to add this source command to your .bashrc file:
$ echo "source ~/crazyflie_ws/devel/setup.bash" >> ~/.bashrc
Make sure the ROS_PACKAGE_PATH environment variable includes the path you 
just sourced:
$ echo $ROS_PACKAGE_PATH
The path /home/<username>/crazyflie_ws/src should be displayed as one of the 
paths on the screen.
Now that the Crazyflie catkin workspace has been created, update the system 
information on the newest versions of packages and their dependencies:
$ sudo apt-get update

Making a Robot Fly
[ 280 ]
Next, move to the Crazyflie workspace source directory and download the software 
from GitHub:
$ cd ~/crazyflie_ws/src
$ git clone https://github.com/whoenig/crazyflie_ros.git
Move to the root of the Crazyflie workspace and build the packages:
$ cd ~/crazyflie_ws
$ catkin_make
These instructions install the latest version of the crazyflie metapackage, which 
has no dependencies on the Bitcraze Crazyflie SDK. To be able to fly Crazyflie 
with a joystick, ensure that the hector_quadrotor software is also installed. The 
instructions were presented earlier in this chapter.
Important!
Check your .bashrc file to make sure that no ROS_MASTER_URI 
variables are set. If they exist, comment them out with a #.
As an alternative, use the following command:
$ export ROS_MASTER_URI=http://localhost:11311
ROS packages from the crazyflie metapackage are described here:
•	
crazyflie_controller: This package contains a PID controller used during 
autonomous navigation operations like hovering and waypoint navigation. It 
is used with an external motion capture system like VICON.
•	
crazyflie_cpp: This package contains a C++ library for Crazyflie and for 
Crazyradio.
•	
crazyflie_demo: This package contains a varied set of launch files and 
Python scripts to provide the user with sample operations of Crazyflie. 
Use these files as a starting point to create your own operational setup for 
Crazyflie.
•	
crazyflie_description This package provides the URDF file and mesh 
files to create a simulation model of the Crazyflie. This model is based on the 
Crazyflie 1.0.
•	
crazyflie_tools: This package contains helpful tools for Crazyflie. The 
only tool at the present time is scan, which scans for Crazyflie(s) and reports 
their URI(s).

Chapter 7
[ 281 ]
•	
crazyflie_driver: This package contains two important launch files. 
The crazyflie_server.launch file launches the crazyflie_server 
node. The crazyflie_server node communicates with all Crazyflies that 
have been dynamically added via crazyflie_add. The crazyflie_add.
launch file launches the crazyflie_add node to establish communication 
with a Crazyflie. These operations are explained in more detail in 
subsequent sections.
Setting up udev rules for Crazyradio
On the computer workstation, Ubuntu uses udev to manage system devices and 
dynamically create and remove device nodes in the /dev directory to handle external 
devices. When the Crazyradio USB dongle is plugged in, udev is notified and special 
system configuration rules, udev rules, link any user-defined device information. 
These udev rules are stored in a file in the directory /etc/udev/rules.d.
The following instructions create rules that will set permission for a user to use the 
Crazyradio without requiring root privileges.
The following steps require Superuser (root) or Administrator privileges. 
If the sudo program is not available on your computer, use the following 
two commands as root:
$ su –
$ apt-get install sudo
The following commands create the group plugdev to which users can be added 
who wish to communicate with Crazyflie via the Crazyradio dongle:
$ sudo groupadd plugdev
$ sudo usermod -a -G plugdev <username>
Next, the udev rules are created to provide vendor and product identification and 
user privileges for the Crazyradio and the Crazyflie. Using the command sudo 
<editor> to access your favorite editor, create the file /etc/udev/rules.d/99-
crazyradio.rules and within the file add the following line:
SUBSYSTEM=="usb", ATTRS{idVendor}=="1915", ATTRS{idProduct}=="7777", 
MODE="0664", GROUP="plugdev"

Making a Robot Fly
[ 282 ]
Using sudo <editor>, create the file /etc/udev/rules.d/99-crazyflie.rules 
and add the following line:
SUBSYSTEM=="usb", ATTRS{idVendor}=="0483", ATTRS{idProduct}=="5740", 
MODE="0664", GROUP="plugdev"
Restart the computer and prepare to communicate with your Crazyflie!
Pre-flight check
It is advisable to mark the right front (M1) leg of the Crazyflie with something bright. 
This marking will help you keep track of the forward direction for your Crazyflie. 
Due to the symmetrical design, it is easy to lose the orientation of the Crazyflie.
The Crazyflie should be placed on a stable surface. As you will see, the startup 
sequence involves spinning the motors and sensor calibration. In the following figure 
of Crazyflie, identify the on/off power button (near the M1 leg) and push it on.
Caution!
The power button is a push button not a slide switch.
Top view of Crazyflie

Chapter 7
[ 283 ]
After the Crazyflie is powered on, the power-on sequence will spin all four 
propellers in order. If a propeller does not spin, be sure to check the motor 
connections. The blue LED lights on the rear of the UAV should be lit to indicate that 
the power is on. The front right red LED light should be pulsing at 1 Hz to indicate 
the vehicle's heartbeat. When Crazyradio communication is established with the 
Crazyflie, the red and green LED lights on the front left will be flashing to indicate 
communication is being exchanged.
Next, insert the Crazyradio into a USB 2.0 slot on your computer and the joystick 
controller into another USB slot. Check that the joystick knobs are in their center 
positions so that the throttle is set to zero. You are now ready to start the Crazyflie 
ROS software.
Flying Crazyflie with teleop
Before flying Crazyflie for the first time, it is recommended to tie the UAV to a heavy 
object, attaching string to the mounting holes on either side of the Printed Circuit 
Board (PCB) body.
Remember that the Crazyflie can climb to 50 meters when 
flying at full throttle. This height is dangerous if the 
battery runs out of power.
The front right LED will be a steady red light when the 
battery is on low power. It is advisable to land to prevent 
damage to the Crazyflie.
Before communicating with your Crazyflie, you need to find the Uniform Resource 
Identifier (URI). This URI is associated with the communication protocol of the 
nRF51 MCU. The format for the URI is as follows:
Interface Type:// Interface Id/Interface Channel/Interface Speed
For the radio interface, this sequence is as follows:
radio://USB dongle number/radio channel number/radio speed
The package crazyflie_tools provides the program scan to identify all the URIs 
that are transmitting. Open a terminal window on your computer workstation and 
enter the following command:
$ rosrun crazyflie_tools scan

Making a Robot Fly
[ 284 ]
The output will be similar to the following two lines:
Configured Dongle with version 0.53
radio://0/80/250K
The URI for your Crazyflie may be different. For our Crazyflie, the USB dongle 
number is 0, the radio channel is 80, and the radio speed is 250K. This means that 
our communication link is over a 2480 MHz channel.
The package crazyflie_demo contains a spectrum of example launch files to use 
with Crazyflie. Launch files are given for using the Xbox 360 controller or the PS3 
controller to teleoperate the Crazyflie. To fly your Crazyflie with an Xbox 360 
controller, type the command:
$ roslaunch crazyflie_demo teleop_xbox360.launch uri:=<CrazyflieURI>
Replace <CrazyflieURI> with the URI found from the scan command. In the screen, 
look for the message:
SYS: Crazyflie is up and running!
You should see something similar to this:
Starting screen for the teleop_xbox360.launch

Chapter 7
[ 285 ]
The window on the left is running rviz and the two rqt_plot windows on the right 
are for temperature and battery data. In rviz, IMU sensor data will be displayed on 
the grid.
If you are anxious to feel the joystick controls in your hands, proceed to using the left 
and right stick controls to navigate your Crazyflie. The stick controls are similar to 
those used for Hector Quadrotor:
•	
Left Stick – Forward (Up): Throttle provides lift
•	
Left Stick – Backward (Down): No throttle
•	
Left Stick – Right: Clockwise yaw
•	
Left Stick – Left: Counter clockwise yaw
•	
Right Stick – Forward (Up): Pitch forward
•	
Right Stick – Backward (Down): Pitch backward
•	
Right Stick – Right: Roll right
•	
Right Stick – Left: Roll left
You are now ready to use the Xbox 360 joystick to fly your Crazyflie around!
Details of teleop_xbox360.launch
The teleop_xbox360.launch file performs a number of operations to launch ROS 
nodes, pass arguments, and set ROS parameters. The following list highlights the 
tasks performed:
•	
The crazyflie_server.launch file in the crazyflie_driver package is 
executed. This file launches the node crazyflie_server. This server handles 
communication with the Crazyflie as soon as it is dynamically added by the 
crazyflie_add.launch file.
•	
The crazyflie_add.launch file in the crazyflie_driver package 
is executed. This file launches the node crazyflie_add using the URI 
parameter that is passed into teleop_xbox360.launch. Other parameters 
that are set are as follows:
°°
tf_prefix: This is the tf prefix for the Crazyflie frame(s)
°°
roll_trim: This is the roll trim in degrees (negative value if drift is to 
the left)
°°
pitch_trim: This is the pitch trim in degrees (negative value if drift 
is forward)
°°
enable_logging: This is the flag to log data

Making a Robot Fly
[ 286 ]
•	
The joy node is launched to handle the joystick controller input.
•	
The xbox360.launch file is included to launch the node quadrotor_teleop 
(in the hector_quadrotor_teleop package) and set the parameters for 
the joystick controller. These parameters include x_axis, y_axis, z_axis, 
yaw_axis, x_velocity_max, y_velocity_max, z_velocity_max, and yaw_
velocity_max.
•	
The crazyflie_demo_controller node in the crazyflie_demo package is 
launched via the script controller.py.
•	
The rviz node in package rviz is launched and the crazyflie.rviz 
configuration file in the crazyflie_demo package launch directory is used to 
configure rviz.
•	
Two rqt_plot nodes from package rqt_plot are launched. One node plots 
the temperature of the Crazyflie, and the other node plots the battery level.
Take a look at this figure to see some of the relationships between the nodes and 
topics. The large rectangle marked Crazyflie is the namespace for the enclosed 
nodes and topics:
Crazyflie ROS teleop nodes and topics
A number of ROS parameters are initialized at the start of this program. Of particular 
interest are the roll_trim and pitch_trim parameter values set by the crazyflie_
add.launch file.

Chapter 7
[ 287 ]
When first flying your Crazyflie, use the roll_trim and pitch_trim values in 
the crazyflie_add.launch file to adjust the response of your craft. Begin with a 
straight forward push on the Left Stick of the joystick (throttle). The Crazyflie should 
lift straight up into the air. If not, adjust these two parameters accordingly and try 
it again. (The battery may also be moved forward and back to change the pitch 
balance. It is recommended to mark the battery position when you get it into the 
desired spot.)
Prior to flying the Crazyflie, using the rostopic list command allows us to see the 
names of the topics that are available. Of primary importance is the /crazyflie/
cmd_vel topic that is similar to the same topic for Hector. This topic is published by 
the quadrotor_teleop node and the data fields for this topic are as follows:
•	
angular.z: The yaw rate value is -200 to 200 degrees per second
•	
linear.x: The pitch value is -30 to 30 degrees
•	
linear.y: The roll value is -30 to 30 degrees
•	
linear.z: The thrust value is 10,000 to 60,000 for pulse-width modulation 
(PWM) output
The crazyflie_server node publishes sensor data from the Crazyflie as 
sensor_msgs:
•	
The imu topic contains gyroscope and accelerometer data and updates 
every 10 milliseconds (ms). The covariance matrices and orientation fields 
are not set.
•	
The temperature topic contains data from the barometer in degrees Celsius. 
This message is updated every 100 ms.
•	
The magnetic_field topic contains data from the magnetometer and is 
updated every 100 ms.
•	
The pressure topic contains readings from the pressure sensor in 
hectopascals (hPa) or millibars (mbar). This message is updated every 
100 ms.
•	
The battery topic contains readings from the battery in volts and is 
updated every 100 ms. The stock battery is a 3.7 volt 240 mAh Lithium 
Polymer battery.
From this point, feel free to try the rostopic echo command with any of the topics 
listed previously to see the data being passed, especially between the computer 
and the Crazyflie. The ROS tool rqt can also be used to monitor these topics 
(Topic Monitor) and publish messages (Message Publisher) to the Crazyflie.

Making a Robot Fly
[ 288 ]
Flying with a motion capture system
The crazyflie_demo package contains launch files to use the Crazyflie with 
external motion capture systems like VICON or Virtual-Reality Peripheral Network 
(VRPN). With an external motion capture system, you can get your Crazyflie to 
hover at a given location or navigate autonomously to a set of waypoints. For 
example, to get Crazyflie to hover a meter above its starting location using a VICON 
system, use the following command:
$ roslaunch crazyflie_demo hover_vicon.launch uri:=<CrazyflieURI>
frame:=<CrazyflieTFframe> x:=0 y:=0 z:=1
Replace <CrazyflieURI> with your Crazyflie's URI and <CrazyflieTFframe> is 
the tf-frame of your Crazyflie. The hover_vicon.launch file will automatically run 
vicon_bridge.
Flying multiple Crazyflies
The crazyflie_demo package also contains launch files for flying multiple Crazyflies 
at the same time. To accomplish this, each Crazyflie must have a different address. 
If Crazyflies are to share communication on the same Crazyradio USB dongle, they 
should share the same channel and data rate to provide optimum performance. 
Up to three Crazyflies have been successfully flown over one Crazyradio dongle. 
Performance degrades as the number of Crazyflies increase due to limitations 
in bandwidth.
To command and control multiple Crazyflies using the Xbox 360 joystick controller, 
use the following command:
$ roslaunch crazyflie_demo multi_teleop_xbox360.launch 
uri1:=radio://0/100/2M/E7E7E7E7E7 uri2:=radio://0/100/2M/E7E7E7E705
The URIs used in the multi_teleop_xbox360.launch command are left as 
examples of the duplication of the radio channel and data rate but the difference in 
the address.
The authors have not performed the commands in these last two sections 
because at present we have only one Crazyflie and no external motion 
capture system. We have included these examples here to entice you to 
extend your Crazyflie experience.

Chapter 7
[ 289 ]
Introducing Bebop
The following image shows the Parrot Bebop quadrotor craft that will be discussed 
in this section. This quadrotor weighs 400 grams and has more advanced features 
than the Crazyflie. Its cost is around $ 500.
Bebop
Parrot is a company with headquarters in Paris, France, that produces products 
such as the Bebop quadrotor. The Bebop is described in detail at their website, 
http://www.parrot.com/usa/products/bebop-drone/.
The Bebop can be controlled by its own Skycontroller or by using smartphones or 
tablets. The Skycontroller is described at the following site: http://blog.parrot.
com/2014/12/15/how-to-pilot-skycontroller/.
Bebop has on-board sensors for autonomous flight through the use of GPS for 
guidance. The Bebop also has a forward-looking camera for aerial photography. The 
Wi-Fi communications module of Bebop allows manual control and control by a ROS 
package called bebop_autonomy. The next section will concentrate on the use of the 
ROS software to control Bebop.
Important!
Before flying the Bebop or using the ROS interface, visit the Bebop website, 
and download and install the latest firmware version for the Bebop.

Making a Robot Fly
[ 290 ]
The bebop_autonomy software is the ROS driver for Parrot Bebop quadrotor 
based on the Parrot AR.Drone SDK3 development kit. This driver was developed 
at Autonomy Lab of Simon Fraser University by Mani Monajjemi. We thank 
Mr. Monajjemi for his generous assistance in helping us prepare this section.
Loading bebop_autonomy software
The http://www.ros.org/ website announced the new package bebop_autonomy 
as a ROS driver for Parrot Bebop Drone:
http://www.ros.org/news/2015/09/new-package-bebop-autonomy-ros-
driver-for-parrot-bebop-drone.html.
The instructions for loading the ROS software and using it for Bebop are well 
described at http://bebop-autonomy.readthedocs.org/en/latest/.
http://bebop-autonomy.readthedocs.org/en/latest/installation.html.
The features to be incorporated into bebop_autonomy and the status of these features 
can be found in the documentation found at: https://media.readthedocs.org/
pdf/bebop-autonomy/latest/bebop-autonomy.pdf.
To load the bebop_autonomy software, first get the required Ubuntu packages with 
the following command:
$ sudo apt-get install build-essential python-rosdep python-catkin-
tools
Create the Bebop workspace bebop_ws and download software from GitHub:
$ mkdir -p ~/bebop_ws/src
$ cd ~/bebop_ws
$ catkin init
$ git clone https://github.com/AutonomyLab/bebop_autonomy.git 
src/bebop_autonomy
Use rosdep to install the bebop_autonomy package dependencies:
$ rosdep update
$ rosdep install --from-paths src -i
Then, build the workspace:
$ catkin build -DCMAKE_BUILD_TYPE=RelWithDebInfo

Chapter 7
[ 291 ]
Be patient as this may take up to 15 minutes on some systems.
Get the Bebop teleop tools with the following commands:
$ cd ~/bebop_ws/src
$ git clone https://github.com/ros-teleop/teleop_tools.git
Then, repeat the update, install, and build steps again:
$ cd ~/bebop_ws
$ rosdep update
$ rosdep install --from-paths src -i
$ catkin build -DCMAKE_BUILD_TYPE=RelWithDebInfo
Once the software is loaded and the files are built, add the following statement in the 
.bashrc file:
source ~/bebop_ws/devel/setup.bash
Alternatively, issue the following command at a terminal:
$ source ~/bebop_ws/devel/setup.bash
Remember, adding the statement to the .bashrc file will apply it to each new 
terminal window opened. The source command at the terminal will only apply to 
that terminal window.
Also, check your ROS package path with the following command:
$ echo $ROS_PACKAGE_PATH
The path /home/<username>/bebop_ws/src should be displayed as one of the paths 
on the screen.
The software download creates a number of packages that are used to control Bebop. 
The bebop_autonomy package is actually a metapackage with a package.xml file 
that lists three other packages as follows:
•	
bebop_driver that contains C++ code for the node bebop_driver_node
•	
bebop_msgs that contains messages used with Bebop
•	
bebop_tools that contains miscellaneous tools for Bebop
Another package teleop_tools is not considered in this book.

Making a Robot Fly
[ 292 ]
Preparing to fly Bebop
Before flying Bebop, review the Bebop User's Guide available at 
http://www.parrot.com/usa/support/parrot-bebop-drone/.
Read the instructions in that document for the following points:
•	
Assembling the Bebop quadrotor, charging the battery, and other 
preliminary matters
•	
Pay attention to the section on Preflight Check that includes many safety tips
•	
Study the section on flying that includes the instructions for calibrating the 
magnetometer of the Bebop
Testing Bebop communications
After a pre-flight check and clearing the flying area of people and obstructions, 
power on Bebop and wait for several minutes.
Using the Systems Settings tab in the Ubuntu desktop, select Network and check 
your wireless connections to see if your computer is communicating with Bebop. 
You should see the Bebop network BebopDrone-<xxxxxxx>. The numbers indicate 
the identification number of your Bebop.
Then, test the communications between the computer workstation and the Bebop by 
pinging the craft. From a terminal window, issue the ping command to the Bebop 
and wait for the response:
$ ping 192.168.42.2
The output is as follows:
PING 192.168.42.2 (192.168.42.2) 56(84) bytes of data.
64 bytes from 192.168.42.2: icmp_seq=1 ttl=64 time=0.012 ms
64 bytes from 192.168.42.2: icmp_seq=2 ttl=64 time=0.016 ms
The IP address here is for the Bebop tested. The IP address should be 192.168.42.x 
where the x value depends on the Bebop.
Flying Bebop using commands
Now, it is time to launch the bebop_driver node and use commands to perform a 
takeoff and landing. Launch Bebop with the following command:
$ roslaunch bebop_driver bebop_node.launch

Chapter 7
[ 293 ]
Explore the nodes and topics:
$ rosnode list
The output should be as follows:
/bebop/bebop_driver
/rosout
Then type the following command:
$ rostopic list
This will produce a long list, in which the following topics are of interest in getting 
Bebop to take off and land:
/bebop/takeoff
/bebop/land
/bebop/reset
/bebop/navigate_home
Note that in both the lists, the namespace (ns) is bebop and items are listed as 
/bebop/<node> or /bebop/<topic>.
Take off
Watch it fly. Bebop goes straight up 1 meter after you execute the following 
command:
$ rostopic pub /bebop/takeoff std_msgs/Empty --once
The output should be as follows:
publishing and latching message for 3.0 seconds
The following rqt_graph shows the topic /bebop/takeoff sent to the node /bebop/
bebop_driver to start the takeoff process. The /rostopic_14655.. node on the left 
represents the rostopic command issued for takeoff.
Bebop takeoff node graph

Making a Robot Fly
[ 294 ]
Landing
Bebop will land when you issue this command:
$ rostopic pub /bebop/land std_msgs/Empty --once
The output is as follows:
publishing and latching message for 3.0 seconds
There are many other options for the Bebop that are not covered here. Refer to the 
Bebop website and the instructions for the bebop_autonomy software previously 
listed to explore all of Bebop's functions.
Summary
This chapter featured information about flying robots that are described as 
quadrotors or quadcopters because of their four propellers. The first few sections of 
the chapter described the quadrotors and their flying characteristics as well as the 
sensors that they might contain. The sensors such as magnetometers, gyroscopes, 
and accelerometers were discussed with the aim of explaining how they allow the 
quadrotors to stabilize themselves in the air. Other accessories such as cameras and 
GPS units were also covered.
The rules for flying safely were presented. These are basically common sense 
rules such as do not fly over people or pets. Government agencies such as the 
FAA in the United States govern the use of airspace and those rules should be 
followed carefully.
The Hector simulator is excellent, particularly for new pilots of quadrotors in that 
they can be flown in simulation without any danger of real crashes. Details of 
downloading the Hector software were covered in this chapter. ROS is used for 
control and message passing to the simulated quadrotor craft.
Finally, two real quadrotors called Crazyflie and Bebop were described and the 
ROS software to control them was discussed. The Crazyflie is relatively inexpensive 
and safe to fly, but it embodies many of the principles of more expensive and 
sophisticated quadrotors. Enjoy your flights!
In the next chapter, Chapter 8, Controlling Your Robots with External Devices, the 
use of peripheral devices to teleoperate your robot will be considered. Interfacing 
with joysticks, controller boards and mobile devices are handled by standard ROS 
packages to help you expedite the process of implementing these devices with 
your robot.

[ 295 ]
Controlling Your Robots with 
External Devices 
In the past few chapters, we have used ROS to control mobile, armed, flying, and 
simulated robots. The similarities and differences between these robots have been 
discussed and we have shown the commonality that ROS has created between all 
these robot types. Commonality is not only in the structure of the software and the 
communications methods, but also in the simulation environment and the tools used 
for visualization and analysis.
The influence of ROS goes even further to provide a common interface to control 
devices external to the robot. These devices include game controllers (gamepads and 
joystick controllers), mobile devices (smart phones and tablets), and even controller 
boards (the Arduino and Raspberry Pi).
In this chapter, you will learn about the following topics:
•	
Adding a game controller to a robot
•	
Using mobile devices to control robot projects
•	
Interfacing controller boards such as Arduino or Raspberry Pi 

Controlling Your Robots with External Devices
[ 296 ]
Creating a custom ROS game controller 
interface
If you have played with the joystick with either Baxter or Crazyflie, you may think 
that the function of certain buttons or joysticks would be better with your own 
special design. Each type of game controller has one or more joysticks and various 
buttons or triggers to cause events depending on the game software being used. 
Here, for the Microsoft Xbox controller, we will do the following:
•	
Show how to determine the mapping between controller joysticks, buttons, 
and triggers and the number corresponding to each using a graphical 
package, jstest-gtk
•	
Use the terminal command jstest; this will enable you to determine the 
corresponding numbers of controller joysticks, buttons, and triggers
The ensuing diagram shows the Xbox 360 game controller. Pushing a button changes 
the output from 0 (off) to 1 (on) on the channel corresponding to the pushed button, 
which in turn can be read by a program and used to start an application. Moving 
the stick outputs a numerical value that can be used to control a robot. The joystick 
movements are described by the axis of the movement; moving the stick up and 
down defines one axis, and moving the stick to the left and right defines another.
Xbox 360 game controller

Chapter 8
[ 297 ]
Testing a game controller
To determine the number of the channel or axis associated with a button or joystick 
of a game controller, the graphical program jstest-gtk can be used. The package 
jstest-gtk is a game controller testing and configuration tool. This package is 
described at https://launchpad.net/ubuntu/trusty/+package/jstest-gtk.
To download and install the package, use the following command:
$ sudo apt-get install jstest-gtk
The command to execute the program is:
$ jstest-gtk
The following screenshot shows the result with the Xbox controller connected to our 
computer. As shown, the Microsoft Xbox 360 controller has 8 axes and 11 buttons:
jstest-gtk Joystick Preference screen

Controlling Your Robots with External Devices
[ 298 ]
By double-clicking on the name Generic X-Box pad, a GUI for the axes and buttons 
of the Xbox 360 controller appears, as in the following screenshot:
jstest-gtk Xbox screen

Chapter 8
[ 299 ]
As you push a button on the controller, the number of the button will be highlighted 
in the display. The relative position of a joystick that is moved is shown graphically, 
and the numerical value associated with the motion is also displayed. More details 
on this are provided at http://manpages.ubuntu.com/manpages/trusty/en/
man1/jstest-gtk.1.html.
Alternative test of a game controller
As an alternative to the graphical display of the game controller properties, terminal 
commands can be used. First, plug your game controller into a USB port of your 
computer and then determine the devices connected to the computer by issuing 
this command:
$ ls /dev/input/
The output should be similar to the following:
by-id      event0  event10  event2  event4  event6  event8  js0   mouse0
by-path  event1  event11  event3  event5  event7  event9  mice
The game controller will appear as a js device in this listing. Note that the system has 
the game controller as js0 in the list. To test the controller and determine the index 
number of the axes and buttons, type the following command:
$ jstest /dev/input/js0
The output for the Xbox controller, should be similar to the following:
Driver version is 2.1.0.
Joystick (Generic X-Box pad) has 8 axes (X, Y, Z, Rx, Ry, Rz, Hat0X, Hat0Y)
and 11 buttons (BtnX, BtnY, BtnTL, BtnTR, BtnTR2, BtnSelect, BtnThumbL, 
BtnThumbR, ?, ?, ?).
Testing ... (interrupt to exit)
Axes:  0:     0  1:     0  2:     0  3:     0  4:     0  5:     0  6:     0  7:     0
Buttons:  0:off  1:off  2:off  3:off  4:off  5:off  6:off  7:off  8:off  9:off 10:off

Controlling Your Robots with External Devices
[ 300 ]
In this case, the controller is the Xbox controller. Experiment with your controller 
to understand the output when the joystick is moved or a button is pressed. When 
a button is pressed, the corresponding value changes to on. Also, when a joystick is 
moved horizontally or vertically, the selected axis value will change from its initial 
value to a new value.
Now for your controller, you can figure out which buttons or axes you will use for 
your application. Next, you must consider the type of software or hardware that is 
to be controlled by the game controller. For ROS, the /joy node is used to read the 
controller output.
A tutorial describing the use of Ubuntu commands is available at http://wiki.ros.
org/joy/Tutorials/ConfiguringALinuxJoystick.
Using the joy ROS package
ROS has a driver for a generic controller with a joystick. The joy package contains 
the /joy node that interfaces a generic Linux joystick to ROS. This node publishes 
a joy message, which contains the current state of each one of the joystick's buttons 
and axes. Before using the /joy node in an application, see whether you have the 
joy package installed by typing this:
$ rospack find joy
The output should be as follows:
/opt/ros/indigo/share/joy
If you do not see the path to the joy package, install it with this command:
$ sudo apt-get install ros-indigo-joy
Controlling Turtlesim with a custom game 
controller interface
In this section, an example use of a game controller and the /joy node is presented 
utilizing the following code files:
•	
An ROS package contains the launch file turtlesim_teleop.launch that 
executes three nodes: /joy, turtlesim, and turtlesim_joy. A Python 
program turtlesim_joy.py initiates the turtlesim_joy node and allows 
the joystick to control the movement of the turtle on the screen.
•	
The program turtlesim_joy.py calls another Python program called 
move_circle.py when a button is pushed on the controller.

Chapter 8
[ 301 ]
The Turtlesim simulator was introduced in the Turtlesim, the first ROS robot simulation 
section in Chapter 1, Getting Started with ROS. If you wish to create an ROS package 
for this example, refer to the instructions in the Creating and building a ROS package 
section in Chapter 2, Creating Your First Two-Wheeled ROS Robot (in Simulation). In our 
case, the files are copied to the ros_robotics package created in Chapter 2, Creating 
Your First Two-Wheeled ROS Robot (in Simulation).
Downloading the example code
You can download the example code files and other support material for 
this book from www.PacktPub.com.
In the package directory, the launch file should be copied to a /launch directory. 
The Python code should be copied to a /src directory of the package. Make sure 
the Python scripts are executable by issuing the following command:
$ chmod +x <filename>.py
Here, <filename> is the name of the Python script. Alternatively, type 
the following:
$ chmod +x *.py
You can use the preceding command to make all the Python files in the 
directory executable.
After loading the files into the directories, issue the catkin_make command in 
the catkin workspace directory to link together the ROS files.
The code for the launch file turtlesim_teleop.launch is as follows:
<?xml version="1.0"?>
<launch>
  <!-- turtlesim and joy node-->
  <node name="turtlesim" pkg="turtlesim" type="turtlesim_node"/>
  <node name="joy" pkg="joy" type="joy_node"/>
  <!-- turtlesim_joy node interfaces Xbox controller to turtlesim 
  -->
  <node name="turtlesim_joy" pkg="ros_robotics" 
  type="turtlesim_joy.py" output="screen"/>
</launch>

Controlling Your Robots with External Devices
[ 302 ]
Use the following command to launch the nodes:
$ roslaunch ros_robotics turtlesim_teleop.launch
You should see output similar to the following (edited):
.
.
SUMMARY
========
PARAMETERS
 * /rosdistro: indigo
 * /rosversion: 1.11.19
NODES
  /
    joy (joy/joy_node)
    turtlesim (turtlesim/turtlesim_node)
    turtlesim_joy (ros_robotics/turtlesim_joy.py)
auto-starting new master
process[master]: started with pid [22649]
ROS_MASTER_URI=http://localhost:11311
.
The preceding command will launch three ROS nodes as shown in the screen output 
after the launch file is executed with the roslaunch command. You can also check 
the list of nodes with this command:
$ rosnode list

Chapter 8
[ 303 ]
The first Python program turtlesim_joy.py allows the turtle of Turtlesim to 
be controlled by the joystick axes [0] and [1] on the game controller. For the 
Xbox controller, these axes correspond to the vertical and horizontal left joystick 
movements, respectively. In addition, pushing the button corresponding to button 
number 0 will cause another Python program, move_circle.py, to drive the turtle in 
a circle. On the Xbox controller, this is the green button:
#!/usr/bin/env python
"""
Node converts joystick inputs into commands for Turtlesim
"""
import rospy
from geometry_msgs.msg import Twist
from sensor_msgs.msg import Joy
from move_circle import move_circle
def joy_listener():
  # start node
  rospy.init_node("turtlesim_joy", anonymous=True)
  # subscribe to joystick messages on topic "joy"
  rospy.Subscriber("joy", Joy, tj_callback, queue_size=1)
  # keep node alive until stopped
  rospy.spin()
# called when joy message is received
def tj_callback(data):
  # start publisher of cmd_vel to control Turtlesim
  pub = rospy.Publisher("turtle1/cmd_vel", Twist, queue_size=1)
  # Create Twist message & add linear x and angular z from left 
joystick
  twist = Twist()
  twist.linear.x = data.axes[1]
  twist.angular.z = data.axes[0]
  # record values to log file and screen

Controlling Your Robots with External Devices
[ 304 ]
  rospy.loginfo("twist.linear: %f ; angular %f", twist.linear.x, 
twist.angular.z)
  # process joystick buttons
  if data.buttons[0] == 1:        # green button on xbox controller
      move_circle()
  # publish cmd_vel move command to Turtlesim
  pub.publish(twist)
if __name__ == '__main__':
  try:
      joy_listener()
  except rospy.ROSInterruptException:
      pass
This code initializes the turtlesim_joy node and subscribes to the joy topic. When 
a joy message is received, the tj_callback function reads the values from axes[0] 
and axes[1] and assigns them to a twist message. If the value of button[0] is 1, 
then this button was pressed and the move_circle function is called.
The listing of move_circle.py is as follows:
#!/usr/bin/env python
"""
Script to move Turtlesim in a circle
"""
import rospy
from geometry_msgs.msg import Twist
def move_circle():
  # Create a publisher which can "talk" to Turtlesim and tell it 
  to move
  pub = rospy.Publisher('turtle1/cmd_vel', Twist, queue_size=1)
   
  # Create a Twist message and add linear x and angular z values
  move_cmd = Twist()
  move_cmd.linear.x = 1.0
  move_cmd.angular.z = 1.0
  # Save current time and set publish rate at 10 Hz
  now = rospy.Time.now()
  rate = rospy.Rate(10)
  # For the next 6 seconds publish cmd_vel move commands to 
  Turtlesim
  while rospy.Time.now() < now + rospy.Duration.from_sec(6):

Chapter 8
[ 305 ]
      pub.publish(move_cmd)
      rate.sleep()
if __name__ == '__main__':
  try:
      move_circle()
  except rospy.ROSInterruptException:
      pass
When executed, this code will create a twist message and set the linear.x and 
angular.z values. As the Python program turtlesim_joy.py is executed, you can 
move the turtle with the joystick. As the selected button is pushed, move_circle.
py is executed, and the turtle then turns with a linear velocity of 1.0 units/second 
and an angular velocity of 1 radian/second for 6 seconds. Thus, the turtle moves in a 
circle. The following screenshot shows the result of one of our experiments with the 
Xbox 360 joystick control of Turtlesim:
Turtlesim screen

Controlling Your Robots with External Devices
[ 306 ]
To see the message published by the /joy node, issue this command:
$ rostopic echo /joy
The results indicate the values of the axes and buttons and other information:
header:
  seq: 218
  stamp:
    secs: 1461884528
    nsecs: 370878390
  frame_id: ''
axes: [-0.0, 0.1265602558851242, 0.0, -0.06729009747505188, -0.0, 0.0, 0.0, 0.0]
buttons: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
Move the stick to start and use Ctrl + C to end the output.
To see the nodes and topics, issue the following command:
$ rqt_graph
This yields the resulting screenshot:
Turtlesim_teleop.launch node and topic graph
A tutorial using a C++ program and a launch file for use with a Turtlesim 
and joystick is available at http://wiki.ros.org/joy/Tutorials/
WritingTeleopNode.

Chapter 8
[ 307 ]
Creating a custom ROS Android device 
interface
The Android operating system is an open source mobile platform which is widely 
used on smart phones and tablets. Its design is based on the Linux kernel, and its 
primary user control interface is via a touchscreen. This user interface consists of 
touch actions, such as swiping, tapping, or pinching elements on the screen. Tapping 
a virtual keyboard is one of the methods of entering text input. Various types of 
game control screens allow user interaction similar to joysticks and pushbuttons. 
The screen interface typically adjusts from portrait display to landscape based on the 
orientation in which the device is held. Sensors such as accelerometers, gyroscopes, 
and proximity sensors are usually available on the mobile device platform and used 
by the application software.
To provide this type of interface for the user of a robot, the ROS Android 
metapackage has been developed and made available for use and further 
development. But before we dive into an explanation of this metapackage, 
we will spend some time playing with the existing Android-TurtleBot 
application manager, Rocon Remocon.
Playing with Turtlebot Remocon
Android apps have been developed for TurtleBot to perform the 
following operations:
•	
Listener
•	
Teleop
•	
Map Navigation
•	
Make a Map
•	
TurtleBot Follower
•	
TurtleBot Panorama
In addition to these, more apps could be added in the future. These 
apps can be downloaded from the Google Play Store on any Android device 
connected to the Internet. The primary manager for these apps is Rocon 
Remocon (Indigo version).

Controlling Your Robots with External Devices
[ 308 ]
The following instructions will lead you through the process of installing the 
software and running your TurtleBot around the room:
1.	 Downloading the Rocon Remocon app:
On your Android device, select the icon for the Google Play Store (see the 
following image). In the search box, enter Rocon Remocon and tap search. 
From the choices displayed, select the Indigo version of Rocon Remocon. 
The other TurtleBot apps can be downloaded through the Rocon Remocon 
app when it is up and running.
The following screenshot shows the Google Play Store and Rocon 
Remocon icons:
Google Play Store and Rocon Remocon icons
2.	 Network connectivity and TurtleBot setup:
Power on the TurtleBot netbook and decide over which Wi-Fi network you 
wish to connect to the TurtleBot and your Android device. Refer to the 
Networking the netbook and remote computer section in Chapter 3, Driving Around 
with a TurtleBot. The ROS_MASTER_URI and the ROS_IP or ROS_HOSTNAME 
variables should be set on TurtleBot in the .bashrc file in order to reflect the 
TurtleBot's IP address on the chosen network. (Refer to the Netbook network 
setup section in Chapter 3, Driving Around with a TurtleBot).
Also, connect your Android device to the same network. Find the settings for 
the Wi-Fi network and connect to it. If you are using a router, you may wish 
to register the URI address for the TurtleBot and your Android device so that 
they will not change when disconnected and reconnected to the router.

Chapter 8
[ 309 ]
3.	 Configuring the Rocon Remocon app:
Tap on the Rocon Remocon icon to open it and select the ADD A MASTER 
button. As shown in the following screenshot (left side), the master URI 
of the TurtleBot should be added to replace the localhost portion of the 
http://localhost:11311/ URI. A button SCAN THE LOCAL NETWORK 
will search for the TurtleBot, but this will only work if the TurtleBot is up 
and running with minimal launch.
When you have entered the TurtleBot's URI, press the ENTER button and 
you should see an image similar to the image shown on the right side of 
the following screenshot. A triangular warning symbol will appear because 
TurtleBot is not up and running yet.
The TurtleBot's URI information will be saved by the app and will be 
available for use each time the app is started. If this URI is always 
registered to the TurtleBot, then this setup is only required once:
Rocon Remocon screens

Controlling Your Robots with External Devices
[ 310 ]
4.	 Launching TurtleBot:
Now it is time to bring TurtleBot to life! Power on the TurtleBot base, open a 
terminal window on the netbook, and type the following command:
$ roslaunch turtlebot_bringup minimal.launch
After TurtleBot has played its little bringup tune, you can close the lid on the 
netbook. All other operations will be done on the Android device.
5.	 Android control:
On your Android device, press the REFRESH button and see the Turtlebot 
icon appear in the place of the triangular warning symbol. Next, touch the 
bar where the Turtlebot icon and the URI are displayed. A pop-up window 
should appear that has Choose your role heading displayed with options 
for Android Pairing and Documentation. Select the circle next to Android 
Pairing and the following screen image should appear. This screen shows 
all the apps currently available for the Turtlebot Remocon. (The number and 
arrangement of your icons may differ.):
TurtleBot – Android Pairing screen

Chapter 8
[ 311 ]
Each of these apps will need to be downloaded from the 
Google Play Store before they will work on your device. 
Be sure that your Android device has Internet access when 
you wish to download a new app.
6.	 Android Teleop:
Now we will try the Teleop app by touching the Teleop icon. A pop-up 
window should appear that confirms that you are using Teleop with an 
Android device: Lovely, you are now allowed to launch this interaction. 
Touch the Launch button at the bottom.
If you are successful, a screen similar to the following screenshot should 
appear. The camera image will be different, of course. If you see a pattern of 
nine dots, be patient and wait for the camera sensor of the TurtleBot to start 
transmitting images. If an image does not appear after a few seconds, 
check the connections to your vision sensor to make sure the sensor is 
installed properly.
The following screen should appear after you press the Launch button:
Teleop screen

Controlling Your Robots with External Devices
[ 312 ]
Now you can drive the TurtleBot around by placing your finger inside the 
circle on the right and sliding it in the direction you want TurtleBot to go. 
Remember, you are controlling TurtleBot as if you were standing behind it. 
If you swipe your finger to the bottom of the screen, TurtleBot will back up 
and swiping-to-the-left will cause it to rotate left. At the top of the screen are 
monitors for the battery levels for the robot base and the netbook.
When you are ready to shut down the app, touch the Back button on the 
Teleop screen and the Leave button on the Android Pairing screen. Then, 
you should shut down the apps from one of your main app managers or the 
menus on your device.
Exploring the rest of TurtleBot Remocon apps is left to you. Some troubleshooting 
help is provided, in case you get stuck.
Troubleshooting TurtleBot Remocon
The following troubleshooting procedures are provided to assist with getting your 
Turtlebot Remocon apps up and running:
•	
Connection Failure:
If the Connection Failure message is displayed on the Rocon Remocon 
screen, the Remocon app was not able to connect with the TurtleBot. Here is 
a list of things to check:
°°
TurtleBot's base should be powered on
°°
TurtleBot's netbook should be powered on and connected to the 
network shared by the TurtleBot and Android device
°°
TurtleBot's .bashrc should identify the proper ROS_MASTER_URI 
and ROS_IP or ROS_HOSTNAME variables
°°
TurtleBot should be running minimal.launch
°°
The Android device should be connected to the network shared by 
Android and the TurtleBot
°°
The TurtleBot's URI should be entered as the master URI in 
Rocon Remocon
•	
Failed to start the paired rapp:
This message appears when another Turtlebot Remocon app is running or 
has not been shut down properly. For this error, you should tap the Cancel 
button and wait for the app that is running to shut down.
If an app is not responding, it is best to shut it down entirely and start over.

Chapter 8
[ 313 ]
•	
Other troubleshooting method:
For additional troubleshooting help, access the tutorial at How to Run 
Turtlebot Android Application, which can be found at http://wiki.ros.org/
turtlebot/Tutorials/indigo/Android%20Interactions/How%20to%20
Run%20Turtlebot%20Andorid%20Application.
Now we can develop an Android interface for any of our ROS robots. The next 
section will describe how to interface an ROS robot with an Android device and 
develop a user control interface unique to the robot.
Custom control of ROS robot using an 
Android device
If you are interested in developing additional apps for the TurtleBot similar to 
the ones in the previous section, refer to the TurtleBot Android Apps Dev Tutorials 
at http://wiki.ros.org/turtlebot/Tutorials/indigo/Android%20
Interactions/Turtlebot%20Android%20Apps%20Dev%20Tutorials.
In this section, we begin by downloading the software to become an Android 
developer using ROS.
Installing Android Studio and Tools
There are two non-ROS software packages to download in order to become an ROS-
Android developer. First, download the Oracle Java Standard Edition Development 
Kit (JDK). This JDK is recommended over the openjdk Java development kit at this 
time due to problems with performance and the user interface. Download the Oracle 
JDK from http://www.oracle.com/technetwork/java/javase/overview/index.
html.
A JDK is a development environment for building Java applications, applets, and 
components. After the .tar.gz archive file is downloaded, you may unpack the 
tarball in a location of your choice. We have chosen to install it at the system 
level in the /opt directory where administrator privileges are necessary to 
add these directories.
Verify the software version is 1.8 (or later) using this command:
$ javac -version

Controlling Your Robots with External Devices
[ 314 ]
Next, you will need Android Studio, which is the official integrated development 
environment (IDE) for Android. Android Studio provides all the tools to build apps 
for every type of Android device. Download the Android Studio package as a ZIP 
file from http://developer.android.com/sdk/index.html.
Scroll to the bottom of the web page for the Linux platform distribution.
It is recommended that you install this package also in the /opt directory (as /opt/
android-studio). Next, run the setup script using the following command:
$ /opt/android-studio/bin/studio.sh
A setup wizard will guide you through the setup process. When prompted to import 
previous Android Studio settings, you may select I do not have a previous version 
of Studio and click on OK. Select the Standard installation type. The Android SDK 
components will be downloaded as part of the setup process.
For systems that run a 64-bit version of Ubuntu or if the message Unable to run 
mksdcard SDK tool is displayed on the screen, the following 32-bit libraries will 
need to be installed using the command in a terminal window:
$ sudo apt-get install lib32z1 lib32ncurses5 lib32bz2-1.0 
lib32stdc++6
Execute the following two commands to add Android Studio to your 
environment variables:
$ echo export PATH=\${PATH}:/opt/android-sdk/tools:/opt/android-
sdk/platform-tools:/opt/android-studio/bin >> ~/.bashrc
$ echo export ANDROID_HOME=/opt/android-sdk >> ~/.bashrc
With the two non-ROS packages installed, the ROS metapackages for rosjava and 
android are needed. The installation of these metapackages is described in the 
upcoming section.
Installing an ROS-Android development 
environment
Our instructions for creating an ROS-Android development environment will be to 
use a Debian installation of rosjava with a catkin workspace built on top. To install 
the rosjava metapackage, type this command:
$ sudo apt-get install ros-indigo-rosjava

Chapter 8
[ 315 ]
Next, to create an empty catkin workspace overlay on top of your ros-indigo files, 
type the following commands:
$ mkdir –p ~/myjava/src
$ cd ~/myjava/src
$ source /opt/ros/indigo/setup.bash
$ catkin_init_workspace
$ cd ~/myjava
$ catkin_make
If you have your own Java source code and you wish to use it in this workspace, 
refer to further instructions at http://wiki.ros.org/rosjava/Tutorials/
indigo/Deb%20Installation.
You can also start from here and create your own rosjava packages. Follow the 
tutorials at http://wiki.ros.org/rosjava_build_tools/Tutorials/indigo/
Creating%20Rosjava%20Packages.
For Android development, we continue by installing the ready-made core android 
libraries. Use the following commands to create an android workspace, which 
overlays the rosjava workspace created previously:
$ mkdir –p ~/myandroid
$ wstool init -j4 ~/myandroid/src my_android_sources.rosinstall
$ source ~/myjava/devel/setup.bash 
$ cd ~/myandroid
$ catkin_make
Other options for installing the source code for ROS Android core libraries can be 
found at http://wiki.ros.org/android/Tutorials/indigo/Installation%20-
%20ROS%20Development%20Environment.
Defining terms
Knowing the following terms should help as you learn more about the rosjava and 
android development environments:
Ant: An Apache Ant is a Java library and command-line tool used to build Java 
applications. Developers either build antlibs that contain Ant tasks and types, 
or they have access to numerous ready-made antlibs. Apache Ant is a term 
trademarked by The Apache Software Foundation, which provides support for 
open source projects.

Controlling Your Robots with External Devices
[ 316 ]
Gradle: Gradle is an automated build system that combines with Ant and Maven to 
offer a method of declaring a project's configuration identifying the order of its build 
tasks. The system can handle large multiproject builds and supports incremental 
builds for only the portion of a project that is dependent on what has been changed.
JAR: A Java Archive (JAR) is the package format that is used to combine Java class 
files and the metadata to be distributed as a software application or library. JARs are 
compressed into ZIP file format for archiving purposes.
Maven: As a software project management and compression tool, Maven 
manages the state of a project's development, including its build, reporting, and 
documentation aspects. For our purposes, a repository in Maven will be used to hold 
build artifacts and dependencies for ROS-Android applications. Apache Maven is 
another open source project of The Apache Software Foundation.
Introducing ROS-Android development
The division between the /myjava and /myandroid workspaces is strategic to the 
development. In the /myjava workspace, you can create and build custom rosjava 
message jars and artifacts. You can also use this space to build and test rosjava 
algorithms for use in your Android applications. To dive into rosjava development, 
refer to the list of tutorials (for ROS Indigo) at http://wiki.ros.org/rosjava.
The /myandroid workspace contains the official ROS android stacks as follows:
•	
android_core: This package contains Damon Kohler's core libraries and 
examples on Android development
•	
android_extras: This package consists of various extra functionalities 
for peripheral devices (such as a hokuyo scanner) or utilities (such as 
imaging qrcodes)
•	
android_apps: This package includes development libraries and apps for 
robot pairing (as with TurtleBot)
•	
android_remocons: This package contains remote control clients for 
appable robots
The following diagram highlights the dependencies between the rosjava and 
android libraries:

Chapter 8
[ 317 ]
From this point, you are ready to begin with the creation of Android packages 
using rosjava scripts. To begin with, tutorials on creating Android packages and 
applications are available at http://wiki.ros.org/rosjava_build_tools/
Tutorials/indigo/Creating%20Android%20Packages.
For a complete list of ROS Android tutorials, refer to the list of tutorials (for ROS 
Indigo) at http://wiki.ros.org/android.
Creating ROS nodes on Arduino or 
Raspberry Pi
The Arduino and Raspberry Pi are two of the most popular embedded systems 
on the market today. Sometimes, their names and capabilities are discussed 
interchangeably, but each platform has its own unique capabilities and usage. In 
robotics, it is important to know the merits of each of these powerful devices and 
how each one can be used with ROS to inherit the advantages of ROS.
Rosserial defines the protocol for ROS communication over serial transmission lines 
to ROS nodes running on microcontrollers or single-board computers. Standard ROS 
messages are serialized/de-serialized in a prescribed format, and topics and services 
are multiplexed for serial ports or network sockets. For low-level details of this 
protocol, refer to http://wiki.ros.org/rosserial/Overview/Protocol.

Controlling Your Robots with External Devices
[ 318 ]
In the following section, the capability of rosserial is demonstrated in an example 
program using Arduino and an ultrasonic sensor.
Using Arduino
The Arduino board contains a microcontroller that can process one ROS node at 
a time. This sequential nature makes it easy to use and understand its processing 
and communication with external devices, such as motors, sensors, and peripheral 
devices. Arduino has a set of digital and analog input/output (I/O) pins to interface 
with a wide variety of external sensors and actuators. This simple board can be 
used to design and build a robot to sense and move about in its environment or to 
enhance an existing robot with extended capabilities.
Interfacing the Arduino board to an external computer allows you to program 
the microcontroller using its own Arduino IDE based on the external computer. 
Programs developed in C in the Arduino IDE are downloaded to the microcontroller 
over its USB connection using its serial communications interface.
Installing the Arduino IDE software
The Arduino IDE allows quick and easy programming of Arduino boards. This 
open source software can be installed on Windows, Mac OS X, and Linux operating 
systems. The IDE can generate software for any Arduino board, but check for special 
instructions for your particular board at https://www.arduino.cc/en/Guide/
HomePage.
To install the Arduino IDE as Debian packages on your Ubuntu operating system, 
open a terminal window and enter the following commands:
$ sudo apt-get update 
$ sudo apt-get install arduino arduino-core
If any additional information is needed, refer to http://playground.arduino.cc/
Linux/Debian.
For installation instructions on how to manually load Arduino, refer to the 
Downloading and maintaining manually section of http://playground.arduino.cc/
Linux/Ubuntu.
The latest software download can be found at www.arduino.cc/en/Main/Software.

Chapter 8
[ 319 ]
Arduino offers extensive documentation guides, tutorials, and examples at the 
following websites:
•	
https://www.arduino.cc/en/Guide/Introduction
•	
https://www.arduino.cc/en/Guide/Environment
•	
https://www.arduino.cc/en/Guide/Libraries
•	
https://www.arduino.cc/en/Tutorial/HomePage
Next, the ROS software for Arduino is installed.
Installing the ROS Arduino software
The rosserial_arduino package allows you to implement the ROS communication 
protocol over Arduino's serial ports. This package helps the software implement ROS 
messages, access ROS system time, and publish tf transforms. Using the rosserial_
arduino package, an independent ROS node running on your Arduino can publish 
and subscribe to messages from ROS nodes running on a remote computer.
To load the rosserial_arduino package on your computer, use the following 
installation commands:
$ sudo apt-get install ros-indigo-rosserial-arduino
$ sudo apt-get install ros-indigo-rosserial
The rosserial metapackage contains the rosserial_msgs, rosserial_client, and 
rosserial_python packages. The rosserial_python package is used to provide 
the remote computer serial interface to communicate with the Arduino node.
ROS bindings for Arduino are implemented as an Arduino library within the IDE. 
This library, ros_lib, must be added to the /libraries subdirectory within the 
user's Arduino sketchbook (the code directory). We are using ~/sketchbook as 
the directory to store our Arduino code (sketches). Within your sketchbook, the 
subdirectory /libraries should already exist or should be created. Change to 
this subdirectory with the following command (<sketchbook> is the path to your 
sketchbook directory):
$ cd <sketchbook>/libraries
If the ros_lib library already exists, delete it with this command:
$ rm –rf ros_lib
To generate the ros_lib library, type the following command:
$ rosrun rosserial_arduino make_libraries.py

Controlling Your Robots with External Devices
[ 320 ]
This command will create a ros_lib subdirectory and a number of subdirectories 
under ros_lib. A ros_lib subdirectory is a wrapper for ROS messages and 
services implemented with Arduino data types. The Arduino C/C++ data types 
are converted by ros_lib through the use of special header files. We will use 
some of these header files in our example with the ultrasound sensor. For now, 
the subdirectory we are interested in is /examples. Type the following commands:
$ cd <sketchbook>/libraries/ros_lib/examples/
$ ls
Verify that the contents are similar to the following files:
ADC       button_example  IrRanger  pubsub              ServoControl  Ultrasound
Blink      Clapper                 Logging   ServiceClient   Temperature
BlinkM  HelloWorld          Odom      ServiceServer   TimeTF
These files contain examples of the ROS-Arduino code. The examples are used to 
provide a basic implementation of ROS on Arduino. For example, the Temperature 
code can be used to read the temperature values of a sensor connected to the 
Arduino board. You can start with the examples and modify them to suit your 
particular application.
Next, with your Arduino board plugged into the USB port of the computer, start the 
Arduino IDE with this command:
$ arduino
A pop-up window may appear that is the Arduino Permission Checker stating 
You need to be added to the dialout group. It is important that you click on the 
Add button and authenticate the selection with a password. An alternative is to 
enter the following command to assign yourself to the dialout group:
$ sudo usermod -a -G dialout <username>
Administrative privileges are necessary to perform either of these actions.

Chapter 8
[ 321 ]
As a result of the arduino command, verify the screenshot is as follows:
Arduino sketchbook

Controlling Your Robots with External Devices
[ 322 ]
Verify that you can access the ros_lib examples by navigating the drop-down 
menus, File | Examples | ros_lib , as shown in the following screenshot:
arduino ros_lib installed
Also, check the serial port connection to the Arduino board. To list the available 
serial ports on your computer, use the following command:
$ ls /dev/tty*

Chapter 8
[ 323 ]
From the Arduino IDE drop-down menu, Tools | Serial Port, check 
to verify the serial port connection. Our Arduino is connected to /dev/ttyACM0, 
as shown in the following screenshot:
Connecting to serial port
The ROS wiki presents a series of tutorials related to the rosserial_arduino 
package. These tutorials are listed and can be accessed from http://wiki.ros.org/
rosserial_arduino/Tutorials.
The following example demonstrates the use of an ROS node running on the 
Arduino publishing a sensor_msgs/Range message. This message contains 
distance data retrieved from an HC-SR04 ultrasonic sensor.

Controlling Your Robots with External Devices
[ 324 ]
Ultrasonic sensor control using ROS and Arduino
As an example of interfacing a sensor with the Arduino board, we will add 
an HC-SR04 ultrasound range sensor. The following screenshot shows the 
face of the sensor:
HC-SR04 ultrasonic sensor
In operation, the sensor emits sound pulses well beyond the range of our hearing. 
If the pulses bounce off an obstacle, the returning sound is received by the sensor. 
Knowing the speed of sound in air as Velocity, it is possible to measure the time 
that the sound takes to reach the obstacle and return to the sensor since the distance 
traveled by the sound waves to the object will be:
Time refers to the total time of travel of the sound wave in its two-way trip.
The speed of sound varies with temperature and other factors, but we will use the 
typical speed of sound in air at 20°C (68°F) to be 343 meters/sec (1125.3 feet/second).

Chapter 8
[ 325 ]
To use the sensor with the Arduino board, it is necessary to write code to trigger the 
sensor to emit the sound and then determine the time of travel of the sound. The 
distance between the sensor and an obstacle is determined from this time, as shown 
in the previous equation. The range of the HC-SR04 sensor is 2 cm to 400 cm or from 
less than 1 inch to 13 ft.
Signals associated with the sensor are shown in the following diagram. The trigger 
pulse is created by the software as an output from the Arduino board digital pins 
to the Trig pin, shown in the preceding screenshot. The sensor sends out the sonic 
burst after the trigger pulse has ended. The echo signal on the Echo pin of the sensor 
when HIGH has a width that is proportional to the time the sound signal takes to 
be emitted and then returned. This signal is connected to a digital input pin of the 
Arduino board. Software is used to determine the length of the return echo pulse in 
microseconds. Then, the time in microseconds is converted into distance using the 
distance formula, as follows:
HC-SR04 sensor signals

Controlling Your Robots with External Devices
[ 326 ]
According to the specifications of the HC-SR04 sensor, the trigger pulse created by 
the software must be at least 10 microseconds long. The repetition time to measure 
the distance to an obstacle must be greater than 25 milliseconds to assure that the 
trigger pulses do not overlap in time with the return echo pulse. This is a repetition 
rate of 40 Hertz. In our code to follow, the repetition time for the published range is 
set at 50 milliseconds.
Connecting the Arduino to the HC-SR04 ultrasonic sensor
For our setup, we used an Arduino UNO to interface with the HC-SR04 sensor. The 
following list describes the connections between the pins of our Arduino UNO board 
and the sensor pins:
•	
Vcc of the sensor to 5V on the Arduino for power
•	
GND of the sensor to GND on the Arduino for the ground
•	
Trig input pin of the sensor to digital I/O pin 6 as the output of the Arduino
•	
Echo output pin of the sensor to digital I/O pin 5 as the input to the Arduino
The Arduino UNO is described at https://www.arduino.cc/en/main/
arduinoBoardUno.
Programming the Arduino to sense distance
An Arduino sketch is provided to interface with the ultrasound sensor and 
determine the distance values detected by the sensor. The C code for the Arduino 
should be downloaded and stored in the <sketchbook>/ultrasound_sr04 directory 
as an .ino file. The <sketchbook> refers to the path of your sketchbook directory.
Downloading the ultrasound_sr04.ino code
You can download the example code files and other support material for 
this book from www.PacktPub.com.
The following code performs these operations:
•	
Defines the pins of the Arduino board used for the sensor and outputs the 
trigger signal when the program runs
•	
Creates the node to publish range data indicating the distance between the 
sensor and an object
•	
Defines the ROS topic /ultrasound and causes the range data to be 
published when the code is run

Chapter 8
[ 327 ]
The code is as follows:
/* 
 * rosserial Ultrasound Example for HC-SR04
 */
#include <ros.h>
#include <ros/time.h>
#include <sensor_msgs/Range.h>
const int echoPin = 5;  //Echo pin
const int trigPin = 6;  //Trigger pin
const int maxRange = 400.0;   //Maximum range in centimeters
const int minRange = 0.0;     //Minimum range
unsigned long range_timer;    //Used to measure 50 ms interval
// instantiate node handle and publisher for 
//  a sensor_msgs/Range message (topic name is /ultrasound)
ros::NodeHandle  nh;
sensor_msgs::Range range_msg;
ros::Publisher pub_range( "ultrasound", &range_msg);
/*
 * getRange() - This function reads the time duration of the echo
 *              and converts it to centimeters.
 */
float getRange(){
    int sample;      //Holds time in microseconds
    
    // Trigger pin goes low then high for 10 us then low
    //  to initiate the ultrasonic burst
    digitalWrite(trigPin, LOW);
    delayMicroseconds(2);
    
    digitalWrite(trigPin, HIGH);
    delayMicroseconds(10);
    digitalWrite(trigPin, LOW);
    
    // read pulse length in microseconds on the Echo pin
    sample = pulseIn(echoPin, HIGH);
    
    // sample in microseconds converted to centimeters

Controlling Your Robots with External Devices
[ 328 ]
    // 343 m/s speed of sound;  time divided by 2
    return sample/58.3;
}
char frameid[] = "/ultrasound";   // global frame id string
void setup()
{
  // initialize the node and message publisher
  nh.initNode();
  nh.advertise(pub_range);
  
  // fill the description fields in the range_msg
  range_msg.radiation_type = sensor_msgs::Range::ULTRASOUND;
  range_msg.header.frame_id =  frameid;
  range_msg.field_of_view = 0.26;
  range_msg.min_range = minRange;
  range_msg.max_range = maxRange;
  
  // set the digital I/O pin modes
  pinMode(echoPin, INPUT);
  pinMode(trigPin, OUTPUT);  
}
void loop()
{
  // sample the range data from the ultrasound sensor and
  // publish the range value once every 50 milliseconds
  if ( (millis()-range_timer) > 50){
    range_msg.range = getRange();
    range_msg.header.stamp = nh.now();
    pub_range.publish(&range_msg);
    range_timer =  millis() + 50;
  }
  nh.spinOnce();
}
Executing the ultrasonic sensor program
To open the Arduino IDE, type the following command:
$ arduino

Chapter 8
[ 329 ]
From the menu bar, navigate the drop-down menus, File | Sketchbook | 
ultrasound_sr04 to see the following screen:
Arduino IDE with the ultrasound_sr04 code
On the tool bar menu, choose the right arrow icon to verify and upload the code to 
the Arduino board and wait for the Done uploading message.

Controlling Your Robots with External Devices
[ 330 ]
In a second terminal window, start the ROS Master by typing:
$ roscore
In a third terminal window, execute the rosserial program by typing:
$ rosrun rosserial_python serial_node.py /dev/<ttyID>
Here, <ttyID> is the identifier of the serial port you are using.
Our system used /dev/ttyACM0, and our output yields the following information:
[INFO] [WallTime: 1463090309.509834] ROS Serial Python Node
[INFO] [WallTime: 1463090309.512368] Connecting to /dev/ttyACM0 at 57600 
baud
[INFO] [WallTime: 1463090312.093936] Note: publish buffer size is 280 bytes
[INFO] [WallTime: 1463090312.094188] Setup publisher on /ultrasound 
[sensor_msgs/Range]
The rosserial_python package contains a Python implementation to allow 
communication between a remote computer and an attached device capable of serial 
data transfer, such as the Arduino board. The serial_node.py script creates the 
/serial_node node.
The ROS data for distance will be published as range data. As the program executes, 
the range value and other information is published on the /ultrasound topic with 
the sensor_msgs/Range message. The message format can be seen by typing in the 
following command in another terminal window:
$ rosmsg show sensor_msgs/Range
The output shows the message format as follows:
uint8 ULTRASOUND=0
uint8 INFRARED=1
std_msgs/Header header
  uint32 seq
  time stamp
  string frame_id
uint8 radiation_type

Chapter 8
[ 331 ]
float32 field_of_view
float32 min_range
float32 max_range
float32 range
To see the numerical output of the /ultrasound topic, type this:
$ rostopic echo /ultrasound
The output should look similar to the following:
---
header:
  seq: 278
  stamp:
    secs: 1463092078
    nsecs:   3101881
  frame_id: /ultrasound
radiation_type: 0
field_of_view: 0.259999990463
min_range: 0.0
max_range: 400.0
range: 50.0
---
In the screen output, we see the information in the message. Note that the 
frame_id, radiation_type, field_of_view (0.26), min_range, and max_range 
variables were defined in the C code, which was shown previously. The range 
value is in centimeters and the values are published every 50 milliseconds.
To show the topic and range values in a graphical form, type the 
following command:
$ rqt_plot

Controlling Your Robots with External Devices
[ 332 ]
The rqt_plot window is shown in the next screenshot. The values will of course 
depend on your setup and the distance between your sensor and the obstacle:
rqt plot of the range values in centimeters
In the screenshot, the range is a constant 50 cm and the max_range field is set to 400 
cm, as defined in the code. The other values are too small to be seen on the scale.
In our test of the HC-SR04 sensor, we noticed some inaccuracies in the range 
measurements. As with any sensor such as HC-SR04, the system should be 
calibrated and tested if you wish to ensure the accuracy of the measurement.
Other applications using ROS and Arduino
There are other sensors for ranging as well as for temperature measurement, 
motor control, and many other applications. A complete list of tutorials for Arduino 
applications using rosserial can be found at: http://wiki.ros.org/rosserial_
arduino/Tutorials.
Another use of rosserial is setting up wireless communication using rosserial_
xbee tools in order to create sensor networks using XBee devices and Arduino. More 
information on this is available at http://wiki.ros.org/rosserial_xbee.

Chapter 8
[ 333 ]
Using Raspberry Pi
The Raspberry Pi board is a general-purpose computer that contains a version of the 
Linux operating system, called Raspbian. The Pi can process multiple ROS nodes at 
a time and can take advantage of many features of ROS. It can handle multiple tasks 
at a time and perform intense processing of images or complex algorithms.
There are several versions of Raspberry Pi available on the market. Each model 
is based on a Broadcom system on a chip (SOC) with an ARM processor and a 
VideoCore graphics processing unit (GPU). Models vary in the amount of board 
memory available, and a Secure Digital (SD) card is used for booting and long-term 
storage. Boards are available configured with a variety of USB ports, HDMI and 
composite video output, RJ45 Ethernet, WiFi 802.11n, and Bluetooth communication.
To set up your Raspberry Pi and configure the Raspian operating system, refer to 
these websites:
•	
https://www.raspberrypi.org/documentation/setup/
•	
https://www.raspberrypi.org/documentation/installation/
To configure your Raspberry Pi, see the website https://www.raspberrypi.org/
documentation/configuration/.
To get started learning about Raspian and interfacing with the general-purpose I/O, 
camera modules, and communication methods, refer to https://www.raspberrypi.
org/documentation/usage/.
Technical documentation of the hardware is available at 
https://www.raspberrypi.org/documentation/hardware/.
Installing ROS on the Raspberry Pi
The installation instructions for loading ROS Indigo onto the Raspberry Pi can be 
found at http://wiki.ros.org/ROSberryPi/Installing%20ROS%20Indigo%20
on%20Raspberry%20Pi.
These instructions are for a source installation of ROS onto a Raspberry Pi with 
Raspian version Wheezy or Jessie installed for the operating system. A catkin 
workspace needs to be created for the source packages, and the ROS-Comm variation 
is recommended to install basic ROS packages, build tools, and communication 
libraries. Packages for GUI tools are not downloaded as part of this variation.

Controlling Your Robots with External Devices
[ 334 ]
The projects you can undertake with the Raspberry Pi and ROS (sometimes called 
ROSberry Pi) are limitless. You can create programs in either Python or C++. 
A collection of examples can be found at these websites:
•	
http://wiki.ros.org/rosserial_embeddedlinux/Tutorials
•	
http://wiki.ros.org/ROS/Tutorials
•	
http://www.takktile.com/tutorial:raspberrypi-ros
Summary
This chapter has described ROS interfaces for a number of external devices used 
for robot control. The advantages of ROS extend to these types of interfaces, as it is 
evident in the common method and message structure across similar devices. For 
game controllers, a custom interface was created using an Xbox 360 controller for the 
Turtlesim simulation. The buttons and axes for the Xbox 360 controller were mapped 
so that we could select a button and axes to use for Turtlesim control. A Python 
script was shown that caused the turtle to move when a joystick was moved or a 
certain button was pressed.
Android devices can also provide a common ROS interface for controlling robots 
like TurtleBot. The Android-TurtleBot application manager Rocon Remocon 
was downloaded to an Android device and used to operate a TurtleBot in the 
teleoperation mode. Instructions for installing the software and tools to set up a 
ROS-Android development environment were explained. An introduction to the 
key terminology for Android development was also presented.
Embedded systems such as the Arduino and Raspberry Pi are often used for 
controlling robots and interfacing to sensors. ROS nodes can run on these 
devices and publish messages over topics to nodes on other computers. For serial 
communication, ROS provides the rosserial protocol and metapackage to standardize 
this interface.  Instructions for installing the Arduino-ROS and Raspberry Pi-ROS 
development environments were presented. Within the Arduino IDE, software was 
written to create a node and publish a message containing sensor data from an SR04 
ultrasound sensor.
In the next chapter Flying a Mission with Crazyflie, we will leverage all of our 
current ROS knowledge (and then some) to autonomously fly a quadrotor to a 
specific location. An external camera will be used to coordinate the flight and 
identify the quadrotor and the location of the target.

[ 335 ]
Flying a Mission with 
Crazyflie
Robots are fun and sometimes frustrating to program. Quadrotors are particularly 
difficult to control due to the number of flight factors and the complexity of 
flight programs to control these factors. Quadrotors are currently being tested 
as surveillance cameras and delivery vehicles for packages and fast food. In this 
chapter, we will explore the subject of programming quadrotors to fly to specific 
destinations. This application may be handy for delivering coffee and paperwork 
around the office. We will begin by using a barebones quadrotor and an inexpensive 
depth camera to sense the quadrotor's location.
This chapter will highlight the use of ROS communication to coordinate the locations 
of the quadrotor and the target. A Kinect sensor will be used to visualize the 
environment and the position of the quadrotor in it to coordinate its landing at a 
marked location. ROS tf transforms and pose messages will be generated to identify 
the reference frames and positions of the quadrotor and the target. The transforms 
enable the control commands to be published to bring the quadrotor to the target 
location. The navigation, flying, and landing mission implements a spectrum of ROS 
components—including nodes, topics, messages, services, launch files, tf transforms, 
rqt, and more— taught in this book.

Flying a Mission with Crazyflie
[ 336 ]
We will set up this mission scenario between a Crazyflie 2.0, a Kinect for Windows 
v2 (called Kinect v2 in this chapter), and a target marker acting as the landing 
position on top of a TurtleBot. The following picture shows the arrangement of our 
setup. Feel free to follow these instructions to prepare an arrangement of quadrotor, 
target, and image sensor with the equipment you have available.
Mission setup
For our mission, Crazyflie will be controlled to hover, fly and land. In this chapter, 
we will address the following to achieve this mission:
•	
Detecting the Crazyflie on a Kinect v2 image
•	
Establishing a tf framework to support the configuration of our camera 
and robot
•	
Determining the Cartesian coordinates (x, y, z) of the Crazyflie with respect 
to the image
•	
Publishing a tf transform of the coordinates
•	
Controlling the Crazyflie to hover at its initial location

Chapter 9
[ 337 ]
•	
Locating a target on the video image, determining its coordinates, and 
publishing its pose
•	
Controlling the Crazyflie to takeoff ,fly to the target and land
Mission components
The components we will use in this mission include a Crazyflie 2.0 quadrotor, a 
Crazyradio PA, a Kinect for the Windows v2 sensor, and a workstation computer. 
Chapter 7, Making a Robot Fly, describes the Crazyflie and Crazyradio and their 
operations. Chapter 4, Navigating the World with TurtleBot, is a good introduction to 
a depth sensor such as the Kinect v2. It is recommended to review these chapters 
before beginning this mission.
Kinect for Windows v2
Kinect v2 is an infrared time of flight depth sensor that operates at a higher 
resolution than the Kinect for Xbox 360. The modulated infrared beam measures 
how long it takes for the light to travel to the object and back, providing a more 
accurate measurement. This sensor has improved performance in dark rooms and 
in sunny outdoor conditions. With a horizontal field of view (FOV) of 70 degrees 
and a vertical FOV of 60 degrees, the infrared sensor can accurately detect distances 
ranging from 0.5 to 4.5 meters (20 inches to 14.75 feet) within this FOV. The image 
resolution for the depth camera is 512 x 424 at a rate of 30 frames per second. The 
Kinect v2 must be connected to a USB 3.0 port on the workstation computer in order 
to provide the image data. External electrical power for the Kinect is also required.
Kinect v2 produces a large amount of image data that can overwhelm the 
workstation computer if it is not equipped with a separate graphics processing 
unit (GPU). The ROS packages libfreenect2 and iai_kinect2 were developed 
to interface with Kinect v2 for image-processing applications. The iai_kinect2 
package provides tools for calibrating, the interfacing, and viewing color and depth 
images from Kinect v2. Kinect images are used with OpenCV tools to process the 
images for object detection. The section OpenCV and ROS provides background 
information and describes how these two tools are interfaced.
Kinect's color images will be evaluated to locate markers for the Crazyflie and the 
target positions. These markers will enable the position and altitude of the quadrotor 
and the target to be determined with respect to the image frame. These positions are 
not related to real-world coordinates but applied in relation to the sensor's image 
frame. An ROS tf transform is published to advertise the location of the Crazyflie.

Flying a Mission with Crazyflie
[ 338 ]
Crazyflie operation
Controlling a quadrotor is the subject of a vast amount of literature. To control 
Crazyflie, our plan is to follow the same type of control prepared by Wolfgang 
Hoenig in his success with the crazyflie metapackage (https://github.com/
whoenig/crazyflie_ros). This package was developed as part of his research 
at the ACTLab at the University of Southern California (http://act.usc.edu/). 
Within his crazyflie_controller package, he created a controller that uses PID 
control for each of Crazyflie's four dimensions of control: pitch, roll, thrust, and 
yaw. Our software design mimics this approach but deviates in key areas where the 
singular image view of the Kinect required changes to the control parameters. We 
also changed the software to Python. A vast amount of testing was required to attain 
control of a Crazyflie in a hover state. When hover results were acceptable, testing 
advanced further to add the challenge of flying to the target. Further testing was 
required to improve flight control.
The controller software uses the difference between the Crazyflie's current position 
and the goal position (either hover or target) to send correction commands to fly 
closer to the goal position. This iteration continues every 20 milliseconds with a new 
position for Crazyflie detected and a new correction computed and sent. This is a 
closed-loop system that computes the difference between positions and commands 
the Crazyflie to fly in direction of the goal position.
During testing, Crazyflie lived up to its name and would arbitrarily fly to various 
corners of the room out of control. Implementing a new ROS node took care of this 
unwanted behavior. The node crazyflie_window was designed to be an observer of 
Crazyflie's location in the image frame. When Crazyflie's location came too close to 
the image's edge, a service command was sent to the controller and an appropriate 
command would be published to Crazyflie. This implementation resulted in no more 
flyaway behavior and saved on broken motor mounts.
Mission software structure
The code developed for this mission is contained in the crazyflie_autonomous 
package and divided into four different nodes:
•	
crazyflie_detector in the detect_crazyflie.py file
•	
target_detector in the detect_target.py file
•	
crazyflie_controller in the control_crazyflie.py and pid.py and 
crazyflie2.yaml files
•	
crazyflie_window in the watcher.py file

Chapter 9
[ 339 ]
This mission also relies on a portion of Wolfgang Hoenig's crazyflie metapackage 
that is described in Chapter 7, Making a Robot Fly. The nodes used are as follows:
•	
crazyflie_server (crazyflie_server.cpp from the crazyflie_driver 
package)
•	
crazyflie_add (crazyflie_add.cpp from the crazyflie_driver package); 
This node runs briefly during Crazyflie startup to set initial parameters for 
the Crazyflie
•	
joystick_controller (controller.py from the crazyflie_demo package)
A third set of nodes is generated by other packages:
•	
baselink (static_transform_publisher from the tf package).
•	
joy (the joy package).
•	
kinect2_bridge (the iai_kinect2/kinect2_bridge package); the 
kinect2_bridge works between the Kinect v2 driver (libfreenect2) and ROS. 
Image topics are produced by the kinect2 node.
The relationship between these nodes is shown in the following node graph:
Nodes and topics for Crazyflie mission

Flying a Mission with Crazyflie
[ 340 ]
All of the code for the Chapter 9, Flying a Mission with Crazyflie is available 
online at the Packt Publishing website. The code is too extensive to 
include in this chapter. Only important portions of the code are described 
in the following sections to aid the learning of the techniques used for this 
mission.
OpenCV and ROS
In the previous two chapters (Chapter 4, Navigating the World with TurtleBot and 
Chapter 6, Wobbling Robot Arms using Joint Control), we introduced and described 
a little about the capabilities of OpenCV. Since this mission heavily relies on the 
interface between ROS and OpenCV and also on the OpenCV library, we will go 
into further background and detail about OpenCV.
OpenCV is a library of powerful computer vision tools for a vast expanse of 
applications. It was originally developed at Intel by Gary Bradsky in 1999 as a 
C library. The upgrade to OpenCV 2.0 was released in October 2009 with a C++ 
interface. Much of this work was done at Willow Garage, headed by Bradsky and 
Vadim Pisarevsky. It is open source software with a BSD license and free for both 
academic and commercial use. OpenCV is available on multiple operating systems, 
including Windows, Linux, Mac OS X, Android, FreeBSD, OpenBSD, iOS, and more. 
The primary interface for OpenCV is C++, but programming language interfaces 
exist for Python, Java, MATLAB/Octave, and wrappers for C# and Ruby.
The OpenCV library contains more than 2,500 effective and efficient vision 
algorithms for a wide range of vision-processing and machine-learning applications. 
The fundamental objective is to support real-time vision applications, such as 
tracking moving objects and detecting and recognizing faces for surveillance. Many 
other algorithms support object identification, the tracking of human gestures and 
facial expressions, the production of 3D models of objects, the construction of 3D 
point clouds from stereo camera data, the modeling of scenes based on multiple 
image sources, to name just a few. This extensive library of tools is used throughout 
the industry and in academia and government as well.
To learn more about OpenCV, visit http://opencv.org/. This website provides 
access to excellent tutorials, documentation on structures and functions, and an API 
interface for C++ and to a lesser extent Python. The current version of OpenCV is 3.0, 
but ROS Indigo (and Jade) supports OpenCV2, whose current version is 2.4.12.
ROS provides the vision_opencv stack to integrate the power of the OpenCV 
library of tools. The wiki website for this interface stack is http://wiki.ros.org/
vision_opencv.

Chapter 9
[ 341 ]
The OpenCV software and the ROS vision_opencv stack were installed when you 
performed the ROS software install of the ros-indigo-desktop-full configuration 
in Chapter 1, Getting Started with ROS. To install only the OpenCV library with the 
ROS interface and Python wrapper, use the following command:
$ sudo apt-get install ros-indigo-vision-opencv
This vision_opencv stack currently provides two packages: cv_bridge, and 
image_geometry. The cv_bridge package is the connection between ROS messages 
and OpenCV. It provides for the conversion of OpenCV images into ROS images 
and vice versa. The image_geometry package contains a powerful library of image-
processing tools for both Python and C++. Images can be handled with respect to the 
camera parameters provided the CameraInfo messages. It is also used with camera 
calibration and image rectification.
For this mission, we will use OpenCV algorithms to analyze the Kinect image and 
detect the Crazyflie and target within the scene. Using the location of the Crazyflie 
and target, the Crazyflie will be given commands to fly to the location of the target. 
This scenario hides the layers of complex computation and understanding required 
to perform this seemingly simple task.
Loading software for the mission
Part of the software that needed to perform this cooperative mission has been 
installed in previous chapters:
•	
The ROS software installation of the ros-indigo-desktop-full 
configuration is described in the Installing and launching ROS section of 
Chapter 1, Getting Started with ROS
•	
The installation of Crazyflie ROS software is described in the Loading 
Crazyflie ROS software section of Chapter 7, Making a Robot Fly
Software for the Kinect v2 to interface with ROS requires the installation of two 
items: libfreenect2 and iai_kinect2. The following sections provide the details 
of these installations.

Flying a Mission with Crazyflie
[ 342 ]
Installing libfreenect2
The libfreenect2 software provides an open-source driver for Kinect v2. This 
driver does not support the Kinect for Xbox 360 or Xbox One. Libfreenect2 provides 
for the image transfer of RGB and depth as well as the combined registration of RGB 
and depth. Image registration aligns the color and depth images for the same 
scene into one reference image. Kinect v2 firmware updates are not supported in 
this software.
The installation instructions can be found at https://github.com/OpenKinect/
libfreenect2. This software is currently under active development, so refer to 
this website for changes made to these installation instructions. The website lists 
installation instructions for Windows, Mac OS X, and Ubuntu (14.04 or later) 
operating systems. It is important to follow these directions accurately and read 
all the related troubleshooting information to ensure a successful installation. The 
instructions provided here are for Ubuntu 14.04 and will load the software into 
the current directory. This installation can be either local to your home directory or 
systemwide if you have sudo privileges.
To install the libfreenect2 software in your home directory, type the following:
$ git clone https://github.com/OpenKinect/libfreenect2.git
$ cd ~/libfreenect2
$ cd depends
$ ./download_debs_trusty.sh
A number of build tools are required to be installed as well:
$ sudo apt-get install build-essential cmake pkg-config
The libusb package provides access for the Kinect v2 to the USB device on 
your operating system. Install a libusb version that is either greater than or 
equal to 1.0.20:
$ sudo dpkg –i debs/libusb*deb
TurboJPEG provides a high-level open-source API for compressing and 
decompressing JPEG images in the memory to improve CPU/GPU performance. 
Install the following packages for TurboJPEG by typing:
$ sudo apt-get install libturbojpeg libjpeg-turbo8-dev
Open Graphics Library (OpenGL) is an open-source cross-platform API with 
a variety of functions designed to improve graphics processing performance. 
To install OpenGL packages, type the following commands:
$ sudo dpkg -i debs/libglfw3*deb

Chapter 9
[ 343 ]
$ sudo apt-get install –f
$ sudo apt-get install libgl1-mesa-dri-lts-vivid
The sudo apt-get install –f command will fix any broken dependencies that 
exist between the packages. If the last command produces errors showing conflicts 
with other packages, do no install it.
Some of these packages may already be installed in your system. You will receive a 
screen message indicating that the latest version of the package is installed.
Additional software packages can be installed to use with libfreenect2, but they 
are optional:
•	
Open Computing Language (OpenCL) creates a common interface despite 
the underlying computer system platform. The libfreenect2 software uses 
OpenCL to perform more effective processing on the system.
The OpenCL software requires that certain underlying software be 
installed to ensure that the libfreenect2 driver can operate on your 
processor. OpenCL dependencies are specific to your computer 
system's GPU. Refer to the detailed instructions at https://github.
com/OpenKinect/libfreenect2.
•	
Installation instructions for CUDA (used with Nvidia), Video Acceleration 
API (VAAP) used with Intel, and Open Natural Interaction (OpenNI2) are 
provided at the libfreenect2 website.
Whether or not you install the optional software, the last step will be to build the 
actual Protonect executable using the following commands:
$ cd ~/libfreenect2
$ mkdir build
$ cd build
$ cmake .. -DCMAKE_INSTALL_PREFIX=$HOME/freenect2 -DENABLE_CXX11=ON
$ make
$ make install

Flying a Mission with Crazyflie
[ 344 ]
Remember that udev rules are used to manage system devices and create device 
nodes for the purpose of handling external devices, such as Kinect. Most likely, an 
udev rule will be required so that you will not need to run Protonect for the Kinect 
with sudo privileges. For this reason, it is necessary to copy the udev rule from its 
place in the downloaded software to the /etc/udev/rules.d directory:
$ sudo cp ~/libfreenect2/platform/linux/udev/90-kinect2.rules 
/etc/udev/rules.d
Now you are ready to test the operation of your Kinect. Verify that the Kinect v2 
device is plugged into power and into the USB 3.0 port of the computer. If your 
Kinect was plugged in prior to installing the udev rule, unplug and reconnect 
the sensor.
Remember that the Kinect v2 can only be used through a 
USB 3.0 port.
Now run the program using the following command:
$./libfreenect2/build/bin/Protonect
You are successful if a terminal window opens with four camera views. 
The following screenshot shows our Kinect pointed at Baxter:
Protonect output

Chapter 9
[ 345 ]
Use Ctrl + C keys in the terminal window to quit Protonect.
At this time, the libfreenect2 software is being updated frequently. So if you are 
experiencing problems, check GitHub for the latest master release.
If you experience errors, refer to the FAQs at https://github.com/OpenKinect/
libfreenect2 and the issues at https://github.com/OpenKinect/libfreenect2/
wiki/Troubleshooting.
Protonect is a very useful tool when it comes to checking out 
the operation of your Kinect v2. Anytime the Kinect seem to 
work improperly, use Protonect to check the operation of the 
libfreenect2 driver. The command to do this is:
$ ./libfreenect2/build/Protonect.
Installing iai_kinect2
The iai_kinect2 software is a library of functions and tools that provide the ROS 
interface for Kinect v2. The libfreenect2 driver is required for using the iai_
kinect2 software library. The iai_kinect2 package was developed by Thiemo 
Wiedemeyer of the Institute for Artificial Intelligence, University of Bremen.
Instructions for the installation of the software can be found at https://github.
com/code-iai/iai_kinect2.
For a cooperative mission, you can decide to either add the software to the 
crazyflie_ws workspace or create a new catkin workspace for this software. The 
authors decided to create a new catkin workspace called mission_ws to contain 
the software for the iai_kinect2 metapackage and the crazyflie_autonomous 
package developed for this mission.
To install the iai_kinect2 software, move on to your catkin workspace src 
directory and clone the repository:
$ cd ~/<your_catkin_ws>/src/
$ git clone https://github.com/code-iai/iai_kinect2.git

Flying a Mission with Crazyflie
[ 346 ]
Next, move into the iai_kinect2 directory, install the dependencies, and build 
the executable:
$ cd iai_kinect2
$ rosdep install -r --from-paths .
$ cd ~/<your_catkin_ws>
$ catkin_make -DCMAKE_BUILD_TYPE="Release"
Notice that running the rosdep command will output an error 
regarding not being able to locate [kinect2_bridge] and [kinect2_
registration]. Disregard this error because these packages are part 
of the iai_kinect2 metapackage and rosdep is unaware of these 
packages at this time.
Now it is time to operate the Kinect sensor using the kinect2_bridge launch file. 
Type the following command:
$ roslaunch kinect2_bridge kinect2_bridge.launch
At the end of a large amount of screen output, you should see this line:
[ INFO] [Kinect2Bridge::main] waiting for clients to connect
If you are successful, congratulations! Great work! If not (like the authors), start your 
diagnosis by referring to the kinect2_bridge is not working/crashing, what is wrong? FAQ 
and other helpful queries at https://github.com/code-iai/iai_kinect2.
When you receive the waiting for clients to contact message, the next step is to view 
the output images of kinect2_bridge. To do this, use kinect2_viewer by typing in 
the following:
$ rosrun kinect2_viewer kinect2_viewer
The output should be as follows:
[ INFO] [main] topic color: /kinect2/qhd/image_color_rect
[ INFO] [main] topic depth: /kinect2/qhd/image_depth_rect
[ INFO] [main] starting receiver...

Chapter 9
[ 347 ]
Our screen showed the following screenshot:
kinect2_viewer output
Use Ctrl + C in the terminal window to quit kinect2_viewer.
As shown in the preceding screenshot, kinect2_viewer has the default settings of 
quarter high definition (qhd) , image_color_rect, and image_depth_rect. This 
Cloud Viewer output is the default viewer. These settings and other options for 
kinect2_viewer will be described in more detail in the following section.
The next section describes the packages that are contained in the iai_kinect2 
metapackage. These packages make the job of interfacing to the Kinect v2 flexible 
and relatively straightforward. It is extremely important to calibrate your Kinect 
sensor to align the RGB camera with the infrared (IR) sensor. This alignment will 
transform the raw images into a rectified image. The kinect2_calibration tool 
that can be used to perform this calibration and the calibration process is described 
in the next section.

Flying a Mission with Crazyflie
[ 348 ]
Using the iai_kinect2 metapackage
The IAI Kinect 2 library provides the following tools for the Kinect v2:
•	
kinect2_calibration: This tool is used to align the Kinect RGB camera 
with its IR camera and depth measurements. It relies on the functions of the 
OpenCV library for image and depth processing.
•	
kinect2_registration: This package projects the depth image onto the 
color image to produce the depth registration image. OpenCL or Eigen must 
be installed for this software to work. It is recommended to use OpenCL to 
reduce the load on the CPU and obtain the best performance possible.
•	
kinect2_bridge: This package provides the interface between the Kinect 
v2 driver, libfreenect2, and ROS. This real-time process delivers Kinect 
v2 images at 30 frames per second to the CPU/GPU. The kinect2_bridge 
software is implemented with OpenCL to take advantage of the system's 
architecture for processing depth registration data.
•	
kinect2_viewer: This viewer provides two types of visualizations: a color 
image overlaid with a depth image or a registered point cloud.
Additional information is provided in later sections.
kinect2_bridge and kinect2_viewer
The kinect2_bridge and kinect2_viewer provide several options for producing 
images and point clouds. Three different resolutions are available from the 
kinect2_bridge interface: Full HD (1920 x 1080), quarter Full HD (960 x 540), and 
raw ir/depth images (512 x 424). Each of these resolutions can produce a number of 
different images, such as the following:
•	
image_color
•	
image_color/compressed
•	
image_color_rect
•	
image_color_rect/compressed
•	
image_depth_rect
•	
image_depth_rect/compressed
•	
image_mono
•	
image_mono/compressed
•	
image_mono_rect
•	
image_mono_rect/compressed
•	
image_ir

Chapter 9
[ 349 ]
•	
image_ir/compressed
•	
image_ir_rect
•	
image_ir_rect/compressed
•	
points
The kinect2_bridge software limits the depth range for the sensor between 0.1 and 
12.0 meters. For more information on these image topics, refer to the documentation 
at https://github.com/code-iai/iai_kinect2/tree/master/kinect2_bridge.
The kinect2_viewer has the command-line options to bring up the different 
resolutions described previous. These modes are as follows:
•	
hd: for Full High Definition (HD)
•	
qhd: for quarter Full HD
•	
sd: for raw IR/depth images
Visualization options for these modes can be image for a color image overlaid with 
a depth image, cloud for a registered point cloud, or both to bring up both the 
visualizations in different windows.
kinect2_calibration
The kinect2_calibration tool requires the use of a chessboard or circle board 
pattern to align the color and depth images. A number of patterns are provided in 
the downloaded iai_kinect2 software inside the kinect2_calibration/patterns 
directory. For a detailed description of how the 3D calibration works, refer to the 
OpenCV website at http://docs.opencv.org/2.4/doc/tutorials/calib3d/
table_of_content_calib3d/table_of_content_calib3d.html.
The iai_kinect2 calibration instructions can be found at https://github.com/
codeiai/iai_kinect2/tree/master/kinect2_calibration.
We used the chess5x7x0.03 pattern to calibrate our Kinect and printed it on plain 
8.5 x 11 inch paper. Be sure to check the dimensions of the squares to assure that they 
are the correct measurement (3 centimeters in our case). Sometimes, printers may 
change the size of objects on the page. Next, mount your pattern on a flat, moveable 
surface assuring that the pattern is smooth and no distortions will be experienced 
that will corrupt your sensor calibration.

Flying a Mission with Crazyflie
[ 350 ]
The Kinect should be mounted on a stationary surface and a tripod works well. The 
Kinect will be positioned in one location for the entire calibration process. Adjust it 
to align it with a point straight ahead in an open area in which you will be moving 
around the calibration chess pattern. The instructions mention the use of a second 
tripod for mounting the chess pattern, but we found it easier to move the pattern 
around by hand (using a steady hand). It is important to obtain clear images of the 
pattern from both the RGB and IR cameras.
If you wish to judge the effects of the calibration process, use the kinect2_bridge 
and kinect2_viewer software to view and take the initial screenshots of the 
registered depth images and point cloud images of your scene. When the calibration 
process is complete, repeat the screenshots and compare the results.
Calibrating your Kinect
First, you will need to make a directory on your computer to hold all of the 
calibration data that is generated for this process. To do this, type the following:
$ mkdir ~/kinect_cal_data
$ cd ~/kinect_cal_data
To start the calibration process, set up your Kinect and run:
$ roscore
In a second terminal window, start kinect2_bridge but pass the parameter 
for setting a low number of frames per second. This will reduce the CPU/GPU 
processing load:
$ rosrun kinect2_bridge kinect2_bridge _fps_limit:=2
Notice as the software runs, similar output should come to the screen:
[Info] [Freenect2Impl] 15 usb devices connected
[Info] [Freenect2Impl] found valid Kinect v2 @4:14 with serial 501493641942
[Info] [Freenect2Impl] found 1 devices
[ INFO] [Kinect2Bridge::initDevice] Kinect2 devices found:
[ INFO] [Kinect2Bridge::initDevice]   0: 501493641942 (selected)

Chapter 9
[ 351 ]
Your data will be different, but note the serial number of your Kinect v2 (ours is 
501493641942). When the waiting for clients to connect text appears on the screen, 
type the following command:
$ rosrun kinect2_calibration kinect2_calibation <type of pattern> 
record color
Our <type of pattern> is chess5x7x0.03 pattern. This command will start the 
process for calibrating the color camera. Notice the output to the screen:
[ INFO] [main] Start settings:
       Mode: record
     Source: color
      Board: chess
 Dimensions: 5 x 7
 Field size: 0.03
Dist. model: 5 coefficients
Topic color: /kinect2/hd/image_mono
   Topic ir: /kinect2/sd/image_ir
Topic depth: /kinect2/sd/image_depth
       Path: ./
[ INFO] [main] starting recorder...
[ INFO] [Recorder::startRecord] Controls:
   [ESC, q] - Exit
 [SPACE, s] - Save current frame
        [l] - decrease min and max value for IR value rage
        [h] - increase min and max value for IR value rage
        [1] - decrease min value for IR value rage

Flying a Mission with Crazyflie
[ 352 ]
        [2] - increase min value for IR value rage
        [3] - decrease max value for IR value rage
        [4] - increase max value for IR value rage
[ INFO] [Recorder::store] storing frame: 0000
As the screen instructions indicate, after you have positioned the pattern board in the 
image frame, hit the spacebar (or S) key on the keyboard to take a picture. Be sure 
that the cursor is focused on the terminal window. Every time you hit the spacebar, a 
.png and .yaml file will be created in the current directory (~/kinect_cal_data).
The following screenshot shows a rainbow-colored alignment pattern that overlays 
the camera image when the pattern is acceptable for calibration. If this pattern does 
not appear, hitting the spacebar will not record the picture. If the complete pattern is 
not visible in the scene, the rainbow colors will all turn red because part of the board 
pattern cannot be observed in the image frame:
Calibration alignment pattern
We recommend that you run the following command in another terminal window:
$ rosrun kinect2_viewer kinect2_viewer  sd image

Chapter 9
[ 353 ]
The image viewer will show the image frame for the combined color/depth image. 
This frame has smaller dimensions than the RGB camera frame in Full HD, and it is 
important to keep all your calibration images within this frame. If not, the calibration 
process will try to shrink the depth data into the center of the full RGB frame and 
your results will be unusable.
Move the pattern from one side of the image to the other, taking pictures from 
multiple spots. Hold the board at different angles to the camera (tilting the board) as 
well as rotating it around its center. The rainbow-colored pattern on the screen will 
be your clue as to when the image can be captured. It is suggested to take pictures 
of the pattern at varying distances from the camera. Keep in mind that the Kinect's 
depth sensor range is from 0.5 to 4.5 meters (20 inches to over 14 feet). In total, a set 
of 100 or more calibration images is suggested for each calibration run.
When you have taken a sufficient number of images, use the Esc key (or Q) to exit 
the program. Execute the following command to compute the intrinsic calibration 
parameters for the color camera:
$ rosrun kinect2_calibration kinect2_calibration chess5x7x0.03 
calibrate color
Be sure to substitute your type of pattern in the command. Next, begin the process 
for calibrating the IR camera by typing in this command:
$ rosrun kinect2_calibration kinect2_calibration chess5x7x0.03 record 
ir
Follow the same process that you did with the color camera, taking an additional 
100 pictures or more. Then, compute the intrinsic calibration parameters for the IR 
camera using the following command:
$ rosrun kinect2_calibration kinect2_calibration chess5x7x0.03 
calibrate ir
Now that the color and IR cameras have been calibrated individually, it is time to 
record images from both the cameras synchronized:
$ rosrun kinect2_calibration kinect2_calibration chess5x7x0.03 record 
sync
Take an additional 100 or more images. The extrinsic calibration parameters are 
computed with the following command:
$ rosrun kinect2_calibration kinect2_calibration chess5x7x0.03 calibrate 
sync

Flying a Mission with Crazyflie
[ 354 ]
The following command calibrates depth measurements:
$ rosrun kinect2_calibration kinect2_calibration chess5x7x0.03 
calibrate depth
At this point, all of the calibration data has been computed and the data must be 
saved to the appropriate location for use by the kinect2_bridge software. Recall the 
serial number of your Kinect that you noted earlier with the kinect2_bridge screen 
output. Create a directory with this serial number under the kinect2_bridge/data 
directory:
$ roscd kinect2_bridge/data
$ mkdir <Kinect serial #>
Copy the following calibration files from ~/kinect_cal_data to the kinect2_
bridge/data/<Kinect serial#> directory you just created:
$ cp ~/kinect_cal_data/c*.yaml <Kinect serial#>
Your kinect2_bridge/data/<Kinect serial#> directory should look similar to 
the following screenshot:
Kinect v2 calibration data files
Running kinect2_viewer again should show an alignment of the color and depth 
images with strong edges at corners and on outlines. The kinect2_bridge software 
will automatically check for the Kinect's serial number under the data directory and 
use the calibration data if it exists.
Setting up the mission
For the Kinect, our workstation computer requires us to run Protonect prior to using 
the kinect2_bridge software. If you have trouble launching the kinect2_bridge 
software, use the following command before you begin:
$ ./libfreenect2/build/bin/Protonect

Chapter 9
[ 355 ]
Verify that Protonect shows color, depth, and IR images and that none of the screens 
are black. Be aware that Protonect has three optional parameters: cl (for OpenCL), 
gl (for OpenGL) or cpu (for CPU support). These options can be useful for testing 
the Kinect v2 operation.
If Protonect has successfully brought up the Kinect image, then press Ctrl + C to close 
this window. The kinect2_bridge and kinect2_viewer should then work properly 
until the system is restarted.
Next, we must determine how to identify our robots within the frame of the 
Kinect image.
Detecting Crazyflie and a target
For our Crazyflie and target location, we have prepared markers to uniquely identify 
them in our lab environment. For the Crazyflie, we have placed a lightweight green 
ball on top of its battery and attached it with a sticky mounting tab. For the target, 
we have placed a pink paper rectangle at the target location. The first step is to 
find a unique way to identify these markers and find their locations within the 
Kinect image.
OpenCV offers over 150 color conversion options for processing images. For object 
tracking, the simplest and recommended method is to convert the blue-green-red 
(BGR) image to hue-saturation and value (HSV). This is an easy and effective 
method for selecting an object of a desired color. An OpenCV tutorial on object 
tracking can be found at http://opencv-pythontutroals.readthedocs.org/en/
latest/py_tutorials/py_imgproc/py_colorspaces/py_colorspaces.html.
A complete method for object tracking is described in the following sections.
Identifying markers in a color image
The color of these identifiers will be used in our software to pinpoint the location of 
the quadrotor and target. First, we must determine the numerical values of the HSV 
components of their colors. This is done by grabbing an image of the marker with 
the Kinect and using the GNU Image Manipulator Program (GIMP) software on 
Ubuntu to classify the HSV numbers.
Start by running the kinect2_bridge launch file and kinect2_viewer; bring up 
an image of your Crazyflie and/or your target on your computer screen. For this 
exercise, these robots can be in the same view, or you can perform these steps one at 
a time for each of the markers. Use the Alt + Print Screen keys or your favorite screen-
capture program to snap a picture of the scene and save it to a file. Click on the Dash 
tool in the Ubuntu Launcher and type in GIMP to find the GIMP application software. 

Flying a Mission with Crazyflie
[ 356 ]
After starting the GIMP software, open the image and select the following two 
options from the top menu bar:
•	
Under Tools, Select Color Picker (notice the cursor change to an eyedropper) 
under Tools
•	
Under Window, Select New Toolbox under Window
Move the eyedropper cursor to the center of the identifier (green ball in our case) 
and click on mouse button. This color will appear in the color rectangle at the bottom 
of the New Toolbox window. Click on this colored rectangle and the Change 
Foreground Color window will appear with the color marked with crosshairs in 
the color image on the left side. On the right side are the hue, saturation, value, 
red, green, and blue values that correspond to that color. The following screenshot 
illustrates the results of this process:
Using GIMP to find HSV numbers
For our green ball, the HSV numbers were H = 109, S = 62, and V = 49, as shown in 
the previous screenshot. These values apply to the scales for GIMP integer ranges of 
Hue (0 - 360), Saturation (0 - 100), and Value (0 - 100).

Chapter 9
[ 357 ]
Now we must convert these values to the OpenCV integer ranges of Hue (0 - 179), 
Saturation (0 - 255), and Value (0 - 255). Therefore, our HSV numbers are computed 
as follows:
Hue:             109 / 2 ≈ 55
Saturation:  62 * 255 / 100 ≈ 158
Value:          49 * 255 / 100 ≈ 125
Now, to pick range values for the software, we will apply the following rules:
Hue:              use range ± 10
Saturation:   use range ± 50
Value:           use range ± 50
Using these as guidelines, we arrive at the range values for our green ball as follows:
Hue:              (45 - 65)
Saturation:  (110 - 210)
Value:          (75 - 175)
The HSV numbers for the pink rectangle target were H = 298, S = 28, and V = 96 
when the target was directly facing the Kinect. Initially, we chose the following 
ranges in the code:
Hue:             (139 - 159)
Saturation: (21 - 121)
Value:         (240 - 255)
We modified these values later as we tested the tracking of these objects in the Kinect 
image viewer. The Saturation (or whiteness) value ranges from white (0) to full 
saturated color (100), and Value (or lightness) ranges from black (0) to the lightest 
color (100) in GIMP.
Problems with target detection
Tracking the target was especially tricky. As the target was placed in a horizontal 
position, the light reflecting off of the top of the target changed the Saturation 
and Value components of its HSV. It is extremely important to test the object 
detection capability from one side of the Kinect image to the other and for different 
orientations of the object. Selecting the appropriate range for HSV values is crucial to 
the success of the mission. For the target, we decreased the lower range of Saturation 
to improve target detection in our lighting conditions.

Flying a Mission with Crazyflie
[ 358 ]
Detecting and viewing markers with OpenCV
OpenCV is used to detect these identifiers in the Kinect image, and we use 
the following code to verify that we have captured the correct identifier for 
the green ball:
#!/usr/bin/env python
import cv2
import numpy
# read png image and convert the image to HSV
image = cv2.imread("<path>/<png filename>", cv2.CV_LOAD_IMAGE_COLOR)
hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)
# find green objects in the image
lower_green = numpy.array([45, 110, 75], numpy.uint8)
upper_green = numpy.array([65, 210, 255], numpy.uint8)
mask = cv2.inRange(hsv, lower_green, upper_green)
cv2.imwrite("hsv_mask.png", mask)
This code is contained in the view_mask.py file. The kinect2_viewer_green_ball.
png file is provided with the code so that you can duplicate our steps and reproduce 
a file with the HSV mask of the green ball.
To briefly explain this code, we will examine the lines in relative groupings. 
First, the packages needed for this code are imported:
#!/usr/bin/env python
import cv2
import numpy
The cv2 package is the OpenCV 2.0 wrapper for Python and provides access 
to a variety of vision processing functions. The numpy package is an extension 
of Python that handles numerical manipulations for large multidimensional 
arrays and matrices.
The next section of code handles the reading of the image from a file and processing 
it for HSV:
image = cv2.imread("<path>/<png filename>", cv2.CV_LOAD_IMAGE_COLOR)
hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)

Chapter 9
[ 359 ]
The first command loads the image from the file using the cv2.imread function. 
The first argument is the image file to be loaded, which can be any image file type. 
We are using .png files. The second argument for cv2.imread specifies to load the 
image as an RGB color image. In OpenCV, the RGB values are identified in reverse 
order as BGR. This loaded image is converted from BGR to HSV using the cv2 
function cvtColor.
Next, two arrays are created to contain the lower bounds and the upper bounds of 
the HSV values for the green ball. The values in these arrays were identified and 
calculated in the previous section, Identifying markers in a color image. These arrays 
are used to find pixels in the image that fit within those bounds:
# find green objects in the image
lower_green = numpy.array([45, 110, 75], numpy.uint8)
upper_green = numpy.array([65, 210, 255], numpy.uint8)
mask = cv2.inRange(hsv, lower_green, upper_green)
cv2.imwrite("hsv_mask.png", mask)
As numpy array is created with unsigned integer values for the lower bounds of H, 
S, and V. A second array is created to contain the upper bounds for these elements. 
The cv2 function inRange performs the comparison of each pixel of the image to 
determine whether it falls within these bounds. If it does, a white pixel is placed in 
the mask image; otherwise, a black pixel is placed in the image. The last command 
imwrite stores the binary image to the hsv_mask.png file. The following image 
shows the resulting HSV mask image of the green ball:
HSV mask image of green ball.
This code is implemented in the detect_crazyflie.py and detect_target.py 
scripts described in the next sections.

Flying a Mission with Crazyflie
[ 360 ]
Using Kinect and OpenCV
Using Kinect to locate the position of the Crazyflie provides only a (pixel) location 
relative to the image frame of the Kinect. Relating this location to the world 
coordinate frame cannot be accurately accomplished.
Advanced camera systems, such as the VICON motion capture system, provide the 
object location in world coordinates. In a VICON system, it is possible to establish 
Crazyflie's position as (0, 0, 0) in x, y, and z and relate the movement in terms of 
meters. If you have this type of system available, check out Wolfgang Hoenig's ROS 
Crazyflie code at http://wiki.ros.org/crazyflie. The crazyflie_controller 
package provides simple navigation to goal using VICON. The crazyflie_demo 
package contains sample scripts and launch files to perform teleoperation, hovering, 
and waypoint navigation. The controller within our crazyflie_autonomous 
package was created based on concepts used in Mr. Hoenig's packages.
For our mission, the Python script detect_crazyflie.py creates the crazyflie_
detector node to handle the process of identifying the location of the Crazyflie 
within the Kinect image frame and publishing its location. This node subscribes to 
three topics published by the kinect2 node specifying the qhd resolution (960 x 540). 
The code for subscribing to these topics is as follows:
rospy.wait_for_message('/kinect2/qhd/camera_info', CameraInfo)
rospy.Subscriber('/kinect2/qhd/camera_info', CameraInfo, 
self.camera_data, queue_size=1)
rospy.Subscriber('/kinect2/qhd/image_color_rect', Image, 
self.image_callback, queue_size=1)
rospy.Subscriber('/kinect2/qhd/image_depth_rect', Image, 
self.depth_callback, queue_size=1)
The rospy call to wait_for_message will assure that the kinect2 node is publishing 
topics. Then, the crazyflie_detector node will subscribe to the three topics: 
camera_info, image_color_rect, and image_depth_rect. The /kinect2/qhd/
camera_info topic will contain a sensor_msgs/CameraInfo message that is 
processed by the callback function camera_data. The camera_data function will 
extract the camera height and width fields from the CameraInfo message and set 
parameters on the Parameter Server for camera_height and camera_width. For qhd 
resolution, these parameters are 540 and 960, respectively.
The /kinect2/qhd/image_color_rect and /kinect2/qhd/image_depth_rect 
topics subscribe to sensor_msgs/Image messages. For the rectified color Image 
message, the function image_callback is called to handle the image processing. 
The rectified depth Image message is processed by the depth_callback function. 
The message queue size is limited to one so that only the latest Image message 
will be processed.

Chapter 9
[ 361 ]
The image_callback function processes the color image similar to the object 
detection method described in the Detecting and viewing markers with OpenCV section. 
The green objects in the image are detected, and a binary mask image is created. The 
cv2.morphologyEx function is called to first dilate and then erode the image with an 
11 x 11 kernel. This process removes the small pixels within the white objects and the 
surrounding black background. More than one white object may be present in the 
binary image. The next step is to find all the objects and order them by size. The cv2 
function findContours finds all the pixels within the contour objects in the binary 
image. A hierarchy of nested contours is created. A check is made to assure that 
there is at least one contour in this hierarchy, then the area of each of the contours is 
calculated. The largest contour area is selected as the green ball on top of Crazyflie.
Since we know that this object is round, the cv2 function minEnclosingCircle 
is used to find the object's center and radius. The horizontal center of the object is 
saved as cf_u, and the vertical center is saved as cf_v. The object center and radius 
values are used by the cv2.circle function to draw a blue circle outline around the 
object in the original color Image message. This image message is then displayed in 
a terminal window using the cv2.imshow function. The resulting image is shown in 
the following screenshot:
Crazyflie detected
When a /kinect2/qhd/image_depth_rect topic arrives, the depth_callback 
function will be called to process this sensor_msgs/Image message. For this rectified 
depth Image message, the cf_u and cf_v values will be used as the pixel coordinates 
to access the depth value, cf_d, at the center of the circle. Sometimes, an erroneous 
value is returned for the depth at this location. If the depth value returned is zero, 
the last depth value will be reused.

Flying a Mission with Crazyflie
[ 362 ]
Next, the update_cf_transform function is called to publish the tf transform of 
Crazyflie. The cf_u, cf_v, and cf_d values are passed to this function to use as the 
x, y, and z values of Crazyflie's transform. A tf.TransformBroadcaster object is 
created to publish transforms from the crazyflie_detector node. The update_cf_
tranform function uses the sendTransform function (from the tf package) for the 
transform broadcaster to publish a transform for the Crazyflie relative to the Kinect. 
Details of this transform are described in the next section.
How to track Crazyflie
Using ROS tf transforms to identify the location of the Crazyflie in an image frame 
is a variation of the concept of ROS tf. Typically, tf transforms relate the coordinate 
frame of a robot's component to the rest of its system and environment (world) 
in which it is operating. Tf keeps all the robot's coordinate frames in a tree-like 
structure that relates it to the world environment. In addition, these coordinate 
frames are tracked by tf with relation to time. Tf provides functions to transform 
from one coordinate frame to any other frame in the tf structure at a desired point in 
time. For more information on this implementation of tf, refer to the Understanding tf 
section in Chapter 6, Wobbling Arms using Joint Control.
For our mission, Crazyflie's tf transforms are limited to operations within the 2D 
image plane of the Kinect's color image and the third dimension of depth from the 
Kinect's depth image. Crazyflie's position with respect to the image's horizontal 
position u and its vertical position v are used to identify its location with respect to 
the color image frame. Its 3D position can be completed by accessing the depth 
value from the depth frame for the (v, u) location. These values are used as the 
x, y, z of Crazyflie's translation fields for the transform message. The rotation 
fields are fixed with values to change the orientation of Crazyflie from the 
Kinect's camera orientation.
For the Kinect coordinate frame, the x axis is horizontal from the upper-left corner of 
the camera image to the right, the y axis is vertical from this same corner downward, 
and the z axis is from the front face of the camera out to the scene. The Kinect 
coordinate frame is represented in the following rviz image, using the convention of 
x (red), y (green), and z (blue). The position of this coordinate frame is at the origin of 
the Kinect and does not represent the location on the image frame:

Chapter 9
[ 363 ]
tf coordinate frames in rviz
For Crazyflie, a rotation of this Kinect coordinate frame must be made to adhere to 
the ROS conventional orientation of x forward, y left, and z up standardized in REP 
103 Coordinate Frame Conventions. Therefore, for rotation fields in Crazyflie's tf 
transform, values are fixed to the set of Euler angles:
•	
Roll of -π/2
•	
Pitch of 0
•	
Yaw of –π

Flying a Mission with Crazyflie
[ 364 ]
These values are used to compute a quaternion with the tf.transformations.
quaternion_from_euler function. The transform pose is published using the 
sendTransform function (from the tf package) as a transform from the kinect2_ir_
optical_frame to the crazyflie/baselink. The kinect2_ir_optical_frame is the parent 
frame of crazyflie/baselink. A diagram of the tf frames broadcast for this mission is 
shown here:
Crazyflie mission tf frames
To implement yaw control of Crazyflie, additional markers could be added to 
Crazyflie's structure to determine its yaw position around its vertical z axis. We have 
selected not to implement yaw control at this time, but plan to control Crazyflie in x, 
y, and z placing the quadrotor so that its x axis aligns parallel to the Kinect's -x axis. 
The value for yaw control will be set to 0.

Chapter 9
[ 365 ]
How to control Crazyflie
As you have seen throughout this book, the cmd_vel topic (the geometry_msgs/
Twist message) is the common control method for ROS robots, whether driving on 
the ground or flying in the air. For TurtleBot, mobile_base_commands/velocity 
and cmd_vel_mux/input/navi are used to move around the base. For Crazyflie, the 
crazyflie/cmd_vel topic is published to control the flight of the quadrotor.
Within the crazyflie_autonomous package, the crazyflie_controller node 
(control_crazyflie.py) determines the Crazyflie's control state and publishes 
the crazyflie/cmd_vel topic. To launch the crazyflie_controller node, 
the control_crazyflie.launch file is used. This launch file also launches the 
crazyflie_window node that observes the Crazyflie and takes action when it flies 
near the edge of the Kinect image frame. The function of this node is described in the 
subsequent section, using an observer mode.
Crazyflie control states
The crazyflie_controller node has five states of flight control: idle, takeoff, 
hover, flight, and land. The private variable _cf_state is used to indicate the 
current control state. Regardless of the control state, the cmd_vel topic is published 
at a rate of 50 Hertz (20 milliseconds). This rate is obtained as the frequency 
parameter from the Parameter Server and can be changed by either adding this 
parameter and a new value to the crazyflie_controller node in the control_
crazyflie.launch file or using the rosparam set command on the terminal 
window command line. The main launch file for this mission is hover_kinectv2.
With respect to the state of control, the fields for the cmd_vel topic (the geometry_
msgs/Twist message) are assigned linear velocity values of x, y, and z, and the 
angular velocity values are left at zero. Recall from Chapter 7, Making a Robot Fly, 
that the data fields for the Crazyflie cmd_vel topic are as follows:
•	
linear.x: The pitch value is from -30 to 30 degrees
•	
linear.y: The roll value is from -30 to 30 degrees
•	
linear.z: The thrust value is from 10,000 to 60,000 (for pulse-width 
modulation (PWM) output)
•	
angular.z: This field is not currently used by the crazyflie_controller 
node

Flying a Mission with Crazyflie
[ 366 ]
The content of these data fields and the operation of the control states are described 
in detail throughout the following sections.
Using ROS services to control takeoff and land
The control states of takeoff and land are activated through ROS service calls. 
Within the crazyflie_controller node, two ROS services are created with 
callback functions to be invoked by a client when a request for the service is sent. 
The services for /crazyflie/land and crazyflie/takeoff are created by the 
following statements in control_crazyflie.py:
s1 = rospy.Service("/crazyflie/land", Empty, self._Land)
s2 = rospy.Service("/crazyflie/takeoff", Empty, self._Takeoff)
Note that the /crazyflie namespace has been appended to these services to identify 
that they are specific for the quadrotor. Land and Takeoff are private callback 
functions that handle the service requests.
These services are of the type Empty, one of the service types provided by the ROS 
std_srvs package. The std_srvs package contains common service patterns for 
signals to an ROS node. The Empty service definition contains no actual data but is 
used only to cause the execution of a function.
For the land service, the following function is executed:
def _Land(self, req):
  rospy.loginfo("Landing requested!")
  self._cf_state = 'land'
  return ()
When the /crazyflie/land service is requested, the loginfo function writes a log 
message to stdout (the terminal window) and to the /rosout topic. The message 
also appears in the ~/.ros/log file for the crazyflie_controller node. The next 
statement changes the Crazyflie control state to land. An Empty service response 
message is returned to the client node.
The takeoff service is handled by a function similar to _Land. It also writes a log 
message and changes the Crazyflie control state to takeoff. An Empty service 
response is sent back to the client node.

Chapter 9
[ 367 ]
Activating takeoff and land
The services of takeoff and land can be activated through the Xbox 360 joystick 
controller. The hover_kinectv2.launch file launches the node for joystick_
controller, which contains requests for Crazyflie takeoff, land, and emergency. 
These service requests are activated by pressing the blue (takeoff), green (land), 
or red (emergency) buttons on the Xbox 360 controller. The emergency service 
request is handled by the crazyflie_server node (crazyflie_server.cpp in the 
crazyflie/crazyflie_driver package). The code for the joystick_controller 
node is found in controller.py in the crazyflie/crazyflie_demo package.
What makes takeoff and land work?
The flight controls for takeoff and land are part of the state-based logic of the 
iteration function of control_crazyflie.py. When _cf_state is idle, the 
linear velocity values of x, y, and z (pitch, roll, and thrust, respectively) are set to 
0.0. The thrust variable is also set to 0.0. The location of the Crazyflie received as 
a transform is saved in the takeoff_position variable. This takeoff_position 
variable is used during the takeoff control state.
When the _cf_state control state is takeoff, the cmd_vel linear velocity values 
of x and y are set to 0.0. The vertical value y of the takeoff_position variable 
(takeoff_position[1]) is used to compute an upper takeoff height of 25 pixels in 
y, above its takeoff y value. When Crazyflie's position in the Kinect's image frame 
has achieved that height, the _cf_state control state will transition to flight. If the 
value of the thrust variable exceeds 50,000, this condition will also transition the 
_cf_state from takeoff to flight.
During takeoff, the value of the cmd_vel linear z velocity (thrust) is incremented 
by 10,000 multiplied by a delta time dt and a fudge factor ff. The delta time is 
computed as the time between the last iteration cycle and the present iteration 
cycle, which is typically 0.02 seconds (based on 50 Hertz). The fudge factor is an 
easy way to vary the amount of thrust increase applied. When the value of the thrust 
reaches 36,000, the increments of additional thrust decrease by approximately 
one-third to slow the ascent of the Crazyflie.
When the upper takeoff height is achieved or thrust is greater than 50,000, the 
previous error and time values for the PID controllers are reset to zero. The initial 
integral value for the z PID controller is set to the following:
(current thrust value - 1500) / (ki for the z PID controller)

Flying a Mission with Crazyflie
[ 368 ]
Success messages are sent to the log file, and the /rosout topic to indicate takeoff is 
achieved. Info messages are also sent to log the data being published in the cmd_vel 
messages.
Using PID control for hover and flight
The control states of hover and flight utilize the PID class constructor, attributes, and 
methods from pid.py and data from crazyflie2.yaml. There are three PID objects 
created to provide proportional, integral, and derivative control for Crazyflie's 
linear x, y, and z (pitch, roll, and thrust) values. The crazyflie_controller 
node instantiates a separate flight PID controller for X, Y, and Z, as shown in the 
statements below:
from pid import PID      # for PID class, attributes and methods
# object instances of type PID with initial attributes assigned
self.m_pidX = PID(rospy.get_param("~PIDs/X/kp"),
                  rospy.get_param("~PIDs/X/kd"),
                  rospy.get_param("~PIDs/X/ki"),
                  rospy.get_param("~PIDs/X/minOutput"),
                  rospy.get_param("~PIDs/X/maxOutput"),
                  rospy.get_param("~PIDs/X/integratorMin"),
                  rospy.get_param("~PIDs/X/integratorMax"))
self.m_pidY = PID(rospy.get_param("~PIDs/Y/kp"),
                  rospy.get_param("~PIDs/Y/kd"),
                  rospy.get_param("~PIDs/Y/ki"),
                  rospy.get_param("~PIDs/Y/minOutput"),
                  rospy.get_param("~PIDs/Y/maxOutput"),
                  rospy.get_param("~PIDs/Y/integratorMin"),
                  rospy.get_param("~PIDs/Y/integratorMax"))
self.m_pidZ = PID(rospy.get_param("~PIDs/Z/kp"),
                  rospy.get_param("~PIDs/Z/kd"),
                  rospy.get_param("~PIDs/Z/ki"),
                  rospy.get_param("~PIDs/Z/minOutput"),
                  rospy.get_param("~PIDs/Z/maxOutput"),
                  rospy.get_param("~PIDs/Z/integratorMin"),
                  rospy.get_param("~PIDs/Z/integratorMax"))
self.m_pidYaw = PID(rospy.get_param("~PIDs/Yaw/kp"),
                    rospy.get_param("~PIDs/Yaw/kd"),
                    rospy.get_param("~PIDs/Yaw/ki"),
                    rospy.get_param("~PIDs/Yaw/minOutput"),
                    rospy.get_param("~PIDs/Yaw/maxOutput"),
                    rospy.get_param("~PIDs/Yaw/integratorMin"),
                    rospy.get_param("~PIDs/Yaw/integratorMax"))

Chapter 9
[ 369 ]
A PID controller is also created for yaw control but is not used at this time. 
The values of parameters kp, kd, ki, minOutput, maxOutput, integratorMin, 
and integratorMax are loaded from the crazyflie2.yaml file as part of the 
control_crazyflie.launch process. This arrangement of loading the parameters 
from the YAML file has made it quick and easy to change parameters while testing 
flight control.
The PID class has several methods to perform operations for the PID controller 
object instance. A method to reset the controller is provided by the reset method. 
This method sets the m_integral and m_previousError values to zero and the 
m_previousTime to the current time. The setIntegral method sets the m_integral 
value to a value passed to the function. The third method update performs the PID 
calculations between the current location and the target location, as shown in the 
following statements:
def update (self, value, targetValue):
  time = float(rospy.Time.to_sec(rospy.Time.now()))
  dt = time - self.m_previousTime
  error = targetValue - value
  self.m_integral += error * dt
  self.m_integral = max(min(self.m_integral, 
  self.m_integratorMax),self.m_integratorMin)
  p = self.m_kp * error
  d = 0
  if dt > 0:
    d = self.m_kd * (error - self.m_previousError)/dt
  i = self.m_ki * self.m_integral
  output = p + d + i
    
  self.m_previousError = error
  self.m_previousTime = time
  return max(min(output, self.m_maxOutput), self.m_minOutput)
Note that rospy.loginfo statements have been removed to enhance clarity.

Flying a Mission with Crazyflie
[ 370 ]
In the update method, the current time in seconds is determined by a call to the 
rospy routines, Time.now and Time_to_sec. The variable dt is set to the number 
of seconds elapsed between the last call to the controller and the current time. 
The difference between value and targetValue is stored as the variable error. 
This error value is multiplied by m_kp to obtain the proportional variable p. The 
difference in this error value and the last error value is calculated and divided by 
the delta time dt. This value is multiplied by m_kd to find the derivative term d. The 
last term, the integral i, is calculated as the value of m_ki times m_integral. The 
three terms p, i, and d are added to compute the output variable. This variable is 
compared to the m_minOutput and m_maxOutput values to determine whether it falls 
within this range. If it does, then the value of output is returned. Otherwise, if the 
output value is larger than m_maxOutput, m_maxOutput is returned. If it is less than 
m_minOutput, m_minOutput is returned.
Using an observer node
Throughout the testing phase for this mission, Crazyflie would exhibit some erratic 
behavior. Due to the modular nature of ROS, we decided to implement an observer 
node that would keep track of the location of Crazyflie. The node crazyflie_window 
(in watcher.py) listens to the tf transforms publishing the location of Crazyflie. In a 
loop that runs at 10 times a second, the following statements are executed:
if listener.frameExists(camera_frame) and 
listener.frameExists(crazyflie_frame):
  t = listener.getLatestCommonTime(camera_frame, crazyflie_frame)
  trans, rotate = listener.lookupTransform(camera_frame, 
  crazyflie_frame, t)
This code checks the transforms that are buffered by the listener for the existence of 
a transform between crazyflie/baselink and kinect2_ir_optical_frame. When 
this specific transform is found, the data fields for translational and rotational data 
are extracted into the trans and rotate variables. The trans variable contains the 
location in the x, y, and z of the Crazyflie. This location is compared to the edge of 
the Kinect image:
if (trans[0] < 100) or (trans[0] > (camera_width - 100)) or 
(trans[1] < 20) or (trans[1] > (camera_height - 20)):
  # Crazyflie is going outside the frame
  rospy.loginfo("Crazyflie outside of window %f %f %f",
                trans[0], trans[1], trans[2])
  rospy.loginfo("Landing requested")
 

Chapter 9
[ 371 ]
  # wait until land service is available, then create handle for it
  rospy.wait_for_service('/crazyflie/land')
  try:
    _land = rospy.ServiceProxy('/crazyflie/land', Empty)
    _land()
  except rospy.ServiceException, e:
    rospy.loginfo("Service call failed: %s", e)
If the position of Crazyflie is within 100 pixels of the left or right edge of the image 
frame or within 20 pixels of the upper or lower edge, a service request is made for 
Crazyflie to land. A private local proxy _land is used to make the service call with 
an Empty service request. The land service request is handled by the crazyflie_
controller node as described in the previous section, Using ROS services to control 
takeoff and land.
Messages are sent to the log file and the /rosout topic to identify the location of 
the Crazyflie that caused the crazyflie_window node to send the service request. 
These messages are important when determining the events of Crazyflie's flight. The 
Kinect's depth data trans[2] is too erratic to use for this monitoring instance.
The next sections describe how the Crazyflie operates when the _cf_state variable 
is set to flight. The Crazyflie will either hover in place or fly to a target depending 
on whether any target_pose messages have been received.
Flying Crazyflie
Now we are finally ready to fly our mission. To make Crazyflie fly to a target 
requires that the quadrotor must be controllable to hover in place. Once this task is 
successful, the next step is to fly to a stationary target. We will introduce the steps to 
accomplish these tasks in the next sections.
Hovering in place
The first step to control Crazyflie's flight is the ability to demonstrate control of the 
quadrotor hovering in one location. To start the process, use the launch command:
$ roslaunch crazyflie_autonomous hover_kinectv2.launch
Then, turn on Crazyflie and let it run through its startup routine. When it is 
complete, type in a second terminal window:
$ roslaunch crazyflie_autonomous control_crazyflie.launch

Flying a Mission with Crazyflie
[ 372 ]
The hover mission can be started by pushing the Takeoff (blue) button on the Xbox 
360 controller. After Crazyflie has achieved takeoff, the quadrotor will begin to 
receive cmd_vel (geometry_msgs/Twist) messages to stay in its same location with 
respect to the Kinect image frame. Crazyflie will try to maintain this location until 
the Land (green) button on the controller is pressed. If Crazyflie drifts to the edge of 
the Kinect image, a land service request will be generated by the crazyflie_window 
node to (hopefully) safely land the quadrotor.
What makes hover work?
As described in the What makes takeoff and land work? section, the _cf_state variable 
changes from takeoff to flight when one of the two takeoff conditions is met. 
These conditions are that the Crazyflie's position in y has changed by 25 pixels or 
that the thrust value is over 50,000. When one of these conditions is met, the initial 
values for the PID controllers are reset, and the initial integral variable for the Z PID 
controller is set.
The initial check in flight mode is to determine whether the target flag has been 
set to True. This flag is set by the _update_target_pose function if a target pose 
(geometry_msgs/PoseStamped) message has been received. If this message has not 
been received, then the target flag is False and _cf_state is set to hover. 
The current x, y, and z position of Crazyflie is captured as the three element 
list hover_position.
As described in the Using Kinect and OpenCV section, the crazyflie_detector node 
publishes the Crazyflie's tf transform as its x, y, and z position in the Kinect image 
frame. The crazyflie_controller node calls the _getTransform function every 
20 milliseconds to get this transform and uses it for processing both flight and 
hover control.
In hover mode, the PID controllers are used to calculate the linear values of x, y, 
and z for the cmd_vel message. Crazyflie's (x, y, z) position in the Kinect frame is 
altered so that the direction of control corresponds to Crazyflie's coordinate frame 
(x – forward, y – left, and z – up). First, the value of Crazyflie's location in its x axis 
needs to increase as it flies to the left in the image. The value in its z axis needs to 
increase as Crazyflie flies up in the image. For Crazyflie's y axis, the values need 
to decrease as it flies closer to the Kinect camera. The following lines of code show 
the remapping of Crazyflie's positions (current and hover) in the camera frame to 
Crazyflie's coordinate axes:
# camera -x position
self.fly.linear.x = self.m_pidX.update(

Chapter 9
[ 373 ]
                        (self.camera_width - cf_trans[0]),
                        (self.camera_width - 
                         self.hover_position[0]))
# camera -z position
if cf_trans[2] == 0.0:
  self.fly.linear.y = self.m_pidY.update(self.hover_position[2],
                                         self.last_depth)
else:
  self.fly.linear.y = self.m_pidY.update(self.hover_position[2],
                                         cf_trans[2])
  self.last_depth = cf_trans[2]
# camera -y position
self.fly.linear.z = self.m_pidZ.update(
                       (self.camera_height - cf_trans[1]),
                       (self.camera_height - 
                        self.hover_position[1]))
Note that rospy.loginfo statements have been removed to enhance clarity.
The m_pidX.update method is called to calculate the correction needed in x to 
maintain the hover position. The x position values for both current and hover are 
subtracted from the camera image width to achieve the correct difference. The value 
returned is used as the cmd_vel linear.x (pitch) control.
The camera's z position (depth) maps to the y axis control for Crazyflie. Sometimes, 
Kinect publishes bad depth values for Crazyflie's current position, so the case that 
the z (depth) value equals zero is checked. If a zero value is found, the last good 
depth value is used. The m_pidY.update method is called to handle these z (depth) 
values, and the resulting value is assigned to the cmd_vel linear.y (roll) control.
The Kinect y position corresponds to the z axis control for Crazyflie. The camera 
height value is used to flip the current location and the hover location values to 
increase as Crazyflie moves closer to the top of the image frame. The m_pidZ.update 
method is called to process the y values and provide the resulting cmd_vel linear.z 
(thrust) control.
Now we will look at how the Crazyflie is automatically controlled as it flies 
to a target.

Flying a Mission with Crazyflie
[ 374 ]
Flying to a stationary target
Each step in this mission builds on the previous step. To get Crazyflie to fly to a 
particular location, a separate node was created and named target_detector to 
handle the operation of locating the target and publishing its location. The code 
for this node is contained in the detect_target.py script in the crazyflie_
autonomous package. To perform this phase of the mission, begin by typing the 
launch command:
$ roslaunch crazyflie_autonomous hover_kinectv2.launch
Then turn on Crazyflie and let it run through its startup routine. When it is complete, 
start the crazyflie_controller node by typing in a second terminal window:
$ roslaunch crazyflie_autonomous control_crazyflie.launch
Then in a third window, execute the following command to start the target_
detector node:
$ rosrun crazyflie_autonomous detect_target.py
The target_detector node will begin to transmit messages containing the location 
of the target marker with respect to the Kinect image frame.
Begin the mission by pushing the Takeoff button on the joystick controller. After 
Crazyflie has achieved takeoff, the quadrotor will begin to receive cmd_vel 
(geometry_msgs/Twist) messages to fly towards the target marker. Since the target 
is stationary, the location message is only published at 1 Hz. The quadrotor will 
hover above the target until the Land button on the joystick controller is pressed.
The following image shows our mission setup. The Crazyflie is positioned on 
the table with a blue circle around its ball marker, and the pink target (on top of 
TurtleBot in the lower-left corner of the screenshot) has a red rectangle around it:

Chapter 9
[ 375 ]
Crazyflie and target positions located
Using the software described in this chapter, Crazyflie was able to take off from 
a position, hover, and then fly and land at a second position identified by a 
target marker.
The next section elaborates on the target detection method.
What makes target detection work?
The target_detector node works similar to the crazyflie_detector node. The 
node subscribes to the Image messages from the Kinect specifying the qhd quality. 
Images of both image_color_rect and image_depth_rect are requested. When a 
color image (image_color_rect) is received, the callback function image_callback 
will use the same object detection techniques, as described for the Crazyflie, to find 
the target within the Kinect color image frame. The u and v pixel coordinates of the 
center of the target are saved by this function. These pixel coordinates are used by 
the callback function depth_callback for the depth image (image_depth_rect) to 
access the depth value at that location.

Flying a Mission with Crazyflie
[ 376 ]
These values of u, v, and depth are used as x, y, and z respectively by the 
update_target_pose function. This function assigns these values to a 
PoseStamped message and publishes the message.
When _cf_state changes to flight from takeoff, the target flag is checked to 
determine whether a target PoseStamped message has been received. If the target 
flag is true, the target message has been received and _cf_state will stay in flight 
mode. The x, y, and z PID controllers are used to calculate the control values for 
the cmd_vel message. Similar to the processing described for hover, the Crazyflie's 
location in the Kinect image frame must be changed to correspond to the direction of 
control for the Crazyflie's coordinate frame. The previous section What makes hover 
work? describes this remapping of Crazyflie's position in the camera frame to its 
coordinate axes. The following lines of code show this remapping:
# camera -x position
self.fly.linear.x = self.m_pidX.update( 
                   (self.camera_width - cf_trans[0]), 
                   (self.camera_width - 
                    self.target_position.pose.position.x))
       
# camera -z position
self.fly.linear.y = self.m_pidY.update(cf_trans[2], 
                    self.target_position.pose.position.z)
# camera -y position
self.fly.linear.z = self.m_pidZ.update( 
                    (self.camera_height - cf_trans[1]), 
                    (self.camera_height – 
                     self.target_position.pose.position.y + 25))
The update methods of the m_pidX, m_pidY, and m_pidZ object instances are used 
to obtain the values for the cmd_vel message, as explained in the previous section. 
The difference is the hover values have been replaced by the PoseStamped position 
values of the target coordinates.
Learned lessons
This chapter would not be complete without documenting a few of the lessons we 
learned along the way and would like to pass on to you. These are as follows:
•	
Weight is important: Even for a 27 gram quadrotor, the position of the 
battery and the weight and the position of a lightweight ball make a big 
difference in the pitch and roll control of the Crazyflie.

Chapter 9
[ 377 ]
•	
PID parameters are hard to select: Testing and changing parameters for the 
PID control of a quadrotor is a never-ending cycle. If you can get the weight 
and balance problem mentioned previously fixed, you have a chance at 
establishing more stable PID control parameters.
•	
Don't fly in a cluttered environment: Sometimes problems are created 
because the camera detects erroneous things in the environment, and this 
fluctuation in data can wreak havoc on your mission.
Logging messages with rosout and rospy
The use of logging messages was critical to the development of this mission. 
ROS provides a number of ways to gain insight into the output of nodes through 
publishing information and debugging messages to rosout. These messages can 
be viewed while processes are active with rqt_console or via stdout (a terminal 
window). Messages can also be examined afterwards through log files written for 
each ROS node under the ~/.ros directory.
There are several levels of logging messages that can be written to rosout. 
These levels include the following:
•	
DEBUG: For information needed when the system is not working, 
but should not be seen when the system is working correctly
•	
INFO: For information useful to the user
•	
WARN: To caution the user regarding less than optimal results
•	
ERROR: To warn the user of a serious condition
•	
FATAL: To warn the user of an unrecoverable condition
The use of rospy.loginfo and rospy.debug messages have been scattered 
throughout the code developed for this mission. We encourage the use of these 
logging methods as you adapt to this software for your purposes.
Now, we wrap up this adventure!

Flying a Mission with Crazyflie
[ 378 ]
Summary
The aim of this chapter was to stretch your knowledge of ROS by implementing 
an advanced practical experience to identify and highlight some of the ROS 
advantages. A ROS system of nodes was created to visualize the environment in 
which a Crazyflie quadrotor was seen and controlled. The Kinect for Windows V2 
depth camera was used to visualize this environment, and ROS nodes handled the 
detection of markers on the Crazyflie and the target. The location of the Crazyflie 
was identified in Cartesian coordinates (x, y, z), with the x and y values referring to 
the quadrotor's position in the image frame and z referring to its distance from the 
camera. These coordinates were converted into a tf transform and published. The 
target location was published in a message by a separate ROS node.
The advantage of ROS layers of tf and message passing leaves lower-level details to 
be handled by another dedicated node. The tf transform for the Crazyflie was used 
by a controller node to apply PID control to Crazyflie's flight. This controller node 
implemented a state machine to control the Crazyflie based on its state of flight. 
These states included idle, takeoff, hover, flight, and land. PID controllers were 
implemented to determine the control command based on the position error for 
hover and flight to the target. Crazyflie control commands included pitch, roll, and 
thrust components to accomplish the feat of hovering or navigating the quadrotor 
accurately to a target location.
The next chapter Extending Your ROS Abilities, will introduce additional advanced 
ROS robotic applications to stretch your imagination even further. The development 
of ROS applications continues to provide an overwhelming realm of possibilities to 
add capabilities to your ROS robots and their network of systems.

[ 379 ]
Extending Your ROS Abilities
Throughout this book, we have taught you ROS through examples using real robots. 
These robots are available in simulation, so purchasing a robot is not required. 
Through simulation and step-by-step instructions, the concepts of ROS have been 
taken from description to implementation in order to make the advantages of ROS 
tangible in your understanding.
The purpose of this final chapter is to spark your imagination by delving into some 
of the more entertaining applications of ROS. At the same time, we will identify the 
resources for you to gain full understanding of the subject matter. The topics selected 
are introduced to entice further exploration of ROS. If your imagination is stimulated 
to eagerly anticipate additional capabilities for your robot, our mission is complete.
In this chapter, we will introduce the following:
•	
Voice control of a robot
•	
Making a robot speak
•	
Enabling a robot to recognize faces
Again, never fear. If you do not own a robot using ROS yet, these capabilities can 
be mastered in simulation through any of the various ROS robots available in the 
Gazebo simulation environment. So, go ahead and implement some of the great ROS 
applications described here in the concluding sections of this book.
Controlling a robot with your voice
A valuable way to interact with your robot is to use voice commands. This method 
is useful when you have no other means of control (keyboard, joystick, or smart 
device), or you would like a more convenient method. This type of technology is 
critical for people with physical disabilities, and in the future, robots will be able to 
provide helpful functions for all mankind through the use of voice commands.

Extending Your ROS Abilities
[ 380 ]
Today though, we can implement voice control over our ROS robot to enable any 
action or process you can control by any other method. These actions are typically 
navigating the robot or controlling an arm, but this list is virtually endless. With 
proper hardware, you could verbally command your robot to do the following:
•	
Change the TV channel
•	
Retrieve a beverage from the kitchen
•	
Let the dog outside
Do whatever you can imagine. Currently, there are two primary speech recognition 
tools available in ROS: Sphinx and rospeex. These tools and their ROS interfaces are 
introduced in the following sections.
Sphinx
Sphinx is an open source speech recognition software developed at Carnegie 
Mellon University (CMU) in 2000. This software is composed of a speech recognizer 
component and a speech decoder that comes with acoustic models and sample 
applications. An acoustic model trainer component is also available to compile 
additional language models. The CMU Sphinx website contains a wealth of tutorials, 
documentation, inspiring ideas, and information on a wide range of application 
developments; refer to http://cmusphinx.sourceforge.net/.
According to the website, the CMU Sphinx toolkit is composed of the following 
elements with their applications listed:
•	
Pocketsphinx – A lightweight recognizer library written in C
•	
Sphinxtrain – Acoustic model training tools
•	
Sphinxbase – A support library required by Pocketsphinx and Sphinxtrain
•	
Sphinx4 – An adjustable, modifiable recognizer written in Java
For our purposes, Pocketsphinx will provide cross-platform support with 
speaker-independence and large-vocabulary speech proficiency.
CMU Sphinx provides language models for US English, UK English, French, 
German, Dutch, Mandarin, Spanish, and Russian, trained with a variety of acoustic 
conditions. Try implementing your voice control in German to impress your friends!
To implement this speech recognition capability in ROS, the pocketsphinx package 
has been developed by Michael Ferguson to provide a Python-based wrapper 
around the CMU Pocketsphinx speech recognizer. This package is described at the 
ROS wiki site, http://wiki.ros.org/pocketsphinx.

Chapter 10
[ 381 ]
To install the gstreamer plugin for Pocketsphinx and the pocketsphinx ROS 
package, type the following commands:
$ sudo apt-get install gstreamer0.10-pocketsphinx
$ sudo apt-get install ros-indigo-pocketsphinx
The gstreamer software will automatically divide the incoming audio levels into 
utterances to be recognized. The pocketsphinx package provides ROS services 
to start (~start) and stop (~stop) listening to the audio stream. These services 
are handled by the recognizer node (in recognizer.py), which converts the 
audio stream into an output (~output) text stream. The audio stream is matched 
with words or phrases in the current vocabulary files. These language model and 
dictionary files must be specified as parameters for the nodes execution. Within the 
pocketsphinx/demo directory, example launch files are provided to enable the user 
to quickly implement vocabulary from two predefined applications: mobile base 
commands and phrases and names from the RoboCup@Home contest.
The pocketsphinx package also provides the voice_cmd_vel node (in voice_cmd_
vel.py) as a simple example program for developing ROS node to convert speech 
commands into cmd_vel topics to control a mobile robot. This node is launched by 
the voice_cmd.launch file in the demo directory. Commands to control a mobile 
robot, such as forward and move left, are available in the corpus file voice_cmd.
corpus.
Language model and dictionary files are automatically built from a vocabulary (or 
corpus) of commands (words or phrases) using the Sphinx Knowledge Base Tool. 
To create your own vocabulary for robot commands, use lmtool at http://www.
speech.cs.cmu.edu/tools/lmtool-new.html.
Software to use a microphone to capture audio is required. The ROS stack 
audio-common provides the audio-capture package to transport voice from 
a microphone or an audio file. The audio-common metapackage can be installed 
using this command:
$ sudo apt-get install ros-indigo-audio-common
The four packages contained in audio-common are:
•	
audio_capture: This package provides the code to capture audio from a 
microphone or a file and transport it to a destination for playback
•	
audio_play: This package receives audio messages from an audio_capture 
node and outputs the messages to local speakers

Extending Your ROS Abilities
[ 382 ]
•	
audio_common_msgs: This package contains message definitions for 
audio transport
•	
sound_play: This package is utilized in playing sound files and 
synthesizing speech
An excellent reference for implementing the voice command capability can be 
found in the ROS by Example-Indigo (Volume 1) book by Patrick Goebel, In Chapter 9 
on Speech Recognition and Synthesis, you are lead through the process of setting up 
the microphone input and testing out the software on the RoboCup@Home corpus 
provided with the pocketsphinx package. Additional information also describes 
how to implement a vocabulary of commands for your own robot and set up a node 
to process these voice commands.
rospeex
The rospeex ROS metapackage provides a spectrum of speech communication 
tools for speech recognition and speech synthesis. The rospeex metapackage 
was developed by the National Institute for Information and Communication 
Technology (NICT) in Kyoto, Japan. This cloud-based software supports English, 
Japanese, Chinese, and Korean speech recognition and synthesis. APIs for both 
Python and C++ are available.
The rospeex metapackage consists of the following packages:
•	
rospeex_audiomonitor: This package provides a stable waveform monitor 
based on the external library qtmobility-dev.
•	
rospeex_core: This package provides the core nodes for rospeex.
•	
rospeex_if: This package contains the interface libraries for C++ 
and Python.
•	
rospeex_launch: This package contains the launch files for rospeex 
core nodes.
•	
rospeex_msgs: This package defines the messages used in rospeex.
•	
rospeex_samples: This package provides example software using rospeex.
•	
rospeex_webaudiomonitor: This package provides the beta version of a 
browser-based waveform monitor. This version requires external support 
from Google Chrome or Mozilla Firefox.
The ROS wiki site for rospeex is http://wiki.ros.org/rospeex.

Chapter 10
[ 383 ]
To install the rospeex metapackage, type the following command:
$ sudo apt-get install ros-indigo-rospeex
Be sure to verify that your web browser will support Rospeex's waveform monitor:
•	
Google Chrome version 35.0 to 46.0
•	
Mozilla Firefox version 30.0 or later
To check your installation and run examples, visit http://rospeex.org/top/.
Enabling a robot to speak
With the ability to speak, your robot can take on a personality of its own. There 
are multiple styles of voices available for your robot, and its ability to carry on a 
conversation is only limited by your imagination.
The two current methods of providing the capability for your robot to speak are 
analogous to the speech recognition abilities mentioned in the previous section. The 
ROS package sound_play (from the audio-common metapackage) offers one method 
and the Rospeex software provides another.
The ROS package sound_play supports playing built-in sound files, OGG and WAV 
files, and processing speech synthesis data produced via Festival. Festival is a text-to-
speech (TTS) ability developed by the University of Edinburgh. Multiple languages 
are supported through this project and the companion Festvox project at CMU. More 
information on these speech synthesis projects can be found at:
•	
http://www.cstr.ed.ac.uk/projects/festival/
•	
http://festvox.org/
•	
http://ubuntuforums.org/showthread.php?t=751169
The sound_play package provides the ROS node sound_play (in soundplay_node.
py) to translate commands on the ROS topic robotsound. API bindings in C++ or 
Python allow this node to be used independent of the details of the sound message 
format. The play_sound package say.py script can be used from the terminal 
command line to input a text string or from standard input. The details of its usage 
can be found at http://wiki.ros.org/sound_play.
It is important to check out the package dependencies listed in the readme file at 
https://github.com/ros-drivers/audio_common/tree/master/sound_play.

Extending Your ROS Abilities
[ 384 ]
It is also important to work through the tutorials to set up your Ubuntu sound 
drivers and USB speaker system to play sounds and speech. The three tutorials for 
setting up and creating the sound_play interface are found at http://wiki.ros.
org/sound_play/Tutorials.
The Rospeex software described in the previous section also enables your robot to 
speak. Its website provides an example of a talking clock to highlight the capabilities 
of this software. Speech synthesis is available for Japanese, English, Chinese, and 
Korean languages. By using its APIs in Python or C++, a sample function for a 
simple spoken dialogue can be written in as little as 10 lines.
Next, we will progress to making it possible for your robot to recognize faces if it is 
equipped with a vision sensor like a RGB or USB camera.
Enabling a robot to recognize faces
This chapter has presented two abilities to make your robot more interactive. To 
talk to your robot and have it speak to you makes it become alive with personal 
characteristics. The more voice commands or phrases it responds to and the more 
phrases your robot can speak, the more personality it will have.
An additional customization would be to interact with your robot face-to-face. 
Having your robot turn when spoken to and having him find the face of the speaker 
will add an air of awareness. The capability to detect a face would be an advantage.
There are two primary methods available in ROS to detect faces from a camera image 
stream. The first method is using a cascade classifier with Haar or LBP-like features. 
This relies on a method provided by OpenCV. The second method uses Eigenfaces 
to recognize faces in an image. Descriptions of these two methods are provided in 
the next two sections. The best approach to test these methods is to begin by using a 
USB webcam to stream the video images.
Face detection with a cascade classifier
With OpenCV fully integrated with ROS, the algorithms and functions available 
from this library for robotics makes adding a new capability, such as face detection, 
extremely easy. The OpenCV algorithms have been designed to perform well and 
also refined for speed, efficiency, and accuracy.

Chapter 10
[ 385 ]
The OpenCV cascade classifier has two stages of implementation: training and 
detection. In the training stage, OpenCV provides the opencv_traincascade 
application to detect features in the images for classification. Two types of features 
can be classified by opencv_traincascade: Haar or Local Binary Patterns (LBPs). 
Datasets generated by either method can provide good quality data for classification, 
but the LBP features are processed much more quickly. The LBP features are 
handled as integer values, which make processing LBP data several times faster 
than processing Haar data.
The OpenCV User Guide provides an in-depth understanding of the cascade 
classifier and instructions on how to prepare training data and run the training 
software; refer to http://docs.opencv.org/2.4/doc/user_guide/ug_
traincascade.html.
The training software generates an .xml classifier file with the classified features. 
For faces, it is interesting to know that a training set of hundreds or even thousands 
of sample faces need to be included in the dataset of positive samples. The 
dataset should contain samples of all race and age groups as well as facial hair 
variations and facial emotions. These positive samples should be .png files and 
the face or facial features should be marked with bounding shapes. The opencv_
createsamples utility is used to generate this image set into vector (vec) format for 
training. A set of negative images is also needed that do not contain any faces. With 
these two datasets, the opencv_traincascade application can be used to train a 
cascade classifier.
Already prepared datasets are available in the OpenCV GitHub repository. Haar 
cascade classifier files exist for detecting a face from the front and also from the 
side. Files with facial feature detection, such as eyes and smiles, are also available. A 
sample vector file trainingfaces_24-24.vec is offered for training a face detector. 
These files can be found at https://github.com/Itseez/opencv/tree/master/
data.
To understand how the cascade classifier in OpenCV works, refer to the tutorial at 
http://docs.opencv.org/2.4/doc/tutorials/objdetect/cascade_classifier
/cascade_classifier.html#cascade-classifier.
This tutorial uses the haarcascade_frontalface_alt.xml and haarcascade_eye_
tree_eyglasses.xml files from the GitHub repository.
The API for cascade classifier object detection is found at http://docs.opencv.
org/2.4/modules/objdetect/doc/cascade_classification.html.

Extending Your ROS Abilities
[ 386 ]
The objdetect module in OpenCV handles the detection of objects after they have 
been classified in the training stage of the cascade classifier. This object detection 
method was proposed by Paul Viola and Michael Jones and improved upon by 
Rainer Lienhart. An explanation of the classifier is provided at the website.
A good example of implementing an OpenCV Haar cascade face detector can be 
found in the book ROS by Example-Indigo (Volume 1) by Patrick Goebel. You will be 
shown how a cascade classifier is used and the software is available to adapt to your 
own application.
Using ROS package face_detector
The ROS package face_detector offers an improvement on the OpenCV face 
detection method provided by the Haar-like cascade classification described in the 
previous section. This package is designed to work with a stereo camera or a 3D 
sensor, such as Kinect, ASUS Xtion, or PrimeSense. The original set of face detections 
obtained from the cascade classifier is processed a second time using the depth 
information from the images. Each positive face image in the dataset is evaluated 
based on the depth information to determine whether the size of the face is realistic 
for a human. If so, the face is preserved in the dataset. Many false positives are 
pruned from the dataset using this method.
The face_detector package can be implemented in two ways: on a continuous 
image stream or as an action. The action method allows you to start face detection 
at any time, and the process runs independently until at least one face is found.
To implement this ROS package on your robot, visit http://wiki.ros.org/face_
detector.
The use of a stereo camera or a 3D sensor is necessary.
Still, this method of face detection is inferior when based on just the detection of 
geometric features within a face. Research has shown that a more robust approach 
is needed to increase the likelihood that face recognition will be accurate. Methods 
such as Eigenfaces and Fisherfaces have been advanced to adopt a more holistic 
approach to face recognition. These methods have been incorporated into a new 
class of objects performing face recognition in OpenCV.

Chapter 10
[ 387 ]
Face recognition with OpenCV
The new FaceRecognizer class in OpenCV implements new technology in 
development for face recognition. The presentation and analysis of the algorithms 
used for this class is not the subject of this book, but we will present the available 
options for this software in order for you to implement them on your robot projects. 
The algorithms available for the FaceRecognizer class are as follows:
•	
Eigenfaces (via createEigenFaceRecognizer())
•	
Fisherfaces (via createFisherFaceRecognizer())
•	
Local Binary Pattern Histograms (via createLBPHFaceRecognizer())
Refer to the OpenCV tutorial on Face Recognition found at http://docs.opencv.
org/2.4/modules/contrib/doc/facerec/facerec_tutorial.html.
A detailed explanation of these face recognition algorithms is provided. Instructions 
on preparing the datasets are also given. Face datasets for this FaceRecognizer class 
reference the databases found at http://face-rec.org/databases/.
The FaceRecognizer class is described at http://docs.opencv.org/2.4/modules/
contrib/doc/facerec/facerec_api.html.
To explore more of OpenCV's face recognition capabilities, such as gender 
classification, check out the full list of documentation at http://docs.opencv.
org/2.4/modules/contrib/doc/facerec/index.html.
Using ROS package cob_people_detection
The ROS metapackage cob_people_perception was developed by the Fraunhofer 
Institute for Manufacturing Engineering and Automation for their Care-o-bot 
research platform (http://www.care-o-bot.de/en/research.html). This 
metapackage contains several packages for detecting and recognizing faces as 
well as detecting and tracking humans with a 3D sensor or laser scanner. The 
cob_people_detection package performs head and face detection utilizing a 
Viola-Jones classifier. The second step employs face identification techniques based 
on either Eigenfaces or Fisherfaces to perform the complete identification of known 
faces. The tracking of these faces increases confidence in the results.

Extending Your ROS Abilities
[ 388 ]
The first time the people_detection client node is run, no identification data exists 
on the computer. The node can be run either in manual or automatic mode, but 
images captured should only have a single face present in the image. In automatic 
mode, images of an individual should be captured between 40 and 100 times varying 
poses and lighting conditions. It is also important to capture the images of more 
than one person during the first session of data gathering. Detected faces should 
be tagged with an identification label. When the training is complete, a recognition 
model should be build from the training set data.
Complete instructions for installing the cob_people_detection package, creating 
the database, and using the software is found at http://wiki.ros.org/cob_
people_detection.
The GitHub repository for cob_people_perception and this package is found at 
https://github.com/ipa-rmb/cob_people_perception.
Using ROS package face_recognition
The ROS package face_recognition for the recognition of faces in a video image 
stream was developed in the Robolab at the University of Luxembourg. This package 
was developed by Shervin Emami and uses the FaceRecognizer class provided by 
OpenCV. The face_recognition ROS package provides a simple actionlib server 
interface for performing different face recognition operations in an image stream.
Face recognition is performed using Eigenfaces and Principal Component Analysis 
(PCA) provided by the FaceRecognition methods. Emami describes the methods 
further on his website (http://www.shervinemami.info/faceRecognition.html).
Further information on this package can be found at http://wiki.ros.org/face_
recognition.
The repository for the face_recognition source code is available at 
https://github.com/procrob/face_recognition.
Shervin Emami has updated his face recognition techniques in his book, Mastering 
OpenCV with Practical Computer Vision Projects; refer to the Face Recognition using 
Eigenfaces or Fisherfaces section in Chapter 8. The source code for this chapter is 
freely available at https://github.com/MasteringOpenCV/code/tree/master/
Chapter8_FaceRecognition.
To use this software, at least three face and eye detection .xml datasets will need 
to be downloaded from the OpenCV GitHub data repository; refer to https://
github.com/Itseez/opencv/tree/master/data (link repeated from earlier).

Chapter 10
[ 389 ]
With this variety of techniques for face detection and recognition, it will be a great 
advantage to have your robot recognize you and your friends!
Summary
In this final chapter of the book, we have presented some concepts and references 
to further extend the capabilities you can endow on your robot. These new concepts 
should increase your desire to learn more about ROS. The concepts and robot 
examples presented in this book were meant to provide a solid foundation of ROS.
With the integration and influence of ROS extending to an ever increasing number of 
robots, this book attempts to cover the essential aspects of robotics systems as they 
relate to ROS. In the first chapter, we started with ROS fundamentals and used the 
Turtlesim simulation to demonstrate the concepts of ROS packages, nodes, topics, 
messages, and services. We continued by exploring a number of example robots that 
encompass a wide range of purposes:
•	
Mobile base in simulation (Turtlesim)
•	
Mobile base (TurtleBot)
•	
Air vehicle in simulation (Hector)
•	
Air vehicles (Crazyflie and Bebop)
•	
Manipulator robot (Baxter)
Along the way, ROS software was introduced that interfaces a variety of sensors, 
control devices, and controller boards. The basic structure of ROS simulation was 
learned through the use of URDF and Xacro to design robot models for simulation. 
These robot descriptions in URDF and Xacro provide the basis for the ROS tf 
transforms that map the 3D coordinate frames of the robot and its environment. 
Through the use of Gazebo and rviz, we interacted with the robots in simulation and 
saw data displayed from their sensors. Two other major ROS tools, rqt and MoveIt, 
were also described.
Now it is time to consider other aspects of ROS that will extend your learning to the 
more advanced features that ROS provides. There are a number of additional books 
and web resources available on ROS. ROS has an extensive community of users that 
develop, use, and support the open source software that we all access. We hope that 
you enjoy becoming a part of this community. We have enjoyed providing you with 
a glimpse into some of the many aspects and advantages of ROS.


[ 391 ]
Index
Symbols
3D calibration
reference  349
3D sensors
ASUS  119, 120
comparing  117
drawbacks  122
Microsoft Kinect  117, 118
PrimeSense Carmine  121, 122
reference  116
testing, in standalone mode  125, 126
3D sensor software installation
about  122
ASUS  124
camera software structure  124, 125
depth cloud  125
Kinect sensors  122
point cloud  125
PrimeSense  124
Registered DepthCloud  125
Registered PointCloud  125
terms, defining  125
3D vision systems
3D vision sensors, comparing  117
3D vision sensors, working  116
about  116
7-DOF arms
reference  188
/myandroid workspace
android_apps  316
android_core  316
android_extras  316
android_remocons  316
<transmission> elements
reference  175
A
accelerometer  259
altimeter  260
amcl  139
amcl node  145
Android packages
reference  317
Android Studio package
reference  314
Apache Ant  315
API, cascade classifier object detection
reference  385
application programming 
interface (API)  187
Arduino
references  319
ultrasonic sensor control  324
Arduino applications
reference  332
ROS and Arduino, using  332
Arduino IDE software
installing  318
reference  318
Arduino UNO
reference  326
Arduino, for creating ROS nodes
about  318
Arduino IDE software, installing  318
ROS Arduino software, installing  319-323
arm joints, Baxter
about  188, 189
control modes  193, 194
coordinate frame  192, 193
grippers  194
pitch joints  190

[ 392 ]
roll joints  190, 191
sensors  195
arm sensors, Baxter  195
articulated robot arm, controlling in Gazebo
about  169
controls, adding to Xacro robot  173, 174
Gazebo-specific elements, adding  170
robot arm, controlling with ROS 
command line  178, 179
robot arm, controlling with 
with rqt  180, 181
robot arm, fixing to world  171, 172
robot arm, viewing in Gazebo  172, 173
articulated robot arm URDF, 
building with Xacro
about  154
mesh, adding to robot arm  163-169
roslaunch, using for rrbot  157-159
Xacro include and macro tags, 
using  159-163
Xacro property tag, using  154-157
ASUS sensor  119, 120
audio-common packages
about  381
audio_capture  381
audio_common_ms  382
audio_play  381
sound_play  382
autonomous navigation, with TurtleBot
about  138, 139
driving, without steering TurtleBot  140, 141
navigation task, accomplishing  144-146
rviz control  141-144
terms, defining  139
B
banking  255
barometer  260
Baxter
about  184
arm joints  188, 189
manufacturing version  186
research robot  186, 187
tasks  185
technical specifications  186
Baxter's arms, flexing
about  208
arm movements, recording  219, 220
arm movements, replaying  219, 220
arms, controlling with joystick  214-216
arms, controlling with keyboard  212
arms, controlling with 
Python script  217, 218
arms, wobbling  211, 212
tuck command  208
untuck command  208-210
Baxter SDK software
installing  196, 197
Baxter shell
configuring  199
Baxter Simulator
about  187
installing  197-199
references  188
Baxter Simulator, launching in Gazebo
about  201
Baxter's arm controllers, tuning  220
Baxter Simulator, starting up  201-205
warm-up exercises  206, 207
Baxter software
loading  195
Baxter troubleshooting
reference  201
Bebop
about  289
communications, testing  292
flying, commands used  292
landing  294
preparing, to fly  292
references  289
take off  293
bebop_autonomy package
bebop_driver  291
bebop_msgs  291
bebop_tools  291
bebop_autonomy software
about  290
loading  290, 291
Bitcraze VM
reference  277

[ 393 ]
blue-green-red (BGR)  355
Bluetooth Low Energy (BLE)  276
Broadcom system on a chip (SOC)  333
C
Care-o-bot research platform
reference  387
Carnegie Mellon University (CMU)  380
cascade classifier
reference  385
catkin workspace
creating  9, 10
reference  9
cheat sheet  205
clockwise (CW)  256
CMU Pocketsphinx speech recognizer
reference  380
CMU Sphinx
about  380
Pocketsphinx  380
reference  380
Sphinx4  380
Sphinxbase  380
cob_people_detection package
reference  388
Complementary 
Metal-Oxide-Semiconductor 
(CMOS) sensor  116
Compressed Real-Time 
Protocol (CRTP)  276
control modes, Baxter's arm
Joint Position Control  193
Joint Torque Control  193
Joint Velocity Control  193
Raw Joint Position Control  194
reference  194
controls, adding to Xacro robot
about  173, 174
control launch file, creating  176, 177
Gazebo ROS control plugin, adding  175
transmission elements, 
defining for joints  174, 175
YAML configuration file, creating  175, 176
coordinate frame, Baxter  192, 193
counterclockwise (CCW)  256
Crazyflie
about  275
controlling  365
Crazyflie 2.0
about  275
communicating, with Crazyradio PA  278
controlling, without ROS  277
Crazyflie ROS software, loading  279, 280
flying, with motion capture system  288
flying, with teleop  283-285
multiple Crazyflies, flying  288
pre-flight checks  282, 283
references  276
system design  276
udev rules, setting up for Crazyradio  281
crazyflie/baselink frame  364
Crazyflie control states
about  365
flight  365
hover  365
idle  365
land  365
observer node, using  370, 371
PID control, for hover 
and flight states  368, 369
ROS services, for controlling takeoff 
and land  366
takeoff  365
takeoff and land, activating  367
takeoff and land, working  367
Crazyflie, flying
about  371
flying, to stationary target  374
hovering, in one place  371
hover, working  372, 373
learned lessons  376, 377
messages, logging with rosout 
and rospy  377
target detection, working  375, 376
crazyflie metapackage
reference  338
Crazyflie PC client
references  277
Crazyflie Python client (PC) software  277
Crazyradio firmware
reference  278

[ 394 ]
CUDA  343
custom control, ROS robot with 
Android device
about  313
Android Studio and Tools, 
installing  313, 314
ROS-Android development  316, 317
ROS-Android development environment, 
installing  314
terms, defining  315
custom ROS Android device interface
creating  307
Turtlebot Remocon, playing with  307-312
custom ROS game controller interface
creating  296
D
Debian package
reference  318
Degrees-Of-Freedom (DOF)  184
demo software
reference  244
drones  263
Dynamic Animation and Robotics 
Toolkit (DART)  57
Dynamic Host Configuration 
Protocol (DHCP)  85
E
Echo pin  325
Eigenfaces  386
Environment toolbar, Gazebo
about  60
rotation mode  61
scale mode  61
selection mode  60
translation mode  61
experience errors
references  345
Extensible Markup Language (XML)  11
F
face datasets
reference  387
face detection, with cascade classifier
about  385
ROS package face_detector, using  386
face recognition capabilities, OpenCV
reference  387
FaceRecognition methods
reference  388
face recognition, with OpenCV
about  387
reference  387
ROS package cob_people_detection, 
using  387
ROS package face_recognition, using  388
FaceRecognizer class
reference  387
Federal Aviation Administration (FAA)
about  263
reference  263
field of view (FOV)  337
Field Service Menu (FSM)  239
Fisherfaces  386
fixed-pitch propellers  253
forward kinematic analysis, Baxter's arms
about  222
Baxter's arms, moving to zero 
positions in angles  227-229
joints  222-227
joint state publisher  222-227
rviz tf frames  229, 230
tf  227
tf tree, of robot elements  231
G
game controller
alternative test  299
testing  297-299
testing, ROS joy package used  300
Gazebo
about  57, 58
Environment toolbar  60
Gazebo model, verifying  64
Insert panel  62
installing  58, 59
Joints panel  62
launching  59

[ 395 ]
main window menu bar  62
model, moving around  67, 68
model, tweaking  67
reference  58
reference, for keyboard shortcuts  60
reference, for tutorials  59
robot URDF, modifying  63
roslaunch, using with  59
ROS simulation environments  69
Simulation panel  63
URDF, viewing  65, 66
using  60
World panel  61
Gazebo Troubleshooting page
reference  203
global costmap  139
global navigation  139
Global Positioning 
System (GPS) sensors  251
gmapping package
reference  137
GNU Image Manipulator 
Program (GIMP) software   355
GPS system
reference  258
Gradle  316
Graphical User Interface (GUI)  34
graphics processing unit (GPU)  337
grippers, Baxter  194
gyroscope  259
H
hardware specifications, TurtleBot
about  91
TurtleBot dashboard  92, 93
Hector Quadrotor
about  264, 265
flying indoors  272-275
flying outdoors  267-272
launching, in Gazebo  267
loading  266, 267
hector_quadrotor metapackage
hector_gazebo_plugins  265
hector_quadrotor_demo  265
hector_quadrotor_description  265
hector_quadrotor_gazebo  265
hector_quadrotor_gazebo_plugins  265
hector_quadrotor_teleop  265
hector_quadrotor software
reference  266
hue-saturation and value (HSV)  355
I
iai_kinect2
installing  345-347
reference  345, 346
iai_kinect2 metapackage
kinect2_bridge  348, 349
kinect2_calibration  348
kinect2_calibration tool  349
kinect2_registration  348
kinect2_viewer  348
using  348
image registration  342
Image Viewer 126
inertial measurement unit (IMU)  260
infrared (IR) sensor  347
input/output (I/O) pins  318
integrated development 
environment (IDE)  314
International Civil Aviation 
Organization (ICAO)
about  263
URL  263
Internet Protocol address (IP address)  85
Inverse Kinematic (IK)
about  245
demonstrating  245-248
references  245
J
Java Archive (JAR)  316
joint_position_waypoints.py command
reference  242
joint_velocity_wobbler operation
reference  212
jstest-gtk package
about  297
reference  297-299

[ 396 ]
K
keyboard teleoperation
used, for moving TurtleBot  94, 95
kinect2_bridge software
reference  349
kinect2_calibration tool
about  349, 350
Kinect, calibrating  350-354
kinect2_ir_optical_frame  364
Kinect Software Develoment Kit (SDK)  117
Kinect v2  337
L
levels, logging messages
DEBUG  377
ERROR  377
FATAL  377
INFO  377
WARN  377
libfreenect2
about  342
installing  342-345
reference  342
line-of-sight (LOS)  278
lmtool
reference  381
Local Binary Patterns (LBPs)  385
local costmap  139
localization  133
local navigation  139
M
magnetometer  260
manifest  11
map  133
Maven  316
mesh design  151
metapackages  11
micro air vehicle (MAV)  275
Microcontroller Unit (MCU)  276
Microsoft Kinect  117
mission components
about  337
Crazyflie operation  338
Kinect for Windows v2  337
mission software structure  338, 339
Open CV  340
ROS  341
mission, setting up
about  354
Crazyflie, detecting  355
Crazyflie, tracking  362-364
Kinect, using  360-362
markers, detecting and viewing with 
OpenCV  358, 359
markers, identifying in color 
image  355-357
OpenCV, using  360-362
problems, with target detection  357
target, detecting  355
move_base node  145
MoveIt
about  183, 200, 231-233
Baxter's arms, moving with  233, 234
installing  200
objects, adding to scene  234-236
obstacles, avoiding with  236-238
position of objects  236
reference  200
N
National Institute for Information 
and Communication 
Technology (NICT)  382
navigation  115
navigation, with TurtleBot
autonomous navigation  138, 139
room, mapping with TurtleBot  133
netbook and remote computer, networking
about  84
netbook network setup  88
network addresses  85, 87
network connection, troubleshooting  89
network setup  89
network types  85
remote computer network setup  87
Secure Shell (SSH) connection  88
TurtleBot system, testing  90

[ 397 ]
O
Object-Oriented Graphics Rendering 
Engine (OGRE)  57
object tracking
reference  355
Occupancy Grid Map (OGM)  133
odometry, TurtleBot
about  106, 133
display, in rviz  110
for simulated TurtleBot  107-109
OpenCV
URL  340
OpenCV GitHub data repository 
reference  388
Open Graphics Library (OpenGL)  342
Open Natural Interaction (OpenNI2)  343
OpenSource Computer 
Vision (OpenCV)  117
Open Source Robotics 
Foundation (OSRF)  3
Oracle JDK
reference  313
P
Parameter Server  17
Parameter Server, turtlesim
about  26
parameters, changing for turtle 
background color  28
rosparam get  28
rosparam help  27
rosparam list, for /turtlesim node  27
rosparam set  28
PID, for quadrotor
reference  257
pitch joints, Baxter  190
Playstation (PS) 3 USB controller  278
Point Cloud Library (PCL)  124
PrimeSense Carmine  121, 122
Principal Component Analysis (PCA)  388
Printed Circuit Board (PCB)  283
proportional-integral-derivative (PID)  175
Protonect  343-345
PS3 joystick controller
reference  214
Q
quadrotor
about  252, 335
characteristics  253
communication methods  259
components  258
counter-rotating propellers  255-258
flying  255-258
pitch  254
precautions  262
pre-flight checklist  262
preparing, to fly  260
reference  253
roll  254
rules and regulations  263
sensors, adding  258, 259
testing  261
tutorials  261
yaw  254
quadrotor communications
about  259
bluetooth  259
GPS  259
Radio Frequency (RF)  259
Wi-Fi  259
quadrotor sensors
about  259
accelerometer  259
altimeter  260
barometer  260
condition sensors  260
gyroscope  259
inertial measurement unit (IMU)  260
magnetometer  260
quarter high definition (qhd)  347
quaternion  79
R
Raspberry Pi
reference  333
ROS, installing  333
Raspbian  333
real Baxter
additional examples  244
controlling  241

[ 398 ]
grasping  244
joint position waypoints, commanding  241
joint torque springs, commanding  242, 243
joint velocity, demonstrating  243
setup, configuring  238-240
visual servoing control  244
rectified image  347
repository, face_recognition source code 
reference  388
Rethink wiki
URL  238
RoboCup@Home contest   381
robot
enabling, to speak  383
robot, controlling with voice
about  379, 380
rospeex metapackage  382, 383
Sphinx, used  380-382
robot, enabling for recognition
about  384
face detection, with cascade 
classifier  384, 385
OpenCV, used  387
Robot Operating System. See  ROS
robot URDF, modifications
<collision> elements, in Gazebo  64
<visual> elements, in Gazebo  64
color, specifying in Gazebo  64
Gazebo tag, adding  63
Rocon Remocon  307
rocon (robots in concert) software  74
roll joints, Baxter  190, 191
room, mapping with TurtleBot
about  133
map, building  134-137
mapping task, accomplishing  137, 138
terms, defining  133
ROS
about  1
controlling  3
installing, on Raspberry Pi  333
launching  5
used, by robots  2-4
using, with UAVs  264
ROS Android core libraries
reference  315
ROS Android tutorials
reference  317
ROS commands
about  31
reference  31
roscore  31
rosmsg  31
rosnode  31
rosparam  31
rosrun  31
rosservice  31
rostopic  31
using, for moving TurtleBot  95, 96
ros_control packages
reference  174
roscore
about  15
reference, for ROS tutorial  15
ROS filesystem
reference  13
ROS forum
reference  5
ROS Indigo  3
ROS-Indigo installation instructions
reference  59
ROS Indigo source code
reference  5
ROS Indigo with Debian packages
reference  5
ROS installation
about  5
environment setup  8
keys, setting up  6
rosdep, initializing  7
ROS environment, examining  8, 9
ROS Indigo, installing  6
rosinstall, obtaining  8
sources.list, setting up  6
Ubuntu repositories, configuring  5
rosjava
reference  315
roslaunch
reference  152
ROS manifest  11
ROS Master
about  15
invoking, roscore used  15, 16

[ 399 ]
Parameter Server  17
reference  15
ROS commands, for determining 
nodes and topic  17, 18
ROS navigation
exploring  148
references  148
ROS nodes
about  13, 14
Arduino, using  318
creating  317
creating, on Arduino  317
publishing  14
Raspberry Pi, using  333
subscribing  14
ROS nodes, for visualization
running  126
visual data, using Image Viewer  126-128
visual data, using rviz  128-131
ROS, on Raspberry Pi
references, for examples  334
ROS organization
URL  3
ROS packages
about  10
building  39, 40
control_toolbox  198
driver_common  198
exploring  11, 12
Gazebo2  198
gazebo_ros_control  198
gazebo_ros_pkgs  198
qt_build  198
realtime_tools  198
reference  10
ros_control  198
ros_controllers  198
rospack find packages  12
rospack list  13
tf_conversions  198
xacro  198
ROS packages, from crazyflie metapackage
crazyflie_controller  280
crazyflie_cpp  280
crazyflie_demo  280
crazyflie_description  280
crazyflie_driver  281
crazyflie_tools  280
rospack find packages  12
rospack list  13
rospeex
references  383
rospeex metapackage
about  382
rospeex_audiomonitor  382
rospeex_core  382
rospeex_if  382
rospeex_launch  382
rospeex_msgs  382
rospeex_samples  382
rospeex_webaudiomonitor  382
ROS robot simulation  18
ROS sensor messages
reference  260
rosserial
about  317
reference  317
rosserial_arduino package
reference  323
rosserial_xbee tools
reference  332
ROS services
for moving turtle  29, 30
rosservice call  30
ROS software repository
reference  6
ROS visualization. See  rviz
ROS wiki
URL  18
rotation fields  362
rotocraft  253
rqt
using  182
rqt_graph  101-104
rqt_reconfigure tool  146-148
rqt tools
about  100
rqt message publisher  104-106
topic monitor  104-106
rviz
about  34
Displays panel  36, 37
installing  34, 35
launching  35

[ 400 ]
main window menu bar  38
mouse controls  38
reference  34-36
Time panel  37
toolbar  38
using  36
Views panel  37
rviz tf frames  229
S
sample vector file
reference  385
SDK example programs
references  206
Secure Digital (SD) card  333
sensors  258
Series Elastic Actuators (SEA)  184
Simultaneous Localization 
and Mapping (SLAM)  116
software, for mission
iai_kinect2, installing  345
iai_kinect2 metapackage, using  348
libfreenect2, installing  342, 343
loading  341
SONIC BURST  325
sound_play package
references  383
Sphinx  380
springiness  184
Standard Units of Measure and 
Coordinate Conventions
reference  193
stock firmware  275
synthesis projects
reference  383
T
telemetry  258
teleop_xbox360.launch
about  285
nodes and topics  286, 287
tasks  285
text-to-speech (TTS) ability  383
tf
about  227
reference  227
time of flight depth sensor  337
toolbar, rviz
2D Nav Goal  38
2D Pose Estimate  38
focus camera  38
interact  38
move camera  38
select  38
topic  14
training cascade software
reference  385
translation fields  362
Transmission Control Protocol/Internet 
Protocol (TCP/IP)  15
TRIGER PULSE  325
Trig pin  325
troubleshooting, Turtlebot Remocon
additional methods  313
Connection Failure  312
failed to start paired rapp  312
tuck and untuck operations
reference  211
TurboJPEG  342
TurtleBot
3D vision systems  116
about  72, 73
automatic docking  111, 112
controlling, with Python script  97, 98
dashboard  92
hardware specifications  91
moving  93
moving, keyboard teleoperation 
used  94, 95
moving, ROS commands used  95, 96
navigating with  132
odometry  106
reference, for tutorials  84
setting up  82
testing  82
URL, for list of manufacturers  72

[ 401 ]
TurtleBot Android Apps Dev Tutorials
reference  313
TurtleBot Remocon
playing with  307-312
troubleshooting  312
TurtleBot simulator software
issues  77
keyboard teleoperation  80, 81
launching, in Gazebo  75, 76
loading  74
ROS commands, and Gazebo  78-80
troubleshooting  77
TurtleBot standalone test  83
Turtlesim
controlling, with custom game controller 
interface  300-306
reference, for tutorial  306
turtlesim nodes
about  21-23
rosrun command  19, 20
turtlesim simulator
about  1, 18, 19
Parameter Server  26
ROS commands  31
ROS services  29
topics and messages  23
turtlesim nodes, starting  19
turtlesim topics and messages
about  23
rosmsg list  24
rosmsg show  25
rostopic echo  25, 26
rostopic list  24
rostopic type  24
two-wheeled differential drive robot
caster, adding  48, 49
collisions, adding  51, 52
color, adding  49-51
physical properties, adding  54, 55
robot chassis, creating  41
robot_state_publisher nodes  54
roslaunch, using  42-45
tf package  54
URDF, building  40
URDF tools  56
wheels, adding  45-48
wheels, moving  52, 53
U
Ubuntu
reference  318
Ubuntu commands
reference  300
udev rules
setting up, for Crazyradio  281
ultrasonic sensor control, ROS 
and Arduino used
about  324
Arduino, connecting to 
HC-SR04 ultrasonic sensor  326
Arduino, programming 
to sense distance  326, 327
time  324
ultrasonic sensor program, 
executing  328-332
Unified Robot Description 
Format (URDF)  33
Uniform Resource Identifier (URI)  16, 283
Unmanned Aerial Vehicles (UAVs)  252, 264
URDF tools
check_urdf  56
trying  56
urdf_to_graphiz tool  56, 57
USB dongle   278
V
VICON  288
Video Acceleration API (VAAP)  343
VideoCore graphics 
processing unit (GPU)  333
Virtual Machine (VM)  277
Virtual-Reality Peripheral 
Network (VRPN)  288
vision_opencv interface
reference  340

[ 402 ]
visual servoing control
about  244
reference  244
X
Xacro
about  152
expanding  153
features  152
macros  152
multiple Xacro files, combining  152
property and property blocks  152
reference  153
rospack commands  152
Simple Math  152
used, for building articulated robot arm 
URDF  154
Xbox joystick controls
mapping  216
Z
Zero Force Gravity (Zero-G) mode  186



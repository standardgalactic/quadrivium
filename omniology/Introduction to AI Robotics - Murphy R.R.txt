R O B I N R. M U R P H Y
I N T R O D U C T I O N
T O  
A I R O B O T I C S

Introduction
to
AI
Robotics

Intelligent Robots and Autonomous Agents
Ronald C. Arkin, editor
Behavior-Based Robotics, Ronald C. Arkin, 1998
Robot Shaping: An Experiment in Behavior Engineering, Marco Dorigo and Marco
Colombetti, 1998
Layered Learning in Multiagent Systems: A Winning Approach to Robotic Soccer,
Peter Stone, 2000
Evolutionary Robotics: The Biology, Intelligence, and Technology of Self-Organizing
Machines, Stefano Nolﬁand Dario Floreano, 2000
Reasoning about Rational Agents, Michael Wooldridge, 2000
Introduction to AI Robotics, Robin R. Murphy, 2000

Introduction
to
AI
Robotics
Robin R. Murphy
A Bradford Book
The MIT Press
Cambridge, Massachusetts
London, England

© 2000 Massachusetts Institute of Technology
All rights reserved. No part of this book may be reproduced in any form by any
electronic or mechanical means (including photocopying, recording, or information
storage and retrieval) without permission in writing from the publisher.
Typeset in 10/13 Lucida Bright by the author using LATEX2
".
Printed and bound in the United States of America.
Library of Congress Cataloging-in-Publication Data
Murphy, Robin, 1957–
Introduction to AI robotics / Robin R. Murphy.
p.
cm.—(Intelligent robotics and autonomous agents. A Bradford Book.)
Includes bibliographical references and index.
ISBN 0-262-13383-0 (hc. : alk. paper)
1. Robotics. 2. Artiﬁcial intelligence. I. Title. II. Series
TJ211.M865
2000
629.8
06263—dc21
00-033251

v
To Kevin
...and Carlyle Ramsey, Monroe Swilley, Chris Trowell


Brief Contents
I
Robotic Paradigms
1
1
From Teleoperation To Autonomy
13
2
The Hierarchical Paradigm
41
3
Biological Foundations of the Reactive Paradigm
67
4
The Reactive Paradigm
105
5
Designing a Reactive Implementation
155
6
Common Sensing Techniques for Reactive Robots
195
7
The Hybrid Deliberative/Reactive Paradigm
257
8
Multi-agents
293
II
Navigation
315
9
Topological Path Planning
325
10
Metric Path Planning
351
11
Localization and Map Making
375
12
On the Horizon
435


Contents
Preface
xvii
I
Robotic Paradigms
1
1
From Teleoperation To Autonomy
13
1.1
Overview
13
1.2
How Can a Machine Be Intelligent?
15
1.3
What Can Robots Be Used For?
16
1.3.1
Social implications of robotics
18
1.4
A Brief History of Robotics
19
1.4.1
Industrial manipulators
21
1.4.2
Space robotics and the AI approach
26
1.5
Teleoperation
28
1.5.1
Telepresence
32
1.5.2
Semi-autonomous control
33
1.6
The Seven Areas of AI
34
1.7
Summary
37
1.8
Exercises
37
1.9
End Notes
39
2
The Hierarchical Paradigm
41
2.1
Overview
41
2.2
Attributes of the Hierarchical Paradigm
42
2.2.1
Strips
44
2.2.2
More realistic Strips example
46
2.2.3
Strips summary
52

x
Contents
2.3
Closed World Assumption and the Frame Problem
53
2.4
Representative Architectures
54
2.4.1
Nested Hierarchical Controller
54
2.4.2
NIST RCS
57
2.4.3
Evaluation of hierarchical architectures
59
2.5
Advantages and Disadvantages
61
2.6
Programming Considerations
62
2.7
Summary
62
2.8
Exercises
63
2.9
End Notes
64
3
Biological Foundations of the Reactive Paradigm
67
3.1
Overview
67
3.1.1
Why explore the biological sciences?
69
3.1.2
Agency and computational theory
70
3.2
What Are Animal Behaviors?
73
3.2.1
Reﬂexive behaviors
74
3.3
Coordination and Control of Behaviors
75
3.3.1
Innate releasing mechanisms
77
3.3.2
Concurrent behaviors
82
3.4
Perception in Behaviors
83
3.4.1
Action-perception cycle
83
3.4.2
Two functions of perception
85
3.4.3
Gibson: Ecological approach
85
3.4.4
Neisser: Two perceptual systems
90
3.5
Schema Theory
91
3.5.1
Behaviors and schema theory
92
3.6
Principles and Issues in Transferring Insights to Robots
97
3.7
Summary
99
3.8
Exercises
100
3.9
End Notes
102
4
The Reactive Paradigm
105
4.1
Overview
105
4.2
Attributes of Reactive Paradigm
108
4.2.1
Characteristics and connotations of reactive
behaviors
110
4.2.2
Advantages of programming by behavior
112
4.2.3
Representative architectures
113

Contents
xi
4.3
Subsumption Architecture
113
4.3.1
Example
115
4.3.2
Subsumption summary
121
4.4
Potential Fields Methodologies
122
4.4.1
Visualizing potential ﬁelds
123
4.4.2
Magnitude proﬁles
126
4.4.3
Potential ﬁelds and perception
128
4.4.4
Programming a single potential ﬁeld
129
4.4.5
Combination of ﬁelds and behaviors
130
4.4.6
Example using one behavior per sensor
134
4.4.7
Pﬁelds compared with subsumption
136
4.4.8
Advantages and disadvantages
145
4.5
Evaluation of Reactive Architectures
147
4.6
Summary
148
4.7
Exercises
149
4.8
End Notes
152
5
Designing a Reactive Implementation
155
5.1
Overview
155
5.2
Behaviors as Objects in OOP
157
5.2.1
Example: A primitive move-to-goal behavior
158
5.2.2
Example: An abstract follow-corridor behavior
160
5.2.3
Where do releasers go in OOP?
162
5.3
Steps in Designing a Reactive Behavioral System
163
5.4
Case Study: Unmanned Ground Robotics Competition
165
5.5
Assemblages of Behaviors
173
5.5.1
Finite state automata
174
5.5.2
A Pick Up the Trash FSA
178
5.5.3
Implementation examples
182
5.5.4
Abstract behaviors
184
5.5.5
Scripts
184
5.6
Summary
187
5.7
Exercises
188
5.8
End Notes
191
6
Common Sensing Techniques for Reactive Robots
195
6.1
Overview
196
6.1.1
Logical sensors
197
6.2
Behavioral Sensor Fusion
198

xii
Contents
6.3
Designing a Sensor Suite
202
6.3.1
Attributes of a sensor
203
6.3.2
Attributes of a sensor suite
206
6.4
Proprioceptive Sensors
207
6.4.1
Inertial navigation system (INS)
208
6.4.2
GPS
208
6.5
Proximity Sensors
210
6.5.1
Sonar or ultrasonics
210
6.5.2
Infrared (IR)
216
6.5.3
Bump and feeler sensors
217
6.6
Computer Vision
218
6.6.1
CCD cameras
219
6.6.2
Grayscale and color representation
220
6.6.3
Region segmentation
226
6.6.4
Color histogramming
228
6.7
Range from Vision
231
6.7.1
Stereo camera pairs
232
6.7.2
Light stripers
235
6.7.3
Laser ranging
239
6.7.4
Texture
241
6.8
Case Study: Hors d’Oeuvres, Anyone?
242
6.9
Summary
250
6.10
Exercises
251
6.11
End Notes
254
7
The Hybrid Deliberative/Reactive Paradigm
257
7.1
Overview
257
7.2
Attributes of the Hybrid Paradigm
259
7.2.1
Characteristics and connotation of reactive
behaviors in hybrids
261
7.2.2
Connotations of “global”
262
7.3
Architectural Aspects
262
7.3.1
Common components of hybrid architectures
263
7.3.2
Styles of hybrid architectures
264
7.4
Managerial Architectures
265
7.4.1
Autonomous Robot Architecture (AuRA)
265
7.4.2
Sensor Fusion Effects (SFX)
268
7.5
State-Hierarchy Architectures
274
7.5.1
3-Tiered (3T)
274

Contents
xiii
7.6
Model-Oriented Architectures
277
7.6.1
Saphira
278
7.6.2
Task Control Architecture (TCA)
280
7.7
Other Robots in the Hybrid Paradigm
283
7.8
Evaluation of Hybrid Architectures
284
7.9
Interleaving Deliberation and Reactive Control
285
7.10
Summary
288
7.11
Exercises
289
7.12
End Notes
291
8
Multi-agents
293
8.1
Overview
293
8.2
Heterogeneity
296
8.2.1
Homogeneous teams and swarms
296
8.2.2
Heterogeneous teams
297
8.2.3
Social entropy
300
8.3
Control
301
8.4
Cooperation
303
8.5
Goals
304
8.6
Emergent Social Behavior
305
8.6.1
Societal rules
305
8.6.2
Motivation
307
8.7
Summary
309
8.8
Exercises
310
8.9
End Notes
312
II
Navigation
315
9
Topological Path Planning
325
9.1
Overview
325
9.2
Landmarks and Gateways
326
9.3
Relational Methods
328
9.3.1
Distinctive places
329
9.3.2
Advantages and disadvantages
331
9.4
Associative Methods
333
9.4.1
Visual homing
334
9.4.2
QualNav
335

xiv
Contents
9.5
Case Study of Topological Navigation with a
Hybrid Architecture
338
9.5.1
Path planning
339
9.5.2
Navigation scripts
343
9.5.3
Lessons learned
346
9.6
Summary
348
9.7
Exercises
348
9.8
End notes
350
10 Metric Path Planning
351
10.1
Objectives and Overview
351
10.2
Conﬁguration Space
353
10.3
Cspace Representations
354
10.3.1
Meadow maps
354
10.3.2
Generalized Voronoi graphs
357
10.3.3
Regular grids
358
10.3.4
Quadtrees
359
10.4
Graph Based Planners
359
10.5
Wavefront Based Planners
365
10.6
Interleaving Path Planning and Reactive Execution
367
10.7
Summary
371
10.8
Exercises
372
10.9
End Notes
374
11 Localization and Map Making
375
11.1
Overview
375
11.2
Sonar Sensor Model
378
11.3
Bayesian
380
11.3.1
Conditional probabilities
381
11.3.2
Conditional probabilities for
P
(H
js)
384
11.3.3
Updating with Bayes’ rule
385
11.4
Dempster-Shafer Theory
386
11.4.1
Shafer belief functions
387
11.4.2
Belief function for sonar
389
11.4.3
Dempster’s rule of combination
390
11.4.4
Weight of conﬂict metric
394
11.5
HIMM
395
11.5.1
HIMM sonar model and updating rule
395
11.5.2
Growth rate operator
398

Contents
xv
11.6
Comparison of Methods
403
11.6.1
Example computations
403
11.6.2
Performance
411
11.6.3
Errors due to observations from stationary robot
412
11.6.4
Tuning
413
11.7
Localization
415
11.7.1
Continuous localization and mapping
416
11.7.2
Feature-based localization
421
11.8
Exploration
424
11.8.1
Frontier-based exploration
425
11.8.2
Generalized Voronoi graph methods
427
11.9
Summary
428
11.10 Exercises
431
11.11 End Notes
434
12 On the Horizon
435
12.1
Overview
435
12.2
Shape-Shifting and Legged Platforms
438
12.3
Applications and Expectations
442
12.4
Summary
445
12.5
Exercises
445
12.6
End Notes
447
Bibliography
449
Index
459


Preface
This book is intended to serve as a textbook for advanced juniors and seniors
and ﬁrst-year graduate students in computer science and engineering. The
reader is not expected to have taken a course in artiﬁcial intelligence (AI),
although the book includes pointers to additional readings and advanced
exercises for more advanced students. The reader should have had at least
one course in object-oriented programming in order to follow the discussions
on how to implement and program robots using the structures described in
this book. These programming structures lend themselves well to laboratory
exercises on commercially available robots, such as the Khepera, Nomad 200
series, and Pioneers. Lego Mindstorms and Rug Warrior robots can be used
for the ﬁrst six chapters, but their current programming interface and sensor
limitations interfere with using those robots for the more advanced material.
A background in digital circuitry is not required, although many instructors
may want to introduce laboratory exercises for building reactive robots from
kits such as the Rug Warrior or the Handy Board.
Introduction to AI Robotics attempts to cover all the topics needed to pro-
gram an artiﬁcially intelligent robot for applications involving sensing, nav-
igation, path planning, and navigating with uncertainty. Although machine
perception is a separate ﬁeld of endeavor, the book covers enough computer
vision and sensing to enable students to embark on a serious robot project
or competition. The book is divided into two parts. Part I deﬁnes what are
intelligent robots and introduces why artiﬁcial intelligence is needed. It cov-
ers the “theory” of AI robotics, taking the reader through a historical journey
from the Hierarchical to the Hybrid Deliberative/Reactive Paradigm for or-
ganizing intelligence. The bulk of the seven chapters is concerned with the
Reactive Paradigm and behaviors. A chapter on sensing and programming
techniques for reactive behaviors is included in order to permit a class to get

xviii
Preface
a head start on a programming project. Also, Part I covers the coordination
and control of teams of multi-agents. Since the fundamental mission of a
mobile robot involves moving about in the world, Part II devotes three chap-
ters to qualitative and metric navigation and path planning techniques, plus
work in uncertainty management. The book concludes with an overview of
how advances in computer vision are now being integrated into robots, and
how successes in robots are driving the web-bot and know-bot craze.
Since Introduction to AI Robotics is an introductory text, it is impossible to
cover all the ﬁne work that has been in the ﬁeld. The guiding principle has
been to include only material that clearly illuminates a speciﬁc topic. Refer-
ences to other approaches and systems are usually included as an advanced
reading question at the end of the chapter or as an end note. Behavior-based
Robotics10 provides a thorough survey of the ﬁeld and should be an instruc-
tor’s companion.
Acknowledgments
It would be impossible to thank all of the people involved in making this
book possible, but I would like to try to list the ones who made the most
obvious contributions. I’d like to thank my parents (I think this is the equiv-
alent of scoring a goal and saying “Hi Mom!” on national TV) and my family
(Kevin, Kate, and Allan). I had the honor of being in the ﬁrst AI robotics
course taught by my PhD advisor Ron Arkin at Georgia Tech (where I was
also his ﬁrst PhD student), and much of the material and organization of this
book can be traced back to his course. I have tried to maintain the intellec-
tual rigor of his course and excellent book while trying to distill the material
for a novice audience. Any errors in this book are strictly mine. David Kor-
tenkamp suggested that I write this book after using my course notes for a
class he taught, which served as a very real catalyst. Certainly the students
at both the Colorado School of Mines (CSM), where I ﬁrst developed my
robotics courses, and at the University of South Florida (USF) merit special
thanks for being guinea pigs. I would like to speciﬁcally thank Leslie Baski,
John Blitch, Glenn Blauvelt, Ann Brigante, Greg Chavez, Aaron Gage, Dale
Hawkins, Floyd Henning, Jim Hoffman, Dave Hershberger, Kevin Gifford,
Matt Long, Charlie Ozinga, Tonya Reed Frazier, Michael Rosenblatt, Jake
Sprouse, Brent Taylor, and Paul Wiebe from my CSM days and Jenn Casper,
Aaron Gage, Jeff Hyams, Liam Irish, Mark Micire, Brian Minten, and Mark
Powell from USF.

Preface
xix
Special thanks go to the numerous reviewers, especially Karen Sutherland
and Ken Hughes. Karen Sutherland and her robotics class at the University
of Wisconsin-LaCrosse (Kristoff Hans Ausderau, Teddy Bauer, Scott David
Becker, Corrie L. Brague, Shane Brownell, Edwin J. Colby III, Mark Erick-
son, Chris Falch, Jim Fick, Jennifer Fleischman, Scott Galbari, Mike Halda,
Brian Kehoe, Jay D. Paska, Stephen Pauls, Scott Sandau, Amy Stanislowski,
Jaromy Ward, Steve Westcott, Peter White, Louis Woyak, and Julie A. Zan-
der) painstakingly reviewed an early draft of the book and made extensive
suggestions and added review questions. Ken Hughes also deserves special
thanks; he also provided a chapter by chapter critique as well as witty emails.
Ken always comes to my rescue.
Likewise, the book would not be possible without my ongoing involve-
ment in robotics research; my efforts have been supported by NSF, DARPA,
and ONR. Most of the case studies came from work or through equipment
sponsored by NSF. Howard Moraff, Rita Rodriguez, and Harry Hedges were
always very encouraging, beyond the call of duty of even the most dedi-
cated NSF program director. Michael Mason also provided encouragement,
in many forms, to hang in there and focus on education.
My editor, Bob Prior, and the others at the MIT Press (Katherine Innis,
Judy Feldmann, Margie Hardwick, and Maureen Kuper) also have my deep-
est appreciation for providing unfailingly good-humored guidance, techni-
cal assistance, and general savvy. Katherine and especially Judy were very
patient and nice— despite knowing that I was calling with Yet Another Cri-
sis. Mike Hamilton at AAAI was very helpful in making available the vari-
ous “action shots” used throughout the book. Chris Manning provided the
LATEX2
"style ﬁles, with adaptations by Paul Anagnostopoulos. Liam Irish
and Ken Hughes contributed helpful scripts.
Besides the usual suspects, there are some very special people who indi-
rectly helped me. Without the encouragement of three liberal arts professors,
Carlyle Ramsey, Monroe Swilley, and Chris Trowell, at South Georgia Col-
lege in my small hometown of Douglas, Georgia, I probably wouldn’t have
seriously considered graduate school in engineering and computer science.
They taught me that learning isn’t a place like a big university but rather a
personal discipline. The efforts of my husband, Kevin Murphy, were, as al-
ways, essential. He worked hard to make sure I could spend the time on this
book without missing time with the kids or going crazy. He also did a se-
rious amount of editing, typing, scanning, and proofreading. I dedicate the
book to these four men who have inﬂuenced my professional career as much
as any academic mentor.


P A R T I
Robotic Paradigms

2
Part I
Contents:
 Overview
 Chapter 1: From Teleoperation to Autonomy
 Chapter 2: The Hierarchical Paradigm
 Chapter 3: Biological Foundations of the Reactive Paradigm
 Chapter 4: The Reactive Paradigm
 Chapter 5: Designing a Reactive Implementation
 Chapter 6: Common Sensing Technique for Reactive Robots
 Chapter 7: The Hybrid Deliberative/Reactive Paradigm
 Chapter 8: Multiple Mobile Robots
Overview
The eight chapters in this part are devoted to describing what is AI robotics
and the three major paradigms for achieving it. These paradigms character-
ize the ways in which intelligence is organized in robots. This part of the
book also covers architectures that provide exemplars of how to transfer the
principles of the paradigm into a coherent, reusable implementation on a
single robot or teams of robots.
What Are Robots?
One of the ﬁrst questions most people have about robotics is “what is a ro-
bot?” followed immediately by “what can they do?”
In popular culture, the term “robot” generally connotes some anthropo-
morphic (human-like) appearance; consider robot “arms” for welding. The
tendency to think about robots as having a human-like appearance may stem
from the origins of the term “robot.” The word “robot” came into the popu-
lar consciousness on January 25, 1921, in Prague with the ﬁrst performance
of Karel Capek’s play, R.U.R. (Rossum’s Universal Robots).37 In R.U.R., an
unseen inventor, Rossum, has created a race of workers made from a vat of
biological parts, smart enough to replace a human in any job (hence “univer-
sal”). Capek described the workers as robots, a term derived from the Czech

Part I
3
word “robota” which is loosely translated as menial laborer. Robot workers
implied that the artiﬁcial creatures were strictly meant to be servants to free
“real” people from any type of labor, but were too lowly to merit respect.
This attitude towards robots has disastrous consequences, and the moral of
the rather socialist story is that work deﬁnes a person.
The shift from robots as human-like servants constructed from biological
parts to human-like servants made up of mechanical parts was probably due
to science ﬁction. Three classic ﬁlms, Metropolis (1926), The Day the Earth
Stood Still (1951), and Forbidden Planet (1956), cemented the connotation that
robots were mechanical in origin, ignoring the biological origins in Capek’s
play. Meanwhile, computers were becoming commonplace in industry and
accounting, gaining a perception of being literal minded. Industrial automa-
tion conﬁrmed this suspicion as robot arms were installed which would go
through the motions of assembling parts, even if there were no parts. Even-
tually, the term robot took on nuances of factory automation: mindlessness
and good only for well-deﬁned repetitious types of work. The notion of
anthropomorphic, mechanical, and literal-minded robots complemented the
viewpoint taken in many of the short stories in Isaac Asimov’s perennial fa-
vorite collection, I, Robot.15 Many (but not all) of these stories involve either
a “robopsychologist,” Dr. Susan Calvin, or two erstwhile trouble shooters,
Powell and Donovan, diagnosing robots who behaved logically but did the
wrong thing.
The shift from human-like mechanical creatures to whatever shape gets
the job done is due to reality. While robots are mechanical, they don’t have to
be anthropomorphic or even animal-like. Consider robot vacuum cleaners;
they look like vacuum cleaners, not janitors. And the HelpMate Robotics,
Inc., robot which delivers hospital meals to patients to permit nurses more
time with patients, looks like a cart, not a nurse.
It should be clear from Fig. I.1 that appearance does not form a useful def-
inition of a robot. Therefore, the deﬁnition that will be used in this book
is an intelligent robot is a mechanical creature which can function autonomously.
INTELLIGENT ROBOT
“Intelligent” implies that the robot does not do things in a mindless, repeti-
tive way; it is the opposite of the connotation from factory automation. The
“mechanical creature” portion of the deﬁnition is an acknowledgment of the
fact that our scientiﬁc technology uses mechanical building blocks, not bi-
ological components (although with recent advances in cloning, this may
change). It also emphasizes that a robot is not the same as a computer. A ro-
bot may use a computer as a building block, equivalent to a nervous system
or brain, but the robot is able to interact with its world: move around, change

4
Part I
a.
b.
Figure I.1 Two views of robots: a) the humanoid robot from the 1926 movie
Metropolis (image courtesty Fr.
Doug Quinn and the Metropolis Home
Page), and b) a HMMWV military vehicle capable of driving on roads and
open terrains. (Photograph courtesy of the National Institute for Standards
and Technology.)
it, etc. A computer doesn’t move around under its own power. “Function
autonomously” indicates that the robot can operate, self-contained, under
all reasonable conditions without requiring recourse to a human operator.
Autonomy means that a robot can adapt to changes in its environment (the
lights get turned off) or itself (a part breaks) and continue to reach its goal.
Perhaps the best example of an intelligent mechanical creature which can
function autonomously is the Terminator from the 1984 movie of the same
name. Even after losing one camera (eye) and having all external cover-
ings (skin, ﬂesh) burned off, it continued to pursue its target (Sarah Connor).
Extreme adaptability and autonomy in an extremely scary robot! A more
practical (and real) example is Marvin, the mail cart robot, for the Baltimore
FBI ofﬁce, described in a Nov. 9, 1996, article in the Denver Post. Marvin is
able to accomplish its goal of stopping and delivering mail while adapting
to people getting in its way at unpredictable times and locations.

Part I
5
What are Robotic Paradigms?
A paradigm is a philosophy or set of assumptions and/or techniques which charac-
PARADIGM
terize an approach to a class of problems. It is both a way of looking at the world
and an implied set of tools for solving problems. No one paradigm is right;
rather, some problems seem better suited for different approaches. For ex-
ample, consider calculus problems. There are problems that could be solved
by differentiating in cartesian
(X
;
Y
;
Z
) coordinates, but are much easier to
solve if polar coordinates
(r
;

) are used. In the domain of calculus problems,
Cartesian and polar coordinates represent two different paradigms for view-
ing and manipulating a problem. Both produce the correct answer, but one
takes less work for certain problems.
Applying the right paradigm makes problem solving easier. Therefore,
knowing the paradigms of AI robotics is one key to being able to successfully
program a robot for a particular application. It is also interesting from a his-
torical perspective to work through the different paradigms, and to examine
the issues that spawned the shift from one paradigm to another.
There are currently three paradigms for organizing intelligence in robots:
ROBOTIC PARADIGMS
hierarchical, reactive, and hybrid deliberative/reactive. The paradigms are
described in two ways.
1. By the relationship between the three commonly accepted primitives
of robotics: SENSE, PLAN, ACT. The functions of a robot can be divided
ROBOT PARADIGM
PRIMITIVES
into three very general categories. If a function is taking in information
from the robot’s sensors and producing an output useful by other func-
tions, then that function falls in the SENSE category. If the function is
taking in information (either from sensors or its own knowledge about
how the world works) and producing one or more tasks for the robot to
perform (go down the hall, turn left, proceed 3 meters and stop), that func-
tion is in the PLAN category. Functions which produce output commands
to motor actuators fall into ACT (turn
	
, clockwise, with a turning veloc-
ity of 0.2mps). Fig. I.2 attempts to deﬁne these three primitives in terms
of inputs and outputs; this ﬁgure will appear throughout the chapters in
Part I.
2. By the way sensory data is processed and distributed through the sys-
tem. How much a person or robot or animal is inﬂuenced by what it
senses. So it is often difﬁcult to adequately describe a paradigm with just
a box labeled SENSE. In some paradigms, sensor information is restricted
to being used in a speciﬁc, or dedicated, way for each function of a robot;

6
Part I
ROBOT PRIMITIVES
INPUT
OUTPUT
SENSE
PLAN
ACT
Sensor data
Sensed information
Information (sensed
and/or cognitive)
Sensed information
or directives
Directives
Actuator commands
Figure I.2 Robot primitives deﬁned in terms of inputs and outputs.
in that case processing is local to each function. Other paradigms expect
SENSING
ORGANIZATION IN
ROBOT PARADIGMS
all sensor information to be ﬁrst processed into one global world model
and then subsets of the model distributed to other functions as needed.
Overview of the Three Paradigms
In order to set the stage for learning details, it may be helpful to begin with
a general overview of the robot paradigms. Fig. I.3 shows the differences
between the three paradigms in terms of the SENSE, PLAN, ACT primitives.
The Hierarchical Paradigm is the oldest paradigm, and was prevalent from
HIERARCHICAL
PARADIGM
1967–1990. Under it, the robot operates in a top-down fashion, heavy on
planning (see Fig. I.3). This was based on an introspective view of how peo-
ple think. “I see a door, I decide to head toward it, and I plot a course around
the chairs.” (Unfortunately, as many cognitive psychologists now know, in-
trospection is not always a good way of getting an accurate assessment of
a thought process. We now suspect no one actually plans how they get out
of a room; they have default schemas or behaviors.) Under the Hierarchical
Paradigm, the robot senses the world, plans the next action, and then acts
(SENSE, PLAN, ACT). Then it senses the world, plans, acts. At each step,
the robot explicitly plans the next move. The other distinguishing feature of
the Hierarchical paradigm is that all the sensing data tends to be gathered
into one global world model, a single representation that the planner can use
and can be routed to the actions. Constructing generic global world models

Part I
7
SENSE
PLAN
ACT
a.
PLAN
SENSE
ACT
b.
ACT
PLAN
SENSE
c.
Figure I.3 Three paradigms: a.) Hierarchical, b.) Reactive, and c.) Hybrid
deliberative/reactive.
turns out to be very hard and brittle due to the frame problem and the need
for a closed world assumption.
Fig. I.4 shows how the Hierarchical Paradigm can be thought of as a tran-
sitive, or Z-like, ﬂow of events through the primitives given in Fig. I.4. Un-
fortunately, the ﬂow of events ignored biological evidence that sensed infor-
mation can be directly coupled to an action, which is why the sensed infor-
mation input is blacked out.

8
Part I
ROBOT PRIMITIVES
INPUT
OUTPUT
SENSE
PLAN
ACT
Sensor data
Sensed information
Information (sensed
and/or cognitive)
Sensed information
or directives
Directives
Actuator commands
Figure I.4 Another view of the Hierarchical Paradigm.
The Reactive Paradigm was a reaction to the Hierarchical Paradigm, and
REACTIVE PARADIGM
led to exciting advances in robotics. It was heavily used in robotics starting
in 1988 and continuing through 1992. It is still used, but since 1992 there
has been a tendency toward hybrid architectures. The Reactive Paradigm
was made possible by two trends. One was a popular movement among AI
researchers to investigate biology and cognitive psychology in order to ex-
amine living exemplars of intelligence. Another was the rapidly decreasing
cost of computer hardware coupled with the increase in computing power.
As a result, researchers could emulate frog and insect behavior with robots
costing less than $500 versus the $100,000s Shakey, the ﬁrst mobile robot,
cost.
The Reactive Paradigm threw out planning all together (see Figs. I.3b and
I.5). It is a SENSE-ACT (S-A) type of organization. Whereas the Hierarchical
Paradigm assumes that the input to a ACT will always be the result of a
PLAN, the Reactive Paradigm assumes that the input to an ACT will always
be the direct output of a sensor, SENSE.
If the sensor is directly connected to the action, why isn’t a robot running
under the Reactive Paradigm limited to doing just one thing? The robot has
multiple instances of SENSE-ACT couplings, discussed in Ch. 4. These cou-
plings are concurrent processes, called behaviors, which take local sensing
data and compute the best action to take independently of what the other
processes are doing. One behavior can direct the robot to “move forward 5
meters” (ACT on drive motors) to reach a goal (SENSE the goal), while an-
other behavior can say “turn
	0
” (ACT on steer motors) to avoid a collision

Part I
9
ROBOT PRIMITIVES
INPUT
OUTPUT
SENSE
PLAN
ACT
Sensor data
Sensed information
Information (sensed
and/or cognitive)
Sensed information
or directives
Directives
Actuator commands
Figure I.5 The reactive paradigm.
with an object dead ahead (SENSE obstacles). The robot will do a combi-
nation of both behaviors, swerving off course temporarily at a

 angle to
avoid the collision. Note that neither behavior directed the robot to ACT with
a

 turn; the ﬁnal ACT emerged from the combination of the two behaviors.
While the Reactive Paradigm produced exciting results and clever robot
insect demonstrations, it quickly became clear that throwing away planning
was too extreme for general purpose robots.
In some regards, the Reac-
tive Paradigm reﬂected the work of Harvard psychologist B. F. Skinner in
stimulus-response training with animals. It explained how some animals
accomplished tasks, but was a dead end in explaining the entire range of
human intelligence.
But the Reactive Paradigm has many desirable properties, especially the
fast execution time that came from eliminating any planning. As a result,
the Reactive Paradigm serves as the basis for the Hybrid Deliberative/Reactive
HYBRID DELIBERA-
TIVE/REACTIVE
PARADIGM
Paradigm, shown in Fig.I.3c. The Hybrid Paradigm emerged in the 1990’s and
continues to be the current area of research. Under the Hybrid Paradigm, the
robot ﬁrst plans (deliberates) how to best decompose a task into subtasks
(also called “mission planning”) and then what are the suitable behaviors to
accomplish each subtask, etc. Then the behaviors start executing as per the
Reactive Paradigm. This type of organization is PLAN, SENSE-ACT (P, S-A),
where the comma indicates that planning is done at one step, then sensing
and acting are done together. Sensing organization in the Hybrid Paradigm
is also a mixture of Hierarchical and Reactive styles. Sensor data gets routed
to each behavior that needs that sensor, but is also available to the planner

10
Part I
ROBOT PRIMITIVES
INPUT
OUTPUT
PLAN
SENSE-ACT
(behaviors)
Sensor data
Information (sensed
and/or cognitive)
Directives
Actuator commands
Figure I.6 The hybrid deliberative/reactive paradigm.
for construction of a task-oriented global world model. The planner may
also “eavesdrop” on the sensing done by each behavior (i.e., the behavior
identiﬁes obstacles that could then be put into a map of the world by the
planner). Each function performs computations at its own rate; deliberative
planning, which is generally computationally expensive may update every
5 seconds, while the reactive behaviors often execute at 1/60 second. Many
robots run at 80 centimeters per second.
Architectures
Determining that a particular paradigm is well suited for an application is
certainly the ﬁrst step in constructing the AI component of a robot. But that
step is quickly followed with the need to use the tools associated with that
paradigm. In order to visualize how to apply these paradigms to real-world
applications, it is helpful to examine representative architectures. These ar-
chitectures provide templates for an implementation, as well as examples of
what each paradigm really means.
What is an architecture? Arkin offers several deﬁnitions in his book, Be-
havior-Based Robots.10 Two of the deﬁnitions he cites from other researchers
capture how the term will be used in this book. Following Mataric,89 an
architecture provides a principled way of organizing a control system. How-
ever, in addition to providing structure, it imposes constraints on the way the
control problem can be solved. Following Dean and Wellman,43 an architec-
ture describes a set of architectural components and how they interact. This
ARCHITECTURE
book is interested in the components common in robot architectures; these
are the basic building blocks for programming a robot. It also is interested in
the principles and rules of thumb for connecting these components together.

Part I
11
To see the importance of an architecture, consider building a house or a
car. There is no “right” design for a house, although most houses share the
same components (kitchens, bathrooms, walls, ﬂoors, doors, etc.). Likewise
with designing robots, there can be multiple ways of organizing the compo-
nents, even if all the designs follow the same paradigm. This is similar to cars
designed by different manufacturers. All internal combustion engine types
of cars have the same basic components, but the cars look different (BMWs
and Jaguars look quite different than Hondas and Fords). The internal com-
bustion (IC) engine car is a paradigm (as contrasted to the paradigm of an
electric car). Within the IC engine car community, the car manufacturers each
have their own architecture. The car manufacturers may make slight mod-
iﬁcations to the architecture for sedans, convertibles, sport-utility vehicles,
etc., to throw out unnecessary options, but each style of car is a particular
instance of the architecture. The point is: by studying representative robot
architectures and the instances where they were used for a robot application,
we can learn the different ways that the components and tools associated
with a paradigm can be used to build an artiﬁcially intelligent robot.
Since a major objective in robotics is to learn how to build them, an im-
portant skill to develop is evaluating whether or not a previously developed
architecture (or large chunks of it) will suit the current application. This skill
will save both time spent on re-inventing the wheel and avoid subtle prob-
lems that other people have encountered and solved. Evaluation requires a
set of criteria. The set that will be used in this book is adapted from Behavior-
Based Robotics:10
1. Support for modularity: does it show good software engineering princi-
MODULARITY
ples?
2. Niche targetability: how well does it work for the intended application?
NICHE TARGETABILITY
3. Ease of portability to other domains: how well would it work for other
PORTABILITY
applications or other robots?
4. Robustness: where is the system vulnerable, and how does it try to re-
ROBUSTNESS
duce that vulnerability?
Note that niche targetability and ease of portability are often at odds with
each other. Most of the architectures described in this book were intended to
be generic, therefore emphasizing portability. The generic structures, how-
ever, often introduce undesirable computational and storage overhead, so in
practice the designer must make trade-offs.

12
Part I
Layout of the Section
This section is divided into eight chapters, one to deﬁne robotics and the
other seven to intertwine both the theory and practice associated with each
paradigm. Ch. 2 describes the Hierarchical Paradigm and two representative
architectures. Ch. 3 sets the stage for understanding the Reactive Paradigm
by reviewing the key concepts from biology and ethology that served to mo-
tivate the shift from Hierarchical to Reactive systems. Ch. 4 describes the
Reactive Paradigm and the architectures that originally popularized this ap-
proach. It also offers deﬁnitions of primitive robot behaviors. Ch. 5 provides
guidelines and case studies on designing robot behaviors. It also introduces
issues in coordinating and controlling multiple behaviors and the common
techniques for resolving these issues. At this point, the reader should be
almost able to design and implement a reactive robot system, either in simu-
lation or on a real robot. However, the success of a reactive system depends
on the sensing. Ch. 6 discusses simple sonar and computer vision processing
techniques that are commonly used in inexpensive robots. Ch. 7 describes
the Hybrid Deliberative-Reactive Paradigm, concentrating on architectural
trends. Up until this point, the emphasis is towards programming a single
robot. Ch. 8 concludes the section by discussing how the principles of the
three paradigms have been transferred to teams of robots.
End Note
Robot paradigm primitives.
While the SENSE, PLAN, ACT primitives are generally accepted, some researchers
are suggesting that a fourth primitive be added, LEARN. There are no formal archi-
tectures at this time which include this, so a true paradigm shift has not yet occurred.

1
From Teleoperation To Autonomy
Chapter Objectives:
 Deﬁne intelligent robot.
 Be able to describe at least two differences between AI and Engineering
approaches to robotics.
 Be able to describe the difference between telepresence and semi-autonomous
control.
 Have some feel for the history and societal impact of robotics.
1.1
Overview
This book concentrates on the role of artiﬁcial intelligence for robots. At
ﬁrst, that may appear redundant; aren’t robots intelligent? The short an-
swer is “no,” most robots currently in industry are not intelligent by any
deﬁnition. This chapter attempts to distinguish an intelligent robot from a
non-intelligent robot.
The chapter begins with an overview of artiﬁcial intelligence and the social
implications of robotics. This is followed with a brief historical perspective
on the evolution of robots towards intelligence, as shown in Fig. 1.1. One
way of viewing robots is that early on in the 1960’s there was a fork in the
evolutionary path. Robots for manufacturing took a fork that has focused on
engineering robot arms for manufacturing applications. The key to success in
industry was precision and repeatability on the assembly line for mass pro-
duction, in effect, industrial engineers wanted to automate the workplace.
Once a robot arm was programmed, it should be able to operate for weeks
and months with only minor maintenance. As a result, the emphasis was

14
1
From Teleoperation To Autonomy
telemanipulators
planetary rovers
vision
manufacturing
1960
1970
1980
1990
2000
Industrial
Manipulators
AI Robotics
telesystems
Figure 1.1
A timeline showing forks in development of robots.
placed on the mechanical aspects of the robot to ensure precision and re-
peatability and methods to make sure the robot could move precisely and
repeatable, quickly enough to make a proﬁt. Because assembly lines were
engineered to mass produce a certain product, the robot didn’t have to be
able to notice any problems. The standards for mass production would make
it more economical to devise mechanisms that would ensure parts would be
in the correct place. A robot for automation could essentially be blind and
senseless.
Robotics for the space program took a different fork, concentrating instead
on highly specialized, one-of-a-kind planetary rovers. Unlike a highly auto-
mated manufacturing plant, a planetary rover operating on the dark side of
the moon (no radio communication) might run into unexpected situations.
Consider that on Apollo 17, astronaut and geologist Harrison Schmitt found
an orange rock on the moon; an orange rock was totally unexpected. Ideally,
a robot would be able to notice something unusual, stop what it was doing
(as long as it didn’t endanger itself) and investigate. Since it couldn’t be pre-
programmed to handle all possible contingencies, it had to be able to notice
its environment and handle any problems that might occur. At a minimum,
a planetary rover had to have some source of sensory inputs, some way of
interpreting those inputs, and a way of modifying its actions to respond to
a changing world. And the need to sense and adapt to a partially unknown
environment is the need for intelligence.
The fork toward AI robots has not reached a termination point of truly au-
tonomous, intelligent robots. In fact, as will be seen in Ch. 2 and 4, it wasn’t
until the late 1980’s that any visible progress toward that end was made. So
what happened when someone had an application for a robot which needed

1.2
How Can a Machine Be Intelligent?
15
real-time adaptability before 1990? In general, the lack of machine intelli-
gence was compensated by the development of mechanisms which allow a
human to control all, or parts, of the robot remotely. These mechanisms are
generally referred to under the umbrella term: teleoperation. Teleoperation
can be viewed as the “stuff” in the middle of the two forks. In practice, in-
telligent robots such as the Mars Sojourner are controlled with some form of
teleoperation. This chapter will cover the ﬂavors of teleoperation, given their
importance as a stepping stone towards truly intelligent robots.
The chapter concludes by visiting the issues in AI, and argues that AI is im-
perative for many robotic applications. Teleoperation is simply not sufﬁcient
or desirable as a long term solution. However, it has served as a reasonable
patch.
It is interesting to note that the two forks, manufacturing and AI, currently
appear to be merging. Manufacturing is now shifting to a “mass customiza-
tion” phase, where companies which can economically make short runs of
special order goods are thriving. The pressure is on for industrial robots,
more correctly referred to as industrial manipulators, to be rapidly repro-
grammed and more forgiving if a part isn’t placed exactly as expected in its
workspace. As a result, AI techniques are migrating to industrial manipula-
tors.
1.2
How Can a Machine Be Intelligent?
The science of making machines act intelligently is usually referred to as artiﬁ-
ARTIFICIAL
INTELLIGENCE
cial intelligence, or AI for short. Artiﬁcial Intelligence has no commonly ac-
cepted deﬁnitions. One of the ﬁrst textbooks on AI deﬁned it as “the study
of ideas that enable computers to be intelligent,” 143 which seemed to beg the
question. A later textbook was more speciﬁc, “AI is the attempt to get the
computer to do things that, for the moment, people are better at.”120 This
deﬁnition is interesting because it implies that once a task is performed suc-
cessfully by a computer, then the technique that made it possible is no longer
AI, but something mundane. That deﬁnition is fairly important to a person
researching AI methods for robots, because it explains why certain topics
suddenly seem to disappear from the AI literature: it was perceived as being
solved! Perhaps the most amusing of all AI deﬁnitions was the slogan for
the now defunct computer company, Thinking Machines, Inc., “... making
machines that will be proud of us.”

16
1
From Teleoperation To Autonomy
The term AI is controversial, and has sparked ongoing philosophical de-
bates on whether a machine can ever be intelligent. As Roger Penrose notes
in his book, The Emperor’s New Mind: “Nevertheless, it would be fair to
say that, although many clever things have indeed been done, the simula-
tion of anything that could pass for genuine intelligence is yet a long way
off.“115 Engineers often dismiss AI as wild speculation. As a result of such
vehement criticisms, many researchers often label their work as “intelligent
systems” or "knowledge-based systems” in an attempt to avoid the contro-
versy surrounding the term “AI.”
A single, precise deﬁnition of AI is not necessary to study AI robotics. AI
robotics is the application of AI techniques to robots. More speciﬁcally, AI
robotics is the consideration of issues traditional covered by AI for applica-
tion to robotics: learning, planning, reasoning, problem solving, knowledge
representation, and computer vision. An article in the May 5, 1997 issue
of Newsweek, “Actually, Chess is Easy,” discusses why robot applications
are more demanding for AI than playing chess. Indeed, the concepts of the
reactive paradigm, covered in Chapter 4, inﬂuenced major advances in tra-
ditional, non-robotic areas of AI, especially planning. So by studying AI ro-
botics, a reader interested in AI is getting exposure to the general issues in
AI.
1.3
What Can Robots Be Used For?
Now that a working deﬁnition of a robot and artiﬁcial intelligence has been
established, an attempt can be made to answer the question: what can intel-
ligent robots be used for? The short answer is that robots can be used for just
about any application that can be thought of. The long answer is that robots
are well suited for applications where 1) a human is at signiﬁcant risk (nu-
clear, space, military), 2) the economics or menial nature of the application
result in inefﬁcient use of human workers (service industry, agriculture), and
3) for humanitarian uses where there is great risk (demining an area of land
mines, urban search and rescue). Or as the well-worn joke among roboticists
goes, robots are good for the 3 D’s: jobs that are dirty, dull, or dangerous.
THE 3 D’S
Historically, the military and industry invested in robotics in order to build
nuclear weapons and power plants; now, the emphasis is on using robots for
environmental remediation and restoration of irradiated and polluted sites.
Many of the same technologies developed for the nuclear industry for pro-
cessing radioactive ore is now being adapted for the pharmaceutical indus-

1.3
What Can Robots Be Used For?
17
try; processing immune suppressant drugs may expose workers to highly
toxic chemicals.
Another example of a task that poses signiﬁcant risk to a human is space
exploration. People can be protected in space from the hard vacuum, solar
radiation, etc., but only at great economic expense. Furthermore, space suits
are so bulky that they severely limit an astronaut’s ability to perform simple
tasks, such as unscrewing and removing an electronics panel on a satellite.
Worse yet, having people in space necessitates more people in space. Solar
radiation embrittlement of metals suggests that astronauts building a large
space station would have to spend as much time repairing previously built
portions as adding new components. Even more people would have to be
sent into space, requiring a larger structure. the problem escalates. A study
by Dr. Jon Erickson’s research group at NASA Johnson Space Center argued
that a manned mission to Mars was not feasible without robot drones capable
of constantly working outside of the vehicle to repair problems introduced
by deadly solar radiation.51 (Interestingly enough, a team of three robots
which did just this were featured in the 1971 ﬁlm, Silent Running, as well as
by a young R2D2 in The Phantom Menace.)
Nuclear physics and space exploration are activities which are often far re-
moved from everyday life, and applications where robots ﬁgure more promi-
nently in the future than in current times.
The most obvious use of robots is manufacturing, where repetitious ac-
tivities in unpleasant surroundings make human workers inefﬁcient or ex-
pensive to retain. For example, robot “arms” have been used for welding
cars on assembly lines. One reason that welding is now largely robotic is
that it is an unpleasant job for a human (hot, sweaty, tedious work) with
a low tolerance for inaccuracy. Other applications for robots share similar
motivation: to automate menial, unpleasant tasks—usually in the service in-
dustry. One such activity is janitorial work, especially maintaining public
rest rooms, which has a high turnover in personnel regardless of payscale.
The janitorial problem is so severe in some areas of the US, that the Postal
Service offered contracts to companies to research and develop robots capa-
ble of autonomously cleaning a bathroom (the bathroom could be designed
to accommodate a robot).
Agriculture is another area where robots have been explored as an eco-
nomical alternative to hard to get menial labor. Utah State University has
been working with automated harvesters, using GPS (global positioning sat-
ellite system) to traverse the ﬁeld while adapting the speed of harvesting
to the rate of food being picked, much like a well-adapted insect. The De-

18
1
From Teleoperation To Autonomy
partment of Mechanical and Material Engineering at the University of West-
ern Australia developed a robot called Shear Majic capable of shearing a live
sheep. People available for sheep shearing has declined, along with proﬁt
margins, increasing the pressure on the sheep industry to develop economic
alternatives. Possibly the most creative use of robots for agriculture is a mo-
bile automatic milker developed in the Netherlands and in Italy.68;32 Rather
than have a person attach the milker to a dairy cow, the roboticized milker
arm identiﬁes the teats as the cow walks into her stall, targets them, moves
about to position itself, and ﬁnally reaches up and attaches itself.
Finally, one of the most compelling uses of robots is for humanitarian pur-
poses. Recently, robots have been proposed to help with detecting unex-
ploded ordinance (land mines) and with urban search and rescue (ﬁnding
survivors after a terrorist bombing of a building or an earthquake). Human-
itarian land demining is a challenging task. It is relatively easy to demine an
area with bulldozer, but that destroys the ﬁelds and improvements made by
the civilians and hurts the economy. Various types of robots are being tested
in the ﬁeld, including aerial and ground vehicles.73
1.3.1
Social implications of robotics
While many applications for artiﬁcially intelligent robots will actively reduce
risk to a human life, many applications appear to compete with a human’s
livelihood.
Don’t robots put people out of work?
One of the pervasive
themes in society has been the impact of science and technology on the dig-
nity of people. Charlie Chaplin’s silent movie, Modern Times, presented the
world with visual images of how manufacturing-oriented styles of manage-
ment reduces humans to machines, just “cogs in the wheel.”
Robots appear to amplify the tension between productivity and the role of
the individual. Indeed, the scientist in Metropolis points out to the corporate
ruler of the city that now that they have robots, they don’t need workers
anymore.
People who object to robots, or technology in general, are of-
ten called Luddites, after Ned Ludd, who is often credited with leading a
LUDDITES
short-lived revolution of workers against mills in Britain. Prior to the indus-
trial revolution in Britain, wool was woven by individuals in their homes
or collectives as a cottage industry. Mechanization of the weaving process
changed the jobs associated with weaving, the status of being a weaver (it
was a skill), and required people to work in a centralized location (like hav-
ing your telecommuting job terminated). Weavers attempted to organize and
destroyed looms and mill owners’ properties in reaction. After escalating vi-

1.4
A Brief History of Robotics
19
olence in 1812, legislation was passed to end worker violence and protect the
mills. The rebelling workers were persecuted. While the Luddite movement
may have been motivated by a quality-of-life debate, the term is often ap-
plied to anyone who objects to technology, or “progress,” for any reason. The
connotation is that Luddites have an irrational fear of technological progress.
The impact of robots is unclear, both what is the real story and how people
interact with robots. The HelpMate Robotics, Inc. robots and janitorial robots
appear to be competing with humans, but are ﬁlling a niche where it is hard
to get human workers at any price. Cleaning ofﬁce buildings is menial and
boring, plus the hours are bad. One janitorial company has now invested in
mobile robots through a Denver-based company, Continental Divide Robot-
ics, citing a 90% yearly turnover in staff, even with proﬁt sharing after two
years. The Robotics Industries Association, a trade group, produces annual
reports outlining the need for robotics, yet possibly the biggest robot money
makers are in the entertainment and toy industries.
The cultural implications of robotics cannot be ignored. While the sheep
shearing robots in Australia were successful and were ready to be commer-
cialized for signiﬁcant economic gains, the sheep industry reportedly re-
jected the robots. One story goes that the sheep ranchers would not accept
a robot shearer unless it had a 0% fatality rate (it’s apparently fairly easy to
nick an artery on a squirming sheep). But human shearers accidently kill
several sheep, while the robots had a demonstrably better rate. The use of
machines raises an ethical question: is it acceptable for an animal to die at the
hands of a machine rather than a person? What if a robot was performing a
piece of intricate surgery on a human?
1.4
A Brief History of Robotics
Robotics has its roots in a variety of sources, including the way machines are
controlled and the need to perform tasks that put human workers at risk.
In 1942, the United States embarked on a top secret project, called the Man-
hattan Project, to build a nuclear bomb. The theory for the nuclear bomb had
existed for a number of years in academic circles. Many military leaders of
both sides of World War II believed the winner would be the side who could
build the ﬁrst nuclear device: the Allied Powers led by USA or the Axis, led
by Nazi Germany.
One of the ﬁrst problems that the scientists and engineers encountered
was handling and processing radioactive materials, including uranium and

20
1
From Teleoperation To Autonomy
Figure 1.2
A Model 8 Telemanipulator. The upper portion of the device is placed
in the ceiling, and the portion on the right extends into the hot cell. (Photograph
courtesy Central Research Laboratories.)
plutonium, in large quantities. Although the immensity of the dangers of
working with nuclear materials was not well understood at the time, all the
personnel involved knew there were health risks. One of the ﬁrst solutions
was the glove box. Nuclear material was placed in a glass box. A person
stood (or sat) behind a leaded glass shield and stuck their hands into thick
rubberized gloves. This allowed the worker to see what they were doing and
to perform almost any task that they could do without gloves.
But this was not an acceptable solution for highly radioactive materials,
and mechanisms to physically remove and completely isolate the nuclear
materials from humans had to be developed. One such mechanism was
a force reﬂecting telemanipulator, a sophisticated mechanical linkage which
TELEMANIPULATOR
translated motions on one end of the mechanism to motions at the other end.
A popular telemanipulator is shown in Fig. 1.2.
A nuclear worker would insert their hands into (or around) the telema-
nipulator, and move it around while watching a display of what the other
end of the arm was doing in a containment cell. Telemanipulators are simi-
lar in principle to the power gloves now used in computer games, but much
harder to use. The mechanical technology of the time did not allow a perfect
mapping of hand and arm movements to the robot arm. Often the opera-

1.4
A Brief History of Robotics
21
tor had to make non-intuitive and awkward motions with their arms to get
the robot arm to perform a critical manipulation—very much like working in
front of a mirror. Likewise, the telemanipulators had challenges in providing
force feedback so the operator could feel how hard the gripper was holding
an object. The lack of naturalness in controlling the arm (now referred to as
a poor Human-Machine Interface) meant that even simple tasks for an un-
encumbered human could take much longer. Operators might take years of
practice to reach the point where they could do a task with a telemanipulator
as quickly as they could do it directly.
After World War II, many other countries became interested in producing a
nuclear weapon and in exploiting nuclear energy as a replacement for fossil
fuels in power plants. The USA and Soviet Union also entered into a nu-
clear arms race. The need to mass-produce nuclear weapons and to support
peaceful uses of nuclear energy kept pressure on engineers to design robot
arms which would be easier to control than telemanipulators. Machines that
looked more like and acted like robots began to emerge, largely due to ad-
vances in control theory. After WWII, pioneering work by Norbert Wiener
allowed engineers to accurately control mechanical and electrical devices us-
ing cybernetics.
1.4.1
Industrial manipulators
Successes with at least partially automating the nuclear industry also meant
the technology was available for other applications, especially general man-
ufacturing.
Robot arms began being introduced to industries in 1956 by
Unimation (although it wouldn’t be until 1972 before the company made a
proﬁt).37 The two most common types of robot technology that have evolved
for industrial use are robot arms, called industrial manipulators, and mobile
carts, called automated guided vehicles (AGVs).
An industrial manipulator, to paraphrase the Robot Institute of America’s
INDUSTRIAL
MANIPULATOR
deﬁnition, is a reprogrammable and multi-functional mechanism that is de-
signed to move materials, parts, tools, or specialized devices. The emphasis
in industrial manipulator design is being able to program them to be able
to perform a task repeatedly with a high degree of accuracy and speed. In
order to be multi-functional, many manipulators have multiple degrees of
freedom, as shown in Fig. 1.4. The MOVEMASTER arm has ﬁve degrees
of freedom, because it has ﬁve joints, each of which is capable of a single
rotational degree of freedom. A human arm has three joints (shoulder, el-

22
1
From Teleoperation To Autonomy
Figure 1.3
An RT3300 industrial manipulator. (Photograph courtesy of Seiko Instru-
ments.)
bow, and wrist), two of which are complex (shoulder and wrist), yielding six
degrees of freedom.
Control theory is extremely important in industrial manipulators. Rapidly
moving around a large tool like a welding gun introduces interesting prob-
lems, like when to start decelerating so the gun will stop in the correct loca-
tion without overshooting and colliding with the part to be welded. Also,
oscillatory motion, in general, is undesirable. Another interesting problem is
the joint conﬁguration. If a robot arm has a wrist, elbow and shoulder joints
like a human, there are redundant degrees of freedom. Redundant degrees
of freedom means there are multiple ways of moving the joints that will ac-
complish the same motion. Which one is better, more efﬁcient, less stressful
on the mechanisms?
It is interesting to note that most manipulator control was assumed to be
ballistic control, or open loop control. In ballistic control, the position trajectory
BALLISTIC CONTROL
OPEN LOOP CONTROL
and velocity proﬁle is computed once, then the arm carries it out. There
are no “in-ﬂight” corrections, just like a ballistic missile doesn’t make any
course corrections. In order to accomplish a precise task with ballistic control,
everything about the device and how it works has to be modeled and ﬁgured
into the computation. The opposite of ballistic control is closed-loop control,
CLOSED-LOOP
CONTROL
where the error between the goal and current position is noted by a sensor(s),

1.4
A Brief History of Robotics
23
a.
b.
Figure 1.4
A MOVEMASTER robot: a.) the robot arm and b.) the associated joints.
and a new trajectory and proﬁle is computed and executed, then modiﬁed on
the next update, and so on. Closed-loop control requires external sensors to
provide the error signal, or feedback.
FEEDBACK
In general, if the structural properties of the robot and its cargo are known,
these questions can be answered and a program can be developed. In prac-
tice, the control theory is complex. The dynamics (how the mechanism moves
and deforms) and kinematics (how the components of the mechanism are
connected) of the system have to be computed for each joint of the robot, then
those motions can be propagated to the next joint iteratively. This requires a
computationally consuming change of coordinate systems from one joint to
the next. To move the gripper in Fig 1.4 requires four changes of coordinates
to go from the base of the arm to the gripper. The coordinate transformations
often have singularities, causing the equations to perform divide by zeros. It
can take a programmer weeks to reprogram a manipulator.
One simplifying solution is to make the robot rigid at the desired velocities,
reducing the dynamics. This eliminates having to compute the terms for
overshooting and oscillating. However, a robot is made rigid by making it

24
1
From Teleoperation To Autonomy
heavier. The end result is that it is not uncommon for a 2 ton robot to be
able to handle only a 200 pound payload. Another simplifying solution is to
avoid the computations in the dynamics and kinematics and instead have the
programmer use a teach pendant. Using a teach pendant (which often looks
TEACH PENDANT
like a joystick or computer game console), the programmer guides the robot
through the desired set of motions. The robot remembers these motions and
creates a program from them. Teach pendants do not mitigate the danger
of working around a 2 ton piece of equipment. Many programmers have to
direct the robot to perform delicate tasks, and have to get physically close
to the robot in order to see what the robot should do next. This puts the
programmer at risk of being hit by the robot should it hit a singularity point
in its joint conﬁguration or if the programmer makes a mistake in directing
a motion. You don’t want to have your head next to a 2 ton robot arm if it
suddenly spins around!
Automatic guided vehicles, or AGVs, are intended to be the most ﬂexible con-
AUTOMATIC GUIDED
VEHICLES
veyor system possible: a conveyor which doesn’t need a continuous belt or
roller table. Ideally an AGV would be able to pick up a bin of parts or man-
ufactured items and deliver them as needed. For example, an AGV might
receive a bin containing an assembled engine. It could then deliver it au-
tomatically across the shop ﬂoor to the car assembly area which needed an
engine. As it returned, it might be diverted by the central computer and in-
structed to pick up a defective part and take it to another area of the shop for
reworking.
However, navigation (as will be seen in Part II) is complex. The AGV has
to know where it is, plan a path from its current location to its goal desti-
nation, and to avoid colliding with people, other AGVs, and maintenance
workers and tools cluttering the factory ﬂoor. This proved too difﬁcult to do,
especially for factories with uneven lighting (which interferes with vision)
and lots of metal (which interferes with radio controllers and on-board radar
and sonar). Various solutions converged on creating a trail for the AGV to
follow. One method is to bury a magnetic wire in the ﬂoor for the AGV to
sense. Unfortunately, changing the path of an AGV required ripping up the
concrete ﬂoor. This didn’t help with the ﬂexibility needs of modern manu-
facturing. Another method is to put down a strip of photochemical tape for
the vehicle to follow. The strip is unfortunately vulnerable, both to wear and
to vandalism by unhappy workers. Regardless of the guidance method, in
the end the simplest way to thwart an AGV was to something on its path.
If the AGV did not have range sensors, then it would be unable to detect
an expensive piece of equipment or a person put deliberately in its path. A

1.4
A Brief History of Robotics
25
few costly collisions would usually led to the AGV’s removal. If the AGV
did have range sensors, it would stop for anything. A well placed lunch box
could hold the AGV for hours until a manager happened to notice what was
going on. Even better from a disgruntled worker’s perspective, many AGVs
would make a loud noise to indicate the path was blocked. Imagine having
to constantly remove lunch boxes from the path of a dumb machine making
unpleasant siren noises.
From the ﬁrst, robots in the workplace triggered a backlash. Many of the
human workers felt threatened by a potential loss of jobs, even though the
jobs being mechanized were often menial or dangerous. This was particu-
larly true of manufacturing facilities which were unionized. One engineer
reported that on the ﬁrst day it was used in a hospital, a HelpMate Robotics
cart was discovered pushed down the stairs. Future models were modiﬁed
to have some mechanisms to prevent malicious acts.
Despite the emerging Luddite effect, industrial engineers in each of the
economic powers began working for a black factory in the 1980’s. A black fac-
BLACK FACTORY
tory is a factory that has no lights turned on because there are no workers.
Computers and robots were expected to allow complete automation of man-
ufacturing processes, and courses in “Computer-Integrated Manufacturing
Systems” became popular in engineering schools.
But two unanticipated trends undermined industrial robots in a way that
the Luddite movement could not. First, industrial engineers did not have
experience designing manufacturing plants with robots.
Often industrial
manipulators were applied to the wrong application. One of the most em-
barrassing examples was the IBM Lexington printer plant. The plant was
built with a high degree of automation, and the designers wrote numerous
articles on the exotic robot technology they had cleverly designed. Unfortu-
nately, IBM had grossly over-estimated the market for printers and the plant
sat mostly idle at a loss. While the plant’s failure wasn’t the fault of robotics,
per se, it did cause many manufacturers to have a negative view of automa-
tion in general. The second trend was the changing world economy. Cus-
tomers were demanding “mass customization.” Manufacturers who could
make short runs of a product tailored to each customer on a large scale were
the ones making the money. (Mass customization is also referred to as “agile
manufacturing.”) However, the lack of adaptability and difﬁculties in pro-
gramming industrial robot arms and changing the paths of AGVs interfered
with rapid retooling. The lack of adaptability, combined with concerns over
worker safety and the Luddite effect, served to discourage companies from
investing in robots through most of the 1990’s.

26
1
From Teleoperation To Autonomy
a.
b.
Figure 1.5
Motivation for intelligent planetary rovers: a.) Astronaut John Young
awkwardly collecting lunar samples on Apollo 16, and b.) Astronaut Jim Irwin stop-
ping the lunar rover as it slides down a hill on Apollo 15. (Photographs courtesy of
the National Aeronautics and Space Administration.)
1.4.2
Space robotics and the AI approach
While the rise of industrial manipulators and the engineering approach to
robotics can in some measure be traced to the nuclear arms race, the rise
of the AI approach can be said to start with the space race. On May 25,
1961, spurred by the success of the Soviet Union’s Sputnik space programs,
President John F. Kennedy announced that United States would put a man
on the moon by 1970. Walking on the moon was just one aspect of space
exploration. There were concerns about the Soviets setting up military bases
on the Moon and Mars and economic exploitation of planetary resources.
Clearly there was going to be a time lag of almost a decade before humans
from the USA would go to the Moon. And even then, it would most likely be
with experimental spacecraft, posing a risk to the human astronauts. Even
without the risk to humans, the bulk of spacesuits would make even triv-
ial tasks difﬁcult for astronauts to perform. Fig. 1.5a shows astronaut John
Young on Apollo 16 collecting samples with a lunar rake. The photo shows
the awkward way the astronaut had to bend his body and arms to complete
the task.
Planetary rovers were a possible solution, either to replace an astronaut or
assist him or her. Unfortunately, rover technology in the 1960’s was limited.
Because of the time delays, a human would be unable to safely control a rover
over the notoriously poor radio links of the time, even if the rover went very

1.4
A Brief History of Robotics
27
slow. Therefore, it would be desirable to have a robot that was autonomous.
One option would be to have mobile robots land on a planetary conduct pre-
liminary explorations, conduct tests, etc., and radio back the results. These
automated planetary rovers would ideally have a high degree of autonomy,
much like a trained dog. The robot would receive commands from Earth
to explore a particular region. It would navigate around boulders and not
fall into canyons, and traverse steep slopes without rolling over. The robot
might even be smart enough to regulate its own energy supply, for example,
by making sure it was sheltered during the planetary nights and to stop what
it was doing and position itself for recharging its solar batteries. A human
might even be able to speak to it in a normal way to give it commands.
Getting a mobile robot to the level of a trained dog immediately presented
new issues. Just by moving around, a mobile robot could change the world-
for instance, by causing a rock slide. Fig. 1.5b shows astronaut Jim Irwin res-
cuing the lunar rover during an extra-vehicular activity (EVA) on Apollo 15
as it begins to slide downhill. Consider that if an astronaut has difﬁculty ﬁnd-
ing a safe parking spot on the moon, how much more challenging it would
be for an autonomous rover. Furthermore, an autonomous rover would have
no one to rescue it, should it make a mistake.
Consider the impact of uncertain or incomplete information on a rover
that didn’t have intelligence. If the robot was moving based on a map taken
from a telescope or an overhead command module, the map could still con-
tain errors or at the wrong resolution to see certain dangers. In order to
navigate successfully, the robot has to compute its path with the new data
or risk colliding with a rock or falling into a hole. What if the robot did
something broke totally unexpected or all the assumptions about the planet
were wrong? In theory, the robot should be able to diagnose the problem
and attempt to continue to make progress on its task. What seemed at ﬁrst
like an interim solution to putting humans in space quickly became more
complicated.
Clearly, developing a planetary rover and other robots for space was go-
ing to require a concentrated, long-term effort. Agencies in the USA such
as NASA Jet Propulsion Laboratory (JPL) in Pasadena, California, were given
the task of developing the robotic technology that would be needed to pre-
pare the way for astronauts in space. They were in a position to take advan-
tage of the outcome of the Dartmouth Conference. The Dartmouth Conference
was a gathering hosted by the Defense Advanced Research Projects Agency
(DARPA) in 1955 of prominent scientists working with computers or on the
theory for computers. DARPA was interested in hearing what the potential

28
1
From Teleoperation To Autonomy
uses for computers were. One outcome of the conference was the term “ar-
tiﬁcial intelligence”; the attending scientists believed that computers might
become powerful enough to understand human speech and duplicate hu-
man reasoning. This in turn suggested that computers might mimic the ca-
pabilities of animals and humans sufﬁciently for a planetary rover to survive
for long periods with only simple instructions from Earth.
As an indirect result of the need for robotics converging with the possi-
bility of artiﬁcial intelligence, the space program became one of the earliest
proponents of developing AI for robotics. NASA also introduced the notion
that AI robots would of course be mobile, rather than strapped to a factory
ﬂoor, and would have to integrate all forms of AI (understanding speech,
planning, reasoning, representing the world, learning) into one program—a
daunting task which has not yet been reached.
1.5
Teleoperation
Teleoperation is when a human operator controls a robot from a distance (tele
TELEOPERATION
means “remote”). The connotation of teleoperation is that the distance is too
great for the operator to see what the robot is doing, so radio controlled toy
cars are not considered teleoperation systems. The operator and robot have
some type of master-slave relationship. In most cases, the human operator
sits at a workstation and directs a robot through some sort of interface, as
seen in Fig. 1.6.
The control interface could be a joystick, virtual reality gear, or any num-
ber of innovative interfaces. The human operator, or teleoperator, is often
referred to as the local (due to being at the local workstation) and the robot
LOCAL
as the remote (since it is operating at a remote location from the teleoperator).
REMOTE
The local must have some type of display and control mechanisms, while the
remote must have sensors, effectors, power, and in the case of mobile robots,
mobility.141 The teleoperator cannot look at what the remote is doing directly,
either because the robot is physically remote (e.g., on Mars) or the local has
to be shielded (e.g., in a nuclear or pharmaceutical processing plant hot cell).
Therefore, the sensors which acquire information about the remote location,
SENSORS
the display technology for allowing the operator to see the sensor data, and
DISPLAY
the communication link between the local and remote are critical components
COMMUNICATION LINK
of a telesystem.141
Teleoperation is a popular solution for controlling remotes because AI tech-
nology is nowhere near human levels of competence, especially in terms of

1.5
Teleoperation
29
Sensor
Mobility
REMOTE
Effector
Power
Display
Control
LOCAL
Communication
Figure 1.6
Organization of a telesystem. (Photographs courtesy of Oak Ridge Na-
tional Laboratory.)
perception and decision making. One example of teleoperation is the explo-
ration of underwater sites such as the Titanic. Having a human control a
robot is advantageous because a human can isolate an object of interest, even
partially obscured by mud in murky water as described by W. R. Uttal. 141
Humans can also perform dextrous manipulation (e.g., screwing a nut on a
bolt), which is very difﬁcult to program a manipulator to do.

30
1
From Teleoperation To Autonomy
Figure 1.7
Sojourner Mars rover. (Photograph courtesy of the National Aeronautics
and Space Administration.)
Another example is the Sojourner robot (shown in Fig. 1.7) which explored
Mars from July 5 to September 27, 1997, until it ceased to reply to radio com-
mands. Since there was little data before Sojourner on what Mars is like,
it is hard to develop sensors and algorithms which can detect important at-
tributes or even control algorithms to move the robot. It is important that any
unusual rocks or rock formations (like the orange rock Dr. Schmitt found on
the Moon during Apollo 17) be detected. Humans are particularly adept at
perception, especially seeing patterns and anomalies in pictures. Current AI
perceptual abilities fall far short of human abilities. Humans are also adept
at problem solving. When the Mars Pathﬁnder craft landed on Mars, the
air bags that had cushioned the landing did not deﬂate properly. When the
petals of the lander opened, an airbag was in the way of Sojourner. The
solution? The ground controllers sent commands to retract the petals and
open them again. That type of problem solving is extremely difﬁcult for the
current capabilities of AI.
But teleoperation is not an ideal solution for all situations. Many tasks
are repetitive and boring. For example, consider using a joystick to drive
a radio-controlled car; after a few hours, it tends to get harder and harder
to pay attention. Now imagine trying to control the car while only looking
through a small camera mounted in front. The task becomes much harder

1.5
Teleoperation
31
because of the limited ﬁeld of view; essentially there is no peripheral vision.
Also, the camera may not be transmitting new images very fast because the
communication link has a limited bandwidth, so the view is jerky. Most peo-
ple quickly experience cognitive fatigue; their attention wanders and they may
COGNITIVE FATIGUE
even experience headaches and other physical symptoms of stress. Even if
the visual display is excellent, the teleoperator may get simulator sickness due
SIMULATOR SICKNESS
to the discordance between the visual system saying the operator is moving
and the inner ear saying the operator is stationary. 141
Another disadvantage of teleoperation is that it can be inefﬁcient to use for
applications that have a large time delay.128 A large time delay can result in
the teleoperator giving a remote a command, unaware that it will place the
remote in jeopardy. Or, an unanticipated event such as a rock fall might occur
and destroy the robot before the teleoperator can see the event and command
the robot to ﬂee. A rule of thumb, or heuristic, is that the time it takes to do
TELEOPERATION
HEURISTIC
a task with traditional teleoperation grows linearly with the transmission
delay. A teleoperation task which took 1 minute for a teleoperator to guide a
remote to do on the Earth might take 2.5 minutes to do on the Moon, and 140
minutes on Mars.142 Fortunately, researchers have made some progress with
predictive displays, which immediately display what the simulation result of
PREDICTIVE DISPLAYS
the command would be.
The impact of time delays is not limited to planetary rovers. A recent ex-
ample of an application of teleoperation are unmanned aerial vehicles (UAV)
used by the United States to verify treaties by ﬂying overhead and taking
videos of the ground below. Advanced prototypes of these vehicles can ﬂy
autonomously, but take-offs and landings are difﬁcult for on-board computer
control. In this case of the Darkstar UAV (shown in Fig. 1.8), human oper-
ators were available to assume teleoperation control of the vehicle should
it encounter problems during take-off. Unfortunately, the contingency plan
did not factor in the 7 second delay introduced by using a satellite as the
communications link. Darkstar no. 1 did indeed experience problems on
take-off, but the teleoperator could not get commands to it fast enough be-
fore it crashed. As a result, it earned the unofﬁcial nickname “Darkspot.”
Another practical drawback to teleoperation is that there is at least one
person per robot, possibly more. The Predator unmanned aerial vehicle has
been used by the United States for veriﬁcation of the Dayton Accords in
Bosnia. One Predator requires at least one teleoperator to ﬂy the vehicle and
another teleoperator to command the sensor payload to look at particular
areas. Other UAVs have teams composed of up to four teleoperators plus a
ﬁfth team member who specializes in takeoffs and landings. These teleop-

32
1
From Teleoperation To Autonomy
Figure 1.8
Dark Star unmanned aerial vehicle.
(Photograph courtesy of De-
fenseLink, Ofﬁce of the Assistant Secretary of Defense-Public Affairs.)
erators may have over a year of training before they can ﬂy the vehicle. In
the case of UAVs, teleoperation permits a dangerous, important task to be
completed, but with a high cost in manpower.
According to Wampler,142 teleoperation is best suited for applications
TASK
CHARACTERISTICS
where:
1. The tasks are unstructured and not repetitive.
2. The task workspace cannot be engineered to permit the use of industrial
manipulators.
3. Key portions of the task intermittently require dextrous manipulation, es-
pecially hand-eye coordination.
4. Key portions of the task require object recognition, situational awareness,
or other advanced perception.
5. The needs of the display technology do not exceed the limitations of the
communication link (bandwidth, time delays).
6. The availability of trained personnel is not an issue.
1.5.1
Telepresence
An early attempt at reducing cognitive fatigue was to add more cameras with
faster update rates to widen the ﬁeld of view and make it more consistent
with how a human prefers to look at the world. This may not be practical

1.5
Teleoperation
33
for many applications because of limited bandwidth. Video telephones, pic-
ture phones, or video-conferencing over the Internet with their jerky, asyn-
chronous updates are usually examples of annoying limited bandwidth. In
these instances, the physical restrictions on how much and how fast informa-
tion can be transmitted result in image updates much slower than the rates
human brains expect. The result of limited bandwidth is jerky motion and
increased cognitive fatigue. So adding more cameras only exacerbates the
problem by adding more information that must be transmitted over limited
bandwidth.
One area of current research in teleoperation is the use of telepresence to
TELEPRESENCE
reduce cognitive fatigue and simulator sickness by making the human-robot
interface more natural. Telepresence aims for what is popularly called virtual
VIRTUAL REALITY
reality, where the operator has complete sensor feedback and feels as if she
were the robot. If the operator turns to look in a certain direction, the view
from the robot is there. If the operator pushes on a joystick for the robot to
move forward and the wheels are slipping, the operator would hear and feel
the motors straining while seeing that there was no visual change. This pro-
vides a more natural interface to the human, but it is very expensive in terms
of equipment and requires very high bandwidth rates. It also still requires
one person per robot. This is better than traditional teleoperation, but a long
way from having one teleoperator control multiple robots.
1.5.2
Semi-autonomous control
Another line of research in teleoperation is semi-autonomous control, often
SEMI-AUTONOMOUS
CONTROL
called supervisory control, where the remote is given an instruction or por-
SUPERVISORY CONTROL
tion of a task that it can safely do on its own.
There are two ﬂavors of
semi-autonomous control: continuous assistance, or shared control, and con-
trol trading.
In continuous assistance systems, the teleoperator and remote share con-
SHARED CONTROL
trol. The teleoperator can either delegate a task for the robot to do or can
do it via direct control. If the teleoperator delegates the task to the robot,
the human must still monitor to make sure that nothing goes wrong. This is
particularly useful for teleoperating robot arms in space. The operator can
relax (relatively) while the robot arm moves into the speciﬁed position near
a panel, staying on alert in case something goes wrong. Then the operator
can take over and perform the actions which require hand-eye coordination.
Shared control helps the operator avoid cognitive fatigue by delegating bor-
ing, repetitive control actions to the robot. It also exploits the ability of a

34
1
From Teleoperation To Autonomy
human to perform delicate operations. However, it still requires a high com-
munication bandwidth.
An alternative approach is control trading, where the human initiates an ac-
CONTROL TRADING
tion for the robot to complete autonomously. The human only interacts with
the robot to give it a new command or to interrupt it and change its orders.
The overall scheme is very much like a parent giving a 10-year old child a
task to do. The parent knows what the child is able to do autonomously
(e.g., clean their room). They have a common deﬁnition (clean room means
go to the bedroom, make the bed, and empty the wastebaskets). The parent
doesn’t care about the details of how the child cleans the room (e.g., whether
the wastebasket is emptied before the bed is made or vice versa). Control
trading assumes that the robot is capable of autonomously accomplishing
certain tasks without sharing control. The advantage is that, in theory, the
local operator can give a robot a task to do, then turn attention to another
robot and delegate a task to it, etc. A single operator could control multiple
robots because they would not require even casual monitoring while they
were performing a task. Supervisory control also reduces the demand on
bandwidth and problems with communication delays. Data such as video
images need to be transferred only when the local is conﬁguring the remote
for a new task, not all the time. Likewise, since the operator is not involved
in directly controlling the robot, a 2.5 minute delay in communication is ir-
relevant; the robot either wrecked itself or it didn’t. Unfortunately, control
trading assumes that robots have actions that they can perform robustly even
in unexpected situations; this may or may not be true. Which brings us back
to the need for artiﬁcial intelligence.
Sojourner exhibited both ﬂavors of supervisory control. It was primarily
programmed for traded control, where the geologists could click on a rock
and Sojourner would autonomously navigate close to it, avoiding rocks, etc.
However, some JPL employees noted that the geologists tended to prefer to
use shared control, watching every movement. A difﬁculty with most forms
of shared control is that it is assumed that the human is smarter than the
robot. This may be true, but the remote may have better sensor viewpoints
and reaction times.
1.6
The Seven Areas of AI
Now that some possible uses and shortcomings of robots have been covered,
it is motivating to consider what are the areas of artiﬁcial intelligence and

1.6
The Seven Areas of AI
35
how they could be used to overcome these problems. The Handbook of Ar-
tiﬁcial Intelligence64 divides up the ﬁeld into seven main areas: knowledge
representation, understanding natural language, learning, planning and problem
solving, inference, search, and vision.
1. Knowledge representation. An important, but often overlooked, issue is
KNOWLEDGE
REPRESENTATION
how does the robot represent its world, its task, and itself. Suppose a robot
is scanning a pile of rubble for a human. What kind of data structure and
algorithms would it take to represent what a human looks like? One way
is to construct a structural model: a person is composed of an oval head,
a cylindrical torso, smaller cylindrical arms with bilateral symmetry, etc.
Of course, what happens if only a portion of the human is visible?
2. Understanding natural language. Natural language is deceptively chal-
UNDERSTANDING
NATURAL LANGUAGE
lenging, apart from the issue of recognizing words which is now being
done by commercial products such as Via Voice and Naturally Speaking.
It is not just a matter of looking up words, which is the subject of the
following apocryphal story about AI. The story goes that after Sputnik
went up, the US government needed to catch up with the Soviet scientists.
However, translating Russian scientiﬁc articles was time consuming and
not many US citizens could read technical reports in Russian. Therefore,
the US decided to use these newfangled computers to create translation
programs. The day came when the new program was ready for its ﬁrst
test. It was given the proverb: the spirit is willing, but the ﬂesh is weak.
The reported output: the vodka is strong, but the meat is rotten.
3. Learning. Imagine a robot that could be programmed by just watching a
LEARNING
human, or by just trying the task repeatedly itself.
4. Planning and problem solving. Intelligence is associated with the ability
PLANNING, PROBLEM
SOLVING
to plan actions needed to accomplish a goal and solve problems with those
plans or when they don’t work. One of the earliest childhood fables, the
Three Pigs and the Big, Bad Wolf, involves two unintelligent pigs who
don’t plan ahead and an intelligent pig who is able to solve the problem
of why his brothers’ houses have failed, as well as plan an unpleasant
demise for the wolf.
5. Inference. Inference is generating an answer when there isn’t complete
INFERENCE
information. Consider a planetary rover looking at a dark region on the
ground. Its range ﬁnder is broken and all it has left is its camera and a
ﬁne AI system. Assume that depth information can’t be extracted from

36
1
From Teleoperation To Autonomy
the camera. Is the dark region a canyon? Is it a shadow? The rover will
need to use inference to either actively or passively disambiguate what
the dark region is (e.g., kick a rock at the dark area versus reason that
there is nothing nearby that could create that shadow).
6. Search. Search doesn’t necessarily mean searching a large physical space
SEARCH
for an object. In AI terms, search means efﬁciently examining a know-
ledge representation of a problem (called a “search space”) to ﬁnd the
answer. Deep Blue, the computer that beat the World Chess master Gary
Kasparov, won by searching through almost all possible combinations of
moves to ﬁnd the best move to make. The legal moves in chess given the
current state of the board formed the search space.
7. Vision. Vision is possibly the most valuable sense humans have. Studies
VISION
by Harvard psychologist Steven Kosslyn suggest that much of problem
solving abilities stem from the ability to visually simulate the effects of
actions in our head. As such, AI researchers have pursued creating vision
systems both to improve robotic actions and to supplement other work in
general machine intelligence.
Finally, there is a temptation to assume that the history of AI Robotics is the
story of how advances in AI have improved robotics. But that is not the
case. In many regards, robotics has played a pivotal role in advancing AI.
Breakthroughs in methods for planning (operations research types of prob-
lems) came after the paradigm shift to reactivity in robotics in the late 1980’s
showed how unpredictable changes in the environment could actually be ex-
ploited to simplify programming. Many of the search engines on the world
wide web use techniques developed for robotics. These programs are called
software agents: autonomous programs which can interact with and adapt to
SOFTWARE AGENTS
their world just like an animal or a smart robot. The term web-bot directly
WEB-BOT
reﬂects on the robotic heritage of these AI systems. Even animation is being
changed by advances in AI robotics. According to a keynote address given
by Danny Hillis at the 1997 Autonomous Agents conference, animators for
Disney’s Hunchback of Notre Dame programmed each cartoon character in the
crowd scenes as if it were a simulation of a robot, and used methods that will
be discussed in Ch. 4.

1.7
Summary
37
1.7
Summary
AI robotics is a distinct ﬁeld, both historically and in scope, from industrial
robotics. Industrial robots has concentrated on control theory issues, particu-
larly solving the dynamics and kinematics of a robot. This is concerned with
having the stationary robot perform precise motions, repetitively in a struc-
tured factory environment. AI robotics has concentrated on how a mobile
robot should handle unpredictable events in an unstructured world. The de-
sign of an AI robot should consider how the robot will represent knowledge
about the world, whether it needs to understand natural language, can it
learn tasks, what kind of planning and problem solving will it have to do,
how much inference is expected, how can it rapidly search its database and
knowledge for answers, and what mechanisms will it use for perceiving the
world.
Teleoperation arose as an intermediate solution to tasks that required au-
tomation but for which robots could not be adequately programmed to han-
dle. Teleoperation methods typically are cognitive fatiguing, require high
communication bandwidths and short communication delays, and require
one or more teleoperators per remote. Telepresence techniques attempt to
create a more natural interface for the human to control the robot and inter-
pret what it is doing and seeing, but at a high communication cost. Supervi-
sory control attempts to delegate portions of the task to the remote, either to
do autonomously (traded control) or with reduced, but continuous, human
interaction (shared control).
1.8
Exercises
Exercise 1.1
List the four attributes for evaluating an architecture. Based on what you know from
your own experience, evaluate MS Windows 95/98/2000 as an architecture for tele-
operating a robot.
Exercise 1.2
Name the three primitives for expressing the components of a robotics paradigm.
Exercise 1.3
Name the three robotic paradigms, and draw the relationship between the primitives.
Exercise 1.4
What is an intelligent robot?

38
1
From Teleoperation To Autonomy
Exercise 1.5
What is a Luddite?
Exercise 1.6
Describe at least two differences between AI and Engineering approaches to robotics.
Exercise 1.7
List three problems with teleoperation.
Exercise 1.8
Describe the components and the responsibilities of the local and the remote members
of a telesystem.
Exercise 1.9
Describe the difference between telepresence and semi-autonomous control.
Exercise 1.10
List the six characteristics of applications that are well suited for teleoperation. Give
at least two examples of potentially good applications for teleoperation not covered
in the chapter.
Exercise 1.11
[World Wide Web]
Search the world wide web for sites that permit clients to use a robot remotely (one
example is Xavier at Carnegie Mellon University). Decide whether each site is using
human supervisory or shared control, and justify your answer.
Exercise 1.12
[World Wide Web]
Search the world wide web for applications and manufacturers of intelligent robots.
Exercise 1.13
[World Wide Web]
Dr. Harrison “Jack” Schmitt is a vocal proponent for space mining of Near Earth
Objects (NEOs) such as mineral-rich asteroids. Because of the economics of manned
mission, the small size of NEOs, human safety concerns, and the challenges of work-
ing in micro-gravity, space mining is expected to require intelligent robots. Search
the web for more information on space mining, and give examples of why robots are
needed.
Exercise 1.14
[Programming]
(This requires a robot with an on-board video camera and a teleoperation interface.)
Teleoperate the robot through a slalom course of obstacles while keeping the robot
in view as if controlling a RC car. Now looking only at the output of the video cam-
era, repeat the obstacle course. Repeat the comparison several times, and keep track
of the time to complete the course and number of collisions with obstacles. Which
viewpoint led to faster completion of the course? Fewer collisions? Why?

1.9
End Notes
39
Exercise 1.15
[Advanced Reading]
Read “Silicon Babies,” Scientiﬁc American, December 1991, pp 125-134, on the chal-
lenges of AI robotics. List the 7 topics of AI and give examples of robots or researchers
addressing each topic.
Exercise 1.16
[Science Fiction]
Read “Stranger in Paradise,” Isaac Asimov, The Complete Robot, Doubleday, 1982,
and enumerate the problems with telepresence illustrated by this story.
Exercise 1.17
[Science Fiction]
Watch the movie Star Gate. The military uses a teleoperated vehicle (in reality, NASA
Jet Propulsion Laboratory’s Hazbot) to ﬁrst go through the star gate and test the envi-
ronmental conditions on the other side. Discuss other ways in which the team could
have continued to use the robot to their advantage.
Exercise 1.18
[Science Fiction]
Watch the 1971 movie, The Andromeda Strain, by Michael Crichton. The movie has
several nerve wracking scenes as the scientists try to telemanipulate an unknown,
deadly organism as fast as possible without dropping it. What do you think can be
done with today’s robots?
1.9
End Notes
Finding robots in the popular press
There is no one-stop-shopping publication or web site for robot applications. Robotics
World is a business oriented publication which often has interesting blurbs. Popular
Mechanics and Popular Science often contain short pieces on new robots and applica-
tions, although those short bits are often enthusiastically optimistic. A new magazine,
Robot Science and Technology, appears to be bridging the gap between hobby ‘bots and
research ‘bots. In addition to surﬁng the web, annual proceedings from the IEEE
International Conference on Robotics and Automation (ICRA) and IEEE/RSJ Interna-
tional Conference on Intelligent Robots and Systems (IROS) contain scientiﬁc articles
on the newest robots emerging from university and government laboratories. Intel-
ligent Robotic Systems, ed. by S.G. Tzafestas, Marcel Dekker Inc, NY, 1991. This is a
collection of chapters by various researchers and laboratory managers. The work is a
bit dated now, but gives some feel for the variety of applications.
About Joe Engleberger
Joe Engleberger is often referred to as the “Father of Industrial Robots.” His impres-
sive resume includes having formed Unimation. His most recent robotics company
made the HelpMate robot for hospitals. Engleberger participates in many robotics

40
1
From Teleoperation To Autonomy
forums, where his sartorial style (he always wears a bow tie) and verbal style (stri-
dently pro-robotics, and that his company should get more money) make him easily
recognizable.
Science ﬁction and robotics
For science ﬁction enthusiasts, take a look at Clute, John, and Nicholls, Peter, “Grolier
Science Fiction: The Multimedia Encyclopedia of Science Fiction,” Grolier Electronic
Publishing, Danbury, CT, 1995. This entertaining CD provides a very detailed, cross-
referenced look at robots as a theme in science ﬁction and a lengthy list (and review)
of movies and books with robots in them. One of the most technically accurate movies
about robots is Silent Running. It was directed by Douglas Trumbull who gained fame
for his work in special effects, including 2001: A Space Odyssey, Close Encounters of the
Third Kind, and Blade Runner. The bulk of the movie concerns the day to day life of
Bruce Dern and three waist-high robots. The robots and how they interact with Dern
and their environment are very realistic and consistent with AI robotics. The only
downside is a laughably ecologically correct plot (written in part by Steven Bochco
and Michael Cimino) complete with songs by Joan Baez. Well worth watching for the
‘bots, especially if the audience is curious about the hippie movement.
Robot name trivia
Marvin, the mail robot, may be named after the cantankerous robot Marvin in The
Hitchhiker’s Guide to the Galaxy. That Marvin is widely assumed to be named after the
cantankerous AI researcher, Dr. Marvin Minksy, at MIT.
Have Spacesuit, Will Travel.
John Blitch brought to my attention the difﬁculty the Apollo astronauts had in accom-
plishing simple tasks due to the bulky space suits.

2
The Hierarchical Paradigm
Chapter Objectives:
 Describe the Hierarchical Paradigm in terms of the three robot paradigms
and its organization of sensing.
 Name and evaluate one representative Hierarchical architecture in terms
of: support for modularity, niche targetability, ease of portability to other
domains, robustness.
 Solve a simple navigation problem using Strips, given a world model, op-
erators, difference table, and difference evaluator. The state of the world
model should be shown after each step.
 Understand the meaning of the following terms as applied to robotics:
precondition, closed world assumption, open world, frame problem.
 Describe the mission planner, navigator, pilot organization of the Nested
Hierarchical Controller.
 List two advantages and disadvantages of the Hierarchical Paradigm.
2.1
Overview
The Hierarchical Paradigm is historically the oldest method of organizing in-
telligence in mainstream robotics. It dominated robot implementations from
1967, with the inception of the ﬁrst AI robot, Shakey (Fig. 2.1) at SRI, up until
the late 1980’s when the Reactive Paradigm burst onto the scene.
This chapter begins with a description of the Hierarchical Paradigm in
terms of the SENSE, PLAN, ACT primitives and by its sensing representa-
tion. The chapter then covers the use of Strips in Shakey to reason and plan

42
2
The Hierarchical Paradigm
Figure 2.1
Shakey, the ﬁrst AI robot. It was built by SRI for DARPA 1967–70. (Pho-
tograph courtesy of SRI.)
a path. Strips will serve to motivate the reader as to the computer challenges
inherent in even as simple a task as walking across a room. However, Strips
is not an architecture, per se, just an interesting technique which emerged
from trying to build an architecture. Two representative architectures are
presented, NHC and RCS, that serve as examples of robot architectures pop-
ular at the time. The chapter concludes with programming considerations.
2.2
Attributes of the Hierarchical Paradigm
As noted in Part I, a robotic paradigm is deﬁned by the relationship between
the three primitives (SENSE, PLAN, ACT) and by the way sensory data is
processed and distributed through the system.
The Hierarchical Paradigm is sequential and orderly, as shown in Figs. 2.2
and 2.3. First the robot senses the world and constructs a global world map.
Then “eyes” closed, the robot plans all the directives needed to reach the

2.2
Attributes of the Hierarchical Paradigm
43
SENSE
PLAN
ACT
Figure 2.2
S,P,A organization of Hierarchical Paradigm.
ROBOT PRIMITIVES
INPUT
OUTPUT
SENSE
PLAN
ACT
Sensor data
Sensed information
Information (sensed
and/or cognitive)
Sensed information
or directives
Directives
Actuator commands
Figure 2.3
Alternative description of how the 3 primitives interact in the Hierarchi-
cal Paradigm.
goal. Finally, the robot acts to carry out the ﬁrst directive. After the robot has
carried out the SENSE-PLAN-ACT sequence, it begins the cycle again: eyes
open, the robot senses the consequence of its action, replans the directives
(even though the directives may not have changed), and acts.
As shown in Fig. 2.3, sensing in the Hierarchical Paradigm is monolithic:
all the sensor observations are fused into one global data structure, which the
planner accesses. The global data structure is generally referred to as a world
WORLD MODEL
model. The term world model is very broad; “world” means both the outside
world, and whatever meaning the robot ascribes to it. In the Hierarchical
Paradigm, the world model typically contains
1. an a priori (previously acquired) representation of the environment the
A PRIORI
robot is operating in (e.g., a map of the building),
2. sensing information (e.g., “I am in a hallway, based on where I’ve trav-
eled, I must be in the northwest hallway”), plus
3. any additional cognitive knowledge that might be needed to accomplish
a task (e.g., all packages received in the mail need to be delivered to Room
118).

44
2
The Hierarchical Paradigm
Creating a single representation which can store all of this information can
be very challenging. Part of the reason for the “sub-turtle” velocity was the
lack of computing power during the 1960’s. However, as roboticists in the
1980’s began to study biological intelligence, a consensus arose that even
with increased computing power, the hierarchical, logic-based approach was
unsatisfactory for navigational tasks which require a rapid response time to
an open world.
2.2.1
Strips
Shakey, the ﬁrst AI mobile robot, needed a generalized algorithm for plan-
ning how to accomplish goals. (An algorithm is a procedure which is correct
ALGORITHM
and terminates.) For example, it would be useful to have the same program
allow a human to type in that the robot is in Ofﬁce 311 and should go to
Ofﬁce 313 or that the robot is in 313 and should the red box.
The method ﬁnally selected was a variant of the General Problem Solver
GENERAL PROBLEM
SOLVER (GPS)
method, called Strips. Strips uses an approach called means-ends analysis,
STRIPS
MEANS-ENDS ANALYSIS
where if the robot can’t accomplish the task or reach the goal in one “move-
ment”, it picks a action which will reduce the difference between what state
it was in now (e.g., where it was) versus the goal state (e.g., where it wanted
to be). This is inspired by cognitive behavior in humans; if you can’t see how
to solve a problem, you try to solve a portion of the problem to see if it gets
you closer to the complete solution.
Consider trying to program a robot to ﬁgure out how to get to the Stan-
ford AI Lab (SAIL). Unless the robot is at SAIL (represented in Strips as a
variable goal state), some sort of transportation will have to arranged.
GOAL STATE
Suppose the robot is in Tampa, Florida (initial state). The robot may
INITIAL STATE
represent the decision process of how to get to a location as function called an
operator which would consider the Euclidean distance (a variable named
OPERATOR
difference) between the goal state and initial state. The differ-
DIFFERENCE
ence between locations could be computed for comparison purposes, or eval-
uation, by the square of the hypotenuse (difference evaluator). For
DIFFERENCE
EVALUATOR
example using an arbitrary frame of reference that put Tampa at the center
of the world with made-up distances to Stanford:
initial state:
Tampa, Florida (0,0)
goal state:
Stanford, California (1000,2828)
difference:
3,000

2.2
Attributes of the Hierarchical Paradigm
45
This could lead to a data structure called a difference table of how to
DIFFERENCE TABLE
make decisions:
difference
operator
d200
fly
100<d
< 200
ride_train
d
 100
drive
d< 1
walk
Different modes of transportation are appropriate for different distances.
A mode of transportation, fly, ride_train, drive, walk, in the table
is really a function in the robot’s program. It is also called an operator, be-
cause it reduces the value stored in difference as to the distance from be-
ing in the initial state of Tampa and wanting to be at the goal state.
A robot following this difference table would begin by ﬂying as close as it
could to SAIL.
But suppose the robot ﬂew into the San Francisco airport. It’d be within
100 miles of SAIL, so the robot appears to have made an intelligent deci-
sion. But now the robot has a new difference to reduce. It examines the
difference table with a new value of difference. The table says the ro-
bot should drive. Drive what? A car? Ooops: if the robot did have a
personal car, it would be back in Tampa. The robot needs to be able to distin-
guish between driving its car and driving a rental car. This is done by listing
the preconditions that have to be true in order to execute that particu-
PRECONDITIONS
lar operator. The preconditions are a column in the difference table, where
a single operator can have multiple preconditions. In practice, the list of
preconditions is quite lengthy, but for the purposes of this example, only
drive_rental, drive_personal will be shown with preconditions.
difference
operator
preconditions
d200
fly
100<d<200
ride_train
d100
drive_rental
at airport
drive_personal
at home
d<1
walk
The difference table is now able to handle the issue of deciding to drive a
rental car. But this introduces a new issue: how does the robot know where
it is at? This is done by monitoring the state of the robot and its world. If
it took an airplane from Tampa to the San Francisco airport, its state has
changed. Its initial state is now at the San Francisco airport, and no

46
2
The Hierarchical Paradigm
longer Tampa. Therefore, whenever the robot executes an operator, there
is almost always something that has to be added to the robot’s knowledge
of the world state (which is entered into a add-list) and something that
ADD-LIST
has to be deleted (delete-list). This two lists are stored in the difference
DELETE-LIST
table so that when the robot picks an operator based on the difference and
operator, it can easily apply the appropriate modiﬁcations to the world. The
difference table below is expanded to show the add and delete lists.
difference
operator
pre-
add-
delete-
conditions
list
list
d200
fly
at Y
at X
at airport
100<d<200
train
at Y
at X
at station
d100
drive_rental
at airport
drive_personal
at home
d<1
walk
Of course, the above difference table is fairly incomplete. Driving a rental
car should have a precondition that there is a rental car available. (And that
the robot have a waiver from the state highway patrol to drive as an experi-
mental vehicle and a satisfactory method of payment.) The number of facts
and preconditions that have to be explicitly represented seem to be growing
explosively. Which is Very Bad from a programming standpoint.
The main point is that the difference table appears to be a good data struc-
ture for representing what a robot needs in planning a trip. It should be
apparent that a recursive function can be written which literally examines
each entry in the table for the ﬁrst operator that reduces the difference. The
resulting list of operators is actually the plan: a list of the steps (operators)
that the robot has to perform in order to reach a goal. The robot actually
constructs the plan before handing it off to another program to execute.
At this point in time, it isn’t likely that a robot will get on a plane and then
drive. So perhaps the criticisms of Strips is because the example used too
complicated a task to be realistic. Let’s see if Strips is more streamlined with
a simple task of getting from one room to another.
2.2.2
More realistic Strips example
The ﬁrst step in creating a Strips planner is to construct a Strips-based rep-
resentation of the world, or world model. Everything in the world that is
CONSTRUCTING A
WORLD MODEL

2.2
Attributes of the Hierarchical Paradigm
47
relevant to the problem is represented by facts, or axioms, in predicate logic.
AXIOMS
Predicates are functions that evaluate to TRUE or FALSE. By years of AI pro-
PREDICATES
gramming convention, predicates are usually written in uppercase.
Consider the problem of a robot named IT in a room, R1, who needs to go
to another room, R2, in the house shown in Fig. 2.4. In order to solve this
problem using Strips, the robot has to be given a way of representing the
world, which will in turn inﬂuence the difference table, a difference evalu-
ator, and how the add and delete lists are written. The world model in the
previous example was never formally deﬁned.
A world model is generally built up from static facts (represented as pred-
icates) from a set of candidates, and things in the world, like the robot. The
robot’s name is in all capitals because it exists (and therefore is TRUE). Low-
ercase identiﬁers indicate that the thing is a variable, that a real thing hasn’t
been assigned to that placeholder yet.
Suppose the robot was limited to knowing only whether a movable object
was in a room, next to a door or another movable object, and whether a door
was open or closed and what rooms it connected. In a programming sense,
there would be only three types of things in the world: movable_object
(such as the robot, boxes it should pick up), room, and door. The robot’s
knowledge could be represented by the following predicates:
INROOM(x, r)
where x is an object of type movable_object,
r is type room
NEXTTO(x, t)
where x is a movable_object,
t is type door or movable_object
STATUS(d, s)
where d is type door,
s is an enumerated type: OPEN or CLOSED
CONNECTS(d, rx, ry) where d is type door,
rx, ry are the room
With the above predicates, the world model for the initial state of the world
in Fig. 2.4 would be represented by:
initial state:
INROOM(IT, R1)
INROOM(B1,R2)
CONNECTS(D1, R1, R2)
CONNECTS(D1, R2, R1)
STATUS(D1,OPEN)

48
2
The Hierarchical Paradigm
Figure 2.4
An example for Strips of two rooms joined by an open door.
This world model captures that a speciﬁc movable_object named IT
is in a room named R1, and B1 is in another room labeled R2. A door D1
connects a room named R1 to R2 and it connects R2 to R1. (Two different
CONNECTS predicates are used to represent that a robot can go through the
door from either door.) A door called D1 has the enumerated value of being
OPEN. The NEXTTO predicate wasn’t used, because it wasn’t true and there
would be nothing to bind the variables to.
Under this style of representation, the world model for the goal state
would be:
goal state:
INROOM(IT,R2)
INROOM(B1,R2)
CONNECTS(D1, R1, R2)
CONNECTS(D1, R2, R1)
STATUS(D1,OPEN)
Once the world model is established, it is possible to construct the differ-
CONSTRUCTING THE
DIFFERENCE TABLE
ence table. The partial difference table is:

2.2
Attributes of the Hierarchical Paradigm
49
operator
preconditions
add-list
delete-list
OP1:
INROOM(IT,rk)
NEXTTO(IT,dx)
GOTODOOR(IT,dx)
CONNECT(dx,rk,rm)
OP2:
CONNECT(dx,rk,rm)
INROOM(IT,rm)
INROOM(IT,rk)
GOTHRUDOOR(IT,dx)
NEXTTO(IT,dx)
STATUS(dx, OPEN)
INROOM(IT,rk)
This difference table says the robot is programmed for only two opera-
tions: go to a door, and go through a door. The GOTODOOR operator can be
applied only if the following two preconditions are true:
 INROOM(IT, rk) The robot is in a room, which will be assigned to the
identiﬁer rk.
 CONNECT(dx, rk, rm) There is a door, which will be assigned to the
identiﬁer dx, which connects rk to some other room called rm.
The label IT is used to constrain the predicates. Notice that only the vari-
ables dx and rk get bound when GOTODOOR is called. rm can be anything.
If GOTODOOR is executed, the robot is now next to the door called dx. Noth-
ing gets deleted from the world state because the robot is still in room rk,
the door dx still connects the two rooms rk and rm. The only thing that has
changed is that the robot is now in a noteworthy position in the room: next
to the door.
The difference table speciﬁes that the GOTHRUDOOR operator will only work
if the robot is in the room next to the door, the door is open, and the door con-
nects the room the robot is in to another room. In this case, predicates must
be added and deleted from the world model when the operator executes.
When the robot is in room rk and goes through the door, it is now in room
rm (which must be added to the world model) and is no longer in room rk
(which must be deleted).
So far, the world model and difference table should seem reasonable, al-
though tedious to construct. But constructing a difference table is pointless
without an evaluation function for differences. (Notice that there wasn’t a
column for the difference in the above table.) The difference evaluator in
the travel example was Euclidean distance. In this example, the evaluator is
predicate calculus, where the initial state is logically subtracted from
the goal state. The logical difference between the initial state goal
state is simply:
:INROOM(IT, R2) or INROOM(IT, R2)=FALSE

50
2
The Hierarchical Paradigm
Reducing differences is a bit like a jigsaw puzzle where Strips tries differ-
ent substitutions to see if a particular operator will reduce the difference. In
order to reduce the difference, Strips looks in the difference table, starting at
REDUCING
DIFFERENCES
the top, under the add-list column for a match. It looks in the add-list rather
than a separate differences column because the add-list expresses what the
result of the operator is. If Strips ﬁnds an operator that produces the goal
state, then that operator eliminates the existing difference between the initial
and goal states.
The add-list in OP2:
GOTHRUDOOR has a match on form. If rm=R2, then
the result of OP2 would be INROOM(IT, R2). This would eliminate the
difference, so OP2 is a candidate operator.
Before the OP2 can be applied, Strips must check the preconditions. To do
this, rm must be replaced with R2 in every predicate in the preconditions.
OP2 has two preconditions, only CONNECTS(dx, rk, rm) is affected. It
becomes CONNECTS(dx, rk, R2). Until dx and rk are bound, the pred-
icate doesn’t have a true or false value. Essentially dx, rk are wildcards,
CONNECTS(*, *, R2). To ﬁll in values for these variables, Strips looks at
the current state of the world model to ﬁnd a match. The predicate in the cur-
rent state of the world CONNECTS(D1, R1, R2) matches CONNECTS(*,
*, R2). D1 is now bound to dx and R1 is bound to rk.
Now Strips propagates the bindings to the next precondition on the list:
NEXTTO(IT, dx). NEXTTO(IT, D1) is FALSE because the predicate is
not in the current world state. NEXTTO(IT, D1) is referred to as a failed
FAILED
PRECONDITIONS
precondition. An informal interpretation is that GOTHRUDOOR(IT, D1) will
get the robot to the goal state, but before it can do that, IT has to be next to
D1.
Rather than give up, STRIPS recurses (uses the programming technique of
RECURSION TO
RESOLVE DIFFERENCES
recursion) to repeat the entire procedure. It marks the original goal state as
G0, pushes it on a stack, then it creates a new sub-goal state, G1.
The difference between NEXTTO(IT, D1) and the current world state is:
:NEXTTO(IT, D1)
Strips once again searches through the add-list in the difference table to
ﬁnd an operator that would negate this. Indeed, OP1:
GOTODOOR(IT,
dx) has a match in the add-list of NEXTTO(IT, dx). Strips has to start over
with reassigning values to the identiﬁers because the program has entered a
new programming scope, so dx=D1.

2.2
Attributes of the Hierarchical Paradigm
51
Again, Strips examines the preconditions. This time rk=R1 and rm=R2
can be matched with CONNECTS(dx, rk, rm), and all preconditions are
satisﬁed (that is, they evaluate to true). Strips puts the operator OP1 on the
plan stack and applies the operator to the world model, changing the state.
(Note that this is the equivalent of a “mental operation”; the robot doesn’t
actually physically go to the door, it just changes the state to imagine what
would happen if it did.)
To recall, the initial state of the world model was:
initial state:
INROOM(IT, R1)
INROOM(B1,R2)
CONNECTS(D1, R1, R2)
CONNECTS(D1, R2, R1)
STATUS(D1,OPEN)
Applying the operator OP1 means making the changes on the add-list and
delete-list. There is only a predicate on the add-list and none on the delete-
list. After adding NEXTTO(IT, D1), the state of the world is:
state after OP1:
INROOM(IT, R1)
INROOM(B1,R2)
CONNECTS(D1, R1, R2)
CONNECTS(D1, R2, R1)
STATUS(D1,OPEN)
NEXTTO(IT, D1)
Strips then returns control back to the previous call. It resumes where it
left off in evaluating the preconditions for OP2 with dx=D1, rm=R2 and
rk=R1, only now the world model has changed. Both STATUS(D1, OPEN)
and INROOM(IT, R1) are true, so all the preconditions for OP2 are satisﬁed.
Strips puts OP2 on its plan stack and changes the world model by applying
the add-list and delete-list predicates. This results in what the state of the
world will be when the plan executes:
state after OP2:
INROOM(IT, R2)
INROOM(B1,R2)

52
2
The Hierarchical Paradigm
CONNECTS(D1, R1, R2)
CONNECTS(D1, R2, R1)
STATUS(D1,OPEN)
NEXTTO(IT, D1)
Strips exits and the plan for the robot to physically execute (in reverse
order on the stack) is: GOTODOOR(IT, D1), GOTHRUDOOR(IT, D1).
2.2.3
Strips summary
Strips works recursively; if it can’t reach a goal directly, it identiﬁes the prob-
lem (a failed precondition), then makes the failed precondition a subgoal.
Once the subgoal is reached, Strips puts the operator for reaching the sub-
goal on a list, then backs up (pops the stack) and resumes trying to reach the
previous goal. Strips plans rather than execute: it creates a list of operators
to apply; it does not apply the operator as it goes. Strips implementations
requires the designer to set up a:
 world model representation
 difference table with operators, preconditions, add, and delete lists
 difference evaluator
The steps in executing Strips are:
1. Compute the difference between the goal state and the initial state using
the difference evaluation function. If there is no difference, terminate.
2. If there is a difference, reduce the difference by selecting the ﬁrst operator
from the difference table whose add-list has a predicate which negates the
difference.
3. Next, examine the preconditions to see if a set of bindings for the variables
can be obtained which are all true. If not, take the ﬁrst false precondition,
make it the new goal and store the original goal by pushing it on a stack.
Recursively reduce that difference by repeating step 2 and 3.
4. When all preconditions for an operator match, push the operator onto
the plan stack and update a copy of the world model. Then return to
the operator with the failed precondition so it can apply its operator or
recurse on another failed precondition.

2.3
Closed World Assumption and the Frame Problem
53
2.3
Closed World Assumption and the Frame Problem
Strips sensitized the robotics community to two pervasive issues: the closed
CLOSED WORLD
ASSUMPTION
world assumption and the frame problem. As deﬁned earlier, the closed world
FRAME PROBLEM
assumption says that the world model contains everything the robot needs to
know: there can be no surprises. If the closed world assumption is violated,
the robot may not be able to function correctly. But, on the other hand, it is
very easy to forget to put all the necessary details into the world model. As a
result, the success of the robot depends on how well the human programmer
can think of everything.
But even assuming that the programmer did come up with all the cases,
the resulting world model is likely to be huge. Consider how big and cum-
bersome the world model was just for moving between 2 rooms. And there
were no obstacles! People began to realize that the number of facts (or ax-
ioms) that the program would have to sort through for each pass through
the difference table was going to become intractable for any realistic appli-
cation. The problem of representing a real-world situation in a way that was
computationally tractable became known as the frame problem. The oppo-
site of the closed world assumption is known as the open world assumption.
OPEN WORLD
ASSUMPTION
When roboticists say that “a robot must function in the open world,” they
are saying the closed world assumption cannot be applied to that particular
domain.
The above example, although trivial, shows how tedious Strips is (though
computers are good at tedious algorithms). In particular, the need to for-
mally represent the world and then maintain every change about it is non-
intuitive.
It also illustrates the advantage of a closed-world assumption:
imagine how difﬁcult it would be to modify the planning algorithm if the
world model could suddenly change. The algorithm could get lost between
recursions. The example should also bring home the meaning of the frame
problem: imagine what happens to the size of the world model if a third
room is added with boxes for the robot to move to and pick up! And this is
only for a world of rooms and boxes. Clearly the axioms which frame the
world will become too numerous for any realistic domain.
One early solution was ABStrips which tried to divide the problem into
multiple layers of abstraction, i.e., solve the problem on a coarse level ﬁrst.
That had its drawbacks, and soon many people who had started out in ro-
botics found themselves working on an area of AI called planning.
The
two ﬁelds became distinct, and by the 1980’s, the planning and robotics re-
searchers had separate conferences and publications. Many roboticists dur-

54
2
The Hierarchical Paradigm
ing the 1970’s and 1980’s worked on either computer vision related issues,
trying to get the robots to be able to better sense the world, or on path plan-
ning, computing the most efﬁcient route around obstacles, etc. to a goal lo-
cation.
2.4
Representative Architectures
As mentioned in Part I an architecture is a method of implementing a para-
digm, of embodying the principles in some concrete way. Ideally, an archi-
tecture is generic; like a good object-oriented program design, it should have
many reusable pieces for other robot platforms and tasks.
Possibly the two best known architectures of the Hierarchical period are
the Nested Hierarchical Controller (NHC) developed by Meystel93 and the
NIST Realtime Control System (RCS) originally developed by Albus,1 with
its teleoperation version for JPL called NASREM.
2.4.1
Nested Hierarchical Controller
As shown in Fig. 2.5, the Nested Hierarchical Controller architecture has
components that are easily identiﬁed as either SENSE, PLAN, or ACT. The
robot begins by gathering observations from its sensors and combining those
observations to form the World Model data structure through the SENSE
activity. The World Model may also contain a priori knowledge about the
world, for example, maps of a building, rules saying to stay away from the
foyer during the start and ﬁnish of business hours, etc. After the World
Model has been created or updated, then the robot can PLAN what actions
it should take. Planning for navigation has a local procedure consisting of
three steps executed by the Mission Planner, Navigator, and Pilot. Each of
these modules has access to the World Model in order to compute their por-
tion of planning. The last step in planning is for the Pilot module to generate
speciﬁc actions for the robot to do (e.g., Turn left, turn right, move straight at
a velocity of 0.6 meters per second). These actions are translated into actua-
tor control signals (e.g., Velocity proﬁle for a smooth turn) by the Low-Level
Controller. Together, the Low-Level Controller and actuators form the ACT
portion of the architecture.
The major contribution of NHC was its clever decomposition of planning
into 3 different functions or subsystems aimed at supporting navigation: the
Mission Planner, the Navigator, and the Pilot. As shown in Fig. 2.6, the Mis-
MISSION PLANNER
NAVIGATOR
PILOT
sion Planner either receives a mission from a human or generates a mission

2.4
Representative Architectures
55
Mission
Planner
Navigator
Pilot
Low-level
Controller
World 
Model/
Knowledge
Base
drive
steer
sensors
sensors
sensors
SENSE
PLAN
ACT
Figure 2.5
Nested Hierarchical Controller.
for itself, for example: pick up the box in the next room. The Mission Plan-
ner is responsible for operationalizing, or translating, this mission into terms
that other functions can understand: box=B1; rm=ROOM311. The Mission
Planner then accesses a map of the building and locates where the robot is
and where the goal is. The Navigator takes this information and generates a
path from the current location to the goal. It generates a set of waypoints, or
straight lines for the robot to follow. The path is passed to the Pilot. The Pilot
takes the ﬁrst straight line or path segment and determines what actions the
robot has to do to follow the path segment. For instance, the robot may need
to turn around to face the way point before it can start driving forward.
What happens if the Pilot gives directions for a long path segment (say 50
meters) or if a person suddenly walks in front of the robot? Unlike Shakey,
under NHC, the robot is not necessarily walking around with its eyes closed.
After the Pilot gives the Low-Level Controller commands and the controller

56
2
The Hierarchical Paradigm
Mission
Planner
Navigator
Pilot
goal
path
path subsegment:
Turn 83 degrees
move 5 meters
5
6
10
Figure 2.6
Examination of planning components in the NHC architecture.
sends actuator signals, the robot polls its sensors again. The World Model
is updated. However, the entire planning cycle does not repeat. Since the
robot has a plan, it doesn’t need to rerun the Mission Planner or the Naviga-
tor. Instead, the Pilot checks the World Model to see if the robot has drifted
off the path subsegment (in which case it generates a new control signal),
has reached the waypoint, or if an obstacle has appeared. If the robot has
reached its waypoint, the Pilot informs the Navigator. If the waypoint isn’t
the goal, then there is another path subsegment for the robot to follow, and
so the Navigator passes the new subsegment to the Pilot. If the waypoint is
the goal, then the Navigator informs the Mission Planner that the robot has
reached the goal. The Mission Planner may then issue a new goal, e.g., Re-
turn to the starting place. If the robot has encountered an obstacle to its path,
the Pilot passes control back to the Navigator. The Navigator must compute
a new path, and subsegments, based on the updated World Model. Then it
gives the updated path subsegment to the Pilot to carry out.

2.4
Representative Architectures
57
NHC has several advantages. It differs from Strips in that it interleaves
planning and acting. The robot comes up with a plan, starts executing it,
then changes it if the world is different than it expected. Notice that the de-
composition is inherently hierarchical in intelligence and scope. The Mission
Planner is “smarter” than the Navigator, who is smarter than the Pilot. The
Mission Planner is responsible for a higher level of abstraction then the Nav-
igator, etc. We will see that other architectures, both in the Hierarchical and
Hybrid paradigms, will make use of the NHC organization.
One disadvantage of the NHC decomposition of the planning function is
that it is appropriate only for navigation tasks. The division of responsibili-
ties seems less helpful, or clear, for tasks such as picking up a box, rather than
just moving over to it. The role of a Pilot in controlling end-effectors is not
clear. At the time of its initial development, NHC was never implemented
and tested on a real mobile robot; hardware costs during the Hierarchical
period forced most roboticists to work in simulation.
2.4.2
NIST RCS
Jim Albus at the National Bureau of Standards (later renamed the National
Institute of Standards and Technology or NIST) anticipated the need for intel-
ligent industrial manipulators, even as engineering and AI researchers were
splitting into two groups. He saw that one of the major obstacles in apply-
ing AI to manufacturing robots was that there were no common terms, no
common set of design standards. This made industry and equipment man-
ufacturers leery of AI, for fear of buying an expensive robot that would not
be compatible with robots purchased in the future. He developed a very de-
tailed architecture called the Real-time Control System (RCS) Architecture to
serve as a guide for manufacturers who wanted to add more intelligence to
their robots. RCS used NHC in its design, as shown in Fig. 2.7.
SENSE activities are grouped into a set of modules under the heading
of sensory perception. The output of the sensors is passed off to the world
modeling module which constructs a global map using information in its
associated knowledge database about the sensors and any domain know-
ledge (e.g., the robot is operating underwater). This organization is similar
to NHC. The main difference is that the sensory perception module intro-
duces a useful preprocessing step between the sensor and the fusion into a
world model. As will be seen in Ch. 6, sensor preprocessing is often referred
to as feature extraction.

58
2
The Hierarchical Paradigm
SENSE
MODEL
ACT
SENSE
PLAN
ACT
Sample Activities
Mission Planner
Navigator
Pilot
a.
Sensory
Perception
World
Modeling
Behavior
Generation
Knowledge
Database
Value
Judgment
changes
and
events
observed
input
perception,
focus of
attention
plans,
state of
actions
commanded
actions
task
goals
simulated
plans
SENSE
PLAN
ACT
b.
Figure 2.7
Layout of RCS: a.) hierarchical layering of sense-model-act, and b.) func-
tional decomposition.

2.4
Representative Architectures
59
The Value Judgment module provides most of the functionality associated
with the PLAN activity: it plans, then simulates the plans to ensure they will
work. Then, as with Shakey, the Planner hands off the plan to another mod-
ule, Behavior Generation, which converts the plans into actions that the robot
can actually perform (ACT). Notice that the Behavior Generation module is
similar to the Pilot in NHC, but there appears to be less focus on navigation
tasks. The term “behavior” will be used by Reactive and Hybrid Delibera-
tive/Reactive architectures. (This use of “behavior” in RCS is a bit of retroﬁt,
as Albus and his colleagues at NIST have attempted to incorporate new ad-
vances. The integration of all sensing into a global world model for planning
and acting keeps RCS a Hierarchical architecture.) There is another module,
operator interface, which is not shown which allows a human to “observe”
and debug what a program constructed with the architecture is doing.
The standard was adapted by many government agencies, such as NASA
and the US Bureau of Mines, who were contracting with universities and
companies to build robot prototypes. RCS serves as a blueprint for saying:
“here’s the types of sensors I want, and they’ll be fused by this module into a
global map, etc.” The architecture was considered too detailed and restrictive
when it was initially developed by most AI researchers, who continued de-
velopment of new architectures and paradigms on their own. Fig. 2.8 shows
three of the diverse mobile robots that have used RCS.
A close inspection of the NHC and RCS architectures suggests that they
are well suited for semi-autonomous control. The human operator could
provide the world model (via eyes and brain), decide the mission, decom-
pose it into a plan, and then into actions. The lower level controller (robot)
would carry out the actions. As robotics advanced, the robot could replace
more functions and “move up” the autonomy hierarchy. For example, tak-
ing over the pilot’s responsibilities; the human could instruct the robot to
stay on the road until the ﬁrst left turn. As AI advanced, the human would
only have to serve as the Mission Planner: “go to the White House.” And so
on. Albus noted this and worked with JPL to develop a version of RCS for
teleoperating a robot arm in space. This is called the NASREM architecture
NASREM
and is still in use today.
2.4.3
Evaluation of hierarchical architectures
Recall from Part I that there are four criteria for evaluating an architecture:
support for modularity, niche targetability, ease of portability to other do-
mains, and robustness. NHC and RCS both provide some guidelines in how

60
2
The Hierarchical Paradigm
a.
b.
c.
Figure 2.8
Three of the diverse mobile robots that have used RCS: a.) a commercial
ﬂoor cleaning robot, b.) a mining robot, and c.) a submersible or underwater robot.
(Photographs courtesy of the National Institute of Standards and Technology.)
to decompose a robot program into intuitive modules. The NHC decomposi-
tion of mission planner, navigator, and pilot was focused strictly on naviga-
tion, while RCS appears broader. Both have been used recently for successful
vehicle guidance, with RCS being used to control mining equipment, sub-
marines, and cars. So both have reasonable niche targetability. The ease of
portability to other domains is unclear. The architectures are expressed at a
very broad level, akin to “a house should have bedrooms, bathrooms, and a
kitchen.” Architectures which are more speciﬁc, “there should be one bath-
room for every two bedrooms,” and which have associated techniques sim-
plify portability. It is hard to see how the code written for a mining machine

2.5
Advantages and Disadvantages
61
could be reused for a submarine, especially since RCS is not object-oriented.
In terms of robustness, RCS does attempt to provide some explicit mech-
anisms. In particular, it assumes the Value Judgment module simulates a
plan to conﬁrm that it should be successful when deployed. The use of sim-
ulation is common for operating equipment in a well-known environment
where every piece of equipment is known. The most notable example is a
nuclear processing cell. With such detailed information, it is fairly straight-
forward (although computationally expensive) to simulate whether a par-
ticular course for a robot would collide with equipment and cause a spill.
This is a very limited form of robustness. The disadvantage is the time delay
caused by the robot mentally rehearsing its actions prior to executing them.
Simulation may not be appropriate for all actions; if a piece of the ceiling is
falling on the robot, it needs to get out of the way immediately or risk coming
up with the best place to move too late to avoid being crushed.
2.5
Advantages and Disadvantages
Robots built in the time period before 1990 typically had a Hierarchical style
of software organization. They were generally developed for a speciﬁc ap-
plication rather than to serve as a generic architecture for future applications.
The robots are interesting because they illustrate the diversity and scope of
applications being considered for mobile robots as far back as 15 or 20 years
ago.
The primary advantage of the Hierarchical Paradigm was that it provides
an ordering of the relationship between sensing, planning, and acting. The
primary disadvantage was planning. Every update cycle, the robot had to
update a global world model and then do some type of planning. The sens-
ing and planning algorithms of the day were extremely slow (and many still
are), so this introduced a signiﬁcant bottleneck. Notice also that sensing and
acting are always disconnected. This effectively eliminated any stimulus-
response types of actions (“a rock is crashing down on me, I should move
anywhere”) that are seen in nature.
The dependence on a global world model is related to the frame problem.
In Strips, in order to do something as simple as opening a door, the robot had
to reason over all sorts of details that were irrelevant (like other rooms, other
doors). NHC and RCS represent attempts to divide up the world model into
pieces best suited for the type of actions; for example, consider the roles of
the Mission Planner, Navigator, and Pilot. Unfortunately, these decomposi-

62
2
The Hierarchical Paradigm
tions appear to be dependent on a particular application. As a result, robotics
gained a reputation as being more of an art than a science.
Another issue that was never really handled by architectures in the Hier-
archical Paradigm was uncertainty. Uncertainty comes in many forms, such
as semantic (how close does NEXTTO mean anyway?), sensor noise, and ac-
tuator errors. Another important aspect of uncertainty is action completion:
did the robot actually accomplish the action? One robotics researcher said
that their manipulator was only able to pick up a cup 60% of the attempts;
therefore they had to write a program to check to see if it was holding a cup
and then restart the action if it wasn’t. Because Shakey essentially closed its
eyes during planning and acting, it was vulnerable to uncertainty in action
completion.
2.6
Programming Considerations
It is interesting to note that the use of predicate logic and recursion by Strips
favors languages like Lisp and PROLOG. These languages were developed
by AI researchers speciﬁcally for expressing logical operations. These lan-
guages do not necessarily have good real-time control properties like C or
C++. However, during the 1960’s the dominant scientiﬁc and engineering
language was FORTRAN IV which did not support recursion. Therefore, re-
searchers in AI robotics often chose the lesser of two evils and programmed
in Lisp. The use of special AI languages for robotics may have aided the split
between the engineering and AI approaches to robotics, as well as slowed
down the infusion of ideas from the the two communities. It certainly dis-
couraged non-AI researchers from becoming involved in AI robotics.
The Hierarchical Paradigm tends to encourage monolithic programming,
rather than object-oriented styles. Although the NHC decomposes the plan-
ning portion of intelligence, the decomposition is strictly functional. In par-
ticular, NHC and RCS don’t provide much guidance on how to build modu-
lar, reusable components.
2.7
Summary
The Hierarchical Paradigm uses a SENSE then PLAN then ACT (S,P,A). It
organizes sensing into a global data structure usually called a world model
that may have an associated knowledge base to contain a priori maps or
knowledge relevant to a task. Global data structures often ﬂag that an ar-

2.8
Exercises
63
chitecture will suffer from the frame problem. The Hierarchical Paradigm
was introduced in the ﬁrst mobile robot, Shakey. Strips is an important plan-
ning technique that came out of the Shakey project at SRI, which focused
on the PLAN primitive in robotics. Concepts and terms which emerged that
continue to play an important role in deﬁning robotics are: preconditions,
the closed and open world assumptions, and the frame problem. Hierar-
chical systems have largely fallen out of favor except for the NIST Realtime
Control Architecture. The decline in popularity is due in part to its focus on
strong niche targetability at the expense of true modularity and portability.
However, as will be seen in the following chapters, insights from biology and
cognitive science have led to paradigms with more intuitive appeal. One of-
ten overlooked property of most hierarchical architectures is that they tend to
support the evolution of intelligence from semi-autonomous control to fully
autonomous control.
2.8
Exercises
Exercise 2.1
Describe the Hierarchical Paradigm in terms of a) the SENSE, PLAN, ACT primitives
and b) sensing organization.
Exercise 2.2
Deﬁne the following terms in one or two sentences. Give an example of how each
arises in a robotic system:
a. precondition
b. closed world assumption
c. open world
d. frame problem
Exercise 2.3
Consider the frame problem. Suppose the World Model for a Strips-based robot con-
sisted of 100 facts. Each fact requires 1KB of memory storage. Every time a new object
is added to the world model, it increases the model by 100 (a linear increase). One
object, 100 facts, 100KB of storage; two objects, 200 facts, 200KB. How many objects
would ﬁll 64KB of memory?
Exercise 2.4
Redo the above exercise where the number of facts in the world model doubles every
time a new object is added (exponential). One object, 100 facts, 1KB, two objects,
200 facts, 200KB, three objects, 400 facts, 400KB. Which is a more reasonable rate to
assume the world model would increase at, linear or exponential?

64
2
The Hierarchical Paradigm
Exercise 2.5
Describe the mission planner, navigator, pilot organization of the Nested Hierarchical
Controller. Write down how it would handle the problem in Sec. 2.2.2.
Exercise 2.6
List 2 advantages and disadvantages of the Hierarchical Paradigm.
Exercise 2.7
Solve the following navigation problem using Strips. Return to the world in Sec 2.2.2.
The robot will move to the box B1 and pick it up.
a. Add a new operator pickup to the difference table.
b. Use the world model, difference table, difference evaluator to construct a plan.
Failed preconditions and new subgoals should be shown after each step.
c. Show the changes in the world model after each operator is applied.
Exercise 2.8
Name and evaluate one representative Hierarchical architecture in terms of: support
for modularity, niche targetability, ease of portability to other domains, robustness.
Exercise 2.9
[World Wide Web]
Search the web for interactive versions of Strips and experiment with them.
2.9
End Notes
A robot that did take a cross-country trip.
Robot vehicles do in fact need special authorization to drive on public roads. In
1996, the Carnegie Mellon University Navlab vehicle project led by Dean Pomerleau
steered itself (the driver handled the gas pedal and brakes) over 90% of the way across
the USA from Washington, DC, to Los Angeles in the “No Hands Across America”
trip. The Navlab (a modiﬁed Saturn station wagon) was reportedly pulled over by
the Kansas State Highway Patrol for driving an experimental vehicle without permis-
sion. The entire trip was placed in jeopardy, but eventually the Navlab was allowed
to continue, and the team and vehicle appeared on the David Letterman show in
Los Angeles. As impressive as the Carnegie Mellon feat was, a group of German
researchers under the direction of Ernst Dickmanns and Volker Graefe have been
ﬁelding even more advanced autonomous highway driving vehicles since 1988.
Shakey.
It can be debated whether Shakey is really the ﬁrst mobile robot. There was a tortoise
built by Grey Walter, but this was never really on the main branch of AI research. See
Behavior-Based Robots 10 for details.

2.9
End Notes
65
Robot name trivia.
Regardless of how Shakey got its name, SRI continued the tradition with Shakey’s
successor being called Flakey, followed by Flakey’s successor, Erratic.
Strips.
The description of Strips and the robot examples are adapted from The Handbook of
Artiﬁcial Intelligence, A. Barr and E. Feigenbaum, editors, vol. 1, William Kaufmann,
Inc., Los Altos, CA, 1981.
Jim Albus.
Jim Albus is one of the statesmen of robotics. Although his RCS architecture is a rep-
resentative of the Hierarchical Paradigm, Albus was heavily by cognitive studies—so
much so that he wrote Brains, Behaviors and Robots (1981). What’s odd is that ex-
ploiting biological and cognitive studies is commonly associated with the Reactive
Paradigm. The RCS architecture was never meant to be totally static, and in recent
years, it has begun to resemble what will be referred to as a model-oriented style of
the Hybrid Paradigm.


3
Biological Foundations of the
Reactive Paradigm
Chapter Objectives:
 Describe the three levels in a computational theory.
 Explain in one or two sentences each of the following terms: reﬂexes, taxes,
ﬁxed-action patterns, schema, affordance.
 Be able to write pseudo-code of an animal’s behaviors in terms of innate
releasing mechanisms, identifying the releasers for the behavior.
 Given a description of an animal’s sensing abilities, its task, and environ-
ment, identify an affordance for each behavior.
 Given a description of an animal’s sensing abilities, its task, and environ-
ment, deﬁne a set of behaviors using schema theory to accomplish the
task.
3.1
Overview
Progress in robotics in the 1970’s was slow. The most inﬂuential robot was
the Stanford Cart developed by Hans Moravec, which used stereo vision to
see and avoid obstacles protruding from the ground. In the late 1970’s and
early 80’s, Michael Arbib began to investigate models of animal intelligence
from the biological and cognitive sciences in the hopes of gaining insight into
what was missing in robotics. While many roboticists had been fascinated
by animals, and many, including Moravec, had used some artifact of animal
behavior for motivation, no one approached the ﬁeld with the seriousness
and dedication of Arbib.

68
3
Biological Foundations of the Reactive Paradigm
At nearly the same time, a slender volume by Valentino Braitenberg, called
Vehicles: Experiments in Synthetic Psychology,25 appeared. It was a series of
gedanken or entirely hypothetical thought experiments, speculating as to
how machine intelligence could evolve.
Braitenberg started with simple
thermal sensor-motor actuator pair (Vehicle 1) that could move faster in warm
areas and slower in cold areas. The next, more complex vehicle had two ther-
mal sensor-motor pairs, one on each side of the vehicle. As a result of the
differential drive effect, Vehicle 2 could turn around to go back to cold areas.
Throughout the book, each vehicle added more complexity. This layering
was intuitive and seemed to mimic the principles of evolution in primates.
Vehicles became a cult tract among roboticists, especially in Europe.
Soon a new generation of AI researchers answered the siren’s call of bio-
logical intelligence. They began exploring the biological sciences in search
of new organizing principles and methods of obtaining intelligence. As will
be seen in the next chapter, this would lead to the Reactive Paradigm. This
chapter attempts to set the stage for the Reactive Paradigm by recapping in-
ﬂuential studies and discoveries, and attempting to cast them in light of how
they can contribute to robotic intelligence.
The chapter ﬁrst covers animal behaviors as the fundamental primitive
for sensing and acting. Next, it covers the work of Lorenz and Tinbergen in
deﬁning how concurrent simple animal behaviors interact to produce com-
plex behaviors through Innate Releasing Mechanisms (IRMs). A key aspect
of an animal behavior is that perception is needed to support the behav-
ior. The previous chapter on the Hierarchical Paradigm showed how early
roboticists attempted to fuse all sensing into a global world map, supple-
mented with a knowledge base. This chapter covers how the work of cogni-
tive psychologists Ulrich Neisser109 and J.J. Gibson59 provides a foundation
for thinking about robotic perception. Gibson refuted the necessity of global
world models, a direct contradiction to the way perception was handled in
the hierarchical paradigm. Gibson’s use of affordances, also called direct per-
ception, is an important key to the success of the Reactive Paradigm. Later
work by Neisser attempts to deﬁne when global models are appropriate and
when an affordance is more elegant.
Many readers ﬁnd the coverage on ethology (study of animal behavior) and
ETHOLOGY
cognitive psychology (study of how humans think and represent knowledge)
COGNITIVE
PSYCHOLOGY
to be interesting, but too remote from robotics. In order to address this con-
cern, the chapter discusses speciﬁc principles and how they can be applied
to robotics. It also raises issues in transferring animal models of behavior
to robots. Finally, the chapter covers schema theory, an attempt in cognitive

3.1
Overview
69
psychology to formalize aspects of behavior. Schema theory has been used
successfully by Arbib to represent both animal and robot behavior. It is im-
plicitly object-oriented and so will serve as the foundation of discussions
through out the remainder of this book.
3.1.1
Why explore the biological sciences?
Why should roboticists explore biology, ethology, cognitive psychology and
other biological sciences? There is a tendency for people to argue against
considering biological intelligence with the analogy that airplanes don’t ﬂap
their wings. The counter-argument to that statement is that almost every-
thing else about a plane’s aerodynamics duplicates a bird’s wing: almost all
the movable surfaces on the wing of a plane perform the same functions as
parts of a bird’s wing. The advances in aeronautics came as the Wright Broth-
ers and others extracted aerodynamic principles. Once the principles of ﬂight
were established, mechanical systems could be designed which adhered to
these principles and performed the same functions but not necessarily in the
same way as biological systems. The “planes don’t ﬂap their wings” argu-
ment turns out to be even less convincing for computer systems: animals
make use of innate capabilities, robots rely on compiled programs.
Many AI roboticists often turn to the biological sciences for a variety of
reasons. Animals and man provide existence proofs of different aspects of
intelligence. It often helps a researcher to know that an animal can do a
particular task, even if it isn’t known how the animal does it, because the
researcher at least knows it is possible. For example, the issue of how to
combine information from multiple sensors (sensor fusion) has been an open
question for years. At one point, papers were being published that robots
shouldn’t even try to do sensor fusion, on the grounds that sensor fusion was
a phenomenon that sounded reasonable but had no basis in fact. Additional
research showed that animals (including man) do perform sensor fusion, al-
though with surprisingly different mechanisms than most researchers had
considered.
The principles of animal intelligence are extremely important to roboti-
cists. Animals live in an open world, and roboticists would like to overcome
OPEN WORLD
ASSUMPTION
the closed world assumption that presented so many problems with Shakey.
CLOSED WORLD
ASSUMPTION
Many “simple” animals such as insects, ﬁsh, and frogs exhibit intelligent be-
havior yet have virtually no brain. Therefore, they must be doing something
that avoids the frame problem.
FRAME PROBLEM

70
3
Biological Foundations of the Reactive Paradigm
3.1.2
Agency and computational theory
Even though it seems reasonable to explore biological and cognitive sciences
for insights in intelligence, how can we compare such different systems: car-
bon and silicon “life” forms? One powerful means of conceptualizing the
different systems is to think of an abstract intelligent system. Consider some-
thing we’ll call an agent. The agent is self-contained and independent. It has
AGENT
its own “brains” and can interact with the world to make changes or to sense
what is happening. It has self-awareness. Under this deﬁnition, a person is
an agent. Likewise, a dog or a cat or a frog is an agent. More importantly,
an intelligent robot would be an agent, even certain kinds of web search en-
gines which continue to look for new items of interest to appear, even after
the user has logged off. Agency is a concept in artiﬁcial intelligence that al-
lows researchers to discuss the properties of intelligence without discussing
the details of how the intelligence got in the particular agent. In OOP terms,
“agent” is the superclass and the classes of “person” and “robot” are derived
from it.
Of course, just referring to animals, robots, and intelligent software pack-
ages as “agents” doesn’t make the correspondences between intelligence any
clearer. One helpful way of seeing correspondences is to decide the level at
which these entities have something in common. The set of levels of com-
monality lead to what is often called a computational theory88 after David
COMPUTATIONAL
THEORY
Marr. Marr was a neurophysiologist who tried to recast biological vision
processes into new techniques for computer vision. The levels in a computa-
tional theory can be greatly simpliﬁed as:
Level 1: Existence proof of what can/should be done. Suppose a roboticist
is interested in building a robot to search for survivors trapped in a build-
ing after an earthquake. The roboticist might consider animals which seek
out humans. As anyone who has been camping knows, mosquitoes are
very good at ﬁnding people. Mosquitoes provide an existence proof that
it is possible for a computationally simple agent to ﬁnd a human being
using heat. At Level 1, agents can share a commonality of purpose or
functionality.
Level 2: Decomposition of “what” into inputs, outputs, and transforma-
tions.
This level can be thought of as creating a ﬂow chart of “black
boxes.” Each box represents a transformation of an input into an output.
Returning to the example of a mosquito, the roboticist might realize from
biology that the mosquito ﬁnds humans by homing on the heat of a hu-

3.1
Overview
71
man (or any warm blooded animal). If the mosquito senses a hot area, it
ﬂies toward it. The roboticist can model this process as: input=thermal
image, output=steering command. The “black box” is how the mos-
quito transforms the input into the output. One good guess might be
to take the centroid of the thermal image (the centroid weighted by the
heat in each area of the image) and steer to that. If the hot patch moves,
the thermal image will change with the next sensory update, and a new
steering command will be generated. This might not be exactly how the
mosquito actually steers, but it presents an idea of how a robot could
duplicate the functionality. Also notice that by focusing on the process
rather than the implementation, a roboticist doesn’t have to worry about
mosquitoes ﬂying, while a search and rescue robot might have wheels. At
Level 2, agents can exhibit common processes.
Level 3: How to implement the process. This level of the computational the-
ory focuses on describing how each transformation, or black box, is imple-
mented. For example, in a mosquito, the steering commands might be im-
plemented with a special type of neural network, while in a robot, it might
be implemented with an algorithm which computes the angle between the
centroid of heat and where the robot is currently pointing. Likewise, a re-
searcher interested in thermal sensing might examine the mosquito to see
how it is able to detect temperature differences in such a small package;
electro-mechanical thermal sensors weigh close to a pound! At Level 3,
agents may have little or no commonality in their implementation.
It should be clear that Levels 1 and 2 are abstract enough to apply to any
agent. It is only at Level 3 that the differences between a robotic agent and
a biological agent really emerge. Some roboticists actively attempt to em-
ulate biology, reproducing the physiology and neural mechanisms. (Most
roboticists are familiar with biology and ethology, but don’t try to make exact
duplicates of nature.) Fig. 3.1 shows work at Case Western Reserve’s Bio-Bot
Laboratory under the direction of Roger Quinn, reproducing a cockroach on
a neural level.
In general, it may not be possible, or even desirable, to duplicate how a
biological agent does something. Most roboticists do not strive to precisely
replicate animal intelligence, even though many build creatures which re-
semble animals, as seen by the insect-like Genghis in Fig. 3.2. But by focus-
ing on Level 2 of a computational theory of intelligence, roboticists can gain
insights into how to organize intelligence.

72
3
Biological Foundations of the Reactive Paradigm
a.
b.
Figure 3.1
Robots built at the Bio-Bot Laboratory at Case Western Reserve Univer-
sity which imitate cockroaches at Level 3: a.) Robot I, an earlier version, and b.) Robot
III. (Photographs courtesy of Roger Quinn.)
Figure 3.2
Genghis, a legged robot built by Colin Angle, IS Robotics, which imitates
an insect at Levels 1 and 2. (Photograph courtesy of the National Aeronautics and
Space Administration.)

3.2
What Are Animal Behaviors?
73
BEHAVIOR
SENSOR
INPUT
PATTERN
OF MOTOR
ACTION
Figure 3.3
Graphical deﬁnition of a behavior.
3.2
What Are Animal Behaviors?
Scientists believe the fundamental building block of natural intelligence is
a behavior. A behavior is a mapping of sensory inputs to a pattern of mo-
BEHAVIOR
tor actions which then are used to achieve a task. For example, if a horse
sees a predator, it ﬂattens its ears, lowers its head, and paws the ground. In
this case, the sensory input of a predator triggers a recognizable pattern of a
defense behavior. The defensive motions make up a pattern because the ac-
tions and sequence is always the same, regardless of details which vary each
episode (e.g., how many times the horse paws the ground). See Fig. 3.3.
Scientists who study animal behaviors are called ethologists. They often
spend years in the ﬁeld studying a species to identify its behaviors. Often
the pattern of motor actions is easy to ascertain; the challenging part is to
discover the sensory inputs for the behavior and why the behavior furthers
the species survival.
Behaviors can be divided into three broad categories. 10 Reﬂexive behaviors
REFLEXIVE BEHAVIOR
are stimulus-response (S-R), such as when your knee is tapped, it jerks up-
STIMULUS-RESPONSE
ward. Essentially, reﬂexive behaviors are “hardwired”; neural circuits ensure
that the stimulus is directly connected to the response in order to produce the
fastest response time. Reactive behaviors are learned, and then consolidated to
REACTIVE BEHAVIOR
where they can be executed without conscious thought. Any behavior that
involves what is referred to in sports as “muscle memory” is usually a re-
active behavior (e.g., riding a bike, skiing). Reactive behaviors can also be
changed by conscious thought; a bicyclist riding over a very narrow bridge
might “pay attention” to all the movements. Conscious behaviors are delibera-
CONSCIOUS BEHAVIOR
tive (assembling a robot kit, stringing together previously developed behav-
iors, etc.).
The categorization is worthy of note for several reasons. First, the Reactive
Paradigm will make extensive use of reﬂexive behaviors, to the point that
some architectures only call a robot behavior a behavior if it is S-R. Second,
the categorization can help a designer determine what type of behavior to
use, leading to insights about the appropriate implementation. Third, the

74
3
Biological Foundations of the Reactive Paradigm
use of the word “reactive” in ethology is at odds with the way the word is
used in robotics. In ethology, reactive behavior means learned behaviors or a
skill; in robotics, it connotes a reﬂexive behavior. If the reader is unaware of
these differences, it may be hard to read either the ethological or AI literature
without being confused.
3.2.1
Reﬂexive behaviors
Reﬂexive types of behaviors are particularly interesting, since they imply no
need for any type of cognition: if you sense it, you do it. For a robot, this
would be a hardwired response, eliminating computation and guaranteed to
be fast. Indeed, many kit or hobby robots work off of reﬂexes, represented
by circuits.
Reﬂexive behaviors can be further divided into three categories:10
1. reﬂexes: where the response lasts only as long as the stimulus, and the
REFLEXES
response is proportional to the intensity of the stimulus.
2. taxes: where the response is to move to a particular orientation. Baby tur-
TAXES
tles exhibit tropotaxis; they are hatched at night and move to the brightest
light. Until recently the brightest light would be the ocean reﬂecting the
moon, but the intrusion of man has changed that. Owners of beach front
property in Florida now have to turn off their outdoor lights during hatch-
ing season to avoid the lights being a source for tropotaxis. Baby turtles
hatch at night, hidden from shore birds who normally eat them. It had
been a mystery as to how baby turtles knew which way was the ocean
when they hatched. The story goes that a volunteer left a ﬂashlight on the
sand while setting up an experiment intended to show that the baby tur-
tles used magnetic ﬁelds to orient themselves. The magnetic ﬁeld theory
was abandoned after the volunteers noticed the baby turtles heading for
the ﬂashlight! Ants exhibit a particular taxis known as chemotaxis; they
follow trails of pheromones.
3. ﬁxed-action patterns: where the response continues for a longer duration
FIXED-ACTION
PATTERNS
than the stimulus. This is helpful for ﬂeeing predators. It is important to
keep in mind that a taxis can be any orientation relative to a stimulus, not
just moving towards.
The above categories are not mutually exclusive. For example, an animal
going over rocks or through a forest with trees to block its view might persist

3.3
Coordination and Control of Behaviors
75
(ﬁxed-action patterns) in orienting itself to the last sensed location of a food
source (taxis) when it loses sight of it.
The tight coupling of action and perception can often be quantiﬁed by
mathematical expressions. An example of this is orienting in angelﬁsh. In
order to swim upright, an angelﬁsh uses an internal (idiothetic) sense of grav-
IDIOTHETIC
ity combined with its vision sense (allothetic) to see the external percept of
ALLOTHETIC
the horizon line of the water to swim upright. If the ﬁsh is put in a tank with
prisms that make the horizon line appear at an angle, the angelﬁsh will swim
cockeyed. On closer inspection, the angle that the angelﬁsh swims at is the
vector sum of the vector parallel to gravity with the vector perpendicular to
the perceived horizon line! The ability to quantify animal behavior suggests
that computer programs can be written which do likewise.
3.3
Coordination and Control of Behaviors
Konrad Lorenz and Niko Tinbergen were the founding fathers of ethology.
KONRAD LORENZ
NIKO TINBERGEN
Each man independently became fascinated not only with individual behav-
iors of animals, but how animals acquired behaviors and selected or coordi-
nated sets of behaviors. Their work provides some insight into four different
ways an animal might acquire and organize behaviors. Lorenz and Tinber-
gen’s work also helps with a computational theory Level 2 understanding of
how to make a process out of behaviors.
The four ways to acquire a behavior are:
1. to be born with a behavior (innate). An example is the feeding behavior in
INNATE
baby arctic terns. Arctic terns, as the name implies, live in the Arctic where
the terrain is largely shades of black and white. However, the Arctic tern
has a bright reddish beak. When babies are hatched and are hungry, they
peck at the beak of their parents. The pecking triggers a regurgitation
reﬂex in the parent, who literally coughs up food for the babies to eat. It
turns out that the babies do not recognize their parents, per se. Instead,
they are born with a behavior that says: if hungry, peck at the largest red
blob you see. Notice that the only red blobs in the ﬁeld of vision should
be the beaks of adult Arctic terns. The largest blob should be the nearest
parent (the closer objects are, the bigger they appear). This is a simple,
effective, and computationally inexpensive strategy.
2. to be born with a sequence of innate behaviors. The animal is born with a
SEQUENCE OF INNATE
BEHAVIORS
sequence of behaviors. An example is the mating cycle in digger wasps.

76
3
Biological Foundations of the Reactive Paradigm
A female digger wasp mates with a male, then builds a nest. Once it sees
the nest, the female lays eggs. The sequence is logical, but the important
point is the role of stimulus in triggering the next step. The nest isn’t built
until the female mates; that is a change in internal state. The eggs aren’t
laid until the nest is built; the nest is a visual stimulus releasing the next
step. Notice that the wasp doesn’t have to “know” or understand the
sequence. Each step is triggered by the combination of internal state and
the environment. This is very similar to Finite State Machines in computer
science programming, and will be discussed later in Ch. 5.
3. to be born with behaviors that need some initialization (innate with mem-
INNATE WITH MEMORY
ory). An animal can be born with innate behaviors that need customizing
based on the situation the animal is born in. An example of this is bees.
Bees are born in hives. The location of a hive is something that isn’t in-
nate; a baby bee has to learn what its hive looks like and how to navigate
to and from it. It is believed that the curious behavior exhibited by baby
bees (which is innate) allows them to learn this critical information. A
new bee will ﬂy out of the hive for a short distance, then turn around and
come back. This will get repeated, with the bee going a bit farther along
the straight line each time. After a time, the bee will repeat the behavior
but at an angle from the opening to the hive. Eventually, the bee will have
circumnavigated the hive. Why? Well, the conjecture is that the bee is
learning what the hive looks like from all possible approach angles. Fur-
thermore, the bee can associate a view of the hive with a motor command
(“ﬂy left and down”) to get the bee to the opening. The behavior of zoom-
ing around the hive is innate; what is learned about the appearance of the
hive and where the opening is requires memory.
4. to learn a set of behaviors. Behaviors are not necessarily innate. In mam-
LEARN
mals and especially primates, babies must spend a great deal of time
learning.
An example of learned behaviors is hunting in lions.
Lion
cubs are not born with any hunting behaviors. If they are not taught
by their mothers over a period of years, they show no ability to fend
for themselves. At ﬁrst it might seem strange that something as funda-
mental as hunting for food would be learned, not innate. However, con-
sider the complexity of hunting for food. Hunting is composed of many
sub-behaviors, such as searching for food, stalking, chasing, and so on.
Hunting may also require teamwork with other members of the pride. It
requires great sensitivity to the type of the animal being hunted and the
terrain. Imagine trying to write a program to cover all the possibilities!

3.3
Coordination and Control of Behaviors
77
While the learned behaviors are very complex, they can still be repre-
sented by innate releasing mechanisms. It is just that the releasers and
actions are learned; the animal creates the program itself.
Note that the number of categories suggests that a roboticist will have a spec-
trum of choices as to how a robot can acquire one or more behaviors: from
being pre-programmed with behaviors (innate) to somehow learning them
(learned). It also suggests that behaviors can be innate but require mem-
ory. The lesson here is that while S-R types of behaviors are simple to pre-
program or hardwire, robot designers certainly shouldn’t exclude the use
of memory. But as will be seen in Chapter 4, this is a common constraint
placed on many robot systems. This is especially true in a popular style of
hobby robot building called BEAM robotics (biology, electronics, aesthetics,
and mechanics), espoused by Mark Tilden. Numerous BEAM robot web sites
guide adherents through construction of circuits which duplicate memory-
less innate reﬂexes and taxes.
An important lesson that can be extracted from Lorenz and Tinbergen’s
work is that the internal state and/or motivation of an agent may play a role
INTERNAL STATE
MOTIVATION
in releasing a behavior. Being hungry is a stimulus, equivalent to the pain
introduced by a sharp object in the robot’s environment. Another way of
looking at it is that motivation serves as a stimulus for behavior. Motiva-
tions can stem from survival conditions (like being hungry) or more abstract
goals (e.g., need to check the mail). One of the most exciting insights is that
behaviors can be sequenced to create complex behaviors. Something as com-
plicated as mating and building a nest can be decomposed into primitives
or certainly more simple behaviors. This has an appeal to the software engi-
neering side of robotics.
3.3.1
Innate releasing mechanisms
Lorenz and Tinbergen attempted to clarify their work in how behaviors are
coordinated and controlled by giving it a special name innate releasing mech-
INNATE RELEASING
MECHANISMS
anisms (IRM). An IRM presupposes that there is a speciﬁc stimulus (either
internal or external) which releases, or triggers, the stereotypical pattern of
action. The IRM activates the behavior. A releaser is a latch or a Boolean vari-
RELEASER
able that has to be set. One way to think of IRMs is as a process of behaviors.
In a computational theory of intelligence using IRMs, the basic black boxes
of the process would be behaviors. Recall that behaviors take sensory input
and produce motor actions. But IRMs go further and specify when a behav-

78
3
Biological Foundations of the Reactive Paradigm
BEHAVIOR
Releaser
Pattern of 
Motor Actions
Sensory
Input
Figure 3.4
Innate Releasing Mechanism as a process with behaviors.
ior gets turned on and off. The releaser acts as a control signal to activate a
behavior. If a behavior is not released, it does not respond to sensory inputs
and does not produce motor outputs. For example, if a baby arctic tern isn’t
hungry, it doesn’t peck at red, even if there is a red beak nearby.
Another way to think of IRMs is as a simple computer program. Imagine
the agent running a C program with a continuous while loop. Each execu-
tion through the loop would cause the agent to move for one second, then
the loop would repeat.
enum
Releaser={PRESENT, NOT_PRESENT};
Releaser
predator;
while (TRUE)
{
predator = sensePredators();
if (predator == PRESENT)
flee();
}
In this example, the agent does only two things: sense the world and then
ﬂees if it senses a predator. Only one behavior is possible: flee. flee is
released by the presence of a predator. A predator is of type Releaser and
has only two possible values: it is either present or it is not. If the agent does
not sense the releaser for the behavior, the agent does nothing. There is no
“default” behavior.
This example also shows ﬁltering of perception. In the above example, the
agent only looks for predators with a dedicated detection function, sense-
Predators(). The dedicated predator detection function could be a spe-
cialized sense (e.g., retina is sensitive to the frequency of motions associated

3.3
Coordination and Control of Behaviors
79
with predator movement) or a group of neurons which do the equivalent of
a computer algorithm.
Another important point about IRMs is that the releaser can be a compound
of releasers. Furthermore, the releaser can be a combination of either external
COMPOUND RELEASERS
(from the environment) or internal (motivation). If the releaser in the com-
pound isn’t satisﬁed, the behavior isn’t triggered. The pseudo-code below
shows a compound releaser.
enum
Releaser={PRESENT, NOT_PRESENT};
Releaser
food;
while (TRUE)
{
food = senseFood();
hungry = checkState();
if (food == PRESENT && hungry==PRESENT)
feed();
}
The next example below shows what happens in a sequence of behaviors,
where the agent eats, then nurses its young, then sleeps, and repeats the
sequence. The behaviors are implicitly chained together by their releasers.
IMPLICIT CHAINING
Once the initial releaser is encountered, the ﬁrst behavior occurs. It executes
for one second (one “movement” interval), then control passes to the next
statement. If the behavior isn’t ﬁnished, the releasers remain unchanged and
no other behavior is triggered. The program then loops to the top and the
original behavior executes again. When the original behavior has completed,
the internal state of the animal may have changed or the state of the environ-
ment may have been changed as a result of the action. When the motivation
and environment match the stimulus for the releaser, the second behavior is
triggered, and so on.
enum
Releaser={PRESENT, NOT_PRESENT};
Releaser
food, hungry, nursed;
while (TRUE) {
food = sense();
hungry = checkStateHunger();
child = checkStateChild();
if (hungry==PRESENT)
searchForFood(); //sets food = PRESENT when done
if (hungry==PRESENT && food==PRESENT)
feed(); // sets hungry = NOT_PRESENT when done

80
3
Biological Foundations of the Reactive Paradigm
if (hungry== NOT_PRESENT && parent==PRESENT)
nurse(); // set nursed = PRESENT when done
if (nursed ==PRESENT)
sleep();
}
The example also reinforces the nature of behaviors. If the agent sleeps
and wakes up, but isn’t hungry, what will it do? According to the releasers
created above, the agent will just sit there until it gets hungry.
In the previous example, the agent’s behaviors allowed it to feed and en-
able the survival of its young, but the set of behaviors did not include ﬂeeing
or ﬁghting predators. Fleeing from predators could be added to the program
as follows:
enum
Releaser={PRESENT, NOT_PRESENT};
Releaser
food, hungry, nursed, predator;
while (TRUE) {
predator = sensePredator();
if (predator==PRESENT)
flee();
food = senseFood();
hungry = checkStateHunger();
parent = checkStateParent();
if (hungry==PRESENT)
searchForFood();
if (hungry==PRESENT && food==PRESENT)
feed();
if (hungry== NOT_PRESENT && parent==PRESENT)
nurse();
if (nursed ==PRESENT)
sleep();
}
Notice that this arrangement allowed the agent to ﬂee the predator re-
gardless of where it was in the sequence of feeding, nursing, and sleeping
because predator is checked for ﬁrst. But ﬂeeing is temporary, because it did
not change the agent’s internal state (except possibly to make it more hungry
which will show up on the next iteration). The code may cause the agent to
ﬂee for one second, then feed for one second.
One way around this is to inhibit, or turn off, any other behavior until
ﬂeeing is completed. This could be done with an if-else statement:

3.3
Coordination and Control of Behaviors
81
while (TRUE) {
predator = sensePredator();
if (predator==PRESENT)
flee();
else {
food = senseFood();
hungry = checkStateHunger();
...
}
}
The addition of the if-else prevents other, less important behaviors
from executing. It doesn’t solve the problem with the predator releaser dis-
appearing as the agents runs away.
If the agent turns and the predator
is out of view (say, behind the agent), the value of predator will go to
NOT_PRESENT in the next update. The agent will go back to foraging, feed-
ing, nursing, or sleeping. Fleeing should be a ﬁxed-pattern action behavior
which persists for some period of time, T. The ﬁxed-pattern action effect can
be accomplished with:
#define T LONG_TIME
while (TRUE) {
predator = sensePredator();
if (predator==PRESENT)
for(time = T; time > 0; time--)
flee();
else {
food = senseFood();
...
}
}
The C code examples were implemented as an implicit sequence, where
the order of execution depended on the value of the releasers. An imple-
mentation of the same behaviors with an explicit sequence would be:
Releaser
food, hungry, nursed, predator;
while (TRUE) {
predator = sensePredator();
if (predator==PRESENT)

82
3
Biological Foundations of the Reactive Paradigm
flee();
food = senseFood();
hungry = checkStateHunger();
parent = checkStateParent();
if (hungry==PRESENT)
searchForFood();
feed();
nurse();
sleep();
}
The explicit sequence at ﬁrst may be more appealing. It is less cluttered
and the compound releasers are hidden.
But this implementation is not
equivalent. It assumes that instead of the loop executing every second and
the behaviors acting incrementally, each behavior takes control and runs to
completion. Note that the agent cannot react to a predator until it has ﬁn-
ished the sequence of behaviors. Calls to the ﬂeeing behavior could be in-
serted between each behavior or ﬂeeing could be processed on an interrupt
basis. But every “ﬁx” makes the program less general purpose and harder to
add and maintain.
The main point here is: simple behaviors operating independently can lead to
what an outside observer would view as a complex sequence of actions.
3.3.2
Concurrent behaviors
An important point from the examples with the IRMs is that behaviors can,
and often do, execute concurrently and independently. What appears to be a
ﬁxed sequence may be the result of a normal series of events. However, some
behaviors may violate or ignore the implicit sequence when the environment
presents conﬂicting stimuli. In the case of the parent agent, ﬂeeing a predator
was mutually exclusive of the feeding, nursing, and sleeping behaviors.
Interesting things can happen if two (or more) behaviors are released that
usually are not executed at the same time. It appears that the strange inter-
actions fall into the following categories:
 Equilibrium (the behaviors seem to balance each other out): Consider feeding
EQUILIBRIUM
versus ﬂeeing in a squirrel when the food is just close enough to a person
on a park bench. A squirrel will often appear to be visibly undecided as
to whether to go for the food or to stay away.

3.4
Perception in Behaviors
83
 Dominance of one (winner take all): you’re hungry and sleepy. You do one
DOMINANCE
or the other, not both simultaneously.
 Cancellation (the behaviors cancel each other out): Male sticklebacks (ﬁsh)
CANCELLATION
when their territories overlap get caught between the need to defend their
territory and to attack the other ﬁsh. So the males make another nest!
Apparently the stimuli cancels out, leaving only the stimulus normally
associated with nest building.
Unfortunately, it doesn’t appear to be well understood when these differ-
ent mechanisms for conﬂicting behaviors are employed. Clearly, there’s no
one method. But it does emphasize that a roboticist who works with be-
haviors should pay close attention to how the behaviors will interact. This
will give rise to the differences in architectures in the Reactive and Hybrid
Paradigms, discussed in later chapters.
3.4
Perception in Behaviors
While Lorenz and Tinbergen’s work provides some insights into behaviors,
it’s clear that behaviors depend on perception. Ulrich Neisser, who literally
created the term “cognitive psychology” in his book, Cognition and Reality,
argued that perception cannot be separated from action. 109 As will be seen
in this section, J.J. Gibson, a very controversial cognitive psychologist, spent
his career advocating an ecological approach to perception. The ecological
approach is the opposite of the top-down, model-based reasoning about the
environment approach favored by psychologists, including Neisser. Inter-
estingly enough, Neisser took a position at Cornell where J.J. Gibson was,
and they became close colleagues. Since then, Neisser has spent signiﬁcant
time and thought trying to reconcile the two views based on studies; this has
led to his identiﬁcation of two perceptual systems.
3.4.1
Action-perception cycle
The action-perception cycle illustrates that perception is fundamental to any in-
ACTION-PERCEPTION
CYCLE
telligent agent. A simple interpretation of the cycle is: When an agent acts, it
interacts with its environment because it is situated in that environment; it is
an integral part of the environment. So as it acts, it changes things or how it
perceives it (e.g., move to a new viewpoint, trigger a rock slide, etc.). There-
fore the agent’s perception of the world is modiﬁed. This new perception is

84
3
Biological Foundations of the Reactive Paradigm
Figure 3.5
Action-Perception Cycle. 7
then used for a variety of functions, including both cognitive activities like
planning for what to do next as well as reacting. The term cognitive activity
includes the concepts of feedback and feedforward control, where the agent
senses an error in what it attempted to do and what actually happened. An
equally basic cognitive activity is determining what to sense next. That ac-
tivity can be something as straightforward as activating processes to look for
releasers, or as complex as looking for a particular face in a crowd.
Regardless of whether there is an explicit conscious processing of the senses
or the extraction of a stimulus or releaser, the agent is now directed in terms
of what it is going to perceive on the next update(s). This is a type of selective
attention or focus-of-attention. As it perceives, the agent perceptually sam-
ples the world. If the agent actually acts in a way to gather more perception
before continuing with its primary action, then that is sometimes referred to
as active perception. Part of the sampling process is to determine the poten-
tial for action. Lorenz and Tinbergen might think of this as the agent having
a set of releasers for a task, and now is observing whether they are present
in the world. If the perception supports an action, the agent acts. The ac-
tion modiﬁes the environment, but it also modiﬁes the agent’s assessment of
the situation. In the simplest case, this could be an error signal to be used
for control or a more abstract difference such as at the level of those used in
STRIPS/MEA.
In some regards, the action-perception cycle appears to bear a superﬁcial
resemblance to the Hierarchical Paradigm of SENSE, PLAN, ACT. However,
note that 1) there is no box which contains ACT, and 2) the cycle does not
require the equivalent of planning to occur at each update. Action is implicit

3.4
Perception in Behaviors
85
in an agent; the interesting aspect of the cycle is where perception and cog-
nition come in. The agent may have to act to acquire more perception or to
accomplish a task. Also, the agent may or may not need to “plan” an action
on each update.
3.4.2
Two functions of perception
Perception in behavior serves two functions. First, as we saw with IRMs,
it serves to release a behavior. However, releasing a behavior isn’t necessar-
RELEASE
ily the same as the second function: perceiving the information needed to
accomplish the behavior. For example, consider an animal in a forest ﬁre.
The ﬁre activates the ﬂeeing. But the ﬂeeing behavior needs to extract in-
formation about open spaces to run through obstacles in order to guide the
GUIDE
behavior. A frightened deer might bolt right past a hunter without appar-
ently noticing.
In both roles as a releaser and as a guide for behavior, perception ﬁl-
ters the incoming stimulus for the task at hand. This is often referred to
as action-oriented perception by roboticists, when they wish to distinguish
their perceptual approach from the more hierarchical global models style of
perception. Many animals have evolved specialized detectors which sim-
plify perception for their behaviors. Some frogs which sit in water all day
with just half their eyes poking up have a split retina: the lower half is good
for seeing in water, the upper half in air.
3.4.3
Gibson: Ecological approach
The central tenet of Gibson’s approach is that “... the world is its own best
representation.” Gibson’s work is especially interesting because it comple-
ments the role of perception in IRM and is consistent with the action-per-
ception cycle. Gibson postulated (and proved) the existence of affordances.
Affordances are perceivable potentialities of the environment for an action. For ex-
AFFORDANCES
ample, to a baby arctic tern, the color red is perceivable and represents the po-
tential for feeding. So an affordance can be a more formal way of deﬁning the
external stimulus in IRM. But like IRMs, an affordance is only a potential—
it doesn’t count until all the other conditions are satisﬁed (the baby tern is
hungry). An affordance can also be the percept that guides the behavior. The
presence of red to a hungry baby arctic tern releases the feeding behavior.
But the feeding behavior consists of pecking at the red object. So in this case,
red is also the percept being used to guide the action, as well as release it.

86
3
Biological Foundations of the Reactive Paradigm
Gibson referred to his work as an “ecological approach” because he be-
lieved that perception evolved to support actions, and that it is silly to try
to discuss perception independently of an agent’s environment, and its sur-
vival behaviors. For example, a certain species of bees prefers one special
type of poppy. But for a long time, the scientists couldn’t ﬁgure out how the
bees recognized that type of poppy because as color goes, it was indistin-
guishable from another type of poppy that grows in the same area. Smell?
Magnetism? Neither. They looked at the poppy under UV and IR light. In
the non-visible bands that type of poppy stood out from other poppy species.
And indeed, the scientists were able to locate retinal components sensitive
to that bandwidth. The bee and poppy had co-evolved, where the poppy’s
color evolved to a unique bandwidth while at the same time the bee’s retina
was becoming specialized at detecting that color. With a retina “tuned” for
the poppy, the bee didn’t have to do any reasoning about whether there was
a poppy in view, and, if so, was it the right species of poppy. If that color was
present, the poppy was there.
Fishermen have exploited affordances since the beginning of time. A ﬁsh-
ing lure attempts to emphasize those aspects of a ﬁsh’s desired food, pre-
senting the strongest stimulus possible: if the ﬁsh is hungry, the stimulus of
the lure will trigger feeding. As seen in Fig. 3.6, ﬁshing lures often look to a
human almost nothing like the bait they imitate.
What makes Gibson so interesting to roboticists is that an affordance is di-
rectly perceivable. Direct perception means that the sensing process doesn’t
DIRECT PERCEPTION
require memory, inference, or interpretation. This means minimal computa-
tion, which usually translates to very rapid execution times (near instanta-
neous) on a computer or robot.
But can an agent actually perceive anything meaningful without some
memory, inference, or interpretation? Well, certainly baby arctic terns don’t
need memory or inference to get food from a parent. And they’re deﬁnitely
not interpreting red in the sense of: “oh, there’s a red blob. It’s a small oval,
which is the right shape for Mom, but that other one is a square, so it must
be a graduate ethology student trying to trick me.” For baby arctic terns, it’s
simply: red = food, bigger red = better.
Does this work for humans? Consider walking down the hall and some-
body throws something at you. You will most likely duck. You also probably
ducked without recognizing the object, although later you may determine it
was only a foam ball. The response happens too fast for any reasoning: “Oh
look, something is moving towards me. It must be a ball. Balls are usually
hard. I should duck.” Instead, you probably used a phenomena so basic that

3.4
Perception in Behaviors
87
Figure 3.6
A collection of artiﬁcial bait, possibly the ﬁrst example of humans ex-
ploiting affordances. Notice that the lures exaggerate one or more attributes of what
a ﬁsh might eat.
you haven’t noticed it, called optic ﬂow. Optic ﬂow is a neural mechanism
OPTIC FLOW
for determining motion. Animals can determine time to contact quite easily
with it. You probably are somewhat familiar with optic ﬂow from driving in
a car. When driving or riding in a car, objects in front seem to be in clear focus
but the side of the road is a little blurry from the speed. The point in space
that the car is moving to is the focus of expansion. From that point outward,
there is a blurring effect. The more blurring on the sides, the faster the car is
going. (They use this all the time in science ﬁction movies to simulate faster-
than-light travel.) That pattern of blurring is known as a ﬂow ﬁeld (because
it can be represented by vectors, like a gravitational or magnetic ﬁeld). It is
straightforward, neurally, to extract the time to contact, represented in the
TIME TO CONTACT
cognitive literature by
.
Gannets and pole vaulters both use optic ﬂow to make last-minute, pre-
cise movements as reﬂexes. Gannets are large birds which dive from high
altitudes after ﬁsh. Because the birds dive from hundreds of feet up in the
air, they have to use their wings as control surfaces to direct their dive at the
targeted ﬁsh. But they are plummeting so fast that if they hit the water with
their wings open, the hollow bones will shatter. Gannets fold their wings just
before hitting the water. Optic ﬂow allows the time to contact,
, to be a stim-
ulus: when the time to contact dwindles below a threshold, fold those wings!

88
3
Biological Foundations of the Reactive Paradigm
Pole vaulters also make minute adjustments in where they plant their pole
as they approach the hurdle. This is quite challenging given that the vaulter
is running at top speed. It appears that pole vaulters use optic ﬂow rather
than reason (slowly) about where the best place is for the pole. (Pole vaulting
isn’t the only instance where humans use optic ﬂow, just one that has been
well-documented.)
In most applications, a fast computer program can extract an affordance.
However, this is not the case (so far) with optic ﬂow. Neural mechanisms in
the retina have evolved to make the computation very rapid. It turns out that
computer vision researchers have been struggling for years to duplicate the
generation of an optical ﬂow ﬁeld from a camera image. Only recently have
we seen any algorithms which ran in real-time on regular computers.48 The
point is that affordances and specialized detectors can be quite challenging
to duplicate in computers.
Affordances are not limited to vision. A common affordance is knowing
when a container is almost ﬁlled to the top. Think about ﬁlling a jug with
water or the fuel tank of a car. Without being able to see the cavity, a person
knows when the tank is almost ﬁlled by the change in sound. That change
in sound is directly perceivable; the person doesn’t need to know anything
about the size or shape of the volume being ﬁlled or even what the liquid is.
One particularly fascinating application of affordances to robotics, which
also serves to illustrate what an affordance is, is the research of Louise Stark
and Kevin Bowyer.135 A seemingly unsurmountable problem in computer
vision has been to have a computer recognize an object from a picture. Liter-
ally, the computer should say, “that’s a chair” if the picture is of a chair.
The traditional way of approaching the problem has been to use structural
STRUCTURAL MODELS
models. A structural model attempts to describe an object in terms of physical
components: “A chair has four legs, a seat, and a back.” But not all chairs ﬁt
the same structural model. A typing chair has only one leg, with supports
at the bottom. Hanging baskets don’t have legs at all. A bench seat doesn’t
have a back. So clearly the structural approach has problems: instead of one
structural representation, the computer has to have access to many different
models. Structural models also lack ﬂexibility. If the robot is presented with a
new kind of chair (say someone has designed a chair to look like your toilet
or an upside down trash can), the robot would not be able to recognize it
without someone explicitly constructing another structural model.
Stark and Bowyer explored an alternative to the structual approach called
GRUFF. GRUFF identiﬁes chairs by function rather than form. Under Gibso-
nian perception, a chair should be a chair because it affords sitting, or serves

3.4
Perception in Behaviors
89
a.
b.
Figure 3.7
The GRUFF system: a.) input, and b.) different types of chairs recognized
by GRUFF. (Figures courtesy of Louise Stark.)
the function of sittability. And that affordance of sittability should be some-
thing that can be extracted from an image:
 Without memory (the agent doesn’t need to memorize all the chairs in the
world).
 Without inference (the robot doesn’t need to reason: “if it has 4 legs, and a
seat and a back, then it’s a chair; we’re in an area which should have lots
of chairs, so this makes it more likely it’s a chair”).
 Without an interpretation of the image (the robot doesn’t need to reason:
“there’s an arm rest, and a cushion, ...”). A computer should just be able
to look at a picture and say if something in that picture is sittable or not.

90
3
Biological Foundations of the Reactive Paradigm
Stark and Bowyer represented sittability as a reasonably level and contin-
uous surface which is at least the size of a person’s butt and at about the
height of their knees. (Everything else like seat backs just serve to specify
the kind of chair.) Stark and Bowyer wrote a computer program which ac-
cepted CAD/CAM drawings from students who tried to come up with non-
intuitive things that could serve as chairs (like toilets, hanging basket chairs,
trash cans). The computer program was able to correctly identify sittable
surfaces that even the students missed.
It should be noted that Stark and Bowyer are hesitant to make claims about
what this says about Gibsonian perception. The computer vision algorithm
can be accused of some inference and interpretation (“that’s the seat, that’s
the right height”). But on the other hand, that level of inference and interpre-
tation is signiﬁcantly different than that involved in trying to determine the
structure of the legs, etc. And the relationship between seat size and height
could be represented in a special neural net that could be released whenever
the robot or animal got tired and wanted to sit down. The robot would start
noticing that it could sit on a ledge or a big rock if a chair or bench wasn’t
around.
3.4.4
Neisser: Two perceptual systems
At this point, the idea of affordances should seem reasonable. A chair is a
chair because it affords sittability. But what happens when someone sits in
your chair? It would appear that humans have some mechanism for recog-
nizing speciﬁc instances of objects. Recognition deﬁnitely involves memory
(“my car is a blue Ford Explorer and I parked it in slot 56 this morning”).
Other tasks, like the kind of sleuthing Sherlock Holmes does, may require
inference and interpretation. (Imagine trying to duplicate Sherlock Holmes
in a computer. It’s quite different than mimicking a hungry baby arctic tern.)
So while affordances certainly are a powerful way of describing perception
in animals, it is clearly not the only way animals perceive. Neisser postulated
that there are two perceptual systems in the brain (and cites neurophysiolog-
ical data):110
1. direct perception. This is the “Gibsonian,” or ecological, track of the brain,
DIRECT PERCEPTION
and consists of structures low in the brain which evolved earlier on and
accounts for affordances.
2. recognition. This is more recent perceptual track in the brain, which ties in
RECOGNITION
with the problem solving and other cognitive activities. This part accounts

3.5
Schema Theory
91
for the use of internal models to distinguish “your coffee cup” from “my
coffee cup.” This is where top-down, model-based perception occurs.
On a more practical note, Neisser’s dichotomy suggests that the ﬁrst de-
cision in designing a behavior is to determine whether a behavior can be
accomplished with an affordance or requires recognition. If it can be accom-
plished with an affordance, then there may be a simple and straightforward
way to program it in a robot; otherwise, we will most certainly have to em-
ploy a more sophisticated (and slower) perceptual algorithm.
3.5
Schema Theory
Schema theory provides a helpful way of casting some of the insights from
above into an object-oriented programming format. 6 Psychologists have used
schema theory since the early 1900’s. It was ﬁrst brought to the serious at-
tention of AI roboticists by Michael Arbib while at the University of Mas-
sachusetts, and later used extensively by Arkin and Murphy for mobile ro-
botics, Lyons and Iberall for manipulation,75 and Draper et al. for vision.46
Schemas were conceived of by psychologists as a way of expressing the
basic unit of activity. A schema consists both of the knowledge of how to act
SCHEMA
and/or perceive (knowledge, data structures, models) as well as the com-
putational process by which it is uses to accomplish the activity (the algo-
rithm). The idea of a schema maps nicely onto a class in object-oriented
programming (OOP). A schema class in C++ or Java would contain both data
SCHEMA CLASS
(knowledge, models, releasers) and methods (algorithms for perceiving and
acting), as shown below.
Schema:
Data
Methods
A schema is a generic template for doing some activity, like riding a bi-
cycle. It is a template because a person can ride different bicycles without
starting the learning process all over. Since a schema is parameterized like a
class, parameters (type of bicycle, height of the bicycle seat, position of the
handlebars) can be supplied to the object at the time of instantiation (when
an object is created from the class). As with object-oriented programming,
the creation of a speciﬁc schema is called a schema instantiation (SI).
SCHEMA
INSTANTIATION (SI)

92
3
Biological Foundations of the Reactive Paradigm
BEHAVIOR
Releaser
Pattern of 
Motor Actions
Sensory
Input
Perceptual
Schema
Motor
Schema
Figure 3.8
Behaviors decomposed into perceptual and motor schemas.
The schema instantiation is the object which is constructed with whatever
parameters are needed to tailor it to the situation. For example, there could
be a move_to_food schema where the agent always heads in a straight line
to the food. Notice that the “always heads in a straight line” is a template
of activity, and a reusable algorithm for motion control. However, it is just a
method; until the move_to_food schema is instantiated, there is no speciﬁc
goal to head for, e.g., the candy bar on the table. The same schema could be
instantiated for moving to a sandwich.
3.5.1
Behaviors and schema theory
In the Arbibian application of schema theory towards a computational the-
ory of intelligence, a behavior is a schema which is composed of a motor
COMPOSITION OF A
BEHAVIOR
schema and a perceptual schema. The motor schema represents the template
MOTOR SCHEMA
for the physical activity, the perceptual schema embodies the sensing. The mo-
PERCEPTUAL SCHEMA
tor schema and perceptual schema are like pieces of a puzzle; both pieces
must be together in place before there is a behavior. This idea is shown below
in Fig. 3.8.
Essentially, the perceptual and motor schema concept ﬁts in with ethology
and cognitive psychology as follows:
 A behavior takes sensory inputs and produces motor actions as an output.

3.5
Schema Theory
93
 A behavior can be represented as a schema, which is essentially an object-
oriented programming construct.
 A behavior is activated by releasers.
 The transformation of sensory inputs into motor action outputs can be
divided into two sub-processes: a perceptual schema and a motor schema.
In OOP terms, the motor schema and perceptual schema classes are de-
rived from the schema class. A primitive behavior just has one motor and
one perceptual schema.
Behavior::Schema
Data
Methods
perceptual_schema()
motor_schema()
Recall from IRMs, more sophisticated behaviors may be constructed by
sequencing behaviors. In the case of a sequence of behaviors, the overall
behavior could be represented in one of two ways. One way is to consider
the behavior to be composed of several primitive behaviors, with the releas-
ing logic to serve as the knowledge as to when to activate each primitive
behaviors. This is probably the easiest way to express a “meta” behavior.
A meta-behavior composed of three behaviors can be thought of as:
Behavior::Schema
Data
releaser1
releaser2
releaser3
IRM_logic()
Methods
behavior1()
behavior2()
behavior3()
However, in more advanced applications, the agent may have a choice of
either perceptual or motor schemas to tailor its behavior. For example, a
person usually uses vision (the default perceptual schema) to navigate out
of a room (motor schema). But if the power is off, the person can use touch
(an alternate perceptual schema) to feel her way out of a dark room. In this
case, the schema-speciﬁc knowledge is knowing which perceptual schema

94
3
Biological Foundations of the Reactive Paradigm
to use for different environmental conditions. Schema theory is expressive
enough to represent basic concepts like IRMs, plus it supports building new
behaviors out of primitive components. This will be discussed in more detail
in later chapters.
This alternative way of creating a behavior by choosing between alterna-
tive perceptual and motor schemas can be thought of as:
Behavior::Schema
Data
environmental_state
Methods
choose_PS(environmental_state)
perceptual_schema_1()
perceptual_schema_2()
motor_schema()
Arbib and colleagues did work constructing computer models of visually
guided behaviors in frogs and toads. They used schema theory to represent
the toad’s behavior in computational terms, and called their model rana com-
RANA COMPUTATRIX
putatrix (rana is the classiﬁcation for toads and frogs). The model explained
Ingle’s observations as to what occasionally happens when a toad sees two
ﬂies at once.33 Toads and frogs can be characterized as responding visually
to either small, moving objects and large, moving objects. Small, moving ob-
jects release the feeding behavior, where the toad orients itself towards the
object (taxis) and then snaps at it. (If the object turns out not to be a ﬂy,
the toad can spit it out.) Large moving objects release the ﬂeeing behavior,
causing the toad to hop away. The feeding behavior can be modeled as a
behavioral schema, or template, shown in Fig. 3.9.
When the toad sees a ﬂy, an instance of the behavior is instantiated; the
toad turns toward that object and snaps at it. Arbib’s group went one level
further on the computational theory.7 They implemented the taxis behavior
as a vector ﬁeld: rana computatrix would literally feel an attractive force
along the direction of the ﬂy. This direction and intensity (magnitude) was
represented as a vector. The direction indicated where rana had to turn and
the magnitude indicated the strength of snapping. This is shown in Fig. 3.10.
What is particularly interesting is that the rana computatrix program pre-
dicts what Ingle saw in real toads and frogs when they are presented with
two ﬂies simultaneously. In this case, each ﬂy releases a separate instance of
the feeding behavior. Each behavior produces the vector that the toad needs
to turn to in order to snap at that ﬂy, without knowing that the other be-

3.5
Schema Theory
95
Releaser
Pattern of 
Motor Actions
Sensory
Input
Perceptual
Schema
Motor
Schema
Get coordinates
of small, moving
object
Turn to
coordinates
of small, 
moving object
appearance of
small, moving object
toad’s vision
toad’s legs
Feeding Behavior
Figure 3.9
Toad’s feeding behavior represented as a behavior with schema theory.
havior exists. According to the vector ﬁeld implementation of the schema
model, the toad now receives two vectors, instead of one. What to do? Well,
rana computatrix summed the two vectors, resulting in a third vector in be-
tween the original two! The toad snaps at neither ﬂy, but in the middle. The
unexpected interaction of the two independent instances probably isn’t that
much of a disadvantage for a toad, because if there are two ﬂies in such close
proximity, eventually one of them will come back into range.
This example illustrates many important lessons for robotics. First, it val-
idates the idea of a computational theory, where functionality in an animal
and a computer can be equivalent. The concept of behaviors is Level 1 of the
computational theory, schema theory (especially the perceptual and motor
schemas) expresses Level 2, and Level 3 is the vector ﬁeld implementation
of the motor action. It shows the property of emergent behavior, where the
agent appears to do something fairly complex, but is really just the result of
interaction between simple modules. The example also shows how behav-
iors correspond to object-orienting programming principles.
Another desirable aspect of schema theory is that it supports reﬂex be-
haviors. Recall that in reﬂex behaviors, the strength of the response is pro-
portional to the strength of the stimulus. In schema theory, the perceptual
schema is permitted to pass both the percept and a gain to the motor schema.
The motor schema can use the gain to compute a magnitude on the output
action. This is an example of how a particular schema can be tailored for a
behavior.

96
3
Biological Foundations of the Reactive Paradigm
activation
condition
motor schema
perceptual schema
behavior
motor SI
perceptual SI
percept,
gain
action,
intensity
fly1
snap
locate_fly
snap(fly1)
locate_fly(fly1)
(x,y,z),
100%
snap at (x,y,z)
with all strength
Figure 3.10
Schema theory of a frog snapping at a ﬂy.
Schema theory does not specify how the output from concurrent behaviors
is combined; that is a Level 3, or implementation, issue. Previous examples in
this chapter have shown that in some circumstances the output is combined
or summed, in others the behaviors would normally occur in a sequence
and not overlap, and sometimes there would be a winner-take-all effect. The
winner-take-all effect is a type of inhibition, where one behavior inhibits the
instantiation of another behavior.
Arbib and colleagues also modeled an instance of inhibition in frogs and
INHIBITION
toads.7 Returning to the example of feeding and ﬂeeing, one possible way to
model this behavior is with two behaviors. The feeding behavior would
consist of a motor schema for moving toward an object, with a perceptual
schema for ﬁnding small, moving objects. The fleeing behavior would be
similar only with a motor schema for moving away from the perception of
large moving objects. Lesion studies with frogs showed something different.
The feeding behavior actually consists of moving toward any moving ob-
ject. So the perceptual schema is more general than anticipated. The frog
would try to eat anything, including predators. The perceptual schema in
the fleeing behavior detects large moving objects. It ﬂees from them, but

3.6
Principles and Issues in Transferring Insights to Robots
97
activation
condition
motor schema
perceptual schema
fly1
snap
locate_fly
behavior
motor SI
perceptual SI
percept,
gain
action,
intensity
snap(fly1)
locate_fly(fly1)
(x,y,z),
100%
snap at (x,y,z)
with all strength
behavior
motor SI
perceptual SI
percept,
gain
action,
intensity
snap(fly2)
locate_fly(fly2)
(x,y,z),
100%
snap at (x,y,z)
with all strength
fly2
vector summation:
snaps at the "average"
(x,y,z)
Figure 3.11
Schema theory of a frog snapping at a ﬂy when presented with two ﬂies
equidistant.
it also inhibits the perceptual schema for feeding. As a result, the inhibition
keeps the frog from trying to both ﬂee from predators and eat them.
3.6
Principles and Issues in Transferring Insights to Robots
To summarize, some general principles of natural intelligence which may be
useful in programming robots:
PRINCIPLES FOR
PROGRAMMING
 Programs should decompose complex actions into independent behav-
iors, which tightly couple sensing and acting. Behaviors are inherently
parallel and distributed.
 In order to simplify control and coordination of behaviors, an agent should
rely on straightforward, boolean activation mechanisms (e.g. IRM).

98
3
Biological Foundations of the Reactive Paradigm
 In order to simplify sensing, perception should ﬁlter sensing and consider
only what is relevant to the behavior (action-oriented perception).
 Direct perception (affordances) reduces the computational complexity of
sensing, and permits actions to occur without memory, inference, or in-
terpretation.
 Behaviors are independent, but the output from one 1) may be combined
with another to produce a resultant output, or 2) may serve to inhibit
another (competing-cooperating).
Unfortunately, studying natural intelligence does not give a complete pic-
ture of how intelligence works. In particular there are several unresolved
issues:
UNRESOLVED ISSUES
 How to resolve conﬂicts between concurrent behaviors?
Robots will be re-
quired to perform concurrent tasks; for example, a rescue robot sent in
to evacuate a building will have to navigate hallways while looking for
rooms to examine for people, as well as look for signs of a spreading ﬁre.
Should the designer specify dominant behaviors? Combine? Let conﬂict-
ing behaviors cancel and have alternative behavior triggered? Indeed, one
of the biggest divisions in robot architectures is how they handle concur-
rent behaviors.
 When are explicit knowledge representations and memory necessary? Direct
perception is wonderful in theory, but can a designer be sure that an af-
fordance has not been missed?
 How to set up and/or learn new sequences of behaviors? Learning appears to be
a fundamental component of generating complex behaviors in advanced
animals. However, the ethological and cognitive literature is unsure of
the mechanisms for learning.
It is also important to remember that natural intelligence does not map
perfectly onto the needs and realities of programming robots. One major
advantage that animal intelligence has over robotic intelligence is evolution.
Animals evolved in a way that leads to survival of the species. But robots are
expensive and only a small number are built at any given time. Therefore, in-
dividual robots must “survive,” not species. This puts tremendous pressure
on robot designers to get a design right the ﬁrst time. The lack of evolution-
ary pressures over long periods of time makes robots extremely vulnerable
to design errors introduced by a poor understanding of the robot’s ecology.

3.7
Summary
99
Ch. 5 will provide a case study of a robot which was programmed to fol-
low white lines in a path-following competition by using the affordance of
white. It was distracted off course by the white shoes of a judge. Fortunately
that design ﬂaw was compensated for when the robot got back on course by
reacting to a row of white dandelions in seed.
Robots introduce other challenges not so critical in animals. One of the
most problematic attributes of the Reactive Paradigm, Ch. 4, is that roboti-
cists have no real mechanism for completely predicting emergent behaviors.
Since a psychologist can’t predict with perfect certainty what a human will
do under a stressful situation, it seems reasonable that a roboticist using prin-
ciples of human intelligence wouldn’t be able to predict what a robot would
do either. However, robotics end-users (military, NASA, nuclear industry)
have been reluctant to accept robots without a guarantee of what it will do
in critical situations.
3.7
Summary
A behavior is the fundamental element of biological intelligence, and will
serve as the fundamental component of intelligence in most robot systems.
A behavior is deﬁned as a mapping of sensory inputs to a pattern of motor
BEHAVIOR
actions which then are used to achieve a task. Innate Releasing Mechanisms
are one model of how intelligence is organized. IRMs model intelligence at
Level 2 of a computational theory, describing the process but not the imple-
mentation. In IRM, releasers activate a behavior. A releaser may be either
an internal state (motivation) and/or an environmental stimulus. Unfortu-
nately, IRMs do not make the interactions between concurrent, or potentially
concurrent, behaviors easy to identify or diagram.
Perception in behaviors serves two roles, either as a releaser for a behavior
or as the percept which guides the behavior. The same percept can be used
both as a releaser and a guide; for example, a ﬁsh can respond to a lure and
follow it. In addition to the way in which perception is used, there appear to
be two pathways for processing perception. The direct perception pathway
uses affordances: perceivable potentialities for action inherent in the envi-
ronment. Affordances are particularly attractive to roboticists because they
can be extracted without inference, memory, or intermediate representations.
The recognition pathway makes use of memory and global representations
to identify and label speciﬁc things in the world.
Important principles which can be extracted from natural intelligence are:

100
3
Biological Foundations of the Reactive Paradigm
 Agents programs should decompose complex actions into independent
behaviors (or objects), which tightly couple sensing and acting. Behaviors
are inherently parallel and distributed.
 In order to simplify control and coordination of behaviors, an agent should
rely on straightforward, boolean activation mechanisms (e.g. IRM).
 In order to simplify sensing, perception should ﬁlter sensing and consider
only what is relevant to the behavior (action-oriented perception).
 Direct perception (affordances) reduces the computational complexity of
sensing, and permits actions to occur without memory, inference, or in-
terpretation.
 Behaviors are independent, but the output from one 1) may be combined
with another to produce a resultant output, or 2) may serve to inhibit
another (competing-cooperating).
Schema theory is an object-oriented way of representing and thinking about
behaviors. The important attributes of schema theory for behaviors are:
 Schema theory is used to represent behaviors in both animals and com-
puters, and is sufﬁcient to describe intelligence at the ﬁrst two levels of a
computational theory.
 A behavioral schema is composed of at least one motor schema and at
least one perceptual schema, plus local, behavior-speciﬁc knowledge about
how to coordinate multiple component schemas.
 More than one behavior schema can be instantiated at a time, but the
schemas act independently.
 A behavior schema can have multiple instantiations which act indepen-
dently, and are combined.
 Behaviors or schemas can be combined, sequenced, or inhibit one another.
3.8
Exercises
Exercise 3.1
Describe the three levels in a Computational Theory.

3.8
Exercises
101
Exercise 3.2
Explain in one or two sentences each of the following terms: reﬂexes, taxes, ﬁxed-
action patterns, schema, affordance.
Exercise 3.3
Represent a schema, behavior, perceptual schema, and motor schema with an Object-
Oriented Design class diagram.
Exercise 3.4
Many mammals exhibit a camouﬂage meta-behavior. The animal freezes when it sees
motion (an affordance for a predator) in an attempt to become invisible. It persists un-
til the predator is very close, then the animal ﬂees. (This explains why squirrels freeze
in front of cars, then suddenly dash away, apparently ﬂinging themselves under the
wheels of a car.) Write pseudo-code of the behaviors involved in the camouﬂage
behavior in terms of innate releasing mechanisms, identifying the releasers for each
behavior.
Exercise 3.5
Consider a mosquito hunting for a warm-blooded mammal and a good place to bite
them. Identify the affordance for a warm-blooded mammal and the associated be-
havior. Represent this with schema theory (perceptual and motor schemas).
Exercise 3.6
One method for representing the IRM logic is to use ﬁnite state automata (FSA),
which are commonly used in computer science. If you have seen FSAs, consider a
FSA where the behaviors are states and releasers serve as the transitions between
state. Express the sequence of behaviors in a female digger wasp as a FSA.
Exercise 3.7
Lego Mindstorms and Rug Warrior kits contain sensors and actuators which are cou-
pled together in reﬂexive behaviors. Build robots which:
a. Reﬂexive avoid: turn left when they touch something (use touch sensor and two
motors)
b. Phototaxis: follow a black line (use the IR sensor to detect the difference between
light and dark)
c. Fixed-action pattern avoid: back up and turn right when robot encounters a “neg-
ative obstacle” (a cliff)
Exercise 3.8
What is the difference between direct perception and recognition?
Exercise 3.9
Consider a cockroach, which typically hides when the lights are turned on. Do you
think the cockroach is using direct perception or recognition of a hiding place? Ex-
plain why. What are the percepts for the cockroach?

102
3
Biological Foundations of the Reactive Paradigm
Exercise 3.10
Describe how cancellation could happen as a result of concurrency and incomplete
FSA.
Exercise 3.11
[Advanced Reading]
Read the ﬁrst four chapters in Braitenberg’s Vehicles. 25 Write a 2-5 page paper:
a. List and describe the principles of behaviors for robotics in Ch. 3.
b. Discuss how Vehicles is consistent with the biological foundations of reactivity. Be
speciﬁc, citing which vehicle illustrates what principle or attribute discussed in
the book.
c. Discuss any ﬂaws in the reasoning or inconsistency between Vehicles with the bio-
logical foundations of reactivity or computer science.
Exercise 3.12
[Advanced Reading]
Read “Sensorimotor transformations in the worlds of frogs and robots,” by Arbib and
Liaw. 7
a. List and describe how the principles of schema theory and potential ﬁelds for ro-
botics given in Ch. 3.
b. Summarize the main contributions of the paper.
3.9
End Notes
For the roboticist’s bookshelf.
Valentino Braitenberg’s Vehicles: Experiments in Synthetic Psychology25 is the cult clas-
sic of AI roboticists everywhere. It doesn’t require any hardware or programming
experience, just a couple hours of time and an average imagination to experience this
radical departure from the mainstream robotics of the 1970’s.
About David Marr.
David Marr’s idea of a computational theory was an offshoot of his work bridging
the gap between vision from a neurophysiological perspective (his) and computer
vision. As is discussed in his book, Vision, 88 Marr had come from England to work in
the MIT AI lab on computer vision. The book represented his three years there, and
he ﬁnished it while literally on his deathbed with leukemia. His preface to the book
is heartbreaking.
A Brief History of Cognitive Science.
Howard Gardner’s The Mind’s New Science 56 gives a nice readable overview of cogni-
tive psychology. He conveys a bit of the controversy Gibson’s work caused.

3.9
End Notes
103
J.J. and E.J. Gibson.
While J.J. Gibson is very well-known, his wife Jackie (E.J. Gibson) is also a prominent
cognitive psychologist. They met when he began teaching at Smith College, where
she was a student. She raised a family, ﬁnished a PhD, and publishes well-respected
studies on learning. At least two of the Gibson’s students followed their husband-
wife teaming: Herb Pick was a student of J.J. Gibson, while his wife, Anne Pick, was
a student of E.J. Gibson. The Picks are at the University of Minnesota, and Herb Pick
has been active in the mobile robotics community.
Susan Calvin, robopsychologist.
Isaac Asimov’s robot stories often feature Dr. Susan Calvin, the ﬁrst robopsycholo-
gist. In the stories, Calvin is often the only person who can ﬁgure out the complex
interactions of concurrent behaviors leading to a robot’s emergent misbehavior. In
some regards, Calvin is the embarrassing Cold War stereotype of a woman scientist:
severe, unmarried, too focused on work to be able to make small talk.


4
The Reactive Paradigm
Chapter Objectives:
 Deﬁne what the reactive paradigm is in terms of i) the three primitives
SENSE, PLAN, and ACT, and ii) sensing organization.
 List the characteristics of a reactive robotic system, and discuss the con-
notations surrounding the reactive paradigm.
 Describe the two dominant methods for combining behaviors in a reactive
architecture: subsumption and potential ﬁeld summation.
 Evaluate subsumption and potential ﬁelds architectures in terms of: sup-
port for modularity, niche targetability, ease of portability to other domains, ro-
bustness.
 Be able to program a behavior using a potential ﬁeld methodology.
 Be able to construct a new potential ﬁeld from primitive potential ﬁelds,
and sum potential ﬁelds to generate an emergent behavior.
4.1
Overview
This chapter will concentrate on an overview of the reactive paradigm and
two representative architectures. The Reactive Paradigm emerged in the late
1980’s. The Reactive Paradigm is important to study for at least two reasons.
First, robotic systems in limited task domains are still being constructed us-
ing reactive architectures. Second, the Reactive Paradigm will form the basis
for the Hybrid Reactive-Deliberative Paradigm; everything covered here will
be used (and expanded on) by the systems in Ch. 7.

106
4
The Reactive Paradigm
Sensors
Extract
Features
Combine
Features
into
Model
Plan
Tasks
Task
Execution
Motor
Control
Actuators
SENSE
PLAN
ACT
Figure 4.1
Horizontal decomposition of tasks into the S,P,A organization of the Hi-
erarchical Paradigm.
The Reactive Paradigm grew out of dissatisfaction with the hierarchical
paradigm and with an inﬂux of ideas from ethology. Although various reac-
tive systems may or may not strictly adhere to principles of biological intelli-
gence, they generally mimic some aspect of biology. The dissatisfaction with
the Hierarchical Paradigm was best summarized by Rodney Brooks,27 who
characterized those systems as having a horizontal decomposition as shown in
HORIZONTAL
DECOMPOSITION
Fig. 4.1.
Instead, an examination of the ethological literature suggests that intelli-
gence is layered in a vertical decomposition, shown in Fig. 4.2. Under a ver-
VERTICAL
DECOMPOSITION
tical decomposition, an agent starts with primitive survival behaviors and
evolves new layers of behaviors which either reuse the lower, older behav-
iors, inhibit the older behaviors, or create parallel tracks of more advanced
behaviors. The parallel tracks can be thought of layers, stacked vertically.
Each layer has access to sensors and actuators independently of any other
layers. If anything happens to an advanced behavior, the lower level be-
haviors would still operate. This return to a lower level mimics degradation
of autonomous functions in the brain. Functions in the brain stem (such as
breathing) continue independently of higher order functions (such as count-
ing, face recognition, task planning), allowing a person who has brain dam-
age from a car wreck to still breathe, etc.
Work by Arkin, Brooks, and Payton focused on deﬁning behaviors and on
mechanisms for correctly handling situations when multiple behaviors are
active simultaneously. Brooks took an approach now known as subsumption
and built insect-like robots with behaviors captured in hardware circuitry.

4.1
Overview
107
SENSE
ACT
SENSE
ACT
SENSE
ACT
SENSE
ACT
build maps
explore
wander
avoid collisions
actuators
sensors
Figure 4.2
Vertical decomposition of tasks into an S-A organization, associated with
the Reactive Paradigm.
Arkin and Payton used a potential ﬁelds methodology, favoring software
implementations. Both approaches are equivalent. The Reactive Paradigm
was initially met with stiff resistance from traditional customers of robot-
ics, particularly the military and nuclear regulatory agencies. These users of
robotic technologies were uncomfortable with the imprecise way in which
discrete behaviors combine to form a rich emergent behavior. In particular,
reactive behaviors are not amenable to mathematical proofs showing they
are sufﬁcient and correct for a task. In the end, the rapid execution times
associated with the reﬂexive behaviors led to its acceptance among users,
just as researchers shifted to the Hybrid paradigm in order to fully explore
layering of intelligence.

108
4
The Reactive Paradigm
The major theme of this chapter is that all reactive systems are composed
of behaviors, though the meaning of a behavior may be slightly different in
each reactive architecture. Behaviors can execute concurrently and/or se-
quentially. The two representative architectures, subsumption and potential
ﬁelds, are compared and contrasted using the same task as an example. This
chapter will concentrate on how architecture handles concurrent behaviors
to produce an emergent behavior, deferring sequencing to the next chapter.
4.2
Attributes of Reactive Paradigm
The fundamental attribute of the reactive paradigm is that all actions are
accomplished through behaviors. As in ethological systems, behaviors are a
BEHAVIORS
direct mapping of sensory inputs to a pattern of motor actions that are then used
to achieve a task. From a mathematical perspective, behaviors are simply a
transfer function, transforming sensory inputs into actuator commands. For
the purposes of this book, a behavior will be treated as a schema, and will
consist of at least one motor schema and one perceptual schema. The mo-
tor schema contains the algorithm for generating the pattern of action in a
physical actuator and the perceptual schema contains the algorithm for ex-
tracting the percept and its strength. Keep in mind that few reactive robot
architectures describe their behaviors in terms of schemas. But in practice,
most behavioral implementations have recognizable motor and perceptual
routines, even though they are rarely referred to as schemas.
The Reactive Paradigm literally threw away the PLAN component of the
SENSE, PLAN, ACT triad, as shown in Fig. 4.3. The SENSE and ACT com-
ponents are tightly coupled into behaviors, and all robotic activities emerge
as the result of these behaviors operating either in sequence or concurrently.
The S-A organization does not specify how the behaviors are coordinated
SENSE-ACT
ORGANIZATION
and controlled; this is an important topic addressed by architectures.
Sensing in the Reactive Paradigm is local to each behavior, or behavior-
BEHAVIOR-SPECIFIC
(LOCAL) SENSING
speciﬁc. Each behavior has its own dedicated sensing. In many cases, this is
implemented as one sensor and perceptual schema per behavior. But in other
cases, more than one behavior can take the same output from a sensor and
process it differently (via the behavior’s perceptual schema). One behavior
literally does not know what another behavior is doing or perceiving. Fig. 4.4
graphically shows the sensing style of the Reactive Paradigm.
Note that this is fundamentally opposite of the global world model used
in the hierarchical paradigm. Sensing is immediately available to the be-

4.2
Attributes of Reactive Paradigm
109
Figure 4.3
S-A organization of the Reactive Paradigm into multiple, concurrent be-
haviors.
havior’s perceptual schema, which can do as little or as much processing
as needed to extract the relevant percept. If a computationally inexpensive
affordance is used, then the sensing portion of the behavior is nearly instan-
taneous and action is very rapid.
As can be seen from the previous chapter on the biological foundations of
the reactive paradigm, behaviors favor the use of affordances. In fact, Brooks
was fond of saying (loudly) at conferences, “we don’t need no stinking rep-
resentations.” It should be noted that often the perceptual schema portion of
the behavior has to use a behavior-speciﬁc representation or data structure
to substitute for specialized detectors capable of extracting affordances. For
example, extracting a red region in an image is non-trivial with a computer
compared with an animal seeing red. The point is that while a computer pro-
gram may have to have data structures in order to duplicate a simple neural
function, the behavior does not rely on any central representation built up
from all sensors.
In early implementations of the reactive paradigm, the idea of “one sensor,
one behavior” worked well. For more advanced behaviors, it became useful
to fuse the output of multiple sensors within one perceptual schema to have
increased precision or a better measure of the strength of the stimulus. This
type of sensor fusion is permitted within the reactive paradigm as long as
the fusion is local to the behavior. Sensor fusion will be detailed in Ch. 6.

110
4
The Reactive Paradigm
sensor 1
sensor 2
actuators
Perceptual
Schema
percept
Motor
Schema
behavior
Perceptual
Schema
percept
Motor
Schema
behavior
Perceptual
Schema
percept
Motor
Schema
behavior
Figure 4.4
Behavior-speciﬁc sensing organization in the Reactive Paradigm: sensing
is local, sensors can be shared, and sensors can be fused locally by a behavior.
4.2.1
Characteristics and connotations of reactive behaviors
As seen earlier, a reactive robotic system decomposes functionality into be-
haviors, which tightly couple perception to action without the use of inter-
vening abstract (global) representations. This is a broad, vague deﬁnition.
Over the years, the reactive paradigm has acquired several connotations and
characteristics from the way practitioners have used the paradigm.
The primary connotation of a reactive robotic system is that it executes
rapidly. The tight coupling of sensing and acting permits robots to oper-
ate in real-time, moving at speeds of 1-2 cm per second. Behaviors can be
implemented directly in hardware as circuits, or with low computational
complexity algorithms (O(n)). This means that behaviors execute quickly re-
gardless of the processor. Behaviors execute not only fast in their own right,
they are particularly fast when compared to the execution times of Shakey
and the Stanford Cart. A secondary connotation is that reactive robotic sys-
tems have no memory, limiting reactive behaviors to what biologists would
call pure stimulus-response reﬂexes. In practice, many behaviors exhibit a

4.2
Attributes of Reactive Paradigm
111
ﬁxed-action pattern type of response, where the behavior persists for a short
period of time without the direct presence of the stimulus. The main point is
that behaviors are controlled by what is happening in the world, duplicating
the spirit of innate releasing mechanisms, rather than by the program stor-
ing and remembering what the robot did last. The examples in the chapter
emphasize this point.
The ﬁve characteristics of almost all architectures that follow the Reactive
Paradigm are:
1. Robots are situated agents operating in an ecological niche. As seen earlier in
Part I, situated agent means that the robot is an integral part of the world. A
SITUATED AGENT
robot has its own goals and intentions. When a robot acts, it changes the
world, and receives immediate feedback about the world through sens-
ing. What the robot senses affects its goals and how it attempts to meet
them, generating a new cycle of actions. Notice that situatedness is de-
ﬁned by Neisser’s Action-Perception Cycle. Likewise, the goals of a robot,
the world it operates in, and how it can perceive the world form the eco-
logical niche of the robot. To emphasize this, many robotic researchers say
they are working on ecological robotics.
ECOLOGICAL ROBOTICS
2. Behaviors serve as the basic building blocks for robotic actions, and the overall
behavior of the robot is emergent. Behaviors are independent, computational
entities and operate concurrently. The overall behavior is emergent: there
is no explicit “controller” module which determines what will be done, or
functions which call other functions. There may be a coordinated control
program in the schema of a behavior, but there is no external controller
of all behaviors for a task. As with animals, the “intelligence” of the ro-
bot is in the eye of the beholder, rather than in a speciﬁc section of code.
Since the overall behavior of a reactive robot emerges from the way its
individual behaviors interact, the major differences between reactive ar-
chitectures is usually the speciﬁc mechanism for interaction. Recall from
Chapter 3 that these mechanisms include combination, suppression, and
cancellation.
3. Only local, behavior-speciﬁc sensing is permitted. The use of explicit abstract
representational knowledge in perceptual processing, even though it is
behavior-speciﬁc, is avoided. Any sensing which does require represen-
tation is expressed in ego-centric (robot-centric) coordinates. For example,
EGO-CENTRIC
consider obstacle avoidance. An ego-centric representation means that it
does not matter that an obstacle is in the world at coordinates (x,y,z), only

112
4
The Reactive Paradigm
where it is relative to the robot. Sensor data, with the exception of GPS, is
inherently ego-centric (e.g., a range ﬁnder returns a distance to the nearest
object from the transducer), so this eliminates unnecessary processing to
create a world model, then extract the position of obstacles relative to the
robot.
4. These systems inherently follow good software design principles. The modular-
ity of these behaviors supports the decomposition of a task into compo-
nent behaviors. The behaviors are tested independently, and behaviors
may be assembled from primitive behaviors.
5. Animal models of behavior are often cited as a basis for these systems or a par-
ticular behavior. Unlike in the early days of AI robotics, where there was a
conscious effort to not mimic biological intelligence, it is very acceptable
under the reactive paradigm to use animals as a motivation for a collection
of behaviors.
4.2.2
Advantages of programming by behavior
Constructing a robotic system under the Reactive Paradigm is often referred
to as programming by behavior, since the fundamental component of any
implementation is a behavior. Programming by behavior has a number of
advantages, most of them consistent with good software engineering princi-
ples. Behaviors are inherently modular and easy to test in isolation from the
system (i.e., they support unit testing). Behaviors also support incremental
expansion of the capabilities of a robot. A robot becomes more intelligent by
having more behaviors. The behavioral decomposition results in an imple-
mentation that works in real-time and is usually computationally inexpen-
sive. Although we’ll see that sometimes duplicating specialized detectors
(like optic ﬂow) is slow. If the behaviors are implemented poorly, then a re-
active implementation can be slow. But generally, the reaction speeds of a
reactive robot are equivalent to stimulus-response times in animals.
Behaviors support good software engineering principles through decom-
position, modularity and incremental testing. If programmed with as high
a degree of independence (also called low coupling) as possible, and high co-
LOW COUPLING
HIGH COHESION
hesion, the designer can build up libraries of easy to understand, maintain,
and reuse modules that minimize side effects. Low coupling means that the
modules can function independently of each other with minimal connections
or interfaces, promoting easy reuse. Cohesion means that the data and op-
erations contained by a module relate only to the purpose of that module.

4.3
Subsumption Architecture
113
Higher cohesion is associated with modules that do one thing well, like the
SQRT function in C. The examples in Sec. 4.3 and 4.4 attempt to illustrate the
choices a designer has in engineering the behavioral software of a robot.
4.2.3
Representative architectures
In order to implement a reactive system, the designer must identify the set
of behaviors necessary for the task. The behaviors can either be new or use
existing behaviors. The overall action of the robot emerges from multiple,
concurrent behaviors. Therefore a reactive architecture must provide mecha-
nisms for 1) triggering behaviors and 2) for determining what happens when
multiple behaviors are active at the same time. Another distinguishing fea-
ture between reactive architectures is how they deﬁne a behavior and any
special use of terminology. Keep in mind that the deﬁnitions presented in
Sec. 4.2 are a generalization of the trends in reactive systems, and do not
necessarily have counterparts in all architectures.
There are many architectures which ﬁt in the Reactive Paradigm. The two
best known and most formalized are the subsumption and potential ﬁeld
methodologies. Subsumption refers to how behaviors are combined. Poten-
tial Field Methodologies require behaviors to be implemented as potential
ﬁelds, and the behaviors are combined by summation of the ﬁelds. A third
style of reactive architecture which is popular in Europe and Japan is rule
RULE ENCODING
encoding, where the motor schema component of behaviors and the com-
bination mechanism are implemented as logical rules. The rules for com-
bining behaviors are often ad hoc, and so will not be covered in this book.
Other methods for combining behaviors exist, including fuzzy methods and
winner-take-all voting, but these tend to be implementation details rather
than an over-arching architecture.
4.3
Subsumption Architecture
Rodney Brooks’ subsumption architecture is the most inﬂuential of the purely
Reactive Paradigm systems. Part of the inﬂuence stems from the publicity
surrounding the very naturalistic robots built with subsumption. As seen
in Fig. 4.5, these robots actually looked like shoe-box sized insects, with
six legs and antennae.
In many implementations, the behaviors are em-
bedded directly in the hardware or on small micro-processors, allowing the
robots to have all on-board computing (this was unheard of in the processor-
impoverished mid-1980’s). Furthermore, the robots were the ﬁrst to be able

114
4
The Reactive Paradigm
Figure 4.5
“Veteran” robots of the MIT AI Laboratory using the subsumption archi-
tecture. (Photograph courtesy of the MIT Artiﬁcial Intelligence Laboratory.)
to walk, avoid collisions, and climb over obstacles without the “move-think-
move-think” pauses of Shakey.
The term “behavior” in the subsumption architecture has a less precise
meaning than in other architectures. A behavior is a network of sensing and
acting modules which accomplish a task. The modules are augmented ﬁnite
state machines AFSM, or ﬁnite state machines which have registers, timers,
and other enhancements to permit them to be interfaced with other modules.
An AFSM is equivalent to the interface between the schemas and the coor-
dinated control strategy in a behavioral schema. In terms of schema theory,
a subsumption behavior is actually a collection of one or more schemas into
an abstract behavior.
Behaviors are released in a stimulus-response way, without an external
program explicitly coordinating and controlling them. Four interesting as-
pects of subsumption in terms of releasing and control are:
1. Modules are grouped into layers of competence. The layers reﬂect a hi-
LAYERS OF
COMPETENCE
erarchy of intelligence, or competence. Lower layers encapsulate basic
survival functions such as avoiding collisions, while higher levels create

4.3
Subsumption Architecture
115
more goal-directed actions such as mapping. Each of the layers can be
viewed as an abstract behavior for a particular task.
2. Modules in a higher layer can override, or subsume, the output from be-
LAYERS CAN SUBSUME
LOWER LAYERS
haviors in the next lower layer. The behavioral layers operate concur-
rently and independently, so there needs to be a mechanism to handle
potential conﬂicts. The solution in subsumption is a type of winner-take-
all, where the winner is always the higher layer.
3. The use of internal state is avoided. Internal state in this case means any
NO INTERNAL STATE
type of local, persistent representation which represents the state of the
world, or a model. Because the robot is a situated agent, most of its in-
formation should come directly from the world. If the robot depends on
an internal representation, what it believes may begin to dangerously di-
verge from reality. Some internal state is needed for releasing behaviors
like being scared or hungry, but good behavioral designs minimize this.
4. A task is accomplished by activating the appropriate layer, which then
activates the lower layers below it, and so on. However, in practice, sub-
sumption style systems are not easily taskable, that is, they can’t be ordered
TASKABLE
to do another task without being reprogrammed.
4.3.1
Example
These aspects are best illustrated by an example, extensively modiﬁed from
Brooks’ original paper27 in order to be consistent with schema theory termi-
nology and to facilitate comparison with a potential ﬁelds methodology. A
robot capable of moving forward while not colliding with anything could be
represented with a single layer, Level 0. In this example, the robot has mul-
LEVEL 0: AVOID
tiple sonars (or other range sensors), each pointing in a different direction,
and two actuators, one for driving forward and one for turning.
Following Fig. 4.6, the SONAR module reads the sonar ranges, does any
ﬁltering of noise, and produces a polar plot. A polar plot represents the range
POLAR PLOT
readings in polar coordinates,
(r
;

), surrounding the robot. As shown in
Fig. 4.7, the polar plot can be “unwound.”
If the range reading for the sonar facing dead ahead is below a certain
threshold, the COLLIDE module declares a collision and sends the halt signal
to the FORWARD drive actuator. If the robot was moving forward, it now
stops. Meanwhile, the FEELFORCE module is receiving the same polar plot.
It treats each sonar reading as a repulsive force, which can be represented

116
4
The Reactive Paradigm
polar
plot
force
heading
halt
TURN
FORWARD
FEEL
FORCE
RUN
AWAY
SONAR
COLLIDE
heading
encoders
Figure 4.6
Level 0 in the subsumption architecture.
0
1
2
3
4
5
6
7
0
sonar number
range reading
1
2
3
4
5
6
7
a.
b.
Figure 4.7
Polar plot of eight sonar range readings: a.) “robo-centric” view of range
readings along acoustic axes, and b.) unrolled into a plot.
as a vector. Recall that a vector is a mathematical construct that consists of
a magnitude and a direction. FEELFORCE can be thought of as summing
the vectors from each of the sonar readings. This results in a new vector.
The repulsive vector is then passed to the TURN module. The TURN module
splits off the direction to turn and passes that to the steering actuators. TURN
also passes the vector to the FORWARD module, which uses the magnitude of
the vector to determine the magnitude of the next forward motion (how far
or how fast). So the robot turns and moves a short distance away from the
obstacle.
The observable behavior is that the robot will sit still if it is in an unoccu-
pied space, until an obstacle comes near it. If the obstacle is on one side of

4.3
Subsumption Architecture
117
Figure 4.8
Level 0 recast as primitive behaviors.
the robot, the robot will turn
0
 the other way and move forward; essen-
tially, it runs away. This allows a person to herd the robot around. The robot
can react to an obstacle if the obstacle (or robot) is motionless or moving; the
response is computed at each sensor update.
However, if part of the obstacle, or another obstacle, is dead ahead (some-
one tries to herd the robot into a wall), the robot will stop, then apply the
results of RUNAWAY. So it will stop, turn and begin to move forward again.
Stopping prevents the robot from side-swiping the obstacle while it is turn-
ing and moving forward. Level 0 shows how a fairly complex set of actions
can emerge from very simple modules.
It is helpful to recast the subsumption architecture in the terms used in this
book, as shown in Fig. 4.8. Note how this looks like the vertical decompo-
sition in Fig. 4.2: the sensor data ﬂows through the concurrent behaviors to
the actuators, and the independent behaviors cause the robot to do the right
thing. The SONAR module would be considered a global interface to the sen-
sors, while the TURN and FORWARD modules would be considered part of
the actuators (an interface). For the purposes of this book, a behavior must
consist of a perceptual schema and a motor schema. Perceptual schemas are
connected to a sensor, while motor schemas are connected to actuators. For
Level 0, the perceptual schemas would be contained in the FEELFORCE and
COLLIDE modules. The motor schemas are RUNAWAY and COLLIDE mod-
ules. COLLIDE combines both perceptual processing (extracts the vector for
the sonar facing directly ahead) and the pattern of action (halt if there is a

118
4
The Reactive Paradigm
polar
plot
force
heading
halt
TURN
FORWARD
FEEL
FORCE
RUN
AWAY
SONAR
COLLIDE
heading
encoders
WANDER
AVOID
heading
force
S
modified
heading
Figure 4.9
Level 1: wander.
reading). The primitive behaviors reﬂect the two paths through the layer.
One might be called the runaway behavior and the other the collide behav-
ior. Together, the two behaviors create a rich obstacle avoidance behavior, or
a layer of competence.
It should also be noticed that the behaviors used direct perception, or af-
fordances. The presence of a range reading indicated there was an obstacle;
the robot did not have to know what the obstacle was.
Consider building a robot which actually wandered around instead of sit-
ting motionless, but was still able to avoid obstacles. Under subsumption, a
second layer of competence (Level 1) would be added, shown in Fig. 4.9. In
LEVEL 1: WANDER
this case, Level 1 consists of a WANDER module which computes a random
heading every
n seconds. The random heading can be thought of as a vector.
It needs to pass this heading to the TURN and FORWARD modules. But it can’t
be passed to the TURN module directly. That would sacriﬁce obstacle avoid-
ance, because TURN only accepts one input. One solution is to add another
module in Level 1, AVOID, which combines the FEELFORCE vector with the
WANDER vector. Adding a new avoid module offers an opportunity to create
a more sophisticated response to obstacles. AVOID combines the direction of
the force of avoidance with the desired heading. This results in the actual
heading being mostly in the right direction rather than having the robot turn

4.3
Subsumption Architecture
119
Figure 4.10
Level 1 recast as primitive behaviors.
around and lose forward progress. (Notice also that the AVOID module was
able to “eavesdrop” on components of the next lower layer.) The heading
output from AVOID has the same representation as the output of RUNAWAY,
so TURN can accept from either source.
The issue now appears to be when to accept the heading vector from which
layer. Subsumption makes it simple: the output from the higher level sub-
sumes the output from the lower level. Subsumption is done in one of two
ways:
1. inhibition. In inhibition, the output of the subsuming module is connected
INHIBITION
to the output of another module. If the output of the subsuming module
is “on” or has any value, the output of the subsumed module is blocked
or turned “off.” Inhibition acts like a faucet, turning an output stream on
and off.
2. suppression. In suppression, the output of of the subsuming module is
SUPPRESSION
connected to the input of another module. If the output of the subsum-
ing module is on, it replaces the normal input to the subsumed module.
Suppression acts like a switch, swapping one input stream for another.
In this case, the AVOID module suppresses (marked in the diagram with
a S) the output from RUNAWAY. RUNAWAY is still executing, but its output
doesn’t go anywhere. Instead, the output from AVOID goes to TURN.

120
4
The Reactive Paradigm
The use of layers and subsumption allows new layers to be built on top
of less competent layers, without modifying the lower layers. This is good
software engineering, facilitating modularity and simplifying testing. It also
adds some robustness in that if something should disable the Level 1 behav-
iors, Level 0 might remain intact. The robot would at least be able to preserve
its self-defense mechanism of ﬂeeing from approaching obstacles.
Fig. 4.10 shows Level 1 recast as behaviors. Note that FEELFORCE was
used by both RUNAWAY and AVOID. FEELFORCE is the perceptual component
(or schema) of both behaviors, with the AVOID and RUNAWAY modules being
the motor component (or schema). As is often the case, behaviors are usu-
ally named after the observable action. This means that the behavior (which
consists of perception and action) and the action component have the same
name. The ﬁgure does not show that the AVOID and RUNAWAY behaviors
share the same FEELFORCE perceptual schema. As will be seen in the next
chapter, the object-oriented properties of schema theory facilitate the reuse
and sharing of perceptual and motor components.
Now consider adding a third layer to permit the robot to move down cor-
LEVEL 2: FOLLOW
CORRIDORS
ridors, as shown in Fig. 4.11. (The third layer in Brooks’ original paper is
“explore,” because he was considering a mapping task.) The LOOK mod-
ule examines the sonar polar plot and identiﬁes a corridor. (Note that this
is another example of behaviors sharing the same sensor data but using it
locally for different purposes.) Because identifying a corridor is more com-
putationally expensive than just extracting range data, LOOK may take longer
to run than behaviors at lower levels. LOOK passes the vector representing
the direction to the middle of the corridor to the STAYINMIDDLE module.
STAYINMIDDLE subsumes the WANDER module and delivers its output to
the AVOID module which can then swerve around obstacles.
But how does the robot get back on course if the LOOK module has not
computed a new direction? In this case, the INTEGRATE module has been
observing the robots actual motions from shaft encoders in the actuators.
This gives an estimate of how far off course the robot has traveled since the
last update by LOOK. STAYINMIDDLE can use the dead reckoning data with
the intended course to compute the new course vector. It serves to ﬁll in
the gaps in mismatches between updates rates of the different modules. No-
tice that LOOK and STAYINMIDDLE are quite sophisticated from a software
perspective.
INTEGRATE is an example of a module which is supplying a dangerous
internal state: it is actually substituting for feedback from the real world. If
for some reason, the LOOK module never updates, then the robot could op-

4.3
Subsumption Architecture
121
polar
plot
force
heading
halt
TURN
FORWARD
FEEL
FORCE
RUN
AWAY
SONAR
COLLIDE
heading
encoders
WANDER
AVOID
force
S
modified
heading
LOOK
STAYIN
MIDDLE
corridor
heading
to middle
S
INTEGRATE
distance, direction traveled
Figure 4.11
Level 2: follow corridors.
erate without any sensor data forever. Or at least until it crashed! Therefore,
subsumption style systems include time constants on suppression and inhi-
bition. If the suppression from STAYINMIDDLE ran for longer than
n seconds
with out a new update, the suppression would cease. The robot would then
begin to wander, and hopefully whatever problem (like the corridor being
totally blocked) that had led to the loss of signal would ﬁx itself.
Of course, a new problem is how does the robot know that it hasn’t started
going down the hallway it just came up? Answer: it doesn’t. The design
assumes that that a corridor will always be present in the robot’s ecological
niche. If it’s not, the robot does not behave as intended. This is an example
of the connotation that reactive systems are “memory-less.”
4.3.2
Subsumption summary
To summarize subsumption:
 Subsumption has a loose deﬁnition of behavior as a tight coupling of sens-
ing and acting. Although it is not a schema-theoretic architecture, it can

122
4
The Reactive Paradigm
be described in those terms. It groups schema-like modules into layers of
competence, or abstract behaviors.
 Higher layers may subsume and inhibit behaviors in lower layers, but be-
haviors in lower layers are never rewritten or replaced. From a program-
ming standpoint, this may seem strange. However, it mimics biological
evolution. Recall that the ﬂeeing behavior in frogs (Ch. 3) was actually the
result of two behaviors, one which always moved toward moving objects
and the other which actually suppressed that behavior when the object
was large.
 The design of layers and component behaviors for a subsumption imple-
mentation, as with all behavioral design, is hard; it is more of an art than
a science. This is also true for all reactive architectures.
 There is nothing resembling a STRIPS-like plan in subsumption. Instead,
behaviors are released by the presence of stimulus in the environment.
 Subsumption solves the frame problem by eliminating the need to model
the world. It also doesn’t have to worry about the open world being
non-monotonic and having some sort of truth maintenance mechanism,
because the behaviors do not remember the past. There may be some
perceptual persistence leading to a ﬁxed-action pattern type of behavior
(e.g., corridor following), but there is no mechanism which monitors for
changes in the environment. The behaviors simply respond to whatever
stimulus is in the environment.
 Perception is largely direct, using affordances. The releaser for a behavior
is almost always the percept for guiding the motor schema.
 Perception is ego-centric and distributed. In the wander (layer 2) exam-
ple, the sonar polar plot was relative to the robot. A new polar plot was
created with each update of the sensors. The polar plot was also avail-
able to any process which needed it (shared global memory), allowing
user modules to be distributed. Output from perceptual schemas can be
shared with other layers.
4.4
Potential Fields Methodologies
Another style of reactive architecture is based on potential ﬁelds. The spe-
ciﬁc architectures that use some type of potential ﬁelds are too numerous to

4.4
Potential Fields Methodologies
123
describe here, so instead a generalization will be presented. Potential ﬁeld
styles of behaviors always use vectors to represent behaviors and vector sum-
VECTORS
VECTOR SUMMATION
mation to combine vectors from different behaviors to produce an emergent
behavior.
4.4.1
Visualizing potential ﬁelds
The ﬁrst tenet of a potential ﬁelds architecture is that the motor action of a
behavior must be represented as a potential ﬁeld. A potential ﬁeld is an array,
or ﬁeld, of vectors. As described earlier, a vector is a mathematical construct
which consists of a magnitude and a direction. Vectors are often used to
represent a force of some sort. They are typically drawn as an arrow, where
the length of the arrow is the magnitude of the force and the angle of the
arrow is the direction. Vectors are usually represented with a boldface capital
letter, for example, V. A vector can also be written as a tuple
(m;
d), where
m
stands for magnitude and
d for direction. By convention the magnitude is a
real number between 0.0 and 1, but the magnitude can be any real number.
The array represents a region of space. In most robotic applications, the
ARRAY REPRESENTING
A FIELD
space is in two dimensions, representing a bird’s eye view of the world just
like a map. The map can be divided into squares, creating a (x,y) grid. Each
element of the array represents a square of space. Perceivable objects in the
world exert a force ﬁeld on the surrounding space. The force ﬁeld is anal-
ogous to a magnetic or gravitation ﬁeld. The robot can be thought of as a
particle that has entered the ﬁeld exuded by an object or environment. The
vector in each element represents the force, both the direction to turn and the
magnitude or velocity to head in that direction, a robot would feel if it were
at that particular spot. Potential ﬁelds are continuous because it doesn’t mat-
ter how small the element is; at each point in space, there is an associated
vector.
Fig. 4.12 shows how an obstacle would exert a ﬁeld on the robot and make
it run away. If the robot is close to the obstacle, say within 5 meters, it is inside
the potential ﬁeld and will fell a force that makes it want to face directly away
from the obstacle (if it isn’t already) and move away. If the robot is not within
range of the obstacle, it just sits there because there is no force on it. Notice
that the ﬁeld represents what the robot should do (the motor schema) based
on if the robot perceives an obstacle (the perceptual schema). The ﬁeld isn’t
concerned with how the robot came to be so close to the obstacle; the robot
feels the same force if it were happening to move within range or if it was
just sitting there and someone put their hand next to the robot.

124
4
The Reactive Paradigm
Figure 4.12
Example of an obstacle exerting a repulsive potential ﬁeld over the ra-
dius of 1 meter.
One way of thinking about potential ﬁelds is to imagine a force ﬁeld acting
on the robot. Another way is to think of them as a potential energy surface
in three dimensions (gravity is often represented this way) and the robot as
a marble. In that case, the vector indicates the direction the robot would
“roll” on the surface. Hills in the surface cause the robot to roll away or
around (vectors would be pointing away from the “peak” of the hill), and
valleys would cause the robot to roll downward (vectors pointing toward
the bottom).
There are ﬁve basic potential ﬁelds, or primitives, which can be combined
FIVE PRIMITIVE FIELDS
to build more complex ﬁelds: uniform, perpendicular, attractive, repulsive, and
tangential. Fig. 4.13 shows a uniform ﬁeld. In a uniform ﬁeld, the robot would
UNIFORM FIELD
feel the same force no matter where it was. No matter where it got set down
and at what orientation, it would feel a need to turn to align itself to the
direction the arrow points and to move in that direction at a velocity propor-

4.4
Potential Fields Methodologies
125
Figure 4.13
Five primitive potential ﬁelds: a.) uniform, b.) perpendicular, c.) attrac-
tion, d.) repulsion, and e.) tangential.
tional to the length of the arrow. A uniform ﬁeld is often used to capture the
behavior of “go in direction
n
.”
Fig. 4.13b shows a perpendicular ﬁeld, where the robot is oriented perpen-
PERPENDICULAR FIELD
dicular to some object or wall or border The ﬁeld shown is directed away
from the gray wall, but a perpendicular ﬁeld can be pointed towards an ob-
ject as well.
Fig. 4.13c illustrates an attractive ﬁeld. The circle at the center of the ﬁeld
ATTRACTIVE FIELD
represents an object that is exerting an attraction on the robot. Wherever the
robot is, the robot will “feel” a force relative to the object. Attractive ﬁelds
are useful for representing a taxis or tropism, where the agent is literally
attracted to light or food or a goal. The opposite of an attractive ﬁeld is a re-
pulsive ﬁeld, shown in Fig. 4.13d. Repulsive ﬁelds are commonly associated
with obstacles, or things the agent should avoid. The closer the robot is to
the object, the stronger the repulsive force
0
 away from it.
The ﬁnal primitive ﬁeld is the tangential ﬁeld in Fig. 4.13e. The ﬁeld is a
TANGENTIAL FIELD
tangent around the object (think of a tangent vector as being perpendicular to

126
4
The Reactive Paradigm
radial lines extending outward from the object). Tangential ﬁelds can “spin”
either clockwise or counterclockwise; Fig. 4.13 shows a clockwise spin. They
are useful for directing a robot to go around an obstacle, or having a robot
investigate something.
4.4.2
Magnitude proﬁles
Notice that in Fig. 4.13, the length of the arrows gets smaller closer to the
object. The way the magnitude of vectors in the ﬁeld change is called the
magnitude proﬁle. (The term “magnitude proﬁle” is used here because the
MAGNITUDE PROFILE
term “velocity proﬁle” is used by control engineers to describe how a robot’s
motors actually accelerate and decelerate to produce a particular movement
without jerking.)
Consider the repulsive ﬁeld in Fig. 4.12. Mathematically, the ﬁeld can be
represented with polar coordinates and the center of the ﬁeld being the origin
(0,0):
V
dir
ection
=
 
(4.1)
V
mag
nitude
=
c
In that case, the magnitude was a constant value,
c: the length of the ar-
rows was the same. This can be visualized with a plot of the magnitude
shown in Fig. 4.14a.
This proﬁle says that the robot will run away (the direction it will run
is
 ) at the same velocity, no matter how close it is to the object, as long
as it is in the range of the obstacle. As soon as the robot gets out of range
of the obstacle, the velocity drops to 0.0, stopping the robot. The ﬁeld is
essentially binary: the robot is either running away at a constant speed or
stopped. In practice there is a problem with a constant magnitude. It leads
to jerky motion on the perimeter of the range of the ﬁeld. This is illustrated
when a robot is heading in a particular direction, then encounters an obsta-
cle. It runs away, leaving the ﬁeld almost immediately, and turns back to its
original path, encounters the ﬁeld again, and so on.
Magnitude proﬁles solve the problem of a constant magnitude. They also
make it possible for a robot designer to represent reﬂexivity (that a response
REFLEXIVITY
should be proportional to the strength of a stimulus) and to create interesting
responses. Now consider the proﬁle in Fig. 4.13c. It can be described as how

4.4
Potential Fields Methodologies
127
a.
b.
c.
Figure 4.14
Plots of magnitude proﬁles for a ﬁeld of radius 5 units: a.) constant
magnitude, b.) linear drop off with a slope of -1, and c.) exponential drop off.

128
4
The Reactive Paradigm
an observer would see a robot behave in that ﬁeld: if the robot is far away
from the object, it will turn and move quickly towards it, then slow up to
keep from overshooting and hitting the object. Mathematically, this is called
a linear drop off, since the rate at which the magnitude of the vectors drops off
LINEAR DROP OFF
can be plotted as a straight line. The formula for a straight line is
y
=
mx
+
b,
where
x is the distance and
y is magnitude.
b biases where the line starts,
and
m is the slope (m
=

y

x ). Any value of
m and
b is acceptable. If it is not
speciﬁed,
m
=
 or -1 (a

 slope up or down) and
b
=
0 in linear functions.
The linear proﬁle in Fig. 4.14b matches the desired behavior of the de-
signer: to have the robot react more, the closer it is. But it shares the prob-
lem of the constant magnitude proﬁle in the sharp transition to 0.0 velocity.
Therefore, another proﬁle might be used to capture the need for a strong
reaction but with more of a taper. One such proﬁle is a exponential drop off
EXPONENTIAL DROP
OFF
function, where the drop off is proportional to the square of the distance: for
every unit of distance away from the object, the force on the robot drops in
half. The exponential proﬁle is shown in Fig. 4.14c.
As can be seen from the previous examples, almost any magnitude proﬁle
is acceptable. The motivation for using magnitude proﬁles is to ﬁne-tune the
behavior. It is important to note that the robot only computes the vectors
acting on it at its current location. The ﬁgures display the entire ﬁeld for
all possible locations of the robot. The question then arises as to why do
the ﬁgures show an entire ﬁeld over space? First, it aids visualizing what
the robot will do overall, not just at one particular time step. Second, since
ﬁelds are continuous representations, it simpliﬁes conﬁrming that the ﬁeld is
correct and makes any abrupt transitions readily apparent.
4.4.3
Potential ﬁelds and perception
In the previous examples, the force of the potential ﬁeld at any given point
was a function of both the relative distance between the robot and an ob-
ject and the magnitude proﬁle. The strength of a potential ﬁeld can be a
function of the stimulus, regardless of distance. As an example recall from
Ch. 3 the feeding behavior of baby arctic terns where the feeding behavior
is guided by the stimulus “red.” This can be modeled by an attractive ﬁeld.
The bigger and redder an object in the baby’s ﬁeld of view, the stronger the
attraction, suggesting that a magnitude proﬁle using an increasing exponen-
tial function. Another important point that has already been mentioned is
that potential ﬁelds are ego-centric because robot perception is ego-centric.

4.4
Potential Fields Methodologies
129
4.4.4
Programming a single potential ﬁeld
Potential ﬁelds are actually easy to program, especially since the ﬁelds are
ego-centric to the robot. The visualization of the entire ﬁeld may appear
to indicate that the robot and the objects are in a ﬁxed, absolute coordinate
system, but they are not. The robot computes the effect of the potential ﬁeld,
usually as a straight line, at every update, with no memory of where it was
previously or where the robot has moved. This should become clear through
the following examples.
A primitive potential ﬁeld is usually represented by a single function. The
vector impacting the robot is computed each update. Consider the case of a
robot with a single range sensor facing forward. The designer has decided
that a repulsive ﬁeld with a linear drop off is appropriate. The formula is:
V
dir
ection
=
 0

(4.2)
V
mag
nitude
=
(
(D
 d)
D
for
d
<=
D
0
for
d
>
D
where
D is the maximum range of the ﬁeld’s effect, or the maximum dis-
tance at which the robot can detect the obstacle. ( D isn’t always the detection
range. It can be the range at which the robot should respond to a stimulus.
For example, many sonars can detect obstacles 20 feet away, producing an al-
most inﬁnitesimal response in emergent behavior but requiring the runtime
overhead of a function call. In practice, a roboticist might set a
D of 2 meters.)
Notice that the formula produces a result where 0.0

V
mag
nitude
 1.0.
Below is a C code fragment that captures the repulsive ﬁeld.
typedef struct {
double
magnitude;
double direction;
} vector;
vector repulsive(double d, double D)
{
if (d <= D) {
outputVector.direction = -180; //turn around!
outputVector.magnitude = (D-d)/D; //linear dropoff
}
else {

130
4
The Reactive Paradigm
outputVector.direction=0.0
outputVector.magnitude=0.0
}
return outputVector;
}
At this point, it is easy to illustrate how a potential ﬁeld can be used by a
behavior, runaway, for the robot with a single sensor. The runaway behav-
ior will use the repulsive() function as the motor schema, and a function
readSonar() as the perceptual schema. The output of the behavior is a
vector. runaway is called by the robot on every update cycle.
vector runaway( ){
double reading;
reading=readSonar();//perceptual schema
vector=repulsive (reading, MAX_DISTANCE); //motor schema
return Voutput;
}
while (robot==ON)
{
Vrunaway=runaway(reading); // motor schema
turn(Vrunaway.direction);
forward(Vrunaway.magnitude*MAX_VELOCITY);
}
4.4.5
Combination of ﬁelds and behaviors
As stated earlier in the chapter, the ﬁrst attribute of a true potential ﬁelds
methodology is that it requires all behaviors to be implemented as potential
ﬁelds. The second attribute is that it combines behaviors not by one sub-
suming others, but by vector summation. A robot will generally have forces
acting on it from multiple behaviors, all acting concurrently. This section
provides two examples of how multiple behaviors arise and how they are
implemented and combined.
The ﬁrst example is simple navigation, where a robot is heading for a goal
(speciﬁed as “10.3m in direction
”) and encounters an obstacle. The motor

4.4
Potential Fields Methodologies
131
Figure 4.15
A bird’s eye view of a world with a goal and obstacle, and the two active
behaviors for the robot who will inhabit this world.
schema of the move2goal behavior is represented with an attractive poten-
tial ﬁeld, which uses the shaft encoders on the robot to tell if it has reached
the goal position. The runaway behavior is a repulsive ﬁeld and uses a range
sensor to detect if something is in front of it. Fig. 4.15 shows a bird’s eye view
of an area, and shows a visualization of the potential ﬁeld. The move2goal
behavior in Fig. 4.16b exerts an attractive ﬁeld over the entire space; where
ever the robot is, it will feel a force from the goal. The runaway behavior in
Fig. 4.15 exerts a repulsive ﬁeld in a radius around the obstacle (technically
the repulsive ﬁeld extends over all of the space as does the move2goal, but
the magnitude of the repulsion is 0.0 beyond the radius). The combined ﬁeld
is shown in Fig. 4.16c.
Now consider the emergent behavior of the robot in the ﬁeld if it starts in
the lower right corner, shown in Fig. 4.17. At time
t
0, the robot senses the
world. It can only perceive the goal and cannot perceive the obstacle, so the
only vector it feels is attraction (runaway returns a vector with magnitude

132
4
The Reactive Paradigm
Figure 4.16
Potential ﬁelds from the world in Fig. 4.15: a.) repulsive from the obsta-
cle, b.) attractive from the goal, and c.) combined.
of 0.0). It moves on a straight line for the goal. At
t
, it updates its sensors
and now perceives both the goal and the obstacle. Both behaviors contribute
a vector; the vectors are summed and the robot now moves off course. At
t
,
the robot has almost moved beyond the obstacle and the goal is exerting the
stronger force. At
t
, it resumes course and reaches the goal.
The example illustrates other points about potential ﬁelds methods: the
impact of update rates, holonomicity, and local minima. Notice that the dis-
tance (length of the arrows) between updates is different. That is due to
the changes in the magnitude of the output vector, which controls the robot
velocity. If the robot has a “shorter” vector, it travels more slowly and, there-
fore, covers less distance in the same amount of time. It can also “overshoot”
as seen between
t
 and
t
 where the robot actually goes farther without turn-
ing and has to turn back to go to the goal. As a result, the path is jagged
with sharp lines. The resulting path will be smoother if the robot has a faster

4.4
Potential Fields Methodologies
133
Figure 4.17
Path taken by the robot.
update interval. Another aspect of the update rate is that the robot can over-
shoot the goal, especially if it is using shaft encoders (the goal is 10.3 meters
from where the robot started). Sometimes designers use attractive ﬁelds with
a magnitude that drops off as the robot approaches, slowing it down so the
that the robot can tell with it has reached the goal. (Programmers usually put
a tolerance around the goal location, for example instead of 10.3m, the goal
is 10.3m+/- 0.5m.)
Potential ﬁelds treat the robot as if it were a particle that could change ve-
locity and direction instantaneously. This isn’t true for real robots. Research
robots such as Kheperas (shown in Fig. 4.18) can turn in any direction in
place, but they have to be stopped and there is a measurable amount of error
due to the contact between the wheels and the surface. Many robots have
Ackerman, or automobile, steering, and anyone who has tried to parallel
park an automobile knows that a car can go only in certain directions.
A third problem is that the ﬁelds may sum to 0.0. Returning to Fig. 4.16,
draw a line between the Goal and the Obstacle. Along that line behind the
Obstacle, the vectors have only a head (direction of the arrow) and no body
(length of the arrow). This means that the magnitude is 0.0 and that if the
robot reaches that spot, it will stop and not move again. This is called the
local minima problem, because the potential ﬁeld has a minima, or valley,
that traps the robot. Solutions to the local minima problem will be described
at the end of the chapter.

134
4
The Reactive Paradigm
Figure 4.18
Khepera miniature robot: a.) a khepera on a ﬂoppy disk, b.) IR sensors
(small black squares) along the “waistline,” and c.) orientation of the IR sensors.
4.4.6
Example using one behavior per sensor
As another example of how powerful this idea of vector summation is, it
is useful to consider how obstacle avoidance runaway is commonly imple-
mented on real robots. Fig. 4.18 shows a layout of the IR range sensors on a
Khepera robot. Since the sensors are permanently mounted on the platform,
their angle

i relative to the front is known. If a sensor receives a range
reading, something is in front of that sensor. Under a repulsive ﬁeld RUN-
AWAY, the output vector will be
0
 opposite

i. The IR sensor isn’t capable
of telling that the obstacle may be a little off the sensor axis, so a reading is
treated as if an obstacle was straight in front of that sensor and perpendicular
to it.
If this sensor were the only sensor on the robot, the RUNAWAY behavior is
very straightforward. But what if, as in the case of a Khepera, the robot has

4.4
Potential Fields Methodologies
135
multiple range sensors? Bigger obstacles will be detected by multiple sensors
at the same time. The common way is to have a RUNAWAY behavior for each
sensor. This called multiple instantiations of the same behavior. Below is
a code fragment showing multiple instantiations; all that had to be done is
add a for loop to poll each sensor. This takes advantage of two properties
of vector addition: it is associative (a+b+c+d can be performed as
((a
+
b)
+
c)
+
d), and it is commutative (doesn’t matter what order the vectors are
summed).
while (robot==ON) {
vector.mag=vector.dir=0.0; //initialize to 0
for (i=0; i<=numberIR; i++) {
vectorCurrent=Runaway(i); // accept a sensor number
vectorOutput = VectorSum(tempVector,vectorCurrent);
}
turn(vector.direction);
forward(vector.magnitude*MAX-VELOCITY);
}
As seen in Fig. 4.19, the robot is able to get out of the cave-like trap called
a box canyon without building a model of the wall. Each instance contributes
BOX CANYON
a vector, some of which have a X or Y component that cancels out.
From an ethological perspective, the above program is elegant because it
is equivalent to behavioral instantiations in animals. Recall from Ch. 3 the
model of rana computrix and its real-life toad counterpart where each eye
sees and responds to a ﬂy independently of the other eye. In this case, the
program is treating the robot as if it had 8 independent eyes!
From a robotics standpoint, the example illustrates two important points.
First, the direct coupling of sensing to action works.
Second, behavioral
programming is consistent with good software engineering practices. The
RUNAWAY function exhibits functional cohesion, where the function does one
FUNCTIONAL
COHESION
thing well and every statement in the function has something directly to do
with the function’s purpose.122 Functional cohesion is desirable, because it
means the function is unlikely to introduce side effects into the main program
or be dependent on another function. The overall organization shows data
DATA COUPLING
coupling, where each function call takes a simple argument.122 Data coupling
is good, because it means all the functions are independent; for example, the
program can be easily changed to accommodate more IRs sensors.

136
4
The Reactive Paradigm
Figure 4.19
Khepera in a box canyon a.) range readings and b.) vectors from each
instance of Runaway (gray) and the summed output vector.
The alternative to multiple instantiation is to have the perceptual schema
for RUNAWAY process all 8 range readings. One approach is to sum all 8 vec-
tors internally. (As an exercise show that the resulting vector is the same.)
This is not as elegant from a software engineering perspective because the
code is now speciﬁc to the robot (the function is said to have procedural co-
PROCEDURAL
COHESION
hesion),122 and can be used only with a robot that has eight range sensors at
those locations. Another approach, which produces a different emergent be-
havior, is to have the perceptual schema return the direction and distance of
the single largest range reading. This makes the behavior more selective.
4.4.7
Pﬁelds compared with subsumption
How can simple behaviors form a more complicated emergent behavior? The
same way more complicated potential ﬁelds are constructed from primitive
ﬁelds: combining multiple instantiations of primitive behaviors. This can be

4.4
Potential Fields Methodologies
137
seen by revisiting the example behaviors used to describe the subsumption
architecture. In the case of Level 0 in subsumption, if there are no obstacles
within range, the robot feels no repulsive force and is motionless. If an obsta-
cle comes within range and is detected by more than one sonar, each of the
sonar readings create a vector, pointing the robot in the opposite direction.
In the subsumption example, it could be imagined that these vectors were
summed in the RUNAWAY module as shown in Fig. 4.20. In a potential ﬁelds
system, each sonar reading would release an instance of the RUNAWAYpf be-
havior (the “pf” will be used to make it clear which runaway is being referred
to). The RUNAWAYpf behavior uses a repulsive potential ﬁeld. The output
vectors would then be summed, and then the resultant vector would be used
to guide the turn and forward motors.
while (robot==ON)
{
vector.magnitude=vector.direction=0;
for (i=0; i<=numberSonars; i++) {
reading=readSonar();
//perceptual schema
currentVector=runaway(reading); // motor schema
vector = vectorSum(vector, currentVector);
}
turn(vector.direction);
forward(vector.magnitude*MAX-VELOCITY);
}
The COLLIDE module in subsumption does not map over to a behavior
in a potential ﬁelds methodology. Recall that the purpose of COLLIDE is
to stop the robot if it touches an obstacle; in effect, if the RUNAWAY behav-
ior has failed. This ﬁts the deﬁnition of a behavior: it has a sensory input
(range to obstacle = 0) and a recognizable pattern of motor activity (stop).
But it doesn’t produce a potential ﬁeld, unless a uniform ﬁeld of vectors
with 0 magnitude is permissible. If it were treated as a behavior, the vec-
tor it contributes to would be summed with any other vectors contributed
by other behaviors. But a vector with 0 magnitude is the identity function
for vector addition, so a COLLISION vector would have no impact. Instead,
collisions are often treated as “panic” situations, triggering an emergency
response outside the potential ﬁeld framework.
Some of the subtle differences between potential ﬁelds and subsumption
appear when the case of Level 2 is considered. The same functionality can
be accomplished by adding only a single instance of the WANDER behavior,
as shown in Fig. 4.21. As before, the behavior generates a new direction

138
4
The Reactive Paradigm
Figure 4.20
Level 0 redone as Potential Fields Methodology.
to move every
n seconds. This would be represented by a uniform ﬁeld
where the robot felt the same attraction to go a certain direction, regardless of
location, for
n seconds. However, by combining the output of WANDER with
the output vectors from RUNAWAYpf, the need for a new AVOID behavior
is eliminated. The WANDER vector is summed with the repulsive vectors,
and as a result, the robot moves both away from the obstacles and towards
the desired direction. This is shown in Fig. 4.22. The primary differences
in this example are that potential ﬁelds explicitly encapsulate sensing and
acting into primitive behaviors, and it did not have to subsume any lower
behaviors. As with subsumption, the robot became more intelligent when
the WANDERpf behavior was added to the RUNAWAYpf behavior.
Now consider how Level 3, corridor following, would be implemented in
a potential ﬁeld system. This further illustrates the conceptual differences
between the two approaches. The robot would have two concurrent behav-
iors: RUNAWAYpf and follow-corridor. RUNAWAYpf would remain the
same as before, but WANDER would be discarded. In the parlance of potential
ﬁelds, the task of following a corridor requires only two behaviors, while the
task of wandering requires two different behaviors.

4.4
Potential Fields Methodologies
139
Figure 4.21
Level 1 redone with Potential Fields Methodology.
Figure 4.22
Example resultant vector of WANDERpf and RUNAWAYpf.

140
4
The Reactive Paradigm
Figure 4.23
The a.)
perpendical and b.)
uniform ﬁelds combining into c.)
a
follow-corridor ﬁeld.
The follow-corridor behavior is interesting, because it requires a more
complex potential ﬁeld. As shown in Fig. 4.23, it would be desirable for
the robot to stay in the middle of the corridor. This can be accomplished
using two potential ﬁelds: a uniform ﬁeld perpendicular to the left boundary
and pointing to the middle, and a uniform ﬁeld perpendicular to the right
boundary and pointing to the middle. Notice that both ﬁelds have a linear
decrease in magnitude as the ﬁeld nears the center of the corridor. In practice,
this taper prevents the robot from see-sawing in the middle.
Also notice that the two uniform ﬁelds are not sufﬁcient because they do
not permit the robot to move forward; the robot would move to the middle
of the corridor and just sit there. Therefore, a third uniform ﬁeld is added
which is parallel to the corridor. All three ﬁelds combined yield a smooth
ﬁeld which sharply pushes the robot back to the middle of the corridor as a
function of its proximity to a wall. In the meantime, the robot is constantly
making forward progress. The ﬁgure below shows the ﬁelds involved. Re-

4.4
Potential Fields Methodologies
141
member, in this example, the robot is not projecting past or future boundaries
of the corridor; the visualization of the ﬁeld makes it appear that way.
The follow-corridor behavior is using the same sonar data as avoid;
therefore, walls will produce a repulsive ﬁeld, which would generally push
the robot onto the middle of the corridor. Why not just use a single uniform
parallel ﬁeld for follow-corridor? First, behaviors are independent. If
there is a corridor following behavior, it must be able to follow halls without
depending on side effects from other behaviors. Second, the polar symmetry
of the repulsive ﬁelds may cause see-sawing, so there is a practical advantage
to having a separate behavior.
The use of behavior-speciﬁc domain knowledge (supplied at instantia-
tion time as an optional initialization parameter) can further improve the
robot’s overall behavior. If the robot knows the width of the hall a priori,
follow-corridor can suppress instances of avoid which are using obsta-
cles it decides form the boundary of the wall. Then it will only avoid ob-
stacles that are in the hall. If there are no obstacles, follow-corridor
will produce a smooth trajectory.
If the obstacles are next to a wall, the
follow-corridor will treat the proﬁle of the obstacle as a wall and move
closer to the center.
The motor schemas for a behavior may be sequenced. One example of
SEQUENCING AND
PARAMETERIZING
POTENTIAL FIELDS
this is the docking behavior.12 Docking is when a robot moves to a speciﬁc
location and orientation relative to a docking station. This is useful for robots
performing materials handling in industry. In order to accept a piece of ma-
terial to carry, the robot has to be close enough to the correct side of the end
of a conveyor and facing the right way. Because docking requires a speciﬁc
position and orientation, it can’t be done with an attraction motor schema.
That ﬁeld would have the robot make a bee-line for the dock, even if it was
coming from behind; the robot would stop at the back in the wrong position
and orientation. Instead, a selective attraction ﬁeld is appropriate. Here the ro-
SELECTIVE
ATTRACTION
bot only “feels” the attractive force when it is within a certain angular range
of the docking station, as shown in Fig. 4.24.
Unfortunately selective attraction does not cover the case of when the ro-
bot approaches from behind or to the side. How does the robot move to
an area where the selective attraction ﬁeld can take effect? One way to do
this is to have tangential ﬁeld, which makes the robot orbit the dock until
it gets into the selective attraction area. The combination of the two motor
schema produces a very smooth ﬁeld which funnels the robot into the correct
position and orientation, as shown in Fig. 4.25.

142
4
The Reactive Paradigm
Figure 4.24
Selective attraction ﬁeld, width of

.
An interesting aspect of the docking behavior is that the robot is operating
in close proximity to the dock. The dock will also release an instance of avoid,
which would prevent the robot from getting near the desired position. In this
case, the docking behavior would lower the magnitude (gain) of the output
vector from an avoid instantiated in the dock area. Essentially, this partially
inhibits the avoid behavior in selected regions. Also, the magnitude or gain
can deﬁne the correct distance: the robot stops where the selective attraction
of the dock balances with the repulsion.
The selective and tangential ﬁelds are not sufﬁcient in practice, because of
the limitations of perception. If the robot can’t see the dock, it can’t deploy
the ﬁelds. But an industrial robot might know the relative direction of a dock,
much as a bee recalls the direction to its hive. Therefore an attractive force
attracts the robot to the vicinity of the dock, and then when the robot sees
the dock, it begins the funnel effect into the correct position and orientation,
even in the presence of obstacles, as seen in Fig. 4.26.
At least three perceptual schemas are needed for the docking behavior.
One is needed to extract the relative direction of the dock for the regular
attraction. Another is a perceptual schema capable of recognizing the dock
in general, even from behind or the sides, in order to support the tangential
ﬁeld. The third perceptual schema is needed for the selective attention ﬁeld;
it has to be able to respond to the front of the dock and extract the robot’s
relative distance and orientation.
The docking behavior is now deﬁned as having three perceptual schemas
and three motor schemas (they could be grouped into 3 primitive behaviors).
A schema-theoretic representation indicates that the behavior has some co-

4.4
Potential Fields Methodologies
143
Figure 4.25
Docking potential ﬁeld showing path of robot entering from slightly off
course.
ordinated control program to coordinate and control these schemas. In the
case of the docking behavior, a type of ﬁnite state machine is a reasonable
choice for coordinating the sequence of perceptual and motor schemas. It
provides a formal means of representing the sequence, and also reminds the
designer to consider state transitions out of the ordinary. For instance, the
robot could be moving toward the dock under the tangential plus selective
attention ﬁelds when a person walks in front. This would occlude the view

144
4
The Reactive Paradigm
Figure 4.26
Visualization of the docking behavior with obstacles.
of the dock. The general attraction motor schema would then be reactivate,
along with the associated perceptual schema to estimate the direction of the
dock. This vector would allow the robot to avoid the human in a direction
favorable as well as turn back towards the dock and try to re-acquire it.
The docking behavior also illustrates how the sensing capabilities of the
robot impact the parameters of the motor schemas. Note that the angular size
of the selective attention ﬁeld would be determined by the angles at which
the third perceptual schema could identify the dock. Likewise the radius of
the tangential and selective attraction ﬁelds is determined by the distance at
which the robot can perceive the dock.

4.4
Potential Fields Methodologies
145
4.4.8
Advantages and disadvantages
Potential ﬁeld styles of architectures have many advantages. The potential
ﬁeld is a continuous representation that is easy to visualize over a large re-
gion of space. As a result, it is easier for the designer to visualize the robot’s
overall behavior. It is also easy to combine ﬁelds, and languages such as C++
support making behavioral libraries. The potential ﬁelds can be parameter-
ized: their range of inﬂuence can be limited and any continuous function
can express the change in magnitude over distance (linear, exponential, etc.).
Furthermore, a two-dimensional ﬁeld can usually be extended into a three-
dimensional ﬁeld, and so behaviors developed for 2D will work for 3D.
Building a reactive system with potential ﬁelds is not without disadvan-
tages. The most commonly cited problem with potential ﬁelds is that mul-
tiple ﬁelds can sum to a vector with 0 magnitude; this is called the local
minima problem. Return to Fig. 4.19, the box canyon. If the robot was being
attracted to a point behind the box canyon, the attractive vector would cancel
the repulsive vector and the robot would remain stationary because all forces
would cancel out. The box canyon problem is an example of reaching a local
minima. In practice, there are many elegant solutions to this problem. One
of the earliest was to always have a motor schema producing vectors with a
small magnitude from random noise.12 The noise in the motor schema would
serve to bump the robot off the local minima.
Another solution is that of navigation templates (NaTs), as implemented
NAVIGATION
TEMPLATES
by Marc Slack for JPL. The motivation is that the local minima problem
most often arises because of interactions between the avoid behavior’s re-
pulsive ﬁeld and other behaviors, such as move-to-goal’s attractive ﬁeld. The
minima problem would go away if the avoid potential ﬁeld was somehow
smarter. In NaTs, the avoid behavior receives as input the vector summed
from the other behaviors. This vector represents the direction the robot would
go if there were no obstacles nearby. For the purposes of this book, this will
be referred to as the strategic vector the robot wants to go. If the robot has a
strategic vector, that vector gives a clue as to whether an obstacle should be
passed on the right or the left. For example, if the robot is crossing a bridge
(see Fig. 4.27), it will want to pass to the left of obstacles on its right in order
to stay in the middle. Note that the strategic vector deﬁnes what is left and
what is right.
NaTs implement this simple heuristic in the potential ﬁeld for RUNAWAY,
promoting it to a true AVOID. The repulsion ﬁeld is now supplemented with
a tangential orbit ﬁeld.
The direction of the orbit (clockwise or counter-

146
4
The Reactive Paradigm
Figure 4.27
Problem with potential ﬁelds: a.) an example, and b.) the use of NaTs
to eliminate the problem.
clockwise) is determined by whether the robot is to the right or left of the
strategic vector. The output of the avoid behavior can be called a tactical
vector, because it carries out the strategic goal of the robot in the face of im-
mediate challenges, just as a military general might tell a captain to capture
a fort but not specify each footstep.
A more recent solution to the local minima problem has been to express
the ﬁelds as harmonic functions.40 Potential ﬁelds implemented as harmonic
functions are guaranteed not to have a local minima of 0. The disadvan-
tage of this technique is that it is computationally expensive, and has to be
implemented on a VLSI chip in order to run in real-time for large areas.
To summarize the major points about potential ﬁelds architectures:
 Behaviors are deﬁned as consisting of one or more of both motor and per-
ceptual schemas and/or behaviors. The motor schema(s) for a behavior
must be a potential ﬁeld.
 All behaviors operate concurrently and the output vectors are summed.
Behaviors are treated equally and are not layered, although as will be seen
in Ch. 5, there may be abstract behaviors which internally sequence be-

4.5
Evaluation of Reactive Architectures
147
haviors. The coordinated control program is not speciﬁed; the designer
can use logic, ﬁnite state machines, whatever is deemed appropriate. Se-
quencing is usually controlled by perceived cues or affordances in the en-
vironment, which are releasers.
 Although all behaviors are treated equally, behaviors may make varying
contributions to the overall action of the robot. A behavior can change
the gains on another behavior, thereby reducing or increasing the magni-
tude of its output. This means that behaviors can inhibit or excite other
behaviors, although this is rarely used in practice.
 Perception is usually handled by direct perception or affordances.
 Perception can be shared by multiple behaviors. A priori knowledge can
be supplied to the perceptual schemas, to emulate a specialized sensor
being more receptive to events such as hall boundary spacing.
4.5
Evaluation of Reactive Architectures
As seen by the follow-corridor example, the two styles of architectures are
very similar in philosophy and the types of results that they can achieve.
Essentially, they are equivalent.
In terms of support for modularity, both decompose the actions and per-
ceptions needed to perform a task into behaviors, although there is some dis-
agreement over the level of abstraction of a behavior. Subsumption seems to
favor a composition suited for a hardware implementation, while potential
ﬁelds methods have nice properties for a software-oriented system.
The niche targetability is also high for both, assuming that the task can
be performed by reﬂexive behaviors. Indeed, the use of direct perception
emphasizes that reactive robots are truly constructed to ﬁll a niche.
The issue of whether these architectures show an ease of portability to
other domains is more open. Reactive systems are limited to applications
which can be accomplished with reﬂexive behaviors. They cannot be trans-
ferred to domains where the robot needs to do planning, reasoning about
resource allocation, etc. (this led to the Hybrid Paradigm to be described in
Ch. 7). In practice, very few of the subsumption levels can be ported to new
applications of navigating in an environment without some changes. The
different applications create layers which need to subsume the lower layers
differently. The potential ﬁelds methodology performs a bit better in that the

148
4
The Reactive Paradigm
designer can create a library of behaviors and schemas to choose from, with
no implicit reliance on a lower layer.
Neither architecture presents systems which could be called genuinely ro-
bust. The layering of subsumption imparts some graceful degradation if an
upper level is destroyed, but it has no mechanisms to notice that a degrada-
tion has occurred. The ﬁnite state mechanisms in the docking behavior show
some resilience, but again, only for situations which can be anticipated and
incorporated into the state diagram. As with animals, a reactive robot will
also do something consistent with its perception of the world, but not always
the right thing.
4.6
Summary
Under the Reactive Paradigm, systems are composed of behaviors, which
tightly couple sensing and acting. The organization of the Reactive Paradigm
is SENSE-ACT or S-A, with no PLAN component. Sensing in the Reactive
Paradigm is local to each behavior, or behavior-speciﬁc. Each behavior has
direct access to one or more sensors independently of the other behaviors. A
behavior may create and use its own internal world representation, but there
is no global world model as with the Hierarchical Paradigm. As a result,
reactive systems are the fastest executing robotic systems possible.
There are four major characteristics of robots constructed under the Reac-
tive Paradigm. Behaviors serve as the basic building blocks for robot actions,
even though different designers may have different deﬁnitions of what a be-
havior entails. As a consequence of using behaviors, the overall behavior
of the robot is emergent. Only local, behavior-speciﬁc sensing is permitted.
The use of explicit representations in perceptual processing, even locally, is
avoided in most reactive systems. Explicit representations of the world are
often referred to as maintaining the state of the world internally, or internal
state. Instead, reactive behaviors rely on the world to maintain state (as ex-
empliﬁed by the gripper controlling whether the robot was looking for soda
cans or for the recycling bin). Animal models are often cited as a basis for
a behavior or the architecture. Behaviors and groups of behaviors which
were inspired by or simulate animal behavior are often considered desir-
able and more interesting than hacks. Finally, reactive systems exhibit good
software engineering principles due to the “programming by behavior” ap-
proach. Reactive systems are inherently modular from a software design
perspective. Behaviors can be tested independently, since the overall behav-

4.7
Exercises
149
ior is emergent. More complex behaviors may be constructed from primitive
behaviors, or from mixing and matching perceptual and motor components.
This supports good software engineering practices, especially low coupling
and high cohesion.
The subsumption architecture is a popular reactive system. Behaviors are
purely reﬂexive and may not use memory. Behaviors are arranged in layers
of competence, where the lower levels encapsulate more general abilities.
The coordination of layers is done by higher layers, which have more speciﬁc
goal-directed behaviors, subsuming lower layers. Behaviors within a layer
are coordinated by ﬁnite state automata, and can be readily implemented in
hardware.
Potential ﬁelds methodologies are another popular reactive system. Be-
haviors in potential ﬁeld systems must be implemented as potential ﬁelds.
All active behaviors contribute a vector; the vectors are summed to produce
a resultant direction and magnitude for travel. Pﬁelds provide a continu-
ous representation, which is easier to visualize than rule encoding, and are
continuous. The ﬁelds can be readily implemented in software, and parame-
terized for ﬂexibility and reuse. The vector summation effect formalizes how
to combine behaviors, eliminating issues in how to design behaviors for sub-
sumption. The ﬁelds are often extensible to three dimensions, adding to the
re-usability and portability. In the example in this chapter, behaviors using
potential ﬁelds were able to encapsulate several layers in subsumption into
a set of concurrent peer behaviors with no layers. Ch. 5 will give examples
of how to sequence, or assemble, behaviors into more abstract behaviors.
Despite the differences, subsumption and potential ﬁelds appear to be
largely equivalent in practice.
Both provide support for modularity and
niche targetability. The ease of portability to other domains is relative to
the complexity of the changes in the task and environment. Neither style
of architecture explicitly addresses robustness, although in theory, if only a
higher layer of a subsumption system failed, the lower layers should ensure
robot survivability.
4.7
Exercises
Exercise 4.1
Deﬁne the reactive paradigm in terms of a.) the SENSE, PLAN, and ACT primitives,
and b.) sensing organization.

150
4
The Reactive Paradigm
Exercise 4.2
Describe the difference between robot control using a horizontal decomposition and
a vertical decomposition.
Exercise 4.3
List the characteristics of a reactive robotic system.
Exercise 4.4
Describe the differences between two dominant methods for combining behaviors in
a reactive architecture, subsumption and potential ﬁeld summation.
Exercise 4.5
Evaluate the subsumption architecture in terms of: support for modularity, niche
targetability, ease of portability to other domains, robustness.
Exercise 4.6
Evaluate potential ﬁeld methodologies in terms of: support for modularity, niche
targetability, ease of portability to other domains, robustness.
Exercise 4.7
What is the difference between the way the term “internal state” was used in ethology
and the way “internal state” means in behavioral robotics?
Exercise 4.8
Diagram Level 2 in the subsumption example in terms of behaviors.
Exercise 4.9
When would an exponentially increasing repulsive ﬁeld be preferable over a linear
increasing repulsive ﬁeld?
Exercise 4.10
Suppose you were to construct a library of potential ﬁelds of the ﬁve primitives. What
parameters would you include as arguments to allow a user to customize the ﬁelds?
Exercise 4.11
Use a spreadsheet, such as Microsoft Excel, to compute various magnitude proﬁles.
Exercise 4.12
Return to Fig. 4.17. Plot the path of the robot if it started in the upper left corner.
Exercise 4.13
Consider the Khepera robot and its IR sensors with the RUNAWAY behavior instanti-
ated for each sensor as in the example in Fig. 4.19. What happens if an IR breaks and
always returns a range reading of
N, meaning an obstacle is
N cm away? What will
be the emergent behavior? and so on. Can a reactive robot notice that it is malfunc-
tioning? Why or why not?

4.7
Exercises
151
Exercise 4.14
How does the Reactive Paradigm handle the frame problem and the open world as-
sumption?
Exercise 4.15
An alternative RUNAWAY behavior is to turn
	0
 (either left or right, depending on
whether its “left handed” or “right handed” robot) rather than
0
. Can this be
represented by a potential ﬁeld?
Exercise 4.16
Using rules, or if-then statements, is a method for representing and combining pro-
gramming units which are often called behaviors; for example “if OBSTACLE-ON-
LEFT and HEADING-RIGHT, then IGNORE.” Can the layers in subsumption for
hall-following be written as a series of rules? Can the potential ﬁelds? Are rules
equivalent to these two methods? Do you think rules are more amenable to good
software engineering practices?
Exercise 4.17
Some researchers consider random wandering as a primitive potential ﬁeld. Recall
that random wandering causes the robot to periodically swap to a new vector with a
random direction and magnitude. How can a wander ﬁeld be represented? Does the
array of the ﬁeld represent a physical area or time? Unlike regular potential ﬁelds, the
vector is computed as a function of time, every
n minutes, rather than on the robot’s
relationship to something perceivable in the world.
Exercise 4.18
[Programming]
Design and implement potential ﬁelds:
a. Construct a potential ﬁeld to represent a “move through door” behavior from
primitive potential ﬁelds. Why won’t a simple attractive ﬁeld work? ANS: if the
robot is coming from a side, it will graze the door frame because the robot is not a
point, it has width and limited turning radius.
b. What happens if a person is exiting the door as the robot enters? Design an appro-
priate “avoid” potential ﬁeld, and show the emergent potential ﬁeld when AVOID
and MOVE-THRU-DOOR are activated at the same time.
c. Simulate this using the Khepera simulator for Unix systems found at:
http://www.k-team.com.
d. Run this on a real khepera.
Exercise 4.19
[Programming]
Program two versions of a phototropic behavior using the Khepera simulator. Both
versions should use the same motor schema, an attractive ﬁeld, but different percep-
tual schemas. In one version, the perceptual schema processes light from a single
sensor and the behavior is instantiated 8 times. In the second version, the perceptual
schema processes light from all sensors and returns the brightest. Set up ﬁve inter-
esting “worlds” in the simulator with different placements of lights. Compare the
emergent behavior for each world.

152
4
The Reactive Paradigm
Exercise 4.20
[Digital Circuits]
For readers with a background in digital circuits, build one or more of the simple
creatures in Flynn and Jones’ Mobile Robots: Inspiration to Implementation 76 using a
Rug Warrior kit.
4.8
End Notes
For a roboticist’s bookshelf.
The subsumption architecture favors a hardware implementation using inexpensive
hardware. Part of the rapid acceptance of the reactive paradigm and the subsump-
tion architecture was due to Mobile Robots: Inspiration to Perspiration 76 by students in
the MIT AI Lab. The straightforward circuits allowed any hobbyist to produce intel-
ligent robots. On a more theoretical note, Rodney Brooks has collected his seminal
papers on subsumption into a volume entitled Cambrian Intelligence, 28 a nice play on
the period in evolution and on Brooks’ location in Cambridge, Massachusetts.
About Rodney Brooks.
Rodney Brooks is perhaps the best known roboticist, with his insect-like (Genghis,
Attila, etc.) and anthropomorphic robots (Cog, Kismet) frequently appearing in the
media. Brooks was one of four “obsessed” people proﬁled in a documentary by Errol
Morris, Fast, Cheap, and Out of Control. The documentary is well worth watching, and
there are some gorgeous shots of robots walking over broken glass giving the terrain
a luminous quality. Brooks’ reactive philosophy appears in an episode of The X-Files
on robotic cockroaches called “War of the Corophages.” The roboticist from the Mas-
sachusetts Institute of Robotics is a combination of Steven Hawking (the character is
disabled), Joe Engleberger (the bow tie), Marvin Minsky (stern, professorial manner),
and Rodney Brooks (the character says almost direct quotes from Brooks’ interviews
in science magazines).
About the “s” in subsumption.
Rodney Brooks’ 1986 paper never ofﬁcially named his architecture. Most papers refer
to it as “Subsumption,” sometimes without the capital “s” and sometimes with a
capital.
Building a robot with petty cash.
Erann Gat and his then boss at NASA’s Jet Propulsion Laboratory, David Miller, cite
another advantage of behavioral robotics: you can build a mobile robot very cheaply.
In the late 1980’s, Miller’s group wanted to build a small reactive robot to compare
with the traditional Hierarchical vehicles currently used for research and to get expe-
rience with subsumption. Request after request was turned down. But they realized
that the individual electronics were cheap. So they bought the parts for their small

4.8
End Notes
153
mobile robot in batches of $50 or less, which could be reimbursed out of petty cash.
The resulting robot was about the size and capability of a Lego Mindstorms kit with
a total cost around $500.
Robot name trivia.
The MIT robots used in the early days of subsumption were all named for conquerors:
Attila, Genghis, etc. Ron Arkin did his initial work on potential ﬁelds methodology
while pursuing his PhD at the University of Massachusetts. The robot he used was
a Denning MRV called HARV (pronounced “Harvey”). HARV stood for Hardly Au-
tonomous Robot Vehicle morphing into Harmful Autonomous Robot Vehicle, when
military research sponsors were present. HARV was also considered a short version
of “Harvey Wallbanger,” both a description of the robot’s emergent behavior and the
name of a cocktail.


5
Designing a Reactive
Implementation
Chapter objectives:
 Use schema theory to design and program behaviors using object-oriented
programming principles.
 Design a complete behavioral system, including coordinating and con-
trolling multiple concurrent behaviors.
 For a given behavioral system, draw a behavioral table specifying the re-
leasers, perceptual schemas, and motor schemas for each behavior.
 Describe the two methods for assembling and coordinating primitive be-
haviors within an abstract behavior: ﬁnite state automata and scripts. Be able
to represent a sequence of behaviors with a state diagram or with a pseudo-
code script.
5.1
Overview
By this point, the sheer simplicity and elegance of reactive behaviors tend
to incite people to start designing and implementing their own robots. Kits
such as Lego Mindstorms and the Rug Warrior permit the rapid coupling
of sensors and actuators, enabling users to build reactive behaviors. How-
ever, the new issues are how to program more intelligence into the software
and how to exploit better sensors than come with kits. Unfortunately, good
intentions in robot programming are often frustrated by two deﬁcits. First,
designing behaviors tends to be an art, not a science. Novice roboticists often
are uncertain as to how to even start the design process, much less how to
know when they’ve got a reasonable system. The second deﬁcit is more sub-
tle. Once the designer has a few well-designed and tested behaviors, how

156
5
Designing a Reactive Implementation
are they integrated into a system? The Reactive movement and early work
described in Ch. 4 was characterized by robots running a very small set of
behaviors which were combined internally to produce an overall emergent
EMERGENT BEHAVIOR
behavior. The key components of a reactive architecture were shown to be
the behaviors plus the mechanism for merging the output of the concurrent
behaviors.
However, many applications are best thought of as a series of behaviors,
SERIES OF BEHAVIORS
each operating in a recognizable sequence. One of the ﬁrst popular appli-
cations that many roboticists looked at was picking up and disposing of an
empty soft drink can. This involved search for a can, move toward the can
when it is found, pick up the can, search for the recycle bin, move toward
the recycle bin, drop the can.39;129;66 It’s counterintuitive to think of these be-
haviors as being concurrent or merged. (There is certainly the possibility of
concurrency, for example avoiding obstacles while moving to the soda can
or recycle bin.) Therefore, new techniques had to be introduced for control-
ling sequences of behaviors. Most of these techniques are conceptually the
equivalent of constructing a macro behavior, where the schema structure is
used recursively to simplify programming the control program.
This chapter attempts to aid the novice designer in constructing a reac-
tive robot system by addressing each of these two deﬁcits. First, an object-
oriented programming approach to designing behaviors is introduced. This
approach is based on schema theory, which was introduced in Ch. 3. The
“art” of design is presented in a ﬂow chart following the waterfall method
of software engineering, along with a case study of a winning entry in the
1994 International Association for Unmanned Systems’ Unmanned Ground
Robotics Competition. The case study emphasizes the importance of estab-
lishing the ecological niche of a reactive robot. Second, two techniques for
managing sequences of behaviors are introduced: ﬁnite-state automata and
scripts. As could be expected from the material presented in Ch. 3, both of
these techniques will look very similar to the Innate Releasing Mechanisms
from Tinbergen and Lorenz. Finally, the chapter shows how these techniques
were applied to entries in the “Pick Up the Trash” events of the 1994 AAAI
and 1995 IJCAI Mobile Robot Competitions. The focus of these examples is
how the logic for coordinating a sequence of behaviors is developed, repre-
sented, and implemented. The use of schemas to “factor out” and program a
small set of generic behaviors rather than designing a large set of specialized
behaviors is emphasized throughout the chapter.

5.2
Behaviors as Objects in OOP
157
Perceptual
Schema
Schema
coordinated control program
Behavior
Motor
Schema
a.
Perceptual
Schema
Behavior
coordinated control program
Behavior
Motor
Schema
*
*
*
b.
Figure 5.1
Classes: a.) schema and b.) behavior.
5.2
Behaviors as Objects in OOP
Although Object-Oriented Programming (OOP) had not become popular dur-
ing the time that the Reactive Paradigm was developed, it is useful to cast
behaviors in OOP terms. Schema theory is well suited for transferring theo-
retical concepts to OOP. Furthermore, schema theory will be used as a bridge
between concepts in biological intelligence and robotics, enabling a practi-
cal implementation of reactivity exploiting innate releasing mechanisms and
affordances.
Recall from software engineering that an object consists of data and meth-
ods, also called attributes and operations. As noted in Ch. 3, schemas con-
tain speciﬁc knowledge and local data structures and other schemas. Fig. 5.1
shows how a schema might be deﬁned. Following Arbib, 6 a schema as a
programming object will be a class. The class will have an optional method
called a coordinated control program. The coordinated control program is a
COORDINATED
CONTROL PROGRAM
function that coordinates any methods or schemas in the derived class.
Three classes are derived from the Schema Class: Behavior, Motor Schema,
and Perceptual Schema. Behaviors are composed of at least one Perceptual
Schema and one Motor Schema; these schemas act as the methods for the
Behavior class. A Perceptual Schema has at least one method; that method
takes sensor input and transforms it into a data structure called a percept. A
PERCEPT
Motor Schema has at least one method which transforms the percept into a
vector or other form of representing an action. Since schemas are indepen-
dent, the Behavior object acts as a place holder or local storage area for the
percept. The Perceptual Schema is linked to the sensor(s), while the Mo-
tor Schema is linked to the robot’s actuators. The sensors and actuators can
be represented by their own software classes if needed; this is useful when
working with software drivers for the hardware.

158
5
Designing a Reactive Implementation
Using the Uniﬁed Modeling Language representation,55 the Schema and
Behavior classes look like Fig. 5.1. The OOP organization allows a behavior
to be composed of multiple perceptual schema and motor schema and even
behaviors. Another way of stating this is that the deﬁnition of a behavior is
recursive. Why is it useful to have multiple perceptual schema and motor
schema? In some cases, it might be helpful to have two perceptual schema,
one for say daylight conditions using a TV camera and one for nighttime
using infra-red. Sec. 5.2.2 provides a more detailed example of why multiple
schemas in a behavior can be helpful.
Recall that a primitive behavior is composed of only one perceptual schema
PRIMITIVE BEHAVIOR
and one motor schema; there is no need to have any coordinated control
program. Primitive behaviors can be thought of being monolithic, where they
do only one (“mono”) thing. Because they are usually a simple mapping
from stimulus to response, they are often programmed as a single method,
not composed from multiple methods or objects. The concept of Perceptual
and Motor Schema is there, but hidden for the sake of implementation.
Behaviors which are assembled from other behaviors or have multiple
perceptual schema and motor schema will be referred to as abstract behav-
ABSTRACT BEHAVIORS
iors, because they are farther removed from the sensors and actuators than
a primitive behavior. The use of the term “abstract behavior” should not be
confused with an abstract class in OOP.
5.2.1
Example: A primitive move-to-goal behavior
This example shows how a primitive behavior can be designed using OOP
principles. In 1994, the annual AAAI Mobile Robot Competitions had a “Pick
Up the Trash” event, which was repeated in 1995 at the joint AAAI-IJCAI
Mobile Robot Competition.129;66 The basic idea was for a robot to be placed
in an empty arena about the size of an ofﬁce. The arena would have Coca-
Cola cans and white Styrofoam cups placed at random locations. In two
of the four corners, there would be a blue recycling bin; in the other two,
a different colored trash bin. The robot who picked up the most trash and
placed them in the correct bin in the allotted time was the winner. In most
years, the strategy was to ﬁnd and recycle the Coca-Cola cans ﬁrst, because
it was easier for the robot’s vision processing algorithms to perceive red and
blue.
One of the most basic behaviors needed for picking up a red soda can and
moving to a blue bin is move_to_goal. When the robot sees a red can, it
must move to it. When it has a can, it must ﬁnd and then move to a blue bin.

5.2
Behaviors as Objects in OOP
159
It is better software engineering to write a general move to goal behavior,
where only what is the goal—a red region or a blue region—varies. The goal
for the current instance can be passed in at instantiation through the object
constructor.
Writing a single generic behavior for move_to_goal(color) is more
desirable than writing a move_to_red and a move_to_blue behaviors.
From a software engineering perspective, writing two behaviors which do
the same thing is an opportunity to introduce a programming bug in one
of them and not notice because they are supposed to be the same. Generic
behaviors also share the same philosophy as factoring in mathematics. Con-
sider simplifying the equation
x

+
	0x
+
. The ﬁrst step is to factor out
any common term to simplify the equation. In this case, 45 can be factored
and the equation rewritten as
(x
+
)
. The color of the goal, red or blue,
was like the common coefﬁcient of 45; it is important, but tends to hide that
the key to the solution was the move-to-goal part, or
x.
Modular, generic code can be handled nicely by schemas as shown in
Fig. 5.2. The behavior move_to_goal would consist of a perceptual schema,
which will be called extract-goal, and a motor schema, which uses an
attractive ﬁeld called pfields.attraction. extract-goal uses the af-
fordance of color to extract where the goal is in the image, and then computes
the angle to the center of the colored region and the size of the region. This
information forms the percept of the goal; the affordance of the Coke can is
AFFORDANCE
the color, while the information extracted from the perception is the angle
and size. The attraction motor schema takes that percept and is responsible
for using it to turn the robot to center on the region and move forward. It
can do this easily by using an attractive ﬁeld, where the larger the region, the
stronger the attraction and the faster the robot moves.
The move_to_goal behavior can be implemented as a primitive behavior,
where goal_color is a numerical means of representing different colors
such as red and blue:
move_to_goal(goal_color):
Object
Behavioral Analog
Identiﬁer
Data
percept
goal_angle
goal_strength
Methods
perceptual_schema
extract_goal(goal_color)
motor_schema
pfields.attraction(goal_angle, goal_strength)
The above table implies some very important points about programming
with behaviors:

160
5
Designing a Reactive Implementation
 The behavior is the “glue” between the perceptual and motor schemas.
The schemas don’t communicate in the sense that both are independent
entities; the perceptual schema doesn’t know that the motor schema ex-
ists. Instead, the behavior puts the percept created by the perceptual schema in a
local place where the motor schema can get it.
 Behaviors can (and should) use libraries of schemas. The pﬁelds sufﬁx on
the pfields.attraction() meant that attraction was a method within
another object identiﬁed as pﬁelds. The ﬁve primitive potential ﬁelds
could be encapsulated into one class called PFields, which any motor
schema could use. PFields would serve as a library. Once the potential
ﬁelds in PFields were written and debugged, the designer doesn’t ever
have to code them again.
 Behaviors can be reused if written properly. In this example, the move to
goal behavior was written to accept a structure (or object) deﬁning a color
and then moving to a region of that color. This means the behavior can be
used with both red Coke cans and blue trash cans.
5.2.2
Example: An abstract follow-corridor behavior
The move to goal example used a single motor schema with a single percep-
tual schema. This example shows how a potential ﬁelds methodology can
be implemented using schemas. In the corridor following example in Ch. 4,
the follow_corridor potential ﬁeld consisted of two primitive ﬁelds: two
instances of perpendicular to the walls and one uniform parallel to the
walls. The follow_corridor ﬁeld could be implemented in schemas in
at least two different ways as shown in Fig. 5.2. In one way, each of the
primitive ﬁelds would be a separate motor schema. The follow corridor mo-
tor schema would consist of the three primitives and the coordinated con-
trol program. The coordinated control program would be the function that
knows that one ﬁeld is perpendicular from the left wall going towards the
center of the corridor, which way is forward, etc. They were summed to-
gether by the coordinated control program in the behavioral schema to pro-
duce a single output vector. The perceptual schema for the follow corridor
would examine the sonar polar plot and extract the relative location of the
corridor walls. The perceptual schema would return the distance to the left
wall and the right wall.
Another way to have achieved the same overall behavior is to have fol-
low_wall composed of two instances of a follow wall behavior: follow_-

5.2
Behaviors as Objects in OOP
161
follow corridor
vector summation of MS
Motor
Schema 1:
perpendicular
Motor
Schema 2:
uniform
Perceptual
Schema 1:
find-walls
(polar plot)
a.
follow corridor
vector summation of behaviors
Behavior 1:
follow-wall
(left)
Perceptual
Schema:
find-walls
(polar plot)
Motor
Schema:
follow-wall
(which-side)
Behavior 1:
follow-wall
(right)
b.
Figure
5.2
Class
diagrams
for
two
different
implementations
of
follow_corridor: a.) use of primitive ﬁelds, and b.) reuse of ﬁelds grouped into a
follow_wall behavior.

162
5
Designing a Reactive Implementation
wall (left) and follow_wall(right). Each instance of follow wall
would receive the sonar polar plot and extract the relevant wall. The associ-
ated class diagram is shown on the right in Fig. 5.2.
In both implementations, the motor schema schemas ran continuously and
the vectors were summed internally in order to produce a single output
vector. Since there were multiple motor schemas, the coordinated control
program for follow-corridor is not null as it was for move-to-goal. The vec-
tor summation and the concurrency form the conceptual coordinated control
program in this case.
5.2.3
Where do releasers go in OOP?
The previous examples showed how behaviors can be implemented using
OOP constructs, such as classes. Another important part of a behavior is how
it is activated. As was discussed in Ch. 3, perception serves two purposes:
TWO PURPOSES OF
PERCEPTION
to release a behavior and to guide it. Perceptual schemas are clearly used for
guiding the behavior, either moving toward a distinctively colored goal or
following a wall. But what object or construct contains the releaser and how
is it “attached” to the behavior?
The answer to the ﬁrst part of the question is that the releaser is itself a per-
ceptual schema. It can execute independently of whatever else is happening
with the robot; it is a perceptual schema not bound to a motor schema. For
example, the robot is looking for red Coke cans with the extract_color
perceptual schema. One way to implement this is when the schema sees
red, it can signal the main program that there is red. The main program can
determine that that is the releaser for the move to goal behavior has been
satisﬁed, and instantiate move_to_goal with goal=red. move_to_goal
can instantiate a new instance of extract_color or the main program
can pass a pointer to the currently active extract_color.
Regardless,
move_to_goal has to instantiate pfield.attraction, since the attrac-
tion motor schema wouldn’t be running. In this approach, the main program
is responsible for calling the right objects at the right time; the releaser is at-
tached to the behavior by the designer with little formal mechanisms to make
sure it is correct. This is awkward to program.
Another, more common programming approach is to have the releaser be
part of the behavior: the single perceptual schema does double duty. This
programming style requires a coordinated control program. The behavior
is always active, but if the releaser isn’t satisﬁed, the coordinated control
program short-circuits processing. The behavior returns an identity function,

5.3
Steps in Designing a Reactive Behavioral System
163
in the case of potential ﬁelds, a vector of (0.0,0.0), which is the same as if the
behavior wasn’t active at all. This style of programming can tie up some
resources, but is generally a simple, effective way to program. Fig. 5.2 shows
the two approaches.
Either way, once the robot saw red, the observable aspect of move to goal
(e.g., moving directly toward the goal) would commence. The extract goal
schema would update the percept data (relative angle of the goal and size of
red region) every time it was called. This percept would then be available to
the motor schema, which would in turn produce a vector.
As will be covered in Sec. 5.5, the releaser must be designed to support the
correct sequence. Depending where the robot was in the sequence of activi-
ties, the robot uses move to goal to move to a red Coke can or a blue recycling
bin. Otherwise, the robot could pursue a red Coke can and a blue recycling
bin simultaneously. There is nothing in the OOP design to prevent that from
happening—in fact, OOP makes it easy. In this situation, there would be two
move to goal objects, one instantiated with goal of “red” and the other with
goal of “blue.” Notice that the move to goal behavior can use any perceptual
schema that can produce a goal angle and goal strength. If the robot needed
to move to a bright light (phototropism), only the perceptual schema would
need to be changed. This is an example of software reusability.
5.3
Steps in Designing a Reactive Behavioral System
Fig. 5.3 shows the steps in designing a reactive behavioral system, which
are taken from Behavior-Based Robotics10 and a case study by Murphy.98 This
section will ﬁrst give a broad discussion of the design process, then work
through each step using the winning approach taken in the 1994 Unmanned
Ground Vehicle Competition.
The methodology in Fig. 5.3 assumes that a designer is given a task for the
robot to do, and a robot platform (or some constraints, if only budgetary).
The goal is to design a robot as a situated agent. Therefore, the ﬁrst three
steps serve to remind the designer to specify the ecological niche of the robot.
The fourth step begins the iterative process of identifying and reﬁning the
set of behaviors for the task. It asks the question: what does the robot do? Deﬁn-
ing the ecological niche deﬁnes constraints and opportunities but doesn’t
necessarily introduce major insights into the situatedness of the robot: how it
acts and reacts to the range of variability in its ecological niche. This step is where
a novice begins to recognize that designing behaviors is an art. Sometimes,

164
5
Designing a Reactive Implementation
Specification
& Analysis:
ecological 
niche
Design
Implementation &
unit testing
System testing
1. Describe the task
2. Describe the robot
3. Describe the environment
4. Describe how the robot should 
act in response to its environment
7. Test behaviors together
5. Implement & refine each behavior
6. Test each behavior independently
Figure 5.3
Steps in designing a reactive behavioral system, following basic software
engineering phases.
a behavioral decomposition appears obvious to a roboticist after thinking
about the ecological niche. For example, in the 1994 and 1995 Pick Up the
Trash events, most of the teams used a partitioning along the lines of: ran-
dom search until see red, move to red, pick up can, random search until see
blue, move to blue, drop can.
Roboticists often attempt to ﬁnd an analogy to a task accomplished by
an animal or a human, then study the ethological or cognitive literature for
more information on how the animal accomplishes that class of tasks. This,
of course, sidesteps the question of how the roboticist knew what class of an-
imal tasks the robot task is similar to, as well as implies a very linear think-
ing process by roboticists. In practice, roboticists who use biological and
cognitive insights tend to read and try to stay current with the ethological
literature so that they can notice a connection later on.
Steps 5-7 are less abstract. Once the candidate set of behaviors has been
proposed, the designer works on designing each individual behavior, spec-

5.4
Case Study: Unmanned Ground Robotics Competition
165
ifying its motor and perceptual schemas. This is where the designer has to
write the algorithm for ﬁnding red blobs in a camera image for the random
search until ﬁnd red and move to red behaviors. The designer usually pro-
grams each schema independently, then integrates them into a behavior and
tests the behavior thoroughly in isolation before integrating all behaviors.
This style of testing is consistent with good software engineering principles,
and emphasizes the practical advantages of the Reactive Paradigm.
The list of steps in implementing a reactive system can be misleading. De-
spite the feedback arrows, the overall process in Fig. 5.3 appears to be linear.
In practice, it is iterative. For example, a supposed affordance may be im-
possible to detect reliably with the robot’s sensors, or an affordance which
was missed in the ﬁrst analysis of the ecological niche suddenly surfaces.
The single source of iteration may be testing all the behaviors together in the
“real world.” Software that worked perfectly in simulation often fails in the
real world.
5.4
Case Study: Unmanned Ground Robotics Competition
This case study is based on the approach taken by the Colorado School of
Mines team to the 1994 Unmanned Ground Robotics Competition.98 The ob-
jective of the competition was to have a small unmanned vehicle (no larger
than a golf cart) autonomously navigate around an outdoor course of white
lines painted on grass. The CSM entry won ﬁrst place and a $5,000 prize.
Each design step is ﬁrst presented in boldface and discussed. What was ac-
tually done by the CSM team follows in italics. This case study illustrates
the effective use of only a few behaviors, incrementally developed, and the
use of affordances combined with an understanding of the ecological niche.
It also highlights how even a simple design may take many iterations to be
workable.
Step 1: Describe the task. The purpose of this step is to specify what the
robot has to do to be successful.
The task was for the robot vehicle to follow a path with hair pin turns, stationary
obstacles in the path, and a sand pit. The robot which went the furthest without going
completely out of bounds was the winner, unless two or more robots went the same
distance or completed the course, then the winner was whoever went the fastest. The
maximum speed was 5 mph. If the robot went partially out of bounds (one wheel or
a portion of a tread remained inside), a distance penalty was subtracted. If the robot
hit an obstacle enough to move it, another distance penalty was levied. Therefore,

166
5
Designing a Reactive Implementation
the competition favored an entry which could complete the course without accruing
any penalties over a faster entry which might drift over a boundary line or bump an
obstacle. Entrants were given three runs on one day and two days to prepare and
test on a track near the course; the times of the heats were determined by lottery.
Step 2: Describe the robot. The purpose of this step is to determine the
basic physical abilities of the robot and any limitations. In theory, it might
be expected that the designer would have control over the robot itself, what
it could do, what sensors it carries, etc. In practice, most roboticists work
with either a commercially available research platform which may have lim-
itations on what hardware and sensors can be added, or with relatively in-
expensive kit type of platform where weight and power restrictions may im-
pact what it can reasonably do. Therefore, the designer is usually handed
some ﬁxed constraints on the robot platform which will impact the design.
In this case, the competition stated that the robot vehicle had to have a footprint
of at least 3ft by 3.5ft but no bigger than a golf cart. Furthermore, the robot had to
carry its own power supply and do all computing on-board (no radio communication
with an off-board processor was permitted), plus carry a 20 pound payload.
The CSM team was donated the materials for a robot platform by Omnitech Ro-
botics, Inc. Fig. 5.4 shows Omnibot. The vehicle base was a Power Wheels battery
powered children’s jeep purchased from a toy store. The base met the minimum foot-
print exactly. It used Ackerman (car-like) steering, with a drive motor powering the
wheels in the rear and a steering motor in the front. The vehicle had a 22
 turning
angle. The on-board computing was handled by a 33MHz 486 PC using Omnitech
CANAMP motor controllers. The sensor suite consisted of three devices: shaft en-
coders on the drive and steer motors for dead reckoning, a video camcorder mounted
on a mast near the center of the vehicle and a panning sonar mounted below the
grille on the front. The output from the video camcorder was digitized by a black and
white framegrabber. The sonar was a Polaroid lab grade ultrasonic transducer. The
panning device could sweep 180
. All coding was done in C++.
Due to the motors and gearing, Omnibot could only go 1.5 mph. This limitation
meant that it could only win if it went farther with less penalty points than any
other entry. It also meant that the steering had to have at least a 150ms update rate
or the robot could veer out of bounds without ever perceiving it was going off course.
The black and white framegrabber eliminated the use of color. Worse yet, the update
rate of the framegrabber was almost 150ms; any vision processing algorithm would
have to be very fast or else the robot would be moving faster than it could react. The
reﬂections from uneven grass reduced the standard range of the sonar from 25.5 ft to
about 10 ft.

5.4
Case Study: Unmanned Ground Robotics Competition
167
Figure 5.4
Omnibot, a mobile robot built from a Power Wheels battery-powered toy
jeep by students and Omnitech Robotics, Inc.
Step 3: Describe the Environment. This step is critical for two reasons.
First, it is a key factor in determining the situatedness of the robot. Second, it
identiﬁes perceptual opportunities for the behaviors, both in how a percep-
tual event will instantiate a new behavior, and in how the perceptual schema
for a behavior will function. Recall from Chapter 4 that the Reactive Para-
digm favors direct perception or affordance-based perception because it has
a rapid execution time and involves no reasoning or memory.
The course was laid out on a grassy ﬁeld with gentle slopes. The course consisted
of a 10 foot wide lane marked in US Department of Transportation white paint,
roughly in the shape of a kidney (see Fig. 5.5). The exact length of the course and
layout of obstacles of the course were not known until the day of the competition, and
teams were not permitted to measure the course or run trials on it. Obstacles were all
stationary and consisted of bales of hay wrapped in either white or red plastic. The
bales were approximately 2 ft by 4 ft and never extended more than 3 feet into the
lane. The sonar was able to reliably detect the plastic covered bales at most angles of
approach at 8 feet away. The vehicles were scheduled to run between 9am and 5pm
on May 22, regardless of weather or cloud cover. In addition to the visual challenges
of changing lighting due to clouds, the bales introduced shadows on the white lines
between 9–11am and 3–5pm. The sand pit was only 4 feet long and placed on a
straight segment of the course.
The analysis of the environment offered a simpliﬁcation of the task. The placing of
the obstacles left a 4 ft wide open area. Since Omnibot was only 3 ft wide, the course
could be treated as having no obstacles if the robot could stay in the center of the lane

168
5
Designing a Reactive Implementation
IV
I
II
III
START
FINISH
SAND PIT
Rock (white bale)
Milestone (red bale)
Obstacle (white bale)
Figure 5.5
The course for the 1994 Ground Robotics Competition.
with a 0.5 ft tolerance. This eliminated the need for an avoid obstacle behavior.
The analysis of the environment also identiﬁed an affordance for controlling the
robot. The only object of interest to the robot was the white line, which should have
a high contrast to the green (dark gray) grass. But the exact lighting value of the
white line changed with the weather. However, if the camera was pointed directly
at one line, instead of trying to see both lines, the majority of the brightest points
in the image would belong to the line (this is a reduction in the signal to noise ratio
because more of the image has the line in it). Some of the bright points would be due
to reﬂections, but these were assumed to be randomly distributed. Therefore, if the
robot tried to keep the centroid of the white points in the center of the image, it would
stay in the center of the lane.
Step 4: Describe how the robot should act in response to its environ-
ment. The purpose of this step is to identify the set of one or more candidate
primitive behaviors; these candidates will be reﬁned or eliminated later. As
the designer describes how the robot should act, behaviors usually become
apparent. It should be emphasized that the point of this step is to concen-

5.4
Case Study: Unmanned Ground Robotics Competition
169
trate on what the robot should do, not how it will do it, although often the
designer sees both the what and the how at the same time.
In the case of the CSM entry, only one behavior was initially proposed: follow-line.
The perceptual schema would use the white line to compute the difference between
where the centroid of the white line was versus where it should be, while the motor
schema would convert that to a command to the steer motor.
In terms of expressing the behaviors for a task, it is often advantageous to
construct a behavior table as one way of at least getting all the behaviors on a
BEHAVIOR TABLE
single sheet of paper. The releaser for each behavior is helpful for conﬁrming
that the behaviors will operate correctly without conﬂict (remember, acci-
dently programming the robotic equivalent of male sticklebacks from Ch. 3
is undesirable). It is often useful for the designer to classify the motor schema
and the percept. For example, consider what happens if an implementation
has a purely reﬂexive move-to-goal motor schema and an avoid-obstacle be-
havior. What happens if the avoid-obstacle behavior causes the robot to lose
perception of the goal? Oops, the perceptual schema returns no goal and the
move-to-goal behavior is terminated! Probably what the designer assumed
was that the behavior would be a ﬁxed-action pattern and thereby the robot
would persist in moving toward the last known location of the goal.
Behavior Table
Releaser
Behavior
Motor Schema
Percept
Perceptual Schema
always on
follow-line()
stay-on-path(c_x)
c_x
compute-centroid(image,white)
As seen from the behavior table above, the CSM team initially proposed only one
behavior, follow-line. The follow-line behavior consisted of a motor schema, stay-on-
path(centroid), which was reﬂexive (stimulus-response) and taxis (it oriented the ro-
bot relative to the stimulus). The perceptual schema, compute-centroid(image,white),
extracted an affordance of the centroid of white from the image as being the line. Only
the x component, or horizontal location, of the centroid was used, c_x.
Step 5: Reﬁne each behavior. By this point, the designer has an overall
idea of the organization of the reactive system and what the activities are.
This step concentrates on the design of each individual behavior. As the
designer constructs the underlying algorithms for the motor and perceptual
schemas, it is important to be sure to consider both the normal range of envi-
ronmental conditions the robot is expected to operate in (e.g., the steady-state
case) and when the behavior will fail.
The follow-line behavior was based on the analysis that the only white things
in the environment were lines and plastic covered bales of hay. While this was a
good assumption, it led to a humorous event during the second heat of the competi-
tion. As the robot was following the white line down the course, one of the judges

170
5
Designing a Reactive Implementation
stepped into view of the camera. Unfortunately, the judge was wearing white shoes,
and Omnibot turned in a direction roughly in-between the shoes and the line. The
CSM team captain, Floyd Henning, realized what was happening and shouted at the
judge to move. Too late, the robot’s front wheels had already crossed over the line; its
camera was now facing outside the line and there was no chance of recovering. Sud-
denly, right before the leftmost rear wheel was about to leave the boundary, Omnibot
straightened up and began going parallel to the line! The path turned to the right,
Omnibot crossed back into the path and re-acquired the line. She eventually went
out of bounds on a hair pin further down the course. The crowd went wild, while the
CSM team exchanged confused looks.
What happened to make Omnibot drive back in bounds? The perceptual schema
was using the 20% brightest pixels in the image for computing the centroid. When it
wandered onto the grass, Omnibot went straight because the reﬂection on the grass
was largely random and the positions cancelled out, leaving the centroid always in
the center of the image. The groundskeepers had cut the grass only in the areas
where the path was to be. Next to the path was a parallel swatch of uncut grass
loaded with dandelion seed puffs. The row of white puffs acted just as a white line,
and once in viewing range Omnibot obligingly corrected her course to be parallel to
them. It was sheer luck that the path curved so that when the dandelions ran out,
Omnibot continued straight and intersected with the path. While Omnibot wasn’t
programmed to react to shoes and dandelions, it did react correctly considering its
ecological niche.
Step 6: Test each behavior independently. As in any software engineering
project, modules or behaviors are tested individually. Ideally, testing occurs
in simulation prior to testing on the robot in its environment. Many commer-
cially available robots such as Khepera and Nomads come with impressive
simulators. However, it is important to remember that simulators often only
model the mechanics of the robot, not the perceptual abilities. This is useful
for conﬁrming that the motor schema code is correct, but often the only way
to verify the perceptual schema is to try it in the real world.
Step 7: Test with other behaviors. The ﬁnal step of designing and im-
plementing a reactive system is to perform integration testing, where the be-
haviors are combined. This also includes testing the behaviors in the actual
environment.
Although the follow-line behavior worked well when tested with white lines, it
did not perform well when tested with white lines and obstacles. The obstacles,
shiny plastic-wrapped bales of hay sitting near the line, were often brighter than the
grass. Therefore the perceptual schema for follow-line included pixels belonging to
the bale in computing the centroid. Invariably the robot became ﬁxated on the bale,

5.4
Case Study: Unmanned Ground Robotics Competition
171
and followed its perimeter rather than the line. The bales were referred to as “visual
distractions.”
Fortunately, the bales were relatively small. If the robot could “close its eyes” for
about two seconds and just drive in a straight line, it would stay mostly on course.
This was called the move-ahead behavior. It used the direction of the robot (steering
angle, dir) to essentially produce a uniform ﬁeld. The issue became how to know
when to ignore the vision input and deploy move-ahead.
The approach to the issue of when to ignore vision was to use the sonar as a releaser
for move-ahead. The sonar was pointed at the line and whenever it returned a range
reading, move-ahead took over for two seconds. Due to the difﬁculties in working
with DOS, the CSM entry had to use a ﬁxed schedule for all processes. It was easier
and more reliable if every process ran every update cycle, even if the results were
discarded. As a result the sonar releaser for move-ahead essentially inhibited follow-
line, while the lack of a sonar releaser inhibited move-ahead. Both behaviors ran all
the time, but only one had any inﬂuence on what the robot did. Fig. 5.6 shows this
inhibition, while the new behavioral table is shown below.
New Behavior Table
Releaser
Inhibited by
Behavior
Motor Schema
Percept
Perceptual Schema
always on
near=read_sonar()
follow_line()
stay-on-path(c_x)
c_x
compute_centroid(image,white)
always on
far=read_sonar()
move_ahead(dir)
uniform(dir)
dir
dead_reckon(shaft-encoders)
The ﬁnal version worked well enough for the CSM team to take ﬁrst place. It went
all the way around the track until about 10 yards from the ﬁnish line. The judges
had placed a shallow sand pit to test the traction. The sand pit was of some concern
since sand is a light color, and might be interpreted as part of the line. Since the sand
was at ground level, the range reading could not be used as an inhibitor. In the end,
the team decided that since the sand pit was only half the length of a bale, it wouldn’t
have enough effect on the robot to be worth changing the delicate schedule of existing
processes.
The team was correct that the sand pit was too small to be a signiﬁcant visual
distraction. However, they forgot about the issue of traction. In order to get more
traction, the team slipped real tires over the slick plastic wheels, but forgot to attach
them. Once in the sand, the robot spun its wheels inside the tires. After the time limit
was up, the team was permitted to nudge the robot along (done with a frustrated kick
by the lead programmer) to see if it would have completed the entire course. Indeed
it did. No other team made it as far as the sand pit.
It is clear that a reactive system was sufﬁcient for this application. The use
of primitive reactive behaviors was extremely computationally inexpensive,

172
5
Designing a Reactive Implementation
distraction?
sonar
vision
follow−line
  behavior
move−ahead
 behavior
shaft encoders
yes
 no
steering actuator
Figure 5.6
The behavioral layout of the CSM entry in the 1994 Unmanned Ground
Vehicle Competition.
allowing the robot to update the actuators almost at the update rate of the
vision framegrabber.
There are several important lessons that can be extracted from this case
study:
 The CSM team evolved a robot which ﬁt its ecological niche. However, it
was a very narrow niche. The behaviors would not work for a similar do-
main, such as following sidewalks, or even a path of white lines with an
intersection. Indeed, the robot reacted to unanticipated objects in the en-
vironment such as the judge’s white shoes. The robot behaved “correctly”
to features of the open world, but not in a desirable fashion.
 This example is a case where the releaser or inhibitor stimulus for a be-
havior was not the same perception as the percept needed to accomplish
the task. The sonar was used to inhibit the behaviors. Follow-line used vi-
sion, while move-ahead integrated shaft encoder data to continue to move
in the last good direction.

5.5
Assemblages of Behaviors
173
 This example also illustrates the tendency among purely reactive motor
schema to assign one sensor per behavior.
5.5
Assemblages of Behaviors
The UGV competition case study illustrated the basic principles of the design
of reactive behaviors. In that case, there were a trivial number of behaviors.
What happens when there are several behaviors, some of which must run
concurrently and some that run in a sequence? Clearly there are releasers
somewhere in the system which determine the sequence. The issue is how
to formally represent the releasers and their interactions into some sort of
sequencing logic. If a set of behaviors form a prototypical pattern of action,
they can be considered a “meta” or “macro” behavior, where a behavior is
assembled from several other primitive behaviors into an abstract behavior.
This raises the issue of how to encapsulate the set of behaviors and their
sequencing logic into a separate module.
The latter issue of encapsulation is straightforward. The same OOP schema
structure used to collect a perceptual schema and a motor schema into a be-
havior can be used to collect behaviors into an abstract behavior, as shown
by a behavior being composed of behaviors in Fig. 5.1. The coordinated con-
trol program member of the abstract behavior expresses the releasers for the
component behaviors.
This leaves the issue of how to formally represent the releasers in a way
that both the robot can execute and the human designer can visualize and
troubleshoot. There are three common ways of representing how a sequence
of behaviors should unfold: ﬁnite state automata, scripts and skills. Finite state
SKILLS
automata and scripts are logically equivalent, but result in slightly differ-
ent ways about thinking about the implementation. Skills collect behavior-
like primitives called Reaction-Action Packages (RAPs) into a “sketchy plan”
which can be ﬁlled in as the robot executes. FSA-type of behavioral coordina-
tion and control were used successfully by the winning Georgia Tech team19
in the 1994 AAAI Pick Up the Trash event, and the winning LOLA team in the
1995 IJCAI competition for the Pick Up the Trash event. Scripts were used
by the Colorado School of Mines team in the 1995 competition; that entry
performed behaviorally as well as the winning teams but did not place due
to a penalty for not having a manipulator. Those three teams used at most
eight behaviors, even though LOLA had a more sophisticated gripper than
the Georgia Tech team. In contrast, CHIP the second place team in the IJCAI

174
5
Designing a Reactive Implementation
a.
b.
Figure 5.7
Two award-winning Pick Up the Trash competitors: a.) Georgia Tech’s
robots with the trash gripper (photograph courtesy of Tucker Balch and AAAI), and
b.) University of North Carolina’s LOLA (photograph courtesy of Rich LeGrand and
AAAI).
competition required 34 skills and 80 RAPs to do the same task, in part be-
cause of the complexity of the arm.53;54 Since in general skills lead to a more
complex implementation than FSA and scripts, they will not be covered here.
The most important point to remember in assembling behaviors is to try to
make the world trigger, or release, the next step in the sequence, rather than
rely on an internal model of what the robot has done recently.
5.5.1
Finite state automata
Finite state automata (FSA) are a set of popular mechanisms for specifying
FINITE STATE
AUTOMATA
what a program should be doing at a given time or circumstance. The FSA
can be written as a table or drawn as a state diagram, giving the designer a
STATE DIAGRAM
visual representation. Most designers do both. There are many variants of

5.5
Assemblages of Behaviors
175
move-
ahead
follow-
line
time-elapsed
range near
time-remaining
range far
a.
M
:
K
=
ffollow-line, move-ahead g,

=
frange near, range farg,
s
=follow-line,
F
=
ffollow-line, move-ahead g
q


(q
;

)
follow-line
range near
move-ahead
follow-line
range far
follow-line
move-ahead
time remaining
move-ahead
move-ahead
time elapsed
follow-line
b.
Figure 5.8
A FSA representation of the coordination and control of behaviors in the
UGV competition: a.) a diagram and b.) the table.
FSA, but each works about the same way. This section follows the notation
used in algorithm development.86
To begin with, the designer has to be able to specify a ﬁnite number of dis-
crete states that the robot should be in. The set of states is represented by
K,
STATES
and each state,
q

K is a listing of the behaviors that should be active at the
same time. In the case of the UGR competition, there were only two states:
following the line and moving forward. States are represented in a table un-
der the heading
q, and by circles in a graph. (See Fig. 5.8.) By convention,
there is always a Start state, and the robot would always start there. The Start
START STATE
state is often written as
s or
q
o and drawn with a double circle. In the case
of the UGR entry, the following-line state was the start state since the robot
always starts with the follow-line behavior active and not suppressed.
The next part of the FSA is the inputs (also called the alphabet). Inputs are
the behavioral releasers, and appear under the column heading
. Unlike
the IRM diagrams, the FSM table considers what happens to each state
q for

176
5
Designing a Reactive Implementation
move-
ahead
follow-
line
time-elapsed
range near
Start
range far
victory-
dance
see finish
line
see finish
line
time-remaining
range far
range near
a.
M:
K
=
ffollow-line, move-ahead g,

=
frange near, range farg,
s
=follow-line,
F
=
ffollow-line,move-ahead g
q


(q
;

)
follow-line
range near
move-ahead
follow-line
range far
follow-line
move-ahead
time remaining
move-ahead
move-ahead
time elapsed
follow-line
b.
Figure 5.9
An alternative FSA representation of the coordination and control of be-
haviors in the UGV competition: a.) a diagram and b.) the table.
all possible inputs. As shown in Fig. 5.8, there are only two releasers for the
UGR example, so the table doesn’t have many rows.
The third part of the FSM is the transition function, called
, which speciﬁes
TRANSITION FUNCTION
what state the robot is in after it encounters an input stimulus,
. The set of
stimulus or affordances
 that can be recognized by the robot is represented
by
 (a capital
). These stimuli are represented by arrows. Each arrow rep-
resents the releaser for a behavior. The new behavior triggered by the releaser
depends on the state the robot is in. This is the same as with Innate Releasing
Mechanisms, where the animal literally ignores releasers that aren’t relevant
to its current state.

5.5
Assemblages of Behaviors
177
Recall also in the serial program implementations of IRMs in Ch. 3 that the
agent “noticed” releasers every second. At one iteration through the loop,
it might be hungry and “enter” the state of feeding. In the next iteration, it
might still be hungry and re-enter the state of feeding. This can be repre-
sented by having arrows starting at the feeding state and pointing back to
the feeding state.
For the example in Fig. 5.8, the robot starts in the state of following a line.
This means that the robot is not prepared to handle a visual distraction (range
near) until it has started following a line. If it does, the program may fail be-
cause the FSA clearly shows it won’t respond to the distraction for at least
one update cycle. By that time, the robot may be heading in the wrong direc-
tion. Starting in the following-line state ﬁne for the UGR competition, where
it was known in advance that there were no bales of hay near the starting
line. A more general case is shown in Fig. 5.9, where the robot can start ei-
ther on a clear path or in the presence of a bale. The FSA doesn’t make it
clear that if the robot starts by a bale, it better be pointed straight down the
path!
The fourth piece of information that a designer needs to know is when the
robot has completed the task. Each state that the robot can reach that termi-
nates the task is a member of the set of ﬁnal state,
F. In the UGR example,
FINAL STATE
the robot is never done and there is no ﬁnal state—the robot runs until it is
turned off manually or runs out of power. Thus, both states are ﬁnal states.
If the robot could recognize the ﬁnish line, then it could have a ﬁnish state.
The ﬁnish state could just be stopped or it could be another behavior, such
as a victory wag of the camera. Notice that this adds more rows to the FSA
table, since there must be one row for each unique state.
In many regards, the FSA table is an extension of the behavioral table.
The resulting table is known as a ﬁnite state machine and abbreviated
M for
machine. The notation:
M
=
fK
;
;

;
s;
F
g
is used as reminder that in order to use a FSA, the designer must know all
the
q states (K), the inputs
 the transitions between the states
, the special
starting state
q
0, and the terminating state(s) ( F). There must be one arrow
in the state diagram for every row in the table. The table below summarizes
the relationship of FSA to behaviors:

178
5
Designing a Reactive Implementation
FSA
Behavioral analog
K
all the behaviors for a task

the releasers for each behavior in
K

the function that computes the new state
s
the behavior the robot starts in when turned on
F
the behavior(s) the robot must reach to terminate
In more complex domains, robots need to avoid obstacles (especially peo-
ple). Avoidance should always be active, so it is often implicit in the FSA.
move-to-goal often is shorthand for move-to-goal and avoid. This implicit
grouping of “interesting task-related behaviors” and “those other behaviors
which protect the robot” will be revisited in Ch. 7 as strategic and tactical
behaviors.
Another important point about using FSA is that they describe the over-
all behavior of a system, but the implementation may vary. Fig. 5.8 is an
accurate description of the state changes in the UGV entry.
The control
for the behaviors could have been implemented exactly as indicated by the
FSA: if following-line is active and range returns range near, then move-
ahead. However, due to timing considerations, the entry was programmed
differently, though with the same result. The following examples will show
how the FSA concept can be implemented with subsumption and schema-
theoretic systems.
5.5.2
A Pick Up the Trash FSA
As another example of how to construct and apply an FSA, consider the Pick
Up the Trash task. Assume that the robot is a small vehicle, such as the ones
used by Georgia Tech shown in Fig. 5.7 or the Pioneer shown in Fig. 5.10,
with a camera, and a bumper switch to tell when the robot has collided with
something. In either case, the robot has a simple gripper. Assume that the
robot can tell if the gripper is empty or full. One way to do this is to put an
IR sensor across the jaw of the gripper. When the IR returns a short range,
the gripper is full and it can immediately close, with a grasping reﬂex. One
problem with grippers is that they are not as good as a human hand. As such,
there is always the possibility that the can will slip or fall out of the gripper.
Therefore the robot should respond appropriately when it is carrying a can
or trash and loses it.
The Pick Up the Trash environment is visually simple, and there are obvi-
ous affordances. Coca-cola cans are the only red objects in the environment,

5.5
Assemblages of Behaviors
179
and trash cans are the only blue objects. Therefore visually extracting red
and blue blobs should be sufﬁcient. All objects are on the ﬂoor, so the robot
only has to worry about where the objects are in the x axis. A basic scenario
is for the robot to start wandering around the arena looking for red blobs.
It should head straight for the center of the largest red blob until it scoops
the can in the forklift. Then it should try three times to grab the can, and if
successful it should begin to wander around looking for a blue blob. There
should only be one blue blob in the image at a time because the two trash
cans are placed in diagonal corners of the arena. Once it sees a blue blob,
the robot should move straight to the center of the blob until the blob gets a
certain size in the image (looming). The robot should stop, let go of the can,
turn around to a random direction and resume the cycle. The robot should
avoid obstacles, so moving to a red or blue blob should be a ﬁxed pattern
action, rather than have the robot immediately forget where it was heading.
The behavior table is:
Releaser
Behavior
Motor Schema
Percept
Perceptual Schema
always on
avoid()
pﬁelds.nat(goal_dir)
bumper_on
read_bumper()
EMPTY=gripper_status()
wander()
pﬁelds.random()
time-remaining
countdown()
EMPTY=gripper_status()
move-to-goal(red)
pﬁelds.attraction(c_x)
c_x
extract-color(red, c_x)
AND
SEE_RED=extract_color(red)
FULL=gripper_status()
grab-trash()
close_gripper()
status
gripper_status()
FULL=gripper_status()
wander()
pﬁelds.random()
time_remaining
countdown()
AND
NO_BLUE=extract_color(blue)
FULL=gripper_status()
move-to-goal(blue)
pﬁelds.attraction(c_x)
c_x
extract-color(blue)
AND
SEE_BLUE=extract_color(blue)
FULL=gripper_status()
drop_trash()
open_gripper()
curr_dir
read_encoders()
AND
turn_new_dir(curr_dir)
AT_BLUE=looming(blue, size=N)
The function calls in the table only show the relevant arguments for brevity.
The avoid behavior is interesting. The robot backs up either to the right or
left (using a NaT) when it bumps something. It may bump an arena wall
at several locations, but eventually a new wander direction will be set. If
the robot bumps a can (as opposed to captures it in its gripper), backing up
gives the robot a second chance. This table shows that the design relies on the
gripper to maintain where the robot is in the nominal sequence. An empty
gripper means the robot should be in the collecting the trash phase, either
looking for a can or moving toward one. A full gripper means the robot is in
the deposit phase. The looming releaser extracts the size of the blue region
in pixels and compares it to the size N. If the region is greater than or equal

180
5
Designing a Reactive Implementation
Figure 5.10
A Pioneer P2-AT with a forklift arm suitable for picking up soda cans.
(Photograph courtesy of ActivMedia, Incorporated.)
to N, then the robot is “close enough” to the trash can and the robot can drop
the can.
There are two problems with the behavior table. The ﬁrst is that it doesn’t
show the sequence, or ﬂow of control, clearly. The second is how did the
designer come up with those behaviors? This is where a FSA is particularly
helpful. It allows the designer to tinker with the sequence and represent the
behavioral design graphically.
Fig. 5.11 shows a FSA that is equivalent to the behavior table. The FSA
may be clearer because it expresses the sequence. It does so at the cost of not
showing precisely how the sequence would be implemented and encour-
aging the designer to create internal states. A programmer might imple-
ment two wander behaviors, one which is instantiated by different releasers
and terminates under different conditions, and two move-to-goal behaviors.
Many designers draw and interpret FSA as carrying forward previous re-
leasers. For example, the correct transition from Grab Trash to Wander For
Trash can is FULL and NO_BLUE, but a designer may be tempted to label the
arrow as only NO_BLUE, since to get that state, the gripper had to be FULL.
This is a very dangerous mistake because it assumes that the implementation
will be keeping up with what internal state the robot is in (by setting a vari-

5.5
Assemblages of Behaviors
181
Start
Wander
for
Trash
Move
to
Trash
Grab
Trash
Wander
for
Trashcan
Move
to
Trashcan
Drop
Trash
FULL
FULL and
NO_BLUE
EMPTY and
SEE_RED
EMPTY and
NO_RED
FULL and
SEE_BLUE
FULL and
AT_BLUE
EMPTY
FULL and 
NO_BLUE
EMPTY
EMPTY
FULL
on
3 tries
EMPTY
FULL and
SEE_BLUE
a.
K
=
fwander for trash, move to trash, grab trash, wander for trash can,
move to trash can, drop trash
g,

=
fon, EMPTY, FULL, SEE_RED,
NO_BLUE, SEE_BLUE, AT_BLUE g,
s
=
S
tar
t,
F
=
K
q


(q
;

)
start
on
wander for trash
wander for trash
EMPTY, SEE_RED
move to trash
wander for trash
FULL
grab trash
move to trash
FULL
grab trash
move to trash
EMPTY, NO_RED
wander for trash
grab trash
FULL, NO_BLUE
wander for trash can
grab trash
FULL, SEE_BLUE
move to trash can
grab trash
EMPTY
wander for trash
wander for trash can
EMPTY
wander for trash
wander for trash can
FULL, SEE_BLUE
move to trash can
move to trash can
EMPTY
wander for trash
move to trash can
FULL, AT_BLUE
drop trash
drop trash
EMPTY
wander for trash
b.
Figure 5.11
A FSA for picking up and recycling Coke cans: a.) state diagram, and
b.) table showing state transitions.

182
5
Designing a Reactive Implementation
able), instead of letting directly perceivable attributes of the world inform the
robot as to what state it is in. Internal state is incompatible with reactivity.
The FSA also hid the role of the avoid behavior. The avoid behavior was
always running, while the other behaviors were asynchronously being in-
stantiated and de-instantiated. It is difﬁcult to show behavioral concurrency
with an FSA. Other techniques, most especially Petri Nets, are used in soft-
ware engineering but have not been commonly used in robotics. The avoid
behavior was not a problem in this case. It was always running and the out-
put of the avoid potential ﬁeld vector could be summed with the output of
whatever other behavior was active.
5.5.3
Implementation examples
In a schema-theoretic implementation, the FSA logic would exist in one of
two places. If the robot’s sole task was to recycle soda cans, the controlling
logic could be placed in the main program. If the robot had many tasks that
it could do, the ability to recycle trash would be an abstract behavior, called by
the main program whenever the robot needed to recycle trash. In that case,
the FSA logic would be placed in the coordinated control program slot of the
behavior schema.
Although the current discussion is on where the FSA goes, it might be use-
ful to spend some time on the overall implementation. While the wander-to-
goal and move-to-goal behaviors can be easily implemented with a potential
ﬁelds methodology, drop-trash cannot. Drop-trash really isn’t a navigation
behavior. It ﬁts the overall proﬁle of a behavioral schema: it has an obvious
motor schema (open the gripper, turn the wheels), a perceptual schema (read
gripper encoders and wheel encoders), a coordinated control program (open
THEN turn), and a releaser (at trash can). While schema-theoretic implemen-
tations use potential ﬁeld methodologies and vector summation for effector
control, not every behavior will generate a vector based on a potential ﬁeld.
One advantage of FSA is that they are abstract, and can be implemented
in a number of ways. The behavior table illustrated one way the FSA could
be implemented with a schema-theoretic system. Fig. 5.12 shows one way
it could be implemented in subsumption. This example shows the power
of inhibition and suppression which is not well represented by FSA state
diagrams.
In keeping with the idea of modularity and incrementally adding behav-
iors, the system starts with an explicit avoid behavior running on top of Level
0 (not shown). At the next level the robot wanders until it sees red. Then

5.5
Assemblages of Behaviors
183
avoid
wander
extract-color(red)
move-to-goal(trash)
extract-color(blue)
move-to-goal(trashcan)
Level 0
gripper-status
drop-trash
I
S
S
I
looming(blue,size=N)
S
grab_trash
FULL
cx
heading
heading
heading
EMPTY
cx
AT_BLUE
Level 1
Figure 5.12
One possible Pick Up the Trash abstract behavior using subsumption.
move-to-trash suppresses wandering and replaces the heading with the di-
rection to the trash. The move-to-trash behavior continues until the can is in
the gripper. Once the gripper is full, the gripper closes and grabs the trash.
Then the move-to-trash behavior is inhibited from executing. This prevents
move-to-trash from sending any directions out, so the wandering direction
is now again the output.
The next level of competency is to deposit trash in the trash can. When
it sees the blue trash can, move-to-trash can begins to suppress wander and
direct the robot to the trash can. If the gripper is empty, the output for move-
to-trash can is inhibited. The robot is simultaneously looking for red and
blue blobs, but as long as the gripper is empty, it only responds to red blobs.
Dropping trash also is constantly executing. If the robot happens to wan-
der next to a blue trash can, it will signal to drop trash and turn around. The
new heading suppresses any heading direction from move-to-trash can. But
the robot will not open its gripper and turn around if the gripper is empty,
because empty inhibits that whole line. The subsumption example produces
a much less complex system than a direct FSA mapping.

184
5
Designing a Reactive Implementation
5.5.4
Abstract behaviors
Finite state automata are a useful tool for expressing the coordinated control
program of an abstract behavior. They fall short as a programming tool for
abstract behaviors in a number of ways. First, in many cases, the assemblage
of behaviors represents a prototypical sequence of events that should be
slightly adapted to different situations, in essence, a template or abstract behav-
TEMPLATE
ABSTRACT BEHAVIOR
ior. In the Pick Up the Trash event, recycling Coke cans was only part of the
task; the robots were also supposed to ﬁnd white Styrofoam cups and deposit
them in yellow trash cans. The behaviors represented by the FSA could be
collected into an abstract behavior: pick-up-the-trash(trash-color,
trash can-color, size-trash can).
Second, templates need to handle different initialization conditions. Ini-
tialization wasn’t a big problem for the Pick Up the Trash task, but it could
be one for other applications. For example, the emergent behavior of the
robot described in the Unmanned Ground Vehicle competition in Sec. 5.4
could be described as an abstract “follow-path” behavior. Recall that the
robot’s program assumed that it started facing the line. A more general pur-
pose, reusable follow-path behavior would need to handle a broader range
of starting conditions, such as starting facing a bale or not perfectly lined up
with the line. Another common initialization behavior is imprinting, where
IMPRINTING
the robot is presented with an object and then records the perceived color (or
other attribute) of the object for use with the nominal behavior. In the Pick
Up the Trash competitions, several teams literally showed the robot the Coke
can and let it determine the best values of “red” for the current lighting con-
ditions. Likewise, some abstract behaviors would need special termination
behavior. In the case of the UGR competition, the termination behaviors was
NULL, but it could have been the victory dance.
Third, some times robots fail in their task; these events are often called
exceptions. An exception might be when the robot does not pick up a soda
EXCEPTIONS
can in over 10 minutes. Another behavior can be substituted (do a raster scan
rather than a random wander) or alternative set of parameters (use different
values for red).
5.5.5
Scripts
Abstract behaviors often use scripts,49;50;87;100 or a related construct called
SCRIPTS
skills,53;54 to create generic templates of assemblages of behaviors. Scripts
SKILLS
provide a different way of generating the logic for an assemblage of behav-

5.5
Assemblages of Behaviors
185
Script
Collection of Behaviors
Example
Goal
Task
pick up and throw away a Coca-Cola can
Places
Applicability
an empty arena
Actors
Behaviors
WANDER_FOR_GOAL, MOVE_TO_GOAL,
GRAB_TRASH, DROP_TRASH
Props, Cues
Percepts
red, blue
Causal Chain
Sequence of Behaviors
WANDER_FOR_GOAL(TRASH), MOVE_TO_GOAL(TRASH),
GRAB_TRASH,WANDER_FOR_GOAL(TRASH CAN),
MOVE_TO_GOAL(TRASH CAN), DROP_TRASH
Subscripts
Exception Handling
if have trash and drop, try GRAB_TRASH three times
Figure 5.13
Comparison of script structures to behaviors.
iors. They encourage the designer to think of the robot and the task literally
in terms of a screenplay. Scripts were originally used in natural language
processing (NLP) to help the audience (a computer) understand actors (peo-
ple talking to the computer or writing summaries of what they did). 123 In the
case of robots, scripts can be used more literally, where the actors are robots
reading the script. The script has more room for improvization though, if the
robot encounters an unexpected condition (an exception), the robot begins
following a sub-script.
SUB-SCRIPT
Fig. 5.13 shows how elements of an actor’s script compares to a robot
script. The main sequence of events is called a causal chain. The causal chain
CAUSAL CHAIN
is critical, because it embodies the coordination control program logic just as
a FSA does. It can be implemented in the same way. In NLP, scripts allow
the computer to keep up with a conversation that may be abbreviated. For
example, consider a computer trying to read and translate a book where the
main character has stopped in a restaurant. Good writers often eliminate all
the details of an event to concentrate on the ones that matter. This missing,
but implied, information is easy to extract. Suppose the book started with
“John ordered lobster.” This is a clue that serves as an index into the cur-
rent or relevant event of the script (eating at a restaurant), skipping over past
events (John arrived at the restaurant, John got a menu, etc.). They also focus
the system’s attention on the next likely event (look for a phrase that indi-
cates John has placed an order), so the computer can instantiate the function
which looks for this event. If the next sentence is “Armand brought out the
lobster and reﬁlled the white wine,” the computer can infer that Armand is
the waiter and that John had previously ordered and received white wine,
without having been explicitly told.

186
5
Designing a Reactive Implementation
In programming robots, people often like to abbreviate the routine por-
tions of control and concentrate on representing and debugging the impor-
tant sequence of events. Finite state automata force the designer to consider
and enumerate every possible transition, while scripts simplify the speciﬁ-
cation. The concepts of indexing and focus-of-attention are extremely valuable
INDEXING
FOCUS-OF-ATTENTION
for coordinating behaviors in robots in an efﬁcient and intuitive manner. Ef-
fective implementations require asynchronous processing, so the implemen-
tation is beyond the scope of this book. For example, suppose a Pick Up the
Trash robot boots up. The ﬁrst action on the causal chain is to look for the
Coke cans. The designer though realizes that this behavior could generate
a random direction and move the robot, missing a can right in front of it.
Therefore, the designer wants the code to permit the robot to skip searching
the arena if it immediately sees a Coke can, and begin to pick up the can
without even calling the wander-for-goal(red) behavior. The designer also
knows that the next releaser after grab-trash exits to look for is blue, because
the cue for moving to the trash can and dropping off trash is blue.
The resulting script for an abstract behavior to accomplish a task is usually
the same as the programming logic derived from an FSA. In the case of Pick
Up the Trash, the script might look like:
for each update...
\\ look for props and cues first: cans, trash cans, gripper
rStatus=extract_color(red, rcx, rSize); \\ ignore rSize
if (rStatus==TRUE)
SEE_RED=TRUE;
else
SEE_RED=FALSE;
bStatus=extract_color(blue, bcx, bSize);
if (bStatus==TRUE){
SEE_BLUE=TRUE; NO_BLUE=FALSE;
} else {
SEE_BLUE=FALSE; NO_BLUE=TRUE;
}
AT_BLUE=looming(size, bSize);
gStatus=gripper_status();
if (gStatus==TRUE) {
FULL=TRUE; EMPTY=FALSE;
} else {
FULL=FALSE; EMPTY=TRUE;
}

5.6
Summary
187
\\index into the correct step in the causal chain
if (EMPTY){
if (SEE_RED){
move_to_goal(red);
else
wander();
} else{
grab_trash();
if (NO_BLUE)
wander();
else if (AT_BLUE)
drop_trash();
else if (SEE_BLUE)
move_to_goal(blue);
}
5.6
Summary
As deﬁned in Ch. 4, a reactive implementation consists of one or more be-
haviors, and a mechanism for combining the output of concurrent behaviors.
While an architectural style (subsumption, potential ﬁelds) may specify the
structure of the implementation, the designer must invest signiﬁcant effort
into developing individual behaviors and into assembling them into a se-
quence or an abstract behavior.
Schema theory is highly compatible with Object Oriented Programming. A
behavior is derived from the schema class; it is a schema that uses at least one
perceptual and motor schema. If a behavior is composed of multiple schema,
it must have a coordinated control program to coordinate them. Finite State
Automata offer a formal representation of the coordination logic needed to
control a sequence of behaviors. Scripts are an equivalent mechanism with a
more natural story-like ﬂow of control.
The steps in designing robot intelligence under the Reactive Paradigm are:
1. Describe the task,
2. Describe the robot,
3. Describe the environment,
4. Describe how the robot should act in response to its environment,

188
5
Designing a Reactive Implementation
5. Reﬁne each behavior,
6. Test each behavior independently,
7. Test with other behaviors,
and repeat the process as needed. A behavior table serves as a means of repre-
senting the component schemas and functions of the behavioral system. For
each behavior, it shows the releasers, the motor schemas, the percept, and
the perceptual schemas.
These steps emphasize the need to fully specify the ecological niche of the
robot in order to design useful behaviors. Since the idea of behaviors in the
Reactive Paradigm is derived from biology, it is not strange that the idea of
a robot being evolved to ﬁt its environment should also be part and parcel
of the design process. Regardless of the implementation of the coordination
program , the control should rely on the world to inform the robot as to what
to do next, rather than rely on the program to remember and maintain state.
5.7
Exercises
Exercise 5.1
What is the difference between a primitive and an abstract behavior?
Exercise 5.2
Deﬁne:
a. behavior table
b. causal chain
c. coordinated control programy
Exercise 5.3
Can the perceptual schema and the motor schema for a behavior execute asynchro-
nously, that is, have different update rates?
Exercise 5.4
Fig. 5.2 shows two methods of implementing a potential ﬁelds-based follow-corridor
behavior. A third way is to have two instances of a move-away-from-wall (perpen-
dicular) behavior with a move-parallel-to-wall behavior. What are the advantages
and disadvantages of such an implementation?
Exercise 5.5
List and describe the steps in designing a reactive system.

5.7
Exercises
189
Exercise 5.6
Consider the Pick Up the Trash example in Sec. 5.5.2. The example assumed the arena
was empty except for walls, cans, and trash cans. What would happen if there were
chairs and tables present? Could the gripper accidently scoop a chair or table leg?
How would the system react? What changes, if any, would need to be made to the
behavior table and FSA?
Exercise 5.7
Solve the 1994 International Association for Unmanned Systems Unmanned Ground
Vehicle problem using STRIPS (Ch. 2). Be sure to include a world model and a differ-
ence table. y
Exercise 5.8
Solve the 1994 AAAI Pick Up the Trash problem using STRIPS (Ch. 2). Be sure to
include a world model and a difference table.
Exercise 5.9
How is deﬁning a robot’s behaviors linked to the robot’s ecological niche?y
Exercise 5.10
What is special about a primitive behavior in terms of perceptual and motor schema?y
Exercise 5.11
Construct a Behavioral Table of the behaviors needed for a robot to move from room
to room. y
Exercise 5.12
What are the two main deﬁcits encountered when designing and implementing reac-
tive robotic systems?y
Exercise 5.13
Make up a task for a robot to complete. Describe what each of the 7 steps of the design
methodology would require you to do to complete the task.y
Exercise 5.14
Describe the two methods for assembling primitive behaviors into abstract behaviors:
ﬁnite state automata and scripts.y
Exercise 5.15
Assume that the CSM robot had been wider and needed to use an avoid-obstacle
behavior. Make a Behavior Table to include this new behavior. y
Exercise 5.16
Assuming the state of the robot in question 1, describe how the coordinated control
program should handle concurrent behaviors. y

190
5
Designing a Reactive Implementation
Exercise 5.17
Recall how some mammals freeze when they see motion (an affordance for a preda-
tor) in an attempt to become invisible, and persist in this behavior until the predator
is very close, then ﬂee. Deﬁne this behavior using Behavior Tables.y
Exercise 5.18
Suppose the competition course described in Section 5.4 has been modiﬁed so that a
hay bale can be placed directly on the white line. The robot must move around the
bale and rejoin the line on the other side. Create a behavior table that not only follows
the line but correctly responds to the bale so that it can continue following the line. y
Exercise 5.19
[World Wide Web]
Search the web for the International Association for Unmanned Systems competition
home page and brieﬂy describe three unmanned vehicle projects linked to the IAUS
site. y
Exercise 5.20
[World Wide Web]
Identify at least 5 robot competitions, and for each describe the basic task. Can the
task be accomplished with a reactive robot? Why or why not?
Exercise 5.21
[Programming]
Using Rug Warrior kits, Lego Mindstorms kits, or similar robotics tools, implement
your own schools version of an IAUS Unmanned Ground Robotics Competition.
Your class should decide on the course, the course objectives, rules, and prizes (if
any). Groups should not begin construction of their robot without ﬁrst describing the
steps involved in designing a reactive behavioral system for the task at hand. y
Exercise 5.22
[Programming]
Write a script in pseudo-code for following a hallway. Consider that the robot may
start in a hallway intersection facing away from the hallway it is supposed to take.
Also the robot might start in the middle of a hall facing a wall, rather than having
forward point to the long axis of the hall.
Exercise 5.23
[Programming]
The Pick Up the Trash competitions were a bit different than actually presented in
this book. For example, the robots were actually permitted to cache up to three cans
before going to the trash can, and the robots could go after white Styrofoam cups as
trash. How would you integrate this into:
a. the FSA and
b. the script described in the book?
Exercise 5.24
[Programming]
Could the exception handling sub-scripts for picking up trash be implemented with
the exception handling functionality provided by Ada or C++?

5.8
End Notes
191
Exercise 5.25
[Advanced Reading]
Read Rodney Brooks’ one paragraph introduction to Chapter 1 in Cambrian Intelli-
gence 28 on the lack of mathematics in behavioral robotics. Now consider the behavior
of the CSM robot around white shoes and dandelions. Certainly it would be useful
to have theories which can prove when a behavioral system will fail. Is it possible?
Questions marked with a
y were suggested by students from the University of
Wisconsin Lacrosse.
5.8
End Notes
For the roboticist’s bookshelf.
Artiﬁcial Intelligence and Mobile Robots 14 is a collection of case studies from top mobile
robotics researchers, detailing successful entries in various robot competitions. The
book provides many insights into putting theory into practice.
The IAUS Ground Robotics Competition.
The International Association for Unmanned Systems (formerly known as the As-
sociation for Unmanned Vehicle Systems) sponsors three annual robot competitions
for student teams: Ground Robotics, Aerial Vehicle, and Underwater. The Ground
Robotics competition is the oldest, having started in 1992. As noted in a Robotics
Competition Corner column, 101 the competitions tend to reward teams for building
platforms rather than programming them.
Reusability principle of software engineering and beer cans.
While researchers worried about picking up and disposing of soda cans, a popular
magazine began discussing what it would take to have a robot go to the refrigerator
and bring the user a beer. In an apparent acknowledgment of the lifestyle of someone
who would have such a robot, the robot was expected to be able to recognize and
avoid dirty underwear lying at random locations on the ﬂoor.
The 1995 IJCAI Mobile Robot Competition.
The competition was held in Montreal, Canada, and many competitors experienced
delays shipping their robots due to delays at Customs. The University of New Mexico
team spent the preparatory days of the competition frantically trying to locate where
their robot was. Almost at midnight the evening before the preliminary rounds were
to begin, a forklift drove up with the crate for their robot. All the teams looked up
from their frantic testing and cheered in support of the New Mexicans. Until the fork
lift came close enough and everyone could clearly see that the “This Side Up” arrow
was pointing down...The New Mexicans didn’t even bother to un-crate the robot to
catalog the damage; they instead sought solace in Montreal’s wonderful night life.

192
5
Designing a Reactive Implementation
The teams with robots properly aligned with gravity went back to programming and
ﬁne tuning their entries.
Cheering at robot competitions.
The chant “Core Dump. Core Dump. Segmentation Fault!” has a nice cadence and is
especially appropriate to yell at competitors using Unix systems.
Subsumption and Soda Cans.
Jon Connell addressed this task in 1989 in his thesis work at MIT, 39 applying sub-
sumption to a robot arm, not a set of ﬂappers. He used a special type of FSM called
an Augmented Finite State Machines (AFSMs), and over 40 behaviors to accomplish
the task.
Being in a Pick Up the Trash competition without a manipulator.
As often happens, robot competitions often pose problems that are a step beyond
the capability of current hardware and software technology. In 1995, arms on mobile
robots were rare; indeed Nomad introduced a forklift arm just in time for the compe-
tition. Participants, such as the Colorado School of Mines with an older robot and no
arm, could have a “virtual manipulator” with a point deduction. The robot would
move to within an agreed tolerance of the object, then either play a sound ﬁle or make
a noise. The virtual manipulator—a human team member, either Tyler Devore, Dale
Hawkins, or Jake Sprouse—would then physically either pick up the trash and place
it on the robot or remove the trash. It made for an odd reversal of roles: the robot
appeared to be the master, and the student, the servant!
About grippers maintaining the state of world.
The Pick Up the Trash event mutated in 1996 to picking up tennis balls in an empty
arena, and in 1997 into a variation on sampling Mars. For the 1997 Find Life on Mars
event, the sponsors brought in real rocks, painted black to contrast with the gray
concrete ﬂoor and blue, green, red, and yellow “martians” or toy blocks. Because of
weight considerations in shipping the rocks, the rocks were about the size of a couple
of textbooks and not that much bigger than the martians. One team’s purely reactive
robot had trouble distinguishing colors during preliminary rounds. It would mis-
identify a rock as a martian during a random search, navigate to it, grip it, and then
attempt to lift it. Since the rock was heavy, the gripper could not reach the full exten-
sion and trigger the next behavior. It would stay there, clutching the rock. Sometimes,
the robot would grip a rock and the gripper would slip. The robot would then try to
grip the rock two more times. Each time it would slip. The robot would give up,
then resume a random search. Unfortunately, the search seemed to invariably direct
the robot back to the same rock, where the cycle would repeat itself. Eventually the
judges would go over and move the robot to another location.

5.8
End Notes
193
Documentaries.
Scientiﬁc American Frontiers did an excellent special on robot competitions called
“Robots Alive!” The special covered the AUVS Aerial Vehicle Competition (take
away lesson: try your robot outdoors before you show up at an outdoor robot com-
petition) and the 1996 AAAI Mobile Robot Competition where the robots picked up
orange tennis balls instead of coca-cola cans.


6
Common Sensing Techniques for
Reactive Robots
Chapter Objectives:
 Describe the difference between active and passive sensors, and give exam-
ples of each.
 Deﬁne each of the following terms in one or two sentences: proprioception,
exterception, exproprioception, proximity sensor, logical sensor, false positive,
false negative, hue, saturation, image, pixel, image function, computer vision.
 List the metrics for rating individual sensors and a sensor suite, and apply
these metrics to a particular application.
 Describe the problems of specular reﬂection, cross-talk, and foreshortening
with an ultrasonic transducer, and if given a 2D line drawing of surfaces,
illustrate where each of these problems would likely occur.
 Describe the types of behavioral sensor fusion and be able to apply to a real-
world problem.
 Write perceptual schemas from any logically equivalent range sensor to
produce a polar plot percept for obstacle avoidance behavior.
 If given a small interleaved RGB image and a range of color values for a
region, be able to 1) threshold the image on color and 2) construct a color
histogram.
 Write computer vision code to enable a robot to imprint on and track a
color.

196
6
Common Sensing Techniques for Reactive Robots
6.1
Overview
To review Ch. 3, perception in a reactive robot system has two roles: to re-
lease a behavior, and to support or guide the actions of the behavior. All sensing
is behavior-speciﬁc, where behaviors may tap into the same sensors, but use
the data independently of each other. Also, the connotation of reactive robots
is that behaviors are most often stimulus-response, relying on direct percep-
tion rather than require memory. Recognition is not compatible with reactiv-
ity. In order for a reactive robot to be successful in theory, it must have reli-
able perception since perception and action are tightly coupled. For it to be
successful in practice, a robot has to have perceptual hardware and software
which updates quickly. This chapter covers the most common sensing tech-
niques for reactive robots. The sensors and sensing techniques described in
this chapter also apply to robot architectures in the Hybrid paradigm, since
Hybrid architectures use reactive behaviors.
Regardless of sensor hardware or application, sensing and sensors can be
thought of interacting with the world and robots as shown in Fig. 6.1. The
sensor is a device that measures some attribute of the world. The term trans-
SENSOR
TRANSDUCER
ducer is often used interchangeably with sensor. A transducer is the mech-
anism, or element, of the sensor that transforms the energy associated with
what is being measured into another form of energy.2 A sensor receives en-
ergy and transmits a signal to a display or a computer. Sensors use transduc-
ers to change the input signal (sound, light, pressure, temperature, etc.) into
an analog or digital form capable of being used by a robot. In a reactive robot,
the sensor observation is intercepted by a perceptual schema which extracts
the relevant percept of the environment for the behavior. This percept is then
used by the motor schema, which leads to an action.
A sensor is often classiﬁed as being either passive sensor or active sensor.
PASSIVE SENSOR
ACTIVE SENSOR
Passive sensors rely on the environment to provide the medium for observa-
tion, e.g., a camera requires a certain amount of ambient light to produce a
usable picture. Active sensors put out energy in the environment to either
change the energy or enhance it. A sonar sends out sound, receives the echo,
and measures the time of ﬂight. An X-ray machine emits X-rays and mea-
sures the amount blocked by various types of tissue. Although a camera is a
passive device, a camera with a ﬂash is active. The term active sensor is not
the same as active sensing. Active sensing connotes the system for using an
ACTIVE SENSING
effector to dynamically position a sensor for a “better look.” A camera with
a ﬂash is an active sensor; a camera on a pan/tilt head with algorithms to
direct the camera to turn to get a better view is using active sensing.

6.1
Overview
197
Environment
Observation 
or Image
Percept
Sensor
Motor
Schema
 Robot 
Action 
Perceptual
Schema  
active  
medium
SENSOR/TRANSDUCER
BEHAVIOR
ACTION
Figure 6.1
A model of sensing.
Different sensors measure different forms of energy. This in turns leads to
different types of processing. Sensors which measure the same form of en-
ergy and process it in similar ways form a sensor modality. A sensor modality
MODALITY
refers to what the raw input to the sensor is: sound, pressure, temperature,
light, and so on. In some regards, modalities are similar to the ﬁve senses in
humans. A modality can be further subdivided, for instance, vision can be
decomposed into visible light, infrared light, X-rays, and other modalities.
6.1.1
Logical sensors
A powerful abstraction of sensors is logical sensors, ﬁrst introduced by Hen-
LOGICAL SENSORS
derson and Shilcrat.65 A logical sensor is a unit of sensing or module that
supplies a particular percept. It consists of the signal processing from the
physical sensor and the software processing needed to extract the percept; it
is the functional building block for perception. A logical sensor can be easily
implemented as a perceptual schema.
An overlooked aspect of a logical sensor is that it contains all available
alternative methods of obtaining that percept. For example, a percept com-
monly used for obstacle avoidance is a polar plot of range data. The logical
sensor for the percept might be named range_360 and return a data struc-
ture or object specifying the polar plot. The logical sensor would go further
and list all the possible ways the robot had for constructing a polar plot of
that form. The robot might be able to use sonar, a laser, stereo vision, or tex-
ture. Each of those modules would be logically equivalent; that is, they would
LOGICAL EQUIVALENCE
return the same percept data structure so they can be used interchangeably.

198
6
Common Sensing Techniques for Reactive Robots
However, they wouldn’t necessarily be equivalent in performance or update
rate. As will be seen in this chapter, the sonar is liable to produce a noisy
percept in a second or two, while stereo vision may take minutes. Even dif-
ferent stereo vision algorithms may produce different results on the same
data stream. Therefore, the logical sensor contains a selector function which
speciﬁes the conditions under which each alternative is useful and should be
selected.
Notice that a logical sensor can be implemented as a perceptual schema,
where the methods are the alternative means of generating the percept and
the coordinated control strategy contains the knowledge as to when a par-
ticular method is appropriate. Also note that each individual method can
be implemented as a perceptual schema, leading to the recursive, building-
block effect.
In reactive systems, the term logical sensor has degenerated somewhat
from its original usage and is essentially equivalent to a perceptual schema.
“Logical sensor” is often used to connote information hiding, where the par-
ticular sensor and processing algorithm is hidden in the “package.” This is
useful because a robot might use the same physical sensor in two different
ways. An avoid behavior might use a polar plot of sonar range data, while a
panic-stop behavior might use the minimum of all the incoming sonar data.
Since the perceptual schema use the raw sonar data differently, it is as if they
were different sensors.
6.2
Behavioral Sensor Fusion
Sensor fusion is a broad term used for any process that combines information
SENSOR FUSION
from multiple sensors into a single percept. The motivation for sensor fusion
stems from three basic combinations of sensors: redundant (or competing),
REDUNDANT
complementary, and coordinated. Although many researchers treat sensor fu-
COMPLEMENTARY
COORDINATED
sion as a means of constructing a global world model in a hierarchical or de-
liberative system, sensor fusion can be incorporated into behaviors through
sensor ﬁssion, action-oriented sensor fusion, and sensor fashion.
In some cases multiple sensors are used when a particular sensor is too
imprecise or noisy to give reliable data. Adding a second sensor can give
another “vote” for the percept. When a sensor leads the robot to believe that
a percept is present, but it is not, the error is called a false positive. The robot
FALSE POSITIVE
has made a positive identiﬁcation of percept, but it was false. Likewise, an
error where the robot misses a percept is known as a false negative. Sensors
FALSE NEGATIVE

6.2
Behavioral Sensor Fusion
199
Figure 6.2
Example of redundant top and bottom sonar rings.
will often produce different false positive and false negative rates. Whether
a robot can tolerate a higher false positive or false negative rate depends on
the task.
When the sensors are both returning the same percept, the sensors are con-
sidered redundant. An example of physical redundancy is shown in Fig. 6.2,
REDUNDANT
PHYSICAL
REDUNDANCY
where a Nomad 200 has two sonar rings. The sonar software returns the
minimum reading (shortest range) from the two, providing a more reliable
reading for low objects which would ordinarily specularly reﬂect the beam
from the upper sonar. Sensors can also be logically redundant, where they re-
LOGICALLY
REDUNDANT
turn identical percepts but use different modalities or processing algorithms.
An example is extracting a range image from stereo cameras and from a laser
range ﬁnder. Sometimes redundant sensors are called competing sensors, be-
COMPETING SENSORS
cause the sensors can be viewed as competing to post the “winning” percept.
Complementary sensors provide disjoint types of information about a per-
cept. In behavioral sensor fusion for urban search and rescue, a robot may
search for survivors by fusing observations from a thermal sensor for body
heat with a camera detecting motion. Both logical sensors return some aspect
of a “survivor,” but neither provides a complete view. Coordinated sensors
use a sequence of sensors, often for cue-ing or providing focus-of-attention.
A predator might see motion, causing it to stop and examine the scene more
closely for signs of prey.
Most of the work on sensor fusion treats it as if it were a deliberative pro-

200
6
Common Sensing Techniques for Reactive Robots
cess: one that requires a global world model. Early work in reactive systems
used robots with only a few simple sensors, a sonar or sonar ring for range
and a camera for color, texture, or motion affordances. As a result, there
was a philosophy of one sensor per behavior. Behaviors could share a sen-
sor stream, but without knowing it. This philosophy led to the approach
taken by Brooks that sensor fusion at the behavioral level was a mirage. In-
stead sensor fusion was really multiple instances of the same behavior with
different sensor inputs being combined together. To an outside observer, it
would look like some complicated process was being enacted inside the ro-
bot, but in fact it would be a simple competition with an emergent behavior.
Brooks dubbed this sensor ﬁssion in part as a take off on the connotations of
SENSOR FISSION
the word “fusion” in nuclear physics. In nuclear fusion, energy is created by
forcing atoms and particles together, while in ﬁssion, energy is creating by
separating atoms and particles. Fig. 6.3a shows a diagram of sensor ﬁssion.
Murphy reported on studies from cognitive psychology and neurophys-
iology showing that behavioral sensor fusion does occur in animals, and
therefore should be part of a robot’s behavioral repertoire.99 The gist of the
studies was that sensor fusion does occur in behaviors. The sensor pathways
throughout the brain remain separate and can be routed to multiple behav-
iors in the superior colliculus. Only when the sensor signals routed to the
portion of the brain associated with a particular behavior arrive at that loca-
tion does there appear to be any transformation. The transformation appears
to be a new common representation. Any or all of these sensor streams can
be active and inﬂuence the resulting behavior. For example, consider the
predation behavior in cats. If a cat hears a noise and sees a movement, it will
react more strongly than if it has only a single stimulus. This type of sensor
fusion is called action-oriented sensor fusion to emphasize that the sensor data
ACTION-ORIENTED
SENSOR FUSION
is being transformed into a behavior-speciﬁc representation in order to sup-
port a particular action, not for constructing a world model. Fig. 6.3b shows
a diagram of action-oriented sensor fusion.
Sensor ﬁssion and action-oriented sensor fusion cover competing and com-
plementary sensing. Sensor ﬁssion is by deﬁnition a competitive method,
though complementary sensors may be used to support a particular instance
of a behavior. Action-oriented sensor fusion is not restricted to either com-
peting or complementary sensors, since the behavior makes a local transfor-
mation anyway. This leaves the category of coordinated sensors untouched.
Arkin ﬁlled in the apparent gap by calling coordination sensor fashion, an allit-
SENSOR FASHION
erative name intended to imply the robot was changing sensors with chang-
ing circumstances just as people change styles of clothes with the seasons. A

6.2
Behavioral Sensor Fusion
201
combination
mechanism
action
action
action
sensor
percept
sensor
sensor
fusion
behavior
percept
percept
percept
b.
percept
behavior
sensor
percept
behavior
sensor
percept
sensor
a.
behavior
percept
sensor
percept
sensor
percept
sensor
c.
percept
behavior
sequence
selector
action
action
action
Figure 6.3
Three types of behavioral sensor fusion: a.) sensor ﬁssion, b.) action-
oriented sensor fusion, and c.) sensor fashion.

202
6
Common Sensing Techniques for Reactive Robots
diagram of sensor fashion is shown in Fig. 6.3c.
Murphy incorporated pathways for all three types of behavioral sensor
fusion as well as other deliberative forms into the Sensor Fusion Effects (SFX)
architecture. SFX will be covered in Ch. 7.
6.3
Designing a Sensor Suite
Historically, reactive robots used either inexpensive infrared (IR) or ultra-
sonic transducers to detect range. The earliest behaviors focused on basic
navigational skills such as obstacle avoidance and wall following. The per-
cept for these behaviors all involve knowing the distance to an occupied area
of space. Now with the advent of inexpensive miniature cameras and laser
range ﬁnders for consumer applications, computer vision is becoming in-
creasingly common. In agricultural and transportation applications of re-
active robots, GPS technology has become popular as well. This chapter
attempts to cover the basics of these sensing modalities, and how they are
used in mobile robots. Because the sensor market is rapidly changing, the
chapter will focus on how to design a suite of sensors for use by a robot,
rather the device details.
An artiﬁcially intelligent robot has to have some sensing in order to be con-
sidered a true AI robot. If it cannot observe the world and the effects of its
actions, it cannot react. As noted in the chapter on “Action-Oriented Percep-
tion” in Artiﬁcial Intelligence and Mobile Robots: Case Studies of Successful Robot
Systems, the design of a set of sensors for a robot begins with an assessment
of the type of information that needs to be extracted. 14 This information can
be either from proprioception (measurements of movements relative to an in-
PROPRIOCEPTION
ternal frame of reference), exteroception (measurements of the layout of the
EXTEROCEPTION
environment and objects relative to the robot’s frame of reference) or expro-
EXPROPRIOCEPTION
prioception (measurement of the position of the robot body or parts relative
to the layout of the environment).
The Colorado School of Mines ﬁelded an entry to the 1995 UGV compe-
tition entry discussed in Ch. 5. This provides an example of different types
of sensing for a path following robot. In 1995, the follow-path behav-
ior was expanded to track both lines of the path using a wide angle lens
on the camera. follow-path could be considered exteroceptive because it
acquired information on the environment. However, the camera for the ro-
bot was mounted on a panning mast, which was intended to turn to keep
the line in view, no matter what direction the path turned in. Therefore, the

6.3
Designing a Sensor Suite
203
robot had to know where the camera was turned relative to the robot’s inter-
nal frame of reference in order to correctly transform the location of a white
line in image coordinates to a steering direction. This meant the information
needed for follow-path had both proprioceptive and exteroceptive com-
ponents, making the perception somewhat exproprioceptive. (If the robot
was extracting the pose of its camera from exteroception, it would be clearly
exproprioceptive.)
Due to a programming error, the follow-path behavior incorrectly assumed
that the exteroceptive camera data had been transformed by the propriocep-
tive shaft encoder data from the panning mast into exproprioceptive data.
The robot needed the exproprioception to determine where it should move
next: turn to follow the path in camera coordinates, plus the compensation
for the current camera pan angle. The programming error resulted in the
robot acting as if the camera was aligned with the center of the robot at all
times. But the camera might be turned slightly to maintain the view of both
lines of the path through the pan-camera behavior. The resulting naviga-
tional command might be to turn, but too little to make a difference, or even
to turn the wrong way. This subtle error surfaced as the robot went around
hair pin turns, causing the robot to go consistently out of bounds.
6.3.1
Attributes of a sensor
As can be seen by the above example, robots may have dead reckoning capa-
bilities, but will always have some type of exteroceptive sensor. Otherwise,
the robot cannot be considered reactive: there would be no stimulus from
the world to generate a reaction. The set of sensors for a particular robot is
called a sensor suite. Following Sensors for Mobile Robots,52 in order to con-
SENSOR SUITE
struct a sensor suite, the following attributes should be considered for each
sensor:
1. Field of view and range. Every exteroceptive sensor has a region of
space that it is intended to cover. The width of that region are speciﬁed by
the sensor’s ﬁeld of view, often abbreviated as FOV. The ﬁeld of view is usu-
ally expressed in degrees; the number of degrees covered vertically may be
different from the number of degrees covered horizontally. Field of view is
frequently used in photography, where different lenses capture different size
and shape areas. A wide angle lens will often cover up to
0
, while a “reg-
ular” lens may only have a ﬁeld of view around

. The distance that the
ﬁeld extends is called the range.
The ﬁeld of view (FOV) can be thought of in terms of egocentric spherical
FIELD OF VIEW (FOV)

204
6
Common Sensing Techniques for Reactive Robots
coordinates, where one angle is the horizontal FOV and the other is the vertical
HORIZONTAL FOV
VERTICAL FOV
FOV. The other aspect is the range, or how far the sensor can make reliable
RANGE
measurements. In spherical coordinates, this would be the values of
r that
deﬁned the depth of the operating range.
Field of view and range are obviously critical in matching a sensor to an
application. If the robot needs to be able to detect an obstacle when it’s 8 feet
away in order to safely avoid it, then a sensor with a range of 5 feet will not
be acceptable.
2. Accuracy, repeatability, and resolution. Accuracy refers to how correct
the reading from the sensor is. But if a reading for the same conditions is
accurate only 20% of the time, then the sensor has little repeatability. If the
sensor is consistently inaccurate in the same way (always 2 or 3 cm low),
then the software can apply a bias (add 2 centimeters) to compensate. If the
inaccuracy is random, then it will be difﬁcult to model and the applications
where such a sensor can be used will be limited. If the reading is measured
in increments of 1 meter, that reading has less resolution than a sensor reading
RESOLUTION
which is measured in increments of 1 cm.
3. Responsiveness in the target domain. Most sensors have particular
environments in which they function poorly. Another way of viewing this is
that the environment must allow the signal of interest to be extracted from
noise and interference (e.g., have a favorable signal-to-noise ratio). As will
be seen below, sonar is often unusable for navigating in an ofﬁce foyer with
large amounts of glass because the glass reﬂects the sound energy in ways
almost impossible to predict. It is important to have characterized the eco-
logical niche of the robot in terms of what will provide, absorb, or deﬂect
energy.
4. Power consumption. Power consumption is always a concern for robots.
Since most robots operate off of batteries, the less power they consume, the
longer they run. For example, the battery life on a Nomad 200, which carries
ﬁve batteries, was improved from four hours to six by shutting off all sensors.
Power is so restricted on most mobile robots that many robot manufacturers
will swap microprocessor chips just to reduce the power drain (which was
part of the motivation for the Transmeta Crusoe chip). Sensors which require
a large amount of power are less desirable than those which do not. In gen-
eral, passive sensors have less power demands than active sensors because
they are not emitting energy into the environment.
The amount of power on a mobile robot required to support a sensor pack-
age (and any other electronics such as a microprocessor and communications
links) is sometimes called the hotel load. The sensor suite is the “guest” of the
HOTEL LOAD

6.3
Designing a Sensor Suite
205
platform. The power needed to move the robot is called the locomotion load.
LOCOMOTION LOAD
Unfortunately, many robot manufacturers focus on only the locomotion load,
balancing power needs with the desire to reduce the overall weight and size.
This leads to a very small hotel load, and often prevents many sensors from
being added to platform.
5. Hardware reliability. Sensors often have physical limitations on how
well they work. For example, Polaroid sonars will produce incorrect range
reading when the voltage drops below 12V. Other sensors have temperature
and moisture constraints which must be considered.
6. Size. The size and weight of a sensor does affect the overall design. A
microrover on the order of a shoebox will not have the power to transport
a large camera or camcorder, but it may be able to use a miniature “Quick-
Cam” type of camera.
The above list concentrated on considerations for the physical aspects of
the sensor. However, the sensors only provide observations; without the soft-
ware perceptual schemas, the behaviors cannot use the sensors. Therefore,
the software that will process the information from a sensor must be consid-
ered as part of the sensor seletion process. 7. Computational complexity.
Computational complexity is the estimate of how many operations an algo-
rithm or program performs. It is often written as a function
O, called the
“order,” where
O
(x) means the number of operations is proportional to
x.
x
is often a function itself. Lower orders are better. An algorithm that executes
with
O
(n) equally consuming operations is faster than one with
O
(n

) oper-
ations. (If you doubt this, see if you can ﬁnd a positive, whole number value
of
n such that
n
>
n
.) Computational complexity has become less critical
for larger robots, with the rapid advances in processors and miniaturization
of components. However, it remains a serious problem for smaller vehicles.
8. Interpretation reliability. The designer should consider how reliable
the sensor will be for the ecological conditions and for interpretation. The
robot will often have no way of determining when a sensor is providing
incorrect information. As a result the robot may “hallucinate” (think it is
seeing things that are not there) and do the wrong thing. Many sensors pro-
duce output which are hard for human to interpret without years of training;
medical X-rays are one example, and synthetic aperature radar (SAR) which
produces polar plots is another. If a sensor algorithm was not working prop-
erly in these modalities, the designer might not be skilled enough to notice
it. Therefore, the algorithms themselves must be reliable.

206
6
Common Sensing Techniques for Reactive Robots
a.
b.
c.
Figure 6.4
Sequence showing a Denning mobile robot with redundant cameras, re-
sponding to sensing failures introduced by Dave Hershberger.
6.3.2
Attributes of a sensor suite
Everett52 recommends that the following attributes should be considered for
the entire sensing suite:
1. Simplicity. As with any design, simple sensors which have straight-
forward hardware and operating principles are more desirable than sensors
which are complex and require constant maintenance.
2. Modularity. In order to reconﬁgure a sensor suite (add new sensors,
remove ones that aren’t appropriate for the task), the designer must be able
to remove one sensor and/or its perceptual schema without impacting any
other sensing capability.
3. Redundancy. In most military and industrial applications, it is impera-
tive that the robot function correctly, since it would put people at risk to try

6.4
Proprioceptive Sensors
207
to retrieve the robot and ﬁx it. Sensors are especially important because a
faulty sensor can cause the robot to “hallucinate.” As a result, sensor suites
may offer some sort of redundancy.
There are two types of redundancy. Physical redundancy means that there
PHYSICAL
REDUNDANCY
are several instances of physically identical sensors on the robot. Fig. 6.4
shows a robot with redundant cameras. In this case, the cameras are mounted
0
 apart, and when one sensor fails, the robot has to “drive backwards” in
order to accomplish its task.
Logical redundancy means that another sensor using a different sensing mo-
LOGICAL
REDUNDANCY
dality can produce the same percept or releaser. For example, the Mars So-
journer mobile robot had a laser striping system for determining the range
to obstacles, which emitted a line of light. If the surface was ﬂat, a cam-
era would see a ﬂat line, whereas if the surface was rough the line would
appear jagged. The robot also carried a second camera which could do ob-
stacle detection via stereo triangulation. The laser striping sensor and the
stereo cameras are logically redundant. They are not physically redundant,
but they produce the same overall information: the location of obstacles rel-
ative to the robot. However, logically redundant sensors are not necessarily
equivalent in processing speed or accuracy and resolution. A stereo range
sensor and algorithm computes the range much slower than a laser striping
system.
Physical redundancy introduces new issues which are the area of active
research investigation. Possibly the most intriguing is how a robot can de-
termine that a sensor (or algorithm) has failed and needs to be swapped out.
Surviving a failure is referred to as fault tolerance. Robots can be programmed
FAULT TOLERANCE
in most cases to tolerate faults as long as they can identify when they occur.
6.4
Proprioceptive Sensors
Proprioception is dead reckoning, where the robot measures a signal origi-
nating within itself. In biology, this is usually some measure of how far an
arm or leg has been extended or retracted. In robotics, actuators are generally
motors. Many motors come with a shaft encoder which measures the number
SHAFT ENCODER
of turns the motor has made. If the gearing and wheel size is known, then the
number of turns of the motor can be used to compute the number of turns
of the robot’s wheels, and that number can be used to estimate how far the
robot has traveled.

208
6
Common Sensing Techniques for Reactive Robots
Proprioception is often only an estimate. This is due to the impact of the
environment on the actual movement of the robot. Arkin in his PhD thesis8
showed that the same wheeled robot, HARV, traveled different distances for
the same encoder count on a sidewalk, grass, and wet grass. The texture of
the different surfaces caused the wheels of HARV mobile to slip to varying
degrees. A robot on a tiled ﬂoor might slip twice as much as on dry grass.
6.4.1
Inertial navigation system (INS)
Aircraft, submarines, and missiles use inertial navigation systems (INS). INS
INERTIAL NAVIGATION
SYSTEMS (INS)
measure movements electronically through miniature accelerometers. As
long as the movements are smooth, with no sudden jarring, and the sam-
ples are taken frequently, an INS can provide accurate dead reckoning to 0.1
percent of the distance traveled.52 However, this technology is unsuitable for
mobile robots for several reasons. The cost of an INS is prohibitive; units run
from $50,000 to $200,000USD. The cost is due in part to having to stabilize the
accelerometers with gyroscopes, as well as the nature of precision electron-
ics. Mobile robots often violate the constraint that motion must be smooth.
A hard bump or sudden turn can exceed the accelerometers’ measurement
range, introducing errors. INS sytems are typically big; smaller devices have
less accuracy. Sojourner, the Mars rover, carried an INS system. In one trek,
it would have stopped 30 cm from a rock it was to sample if it had just used
proprioception. Instead, by using exteroception, it got within 4 cm.
6.4.2
GPS
GPS, or Global Positioning System, is becoming more common on robots,
especially those used to automate farm equipment (an effort called precision
PRECISION
AGRICULTURE
agriculture). GPS systems work by receiving signals from satellites orbiting
the Earth. The receiver triangulates itself relative to four GPS satellites, com-
puting its position in terms of latitude, longitude, altitude, and change in
time. GPS isn’t a proprioceptive sensor per se since the robot must receive
signals from the satellites, external to the robot. However, they are not exte-
roceptive sensors either, since the robot isn’t computing its position relative
to its environment. Since they tend to be used in place of dead reckoning on
outdoor robots, only GPS will be covered here.
Currently the only sets of GPS satellites that a receiver can triangulate it-
self against are the Navstar “constellation” maintained by the United States
Air Force Space Command or the Russian counterpart, GLONOSS, main-

6.4
Proprioceptive Sensors
209
tained by the Russian Federation Ministry of Defense. There are two types
of channels on Navstar, one public, called the Standard Positioning System,
and an encrypted signal, the Precise Positioning System. Until early in the
year 2000, the U.S. military actually introduced an error in the satellite mes-
sage as to where the satellite actually is, which could result in triangulation
errors of up to 100 meters. The error was called selective availability, because it
SELECTIVE
AVAILABILITY
made accurate positioning available only to those users selected by the U.S.
military. This was intended to prevent a hostile country from putting a GPS
receiver on a guided missile and precisely targeting where the President is
giving a talk. Selective availability was turned off in part because of the rise
of civilian uses of GPS, and because it led to interoperability with groups
working with the U.S. military who were using commercial, not military,
GPS.
Many inexpensive hand-held receivers sold to hunters and hikers attempt
to improve on localization by averaging or ﬁltering the readings. This can
reduce the error down to 10-15 meters. Surveyors and GPS specialty compa-
nies such as Trimble and Rockwell have found a way to subtract the error in
the public channel and get performance near the Y-code’s rumored accuracy
of centimeters. The method is called differential GPS (DGPS), where two GPS
DIFFERENTIAL GPS
(DGPS)
receivers are used. One remains stationary, while the other is put on the ro-
bot. If the two receivers are observing the same satellites, then any sudden
change in position on the stationary “base” receiver is due to the induced
error and can be subtracted from the readings at the robot GPS. The ultimate
ﬁx to the induced error will probably come in a few years due to the com-
mercial sector. A consortium of private companies is planning to launch a
new constellation of GPS satellites, Teledesic, which will emit accurate data
at all times in a format that can be decoded only by chips licensed by the
consortium. Teledesic is scheduled to go on-line in 2004.
GPS and DGPS are not complete solutions to the dead reckoning problem
in mobile robots for at least two reasons. First, GPS does not work indoors
in most buildings, especially ofﬁces or factories with large amounts of steel-
reinforced concrete. As with cellular phones, these structures interrupt the
reception of radio signals. Likewise, GPS may not work outdoors in major
cities where skyscrapers act as urban canyons and interfere with reception.
URBAN CANYONS
Second, commercial DGPS systems cost on the order of $30,000 USD, which
is prohibitively high. Several web sites now offer free “do-it-yourself” DGPS
code to create a DGPS from two inexpensive receivers.

210
6
Common Sensing Techniques for Reactive Robots
6.5
Proximity Sensors
Proximity sensors measure the relative distance (range) between the sensor
and objects in the environment. Since the sensor is mounted on the robot, it is
a straightforward computation to translate a range relative to the sensor to a
range relative to the robot at large. Most proximity sensors are active. Sonar,
also called ultrasonics, is the most popular proximity sensor, with infrared,
bump, and feeler sensors not far behind.
6.5.1
Sonar or ultrasonics
Sonar refers to any system for using sound to measure range. Sonars for
different applications operate at different frequencies; for example, a sonar
for underwater vehicles would use a frequency appropriate for traveling
through water, while a ground vehicle would use a frequency more suited
for air. Ground vehicles commonly use sonars with an ultrasonic frequency,
just at the edge of human hearing. As a result the terms “sonar” and “ul-
trasonics” are used interchangeably when discussing extracting range from
acoustic energy.
Ultrasonics is possibly the most common sensor on commercial robots op-
erating indoors and on research robots. They are active sensors which emit a
sound and measure the time it takes for the sound to bounce back. The time
TIME OF FLIGHT
of ﬂight (time from emission to bounce back) along with the speed of sound
in that environment (remember, even air changes density with altitude) is
sufﬁcient to compute the range of the object.
Ultrasonics is common for several reasons. Its evolution paralleled the rise
of the Reactive Paradigm. In the mid-1980’s, Hans Moravec did impressive
robot navigation with a ring of sonars. The ring conﬁguration gave a
0

coverage as a polar plot. This ring was developed by one of the ﬁrst mobile
robot manufacturers, Denning Robotics, and since then sonar rings are often
referred to as “Denning rings,” regardless of manufacturer. Besides provid-
ing direct range measurements, the transducers were cheap, fast, and had
terriﬁc coverage. In the early 1980’s, the Polaroid Land Corporation had de-
veloped small, inexpensive sonars for use as camera range ﬁnders. A bigger
version, the Polaroid Lab Grade ultrasonic transducer, costs on the order of
$30 USD and can measure ranges from 1 to 25 feet with inch resolution over
a ﬁeld of view of
0
. Furthermore, the measurement time was on the order
of seconds versus hours for computer vision. Ultrasonics became the sensor
of choice for behavior-based robots.

6.5
Proximity Sensors
211
Figure 6.5
Polaroid ultrasonic transducer. The membrane is the disk.
A robotic sonar transducer is shown in Fig. 6.5. The transducer is about the
size and thickness of a dollar coin, and consists of a thin metallic membrane.
A very strong electrical pulse generates a waveform, causing the membrane
on the transducer to produce a sound. The sound is audible as a faint click-
ing noise, like a crab opening and closing its pinchers. Meanwhile a timer
is set, and the membrane becomes stationary. The reﬂected sound, or echo,
ECHO
vibrates the membrane which is ampliﬁed and then thresholded on return
signal strength; if too little sound was received, then the sensor assumes the
sound is noise and so ignores it. If the signal is strong enough to be valid, the
timer is tripped, yielding the time of ﬂight.
The key to how useful the data is requires understanding how the sound
wave is generated by the transducer. In reality, the sound beam produces
multiple secondary sound waves which interact over different regions of
space around the transducer before dissipating. Secondary sound waves are
called side lobes. Most robot systems assume that only sound from the main,
SIDE LOBES
or centermost, lobe is responsible for a range measurement. The width of
the main lobe is often modeled as being
0
 wide at about 5 meters away.
However, in practice, reactive robots need to respond to obstacles in the 0.3

212
6
Common Sensing Techniques for Reactive Robots
to 3 meter range. As a result many algorithms only treat the lobe as being
between

 and

 wide depending on how reliable the range readings are
in a particular environment. Ch. 11 will go over this in more detail.
The strength of the main lobe in the environment determines the maxi-
mum range that the sonar can extract reliability. In ideal indoor venues, a
sonar might return ranges of up to 25 feet, while in the outdoors, the same
sonar might go no further than 8 feet with any repeatability. So while the up-
per limit of the range reading depends on the sensor and the environment,
the lower limit does not. Ultrasonic transducers have a “dead time” imme-
diately following emission while the membrane vibration decays. The decay
time translates into an inability to sense objects within 11 inches because
measurements made during this period are unreliable because the mem-
brane may not have stopped ringing.
Regardless of the maximum allowed range return (i.e., does the program
ignore any reading over 3 meters?) and the width of the lobe, most computer
programs divide the area covered by a sonar into the three regions shown in
Fig. 6.6. Region I is the region associated with the range reading. It is an
arc, because the object that returned the sound could be anywhere in the
beam. The arc has a width, because there are some resolution and measure-
ment errors; the width of Region I is the tolerance. Region II is the area that
is empty. If that area was not empty, the range reading would have been
shorter. Region III is the area that is theoretically covered by the sonar beam,
but is unknown whether it is occupied or empty because it is in the shadow
of whatever was in Region I. Region IV is outside of the beam and not of
interest.
Although they are inexpensive, fast, and have a large operating range, ul-
trasonic sensors have many shortcomings and limitations which a designer
should be aware of. Ultrasonic sensors rely on reﬂection, and so are suscep-
tible to specular reﬂection. Specular reﬂection is when the wave form hits a
SPECULAR REFLECTION
surface at an acute angle and the wave bounces away from the transducer.
Ideally all objects would have a ﬂat surface perpendicular to the transducer,
but of course, this rarely happens. To make matters worse, the reﬂected sig-
nal may bounce off of a second object, and so on, until by coincidence return
some energy back to the transducer. In that case, the time of ﬂight will not
correspond to the true relative range.
Even with severely acute angles, the surface is usually rough enough to
send some amount of sound energy back. An exception to this is glass, which
is very common in hospitals and ofﬁces where mail robots operate, but in-
duces serious specular reﬂection. Fortunately this energy is often sufﬁciently

6.5
Proximity Sensors
213
I
III
II
β
R
IV
Figure 6.6
The regions of space observed by an ultrasonic sensor.
range returned
a.
θ
b.
b.
Figure 6.7
Three problems with sonar range readings: a.) foreshortening, b.) spec-
ular reﬂection, and c.) cross-talk.

214
6
Common Sensing Techniques for Reactive Robots
strong to pass the thresholding in the transducer circuit. However, a new
problem, foreshortening, may occur. Recall that a sonar has a
0
 ﬁeld of view.
FORESHORTENING
This means that sound is being broadcast in a
0
 wide cone. If the surface is
not perpendicular to the transducer, one side of the cone will reach the object
ﬁrst and return a range ﬁrst. Most software assumes the reading is along the
axis of the sound wave. If it uses the reading (which is really the reading for

), the robot will respond to erroneous data. There is no solution to this
problem.
Specular reﬂection is not only by itself a signiﬁcant source of erroneous read-
SPECULAR REFLECTION
ings; it can introduce a new type of error in rings of sonars. Consider a ring
of multiple sonars. Suppose the sonars ﬁre (emit a sound) at about the same
time. Even though they are each covering a different region around the ro-
bot, some specularly reﬂected sound from a sonar might wind up getting
received by a completely different sonar. The receiving sonar is unable to tell
the difference between sound generated by itself or by its peers. This source
of wrong reading is called cross-talk, because the sound waves are getting
CROSS-TALK
crossed. Most robot systems stagger the ﬁring of the sonars in a ﬁxed pattern
of four sonars, one from each quadrant of the ring) at a time. This helps some
with cross-talk, but is not a complete or reliable solution. If the sonar sound
frequency and ﬁring rate can be changed (which is generally not the case),
then sophisticated aliasing techniques can be applied. These techniques are
outside the scope of this book.
One researcher, Monet Soldo, told a story of when she developed a reactive
mobile robot for IBM’s T.J.Watson Laboratories during the late 1980’s. The
robot used sonar as its primary sensors, and she had written behaviors to
guide the robot through doors, rooms, and hall successfully at reasonable
speeds. The day came for the big demonstration, which was to be held not
in the hallways of the laboratory but in the front reception area. The robot
navigated successfully out of the lab, down the halls, and then went berserk
when it got to the atrium. She rebooted, and tried again, but with the same
result. After days of trying to debug the code, she realized it wasn’t a code
problem, it was an environment problem: most of the atrium reception area
consisted of glass partitions. The specular reﬂection and cross-talk caused
the robot to hallucinate, although in different ways each time.
I had a similar problem when my robot started navigating in an ofﬁce en-
vironment. In that environment, the robot was expected to navigate among
ofﬁce cubes, or work areas delimited by partitions. The partitions were cov-
ered with cloth to dampen the sound from the workers. Unfortunately, the
cloth also absorbed the sound from the sonars! These stories emphasize the

6.5
Proximity Sensors
215
a.
b.
Figure 6.8
Maps produced by a mobile robot using sonars in: a.) a lab and b.) a
hallway. (The black line is the path of the robot.)
need to consider the operating environment for the sensor and its impact on
the signal.
The impact of the problems of specular reﬂection and cross talk become
easier to see with plots of sonar returns overlaid with the perimeter of the
area they were taken in; see Fig. 6.8. Some walls are invisible, others are too
close. Clearly sonar readings have to be taken with a grain of salt.
The
0
 cone also creates resolution problems. While sonars often have
excellent resolution in depth, they can only achieve that at large distances if
the object is big enough to send back a signiﬁcant portion of the sound wave.
The further away from the robot, the larger the object has to be. Most desk
chairs and table tops present almost no surface area to the sensor and so the
robot will often not perceive their presence and run right into them.
In practice, another problem leads to spurious sonar readings: power. The
generation of a sound wave requires a signiﬁcant pulse of energy. If the robot
is operating at low power levels, the correct waveform will not be gener-
ated and the return signal worthless. This problem is often difﬁcult to debug

216
6
Common Sensing Techniques for Reactive Robots
Figure 6.9
The ring of IR sensors on a Khepera robot. Each black square mounted
on three posts is an emitter and receiver.
by looking at the sonar returns, which often suggest specular reﬂection or
crosstalk. One team in a robot competition attached a voltmeter next to the
emergency stop button on their robot to ensure that this problem would not
go undetected.
One method of eliminating spurious readings, regardless of cause, is to
take the average of three readings (current plus the last two) from each sen-
sor. This method is fairly common on purely reactive robots, but is ad hoc.
As will be seen in later chapters, other approaches treat the reading as be-
ing uncertain and apply formal evidential reasoning techniques to reﬁne the
reading. These uncertainty techniques are employed by architectures oper-
ating under the Hybrid Paradigm, and will be covered in Ch. 11.
6.5.2
Infrared (IR)
Infrared sensors are another type of active proximity sensor. They emit near-
infrared energy and measure whether any signiﬁcant amount of the IR light
is returned. If so, there is an obstacle present, giving a binary signal. IR
sensors have a range of inches to several feet, depending on what frequency
of light is used and the sensitivity of the receiver. The simplest IR proximity
sensors can be constructed from LEDs, which emit light into the environment
and have a range of 3-5 inches. Fig. 6.9 shows the IR emitters and receivers
placed side by side in a single rectangular package on a Khepera robot. These
often fail in practice because the light emitted is often “washed out” by bright

6.5
Proximity Sensors
217
ambient lighting or is absorbed by dark materials (i.e., the environment has
too much noise).
In more sophisticated IR sensors, different IR bands can be selected or
modulated to change the signal-to-noise ratio. This typically ensures that
an object in range doesn’t absorb the light, causing the sensor to miss the
presence of the object. Nomad robots have an IR sensor option.
6.5.3
Bump and feeler sensors
Another popular class of robotic sensing is tactile, or touch, done with bump
TACTILE
and feeler sensors. Feelers or whiskers can be constructed from sturdy wires.
Bump sensors are usually a protruding ring around the robot consisting of
two layers. Contact with an object causes the two layers to touch, creating
an electrical signal. In theory, the sensitivity of a bump sensor can be ad-
justed for different contact pressures; some robots may want a “light” touch
to create a signal rather than a “heavier” touch. In practice, bump sensors are
annoyingly difﬁcult to adjust. In the “Hors d’Oeuvres, Anyone?” event of
the 1997 AAAI Mobile Robot Competition, humans were served ﬁnger food
by robot “waiters.” The robot waiters were various research robots with serv-
ing trays attached. Humans were supposed to communicate to the Colorado
School of Mines’ Nomad 200 robot waiter (seen in Fig. 6.10) that they were
done eating by kicking the bump sensor. The sensitivity of the bump sensor
was so low that it often required many kicks, producing a very comical scene
with Bruce Lee overtones.
Placement of bump sensors is a very important issue. The bump sensors
on a Nomad 200 base clearly protect the robot only from low obstacles not
perceivable by sonar. The Denning mobile robot platforms built in the 1980’s
used a bump sensor that looked much like a thick piece of gray tape. Den-
ning mobile robots look like a fat version of the Nomad 200’s, and the bump
sensor is wrapped around the cowling of the robot at the waist level. Un-
fortunately, in certain turning conﬁgurations, the wheels extend beyond the
cowling. In those situations, the bump sensor is totally useless in preventing
the expensive synchro-drive mechanism from being damaged in a collison.
Feeler sensors are whiskers or antennae, only not as sensitive as those on
animals. Contact of a whisker and an object will trip a binary switch, whereas
there is reason to believe that an insect or animal can extract much more
information. Bump and feeler sensors are actually tactile sensors since they
require the robot to be touching something in order to generate a reading.

218
6
Common Sensing Techniques for Reactive Robots
a.
b.
Figure 6.10
CRISbot, the Colorado School of Mines’ entry in the 1997 AAAI Mo-
bile Robot Competition Hors d’Oeuvres, Anyone? event. a.) Interacting with the
audience. b.) Audience members “communicated” by kicking the two protruding
bumpers near the bottom of the robot. (Photographs courtesy of AAAI.)
However, they are often interchangeable with IR sensors because IR sensors
often operate over the short range (inches) with less reliability.
6.6
Computer Vision
Computer vision refers to processing data from any modality which uses the
COMPUTER VISION
electromagnetic spectrum which produces an image. An image is essentially
IMAGE
a way of representing data in a picture-like format where there is a direct
physical correspondence to the scene being imaged. Unlike sonar, which
returns a single range reading which could correspond to an object any-
where within a
0
 cone, an image implies multiple readings placed in a
two-dimensional array or grid. Every element in the array maps onto a small
region of space. The elements in image arrays are called pixels, a contraction
PIXELS

6.6
Computer Vision
219
of the words “picture element.” The modality of the device determines what
the image measures. If a visible light camera is used, then the value stored
at each pixel is the value of the light (e.g., color). If a thermal camera is used,
then the value is the heat at that region. The function that converts a signal
into a pixel value is called an image function.
IMAGE FUNCTION
Computer vision includes cameras, which produce images over the same
electromagnetic spectrum that humans see, to more exotic technologies: ther-
mal sensors, X-rays, laser range ﬁnders, and synthetic aperature radar. Sim-
ple forms of computer vision are becoming more popular due to the drop in
prices and miniaturization of cameras and because reactive robots need to
exploit affordances such as color or texture.
As noted in the Introduction, computer vision is a separate ﬁeld of study
from robotics, and has produced many useful algorithms for ﬁltering out
noise, compensating for illumination problems, enhancing images, ﬁnding
lines, matching lines to models, extracting shapes and building 3D represen-
tations. Reactive robots tend not to use those algorithms. Most of the algo-
rithms, especially those that remove noise, require many computations on
each pixel in the image; until recently, the algorithms were too computation-
ally expensive to run in real-time. Also, there was a resistance to algorithms
which required any type of memory or modeling. Therefore a robot designed
to follow paths which used vision to extract the path boundary lines in the
current image based on knowledge of the width, then predicted where the
path boundary lines should be in the next image would be on the borderline
of reactivity.
6.6.1
CCD cameras
Computer vision on reactive robots is most often from a video camera, which
uses CCD (charged couple device) technology to detect visible light. A video
camera, such as a camcorder, is arranged so that light falls on an array of
closely spaced metal-oxide semiconductor (MOS) capacitors. Interestingly,
the MOS capacitors are rectangular, not square, so there is some distortion
in creating the image. The capacitors form a shift register, and output is
either a line at a time (“line transfer”) or the whole array at one time (“frame
transfer").
The output of most consumer video signals is analog, and must be digi-
tized for use by a computer. Consumer digital cameras post an analog signal,
but the update rate is too slow at this time for real-time reactive robot con-
trol. The A/D conversion process takes longer than the CCD array can sense

220
6
Common Sensing Techniques for Reactive Robots
light, so the camera device can either have many frame buffers, which create
a pipeline of images (but is expensive), or have a low frame rate.
A framegrabber is a card which ﬁts inside a computer, accepts analog cam-
FRAMEGRABBER
era signals and outputs the digitized results. The card has a software driver
which allows the robot software to communicate with the board. Framegrab-
bers can produce a grayscale or a color digital image. In the early part of
the 1990’s, color-capable framegrabbers were prohibitively expensive, cost-
ing around $3,000 USD. Now color framegrabbers can be purchased from
$300 to $500 USD, and TV tuners which can capture a single frame are avail-
able for $50 USD.
6.6.2
Grayscale and color representation
The framegrabber usually expresses the grayscale value of a pixel as an 8 bit
number (1 byte of computer memory). This leads to 256 discrete values of
gray, with 0 representing black and 255 representing white. (Remember, 256
values means 0...255.)
Color is represented differently. First, there are many different methods of
expressing color. Home PC printers use a subtractive method, where cyan
plus yellow make green. Most commercial devices in the U.S. use a NTSC
(television) standard. Color is expressed as the sum of three measurements:
red, green, and blue. This is simply abbreviated as RGB.
RGB
RGB is usually represented as three color planes, or axes of a 3D cube as
COLOR PLANES
shown in Fig. 6.11. The cubic represents all possible colors. A speciﬁc color
is represented by a tuple of three values to be summed: (R, G, B). Black is
(0,0,0) or 0+0+0, or no measurements on any of the three color planes. White
is (255, 255, 255). The pure colors of red, green, and blue are represented
by (255,0,0), (0,255,0), and (0,0,255) respectively. This is the same as in color
graphics.
Notice that the cube dimensions in the ﬁgure are 256 by 256 by 256, where
256 is the range of integers that can be expressed with 8 bits. Since there
are three color dimensions, a manufacturer may refer to this as 24-bit color
(3 x 8), to distinguish their framegrabber from ones which map color onto a
linear grayscale. The 8-bit color model is what is used to colorize old black
and white movies. There are only 256 values of color, which is quite limited,
and the gray values are often ambiguous. The pixel values of a person’s red
lips might be 185, while their dark blue dress is also 185. A person may
have to indicate which regions in each frame of the ﬁlm where 185=red and
185=dark blue. 8-bit color is not often used for robots, unless the robot will

6.6
Computer Vision
221
Blue    
(0,0,255)
Cyan       
(0,255,255)
Magenta   
(255,0,255)
Red     
(255,0,0)
Black 
(0,0,0)
Green   
(0,255,0)
White         
(255,255,255)
Yellow    
(255,255,0)
Figure 6.11
RGB color cube.
be operating in an environment where the only visible colors will not have
an ambiguity.
24-bit color is usually sufﬁcient for robotics. For other applications of com-
puter vision such as medical imaging, weather forecasting, or military re-
conaissance, an 8 bit resolution is often insufﬁcient. Those applications may
use 10 bits for each color plane. Since 10 bits do not fall on a byte boundary,
it can be awkward to program algorithms for representing and manipulat-
ing this kind of image. Companies such as Pixar make special computers for
these applications.
In programming terms, 24-bit color image is often declared in computer
programs in two ways:
1. Interleaved. Interleaved means the colors are stored together, RGB RGB
RGB ..., and it is the more common representation. The order is almost
always red, green, then blue, although there may be framegrabbers which
do not follow that convention. Below is a code fragment where the color
is displayed for a pixel at location row, col.
#define
RED
0
#define
GREEN 1
#define
BLUE 2

222
6
Common Sensing Techniques for Reactive Robots
int
image[ROW][COLUMN][COLOR_PLANE];
...
red = image[row][col][RED];
green = image[row][col][GREEN];
blue = image[row][col][BLUE];
display_color(red, green, blue);
2. Separate. Some framegrabbers store an image as three separate two-di-
mensional arrays, as shown below. Some framegrabbers have functions
which return each color plane separately or interleaved. The equivalent
code fragment is:
int
image_red[ROW][COLUMN];
int
image_green[ROW][COLUMN];
int
image_blue[ROW][COLUMN];
...
red = image_red[row][col];
red = image_green[row][col];
red = image_blue[row][col];
display_color(red, green, blue);
The RGB representation has disadvantages for robotics. Color in RGB is
a function of the wavelength of the light source, how the surface of the ob-
ject modiﬁes it (surface reﬂectance), and the sensitivity of the sensor. The
ﬁrst problem is that color is not absolute. RGB is based on the sensitivity
of the three color sensing elements to reﬂected light. An object may appear
to be at different values at different distances due to the intensity of the re-
ﬂected light. Fig. 6.12 shows the same program and parameters to segment
an orange landmark that is serving as a “ﬂag” for tracking a small robot just
outside of the image. The RGB segmentation in Fig. 6.12a is more correct than
in Fig. 6.12b. The only difference is that the ﬂagged robot has moved, thereby
changing the incidence of the light. This degradation in segmentation qual-
ity is called visual erosion, because the object appears to erode with changes
VISUAL EROSION
in lighting. Moreover, CCD devices are notoriously insensitive to red. This
means that one of the three color planes is not as helpful in distinguishing
colors.
Clearly a device which was sensitive to the absolute wavelength of the
reﬂected light (the hue) would be more advantageous than having to work

6.6
Computer Vision
223
a.
b.
Figure 6.12
Images showing visual erosion of an orange landmark sticking up from
a small robot (not visible): a.) Original image and RGB segmentation and b.) original
image and degradation in RGB segmentation as robot moves farther from camera.
around the limitations of RGB. Such a device would work on the HSI (hue,
HSI
saturation, intensity) representation of color. The hue is the dominant wave-
HUE
length and does not change with the robot’s relative position or the object’s
shape. Saturation is the lack of whiteness in the color; red is saturated, pink is
SATURATION
less saturated. The value or intensity measure is the quantity of light received
VALUE
INTENSITY
by the sensor. So HSV is a very different color scheme than RGB.

224
6
Common Sensing Techniques for Reactive Robots
saturation:0-1
(decreasing whiteness)
intensity: 0-1
(increasing 
signal strength)
hue: 0-360
(wavelength)
red 0
cyan 
orange
green
magenta
white
pastel
"bright"
saturation
Figure 6.13
HSV space representation.
HSV is a three-dimensional space in that it has three variables, but it is def-
initely not a cube representation, more of a cone as seen in Fig. 6.13. The hue,
or color, is measured in degrees from 0 to 360. Saturation and intensity are
real numbers between 0 and 1. These are generally scaled to 8-bit numbers.
Accordingly, red is both 0 and 255, orange is 17, green is at 85, blue is 170,
with magenta at 200.
HSV space is challenging for roboticists for many reasons. First, it re-
quires special cameras and framegrabbers to directly measure color in HSV
space. This equipment is prohibitively expensive. Second, there is a software
conversion from the RGB space measured by consumer electronics, but it is
computationally expensive and has singularities (values of RGB where the
conversion fails). These singularities occur at places where the three colors
for a pixel are the same; the ﬂatness of the red color plane in CCD cameras
increases the likelihood that a singularity will occur.
An alternative color space that is currently being explored for robotics is
the Spherical Coordinate Transform (SCT).140 That color space was designed

6.6
Computer Vision
225
22.5°
67.5°
45°
0°
0°
0°
22.5°
45°
67.5°
90°
90°
L
Angle A
Angle B
Figure 6.14
Spherical coordinate transform.
a.
b.
c.
d.
Figure 6.15
A comparison of color spaces: a.) a landmark in the original image, b.)
RGB segementation, c.) HSI segmentation, and d.) SCT segmentation.

226
6
Common Sensing Techniques for Reactive Robots
to transform RGB data to a color space that more closely duplicates the re-
sponse of the human eye. It is used in biomedical imaging, but has not been
widely considered for robotics. The shape of the color space is triangular,
as shown in Fig. 6.14. Initial results indicate it is much more insensitive to
changes in lighting.74 Fig. 6.15 shows an image and the results of segmenting
a color in RGB, HSI, and SCT spaces.
6.6.3
Region segmentation
The most ubiquitous use of computer vision in reactive robotics is to identify
a region in the image with a particular color, a process called region segmen-
REGION
SEGMENTATION
tation. Region segmentation and color affordances are a staple perceptual
algorithm for successful entries to many different international robot com-
petitions, including the AAAI Mobile Robot Competition, RoboCup, and
MIROSOT. There are many color region segmentation algorithms available
and Newton Labs sells a Cognachrome board dedicated to the rapid extrac-
tion of colored regions. The basic concept is to identify all the pixels in an
image which are part of the region and then navigate to the region’s center
(centroid). The ﬁrst step is to threshold all pixels which share the same color
(thresholding), then group those together and throw out any pixels which
don’t seem to be in the same area as the majority of the pixels (region grow-
ing).
Ch. 5 described a robot which used red to signify a red Coca-Cola can for
recycling. Ideally, the robot during the searching for the can behavior would
see the world as a binary image (having only 2 values) consisting of
red,
BINARY IMAGE
not-red. This partitioning of the world can be achieved by thresholding the
THRESHOLDING
image and creating a binary image. A C/C++ code example is shown below:
for (i= = 0; i < numberRows; i++)
for (j= = 0; j < numberColumns; j++) {
if ((ImageIn[i][j][RED] == redValue)
&& (ImageIn[i][j][GREEN] == greenValue)
&& (ImageIn[i][I][BLUE] == blueValue)) {
ImageOut[i][j] = 255;
}
else {
ImageOut[i][j] = 0;
}
}

6.6
Computer Vision
227
Note that the output of a thresholded color image is a two-dimensional
array, since there is no need to have more than one value attached at each
pixel. Also, in theory a binary image would permit only values of 0 and
1. However, on many compilers there is no particular beneﬁt to doing a bit
level representation, and it can complicate code reuse. Also, most display
software is used to displaying at least 256 values. The difference between 1
and 0 is not detectable by the human eye. Therefore, it is more common to
replace the 1 with a 255 and use a full byte per pixel.
Thresholding works better in theory than in practice due to the lack of
color constancy. The shape of an object will mean that even though a human
sees the object as a solid color, the computer will see it as a set of similar
colors. The common solution is to specify a range of high and low values on
each color plane. The C/C++ code would now become:
for (i= = 0; i< numberRows; i++)
for (j= = 0; j<numberColumns; j++) {
if (((ImageIn[i][j][RED] >= redValueLow)
&& (ImageIn[i][j][RED] <= redValueHigh))
&&((ImageIn[i][j][GREEN]>=greenValueLow)
&&(ImageIn[i][j][GREEN] <= greenValueHigh))
&&((ImageIn[i][j][BLUDE]>=blueValueLow)
&&(ImageIn[i][j][BLUE] <= blueValueHigh))) {
ImageOut[i][j] = 255;
}
else {
ImageOut[i][j] = 0;
}
}
The change in viewpoints and lighting means that the range which de-
ﬁnes the object from the robot’s current position is likely to change. The
color range for the object has to be made even wider to include the set of
color values the object will take. If the object color is unique for that envi-
ronment, this increase in the color range is acceptable. Otherwise, if there
are objects which have a color close enough to the object of interest, those
objects may be mistaken for the target. In some circles, the object of interest
is called the foreground, while everything else in the image is called the back-
FOREGROUND
BACKGROUND
ground. Thresholding an image requires a signiﬁcant contrast between the
background and foreground to be successful.

228
6
Common Sensing Techniques for Reactive Robots
a.
b.
Figure 6.16
Segmentation of a red Coca Cola can: a.) original image and b.) result-
ing red regions. Note that some non-object pixels showed a reddish color.
Fig. 6.16 is the output of a threshold on color alone. If a robot was to move
to the “red” in the image, how would it know which pixel to follow? The
perceptual schema could be instantiated for each red pixel; this is simple,
but it would waste a lot of execution cycles. The perceptual schema could
take the weighted centroid of all the red pixels. In this case, it would be
somewhat about where most people would say the center of the can was.
Or, the perceptual schema could attempt to ﬁnd the largest region where red
pixels were adjacent to each other, then take the centroid of that region. (The
region is often referred to as a “blob,” and the extraction process is known as
blob analysis.)
Color regions can also be helpful in cluttered environments.
Fig. 6.17
shows a Denning mobile robot simulating a search of a collapsed building.
The international orange vest of the workman provides an important cue.
The robot can signal a teleoperator when it sees bright colors.
6.6.4
Color histogramming
Thresholding works well for objects which consist of one color or one dom-
inant color. A different technique popularized by Mike Swain, called color
COLOR
HISTOGRAMMING
histogramming, can be used to identify a region with several colors.137 Essen-
tially, color histogramming is a way to match the proportion of colors in a
region.

6.6
Computer Vision
229
a.
b.
c.
Figure 6.17
An urban search and rescue scene. a.) a Denning mobile robot search-
ing, b.) image from the camera, and c.) segmentation for orange.
A color histogram is a type of histogram. A histogram is a bar chart of
data. The user speciﬁes the range of values for each bar, called buckets. The
size of the bar is the number of data points whose value falls into the range
for that bucket. For example, a histogram for a grayscale image might have
8 buckets (0-31, 32-63, 64-95, 96-127, 128-159, 160-191, 192-223, 224-251) and
each bucket contains the number of pixels in the image that fell into that
range. Constructing a color histogram is straightforward for a region in hue
space, as shown in Fig. 6.18.
A color histogram in RGB or any other distributed color space is a bit
harder to visualize. The grayscale and hue image histograms had only one
axis for buckets, because these images have only one plane that matters. But
a color image has 3 planes in the RGB coordinate system. As a result, it has
buckets for each color plane or axis. Assuming that each plane is divided

230
6
Common Sensing Techniques for Reactive Robots
red
green
blue
white
a.
b.
Figure 6.18
a.) A histogram for b.) the image of the children’s toy, Barney.
into 8 buckets, the ﬁrst bucket would be the number of pixels which fell into
the range of (R, G, B) of (0-31, 0-31, 0-31).
The real advantage of a color histogram for reactive robots is that color
histograms can be subtracted from each other to determine if the current
image (or some portion),
I, matches a previously constructed histogram,
E.
The histograms are subtracted bucket by bucket (j buckets total), and the
difference indicates the number of pixels that didn’t match. The number
of mismatched pixels divided by the number of pixels in the image gives a
percentage match. This is called the histogram intersection:
inter
section
=
P
n
j
=
min(I
j
 E
j
)
P
n
j
=
E
j
(6.1)
For example, a robot can “wake up” and imprint the object in front of it by
constructing the color histogram. Then a perceptual schema for a releaser or
behavior can compute the color histogram intersection of the current image
with the imprint. The robot can use the color histogram to determine if a
particular object is of interest or not.

6.7
Range from Vision
231
Because the color histogram of a current image can be matched with an-
other image, the technique appears to be model-based, or recognition. But
reactive systems do not permit recognition types of perception. Is this a con-
tradiction? No; a color histogram is an example of a local, behavior-speciﬁc
representation which can be directly extracted from the environment. For
example, a robot could be shown a Barney doll with its distinct purple color
with green belly as the percept for the goal for a move-to-goal behavior.
However, the robot will follow a purple triangle with a green region, be-
cause the ratio of colors is the same. There is no memory and no inference,
just a more complex stimulus.
Note that the intersection can be considered to be a measure of the strength
of the stimulus, which is helpful in reactive robotics. In one set of experi-
ments, a robot was presented a poster of Sylvester and Tweety. It learned the
histogram, then after learning the object (e.g., ﬁxating on it), it would begin to
move towards it, playing a game of tag as a person moved the poster around.
The robot used a simple attractive potential ﬁelds-based move-to-goal be-
havior, where the perceptual schema provided the location of the poster and
the percent intersection. The motor schema used the location to compute the
direction to the poster, but the intersection inﬂuenced the magnitude of the
output vector. If the person moved the poster into a dark area or turned it at
an angle, the intersection would be low and the robot would move slower.
If the match was strong, the robot would speed up. Overall, it produced a
very dog-like behavior where the robot appeared to play tag quickly (and
happily) until the human made it too difﬁcult. Then if the human moved the
poster back to a more favorable position, the robot would resume playing
with no hard feelings.
6.7
Range from Vision
An important topic in computer vision is how to extract range information.
Humans extract depth from vision. In most cases, though not always, depth
perception is due to having two eyes and being able to triangulate as shown
in Fig. 6.20, also known as stereopsis. Other times, perception of depth is
STEREOPSIS
related to optic ﬂow and simple cues such as shadows, texture, and expected
OPTIC FLOW
size of objects. This section covers three types of vision sensors which are
commonly used to create an image representing a depth map: stereo camera
pairs, light stripers, and laser ranger ﬁnders.

232
6
Common Sensing Techniques for Reactive Robots
Figure 6.19
A Denning mobile robot using a color histogram to play tag with a
poster of Sylvester and Tweety.
6.7.1
Stereo camera pairs
Using two cameras to extract range data is often referred to as range from
stereo, stereo disparity, binocular vision, or just plain “stereo.” One way to ex-
tract depth is to try to superimpose a camera over each eye as in Fig. 6.20a.
Each camera ﬁnds the same point in each image, turns itself to center that
point in the image, then measures the relative angle. The cameras are known
as the stereo pair.
STEREO PAIR
This method has two challenges. The ﬁrst is that it is hard to design and
build a mechanism which can precisely move to verge on the points. (It is
even harder to design and build an inexpensive vergence mechanism.) The
second challenge is even more fundamental: how does the robot know that it
is looking at the same point in both images? This problem is referred to as the
correspondence problem, since the task is to ﬁnd a point in one image that cor-
CORRESPONDENCE
responds to a point in the other image. A common approach is to identify
“interesting” or potentially uniquely valued pixels in the image, such as very
bright or dark spots or edges. The algorithm that selects interesting pixels is
called an interest operator. Since even minute changes in lighting make a dif-
INTEREST OPERATOR
ference in the image, there is no guarantee that the two images, even though
acquired at precisely the same time from two cameras, will “see” the same
values for corresponding pixels. Therefore, interest operator algorithms usu-
ally return a list of interesting pixels, not just one, and a matching algorithm
tries to ﬁnd the best correspondence between all of them. After the interest

6.7
Range from Vision
233
θ1
θ2
a.
baseline
θ1
θ2
b.
Figure 6.20
Ways of extracting depth from a pair of cameras: a.) vergence of the
cameras to determine the depth of a point, and b.) a set of rectiﬁed stereo images.
points are established, the rest of the pixels have to be labeled with a depth
relative to those points.
Fortunately, it is not necessary to have a mechanical vergence system. In-
stead, cameras can be mounted in place with the optic axes parallel to each
other and perpendicular to the mount, producing rectiﬁed images.41 This type
RECTIFIED IMAGES
of traditional stereo “head” is shown in Fig. 6.21. The space between the axes
of two cameras is called the baseline. The distance in the location of the point
of interest between the images is called the disparity; the distance of the point
DISPARITY
from the cameras is inversely proportional to disparity.69 Fig. 6.20b shows
the geometry behind a stereo pair.
While rectiﬁed images eliminate the need for expensive mechanical ver-
gence systems, they don’t solve the correspondence problem. If the cameras
are aligned precisely, then one row in the left image will correspond to a row
in the right image. These rows are said to be epipolar lines or projections of
EPIPOLAR LINES
a epipolar plane. Whenever the robot ﬁnds an interesting point in one image,
EPIPOLAR PLANE
it only has to consider the pixels along the epipolar line in the other image.
This is a tremendous computational savings. However, it only works if the

234
6
Common Sensing Techniques for Reactive Robots
Figure 6.21
A stereo camera pair mounted on a pan/tilt head.
cameras are perfectly matched optically and remain in alignment. In prac-
tice, robots move, bump, and suffer alignment drifts, plus the cameras may
have some ﬂaws in their optics. The alignment can be periodically compen-
sated for in software through a camera calibration process, where the robot
CAMERA CALIBRATION
is presented with a standard and then creates a calibration look up table or
function. Fig. 6.22 shows the CMU Uranus robot calibrating its camera sys-
tem. As a result, many researchers are turning to units which package a
stereo pair in one ﬁxed case, where the alignment cannot be altered. Fig. 6.25
shows the results using a stereo range system using three cameras in a ﬁxed
conﬁguration.
The ﬁrst robot to use stereo vision successfully was Hans Moravec’s Stan-
ford Cart shown in Fig. 6.23a Moravec worked on the Cart while at graduate
school at Stanford between 1973 and 1980. Fig. 6.23b shows the Marsokhod
rover developed in the late 1990’s which used a stereo pair for real-time nav-
igation. Longer baselines tend to be more accurate because a slight mea-
surement error has a smaller impact, but smaller baselines have a smaller
“footprint,” in effect, take up less room. The same point in both images still
has to be identiﬁed.
Fig. 6.24 shows the simpliﬁed ﬂow of operations in extracting range from
a pair of images. The process begins with two images, the left-right pair, and
results in a third image called the range image or the depth map. The left-right
RANGE IMAGE
DEPTH MAP
pair can be grayscale or color, but the depth map is a grayscale map, where
intensity is proportional to the distance the pixel is away from the cameras.
Fig. 6.25 shows two stereo images and the resulting depth map.

6.7
Range from Vision
235
Figure 6.22
Uranus undergoing calibration.
(Photograph courtesy of Hans
Moravec.)
The major drawback with extracting range information from vision is that
the algorithms tend to be computationally complex. Stereo matching algo-
rithms are typically on the order of
O
(n

m

). This means that to process an
image of size 640 by 480 takes on the order of
	

0
0 instructions, while
to simply segment a color is on the order of
O
(nm) or


0
 instructions.
Even with advances in microprocessors, a stereo range map can take minutes
to compute.
6.7.2
Light stripers
Light striping, light stripers or structured light detectors work by projecting a
colored line (or stripe), grid, or pattern of dots on the environment. Then a
regular vision camera observes how the pattern is distorted in the image. For
example, in Fig. 6.26a, the striper projects four lines. The lines should occur
at speciﬁc, evenly spaced rows in the camera image if the surface is ﬂat. If
the surface is not ﬂat, as shown in Fig. 6.26b, the lines will have breaks or

236
6
Common Sensing Techniques for Reactive Robots
a.
b.
Figure 6.23
Robots and stereo: a.) The Stanford Cart developed in the late 1970’s.
(Photograph courtesy of Hans Moravec.) b.)The Marsokhod rover developed in the
late 1990’s jointly by scientists from McDonnell Douglas, Russia, and NASA Ames
Research Center. (Image courtesy of the National Aeronautics and Space Adminis-
tration.)
discontinuities. A vision algorithm can quickly scan each of the designated
rows to see if the projected line is continuous or not. The location of the
breaks in the line give information about the size of the obstacle. The vision
algorithm can also look for where the dislocated line segments appears, since
the distance in image coordinates is proportional to the depth of the object.
The relative placement of the lines indicates whether the object is above the
ground plane (an obstacle) or below (a hole or negative obstacle). The more
NEGATIVE OBSTACLE
lines or ﬁner-grained grid, the more depth information.
Light stripers are less expensive for many reasons. First, since they are pro-
ducing a line or pattern to be measured, expensive time-of-ﬂight detectors
are unnecessary. The detection is done by the camera. Second, producing a
thick line that can be detected by a camera does not require a laser. Instead it
can be done with structured light, an optical method which allows “regular”
light to mimic desirable properties of laser light. Finally, light stripers pro-

6.7
Range from Vision
237
Find interest points
in one image
Matching
Depth map
epipolar plane
scene
Figure 6.24
A simpliﬁed ﬂow of operations in extracting range from a stereo pair.
Figure 6.25
A set of stereo images from a Triclops stereo camera and resulting depth
map. (Images courtesy of Chandra Kambhamettu.)

238
6
Common Sensing Techniques for Reactive Robots
a.
b.
c.
Figure 6.26
Situations and resulting images of a.) ﬂat surface, b.) an obstacle, and
c.) a negative obstacle.
duce a fairly coarse pattern; they don’t project a line or grid onto every pixel
in the image. This means the device is overall less demanding, and therefore
less expensive to build.
Light stripers were popular in the late 1980’s and early 1990’s, and labora-
tories such as the University of Pennsylvania’s GRASP Lab under the direc-
tion of Ruzena Bajcsy produced excellent results in extracting 3D informa-
tion. However, these efforts focused on using the depth map to recognize an
object under laboratory conditions. The results did not transfer particularly
well to mobile robots. Reactive robots are not concerned with recognition,
so many of the algorithms were not particularly useful or provided a quick,
reﬂexive response. Also, in the open world, objects were often the same color
as the projected light or near enough to it to confuse the vision system. The
amount and direction of lighting could also confuse the striper, with brightly
lit rooms making it difﬁcult to see the bright laser or structured light.

6.7
Range from Vision
239
a.
b.
Figure 6.27
Lidar images from an Odetics LADAR camera: a.) range and b.) in-
tensity. (Images courtesy of University of South Florida and Oak Ridge National
Laboratory.)
The Sojourner Mars rover used a light striping system for obstacle avoid-
ance. The system projected ﬁve lines ahead of the vehicle.134 This worked
well because Mars has little color or light diversity. Interestingly enough, the
light striper used one member of a stereo pair, but unlike the Marsokhod,
Sojourner did not use stereo for navigation. Instead, the robot would period-
ically take a stereo pair for reconstruction and map-making on Earth.
6.7.3
Laser ranging
Ultrasonics introduces acoustic energy into the environment and measures
the time of ﬂight of the signal to return. The same principle can be used with
lasers; a laser beam can emitted and then the reﬂectance measured. Unlike a
sonar beam which has a very wide ﬁeld of view (almost
0
), a laser produces
an almost inﬁnitesimal ﬁeld of view. If the laser is directed in a scan, just like
a raster scan on a CRT, the device can cover a reasonable area and produce
an image, where the image function produces depth values. Devices which
use lasers to produce a depth map or image are often called laser radar, ladar,
LASER RADAR
LADAR
or lidar. They can generate up to one million range pixels per second, 52 with
LIDAR
a range of 30 meters and an accuracy of a few millimeters. The mechanical

240
6
Common Sensing Techniques for Reactive Robots
Figure 6.28
Sick laser, covering a
0
 area.
scanning component makes lidars very expensive, on the order of $30,000 to
$100,000 USD. A less expensive solution for navigation is to create a planar
laser range ﬁnder.
A lidar produces two images: intensity and range. Fig. 6.27 shows the
images produced by a Odetics laser range (LADAR) camera. The intensity
map is essentially a black and white photograph and measures the intensity
of the light reﬂected or absorbed by objects in the scene. This corresponds to
how humans perceive the scene The image function for the range image is
depth from the camera. Pixels that are black, or have a value of 0, are closer
than white pixels. A ﬂat ﬂoor usually appears as a radiating set of semi-
circles going from near to far; trigonometry is then used to compute that the
circles represent a ﬂat surface. This process is called range segmentation and
RANGE SEGMENTATION
can be quite difﬁcult.
Lidars have some problems in practice. For example, Fig. 6.27 shows an
area on the range image that is pure black or very near. But as can be seen
from the intensity image the area is actually far away. Likewise, the black
moulding between the wall and ﬂoor appear to be very far away on the range
image. The errors were due to out of range conditions, absorption of the light
(not enough light returned), or to the optical equivalent of specular reﬂection
(light hitting corners gets reﬂected away from the receiver).
A planar laser range ﬁnder, such as the Sick shown in Fig. 6.28, provides a
narrow horizontal range map. The map is essentially a high resolution polar

6.7
Range from Vision
241
plot. Robots such as Nomads and Pioneers originally came with Sick lasers
mounted in parallel to the ﬂoor. This was useful for obstacle avoidance (as
long as the obstacle was tall enough to break the laser plane), but not partic-
ularly helpful for extracting 3D information. Also, as with sonars, robots ran
the risk of being decapitated by obstacles such as tables which did not appear
in the ﬁeld of view of the range sensor but could hit a sensor pod or antenna.
To combat this problem, researchers have recently begun mounting planar
laser range ﬁnders at a slight angle upward. As the robot moves forward,
it gets a different view of upcoming obstacles. In some cases, researchers
have mounted two laser rangers, one tilted slightly up and the other slightly
down, to provide coverage of overhanging obstacles and negative obstacles.
6.7.4
Texture
The variety of sensors and algorithms available to roboticists can actually
distract a designer from the task of designing an elegant sensor suite. In
most cases, reactive robots use range for navigation; robots need a sensor to
keep it from hitting things. Ian Horswill designed the software and camera
system of Polly, shown in Fig. 6.29, speciﬁcally to explore vision and the
relationship to the environment using subsumption.70 Horswill’s approach
is called lightweight vision, to distinguish its ecological ﬂavor from traditional
LIGHTWEIGHT VISION
model-based methods.
Polly served as an autonomous tour-guide at the MIT AI Laboratory and
Brown University during the early 1990’s. At that time vision processing was
slow and expensive, which was totally at odds with the high update rates
needed for navigation by a reactive mobile robot. The percept for the obstacle
avoidance behavior was based on a clever affordance: texture. The halls of
the AI Lab were covered throughout with the same carpet. The “color” of the
carpet in the image tended to change due to lighting, but the overall texture
or “grain” did not. In this case, texture was measured as edges per unit area,
as seen with the ﬁne positioning discussed in Ch. 3.
The robot divided the ﬁeld of view into angles or sectors, creating a radial
RADIAL DEPTH MAP
depth map, or the equivalent of a polar plot. Every sector with the texture
of the carpet was marked empty. If a person was standing on the carpet,
that patch would have a different texture and the robot would mark the area
as occupied. Although this methodology had some problems—for exam-
ple, strong shadows on the ﬂoor created “occupied” areas—it was fast and
elegant.

242
6
Common Sensing Techniques for Reactive Robots
Figure 6.29
Polly, a visually guided reactive robot using a black and white camera.
(Photograph courtesy of Ian Horswill and AAAI.)
6.8
Case Study: Hors d’Oeuvres, Anyone?
The University of South Florida’s (USF) entry in the 1999 AAAI Mobile Ro-
bot Competition Hors d’Oeuvres, Anyone? event provides a case study of
selecting sensors, constructing reactive behaviors, and using behavioral sen-
sor fusion. The entry used two cooperative robots. The goal was to push the
envelope in robotic sensing by using six different sensing modalities with
40 different physical devices on one robot and four modalities with 23 de-
vices on the other. Although the robots executed under a hybrid delibera-
tive/reactive style of architecture (covered later in Ch. 7), the basic design
process followed the steps in Ch. 5, and produced a largely behavioral sys-
tem. The entry won a Technical Achievement award of Best Conceptual and
Artistic Design after a series of hardware and software timing failures pre-
vented it from working at the competition.

6.8
Case Study: Hors d’Oeuvres, Anyone?
243
a.
b.
c.
d.
Figure 6.30
USF robots in the “Hors d’Oeuvres, Anyone?” event. a.) Family por-
trait, where Borg Shark is on the left, Puffer Fish on the right, with b.) the thermal
sensor located as the Borg Shark’s “third eye,” c.) the SICK laser located behind the
Borg Shark’s teeth (head piece is removed for better view), and d.) a proﬁle of Puffer
Fish’s skirt showing spatial relationship to sonar.

244
6
Common Sensing Techniques for Reactive Robots
Step 1: Describe the task. The “Hors d’Oeuvres, Anyone?” event required
fully autonomous robots to circulate in the reception area at the AAAI confer-
ence with a tray of ﬁnger food, ﬁnd and approach people, interact with them,
and reﬁll the serving tray. Each robot was scored on covering the area, notic-
ing when the tray needed reﬁlling, interacting with people naturally, having
a distinct personality, and recognizing VIPs. The USF entry used two robots,
shown in Fig. 6.30, costumed by the USF Art Department in order to attract
attention. The Borg Shark was the server robot, and navigated through au-
dience following a pre-planned route. It would stop and serve at regular
intervals or whenever a treat was removed from the tray. It used a DEC-talk
synthesizer to broadcast audio ﬁles inviting audience members to remove a
treat from its mouth, but it had no way of hearing and understanding nat-
ural language human commands. In order to interact more naturally with
people, the Borg Shark attempted to maintain eye contact with people. If it
saw a person, it estimated the location in image coordinates of where a VIP’s
colored badge might be, given the location of the face.
When the Borg Shark was almost out of food, she would call over radio
ethernet her assistant robot, Puffer Fish. Puffer Fish would be stationary in
sleep mode, inhaling and exhaling through her inﬂatable skirt and turning
her cameras as if avoiding people crowding her. When Puffer Fish awoke,
she would head with a full tray of food (placed on her stand by a human)
to the coordinates given to her by Borg Shark.
She would also look for
Borg Shark’s distinctive blue costume, using both dead reckoning and vi-
sual search to move to goal. Once within 2 meters of Borg Shark, Puffer Fish
would stop. A human would physically swap trays, then kick the bumpers
to signal that the transfer was over. Borg Shark would resume its serving
cycle, while Puffer Fish would return to its home reﬁll station.
Both robots were expected to avoid all obstacles: tables, chairs, people.
Since there was a tendency for people to surround the robotos, preventing
coverage of the area or reﬁlling, the robots had different responses. Borg
Shark, who was programmed to be smarmy, would announce that it was
coming through and begin moving. Puffer Fish, with a grumpy, sullen per-
sonality, would vocally complain, then loudly inﬂate her skirt and make a
rapid jerk forward, usually causing spectators to back up and give her room.
Step 2: Describe the robots. The robots used for the entry were Nomad 200
bases, each with a unique sensor suite.

6.8
Case Study: Hors d’Oeuvres, Anyone?
245
The original sensors on Borg Shark included a pair of color cameras mounted
on a pan-tilt head, redundant sonar rings, and a SICK planar laser:
Type
Modality
Devices
exteroceptive
vision
2 cameras
exteroceptive
laser
1 planar ranger
exteroceptive
sonar
15 ultrasonics (upper)
15 ultrasonics (lower)
exteroceptive
tactile
1 bumper switch
proprioceptive
motor encoders
3 drive, steer, turret control
2 pan, tilt control
The sensors on Puffer Fish were:
Type
Modality
Devices
exteroceptive
vision
2 cameras
exteroceptive
sonar
15 ultrasonics
exteroceptive
tactile
1 bumper switch
proprioceptive
motor encoders
3 drive, steer, turret control
2 pan, tilt control
Step 3: Describe the environment. The environment was a convention center
arena, with variable lighting and many people causally dressed. The major
perceptual challenge was to maintain eye contact with people and to deter-
mine if a person is a VIP. The VIPs’ badges were marked with distinctively
colored ribbons, so if the robot was sure it was looking in the right area, the
color would afford a VIP. However, the colors of the ribbon were fairly com-
mon. If the robot scanned at about chest height for ribbons, it would likely
ﬁnd a shirt of that color in the crowd who would be wear wearing bright
tourist wear. This would cause it to mis-identify a VIP, losing points. An-
other approach was to use sonars to ﬁnd a person: read a range, point the
camera in that direction and tilt to the angle where the eyes would be for a
person of average height. This focus-of-attention mechanism could be used
to look in the likely location of a badge. However, it was known from expe-
rience that people rarely stood along the acoustic axis of a sonar. If a single
person was picked up by two sonars, the robot would look for 10 seconds
to the left of the person and then 10 seconds to the right. If multiple peo-
ple were present, the robot would seem even more dysfunctional, and the
focus-of-attention mechanism would not work correctly.

246
6
Common Sensing Techniques for Reactive Robots
A better solution would be to detect a person using vision. Notice that
detection is not the same thing as recognition. Detection means that the robot
is able to identify a face, which is reactive. Recognition would label the face
and be able to recognize it at a later time, which is a deliberative function, not
reactive. Is there a simple visual affordance of a face? Actually, to a vision
system human skin is remarkably similar in color, regardless of ethnicity.
Once the robot had found a colored region about the size and shape of a
head, it could then more reliably ﬁnd the VIP badges.
The other opportunity for an affordance was Puffer Fish’s navigation to
Borg Shark. Although Puffer Fish would receive Borg Shark’s coordinates,
it was unlikely that Puffer Fish could reliably navigate to Borg Shark using
only dead reckoning. The coordinates were likely to be incorrect from Borg
Shark’s own drift over time. Then Puffer Fish would accumulate dead reck-
oning error, more so if it had to stop and start and avoid people. Therefore,
it was decided that Puffer Fish should look for Borg Shark. Borg Shark’s
head piece was deliberately made large and a distinctive blue color to afford
visibility over the crowd and reduce the likelihood of ﬁxating on someone’s
shirt.
Step 4-7: Design, test, and reﬁne behaviors. The choice of sensors for other
behaviors, such as treat removal, was inﬂuenced by the physical location of
the sensors. For example, the SICK laser for Borg Shark came mounted on the
research platform as shown in Fig. 6.30b. The research platform, nominally
the top of the robot, was at hand height, making it a logical place to attach a
tray for holding food. It was obvious that the laser could be used to monitor
the food tray area. Other teams tried various approaches such as having a
colored tray and counting the amount of that color visible (more color means
fewer treats on the tray, covering the color). Another approach was to build
a scale and monitor the change in weight.
An interesting aspect of the robots that impacted the sensor suite indi-
rectly were the costumes. As part of giving the robots personality, each robot
had a costume. The Puffer Fish had an inﬂatable skirt that puffed out when
the robot was crowded or annoyed. The team had to empirically test and
modify the skirt to make sure it would not interfere with the sonar readings.
Fig. 6.30c shows the proﬁle of the skirt.
As seen in the behavior table below, the only behavior using any form of
sensor fusion was move-to-goal in Puffer Fish, which had two competing
instances of the goal making it sensor ﬁssion.

6.8
Case Study: Hors d’Oeuvres, Anyone?
247
The initial behaviors for Borg Shark are given in the behavior table below:
Releaser
Behavior
Motor Schema
Percept
Perceptual Schema
always on
avoid()
vfh()
most-open-direction
polar-plot(sonar)
FOOD-REMOVED=
track-face
center-face(face-centroid)
face-centroid
ﬁnd-face(vision)
treat-removal(laser)
track-face()
check-VIP()
ribbon-color
look-for-ribbon(VIP-color)
SERVING-TIME-OUT,
move-to-goal
pﬁelds.attraction(waypoint)
waypoint
list of waypoints
TRAY-FULL=bumper()
FOOD-DEPLETED=
track-face
center-face(face-centroid)
face-centroid
ﬁnd-face(vision)
treat-removal(laser)
The initial behavior table for Puffer Fish was:
Releaser
Behavior
Motor Schema
Percept
Perceptual Schema
always on
avoid()
vfh()
most-open-direction
polar-plot(sonar)
AT-HOME=
sleep()
turn-camera-head()
obstacle
polar-plot(sonar)
dead-reckoning(encoders)
cycle-skirt()
AWAKE=radio-signal()
move-to-goal()
pﬁelds.attraction(location)
relative-location
read-encoders()
AWAKE=radio-signal()
move-to-goal()
pﬁelds.attraction(shark)
shark
ﬁnd-shark-blue(camera)
TRAY-FULL=bumper()
move-to-goal()
pﬁelds.attraction(home)
relative-location
read-encoders()
The vfh behavior is an obstacle avoidance behavior using polar plots de-
rived from models described in Ch. 11. As the team tested the behaviors
individually, the find-face and treat-removal behaviors proved to be
unreliable. While color was a reasonable affordance for a face, the algorithm
often returned false negatives, missing faces unless in bright light. Mean-
while the laser appeared to occasionally get a reﬂection from the teeth, also
generating false positives, and more than 75% of the time it would miss a
person’s hand if the motion was quick. The rates were:
logical sensor
False Positives
False Negatives
Face-Find
1.7%
27.5%
Food-Count
6.7%
76.7%
The solution to the find-face performance was to exploit another affor-
dance of a human, one used by mosquitoes: heat. The problem was partial
segmentation; candidate regions were getting rejected on being too small.
Heat would make a good the decision criteria. If a candidate region was co-
located with a hot region, then it was declared a face. Fortunately, the team
was able to transfer an
E

T digital thermometer used on another robot to
Borg Shark. The thermal sensor shown in Fig. 6.30 was intended for deter-

248
6
Common Sensing Techniques for Reactive Robots
map:
evidence grid
awaiting
refill:
finding faces
serving food:
finding faces,
counting treat
removal
waypoint
navigation:
move to goal, 
avoid
food 
depleted
full 
tray
at waypoint
OR 
food removed
time limit
serving exceeded
sonars
shaft
encoders
thermal
vision
laser range
sonars
thermal
vision
Figure 6.31
State diagrams for the Borg Shark, annotated to show sensors being
used.
mining the temperature of a person on contact, but it was able to detect an
increase in temperature above the ambient from a person up to two meters
away.
Why wasn’t the vision system simply replaced with the thermal sensor?
This gets back to the attributes of sensors and how they ﬁt with the environ-
ment. The thermal sensor has a

 ﬁeld of view, making it too slow to scan.
Instead, the vision system covered a much wider ﬁeld of view and could gen-
erate a small list of candidate regions. Then as the camera turned to center
on the largest region, the thermal probe could decide whether this was really
a person or not.
The problem with the food-count was greatly reduced by a simple AND
function with the sonars. The system counted a treat as being removed only
if there was a close range reading in front of the tray at the same time.

6.8
Case Study: Hors d’Oeuvres, Anyone?
249
The false reading rate dropped considerably as seen below:
Logical sensor
W/O Fusion
Fusion
FP
FN
FP
FN
Face-Find
1.7%
27.5%
2.5%
0%
Food-Count
6.7%
76.7%
6.7%
1.7%
At this point it is helpful to step back and examine the sensing for the
Hors d’Oeuvres, Anyone? entry in terms of the attributes listed in Sec. 6.3.
Recall that the attributes for evaluating the suitability of an individual sen-
sor were ﬁeld of view, range, accuracy, repeatability, resolution, responsiveness in
target domain, power consumption, reliability, and size. The ﬁeld of view and
range of the sensors was an issue, as seen by the differences in vision and
thermal sensors for the face-ﬁnding behavior. The camera had a much better
ﬁeld of view than the thermal sensor, so it was used to focus the attention of
the heat sensor. Repeatability was clearly a problem for laser with its high
false positive/false negative rate. The sonars could not be used for estimat-
ing the location of a face because the resolution was too coarse. Each of the
sensors had reasonable responsiveness from a hardware perspective, though
the algorithms may not have been able to take advantage of them. Power
consumption was not an issue because all sensors were on all the time due
to the way the robots were built. Reliability and size of the hardware were
not serious considerations since the hardware was already on the robots.
The algorithmic inﬂuences on the sensor design were computational com-
plexity and reliabilty. Both were deﬁnitely a factor in the design of the per-
ceptual schemas for the reactive behaviors. The robots had the hardware
to support stereo range (two cameras with dedicated framegrabbers). This
could have been used to ﬁnd faces, but given the high computational com-
plexity, even a Pentium class processor could not process the algorithm in
real-time. Reliability was also an issue. The vision face ﬁnding algorithm
was very unreliable, not because of the camera but because the algorithm
was not well-suited for the environment and picked out extraneous blobs.
Finally, the sensing suite overall can be rated in terms of simplicity, modu-
larity, and redundancy. The sensor suite for both Nomad robots can be con-
sidered simple and modular in that it consisted of several separate sensors,
mostly commercially available, able to operate independently of each other.
The sensor suite did exhibit a high degree of physical redundancy: one ro-
bot had dual sonar rings, and the sonars, laser, and camera pair could have

250
6
Common Sensing Techniques for Reactive Robots
been used for range, ignoring the placement of the shark teeth and the com-
putational complexity. There was also a large amount of logical redundancy,
which was exploited through the use of behavioral sensor fusion.
6.9
Summary
The success of a reactive system depends on the suitability of the robot’s sen-
sor suite. It is often more useful to think of sensing in terms of perceptual
schemas or logical sensors needed to accomplish a task, rather than focus on
the characteristics of a particular transducer or modality. Reactive percep-
tion may appear limited since it uses only behavior-speciﬁc representations
and does not involve recognition. However, it supports a diversity of forms
of perception, including behavioral sensor fusion. Advances in electronics
have led to a plethora of range sensors and algorithms. Many of these are
logically equivalent and can be used interchangeably; for example, with ob-
stacle avoidance behaviors.
The design of a perceptual schema or a sensor suite requires a careful anal-
ysis. Each individual sensor should ﬁt the task, power, and processing con-
straints. Likewise, the entire sensor suite should provide complete coverage
of all perceptual processing required by the robot.
Almost all mobile robots have some form of proprioception, most likely
shaft or wheel encoders used to estimate the distance traveled based on the
number of times the motor has turned. Outdoor robots may carry GPS, and
this trend is expected to increase as the cost of receivers goes down and in-
expensive DGPS systems emerge.
Reactive navigation requires exteroception, whereby the robot observes
the environment. Proprioception can guide a robot on a path, but exterocep-
tion can prevent it from hitting an unmodeled obstacle or falling off a cliff.
The most common exteroceptive sensor on reactive robots is an ultrasonic
transducer or sonar. An ultrasonic transducer is an active sensor which re-
turns a single range reading based on the time-of-ﬂight of an acoustic wave.
Some of the difﬁculties associated with ultrasonics include erroneous read-
ings due to specular reﬂection, crosstalk, and foreshortening. Other popular
proximity sensors are IR and laser rangers.
Due to the low price and availability of consumer electronics, computer
vision is becoming more common in robotic systems. Computer vision pro-
cessing operates on images, regardless of the modality which generated it.
Color coordinate systems tend to divide an image into 3 planes. The two

6.10
Exercises
251
most common color coordinate systems are RGB and HSV. HSV treats color
in absolute terms, but RGB is favored by equipment manufacturers. A color
space used in biomedical imaging, SCT, appears to be less sensitive to light-
ing conditions than RGB and RGB-derived HSV. Many reactive robots exploit
color as an affordance. This can be done by thresholding an image and iden-
tifying regions of the appropriate color. A color affordance method which
works well for objects with multiple colors is color histogramming. Stereo
range ﬁnding is an important class of algorithms for navigation, though the
computational complexity has prevented it being ported to many mobile ro-
bot applications. Laser range ﬁnders, particularly the inexpensive planar
rangers, have grown in popularity over the past few years.
Despite the diversity of sensors and affordances inherent in the environ-
ment, reactive robotics is remarkable for its lack of sophistication in sensing.
This may stem from the split between computer vision and robotics in the
formative years of the ﬁeld. Many roboticists still assume algorithms de-
veloped by computer vision specialists are too computationally expensive
to work on commercially available on-board processors. This is no longer
true, in part because of the increased computational power of general pur-
pose chips. Readers are encouraged to explore the large body of literature on
computer vision and free tools on the web.
6.10
Exercises
Exercise 6.1
Deﬁne sensor suite, active/passive sensors, dead reckoning, computer vision.
y
Exercise 6.2
Compare and contrast the RGB and HSV color representations specifying the advan-
tages and disadvantages of each type.
y
Exercise 6.3
Ultrasonic sensors have many positive and negative attributes. Name and describe
three positive and three negative attributes.
y
Exercise 6.4
What is the difference between physical and logical redundancy?
y
Exercise 6.5
Describe the three major problems of ultrasonic sensing, and deﬁne a hypothetical
instance in which a robot would encounter each problem (such as a room with a
large amount of glass surfaces).
y

252
6
Common Sensing Techniques for Reactive Robots
Exercise 6.6
Describe with examples the three attributes that should be considered for an entire
sensing suite.
y
Exercise 6.7
Consider a Lego Mindstorms robot. Classify the available sensors for it in terms of
modalities.
Exercise 6.8
Describe the problems of specular reﬂection, cross talk, and foreshortening with an
ultrasonic transducer.
y
Exercise 6.9
List the metrics for rating individual sensors and a sensor suite, and apply these met-
rics to a particular application.
y
Exercise 6.10
An alternative to a Denning ring is to mount one or more sonars on a mast and turn
the mast. Turning gives the robot a
0
 coverage. Which do you think would be
better, a ﬁxed ring or a panning mast? Would a panning mast reduce problems of
foreshortening, cross-talk, and specular reﬂection.
Exercise 6.11
List and describe advantages and disadvantages of 3 different sensors, including one
type of computer vision.
y
Exercise 6.12
Describe all the different characteristics of sensors that must be evaluated when de-
signing a sensor suite. In addition, give priorities for each to determine which you
would consider to be the most and least important of these characteristics for a ro-
bot that was going to be designed for the 1994 AUVS Unmanned Ground Robotics
Competition.
y
Exercise 6.13
Pick a sensor suite that you think would do a good job if you were designing a robot
for the 1995 UGV competition described in an earlier chapter. Explain what each sen-
sor would do as well as describe the sensors and sensor suite in terms of the attributes
listed in this chapter.
y
Exercise 6.14
You are to design a sensor suite for a new robot for use by ﬁre ﬁghters. The robot is
designed to seek out people in a smoke ﬁlled building. Keep in mind the following:
1. Visibility is often very limited due to smoke.
2. Heat can be both an attactive force (e.g. human) or repulsive (e.g. open ﬂame).
3. Obstacles may have a wide variety of sound absorbtion (e.g. carpeting or furni-
ture).

6.10
Exercises
253
Describe the types of sensors that may be needed and how they will be used. Do not
focus on how the robot moves around just on the sensors it will need to use. Extra
credit: Comment on using simple voice recognition software and a microphone to
seek out human voices (e.g., cries for help).
y
Exercise 6.15
How are the concepts of logical sensors in robotics and polymorphism in object-
oriented programming similar?
Exercise 6.16
Deﬁne image function. What is the image function for
a. the left-right images in a stereo pair?
b. the depth map?
Exercise 6.17
What are two disadvantages of light stripers?
Exercise 6.18
Consider an obstacle avoidance behavior which consists of a perceptual schema that
provides a polar plot of range and motor schema which directs the robot to the most
open sector. List all the logical sensors covered in this chapter that can be used in-
terchangeably for the perceptual schema. Which of these are logically redundant?
Physically redundant?
Exercise 6.19
Assume you had a mobile robot about 0.5 meters high and a planar laser range ﬁnder.
What angle would you mount the laser at if the robot was intended to navigate
a. in a classroom?
b. in a hallway or reception area where the primary obstacle is people?
c. outdoors in unknown terrain?
State any assumptions your design is based on. Is there more information needed; if
so, what?
Exercise 6.20
[Programming]
Write your own code to give you the threshold of a small interleaved image.
y
Exercise 6.21
[World Wide Web]
Search for Kludge, another robot built by Ian Horswill. Describe Kludge’s homemade
whisker system and how well it works.
Exercise 6.22
[Programming]
Program a mobile robot to ﬁnd and bump an orange tennis or soccer ball.

254
6
Common Sensing Techniques for Reactive Robots
Exercise 6.23
[Programming]
Create a color histogram program. Construct the color histogram,
E, for four differ-
ent brightly colored objects, such as dolls of the South Park or the Simpsons cartoon
characters. Present the program with a different image,
I, of one of the characters and
compute the histogram intersection with each of the four
E. Does the highest match
correctly identify the character? Why or why not?
6.11
End Notes
For the roboticist’s bookshelf.
Hobart Everett literally wrote the book, Sensors for Mobile Robots, 52 on robotic sen-
sors, which provides both analytical details of sensors plus practical experience from
Everett’s many years with the Navy work on robots. He has built a series of mobile
robots called ROBART (a pun on Everett’s nickname, Bart); ROBART II has been in
continuous operation since 1982. Everett’s laboratory has to be one of the most ideally
situated in the world. It is in San Diego, overlooks the Paciﬁc Ocean, and is adjacent
to a frequently used volleyball court.
Hans Moravec.
If Joe Engelberger is known as the father of industrial robotics, Hans Moravec is best
known as the father of AI robotics. He has also become a well-known author, argu-
ing for the inevitability of machine intelligence in controversial books such as Mind
Children 94 and Robot: mere machine to transcendent mind. 96 His work with the Stan-
ford Cart was a catalyzing event in attracting attention to robotics after the years of
slow progress following Shakey. Documentaries will occasionally run edited footage
of the Stanford Cart navigating outdoors, avoiding obstacles. Since the cart trav-
eled in a stop-start fashion, with 15 minutes or so between updates, the location of
the shadows visibly change. Moravec’s ofﬁce mate, Rodney Brooks, helped with the
recording.
Undergraduates and sonars.
It is interesting to note that the ﬁrst serious analysis of the Polaroid sonars was done
by an undergraduate at MIT, Michael Drumheller. Drumheller’s paper, “Mobile Ro-
bot Localization Using Sonar,” was eventually published in the IEEE Transactions on
Pattern Analysis and Machine Intelligence in 1987, and became a classic.
Ballard and Brown.
Dana Ballard and Chris Brown wrote the ﬁrst textbook on computer vision, entitled
appropriately Computer Vision. Both have worked with robots, though Brown more
than Ballard. I met both of them at a workshop at a beach resort in Italy in early 1990.
I had just arrived from a harrowing bus ride from the airport and had to go past the
pool to my room in the hopes of recovering from jet lag before the meeting started in

6.11
End Notes
255
the morning. As I walked past the pool, one of my friends said, “Oh, Chris Brown
is over there!” I immediately turned to look for what I thought would be an older,
digniﬁed author wearing a suit and tie. Instead, I got soaked by a tall, youngish man
impishly doing a cannonball into the pool. From that day on, I never assumed that
textbook authors were dull and digniﬁed.
The tallest roboticist.
Tom Henderson at the University of Utah was one of the founders of the concept of
logical sensors. Henderson is also the tallest known roboticist, and played basketball
in college for Louisiana State University.
Stereo with a single camera.
Ray Jarvis at Monash University in Austraila came up with a clever way of gathering
rectiﬁed stereo images from a single camera. He used a prism to project two slightly
different veiwpoints onto the lens of a camera, creating an image which had a differ-
ent image on each side. The algorithm knew which pixels belong to each image, so
there was no problem with processing.
USAR and Picking Up the Trash.
While picking up trash seems mundane, most people would agree that ﬁnding and
rescuing survivors of an earthquake is not. The two tasks have much in common,
as illustrated by the work of Jake Sprouse, and behaviors and schemas developed
in one domain can be transferred to another. Sprouse was a member of the Colorado
School of Mines’ 1995 IJCAI competition team. The same vision program he wrote for
ﬁnding “red” was used to ﬁnd “international orange.” Later, he extended the search
strategies to incorporate aspects of how insects forage for food. 108
Run over Barney.
The ﬁgures and materials on color histogramming used in this chapter were part
of research work conducted by Dale Hawkins in persistence of belief. His use of a
stuffed Barney doll started out from a class project: program a mobile robot to ﬁnd a
Barney doll and run over it. This is actually a straightforward reactive project. The
Barney doll is a distinctive purple, making it easy for the vision system to ﬁnd it. The
project allowed the programmers to use the ﬂat earth assumption, so trigonometry
could be used to estimate the location to the doll based on the location in image coor-
dinates. Hawkins’ program was the clear winner, running completely over Barney. It
also gave him vision code that he could reuse for his thesis.
Reactive soccer.
Color regions are often used to simplify tracking balls (and other robots) in robot
soccer competitions such as RoboCup and MIROSOT. One amusing aspect is that
many of these behaviors are purely reﬂexive; if the robot sees the ball, it responds,
but if it loses the ball, it stops. Ann Brigante and Dale Hawkins programmed a No-

256
6
Common Sensing Techniques for Reactive Robots
mad 200 to reﬂexively track a soccer ball to compare what would happen if the robot
had some concept of object permanence. Because of the angle of the camera, the ro-
bot would lose sight of the ball when it was almost touching it. The behaviors that
emerged worked, but always generated much laughter. The robot would see the ball
and accelerate rapidly to its estimated location to “kick” it. When it got to the ball, it
suddenly deccelerated but had enough momentum to bump the ball. The ball would
slowly roll forward, back into the now-stationary robot’s ﬁeld of view. The robot
would again jump forward, and the cycle would repeat endlessly.
Photographs and scanning.
Dale Hawkins, Mark Micire, Brian Minten, Mark Powell, and Jake Sprouse helped
photograph robots, sensors, and demonstrations of perceptual behaviors.

7
The Hybrid Deliberative/Reactive
Paradigm
Chapter objectives:
 Be able to describe the Hybrid Deliberative/Reactive paradigm in terms
of i) sensing, acting, and planning and ii) sensing organization.
 Name and evaluate one representative Hybrid architecture in terms of:
support for modularity, niche targetability, ease of portability to other do-
mains, robustness.
 Given a list of responsibilities, be able to say whether it belongs in the
deliberative layer or in the reactive layer.
 List the ﬁve basic components of a Hybrid architecture: sequencer agent,
resource manager, cartographer, mission planner, performance monitor-
ing and problem solving agent.
 Be able to describe the difference between managerial, state hierarchy, and
model-oriented styles of Hybrid architectures.
 Be able to describe the use of state to deﬁne behaviors and deliberative
responsibilities in state hierarchy styles of Hybrid architectures.
7.1
Overview
By the end of the 1980’s, the trend in artiﬁcially intelligent robots was to
design and program using the Reactive Paradigm. The Reactive Paradigm
allowed robots to operate in real-time using inexpensive, commercially avail-
able processors (e.g., HC6811) with no memory. But the cost of reactivity, of
course, was a system that eliminated planning or any functions which in-
volved remembering or reasoning about the global state of the robot relative

258
7
The Hybrid Deliberative/Reactive Paradigm
to its environment. This meant that a robot could not plan optimal trajecto-
ries (path planning), make maps, monitor its own performance, or even se-
lect the best behaviors to use to accomplish a task (general planning). Notice
that not all of these functions involve planning per se; map making involves
handling uncertainty, while performance monitoring (and the implied objec-
tive of what to do about degraded performance) involves problem solving
and learning. In order to differentiate these more cognitively oriented func-
tions from path planning, the term deliberative was coined.
DELIBERATIVE
The Reactive Paradigm also suffered somewhat because most people found
that designing behaviors so that the desired overall behavior would emerge
was an art, not a science. Techniques for sequencing or assembling behav-
iors to produce a system capable of achieving a series of sub-goals also relied
heavily on the designer. Couldn’t the robot be made to be smart enough to
select the necessary behaviors for a particular task and generate how they
should be sequenced over time?
Therefore, the new challenge for AI robotics at the beginning of the 1990’s
was how to put the planning, and deliberation, back into robots, but without
disrupting the success of the reactive behavioral control. The consensus was
that behavioral control was the “correct” way to do low level control, because
of its pragmatic success, and its elegance as a computational theory for both
biological and machine intelligence. As early as 1988, Ron Arkin was pub-
lishing work on how to add more cognitive functions to a behavioral system
in the form of the Autonomous Robot Architecture (AuRA). Many roboticists
looked at adding layers of higher, more cognitive functions to their behav-
ioral systems, emulating the evolution of intelligence. This chapter will cover
ﬁve examples of architectures which illustrate this bottom-up, layering ap-
proach: AuRA, Sensor Fusion Effects (SFX), 3T, Saphira, and TCA. Other robot
systems which do not strongly adhere to an architectural style, such as Rhino
and Minerva, will be discussed in later chapters.
During the 1990’s, members of the general AI community had become ex-
posed to the principles of reactive robots. The concept of considering an
intelligent system, or agent, as being situated in its environment, combined
with the existence proof that detailed, Shakey-like world representations are
not always necessary, led to a new style of planning. This change in planning
was called reactive planning. Many researchers who had worked in traditional
REACTIVE PLANNING
AI became involved in robotics. One type of reactive planner for robots,
Jim Firby’s reactive-action packages (RAPs),53 was integrated as a layer within
the 3T architecture.21 Architectures stemming from the planning community
roots showed their traditional AI roots. They use a more top-down, hierar-

7.2
Attributes of the Hybrid Paradigm
259
chical ﬂavor with global world models, especially Saphira77 and TCA.131
Regardless of the bottom-up or top-down inspiration for including non-
behavioral intelligence, architectures which use reactive behaviors, but also
incorporate planning, are now referred to as being part of the Hybrid De-
liberative/Reactive Paradigm. At ﬁrst, Hybrids were viewed as an artifact
of research, without any real merit for robotic implementations. Some re-
searchers went so far as to recommend that if a robot was being designed to
operate in an unstructured environment, the designer should use the Reac-
tive Paradigm. If the task was to be performed in a knowledge-rich environ-
ment, easy to model, then the Hierarchical Paradigm was preferable, because
the software could be engineered speciﬁcally for the mission. Hybrids were
believed to be the worst of both worlds, saddling the fast execution times of
reactivity with the difﬁculties in developing hierarchical models.
The current thinking in the robotics community is that Hybrids are the
best general architectural solution for several reasons. First, the use of asyn-
chronous processing techniques (multi-tasking, threads, etc.) allow deliber-
ative functions to execute independently of reactive behaviors. A planner
can be slowly computing the next goal for a robot to navigate to, while the
robot is reactively navigating toward its current goal with fast update rates.
Second, good software modularity allows subsystems or objects in Hybrid
architectures to be mixed and matched for speciﬁc applications. Applica-
tions which favor purely reactive behaviors can implement just the subset of
the architecture for behaviors, while more cognitively challenging domains
can use the entire architecture.
7.2
Attributes of the Hybrid Paradigm
The organization of a Hybrid Deliberative/Reactive system can be described
as: PLAN, then SENSE-ACT. It is shown in Fig. 7.1. The PLAN box includes
all deliberation and global world modeling, not just task or path planning.
The robot would ﬁrst plan how to accomplish a mission (using a global world
model) or a task, then instantiate or turn on a set of behaviors (SENSE-ACT)
to execute the plan (or a portion of the plan). The behaviors would execute
until the plan was completed, then the planner would generate a new set of
behaviors, and so on.
The idea of PLAN, then SENSE-ACT evolved from two assumptions of
the Hybrid Paradigm. First, planning covers a long time horizon and re-
quires global knowledge, so it should be decoupled from real-time execution

260
7
The Hybrid Deliberative/Reactive Paradigm
ACT
PLAN
SENSE
Figure 7.1
P,SA Organization of Hybrid Deliberative/Reactive Paradigm in terms
of Primitives.
just on the software engineering principle of coherence (dissimilar functions
should be placed in different objects). It is good for setting objectives and se-
lecting methods, but not for making ﬁnely grained decisions. Another way
of stating this is that deliberation works with symbols, e.g., the goal is to pick up
DELIBERATION AND
SYMBOLS
a “Coca-Cola can,” while reaction works with sensors and actuators, e.g., the
percept is a “red blob” which exerts an attractive ﬁeld. Second, planning and
global modeling algorithms are computationally expensive, so they should
be decoupled from real-time execution just from a standpoint of practicality
because they would slow down the reaction rate.
The organization of sensing in the Hybrid architecture is more complex.
Sensing is truly hybrid, as seen in Fig. 7.2. In the behaviors, sensing remains
as it was for the Reactive Paradigm: local and behavior speciﬁc. But plan-
ning and deliberation requires global world models. Therefore, planning
functions have access to a global world model. The model is constructed by
processes independent of the behavior-speciﬁc sensing. However, both the
perceptual schemas for the behaviors and the model making processes can
share the same sensors. Furthermore, the model making processes can share
the percepts created by the perceptual schemas for behaviors (eavesdrop) or
it can have sensors which are dedicated to providing observations which are
useful for world modeling but aren’t used for any active behaviors.
The organization of the SENSE, PLAN, ACT primitives in the Hybrid Para-
digm is conceptually divided into a reactive (or reactor) portion and a deliber-
REACTOR
ation (or deliberator) portion. Although many architectures will have discrete
DELIBERATOR

7.2
Attributes of the Hybrid Paradigm
261
 World Map/    
 Knowledge Rep 
Behavior
Behavior
Behavior
Sensor 3 Sensor 1 Sensor 2
virtual sensor
Figure 7.2
Sensing Organization in Hybrid Paradigm, showing that the global
world model can have its own sensors, can “eavesdrop” on sensors or percepts
used/created by behaviors, and can act as a virtual sensor for use by a behavior.
layers of functionalities within the reactor and deliberator, each architecture
in the Hybrid Paradigm has obvious partitions between reactive and delib-
erative functions.
7.2.1
Characteristics and connotation of reactive behaviors in hybrids
The Hybrid Paradigm is an extension of the Reactive Paradigm, and from
the above description, it would appear that the behavioral component is un-
touched. That is not entirely true. Behaviors in the Hybrid Paradigm have a
slightly different connotation than in the Reactive Paradigm. In the Reactive
Paradigm, “behavior” connotes purely reﬂexive behaviors. In the Hybrid
Paradigm, the term “behavior” is usually more consistent with the etholog-
ical use and includes reﬂexive, innate, and learned behaviors. This can be
confusing, and at least one architecture uses the term skill instead of “be-
SKILL VS. BEHAVIOR
havior” to avoid confusion with purely reﬂexive behaviors. Also, Hybrid
implementations tend to use assemblages of behaviors sequenced over time,

262
7
The Hybrid Deliberative/Reactive Paradigm
rather than primitive behaviors. Because the Hybrid implementations are
interested in more complex emergent behaviors, there is more diversity in
methods for combining the output from concurrent behaviors.
7.2.2
Connotations of “global”
The term “global” is used almost synonymously with “deliberative” and “lo-
cal” with “reactive.” This can lead to signiﬁcant confusion, because “global”
isn’t always truly global in Hybrids.
The deliberative portion of a Hybrid architecture contains modules and
functions for things which are not easy to represent in reactive behaviors.
Some of these functions clearly require a global world model; path plan-
ning and map making are probably the best examples. But other activities
require global knowledge of a different sort. Behavorial management (plan-
BEHAVORIAL
MANAGEMENT
ning which behaviors to use) requires knowing something about the current
mission and the current (and projected) state of the environment. This is
global knowledge in that it requires the module to know something outside
of itself, as compared to a reactive behavior which can function without any
knowledge of whether there are other behaviors actively executing. Like-
wise, performance monitoring to see if the robot is actually making progress
PERFORMANCE
MONITORING
to its goal, and problem solving is a global activity. Consider writing a pro-
gram to diagnose whether the robot’s not moving forward is a problem with
the terrain (it’s stuck in the mud) or a sensor (the shaft encoders don’t report
wheel turns correctly). In order to perform the diagnostics, the program has
to know what the behaviors were trying to accomplish, if there are any other
sensors or knowledge sources to corroborate any hypotheses, etc. Therefore,
a deliberative function may not need a global world model, but may need
to know about the internal workings of the robot on a global scale, if only
to know what other modules or deliberative capabilities the program should
interact with.
7.3
Architectural Aspects
The differences between various Hybrid Deliberative/Reactive architectures
fall into three areas based on how they answer the following questions: How
does the architecture distinguish between reaction and deliberation? How does it
organize responsibilities in the deliberative portion? How does the overall behavior
emerge? The difference between reaction and deliberation is a critical issue in
building a successful, reusable object-oriented implementation. This deter-

7.3
Architectural Aspects
263
mines what functionality goes in what modules, what modules have access
to global knowledge (which leads to specifying public and friend classes
in C++), and what that global knowledge (shared data structures) should
be. Likewise, it is important to subdivide the deliberative portion into mod-
ules or objects. A good decomposition will ensure portability and reusabil-
ity. While Hybrid architectures are most noteworthy for how they incorpo-
rate deliberation into mobile robotics, they also introduce some changes in
the way reaction is organized. Many researchers found the two primary
means of combining reactive behaviors—subsumption and potential ﬁeld
summation—to be limited. Since then at least three other mechanisms have
been introduced: voting (in the DAMN architecture),121 fuzzy logic (Saphira),77
and ﬁltering (SFX).107
The number of Hybrid architectures is rapidly increasing. This section
attempts to introduce some conceptual organization on Hybrids in two ways.
First, it offers a set of common components—essentially, things to look for in
a Hybrid architecture. Second, it divides Hybrid into three broad categories:
managerial, state hierarchies, and model-oriented.
7.3.1
Common components of hybrid architectures
While Hybrid architectures vary signiﬁcantly in how they implement de-
liberative functionality, what they implement is fairly similar. Generally a
Hybrid architecture has the following modules or objects:
 A Sequencer agent which generates the set of behaviors to use in order to
SEQUENCER
accomplish a subtask, and determines any sequences and activation con-
ditions. The sequence is usually represented as a dependency network or
a ﬁnite state machine, but the sequencer should either generate this struc-
ture or be able to dynamically adapt it. Recall from Ch. 5 that assemblages
of reactive behavior are manually constructed.
 A Resource manager which allocates resources to behaviors, including se-
RESOURCE MANAGER
lecting from libraries of schemas. For example, a robot may have stereo
vision, sonars, and IR sensors, all of which are capable of range detec-
tion. The behavioral manager would ascertain whether the IR sensors can
detect at a sufﬁcient range, the stereo vision can update fast enough to
match the robot’s desired velocity, and the sonars have enough power to
produce reliable readings. In reactive architectures, the resources for a
behavior were often hard-coded or hardwired, despite the ability of hu-

264
7
The Hybrid Deliberative/Reactive Paradigm
mans to use alternative sensors and effectors (e.g., opening a door with
the other hand when the holding something in the preferred hand).
 A Cartographer which is responsible for creating, storing, and maintain-
CARTOGRAPHER
ing map or spatial information, plus methods for accessing the data. The
cartographer often contains a global world model and knowledge repre-
sentation, even if it is not a map.
 A Mission Planner which interacts with the human, operationalizes the
MISSION PLANNER
commands into robot terms, and constructs a mission plan. For example,
the ideal robot assistant might be given a command: “Lassie, go get the
sheriff.” The mission planner would interpret that command to mean to
ﬁrst physically search for a person, identify him as a sheriff because of
the uniform, attract his attention, and then lead him back to the current
location. The mission planner might have access to information that sher-
iffs are most likely to be in their ofﬁce, donut shops, or the last place they
were seen. The plan might ﬁrst be to navigate to the sheriff’s ofﬁce.
 A Performance Monitoring and Problem Solving agent which allows the robot
PERFORMANCE
MONITORING AND
PROBLEM SOLVING
to notice if it is making progress or not. Notice that this requires the robot
to exhibit some type of self-awareness.
7.3.2
Styles of hybrid architectures
Architectural styles can be loosely divided into three categories. Managerial
MANAGERIAL STYLES
styles focus on subdividing the deliberative portion into layers based on the
scope of control, or managerial responsibility, of each deliberative function.
A Mission Planning module would be able to direct other, subordinate de-
liberative modules such as navigation, because Mission Planning (where to
go) is more abstract than Path Planning (how to get there). State hierarchies
STATE HIERARCHIES
use the knowledge of the robot’s state to distinguish between reactive and
deliberative activities. Reactive behaviors are viewed as having no state, no
self-awareness, and function only in the Present. Deliberative functions can
be divided into those that require knowledge about the robot’s Past state
(where it is in a sequence of commands) and about the Future (mission and
path planning). Model-oriented styles are more nebulous. They are character-
MODEL-ORIENTED
STYLES
ized by behaviors that have access to portions of a world model, often to the
point that they appear to have returned to the Hierarchical monolithic global
world model.

7.4
Managerial Architectures
265
7.4
Managerial Architectures
Managerial styles of Hybrid architectures are recognizable by their decom-
position of responsibilities similar to business management. At the top are
agents which do high level planning, then pass off the plan to subordinates,
who reﬁne the plan and gather resources, and pass those down to the lowest
level workers, the reactive behaviors. Higher level agents can see the results
of their subordinate lower level agents (essentially eavesdrop on them), and
can give them directions. As with subsumption, a layer can only modify the
layer below it. In Managerial styles, each layer attempts to carry out its direc-
tive, identify problems and correct them locally. Only when an agent cannot
solve its own problem does it ask for help from a superior agent; the agent is
said to fail upwards in this case.
FAIL UPWARDS
7.4.1
Autonomous Robot Architecture (AuRA)
Autonomous Robot Architecture (AuRA) is the oldest of the Hybrids. It was
AUTONOMOUS ROBOT
ARCHITECTURE
(AURA)
actually designed and implemented by Arkin at the same time Brooks was
beginning to publish his work with subsumption. AuRA is based on schema
theory, and consists of ﬁve subsystems, equivalent to object-oriented classes.
Two of the subsystems comprise the deliberative portion: the Planner, and
the Cartographer. The Planner is responsible for mission and task planning.
It is subdivided into three components, equivalent to the Nested Hierarchical
Controller93 discussed in Ch. 2. The Cartographer encapsulates all the map
making and reading functions needed for navigation. The Cartographer can
also be given an a priori map. For example, a human operator might load in
a map ﬁle for the Cartographer to use. The three components of the Planner
would interact with the Cartographer through methods to obtain a path to
follow, broken down into subsegments.
The Planner subsystem is divided into the Mission Planner, Navigator, and
Pilot. The Mission Planner serves as the interface with the human, and the
current implementation of AuRA has one of the most extensive and user
friendly robot interfaces available. The Navigator works with the Cartogra-
pher to compute a path for the robot and break it into subtasks (go over the
mountain to the water tower, follow the road along the ridge to the camp).
The Pilot takes the ﬁrst subtask (go over the mountain to the water tower)
and gets relevant information (terrain, foliage types, what the water tower
looks like) to generate behaviors. The Pilot portion of the Planning subsys-
tem interacts with the Motor Schema Manager in the Motor subsystem, giv-

266
7
The Hybrid Deliberative/Reactive Paradigm
mission planner
navigator
pilot
Cartographer
motor schema manager
Homeostatic
Control
ms1
ms2
ms3
Σ
sensors
actuators
Cartographer
Mission Planner,
Sequencer,
Performance Monitoring Agent
Behavioral Manager
Reactive Layer
Deliberative Layer
Motor
Sensor
ps1
ps2
ps3
Planner
Figure 7.3
Layout of AuRA, showing the ﬁve subsystems.
ing it the list of behaviors it needs to accomplish the current subsegment. The
Motor Schema Manager composes each behavior by examining the libraries
of perceptual schemas in the Sensing subsystem and of motor schemas in the
Motor subsystem. The motor schemas represent actions with potential ﬁelds,
and the overall behavior emerges due to vector summation.
The Sensor and Motor subsystems make up the reactive portion of the ar-
chitecture. These classes contain libraries of perceptual and motor schemas,

7.4
Managerial Architectures
267
which form the behavior schema. The schemas themselves can consist of as-
semblages of primitive schemas, coordinated by ﬁnite state machines. Sche-
mas can share information, if necessary, through links established by the
Motor Schema Manager. Behaviors are not restricted to being purely reﬂex-
ive; behavior speciﬁc knowledge, representations, and memory is permitted
within the schemas. The motor schemas, however, are restricted to potential
ﬁelds.
The ﬁfth subsystem, Homeostatic Control, falls into a gray area between
deliberation and reaction. The purpose of Homeostatic control is to modify
the relationship between behaviors by changing the gains as a function of the
“health” of the robot or other constraints. As an example, consider a plan-
etary rover operating on a rocky planet. The robot is tasked to physically
remove rock samples from various locations around the planet and deliver
them to a return vehicle. The return vehicle has a ﬁxed launch date; it will
blast off, returning to Earth on a set day and time no matter what. Now, the
rover may be provided with default gains on its behaviors which produce
a conservative behavior. It may stay two meters away from each obstacle,
giving itself a wide margin of error. At the beginning of the mission, such a
conservative overall behavior appears reasonable. Now consider what hap-
pens towards the time when the return vehicle is set to launch. If the robot is
near the return vehicle, it should be willing to shave corners and reduce the
margin by which it avoids obstacles in order to ensure delivery. The robot
should be willing to perform the equivalent of sacriﬁcing its own existence
for the sake of the mission.
The issue becomes how to do homeostatic. Many aspects of AuRA are
motivated by biology, and homeostatic control is no exception. Rather than
put a module in the deliberative portion to explicitly reason about how to
change the overall behavior of the robot, biology suggests that animals sub-
consciously modify their behaviors all the time in response to internal needs.
For example, an animal who needs food becomes increasingly focused on
ﬁnding food. Human behavior changes in response to insulin. Fortunately,
changing the emergent behavior is straightforward in a potential ﬁelds rep-
resentation of behaviors, since the output vector produced by each behavior
can be scaled by a gain term. Returning to the case of the planetary rover
rushing to make the last delivery, the gain on the move-to-goal behavior at-
tracting the rover to the return vehicle should start going up, while the gain
on the avoid-obstacle behavior should start going down as a function of time
to launch.

268
7
The Hybrid Deliberative/Reactive Paradigm
The table below summarizes AuRA in terms of the common components
and style of emergent behavior:
AuRA Summary
Sequencer Agent
Navigator, Pilot
Resource Manager
Motor Schema Manager
Cartographer
Cartographer
Mission Planner
Mission Planner
Performance Monitoring Agent
Pilot, Navigator, Mission Planner
Emergent behavior
Vector summation, spreading activation
of behaviors, homeostatic control
7.4.2
Sensor Fusion Effects (SFX)
Another managerial style of architecture is the Sensor Fusion Effects (SFX)
architecture, which started out as an extension to AuRA by Murphy to in-
corporate pathways for sensing. The extension was to add modules to spec-
ify how sensing, including sensor fusion and handling sensor failures, are
handled. Over time, SFX has reorganized both the reactive and deliberative
components of AuRA although the two architectures remain philosophically
identical. SFX is an example of how robustness can be built into an architec-
ture. SFX has been used on eight robots for indoor ofﬁce navigation, outdoor
road following, and urban search and rescue.
Fig. 7.4 shows a neurophysiological model of sensing based on studies
with sensing in cats. The model suggests that sensory processing is initially
local to each sensor, and may have its own sensor-dependent receptive ﬁeld.
This is consistent with reactive robot behaviors, and at least with the mo-
tivation for sensor ﬁssion. Sensor processing then appears to branch, with
duplicates going to the superior colliculus (a mid-brain structure responsible
for motor behaviors) and the other to the cerebral cortex (responsible for more
cognitive functions). The branching allows the same sensor stream to be
used simultaneously in multiple ways. In SFX, the equivalent superior col-
liculus functions are implemented in the reactive layer, and cortical activities
in the deliberative layer. Branching of perception is done through the use
of whiteboards, common in numerous AI systems as global cognitive data
structures.
The deliberative component is divided into modules or object classes, each
of which is actually a software agent, a self-contained software program
which specializes in some area of competence and can interact with other

7.4
Managerial Architectures
269
sense
receptive
field
sense
receptive
field
sense
receptive
field
muscle
muscle
muscle
Superior
Colliculus
focus-of-attention,
feedback for
recalibration, adaptation
Cerebral Cortex
cognitive
influences
cognitive model
focus-of-attention,
feedback for
recalibration, adaptation
behaviors
sense
receptive
field
sense
receptive
field
sensor
receptive
field
effector
effector
effector
Cartographer
Sensor
Manager
Task
Planner
Effector
Manager
Deliberative Layer
Reactive Layer
behaviors
behaviors
behavior instantiation
sensor allocation
SENSOR WHITEBOARD
BEHAVIORAL WHITEBOARD
SFX implementation
Figure 7.4
The cognitive model used for the implementation of the Sensor Fusion
Effects Architecture.

270
7
The Hybrid Deliberative/Reactive Paradigm
agents. The dominant agent is called the Mission Planner agent. This agent
serves to interact with the human and specify the mission constraints to the
other agents in the deliberative layer. The agents in the deliberative layer
attempt to ﬁnd (and maintain) a set of behaviors which can accomplish the
mission while meeting the constraints. The software agents in each deliber-
ative layer are peers; just as with behaviors, they operate independently of
each other. But the nature of deliberation suggests that they have to negoti-
ate with other peers to ﬁnd a satisfactory set of behaviors to accomplish the
current task. One way to think of this partitioning is that the Mission Plan-
ner acts as a president or CEO in a large company, giving directions, while
the behaviors are the workers. The agents in the lower deliberative layer are
middle-management, planning how to organize the workers’ assignments,
monitoring productivity, and adapting assignments if necessary.
Within the deliberative layer, the Task Manager, Sensing Manager, and Ef-
fector Manager serve as the resource managers. The resource manager func-
tions are divided across managers because the types of knowledge and algo-
rithms are different for managing sensors and actions. The managers use AI
planning, scheduling, and problem solving techniques to determine the best
allocation of effector and sensing resources given the set of motor and per-
ceptual schemas for a behavior. They are not allowed to relax any constraints
speciﬁed by the Mission Planner, so they essentially know what the robot is
supposed to do, but only the Mission Planner knows why. The advantage of
this middle-management layer is that it simpliﬁes the AI techniques needed
for behavioral management.
The Sensing Manager in SFX is particularly noteworthy because of its ex-
plicit commitment to performance monitoring and problem solving. It has
two software agents for monitoring both the task performance and whether
the habitat has changed (if so, a performance failure is likely to occur). If
a behavior fails or a perceptual schema detects that sensor values are not
consistent or reasonable, the Sensing Manager is alerted. It can then identify
alternative perceptual schemas, or even behaviors, to replace the problematic
behavior immediately. Imagine a mobile robot in a convoy of robots hauling
food to refugees. If the robot had a glitch in a sensor, it shouldn’t suddenly
just stop and think about the problem. Instead, it should immediately switch
to a back-up plan or even begin to smoothly slow down while it identiﬁes
a back-up plan. Otherwise, the whole convoy would stop, there might be
wrecks, etc. Then working in a background mode, the Sensing Manager can
attempt to diagnose the cause of the problem and correct it. In one demon-
stration, a robot using SFX resorted to shaking its camera to shake off a T-

7.4
Managerial Architectures
271
a.
b.
c.
d.
e.
Figure 7.5
A robot using SFX to ﬁgure out that something is wrong with the camera.
It cannot identify the problem, so it attempts to correct the problem by shaking.
Shirt covering the lens, as shown in Fig. 7.5.
The lower deliberative layer also contains a Cartographer agent and per-
formance monitoring agents. The Cartographer is responsible for map mak-
ing and path planning, while the performance agents attempt to observe the
progress of the robot toward its goals and notice when the robot is unsuc-
cessful, using work by Hughes.71 An example is when a robot gets stuck in
the mud on a dirt road. The follow-road behavior is executing correctly; it is
seeing the road and giving actuator commands correctly. The shaft encoders
show that the wheels are turning. So some other process with more self-
awareness than a reactive behavior is needed to notice that the robot isn’t
moving. In this case, if the robot is moving, the vision should change over
time, not remain the same.

272
7
The Hybrid Deliberative/Reactive Paradigm
The reactive component is also divided into two layers. In this case, the
layers reﬂect strategic versus tactical behaviors, deﬁned below. As noted ear-
STRATEGIC
TACTICAL BEHAVIORS
lier, just about every hybrid architecture has its own method of combining
behaviors in the reactive portion. AuRA uses potential ﬁeld combination,
whereas SFX uses a ﬁltering method. The SFX philosophy is similar to sub-
sumption: there are some fundamental behaviors that should override other
behaviors. The difference is that in SFX, it is the “lowest” tactical behaviors
which do the subsuming, not the “higher” strategic ones.
The idea of a tactical behavior is best seen by example. Recall the case
of obstacle avoidance with potential ﬁelds. The use of a repulsive ﬁeld was
simple, but could lead to a global minima, where the repulsion could can-
cel out any other motive ﬁelds such as move-to-goal. The NaTs132 solution
was to use the vector created by that other ﬁeld as an input to the avoid-
obstacle ﬁeld. That vector would lead to a tangential ﬁeld in addition to the
repulsive ﬁeld, resulting in an avoid-obstacle ﬁeld which repulses the robot
towards the direction it was heading in to begin with. In this case, the move-
to-goal ﬁeld was offering a strategic command; like a general, it was telling
the troops to move ahead. But the avoid-obstacle ﬁeld was like an infantry
man; it was trying to go in the direction given by the general but not hit
anything. The avoid-obstacle behavior ﬁlters the strategic direction (given
by move-to-goal) with the immediate tactical situation (the presence of an
obstacle).
Another example of a tactical behavior in SFX is speed-control. Speed-
control in AuRA and many architectures is a by-product of the mechanism
used for combining behaviors. The emergent speed in AuRA is the mag-
nitude of the vector summed from all the active behaviors. In SFX, speed-
control is considered a separate behavior. The safe velocity of a robot de-
pends on many inﬂuences. If the robot cannot turn in place (in effect, turns
like a car), it will need to be operating at a slow speed to make the turn with-
out overshooting. Likewise, it may need to go slower as it goes up or down
hills. These inﬂuences are derived from sensors, and the action is a template
(the robot always slows down on hills), so speed control is a legitimate be-
havior. But the other behaviors should have some inﬂuence on the speed as
well. So these other, strategic behaviors contribute a strategic speed to the
speed-control behavior. If the strategic speed is less than the safe speed com-
puted from tactical inﬂuences, then the output speed is the strategic speed.
But if the tactical safe speed is lower, the output speed to the actuator is the
tactical speed. Tactical behaviors serve as ﬁlters on strategic commands to

7.4
Managerial Architectures
273
slope
Strategic
Velocity
Clutter
Obstacles
How Much Vehicle Turns
Swivel Camera
Camera Direction
Safe Velocity
Direction To Path
Inclino-
meter    
Camera
Sonar
Follow-Path
Speed-Control
Avoid
Center-Camera
Sensors
Strategic Behaviors
Tactical Behaviors
Actuators
Drive 
Motor
Steer  
Motor
Camera
Pan      
Motor  
Figure 7.6
Strategic and tactical behaviors for following an outdoor path, used in
the 1995 UGV Unmanned Ground Robot competition.
ensure that the robot acts in a safe manner in as close accordance with the
strategic intent as possible. The interaction of strategic and tactical behaviors
is still considered emergent behavior.
One outcome of the strategic-tactical partitioning was the discovery that
every task to date could be done with one strategic behavior and several
tactical behaviors. This means that the need to combine behaviors does not
occur often, and so the combination mechanism is not particularly important.
However, it should be emphasized that the strategic behaviors were often
assemblages in the form of scripts. There were many strategic behaviors,
but they were explicitly coordinated and controlled according to behavior-
speciﬁc knowledge.

274
7
The Hybrid Deliberative/Reactive Paradigm
The table below summarizes SFX in terms of the common components and
style of emergent behavior:
SFX Summary
Sequencer Agent
Task Manager
Resource Manager
Sensing and Task Managers
Cartographer
Cartographer
Mission Planner
Mission Planner
Performance Monitoring Agent
Performance Monitor, Habitat Monitor
Emergent behavior
Strategic behaviors grouped into abstract
behaviors or scripts, then ﬁltered by
tactical behaviors
7.5
State-Hierarchy Architectures
State-hierarchy styles of architectures organize activities by the scope of time
knowledge. Since time is generally thought of as being Present, Past, and
Future, state-hierarchy styles of architectures usually have 3 layers. Unfortu-
nately, many other Hybrid architectures have 3 layers but with the layers
meaning something totally different. Therefore, this book refers to them
as state-hierarchies to emphasize the layering based on the state of know-
ledge. As with managerial styles, the organization is broken down into lay-
ers. Within each layer are peers of software agents or functions that accom-
plish the goals of that layer. As with managerial styles, a higher layer has
access to the output of lower layers and can operate on the next lower layer.
7.5.1
3-Tiered (3T)
The 3T, or 3-Tiered, mobile robot architecture is the best example of a state-
hierarchy system and is predominately used at NASA. The roots of 3T stem
from continuing work at NASA, where aspects of Slack’s NaT system, 132
Gat’s subsumption-style ATLANTIS architecture,57 and Firby’s RAPs sys-
tem53 were merged at JPL under the initial direction of David Miller, and
reﬁned by Pete Bonasso and Dave Kortenkamp at NASA’s Johnson Space
Center. As the name suggests, 3T divides itself into 3 layers, one clearly reac-
tive, one clearly deliberative, and one which serves as the interface between
the two. Fig. 7.7 show the layers actually running on three different comput-
ers at NASA Johnson Space Center controlling the Extra-Vehicular Activity
Helper-Retriever (EVAHR) robot simulator. 3T has been used primarily for
planetary rovers, underwater vehicles, and robot assistants for astronauts.

7.5
State-Hierarchy Architectures
275
Figure 7.7
Each of the 3T layers running on a separate computer, controlling the
EVAHR robot. (Photograph courtesy of the National Aeronautics and Space Admin-
istration, Johnson Space Center.)
The top layer of 3T is Planner. It fulﬁlls the duties of the mission planner
and cartographer by setting goals and strategic plans. These goals are passed
to the middle layer, called the Sequencer. The Sequencer uses a reactive plan-
ning technique called RAPs to select a set of primitive behaviors from a li-
brary and develops a task network specifying the sequence of execution for
the behaviors for the particular subgoal. The Sequencer is responsible for the
sequencer and performance monitoring functions of a generic Hybrid archi-
tecture. The Sequencer layer instantiates a set of behaviors (called skills) to
carry out the plan. These behaviors form the bottom layer, called the Con-
troller or Skill Manager. In order to avoid confusion with the connotations of
purely reﬂexive behaviors left over from the Reactive Paradigm, 3T does not
call its behaviors “behaviors.” Its behaviors have the same broader scope as
AuRA and SFX, permitting sensor fusion and assemblages of primitive be-
haviors. The preferred term is “skill” to distinguish its behaviors from the
connotations of behaviors popularized by the subsumption architecture. A
skill is often an assemblage of primitive skills; indeed, one of the interesting
aspects of 3T is its foundation as a tool for learning assemblages.

276
7
The Hybrid Deliberative/Reactive Paradigm
Subgoal
Subgoal
Subgoal
Planning
Monitoring
Task
Task
Task
Task
Task  
World
Model
Interpreter
RAP 
Memory
Skill 
Skill 
Event
Skill 
Event
Skill
Manager
Goal   
Deliberative Layer
Reactive Layer
Planner
Sequencer
Sequencer
Resource 
Manager
Performance Monitor Agent,
Mission Planner
Cartographer
Agenda
Task
Subtask
Primitive
Wait-for
Subtask
Figure 7.8
3T architecture.
A powerful attribute of the lower level is that the skills have associated
events, which serve as checkpoints to verify explicitly that an action has had
EVENTS
the correct effect. In some regards, events are equivalent to the innate releas-
ing mechanisms; both let the world be its own best representation.
The three layers represent true deliberation, reactive planning, and reac-
tive control. They also represent a philosophy organized by scope of state,
rather than scope of responsibility. The Skill Manager layer is composed
of skills that operate only in the Present (although with some allowances
to permit the persistence of some behaviors when the stimulus temporarily

7.6
Model-Oriented Architectures
277
disappears). The components in the Sequencer layer operate on state infor-
mation reﬂecting memories about the Past, as well as the Present. Therefore,
sequences of behaviors can be managed by remembering what the robot has
already done and whether that was successful or not. This adds a great
deal of robustness and supports performance monitoring. The Planner layer
works with state information predicting the Future. It can also use informa-
tion from the Past (what the robot has done or tried) and Present (what the
robot is doing right now). In order to plan a mission, the planner needs to
project what the environment will be and other factors.
In practice, 3T does not strictly organize its functions into layers by state
(Past, Present, Future); instead it often uses update rate. Algorithms that
UPDATE RATE.
update slowly are put in the Planner, while fast algorithms go in the Skill
Manager layer. This appears to be a situation where the pragmatic consider-
ations of computation inﬂuenced the design rules; in the early 1990’s behav-
iors were very fast, reactive planning (especially RAPs and Universal Plans)
were fast, and mission planning was very slow. However, many sensor algo-
rithms involving computer vision were also slow, so they were placed in the
Planner despite their low-level sensing function.
The table below summarizes 3T in terms of the common components and
style of emergent behavior:
3T
Sequencer Agent
Sequencer
Resource Manager
Sequencer (Agenda)
Cartographer
Planner
Mission Planner
Planner
Performance Monitoring Agent
Planner
Emergent behavior
Behaviors grouped into skills,
skills grouped into task networks
7.6
Model-Oriented Architectures
Both the managerial and state-hierarchy styles of architectures evolved di-
rectly from the Reactive Paradigm. The designers sought to add more cogni-
tive functionality to the Reactive Paradigm. As such, managerial and state-
hierarchy styles are more bottom-up in ﬂavor, emphasizing behaviors or
skills as the basic building blocks. However, a new inﬂux of researchers
from the traditional AI community, particularly Stanford and SRI, has en-

278
7
The Hybrid Deliberative/Reactive Paradigm
tered robotics and made a distinct contribution. Their architectures have a
more top-down, symbolic ﬂavor than managerial and state-hierarchies. One
hallmark of these architectures is that they concentrate symbolic manipu-
lation around a global world model. However, unlike most other Hybrid
architectures, which create a global world model in parallel with behavior-
speciﬁc sensing, this global world model also serves to supply perception to
the behaviors (or behavior equivalents). In this case, the global world model
serves as a virtual sensor.
The use of a single global world model for sensing appears to be a throw-
back to the Hierarchical Paradigm, and conceptually it is. However, there
are four practical differences. First, the monolithic global world model is
often less ambitious in scope and more cleverly organized than earlier sys-
tems. The world model is often only interested in labeling regions of the
sensed world with symbols such as: hallway, door, my ofﬁce, etc. Second,
perceptual processing is often done with distributed processing, so that slow
perceptual routines run asynchronously of faster routines, and the behav-
iors have access to the latest information. In effect, the “eavesdropping”
on perception for behaviors is an equivalent form of distributed processing.
Third, sensor errors and uncertainty can be ﬁltered using sensor fusion over
time. This can dramatically improve the performance of the robot. Fourth,
increases in processor speeds and optimizing compilers have mitigated the
processing bottleneck.
Two of the best known model-oriented architectures are the Saphira archi-
tecture developed by Kurt Konolige with numerous others at SRI, and the
Task Control Architecture (TCA) by Reid Simmons which has been extended
to do multi-task planning with the Prodigy system. The Saphira architecture
comes with the ActivMedia Pioneer robots.
7.6.1
Saphira
The Saphira architecture, shown in Fig. 7.9, has been used at SRI on a vari-
SAPHIRA
ety of robots, including Shakey’s direct descendents: Flakey and Erratic. The
motivation for the architecture stems from the basic tenet that there are three
keys to a mobile robot operating successfully in the open world: coordina-
COORDINATION,
COHERENCE,
COMMUNICATION
tion, coherence, and communication.77 A robot must coordinate its actuators
and sensors (as has been seen through in the Reactive Paradigm), but it must
also coordinate its goals over a period of time (which is not addressed by the
Reactive Paradigm). Whereas the motivation for coordination is compatible
with reactivity, coherence is an explicit break from the Reactive Paradigm.

7.6
Model-Oriented Architectures
279
        Surface       
Construction   
        Object     
Recognition
People-
Tracking
PRS-Lite
(planning agent)
Topological
Planner    
Navigation
Tasks     
Local Perceptual
Space(LPS)    
Localization,     
Map Maintenance
Reactive  
Behaviors
Software
Agents 
 
Virtual      
Sensors     
Actuators
Sensors
Reactive layer
Deliberative Layer
Resource Manager,
Mission Planner,
Performance Monitoring Agent
Cartographer
Sequencer
Fuzzy
Logic
Figure 7.9
Simplifed view of Saphira architecture.
Coherence is essentially the ability of the robot to maintain global world
worlds, which Konolige and Myers argue is essential for good behavioral
performance and interacting with humans.77 Finally, communication is im-
portant because robots have to interact with humans and, as will be seen in
Ch. 8, other robots. This introduces the problem of having common frames
of reference (anchors) for communicating.
The bulk of the architecture is concerned with planning, and uses a type
of reactive planner called PRS-lite for Procedural Reasoning System-lite.77
PRS-LITE
PRS-lite is capable of taking natural language voice commands from humans
(“deliver this to John”) and then operationalizing that into navigation tasks
and perceptual recognition routines. Both planning and execution relies on
the Local Perceptual Space, the central world model. Much processing is

280
7
The Hybrid Deliberative/Reactive Paradigm
devoted to maintaining an accurate model of the world based on the robot’s
sensors and assigning symbolic labels to regions. Saphira also divides the
deliberation activities among software agents. This provides a high degree
of ﬂexibility. Since software agents are independent, they don’t even have to
run on-board the robot they’re controlling. In the 1996 AAAI Mobile Robot
Competition,78 robots running Saphira actually had some of their planning
reside on a local workstation, transmitted through a radio link.62 This will be
covered in more detail in Ch. 8.
The reactive component of Saphira consists of behaviors. The behaviors
extract virtual sensor inputs from the central world model, the Local Percep-
VIRTUAL SENSOR
tual Space. The behavioral output is fuzzy rules, which are fused using fuzzy
logic into a velocity and steer command. Fuzzy logic turns out to be a very
natural way of fusing competing demands, and is less ad hoc than Boolean
logic rules (e.g., “if x and y but not z, then turn left”). The behaviors are man-
aged by the plan execution of the planned navigation tasks. The fuzzy logic
mechanism for combining behaviors produces essentially the same results
as a potential ﬁeld methodology, as described by Konolige and Myers. 77 The
Local Perceptual Space can improve the quality of the robot’s overall behav-
ior because it can smooth out sensor errors. Although this central processing
introduces a computational penalty, the increases in processor power and
clock speeds have made the computational costs acceptable.
The table below summarizes Saphira in terms of the common components
and style of emergent behavior:
Saphira
Sequencer Agent
Topological planner, Navigation Tasks
Resource Manager
PRS-lite
Cartographer
LPS
Mission Planner
PRS-lite
Performance Monitoring Agent
PRS-lite
Emergent behavior
Behaviors fused with fuzzy logic
7.6.2
Task Control Architecture (TCA)
Reid Simmon’s Task Control Architecture (TCA) has been used extensively by
TASK CONTROL
ARCHITECTURE (TCA)
robots designed for NASA, including Ambler, Dante (Fig. 7.11), and ser-
vice robots. It is also the intelligence inside Xavier (shown in Fig. 7.11, the
Carnegie Mellon University robot which is accessible over the web). Xavier
has has traveled autonomously over 210 kilometers in the hallways at CMU

7.6
Model-Oriented Architectures
281
Task Scheduling
(PRODIGY)
Path Planning
Navigation
(POMDP)
Obstacle Avoidance
(CVM)
Reactive Layer
Deliberative Layer
Global
World
Models
Sensors
Effectors
Sequencer,
Resource Manager
Cartographer
Mission Planner
Figure 7.10
Layout of the Task Control Architecture.
in response to 30,000 requests by web users to navigate to a particular site.130
TCA is more difﬁcult to evaluate as a Hybrid architecture, in part because
it has more of an operating system ﬂavor than a general purpose architec-
ture. Also, there are no behaviors per se. However, many of the low level
tasks resemble behaviors, and TCA shares the hybrid philosophy of layering
intelligence, and having lower modules fail upwards.
The basic layout of TCA for indoor navigation is presented in a somewhat
different form than usual131 in Fig. 7.10 to provide consistency with the ter-
minology of this chapter. TCA uses dedicated sensing structures such as
evidence grids (see Ch. 11) which can be thought of as a distributed global
world model. Sensor information percolates up through the global models.
The basic task ﬂow is determined by the Task Scheduling Layer, which uses
the Prodigy planner. (A layer is more along the lines of a software agent or
a subsystem in other architectures.) This layer interacts with the user and
determines the goals and order of execution. For example, if the robot is

282
7
The Hybrid Deliberative/Reactive Paradigm
a.
b.
Figure 7.11
Two robots using TCA: a.) Xavier, a RWI robot at Carnegie Mellon Uni-
versity (photograph courtesy of Reid Simmons) and b.) Dante (photograph courtesy
of NASA Ames Research Center).
given several jobs to drop, Prodigy can prioritize and optimize the sched-
ule. Once the current task has been established, the Path Planning layer is
engaged. Navigation is handled by a Partially Observable Markov Decision
Process (POMDP, pronounced “pom D P”) module which determines what
the robot should be looking for, where it is, and where it has been. As with
the relationship between strategic and tactical behaviors in SFX, the Obsta-
cle Avoidance Layer takes the desired heading and adapts it to the obstacles
extracted from the evidence grid virtual sensor. The layer uses a curvature-
CURVATURE-VELOCITY
METHOD
velocity method (CVM) to factor in not only obstacles but how to respond with
a smooth trajectory for the robot’s current velocity.
The table below summarizes TCA in terms of the common components
and style of emergent behavior:
TCA
Sequencer Agent
Navigation Layer
Resource Manager
Navigation Layer
Cartographer
Path-Planning Layer
Mission Planner
Task Scheduling Layer
Performance Monitoring Agent
Navigation, Path-Planning, Task-Scheduling
Emergent behavior
Filtering

7.7
Other Robots in the Hybrid Paradigm
283
7.7
Other Robots in the Hybrid Paradigm
One implicit criterion for evaluating the utility of a Paradigm and deriva-
tive architectures is its popularity. The Hybrid Paradigm describes many of
the robots being developed in laboratories today. It is clearly popular. A
large number of these robots fall into the Hybrid category by virtue of being
“neither ﬁsh nor fowl,” in this case being neither purely Reactive nor Hierar-
chical. Almost every robot has a set of functions that are equivalent to behav-
iors, though several have followed the lead of 3T in calling those functions
by other names (e.g., “skills”) to reduce the connotations of purely reﬂexive
behaviors. Even architectures that started out as Hierarchical, namely Albus’
Real-time Control Architecture1 and Simmons’ Task Control Architecture,131
recast many of their functions into behavioral components.
One of the major inﬂuences on the mobile robot community has been the
DARPA UGV Demo II and Demo III projects, which forwarded the state of
the art in outdoor ground vehicle control and navigation. The tasks were
well-suited for a hybrid approach: the idea was to give a HMMWV military
jeep a map, a set of directions, and send it off. It would autonomously plan
a route, drive along roads, then go off roads as needed. In a military recon-
naissance mode, the HMMWV was expected to autonomously stay behind
hills, avoid trees and rocks, and covertly position itself so that Reconnais-
sance, Surveillance, and Target Acquisition (RSTA, pronounced “rist-tah”)
sensors could peek over and ﬁnd enemy locations. Demo II even had multi-
ple HMMWVs traveling autonomously in formation.
The European Community ESPRIT agency has also signiﬁcantly inﬂuenced
mobile robotics, sponsoring research in automating highway vehicles via ini-
tiatives in the 1990’s. The United States has shown some interest through
several iterations of “intelligent vehicle highway systems” programs. The
most notable of these automated vehicles were constructed by Dickmanns
and Graefe in Germany45 and the CMU Navlab led by Dean Pomerleau. 116
A third inﬂuence on mobile robots has been NASA’s push for autono-
mous planetary rovers, which culminated in the Mars Pathﬁnder mission
with the Sojourner rover. Mapping a planet’s surface is intrinsically a delib-
erative function, as is planning paths for optimal exploration. Unfortunately,
from the Viking lander missions, Mars was known to be very rocky. The ap-
proaches quickly fell into camps. One, led by MIT, favored deploying many
small, legged robots, such as Genghis, that would climb over small rocks and
go around others. The “low-to-the-ground” viewpoint, which limits what
a robot—and therefore, the scientists on Earth—can sense, would be over-

284
7
The Hybrid Deliberative/Reactive Paradigm
come by the large number of robots. The other approach, characterized by
Carnegie Mellon University’s Ambler robot, proposed a single large robot
with a higher viewpoint and stability. Ambler was built by “Red” Whitaker
at the CMU Field Robotics Institute to be able to maintain a sensor platform
at a level height by stepping over the majority of rocks, but at a tremendous
penalty in size, weight, and power. In the end, planetary rover researchers
have gravitated towards wheeled vehicles with some type of articulation to
maintain stability, such as seen with Sandia National Laboratories’ Rattler.
An extension of the Ambler design philosophy was manifested in the Dante
robots. These were built to rappel down steep canyons and volcanoes on
Mars (and Earth). Dante was able to lower itself successfully most of the way
into a volcano in Antarctica, but could not climb back out. It was dropped
while being lifted out by a helicopter, twisting its frame.
7.8
Evaluation of Hybrid Architectures
In some regards, it is difﬁcult to evaluate Hybrid architectures individually.
Each architecture is still evolving and the deliberative component is being
expanded practically daily. Returning to the four criteria set forth in the
overview in Part I, it is interesting to evaluate the architectures as a whole.
In terms of support for modularity, each architecture is highly modular.
Most are divided into layers, which are then subdivided into modules. As
the software agent programming style for AI gains in popularity, probably
more architectures will implement deliberative modules as independent spe-
cialists. AuRA and SFX clearly exhibit an organization which lends itself to
object-oriented programming. The use of specialists also enhances the ease
of portability.
Hybrids tend to have a high degree of niche targetability. The addition of
the deliberative component allows Hybrids to be used for applications not
appropriate for purely Reactive systems. However, the partitioning between
reaction and deliberation allows the reactive portion to be used alone for
purely reactive applications.
Another attractive aspect of Hybrid architectures is that they often explic-
itly attempt to ensure robustness. In the SFX and 3T architecture, modules
within the various deliberative components attempt to monitor the perfor-
mance of the reactive behaviors and either replace or adapt the conﬁguration
as needed.

7.9
Interleaving Deliberation and Reactive Control
285
An evaluation of Hybrid architectures would not be complete without ask-
ing the question, “so what’s the difference between what Shakey could do (if
it had had inﬁnite computation power) and what a Hybrid can do?” Speed of
execution aside, it is interesting to ponder whether the only fundamental dif-
ference is how Hierarchical and Hybrids achieve the same ends. Hybrids cer-
tainly explicitly reﬂect more of the principles of software engineering (mod-
ularity, coherence, design for reuse, etc.). The two paradigms also certainly
reﬂect different attitudes toward world modeling. In the Hybrid Paradigm,
global models are used only for symbolic functions. The frame problem ei-
ther doesn’t exist or is minor because 1) execution is reactive and therefore
well-suited for unstructured environments, and 2) software agents can use
agent-speciﬁc abstractions to exploit the structure of an environment in or-
der to fulﬁll their particular role in deliberation. Global models are generally
closed world, but the world is “closed” at the deliberative level. The robot
can think in terms of a closed world, while it acts in an open world. Another
major philosophical difference is the role of planning and execution. Under
STRIPS, Shakey planned every move, down to the lowest level of granu-
larity, and had problems conﬁrming that an action had been accomplished.
It should be noted that modern planners often produce only partial plans,
then execute that part of the plan, note the results and plan the next step.
It is not clear whether a SENSE, PLAN, ACT paradigm with a more suit-
able planner would produce a more intuitively appealing robot architecture.
The ﬁnal distinction between the Hierarchical and Hybrid Paradigms may
be the inﬂuence of biology. The Hybrid Paradigm has its roots in ethology,
and provides a framework for exploring cognitive science. The Hierarchical
Paradigm is less clearly cognitively plausible, although both share the same
cognitive motivation.
7.9
Interleaving Deliberation and Reactive Control
The primary contribution of a Hybrid architecture is to provide a template
for merging deliberation and reaction. However, interleaving the two func-
tions is often task dependent. In navigation, there is the issue of interleaving
path planning and reactive execution of the path. Another issue is monitor-
ing the progress of behaviors and terminating behaviors correctly.
One of the obvious task domains for a mobile robot is navigation, simply
getting around in the world. Many robot architectures have well-developed
modules for handling navigational tasks.
In the early 1990’s, navigation

286
7
The Hybrid Deliberative/Reactive Paradigm
served as an excellent example of why deliberation and reaction were largely
separate, and reinforced a “plan once, execute lots” mentality. However, ad-
vances in technology and new domains are causing researchers to rethink
the strict separation.
By the early 1990’s, many algorithms existed which if given a two or even
three dimensional map could compute an optimal path for a robot to take
through the world. These path planning algorithms exhibited several draw-
backs. One was that they were all computationally expensive, taking be-
tween 5 and 15 minutes to execute. This prevented the robot from continu-
ously generating the path. But as was seen with Shakey, if the robot tried to
execute a pre-computed path, it would be vulnerable to unexpected changes
in the world. That map would represent the best, currently available know-
ledge about the world: where obstacles were, unpassable terrain, etc. But
a map is at best a representation of a closed world; it can’t show what has
changed since it was built, unless a robot goes there and senses it. So the
robot can generate an optimal path, but it may discover an unmodeled ob-
stacle blocking that path. If the robot was just using the precomputed path to
navigate, it would have to stop, update the map with the newly discovered
obstacle, then replan the optimal path, resulting in slow progress.
If the robot used reactive behaviors to navigate, there was no guarantee
that it would make it to the goal any faster. Even if it headed off in the right
direction, it might spend time getting into and out of box canyons. By letting
the robot solely react to the world, it might make poor choices.
The common solution was to mix path planning and reaction by having
a planning algorithm in the cartographer generate a complete optimal route
for the robot to take, but then decompose that route into segments, usually
straight lines. The end of each path segment is called a waypoint, because
WAYPOINT
it is a point along the way to the ﬁnal goal. Each waypoint can be a goal
to reach, which can be accomplished by behaviors (move-to-goal(), avoid-
obstacles()). When the robot reaches the ﬁrst goal, the sequencer agent can
give the behavioral manager the coordinates or landmarks for the next goal,
and so on. Notice this allows computationally expensive path planning to
occur only once, and then the behaviors take care of actually navigating.
As will be seen in Part II, this strategy has its own set of drawbacks. But
for the most part it is a well-accepted means of partitioning deliberation and
reaction in navigation. However, with the advent of the Intel Pentium and
Sparc microprocessors, the computational demands of path planning algo-
rithms have been mitigated. Algorithms which ran on the order of once
every 10 minutes can now execute once every second. As a result, the mis-

7.9
Interleaving Deliberation and Reactive Control
287
matches in planning and reaction times are no longer a compelling reason to
enforce a strict separation of deliberation and reaction. However, the soft-
ware engineering reason for the partition remains: things which operate on
symbols and global information should be in the deliberator; things which
operate directly on sensors and actuators should be in the reactor.
The above example of Navigation illustrates a top-down interleaving of
deliberation and reaction. The deliberative layer(s) decompose the mission
into ﬁner and ﬁner steps until it arrives at a set of behaviors capable of ac-
complishing the ﬁrst subgoal. Another example of interleaving deliberation
and reaction is the role of the deliberative layer(s) in supplying the reactor
with expectations and virtual sensors. And it is important to note that delib-
eration can be triggered bottom-up as well.
It is not always the case that the deliberator generates the set of behaviors,
turns them on, and lets them execute until they either complete the subtask
or fail. In the case of sensing, it might be desirable for the deliberator to make
some of the global models being constructed available to the behaviors. For
example, consider a robot vehicle following a road. Now suppose that it is
looking for the large tree, since it is supposed to turn right at the intersec-
tion after the tree. In order to maintain the correct sequence of behaviors,
the deliberator has a global world model where trees are noted. The reactive
behaviors of following a road and avoid an obstacle do not need the explicit
representation of a tree. But what if the tree casts a shadow which might con-
fuse the follow-path behavior and cause it to give the robot incorrect steering
commands? In this case, it would be useful if the information about the pres-
ence of a tree could be absorbed by the behavior. One way to do this is to
permit methods on the world model to act as virtual sensors or perceptual
schema. Then the follow-road behavior could use the virtual sensor, which
tells the behavior when the road boundaries extracted by the vision sensor
are probably distorted, to ignore affected regions in the image that would
normally be observed by the vision sensor. This is an example of how the
deliberative layer can aid with selective attention, or ﬁltering of perception for
SELECTIVE ATTENTION
a context.
But the reactor may wish to trigger deliberation under many circumstances.
Most of these cases involve failure or exception handling. Suppose that a
sensor breaks and the perceptual schema cannot produce a percept. The
schema cannot diagnose itself. Therefore, its failure would cause a delib-
erative function, such as the Sensing Manager in SFX, to be triggered. If the
Sensing Manager cannot ﬁnd a replacement perceptual schema or equivalent
behavior, then it can no longer meet the constraints imposed by the Mission

288
7
The Hybrid Deliberative/Reactive Paradigm
Planner. In the SFX system, the Sensing Manager would trigger the Mission
Planner. The Mission Planner presumably has the intelligence to relax con-
straints on the mission, or in practice to alert the human supervisor. This
is an example of “failing upwards,” where a lower level module fails and a
higher level, or smarter, module has to take over.
Failing upwards is also known as a cognizant failure, meaning that the fail-
COGNIZANT FAILURE
ure takes some amount of awareness of the context to resolve, not that the
cognition itself has failed. Erann Gat motivated the need for cognizant fail-
ures through the “Wesson Oil” problem.57 In old Wesson Oil TV advertise-
ments, a mother would be cooking chicken in oil when she would have to
stop to rush her son to the emergency room for treatment of a broken arm.
Ideally the mother would not only turn off the stove but remove the chicken
from the oil. In the commercial, the mother neglects to remove the chicken.
Since she is using Wesson Oil, the chicken isn’t ruined when they return.
The point of the Wesson Oil problem for robotics is what happens when a
set of reactive behaviors are turned off in the middle of a sequence? In many
cases, it may not be just a simple act of de-instantiating the behaviors and
activating new ones. There may be behaviors in the sequence that have to
execute or disaster will occur (turn off the stove) as well behaviors which
must run or lead to merely undesirable side effects (chicken sits in oil if it is
not removed). All of this means that the sequencer must know why a failure
or need to change behaviors has occurred and what the intent of the current
sequence is. Deliberation is deﬁnitely not trivial!
7.10
Summary
The Hybrid Deliberative-Reactive Paradigm can be thought of as PLAN, then
SENSE-ACT (P,S-A). The SENSE-ACT portion is always done with reactive
behaviors, while PLAN includes a broader range of intelligent activities than
just path or mission planning. Planning can be interleaved with execution;
then for the most part, the robot plans a set of behaviors which will exe-
cute for a long time. Sensing is also hybrid; the reactive portion uses local,
behavior-speciﬁc representations while the deliberative portion uses global
world models.
Architectures in the Hybrid paradigm usually encapsulate functionality
into modules. The basic modules are the mission planner, sequencer agent, be-
havioral manager, cartographer, and performance monitor. Hybrid architectures
always have portions which are recognizably deliberative and reactive. The

7.11
Exercises
289
rule of thumb is that functions which operate on symbolic information go in
the deliberative layer, or deliberator, while functions which transform sensor
data into actuator commands go in the reactive layer, or reactor. The reactive
component is organized by behaviors, but the deﬁnition of behavior is wider
than the purely reﬂexive behaviors in the Reactive Paradigm. Behaviors may
be called skills or by other names to eliminate confusion with reﬂexive be-
haviors. Hybrid architectures also exhibit a wide variety of mechanisms for
combining behaviors, including subsumption, potential ﬁeld combination,
ﬁltering, voting, and fuzzy logic. The reactive portion tends to make more
use of assemblages of behaviors than in the Reactive Paradigm.
The deliberative component is often subdivided into layers. The layering
may reﬂect scope in managerial responsibilities or in time horizon. Layers
often consist of modules implemented as software agents. These agents may
have agent-speciﬁc world models or share a single global world model. The
world model can also serve as a virtual sensor for use by behaviors.
Managerial styles of architectures favor a bottom-up organization, concen-
trating on layering planning on top of behaviors and behavior management.
Deliberation is based on whether symbols are being manipulated. The over-
all behavior emerges as a function of the behavioral layer. State-hierarchies
divide deliberation and reaction by the state, or scope of knowledge, avail-
able to the modules or agents operating at that layer. The three states are
Past, Present, Future. Responsibilities are organized into planning and se-
quencing components. The overall emergent behavior is more due to the se-
quencing of behaviors, or skills, rather than concurrency. Model-based styles
favor a top-down organization, focusing on the creation and maintenance of
a global world model. The world model then serves as a virtual sensor to
the behaviors and as a data structure for planning. Often the software agents
within the deliberative layer in all three architectural styles exhibit self mon-
itoring and problem solving abilities, and fail upwards if the mission cannot
be accomplished. As such, they distribute planning and problem-solving.
7.11
Exercises
Exercise 7.1
Describe the Hybrid paradigm in terms of:
a. sensing, acting, and planning, and
b. sensing organization.

290
7
The Hybrid Deliberative/Reactive Paradigm
Exercise 7.2
Decide whether each of the following is deliberative or behavioral: path planning,
resource allocation, map making, avoiding obstacles, navigation.
Exercise 7.3
Name and evaluate the following architectures in terms of support for modularity,
niche targetability, ease of portability to other domains, robustness.
a. AuRA
b. SFX
c. 3T
d. Saphira
e. TCA
Exercise 7.4
Why do behavioral management and performance monitoring require global know-
ledge?
Exercise 7.5
How does the Hybrid Paradigm address the frame problem and the open world?
Exercise 7.6
List the ﬁve basic components of a Hybrid architecture.
Exercise 7.7
Describe the difference between managerial, state hierarchy, and model-oriented styles
of Hybrid architectures and give one example of each.
Exercise 7.8
Describe the use of state (Past, Present, Future) to deﬁne behaviors and deliberative
responsibilities in a state-hierarchy architecture.
Exercise 7.9
[Advanced Reading]
Search for technical papers on Cog, Rodney Brooks’ controversial humanoid robot
project. What paradigm best describes the architecture and why?
Exercise 7.10
[Advanced Reading]
Look up technical reports on Shakey. Compare Shakey with the Hybrid architectures.
Now consider the possible impact of the radical increases in processing power since
the 1960’s. Do you agree or disagree with the statement that Shakey would be as
capable as any Hybrid if it were built today? Justify your answer. (This question was
posed by Tom Garvey of SRI.)

7.12
End Notes
291
Exercise 7.11
[Advanced Reading]
Read more about the Saphira architecture in “The Saphira Architecture for Autono-
mous Mobile Robots” by Kurt Konolige and Karen Myers in Artiﬁcial Intelligence and
Mobile Robots. 77 They describe the system built for a demonstration for the TV special,
Scientiﬁc American Frontiers. Summarize the behaviors and agents used. How would
the same tasks be accomplished using AuRA, SFX, 3T, or TCA?
7.12
End Notes
For the roboticist’s bookshelf.
Behavior-Based Robotics by Ron Arkin10 is the most complete work on AI robotics.
It has a comprehensive list of inﬂuential robot architectures, and explores in detail
many of the issues only lightly touched upon in this book. Well worth reading and
using as a reference.
About Saphira and Pioneer robots.
So why did RWI/ActivMedia choose the Saphira architecture for their Pioneer line?
Because the Pioneer robot is based on a robot designed by a class taught at Stanford
by Kurt Konolige, one of the architects of Saphira.
Foam-henge and the importance of architectures.
One criticism that applies to many of the Hybrid architectures is that they have
moved away from subsumption or vector summation to more open-ended meth-
ods such as voting, rule-based arbitration, fuzzy logic, etc. There are deﬁnitely cases
where there is a single best action to take. For example, consider what happened at a
1995 demonstration of the UGV Demo II program. The demonstration took place at
the Martin Marietta (now Lockheed Martin) facility near Denver, Colorado. The Mar-
tin team was responsible for integrating various research contracts into a form usable
by the HMMWV and for hosting demos. To help with the demo, they had several
very large foam boulders created for the demo by a California special effects com-
pany. That way, the team could arrange the boulders in challenging positions to test
the Demo II vehicles (versus trying to move real boulders). The boulders eventually
became nicknamed “Foam-henge” as a play on Stonehenge.
The demonstration involving Foam-henge started with the audience standing at
the rise of a canyon. Foam-henge was placed in the middle of the predicted path for
an autonomous HMMWV. This was intended to be a test of its ability to detect and
respond to unmodeled obstacles. The audience watched as a HMMWV navigating
slowly, but autonomously, out of the canyon. The vehicle approached Foam-henge,
then stopped, clearly detecting the boulder nearest it. Still stopped, the front wheels
of the robot turned hard to the right. The wheels then turned hard to the left. Just as
quickly, the wheels returned to the straight position, and the HMMWV accelerated

292
7
The Hybrid Deliberative/Reactive Paradigm
straight into the boulder! In this case, the behavior arbiter (a precursor of DAMN) 121
had apparently viewed the choices between avoiding to the right and left as equal
and thereby cancelling each other out.
About “Red” Whitaker.
William Whitaker used to have red hair, hence the nickname. Whitaker has had an
impressive career focusing on building the hardware for industrial applications of
mobile robots, including the robots used to clean up Chernobyl (through the com-
pany RedZone Robotics) and Three-Mile Island. Typically, the robots are teleoperated
or contain software developed by other CMU scientists such as Reid Simmons and
Tony Stentz.
Dante and the media.
The Dante project received heavy media coverage, and percolated into the entertain-
ment industry. A small, Dante-like robot appears in the movie, Dante’s Peak, as part
of the team of vulcanologists led by Pierce Brosnan. The robot is continually breaking
in the movie, which is unfortunately an all-too-accurate reﬂection of the current state
of robotics. In the “Firewalker” episode of the TV series The X-Files, a robot looking
like an Odetics “spider” robot is down in a volcano and broadcasts disturbing images
implying that the vulcanologists have been attacked by a mysterious life form. The
culprit turned out to be a scientist who had quit taking his medication.
EVAHR.
The EVAHR project was cancelled in the mid 1990’s. It was envisioned as a jet-pack
with arms to help astronauts construct the space station. The heuristic is that less
than 20% of the research conducted by NASA will actually be integrated into a ﬂight
package.

8
Multi-agents
Chapter Objectives:
 Deﬁne the types of control regimes, cooperation strategies, and goals in multi-
agents.
 Given a description of an intended task, a collection of robots, and the
permitted interactions between robots, design a multi-agent system and
describe the system in terms of heterogeneity, control, cooperation, and
goals.
 Compute the social entropy of a team.
 Be able to program a set of homogeneous reactive robots to accomplish a
foraging task.
 Describe use of social rules and internal motivation for emergent societal
behavior.
8.1
Overview
This chapter explores artiﬁcial intelligence methods for coordinating and
controlling collections of mobile robots working on completing a task. Col-
lections of two or more mobile robots working together are often referred to
as teams or societies of multiple mobile robots, or more concisely multi-agents.
SOCIETIES
MULTI-AGENTS
Multi-agent teams are desirable for many reasons. In the case of planetary
explorers or removing land mines, more robots should be able to cover more
area. Like ants and other insects, many cheap robots working together could
replace a single expensive robot, making multi-agents more cost effective.
Indeed, the term swarm robots is becoming popular to refer to large numbers
SWARM ROBOTS

294
8
Multi-agents
of robots working on a single task. Another motivation of multiple robots is
redundancy: if one robot fails or is destroyed, the other robots can continue
and complete the job, though perhaps not as quickly or as efﬁciently. Rodney
Brooks at MIT ﬁrst proposed to NASA that teams of hundreds of inexpensive
ant-like reactive robots be sent to Mars in a technical report entitled “Fast,
Cheap and Out of Control”30 in part because having many robots meant that
several robots could be destroyed in transit or during landing without a real
impact on the overall mission.
Multi-agent teams are becoming quite popular in robot competitions, espe-
cially two international robot soccer competitions: RoboCup and MIROSOT.
In these competitions, teams of real or simulated robots play soccer against
other teams. The soccer task explicitly requires multiple robots that must
cooperate with each other, yet react as individuals.
Readers with a strong background in artiﬁcial intelligence may notice sim-
ilarities between teams of mobile robots and teams of software agents (“we-
bots” which search the web and “knowbots” which do data mining). Those
similarities are not accidental; software and physical agents fall into a re-
search area in Artiﬁcial Intelligence often referred to as Distributed Artiﬁcial
DISTRIBUTED
ARTIFICIAL
INTELLIGENCE (DAI)
Intelligence (DAI). Most of the issues in organizing teams of robots apply to
software agents as well. Arkin,10 Bond and Gasser,119 Brooks,26 and Oliveira
et al.113 all cite the problems with teams of multiple agents, condensed here
as:
 Designing teams is hard. How does a designer recognize the characteristics
of a problem that make it suitable for multi-agents? How does the de-
signer (or the agents themselves) divide up the task? Are there any tools
to predict and verify the social behavior?
 There is a “too many cooks spoil the broth” effect. Having more robots work-
ing on a task or in a team increases the possibility that individual robots
with unintentionally interfere with each other, lowering the overall pro-
INTERFERENCE
ductivity.
 It is hard for a team to recognize when it, or members, are unproductive. One
solution to the “too many cooks spoil the broth” problem is to try engi-
neering the team so that interference cannot happen. But this may not be
possible for every type of team or the vagaries of the open world may un-
dermine that engineering. To defend itself, the team should be capable of
monitoring itself to make sure it is productive. This in turn returns to the
issue of communication.

8.1
Overview
295
 It is not clear when communication is needed between agents, and what to say.
Many animals operate in ﬂocks, maintaining formation without explicit
communication (e.g., songs in birds, signals like a deer raising its tail to
display white, speaking). Formation control is often done simply by per-
ceiving the proximity to or actions of other agents; for example, school-
ing ﬁsh try to remain equally close to ﬁsh on either side. But robots and
modern telecommunications technology make it possible for all agents
in a team to literally know whatever is in the mind of the other robots,
though at a computational and hardware cost. How can this unparalleled
ability be exploited? What happens if the telecommunications link goes
bad? Cell phones aren’t 100% reliable, even though there is tremendous
consumer pressure on cell phones, so it is safe to assume that robot com-
munications will be less reliable. Is there a language for multi-agents that
can abstract the important information and minimize explicit communi-
cation?
 The “right” level of individuality and autonomy is usually not obvious in a prob-
lem domain. Agents with a high degree of individual autonomy may cre-
ate more interference with the group goals, even to the point of seeming
“autistic.”113 But agents with more autonomy may be better able to deal
with the open world.
The ﬁrst question in the above list essentially asks what are the architectures
for multi-agents? The answer to that question at this time is unclear. Individ-
ual members of multi-agent teams are usually programmed with behaviors,
following either the Reactive (Ch. 4) or Hybrid Deliberative/Reactive (Ch. 7)
paradigms. Recall that under the Reactive Paradigm, the multiple behaviors
acting concurrently in a robot led to an emergent behavior. For example, a ro-
bot might respond to a set of obstacles in a way not explicitly programmed
in. Likewise in multi-agents, the concurrent but independent actions of each
robot leads to an emergent social behavior. The group behavior can be different
EMERGENT SOCIAL
BEHAVIOR
from the individual behavior, emulating “group dynamics” or possibly “mob
psychology.” As will be seen in this chapter, fairly complex team actions such
as ﬂocking or forming a line to go through a door emerge naturally from re-
active robots with little or no communication between each other. But as
with emergent behavior in individual robots, emergent social behavior is of-
ten hard to predict. Complete architectures for designing teams of robots are
still under development; Lynne Parker’s ALLIANCE architecture114 is possi-
bly the most comprehensive system to date. The whole ﬁeld of multi-agents
is so new that there is no consensus on what are the important dimensions,

296
8
Multi-agents
or characteristics, in describing a team. For the purposes of this chapter,
heterogeneity, control, cooperation, and goals will be used as the dimensions.117
8.2
Heterogeneity
Heterogeneity refers to the degree of similarity between individual robots that
HETEROGENEITY
are within a collection. Collections of robots are characterized as being ei-
ther heterogeneous or homogeneous. Heterogeneous teams have at least two
HETEROGENEOUS
TEAMS
members with different hardware or software capabilities, while in homoge-
HOMOGENEOUS TEAMS
neous teams the members are all identical. To make matter more confusing,
members can be homogeneous for one portion of a task by running identi-
cal behaviors, then become heterogeneous if the team members change the
behavioral mix or tasks.
8.2.1
Homogeneous teams and swarms
Most multi-agent teams are homogeneous swarms. Each robot is identical,
which simpliﬁes both the manufacturing cost and the programming. The
biological model for these teams are often ants or other insects which have
large numbers of identical members. As such, swarms favor a purely reactive
approach, where each robot operates under the Reactive Paradigm. Insect
swarms have been modeled and mimicked since the 1980’s. The proceedings
of the annual conference on the Simulation of Adaptive Behavior (also called
“From Animals to Animats”) is an excellent starting point.
An example of a successful team of homogeneous robots is Ganymede, Io,
and Callisto ﬁelded by Georgia Tech. These three robots won ﬁrst place in the
“Pick Up the Trash” event of the 1994 AAAI Mobile Robot Competition,129
also discussed in Ch. 5. Recall that the objective of that event was to pick up
the most trash (coca-cola cans) and deposit it in a refuse area. The majority
of the entries used a single agent, concentrating on model-based vision for
recognizing trash, cans, and bins and on complex grippers.
The three identical robots entered by Georgia Tech were simple, both phys-
ically and computationally, and are described in detail in a 1995 AI Magazine
article.19 The robots are shown in Fig. 8.1, and were constructed from an In-
tel 386 PC motherboard mounted on a radio-controlled toy tracked vehicle.
The robots had a miniature wide-angle video camera and framegrabber. The
ﬂapper-style grippers had an IR to indicate when something was in the grip-
per. The robots also had a bump sensor in front for collisions. The robots
were painted ﬂuorescent green.

8.2
Heterogeneity
297
Each robot was programmed with a sequence of simple reactive behav-
iors (renamed here for clarity), following the reactive layer of Arkin’s AuRA
architecture described in Ch. 4:
wander-for-goal This behavior was instantiated for two goals: trash and
trashcan. The motor schema was a random potential ﬁeld, the perceptual
schema was color blob detection, where trash=“red” and trashcan=“blue.”
move-to-goal This behavior also had two different goals: trash and trashcan.
The motor schema was an attractive potential ﬁeld, and the perceptual
schema for the trash and trashcan were the same as in the wander-for-
goal.
avoid-obstacle This behavior used the bump switch as the perceptual sche-
ma, and a repulsive ﬁeld as the motor schema.
avoid-other-robots The three robots did not communicate with each other,
instead using only the repulsive ﬁeld created by avoid-other-robots to re-
duce interference. The motor schema was a repulsive potential ﬁeld (lin-
ear dropoff), while the perceptual schema detected “green.”
grab-trash The robot would move toward the trash until the perceptual sche-
ma reported that the IR beam on the gripper was broken; the motor schema
would close the gripper and back up the robot.
drop-trash When the robot reached the trashcan with trash in its gripper, the
motor schema would open the gripper and back up the robot, and turn 90
degrees.
8.2.2
Heterogeneous teams
A new trend in multi-agents is heterogeneous teams. A common heteroge-
neous team arrangement is to have one team member with more expensive
computer processing. That robot serves as the team leader and can direct the
other, less intelligent robots, or it can be used for special situations. The dan-
ger is that the specialist robot will fail or be destroyed, preventing the team
mission from being accomplished.
One interesting combination of vehicle types is autonomous air and ground
vehicles. Researchers as the University of Southern California under the di-
rection of George Bekey have been working on the coordination of teams
of ground robots searching an area based on feedback from an autonomous

298
8
Multi-agents
Figure 8.1
Georgia Tech’s winning robot team for the 1994 AAAI Mobile Robot
Competition, Pick Up the Trash event. (Photograph courtesy of Tucker Balch and
AAAI.)
miniature helicopter. This combination permits the team to send a human
observer a comprehensive view of a particular site, such as a hostage situa-
tion.
A special case of a cooperative, heterogeneous team of robots has been
dubbed marsupial robots. The motivation for marsupial robots stemmed from
concerns about deploying micro-rovers for applications such as Urban Search
and Rescue.
Micro-rovers often have limited battery power, which they
can’t afford to spend just traveling to a site. Likewise, micro-rovers may
not be able carry much on-board processing power and need to have an-
other, more computationally powerful workstation do proxy (remote) pro-
cessing. A marsupial team consists of a large robot which carries one or
more smaller robots to the task site, much like a kangaroo mother carries
a joey in her pouch. Like a joey, the daughter robot is better protected in
the pouch and can conserve energy or be recharged during transport. The
mother can protect a delicate mechanism or sensor from collisions while it
navigates through an irregular void. The mother can also carry a payload
of batteries to recharge (feed) the daughter. It can serve as a proxy worksta-
tion, moving to maintain communications. The mother is likely to be a larger
robot, while the daughter might be a micro-rover with sensors very close to
the ground. The mother will have a better viewpoint and sensors, so in some
circumstances it can communicate advice to the smaller daughter to help it
cope with a “mouse’s eye” view of the world. A teleoperator can also control

8.2
Heterogeneity
299
a.
b.
Figure 8.2
Two views of a marsupial robot team at University of South Florida. a.)
Silver Bullet is the “mother” connected by an umbilical tether to a tracked chemical
inspection robot Bujold, the “daughter.” b.) Bujold exits from the rear of the jeep.
(Photographs by Tom Wagner.)
the daughter more easily in some situations by looking through the mother’s
camera.
At this time, there appear to be only two physically realized implemen-
tations of autonomous marsupials: the University of South Florida teams,
one of which is shown in Fig. 8.2, and the robots at the US Department of
Energy’s Idaho National Energy and Engineering Laboratory (INEEL).3 The
USF team is the only one where a mother robot carries a micro-rover inside
the structure to protect it. The Mars Pathﬁnder mission is similar to a mar-
supial robot in that a micro-rover was transported to a mission site and the
transport vehicle served as a support mechanism. However, our deﬁnition
of marsupial assumes the mother is a fully mobile agent and can recover and
retask the micro-rover.

300
8
Multi-agents
8.2.3
Social entropy
The above examples show how different heterogeneous teams can be. One
rough measure of the degree of heterogeneity is the social entropy metric cre-
SOCIAL ENTROPY
ated by Tucker Balch.16 (Entropy is a measure of disorder in a system, espe-
cially in the sense of the Third Law of Thermodynamics. It was also adapted
by Shannon for use in information theory to quantify the amount or quality
of information in a system.) The point of social entropy is to assign a numeri-
cal value for rating diversity (or disorder) in a team. The number should be 0
if all team members are the same (homogeneous). The number should have
the maximum value if all the team members are different. The number of
team members which are different should make the overall number higher.
To compute social entropy, consider a marsupial team
R with a mother
robot and three identical (hardware and software) micro-rovers. The formula
for the social entropy,
H
et(R), is:
H
et(R)
=
 c
X
i=
p
i
log

(p
i
)
(8.1)
There are two types of robots in the team, called castes or
c: the mother and
CASTES
the daughters. Therefore
c
=
. The term
p
i is the decimal percent of robots
belonging to caste
c
i. If
i
=
 for the mother, and
i
=
 for the daughters:
p

=


=
0:
(8.2)
p

=


=
0:
Substituting into Eqn. 8.1 (and remembering that
log

n
=
log
0
n
log
0
 ), the so-
cial entropy is:
H
et(R)
=
 c
X
i=
p
i
log

(p
i
)
(8.3)
=
 (0:
log

0:
+
0:
log

0:)
=
 (( 0:0)
+
( 0:))
=
0:

8.3
Control
301
Now consider a case where the daughters are not identical. Suppose that
one of the three micro-rovers has a different sensor suite and behaviors from
the other two. In that case
c
=
, where
p

=

,
p

=

, and
p

=

.
Substituting into Eqn. 8.1 yields 1.5. Since
:
>
0:, the marsupial team
with the different daughter is more diverse that the marsupial team with all
identical daughters.
8.3
Control
Control of multi-agents can fall in a spectrum bounded by centralized control
CENTRALIZED
CONTROL
and distributed control regimes. In centralized control, the robots communi-
DISTRIBUTED CONTROL
cate with a central computer. The central computer distributes assignments,
goals, etc., to the remote robots. The robots are essentially semi-autonomous,
with the centralized computer playing the role of a teleoperator in a teleop-
erated system. In distributed control, each robot makes its own decisions
and acts independently. Of course, there is a range of regimes between fully
centralized and fully distributed; the robots can interact with a central con-
troller to receive new goals, then operate for the duration of the mission in a
distributed manner.
Examples of full and partial centralized control can be found by compar-
ing the RoboCup and MIROSOT robot soccer competitions. In those soccer
competition events, teams of robots are controlled remotely by a central com-
puter. In the small sized league of RoboCup and MIROSOT, teams of three,
very small self-contained robots (7.5cm x 7.5cm x 7.5cm) play on a 130cm
x 90cm arena with an orange golf ball serving as the miniature soccer ball.
Each robot had a unique pattern of bright colors to make it visible from the
overhead cameras, and the overhead camera is connected to a central pro-
cessor. The robots communicate with the central processor over a radio link.
In MIROSOT, the central processor commands each robot by supplying the
direction to move. In RoboCup, the central processor can give either explicit
directions or just locations of other robots and the ball, letting the robot’s on-
board behaviors generate the (one hopes) correct response. Fig. 8.3 shows a
view of the small-sized league from the 1998 RoboCup World Cup.
MIROSOT robots are more drone-like than their RoboCup counterparts,
since they are not required to carry any on-board sensing. They represent
the extreme of centralized control, where everything must go through a sin-
gle computer, much like the battle-droids in the Star Wars movie, The Phan-
tom Menace. RoboCup robots are required to have some type of on-board

302
8
Multi-agents
a.
b.
c.
Figure 8.3
RoboCup soccer competition: a.) Overhead view of the RoboCup playing
ﬁeld and teams for the small-sized league (overhead camera is not shown), b.) Mid-
sized league, and c.) Legged league using Sony Aibo robots. (Photographs c
The
RoboCup Federation 1999. All rights reserved.)
sensing for reﬂexive obstacle avoidance. In the RoboCup case, the robots
must have a set of basic tactical behaviors (see Ch. 7), but may either receive
strategic commands from the central computer or have on-board strategic
behaviors.
This type of control is conceptually equivalent to the Hybrid
Reactive-Deliberative Paradigm, where the reactive layer physically resides
on the robot and the deliberative layer resides on the central workstation.
Distributed control is more natural for soccer playing than centralized
control, because each player reacts independently to the situation. An ex-
ample of distributed control in robot soccer playing is the mid-sized league
in RoboCup. Notice that in robot soccer the robots are inherently hetero-
geneous.
Although they may be physically the same, each robot is pro-
grammed with a different role, most especially Goalie, Striker, and Defender.

8.4
Cooperation
303
Likewise, there are signiﬁcant advantages to planning and learning strate-
gies, two deliberative functions. Manuela Veloso and Peter Stone have used
RoboCup as a test domain for research in deliberation.
8.4
Cooperation
Cooperation refers to how the robots interact with each other in pursuing a
COOPERATION
goal. Robots can show active cooperation by acknowledging one another and
ACTIVE COOPERATION
working together. Note that this does not necessarily mean the robots com-
municate with each other. For example, in robot soccer, one robot can pass
the ball to another robot as part of an offensive play. The cooperation does
not require communication—if a robot has the ball, can’t see goal and can see
team mate, then it passes to team mate, but this does require being aware of
the teammates.
More often robots are programmed to exhibit non-active cooperation, where-
NON-ACTIVE
COOPERATION
by they individually pursue a goal without acknowledging other robots but
cooperation emerges. The choice of cooperation schemes is often inﬂuenced
by the sensory capabilities of the robots. Active cooperation requires that
robot be able to distinguish its peer robots from other aspects of the envi-
ronment. In the case of the Georgia Tech entry, each robot was covered in
ﬂuorescent green poster paper easily segmented as a color region. If the
robots had not been green, they would have been treated as obstacles to be
avoided. Non-active cooperation has attracted much interest in the robotics
community because it requires very little sensing or behaviors.
It is easy to think of cooperation in terms of robots working together on a
task. Another aspect of cooperation is physical cooperation, where the robots
PHYSICAL
COOPERATION
physically aid each other or interact in similar ways. Marsupial robots are
certainly a type of physical cooperation, especially during deployment and
docking. An even more exciting type of cooperation occurs between reconﬁg-
RECONFIGURABLE
ROBOTS
urable robots. One of the ﬁrst such systems was proposed by Toshio Fukuda,
called CEBOT for “celluar robot system.”31 These are small identical robots
that hook up to form a useful robot. Another aspect of reconﬁgurable robots
is cooperative mobility, where one robot might come over and help another
COOPERATIVE
MOBILITY
robot in trouble. Shigeo Hirose simulated robots which could link up with
each other to gain more stability or traction in rough terrain.67

304
8
Multi-agents
8.5
Goals
The ﬁnal dimension for characterizing a collection of multi-agents is how the
robot works on a goal. If all the robots in the collection work on attaining the
same explicit goal, then they are said to share a single goal, versus having
individual goals.
An example of robots working a single goal is the winning team for the
Ofﬁce Navigation event in the 1996 AAAI Mobile Robot Competition. 78 The
ofﬁce navigation event had a robot that was supposed to search a series of
rooms, ﬁnd an empty conference room, and then go to a list of rooms where
people were and tell them that a meeting was going to begin in the empty
conference room. The event was originally conceptualized as a single agent
task, but the SRI entry under the direction of Kurt Konolige consisted of three
robots.62 Each of the three robots ran the Saphira architecture (see Ch. 7) and
were coordinated by a central workstation. While the robots were responsi-
ble for autonomous navigation, their goals were set by the central strategy
agent. Even though they were navigating through different parts of the ofﬁce
maze, the robots were working on a single goal and the software agents on
the central workstation were explicitly coordinating the actions of the robots.
The robots were able to ﬁnd an empty room and inform the attendees in 4
minutes and 30 seconds. The next best time was close to 10 minutes.
An example of purely reactive robots working on individual goals is a
problem originally posed by Ron Arkin: 9 a group of robotic space “ants”
foraging for mineral rich asteroids or Near Earth Objects (NEOs). If each
robot in the group forages for its own asteroid, then they have individual
goals. (Notice that a behavior that permits them to notice other robots and
be repulsed will help disperse the robots.) If the robots are programmed so
that they will all go to one speciﬁc asteroid, then they share a common goal.
Emergent cooperation is not the same thing as having a single goal. For
example, suppose the robotic space ants are programmed to go to the near-
est non-moving asteroid and bring it back to base. Each robot might have a
set of behaviors: ﬁnd-stationary-asteroid, move-to-asteroid, push-asteroid-
to-home, and avoid-robots. The ﬁnd-stationary-asteroid could be done with
a random potential ﬁeld (in 3 dimensions, of course). An attractive “asteroid-
tropic” potential ﬁeld could be used for the move-to-asteroid behavior. Like-
wise an attractive ﬁeld could be used for the push-asteroid-to-home behav-
ior, where the robot tries to stay behind the asteroid as it moves to home
rather than avoid the asteroid. Avoid-robot could be done with a repul-
sive ﬁeld. These behaviors give the robots individual goals, since there is

8.6
Emergent Social Behavior
305
no awareness of the goals of the other team members.
Now consider what happens when a robot ant encounters an asteroid it
can’t move. The robot stays there pushing. Eventually another robot will
come along because the asteroid is not moving. As it is attracted to the “dark
side” of the asteroid, it will come into range of the ﬁrst robot. What hap-
pens? The avoid-robot behavior should be instantiated, causing the ﬁrst ro-
bot to move over a bit. The second robot will also feel a repulsive force and
slow down. As the ﬁrst robot moves out of the way, the angle of repulsion
changes, forcing the second robot to move sideways as well, as it continues
to move to the asteroid. Together, the interaction between the two robots
should cause them to naturally balance themselves behind the asteroid and
push together. The point is that the robots were not explicitly directed to all
work on the same NEO; they were each directed to ﬁnd their own NEO, but
circumstances led them to the same one.
8.6
Emergent Social Behavior
The examples of heterogeneity, cooperation, control, and goals give some
hint of how an overall social behavior emerges from the actions of autono-
mous robots. The robot teams often are the result of extensive design efforts,
where the teams aren’t too large to interfere with each other, and are opti-
mally sized for the particular task, etc. Many researchers are exploring the
issues of what happens when the designer doesn’t have a choice about the
size of the robot population. How do social behaviors emerge in those cases?
And how can social rules or conventions be established to make the team
self-regulating and productive? This section summarizes two approaches:
creating social rules for the robots to follow, and allowing internal motiva-
tion to cause the robots to adapt their behavior to problems.
8.6.1
Societal rules
Maja Mataric has focused her researchon how group dynamics might emerge
in herds of multiple agents operating under fully distributed control. She ex-
plored the impact of density and the impact of societal rules on overall team
performance.90 Each IS Robotics R2 robot was programmed with behaviors
using the Subsumption architecture. She set up a scenario where up to 20
identical robots (now known as “The Nerd Herd”) were given the same lo-
cation as a goal. The goal, however, was on the other side of a partition with
a narrow door, permitting only one robot to pass through the partition at a

306
8
Multi-agents
Figure 8.4
The Nerd Herd. (Photograph courtesy of USC Interaction Laboratory.)
time. The robots were placed randomly on the same side of the partition and
started moving at the same time.
In the ﬁrst set of demonstrations, the robots functioned with ignorant coex-
IGNORANT
COEXISTENCE
istence. The robots coexisted in a team, but did not have any knowledge of
each other. A robot treated another robot as an obstacle. Each robot had the
equivalent of a move-to-goal and an avoid-obstacle behavior. Since robots
were treated as obstacles, once the robots gathered at the opening, they spent
most of their time avoiding each other. The team as a whole made slow
progress through the door to the goal location. Worse yet, the larger the
number of robots ﬁelded, the larger the trafﬁc jam, and the longer to get all
the team members through.
In the second demonstration, informed coexistence, the robots were allowed
INFORMED
COEXISTENCE
to recognize each other and given a simple social rule governing inter-robot
interactions. In addition to move-to-goal and avoid-obstacle, a third behav-
ior was created for avoiding robots. If a robot detected another robot, it
would stop and wait for time
p. If the blocking robot was still in the way
after
p, the robot would turn left and then resume moving to the goal. The
result of the new behavior was to reduce the trafﬁc jams, and the group got
through the door in about the same time as a single agent going back and
forth through the opening 20 times.
The real surprise came in the third demonstration, intelligent coexistence.
INTELLIGENT
COEXISTENCE

8.6
Emergent Social Behavior
307
Figure 8.5
Robots cooperatively tracking an object under the ALLIANCE architec-
ture. (Photograph courtesy of Oak Ridge National Laboratories.)
The social behavior for avoiding robots was replaced with another heuristic:
the robots were repulsed from other robots, but as it moves away, it tries to
move in the same direction as a majority of other robots. (Each robot broad-
cast its heading over a radio transmitter to compensate for the inability to
recognize each other by vision or sonar, so that isn’t considered communica-
tion.) As a result, the robots exhibited a ﬂocking behavior and went through
the door in single ﬁle! The need to go in the same direction created a ten-
dency to form a line, while repulsion caused the robots to essentially create
spaces for robots to merge into line. Together the two effects created a strong
need to go through the door single ﬁle, even though there was no such ex-
plicit direction. Not only were trafﬁc jams reduced, but the overall task was
accomplished faster.
8.6.2
Motivation
In Mataric’s work, the robots reduced interference through simple social
rules with no communication, but the members of the team could not ac-
tively help out failed colleagues or change tasks dynamically. Lynne Parker
has attempted to address the larger issues of robustness and fault tolerance
with the ALLIANCE architecture,114 an outgrowth of the Subsumption ar-
chitecture. The central idea is that members in the team can either observe or

308
8
Multi-agents
“hear” the progress of others in teams, as well as their own progress. If they
get frustrated with their own progress, they should stop what they’re doing
and move on to something else. Likewise, if a robot is free and another robot
has been unable to accomplish a task, it should try to complete the unﬁnished
task. This is particularly useful for tasks where there is a logical sequence of
behaviors, where all of a particular task (like dusting) needs to be done for an
area before the robots begin working on another task (e.g., sweeping). These
changes in behaviors are regulated by a simple mechanism: motivation. The
MOTIVATION
motivation of a robot to do a task is regulated by two internal motivations,
robot impatience and robot acquiescence. The more frustrated a robot with gets
ROBOT IMPATIENCE
ROBOT ACQUIESCENCE
with another robot’s performance on
t
i, the higher the impatience associated
with that task
t
i. Likewise, the more frustrated a robot gets with its own per-
formance for a task, the higher the acquiescence. If the frustration threshold
is exceeded, then the robot either takes over the unﬁnished task or abandons
its current task and changes behavior.
Fig. 8.6 shows the time trace for an example of motivation for two space
ants foraging for asteroids. (This example isn’t really a sequential series of
tasks in the manner used by ALLIANCE, but this conveys the elegance of
motivation.) In this case, the reactive space ants have to either broadcast
what they’re doing or be able to perceive the other’s progress. This makes it
a bit different than the “no communication” approach. At time 0, both robots
start by looking for asteroids. (We assume there is no frustration for the ﬁnd
task.) Both see asteroid A1, but Robot 1 is the ﬁrst there. Robot 1 has now
taken responsibility for Task 1 (T1), pushing A1 to home. Even though A1
is still stationary at time 3, Robot 2 does not join in as it would in the no-
communication method. Instead, it begins to accrue impatience about T1.
Once Robot 1 begins to push A1, it starts accruing frustration in the form of
acquiescence. As with the no-communication example, a single robot cannot
push the asteroid.
While Robot 1 is trying to push asteroid A1, Robot 2 sees and moves to
asteroid A2. All the while its impatience over T1 is growing. At time 7, Robot
2 is trying unsuccessfully to push asteroid A2 (task T2) and its acquiescence
counter is increasing. Also at time 7, Robot 2’s patience with Robot 1 and
task T1 has been exceeded. It pushes T1 onto its stack of things to do when
it completes its curent task. Meanwhile, at time 9, Robot 1 gives up on T1.
Although it is frustrated with Robot 2, it assumes that T2 is still under control
and so begins to forage again. Finally, at time 10, the frustration over T2
reaches the limit and Robot 1 is free to help Robot 2.

8.7
Summary
309
time
Robot 1
Robot 2
0
ﬁnd-stationary-asteroid
ﬁnd-stationary-asteroid
1
sees A1
sees A1
2
move-to-asteroid(A1)
move-to-asteroid(A1)
3
arrives at A1
resumes ﬁnd-stationary-asteroid
4
push-asteroid-to-home(A1)
ﬁnd-stationary-asteroid
T1-acquiescence++
T1-impatience++
5
push-asteroid-to-home(A1)
sees A2
T1-acquiescence++
T1-impatience++
6
push-asteroid-to-home(A1)
move-to-asteroid(A2)
T1-acquiescence++
T1-impatience++
7
push-asteroid-to-home(A1)
push-asteroid-to-home(A2)
T1-acquiescence++
T1-impatience>limit
T2-impatience++
put T1 on stack
T2-acquiescence++
8
push-asteroid-to-home(A1)
push-asteroid-to-home(A2)
T1-acquiescence++
A1-impatience++
T2-impatience++
T2-acquiescence++
9
T1-acquiescence>limit
push-asteroid-to-home(A2)
gives up on T1
T2-acquiescence++
ﬁnd-stationary-asteroid
T2-impatience++
10
T2-impatience>limit
T2-acquiescence++
now attempts T2
move-to-asteroid(A2)
11
push-asteroid-to-home(A2)
push-asteroid-to-home(A2)
T2-acquiescence = 0
T2-acquiescence = 0
12
arrives at HOME
arrives at HOME
Figure 8.6
Example of how the internal motivation in ALLIANCE might be ex-
tended to work with two space ants.
8.7
Summary
In summary, many tasks favor the use of many cheap robots rather than a
single expensive one. These collections of multiple robots are often referred
to as multi-agents. Individual robots in a multi-agent team are generally
programmed with behaviors, most often as purely reactive systems, but oc-
casionally with a hybrid architecture. As with an overall behavior emerging

310
8
Multi-agents
on a single reactive agent, societies of reactive agents often exhibit an emer-
gent societal behavior.
Multi-agent societies can be characterized according to where they fall on
at least four dimensions (since multi-agent theory is relatively new, even for
robotics, new dimensions may surface over time). Heterogeneity refers to
whether the member robots are identical in software and hardware. Cooper-
ation may be either active or non-active, while control may fall in the spec-
trum from fully distributed to fully centralized. A robot society may have
a single, explicitly shared goal or each robot may have its own goal. When
communication between agents is appropriate is a pervasive, open question.
From a practical side, the emphasis in multi-agents has been on how fa-
vorable group dynamics emerge from teams of homogeneous, purely reac-
tive robots while operating under distributed control. Problems in emer-
gent societal behaviors such as interference and the need to adapt to the
open world can often been addressed by specifying social rules and internal
motivation. However, more interest is emerging in robots that have either
heterogeneous software or hardware capabilities, such as marsupial and re-
conﬁgurable robots. The diversity of a heterogeneous team can be captured
somewhat by the social entropy metric.
8.8
Exercises
Exercise 8.1
Give three reasons why multi-agents are desirable. Describe the general attributes of
applications which are well-suited for multi-agents, and give one example.
Exercise 8.2
Deﬁne the following:
a. heterogeneity
b. control
c. cooperation
d. goals
Exercise 8.3
Consider the example of space ants. What would happen if the ﬁrst robot commu-
nicated with the other robots to recruit them to help move the asteroid? Would the
behaviors or the goal structure necessarily change? Why or why not?
Exercise 8.4
Draw a FSA or write a script to coordinate the sequencing of the Pick Up the Trash
behaviors for Io, Ganymede, and Callisto.

8.8
Exercises
311
Exercise 8.5
Describe three approaches to societal behavior: social rules, internal motivation, and
leadership.
Exercise 8.6
Were the behaviors for the Nerd Herd purely reactive? Why or why not?
Exercise 8.7
[Programming)]
Implement the space ant example with 3-5 robots capable of phototaxis and dead
reckoning.
a. Multi-agent foraging. Start with only a phototropic and avoid-robot behavior,
where a robot is an obstacle that isn’t a light. The program will start with an
empty world consisting of a light (you may need to make a "bigger light" by plac-
ing lights next to each other). Between 2 and 5 phototropic robots will be placed
at different random starting locations in the world. Each will wander through the
world, avoiding obstacles, until it comes to a light. Then it will move directly to
the light (attractive ﬁeld). If more than one robot is attracted to the same light,
they should center themselves evenly around the light. Compare this with the
program in Ch. 5 which had a single robot forage for lights. Which gets more
lights faster?
b. Cooperating to bring the food home. Now add the push-to-home behavior where
the robot wants to be on a straight line behind the light to home. What happens
now?
Exercise 8.8
[World Wide Web]
Visit the RoboCup web site at www.robocup.org. Which team has performed the best
over the past 3 years? Describe the multi-agent organization in terms of control and
cooperation.
Exercise 8.9
[Advanced Reading]
Read Ed Durfee’s humorous invited paper on DAI, “What Your Computer Really
Needs to Know, You Learned in Kindergarten” (proceedings of the Tenth National
Conference on Artiﬁcial Intelligence, 1992). For each of his ten issues (“Share Every-
thing,” “Play Fair,” etc.), describe how this applies to robots. For each issue give an
example of how it applies to a robot team described in this chapter.
Exercise 8.10
[Advanced Reading]
Read and summarize “Behavior-Based Formation Control for Multirobot Teams,” by
Tucker Balch and Ron Arkin in IEEE Transactions on Robotics and Automation, vol. 14,
no 6., 1998.
Exercise 8.11
[Science Fiction]
Watch the movie "Silent Running" about a team of three mobile robots (Huey, Dewey,
and Louie) working on a space station. Classify their teamwork in terms of hetero-
geneity, control, cooperation and goals.

312
8
Multi-agents
8.9
End Notes
For further reading.
Chapter 9, “Social Behavior,” of Behavior-Based Robotics by Ron Arkin has a detailed
and comprehensive presentation of the ethology, philosophical considerations, and
different robot architectures for multi-agents. It is well worth reading.
Swarms and ﬂocks.
The references to swarm robots are too numerous to cite here; many papers explore
details of insect behavior and coordination strategies as well as provide simulations.
Jean-Louis Deneubourg has produced many interesting articles synthesizing insights
form insect colonies into a form useful for programming mobile robots. As noted
in Behavior-Based Robotics, Craig Reynolds’ work in computer graphic simulation of
ﬂocks of in “Flocks, herds, and schools: A distributed behavior model,” in Computer
Graphics, 1987, showed how ﬂocks emerge from simple, individual interactions.
“Fast, Cheap and Out of Control: The Movie.”
The term “Fast, Cheap and Out of Control” later became the title of a 1997 award-
winning documentary by Errol Morris on four men, including Rodney Brooks. The
movie title implied that Morris saw human kind shifting from highly individualistic
relations with the world developed over time (as seen by the lion tamer and topiary
gardener) to decentralized, reactive mobs. Although the movie is not about robotics
per se, it features interviews with Brooks and contains stunning shots of some of
Brooks’ robots walking over broken glass, shining like diamonds in the bright lights.
Maja Mataric, one of Brooks’ students at the time of the ﬁlming, can be seen in one of
the shots wearing shorts.
Languages for multi-agents.
Researchers are beginning to work on languages for multi-agent coordination, in-
cluding Holly Yanco and Lynn Stein at MIT.
Robot soccer.
There is some dispute over which competition was the ﬁrst robot soccer competi-
tion: MIROSOT or RoboCup. RoboCup was originally proposed by Minoru Asada,
a noted Japanese researcher in visually guided mobile robots, in 1995 for the 1997 IJ-
CAI, giving researchers two years to prepare. Hiroaki Kitano has been responsible for
much of the organization of the competition and funding from the Sony Corporation
(among others). MIROSOT was started by Jong-Kwan Kim in Korea in 1996, a year
after RoboCup was announced but a year before the ﬁrst ofﬁcial RoboCup game.
Robot name trivia.
More robots are being named after women these days. The robots in Lynne Parker’s
laboratory at Oak Ridge National Laboratory are named after women pioneers in

8.9
End Notes
313
computer science. Rogue at Carnegie Mellon University was named after the woman
mutant from the X-Men comics (to complement the robot Xavier, named after the
wheeled super hero). The robots in my lab at the University of South Florida are
named after women science ﬁction authors. (Except one robot which was named
after the Coors beer, Silver Bullet, while I was out of town.)
More robot name trivia.
The Nerd Herd consists of IS Robotics R2 robots which look like toasters. Brightly
colored toasters, but toasters nonetheless. The 20 robots are named for things that
come out of toasters, for example, Bagel.


P A R T I I
Navigation

316
Part II
Contents:
 Chapter 9: Topological Path Planning
 Chapter 10: Metric Path Planning
 Chapter 11: Localization and Map Making
 Chapter 12: On the Horizon
Overview
The ﬁrst three chapters in this part focus on navigation, a critical ability for a
robot that claims to be mobile. Navigation remains one of the most challeng-
ing functions to perform, in part because it involves practically everything
about AI robotics: sensing, acting, planning, architectures, hardware, com-
putational efﬁciencies, and problem solving. Reactive robots have provided
behaviors for moving about the world without collisions, but navigation is
more purposeful and requires deliberation. In particular, a robot needs to be
able to plan how to get a particular location. Two different categories of tech-
niques have emerged: topological navigation and metric navigation, also known
TOPOLOGICAL
NAVIGATION
METRIC NAVIGATION
as qualitative navigation and quantitative navigation, respectively. Another im-
QUALITATIVE
NAVIGATION
QUANTITATIVE
NAVIGATION
portant class of problems is how to make maps, which introduces the issue
of how the robot can accurately localize itself as it moves, despite the prob-
lems seen with proprioceptive sensors in Ch. 6. The ﬁnal chapter provides
an overview of robots and AI techniques on the horizon.
The Four Questions
Part I presented the material in a historical sequence, following the steps
in development of the three AI robotic paradigms. Navigation is more of
a collection of speciﬁc algorithms than a linear progression; therefore the
material is organized by functionality. The functions of navigation can be
expressed by four questions presented below.
Where am I going? This is usually determined by a human or a mission
planner. A planetary rover may be directed to move to a far away crater and
look for a speciﬁc rock in the caldera. Roboticists generally do not include
this as part of navigation, assuming that the robot has been directed to a
particular goal.
What’s the best way there? This is the problem of path planning, and is
the area of navigation which has received the most attention. Path planning

Part II
317
methods fall into two categories: qualitative and quantitative (or metric).
Qualitative path planning methods will be covered in Ch. 9, while metric
methods will be presented in Ch. 10.
Where have I been? Map making is an overlooked aspect of navigation.
As a robot explores new environments, it may be part of its mission to map
the environment. But even if the robot is operating in the same environment
(e.g., delivering mail in an ofﬁce building), it may improve its performance
by noting changes. A new wall may be erected, furniture rearranged, paint-
ings moved or replaced. Or, as discovered by Xavier, one of the indoor robots
at the Robotics Institute at Carnegie Mellon University, certain hallways and
foyers are too crowded at speciﬁc times of the day (end of class, lunchtime)
and should be avoided.
Where am I? In order to follow a path or build a map, the robot has to
know where it is; this is referred to as localization. However, as has already
been seen in Ch. 6, a robot is very poor at dead reckoning. Imagine what
happens to map making when a robot goes around the perimeter of a room
but cannot record its footsteps. Localization can be relative to a local environ-
ment (e.g., the robot is in the center of the room), in topological coordinates
(e.g., in Room 311), or in absolute coordinates (e.g., latitude, longitude, alti-
tude). Both map making and localization are covered in Ch. 11.
The Rhino and Minerva Tour Guide Robots (shown in Fig. II) provide an
excellent example of how these four questions arise naturally in applica-
tions.139 Rhino and Minerva were designed by researchers from CMU and
the University of Bonn to perform all the functions of a tour guide in a mu-
seum, including leading groups of people to exhibits upon request and an-
swering questions. Rhino hosted tours of the Deutsches Museum in Bonn,
Germany, while Minerva was used in the Smithsonian’s National Museum
of History in Washington, DC.
The tour guide robots had to know where they were at any given time (lo-
calization) in order to answer questions about an exhibit or give directions
to another exhibit. Minerva also could create a custom tour of requested
exhibits, requiring it to know how to get to each of the exhibits (path plan-
ning). The tour path had to be reasonably efﬁcient, otherwise the robot might
have the group pass a targeted exhibit repeatedly before stopping at it. This
also means that the robot had to remember where it had been. Rhino op-
erated using a map of the museum, while Minerva created her own. An
important aspect of the map was that it included the time needed to navi-
gate between exhibits, not just the distance. This time of information allows
a planner to factor in areas to avoid at certain times of day. For example,

318
Part II
a.
b.
Figure II.1 Two Tour Guide Robots: a) Rhino in the Deutsches Museum
in Bonn (in the center), and b) a close up of the more emotive Minerva.
(Photographs courtesy of Sebastian Thrun and Wolfram Burgard.)

Part II
319
Xavier at CMU has learned to avoid cutting through a particular foyer when
classes are changing, since the density of people moving about slows down
its progress.
Criteria for Evaluating Path Planners
Although it should be clear by the end of Part II that navigation is more than
path planning, path planning is the most obvious aspect of navigation from
a programmer’s standpoint. A designer has a wide variety of techniques to
choose from, spanning close to 30 years of research. As with everything else
robotic, the choice of technique depends on the ecological niche the robot
will operate in. The criteria for evaluating the suitability of a path planner
includes:
1. Complexity. Is the algorithm too computationally or space intensive to
execute or reside within the limitations of the robot?
2. Sufﬁciently represents the terrain. Many researchers work in indoor environ-
ments, which are ﬂat. Outdoor robots may be operating in rough terrain
with steep inclines or undesirable areas, such as slippery sand or mud. If
the path planning algorithm is built to generate paths from a binary rep-
resentation (a region is either navigable or not), then it may lead the robot
into trouble if it is applied to a more diverse environment.
3. Sufﬁciently represents the physical limitations of the robot platform. Robots
have physical limitations. The most profound limitation which impacts
path planning is whether a robot is holonomic or not. Recall that holo-
nomic robots can turn in place. Since they can spin on a dime, the path
planning algorithm doesn’t have to consider the steering radius of the
robot. Likewise, robots may not be round. Robots made for research pur-
poses such as Nomad 200’s, Khepera, and RWI B-series are often round
so that when they turn in place, they won’t bump into anything. A robot
which is not round introduces more complexity. In order to turn in a nar-
row hall, it may have to “watch its back” to make sure it doesn’t bump
anything.
4. Compatible with the reactive layer. Path planners are deliberative by def-
inition. But in a Hybrid style of architecture, the reactive layer will be
responsible for carrying out the path. A technique which simpliﬁes this
transformation is desirable.

320
Part II
5. Supports corrections to the map and re-planning. Path planning requires an a
priori map, which may turn out to be seriously wrong. Therefore, a robot
may start out with one map, discover it is incorrect, and need to update
the map and re-plan. Clearly techniques which permit the existing plan
to be repaired rather than be scrapped and computed from scratch are
desirable.
The Impact of Sensor Uncertainty
Since navigation is a fundamental capability of a mobile robot, researchers
have been investigating navigational techniques since the 1960’s. But as was
seen in Part I, it was only since the early 1990’s that robots became afford-
able, and had on-board sensing and reasonable computational power. As
a result, most researchers in navigation were forced to develop techniques
using simulators and assumptions about how real robots would physically
work.
Two of the most pervasive assumptions of these researchers turned out to
be unfortunate in retrospect. First, it was generally assumed that the robot
could localize itself accurately at each update. This assumption was based in
part on another assumption: that sensors would give an accurate represen-
tation of the world. As was seen just with sonars in Ch. 6, this is often not
true. Sensors are always noisy and have vulnerabilities.
Therefore, a robot has to operate in the presence of uncertainty. In the Re-
active Paradigm, the way in which the sensors were coupled with the actua-
tors accepted this uncertainty. If the sonar or IR returned an incorrect range
reading, the robot may appear to start to avoid an imaginary obstacle. How-
ever, the process of moving often eliminated the source of the noisy data,
and soon the robot was back to doing the right thing. Uncertainty becomes
more serious when dealing with map making and localization; therefore a
new wave of techniques has been developed to smooth over sensor noise
and ascertain the correct state of the world. These methods are mathematical
in nature and are covered in Ch. 11.
Navigation and the Robotic Paradigms
The questions posed call to mind deliberation. Planning, just from the name
alone, is deliberative. Map making and localization imply memory and la-
beling speciﬁc locations (room, hall, river, canyon); these are symbolic rep-
resentations and so also ﬁt the notion of deliberation from the Hybrid Para-

Part II
321
digm. Most of the techniques presented in Part II will go into the deliberative
component of Hybrid architectures.
One important observation is that the four questions of navigation largely
ignore an implicit ﬁfth question: how am I going to get there? Based on Part I,
the obvious answer is “by using reactive behaviors.” But navigation is delib-
erative, and the issue of integrating deliberation and reaction for navigation
in a Hybrid architecture is still largely open. Work addressing this issue of
interleaving planning and execution is presented in Ch. 9.
Spatial Memory
The answer to what’s the best way there? depends on the representation of
the world that the robot is using. The world representation will be called
the robot’s spatial memory.63 Spatial memory is the heart of the cartographer
SPATIAL MEMORY
object class (or its equivalent) in a Hybrid architecture, as described in Ch. 7.
Spatial memory should provide methods and data structures for process-
ing and storing output from current sensory inputs. For example, suppose
a robot is directed to “go down the hall to the third red door on the right.”
Even for the coordination and control of reactive behaviors, the robot needs
to operationalize concepts such as “hall,” “red,” “door” into features to look
for with a perceptual schema. It also needs to remember how many red doors
it has gone past (and not count the same door twice!). It would also be ad-
vantageous if the robot sensed a barrier or dead-end and updated its map of
the world.
Spatial memory should also be organized to support methods which can
extract the relevant expectations about a navigational task. Suppose a robot
is directed this time to to “go down the hall to the third door on the right.”
It could consult its spatial memory and notice that odd numbered doors are
red, and even numbered are yellow. By looking for “red” and “yellow” in
addition to other perceptual features of a door, the robot can more reliably
identify doors, either by focus of attention (the robot only runs door detec-
tion on red and yellow areas, not every image) or by sensor fusion (more
sources of data means a more certain percept).
Spatial memory supports four basic functions:
1. Attention. What features, landmarks to look for next?
ATTENTION
2. Reasoning. Can that surface support my weight?
REASONING
3. Path planning. What is the best way through this space?
PATH PLANNING

322
Part II
4. Information collection. What does this place look like? Have I ever seen it
INFORMATION
COLLECTION
before? What has changed since the last time I was here?
Spatial memory takes two forms: route, or qualitative, and layout, or metric,
ROUTE OR
QUALITATIVE
representations. Route representations express space in terms of the connec-
tions between landmarks. An example of a route representation is when a
person gives directions propositionally (as a list): “go out of the parking lot,
and turn left onto Fowler Drive. Look for the museum on the right, and turn
left at next trafﬁc light.” Notice that is perspective dependent; landmarks
that are easy for a human to see may not be visible to a small robot operating
close to the ﬂoor. Route representations also tend to supply orientation cues:
“out of the parking lot” (versus being contained in it), “turn left,” “on the
right.” These orientation cues are egocentric, in that they assume the agent
is following the directions at each step.
Layout representations are the opposite of route representations. When a
LAYOUT OR METRIC
person gives directions by drawing a map, the map is a layout representa-
tion. Layout representations are often called metric representations because
most maps have some approximate scale to estimate distances to travel. The
major differences between layout and route representations are the view-
point and utility. A layout representation is essentially a bird’s-eye view of
the world. It is not dependent of the perspective of the agent; the agent is
assumed to be able to translate the layout into features to be sensed. The lay-
out is orientation and position independent. Layout representations can be
used to generate a route representation, but this doesn’t necessarily work the
other way. (Consider how easy it is to read a map and give verbal directions
to a driver, versus drawing an accurate map of a road you’ve only been on
once.) Most maps contain extra information, such as cross streets. An agent
can use this information to generate alternative routes if the desired route is
blocked.
While spatial memory is clearly an important key to robust navigation, it
does involve memory, representation, and planning. The successes of the Re-
active Paradigm suggest that for robots “less is more.” Therefore, it merits
considering how much spatial memory an agent needs to navigate. This is a
gray area. The amount of representation needed depends on many factors.
How accurately and efﬁciently does the robot have to navigate? Is time crit-
ical, or can it take a slightly sub-optimal route? Navigational tasks which
require optimality tend to require more dense and complex world represen-
tations. What are the characteristics of the environment? Are there land-
marks to provide orientation cues? Are distances known accurately? What

Part II
323
are the sources of information about that environment that specify terrains,
surface properties, obstacles, etc.? What are the properties of the available
sensors in that environment?
End Notes
Invasion of the robot tour guides...
The success of Rhino and Minerva, coupled with the interesting technical challenges
posed by navigating through buildings crowded with people, has prompted other
groups to create robot tour guides. A group also at the CS department at CMU devel-
oped SAGE, a tour guide for the Carnegie Museum in Pittsburgh, under the direction
of Illah Nourbakhsh.


9
Topological Path Planning
Chapter Objectives:
 Deﬁne the difference between a natural and artiﬁcial landmark and give one
example of each.
 Given a description of an indoor ofﬁce environment and a set of behav-
iors, build a relational graph representation labeling the distinctive places
and local control strategies using gateways.
 Describe in one or two sentences: gateway, image signature, visual homing,
viewframe, orientation region.
 Given a ﬁgure showing landmarks, create a topological map showing
landmarks, landmark pair boundaries, and orientation regions.
9.1
Overview
Topological, route, or qualitative navigation is often viewed as being more sim-
TOPOLOGICAL
NAVIGATION
ple and natural for a behavior-based robot. Certainly people frequently give
other people routes as directives; therefore it seems natural to expect a robot
to be able to parse commands such as “go down the hall, turn to the left at
the dead end, and enter the second room on the right.” Even without a map
of where everything is, there is enough information for navigation as long as
the robot knows what a “hall,” “dead-end,“ and “room” is.
Route representations fall into one of two approaches:
1. Relational. Relational techniques are the most popular, and can be thought
of as giving the robot an abbreviated, “connect the dots” graph-style of
spatial memory.

326
9
Topological Path Planning
2. Associative. Relational techniques tend to focus on the graph-like rep-
resentation of spatial memory. Associative techniques focus on coupling
sensing with localization in a manner which parallels the tight coupling
of sensing to acting found in reﬂexive behaviors.
Because relational techniques use an explicit representation, they can sup-
port path planning. Associative techniques are better for retracing known
paths.
9.2
Landmarks and Gateways
Topological navigation depends on the presence of landmarks. A landmark is
LANDMARK
one or more perceptually distinctive features of interest on an object or locale
of interest. Note that a landmark is not necessarily a single, self-contained
object like “red door.” A landmark can be a grouping of objects; for exam-
ple, “McDonald’s” means a tall sign, bright building of a certain shape, and
a parking lot with a lot of activity. Another outdoor landmark might be a
“stand of aspen trees.”
Landmarks are used in most aspects of navigation. If a robot ﬁnds a land-
mark in the world and that landmark appears on a map, then the robot is
localized with respect to the map. If the robot plans a path consisting of seg-
ments, landmarks are needed so the robot can tell when it has completed a
segment and another should begin. If a robot ﬁnds new landmarks, they can
be added to its spatial memory, creating or extending a map.
Dave Kortenkamp popularized a particularly interesting special case of
landmarks: gateways.79 A gateway is an opportunity for a robot to change its
GATEWAYS
overall direction of navigation. For example, an intersection of two hallways
is a gateway; the robot can choose to go straight or turn left or right. Because
gateways are navigational opportunities, recognizing gateways is critical for
localization, path planning, and map making.
Landmarks may be either artiﬁcial or natural. The terms “artiﬁcial” and
ARTIFICIAL
LANDMARKS
NATURAL LANDMARKS
“natural“ should not be confused with “man-made” and “organic.” An ar-
tiﬁcial landmark is a set of features added to an existing object or locale in
order to either support recognition of the landmark or some other perceptual
activity. An interstate highway exit sign is an example of an artiﬁcial land-
mark. It is put there with the purpose of being easy to see (retro-reﬂective)
and the white-on-green font is sized for optimal visibility (perceptual activ-
ity is reading the sign). A natural landmark is a conﬁguration of exisiting
features selected for recognition which were not expressly designed for the

9.2
Landmarks and Gateways
327
perceptual activity. If someone gives directions to their house, “take the sec-
ond right after the McDonald’s,” the McDonald’s is being used as an orienta-
tion cue for navigation to their house. Clearly, the McDonald’s was not built
with the purpose of being a navigational cue to a private home. The fact that
it was used for another purpose means it is a natural landmark.
Regardless of whether a landmark is artiﬁcial or natural, it must satisfy
CRITERIA FOR
LANDMARKS
three criteria:
1. Be readily recognizable. If the robot can’t ﬁnd the landmark, it isn’t useful.
2. Support the task dependent activity. If the task dependent activity is simply
an orientation cue (“take the second right after the McDonald’s”), then
being recognized is enough. Suppose a landmark is intended to provide
position estimation to guide docking the space shuttle to a space station.
In that case, the landmark should make it easy to extract the relative dis-
tance to contact.
3. Be perceivable from many different viewpoints. If the landmark is widely vis-
ible, the robot may never ﬁnd it.
Fig. 9.1 shows artiﬁcial landmarks constructed for use in the 1992 AAAI
Mobile Robot Competition.42 Each robot was supposed to follow a route be-
tween any sequence of waypoints in an arena. The teams were allowed to
mark the waypoints with artiﬁcial landmarks. Each waypoint is readily rec-
ognizable by the checkerboard pattern. The task dependent activity of going
to the correct waypoint is facilitated by the cylindrical barcodes which are
unique for each station. Notice that the use of a cylinder guaranteed that
landmarks would be perceivable from any viewpoint in the arena.
A good landmark has many other desirable characteristics. It should be
passive in order to be available despite a power failure. It should be perceiv-
able over the entire range where the robot might need to see it. It should have
distinctive features, and, if possible, unique features. Distinctive features are
those which are locally unique; they appear only as part of the landmark
from every viewpoint of the robot in that region of the world (e.g., there is
only one McDonald’s on Busch Boulevard). If the feature occurs only once in
the entire region of operations (e.g., there is only one McDonald’s in Tampa),
then the feature would be considered globally unique. In addition to being
perceivable for recognition purposes, it must be perceivable for the task. If
the robot needs to position itself to within 0.5 meters of the landmark, then
the landmark must be designed to achieve that accuracy.

328
9
Topological Path Planning
Figure 9.1
Artiﬁcial landmarks used in the 1992 AAAI Mobile Robot Competition.
(Photograph courtesy of AAAI.)
9.3
Relational Methods
Relational methods represent the world as a graph or network of nodes and
edges. Nodes represent gateways, landmarks, or goals. Edges represent
a navigable path between two nodes, in effect that two nodes have a spa-
tial relationship. Additional information may be attached to edges, such
as direction (N,S,E,W), approximate distance, terrain type, or the behaviors
needed to navigate that path. Paths can be computed between two points us-
ing standard graph algorithms, such as Dijkstra’s single source shortest path
algorithm. (See any algorithm textbook for details.)
One of the earliest investigations of relational graphs for navigation was by
Smith and Cheeseman.133 They represented the world as a relational graph,
where the edges represented the direction and distance between nodes. They

9.3
Relational Methods
329
Figure 9.2
Representation of a ﬂoor plan as a relational graph.
simulated what would happen if a robot used dead-reckoning to navigate.
As would be expected from the section on proprioception in Ch. 6, they
found that the error would continually increase and soon the robot would
be unable to reach any of the nodes.
9.3.1
Distinctive places
Kuipers and Byun tied relational graphs to sensing in their seminal work
with distinctive places.81 A distinctive place is a landmark that the robot could
DISTINCTIVE PLACE
detect from a nearby region called a neighborhood. Their work was moti-
vated by research in cognitive science indicating that spatial representation
in the animal kingdom forms a multi-level hierarchy. (More recent stud-
ies suggest this hierarchy isn’t as clearly partitioned as previously thought.)
The lowest level, or most primitive way of representing space, was by iden-
tifying landmarks (doors, hallways) and the procedural knowledge to travel

330
9
Topological Path Planning
landmark 1
landmark 2
landmark 3
landmark 4
Figure 9.3
Propagation of error in a relational graph.
between them (follow-hall, move-thru-door). The next layer up was topolog-
ical. It represented the landmarks and procedural knowledge in a relational
graph, which supported planning and reasoning. The uppermost level was
metric, where the agent had learned the distances and orientation between
the landmarks and could place them in a ﬁxed coordinate system. Higher
layers represented increasing intelligence.
Kuipers and Byun’s representation is of particular interest. Each node rep-
resents a distinctive place. Once in the neighborhood, the robot can position
itself in a known spot relative to the landmark using sensor readings. One
example of a distinctive place was a corner. (Kuipers and Byun worked in
simulation; this did not turn out to be realistic with sonars.) The idea was
that the robot could repeatably move around in the neighborhood the corner
until, for example, 1 meter from each wall. Then the robot would be localized
on the map.
An arc or edge in the relational graph was called a local control strategy,
LOCAL CONTROL
STRATEGY
or lcs. The local control strategy is the procedure for getting from the cur-
rent node to the next node. When the robot senses the landmark it is ﬁlling
in values for a set of features. The robot uses a hill-climbing algorithm. The
HILL-CLIMBING
ALGORITHM
hill-climbing algorithm directs the robot around in the neighborhood until
a measurement function (e.g., how far away the walls are) indicates when
the robot is at a position where the feature values are maximized (e.g., both

9.3
Relational Methods
331
Metric: distances, directions,
shapes in coordinate system
Topological: connectivity
Landmark definitions,
procedural knowledge for traveling
Figure 9.4
Multi-level spatial hierarchy, after Byun and Kuipers. 81
are 1 meter away). The point where the feature values are maximized is the
distinctive place. The hill-climbing algorithm is a very simple approach. The
idea is that for most hills, if you always choose the next step which is the
highest (you can’t look ahead), then you will get to the top quickly. Hence,
the robot always moves in the direction which appears to cause the most
postive change in the measurement function.
Although developed independently of each other, reactive behaviors map
nicely onto distinctive places and local control strategies, as shown in Fig. 9.6.
Consider a robot navigating down a wide hall to a dead-end. The local con-
trol strategy is a behavior, such as follow-hall, which operates in conjunc-
tion with a releaser, look-for-corner. The releaser is an exteroceptive
cue. When it is triggered, the robot is in the neighborhood of the distinctive
place, and the released behavior, hillclimb-to-corner(distance=1),
directs the robot to 1 meter from each wall.
9.3.2
Advantages and disadvantages
The distinctive place concept eliminates any navigational errors at each node.
The robot may drift off-course because the hall is wide, but as soon as it
reaches the neighborhood, it self-corrects and localizes itself. Kuipers and
Byun were able to show in simulation how a robot with dead reckoning er-

332
9
Topological Path Planning
neighborhood
boundary
path of robot as it moves
into neighborhood and
to the distinctive place
distinctive
place (within 
the corner)
Figure 9.5
A robot reacting to a corner and moving to a distinctive place within the
neighborhood.
dead end
T junction
front door
blue door
follow-hall,
with
look-for-door
look-for-blue
follow-hall,
with
look-for-T
follow-hall,
with
look-for-T
follow-hall,
with
look-for-dead-end
follow-hall,
with
look-for-T
follow-hall,
with
look-for-door
Figure 9.6
Behaviors serving as local control strategies, and releasers as means of
signalling the entrance to a neighborhood.

9.4
Associative Methods
333
rors could use multiple trips between nodes to build up a reasonable metric
map, since most of the errors would average out. Another attractive aspect
of the distinctive place approach is that it supports discovery of new land-
marks as the robot explores an unknown environment. As long as the robot
found something distinctive that it could reliably localize itself to, it could be
put on a topological map. Then as it repeatedly moved to it, the robot could
construct a metric map.
Returning to the discussion of landmarks, it should be noticed that a land-
mark must be unique to a node pair. There can’t be any corners in the real
world that are not on the graph between the nodes or else the robot will
localize itself incorrectly.
The distinctive place approach as originally formulated encountered some
problems when behavior-based roboticists began to apply it to real robots.
One of the most challenging problems was perception.
Good distinctive
places are hard to come by; conﬁgurations that seemed useful to humans,
like corners, proved difﬁcult to reliably sense and localize against. Features
that were easy to sense often were too numerous in the world, and so were
not locally unique. Another challenge was learning the local control strategy.
As the robot explored an unknown environment, it was easy to imagine that
it could ﬁnd distinctive places. But how did it learn the appropriate local
control strategy? In an indoor environment, the robot might resort to always
using wall following, even though other behaviors would be better suited.
How would it ever try something different? Another open issue is the prob-
lem of indistinguishable locations. The issue of indistinguishable locations
has also been tackled to some degree by work with probablistic methods,
which will be covered in Ch. 11.
9.4
Associative Methods
Associative methods for topological navigation essentially create a behavior
ASSOCIATIVE METHODS
which converts sensor observations into the direction to go to reach a partic-
ular landmark. The underlying assumption is that a location or landmark of
interest for navigation usually has two attributes:
1. perceptual stability: that views of the location that are close together should
PERCEPTUAL STABILITY
look similar
2. perceptual distinguishability: that views far away should look different.
PERCEPTUAL
DISTINGUISHABILITY

334
9
Topological Path Planning
Figure 9.7
Derivation of an image signature. Clockwise from the upper left: a raw
image, the image partitioned into 16 sections, and the image with the sections values
shown as crosshatching.
These principles are implicit in the idea of a neighborhood around a dis-
tinctive place. If the robot is in the neighborhood, the views of the landmark
look similar. The main difference is that associative methods use very coarse
computer vision.
9.4.1
Visual homing
Work done by Nelson,111 and later by Engelson,50 relied on an image signa-
ture. An image signature is created by partitioning an image into sections.
Fig. 9.7 shows an image partitioned into 16 sections. Next, each section is
examined and a measurement is made of some attribute in the section. Some
possible measurements are the edge density (the number of pixels with edges
divided by the number of pixels in the section), dominant edge orientation
(the angle the majority of the edges are pointing), average intensity, and so
on. The image signature reduces the image to
n measurements. One way to
think of this is that it changes the resolution of the image into 16 pixels.
Notice that the image signature in the ﬁgure forms a pattern. If the robot

9.4
Associative Methods
335
Figure 9.8
Image measurements and the appropriate response from two different
views.
is in the neighborhood of the location, then the image measurements should
be approximately the same pattern as the image signature, only the pattern
may be offset due to the not-quite-at-the-right-place viewpoint.
If the robot can identify the image signature, or portion of it, in the current
image, it will then know whether to turn left or right to localize itself relative
to the location. The use of image signatures to direct a robot to a speciﬁc
location is called visual homing. The inspiration for visual homing came
from Nelson’s research into how bees navigate. It is easy to speculate that
baby bees are learning the image signatures of their hive as they execute the
zoom in/out behavior described in Ch. 3. In that case, the compound eyes
serve as de facto partitions of what to humans would be a single image.
9.4.2
QualNav
Levitt and Lawton took the ideas of neighborhoods and visual homing to an
extreme for outdoor navigation over large distances as part of the Defense
Advance Research Projects Agency (DARPA) Autonomous Land Vehicle (ALV)
AUTONOMOUS LAND
VEHICLE (ALV)
project in the late 1980’s.85 At that time, topological navigation using rela-
tional graphs appeared promising for indoor environments, but seemed to
resist application to outdoor terrains. Part of the challenge was the notion of

336
9
Topological Path Planning
Figure 9.9
Division of an outdoor area into orientation regions, after Levitt and
Lawton. 85
localization. With that challenge went the associated question of what could
the robot use as a landmark. The land vehicle might be expected to travel
for 50 miles deliberately avoiding obvious landmarks such as houses, road
intersections, and radio towers. The idea was to have a robot vehicle capable
of stealthily scouting an area and seeing if enemy forces were nearby. The
robot vehicle has to provide some indication of where it found the enemy
forces as well as return to base.
Inside of trying to localize the vehicle to a particular landmark, the idea
behind QualNav85 (an acronym for “qualitative navigation”) was to localize
QUALNAV
the vehicle to a particular orientation region, or a patch of the world. The
ORIENTATION REGION
orientation region is deﬁned by landmark pair boundaries. A landmark pair
LANDMARK PAIR
BOUNDARIES
boundary is an imaginary line drawn between two landmarks. As seen in
the ﬁgure below, drawing lines between all the candidate landmark pairs
will partition the world into orientation regions, although the orientation
regions may be very oddly shaped.
An orientation region is conceptually similar to a neighborhood. Within
an orientation region, all the landmarks appear in the same relationship. If

9.4
Associative Methods
337
the vehicle is in OR1, the landmarks are found in the same order if the ro-
bot always turns in the same direction (either always clockwise or always
counter-clockwise): building-mountain-tower. If the vehicle is in OR2, the
landmarks will be tree-building-intersection.
An interesting attribute of orientation regions and landmark pair bound-
aries is that the vehicle can directly perceive when it has entered a new
orientation region. For example, as the robot moves from OR1 to OR2, the
building-tower landmark pair boundary is in front of it, then is on both sides
of the vehicle, and is ﬁnally behind it. The transition is a perceptual event,
and does not require knowledge about the distance of the landmarks, just
that the relationship of the landmarks to the robot has just changed.
The use of orientation regions allows the robot to create an outdoor topo-
logical map as it explores the world, or to localize itself (coarsely) to a metric
map. The robot does not have to be able to estimate the range to any of the
landmarks.
How the robot navigates inside an orientation region has the ﬂavor of vi-
sual homing. The robot may need to retrace its path as precisely as possible
through an orientation region. (A military robot might need to return with-
out being seen.) Without knowing the range to the landmarks bounding the
orientation region, the robot is helpless. But if it has to remember the angles
to each landmark every
n minutes, it can move to follow the angles. A set of
angles remembered at a point along the path is called a viewframe.
One amusing aspect of the viewframe approach is that it assumed the ro-
bot had cameras literally in the back of its head. Unfortunately the ALV
vehicle did not; all the sensors faced forward. Daryl Lawton tells the story
of trying to convince the union driver of the vehicle to stop every 10 feet
and do a
0
 turn so he could record the viewframes. After much plead-
ing, the driver ﬁnally honored the request, though it wasn’t clear if he ever
understood why the crazy scientist wanted a high-tech truck capable of au-
tonomous navigation to go in circles every 10 feet!
Associative methods are interesting because of the tight coupling of sens-
ing to homing. The image signature and viewframe concepts do not require
the robot to recognize explicitly what a landmark is, only that it is percep-
tually stable and distinguishable for the region of interest. Unfortunately
associative methods require massive storage and are brittle in the presence
of a dynamic world where landmarks may be suddenly occluded or change.
Of course, this is more problematic for indoor environments than outdoor
ones.

338
9
Topological Path Planning
Figure 9.10
A student testing an unidentiﬁed entry in the competition arena at the
1994 AAAI Mobile Robot Competition “Ofﬁce Delivery” event. (Photograph courtesy
of AAAI.)
9.5
Case Study of Topological Navigation with a
Hybrid Architecture
This section presents a case study of topological navigation using the SFX
architecture in the 1994 AAAI Mobile Robot Competition by a team of un-
dergraduates from the Colorado School of Mines. The 1994 competition had
an ofﬁce navigation event.129 Each robot was placed in a random room, then
had to navigate out of the room and to another room within 15 minutes.
Entrants were given a topological map, but weren’t allowed to measure the
layout of the rooms and halls. This case study reviews how the topological
map was entered, the activities of the Cartographer, how scripts (discussed
in Ch. 5) were used to simplify behavioral management, and the lessons
learned.

9.5
Case Study of Topological Navigation with a
Hybrid Architecture
339
9.5.1
Path planning
The topological map was entered as an ASCII ﬁle in Backus-Naur Form. A
sample map is shown in Fig. 9.11. The input map consists of three node types:
room (R), hall (H), and foyer (F). The world is assumed to be orthogonal.
Since edges between nodes can be in only four directions, it is convenient
to refer to them as north (N), south (S), east (E), and west (W), where N is set
arbitrarily in the map. The robot is given its starting node but as an extra
challenge, the robot is not given the direction it is initially facing relative to
the map. The topological map is structurally correct, but does not necessarily
represent if a corridor or door is blocked. Such blockages may occur or be
moved at any time. An additional assumption is that the outside of each
door is marked with a landmark such as a room number or room name.
The Cartographer in SFX is responsible for constructing the route. It takes
as input a gateway type of topological map and a start node and goal node,
and produces a list of nodes representing the best path between the start and
goal. The cartographer operates in two steps: preprocessing of the map to
support path planning, and path planning. The preprocessing step begins by
building a database of the nodes in the input map, reclassifying the corridor
nodes which represent a hall to door connection as Hd.
Once the start and goal nodes are known, the Cartographer eliminates ex-
traneous gateways. A Hd node may be connected to a room which is not
visitable, that is, it is neither the goal room, the start room, or a room with
more than one door. If that occurs, then both the R and Hd node entries are
eliminated from the database. A sample input graphical topological repre-
sentation for a metric map is shown in Fig. 9.11. If R3 was selected as the
start node and R7 the goal, the reﬁned graphical representation is as shown
in Fig. 9.11. Note that Hd3-R1 and Hd4-R6 were eliminated because they
were not associated with the start or goal rooms and could not be used as a
shortcut since they had only one entrance. Gateway nodes such as H10 re-
main in the path because they may be useful if a blocked path occurs. For
example, if the robot is going down the hallway from H11 to H8 and the path
is blocked, the robot will need to return to a known location in order to reori-
ent itself and replan. However, if the H10 is eliminated, then the robot must
return to the very beginning of the corridor, because it does not know where
it is with respect to H10. To solve this dilemma of traveling the same part
of the corridor possibly three times, the cartographer maintains nodes which
represent possible alternative strategies. Different types of gates, tasks, and
robot capabilities will lead to different preprocessing strategies.

340
9
Topological Path Planning
R7
R6
R5
R4
R3
R2
R1
H1
H2
H3
H4
H5 H6
H7
H8
H20
H9
H10
H11
H12
H13
H14
H15
H16
H17
H18
H19
F1
F2
R7*
R5
R4
R3*
R2
Hd1
H2
H3
H4
H6
Hd7
H8
H20
H10
H11
Hd12
Hd13
H14
H15
H16
Hd18
H19
F1
F2
R7
R6
R5
R4
R3
R2
R1
H1
H2
H3
H4
H5
H6
H7
H8
H20
H9
H10
H11
H12
H13
H14
H15
H16
H17
H18
H19
F1
F2
a.
b.
c.
Hd9
Hd5
Hd17
Figure 9.11
a.) Metric map of an ofﬁce layout, b.) graphical topological representa-
tion, and c.) reﬁned graph for traveling from R3 to R7.
An optimal path is computed using Dijkstra’s single source shortest path
algorithm.
The algorithm generates the shortest path by considering the
costs between each pair of connected nodes; the costs are expressed as the
length or weight of an edge in the graph. Since the topological representa-
tion is not metric, connections between the nodes reﬂect preferences in con-
structing routes. These preferences are expressed as edge weights. In this
implementation, navigating through foyers was considerably more compu-
tationally expensive and unreliable; therefore the edge weighting for any
path subsegment originating from a foyer was set at 3. The edge weight
for going from a room to another room was set at 2 to discourage the robot
from ﬁnding solutions which were “impolite” (e.g., cutting through people’s
ofﬁces). All other connections were set to 1.

9.5
Case Study of Topological Navigation with a
Hybrid Architecture
341
The Task Manager in SFX uses the path computed by the cartographer to
select the appropriate abstract navigation behavior (Ch. 5) for traveling between
a particular pair of nodes. It maintains a pointer to the current node and the
next intended node from the path. The current and next node type determine
the appropriate behavior according to the transition table shown below:
TRANSITION TABLE
To
From
H
F
R
H
H
Navigate-hall
Navigate-hall
Undeﬁned
Navigate-hall
F
Navigate-hall
Navigate-foyer
Navigate-door
Navigate-hall
R
Undeﬁned
Navigate-door
Navigate-door
Navigate-door
Hd
Navigate-hall
Navigate-hall
Navigate-door
Navigate-hall
The transition table shows that not all combinations of nodes are permit-
ted; by deﬁnition, the robot cannot move from a hall node H to a room node
R without going through a Hd node. Also, the table is not necessarily sym-
metrical. In the case of rooms, navigate-door must be employed to either
enter or exit, but the case of moving to a foyer will use different strategies
depending on whether the robot is traversing a hall or a foyer. The ANB it-
self uses the information in the database entries for the nodes as parameters
for instantiating the script to the current waypoint pair.
One novel aspect of this implementation is how the Task Manager han-
dles a blocked path; it does not reverse the currently instantiated abstract
navigation behavior (ANB). If the obstacle avoidance behavior posts to the
whiteboard structure that a BLOCKED condition has occurred, the Task Man-
ager terminates the currently active abstract navigation behavior. Because
the robot is between nodes, the Task Manager directs the robot to return to
the current node. But it triggers a simple move-to-goal behavior, which al-
lows the robot to reorient itself more quickly than reinstantiating the abstract
navigation behavior with new parameters.
Once the robot has returned to approximately the last known location, the
Task Manager requests a new path from the Cartographer. The Cartogra-
pher removes the connection between the current and intended nodes, then
recomputes a new path with the current node as the start. Once the new path
is completed, the Task Manager resumes control.
Fig. 9.12 demonstrates the robot moving through the map handed out at
the actual AAAI Mobile Robot Competition. The start node is R7 and the
goal is R2. The cartographer computes the path as R7-H1-H2-H5-R2, elim-
inating H3 and H4 because they are not relevant for this task.

342
9
Topological Path Planning
R7
R6
R5
R4
R3
R2
R1
H1
H2
H3
H4
H5 H6
H7
H8
H20
H9
H10
H11
H12
H13
H14
H15
H16
H17
H18
H19
F1
F2
Navigating Door
Navigating Hall
N
R7 -> R2
R7 - H1 - H2 - H5 - R2
Moving from R7 to H1, going SOUTH
In navigating door behavior
ultra looking for door towards the: SOUTH
MOVE AHEAD MOTOR ACTIVE
Found door - Initialization terminated
MOVE THROUGH DOOR MOTOR ACTIVE
Moved through door - Nominal Behavior terminated
Moving from H1 to H2, going SOUTH
In navigating hall behavior
turning towards the: SOUTH
Turned towards hall - Initialization terminated
looking for hall towards the: EAST
HALL FOLLOW MOTOR ACTIVE
Found hall - Nominal Behavior terminated
Moving from H2 to H5, going EAST
In navigating hall behavior
turning towards the: EAST
Turned towards hall - Initialization terminated
vision looking for door relative: 90 (right side)
HALL FOLLOW MOTOR ACTIVE
Found door (vision) - Nominal Behavior terminated
Moving from H5 to R2, going SOUTH
In navigating door behavior
ultra looking for door towards the: SOUTH
following wall on left (right ground truth)
WALL FOLLOW MOTOR ACTIVE
Found door - Initialization terminated
MOVE THROUGH DOOR MOTOR ACTIVE
Moved through door - Nominal Behavior terminated
Goal location reached!
Figure 9.12
Scenario for moving from R7 to R2. Shaded gateways are extraneous
and discarded by the planner.

9.5
Case Study of Topological Navigation with a
Hybrid Architecture
343
As shown by the output, the Cartographer computes the path shown in
the ﬁrst line. The second line shows that the current navigational task is to
move from R7 to H1. The Task Manager selects navigate-door and it begins
by looking for the door since the user did not specify where it was. By ex-
amining the database, the door is known to be to the south. Therefore, the
behavior initiates the move-ahead behavior in the south direction. When the
door is found, the initialization phase of the abstract behavior is over and the
nominal activity of moving through the door is triggered. Once the robot is
through the door, the nominal behavior is terminated, terminating the entire
abstract behavior as well.
The next navigational task is to move from H1 to H2, again in the direction
south. The task manger selects navigate-hall. The task of moving from H2
to H5 is interesting because it shows that the termination condition for hall
following is different than for H1 to H2. Since H5 is a gateway to a room of
interest, a different perceptual process is used to visually identify the room.
Once the door has been identiﬁed visually, the robot is considered at H5.
The H5-R2 connection instantiates navigate-door. However, in this case, the
ultrasonics has not yet identiﬁed the door opening and so the initialization
phase consists of wall following until the door opening is recognized. Then
the nominal behavior is activated and the robot moves into the room, suc-
cessfully completing the task.
The second simulation shown in Fig. 9.13 uses the sample topological map
provided prior to the competition. The intended path was R1-H1-F1-H8-
-H0-R0, but H8-H0 was blocked. As shown in the ﬁgure, the robot detects
that the path is blocked and uses move to goal to return to H8. The Cartogra-
pher updates the topological map and computes a new route and the robot
resumes execution, in this case H8-F1-H5-H6-H7-H0-R0. The output of the
script below shows the robot conﬁrming the nodes H5 and H6 even though
the plan did not have them enter the associated rooms. These nodes were
kept in the path because if another blocked hall was encountered, the robot
would then be able to return to either R5 or R6 and attempt to use a route
through the door between.
9.5.2
Navigation scripts
The path planning and execution components are clearly deliberative. The
Cartographer is maintaining a map in the form of a graph and is monitoring
progress. The transition table takes the role of a high level sequencer. Scripts

344
9
Topological Path Planning
H9
R2
R3
R4
R5
R6
R7
R8
R9
R0
H1
H2
H3
H4
H5
H6
H7
H8
H0
F1
R1
Navigating Door
Navigating Hall
Move to Last Known
Blocked area
Moving from H8 to H0, going EAST
In navigating hall behavior
turning towards the: EAST
Turned towards hall - Initialization terminated
HALL FOLLOW MOTOR ACTIVE
User stopped or BLOCKED - behavior terminated
In last known location loop
MOVE TO GOAL MOTOR ACTIVE
Moving from H8 to F1, going NORTH
In navigating hall behavior
turning towards the: NORTH
Turned towards hall - Initialization terminated
HALL FOLLOW MOTOR ACTIVE
will follow wall on right (left ground truth) to stay in foyer
Found foyer - Nominal Behavior terminated
Moving from F1 to H5, going EAST
In navigating hall behavior
following wall on left (right ground truth)
trying to exit foyer
looking for hall towards the: EAST
WALL FOLLOW MOTOR ACTIVE
Found hall - Initialization terminated
HALL FOLLOW MOTOR ACTIVE
Found door (vision) - Nominal Behavior terminated
Figure 9.13
Scenario for moving from R1 to R0 with blocked path.

9.5
Case Study of Topological Navigation with a
Hybrid Architecture
345
are used to specify and carry out the implied details of the plan in a modular
and reusable way.
The implementation consisted of three scripts for navigating doors, halls,
and foyers. The pseudocode for navigate-door is shown below in
C
++
style and takes advantage of the indexing properties of the switch statement:
switch(door)
case door-not-found:
//initialization phase
//follow wall until find door
if wall is found
wallfollow to door
else
move-ahead to find a wall
case door-found:
//nominal activity phase
move-thru-door(door-location)
The nominal behavior, move-thru-door, is self-terminating, so there is
no separate termination condition. The perception of the door is the key
determinant in what the robot does.
The navigate-hall is used for traveling between corridors, foyers and
corridors, and from corridors to corridor/door nodes. It has two different
starting conditions. One condition is that the robot is in a foyer and detects a
hall. Since the hall may be somewhat ahead and the hallfollow behavior
assumes the robot is in the hall, the script has the robot employ wallfollow
to ﬁnd the hall, then begin following the hall. The task manager uses the
directional information stored in the database to determine which wall of
the foyer to follow. In the other condition the robot is already in a hall. Since
the robot is not guaranteed to be facing the centerline of the hall (ANBs have
no knowledge of what was done prior to their instantiation except via global
variables), the sub-script turns it to line itself up with the desired hall.
switch(hall)
case not-facing-hall:
//initialization phase
if starting in a FOYER

346
9
Topological Path Planning
if hall-not-found
wallfollow until find the hall
else
if not facing hall
turn to face hall
else starting in a HALL
if not facing hall
turn to face hall
case facing-hall:
//nominal activity phase
hallfollow until next gateway
The navigate-hall ANB terminates when the next expected gateway
(the next node in the path) is found. There are three behaviors which look
for gateways. hallwatch looks for the ultrasonic signature of a hall in the
expected direction; foyerwatch similarly looks for foyers, and the robot
uses vision through the confirm-door behavior to detect the landmark as-
sociated with the room. These behaviors run concurrently with the nominal
behaviors.
The navigate-foyer ANB is used to move the robot between two foy-
ers. It assumes that two foyers’ nodes connected are a connvenient repre-
sentation of a single large foyer with entries from different directions (i.e.,
multiple gateways into the foyer). The script moves the robot
n feet into the
ﬁrst foyer in the direction of the second foyer. This gets the robot away from
potentially confusing ultrasonic signatures. Then the task manager deter-
mines which side of the foyer to wall-follow until the next expected gateway
is detected. There is no case statement in the pseudocode since the sequence
of activities is ﬁxed.
//step 1
move-to-goal(n, dir) in direction of next foyer
//step 2
wallfollow until next gateway is detected
9.5.3
Lessons learned
The CSM team did not place at the AAAI competition; the robot suffered a
series of hardware failures, causing both the power supply and the sonars to

9.5
Case Study of Topological Navigation with a
Hybrid Architecture
347
fail. The robot was eventually ﬁxed and the software worked ﬁne. The tests
before and after the competition offered several practical lessons.
First, it is critical to build the abstract navigation behaviors out of robust
primitives. The biggest problem with the implementation has not been with
the scripts per se, but rather with the quality of the primitive behaviors.
If the robot cannot reliably detect an open doorway while it is following a
wall around a room, it will never exit no matter what the script says. Sec-
ond, contention for sensing resources is an emerging issue for robotic con-
trol schemes.
The video camera on the robot did not have a pan mech-
anism; essentially, Clementine’s directional control is the camera effector.
In one instantiation of the navigate-hall behavior between a H node
and a Hd node, the nominal behavior of hallfollow frequently pointed
Clementine and her camera away from the anticipated location of the door,
interfering with the confirm-door perceptual behavior used to terminate
navigate-hall. Even at slow speeds, Clementine frequently missed the
door. One alternative is to let the two active behaviors take turns with con-
trol of the robot. This leads to a move, stop, sense scenario which slowed the
robot’s progress and still allowed the robot to roll past the door.
Note that the representation and path planning algorithm support the ad-
dition of metric information.
The metric distance between every visited
could be stored in the database. The distance could be attached to the ref-
erence index in the node class, much the same as the nodes connected to the
N, E, S, W are stored. One problem with simply attaching the metric dis-
tance is deciding what distance value to use. If wallfollow takes the robot
around the perimeter of a foyer in order to exit, the distance traveled will not
reﬂect the straight line distance between nodes. Nor is the measure bidirec-
tional; going N-S through the foyer could be signiﬁcantly longer than S-N if
the foyer is asymmetric and the robot follows different walls each time. On
the other hand, if the width of a foyer were known, the robot might possibly
be able to use dead reckoning to move directly across it to the desired exit.
The impact of obstacles lengthening the distance traveled can also be a prob-
lem, especially if the obstacles are people moving down a hall. A short hall
can take a long time to navigate if it is cluttered during one visit, but a short
time during another. Another difﬁculty with adding metric distances is how
to use it effectively. Metric distances may not be known for all pairs of nodes,
making it difﬁcult to apply Dijkstra’s algorithm. Likewise, distance may be
only one factor in selecting a route; our current implementation prefers go-
ing through halls versus taking short cuts between rooms. Utility theory can
be used to quantitify the impact of these competing concerns.

348
9
Topological Path Planning
9.6
Summary
Landmarks simplify the where am I? problem by providing orientation cues.
Gateways are a special case of landmarks which reﬂect a potential for the
robot to change directions (turn down a different road or hall, enter a room,
etc.). There are two categories of qualitative navigation methods: relational
and associative. Relational methods relate distinctive places (nodes) to each
other by the local control strategies (lcs), or behaviors, needed to travel between
them (edges), forming a graph. The robot can use the graph to plan a path us-
ing techniques such as the single source shortest path algorithm. It executes
the path by employing the behavior associated with that edge it is travers-
ing. When it sees the landmark of the location, it is in the neighborhood, and
then can use another behavior, such as hill-climbing, to localize itself rela-
tive to the landmark. Associative methods directly couple perception with
acting. An image can be compared to a image signature or a viewframe to
generate the next movement for the robot to take. Relational methods are
commonly used for topological navigation.
9.7
Exercises
Exercise 9.1
List the four questions associated with navigation.
Exercise 9.2
List and describe the criteria for evaluating a path planner.
Exercise 9.3
Describe the difference between route and metric representations of space.
Exercise 9.4
Deﬁne the difference between natural and artiﬁcial landmarks and give one example
of each.
Exercise 9.5
Deﬁne a gateway. What gateways would be in a museum? What sensors would be
needed to reliably detect them?
Exercise 9.6
Build a relational graph representation labeling the distinctive places and local con-
trol strategies using gateways for a ﬂoor of a building on campus.

9.7
Exercises
349
Exercise 9.7
Describe each in one or two sentences: gateway, image signature, visual homing,
viewframe, orientation region.
Exercise 9.8
Create a topological map showing landmarks, landmark pair boundaries, and orien-
tation regions for an area of campus.
Exercise 9.9
Consider the spatial hierarchy of Kuipers and Byun. Do these three levels ﬁt naturally
within a Hybrid architecture? How would they be implemented in a state-hierarchy
style? A model-oriented style?
Exercise 9.10
Did Minerva use topological navigation? What did it use for landmarks?
Exercise 9.11
[World Wide Web]
Visit the Minerva site as http://www.cs.cmu.edu/Minerva. Write a one-page paper
summarizing the project.
Exercise 9.12
[Advanced Reading]
Read the scientiﬁc papers at the Minerva web site. Describe:
a. The path planner used and evaluate it using the criteria presented in this overview.
b. Discuss the impact of sensor uncertainty on Minerva’s navigation and how it was
addressed.
c. List which of the four functions of spatial memory were used.
Exercise 9.13
[Advanced Reading]
Read the article about the two winners of the 1992 AAAI Mobile Robot Competition
in Congdon et al. 38 Describe each team’s use of landmarks and topological navigation
strategy.
Exercise 9.14
[Programming]
Look up the following algorithms and describe how they work. Can they be used
interchangeably?
a. Dijkstra’s single source shortest path
b. hill-climbing
Exercise 9.15
[Programming]
Design a visual landmark and implement a hill-climbing algorithm to localize the
robot relative to the landmark.
Exercise 9.16
[Programming]
Design 4 unique landmarks.

350
9
Topological Path Planning
a. Program the robot to visit each landmark in any order speciﬁed by a user.
b. Place the landmarks at different locations. Implement Dijkstra’s single source
shortest path algorithm to compute the shortest path between two points speciﬁed
by a user.
c. Implement a minimal spanning tree algorithm to allow the robot to visit all way-
points efﬁciently.
9.8
End notes
Of batteries and topological navigation.
The CSM entry did not place due to massive hardware failures with the power supply
on Clementine. At one point, the team was using the car battery from the school
van parked outside the Seattle Convention Center. We’d drive up, park, I’d take the
battery out in the open, and walk into the convention center, and we’d return it to the
van to drive to their housing in the wee hours of the morning. No security guard or
policeman ever said a word. See the Summer 1995 issue of AI Magazine for an article
on the winners.
Topological navigation examples.
The ﬁgures and printouts of the topological navigation system used by the CSM team
were prepared by Paul Wiebe.

10
Metric Path Planning
Chapter objectives:
 Deﬁne Cspace, path relaxation, digitization bias, subgoal obsession, termination
condition.
 Explain the difference between graph and wavefront planners.
 Represent an indoor environment with a generalized Voronoi graph, a regu-
lar grid, or a quadtree, and create a graph suitable for path planning.
 Apply the A* search algorithm to a graph to ﬁnd the optimal path between
two locations.
 Apply wavefront propagation to a regular grid.
 Explain the differences between continuous and event-driven replanning.
10.1
Objectives and Overview
Metric path planning, or quantitative navigation, is the opposite of topologi-
QUANTITATIVE
NAVIGATION
cal navigation. As with topological methods, the objective is to determine
a path to a speciﬁed goal. The major philosophical difference is that metric
methods generally favor techniques which produce an optimal, according to
some measure of best, while qualitative methods seem content to produce
a route with identiﬁable landmarks or gateways. Another difference is that
metric paths are usually decompose the path into subgoals consisting of way-
WAYPOINTS
points. These waypoints are most often a ﬁxed location or coordinate (x,y).
These locations may have a mathematical meaning, but as will be seen with
meadow maps, they may not have a sensible correspondence to objects or

352
10
Metric Path Planning
landmarks in the world. Topological navigation focused on subgoals which
are gateways or locations where the robot could change its primary heading.
The terms “optimal” and “best” have serious ramiﬁcations for robotics.
In order to say a path is optimal, there is an implied comparison. As will
be seen, some metric methods are able to produce an optimal path because
they consider all possible paths between points. This can be computationally
expensive. Fortunately, some algorithms (especially one named “A*” for rea-
sons that will be discussed later) are more clever about rejecting non-optimal
paths sooner than others.
Surprisingly, an optimal path may not appear optimal to the human eye;
for example, a mathematically optimal path of a world divided into tiles or
grids may be very jagged and irregular rather than straight. The ability to
produce and compare all possible paths also assumes that the planning has
access to a pre-exisiting (or a priori) map of the world. Equally as important,
it assumes that the map is accurate and up to date. As such, metric methods
are compatible with deliberation, while qualitative methods work well with
more reactive systems. As a deliberative function, metric methods tend to
be plagued by the same sorts of difﬁculties that were seen in Hierarchical
systems: challenges in world representation, handling dynamic changes and
surprises, and computation complexity.
Metric path planners have two components: the representation (data struc-
COMPONENTS OF
METRIC PATH
PLANNERS
ture) and the algorithm. Path planners ﬁrst partition the world into a structure
amenable for path planning. They use a variety of techniques to represent
the world; no one technique is dominant, although regular grids appear to
be popular. The intent of any representation is to represent only the salient
features, or the relevant conﬁguration of navigationally relevant objects in
the space of interest; hence the term conﬁguration space. Path planning al-
gorithms generally work on almost any conﬁguration space representation,
although as with any algorithm, some methods work better on certain data
structures. The algorithms fall into two broad categories: those which treat
path planning as a graph search problem, and those which treat path plan-
ning as a graphics coloring problem. Regardless of what algorithm is used,
there is always the issue in a Hybrid architecture of when to use it. This is
sometimes called the issue of interleaving reaction and planning.

10.2
Conﬁguration Space
353
Figure 10.1
Reduction of a 6DOF world space to a 2DOF conﬁguration space.
10.2
Conﬁguration Space
The physical space robots and obstacles exist in can be thought of as the
world space. The conﬁguration space, or Cspace for short, is a data structure
CONFIGURATION SPACE
which allows the robot to specify the position (location and orientation) of
any objects and the robot.
A good Cspace representation reduces the number of dimensions that a
planner has to contend with. Consider that it takes six dimensions (also
called degrees of freedom or DOF) to represent precisely where an object is. A
DEGREES OF FREEDOM
person may specify the location of the object as a
(x;
y
;
z
) coordinate in some
frame of reference. But an object is three-dimensional; it has a front and back,
top and bottom. Three more degrees are needed to represent where the front
of the chair is facing, whether it is tilted or not, or even upside down. Those
are the Euler (pronounced “Oiler”) angles,
;

;
, also known as pitch, yaw,
and roll.
Six degrees of freedom is more than is needed for a mobile ground robot in
most cases for planning a path. The
z (height) coordinate can be eliminated
if every object the robot sits on the ﬂoor. However, the
z coordinate will be
of interest if the robot is an aerial or underwater vehicle. Likewise, the Euler
angles may be unnecessary. Who cares which way the robot is facing if all
the robot wants to do is to plan a path around it? But the pitch of a planetary
rover or slope of an upcoming hill may be critical to a mission over rocky
terrain.
Fig. 10.1 shows a transformation of an object into Cspace. In general, met-
ric path planning algorithms for mobile robots have assumed only two DOF,
including for the robot. For path planning purposes, the robot can be mod-
eled as round so that the orientation doesn’t matter. This implicitly assumes

354
10
Metric Path Planning
the robot is holonomic, or can turn in place. Many research robots are sufﬁ-
HOLONOMIC
ciently close to holonomic to meet this assumption. However, robots made
from radio-controlled cars or golf carts are most certainly not holonomic.
Much work is being done in non-holonomic path planning, where the pose
of the robot must be considered (i.e., can it actually turn that sharply without
colliding?), but there no one algorithm appears to be used in practice. This
chapter will restrict itself to holonomic methods.
10.3
Cspace Representations
The number of different Cspace representations is too large to include more
than a coarse sampling here. The most representative ones are Voronoi di-
agrams, regular grids, quadtrees (and their 3D extension, octrees), vertex
graphs, and hybrid free space/vertex graphs. The representations offer dif-
ferent ways of partitioning free space. Any open space not occupied by an
object (a wall, a chair, an un-navigable hill) is free space, where the robot is
free to move without hitting anything modeled. Each partition can be la-
beled with additional information: “the terrain is rocky,” “this is off-limits
from 9am to 5am,” etc.
10.3.1
Meadow maps
Many early path planning algorithms developed for mobile robots assumed
that the robot would have a highly accurate map of the world in advance.
This map could be digitized in some manner, and then the robot could apply
various algorithms to convert the map to a suitable Cspace representation.
An example of a Cspace representation that might be used with an a priori
map is the meadow map or hybrid vertex-graph free-space model.
MEADOW MAP
Meadow maps transform free space into convex polygons. Convex poly-
gons have an important property: if the robot starts on the perimeter and
goes in a straight line to any other point on the perimeter, it will not go out of
the polygon. The polygon represents a safe region for the robot to traverse.
The path planning problem becomes a matter of picking the best series of
polygons to transit through. Meadow maps are not that common in robotics,
but serve to illustrate the principles of effecting a conﬁguration space and
then planning a path over it.
The programming steps are straightforward. First, the planner begins with
a metric layout of the world space. Most planners will increase the size of
every object by the size of the robot as shown in Fig. 10.2; this allows the

10.3
Cspace Representations
355
Figure 10.2
A a priori map, with object boundaries “grown” to the width of the robot
(shown in gray).
a.
b.
Figure 10.3
Meadow maps: a.) partitioning of free space into convex polygons, and
b.) generating a graph using the midpoints.
planner to treat the robot as if it were a point, not a 2D object. Notice how
from the very ﬁrst step, path planners assume holonomic vehicles.
The next step in the algorithm is to construct convex polygons by consid-
ering the line segments between pairs of some interesting feature. In the case
of indoor maps, these are usually corners, doorways, boundaries of objects,
etc. The algorithm can then determine which combination of line segments
partitions the free space into convex polygons.
The meadow map is now technically complete, but it is not in a format that
supports path planning. Each convex polygon represents a safe passage for
the robot. But there is still some work to be done. Some of the line segments

356
10
Metric Path Planning
Figure 10.4
String tightening as a relaxation of an initial path.
forming the perimeter aren’t connected to another a polygon (i.e., they are
part of a wall), so they should be off limits to the planning algorithm. Also,
as can be seen by the above ﬁgure, some of the line segments are quite long.
It would make a difference to the overall path length where the robot cuts
across the polygon. It is hard for the planner to discretize a continuous line
segment. So the issue becomes how to specify candidate points on the poly-
gon. One solution is to ﬁnd the middle of each line segment which borders
another polygon. Note that each of these midpoints becomes a node, and
if edges are drawn between them, an undirected graph emerges. A path
planning algorithm would determine the best path to take.
One disadvantage of a meadow map, indeed of any Cspace representa-
tion, is evident on inspection of Fig. 10.3: any path which is chosen will be
somewhat jagged. Each of the inﬂection points is essentially a waypoint.
One outcome of the partitioning process is that the free space is divided up
in a way that makes sense geometrically, but not necessarily for a robot to
actually travel. Why go halfway down the hall, then angle off? This may be
mathematically optimal on paper, but in practice, it seems downright silly.
Chuck Thorpe at CMU devised a solution to paths generated from any kind
of discretization of free space.138 Imagine that the path is a string. Now imag-
ine pulling on both ends to tighten the string (the technical name for this is
path relaxation) This would remove most of the kinks from the path without
PATH RELAXATION
violating the safe zone property of convex polygons.
Meadow maps have three problems which limit their usefulness.
One
problem is that the technique to generate the polygons is computationally

10.3
Cspace Representations
357
Figure 10.5
Graph from a generalized Voronoi graph (GVG). (Graph courtesy of
Howie Choset.)
complex. But more importantly, it uses artifacts of the map to determine the
polygon boundaries, rather than things which can be sensed. Unless the ro-
bot has accurate localization, how does it know it is halfway down a long,
featureless hall and should turn
0
? The third major disadvantage is that
it is unclear how to update or repair the diagrams as the robot discovers
discrepancies between the a priori map and the real world.
10.3.2
Generalized Voronoi graphs
Generalized Voronoi Graphs, or GVGs, are a popular mechanism for represent-
GENERALIZED
VORONOI GRAPHS
ing Cspace and generating a graph. Unlike a meadow map, a GVG can be
constructed as the robot enters a new environment, thereby creating a topo-
logical map as shown by Howie Choset at CMU.36
The basic idea of a GVG is to generate a line, called a Voronoi edge, equidis-
VORONOI EDGE
tant from all points. As seen in Fig. 10.5, this line goes down the middle of
hallways and openings. The point where many Voronoi edges meet is known
as a Voronoi vertex. Notice the vertices often have a physical correspondence
to conﬁgurations that can be sensed in the environment. This makes it much
easier for a robot to follow a path generated from a GVG, since there is an
implicit local control strategy of staying equidistant from all obstacles.
If the robot follows the Voronoi edge, it will not collide with any modeled
obstacles, because it is staying “in the middle.” This obviates the need to
grow the obstacle boundaries. Edges serve as freeways or major thorough-

358
10
Metric Path Planning
Figure 10.6
Regular grid.
fares. It should also be noted that the curved edges in a GVG do not matter
to graph theory or graph algorithms. It is only the length, not the physical
reality, of the edges that make any difference.
10.3.3
Regular grids
Another method of partitioning the world space is a regular grid. The regular
grid method superimposes a 2D Cartesian grid on the world space, as shown
in Fig. 10.6. If there is any object in the area contained by a grid element, that
element is marked occupied. Hence, regular grids are often referred to as
occupancy grids. Occupancy grids will be detailed in Ch. 11.
Regular grids are straightforward to apply. The center of each element
in the grid can become a node, leading to a highly connected graph. Grids
are either considered 4-connected or 8-connected, depending on whether they
4-CONNECTED
NEIGHBORS
8-CONNECTED
NEIGHBORS
permit an arc to be drawn diagonally between nodes or not.
Unfortunately, regular grids are not without problems. First, they intro-
duce digitization bias, which means that if an object falls into even the smallest
DIGITIZATION BIAS
portion of a grid element, the whole element is marked occupied. This leads
to wasted space and leads to very jagged objects. To reduce the wasted space,
regular grids for an indoor room are often ﬁnely grained, on the order of 4- to
6-inches square. This ﬁne granularity means a high storage cost, and a high
number of nodes for a path planning algorithm to consider.

10.4
Graph Based Planners
359
Figure 10.7
Quadtree Cspace representation.
10.3.4
Quadtrees
A variant on regular grids that attempts to avoid wasted space is quadtrees.
QUADTREES
Quadtrees are a recursive grid. The representation starts out with grid el-
ements representing a large area, perhaps 64-inches square (8 by 8 inches).
If an object falls into part of the grid, but not all of it, the Cspace algorithm
divides the element into four (hence the name “quad”) smaller grids, each 16-
inches square. If the object doesn’t ﬁll a particular sub-element, the algorithm
does another recursive division of that element into four more sub-elements,
represented a 4-inches square region. A three dimensional quadtree is called
an octree.
10.4
Graph Based Planners
As seen in the previous section, most Cspace representations can be con-
verted to graphs. This means that the path between the initial node and the
INITIAL NODE
goal node can be computed using graph search algorithms. Graph search al-
GOAL NODE
gorithms appear in networks and routing problems, so they form a class of
algorithms well understood by computer science. However, many of those
algorithms require the program to visit each node on the graph to determine
the shortest path between the initial and goal nodes. Visiting every node may
be computationally tractable for a sparsely connected graph such as derived
from a Voronoi diagram, but rapidly becomes computationally expensive for
a highly connected graph such as from a regular grid. Therefore, there has

360
10
Metric Path Planning
been a great deal of interest in path planners which do a “branch and bound”
style of search; that is, ones which prune off paths which aren’t optimal. Of
course, the trick is knowing when to prune!
The A* search algorithm is the classic method for computing optimal paths
for holonomic robots. It is derived from the A search method. In order to
explain how A* works, A search will be ﬁrst presented with an example, then
A*. Both assume a metric map, where the location of each node is known in
absolute coordinates, and the graph edges represent whether it is possible to
traverse between those nodes.
The A search algorithm produces an optimal path by starting at the initial
node and then working through the graph to the goal node. It generates the
optimal path incrementally; each update, it considers the nodes that could
be added to the path and picks the best one. It picks the “right” node to add
to the path every time it expands the path (the "right node” is more formally
known as the plausible move). The heart of the method is the formula (or
evaluation function) for measuring the plausibility of a node:
f
(n)
=
g
(n)
+
h(n)
where:

f
(n) measures how good the move to node
n is

g
(n) measures the cost of getting to node
n from the initial node. Since A
expands from the initial node outward, this is just the distance of the path
generated so far plus the distance of the edge to node
n

h(n) is the cheapest cost of getting from
n to goal
Consider how the formula is used in the example below. Assume that a
Cspace representation yielded the graph in Fig. 10.8.
The A search algorithm begins at node
A, and creates a decision tree-like
structure to determine which are the possible nodes it can consider adding
to its path. There are only two nodes to choose from: B and C.
In order to determine which node is the right node to add, the A search
algorithm evaluates the plausibility of B and C by looking at the edges. The
plausibility of B as the next move is:
f
(B
)
=
g
(B
)
+
h(B
)
=

+
:
=
:
where
g
(B
) is the cost of going from
A to
B, and
h(B
) is the cost to get from
B to the goal
E.

10.4
Graph Based Planners
361
Figure 10.8
Graph for A search algorithm.
The plausibility of
C is:
f
(C
)
=
g
(C
)
+
h(C
)
=

+

=
:0
where
g
(C
) is the cost of going from
A to
C, and
h(C
) is the cost of getting
from
C to
E. Since
f
(C
)
>
f
(B
), the path should go from
A to
C.
But this assumes that
h(n) was known at every node. This meant that the
algorithm had to recurse in order to ﬁnd the correct value of
h(n). This leads
to visiting all the nodes.
A search is guaranteed to produce the optimal path because it compares
all possible paths to each other. A* search takes an interesting approach to
reducing the amount of paths that have to be generated and compared: it
compares the possible paths to the best possible path, even if there isn’t a
path in the real world that goes that way. The algorithm estimates
h rather
than checks to see if there is actually a path segment that can get to the goal in
that distance. The estimate can then be used to determine which nodes are
the most promising, and which paths have no chance of reaching the goal
better than other candidates and should be pruned from the search.
Under A* the evaluation function becomes:
f

(n)
=
g

(n)
+
h

(n)
where the
 means that the functions are estimates of the values that would

362
10
Metric Path Planning
Figure 10.9
An A* example.
have been plugged into the A search evaluation. In path planning,
g

(n)
is the same as
g
(n): the cost of getting from the initial node to n, which is
known through the incremental build of the path.
h

(n) is the real difference.
So what is a way to estimate the cost of going from
n to the goal? Further-
more, how can we be sure that the estimate will be accurate enough that we
don’t end up choosing a path which isn’t truly optimal? This can be done by
making sure that
h

(n) will never be smaller than
h(n). The restriction that
h

(n)

h(n) is called the admissibility condition. Since
h

(n) is an estimate, it
ADMISSIBILITY
CONDITION
is also called a heuristic function, since it uses a rule of thumb to decide which
HEURISTIC FUNCTION
is the best node to consider.
Fortunately, there is a natural heuristic function for estimating the cost
from n to the goal: the Euclidean (straight line) distance. Recall that the
locations of each node are known independently of the edges. Therefore, it
is straightforward to compute the straight line distance between two nodes.
The straight line distance is always the shortest path between two points,
barring curvature of the earth, etc. Since the real path can never be shorter
than that, the admissibility condition is satisﬁed.
To see how A* uses this to actually eliminate visiting nodes, consider the
example in Fig. 10.9. As with A search, the ﬁrst step in A* is to consider the
choices from the initial node.
The choices are
B and
D, which can be thought of as a search tree (see
Fig. 10.10a) or as a subset of the original graph (see Fig 10.10b). Regardless

10.4
Graph Based Planners
363
Figure 10.10
a.) What A* “sees” as it considers a path A-?-E, and b.) the original
graph. c.) What A* “sees” from considering a path A-D-?-E, and d.) the associated
graph.

364
10
Metric Path Planning
of how it is visualized, each node is evaluated to determine which is the
most plausible move. The above ﬁgure shows what the algorithm “sees” at
this point in the execution. The choices evaluate to:
f

(B
)
=
g

(B
)
+
h

(B
)
=

+
:
=
:
f

(D
)
=
g

(D
)
+
h

(D
)
=
:
+
:
=
:
A path going from
A
 D
 ?
 E has the potential to be shorter than a
path going from
A
 B
 ?
 E. So,
D is the most plausible node. Notice
that A* can’t eliminate a path through
B because the algorithm can’t “see” a
path that actually goes from
D to
E and determine if it is indeed as short as
possible.
At step 2, A* recurses (repeats the evaluation) from
D, since
D is the most
plausible, as shown in Fig. 10.10.
The two options from
D are
E and
F, which are evaluated next:
f

(E
)
=
g

(E
)
+
h

(E
)
=
:
+
0
=
:
f

(F
)
=
g

(F
)
+
h

(F
)
=
:
+
:0
=
:
Now the algorithm sees that
E is the best choice among the leaves of the
search tree, including the branch through
B. (If
B was the best, then the
algorithm would have changed branches.) It is better than
F and
B. When it
goes to expand
E, it notices that
E is the goal, so the algorithm is complete.
The optimal path is
A
 D
 E, and we didn’t have to explicitly consider
A
 B
 F
 E. There are other ways to improve the procedure described
so far.
f

(F
) didn’t need to be computed if the algorithm looks at its choices
and sees that one of them is the goal. Any other choice has to be longer
because edges aren’t allowed to be negative, so
D
 F
 E has to be longer
than
D
 E.
Another important insight is that any path between
A and
E has to go
through
D, so the
B branch of the search tree could have been pruned. Of
course, in the above example the algorithm never had an opportunity to no-
tice this because
B was never expanded. It’s easy to imagine in a larger graph
that there might be a case where after several expansions through
D, the leaf
at
B in the search tree came up the most plausible. Then the algorithm would
have expanded
A and seen the choices were
D. Since
D already occurred in
another branch, with a cheaper
g

(D
), the
B branch could be safely pruned.

10.5
Wavefront Based Planners
365
This is particularly useful when A* is applied to a graph created from a reg-
ular grid, where the resulting graph is highly connected.
One very attractive feature of the A* path planner is that it can be used
with any Cspace representation that can be transformed into a graph. The
major impact that Cspace has on the A* planner is how many computations
it takes to ﬁnd the path.
A limitation of A* is that it is very hard to use for path planning where
there are factors other than distance to consider in generating the path. For
example, the straight line distance may be over rocky terrain or sand that
poses a risk to the robot. Likewise, the robot may wish to avoid going over
hills in order to conserve energy, but at the same time might wish to go down
hills whenever possible for the same reason. In order to factor in the impact
of terrain on the path costs, the heuristic function
h

(n) has to be changed.
But recall that the new heuristic function must continue to satisfy the admis-
sibility condition:
h


h. If the new
h
 just takes the worst case energy cost
or safety cost, it will be admissible, but not particularly useful in pruning
paths. Also, gaining energy going downhill is essentially having an edge in
the graph with a negative weight, which A* can’t handle. (Negative weights
pose an interesting problem in that the robot can get into a loop of rolling
down a hill repeatedly because it’s an energy-efﬁcient solution rather than
actually making forward progress! Bellman-Ford types of algorithms deal
with this situation.)
10.5
Wavefront Based Planners
Wavefront propagation styles of planners are well suited for grid types of
representations. The basic principle is that a wavefront considers the Cspace
to be a conductive material with heat radiating out from the initial node to
the goal node. Eventually the heat will spread and reach the goal, if there
is a way, as shown in the sequence in Fig. 10.11. Another analogy for wave-
front planners is region coloring in graphics, where the color spreads out to
neighboring pixels. An interesting aspect of wavefront propagation is that
the optimal path from all grid elements to the goal can be computed as a
side effect. The result is a map which looks like a potential ﬁeld. One of the
many wavefront types of path planners is the Trulla algorithm developed
by Ken Hughes.106 It exploits the similarities with a potential ﬁeld to let the
path itself represent what the robot should do as if the path were a sensor
observation.

366
10
Metric Path Planning
a.
b.
c.
d.
e.
f.
g.
h.
Figure 10.11
A wave propagating through a regular grid. Elements holding the
current front are shown in gray, older elements are shown in dark gray.

10.6
Interleaving Path Planning and Reactive Execution
367
An attractive feature of wavefront propagation is how it treats terrain types.
A modeled obstacle can have a conductivity of 0.0 (no heat can be propagated
through that grid element), while an open area has inﬁnite conductivity. But
undesirable terrains, which are traversable, but not desirable to go across,
can be given a low value of conductivity. This means that heat will travel
slower through undesirable regions. But it may turn out that this is the best
path, even with the loss through the undesirable regions. A wavefront nat-
urally handles the tradeoffs between a longer path through desirable terrain
versus taking shortcuts through less desirable terrain.
The examples in Fig. 10.12 use the Trulla planner. Undesirable terrain is
shown on the Trulla display in gray, obstacles in black and open areas in
white. The intensity of gray reﬂects the degree of undesirability of the ter-
rain. In Fig. 10.12a, the robot Clementine can move over an extension cord
but would prefer not to, as shown by the gray region. The path planner ac-
cordingly routes her around the cord. In Fig. 10.12 this isn’t possible due to
the placement of obstacles. Therefore, she has to cut through.
10.6
Interleaving Path Planning and Reactive Execution
Most path planning algorithms are employed in a strictly plan once, then
reactively execute style. Almost all techniques break a path into segments;
even a wavefront planner actually produces a goal location (waypoint) for
each directional vector. This is well suited for a Hybrid architecture with a
Cartographer handing off path segments, or an entire path, for a Sequencer.
The Sequencer can then employ a series of move-to-goal behaviors, deacti-
vating and re-instantiating the behavior as a new subgoal is met. Unfortu-
nately there are two problems with reactive execution of a metric path as
described above: subgoal obsession and the lack of opportunistic replanning.
SUBGOAL OBSESSION
OPPORTUNISTIC
REPLANNING
Subgoal obsession is when the robot spends too much time and energy trying
to reach the exact subgoal position, or more precisely, when the termination
conditions are set with an unrealistic tolerance. The problem with termina-
tion conditions for subgoals is best deﬁned by an example. Suppose the next
waypoint is at location (35, 50). If the robot has shaft encoders or GPS, this
should be straightforward to do in theory. In practice, it is very hard for
a robot, even a holonomic robot, to reach any location exactly because it is
hard to give the robot precise movement. The robot may reach (34.5, 50). The
behavior sees the goal is now 0.5 meters ahead, and move again to attempt
to reach (35, 50). On that move, it may overshoot and end up at (35.5,50.5).

368
10
Metric Path Planning
a.
b.
Figure 10.12
a.) Trulla output and photo of Clementine navigating around cord. b.)
Trulla output and photo of Clementine navigating over the cord.
Now it has to turn and move again. A resulting see-sawing set of motions
emerges that can go on for minutes. This wastes time and energy, as well as
makes the robot appear to be unintelligent. The problem is exacerbated by
non-holonomic vehicles which may have to back up in order to turn to reach
a particular location. Backing up almost always introduces more errors in
navigation.
To handle subgoal obsession, many roboticists program into their move-to-
goal behaviors a tolerance on the terminating condition for reaching a goal.
A common heuristic for holonomic robots is to place a tolerance of +/- the
width of the robot. So if a cylindrical, holonomic robot with a diameter of 1
meter is given a goal of (35, 50), it will stop when 34.5
<
y
< 35.5 and 49.5

10.6
Interleaving Path Planning and Reactive Execution
369
<
y
< 50.5. There is no common heuristic for non-holonomic robots, because
the maneuverability of each platform is different. A more vexing aspect of
subgoal obsession is when the goal is blocked and the robot can’t reach the
terminating condition. For example, consider a subgoal at the opposite end
of a hall from a robot, but the hall is blocked and there is no way around.
Because the robot is executing reactively, it doesn’t necessarily realize that it
isn’t making progress. One solution is for the Sequencer to estimate a maxi-
mum allowable time for the robot to reach the goal. This can be implemented
either as a parameter on the behavior (terminate with an error code after
n
seconds), or as an internal state releaser. The advantage of the latter solu-
tion is that the code can become part of a monitor, leading to some form of
self-awareness.
Related to subgoal obsession is the fact that reactive execution of plans
often lacks opportunistic improvements. Suppose that the robot is heading
for Subgoal 2, when an unmodeled obstacle diverts the robot from its in-
tended path. Now suppose that from its new position, the robot can perceive
Subgoal 3. In a classic implementation, the robot would not be looking for
Subgoal 3, so it would continue to move to Subgoal 2 even though it would
be more optimal to head straight for Subgoal 3.
The need to opportunistically replan also arises when an a priori map turns
out to be incorrect. What happens when the robot discovers it is being sent
through a patch of muddy ground? Trying to reactively navigate around
the mud patch seems unintelligent because choosing left or right may have
serious consequences. Instead the robot should return control to the Car-
tographer, which will update its map and replan. The issue becomes how
does a robot know it has deviated too far from its intended path and needs
to replan?
Two different types of planners address the problems of subgoal obses-
sion and replanning: the D* algorithm developed by Tony Stentz136 which is
D* ALGORITHM
a variant of the A* replanning algorithm, and an extension to the Trulla al-
gorithm. Both planners begin with an a priori map and compute the optimal
path from every location to the goal. D* does it by executing an A* search
from each possible location to the goal in advance; this converts A* from be-
ing a single-source shortest path algorithm into an all-paths algorithm. This
is computationally expensive and time-consuming, but that isn’t a problem
since the paths are computed when the robot starts the mission and is sitting
still. Since Trulla is a wavefront type of planner, it generates the optimal path
between all pairs of points in Cspace as a side effect of computing the path
from the starting location to the goal.

370
10
Metric Path Planning
start
unmodeled
obstacle
goal
a.
b.
Figure 10.13
Layout showing unmodeled obstacle. a.) Gray line shows expected
path, long dashed line the actual path with Trulla, and short dashed line shows purely
reactive path. b.) Clementine opportunistically turning.
Computing the optimal path from every location to the goal actually helps
with reactive execution of the path. It means that if the robot can localize
itself on the a priori map, it can read the optimal subgoal for move-to-goal
on each update. If the robot has to swing wide to avoid an unmodeled obsta-
cle in Fig. 10.13, the robot automatically becomes redirected to the optimal
path without having to replan. Note how the metric path becomes a virtual
sensor, guiding the move-to-goal behavior replacing the direct sensor data.
This is a rich mechanism for the deliberative and reactive components of
Hybrid architectures to interact.
This approach eliminates subgoal obsession, since the robot can change
“optimal” paths reactively and opportunistically move to a closer waypoint.
As with most things in life, too much of a good thing is bad.
At some
point though, the sheer number of unmodeled obstacles might force the ro-
bot to get trapped or wander about, changing subgoals but making no real
progress. The D* solution to this problem is to continuously update the map
and dynamically repair the A* paths affected by the changes in the map. D*
represents one extreme on the replanning scale: continuous replanning.
CONTINUOUS
REPLANNING
Continuous replanning has two disadvantages. First, it may be too compu-
tationally expensive to be practical for a robot with an embedded processor

10.7
Summary
371
and memory limitations, such as a planetary rover. Second, continuous re-
planning is highly dependent on the sensing quality. If the robot senses an
unmodeled obstacle at time T1, it computes a new path and makes a large
course correction. If it no longer senses that obstacle at time T2 because the
ﬁrst reading was a phantom from sensor noise, it will recompute another
large course correction. The result can be a robot which has a very jerky
motion and actually takes longer to reach the goal.
In the cases of path planning with embedded processors and noisy sensors,
it would be desirable to have some sort of event-driven scheme, where an
EVENT-DRIVEN
REPLANNING
event noticeable by the reactive system would trigger replanning. Trulla uses
the dot-product of the intended path vector and the actual path vector. When
the actual path deviates by
	0
 or more, the dot product of the path vector
and the actual vector the robot is following becomes 0 or negative. Therefore
the dot product acts as an affordance for triggering replanning: the robot
doesn’t have to know why it is drifting off-course, only that it has drifted
noticeably off-course.
This is very good for situations that would interfere with making progress
on the originally computed path, in effect, situations where the real world
is less amenable to reaching the intended goal. But it doesn’t handle the
situation where the real world is actually friendlier. In Fig. 10.14, an obstacle
thought to be there really isn’t. The robot could achieve a signiﬁcant savings
in navigation by opportunistically going through the gap.
Such opportunism requires the robot to notice that the world is really
more favorable than originally modeled. A continuous replanner such as D*
has a distinct advantage, since it will automatically notice the change in the
world and respond appropriately, whereas Trulla will not notice the favor-
able change because it won’t lead to a path deviation. It is an open research
question whether there are affordances for noticing favorable changes in the
world that allow the robot to opportunistically optimize it path.
10.7
Summary
Metric path planning converts the world space to a conﬁguration space, or
Cspace, representation that facilitates path planning. Cspace representations
such as generalized Voronoi diagrams exploit interesting geometric prop-
erties of the environment. These representations can then be converted to
graphs, suitable for an A* search. Since Voronoi diagrams tend to produce
sparser graphs, they work particularly well with A*. Regular grids work

372
10
Metric Path Planning
start
goal
missing
wall
Figure 10.14
Opportunity to improve path. The gray line is the actual path, while
the dashed line represents a more desirable path.
well with wavefront planners, which treat path planning as an expanding
heat wave from the initial location.
Metric path planning tends to be expensive, both in computation and in
storage. However, they can be interleaved with the reactive component of
a Hybrid architecture, where the Cartographer gives the Sequencer a set of
waypoints. Two problems in interleaving metric path planning with reactive
execution are subgoal obsession and when to replan. Optimal path planning
techniques for a priori ﬁxed maps are well understood, but it is less clear how
to update or repair the path(s) without starting over if the robot encounters
a signiﬁcant deviation from the a priori map. One solution is to continuously
replan if resources permit and sensor reliability is high; another is event-
driven replanning which uses affordances to detect when to replan.
Cspace representations and algorithms often do not consider how to rep-
resent and reason about terrain types, and special cases such as the robot ac-
tually conserving or generating energy going downhill are usually ignored.
A possibly even more serious omission is that popular path planners are
applicable for only holonomic vehicles.
10.8
Exercises
Exercise 10.1
Represent your indoor environment as a GVG, regular grid, and quadtree.

10.8
Exercises
373
Exercise 10.2
Represent your indoor environment as a regular grid with a 10cm scale. Write down
the rule you use to decide how to mark a grid element empty or occupied when there
is a small portion overlapping it.
Exercise 10.3
Deﬁne path relaxation.
Exercise 10.4
Consider a regular grid of size 20 by 20. How many edges will a graph have if the
neighbors are
a. 4-connected?
b. 8-connected?
Exercise 10.5
Convert a meadow map into a graph using:
a. the midpoints of the open boundaries, and
b. the midpoints plus the 2 endpoints.
Draw a path from A to B on both graphs. Describe the differences.
Exercise 10.6
Apply the A* search algorithm by hand to a small graph to ﬁnd the optimal path
between two locations.
Exercise 10.7
What is a heuristic function?
Exercise 10.8
Apply wavefront propagation to a regular grid.
Exercise 10.9
Subgoal obsession has been described as a problem for metric planners. Can hybrid
systems which use topological planning exhibit subgoal obsession?
Exercise 10.10
Trulla uses a dot product of 0 or less to trigger replanning, which corresponds to
	0
 from the desired path. What are the advantages or disadvantages of
	0
? What
would happen if Trulla used

 or

?
Exercise 10.11
Describe the difference between continuous and event-driven replanning. Which would
be more appropriate for a planetary rover? Justify your answer.

374
10
Metric Path Planning
Exercise 10.12
[Programming]
Obtain a copy of the Trulla simulator suitable for running under Windows. Model at
least three different scenarios and see what the path generated is.
Exercise 10.13
[Programming]
Program an A* path planner. Compare the results to the results for the Dijkstra’s
single source shortest path program from Ch. 9.
10.9
End Notes
For a roboticist’s bookshelf.
Robot Motion Planning by Jean-Claude Latombe of Stanford University is the best-
known book dealing with conﬁguration space. 82
Voronoi diagrams.
Voronoi diagrams may be the oldest Cspace representation. Computational Geometry:
Theory and Applications 44 reports that the principles were ﬁrst uncovered in 1850, al-
though it wasn’t until 1908 that Voronoi wrote about them, thereby getting his name
on the diagram.
On heuristic functions for path planning.
Kevin Gifford and George Morgenthaler have explored some possible formulations
of a heuristic function for path planning over different terrain types. Gifford also
developed an algorithm which can consider the energy savings or capture associated
with going downhill. 60
Trulla.
The Trulla algorithm was developed by Ken Hughes for integration on a VLSI chip.
Through grants from the CRA Distributed Mentoring for Women program, Eva Noll
and Aliza Marzilli implemented it in software. The algorithm was found to work fast
enough for continuous updating even on an Intel 486 processor. Dave Hershberger
helped write the display routines used in this book.

11
Localization and Map Making
Chapter Objectives:
 Describe the difference between iconic and feature-based localization.
 Be able to update an occupancy grid using either Bayesian, Dempster-
Shafer, or HIMM methods.
 Describe the two types of formal exploration strategies.
11.1
Overview
The two remaining questions of navigation are: where am I? and where have I
been? The answers to these questions are generally referred to as localization
and map-making, respectively. Both are closely related, because a robot cannot
create an accurate map if it does not know where it is. Fig. 11.1 shows a
hallway in black in a building. The hallway makes a complete circuit around
the center of the building. The gray shows the hallway as sensed by a mobile
robot. The mobile robot senses, updates the map with the portions of the
hallway that have come into view, then moves, updates, and so on. In this
case, it uses shaft encoders to determine where it has moved to and how to
update the map.
As can been seen from the ﬁgure, as well as discussions in Ch. 6, shaft
encoders are notoriously inaccurate. Worse yet, the inaccuracies are highly
dependent on surfaces. For example, the robot’s wheel will slip differently
on carpet than on a polished ﬂoor. Developing an error model to estimate the
slippage is often unrealistically difﬁcult. The shaft encoder problem might
appear to be eliminated by new hardware technology, especially GPS and
MEMS (micro electrical-mechanical systems) inertial guidance systems (INS).

376
11
Localization and Map Making
Figure 11.1
A map of a circuit of a hallway created from sonars by a Nomad 200
showing the drift in localization. The ground truth is in black.
However, GPS only works reliably outdoors. The signal is often unobtain-
able indoors, in tunnels, or in cities with large buildings (sometimes referred
to as urban canyons). MEMS inertial navigation devices are small, but suf-
URBAN CANYONS
fer from signiﬁcant inaccuracies and have not been packaged in a way to be
easily used with robots.
Researchers have attempted to solve the localization problem in a number
of ways. The ﬁrst approach was to simply ignore localization errors. While
this had the advantage of being simple, it eliminated the use of global path
planning methods. This was part of the motivation and appeal of purely re-
active systems, which had a “go until you get there” philosophy. Another
approach was to use topological maps, which have some symbolic informa-
tion for localization at certain points such as gateways, but don’t require con-
tinuous localization. Unfortunately, for reasons discussed in Ch. 9, it is hard
to have unique gateways. The move to topological mapping gave rise to a
whole subﬁeld of reasoning about indistinguishable locations.
More sophisticated systems either identiﬁed natural landmarks which had
noticeable geometric properties or added artiﬁcial landmarks. One robot
proposed for a Canadian mining company intended to navigate through rel-
atively featureless mine shafts by dropping beacons at different intersections,
much like Hansel and Gretel dropping cookie crumbs for a path. (This in-

11.1
Overview
377
spired much humorous discussion of the merits of the biological equivalent
of robot droppings and robots imitating animals that “mark” their territory
in the wild.) Other techniques attempted to match the raw sensor data to
an a priori map using interpretation trees or similar structures. One of the
many problems with these techniques is that the sensor data rarely comes in
a form amenable to matching against a map. Consider attempting to match
noisy sonar data to the layout of a room. In the end, the basic approach used
by most systems is to move a little, build up a small map, match the new map
to the last map, and merge it in, then merge the small map with the overall
map. The use of small, local maps for localization brings the process back
full circle to the need for good map-making methods.
Localization algorithms fall into two broad categories: iconic and feature-
based. Iconic algorithms appear to be the more popular in practice, in part
because they usually use an occupancy grid. Occupancy grids are a mech-
anism for fusing sensor data into a world model or map. Fusion is done
either following an algorithm provided by a formal theory of evidence, ei-
ther Bayesian or Dempster-Shafer, or by a popular quasi-evidential method
known as HIMM. Since occupancy grids fuse sensor data, the resulting map
does not contain as much sensor noise. Many Hybrid architectures also use
the occupancy grid as a virtual sensor for obstacle avoidance.
The chapter ﬁrst covers occupancy grids, which are also known as cer-
tainty and evidence grids. Since sonars are a popular range sensor for map-
ping and obstacle avoidance, the chapter next covers sonar sensor models
and the three methods for using sensor models to update a grid: Bayesian,
Dempster-Shafer, and HIMM. The Bayesian and Dempster-Shafer methods
can be used with any sensor, not just range from sonar. The comparison
of the three methods discusses practical considerations such as performance
and ease in tuning the method for a new environment. Iconic localization
is described next. It is useful for metric map building and generally uses
an occupancy grid-like structure. Feature-based localization, which is better
suited for topological map building, is discussed next. Feature-based meth-
ods have become popular with the advent of partially ordered Markov deci-
sion process (POMDP) methods to simplify reasoning about them; POMDPs
are outside the scope of this book but the basic localization strategy is pre-
sented. The chapter ends with a brief description of frontier and Voronoi
methods of using the data in an occupancy grid to direct exploration of an
unknown environment.

378
11
Localization and Map Making
11.2
Sonar Sensor Model
All methods of updating uncertainty require a sensor model. Models of sen-
sor uncertainty can be generated in a number of ways. Empirical methods for
generating a sensor model focus on testing the sensor and collecting data as
to the correctness of the result. The frequency of a correct reading leads to
a belief in an observation; the set of beliefs from all possible observations
form the model. Analytical methods generate the sensor model directly from
an understanding of the physical properties of the device. Subjective methods
rely on a designer’s experience, which are often an unconscious expression
of empirical testing.
One robotic sensor which has been heavily studied is the Polaroid ultra-
sonic transducer, or sonar. This chapter will use Polaroid sonars as an exam-
ple; however, the principles of scoring and fusing belief apply to any sensor.
Most roboticists have converged on a model of sonar uncertainty which looks
like Fig. 11.2, originally presented in Ch. 6.
The basic model of a single sonar beam has a ﬁeld of view speciﬁed by
, the
SONAR MODAL
PARAMETERS
half angle representing the width of the cone, and
R, the maximum range it
can detect. This ﬁeld of view can be projected onto a regular grid. The grid
will be called an occupancy grid, because each element l (for eLement) in the
OCCUPANCY GRID
ELEMENT L
grid will hold a value representing whether the location in space is occupied
or empty. As shown in Fig. 11.2, the ﬁeld of view can be divided into three
regions:
Region I: where the affected elements are probably occupied (drawn as a
“hill”),
Region II: where the affected elements are probably empty (drawn as a “val-
ley”), and
Region III: where the condition of the affected elements is unknown (drawn
as a ﬂat surface).
Given a range reading, Region II is more likely to be really empty than Re-
gion I is to be really occupied. Regardless of empty or occupied, the readings
are more likely to be correct along the acoustic axis than towards the edges.
Recall that this is in part because an obstacle which was only along one edge
would be likely to reﬂect the beam specularly or generate other range errors.
While the sensor model in Fig 11.2 reﬂects a general consensus, there is
much disagreement over how to convert the model into a numerical value

11.2
Sonar Sensor Model
379
a.
I
III
II
β
R
IV
b.
Figure 11.2
A sensor model for a sonar: a.) three dimensional representation and
b.) two dimensional representation projected onto an occupancy grid.

380
11
Localization and Map Making
Figure 11.3
Neptune, a robot using occupancy grids during the early 1980’s. (Pho-
tograph courtesy of Hans Moravec.)
for belief. Each of the three methods covered in the following sections does
the translation slightly differently.
11.3
Bayesian
The most popular evidential method for fusing evidence is to translate sen-
sor readings into probabilities and to combine probabilities using Bayes’ rule.
Elfes and Moravec at Carnegie Mellon University pioneered the probabilis-
tic approach in the early 1980’s. Later Moravec turned to a form of Bayes’
Rule which uses probabilities expressed as likelihoods and odds.95 This has
some computational advantages and also side-steps some of the problems
with priors. The likelihood/odds formulation is equivalent to the traditional

11.3
Bayesian
381
approach presented here. In a Bayesian approach, the sensor model gener-
ates conditional probabilities of the form
P
(sjH
). These are then converted
to
P
(H
js) using Bayes’ rule. Two probabilities, either from two different sen-
sors sensing at the same time or from two different times, can be fused using
Bayes’ rule.
11.3.1
Conditional probabilities
To review, a probability function scores evidence on a scale of 0 to 1 as to
PROBABILITY
FUNCTION
whether a particular event H (H stands for “hypothesis”) has occurred given
an experiment. In the case of updating an occupancy grid with sonar read-
ings, the experiment is sending the acoustic wave out and measuring the
time of ﬂight, and the outcome is the range reading reporting whether the
region being sensed is Occupied or Empty.
Sonars can observe only one event: whether an element
g
r
id[i][j
] is Occu-
pied or Empty. This can be written H
=
fH
;
:H
g or H
=
fO
ccupied;
E
mpty
g.
The probability that
H has really occurred is represented by
P
(H
):
0

P
(H
)


An important property of probabilities is that the probability that
H didn’t
happen,
P
(:H
), is known if
P
(H
) is known. This is expressed by:

 P
(H
)
=
P
(:H
)
As a result, if
P
(H
) is known,
P
(:H
) can be easily computed.
Probabilities of the form
P
(H
) or
P
(:H
) are called unconditional probabil-
UNCONDITIONAL
PROBABILITIES
ities. An example of an unconditional probability is a robot programmed to
explore an area on Mars where 75% of the area is covered with rocks (obsta-
cles). The robot knows in advance (or a priori) that the next region it scans
has
P
(H
=
O
ccupied)
=
0:.
Unconditional probabilities are not particularly interesting because they
only provide a priori information. That information does not take into ac-
count any sensor readings,
S. It is more useful to a robot to have a func-
tion that computes the probability that a region
g
r
id[i][j
] is either Occupied
or Empty given a particular sensor reading
s. Probabilities of this type are
called conditional probabilities.
P
(H
js) is the probability that
H has really oc-
CONDITIONAL
PROBABILITIES
curred given a particular sensor reading
s (the “ j” denotes “given”). Uncon-
ditional probabilities also have the property that
P
(H
js)
+
P
(:H
js)
=
:0.

382
11
Localization and Map Making
In an occupancy grid,
P
(O
ccupiedjs) and
P
(E
mpty
js) are computed for
each element,
g
r
id[i][j
], that is covered by a sensor scan. At each grid ele-
ment, the tuple of the two probabilities for that region is stored. A tuple can
be implemented as a C struct
typedef struct {
double
occupied;
double
empty;
} P;
P occupancy_grid[ROWS][COLUMNS];
Probabilities provide a representation for expressing the certainty about a
region
g
r
id[i][j
]. There still needs to be a function which transfers a partic-
ular sonar reading into the probability for each grid element in a way that
captures Fig. 11.2. One set of functions which quantify this model into prob-
abilities is given below.
For every grid element falling into Region I:
P
(O
ccupied)
=
(
R r
R
)
+
(

 

)


M
ax
occupied
P
(E
mpty
)
=
:0
 P
(O
ccupied)
(11.1)
where
r and
 are the distance and angle to the grid element, respectively.
The

 

term in Eqn. 11.1 captures the idea that the closer the grid element
is to the acoustic axis, the higher the belief. Likewise, the nearer the grid ele-
ment is to the origin of the sonar beam, the higher the belief (the
R r
R
term).
The
M
ax
occupied term expresses the assumption that a reading of occupied is
never fully believable. A
M
ax
occupied
=
0:	 means that a grid element can
never have a probability of being occupied greater than 0.98.
It is important to note that Region I in Fig. 11.2 has a ﬁnite thickness. Due
to the resolution of the sonar, a range reading of 0.87 meters might actually be
between 0.82 and 0.92 meters, or 0.870.05 meters. The
0:0 is often called
a tolerance. It has the impact of making Region I wider, thereby covering
TOLERANCE
more grid elements.
Each grid element in Region II should be updated using these equations:
P
(O
ccupied)
=
:0
 P
(E
mpty
)
P
(E
mpty
)
=
(
R r
R
)
+
(

 

)

(11.2)

11.3
Bayesian
383
s=6
R=10
β=15
r=3.5
α=0
Figure 11.4
Example 1: Updating an element in Region II (sonar reading of 6).
Note that unlike an element in Region I, an element in Region II can have
a probability of being empty of 1.0.
To see how these formulas would be applied, consider the example in
Fig. 11.4. The sonar has returned a range reading of 6.0 feet with a tolerance
of
0: feet. The
M
ax
occupied value is 0.98. The robot is shown on a grid,
and all elements are measured relative to it. The element of interest
g
r
id[i][j
]
is shown in black, and is at a distance
r
=
: feet and an angle of

=
0

from the robot. In a computer program,
r and
 would be computed from
the distance and arctangent between the element of interest and the element
representing the origin of the sonar, but for the sake of focus, these examples
will give
r and
.
The ﬁrst step is to determine which region covers the element. Since
:
<
(:0
 0:), the element is in Region II. Therefore, the correct formulas to
apply are those in Eqn. 11.2:
P
(E
mpty
)
=
(
R r
R
)+(

 

)

=
(
0 :
0
)+(
 0

)

=
0:
P
(O
ccupied)
=
:0
 P
(E
mpty
)
=

 0:
=
0:
The example in Fig. 11.5 shows an element in Region I. The probability for
the element in black is computed the same way, only using the equations for
that region.

384
11
Localization and Map Making
r=6
α=5
R=10
β=15
s=6
Figure 11.5
Example 2: Updating a element in Region I (sonar reading at 6).
P
(O
ccupied)
=
(
R r
R
)+(

 

)


M
ax
occupied
=
(
0 
0
)+(
 

)


0:	
=
0:
P
(E
mpty
)
=
:0
 P
(O
ccupied)
=

 0:
=
0:
11.3.2
Conditional probabilities for
P
(H
js)
The sensor model represents
P
(sjH
): the probability that the sensor would
return the value being considered given it was really occupied. Unfortu-
nately, the probability of interest is
P
(H
js): the probability that the area
at
g
r
id[i][j
] is really occupied given a particular sensor reading. The laws
of probability don’t permit us to use the two conditionals interchangeably.
However, Bayes’ rule does specify the relationship between them:
P
(H
js)
=
P
(sjH
)P
(H
)
P
(sjH
)P
(H
)
+
P
(sj:H
)P
(:H
)
(11.3)
Substituting in
O
ccupied for
H, Eqn. 11.3 becomes:
P
(O
ccupiedjs)
=
P
(sjO
ccupied) P(Occupied)
P
(sjO
ccupied) P(Occupied)
+
P
(sjE
mpty
) P(Empty)
(11.4)
P
(sjO
ccupied) and
P
(sjE
mpty
) are known from the sensor model. The
other terms,
P
(O
ccupied) and
P
(E
mpty
), are the unconditional probabili-
ties, or prior probabilities sometimes called priors. The priors are shown in

11.3
Bayesian
385
Eqn. 11.4 in boxes. If these are known, then it is straightforward to convert
the probabilities from the sonar model to the form needed for the occupancy
grid.
In some cases, such as for a planetary rover, there may be some knowledge
that produces the prior probabilities. In most cases, that knowledge isn’t
available. In those cases, it is assumed that
P
(O
ccupied)
=
P
(E
mpty
)
=
0:. Using that assumption, the probabilities generated for the example in
Fig. 11.5 can be transformed as follows.
For
g
r
id[i][j
]:
P
(s
=
jO
ccupied)
=
0:
P
(s
=
jE
mpty
)
=
0:
P
(O
ccupied)
=
0:
P
(E
mpty
)
=
0:
Substituting into Eqn. 11.4 yields:
P
(O
ccupiedjs
=
)
=
(0:)(0:)
(0:)(0:)
+
(0:)(0:)
=
0:
P
(E
mpty
js
=
)
=
(0:)(0:)
(0:)(0:)
+
(0:)(0:)
=
0:
The use of 0.5 for the priors made
P
(O
ccupiedjs) numerically equivalent
to
P
(sjO
ccupied), but in general
P
(H
js)
=
P
(sjH
).
11.3.3
Updating with Bayes’ rule
Now that there is a method for computing conditional probabilities of the
correct form, the question becomes how to fuse it with other readings. The
ﬁrst update is simple. Each element in the occupancy grid is initialized with
the a priori probability of being occupied or empty. Recall that this is gener-
ally implemented as a data structure consisting of two ﬁelds. If the a priori
probabilities are not known, it is assumed
P
(H
)
=
P
(:H
)
=
0:. The ﬁrst
observation affecting
g
r
id[i][j
] can use Bayes’ rule to compute a new proba-
bility and replace the prior
P
(H
)
=
0: with the new value.
But what about the second observation? Or an observation made from
another sonar at the same time? It turns out that in both cases, Bayes’ rule

386
11
Localization and Map Making
can be used iteratively where the probability at time
t
n  becomes the prior
and is combined with the current observation ( t
n).
To see this, consider the following derivation. For
n multiple observations,
s

;
s

;
:
:
:
s
n, Bayes’ rule becomes:
P
(H
js

;
s

;
:
:
:
s
n
)
=
P
(s

;
s

;
:
:
:
s
n
jH
)P
(H
)
P
(s

;
s

;
:
:
:
s
n
jH
)P
(H
)
+
P
(s

;
s

;
:
:
:
s
n
j:H
)P
(:H
)
(11.5)
This introduces the problem of generating
P
(s

;
s

;
:
:
:
s
n
jH
). Ideally, this
requires a sonar model of getting occupied and empty values for all
g
r
id[i][j
]
with
n combinations of sensor readings. Fortunately, if the reading from
s

can be considered the result of a different experiment than
s
 and the others,
P
(s

;
s

;
:
:
:
s
n
jH
) simpliﬁes to
P
(s

jH
)P
(s

jH
)
:
:
:
P
(s
n
jH
). Now, the pro-
gram only has to remember all previous
n
  readings. Since there is no way
of predicting how many times a particular grid element will be sensed, this
creates quite a programming problem. The occupancy grid goes from being
a two dimensional array with a single two ﬁeld structure to being a two di-
mensional array with each element a potentially very long linked list. Plus,
whereas Eqn. 11.3 involved 3 multiplications, updating now takes
(n
 )
multiplications. The computational overhead begins to be considerable since
an element in a hallway may have over 100 observations.
Fortunately, by clever use of
P
(H
js)P
(s)
=
P
(sjH
)P
(H
), a recursive ver-
sion of Bayes’ rule can be derived:
P
(H
js
n
)
=
P
(s
n
jH
)P
(H
js
n 
)
P
(s
n
jH
)P
(H
js
n 
)
+
P
(s
n
j:H
)P
(:H
js
n 
)
(11.6)
So at each time a new observation is made, Eqn. 11.6 can be employed and
the result stored at
g
r
id[i][j
]. The rule is commutative, so it doesn’t matter in
what order two or more simultaneous readings are processed.
11.4
Dempster-Shafer Theory
An alternative theory of evidence is Dempster-Shafer theory which produces
results similar to Bayesian probabilities. It is a much newer theory, origi-
nating in the work of A.P. Dempster, a mathematician at Harvard, during
the 1960’s with extensions by Glen Shafer in 1987.126 Whereas Bayes’ rule
relies on evidence being represented by probability functions, Dempster-
Shafer theory represents evidence as a possibilistic belief function. Possibilis-
tic means that the function represents partial evidence. For example, a reading

11.4
Dempster-Shafer Theory
387
may provide direct evidence for an event H, but, due to occlusions, it may
not be perceiving the entire object. Therefore, there is a possibility that the
evidence could be higher than was reported. The possibilistic belief func-
tions, also called Shafer belief functions, are combined used Dempster’s rule
of combination. The rule of combination is very different from Bayes’ rule,
although they provide similar results. Unlike Bayes’ rule, Dempster’s rule
has a term which indicates when multiple observations disagree. This con-
ﬂict metric can be used by the robot to detect when its occupancy grid may
be subject to errors.
11.4.1
Shafer belief functions
Belief is represented by Shafer belief functions in Dempster-Shafer theory.
The belief functions serve the same purpose as probabilities in Bayesian evi-
dential reasoning, although they are quite different in ﬂavor. Instead of mea-
suring the probability of a proposition, belief functions measure the belief
mass,
m. Each sensor contributes a belief mass of 1.0, but can distribute that
mass to any combination of propositions. This can be illustrated by a direct
comparison with probabilities.
A probability function quantiﬁes the evidence for a set of outcomes, H
=
fH
;
:H
g. A belief function calls the set of propositions the frame of discern-
ment, signifying what can be discerned (or observed) by an observer or sen-
sor. The frame of discernment is either abbreviated by FOD or represented
by capital theta,
. The frame of discernment for an occupancy grid is:

=
fO
ccupied;
E
mpty
g
Unlike in probability theory, H
=
 does not have to be composed of mu-
tually exclusive propositions. A belief function can represent that the sensor
had an ambiguous reading, that it literally doesn’t know what is out there.
The sensor can distribute some of its quanta of belief mass to the proposition
that the area is occupied, but it can also mark a portion of its belief mass to
being unable to tell if the area is occupied or empty.
The number of all possible subsets that the belief mass can be distributed to
by a belief function is

 or 2 raised to the power of the number of elements
in the set
. For the case of an occupancy grid, the possible subsets are:
fO
ccupiedg,
fE
mpty
g,
fO
ccupied;
E
mpty
g, and the empty set
;. Belief that
an area is
fO
ccupied;
E
mpty
g means that it is either Occupied or Empty. This
is the same set as
, and represents the “don’t know” ambiguity (if any)
associated with a sensor observation. The term dontknow will be used instead

388
11
Localization and Map Making
of
 to reduce confusion when belief mass is assigned to the proposition
fO
ccupied;
E
mpty
g.
A belief function
B
el must satisfy the following three conditions:
1.
B
el
(;)
=
0: This prohibits any belief to be assigned to the empty set
; or
“nothing.” A sensor may return a totally ambiguous reading, but it did
make an observation. The practical ramiﬁcation for occupancy grids is
that there are only 3 elements for belief:
O
ccupied,
E
mpty, and
.
2.
B
el
()
=
: This speciﬁes the quantum of belief to be 1. Just as with
Bayesian probabilities where
P
(H
)
+
P
(:H
)
=
:0, Condition 2 means
that
B
el
(H
)
+
B
el
(:H
)
+
B
el
()
=
:0.
3. For every positive integer
n and every collection
A

;
:
:
:
;
A
n of subsets of
,
B
el
(A

:
:
:
A
n
)

X
I
f;:::
;ng;I
=;
( )
jI
j+
B
el
(
\
iI
A
i
)
This says that more than one belief function contributing evidence over
 can be summed, and that the resulting belief in a proposition can be
higher after the summation.
To summarize, a belief function representing the belief that an area
g
r
id[i][j
]
is expressed as a tuple with three members (unlike the two in probabilities),
occupied, empty, dontknow. The belief function can be written as:
B
el
=
m(O
ccupied);
m(E
mpty
);
m(dontk
now
)
An occupancy grid using belief functions would have a data structure sim-
ilar to the typedef struct P used in a Bayesian grid. One possible im-
plementation is:
typedef struct {
double
occupied;
double
empty;
double
dontknow;
} BEL;
BEL occupancy_grid[ROWS][COLUMNS];

11.4
Dempster-Shafer Theory
389
An interesting property of belief functions is that the total belief mass
can be assigned to
dontk
now, or
m(dontk
now
)
=
:0. This means the ob-
server is completely uncertain, and the belief function is humorously called
the vacuous belief function. The vacuous belief function is equivalent to the
VACUOUS BELIEF
FUNCTION
P
(H
)
=
P
(:H
)
=
0: assignment in Bayesian probabilities. It is also used to
initialize the occupancy grid if there is no a priori belief.
11.4.2
Belief function for sonar
Returning the sonar model in Sec. 11.2, the Shafer belief function for a sonar
reading can be expressed as:
For Region I:
m(O
ccupied)
=
(
R r
R
)
+
(

 

)


M
ax
occupied
m(E
mpty
)
=
0:0
(11.7)
m(dontk
now
)
=
:00
 m(O
ccupied)
For Region II:
m(O
ccupied)
=
0:0
m(E
mpty
)
=
(
R r
R
)
+
(

 

)

(11.8)
m(dontk
now
)
=
:00
 m(E
mpty
)
The main conceptual difference between the probability and the Shafer
belief function is that any uncertainty in the reading counts as belief mass for
“don’t know.”
Returning to the examples in Figs. 11.4 and 11.5, the computations are
largely the same. For Example 1:
m(O
ccupied)
=
0:0
m(E
mpty
)
=
(
R r
R
)+(

 

)

=
(
0 :
0
)+(
 0

)

=
0:
m(dontk
now
)
=
:0
 m(E
mpty
)
=

 0:
=
0:
Resulting in:
B
el
=
m(O
ccupied)
=
0:0;
m(E
mpty
)
=
0:;
m(dontk
now
)
=
0:

390
11
Localization and Map Making
For Example 2:
m(O
ccupied)
=
(
R r
R
)
+
(

 

)


M
ax
occupied
=
(
0 
0
)
+
(
 

)


0:	
=
0:
m(E
mpty
)
=
0:0
m(dontk
now
)
=
:0
 m(O
ccupied)
=

 0:
=
0:
Resulting in:
B
el
=
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:
The belief function produced by the sonar model is now ready to be com-
bined with any other observations. It is the equivalent of
P
(H
js). Recall that
the sonar model generated the probability of
P
(sjH
) and Bayes’ rule had to
be applied with an assumption of
P
(H
)
=
P
(:H
) to convert it
P
(H
js).
11.4.3
Dempster’s rule of combination
In theory there are many possible ways to combine belief functions. The
most popular is the original rule, called Dempster’s rule of combination or the
orthogonal sum. Dempster’s rule treats combining two belief functions,
B
el

and
B
el
, as if they represented a physical intersection. Dempster’s rule is
very similar to Bayes’ rule in that it can be applied to any two belief functions
as long as they are independent. This means that the rule can be applied
to observations about
l from overlapping sonars or readings made at two
different times.
Dempster’s rule of combination is notationally dense, so a graphical ex-
ample will be given ﬁrst, followed by the formal mathematical expression.
Consider the case where two observations about
g
r
id[i][j
] need to be com-
bined. Both observations believe that
g
r
id[i][j
] is in Region I. Suppose the
two belief functions are:
B
el

=
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:
B
el

=
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:
Fig. 11.6a shows that the two belief functions can be represented as a num-
berline of length 1.0 corresponding to the one quanta of belief mass. The

11.4
Dempster-Shafer Theory
391
0.0
0.4
1.0
occupied
don’t know
0.0
0.6
1.0
occupied
don’t know
Bel 1
Bel 2
a.
0.0
0.4
1.0
occupied
don’t know
0.0
0.6
1.0
occupied
don’t know
Bel 1
Bel 2
Occupied 
U
Occupied 
= Occupied
Occupied 
U
don’t know 
= Occupied
don’t know 
U
Occupied 
= Occupied
don’t know 
U
don’t know 
= don’t know
b.
0.0
0.4
1.0
occupied
don’t know
0.0
0.6
1.0
occupied
don’t know
Bel 1
Bel 2
Occupied 
U
Occupied 
= Occupied
Occupied 
U
don’t know 
= Occupied
don’t know 
U
Occupied 
= Occupied
don’t know 
U
don’t know 
= don’t know
0.6 X 0. 6 = 0.36
0.4 X 0. 6 = 0.24
0.4 X 0. 4 = 0.16
0.6 X 0. 4 = 0.24
c.
0.0
0.76
1.0
occupied
don’t
know
Bel 3
d.
Figure 11.6
Graphical description of Dempster’s rule of combination. a.) Two belief
functions as numberlines, b.) transformed into axes of a unit square, c.) associated
belief mass, and d.) resultant belief function.
numberline can be divided up into the mass associated with each focal ele-
ment; note that since the masses all add up to 1.0, the order of the elements
on the line does not matter.
In Dempster’s rule, the two numberlines form orthogonal axes, forming
a square of unit area 1.0. The interior of the square can be divided into
subregions representing the belief in the focal element produced by the set
intersection from the two axes.
The set intersections are shown in Fig. 11.6b. Note that there are four sub-

392
11
Localization and Map Making
regions: three for
O
ccupied and one for
dontk
now. Since
dontk
now is the
set

=
fO
ccupied;
E
mpty
g, the intersection of
dontk
now and
O
ccupied is
O
ccupied. The subregions of a square of unit 1.0 area can be projected onto a
numberline of length one. This means that the “area” of belief mass created
by the intersections of the two belief functions forms a third belief function!
The belief mass for
O
ccupied is taken by summing the areas of each of the
three subregions, as seen in Fig. 11.6c. Therefore, the orthogonal sum is:
B
el

=
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0
m(dontk
now
)
=
0:
B
el

=
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:
B
el

=
B
el


B
el

=
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:
Now consider the case where the two readings are contradictory:
B
el

=
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:
B
el

=
m(O
ccupied)
=
0:0;
m(E
mpty
)
=
0:;
m(dontk
now
)
=
0:
As shown in Fig. 11.7, there is now a region where the intersection of Oc-
cupied and Empty occurs. Since these are mutually exclusive propositions,
the resulting set intersection is the empty set
;. The emergence of a subre-
gion associated with
; is a problem. Recall from the deﬁnition of a Shafer
belief function that no belief mass can be assigned to
;. But if the area of 0.24
associated with
; is simply left out, the resulting combined belief function
will not equal 1.0.
Dempster’s rule solves this problem by normalizing the belief function; the
area for
; is distributed equally to each of the non-empty areas. Each area
gets a little bigger and they now all sum to 1.0. The normalization can be
carried out by noticing that the belief mass for a particular proposition
C
is really the sum of all
k areas with set
C divided by the total area of valid

11.4
Dempster-Shafer Theory
393
0.0
0.4
1.0
occupied
don’t know
0.0
0.6
1.0
empty
don’t know
Bel 1
Bel 2
Occupied 
U
Empty 
= 
Empty 
U
don’t know 
= Empty
don’t know 
U
Occupied 
= Occupied
don’t know 
U
don’t know 
= don’t know
0.6 X 0. 6 = 0.36
0.4 X 0. 6 = 0.24
0.4 X 0. 4 = 0.16
0.6 X 0. 4 = 0.24
φ
Figure 11.7
Example of the need for normalization.
belief. For the previous example the total area of valid belief was 1.0, so the
belief for a set was computed as:
m(C
)
=
P
k
(m(C
k
)
:0
If a portion of that area belongs to
; (or there are
p areas for
;), then the
total area is
:0
 m(;):
m(C
)
=
P
k
(m(C
k
)

 P
p
m(;
p
)
Normalizing leads to:
m(O
ccupied)
=
0:
 0:
=
0:
m(E
mpty
)
=
0:
 0:
=
0:
m(dontk
now
)
=
0:
 0:
=
0:
This idea of summing up the areas for a particular proposition and nor-
malizing can be captured with a mathematical formula known as Dempster’s

394
11
Localization and Map Making
rule. It speciﬁes the combined probability mass assigned to each
C
k, where
C is the set of all subsets produced by
A
\
B. The rule is:
m(C
k
)
=
P
A
i
\B
j
=C
k
;C
k
=;
m(A
i
)m(B
j
)

 P
A
i
\B
j
=;
m(A
i
)m(B
j
)
(11.9)
where the focal elements of
B
el
 and
B
el
 are:
B
el

=
A
=
fA

;
:
:
:
;
A
i
g
B
el

=
B
=
fB

;
:
:
:
;
B
j
g
The computation is repeated
k times, once for every distinct subset that
emerged from the orthogonal sum, and results in
B
el

=
m(C

);
m(C

);
:
:
:
m(C
k
)
For the case of any occupancy grid, there are only three possible focal ele-
ments (Occupied, Empty, and dontknow). Dempster’s rule reduces to:
m(O
ccupied)
=
P
A
i
\B
j
=O
ccupied
m(A
i
)m(B
j
)

 P
A
i
\B
j
=;
m(A
i
)m(B
j
)
m(E
mpty
)
=
P
A
i
\B
j
=E
mpty
m(A
i
)m(B
j
)

 P
A
i
\B
j
=;
m(A
i
)m(B
j
)
m(don
0
tk
now
)
=
P
A
i
\B
j
=dontk
now
m(A
i
)m(B
j
)

 P
A
i
\B
j
=;
m(A
i
)m(B
j
)
11.4.4
Weight of conﬂict metric
Normalizing contradictory evidence may produce a justiﬁable measure of
evidence, but a robot needs to be aware of such discordances. Instead, the
renormalization term can be viewed as a measure of the conﬂict between the
pair of belief functions. The larger the area assigned to
;, the more disagree-
ment between the two beliefs about the FOD. Shafer deﬁnes such a measure
in the weight of conﬂict metric,
C
on:126
C
on(B
el

;
B
el

)
=
log(


 
);
w
her
e

=
X
A
i
\B
j
=;
m

(A
i
)m

(B
j
)
(11.10)
C
on takes a value between
0 and
; as

!
0:0,
C
on
!
0:0, and as

!
:0,
C
on
!
. It is additive, which means that the conﬂict from a summation of

11.5
HIMM
395
more than two belief functions can be measured. Recent work by Murphy 102
has begun to use
C
on to indicate when more sensing is necessary to disam-
biguate readings.
C
on can be stored as a fourth ﬁeld in the BEL structure,
but most Dempster-Shafer implementations of occupancy grids do not.
11.5
HIMM
The Histogrammic in Motion Mapping (HIMM) algorithm developed by Boren-
stein and Koren23 at the University of Michigan provides a different ap-
proach to scoring whether a particular element in an occupancy grid is oc-
cupied or empty. The motivation for HIMM stemmed from a need to im-
prove obstacle avoidance. In order to safely run their Cybermotion robot,
CARMEL, at its top speed of 0.8 meters/sec, they needed to update an occu-
pancy grid on the order of 160ms. The processor was a 20MHz 80386, fast for
1992, but still quite slow. The Bayesian model at that time was well accepted
but the sheer number of computations involved in updating prevented the
algorithm from a satisfactory fast execution.
HIMM was developed as a
quasi-evidential technique where it scores certainty in a highly specialized
way suitable for sonars.
The University of Michigan’s robotics team entered CARMEL in the 1992
AAAI Mobile Robot Competition, shown in Fig. 11.5. CARMEL resound-
ingly won ﬁrst place that year in the task of navigating between a series of
waypoints marked by landmarks on posts. By using HIMM in conjunction
with the vector ﬁeld histogram (VFH) obstacle avoidance algorithm, CARMEL
was able to navigate at velocities an order of magnitude higher than the other
entries, and avoided obstacles of all sizes more reliably. After that, HIMM
and occupancy grids became standard on many platforms.
11.5.1
HIMM sonar model and updating rule
HIMM uses a simple sonar model shown in Fig. 11.9. The model has two
striking features in contrast to the sonar model in Fig. 11.2. First, only el-
ements along the acoustic axis are updated. This eliminates up to 90% of
the grid elements that are updated by Bayesian and Dempster-Shafer meth-
ods, thereby signiﬁcantly reducing the order complexity. It is important to
note that the sonar reading is the same for Bayesian, Dempster-Shafer, and
HIMM; HIMM interprets the information from the sonar reading differently
and over fewer elements than the other methods. Second, the uncertainty
score is expressed as an integer from 0 to 15, which means each element

396
11
Localization and Map Making
Figure 11.8
CARMEL at the 1992 AAAI Mobile Robot Competition. (Photograph
courtesy of David Kortenkamp and AAAI.)
can be represented by a byte, rather than a structure containing at least two
ﬂoating point numbers.
The sonar model also shows the updating rule. In HIMM, the
l are incre-
mented with
I each time the acoustic axis of a sonar reading passes over it.
If an
l is empty, the increment
I is -1. Occupied
l is incremented with
I = +3.
This implies that HIMM believes a region is occupied more than it believes
the intervening regions are empty; this is the opposite of evidential meth-
ods. The update rule also is computationally efﬁcient; it reduces scoring to
addition and subtraction of integers, which takes far fewer CPU clock cycles
to execute than multiplication and division of ﬂoating point numbers. The

11.5
HIMM
397
III
I
II
-1
-1
-1
+3
-1
Figure 11.9
HIMM sonar model.
basic formula is given below:
g
r
id[i][j
]
=
g
r
id[i][j
]
+
I
where
0

g
r
id[i][j
]


I
=

I
+ if occupied
I
  if empty
(11.11)
While it should be clear that HIMM executes much more quickly than true
evidential methods, it may not be clear why it would produce more reliable
occupancy grids for navigation. Updating along the acoustic axis leads to
a small sample size, as shown in Fig. 11.10. This means a wall (shown in
gray dots) would be appear on the grid as a series of one or more small,
isolated “posts.” There is a danger that the robot might think it could move
between the posts when in fact there was a wall there. If the robot moves,
the gaps get ﬁlled in with subsequent readings. HIMM works best when the
robot is moving at a high velocity. If the velocity and grid update rates are
well matched, the gaps in the wall will be minimized as seen in Fig. 11.10a.
Otherwise, some gaps will appear and take longer to be ﬁlled in, as shown
in Fig. 11.10b. HIMM actually suffers in performance when the robot moves
slowly because it sees too little of the world.
Fig. 11.11 shows the application of Eqn. 11.11 to an occupancy grid for a
series of 8 observations for a single sonar. In the example, the HIMM sonar
model is shown as a rectangle with darker outlines. For simplicity, the range

398
11
Localization and Map Making
t1
t2
t3
t1
t2
t3
Figure 11.10
HIMM updating example for a wall a.) when velocity and update rates
are well matched, and b.) when update rate is slower than velocity, leading to holes.
returned by the sonar is always 3 grid elements long: therefore 1 element will
be scored as occupied and 2 elements will scored as empty. Also, the robot’s
motions are such that the sonar is always facing “up” and aligned with the
grid. The grid is initialized to 0, indicating that the robot assumes that the
world is empty.
11.5.2
Growth rate operator
A disadvantage of HIMM is that the scattered sampling means that a par-
ticular element
g
r
id[i][j
] may get only one or two updates. This has two
ramiﬁcations. First, it means the values in the grid tend to be low; an el-
ement that is occupied and only has two updates has a score of 6, which
is less than half of the maximum score of 15 for occupancy. Second, small
obstacles such as poles or chairs which present a small proﬁle to the sonar
versus a wall never receive a high score. One way to handle this is to change
the scoring increments. If this is done, the updating process tends to produce
maps with many phantom obstacles.
Another approach is to consider nearby grid elements. A heuristic is that
the more neighbors an element has which are occupied, the more likely there
is something really there and its score should be increased. Note that this
is handled in evidential methods by having a sonar model with a large
.

11.5
HIMM
399
0 0 0 0 0
0 0 0 0 0
3 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 3 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 6 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 0 0 0 0
5 6 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 3 0 0 0
5 5 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 3 3 0 0
5 5 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 3 0 0
3 3 2 0 0
5 5 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 3 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 6 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 0 0 0 0
5 6 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 3 0 0 0
5 5 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 3 2 0 0
5 5 0 0 0
0 0 0 0 0
0 0 0 0 0
reading 1
reading 2
reading 3
reading 4
reading 5
reading 6
reading 7
reading 8
Figure 11.11
HIMM updating for a series of 8 observations.
HIMM instead uses a companion mechanism called a Growth Rate Operator
GROWTH RATE
OPERATOR
or GRO to capture this heuristic.
Figure 11.12 shows the GRO and how it works. When an element
g
r
id[i][j
]
is updated with an occupied reading, GRO is applied as an extra step. The
GRO uses a mask,
W, to deﬁne “neighbors.” Masks are a common data struc-
MASK
ture in robotics and computer vision used to deﬁne an area of interest on an
array, grid, or image. The mask is literally a small array overlaid on the oc-

400
11
Localization and Map Making
(0.5x0)+(0.5x2)+(0.5x3)+
(0.5x3)+(1.0x3)+(0.5x7)+
(0.5x15)+(0.5x9)+(0.5x1)=23
0
2
3
3
15
9
1
7
0
0
2
3
3
15
9
1
7
0
0
2
3
3
15
9
1
7
15
0
2
3
3
15
9
1
7
0
2. Apply the mask & compute:
The value has a ceiling of 15:
minimum(15, 23) = 15
3. Replace the element with the new value
1. Identify the element to be
     updated as occupied. Apply I=+3
0
2
3
3
15
9
1
7
0
0
2
3
3
15
9
1
7
0
0
2
3
3
15
9
1
7
0
0
2
3
3
15
9
1
7
0
0.5
0.5
0.5
0.5
0.5
0.5
0.5
0.5
1.0
0
2
3
3
15
9
1
7
0
0
2
3
3
15
9
1
7
0
0
2
3
3
15
9
1
7
3
0
2
3
3
15
9
1
7
0
Figure 11.12
The GRO mechanism. Only the effects of incrementing an occupied
element are shown.

11.5
HIMM
401
cupancy grid. The center of the mask ﬁts directly over
g
r
id[i][j
]. In this case,
the mask is a 3x3 mask; it has 3 columns and 3 rows. A 3x3 mask represents
that
g
r
id[i][j
] has 8 neighbors that should be considered. Masks of larger
sizes can be applied using the same method; larger sizes will grow the score
of clusters of occupied regions much quicker since it considers the inﬂuence
of more neighbors.
While the more occupied neighbors
g
r
id[i][j
] has should increase the score,
the increase should also be affected by neighbor’s score. For example, the fact
that
l has an occupied neighbor should increase the score, but if the score of
the neighbor is low,
g
r
id[i][j
]’s score shouldn’t increase very much. Therefore
each element of the mask contains a weighting factor. In Fig. 11.12, the mask
W has a center with 1.0 and the surrounding elements are 0.5. When
g
r
id[i][j
]
is updated as occupied, the update now has essentially two steps:
g
r
id[i][j
]
=
g
r
id[i][j
]
+
I
This is the standard HIMM updating, but now it only produces a tempo-
rary value for
g
r
id[i][j
] that will be used in the next step. The second step is
to apply the weighting speciﬁed by the mask,
W.
g
r
id[i][j
]
=
X
p;q
= :::
g
r
id[i][j
]

W
[i
+
p][j
+
q
]
Note that the mask
W is given indices from
 
:
:
:
. This is telling the
program to start with the element 1 row “up” and 1 column “to the left” of
the grid element. This expands to:
g
r
id[i][j
]
=
(g
r
id[i
 ][j
 ]

0:)
+
(g
r
id[i
 ][j
]

0:)
+
(g
r
id[i
 ][j
+
]

0:)
+
(g
r
id[i][j
 ]

0:)
+
(g
r
id[i
 ][j
]

:0)
+
(g
r
id[i][j
+
]

0:)
+
(g
r
id[i
+
][j
 ]

0:)
+
(g
r
id[i
+
][j
]

0:)
+
(g
r
id[i
+
][j
+
]

0:)
Fig. 11.13 shows the difference in the ﬁnal occupancy grid if GRO is used
instead of the basic HIMM in Fig. 11.11. The resulting occupancy grid now
shows that there is deﬁnitely an obstacle present. In 8 readings, GRO was
able to reﬂect the presence of an obstacle that the basic HIMM updating rule
could have only approached with 40 readings.

402
11
Localization and Map Making
0 0 0 0 0
0 0 0 0 0
3 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 6 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 12 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
12 0 0 0 0
5 12 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
1215 0 0 0
5 11 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
1215 15 0 0
5 11 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 15 0 0
1215 14 0 0
5 11 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
3 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 6 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
6 12 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
12 0 0 0 0
5 12 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
1215 0 0 0
5 11 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
1215 15 0 0
5 11 0 0 0
0 0 0 0 0
0 0 0 0 0
reading 1
reading 2
reading 3
reading 4
reading 5
reading 6
reading 7
reading 8
Figure 11.13
Updating with GRO, following Fig. 11.11.
The advantage of GRO is that it is very sensitive to potential obstacles. But
that leads to its disadvantage: obstacles tend to be bigger and persist longer
than is appropriate. Obstacles tend to be bigger because sonar errors and
errors in localization make the readings around a small object much larger.
Since the noisy readings are near the true readings, GRO scores them very
high, adding to the perceived size of the object. As long as the robot is not
trying to navigate in cluttered, narrow areas, this is not a signiﬁcant problem.

11.6
Comparison of Methods
403
11.6
Comparison of Methods
Occupancy grid methods have their unique advantages and disadvantages.
Bayesian and Dempster-Shafer theory are formal theories, and other read-
ings from other sensor modalities, such as range from stereo or a laser, can be
easily fused as long as there is a sensor model. HIMM is limited to sonars but
it has signiﬁcant computational advantages. As seen in Fig. 11.14, all three
produce similar occupancy grids, with a slight advantage going to Bayesian
and Dempster-Shafer grids. In practice, Bayesian and Dempster-Shafer have
fewer parameters to tune, making them more straightforward to adapt to
new environments.
11.6.1
Example computations
The similarities and differences between the three methods is best seen by
an example. The following example covers how to initialize the occupancy
grid, compute the score at a grid element for a sensor reading, update the
grid, and repeat for three different observations.
Step 1: Initialize the Occupancy Grid.
Consider a robot beginning to map a new area. The occupancy grid shown
in Fig. 11.15 covers an area of 12 units by 10 units. The grid is an array of
size 24 x 21, with 2 grid elements per unit of distance. The grid starts in
an initial unsensed state. In a Bayesian approach, each element in the grid
would be a structure P with two ﬁelds:
P
(O
ccupied) and
P
(E
mpty
). The
value of each ﬁeld depends on the unconditional probability that the area
represented by the grid is occupied or empty. Unless there is some prior
knowledge, the assumption is that an element has equal chances of being oc-
cupied or empty. This translates to
P
(O
ccupied)
=
P
(E
mpty
)
=
0:. Every
element in the grid would start with (0.5, 0.5). In a Dempster-Shafer im-
plementation, each element in the grid would be a structure Bel with three
ﬁelds:
m(O
ccupied);
m(E
mpty
) and
m(dontk
now
). Since the grid represents
areas that have not been sensed, the entire belief mass
m is initialized as
m(dontk
now
)
=
:0. Every element in the grid would start with (0.0, 0.0,
1.0). Every element in a HIMM occupancy grid would be a single 8-bit inte-
ger, and would be initialized to 0.
Consider how three different sonar updates create a certainty value for a
particular grid element,
g
r
id[][0], shown in Fig. 11.15. At time
t
, the sonar

404
11
Localization and Map Making
a.
b.
c.
Figure 11.14
Comparison of occupancy grids generated from the same set of read-
ings in a favorable hallway by a.) Bayesian, b.) Dempster-Shafer, and c.) HIMM. The
Bayesian and Dempster-Shafer maps show the path of the robot, while the HIMM
map does not. The HIMM map is slightly offset due to show spurious readings.

11.6
Comparison of Methods
405
R=10
tolerance =+/- 0.5
Max_occupied = 0.98
β=15
t1
t3
[0][0]
[23][20]
Sonar model parameters:
t2
Figure 11.15
Example occupancy grid size 24 x 21 elements, showing the robot’s
position and range reading for three updates. (Only the acoustic axis is shown.)
reading returns 9.0. The robot is at
g
r
id[][0], so
g
r
id[][0] is 9 units away
from the robot ( r
=
	) with an
 of 0
.
The ﬁrst step in updating a particular grid element is to see if the sonar
reading affects that grid element. For Bayesian and Dempster-Shafer uncer-
tainty models, the grid element must satisfy two conditions:

jj

j
j, the grid element is within the angular ﬁeld of view of the sonar
model, and

r

s
+
tol
er
ance, the grid element is within the upper bound of the range
reading.
Based on those tests,
g
r
id[][0] is affected by the reading at
t
. Since HIMM
considers only grid elements on the acoustic axis, the key tests are:


=
0, the grid element is on the acoustic axis of the sonar model, and

r

s
+
tol
er
ance, the grid element is within the upper bound of the range
reading.
g
r
id[][0] passes these tests, so it is also affected by the sonar model in
HIMM.

406
11
Localization and Map Making
Step 2: Compute the uncertainty of the observation.
The second step is to compute the uncertainty score of an observation, re-
membering that a grid element will only have a score if it is covered by the
sonar reading. This computation can be done in a series of sub-steps. The
process begins by determining whether
g
r
id[][0] falls in Region I or Re-
gion II, since this speciﬁes which equations or increment to use. Region I,
O
ccupied, extends for
s

tol
er
ance. The test for falling in Region I is the
same for all three methods: if
r satisﬁes
s
 tol
er
ance

r

s
+
tol
er
ance,
then it is in Region I. In this case,
s
=
	,
tol
er
ance
=
0:, and
r
=
	, and the
substitution results in
	
 0

	

	
+
0: being true. Therefore
g
r
id[][0] is
in Region I.
At this point, the three methods diverge in computing the “score” from
the reading at
t
. The next step in Bayesian methods is to compute the prob-
ability,
P
(sjO
ccupied), that the sensor
s will correctly report that
g
r
id[][0]
is
O
ccupied if there is really something at
s
=
	. This is done using Eqn. 11.1:
P
(sjO
ccupied)
=
(
R r
R
)
+
(

 

)


M
ax
occupied
=
(
0 	
0
)
+
(
 0

)


0:	
=
0:
P
(sjE
mpty
)
=
:0
 P
(sjO
ccupied)
=
:0
 0:
=
0:
Dempster-Shafer theory uses Eqn. 11.8, which produces essentially the
same score for the sensor reading as with the Bayesian method:
m(O
ccupied)
=
(
R r
R
)
+
(

 

)


M
ax
occupied
=
(
0 	
0
)
+
(
 0

)


0:	
=
0:
m(E
mpty
)
=
0:0
m(dontk
now
)
=
:00
 m(O
ccupied)
=
:0
 0:
=
0:
The HIMM score is the
I term in Eqn. 11.11. Since
g
r
id[][0] is in Region 1
of the HIMM sonar model,
I
=
I
+
=
+.

11.6
Comparison of Methods
407
Step 3: Update the current score with the past score and update the occu-
pancy grid.
The third step in the updating process is to combine the score for the grid
element from the current reading with the previous score stored there.
The Bayesian method uses the recursive form of Bayes’ rule, Eqn. 11.6:
P
(H
js
n
)
=
P
(s
n
jH
)P
(H
js
n 
)
P
(s
n
jH
)P
(H
js
n 
)
+
P
(s
n
j:H
)P
(:H
js
n 
)
(11.12)
where
s
n  terms are from the reading stored at
g
r
id[][0] and the
s
n terms
are the current reading. Substituting
O
ccupied for
H and
E
mpty for
:H,
P
(s
n
jH
) becomes:
P
(s
t

jO
)
=
0:
P
(s
t

jE
)
=
0:
P
(s
t
0
jO
)
=
0:0
P
(s
t
0
jE
)
=
0:0
This yields:
P
(O
js
t

)
=
P
(s
t

jO
)P
(O
js
t
0
)
P
(s
t

jO
)P
(O
js
t
0
)
+
P
(s
t

jE
)P
(E
js
t
0
)
=
(0:)(0:0)
(0:)(0:0)
+
(0:)(0:0)
=
0:
P
(E
js
t

)
=

 P
(O
js
t

)
=
0:
The Dempster-Shafer updating takes the belief function stored at
g
r
id[][0]
at time
t
0 and combines it with the current belief function. This can be written
recursively as:
B
el
0
t
n
=
B
el
t
n

B
el
0
t
n 
(11.13)
The
0 means that the value is the one after any combination, so
B
el
0
t
n 
is the value currently stored at
g
r
id[i][j
] and
B
el
0
t
n is the value that will be
stored at
g
r
id[i][j
] after the update. The method uses Dempster’s rule of
combination given in Eqn. 11.9 and reprinted below:
m(C
k
)
=
P
A
i
\B
j
=C
k
;C
k
=;
m(A
i
)m(B
j
)

 P
A
i
\B
j
=;
m(A
i
)m(B
j
)
(11.14)

408
11
Localization and Map Making
0.0
1.0
don’t know
0.0
0.56
1.0
occupied
don’t know
Bel’ 1
t
Bel 1
t
don’t know 
U
don’t know 
= don’t know
don’t know 
U
Occupied 
= Occupied
a.
0.0
1.0
don’t know
0.0
0.69
1.0
occupied
don’t know
Bel’ 1
t
Bel 2
t
don’t know 
U
don’t know 
= don’t know
don’t know 
U
Occupied 
= Occupied
occupied
Occupied 
U
Occupied
= Occupied
Occupied 
U
don’t know 
= Occupied
0.54
b.
0.0
1.0
don’t
know
0.0
0.50
1.0
empty
don’t know
Bel’ 2
t
Bel 3
t
don’t know 
U
don’t know 
= don’t know
don’t know 
U
Empty 
= Empty
occupied
Occupied 
U
don’t know 
= Occupied
0.86
Occupied 
U
Empty
= φ
c.
Figure 11.16
Applying Dempster’s rule after three updates for the example.
The ﬁrst step in applying Dempster’s rule is determine the focal elements
C
k in the combined belief
B
el that result from the set intersection of the focal
elements
A
i from
t
n  and
B
j from
t
n. Although this can be done analyti-
cally, it is easier to sketch the unit square out, as shown in Fig. 11.16a.
The total belief has only two focal elements,
O
ccupied and
dontk
now. There
is no belief mass for
;. The computation of the belief mass is:
m(O
ccupied)
=
(:0)(0:)
:0
 0:0
=
0:

11.6
Comparison of Methods
409
m(dontk
now
)
=
(:0)(0:)
:0
 0:0
=
0:
Therefore, the updated belief at
g
r
id[][0] is:
m(O
ccupied)
=
0:;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:
(11.15)
The HIMM updating is governed by Eqn. 11.11, below. Since
g
r
id[][0] is
in the HIMM occupied region, the increment term
I is
I
+
=
.
g
r
id[][0]
=
g
r
id[][0]
+
I
+
=
0
+

=

Step 4: Repeat Steps 2 and 3 for each new reading.
At
t
, the robot has translated forward. The sonar reading is 6.0. The grid
element is now 6.0 units away with an

=
0
.
g
r
id[][0] still falls in Region
I for all three updating methods. The application of Eqn. 11.1 produces a
probability of
P
(sjO
ccupied)
=
0:	;
P
(sjE
mpty
)
=
0:, a belief function of
m(O
ccupied)
=
0:	;
m(E
mpty
)
=
0:0;
m(dontk
now
)
=
0:, and a HIMM
increment of +3.
Updating the grid produces an increase in the occupancy score, as would
be expected, given two direct sonar observations of an object. The Bayesian
updating is:
P
(O
js
t

)
=
P
(s
t

jO
)P
(O
js
t

)
P
(s
t

jO
)P
(O
js
t

)
+
P
(s
t

jE
)P
(E
js
t

)
=
(0:	)(0:)
(0:	)(0:)
+
(0:)(0:)
=
0:
P
(E
js
t

)
=

 P
(O
js
t

)
=
0:
The Dempster-Shafer updating generates a higher occupancy score, which
can also be seen from Fig. 11.16b. The ﬁnal score is:
m(O
ccupied)
=
(0:)(0:	)
+
(0:)(0:	)
+
(0:)(0:)
:0
 0:0
=
0:
m(dontk
now
)
=
(0:)(0:)
:0
 0:0
=
0:
m(E
mpty
)
=
0:0

410
11
Localization and Map Making
The HIMM updating scheme is simple where:
g
r
id[][0]
=
g
r
id[][0]
+

=

+

=

At
t
, the sonar returns a value of 8.5 units. The robot has moved to the
side and rotated; it is now 6.7 units from the grid element with an
 of

.
In this case
g
r
id[][0] is in Region II for the Bayesian and Dempster-Shafer
sonar models, and is not affected by the HIMM model at all.
The probability score for the Bayesian model is computed using Eqn. 11.2
instead of Eqn. 11.1:
P
(sjE
mpty
)
=
(
R r
R
)
+
(

 

)

=
(
0 :
0
)
+
(
 

)

=
0:0
P
(sjO
ccupied)
=
:0
 P
(sjE
mpty
)
=
:0
 0:0
=
0:0
The result happens to be an almost even probability that
g
r
id[][0] is oc-
cupied. This probability is then substituted into Bayes rule (Eqn. 11.6) with
the previously stored probability:
P
(O
js
t

)
=
P
(s
t

jO
)P
(O
js
t
0
)
P
(s
t

jO
)P
(O
js
t
0
)
+
P
(s
t

jE
)P
(E
js
t
0
)
=
(0:0)(0:)
(0:0)(0:)
+
(0:0)(0:)
=
0:
P
(E
js
t

)
=

 P
(O
js
t

)
=
0:
The Dempster-Shafer belief function is computed using Eqn. 11.9, yielding
m(O
ccupied)
=
0:0;
m(E
mpty
)
=
0:0);
m(dontk
now
)
=
0:0). The differ-
ence between the probability and belief function is that the
:E
mpty score
was assigned to
P
(sjO
ccupied) in the Bayesian method and to
m(dontk
now
)
in the Dempster-Shafer. The combination is shown in Fig. 11.16c, and pro-
duces:
m(O
ccupied)
=
(0:)(0:)
:0
 (0:)(0:)
=
0:

11.6
Comparison of Methods
411
m(dontk
now
)
=
(0:)(0:)
:0
 (0:)(0:)
=
0:
m(E
mpty
)
=
(0:)(0:)
:0
 (0:)(0:)
=
0:
Since
g
r
id[][0] is not affected by the HIMM sonar model for the reading
at
t
, there is no update.
The above computations can be summarized as follows. The score for
g
r
id[][0] at each observation is:
sonar
Bayesian
Dempster-Shafer
HIMM
certainty:
P
(sjO
)
P
(sjE
)
m(O
)
m(E
)
m( dontk
now)
t

0.54
0.46
0.56
0.00
0.46
+3
t

0.69
0.31
0.69
0.00
0.31
+3
t

0.50
0.50
0.00
0.50
0.50
n/a
Notice the differences in the Bayesian and Dempster-Shafer scores. The
numbers are the same, but where those numbers go is quite different. At
t
, both methods score the occupancy of the grid element as 0.69. But the
Bayesian scores the emptiness as 0.31, while Dempster-Shafer doesn’t com-
mit to the area being empty; rather it can’t tell if it is empty or occupied. At
t
, there is no HIMM score because
g
r
id[][0] is not covered by the HIMM
sonar model’s ﬁeld of view.
The updated value of
g
r
id[][0] after each observation, that is, the combi-
nation of the current score with the previous score, is:
after
Bayesian
Dempster-Shafer
HIMM
update:
P
(O
js)
P
(E
js)
m(O
)
m(E
)
m( dontk
now)
t

0.54
0.46
0.54
0.00
0.46
3
t

0.72
0.28
0.86
0.00
0.14
6
t

0.72
0.28
0.76
0.12
0.12
6
Notice that the end results of the Bayesian and Dempster-Shafer fusion
methods are very similar, though the intermediate values are different. In
the HIMM, the value of
g
r
id[][0] after
t
 is 6 because nothing is done to it
after
t
; it is neither incremented nor decremented.
11.6.2
Performance
Fig. 11.14 shows the three methods used to generate occupancy grids for
data collected from the same hallway. Performance scores are easy to com-
pute. The ground truth is expressed as a “perfect” occupancy grid, manually

412
11
Localization and Map Making
constructed with occupied points receiving 1.0 or 15 values and empty grid
elements 0. The performance can be computed as the sum of the differences
on a grid element by grid element basis:
scor
e
=
X
i;j
jtr
uth[i][j
]
 g
r
id[i][j
]j
(11.16)
Low scores mean there was less difference between the perfect map and the
sensed map.
Bayesian and Dempster-Shafer theory produce essentially the same re-
sults. This is not surprising since they both use the same model of uncer-
tainty for sonars. The major difference between Bayesian and Dempster-
Shafer is the weight of conﬂict metric.
HIMM tends to be less accurate than Bayesian or Dempster-Shafer theory,
as would be expected from a method that only updates along the acoustic
axis. But HIMM has almost an order of magnitude fewer elements to update
on average after each reading, making it much faster to execute.
Two solutions are available to improve Bayesian and Dempster-Shafer per-
formance. The ﬁrst is to convert all ﬂoating point numbers to integers. The
area of coverage can be dynamically adapted as a function of the robot speed.
When the robot is going fast, it can’t afford to spend much time updating the
occupancy grid. At that point, it becomes reasonable to update only along
the acoustic axis. Errors due to the obstacle not being on the axis will be
smoothed out as the robot quickly moves to a new position and receives up-
dates. In this case, the
 term in the sensor model changes as a function of
speed. Murphy, Gomes, and Hershberger did a comparison of Dempster-
Shafer and HIMM with variable
; their results showed that the adaptive
approach produced better grids than a ﬁxed
 or HIMM.103
11.6.3
Errors due to observations from stationary robot
All three methods produce incorrect results if the robot is stationary and re-
peatedly returns the same range reading. HIMM is particularly vulnerable
to errors due to incorrect readings reinforcing extreme values on the grid.
Due to updating on the acoustic axis, only a very small part of the world is
updated after each observation. As a result, the robot sees a wall as a set of
isolated poles. If the wall is far enough away, the gaps between “poles” can
be quite large, causing the robot to attempt to head through them and then
have to avoid as subsequent updates prove the previous map wrong. If the
robot is experiencing incorrect or missing readings from specular reﬂection

11.6
Comparison of Methods
413
or crosstalk, obstacles might be missed or appear at the wrong distance. If
the robot is stationary, HIMM will generate high belief in an incorrect map.
Bayesian and Dempster-Shafer theory also suffer from the same defect. Since
they usually cover a larger area, the problem with gaps in walls is usually
avoided. But problems with phantom readings still cause incorrect maps.
The plots of the rate of accrual of belief show that multiple identical read-
ings will cause the robot to quickly believe that its occupancy grid is correct.
Once
P
(H
jS
) or
m(H
) reach 1.0, there is no revision downward. HIMM can
revise belief because it subtracts strictly based on the current reading. But
HIMM must have a new, contradictory reading to cause this to happen.
The reason Bayesian and Dempster-Shafer methods degenerate when the
robot is stationary and receives multiple, identical readings is because the
assumption that the observations are independent has been violated. If the
robot is at the same location sensing the same object, the value of reading
S
t
n+ is likely to be the same as
S
t
n. Since the robot hasn’t moved, the ob-
servations cannot be considered to be taken from two different experiments
or by two different observers. This serves as a cautionary note about making
simplifying assumptions: it is important to understand when those assump-
tions lead to counterproductive results.
11.6.4
Tuning
Fig. 11.14 shows the performance of the three updating methods for a hall-
way with signiﬁcant specular reﬂection. All three methods show the hallway
as being wider than it really is. This would be a serious problem for naviga-
tion and obstacle avoidance. The sensor noise was not eliminated by the use
of an occupancy grid. In many cases, a large amount of sensor noise can be
eliminated by tuning the model and updating algorithms.
Therefore an important criterion for an algorithm is how easily it can be
tuned for a particular environment. For example, in environments which
provoke a high degree of specular reﬂection in sonars, a

<

 is often
used to reduce the registration of noise in the occupancy grid. Why put false
readings into the grid over a large area that will take several contradictory
readings to eliminate? It can often take one or more days to tune a set of
sonars which were producing near perfect occupancy grids in a laboratory
for a new building.
Occupancy grids can be tuned for a task environment in at least three
ways. One way is to leave all the algorithms the same but concentrate on
3 WAYS TO TUNE
adjusting the physical equipment. For example, the time of ﬂight of sound

414
11
Localization and Map Making
depends on the density of air, so if a robot is going from sea level to high
mountains, an adjustment factor can be added to the raw readings. Another
approach is to change the threshold on what the robot considers a “really
occupied” region. Lowering the threshold makes the interpretation of the oc-
cupancy map more conservative; more occupied regions that may be phan-
toms are treated as if they were real. This typically doesn’t work well in
cluttered or narrow environments because the robot can get blocked in by
false readings. Increasing the threshold can make the robot less sensitive to
small occupied regions which may not get many readings. Finally, a com-
mon solution is to slow the robot’s velocity down; however, this exacerbates
sensor noise in HIMM/GRO updating mechanisms and to a lesser degree in
Bayesian and Dempster-Shafer.
Other possibilities for tuning the performance include changing the sonar
model and the update rules. In practice, only two aspects of the Bayesian
TUNING BAYESIAN
MODEL
sonar model are tuned: the ﬁeld of view and the prior probability that an
area is occupied. In difﬁcult environments, the range
R accepted as valid is
often shortened. A robot might treat a range reading greater than 4 feet as
being empty even though the sonar range is theoretically covers 25 feet or
more. The rationale is that the likelihood that long readings are accurate is
small and the robot is more interested in obstacles nearby. Of course, this
can limit the robot’s maximum safe velocity since it may be able to cover
a distance faster than it can determine reliably that there is anything in it.
Likewise, the
 for the ﬁeld of view is often adjusted. In Sec. 11.3, the prior
probability was assumed to be
P
(H
)
=
P
(:H
)
=
0:. However, this isn’t
necessarily true. In some cases, the area to be covered is actually more likely
to be occupied. Consider a robot operating in a narrow hallway. Compare
the hallway to the area that can be covered by the robots sonars. Most of
the ﬁeld of view is likely to be occupied, which may argue for a
P
(H
)

P
(:H
). Moravec’s ongoing work in sonar-based occupancy grids has shown
improvement based on using more accurate priors. However, this requires
the robot or designer to gather data in advance of the robot being able to use
the data. There is work in adaptive learning of the parameters.
Dempster-Shafer theoretic methods have less to tune.
Priors are not re-
TUNING DS MODEL
quired as with Bayesian; Dempster-Shafer assigns all unsensed space a belief
of
m(dontk
now
)
=
:0. If there is prior information, the appropriate expecta-
tions can be placed into the grid. However, this is rarely if ever done. Tuning
with Dempster-Shafer consists primarily of changing the ﬁeld of view pa-
rameters,
 and
R.
HIMM/GRO have many more parameters that can be tuned, which can
TUNING HIMM

11.7
Localization
415
often be a disadvantage in the ﬁeld as tweaking more than one can have
conﬂicting side effects. The basic increments,
I
+ and
I
  are often changed.
Less frequently, the size of the mask
W and the individual weights
W
p;q are
changed.
11.7
Localization
Fig. 11.1 shows a metric map built up from sensor data using shaft encoders
to localize. As can be seen, the shaft encoders are so inaccurate that the hall-
ways never connect.
Localization can either use raw sensor data directly (iconic) or use fea-
ICONIC
tures extracted from the sensor data (feature-based). For example, iconic lo-
FEATURE-BASED
LOCALIZATION
calization would match current sensor readings to data fused with the previ-
ous sensor measurements in the occupancy grid. Feature-based localization
might ﬁrst extract a corner from the sonar data or occupancy grid, then on
the next data acquisition, the robot would extract the corner and compute the
true change in position. Feature-based localization is conceptually similar to
the idea of distinctive places in topological navigation, in the sense that there
are features in the environment that can be seen from several viewpoints.
Current metric map-making methods rely heavily on iconic localization,
and many methods use some form of continuous localization and mapping.
Essentially the robot moves a short distance and matches what it sees to what
it has built up in its map. Map matching is made more complex by the uncer-
tainties in the occupancy grid itself: what the robot thought it was seeing at
time
t
n  may have been wrong and the observations at
t
n are better. These
methods can be extremely accurate, though are often computationally ex-
pensive.
There is rising interest in feature-based methods for topological map-mak-
ing because gateways are of interest for maps and can be readily perceived.
The primary issue in topological map-making is the possibility that the ro-
bot mistakes one gateway for another, for example, interprets an intersection
with a hallway as a door.
Shaffer et al. compared iconic and feature-based methods. 127 They con-
cluded that iconic methods were more accurate for localization than feature-
based methods with fewer data points. Also, they noted that iconic methods
impose fewer restrictions on the environment (such as having to know the
types of features that will be available). However, feature-based algorithms
were often faster because there was less data to match during the localization

416
11
Localization and Map Making
a.
b.
Figure 11.17
Example of a.) a global map constructed from previous readings and
b.) a new local observation that must be ﬁt to the global map. Shaded elements in a.
represent possible matches to the shaded element in b.
process. This ignored the cost of feature extraction, however. Feature-based
algorithms were also better able to handle poor initial location estimates. So
if the robot was placed in an ofﬁce building and told it was facing North
when it was facing South, it would be able to correct that error after it en-
countered one or more gateways.
An important point to remember about localization is that no technique
handles a dynamic environment. If there are people moving about, each lo-
cal update will be different and it may be next to impossible for the robot to
match the past and current observations. If the robot is localizing itself to an
a priori map, it cannot tolerate a large number of discrepancies between the
map and the current state of the real world. For example, furniture shown
in one place on the map but which is actually in another is hard to handle.
Likewise, a hallway in a hospital which is usually clear but suddenly clut-
tered with gurneys and equipment presents a challenge.
11.7.1
Continuous localization and mapping
In order to eliminate the problems with shaft encoders or other propriocep-
tive techniques, current localization methods use exteroception. Exteroceptive
methods involve the robot matching its current perception of the world with
its past observations. Usually the past observations are the map itself. Once
the true position of the robot is known with respect to the map, the current
perception is then added to the map in a process often called registration.
REGISTRATION
As seen in Fig. 11.17, matching the current observation to past observations
is not as simple as it sounds. The robot has moved from
a to
b according to

11.7
Localization
417
its shaft encoders. It has created a map in the form of an occupancy grid,
shown in Fig. 11.17a. However, the robot can have signiﬁcant (and different
amounts of) errors in each of its three degrees of freedom,
(x;
y
;

). As a
result, based on the current, or local, observation, the robot could actually be
in any one of the shaded elements. Actually, if the robot was facing a different
orientation, the number of possible matches of the local observation to the
global map increases. The shaft encoders provide a set of possible locations,
not a single location, and orientations; each possible
(x;
y
;

) will be called a
pose.
POSE
In theory, the “local” occupancy grid built from current range readings
should help determine which location is correct. If the robot has not moved
very far, then a large portion of the grid should match what is already in
the global map. As can be seen in Fig. 11.17, the current sensor readings
have at least three possible ﬁts with the global occupancy grid, or map. The
possibilities get higher if the uncertainty in the local and global grids are
considered. Unfortunately, there is no guarantee the the current perception
is correct; it could be wrong, so matching it to the map would be unreliable.
Localization methods, therefore, attempt to balance competing interests.
On one hand, more frequent localization is good. The less the robot has
moved, the smaller the set of possible poses to be considered. Likewise, the
less the robot has moved, the more likely there is to be an intersection of cur-
rent perception with past perceptions. But if the robot localizes itself every
time it gets a sensor reading, the more likely it is that the current observation
will have noise and that the method will produce the wrong match. So there
are tradeoffs between 1) localizing after every sensor update and 2) localiz-
ing after
n sensor updates which have been fused. Usually the choice of
n is
done by trial and error to determine which value works well and can execute
on the particular hardware (processor, available memory). Notice that the is-
sues in balancing these issues are similar to those of balancing continuous
versus event-driven replanning discussed in Ch. 10.
The general approach to balancing these interests is shown in Fig. 11.18. In
order to ﬁlter sensor noise, the robot constructs a local occupancy grid from the
LOCAL OCCUPANCY
GRID
past
n readings. After
n readings, the local grid is matched to the global occu-
GLOBAL OCCUPANCY
GRID
pancy grid or map. Matching is done
k times, one for every possible
(x;
y
;

)
pose generated from the shaft encoder data. In order to generate
k, the robot
has to consider the translation of the robot (which
g
r
id[i][j
] the robot is actu-
ally occupying) and the rotation of the robot (what direction
 it is facing). In
theory, the possible number of poses,
k, is actually inﬁnite. Consider just the
 error for a “move ahead 1 meter” command, which might be between -5


418
11
Localization and Map Making
construct
local map
 for n observations
raw
sensor
readings
local map
match and
score
pose
generation
shaft
encoder
readings
k possible poses
global map
best pose
integrate
local into
global map
Figure 11.18
Flowchart showing the continuous localization and mapping process.
Mapping phase is shown shaded.
and +10
, because the robot has a tendency to drift to the right and because
it initially might not be pointed straight. This is a range of real values, so
the possible values to consider are inﬁnite! In order to make this tractable
the pose generation function has to convert this continuous range into discrete
POSE GENERATION
FUNCTION
units, much as an occupancy grid represents area with elements of a ﬁnite
width and height. The function might only consider
 errors in increments
of

. In that case, the possible poses will consider only

=
 ;
0;
;
0. Of
course, the pose generation function has to discretize the errors in the
x and
y degrees of freedom too. A pose generation function may return on the
order of 128 different poses to consider. In order to cut down on the compu-
tations while the robot is navigating, a pose generation function might use a
lookup table which stores possible poses for a particular motion computed
in advance.
The localization algorithm must match the local occupancy grids to the
global occupancy. To do this, it essentially overlays the center of the local
grid at the
k pose onto the global grid. This marks a set of grid elements
which overlap in both grids. The algorithm scores how well the values in the
local grid match the values in the overlapping global occupancy grid. Since
the local and global grids will have uncertainty values, the matching function
must somehow account for this. There are a variety of possible measures;
Eqn. 11.16 can be adapted as an example of a simple one:
scor
e
=
K
X
k
=i;j
jg
l
obal
[i][j
]
 l
ocal
[i][j
]j

11.7
Localization
419
Once the score for all matches has been computed, the algorithm chooses
the pose with the best score. This
(x;
y
;

) is declared the correct current
position of the robot on the global grid and is used to update the robot’s
odometry. The process then repeats.
A good example is the continuous localization component of the ARIEL
system125 developed under the direction of Alan Schultz at the US Naval
Research Laboratory. ARIEL runs on Nomad 200 bases that have both sonar
and a structured light rangeﬁnder for range detection, but can be used with
any type of sensor that returns range data that is fused into the occupancy
grids according to an appropriate sensor model.
The localization process is illustrated in Fig. 11.19c. The local occupancy
grid is called the Short-Term Perception Map, and it covers the area that
the robot can ideally sense; it is only about as large as the sensor coverage.
Fig. 11.19a shows an example of a short-term perception map generated by
the sonars in a large room. The grid is constructed by fusing range data with
a variant of the Bayesian combination method.
A second, global occupancy grid, the Long-Term Perception Map, repre-
sents the robot’s larger environment. It covers an area such as an entire room,
and is illustrated in Fig. 11.19b. In terms of updating, it serves as an a priori
map.
The robot’s odometry obtained from shaft encoders drifts with motion,
so the continuous localization process adapts the frequency of relocalization
with the distance traveled. More frequent localization reduces the amount
of motion between each attempt, which in turn reduces the odometric drift,
and fewer poses are needed to adequately cover the uncertainty in position,
which in turn decreases the computational effort.
As the robot moves, sensor data is obtained and fed into the Short-Term
Map. Every two feet (which corresponds to about 800 individual sensor read-
ings), the matching function estimates the possible choices k, and for each
pose compares the match between the mature Short-Term Map and the Long-
Term Map. The pose with the best ﬁt is chosen, and the robot’s odometry is
updated.
Fig. 11.20 shows the results of using the ARIEL continuous localization and
mapping process to map a previously unknown environment (a 70-foot-long
hallway). ARIEL was able to reduce the errors in the ﬁnal map (both the
dimensions of the hall and how well the maps lined up) by 75% compared
to maps generated by just shaft encoder localization.
But what if the environment is dynamic? After the Long-Term Map is
obtained, if a hallway is obstructed or a door is closed, the robot needs to

420
11
Localization and Map Making
a.
b.
c.
Figure 11.19
Continuous localization in ARIEL: a.)
short-term perception map.
Height of pixels indicates the certainty, b.) long-term perception map, and c.) reg-
istration of short-term perception map with long-term map. (Figures courtesy of the
Navy Center for Applied Research in Artiﬁcial Intelligence, Naval Research Labora-
tory.)

11.7
Localization
421
be able to notice the change so that it can react accordingly. To allow this,
ARIEL has a map adaptation mechanism (the dashed lines in Fig. 11.19c).
After each localization, the Short-Term Map is integrated into the Long-
Term Map at the determined pose using Bayes’ rule. Changes in the envi-
ronment sensed by the robot and put into the Short-Term Perception Map
therefore make their way into the Long-Term Map, where they can be used
for improved localization (e.g. when a box shows up in a previously feature-
less hallway) and planning (e.g. planning a path to a goal via another route).
In this way, the robot maintains its ability to stay localized despite changes
in the environment while adding the ability to react to those changes.
Although continuous localization can be computationally intensive, the
fusion of sensor data over each element in the Short-Term Map would be
done for obstacle avoidance anyway. The added cost of the registration is
periodic and can be performed in parallel on a separate processor to reduce
its impact. The map adaptation mechanism has a small cost, as it reuses
the fused data in the Short-Term Map, rather than resensing and fusing the
individual sensor data into the Long-Term Map. Overall, the NRL algorithm
is able to localize at about a 1Hz rate on a Pentium II class processor.
11.7.2
Feature-based localization
Feature-based localization has two ﬂavors. One ﬂavor is similar to contin-
uous localization and mapping: the robot extracts a feature such as a wall,
opening, or corner, then tries to ﬁnd the feature in the next sensor update.
The feature acts as a local, natural landmark. As noted earlier in Ch. 9, the
use of natural landmarks can be challenging for navigation in general, espe-
cially since a “good” set of features to extract may not be known in advance.
Trying to ﬁnd and use natural landmarks for accurate localization is at least
doubly hard.
The second ﬂavor is really an extension of topological navigation. The
robot localizes itself relative to topological features such as gateways. It may
not know how long a hallway is, but it knows what the end of the hallway
looks like and important information (e.g., the hallway terminates in a t-
junction and has 3 doors on the left and 3 on the right). Once the robot has
constructed a topological map, it can locate itself.
An interesting question that has been considered by many researchers is
what happens when a robot is given a topological map (such as in Fig. 11.21),
told it was at Position A, and was really at Position B? How would the ro-
bot ever notice it was misinformed, rather than assume its sensors were re-

422
11
Localization and Map Making
Figure 11.20
Mapping of a 70-foot hall without continuous localization (upper) and
with (lower). (Figure courtesy of the Navy Center for Applied Research in Artiﬁcial
Intelligence, Naval Research Laboratory.)
porting erroneous readings? And, how many moves would it take to ﬁnd a
unique path that would permit the robot to localize itself? These questions
were explored by the AAAI Mobile Robot Competitions from 1993 through
1996.
Sensor uncertainty plays a large role in localization, even in a topological
map. Nourbakhsh, Power, and Birchﬁeld, who won the AAAI competition in
1994,112 collected large amounts of data with their robot’s sonar range ﬁnd-
ers. They wrote algorithms to detect walls, closed doors, open doors, and
hallways. Even with their best algorithms, the sonar noise was such that a
hallway had about a 10% chance of being classiﬁed as an open door, while

11.7
Localization
423
1
2
2-3
3
3-4
4
4-5
5-9
9
9-10
10
5
6
7
8
6-7
5-6
A
B
N
S
W
E
Figure 11.21
State-sets of a topological layout of an ofﬁce building.
a wall had a 30% chance of being seen as a closed door. To factor in this
sensor uncertainty, most roboticists use techniques based on a probabilistic
method known as Partially Observable Markov Decision Processes (POMDP,
pronounced “pom D P”). POMDPs are beyond the scope of this book, but the
basic idea is illustrated by the quasi-evidential state-set progression methodol-
STATE-SET
PROGRESSION
ogy developed by Illah Nourbakhsh.112
Returning to Fig. 11.21, it can be seen that there are 10 gateways. The
gateways are labeled with a single number and represent a state where the
robot can localize itself relative to the map. (The numbering is arbitrary.)
Notice that the hallways are not gateways. The robot knows that it is in a
hallway, but unless it uses continuous localization and mapping, it does not
know where it is in the hallway (e.g., how close it is to the next gateway).
Since hallways can be considered as the state of the robot which is between
gateways, the hallway are labeled with two numbers,
g
atew
ay
 g
atew
ay,
representing the pair of gateways the robot is between.
If the robot “wakes up” and senses it is in a hallway facing east, it knows it
is in one of the following states: {{2-3},{5-6},{6-7}}. So it is currently in one of
three possible states. As it moves and detects a gateway, the new information

424
11
Localization and Map Making
should reduce the possible states. For example, it the robot starts moving
east, it will encounter different gateways based on where it really is. The
possible progression of the states from the initial set of states is:

f
 g
!
fg, where {3} is a
w
al
l
r
ig
ht,
hal
l
lef
t,
w
al
l
f
r
ont

f
 g
!
fg, where {5} is a
hal
l
r
ig
ht,
hal
l
lef
t,
w
al
l
f
r
ont

f
 g
!
fg, where {6} is a
w
al
l
r
ig
ht,
door
lef
t,
hal
l
f
r
ont
Therefore if the robot encounters a
door
lef
t, the set of possible states rep-
resenting its location on the topological map reduces to
fg.
The above method works very well for indoor environments with orthog-
onal intersections. The challenge becomes how to handle sensor noise; there
is a large possibility the robot will misclassify the door as a hallway. This
would lead to a belief that the robot was more likely to be at {3} rather than
at {6}. The basic solution is to keep track of the probabilities of where the
robot is at and then after many moves, the probability that the robot was at
{6} eventually becomes 1.0.
11.8
Exploration
Exploration attempts to answer the question of where haven’t I been? versus
where am I? A central concern in exploration is how to cover an unknown
area efﬁciently. One way is to do this with a random search; the robot liter-
ally wanders around randomly (using a random potential ﬁeld). After some
(often very large) period of time, statistics indicate it should have covered
the entire area. Another reactive method is to permit a short-term persis-
tence of proprioception (odometry). Then the robot is repulsed by areas that
have been recently visited. This can be implemented as a repulsive ﬁeld,
generated by every visited cell in a coarse occupancy grid or for every previ-
ous heading. This “avoid past” behavior17 when combined with the random
potential ﬁeld drives the robot towards new areas.
Another behavioral approach is to exploit evidential information in the
occupancy grid. As the robot explores a new area, many cells on the grid
will be unknown, either
P
(O
ccupied)
=
P
(E
mpty
) in a Bayesian system or
m(dontk
now
)
=
 in a Dempster-Shafer implementation. The robot takes the
centroid of the unknown area and uses that as a goal for a move-to-goal.
As it moves to the goal (in conjunction with avoid-obstacle), new sensor

11.8
Exploration
425
readings will update the occupancy grid, reducing the amount and loca-
tion of unknown areas, and creating a new goal. Hughes and Murphy105
showed that this move-to-unknown-area behavior was suitable for indoor
exploration and even localization. While the above behavior-oriented ap-
proaches are simple and easy to implement, they often are inefﬁcient, espe-
cially when presented with two or more unexplored areas. Suppose a robot
has encountered a hallway intersection; how does it choose which area to
explore?
Two basic styles of exploration methods have emerged which rank unex-
plored areas and make rational choices: frontier-based and generalized Voronoi
graph methods. Both work well for indoor environments; it is less clear how
these work over large open spaces. Both use behaviors for navigation, but
are different in how they set the navigational goals. This section provides a
highly simpliﬁed overview of each method.
11.8.1
Frontier-based exploration
Frontier-based exploration was pioneered by Brian Yamauchi.125 The ap-
proach assumes the robot is using a Bayesian occupancy grid (a Dempster-
Shafer grid can be used as well). As shown in Fig. 11.22, when a robot enters
a new area, there is a boundary between each area that has been sensed and
is open and the area that has not been sensed. (The boundary between occu-
pied areas and unknown areas are not interesting because the robot cannot
go through the occupied area to sense what is behind it.) There are two such
boundaries in Fig. 11.22; each of these lines form a frontier that should be
FRONTIER
explored.
The choice of which frontier to be explored ﬁrst can be made in a variety of
ways. A simple strategy is to explore the nearest frontier ﬁrst. Another is to
explore the biggest frontier ﬁrst. Since the world is unknown, the robot has
no way of knowing if upon reaching a big frontier it will discover a wall just
a meter away. This means that the robot might move across a room, brieﬂy
explore one area, then return back to almost at its starting point, explore that
area, and then go to another place, and so on. In practice, this doesn’t happen
that often with indoor environments.
The size of the frontier can be measured by the number of edge cells. Every
cell in the occupancy grid that the boundary runs through is considered an
edge. If an edge “touches” an edge in one of its eight surrounding neighbors,
the edges are connected and form the line. In order to eliminate the effects of

426
11
Localization and Map Making
Robot Current Position
Occupied Space
Frontier
Centroid
Open Space
Unknown Space
Figure 11.22
Example of a robot exploring an area using frontier-based method.
(Figure courtesy of the Navy Center for Applied Research in Artiﬁcial Intelligence,
Naval Research Laboratory.)
sensor noise there is usually a threshold of the minimum of number of cells
in a line before it can be treated as a frontier.
Once the robot has chosen a frontier to investigate, it must decide precisely
where to head. A good goal that will get the robot to the frontier is the cen-
troid,
(x
c
;
y
c
), of the edges in the frontier line.
x and
y is the location of the
centroid, either in map coordinates or the
i, j indices in the grid. The centroid
for a two dimensional object is the center, or the “average”
(x;
y
) location.
The computation for the centroid in pseudocode is:
x_c = y_c = count = 0
for every cell on the frontier line with a location of (x, y)
x_c = x_c + x
y_c = y_c + y
count++

11.8
Exploration
427
x_c = x_c/count
y_c = y_c/count
Once the centroid has been established, the robot can navigate either using
reactive move-to-goal and avoid-obstacle behaviors, plan a path and reac-
tively execute the path, or continuously replan and execute the path. Regard-
less, once the robot is at the frontier, the map is updated and new frontiers
may be discovered. These frontiers are then evaluated, a new one is chosen,
and the rest are stored.
11.8.2
Generalized Voronoi graph methods
Another method of deciding how to explore a space is to have the robot build
a reduced generalized Voronoi graph (GVG) as it moves through the world.
This method has been used extensively by Howie Choset. 35;34
To review Ch. 10, as the robot moves, it attempts to maintain a path that
places it equidistant from all objects it senses. Essentially, the robot tries to
move ahead but stay in the “middle” or at a tangent to surrounding objects.
This path is a GVG edge, the same as would be generated by decompos-
ing the space into a Voronoi graph. Generating and following the path is
straightforward to do with a behavior.
When the robot comes to a dead end or a gateway, there are multiple GVG
edges that the robot could follow. As shown in Fig. 11.23, dead ends produce
two GVG edges. But in this case, the robot can perceive that both of these
edges end at objects, so there is no reason to follow them. The robot can
then backtrack along the path it had been on, either to the start or to another
branch. If the robot encounters branches in the GVG edges, it can choose one
at random to follow.
Fig. 11.23 shows how the robot would explore the an area. For conve-
nience, the ﬁgure shows the entire GVG that would be drawn after the robot
had fully explored the area. The robot would begin by sensing the walls on
each side (without any recognition that it was in a hall). The sensed area is
shown in light gray. It would attempt to center itself between the walls while
moving perpendicular to the line of intersection. Eventually it reaches an in-
tersection. This creates two edges, neither of which appears to terminate at
an object. It arbitrarily chooses the left edge and saves the right. It does this
repeatedly, as seen in Fig. 11.23b. It continues up the hall until it comes to
the dead end. The robot then backtracks using the same edge behavior (stay
in the middle). It continues to favor exploring to the left until it comes to a

428
11
Localization and Map Making
dead end in the lower left, then backtracks as shown in Fig. 11.24a. The robot
continues to explore and backtrack until the entire area is covered, as shown
in Fig. 11.24b.
11.9
Summary
Map-making converts a local, or robot-centered, set of sensor observations
into a global map, independent of robot position. One of the most common
data structures used to represent metric maps is the occupancy grid. An occu-
pancy grid is a two dimensional array regular grid which represents a ﬁxed
area in an absolute coordinate system. Grids have a high resolution, on the
order of 5-10 cm per grid element.
Greater accuracy is obtained in occupancy grids by fusing together multi-
ple uncertain sensor readings. Sensor fusion requires a sensor model for trans-
lating a sensor reading into a representation of uncertainty, and an update
rule for combining the uncertainty for a pair of observations. Bayesian meth-
ods use a probabilistic sensor model, representing uncertainty as probabilities
and updating with Bayes’ rule. Dempster-Shafer methods use a possibilis-
tic sensor model with Shafer belief functions combined by Dempster’s rule.
HIMM uses an ad hoc sensor model and update rule. HIMM is less accurate
and harder to tune, but requires signiﬁcantly less computation than tradi-
tional implementations of Bayesian or Dempster-Shafer methods. Because
of the improvement due to sensor fusion, occupancy grids are often used for
obstacle avoidance, serving as a virtual sensor for reactive behaviors.
Producing a global map based in a ﬁxed coordinate system requires local-
ization. In general, the more often the robot is able to localize itself, the more
accurate the map. However, localization is often computationally expensive
so it may not be run at the same update rate as reactive behaviors. Raw sen-
sor data, especially odometry, is imperfect and confounds the localization
and map-making process. Most techniques concurrently map and localize.
The two categories of localization methods are iconic and feature-based. Of
the two, iconic methods are better suited for metric map-making and occu-
pancy grids. They ﬁt raw observations into the map directly. An example
is creating a local short-term occupancy grid from sonar readings, then after
three moves, matching that grid to the long-term occupancy grid.
Feature-based methods perform less well for metric map-making, but work
satisfactorily for topological map-making.
Feature-based methods match
current observations to the map by matching features rather than raw sen-

11.9
Summary
429
a.
b.
Figure 11.23
Example of a robot exploring an indoor area using a GVG method. a.)
The robot starts in the lower right corner and b.) explores the center. The black lines
indicate the ideal GVG, the white lines indicate the portion of the GVG that the robot
has traveled. (Figures courtesy of Howie Choset.)

430
11
Localization and Map Making
a.
b.
Figure 11.24
Example of a robot exploring an indoor area using a GVG method,
continued. a.) reaches a dead end and backtracks, and b.) completely covers the area,
though it hasn’t traveled all edges. (Figures courtesy of Howie Choset.)

11.10
Exercises
431
sor readings. The features can be either local features such as a corner, or
more topologically signiﬁcant, such as a landmark or gateway. The choice
of features gives rise to two problems: the certainty with which a feature
can be detected (how likely is the robot to miss a hallway on its left) and
reasoning about indistinguishable locations (one hallway intersection looks
like another). Topological localization can be done by matching the gate-
ways encountered with the expected gateways given on a topological map.
All existing map-making methods tend to break in the presence of changing
environments.
Formal exploration strategies for mapping new areas fall into two cate-
gories of techniques: frontier-based and GVG. Frontier-based methods look as
the perimeter of the of sensed region on an occupancy grid and then rank
areas to explore. Voronoi methods represent the explored world as a Voronoi
diagram and use geometric properties to ensure that open areas are oppor-
tunistically explored as the robot moves.
11.10
Exercises
Exercise 11.1
An important property of an occupancy grid is that it supports sensor fusion. Deﬁne
sensor fusion in one or two sentences and give two examples.
Exercise 11.2
What is the difference between an occupancy grid, a certainty grid, and an evidence
grid?
Exercise 11.3
What is the difference between iconic and feature-based localization?
Exercise 11.4
List the desirable attributes of a landmark. Discuss why or why not each of the fol-
lowing might make a good landmark for feature-based localization:
a. Corner
b. Intersection of hallways
c. Open doorway
Exercise 11.5
Name three applications where metric map-making might be useful.
Exercise 11.6
Why is localization important in map-making?

432
11
Localization and Map Making
Exercise 11.7
Suppose a localization algorithm had an order complexity of
O
(m

n), where
m is
the number of columns and
n is the number of rows in the occupancy grid. Therefore
if the algorithm operated over a
0

0 grid, approximately 100 operations would be
executed for localization. What would be the number of operations if the grid were:
a.
00

00?
b.
000

000?
c.
000

000?
Exercise 11.8
Repeat the previous problem where the order complexity is
O
((m

n)

).
Exercise 11.9
Consider a robot beginning to map a new area (the occupancy grid is at an ini-
tial unsensed state), and how three different sonar updates create a certainty value
for a particular grid element. The sonar model is: range is 12, it’s

=

, the
M
ax
occupancy
=
0:	, and the tolerance is
pm:0. At time
t, the sonar reading re-
turns 10.0. The grid element of interest is 9.5 units away from the robot with an

of 0.0. At
t, the robot has translated sideways. The sonar reading is 11.0. The grid
element is now 9.5 units away with an

=
:0. At
t, the sonar returns a value of
8.5 units. The robot has moved to the side and rotated; it is now 9.0 units from the
grid element with an
 of

.
a. What is the initial value of every element in the occupancy grid for

i) Bayesian

ii) Dempster-Shafer

iii) HIMM
b. Fill in the probabilities for the grid element for each sensor reading:
sonar
Bayesian
Dempster-Shafer
HIMM
certainty:
P
(sjO
)
P
(sjE
)
m(O
)
m(E
)
m(dontk
now)
t1
t2
t3
c. Fill in the values for the grid element after every update:
after
Bayesian
Dempster-Shafer
HIMM
update:
P
(O
js)
P
(E
js)
m(O
)
m(E
)
m(dontk
now)
t1
t2
t3
Exercise 11.10
Construct a series of readings that using the HIMM update rule would produce the
same ﬁnal grid as produced by GRO in Fig. 11.13.

11.10
Exercises
433
Exercise 11.11
Consider how the robot moves from one frontier to another in frontier-based explo-
ration. What are the advantages and disadvantages of explicitly planning a path
between frontier centroids versus using a purely reactive move-to-goal and avoid set
of behaviors?
Exercise 11.12
Compare frontier-based exploration with GVG exploration. Is one better suited for
certain environments than the other?
Exercise 11.13
Consider how both exploration strategies (frontier and GVG) often have to choose
between possible areas to explore and then save the choices not taken for further
exploration. Which data structure do you think would be better to store the choices,
a stack or a priority queue? Why?
Exercise 11.14
[Programming]
Write a program that displays a sonar model on a 26 by 17 grid. The origin of the
sonar should be at (26,8), with a

=

. Assume a sensor reading of
s
=
0 with a
tolerance of 2 units.
a. Create a simple ASCII display. Each element should be labeled with the number
of its corresponding region: 1, 2, or 3. For elements outside of the ﬁeld of view,
the label is 4.
b. Create a simple Bayesian display. Each element should be labeled with the correct
P
(sjO
ccupied) value .
c. Create a simple Dempster-Shafer display. Each element should be labeled with the
correct
m(O
ccupied) value (Dempster-Shafer).
d. Save the data from parts b. and c. and put in a spreadsheet or plotting program.
Display as a 3D plot.
Exercise 11.15
[Programming]
Write a program for a robot which moves to the centroid of an unknown area. Test it
for a variety of room conﬁgurations, and describe where it stops in each case.
Exercise 11.16
[Advanced Reading]
Read “Dervish: An Ofﬁce-Navigating Robot,” by Illah Nourbakhsh, and “Xavier: A
Robot Navigation Architecture Based on Partially Observable Markov Decision Pro-
cess Models,” by Sven Koenig and Reid Simmons in Artiﬁcial Intelligence and Mobile
Robots: Case Studies of Succesful Robot System, ed. Kortenkamp, Bonasso, Murphy,
1998. Describe how these approaches would work for the example in Fig. 11.21. Com-
ment on the similarities and differences.

434
11
Localization and Map Making
11.11
End Notes
Robot name trivia.
CARMEL stands for Computer Aided Robotics for Maintenance, Emergency, and
Life Support. Many of Moravec’s robots are named for (distant) planets: Neptune,
Uranus, Pluto.
Uncertainty and mapping.
The ﬁgures of hallways were generated by Brian Sjoberg and Jeff Hyams at the Uni-
versity of South Florida and Bill Adams at the Naval Research Laboratory. Alan
Schultz and Bill Adams also provided the description of ARIEL.

12
On the Horizon
Chapter Objectives:
 Deﬁne: polymorphism, adjustable autonomy.
 Be familiar with trends in AI robotics, both in research and in society.
12.1
Overview
At this point in time, AI robotics remains largely a research or specialized sci-
entiﬁc endeavor. There has been some move to commercialize the progress
made by the Reactive Paradigm, as seen by the popularity of Lego Mind-
storms and Sony Aibo, but overall the ﬁeld is nascent, waiting for what soft-
ware gurus call the “killer app.” This chapter attempts to identify new trends
on the horizon.
Ch. 1 offered a timeline of the historical forks in the development of ro-
botics. This is repeated in Fig. 12.1 for convenience. In retrospect, the most
unfortunate fork was not the split between industrial and AI robots in the
1960’s, but rather the split between vision and robotics communities. As can
be seen from Ch. 4, the success of a robot is based largely on its perceptual ca-
pabilities. Vision is a powerful modality, and was overlooked by researchers
due to high hardware prices and high computational demands.
Now there are signs of the two communities reuniting. Many computer vi-
sion groups are now working with mobile robots. Early forays by the vision
community into robotics could be characterized as being “disembodied” or
separating vision from the rest of the robot. Robots were a way of getting
a series of images over time, rather than an opportunity to explore how to
organize vision such that a robot could use it in real-time. One of the ﬁrst
computer vision scientists to take a more principled approach to merging

436
12
On the Horizon
telemanipulators
planetary rovers
vision
manufacturing
1960
1970
1980
1990
2000
Industrial
Manipulators
AI Robotics
telesystems
Figure 12.1
A timeline showing forks in development of robots.
vision and robotics was Avi Kak at Purdue University. His Cybermotion ro-
bot was one of the ﬁrst to navigate in hallways using vision; in this case, a
technique known as a Hough (pronounced “huff”) transform.80 In the early
1990’s, with slow hardware, the robot could go 8 to 9 meters per second.
The computer vision community went through the equivalent of a Reactive
movement, moving to approaches now called animate vision, purposive vision,
ANIMATE, PURPOSIVE,
ACTIVE VISION
and active vision.
Another positive inﬂuence to reunite robotics and vision has been the var-
ious DARPA and ESPIRIT projects in mobility. Both agencies have provided
funding for large-scale projects, such as fully autonomous off-road and high-
way vehicles. The size of the projects require hundreds of researchers from
many universities to collaborate, providing the opportunity to cross-fertilize
the ﬁelds.
A more cost-effective motivation to have roboticists work with vision, and
vice versa, has been the various robot competitions. The prestigious AAAI
Mobile Robot Competitions have changed the events and the rules each year
to reward researchers who use computer vision in creative ways, instead of
relying on the standard sonars. A newer event, RoboCup, mandates vision.
There is no other way to play soccer than to see the ball and the players in
real-time. The competitions have already spawned at least one commercial
product: the Cognachrome fast vision board developed by Newton Labs.
The ground and aerial robot competitions sponsored by the Association for
Unmanned Vehicle Systems also promote the integration of computer vision
with robotics, but winning those competitions still largely depends on hard-
ware, not software algorithms.

12.1
Overview
437
The central theme of the ﬁrst part of this book has been the paradigms
of robotics. The Reactive and Hybrid Paradigms have been in dominance
for a decade. It is reasonable to expect that a new paradigm is on the hori-
zon. If so, that paradigm shift might be the addition of a fourth robot prim-
itive, learning. Right now, robots have emergent behavior from manually
coded schemas and deliberative functions. Certainly, learning exists in bi-
ological systems.
The lack of a computational theory of learning and of
bridge-builders such as Arbib, Brooks, and Arkin is possibly delaying this
next step.
This is not to say that exciting work has not been going on in robot learn-
ing. Connell and Mahadevan published an inﬂuential book, Robot Learning,
in 1993, and researchers such as Patty Maes have demonstrated multi-legged
insect robots waking up and learning to walk within minutes. Leslie Kae-
bling has recently been recognized by AAAI for her work with robot learn-
ing. The ﬁeld of robotic learning is growing rapidly and it is too large to
cover in this book, but techniques such as neural networks and genetic algo-
rithms are making increasing contributions and roboticists should keep an
eye on developments in those communities.
The past suggests that the future will involve vision and paradigms. What
trends can be gleaned from the current state of robotics? One assessment
is that mobile robots are now truly mobile; they have demonstrated that
they can reliably navigate. Given the ability to navigate, what would further
shape the ﬁeld? The rest of this chapter explores the potential impact of four
“usual suspects”: hardware advances, software trends, emerging applications, and
changing societal expectations.
In terms of hardware, mobile robot design has been largely static for the
past four or ﬁve years, centered about three main types or “species.” There
are the round, mostly holonomic research robots such as those made by No-
madic, RWI, Cybermotion, and Denning. Another robot species is that of
the somewhat fragile insectoids. An in-between size of robot, somewhere
on the order of a large radio-controller car powered with a laptop, has also
emerged, most notably the Pioneers and B-12. New developments have led
to polymorphic or shape-shifting robots, in addition to continued progress in re-
conﬁgurable platforms. Aerial and underwater platforms are also becoming
more mainstream.
Software advances, particularly in agency, are also likely to have a pro-
found inﬂuence on mobile robotics. As noted in Ch. 1, the Reactive move-
ment contributed signiﬁcantly to the rise of software agents, especially we-
bots and know-bots. Now the rapid assimilation and commercial develop-

438
12
On the Horizon
ment of these concepts, combined with the “thin client” and “appliance”
movement, suggest that these intellectual children will return home and teach
their parents new tricks. Another software trend is the push for software
toolkits and better development environments for robots. Georgia Tech’s
Mission Lab software package is one example of a toolkit that enables rapid
assemblage of behaviors. 92
For whatever reason, robots are being considered for a broader set of ap-
plications than ever before. In the 1960’s, robots were targeted for hazardous
operations, including nuclear processing and space exploration. In the 1970’s
and 1980’s, robots became identiﬁed with manufacturing. The 1990’s have
seen a shift from manufacturing to entertainment, medical, personal care,
and humanitarian efforts.
A subtle, but noticeable, change has been societal expectations of robots.
Throughout the 1960’s, 1970’s, and 1980’s, robots were envisioned as solitary
creatures, working on dirty, dull, and dangerous tasks. This view changed
a bit in the research community with the appearance of robot societies. Re-
gardless of whether it was one smart robot or a colony of stupid robots, the
human was nowhere to be found. Or the human was present, but actively
supporting the robot. Now there is an expectation that robots will be inter-
acting with humans. The end of the century has already seen robot pets and
robot tour guides. Work is being done in medical robots and in personal care
for the elderly and disabled.
12.2
Shape-Shifting and Legged Platforms
One of the biggest changes in platforms is the recent move towards polymor-
POLYMORPHISM
phism (from “multiple” and “change”), literally shape-shifting robots. Snake-
SHAPE-SHIFTING
like robots constitute an important class of shape-shifters. These robots have
a high number of joints and are difﬁcult to control, but have the potential
advantage of being able to conform to twists and turns in narrow passages.
Fig. 12.2 shows a snake at CMU exploring a search and rescue site.
Traditional tracked vehicle designs are being reworked to provide poly-
morphism. Perhaps the best known is the IS Robotics’ Urban robot shown in
Fig. 12.3. The Urban has a set of front ﬂippers. It can use the ﬂippers to help
navigate over rough terrain, raise the sensor platform for a better view, and
to right itself should it fall over. Another polymorphic tracked vehicle is the
Inuktun VGTV shown in Fig. 12.4.

12.2
Shape-Shifting and Legged Platforms
439
Figure 12.2
A snake robot exploring a search and rescue site. (Photograph courtesy
of Howie Choset.)
Shape-shifting almost certainly necessitates an ecological approach to pro-
gramming. For example, adaptation with a current shape regime is a gradual
reﬁnement of shape or posture, as in a robot lowering itself to ﬁt into an in-
creasingly smaller tunnel. This type of deformation is a direct response to
the environment; it doesn’t matter what type of material is making the tun-
nel smaller, the robot must get smaller to squeeze in. This suggests that the
most elegant software approach will be to use a reactive behavior. For exam-
ple, cockroaches and rats both exhibit a thigmotropic behavior: they prefer
to be touching something. Cockroaches, in particular, like to be squeezed
on all sides. In order to accomplish this, roaches and rats use feedback from
cilia and hair distributed throughout their bodies to determine proximity.
Once again, it is clear that the task, sensors, platforms, and intelligence must
co-evolve to create a truly useful robot.
Another major hardware advance is legged locomotion. Legs have certain
LEGGED LOCOMOTION
advantages for moving through rugged terrain and for power consumption.

440
12
On the Horizon
Figure 12.3
IS Robotics’ Urban vehicle, with ﬂippers raising the robot body. (Pho-
tographs courtesy of IS Robotics.)

12.2
Shape-Shifting and Legged Platforms
441
a.
b.
c.
Figure 12.4
An Inuktun VGTV in a.) full upright position, b.) half upright, and c.)
ﬂat.
Early work in the 1980’s by Ken Waldron with the Ohio State Hexapod was
disappointing from an AI standpoint. The traditional control theory meth-
ods for operating the hexapod were unable to provide real-time control with
the processors of the time. In the end, the robot had to be teleoperated.
Marc Raibert had a major impact on legged locomotion in the mid-1980’s.
He created a series of one-, two-, and four-legged robots for his PhD the-
sis, based in part on studies of gaits in animals.118 One outcome of these
studies was the understanding that one- and two-legged control were sim-
ply degenerate cases of multi-legged control; therefore the same simple gait
control programs could be applied to all legged platforms. Although the
control was biologically motivated, the hopping mechanisms were built like
pogo-sticks. The ﬁeld is concentrating now on maintaining balance, getting
up (and sitting down), and dynamically adapting the gaits to terrain using
more sophisticated legs.

442
12
On the Horizon
12.3
Applications and Expectations
In his 1950 book, The Human Use of Human Beings: Cybernetics and Society,
Norbert Wiener argued that robots could free people from being used inap-
propriately; for example, working in hazardous situations, allowing them to
be “used” as human beings. Many of the applications being pursued by AI
robotics researchers are fulﬁlling that potential. In addition to the traditional
applications of nuclear and hazardous waste management, robots are now
being developed for humanitarian purposes. One area is demining, where
DEMINING
land mines endangering civilians are safely removed and disposed of. An-
other effort is focusing on urban search and rescue. The Oklahoma City bomb-
URBAN SEARCH AND
RESCUE
ing in the US, and earthquakes in Japan, Mexico, Taiwan, and Turkey have
offered painful lessons. Though no robots were available for those rescue
efforts, it has become clear that robots could enter places dogs and humans
cannot, and reduce the risk to workers as they search through area of dubious
structural integrity.
Robots can be used for more commonplace tasks. Several companies have
struggled to ﬁeld reliable janitorial robots. This introduces a new challenge:
robots’ close proximity with humans. Robot pets are certainly intended to be
with humans. The Sony Aibo dog (Fig. 12.5) is perhaps the best known of
the robot pets to enter the commercial market. Humans interact with other
agents in an emotional way. For example, people communicate in non-verbal
ways such as facial expressions. Many researchers are now working on phys-
ically expressive interfaces, where the robot literally has a face and changes
expresses with moods. Kismet, shown in Fig. 12.6 is one example. Minerva,
the tour guide robot discussed in the Overview of Part II, used a similar ro-
bot face. Data collected during her stay at the Smithsonian indicated that the
audience preferred the expressive face.139
Robots are also being examined for health care. One target audience is the
elderly. Robots could be used to help pick up and retrieve objects, notice
when a person has fallen or is trouble, and even to keep an invalid com-
pany. Much of the robotic technology developed for autonomous vehicles
can be adapted for use by the disabled. Fig. 12.7 shows a semi-autonomous
wheelchair developed by Holly Yanco at MIT and demonstrated at AAAI in
1996. The wheelchair uses sonars to simplify the task of going through door-
ways and areas of clutter. A wheelchair-bound person might not have the
ﬁne motor control needed to efﬁciently negotiate those situations; therefore
the same behaviors for autonomy can reduce the need for a person to be in-
timately a part of the control loop. Although the robotic wheelchair can go

12.3
Applications and Expectations
443
Figure 12.5
The Sony Aibo robot pet. (Photograph courtesy of the Sony Corpora-
tion, Aibo c
The Sony Corporation.)
Figure 12.6
Kismet. (Photograph courtesy of the MIT Artiﬁcial Intelligence Labora-
tory.)

444
12
On the Horizon
Figure 12.7
Wheelesley, a robotic wheelchair being guided by a person using the
Boston University EagleEyes system.
(Photograph courtesy of Holly Yanco and
AAAI.)
through a door or navigate through an obstacle course by itself, the human
can take control over as much of the wheelchair as desired. This is often
referred to as adjustable autonomy. The human is allowed to dynamically ad-
ADJUSTABLE
AUTONOMY
just the level of autonomy of the robotic agent. Note that this is conﬂuence
of ideas from the teleoperation community from Ch. 1 with the autonomy
community.
As society shifts to expecting robots to work with and around humans, the
ecological niche becomes increasingly deﬁned by human workspaces. Ev-
erything the robot interacts with is designed for human ergonomics and un-
likely to be reengineered. At one time, the prevailing line of thought was to
deliberately avoid designing humanoid robots. Due to hardware and control

12.4
Summary
445
limitations, there was no way a robot could mimic even a portion of human
physiology. Now the pendulum is swinging towards humanoid robots, fu-
eled in part by the need for robots to act in human spaces and by advances
such as with legged locomotion.
Humanoid robots have been pursued by many research groups, especially
Japanese. Human-like functionality seems to favor an implementation un-
der the Hybrid Paradigm. The Honda humanoid robot, shown in Fig. 12.8a,
is possibly the most successful to date, and certainly the most convincing.
Most observers report a feeling that there is a person inside a space suit,
just crouched a bit lower than usual. A quick look to the leg joints and feet
make it clear that it is not an costumed person. The motivation for the ro-
bot stemmed from the company setting a long-term research challenge. The
two keywords which described Honda’s future market was “intelligent” and
“mobility.” The most advanced form of intelligent mobility is an autono-
mous mobile robot. The hardest form of a mobile robot is humanoid. Ergo,
the research goal became a humanoid robot.
Rodney Brooks has also been working on a humanoid robot,
C
og, since the
mid-1990’s. Cog is currently without legs, and much of the effort has been
directed at vision and learning. Cog is shown in Fig. 12.8b.
NASA Johnson Space Center has moved from the EVAHR concept dis-
cussed in Ch. 7 to the more humanoid robonaut shown in Fig. 12.8. Robonaut
is a human-sized robot complete with a stereo vision head, a torso, two arms
and two ﬁve-ﬁngered hands.
12.4
Summary
The ﬁeld of AI robotics has created a large class of robots with basic physical
and navigational competencies. At the same time, society has begun to move
towards the incorporation of robots into everyday life, from entertainment to
health care. Whereas robots were initially developed for dirty, dull, and dan-
gerous applications, they are now being considered for personal assistants.
Regardless of application, robots will require more intelligence, not less.
12.5
Exercises
Exercise 12.1
Deﬁne polymorphism. List the challenges for sensing and intelligent control for a poly-
morphic platform.

446
12
On the Horizon
a.
b.
c.
Figure 12.8
Humanoid robots: a.) the Honda humanoid robot (photograph courtesy
of Honda Corporation), b.) Robonaut, an anthropomorphic robot under development
at NASA’s Johnson Space Center (photograph courtesy of NASA), and c.) Cog play-
ing with two MIT students. (Photograph courtesy of the MIT Artiﬁcial Intelligence
Laboratory.)

12.6
End Notes
447
Exercise 12.2
Consider designing a robot platform for urban search and rescue. What are the ad-
vantages and disadvantages of polymorphism? What are the advantages and disad-
vantages of a humanoid platform?
Exercise 12.3
Deﬁne adjustable autonomy. What teleoperation scheme does it most closely resemble?
Is there a real difference between it and teleoperation?
Exercise 12.4
Propose a robot or robot society for the humanitarian applications below. Specify the
Paradigm and architecture you would use. Justify your answer.
a. Demining.
b. Urban search and rescue.
Exercise 12.5
[Advanced Reading]
Look up and deﬁne neural networks. How are they being used for robotics? What
makes them different from other approaches? Who are the researchers using neural
networks?
Exercise 12.6
[Advanced Reading]
Look up and deﬁne genetic algorithms. How are they being used for robotics? What
makes them different from other approaches? Who are the researchers using genetic
algorithms?
Exercise 12.7
[World Wide Web]
Do a web search and report on the types of robot pets currently available or proposed.
What is the motivation for each pet? What are the difﬁcult issues being addressed?
Exercise 12.8
[World Wide Web]
Search the web for more information about the Honda humanoid robot, Cog, and
Robonaut. Compare and contrast the motivation and objectives of each.
12.6
End Notes
Robots that run. And topple and fall . . .
Marc Raibert’s thesis was published as Legged Robots that Balance in 1986. A video
of his thesis work was also published. The video contained a blooper reel, show-
ing memorable events such as when the one-legged hopper jumped on an obstacle
instead of over it. The blooper reel did not detract from the scientiﬁc contributions;

448
12
On the Horizon
instead it actually added conﬁdence that Raibert’s remarkable results were not the
results of over-engineering the demonstrations.
The one-legged hopper appears for a few seconds in the movie “Rising Sun.” Sean
Connery and Wesley Snipes walk past it as they enter a high-tech research center. It
has been reported that it took several days to get the hopper hopping long enough,
coordinated with the actors, for the ﬁlm.
Robonaut.
Bob Savely provided the photo of Robonaut.
The controversy over Cog.
Cog has a very ambitious set of goals, announced in a talk. 29 The goals looked a
bit like science ﬁction author Robert Heinlein’s “Future History,” a timeline of all the
books and stories that Heinlein intended to write. Although Heinlein had not written
the stories yet, the timeline itself was published by John Campbell. Heinlein made
good, writing almost every book and story on the timeline by the end of his 40+ year
career. Progress with Cog has been relatively slow, despite frequent coverage by sci-
ence magazines, leading many in the community to criticize the project as premature.
Marvin Minsky, co-founder of the MIT AI Laboratory (which Brooks now directs),
has repeatedly declared that “Cog is not a research project, it’s a press release.” As
with all other aspects of AI robotics, time will tell.

Bibliography
[1] Albus, J., and Proctor, F.G., “A Reference Model Architecture for Intelligent Hybrid
Control Systems,” proceedings of the International Federation of Automatic Control,
San Francisco, CA, June 30–July 5, 1996.
[2] Allocca, J. A., and Stuart, A., Transducers: Theory and Application, Prentice-Hall, 1984.
[3] Anderson, M.O., McKay, M.D., Richardson, B.S., “Multirobot Automated Indoor
Floor Characterization Team,” proceedings of the 1996 IEEE International Confer-
ence on Robotics and Automation, Minneapolis, MN, April, 1996, pp. 1750–1753.
[4] Anderson, T.L., and Donath, M., “Animal Behavior as a Paradigm for Developing
Robot Autonomy,” Robotics and Autonomous Systems, vol. 6, 1990, pp. 145–186.
[5] Arbib, M., “Perceptual Structures and Distributed Motor Control,” Handbook of
Physiology—The Nervous System II, ed. Brooks, pp. 1449–1465, 1981.
[6] Arbib, M., “Schema Theory,” The Handbook of Brain Theory and Neural Networks,
Michael A. Arbib, ed., MIT Press, pp. 830–834.
[7] Arbib, M., and Liaw, J.-S., “Sensorimotor transformations in the worlds of frogs and
toads,” Computational Theories of Interaction and Agency, Philip Agre and Stanley J.
Rosenschein, ed., MIT Press, 1996, pp. 53–80.
[8] Arkin, R.C., “Towards Cosmopolitan Robots: Intelligent Navigation in Extended
Man-Made Environments,” COINS Technical Report 87-80, Computer and Informa-
tion Science, University of Massachusetts at Amherst, 1987.
[9] Arkin, R., personal communication, 1988.
[10] Arkin, R., Behavior-Based Robotics, MIT Press, 1998.
[11] Arkin, R., and MacKenzie, D., "Temporal Coordination of Perceptual Algorithms
for Mobile Robot Navigation," IEEE Transactions on Robotics and Automation, vol.
10, no. 3, June, 1994, pp. 276–286.
[12] Arkin, R.C. and Murphy, R.R., “Autonomous Navigation in a Manufacturing Envi-
ronment,” IEEE Transactions on Robotics and Automation, vol. 6, no. 4, August, 1990,
pp. 445–454.

450
Bibliography
[13] Arkin, R. C., Riseman, E. M., and Hansen, A., “AuRA: An Architecture for Vision-
Based Robot Navigation,” proceedings of the DARPA Image Understanding Work-
shop, Los Angeles, CA, February, 1987, pp. 417–413.
[14] Artiﬁcial Intelligence and Mobile Robots, D. Kortenkamp, P. Bonasso, R. Murphy, edi-
tors, MIT Press, 1998.
[15] Asimov, I., I, Robot, Bantam Books, 1994 (reprint).
[16] Balch, T., “Social Entropy: a New Metric for Learning Multi-Robot Teams,” proc.
10th International FLAIRS Conference, 1997.
[17] Balch, T.R. and Arkin R.C., “Avoiding the Past: A Simple but Effective Strategy
for Reactive Navigation,” IEEE International Conference on Robotics and Automation,
1993.
[18] Balch, T., and Arkin, R.C., “Behavior-Based Formation Control for Multirobot
Teams,” IEEE Transactions on Robotics and Automation, vol. 14, no. 6, 1999, pp. 926–
939.
[19] Balch, T., Boone, G., Collins, T., Forbes, H., MacKenzie, D., and Santamaria, J., “Io,
Ganymede, and Callisto: A Multiagent Robot Trash-Collecting Team,” AI Maga-
zine, vol. 16, no. 2, 1995, pp. 39–51.
[20] Barlow, J.B., “General Principles: The Senses Considered as Physical Instruments,”
The Senses, ed. Barlow, J.B., and Mollon, J.B., Cambridge University Press, 1982, pp.
1–34.
[21] Bonasso, R.P., Firby, J., Gat, E., Kortenkamp, D., Miller, D., and Slack, M., “A Proven
Three-tiered Architecture for Programming Autonomous Robots,” Journal of Exper-
imental and Theoretical Artiﬁcial Intelligence, vol. 9, no. 2, 1997.
[22] Borenstein, J., and Koren, Y., “The Vector Field Histogram- Fast Obstacle Avoidance
for Mobile Robots,” IEEE Transactions on Robotics and Automation, vol.7, no.3, June,
1991, pp. 278–288.
[23] Borenstein, J., and Koren, Y., “Histogramic In-Motion Mapping for Mobile Robot
Obstacle Avoidance,” IEEE Transactions on Robotics and Automation, vol.7, no.4, Au-
gust, 1991, pp. 535–539.
[24] Bower, T.G.R., “The Evolution of Sensory Systems,” Perception: Essays in Honor of
James J. Gibson, ed. by R.B. MacLeod and H.L. Pick, Jr., Cornell University Press,
Ithaca, NY, 1974, pp. 141–153.
[25] Braitenberg, V., Vehicles: Experiments in Synthetic Psychology, MIT Press, 1984.
[26] Brooks, R., “Challenges for Complete Creature Architectures,” proceedings of First
Intenational Conference on Simulation of Adaptive Behavior, pp. 434–443.
[27] Brooks, R.A., "A Robust Layered Control System for a Mobile Robot," IEEE Journal
of Robotics and Automation (now IEEE Transactions on Robotics and Automation), vol.
1, no. 1, 1986, pp. 1–10.

Bibliography
451
[28] Brooks, R., Cambrian Intelligence: The Early History of the New AI, MIT Press, 1999.
[29] Brooks, R., Breazeal, C., Irie, R., Kemp, C., Marjanovic, M., Scassellati, B.,
Williamson, M., “Alternative Essences on Intelligence,” proceedings AAAI-98,
1998, pp. 961–968.
[30] Brooks, R., and Flynn, A., “Fast, Cheap and Out of Control,” AI Memo 1182, MIT
AI Laboratory, December, 1989.
[31] Cai, A.; Fukuda, T.; Arai, F., “Information sharing among multiple robots for coop-
eration in cellular robotic system,” proceedings 1997 IEEE/RSJ International Confer-
ence on Intelligent Robot and Systems, vol. 3, pp. 1768–74.
[32] Cattaneo, M., Cavalchini, A.G., Rogonoi, G.L., "Design and construction of a robotic
milking system," Sixth International Conference on Computers in Agriculture,
Cancun, Mexico, June 1996, pp. 155–60.
[33] Cervantes-Perez, F., “Visuomotor Coordination in Frogs and Toads,” The Handbook
of Brain Theory and Neural Networks, Michael A. Arbib, ed., MIT Press, pp. 1036–
1042.
[34] Choset, H. and Burdick, J., “Sensor Based Motion Planning: Incremental Construc-
tion of the Hierarchical Generalized Voronoi Graph,” To appear in International
Journal of Robotics Research, 2000.
[35] Choset, H. and Burdick, J., “Sensor Based Motion Planning: The Hierarchical Gen-
eralized Voronoi Graph,” to appear in International Journal of Robotics Research, 2000.
[36] Choset, H. and Burdick, J., “Sensor Based Motion Planning: The Hierarchical Gen-
eralized Voronoi Graph,” to appear in the International Journal of Robotics Research.
[37] Concise International Encyclopedia of Robotics: Applications and Automation, Richard C.
Dorf, editor-in-chief, John Wiley and Sons, Inc., NY, 1990.
[38] Congdon, C., Huber, M., Kortenkamp, D., Konolige, K., Myers, K., Safﬁotti, A., and
Ruspini, E., “Carmel vs. Flakey: A Comparison of Two Winners,” AI Magazine, vol.
14, no. 1, Spring 1993, pp. 49–57.
[39] Connell, J., Minimalist Mobile Robotics: A Colony Architecture for an Artiﬁcial Creature,
Academic Press, 1990.
[40] Connolly, C.I., and Grupen, R.A., “The applications of harmonic functions to robot-
ics,” Journal of Robotic Systems, vol. 10, October 1993, pp. 931–946.
[41] Dean, T., Allen, J., and Aloimonis, Y., Artiﬁcial Intelligence: Theory and Practice, Ben-
jamin/Cummings Publishing, 1995.
[42] Dean, T., Bonasso, R.P., “1992 AAAI Robot Exhibition and Competition,” AI Maga-
zine, vol. 14, no. 1, Spring 1993, pp. 35–48.
[43] Dean, T., and Wellman, M., Planning and Control, Morgan Kaufmann Publishers,
San Mateo, CA, 1991.

452
Bibliography
[44] deBerg, M., vanKreveld, M., Overmars, M., Schwarzkoph, O., Computational Geom-
etry: Theory and Applications, Springer-Verlag, 1999.
[45] Dickmanns, E.D. and Graefe, V., “Dynamic Monocular Machine Vision,” Int. J. Ma-
chine Vision and Applications, vol. 1, 1988, pp. 223–240.
[46] daaper. Draper, B., Collins, J., Brolio, A. Hanson and E. Riseman. “The Schema Sys-
tem,” International Journal of Computer Vision, vol. 2, 1989, pp. 209–250.
[47] Drumheller, M., “Mobile Robot Localization Using Sonar”, IEEE Transactions on Pat-
tern Analysis and Machine Intelligence, vol. 9, no . 2, Mar 1987, pp. 325–332.
[48] Duchon, A.P., and Warren, W.H., “Robot Navigation form a Gibsonian Viewpoint,”
1994 IEEE International conference on Systems, Man, and Cybernetics, vol. 3, pp. 2272–
7.
[49] Edwards, G.R., Burnard, R.H., Bewley, W.L., and Bullock, B.L., “The Ground Vehicle
Manager’s Associate,” Tech Report AIAA-94-1249-CP, 1994, pp. 520–526.
[50] Engelson, S. P., and McDermott, D. V., “Passive Robot Map Building with Explo-
ration Scripts,” Technical Report: YALEU/DCS/TR-898, Yale University, Dept. of
Computer Science, March, 1992.
[51] Erickson, J., "Supervised Space Robots are Needed in Space Exploration," Conference
on Intelligent Robotics in Field, Factory, Service, and Space, NASA Conference Publica-
tion 3251, vol. 2, 1994, pp. 705–716.
[52] Everett, H.R., Sensors for Mobile Robots: Theory and Application, A. K. Peters, Ltd,
Wellesley, MA, 1995.
[53] Firby, R.J., Kahn, R.E., Prokopwicz, P.N., and Swain, M.J., “An Architecture for
Vision and Action,” proceedings of 1995 International Joint Conference on Artiﬁcial
Intelligence, Morgan Kaufmann Pub., San Mateo, CA, vol. 1, 1995, pp. 72–79.
[54] Firby, R.J.,Prokopwicz, P.N., Swain, M.J.,Kahn, R.E., and Franklin, D., “Program-
ming CHIP for the IJCAI-95 Robot Competition,” AI Magazine, Spring 1996, pp.
71–81.
[55] Fowler, M., Scot, K., Booch, G., UML Distilled, 2nd ed., Addison-Wesley, 1999.
[56] Gardner, H., The Mind’s New Science, Basic Books, Inc., New York, 1987.
[57] Gat, E., “Reliable Goal-directed Reactive Control for Real-World Autonomous Mo-
bile Robots,” Ph.D. dissertation, Dept. of Computer Science, Virginia Polytechnic
Institute and State University, 1991.
[58] Gat, E., "Three-layer Architectures," Artiﬁcial Intelligence and Mobile Robots, D. Kor-
tenkamp, R. Bonasson, R. Murphy, editors, MIT Press, 1998.
[59] Gibson, J.J., The Ecological Approach to Visual Perception, Houghton Mifﬂin, Boston
MA, 1979.

Bibliography
453
[60] Gifford, K.K., and Morgenthaler, G.W., “Optimal Path Coverage Strategies for Plan-
etary Rover Vehicles,” 46th International Astronautical Federation Congress, Oslo,
Norway, October, 1995.
[61] Graefe, V. and Kuhnert, K.-D., “Toward a Vision-Based Robot with a Driver’s Li-
cense,” Proceedings, IEEE International Workshop on Intelligent Robots and Systems,
(IROS’88), Tokyo, 1988, pp. 627–632.
[62] Guzzoni, D., Cheyer, A., Julia, L., and Konolige, K., “Many Robots Make Short
Work: Report of the SRI International Mobile Robot Team,” AI Magazine, Spring,
vol. 18, no. 1, 1997, pp. 55–64.
[63] Haber, R.N., and Haber, L., "Why mobile robots need a spatial memory," SPIE Sensor
Fusion III, vol. 1383, 1990, pp. 411–424.
[64] The Handbook of Artiﬁcial Intelligence, A. Barr and E. Feigenbaum, editors, William
Kaufmann, Inc., 1981.
[65] Henderson, T., and Shilcrat, E., “Logical Sensor Systems,” Journal of Robotics Sys-
tems, vol. 1, no.2, 1984, pp. 169–193.
[66] Hinkle, D., Kortenkamp, D, Miller, D., “The 1995 Robot Competition and Exhibi-
tion,” AI Magazine, vol. 17, no. 1, Spring 1996, pp. 31–45.
[67] Hirose, S., untitled talk at the Planetary Rover Technology and Systems Workshop,
1996 IEEE International Conference on Robotics and Automation, April, 1996.
[68] Honderd, G, Jongkind, W, Klomp, C., Dessing, J., and Paliwoda, R., “Strategy and
control of an autonomous cow-milking robot system,” Robotics and Autonomous
Systems, vol. 7, no. 2–3, pp. 165–79.
[69] Horn, B.K.P., Robot Vision, MIT Press, 1986.
[70] Horswill, I., “Polly: A Vision-Based Artiﬁcial Agent,” proceedings AAAI-93, July
1993, pp. 824–829.
[71] Hughes, K., “Managing sensor performance uncertainty in a multi-sensor robotic
system,” Ph. D. thesis, Computer Science and Engineering, University of South
Florida, 1994.
[72] Hughes, K, and Murphy, R., “Ultrasonic robot localization using Dempster-Shafer
theory,” SPIE Stochastic Methods in Signal Processing, Image Processing, and Computer
Vision, 1992.
[73] Humanitarian Demining: Developmental Technologies 1998, CECOM NVESD, DoD Hu-
manitarian Demining R&D Program, Fort Belvoir, VA 22060-5606.
[74] Hyams, J., Powell, M., Murphy, R.R., “Position Estimation and Cooperative Navi-
gation of Micro-rovers Using Color Segmentation,” to appear Autonomous Robots,
special issue on CIRA’99.
[75] Iberall, T. and Lyons, D., “Towards Perceptual Robotics,” proceedings of 1984 IEEE
International Conference on Systems, Man and Cybernetics, Halifax, NS, Canada, Oct.
10–12, 1984, pp. 147–157.

454
Bibliography
[76] Jones, J., Flynn, A., Seiger, B., Mobile Robots: Inspiration to Implementation, second
ed., A.K. Peters, 1998.
[77] Konolige, K., and Myers, K., “The Saphira Architecture for Autonomous Mobile
Robots,” Artiﬁcial Intelligence and Mobile Robots, D. Kortenkamp, R. Bonasson, R.
Murphy, editors, MIT Press, 1998.
[78] Kortenkamp, D, Nourbakhsh, I., and Hinkle, D., “The 1996 AAAI Mobile Robot
Competition and Exhibition,” AI Magazine, Spring, vol. 18, no. 1, 1997, pp. 55–64.
[79] Kortenkamp, D., and Weymouth, T., “Topological Mapping for Mobile Robots using
a Combination of Sonar and Vision Sensing,” proceedings AAAI-94, 1994, pp. 974–
984.
[80] Kosaka, A. and Kak, A.C., “Fast Vision-Guided Mobile Robot Navigation using
Model-Based Reasoning and Prediction of Uncertainties,” proceedings Computer
Vision, Graphics, and Image Processing – Image Understanding, November 1992, pp.
271–329.
[81] Kuipers, B, and Byun, Y.T., “A Robot Exploration and Mapping Strategy Based on a
Semantic Hierarchy of Spatial Representations,” Robotics and Autonomous Systems,
vol. 8, 1991, pp. 47–63.
[82] Latombe, J.C., Robot Motion Planning, Kluwer Academic, 1990.
[83] Lee, D., “The Functions of Vision,” Modes of Perceiving and Processing Information, ed.
by H.L. Pick, Jr. and E. Saltzman, John Wiley and Sons, NY, 1978, pp. 159–170.
[84] LeGrand, R., and Luo, R.C., “LOLA: Object Manipulation in an Unstructured Envi-
ronment,” AI Magazine, vol. 17, no. 1, 1996, pp. 63–70.
[85] Levitt, T.S., and Lawton, D.T., “Qualitative Navigation for Mobile Robots,” AI Jour-
nal, vol. 44, no. 3, Aug 1990, pp. 305–360.
[86] Lewis, H.R., Papadimitriou, C.H., Elements of the Theory of Computation, Prentice-
Hall, Inc., Englewood Cliffs, New Jersey, 1981.
[87] Lim, W., and Eilbert, J., “Plan-Behavior Interaction in Autonomous Navigation,”
SPIE Mobile Robots V, Boston, MA, Nov. 8-9, 1990, pp. 464–475.
[88] Marr, D., Vision: A Computational Investigation into the Human Representation and Pro-
cessing of Visual Information, W.H. Freeman and Co., 1982.
[89] Mataric, M., “Behavior-Based Control: Main Properties and Implications,” proceed-
ings of Workshop on Intelligent Control Systems, IEEE International Conference on Ro-
botics and Automation, Nice, France, 1992.
[90] Mataric, M.,“Minimizing Complexity in Controlling a Mobile Robot Population,”
proceedings, 1992 IEEE International Conference on robotics and Automation, Nice,
France, May, 1992, pp. 830–835.
[91] Matthies, L., and Elfes, A., “Probabilistic Estimation Mechanisms and Tesselated
Representations for Sensor Fusion,” SPIE Sensor Fusion: Spatial Reasoning and Scene
Interpretation, vol. 1003, 1988, pp. 1–11.

Bibliography
455
[92] McKenzie, D.C.; Arkin, R.C.; Cameron, J.M., “ Multiagent mission speciﬁcation and
execution,” Autonomous Robots, vol.4, no.1, 1997, p. 29–52.
[93] Meystel, A., “Knowledge Based Nested Hierarchical Control,” in Advances in Au-
tomation and Robotics, vol. 2, Ed. G. Saridis, JAI Press, Greenwich, CT, 1990, pp.
63–152.
[94] Moravec, H., Mind Children, Harvard University Press, 1988.
[95] Moravec, H.P., “Sensor Fusion in Certainty Grids for Mobile Robots,” AI Magazine,
vol. 9, no. 2, Summer, 1988, pp. 61–74.
[96] Moravec, H., Robot: Mere Machine to Transcendent Mind, Oxford Press, 1998.
[97] Murphy, R.R., “A Strategy for the Fine Positioning of a Mobile Robot using Tex-
ture,” proceedings of SPIE Mobile Robots IV, Philadephia, PA, Nov. 5-10, 1989, pp.
267–279.
[98] Murphy, R. R., “An Artiﬁcial Intelligence Approach to the 1994 AUVS Unmanned
Ground Vehicle Competition,” 1995 IEEE International Conference on Systems, Man
and Cybernetics, Oct. 1995, Vancouver, BC., pp. 1723–1728.
[99] Murphy, R.R., “Biological and Cognitive Foundations of Intelligent Sensor Fusion,”
IEEE Transactions on Systems, Man, and Cybernetics, vol. 26, No. 1, Jan, 1996, pp.
42–51.
[100] Murphy, R.R, “Use of Scripts for Coordinating Perception and Action,” IROS-96,
Nov. 1996, Osaka, Japan, pp. 156–161.
[101] Murphy, R., “1997 International Ground Robotics Vehicle Competition,” Robotics
Competition Corner, Robostics and Autonomous Systems, Elsevier, vol. 20, 1997.
[102] Murphy, R., “Dempster-Shafer Theory for Sensor Fusion in Autonomous Mobile
Robots,” IEEE Transactions on Robotics and Automation, vol. 14, no. 2, April, 1998.
[103] Murphy, R.R., Gomes, K., and Hershberger, D., “Ultrasonic Data Fusion as a Func-
tion of Robot Velocity,” 1996 SPIE Sensor Fusion and Distributed Robotic Agents, Nov.
1996, Boston, MA., pp. 114–126.
[104] Murphy, R. R., Hawkins, D.K., and Schoppers, M.J., “Reactive Combination of
Belief Over Time Using Direct Perception,” IJCAI-97, Nagoya, Japan, Aug. 1997,
pp. 1353–1358.
[105] Murphy, R.R., and Hughes, K., “Ultrasonic Robot Localization using Dempster-
Shafer Theory,” SPIE Stochastic Methods in Signal Processing, Image Processing, and
Computer Vision, invited session on Applications for Vision and Robotics, San
Diego, CA, July 19-24, 1992.
[106] Murphy, R., Hughes, K., Noll, E., and Marzilli, A., “Integrating Explicit Path Plan-
ning with Reactive Control for Mobile Robots using Trulla,” Robotics and Autono-
mous Systems, no. 27, 1999, pp. 225–245.

456
Bibliography
[107] Murphy, R., and Mali, A., “Lessons Learned in Integrating Sensing into Autono-
mous Mobile Robot Architectures,” Journal of Experimental and Theoretical Artiﬁcial
Intelligence special issue on Software Architectures for Hardware Agents, vol. 9, no. 2,
1997, pp. 191–209.
[108] Murphy, R., and Sprouse, J., “Strategies for Searching an Area with Semi-
Autonomous Mobile Robots,” 2nd Conference on Robotics for Challenging Environ-
ments, June 1-6, 1996, Albuquerque, NM. pp. 15–21.
[109] Neisser, U., Cognition and Reality, W.H. Freeman, 1976.
[110] Neisser, U., “Direct Perception and Recognition as Distinct Perceptual Systems,”
address presented to the Cognitive Science Society, August, 1991.
[111] Nelson, R.C., “Visual homing using an associative memory,” Biological Cybernetics,
vol. 65, no. 4, 1991, pp. 281–291.
[112] Nourbakhsh, I., Powers, R., Birchﬁeld, S., “DERVISH: an ofﬁce-navigating robot,”
AI Magazine, vol. 16, no. 2, pp. 53–60.
[113] Oliveira, E., Fischer, K., and Stephankova, O., “Multi-agent systems: which re-
search for which applications,” Robotics and Autonomous Systems, vol. 27, 1999, pp.
91–106.
[114] Parker, L.E., “ALLIANCE: An Architecture for Fault Tolerant Multirobot Cooper-
ation,” IEEE Transactions on Robotics and Automation, vol. 14, no. 2, pp. 2200–240.
[115] Penrose, Roger, The Emperor’s New Mind: Concerning Computers, Minds, and the Laws
of Physics, Oxford University Press, Oxford, 1989.
[116] Pomerleau, D., and Jochem, T., “Rapidly Adapting Machine Vision for Automated
Vehicle Steering,” IEEE Expert, Apr., 1996.
[117] Premvuti, S., and Yuta, S., “Considerations on the cooperation of muliple auton-
omous mobile robots,” proceedings of IEEE International Workshop on Intelligent
Robots and Systems (IROS), 1990.
[118] Raibert, M., Legged Robots that Balance, MIT Press, 1986.
[119] Readings in Distributed Artiﬁcial Intelligence, A. Bond and L. Gasser, editors, Morgan
Kaufmann, 1988.
[120] Rich, E., and Knight, K., Artiﬁcial Intelligence, McGraw Hill Text, 1991.
[121] Rosenblatt, J.K, and Thorpe, C.E., “Combining Multiple Goals in a Behavior-
Based Architecture,” proceedings 1995 IEEE/RSJ International Conference on Intel-
ligent Robots and Systems, Pittsburgh, PA, Aug. 5-9, 1995, pp. 136–141.
[122] Schach, S, Classical and Object-Oriented Software Engineering, 3rd edition, Richard
D. Irwin publishers, 1996.
[123] Schank, R. C., and Riesbeck, C. K., Inside Computer Understanding: Five Programs
Plus Miniatures, Lawrence Erlbaum Associates, Hillsdale, NJ, 1981.

Bibliography
457
[124] Schöne, H., Spatial Orientation: The Spatial Control of Behavior in Animals and Man,
translated by C. Strausfeld, Princeton University Press, NJ, 1984.
[125] Schultz, A., Adams, W., and Yamauchi, B., “Integrating Exploration, Localization,
Navigation and Planning With a Common Representation,” Autonomous Robots,
vol. 6 no. 3, May 1999, pp. 293–308.
[126] Shafer, G., A Mathematical Theory of Evidence, Princeton University Press, 1976.
[127] Shaffer, G., Gonzalez, J., Stentz, A., “Comparison of two range-based pose estima-
tors for a mobile robot,” SPIE Mobile Robots VI, Boston, MA, USA; Nov. 1992, pp.
661–7.
[128] Sheridan, T., “Space Teleoperation Through Time Delay: Review and Prognosis,”
IEEE Transactions on Robotics and Automation, vol. 9, no. 5, Oct. 1993, pp. 592–605.
[129] Simmons, R., AI Magazine, Summer, vol. 16, no. 2, 1995, pp. 19–30.
[130] Simmons, R., Fernandez, J., Goodwin, R., Koenig, S., O’Sullivan, J., “Xavier: An
Autonomous Mobile Robot on the Web,” Robotics and Automation Society Magazine,
June, 1999.
[131] Simmons, R., Goodwin, R., Haigh, K., Koenig, S., and O’Sullivan, J., “A Layered
Architecture for Ofﬁce Delivery Robots,” proceedings Autonomous Agents 97, 1997,
pp. 245–252.
[132] Slack, M., “Navigation templates: mediating qualitative guidance and quantitative
control in mobile robots,” IEEE Transactions on Systems, Man and Cybernetics, vol. 23,
no. 2, 1993, p. 452–66.
[133] Smith, R., and Cheeseman, P., “On the Representation of and Estimation of Spatial
Uncertainty,” International Journal of Robotics Research, vol. 5, 1986, pp. 56–68.
[134] “Sojourner’s ‘Smarts’ Reﬂect Latest in Automation,” JPL Press Release, Aug. 8,
1997.
[135] Stark, L., and Bowyer, K., Generic Object Recognition Using Form and Function, World
Scientiﬁc, 1996.
[136] Stentz, A., “The Focussed D* Algorithm for Real-Time Replanning,” proceedings
1995 International Joint Conference on Artiﬁcial Intelligence,Montreal, CA, Aug. 1995,
pp. 1652–1659.
[137] Swain, M., and Ballard, D., “Color indexing,” International Journal of Computer Vi-
sion, vol. 7, 1991, pp. 11–32.
[138] Thorpe, C.E., “Path Relaxation: Path Planning for a Mobile Robot,” 1984 Interna-
tional Conference on AI (AAAI-84), 1984, pp. 318–321.
[139] Thrun,S., Bennewitz, M., Burgard, M., Dellaert, F., Fox, D., Haehnel, C., Rosenberg,
C., Roy, N., Schulte, J., and Schulz, D., “MINERVA: A second generation mobile
tour-guide robot,” proceedings 1999 IEEE International Conference on Robotics and
Automation, vol. 3, pp. 1999–2005.

458
Bibliography
[140] Umbaugh, Scott E. C. Computer Vision and Image Processing, Englewood Cliffs,
NJ:Prentice-Hall, 1998.
[141] Uttal, W. R., “Teleoperators,” Scientiﬁc American, December 1989, pp. 124–129.
[142] Wampler, C., “Teleoperators, Supervisory Control,” Concise International Encyclo-
pedia of Robotics: Applications and Automation, R. C. Dorf, editor-in-chief, John Wiley
and Sons, Inc., 1990, p. 997.
[143] Winston, P.H., Artiﬁcial Intelligence, Addison-Wesley, 1992.

Index
3T, 258, 274–277, 283, 284, 290, 291
4-connected neighbors, 358
8-connected neighbors, 358
a priori, 43
abstract behavior, 158, 184
action-oriented sensor fusion, 200
action-perception cycle, 83
active cooperation, 303
active sensing, 196
active sensor, 196
Adams, B., 434
add-list, 46
adjustable autonomy, 444
admissibility condition, 362
affordance, 68, 85, 159
agent, 70
AI areas, 35, 36
Aibo, 435, 443
Albus, J., 54, 57, 59, 65, 283
algorithm deﬁnition, 44
ALLIANCE, 295, 307
allothetic, 75
Ambler, 280, 284
Angle, C., 72
Arbib, M., 67, 69, 91, 94, 96, 102, 437
architecture deﬁnition, 10
ARIEL, 419–421, 434
Arkin, R., 10, 91, 106, 107, 153, 200,
208, 258, 265, 291, 297, 304, 311,
312, 437
artiﬁcial intelligence, 15
artiﬁcial landmarks, 326
Asada, M., 312
Asimov, I., 3, 39, 103
associative navigation, 333
attention, 321
Attila, 152, 153
attractive ﬁeld, 125
AuRA, 258, 265–268, 272, 275, 284, 290,
291, 297
automatic guided vehicles, 24
Autonomous Land Vehicle (ALV), 335
Autonomous Robot Architecture (AuRA),
265
axioms, 47
background, 227
Bagel, 313
Bajcsy, R., 238
Balch, T., 174, 298, 300, 311
Ballard, D., 254
ballistic control, 22
Bayes rule, 380, 381, 384–387, 390, 407,
410, 421, 428
behavior, 73, 99, 108
behavior table, 169
behavior, innate, 75

460
Index
behavior, innate with memory, 76
behavior, learned, 76
behavior, sequence of innate, 75
Behavorial management, 262
binary image, 226
black factory, 25
Blitch, J., 40
Bonasso, R.P., 274, 433
Borenstein, J., 395
Bowyer, K., 88, 90
box canyon, 135
Braitenberg, V., 68, 102
Brigante, A., 255
Brooks, R., 106, 109, 113, 115, 120, 152,
191, 200, 254, 265, 290, 294, 312,
437, 445, 448
Brown, C., 254, 255
Bujold, 299
Burgard, W., 318
Byun, Y.T., 329–331, 349
Callisto, 296, 310
Calvin, S., 3, 103
camera calibration, 234
CANAMP, 166
cancellation, 83
Capek, K., 2, 3
CARMEL, 395, 396, 434
Cartographer, 264
castes, 300
causal chain, 185
CEBOT, 303
centralized control, 301
Chaplin, C., 18
Cheeseman, P., 328
CHIP, 146
Choset, H., 357, 427, 429, 430, 439
Clementine, 347, 350, 367, 368
closed world assumption, 53, 69
closed-loop control, 22
Cog, 152, 290, 445–448
Cognachrome, 226
cognitive fatigue, 31
cognitive psychology, 68
cognizant failure, 288
color histogramming, 228
color planes, 220
communication link, 28
competing sensors, 199
computational theory, 70
Computer vision, 218
conditional probabilities, 381
conﬁguration space, 353
Connell, J., 192, 437
conscious behavior, 73
continuous replanning, 370
control trading, 34
Cooperation, 303
cooperative mobility, 303
coordinated control program, 157
correspondence, 232
cross-talk, 214
curvature-velocity method, 282
D* algorithm, 369
Dante, 280, 282, 284, 292
Darkstar, 31
DARPA, 27, 42, 283, 335, 436
data coupling, 135
Dean, T., 10
degrees of freedom, 353
delete-list, 46
deliberation, 260
deliberative, 258
deliberator, 260
demining, 442
Dempster’s rule, 390
Deneubourg, J-L., 312
depth map, 234
Devore, T., 192
Dickmanns, E., 64, 283
difference, 44
difference evaluator, 44
difference table, 45

Index
461
differential GPS (DGPS), 209
digitization bias, 358
Dijkstra single source shortest path al-
gorithm, 328, 340, 347, 349, 350,
374
direct perception, 68, 90
disparity, 233
display, 28
distinctive place, 329
Distributed Artiﬁcial Intelligence (DAI),
294
distributed control, 301
dominance, 83
Draper, B., 91
drop off, exponential, 128
drop off, linear, 128
Drumheller, M., 254
EagleEyes, 444
echo, 211
ecological approach, 85
ecological robotics, 111
ego-centric, 111
Elfes, A., 380
emergent behavior, 156
emergent social behavior, 295
Engelson, S., 334
Engleberger, J., 39, 152, 254
epipolar lines, 233
epipolar plane, 233
equilibrium, 82
Erickson, J., 17
Erratic, 65, 278
ethology, 68
EVAHR, 274
event-driven replanning, 371
events, 276
Everett, H., 206, 254
exceptions, 184
exponential drop off, 128
exproprioception, 202
exteroception, 202
fail upwards, 265
failed precondition, 50
false negative, 198
false positive, 198
fault tolerance, 207
feature-based localization, 415
feedback, 23
ﬁeld of view (FOV), 203
ﬁnal state, 177
ﬁnite state automata, 156, 174
Firby, J., 258, 274
Flakey, 65, 278
focus-of-attention, 186
foreground, 227
foreshortening, 214
frame problem, 53, 69
framegrabber, 220
frontier, 425
Fukuda, T., 303
functional cohesion, 135
Ganymede, 296, 310
Gat, E., 152, 274, 288
gateways, 326
General Problem Solver (GPS), 44
Generalized Voronoi Graphs, 357
Genghis, 71, 72, 152, 153, 283
Gibson, E.J., 103
Gibson, J.J., 68, 83, 85, 103
Gifford, K., 374
global occupancy grid, 417
goal node, 359
goal state, 44
Gomes, K., 412
Graefe, V., 64, 283
Growth Rate Operator, 399
GRUFF, 88
HARV, 153, 208
Hawkins, D., 192, 255, 256
Hazbot, 39
HelpMate, 3, 19, 25, 39

462
Index
Henning, F., 170
Hershberger, D., 206, 374, 412
Heterogeneity, 296
Heterogeneous teams, 296
heuristic, 31
heuristic function, 362
Hierarchical Paradigm, 6
high cohesion, 112
hill-climbing algorithm, 330, 348, 349
Hillis, D., 36
HIMM, 377, 395
Hirose, S., 303
holonomic, 354
homogeneous teams, 296
Honda humanoid robot, 446
horizontal decomposition, 106
horizontal FOV, 204
Horswill, I., 241, 242, 253
hotel load, 204
HSI, 223
hue, 223
Hughes, K., 271, 374, 425
Hybrid Deliberative/Reactive Paradigm,
9
Iberall, T., 91
iconic, 415
idiothetic, 75
ignorant coexistence, 306
image, 218
image function, 219
imprinting, 184
indexing, 186
industrial manipulator, 21
inertial navigation systems (INS), 208
information collection, 322
informed coexistence, 306
inhibition, 96, 119
initial node, 359
initial state, 44
innate releasing mechanisms, 77, 111
intelligent coexistence, 306
intelligent robot, 3
intensity, 223
interest operator, 232
interference, 294
internal state, 77, 115
Io, 296, 310
Jarvis, R., 255
Kaebling, L., 437
Kak, A., 436
Kambhamettu, C., 237
Khepera, 134, 136, 150, 151, 170, 216,
319
Kim, J-K., 312
Kismet, 152, 442, 443
Kitano, H., 312
Kludge, 253
Koenig, S., 433
Konolige, K., 278, 279, 291, 304
Koren, Y., 395
Kortenkamp, D., 274, 326, 433
Kosslyn, S., 36
Kuipers, B., 329–331, 349
ladar, 239
landmark, 326, 349
landmark pair boundaries, 336
laser radar, 239
Latombe, J.C., 374
Lawton, D., 335–337
layers of competence, 114
legged locomotion, 439
Lego Mindstorms, 101, 153, 155, 190,
252
LeGrand, R., 174
Levitt, T., 336
lidar, 239
lightweight vision, 241
linear drop off, 128
local control strategy, 330
local occupancy grid, 417
locomotion load, 205

Index
463
logical equivalence, 197
logical redundancy, 207
logical sensors, 197
logically redundant, 199
LOLA, 173, 174
Lorenz, K., 68, 75, 77, 83, 84, 156
low coupling, 112
Luddites, 18
Lyons, D., 91
Maes, P., 437
magnitude proﬁle, 126
Mahadevan, S., 437
Managerial styles, 264
Marr, D., 70, 102
Marsokhod, 234, 236, 239
Marvin, mail bot, 4, 40
Marzilli, A., 374
mask, 399
Mataric, M., 10, 305, 307, 312
meadow map, 354
means-ends analysis, 44
metric navigation, 316
Meystel, A., 54
Micire, M., 256
Miller, D., 152, 274
Minerva, 258, 317, 318, 323, 349, 442
minimal spanning tree algorithm, 350
Minsky, M., 40, 152, 448
Minten, B., 256
MIROSOT, 226, 255, 294, 301, 312
Mission Planner, 54, 264
modality, 197
Model-oriented styles, 264
modularity, 11
Moravec, H., 67, 210, 234–236, 254, 380,
414, 434
Morgenthaler, G., 374
Morris, E., 152, 312
motivation, 77, 308
motor schema, 92
multi-agents, 293
Murphy, R., 91, 200, 202, 268, 395, 412,
425, 433
Myers, K., 279, 291
NASREM, 59
natural landmarks, 326
navigation templates, 145
Navigator, 54
Navlab, 64, 283
negative obstacle, 236
Neisser, U., 68, 83, 90, 91, 111
Nelson, R., 334, 335
Neptune, 380
Nerd Herd, 305, 306, 311, 313
Nested Hierarchical Controller, 54
niche targetability, 11
Noll, E., 374
Nomad, 170, 192, 199, 204, 217, 244,
249, 256, 319, 376, 419
non-active cooperation, 303
Nourbakhsh, I., 323, 422, 423, 433
occupancy grid, 378
Odetics, 239, 240, 292
Omnibot, 166, 167, 170
open loop control, 22
open world assumption, 53, 69
operator, 44
opportunistic replanning, 367
optic ﬂow, 87, 88, 231
orientation region, 336
paradigm, 5
Parker, L., 295, 307, 312
passive sensor, 196
path planning, 321, 351
path relaxation, 356
Payton, D., 106, 107
Penrose, R., 16
percept, 157
perception, 162
perceptual distinguishability, 333
perceptual schema, 92

464
Index
perceptual stability, 333
performance monitoring, 262
Performance Monitoring and Problem
Solving, 264
perpendicular ﬁeld, 125
physical cooperation, 303
physical redundancy, 199, 207
Pick, A., 103
Pick, H., 103
Pilot, 54
Pioneer, 178, 180, 278, 291
pixels, 218
polar plot, 115
Polly, 241, 242
polymorphism, 438
Pomerleau, D., 64, 283
portability, 11
pose, 417
pose generation function, 418
Powell, M., 256
precision agriculture, 208
preconditions, 45
Predator, 31
predicates, 47
predictive displays, 31
primitive behavior, 158
probability function, 381
procedural cohesion, 136
proprioception, 202
PRS-lite, 279
quadtrees, 359
qualitative navigation, 316
QualNav, 336
quantitative navigation, 316, 351
Quinn, R., 71, 72
R2D2, 17
radial depth map, 241
Raibert, M., 441, 447, 448
rana computatrix, 94
range image, 234
range segmentation, 240
reactive behavior, 73
Reactive Paradigm, 8
reactive planning, 258
reactor, 260
Real-time Control System (RCS), 57
reasoning, 321
recognition, 90
reconﬁgurable robots, 303
rectiﬁed images, 233
redundant, 199
reﬂex, 126
reﬂexive behavior, 73
reﬂexive behaviors, ﬁxed-action pat-
terns, 74
reﬂexive behaviors, reﬂexes, 74
reﬂexive behaviors, taxes, 74
region segmentation, 226
registration, 416
releaser, 77
resolution, 204
Resource manager, 263
Reynolds, C., 312
RGB, 220
Rhino, 258, 317, 318, 323
ROBART, 254
RoboCup, 226, 255, 294, 301–303, 311,
312, 436
Robonaut, 446
robot acquiescence, 308
robot impatience, 308
robot paradigm primitives, 5
robotic paradigms, 5
Robotics Industries Association, 19
robustness, 11
Rogue, 313
Rug Warrior, 101, 152, 155, 190
rule encoding, 113
SAGE, 323
Saphira, 258, 278–280, 290, 291, 304
saturation, 223

Index
465
schema, 91
schema class, 91
schema instantiation (SI), 91
Schmitt, H., 14, 30, 38
Schultz, A., 419, 434
scripts, 156, 184
selective attention, 287
selective attraction, 141
selective availability, 209
semi-autonomous control, 33
sensing organization, 6
sensor, 28, 196
sensor fashion, 200
sensor ﬁssion, 200
Sensor fusion, 198
sensor suite, 203
sensors, complementary, 198
sensors, coordinated, 198
sensors, redundant, 198
Sequencer, 263
SFX, 202, 258, 268, 270–272, 274, 275,
282, 284, 287, 288, 290, 291, 338,
339, 341
shaft encoder, 207
Shakey, 8, 41, 42, 44, 55, 59, 62–65, 69,
110, 114, 254, 258, 278, 285, 286,
290
Shannon, information theory, 300
shape-shifting, 438
shared control, 33
Shear Majic, 18
side lobes, 211
Simmons, R., 278, 280, 282, 283, 292,
433
simulator sickness, 31
situated agent, 111
skill, 261
skills, 173, 184, 275
Skinner, B.F., 9
Slack, M., 145, 274
Smith, R., 328
snake robot, 439
social entropy, 300
societies, 293
software agents, 36
Sojourner, 15, 30, 34, 207, 208, 239, 283
Soldo, M., 214
sonar modal, 378
spatial memory, 321
specular reﬂection, 212, 214
Sprouse, J., 192, 255, 256
SRI, 41, 63, 277, 278, 290, 304
Stanford Cart, 110, 234, 254
Stark, L., 88–90
Start state, 175
state diagram, 174
State hierarchies, 264
state-set progression, 423
states, 175
Stein, L., 312
Stentz, A., 292, 369
stereo pair, 232
stereopsis, 231
stimulus-response, 73
Stone, P., 303
strategic, 272
Strips, 44
structural models, 88
sub-script, 185
subgoal obsession, 367
supervisory control, 33
suppression, 119
Swain, M., 228
swarm robots, 293
tactical behaviors, 272
tactile, 217
tangential ﬁeld, 125
Task Control Architecture (TCA), 258,
278, 280–282, 290, 291
taskable, 115
teach pendant, 24
telemanipulator, 20
teleoperation, 28

466
Index
teleoperation, local, 28
teleoperation, remote, 28
telepresence, 33
teloperation task characteristics, 32
template, 184
Terminator, 4
the 3 D’s, 16
Thorpe, C., 356
thresholding, 226
Thrun, S., 318
Tilden, M., 77
time of ﬂight, 210
time to contact, 87
Tinbergen, N., 68, 75, 77, 83, 84, 156
tolerance, 382
topological navigation, 316, 325
transducer, 196
transition function, 176
transition table, 341
unconditional probabilities, 381
uniform ﬁeld, 124
update rate., 277
Uranus, 235
Urban, 440
urban canyons, 209, 376
urban search and rescue, 442
vacuous belief function, 389
value, 223
vector summation, 123
vectors, 123
Veloso, M., 303
vertical decomposition, 106
vertical FOV, 204
VFH, 395
Viking, 283
virtual reality, 33
virtual sensor, 280
visual erosion, 222
Voronoi edge, 357
Waldron, K., 441
Walter, G., 64
waypoint, 286
waypoints, 351
web-bot, 36
Wellman, M., 10
Wheelesley, 444
Whitaker, W., 284, 292
Wiebe, P., 350
Wiener, N., 21, 442
world model, 43, 46
Xavier, 38, 280, 282, 313, 317, 319, 433
Yamauchi, B., 425
Yanco, H., 312, 442, 444
Young, J., 26


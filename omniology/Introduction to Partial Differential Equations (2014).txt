Undergraduate Texts in Mathematics
Peter J. Olver
Introduction to 
Partial Diﬀ erential Equations

Undergraduate Texts in Mathematics

Undergraduate Texts in Mathematics
Series Editors:
Sheldon Axler
San Francisco State University, San Francisco, CA, USA
Kenneth Ribet
University of California, Berkeley, CA, USA
Advisory Board:
Colin Adams, Williams College, Williamstown, MA, USA
Alejandro Adem, University of British Columbia, Vancouver, BC, Canada
Ruth Charney, Brandeis University, Waltham, MA, USA
Irene M. Gamba, The University of Texas at Austin, Austin, TX, USA
Roger E. Howe, Yale University, New Haven, CT, USA
David Jerison, Massachusetts Institute of Technology, Cambridge, MA, USA
Jeffrey C. Lagarias, University of Michigan, Ann Arbor, MI, USA
Jill Pipher, Brown University, Providence, RI, USA
Fadil Santosa, University of Minnesota, Minneapolis, MN, USA
Amie Wilkinson, University of Chicago, Chicago, IL, USA
For further volumes:
http://www.springer.com/series/666
Undergraduate Texts in Mathematics are generally aimed at third- and fourth-year undergraduate
mathematics students at North American universities. These texts strive to provide students and teachers
with new perspectives and novel approaches. The books include motivation that guides the reader to an
key concepts as well as exercises that strengthen understanding.
 
 
 
 
 
appreciation of interrelations among different aspects of the subject. They feature examples that illustrate

Peter J. Olver
Equations
Introduction to 
Partial Differential 

 
© Springer 
 201  
 
Printed on acid-free paper 
 
ISBN 978-3-319-02098-3
ISBN 978-3-319-02099-0 (eBook)
DOI 10.1007/978-3-319-02099-0
Springer Cham Heidelberg New York Dordrecht London 
Library of Congress Control Number: 
Springe
Mathematics Subject Classification: 35-01, 42-01, 65-01
University of Minnesota
ISSN 
-
 
0172
r is part of Springer Science+Business Media (www.springer.com)
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is 
concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on 
microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer 
software, or by similar or dissimilar methodology now known or hereafter developed. Exempted from this legal reservation are 
brief excerpts in connection with reviews or scholarly analysis or material supplied specifically for the purpose of being entered 
and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts 
thereof is permitted only under the provisions of the Copyright Law of the Publisher’s location, in its current version, and 
permission for use must always be obtained from Springer. Permissions for use may be obtained through RightsLink at the 
Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even 
in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and 
therefore free for general use. 
While the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors 
nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher 
makes no warranty, express or implied, with respect to the material contained herein. 
 
 
International Publishing Switzerland
6056
ISSN 
-
2197 5604 (electronic)
Peter J. Olver
School of Mathematics
Minneapolis, MN
USA
4
2013954394

To the memory of my father, Frank W.J. Olver (1924-2013) and mother
(née Smith, 1927-1980), whose love, patience, and guidance formed the heart of it all.
, Grace E. Olver


Preface
The momentous revolution in science precipitated by Isaac Newton’s calculus soon re-
vealed the central role of partial diﬀerential equations throughout mathematics and its
manifold applications.
Notable examples of fundamental physical phenomena modeled
by partial diﬀerential equations, most of which are named after their discovers or early
proponents, include quantum mechanics (Schr¨odinger, Dirac), relativity (Einstein), elec-
tromagnetism (Maxwell), optics (eikonal, Maxwell–Bloch, nonlinear Schr¨odinger), ﬂuid me-
chanics (Euler, Navier–Stokes, Korteweg–de Vries, Kadomstev–Petviashvili), superconduc-
tivity (Ginzburg–Landau), plasmas (Vlasov), magneto-hydrodynamics (Navier–Stokes +
Maxwell), elasticity (Lam´e, von Karman), thermodynamics (heat), chemical reactions
(Kolmogorov–Petrovsky–Piskounov), ﬁnance (Black–Scholes), neuroscience (FitzHugh–
Nagumo), and many, many more. The challenge is that, while their derivation as physi-
cal models — classical, quantum, and relativistic — is, for the most part, well established,
[57, 69], most of the resulting partial diﬀerential equations are notoriously diﬃcult to solve,
and only a small handful can be deemed to be completely understood. In many cases, the
only means of calculating and understanding their solutions is through the design of so-
phisticated numerical approximation schemes, an important and active subject in its own
right. However, one cannot make serious progress on their numerical aspects without a
deep understanding of the underlying analytical properties, and thus the analytical and
numerical approaches to the subject are inextricably intertwined.
This textbook is designed for a one-year course covering the fundamentals of partial
diﬀerential equations, geared towards advanced undergraduates and beginning graduate
students in mathematics, science, and engineering. No previous experience with the subject
is assumed, while the mathematical prerequisites for embarking on this course of study
will be listed below.
For many years, I have been teaching such a course to students
from mathematics, physics, engineering, statistics, chemistry, and, more recently, biology,
ﬁnance, economics, and elsewhere. Over time, I realized that there is a genuine need for
a well-written, systematic, modern introduction to the basic theory, solution techniques,
qualitative properties, and numerical approximation schemes for the principal varieties of
partial diﬀerential equations that one encounters in both mathematics and applications. It
is my hope that this book will ﬁll this need, and thus help to educate and inspire the next
generation of students, researchers, and practitioners.
While the classical topics of separation of variables, Fourier analysis, Green’s functions,
and special functions continue to form the core of an introductory course, the inclusion
of nonlinear equations, shock wave dynamics, dispersion, symmetry and similarity meth-
ods, the Maximum Principle, Huygens’ Principle, quantum mechanics and the Schr¨odinger
equation, and mathematical ﬁnance makes this book more in tune with recent developments
and trends. Numerical approximation schemes should also play an essential role in an in-
troductory course, and this text covers the two most basic approaches: ﬁnite diﬀerences
and ﬁnite elements.
vii

viii
Preface
On the other hand, modeling and the derivation of equations from physical phenomena
and principles, while not entirely absent, has been downplayed, not because it is unimpor-
tant, but because time constraints limit what one can reasonably cover in an academic
year’s course. My own belief is that the primary purpose of a course in partial diﬀerential
equations is to learn the principal solution techniques and to understand the underlying
mathematical analysis. Thus, time devoted to modeling eﬀectively lessens what can be ad-
equately covered in the remainder of the course. For this reason, modeling is better left to
a separate course that covers a wider range of mathematics, albeit at a more cursory level.
(Modeling texts worth consulting include [57, 69].) Nevertheless, this book continually
makes contact with the physical applications that spawn the partial diﬀerential equations
under consideration, and appeals to physical intuition and familiar phenomena to motivate,
predict, and understand their mathematical properties, solutions, and applications. Nor
do I attempt to cover stochastic diﬀerential equations — see [83] for this increasingly im-
portant area — although I do work through one important by-product: the Black–Scholes
equation, which underlies the modern ﬁnancial industry. I have tried throughout to bal-
ance rigor and intuition, thus giving the instructor ﬂexibility with their relative emphasis
and time to devote to solution techniques versus theoretical developments.
The course material has now been developed, tested, and revised over the past six years
here at the University of Minnesota, and has also been used by several other universities in
both the United States and abroad. It consists of twelve chapters along with two appendices
that review basic complex numbers and some essential linear algebra. See below for further
details on chapter contents and dependencies, and suggestions for possible semester and
year-long courses that can be taught from the book.
Prerequisites
The initial prerequisite is a reasonable level of mathematical sophistication, which includes
the ability to assimilate abstract constructions and apply them in concrete situations.
Some physical insight and familiarity with basic mechanics, continuum physics, elemen-
tary thermodynamics, and, occasionally, quantum mechanics is also very helpful, but not
essential.
Since partial diﬀerential equations involve the partial derivatives of functions, the most
fundamental prerequisite is calculus — both univariate and multivariate. Fluency in the
basics of diﬀerentiation, integration, and vector analysis is absolutely essential. Thus, the
student should be at ease with limits, including one-sided limits, continuity, diﬀerentiation,
integration, and the Fundamental Theorem. Key techniques include the chain rule, product
rule, and quotient rule for diﬀerentiation, integration by parts, and change of variables in
integrals. In addition, I assume some basic understanding of the convergence of sequences
and series, including the standard tests — ratio, root, integral — along with Taylor’s
theorem and elementary properties of power series. (On the other hand, Fourier series will
be developed from scratch.)
When dealing with several space dimensions, some familiarity with the key construc-
tions and results from two- and three-dimensional vector calculus is helpful: rectangular
(Cartesian), polar, cylindrical, and spherical coordinates; dot and cross products; partial
derivatives; the multivariate chain rule; gradient, divergence, and curl; parametrized curves
and surfaces; double and triple integrals; line and surface integrals, culminating in Green’s
Theorem and the Divergence Theorem — as well as very basic point set topology: notions of

Preface
open, closed, bounded, and compact subsets of Euclidean space; the boundary of a domain
and its normal direction; etc. However, all the required concepts and results will be quickly
reviewed in the text at the appropriate juncture: Section 6.3 covers the two-dimensional
material, while Section 12.1 deals with the three-dimensional counterpart.
Many solution techniques for partial diﬀerential equations, e.g., separation of variables
and symmetry methods, rely on reducing them to one or more ordinary diﬀerential equa-
tions. In order the make progress, the student should therefore already know how to ﬁnd
the general solution to ﬁrst-order linear equations, both homogeneous and inhomogeneous,
along with separable nonlinear ﬁrst-order equations, linear constant-coeﬃcient equations,
particularly those of second order, and ﬁrst-order linear systems with constant-coeﬃcient
matrices, in particular the role of eigenvalues and the construction of a basis of solutions.
The student should also be familiar with initial value problems, including statements of
the basic existence and uniqueness theorems, but not necessarily their proofs. Basic ref-
erences include [18, 20, 23], while more advanced topics can be found in [52, 54, 59]. On
the other hand, while boundary value problems for ordinary diﬀerential equations play a
central role in the analysis of partial diﬀerential equations, the book does not assume any
prior experience, and will develop solution techniques from the beginning.
Students should also be familiar with the basics of complex numbers, including real
and imaginary parts; modulus and phase (or argument); and complex exponentials and
Euler’s formula.
These are reviewed in Appendix A. In the numerical chapters, some
familiarity with basic computer arithmetic, i.e., ﬂoating-point and round-oﬀerrors, is as-
sumed. Also, on occasion, basic numerical root ﬁnding algorithms, e.g., Newton’s Method;
numerical linear algebra, e.g., Gaussian Elimination and basic iterative methods; and nu-
merical solution schemes for ordinary diﬀerential equations, e.g., Runge–Kutta Methods,
are mentioned.
Students who have forgotten the details can consult a basic numerical
analysis textbook, e.g., [24, 60], or reference volume, e.g., [94].
Finally, knowledge of the basic results and conceptual framework provided by modern
linear algebra will be essential throughout the text. Students should already be on familiar
terms with the fundamental concepts of vector space, both ﬁnite- and inﬁnite-dimensional,
linear independence, span, and basis, inner products, orthogonality, norms, and Cauchy–
Schwarz and triangle inequalities, eigenvalues and eigenvectors, determinants, and linear
systems. These are all covered in Appendix B; a more comprehensive and recommended
reference is my previous textbook, [89], coauthored with my wife, Cheri Shakiban, which
provides a ﬁrm grounding in the key ideas, results, and methods of modern applied linear
algebra. Indeed, Chapter 9 here can be viewed as the next stage in the general linear
algebraic framework that has proven to be so indispensable for the modern analysis and
numerics of not just linear partial diﬀerential equations but, indeed, all of contemporary
pure and applied mathematics.
While applications and solution techniques are paramount, the text does not shy away
from precise statements of theorems and their proofs, especially when these help shed
light on the applications and development of the subject. On the other hand, the more
advanced results that require analytical sophistication beyond what can be reasonably
assumed at this level are deferred to a subsequent, graduate-level course. In particular,
the book does not assume that the student has taken a course in real analysis, and hence,
while the basic ideas underlying Hilbert space are explained in the context of Fourier
analysis, no knowledge of measure theory or Lebesgue integration is neither assumed nor
used.
Consequently, the precise deﬁnitions of Hilbert space and generalized functions
(distributions) are necessarily left somewhat vague, with the level of detail being similar
ix

Preface
to that found in a basic physics course on quantum mechanics. Indeed, one of the goals of
the course is to inspire mathematics students (and others) to take a rigorous real analysis
course, because it is so indispensable to the more advanced theory and applications of
partial diﬀerential equations that build on the material presented here.
Outline of Chapters
The ﬁrst chapter is brief and serves to set the stage, introducing some basic notation
and describing what is meant by a partial diﬀerential equation and a (classical) solution
thereof. It then describes the basic structure and properties of linear problems in a general
sense, appealing to the underlying framework of linear algebra that is summarized in Ap-
pendix B. In particular, the fundamental superposition principles for both homogeneous
and inhomogeneous linear equations and systems are employed throughout.
The ﬁrst three sections of Chapter 2 are devoted to ﬁrst-order partial diﬀerential equa-
tions in two variables — time and a single space coordinate — starting with simple linear
cases. Constant-coeﬃcient equations are easily solved, leading to the important concepts
of characteristic and traveling wave. The method of characteristics is then extended, ini-
tially to linear ﬁrst-order equations with variable coeﬃcients, and then to the nonlinear
case, where most solutions break down into discontinuous shock waves, whose subsequent
dynamics relies on the underlying physics. The material on shocks may be at a slightly
higher level of diﬃculty than the instructor wishes to deal with this early in the course,
and hence may be downplayed or even omitted, perhaps returned to at a later stage, e.g.,
when studying Burgers’ equation in Section 8.4, or when the concept of weak solution
is introduced in Chapter 10. The ﬁnal section of Chapter 2 is essential, and shows how
the second-order wave equation can be reduced to a pair of ﬁrst-order partial diﬀerential
equations, thereby producing the celebrated solution formula of d’Alembert.
Chapter 3 covers the essentials of Fourier series, which is the most important tool in
our analytical arsenal. After motivating the subject by adapting the eigenvalue method for
solving linear systems of ordinary diﬀerential equations to the heat equation, the remainder
of the chapter develops basic Fourier series analysis, in both real and complex forms. The
ﬁnal section investigates the various modes of convergence of Fourier series: pointwise,
uniform, in norm.
Along the way, Hilbert space and completeness are introduced, at
an appropriate level of rigor. Although more theoretical than most of the material, this
section is nevertheless strongly recommended, even for applications-oriented students, and
can serve as a launching pad for higher-level analysis.
Chapter 4 immediately delves into the application of Fourier techniques to construct
solutions to the three paradigmatic second-order partial diﬀerential equations in two in-
dependent variables — the heat, wave, and Laplace/Poisson equations — via the method
of separation of variables. For dynamical problems, the separation of variables approach
reinforces the importance of eigenfunctions. In the case of the Laplace equation, separation
is performed in both rectangular and polar coordinates, thereby establishing the averaging
property of solutions and, consequently, the Maximum Principle as important by-products.
The chapter concludes with a short discussion of the classiﬁcation of second-order partial
diﬀerential equations, in two independent variables, into parabolic, hyperbolic, and elliptic
categories, emphasizing their disparate natures and the role of characteristics.
Chapter 5 is the ﬁrst devoted to numerical approximation techniques for partial
diﬀerential equations.
Here the emphasis is on ﬁnite diﬀerence methods.
All of the
x

Preface
preceding cases are discussed: heat equation, transport equations, wave equation, and
Laplace/Poisson equation. The student learns that, in contrast to the ﬁeld of ordinary
diﬀerential equations, numerical methods must be specially adapted to the particularities
of the partial diﬀerential equation under investigation, and may well not converge unless
certain stability constraints are satisﬁed.
Chapter 6 introduces a second important solution method, founded on the notion of a
Green’s function. Our development relies on the use of distributions (generalized functions),
concentrating on the extremely useful “delta function”, which is characterized both as an
unconventional limit of ordinary functions and, more rigorously but more abstractly, by
duality in function space.
While, as with Hilbert space, we do not assume familiarity
with the analysis tools required to develop the fully rigorous theory of such generalized
functions, the aim is for the student to assimilate the basic ideas and comfortably work
with them in the context of practical examples. With this in hand, the Green’s function
approach is then ﬁrst developed in the context of boundary value problems for ordinary
diﬀerential equations, followed by consideration of elliptic boundary value problems for the
Poisson equation in the plane.
Chapter 7 returns to Fourier analysis, now over the entire real line, resulting in the
Fourier transform.
Applications to boundary value problems are followed by a further
development of Hilbert space and its role in modern quantum mechanics. Our discussion
culminates with the Heisenberg Uncertainty Principle, which is viewed as a mathematical
property of the Fourier transform. Space and time considerations persuaded me not to
press on to develop the Laplace transform, which is a special case of the Fourier transform,
although it can be proﬁtably employed to study initial value problems for both ordinary
and partial diﬀerential equations.
Chapter 8 integrates and further develops several diﬀerent themes that arise in the
analysis of dynamical evolution equations, both linear and nonlinear. The ﬁrst section
introduces the fundamental solution for the heat equation, and describes applications in
mathematical ﬁnance through the celebrated Black–Scholes equation. The second section
is a brief discussion of symmetry methods for partial diﬀerential equations, a favorite topic
of the author and the subject of his graduate-level monograph [87]. Section 8.3 introduces
the Maximum Principle for the heat equation, an important tool, inspired by physics, in
the advanced analysis of parabolic problems. The last two sections study two basic higher-
order nonlinear equations. Burgers’ equation combines dissipative and nonlinear eﬀects,
and can be regarded as a simpliﬁed model of viscous ﬂuid mechanics. Interestingly, Burg-
ers’ equation can be explicitly solved by transforming it into the linear heat equation. The
convergence of its solutions to the shock-wave solutions of the limiting nonlinear transport
equation underlies the modern analytic method of viscosity solutions. The ﬁnal section
treats basic third-order linear and nonlinear evolution equations arising, for example, in
the modeling of surface waves. The linear equation serves to introduce the phenomenon of
dispersion, in which diﬀerent Fourier modes move at diﬀerent velocities, producing com-
mon physical eﬀects observed in, for instance, water waves. We also highlight the recently
discovered and fascinating Talbot eﬀect of dispersive quantization and fractalization on
periodic domains. The nonlinear Korteweg–de Vries equation has many remarkable prop-
erties, including localized soliton solutions, ﬁrst discovered in the 1960s, that result from
its status as a completely integrable system.
Before proceeding further, Chapter 9 takes time to formulate a general abstract frame-
work that underlies much of the more advanced analysis of linear partial diﬀerential equa-
tions. The material is at a slightly higher level of abstraction (although amply illustrated
xi

xii
Preface
by concrete examples), so the more computationally oriented reader may wish to skip
ahead to the last two chapters, referring back to the relevant concepts and general re-
sults in particular contexts as needed. Nevertheless, I strongly recommend covering at
least some of this chapter, both because the framework is important to understanding the
commonalities among various concrete instantiations, and because it demonstrates the per-
vasive power of mathematical analysis, even for those whose ultimate goal is applications.
The development commences with the adjoint of a linear operator between inner product
spaces — a powerful and far-ranging generalization of the matrix transpose — which nat-
urally leads to consideration of self-adjoint and positive deﬁnite operators, all illustrated
by ﬁnite-dimensional linear algebraic systems and boundary value problems governed by
ordinary and partial diﬀerential equations. A particularly important construction, forming
the foundation of the ﬁnite element numerical method, is the characterization of solutions
to positive deﬁnite boundary value problems via minimization principles. Next, general
results concerning eigenvalues and eigenfunctions of self-adjoint and positive deﬁnite op-
erators are established, which serve to explain the key features of reality, orthogonality,
and completeness that underlie Fourier and more general eigenfunction series expansions.
A general characterization of complete eigenfunction systems based on properties of the
Green’s function nicely ties together two of the principal themes of the text.
Chapter 10 returns to the numerical analysis of partial diﬀerential equations, intro-
ducing the powerful ﬁnite element method. After outlining the general construction based
on the preceding abstract minimization principle, we present its practical implementation,
ﬁrst for one-dimensional boundary value problems governed by ordinary diﬀerential equa-
tions and then for elliptic boundary value problems governed by the Laplace and Poisson
equations in the plane. The ﬁnal section develops an alternative approach, based on the
idea of a weak solution to a partial diﬀerential equation, a concept of independent inter-
est. Indeed, the nonclassical shock-wave solutions encountered in Section 2.3 are properly
characterized as weak solutions.
The ﬁnal two Chapters, 11 and 12, survey the analysis of partial diﬀerential equations
in, respectively, two and three space dimensions, concentrating, as before, on the Laplace,
heat, and wave equations. Much of the analysis relies on separation of variables, which, in
curvilinear coordinates, leads to new classes of special functions that arise as solutions to
certain linear second-order non-constant-coeﬃcient ordinary diﬀerential equations. Since
we are not assuming familiarity with this subject, the method of power series solutions to
ordinary diﬀerential equations is developed in some detail. We also present the methods
of Green’s functions and fundamental solutions, including their qualitative properties and
various applications. The material has been arranged according to spatial dimension rather
than equation type; thus Chapter 11 deals with the planar heat and wave equations (the
planar Laplace and Poisson equations having been treated earlier, in Chapters 4 and 6),
while Chapter 12 covers all their three-dimensional counterparts. This arrangement allows
a more orderly treatment of the required classes of special functions; thus, Bessel functions
play the leading role in Chapter 11, while spherical harmonics, Legendre/Ferrers functions,
and Laguerre polynomials star in Chapter 12. The last chapter also presents the Kirchhoﬀ
formula that solves the wave equation in three-dimensional space, an important conse-
quence being the validity of Huygens’ Principle concerning the localization of disturbances
in space, which, surprisingly, does not hold in a two-dimensional universe. The book cul-
minates with an analysis of the Schr¨odinger equation for the hydrogen atom, whose bound
states are the atomic energy levels underlying the periodic table, atomic spectroscopy, and
molecular chemistry.

Preface
xiii
Course Outlines and Chapter Dependencies
With suﬃcient planning and a suitably prepared and engaged class, most of the material
in the text can be covered in a year. The typical single-semester course will ﬁnish with
Chapter 6. Some pedagogical suggestions:
Chapter 1: Go through quickly, the main take-away being linearity and superposition.
Chapter 2: Most is worth covering and needed later, although Section 2.3, on shock waves,
is optional, or can be deferred until later in the course.
Chapter 3: Students that have already taken a basic course in Fourier analysis can move
directly ahead to the next chapter.
The last section, on convergence, is
important, but could be shortened or omitted in a more applied course.
Chapter 4: The heart of the ﬁrst semester’s course. Some of the material at the end of
Section 4.1 — Robin boundary conditions and the root cellar problem — is
optional, as is the very last subsection, on characteristics.
Chapter 5: A course that includes numerics (as I strongly recommend) should start with
Section 5.1 and then cover at least a couple of the following sections, the
selection depending upon the interests of the students and instructor.
Chapter 6: The material on distributions and the delta function is important for a student’s
general mathematical education, both pure and applied, and, in particular,
for their role in the design of Green’s functions. The proof of Green’s repre-
sentation formula (6.107) might be heavy going for some, and can be omitted
by just covering the preceding less-rigorous justiﬁcation of the logarithmic
formula for the free-space Green’s function.
Chapter 7: Sections 7.1 and 7.2 are essential, and convolution in Section 7.3 is also impor-
tant. Section 7.4, on Hilbert space and quantum mechanics, can easily be
omitted.
Chapter 8: All ﬁve sections are more or less independent of each other and, except for the
fundamental solution and maximum principle for the heat equation, not used
subsequently. Thus, the instructor can pick and choose according to interest
and time alotted.
Chapter 9: This chapter is at a more abstract level than the bulk of the text, and can
be skipped entirely (referring back when required), although if one intends
to cover the ﬁnite element method, the material in the ﬁrst three sections
leading to minimization principles is required. Chapters 11 and 12 can, if
desired, be launched into straight after Chapter 8, or even Chapter 7 plus
the material on the heat equation in Chapter 8.
Chapter 10: Again, for a course that includes numerics, ﬁnite elements is extremely im-
portant and well worth covering. The ﬁnal Section 10.4, on weak solutions,
is optional, particularly the revisiting of shock waves, although if this was
skipped in the early part of the course, now might be a good time to revisit
Section 2.3.
Chapters 11 and 12: These constitute another essential component of the classical partial
diﬀerential equations course.
The detour into series solutions of ordinary

xiv
Preface
diﬀerential equations is worth following, unless this is done elsewhere in the
curriculum. I recommend trying to cover as much as possible, although one
may well run out of time before reaching the end, in which case, consider
omitting the end of Section 11.6, on Chladni ﬁgures and nodal curves, Sec-
tion 12.6, on Kirchhoﬀ’s formula and Huygens’ Principle, and Section 12.7,
on the hydrogen atom. Of course, if Chapter 6, on Green’s functions, and
Section 8.1, on fundamental solutions, were omitted, those aspects will also
presumably be omitted here; even if they were covered, there is not a com-
pelling reason to revisit these topics in higher dimensions, and one may prefer
to jump ahead to the more novel material appearing in the ﬁnal sections.
Exercises and Software
Exercises appear at the end of almost every subsection, and come in a variety of genres.
Most sets start with some straightforward computational problems to develop and reinforce
the principal new techniques and ideas. Ability to solve these basic problems is a minimal
requirement for successfully assimilating the material. More advanced exercises appear
later on. Some are routine, but others involve challenging computations, computer-based
projects, additional practical and theoretical developments, etc. Some will challenge even
the most advanced reader. A number of straightforward technical proofs, as well as inter-
esting and useful extensions of the material, particularly in the later chapters, have been
relegated to the exercises to help maintain continuity of the narrative.
Don’t be afraid to assign only a few parts of a multi-part exercise.
I have found
the True/False exercises to be particularly useful for testing of a student’s level of under-
standing. A full answer is not merely a T or F, but must include a detailed explanation
of the reason, e.g., a proof or a counterexample, or a reference to a result in the text.
Many computer projects are included, particularly in the numerical chapters, where they
are essential for learning the practical techniques. However, computer-based exercises are
not tied to any speciﬁc choice of language or software; in my own course, Matlab is the
preferred programming platform. Some exercises could be streamlined or enhanced by the
use of computer algebra systems, such as Mathematica and Maple, but, in general, I
have avoided assuming access to any symbolic software.
As a rough guide, some of the exercises are marked with special signs:
♦
indicates an exercise that is referred to in the body of the text, or is important for
further development or applications of the subject. These include theoretical details,
omitted proofs, or new directions of importance.
♥indicates a project — usually a longer exercise with multiple interdependent parts.
♠indicates an exercise that requires (or at least strongly recommends) use of a computer.
The student could be asked either to write their own computer code in, say, Matlab,
Maple, or Mathematica, or to make use of pre-existing packages.
♣= ♠+ ♥indicates a more extensive computer project.
Movies
In the course of writing this book, I have made a number of movies to illustrate the
dynamical behavior of solutions and their numerical approximations. I have found that

Preface
xv
they are an extremely eﬀective pedagogical tool and strongly recommend showing them
in the classroom with appropriate commentary and discussion. They are an ideal medium
for fostering a student’s deep understanding and insight into the phenomena exhibited by
the at times indigestible analytical formulas — much better than the individual snapshots
that appear in the ﬁgures in the printed book.
While it is clearly impossible to include the movies directly in the printed text, the
electronic e-book version will contain direct links. In addition, I have posted all the movies
on my own web site, along with the Mathematica code used to generate them:
http://www.math.umn.edu/∼olver/mov.html
When a movie is available, the sign

appears in the ﬁgure caption.
Conventions and Notation
A complete list of symbols employed can be found in the Symbol Index that appears at
the end of the book.
Equations are numbered consecutively within chapters, so that, for example, (3.12)
refers to the 12th equation in Chapter 3, irrespecive of which section it appears in.
Theorems, lemmas, propositions, deﬁnitions, and examples are also numbered con-
secutively within each chapter, using a single scheme. Thus, in Chapter 1, Deﬁnition 1.2
follows Example 1.1, and precedes Proposition 1.3 and Theorem 1.4. I ﬁnd this numbering
system to be the most helpful for speedy navigation through the book.
References (books, papers, etc.) are listed alphabetically at the end of the text, and
are referred to by number.
Thus, [89] is the 89th listed reference, namely my Applied
Linear Algebra text.
Q.E.D. signiﬁes the end of a proof, an acronym for “quod erat demonstrandum”, which
is Latin for “which was to be demonstrated”.
The variables that appear throughout will be subject to consistent notational conven-
tions. Thus t always denotes time, while x, y, z represent (Cartesian) space coordinates.
Polar coordinates r, θ, cylindrical coordinates r, θ, z, and spherical coordinates r, θ, ϕ, will
also be used when needed, and our conventions appear at the appropriate places in the ex-
position; be espcially careful with the last case, since the angular variables θ, ϕ are subject
to two contradictory conventions in the literature. The above are almost always indepen-
dent variables in the partial diﬀerential equations under study; the dependent variables
or unknowns will mostly be denoted by u, v, w, while f, g, h and F, G, H represent known
functions, appearing as forcing terms or in boundary data. See Chapter 4 for our conven-
tion, used in diﬀerential geometry, used to denote functions in diﬀerent coordinate systems,
i.e., u(x, y) versus u(r, θ).
In accordance with standard contemporary mathematical notation, the “blackboard
bold” letter R denotes the real number line, C denotes the ﬁeld of complex numbers, Z
denotes the set of integers, both positive and negative, while N denotes the natural numbers,
i.e., the nonnegative integers, including 0. Similarly, Rn and Cn denote the corresponding
n-dimensional real and complex vector spaces consisting of n–tuples of elements of R and
C, respectively. The zero vector in each is denoted by 0.
Boldface lowercase letters, e.g., v, x, a, usually denote vectors (almost always column
vectors), whose entries are indicated by subscripts: v1, xi, etc. Matrices are denoted by
ordinary capital letters, e.g., A, C, K, M — but not all such letters refer to matrices; for

xvi
Preface
instance, V often refers to a vector space, while F is typically a forcing function. The entries
of a matrix, say A, are indicated by the corresponding subscripted lowercase letters: aij,
with i the row index and j the column index.
Angles are always measured in radians, although occasionally degrees will be men-
tioned in descriptive sentences. All trigonometric functions are evaluated on radian angles.
Following the conventions advocated in [85, 86], we use ph z to denote the phase of a
complex number z ∈C, which is more commonly called the argument and denoted by
arg z. Among the many reasons to prefer “phase” are to avoid potential confusion with
the argument x of a function f(x), as well as to be in accordance with the “Method of
Stationary Phase” mentioned in Chapter 8.
We use { f | C } to denote a set, where f gives the formula for the members of the
set and C is a (possibly empty) list of conditions. For example, { x | 0 ≤x ≤1 } means
the closed unit interval from 0 to 1, also written [0, 1], while { ax2 + bx + c | a, b, c ∈R }
is the set of real quadratic polynomials, and {0} is the set consisting only of the number
0. We use x ∈S to indicate that x is an element of the set S, while y ̸∈S says that y
is not an element. Set theoretic union and intersection are denoted by S ∪T and S ∩T,
respectively. The subset sign S ⊂U includes the possibility that the sets S and U might
be equal, although for emphasis we sometimes write S ⊆U. On the other hand, S ⊊U
speciﬁcally implies that the two sets are not equal. We use U \ S = { x | x ∈U, x ̸∈S } to
denote the set-theoretic diﬀerence, meaning all elements of U that do not belong to S. We
use the abbreviations max and min to denote the maximum and minimum elements of a
set of real numbers, or of a real-valued function.
The symbol ≡is used to emphasize when two functions are identically equal, so f(x) ≡
1 means that f is the constant function, equal to 1 at all values of x. It is also occasionally
used in modular arithmetic, whereby i ≡j mod n means i−j is divisible by n. The symbol
:= will deﬁne a quantity, e.g., f(x) := x2 −1. An arrow is used in two senses: ﬁrst, to
indicate convergence of a sequence, e.g., xn →x⋆as n →∞, or, alternatively, to indicate
a function, so f: X →Y means that the function f maps the domain set X to the image
or target set Y , with formula y = f(x). Composition of functions is denoted by f ◦g, while
f −1 indicates the inverse function. Similarly, A−1 denotes the inverse of a matrix A.
By an elementary function we mean a combination of rational, algebraic, trigono-
metric, exponential, logarithmic, and hyperbolic functions. Familiarity with their basic
properties is assumed. We always use log x for the natural (base e) logarithm — avoiding
the ugly modern notation ln x. On the other hand, the required properties of the various
special functions — the error and complementary error functions, the gamma function, Airy
functions, Bessel and spherical Bessel functions, Legendre and Ferrers functions, Laguerre
functions, spherical harmonics, etc. — will be developed as needed.
Summation notation is used throughout, so
n

i=1
ai denotes the ﬁnite sum a1 + a2 +
· · · + an or, if the upper limit is n = ∞, an inﬁnite series. Of course, the lower limit need
not be 1; if it is −∞and the upper limit is +∞, the result is a doubly inﬁnite series,
e.g., the complex Fourier series in Chapter 3. We use
lim
n →∞an to denote the usual limit
of a sequence an. Similarly, lim
x →a f(x) denotes the limit of the function f(x) at a point a,
while f(x−) =
lim
x →a−f(x) and f(x+) =
lim
x →a+ f(x) are the one-sided (left- and right-hand,
respectively) limits, which agree if and only if lim
x →a f(x) exists.
We will employ a variety of standard notations for derivatives. In the case of ordinary

Preface
xvii
derivatives, the most basic is the Leibniz notation du
dx for the derivative of u with respect to
x. As for partial derivatives, both the full Lebiniz notation ∂u
∂t , ∂u
∂x , ∂2u
∂x2 ,
∂3u
∂t ∂x2 , and the
more compact subscript notation ut, ux, uxx, utxx, etc. will be interchangeably employed
throughout; see also Chapter 1. Unless speciﬁcally mentioned, all functions are assumed to
be suﬃciently smooth that any indicated derivatives exist and the relevant mixed partial
derivatives are equal. Ordinary derivatives can also be indicated by the Newtonian notation
u′ instead of du
dx and u′′ for d2u
dx2 , while u(n) denotes the nth order derivative dnu
dxn . If the
variable is time, t, instead of space, x, then we may employ dots,
u,
u, instead of primes.
Deﬁnite integrals are denoted by
 b
a
f(x) dx, while

f(x) dx is the corresponding
indeﬁnite integral or anti-derivative. We assume familiarity only with the Riemann theory
of integration, although students who have learned Lebesgue integration may wish to take
advantage of that on occasion, e.g., during the discussion of Hilbert space.
Historical Matters
Mathematics is both a historical and a social activity, and many notable algorithms, the-
orems, and formulas are named after famous (and, on occasion, not-so-famous) mathe-
maticians, scientists, and engineers — usually, but not necessarily, the discover(s). The
text includes a succinct description of many of the named contributors. Readers who are
interested in more extensive historical details, complete biographies, and, when available,
portraits or photos, are urged to consult the informative University of St. Andrews Mac-
tutor web site:
http://www-history.mcs.st-andrews.ac.uk/history/index.html
Early prominent contributors to the subject include the Bernoulli family, Euler, d’Alembert,
Lagrange, Laplace, and, particularly, Fourier, whose remarkable methods in part sparked
the nineteenth century’s rigorization of mathematical analysis and then mathematics in
general, as pursued by Cauchy, Riemann, Cantor, Weierstrass, and Hilbert. In the twen-
tieth century, the subject of partial diﬀerential equations reached maturity, producing an
ever-increasing number of research papers, both theoretical and applied. Nevertheless, it
remains one of the most challenging and active areas of mathematical research, and, in
some sense, we have only scratched the surface of this deep and fascinating subject.
Textbooks devoted to partial diﬀerential equations began to appear long ago. Of par-
ticular note, Courant and Hilbert’s monumental two-volume treatise, [34, 35], played a
central role in the development of applied mathematics in general, and partial diﬀeren-
tial equations in particular. Indeed, it is not an exaggeration to state that all modern
treatments, including this one, as well as large swaths of research, have been directly inﬂu-
enced by this magniﬁcent text. Modern undergraduate textbooks worth consulting include
[50, 91, 92, 114, 120], which are more or less at the same mathematical level but have a va-
riety of points of view and selection of topics. The graduate-level texts [38, 44, 61, 70, 99]
are recommended starting points for the more advanced reader and beginning researcher.
More specialized monographs and papers will be referred to at the appropriate junctures.
This book began life in 1999 as a part of a planned comprehensive introduction to
applied math, inspired in large part by Gilbert Strang’s wonderful text, [112]. After some

xviii
Preface
time and much eﬀort, it was realized that the original vision was much too ambitious a
goal, so my wife, Cheri Shakiban, and I recast the ﬁrst part as our applied linear algebra
textbook, [89]. I later decided that a large fraction of the remainder could be reworked
into an introduction to partial diﬀerential equations, which, after some time and classroom
testing, resulted in the book you are now reading.
Some Final Remarks
To the student:
You are about to delve into the vast and important ﬁeld of partial
diﬀerential equations. I hope you enjoy the experience and proﬁt from it in your future
studies and career, wherever they may take you. Please send me your comments. Did you
ﬁnd the explanations helpful or confusing? Were enough examples included? Were the
exercises of suﬃcient variety and appropriate level to enable you to learn the material? Do
you have suggestions for improvements to be incorporated into a new edition?
To the instructor: Thank you for adopting this text! I hope you enjoy teaching from
it as much as I enjoyed writing it. Whatever your experience, I want to hear from you. Let
me know which parts you liked and which you didn’t. Which sections worked and which
were less successful. Which parts your students enjoyed, which parts they struggled with,
and which parts they disliked. How can it be improved?
To all readers: Like every author, I sincerely hope that I have eliminated all errors in
the text. But, more realistically, I know that no matter how many times one proofreads,
mistakes still manage to squeeze through (or, worse, be generated during the editing pro-
cess). Please email me your questions, typos, mathematical errors, comments, suggestions,
and so on. The book’s dedicated web site
http://www.math.umn.edu/∼olver/pde.html
will actively maintain a comprehensive list of known corrections, commentary, feedback,
and resources, as well as links to the movies and Mathematica code mentioned above.

Preface
xix
Acknowledgments
I have immensely proﬁted from the many comments, corrections, suggestions, and remarks
by students and mathematicians over the years. I would like to particularly thank my
current and former colleagues at the University of Minnesota — Markus Keel, Svitlana
Mayboroda, Willard Miller, Jr., Fadil Santosa, Guillermo Sapiro, Hans Weinberger, and
the late James Serrin — for their invaluable advice and help. Over the past few years,
Ariel Barton, Ellen Bao, Stefanella Boatto, Ming Chen, Bernard Deconinck, Greg Pierce,
Thomas Scoﬁeld, and Steven Taylor all taught from these notes, and alerted me to a number
of errors, made valuable suggestions, and shared their experiences in the classroom.
I
would like to thank Kendall Atkinson, Constantine Dafermos, Mark Dunster, and Gil
Strang, for references and answering questions.
Others who sent me commentary and
corrections are Steven Brown, Bruno Carballo, Gong Chen, Neil Datta, Ren´e Gonin, Zeng
Jianxin, Ben Jordan, Charles Lu, Anders Markvardsen, Cristina Santa Marta, Carmen
Putrino, Troy Rockwood, Hullas Sehgal, Lubos Spacek, Rob Thompson, Douglas Wright,
and Shangrong Yang. The following students caught typos during various classes: Dan
Brinkman, Haoran Chen, Justin Hausauer, Matt Holzer, JeﬀGassmann, Keith Jackson,
Binh Lieu, Dan Ouellette, Jessica Senou, Mark Stier, Hullas Seghan, David Toyli, Tom
Trogdon, and Fei Zheng. While I didn’t always agree with or follow their suggestions, I
particularly want to thank the many reviewers of the book for their insightful comments
on earlier drafts and valuable suggestions.
I would like to thank Achi Dosanjh for encouraging me to publish this book with
Springer and for her enthusiastic encouragement and help during the production process.
I am grateful to David Kramer for his thorough job copyediting the manuscript. While
I did not always follow his suggested changes (and, somethimes, chose to deliberately go
against certain grammatical and stylistic conventions in the interests of clarity), they were
all seriously considered and the result is a much-improved exposition.
And last, but far from least, my mathematical family — my wife, Cheri Shakiban, my
father, Frank W.J. Olver, and my son, Sheehan Olver — had a profound impact with their
many comments, help, and advice over the years. Sadly, my father passed away at age 88
on April 23, 2013, and so never got to see the ﬁnal printed version. I am dedicating this
book to him and to my mother, Grace, who died in 1980, for their amazing inﬂuence on
my life.
Peter J. Olver
University of Minnesota
olver@umn.edu
http://www.math.umn.edu/∼olver
September 2013


Table of Contents
Preface
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
Chapter 1.
What Are Partial Diﬀerential Equations? . . . . . . . .
1
Classical Solutions
. . . . . . . . . . . . . . . . . . . . .
4
Initial Conditions and Boundary Conditions
. . . . . . . . . .
6
Linear and Nonlinear Equations
. . . . . . . . . . . . . . .
8
Chapter 2.
Linear and Nonlinear Waves
. . . . . . . . . . . . . 15
2.1. Stationary Waves
. . . . . . . . . . . . . . . . . . . . . . . . 16
2.2. Transport and Traveling Waves
. . . . . . . . . . . . . . . . . . 19
Uniform Transport
. . . . . . . . . . . . . . . . . . . . . 19
Transport with Decay . . . . . . . . . . . . . . . . . . . . 22
Nonuniform Transport . . . . . . . . . . . . . . . . . . . . 24
2.3. Nonlinear Transport and Shocks . . . . . . . . . . . . . . . . . . 31
Shock Dynamics
. . . . . . . . . . . . . . . . . . . . . . 37
More General Wave Speeds . . . . . . . . . . . . . . . . . . 46
2.4. The Wave Equation: d’Alembert’s Formula
. . . . . . . . . . . . . 49
d’Alembert’s Solution
. . . . . . . . . . . . . . . . . . . . 50
External Forcing and Resonance
. . . . . . . . . . . . . . . 56
Chapter 3.
Fourier Series . . . . . . . . . . . . . . . . . . . . 63
3.1. Eigensolutions of Linear Evolution Equations
. . . . . . . . . . . . 64
The Heated Ring
. . . . . . . . . . . . . . . . . . . . . . 69
3.2. Fourier Series
. . . . . . . . . . . . . . . . . . . . . . . . . . 72
Periodic Extensions . . . . . . . . . . . . . . . . . . . . . 77
Piecewise Continuous Functions . . . . . . . . . . . . . . . . 79
The Convergence Theorem . . . . . . . . . . . . . . . . . . 82
Even and Odd Functions . . . . . . . . . . . . . . . . . . . 85
Complex Fourier Series
. . . . . . . . . . . . . . . . . . . 88
3.3. Diﬀerentiation and Integration . . . . . . . . . . . . . . . . . . . 92
Integration of Fourier Series
. . . . . . . . . . . . . . . . . 92
Diﬀerentiation of Fourier Series . . . . . . . . . . . . . . . . 94
3.4. Change of Scale
. . . . . . . . . . . . . . . . . . . . . . . . . 95
3.5. Convergence of Fourier Series
. . . . . . . . . . . . . . . . . . . 98
Pointwise and Uniform Convergence . . . . . . . . . . . . . . 99
Smoothness and Decay
. . . . . . . . . . . . . . . . . .
104
Hilbert Space . . . . . . . . . . . . . . . . . . . . . . .
106
Convergence in Norm
. . . . . . . . . . . . . . . . . . .
109
Completeness . . . . . . . . . . . . . . . . . . . . . . .
112
Pointwise Convergence . . . . . . . . . . . . . . . . . . .
115
xxi
vii

Chapter 4.
Separation of Variables . . . . . . . . . . . . . . .
121
4.1. The Diﬀusion and Heat Equations . . . . . . . . . . . . . . . .
122
The Heat Equation
. . . . . . . . . . . . . . . . . . . .
124
Smoothing and Long Time Behavior . . . . . . . . . . . . .
126
The Heated Ring Redux . . . . . . . . . . . . . . . . . .
130
Inhomogeneous Boundary Conditions
. . . . . . . . . . . .
133
Robin Boundary Conditions
. . . . . . . . . . . . . . . .
134
The Root Cellar Problem
. . . . . . . . . . . . . . . . .
136
4.2. The Wave Equation
. . . . . . . . . . . . . . . . . . . . . .
140
Separation of Variables and Fourier Series Solutions . . . . . .
140
The d’Alembert Formula for Bounded Intervals . . . . . . . .
146
4.3. The Planar Laplace and Poisson Equations
. . . . . . . . . . . .
152
Separation of Variables
. . . . . . . . . . . . . . . . . .
155
Polar Coordinates . . . . . . . . . . . . . . . . . . . . .
160
Averaging, the Maximum Principle, and Analyticity . . . . . .
167
4.4. Classiﬁcation of Linear Partial Diﬀerential Equations . . . . . . . .
171
Characteristics and the Cauchy Problem . . . . . . . . . . .
174
Chapter 5.
Finite Diﬀerences
. . . . . . . . . . . . . . . . .
181
5.1. Finite Diﬀerence Approximations
. . . . . . . . . . . . . . . .
182
5.2. Numerical Algorithms for the Heat Equation
. . . . . . . . . . .
186
Stability Analysis . . . . . . . . . . . . . . . . . . . . .
188
Implicit and Crank–Nicolson Methods . . . . . . . . . . . .
190
195
The CFL Condition . . . . . . . . . . . . . . . . . . . .
196
Upwind and Lax–WendroﬀSchemes . . . . . . . . . . . . .
198
5.4. Numerical Algorithms for the Wave Equation
. . . . . . . . . . .
201
5.5. Finite Diﬀerence Algorithms for the Laplace and Poisson Equations
.
207
Solution Strategies
. . . . . . . . . . . . . . . . . . . .
211
Chapter 6.
Generalized Functions and Green’s Functions
. . . . .
215
6.1. Generalized Functions
. . . . . . . . . . . . . . . . . . . . .
216
The Delta Function . . . . . . . . . . . . . . . . . . . .
217
Calculus of Generalized Functions . . . . . . . . . . . . . .
221
The Fourier Series of the Delta Function . . . . . . . . . . .
229
6.2. Green’s Functions for One–Dimensional Boundary Value Problems . .
234
6.3. Green’s Functions for the Planar Poisson Equation . . . . . . . . .
242
Calculus in the Plane
. . . . . . . . . . . . . . . . . . .
242
The Two–Dimensional Delta Function . . . . . . . . . . . .
246
The Green’s Function
. . . . . . . . . . . . . . . . . . .
248
The Method of Images . . . . . . . . . . . . . . . . . . .
256
Table of Contents
5.3. Numerical Algorithms for First–Order Partial Diﬀerential Equations
.
xxii

Chapter 7.
Fourier Transforms . . . . . . . . . . . . . . . . .
263
7.1. The Fourier Transform . . . . . . . . . . . . . . . . . . . . .
263
Concise Table of Fourier Transforms . . . . . . . . . . . . .
272
7.2. Derivatives and Integrals
. . . . . . . . . . . . . . . . . . . .
275
Diﬀerentiation
. . . . . . . . . . . . . . . . . . . . . .
275
Integration . . . . . . . . . . . . . . . . . . . . . . . .
276
7.3. Green’s Functions and Convolution
. . . . . . . . . . . . . . .
278
Solution of Boundary Value Problems . . . . . . . . . . . .
278
Convolution
. . . . . . . . . . . . . . . . . . . . . . .
281
7.4. The Fourier Transform on Hilbert Space
. . . . . . . . . . . . .
284
Quantum Mechanics and the Uncertainty Principle
. . . . . .
286
Chapter 8.
Linear and Nonlinear Evolution Equations
. . . . . .
291
8.1. The Fundamental Solution to the Heat Equation . . . . . . . . . .
292
The Forced Heat Equation and Duhamel’s Principle . . . . . .
296
The Black–Scholes Equation and Mathematical Finance
. . . .
299
8.2. Symmetry and Similarity . . . . . . . . . . . . . . . . . . . .
305
Similarity Solutions . . . . . . . . . . . . . . . . . . . .
308
8.3. The Maximum Principle
. . . . . . . . . . . . . . . . . . . .
312
8.4. Nonlinear Diﬀusion
. . . . . . . . . . . . . . . . . . . . . .
315
Burgers’ Equation . . . . . . . . . . . . . . . . . . . . .
315
The Hopf–Cole Transformation . . . . . . . . . . . . . . .
317
8.5. Dispersion and Solitons . . . . . . . . . . . . . . . . . . . . .
323
Linear Dispersion . . . . . . . . . . . . . . . . . . . . .
324
The Dispersion Relation . . . . . . . . . . . . . . . . . .
330
The Korteweg–de Vries Equation
. . . . . . . . . . . . . .
333
Chapter 9.
A General Framework for
Linear Partial Diﬀerential Equations
. . .
339
9.1. Adjoints
. . . . . . . . . . . . . . . . . . . . . . . . . . .
340
Diﬀerential Operators . . . . . . . . . . . . . . . . . . .
342
Higher–Dimensional Operators
. . . . . . . . . . . . . . .
345
The Fredholm Alternative
. . . . . . . . . . . . . . . . .
350
9.2. Self–Adjoint and Positive Deﬁnite Linear Functions
. . . . . . . .
353
Self–Adjointness
. . . . . . . . . . . . . . . . . . . . .
354
Positive Deﬁniteness . . . . . . . . . . . . . . . . . . . .
355
Two–Dimensional Boundary Value Problems . . . . . . . . .
359
9.3. Minimization Principles . . . . . . . . . . . . . . . . . . . . .
362
Sturm–Liouville Boundary Value Problems . . . . . . . . . .
363
The Dirichlet Principle
. . . . . . . . . . . . . . . . . .
368
9.4. Eigenvalues and Eigenfunctions
. . . . . . . . . . . . . . . . .
371
Self–Adjoint Operators
. . . . . . . . . . . . . . . . . .
371
The Rayleigh Quotient
. . . . . . . . . . . . . . . . . .
375
Eigenfunction Series . . . . . . . . . . . . . . . . . . . .
378
Green’s Functions and Completeness
. . . . . . . . . . . .
379
xxiii
Table of Contents

9.5. A General Framework for Dynamics
. . . . . . . . . . . . . . .
385
Evolution Equations . . . . . . . . . . . . . . . . . . . .
386
Vibration Equations . . . . . . . . . . . . . . . . . . . .
388
Forcing and Resonance
. . . . . . . . . . . . . . . . . .
389
The Schr¨odinger Equation
. . . . . . . . . . . . . . . . .
394
Chapter 10.
Finite Elements and Weak Solutions
. . . . . . . .
399
10.1. Minimization and Finite Elements
. . . . . . . . . . . . . . .
400
10.2. Finite Elements for Ordinary Diﬀerential Equations . . . . . . . .
403
10.3. Finite Elements in Two Dimensions . . . . . . . . . . . . . . .
410
Triangulation . . . . . . . . . . . . . . . . . . . . . . .
411
The Finite Element Equations
. . . . . . . . . . . . . . .
416
Assembling the Elements
. . . . . . . . . . . . . . . . .
418
The Coeﬃcient Vector and the Boundary Conditions
. . . . .
422
Inhomogeneous Boundary Conditions
. . . . . . . . . . . .
424
10.4. Weak Solutions . . . . . . . . . . . . . . . . . . . . . . . .
427
Weak Formulations of Linear Systems . . . . . . . . . . . .
428
Finite Elements Based on Weak Solutions
. . . . . . . . . .
430
Shock Waves as Weak Solutions . . . . . . . . . . . . . . .
431
Chapter 11.
Dynamics of Planar Media . . . . . . . . . . . . .
435
11.1. Diﬀusion in Planar Media
. . . . . . . . . . . . . . . . . . .
435
Derivation of the Diﬀusion and Heat Equations . . . . . . . .
436
Separation of Variables
. . . . . . . . . . . . . . . . . .
439
Qualitative Properties . . . . . . . . . . . . . . . . . . .
440
Inhomogeneous Boundary Conditions and Forcing . . . . . . .
442
The Maximum Principle . . . . . . . . . . . . . . . . . .
443
11.2. Explicit Solutions of the Heat Equation
. . . . . . . . . . . . .
445
Heating of a Rectangle
. . . . . . . . . . . . . . . . . .
445
Heating of a Disk — Preliminaries
. . . . . . . . . . . . .
450
11.3. Series Solutions of Ordinary Diﬀerential Equations
. . . . . . . .
453
The Gamma Function . . . . . . . . . . . . . . . . . . .
453
Regular Points
. . . . . . . . . . . . . . . . . . . . . .
455
The Airy Equation
. . . . . . . . . . . . . . . . . . . .
459
Regular Singular Points
. . . . . . . . . . . . . . . . . .
463
Bessel’s Equation . . . . . . . . . . . . . . . . . . . . .
466
11.4. The Heat Equation in a Disk, Continued . . . . . . . . . . . . .
474
11.5. The Fundamental Solution to the Planar Heat Equation . . . . . .
481
11.6. The Planar Wave Equation
. . . . . . . . . . . . . . . . . .
486
Separation of Variables
. . . . . . . . . . . . . . . . . .
487
Vibration of a Rectangular Drum . . . . . . . . . . . . . .
488
Vibration of a Circular Drum . . . . . . . . . . . . . . . .
490
Scaling and Symmetry . . . . . . . . . . . . . . . . . . .
494
Chladni Figures and Nodal Curves
. . . . . . . . . . . . .
497
xxiv
Table of Contents

Chapter 12.
Partial Diﬀerential Equations in Space
. . . . . . .
503
12.1. The Three–Dimensional Laplace and Poisson Equations
. . . . . .
504
Self–Adjoint Formulation and Minimum Principle . . . . . . .
505
12.2. Separation of Variables for the Laplace Equation
. . . . . . . . .
507
Laplace’s Equation in a Ball
. . . . . . . . . . . . . . . .
508
The Legendre Equation and Ferrers Functions
. . . . . . . .
510
Spherical Harmonics . . . . . . . . . . . . . . . . . . . .
517
Harmonic Polynomials . . . . . . . . . . . . . . . . . . .
519
Averaging, the Maximum Principle, and Analyticity . . . . . .
521
12.3. Green’s Functions for the Poisson Equation
. . . . . . . . . . .
527
The Free–Space Green’s Function . . . . . . . . . . . . . .
528
Bounded Domains and the Method of Images . . . . . . . . .
531
12.4. The Heat Equation for Three–Dimensional Media . . . . . . . . .
535
Heating of a Ball
. . . . . . . . . . . . . . . . . . . . .
537
Spherical Bessel Functions . . . . . . . . . . . . . . . . .
538
The Fundamental Solution of the Heat Equation
. . . . . . .
543
12.5. The Wave Equation for Three–Dimensional Media
. . . . . . . .
545
Vibration of Balls and Spheres
. . . . . . . . . . . . . . .
547
12.6. Spherical Waves and Huygens’ Principle . . . . . . . . . . . . .
551
Spherical Waves
. . . . . . . . . . . . . . . . . . . . .
551
Kirchhoﬀ’s Formula and Huygens’ Principle
. . . . . . . . .
558
Descent to Two Dimensions
. . . . . . . . . . . . . . . .
561
12.7. The Hydrogen Atom
. . . . . . . . . . . . . . . . . . . . .
564
Bound States . . . . . . . . . . . . . . . . . . . . . . .
565
Atomic Eigenstates and Quantum Numbers
. . . . . . . . .
567
Appendix A.
Complex Numbers . . . . . . . . . . . . . . . .
571
Appendix B.
Linear Algebra . . . . . . . . . . . . . . . . . .
575
B.1. Vector Spaces and Subspaces
. . . . . . . . . . . . . . . . . .
575
B.2. Bases and Dimension
. . . . . . . . . . . . . . . . . . . . .
576
B.3. Inner Products and Norms
. . . . . . . . . . . . . . . . . . .
578
B.4. Orthogonality . . . . . . . . . . . . . . . . . . . . . . . . .
581
B.5. Eigenvalues and Eigenvectors . . . . . . . . . . . . . . . . . .
582
B.6. Linear Iteration . . . . . . . . . . . . . . . . . . . . . . . .
583
B.7. Linear Functions and Systems . . . . . . . . . . . . . . . . . .
585
References . . . . . . . . . . . . . . . . . . . . . . . . . .
589
Symbol Index
. . . . . . . . . . . . . . . . . . . . . . . .
595
Author Index
. . . . . . . . . . . . . . . . . . . . . . . .
603
Subject Index
. . . . . . . . . . . . . . . . . . . . . . . .
607
xxv
Table of Contents

Chapter 1
What Are Partial Diﬀerential Equations?
Let us begin by delineating our ﬁeld of study. A diﬀerential equation is an equation that
relates the derivatives of a (scalar) function depending on one or more variables.
For
example,
d4u
dx4 + d2u
dx2 + u2 = cos x
(1.1)
is a diﬀerential equation for the function u(x) depending on a single variable x, while
∂u
∂t = ∂2u
∂x2 + ∂2u
∂y2 −u
(1.2)
is a diﬀerential equation involving a function u(t, x, y) of three variables.
A diﬀerential equation is called ordinary if the function u depends on only a single
variable, and partial if it depends on more than one variable. Usually (but not quite always)
the dependence of u can be inferred from the derivatives that appear in the diﬀerential
equation. The order of a diﬀerential equation is that of the highest-order derivative that
appears in the equation. Thus, (1.1) is a fourth-order ordinary diﬀerential equation, while
(1.2) is a second-order partial diﬀerential equation.
Remark: A diﬀerential equation has order 0 if it contains no derivatives of the function
u. These are more properly treated as algebraic equations,† which, while of great interest
in their own right, are not the subject of this text. To be a bona ﬁde diﬀerential equation,
it must contain at least one derivative of u, and hence have order ≥1.
There are two common notations for partial derivatives, and we shall employ them
interchangeably. The ﬁrst, used in (1.1) and (1.2), is the familiar Leibniz notation that
employs a d to denote ordinary derivatives of functions of a single variable, and the ∂
symbol (usually also pronounced “dee”) for partial derivatives of functions of more than
one variable. An alternative, more compact notation employs subscripts to indicate par-
tial derivatives. For example, ut represents ∂u/∂t, while uxx is used for ∂2u/∂x2, and
∂3u/∂x2∂y for uxxy. Thus, in subscript notation, the partial diﬀerential equation (1.2) is
written
ut = uxx + uyy −u.
(1.3)
†
Here, the term “algebraic equation” is used only to distinguish such equations from true
“diﬀerential equations”. It does not mean that the deﬁning functions are necessarily algebraic,
e.g., polynomials. For example, the transcendental equation tan u = u, which appears later in
(4.50), is still regarded as an algebraic equation in this book.
DOI 10.1007/978-3-
-
-0_1
, 
1
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
319 02099
, © Springer International Publishing Switzerland 2014
.J.

2
1 What Are Partial Diﬀerential Equations?
We will similarly abbreviate partial diﬀerential operators, sometimes writing ∂/∂x as ∂x,
while ∂2/∂x2 can be written as either ∂2
x or ∂xx, and ∂3/∂x2∂y becomes ∂xxy = ∂2
x ∂y.
It is worth pointing out that the preponderance of diﬀerential equations arising in
applications, in science, in engineering, and within mathematics itself are of either ﬁrst
or second order, with the latter being by far the most prevalent. Third-order equations
arise when modeling waves in dispersive media, e.g., water waves or plasma waves. Fourth-
order equations show up in elasticity, particularly plate and beam mechanics, and in image
processing. Equations of order ≥5 are very rare.
A basic prerequisite for studying this text is the ability to solve simple ordinary diﬀer-
ential equations: ﬁrst-order equations; linear constant-coeﬃcient equations, both homoge-
neous and inhomogeneous; and linear systems. In addition, we shall assume some familiar-
ity with the basic theorems concerning the existence and uniqueness of solutions to initial
value problems. There are many good introductory texts, including [18, 20, 23]. More
advanced treatises include [31, 52, 54, 59]. Partial diﬀerential equations are considerably
more demanding, and can challenge the analytical skills of even the most accomplished
mathematician. Many of the most eﬀective solution strategies rely on reducing the partial
diﬀerential equation to one or more ordinary diﬀerential equations. Thus, in the course of
our study of partial diﬀerential equations, we will need to develop, ab initio, some of the
more advanced aspects of the theory of ordinary diﬀerential equations, including boundary
value problems, eigenvalue problems, series solutions, singular points, and special functions.
Following the introductory remarks in the present chapter, the exposition begins in
earnest with simple ﬁrst-order equations, concentrating on those that arise as models of
wave phenomena. Most of the remainder of the text will be devoted to understanding and
solving the three essential linear second-order partial diﬀerential equations in one, two,
and three space dimensions:† the heat equation, modeling thermodynamics in a continuous
medium, as well as diﬀusion of animal populations and chemical pollutants; the wave
equation, modeling vibrations of bars, strings, plates, and solid bodies, as well as acoustic,
ﬂuid, and electromagnetic vibrations; and the Laplace equation and its inhomogeneous
counterpart, the Poisson equation, governing the mechanical and thermal equilibria of
bodies, as well as ﬂuid-mechanical and electromagnetic potentials.
Each increase in dimension requires an increase in mathematical sophistication, as
well as the development of additional analytic tools — although the key ideas will have
all appeared once we reach our physical, three-dimensional universe. The three starring
examples — heat, wave, and Laplace/Poisson — are not only essential to a wide range
of applications, but also serve as instructive paradigms for the three principal classes of
linear partial diﬀerential equations — parabolic, hyperbolic, and elliptic. Some interesting
nonlinear partial diﬀerential equations, including ﬁrst-order transport equations modeling
shock waves, the second-order Burgers’ equation governing simple nonlinear diﬀusion pro-
cesses, and the third-order Korteweg–de Vries equation governing dispersive waves, will
also be discussed. But, in such an introductory text, the further reaches of the vast realm
of nonlinear partial diﬀerential equations must remain unexplored, awaiting the reader’s
more advanced mathematical excursions.
More generally, a system of diﬀerential equations is a collection of one or more equa-
tions relating the derivatives of one or more functions. It is essential that all the functions
†
For us, dimension always refers to the number of space dimensions. Time, although theoreti-
cally also a dimension, plays a very diﬀerent physical role, and therefore (at least in nonrelativistic
systems) is to be treated on a separate footing.

1 What Are Partial Diﬀerential Equations?
3
occurring in the system depend on the same set of variables. The symbols representing
these functions are known as the dependent variables, while the variables that they depend
on are called the independent variables. Systems of diﬀerential equations are called ordi-
nary or partial according to whether there are one or more independent variables. The
order of the system is the highest-order derivative occurring in any of its equations.
For example, the three-dimensional Navier–Stokes equations
∂u
∂t + u ∂u
∂x + v ∂u
∂y + w ∂u
∂z = −∂p
∂x + ν
 ∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2

,
∂v
∂t + u ∂v
∂x + v ∂v
∂y + w ∂v
∂z = −∂p
∂y + ν
 ∂2v
∂x2 + ∂2v
∂y2 + ∂2v
∂z2

,
∂w
∂t + u ∂w
∂x + v ∂w
∂y + w ∂w
∂z = −∂p
∂z + ν
 ∂2w
∂x2 + ∂2w
∂y2 + ∂2w
∂z2

,
∂u
∂x + ∂v
∂y + ∂w
∂z = 0,
(1.4)
is a second-order system of diﬀerential equations that involves four functions, u(t, x, y, z),
v(t, x, y, z), w(t, x, y, z), p(t, x, y, z), each depending on four variables, while ν ≥0 is a
ﬁxed constant. (The function p necessarily depends on t, even though no t derivative of
it appears in the system.) The independent variables are t, representing time, and x, y, z,
representing space coordinates. The dependent variables are u, v, w, p, with v = (u, v, w)
representing the velocity vector ﬁeld of an incompressible ﬂuid ﬂow, e.g., water, and p the
accompanying pressure. The parameter ν measures the viscosity of the ﬂuid. The Navier–
Stokes equations are fundamental in ﬂuid mechanics, [12], and are notoriously diﬃcult to
solve, either analytically or numerically. Indeed, establishing the existence or nonexistence
of solutions for all future times remains a major unsolved problem in mathematics, whose
resolution will earn you a $1,000,000 prize; see http://www.claymath.org for details. The
Navier–Stokes equations ﬁrst appeared in the early 1800s in works of the French applied
mathematician/engineer Claude-Louis Navier and, later, the British applied mathemati-
cian George Stokes, whom you already know from his eponymous multivariable calculus
theorem.† The inviscid case, ν = 0, is known as the Euler equations in honor of their dis-
coverer, the incomparably inﬂuential eighteenth-century Swiss mathematician Leonhard
Euler.
We shall be employing a few basic notational conventions regarding the variables that
appear in our diﬀerential equations. We always use t to denote time, while x, y, z will rep-
resent (Cartesian) space coordinates. Polar coordinates r, θ, cylindrical coordinates r, θ, z,
and spherical coordinates‡ r, θ, ϕ, will also be used when needed. An equilibrium equation
models an unchanging physical system, and so involves only the space variable(s). The
time variable appears when modeling dynamical, meaning time-varying, processes. Both
time and space coordinates are (usually) independent variables. The dependent variables
will mostly be denoted by u, v, w, although occasionally — particularly in representing
†
Interestingly, Stokes’ Theorem was taken from an 1850 letter that Lord Kelvin wrote to
Stokes, who turned it into an undergraduate exam question for the Smith Prize at Cambridge
University in England. However, unbeknownst to either, the result had, in fact, been discovered
earlier by George Green, the father of Green’s Theorem and also the Green’s function, which will
be the subject of Chapter 6.
‡
See Section 12.2 for our notational convention.

4
1 What Are Partial Diﬀerential Equations?
particular physical quantities — other letters may be employed, e.g., the pressure p in
(1.4). On the other hand, the letters f, g, h typically represent speciﬁed functions of the
independent variables, e.g., forcing or boundary or initial conditions.
In this introductory text, we must conﬁne our attention to the most basic analytic
and numerical solution techniques for a select few of the most important partial diﬀerential
equations. More advanced topics, including all systems of partial diﬀerential equations,
must be deferred to graduate and research-level texts, e.g., [35, 38, 44, 61, 99]. In fact,
many important issues remain incompletely resolved and/or poorly understood, making
partial diﬀerential equations one of the most active and exciting ﬁelds of contemporary
mathematical research. One of my goals is that, by reading this book, you will be both
inspired and equipped to venture much further into this fascinating and essential area of
mathematics and/or its remarkable range of applications throughout science, engineering,
economics, biology, and beyond.
Exercises
1.1. Classify each of the following diﬀerential equations as ordinary or partial, and equilibrium
or dynamic; then write down its order.
(a) du
dx + xu = 1, (b) ∂u
∂t + u ∂u
∂x = x,
(c) utt = 9uxx, (d) ∂u
∂t = ∂2u
∂x2 + ∂u
∂x , (e) −∂2u
∂x2 −∂2u
∂y2 = x2 + y2,
(f ) d2u
dt2 + 3u = sin t, (g) uxx + uyy + uzz + (x2 + y2 + z2)u = 0, (h) uxx = x + u2,
(i) ∂u
∂t + ∂3u
∂x3 + u ∂u
∂x = 0, (j) ∂2u
∂x2 + ∂2u
∂y ∂z = u, (k) utt = uxxxx + 2uxxyy + uyyyy.
1.2. In two space dimensions, the Laplacian is deﬁned as the second-order partial diﬀerential
operator Δ = ∂2
x + ∂2
y. Write out the following partial diﬀerential equations in (i) Leibniz
notation; (ii) subscript notation:
(a) the Laplace equation Δu = 0; (b) the Poisson equa-
tion −Δu = f; (c) the two-dimensional heat equation ∂tu = Δu; (d) the von Karman
plate equation Δ2u = 0.
1.3. Answer Exercise 1.2 for the three-dimensional Laplacian Δ = ∂2
x + ∂2
y + ∂2
z.
1.4. Identify the independent variables, the dependent variables, and the order of the following
systems of partial diﬀerential equations:
(a) ∂u
∂x = ∂v
∂y ,
∂u
∂y = −∂v
∂x ;
(b) uxx + vyy = cos(x + y),
uxvy −uyvx = 1;
(c) ∂u
∂t = ∂v
∂x ,
∂2v
∂t2 = ∂2u
∂x2 ;
(d) ut + u ux + v uy = px,
vt + u vx + v vy = py,
ux + vy = 0;
(e) ut = vxxx + v(1 −v),
vt = uxxy + v w,
wt = ux + vy.
Classical Solutions
Let us now focus our attention on a single diﬀerential equation involving a single, scalar-
valued function u that depends on one or more independent variables. The function u

1 What Are Partial Diﬀerential Equations?
5
is usually real-valued, although complex-valued functions can, and do, play a role in the
analysis.
Everything that we say in this section will, when suitably adapted, apply to
systems of diﬀerential equations.
By a solution we mean a suﬃciently smooth function u of the independent variables
that satisﬁes the diﬀerential equation at every point of its domain of deﬁnition. We do not
necessarily require that the solution be deﬁned for all possible values of the independent
variables. Indeed, usually the diﬀerential equation is imposed on some domain D contained
in the space of independent variables, and we seek a solution deﬁned only on D. In general,
the domain D will be an open subset, usually connected and, particularly in equilibrium
equations, often bounded, with a reasonably nice boundary, denoted by ∂D.
We will call a function smooth if it can be diﬀerentiated suﬃciently often, at least
so that all of the derivatives appearing in the equation are well deﬁned on the domain
of interest D. More speciﬁcally, if the diﬀerential equation has order n, then we require
that the solution u be of class Cn, which means that it and all its derivatives of order
≤n are continuous functions in D, and such that the diﬀerential equation that relates the
derivatives of u holds throughout D. However, on occasion, e.g., when dealing with shock
waves, we will consider more general types of solutions. The most important such class
consists of the so-called “weak solutions” to be introduced in Section 10.4. To emphasize
the distinction, the smooth solutions described above are often referred to as classical
solutions. In this book, the term “solution” without extra qualiﬁcation will usually mean
“classical solution”.
Example 1.1. A classical solution to the heat equation
∂u
∂t = ∂2u
∂x2
(1.5)
is a function u(t, x), deﬁned on a domain D ⊂R2, such that all of the functions
u(t, x),
∂u
∂t (t, x),
∂u
∂x (t, x),
∂2u
∂t2 (t, x),
∂2u
∂t ∂x (t, x) = ∂2u
∂x ∂t (t, x),
∂2u
∂x2 (t, x),
are well deﬁned and continuous† at every point (t, x) ∈D, so that u ∈C2(D), and,
moreover, (1.5) holds at every (t, x) ∈D. Observe that, even though only ut and uxx
explicitly appear in the heat equation, we require continuity of all the partial derivatives
of order ≤2 in order that u qualify as a classical solution. For example,
u(t, x) = t + 1
2 x2
(1.6)
is a solution to the heat equation that is deﬁned on the full domain D = R2 because it is‡
C2, and, moreover,
∂u
∂t = 1 = ∂2u
∂x2 .
Another, more complicated but extremely important, solution is
u(t, x) = e−x2/(4t)
2
√
π t
.
(1.7)
†
The equality of the mixed partial derivatives follows from a general theorem in multivariable
calculus, [8, 97, 108]. Classical solutions automatically enjoy equality of all their relevant mixed
partial derivatives.
‡
In fact, the function (1.6) is C∞, meaning inﬁnitely diﬀerentiable, on all of R2.

6
1 What Are Partial Diﬀerential Equations?
One easily veriﬁes that u ∈C2 and, moreover, solves the heat equation on the domain
D = {t > 0} ⊂R2. The reader is invited to verify this by computing ∂u/∂t and ∂2u/∂x2,
and then checking that they are equal. Finally, with i = √−1 denoting the imaginary
unit, we note that
u(t, x) = e−t+ i x = e−t cos x + i e−t sin x,
(1.8)
the second expression following from Euler’s formula (A.11), deﬁnes a complex-valued
solution to the heat equation. This can be veriﬁed directly, since the rules for diﬀerentiating
complex exponentials are identical to those for their real counterparts:
∂u
∂t = −e−t+ i x,
∂u
∂x = i e−t+ i x,
and so
∂2u
∂x2 = −e−t+ i x = ∂u
∂t .
It is worth pointing out that both the real part, e−t cos x, and the imaginary part, e−t sin x,
of the complex solution (1.8) are individual real solutions, which is indicative of a fairly
general property.
Incidentally, most partial diﬀerential equations arising in physical applications are real,
and, although complex solutions often facilitate their analysis, at the end of the day we
require real, physically meaningful solutions. A notable exception is quantum mechanics,
which is an inherently complex-valued physical theory. For example, the one-dimensional
Schr¨odinger equation
i ℏ∂u
∂t = −ℏ2
2m
∂2u
∂x2 + V (x) u,
(1.9)
with ℏdenoting Planck’s constant, which is real, governs the dynamical evolution of the
complex-valued wave function u(t, x) describing the probabilistic distribution of a quantum
particle of mass m, e.g., an electron, moving in the force ﬁeld prescribed by the (real)
potential function V (x). While the solution u is complex-valued, the independent variables
t, x, representing time and space, remain real.
Initial Conditions and Boundary Conditions
How many solutions does a partial diﬀerential equation have?
In general, lots.
Even
ordinary diﬀerential equations have inﬁnitely many solutions. Indeed, the general solution
to a single nth order ordinary diﬀerential equation depends on n arbitrary constants. The
solutions to partial diﬀerential equations are yet more numerous, in that they depend
on arbitrary functions. Very roughly, we can expect the solution to an nth order partial
diﬀerential equation involving m independent variables to depend on n arbitrary functions
of m−1 variables. But this must be taken with a large grain of salt — only in a few special
instances will we actually be able to express the solution in terms of arbitrary functions.
The solutions to dynamical ordinary diﬀerential equations are singled out by the im-
position of initial conditions, resulting in an initial value problem. On the other hand,
equations modeling equilibrium phenomena require boundary conditions to specify their
solutions uniquely, resulting in a boundary value problem. We assume that the reader is
already familiar with the basics of initial value problems for ordinary diﬀerential equations.
But we will take time to develop the perhaps less familiar case of boundary value problems
for ordinary diﬀerential equations in Chapter 6.
A similar speciﬁcation of auxiliary conditions applies to partial diﬀerential equations.
Equations modeling equilibrium phenomena are supplemented by boundary conditions im-
posed on the boundary of the domain of interest. In favorable circumstances, the boundary

1 What Are Partial Diﬀerential Equations?
7
conditions serve to single out a unique solution. For example, the equilibrium temperature
of a body is uniquely speciﬁed by its boundary behavior. If the domain is unbounded,
one must also restrict the nature of the solution at large distances, e.g., by asking that it
remain bounded. The combination of a partial diﬀerential equation along with suitable
boundary conditions is referred to as a boundary value problem.
There are three principal types of boundary value problems that arise in most appli-
cations. Specifying the value of the solution along the boundary of the domain is called a
Dirichlet boundary condition, to honor the nineteenth-century analyst Johann Peter Gus-
tav Lejeune Dirichlet. Specifying the normal derivative of the solution along the boundary
results in a Neumann boundary condition, named after his contemporary Carl Gottfried
Neumann. Prescribing the function along part of the boundary and the normal derivative
along the remainder results in a mixed boundary value problem. For example, in thermal
equilibrium, the Dirichlet boundary value problem speciﬁes the temperature of a body
along its boundary, and our task is to ﬁnd the interior temperature distribution by solv-
ing an appropriate partial diﬀerential equation. Similarly, the Neumann boundary value
problem prescribes the heat ﬂux through the boundary. In particular, an insulated bound-
ary has no heat ﬂux, and hence the normal derivative of the temperature is zero on the
boundary. The mixed boundary value problem prescribes the temperature along part of
the boundary and the heat ﬂux along the remainder. Again, our task is to determine the
interior temperature of the body.
For partial diﬀerential equations modeling dynamical processes, in which time is one of
the independent variables, the solution is to be speciﬁed by one or more initial conditions.
The number of initial conditions required depends on the highest-order time derivative
that appears in the equation. For example, in thermodynamics, which involves only the
ﬁrst-order time derivative of the temperature, the initial condition requires specifying the
temperature of the body at the initial time. Newtonian mechanics describes the accelera-
tion or second-order time derivative of the motion, and so requires two initial conditions:
the initial position and initial velocity of the system. On bounded domains, one must also
impose suitable boundary conditions in order to uniquely characterize the solution and
hence the subsequent dynamical behavior of the physical system. The combination of the
partial diﬀerential equation, the initial conditions, and the boundary conditions leads to an
initial-boundary value problem. We will encounter, and solve, many important examples
of such problems during the course of this text.
Remark: An additional consideration is that, besides any smoothness required by the
partial diﬀerential equation within the domain, the solution and any of its derivatives
speciﬁed in any initial or boundary condition should also be continuous at the initial
or boundary point where the condition is imposed. For example, if the initial condition
speciﬁes the function value u(0, x) for a < x < b, while the boundary conditions specify the
derivatives ∂u
∂x (t, a) and ∂u
∂x (t, b) for t > 0, then, in addition to any smoothness required
inside the domain {a < x < b, t > 0}, we also require that u be continuous at all initial
points (0, x), and that its derivative ∂u
∂x be continuous at all boundary points (t, a) and
(t, b), in order that u(t, x) qualify as a classical solution to the initial-boundary value
problem.

8
1 What Are Partial Diﬀerential Equations?
Exercises
1.5. Show that the following functions u(x, y) deﬁne classical solutions to the two-dimensional
Laplace equation ∂2u
∂x2 + ∂2u
∂y2 = 0. Be careful to specify an appropriate domain.
(a) ex cos y, (b) 1+x2−y2, (c) x3−3xy2, (d) log(x2+y2), (e) tan−1(y/x), (f )
x
x2 + y2 .
1.6. Find all solutions u = f(r) of the two-dimensional Laplace equation uxx + uyy = 0 that
depend only on the radial coordinate r =

x2 + y2.
1.7. Find all (real) solutions to the two-dimensional Laplace equation uxx +uyy = 0 of the form
u = log p(x, y), where p(x, y) is a quadratic polynomial.
1.8.(a) Find all quadratic polynomial solutions of the three-dimensional Laplace equation
∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 = 0.
(b) Find all the homogeneous cubic polynomial solutions.
1.9. Find all polynomial solutions p(t, x) of the heat equation ut = uxx with deg p ≤3.
1.10. Show that each of the following functions u(t, x) is a solution to the wave equation
utt = 4uxx:
(a) 4t2 −x2; (b) cos(x + 2t); (c) sin 2t cos x; (d) e−(x−2t)2.
1.11. Find all polynomial solutions p(t, x) of the wave equation utt = uxx with
(a) deg p ≤2,
(b) deg p = 3.
1.12. Suppose u(t, x) and v(t, x) are C2 functions deﬁned on R2 that satisfy the ﬁrst-order sys-
tem of partial diﬀerential equations ut = vx, vt = ux.
(a) Show that both u and v are classical solutions to the wave equation utt = uxx. Which
result from multivariable calculus do you need to justify the conclusion?
(b) Conversely, given a classical solution u(t, x) to the wave equation, can you construct a
function v(t, x) such that u(t, x), v(t, x) form a solution to the ﬁrst-order system?
1.13. Find all solutions u = f(r) of the three-dimensional Laplace equation
uxx + uyy + uzz = 0 that depend only on the radial coordinate r =

x2 + y2 + z2.
1.14. Let u(x, y) be deﬁned on a domain D ⊂R2. Suppose you know that all its second-order
partial derivatives, uxx, uxy, uyx, uyy, are deﬁned and continuous on all of D. Can you con-
clude that u ∈C2(D)?
1.15. Write down a partial diﬀerential equation that has
(a) no real solutions; (b) exactly one real solution; (c) exactly two real solutions.
1.16. Let u(x, y) = xy x2 −y2
x2 + y2 for (x, y) ̸= (0, 0), while u(0, 0) = 0. Prove that
∂2u
∂x ∂y (0, 0) = 1 ̸= −1 =
∂2u
∂x ∂y (0, 0).
Explain why this example does not contradict the theorem on the equality of mixed partials.
Linear and Nonlinear Equations
As with algebraic equations and ordinary diﬀerential equations, there is a crucial distinction

1 What Are Partial Diﬀerential Equations?
9
between linear and nonlinear partial diﬀerential equations, and one must have a ﬁrm grasp
of the linear theory before venturing into the nonlinear wilderness. While linear algebraic
equations are (modulo numerical diﬃculties) eminently solvable by a variety of techniques,
linear ordinary diﬀerential equations, of order ≥2, already present a challenge, as most
cannot be solved in terms of elementary functions. Indeed, as we will learn in Chapter 11,
solving many of those equations that arise in applications requires introducing new types
of “special functions” that are typically not encountered in a basic calculus course. Linear
partial diﬀerential equations are of a yet higher level of diﬃculty, and only a small handful
of speciﬁc equations can be completely solved. Moreover, explicit solutions tend to be
expressible only in the form of inﬁnite series, requiring subtle analytic tools to understand
their convergence and properties. For the vast majority of partial diﬀerential equations, the
only feasible means of producing general solutions is through numerical approximation. In
this book, we will study the two most basic numerical schemes: ﬁnite diﬀerences and ﬁnite
elements. Keep in mind that, in order to develop and understand numerics for partial
diﬀerential equations, one must already have a good understanding of their analytical
properties.
The distinguishing feature of linearity is that it enables one to straightforwardly com-
bine solutions to form new solutions, through a general Superposition Principle. Linear
superposition is universally applicable to all linear equations and systems, including linear
algebraic systems, linear ordinary diﬀerential equations, linear partial diﬀerential equa-
tions, linear initial and boundary value problems, as well as linear integral equations,
linear control systems, and so on. Let us introduce the basic idea in the context of a single
diﬀerential equation.
A diﬀerential equation is called homogeneous linear if it is a sum of terms, each of
which involves the dependent variable u or one of its derivatives to the ﬁrst power; on
the other hand, there is no restriction on how the terms involve the independent variables.
Thus,
d2u
dx2 +
u
1 + x2 = 0
is a homogeneous linear second-order ordinary diﬀerential equation. Examples of homo-
geneous linear partial diﬀerential equations include the heat equation (1.5), the partial
diﬀerential equation (1.2), and the equation
∂u
∂t = ex ∂2u
∂x2 + cos(x −t) u.
On the other hand, Burgers’ equation
∂u
∂t + u ∂u
∂x = ∂2u
∂x2
(1.10)
is not linear, since the second term involves the product of u and its derivative ux. A
similar terminology is applied to systems of partial diﬀerential equations. For example, the
Navier–Stokes system (1.4) is not linear because of the terms uux, vuy, etc. — although
its ﬁnal constituent equation is linear.
A more precise deﬁnition of a homogeneous linear diﬀerential equation begins with the
concept of a linear diﬀerential operator L. Such operators are assembled by summing the
basic partial derivative operators, with either constant coeﬃcients or, more generally, coef-
ﬁcients depending on the independent variables. The operator acts on suﬃciently smooth

10
1 What Are Partial Diﬀerential Equations?
functions depending on the relevant independent variables. According to Deﬁnition B.32,
linearity imposes two key requirements:
L[u + v] = L[u] + L[v],
L[cu] = cL[u],
(1.11)
for any two (suﬃciently smooth) functions u, v, and any constant c.
Deﬁnition 1.2. A homogeneous linear diﬀerential equation has the form
L[u] = 0,
(1.12)
where L is a linear diﬀerential operator.
As a simple example, consider the second-order diﬀerential operator
L = ∂2
∂x2 ,
whereby
L[u] = ∂2u
∂x2
for any C2 function u(x, y). The linearity requirements (1.11) follow immediately from
basic properties of diﬀerentiation:
L[u + v] = ∂2
∂x2 (u + v) = ∂2u
∂x2 + ∂2v
∂x2 = L[u] + L[v],
L[cu] = ∂2
∂x2 (cu) = c ∂2u
∂x2 = cL[u],
which are valid for any C2 functions u, v and any constant c. The corresponding homoge-
neous linear diﬀerential equation L[u] = 0 is
∂2u
∂x2 = 0.
The heat equation (1.5) is based on the linear partial diﬀerential operator
L = ∂t −∂2
x,
with
L[u] = ∂tu −∂2
xu = ut −uxx = 0.
(1.13)
Linearity follows as above:
L[u + v] = ∂t(u + v) −∂2
x(u + v) = (∂tu −∂2
xu) + (∂tv −∂2
xv) = L[u] + L[v],
L[cu] = ∂t(cu) −∂2
x(cu) = c (∂tu −∂2
xu) = cL[u].
Similarly, the linear diﬀerential operator
L = ∂2
t −∂x κ(x) ∂x = ∂2
t −κ(x) ∂2
x −κ′(x) ∂x,
where κ(x) is a prescribed C1 function of x alone, deﬁnes the homogeneous linear partial
diﬀerential equation
L[u] = ∂2
t u −∂x(κ(x) ∂xu) = utt −∂x(κ(x) ux) = utt −κ(x) uxx −κ′(x) ux = 0,
which is used to model vibrations in a nonuniform one-dimensional medium.
The deﬁning attributes of linear operators (1.11) imply the key properties shared by
all homogeneous linear (diﬀerential) equations.
Proposition 1.3.
The sum of two solutions to a homogeneous linear diﬀerential
equation is again a solution, as is the product of a solution with any constant.

1 What Are Partial Diﬀerential Equations?
11
Proof : Let u1, u2 be solutions, meaning that L[u1 ] = 0 and L[u2 ] = 0. Then, thanks
to linearity,
L[u1 + u2 ] = L[u1 ] + L[u2 ] = 0,
and hence their sum u1 +u2 is a solution. Similarly, if c is any constant and u any solution,
then
L[cu] = c L[u] = c 0 = 0,
and so the scalar multiple cu is also a solution.
Q.E.D.
As a result, starting with a handful of solutions to a homogeneous linear diﬀerential
equation, by repeating these operations of adding solutions and multiplying by constants,
we are able to build up large families of solutions. In the case of the heat equation (1.5),
we are already in possession of two solutions, namely (1.6) and (1.7). Multiplying each by
a constant produces two inﬁnite families of solutions:
u(t, x) = c1(t + 1
2 x2)
and
u(t, x) = c2 e−x2/(4t)
2
√
π t
,
where c1, c2 are arbitrary constants. Moreover, one can add the latter solutions together,
producing a two-parameter family of solutions
u(t, x) = c1(t + 1
2 x2) + c2 e−x2/(4t)
2
√
π t
,
valid for any choice of the constants c1, c2.
The preceding construction is a special case of the general Superposition Principle for
homogeneous linear equations:
Theorem 1.4. If u1, . . . , uk are solutions to a common homogeneous linear equation
L[u] = 0, then the linear combination, or superposition, u = c1u1 + · · ·+ ckuk is a solution
for any choice of constants c1, . . . , ck.
Proof : Repeatedly applying the linearity requirements (1.11), we ﬁnd
L[u] = L[c1u1 + · · · + ckuk ] = L[c1u1 + · · · + ck−1uk−1 ] + L[ckuk ]
= · · · = L[c1u1 ] + · · · + L[ckuk ] = c1L[u1 ] + · · · + ckL[uk ].
(1.14)
In particular, if the functions are solutions, so L[u1 ] = 0, . . . , L[uk ] = 0, then the right-
hand side of (1.14) vanishes, proving that u also solves the equation L[u] = 0.
Q.E.D.
In the linear algebraic language of Appendix B, Theorem 1.4 tells us that the solu-
tions to a homogeneous linear partial diﬀerential equation form a vector space. The same
holds true for linear algebraic equations, [89], and linear ordinary diﬀerential equations,
[18, 20, 23, 52]. In the latter two situations, once one ﬁnds a suﬃcient number of inde-
pendent solutions, the general solution is obtained as a linear combination thereof. In
the language of linear algebra, the solution space is ﬁnite-dimensional. In contrast, most
linear systems of partial diﬀerential equations admit an inﬁnite number of independent
solutions, meaning that the solution space is inﬁnite-dimensional, and, as a consequence,
one cannot hope to build the general solution by taking ﬁnite linear combinations. Instead,
one requires the far more delicate operation of forming inﬁnite series involving the basic
solutions. Such considerations will soon lead us into the heart of Fourier analysis, and
require spending an entire chapter developing the required analytic tools.

12
1 What Are Partial Diﬀerential Equations?
Deﬁnition 1.5. An inhomogeneous linear diﬀerential equation has the form
L[v] = f,
(1.15)
where L is a linear diﬀerential operator, v is the unknown function, and f is a prescribed
nonzero function of the independent variables alone.
For example, the inhomogeneous form of the heat equation (1.13) is
L[v] = ∂tv −∂2
xv = vt −vxx = f(t, x),
(1.16)
where f(t, x) is a speciﬁed function. This equation models the thermodynamics of a one-
dimensional medium subject to an external heat source.
You already learned the basic technique for solving inhomogeneous linear equations
in your study of elementary ordinary diﬀerential equations. Step one is to determine the
general solution to the homogeneous equation. Step two is to ﬁnd a particular solution to
the inhomogeneous version. The general solution to the inhomogeneous equation is then
obtained by adding the two together. Here is the general version of this procedure:
Theorem 1.6. Let v⋆be a particular solution to the inhomogeneous linear equation
L[v⋆] = f. Then the general solution to L[v] = f is given by v = v⋆+ u, where u is the
general solution to the corresponding homogeneous equation L[u] = 0.
Proof : Let us ﬁrst show that v = v⋆+ u is also a solution whenever L[u] = 0. By
linearity,
L[v] = L[v⋆+ u] = L[v⋆] + L[u] = f + 0 = f.
To show that every solution to the inhomogeneous equation can be expressed in this man-
ner, suppose v satisﬁes L[v] = f. Set u = v −v⋆. Then, by linearity,
L[u] = L[v −v⋆] = L[v] −L[v⋆] = 0,
and hence u is a solution to the homogeneous diﬀerential equation. Thus, v = v⋆+ u has
the required form.
Q.E.D.
In physical applications, one can interpret the particular solution v⋆as a response of
the system to the external forcing function. The solution u to the homogeneous equation
represents the system’s internal, unforced behavior. The general solution to the inhomo-
geneous linear equation is thus a combination, v = v⋆+ u, of the external and internal
responses.
Finally, the Superposition Principle for inhomogeneous linear equations allows one to
combine the responses of the system to diﬀerent external forcing functions. The proof of
this result is left to the reader as Exercise 1.26.
Theorem 1.7.
Let v1, . . . , vk be solutions to the inhomogeneous linear systems
L[v1 ] = f1, . . . , L[vk ] = fk, involving the same linear operator L.
Then, given any
constants c1, . . . , ck, the linear combination v = c1v1 +· · ·+ckvk solves the inhomogeneous
system L[v] = f for the combined forcing function f = c1f1 + · · · + ckfk.
The two general Superposition Principles furnish us with powerful tools for solving
linear partial diﬀerential equations, which we shall repeatedly exploit throughout this text.
In contrast, nonlinear partial diﬀerential equations are much tougher, and, typically, knowl-
edge of several solutions is of scant help in constructing others. Indeed, ﬁnding even one
solution to a nonlinear partial diﬀerential equation can be quite a challenge. While this text

1 What Are Partial Diﬀerential Equations?
13
will primarily concentrate on analyzing the solutions and their properties to some of the
most basic and most important linear partial diﬀerential equations, we will have occasion
to brieﬂy venture into the nonlinear realm, introducing some striking recent developments
in this fascinating arena of contemporary research.
Exercises
1.17. Classify the following diﬀerential equations as either
(i) homogeneous linear; (ii) inhomogeneous linear; or (iii) nonlinear:
(a) ut = x2uxx + 2xux, (b) −uxx −uyy = sin u; (c) uxx + 2y uyy = 3;
(d) ut + uux = 3u; (e) eyux = exuy; (f ) ut = 5uxxx + x2 u + x.
1.18. Write down all possible solutions to the Laplace equation you can construct from the var-
ious solutions provided in Exercise 1.5 using linear superposition.
1.19.(a) Show that the following functions are solutions to the wave equation utt = 4uxx:
(i) cos(x −2t),
(ii) ex+2t;
(iii) x2 + 2xt + 4t2.
(b) Write down at least four other solutions to the wave equation.
1.20. The displacement u(t, x) of a forced violin string is modeled by the partial diﬀerential
equation utt = 4uxx+F(t, x). When the string is subjected to the external forcing F(t, x) =
cos x, the solution is u(t, x) = cos(x −2t) + 1
4 cos x, while when F(t, x) = sin x, the solution
is u(t, x) = sin(x −2t) + 1
4 sin x. Find a solution when the forcing function F(t, x) is
(a) cos x −5 sin x,
(b) sin(x −3).
1.21.(a) Show that the partial derivatives ∂x[f ] = ∂f
∂x and ∂y[f ] = ∂f
∂y both deﬁne linear
operators on the space of continuously diﬀerentiable functions f(x, y). (b) For which values
of a, b, c, d is the diﬀerential operator L[f ] = a ∂f
∂x + b ∂f
∂y + c f + d linear?
1.22.(a) Prove that the Laplacian Δ = ∂2
x + ∂2
y deﬁnes a linear diﬀerential operator.
(b) Write out the Laplace equation Δ[u] = 0 and the Poisson equation −Δ[u] = f.
1.23. Prove that, on R3, the gradient, curl, and divergence all deﬁne linear operators.
1.24. Let L and M be linear partial diﬀerential operators. Prove that the following are also
linear partial diﬀerential operators:
(a) L −M, (b) 3L, (c) f L, where f is an arbitrary
function of the independent variables; (d) L ◦M.
1.25. Suppose L and M are linear diﬀerential operators and let N = L + M.
(a) Prove that N is a linear operator.
(b) True or false: If u solves L[u] = f and v solves
M[v ] = g, then w = u + v solves N[w] = f + g.
♦1.26. Prove Theorem 1.7.
1.27. Solve the following inhomogeneous linear ordinary diﬀerential equations:
(a) u′ −4u = x −3,
(b) 5u′′ −4u′ + 4u = ex cos x,
(c) u′′ −3u′ = e3x.
1.28. Use superposition to solve the following inhomogeneous ordinary diﬀerential equations:
(a) u′ + 2u = 1 + cos x, (b) u′′ −9u = x + sin x, (c) 9u′′ −18u′ + 10u = 1 + ex cos x,
(d) u′′ + u′ −2u = sinh x, where sinh x = 1
2(ex −e−x), (e) u′′′ + 9u′ = 1 + e3x.

Chapter 2
Linear and Nonlinear Waves
Our initial foray into the vast mathematical continent that comprises partial diﬀerential
equations will begin with some basic ﬁrst-order equations.
In applications, ﬁrst-order
partial diﬀerential equations are most commonly used to describe dynamical processes,
and so time, t, is one of the independent variables. Our discussion will focus on dynamical
models in a single space dimension, bearing in mind that most of the methods we introduce
can be extended to higher-dimensional situations. First-order partial diﬀerential equations
and systems model a wide variety of wave phenomena, including transport of pollutants in
ﬂuids, ﬂood waves, acoustics, gas dynamics, glacier motion, chromatography, traﬃc ﬂow,
and various biological and ecological systems.
A basic solution technique relies on an inspired change of variables, which comes
from rewriting the equation in a moving coordinate frame. This naturally leads to the
fundamental concept of characteristic curve, along which signals and physical disturbances
propagate.
The resulting method of characteristics is able to solve a ﬁrst-order linear
partial diﬀerential equation by reducing it to one or more ﬁrst-order nonlinear ordinary
diﬀerential equations.
Proceeding to the nonlinear regime, the most important new phenomenon is the pos-
sible breakdown of solutions in ﬁnite time, resulting in the formation of discontinuous
shock waves. A familiar example is the supersonic boom produced by an airplane that
breaks the sound barrier. Signals continue to propagate along characteristic curves, but
now the curves may cross each other, precipitating the onset of a shock discontinuity. The
ensuing shock dynamics is not uniquely speciﬁed by the partial diﬀerential equation, but
relies on additional physical properties, to be speciﬁed by an appropriate conservation law
along with a causality condition. A full-ﬂedged analysis of shock dynamics becomes quite
challenging, and only the basics will be developed here.
Having attained a basic understanding of ﬁrst-order wave dynamics, we then focus
our attention on the ﬁrst of three paradigmatic second-order partial diﬀerential equations,
known as the wave equation, which is used to model waves and vibrations in an elastic
bar, a violin string, or a column of air in a wind instrument. Its multi-dimensional versions
serve to model vibrations of membranes, solid bodies, water waves, electromagnetic waves,
including light, radio waves, microwaves, acoustic waves, and many other physical phenom-
ena. The one-dimensional wave equation is one of a small handful of physically relevant
partial diﬀerential equations that has an explicit solution formula, originally discovered by
the eighteenth-century French mathematician (and encyclopedist) Jean d’Alembert. His
solution is the result of being able to “factorize” the second-order wave equation into a
pair of ﬁrst-order partial diﬀerential equations, of a type solved in the ﬁrst part of this
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
2
15
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

16
2 Linear and Nonlinear Waves
x
u
t = 0
x
u
t = 1
x
u
t = 2
Figure 2.1.
Stationary wave.

chapter. We investigate the consequences of d’Alembert’s solution formula for the initial
value problem on the entire real line; solutions on bounded intervals will be deferred until
Chapter 4. Unfortunately, d’Alembert’s method is of rather limited scope, and does not
extend beyond the one-dimensional case, nor to equations modeling vibrations of nonuni-
form media. The analysis of the wave equation in more than one space dimension can be
found in Chapters 11 and 12.
2.1 Stationary Waves
When entering a new mathematical subject — in our case, partial diﬀerential equations —
one should ﬁrst analyze and fully understand the very simplest examples. Indeed, mathe-
matics is, at its core, a bootstrapping enterprise, in which one builds on one’s knowledge
of and experience with elementary topics — in the present case, ordinary diﬀerential equa-
tions — to make progress, ﬁrst with the simpler types of partial diﬀerential equations, and
then, by developing and applying each newly gained insight and technique, to more and
more complicated situations.
The simplest partial diﬀerential equation, for a function u(t, x) of two variables, is
∂u
∂t = 0.
(2.1)
It is a ﬁrst-order, homogeneous, linear equation.
If (2.1) were an ordinary diﬀerential
equation† for a function u(t) of t alone, the solution would be obvious: u(t) = c must be
constant. A proof of this basic fact proceeds by integrating both sides with respect to t
and then appealing to the Fundamental Theorem of Calculus. To solve (2.1) as a partial
diﬀerential equation for u(t, x), let us similarly integrate both sides of the equation from,
say, 0 to t, producing
0 =
 t
0
∂u
∂t (s, x) ds = u(t, x) −u(0, x).
Therefore, the solution takes the form
u(t, x) = f(x),
where
f(x) = u(0, x),
(2.2)
and hence is a function of the space variable x alone. The only requirement is that f(x)
be continuously diﬀerentiable, so f ∈C1, in order that u(t, x) be a bona ﬁde classical
†
Of course, in this situation, we would write the equation as du/dt = 0.

2.1 Stationary Waves
17
t
x
a
Da
D
Figure 2.2.
Domain for stationary-wave solution.
solution of the ﬁrst-order partial diﬀerential equation (2.1). The solution (2.2) represents
a stationary wave, meaning that it does not change in time. The initial proﬁle stays frozen
in place, and the system remains in equilibrium. Figure 2.1 plots a representative solution
as a function of x at three successive times.
The preceding analysis seems very straightforward and perhaps even a little boring.
But, to be completely rigorous, we need to take a bit more care. In our derivation, we
implicitly assumed that the solution u(t, x) was deﬁned everywhere on R2. And, in fact,
the solution formula (2.2) is not completely valid as stated if the solution u(t, x) is deﬁned
only on a subdomain D ⊂R2.
Indeed, a solution u(t) to the corresponding ordinary diﬀerential equation du/dt = 0 is
constant, provided it is deﬁned on a connected subinterval I ⊂R. A solution that is deﬁned
on a disconnected subset D ⊂R need only be constant on each connected subinterval
I ⊂D. For instance, the nonconstant function
u(t) =
 1,
t > 0,
−1,
t < 0,
satisﬁes
du
dt = 0
everywhere on its domain of deﬁnition, that is, D = {t ̸= 0}, but is constant only on the
connected positive and negative half-lines.
Similar counterexamples can be constructed in the case of the partial diﬀerential equa-
tion (2.1). If the domain of deﬁnition is disconnected, then we do not expect u(t, x) to
depend only on x if we move from one connected component of D to another. Even that
is not the full story. For example, the function
u(t, x) =
⎧
⎨
⎩
0,
x > 0,
x2,
x ≤0,
t > 0,
−x2,
x ≤0,
t < 0,
(2.3)
is continuously diﬀerentiable† on its domain of deﬁnition, namely D = R2\{ (0, x) | x ≤0 },
satisﬁes ∂u/∂t = 0 everywhere in D, but, nevertheless, is not a function of x alone, because,
for example, u(1, x) = x2 ̸= u(−1, x) = −x2.
†
You are asked to rigorously prove diﬀerentiability in Exercise 2.1.10.

18
2 Linear and Nonlinear Waves
A completely correct formulation can be stated as follows: If u(t, x) is a classical
solution to (2.1), deﬁned on a domain D ⊂R2 whose intersection with any horizontal‡ line,
namely Da = D ∩{ (t, a) | t ∈R }, for each ﬁxed a ∈R, is either empty or a connected
interval, then u(t, x) = f(x) is a function of x alone. An example of such a domain is
sketched in Figure 2.2. In Exercise 2.1.9, you are asked to justify these statements.
We are thus slightly chastened in our dismissal of (2.1) as a complete triviality. The
lesson is that, in future, one must always be careful when interpreting such “general”
solution formulas — since they often rely on unstated assumptions on their underlying
domain of deﬁnition.
Exercises
2.1.1. Solve the partial diﬀerential equation ∂u
∂t = x for u(t, x).
2.1.2. Solve the partial diﬀerential equation ∂2u
∂t2 = 0 for u(t, x).
2.1.3. Find the general solution u(t, x) to the following partial diﬀerential equations:
(a) ux = 0, (b) ut = 1, (c) ut = x−t, (d) ut+3u = 0, (e) ux+t u = 0, (f ) utt+4u = 1.
2.1.4. Suppose u(t, x) is deﬁned for all (t, x) ∈R2 and solves ∂u/∂t + 2u = 0. Prove that
lim
t →∞u(t, x) = 0 for all x.
2.1.5. Write down the general solution to the partial diﬀerential equation ∂u/∂t = 0 for a func-
tion of three variables u(t, x, y). What assumptions should be made on the domain of deﬁ-
nition for your solution formula to be valid?
2.1.6. Solve the partial diﬀerential equation
∂2u
∂x ∂y = 0 for u(x, y).
2.1.7. Answer Exercise 2.1.6 when u(x, y, z) depends on the three independent variables x, y, z.
♥2.1.8. Let u(t, x) solve the initial value problem ∂u
∂t + u2 = 0, u(0, x) = f(x), where f(x) is a
bounded C1 function of x ∈R. (a) Show that if f(x) ≥0 for all x, then u(t, x) is deﬁned
for all t > 0, and
lim
t →∞u(t, x) = 0.
(b) On the other hand, if f(x) < 0, then the solution
u(t, x) is not deﬁned for all t > 0, but in fact,
lim
t →τ −u(t, x) = −∞for some 0 < τ < ∞.
Given x, what is the corresponding value of τ? (c) Given f(x) as in part (b), what is the
longest time interval 0 < t < t⋆on which u(t, x) is deﬁned for all x ∈R?
♦2.1.9. Justify the claim in the text that if u(t, x) is a solution of ∂u/∂t = 0 that is deﬁned on
a domain D ⊂R2 with the property that Da = D ∩{ (a, x) | x ∈R } is either empty or a
connected interval, then u(t, x) = v(x) depends only on x ∈D.
♦2.1.10. Prove that the function in (2.3) is continuously diﬀerentiable at all points (t, x) in its
domain of deﬁnition.
‡
Important: We will adopt the (slightly unusual) convention of displaying the (t, x)–plane
with time t along the horizontal axis and space x along the vertical axis — which also conforms
with our convention of writing t before x in expressions like u(t, x). Later developments will amply
vindicate our adoption of this convention.

2.2 Transport and Traveling Waves
19
2.2 Transport and Traveling Waves
In many respects, the stationary-wave equation (2.1) does not quite qualify as a partial
diﬀerential equation. Indeed, the spatial variable x enters only parametrically in the so-
lution to what is, in essence (ignoring technical diﬃculties with domains), a very simple
ordinary diﬀerential equation.
Let us then turn to a more “genuine” example. Consider the linear, homogeneous
ﬁrst-order partial diﬀerential equation
∂u
∂t + c ∂u
∂x = 0,
(2.4)
for a function u(t, x), in which c is a ﬁxed, nonzero constant, known as the wave speed for
reasons that will soon become apparent. We will refer to (2.4) as the transport equation,
because it models the transport of a substance, e.g., a pollutant, in a uniform ﬂuid ﬂow that
is moving with velocity c. In this model, the solution u(t, x) represents the concentration of
the pollutant at time t and spatial position x. Other common names for (2.4) are the ﬁrst-
order or unidirectional wave equation. But for brevity, as well as to avoid any confusion
with the second-order, bidirectional wave equation discussed extensively later on, we will
stick with the designation “transport equation” here. Solving the transport equation is
slightly more challenging, but, as we will see, not diﬃcult.
Since the transport equation involves time, its solutions are distinguished by their
initial values. As a ﬁrst-order equation, we need only specify the value of the solution at
an initial time t0, leading to the initial value problem
u(t0, x) = f(x)
for all
x ∈R.
(2.5)
As we will show, as long as f ∈C1, i.e., is continuously diﬀerentiable, the initial conditions
serve to specify a unique classical solution. Also, by replacing the time variable t by t −t0,
we can, without loss of generality, set t0 = 0.
Uniform Transport
Let us begin by assuming that the wave speed c is constant.
In general, when one is
confronted with a new equation, one solution strategy is to try to convert it into an equation
that you already know how to solve. In this case, we will introduce a simple change of
variables that eﬀectively rewrites the equation in a moving coordinate system, inspired by
the interpretation of c as the overall transport speed.
If x represents the position of an object in a ﬁxed coordinate frame, then
ξ = x −ct
(2.6)
represents the object’s position relative to an observer who is uniformly moving with ve-
locity c. Think of a passenger in a moving train to whom stationary objects appear to
be moving backwards at the train’s speed c. To formulate a physical process in the refer-
ence frame of the passenger, we replace the stationary space-time coordinates (t, x) by the
moving coordinates (t, ξ).
Remark: These are the same changes of reference frame that underlie Einstein’s spe-
cial theory of relativity. However, unlike Einstein, we are working in a purely classical,

20
2 Linear and Nonlinear Waves
x
u
t = 0
x
u
t = 1
x
u
t = 2
Figure 2.3.
Traveling wave with c > 0.

nonrelativistic universe here. Such changes to moving coordinates are, in fact, of a much
older vintage, and named Galilean boosts in honor of Galileo Galilei, who was the ﬁrst to
champion such “relativistic” moving coordinate systems.
Let us see what happens when we re-express the transport equation in terms of the
moving coordinate frame. We rewrite
u(t, x) = v(t, x −ct) = v(t, ξ)
(2.7)
in terms of the characteristic variable ξ = x −ct, along with the time t. To write out
the diﬀerential equation satisﬁed by v(t, ξ), we apply the chain rule from multivariable
calculus, [8, 108], to express the derivatives of u in terms of those of v:
∂u
∂t = ∂v
∂t −c ∂v
∂ξ ,
∂u
∂x = ∂v
∂ξ .
Therefore,
∂u
∂t + c ∂u
∂x = ∂v
∂t −c ∂v
∂ξ + c ∂v
∂ξ = ∂v
∂t .
(2.8)
We deduce that u(t, x) solves the transport equation (2.4) if and only if v(t, ξ) solves the
stationary-wave equation
∂v
∂t = 0.
(2.9)
Thus, the eﬀect of using a moving coordinate system is to convert a wave moving with
velocity c into a stationary wave. Think again of the passenger in the train — a second
train moving at the same speed appears as if it were stationary.
According to our earlier discussion, the solution v = v(ξ) to the stationary-wave
equation (2.9) is a function of the characteristic variable alone. (For simplicity, we assume
that v(t, ξ) has an appropriate domain of deﬁnition, e.g., it is deﬁned everywhere on R2.)
Recalling (2.7), we conclude that the solution
u = v(ξ) = v(x −ct)
to the transport equation must be a function of the characteristic variable only. We have
therefore proved the following result:
Proposition 2.1. If u(t, x) is a solution to the partial diﬀerential equation
ut + cux = 0,
(2.10)
which is deﬁned on all of R2, then
u(t, x) = v(x −ct),
(2.11)
where v(ξ) is a C1 function of the characteristic variable ξ = x −ct.

2.2 Transport and Traveling Waves
21
t
x
(0, y)
(t, x)
Figure 2.4.
Characteristic line.
In other words, any (reasonable) function of the characteristic variable, e.g., ξ2 +1, or
cos ξ, or eξ, will produce a corresponding solution, (x−ct)2 +1, or cos(x−ct), or ex−ct, to
the transport equation with constant wave speed c. And, in accordance with the counting
principle of Chapter 1, the general solution to this ﬁrst-order partial diﬀerential equation
in two independent variables depends on one arbitrary function of a single variable.
To a stationary observer, the solution (2.11) appears as a traveling wave of unchanging
form moving at constant velocity c. When c > 0, the wave translates to the right, as illus-
trated in Figure 2.3. When c < 0, the wave translates to the left, while c = 0 corresponds
to a stationary wave form that remains ﬁxed at its original location, as in Figure 2.1.
At t = 0, the wave has the initial proﬁle
u(0, x) = v(x),
(2.12)
and so (2.11) provides the (unique) solution to the initial value problem (2.4, 12). For
example, the solution to the particular initial value problem
ut + 2ux = 0,
u(0, x) =
1
1 + x2 ,
is
u(t, x) =
1
1 + (x −2t)2 .
Since it depends only on the characteristic variable ξ = x −ct, every solution to the
transport equation is constant on the characteristic lines of slope† c, namely
x = ct + k,
(2.13)
where k is an arbitrary constant. At any given time t, the value of the solution at posi-
tion x depends only on its original value on the characteristic line passing through (t, x).
†
This makes use of our convention that the t–axis is horizontal and the x–axis is vertical.
Reversing the axes will replace the slope by its reciprocal.

22
2 Linear and Nonlinear Waves
x
u
t = 0
x
u
t = 1
x
u
t = 2
Figure 2.5.
Decaying traveling wave.

This is indicative of a general fact concerning such wave models: Signals propagate along
characteristics. Indeed, a disturbance at an initial point (0, y) only aﬀects the value of the
solution at points (t, x) that lie on the characteristic line x = ct + y emanating therefrom,
as illustrated in Figure 2.4.
Transport with Decay
Let a > 0 be a positive constant, and c an arbitrary constant. The homogeneous linear
ﬁrst-order partial diﬀerential equation
∂u
∂t + c ∂u
∂x + au = 0
(2.14)
models the transport of, say, a radioactively decaying solute in a uniform ﬂuid ﬂow with
wave speed c. The coeﬃcient a governs the rate of decay. We can solve this variant of the
transport equation by the self-same change of variables to a uniformly moving coordinate
system.
Rewriting u(t, x) in terms of the characteristic variable, as in (2.7), and then recalling
our chain rule calculation (2.8), we ﬁnd that v(t, ξ) = u(t, ξ + ct) satisﬁes the partial
diﬀerential equation
∂v
∂t + av = 0.
The result is, eﬀectively, a homogeneous linear ﬁrst-order ordinary diﬀerential equation,
in which the characteristic variable ξ enters only parametrically. The standard solution
technique learned in elementary ordinary diﬀerential equations, [20, 23], tells us to multiply
the equation by the exponential integrating factor eat, leading to
eat
 ∂v
∂t + av

= ∂
∂t (eatv) = 0.
We conclude that w = eatv solves the stationary-wave equation (2.1). Thus,
w = eatv = f(ξ),
and hence
v(t, ξ) = f(ξ) e−at,
where f(ξ) is an arbitrary function of the characteristic variable. Reverting to physical
coordinates, we produce the solution formula
u(t, x) = f(x −ct) e−at,
(2.15)
which solves the initial value problem u(0, x) = f(x). It represents a wave that is moving
along with ﬁxed velocity c while simultaneously decaying at an exponential rate as pre-
scribed by the coeﬃcient a > 0. A typical solution, for c > 0, is plotted at three successive

2.2 Transport and Traveling Waves
23
times in Figure 2.5. While the solution (2.15) is no longer constant on the characteris-
tics, signals continue to propagate along them, since a solution’s initial value at a point
(0, y) will only aﬀect its subsequent (decaying) values on the associated characteristic line
x = ct + y.
Exercises
2.2.1. Find the solution to the initial value problem ut + ux = 0, u(1, x) = x/(1 + x2).
2.2.2. Solve the following initial value problems and graph the solutions at times t = 1, 2, and 3:
(a) ut −3ux = 0, u(0, x) = e−x2
;
(b) ut + 2ux = 0, u(−1, x) = x/(1 + x2);
(c) ut + ux + 1
2 u = 0, u(0, x) = tan−1 x;
(d) ut −4ux + u = 0, u(0, x) = 1/(1 + x2).
2.2.3. Graph some of the characteristic lines for the following equations, and write down a for-
mula for the general solution:
(a) ut −3ux = 0,
(b) ut + 5ux = 0,
(c) ut + ux + 3u = 0,
(d) ut −4ux + u = 0.
2.2.4. Solve the initial value problem ut + 2ux = 1, u(0, x) = e−x2
.
Hint: Use characteristic coordinates.
2.2.5. Answer Exercise 2.2.4 for the initial value problem ut + 2ux = sin x, u(0, x) = sin x.
♦2.2.6. Let c be constant. Suppose that u(t, x) solves the initial value problem ut + cux = 0,
u(0, x) = f(x). Prove that v(t, x) = u(t −t0, x) solves the initial value problem vt + cvx = 0,
v(t0, x) = f(x).
2.2.7. Is Exercise 2.2.6 valid when the transport equation is replaced by the damped transport
equation (2.14)?
2.2.8. Let c ̸= 0. (a) Prove that if the initial data satisﬁes u(0, x) = v(x) →0 as x →±∞,
then, for each ﬁxed x, the solution to the transport equation (2.4) satisﬁes u(t, x) →0 as
t →∞.
(b) Is the convergence uniform in x?
2.2.9.(a) Prove that if the initial data is bounded, | f(x) | ≤M for all x ∈R, then the solu-
tion to the damped transport equation (2.14) with a > 0 satisﬁes u(t, x) →0 as t →∞.
(b) Find a solution to (2.14) that is deﬁned for all (t, x) but does not satisfy u(t, x) →0
as t →∞.
2.2.10. Let F(t, x) be a C1 function of (t, x) ∈R2. (a) Write down a formula for the general
solution u(t, x) to the inhomogeneous partial diﬀerential equation ut = F(t, x).
(b) Solve the inhomogeneous transport equation ut + c ux = F(t, x).
♥2.2.11.(a) Write down a formula for the general solution to the nonlinear partial diﬀerential
equation ut + ux + u2 = 0.
(b) Show that if the initial data is positive and bounded,
0 ≤u(0, x) = f(x) ≤M, then the solution exists for all t > 0, and u(t, x) →0 as t →∞.
(c) On the other hand, if the initial data is negative at some x, then the solution blows up
at x in ﬁnite time:
lim
t →τ −u(t, x) →−∞for some τ > 0.
(d) Find a formula for the earli-
est blow-up time τ⋆> 0.
2.2.12. A sensor situated at position x = 1 monitors the concentration of a pollutant u(t, 1) as
a function of t for t ≥0. Assuming that the pollutant is transported with wave speed c = 3,
at what locations x can you determine the initial concentration u(0, x)?
2.2.13. Write down a solution to the transport equation ut + 2ux = 0 that is deﬁned on a
connected domain D ⊂R2 and that is not a function of the characteristic variable alone.

24
2 Linear and Nonlinear Waves
2.2.14. Let c > 0. Consider the uniform transport equation ut + cux = 0 restricted to the
quarter-plane Q = {x > 0, t > 0} and subject to initial conditions u(0, x) = f(x) for x ≥0,
along with boundary conditions u(t, 0) = g(t) for t ≥0. (a) For which initial and bound-
ary conditions does a classical solution to this initial-boundary value problem exist? Write
down a formula for the solution. (b) On which regions are the eﬀects of the initial condi-
tions felt? What about the boundary conditions? Is there any interaction between the two?
2.2.15. Answer Exercise 2.2.14 when c < 0.
Nonuniform Transport
Slightly more complicated, but still linear, is the nonuniform transport equation
∂u
∂t + c(x) ∂u
∂x = 0,
(2.16)
where the wave speed c(x) is now allowed to depend on the spatial position. Characteristics
continue to guide the behavior of solutions, but when the wave speed is not constant, we
can no longer expect them to be straight lines. To adapt the method of characteristics,
let us look at how the solution varies along a prescribed curve in the (t, x)–plane. Assume
that the curve is identiﬁed with the graph of a function x = x(t), and let
h(t) = u

t, x(t)

be the value of the solution on it. We compute the rate of change in the solution along
the curve by diﬀerentiating h with respect to t. Invoking the multivariable chain rule, we
obtain
dh
dt = d
dt u

t, x(t)

= ∂u
∂t

t, x(t)

+ ∂u
∂x

t, x(t)
 dx
dt .
(2.17)
In particular, if x(t) satisﬁes
dx
dt = c

x(t)

,
then
dh
dt = ∂u
∂t

t, x(t)

+ c

x(t)
 ∂u
∂x

t, x(t)

= 0,
since we are assuming that u(t, x) solves the transport equation (2.16) for all values of
(t, x), including those points

t, x(t)

on the curve. Since its derivative is zero, h(t) must
be a constant, which motivates the following deﬁnition.
Deﬁnition 2.2. The graph of a solution x(t) to the autonomous ordinary diﬀerential
equation
dx
dt = c(x)
(2.18)
is called a characteristic curve for the transport equation with wave speed c(x).
In other words, at each point (t, x), the slope of the characteristic curve equals the
wave speed c(x) there. In particular, if c is constant, the characteristic curves are straight
lines of slope c, in accordance with our earlier construction.
Proposition 2.3.
Solutions to the linear transport equation (2.16) are constant
along characteristic curves.

25
t
x
(t, x)
(0, y)
Figure 2.6.
Characteristic curve.
The characteristic curve equation (2.18) is an autonomous ﬁrst-order ordinary diﬀer-
ential equation. As such, it can be immediately solved by separating variables, [20, 23].
Assuming c(x) ̸= 0, we divide both sides of the equation by c(x), and then integrate the
resulting equation:
dx
c(x) = dt,
whereby
β(x) :=

dx
c(x) = t + k,
(2.19)
with k denoting the integration constant. For each ﬁxed value of k, (2.19) serves to im-
plicitly deﬁne a characteristic curve, namely,
x(t) = β−1(t + k),
with β−1 denoting the inverse function. On the other hand, if c(x⋆) = 0, then x⋆is a
ﬁxed point for the ordinary diﬀerential equation (2.18), and the horizontal line x ≡x⋆is a
stationary characteristic curve.
Since the solution u(t, x) is constant along the characteristic curves, it must therefore
be a function of the characteristic variable
ξ = β(x) −t
(2.20)
alone, and hence of the form
u(t, x) = v

β(x) −t

,
(2.21)
where v(ξ) is an arbitrary C1 function. Indeed, it is easy to check directly that, provided
β(x) is deﬁned by (2.19), u(t, x) solves the partial diﬀerential equation (2.16) for any choice
of C1 function v(ξ). (But keep in mind that the algebraic solution formula (2.21) may fail
to be valid at points where the wave speed vanishes: c(x⋆) = 0.)
Warning: The deﬁnition of characteristic variable used here is slightly diﬀerent from
that in the constant wave speed case, which, by (2.20), would be ξ = x/c −t = (x −ct)/c.
Clearly, rescaling the characteristic variable by 1/c is an inessential modiﬁcation of our
original deﬁnition.
2.2 Transport and Traveling Waves

26
2 Linear and Nonlinear Waves
t
x
Figure 2.7.
Characteristic curves for ut + (x2 + 1)−1ux = 0.
To ﬁnd the solution that satisﬁes the prescribed initial conditions
u(0, x) = f(x),
(2.22)
we merely substitute the general solution formula (2.21). This leads to the implicit equation
v(β(x)) = f(x) for the function v(ξ) = f ◦β−1(ξ). The resulting solution formula
u(t, x) = f ◦β−1
β(x) −t

(2.23)
is not particularly enlightening, but it does have a simple graphical interpretation: To ﬁnd
the value of the solution u(t, x), we look at the characteristic curve passing through the
point (t, x). If this curve intersects the x–axis at the point (0, y), as in Figure 2.6, then
u(t, x) = u(0, y) = f(y), since the solution must be constant along the curve. On the other
hand, if the characteristic curve through (t, x) doesn’t intersect the x–axis, the solution
value u(t, x) is not prescribed by the initial data.
Example 2.4. Let us solve the nonuniform transport equation
∂u
∂t +
1
x2 + 1
∂u
∂x = 0
(2.24)
by the method of characteristics. According to (2.18), the characteristic curves are the
graphs of solutions to the ﬁrst-order ordinary diﬀerential equation
dx
dt =
1
x2 + 1 .
Separating variables and integrating, we obtain
β(x) =

(x2 + 1) dx = 1
3 x3 + x = t + k,
(2.25)
where k is the integration constant. Representative curves are plotted in Figure 2.7. (In this
case, inverting the function β, i.e., solving (2.25) for x as a function of t, is not particularly
enlightening.)

2.2 Transport and Traveling Waves
27
t = 0
t = 2
t = 5
t = 12
t = 25
t = 50
Figure 2.8.
Solution to ut +
1
x2 + 1 ux = 0.

According to (2.20), the characteristic variable is ξ =
1
3 x3 + x −t, and hence the
general solution to the equation takes the form
u = v
 1
3 x3 + x −t

,
(2.26)
where v(ξ) is an arbitrary C1 function. A typical solution, corresponding to initial data
u(0, x) =
1
1 + (x + 3)2 ,
(2.27)
is plotted† at the indicated times in Figure 2.8.
Although the solution remains constant
along each individual curve, a stationary observer will witness a dynamically changing
proﬁle as the wave moves through the nonuniform medium. In this example, since c(x) > 0
everywhere, the wave always moves from left to right; its speed as it passes through a point
x determined by the magnitude of c(x) = (x2 + 1)−1, with the consequence that each part
accelerates as it approaches the origin from the left, and then slows back down once it
passes by and c(x) decreases in magnitude. To a stationary observer, the wave spreads out
as it speeds through the origin, and then becomes progressively narrower and slower as it
gradually moves oﬀto +∞.
Example 2.5. Consider the nonuniform transport equation
ut + (x2 −1)ux = 0.
(2.28)
†
The required function v(ξ) in (2.26) is implicitly given by the equation v
 1
3 x3 + x

= u(0, x),
and so the explicit formula for u(t, x) is not very instructive or useful. Indeed, to make the plots,
we instead sampled the initial data (2.27) at a collection of uniformly spaced points y1 < y2 <
· · · < yn. Since the solution is constant along the characteristic curve (2.25) passing through each
sample point (0, yi), we can ﬁnd nonuniformly spaced sample values for u(t, xi) at any later time.
The smooth solution curve u(t, x) is then approximated using spline interpolation, [89; §11.4], on
these sample values.

28
2 Linear and Nonlinear Waves
t
x
Figure 2.9.
Characteristic curves for ut + (x2 −1)ux = 0.
In this case, the characteristic curves are the solutions to
dx
dt = x2 −1,
and so
β(x) =

dx
x2 −1 = 1
2 log

x −1
x + 1
 = t + k.
(2.29)
One must also include the horizontal lines x = x± = ±1 corresponding to the roots of
c(x) = x2 −1. The curves are graphed in Figure 2.9. Note that those curves starting below
x+ = 1 converge to x−= −1 as t →∞, while those starting above x+ = 1 veer oﬀto ∞
in ﬁnite time. Owing to the sign of c(x) = x2 −1, points on the graph of u(0, x) lying over
| x | < 1 will move to the left, while those over | x | > 1 will move to the right.
In Figure 2.10, we graph several snapshots of the solution whose initial value is a
bell-shaped Gaussian proﬁle
u(0, x) = e−x2.
The initial conditions uniquely prescribe the value of the solution along the characteristic
curves that intersect the x–axis. On the other hand, if
x ≤1 + e2t
1 −e2t
for
t > 0,
the characteristic curve through (t, x) does not intersect the x–axis, and hence the value
of the solution at such points, lying in the shaded region in Figure 2.9, is not prescribed
by the initial data. Let us arbitrarily assign the solution to be u(t, x) = 0 at such points.
At other values of (t, x) with t ≥0, the solution (2.23) is
u(t, x) = exp

−
 x + 1 + (x −1)e−2t
x + 1 −(x −1)e−2t
2 
.
(2.30)

29
t = 0
t = .2
t = 1
t = 2
t = 3
t = 5
Figure 2.10.
Solution to ut + (x2 −1)ux = 0.

(The derivation of this solution formula is left as Exercise 2.2.23.) As t increases, the
solution’s peak becomes more and more concentrated near x−= −1, while the section of
the wave above x > x+ = 1 rapidly spreads out to ∞. In the long term, the solution
converges (albeit nonuniformly) to a step function of height 1/e:
u(t, x) −→s(x) =
 1/e ≈.367879,
x ≥−1,
0,
x < −1,
as
t −→∞.
Let us ﬁnish by making a few general observations concerning the characteristic curves
of transport equations whose wave speed c(x) depends only on the position x. Using the
basic existence and uniqueness theory for such autonomous ordinary diﬀerential equations,
[20, 23, 52], and assuming that c(x) is continuously diﬀerentiable:†
• There is a unique characteristic curve passing through each point (t, x) ∈R2.
• Characteristic curves cannot cross each other.
• If t = β(x) is a characteristic curve, then so are all its horizontal translates:
t = β(x) + k for any k.
• Each non-horizontal characteristic curve is the graph of a strictly monotone function.
Thus, each point on a wave always moves in the same direction, and can never
reverse its direction of propagation.
• As t increases, the characteristic curve either tends to a ﬁxed point, x(t) →x⋆as
t →∞, with c(x⋆) = 0, or goes oﬀto ±∞in either ﬁnite or inﬁnite time.
Proofs of these statements are assigned to the reader in Exercise 2.2.25.
†
For those who know about such things, [18, 52], this assumption can be weakened to just
Lipschitz continuity.
2.2 Transport and Traveling Waves

30
2 Linear and Nonlinear Waves
Exercises
2.2.16.(a) Find the general solution to the ﬁrst-order equation ut + 3
2 ux = 0.
(b) Find a solution satisfying the initial condition u(1, x) = sin x. Is your solution unique?
2.2.17.(a) Solve the initial value problem ut −xux = 0, u(0, x) = (x2 + 1)−1.
(b) Graph the solution at times t = 0, 1, 2, 3. (c) What is
lim
t →∞u(t, x)?
2.2.18. Suppose the initial data u(0, x) = f(x) of the nonuniform transport equation (2.28) is
continuous and satisﬁes f(x) →0 as | x | →∞. What is the limiting solution proﬁle u(t, x)
as (a) t →∞? (b) t →−∞?
♥2.2.19.(a) Find and graph the characteristic curves for the equation ut + (sin x)ux = 0.
(b) Write down the solution with initial data u(0, x) = cos 1
2 πx. (c) Graph your solution
at times t = 0, 1, 2, 3, 5, and 10. (d) What is the limiting solution proﬁle as t →∞?
2.2.20. Consider the linear transport equation ut + (1 + x2)ux = 0. (a) Find and sketch the
characteristic curves.
(b) Write down a formula for the general solution.
(c) Find the
solution to the initial value problem u(0, x) = f(x) and discuss its behavior as t increases.
2.2.21. Prove that, for t ≫0, the speed of the wave in Example 2.4 is asymptotically propor-
tional to t−2/3.
2.2.22. Verify directly that formula (2.21) deﬁnes a solution to the diﬀerential equation (2.16).
♦2.2.23. Explain how to derive the solution formula (2.30). Justify that it deﬁnes a solution to
equation (2.28).
2.2.24. Let c(x) be a bounded C1 function, so | c(x) | ≤c⋆< ∞for all x. Let f(x) be any C1
function. Prove that the solution u(t, x) to the initial value problem ut + c(x) ux = 0,
u(0, x) = f(x), is uniquely deﬁned for all (t, x) ∈R2.
♥2.2.25. Suppose that c(x) ∈C1 is continuously diﬀerentiable for all x ∈R. (a) Prove that the
characteristic curves of the transport equation (2.16) cannot cross each other. (b) A point
where c(x⋆) = 0 is known as a ﬁxed point for the characteristic equation dx/dt = c(x).
Explain why the characteristic curve passing through a ﬁxed point (t, x⋆) is a horizontal
straight line. (c) Prove that if x = g(t) is a characteristic curve, then so are all the horizon-
tally translated curves x = g(t + δ) for any δ. (d) True or false: Every characteristic curve
has the form x = g(t + δ), for some ﬁxed function g(t). (e) Prove that each non-horizontal
characteristic curve is the graph x = g(t) of a strictly monotone function. (f ) Explain why
a wave cannot reverse its direction.
(g) Show that a non-horizontal characteristic curve
starts, in the distant past, t →−∞, at either a ﬁxed point or at −∞and ends, as
t →+∞, at either the next-larger ﬁxed point or at +∞.
♥2.2.26. Consider the transport equation ∂u
∂t + c(t, x) ∂u
∂x = 0 with time-varying wave speed.
Deﬁne the corresponding characteristic ordinary diﬀerential equation to be dx
dt = c(t, x),
the graphs of whose solutions x(t) are the characteristic curves. (a) Prove that any so-
lution u(t, x) to the partial diﬀerential equation is constant on each characteristic curve.
(b) Suppose that the general solution to the characteristic equation is written in the form
ξ(t, x) = k, where k is an arbitrary constant. Prove that ξ(t, x) deﬁnes a characteristic vari-
able, meaning that u(t, x) = f(ξ(t, x)) is a solution to the time-varying transport equation
for any continuously diﬀerentiable scalar function f ∈C1.
2.2.27.(a) Apply the method in Exercise 2.2.26 to ﬁnd the characteristic curves for the equa-
tion ut + t2 ux = 0.
(b) Find the solution to the initial value problem u(0, x) = e−x2, and
discuss its dynamic behavior.

2.3 Nonlinear Transport and Shocks
31
2.2.28. Solve Exercise 2.2.27 for the equation ut + (x −t)ux = 0.
♥2.2.29. Consider the ﬁrst-order partial diﬀerential equation ut + (1 −2t)ux = 0. Use Exercise
2.2.26 to: (a) Find and sketch the characteristic curves.
(b) Write down the general solu-
tion.
(c) Solve the initial value problem with u(0, x) =
1
1 + x2 . (d) Describe the behavior
of your solution u(t, x) from part (c) as t →∞. What about t →−∞?
2.2.30. Discuss which of the conclusions of Exercise 2.2.25 are valid for the characteristic curves
of the transport equation with time-varying wave speed, as analyzed in Exercise 2.2.26.
♦2.2.31. Consider the two-dimensional transport equation ∂u
∂t + c(x, y) ∂u
∂x + d(x, y) ∂u
∂y = 0,
whose solution u(t, x, y) depends on time t and space variables x, y. (a) Deﬁne a character-
istic curve, and prove that the solution is constant along it. (b) Apply the method of char-
acteristics to solve the initial value problem ut + y ux −xuy, u(0, x, y) = e−(x−1)2−(y−1)2
.
(c) Describe the behavior of your solution.
2.3 Nonlinear Transport and Shocks
The ﬁrst-order nonlinear partial diﬀerential equation
ut + uux = 0
(2.31)
has the form of a transport equation (2.4), but the wave speed c = u now depends, not
on the position x, but rather on the size of the disturbance u. Larger waves will move
faster, and overtake smaller, slower-moving waves. Waves of elevation, where u > 0, move
to the right, while waves of depression, where u < 0, move to the left. This equation
is considerably more challenging than the linear transport models analyzed above, and
was ﬁrst systematically studied in the early nineteenth century by the inﬂuential French
mathematician Sim´eon–Denis Poisson and the great German mathematician Bernhard Rie-
mann.† It and its multi-dimensional and multi-component generalizations play a crucial
role in the modeling of gas dynamics, acoustics, shock waves in pipes, ﬂood waves in rivers,
chromatography, chemical reactions, traﬃc ﬂow, and so on. Although we will be able to
write down a solution formula, the complete analysis is far from trivial, and will require us
to confront the possibility of discontinuous shock waves. Motivated readers are referred to
Whitham’s book, [122], for further details.
Fortunately, the method of characteristics that was developed for linear transport
equations also works in the present context and leads to a complete mathematical solution.
Mimicking our previous construction, (2.18), but now with wave speed c = u, let us deﬁne
a characteristic curve of the nonlinear wave equation (2.31) to be the graph of a solution
x(t) to the ordinary diﬀerential equation
dx
dt = u(t, x).
(2.32)
†
In addition to his fundamental contributions to partial diﬀerential equations, complex anal-
ysis, and number theory, Riemann also was the inventor of Riemannian geometry, which turned
out to be absolutely essential for Einstein’s theory of general relativity some 70 years later!

32
2 Linear and Nonlinear Waves
As such, the characteristics depend upon the solution u, which, in turn, is to be speciﬁed
by its characteristics. We appear to be trapped in a circular argument.
The resolution of the conundrum is to argue that, as in the linear case, the solution
u(t, x) remains constant along its characteristics, and this fact will allow us to simultane-
ously specify both. To prove this claim, suppose that x = x(t) parametrizes a characteristic
curve associated with the given solution u(t, x). Our task is to show that h(t) = u

t, x(t)

,
which is obtained by evaluating the solution along the curve, is constant, which, as usual,
is proved by checking that its derivative is identically zero.
Repeating our chain rule
computation (2.17), and using (2.32), we deduce that
dh
dt = d
dt u

t, x(t)

= ∂u
∂t

t, x(t)

+ dx
dt
∂u
∂x

t, x(t)

= ∂u
∂t

t, x(t)

+u

t, x(t)
∂u
∂x

t, x(t)

= 0,
since u is assumed to solve the nonlinear transport equation (2.31) at all values of (t, x),
including those on the characteristic curve. We conclude that h(t) is constant, and hence
u is indeed constant on the characteristic curve.
Now comes the clincher. We know that the right-hand side of the characteristic ordi-
nary diﬀerential equation (2.32) is a constant whenever x = x(t) deﬁnes a characteristic
curve. This means that the derivative dx/dt is a constant — namely the ﬁxed value of u
on the curve. Therefore, the characteristic curve must be a straight line,
x = ut + k,
(2.33)
whose slope equals the value assumed by the solution u on it.
And, as before, since the solution is constant along each characteristic line, it must be
a function of the characteristic variable
ξ = x −tu
(2.34)
alone, and so
u = f(x −tu),
(2.35)
where f(ξ) is an arbitrary C1 function. Formula (2.35) should be viewed as an algebraic
equation that implicitly deﬁnes the solution u(t, x) as a function of t and x. Veriﬁcation
that ther resulting function is indeed a solution to (2.31) is the subject of Exercise 2.3.14.
Example 2.6. Suppose that
f(ξ) = αξ + β,
with α, β constant. Then (2.35) becomes
u = α(x −tu) + β,
and hence
u(t, x) = αx + β
1 + α t
(2.36)
is the corresponding solution to the nonlinear transport equation. At each ﬁxed t, the graph
of the solution is a straight line. If α > 0, the solution ﬂattens out: u(t, x) →0 as t →∞.
On the other hand, if α < 0, the straight line rapidly steepens to vertical as t approaches
the critical time t⋆= −1/α, at which point the solution ceases to exist. Figure 2.11 graphs
two representative solutions. The top row shows the solution with α = 1, β = .5, plotted
at times t = 0, 1, 5, and 20; the bottom row takes α = −.2, β = .1, and plots the solution
at times t = 0, 3, 4, and 4.9. In the second case, the solution blows up by becoming vertical
as t →5.

2.3 Nonlinear Transport and Shocks
33
t = 0
t = 1
t = 5
t = 20
t = 0
t = 3
t = 4
t = 5
Figure 2.11.
Two solutions to ut + uux = 0.

Remark: Although (2.36) remains a valid solution formula after the blow-up time,
t > 5, this is not to be viewed as a part of the original solution. With the appearance of
such a singularity, the physical solution has broken down, and we stop tracking it.
To solve the general initial value problem
u(0, x) = f(x),
(2.37)
we note that, at t = 0, the implicit solution formula (2.35) reduces to (2.37), and hence the
function f coincides with the initial data. However, because our solution formula (2.35) is
an implicit equation, it is not immediately evident
(a) whether it can be solved to give a well-deﬁned function u(t, x), and,
(b) even granted this, how to describe the resulting solution’s qualitative features and
dynamical behavior.
A more instructive approach is founded on the following geometrical construction.
Through each point (0, y) on the x–axis, draw the characteristic line
x = tf(y) + y
(2.38)
whose slope, namely f(y) = u(0, y), equals the value of the initial data (2.37) at that point.
According to the preceding discussion, the solution will have the same value on the entire
characteristic line (2.38), and so
u(t, tf(y) + y) = f(y)
for all t.
(2.39)
For example, if f(y) = y, then u(t, x) = y whenever x = ty + y; eliminating y, we ﬁnd
u(t, x) = x/(t + 1), which agrees with one of our straight line solutions (2.36).
Now, the problem with this construction is immediately apparent from Figure 2.12,
which plots the characteristic lines associated with the initial data
u(0, x) = 1
2 π −tan−1 x.

34
2 Linear and Nonlinear Waves
t
x
Figure 2.12.
Characteristics lines for u(0, x) = 1
2 π −tan−1 x.
Two characteristic lines that are not parallel must cross each other somewhere. The value
of the solution is supposed to equal the slope of the characteristic line passing through the
point. Hence, at a crossing point, the solution is required to assume two diﬀerent values,
one corresponding to each line. Something is clearly amiss, and we need to resolve this
apparent paradox.
There are three principal scenarios. The ﬁrst, trivial, situation occurs when all the
characteristic lines are parallel, and so the diﬃculty does not arise. In this case, they all
have the same slope, say c, which means that the solution has the same value on each one.
Therefore, u(t, x) ≡c is a constant solution.
The next-simplest case occurs when the initial data is everywhere nondecreasing, so
f(x) ≤f(y) whenever x ≤y, which is assured if its derivative is never negative: f ′(x) ≥0.
In this case, as sketched in Figure 2.13, the characteristic lines emanating from the x axis
fan out into the right half-plane, and so never cross each other at any future time t > 0.
Each point (t, x) with t ≥0 lies on a unique characteristic line, and the value of the
solution at (t, x) is equal to the slope of the line. We conclude that the solution u(t, x)
is well deﬁned at all future times t ≥0. Physically, such solutions represent rarefaction
waves, which spread out as time progresses. A typical example, corresponding to initial
data
u(0, x) = 1
2 π + tan−1(3x),
has its characteristic lines plotted in Figure 2.13, while Figure 2.14 graphs some represen-
tative solution proﬁles.
The more interesting case occurs when the initial data is a decreasing function, and so
f ′(x) < 0. Now, as in Figure 2.12, some of the characteristic lines starting at t = 0 will cross
at some point in the future. If a point (t, x) lies on two or more distinct characteristic lines,
the value of the solution u(t, x), which should equal the characteristic slope, is no longer
uniquely determined. Although, in a purely mathematical context, one might be tempted
to allow such multiply valued solutions, from a physical standpoint this is unacceptable.
The solution u(t, x) is supposed to represent a measurable quantity, e.g., concentration,

2.3 Nonlinear Transport and Shocks
35
t
x
Figure 2.13.
Characteristic lines for a rarefaction wave.
t = 0
t = 1
t = 2
t = 3
Figure 2.14.
Rarefaction wave.

velocity, pressure, and must therefore assume a unique value at each point. In eﬀect, the
mathematical model has broken down and no longer conforms to physical reality.
However, before confronting this diﬃculty, let us ﬁrst, from a purely theoretical stand-
point, try to understand what happens if we mathematically continue the solution as a
multiply valued function. For speciﬁcity, consider the initial data
u(0, x) = 1
2 π −tan−1 x,
(2.40)
appearing in the ﬁrst graph in Figure 2.15.
The corresponding characteristic lines are
displayed in Figure 2.12. Initially, they do not cross, and the solution remains a well-
deﬁned, single-valued function. However, after a while one reaches a critical time, t⋆> 0,
when the ﬁrst two characteristic lines cross each other.
Subsequently, a wedge-shaped
region appears in the (t, x)–plane, consisting of points that lie on the intersection of three

36
2 Linear and Nonlinear Waves
t = 0
t = .5
t = 1
t = 1.5
t = 2
t = 2.5
Figure 2.15.
Multiply valued compression wave.

distinct characteristic lines with diﬀerent slopes; at such points, the mathematical solution
achieves three distinct values. Points outside the wedge lie on a single characteristic line,
and the solution remains single-valued there. The boundary of the wedge consists of points
where precisely two characteristic lines cross.
To fully appreciate what is going on, look now at the sequence of pictures of the
multiply valued solution in Figure 2.15, plotted at six successive times. Since the initial
data is positive, f(x) > 0, all the characteristic slopes are positive. As a consequence,
every point on the solution curve moves to the right, at a speed equal to its height. Since
the initial data is a decreasing function, points on the graph lying to the left will move
faster than those to the right and eventually overtake them. At ﬁrst, the solution merely
steepens into a compression wave. At the critical time t⋆when the ﬁrst two characteristic
lines cross, say at position x⋆, so that (t⋆, x⋆) is the tip of the aforementioned wedge, the
solution graph has become vertical:
∂u
∂x (t, x⋆) −→∞
as
t −→t⋆,
and u(t, x) is no longer a classical solution. Once this occurs, the solution graph ceases to
be a single-valued function, and its overlapping lobes lie over the points (t, x) belonging to
the wedge.
The critical time t⋆can, in fact, be determined from the implicit solution formula (2.35).
Indeed, if we diﬀerentiate with respect to x, we obtain
∂u
∂x = ∂
∂x f(ξ) = f ′(ξ) ∂ξ
∂x = f ′(ξ)

1 −t ∂u
∂x

,
where
ξ = x −tu.
Solving for
∂u
∂x =
f ′(ξ)
1 + tf ′(ξ) ,

2.3 Nonlinear Transport and Shocks
37
we see that the slope blows up:
∂u
∂x −→∞
as
t −→−
1
f ′(ξ) .
In other words, if the initial data has negative slope at position x, so f ′(x) < 0, then the
solution along the characteristic line emanating from the point (0, x) will fail to be smooth
at the time −1/f ′(x). The earliest critical time is, thus,
t⋆:= min

−
1
f ′(x)
 f ′(x) < 0

.
(2.41)
If x0 is the value of x that produces the minimum t⋆, then the slope of the solution proﬁle
will ﬁrst become inﬁnite at the location where the characteristic starting at x0 is at time
t⋆, namely
x⋆= x0 + f(x0) t⋆.
(2.42)
For instance, for the particular initial conﬁguration (2.40) represented in Figure 2.15,
f(x) = π
2 −tan−1 x,
f ′(x) = −
1
1 + x2 ,
and so the critical time is
t⋆= min {1 + x2 } = 1,
with
x⋆= f(0) t⋆= 1
2 π,
since the minimum value occurs at x0 = 0.
Now, while mathematically plausible, such a multiply valued solution is physically
untenable. So what really happens after the critical time t⋆? One needs to decide which
(if any) of the possible solution values is physically appropriate. The mathematical model,
in and of itself, is incapable of resolving this quandary.
We must therefore revisit the
underlying physics, and ask what sort of phenomenon we are trying to model.
Shock Dynamics
To be speciﬁc, let us regard the transport equation (2.31) as a model of compressible ﬂuid
ﬂow in a single space variable, e.g., the motion of gas in a long pipe. If we push a piston
into the pipe, then the gas will move ahead of it and thereby be compressed. However, if
the piston moves too rapidly, then the gas piles up on top of itself, and a shock wave forms
and propagates down the pipe. Mathematically, the shock is represented by a discontinuity
where the solution abruptly changes value. The formulas (2.41) and (2.42) determine the
time and position for the onset of the shock-wave discontinuity. Our goal now is to predict
its subsequent behavior, and this will be based on use of a suitable physical conservation
law. Indeed, one expects mass to be conserved – even through a shock discontinuity —
since gas atoms can neither be created nor destroyed. And, as we will see, conservation of
mass (almost) suﬃces to prescribe the subsequent motion of the shock wave.
Before investigating the implications of conservation of mass, let us ﬁrst convince
ourselves of its validity for the nonlinear transport model. (Just because a mathematical
equation models a physical system does not automatically imply that it inherits any of its

38
2 Linear and Nonlinear Waves
physical conservation laws.) If u(t, x) represents density, then, at time t, the total mass
lying in an interval a ≤x ≤b is calculated by integration:
Ma,b(t) =
 b
a
u(t, x) dx.
(2.43)
Assuming that u(t, x) is a classical solution to the nonlinear transport equation (2.31), we
can determine the rate of change of mass on this interval by diﬀerentiation:
dMa,b
dt
= d
dt
 b
a
u(t, x) dx =
 b
a
∂u
∂t (t, x) dx = −
 b
a
u(t, x) ∂u
∂x(t, x) dx
= −
 b
a
∂
∂x
 1
2 u(t, x)2 
dx = −1
2 u(t, x)2 
b
x=a =
1
2 u(t, a)2 −1
2 u(t, b)2.
(2.44)
The ﬁnal expression represents the net mass ﬂux through the endpoints of the interval.
Thus, the only way in which the mass on the interval [a, b] changes is through its endpoints;
inside, mass can be neither created nor destroyed, which is the precise meaning of the mass
conservation law in continuum mechanics. In particular, if there is zero net mass ﬂux, then
the total mass is constant, and hence conserved. For example, if the initial data (2.37) has
ﬁnite total mass,

 ∞
−∞
f(x) dx
 < ∞,
(2.45)
which requires that f(x) →0 reasonably rapidly as | x | →∞, then the total mass of the
solution — at least up to the formation of a shock discontinuity — remains constant and
equal to its initial value:
 ∞
−∞
u(t, x) dx =
 ∞
−∞
u(0, x) dx =
 ∞
−∞
f(x) dx.
(2.46)
Similarly, if u(t, x) represents the traﬃc density on a highway at time t and position x,
then the integrated conservation law (2.44) tells us that the rate of change in the number
of vehicles on the stretch of road between a and b equals the number of vehicles entering
at point a minus the number leaving at point b — which assumes that there are no other
exits or entrances on this part of the highway. Thus, in the traﬃc model, (2.44) represents
the conservation of vehicles.
The preceding calculation relied on the fact that the integrand can be written as an x
derivative. This is a common feature of physical conservation laws in continuum mechanics,
and motivates the following general deﬁnition.
Deﬁnition 2.7.
A conservation law, in one space dimension, is an equation of the
form
∂T
∂t + ∂X
∂x = 0.
(2.47)
The function T is known as the conserved density, while X is the associated ﬂux.
In the simplest situations, the conserved density T(t, x, u) and ﬂux X(t, x, u) depend
on the time t, the position x, and the solution u(t, x) to the physical system. (Higher-order
conservation laws, which also depend on derivatives of u, arise in the analysis of integrable
partial diﬀerential equations; see Section 8.5 and [36, 87].) For example, the nonlinear
transport equation (2.31) is itself a conservation law, since it can be written in the form
∂u
∂t + ∂
∂x
 1
2 u2 
= 0,
(2.48)

2.3 Nonlinear Transport and Shocks
39
x
u
Figure 2.16.
Equal Area Rule.
and so the conserved density is T = u and the ﬂux is X =
1
2 u2.
And indeed, it was
this identity that made our computation (2.44) work. The general result, proved by an
analogous computation, justiﬁes calling (2.47) a conservation law.
Proposition 2.8.
Given a conservation law (2.47), then, on any closed interval
a ≤x ≤b,
d
dt
 b
a
T dx = −X

b
x=a .
(2.49)
Proof : The proof is an immediate consequence of the Fundamental Theorem of Cal-
culus — assuming suﬃcient smoothness that allows one to bring the derivative inside the
integral sign:
d
dt
 b
a
T dx =
 b
a
∂T
∂t dx = −
 b
a
∂X
∂x dx = −X

b
x=a .
Q.E.D.
We will refer to (2.49) as the integrated form of the conservation law (2.47). It states
that the rate of change of the total density, integrated over an interval, is equal to the
amount of ﬂux through its two endpoints. In particular, if there is no net ﬂux into or out
of the interval, then the integrated density is conserved, meaning that it remains constant
over time. All physical conservation laws — mass, momentum, energy, and so on — for
systems governed by partial diﬀerential equations are of this form or its multi-dimensional
extensions, [87].
With this in hand, let us return to the physical context of the nonlinear transport
equation. By deﬁnition, a shock is a discontinuity in the solution u(t, x). We will make
the physically plausible assumption that mass (or vehicle) conservation continues to hold
even within the shock.
Recall that the total mass, which at time t is the area† under
the curve u(t, x), must be conserved. This continues to hold even when the mathematical
solution becomes multiply valued, in which case one employs a line integral

C
u dx, where
C represents the graph of the solution, to compute the mass/area. Thus, to construct a
discontinuous shock solution with the same mass, one replaces part of the multiply valued
†
We are implicitly assuming that the mass is ﬁnite, as in (2.45), although the overall con-
struction does not rely on this restriction.

40
2 Linear and Nonlinear Waves
x
u
a
b
bt
at
Figure 2.17.
Multiply–valued step wave.

graph by a vertical shock line in such a way that the resulting function is single-valued and
has the same area under its graph. Referring to Figure 2.16, observe that the region under
the shock graph is obtained from that under the multi-valued solution graph by deleting
the upper shaded lobe and appending the lower shaded lobe. Thus the resulting area will
be the same, provided the shock line is drawn so that the areas of the two shaded lobes are
equal. This construction is known as the Equal Area Rule; it ensures that the total mass
of the shock solution matches that of the multiply valued solution, which in turn is equal
to the initial mass, as required by the physical conservation law.
Example 2.9. An illuminating special case occurs when the initial data has the form
of a step function with a single discontinuity at the origin:
u(0, x) =
 a,
x < 0,
b,
x > 0.
(2.50)
If a > b, then the initial data is already in the form of a shock wave. For t > 0, the
mathematical solution constructed by continuing along the characteristic lines is multiply
valued in the region bt < x < at, where it assumes both values a and b; see Figure 2.17.
Moreover, the initial vertical line of discontinuity has become a tilted line, because each
point (0, u) on it has moved along the associated characteristic a distance ut. The Equal
Area Rule tells us to draw the shock line halfway along, at x = 1
2 (a+b)t, in order that the
two triangles have the same area. We deduce that the shock moves with speed c = 1
2 (a+b),
equal to the average of the two speeds at the jump. The resulting shock-wave solution is
u(t, x) =
 a,
x < ct,
b,
x > ct,
where
c = a + b
2
.
(2.51)
A plot of its characteristic lines appears in Figure 2.18. Observe that colliding pairs of
characteristic lines terminate at the shock line, whose slope is the average of their individual
slopes.
The fact that the shock speed equals the average of the solution values on either side
is, in fact, of general validity, and is known as the Rankine–Hugoniot condition, named af-
ter the nineteenth-century Scottish physicist William Rankine and French engineer Pierre
Hugoniot, although historically these conditions ﬁrst appeared in a 1849 paper by George
Stokes, [109]. However, intimidated by criticism by his contemporary applied mathemati-
cians Lords Kelvin and Rayleigh, Stokes thought he was mistaken, and even ended up

2.3 Nonlinear Transport and Shocks
41
t
x
Figure 2.18.
Characteristic lines for the step wave shock.
deleting the relevant part when his collected works were published in 1883, [110]. The
missing section was restored in the 1966 reissue, [111].
Proposition 2.10. Let u(t, x) be a solution to the nonlinear transport equation that
has a discontinuity at position x = σ(t), with ﬁnite, unequal left- and right-hand limits
u−(t) = u

t, σ(t)−
=
lim
x →σ(t)−u(t, x),
u+(t) = u

t, σ(t)+
=
lim
x →σ(t)+ u(t, x), (2.52)
on either side of the shock discontinuity. Then, to maintain conservation of mass, the speed
of the shock must equal the average of the solution values on either side:
dσ
dt = u−(t) + u+(t)
2
.
(2.53)
Proof : Referring to Figure 2.19, consider a small time interval, from t to t + Δt,
with Δt > 0.
During this time, the shock moves from position a = σ(t) to position
b = σ(t + Δt). The total mass contained in the interval [a, b] at time t, before the shock
has passed through, is
M(t) =
 b
a
u(t, x) dx ≈u+(t) (b −a) = u+(t)

σ(t + Δt) −σ(t)

,
where we assume that Δt ≪1 is very small, and so the integrand is well approximated by
its limiting value (2.52). Similarly, after the shock has passed, the total mass remaining in
the interval is
M(t + Δt) =
 b
a
u(t + Δt, x) dx ≈u−(t + Δt) (b −a) = u−(t + Δt)

σ(t + Δt) −σ(t)

.

42
2 Linear and Nonlinear Waves
t
t
t + Δt
a = σ(t)
b = σ(t + Δt)
u+
u−
x
Figure 2.19.
Conservation of mass near a shock.
Thus, the rate of change in mass across the shock at time t is given by
dM
dt = lim
Δt →0
M(t + Δt) −M(t)
Δt
= lim
Δt →0

u−(t + Δt) −u+(t)
 σ(t + Δt) −σ(t)
Δt
=

u−(t) −u+(t)
 dσ
dt .
On the other hand, at any t < τ < t + Δt, the mass ﬂux into the interval [a, b] through
the endpoints is given by the right-hand side of (2.44):
1
2

u(τ, a)2 −u(τ, b)2 
−→
1
2

u−(t)2 −u+(t)2 
,
since τ →t as Δt →0.
Conservation of mass requires that the rate of change in mass be equal to the mass ﬂux:
dM
dt =

u−(t) −u+(t)
 dσ
dt = 1
2

u−(t)2 −u+(t)2 
.
Solving for dσ/dt establishes (2.53).
Q.E.D.
Example 2.11. By way of contrast, let us investigate the case when the initial data
is a step function (2.50), but with a < b, so the jump goes upwards. In this case, the
characteristic lines diverge from the initial discontinuity, and the mathematical solution is
not speciﬁed at all in the wedge-shaped region at < x < bt. Our task is to decide how to
“ﬁll in” the solution values between the two regions where the solution is well deﬁned and
constant.
One possible connection is by a straight line. Indeed, a simple modiﬁcation of the
rational solution (2.36) produces the similarity solution†
u(t, x) = x
t ,
†
See Section 8.2 for general techniques for constructing similarity (scale-invariant) solutions
to partial diﬀerential equations.

2.3 Nonlinear Transport and Shocks
43
Figure 2.20.
Rarefaction wave.

which not only solves the diﬀerential equation, but also has the required values u(t, at) = a
and u(t, bt) = b at the two edges of the wedge. This can be used to construct the piecewise
aﬃne rarefaction wave
u(t, x) =
⎧
⎨
⎩
a,
x ≤at,
x/t,
at ≤x ≤bt,
b,
x ≥bt,
(2.54)
which is graphed at four representative times in Figure 2.20.
A second possibility would be to continue the discontinuity as a shock wave, whose
speed is governed by the Rankine-Hugoniot condition, leading to a discontinuous solution
having the same formula as (2.51).
Which of the two competing solutions should we
use? The ﬁrst, (2.54), makes better physical sense; indeed, if we were to smooth out the
discontinuity, then the resulting solutions would converge to the rarefaction wave and not
the reverse shock wave; see Exercise 2.3.13. Moreover, the discontinuous solution (2.51)
has characteristic lines emanating from the discontinuity, which means that the shock is
creating new values for the solution as it moves along, and this can, in fact, be done in a
variety of ways. In other words, the discontinuous solution violates causality, meaning that
the solution proﬁle at any given time uniquely prescribes its subsequent motion. Causality
requires that, while characteristics may terminate at a shock discontinuity, they cannot
begin there, because their slopes will not be uniquely prescribed by the shock proﬁle, and
hence the characteristics to the left of the shock must have larger slope (or speed), while
those to the right must have smaller slope. Since the shock speed is the average of the two
characteristic slopes, this requires the Entropy Condition
u−(t) > dσ
dt = u−(t) + u+(t)
2
> u+(t).
(2.55)
With further analysis, it can be shown, [57], that the rarefaction wave (2.54) is the unique
solution† to the initial value problem satisfying the entropy condition (2.55).
†
Albeit not a classical solution, but rather a weak solution, as per Section 10.4.

44
2 Linear and Nonlinear Waves
x
u
1
σ(t)
(1 + t, 1)
Figure 2.21.
Equal Area Rule for the triangular wave.

These prototypical solutions epitomize the basic phenomena modeled by the nonlinear
transport equation: rarefaction waves, which emanate from regions where the initial data
satisﬁes f ′(x) > 0, causing the solution to spread out as time progresses, and compression
waves, emanting from regions where f ′(x) < 0, causing the solution to progressively steepen
and eventually break into a shock discontinuity. Anyone caught in a traﬃc jam recognizes
the compression waves, where the vehicles are bunched together and almost stationary,
while the interspersed rarefaction waves correspond to freely moving traﬃc. (An intelligent
driver will take advantage of the rarefaction waves moving backwards through the jam
to switch lanes!) The familiar, frustrating traﬃc jam phenomenon, even on accident- or
construction-free stretches of highway, is, thus, an intrinsic eﬀect of the nonlinear transport
models that govern traﬃc ﬂow, [122].
Example 2.12. Triangular wave: Suppose the initial data has the triangular proﬁle
u(0, x) = f(x) =
 x,
0 ≤x ≤1,
0,
otherwise,
as in the ﬁrst graph in Figure 2.22. The initial discontinuity at x = 1 will propagate as a
shock wave, while the slanted line behaves as a rarefaction wave. To ﬁnd the proﬁle at time
t, we ﬁrst graph the multi-valued solution obtained by moving each point on the graph of
f to the right an amount equal to t times its height. As noted above, this motion preserves
straight lines. Thus, points on the x–axis remain ﬁxed, and the diagonal line now goes
from (0, 0) to (1 + t, 1), which is where the uppermost point (1, 1) on the graph of f has
moved to, and hence has slope (1 + t)−1, while the initial vertical shock line has become
tilted, going from (1, 0) to (0, 1 + t). We now need to ﬁnd the position σ(t) of the shock
line in order to satisfy the Equal Area Rule, namely so that the areas of the two shaded
regions in Figure 2.21 are identical. The reader is invited to determine this geometrically;
instead, we invoke the Rankine–Hugoniot condition (2.53). At the shock line, x = σ(t),
the left- and right-hand limiting values are, respectively,
u−(t) = u

t, σ(t)−
= σ(t)
1 + t,
u+(t) = u

t, σ(t)+
= 0,
and hence (2.53) prescribes the shock speed to be
dσ
dt = 1
2
 σ(t)
1 + t + 0

=
σ(t)
2(1 + t) .

2.3 Nonlinear Transport and Shocks
45
t = 0
t = 1
t = 2
Figure 2.22.
Triangular-wave solution.

t
x
Figure 2.23.
Characteristic lines for the triangular-wave shock.
The solution to the resulting separable ordinary diﬀerential equation is easily found. Since
the shock starts out at σ(0) = 1, we deduce that
σ(t) =
√
1 + t ,
with
dσ
dt =
1
2√1 + t .
Further, the strength of the shock, namely its height, is
u−(t) = σ(t)
1 + t =
1
√1 + t .
We conclude that, as t increases, the solution remains a triangular wave, of steadily decreas-
ing slope, while the shock moves oﬀto x = +∞at a progressively slower speed and smaller
height. Its position follows a parabolic trajectory in the (t, x)–plane. See Figure 2.22 for
representative plots of the triangular-wave solution, while Figure 2.23 illustrates the char-
acteristic lines and shock-wave trajectory.
In more general situations, continuing on after the initial shock formation, other char-
acteristic lines may start to cross, thereby producing new shocks. The shocks themselves
continue to propagate, often at diﬀerent velocities. When a fast-moving shock catches up

46
2 Linear and Nonlinear Waves
with a slow-moving shock, one must then decide how to merge the shocks so as to retain a
physically meaningful solution. The Rankine–Hugoniot (Equal Area) and Entropy Condi-
tions continue to uniquely specify the dynamics. However, at this point, the mathematical
details have become too intricate for us to pursue any further, and we refer the interested
reader to Whitham’s book, [122].
See also [57] for a proof of the following existence
theorem for shock-wave solutions to the nonlinear transport equation.
Theorem 2.13. If the initial data u(0, x) = f(x) is piecewise† C1 with ﬁnitely many
jump discontinuities, then, for t > 0, there exists a unique (weak) solution to the nonlinear
transport equation (2.31) that also satisﬁes the Rankine–Hugoniot condition (2.53) and
the entropy condition (2.55).
Remark: Our derivation of the Rankine–Hugoniot shock speed condition (2.53) relied
on the fact that we can write the original partial diﬀerential equation in the form of a
conservation law. But there are, in fact, other ways to do this. For instance, multiplying the
nonlinear transport equation (2.31) by u allows us write it in the alternative conservative
form
u ∂u
∂t + u2 ∂u
∂x = ∂
∂t
 1
2 u2 
+ ∂
∂x
 1
3 u3 
= 0.
(2.56)
In this formulation, the conserved density is T = 1
2 u2, and the associated ﬂux is X = 1
3 u3.
The integrated form (2.49) of the conservation law (2.56) is
d
dt
 b
a
1
2 u(t, x)2 dx = 1
3

u(t, a)3 −u(t, b)3 
.
(2.57)
In some physical models, the integral on the left-hand side represents the energy within the
interval [a, b], and the conservation law tells us that energy can enter the interval as a ﬂux
only through its ends. If we assume that energy is conserved at a shock, then, repeating
our previous argument, we are led to the alternative equation
dσ
dt =
1
3

u−(t)3 −u+(t)3 
1
2

u−(t)2 −u+(t)2  = 2
3
u−(t)2 + u−(t)u+(t) + u+(t)2
u−(t) + u+(t)
(2.58)
for the shock speed. Thus, a shock that conserves energy moves at a diﬀerent speed from
one that conserves mass! The evolution of a shock wave depends not just on the underlying
diﬀerential equation, but also on the physical assumptions governing the selection of a
suitable conservation law.
More General Wave Speeds
Let us ﬁnish this section by considering a nonlinear transport equation
ut + c(u)ux = 0,
(2.59)
whose wave speed is a more general function of the disturbance u. (Further extensions,
allowing c to depend also on t and x, are discussed in Exercise 2.3.20.)
Most of the
†
Meaning continuous everywhere, and continuously diﬀerentiable except at a discrete set of
points; see Deﬁnition 3.7 below for the precise deﬁnition.

2.3 Nonlinear Transport and Shocks
47
development is directly parallel to the special case (2.31) discussed above, and so the
details are left for the reader to ﬁll in, although the shock dynamics does require some
care.
In this case, the characteristic curve equation is
dx
dt = c

u(t, x)

.
(2.60)
As before, the solution u is constant on characteristics, and hence the characteristics are
straight lines, now with slope c(u). Thus, to solve the initial value problem
u(0, x) = f(x),
(2.61)
through each point (0, y) on the x–axis, one draws the characteristic line of slope c(u(0, y)) =
c(f(y)). Until the onset of a shock discontinuity, the solution maintains its initial value
u(0, y) = f(y) along the characteristic line.
A shock forms whenever two characteristic lines cross. As before, the mathematical
equation no longer uniquely speciﬁes the subsequent dynamics, and we need to appeal to
an appropriate conservation law. We write the transport equation in the form
∂u
∂t + ∂
∂x C(u) = 0,
where
C(u) =

c(u) du
(2.62)
is any convenient anti-derivative of the wave speed. Thus, following the same computation
as in (2.44), we discover that conservation of mass now takes the integrated form
d
dt
 b
a
u(t, x) dx = C(u(t, a)) −C(u(t, b)),
(2.63)
with C(u) playing the role of the mass ﬂux. Requiring the conservation of mass, i.e., of
the area under the graph of the solution, means that the Equal Area Rule remains valid.
However, the Rankine–Hugoniot shock-speed condition must be modiﬁed in accordance
with the new dynamics. Mimicking the preceding argument, but with the modiﬁed mass
ﬂux, we ﬁnd that the shock speed is now given by
dσ
dt = C(u−(t)) −C(u+(t))
u−(t) −u+(t)
.
(2.64)
Note that if
c(u) = u,
then
C(u) =

u du = 1
2 u2,
and so (2.64) reduces to our earlier formula (2.53). Moreover, in the limit as the shock
magnitude approaches zero, u−(t) −u+(t) →0, the right-hand side of (2.64) converges to
the derivative C′(u) = c(u) and hence recovers the wave speed, as it should.
Exercises
2.3.1. Discuss the behavior of the solution to the nonlinear transport equation (2.31) for the
following initial data:
(a) u(0, x) =
 2,
x < −1,
1,
x > −1;
(b) u(0, x) =
 −2,
x < −1,
1,
x > −1;
(c) u(0, x) =
 1,
x < 1,
−2,
x > 1.

48
2 Linear and Nonlinear Waves
2.3.2. Solve the following initial value problems:
(a) ut + 3uux = 0, u(0, x) =
 2,
x < 1,
0,
x > 1;
(b) ut −uux = 0, u(1, x) =
 −1,
x < 0,
3,
x > 0;
(c) ut −2uux = 0, u(0, x) =
 1,
x < 1,
0,
x > 1.
2.3.3. Let u(0, x) = (x2 + 1)−1. Does the resulting solution to the nonlinear transport equation
(2.31) produce a shock wave? If so, ﬁnd the time of onset of the shock, and sketch a graph
of the solution just before and soon after the shock wave. If not, explain what happens to
the solution as t increases.
2.3.4. Solve Exercise 2.3.3 when u(0, x) =
(a) −(x2 + 1)−1,
(b) x(x2 + 1)−1.
2.3.5. Consider the initial value problem ut −2uux = 0, u(0, x) = e−x2
. Does the resulting
solution produce a shock wave? If so, ﬁnd the time of onset of the shock and the position
at which it ﬁrst forms. If not, explain what happens to the solution as t increases.
2.3.6.(a) For what values of α, β, γ, δ is u(t, x) = αx + β
γ t + δ a solution to (2.31)?
(b) For what values of α, β, γ, δ, λ, μ is u(t, x) = λt + αx + β
γ t + μx + δ a solution to (2.31)?
2.3.7. A triangular wave is a shock-wave solution to the initial value problem for (2.31) that
has initial data u(0, x) =
 mx,
0 ≤x ≤ℓ,
0,
otherwise.
Assuming m > 0, write down a formula for
the triangular-wave solution at times t > 0. Discuss what happens to the triangular wave as
time progresses.
2.3.8. Solve Exercise 2.3.7 when m < 0.
2.3.9. Solve (2.31) for t > 0 subject to the following initial conditions, and graph your solution
at some representative times. In what sense does your solution conserve mass?
(a) u(0, x) =
 1,
0 < x < 1,
0,
otherwise,
(b) u(0, x) =
 x,
−1 < x < 1,
0,
otherwise,
(c) u(0, x) =
 −x,
−1 < x < 1,
0,
otherwise,
(d) u(0, x) =
 1 −| x |,
−1 < x < 1,
0,
otherwise.
2.3.10. An N–wave is a solution to the nonlinear transport equation (2.31) that has initial con-
ditions u(0, x) =
 mx,
−ℓ≤x ≤ℓ,
0,
otherwise,
where m > 0. (a) Write down a formula for the
N–wave solution at times t > 0.
(b) What about when m < 0?
♦2.3.11. Suppose u(t, x) and u(t, x) are two solutions to the nonlinear transport equation (2.31)
such that, for some t⋆> 0, they agree: u(t⋆, x) = u(t⋆, x) for all x. Do the solutions nec-
essarily have the same initial conditions: u(0, x) = u(0, x)? Use your answer to discuss the
uniqueness of solutions to the nonlinear transport equation.
2.3.12. Suppose that x1 < x2 are such that the characteristic lines of (2.31) through (0, x1)
and (0, x2) cross at a shock at (t, σ(t)) and, moreover, the left- and right-hand shock values
(2.52) are f(x1) = u−(t), f(x1) = u+(t). Explain why the signed area of the region between
the graph of f(x) and the secant line connecting (x1, f(x1)) to (x2, f(x2)) is zero.
♦2.3.13. Consider the initial value problem uε(0, x) = 2 + tan−1(x/ε) for the nonlinear trans-
port equation (2.31). (a) Show that, as ε →0+, the initial condition converges to a step
function (2.51). What are the values of a, b?
(b) Show that, moreover, the resulting solu-
tion uε(0, x) to the nonlinear transport equation converges to the corresponding rarefaction
wave (2.54) resulting from the limiting initial condition.

2.4 The Wave Equation: d’Alembert’s Formula
49
♦2.3.14.(a) Under what conditions can equation (2.35) be solved for a single-valued function
u(t, x)? Hint: Use the Implicit Function Theorem. (b) Use implicit diﬀerentiation to prove
that the resulting function u(t, x) is a solution to the nonlinear transport equation.
2.3.15. For what values of α, β, γ, δ, k is u(t, x) =
αx + β
γ t + δ
k
a solution to the transport equa-
tion ut + u2 ux = 0?
2.3.16.(a) Solve the initial value problem ut + u2 ux = 0, u(0, x) = f(x), by the method of
characteristics. (b) Discuss the behavior of solutions and compare/contrast with (2.31).
2.3.17.(a) Determine the Rankine–Hugoniot condition, based on conservation of mass, for the
speed of a shock for the equation ut + u2 ux = 0.
(b) Solve the initial value problem
u(0, x) =
 a, x < 0,
b, x > 0,
when (i) | a | > | b |, (ii) | a | < | b |. Hint: Use Exercise 2.3.15
to determine the shape of a rarefaction wave.
2.3.18. Solve Exercise 2.3.17 when the wave speed c(u) = (i) 1 −2u, (ii) u3, (iii) sin u.
♦2.3.19. Justify the shock-speed formula (2.58).
♦2.3.20. Consider the general quasilinear ﬁrst-order partial diﬀerential equation
∂u
∂t + c(t, x, u) ∂u
∂x = h(t, x, u).
Let us deﬁne a lifted characteristic curve to be a solution (t, x(t), u(t)) to the system of or-
dinary diﬀerential equations dx
dt = c(t, x, u),
du
dt = h(t, x, u). The corresponding charac-
teristic curve

t, x(t)

is obtained by projecting to the (t, x)–plane. Prove that if u(t, x) is a
solution to the partial diﬀerential equation, and u(t0, x0) = u0, then the lifted characteristic
curve passing through (t0, x0, u0) lies on the graph of u(t, x). Conclude that the graph of
the solution to the initial value problem u(t0, x) = f(x) is the union of all lifted characteris-
tic curves passing through the initial data points

t0, x0, f(x0)

.
2.3.21. Let a > 0. (a) Apply the method of Exercise 2.3.20 to solve the initial value problem
for the damped transport equation: ut + u ux + a u = 0, u(0, x) = f(x).
(b) Does the damping eliminate shocks?
2.3.22. Apply the method of Exercise 2.3.20 to solve the initial value problem
ut + tux = u2,
u(0, x) =
1
1 + x2 .
2.4 The Wave Equation: d’Alembert’s Formula
Newton’s Second Law states that force equals mass times acceleration. It forms the bedrock
underlying the derivation of mathematical models describing all of classical dynamics.
When applied to a one-dimensional medium, such as the transverse displacements of a
violin string or the longitudinal motions of an elastic bar, the resulting model governing
small vibrations is the second-order partial diﬀerential equation
ρ(x) ∂2u
∂t2 = ∂
∂x

κ(x) ∂u
∂x

.
(2.65)
Here u(t, x) represents the displacement of the string or bar at time t and position x,
while ρ(x) > 0 denotes its density and κ(x) > 0 its stiﬀness or tension, both of which are

50
2 Linear and Nonlinear Waves
assumed not to vary with t. The right-hand side of the equation represents the restoring
force due to a (small) displacement of the medium from its equilibrium, whereas the left-
hand side is the product of mass per unit length and acceleration. A correct derivation of
the model from ﬁrst principles would require a signiﬁcant detour, and we refer the reader
to [120, 124] for the details.
We will simplify the general model by assuming that the underlying medium is uni-
form, and so both its density ρ and stiﬀness κ are constant. Then (2.65) reduces to the
one-dimensional wave equation
∂2u
∂t2 = c2 ∂2u
∂x2 ,
where the constant
c =

κ
ρ > 0
(2.66)
is known as the wave speed, for reasons that will soon become apparent.
In general, to uniquely specify the solution to any dynamical system arising from
Newton’s Second Law, including the wave equation (2.66) and the more general vibration
equation (2.65), one must ﬁx both its initial position and initial velocity. Thus, the initial
conditions take the form
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
(2.67)
where, for simplicity, we set the initial time t0 = 0. (See also Exercise 2.4.6.) The initial
value problem seeks the corresponding C2 function u(t, x) that solves the wave equation
(2.66) and has the required initial values (2.67).
In this section, we will learn how to
solve the initial value problem on the entire line −∞< x < ∞.
The analysis of the
wave equation on bounded intervals will be deferred until Chapters 4 and 7. The two-
and three-dimensional versions of the wave equation are treated in Chapters 11 and 12,
respectively.
d’Alembert’s Solution
Let us now derive the explicit solution formula for the second-order wave equation (2.66)
ﬁrst found by d’Alembert. The starting point is to write the partial diﬀerential equation
in the suggestive form
□u = (∂2
t −c2 ∂2
x) u = utt −c2 uxx = 0.
(2.68)
Here
□= ∂2
t −c2 ∂2
x
is a common mathematical notation for the wave operator, which is a linear second-order
partial diﬀerential operator. In analogy with the elementary polynomial factorization
t2 −c2 x2 = (t −cx)(t + cx),
we can factor the wave operator into a product of two ﬁrst-order partial diﬀerential oper-
ators:†
□= ∂2
t −c2 ∂2
x = (∂t −c ∂x) (∂t + c ∂x).
(2.69)
†
The cross terms cancel, thanks to the equality of mixed partial derivatives: ∂t∂xu = ∂x∂tu.
Constancy of the wave speed c is essential here.

2.4 The Wave Equation: d’Alembert’s Formula
51
Now, if the second factor annihilates the function u(t, x), meaning
(∂t + c ∂x) u = ut + c ux = 0,
(2.70)
then u is automatically a solution to the wave equation, since
□u = (∂t −c ∂x) (∂t + c ∂x) u = (∂t −c ∂x) 0 = 0.
We recognize (2.70) as the ﬁrst-order transport equation (2.4) with constant wave speed c.
Proposition 2.1 tells us that its solutions are traveling waves with wave speed c :
u(t, x) = p(ξ) = p(x −ct),
(2.71)
where p is an arbitrary function of the characteristic variable ξ = x −ct.
As long as
p ∈C2 (i.e., is twice continuously diﬀerentiable), the resulting function u(t, x) is a classical
solution to the wave equation (2.66), as you can easily check.
Now, the factorization (2.69) can equally well be written in the reverse order:
□= ∂2
t −c2 ∂2
x = (∂t + c ∂x) (∂t −c ∂x).
(2.72)
The same argument tells us that any solution to the “backwards” transport equation
ut −c ux = 0,
(2.73)
with constant wave speed −c, also provides a solution to the wave equation. Again, by
Proposition 2.1, with c replaced by −c, the general solution to (2.73) has the form
u(t, x) = q(η) = q(x + ct),
(2.74)
where q is an arbitrary function of the alternative characteristic variable η = x + ct. The
solutions (2.74) represent traveling waves moving to the left with constant speed c > 0.
Provided q ∈C2, the functions (2.74) will provide a second family of solutions to the wave
equation.
We conclude that, unlike ﬁrst-order transport equations, the wave equation (2.68)
is bidirectional in that it admits both left and right traveling-wave solutions. Moreover,
by linearity the sum of any two solutions is again a solution, and so we can immediately
construct solutions that are superpositions of left and right traveling waves. The remarkable
fact is that every solution to the wave equation can be so represented.
Theorem 2.14.
Every solution to the wave equation (2.66) can be written as a
superposition,
u(t, x) = p(ξ) + q(η) = p(x −ct) + q(x + ct),
(2.75)
of right and left traveling waves.
Here p(ξ) and q(η) are arbitrary C2 functions, each
depending on its respective characteristic variable
ξ = x −ct,
η = x + ct.
(2.76)
Proof : As in our treatment of the transport equation, we will simplify the wave equa-
tion through an inspired change of variables. In this case, the new independent variables
are the characteristic variables ξ, η deﬁned by (2.76). We set
u(t, x) = v(x −ct, x + ct) = v(ξ, η),
whereby
v(ξ, η) = u
η −ξ
2 c
, η + ξ
2

. (2.77)

52
2 Linear and Nonlinear Waves
Then, employing the chain rule to compute the partial derivatives,
∂u
∂t = c

−∂v
∂ξ + ∂v
∂η

,
∂u
∂x = ∂v
∂ξ + ∂v
∂η ,
(2.78)
and, further,
∂2u
∂t2 = c2
 ∂2v
∂ξ2 −2 ∂2v
∂ξ ∂η + ∂2v
∂η2

,
∂2u
∂x2 = ∂2v
∂ξ2 + 2 ∂2v
∂ξ ∂η + ∂2v
∂η2 .
Therefore
□u = ∂2u
∂t2 −c2 ∂2u
∂x2 = −4c2 ∂2v
∂ξ ∂η .
(2.79)
We conclude that u(t, x) solves the wave equation □u = 0 if and only if v(ξ, η) solves the
second-order partial diﬀerential equation
∂2v
∂ξ ∂η = 0,
which we write in the form
∂
∂ξ
 ∂v
∂η

= ∂w
∂ξ = 0,
where
w = ∂v
∂η .
Thus, applying the methods of Section 2.1 (and making the appropriate assumptions on
the domain of deﬁnition of w), we deduce that
w = ∂v
∂η = r(η),
where r is an arbitrary function of the characteristic variable η. Integrating both sides of
the latter partial diﬀerential equation with respect to η, we ﬁnd
v(ξ, η) = p(ξ) + q(η),
where
q(η) =

r(η) dη,
while p(ξ) represents the η integration “constant”. Replacing the characteristic variables
by their formulas in terms of t and x completes the proof.
Q.E.D.
Let us see how the solution formula (2.75) can be used to solve the initial value problem
(2.67). Substituting into the initial conditions, we deduce that
u(0, x) = p(x) + q(x) = f(x),
∂u
∂t (0, x) = −c p′(x) + c q′(x) = g(x).
(2.80)
To solve this pair of equations for the functions p and q, we diﬀerentiate the ﬁrst,
p′(x) + q′(x) = f ′(x),
and then subtract oﬀthe second equation divided by c; the result is
2 p′(x) = f ′(x) −1
c g(x).
Therefore,
p(x) = 1
2 f(x) −1
2 c
 x
0
g(z) dz + a,

2.4 The Wave Equation: d’Alembert’s Formula
53
t = 0
t = 1
t = 2
t = 3
t = 4
t = 5
Figure 2.24.
Splitting of waves.

where a is an integration constant. The ﬁrst equation in (2.80) then yields
q(x) = f(x) −p(x) = 1
2 f(x) + 1
2 c
 x
0
g(z) dz −a.
Substituting these two expressions back into our solution formula (2.75), we obtain
u(t, x)= p(ξ) + q(η) = f(ξ) + f(η)
2
−1
2 c
 ξ
0
g(z) dz + 1
2 c
 η
0
g(z) dz
= f(ξ) + f(η)
2
+ 1
2 c
 η
ξ
g(z) dz,
where ξ, η are the characteristic variables (2.76).
In this manner, we have arrived at
d’Alembert’s solution to the initial value problem for the wave equation on the real line.
Theorem 2.15. The solution to the initial value problem
∂2u
∂t2 = c2 ∂2u
∂x2 ,
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
−∞< x < ∞,
(2.81)
is given by
u(t, x) = f(x −ct) + f(x + ct)
2
+ 1
2 c
 x+ct
x−ct
g(z) dz.
(2.82)
Remark: In order that (2.82) deﬁne a classical solution to the wave equation, we
need f ∈C2 and g ∈C1.
However, the formula itself makes sense for more general
initial conditions. We will continue to treat the resulting functions as solutions, albeit
nonclassical, since they ﬁt under the more general rubric of “weak solution”, to be developed
in Section 10.4.
Example 2.16.
Suppose there is no initial velocity, so g(x) ≡0, and hence the
motion is purely the result of the initial displacement u(0, x) = f(x). In this case, (2.82)
reduces to
u(t, x) = 1
2 f(x −ct) + 1
2 f(x + ct).
(2.83)

54
2 Linear and Nonlinear Waves
t = 0
t = 1
t = 2
t = 3
t = 4
t = 5
Figure 2.25.
Interaction of waves.

The eﬀect is that the initial displacement splits into two waves, one moving to the right
and the other moving to the left, each of constant speed c, and each of exactly the same
shape as f(x), but only half as tall. For example, if the initial displacement is a localized
pulse centered at the origin, say
u(0, x) = e−x2,
∂u
∂t (0, x) = 0,
then the solution
u(t, x) = 1
2 e−(x−ct)2 + 1
2 e−(x+ct)2
consists of two half size pulses running away from the origin with the same speed c, but
in opposite directions. A graph of the solution at several successive times can be seen in
Figure 2.24.
If we take two initially separated pulses, say
u(0, x) = e−x2 + 2 e−(x−1)2,
∂u
∂t (0, x) = 0,
centered at x = 0 and x = 1, then the solution
u(t, x) = 1
2 e−(x−ct)2 + e−(x−1−ct)2 + 1
2 e−(x+ct)2 + e−(x−1+ct)2
will consist of four pulses, two moving to the right and two to the left, all with the same
speed. An important observation is that when a right-moving pulse collides with a left-
moving pulse, they emerge from the collision unchanged, which is a consequence of the
inherent linearity of the wave equation. In Figure 2.25, the ﬁrst picture plots the initial
displacement. In the second and third pictures, the two localized bumps have each split into
two copies moving in opposite directions. In the fourth and ﬁfth, the larger right-moving
bump is in the process of interacting with the smaller left-moving bump. Finally, in the
last picture the interaction is complete, and the individual pairs of left- and right-moving
waves move oﬀin tandem in opposing directions, experiencing no further collisions.
In general, if the initial displacement is localized, so that | f(x) | ≪1 for | x | ≫0, then,
after a ﬁnite time, the left- and right-moving waves will separate, and the observer will see
two half-size replicas running away, with speed c, in opposite directions. If the displacement

2.4 The Wave Equation: d’Alembert’s Formula
55
−1
1
−2
−1
1
2
Figure 2.26.
The error function erf x.
is not localized, then the left and right traveling waves will never fully disengage, and one
might be hard pressed to recognize that a complicated solution pattern is, in reality, just
the superposition of two simple traveling waves. For example, consider the elementary
trigonometric solution
cos ct cos x = 1
2 cos(x −ct) + 1
2 cos(x + ct).

(2.84)
In accordance with the left-hand expression, an observer will see a standing cosinusoidal
wave that vibrates up and down with frequency c. However, the d’Alembert form of the
solution on the right-hand side says that this is just the sum of left- and right-traveling
cosine waves! The interactions of their peaks and troughs reproduce the standing wave.
Thus, the same solution can be interpreted in two seemingly incompatible ways. And,
in fact, this paradox lies at the heart of the perplexing wave-particle duality of quantum
physics.
Example 2.17.
By way of contrast, suppose there is no initial displacement, so
f(x) ≡0, and the motion is purely the result of the initial velocity ut(0, x) = g(x).
Physically, this models a violin string at rest being struck by a “hammer blow” at the
initial time. In this case, the d’Alembert formula (2.82) reduces to
u(t, x) = 1
2 c
 x+ct
x−ct
g(z) dz.
(2.85)
For example, when u(0, x) = 0, ut(0, x) = e−x2, the resulting solution (2.85) is
u(t, x) = 1
2 c
 x+ct
x−ct
e−x2 dz =
√π
4 c

erf(x + ct) −erf(x −ct)

,
(2.86)
where
erf x =
2
√π
 x
0
e−z2 dz
(2.87)
is known as the error function due to its many applications throughout probability and
statistics, [39].
The error function integral cannot be written in terms of elementary
functions; nevertheless, its properties have been well studied and its values tabulated,
[86]. A graph appears in Figure 2.26. The constant in front of the integral (2.87) has been
chosen so that the error function has asymptotic values
lim
x →∞erf x = 1,
lim
x →−∞erf x = −1,
(2.88)

56
2 Linear and Nonlinear Waves
t = 0
t = 1
t = 2
t = 3
t = 4
t = 5
Figure 2.27.
Error function solution to the wave equation.

which follow from a well-known integration formula to be derived in Exercise 2.4.21.
A graph of the solution (2.86) at successive times is displayed in Figure 2.27. The
ﬁrst graph shows the zero initial displacement. Gradually, the eﬀect of the initial hammer
blow is felt further and further away along the string, as the two wave fronts propagate
away from the origin, both with speed c, but in opposite directions. Thus, unlike the case
of a nonzero initial displacement in Figure 2.24, where the solution eventually returns to
its equilibrium position u = 0 after the wave passes by, a nonzero initial velocity leaves the
string permanently deformed.
In general, the lines of slope ±c, where the respective characteristic variables are
constant,
ξ = x −ct = a,
η = x + ct = b,
(2.89)
are known as the characteristics of the wave equation. Thus, the second-order wave equa-
tion has two distinct characteristic lines passing through each point in the (t, x)–plane.
Remark: The characteristic lines are the one-dimensional counterparts of the light
cone in Minkowski space-time, which plays a starring role in special relativity, [70, 75].
See Section 12.5 for further details.
In Figure 2.28, we plot the two characteristics going through a point (0, y) on the x
axis. The wedge-shaped region {y −ct ≤x ≤y + ct, t ≥0} lying between them is known
as the domain of inﬂuence of the point (0, y), since, in general, the value of the initial data
at a point will aﬀect the subsequent solution values only in its domain of inﬂuence. Indeed,
the eﬀect of an initial displacement at the point y propagates along the two characteristic
lines, while the eﬀect of an initial velocity there will be felt at every point in the triangular
wedge.
External Forcing and Resonance
When a homogeneous vibrating medium is subjected to external forcing, the wave equation
acquires an additional, inhomogeneous term:
∂2u
∂t2 = c2 ∂2u
∂x2 + F(t, x),
(2.90)

2.4 The Wave Equation: d’Alembert’s Formula
57
t
x
(0, y)
Figure 2.28.
Characteristic lines and domain of inﬂuence.
in which F(t, x) represents a force imposed at time t and spatial position x. With a bit
more work, d’Alembert’s solution technique can be readily adapted to incorporate the
forcing term.
Let us, for simplicity, assume that the diﬀerential equation is supplemented by homo-
geneous initial conditions,
u(0, x) = 0,
ut(0, x) = 0,
(2.91)
meaning that there is no initial displacement or velocity. To solve the initial value problem
(2.90–91), we switch to the same characteristic coordinates (2.76), setting
v(ξ, η) = u
η −ξ
2 c
, η + ξ
2

.
Invoking the chain rule formulas (2.79), we ﬁnd that the forced equation (2.90) becomes
∂2v
∂ξ ∂η = −1
4c2 F
η −ξ
2 c
, η + ξ
2

.
(2.92)
Let us integrate both sides of the equation with respect to η, on the interval ξ ≤ζ ≤η:
∂v
∂ξ (ξ, η) −∂v
∂ξ (ξ, ξ) = −1
4c2
 η
ξ
F
ζ −ξ
2 c
, ζ + ξ
2

dζ.
(2.93)
But, recalling (2.78),
∂v
∂ξ (ξ, η) = 1
2c
∂u
∂t
η −ξ
2 c
, η + ξ
2

+ 1
2
∂u
∂x
η −ξ
2 c
, η + ξ
2

,
and so, in particular,
∂v
∂ξ (ξ, ξ) = 1
2c
∂u
∂t (0, ξ) + 1
2
∂u
∂x (0, ξ) = 0,
which vanishes owing to our choice of homogeneous initial conditions (2.91). Indeed, the
initial velocity condition says that ut(0, x) = 0, while diﬀerentiating the initial displacement

58
2 Linear and Nonlinear Waves
condition u(0, x) = 0 with respect to x implies that ux(0, x) = 0 for all x, including x = ξ.
As a result, (2.93) simpliﬁes to
∂v
∂ξ (ξ, η) = −1
4c2
 η
ξ
F
ζ −ξ
2 c
, ζ + ξ
2

dζ.
We now integrate the latter equation with respect to ξ on the interval ξ ≤χ ≤η, producing
−v(ξ, η) = v(η, η) −v(ξ, η) = −1
4c2
 η
ξ
 η
χ
F
ζ −χ
2 c
, ζ + χ
2

dζ dχ,
since v(η, η) = u(0, η) = 0, thanks again to the initial conditions. In this manner, we
have produced an explicit formula for the solution to the characteristic variable version of
the forced wave equation subject to the homogeneous initial conditions. Reverting to the
original physical coordinates, the left-hand side of this equation becomes −u(t, x). As for
the double integral on the right-hand side, it takes place over the triangular region
T(ξ, η) = { (χ, ζ) | ξ ≤χ ≤ζ ≤η } .
(2.94)
Let us introduce “physical” integration variables by setting
χ = y −c s,
ζ = y + c s.
The deﬁning inequalities of the triangle (2.94) become
x −ct ≤y −c s ≤y + c s ≤x + ct,
and so, in the physical coordinates, the triangular integration domain assumes the form
D(t, x) = { (s, y) | x −c (t −s) ≤y ≤x + c (t −s), 0 ≤s ≤t } ,
(2.95)
which is graphed in Figure 2.29.
The change of variables formula for double integrals
requires that we compute the Jacobian determinant
det
 ∂χ/∂y
∂χ/∂s
∂ζ/∂y
∂ζ/∂s

= det

1
−c
1
c

= 2c,
and so dχ dζ = 2c ds dy. Therefore,
u(t, x) = 1
2c
 
D(t,x)
F(s, y) ds dy = 1
2c
 t
0
 x+c (t−s)
x−c (t−s)
F(s, y) dy ds,
(2.96)
which gives the solution formula for the forced wave equation when subject to homogeneous
initial conditions.
To solve the general initial value problem, we appeal to linear superposition, writing its
solution as a sum of the solution (2.96) to the forced wave equation subject to homogeneous
initial conditions plus the d’Alembert solution (2.82) to the unforced equation subject to
inhomogeneous boundary conditions.
Theorem 2.18. The solution to the general initial value problem
utt = c2uxx + F(t, x),
u(0, x) = f(x),
ut(0, x) = g(x),
−∞< x < ∞,
t > 0,
for the wave equation subject to an external forcing is given by
u(t, x) = f(x −ct) + f(x + ct)
2
+ 1
2 c
 x+ct
x−ct
g(y) dy + 1
2c
 t
0
 x+c (t−s)
x−c (t−s)
F(s, y) dy ds.
(2.97)

2.4 The Wave Equation: d’Alembert’s Formula
59
t
x
(t, x)
(0, x + ct)
(0, x −ct)
Figure 2.29.
Domain of dependence.
Observe that the solution is a linear superposition of the respective eﬀects of the initial
displacement, the initial velocity, and the external forcing.
The triangular integration
region (2.95), lying between the x–axis and the characteristic lines going backwards from
(t, x), is known as the domain of dependence of the point (t, x). This is because, for any
t > 0, the solution value u(t, x) depends only on the values of the initial data and the
forcing function at points lying within the domain of dependence D(t, x). Indeed, the ﬁrst
term in the solution formula (2.97) requires only the initial displacement at the corners
(0, x + ct), (0, x −ct); the second term requires only the initial velocity at points on the
x–axis lying on the vertical side of D(t, x); while the ﬁnal term requires the value of the
external force on the entire triangular region.
Example 2.19. Let us solve the initial value problem
utt = uxx + sin ωt sin x,
u(0, x) = 0,
ut(0, x) = 0,
for the wave equation with unit wave speed subject to a sinusoidal forcing function whose
amplitude varies periodically in time with frequency ω > 0. According to formula (2.96),
the solution is
u(t, x) = 1
2
 t
0
 x+t−s
x−t+s
sin ωs sin y dy ds
= 1
2
 t
0
sin ωs

cos(x −t + s) −cos(x + t −s)

ds
=
⎧
⎪
⎪
⎨
⎪
⎪
⎩
sin ωt −ω sin t
1 −ω2
sin x,
0 < ω ̸= 1,
sin t −t cos t
2
sin x,
ω = 1.
Notice that, when ω ̸= 1, the solution is bounded, being a combination of two vibrational
modes: an externally induced mode at frequency ω along with an internal mode, at fre-
quency 1. If ω = p/q ̸= 1 is a rational number, then the solution varies periodically in

60
2 Linear and Nonlinear Waves
cos t + cos 7
3 t
cos t + cos
√
5 t
Figure 2.30.
Periodic and quasiperiodic functions.
time. On the other hand, if ω is irrational, then the solution is only quasiperiodic, and never
exactly repeats itself. Finally, if ω = 1, the solution grows without limit as t increases,
indicating that this is a resonant frequency. We will investigate external forcing and the
mechanisms leading to resonance in dynamical partial diﬀerential equations in more detail
in Chapters 4 and 6.
Example 2.20.
To appreciate the diﬀerence between periodic and quasiperiodic
vibrations, consider the elementary trigonometric function
u(t) = cos t + cos ωt,
which is a linear combination of two simple periodic vibrations, of frequencies 1 and ω. If
ω = p/q is a rational number, then u(t) is a periodic function of period 2πq, so u(t+2πq) =
u(t). However, if ω is an irrational number, then u(t) is not periodic, and never repeats.
You are encouraged to inspect the graphs in Figure 2.30. The ﬁrst is periodic — can you
spot where it begins to repeat? — whereas the second is only quasiperiodic. The only
quasiperiodic functions we will encounter in this text are linear combinations of periodic
trigonometric functions whose frequencies are not all rational multiples of each other. To
the uninitiated, such quasiperiodic motions may appear to be random, even though they are
built from a few simple periodic constituents. While ostensibly complicated, quasiperiodic
motion is not true chaos, which is is an inherently nonlinear phenomenon, [77].
Exercises
2.4.1. Solve the initial value problem utt = c2uxx, u(0, x) = e−x2
, ut(0, x) = sin x.
2.4.2.(a) Solve the wave equation utt = uxx when the initial displacement is the box function
u(0, x) =
 1,
1 < x < 2,
0,
otherwise,
while the initial velocity is 0.
(b) Sketch the resulting solution at several representative times.

2.4 The Wave Equation: d’Alembert’s Formula
61
2.4.3. Answer Exercise 2.4.2 when the initial velocity is the box function, while the initial dis-
placement is zero.
2.4.4. Write the following solutions to the wave equation utt = uxx in d’Alembert form (2.82).
Hint: What is the appropriate initial data?
(a) cos x cos t,
(b) cos 2x sin 2t,
(c) ex+t,
(d) t2 + x2,
(e) t3 + 3tx2.
♥2.4.5.(a) Solve the dam break problem, that is, the wave equation when the initial displacement
is a step function σ(x) =
 1,
x > 0,
0,
x < 0,
and there is no initial velocity. (b) Analyze the
case in which there is no initial displacement, while the initial velocity is a step function.
(c) Are your solutions classical solutions? Explain your answer. (d) Prove that the step
function is the limit, as n →∞, of the functions fn(x) = 1
π tan−1 nx + 1
2 . (e) Show that,
in both cases, the step function solution can be realized as the limit, as n →∞, of solutions
to the initial value problems with the functions fn(x) as initial displacement or velocity.
♦2.4.6. Suppose u(t, x) solves the initial value problem u(0, x) = f(x), ut(0, x) = g(x), for the
wave equation (2.66). Prove that the solution to the initial value problem u(t0, x) = f(x),
ut(t0, x) = g(x), is u(t −t0, x).
2.4.7. Find all resonant frequencies for the wave equation with wave speed c when subject to
the external forcing function F(t, x) = sin ω t sin kx for ﬁxed ω, k > 0.
2.4.8. Consider the initial value problem utt = 4uxx + F(t, x), u(0, x) = f(x), ut(0, x) = g(x).
Determine (a) the domain of inﬂuence of the point (0, 2); (b) the domain of dependence of
the point (3, −1); (c) the domain of inﬂuence of the point (3, −1).
2.4.9.(a) A solution to the wave equation utt = 2uxx is generated by a displacement concen-
trated at position x0 = 1 and time t0 = 0, but no initial velocity. At what time will an
observer at position x1 = 5 feel the eﬀect of this displacement? Will the observer continue
to feel an eﬀect in the future? (b) Answer part (a) when there is an initial velocity concen-
trated at position x0 = 1 and time t0 = 0, but no initial displacement.
2.4.10. Suppose u(t, x) solves the initial value problem utt = 4uxx + sin ω t cos x, u(0, x) = 0,
ut(0, x) = 0. Is h(t) = u(t, 0) a periodic function?
♥2.4.11.(a) Write down an explicit formula for the solution to the initial value problem
∂2u
∂t2 −4 ∂2u
∂x2 = 0,
u(0, x) = sin x,
∂u
∂t (0, x) = cos x,
−∞< x < ∞,
t ≥0.
(b) True or false: The solution is a periodic function of t.
(c) Now solve the forced initial value problem
∂2u
∂t2 −4 ∂2u
∂x2 = cos 2t,
u(0, x) = sin x,
∂u
∂t (0, x) = cos x,
−∞< x < ∞,
t ≥0.
(d) True or false: The forced equation exhibits resonance. Explain.
(e) Does the answer to part (d) change if the forcing function is sin 2t?
2.4.12. Given a classical solution u(t, x) of the wave equation, let E =
1
2 (u2
t + c2u2
x) be the
associated energy density and P = utux the momentum density.
(a) Show that both E and P are conserved densities for the wave equation.
(b) Show that E(t, x) and P(t, x) both satisfy the wave equation.
♦2.4.13. Let u(t, x) be a classical solution to the wave equation utt = c2uxx. The total energy
E(t) =
	 ∞
−∞
1
2

 ∂u
∂t
2
+ c2
∂u
∂x
2 
dx
(2.98)
represents the sum of kinetic and potential energies of the displacement u(t, x) at time t.
Suppose that ∇u →0 suﬃciently rapidly as x →±∞; more precisely, one can ﬁnd α > 1
2
and C(t) > 0 such that | ut(t, x) |, | ux(t, x) | ≤C(t)/| x |α for each ﬁxed t and all suﬃciently
large | x | ≫0. For such solutions, establish the Law of Conservation of Energy by showing
that E(t) is ﬁnite and constant. Hint: You do not need the formula for the solution.

62
2 Linear and Nonlinear Waves
♦2.4.14.(a) Use Exercise 2.4.13 to prove that the only classical solution to the initial-boundary
value problem utt = c2uxx, u(0, x) = 0, ut(0, x) = 0, satisfying the indicated decay assump-
tions is the trivial solution u(t, x) ≡0. (b) Establish the following Uniqueness Theorem for
the wave equation: there is at most one such solution to the initial-boundary value problem
utt = c2uxx, u(0, x) = f(x), ut(0, x) = g(x).
2.4.15. The telegrapher’s equation utt + aut = c2uxx, with a > 0, models the vibration of
a string under frictional damping. (a) Show that, under the decay assumptions of Exer-
cise 2.4.13, the wave energy (2.98) of a classical solution is a nonincreasing function of t.
(b) Prove uniqueness of such solutions to the initial value problem for the telegrapher’s
equation.
2.4.16. What happens to the proof of Theorem 2.14 if c = 0?
2.4.17.(a) Explain why the d’Alembert factorization method doesn’t work when the wave speed
c(x) depends on the spatial variable x.
(b) Does it work when c(t) depends only on the time t?
2.4.18. The Poisson–Darboux equation is ∂2u
∂t2 −∂2u
∂x2 −2
x
∂u
∂x = 0. Solve the initial value problem
u(0, x) = 0, ut(0, x) = g(x), where g(x) = g(−x) is an even function. Hint: Set w = xu.
♥2.4.19.(a) Solve the initial value problem utt −2utx −3uxx = 0, u(0, x) = x2, ut(0, x) = ex.
Hint: Factor the associated linear diﬀerential operator. (b) Determine the domain of inﬂu-
ence of a point (0, x). (c) Determine the domain of dependence of a point (t, x) with t > 0.
♦2.4.20.(a) Use polar coordinates to prove that, for any a > 0,
		
R2 e−a(x2+y2) dx dy = π
a .
(2.99)
(b) Explain why
	 ∞
−∞e−ax2
dx =

π
a .
(2.100)
♦2.4.21. Use Exercise 2.4.20 to prove the error function formulae (2.88).

Chapter 3
Fourier Series
Just before 1800, the French mathematician/physicist/engineer Jean Baptiste Joseph
Fourier made an astonishing discovery, [42]. Through his deep analytical investigations
into the partial diﬀerential equations modeling heat propagation in bodies, Fourier was
led to claim that “every” function could be represented as an inﬁnite series of elementary
trigonometric functions: sines and cosines. For example, consider the sound produced by
a musical instrument, e.g., piano, violin, trumpet, or drum. Decomposing the signal into
its trigonometric constituents reveals the fundamental frequencies (tones, overtones, etc.)
that combine to produce the instrument’s distinctive timbre. This Fourier decomposition
lies at the heart of modern electronic music; a synthesizer combines pure sine and cosine
tones to reproduce the diverse sounds of instruments, both natural and artiﬁcial, according
to Fourier’s general prescription.
Fourier’s claim was so remarkable and counterintuitive that most of the leading math-
ematicians of the time did not believe him. Nevertheless, it was not long before scientists
came to appreciate the power and far-ranging applicability of Fourier’s method, thereby
opening up vast new realms of mathematics, physics, engineering, and beyond. Indeed,
Fourier’s discovery easily ranks in the “top ten” mathematical advances of all time, a list
that would also include Newton’s invention of the calculus, and Gauss and Riemann’s
diﬀerential geometry, which, 70 years later, became the foundation of Einstein’s general
relativity. Fourier analysis is an essential component of much of modern applied (and pure)
mathematics. It forms an exceptionally powerful analytic tool for solving a broad range of
linear partial diﬀerential equations. Applications in physics, engineering, biology, ﬁnance,
etc., are almost too numerous to catalogue: typing the word “Fourier” in the subject index
of a modern science library will dramatically demonstrate just how ubiquitous these meth-
ods are. Fourier analysis lies at the heart of signal processing, including audio, speech,
images, videos, seismic data, radio transmissions, and so on. Many modern technologi-
cal advances, including television, music CDs and DVDs, cell phones, movies, computer
graphics, image processing, and ﬁngerprint analysis and storage, are, in one way or another,
founded on the many ramiﬁcations of Fourier theory. In your career as a mathematician,
scientist, or engineer, you will ﬁnd that Fourier theory, like calculus and linear algebra, is
one of the most basic weapons in your mathematical arsenal. Mastery of the subject is
essential.
Furthermore, a surprisingly large fraction of modern mathematics rests on subsequent
attempts to place Fourier series on a ﬁrm mathematical foundation. Thus, many of modern
analysis’ most basic concepts, including the deﬁnition of a function, the ε–δ deﬁnition
of limit and continuity, convergence properties in function space, the modern theory of
integration and measure, generalized functions such as the delta function, and many others,
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
63
3
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

64
3 Fourier Series
all owe a profound debt to the prolonged struggle to establish a rigorous framework for
Fourier analysis. Even more remarkably, modern set theory, and, thus, the foundations
of modern mathematics and logic, can be traced directly back to the nineteenth-century
German mathematician Georg Cantor’s attempts to understand the sets on which Fourier
series converge!
We begin our development of Fourier methods by explaining why Fourier series nat-
urally appear when we try to solve the one-dimensional heat equation. The reader unin-
terested in such motivations can safely omit this initial section, since the same material
reappears in Chapter 4, where we apply Fourier methods to solve several important linear
partial diﬀerential equations. Beginning in Section 3.2, we shall introduce the most basic
computational techniques for Fourier series. The ﬁnal section is an abbreviated introduc-
tion to the analytic background required to develop a rigorous foundation for Fourier series
methods. While this section is a bit more mathematically sophisticated than what has ap-
peared so far, the student is strongly encouraged to delve into it to gain additional insight
and see further developments, including some of direct importance in applications.
3.1 Eigensolutions of Linear Evolution Equations
Following our studies of ﬁrst-order partial diﬀerential equations in Chapter 2, the next
important example to merit investigation is the second-order linear equation
∂u
∂t = ∂2u
∂x2 ,
(3.1)
known as the heat equation, since it models (among other diﬀusion processes) heat ﬂow
in a one-dimensional medium, e.g., a metal bar. For simplicity, we have set the physical
parameters equal to 1 in order to focus on the solution techniques.
A more complete
discussion, including a brief derivation from physical principles, will appear in Chapter 4.
Unlike the wave equation considered in Chapter 2, there is no comparably elementary
formula for the general solution to the heat equation.
Instead, we will write solutions
as inﬁnite series in certain simple, explicit solutions. This solution method, pioneered by
Fourier, will lead us immediately to the deﬁnition of a Fourier series. The remainder of this
chapter will be devoted to developing the basic properties and calculus of Fourier series.
Once we have mastered these essential mathematical techniques, we will start applying
them to partial diﬀerential equations in Chapter 4.
Let us begin by writing the heat equation (3.1) in a more abstract, but suggestive,
linear evolutionary form
∂u
∂t = L[u],
(3.2)
in which
L[u] = ∂2u
∂x2
(3.3)
is a linear second-order diﬀerential operator. Recall, (1.11), that linearity imposes two
requirements on the operator L:
L[u + v] = L[u] + L[v],
L[cu] = cL[u],
(3.4)

3.1 Eigensolutions of Linear Evolution Equations
65
for any functions† u, v and any constant c. Moreover, since L involves diﬀerentiation only
with respect to x, it also satisﬁes
L[c(t)u] = c(t)L[u]
(3.5)
for any function c(t) that does not depend on x.
Of course, there are many other possible linear diﬀerential operators, and so our ab-
stract linear evolution equation (3.2) can represent a wide range of linear partial diﬀerential
equations. For example, if
L[u] = −c(x) ∂u
∂x ,
(3.6)
where c(x) is a function representing the wave speed in a nonuniform medium, then (3.2)
becomes the transport equation
∂u
∂t = −c(x) ∂u
∂x
(3.7)
that we studied in Chapter 2. If
L[u] =
1
σ(x)
∂
∂x

κ(x) ∂u
∂x

,
(3.8)
where σ(x) > 0 represents heat capacity and κ(x) > 0 thermal conductivity, then (3.2)
becomes the generalized heat equation
∂u
∂t =
1
σ(x)
∂
∂x

κ(x) ∂u
∂x

,
(3.9)
governing the diﬀusion of heat in a nonuniform bar. If
L[u] = ∂2u
∂x2 −γ u,
(3.10)
where γ > 0 is a positive constant, then (3.2) becomes the damped heat equation
∂u
∂t = ∂2u
∂x2 −γ u,
(3.11)
which models the temperature of a bar that is cooling oﬀdue to radiation of heat energy.
We can even take u to be a function of more than one space variable, e.g., u(t, x, y) or
u(t, x, y, z), in which case (3.2) includes higher-dimensional versions of the heat equation
for plates and solid bodies, which we will study in due course.
In all cases, the key
requirements on the operator L are (a) linearity, and (b) only diﬀerentiation with respect
to the spatial variables is allowed.
Fourier’s inspired idea for solving such linear evolution equations is a direct adaptation
of the eigensolution method for ﬁrst-order linear systems of ordinary diﬀerential equations,
[20, 23, 89], which we now recall. The starting point is the elementary scalar ordinary
diﬀerential equation
du
dt = λ u.
(3.12)
†
We assume throughout that the functions are suﬃciently smooth so that the indicated
derivatives are well deﬁned.

66
3 Fourier Series
The general solution is an exponential function
u(t) = c eλt,
(3.13)
whose coeﬃcient c is an arbitrary constant. This elementary observation motivates the
solution method for a ﬁrst-order homogeneous linear system of ordinary diﬀerential equa-
tions
du
dt = Au,
(3.14)
in which A is a constant n × n matrix. Working by analogy, we will seek solutions of
exponential form
u(t) = eλt v,
(3.15)
where v ∈Rn is a constant vector. We substitute this ansatz † into the equation. First,
du
dt = d
dt

eλt v

= λ eλt v.
On the other hand, since eλt is a scalar, it commutes with matrix multiplication, and so
Au = A eλt v = eλtAv.
Therefore, u(t) will solve the system (3.14) if and only if v satisﬁes
Av = λv.
(3.16)
We recognize this as the eigenequation that determines the eigenvalues of the matrix A.
Namely, (3.16) has a nonzero solution v ̸= 0 if and only if λ is an eigenvalue and v a
corresponding eigenvector. Each eigenvalue λ and eigenvector v produces a nonzero, expo-
nentially varying eigensolution (3.15) to the linear system of ordinary diﬀerential equations.
Remark: Any nonzero scalar multiple of an eigenvector v = cv, for c ̸= 0, is auto-
matically another eigenvector for the same eigenvalue λ. However, the only eﬀect is to
multiply the eigensolution by the scalar c. Thus, to obtain a complete system of indepen-
dent solutions, we need only the independent eigenvectors.
For simplicity — and also because all of the linear partial diﬀerential equations we
will treat will have the analogous property — suppose that the n × n matrix A has a
complete system of real eigenvalues λ1, . . . , λn and corresponding real, linearly independent
eigenvectors v1, . . . , vn, which therefore form an eigenvector basis of the underlying space
Rn. (We allow the possibility of repeated eigenvalues, but require that all eigenvectors be
independent to avoid superﬂuous solutions.) For example, according to Theorem B.26 (see
also [89; Theorem 8.20]), all real, symmetric matrices, A = AT , are complete. Complex
eigenvalues lead to complex exponential solutions, whose real and imaginary parts can be
used to construct the associated real solutions. Incomplete matrices, having an insuﬃcient
number of eigenvectors, are trickier, and the solution to the corresponding linear system
†
The German word ansatz refers to the method of ﬁnding a solution to a complicated equation
by postulating that it is of a special form. Usually, an ansatz will depend on one or more free
parameters — in this case, the entries of the vector v along with the scalar λ — that, with some
luck, can be adjusted to fulﬁll the requirements imposed by the equation. Thus, a reasonable
English translation of “ansatz” is “inspired guess”.

3.1 Eigensolutions of Linear Evolution Equations
67
requires use of the Jordan canonical form, [89; Section 8.6]. Fortunately, we do not have
to deal with the latter, technically annoying, cases here.
Using our completeness assumption, we can produce n independent real exponential
eigensolutions
u1(t) = eλ1tv1,
. . .
un(t) = eλntvn,
to the linear system (3.14). The Linear Superposition Principle of Theorem 1.4 tells us
that, for any choice of scalars c1, . . ., cn, the linear combination
c1u1(t) + · · · + cnun(t) = c1eλ1t v1 + · · · + cneλnt vn
(3.17)
is also a solution. The basic Existence and Uniqueness Theorems for ﬁrst-order systems of
ordinary diﬀerential equations, [18, 23, 52], imply that (3.17) forms the general solution
to the original linear system, and so the eigensolutions form a basis for the solution space.
Let us now adapt this seminal idea to construct exponentially varying solutions to the
heat equation (3.1) or, for that matter, any linear evolution equation in the form (3.2). To
this end, we introduce an analogous exponential ansatz:
u(t, x) = eλt v(x),
(3.18)
in which we replace the vector v in (3.15) by a function v(x). We substitute the expression
(3.18) into the dynamical equations (3.2). First, the time derivative of such a function is
∂u
∂t = ∂
∂t

eλt v(x)

= λ eλt v(x).
On the other hand, in view of (3.5),
L[u] = L

eλt v(x)

= eλt L[v].
Equating these two expressions and canceling the common exponential factor, we conclude
that v(x) must satisfy the eigenequation
L[v] = λ v
(3.19)
for the linear diﬀerential operator L, in which λ is the eigenvalue, while v(x) is the corre-
sponding eigenfunction. Each eigenvalue and eigenfunction pair will produce an exponen-
tially varying eigensolution (3.18) to the partial diﬀerential equation (3.2). We will then
appeal to Linear Superposition to combine the resulting eigensolutions to form additional
solutions.
The key complication is that partial diﬀerential equations admit an inﬁnite
number of independent eigensolutions, and thus one cannot hope to write the general solu-
tion as a ﬁnite linear combination thereof. Rather, one is led to try constructing solutions
as inﬁnite series in the eigensolutions. However, justifying such series solution formulas
requires additional analytical skills and sophistication. Not every inﬁnite series converges
to a bona ﬁde function. Moreover, a convergent series of diﬀerentiable functions need not
converge to a diﬀerentiable function, and hence the series may not represent a (classical)
solution to the partial diﬀerential equation. We are being reminded, yet again, that partial
diﬀerential equations are much wilder creatures than their relatively tame cousins, ordinary
diﬀerential equations.
Let us, for speciﬁcity, focus our attention on the heat equation, for which the linear
operator L is given by (3.3). If v(x) is a function of x alone, then
L[v] = v′′(x).

68
3 Fourier Series
Thus, our eigenequation (3.19) becomes
v′′ = λv.
(3.20)
This is a linear second-order ordinary diﬀerential equation for v(x), and so has two linearly
independent solutions. The explicit solution formulas depend on the sign of the eigenvalue
λ, and can be found in any basic text on ordinary diﬀerential equations, e.g., [20, 23]. The
following table summarizes the results for real eigenvalues λ; the case of complex λ is left
as Exercise 3.1.3 for the reader. The resulting exponential eigensolutions are also referred
to as separable solutions to indicate that they are the product of a function of t alone and a
function of x alone. The general method of separation of variables will be one of our main
tools for solving linear partial diﬀerential equations, to be developed in detail starting in
Chapter 4.
Real Eigensolutions of the Heat Equation
λ
Eigenfunctions v(x)
Eigensolutions u(t, x) = eλt v(x)
λ = −ω2 < 0
cos ωx, sin ωx
e−ω2t cos ωx, e−ω2t sin ωx
λ = 0
1, x
1, x
λ = ω2 > 0
e−ω x, eω x
eω2t−ω x, eω2t+ω x
Remark: Thus, in the absence of boundary conditions, each real number λ qualiﬁes as
an eigenvalue of the linear diﬀerential operator (3.3), possessing two linearly independent
eigenfunctions, and thus two linearly independent eigensolutions to the heat equation. As
with eigenvectors, any (nonzero) linear combination of eigenfunctions (eigensolutions) with
the same eigenvalue is also an eigenfunction (eigensolution). Thus, the preceding table lists
only independent eigenfunctions and eigensolutions.
As noted above, any ﬁnite linear combination of these basic eigensolutions is auto-
matically a solution. Thus, for example,
u(t, x) = c1e−t cos x + c2e−4t sin 2x + c3x + c4
is a solution to the heat equation for any choice of constants c1, c2, c3, c4, as you can easily
check. But, since there are inﬁnitely many independent eigensolutions, we cannot expect
to be able to represent every solution to the heat equation as a ﬁnite linear combination
of eigensolutions. And so, we must learn how to deal with inﬁnite series of eigensolutions.
Remark: Eigensolutions in the ﬁrst class, where λ < 0, are exponentially decaying,
which is in accord with our physical intuition as to how the temperature of a body should
behave. Those in the second class are constant in time — also physically reasonable. How-
ever, those in the third class, corresponding to positive eigenvalues λ > 0, are exponentially
growing in time. In the absence of external heat sources, physical bodies should approach
some sort of thermal equilibrium, and certainly not an exponentially growing temperature!
However, notice that the latter eigensolutions (as well as the solution x) are not bounded in
space, and so include an inﬁnite amount of heat energy being supplied to the system from

3.1 Eigensolutions of Linear Evolution Equations
69
inﬁnity. As we will soon come to appreciate, physically relevant boundary conditions —
posed either on a bounded interval or by specifying the asymptotics of the solutions at large
distances — will separate out the physically reasonable solutions from the mathematically
valid but physically irrelevant ones.
The Heated Ring
So far, we have not paid any attention to boundary conditions. As noted above, these will
eliminate nonphysical eigensolutions and thereby reduce the collection to a manageable,
albeit still inﬁnite, number. In this subsection, we will discuss a particularly important
case, which, following Fourier’s line of reasoning, leads us directly into the heart of Fourier
series.
Consider the heat equation on the interval −π ≤x ≤π, subject to the periodic
boundary conditions
∂u
∂t = ∂2u
∂x2 ,
u(t, −π) = u(t, π),
∂u
∂x (t, −π) = ∂u
∂x (t, π).
(3.21)
The physical problem being modeled is the thermodynamic behavior of an insulated circular
ring, in which x represents the angular coordinate. The boundary conditions ensure that
the temperature remains continuously diﬀerentiable at the junction point where the angle
switches over from −π to π. Given the ring’s initial temperature distribution
u(0, x) = f(x),
−π ≤x ≤π,
(3.22)
our task is to determine the temperature of the ring u(t, x) at each subsequent time t > 0.
Let us ﬁnd out which of the preceding eigensolutions respect the boundary conditions.
Substituting our exponential ansatz (3.18) into the diﬀerential equation and boundary
conditions (3.21), we ﬁnd that the eigenfunction v(x) must satisfy the periodic boundary
value problem
v′′ = λ v,
v(−π) = v(π),
v′(−π) = v′(π).
(3.23)
Our task is to ﬁnd those values of λ for which (3.23) has a nonzero solution v(x) ̸≡0.
These are the eigenvalues and eigenfunctions.
As noted above, there are three cases, depending on the sign of λ. First, suppose
λ = ω2 > 0. Then the general solution to the ordinary diﬀerential equation is
v(x) = aeω x + be−ω x,
where a, b are arbitrary constants. Substituting into the boundary conditions, we ﬁnd that
a, b must satisfy the pair of linear equations
ae−ω π + beω π = aeω π + be−ω π,
aωe−ω π −bωeω π = aωeω π −bωe−ω π.
Since ω ̸= 0, the ﬁrst equation implies that a = b, while the second requires a = −b. So,
the only way to satisfy both boundary conditions is to take a = b = 0, and so v(x) ≡0 is
a trivial solution. We conclude that there are no positive eigenvalues.
Second, if λ = 0, then the ordinary diﬀerential equation reduces to v′′ = 0, with
solution
v(x) = a + bx.

70
3 Fourier Series
Substituting into the boundary conditions requires
a −bπ = a + bπ,
b = b.
The ﬁrst equation implies that b = 0, but this is the only condition. Therefore, any constant
function, v(x) ≡a, solves the boundary value problem, and hence λ = 0 is an eigenvalue.
We take v0(x) ≡1 as the unique independent eigenfunction, bearing in mind that any
constant multiple of an eigenfunction is automatically also an eigenfunction. We will call
1 a null eigenfunction, indicating that it is associated with the zero eigenvalue λ = 0. The
corresponding eigensolution (3.18) is u(t, x) = e0t v0(x) = 1, a constant solution to the
heat equation.
Finally, we must deal with the case λ = −ω2 < 0. Now, the general solution to the
diﬀerential equation in (3.23) is a trigonometric function:
v(x) = a cos ωx + b sin ωx.
(3.24)
Since
v′(x) = −aω sin ωx + bω cos ωx,
when we substitute into the boundary conditions, we obtain
a cos ωπ −b sin ωπ = a cos ωπ + b sin ωπ,
a sin ωπ + b cos ωπ = −a sin ωπ + b cos ωπ,
where we canceled out a common factor of ω in the second equation. These simplify to
2b sin ωπ = 0,
2a sin ωπ = 0.
If sin ωπ ̸= 0, then a = b = 0, and so we have only the trivial solution v(x) ≡0. Thus, to
obtain a nonzero eigenfunction, we must have
sin ωπ = 0,
which requires that ω = 1, 2, 3, . . . be a positive integer. For such ωk = k, every solution
v(x) = a cos kx + b sin kx,
k = 1, 2, 3, . . . ,
satisﬁes both boundary conditions, and hence (unless identically zero) qualiﬁes as an eigen-
function of the boundary value problem. Thus, the eigenvalue λk = −k2 admits a two-
dimensional space of eigenfunctions, with basis vk(x) = cos kx and vk(x) = sin kx.
Consequently, the basic trigonometric functions
1,
cos x,
sin x,
cos 2x,
sin 2x,
cos 3x,
. . .
(3.25)
form a system of independent eigenfunctions for the periodic boundary value problem
(3.23). The corresponding exponentially varying eigensolutions are
uk(x) = e−k2 t cos kx,
uk(x) = e−k2 t sin kx,
k = 0, 1, 2, 3, . . .,
(3.26)
each of which, by design, is a solution to the heat equation (3.21) and satisﬁes the periodic
boundary conditions. Note that we subsumed the case λ0 = 0 into (3.26), keeping in mind
that, when k = 0, the sine function is trivial, and hence u0(x) ≡0 is not needed. So the null
eigenvalue λ0 = 0 provides (up to a constant multiple) only one eigensolution, whereas the
strictly negative eigenvalues λk = −k2 < 0 each provide two independent eigensolutions.

3.1 Eigensolutions of Linear Evolution Equations
71
Remark: For completeness, one should also consider the possibility of complex eigen-
values. If λ = ω2 ̸= 0, where ω is now allowed to be complex, then all solutions to the
diﬀerential equation (3.23) are of the form
v(x) = aeω x + be−ω x.
The periodic boundary conditions require
ae−ω π + beω π = aeω π + be−ω π,
aωe−ω π −bωeω π = aωeω π −bωe−ω π.
If eω π ̸= e−ω π, or, equivalently, e2ω π ̸= 1, then the ﬁrst condition implies a = b, but then
the second implies a = b = 0, and so λ = ω2 is not an eigenvalue. Thus, the eigenvalues
only occur when e2ω π = 1. This implies ω = k i , where k is an integer, and so λ = −k2,
leading back to the known trigonometric solutions. Later, in Section 9.5, we will learn that
the “self-adjoint” structure of the underlying boundary value problem implies, a priori, that
all its eigenvalues are necessarily real and nonpositive. So a good part of the preceding
analysis was, in fact, superﬂuous.
We conclude that there is an inﬁnite number of independent eigensolutions (3.26) to
the periodic heat equation (3.21). Linear Superposition, as described in Theorem 1.4, tells
us that any ﬁnite linear combination of the eigensolutions is automatically a solution to
the periodic heat equation. However, only solutions whose initial data u(0, x) = f(x) hap-
pens to be a ﬁnite linear combination of the trigonometric eigenfunctions (a trigonometric
polynomial) can be so represented. Fourier’s brilliant idea was to propose taking inﬁnite
“linear combinations” of the eigensolutions in an attempt to solve the general initial value
problem. Thus, we try representing a general solution to the periodic heat equation as an
inﬁnite series of the form†
u(t, x) = a0
2 +
∞

k=1

ak e−k2 t cos kx + bk e−k2 t sin kx

.
(3.27)
The coeﬃcients a0, a1, a2, . . . , b1, b2, . . . , are constants, to be ﬁxed by the initial condition.
Indeed, substituting our proposed solution formula (3.27) into (3.22), we obtain
f(x) = u(0, x) = a0
2 +
∞

k=1

ak cos kx + bk sin kx

.
(3.28)
Thus, we must represent the initial temperature distribution f(x) as an inﬁnite Fourier
series in the elementary trigonometric eigenfunctions. Once we have prescribed the Fourier
coeﬃcients a0, a1, a2, . . . , b1, b2, . . . , we expect that the corresponding eigensolution series
(3.27) will provide an explicit formula for the solution to the periodic initial-boundary
value problem for the heat equation.
However, inﬁnite series are much more delicate than ﬁnite sums, and so this formal
construction requires some serious mathematical analysis to place it on a rigorous founda-
tion. The key questions are:
• When does an inﬁnite trigonometric Fourier series converge?
• What kinds of functions f(x) can be represented by a convergent Fourier series?
†
For technical reasons, one takes the basic null eigenfunction to be 1
2 instead of 1. The reason
for this choice will be revealed in the following section.

72
3 Fourier Series
• Given such a function, how do we determine its Fourier coeﬃcients ak, bk?
• Are we allowed to diﬀerentiate a Fourier series?
• Does the result actually form a solution to the initial-boundary value problem for the
heat equation?
These are the basic issues in Fourier analysis, which must be properly addressed before we
can make any serious progress towards actually solving the heat equation. Thus, we will
leave partial diﬀerential equations aside for the time being, and start a detailed investigation
into the mathematics of Fourier series.
Exercises
3.1.1. For each of the following diﬀerential operators, (i) prove linearity; (ii) prove (3.5);
(iii) write down the corresponding linear evolution equation (3.2):
(a)
∂
∂x ,
(b)
∂
∂x + 1,
(c)
∂2
∂x2 + 3 ∂
∂x ,
(d)
∂
∂x ex ∂
∂x ,
(e)
∂2
∂x2 + ∂2
∂y2 .
3.1.2. Find all separable eigensolutions to the heat equation ut = uxx on the interval 0 ≤x ≤π
subject to (a) homogeneous Dirichlet boundary conditions u(t, 0) = 0, u(t, π) = 0;
(b) mixed boundary conditions u(t, 0) = 0, ux(t, π) = 0;
(c) Neumann boundary conditions ux(t, 0) = 0, ux(t, π) = 0.
♦3.1.3. Complete the table of eigensolutions to the heat equation, in the absence of boundary
conditions, by allowing the eigenvalue λ to be complex.
3.1.4. Find all separable eigensolutions to the following partial diﬀerential equations:
(a) ut = ux,
(b) ut = ux −u,
(c) ut = xux.
3.1.5.(a) Find the real eigensolutions to the damped heat equation ut = uxx −u. (b) Which
solutions satisfy the periodic boundary conditions u(t, −π) = u(t, π), ux(t, −π) = ux(t, π)?
3.1.6. Answer Exercise 3.1.5 for the diﬀusive transport equation ut + cux = uxx modeling the
combined diﬀusion and transport of a solute in a uniform ﬂow with constant wave speed c.
♥3.1.7.(a) Find the real eigensolutions to the diﬀusion equation ut = (x2 ux)x modeling diﬀusion
in an inhomogeneous medium on the half-line x > 0.
(b) Which solutions satisfy the Dirichlet boundary conditions u(t, 1) = u(t, 2) = 0?
3.2 Fourier Series
The preceding section served to motivate the development of Fourier series as a tool for
solving partial diﬀerential equations. Our immediate goal is to represent a given function
f(x) as a convergent series in the elementary trigonometric functions:
f(x) = a0
2
+
∞

k=1
[ ak cos kx + bk sin kx ] .
(3.29)
The ﬁrst order of business is to determine the formulae for the Fourier coeﬃcients ak, bk;
only then will we deal with convergence issues.

3.2 Fourier Series
73
The key that unlocks the Fourier treasure chest is orthogonality. Recall that two vec-
tors in Euclidean space are called orthogonal if they meet at a right angle. More explicitly,
v, w are orthogonal if and only if their dot product is zero: v · w = 0. Orthogonality,
and particularly orthogonal bases, has profound consequences that underpin many mod-
ern computational algorithms. See Section B.4 for the basics, and [89] for full details on
ﬁnite-dimensional developments. In inﬁnite-dimensional function space, were it not for or-
thogonality, Fourier theory would be vastly more complicated, if not completely impractical
for applications.
The starting point is the introduction of a suitable inner product on function space, to
assume the role played by the dot product in the ﬁnite-dimensional context. For classical
Fourier series, we use the rescaled L2 inner product
⟨f , g ⟩= 1
π
 π
−π
f(x) g(x) dx
(3.30)
on the space of continuous functions deﬁned on the interval† [−π, π ]. It is not hard to
show that (3.30) satisﬁes the basic inner product axioms listed in Deﬁnition B.10. The
associated norm is
∥f ∥=

⟨f , f ⟩=

1
π
 π
−π
f(x)2 dx .
(3.31)
Lemma 3.1. Under the rescaled L2 inner product (3.30), the trigonometric functions
1, cos x, sin x, cos 2x, sin 2x, . . . , satisfy the following orthogonality relations:
⟨cos kx , cos lx ⟩= ⟨sin kx , sin lx ⟩= 0,
⟨cos kx , sin lx ⟩= 0,
∥1 ∥=
√
2 ,
∥coskx ∥= ∥sin kx ∥= 1,
for
k ̸= l,
for all k, l,
for
k ̸= 0,
(3.32)
where k and l indicate nonnegative integers.
Proof : The formulas follow immediately from the elementary integration identities
 π
−π
cos kx cos lx dx =
⎧
⎨
⎩
0,
k ̸= l,
2π,
k = l = 0,
π,
k = l ̸= 0,
 π
−π
sin kx sin lx dx =

0,
k ̸= l,
π,
k = l ̸= 0,
 π
−π
cos kx sin lx dx = 0,
(3.33)
which are valid for all nonnegative integers k, l ≥0.
Q.E.D.
Lemma 3.1 implies that the elementary trigonometric functions form an orthogonal
system, meaning that any distinct pair are orthogonal under the chosen inner product. If
we were to replace the constant function 1 by
1
√
2, then the resulting functions would form
an orthonormal system meaning that, in addition, they all have norm 1. However, the
extra
√
2 is utterly annoying, and best omitted.
†
We have chosen to use the interval [−π, π ] for convenience. A common alternative is to
develop Fourier series on the interval [0, 2π ]. In fact, since the basic trigonometric functions are
2π–periodic, any interval of length 2π will serve equally well. Adapting Fourier series to other
intervals will be discussed in Section 3.4.

74
3 Fourier Series
Remark: As with all essential mathematical facts, the orthogonality of the trigonomet-
ric functions is not an accident, but indicates that something deeper is going on. Indeed,
orthogonality is a consequence of the fact that the trigonometric functions are the eigen-
functions for the “self-adjoint” boundary value problem (3.23), which is the function space
counterpart to the orthogonality of eigenvectors of symmetric matrices, cf. Theorem B.26.
The general framework will be developed in detail in Section 9.5, and then applied to the
more complicated systems of eigenfunctions we will encounter when dealing with higher-
dimensional partial diﬀerential equations.
If we ignore convergence issues, then the trigonometric orthogonality relations serve
to prescribe the Fourier coeﬃcients: Taking the inner product of both sides of (3.29) with
cos lx for l > 0, and invoking linearity of the inner product, yields
⟨f , cos lx ⟩= a0
2 ⟨1 , coslx ⟩+
∞

k=1
[ ak ⟨cos kx , cos lx ⟩+ bk ⟨sin kx , cos lx ⟩]
= al ⟨coslx , cos lx ⟩= al,
since, by the orthogonality relations (3.32), all terms but the lth vanish. This serves to pre-
scribe the Fourier coeﬃcient al. A similar manipulation with sin lx ﬁxes bl = ⟨f , sin lx ⟩,
while taking the inner product with the constant function 1 gives
⟨f , 1 ⟩= a0
2 ⟨1 , 1 ⟩+
∞

k=1
[ ak ⟨cos kx , 1 ⟩+ bk ⟨sin kx , 1 ⟩] = a0
2 ∥1 ∥2 = a0,
which agrees with the preceding formula for al when l = 0, and explains why we include
the extra factor
1
2 in the constant term.
Thus, if the Fourier series converges to the
function f(x), then its coeﬃcients are prescribed by taking inner products with the basic
trigonometric functions.
Deﬁnition 3.2. The Fourier series of a function f(x) deﬁned on −π ≤x ≤π is
f(x) ∼a0
2
+
∞

k=1
[ ak cos kx + bk sin kx ] ,
(3.34)
whose coeﬃcients are given by the inner product formulae
ak = ⟨f , cos kx ⟩= 1
π
 π
−π
f(x) coskx dx,
k = 0, 1, 2, 3, . . .,
bk = ⟨f , sin kx ⟩= 1
π
 π
−π
f(x) sin kx dx,
k = 1, 2, 3, . . ..
(3.35)
The function f(x) cannot be completely arbitrary, since, at the very least, the integrals
in the coeﬃcient formulae must be well deﬁned and ﬁnite. Even if the coeﬃcients (3.35)
are ﬁnite, there is no guarantee that the resulting inﬁnite series converges, and, even if it
converges, no guarantee that it converges to the original function f(x). For these reasons,
we will tend to use the ∼symbol instead of an equal sign when writing down a Fourier
series. Before tackling these critical issues, let us work through an elementary example.

3.2 Fourier Series
75
Example 3.3. Consider the function f(x) = x. We may compute its Fourier coeﬃ-
cients directly, employing integration by parts to evaluate the integrals:
a0 = 1
π
 π
−π
x dx = 0,
ak = 1
π
 π
−π
x cos kx dx = 1
π
 x sin kx
k
+ cos kx
k2
 
π
x=−π
= 0,
bk = 1
π
 π
−π
x sin kx dx = 1
π

−x cos kx
k
+ sin kx
k2
 
π
x=−π
= 2
k (−1)k+1 .
(3.36)
The resulting Fourier series is
x ∼2

sin x −sin 2x
2
+ sin 3x
3
−sin 4x
4
+ · · ·

.
(3.37)
Establishing convergence of this inﬁnite series is far from elementary. Standard calculus
criteria, including the ratio and root tests, are inconclusive. Even if we know that the series
converges (which it does — for all x), it is certainly not obvious what function it converges
to. Indeed, it cannot converge to the function f(x) = x everywhere! For instance, if x = π,
then every term in the Fourier series is zero, and so it converges to 0 — which is not the
same as f(π) = π.
Recall that the convergence of an inﬁnite series is predicated on the convergence of its
sequence of partial sums, which, in this case, are
sn(x) = a0
2 +
n

k=1
[ ak cos kx + bk sin kx ] .
(3.38)
By deﬁnition, the Fourier series converges at a point x if and only if its partial sums have
a limit:
lim
n →∞sn(x) = f(x),
(3.39)
which may or may not equal the value of the original function f(x). Thus, a key requirement
is to ﬁnd conditions on the function f(x) that guarantee that the Fourier series converges,
and, even more importantly, that the limiting sum reproduces the original function: f(x) =
f(x). This will all be done in detail below.
Remark: A ﬁnite Fourier sum, of the form (3.38), is also known as a trigonometric
polynomial. This is because, by trigonometric identities, it can be re-expressed as a poly-
nomial P(cos x, sin x) in the cosine and sine functions; vice versa, every such polynomial
can be uniquely written as such a sum; see [89] for details.
The passage from trigonometric polynomials to Fourier series might be viewed as
analogous to the passage from polynomials to power series. Recall that the Taylor series
of an inﬁnitely diﬀerentiable function f(x) at the point x = 0 is
f(x) ∼c0 + c1 x + · · · + cn xn + · · · =
∞

k=0
ck xk,
where, according to Taylor’s formula, the coeﬃcients ck = f (k)(0)
k!
are expressed in terms
of its derivatives at the origin, not by an inner product. The partial sums
sn(x) = c0 + c1 x + · · · + cn xn =
n

k=0
ck xk

76
3 Fourier Series
of a power series are ordinary polynomials, and the same basic convergence issues arise.
Although superﬁcially similar, in actuality the two theories are profoundly diﬀerent.
Indeed, while the theory of power series was well established in the early days of the
calculus, there remain, to this day, unresolved foundational issues in Fourier theory. A
power series in a real variable x either converges everywhere, or on an interval centered
at 0, or nowhere except at 0. On the other hand, a Fourier series can converge on quite
bizarre sets. Secondly, when a power series converges, it converges to an analytic function,
whose derivatives are represented by the diﬀerentiated power series. Fourier series may
converge, not only to continuous functions, but also to a wide variety of discontinuous
functions and even more general objects. Therefore, term-wise diﬀerentiation of a Fourier
series is a nontrivial issue.
Once one appreciates how radically diﬀerent the two subjects are, one begins to un-
derstand why Fourier’s astonishing claims were initially widely disbelieved. Before that
time, all functions were taken to be analytic. The fact that Fourier series might converge
to a nonanalytic, even discontinuous function was extremely disconcerting, resulting in a
profound re-evaluation of the foundations of function theory and the calculus, culminating
in the modern deﬁnitions of function and convergence that you now learn in your ﬁrst
courses in analysis, [8, 96, 97]. Only through the combined eﬀorts of many of the leading
mathematicians of the nineteenth century was a rigorous theory of Fourier series ﬁrmly
established. Section 3.5 contains the most important details, while more comprehensive
treatments can be found in the advanced texts [37, 68, 128].
Exercises
3.2.1. Find the Fourier series of the following functions:
(a) sign x,
(b) | x |,
(c) 3x −1,
(d) x2,
(e) sin3 x,
(f ) sin x cos x,
(g) | sin x |,
(h) x cos x.
3.2.2. Find the Fourier series of the following functions:
(a)

1,
| x | < 1
2 π,
0,
otherwise,
(b)

1,
1
2 π < | x | < π,
0,
otherwise,
(c)

1,
1
2 π < x < π,
0,
otherwise,
(d)

x,
| x | < 1
2 π,
0,
otherwise,
(e)

cos x,
| x | < 1
2 π,
0,
otherwise.
3.2.3. Find the Fourier series of sin2 x and cos2 x without directly calculating the Fourier coeﬃ-
cients. Hint: Use some standard trigonometric identities.
♦3.2.4. Let g(x) = 1
2 p0 +
n

k=1
(pk cos kx + qk sin kx) be a trigonometric polynomial. Explain why
its Fourier coeﬃcients are ak = pk and bk = qk for k ≤n, while ak = bk = 0 for k > n.
3.2.5. True or false: (a) The Fourier series for the function 2f(x) is obtained by multiplying
each term in the Fourier series for f(x) by 2. (b) The Fourier series for the function f(2x)
is obtained by replacing x by 2x in the Fourier series for f(x). (c) The Fourier coeﬃcients
of f(x) + g(x) can be found by adding the corresponding Fourier coeﬃcients of f(x) and
g(x). (d) The Fourier coeﬃcients of f(x) g(x) can be found by multiplying the correspond-
ing Fourier coeﬃcients of f(x) and g(x).

3.2 Fourier Series
77
−2π
−π
π
2π
3π
4π
5π
−π
π
Figure 3.1.
2π–periodic extension of x.
Periodic Extensions
The trigonometric constituents (3.25) of a Fourier series are all periodic functions of period
2π. Therefore, if the series converges, the limiting function f(x) must also be periodic of
period 2π:
f(x + 2π) = f(x)
for all
x ∈R.
A Fourier series can converge only to a 2π–periodic function. So it was unreasonable to
expect the Fourier series (3.37) to converge to the aperiodic function f(x) = x everywhere.
Rather, it should converge to its “periodic extension”, which we now deﬁne.
Lemma 3.4. If f(x) is any function deﬁned for −π < x ≤π, then there is a unique
2π–periodic function f, known as the 2π–periodic extension of f, that satisﬁes f(x) = f(x)
for all −π < x ≤π.
Proof : Pictorially, the graph of the periodic extension of a function f(x) is obtained
by repeatedly copying the part of its graph between −π and π to adjacent intervals of
length 2π; Figure 3.1 shows a simple example. More formally, given x ∈R, there is a
unique integer m such that (2m−1)π < x ≤(2m+1)π. Periodicity of f leads us to deﬁne
f(x) = f(x −2mπ) = f(x −2mπ).
(3.40)
In particular, if −π < x ≤π, then m = 0, and hence f(x) = f(x) for such x. The proof
that the resulting function f is 2π–periodic is left as Exercise 3.2.8.
Q.E.D.
Remark: The construction of the periodic extension in Lemma 3.4 uses the value f(π)
at the right endpoint and requires f(−π) = f(π) = f(π). One could, alternatively, require
f(π) = f(−π) = f(−π), which, if f(−π) ̸= f(π), leads to a slightly diﬀerent 2π–periodic
extension of the function. There is no a priori reason to prefer one over the other. In fact,
as we shall discover, the preferred Fourier periodic extension f(x) takes the average of the
two values:
f(π) = f(−π) = 1
2

f(π) + f(−π)

,
(3.41)
which then ﬁxes its values at the odd multiples of π.

78
3 Fourier Series
Example 3.5.
The 2π–periodic extension of f(x) = x is the “sawtooth” function
f(x) graphed in Figure 3.1. It agrees with x between −π and π. Since f(π) = π, f(−π) =
−π, the Fourier extension (3.41) sets f(kπ) = 0 for any odd integer k. Explicitly,
f(x) =
 x −2mπ,
(2m −1)π < x < (2m + 1)π,
0,
x = (2m −1)π,
where m is any integer.
With this convention, it can be proved that the Fourier series (3.37) converges everywhere
to the 2π–periodic extension f(x). In particular,
2
∞

k=1
(−1)k+1 sin kx
k
=
 x,
−π < x < π,
0,
x = ±π.
(3.42)
Even this very simple example has remarkable and nontrivial consequences. For in-
stance, if we substitute x = 1
2 π in (3.42) and divide by 2, we obtain Gregory’s series
π
4 = 1 −1
3 + 1
5 −1
7 + 1
9 −· · · .
(3.43)
While this striking formula predates Fourier theory — it was, in fact, ﬁrst discovered by
Leibniz — a direct proof is not easy.
Remark: While numerologically fascinating, Gregory’s series is of scant practical use
for actually computing π, since its rate of convergence is painfully slow. The reader may
wish to try adding up terms to see how far out one needs to go to accurately compute
even the ﬁrst two decimal digits of π. Round-oﬀerrors will eventually interfere with any
attempt to numerically compute the summation with any reasonable degree of accuracy.
Exercises
3.2.6. Graph the 2π–periodic extension of each of the following functions. Which extensions
are continuous? Diﬀerentiable?
(a) x2,
(b) (x2 −π2)2,
(c) ex,
(d) e−| x |,
(e) sinh x,
(f ) 1 + cos2 x;
(g) sin 1
2 πx,
(h) 1
x ,
(i)
1
1 + x2 .
3.2.7. Sketch a graph of the 2π–periodic extension of each of the functions in Exercise 3.2.2.
♦3.2.8. Complete the proof of Lemma 3.4 by showing that f(x) is 2π periodic.
♦3.2.9. Suppose f(x) is periodic with period ℓand integrable. Prove that, for any a,
(a)
	 a+ℓ
a
f(x) dx =
	 ℓ
0 f(x) dx,
(b)
	 ℓ
0 f(x + a) dx =
	 ℓ
0 f(x) dx.
♥3.2.10. Let f(x) be a suﬃciently nice 2π–periodic function. (a) Prove that f′(x) is 2π–periodic.
(b) Show that if f(x) has mean zero, so
	 π
−π f(x) dx = 0, then g(x) =
	 x
0 f(y) dy is 2π–
periodic; (c) Does the result in part (b) rely on the fact that the lower limit in the integral
for g(x) is 0? (d) More generally, prove that if f(x) has mean m =
1
2π
	 π
−π f(x) dx, then
the function g(x) =
	 x
0 f(y) dy −mx is 2π–periodic.

3.2 Fourier Series
79
Figure 3.2.
Piecewise continuous function.
♦3.2.11. Given a function f(x) deﬁned for 0 ≤x < ℓ, prove that there is a unique periodic
function of period ℓthat agrees with f on the interval [0, ℓ). If ℓ= 2π, is this the same
periodic extension as we constructed in the text? Explain your answer. Try the case f(x) =
x as an illustrative example.
3.2.12. Use the method in Exercise 3.2.11 to construct and graph the 1–periodic extensions of
the following functions:
(a) x2,
(b) e−x,
(c) cos π x,
(d)

1,
| x | < 1
2 π,
0,
otherwise.
♠3.2.13.(a) How many terms in Gregory’s series (3.43) are required to compute the ﬁrst two
decimal digits of π? (b) The ﬁrst 10 decimal digits? Hint: Use the fact that it is an al-
ternating series. (c) For part (a), try summing up the required number of terms on your
computer, and check whether you obtain an accurate result.
Piecewise Continuous Functions
As we shall see, all continuously diﬀerentiable 2π–periodic functions can be represented
as convergent Fourier series.
More generally, we can allow functions that have simple
discontinuities.
Deﬁnition 3.6.
A function f(x) is said to be piecewise continuous on an interval
[a, b] if it is deﬁned and continuous except possibly at a ﬁnite number of points a ≤x1 <
x2 < · · · < xn ≤b. Furthermore, at each point of discontinuity, we require that the left-
and right-hand limits
f(x−
k ) =
lim
x →x−
k
f(x),
f(x+
k ) =
lim
x →x+
k
f(x),
(3.44)
exist. (At the endpoints a, b, existence of only one of the limits, namely f(a+) and f(b−)
is required.) Note that we do not require that f(x) be deﬁned at xk. Even if f(xk) is
deﬁned, it does not necessarily equal either the left- or the right-hand limit.
A representative graph of a piecewise continuous function appears in Figure 3.2. The

80
3 Fourier Series
Figure 3.3.
The unit step function.
points xk are known as jump discontinuities of f(x), and the diﬀerence
βk = f(x+
k ) −f(x−
k ) =
lim
x →x+
k
f(x) −
lim
x →x−
k
f(x)
(3.45)
between the left- and right-hand limits is the magnitude of the jump. Note the value of
the function at the discontinuity, namely f(xk) — which may not even be deﬁned — plays
no role in the speciﬁcation of the jump magnitude. The jump magnitude is positive if
the function jumps up (when moving from left to right) at xk and negative if it jumps
down.
If the jump magnitude vanishes, βk = 0, the left- and right-hand limits agree,
and the discontinuity is removable, since redeﬁning f(xk) = f(x+
k ) = f(x−
k ) makes f(x)
continuous at x = xk. Since removable discontinuities have no eﬀect in either the theory
or applications, they can always be removed without penalty.
The simplest example of a piecewise continuous function is the unit step function
σ(x) =
 1,
x > 0,
0,
x < 0,
(3.46)
graphed in Figure 3.3. It has a single jump discontinuity at x = 0 of magnitude 1:
σ(0+) −σ(0−) = 1 −0 = 1,
and is continuous — indeed, locally constant — everywhere else. If we translate and scale
the step function, we obtain a function
h(x) = β σ(x −ξ) =
 β,
x > ξ,
0,
x < ξ,
(3.47)
with a single jump discontinuity of magnitude β at the point x = ξ.
If f(x) is any piecewise continuous function on [−π, π ], then its Fourier coeﬃcients
are well deﬁned — the integrals (3.35) exist and are ﬁnite. Continuity, however, is not
enough to ensure convergence of the associated Fourier series.
Deﬁnition 3.7.
A function f(x) is called piecewise C1 on an interval [a, b] if it is
deﬁned, continuous, and continuously diﬀerentiable except at a ﬁnite number of points

3.2 Fourier Series
81
Figure 3.4.
Piecewise C1 function.
a ≤x1 < x2 < · · · < xn ≤b. At each exceptional point, the left- and right-hand limits† of
both the function and its derivative exist:
f(x−
k ) = lim
x→x−
k
f(x),
f(x+
k ) = lim
x→x+
k
f(x),
f ′(x−
k ) = lim
x→x−
k
f ′(x),
f ′(x+
k ) = lim
x→x+
k
f ′(x).
See Figure 3.4 for a representative graph. For a piecewise C1 function, an exceptional
point xk is either
• a jump discontinuity where the left- and right-hand derivatives exist, or
• a corner, meaning a point where f is continuous, so f(x−
k ) = f(x+
k ), but has diﬀerent
left- and right-hand derivatives: f ′(x−
k ) ̸= f ′(x+
k ).
Thus, at each point, including jump discontinuities, the graph of f(x) has well-deﬁned
right and left tangent lines. For example, the function f(x) = | x | is piecewise C1, since it
is continuous everywhere and has a corner at x = 0, with f ′(0+) = +1, f ′(0−) = −1.
There is an analogous deﬁnition of piecewise Cn functions.
One requires that the
function have n continuous derivatives, except at a ﬁnite number of points. Moreover,
at every point, the function must have well deﬁned left- and right-hand limits of all its
derivatives up to order n.
Finally, a function f(x) deﬁned for all x ∈R is piecewise continuous (or C1 or Cn)
provided it is piecewise continuous (or C1 or Cn) on any bounded interval. Thus, a piecewise
continuous function on R can have an inﬁnite number of discontinuities, but they are not
allowed to accumulate at any ﬁnite limit point. In particular, a 2π–periodic function f(x)
is piecewise continuous if and only if it is piecewise continuous on the interval [−π, π ].
Exercises
3.2.14. Find the discontinuities and the jump magnitudes for the following piecewise continu-
ous functions:
†
As before, at the endpoints we require only the appropriate one-sided limits, namely f(a+),
f ′(a+), and f(b−), f ′(b−), to exist.

82
3 Fourier Series
(a) 2σ(x) + σ(x + 1) −3σ(x −1), (b) sign(x2 −2x), (c) σ(x2 −2x), (d) | x2 −2x |,
(e)

| x −2 | , (f ) σ(sin x), (g) sign(sin x), (h) | sin x |, (i) eσ(x), (j) σ(ex), (k) e| x−2 |.
3.2.15. Graph the following piecewise continuous functions. List all discontinuities and jump
magnitudes.
(a)

ex,
1 < | x | < 2,
0,
otherwise,
(b)

sin x,
0 < x < 1
2 π,
0,
otherwise,
(c)
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
sin x
x
,
0 < | x | < 2π,
1,
x = 0,
0,
otherwise,
(d)
 x
| x | ≤1,
x2,
| x | > 1,
(e)
⎧
⎪
⎨
⎪
⎩
x,
−1 < x < 0,
sin x,
0 < x < π,
0,
otherwise,
(f )
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
−1
x ,
| x | ≥1,
2
1 + x2 ,
| x | < 1.
3.2.16. Are the functions in Exercises 3.2.14 and 3.2.15 piecewise C1? If so, list all corners.
3.2.17. Prove that the nth order ramp function ρn(x −ξ) =
⎧
⎪
⎨
⎪
⎩
(x −ξ)n
n!
,
x > ξ,
0,
x < ξ,
is piecewise
Ck for any k ≥0.
3.2.18. Is x1/3 piecewise continuous? piecewise C1? piecewise C2?
3.2.19. Answer Exercise 3.2.18 for
(a)

| x |,
(b) 1
x ,
(c) e−1/| x |,
(d) x3 sin 1
x ,
(e) | x |3 ,
(f ) | x |3/2.
3.2.20.(a) Give an example of a function that is continuous but not piecewise C1.
(b) Give an example that is piecewise C1 but not piecewise C2.
3.2.21.(a) Prove that the sum f + g of two piecewise continuous functions is piecewise contin-
uous. (b) Where are the jump discontinuities of f + g? What are the jump magnitudes?
(c) Check your result by summing the functions in parts (a) and (b) of Exercise 3.2.14.
3.2.22. Give an example of two piecewise continuous (but not continuous) functions f, g whose
sum f + g is continuous. Can you characterize all such pairs of functions?
♦3.2.23.(a) Prove that if f(x) is piecewise continuous on [−π, π ], then its 2π–periodic extension
is piecewise continuous on all of R. Where are its jump discontinuities and what are their
magnitudes? (b) Similarly, prove that if f(x) is piecewise C1, then its periodic extension is
piecewise C1. Where are the corners?
3.2.24. True or false: (a) If f(x) is a piecewise continuous function, its absolute value | f(x) | is
piecewise continuous. If true, what are the jumps and their magnitudes?
(b) If f(x) is piecewise C1, then | f(x) | is piecewise C1. If true, what are the corners?
The Convergence Theorem
We are now able to state the fundamental convergence theorem for Fourier series. But we
will postpone a discussion of its proof until the end of Section 3.5.
Theorem 3.8. If f(x) is a 2π–periodic, piecewise C1 function, then, at any x ∈R,
its Fourier series converges to
f(x),
if f is continuous at x,
1
2
 f(x+) + f(x−)

,
if x is a jump discontinuity.

3.2 Fourier Series
83
Figure 3.5.
Splitting the diﬀerence.
Thus, the Fourier series converges, as expected, to f(x) at all points of continuity.
At discontinuities, it apparently can’t decide whether to converge to the left- or right-
hand limit, and so ends up “splitting the diﬀerence” by converging to their average; see
Figure 3.5. If we redeﬁne f(x) at its jump discontinuities to have the average limiting
value, so
f(x) = 1
2
 f(x+) + f(x−)

(3.48)
— an equation that automatically holds at all points of continuity — then Theorem 3.8
would say that the Fourier series converges to the 2π–periodic piecewise C1 function f(x)
everywhere.
Example 3.9. Let σ(x) denote the unit step function (3.46). Its Fourier coeﬃcients
are easily computed:
a0 = 1
π
 π
−π
σ(x) dx = 1
π
 π
0
dx = 1,
ak = 1
π
 π
−π
σ(x) coskx dx = 1
π
 π
0
cos kx dx = 0,
bk = 1
π
 π
−π
σ(x) sin kx dx = 1
π
 π
0
sin kx dx =
⎧
⎨
⎩
2
kπ ,
k = 2l + 1 odd,
0,
k = 2l even.
Therefore, the Fourier series for the step function is
σ(x) ∼1
2 + 2
π

sin x + sin 3x
3
+ sin 5x
5
+ sin 7x
7
+ · · ·

.
(3.49)
According to Theorem 3.8, the Fourier series will converge to its 2π–periodic extension,
σ(x) =
⎧
⎪
⎨
⎪
⎩
0,
(2m −1)π < x < 2mπ,
1,
2mπ < x < (2m + 1)π,
1
2,
x = mπ,
where m is any integer,
which is plotted in Figure 3.6. Observe that, in accordance with Theorem 3.8, σ(x) takes
the midpoint value 1
2 at the jump discontinuities 0, ±π, ±2π, . . . .

84
3 Fourier Series
−π
π
2π
3π
4π
1
2
1
Figure 3.6.
2π–periodic step function.
Figure 3.7.
Gibbs phenomenon.
It is instructive to investigate the convergence of this particular Fourier series in some
detail.
Figure 3.7 displays a graph of the ﬁrst few partial sums, taking, respectively,
n = 4, 10, and 20 terms. The reader will notice that away from the discontinuities, the
series indeed appears to be converging, albeit slowly. However, near the jumps there is a
consistent overshoot of about 9% of the jump magnitude. The region where the overshoot
occurs becomes narrower and narrower as the number of terms increases, but the actual
amount of overshoot persists no matter how many terms are summed up. This was ﬁrst
noted by the American physicist Josiah Gibbs, and is now known as the Gibbs phenomenon
in his honor. The Gibbs overshoot is a manifestation of the subtle nonuniform convergence
of the Fourier series.
Exercises
3.2.25.(a) Sketch the 2π–periodic half-wave f(x) =
 sin x,
0 < x ≤π,
0,
−π ≤x < 0.
(b) Find its
Fourier series. (c) Graph the ﬁrst ﬁve Fourier sums and compare with the function.
(d) Discuss convergence of the Fourier series.
3.2.26. Answer Exercise 3.2.25 for the cosine half-wave f(x) =
 cos x,
0 < x ≤π,
0,
−π ≤x < 0.
3.2.27.(a) Find the Fourier series for f(x) = ex. (b) For which values of x does the Fourier
series converge? Is the convergence uniform? (c) Graph the function it converges to.
♠3.2.28.(a) Use a graphing package to investigate the Gibbs phenomenon for the Fourier series
(3.37) of the function x. Determine the amount of overshoot of the partial sums at the dis-
continuities.
(b) How many terms do you need to approximate the function to within two
decimal places at x = 2.0? At x = 3.0?

3.2 Fourier Series
85
3.2.29. Use the Fourier series (3.49) for the step function to rederive Gregory’s series (3.43).
♦3.2.30. Suppose ak, bk are the Fourier coeﬃcients of the function f(x). (a) To which function
does the Fourier series a0
2
+
∞

k=1
[ ak cos 2kx + bk sin 2kx ] converge? Hint: The answer is
not f(2x). (b) Test your answer with the Fourier series (3.37) for f(x) = x.
Even and Odd Functions
We already noted that the Fourier cosine coeﬃcients of the function f(x) = x are all 0.
This is not an accident, but, rather, a consequence of the fact that x is an odd function.
Recall ﬁrst the basic deﬁnition:
Deﬁnition 3.10. A function is called even if f(−x) = f(x). A function is called odd
if f(−x) = −f(x).
For example, the functions 1, cos kx, and x2 are all even, whereas x, sin kx, and sign x
are odd. Note that an odd function necessarily has f(0) = 0. We require three elementary
lemmas, whose proofs are left to the reader.
Lemma 3.11. The sum, f(x) + g(x), of two even functions is even; the sum of two
odd functions is odd.
Remark: Every function can be represented as the sum of an even and an odd function;
see Exercise 3.2.32.
Lemma 3.12. The product f(x) g(x) of two even functions, or of two odd functions,
is an even function. The product of an even and an odd function is odd.
Lemma 3.13. If f(x) is odd and integrable on the symmetric interval [−a, a], then
 a
−a
f(x) dx = 0. If f(x) is even and integrable, then
 a
−a
f(x) dx = 2
 a
0
f(x) dx.
The next result is an immediate consequence of applying Lemmas 3.12 and 3.13 to the
Fourier integrals (3.35).
Proposition 3.14. If f(x) is even, then its Fourier sine coeﬃcients all vanish, bk = 0,
and so f(x) can be represented by a Fourier cosine series
f(x) ∼a0
2
+
∞

k=1
ak cos kx ,
(3.50)
where
ak = 2
π
 π
0
f(x) coskx dx,
k = 0, 1, 2, 3, . . . .
(3.51)
If f(x) is odd, then its Fourier cosine coeﬃcients vanish, ak = 0, and so f(x) can be
represented by a Fourier sine series
f(x) ∼
∞

k=1
bk sin kx ,
(3.52)

86
3 Fourier Series
−2π
−π
π
2π
3π
4π
5π
π
Figure 3.8.
2π–periodic extension of | x |.
where
bk = 2
π
 π
0
f(x) sin kx dx,
k = 1, 2, 3, . . . .
(3.53)
Conversely, a convergent Fourier cosine series always represents an even function, while a
convergent sine series always represents an odd function.
Example 3.15. The absolute value f(x) = | x | is an even function, and hence has a
Fourier cosine series. The coeﬃcients are
a0 = 2
π
 π
0
x dx = π,
(3.54)
ak = 2
π
 π
0
x cos kx dx = 2
π
 x sin kx
k
+ cos kx
k2
π
x=0
=
⎧
⎨
⎩
0,
0 ̸= k even,
−
4
k2 π ,
k odd.
Therefore
| x | ∼π
2 −4
π

cos x + cos 3x
9
+ cos 5x
25
+ cos 7x
49
+ · · ·

.
(3.55)
According to Theorem 3.8, this Fourier cosine series converges to the 2π–periodic extension
of | x |, the “sawtooth function” graphed in Figure 3.8.
In particular, if we substitute x = 0, we obtain another interesting series:
π2
8 = 1 + 1
9 +
1
25 +
1
49 + · · · =
∞

j =0
1
(2j + 1)2 .
(3.56)
It converges faster than Gregory’s series (3.43), and, while far from optimal in this regard,
can be used to compute reasonable approximations to π. One can further manipulate this
result to compute the sum of the series
S =
∞

k=1
1
k2 = 1 + 1
4 + 1
9 +
1
16 +
1
25 +
1
36 +
1
49 + · · · .
We note that
S
4 =
∞

k=1
1
4k2 =
∞

k=1
1
(2k)2 = 1
4 +
1
16 +
1
36 +
1
64 + · · · .
Therefore, by (3.56),
3
4 S = S −S
4 = 1 + 1
9 +
1
25 +
1
49 + · · · = π2
8 ,

3.2 Fourier Series
87
from which we conclude that
S =
∞

k=1
1
k2 = 1 + 1
4 + 1
9 +
1
16 +
1
25 + · · · = π2
6 .
(3.57)
Remark: The most famous function in number theory — and the source of the most
outstanding problem in mathematics, the Riemann hypothesis — is the Riemann zeta
function
ζ(s) =
∞

k=1
1
ks .
(3.58)
Formula (3.57) shows that ζ(2) = 1
6 π2. In fact, the value of the zeta function at any even
positive integer s = 2j is a rational polynomial in π, [9]. Because of its importance to the
study of prime numbers, locating all the complex zeros of the zeta function will earn you
$1,000,000 — see http://www.claymath.org for details.
Any function f(x) deﬁned on [0, π ] has a unique even extension to [−π, π ], obtained
by setting f(−x) = f(x) for −π ≤x < 0, and also a unique odd extension, where now
f(−x) = −f(x) and f(0) = 0. These in turn can be periodically extended to the entire real
line. The Fourier cosine series of f(x) is deﬁned by the formulas (3.50–51), and represents
the even, 2π–periodic extension. Similarly, the formulas (3.52–53) deﬁne the Fourier sine
series of f(x), representing its odd, 2π–periodic extension.
Example 3.16. Suppose f(x) = sin x. Its Fourier cosine series has coeﬃcients
ak = 2
π
 π
0
sin x coskx dx =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
2
π ,
k = 0,
0,
k odd,
−
4
(k2 −1)π ,
0 < k even.
The resulting cosine series represents the even, 2π–periodic extension of sin x, namely
| sin x | ∼2
π −4
π
∞

j =1
cos 2j x
4j2 −1 .
On the other hand, f(x) = sin x is already odd, and so its Fourier sine series coincides with
its ordinary Fourier series, namely sin x, all the other Fourier sine coeﬃcients being zero;
in other words, b1 = 1, while bk = 0 for k > 1.
Exercises
3.2.31. Are the following functions even, odd, or neither?
(a) x2,
(b) ex,
(c) sinh x,
(d) sin πx,
(e) 1
x ,
(f )
1
1 + x2 ,
(g) tan−1 x.
♦3.2.32. Prove that (a) the sum of two even functions is even; (b) the sum of two odd functions
is odd; (c) every function is the sum of an even and an odd function.

88
3 Fourier Series
♦3.2.33. Prove (a) Lemma 3.12; (b) Lemma 3.13.
3.2.34. If f(x) is odd, is f′(x) (i) even? (ii) odd? (iii) neither? (iv) could be either?
3.2.35. If f′(x) is even, is f(x) (i) even? (ii) odd? (iii) neither? (iv) could be either? How
do you reconcile your answer with Exercise 3.2.34?
3.2.36. Answer Exercise 3.2.34 for f′′(x).
3.2.37. True or false: (a) If f(x) is odd, its 2π–periodic extension is odd.
(b) If the 2π–periodic extension of f(x) is odd, then f(x) is odd.
3.2.38. Let f(x) denote the odd, 2π–periodic Fourier extension of a function f(x) deﬁned on
[0, π ]. Explain why f(kπ) = 0 for any integer k.
3.2.39. Construct and graph the even and odd 2π–periodic extensions of the function f(x) =
1 −x. What are their Fourier series? Discuss convergence of each.
3.2.40. Find the Fourier series and discuss convergence for:
(a) the box function
b(x) =
⎧
⎨
⎩
1,
| x | < 1
2 π,
0,
1
2 π < | x | < π,
(b) the hat function h(x) =
 1 −| x |,
| x | < 1,
0,
1 < | x | < π.
3.2.41. Find the Fourier sine and cosine series of the following functions. Then graph the func-
tion to which the series converges.
(a) 1, (b) cos x, (c) sin3 x, (d) x(π −x).
3.2.42. Find the Fourier series of the hyperbolic functions cosh mx and sinh mx.
3.2.43.(a) Find the Fourier cosine series of the function | sin x |.
(b) Use the series to evaluate the sums
∞

k=1
(4k2 −1)−1 and
∞

k=1
(−1)k−1(4k2 −1)−1.
3.2.44. True or false: The sum of the Fourier cosine series and the Fourier sine series of the
function f(x) is the Fourier series for f(x). If false, what function is represented by the
combined Fourier series?
3.2.45.(a) Show that if a function is periodic of period π, then its Fourier series contains only
even terms, i.e., ak = bk = 0 whenever k = 2j + 1 is odd. (b) What if the period is 1
2 π?
3.2.46. Under what conditions on f(x) does its Fourier sine series contain only even terms, i.e.,
its Fourier sine coeﬃcients bk = 0 whenever k is odd?
♠3.2.47. Graph the partial sums s3(x), s5(x), s10(x) of the Fourier series (3.55). Do you notice a
Gibbs phenomenon? If so, what is the amount of overshoot?
3.2.48. Explain why, in the case of the step function σ(x), all its Fourier cosine coeﬃcients van-
ish, ak = 0, except for a0 = 1.
♠3.2.49. How many terms do you need to sum in (3.56) to correctly approximate π to two deci-
mal digits? To ten digits?
3.2.50. Prove that
∞

k=1
(−1)k−1
k2
= 1 −1
4 + 1
9 −1
16 + 1
25 −1
36 + 1
49 −· · · = π2
12 .
Complex Fourier Series
An alternative, and often more convenient, approach to Fourier series is to use complex
exponentials instead of sines and cosines. Indeed, Euler’s formula
e i kx = cos kx + i sin kx,
e−i kx = cos kx −i sin kx,
(3.59)

3.2 Fourier Series
89
shows how to write the trigonometric functions
cos kx = e i kx + e−i kx
2
,
sin kx = e i kx −e−i kx
2 i
,
(3.60)
in terms of complex exponentials, and so we can easily go back and forth between the two
representations.
Like their trigonometric antecedents, complex exponentials are also endowed with an
underlying orthogonality. But here, since we are dealing with the vector space of complex-
valued functions on the interval [−π, π ], we need to use the rescaled L2 Hermitian inner
product
⟨f , g ⟩= 1
2π
 π
−π
f(x) g(x) dx ,
(3.61)
in which the second function acquires a complex conjugate, as indicated by the overbar.
This is needed to ensure that the associated L2 Hermitian norm
∥f ∥=

1
2π
 π
−π
| f(x) |2 dx
(3.62)
is real and positive for all nonzero complex functions: ∥f ∥> 0 when f ̸≡0. Orthonor-
mality of the complex exponentials is proved by direct computation:
⟨e i kx , e i lx ⟩= 1
2π
 π
−π
e i (k−l)x dx =

1,
k = l,
0,
k ̸= l,
∥e i kx ∥2 = 1
2π
 π
−π
| e i kx |2 dx = 1.
(3.63)
The complex Fourier series for a (piecewise continuous) real or complex function f is
the doubly inﬁnite series
f(x) ∼
∞

k=−∞
ck e i kx = · · · + c−2 e−2 i x + c−1 e−i x + c0 + c1 e i x + c2 e2 i x + · · · . (3.64)
The orthonormality formulae (3.63) imply that the complex Fourier coeﬃcients are ob-
tained by taking the inner products
ck = ⟨f , e i kx ⟩= 1
2π
 π
−π
f(x) e−i kx dx.
(3.65)
Pay particular attention to the minus sign appearing in the integrated exponential, which
happens because the second argument in the Hermitian inner product (3.61) requires a
complex conjugate.
It must be emphasized that the real (3.34) and complex (3.64) Fourier formulae are just
two diﬀerent ways of writing the same series! Indeed, if we substitute Euler’s formula (3.59)
into (3.65) and compare the result with the real Fourier formulae (3.35), we ﬁnd that the
real and complex Fourier coeﬃcients are related by
ak = ck + c−k,
bk = i (ck −c−k),
ck = 1
2(ak −i bk),
c−k = 1
2(ak + i bk),
k = 0, 1, 2, . . . .
(3.66)

90
3 Fourier Series
−π
π
3π
5π
cosh π
eπ
Figure 3.9.
2π–periodic extension of ex.
Remark: We already see one advantage of the complex version. The constant function
1 = e0 i x no longer plays an anomalous role — the annoying factor of 1
2 in the real Fourier
series (3.34) has mysteriously disappeared!
Example 3.17.
For the unit step function σ(x) considered in Example 3.9, the
complex Fourier coeﬃcients are
ck = 1
2π
 π
−π
σ(x) e−i kx dx = 1
2π
 π
0
e−i kx dx =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
1
2,
k = 0,
0,
0 ̸= k even,
1
i k π ,
k odd.
Therefore, the step function has the complex Fourier series
σ(x) ∼1
2 −
i
π
∞

l=−∞
e(2l+1) i x
2l + 1
.
(3.67)
You should convince yourself that this is exactly the same series as the real Fourier series
(3.49). We are merely rewriting it using complex exponentials instead of real sines and
cosines.
Example 3.18. Let us ﬁnd the Fourier series for the exponential function eax. It is
much easier to evaluate the integrals for the complex Fourier coeﬃcients, and so
ck = ⟨eax , e i kx ⟩= 1
2π
 π
−π
e(a−i k)x dx =
e(a−i k)x
2π(a −i k)

π
x=−π
= e(a−i k)π −e−(a−i k)π
2π(a −i k)
= (−1)k eaπ −e−aπ
2π(a −i k) = (−1)k(a + i k) sinh aπ
π(a2 + k2)
.
Therefore, the desired Fourier series is
eax ∼sinh aπ
π
∞

k=−∞
(−1)k(a + i k)
a2 + k2
e i kx.
(3.68)

91
As an exercise, the reader should try writing this as a real Fourier series, either by breaking
up the complex series into its real and imaginary parts, or by direct evaluation of the real
coeﬃcients via their integral formulae (3.35). According to Theorem 3.8 (which is equally
valid for complex Fourier series), the Fourier series converges to the 2π–periodic extension
of the exponential function, as graphed in Figure 3.9.
In particular, its values at odd
multiples of π is the average of the limiting values there, namely cosh aπ = 1
2(eaπ +e−aπ).
Exercises
3.2.51. Find the complex Fourier series of the following functions:
(a) sin x, (b) sin3 x,
(c) x, (d) | x |, (e) | sin x |, (f ) sign x, (g) the ramp function ρ(x) =
 x,
x ≥0,
0,
x ≤0.
3.2.52. Let −π < ξ < π. Determine the complex Fourier series for the shifted step function
σ(x −ξ), and graph the function it converges to.
3.2.53. Let a ∈R. Find the real form of the Fourier series for the exponential function eax:
(a) by breaking up the complex series (3.68) into its real and imaginary parts;
(b) by direct evaluation of the real coeﬃcients via their integral formulae (3.35).
Make sure that your results agree!
3.2.54. Prove that coth π = 1
π + 2
π

1
1 + 12 +
1
1 + 22 +
1
1 + 32 + · · ·

, where
coth x = cosh x
sinh x = ex + e−x
ex −e−x
is the hyperbolic cotangent function.
3.2.55.(a) Find the complex Fourier series for xe i x.
(b) Use your result to write down the real Fourier series for x cos x and x sin x.
♦3.2.56. Prove that if f(x) =
n

k=m
rk e i kx is a complex trigonometric polynomial, with
−∞< m ≤n < ∞, then its Fourier coeﬃcients are ck =
 rk,
m ≤k ≤n,
0,
otherwise.
3.2.57. True or false: If the complex function f(x) = g(x) + i h(x) has Fourier coeﬃcients ck,
then g(x) = Re f(x) and h(x) = Im f(x) have, respectively, complex Fourier coeﬃcients
Re ck and Im ck.
♦3.2.58. Let f(x) be 2π–periodic. Explain how to construct the complex Fourier series for
f(x −a) from that of f(x).
♦3.2.59.(a) Show that if ck are the complex Fourier coeﬃcients for f(x), then the Fourier coef-
ﬁcients of f(x) = f(x) e i x are ck = ck−1. (b) Let m be an integer. Which function has
complex Fourier coeﬃcients ck = ck+m? (c) If ak, bk are the Fourier coeﬃcients of the real
function f(x), what are the Fourier coeﬃcients of f(x) cos x and f(x) sin x?
♦3.2.60. Can you recognize whether a function is real by looking at its complex Fourier coeﬃ-
cients?
♦3.2.61. Can you characterize the complex Fourier coeﬃcients of an even function?
an odd function?
♦3.2.62. What does it mean for a doubly inﬁnite series
∞

k=−∞
ck to converge? Be precise!
3.2 Fourier Series

92
3 Fourier Series
3.3 Diﬀerentiation and Integration
Under appropriate hypotheses, if a series of functions converges, then one will be able
to integrate or diﬀerentiate it term by term, and the resulting series should converge to
the integral or derivative of the original sum. For example, integration and diﬀerentiation
of power series is always valid within the range of convergence, and is used extensively
in the construction of series solutions of diﬀerential equations, series for integrals of non-
elementary functions, and so on. (See Section 11.3 for further details.) The convergence
of Fourier series is considerably more delicate, and so one must exercise due care when
diﬀerentiating or integrating. Nevertheless, in favorable situations, both operations lead to
valid results, and are quite useful for constructing Fourier series of more intricate functions.
Integration of Fourier Series
Integration is a smoothing operation — the integrated function is always nicer than the
original. Therefore, we should anticipate being able to integrate Fourier series without
diﬃculty. There is, however, one complication: the integral of a periodic function is not
necessarily periodic. The simplest example is the constant function 1, which is certainly
periodic, but its integral, namely x, is not. On the other hand, integrals of all the other
periodic sine and cosine functions appearing in the Fourier series are periodic. Thus, only
the constant term
a0
2 = 1
2π
 π
−π
f(x) dx
(3.69)
might cause us diﬃculty when we try to integrate a Fourier series (3.34). Note that (3.69)
is the mean, or average, of the function f(x) over the interval [−π, π ], and so a function
has no constant term in its Fourier series, i.e., a0 = 0, if and only if it has mean zero. It
is easily shown, cf. Exercise 3.2.10, that the mean-zero functions are precisely those that
remain periodic upon integration. In particular, Lemma 3.13 implies that all odd functions
automatically have mean zero, and hence have periodic integrals.
Lemma 3.19.
If f(x) is 2π–periodic, then its integral g(x) =
 x
0
f(y) dy is 2π–
periodic if and only if
 π
−π
f(x) dx = 0, so that f has mean zero on the interval [−π, π ].
In view of the elementary integration formulae

cos kx dx = sin kx
k
,

sin kx dx = −cos kx
k
,
(3.70)
termwise integration of a Fourier series without constant term is straightforward.
Theorem 3.20.
If f is piecewise continuous and has mean zero on the interval
[−π, π ], then its Fourier series
f(x) ∼
∞

k=1
[ ak cos kx + bk sin kx ]
can be integrated term by term, to produce the Fourier series
g(x) =
 x
0
f(y) dy ∼m +
∞

k=1

−bk
k cos kx + ak
k sin kx

.
(3.71)

3.3 Diﬀerentiation and Integration
93
The constant term
m = 1
2π
 π
−π
g(x) dx
(3.72)
is the mean of the integrated function.
Example 3.21. The function f(x) = x is odd, and so has mean zero:
 π
−π
x dx = 0.
Let us integrate its Fourier series
x ∼2
∞

k=1
(−1)k−1
k
sin kx,
(3.73)
which we found in Example 3.3. The result is the Fourier series
1
2 x2 ∼π2
6
−2
∞

k=1
(−1)k−1
k2
cos kx
= π2
6
−2

cos x −cos 2x
4
+ cos 3x
9
−cos 4x
16
+ · · ·

,
(3.74)
whose constant term is the mean of the left-hand side:
1
2π
 π
−π
x2
2 dx = π2
6 .
Let us revisit the derivation of the integrated Fourier series from a slightly diﬀerent
standpoint. If we were to integrate each trigonometric summand in a Fourier series (3.34)
from 0 to x, we would obtain
 x
0
cos ky dy = sin kx
k
,
whereas
 x
0
sin ky dy = 1
k −cos kx
k
.
The extra 1/k terms coming from the deﬁnite sine integrals did not appear explicitly in
our previous expression for the integrated Fourier series, (3.71), and so must be hidden in
the constant term m. We deduce that the mean value of the integrated function can be
computed using the Fourier sine coeﬃcients of f via the formula
1
2π
 π
−π
g(x) dx = m =
∞

k=1
bk
k .
(3.75)
For example, integrating both sides of the Fourier series (3.73) for f(x) = x from 0 to x
produces
x2
2
∼2
∞

k=1
(−1)k−1
k2
(1 −cos kx).
The constant terms sum to yield the mean value of the integrated function:
2

1 −1
4 + 1
9 −1
16 + · · ·

= 2
∞

k=1
(−1)k−1
k2
=
1
2π
 π
−π
x2
2 dx = π2
6 ,
(3.76)
which reproduces a formula established in Exercise 3.2.50.

94
3 Fourier Series
More generally, if f(x) does not have mean zero, its Fourier series contains a nonzero
constant term,
f(x) ∼a0
2 +
∞

k=1
[ ak cos kx + bk sin kx ] .
In this case, the result of integration will be
g(x) =
 x
0
f(y) dy ∼a0
2 x + m +
∞

k=1

−bk
k cos kx + ak
k sin kx

,
(3.77)
where m is given in (3.75). The right-hand side is not, strictly speaking, a Fourier series.
There are two ways to interpret this formula within the Fourier framework. We can write
(3.77) as the Fourier series for the diﬀerence
g(x) −a0
2 x ∼m +
∞

k=1

−bk
k cos kx + ak
k sin kx

,
(3.78)
which, by Exercise 3.2.10(d), is a 2π–periodic function. Alternatively, we can replace x
by its Fourier series (3.37), and the result will be the Fourier series for the 2π–periodic
extension of the integral g(x) =
 x
0
f(y) dy.
Diﬀerentiation of Fourier Series
Diﬀerentiation has the opposite eﬀect — it makes a function worse. Therefore, to justify
taking the derivative of a Fourier series, we need to know that the derived function remains
reasonably nice. Since we need the derivative f ′(x) to be piecewise C1 for the Convergence
Theorem 3.8 to be applicable, we require that f(x) itself be continuous and piecewise C2.
Theorem 3.22.
If f(x) has a piecewise C2 and continuous 2π–periodic extension,
then its Fourier series can be diﬀerentiated term by term, to produce the Fourier series for
its derivative
f ′(x) ∼
∞

k=1

k bk cos kx −k ak sin kx

=
∞

k=−∞
i k ck e i kx.
(3.79)
Example 3.23.
The derivative (6.31) of the absolute value function f(x) = | x | is
the sign function:
d
dx | x | = sign x =
 +1,
x > 0,
−1,
x < 0.
(3.80)
Therefore, if we diﬀerentiate its Fourier series (3.55), we obtain the Fourier series
sign x ∼4
π

sin x + sin 3x
3
+ sin 5x
5
+ sin 7x
7
+ · · ·

.
(3.81)
Note that sign x = σ(x)−σ(−x) is the diﬀerence of two step functions. Indeed, subtracting
the step function Fourier series (3.49) at x from the same series at −x reproduces (3.81).

3.4 Change of Scale
95
Exercises
3.3.1. Starting with the Fourier series (3.49) for the step function σ(x), use integration to:
(a) Find the Fourier series for the ramp function ρ(x) =
 x,
x > 0,
0,
x < 0.
(b) Then, ﬁnd the Fourier series for the second-order ramp function ρ2(x) =

1
2 x2,
x > 0,
0,
x < 0.
3.3.2. Find the Fourier series for the function f(x) = x3. If you diﬀerentiate your series, do you
recover the Fourier series for f′(x) = 3x2? If not, explain why not.
3.3.3. Answer Exercise 3.3.2 when f(x) = x4.
3.3.4. Use Theorem 3.20 to construct the Fourier series for (a) x3, (b) x4.
3.3.5. Write down the identities obtained by substituting x = 0, 1
2 π , and 1
3 π in the Fourier
series (3.74).
♦3.3.6. Suppose f(x) is a 2π–periodic function with complex Fourier coeﬃcients ck, and g(x)
is a 2π–periodic function with complex Fourier coeﬃcients dk. (a) Find the Fourier coeﬃ-
cients ek of their periodic convolution f(x) ∗g(x) =
	 π
−π f(x −y) g(y) dy.
(b) Find the complex Fourier series for the periodic convolution of cos 3x and sin 2x.
(c) Answer part (b) for the functions x and sin 2x.
♦3.3.7. Suppose f is piecewise continuous on [−π, π ]. Prove that the mean of the integrated
function g(x) =
	 x
0 f(y) dy equals 1
2
	 π
−π

sign x −x
π

f(x) dx.
3.3.8. Suppose the 2π–periodic extension of f(x) is continuous and piecewise C1. Prove di-
rectly from the formulas (3.35) that the Fourier coeﬃcients of its derivative f(x) = f′(x)
are, respectively, ak = kbk and bk = −kak, where ak, bk are the Fourier coeﬃcients of f(x).
♦3.3.9. Explain how to integrate a complex Fourier series (3.64). Under what conditions is your
formula valid?
♥3.3.10. The initial value problem d2u
dt2 + u = f(t), u(0) = 0,
du
dt (0) = 0, describes the forced
motion of an initially motionless unit mass attached to a unit spring.
(a) Solve the initial value problem when f(t) = cos kt and f(t) = sin kt for k = 0, 1, . . . .
(b) Assuming that the forcing function f(t) is 2π–periodic, write out its Fourier series, and
then use your result from part (b) to write out a series for the solution u(t).
(c) Under what conditions is the result a convergent Fourier series, and hence the solution
u(t) remains 2π–periodic?
(d) Explain why f(t) induces a resonance of the mass-spring system if and only if its Fourier
coeﬃcients of order 1 are not both zero: a2
1 + b2
1 ̸= 0.
3.4 Change of Scale
So far, we have dealt only with Fourier series on the standard interval of length 2π. We
chose [−π, π ] for convenience, but all of the results and formulas are easily adapted to any
other interval of the same length, e.g., [0, 2π ]. However, since physical objects like bars

96
3 Fourier Series
and strings do not all come in this particular length, we need to understand how to adapt
the formulas to more general intervals.
Any symmetric interval [−ℓ, ℓ] of length 2ℓcan be rescaled (stretched) to the standard
interval [−π, π ] through the linear change of variables
x = ℓ
π y,
so that
−π ≤y ≤π
whenever
−ℓ≤x ≤ℓ.
(3.82)
Given a function f(x) deﬁned on [−ℓ, ℓ], the rescaled function F(y) = f
 ℓ
π y

lives on
[−π, π ]. Let
F(y) ∼a0
2
+
∞

k=1

ak cos ky + bk sin ky

be the standard Fourier series for F(y), so that
ak = 1
π
 π
−π
F(y) cos ky dy,
bk = 1
π
 π
−π
F(y) sin ky dy.
(3.83)
Then, reverting to the unscaled variable x, we deduce that
f(x) ∼a0
2
+
∞

k=1

ak cos kπx
ℓ
+ bk sin kπx
ℓ

(3.84)
is the Fourier series of f(x) on the interval [−ℓ, ℓ]. The Fourier coeﬃcients ak, bk can,
in fact, be computed directly without appealing to the rescaling. Indeed, replacing the
integration variable in (3.83) by y = πx/ℓ, and noting that dy = (π/ℓ) dx, we deduce the
rescaled formulae
ak = 1
ℓ
 ℓ
−ℓ
f(x) cos kπx
ℓ
dx,
bk = 1
ℓ
 ℓ
−ℓ
f(x) sin kπx
ℓ
dx,
(3.85)
for the Fourier coeﬃcients of f(x) on the interval [−ℓ, ℓ].
All of the convergence results, integration and diﬀerentiation formulae, etc., that are
valid for the interval [−π, π ] carry over, essentially unchanged, to Fourier series on non-
standard intervals. In particular, adapting our basic convergence Theorem 3.8, we conclude
that if f(x) is piecewise C1, then its rescaled Fourier series (3.84) converges to its 2ℓpe-
riodic extension f(x), subject to the proviso that f(x) takes on the midpoint values at all
jump discontinuities.
Example 3.24. Let us compute the Fourier series for the function f(x) = x on the
interval −1 ≤x ≤1. Since f is odd, only the sine coeﬃcients will be nonzero. We have
bk =
 1
−1
x sin kπx dx =

−x coskπx
kπ
+ sin kπx
(kπ)2
1
x=−1
= 2(−1)k+1
kπ
.
The resulting Fourier series is
x ∼2
π

sin πx −sin 2πx
2
+ sin 3πx
3
−· · ·

.
The series converges to the 2–periodic extension of the function x, namely
f(x) =
 x −2m,
2m −1 < x < 2m + 1,
0,
x = m,
where m ∈Z is an arbitrary integer,
which is plotted in Figure 3.10.

3.4 Change of Scale
97
−4
−3
−2
−1
1
2
3
4
−1
1
Figure 3.10.
2–periodic e3xtension of x.
We can similarly reformulate complex Fourier series on the nonstandard interval
[−ℓ, ℓ]. Using (3.82) to rescale the variables in (3.64), we obtain
f(x) ∼
∞

k=−∞
ck e i kπx/ℓ,
where
ck = 1
2ℓ
 ℓ
−ℓ
f(x) e−i kπx/ℓdx.
(3.86)
Again, this is merely an alternative way of writing the real Fourier series (3.84).
When dealing with a more general interval [a, b], there are two possible options. The
ﬁrst is to take a function f(x) deﬁned for a ≤x ≤b and periodically extend it to a function
f(x) that agrees with f(x) on [a, b] and has period b−a. One can then compute the Fourier
series (3.84) for its periodic extension f(x) on the symmetric interval [−ℓ, ℓ] of width
2ℓ= b −a; the resulting Fourier series will (under the appropriate hypotheses) converge
to f(x) and hence agree with f(x) on the original interval. An alternative approach is to
translate the interval by an amount 1
2(a+ b) so as to make it symmetric around the origin;
this is accomplished by the change of variables x = x −1
2(a + b), followed by an additional
rescaling to convert the interval into [−π, π ]. The two methods are essentially equivalent,
and details are left to the reader.
Exercises
3.4.1. Let f(x) = x2 for 0 ≤x ≤1. Find its (a) Fourier sine series; (b) Fourier cosine series.
3.4.2. Find the Fourier sine series and the Fourier cosine series of the following functions de-
ﬁned on the interval [0, 1]; then graph the function to which the series converges:
(a) 1,
(b) sin πx,
(c) sin3 πx,
(d) x(1 −x).
3.4.3. Find the Fourier series for the following functions on the indicated intervals, and graph
the function that the Fourier series converges to.
(a) | x |, −3 ≤x ≤3,
(b) x2 −4, −2 ≤x ≤2,
(c) ex, −10 ≤x ≤10,
(d) sin x, −1 ≤x ≤1,
(e) σ(x), −2 ≤x ≤2.
3.4.4. For each of the functions in Exercise 3.4.3, write out the diﬀerentiated Fourier series, and
determine whether it converges to the derivative of the original function.
3.4.5. Find the Fourier series for the integral of each of the functions in Exercise 3.4.3.
♦3.4.6. Write down formulas for the Fourier series of both even and odd functions on [−ℓ, ℓ].

98
3 Fourier Series
3.4.7. Let f(x) be a continuous function on [0, ℓ].
(a) Under what conditions is its odd 2ℓ–periodic extension also continuous?
(b) Under what conditions is its odd extension also continuously diﬀerentiable?
3.4.8.(a) Write down the formulae for the Fourier series for a function f(x) deﬁned on the in-
terval 0 ≤x ≤2π. (b) Use your formula in the case f(x) = x. Is the result the same as
(3.37)? Explain, and, if diﬀerent, discuss the connection between the two Fourier series.
3.4.9. Find the Fourier series for the function f(x) = x on the interval 1 ≤x ≤2 using the two
diﬀerent methods described in the last paragraph of this subsection. Are your Fourier series
the same? Explain. Graph the functions that the Fourier series converge to.
3.4.10. Answer Exercise 3.4.9 when f(x) = sin x on the interval π ≤x ≤2π.
3.5 Convergence of Fourier Series
The goal of this ﬁnal section is to establish some of the most basic convergence results for
Fourier series. This is not a purely theoretical enterprise, since convergence considerations
impinge directly upon applications. One particularly important consequence is the connec-
tion between the degree of smoothness of a function and the decay rate of its high-order
Fourier coeﬃcients — a result that is exploited in signal and image denoising and in the
analytic properties of solutions to partial diﬀerential equations.
This section is written at a slightly more theoretically sophisticated level than what you
have read so far. However, an appreciation of the full scope, and limitations, of Fourier
analysis requires some familiarity with the underlying theory.
Moreover, the required
techniques and proofs serve as an excellent introduction to some of the most important
tools of modern mathematical analysis, and the eﬀort you expend to assimilate this material
will be more than amply rewarded in both this book and your subsequent mathematical
studies, be they applied or pure.
Unlike power series, which converge to analytic functions on the interval of conver-
gence, and diverge elsewhere (the only tricky point being whether or not the series converges
at the endpoints), the convergence of a Fourier series is a much subtler matter, and still
not completely understood.
A large part of the diﬃculty stems from the intricacies of
convergence in inﬁnite-dimensional function spaces. Let us therefore begin with a brief
outline of the key issues.
We assume that you are familiar with the usual calculus deﬁnition of the limit of a
sequence of real numbers:
lim
n →∞an = a⋆.
In any ﬁnite-dimensional vector space, e.g.,
Rm, there is essentially only one way for a sequence of vectors v(0), v(1), v(2), . . . ∈Rm to
converge, as guaranteed by any one of the following equivalent criteria:
• The vectors converge: v(n) →v⋆∈Rm as n →∞.
• The individual components of v(n) = (v(n)
1
, . . . , v(n)
m ) converge, so
lim
n →∞v(n)
j
= v⋆
j for
all j = 1, . . ., m.
• The norm of the diﬀerence goes to zero: ∥v(n) −v⋆∥→0 as n →∞.
The last requirement, known as convergence in norm, does not, in fact, depend on which
norm is chosen.
Indeed, on a ﬁnite-dimensional vector space, all norms are essentially
equivalent, and if one norm goes to zero, so does any other norm, [89; Theorem 3.17].

3.5 Convergence of Fourier Series
99
On the other hand, the analogous convergence criteria are certainly not the same in
inﬁnite-dimensional spaces. There is, in fact, a bewildering variety of convergence mecha-
nisms in function space, including pointwise convergence, uniform convergence, convergence
in norm, weak convergence, and so on. Each plays a signiﬁcant role in advanced mathe-
matical analysis, and hence all are deserving of study. Here, though, we shall cover just
the most basic aspects of convergence of the Fourier series and their applications to partial
diﬀerential equations, leaving the complete development to a more specialized text, e.g.,
[37, 128].
Pointwise and Uniform Convergence
The most familiar convergence mechanism for a sequence of functions vn(x) is pointwise
convergence. This requires that the functions’ values at each individual point converge in
the usual sense:
lim
n →∞vn(x) = v⋆(x)
for all
x ∈I,
(3.87)
where I ⊂R denotes an interval contained in their common domain. Even more explicitly,
pointwise convergence requires that, for every ε > 0 and every x ∈I, there exist an integer
N, depending on ε and x, such that
| vn(x) −v⋆(x) | < ε
for all
n ≥N.
(3.88)
Pointwise convergence can be viewed as the function space version of the convergence of the
components of a vector. We have already stated the Fundamental Theorem 3.8 regarding
pointwise convergence of Fourier series; the proof will be deferred until the end of this
section.
On the other hand, establishing uniform convergence of a Fourier series is not so
diﬃcult, and so we will begin there. The basic deﬁnition of uniform convergence looks very
similar to that of pointwise convergence, with a subtle, but important, diﬀerence.
Deﬁnition 3.25.
A sequence of functions vn(x) is said to converge uniformly to a
function v⋆(x) on a subset I ⊂R if, for every ε > 0, there exists an integer N, depending
solely on ε, such that
| vn(x) −v⋆(x) | < ε
for all x ∈I and all n ≥N.
(3.89)
Clearly, a uniformly convergent sequence of functions converges pointwise, but the
converse does not hold.
The key diﬀerence — and the reason for the term “uniform
convergence” — is that the integer N depends only on ε and not on the point x ∈I.
According to (3.89), the sequence converges uniformly if and only if for every small ε, the
graphs of the functions eventually lie inside a band of width 2ε centered on the graph of
the limiting function, as in the ﬁrst plot in Figure 3.11. The Gibbs phenomenon shown
in Figure 3.7 is a prototypical example of nonuniform convergence: For a given ε > 0, the
closer x is to the discontinuity, the larger n must be chosen so that the inequality in (3.89)
holds. Hence, there is no uniform choice of N that makes the inequality (3.89) valid for
all x and all n ≥N.
A key feature of uniform convergence is that it preserves continuity.
Theorem 3.26. If each vn(x) is continuous and vn(x) →v⋆(x) converges uniformly,
then v⋆(x) is also a continuous function.

100
3 Fourier Series
Figure 3.11.
Uniform and nonuniform convergence of functions.
The proof is by contradiction. Intuitively, if v⋆(x) were to have a discontinuity, then, as
sketched in the second plot in Figure 3.11, a suﬃciently small band around its graph would
not connect together, and this prevents the connected graph of any continuous function,
such as vn(x), from remaining entirely within the band. A detailed discussion of these
issues, including the proofs of the basic theorems, can be found in any introductory real
analysis text, [8, 96, 97].
Warning: A sequence of continuous functions can converge nonuniformly to a contin-
uous function. For example, the sequence
vn(x) =
2nx
1 + n2x2
converges pointwise to v⋆(x) ≡0 (why?) but not uniformly, since
max| vn(x) | = vn
 1
n

= 1,
which implies that (3.89) cannot hold when ε < 1.
The convergence (pointwise, uniform, etc.) of an inﬁnite series ∞
k=1 uk(x) is, by
deﬁnition, dictated by the convergence of its sequence of partial sums
vn(x) =
n

k=1
uk(x).
(3.90)
The most useful test for uniform convergence of series of functions is known as the Weier-
strass M–test, in honor of the nineteenth century German mathematician Karl Weierstrass,
known as the “father of modern analysis”.
Theorem 3.27.
Let I ⊂R. Suppose that, for each k = 1, 2, 3, . . . , the function
uk(x) is bounded:
| uk(x) | ≤mk
for all
x ∈I,
(3.91)
where mk ≥0 is a nonnegative constant. If the constant series
∞

k=1
mk < ∞
(3.92)

3.5 Convergence of Fourier Series
101
converges, then the function series
∞

k=1
uk(x) = f(x)
(3.93)
converges uniformly and absolutely† to a function f(x) for all x ∈I. In particular, if the
summands uk(x) are continuous, so is the sum f(x).
Warning: Failure of the M–test strongly indicates, but does not necessarily preclude,
that a pointwise convergent series does not converge uniformly.
With some care, we can manipulate uniformly convergent series just like ﬁnite sums.
Thus, if (3.93) is a uniformly convergent series, so is its term-wise product
∞

k=1
g(x)uk(x) = g(x)f(x)
(3.94)
with any bounded function: | g(x) | ≤C for all x ∈I.
We can integrate a uniformly
convergent series term by term,‡ and the resulting integrated series
 x
a

∞

k=1
uk(y)

dy =
∞

k=1
 x
a
uk(y) dy =
 x
a
f(y) dy
(3.95)
is uniformly convergent. Diﬀerentiation is also allowed — but only when the diﬀerentiated
series converges uniformly.
Proposition 3.28. If the diﬀerentiated series
∞

k=1
u′
k(x) = g(x) is uniformly conver-
gent, then
∞

k=1
uk(x) = f(x) is also uniformly convergent, and, moreover, f ′(x) = g(x).
We are particularly interested in the convergence of a Fourier series, which, to facilitate
the exposition, we take in its complex form
f(x) ∼
∞

k=−∞
ck e i kx.
(3.96)
Since x is real,
 e i kx  ≤1, and hence the individual summands are bounded by
 ck e i kx  ≤| ck |
for all x.
Applying the Weierstrass M–test, we immediately deduce the basic result on uniform
convergence of Fourier series.
†
Recall that a series
∞

n=1
an = a⋆is said to converge absolutely if
∞

n=1
| an | converges.
‡
Assuming that the individual functions are all integrable.

102
3 Fourier Series
Theorem 3.29. If the Fourier coeﬃcients ck of a function f(x) satisfy
∞

k=−∞
| ck | < ∞,
(3.97)
then the Fourier series (3.96) converges uniformly to a continuous function f(x) that has
the same Fourier coeﬃcients: ck = ⟨f , e i kx ⟩= ⟨f , e i kx ⟩.
Proof : Uniform convergence and continuity of the limiting function follow from Theo-
rem 3.27. To show that the ck actually are the Fourier coeﬃcients of the sum, we multiply
the Fourier series by e−i kx and integrate term by term from −π to π. As in (3.94, 95),
both operations are valid thanks to the uniform convergence of the series.
Q.E.D.
Remark: As with the Weierstrass test, failure of condition (3.97) strongly indicates
that the Fourier series does not converge uniformly, but does not completely rule it out;
nor does it say anything about nonuniform convergence or lack thereof.
The one thing that Theorem 3.29 does not guarantee is that the original function f(x)
used to compute the Fourier coeﬃcients ck is the same as the function f(x) obtained by
summing the resulting Fourier series! Indeed, this may very well not be the case. As we
know, the function that the series converges to is necessarily 2π–periodic. Thus, at the very
least, f(x) will be the 2π periodic extension of f(x). But even this may not suﬃce. Indeed,
two functions f(x) and f(x) that have the same values except at a ﬁnite set of points
x1, . . . , xm have the same Fourier coeﬃcients. (Why?) For example, the discontinuous
function f(x) =
 1,
x = 0,
0,
otherwise, has all zero Fourier coeﬃcients, and hence its Fourier
series converges to the continuous zero function. More generally, two functions that agree
everywhere outside a set of “measure zero” will have identical Fourier coeﬃcients. In this
way, a convergent Fourier series singles out a distinguished representative from a collection
of essentially equivalent 2π–periodic functions.
Remark: The term “measure” refers to a rigorous generalization of the notion of the
length of an interval to more general subsets S ⊂R. In particular, S has measure zero if
it can be covered by a collection of intervals of arbitrarily small total length. For example,
any set consisting of ﬁnitely many points, or even countably many points, e.g., the rational
numbers, has measure zero; see Exercise 3.5.19. The proper development of the notion of
measure, and the consequential Lebesgue theory of integration, is properly studied in a
course in real analysis, [96, 98].
As a consequence of Theorem 3.26, a Fourier series cannot converge uniformly when
discontinuities are present. However, it can be proved, [128], that even when the function
is not everywhere continuous, its Fourier series is uniformly convergent on any closed subset
of continuity.
Theorem 3.30. Let f(x) be 2π–periodic and piecewise C1. If f(x) is continuous on
the open interval a < x < b, then its Fourier series converges uniformly to f(x) on any
closed subinterval a + δ ≤x ≤b −δ for 0 < δ < 1
2 (b −a).
For example, the Fourier series (3.49) for the unit step function converges uniformly
if we stay away from the discontinuities — for instance, by restriction to a subinterval of
the form [δ, π −δ ] or [−π + δ, −δ ] for any 0 < δ < 1
2 π. This reconﬁrms our observation

3.5 Convergence of Fourier Series
103
that the nonuniform Gibbs behavior becomes progressively more and more localized at the
discontinuities.
Exercises
3.5.1. Consider the following sequence of planar vectors v(n) =

1 −1
n , e−n

, n = 1, 2, 3, . . . .
Prove that v(n) converges to v⋆= ( 1, 0 ) as n →∞by showing that: (a) the individual
components converge; (b) the Euclidean norms converge: ∥v(n) −v⋆∥2 →0.
3.5.2. Which of the following sequences of vectors converge as n →∞? What is the limit?
(a)
⎛
⎝
1
1 + n2 ,
n2
1 + 2n2
⎞
⎠, (b) ( cos n, sin n ), (c)
 cos n
n
, sin n
n

, (d)

cos 1
n, sin 1
n

,
(e)
 1
n cos 1
n , 1
n sin 1
n

, (f )

e−n, n e−n, n2 e−n 
, (g)
⎛
⎝log n
n
, (log n)2
n2
, (log n)3
n3
⎞
⎠,
(h)
⎛
⎝1 −n
1 + n , 1 −n
1 + n2 , 1 −n2
1 + n2
⎞
⎠, (i)
 
1 + 1
n
n
,

1 −1
n
−n 
,
(j)
 en −1
n
, cos n −1
n2

, (k)

n

e1/n −1

, n2 
cos 1
n −1
 
.
3.5.3. Which of the following sequences of functions converge pointwise for x ∈R as n →∞?
What is the limit? (a) 1 −x2
n2 , (b) e−nx, (c) e−nx2, (d) | x −n |, (e)
1
1 + (x −n)2 ,
(f )
 1,
x < n,
2,
x > n,
(g)

n2,
1
n < x < 2
n,
0,
otherwise,
(h)
 x,
| x | < n,
nx−2,
| x | ≥n.
3.5.4. Prove that the sequence vn(x) =

1,
0 < x < 1
n,
0,
otherwise,
converges pointwise, but not uni-
formly, to the zero function.
3.5.5. Which of the following sequences of functions converge pointwise to the zero function for
all x ∈R? Which converge uniformly?
(a) −x2
n2 ,
(b) e−n| x |,
(c) x e−n| x |,
(d)
1
n(1 + x2) ,
(e)
1
1 + (x −n)2 ,
(f ) | x −n |, (g)

1
n,
0 < | x | < n,
0,
otherwise,
(h)

n,
0 < | x | < 1
n,
0,
otherwise,
(i)
 x/n,
| x | < 1,
1/(nx),
| x | ≥1.
3.5.6. Does the sequence vn(x) = nx e−nx2 converge pointwise to the zero function for x ∈R?
Does it converge uniformly?
3.5.7. Answer Exercise 3.5.6 when
(a) vn(x) = x e−nx2,
(b) vn(x) =
 1,
n < x < n + 1,
0,
otherwise,
(c) vn(x) =
 1,
n < x < n + 1/n,
0,
otherwise,
(d) vn(x) =
 1/n,
n < x < 2n,
0,
otherwise,
(e) vn(x) =

1/√n,
n < x < 2n,
0,
otherwise,
(f ) vn(x) =

n2x2 −1,
−1/n < x < 1/n,
0,
otherwise.
3.5.8.(a) What is the limit of the functions vn(x) = tan−1 nx as n →∞?
(b) Is the conver-
gence uniform on all of R?
(c) on the interval [−1, 1]?
(d) on the subset {x ≥1}?
3.5.9. True or false: If pn(x) is a sequence of polynomials that converge pointwise to a polyno-
mial p⋆(x), then the convergence is uniform.

104
3 Fourier Series
3.5.10. Suppose vn(x) are continuous functions such that vn →v⋆pointwise on all of R.
True or false: (a) vn −v⋆→0 pointwise; (b) if v⋆(x) ̸= 0 for all x, then
vn
v⋆→1 pointwise.
3.5.11. Which of the following series satisfy the M–test and hence converge uniformly on the
interval [0, 1]?
(a)
∞

k=1
cos kx
k2
,
(b)
∞

k=1
sin kx
k
,
(c)
∞

k=1
xk ,
(d)
∞

k=1
(x/2)k ,
(e)
∞

k=1
ekx
k2 ,
(f )
∞

k=1
e−kx
k2
,
(g)
∞

k=1
ex/k −1
k
.
3.5.12. Prove that the power series
∞

k=1
xk
k(k + 1) converges uniformly for −1 ≤x ≤1.
♦3.5.13.(a) Prove the following result: Suppose | g(x) | ≤M for all x ∈I. If (3.93) is a uni-
formly convergent series on I, so is the term-wise product (3.94).
(b) Find a counterexample when g(x) is not uniformly bounded.
♦3.5.14. Suppose each uk(x) is continuous, and the series
∞

k=1
uk(x) = f(x) converges uniformly
on the bounded interval a ≤x ≤b. Prove that the integrated series (3.95) is uniformly
convergent.
♦3.5.15. Prove that if
∞

k=1

a2
k + b2
k < ∞, then the real Fourier series (3.34) converges uniformly
to a continuous 2π–periodic function.
3.5.16. Suppose
∞

k=1
| ak | < ∞and
∞

k=1
| bk | < ∞. Does the conclusion of Exercise 3.5.15 still
hold?
3.5.17. Explain why you only need check the inequalities (3.91) for all suﬃciently large k ≫0
in order to use the Weierstrass M–test.
3.5.18. Suppose we say that a sequence of vectors v(k) ∈Rm converges uniformly to v⋆∈Rm
if, for every ε > 0, there is an N, depending only on ε, such that | v(k)
i
−v⋆
i | < ε, for all
k ≥N and all i = 1, . . . , m. Prove that every convergent sequence of vectors converges
uniformly.
♦3.5.19.(a) Let S = {x1, x2, x3, . . . } ⊂R be a countable set. Prove that S has measure zero by
showing that, for every ε > 0, there exists a collection of open intervals I1, I2, I3, . . . ⊂R,
with respective lengths ℓ1, ℓ2, ℓ3, . . . , such that S ⊂ Ij, while the total length
 ℓj = ε.
(b) Explain why the set of rational numbers Q ⊂R is dense but nevertheless has measure
zero.
Smoothness and Decay
The criterion (3.97), which guarantees uniform convergence of a Fourier series, requires,
at the very least, that the Fourier coeﬃcients go to zero: ck →0 as k →±∞. And they
cannot decay too slowly. For example, the individual summands of the inﬁnite series
∞

0̸=k=−∞
1
| k |α
(3.98)
go to 0 as k →∞whenever α > 0, but the series converges only when α > 1. (This is an

3.5 Convergence of Fourier Series
105
immediate consequence of the standard integral convergence test, [8, 97, 108].) Thus, if
we can bound the Fourier coeﬃcients by
| ck | ≤
M
| k |α
for all
| k | ≫0,
(3.99)
for some exponent α > 1 and some positive constant M > 0, then the Weierstrass M–test
will guarantee that the Fourier series converges uniformly to a continuous function.
An important consequence of the diﬀerentiation formula (3.79) for Fourier series is that
one can detect the degree of smoothness of a function by seeing how rapidly its Fourier
coeﬃcients decay to zero. More rigorously:
Theorem 3.31. Let 0 ≤n ∈Z. If the Fourier coeﬃcients of f(x) satisfy
∞

k=−∞
| k |n | ck | < ∞,
(3.100)
then the Fourier series (3.64) converges uniformly to an n–times continuously diﬀerentiable
function f(x) ∈Cn, which is the 2π–periodic extension of f(x). Furthermore, for any 0 <
m ≤n, the m–times diﬀerentiated Fourier series converges uniformly to the corresponding
derivative f (m)(x).
Proof : Iterating (3.79), the Fourier series for the nth derivative of a function is
f (n)(x) ∼
∞

k=−∞
in kn ck e i kx.
(3.101)
If (3.100) holds, the Weierstrass M–test implies the uniform convergence of the diﬀerenti-
ated series (3.101) to a continuous 2π–periodic function. Proposition 3.28 guarantees that
the limit is the nth derivative of the original Fourier series.
Q.E.D.
This result enables us to quantify the rule of thumb that, the smaller the high-
frequency Fourier coeﬃcients, the smoother the function.
Corollary 3.32. If the Fourier coeﬃcients satisfy (3.99) for some α > n+1, then the
Fourier series converges uniformly to an n–times continuously diﬀerentiable 2π–periodic
function.
If the Fourier coeﬃcients go to zero faster than any power of k, e.g., exponentially
fast, then the function is inﬁnitely diﬀerentiable. Analyticity is more delicate, and we refer
the reader to [128] for details.
Example 3.33.
The 2π–periodic extension of the function | x | is continuous with
piecewise continuous ﬁrst derivative.
Its Fourier coeﬃcients (3.54) satisfy the estimate
(3.99) for α = 2, which is not quite fast enough to ensure a continuous second derivative.
On the other hand, the Fourier coeﬃcients (3.36) of the step function σ(x) tend to zero only
as 1/| k |, so α = 1, reﬂecting the fact that its periodic extension is piecewise continuous,
but not continuous.

106
3 Fourier Series
Exercises
3.5.20.(a) Prove that the complex Fourier series f(x) =
∞

k=1
1
k2 e i kx converges uniformly on
the interval [−π, π ].
(b) Is the sum f(x) continuous? Why or why not?
(c) Is f(x) continuously diﬀerentiable? Why or why not?
3.5.21. First, without explicitly evaluating them, how fast do you expect the Fourier coeﬃ-
cients of the following functions to go to zero as k →∞? Then prove your claim by eval-
uating the coeﬃcients.
(a) x −π,
(b) | x |,
(c) x2,
(d) x4 −2π2 x2,
(e) sin2 x,
(f ) | sin x |.
3.5.22. Using the criteria of Theorem 3.31, determine how many continuous derivatives the
functions represented by the following Fourier series have:
(a)
∞

k=−∞
e i kx
1 + k4 ,
(b)
∞

k=−∞
k̸=0
e i kx
k2 + k5 ,
(c)
∞

k=−∞
e i kx−k2
,
(d)
∞

k=0
e i kx
k + 1 ,
(e)
∞

k=−∞
e i kx
| k |! ,
(f )
∞

k=1

1 −cos 1
k2

e i kx.
♣3.5.23. Discuss convergence of each of the following Fourier series. How smooth is the sum?
Graph the partial sums to obtain a reasonable approximation to the graph of the summed
series. How many summands are needed to obtain accuracy in the second decimal digit over
the entire interval? Point out discontinuities, corners, and other features that you observe.
(a)
∞

k=0
e−kcos kx,
(b)
∞

k=0
cos kx
k + 1 ,
(c)
∞

k=1
sin kx
k3/2 ,
(d)
∞

k=1
sin kx
k3 + k .
3.5.24. Prove that if | ak |, | bk | ≤M k−α for some M > 0 and α > n + 1, then the real Fourier
series (3.34) converges uniformly to an n–times continuously diﬀerentiable 2π–periodic
function f ∈Cn.
3.5.25. Give a simple explanation of why, if the Fourier coeﬃcients ak = bk = 0 for all suﬃ-
ciently large k ≫0, then the Fourier series converges to an analytic function.
Hilbert Space
In order to make further progress, we must take a little detour. The proper setting for
the rigorous theory of Fourier series turns out to be the most important function space in
modern analysis and modern physics, known as Hilbert space in honor of the great late-
nineteenth-/early-twentieth-century German mathematician David Hilbert.
The precise
deﬁnition of this inﬁnite-dimensional inner product space is somewhat technical, but a
rough version goes as follows:
Deﬁnition 3.34. A complex-valued function f(x) is called square-integrable on the
interval [−π, π ] if it has ﬁnite L2 norm:
∥f ∥2 = 1
2π
 π
−π
| f(x) |2 dx < ∞.
(3.102)
The Hilbert space L2 = L2[−π, π ] is the vector space consisting of all complex-valued
square-integrable functions.

3.5 Convergence of Fourier Series
107
The triangle inequality
∥f + g ∥≤∥f ∥+ ∥g ∥
implies that if f, g ∈L2, so ∥f ∥, ∥g ∥< ∞, then ∥f + g ∥< ∞, and so f + g ∈L2.
Moreover, for any complex constant c,
∥c f ∥= | c | ∥f ∥,
and so c f ∈L2 also. Thus, as claimed, Hilbert space is a complex vector space. The
Cauchy–Schwarz inequality
| ⟨f , g ⟩| ≤∥f ∥∥g ∥
implies that the L2 Hermitian inner product
⟨f , g ⟩= 1
2π
 π
−π
f(x) g(x) dx
(3.103)
of two square-integrable functions is well deﬁned and ﬁnite.
In particular, the Fourier
coeﬃcients of a function f ∈L2 are speciﬁed by its inner products
ck = ⟨f , e i kx ⟩= 1
2π
 π
−π
f(x) e−i kx dx
with the complex exponentials (which, by (3.63), are in L2), and hence are all well deﬁned
and ﬁnite.
There are some interesting analytic subtleties that arise when one tries to prescribe
precisely which functions are in the Hilbert space. Every piecewise continuous function
belongs to L2. But some functions with singularities are also members. For example, the
power function | x |−α belongs to L2 for any α < 1
2, but not if α ≥1
2.
Analysis relies on limiting procedures, and it is essential that Hilbert space be “com-
plete” in the sense that appropriately convergent† sequences of functions have a limit. The
completeness requirement is not elementary, and relies on the development of the more
sophisticated Lebesgue theory of integration, which was formalized in the early part of
the twentieth century by the French mathematician Henri Lebesgue. Any function which
is square-integrable in the Lebesgue sense is admitted into L2. This includes such non-
piecewise-continuous functions as sin 1
x and x−1/3, as well as the strange function
r(x) =
 1
if x is a rational number,
0
if x is irrational.
(3.104)
Thus, while well behaved in some respects, square-integrable functions can be quite wild
in others.
Remark: The completeness of Hilbert space can be viewed as the inﬁnite-dimensional
analogue of the completeness of the real line R, meaning that every convergent Cauchy
sequence of real numbers has a limit in R. On the other hand, the rational numbers Q are
not complete — since a convergent sequence of rational numbers may well have an irrational
limit — but form a dense subset of R, because every real number can be arbitrarily closely
†
The precise technical requirement is that every Cauchy sequence of functions vk ∈L2
converge to a function v⋆∈L2; see [37, 96, 98] and also Exercise 3.5.42 for details.

108
3 Fourier Series
approximated by rational numbers, e.g., its truncated decimal expansions. Indeed, a fully
rigorous deﬁnition of the real numbers R is somewhat delicate, [97, 96].
Similarly, the space of continuous functions C0[−π, π ] is not complete, in that (nonuni-
formly) convergent sequences of continuous functions are not, in general, continuous, but it
does form a dense subspace of the Hilbert space L2[−π, π ], since every L2 function can be
arbitrarily closely approximated (in norm) by continuous functions, e.g., its approximating
trigonometric polynomials. Thus, just as R can be viewed as the completion of Q under
the Euclidean norm, so Hilbert space can be viewed as the completion of the space of con-
tinuous functions under the L2 norm, and, just like that of R, its fully rigorous deﬁnition
is rather subtle.
A second complication is that (3.102) does not, strictly speaking, deﬁne a norm once
we allow discontinuous functions into the fold.
For example, the piecewise continuous
function
f0(x) =
 1,
x = 0,
0,
x ̸= 0,
(3.105)
has norm zero, ∥f0 ∥= 0, even though it is not zero everywhere. Indeed, any function
that is zero except on a set of measure zero also has norm zero, including the function
(3.104). Therefore, in order to make (3.102) into a legitimate norm, we must agree to
identify any two functions that have the same values except on a set of measure zero.
Thus, the zero function 0 along with the preceding examples (3.104) and (3.105) are all
viewed as deﬁning the same element of Hilbert space. So, an element of Hilbert space is
not, in fact, a function, but, rather, an equivalence class of functions all diﬀering on a set
of measure zero. All this may strike the applications-oriented reader as becoming much too
abstract and arcane. In practice, you will not lose much by working with the elements of
L2 as if they were ordinary functions, and, even better, assuming that said “functions” are
always piecewise continuous and square-integrable. Nevertheless, the full analytical power
of Hilbert space theory is unleashed only by including completely general square-integrable
functions.
After its invention by pure mathematicians around the turn of the twentieth century,
physicists in the 1920s suddenly realized that Hilbert space was the ideal setting for the
modern theory of quantum mechanics, [66, 72, 115]. A quantum-mechanical wave function
is an element‡ ϕ ∈L2 that has unit norm: ∥ϕ ∥= 1. Thus, the set of wave functions is
merely the “unit sphere” in Hilbert space.
Quantum mechanics endows each physical
wave function with a probabilistic interpretation. Suppose the wave function represents
a single subatomic particle — photon, electron, etc. Then the squared modulus of the
wave function, | ϕ(x) |2, represents the probability density that quantiﬁes the chance of the
particle being located at position x. More precisely, the probability that the particle resides
in a prescribed interval [a, b] ⊂[−π, π ] is equal to

1
2π
 b
a
| ϕ(x) |2 dx . In particular, the
wave function has unit norm,
∥ϕ ∥=

1
2π
 π
−π
| ϕ(x) |2 dx = 1,
(3.106)
‡
Here we are acting as if the physical universe were represented by the one-dimensional interval
[−π, π ].
The more apt context of three-dimensional physical space is developed analogously,
replacing the single integral by a triple integral over all of R3. See also Section 7.4.

3.5 Convergence of Fourier Series
109
because the particle must certainly, i.e., with probability 1, be somewhere!
Convergence in Norm
We are now in a position to discuss convergence in norm of a Fourier series. We begin with
the basic deﬁnition, which makes sense on any normed vector space.
Deﬁnition 3.35. Let V be a normed vector space. A sequence s1, s2, s3, . . . ∈V is
said to converge in norm to f ∈V if ∥sn −f ∥→0 as n →∞.
As we noted earlier, on ﬁnite-dimensional vector spaces such as Rm, convergence in
norm is equivalent to ordinary convergence. On the other hand, on inﬁnite-dimensional
function spaces, convergence in norm diﬀers from pointwise convergence. For instance, it
is possible to construct a sequence of functions that converges in norm to 0, but does not
converge pointwise anywhere! (See Exercise 3.5.43.)
While our immediate interest is in the convergence of the Fourier series of a square-
integrable function f ∈L2[−π, π ], the methods we develop are of very general utility.
Indeed, in later chapters we will require the analogous convergence results for other types
of series solutions to partial diﬀerential equations, including multiple Fourier series as well
as series involving Bessel functions, spherical harmonics, Laguerre polynomials, and so on.
Since it distills the key issues down to their essence, the general, abstract version is, in fact,
easier to digest, and, moreover, will be immediately applicable, not just to basic Fourier
series, but to very general “eigenfunction series”.
Let V be an inﬁnite-dimensional inner product space, e.g., L2[−π, π ].
Suppose
ϕ1, ϕ2, ϕ3, . . . , are an orthonormal collection of elements of V , meaning that
⟨ϕj , ϕk ⟩=
 1
j = k,
0,
j ̸= k.
(3.107)
A straightforward argument — see Exercise 3.5.33 — proves that the ϕk are linearly
independent. Given f ∈V , we form its generalized Fourier series
f ∼
∞

k=1
ck ϕk,
where
ck = ⟨f , ϕk ⟩.
(3.108)
The formula for the coeﬃcient ck is obtained by formally taking the inner product of the
series with ϕk and invoking the orthonormality conditions (3.107). The two main examples
are the real and complex L2 spaces:
• V consists of real square-integrable functions deﬁned on [−π, π ] under the rescaled L2
inner product ⟨f , g ⟩= 1
π
 π
−π
f(x) g(x) dx. The orthonormal system {ϕk } consists
of the basic trigonometric functions, numbered as follows:
ϕ1 =
1
√
2
,
ϕ2 = cos x,
ϕ3 = sin x,
ϕ4 = cos 2x,
ϕ5 = sin 2x,
ϕ6 = cos 3x,
. . . .
• V consists of complex square-integrable functions deﬁned on [−π, π ] using the Hermi-
tian inner product (3.103). The orthonormal system {ϕk } consists of the complex
exponentials, which we order as follows:
ϕ1 = 1,
ϕ2 = e i x,
ϕ3 = e−i x,
ϕ4 = e2 i x,
ϕ5 = e−2 i x,
ϕ6 = e3 i x,
. . . .

110
3 Fourier Series
In each case, the generalized Fourier series (3.108) reduces to the ordinary Fourier se-
ries, with a minor change of indexing. Later, when we extend the separation of variables
technique to partial diﬀerential equations in more than one space dimension, we will en-
counter a variety of other important examples, in which the ϕk are the eigenfunctions of a
self-adjoint linear operator.
For the remainder of this section, to streamline the ensuing proofs, we will henceforth
assume that V is a real inner product space. However, all results will be formulated so
they are also valid for complex inner product spaces; the slightly more complicated proofs
in the complex case are relegated to the exercises.
By deﬁnition, the generalized Fourier series (3.108) converges in norm to f if the
sequence provided by its partial sums
sn =
n

k=1
ck ϕk
(3.109)
satisﬁes the criterion of Deﬁnition 3.35. Our ﬁrst result states that the partial Fourier
sum (3.109), with ck given by the inner product formula in (3.108), is, in fact, the best
approximation to f ∈V in the least squares sense, [89].
Theorem 3.36. Let Vn = span {ϕ1, ϕ2, . . . , ϕn} ⊂V be the n-dimensional subspace
spanned by the ﬁrst n elements of the orthonormal system. Then the nth order Fourier
partial sum sn ∈Vn is the best least squares approximation to f that belongs to the
subspace, meaning that it minimizes ∥f −pn ∥among all possible pn ∈Vn.
Proof : Given any element
pn =
n

k=1
dk ϕk ∈Vn,
we have, in view of the orthonormality relations (3.107),
∥pn ∥2 = ⟨pn , pn ⟩
=

n

j =1
dj ϕj ,
n

k=1
dk ϕk

=
n

j,k=1
dj dk ⟨ϕj , ϕk ⟩=
n

k=1
| dk |2,
(3.110)
reproducing the formula (B.27) for the norm with respect to an orthonormal basis. There-
fore, by the symmetry property of the real inner product,
∥f −pn ∥2 = ⟨f −pn , f −pn ⟩= ∥f ∥2 −2 ⟨f , pn ⟩+ ∥pn ∥2
= ∥f ∥2 −2
n

k=1
dk ⟨f , ϕk ⟩+ ∥pn ∥2 = ∥f ∥2 −2
n

k=1
ckdk +
n

k=1
| dk |2
= ∥f ∥2 −
n

k=1
| ck |2 +
n

k=1
| ck −dk |2.
The ﬁnal equality results from adding and subtracting the squared norm of the partial sum
(3.109),
∥sn ∥2 =
n

k=1
| ck |2,
(3.111)

3.5 Convergence of Fourier Series
111
which is a particular case of (3.110). We conclude that
∥f −pn ∥2 = ∥f ∥2 −∥sn ∥2 +
n

k=1
| ck −dk |2.
(3.112)
The ﬁrst and second terms on the right-hand side of (3.112) are uniquely determined by
f and hence cannot be altered by the choice of pn ∈Vn, which aﬀects only the ﬁnal
summation. Since the latter is a sum of nonnegative quantities, it is clearly minimized by
setting all its summands to zero, i.e., setting dk = ck for all k = 1, . . . , n. We conclude
that ∥f −pn ∥achieves its minimum value among all pn ∈Vn if and only if dk = ck, which
implies that pn = sn is the Fourier partial sum (3.109).
Q.E.D.
Example 3.37. Consider the ordinary real Fourier series. The subspace T (n) ⊂L2
spanned by the trigonometric functions cos kx, sin kx, for 0 ≤k ≤n, consists of all
trigonometric polynomials (ﬁnite Fourier sums) of degree ≤n:
pn(x) = r0
2 +
n

k=1
[ rk cos kx + sk sin kx ] .
(3.113)
Theorem 3.36 implies that the nth Fourier partial sum (3.38) is distinguished as the one
that best approximates f(x) in the least squares sense, meaning that it minimizes the L2
norm of the diﬀerence,
∥f −pn ∥=

1
π
 π
−π
| f(x) −pn(x) |2 dx ,
(3.114)
among all such trigonometric polynomials (3.113).
Returning to the general framework, if we set pn = sn, so dk = ck, in (3.112), we
conclude that the minimizing least squares error for the Fourier partial sum is
0 ≤∥f −sn ∥2 = ∥f ∥2 −∥sn ∥2 = ∥f ∥2 −
n

k=1
| ck |2.
(3.115)
We conclude that the general Fourier coeﬃcients of the function f must satisfy the in-
equality
n

k=1
| ck |2 ≤∥f ∥2.
(3.116)
Let us see what happens in the limit as n →∞. Since we are summing a sequence of
nonnegative numbers, with uniformly bounded partial sums, the limiting summation must
exist and be subject to the same bound. We have thus established Bessel’s inequality, a
key step on the road to the general theory.
Theorem 3.38. The sum of the squares of the general Fourier coeﬃcients of f ∈V
is bounded by
∞

k=1
| ck |2 ≤∥f ∥2.
(3.117)
Now, if a series, such as that on the left-hand side of Bessel’s inequality (3.117), is to
converge, the individual summands must go to zero. Thus, we immediately deduce:

112
3 Fourier Series
Corollary 3.39. The general Fourier coeﬃcients of f ∈V satisfy ck →0 as k →∞.
In the case of the trigonometric Fourier series, Corollary 3.39 yields the following
simpliﬁed form of what is known as the Riemann–Lebesgue Lemma.
Lemma 3.40.
If f ∈L2[−π, π ] is square-integrable, then its Fourier coeﬃcients
satisfy
ak = 1
π
 π
−π
f(x) cos kx dx
bk = 1
π
 π
−π
f(x) sin kx dx
⎫
⎪
⎪
⎬
⎪
⎪
⎭
−→
0
as
k −→∞.
(3.118)
Remark: This result is equivalent to the decay of the complex Fourier coeﬃcients
ck = 1
2π
 π
−π
f(x) e−i kx dx −→0
as
| k | −→∞,
(3.119)
of any complex-valued square-integrable function.
Convergence of the sum (3.117) requires that the coeﬃcients ck not tend to zero too
slowly. For instance, requiring the power bound (3.99) for some α > 1
2 suﬃces to ensure
that
∞

k=−∞
| ck |2 < ∞. Thus, as we should have expected, convergence in norm of the
Fourier series imposes less-restrictive requirements on the decay of the Fourier coeﬃcients
than uniform convergence — which needed α > 1. Indeed, a Fourier series with slowly
decaying coeﬃcients may very well converge in norm to a discontinuous L2 function, which
is not possible under uniform convergence.
Completeness
Calculations in vector spaces rely on the speciﬁcation of a basis, meaning a set of linearly
independent elements that span the space. The choice of basis serves to introduce a system
of local coordinates on the space, namely, the coeﬃcients in the expression of an element
as a linear combination of basis elements. Orthogonal and orthonormal bases are partic-
ularly handy, since the coordinates are immediately calculated by taking inner products,
while general bases require solving linear systems. In ﬁnite-dimensional vector spaces, all
bases contain the same number of elements, which, by deﬁnition, is the dimension of the
space. A vector space is, therefore, inﬁnite-dimensional if it contains an inﬁnite number
of linearly independent elements. However, the question when such a collection forms a
basis for the space is considerably more delicate, and mere counting will no longer suﬃce.
Indeed, omitting a ﬁnite number of elements from an inﬁnite collection would still leave an
inﬁnite number, but the latter will certainly not span the space. Moreover, we cannot, in
general, expect to write a general element of an inﬁnite-dimensional space as a ﬁnite linear
combination of basis elements, and so subtle questions of convergence of inﬁnite series must
also be addressed if we are to properly formulate the concept.
The deﬁnition of a basis of an inﬁnite-dimensional vector space rests on the idea of
completeness. We shall discuss completeness in the general abstract setting, but the key
example is, of course, the Hilbert space L2[−π, π ] and the systems of trigonometric or com-
plex exponential functions. For simplicity, we deﬁne completeness in terms of orthonormal

3.5 Convergence of Fourier Series
113
systems here. (Similar arguments will clearly apply to orthogonal systems, but normality
helps to streamline the presentation.)
Deﬁnition 3.41.
An orthonormal system ϕ1, ϕ2, ϕ3, . . . ∈V is called complete if,
for every f ∈V , its generalized Fourier series (3.108) converges in norm to f:
∥f −sn ∥−→0,
as
n →∞,
where
sn =
n

k=1
ck ϕk,
ck = ⟨f , ϕk ⟩,
(3.120)
is the nth partial sum of the generalized Fourier series (3.108).
Thus, completeness requires that every element of V can be arbitrarily closely ap-
proximated (in norm) by a ﬁnite linear combination of the basis elements. A complete
orthonormal system should be viewed as the inﬁnite-dimensional version of an orthonor-
mal basis of a ﬁnite-dimensional vector space. An orthogonal system is called complete
whenever the corresponding orthonormal system obtained by dividing the elements by
their norms is complete. Existence of a complete orthonormal system is directly tied to
completeness of the underlying Hilbert space.
Determining whether a given orthonormal or orthogonal system of functions is com-
plete is a diﬃcult problem, and requires some detailed analysis of their properties. The
key result for classical Fourier series is that the trigonometric functions, or, equivalently,
the complex exponentials, form a complete system; an indication of its proof will appear
below. A general characterization of complete orthonormal eigenfunction systems can be
found in Section 9.4.
Theorem 3.42.
The trigonometric functions 1, coskx, sin kx, k = 1, 2, 3, . . . , form
a complete orthogonal system in L2 = L2[−π, π ]. In other words, if sn(x) denotes the
nth partial sum of the Fourier series of the square-integrable function f(x) ∈L2, then
lim
n →∞∥f −sn ∥= 0.
To better comprehend completeness, let us describe some equivalent characterizations
and consequences. One is the inﬁnite-dimensional counterpart of formula (B.27) for the
norm of a vector in terms of its coordinates with respect to an orthonormal basis.
Theorem 3.43. The orthonormal system ϕ1, ϕ2, ϕ3, . . . ∈V is complete if and only
if Plancherel’s formula
∥f ∥2 =
∞

k=1
| ck |2 =
∞

k=1
⟨f , ϕk ⟩2
(3.121)
holds for every f ∈V .
Proof : Theorem 3.43, thus, states that the system of functions is complete if and only
if the Bessel inequality (3.117) is, in fact, an equality. Indeed, letting n →∞in (3.115),
we ﬁnd
lim
n →∞∥f −sn ∥2 = ∥f ∥2 −lim
n →∞
n

k=1
| ck |2 = ∥f ∥2 −
∞

k=1
| ck |2.
Therefore, the completeness condition (3.120) holds if and only if the right-hand side
vanishes, which is the Plancherel identity (3.121).
Q.E.D.

114
3 Fourier Series
An analogous result holds for the inner product between two elements, which we state
in its general complex form, although the proof given here is for the real version; in Exercise
3.5.35 the reader is asked to supply the slightly more intricate complex proof.
Corollary 3.44. The Fourier coeﬃcients ck = ⟨f , ϕk ⟩, dk = ⟨g , ϕk ⟩, of any f, g ∈
V satisfy Parseval’s formula
⟨f , g ⟩=
∞

k=1
ck dk.
(3.122)
Proof : Since, for a real inner product,
⟨f , g ⟩= 1
4

∥f + g ∥2 −∥f −g ∥2 
,
(3.123)
Parseval’s formula results from applying Plancherel’s formula (3.121) to each term on the
right-hand side:
⟨f , g ⟩= 1
4
∞

k=1

(ck + dk)2 −(ck −dk)2 
=
∞

k=1
ck dk,
which agrees with (3.122), since we are assuming that dk = dk are all real.
Q.E.D.
Note that Plancherel’s formula is a special case of Parseval’s formula,† obtained by
setting f = g. In the particular case of the complex exponential basis e i kx of L2[−π, π ],
the Plancherel and Parseval formulae take the form
1
2π
 π
−π
| f(x) |2 dx =
∞

k=−∞
| ck |2,
1
2π
 π
−π
f(x) g(x) dx =
∞

k=−∞
ck dk ,
(3.124)
where ck = ⟨f , e i kx ⟩, dk = ⟨g , e i kx ⟩are the ordinary Fourier coeﬃcients of the complex-
valued functions f(x) and g(x). In Exercise 3.5.38, you are asked to write the corresponding
formulas for the real Fourier coeﬃcients.
Completeness also tells us that a function is uniquely determined by its Fourier coef-
ﬁcients.
Proposition 3.45. If the orthonormal system ϕ1, ϕ2, . . . ∈V is complete, then the
only element f ∈V with all zero Fourier coeﬃcients, 0 = c1 = c2 = · · ·, is the zero element:
f = 0. More generally, two elements f, g ∈V have the same Fourier coeﬃcients if and only
if they are the same: f = g.
Proof : The proof is an immediate consequence of Plancherel’s formula. Indeed, if
ck = 0, then (3.121) implies that ∥f ∥= 0 and hence f = 0. The second statement follows
by applying the ﬁrst to their diﬀerence f −g.
Q.E.D.
Another way of stating this result is that the only function that is orthogonal to every
element of a complete orthonormal system is the zero function.‡ In other words, a complete
orthonormal system is maximal in the sense that no further orthonormal elements can be
appended to it.
†
Curiously, Marc-Antoine Parseval des Chˆenes’ contribution slightly predates Fourier, whereas
Michel Plancherel’s appeared almost a century later.
‡
Or, to be more technically accurate, any function that is zero outside a set of measure zero.

3.5 Convergence of Fourier Series
115
Let us now discuss the completeness of the Fourier trigonometric and complex expo-
nential functions. We shall establish the completeness property only for suﬃciently smooth
functions, leaving the harder general proof to the references, [37, 128].
According to Theorem 3.30, if f(x) is continuous, 2π periodic, and piecewise C1, its
Fourier series converges uniformly,
f(x) =
∞

k=−∞
ck e i kx
for all
−π ≤x ≤π.
The same holds for its complex conjugate f(x). Therefore,
| f(x) |2 = f(x) f(x) = f(x)
∞

k=−∞
ck e−i kx =
∞

k=−∞
ck f(x) e−i kx,
which also converges uniformly by (3.94). Formula (3.95) permits us to integrate both
sides from −π to π, yielding
∥f ∥2 = 1
2π
 π
−π
| f(x) |2 dx =
∞

k=−∞
ck
2π
 π
−π
f(x) e−i kx dx =
∞

k=−∞
ck ck =
∞

k=−∞
| ck |2.
Therefore, Plancherel’s formula (3.121) holds for any continuous, piecewise C1 function.
With some additional technical work, this result is used to establish the validity of
Plancherel’s formula for all f ∈L2, the key step being to suitably approximate f by such
continuous, piecewise C1 functions.
With this in hand, completeness is an immediate
consequence of Theorem 3.43.
Q.E.D.
Pointwise Convergence
Let us ﬁnally return to the Pointwise Convergence Theorem 3.8 for the trigonometric
Fourier series. The goal is to prove that, under the appropriate hypotheses on f(x), namely
2π–periodic and piecewise C1, the limit of its partial Fourier sums is
lim
n →∞sn(x) = 1
2

f(x+) + f(x−)

.
(3.125)
We begin by substituting the formulae (3.65) for the complex Fourier coeﬃcients into the
formula (3.109) for the nth partial sum:
sn(x) =
n

k=−n
ck e i kx =
n

k=−n
 1
2π
 π
−π
f(y) e−i ky dy

e i kx
= 1
2π
 π
−π
f(y)

n

k=−n
e i k(x−y)

dy.
(3.126)
To proceed further, we need to calculate the ﬁnal summation
n

k=−n
e i kx = e−i nx + · · · + e−i x + 1 + e i x + · · · + e i nx.

116
3 Fourier Series
This, in fact, has the form of a geometric sum,
m

k=0
a rk = a + a r + a r2 + · · · + a rm = a
rm+1 −1
r −1

,
(3.127)
with m + 1 = 2n + 1 summands, initial term a = e−i nx, and ratio r = e i x. Therefore,
n

k=−n
e i kx = e−i nx
e i (2n+1)x −1
e i x −1

= e i (n+1)x −e−i nx
e i x −1
= e i

n+ 1
2

x −e−i

n+ 1
2

x
e i x/2 −e−i x/2
=
sin

n + 1
2

x
sin 1
2 x
.
(3.128)
In this computation, to pass from the ﬁrst to the second line, we multiplied numerator and
denominator by e−i x/2, after which we used the formula (3.60) for the sine function in terms
of complex exponentials. Incidentally, (3.128) is equivalent to the intriguing trigonometric
summation formula
1 + 2

cos x + cos 2x + cos 3x + · · · + cos nx

=
sin

n + 1
2

x
sin 1
2 x
.
(3.129)
Therefore, substituting back into (3.126), we obtain
sn(x) = 1
2π
 π
−π
f(y)
sin

n + 1
2

(x −y)
sin 1
2 (x −y)
dy
= 1
2π
 x+π
x−π
f(x + y)
sin

n + 1
2

y
sin 1
2 y
dy = 1
2π
 π
−π
f(x + y)
sin

n + 1
2

y
sin 1
2 y
dy.
The second equality is the result of changing the integration variable from y to x + y;
the ﬁnal equality follows since the integrand is 2π–periodic, and so its integrals over any
interval of length 2π all have the same value; see Exercise 3.2.9.
Thus, to prove (3.125), it suﬃces to show that
lim
n →∞
1
π
 π
0
f(x + y)
sin

n + 1
2

y
sin 1
2 y
dy = f(x+),
lim
n →∞
1
π
 0
−π
f(x + y)
sin

n + 1
2

y
sin 1
2 y
dy = f(x−).
(3.130)
The proofs of the two formulas are identical, and so we concentrate on establishing the
ﬁrst. Using the fact that the integrand is even, and then our summation formula (3.128)
in reverse, yields
1
π
 π
0
sin

n + 1
2

y
sin 1
2 y
dy = 1
2π
 π
−π
sin

n + 1
2

y
sin 1
2 y
dy = 1
2π
 π
−π
n

k=−n
e i ky dy = 1,
because only the constant term has a nonzero integral. Multiplying this formula by f(x+)
and then subtracting the result from the ﬁrst formula in (3.130) leads to
lim
n →∞
1
π
 π
0
f(x + y) −f(x+)
sin 1
2 y
sin

n + 1
2

y dy = 0,
(3.131)

3.5 Convergence of Fourier Series
117
which we now proceed to prove.
We claim that, for each ﬁxed value of x, the function
g(y) = f(x + y) −f(x+)
sin 1
2 y
is piecewise continuous for all 0 ≤y ≤π. Owing to our hypotheses on f(x), the only
problematic point is at y = 0, but then, by l’Hˆopital’s Rule (for one-sided limits),
lim
y →0+ g(y) =
lim
y →0+
f(x + y) −f(x+)
sin 1
2 y
=
lim
y →0+
f ′(x + y)
1
2 cos 1
2 y = 2 f ′(x+).
Consequently, (3.131) will be established if we can show that
lim
n →∞
1
π
 π
0
g(y) sin

n + 1
2

y dy = 0
(3.132)
whenever g is piecewise continuous. Were it not for the extra 1
2 , this would immediately
follow from the simpliﬁed Riemann–Lebesgue Lemma 3.40. More honestly, we can invoke
the addition formula for sin

n + 1
2

y to write
1
π
 π
0
g(y) sin

n + 1
2

y dy = 1
π
 π
0

g(y) sin 1
2 y

cos ny dy + 1
π
 π
0

g(y) cos 1
2 y

sin ny dy.
The ﬁrst integral is the nth Fourier cosine coeﬃcient for the piecewise continuous function
g(y) sin 1
2 y, while the second integral is the nth Fourier sine coeﬃcient for the piecewise
continuous function g(y) cos 1
2 y. Lemma 3.40 implies that both of these converge to zero
as n →∞, and hence (3.132) holds. This completes the proof, thus establishing pointwise
convergence of the Fourier series.
Q.E.D.
Remark: An alternative approach to the last part of the proof is to use the general
Riemann–Lebesgue Lemma, whose proof can be found in [37, 128].
Lemma 3.46. Suppose g(x) is piecewise continuous on [a, b]. Then
0 = lim
ω →∞
 b
a
g(x) e i ω x dx
= lim
ω →∞
 b
a
g(x) cosωx dx + i
lim
ω →∞
 b
a
g(x) sin ωx dx.
(3.133)
Intuitively, the Riemann–Lebesgue Lemma says that, as the frequency ω gets larger
and larger, the increasingly rapid oscillations of the integrand tend to cancel each other
out.
Remark: While the Fourier series of a merely continuous function need not converge
pointwise everywhere, a deep theorem, proved by the Swedish mathematician Lennart
Carleson in 1966, [28], states that the set of points where it does not converge has measure
zero, and hence the exceptional points form a very small subset.

118
3 Fourier Series
Exercises
3.5.26. Which of the following sequences converge in norm to the zero function for x ∈R?
(a) vn(x) =
nx
1 + n2 x2 ,
(b) vn(x) =
 1,
n < x < n + 1,
0,
otherwise,
(c) vn(x) =
 1,
n < x < n + 1/n,
0,
otherwise,
(d) vn(x) =
 1/n,
n < x < 2n,
0,
otherwise,
(e) vn(x) =

1/√n,
n < x < 2n,
0,
otherwise,
(f ) vn(x) =

n2x2 −1,
−1/n < x < 1/n,
0,
otherwise.
3.5.27. Discuss pointwise and L2 convergence of the following sequences on the interval [0, 1]:
(a) 1 −x2
n2 ,
(b)

n,
1/n2 < x < 1/n,
x,
otherwise,
(c) e−nx,
(d) sin nx.
3.5.28. Prove, directly from the deﬁnition, the convergence in norm of the Fourier series (3.49)
of the step function.
3.5.29. Let f(x) ∈L2[a, b] be square integrable. Which constant function g(x) ≡c best ap-
proximates f in the least squares sense?
3.5.30. Suppose the sequence fn(x) converges pointwise to a function f⋆(x) on an interval [a, b],
and converges to g⋆(x) in the L2 norm on [a, b]. Is f⋆(x) = g⋆(x) at every a ≤x ≤b?
3.5.31. Find a formula for the L2 norm of the Fourier series in Exercises 3.5.20 and 3.5.22.
3.5.32. Under what conditions on the function f(x) is the least squares error due to the nth
order Fourier partial sum equal to zero: ∥f −sn ∥= 0?
♦3.5.33. Let V be an inner product space. Prove that the elements of a (ﬁnite or inﬁnite) or-
thonormal system ϕ1, ϕ2, . . . ∈V are linearly independent, meaning that any ﬁnite linear
combination vanishes, c1ϕ1 + · · · + cnϕn = 0, if and only if the coeﬃcients are all zero:
c1 = · · · = cn = 0.
♦3.5.34. Let V be a complex inner product space. Prove that, for all f, g ∈V ,
(a) ∥f + g ∥2 = ∥f ∥2 + 2 Re ⟨f , g ⟩+ ∥g ∥2;
(b) ⟨f , g ⟩= 1
4

∥f + g ∥2 −∥f −g ∥2 + i ∥f + i g ∥2 −i ∥f −i g ∥2 
.
♦3.5.35. Let V be an inﬁnite-dimensional complex inner product space, and ϕk ∈V a complete
orthonormal system. Prove the corresponding Plancherel and Parseval formulas.
Hint: Use the identities in Exercise 3.5.34.
3.5.36. What does Plancherel’s formula (3.121) tell us in a ﬁnite-dimensional vector space?
What about Parseval’s formula (3.122)?
3.5.37. Let f(x) = x, g(x) = sign x. (a) Write out Plancherel’s formula for the complex
Fourier coeﬃcients of f. (b) Write out Plancherel’s formula for the complex Fourier coef-
ﬁcients of g. (c) Write out Parseval’s formula for the complex Fourier coeﬃcients of f, g.
♦3.5.38.(a) Prove the real version of the Plancherel formula
1
π
	 π
−π | f(x) |2 dx = 1
2 a2
0 +
∞

k=1
(a2
k + b2
k)
(3.134)
for the trigonometric Fourier coeﬃcients of a real function f(x).
(b) What is the real version of Parseval’s formula?
3.5.39. Give an alternative proof of formula (3.129) that does not require complex functions by
ﬁrst multiplying through by sin 1
2 x and then invoking a suitable trigonometric identity for
the product terms.

3.5 Convergence of Fourier Series
119
3.5.40.(a) Prove that the functions ϕn(x) = sin

n −1
2

x, for n = 1, 2, 3, . . . , form an orthogo-
nal sequence on the interval [0, π ] relative to the L2 inner product ⟨f , g ⟩=
	 π
0 f(x) g(x) dx.
(b) Find the formula for the Fourier coeﬃcients of a function f(x) relative to the orthogo-
nal sequence ϕn(x).
(c) State Bessel’s inequality and Plancherel’s formula in this case.
Carefully state any hypotheses that might be required for the validity of your formulas.
♦3.5.41. Prove that a sequence of vectors v(n) ∈Rm converges in the Euclidean norm,
∥v(n) −v⋆∥→0 as n →∞, if and only if their individual components converge:
v(n)
i
→v⋆
i for i = 1, . . . , m.
♦3.5.42. Let V be a normed vector space. A sequence vn ∈V is called a Cauchy sequence if for
every ε > 0 there exists an N such that ∥vm −vn ∥< ε whenever both m, n ≥N. Prove
that a sequence that converges in norm, ∥vn −v⋆∥→0 as n →∞, is necessarily a Cauchy
sequence.
Remark: A normed vector space is called complete if every Cauchy sequence
converges in norm. It can be proved, [96, 98], that any ﬁnite-dimensional normed vector
space is complete, but this is not necessarily the case in inﬁnite dimensions. For example,
the vector spaces consisting of all trigonometric polynomials and of all polynomials are not
complete in the L2 norm. The most important example of a complete inﬁnite-dimensional
vector space is the Hilbert space L2.
♦3.5.43. For each n = 1, 2, . . . , deﬁne the function fn(x) =

1,
k
m ≤x ≤k+1
m ,
0,
otherwise,
where
n = 1
2 m(m + 1) + k and 0 ≤k ≤m. Show ﬁrst that m, k are uniquely determined by n.
Then prove that, on the interval [0, 1], the sequence fn(x) converges in norm to 0 but does
not converge pointwise anywhere!
♥3.5.44. Let u(t, x) solve the initial value problem ∂2u
∂t2 = c2 ∂2u
∂x2 , u(0, x) = f(x),
∂u
∂t (0, x) = 0,
for −∞< x < ∞, where f(x) →0 as | x | →∞. True or false: As t →∞, the solution
u(t, x) converges to an equilibrium solution (a) pointwise; (b) uniformly; (c) in norm.
♥3.5.45. Answer Exercise 3.5.44 for the initial conditions u(0, x) = 0,
∂u
∂t (0, x) = g(x), with
g(x) →0 as | x | →∞.

Chapter 4
Separation of Variables
Three cardinal linear second-order partial diﬀerential equations have collectively driven the
development of the entire subject. The ﬁrst two we have already encountered: The wave
equation describes vibrations and waves in continuous media, including sound waves, water
waves, elastic waves, electromagnetic waves, and so on. The heat equation models diﬀusion
processes, including thermal energy in solids, solutes in liquids, and biological populations.
Third, and in many ways the most important of all, is the Laplace equation and its inho-
mogeneous counterpart, the Poisson equation, which govern equilibrium mechanics. The
latter two equations arise in an astonishing variety of mathematical and physical contexts,
ranging through elasticity and solid mechanics, ﬂuid mechanics, electromagnetism, poten-
tial theory, thermomechanics, geometry, probability, number theory, and many other ﬁelds.
The solutions to the Laplace equation are known as harmonic functions, and the discov-
ery of their many remarkable properties forms one of the most celebrated chapters in the
history of mathematics. All three equations, along with their multi-dimensional kin, will
appear repeatedly throughout this text.
The aim of the current chapter is to develop the method of separation of variables
for solving these key partial diﬀerential equations in their two-independent-variable incar-
nations. For the wave and heat equations, the variables are time, t, and a single space
coordinate, x, leading to initial-boundary value problems modeling the dynamical behav-
ior of a one-dimensional medium. For the Laplace and Poisson equations, both variables
represent space coordinates, x and y, and the associated boundary value problems model
the equilibrium conﬁguration of a planar body, e.g., the deformations of a membrane. Sep-
aration of variables seeks special solutions that can be written as the product of functions
of the individual variables, thereby reducing the partial diﬀerential equation to a pair of
ordinary diﬀerential equations. More-general solutions can then be expressed as inﬁnite
series in the appropriate separable solutions. For the two-variable equations considered
here, this results in a Fourier series representation of the solution. In the case of the wave
equation, separation of variables serves to focus attention on the vibrational character of
the solution, whereas the earlier d’Alembert approach emphasizes its particle-like aspects.
Unfortunately, for the Laplace equation, separation of variables applies only to boundary
value problems in very special geometries, e.g., rectangles and disks. Further development
of the separation of variables method for solving partial diﬀerential equations in three or
more variables can be found in Chapters 11 and 12.
In the ﬁnal section, we take the opportunity to summarize the fundamental tripar-
tite classiﬁcation of planar second-order partial diﬀerential equations. Each of the three
paradigmatic equations epitomizes one of the classes: hyperbolic, such as the wave equa-
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
121
4
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

122
4 Separation of Variables
tion; parabolic, such as the heat equation; and elliptic, such as the Laplace and Poisson
equations. Each category enjoys its own distinctive properties and features, both analytic
and numeric, and, in eﬀect, forms a separate mathematical subdiscipline.
4.1 The Diﬀusion and Heat Equations
Let us begin with a brief physical derivation of the heat equation from ﬁrst principles.
We consider a bar — meaning a thin, heat-conducting body. “Thin” means that we can
regard the bar as a one-dimensional continuum with no signiﬁcant transverse temperature
variation. We will assume that the bar is fully insulated along its length, and so heat can
enter (or leave) only through its uninsulated endpoints. We use t to represent time, and
a ≤x ≤b to denote spatial position along the bar, which occupies the interval [a, b]. Our
goal is to ﬁnd the temperature u(t, x) of the bar at position x and time t.
The dynamical equations governing the temperature are based on three fundamental
physical principles. First is the Law of Conservation of Heat Energy. Recalling the general
Deﬁnition 2.7, this particular conservation law takes the form
∂ε
∂t + ∂w
∂x = 0,
(4.1)
in which ε(t, x) represents the thermal energy density at time t and position x, while
w(t, x) denotes the heat ﬂux, i.e., the rate of ﬂow of thermal energy along the bar. Our
sign convention is that w(t, x) > 0 at points where the energy ﬂows in the direction of
increasing x (left to right). The integrated form (2.49) of the conservation law, namely
d
dt
 b
a
ε(t, x) dx = w(t, a) −w(t, b),
(4.2)
states that the rate of change in the thermal energy within the bar is equal to the total
heat ﬂux passing through its uninsulated ends. The signs of the boundary terms conﬁrm
that heat ﬂux into the bar results in an increase in temperature.
The second ingredient is a constitutive assumption concerning the bar’s material prop-
erties. It has been observed that, under reasonable conditions, thermal energy is propor-
tional to temperature:
ε(t, x) = σ(x) u(t, x).
(4.3)
The factor
σ(x) = ρ(x) χ(x) > 0
(4.4)
is the product of the density ρ of the material and its speciﬁc heat capacity χ, which is
the amount of heat energy required to raise the temperature of a unit mass of the material
by one degree. Note that we are assuming that the medium is not changing in time, and
so physical quantities such as density and speciﬁc heat depend only on position x. We
also assume, perhaps with less physical justiﬁcation, that its material properties do not
depend upon the temperature; otherwise, we would be forced to deal with a much thornier
nonlinear diﬀusion equation, [70, 99].
The third physical principle relates heat ﬂux and temperature. Physical experiments
show that the thermal energy moves from hot to cold at a rate that is in direct proportion to

4.1 The Diﬀusion and Heat Equations
123
the temperature gradient, which, in the one-dimensional case, means its derivative ∂u/∂x.
The resulting relation
w(t, x) = −κ(x) ∂u
∂x
(4.5)
is known as Fourier’s Law of Cooling. The proportionality factor κ(x) > 0 is the thermal
conductivity of the bar at position x, and the minus sign reﬂects the everyday observation
that heat energy moves from hot to cold. A good heat conductor, e.g., silver, will have
high conductivity, while a poor conductor, e.g., glass, will have low conductivity.
Combining the three laws (4.1, 3, 5) produces the linear diﬀusion equation
∂
∂t

σ(x) u

= ∂
∂x

κ(x) ∂u
∂x

,
a < x < b,
(4.6)
governing the thermodynamics of a one-dimensional medium. It is also used to model a
wide variety of diﬀusive processes, including chemical diﬀusion, diﬀusion of contaminants
in liquids and gases, population dispersion, and the spread of infectious diseases. If there
is an external heat source along the length of the bar, then the diﬀusion equation acquires
an additional prescribed inhomogeneous term:
∂
∂t

σ(x) u

= ∂
∂x

κ(x) ∂u
∂x

+ h(t, x),
a < x < b.
(4.7)
In order to uniquely prescribe the solution u(t, x), we need to specify an initial tem-
perature distribution
u(t0, x) = f(x),
a ≤x ≤b.
(4.8)
In addition, we must impose a suitable boundary condition at each end of the bar. There
are three common types. The ﬁrst is a Dirichlet boundary condition, where the end is held
at a prescribed temperature. For example,
u(t, a) = α(t)
(4.9)
ﬁxes the temperature (possibly time-varying) at the left end. Alternatively, the Neumann
boundary condition
∂u
∂x (t, a) = μ(t)
(4.10)
prescribes the heat ﬂux w(t, a) = −κ(a)ux(t, a) there. In particular, a homogeneous Neu-
mann condition, ux(t, a) ≡0, models an insulated end that prevents thermal energy ﬂowing
in or out. The Robin† boundary condition,
∂u
∂x (t, a) + β(t) u(t, a) = τ(t),
(4.11)
models the heat exchange resulting from the end of the bar being placed in a heat bath
(thermal reservoir) at temperature τ(t).
Each end of the bar is required to satisfy one of these boundary conditions.
For
example, a bar with both ends having prescribed temperatures is governed by the pair of
Dirichlet boundary conditions
u(t, a) = α(t),
u(t, b) = β(t),
(4.12)
†
Since it is named after the nineteenth-century French analyst Victor Gustave Robin, the
pronunciation should be with a French accent.

124
4 Separation of Variables
whereas a bar with two insulated ends requires two homogeneous Neumann boundary
conditions
∂u
∂x (t, a) = 0,
∂u
∂x (t, b) = 0.
(4.13)
Mixed boundary conditions, with one end at a ﬁxed temperature and the other insulated,
are similarly formulated, e.g.,
u(t, a) = α(t),
∂u
∂x (t, b) = 0.
(4.14)
Finally, the periodic boundary conditions
u(t, a) = u(t, b),
∂u
∂x (t, a) = ∂u
∂x (t, b),
(4.15)
correspond to a circular ring obtained by joining the two ends of the bar. As before, we
are assuming that the heat is allowed to ﬂow only around the ring — insulation prevents
the radiation of heat from one side of the ring aﬀecting the other side.
The Heat Equation
In this book, we will retain the term “heat equation” to refer to the case in which the
bar is composed of a uniform material, and so its density ρ, conductivity κ, and speciﬁc
heat χ are all positive constants. We also exclude external heat sources (other than at the
endpoints), meaning that the bar remains insulated along its entire length. Under these
assumptions, the general diﬀusion equation (4.6) reduces to the homogeneous heat equation
∂u
∂t = γ ∂2u
∂x2
(4.16)
for the temperature u(t, x) at time t and position x. The constant
γ = κ
σ = κ
ρ χ
(4.17)
is called the thermal diﬀusivity; it incorporates all of the bar’s relevant physical properties.
The solution u(t, x) will be uniquely prescribed once we specify initial conditions (4.8) and
a suitable boundary condition at both of its endpoints.
As we learned in Section 3.1, the separable solutions to the heat equation are based
on the exponential ansatz†
u(t, x) = e−λt v(x),
(4.18)
where v(x) depends only on the spatial variable. Functions of this form, which “separate”
into a product of a function of t times a function of x, are known as separable solutions.
Substituting (4.18) into (4.16) and canceling the common exponential factors, we ﬁnd that
v(x) must solve the second-order linear ordinary diﬀerential equation
−γ d2v
dx2 = λ v.
†
Anticipating the eventual signs of the eigenvalues, and to facilitate later discussions, we now
include a minus sign in the exponential term.

4.1 The Diﬀusion and Heat Equations
125
Each nontrivial solution v(x) ̸≡0 is an eigenfunction, with associated eigenvalue λ, for the
linear diﬀerential operator L[v] = −γ v′′(x). With the separable eigensolutions (4.18) in
hand, we will then be able to reconstruct the desired solution u(t, x) as a linear combination,
or rather inﬁnite series, thereof.
Let us concentrate on the simplest case: a uniform, insulated bar of length ℓthat is
held at zero temperature at both ends. We specify its initial temperature f(x) at time
t0 = 0, and so the relevant initial and boundary conditions are
u(t, 0) = 0,
u(t, ℓ) = 0,
t ≥0,
u(0, x) = f(x),
0 ≤x ≤ℓ.
(4.19)
The eigensolutions (4.18) are found by solving the Dirichlet boundary value problem
γ d2v
dx2 + λ v = 0,
v(0) = 0,
v(ℓ) = 0.
(4.20)
By direct calculation (as you are asked to do in Exercises 4.1.19–20), one ﬁnds that if λ
is either complex, or real and nonpositive, then the only solution to the boundary value
problem (4.20) is the trivial solution v(x) ≡0. This means that all the eigenvalues must
necessarily be real and positive. In fact, the reality and positivity of the eigenvalues need
not be explicitly checked. Rather, they follow from very general properties of positive
deﬁnite boundary value problems, of which (4.20) is a particular case. See Section 9.5 for
the underlying theory and Theorem 9.34 for the relevant result.
When λ > 0, the general solution to the diﬀerential equation is a trigonometric func-
tion
v(x) = a cos ωx + b sin ωx,
where
ω =

λ/γ ,
and a and b are arbitrary constants. The ﬁrst boundary condition requires v(0) = a = 0.
This serves to eliminate the cosine term, and then the second boundary condition requires
v(ℓ) = b sin ωℓ= 0.
Therefore, since we require b ̸= 0 — otherwise, the solution is trivial and does not qualify
as an eigenfunction — ωℓmust be an integer multiple of π, and so
ω = π
ℓ,
2π
ℓ,
3π
ℓ,
. . . .
We conclude that the eigenvalues and eigenfunctions of the boundary value problem (4.20)
are
λn = γ
"nπ
ℓ
#2
,
vn(x) = sin nπx
ℓ
,
n = 1, 2, 3, . . ..
(4.21)
The corresponding eigensolutions (4.18) are
un(t, x) = exp

−γ n2 π2 t
ℓ2

sin nπx
ℓ
,
n = 1, 2, 3, . . . .
(4.22)
Each represents a trigonometrically oscillating temperature proﬁle that maintains its form
while decaying to zero at an exponentially fast rate.
To solve the general initial value problem, we assemble the eigensolutions into an
inﬁnite series,
u(t, x) =
∞

n=1
bn un(t, x) =
∞

n=1
bn exp

−γ n2 π2 t
ℓ2

sin nπx
ℓ
,
(4.23)

126
4 Separation of Variables
whose coeﬃcients bn are to be ﬁxed by the initial conditions. Indeed, assuming that the
series converges, the initial temperature proﬁle is
u(0, x) =
∞

n=1
bn sin nπx
ℓ
= f(x).
(4.24)
This has the form of a Fourier sine series (3.52) on the interval [0, ℓ]. Thus, the coeﬃcients
are determined by the Fourier formulae (3.53), and so
bn = 2
ℓ
 ℓ
0
f(x) sin nπx
ℓ
dx,
n = 1, 2, 3, . . . .
(4.25)
The resulting formula (4.23) describes the Fourier sine series for the temperature u(t, x) of
the bar at each later time t ≥0.
Example 4.1. Consider the initial temperature proﬁle
u(0, x) = f(x) =
⎧
⎪
⎨
⎪
⎩
−x,
0 ≤x ≤1
5,
x −2
5,
1
5 ≤x ≤
7
10,
1 −x,
7
10 ≤x ≤1,
(4.26)
on a bar of length 1, plotted in the ﬁrst graph in Figure 4.1. Using (4.25), the ﬁrst few
Fourier coeﬃcients of f(x) are computed (by either exact or numerical integration) to be
b1 ≈.0897,
b2 ≈−.1927,
b3 ≈−.0289,
b4 = 0,
b5 ≈−.0162,
b6 ≈.0132,
b7 ≈.0104,
b8 = 0,
. . . .
The resulting Fourier series solution to the heat equation is
u(t, x) =
∞

n=1
bn un(t, x) =
∞

n=1
bn e−γ n2 π2 t sin nπx
≈.0897 e−γ π2 t sin πx −.1927 e−4γ π2 t sin 2πx −.0289 e−9γ π2 t sin 3πx −· · · .
In Figure 4.1, the solution, for γ = 1, is plotted at some representative times. Observe
that the corners in the initial proﬁle are immediately smoothed out. As time progresses,
the solution decays, at a fast exponential rate of e−π2 t ≈e−9.87t, to a uniform, zero tem-
perature, which is the equilibrium temperature distribution for the homogeneous Dirichlet
boundary conditions. As the solution decays to thermal equilibrium, the higher Fourier
modes rapidly disappear, and the solution assumes the progressively more symmetric shape
of a single sine arc, of rapidly decreasing amplitude.
Smoothing and Long–Time Behavior
The fact that we can write the solution to an initial-boundary value problem in the form
of an inﬁnite series (4.23) is progress of a sort. However, because we are unable to sum the
series in closed form, this “solution” is much less satisfying than a direct, explicit formula.
Nevertheless, there are important qualitative and quantitative features of the solution that
can be easily gleaned from such series expansions.

4.1 The Diﬀusion and Heat Equations
127
t = 0
t = .001
t = .01
t = .03
t = .05
t = .1
Figure 4.1.
A solution to the heat equation.

If the initial data f(x) is integrable (e.g., piecewise continuous), then its Fourier coef-
ﬁcients are uniformly bounded; indeed, for any n ≥1,
| bn | ≤2
ℓ
 ℓ
0
 f(x) sin nπx
ℓ
 dx ≤2
ℓ
 ℓ
0
| f(x) | dx ≡M.
(4.27)
This property holds even for quite irregular data. Under these conditions, each term in the
series solution (4.23) is bounded by an exponentially decaying function
 bn exp

−γ n2 π2
ℓ2
t

sin nπx
ℓ
 ≤M exp

−γ n2 π2
ℓ2
t

.
This means that, as soon as t > 0, most of the high-frequency terms, n ≫0, will be
extremely small. Only the ﬁrst few terms will be at all noticeable, and so the solution
essentially degenerates into a ﬁnite sum over the ﬁrst few Fourier modes. As time increases,
more and more of the Fourier modes will become negligible, and the sum further degenerates
into fewer and fewer signiﬁcant terms. Eventually, as t →∞, all of the Fourier modes will
decay to zero. Therefore, the solution will converge exponentially fast to a zero temperature
proﬁle: u(t, x) →0 as t →∞, representing the bar in its ﬁnal uniform thermal equilibrium.
The fact that its equilibrium temperature is zero is the result of holding both ends of the
bar ﬁxed at zero temperature, whereby any initial thermal energy is eventually dissipated
away through the ends. The small-scale temperature ﬂuctuations tend to rapidly cancel
out through diﬀusion of thermal energy, and the last term to disappear is the one with the
slowest decay, namely
u(t, x) ≈b1 exp

−γ π2
ℓ2
t

sin πx
ℓ,
where
b1 = 1
π
 π
0
f(x) sin x dx.
(4.28)
For generic initial data, the coeﬃcient b1 ̸= 0, and the solution approaches thermal equilib-
rium at an exponential rate prescribed by the smallest eigenvalue, λ1 = γ π2/ℓ2, which is
proportional to the thermal diﬀusivity divided by the square of the length of the bar. The

128
4 Separation of Variables
t = 0
t = .00001
t = .00005
t = .0001
t = .001
t = .01
Figure 4.2.
Denoising a signal with the heat equation.

longer the bar, or the smaller the diﬀusivity, the longer it takes for the eﬀect of holding the
ends at zero temperature to propagate along its entire length. Also, again provided b1 ̸= 0,
the asymptotic shape of the temperature proﬁle is a small, exponentially decaying sine arc,
just as we observed in Example 4.1. In exceptional situations, namely when b1 = 0, the
solution decays even faster, at a rate equal to the eigenvalue λk = γ k2 π2/ℓ2 corresponding
to the ﬁrst nonzero term, bk ̸= 0, in the Fourier series; its asymptotic shape now oscillates
k times over the interval.
Another, closely related, observation is that, for any ﬁxed time t > 0 after the initial
moment, the coeﬃcients in the Fourier sine series (4.23) decay exponentially fast as n →∞.
According to the discussion at the end of Section 3.3, this implies that the Fourier series
converges to an inﬁnitely diﬀerentiable function of x at each positive time t, no matter how
unsmooth the initial temperature proﬁle. We have discovered the basic smoothing property
of heat ﬂow, which we state for a general initial time t0.
Theorem 4.2. If u(t, x) is a solution to the heat equation with piecewise continuous
initial data f(x) = u(t0, x), or, more generally, initial data satisfying (4.27), then, for any
t > t0, the solution u(t, x) is an inﬁnitely diﬀerentiable function of x.
In other words, the heat equation instantaneously smoothes out any discontinuities
and corners in the initial temperature proﬁle by fast damping of the high-frequency modes.
The heat equation’s eﬀect on irregular initial data underlies its eﬀectiveness for smoothing
and denoising signals. We take the initial data u(0, x) = f(x) to be a noisy signal, and
then evolve the heat equation forward to a prescribed time t⋆> 0. The resulting function
g(x) = u(t⋆, x) will be a smoothed version of the original signal f(x) in which most of
the high-frequency noise has been eliminated. Of course, if we run the heat ﬂow for too
long, all of the low-frequency features will also be smoothed out and the result will be
a uniform, constant signal. Thus, the choice of stopping time t⋆is crucial to the success

4.1 The Diﬀusion and Heat Equations
129
of this method. Figure 4.2 shows the eﬀect of running the heat equation,† with γ = 1,
on a signal that has been contaminated by random noise. Observe how quickly the noise
is removed. By the ﬁnal time, the overall smoothing eﬀect of the heat ﬂow has caused
signiﬁcant degradation (blurring) of the original signal. The heat equation approach to
denoising has the advantage that no Fourier coeﬃcients need be explicitly computed, nor
does one need to reconstruct the smoothed signal. Basic numerical solution schemes for
the heat equation are to be discussed in Chapter 5.
An important theoretical consequence of the smoothing property is that diﬀusion is a
one-way process — one cannot run time backwards and accurately infer what a temperature
distribution looked like in the past. In particular, if the initial data u(0, x) = f(x) is not
smooth, then the value of u(t, x) for any t < 0 cannot be deﬁned, because if u(t0, x) were
deﬁned and integrable at some t0 < 0 then, by Theorem 4.2, u(t, x) would be smooth at all
subsequent times t > t0, including t = 0, in contradiction to our assumption. Moreover, for
most initial data, the Fourier coeﬃcients in the solution formula (4.23) are, at any t < 0,
exponentially growing as n →∞, indicating that high-frequency noise has completely
overwhelmed the solution, thereby precluding any kind of convergence of the Fourier series.
Mathematically, we can reverse future and past by changing t to −t. In the diﬀerential
equation, this merely reverses the sign of the time-derivative term; the x derivatives are
unaﬀected. Thus, by the above reasoning, the backwards heat equation
∂u
∂t = −γ ∂2u
∂x2 ,
with a negative diﬀusion coeﬃcient
−γ < 0,
(4.29)
is an ill-posed problem in the sense that small changes in the initial data — e.g., a small
perturbation of a high-frequency mode — can produce arbitrarily large changes in the
solution arbitrarily close to the initial time. In other words, the solution does not depend
continuously on the initial data. Even worse, for nonsmooth initial data, the solution is not
even well deﬁned in forwards time t > 0 (although it is well-posed if we run t backwards).
The same holds for more general diﬀusion processes, e.g., (4.6). If, as in all physically
relevant cases, the coeﬃcient of uxx is everywhere positive, then the initial value problem
is well-posed for t > 0, but ill-posed for t < 0. On the other hand, if the coeﬃcient is
everywhere negative, the reverse holds. A coeﬃcient that changes signs would cause the
diﬀerential equation to be ill-posed in both directions.
While theoretically undesirable, the unsmoothing eﬀect of the backwards heat equa-
tion has potential beneﬁts in certain contexts. For example, in image processing, diﬀusion
will gradually blur an image by damping out the high-frequency modes. Image enhance-
ment is the reverse process, and can be based on running the heat ﬂow backwards in some
stable manner. In forensics, determining the time of death based on the current temper-
ature of a corpse also requires running the equations governing the dissipation of body
heat backwards in time. One option would be to restrict the backwards evolution to the
ﬁrst few Fourier modes, which prevents the small-scale ﬂuctuations from overwhelming the
computation. Ill-posed problems also arise in the reconstruction of subterranean proﬁles
from seismic data, a central problem of the oil and gas industry. These and other applica-
tions are driving contemporary research into how to cleverly circumvent the ill-posedness
of backwards diﬀusion processes.
†
To avoid artifacts at the ends of the interval, we are, in fact, using periodic boundary
conditions in the plots.
Away from the ends, running the equation with Dirichlet boundary
conditions leads to almost identical results.

130
4 Separation of Variables
Remark: The irreversibility of the heat equation, along with the irreversibility of non-
linear transport in the presence of shock waves discussed in Section 2.3, highlight a crucial
distinction between partial diﬀerential equations and ordinary diﬀerential equations. Or-
dinary diﬀerential equations are always reversible — the existence, uniqueness, and con-
tinuous dependence properties of solutions are all equally valid in reverse time (although
their detailed qualitative and quantitative properties will, of course, depend upon whether
time is running forwards or backwards). The irreversibility and ill-posedness of partial
diﬀerential equations modeling thermodynamical, biological, and other diﬀusive processes
in our universe may explain why Time’s Arrow points exclusively to the future.
The Heated Ring Redux
Let us next consider the periodic boundary value problem modeling heat ﬂow in an in-
sulated circular ring.
We ﬁx the length of the ring to be ℓ= 2π, with −π ≤x ≤π
representing the “angular” coordinate around the ring. For simplicity, we also choose units
in which the thermal diﬀusivity is γ = 1. Thus, we seek to solve the heat equation
∂u
∂t = ∂2u
∂x2 ,
−π < x < π,
t > 0,
(4.30)
subject to periodic boundary conditions
u(t, −π) = u(t, π),
∂u
∂x (t, −π) = ∂u
∂x (t, π),
t ≥0,
(4.31)
that ensure continuity of the solution when the angular coordinate switches from −π to π.
The initial temperature distribution is
u(0, x) = f(x),
−π < x ≤π.
(4.32)
The resulting temperature u(t, x) will be a periodic function in x of period 2π.
Substituting the separable solution ansatz (3.15) into the heat equation and the bound-
ary conditions results in the periodic eigenvalue problem
d2v
dx2 + λ v = 0,
v(−π) = v(π),
v′(−π) = v′(π).
(4.33)
As we already noted in Section 3.1, the eigenvalues of this particular boundary value
problem are λn = n2, where n = 0, 1, 2, . . . is a nonnegative integer; the corresponding
eigenfunctions are the trigonometric functions
vn(x) = cos nx,
vn(x) = sin nx,
n = 0, 1, 2, . . . .
Note that λ0 = 0 is a simple eigenvalue, with constant eigenfunction cos 0x = 1 — the
sine solution sin 0x ≡0 is trivial — while the positive eigenvalues are, in fact, double, each
possessing two linearly independent eigenfunctions. The corresponding eigensolutions to
the heated ring equation (4.30–31) are
un(t, x) = e−n2t cos nx,
un(t, x) = e−n2t sin nx,
n = 0, 1, 2, 3, . . . .
The resulting inﬁnite series solution is
u(t, x) = 1
2 a0 +
∞

n=1

an e−n2 t cos nx + bn e−n2t sin nx

,
(4.34)

4.1 The Diﬀusion and Heat Equations
131
with as yet unspeciﬁed coeﬃcients an, bn. The initial conditions require
u(0, x) = 1
2 a0 +
∞

n=1
(an cos nx + bn sin nx) = f(x),
(4.35)
which is precisely the complete Fourier series (3.34) of the initial temperature proﬁle f(x).
Consequently,
an = 1
π
 π
−π
f(x) cos nx dx,
bn = 1
π
 π
−π
f(x) sin nx dx,
(4.36)
are its usual Fourier coeﬃcients (3.35).
As in the Dirichlet problem, after the initial instant, the high-frequency terms in the
series (4.34) become extremely small, since e−n2t ≪1 for n ≫0. Therefore, as soon as
t > 0, the solution instantaneously becomes smooth, and quickly degenerates into what is
in essence a ﬁnite sum over the ﬁrst few Fourier modes. Moreover, as t →∞, all of the
Fourier modes will decay to zero with the exception of the constant mode, associated with
the null eigenvalue λ0 = 0. Consequently, the solution will converge, at an exponential
rate, to a constant-temperature proﬁle,
u(t, x) −→
1
2 a0 =
1
2π
 π
−π
f(x) dx,
which equals the average of the initial temperature proﬁle. In physical terms, since the
insulation prevents any thermal energy from escaping the ring, it rapidly redistributes itself
so that the ring achieves a uniform constant temperature — its eventual equilibrium state.
Prior to attaining equilibrium, only the very lowest frequency Fourier modes will still
be noticeable, and so the solution will asymptotically look like
u(t, x) ≈1
2 a0 + e−t (a1 cos x + b1 sin x) = 1
2 a0 + r1 e−t cos(x + δ1),
(4.37)
where
a1 = r1 cos δ1 = 1
2π
 π
−π
f(x) cos x dx,
b1 = r1 sin δ1 = 1
2π
 π
−π
f(x) sin x dx.
Thus, for most initial data, the solution approaches thermal equilibrium at an exponential
rate of e−t. The exceptions are when a1 = b1 = 0, for which the rate of convergence is
even faster, namely at a rate e−k2 t, where k is the smallest integer such that at least one
of the kth order Fourier coeﬃcients ak, bk is nonzero.
In fact, once we are convinced that the bar must tend to thermal equilibrium as t →∞,
we can predict the ﬁnal temperature without knowing the explicit solution formula. Our
derivation in Section 4.1 implies that the heat equation has the form of a conservation law
(4.1), with the conserved density being the temperature u(t, x). As in (4.2), the integrated
form of the conservation law reads
d
dt
 π
−π
u(t, x) dx =
 π
−π
∂u
∂t (t, x) dx = γ
 π
−π
∂2u
∂x2 (t, x) dx
= γ
 ∂u
∂x (t, π) −∂u
∂x (t, −π)

= 0,
where the ﬂux terms cancel thanks to the periodic boundary conditions (4.31). Physically,
any ﬂux out of one end of the circular bar is immediately fed into the other, abutting end,

132
4 Separation of Variables
and so there is no net loss of thermal energy. We conclude that, for the periodic boundary
value problem, the total thermal energy
E(t) =
 π
−π
u(t, x) dx = constant
(4.38)
remains constant for all time. (In contrast, the thermal energy does not remain constant
for the Dirichlet boundary value problem, decaying steadily to 0 due to the out-ﬂux of heat
through the ends of the bar; see Exercise 4.1.13 for further details.)
Remark: More correctly, according to (4.3), the thermal energy is obtained by multi-
plying the temperature by the product, σ = ρ χ, of the density and the speciﬁc heat of the
body. For the heat equation, both are constant, and so the physical thermal energy equals
σ E(t). Mathematically, we can safely ignore this extra constant factor, or, equivalently,
work in physical units in which σ = 1. This does not extend to nonuniform bodies, whose
thermal energy is given by E(t) =
 π
−π
σ(x) u(t, x) dx, and whose constancy, under suitable
boundary conditions, follows from the conservation-law form (4.6) of the linear diﬀusion
equation.
In general, a system is in (static) equilibrium if it remains unaltered as time progresses.
Thus, any equilibrium conﬁguration has the form u = u⋆(x), and hence satisﬁes ∂u⋆/∂t = 0.
If, in addition, u⋆(x) is an equilibrium solution to the periodic heat equation (4.30–33),
then it must satisfy
∂u⋆
∂t = 0 = ∂2u⋆
∂x2 ,
u⋆(−π) = u⋆(π),
∂u⋆
∂x (−π) = ∂u⋆
∂x (π).
(4.39)
In other words, u⋆is a solution to the periodic boundary value problem (4.33) for the null
eigenvalue λ = 0. Thus, the null eigenfunctions (including the zero solution) are all the
possible equilibrium solutions. In particular, for the periodic boundary value problem, the
null eigenfunctions are constant, and therefore solutions to the periodic heat equation will
tend to a constant equilibrium temperature.
Now, once we know that the solution tends to a constant, u(t, x) →a as t →∞, then
its thermal energy tends to
E(t) =
 π
−π
u(t, x) dx −→
 π
−π
a dx = 2πa
as
t −→∞.
On the other hand, as we just demonstrated, the thermal energy is constant, so
E(t) = E(0) =
 π
−π
u(0, x) dx =
 π
−π
f(x) dx.
Combining these two, we conclude that
 π
−π
f(x) dx = 2πa,
and so the equilibrium temperature
a = 1
2π
 π
−π
f(x) dx
equals the average initial temperature. This reconﬁrms our earlier result, but avoids having
to know an explicit series solution formula. As a result, the latter method can be applied
to a much wider range of situations.

4.1 The Diﬀusion and Heat Equations
133
Inhomogeneous Boundary Conditions
So far, we have concentrated our attention on homogeneous boundary conditions. There is
a simple trick that will convert a boundary value problem with inhomogeneous but constant
Dirichlet boundary conditions,
∂u
∂t = γ ∂2u
∂x2 ,
u(t, 0) = α,
u(t, ℓ) = β,
t ≥0,
(4.40)
into a homogeneous Dirichlet problem. We begin by solving for the equilibrium temperature
proﬁle. As in (4.39), the equilibrium does not depend on t and hence satisﬁes the boundary
value problem
∂u⋆
∂t = 0 = γ ∂2u⋆
∂x2 ,
u⋆(0) = α,
u⋆(ℓ) = β.
Solving the ordinary diﬀerential equation yields u⋆(x) = a+b x, where the constants a, b are
ﬁxed by the boundary conditions. We conclude that the equilibrium solution is a straight
line connecting the boundary values:
u⋆(x) = α + β −α
ℓ
x.
(4.41)
The diﬀerence
u(t, x) = u(t, x) −u⋆(x) = u(t, x) −α −β −α
ℓ
x
(4.42)
measures the deviation of the solution from equilibrium. It clearly satisﬁes the homoge-
neous boundary conditions at both ends:
u(t, 0) = 0 = u(t, ℓ).
Moreover, by linearity, since both u(t, x) and u⋆(x) are solutions to the heat equation, so
is u(t, x). The initial data must be similarly adapted:
u(0, x) = u(t, x) −u⋆(x) = f(x) −α −β −α
ℓ
x ≡f(x).
(4.43)
Solving the resulting homogeneous initial-boundary value problem, we write u(t, x) in
Fourier series form (4.23), where the Fourier coeﬃcients are speciﬁed by the modiﬁed
initial data f(x) in (4.43). The solution to the inhomogeneous boundary value problem
thus has the series form
u(t, x) = α + β −α
ℓ
x +
∞

n=1
bn exp

−γ n2 π2
ℓ2
t

sin nπx
ℓ
,
(4.44)
where
bn = 2
ℓ
 ℓ
0
f(x) sin nπx
ℓ
dx,
n = 1, 2, 3, . . . .
(4.45)
Since u(t, 0) decays to zero at an exponential rate as t →∞, the actual temperature proﬁle
(4.44) will asymptotically decay to the equilibrium proﬁle,
u(t, x)
−→
u⋆(x) = α + β −α
ℓ
x,
at the same exponentially fast rate, governed by the ﬁrst eigenvalue λ1 = π2/ℓ2 — unless
b1 = 0, in which case the decay rate is even faster.

134
4 Separation of Variables
This method does not work as well when the boundary conditions are time-dependent:
u(t, 0) = α(t),
u(t, ℓ) = β(t).
Attempting to mimic the preceding technique, we discover that the deviation†
u(t, x) = u(t, x) −u⋆(t, x),
where
u⋆(t, x) = α(t) + β(t) −α(t)
ℓ
x,
(4.46)
satisﬁes the homogeneous boundary conditions, but now solves an inhomogeneous or forced
version of the heat equation:
∂u
∂t = ∂2u
∂x2 + h(t, x),
where
h(t, x) = −∂u⋆
∂t (t, x) = −α′(t) −β′(t) −α′(t)
ℓ
x.
(4.47)
Solution techniques for the latter partial diﬀerential equation will be discussed in Section 8.1
below.
Robin Boundary Conditions
Consider a bar of unit length and unit thermal diﬀusivity, insulated along its length,
which has one of its ends held at 0◦and the other put in a heat bath.
The resulting
thermodynamics are modeled by the heat equation subject to Dirichlet boundary conditions
at x = 0 and Robin boundary conditions at x = 1:
∂u
∂t = ∂2u
∂x2 ,
u(t, 0) = 0,
∂u
∂x (t, 1) + β u(t, 1) = 0,
(4.48)
where β ̸= 0 is a constant‡ that measures the rate of transfer of thermal energy, with β > 0
when the bath is cold and so the energy is being extracted from the bar. As before, the
general solution to the resulting initial-boundary value problem can be assembled from
the separable eigensolutions based on our usual exponential ansatz u(t, x) = e−λt v(x).
Substituting this expression into (4.48), we ﬁnd that the eigenfunction v(x) must satisfy
the boundary value problem
−d2v
dx2 = λ v,
v(0) = 0,
v′(1) + β v(1) = 0.
(4.49)
In order to ﬁnd nontrivial solutions v(x) ̸≡0 to (4.49), let us ﬁrst assume λ = ω2 > 0,
where, without loss of generality, ω > 0. The solution to the ordinary diﬀerential equation
that satisﬁes the Dirichlet boundary condition at x = 0 is a constant multiple of v(x) =
sin ω x. Substituting this function into the Robin boundary condition at x = 1, we ﬁnd
ω cos ω + β sin ω = 0,
or, equivalently,
ω = −β tan ω.
(4.50)
It is not hard to see that there is an inﬁnite number of real, positive solutions 0 < ω1 < ω2 <
ω3 < · · · →∞to the latter transcendental equation. Indeed, they can be characterized as
the abscissas ωn > 0 of the intersection points of the graphs of the two functions f(ω) = ω
†
In this case, u⋆(t, x) is not an equilibrium solution. Indeed, we do not expect the bar to go
to equilibrium if the temperature of its endpoints is constantly changing.
‡
The case β = 0 reduces to the mixed boundary value problem, whose analysis is left to the
reader.

4.1 The Diﬀusion and Heat Equations
135
ω1
ω2
ω3
λ > 0, β = 1.
λ < 0, β = −.5.
ω0
λ < 0, β = −2.
Figure 4.3.
Eigenvalue equation for Robin boundary conditions.
and g(ω) = −β tan ω, as shown in the ﬁrst plot in Figure 4.3. Each root ωn deﬁnes a
positive eigenvalue λn = ω2
n > 0 to the boundary value problem (4.49) and hence an
exponentially decaying eigensolution
un(t, x) = e−λn t sin ωnx
(4.51)
to the Robin boundary value problem (4.48).
While there is no explicit formula, nu-
merical approximations to the eigenvalues are easily found via a numerical root ﬁnder,
e.g., Newton’s Method, [24, 94]. In particular, for β = 1, the ﬁrst three eigenvalues are
λ1 = ω2
1 ≈4.1159, λ2 = ω2
2 ≈24.1393, λ3 = ω2
3 ≈63.6591.
What about a zero eigenvalue? If λ = 0 in (4.49), then the solution to the ordinary
diﬀerential equation that satisﬁes the Dirichlet boundary condition is a constant multiple
of v(x) = x. This function satisﬁes the Robin boundary condition v′(1) + β v(1) = 0 if and
only if β = −1. In this special conﬁguration, the heat equation admits a time-independent
eigensolution u0(t, x) = x with eigenvalue λ0 = 0. Physically, the rate of transfer of thermal
energy into the bar through its end in the heat bath is exactly enough to cancel the heat
loss through the Dirichlet end, resulting in a steady-state solution. All other eigenmodes
correspond to positive eigenvalues, and hence are exponentially decaying.
The general
solution decays to the steady state, which is a constant multiple of the null eigensolution:
u(t, x) →c x as t →∞, at an exponential rate prescribed, generically, by the ﬁrst positive
eigenvalue λ1 > 0.
However, in contrast to the more common types of boundary conditions (Dirichlet,
Neumann, mixed, periodic), we cannot automatically rule out the existence of negative
eigenvalues in the Robin case. Suppose λ = −ω2 < 0 with ω > 0. Now the solution to
(4.49) that satisﬁes the Dirichlet boundary condition at x = 0 is a constant multiple of
the hyperbolic sine function v(x) = sinh ω x. Substituting this expression into the Robin
boundary condition at x = 1 produces
ω cosh ω + β sinh ω = 0,
or, equivalently,
ω = −β tanh ω,
(4.52)
where
tanh ω = sinh ω
cosh ω = eω −e−ω
eω + e−ω
(4.53)
is the hyperbolic tangent. If β > −1, there are no solutions ω > 0 to this transcendental
equation, and in this case all the eigenvalues are strictly positive and all solutions to the

136
4 Separation of Variables
heat equation are exponentially decaying. On the other hand, if β < −1, there is a single
solution ω0 > 0, which produces a single negative eigenvalue λ0 = −ω2
0. Representative
graphs illustrating the two possibilities appear in Figure 4.3; in the ﬁrst, the graph of
f(ω) = ω does not intersect the graph of g(ω) = 1
2 tanh ω when ω > 0, whereas it intersects
the graph of g(ω) = 2 tanh ω at a single point, with abscissa ω0 ≈1.9150, producing the
negative eigenvalue λ0 ≈−ω2
0 ≈−3.6673. Thus, when β < −1, there is, in addition to all
the exponentially decaying eigenmodes associated with the positive eigenvalues, a single
unstable exponentially growing eigenmode
u0(t, x) = eλ0 t sinh ω0 x.
(4.54)
Physically, β < −1 implies that thermal energy is entering the Robin end of the bar at a
faster rate than can be removed through the Dirichlet end, and hence the bar experiences
an exponential increase in its overall temperature.
Remark: Even though some Robin boundary conditions admit exponentially growing
solutions, and hence lead to unstable dynamics, the initial-boundary value problem remains
well-posed because the solution exists and is uniquely determined by the initial data, and,
moreover, small changes in the initial conditions induce relatively small changes in the
resulting solution on bounded time intervals.
The Root Cellar Problem
As a ﬁnal example, we discuss a problem that involves analysis of the heat equation on
a semi-inﬁnite interval. The question is this: how deep should you dig a root cellar? In
the prerefrigeration era, a root cellar was used to keep food cool in the summer, but not
freeze in the winter. We assume that the temperature inside the Earth depends only on
the depth and the time of year. Let u(t, x) denote the deviation in the temperature from
its annual mean at depth x > 0 and time t. We shall assume that the temperature at the
Earth’s surface, x = 0, ﬂuctuates in a periodic manner; speciﬁcally, we set
u(t, 0) = a cos ω t,
(4.55)
where the oscillatory frequency
ω =
2π
365.25 days = 2.0 × 10−7sec−1
(4.56)
refers to yearly temperature variations. In this model, we shall ignore daily temperature
ﬂuctuations, since their eﬀect is not signiﬁcant below a very thin surface layer. At large
depths the temperature is assumed to be unvarying:
u(t, x) −→0
as
x −→∞,
(4.57)
where 0 refers to the mean temperature.
Thus, we must solve the heat equation on a semi-inﬁnite bar 0 < x < ∞, with time-
dependent boundary conditions (4.55, 57) at the ends. The analysis will be simpliﬁed a
little if we replace the cosine by a complex exponential, and so we look for a complex
solution with boundary conditions
u(t, 0) = a e i ω t,
lim
x →∞u(t, x) = 0.
(4.58)

4.1 The Diﬀusion and Heat Equations
137
Let us try a separable solution of the form
u(t, x) = v(x) e i ω t.
(4.59)
Substituting this expression into the heat equation ut = γ uxx leads to
i ω v(x) e i ω t = γ v′′(x) e i ω t.
Canceling the common exponential factors, we conclude that v(x) should solve the bound-
ary value problem
γ v′′(x) = i ω v,
v(0) = a,
lim
x →∞v(x) = 0.
The solutions to the ordinary diﬀerential equation are
v1(x) = e
√
i ω/γ x = e
√
ω/(2γ) (1+ i ) x,
v2(x) = e−√
i ω/γ x = e−√
ω/(2γ) (1+ i ) x .
The ﬁrst solution is exponentially growing as x →∞, and so not germane to our prob-
lem.
The solution to the boundary value problem must therefore be a multiple of the
exponentially decaying solution:
v(x) = a e−√
ω/(2γ) (1+ i ) x.
Substituting back into (4.59), we ﬁnd the (complex) solution to the root cellar problem to
be
u(t, x) = a e−x√
ω/(2γ) e i (ω t−√
ω/(2γ) x).
(4.60)
The corresponding real solution is obtained by taking the real part,
u(t, x) = a e−x√
ω/(2γ) cos

ω t −
 ω
2γ x

.
(4.61)
The ﬁrst factor in (4.61) is exponentially decaying as a function of the depth. Thus, the
further underground one is, the less noticeable is the eﬀect of the surface temperature
ﬂuctuations. The second factor is periodic in time, with the same annual frequency ω. The
interesting feature is that the temperature variations (4.61) are typically out of phase with
respect to the surface temperature ﬂuctuations, having an overall phase lag of
δ =
 ω
2γ x
that depends linearly on the depth x. In particular, a cellar built at a depth where δ is an
odd multiple of π will be completely out of phase, being hottest in the winter, and coldest
in the summer. Thus, the (shallowest) ideal depth at which to build a root cellar would
take δ = π, corresponding to a depth of
x = π

2γ
ω .
(4.62)
For typical soils in the Earth, γ ≈10−6 meters2 sec−1, and so, with ω given by (4.56),
x ≈9.9 meters. However, at this depth, the relative amplitude of the oscillations is
e−x√
ω/2γ = e−π = .04,
and hence there is only a 4% temperature ﬂuctuation. In Minneapolis, the temperature
varies, roughly, from −40◦C to +40◦C, and hence our 10-meter-deep root cellar would

138
4 Separation of Variables
experience only a 3.2◦C annual temperature deviation from the winter, when it is the
warmest, to the summer, when it is the coldest. Building the cellar twice as deep would
lead to a temperature ﬂuctuation of .2%, now in phase with the surface variations, which
means that the cellar would be, for all practical purposes, at constant temperature year
round.
Exercises
4.1.1. Suppose the ends of a bar of length 1 and thermal diﬀusivity γ = 1 are held ﬁxed at
respective temperatures 0◦and 10◦. (a) Determine the equilibrium temperature proﬁle.
(b) Determine the rate at which the equilibrium temperature proﬁle is approached.
(c) What does the temperature proﬁle look like as it nears equilibrium?
4.1.2. A uniform insulated bar 1 meter long is stored at room temperature of 20◦Celsius. An
experimenter places one end of the bar in boiling water and the other end in ice water.
(a) Set up an initial-boundary value problem that models the temperature in the bar.
(b) Find the equilibrium temperature distribution.
(c) Discuss how your answer depends on the material properties of the bar.
4.1.3. Consider the initial-boundary value problem
∂u
∂t = ∂2u
∂x2 ,
u(t, 0) = 0 = u(t, 10),
t > 0,
u(0, x) = f(x),
0 < x < 10,
for the heat equation where the initial data has the following form:
f(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
x −1,
1 ≤x ≤2,
11 −5x,
2 ≤x ≤3,
5x −19,
3 ≤x ≤4,
5 −x,
4 ≤x ≤5,
0,
otherwise.
Discuss what happens to the solution as t increases. You do not need to write down an ex-
plicit formula, but for full credit you must explain (sketches can help) at least three or four
interesting things that happen to the solution as time progresses.
4.1.4. Find a series solution to the initial-boundary value problem for the heat equation
ut = uxx for 0 < x < 1 when one the end of the bar is held at 0◦and the other is insulated.
Discuss the asymptotic behavior of the solution as t →∞.
4.1.5. Answer Exercise 4.1.4 when both ends of the bar are insulated.
4.1.6. A metal bar, of length ℓ= 1 meter and thermal diﬀusivity γ = 2, is taken out of a 100◦
oven and then fully insulated except for one end, which is ﬁxed to a large ice cube at 0◦.
(a) Write down an initial-boundary value problem that describes the temperature u(t, x) of
the bar at all subsequent times.
(b) Write a series formula for the temperature distribu-
tion u(t, x) at time t > 0.
(c) What is the equilibrium temperature distribution in the bar,
i.e., for t ≫0? How fast does the solution go to equilibrium?
(d) Just before the tempera-
ture distribution reaches equilibrium, what does it look like? Sketch a picture and discuss.
4.1.7. A metal bar of length ℓ= 1 and thermal diﬀusivity γ = 1 is fully insulated, including its
ends. Suppose the initial temperature distribution is u(0, x) =
⎧
⎨
⎩
x,
0 ≤x ≤1
2,
1 −x,
1
2 ≤x ≤1.
(a) Use Fourier series to write down the temperature distribution at time t > 0.

4.1 The Diﬀusion and Heat Equations
139
(b) What is the equilibrium temperature distribution in the bar, i.e., for t ≫0?
(c) How fast does the solution go to equilibrium? (d) Just before the temperature distribu-
tion reaches equilibrium, what does it look like? Sketch a picture and discuss.
4.1.8.(a) Find the series solution to the heat equation ut = uxx on −2 < x < 2, t > 0, when
subject to the Dirichlet boundary conditions u(t, −2) = u(t, 2) = 0 and the initial condi-
tion u(0, x) =
 x,
| x | < 1,
0,
otherwise.
(b) Sketch a graph of the solution at some representative
times.
(c) At what rate does the temperature approach thermal equilibrium?
4.1.9. Solve the heat equation when the right-hand end of a bar of unit length is held at a ﬁxed
constant temperature α while the left-hand end is insulated. Discuss the asymptotic behav-
ior of the solution.
4.1.10. For each of the following initial temperature distributions, (i) write out the Fourier se-
ries solution to the heated ring (4.30–32), and (ii) ﬁnd the resulting equilibrium tempera-
ture as t →∞:
(a) cos x,
(b) sin3 x,
(c) | x |,
(d)
 1,
−π < x < 0,
0,
0 < x < π.
♦4.1.11. Suppose that the temperature u(t, x) of a homogeneous bar satisﬁes the heat equation.
Show that the associated heat ﬂux w(t, x) is also a solution to the same heat equation.
♦4.1.12. Show that the time derivative v = ut of any solution to the heat equation is also a so-
lution. If u(t, x) satisﬁes the initial condition u(0, x) = f(x), what initial condition does
v(t, x) inherit?
♦4.1.13. Explain why the thermal energy E(t) =
	 ℓ
0 u(t, x) dx is not constant for the Dirichlet
initial-boundary value problem for the heat equation on the interval [0, ℓ].
♦4.1.14.(a) Show that the thermal energy E(t) =
	 ℓ
0 u(t, x) dx is constant for the Neumann
boundary value problem on the interval [0, ℓ].
(b) Use part (a) to prove that the constant
equilibrium solution for the homogeneous Neumann boundary value problem is equal to the
mean initial temperature u(0, x).
4.1.15. Let u(t, x) be any nonconstant solution to the periodic heat equation (4.30–31). Prove
that the squared L2 norm of the solution, N(t) =
	 π
−π u(t, x)2 dx, is a strictly decreasing
function of t. Remark: Interestingly, comparing this result with formula (4.38), we ﬁnd
that, for the periodic boundary value problem, the integral of u is constant, but the inte-
gral of u2 is strictly decreasing. How is this possible?
♥4.1.16. The cable equation vt = γ vxx −αv, with γ, α > 0, also known as the lossy heat
equation,was derived by the nineteenth-century Scottish physicist William Thomson to
model propagation of signals in a transatlantic cable. Later, in honor of his work on ther-
modynamics, including determining the value of absolute zero temperature, he was named
Lord Kelvin by Queen Victoria. The cable equation was later used to model the electrical
activity of neurons. (a) Show that the general solution to the cable equation is given by
v(t, x) = e−α t u(t, x), where u(t, x) solves the heat equation ut = γ uxx.
(b) Find a Fourier series solution to the Dirichlet initial-boundary value problem
vt = γ vxx −α v,
v(0, x) = f(x),
v(t, 0) = 0 = v(t, 1),
0 ≤x ≤1,
t > 0.
Does your solution approach an equilibrium value? If so, how fast?
(c) Answer part (b) for the Neumann problem
vt = γ vxx −α v,
v(0, x) = f(x),
vx(t, 0) = 0 = vx(t, 1),
0 ≤x ≤1,
t > 0.
♦4.1.17. The convection-diﬀusion equation ut + cux = γ uxx is a simple model for the diﬀusion
of a pollutant in a ﬂuid ﬂow moving with constant speed c. Show that v(t, x) = u(t, x + ct)
solves the heat equation. What is the physical interpretation of this change of variables?
4.1.18. Combine Exercises 4.1.16–17 to solve the lossy convection-diﬀusion equation
ut = γ uxx + cux −αu.

140
4 Separation of Variables
♦4.1.19. Let γ > 0 and λ ≤0. (a) Find all solutions to the diﬀerential equation γ v′′ + λ v = 0.
(b) Prove that the only solution that satisﬁes the boundary conditions v(0) = 0, v(ℓ) = 0,
is the zero solution v(x) ≡0.
♦4.1.20. Answer Exercise 4.1.19 when λ is a non-real complex number.
4.2 The Wave Equation
Let us return to the one-dimensional wave equation
∂2u
∂t2 = c2 ∂2u
∂x2 ,
(4.63)
with constant wave speed c > 0, used to model the vibrations of bars and strings. In Chap-
ter 2, we learned how to explicitly solve the wave equation by the method of d’Alembert.
Unfortunately, d’Alembert’s approach does not extend to other equations of interest to us,
and so alternative solution techniques, particularly those based on Fourier methods, are
worth developing. Indeed, the resulting series solutions provide valuable insight into wave
dynamics on bounded intervals.
Separation of Variables and Fourier Series Solutions
One of the oldest — and still one of the most widely used — techniques for constructing
explicit analytic solutions to a wide range of linear partial diﬀerential equations is the
method of separation of variables. We have, in fact, already employed a simpliﬁed version
of the method when constructing each eigensolution to the heat equation as an exponential
function of t times a function of x. In general, the separation of variables method seeks
solutions to the partial diﬀerential equation that can be written as the product of functions
of the individual independent variables. For the wave equation, we seek solutions
u(t, x) = w(t) v(x)
(4.64)
that can be written as the product of a function of t alone and a function of x alone.
When the method succeeds (which is not guaranteed in advance), both factors are found
as solutions to certain ordinary diﬀerential equations.
Let us see whether such an expression can possibly solve the wave equation. First of
all,
∂2u
∂t2 = w′′(t) v(x),
∂2u
∂x2 = w(t) v′′(x),
where the primes indicate ordinary derivatives. Substituting these expressions into the
wave equation (4.63), we obtain
w′′(t) v(x) = c2 w(t) v′′(x).
Dividing both sides by w(t) v(x) (which we assume is not identically zero, since otherwise,
the solution would be trivial) yields
w′′(t)
w(t) = c2 v′′(x)
v(x) ,

4.2 The Wave Equation
141
which eﬀectively “separates” the t and x variables on each side of the equation, whence
the name “separation of variables”.
Now, how could a function of t alone be equal to a function of x alone? A moment’s
reﬂection should convince the reader that this can happen if and only if the two functions
are constant,† so
w′′(t)
w(t) = c2 v′′(x)
v(x) = λ,
(4.65)
where we use λ to indicate the common separation constant. Thus, the individual factors
w(t) and v(x) must satisfy ordinary diﬀerential equations
d2w
dt2 −λ w = 0,
d2v
dx2 −λ
c2 v = 0,
as promised. We already know how to solve both of these ordinary diﬀerential equations
by elementary techniques. There are three diﬀerent cases, depending on the sign of the
separation constant λ. As a result, each value of λ leads to four independent separable
solutions to the wave equation, as listed in the accompanying table.
Separable Solutions to the Wave Equation
λ
w(t)
v(x)
u(t, x) = w(t) v(x)
λ = −ω2 < 0
cos ωt, sin ωt
cos ωx
c , sin ωx
c
cos ωt cos ωx
c ,
sin ωt cos ωx
c ,
cos ωt sin ωx
c ,
sin ωt sin ωx
c
λ = 0
1, t
1, x
1, x, t, tx
λ = ω2 > 0
e−ω t, eω t
e−ω x/c, eω x/c
e−ω (t+x/c),
e−ω (t−x/c),
eω (t−x/c),
eω (t+x/c)
So far, we have not taken the boundary conditions into account. Consider ﬁrst the
case of a string of length ℓwith two ﬁxed ends, and thus subject to homogeneous Dirichlet
boundary conditions
u(t, 0) = 0 = u(t, ℓ).
Substituting the separable ansatz (4.65), we ﬁnd that v(x) must satisfy
d2v
dx2 −λ
c2 v = 0,
v(0) = 0 = v(ℓ).
(4.66)
The complete system of (nontrivial) solutions to this boundary value problem were found
in (4.21):
vn(x) = sin nπx
ℓ
,
λn = −
"nπc
ℓ
#2
,
n = 1, 2, 3, . . . .
†
Technical detail: one should assume that the underlying domain is connected for this to be
valid as stated. In practice, this technicality can be safely ignored.

142
4 Separation of Variables
Hence, according to the table, the corresponding separable solutions are
un(t, x) = cos nπc t
ℓ
sin nπx
ℓ
,
un(t, x) = sin nπc t
ℓ
sin nπx
ℓ
.
(4.67)
We will now employ these solutions to construct a candidate series solution to the wave
equation subject to the prescribed boundary conditions:
u(t, x) =
∞

n=1

bn cos nπc t
ℓ
sin nπx
ℓ
+ dn sin nπc t
ℓ
sin nπx
ℓ

.
(4.68)
The solution is thus a linear combination of the natural Fourier modes vibrating with
frequencies
ωn = nπc
ℓ
= nπ
ℓ
κ
ρ ,
n = 1, 2, 3, . . . ,
(4.69)
where the second expression follows from (2.66). Observe that, the longer the length ℓ
of the string, or the higher its density ρ, the slower the vibrations, whereas increasing its
stiﬀness or tension κ speeds them up — in exact accordance with our physical intuition.
The Fourier coeﬃcients bn and dn in (4.68) will be uniquely determined by the initial
conditions
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
0 < x < ℓ.
Diﬀerentiating the series term by term, we discover that we must represent the initial
displacement and velocity as Fourier sine series
u(0, x) =
∞

n=1
bn sin nπx
ℓ
= f(x),
∂u
∂t (0, x) =
∞

n=1
dn
nπc
ℓ
sin nπx
ℓ
= g(x).
Therefore,
bn = 2
ℓ
 ℓ
0
f(x) sin nπx
ℓ
dx,
n = 1, 2, 3, . . . ,
(4.70)
are the Fourier sine coeﬃcients (3.85) of the initial displacement f(x), while
dn =
2
nπc
 ℓ
0
g(x) sin nπx
ℓ
dx,
n = 1, 2, 3, . . . .
(4.71)
are rescaled versions of the Fourier sine coeﬃcients of the initial velocity g(x).
Example 4.3.
A string of unit length ﬁxed at both ends is held taut at its center
and then released. Our task is to describe the ensuing vibrations. Let us assume that the
physical units are chosen so that c2 = 1, and so we are asked to solve the initial-boundary
value problem
utt = uxx,
u(0, x) = f(x),
ut(0, x) = 0,
u(t, 0) = u(t, 1) = 0.
(4.72)
To be speciﬁc, we assume that the center of the string has been moved by half a unit, and
so the initial displacement is
f(x) =

x,
0 ≤x ≤1
2,
1 −x,
1
2 ≤x ≤1.

4.2 The Wave Equation
143
t = 0
t = .2
t = .4
t = .6
t = .8
t = 1
Figure 4.4.
Plucked string solution of the wave equation.

The vibrational frequencies ωn = nπ are the integral multiples of π, and so the natural
modes of vibration are
cos nπt sin nπx
and
sin nπt sin nπx
for
n = 1, 2, . . . .
Consequently, the general solution to the boundary value problem is
u(t, x) =
∞

n=1

bn cos nπt sin nπx + dn sin nπt sin nπx

,
where
bn = 2
 1
0
f(x) sin nπx dx =
⎧
⎨
⎩
4
 1/2
0
x sin nπ x dx =
4 (−1)k
(2k + 1)2 π2 ,
n = 2k + 1,
0,
n = 2k,
while dn = 0. Therefore, the solution is the Fourier sine series
u(t, x) = 4
π2
∞

k=0
(−1)k cos(2k + 1)π t sin(2k + 1)πx
(2k + 1)2
,
(4.73)
whose proﬁle is depicted in Figure 4.4. At time t = 1, the original displacement is re-
produced exactly, but upside down. The subsequent dynamics proceeds as before, but in
mirror-image form. The original displacement reappears at time t = 2, after which time

144
4 Separation of Variables
the motion is periodically repeated. Interestingly, at times tk = .5, 1.5, 2.5, . . . , the dis-
placement is identically zero, u(tk, x) ≡0, although the velocity is not, ut(tk, x) ̸≡0. The
solution appears to be piecewise aﬃne, i.e., its graph is a collection of straight lines. This
can, in fact, be proved as a consequence of the d’Alembert formula; see Exercise 4.2.13.
Observe that, unlike the heat equation, the wave equation does not smooth out discontinu-
ities and corners in the initial data. And, although we will loosely refer to such piecewise
C2 functions as “solutions”, they are not, in fact, classical solutions. (Their status as weak
solutions, though, can be established using the methods of Section 10.4.)
While the series form (4.68) of the solution is perhaps less satisfying than a d’Alembert-
style formula, we can still use it to deduce important qualitative properties. First of all,
since each term is periodic in t with period 2 ℓ/c, the entire solution is time periodic with
that period: u(t + 2ℓ/c, x) = u(t, x). In fact, after half a period, the solution reduces to
u
ℓ
c , x

=
∞

n=1
(−1)n bn sin nπx
ℓ
= −
∞

n=1
bn sin nπ(ℓ−x)
ℓ
= −u(0, ℓ−x) = −f(ℓ−x).
In general,
u

t + ℓ
c , x

= −u(t, ℓ−x),
u

t + 2 ℓ
c , x

= u(t, x).
(4.74)
Therefore, the initial wave form is reproduced, ﬁrst as an upside down mirror image of
itself at time t = ℓ/c, and then in its original form at time t = 2 ℓ/c. This has the impor-
tant consequence that vibrations of (homogeneous) one-dimensional media are inherently
periodic, because the fundamental frequencies (4.69) are all integer multiples of the lowest
one: ωn = nω1.
Remark: The immediately preceding remark has important musical consequences. To
the human ear, sonic vibrations that are integral multiples of a single frequency, and thus
periodic in time, sound harmonious, whereas those with irrationally related frequencies,
and hence experiencing aperiodic vibrations, sound dissonant.
This is why most tonal
instruments rely on vibrations in one dimension, be it a violin or piano string, a column
of air in a wind instrument (ﬂute, clarinet, trumpet, or saxophone), a xylophone bar, or
a triangle. On the other hand, most percussion instruments rely on the vibrations of two-
dimensional media, e.g., drums and cymbals, or three-dimensional solid bodies, e.g., blocks.
As we shall see in Chapters 11 and 12, the frequency ratios of the latter are irrationally
related, and hence their motion is only quasiperiodic, as in Example 2.20. For some reason,
our appreciation of music is psychologically attuned to the diﬀerences between rationally
related/periodic and irrationally related/quasiperiodic vibrations, [105].
Consider next a string with both ends left free, and so subject to the Neumann bound-
ary conditions
∂u
∂x (t, 0) = 0 = ∂u
∂x (t, ℓ).
(4.75)
The solutions of (4.66) satisfying v′(0) = 0 = v′(ℓ) are now
vn(x) = cos nπx
ℓ
with
ωn = nπc
ℓ
,
n = 0, 1, 2, 3, . . . .
The resulting solution takes the form of a Fourier cosine series
u(t, x) = a0 + c0 t +
∞

n=1

an cos nπc t
ℓ
cos nπx
ℓ
+ cn sin nπc t
ℓ
cos nπx
ℓ

.
(4.76)

4.2 The Wave Equation
145
The ﬁrst two terms come from the null eigenfunction v0(x) = 1 with ω0 = 0. The string
vibrates with the same fundamental frequencies (4.69) as in the ﬁxed-end case, but there
is now an additional unstable mode c0 t that is no longer periodic, but grows linearly in
time. In general, the presence of null eigenfunctions implies that the wave equation admits
unstable modes.
Substituting (4.76) into the initial conditions
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
0 < x < ℓ,
we ﬁnd that the Fourier coeﬃcients are prescribed, as before, by the initial displacement
and velocity:
an = 2
ℓ
 ℓ
0
f(x) cos nπx
ℓ
dx,
cn =
2
nπc
 ℓ
0
g(x) cos nπx
ℓ
dx,
n = 1, 2, 3, . . . .
The order-zero coeﬃcients†
a0 = 1
ℓ
 ℓ
0
f(x) dx,
c0 = 1
ℓ
 ℓ
0
g(x) dx,
are equal to the average initial displacement and average initial velocity of the string. In
particular, when c0 = 0, there is no net initial velocity, and the unstable mode is not
excited. In this case, the solution is time-periodic, oscillating around the position given by
the average initial displacement. On the other hand, if c0 ̸= 0, the string will move oﬀwith
constant average speed c0, all the while vibrating at the same fundamental frequencies.
Similar considerations apply to the periodic boundary value problem for the wave
equation on a circular ring. The details are left as Exercise 4.2.6 for the reader.
Exercises
4.2.1. In music, an octave corresponds to doubling the frequency of the sound waves. On my
piano, the middle C string has length .7 meter, while the string for the C an octave higher
has length .6 meter. Assuming that they have the same density, how much tighter does the
shorter string need to be tuned?
4.2.2. How much longer would a piano string have to be to make the same sound when it is
pulled twice as tight?
4.2.3. Write down the solutions to the following initial-boundary value problems for the wave
equation in the form of a Fourier series:
(a) utt = uxx, u(t, 0) = u(t, π) = 0, u(0, x) = 1, ut(0, x) = 0;
(b) utt = 2uxx, u(t, 0) = u(t, π) = 0, u(0, x) = 0, ut(0, x) = 1;
(c) utt = 3uxx, u(t, 0) = u(t, π) = 0, u(0, x) = sin3 x, ut(0, x) = 0;
(d) utt = 4uxx, u(t, 0) = u(t, 1) = 0, u(0, x) = x, ut(0, x) = −x;
(e) utt = uxx, u(t, 0) = ux(t, 1) = 0, u(0, x) = 1, ut(0, x) = 0;
(f ) utt = 2uxx, ux(t, 0) = ux(t, 2π) = 0, u(0, x) = −1, ut(0, x) = 1;
(g) utt = uxx, ux(t, 0) = ux(t, 1) = 0, u(0, x) = x(1 −x), ut(0, x) = 0.
†
Note that we have not included the usual 1
2 factor in the constant terms in the Fourier series
(4.76).

146
4 Separation of Variables
4.2.4. Find all separable solutions to the wave equation utt = uxx on the interval 0 ≤x ≤π
subject to (a) mixed boundary conditions u(t, 0) = 0, ux(t, π) = 0;
(b) Neumann boundary conditions ux(t, 0) = 0, ux(t, π) = 0.
4.2.5.(a) Under what conditions is the solution to the Neumann boundary value problem (4.75)
a periodic function of t? What is the period? (b) Establish explicit periodicity formulas of
the form (4.74). (c) Under what conditions is the velocity ∂u/∂t periodic in t?
♥4.2.6.(a) Formulate the periodic initial-boundary value problem for the wave equation on the
interval −π ≤x ≤π, modeling the vibrations of a circular ring. (b) Write out a formula for
the solution to your problem in the form of a Fourier series. (c) Is the solution a periodic
function of t? If so, what is the period? (d) Suppose the initial displacement coincides with
that in Figure 4.6, while the initial velocity is zero. Describe what happens to the solution
as time evolves.
4.2.7. Show that the time derivative, v = ∂u/∂t, of any solution to the wave equation is also a
solution. If you know the initial conditions of u, what initial conditions does v satisfy?
4.2.8. Find all the separable real solutions to the wave equation subject to a restoring force:
utt = uxx −u. Discuss their long-term behavior.
♥4.2.9. Let a, c > 0 be positive constants. The telegrapher’s equation utt + aut = c2 uxx repre-
sents a damped version of the wave equation. Consider the Dirichlet boundary value prob-
lem u(t, 0) = u(t, 1) = 0, on the interval 0 ≤x ≤1, with initial conditions u(0, x) = f(x),
ut(0, x) = 0. (a) Find all separable solutions to the telegrapher’s equation that satisfy the
boundary conditions. (b) Write down a series solution for the initial boundary value prob-
lem. (c) Discuss the long term behavior of your solution. (d) State a criterion that distin-
guishes overdamped from underdamped versions of the equation.
4.2.10. The fourth-order partial diﬀerential equation utt = −uxxxx is a simple model for a vi-
brating elastic beam. (a) Find all separable real solutions to the beam equation. (b) Show
that any (complex) solution to the Schr¨odinger equation i ut = uxx solves the beam equa-
tion.
4.2.11. The initial-boundary value problem
utt = −uxxxx,
u(t, 0) = uxx(t, 0) = u(t, 1) = uxx(t, 1) = 0,
u(0, x) = f(x),
ut(0, x) = 0,
0 < x < 1,
t > 0,
models the vibrations of an elastic beam of unit length with simply supported ends, sub-
ject to a nonzero initial displacement f(x) and zero initial velocity. (a) What are the vibra-
tional frequencies for the beam?
(b) Write down the solution to the initial-boundary value
problem as a Fourier series.
(c) Does the beam vibrate periodically
(i) for all initial conditions? (ii) for some initial conditions? (iii) for no initial conditions?
4.2.12. Multiple choice: The initial-boundary value problem
utt = uxxxx,
u(t, 0) = uxx(t, 0) = u(t, 1) = uxx(t, 1) = 0,
u(0, x) = f(x),
ut(0, x) = g(x),
0 < x < 1,
t > 0,
is well-posed for (a) t > 0; (b) t < 0; (c) all t; (d) no t. Explain your answer.
The d’Alembert Formula for Bounded Intervals
In Theorem 2.15, we derived the explicit d’Alembert formula
u(t, x) = f(x −c t) + f(x + c t)
2
+ 1
2 c
 x+c t
x−c t
g(z) dz,
(4.77)

4.2 The Wave Equation
147
Figure 4.5.
Odd periodic extension of a concentrated pulse.
for solving the basic initial value problem for the wave equation on an inﬁnite interval:
∂2u
∂t2 = c2 ∂2u
∂x2 ,
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
−∞< x < ∞.
In this section we explain how to adapt the formula in order to solve initial-boundary value
problems on bounded intervals, thereby eﬀectively summing the Fourier series solution.
The easiest case to deal with is the periodic problem on 0 ≤x ≤ℓ, with boundary
conditions
u(t, 0) = u(t, ℓ),
ux(t, 0) = ux(t, ℓ).
(4.78)
If we extend the initial displacement f(x) and velocity g(x) to be periodic functions of
period ℓ, so f(x+ℓ) = f(x) and g(x+ℓ) = g(x) for all x ∈R, then the resulting d’Alembert
solution (4.77) will also be periodic in x, so u(t, x + ℓ) = u(t, x). In particular, it satisﬁes
the boundary conditions (4.78) and so coincides with the desired solution. Details are to
be supplied in Exercises 4.2.27–28.
Next, suppose we have ﬁxed (Dirichlet) boundary conditions
u(t, 0) = 0,
u(t, ℓ) = 0.
(4.79)
The resulting solution can be written as a Fourier sine series (4.68), and hence is both odd
and 2ℓ–periodic in x. Therefore, to write the solution in d’Alembert form (4.77), we extend
the initial displacement f(x) and velocity g(x) to be odd, periodic functions of period 2ℓ:
f(−x) = −f(x),
f(x + 2ℓ) = f(x),
g(−x) = −g(x),
g(x + 2ℓ) = g(x).
This will ensure that the d’Alembert solution also remains odd and periodic. As a result,
it satisﬁes the homogeneous Dirichlet boundary conditions (4.79) for all t, cf. Exercise
4.2.31. Keep in mind that, while the solution u(t, x) is deﬁned for all x, the only physically
relevant values occur on the interval 0 ≤x ≤ℓ. Nevertheless, the eﬀects of displacements
in the unphysical regime will eventually be felt as the propagating waves pass through the
physical interval.
For example, consider an initial displacement that is concentrated near x = ξ for some
0 < ξ < ℓ. Its odd 2ℓ–periodic extension consists of two sets of replicas: those of the same
form occurring at positions ξ ± 2ℓ, ξ ± 4ℓ, . . . , and their upside-down mirror images at
the intermediate positions −ξ, −ξ ± 2ℓ, −ξ ± 4ℓ, . . . ; Figure 4.5 shows a representative
example. The resulting solution begins with each of the pulses, both positive and negative,
splitting into two half-size replicas that propagate with speed c in opposite directions.
When a left and right moving pulse meet, they emerge from the interaction unaltered. The
process repeats periodically, with an inﬁnite row of half-size pulses moving to the right
kaleidoscopically interacting with an inﬁnite row moving to the left.
However, only the part of this solution that lies on 0 ≤x ≤ℓis actually observed
on the physical string. The eﬀect is as if one were watching the full solution as it passes
by a window of length ℓ. Such observers will interpret what they see a bit diﬀerently. To

148
4 Separation of Variables
Figure 4.6.
Solution to wave equation with ﬁxed ends.

wit, the original pulse starting at position 0 < ξ < ℓsplits up into two half-size replicas
that move oﬀin opposite directions. As each half-size pulse reaches an end of the string,
it meets a mirror-image pulse that has been propagating in the opposite direction from
the nonphysical regime. The pulse is reﬂected at the end of the interval and becomes an
upside-down mirror image moving in the opposite direction. The original positive pulse
has moved oﬀthe end of the string just as its mirror image has moved into the physical
regime. (A common physical realization is a pulse propagating down a jump rope that is
held ﬁxed at its end; the reﬂected pulse returns upside down.) A similar reﬂection occurs
as the other half-size pulse hits the other end of the physical interval, after which the
solution consists of two upside-down half-size pulses moving back towards each other. At
time t = ℓ/c they recombine at the point ℓ−ξ to instantaneously form a full-sized, but
upside-down mirror image of the original disturbance — in accordance with (4.74). The
recombined pulse in turn splits apart into two upside-down half-size pulses that, when each
collides with the end, reﬂect and return to their original upright form. At time t = 2ℓ/c,
the pulses recombine to exactly reproduce the original displacement. The process then
repeats, and the solution is periodic in time with period 2ℓ/c.
In Figure 4.6, the ﬁrst picture displays the initial displacement. In the second, it has
split into left- and right-moving half-size clones. In the third picture, the left-moving bump
is in the process of colliding with the left end of the string. In the fourth picture, it has
emerged from the collision, and is now upside down, reﬂected, and moving to the right.
Meanwhile, the right-moving pulse is starting to collide with the right end. In the ﬁfth
picture, both pulses have completed their collisions and are now moving back towards each
other, where, in the last picture, they recombine into an upside-down mirror image of the
original pulse. The process then repeats itself, in mirror image, ﬁnally recombining to the
original pulse, at which point the entire process starts over.
The Neumann (free) boundary value problem
∂u
∂x (t, 0) = 0,
∂u
∂x (t, ℓ) = 0,
(4.80)
is handled similarly. Since the solution has the form of a Fourier cosine series in x, we

4.2 The Wave Equation
149
extend the initial conditions to be even 2ℓ–periodic functions
f(−x) = f(x),
f(x + 2ℓ) = f(x),
g(−x) = g(x),
g(x + 2ℓ) = g(x).
The resulting d’Alembert solution (4.77) is also even and 2ℓ–periodic in x, and hence
satisﬁes the boundary conditions, cf. Exercise 4.2.31(b). In this case, when a pulse hits
one of the ends, its reﬂection remains upright, but becomes a mirror image of the original;
a familiar physical illustration is a water wave that reﬂects oﬀa solid wall. Further details
are left to the reader in Exercise 4.2.22
In summary, we have now studied two very diﬀerent ways to solve the one-dimensional
wave equation. The ﬁrst, based on the d’Alembert formula, emphasizes their particle-like
aspects, where individual wave packets collide with each other, or reﬂect at the boundary,
all the while maintaining their overall form, while the second, based on Fourier analysis,
emphasizes the vibrational or wave-like character of the solutions. Some solutions look
like vibrating waves, while others appear much more like interacting particles. But, like
the proverbial blind men describing an elephant, these are merely two facets of the same
solution. The Fourier series formula shows how every particle-like solution can be decom-
posed into its constituent vibrational modes, while the d’Alembert formula demonstrates
how vibrating solutions combine into moving wave packets.
The coexistence of particle and wave features is reminiscent of the long-running his-
torical debate over the nature of light. Newton and his disciples proposed a particle-based
theory, anticipating the modern concept of photons. However, until the beginning of the
twentieth century, most physicists advocated a wave-like or vibrational viewpoint. Ein-
stein’s explanation of the photoelectric eﬀect served to resurrect the particle interpretation.
Only with the establishment of quantum mechanics was the debate resolved — light, and,
indeed, all subatomic particles manifest both particle and wave features, depending upon
the experiment and the physical situation. But a theoretical basis for the perplexing wave-
particle duality could have been found already in Fourier’s and d’Alembert’s competing
solution formulae for the classical wave equation!
Exercises
♦4.2.13.(a) Solve the initial-boundary value problem from Example 4.3 using the d’Alembert
method.
(b) Verify that your solution coincides with the Fourier series solution derived above.
(c) Justify our earlier observation that, at each time t, the solution u(t, x) is a piecewise
aﬃne function of x.
4.2.14. Sketch the solution of the wave equation utt = uxx and describe its behavior when
the initial displacement is the box function u(0, x) =
 1,
1 < x < 2,
0,
otherwise,
while the initial
velocity is 0 in each of the following scenarios: (a) on the entire line −∞< x < ∞;
(b) on the half-line 0 ≤x < ∞, with homogeneous Dirichlet boundary condition at the
end; (c) on the half-line 0 ≤x < ∞, with homogeneous Neumann boundary condition at
the end; (d) on the bounded interval 0 ≤x ≤5 with homogeneous Dirichlet boundary
conditions; (e) on the bounded interval 0 ≤x ≤5 with homogeneous Neumann boundary
conditions.

150
4 Separation of Variables
4.2.15. Answer Exercise 4.2.14 when the initial velocity is the box function, while the initial
displacement is zero.
4.2.16. Consider the initial-boundary value problem
∂2u
∂t2 = ∂2u
∂x2 ,
u(t, 0) = 0 = u(t, 10),
t > 0,
u(0, x) = f(x),
ut(0, x) = 0,
0 < x < 10,
for the wave equation, where the initial data has the following form:
f(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
3x −7.5,
2.5 ≤x ≤3,
6 −1.5x,
3 ≤x ≤4.5,
1.5x −7.5,
4.5 ≤x ≤5,
0,
otherwise.
Discuss what happens to the solution. You do not need to write down an explicit formula
for the solution, but for full credit you must explain (sketches can help) at least three or
four interesting things that happen to the solution as time progresses.
4.2.17. Repeat Exercise 4.2.16 for the Neumann boundary conditions.
4.2.18. Suppose the initial displacement of a string of length ℓlooks like
the graph to the right. Assuming that the ends of the string are held
ﬁxed, graph the string’s proﬁle at times t = ℓ/c and 2ℓ/c.
♣4.2.19. Consider the wave equation utt = uxx on the interval 0 ≤x ≤1, with homogeneous
Dirichlet boundary conditions at both ends. (a) Use the d’Alembert formula to explicitly
solve the initial value problem u(0, x) = x −x2, ut(0, x) = 0. (b) Graph the solution
proﬁle at some representative times, and discuss what you observe. (c) Find the Fourier
series at each t of your solution and compare the two.
(d) How many terms do you need
to sum to obtain a reasonable approximation to the exact solution?
♣4.2.20. Solve Exercise 4.2.19 for the initial conditions u(0, x) = 0, ut(0, x) = x2 −x.
♣4.2.21. Solve (i) Exercise 4.2.19, (ii) Exercise 4.2.20, when the solution is subject to homoge-
neous Neumann boundary conditions.
♦4.2.22. Under what conditions is the solution to the Neumann boundary value problem for the
wave equation on a bounded interval [0, ℓ] periodic in time? What is the period?
4.2.23. Discuss and sketch the behavior of the solution to the Neumann boundary value prob-
lem utt = 4uxx, 0 < x < 1, ux(t, 0) = 0 = ux(t, 1), u(0, x) = f(x), ut(0, x) = g(x), for
(a) a localized initial displacement: f(x) =
 1,
.2 < x < .3,
0,
otherwise.
g(x) = 0;
(b) a localized initial velocity: f(x) = 0, g(x) =
 1,
.2 < x < .3,
0,
otherwise.
.
4.2.24.(a) Explain how to solve the Neumann initial-boundary value problem
∂2u
∂t2 = ∂2u
∂x2 ,
∂u
∂x (t, 0) = 0 = ∂u
∂x (t, 1),
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
on the interval 0 ≤x ≤1.
(b) Let f(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x −1
4,
1
4 ≤x ≤1
2,
3
4 −x,
1
2 ≤x ≤3
4,
0,
otherwise,
and g(x) = 0. Sketch the graph of the solution at
a few representative times, and discuss what is happening. Is the solution periodic in
time? If so, what is the period?
(c) Do the same when f(x) = 0 and g(x) = x.

4.2 The Wave Equation
151
4.2.25.(a) Write down a formula for the solution u(t, x) to the initial-boundary value problem
∂2u
∂t2 −4 ∂2u
∂x2 = 0, u(0, x) = sin x,
∂u
∂t (0, x) = ∂u
∂x (t, 0) = ∂u
∂x (t, π) = 0, 0 < x < π, t > 0.
(b) Find u
 π
2 , π
2

.
(c) Prove that h(t) = u

t, π
2

is a periodic function of t and ﬁnd its
period.
(d) Does ∂u
∂x have any discontinuities? If so, discuss their behavior.
4.2.26. Answer Exercise 4.2.25 for the mixed boundary conditions u(t, 0) = 0 = ux(t, π).
♥4.2.27.(a) Explain how to use d’Alembert’s formula (4.77) to solve the periodic initial-boundary
value problem for the wave equation given in Exercise 4.2.6.
(b) Do the d’Alembert and Fourier series formulae represent the same solution? If so, can
you justify it? If not, explain why they are diﬀerent.
♦4.2.28. Show that the solution u(t, x) to the wave equation on an interval [0, ℓ], subject to pe-
riodic boundary conditions u(t, 0) = u(t, ℓ), ux(t, 0) = ux(t, ℓ), is a periodic function of t if
and only if there is no net initial velocity:
	 ℓ
0 g(x) dx = 0.
4.2.29.(a) Explain how to solve the wave equation on a half-line x > 0 when subject to Dirich-
let boundary conditions u(t, 0) = 0.
(b) Assuming c = 1, ﬁnd the solution satisfying
u(0, x) = (x −2) e−5(x−2.2)2
, ut(0, x) = 0. (c) Sketch a picture of your solution at some
representative times, and discuss what is happening.
4.2.30. Solve Exercise 4.2.29 for homogeneous Neumann boundary conditions at x = 0.
♦4.2.31.(a) Given that f(x) is odd and 2ℓ–periodic, explain why f(0) = 0 = f(ℓ).
(b) Given that f(x) is even and 2ℓ–periodic, explain why f′(0) = 0 = f′(ℓ).
♦4.2.32.(a) Prove that if f(−x) = −f(x), f(x + 2ℓ) = f(x), for all x, then
u(t, x) = 1
2 [f(x −c t) + f(x + c t)] satisﬁes the Dirichlet boundary conditions (4.79).
(b) Prove that if g(−x) = −g(x), g(x + 2ℓ) = g(x) for all x, then
u(t, x) = 1
2 c
	 x+c t
x−c t g(z) dz also satisﬁes the Dirichlet boundary conditions.
4.2.33. If both u(0, x) = f(x) and ut(0, x) = g(x) are even functions, show that the solution
u(t, x) of the wave equation is even in x for all t.
4.2.34.(a) Prove that the solution u(t, x) to the wave equation for x ∈R is an even function of
t if and only if its initial velocity, at t = 0, is zero.
(b) Under what conditions is u(t, x) an odd function of t?
♦4.2.35. Let u(t, x) be a classical solution to the wave equation utt = c2uxx on the interval
0 < x < ℓ, satisfying homogeneous Dirichlet boundary conditions. The total energy of u at
time t is
E(t) =
	 ℓ
0
1
2
⎡
⎣
∂u
∂t
2
+ c2
∂u
∂x
2 ⎤
⎦dx.
(4.81)
Establish the Law of Conservation of Energy by showing that E(t) = E(0) is a constant
function.
♦4.2.36.(a) Use Exercise 4.2.35 to prove that the only C2 solution to the initial-boundary value
problem vtt = c2vxx, v(t, 0) = v(t, ℓ) = 0, v(0, x) = 0, vt(0, x) = 0, is the trivial solu-
tion v(t, x) ≡0. (b) Establish the following Uniqueness Theorem for the wave equation:
given f(x), g(x) ∈C2, there is at most one C2 solution u(t, x) to the initial-boundary value
problem utt = c2uxx, u(t, 0) = u(t, ℓ) = 0, u(0, x) = f(x), ut(0, x) = g(x).
4.2.37. Referring back to Exercises 4.2.35 and 4.2.36: (a) Does conservation of energy hold for
solutions to the homogeneous Neumann initial-boundary value problem?
(b) Can you establish a uniqueness theorem for the Neumann problem?

152
4 Separation of Variables
4.2.38. Explain how to solve the Dirichlet initial-boundary value problem
utt = c2uxx + F(t, x),
u(0, x) = f(x),
ut(0, x) = g(x),
u(t, 0) = u(t, ℓ) = 0,
for the wave equation subject to an external forcing on the interval [0, ℓ].
4.3 The Planar Laplace and Poisson Equations
The two-dimensional Laplace equation is the second-order linear partial diﬀerential equa-
tion
∂2u
∂x2 + ∂2u
∂y2 = 0,
(4.82)
named in honor of the inﬂuential eighteenth-century French mathematician Pierre–Simon
Laplace. It, along with its higher-dimensional versions, is arguably the most important
diﬀerential equation in all of mathematics. A real-valued solution u(x, y) to the Laplace
equation is known as a harmonic function. The space of harmonic functions can thus be
identiﬁed as the kernel of the second-order linear partial diﬀerential operator
Δ = ∂2
∂x2 + ∂2
∂y2 ,
(4.83)
known as the Laplace operator, or Laplacian for short.
The inhomogeneous or forced
version, namely
−Δ[u] = −∂2u
∂x2 −∂2u
∂y2 = f(x, y),
(4.84)
is known as Poisson’s equation, named after Sim´eon–Denis Poisson, who was taught by
Laplace. The mathematical and physical reasons for including the minus sign will gradually
become clear.
Besides their theoretical importance, the Laplace and Poisson equations arise as the
basic equilibrium equations in a remarkable variety of physical systems. For example, we
may interpret u(x, y) as the displacement of a membrane, e.g., a drum skin; the inhomo-
geneity f(x, y) in the Poisson equation represents an external forcing over the surface of
the membrane. Another example is in the thermal equilibrium of ﬂat plates; here u(x, y)
represents the temperature and f(x, y) an external heat source. In ﬂuid mechanics, u(x, y)
represents the potential function whose gradient v = ∇u is the velocity vector ﬁeld of a
steady planar ﬂuid ﬂow. Similar considerations apply to two-dimensional
electrostatic
and gravitational potentials. The dynamical counterparts to the Laplace equation are the
two-dimensional versions of the heat and wave equations, to be analyzed in Chapter 11.
Since both the Laplace and Poisson equations describe equilibrium conﬁgurations, they
almost always appear the context of boundary value problems. We seek a solution u(x, y)
to the partial diﬀerential equation deﬁned at points (x, y) belonging to a bounded, open
domain Ω ⊂R2. The solution is required to satisfy suitable conditions on the boundary
of the domain, denoted by ∂Ω, which will consist of one or more simple closed curves, as
illustrated in Figure 4.7. As in one-dimensional boundary value problems, there are several
especially important types of boundary conditions.

4.3 The Planar Laplace and Poisson Equations
153
Ω
∂Ω
n
n
n
n
Figure 4.7.
A planar domain with outward unit normals on its boundary.
The ﬁrst are the ﬁxed or Dirichlet boundary conditions, which specify the value of the
function u on the boundary:
u(x, y) = h(x, y)
for
(x, y) ∈∂Ω.
(4.85)
Under mild regularity conditions on the domain Ω, the boundary values h, and the forcing
function f, the Dirichlet conditions (4.85) serve to uniquely specify the solution u(x, y) to
the Laplace or the Poisson equation. Physically, in the case of a free or forced membrane,
the Dirichlet boundary conditions correspond to gluing the edge of the membrane to a
wire at height h(x, y) over each boundary point (x, y) ∈∂Ω, as illustrated in Figure 4.8.
A physical realization can be easily obtained by dipping the wire in a soap solution; the
resulting soap ﬁlm spanning the wire forms a minimal surface, which, if the wire is reason-
ably close to planar shape,† is the solution to the Dirichlet problem prescribed by the wire.
Similarly, in the modeling of thermal equilibrium, a Dirichlet boundary condition repre-
sents the imposition of a prescribed temperature distribution, represented by the function
h, along the boundary of the plate.
The second important class consists of the Neumann boundary conditions
∂u
∂n = ∇u · n = k(x, y)
on
∂Ω,
(4.86)
in which the normal derivative of the solution u on the boundary is prescribed. In general, n
denotes the unit outwards normal to the boundary ∂Ω, i.e., the vector of unit length, ∥n ∥=
1, that is orthogonal to the tangent to the boundary and points away from the domain; see
Figure 4.7. For example, in thermomechanics, a Neumann boundary condition speciﬁes
the heat ﬂux out of a plate through its boundary. The “no-ﬂux” or homogeneous Neumann
boundary conditions, where k(x, y) ≡0, correspond to a fully insulated boundary. In the
case of a membrane, homogeneous Neumann boundary conditions correspond to a free,
unattached edge of a drum. In ﬂuid mechanics, the Neumann conditions prescribe the
ﬂuid ﬂux through the boundary; in particular, homogeneous Neumann boundary conditions
†
More generally, the minimal surface formed by the soap ﬁlm solves the vastly more compli-
cated nonlinear minimal surface equation (1 + u2
x)uxx −2uxuyuxy + (1 + u2
y)uyy = 0, which, for
surfaces with small variation, i.e., with ∥∇u ∥≪1, can be approximated by the Laplace equation.

154
4 Separation of Variables
∂Ω
h(x, y)
Figure 4.8.
Dirichlet boundary conditions.
correspond to a solid boundary that the ﬂuid cannot penetrate. More generally, the Robin
boundary conditions
∂u
∂n + β(x, y) u = k(x, y)
on
∂Ω,
also known as impedance boundary conditions due to their applications in electromag-
netism, are used to model insulated plates in heat baths, or membranes attached to springs.
Finally, one can mix the previous kinds of boundary conditions, imposing, say, Dirich-
let conditions on part of the boundary and Neumann conditions on the complementary
part. A typical mixed boundary value problem has the form
−Δu = f
in
Ω,
u = h
on
D,
∂u
∂n = k
on
N,
(4.87)
with the boundary ∂Ω = D ∪N being the disjoint union of a “Dirichlet segment”, denoted
by D, and a “Neumann segment” N. For example, if u represents the equilibrium tem-
perature in a plate, then the Dirichlet segment of the boundary is where the temperature
is ﬁxed, while the Neumann segment is insulated, or, more generally, has prescribed heat
ﬂux. Similarly, when modeling the displacement of a membrane, the Dirichlet segment is
where the edge of the drum is attached to a support, while the homogeneous Neumann
segment is left hanging free.
Exercises
4.3.1.(a) Solve the boundary value problem Δu = 1 for x2 + y2 < 1 and u(x, y) = 0 for
x2 + y2 = 1 directly. Hint: The solution is a simple polynomial.
(b) Graph your solution, interpreting it as the equilibrium displacement of a circular drum
under a constant gravitational force.
4.3.2. Set up the boundary value problem corresponding to the equilibrium of a circular mem-
brane subject to a constant downwards gravitational force, half of whose boundary is glued
to a ﬂat semicircular wire, while the other half is unattached.
4.3.3. Set up the boundary value problem corresponding to the thermal equilibrium of a rect-
angular plate that is insulated on two of its sides, has 0◦at its top edge and 100◦at the

4.3 The Planar Laplace and Poisson Equations
155
bottom edge. Where do you expect the maximum temperature to be located? What is its
value? Can you ﬁnd a formula for the temperature inside the plate? Hint: The solution is
constant along horizontal lines.
4.3.4. Set up the boundary value problem corresponding to the thermal equilibrium of an in-
sulated semi-circular plate with unit diameter, whose curved edge is kept at 0◦and whose
straight edge is at 50◦.
4.3.5. Explain why the solution to the homogeneous Neumann boundary value problem for the
Laplace equation is not unique.
4.3.6. Write down the Dirichlet boundary value problem for the Laplace equation on the unit
square 0 ≤x, y ≤1 that is satisﬁed by u(x, y) = 1 + xy.
4.3.7. Write down the Neumann boundary value problem for the Poisson equation on the unit
disk x2 + y2 ≤1 that is satisﬁed by u(x, y) = x3 + xy2.
♦4.3.8. Suppose u(x, y) is a solution to the Laplace equation.
(a) Show that any translate U(x, y) = u(x −a, y −b), where a, b ∈R, is also a solution.
(b) Show that the rotated function U(x, y) = u(x cos θ + y sin θ, −x sin θ + y cos θ), where
−π < θ ≤π, is also a solution.
♦4.3.9.(a) Show that if u(x, y) solves the Laplace equation, then so does the rescaled function
U(x, y) = c u(αx, αy) for any constants c, α.
(b) Discuss the eﬀect of scaling on the Dirichlet boundary value problem.
(c) What happens if we use diﬀerent scaling factors in x and y?
Separation of Variables
Our ﬁrst approach to solving the Laplace equation
Δu = ∂2u
∂x2 + ∂2u
∂y2 = 0
(4.88)
will be based on the method of separation of variables. As in (4.64), we seek solutions that
can be written as a product
u(x, y) = v(x) w(y)
(4.89)
of a function of x alone times a function of y alone. We compute
∂2u
∂x2 = v′′(x) w(y),
∂2u
∂y2 = v(x) w′′(y),
and so
Δu = ∂2u
∂x2 + ∂2u
∂y2 = v′′(x) w(y) + v(x) w′′(y) = 0.
We then separate the variables by placing all the terms involving x on one side of the
equation and all the terms involving y on the other; this is accomplished by dividing by
v(x) w(y) and then writing the resulting equation in the separated form
v′′(x)
v(x) = −w′′(y)
w(y)
= λ.
(4.90)

156
4 Separation of Variables
As we argued in (4.65), the only way a function of x alone can be equal to a function of y
alone is if both functions are equal to a common separation constant λ. Thus, the factors
v(x) and w(y) must satisfy the elementary ordinary diﬀerential equations
v′′ −λ v = 0,
w′′ + λ w = 0.
As before, the solution formulas depend on the sign of the separation constant λ. We list
the resulting collection of separable harmonic functions in the following table:
Separable Solutions to Laplace’s Equation
λ
v(x)
w(y)
u(x, y) = v(x) w(y)
λ = −ω2 < 0
cos ωx, sin ωx
e−ω y, eω y,
eω y cos ωx,
e−ω y cos ωx,
eω y sin ωx,
e−ω y sin ωx
λ = 0
1, x
1, y
1, x, y, xy
λ = ω2 > 0
e−ω x, eω x
cos ωy, sin ωy
eω x cos ωy,
e−ω x cos ωy,
eω x sin ωy,
e−ω x sin ωy
Since Laplace’s equation is a homogeneous linear system, any linear combination of
solutions is also a solution. So, we can build more general solutions as ﬁnite linear combi-
nations, or, provided we pay proper attention to convergence issues, inﬁnite series in the
separable solutions. Our goal is to solve boundary value problems, and so we must ensure
that the resulting combination satisﬁes the boundary conditions. But this is not such an
easy task, unless the underlying domain has a rather special geometry.
In fact, the only bounded domains on which we can explicitly solve boundary value
problems using the preceding separable solutions are rectangles. So, we will concentrate
on boundary value problems for Laplace’s equation
Δu = 0
on a rectangle
R = {0 < x < a,
0 < y < b}.
(4.91)
To make progress, we will allow nonzero boundary values on only one of the four sides of
the rectangle. To illustrate, we will focus on the following Dirichlet boundary conditions:
u(x, 0) = f(x),
u(x, b) = 0,
u(0, y) = 0,
u(a, y) = 0.
(4.92)
Once we know how to solve this type of problem, we can employ linear superposition to
solve the general Dirichlet boundary value problem on a rectangle; see Exercise 4.3.12 for
details. Other boundary conditions can be treated in a similar fashion — with the proviso
that the condition on each side of the rectangle is either entirely Dirichlet or entirely
Neumann or, more generally, entirely Robin with constant transfer coeﬃcient.
To solve the boundary value problem (4.91–92), the ﬁrst step is to narrow down the
separable solutions to only those that respect the three homogeneous boundary conditions.
The separable function u(x, y) = v(x) w(y) will vanish on the top, right, and left sides of
the rectangle, provided
v(0) = v(a) = 0
and
w(b) = 0.

4.3 The Planar Laplace and Poisson Equations
157
Referring to the preceding table, the ﬁrst condition v(0) = 0 requires
v(x) =
⎧
⎪
⎨
⎪
⎩
sin ωx,
λ = −ω2 < 0,
x,
λ = 0,
sinh ωx,
λ = ω2 > 0,
where sinh z =
1
2(ez −e−z) is the usual hyperbolic sine function. However, the second
and third cases cannot satisfy the second boundary condition v(a) = 0, and so we discard
them. The ﬁrst case leads to the condition
v(a) = sin ω a = 0,
and hence
ω a = π, 2π, 3π, . . . .
The corresponding separation constants and solutions (up to constant multiple) are
λn = −ω2 = −n2 π2
a2
,
vn(x) = sin nπx
a
,
n = 1, 2, 3, . . . .
(4.93)
Note: So far, we have merely recomputed the known eigenvalues and eigenfunctions
of the familiar boundary value problem v′′ −λ v = 0, v(0) = v(a) = 0.
Next, since λ = −ω2 < 0, we have w(y) = c1eω y + c2e−ω y for constants c1, c2. The
third boundary condition w(b) = 0 then requires that, up to constant multiple,
wn(y) = sinh ω (b −y) = sinh nπ(b −y)
a
.
(4.94)
We conclude that the harmonic functions
un(x, y) = sin nπx
a
sinh nπ(b −y)
a
,
n = 1, 2, 3, . . . ,
(4.95)
provide a complete list of separable solutions that satisfy the three homogeneous boundary
conditions. It remains to analyze the inhomogeneous boundary condition along the bottom
edge of the rectangle. To this end, let us try a linear superposition of the relevant separable
solutions in the form of an inﬁnite series
u(x, y) =
∞

n=1
cnun(x, y) =
∞

n=1
cn sin nπx
a
sinh nπ (b −y)
a
,
whose coeﬃcients c1, c2, . . . are to be prescribed by the remaining boundary condition. At
the bottom edge, y = 0, we ﬁnd
u(x, 0) =
∞

n=1
cn sinh nπb
a
sin nπx
a
= f(x),
0 ≤x ≤a,
(4.96)
which takes the form of a Fourier sine series for the function f(x). Let
bn = 2
a
 a
0
f(x) sin nπx
a
dx
(4.97)
be its Fourier sine coeﬃcients, whence cn = bn/ sinh(nπb/a). We thus anticipate that the
solution to the boundary value problem can be expressed as the inﬁnite series
u(x, y) =
∞

n=1
bn sin nπx
a
sinh nπ(b −y)
a
sinh nπb
a
.
(4.98)

158
4 Separation of Variables
Figure 4.9.
Square membrane on a wire.
Does this series actually converge to the solution to the boundary value problem?
Fourier analysis says that, under very mild conditions on the boundary function f(x), the
answer is yes. Suppose that its Fourier coeﬃcients are uniformly bounded,
| bn | ≤M
for all
n ≥1,
(4.99)
which, according to (4.27), is true whenever f(x) is piecewise continuous or, more generally,
integrable:
 a
0
| f(x) | dx < ∞. In this case, as you are asked to prove in Exercise 4.3.20,
the coeﬃcients of the Fourier sine series (4.98) go to zero exponentially fast:
bn sinh nπ(b −y)
a
sinh nπb
a
−→0
as
n −→∞
for all
0 < y ≤b,
(4.100)
and so, at each point inside the rectangle, the series can be well approximated by partial
summation. Theorem 3.31 tells us that, for each 0 < y ≤b, the solution u(x, y) is an
inﬁnitely diﬀerentiable function of x. Moreover, by term-wise diﬀerentiation of the series
with respect to y and use of Proposition 3.28, we also establish that the solution is inﬁnitely
diﬀerentiable with respect to y; see Exercise 4.3.21. (In fact, as we shall see, solutions to
the Laplace equation are always analytic functions inside their domain of deﬁnition — even
when their boundary values are rather rough.) Since the individual terms all satisfy the
Laplace equation, we conclude that the series (4.98) is indeed a classical solution to the
boundary value problem.
Example 4.4.
A membrane is stretched over a wire in the shape of a unit square
with one side bent in half, as graphed in Figure 4.9. The precise boundary conditions are
u(x, y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
x,
0 ≤x ≤1
2,
y = 0,
1 −x,
1
2 ≤x ≤1,
y = 0,
0,
0 ≤x ≤1,
y = 1,
0,
x = 0,
0 ≤y ≤1,
0,
x = 1,
0 ≤y ≤1.

4.3 The Planar Laplace and Poisson Equations
159
The Fourier sine series of the inhomogeneous boundary function is readily computed:
f(x) =

x,
0 ≤x ≤1
2,
1 −x,
1
2 ≤x ≤1,
= 4
π2

sin πx −sin 3πx
9
+ sin 5πx
25
−· · ·

= 4
π2
∞

j =0
(−1)j sin(2j + 1)πx
(2j + 1)2
.
Specializing (4.98) to a = b = 1, we conclude that the solution to the boundary value
problem can be expressed as a Fourier series
u(x, y) = 4
π2
∞

j =0
(−1)j sin(2j + 1)πx sinh(2j + 1)π(1 −y)
(2j + 1)2 sinh(2j + 1)π
.
In Figure 4.9 we plot the sum of the ﬁrst 10 terms in the series. This gives a reasonably good
approximation to the actual solution, except when we are very close to the raised corner
of the boundary wire — which is the point of maximal displacement of the membrane.
Exercises
4.3.10. Solve the following boundary value problems for Laplace’s equation on the square
Ω = {0 ≤x ≤π,
0 ≤y ≤π }.
(a) u(x, 0) = sin3 x,
u(x, π) = 0,
u(0, y) = 0,
u(π, y) = 0.
(b) u(x, 0) = 0,
u(x, π) = 0,
u(0, y) = sin y,
u(π, y) = 0.
(c) u(x, 0) = 0,
u(x, π) = 1,
u(0, y) = 0,
u(π, y) = 0.
(d) u(x, 0) = 0,
u(x, π) = 0,
u(0, y) = 0,
u(π, y) = y(π −y).
♦4.3.11.(a) Explain how to use linear superposition to solve the boundary value problem
Δu = 0,
u(x, 0) = f(x),
u(x, b) = g(x),
u(0, y) = h(y),
u(a, y) = k(y),
on the rectangle R = {0 < x < a, 0 < y < b}, by splitting it into four separate boundary
value problems for which each of the solutions vanishes on three sides of the rectangle.
(b) Write down a series formula for the resulting solution.
4.3.12. Solve the following Dirichlet problems for Laplace’s equation on the unit square
S = {0 < x, y < 1}. Hint: Use superposition as in Exercise 4.3.11.
(a) u(x, 0) = sin πx,
u(x, 1) = 0,
u(0, y) = sin πy,
u(1, y) = 0;
(b) u(x, 0) = 1,
u(x, 1) = 0,
u(0, y) = 1,
u(1, y) = 0;
(c) u(x, 0) = 1,
u(x, 1) = 1,
u(0, y) = 0,
u(1, y) = 0;
(d) u(x, 0) = x,
u(x, 1) = 1 −x,
u(0, y) = y,
u(1, y) = 1 −y.
4.3.13. Solve the following mixed boundary value problems for Laplace’s equation Δu = 0 on
the square S = {0 < x, y < π}.
(a) u(x, 0) = sin 1
2 x,
uy(x, π) = 0,
u(0, y) = 0,
ux(π, y) = 0;
(b) u(x, 0) = sin 1
2 x,
uy(x, π) = 0,
ux(0, y) = 0,
ux(π, y) = 0;
(c) u(x, 0) = x,
u(x, π) = 0,
ux(0, y) = 0,
ux(π, y) = 0;
(d) u(x, 0) = x,
u(x, π) = 0,
u(0, y) = 0,
ux(π, y) = 0.
4.3.14. Find the solution to the boundary value problem
Δu = 0,
uy(x, 0) = uy(x, 2) = 0,
u(0, y) = 2 cos πy −1,
u(1, y) = 0,
0 < x < 1,
0 < y < 2.

160
4 Separation of Variables
4.3.15. Find the solution to the boundary value problem
Δu = 0,
u(x, 0) = 2 cos 7πx −4,
u(x, 1) = 5 cos 3πx,
ux(0, y) = ux(1, y) = 0,
0 < x, y < 1.
4.3.16. Let u(x, y) be the solution to the boundary value problem
Δu = 0, u(x, −1) = f(x), u(x, 1) = 0, u(−1, y) = 0, u(1, y) = 0, −1 < x < 1, −1 < y < 1.
(a) True or false: If f(−x) = −f(x) is odd, then u(0, y) = 0 for all −1 ≤y ≤1.
(b) True or false: If f(0) = 0, then u(0, y) = 0 for all −1 ≤y ≤1.
(c) Under what conditions on f(x) is u(x, 0) = 0 for all −1 ≤x ≤1?
4.3.17. Use separation of variables to solve the following boundary value problem:
uxx + 2uy + uyy = 0,
u(x, 0) = 0,
u(x, 1) = f(x),
u(0, y) = 0,
u(1, y) = 0.
4.3.18. Use separation of variables to solve the Helmholtz boundary value problem Δu = u,
u(x, 0) = 0, u(x, 1) = f(x), u(0, y) = 0, u(1, y) = 0, on the unit square 0 < x, y < 1.
♦4.3.19. Provide the details for the derivation of (4.94).
♦4.3.20. Justify the statement that if | bn | ≤M are uniformly bounded, then the coeﬃcients
given in (4.100) go to zero exponentially fast as n →∞for any 0 < y ≤b.
♦4.3.21. Let u(x, y) denote the solution to the boundary value problem (4.91–92).
(a) Write down the Fourier sine series for ∂u/∂y . (b) Prove that ∂u/∂y is an inﬁnitely
diﬀerentiable function of x. (c) Justify the same result for the functions ∂ku/∂yk for each
k ≥0. Hint: Don’t forget that u(x, y) solves the Laplace equation.
Polar Coordinates
The method of separation of variables can be successfully exploited in certain other very
special geometries. One particularly important case is a circular disk. To be speciﬁc, let
us take the disk to have radius 1 and be centered at the origin. Consider the Dirichlet
boundary value problem
Δu = 0,
x2 + y2 < 1,
and
u = h,
x2 + y2 = 1,
(4.101)
so that the function u(x, y) satisﬁes the Laplace equation on the unit disk and the speciﬁed
Dirichlet boundary conditions on the unit circle. For example, u(x, y) might represent the
displacement of a circular drum that is attached to a wire of height
h(x, y) = h(cos θ, sin θ) ≡h(θ),
−π < θ ≤π,
(4.102)
at each point (x, y) = (cos θ, sin θ) on its edge.
The rectangular separable solutions are not particularly helpful in this situation, and
so we look for solutions that are better adapted to a circular geometry. This inspires us to
adopt polar coordinates
x = r cos θ,
y = r sin θ,
or
r =

x2 + y2 ,
θ = tan−1 y
x ,
(4.103)
and write the solution u(r, θ) as a function thereof.
Warning: We will often retain the same symbol, e.g., u, when rewriting a function
in a diﬀerent coordinate system. This is the convention of tensor analysis, physics, and

4.3 The Planar Laplace and Poisson Equations
161
diﬀerential geometry, [3], that treats the function (scalar ﬁeld) as an intrinsic object, which
is concretely realized through its formula in any chosen coordinate system. For instance,
if u(x, y) = x2 + 2y in rectangular coordinates, then its expression in polar coordinates
is u(r, θ) = (r cos θ)2 + 2r sin θ, not r2 + 2θ. This convention avoids the inconvenience of
having to devise new symbols when changing coordinates.
We need to relate derivatives with respect to x and y to those with respect to r and
θ. Performing a standard multivariate chain rule computation based on (4.103), we obtain
∂
∂r = cos θ ∂
∂x + sin θ ∂
∂y ,
∂
∂θ = −r sin θ ∂
∂x + r cos θ ∂
∂y ,
so
∂
∂x = cos θ ∂
∂r −sin θ
r
∂
∂θ ,
∂
∂y = sin θ ∂
∂r + cos θ
r
∂
∂θ .
(4.104)
Applying the squares of the latter diﬀerential operators to u(r, θ), we ﬁnd, after a calcula-
tion in which many of the terms cancel, the polar coordinate form of the Laplace equation:
Δu = ∂2u
∂x2 + ∂2u
∂y2 = ∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2 = 0.
(4.105)
The boundary conditions are imposed on the unit circle r = 1, and so, by (4.102), take the
form
u(1, θ) = h(θ).
(4.106)
Keep in mind that, in order to be single-valued functions of x, y, the solution u(r, θ) and
its boundary values h(θ) must both be 2π–periodic functions of the angular coordinate:
u(r, θ + 2π) = u(r, θ),
h(θ + 2π) = h(θ).
(4.107)
Polar separation of variables is based on the ansatz
u(r, θ) = v(r) w(θ),
(4.108)
which assumes that the solution is a product of functions of the individual variables. Sub-
stituting (4.108) into the polar form (4.105) of Laplace’s equation yields
v′′(r) w(θ) + 1
r v′(r) w(θ) + 1
r2 v(r) w′′(θ) = 0.
We now separate variables by moving all the terms involving r onto one side of the equation
and all the terms involving θ onto the other. This is accomplished by ﬁrst multiplying the
equation by r2/

v(r) w(θ)

and then moving the ﬁnal term to the right-hand side:
r2 v′′(r) + r v′(r)
v(r)
= −w′′(θ)
w(θ) = λ.
As in the rectangular case, a function of r can equal a function of θ if and only if both are
equal to a common separation constant, which we call λ. The partial diﬀerential equation
thus splits into a pair of ordinary diﬀerential equations
r2 v′′ + r v′ −λ v = 0,
w′′ + λ w = 0,
(4.109)
that will prescribe the separable solution (4.108). Observe that both have the form of an
eigenfunction equation in which the separation constant λ plays the role of the eigenvalue.
We are, as always, interested only in nonzero solutions.

162
4 Separation of Variables
We have already solved the eigenvalue problem for w(θ).
According to (4.107),
w(θ + 2π) = w(θ) must be a 2π–periodic function. Therefore, by our earlier discussion,
this periodic boundary value problem has the nonzero eigenfunctions
1,
sin nθ,
cos nθ,
n = 1, 2, . . . ,
(4.110)
corresponding to the eigenvalues (separation constants)
λ = n2,
n = 0, 1, 2, . . ..
With the value of λ ﬁxed, the linear ordinary diﬀerential equation for the radial component,
r2v′′ + rv′ −n2v = 0,
(4.111)
does not have constant coeﬃcients. But, fortunately, it has the form of a second-order Euler
ordinary diﬀerential equation, [23, 89], and hence can be readily solved by substituting the
power ansatz v(r) = rk. (See also Exercise 4.3.23.) Note that
v′(r) = krk−1,
v′′(r) = k(k −1) rk−2,
and hence, by substituting into the diﬀerential equation,
r2v′′ + rv′ −n2v =

k(k −1) + k −n2 
rk = (k2 −n2)rk.
Thus, rk is a solution if and only if
k2 −n2 = 0,
and hence
k = ± n.
For n ̸= 0, we have found the two linearly independent solutions:
v1(r) = rn,
v2(r) = r−n,
n = 1, 2, . . ..
(4.112)
When n = 0, the power ansatz yields only the constant solution. But in this case, the
equation r2v′′ + rv′ = 0 is eﬀectively of ﬁrst order and linear in v′, and hence readily
integrated. This provides the two independent solutions
v1(r) = 1,
v2(r) = log r,
n = 0.
(4.113)
Combining (4.110) and (4.112–113), we produce the complete list of separable polar coor-
dinate solutions to the Laplace equation:
1,
rn cos nθ,
rn sin nθ,
log r,
r−n cos nθ,
r−n sin nθ,
n = 1, 2, 3, . . ..
(4.114)
Now, the solutions in the top row of (4.114) are continuous (in fact analytic) at the origin,
where r = 0, whereas the solutions in the bottom row have singularities as r →0. The
latter are not of use in the present situation, since we require that the solution remain
bounded and smooth — even at the center of the disk.
Thus, we should use only the
nonsingular solutions to concoct a candidate series solution
u(r, θ) = a0
2
+
∞

n=1

anrn cos nθ + bnrn sin nθ

.
(4.115)

4.3 The Planar Laplace and Poisson Equations
163
The coeﬃcients an, bn will be prescribed by the boundary conditions (4.106). Substituting
r = 1, we obtain
u(1, θ) = a0
2 +
∞

n=1

an cos nθ + bn sin nθ

= h(θ).
We recognize this as a standard Fourier series (3.29) (with θ replacing x) for the 2π periodic
function h(θ). Therefore,
an = 1
π
 π
−π
h(θ) cos nθ dθ,
bn = 1
π
 π
−π
h(θ) sin nθ dθ,
(4.116)
are precisely its Fourier coeﬃcients, cf. (3.35). In this manner, we have produced a series
solution (4.115) to the boundary value problem (4.105–106).
Remark: Introducing the complex variable
z = x + i y = r e i θ = r cos θ + i r sin θ
(4.117)
allows us to write
zn = rn e i nθ = rn cos nθ + i rn sin nθ.
(4.118)
Therefore, the nonsingular separable solutions are the harmonic polynomials
rn cos nθ = Re zn,
rn sin nθ = Im zn.
(4.119)
The ﬁrst few are listed in the following table:
n
Re zn
Im zn
0
1
0
1
x
y
2
x2 −y2
2xy
3
x3 −3xy2
3x2 y −y3
4
x4 −4x2y2 + y4
4x3 y −4xy3
Their general expression is obtained using the Binomial Formula:
zn = (x + i y)n
= xn + nxn−1( i y) +
n
2

xn−2( i y)2 +
n
3

xn−3( i y)3 + · · · + ( i y)n
= xn + i nxn−1 y −
n
2

xn−2 y2 −i
n
3

xn−3 y3 + · · · ,
where
n
k

=
n!
k! (n −k)!
(4.120)

164
4 Separation of Variables
Figure 4.10.
Membrane attached to a helical wire.
are the usual binomial coeﬃcients. Separating the real and imaginary terms, we produce
the explicit formulae
rn cos nθ = Re zn = xn −
n
2

xn−2 y2 +
n
4

xn−4 y4 + · · · ,
rn sin nθ = Im zn = nxn−1 y −
n
3

xn−3 y3 +
n
5

xn−5 y5 + · · · ,
(4.121)
for the two independent harmonic polynomials of degree n.
Example 4.5. Consider the Dirichlet boundary value problem on the unit disk with
u(1, θ) = θ
for
−π < θ < π.
(4.122)
The boundary data can be interpreted as a wire in the shape of a single turn of a spiral
helix sitting over the unit circle. The wire has a single jump discontinuity, of magnitude
2π, at the boundary point (−1, 0). The required Fourier series
h(θ) = θ ∼2

sin θ −sin 2θ
2
+ sin 3θ
3
−sin 4θ
4
+ · · ·

was already computed in Example 3.3. Therefore, invoking our solution formula (4.115–
116), we have
u(r, θ) = 2

r sin θ −r2 sin 2θ
2
+ r3 sin 3θ
3
−r4 sin 4θ
4
+ · · ·

(4.123)
is the desired solution, which is plotted in Figure 4.10. In fact, this series can be explicitly
summed. In view of (4.119) and the usual formula (A.13) for the complex logarithm, we
have
u = 2 Im

z −z2
2 + z3
3 −z4
4 + · · ·

= 2 Im log(1 + z) = 2ψ,
(4.124)

4.3 The Planar Laplace and Poisson Equations
165
ψ
(x, y)
Figure 4.11.
Geometric construction of the solution.
where
ψ = tan−1
y
1 + x
is the angle that the line passing through the two points (x, y) and (−1, 0) makes with the
x-axis, as sketched in Figure 4.11. You should try to convince yourself that, on the unit
circle, 2 ψ = θ has the correct boundary values. Observe that, even though the boundary
values are discontinuous, the solution is an analytic function inside the disk.
In fact, unlike the rectangular series (4.98), the general polar series solution for-
mula (4.115) can, in fact, be summed in closed form! If we substitute the explicit Fourier
formulae (4.116) into (4.115) — remembering to change the integration variable to, say, φ
to avoid a notational conﬂict — we obtain
u(r, θ) = a0
2 +
∞

n=1

an rn cos nθ + bn rn sin nθ

= 1
2π
 π
−π
h(φ) dφ +
∞

n=1
 rn cos nθ
π
 π
−π
h(φ) cos nφ dφ + rn sin nθ
π
 π
−π
h(φ) sin nφ dφ

= 1
π
 π
−π
h(φ)
$
1
2 +
∞

n=1
rn
cos nθ cos nφ + sin nθ sin nφ

%
dφ
= 1
π
 π
−π
h(φ)
$
1
2 +
∞

n=1
rn cos n(θ −φ)
%
dφ.
(4.125)
We next show how to sum the ﬁnal series. Using (4.118), we can write it as the real part
of a geometric series:
1
2 +
∞

n=1
rn cos nθ = Re

1
2 +
∞

n=1
zn

= Re
 1
2 +
z
1 −z

= Re

1 + z
2(1 −z)

= Re
 (1 + z)(1 −z)
2 | 1 −z |2

= Re (1 + z −z −| z |2)
2 | 1 −z |2
= 1 −| z |2
2 | 1 −z |2 =
1 −r2
2(1 + r2 −2r cos θ) ,
which is known as the Poisson kernel.
Substituting back into (4.125) establishes the
important Poisson Integral Formula for the solution to the boundary value problem.

166
4 Separation of Variables
Figure 4.12.
Equilibrium temperature of a disk.
Theorem 4.6.
The solution to the Laplace equation in the unit disk subject to
Dirichlet boundary conditions u(1, θ) = h(θ) is
u(r, θ) = 1
2π
 π
−π
h(φ)
1 −r2
1 + r2 −2 r cos(θ −φ) dφ.
(4.126)
Example 4.7. A uniform metal disk of unit radius has half of its circular boundary
held at 1◦, while the other half is held at 0◦. Our task is to ﬁnd the equilibrium temperature
u(x, y). In other words, we seek the solution to the Dirichlet boundary value problem
Δu = 0,
x2 + y2 < 1,
u(x, y) =

1,
x2 + y2 = 1,
y > 0,
0,
x2 + y2 = 1,
y < 0.
(4.127)
In polar coordinates, the boundary data is a (periodic) step function
h(θ) =
 1,
0 < θ < π,
0,
−π < θ < 0.
Therefore, according to the Poisson formula (4.126), the solution is given by†
u(r, θ) = 1
2π
 π
0
1 −r2
1 + r2 −2r cos(θ −φ) dφ =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1 −1
π tan−1
 1 −r2
2r sin θ

,
0 < θ < π,
1
2 ,
θ = 0, ±π,
−1
π tan−1
 1 −r2
2r sin θ

,
−π < θ < 0,
(4.128)
†
The detailed derivation of the ﬁnal expressions is left to the reader as Exercise 4.3.40.

4.3 The Planar Laplace and Poisson Equations
167
where we use the principal branch −1
2 π < tan−1 t < 1
2 π of the inverse tangent. Revert-
ing to rectangular coordinates, we ﬁnd that the equilibrium temperature has the explicit
formula
u(x, y) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
1 −1
π tan−1
 1 −x2 −y2
2y

,
x2 + y2 < 1,
y > 0,
1
2 ,
x2 + y2 < 1,
y = 0,
−1
π tan−1
 1 −x2 −y2
2y

,
x2 + y2 < 1,
y < 0.
(4.129)
The result is depicted in Figure 4.12.
Averaging, the Maximum Principle, and Analyticity
Let us investigate some important consequences of the Poisson integral formula (4.126).
First, setting r = 0 yields
u(0, θ) = 1
2π
 π
−π
h(φ) dφ.
(4.130)
The left-hand side is the value of u at the origin — the center of the disk — and so
independent of θ; the right-hand side is the average of its boundary values around the unit
circle. This formula is a particular instance of an important general fact.
Theorem 4.8. Let u(x, y) be harmonic inside a disk of radius a centered at a point
(x0, y0) with piecewise continuous (or, more generally, integrable) boundary values on the
circle C = {(x −x0)2 + (y −y0)2 = a2 }. Then its value at the center of the disk is equal
to the average of its values on the boundary circle:
u(x0, y0) =
1
2πa
&
C
u ds = 1
2π
 π
−π
u(x0 + a cos θ, y0 + a sin θ) dθ.
(4.131)
Proof : We use the scaling and translation symmetries of the Laplace equation, cf. Ex-
ercises 4.3.8–9, to map the disk of radius a centered at (x0, y0) to the unit disk centered at
the origin. Speciﬁcally, we set
U(x, y) = u(x0 + ax, y0 + ay).
(4.132)
An easy chain rule computation proves that U(x, y) also satisﬁes the Laplace equation on
the unit disk x2 + y2 < 1, with boundary values
h(θ) = U(cosθ, sin θ) = u(x0 + a cosθ, y0 + a sin θ).
Therefore, by (4.130),
U(0, 0) = 1
2π
 π
−π
h(θ) dθ = 1
2π
 π
−π
U(cos θ, sin θ) dθ.
Replacing U by its formula (4.132) produces the desired result.
Q.E.D.
An important consequence of the integral formula (4.131) is the Strong Maximum
Principle for harmonic functions.

168
4 Separation of Variables
Theorem 4.9.
Let u be a nonconstant harmonic function deﬁned on a bounded
domain Ω and continuous on ∂Ω. Then u achieves its maximum and minimum values only
at boundary points of the domain. In other words, if
m = min { u(x, y) | (x, y) ∈∂Ω } ,
M = max{ u(x, y) | (x, y) ∈∂Ω },
are, respectively, its maximum and minimum values on the boundary, then
m < u(x, y) < M
at all interior points
(x, y) ∈Ω.
Proof : Let M ⋆≥M be the maximum value of u on all of Ω = Ω ∪∂Ω, and assume
u(x0, y0) = M ⋆at some interior point (x0, y0) ∈Ω. Theorem 4.8 implies that u(x0, y0)
equals its average over any circle C centered at (x0, y0) that bounds a closed disk contained
in Ω. Since u is continuous and ≤M ⋆on C, its average must be strictly less than M ⋆
— except in the trivial case in which it is constant and equal to M ⋆on all of C. Thus,
our assumption implies that u(x, y) = M ⋆= u(x0, y0) for all (x, y) belonging to any
circle C ⊂Ω centered at (x0, y0). Since Ω is connected, this allows us to conclude† that
u(x, y) = M ⋆is constant throughout Ω, in contradiction to our original assumption.
A similar argument works for the minimum; alternatively, one can interchange maxi-
mum and minimum by replacing u by −u.
Q.E.D.
Physically, if we interpret u(x, y) as the vertical displacement of a membrane stretched
over a wire, then Theorem 4.9 says that, in the absence of external forcing, the membrane
cannot have any internal bumps — its highest and lowest points are necessarily on the
boundary of the domain. This reconﬁrms our physical intuition: the restoring force exerted
by the stretched membrane will serve to ﬂatten any bump, and hence a membrane with a
local maximum or minimum cannot be in equilibrium. A similar interpretation holds for
heat conduction. A body in thermal equilibrium will achieve its maximum and minimum
temperature only at boundary points. Indeed, thermal energy would ﬂow away from any
internal maximum, or towards any local minimum, and so if the body contained a local
maximum or minimum in its interior, it could not remain in thermal equilibrium.
The Maximum Principle immediately implies the uniqueness of solutions to the Dirich-
let boundary value problem for both the Laplace and Poisson equations:
Theorem 4.10. If u and u both satisfy the same Poisson equation −Δu = f = −Δu
within a bounded domain Ω, and u = u on ∂Ω, then u ≡u throughout Ω.
Proof : By linearity, the diﬀerence v = u−u satisﬁes the homogeneous boundary value
problem Δv = 0 in Ω and v = 0 on ∂Ω. Our assumption implies that the maximum and
minimum boundary values of v are both 0 = m = M. Theorem 4.9 implies that v(x, y) ≡0
at all (x, y) ∈Ω, and hence u ≡u everywhere in Ω.
Q.E.D.
Finally, let us discuss the analyticity of harmonic functions. In view of (4.119), the
nth order term in the polar series solution (4.115), namely,
an rn cos nθ + bn rn sin nθ = an Re zn + bn Im zn = Re

(an −i bn)zn 
,
is, in fact, a homogeneous polynomial in (x, y) of degree n. This means that, when written
in rectangular coordinates x and y, (4.115) is, in fact, a power series for the harmonic
†
You are asked to supply the details in Exercise 4.3.42.

4.3 The Planar Laplace and Poisson Equations
169
function u(x, y). It is well known, [8, 23, 97], that any convergent power series converges
to an analytic function — in this case u(x, y). Moreover, the power series must, in fact, be
the Taylor series for u(x, y) based at the origin, and so its coeﬃcients are multiples of the
derivatives of u at x = y = 0. Details are worked out in Exercise 4.3.49.
We can adapt this argument to prove analyticity of all solutions to the Laplace equa-
tion. Note especially the contrast with the wave equation, which has many non-analytic
solutions.
Theorem 4.11. A harmonic function is analytic at every point in the interior of its
domain of deﬁnition.
Proof : Let u(x, y) be a solution to the Laplace equation on the open domain Ω ⊂R2.
Let x0 = (x0, y0) ∈Ω, and choose a > 0 such that the closed disk of radius a centered at
x0 is entirely contained within Ω:
Da(x0) = {∥x −x0 ∥≤a} ⊂Ω,
where ∥· ∥is the usual Euclidean norm. Then the function U(x, y) deﬁned by (4.132) is
harmonic on the unit disk, with well-deﬁned boundary values. Thus, by the preceding
remarks, U(x, y) is analytic at every point inside the unit disk, and hence so is
u(x, y) = U
x −x0
a
, y −y0
a

at every point (x, y) in the interior of the disk Da(x0). Since x0 ∈Ω was arbitrary, this
establishes the analyticity of u throughout the domain.
Q.E.D.
This concludes our discussion of the method of separation of variables for the planar
Laplace equation and some of its important consequences. The method can be used in a
few other special coordinate systems. See [78, 79] for a complete account, including the
fascinating connections with the underlying symmetry properties of the equation.
Exercises
4.3.22. Solve the following Euler diﬀerential equations by use of the power ansatz:
(a) x2 u′′ + 5xu′ −5u = 0,
(b) 2x2 u′′ −xu′ −2u = 0,
(c) x2 u′′ −u = 0,
(d) x2 u′′ + xu′ −3u = 0,
(e) 3x2 u′′ −5xu′ −3u = 0,
(f ) d2u
dx2 + 2
x
du
dx = 0.
♦4.3.23. (i) Show that if u(x) solves the Euler diﬀerential equation
ax2 d2u
dx2 + bx du
dx + cu = 0,
(4.133)
then v(y) = u(ey) solves a linear constant-coeﬃcient diﬀerential equation.
(ii) Use this technique to solve the Euler diﬀerential equations in Exercise 4.3.22.
4.3.24.(a) Use the method in Exercise 4.3.23 to solve an Euler equation whose characteristic
equation has a double root r1 = r2 = r.
(b) Solve the speciﬁc equations
(i) x2 u′′ −xu′ + u = 0,
(ii) d2u
dx2 + 1
x
du
dx = 0.

170
4 Separation of Variables
4.3.25. Solve the following boundary value problems:
(a) Δu = 0, x2 + y2 < 1,
u = x3, x2 + y2 = 1;
(b) Δu = 0, x2 + y2 < 2,
u = log(x2 + y2), x2 + y2 = 1;
(c) Δu = 0, x2 + y2 < 4,
u = x4, x2 + y2 = 4;
(d) Δu = 0, x2 + y2 < 1,
∂u
∂n = x, x2 + y2 = 1.
4.3.26. Let u(x, y) be the solution to the boundary value problem uxx + uyy = 0, x2 + y2 < 1,
u(x, y) = x2, x2 + y2 = 1. Find u(0, 0).
♥4.3.27.(a) Find the equilibrium temperature on a disk of radius 1 when half the boundary is
held at 1◦and the other half is held at −1◦.
(b) Find the equilibrium temperature on a
half-disk of radius 1 when the temperature is held to 1◦on the curved edge and 0◦on the
straight edge. (c) Find the equilibrium temperature on a half disk of radius 1 when the
temperature is held to 0◦on the curved edge and 1◦on the straight edge.
4.3.28. Find the solution to Laplace’s equation uxx + uyy = 0 on the semi-disk x2 + y2 < 1,
y > 0, that satisﬁes the boundary conditions u(x, 0) = 0 for −1 < x < 1 and u(x, y) = y3
for x2 + y2 = 1, y > 0.
4.3.29. Find the equilibrium temperature on a half-disk of radius 1 when the temperature is
held to 1◦on the curved edge, while the straight edge is insulated.
4.3.30. Solve the Dirichlet boundary value problem for the Laplace equation on the pie wedge
W = {0 < θ < 1
4 π, 0 < r < 1}, when the nonzero boundary data u(1, θ) = h(θ) appears
only on the curved portion of its boundary.
4.3.31. Find a harmonic function u(x, y) deﬁned on the annulus 1
2 < r < 1 subject to the
constant Dirichlet boundary conditions u = a on r = 1
2 and u = b on r = 1.
4.3.32. Boiling water ﬂows continually through a long circular metal pipe of inner radius 1 cm
and outer radius 1.2 cm placed in an ice water bath. True or false: The temperature at the
midpoint, at radius 1.1 cm, is 50◦. If false, what is the temperature at this point?
4.3.33. Write out the series solution to the boundary value problem u(1, θ) = 0, u(2, θ) = h(θ),
for the Laplace equation on an annulus 1 < r < 2. Hint: Use all of the separable solutions
listed in (4.114).
4.3.34. Solve the following boundary value problems for the Laplace equation on the annulus
1 < r < 2:
(a) u(1, θ) = 0, u(2, θ) = 1,
(b) u(1, θ) = 0, u(2, θ) = cos θ,
(c) u(1, θ) = sin 2θ, u(2, θ) = cos 2θ,
(d) ur(1, θ) = 0, u(2, θ) = 1,
(e) ur(1, θ) = 0, u(2, θ) = sin 2θ,
(f ) ur(1, θ) = 0, ur(2, θ) = 1,
(g) ur(1, θ) = 2, ur(2, θ) = 1.
4.3.35. Solve the following boundary value problems for the Laplace equation on the semi-
annular domain D = {1 < x2 + y2 < 2, y > 0}:
(a) u(x, y) = 0,
x2 + y2 = 1,
u(x, y) = 1,
x2 + y2 = 2,
u(x, 0) = 0;
(b) u(x, y) = 0,
x2 + y2 = 1 or 2,
u(x, 0) = 0,
x > 0,
u(x, 0) = 1,
x < 0.
4.3.36. Solve the following boundary value problem:
(x2 + y2)(uxx + uyy) + 2xux + 2y uy = 0, x2 + y2 < 1, u(x, y) = 1 + 3x, x2 + y2 = 1.
♦4.3.37. Justify the chain rule computation (4.104). Then justify formula (4.105) for the Lapla-
cian in polar coordinates.
4.3.38. Suppose
	 π
−π | h(θ) | dθ < ∞. Prove that (4.115) converges uniformly to the solution to
the boundary value problem (4.101) on any smaller disk Dr⋆= {r ≤r⋆< 1} ⊊D1.
4.3.39. Prove directly that (4.124) satisﬁes the boundary conditions (4.122).

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations
171
♦4.3.40. Justify the integration formula in (4.128).
4.3.41. Provide a complete proof that (4.129) is indeed the solution to the boundary value
problem (4.127).
♦4.3.42. Complete the proof of Theorem 4.9 by showing that u(x, y) = M⋆for all (x, y) ∈Ω.
Hint: Join (x0, y0) to (x, y) by a curve C ⊂Ω of ﬁnite length, and use the preceding part
of the proof to inductively deduce the existence of a ﬁnite sequence of points (xi, yi) ∈C,
i = 0, . . . , n, with (xn, yn) = (x, y), and such that u(xi, yi) = M⋆.
♦4.3.43. Derive the analogue of the Poisson integral formula for the solution to the Neumann
boundary value problem Δu = 0, x2 + y2 < 1, ∂u/∂n = h, x2 + y2 = 1, on the unit disk.
Pay careful attention to the existence and uniqueness of solutions in your formulation.
4.3.44. Give an example of a solution to Poisson’s equation on the unit disk that achieves its
maximum at an interior point. Interpret your construction physically.
4.3.45. Let p(x, y) be a polynomial (not necessarily harmonic). Suppose u(x, y) is harmonic
and equals p(x, y) on the unit circle x2 + y2 = 1. Prove that u(x, y) is a harmonic polyno-
mial.
4.3.46. Write down an integral formula for the solution to the Dirichlet boundary value prob-
lem on a disk of radius R > 0, namely, Δu = 0, x2 + y2 < R2, u = h, x2 + y2 = R2.
4.3.47. State and prove a one-dimensional version of Theorem 4.8. Does the analogue of Theo-
rem 4.9 hold?
4.3.48. A unit area square plate has 100◦temperature on its top edge and 0◦on its three other
edges. True or false: The temperature at the center equals the average edge temperature.
♦4.3.49. Let u(x, y) be a harmonic function on the unit disk with boundary values h(θ) when
r = 1. Using the fact that (4.115) is the Taylor series for u(x, y) at the origin: (a) Find
integral formulas for its partial derivatives ux(0, 0), uy(0, 0), involving the boundary values
h(θ). (b) Generalize part (a) to the second-order derivatives uxx(0, 0), uxy(0, 0), uyy(0, 0).
4.3.50. Prove that if u(x, y) is a bounded harmonic function deﬁned on all of R2, then u is con-
stant. Hint: First generalize Exercise 4.3.49(a) to ﬁnd the value of its gradient, ∇u(x0, y0),
in terms of the values of u on a circle of radius a centered at (x0, y0). Then see what hap-
pens when the radius of the circle goes to ∞.
4.4 Classiﬁcation of Linear Partial Diﬀerential Equations
We have, at last, been introduced to the three paradigmatic linear second-order partial
diﬀerential equations for functions of two variables. The homogeneous versions are
(a) The wave equation:
utt −c2 uxx = 0,
hyperbolic,
(b) The heat equation:
ut −γ uxx = 0,
parabolic,
(c) Laplace’s equation:
uxx + uyy = 0,
elliptic.
The last column indicates the equation’s type, in accordance with the standard taxonomy
of partial diﬀerential equations; an explanation will appear momentarily. The wave, heat,
and Laplace equations are the prototypical representatives of these three fundamental gen-
res. Each genre has its own distinctive analytic features, physical manifestations, and even
numerical solution schemes. Equations governing vibrations, such as the wave equation,

172
4 Separation of Variables
are typically hyperbolic.
Equations modeling diﬀusion, such as the heat equation, are
parabolic. Hyperbolic and parabolic equations both typically represent dynamical pro-
cesses, and so one of the independent variables is identiﬁed as time. On the other hand,
equations modeling equilibrium phenomena, including the Laplace and Poisson equations,
are usually elliptic, and involve only spatial variables. Elliptic partial diﬀerential equations
are associated with boundary value problems, whereas parabolic and hyperbolic equations
require initial and initial-boundary value problems.
The classiﬁcation theory of real linear second-order partial diﬀerential equations for a
scalar-valued function u(t, x) depending on two variables† proceeds as follows. The most
general such equation has the form
L[u] = Autt + B utx + C uxx + Dut + E ux + F u = G,
(4.134)
where the coeﬃcients A, B, C, D, E, F are all allowed to be functions of (t, x), as is the
inhomogeneity or forcing function G(t, x). The equation is homogeneous if and only if
G ≡0. We assume that at least one of the leading coeﬃcients A, B, C is not identically
zero, since otherwise, the equation degenerates to a ﬁrst-order equation.
The key quantity that determines the type of such a partial diﬀerential equation is its
discriminant
Δ = B2 −4AC.
(4.135)
This should (and for good reason) remind the reader of the discriminant of the quadratic
equation
Q(x, y) = Ax2 + B xy + C y2 + Dx + E y + F = 0,
(4.136)
whose solutions trace out a plane curve — a conic section. In the nondegenerate cases, the
discriminant (4.135) ﬁxes its geometric type:
•
a hyperbola when Δ > 0,
•
a parabola when Δ = 0,
•
an ellipse when Δ < 0.
This motivates the choice of terminology used to classify second-order partial diﬀerential
equations.
Deﬁnition 4.12. At a point (t, x), the linear second-order partial diﬀerential equa-
tion (4.134) is called
• hyperbolic
if
Δ(t, x) > 0,
• parabolic
if
Δ(t, x) = 0, but A2 + B2 + C2 ̸= 0,
• elliptic
if
Δ(t, x) < 0,
• singular
if
A = B = C = 0.
In particular:
• The wave equation utt −uxx = 0 has discriminant Δ = 4, and is hyperbolic.
• The heat equation uxx −ut = 0 has discriminant Δ = 0, and is parabolic.
• The Poisson equation utt + uxx = −f has discriminant Δ = −4, and is elliptic.
†
For equilibrium equations, we identify t with the space variable y.

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations
173
Example 4.13. When the coeﬃcients A, B, C vary, the type of the partial diﬀerential
equation may not remain ﬁxed over the entire domain. Equations that change type are
less common, as well as being much harder to analyze and solve, both analytically and
numerically. One example arising in the theory of supersonic aerodynamics, [44], is the
Tricomi equation
x ∂2u
∂t2 −∂2u
∂x2 = 0.
(4.137)
Comparing with (4.134), we ﬁnd that
A = x,
B = 0,
C = −1,
while
D = E = F = G = 0.
The discriminant in this particular case is
Δ = B2 −4AC = 4x,
and hence the equation is hyperbolic when x > 0, elliptic when x < 0, and parabolic on the
transition line x = 0. In the physical model, the hyperbolic region corresponds to subsonic
ﬂow, while the supersonic regions are of elliptic type. The transitional parabolic boundary
represents the shock line between the sub- and super-sonic regions — the familiar sonic
boom as an airplane crosses the sound barrier.
While this tripartite classiﬁcation into hyperbolic, parabolic, and elliptic equations
initially appears in the bivariate context, the terminology, underlying properties, and as-
sociated physical models carry over to second-order partial diﬀerential equations in higher
dimensions. Most of the partial diﬀerential equations arising in applications fall into one
of these three categories, and it is fair to say that the ﬁeld of partial diﬀerential equations
splits into three distinct subﬁelds. Or rather four subﬁelds, the last containing all the equa-
tions, including higher-order equations, that do not ﬁt into the preceding categorization.
(One important example appears in Section 8.5.)
Remark: The classiﬁcation into hyperbolic, parabolic, elliptic, and singular types car-
ries over as stated to quasilinear second-order equations, whose coeﬃcients A, . . . , G are
allowed to depend on u and its ﬁrst-order derivatives, ut, ux. Here the type of the equation
can vary with both the point in the domain and the particular solution being considered.
Even more generally, for a fully nonlinear second-order partial diﬀerential equation
H(t, x, u, ut, ux, utt, utx, uxx) = 0,
(4.138)
one deﬁnes its discriminant to be
Δ =
 ∂H
∂utx
2
−4 ∂H
∂utt
∂H
∂uxx
.
(4.139)
Its sign determines the type of the equation as above — again depending on the point in
the domain and the solution under consideration.
Exercises
4.4.1. Plot the following conic sections and classify their type:
(a) x2 + 3y2 = 1,
(b) xy + x + y = 4,
(c) x2 −xy + y2 = x −2y,
(d) x2 + 2xy + y2 + y = 1,
(e) x2 −2y2 = 6x + 8y + 1.

174
4 Separation of Variables
4.4.2. Determine the type of the following partial diﬀerential equations:
(a) utt + 3uxx = 0,
(b) utx + ut + ux = u,
(c) utt + ut + ux = 0,
(d) utt −utx + uxx = u,
(e) utt + 4utx + 4uxx = ut,
(f ) utx + uxx = 0.
4.4.3. Consider the partial diﬀerential equation xutt + (t + x)uxx = 0. At what points of the
plane is the equation elliptic? hyperbolic? parabolic? degenerate?
4.4.4. Answer Exercise 4.4.3 for the equations
(a) x2 uxx + x ux + uyy = 0,
(b) ∂x(x ux) = ∂y(y uy),
(c) ut = ∂x[(x + t)ux ],
(d) ∇· (c(x, y)∇u) = u, where c(x, y) is a given function.
4.4.5. Steady ﬂow of air past an airplane is modeled by the partial diﬀerential equation
(m2 −1)uxx + uyy = 0, in which x is the ﬂight direction, y the transverse direction, and
m ≥0 is the Mach number — the ratio of the airplane’s speed to the speed of sound. Show
that the equation is hyperbolic for subsonic ﬂight, but elliptic for supersonic ﬂight.
4.4.6. Show that the second-order partial diﬀerential equation
−∂
∂x

p(x, y) ∂u
∂x

−∂
∂y

q(x, y) ∂u
∂y

+ r(x, y) u = f(x, y)
is elliptic if and only if p(x, y) and q(x, y) are nonzero and have the same sign.
♦4.4.7. True or false: The type of a linear second-order partial diﬀerential equation is not af-
fected by a change of independent variables: τ = ϕ(t, x), ξ = ψ(t, x).
4.4.8. Let v(t, x) = a(t, x) u(t, x) + b(t, x), where a, b are ﬁxed functions with a ̸= 0. Suppose u
is a solution to a second-order linear partial diﬀerential equation. Prove that v also solves a
linear partial diﬀerential equation of the same type.
♦4.4.9. True or false: The polar coordinate form (4.105) of the Laplace equation is elliptic.
4.4.10. Rewrite the Laplace equation uxx + uyy = 0 in terms of parabolic coordinates ξ, η, as
deﬁned by the equations x = ξ2 −η2, y = 2ξη. Is the resulting equation elliptic?
♦4.4.11. Prove that the complex change of variables x = x, t = i y, maps the Laplace equation
uxx +uyy = 0 to the wave equation utt = uxx. Explain why the type of a partial diﬀerential
equation is not necessarily preserved under a complex change of variables.
♥4.4.12. Suppose, against all advice, we pose the elliptic Laplace equation as an initial value
problem, namely
utt = −uxx
for
0 < x < 1,
t > 0,
u(0, x) = f(x),
ut(0, x) = 0,
0 ≤x ≤1,
u(t, 0) = 0 = u(t, 1),
t ≥0.
(a) Prove that for any positive integer n > 0, the function un(t, x) = sin nπ t cosh nπ x
n
satisﬁes the initial value problem. Determine the initial condition un(0, x) = fn(x).
(b) Prove that, as n →∞, the initial condition fn(x) →0 becomes vanishingly small,
whereas, at any t > 0, the solution value un

t, 1
2

→∞.
(c) Explain why this represents an ill-posed problem.
4.4.13. The minimal surface equation (1+u2
x)uxx−2uxuyuxy+(1+u2
y)uyy = 0 is (a) hyperbolic,
(b) parabolic, (c) elliptic, (d) singular, (e) of variable type depending on the point in the
domain, or (f ) of variable type depending on the solution and the point in the domain.
Characteristics and the Cauchy Problem
In Chapter 2, we discovered that the characteristic curves guide the behavior of solutions
to ﬁrst-order partial diﬀerential equations. Characteristics play a similarly fundamental
role in the analysis of more general hyperbolic partial diﬀerential equations and systems.

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations
175
In particular, they provide a mechanism for distinguishing among the various classes of
second-order partial diﬀerential equations.
As above, we will focus our attention on partial diﬀerential equations involving two
independent variables. The starting point is the general initial value problem, also known
as the Cauchy problem, in honor of the proliﬁc nineteenth-century French mathemati-
cian Augustin–Louis Cauchy, justly famous for his wide-ranging contributions throughout
mathematics and its applications, including the Cauchy–Schwarz inequality, many of the
fundamental concepts in complex analysis, as well as the foundations of elasticity and
materials science. The general Cauchy problem speciﬁes appropriate initial data along a
smooth curve† Γ ⊂R2 and seeks a solution to the partial diﬀerential equation that as-
sumes the given initial data on Γ. In all our examples, the curve in question has been a
straight line, e.g., the x–axis, but one could easily envisage more general situations. If the
partial diﬀerential equation has order n, then the Cauchy data consists of the values of the
dependent variable u along with all its partial diﬀerential equations up to order n −1 on
the curve Γ. For most curves, there is a unique solution u(t, x) to the partial diﬀerential
equation that achieves the speciﬁed values along Γ. More rigorously, if we are in the an-
alytic category, meaning that the partial diﬀerential equation, the curve, and the Cauchy
data are all speciﬁed by analytic functions, then the fundamental Cauchy–Kovalevskaya
Theorem guarantees the existence of an analytic solution u(t, x) to the Cauchy problem
near any point on the initial curve. The statement of proof of this important theorem, due
to Cauchy and, in general form, the inﬂuential nineteenth-century Russian mathematician
Soﬁa Kovalevskaya, relies on the construction of convergent power series for the desired
solution and would take us too far aﬁeld. We refer the interested reader to [35, 44]. The
exceptional curves, for which the Cauchy–Kovalevskaya Existence Theorem does not apply,
are called the characteristics of the underlying partial diﬀerential equations.
More prosaically, a curve Γ will be called non-characteristic for the given partial
diﬀerential equation if one can determine the values of all the derivatives of u along Γ
from the speciﬁed Cauchy data. Indeed, the determination of the values of the higher-
order derivatives along the curve is a necessary preliminary step towards establishing the
Cauchy–Kovalevskaya existence result. As we will now show, this requirement serves to dis-
tinguish the characteristic and non-characteristic curves for the examples we have already
encountered, and hence to lead to their characterization in much more general contexts.
To illustrate the preceding requirement, let us begin with a ﬁrst-order linear partial
diﬀerential equation of the form
∂u
∂t + c(t, x) ∂u
∂x = f(t, x).
(4.140)
Let Γ ⊂R2 be a smooth curve parametrized§ by x(s) =

t(s), x(s)
T , where smoothness
necessitates that its tangent vector not vanish: x′(s) = (dt/ds, dx/ds)T ̸= 0. Since the
equation is of order n = 1, the Cauchy data requires specifying the values of the dependent
variable u only along Γ — in other words, the function
h(s) = u

t(s), x(s)

.
(4.141)
†
More generally, for partial diﬀerential equations in m > 2 independent variables, the curve
is replaced by a hypersurface S ⊂Rm of dimension m −1.
§
The parameter s could be the arc length, but this is not required. See also Exercise 4.4.20.

176
4 Separation of Variables
The curve will be non-characteristic if we can then determine the values of the derivatives
of u along Γ, starting with
∂u
∂t

t(s), x(s)

,
∂u
∂x

t(s), x(s)

.
(4.142)
To this end, let us diﬀerentiate the Cauchy data (4.141): applying the chain rule, we obtain
h′(s) = d
ds u

t(s), x(s)

= ∂u
∂t

t(s), x(s)
 dt
ds + ∂u
∂x

t(s), x(s)
 dx
ds .
(4.143)
On the other hand, we are assuming that u(t, x) solves the partial diﬀerential equation
(4.140) at all points in its domain of deﬁnition. In particular, at points on the curve Γ, the
partial diﬀerential equation requires
∂u
∂t

t(s), x(s)

+ c

t(s), x(s)
 ∂u
∂x

t(s), x(s)

= f

t(s), x(s)

.
(4.144)
We can regard (4.143–144) as a pair of inhomogeneous linear algebraic equations, which
can be uniquely solved for the as yet unknown quantities (4.142), unless the determinant
of their coeﬃcient matrix vanishes:
det

1
c

t(s), x(s)

dt/ds
dx/ds

= dx
ds −c

t(s), x(s)
 dt
ds = 0.
(4.145)
This condition serves to deﬁne a characteristic curve for the ﬁrst-order partial diﬀerential
equation (4.140). In particular, if the curve is parametrized by s = t, i.e., can be identiﬁed
with the graph of a function x = g(t), then the characteristic condition (4.145) reduces to
dx
dt = c(t, x),
(4.146)
thus reproducing our original deﬁnition of characteristic curve, as in (2.18) and, more
generally, Exercise 2.2.26. On the other hand, if the determinant (4.145) is nonzero, then
one can solve (4.143–144) for the values of the ﬁrst-order derivatives (4.142) along Γ.
Further diﬀerentiation of these conditions proves that one can, in fact, determine the
values of all the higher-order derivatives of the solution u along the curve, which is hence
non-characteristic.
Next, consider a nonsingular linear second-order partial diﬀerential equation of the
form (4.134).
Since the equation has order n = 2, the Cauchy data along a curve Γ
parametrized as above consists of the values of the function and its ﬁrst derivatives:
u

t(s), x(s)

,
∂u
∂t

t(s), x(s)

,
∂u
∂x

t(s), x(s)

.
(4.147)
However, the latter cannot be speciﬁed independently.
Indeed, given the value of the
dependent variable, h(s) = u

t(s), x(s)

, along Γ, its derivative
h′(s) = d
ds u

t(s), x(s)

= ∂u
∂t

t(s), x(s)
 dt
ds + ∂u
∂x

t(s), x(s)
 dx
ds
(4.148)
prescribes a particular combination of the two ﬁrst-order derivatives.
Thus, once the
value of one derivative of u on Γ is known, the other is automatically ﬁxed by the relation
(4.148). For example, if dx/ds ̸= 0, we can use (4.148) to determine ux

t(s), x(s)

, knowing
u

t(s), x(s)

and ut

t(s), x(s)

. Similarly, if we diﬀerentiate the values of the ﬁrst-order

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations
177
derivatives with respect to the curve parameter, we can determine two combinations of
second-order derivatives along the curve Γ:
d
ds
∂u
∂t

t(s), x(s)

= ∂2u
∂t2

t(s), x(s)
 dt
ds + ∂2u
∂t ∂x

t(s), x(s)
 dx
ds ,
d
ds
∂u
∂x

t(s), x(s)

= ∂2u
∂t ∂x

t(s), x(s)
 dt
ds + ∂2u
∂x2

t(s), x(s)
 dx
ds .
(4.149)
On the other hand, the partial diﬀerential equation (4.134) induces yet a third relation
among the second-order partial derivatives utt, utx, uxx. These three linear equations can
be uniquely solved for values of these derivatives on Γ if and only if the determinant of
their coeﬃcient matrix is nonzero:
det
⎛
⎝
A(t, x)
B(t, x)
C(t, x)
dt/ds
dx/ds
0
0
dt/ds
dx/ds
⎞
⎠= A(t, x)
dx
ds
2
−B(t, x) dt
ds
dx
ds + C(t, x)
 dt
ds
2
= 0.
(4.150)
We conclude that a smooth curve x(s) =

t(s), x(s)
T ⊂R2 is a characteristic curve
for the nonsingular linear second-order partial diﬀerential equation (4.134) whenever its
tangent vector x′(s) = (dt/ds, dx/ds)T ̸= 0 satisﬁes the quadratic characteristic equation
(4.150). Conversely, if the curve is non-characteristic, meaning that its tangent does not
satisfy (4.150) anywhere, then one can, with some further work, determine all the higher-
order derivatives of the solution u(t, x) along Γ, and then, at least in the analytic category,
prove existence of a solution to the Cauchy problem, [35].
According to Exercise 4.4.20, the status of a curve as characteristic or not does not
depend on the choice of parametrization. In particular, if the curve is given by the graph
of the function x = x(t), which we parametrize by s = t, then the characteristic equation
(4.150) takes the form of a quadratically nonlinear ﬁrst-order ordinary diﬀerential equation
A(t, x)
dx
dt
2
−B(t, x) dx
dt + C(t, x) = 0,
(4.151)
whose solutions are characteristic curves of the second-order partial diﬀerential equation.
Warning: If A(t, x) = 0, then the partial diﬀerential equation admits characteristic
curves with vertical tangents that cannot be parametrized by s = t.
For example, if
A(t, x) ≡0, then the vertical lines e.g., t = constant, x = s, are characteristic, satisfying
(4.150), but do not appear as solutions to (4.151).
For example, consider the hyperbolic wave equation
utt −c2 uxx = 0.
According to (4.151), any characteristic curve that is given by the graph of x(t) must solve
dx
dt
2
−c2 = 0,
which implies that
dx
dt = ± c.
Thus, in accordance with our previous analysis, the characteristic curves are the straight
lines of slope ±c, and there are two characteristic curves passing through each point of the
(t, x)–plane. On the other hand, the elliptic Laplace equation
utt + uxx = 0

178
4 Separation of Variables
has no (real) characteristic curves, since the characteristic equation (4.150) reduces to
dx
ds
2
+
 dt
ds
2
= 0,
and ts and xs are not allowed to vanish simultaneously. Finally, for the parabolic heat
equation
uxx −ut = 0,
the characteristic curve equation (4.150) is simply
 dt
ds
2
= 0
(since the ﬁrst-derivative term plays no role), and so there is only one characteristic curve
passing through each point, namely the vertical line t = constant. Observe that the stan-
dard initial value problem u(0, x) = f(x) for the heat equation takes place on a character-
istic curve — the x–axis — but does not take the form of a Cauchy problem, which would
also require specifying the ﬁrst-order derivatives ut(0, x), ux(0, x) there. And indeed, the
standard initial value problem is not well-posed near the characteristic x–axis for negative
t < 0.
In general, the number of real solutions to the nondegenerate quadratic characteristic
curve equation (4.150) depends on its discriminant Δ = B2 −4AC: In the hyperbolic
case, Δ > 0, and there are two real characteristic curves passing through each point; in
the parabolic case, Δ = 0, and there is just one real characteristic curve passing through
each point; in the elliptic case, Δ < 0, and there are no real characteristic curves. In this
manner, elliptic, parabolic, and hyperbolic partial diﬀerential equations are distinguished
by the number of (real) characteristic curves passing through a point — namely, zero,
one, and two, respectively.
First-order partial diﬀerential equations are also viewed as
hyperbolic, since they always admit real characteristic curves.
With further analysis, [35, 70, 122], it can be shown that, as with the wave equation,
signals and disturbances propagate along characteristic curves. Thus, hyperbolic equa-
tions share many qualitative properties with the wave equation, with signals moving in
two diﬀerent directions. For example, light rays move along characteristic curves, and are
thereby subject to the optical phenomena of refraction and focusing. Similarly, since the
characteristic curves for the parabolic heat equation are the vertical lines, this indicates
that the eﬀect of a disturbance at a point (t, x) = (t0, x0) is simultaneously felt along the
entire contemporaneous vertical line t = t0. This has the implication that disturbances in
the heat equation propagate at inﬁnite speed — a counterintuitive fact that will be further
expounded on in Section 8.1. Elliptic equations have no characteristics, and as a conse-
quence, do not support propagating signals; indeed, the eﬀect of a localized disturbance
is immediately felt throughout the domain. For example, even when an external force is
concentrated near a single point, it displaces the entire membrane.
Exercises
4.4.14. Find and graph the real characteristic curves for each of the partial diﬀerential equa-
tions in Exercise 4.4.2.

4.4 Classiﬁcation of Linear Partial Diﬀerential Equations
179
4.4.15. Graph the characteristic curves for the Tricomi equation (4.137) in its hyperbolic region.
What happens to the characteristics as one approaches the parabolic transition boundary?
4.4.16. True or false: The characteristic curves of the Helmholtz equation uxx + uyy −u = 0 are
circles.
4.4.17.(a) At what points of the plane is the partial diﬀerential equation xuxx + y uyy = 0
elliptic? parabolic? hyperbolic? (b) How many characteristics are there through the point
(1, −1)? (c) Find them explicitly.
4.4.18. Consider the partial diﬀerential equation uxx + y uxy = y2.
(a) On which regions of the (x, y)–plane is the equation elliptic? parabolic? hyperbolic?
(b) Find the characteristics in the hyperbolic region.
(c) Find the general solution in the hyperbolic region. Hint: Use characteristic coordinates.
4.4.19. Find a partial diﬀerential equation whose characteristic curves are:
(a) the lines x −y = a, x + 2y = b, where a, b ∈R are arbitrary constants;
(b) the exponential curves y = cex for c ∈R;
(c) the concentric circles x2 + y2 = a for a ≥0, and the rays y = bx.
♦4.4.20. Prove that any reparametrization of a characteristic curve for a given second-order lin-
ear partial diﬀerential equation is also a characteristic curve.
4.4.21. True or false: You can uniquely recover a second-order partial diﬀerential equation by
knowing all its characteristic curves.
♦4.4.22. Prove that any invertible change of variables, as in Exercise 4.4.7, maps the character-
istic curves of the original linear partial diﬀerential equation to the characteristic curves of
the transformed equation. Thus, characteristic curves are intrinsic: they do not depend on
the parametrization, nor on the coordinates used to represent the partial diﬀerential equa-
tion.

Chapter 5
Finite Diﬀerences
As one quickly learns, the diﬀerential equations that can be solved by explicit analytic
formulas are few and far between. Consequently, the development of accurate numerical
approximation schemes is an essential tool for extracting quantitative information as well
as achieving a qualitative understanding of the possible behaviors of solutions to the vast
majority of partial diﬀerential equations. (On the other hand, the successful design of
numerical algorithms necessitates a fairly deep understanding of their basic analytic prop-
erties, and so exclusive reliance on numerics is not an option.) Even in cases, such as
the heat and wave equations, in which explicit solution formulas (either in closed form or
inﬁnite series) exist, numerical methods can still be proﬁtably employed. Indeed, one can
accurately test a proposed numerical algorithm by running it on a known solution. As we
will see, the lessons learned in the design and testing of numerical algorithms on simpler
“solved” examples are of inestimable value when confronting more challenging problems.
Many of the basic numerical solution schemes for partial diﬀerential equations can be
ﬁt into two broad themes. The ﬁrst, to be presented in the present chapter, is that of
ﬁnite diﬀerence methods, obtained by replacing the derivatives in the equation by appro-
priate numerical diﬀerentiation formulae. We thus start with a brief discussion of some
elementary ﬁnite diﬀerence formulas used to numerically approximate ﬁrst- and second-
order derivatives of functions. We then establish and analyze some of the most basic ﬁnite
diﬀerence schemes for the heat equation, ﬁrst-order transport equations, the second-order
wave equation, and the Laplace and Poisson equations. As we will learn, not all ﬁnite dif-
ference schemes produce accurate numerical approximations, and one must confront issues
of stability and convergence in order to distinguish reliable from worthless methods. In
fact, inspired by Fourier analysis, the key numerical stability criterion is a consequence of
the scheme’s handling of complex exponentials.
The second category of numerical solution techniques comprises the ﬁnite element
methods, which will be the topic of Chapter 10. These two chapters should be regarded as
but a preliminary excursion into this vast and active area of contemporary research. More
sophisticated variations and extensions, as well as other classes of numerical integration
schemes, e.g., spectral, pseudo-spectral, multigrid, multipole, probabilistic (Monte Carlo,
etc.), geometric, symplectic, and many more, can be found in specialized numerical analysis
texts, including [6, 51, 60, 80, 94], and research papers. Also, the journal Acta Numerica
is an excellent source of survey papers on state-of-the-art numerical methods for a broad
range of disciplines.
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
181
5
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

182
5 Finite Diﬀerences
5.1 Finite Diﬀerence Approximations
In general, a ﬁnite diﬀerence approximation to the value of some derivative of a scalar
function u(x) at a point x0 in its domain, say u′(x0) or u′′(x0), relies on a suitable com-
bination of sampled function values at nearby points. The underlying formalism used to
construct these approximation formulas is known as the calculus of ﬁnite diﬀerences. Its
development has a long and inﬂuential history, dating back to Newton.
We begin with the ﬁrst-order derivative. The simplest ﬁnite diﬀerence approximation
is the ordinary diﬀerence quotient
u(x + h) −u(x)
h
≈u′(x),
(5.1)
which appears in the original calculus deﬁnition of the derivative. Indeed, if u is diﬀeren-
tiable at x, then u′(x) is, by deﬁnition, the limit, as h →0 of the ﬁnite diﬀerence quotients.
Geometrically, the diﬀerence quotient measures the slope of the secant line through the
two points (x, u(x)) and (x + h, u(x + h)) on its graph. For small enough h, this should be
a reasonably good approximation to the slope of the tangent line, u′(x), as illustrated in
the ﬁrst picture in Figure 5.1. Throughout our discussion, h, the step size, which may be
either positive or negative, is assumed to be small: | h | ≪1. When h > 0, (5.1) is referred
to as a forward diﬀerence, while h < 0 yields a backward diﬀerence.
How close an approximation is the diﬀerence quotient? To answer this question, we
assume that u(x) is at least twice continuously diﬀerentiable, and examine its ﬁrst-order
Taylor expansion
u(x + h) = u(x) + u′(x) h + 1
2 u′′(ξ) h2
(5.2)
at the point x. We have used Lagrange’s formula for the remainder term, [8, 97], in which
ξ, which depends on both x and h, is a point lying between x and x + h. Rearranging
(5.2), we obtain
u(x + h) −u(x)
h
−u′(x) = 1
2 u′′(ξ) h.
Thus, the error in the ﬁnite diﬀerence approximation (5.1) can be bounded by a multiple
of the step size:

u(x + h) −u(x)
h
−u′(x)
 ≤C | h |,
where C = max 1
2 | u′′(ξ) | depends on the magnitude of the second derivative of the function
over the interval in question. Since the error is proportional to the ﬁrst power of h, we
say that the ﬁnite diﬀerence quotient (5.1) is a ﬁrst-order approximation to the derivative
u′(x). When the precise formula for the error is not so important, we will write
u′(x) = u(x + h) −u(x)
h
+ O(h).
(5.3)
The “big Oh” notation O(h) refers to a term that is proportional to h, or, more precisely,
whose absolute value is bounded by a constant multiple of | h | as h →0.
Example 5.1. Let u(x) = sin x. Let us try to approximate
u′(1) = cos 1 = .5403023 . . .

5.1 Finite Diﬀerence Approximations
183
Forward diﬀerence
Central diﬀerence
Figure 5.1.
Finite diﬀerence approximations.
by computing ﬁnite diﬀerence quotients
cos 1 ≈sin(1 + h) −sin 1
h
.
The result for smaller and smaller (positive) values of h is listed in the following table.
h
.1
.01
.001
.0001
approximation
.497364
.536086
.539881
.540260
error
−.042939
−.004216
−.000421
−.000042
We observe that reducing the step size by a factor of
1
10 reduces the size of the error by
approximately the same factor. Thus, to obtain 10 decimal digits of accuracy, we anticipate
needing a step size of about h = 10−11. The fact that the error is more or less proportional
to the step size conﬁrms that we are dealing with a ﬁrst-order numerical approximation.
To approximate higher-order derivatives, we need to evaluate the function at more
than two points. In general, an approximation to the nth order derivative u(n)(x) requires
at least n + 1 distinct sample points. For simplicity, we restrict our attention to equally
spaced sample points, although the methods introduced can be readily extended to more
general conﬁgurations.
For example, let us try to approximate u′′(x) by sampling u at the particular points
x, x + h, and x −h. Which combination of the function values u(x −h), u(x), u(x + h)
should be used? The answer is found by consideration of the relevant Taylor expansions†
u(x + h) = u(x) + u′(x) h + u′′(x) h2
2 + u′′′(x) h3
6 + O(h4),
u(x −h) = u(x) −u′(x) h + u′′(x) h2
2 −u′′′(x) h3
6 + O(h4),
(5.4)
where the error terms are proportional to h4. Adding the two formulas together yields
u(x + h) + u(x −h) = 2u(x) + u′′(x) h2 + O(h4).
†
Throughout, the function u(x) is assumed to be suﬃciently smooth so that any derivatives
that appear are well deﬁned and the expansion formula is valid.

184
5 Finite Diﬀerences
Dividing by h2 and rearranging terms, we arrive at the centered ﬁnite diﬀerence approxi-
mation to the second derivative of a function:
u′′(x) = u(x + h) −2u(x) + u(x −h)
h2
+ O(h2).
(5.5)
Since the error is proportional to h2, this forms a second-order approximation.
Example 5.2. Let u(x) = ex2, with u′′(x) = (4x2 + 2)ex2. Let us approximate
u′′(1) = 6e = 16.30969097 . . .
using the ﬁnite diﬀerence quotient (5.5):
u′′(1) = 6e ≈e(1+h)2 −2e + e(1−h)2
h2
.
The results are listed in the following table.
h
.1
.01
.001
.0001
approximation
16.48289823
16.31141265
16.30970819
16.30969115
error
.17320726
.00172168
.00001722
.00000018
Each reduction in step size by a factor of
1
10 reduces the size of the error by a factor of
about
1
100, thereby gaining two new decimal digits of accuracy, which conﬁrms that the
centered ﬁnite diﬀerence approximation is of second order.
However, this prediction is not completely borne out in practice. If we take h = .00001
then the formula produces the approximation 16.3097002570, with an error of .0000092863
— which is less accurate than the approximation with h = .0001. The problem is that
round-oﬀerrors due to the ﬁnite precision of numbers stored in the computer (in the pre-
ceding computation we used single-precision ﬂoating-point arithmetic) have now begun to
aﬀect the computation. This highlights the inherent diﬃculty with numerical diﬀerentia-
tion: Finite diﬀerence formulae inevitably require dividing very small quantities, and so
round-oﬀinaccuracies may produce noticeable numerical errors. Thus, while they typi-
cally produce reasonably good approximations to the derivatives for moderately small step
sizes, achieving high accuracy requires switching to higher-precision computer arithmetic.
Indeed, a similar comment applies to the previous computation in Example 5.1. Our ex-
pectations about the error were not, in fact, fully justiﬁed, as you may have discovered had
you tried an extremely small step size.
Another way to improve the order of accuracy of ﬁnite diﬀerence approximations is to
employ more sample points. For instance, if the ﬁrst-order approximation (5.3) to u′(x)
based on the two points x and x + h is not suﬃciently accurate, one can try combining the
function values at three points, say x, x+h, and x−h. To ﬁnd the appropriate combination
of function values u(x −h), u(x), u(x + h), we return to the Taylor expansions (5.4). To
solve for u′(x), we subtract the two formulas, and so
u(x + h) −u(x −h) = 2u′(x)h + O(h3).
Rearranging the terms, we are led to the well-known centered diﬀerence formula
u′(x) = u(x + h) −u(x −h)
2h
+ O(h2),
(5.6)

5.1 Finite Diﬀerence Approximations
185
which is a second-order approximation to the ﬁrst derivative. Geometrically, the centered
diﬀerence quotient represents the slope of the secant line passing through the two points
(x −h, u(x −h)) and (x + h, u(x + h)) on the graph of u, which are centered symmetrically
about the point x. Figure 5.1 illustrates the two approximations, and the advantage of
the centered diﬀerence version is graphically evident. Higher-order approximations can be
found by evaluating the function at yet more sample points, say, x + 2h, x −2h, etc.
Example 5.3. Return to the function u(x) = sin x considered in Example 5.1. The
centered diﬀerence approximation to its derivative u′(1) = cos 1 = .5403023 . . . is
cos 1 ≈sin(1 + h) −sin(1 −h)
2h
.
The results are tabulated as follows:
h
.1
.01
.001
.0001
approximation
.53940225217
.54029330087
.54030221582
.54030230497
error
−.00090005370
−.00000900499
−.00000009005
−.00000000090
As advertised, the results are much more accurate than the one-sided ﬁnite diﬀerence
approximation used in Example 5.1 at the same step size.
Since it is a second-order
approximation, each reduction in the step size by a factor of
1
10 results in two more decimal
places of accuracy — up until the point where the eﬀects of round-oﬀerror kick in.
Many additional ﬁnite diﬀerence approximations can be constructed by similar ma-
nipulations of Taylor expansions, but these few very basic formulas, along with a couple
that are derived in the exercises, will suﬃce for our purposes. (For a thorough treatment
of the calculus of ﬁnite diﬀerences, the reader can consult [74].) In the following sections,
we will employ the ﬁnite diﬀerence formulas to devise numerical solution schemes for a va-
riety of partial diﬀerential equations. Applications to the numerical integration of ordinary
diﬀerential equations can be found, for example, in [24, 60, 63].
Exercises
♣5.1.1. Use the ﬁnite diﬀerence formula (5.3) with step sizes h = .1, .01, and .001 to approximate
the derivative u′(1) of the following functions u(x). Discuss the accuracy of your approxi-
mation.
(a) x4,
(b)
1
1 + x2 ,
(c) log x,
(d) cos x,
(e) tan−1 x.
♣5.1.2. Repeat Exercise 5.1.1 using the centered diﬀerence formula (5.6). Compare your ap-
proximations with those in the previous exercise — are the values in accordance with the
claimed orders of accuracy?
♣5.1.3. Approximate the second derivative u′′(1) of the functions in Exercise 5.1.1 using the
ﬁnite diﬀerence formula (5.5) with h = .1, .01, and .001. Discuss the accuracy of your
approximations.
5.1.4. Construct ﬁnite diﬀerence approximations to the ﬁrst and second derivatives of a func-
tion u(x) using its values at the points x−k, x, x+h, where h, k ≪1 are of comparable size,
but not necessarily equal. What can you say about the error in the approximation?

186
5 Finite Diﬀerences
♠5.1.5. In this exercise, you are asked to derive some basic one-sided ﬁnite diﬀerence formulas,
which are used for approximating derivatives of functions at or near the boundary of their
domain. (a) Construct a ﬁnite diﬀerence formula that approximates the derivative u′(x)
using the values of u(x) at the points x, x + h, and x + 2h. What is the order of your for-
mula? (b) Find a ﬁnite diﬀerence formula for u′′(x) that involves the same three function
values. What is its order? (c) Test your formulas by computing approximations to the ﬁrst
and second derivatives of u(x) = ex2 at x = 1 using step sizes h = .1, .01, and .001. What
is the error in your numerical approximations? Are the errors compatible with the theoreti-
cal orders of the ﬁnite diﬀerence formulas? Discuss why or why not. (d) Answer part (c) at
the point x = 0.
♣5.1.6.(a) Using the function values u(x), u(x + h), u(x + 3h), construct a numerical approxi-
mation to the derivative u′(x).
(b) What is the order of accuracy of your approximation?
(c) Test your approximation on the function u(x) = cos x at x = 1 using the step sizes
h = .1, .01, and .001. Are the errors consistent with your answer in part (b)?
♣5.1.7. Answer Exercise 5.1.6 for the second derivative u′′(x).
5.1.8.(a) Find the order of the ﬁve-point centered ﬁnite diﬀerence approximation
u′(x) ≈−u(x + 2h) + 8u(x + h) −8u(x −h) + u(x −2h)
12h
.
(b) Test your result on the function (1 + x2)−1 at x = 1 using the values h = .1, .01, .001.
5.1.9.(a) Using the formula in Exercise 5.1.8 as a guide, ﬁnd ﬁve-point ﬁnite diﬀerence formu-
las to approximate (i) u′′(x), (ii) u′′′(x), (iii) u(iv)(x). What is the order of accuracy?
(b) Test your formulas on the function (1+ x2)−1 at x = 1 using the values h = .1, .01, .001.
5.2 Numerical Algorithms for the Heat Equation
Consider the heat equation
∂u
∂t = γ ∂2u
∂x2 ,
0 < x < ℓ,
t > 0,
(5.7)
on an interval of length ℓ, with constant thermal diﬀusivity γ > 0.
We impose time-
dependent Dirichlet boundary conditions
u(t, 0) = α(t),
u(t, ℓ) = β(t),
t > 0,
(5.8)
ﬁxing the temperature at the ends of the interval, along with the initial conditions
u(0, x) = f(x),
0 ≤x ≤ℓ,
(5.9)
specifying the initial temperature distribution. In order to eﬀect a numerical approximation
to the solution to this initial-boundary value problem, we begin by introducing a rectangular
mesh consisting of nodes (tj, xm) ∈R2 with
0 = t0 < t1 < t2 < · · ·
and
0 = x0 < x1 < · · · < xn = ℓ.
For simplicity, we maintain a uniform mesh spacing in both directions, with
Δt = tj+1 −tj,
Δx = xm+1 −xm = ℓ
n ,

5.2 Numerical Algorithms for the Heat Equation
187
representing, respectively, the time step size and the spatial mesh size. It will be essential
that we do not a priori require that the two be the same. We shall use the notation
uj,m ≈u(tj, xm),
where
tj = j Δt,
xm = m Δx,
(5.10)
to denote the numerical approximation to the solution value at the indicated node.
As a ﬁrst attempt at designing a numerical solution scheme, we shall employ the
simplest ﬁnite diﬀerence approximations to the derivatives appearing in the equation. The
second-order space derivative is approximated by the centered diﬀerence formula (5.5), and
hence
∂2u
∂x2 (tj, xm) ≈u(tj, xm+1) −2 u(tj, xm) + u(tj, xm−1)
(Δx)2
+ O

(Δx)2 
≈uj,m+1 −2 uj,m + uj,m−1
(Δx)2
+ O

(Δx)2 
,
(5.11)
where the error in the approximation is proportional to (Δx)2. Similarly, the one-sided
ﬁnite diﬀerence approximation (5.3) is used to approximate the time derivative, and so
∂u
∂t (tj, xm) ≈
u(tj+1, xm) −u(tj, xm)
Δt
+ O(Δt) ≈
uj+1,m −uj,m
Δt
+ O(Δt),
(5.12)
where the error is proportional to Δt.
In general, one should try to ensure that the
approximations have similar orders of accuracy, which leads us to require
Δt ≈(Δx)2.
(5.13)
Assuming Δx < 1, this implies that the time steps must be much smaller than the space
mesh size.
Remark: At this stage, the reader might be tempted to replace (5.12) by the second-
order central diﬀerence approximation (5.6). However, this introduces signiﬁcant compli-
cations, and the resulting numerical scheme is not practical; see Exercise 5.2.10.
Replacing the derivatives in the heat equation (5.14) by their ﬁnite diﬀerence approx-
imations (5.11, 12) and rearranging terms, we end up with the linear system
uj+1,m = μuj,m+1 + (1 −2μ)uj,m + μuj,m−1,
j = 0, 1, 2, . . . ,
m = 1, . . . , n −1,
(5.14)
in which
μ = γ Δt
(Δx)2 .
(5.15)
The resulting scheme is of iterative form, whereby the solution values uj+1,m ≈u(tj+1, xm)
at time tj+1 are successively calculated, via (5.14), from those at the preceding time tj.
The initial condition (5.9) indicates that we should initialize our numerical data by
sampling the initial temperature at the nodes:
u0,m = fm = f(xm),
m = 1, . . ., n −1.
(5.16)
Similarly, the boundary conditions (5.8) require that
uj,0 = αj = α(tj),
uj,n = βj = β(tj),
j = 0, 1, 2, . . . .
(5.17)

188
5 Finite Diﬀerences
For consistency, we should assume that the initial and boundary conditions agree at the
corners of the domain:
f0 = f(0) = u(0, 0) = α(0) = α0,
fn = f(ℓ) = u(0, ℓ) = β(0) = β0.
The three equations (5.14, 16, 17) completely prescribe the numerical approximation scheme
for the solution to the initial-boundary value problem (5.7–9).
Let us rewrite the preceding equations in a more transparent vectorial form. First, let
u(j) =

uj,1, uj,2, . . . , uj,n−1
T ≈

u(tj, x1), u(tj, x2), . . . , u(tj, xn−1)
T
(5.18)
be the vector whose entries are the numerical approximations to the solution values at time
tj at the interior nodes. We omit the boundary nodes (tj, x0), (tj, xn), since those values
are ﬁxed by the boundary conditions (5.17). Then (5.14) takes the form
u(j+1) = Au(j) + b(j),
(5.19)
where
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1 −2μ
μ
μ
1 −2μ
μ
μ
1 −2μ
μ
μ
...
...
...
...
μ
μ
1 −2μ
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
b(j) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
μ αj
0
0
...
0
μ βj
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(5.20)
The (n−1)×(n−1) coeﬃcient matrix A is symmetric and tridiagonal, and only its nonzero
entries are displayed. The contributions (5.17) of the boundary nodes appear in the vector
b(j) ∈Rn−1. This numerical method is known as an explicit scheme, since each iterate is
computed directly from its predecessor without having to solve any auxiliary equations —
unlike the implicit schemes to be discussed next.
Example 5.4.
Let us ﬁx the diﬀusivity γ = 1 and the interval length ℓ= 1. For
illustrative purposes, we take a spatial step size of Δx = .1. We work with the initial data
u(0, x) = f(x) =
⎧
⎪
⎨
⎪
⎩
−x,
0 ≤x ≤1
5,
x −2
5,
1
5 ≤x ≤
7
10,
1 −x,
7
10 ≤x ≤1,
used earlier in Example 4.1. In Figure 5.2 we compare the numerical solutions resulting
from two (slightly) diﬀerent time step sizes. The ﬁrst row uses Δt = (Δx)2 = .01 and plots
the solution at the indicated times. The numerical solution is already showing signs of
instability (the ﬁnal plot does not even ﬁt in the window), and indeed, soon thereafter, it
becomes completely wild. The second row takes Δt = .005. Even though we are employing
a rather coarse mesh, the numerical solution is not too far away from the true solution to
the initial value problem, which can be seen in Figure 4.1.
Stability Analysis
In light of the preceding calculation, we need to understand why our numerical scheme
sometimes gives reasonable answers but sometimes utterly fails. To this end, we investigate

5.2 Numerical Algorithms for the Heat Equation
189
t = 0
t = .02
t = .04
Figure 5.2.
Numerical solutions for the heat equation

based on the explicit scheme.
the eﬀect of the numerical scheme on simple functions. As we know, the general solution
to the heat equation can be decomposed into a sum over the various Fourier modes. Thus,
we can concentrate on understanding what the numerical scheme does to an individual
complex exponential,† bearing in mind that we can then reconstruct its eﬀect on more
general initial data by taking suitable linear combinations of exponentials.
To this end, suppose that, at time t = tj, the solution is a sampled exponential
u(tj, x) = e i kx,
and so
uj,m = u(tj, xm) = e i kxm,
(5.21)
where k is a real parameter. Substituting the latter values into our numerical equations
(5.14), we ﬁnd that the updated value at time tj+1 is also a sampled exponential:
uj+1,m = μuj,m+1 + (1 −2μ)uj,m + μuj,m−1
= μe i kxm+1 + (1 −2μ)e i kxm + μe i kxm−1
= μe i k(xm+Δx) + (1 −2μ)e i kxm + μe i k(xm−Δx)
= λe i kxm,
(5.22)
where
λ = λ(k) = μe i kΔx + (1 −2μ) + μe−i kΔx
= 1 −2μ

1 −cos(kΔx)

= 1 −4μ sin2 1
2 kΔx

.
(5.23)
Thus, the eﬀect of a single step is to multiply the complex exponential (5.21) by the
magniﬁcation factor λ:
u(tj+1, x) = λe i kx.
(5.24)
†
As usual, complex exponentials are easier to work with than real trigonometric functions.

190
5 Finite Diﬀerences
In other words, e i kx plays the role of an eigenfunction, with the magniﬁcation factor λ(k)
the corresponding eigenvalue, of the linear operator governing each step of the numerical
scheme. Continuing in this fashion, we ﬁnd that the eﬀect of p further iterations of the
scheme is to multiply the exponential by the pth power of the magniﬁcation factor:
u(tj+p, x) = λp e i kx.
(5.25)
As a result, the stability is governed by the size of the magniﬁcation factor: If | λ | > 1,
then λp grows exponentially, and so the numerical solutions (5.25) become unbounded as
p →∞, which is clearly incompatible with the analytical behavior of solutions to the
heat equation. Therefore, an evident necessary condition for the stability of our numerical
scheme is that its magniﬁcation factor satisfy
| λ | ≤1.
(5.26)
This method of stability analysis was developed by the mid-twentieth-century Hun-
garian/American mathematician — and father of the electronic computer — John von
Neumann.
The stability criterion (5.26) eﬀectively distinguishes the stable, and hence
valid, numerical algorithms from the unstable, and hence ineﬀectual, schemes. For the
particular case (5.23), the von Neumann stability criterion (5.26) requires
−1 ≤1 −4μ sin2 1
2 kΔx

≤1,
or, equivalently,
0 ≤μ sin2 1
2 kΔx

≤1
2.
Since this is required to hold for all possible k, we must have
0 ≤μ = γ Δt
(Δx)2 ≤1
2 ,
and hence
Δt ≤(Δx)2
2γ
,
(5.27)
since γ > 0. Thus, once the space mesh size is ﬁxed, stability of the numerical scheme
places a restriction on the allowable time step size. For instance, if γ = 1, and the space
mesh size Δx = .01, then we must adopt a minuscule time step size Δt ≤.00005. It
would take an exorbitant number of time steps to compute the value of the solution at
even moderate times, e.g., t = 1. Moreover, the accumulation of round-oﬀerrors might
then cause a signiﬁcant reduction in the overall accuracy of the ﬁnal solution values. Since
not all choices of space and time steps lead to a convergent scheme, the explicit scheme
(5.14) is called conditionally stable.
Implicit and Crank–Nicolson Methods
An unconditionally stable method — one that does not restrict the time step — can be
constructed by replacing the forward diﬀerence formula (5.12) used to approximate the
time derivative by the backwards diﬀerence formula
∂u
∂t (tj, xm) ≈u(tj, xm) −u(tj−1, xm)
Δt
+ O

(Δt)2 
.
(5.28)
Substituting (5.28) and the same centered diﬀerence approximation (5.11) for uxx into the
heat equation, and then replacing j by j + 1, leads to the iterative system
−μuj+1,m+1 + (1 + 2μ)uj+1,m −μuj+1,m−1 = uj,m,
j = 0, 1, 2, . . . ,
m = 1, . . ., n −1,
(5.29)

5.2 Numerical Algorithms for the Heat Equation
191
t = .02
t = .04
t = .06
Figure 5.3.
Numerical solutions for the heat equation

based on the implicit scheme.
where the parameter μ = γ Δt/(Δx)2 is as before. The initial and boundary conditions
have the same form (5.16, 17). The latter system can be written in the matrix form
A u(j+1) = u(j) + b(j+1),
(5.30)
where A is obtained from the matrix A in (5.20) by replacing μ by −μ. This serves to
deﬁne an implicit scheme, since we have to solve a linear system of algebraic equations
at each step in order to compute the next iterate u(j+1). However, since the coeﬃcient
matrix A is tridiagonal, the solution can be computed extremely rapidly, [89], and so its
calculation is not an impediment to the practical implementation of this implicit scheme.
Example 5.5. Consider the same initial-boundary value problem considered in Ex-
ample 5.4.
In Figure 5.3, we plot the numerical solutions obtained using the implicit
scheme. The initial data is not displayed, but we graph the numerical solutions at times
t = .2, .4, .6 with a mesh size of Δx = .1. In the top row, we use a time step of Δt = .01,
while in the bottom row Δt = .005. In contrast to the explicit scheme, there is very little
diﬀerence between the two — indeed, both come much closer to the actual solution than
the explicit scheme. In fact, even signiﬁcantly larger time steps yield reasonable numerical
approximations to the solution.
Let us apply the von Neumann analysis to investigate the stability of the implicit
scheme. Again, we need only look at the eﬀect of the scheme on a complex exponential.
Substituting (5.21, 24) into (5.29) and canceling the common exponential factor leads to
the equation
λ (−μe i kΔx + 1 + 2μ −μe−i kΔx) = 1.

192
5 Finite Diﬀerences
t = .02
t = .04
t = .06
Figure 5.4.
Numerical Solutions for the heat equation

based on the Crank–Nicolson scheme.
We solve for the magniﬁcation factor
λ =
1
1 + 2μ

1 −cos(kΔx)
 =
1
1 + 4μ sin2 1
2 kΔx
 .
(5.31)
Since μ > 0, the magniﬁcation factor is always less than 1 in absolute value, and so the
stability criterion (5.26) is satisﬁed for any choice of step sizes. We conclude that the
implicit scheme (5.14) is unconditionally stable.
Another popular numerical scheme for solving the heat equation is the Crank–Nicolson
method, due to the British numerical analysts John Crank and Phyllis Nicolson:
uj+1,m −uj,m = 1
2 μ(uj+1,m+1 −2 uj+1,m + uj+1,m−1 + uj,m+1 −2 uj,m + uj,m−1), (5.32)
which can be obtained by averaging the explicit and implicit schemes (5.14) and (5.29).
We can write (5.32) in vectorial form
B u(j+1) = B u(j) + 1
2

b(j) + b(j+1)
,
where
B =
⎛
⎜
⎜
⎜
⎝
1 + μ
−1
2 μ
−1
2 μ
1 + μ
−1
2 μ
−1
2 μ
...
...
...
...
⎞
⎟
⎟
⎟
⎠,
B =
⎛
⎜
⎜
⎜
⎝
1 −μ
1
2 μ
1
2 μ
1 −μ
1
2 μ
1
2 μ
...
...
...
...
⎞
⎟
⎟
⎟
⎠,
(5.33)
are both tridiagonal.
Applying the von Neumann analysis as before, we deduce that the magniﬁcation factor
has the form
λ =
1 −2μ sin2 1
2 kΔx

1 + 2μ sin2 1
2 kΔx
 .
(5.34)

5.2 Numerical Algorithms for the Heat Equation
193
Since μ > 0, we see that | λ | ≤1 for all choices of step size, and so the Crank–Nicolson
scheme is also unconditionally stable. A detailed analysis based on a Taylor expansion of
the solution reveals that the errors are of order (Δt)2 and (Δx)2, and so it is reasonable to
choose the time step to have the same order of magnitude as the space step: Δt ≈Δx. This
gives the Crank–Nicolson scheme a signiﬁcant advantage over the previous two methods,
in that one can get away with far fewer time steps. However, applying it to the initial
value problem considered above reveals a subtle weakness. The top row in Figure 5.4 has
space and time step sizes Δt = Δx = .01, and does a reasonable job of approximating
the solution except near the corners, where an annoying and incorrect local oscillation
persists as the solution decays. The bottom row uses Δt = Δx = .001, and performs much
better, although a similar oscillatory error can be observed at much smaller times. Indeed,
unlike the implicit scheme, the Crank–Nicolson method fails to rapidly damp out the high-
frequency Fourier modes associated with small-scale features such as discontinuities and
corners in the initial data, although it performs quite well in smooth regimes. Thus, when
dealing with irregular initial data, a good strategy is to ﬁrst run the implicit scheme until
the small-scale noise is dissipated away, and then switch to Crank–Nicolson with a much
larger time step to determine the later large scale dynamics.
Finally, we remark that the ﬁnite diﬀerence schemes developed above for the heat
equation can all be readily adapted to more general parabolic partial diﬀerential equations.
The stability criteria and observed behaviors are fairly similar, and a couple of illustrative
examples can be found in the exercises.
Exercises
5.2.1. Suppose we seek to approximate the solution to the initial-boundary value problem
ut = 5uxx,
u(t, 0) = u(t, 3) = 0,
u(0, x) = x(x −1)(x −3),
0 ≤x ≤3,
by employing the explicit scheme (5.14). (a) Given the spatial mesh size Δx = .1, what
range of time steps Δt can be used to produce an accurate numerical approximation?
(b) Test your prediction by implementing the scheme using one value of Δx in the allowed
range and one value outside.
5.2.2. Solve the following initial-boundary value problem
ut = uxx,
u(t, 0) = u(t, 1) = 0,
u(0, x) = f(x),
0 ≤x ≤1,
with initial data
f(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
2
 x −1
6
 −1
3,
0 ≤x ≤1
3,
0,
1
3 ≤x ≤2
3,
1
2 −3
 x −5
6
 ,
2
3 ≤x ≤1,
using
(i) the explicit scheme (5.14); (ii) the implicit scheme (5.29); and (iii) the Crank–Nicolson
scheme (5.32). Use space step sizes Δx = .1 and .05, and suitably chosen time steps Δt.
Discuss which features of the solution can be observed in your numerical approximations.
5.2.3. Repeat Exercise 5.2.2 for the initial-boundary value problem ut = 3uxx, u(0, x) = 0,
u(t, −1) = 1, u(t, 1) = −1, using space step sizes Δx = .2 and .1.
5.2.4.(a) Solve the initial-boundary value problem
ut = uxx,
u(t, −1) = u(t, 1) = 0,
u(0, x) = | x |1/2 −x2,
−1 ≤x ≤1,
using (i) the explicit scheme (5.14); (ii) the implicit scheme (5.29); (iii) the Crank–Nicolson
scheme (5.32). Use Δx = .1 and an appropriate time step Δt. Compare your numerical so-
lutions at times t = 0, .01, , .02, .05, .1, .3, .5, 1.0, and discuss your ﬁndings.
(b) Repeat

194
5 Finite Diﬀerences
part (a) for the implicit and Crank-Nicolson schemes with Δx = .01. Why aren’t you being
asked to implement the explicit scheme?
5.2.5. Use the implicit scheme with spatial mesh sizes Δx = .1 and .05 and appropriately cho-
sen values of the time step Δt to investigate the solution to the periodically forced bound-
ary value problem ut = uxx, u(0, x) = 0, u(t, 0) = sin 5π t, u(t, 1) = cos 5π t. Is your
solution periodic in time?
♥5.2.6.(a) How would you modify (i) the explicit scheme; (ii) the implicit scheme; to deal with
Neumann boundary conditions? Hint: Use the one-sided ﬁnite diﬀerence formulae found in
Exercise 5.1.5 to approximate the derivatives at the boundary.
(b) Test your proposals on the boundary value problem
ut = uxx,
u(0, x) = 1
2 + cos 2πx −1
2 cos 3πx,
ux(t, 0) = 0 = ux(t, 1),
using space step sizes Δx = .1 and .01 and appropriate time steps. Compare your nu-
merical solution with the exact solution at times t = .01, .03, .05, and explain any dis-
crepancies.
5.2.7.(a) Design an explicit numerical scheme for approximating the solution to the initial-
boundary value problem
ut = γ uxx + s(x),
u(t, 0) = u(t, 1) = 0,
u(0, x) = f(x),
0 ≤x ≤1,
for the heat equation with a source term s(x). (b) Test your scheme when
γ = 1
6 ,
s(x) = x(1 −x)(10 −22x),
f(x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
2
 x −1
6
 −1
3,
0 ≤x ≤1
3,
0,
1
3 ≤x ≤2
3,
1
2 −3
 x −5
6
 ,
2
3 ≤x ≤1,
using space step sizes Δx = .1 and .05, and a suitably chosen time step Δt. Are your two
numerical solutions close? (c) What is the long-term behavior of the solution? Can you
ﬁnd a formula for its eventual proﬁle? (d) Design an implicit scheme for the same problem.
Does this aﬀect the behavior of your numerical solution? What are the advantages of the
implicit scheme?
5.2.8. Consider the initial-boundary value problem for the lossy diﬀusion equation
∂u
∂t = ∂2u
∂x2 −α u,
u(t, 0) = u(t, 1) = 0,
u(0, x) = f(x),
t ≥0,
0 ≤x ≤1,
where α > 0 is a positive constant. (a) Devise an explicit ﬁnite diﬀerence method for com-
puting a numerical appoximation to the solution.
(b) For what mesh sizes would you ex-
pect your method to provide a good approximation to the solution?
(c) Discuss the case
when α < 0.
5.2.9. Consider the initial-boundary value problem for the diﬀusive transport equation
∂u
∂t = ∂2u
∂x2 + 2 ∂u
∂x ,
u(t, 0) = u(t, 1) = 0,
u(0, x) = x(1 −x),
t ≥0,
0 ≤x ≤1.
(a) Devise an explicit ﬁnite diﬀerence scheme for computing numerical appoximations to
the solution. Hint: Make sure your approximations are of comparable order. (b) For what
range of time step sizes would you expect your method to provide a decent approximation
to the solution? (c) Test your answer in part (b) for the spatial step size Δx = .1.
♦5.2.10.(a) Show that using the centered diﬀerence approximation (5.6) to approximate the
time derivative leads to Richardson’s method for numerically solving the heat equation:
uj+1,m = uj−1,m + 2μ (uj,m+1 −2uj,m + uj,m−1),
j = 1, 2, . . . ,
m = 1, . . . , n −1,
where μ = γ Δt/(Δx)2 is as in (5.15). (b) Discuss how to start Richardson’s method.
(c) Discuss the stability of Richardson’s method. (d) Test Richardson’s method on the
initial-boundary value problem in Exercise 5.2.2. Does your numerical solution conform
with your expectations from part (b)?

195
5.3 Numerical Algorithms for
First–Order Partial Diﬀerential Equations
Let us next apply the method of ﬁnite diﬀerences to construct some basic numerical meth-
ods for ﬁrst-order partial diﬀerential equations. As noted in Section 4.4, ﬁrst-order partial
diﬀerential equations are prototypes for hyperbolic equations, and so many of the lessons
learned here carry over to the general hyperbolic regime, including the second-order wave
equation, which we analyze in detail in the following section.
Consider the initial value problem for the elementary transport equation
∂u
∂t + c ∂u
∂x = 0,
u(0, x) = f(x),
−∞< x < ∞,
(5.35)
with constant wave speed c. Of course, as we learned in Section 2.2, the solution is a simple
traveling wave
u(t, x) = f(x −ct)
(5.36)
that is constant along the characteristic lines of slope c in the (t, x)–plane. Although the
analytical solution is completely elementary, there will be valuable lessons to be learned
from our attempt to reproduce it by numerical approximation. Indeed, each of the nu-
merical schemes developed below has an evident adaptation to transport equations with
variable wave speeds c(t, x), and even to nonlinear transport equations whose wave speed
depends on the solution u, and so admit shock-wave solutions.
As before, we restrict our attention to a rectangular mesh (tj, xm) with uniform time
step size Δt = tj+1 −tj and space mesh size Δx = xm+1 −xm. We use uj,m ≈u(tj, xm)
to denote our numerical approximation to the solution u(t, x) at the indicated node. The
simplest numerical scheme is obtained by replacing the time and space derivatives by their
ﬁrst-order ﬁnite diﬀerence approximations (5.1):
∂u
∂t (tj, xm) ≈uj+1,m −uj,m
Δt
+ O(Δt),
∂u
∂x (tj, xm) ≈uj,m+1 −uj,m
Δx
+ O(Δx).
(5.37)
Substituting these expressions into the transport equation (5.35) leads to the explicit nu-
merical scheme
uj+1,m = −σ uj,m+1 + (σ + 1)uj,m,
(5.38)
in which the parameter
σ = c Δt
Δx
(5.39)
depends on the wave speed and the ratio of time to space step sizes. Since we are employ-
ing ﬁrst-order approximations to both derivatives, we should choose the step sizes to be
comparable: Δt ≈Δx. When working on a bounded interval, say 0 ≤x ≤ℓ, we will need
to specify a value for the numerical solution at the right end, e.g., setting uj,n = 0, which
corresponds to imposing the boundary condition u(t, ℓ) = 0.
In Figure 5.5, we plot the numerical solutions, at times t = .1, .2, .3, arising from the
following initial condition:
u(0, x) = f(x) = .4 e−300(x−.5)2 + .1 e−300(x−.65)2.
(5.40)
We use step sizes Δt = Δx = .005, and try four diﬀerent values of the wave speed. The
cases c = .5 and c = −1.5 clearly exhibit some form of numerical instability. The numerical
5.3 Numerical Algorithms for First Order Partial Diﬀerential Equations

196
5 Finite Diﬀerences
c = .5
c = −.5
c = −1
c = −1.5
Figure 5.5.
Numerical solutions to the transport equation.

solution when c = −.5 is a bit more reasonable, although one can already observe some
degradation due to the relatively low accuracy of the scheme. This can be alleviated by
employing a smaller step size. The case c = −1 looks exceptionally good, and you are
asked to provide an explanation in Exercise 5.3.6.
The CFL Condition
The are two ways to understand the observed numerical instability. First, we recall that
the exact solution (5.36) is constant along the characteristic lines x = ct + ξ, and hence
the value of u(t, x) depends only on the initial value f(ξ) at the point ξ = x −ct. On
the other hand, at time t = tj, the numerical solution uj,m ≈u(tj, xm) computed using
(5.38) depends on the values of uj−1,m and uj−1,m+1. The latter two values have been

197
t
x
Stable
t
x
Unstable
Figure 5.6.
The CFL condition.
computed from the previous approximations uj−2,m, uj−2,m+1, uj−2,m+2.
And so on.
Going all the way back to the initial time t0 = 0, we ﬁnd that uj,m depends on the initial
values u0,m = f(xm), . . . , u0,m+j = f(xm + j Δx) at the nodes lying in the interval
xm ≤x ≤xm + j Δx. On the other hand, the actual solution u(tj, xm) depends only on
the value of f(ξ), where
ξ = xm −ctj = xm −cj Δt.
Thus, if ξ lies outside the interval [xm, xm + j Δx], then varying the initial condition
near the point x = ξ will change the actual solution value u(tj, xm) without altering its
numerical approximation uj,m at all! So the numerical scheme cannot possibly provide an
accurate approximation to the solution value. As a result, we must require
xm ≤ξ = xm −cj Δt ≤xm + j Δx,
and hence
0 ≤−cΔt ≤Δx,
which we rewrite as
0 ≥σ = cΔt
Δx ≥−1,
or, equivalently,
−Δx
Δt ≤c ≤0.
(5.41)
This is the simplest manifestation of what is known as the Courant–Friedrichs–Lewy con-
dition, or CFL condition for short, which was established in the groundbreaking 1928
paper [33] by three of the pioneers in the development of numerical methods for partial
diﬀerential equations: the German (soon to be American) applied mathematicians Richard
Courant, Kurt Friedrichs, and Hans Lewy. Note that the CFL condition requires that the
wave speed be negative, and the time step size not too large. Thus, for allowable wave
speeds, the ﬁnite diﬀerence scheme (5.38) is conditionally stable.
The CFL condition can be recast in a more geometrically transparent manner as
follows. For the ﬁnite diﬀerence scheme (5.38), the numerical domain of dependence of a
point (tj, xm) is the triangle
T(tj,xm) =
-
(t, x)
 0 ≤t ≤tj, xm ≤x ≤xm + tj −t
.
.
(5.42)
The reason for this nomenclature is that, as we have just seen, the numerical approximation
to the solution at the node (tj, xm) depends on the computed values at the nodes lying
5.3 Numerical Algorithms for First Order Partial Diﬀerential Equations

198
5 Finite Diﬀerences
within its numerical domain of dependence; see Figure 5.6.
The CFL condition (5.41)
requires that, for all 0 ≤t ≤tj, the characteristic passing through the point (tj, xm) lie
entirely within the numerical domain of dependence (5.42). If the characteristic ventures
outside the domain, then the scheme will be numerically unstable. With this geometric
reformulation, the CFL criterion can be applied to both linear and nonlinear transport
equations that have nonuniform wave speeds.
The CFL criterion (5.41) is reconﬁrmed by a von Neumann stability analysis.
As
before, we test the numerical scheme on an exponential function. Substituting
uj,m = e i kxm,
uj+1,m = λe i kxm,
(5.43)
into (5.38) leads to
λe i kxm = −σ e i kxm+1 + (σ + 1)e i kxm =

−σ e i kΔx + σ + 1

e i kxm.
The resulting (complex) magniﬁcation factor
λ = 1 + σ

1 −e i kΔx
=

1 + σ −σ cos(kΔx)

−i σ sin(kΔx)
satisﬁes the stability criterion | λ | ≤1 if and only if
| λ |2 =

1 + σ −σ cos(kΔx)
2 +

σ sin(kΔx)
2
= 1 + 2σ(σ + 1)

1 −cos(kΔx)

= 1 + 4σ(σ + 1) sin2 1
2 kΔx

≤1
for all k. Thus, stability requires that σ(σ + 1) ≤0, and thus −1 ≤σ ≤0, in complete
accord with the CFL condition (5.41).
Upwind and Lax–WendroﬀSchemes
To obtain a ﬁnite diﬀerence scheme that can be used for positive wave speeds, we replace the
forward ﬁnite diﬀerence approximation to ∂u/∂x by the corresponding backwards diﬀerence
quotient, namely, (5.1) with h = −Δx, leading to the alternative ﬁrst-order numerical
scheme
uj+1,m = −(σ −1)uj,m + σ uj,m−1,
(5.44)
where σ = cΔt/Δx is as before.
A similar analysis, left to the reader, produces the
corresponding CFL stability criterion
0 ≤σ = cΔt
Δx ≤1,
and so this scheme can be applied for suitable positive wave speeds.
In this manner, we have produced one numerical scheme that works for negative wave
speeds, and an alternative scheme for positive speeds. The question arises — particularly
when one is dealing with equations with variable wave speeds — whether one can devise
a scheme that is (conditionally) stable for both positive and negative wave speeds. One
might be tempted to use the centered diﬀerence approximation (5.6):
∂u
∂x (tj, xm) ≈uj,m+1 −uj,m−1
Δx
+ O

(Δx)2 
.
(5.45)

199
t
x
Figure 5.7.
The CFL condition for the centered diﬀerence scheme.
Substituting (5.45) and the previous approximation to the time derivative (5.37) into (5.35)
leads to the numerical scheme
uj+1,m = −1
2 σ uj,m+1 + uj,m + 1
2 σ uj,m−1,
(5.46)
where, as usual, σ = c Δt/Δx. In this case, the numerical domain of dependence of the
node (tj, xm) consists of the nodes in the triangle
T (tj,xm) =
-
(t, x)
 0 ≤t ≤tj, xm −tj + t ≤x ≤xm + tj −t
.
.
(5.47)
The CFL condition requires that, for 0 ≤t ≤tj, the characteristic going through (tj, xm)
lie within this triangle, as in Figure 5.7, which imposes the condition
| σ | =

c Δt
Δx
 ≤1,
or, equivalently,
| c | ≤Δx
Δt .
(5.48)
Unfortunately, although it satisﬁes the CFL condition over this range of wave speeds, the
centered diﬀerence scheme is, in fact, always unstable! For instance, the instability of the
numerical solution to the preceding initial value problem (5.40) for c = 1 can be observed
in Figure 5.8. This is conﬁrmed by applying a von Neumann analysis: substitute (5.43)
into (5.46), and cancel the common exponential factors. Provided σ ̸= 0, which means that
c ̸= 0, the resulting magniﬁcation factor
λ = 1 −i σ sin(kΔx)
satisﬁes | λ | > 1 for all k with sin(kΔx) ̸= 0. Thus, for c ̸= 0, the centered diﬀerence
scheme (5.46) is unstable for all (nonzero) wave speeds!
5.3 Numerical Algorithms for First Order Partial Diﬀerential Equations

200
5 Finite Diﬀerences
t = .15
t = .3
t = .45
Figure 5.8.
Centered diﬀerence numerical solution to the transport equation.

One possible means of overcoming the sign restriction on the wave speed is to use
the forward diﬀerence scheme (5.38) when the wave speed is negative and the backwards
scheme (5.44) when it is positive. The resulting scheme, valid for varying wave speeds
c(t, x), takes the form
uj+1,m =
 −σj,m uj,m+1 + (σj,m + 1)uj,m,
cj,m ≤0,
−(σj,m −1)uj,m + σj,m uj,m−1,
cj,m > 0,
(5.49)
where
σj,m = cj,m
Δt
Δx ,
cj,m = c(tj, xm).
(5.50)
This is referred to as an upwind scheme, since the second node always lies “upwind” —
that is, away from the direction of motion — from the reference point (tj, xm).
The
upwind scheme works reasonably well over short time intervals, assuming that the space
step size is suﬃciently small and the time step satisﬁes the CFL condition Δx/Δt ≤| cj,m |
at each node, cf. (5.41). However, over longer time intervals, as we already observed in
Figure 5.5, the simple upwind scheme tends to produce a noticeable damping of waves or,
alternatively, require an unacceptably small step size. One way of overcoming this defect is
to use the popular Lax–Wendroﬀscheme, which is based on second-order approximations
to the derivatives. In the case of constant wave speed, the iterative step takes the form
uj+1,m = 1
2 σ(σ −1)uj,m+1 −(σ2 −1)uj,m + 1
2 σ(σ + 1)uj,m−1.
(5.51)
The stability analysis of the Lax–Wendroﬀscheme is relegated to the exercises. Extensions
to variable wave speeds are more subtle, and we refer the reader to [80] for a detailed
derivation.

5.4 Numerical Algorithms for the Wave Equation
201
Exercises
5.3.1. Solve the initial value problem ut = 3ux, u(0, x) = 1/(1 + x2), on the interval [−10, 10]
using an upwind scheme with space step size Δx = .1. Decide on an appropriate time step
size, and graph your solution at times t = .5, 1, 1.5. Discuss what you observe.
5.3.2. Solve Exercise 5.3.1 for the nonuniform transport equations
(a) ut + 4(1 + x2)−1 ux = 0,
(b) ut =

3 −2e−x2/4
ux,
(c) ut + 7x(1 + x2)−1 ux = 0,
(d) ut +

2 tan−1 1
2 x

ux = 0.
5.3.3. Consider the initial value problem
ut +
3x
x2 + 1 ux = 0,
u(0, x) =

1 −1
2 x2
e−x2/3.
On the interval [−5, 5], using space step size Δx = .1 and time step size Δt = .025, apply
(a) the forward scheme (5.38) (suitably modiﬁed for variable wave speed), (b) the back-
ward scheme (5.44) (suitably modiﬁed for variable wave speed), and (c) the upwind scheme
(5.49). Graph the resulting numerical solutions at times t = .5, 1, 1.5, and discuss what you
observe in each case. Which of the schemes are stable?
5.3.4. Use the centered diﬀerence scheme (5.46) to solve the initial value problem in Exercise
5.3.1. Do you observe any instabilities in your numerical solution?
5.3.5. Use the Lax–Wendroﬀscheme (5.51) to solve the initial value problem in Exercise 5.3.1.
Discuss the accuracy of your solution in comparison with the upwind scheme.
♦5.3.6. Can you explain why, in Figure 5.5, the numerical solution in the case c = −1 is signiﬁ-
cantly better than for c = −.5, or, indeed, for any other c in the stable range.
5.3.7. Nonlinear transport equations are often solved numerically by writing them in the form
of a conservation law, and then applying the ﬁnite diﬀerence formulas directly to the con-
served density and ﬂux. (a) Devise an upwind scheme for numerically solving our favorite
nonlinear transport equation, ut + 1
2 (u2)x = 0.
(b) Test your scheme on the initial value problem u(0, x) = e−x2.
5.3.8.(a) Design a stable numerical solution scheme for the damped transport equation
ut + 3
4 ux + u = 0.
(b) Test your scheme on the initial value problem with u(0, x) = e−x2.
♦5.3.9. Analyze the stability of the numerical scheme (5.44) by applying (a) the CFL condition;
(b) a von Neumann analysis. Are your conclusions the same?
♦5.3.10. For what choices of step size Δt, Δx is the Lax–Wendroﬀscheme (5.51) stable?
5.4 Numerical Algorithms for the Wave Equation
Let us now develop some basic numerical solution techniques for the second-order wave
equation.
As above, although we are in possession of the explicit d’Alembert solution
formula (2.82), the lessons learned in designing viable schemes here will carry over to more
complicated situations, including inhomogeneous media and higher-dimensional problems,
for which analytic solution formulas may no longer be readily available.

202
5 Finite Diﬀerences
Consider the wave equation
∂2u
∂t2 = c2 ∂2u
∂x2 ,
0 < x < ℓ,
t ≥0,
(5.52)
on a bounded interval of length ℓwith constant wave speed c > 0. For speciﬁcity, we
impose (possibly time-dependent) Dirichlet boundary conditions
u(t, 0) = α(t),
u(t, ℓ) = β(t),
t ≥0,
(5.53)
along with the usual initial conditions
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
0 ≤x ≤ℓ.
(5.54)
As usual, we adopt a uniformly spaced mesh
tj = j Δt,
xm = m Δx,
where
Δx = ℓ
n .
Discretization is implemented by replacing the second-order derivatives in the wave equa-
tion by their standard ﬁnite diﬀerence approximations (5.5):
∂2u
∂t2 (tj, xm) ≈u(tj+1, xm) −2u(tj, xm) + u(tj−1, xm)
(Δt)2
+ O

(Δt)2 
,
∂2u
∂x2 (tj, xm) ≈u(tj, xm+1) −2u(tj, xm) + u(tj, xm−1)
(Δx)2
+ O

(Δx)2 
.
(5.55)
Since the error terms are both of second order, we anticipate being able to choose the
space and time step sizes to have comparable magnitudes: Δt ≈Δx. Substituting the
ﬁnite diﬀerence formulas (5.55) into the partial diﬀerential equation (5.52) and rearranging
terms, we are led to the iterative system
uj+1,m = σ2 uj,m+1 + 2 (1 −σ2) uj,m + σ2 uj,m−1 −uj−1,m,
j = 1, 2, . . . ,
m = 1, . . ., n −1, (5.56)
for the numerical approximations uj,m ≈u(tj, xm) to the solution values at the nodes. The
parameter
σ = c Δt
Δx > 0
(5.57)
depends on the wave speed and the ratio of space and time step sizes.
The boundary
conditions (5.53) require that
uj,0 = αj = α(tj),
uj,n = βj = β(tj),
j = 0, 1, 2, . . . .
(5.58)
This allows us to rewrite the iterative system in vectorial form
u(j+1) = B u(j) −u(j−1) + b(j),
(5.59)
where
B =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
2 (1 −σ2)
σ2
σ2
2 (1 −σ2)
σ2
σ2
...
...
...
...
σ2
σ2
2 (1 −σ2)
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, u(j) =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
uj,1
uj,2
...
uj,n−2
uj,n−1
⎞
⎟
⎟
⎟
⎟
⎟
⎠
, b(j) =
⎛
⎜
⎜
⎜
⎜
⎝
σ2 αj
0
...
0
σ2 βj
⎞
⎟
⎟
⎟
⎟
⎠
.
(5.60)

5.4 Numerical Algorithms for the Wave Equation
203
The entries of u(j) ∈Rn−1 are, as in (5.18), the numerical approximations to the solution
values at the interior nodes. Note that (5.59) describes a second-order iterative scheme,
since computing the subsequent iterate u(j+1) requires knowing the values of the preceding
two: u(j) and u(j−1).
The one subtlety is how to get the method started. We know u(0), since its entries
u0,m = fm = f(xm) are determined by the initial position. However, we also need u(1)
in order to launch the iteration and compute u(2), u(3), . . . . Its entries u1,m ≈u(Δt, xm)
approximate the solution at time t1 = Δt, whereas the initial velocity ut(0, x) = g(x)
prescribes the derivatives ut(0, xm) = gm = g(xm) at the initial time t0 = 0. To resolve
this diﬃculty, a ﬁrst thought might be to use the ﬁnite diﬀerence approximation
gm = ∂u
∂t (0, xm) ≈u(Δt, xm) −u(0, xm)
Δt
≈u1,m −fm
Δt
(5.61)
to compute the required values u1,m = fm + gm Δt. However, the approximation (5.61) is
accurate only to order Δt, whereas the rest of the scheme has errors proportional to (Δt)2.
The eﬀect would be to introduce an unacceptably large error at the initial step, and the
resulting solution would fail to conform to the desired order of accuracy.
To construct an initial approximation to u(1) with error on the order of (Δt)2, we need
to analyze the error in the approximation (5.61) in more depth. Note that, by Taylor’s
Theorem,
u(Δt, xm) −u(0, xm)
Δt
= ∂u
∂t (0, xm) + 1
2
∂2u
∂t2 (0, xm)Δt + O

(Δt)2 
= ∂u
∂t (0, xm) + c2
2
∂2u
∂x2 (0, xm) Δt + O

(Δt)2 
,
since u(t, x) solves the wave equation. Therefore,
u1,m = u(Δt, xm) ≈u(0, xm) + ∂u
∂t (0, xm)Δt + c2
2
∂2u
∂x2 (0, xm)(Δt)2
= f(xm) + g(xm) Δt + c2
2 f ′′(xm)(Δt)2
≈fm + gm Δt + c2(fm+1 −2fm + fm−1)(Δt)2
2(Δx)2
,
where the last line, which employs the ﬁnite diﬀerence approximation (5.5) to the sec-
ond derivative, can be used if the explicit formula for f ′′(x) is either not known or too
complicated to evaluate directly. Therefore, we initiate the scheme by setting
u1,m = 1
2 σ2fm+1 + (1 −σ2)fm + 1
2 σ2fm−1 + gm Δt,
(5.62)
or, in vectorial form,
u(0) = f,
u(1) = 1
2 B u(0) + g Δt + 1
2 b(0),
(5.63)
where f =

f1, f2, . . . , fn−1
T , g =

g1, g2, . . . , gn−1
T, are the sampled values of the
initial data. This serves to maintain the desired second-order accuracy of the scheme.
Example 5.6. Consider the particular initial value problem
utt = uxx,
u(0, x) = e−400 (x−.3)2,
ut(0, x) = 0,
u(t, 0) = u(t, 1) = 0,
0 ≤x ≤1,
t ≥0,

204
5 Finite Diﬀerences
t = 0
t = .1
t = .2
t = .3
t = .4
t = .5
Figure 5.9.
Numerically stable waves.

t = 0
t = .04
t = .08
t = .12
t = .16
t = .2
Figure 5.10.
Numerically unstable waves.

subject to homogeneous Dirichlet boundary conditions on the interval [0, 1]. The initial
data is a fairly concentrated hump centered at x = .3. As time progresses, we expect the
initial hump to split into two half-sized humps, which then collide with the ends of the
interval, reversing direction and orientation.
For our numerical approximation, let us use a space discretization consisting of 90

5.4 Numerical Algorithms for the Wave Equation
205
t
x
Stable
t
x
Unstable
Figure 5.11.
The CFL condition for the wave equation.
equally spaced points, and so Δx =
1
90 = .0111 . . . . If we choose a time step of Δt = .01,
whereby σ = .9, then we obtain a reasonably accurate solution over a fairly long time
range, as plotted in Figure 5.9. On the other hand, if we double the time step, setting
Δt = .02, so σ = 1.8, then, as shown in Figure 5.10, we induce an instability that eventually
overwhelms the numerical solution. Thus, the preceding numerical scheme appears to be
only conditionally stable.
Stability analysis proceeds along the same lines as in the ﬁrst-order case. The CFL
condition requires that the characteristics emanating from a node (tj, xm) remain, for times
0 ≤t ≤tj, in its numerical domain of dependence, which, for our particular numerical
scheme, is the same triangle
T (tj,xm) =
-
(t, x)
 0 ≤t ≤tj, xm −tj + t ≤x ≤xm + tj −t
.
,
now plotted in Figure 5.11. Since the characteristics are the lines of slope ±c, the CFL
condition is the same as in (5.48):
σ = c Δt
Δx ≤1,
or, equivalently,
0 < c ≤Δx
Δt .
(5.64)
The resulting stability criterion explains the observed diﬀerence between the numerically
stable and unstable cases.
However, as we noted above, the CFL condition is, in general, only necessary for stabil-
ity of the numerical scheme; suﬃciency requires that we perform a von Neumann stability
analysis. To this end, we specialize the calculation to a single complex exponential e i kx.
After one time step, the scheme will have the eﬀect of multiplying it by the magniﬁcation
factor λ = λ(k), after another time step by λ2, and so on. To determine λ, we substitute
the relevant sampled exponential values
uj−1,m = e i kxm,
uj,m = λ e i kxm,
uj+1,m = λ2 e i kxm,
(5.65)

206
5 Finite Diﬀerences
into the scheme (5.56). After canceling the common exponential, we ﬁnd that the magni-
ﬁcation factor satisﬁes the following quadratic equation:
λ2 =

2 −4σ2 sin2 1
2 kΔx
 
λ −1,
whence
λ = α ±

α2 −1 ,
where
α = 1 −2σ2 sin2 1
2 kΔx

.
(5.66)
Thus, there are two diﬀerent magniﬁcation factors associated with each complex expo-
nential — which is, in fact, a consequence of the scheme being of second order. Stability
requires that both be ≤1 in modulus.
Now, if the CFL condition (5.64) holds, then
| α | ≤1, which implies that both magniﬁcation factors (5.66) are complex numbers of
modulus | λ | = 1, and thus the numerical scheme satisﬁes the stability criterion (5.26).
On the other hand, if σ > 1, then α < −1 over a range of values of k, which implies that
the two magniﬁcation factors (5.66) are both real and one of them is < −1, thus violating
the stability criterion. Consequently, the CFL condition (5.64) does indeed distinguish
between the stable and unstable ﬁnite diﬀerence schemes for the wave equation.
Exercises
5.4.1. Suppose you are asked to numerically approximate the solution to the initial-boundary
value problem
utt = 64uxx,
u(t, 0) = u(t, 3) = 0,
u(0, x) =
 1 −2| x −1 |,
1
2 ≤x ≤3
2 ,
0,
otherwise,
ut(0, x) = 0,
on the interval 0 ≤x ≤3, using (5.56) with space step size Δx = .1. (a) What range of
time steps Δt are allowed?
(b) Test your answer by implementing the numerical solution
for one value of Δt in the allowable range and one value outside. Discuss what you observe
in your numerical solutions.
(c) In the stable range, compare your numerical solution with
that obtained using the smaller step size Δx = .01 and a suitable time step Δt.
5.4.2. Solve Exercise 5.4.1 for the boundary value problem
utt = 64uxx,
u(t, 0) = 0 = u(t, 3),
u(0, x) = 0,
ut(0, x) =

1 −2| x −1 |,
1
2 ≤x ≤3
2 ,
0,
otherwise.
5.4.3. Solve the following initial-boundary value problem
utt = 9uxx,
u(t, 0) = u(t, 1) = 0,
u(0, x) = 1
2 +
 x −1
4
 −
 2x −3
4
 ,
ut(0, x) = 0,
on the interval 0 ≤x ≤1, using the numerical scheme (5.56) with space step sizes Δx =
.1, .01 and .001 and suitably chosen time steps. Discuss which features of the solution can
be observed in your numerical approximations.
5.4.4.(a) Use a numerical integrator with space step size Δx = .05 to solve the periodically
forced boundary value problem
utt = uxx,
u(0, x) = ut(0, x) = 0,
u(t, 0) = sin t,
u(t, 1) = 0.
Is your solution periodic?
(b) Repeat the computation using the alternative boundary
condition u(t, 0) = sin π t. Discuss any observed diﬀerences between the two problems.
5.4.5.(a) Design an explicit numerical scheme for solving the initial-boundary value problem
utt = c2uxx + F(t, x),
u(t, 0) = u(t, 1) = 0,
u(0, x) = f(x),
ut(0, x) = g(x),
0 ≤x ≤1,
for the wave equation with an external forcing term F(t, x). Clearly state any stability
conditions that need to be imposed on the time and space step sizes.

5.5 Finite Diﬀerence Algorithms for the Laplace and Poisson Equations
207
(b) Test your scheme on the particular case c = 1
4 , F(t, x) = 3 sign

x −1
2

sin π t, f(x) ≡
g(x) ≡0, using space step sizes Δx = .05 and .01, and suitably chosen time steps.
5.4.6. Let β > 0. (a) Design a ﬁnite diﬀerence scheme for approximating the solution to the
initial-boundary value problem
utt + β ut = c2uxx,
u(t, 0) = u(t, 1) = 0,
u(0, x) = f(x),
ut(0, x) = g(x),
for the damped wave equation on the interval 0 ≤x ≤1. (b) Discuss the stability of your
scheme. What choice of step sizes will ensure stability? (c) Test your scheme with c = 1,
β = 1, using the initial data f(x) = e−(x−.7)2
, g(x) = 0.
5.5 Finite Diﬀerence Algorithms for
the Laplace and Poisson Equations
Finally, let us discuss the implementation of ﬁnite diﬀference numerical schemes for elliptic
boundary value problems.
We concentrate on the simplest cases: the two-dimensional
Laplace and Poisson equations. The basic issues are already apparent in this particular
context, and extensions to more general equations, higher dimensions, and higher-order
schemes are all reasonably straightforward. In Chapter 10, we will present a competitor
— the renowned ﬁnite element method — which, while relying on more sophisticated
mathematical machinery, enjoys several advantages, including more immediate adaptability
to variable mesh sizes and more sophisticated geometries.
For speciﬁcity, we concentrate on the Dirichlet boundary value problem
−Δu = −uxx −uyy = f(x, y),
u(x, y) = g(x, y),
for
(x, y) ∈Ω,
(x, y) ∈∂Ω,
(5.67)
on a bounded planar domain Ω ⊂R2. The ﬁrst step is to discretize the domain Ω by
constructing a rectangular mesh. Thus, the ﬁnite diﬀerence method is particularly suited
to domains whose boundary lines up with the coordinate axes; otherwise, the mesh nodes
do not, generally, lie exactly on ∂Ω, making the approximation of the boundary data more
challenging — although not insurmountable.
For simplicity, let us study the case in which
Ω = {a < x < b, c < y < d}
is a rectangle.
We introduce a regular rectanglar mesh, with x and y spacings given,
respectively, by
Δx = b −a
m
,
Δy = c −d
n
,
for positive integers m, n. Thus, the interior of the rectangle contains (m−1)(n−1) interior
nodes
(xi, yj) = (a + i Δx, c + j Δy)
for
0 < i < m,
0 < j < n.
In addition, the 2m + 2n boundary nodes (x0, yj) = (a, yj), (xm, yj) = (b, yj), (xi, y0) =
(xi, c), (xi, yn) = (xi, d), lie on the boundary of the rectangle.

208
5 Finite Diﬀerences
At each interior node, we employ the centered diﬀerence formula (5.5) to approximate
the relevant second-order derivatives:
∂2u
∂x2 (xi, yj) = u(xi+1, yj) −2u(xi, yj) + u(xi−1, yj)
(Δx)2
+ O

(Δx)2 
,
∂2u
∂y2 (xi, yj) = u(xi, yj+1) −2u(xi, yj) + u(xi, yj−1)
(Δy)2
+ O

(Δy)2 
.
(5.68)
Substituting these ﬁnite diﬀerence formulae into the Poisson equation produces the linear
system
−ui+1,j −2ui,j + ui−1,j
(Δx)2
−ui,j+1 −2ui,j + ui,j−1
(Δy)2
= fi,j,
i = 1, . . . , m −1,
j = 1, . . ., n −1,
(5.69)
in which ui,j denotes our numerical approximation to the solution values u(xi, yj) at the
nodes, while fi,j = f(xi, yj). If we set
ρ = Δx
Δy ,
(5.70)
then (5.69) can be rewritten in the form
2(1 + ρ2)ui,j −(ui−1,j + ui+1,j) −ρ2(ui,j−1 + ui,j+1) = (Δx)2fi,j,
i = 1, . . . , m −1,
j = 1, . . . , n −1.
(5.71)
Since both ﬁnite diﬀerence approximations (5.68) are of second order, one should choose
Δx and Δy to be of comparable size, thus keeping ρ around 1.
The linear system (5.71) forms the ﬁnite diﬀerence approximation to the Poisson
equation at the interior nodes. It is supplemented by the discretized Dirichlet boundary
conditions
ui,0 = gi,0,
ui,n = gi,n,
i = 0, . . . , m,
u0,j = g0,j,
um,j = gm,j,
j = 0, . . . , n.
(5.72)
These boundary values can be substituted directly into the system, making (5.71) a system
of (m−1)(n−1) linear equations involving the (m−1)(n−1) unknowns ui,j for 1 ≤i ≤m−1,
1 ≤j ≤n −1. We impose some convenient ordering for these entries, e.g., from left to
right and then bottom to top, forming the column vector of unknowns
w = (w1, w2, . . . , w(m−1)(n−1))T
= (u1,1, u2,1, . . . , um−1,1, u1,2, u2,2, . . . , um−1,2, u1,3, . . . , um−1,n−1)T .
(5.73)
The combined linear system (5.71–72) can then be rewritten in matrix form
Aw = f,
(5.74)
where the right-hand side is obtained by combining the column vector f = ( . . . fi,j . . . )T
with the boundary data provided by (5.72) according to where they appear in the system.
The implementation will become clearer once we work through a small-scale example.
Example 5.7. To better understand how the process works, let us look at the case
in which Ω = {0 < x < 1, 0 < y < 1} is the unit square. In order to write everything in

5.5
209
Figure 5.12.
Square mesh with Δx = Δy = 1
4.
full detail, we start with a very coarse mesh with Δx = Δy = 1
4; see Figure 5.12. Thus
m = n = 4, resulting in a total of nine interior nodes. In this case, ρ = 1, and hence the
ﬁnite diﬀerence system (5.71) consists of the following nine equations:
−u1,0 −u0,1 + 4u1,1 −u2,1 −u1,2 =
1
16 f1,1,
−u2,0 −u1,1 + 4u2,1 −u3,1 −u2,2 =
1
16 f2,1,
−u3,0 −u2,1 + 4u3,1 −u4,1 −u3,2 =
1
16 f3,1,
−u1,1 −u0,2 + 4u1,2 −u2,2 −u1,3 =
1
16 f1,2,
−u2,1 −u1,2 + 4u2,2 −u3,2 −u2,3 =
1
16 f2,2,
−u3,1 −u2,2 + 4u3,2 −u4,2 −u3,3 =
1
16 f3,2,
−u1,2 −u0,3 + 4u1,3 −u2,3 −u1,4 =
1
16 f1,3,
−u2,2 −u1,3 + 4u2,3 −u3,3 −u2,4 =
1
16 f2,3,
−u3,2 −u2,3 + 4u3,3 −u4,3 −u3,4 =
1
16 f3,3.
(5.75)
(Note that the values at the four corner nodes, u0,0, u4,0, u0,4, u4,4, do not appear.) The
boundary data imposes the additional conditions (5.72), namely
u0,1 = g0,1,
u0,2 = g0,2,
u0,3 = g0,3,
u1,0 = g1,0,
u2,0 = g2,0,
u3,0 = g3,0,
u4,1 = g4,1,
u4,2 = g4,2,
u4,3 = g4,3,
u1,4 = g1,4,
u2,4 = g2,4,
u3,4 = g3,4.
The system (5.75) can be written in matrix form Aw = f, where
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
4
−1
0
−1
0
0
0
0
0
−1
4
−1
0
−1
0
0
0
0
0
−1
4
0
0
−1
0
0
0
−1
0
0
4
−1
0
−1
0
0
0
−1
0
−1
4
−1
0
−1
0
0
0
−1
0
−1
4
0
0
−1
0
0
0
−1
0
0
4
−1
0
0
0
0
0
−1
0
−1
4
−1
0
0
0
0
0
−1
0
−1
4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(5.76)
Finite Diﬀerence Algorithms for the Laplace and Poisson Equations

210
5 Finite Diﬀerences
and
w =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
w1
w2
w3
w4
w5
w6
w7
w8
w9
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
u1,1
u2,1
u3,1
u1,2
u2,2
u3,2
u1,3
u2,3
u3,3
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
f =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
16 f1,1 + g1,0 + g0,1
1
16 f2,1 + g2,0
1
16 f3,1 + g3,0 + g4,1
1
16 f1,2 + g0,2
1
16 f2,2
1
16 f3,2 + g4,2
1
16 f1,3 + g0,3 + g1,4
1
16 f2,3 + g2,4
1
16 f3,3 + g4,3 + g3,4
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Note that the known boundary values, namely ui,j = gi,j when i or j equals 0 or 4, have
been incorporated into the right-hand side f of the ﬁnite diﬀerence linear system (5.74).
The resulting linear system is easily solved by Gaussian Elimination, [89]. Finer meshes
lead to correspondingly larger linear systems, all endowed with a common overall structure,
as discussed below.
For example, the function
u(x, y) = y sin(πx)
solves the particular boundary value problem
−Δu = π2y sin(πx),
u(x, 0) = u(0, y) = u(1, y) = 0,
u(x, 1) = sin(πx),
0 < x, y < 1.
Setting up and solving the linear system (5.75) produces the ﬁnite diﬀerence solution values
u1,1 = .1831,
u1,2 = .2589,
u1,3 = .1831,
u2,1 = .3643,
u2,2 = .5152,
u2,3 = .3643,
u3,1 = .5409,
u3,2 = .7649,
u3,3 = .5409,
leading to the numerical approximation plotted in the ﬁrst graph† of Figure 5.13. The
maximal error between the numerical and exact solution values is .01520, which occurs at
the center of the square. In the second and third graphs, the mesh spacing is successively
reduced by half, so there are, respectively, m = n = 8 and 16 nodes in each coordinate
direction.
The corresponding maximal numerical errors at the nodes are .004123 and
.001035. Observe that halving the step size reduces the error by a factor of 1
4, which is
consistent with the numerical scheme being of second order.
Remark: The preceding test is a particular instance of the method of manufactured
solutions, in which one starts with a preselected function that almost certainly is not
a solution to the exact problem at hand.
Nevertheless, substituting this function into
the diﬀerential equation and the relevant initial and/or boundary conditions leads to an
inhomogeneous problem of the same character as the original. After running the numerical
scheme on the modiﬁed problem, one can test for accuracy by comparing the numerical
output with the preselected function.
†
We are using ﬂat triangles to interpolate the nodal data. Smoother interpolation schemes,
e.g., splines, [102], will produce a more realistic reproduction of the analytic solution graph.

5.5
211
Δx = Δy = .25
Δx = Δy = .125
Δx = Δy = .0625
Figure 5.13.
Finite diﬀerence solutions to a Poisson boundary value problem.
Solution Strategies
The linear algebraic system resulting from a ﬁnite diﬀerence discretization can be rather
large, and it behooves us to devise eﬃcient solution strategies. The general ﬁnite diﬀerence
coeﬃcient matrix A has a very structured form, which can already be inferred from the
very simple case (5.76). When the underlying domain is a rectangle, it assumes a block
tridiagonal form
A =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Bρ
−ρ2 I
−ρ2 I
Bρ
−ρ2 I
−ρ2 I
Bρ
−ρ2 I
...
...
...
−ρ2 I
Bρ
−ρ2 I
−ρ2 I
Bρ
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(5.77)
where I is the (m −1) × (m −1) identity matrix, while
Bρ =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2(1 + ρ2)
−ρ2
−ρ2
2(1 + ρ2)
−ρ2
−ρ2
2(1 + ρ2)
−ρ2
−ρ2
2(1 + ρ2)
−ρ2
...
...
...
−ρ2
2(1 + ρ2)
−ρ2
−ρ2
2(1 + ρ2)
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
(5.78)
is itself an (m −1) × (m −1) tridiagonal matrix. (Here and below, all entries not explicitly
indicated are zero.) There are n −1 blocks in both the row and column directions.
When the ﬁnite diﬀerence linear system is of moderate size, it can be eﬃciently solved
by Gaussian Elimination, which eﬀectively factorizes A = LU into a product of lower
and upper triangular matrices. (This follows since A is symmetric and nonsingular, as
guaranteed by Theorem 5.8 below.) In the present case, the factors are block bidiagonal
Finite Diﬀerence Algorithms for the Laplace and Poisson Equations

212
5 Finite Diﬀerences
matrices:
L =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
I
L1
I
L2
I
...
...
Ln−3
I
Ln−2
I
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
U =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
U1
−ρ2 I
U2
−ρ2 I
U3
−ρ2 I
...
...
Un−2
−ρ2 I
Un−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(5.79)
where the individual blocks are again of size (m−1)×(m−1). Indeed, multiplying out the
matrix product LU and equating the result to (5.77) leads to the iterative matrix system
U1 = Bρ,
Lj = −ρ2 U −1
j
,
Uj+1 = Bρ + ρ2Lj,
j = 1, . . . , n −2,
(5.80)
which produces the individual blocks.
With the LU factors in place, we can apply Forward and Back Substitution to solve
the block tridiagonal linear system Aw = f by solving the block lower and upper triangular
systems
L z = f,
U w = z.
(5.81)
In view of the forms (5.79) of L and U, if we write
w =
⎛
⎜
⎜
⎝
w(1)
w(2)
...
w(n−1)
⎞
⎟
⎟
⎠,
z =
⎛
⎜
⎜
⎝
z(1)
z(2)
...
z(n−1)
⎞
⎟
⎟
⎠,
f =
⎛
⎜
⎜
⎝
f (1)
f (2)
...
f (n−1)
⎞
⎟
⎟
⎠,
so that each w(j), z(j),f (j), is a vector with m −1 entries, then we must successively solve
z(1) = f (1),
z(j+1) = f (j+1) −Ljz(j),
j = 1, 2, . . ., n −2,
w(n−1) = z(n−1),
Ujw(k) = z(k) −ρ2 w(k+1),
k = n −2, n −3, . . . , 1,
(5.82)
in the prescribed order. In view of the identiﬁcation of Lj with −ρ2 times the inverse of
Uj, the last set of equations in (5.82) is perhaps better written as
w(k) = Lj

w(k+1) −ρ−2 z(k)
,
k = n −2, n −3, . . . , 1.
(5.83)
As the number of nodes becomes large, the preceding elimination/factorization ap-
proach to solving the linear system becomes increasingly ineﬃcient, and one often switches
to an iterative solution method such as Gauss–Seidel, Jacobi, or, even better, Successive
Over–Relaxation (SOR); indeed, SOR was originally designed to speed up the solution of
the large-scale linear systems arising from the numerical solution of elliptic partial diﬀer-
ential equations. Detailed discussions of iterative matrix methods can be found in [89;

5.5
213
Chapter 10] and [118]. For the SOR method, a good choice for the relaxation parameter
is
ω =
4
2 +
/
4 −cos2(π/m) −cos2(π/n)
.
(5.84)
Iterative solution methods are even more attractive in dealing with irregular domains,
whose ﬁnite diﬀerence coeﬃcient matrix, while still sparse, is less structured than in the
rectangular case, and hence less amenable to fast Gaussian Elimination algorithms.
Finally, let us address the question of unique solvability of the ﬁnite diﬀerence linear
system obtained by discretization of the Poisson equation on a bounded domain subject to
Dirichlet boundary conditions. As in the Uniqueness Theorem 4.10 for the original bound-
ary value, this will follow from an easily established Maximum Principle for the discrete
system that directly mimics the Laplace equation maximum principle of Theorem 4.9.
Theorem 5.8. Let Ω be a bounded domain. Then the ﬁnite diﬀerence linear system
(5.74) has a unique solution.
Proof : The result will follow if we can prove that the only solution to the corresponding
homogeneous linear system Aw = 0 is the trivial solution w = 0.
The homogeneous
system corresponds to discretizing the Laplace equation subject to zero Dirichlet boundary
conditions.
Now, in view of (5.71), each equation in the homogeneous linear system can be written
in the form
ui,j =
ui−1,j + ui+1,j + ρ2ui,j−1 + ρ2ui,j+1
2(1 + ρ2)
.
(5.85)
If ρ = 1, then (5.85) says that the value of ui,j at the node (xi, yj) is equal to the average
of the values at the four neighboring nodes. For general ρ, it says that ui,j is a weighted
average of the four neighboring values. In either case, the value of ui,j must lie strictly
between the maximum and minimum values of ui−1,j, ui+1,j, ui,j−1 and ui,j+1 — unless
all these values are the same, in which case ui,j also has the same value. This observation
suﬃces to establish a Maximum Principle for the ﬁnite diﬀerence system for the Laplace
equation — namely, that its solution cannot achieve a local maximum or minimum at an
interior node.
Now suppose that the homogeneous ﬁnite diﬀerence system Aw = 0 for the domain
has a nontrivial solution w ̸= 0. Let ui,j = wk be the maximal entry of this purported
solution. The Maximum Principle requires that all four of its neighboring values must have
the same maximal value. But then the same argument applies to the neighbors of those
entries, to their neighbors, and so on. Eventually one of the neighbors is at a boundary
node, but, since we are dealing with the homogeneous Dirichlet boundary value problem,
its value is zero. This immediately implies that all the entries of w must be zero, which is
a contradiction.
Q.E.D.
Rigorously establishing convergence of the ﬁnite diﬀerence solution to the analytic
solution to the boundary value problem as the step size goes to zero will not be discussed
here, and we refer the reader to [6, 80] for precise results and proofs.
Finite Diﬀerence Algorithms for the Laplace and Poisson Equations

214
5 Finite Diﬀerences
Exercises
♠5.5.1. Solve the Dirichlet problem Δu = 0, u(x, 0) = sin3 x, u(x, π) = 0, u(0, y) = 0,
u(π, y) = 0, numerically using a ﬁnite diﬀerence scheme. Compare your approximation with
the solution you obtained in Exercise 4.3.10(a).
♠5.5.2. Solve the Dirichlet problem Δu = 0, u(x, 0) = x, u(x, 1) = 1 −x, u(0, y) = y, u(1, y) =
1 −y, numerically via ﬁnite diﬀerences. Compare your approximation with the solution you
obtained in Exercise 4.3.12(d).
♠5.5.3. Consider the Dirichlet boundary value problem Δu = 0
u(x, 0) = sin x, u(x, π) = 0,
u(0, y) = 0, u(π, y) = 0, on the square {0 < x, y < π }. (a) Find the exact solution. (b) Set
up and solve the ﬁnite diﬀerence equations based on a square mesh with m = n = 2 squares
on each side of the full square. How close is this value to the exact solution at the center of
the square: u
 1
2 π, 1
2 π

? (c) Repeat part (b) for m = n = 4 squares per side. Is the value
of your approximation at the center of the unit square closer to the true solution? (d) Use
a computer to ﬁnd a ﬁnite diﬀerence approximation to u
 1
2 π, 1
2 π

using m = n = 8 and
16 squares per side. Is your approximation converging to the exact solution as the mesh
becomes ﬁner and ﬁner? Is the convergence rate consistent with the order of the ﬁnite dif-
ference approximation?
♠5.5.4.(a) Use ﬁnite diﬀerences to approximate a solution to the Helmholtz boundary value
problem Δu = u, u(x, 0) = u(x, 1) = u(0, y) = 0, u(1, y) = 1, on the unit square
0 < x, y < 1. (b) Use separation of variables to construct a series solution. Do your ana-
lytic and numerical solutions match? Explain any discrepancies.
♠5.5.5. A drum is in the shape of an L, as in the accompanying ﬁgure, whose
short sides all have length 1. (a) Use a ﬁnite diﬀerence scheme with mesh
spacing Δx = Δy = .1 to ﬁnd and graph the equilibrium conﬁguration
when the drum is subject to a unit upwards force while all its sides are
ﬁxed to the (x, y)–plane. What is the maximal deﬂection, and at which
point(s) does it occur? (b) Check the accuracy of your answer in part (a)
by reducing the step size by half: Δx = Δy = .05.
♣5.5.6. A metal plate has the shape of a 3 cm square with a 1 cm square hole cut out of the
middle. The plate is heated by making the inner edge have temperature 100◦while keep-
ing the outer edge at 0◦. (a) Find the (approximate) equilibrium temperature using ﬁnite
diﬀerences with a mesh width of Δx = Δy = .5 cm. Plot your approximate solution us-
ing a three-dimensional graphics program. (b) Let C denote the square contour lying mid-
way between the inner and outer square boundaries of the plate. Using your ﬁnite diﬀer-
ence approximation, determine at what point(s) on C the temperature is (i) minimized;
(ii) maximimized; (iii) equal to the average of the two boundary temperatures.
(c) Repeat part (a) using a smaller mesh width of Δx = Δy = .2. How much does this
aﬀect your answers in part (b)?
♣5.5.7. Answer Exercise 5.5.6 when the plate is additionally subjected to a constant heat source
f(x, y) = 600x + 800y −2400.
♠5.5.8.(a) Explain how to adapt the ﬁnite diﬀerence method to a mixed boundary value prob-
lem on a rectangle with inhomogeneous Neumann conditions. Hint: Use a one-sided diﬀer-
ence formula of the appropriate order to approximate the normal derivative at the bound-
ary.
(b) Apply your method to the problem
Δu = 0,
u(x, 0) = 0,
u(x, 1) = 0,
∂u
∂x (0, y) = y(1 −y),
u(1, y) = 0,
using mesh sizes Δx = Δy = .1, .01, and .001. Compare your answers.
(c) Solve the
boundary value problem via separation of variables, and compare the value of the solution
and the numerical approximations at the center of the square.

Chapter 6
Generalized Functions and Green’s Functions
Boundary value problems, involving both ordinary and partial diﬀerential equations, can
be proﬁtably viewed as the inﬁnite-dimensional function space versions of ﬁnite-dimen-
sional systems of linear algebraic equations. As a result, linear algebra not only provides
us with important insights into their underlying mathematical structure, but also motivates
both analytical and numerical solution techniques. In the present chapter, we develop the
method of Green’s functions, pioneered by the early-nineteenth-century self-taught English
mathematician (and miller!) George Green, whose famous Theorem you already encoun-
tered in multivariable calculus. We begin with the simpler case of ordinary diﬀerential
equations, and then move on to solving the two-dimensional Poisson equation, where the
Green’s function provides a powerful alternative to the method of separation of variables.
For inhomogeneous linear systems, the basic Superposition Principle says that the
response to a combination of external forces is the self-same combination of responses to the
individual forces. In a ﬁnite-dimensional system, any forcing function can be decomposed
into a linear combination of unit impulse forces, each applied to a single component of the
system, and so the full solution can be obtained by combining the solutions to the individual
impulse problems. This simple idea will be adapted to boundary value problems governed
by diﬀerential equations, where the response of the system to a concentrated impulse
force is known as the Green’s function. With the Green’s function in hand, the solution
to the inhomogeneous system with a general forcing function can be reconstructed by
superimposing the eﬀects of suitably scaled impulses. Understanding this construction will
become increasingly important as we progress to partial diﬀerential equations, where direct
analytic solution techniques are far harder to come by.
The obstruction blocking a direct implementation of this idea is that there is no
ordinary function that represents an idealized concentrated impulse! Indeed, while this
approach was pioneered by Green and Cauchy in the early 1800s, and then developed
into an eﬀective computational tool by Heaviside in the 1880s, it took another 60 years
before mathematicians were able to develop a completely rigorous theory of generalized
functions, also known as distributions. In the language of generalized functions, a unit
impulse is represented by a delta function.† While we do not have the analytic tools to
completely develop the mathematical theory of generalized functions in its full, rigorous
glory, we will spend the ﬁrst section learning the basic concepts and developing the practical
computational skills, including Fourier methods, required for applications.
The second
†
Warning: We follow common practice and refer to the “delta distribution” as a function,
even though, as we will see, it is most deﬁnitely not a function in the usual sense.
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
6
215
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

216
6 Generalized Functions and Green’s Functions
section will discuss the method of Green’s functions in the context of one-dimensional
boundary value problems governed by ordinary diﬀerential equations. In the ﬁnal section,
we develop the Green’s function method for solving basic boundary value problems for the
two-dimensional Poisson equation, which epitomizes the class of planar elliptic boundary
value problems.
6.1 Generalized Functions
Our goal is to solve inhomogeneous linear boundary value problems by ﬁrst determining
the eﬀect of a concentrated impulse force. The response to a general forcing function is
then found by linear superposition. But before diving in, let us ﬁrst review the relevant
constructions in the case of linear systems of algebraic equations.
Consider a system of n linear equations in n unknowns† u = ( u1, u2, . . . , un )T , written
in matrix form
Au = f.
(6.1)
Here A is a ﬁxed n × n matrix, assumed to be nonsingular, which ensures the existence
of a unique solution u for any choice of right-hand side f = ( f1, f2, . . . , fn )T ∈Rn. We
regard the linear system (6.1) as representing the equilibrium equations of some physical
system, e.g., a system of masses interconnected by springs. In this context, the right hand
side f represents an external forcing, so that its ith entry, fi, represents the amount of force
exerted on the ith mass, while the ith entry of the solution vector, ui, represents the ith
mass’ induced displacement.
Let
e1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
e2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
. . .
en =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
...
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(6.2)
denote the standard basis vectors of Rn, so that ej has a single 1 in its jth entry and all
other entries 0. We interpret each ej as a concentrated unit impulse force that is applied
solely to the jth mass in our physical system. Let uj = (uj,1, . . . , uj,n)T be the induced
response of the system, that is, the solution to
Auj = ej.
(6.3)
Let us suppose that we have calculated the response vectors u1, . . . , un to each such impulse
force. We can express any other force vector as a linear combination,
f =
⎛
⎜
⎜
⎜
⎝
f1
f2
...
fn
⎞
⎟
⎟
⎟
⎠= f1 e1 + f2 e2 + · · · + fn en,
(6.4)
†
All vectors are column vectors, but we sometimes write the transpose, which is a row vector,
to save space.

6.1 Generalized Functions
217
of the impulse forces. The Superposition Principle of Theorem 1.7 then implies that the
solution to the inhomogeneous system (6.1) is the selfsame linear combination of the indi-
vidual impulse responses:
u = f1 u1 + f2 u2 + · · · + fn un.
(6.5)
Thus, knowing how the linear system responds to each impulse force allows us to immedi-
ately calculate its response to a general external force.
Remark: The alert reader will recognize that u1, . . . , un are the columns of the inverse
matrix, A−1, and so formula (6.5) is, in fact, reconstructing the solution to the linear system
(6.1) by inverting its coeﬃcient matrix: u = A−1f. Thus, this observation is merely a
restatement of a standard linear algebraic system solution technique.
The Delta Function
The aim of this chapter is to adapt the preceding algebraic solution technique to boundary
value problems. Suppose we want to solve a linear boundary value problem governed by
an ordinary diﬀerential equation on an interval a < x < b, the boundary conditions being
imposed at the endpoints. The key issue is how to characterize an impulse force that is
concentrated at a single point.
In general, a unit impulse at position a < ξ < b will be described by something called
the delta function, and denoted by δξ(x). Since the impulse is supposed to be concentrated
solely at x = ξ, our ﬁrst requirement is
δξ(x) = 0
for
x ̸= ξ.
(6.6)
Moreover, since the delta function represents a unit impulse, we want the total amount
of force to be equal to one.
Since we are dealing with a continuum, the total force is
represented by an integral over the entire interval, and so we also require that the delta
function satisfy
 b
a
δξ(x) dx = 1,
provided
a < ξ < b.
(6.7)
Alas, there is no bona ﬁde function that enjoys both of the required properties! Indeed,
according to the basic facts of Riemann (or even Lebesgue) integration, two functions that
are the same everywhere except at a single point have exactly the same integral, [96, 98].
Thus, since δξ is zero except at one point, its integral should be 0, not 1. The mathematical
conclusion is that the two requirements, (6.6–7) are inconsistent!
This unfortunate fact stopped mathematicians dead in their tracks. It took the imagi-
nation of a British engineer, Oliver Heaviside, who was not deterred by the lack of rigorous
justiﬁcation, to start utilizing delta functions in practical applications — with remarkable
eﬀect. Despite his success, Heaviside was ridiculed by the mathematicians of his day, and
eventually succumbed to mental illness. But, some thirty years later, the great British
theoretical physicist Paul Dirac resurrected the delta function for quantum-mechanical ap-
plications, and this ﬁnally made the mathematicians sit up and take notice. (Indeed, the
term “Dirac delta function” is quite common, even though Heaviside should rightly have
priority.) In 1944, the French mathematician Laurent Schwartz ﬁnally established a rigor-
ous theory of distributions that incorporated such useful but nonstandard objects, [103].
Thus, to be more accurate, we should really refer to the delta distribution; however, we

218
6 Generalized Functions and Green’s Functions
will retain the more common, intuitive designation “delta function” throughout. It is be-
yond the scope of this introductory text to develop a fully rigorous theory of distributions.
Rather, in the spirit of Heaviside, we shall concentrate on learning, through practice with
computations and applications, how to make eﬀective use of these exotic mathematical
creatures.
There are two possible ways to introduce the delta distribution. Both are important
and worth understanding.
Method #1. Limits:
The ﬁrst approach is to regard the delta function δξ(x) as a
limit of a sequence of ordinary smooth functions† gn(x). These will represent progressively
more and more concentrated unit forces, which, in the limit, converge to the desired unit
impulse concentrated at a single point, x = ξ. Thus, we require
lim
n →∞gn(x) = 0,
x ̸= ξ,
(6.8)
while the total amount of force remains ﬁxed at
 b
a
gn(x) dx = 1
for all n.
(6.9)
On a formal level, the limit “function”
δξ(x) = lim
n →∞gn(x)
will satisfy the key properties (6.6–7).
An explicit example of such a sequence is provided by the rational functions
gn(x) =
n
π(1 + n2x2) .
(6.10)
These functions satisfy
lim
n →∞gn(x) =
 0,
x ̸= 0,
∞,
x = 0,
(6.11)
while‡
 ∞
−∞
gn(x) dx = 1
π tan−1 nx

∞
x=−∞
= 1.
(6.12)
Therefore, formally, we identify the limiting function
lim
n →∞gn(x) = δ(x) = δ0(x)
(6.13)
with the unit-impulse delta function concentrated at x = 0. As sketched in Figure 6.1, as n
gets larger and larger, each successive function gn(x) forms a more and more concentrated
spike, while maintaining a unit total area under its graph. Thus, the limiting delta function
can be thought of as an inﬁnitely tall spike of zero width, entirely concentrated at the origin.
†
To keep the notation compact, we suppress the dependence of the functions gn on the point
ξ where the limiting delta function is concentrated.
‡
For the moment, it will be slightly simpler to consider the entire real line −∞< x < ∞.
Exercise 6.1.8 discusses how to adapt the construction to a ﬁnite interval.

6.1 Generalized Functions
219
Figure 6.1.
Delta function as limit.
Remark: There are many other possible choices for the limiting functions gn(x). See
Exercise 6.1.7 for another important example.
Remark: This construction of the delta function highlights the perils of interchanging
limits and integrals without rigorous justiﬁcation. In any standard theory of integration
(Riemann, Lebesgue, etc.), the limit of the functions gn would be indistinguishable from
the zero function, so the limit of their integrals (6.12) would not equal the integral of their
limit:
1 = lim
n →∞
 ∞
−∞
gn(x) dx ̸=
 ∞
−∞
lim
n →∞gn(x) dx = 0.
The delta function is, in a sense, a means of sidestepping this analytic inconvenience. The
full ramiﬁcations and theoretical constructions underlying such limits must, however, be
deferred to a rigorous course in real analysis, [96, 98].
Once we have deﬁned the basic delta function δ(x) = δ0(x) concentrated at the ori-
gin, we can obtain the delta function concentrated at any other position ξ by a simple
translation:
δξ(x) = δ(x −ξ).
(6.14)
Thus, δξ(x) can be realized as the limit, as n →∞, of the translated functions
gn(x) = gn(x −ξ) =
n
π

1 + n2(x −ξ)2  .
(6.15)
Method #2. Duality:
The second approach is a bit more abstract, but much closer
in spirit to the proper rigorous formulation of the theory of distributions like the delta
function. The critical property is that if u(x) is any continuous function, then
 b
a
δξ(x) u(x) dx = u(ξ),
for
a < ξ < b.
(6.16)

220
6 Generalized Functions and Green’s Functions
Indeed, since δξ(x) = 0 for x ̸= ξ, the integrand depends only on the value of u at the
point x = ξ, and so
 b
a
δξ(x) u(x) dx =
 b
a
δξ(x) u(ξ) dx = u(ξ)
 b
a
δξ(x) dx = u(ξ).
Equation (6.16) serves to deﬁne a linear functional† Lξ: C0[a, b] →R that maps a contin-
uous function u ∈C0[a, b] to its value at the point x = ξ :
Lξ[u] = u(ξ).
(6.17)
The basic linearity requirements (1.11) are immediately established:
Lξ[u + v] = u(ξ) + v(ξ) = Lξ[u] + Lξ[v],
Lξ[cu] = cu(ξ) = c Lξ[u],
for any functions u(x), v(x).
In the dual approach to generalized functions, the delta
function is, in fact, deﬁned as this particular linear functional (6.17). The function u(x)
is sometimes referred to as a test function, since it serves to “test” the form of the linear
functional Lξ.
Remark: If the impulse point ξ lies outside the integration domain, then
 b
a
δξ(x) u(x) dx = 0
whenever
ξ < a
or
ξ > b,
(6.18)
because the integrand is identically zero on the entire interval. For technical reasons, we
will not attempt to deﬁne the integral (6.18) if the impulse point ξ = a or ξ = b lies on the
boundary of the interval of integration.
The interpretation of the linear functional Lξ as representing a kind of function δξ(x)
is based on the following line of thought. According to Corollary B.34, every scalar-valued
linear function L: Rn →R on the ﬁnite-dimensional vector space Rn is obtained by taking
the dot product with a ﬁxed element a ∈Rn, so
L[u] = a · u.
In this sense, linear functions on Rn are the “same” as vectors. Similarly, on the inﬁnite-
dimensional function space C0[a, b], the L2 inner product
Lg[u] = ⟨g , u ⟩=
 b
a
g(x) u(x) dx,
(6.19)
taken with a ﬁxed continuous function g ∈C0[a, b], deﬁnes a real-valued linear functional
Lg: C0[a, b] →R. However, unlike the ﬁnite-dimensional situation, not every real-valued
linear functional is of this form! In particular, there is no bona ﬁde function δξ(x) such
that the identity
Lξ[u] = ⟨δξ , u ⟩=
 b
a
δξ(x) u(x) dx = u(ξ)
(6.20)
holds for every continuous function u(x).
The bottom line is that every (continuous)
function deﬁnes a linear functional, but not every linear functional arises in this manner.
†
The term “functional” is used to refer to a linear function whose domain is a function space,
thus avoiding confusion with the functions it acts on.

6.1 Generalized Functions
221
But the dual interpretation of generalized functions acts as if this were true. Gen-
eralized functions are, in actuality, real-valued linear functionals on function space, but
intuitively interpreted as a kind of function via the L2 inner product. Although this iden-
tiﬁcation is not to be taken too literally, one can, with some care, manipulate generalized
functions as if they were actual functions, but always keeping in mind that a rigorous
justiﬁcation of such computations must ultimately rely on their innate characterization as
linear functionals.
The two approaches — limits and duality — are completely compatible. Indeed, one
can recover the dual formula (6.20) as the limit
u(ξ) = lim
n →∞⟨gn , u ⟩= lim
n →∞
 b
a
gn(x) u(x) dx =
 b
a
δξ(x) u(x) dx = ⟨δξ , u ⟩
(6.21)
of the inner products of the function u with the approximating concentrated impulse func-
tions gn(x) satisfying (6.8–9). In this manner, the limiting linear functional represents the
delta function:
u(ξ) = Lξ[u] = lim
n →∞Ln[u],
where
Ln[u] =
 ℓ
0
gn(x) u(x) dx.
The choice of interpretation of the generalized delta function is, at least on an operational
level, a matter of taste. For the beginner, the limit version is perhaps easier to digest
initially. However, the dual, linear functional interpretation has stronger connections with
the rigorous theory and, even in applications, oﬀers some signiﬁcant advantages.
Although the delta function might strike you as somewhat bizarre, its utility through-
out modern applied mathematics and mathematical physics more than justiﬁes including
it in your analytical toolbox. While probably not yet comfortable with either deﬁnition,
you are advised to press on and familiarize yourself with its basic properties. With a little
care, you usually won’t go far wrong by treating it as if it were a genuine function. After
you gain more practical experience, you can, if desired, return to contemplate just exactly
what kind of creature the delta function really is.
Calculus of Generalized Functions
In order to make use of the delta function, we need to understand how it behaves under
the basic operations of linear algebra and calculus. First, we can take linear combinations
of delta functions. For example,
h(x) = 2 δ(x) −3 δ(x −1) = 2 δ0(x) −3 δ1(x)
represents a combination of an impulse of magnitude 2 concentrated at x = 0 and one
of magnitude −3 concentrated at x = 1. In the dual interpretation, h deﬁnes the linear
functional
Lh[u] = ⟨h , u ⟩= ⟨2 δ0 −3 δ1 , u ⟩= 2 ⟨δ0 , u ⟩−3 ⟨δ1 , u ⟩= 2u(0) −3u(1),
or, more explicitly,
Lh[u] =
 b
a
h(x) u(x) dx =
 b
a

2 δ(x) −3 δ(x −1)

u(x) dx
= 2
 b
a
δ(x) u(x) dx −3
 b
a
δ(x −1) u(x) dx = 2u(0) −3u(1),

222
6 Generalized Functions and Green’s Functions
Figure 6.2.
Step function as limit.
provided a < 0 and b > 1.
Next, since δξ(x) = 0 for any x ̸= ξ, multiplying the delta function by an ordinary
function is the same as multiplying by a constant:
g(x) δξ(x) = g(ξ) δξ(x),
(6.22)
provided g(x) is continuous at x = ξ. For example, x δ(x) ≡0 is the same as the constant
zero function.
Warning: Since they are inherently linear functionals, it is not permissible to multi-
ply delta functions together, or to apply more complicated nonlinear operations to them.
Expressions like δ(x)2, 1/δ(x), eδ(x), etc., are not well deﬁned in the theory of general-
ized functions — although this makes their application to nonlinear diﬀerential equations
problematic.
The integral of the delta function is the unit step function:
 x
a
δξ(t) dt = σξ(x) = σ(x −ξ) =
 0,
x < ξ,
1,
x > ξ,
provided
a < ξ.
(6.23)
Unlike the delta function, the step function σξ(x) is an ordinary function. It is continuous
— indeed constant — except at x = ξ. The value of the step function at the discontinuity
x = ξ is left unspeciﬁed, although a wise choice — compatible with Fourier theory — is to
set σξ(y) = 1
2, the average of its left- and right-hand limits.
We note that the integration formula (6.23) is compatible with our characterization of
the delta function as the limit of highly concentrated forces. Integrating the approximating
functions (6.10), we obtain
fn(x) =
 x
−∞
gn(t) dt = 1
π tan−1 nx + 1
2 .
Since
lim
y →∞tan−1 y = 1
2 π,
while
lim
y →−∞tan−1 y = −1
2 π,
these functions converge (nonuniformly) to the step function:
lim
n →∞fn(x) = σ(x) =
⎧
⎨
⎩
0,
x < 0,
1
2,
x = 0,
1,
x > 0.
(6.24)

6.1 Generalized Functions
223
Figure 6.3.
First and second-order ramp functions.
A graphical illustration of this limiting process appears in Figure 6.2.
The integral of the discontinuous step function (6.23) is the continuous ramp function
 x
a
σξ(t) dt = ρξ(x) = ρ(x −ξ) =
 0,
x < ξ,
x −ξ,
x > ξ,
provided
a < ξ,
(6.25)
which is graphed in Figure 6.3. Note that ρξ(x) has a corner at x = ξ, and so is not
diﬀerentiable there; indeed, its derivative ρ′(x −ξ) = σ(x −ξ) has a jump discontinuity.
We can continue to integrate; the (n + 1)st integral of the delta function is the nth order
ramp function
ρn,ξ(x) = ρn(x −ξ) =
⎧
⎨
⎩
0,
x < ξ,
(x −ξ)n
n!
,
x > ξ.
(6.26)
Note that ρn,ξ ∈Cn−1 has only n −1 continuous derivatives.
What about diﬀerentiation? Motivated by the Fundamental Theorem of Calculus,
we shall use formula (6.23) to identify the derivative of the step function with the delta
function
dσ
dx = δ .
(6.27)
This fact is highly signiﬁcant. In elementary calculus, one is not allowed to diﬀerentiate
a discontinuous function. Here, we discover that the derivative can be deﬁned, not as an
ordinary function, but rather as a generalized delta function!
In general, the derivative of a piecewise C1 function with jump discontinuities is a gen-
eralized function that includes a delta function concentrated at each discontinuity, whose
magnitude equals the jump magnitude.
More explicitly, suppose that f(x) is diﬀeren-
tiable, in the usual calculus sense, everywhere except at a point ξ, where it has a jump
discontinuity of magnitude β. Using the step function (3.47), we can re-express
f(x) = g(x) + β σ(x −ξ),
(6.28)
where g(x) is continuous everywhere, with a removable discontinuity at x = ξ, and diﬀer-
entiable except possibly at the jump. Diﬀerentiating (6.28), we ﬁnd that
f ′(x) = g′(x) + β δ(x −ξ)
(6.29)
has a delta spike of magnitude β at the discontinuity. Thus, the derivatives of f and g
coincide everywhere except at the discontinuity.

224
6 Generalized Functions and Green’s Functions
−1
1
2
−1
1
f(x)
−1
1
2
−1
1
f ′(x)
Figure 6.4.
The derivative of the discontinuous function in Example 6.1.
Example 6.1. Consider the function
f(x) =
 −x,
x < 1,
1
5 x2,
x > 1,
(6.30)
which we graph in Figure 6.4. We note that f has a single jump discontinuity at x = 1 of
magnitude
f(1+) −f(1−) = 1
5 −(−1) = 6
5.
This means that
f(x) = g(x) + 6
5 σ(x −1),
where
g(x) =
 −x,
x < 1,
1
5 x2 −6
5,
x > 1,
is continuous everywhere, since its right- and left-hand limits at the original discontinuity
are equal: g(1+) = g(1−) = −1. Therefore,
f ′(x) = g′(x) + 6
5 δ(x −1),
where
g′(x) =
 −1,
x < 1,
2
5 x,
x > 1,
while g′(1) and f ′(1) are not deﬁned. In Figure 6.4, the delta spike in the derivative of f is
symbolized by a vertical line, although this pictorial device fails to indicate its magnitude
of 6
5 .
Note that in this particular example, g′(x) can be found by directly diﬀerentiating
the formula for f(x). Indeed, in general, once we determine the magnitude and location
of the jump discontinuities of f(x), we can compute its derivative without introducing the
auxiliary function g(x).
Example 6.2. As a second, more streamlined, example, consider the function
f(x) =
⎧
⎨
⎩
−x,
x < 0,
x2 −1,
0 < x < 1,
2e−x,
x > 1,
which is plotted in Figure 6.5. This function has jump discontinuities of magnitude −1 at
x = 0, and of magnitude 2/e at x = 1. Therefore, in light of the preceding remark,
f ′(x) = −δ(x) + 2
e δ(x −1) +
⎧
⎨
⎩
−1,
x < 0,
2x,
0 < x < 1,
−2e−x,
x > 1,

6.1 Generalized Functions
225
−1
1
2
−1
1
f(x)
−1
1
−2
2
4
f ′(x)
Figure 6.5.
The derivative of the discontinuous function in Example 6.2.
where the ﬁnal terms are obtained by directly diﬀerentiating f(x).
Example 6.3. The derivative of the absolute value function
a(x) = | x | =
 x,
x > 0,
−x,
x < 0,
is the sign function
a′(x) = sign x =
 +1,
x > 0,
−1,
x < 0.
(6.31)
Note that there is no delta function in a′(x) because a(x) is continuous everywhere. Since
sign x has a jump of magnitude 2 at the origin and is otherwise constant, its derivative is
twice the delta function:
a′′(x) = d
dx sign x = 2δ(x).
Example 6.4.
We are even allowed to diﬀerentiate the delta function.
Its ﬁrst
derivative δ ′(x) can be interpreted in two ways. First, as the limit of the derivatives of the
approximating functions (6.10):
dδ
dx = lim
n →∞
dgn
dx = lim
n →∞
−2n3 x
π(1 + n2 x2)2 .
(6.32)
The graphs of these rational functions take the form of more and more concentrated spiked
“doublets”, as illustrated in Figure 6.6. To determine the eﬀect of the derivative on a test
function u(x), we compute the limiting integral
⟨δ ′ , u ⟩=
 ∞
−∞
δ ′(x) u(x) dx = lim
n →∞
 ∞
−∞
g′
n(x) u(x) dx
= −lim
n →∞
 ∞
−∞
gn(x) u′(x) dx = −
 ∞
−∞
δ(x) u′(x) dx = −u′(0).
(6.33)
The middle step is the result of an integration by parts, noting that the boundary terms
at ±∞vanish, provided that u(x) is continuously diﬀerentiable and bounded as | x | →∞.
Pay attention to the minus sign in the ﬁnal answer.

226
6 Generalized Functions and Green’s Functions
Figure 6.6.
Derivative of delta function as limit of doublets.
In the dual interpretation, the generalized function δ ′(x) corresponds to the linear
functional
L′[u] = −u′(0) = ⟨δ ′ , u ⟩=
 b
a
δ ′(x) u(x) dx,
where
a < 0 < b,
(6.34)
which maps a continuously diﬀerentiable function u(x) to minus its derivative at the origin.
We note that (6.34) is compatible with a formal integration by parts:
 b
a
δ ′(x) u(x) dx = δ(x) u(x)

b
x=a
−
 b
a
δ(x) u′(x) dx = −u′(0).
The boundary terms at x = a and x = b automatically vanish, since δ(x) = 0 for x ̸= 0.
Remark: While we can test the delta function with any continuous function, we are
permitted to test its derivative only on continuously diﬀerentiable functions.
To avoid
keeping track of such technicalities, one often restricts to only inﬁnitely diﬀerentiable test
functions.
Warning: The functions gn(x) = gn(x) + g′
n(x), cf. (6.10, 32), satisfy
lim
n →∞gn(x) = 0
for all x ̸= 0, while
 ∞
−∞
gn(x) dx = 1. However,
lim
n →∞gn = lim
n →∞gn + lim
n →∞g′
n = δ + δ ′.
Thus, our original conditions (6.8–9) are not in fact suﬃcient to characterize whether a
sequence of functions has the delta function as a limit. To be absolutely sure, one must,
in fact, verify the more comprehensive limiting formula (6.21).

6.1 Generalized Functions
227
Exercises
6.1.1. Evaluate the following integrals: (a)
	 π
−π δ(x) cos x dx, (b)
	 2
1 δ(x) (x −2) dx,
(c)
	 3
0 δ1(x) ex dx, (d)
	 e
1 δ(x −2) log x dx, (e)
	 1
0 δ

x −1
3

x2 dx, (f )
	 1
−1
δ(x + 2) dx
1 + x2
.
6.1.2. Simplify the following generalized functions; then write out how they act on a suitable
test function u(x):
(a) ex δ(x),
(b) x δ(x −1),
(c) 3 δ1(x) −3x δ−1(x),
(d) δ(x −1)
x + 1
,
(e) (cos x)

δ(x) + δ(x −π) + δ(x + π)
 
,
(f ) δ1(x) −δ2(x)
x2 + 1
.
6.1.3. Deﬁne the generalized function ϕ(x) = δ(x + 1) −δ(x −1):
(a) as a limit of ordinary functions; (b) using duality.
6.1.4. Find and sketch a graph of the derivative (in the context of generalized functions) of the
following functions:
(a) f(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
x2,
0 < x < 3,
x,
−1 < x < 0,
0,
otherwise,
(b) g(x) =

sin | x |,
| x | < 1
2π,
0,
otherwise,
(c) h(x) =
⎧
⎪
⎨
⎪
⎩
sin πx,
x > 1,
1 −x2,
−1 < x < 1,
ex,
x < −1,
(d) k(x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
sin x,
x < −π,
x2 −π2,
−π < x < 0,
e−x,
x > 0.
6.1.5. Find the ﬁrst and second derivatives of the functions (a) f(x) =
⎧
⎪
⎨
⎪
⎩
x + 1,
−1 < x < 0,
1 −x,
0 < x < 1,
0,
otherwise,
(b) k(x) =
 | x |,
−2 < x < 2,
0,
otherwise,
(c) s(x) =
 1 + cos πx,
−1 < x < 1,
0,
otherwise.
6.1.6. Find the ﬁrst and second derivatives of f(x) =
(a) e−| x | ,
(b) 2| x | −| x −1 | ,
(c) | x2 + x | ,
(d) x sign(x2 −4),
(e) sin | x |,
(f ) | sin x |,
(g) sign(sin x).
♦6.1.7. Explain why the Gaussian functions gn(x) =
n
√π e−n2 x2 have the delta function δ(x) as
their limit as n →∞.
♦6.1.8. In this exercise, we realize the delta function δξ(x) as a limit of functions on a ﬁnite
interval [a, b]. Let a < ξ < b.
(a) Prove that the functions gn(x) = gn(x −ξ)
Mn
, where gn(x) is given by (6.10) and
Mn =
	 b
a gn(x −ξ) dx, satisfy (6.8–9), and hence
lim
n →∞
gn(x) = δξ(x).
(b) One can, alternatively, relax the second condition (6.9) to
lim
n →∞
	 b
a gn(x −ξ) dx = 1.
Show that, under this relaxed deﬁnition,
lim
n →∞gn(x −ξ) = δξ(x).
♥6.1.9. For each positive integer n, let gn(x) =

1
2 n,
| x | < 1/n,
0,
otherwise.
(a) Sketch a graph of
gn(x). (b) Show that
lim
n →∞gn(x) = δ(x). (c) Evaluate fn(x) =
	 x
−∞gn(y) dy and sketch
a graph. Does the sequence fn(x) converge to the step function σ(x) as n →∞? (d) Find
the derivative hn(x) = g ′
n(x).
(e) Does the sequence hn(x) converge to δ ′(x) as n →∞?
♥6.1.10. Answer Exercise 6.1.9 for the hat functions gn(x) =

n −n2 | x |,
| x | < 1/n,
0,
otherwise.

228
6 Generalized Functions and Green’s Functions
6.1.11. Justify the formula x δ(x) = 0 using (a) limits, (b) duality.
♦6.1.12.(a) Justify the formula δ(2x) = 1
2 δ(x) by (i) limits, (ii) duality.
(b) Find a similar
formula for δ(ax) when a > 0. (c) What about when a < 0?
6.1.13.(a) Prove that σ(λx) = σ(x) for any λ > 0.
(b) What about if λ < 0? (c) Use parts
(a,b) to deduce that δ(λx) =
1
| λ | δ(x) for any λ ̸= 0.
6.1.14. Let g(x) be a continuously diﬀerentiable function with g′(x) ̸= 0 for all x ∈R. Does the
composition δ(g(x)) make sense as a distribution? If so, can you identify it?
6.1.15. Let ξ < a. Sketch the graphs of (a) s(x) =
	 x
a δξ(z) dz, (b) r(x) =
	 x
a σξ(z) dz.
6.1.16. Justify the formula
lim
n →∞n

δ

x −1
n

−δ

x + 1
n
  
= −2 δ ′(x).
6.1.17. Deﬁne the generalized function δ ′′(x):
(a) as a limit of ordinary functions; (b) using duality.
6.1.18. Let δ(k)
ξ
(x) denote the kth derivative of the delta function δξ(x). Justify the formula
⟨δ(k)
ξ
, u ⟩= (−1)k u(k)(ξ) whenever u ∈Ck is k–times continuously diﬀerentiable.
6.1.19. According to (6.22), x δ(x) = 0. On the other hand, by Leibniz’ rule,
(x δ(x))′ = δ(x) + x δ ′(x) is apparently not zero. Can you explain this paradox?
6.1.20. If f ∈C1, should (f δ)′ = f δ ′ or f′ δ + f δ ′?
♦6.1.21.(a) Use duality to justify the formula f(x) δ ′(x) = f(0) δ ′(x) −f′(0) δ(x) when f ∈C1.
(b) Find a similar formula for f(x) δ(n)(x) as the product of a suﬃciently smooth function
and the nth derivative of the delta function.
6.1.22. Use Exercise 6.1.21 to simplify the following generalized functions; then write out how
they act on a suitable test function u(x):
(a) ϕ(x) = (x −2) δ ′(x),
(b) ψ(x) = (1 + sin x)

δ(x) + δ ′(x)
 
,
(c) χ(x) = x2
δ(x −1) −δ ′(x −2)
 
,
(d) ω(x) = ex δ ′′(x + 1).
♦6.1.23. Prove that if f(x) is a continuous function, and
	 b
a f(x) dx = 0 for every interval [a, b],
then f(x) ≡0 everywhere.
♦6.1.24. Write out a rigorous proof that there is no continuous function δξ(x) such that the in-
ner product identity (6.20) holds for every continuous function u(x).
♦6.1.25. True or false: The sequence (6.24) converges uniformly.
6.1.26. True or false: ∥δ ∥= 1.
The Fourier Series of the Delta Function
Let us next investigate the capability of Fourier series to represent generalized functions.
We begin with the delta function δ(x), based at the origin.
Using the characterizing
properties (6.16), its real Fourier coeﬃcients are
ak = 1
π
 π
−π
δ(x) coskx dx = 1
π cos k0 = 1
π ,
bk = 1
π
 π
−π
δ(x) sin kx dx = 1
π sin k0 = 0.
(6.35)

6.1 Generalized Functions
229
Therefore, at least on a formal level, its Fourier series is
δ(x) ∼
1
2π + 1
π

cos x + cos 2x + cos 3x + · · ·

.
(6.36)
Since δ(x) = δ(−x) is an even function (why?), it should come as no surprise that it has
a cosine series. Alternatively, we can rewrite the series in complex form
δ(x) ∼
1
2π
∞

k=−∞
e i kx = 1
2π ( · · · + e−2 i x + e−i x + 1 + e i x + e2 i x + · · · ),
(6.37)
where the complex Fourier coeﬃcients are computed† as
ck = 1
2π
 π
−π
δ(x) e−i kx dx = 1
2π .
Remark: Although we stated that the Fourier series (6.36) represents the delta func-
tion, this is not entirely correct. Remember that a Fourier series converges to the 2π–
periodic extension of the original function. Therefore, (6.37) actually represents the peri-
odic extension of the delta function, sometimes called the Dirac comb,
δ(x) = · · · +δ(x+4π)+δ(x+2π)+δ(x)+δ(x−2π)+δ(x−4π)+δ(x−6π)+ · · · , (6.38)
consisting of a periodic array of unit impulses concentrated at all integer multiples of 2π.
Let us investigate in what sense (if any) the Fourier series (6.36) or, equivalently,
(6.37), represents the delta function. The ﬁrst observation is that, because its summands
do not tend to zero, the series certainly doesn’t converge in the usual, calculus, sense.
Nevertheless, in a “weak” sense, the series can be regarded as converging to the (periodic
extension of the) delta function.
To understand the convergence mechanism, we recall that we already established a
formula (3.129) for the partial sums:
sn(x) = 1
2π
n

k=−n
e i kx = 1
2π + 1
π
n

k=1
cos kx = 1
2π
sin

n + 1
2

x
sin 1
2 x
.
(6.39)
Graphs of some of the partial sums on the interval [−π, π ] are displayed in Figure 6.7.
Note that, as n increases, the spike at x = 0 becomes progressively taller and thinner,
converging to an inﬁnitely tall delta spike. (We had to truncate the last two graphs; the
spike extends beyond the top.) Indeed, by l’Hˆopital’s Rule,
lim
x→0
1
2π
sin

n + 1
2

x
sin 1
2 x
= lim
x→0
1
2π

n + 1
2

cos

n + 1
2

x
1
2 cos 1
2 x
=
n + 1
2
π
−→∞
as
n →∞.
(An elementary proof of this formula is to note that, at x = 0, every term in the original
sum (6.36) is equal to 1.) Furthermore, the integrals remain ﬁxed,
1
2π
 π
−π
sn(x) dx = 1
2π
 π
−π
sin

n + 1
2

x
sin 1
2 x
dx = 1
2π
 π
−π
n

k=−n
e i kx dx = 1,
(6.40)
†
Or we could use (3.66).

230
6 Generalized Functions and Green’s Functions
s1(x)
s5(x)
s10(x)
s25(x)
s50(x)
s100(x)
Figure 6.7.
Partial Fourier sums approximating the delta function.
as required for convergence to the delta function. However, away from the spike, the partial
sums do not go to zero! Rather, they oscillate ever more rapidly, while maintaining a ﬁxed
overall amplitude of
1
2π csc 1
2 x =
1
2π sin 1
2 x .
(6.41)
As n increases, the amplitude function (6.41) can be seen, as in Figure 6.7, as the envelope
of the increasingly rapid oscillations. So, roughly speaking, the convergence sn(x) →δ(x)
means that the “inﬁnitely fast” oscillations are somehow canceling each other out, and the
net eﬀect is zero away from the spike at x = 0. So the convergence of the Fourier sums to
δ(x) is much more subtle than in the original limiting deﬁnition (6.10).
The technical term is weak convergence, which plays a very important role in advanced
mathematical analysis, signal processing, composite materials, and elsewhere.
Deﬁnition 6.5.
A sequence of functions fn(x) is said to converge weakly to f⋆(x)
on an interval [a, b] if their L2 inner products with every continuous test function u(x) ∈
C0[a, b] converge:
 b
a
fn(x) u(x) dx −→
 b
a
f⋆(x) u(x) dx
as
n −→∞.
(6.42)
Weak convergence is often indicated by a half-pointed arrow: fn ⇀g.
Remark: On unbounded intervals, one usually restricts the test functions to have
compact support, meaning that u(x) = 0 for all suﬃciently large | x | ≫0. One can also
restrict to smooth test functions only, e.g., require that u ∈C∞[a, b].

6.1 Generalized Functions
231
Example 6.6. Let us show that the trigonometric functions fn(x) = cos nx converge
weakly to the zero function:
cos nx −⇀0
as
n −→∞
on the interval
[−π, π ].
(Actually, this holds on any interval; see Exercise 6.1.38.) According to the deﬁnition, we
need to prove that
lim
n →∞
 π
−π
u(x) cosnx dx = 0
for any continuous function u ∈C0[−π, π ]. But this is just a restatement of the Riemann–
Lebesgue Lemma 3.40, which says that the high-frequency Fourier coeﬃcients of a continu-
ous (indeed, even square-integrable) function u(x) go to zero. The same remark establishes
the weak convergence sin nx ⇀0.
Observe that the functions cos nx fail to converge pointwise to 0 at any value of x.
Indeed, if x is an integer multiple of 2π, then cos nx = 1 for all n.
If x is any other
rational multiple of π, the values of cos nx periodically cycle through a ﬁnite number of
diﬀerent values, and never go to 0, while if x is an irrational multiple of π, they oscillate
aperiodically between −1 and +1. The functions also fail to converge in norm to 0, since
their (unscaled) L2 norms remain ﬁxed at
∥cosnx ∥=
 π
−π
cos2 nx dx = √π
for all
n > 0.
The cancellation of oscillations in the high-frequency limit is a characteristic feature of
weak convergence.
Let us now explain why, although the Fourier series (6.36) does not converge to the
delta function either pointwise or in norm (indeed, ∥δ ∥is not even deﬁned!), it does
converge weakly on [−π, π ]. More speciﬁcally, we need to prove that the partial sums
sn ⇀δ, meaning that
lim
n →∞
 π
−π
sn(x) u(x) dx =
 π
−π
δ(x) u(x) dx = u(0)
(6.43)
for every suﬃciently nice function u, or, equivalently,
lim
n →∞
1
2π
 π
−π
u(x) sin

n + 1
2

x
sin 1
2 x
dx = u(0).
(6.44)
But this is a restatement of a special case of the identities (3.130) used in the proof of
the Pointwise Convergence Theorem 3.8 for the Fourier series of a (piecewise) C1 function.
Indeed, summing the two identities in (3.130) and then setting x = 0 reproduces (6.44),
since, by continuity, u(0) = 1
2

u(0+) + u(0−)

. In other words, the pointwise convergence
of the Fourier series of a C1 function is equivalent to the weak convergence of the Fourier
series of the delta function!
Remark: Although not every continuous function has an everywhere pointwise con-
vergent Fourier series, (6.44) nevertheless continues to be valid whenever u is continuous,
as a consequence of Carleson’s Theorem, [28]; see the remark at the end of Section 3.5.

232
6 Generalized Functions and Green’s Functions
Example 6.7. If we diﬀerentiate the Fourier series
x ∼2
∞

k=1
(−1)k−1
k
sin kx = 2

sin x −sin 2x
2
+ sin 3x
3
−sin 4x
4
+ · · ·

,
we obtain an apparent contradiction:
1 ∼2
∞

k=1
(−1)k+1 cos kx = 2 cosx −2 cos2x + 2 cos3x −2 cos4x + · · · .
(6.45)
But the Fourier series for 1 consists of just a single constant term! (Why?)
The resolution of this paradox is not diﬃcult.
The Fourier series (3.37) does not
converge to x, but rather to its 2π–periodic extension f(x), which has jump discontinuities
of magnitude 2π at odd multiples of π; see Figure 3.1. Thus, Theorem 3.22 is not directly
applicable. Nevertheless, we can assign a consistent interpretation to the diﬀerentiated
series. The derivative f ′(x) of the periodic extension is not equal to the constant function
1, but rather has an additional delta function concentrated at each jump discontinuity:
f ′(x) = 1 −2π
∞

j =−∞
δ

x −(2j + 1)π

= 1 −2π δ(x −π),
where δ denotes the 2π–periodic extension of the delta function, cf. (6.38). The diﬀerenti-
ated Fourier series (6.45) does, in fact, represent f ′(x). Indeed, the Fourier coeﬃcients of
δ(x −π) are
ak = 1
π
 2π
0
δ(x −π) cos kx dx = 1
π cos kπ = (−1)k
π
,
bk = 1
π
 2π
0
δ(x −π) sin kx dx = 1
π sin kπ = 0.
Observe that we changed the interval of integration to [0, 2π ] to avoid placing the delta
function singularities at the endpoints. Thus,
δ(x −π) ∼
1
2π + 1
π

−cos x + cos 2x −cos 3x + · · ·

,
(6.46)
which serves to resolve the contradiction.
Example 6.8. Let us diﬀerentiate the Fourier series
σ(x) ∼1
2 + 2
π

sin x + sin 3x
3
+ sin 5x
5
+ sin 7x
7
+ · · ·

for the unit step function we found in Example 3.9 and see whether we end up with the
Fourier series (6.36) for the delta function. We compute
dσ
dx ∼2
π

cos x + cos 3x + cos 5x + cos 7x + · · ·

,
(6.47)
which does not agree with (6.36) — half the terms are missing! The explanation is similar
to the preceding example: the 2π–periodic extension σ(x) of the step function has two

6.1 Generalized Functions
233
jump discontinuities, of magnitudes +1 at even multiples of π and −1 at odd multiples;
see Figure 3.6. Therefore, its derivative
d σ
dx = δ(x) −δ(x −π)
is the diﬀerence of the 2π–periodic extension of the delta function at 0, with Fourier series
(6.36), minus the 2π–periodic extension of the delta function at π, with Fourier series
(6.46), which produces (6.47).
It is a remarkable, profound fact that Fourier analysis is entirely compatible with the
calculus of generalized functions, [68]. For instance, term-wise diﬀerentiation of the Fourier
series for a piecewise C1 function leads to the Fourier series for the diﬀerentiated function
that incorporates delta functions of the appropriate magnitude at each jump discontinuity.
This fact further reassures us that the rather mysterious construction of delta functions
and their generalizations is indeed the right way to extend calculus to functions that do
not possess derivatives in the ordinary sense.
Exercises
6.1.27. Determine the real and complex Fourier series for δ(x −ξ), where −π < ξ < π. What
periodic generalized function(s) do they represent?
6.1.28. Determine the Fourier sine series and the Fourier cosine series for δ(x −ξ), where
0 < ξ < π. Which periodic generalized functions do they represent?
♥6.1.29. Let n > 0 be a positive integer. (a) For integers 0 ≤j < n, ﬁnd the complex Fourier
series of the 2π–periodically extended delta functions δj(x) = δ(x −2j π/n). (b) Prove that
their Fourier coeﬃcients satisfy the periodicity condition ck = cl whenever k ≡l mod n.
(c) Conversely, given complex Fourier coeﬃcients that satisfy the periodicity condition
ck = cl whenever k ≡l mod n, prove that the corresponding Fourier series represents a lin-
ear combination of the preceding periodically extended delta functions δ0(x), . . . , δn−1(x).
Hint: Use Example B.22. (d) Prove that a complex Fourier series represents a 2π–periodic
function that is constant on the subintervals 2π j/n < x < 2π(j + 1)/n, for j ∈Z, if and
only if its Fourier coeﬃcients satisfy the conditions
k ck = l cl,
k ≡l ̸≡0 mod n,
ck = 0,
0 ̸= k ≡0 mod n.
♣6.1.30.(a) Find the complex Fourier series for the derivative of the delta function δ ′(x) by di-
rect evaluation of the coeﬃcient formulas. (b) Verify that your series can be obtained by
term-by-term diﬀerentiation of the series for δ(x). (c) Write a formula for the nth partial
sum of your series. (d) Use a computer graphics package to investigate the convergence of
the series.
6.1.31. What is the Fourier series for the generalized function g(x) = x δ(x)? Can you obtain
this result through multiplication of the individual Fourier series (3.37), (6.37)?
6.1.32. Apply the method of Exercise 3.2.59 to ﬁnd the complex Fourier series for the function
f(x) = δ(x) e i x. Which Fourier series do you get? Can you explain what is going on?
6.1.33. In Exercise 6.1.12 we established the identity δ(x) = 2 δ(2x). Does this hold on the
level of Fourier series? Can you explain why or why not?
6.1.34. How should one interpret the formula (6.38) for the periodic extension of the delta
function (a) as a limit? (b) as a linear functional?

234
6 Generalized Functions and Green’s Functions
6.1.35. Write down the complex Fourier series for ex. Diﬀerentiate term by term. Do you get
the same series? Explain your answer.
6.1.36. True or false: If you integrate the Fourier series for the delta function δ(x) term by
term, you obtain the Fourier series for the step function σ(x).
6.1.37. Find the Fourier series for the function δ(x) on the interval −1 ≤x ≤1. Which (gener-
alized) function does the Fourier series represent?
♦6.1.38.. Prove that cos nx ⇀0 (weakly) as n →∞on any bounded interval [a, b].
♦6.1.39. Prove that if un →u in norm, then un ⇀u weakly.
6.1.40. True or false: (a) If un →u uniformly on [a, b], then un ⇀u weakly.
(b) If un(x) →u(x) pointwise, then un ⇀u weakly.
6.1.41. Prove that the sequence fn(x) = cos2 nx converges weakly on [−π, π ]. What is the
limiting function?
6.1.42. Answer Exercise 6.1.41 when fn(x) = cos3 nx.
6.1.43. Discuss the weak convergence of the Fourier series for the derivative δ ′(x) of the delta
function.
6.2 Green’s Functions for
One–Dimensional Boundary Value Problems
We will now put the delta function to work by developing a general method for solving
inhomogeneous linear boundary value problems. The key idea, motivated by the linear
algebra technique outlined at the beginning of the previous section, is to ﬁrst solve the sys-
tem when subject to a unit delta function impulse, which produces the Green’s function.
We then apply linear superposition to write down the solution for a general forcing inho-
mogeneity. The Green’s function approach has wide applicability, but will be developed
here in the context of a few basic examples.
Example 6.9. The boundary value problem
−cu′′ = f(x),
u(0) = 0 = u(1),
(6.48)
models the longitudinal deformation u(x) of a homogeneous elastic bar of unit length and
constant stiﬀness c that is ﬁxed at both ends while subject to an external force f(x). The
associated Green’s function refers to the family of solutions
u(x) = Gξ(x) = G(x; ξ)
induced by unit-impulse forces concentrated at a single point 0 < ξ < 1:
−cu′′ = δ(x −ξ),
u(0) = 0 = u(1).
(6.49)
The solution to the diﬀerential equation can be straightforwardly obtained by direct inte-
gration. First, by (6.23),
u′(x) = −σ(x −ξ)
c
+ a,

6.2 Green’s Functions for One–Dimensional Boundary Value Problems
235
ξ
1
ξ (1 −ξ)/c
Figure 6.8.
Green’s function for a bar with ﬁxed ends.
where a is a constant of integration. A second integration leads to
u(x) = −ρ(x −ξ)
c
+ a x + b,
(6.50)
where ρ is the ramp function (6.25). The integration constants a, b are ﬁxed by the bound-
ary conditions; since 0 < ξ < 1, we have
u(0) = b = 0,
u(1) = −1 −ξ
c
+ a + b = 0,
and so
a = 1 −ξ
c
,
b = 0.
We deduce that the Green’s function for the problem is
G(x; ξ) = (1 −ξ)x −ρ(x −ξ)
c
=
 (1 −ξ)x/c,
x ≤ξ,
ξ (1 −x)/c,
x ≥ξ.
(6.51)
As sketched in Figure 6.8, for each ﬁxed ξ, the function Gξ(x) = G(x; ξ) depends continu-
ously on x; its graph consists of two connected straight line segments, with a corner at the
point of application of the unit impulse force.
Once we have determined the Green’s function, we are able to solve the general in-
homogeneous boundary value problem (6.48) by linear superposition. We ﬁrst express the
forcing function f(x) as a linear combination of impulses concentrated at various points
along the bar. Since there is a continuum of possible positions 0 < ξ < 1 at which impulse
forces may be applied, we will use an integral to sum them, thereby writing the external
force as
f(x) =
 1
0
δ(x −ξ) f(ξ) dξ.
(6.52)
We can interpret (6.52) as the (continuous) superposition of an inﬁnite collection of im-
pulses, namely f(ξ) δ(x −ξ), of magnitude f(ξ) and concentrated at position ξ.
The Superposition Principle states that linear combinations of inhomogeneities pro-
duce the selfsame linear combinations of solutions. Again, we adapt this principle to the
continuum by replacing the sums by integrals. Thus, the solution to the boundary value
problem will be the linear superposition
u(x) =
 1
0
G(x; ξ) f(ξ) dξ
(6.53)
of the Green’s function solutions to the individual unit-impulse problems.

236
6 Generalized Functions and Green’s Functions
For the particular boundary value problem (6.48), we use the formula (6.51) for the
Green’s function. Breaking the resulting integral (6.53) into two parts, over the subintervals
0 ≤ξ ≤x and x ≤ξ ≤1, we arrive at the explicit solution formula
u(x) = 1
c
 x
0
(1 −x)ξ f(ξ) dξ + 1
c
 1
x
x(1 −ξ)f(ξ) dξ.
(6.54)
For example, under a constant unit force f, (6.54) yields the solution
u(x) = f
c
 x
0
(1 −x) ξ dξ + f
c
 1
x
x(1 −ξ) dξ = f
2c (1−x)x2+ f
2c x(1−x)2 = f
2c (x−x2).
Let us, ﬁnally, convince ourselves that the superposition formula (6.54) indeed gives the
correct answer. First,
c du
dx = (1 −x)xf(x) +
 x
0

−ξ f(ξ)

dξ −x(1 −x)f(x) +
 1
x
(1 −ξ)f(ξ) dξ
= −
 1
0
ξ f(ξ) dξ +
 1
x
f(ξ) dξ.
Diﬀerentiating again with respect to x, we see that the ﬁrst term is constant, and so
−c d2u
dx2 = f(x), as claimed.
Remark: In computing the derivatives of u, we made use of the calculus formula
d
dx
 β(x)
α(x)
F(x, ξ) dξ = F(x, β(x)) dβ
dx −F(x, α(x)) dα
dx +
 β(x)
α(x)
∂F
∂x (x, ξ) dξ
(6.55)
for the derivative of an integral with variable limits — which is a straightforward conse-
quence of the Fundamental Theorem of Calculus and the chain rule, [8, 108]. As always,
one must exercise due care when interchanging diﬀerentiation and integration.
We note the following basic properties, which serve to uniquely characterize the Green’s
function. First, since the delta forcing vanishes except at the point x = ξ, the Green’s
function satisﬁes the homogeneous diﬀerential equation†
−c ∂2G
∂x2 (x; ξ) = 0
for all
x ̸= ξ.
(6.56)
Second, by construction, it must satisfy the boundary conditions
G(0; ξ) = 0 = G(1; ξ).
Third, for each ﬁxed ξ, G(x; ξ) is a continuous function of x, but its derivative ∂G/∂x
has a jump discontinuity of magnitude −1/c at the impulse point x = ξ. As a result, the
second derivative ∂2G/∂x2 has a delta function discontinuity there, and hence solves the
original impulse boundary value problem (6.49).
Finally, we cannot help but notice that the Green’s function (6.51) is a symmetric
function of its two arguments: G(x; ξ) = G(ξ; x). Symmetry has the interesting physi-
cal consequence that the displacement of the bar at position x due to an impulse force
†
Since G(x; ξ) is a function of two variables, we switch to partial derivative notation to indicate
its derivatives.

6.2
237
concentrated at position ξ is exactly the same as the displacement of the bar at ξ due
to an impulse of the same magnitude being applied at x. This turns out to be a rather
general, although perhaps unanticipated, phenomenon. Symmetry of the Green’s function
is a consequence of the underlying symmetry, or, more accurately, “self-adjointness”, of
the boundary value problem, a topic that will be developed in detail in Section 9.2.
Example 6.10. Let ω2 > 0 be a ﬁxed positive constant. Let us solve the inhomoge-
neous boundary value problem
−u′′ + ω2 u = f(x),
u(0) = u(1) = 0,
(6.57)
by constructing its Green’s function. To this end, we ﬁrst analyze the eﬀect of a delta
function inhomogeneity
−u′′ + ω2 u = δ(x −ξ),
u(0) = u(1) = 0.
(6.58)
Rather than try to integrate this diﬀerential equation directly, let us appeal to the deﬁning
properties of the Green’s function. The general solution to the homogeneous equation is a
linear combination of the two basic exponentials eω x and e−ω x, or better, the hyperbolic
functions
cosh ωx = eω x + e−ω x
2
,
sinh ωx = eω x −e−ω x
2
.
(6.59)
The solutions satisfying the ﬁrst boundary condition are multiples of sinh ωx, while those
satisfying the second boundary condition are multiples of sinh ω(1 −x). Therefore, the
solution to (6.58) has the form
G(x; ξ) =
 a sinh ωx,
x ≤ξ,
b sinh ω(1 −x),
x ≥ξ.
Continuity of G(x; ξ) at x = ξ requires
a sinh ωξ = b sinh ω(1 −ξ).
(6.60)
At x = ξ, the derivative ∂G/∂x must have a jump discontinuity of magnitude −1 in order
that the second derivative term in (6.58) match the delta function. (The ω2 u term clearly
cannot produce the required singularity.) Since
∂G
∂x (x; ξ) =

a ω cosh ωx,
x < ξ,
−b ω cosh ω(1 −x),
x > ξ,
the jump condition requires
a ω cosh ωξ −1 = −b ω cosh ω(1 −ξ).
(6.61)
Multiplying (6.60) by ω cosh ω(1 −ξ) and (6.61) by sinh ω(1 −ξ), and then adding the
results together, we obtain
sinh ω(1 −ξ) = a ω

sinh ωξ cosh ω(1 −ξ) + cosh ωξ sinh ω(1 −ξ)

= a ω sinh ω, (6.62)
where we made use of the addition formula for the hyperbolic sine:
sinh(α + β) = sinh α cosh β + cosh α sinh β,
(6.63)
Green’s Functions for One–Dimensional Boundary Value Problems

238
6 Generalized Functions and Green’s Functions
ξ
1
sinh ωξ sinh ω(1 −ξ)
ω sinh ω
Figure 6.9.
Green’s function for the boundary value problem (6.57).
which you are asked to prove in Exercise 6.2.13. Therefore, solving (6.61–62) for
a = sinh ω(1 −ξ)
ω sinh ω
,
b = sinh ωξ
ω sinh ω ,
produces the explicit formula
G(x; ξ) =
⎧
⎪
⎨
⎪
⎩
sinh ωx sinh ω(1 −ξ)
ω sinh ω
,
x ≤ξ,
sinh ω(1 −x) sinh ωξ
ω sinh ω
,
x ≥ξ.
(6.64)
A representative graph appears in Figure 6.9. As before, a corner, indicating a discontinuity
in the ﬁrst derivative, appears at the point x = ξ where the impulse force is applied.
Moreover, as in the previous example, G(x; ξ) = G(ξ; x) is a symmetric function.
The general solution to the inhomogeneous boundary value problem (6.57) is then
given by the superposition formula (6.53); explicitly,
u(x) =
 1
0
G(x; ξ)f(ξ) dξ
=
 x
0
sinh ω(1 −x) sinh ωξ
ω sinh ω
f(ξ) dξ +
 1
x
sinh ωx sinh ω(1 −ξ)
ω sinh ω
f(ξ) dξ.
(6.65)
For example, under a constant unit force f(x) ≡1, the solution is
u(x) =
 x
0
sinh ω(1 −x) sinh ωy
ω sinh ω
dξ +
 1
x
sinh ωx sinh ω(1 −ξ)
ω sinh ω
dξ
= sinh ω(1 −x)

cosh ωx −1

ω2 sinh ω
+ sinh ωx

cosh ω(1 −x) −1

ω2 sinh ω
=
1
ω2 −sinh ωx + sinh ω(1 −x)
ω2 sinh ω
.
For comparative purposes, the reader may wish to rederive this particular solution by a
direct calculation, without appealing to the Green’s function.
Example 6.11. Finally, consider the Neumann boundary value problem
−cu′′ = f(x),
u′(0) = 0 = u′(1),
(6.66)

6.2 Green’s Functions for One–Dimensional Boundary Value Problems
239
modeling the equilibrium deformation of a homogeneous bar with two free ends when
subject to an external force f(x). The Green’s function should satisfy the particular case
−cu′′ = δ(x −ξ),
u′(0) = 0 = u′(1),
when the forcing function is a concentrated impulse.
As in Example 6.9, the general
solution to the latter diﬀerential equation is
u(x) = −ρ(x −ξ)
c
+ ax + b,
where a, b are integration constants, and ρ is the ramp function (6.25).
However, the
Neumann boundary conditions require that
u′(0) = a = 0,
u′(1) = −1
c + a = 0,
which cannot both be satisﬁed. We conclude that there is no Green’s function in this case.
The diﬃculty is that the Neumann boundary value problem (6.66) does not have
a unique solution, and hence cannot admit a Green’s function solution formula (6.53).
Indeed, integrating twice, we ﬁnd that the general solution to the diﬀerential equation is
u(x) = ax + b −1
c
 x
0
 y
0
f(z) dz dy,
where a, b are integration constants. Since
u′(x) = a −1
c
 x
0
f(z) dz,
the boundary conditions require that
u′(0) = a = 0,
u′(1) = a −1
c
 1
0
f(z) dz = 0.
These equations are compatible if and only if
 1
0
f(z) dz = 0.
(6.67)
Thus, the Neumann boundary value problem admits a solution if and only if there is no
net force on the bar. Indeed, physically, if (6.67) does not hold, then, because its ends are
not attached to any support, the bar cannot stay in equilibrium, but will move oﬀin the
direction of the net force. On the other hand, if (6.67) holds, then the solution
u(x) = b −1
c
 x
0
 y
0
f(z) dz dy
is not unique, since b is not constrained by the boundary conditions, and so can assume
any constant value. Physically, this means that any equilibrium conﬁguration of the bar
can be freely translated to assume another valid equilibrium.
Remark: The constraint (6.67) is a manifestation of the Fredholm Alternative, to be
developed in detail in Section 9.1.

240
6 Generalized Functions and Green’s Functions
Let us summarize the fundamental properties that serve to completely characterize
the Green’s function of boundary value problems governed by second-order linear ordinary
diﬀerential equations
p(x) d2u
dx2 + q(x) du
dx + r(x) u(x) = f(x),
(6.68)
combined with a pair of homogeneous boundary conditions at the ends of the interval
[a, b]. We assume that the coeﬃcient functions are continuous, p, q, r, f ∈C0[a, b], and
that p(x) ̸= 0 for all a ≤x ≤b.
Basic Properties of the Green’s Function G(x; ξ)
(i) Solves the homogeneous diﬀerential equation at all points x ̸= ξ.
(ii) Satisﬁes the homogeneous boundary conditions.
(iii) Is a continuous function of its arguments.
(iv) For each ﬁxed ξ, its derivative ∂G/∂x is piecewise C1, with a single jump discontinuity
of magnitude 1/p(ξ) at the impulse point x = ξ.
With the Green’s function in hand, we deduce that the solution to the general bound-
ary value problem (6.68) subject to the appropriate homogeneous boundary conditions is
expressed by the Green’s Function Superposition Formula
u(x) =
 b
a
G(x; ξ) f(ξ) dξ.
(6.69)
The symmetry of the Green’s function is more subtle, for it relies on the self-adjointness of
the boundary value problem, an issue to be addressed in detail in Chapter 9. In the present
situation, self-adjointness requires that q(x) = p′(x), in which case G(ξ; x) = G(x; ξ) will
be symmetric in its arguments.
Finally, as we saw in Example 6.11, not every such boundary value problem admits
a solution, and one expects to ﬁnd a Green’s function only in cases in which the solution
exists and is unique.
Theorem 6.12. The following are equivalent:
• The only solution to the homogeneous boundary value problem is the zero function.
• The inhomogeneous boundary value problem has a unique solution for every choice of
forcing function.
• The boundary value problem admits a Green’s function.
Exercises
6.2.1. Let c > 0. Find the Green’s function for the boundary value problem −cu′′ = f(x),
u(0) = 0, u′(1) = 0, which models the displacement of a uniform bar of unit length with
one ﬁxed and one free end under an external force. Then use superposition to write down
a formula for the solution. Verify that your integral formula is correct by direct diﬀerentia-
tion and substitution into the diﬀerential equation and boundary conditions.

241
6.2.2. A uniform bar of length ℓ= 4 has constant stiﬀness c = 2. Find the Green’s function for
the case that (a) both ends are ﬁxed; (b) one end is ﬁxed and the other is free. (c) Why is
there no Green’s function when both ends are free?
6.2.3. A point 2 cm along a 10 cm bar experiences a displacement of 1 mm under a concen-
trated force of 2 newtons applied at the midpoint of the bar. How far does the midpoint
deﬂect when a concentrated force of 1 newton is applied at the point 2 cm along the bar?
♥6.2.4. The boundary value problem −d
dx

c(x) du
dx

= f(x), u(0) = u(1) = 0, models the
displacement u(x) of a nonuniform elastic bar with stiﬀness c(x) =
1
1 + x2 for 0 ≤x ≤1.
(a) Find the displacement when the bar is subjected to a constant external force, f ≡1.
(b) Find the Green’s function for the boundary value problem.
(c) Use the resulting su-
perposition formula to check your solution to part (a).
(d) Which point 0 < ξ < 1 on the
bar is the “weakest”, i.e., the bar experiences the largest displacement under a unit impulse
concentrated at that point?
6.2.5. Answer Exercise 6.2.4 when c(x) = 1 + x.
♥6.2.6. Consider the boundary value problem −u′′ = f(x), u(0) = 0, u(1) = 2u′(1).
(a) Find the Green’s function.
(b) Which of the fundamental properties does your Green’s
function satisfy?
(c) Write down an explicit integral formula for the solution to the bound-
ary value problem, and prove its validity by a direct computation.
(d) Explain why the
related boundary value problem −u′′ = f, u(0) = 0, u(1) = u′(1), does not have a Green’s
function.
♥6.2.7. For n a positive integer, set fn(x) =

1
2 n,
| x −ξ | < 1
n ,
0,
otherwise.
(a) Find the solution un(x) to the boundary value problem −u′′ = fn(x), u(0) = u(1) = 0,
assuming 0 < ξ −1
n < ξ + 1
n < 1.
(b) Prove that
lim
n →∞un(x) = G(x; ξ) converges to the
Green’s function (6.51). Why should this be the case? (c) Reconﬁrm the result in part (b)
by graphing u5(x), u15(x), u25(x), along with G(x; ξ) when ξ = .3.
6.2.8. Solve the boundary value problem −4u′′ + 9u = 0, u(0) = 0, u(2) = 1. Is your solution
unique?
6.2.9. True or false: The Neumann boundary value problem −u′′ + u = 1, u′(0) = u′(1) = 0,
has a unique solution.
6.2.10. Use the Green’s function (6.64) to solve the boundary value problem (6.57) when the
forcing function is f(x) =
⎧
⎨
⎩
1,
0 ≤x < 1
2,
−1,
1
2 < x ≤1.
6.2.11. Let ω > 0. (a) Find the Green’s function for the mixed boundary value problem
−u′′ + ω2 u = f(x), u(0) = 0, u′(1) = 0.
(b) Use your Green’s function to ﬁnd the solution when f(x) =
⎧
⎨
⎩
1,
0 ≤x < 1
2,
−1,
1
2 < x ≤1.
6.2.12. Suppose ω > 0. Does the Neumann boundary value problem −u′′ + ω2 u = f(x),
u′(0) = u′(1) = 0 admit a Green’s function? If not, explain why not. If so, ﬁnd it, and then
write down an integral formula for the solution of the boundary value problem.
♦6.2.13.(a) Prove the addition formula (6.63) for the hyperbolic sine function.
(b) Find the corresponding addition formula for the hyperbolic cosine.
♦6.2.14. Prove the diﬀerentiation formula (6.55).
6.2 Green’s Functions for One–Dimensional Boundary Value Problems

242
6 Generalized Functions and Green’s Functions
6.3 Green’s Functions for the Planar Poisson Equation
Now we develop the Green’s function approach to solving boundary value problems in-
volving the two-dimensional Poisson equation (4.84). As before, the Green’s function is
characterized as the solution to the homogeneous boundary value problem in which the
inhomogeneity is a concentrated unit impulse — a delta function. The solution to the
general forced boundary value problem is then obtained via linear superposition, that is,
as a convolution integral with the Green’s function.
However, before proceeding, we need to quickly review some basic facts concerning
vector calculus in the plane. The student may wish to consult a standard multivariable
calculus text, e.g., [8, 108], for additional details.
Calculus in the Plane
Let x = (x, y) denote the usual Cartesian coordinates on R2. The term scalar ﬁeld is
synonymous with a real-valued function u(x, y), deﬁned on a domain Ω ⊂R2. A vector-
valued function
v(x) = v(x, y) =

v1(x, y)
v2(x, y)

(6.70)
is known as a (planar) vector ﬁeld. A vector ﬁeld assigns a vector v(x, y) ∈R2 to each point
(x, y) ∈Ω in its domain of deﬁnition, and hence deﬁnes a function v: Ω →R2. Physical
examples include velocity vector ﬁelds of ﬂuid ﬂows, heat ﬂux ﬁelds in thermodynamics,
and gravitational and electrostatic force ﬁelds.
The gradient operator ∇maps a scalar ﬁeld u(x, y) to the vector ﬁeld
∇u =

∂u/∂x
∂u/∂y

.
(6.71)
The scalar ﬁeld u is often referred to as a potential function for its gradient vector ﬁeld
v = ∇u. On a connected domain Ω, the potential, when it exists, is uniquely determined
up to addition of a constant.
The divergence of the planar vector ﬁeld v = ( v1, v2 )T is the scalar ﬁeld
∇· v = div v = ∂v1
∂x + ∂v2
∂y .
(6.72)
Its curl is deﬁned as
∇× v = curl v = ∂v2
∂x −∂v1
∂y .
(6.73)
Notice that the curl of a planar vector ﬁeld is a scalar ﬁeld. (In contrast, in three dimen-
sions, the curl of a vector ﬁeld is another vector ﬁeld.) Given a smooth potential u ∈C2,
the curl of its gradient vector ﬁeld automatically vanishes:
∇× ∇u = ∂
∂x
∂u
∂y −∂
∂y
∂u
∂x ≡0,
by the equality of mixed partials. Thus, a necessary condition for a vector ﬁeld v to admit
a potential is that it be irrotational, meaning ∇× v = 0; this condition is suﬃcient if

6.3 Green’s Functions for the Planar Poisson Equation
243
Figure 6.10.
Orientation of the boundary of a planar domain.
the underlying domain Ω is simply connected, i.e., has no holes. On the other hand, the
divergence of a gradient vector ﬁeld coincides with the Laplacian of the potential function:
∇· ∇u = Δu = ∂2u
∂x2 + ∂2u
∂y2 .
(6.74)
A vector ﬁeld is incompressible if it has zero divergence: ∇· v = 0; for the velocity vector
ﬁeld of a steady-state ﬂuid ﬂow, incompressibility means that the ﬂuid does not change
volume.
(Water is, for all practical purposes, an incompressible ﬂuid.)
Therefore, an
irrotational vector ﬁeld with potential u is also incompressible if and only if the potential
solves the Laplace equation Δu = 0.
Remark: Because of formula (6.74), the Laplacian operator is also sometimes written
as Δ = ∇2. The factorization of the Laplacian into the product of the divergence and the
gradient operators is, in fact, of great importance, and underlies its “self-adjointness”, a
fundamental property whose ramiﬁcations will be explored in depth in Chapter 9.
Let Ω ⊂R2 be a bounded domain whose boundary ∂Ω consists of one or more piecewise
smooth closed curves. We orient the boundary so that the domain is always on one’s left
as one goes around the boundary curve(s). Figure 6.10 sketches a domain with two holes;
its three boundary curves are oriented according to the directions of the arrows. Note that
the outer boundary curve is traversed in a counterclockwise direction, while the two inner
boundary curves are oriented clockwise.
Green’s Theorem, ﬁrst formulated by George Green to use in his seminal study of
partial diﬀerential equations and potential theory, relates certain double integrals over a
domain to line integrals around its boundary. It should be viewed as the extension of the
Fundamental Theorem of Calculus to double integrals.
Theorem 6.13.
Let v(x) be a smooth† vector ﬁeld deﬁned on a bounded domain
Ω ⊂R2. Then the line integral of v around the boundary ∂Ω equals the double integral of
its curl over the domain:
 
Ω
∇× v dx dy =
&
∂Ω
v · dx,
(6.75)
†
To be precise, we require v to be continuously diﬀerentiable within the domain, and contin-
uous up to the boundary, so v ∈C0(Ω) ∩C1(Ω), where Ω = Ω ∪∂Ω denotes the closure of the
domain Ω.

244
6 Generalized Functions and Green’s Functions
or, in full detail,
 
Ω
∂v2
∂x −∂v1
∂y

dx dy =
&
∂Ω
v1 dx + v2 dy .
(6.76)
Example 6.14.
Let us apply Green’s Theorem 6.13 to the particular vector ﬁeld
v = ( y, 0 )T . Since ∇× v ≡−1, we obtain
&
∂Ω
y dx =
 
Ω
(−1) dx dy = −area Ω.
(6.77)
This means that we can determine the area of a planar domain by computing the negative
of the indicated line integral around its boundary.
For later purposes, we rewrite the basic Green identity (6.75) in an equivalent “diver-
gence form”. Given a planar vector ﬁeld v = ( v1, v2 )T , let
v⊥=

−v2
v1

(6.78)
denote the “perpendicular” vector ﬁeld. We note that its curl
∇× v⊥= ∂v1
∂x + ∂v2
∂y = ∇· v
(6.79)
coincides with the divergence of the original vector ﬁeld.
When we replace v in Green’s identity (6.75) by v⊥, the result is
 
Ω
∇· v dx dy =
 
Ω
∇× v⊥dx dy =
&
∂Ω
v⊥· dx =
&
∂Ω
v · n ds,
where n denotes the unit outwards normal to the boundary of our domain, while ds denotes
the arc-length element along the boundary curve. This yields the divergence form of Green’s
Theorem:
 
Ω
∇· v dx dy =
&
∂Ω
v · n ds.
(6.80)
Physically, if v represents the velocity vector ﬁeld of a steady-state ﬂuid ﬂow, then the
line integral in (6.80) represents the net ﬂuid ﬂux out of the region Ω. As a result, the
divergence ∇· v represents the local change in area of the ﬂuid at each point, which serves
to justify our earlier statement on incompressibility.
Consider next the product vector ﬁeld u v obtained by multiplying a vector ﬁeld v by
a scalar ﬁeld u. An elementary computation proves that its divergence is
∇· (u v) = u ∇· v + ∇u · v.
(6.81)
Replacing v by u v in the divergence formula (6.80), we deduce what is usually referred to
as Green’s formula
 
Ω

u ∇· v + ∇u · v

dx dy =
&
∂Ω
u (v · n) ds,
(6.82)
which is valid for arbitrary bounded domains Ω, and arbitrary C1 scalar and vector ﬁelds
deﬁned thereon. Rearranging the terms produces
 
Ω
∇u · v dx dy =
&
∂Ω
u (v · n) ds −
 
Ω
u ∇· v dx dy.
(6.83)

6.3 Green’s Functions for the Planar Poisson Equation
245
We will view this identity as an integration by parts formula for double integrals. Indeed,
comparing with the one-dimensional integration by parts formula
 b
a
u′(x) v(x) dx = u(x) v(x)

b
x=a −
 b
a
u(x) v′(x) dx,
(6.84)
we observe that the single integrals have become double integrals; the derivatives are vector
derivatives (gradient and divergence), while the boundary contributions at the endpoints
of the interval are replaced by a line integral around the entire boundary of the two-
dimensional domain.
A useful special case of (6.82) is that in which v = ∇v is the gradient of a scalar ﬁeld
v. Then, in view of (6.74), Green’s formula (6.82) becomes
 
Ω

u Δv + ∇u · ∇v

dx dy =
&
∂Ω
u ∂v
∂n ds,
(6.85)
where ∂v/∂n = ∇v · n is the normal derivative of the scalar ﬁeld v on the boundary of the
domain. In particular, setting v = u, we deduce
 
Ω

u Δu + ∥∇u ∥2 
dx dy =
&
∂Ω
u ∂u
∂n ds.
(6.86)
As an application, we establish a basic uniqueness theorem for solutions to the boundary
value problems for the Poisson equation:
Theorem 6.15. Suppose u and u both satisfy the same inhomogeneous Dirichlet or
mixed boundary value problem for the Poisson equation on a connected, bounded domain
Ω. Then u = u. On the other hand, if u and u satisfy the same Neumann boundary value
problem, then u = u + c for some constant c.
Proof : Since, by assumption, −Δu = f = −Δu, the diﬀerence v = u −u satisﬁes
the Laplace equation Δv = 0 in Ω, and satisﬁes the homogeneous boundary conditions.
Therefore, applying (6.86) to v, we ﬁnd
 
Ω
∥∇v ∥2 dx dy =
&
∂Ω
v ∂v
∂n ds = 0,
since, at every point on the boundary, either v = 0 or ∂v/∂n = 0. Since the integrand is
continuous and everywhere nonnegative, we immediately conclude that ∥∇v ∥2 = 0, and
hence ∇v = 0 throughout Ω. On a connected domain, the only functions annihilated by
the gradient operator are the constants:
Lemma 6.16.
If v(x, y) is a C1 function deﬁned on a connected domain Ω ⊂R2,
then ∇v ≡0 if and only if v(x, y) ≡c is a constant.
Proof : Let a, b be any two points in Ω. Then, by connectivity, we can ﬁnd a curve C
connecting them. The Fundamental Theorem for line integrals, [8, 108], states that

C
∇v · dx = v(b) −v(a).
Thus, if ∇v ≡0, then v(b) = v(a) for all a, b ∈Ω, which implies that v must be con-
stant.
Q.E.D.

246
6 Generalized Functions and Green’s Functions
Returning to our proof, we conclude that u = u + v = u + c, which proves the result
in the Neumann case. In the Dirichlet or mixed problems, there is at least one point on
the boundary where v = 0, and hence the only possible constant is v = c = 0, proving that
u = u.
Q.E.D.
Thus, the Dirichlet and mixed boundary value problems admit at most one solution,
while the Neumann boundary value problem has either no solutions or inﬁnitely many
solutions. Proof of existence of solutions is more challenging, and will be left to a more
advanced text, e.g., [35, 44, 61, 70].
If we subtract from formula (6.85) the formula
 
Ω

v Δu + ∇u · ∇v

dx dy =
&
∂Ω
v ∂u
∂n ds,
(6.87)
obtained by interchanging u and v, we obtain the identity
 
Ω

u Δv −v Δu

dx dy =
&
∂Ω

u ∂v
∂n −v ∂u
∂n

ds,
(6.88)
which will play a major role in our analysis of the Poisson equation. Setting v = 1 in (6.87)
yields
 
Ω
Δu dx dy =
&
∂Ω
∂u
∂n ds.
(6.89)
Suppose u solves the Neumann boundary value problem
−Δu = f,
in
Ω
∂u
∂n = h
on
∂Ω.
Then (6.89) requires that
 
Ω
f dx dy +
&
∂Ω
h ds = 0,
(6.90)
which thus forms a necessary condition for the existence of a solution u to the inhomo-
geneous Neumann boundary value problem.
Physically, if u represents the equilibrium
temperature of a plate, then the integrals in (6.89) measure the net gain or loss in heat en-
ergy due to, respectively, the external heat source and the heat ﬂux through the boundary.
Equation (6.90) is telling us that, for the plate to remain in thermal equilibrium, there can
be no net change in its total heat energy.
The Two–Dimensional Delta Function
Now let us return to the business at hand — solving the Poisson equation on a bounded
domain Ω ⊂R2. We will subject the solution to either homogeneous Dirichlet boundary
conditions or homogeneous mixed boundary conditions. (As we just noted, the Neumann
boundary value problem does not admit a unique solution, and hence does not possess a
Green’s function.) The Green’s function for the boundary value problem arises when the
forcing function is a unit impulse concentrated at a single point in the domain.
Thus, our ﬁrst task is to establish the proper form for a unit impulse in our two-
dimensional context. The delta function concentrated at a point ξ = (ξ, η) ∈R2 is denoted
by
δ(ξ,η)(x, y) = δξ(x) = δ(x −ξ) = δ(x −ξ, y −η),
(6.91)

6.3 Green’s Functions for the Planar Poisson Equation
247
Figure 6.11.
Gaussian functions converging to the delta function.
and is designed so that
δξ(x) = 0,
x ̸= ξ,
 
Ω
δ(ξ,η)(x, y) dx dy = 1,
ξ ∈Ω.
(6.92)
In particular, δ(x, y) = δ0(x, y) represents the delta function at the origin.
As in the
one-dimensional version, there is no ordinary function that satisﬁes both criteria; rather,
δ(x, y) is to be viewed as the limit of a sequence of more and more highly concentrated
functions gn(x, y), with
lim
n →∞gn(x, y) = 0,
for
(x, y) ̸= (0, 0),
while
 
R2 gn(x, y) dx dy = 1.
A good example of a suitable sequence is provided by the radial Gaussian functions
gn(x, y) = n
π e−n (x2+y2) .
(6.93)
As plotted in Figure 6.11, as n →∞, the Gaussian proﬁles become more and more con-
centrated near the origin, while maintaining a unit volume underneath their graphs. The
fact that their integral over R2 equals 1 is a consequence of (2.99).
Alternatively, one can assign the delta function a dual interpretation as the linear
functional
L(ξ,η)[u] = Lξ[u] = u(ξ) = u(ξ, η),
(6.94)
which assigns to each continuous function u ∈C0(Ω) its value at the point ξ = (ξ, η) ∈Ω.
Then, using the L2 inner product
⟨u , v ⟩=
 
Ω
u(x, y) v(x, y) dx dy
(6.95)
between scalar ﬁelds u, v ∈C0(Ω), we formally identify the linear functional L(ξ,η) with
the delta “function” by the integral formula
⟨δ(ξ,η) , u ⟩=
 
Ω
δ(ξ,η)(x, y) u(x, y)dx dy =
 u(ξ, η),
(ξ, η) ∈Ω,
0,
(ξ, η) ∈R2 \ Ω,
(6.96)

248
6 Generalized Functions and Green’s Functions
for any u ∈C0(Ω). As in the one-dimensional version, we will avoid deﬁning the integral
when the delta function is concentrated at a boundary point of the domain.
Since double integrals can be evaluated as repeated one-dimensional integrals, we can
conveniently view
δ(ξ,η)(x, y) = δξ(x) δη(y) = δ(x −ξ) δ(y −η)
(6.97)
as the product† of a pair of one-dimensional delta functions. Indeed, if the impulse point
(ξ, η) ∈R =
-
a < x < b, c < y < d
.
⊂Ω
is contained in a rectangle that lies within the domain, then
 
Ω
δ(ξ,η)(x, y) u(x, y)dx dy =
 
R
δ(ξ,η)(x, y) u(x, y) dx dy
=
 b
a
  d
c
δ(x −ξ) δ(y −η) u(x, y) dy

dx =
 b
a
δ(x −ξ) u(x, η) dx = u(ξ, η).
The Green’s Function
As in the one-dimensional context, the Green’s function is deﬁned as the solution to the
inhomogeneous diﬀerential equation when subject to a concentrated unit delta impulse at
a prescribed point ξ = (ξ, η) ∈Ω inside the domain. In the current situation, the Poisson
equation takes the form
−Δu = δξ,
or, explicitly,
−∂2u
∂x2 −∂2u
∂y2 = δ(x −ξ) δ(y −η).
(6.98)
The function u(x, y) is also subject to some homogeneous boundary conditions, e.g., the
Dirichlet conditions u = 0 on ∂Ω. The resulting solution is called the Green’s function for
the boundary value problem, and written
Gξ(x) = G(x; ξ) = G(x, y; ξ, η).
(6.99)
Once we know the Green’s function, the solution to the general Poisson boundary
value problem
−Δu = f
in
Ω,
u = 0
on
∂Ω
(6.100)
is reconstructed as follows. We regard the forcing function
f(x, y) =
 
Ω
δ(x −ξ) δ(y −η)f(ξ, η) dξ dη
as a superposition of delta impulses, whose strength equals the value of f at the impulse
point. Linearity implies that the solution to the boundary value problem is the correspond-
ing superposition of Green’s function responses to each of the constituent impulses. The
net result is the fundamental superposition formula
u(x, y) =
 
Ω
G(x, y; ξ, η) f(ξ, η) dξ dη
(6.101)
†
This is an exception to our earlier injunction not to multiply delta functions. Multiplication
is allowed when they depend on diﬀerent variables.

6.3 Green’s Functions for the Planar Poisson Equation
249
for the solution to the boundary value problem. Indeed,
−Δu(x, y) =
 
Ω
−ΔG(x, y; ξ, η)f(ξ, η) dξ dη
=
 
Ω
δ(x −ξ, y −η) f(ξ, η) dξ dη = f(x, y),
while the fact that G(x, y; ξ, η) = 0 for all (x, y) ∈∂Ω implies that u(x, y) = 0 on the
boundary.
The Green’s function inevitably turns out to be symmetric under interchange of its
arguments:
G(ξ, η; x, y) = G(x, y; ξ, η).
(6.102)
As in the one-dimensional case, symmetry is a consequence of the self-adjointness of the
boundary value problem, and will be explained in full in Chapter 9. Symmetry has the
following intriguing physical interpretation: Let x, ξ ∈Ω be any two points in the domain.
We apply a concentrated unit force to the membrane at the ﬁrst point and measure its
deﬂection at the second; the result is exactly the same as if we applied the impulse at
the second point and measured the deﬂection at the ﬁrst. (Deﬂections at other points
in the domain will typically have no obvious relation with one another.)
Similarly, in
electrostatics, the solution u(x, y) is interpreted as the electrostatic potential for a system
of charges in equilibrium. A delta function corresponds to a point charge, e.g., an electron.
The symmetry property says that the electrostatic potential at x due to a point charge
placed at position ξ is exactly the same as the potential at ξ due to a point charge at x.
The reader may wish to meditate on the physical plausibility of these striking facts.
Unfortunately, most Green’s functions cannot be written down in closed form. One
important exception occurs when the domain is the entire plane: Ω = R2. The solution
to the Poisson equation (6.98) is the free-space Green’s function G0(x, y; ξ, η) = G0(x; ξ),
which measures the eﬀect of a unit impulse, concentrated at ξ, throughout two-dimensional
space, e.g., the gravitational potential due to a point mass or the electrostatic potential
due to a point charge. To motivate the construction, let us appeal to physical intuition.
First, since the concentrated impulse is zero when x ̸= ξ, the function must solve the
homogeneous Laplace equation
−ΔG0 = 0
for all
x ̸= ξ.
(6.103)
Second, since the Poisson equation is modeling a homogeneous, uniform medium, in the
absence of boundary conditions the eﬀect of a unit impulse should depend only on the
distance from its source. Therefore, we expect G0 to be a function of the radial variable
alone:
G0(x, y; ξ, η) = v(r),
where
r = ∥x −ξ ∥=

(x −ξ)2 + (y −η)2 .
According to (4.113), the only radially symmetric solutions to the Laplace equation are
v(r) = a + b log r,
(6.104)
where a and b are constants.
The constant term a has zero derivative, and so cannot
contribute to the delta function singularity. Therefore, we expect the required solution to
be a multiple of the logarithmic term. To determine the multiple, consider a closed disk of
radius ε > 0 centered at ξ,
Dε =
-
0 ≤r ≤ε
.
=
-
∥x −ξ ∥≤ε
.
,

250
6 Generalized Functions and Green’s Functions
with circular boundary
Cε = ∂Dε = {r = ∥x −ξ ∥= ε} = { ( ξ + ε cosθ, η + ε sin θ ) | −π ≤θ ≤π } .
Then, by (6.74) and the divergence form (6.80) of Green’s Theorem,
1 =
 
Dε
δ(x, y) dx dy = −b
 
Dε
Δ(log r) dx dy = −b
 
Dε
∇· ∇(log r) dx dy
= −b
&
Cε
∂(log r)
∂n
ds = −b
&
Cε
∂(log r)
∂r
ds = −b
&
Cε
1
r ds = −b
 π
−π
dθ = −2π b,
(6.105)
and hence b = −1/(2π). We conclude that the free-space Green’s function should have the
logarithmic form
G0(x, y; ξ, η) = −1
2π log r = −1
2π log ∥x −ξ ∥= −1
4π log

(x −ξ)2 + (y −η)2 
.
(6.106)
A fully rigorous, albeit more diﬃcult, justiﬁcation of (6.106) comes from the following
important result, known as Green’s representation formula.
Theorem 6.17. Let Ω ⊂R2 be a bounded domain, with piecewise C1 boundary ∂Ω.
Suppose u ∈C2(Ω) ∩C1(Ω). Then, for any (x, y) ∈Ω,
u(x, y) = −
 
Ω
G0(x, y; ξ, η) Δu(ξ, η) dξ dη
+
&
∂Ω

G0(x, y; ξ, η) ∂u
∂n (ξ, η) −∂G0
∂n (x, y; ξ, η) u(ξ, η)

ds,
(6.107)
where the Laplacian and the normal derivatives on the boundary are all taken with respect
to the integration variables ξ = (ξ, η).
In particular, if both u and ∂u/∂n vanish on ∂Ω, then (6.107) reduces to
u(x, y) = −
 
R2 G0(x, y; ξ, η) Δu(ξ, η) dξ dη.
Invoking the deﬁnition of the delta function on the left-hand side and formally applying
the Green identity (6.88) to the right-hand side produces
 
R2 δ(x −ξ) δ(y −η) u(ξ, η) dξ dη =
 
R2 −ΔG0(x, y; ξ, η) u(ξ, η) dξ dη.
(6.108)
It is in this dual sense that we justify the desired formula
−ΔG0(x; ξ) = 1
2π Δ

log ∥x −ξ ∥

= δ(x −ξ).
(6.109)
Proof of Theorem 6.17:
We ﬁrst note that, even though G0(x, ξ) has a logarithmic
singularity at x = ξ, the double integral in (6.107) is ﬁnite. Indeed, after introducing polar
coordinates ξ = x + r cos θ, η = y + r sin θ, and recalling dξ dη = r dr dθ, we see that it
equals
1
2π
 
(r log r) Δu dr dθ.

6.3 Green’s Functions for the Planar Poisson Equation
251
∂Ω
Cε
x
Figure 6.12.
Domain Ωε = Ω \ Dε(x).
The product r log r is everywhere continuous — even at r = 0 — and so, provided Δu is
well behaved, e.g., continuous, the integral is ﬁnite. There is, of course, no problem with
the line integral in (6.107), since the contour does not go through the singularity.
Let us now avoid dealing directly with the singularity by working on a subdomain
Ωε = Ω \ Dε(x) = { ξ ∈Ω | ∥x −ξ ∥> ε }
obtained by cutting out a small disk
Dε(x) = { ξ | ∥x −ξ ∥≤ε }
of radius ε > 0 centered at x. We choose ε suﬃciently small in order that Dε(x) ⊂Ω, and
hence
∂Ωε = ∂Ω ∪Cε,
where
Cε =
-
∥x −ξ ∥= ε
.
is the circular boundary of the disk.
The subdomain Ωε is represented by the shaded
region in Figure 6.12. Since the double integral is well deﬁned, we can approximate it by
integrating over Ωε:
 
Ω
G0(x, y; ξ, η) Δu(ξ, η) dξ dη = lim
ε →0
 
Ωε
G0(x, y; ξ, η) Δu(ξ, η) dξ dη.
(6.110)
Since G0 has no singularities in Ωε, we are able to apply the Green formula (6.85) and then
(6.103) to evaluate
 
Ωε
G0(x, y; ξ, η) Δu(ξ, η) dξ dη
=
&
∂Ω

G0(x, y; ξ, η) ∂u
∂n (ξ, η) −∂G0
∂n (x, y; ξ, η) u(ξ, η)

ds
−
&
Cε

G0(x, y; ξ, η) ∂u
∂n (ξ, η) −∂G0
∂n (x, y; ξ, η) u(ξ, η)

ds,
(6.111)
where the line integral around Cε is taken in the usual counterclockwise direction — the
opposite orientation to that induced by its status as part of the boundary of Ωε. Now, on

252
6 Generalized Functions and Green’s Functions
the circle Cε,
G0(x, y; ξ, η) = −log r
2π

r =ε
= −log ε
2π ,
(6.112)
while, in view of Exercise 6.3.1,
∂G0
∂n (x, y; ξ, η) = −1
2π
∂(log r)
∂r

r =ε
= −
1
2πε .
(6.113)
Therefore,
&
Cε
∂G0
∂n (x, y; ξ, η) u(ξ, η) ds = −
1
2πε
&
Cε
u(ξ, η) ds,
which we recognize as minus the average of u on the circle of radius ε. As ε →0, the circles
shrink down to their common center, and so, by continuity, the averages tend to the value
u(x, y) at the center; thus,
lim
ε →0
&
Cε
∂G0
∂n (x, y; ξ, η) u(ξ, η) ds = −u(x, y).
(6.114)
On the other hand, using (6.112), and then (6.89) on the disk Dε, we have
&
Cε
G0(x, y; ξ, η) ∂u
∂n (ξ, η) ds = −log ε
2π
&
Cε
∂u
∂n (ξ, η) ds
= −log ε
2π
 
Dε
Δu(ξ, η) dξ dη = −(ε2 log ε) Δuε,
where
Δuε =
1
2πε2
 
Dε
Δu(ξ, η) dξ dη
is the average of Δu over the disk Dε. As above, as ε →0, the averages over the disks
converge to the value at their common center, Δuε →Δu(x, y), and hence
lim
ε →0
&
Cε
G0(x, y; ξ, η) ∂u
∂n (ξ, η) ds = lim
ε →0 (−ε2 log ε) Δuε = 0.
(6.115)
In view of (6.110, 114, 115), the ε →0 limit of (6.111) is exactly the Green representation
formula (6.107).
Q.E.D.
As noted above, the free space Green’s function (6.106) represents the gravitational
potential in empty two-dimensional space due to a unit point mass, or, equivalently, the
two-dimensional electrostatic potential due to a unit point charge sitting at position ξ. The
corresponding gravitational or electrostatic force ﬁeld is obtained by taking its gradient:
F = ∇G0 = −
x −ξ
2π ∥x −ξ ∥2 .
Its magnitude
∥F ∥=
1
2π ∥x −ξ ∥
is inversely proportional to the distance from the mass or charge, which is the two-
dimensional form of Newton’s and Coulomb’s three-dimensional inverse square laws.

6.3 Green’s Functions for the Planar Poisson Equation
253
The gravitational potential due to a two-dimensional mass, e.g., a ﬂat plate, in the
shape of a domain Ω ⊂R2 is obtained by superimposing delta function sources with
strengths equal to the density of the material at each point. The result is the potential
function
u(x, y) = −1
4π
 
Ω
ρ(ξ, η) log

(x −ξ)2 + (y −η)2 
dξ dη,
(6.116)
in which ρ(ξ, η) denotes the density at position (ξ, η) ∈Ω.
Example 6.18. The gravitational potential due to a circular disk D = {x2 + y2 ≤1}
of unit radius and unit density ρ ≡1 is
u(x, y) = −1
4π
 
D
log

(x −ξ)2 + (y −η)2 
dξ dη.
(6.117)
A direct evaluation of this double integral is not so easy. However, we can write down the
potential in closed form by recalling that it solves the Poisson equation
−Δu =
 1,
∥x ∥< 1,
0,
∥x ∥> 1.
(6.118)
Moreover, u is clearly radially symmetric, and hence a function of r alone. Thus, in the
polar coordinate expression (4.105) for the Laplacian, the θ derivative terms vanish, and
so (6.118) reduces to
d2u
dr2 + 1
r
du
dr =
 −1,
r < 1,
0,
r > 1,
which is eﬀectively a ﬁrst-order linear ordinary diﬀerential equation for du/dr. Solving
separately on the two subintervals produces
u(r) =
 a + b log r −1
4 r2,
r < 1,
c + d log r,
r > 1,
where a, b, c, d are constants. Continuity of u(r) and u′(r) at r = 1 implies c = a −1
4,
d = b −1
2. Moreover, the potential for a non-concentrated mass cannot have a singularity
at the origin, and so b = 0.
Direct evaluation of (6.117) at x = y = 0, using polar
coordinates, proves that a = 1
4 . We conclude that the gravitational potential (6.117) due
to a uniform disk of unit radius, and hence total mass (area) π, is, explicitly,
u(x, y) =

1
4 (1 −r2) = 1
4 (1 −x2 −y2),
x2 + y2 ≤1,
−1
2 log r = −1
4 log(x2 + y2),
x2 + y2 ≥1.
(6.119)
Observe that, outside the disk, the potential is exactly the same as the logarithmic potential
due to a point mass of magnitude π located at the origin. Consequently, the gravitational
force ﬁeld outside a uniform disk is the same as if all its mass were concentrated at the
origin.
With the free-space logarithmic potential in hand, let us return to the question of ﬁnd-
ing the Green’s function for a boundary value problem on a bounded domain Ω ⊂R2. Since
the logarithmic potential (6.106) is a particular solution to the Poisson equation (6.98), the
general solution, according to Theorem 1.6, is given by u = G0 + z, where z is an arbitrary
solution to the homogeneous equation Δz = 0, i.e., an arbitrary harmonic function. Thus,
constructing the Green’s function has been reduced to the problem of ﬁnding the harmonic
function z such that G = G0 + z satisﬁes the desired homogeneous boundary conditions.
Let us explicitly formulate this result for the (inhomogeneous) Dirichlet problem.

254
6 Generalized Functions and Green’s Functions
Theorem 6.19. The Green’s function for the Dirichlet boundary value problem for
the Poisson equation on a bounded domain Ω ⊂R2 has the form
G(x, y; ξ, η) = G0(x, y; ξ, η) + z(x, y; ξ, η),
(6.120)
where the ﬁrst term is the logarithmic potential (6.106), while, for each (ξ, η) ∈Ω, the
second term is the harmonic function that solves the boundary value problem
Δz = 0
on
Ω,
z(x, y; ξ, η) = 1
4π log

(x −ξ)2 + (y −η)2 
for
(x, y) ∈∂Ω.
(6.121)
If u(x, y) is a solution to the inhomogeneous Dirichlet problem
−Δu = f,
x ∈Ω,
u = h,
x ∈∂Ω,
(6.122)
then
u(x, y) =
 
Ω
G(x, y; ξ, η) f(ξ, η) dξ dη −
&
∂Ω
∂G
∂n (x, y; ξ, η) h(ξ, η) ds,
(6.123)
where the normal derivative of G is taken with respect to (ξ, η) ∈∂Ω.
Proof : To show that (6.120) is the Green’s function, we note that
−ΔG = −ΔG0 −Δz = δ(ξ,η)
in
Ω,
(6.124)
while
G(x, y; ξ, η) = G0(x, y; ξ, η) + z(x, y; ξ, η) = 0
on
∂Ω.
(6.125)
Next, to establish the solution formula (6.123), since both z and u are C2, we can use
(6.88) (with v = z, keeping in mind that Δz = 0) to establish
0 = −
 
Ω
z(x, y; ξ, η) Δu(ξ, η)dξ dη
+
&
∂Ω

z(x, y; ξ, η) ∂u
∂n (ξ, η) −∂z
∂n (x, y; ξ, η) u(ξ, η)

ds.
Adding this to Green’s representation formula (6.107), and using (6.125), we deduce that
u(x, y) = −
 
Ω
G(x, y; ξ, η) Δu(ξ, η) dξ dη −
&
∂Ω
∂G(x, y; ξ, η)
∂n
u(ξ, η) ds,
which, given (6.122), produces (6.123).
Q.E.D.
The one subtle issue left unresolved is the existence of the solution. Read properly,
Theorem 6.19 states that if a classical solution exists, then it is necessarily given by the
Green’s function formula (6.123). Proving existence of the solution — and also the existence
of the Green’s function, or equivalently, the solution z to (6.121) — requires further in-
depth analysis, lying beyond the scope of this text. In particular, to guarantee existence,
the underlying domain must have a reasonably nice boundary, e.g., a piecewise smooth
curve without sharp cusps. Interestingly, lack of regularity at sharp cusps in the boundary
underlies the electromagnetic phenomenon known as St. Elmo’s ﬁre, cf. [121]. Extensions
to irregular domains, e.g., those with fractal boundaries, is an active area of contemporary
research. Moreover, unlike one-dimensional boundary value problems, mere continuity of

6.3 Green’s Functions for the Planar Poisson Equation
255
the forcing function f is not quite suﬃcient to ensure the existence of a classical solution to
the Poisson boundary value problem; diﬀerentiability does suﬃce, although this assumption
can be weakened. We refer to [61, 70], for a development of the Perron method based on
approximating the solution by a sequence of subsolutions, which, by deﬁnition, solve the
diﬀerential inequality −Δu ≤f. An alternative proof, using the direct method of the
calculus of variations, can be found in [35]. The latter proof relies on the characterization
of the solution by a minimization principle, which we discuss in some detail in Chapter 9.
Exercises
♦6.3.1. Let CR be a circle of radius R centered at the origin and n its unit outward normal. Let
f(r, θ) be a function expressed in polar coordinates. Prove that ∂f/∂n = ∂f/∂r on CR.
6.3.2. Let f(x) > 0 be a continuous, positive function on the interval a ≤x ≤b. Let Ω be the
domain lying between the graph of f(x) on the interval [a, b] and the x–axis. Explain why
(6.77) reduces to the usual calculus formula for the area under the graph of f.
6.3.3. Explain what happens to the conclusion of Lemma 6.16 if Ω is not a connected domain.
6.3.4. Can you ﬁnd constants cn such that the functions gn(x, y) = cn[1 + n2(x2 + y2)]−1
converge to the two-dimensional delta function: gn(x, y) →δ(x, y) as n →∞?
6.3.5. Explain why the two-dimensional delta function satisﬁes the scaling law
δ(β x, β y) = 1
β2 δ(x, y),
for
β > 0.
♦6.3.6. Write out a polar coordinate formula, in terms of δ(r −r0) and δ(θ −θ0), for the two-
dimensional delta function δ(x −x0, y −y0) = δ(x −x0) δ(y −y0).
6.3.7. True or false: δ(x) = δ(∥x ∥).
♦6.3.8. Suppose that ξ = f(x, y), η = g(x, y) deﬁnes a one-to-one C1 map from a domain
D ⊂R2 to the domain Ω = { (ξ, η) = (f(x, y), g(x, y)) | (x, y) ∈D } ⊂R2, and has nonzero
Jacobian determinant: J(x, y) = fxgy −fygx ̸= 0 for all (x, y) ∈D. Suppose further that
(0, 0) = (f(x0, y0), g(x0, y0)) ∈Ω for (x0, y0) ∈D. Prove the following formula governing
the eﬀect of the map on the two-dimensional delta function:
δ(f(x, y), g(x, y)) = δ(x −x0, y −y0)
| J(x0, y0) |
.
(6.126)
6.3.9. Suppose f(x, y) =
 1,
3x −2y > 1,
0,
3x −2y < 1.
Compute its partial derivatives ∂f
∂x and ∂f
∂y in
the sense of generalized functions.
6.3.10. Find a series solution to the rectangular boundary value problem (4.91–92) when the
boundary data f(x) = δ(x −ξ) is a delta function at a point 0 < ξ < a. Is your solution
inﬁnitely diﬀerentiable inside the rectangle?
6.3.11. Answer Exercise 6.3.10 when f(x) = δ ′(x −ξ) is the derivative of the delta function.
6.3.12. A 1 meter square plate is subject to the Neumann boundary conditions ∂u/∂n = 1 on
its entire boundary. What is the equilibrium temperature? Explain.
♦6.3.13. A conservation law for an equilibrium system in two dimensions is, by deﬁnition, a di-
vergence expression
∂X
∂x + ∂Y
∂y = 0
(6.127)

256
6 Generalized Functions and Green’s Functions
that vanishes for all solutions.
(a) Given a conservation law prescribed by v = (X, Y ) deﬁned on a simply connected do-
main D, show that the line integral
	
C v · n ds =
	
C X dy −Y dx is path-independent,
meaning that its value depends only on the endpoints of the curve C.
(b) Show that the Laplace equation can be written as a conservation law, and write down
the corresponding path-independent line integral.
Note: Path-independent integrals are of importance in the study of cracks, dislocations, and
other material singularities, [49].
♦6.3.14. In two-dimensional dynamics, a conservation law is an equation of the form
∂T
∂t + ∂X
∂x + ∂Y
∂y = 0,
(6.128)
in which T is the conserved density, while v = (X, Y ) represents the associated ﬂux.
(a) Prove that, on a bounded domain Ω ⊂R2, the rate of change of the integral
		
Ω T dx dy
of the conserved density depends only on the ﬂux through the boundary ∂Ω.
(b) Write the partial diﬀerential equation ut + uux + uuy = 0 as a conservation law. What
is the integrated version?
The Method of Images
The preceding analysis exposes the underlying form of the Green’s function, but we are
still left with the determination of the harmonic component z(x, y) required to match the
logarithmic potential boundary values, cf. (6.121). We will discuss two principal analytic
techniques employed to produce explicit formulas. The ﬁrst is an adaptation of the method
of separation of variables, which leads to inﬁnite series expressions. We will not dwell on
this approach here, although a couple of the exercises ask the reader to work through some
of the details; see also the discussion leading up to (9.110). The second is the Method
of Images, which will be developed in this section.
Another approach is based on the
theory of conformal mapping; it can be found in books on complex analysis, including
[53, 98]. While the ﬁrst two methods are limited to a fairly small class of domains, they
extend to higher-dimensional problems, as well as to certain other types of elliptic boundary
value problems, whereas conformal mapping is, unfortunately, restricted to two-dimensional
problems involving the Laplace and Poisson equations.
We already know that the singular part of the Green’s function for the two-dimensional
Poisson equation is provided by a logarithmic potential. The problem, then, is to construct
the harmonic part, called z(x, y) in (6.120), so that the sum has the correct homogeneous
boundary values, or, equivalently, so that z(x, y) has the same boundary values as the
logarithmic potential. In certain cases, z(x, y) can be thought of as the potential induced
by one or more hypothetical electric charges (or, equivalently, gravitational point masses)
that are located outside the domain Ω, arranged in such a manner that their combined
electrostatic potential happens to coincide with the logarithmic potential on the boundary
of the domain.
The goal, then, is to place image charges of suitable strengths in the
appropriate positions.
Here, we will only consider the case of a single image charge, located at a position
η ̸∈Ω. We scale the logarithmic potential (6.106) by the charge strength, and, for added

6.3 Green’s Functions for the Planar Poisson Equation
257
0
ξ
η
x
Figure 6.13.
Method of Images for the unit disk.
ﬂexibility, include an additional constant — the charge’s potential baseline:
z(x, y) = a log ∥x −η ∥+ b,
η ∈R2 \ Ω.
The function z(x, y) is harmonic inside Ω, since the logarithmic potential is harmonic
everywhere except at the external singularity η. For the Dirichlet boundary value problem,
then, for each point ξ ∈Ω, we must ﬁnd a corresponding image point η ∈R2 \ Ω and
constants a, b ∈R such that†
log ∥x −ξ ∥= a log ∥x −η ∥+ b
for all
x ∈∂Ω,
or, equivalently,
∥x −ξ ∥= λ ∥x −η ∥a
for all
x ∈∂Ω,
(6.129)
where λ = eb. For each ﬁxed ξ, η, λ, a, the equation in (6.129) will, typically, implicitly
prescribe a plane curve, but it is not clear that one can always arrange that these curves
all coincide with the boundary of our domain.
To make further progress, we appeal to a geometric construction based on similar
triangles. Let us select η = c ξ to be a point lying on the ray through ξ. Its location
is chosen so that the triangle with vertices 0, x, η is similar to the triangle with vertices
0, ξ, x, noting that they have the same angle at the common vertex 0 — see Figure 6.13.
Similarity requires that the triangles’ corresponding sides have a common ratio, and so
∥ξ ∥
∥x ∥= ∥x ∥
∥η ∥= ∥x −ξ ∥
∥x −η ∥= λ.
(6.130)
The last equality implies that (6.129) holds with a = 1. Consequently, if we choose
∥η ∥=
1
∥ξ ∥,
so that
η =
ξ
∥ξ ∥2 ,
(6.131)
then
∥x ∥2 = ∥ξ ∥∥η ∥= 1.
†
To simplify the formulas, we have omitted the 1/(2π) factor, which can easily be reinstated
at the end of the analysis.

258
6 Generalized Functions and Green’s Functions
Figure 6.14.
Green’s function for the unit disk.
Thus x lies on the unit circle, and, as a result, λ = ∥ξ ∥= 1/∥η ∥. The map taking a
point ξ inside the disk to its image point η deﬁned by (6.131) is known as inversion with
respect to the unit circle.
We have now demonstrated that the potentials
1
2π log ∥x −ξ ∥= 1
2π log

∥ξ ∥∥x −η ∥

= 1
2π log ∥∥ξ ∥2 x −ξ ∥
∥ξ ∥
,
∥x ∥= 1,
(6.132)
have the same boundary values on the unit circle. Consequently, their diﬀerence
G(x; ξ) = −1
2π log ∥x −ξ ∥+ 1
2π log ∥∥ξ ∥2 x −ξ ∥
∥ξ ∥
= 1
2π log ∥∥ξ ∥2 x −ξ ∥
∥ξ ∥∥x −ξ ∥
(6.133)
has the required properties for the Green’s function for the Dirichlet problem on the unit
disk. Writing this in terms of polar coordinates
x = (r cos θ, r sin θ),
ξ = (ρ cos φ, ρ sin φ),
and applying the Law of Cosines to the triangles in Figure 6.13 produces the explicit
formula
G(r, θ; ρ, φ) = 1
4π log
 1 + r2ρ2 −2rρ cos(θ −φ)
r2 + ρ2 −2rρ cos(θ −φ)

.
(6.134)
In Figure 6.14 we sketch the Green’s function for the Dirichlet boundary value problem
corresponding to a unit impulse being applied at a point halfway between the center and
the edge of the disk. We also require its radial derivative
∂G
∂r (r, θ; ρ, φ) = −1
2π
1 −r2
1 + r2 −2 r cos(θ −φ) ,
(6.135)
which coincides with its normal derivative on the unit circle. Thus, specializing (6.123),
we arrive at a solution to the general Dirichlet boundary value problem for the Poisson
equation in the unit disk.

6.3 Green’s Functions for the Planar Poisson Equation
259
Figure 6.15.
The Poisson kernel.
Theorem 6.20. The solution to the inhomogeneous Dirichlet boundary value prob-
lem
−Δu = f,
for
r = ∥x ∥< 1,
u = h,
for
r = 1,
is, when expressed in polar coordinates,
u(r, θ) = 1
4π
 π
−π
 1
0
f(ρ, φ) log
 1 + r2ρ2 −2rρ cos(θ −φ)
r2 + ρ2 −2rρ cos(θ −φ)

ρ dρ dφ
+
1
2π
 π
−π
h(φ)
1 −r2
1 + r2 −2 r cos(θ −φ) dφ.
(6.136)
When f ≡0, formula (6.136) recovers the Poisson integral formula (4.126) for the
solution to the Dirichlet boundary value problem for the Laplace equation. In particular,
the boundary data h(θ) = δ(θ −φ), corresponding to a concentrated unit heat source
applied to a single point on the boundary, produces the Poisson kernel
u(r, θ) =
1 −r2
2π

1 + r2 −2r cos(θ −φ)
 .
(6.137)
The reader may enjoy verifying that this function indeed solves the Laplace equation and
has the correct boundary values in the limit as r →1.
Exercises
6.3.15. A circular disk of radius 1 is subject to a heat source of unit magnitude on the subdisk
r ≤1
2. Its boundary is kept at 0◦.
(a) Write down an integral formula for the equilibrium temperature.
(b) Use radial symmetry to ﬁnd an explicit formula for the equilibrium temperature.

260
6 Generalized Functions and Green’s Functions
6.3.16. A circular disk of radius 1 meter is subject to a unit concentrated heat source at its
center and has completely insulated boundary. What is the equilibrium temperature?
♥6.3.17.(a) For n > 0, ﬁnd the solution to the boundary value problem
−Δu = n
π e−n (x2+y2),
x2 + y2 < 1,
u(x, y) = 0,
x2 + y2 = 1.
(b) Discuss what happens in the limit as n →∞.
♥6.3.18.(a) Use the Method of Images to construct the Green’s function for a half-plane {y > 0}
that is subject to homogeneous Dirichlet boundary conditions. Hint: The image point is
obtained by reﬂection. (b) Use your Green’s function to solve the boundary value problem
−Δu =
1
1 + y ,
y > 0,
u(x, 0) = 0.
6.3.19. Construct the Green’s function for the half-disk Ω = {x2 + y2 < 1, y > 0} when sub-
ject to homogeneous Dirichlet boundary conditions. Hint: Use three image points.
6.3.20. Prove directly that the Poisson kernel (6.137) solves the Laplace equation for all r < 1.
♥6.3.21. Provide the details for the following alternative method for solving the homogeneous
Dirichlet boundary value problem for the Poisson equation on the unit square:
uxx −uyy = f(x, y),
u(x, 0) = 0,
u(x, 1) = 0,
u(0, y) = 0,
u(1, y) = 0,
0 < x, y < 1.
(a) Write both u(x, y) and f(x, y) as Fourier sine series in y whose coeﬃcients depend on x.
(b) Substitute these series into the diﬀerential equation, and equate Fourier coeﬃcients to
obtain an inﬁnite system of ordinary boundary value problems for the x-dependent Fourier
coeﬃcients of u. (c) Use the Green’s functions for each boundary value problem to write
out the solution and hence a series for the solution to the original boundary value problem.
(d) Implement this method for the following forcing functions:
(i) f(x, y) = sin πy,
(ii) f(x, y) = sin πx sin 2πy,
(iii) f(x, y) = 1.
♦6.3.22. Use the method of Exercise 6.3.21 to ﬁnd a series representation for the Green’s func-
tion of a unit square subject to Dirichlet boundary conditions.
6.3.23. Write out the details of how to derive (6.134) from (6.133).
6.3.24. True or false: If the gravitational potential at a point a is greater than its value at the
point b, then the magnitude of the gravitational force at a is greater than its value at b.
♠6.3.25.(a) Write down integral formulas for the gravitational potential and force due to a square
plate S = {−1 ≤x, y ≤1} of unit density ρ = 1. (b) Use numerical integration to calculate
the gravitational force at the points (2, 0) and
√
2 ,
√
2

. Before starting, try to predict
which point experiences the stronger force, and then check your prediction.
♠6.3.26. An equilateral triangular plate with unit area exerts a gravitational force on an ob-
server sitting a unit distance away from its center. Is the force greater if the observer is lo-
cated opposite a vertex of the triangle or opposite a side? Is the force greater than or less
than that exerted by a circular plate of the same area? Use numerical integration to evalu-
ate the double integrals.
6.3.27. Consider the wave equation utt = c2uxx on the line −∞< x < ∞. Use the d’Alembert
formula (2.82) to solve the initial value problem u(0, x) = δ(x −a), ut(0, x) = 0. Can you
realize your solution as the limit of classical solutions?
♦6.3.28. Consider the wave equation utt = c2uxx on the line −∞< x < ∞. Use the d’Alembert
formula (2.82) to solve the initial value problem u(0, x) = 0, ut(0, x) = δ(x −a), modeling
the eﬀect of striking the string with a highly concentrated blow at the point x = a. Graph
the solution at several times. Discuss the behavior of any discontinuities in the solution. In
particular, show that u(t, x) ̸= 0 on the domain of inﬂuence of the point (a, 0).

6.3 Green’s Functions for the Planar Poisson Equation
261
6.3.29.(a) Write down the solution u(t, x) to the wave equation utt = 4uxx on the real line
with initial data u(0, x) =
 1 −| x |,
| x | ≤1,
0,
otherwise,
∂u
∂t (0, x) = 0. (b) Explain why u(t, x) is
not a classical solution to the wave equation.
(c) Determine the derivatives ∂2u/∂t2 and
∂2u/∂x2 in the sense of distributions (generalized functions) and use this to justify the fact
that u(t, x) solves the wave equation in a distributional sense.
♥6.3.30. A piano string of length ℓ= 3 and wave speed c = 2 with both ends ﬁxed is hit by a
hammer 1
3 of the way along. The initial-boundary value problem that governs the resulting
vibrations of the string is
∂2u
∂t2 = 4 ∂2u
∂x2 ,
u(t, 0) = 0 = u(t, 3),
u(0, x) = 0,
∂u
∂t (0, x) = δ(x −1).
(a) What are the fundamental frequencies of vibration?
(b) Write down the solution to the initial-boundary value problem in Fourier series form.
(c) Write down the Fourier series for the velocity ∂u/∂t of your solution.
(d) Write down the d’Alembert formula for the solution, and sketch a picture of the string
at four or ﬁve representative times.
(e) True or false: The solution is periodic in time. If true, what is the period? If false, ex-
plain what happens as t increases.
6.3.31.(a) Write down a Fourier series for the solution to the initial-boundary value problem
∂2u
∂t2 = ∂2u
∂x2 ,
u(t, −1) = 0 = u(t, 1),
u(0, x) = δ(x),
∂u
∂t (0, x) = 0.
(b) Write down an analytic formula for the solution, i.e., sum your series. (c) In what
sense does the series solution in part (a) converge to the true solution? Do the partial sums
provide a good approximation to the actual solution?
6.3.32. Answer Exercise 6.3.31 for
∂2u
∂t2 = ∂2u
∂x2 ,
u(t, −1) = 0 = u(t, 1),
u(0, x) = 0,
∂u
∂t (0, x) = δ(x).

Chapter 7
Fourier Transforms
Fourier series and their ilk are designed to solve boundary value problems on bounded
intervals. The extension of the Fourier calculus to the entire real line leads naturally to the
Fourier transform, a powerful mathematical tool for the analysis of aperiodic functions.
The Fourier transform is of fundamental importance in a remarkably broad range of ap-
plications, including both ordinary and partial diﬀerential equations, probability, quantum
mechanics, signal and image processing, and control theory, to name but a few.
In this chapter, we motivate the construction by investigating how (rescaled) Fourier
series behave as the length of the interval goes to inﬁnity. The resulting Fourier transform
maps a function deﬁned on physical space to a function deﬁned on the space of frequencies,
whose values quantify the “amount” of each periodic frequency contained in the original
function. The inverse Fourier transform then reconstructs the original function from its
transformed frequency components. The integrals deﬁning the Fourier transform and its
inverse are, remarkably, almost identical, and this symmetry is often exploited, for example
in assembling tables of Fourier transforms.
One of the most important properties of the Fourier transform is that it converts
calculus — diﬀerentiation and integration — into algebra — multiplication and division.
This underlies its application to linear ordinary diﬀerential equations and, in the following
chapters, partial diﬀerential equations. In engineering applications, the Fourier transform
is sometimes overshadowed by the Laplace transform, which is a particular subcase. The
Fourier transform is used to analyze boundary value problems on the entire line.
The
Laplace transform is better suited to solving initial value problems, [23], but will not be
developed in this text.
The Fourier transform is, like Fourier series, completely compatible with the calculus
of generalized functions, [68]. The ﬁnal section contains a brief introduction to the analytic
foundations of the subject, including the basics of Hilbert space. However, a full, rigorous
development requires more powerful analytical tools, including the Lebesgue integral and
complex analysis, and the interested reader is therefore referred to more advanced texts,
including [37, 68, 98, 117].
7.1 The Fourier Transform
We begin by motivating the Fourier transform as a limiting case of Fourier series. Although
the rigorous details are subtle, the underlying idea can be straightforwardly explained. Let
f(x) be a function deﬁned for all −∞< x < ∞. The goal is to construct a Fourier expan-
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
7
263
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

264
7 Fourier Transforms
sion for f(x) in terms of basic trigonometric functions. One evident approach is to construct
its Fourier series on progressively longer and longer intervals, and then take the limit as
their lengths go to inﬁnity. This limiting process converts the Fourier sums into integrals,
and the resulting representation of a function is renamed the Fourier transform. Since we
are dealing with an inﬁnite interval, there are no longer any periodicity requirements on
the function f(x). Moreover, the frequencies represented in the Fourier transform are no
longer constrained by the length of the interval, and so we are eﬀectively decomposing a
quite general aperiodic function into a continuous superposition of trigonometric functions
of all possible frequencies.
Let us present the details in a more concrete form. The computations will be signif-
icantly simpler if we work with the complex version of the Fourier series from the outset.
Our starting point is the rescaled Fourier series (3.86) on a symmetric interval [−ℓ, ℓ] of
length 2 ℓ, which we rewrite in the adapted form
f(x) ∼
∞

ν =−∞
π
2
fℓ(kν)
ℓ
e i kνx.
(7.1)
The sum is over the discrete collection of frequencies
kν = πν
ℓ,
ν = 0, ±1, ±2, . . . ,
(7.2)
corresponding to those trigonometric functions that have period 2 ℓ. For reasons that will
soon become apparent, the Fourier coeﬃcients of f are now denoted as
cν = 1
2 ℓ
 ℓ
−ℓ
f(x) e−i kν x dx =
π
2
fℓ(kν)
ℓ
,
(7.3)
so that
fℓ(kν) =
1
√
2π
 ℓ
−ℓ
f(x) e−i kν x dx.
(7.4)
This reformulation of the basic Fourier series formula allows us to easily pass to the limit
as the interval’s length ℓ→∞.
On an interval of length 2 ℓ, the frequencies (7.2) required to represent a function in
Fourier series form are equally distributed, with interfrequency spacing
Δk = kν+1 −kν = π
ℓ.
(7.5)
As ℓ→∞, the spacing Δk →0, and so the relevant frequencies become more and more
densely packed in the line −∞< k < ∞. In the limit, we thus anticipate that all possible
frequencies will be represented. Indeed, letting kν = k be arbitrary in (7.4), and sending
ℓ→∞, results in the inﬁnite integral
f(k) =
1
√
2π
 ∞
−∞
f(x) e−i kx dx,
(7.6)
known as the Fourier transform of the function f(x). If f(x) is a suﬃciently nice function,
e.g., piecewise continuous and decaying to 0 reasonably quickly as | x | →∞, its Fourier
transform f(k) is deﬁned for all possible frequencies k ∈R. The preceding formula will
sometimes conveniently be abbreviated as
f(k) = F[f(x)],
(7.7)

7.1 The Fourier Transform
265
where F is the Fourier transform operator, which maps each (suﬃciently nice) function of
the spatial variable x to a function of the frequency variable k.
To reconstruct the function from its Fourier transform, we apply a similar limiting
procedure to the Fourier series (7.1), which we ﬁrst rewrite in a more suggestive form,
f(x) ∼
1
√
2π
∞

ν =−∞
fℓ(kν) e i kν x Δk,
(7.8)
using (7.5). For each ﬁxed value of x, the right-hand side has the form of a Riemann sum
approximating the integral
1
√
2π
 ∞
−∞
fℓ(k) e i kx dk.
As ℓ→∞, the functions (7.4) converge to the Fourier transform: fℓ(k) →f(k); moreover,
the interfrequency spacing Δk = π/ℓ→0, and so one expects the Riemann sums to
converge to the limiting integral
f(x) ∼
1
√
2π
 ∞
−∞
f(k) e i kx dk.
(7.9)
The resulting formula serves to deﬁne the inverse Fourier transform, which is used to re-
cover the original signal from its Fourier transform. In this manner, the Fourier series has
become a Fourier integral that reconstructs the function f(x) as a (continuous) superposi-
tion of complex exponentials e i kx of all possible frequencies, with f(k)/
√
2π quantifying
the amount contributed by the complex exponential of frequency k. In abbreviated form,
formula (7.9) can be written
f(x) = F −1[ f(k)],
(7.10)
thus deﬁning the inverse of the Fourier transform operator (7.7).
It is worth pointing out that both the Fourier transform (7.7) and its inverse (7.10)
deﬁne linear operators on function space. This means that the Fourier transform of the
sum of two functions is the sum of their individual transforms, while multiplying a function
by a constant multiplies its Fourier transform by the same factor:
F[f(x) + g(x)] = F[f(x)] + F[g(x)] = f(k) + g(k),
F[cf(x)] = c F[f(x)] = c f(k).
(7.11)
A similar statement holds for the inverse Fourier transform F −1.
Recapitulating, by letting the length of the interval go to ∞, the discrete Fourier series
has become a continuous Fourier integral, while the Fourier coeﬃcients, which were deﬁned
only at a discrete collection of possible frequencies, have become a complete function f(k)
deﬁned on all of frequency space. The reconstruction of f(x) from its Fourier transform
f(k) via (7.9) can be rigorously justiﬁed under suitable hypotheses. For example, if f(x)
is piecewise C1 on all of R and decays reasonably rapidly, f(x) →0 as | x | →∞, so
that its Fourier integral (7.6) converges absolutely, then it can be proved, [37, 117], that
the inverse Fourier integral (7.9) will converge to f(x) at all points of continuity, and to
the midpoint 1
2(f(x−) + f(x+)) at jump discontinuities — just like a Fourier series. In
particular, its Fourier transform f(k) →0 must also decay as | k | →∞, implying that (as
with Fourier series) the very high frequency modes make negligible contributions to the

266
7 Fourier Transforms
f(x)
f(k)
Figure 7.1.
Fourier transform of a rectangular pulse.
reconstruction of such a signal. A more precise result will be formulated in Theorem 7.15
below.
Example 7.1. The Fourier transform of the rectangular pulse†
f(x) = σ(x + a) −σ(x −a) =
 1,
−a < x < a,
0,
| x | > a,
(7.12)
of width 2a, is easily computed:
f(k) =
1
√
2π
 a
−a
e−i kx dx = e i ka −e−i ka
√
2π i k
=

2
π
sin ak
k
.
(7.13)
On the other hand, the reconstruction of the pulse via the inverse transform (7.9) tells us
that
1
π
 ∞
−∞
e i kx sin ak
k
dk = f(x) =
⎧
⎨
⎩
1,
−a < x < a,
1
2,
x = ± a,
0,
| x | > a.
(7.14)
Note the convergence to the middle of the jump discontinuities at x = ±a. The real part
of this complex integral produces a striking trigonometric integral identity:
1
π
 ∞
−∞
cos xk sin ak
k
dk =
⎧
⎨
⎩
1,
−a < x < a,
1
2 ,
x = ±a,
0,
| x | > a.
(7.15)
Just as many Fourier series yield nontrivial summation formulas, the reconstruction of a
function from its Fourier transform often leads to nontrivial integration formulas. One
†
σ(x) is the unit step function (3.46).

7.1 The Fourier Transform
267
cannot compute the integral (7.14) by the Fundamental Theorem of Calculus, since there
is no elementary function whose derivative equals the integrand.† In Figure 7.1 we display
the box function with a = 1, its Fourier transform, along with a reconstruction obtained
by numerically integrating (7.15). Since we are dealing with an inﬁnite integral, we must
break oﬀthe numerical integrator by restricting it to a ﬁnite interval. The ﬁrst graph
in the second row is obtained by integrating from −5 ≤k ≤5, while the second is from
−10 ≤k ≤10.
The nonuniform convergence of the integral leads to the appearance
of a Gibbs phenomenon at the two discontinuities, similar to what we observed in the
nonuniform convergence of a Fourier series.
On the other hand, the identity resulting from the imaginary part,
1
π
 ∞
−∞
sin kx sin ak
k
dk = 0,
is, on the surface, not surprising, because the integrand is odd. However, it is far from
obvious that either integral converges; indeed, the amplitude of the oscillatory integrand
decays like 1/| k |, but the latter function does not have a convergent integral, and so the
usual comparison test for inﬁnite integrals, [8, 97], fails to apply. Their convergence is
marginal at best, and the trigonometric oscillations somehow manage to ameliorate the
slow rate of decay of 1/k.
Example 7.2. Consider an exponentially decaying right-handed pulse‡
fr(x) =
 e−ax,
x > 0,
0,
x < 0,
(7.16)
where a > 0. We compute its Fourier transform directly from the deﬁnition:
fr(k) =
1
√
2π
 ∞
0
e−ax e−i kx dx = −
1
√
2π
e−(a+ i k)x
a + i k

∞
x=0
=
1
√
2π (a + i k) .
As in the preceding example, the inverse Fourier transform produces a nontrivial integral
identity:
1
2π
 ∞
−∞
e i kx
a + i k dk =
⎧
⎪
⎨
⎪
⎩
e−ax,
x > 0,
1
2 ,
x = 0,
0,
x < 0.
(7.17)
Similarly, a pulse that decays to the left,
fl(x) =
 eax,
x < 0,
0,
x > 0,
(7.18)
where a > 0 is still positive, has Fourier transform
fl(k) =
1
√
2π (a −i k)
.
(7.19)
†
One can use Euler’s formula (3.59) to reduce (7.14) to a complex version of the exponential
integral
	
(eαk/k) dk, but it can be proved, [25], that neither integral can be written in terms of
elementary functions.
‡
Note that we cannot Fourier transform the entire exponential function e−ax, because it does
not go to zero at both ±∞, which is required for the integral (7.6) to converge.

268
7 Fourier Transforms
Right pulse fr(x)
Left pulse fl(x)
Even pulse fe(x)
Odd pulse fo(x)
Figure 7.2.
Exponential pulses.
This also follows from the general fact that the Fourier transform of f(−x) is f(−k); see
Exercise 7.1.10. The even exponentially decaying pulse
fe(x) = e−a| x |
(7.20)
is merely the sum of left and right pulses: fe = fr + fl. Thus, by linearity,
fe(k) = fr(k) + fl(k) =
1
√
2π (a + i k)
+
1
√
2π (a −i k)
=

2
π
a
k2 + a2 .
(7.21)
The resulting Fourier transform is real and even because fe(x) is a real-valued even func-
tion; see Exercise 7.1.12. The inverse Fourier transform (7.9) produces another nontrivial
integral identity:
e−a| x | = 1
π
 ∞
−∞
a e i kx
k2 + a2 dk = a
π
 ∞
−∞
cos kx
k2 + a2 dk.
(7.22)
(The imaginary part of the integral vanishes, because its integrand is odd.) On the other
hand, the odd exponentially decaying pulse,
fo(x) = (sign x) e−a| x | =
 e−ax,
x > 0,
−eax,
x < 0,
(7.23)
is the diﬀerence of the right and left pulses, fo = fr −fl, and has purely imaginary and
odd Fourier transform
fo(k) = fr(k) −fl(k) =
1
√
2π (a + i k) −
1
√
2π (a −i k) = −i

2
π
k
k2 + a2 .
(7.24)

7.1 The Fourier Transform
269
The inverse transform is
(sign x) e−a| x | = −i
π
 ∞
−∞
k e i kx
k2 + a2 dk = 1
π
 ∞
−∞
k sin kx
k2 + a2 dk.
(7.25)
As a ﬁnal example, consider the rational function
f(x) =
1
x2 + a2 ,
where
a > 0.
(7.26)
Its Fourier transform requires integrating
f(k) =
1
√
2π
 ∞
−∞
e−i kx
x2 + a2 dx.
(7.27)
The indeﬁnite integral (anti-derivative) does not appear in basic integration tables, and, in
fact, cannot be done in terms of elementary functions. However, we have just managed to
evaluate this particular integral! Look at (7.22). If we change x to k and k to −x, then we
exactly recover the integral (7.27) up to a factor of a

2/π. We conclude that the Fourier
transform of (7.26) is
f(k) =
π
2
e−a| k |
a
.
(7.28)
This last example is indicative of an important general fact. The reader has no doubt
already noted the remarkable similarity between the Fourier transform (7.6) and its inverse
(7.9). Indeed, the only diﬀerence is that the former has a minus sign in the exponential.
This implies the following Symmetry Principle relating the direct and inverse Fourier trans-
forms.
Theorem 7.3. If the Fourier transform of the function f(x) is f(k), then the Fourier
transform of f(x) is f(−k).
The Symmetry Principle allows us to reduce the tabulation of Fourier transforms by
half. For instance, referring back to Example 7.1, we deduce that the Fourier transform of
the function
f(x) =

2
π
sin ax
x
is
f(k) = σ(−k + a) −σ(−k −a) = σ(k + a) −σ(k −a) =
⎧
⎨
⎩
1,
−a < k < a,
1
2,
k = ±a,
0,
| k | > a.
(7.29)
Note that, by linearity, we can divide both f(x) and f(k) by

2/π to deduce the Fourier
transform of sin ax
x
.
Warning: Some authors omit the
√
2π factor in the deﬁnition (7.6) of the Fourier
transform f(k). This alternative convention does have a slight advantage of eliminating
many
√
2π factors in the Fourier transform expressions.
However, this necessitates an
extra such factor in the reconstruction formula (7.9), which is achieved by replacing
√
2π
by 2π. A signiﬁcant disadvantage is that the resulting formulas for the Fourier transform
and its inverse are less similar, and so the Symmetry Principle of Theorem 7.3 requires
some modiﬁcation.
(On the other hand, convolution — to be discussed below — is a
little easier without the extra factor.) Yet another, more recent, convention can be found
in Exercise 7.1.18. When consulting any particular reference, the reader always needs to
check which version of the Fourier transform is being used.

270
7 Fourier Transforms
All of the functions in Example 7.2 required a > 0 for the Fourier integrals to converge.
The functions that emerge in the limit as a goes to 0 are of special interest. Let us start
with the odd exponential pulse (7.23). When a →0, the function fo(x) converges to the
sign function
f(x) = sign x = σ(x) −σ(−x) =
 +1,
x > 0,
−1,
x < 0.
(7.30)
Taking the limit of the Fourier transform (7.24) leads to
f(k) = −i

2
π
1
k .
(7.31)
The nonintegrable singularity of f(k) at k = 0 is indicative of the fact that the sign function
does not decay as | x | →∞. In this case, neither the Fourier transform integral nor its
inverse are well deﬁned as standard (Riemann, or even Lebesgue) integrals. Nevertheless, it
is possible to rigorously justify these results within the framework of generalized functions.
More interesting are the even pulse functions fe(x), which, in the limit a →0, become
the constant function
f(x) ≡1.
(7.32)
The limit of the Fourier transform (7.21) is
lim
a →0

2
π
2a
k2 + a2 =
 0,
k ̸= 0,
∞,
k = 0.
(7.33)
This limiting behavior should remind the reader of our construction (6.10) of the delta
function as the limit of the functions
δ(x) = lim
n →∞
n
π (1 + n2 x2) = lim
a →0
a
π (a2 + x2) .
Comparing with (7.33), we conclude that the Fourier transform of the constant function
(7.32) is a multiple of the delta function in the frequency variable:
f(k) =
√
2π δ(k).
(7.34)
The direct transform integral
δ(k) = 1
2π
 ∞
−∞
e−i kx dx
(7.35)
is, strictly speaking, not deﬁned, because the inﬁnite integrals of the oscillatory sine and
cosine functions don’t converge! However, this identity can be validly interpreted within
the framework of weak convergence and generalized functions. On the other hand, the
inverse transform formula (7.9) yields
 ∞
−∞
δ(k) e i kx dk = e i k0 = 1,
which is in accord with the basic deﬁnition (6.16) of the delta function. As in the preceding
case, the delta function singularity at k = 0 manifests the lack of decay of the constant
function.

7.1 The Fourier Transform
271
Conversely, the delta function δ(x) has constant Fourier transform
δ(k) =
1
√
2π
 ∞
−∞
δ(x) e−i kx dx = e−i k0
√
2π
≡
1
√
2π
,
(7.36)
a result that also follows from the Symmetry Principle of Theorem 7.3. To determine the
Fourier transform of a delta spike δξ(x) = δ(x −ξ) concentrated at position x = ξ, we
compute
δξ(k) =
1
√
2π
 ∞
−∞
δ(x −ξ) e−i kx dx = e−i kξ
√
2π
.
(7.37)
The result is a pure exponential in frequency space. Applying the inverse Fourier transform
(7.9) leads, at least on a formal level, to the remarkable identity
δξ(x) = δ(x −ξ) = 1
2π
 ∞
−∞
e i k(x−ξ) dk = 1
2π ⟨e i kx , e i kξ ⟩,
(7.38)
where ⟨· , · ⟩denotes the L2 Hermitian inner product of complex-valued functions of k ∈
R. Since the delta function vanishes for x ̸= ξ, this identity is telling us that complex
exponentials of diﬀering frequencies are mutually orthogonal. However, as with (7.35),
this makes sense only within the language of generalized functions. On the other hand,
multiplying both sides of (7.38) by f(ξ) and then integrating with respect to ξ produces
f(x) = 1
2π
 ∞
−∞
 ∞
−∞
f(ξ) e i k(x−ξ) dx dk.
(7.39)
This is a perfectly valid formula, being a restatement (or, rather, combination) of the
basic formulas (7.6) and (7.9) connecting the direct and inverse Fourier transforms of the
function f(x).
Conversely, the Symmetry Principle tells us that the Fourier transform of a pure
exponential e i κx will be a shifted delta spike
√
2π δ(k −κ), concentrated at frequency
k = κ. Both results are particular cases of the following Shift Theorem, whose proof is left
as an exercise for the reader.
Theorem 7.4. If f(x) has Fourier transform f(k), then the Fourier transform of the
shifted function f(x −ξ) is e−i kξ f(k). Similarly, the transform of the product function
e i κx f(x), for real κ, is the shifted transform f(k −κ).
In a similar vein, the Dilation Theorem gives the eﬀect of a scaling transformation on
the Fourier transform. Again, the proof is left to the reader.
Theorem 7.5. If f(x) has Fourier transform f(k), then the Fourier transform of the
rescaled function f(cx) for 0 ̸= c ∈R is
1
| c |
f
k
c

.

272
7 Fourier Transforms
Concise Table of Fourier Transforms
f(x)
f(k)
1
√
2π δ(k)
δ(x)
1
√
2π
σ(x)
π
2 δ(k) −
i
√
2π k
sign x
−i

2
π
1
k
σ(x + a) −σ(x −a)

2
π
sin ak
k
e−ax σ(x)
1
√
2π (a + i k)
eax (1 −σ(x))
1
√
2π (a −i k)
e−a| x |

2
π
a
k2 + a2
e−ax2
e−k2/(4a)
√
2a
tan−1 x
π3/2
√
2 δ(k) −i
π
2
e−| k |
k
f(cx + d)
e i k d/c
| c |
f
k
c

f(x)
f(−k)
f(x)
f(−k)
f ′(x)
i k f(k)
xf(x)
i f ′(k)
f ∗g(x)
√
2π f(k) g(k)
Note: The parameters a, c, d are real, with a > 0 and c ̸= 0.

7.1 The Fourier Transform
273
Example 7.6.
Let us determine the Fourier transform of the Gaussian function
g(x) = e−x2. To evaluate its Fourier integral, we ﬁrst complete the square in the exponent:
g(k) =
1
√
2π
 ∞
−∞
e−x2−i kx dx =
1
√
2π
 ∞
−∞
e−(x−i k/2)2−k2/4 dx
= e−k2/4
√
2π
 ∞
−∞
e−y2 dy = e−k2/4
√
2
.
The next-to-last equality employed the change of variables† y = x −1
2 i k, while the ﬁnal
step used formula (2.100).
More generally, to ﬁnd the Fourier transform of ga(x) = e−ax2, where a > 0, we invoke
the Dilation Theorem 7.5 with c = √a to deduce that ga(k) = e−k2/(4a)/
√
2a.
Since the Fourier transform uniquely associates a function f(k) on frequency space
with each (reasonable) function f(x) on physical space, one can characterize functions by
their transforms. Many practical applications rely on tables (or, even better, computer
algebra systems such as Mathematica and Maple) that recognize a wide variety of
transforms of basic functions of importance in applications. The accompanying table lists
some of the most important examples of functions and their Fourier transforms, based
on our convention (7.6).
Keep in mind that, by applying the Symmetry Principle of
Theorem 7.3, each entry can be used to deduce two diﬀerent Fourier transforms. A more
extensive collection of Fourier transforms can be found in [82].
Exercises
7.1.1. Find the Fourier transform of the following functions:
(a) e−(x+4)2,
(b) e−| x+1 |,
(c)
 x,
| x | < 1,
0,
otherwise,
(d)
⎧
⎨
⎩
e−2x,
x ≥0,
e3x,
x ≤0,
(e)
⎧
⎨
⎩
e−| x |,
| x | ≥1,
e−1,
| x | ≤1,
(f )

e−x sin x,
x > 0,
0,
x ≤0,
(g)
 1 −| x |,
| x | ≤1,
0,
otherwise.
7.1.2. Find the Inverse Fourier transform of the following functions: (a) e−k2, (b) e−| k |,
(c)

e−k sin k,
k ≥0,
0,
k ≤0,
(d)
 1,
α < k < β,
0,
otherwise,
(e)
 1 −| k |,
| k | < 1,
0,
otherwise.
7.1.3. Find the inverse Fourier transform of the function 1/(k + c) when (a) c = a is real;
(b) c = i b is purely imaginary; (c) c = a + i b is an arbitrary complex number.
7.1.4. Find the inverse Fourier transform of 1/(k2 −a2), where a > 0 is real.
Hint: Use Exercise 7.1.3.
♦7.1.5.(a) Find the Fourier transform of e i ω x.
(b) Use this to ﬁnd the Fourier transforms of
the basic trigonometric functions cos ω x and sin ω x.
7.1.6. Write down two real integral identites that result from the inverse Fourier transform of (7.28).
†
Since this represents a complex change of variables, a fully rigorous justiﬁcation of this step
requires the use of complex integration.

274
7 Fourier Transforms
7.1.7. Write down two real integral identities that follow from (7.17).
7.1.8.(a) Find the Fourier transform of the hat function fn(x) =

n −n2 | x |,
| x | ≤1/n,
0,
otherwise.
(b) What is the limit, as n →∞, of fn(k)?
(c) In what sense is the limit the Fourier transform of the limit of fn(x)?
7.1.9.(a) Justify the linearity of the Fourier transform, as in (7.11).
(b) State and justify the linearity of the inverse Fourier transform.
♦7.1.10. If the Fourier transform of f(x) is f(k), prove that (a) the Fourier transform of f(−x)
is f(−k); (b) the Fourier transform of the complex conjugate function f(x) is f(−k).
7.1.11. True or false: If the complex-valued function f(x) = g(x)+ i h(x) has Fourier transform
f(k) = g(k)+ i h(k), then g(x) has Fourier transform g(k) and h(x) has Fourier transform h(k).
♦7.1.12.(a) Prove that the Fourier transform of an even function is even. (b) Prove that the
Fourier transform of a real even function is real and even. (c) What can you say about the
Fourier transform of an odd function? (d) Of a real odd function? (e) What about a gen-
eral real function?
♦7.1.13. Prove the Shift Theorem 7.4.
♦7.1.14. Prove the Dilation Theorem 7.5.
7.1.15. Given that the Fourier transform of f(x) is f(k), ﬁnd, from ﬁrst principles, the Fourier
transform of g(x) = f(ax + b), where a and b are ﬁxed real constants.
7.1.16. Let a be a real constant. Given the Fourier transform f(k) of f(x), ﬁnd the Fourier
transforms of (a) f(x) e i ax, (b) f(x) cos ax, (c) f(x) sin ax.
♦7.1.17. A common alternative convention for the Fourier transform is to deﬁne
f1(k) =
	 ∞
−∞f(x) e−i kx dx.
(a) What is the formula for the corresponding inverse Fourier transform?
(b) How is f1(k) related to our Fourier transform f(k)?
♦7.1.18. Another convention for the Fourier transform is to deﬁne f2(k) =
	 ∞
−∞f(x) e−2π i kx dx.
Answer the questions in Exercise 7.1.17 for this version of the Fourier transform.
♥7.1.19. The cosine and sine transforms of a real function f(x) are deﬁned as
c(k) =
	 ∞
−∞f(x) cos kx dx,
s(k) =
	 ∞
−∞f(x) sin kx dx.
(7.40)
(i) Prove that f(k) = c(k) −i s(k).
(ii) Find the cosine and sine transforms of the func-
tions in Exercise 7.1.1.
(iii) Show that c(k) is an even function, while s(k) is an odd func-
tion.
(iv) Show that if f is an even function, then s(k) ≡0, while if f is an odd function,
then c(k) ≡0.
♦7.1.20. The two-dimensional Fourier transform of a function f(x, y) deﬁned for (x, y) ∈R2 is
f(k, l) = 1
2π
	 ∞
−∞
	 ∞
−∞f(x, y) e−i (kx+ly) dx dy.
(7.41)
(a) Compute the Fourier transform of the following functions:
(i) e−| x |−| y |;
(ii) e−x2−y2;
(iii) the delta function δ(x −ξ) δ(y −η),
(iv)
 1,
| x |, | y | ≤1,
0,
otherwise,
(v)
 1,
| x | + | y | ≤1,
0,
otherwise,
(vi) cos(x −y).
(b) Show that if f(x, y) = g(x) h(y), then f(k, l) = g(k) h(l).
(c) What is the formula for the inverse two-dimensional Fourier transform, i.e., how can you
reconstruct f(x, y) from f(k, l)?

7.2 Derivatives and Integrals
275
7.2 Derivatives and Integrals
One of the most signiﬁcant features of the Fourier transform is that it converts calculus
into algebra! More speciﬁcally, the two basic operations in calculus — diﬀerentiation and
integration of functions — are realized as algebraic operations on their Fourier transforms.
(The downside is that algebraic operations become more complicated in the frequency
domain.)
Diﬀerentiation
Let us begin with derivatives.
If we diﬀerentiate† the basic inverse Fourier transform
formula
f(x) ∼
1
√
2π
 ∞
−∞
f(k) e i kx dk
with respect to x, we obtain
f ′(x) ∼
1
√
2π
 ∞
−∞
i k f(k) e i kx dk.
(7.42)
The resulting integral is itself in the form of an inverse Fourier transform, namely of i k f(k),
which immediately implies the following key result.
Proposition 7.7.
The Fourier transform of the derivative f ′(x) of a function is
obtained by multiplication of its Fourier transform by i k:
F[f ′(x)] = i k f(k).
(7.43)
Similarly, the Fourier transform of the product function x f(x) is obtained by diﬀerentiating
the Fourier transform of f(x):
F[x f(x)] = i d f
dk .
(7.44)
The second statement follows easily from the ﬁrst via the Symmetry Principle of
Theorem 7.3. While the result is stated for ordinary functions, as noted earlier, the Fourier
transform — just like Fourier series — is entirely compatible with the calculus of generalized
functions.
Example 7.8.
The derivative of the even exponential pulse fe(x) = e−a| x | is a
multiple of the odd exponential pulse fo(x) = (sign x) e−a| x |:
f ′
e(x) = −a (sign x) e−a| x | = −afo(x).
Proposition 7.7 says that their Fourier transforms are related by
i k fe(k) = i

2
π
ka
k2 + a2 = −a fo(k),
†
We are assuming that the integrand is suﬃciently nice in order to bring the derivative under
the integral sign; see [37, 117] for a fully rigorous justiﬁcation.

276
7 Fourier Transforms
as previously noted in (7.21, 24). On the other hand, the odd exponential pulse has a jump
discontinuity of magnitude 2 at x = 0, and so its derivative contains a delta function:
f ′
o(x) = −a e−a| x | + 2 δ(x) = −afe(x) + 2 δ(x).
This is reﬂected in the relation between their Fourier transforms. If we multiply (7.24) by
i k, we obtain
i k fo(k) =

2
π
k2
k2 + a2 =

2
π −

2
π
a2
k2 + a2 = 2 δ(k) −a fe(k).
Higher-order derivatives are handled by iterating the ﬁrst-order formula (7.43).
Corollary 7.9. The Fourier transform of f (n)(x) is ( i k)n f(k).
This result has an important consequence: the smoothness of the function f(x) is
manifested in the rate of decay of its Fourier transform f(k). We already noted that the
Fourier transform of a (nice) function must decay to zero at large frequencies: f(k) →0
as | k | →∞. (This result can be viewed as the Fourier transform version of the Riemann–
Lebesgue Lemma 3.46.) If the nth derivative f (n)(x) is also a reasonable function, then its
Fourier transform 0
f (n)(k) = ( i k)n f(k) must go to zero as | k | →∞. This requires that
f(k) go to zero more rapidly than | k |−n. Thus, the smoother f(x), the more rapid the
decay of its Fourier transform. As a general rule of thumb, local features of f(x), such as
smoothness, are manifested by global features of f(k), such as the rate of decay for large
| k |. The Symmetry Principle implies that the reverse is also true: global features of f(x)
correspond to local features of f(k). For instance, the degree of smoothness of f(k) governs
the rate of decay of f(x) as x →±∞. This local-global duality is one of the major themes
of Fourier theory.
Integration
Integration is the inverse operation to diﬀerentiation, and so should correspond to division
by i k in frequency space. As with Fourier series, this is not completely correct; there is
an extra constant involved, which contributes an additional delta function.
Proposition 7.10.
If f(x) has Fourier transform f(k), then the Fourier transform
of its integral g(x) =
 x
−∞
f(y) dy is
g(k) = −i
k
f(k) + π f(0) δ(k).
(7.45)
Proof : First notice that
lim
x →−∞g(x) = 0,
lim
x →+∞g(x) =
 ∞
−∞
f(x) dx =
√
2π f(0).
Therefore, if we subtract a suitable multiple of the step function from the integral, the
resulting function
h(x) = g(x) −
√
2π f(0) σ(x)

7.2 Derivatives and Integrals
277
decays to 0 at both ±∞. Consulting our table of Fourier transforms, we ﬁnd
h(k) = g(k) −π f(0) δ(k) + i
k
f(0) .
(7.46)
On the other hand,
h′(x) = f(x) −
√
2π f(0) δ(x).
Since h(x) →0 as | x | →∞, we can apply our diﬀerentiation rule (7.43), and conclude
that
i k h(k) = f(k) −f(0).
(7.47)
Combining (7.46) and (7.47) establishes the desired formula (7.45).
Q.E.D.
Example 7.11. The Fourier transform of the inverse tangent function
f(x) = tan−1 x =
 x
0
dy
1 + y2 =
 x
−∞
dy
1 + y2 −π
2
can be computed by combining Proposition 7.10 with (7.28, 34):
f(k) =

−i
k
π
2
e−| k |
k
+ π3/2
√
2
δ(k)

−π3/2
√
2
δ(k) = −i
π
2
e−| k |
k
.
The singularity at k = 0 reﬂects the lack of decay of the inverse tangent as | x | →∞.
Exercises
7.2.1. Determine the Fourier transform of the following functions:
(a) e−x2/2,
(b) x e−x2/2,
(c) x2 e−x2/2,
(d) x,
(e) x e−2 | x |,
(f ) x tan−1 x.
7.2.2. Find the Fourier transform of
(a) the error function erf x =
2
√π
	 x
0 e−z2
dz;
(b) the complementary error function erfc x =
2
√π
	 ∞
x
e−z2
dz.
7.2.3. Find the inverse Fourier transform of the following functions:
(a) k,
(b) k e−k2,
(c)
k
(1 + k2)2 ,
(d)
k2
k −i ,
(e)
1
k2 −k .
7.2.4. Is the usual formula σ′(x) = δ(x) relating the step and delta functions compatible with
their Fourier transforms? Justify your answer.
7.2.5. Find the Fourier transform of the derivative δ ′(x) of the delta function in three ways:
(a) First, directly from the deﬁnition of δ ′(x); (b) second, using the formula for the Fourier
transform of the derivative of a function; (c) third, as a limit of the Fourier transforms of
the derivatives of the functions in Exercise 7.1.8. (d) Are your answers all the same? If
not, can you explain any discrepancies?
7.2.6. Show that one can obtain the Fourier transform of the Gaussian function f(x) = e−x2/2
by the following trick. First, prove that f ′(k) = −k f(k). Use this to deduce that f(k) =
c e−k2/2 for some constant c. Finally, use the Symmetry Principle to determine c.
7.2.7. If f(x) has Fourier transform f(k), which function has Fourier transform
f(k)
k
?

278
7 Fourier Transforms
♦7.2.8. If f(x) has Fourier transform f(k), what is the Fourier transform of f(x)
x
?
7.2.9. Use Exercise 7.2.8 to ﬁnd the Fourier transform of
(a) 1/x,
(b) x−1e−| x |,
(c) x−1e−x2,
(d) (x3 + 4x)−1.
7.2.10. Directly justify formula (7.43) by integrating the relevant Fourier transform integral by
parts. What do you need to assume about the behavior of f(x) for large | x |?
7.2.11. Given the Fourier transform f(k) of f(x), ﬁnd the Fourier transform of its integral
g(x) =
	 x
a f(y) dy starting at the point a ∈R.
♦7.2.12.(a) Explain why the Fourier transform of a 2π–periodic function f(x) is a linear combi-
nation of delta functions, f(k) =
∞

n=−∞
cn δ(k −n), where cn are the (complex) Fourier
series coeﬃcients (3.65) of f(x) on [−π, π ].
(b) Find the Fourier transform of the following periodic functions:
(i) sin 2x, (ii) cos3 x, (iii) the 2π–periodic extension of f(x) = x,
(iv) the sawtooth function h(x) = x mod 1, i.e., the fractional part of x.
7.2.13. Determine the Fourier transforms of (a) cos x −1, (b) cos x −1
x
,
(c) cos x −1
x2
.
Hint: Use Exercises 7.2.8 and 7.2.12.
♦7.2.14. Write down the formulas for diﬀerentiation and integration for the alternative Fourier
transforms of Exercises 7.1.17 and 7.1.18.
7.2.15.(a) What is the two-dimensional Fourier transform, (7.41), of the gradient ∇f(x, y) of a
function of two variables?
(b) Use your formula to ﬁnd the Fourier transform of the gradient of f(x, y) = e−x2−y2.
7.3 Green’s Functions and Convolution
The fact that the Fourier transform converts diﬀerentiation in the physical domain into
multiplication in the frequency domain is one of its most compelling features. A particularly
important consequence is that it eﬀectively transforms diﬀerential equations into algebraic
equations, and thereby facilitates their solution by elementary algebra. One begins by ap-
plying the Fourier transform to both sides of the diﬀerential equation under consideration.
Solving the resulting algebraic equation will produce a formula for the Fourier transform of
the desired solution, which can then be immediately reconstructed via the inverse Fourier
transform. In the following chapter, we will use these techniques to solve partial diﬀerential
equations.
Solution of Boundary Value Problems
The Fourier transform is particularly well adapted to boundary value problems on the
entire real line. In place of the boundary conditions used on ﬁnite intervals, we look for
solutions that decay to zero suﬃciently rapidly as | x | →∞— in order that their Fourier
transform be well deﬁned (in the context of ordinary functions). In quantum mechanics,
[66, 72], these solutions are known as the bound states, and they correspond to subatomic

7.3 Green’s Functions and Convolution
279
particles that are trapped or localized in a region of space. For example, the electrons in
an atom are bound states localized by the electrostatic attraction of the nucleus.
As a speciﬁc example, consider the boundary value problem
−d2u
dx2 + ω2 u = h(x),
−∞< x < ∞,
(7.48)
where ω > 0 is a positive constant. The boundary conditions require that the solution
decay: u(x) →0, as | x | →∞.
We will solve this problem by applying the Fourier
transform to both sides of the diﬀerential equation. Taking Corollary 7.9 into account, the
result is the linear algebraic equation
k2 u(k) + ω2 u(k) = h(k)
relating the Fourier transforms of u and h. Unlike the diﬀerential equation, the transformed
equation can be immediately solved for
u(k) =
h(k)
k2 + ω2 .
(7.49)
Therefore, we can reconstruct the solution by applying the inverse Fourier transform for-
mula (7.9):
u(x) =
1
√
2π
 ∞
−∞
h(k) e i kx
k2 + ω2 dk.
(7.50)
For example, if the forcing function is an even exponential pulse,
h(x) = e−| x |
with
h(k) =

2
π
1
k2 + 1 ,
then (7.50) writes the solution as a Fourier integral:
u(x) = 1
π
 ∞
−∞
e i kx
(k2 + ω2)(k2 + 1) dk = 1
π
 ∞
−∞
cos kx
(k2 + ω2)(k2 + 1) dk ,
where we note that the imaginary part of the complex integral vanishes because the inte-
grand is an odd function. (Indeed, if the forcing function is real, the solution must also be
real.) The Fourier integral can be explicitly evaluated using partial fractions to rewrite
u(k) =

2
π
1
(k2 + ω2)(k2 + 1) =

2
π
1
ω2 −1

1
k2 + 1 −
1
k2 + ω2

,
ω2 ̸= 1.
Thus, according to our table of Fourier transform, the solution to this boundary value
problem is
u(x) =
e−| x | −1
ω e−ω | x |
ω2 −1
when
ω2 ̸= 1.
(7.51)
The reader may wish to verify that this function is indeed a solution, meaning that it is
twice continuously diﬀerentiable (which is not so immediately apparent from the formula),
decays to 0 as | x | →∞, and satisﬁes the diﬀerential equation everywhere. The “resonant”
case ω2 = 1 is left to Exercise 7.3.6.
Remark: The method of partial fractions that you learned in ﬁrst-year calculus is often
an eﬀective tool for evaluating (inverse) Fourier transforms of such rational functions.

280
7 Fourier Transforms
A particularly important case is that in which the forcing function
h(x) = δξ(x) = δ(x −ξ)
represents a unit impulse concentrated at x = ξ. The resulting solution is the Green’s
function G(x; ξ) for the boundary value problem. According to (7.49), its Fourier transform
with respect to x is
G(k; ξ) =
1
√
2π
e−i kξ
k2 + ω2 ,
which is the product of an exponential factor e−i kξ, representing the Fourier transform of
δξ(x), times a multiple of the Fourier transform of the even exponential pulse e−ω | x |. We
apply the Shift Theorem 7.4, and conclude that the Green’s function for this boundary
value problem is an exponential pulse centered at ξ, namely
G(x; ξ) =
1
2 ω e−ω | x−ξ | = g(x −ξ),
where
g(x) = G(x; 0) =
1
2 ω e−ω | x |.
(7.52)
Observe that, as with other self-adjoint boundary value problems, the Green’s function
is symmetric under interchange of x and ξ, so G(x; ξ) = G(ξ; x). As a function of x, it
satisﬁes the homogeneous diﬀerential equation −u′′ + ω2 u = 0, except at the point x = ξ,
where its derivative has a jump discontinuity of unit magnitude. It also decays as | x | →∞,
as required by the boundary conditions. The fact that G(x; ξ) = g(x −ξ) depends only
on the diﬀerence x −ξ is a consequence of the translation invariance of the boundary
value problem. The superposition principle based on the Green’s function tells us that the
solution to the inhomogeneous boundary value problem (7.48) under a general forcing can
be represented in the integral form
u(x) =
 ∞
−∞
G(x; ξ) h(ξ) dξ =
 ∞
−∞
g(x −ξ) h(ξ) dξ = 1
2ω
 ∞
−∞
e−ω| x−ξ | h(ξ) dξ.
(7.53)
The reader may enjoy recovering the particular exponential solution (7.51) from this inte-
gral formula.
Exercises
7.3.1. Use partial fractions to compute the inverse Fourier transform of the following rational
functions. Hint: First solve Exercise 7.1.3.
(a)
1
k2 −5k −6 ,
(b)
e i k
k2 −1 ,
(c)
1
k4 −1 ,
(d)
sin 2k
k2 + 2k −3 .
7.3.2. Find the inverse Fourier transform of the function
1
k2 + 2k + 5 :
(a) using partial fractions; (b) by completing the square. Are your answers the same?
7.3.3. Use partial fractions to compute the Fourier transform of the following functions:
(a)
1
x2 −x −2 ,
(b)
1
x3 + x ,
(c)
cos x
x2 −9 .
7.3.4. Find a solution to the diﬀerential equation −d2u
dx2 + 4u = δ(x) using the Fourier trans-
form.

7.3 Green’s Functions and Convolution
281
7.3.5. Use the Fourier transform to solve the boundary value problem
−u′′ + u = δ′(x −1) for −∞< x < ∞, with u(x) →0 as x →±∞.
♦7.3.6.(a) Use the Fourier transform to solve (7.48) with h(x) = e−| x | when ω = 1.
(b) Verify that your solution can be obtained as a limit of (7.51) as ω →1.
7.3.7. Use the Fourier transform to ﬁnd a bounded solution to the diﬀerential equation
u′′′′ + u = e−2 | x |.
7.3.8. Use the Fourier transform to ﬁnd an integral formula for a bounded solution to the Airy
diﬀerential equation −d2u
dx2 = x u.
♦7.3.9. Prove that (7.51) is a twice continuously diﬀerentiable function of x and satisﬁes the dif-
ferential equation (7.48).
Convolution
In our solution to the boundary value problem (7.48), we ended up deriving a formula for
its Fourier transform (7.49) as the product of two known Fourier transforms. The ﬁnal
Green’s function formula (7.53), obtained by applying the inverse Fourier transform, is
indicative of a general property, in that it is given by a convolution product.
Deﬁnition 7.12.
The convolution of scalar functions f(x) and g(x) is the scalar
function h = f ∗g deﬁned by the formula
h(x) = f ∗g(x) =
 ∞
−∞
f(x −ξ) g(ξ) dξ.
(7.54)
We list the basic properties of the convolution product, leaving their veriﬁcation as
exercises for the reader. All of these assume that the implied convolution integrals converge.
(a)
Symmetry:
f ∗g = g ∗f,
(b)
Bilinearity:

f ∗(ag + bh) = a(f ∗g) + b(f ∗h),
(af + bg) ∗h = a(f ∗h) + b(g ∗h),
a, b ∈C,
(c)
Associativity:
f ∗(g ∗h) = (f ∗g) ∗h,
(d)
Zero function:
f ∗0 = 0,
(e)
Delta function:
f ∗δ = f.
One tricky feature is that the constant function 1 is not a unit for the convolution
product; indeed,
f ∗1 = 1 ∗f =
 ∞
−∞
f(ξ) dξ
is a constant function, namely the total integral of f, and not the original function f(x). In
fact, according to the ﬁnal property, the delta function plays the role of the “convolution
unit”:
f ∗δ(x) =
 ∞
−∞
f(x −ξ) δ(ξ) dξ = f(x).

282
7 Fourier Transforms
In particular, our solution (7.52) has the form of a convolution product between an
even exponential pulse g(x) = (2ω)−1 e−ω| x | and the forcing function:
u(x) = g ∗h(x).
On the other hand, its Fourier transform (7.49) is, up to a factor, the ordinary multiplicative
product
u(k) =
√
2π g(k) h(k)
of the Fourier transforms of g and h. In fact, this is a general property of the Fourier trans-
form: convolution in the physical domain corresponds to multiplication in the frequency
domain, and conversely.
Theorem 7.13.
The Fourier transform of the convolution h(x) = f ∗g(x) of two
functions is a multiple of the product of their Fourier transforms:
h(k) =
√
2π f(k) g(k).
(7.55)
Conversely, the Fourier transform of their product h(x) = f(x) g(x) is, up to a multiple,
the convolution of their Fourier transforms:
h(k) =
1
√
2π
f ∗g(k) =
1
√
2π
 ∞
−∞
f(k −κ) g(κ) dκ.
(7.56)
Proof : Combining the deﬁnition of the Fourier transform with the convolution for-
mula (7.54), we obtain
h(k) =
1
√
2π
 ∞
−∞
h(x) e−i kx dx =
1
√
2π
 ∞
−∞
 ∞
−∞
f(x −ξ) g(ξ) e−i kx dx dξ.
Applying the change of variables η = x −ξ in the inner integral produces
h(k) =
1
√
2π
 ∞
−∞
 ∞
−∞
f(η) g(ξ) e−i k(ξ+η) dξ dη
=
√
2π

1
√
2π
 ∞
−∞
f(η) e−i kη dη
 
1
√
2π
 ∞
−∞
g(ξ) e−i kξ dξ

=
√
2π f(k) g(k),
proving (7.55). The second formula can be proved in a similar fashion, or by simply noting
that it follows directly from the Symmetry Principle of Theorem 7.3.
Q.E.D.
Example 7.14. We already know, (7.29), that the Fourier transform of
f(x) = sin x
x
is the box function
f(k) =
π
2

σ(k + 1) −σ(k −1)

=
⎧
⎨
⎩
π
2 ,
−1 < k < 1,
0,
| k | > 1.
We also know that the Fourier transform of
g(x) = 1
x
is
g(k) = −i
π
2 sign k.
Therefore, the Fourier transform of their product
h(x) = f(x) g(x) = sin x
x2

7.3 Green’s Functions and Convolution
283
can be obtained by convolution:
h(k) =
1
√
2π
f ∗g(k) =
1
√
2π
 ∞
−∞
f(κ) g(k −κ) dκ
= −i
π
8
 1
−1
sign(k −κ) dκ =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
i
π
2
k < −1,
−i
π
2 k,
−1 < k < 1,
−i
π
2
k > 1.
Exercises
7.3.10.(a) Find the Fourier transform of the convolution h(x) = fe ∗g(x) of an even exponential
pulse fe(x) = e−| x | and a Gaussian g(x) = e−x2.
(b) What is h(x)?
7.3.11. What is the convolution of a Gaussian kernel e−x2 with itself? Hint: Use the Fourier
transform.
7.3.12. Find the function whose Fourier transform is f(k) = (k2 + 1)−2.
♥7.3.13.(a) Write down the Fourier transform of the box function f(x) =
⎧
⎪
⎨
⎪
⎩
1,
| x | < 1
2 ,
0,
| x | > 1
2 .
(b) Graph the hat function h(x) = f ∗f(x) and ﬁnd its Fourier transform.
(c) Determine the cubic B spline s(x) = h ∗h(x) and its Fourier transform.
7.3.14. Let f(x) =
 sin x,
0 < x < π,
0,
otherwise,
g(x) =
 cos x,
0 < x < π,
0,
otherwise.
(a) Find the Fourier transforms of f(x) and g(x); (b) compute the convolution
h(x) = f ∗g(x); (c) ﬁnd its Fourier transform h(k).
7.3.15. Use convolution to ﬁnd an integral formula for the function whose Fourier transform is
(a)
e−k2
k2 + 1 ,
(b)
sin k
k(k2 + 1) ,
(c) sin2 k
k2
,
(d)
sign k
1 + i k .
If possible, evaluate the resulting convolution integral.
7.3.16. Let f(x) be a smooth function. (a) Find its convolution δ ′ ∗f with the derivative of the
delta ﬁunction.
(b) More generally, ﬁnd δ(n) ∗f.
7.3.17. According to Proposition 7.7, the Fourier transform of the derivative f′(x) is obtained
by multiplying f(k) by i k. Can you reconcile this result with the Convolution Theorem 7.13?
♦7.3.18. The Hilbert transform of a function f(x) is deﬁned as the integral
h(x) = 1
π
	 ∞
−∞
−
f(ξ) dξ
ξ −x .
(7.57)
Find a formula for its Fourier transform h(k) in terms of f(k). Remark: The bar on the
integral indicates the principal value integral, [2], which is
lim
δ →0+
 	 x−δ
−∞+
	 ∞
x+δ
 f(ξ) dξ
ξ −x ,
and is employed to avoid the integral diverging at the singular point x = ξ.

284
7 Fourier Transforms
7.3.19. Use the Fourier transform to solve the integral equation
	 ∞
−∞e−| x−ξ | u(ξ) dξ = f(x).
Then verify your solution when f(x) = e−2 | x |.
7.3.20. Suppose that f(x) and g(x) are identically 0 for all x < 0. Prove that their convolution
product h = f ∗g reduces to a ﬁnite integral: h(x) =
⎧
⎨
⎩
	 x
0 f(x −ξ) g(ξ) dξ,
x > 0,
0,
x ≤0.
7.3.21. Given that the support of f(x) is contained in the interval [a, b] and the support of
g(x) is contained in [c, d], what can you say about the support of their convolution
h(x) = f ∗g(x)?
♦7.3.22. Prove the convolution properties (a–e).
♦7.3.23. In this exercise, we explain how convolution can be used to smooth out rough data. Let
gε(x) =
ε
π(ε2 + x2) . (a) If f(x) is any (reasonable) function, show that fε(x) = gε ∗f(x)
for ε ̸= 0 is a C∞function. (b) Show that lim
ε →0 fε(x) = f(x).
7.3.24. Explain why the Shift Theorem 7.4 is a special case of the Convolution Theorem 7.13.
♦7.3.25. Suppose f(x) and g(x) are 2π–periodic and have respective complex Fourier coeﬃcients
ck and dk. Prove that the complex Fourier coeﬃcients ek of the product function f(x) g(x)
are given by the convolution summation ek =
∞

j =−∞
cj dk−j. Hint: Substitute the formulas
for the complex Fourier coeﬃcients into the summation, making sure to use two diﬀerent
integration variables, and then use (6.37).
7.4 The Fourier Transform on Hilbert Space
While we do not possess all the analytic tools to embark on a fully rigorous treatment of the
mathematical theory underlying the Fourier transform, it is worth outlining a few of the
more important features. We have already noted that the Fourier transform, when deﬁned,
is a linear operator, taking functions f(x) on physical space to functions f(k) on frequency
space. A critical question is the following: to precisely which function space should the
theory be applied? Not every function admits a Fourier transform in the classical sense†
— the Fourier integral (7.6) is required to converge, and this places restrictions on the
function and its asymptotics at large distances.
It turns out the proper setting for the rigorous theory is the Hilbert space of complex-
valued square-integrable functions — the same inﬁnite-dimensional vector space that lies
at the heart of modern quantum mechanics. In Section 3.5, we already introduced the
Hilbert space L2[a, b] on a ﬁnite interval; here we adapt Deﬁnition 3.34 to the entire real
line. Thus, the Hilbert space L2 = L2(R) is the inﬁnite-dimensional vector space consisting
of all complex-valued functions f(x) that are deﬁned for all x ∈R and have ﬁnite L2 norm:
∥f ∥2 =
 ∞
−∞
| f(x) |2 dx < ∞.
(7.58)
†
We leave aside the more advanced issues involving generalized functions.

7.4 The Fourier Transform on Hilbert Space
285
For example, any piecewise continuous function that satisﬁes the decay criterion
| f(x) | ≤
M
| x |1/2+δ ,
for all suﬃciently large
| x | ≫0,
(7.59)
for some M > 0 and δ > 0, belongs to L2. However, as in Section 3.5, Hilbert space contains
many more functions, and the precise deﬁnitions and identiﬁcation of its elements is quite
subtle. On the other hand, most nondecaying functions do not belong to L2, including the
constant function f(x) ≡1 as well as all oscillatory complex exponentials, e i kx for k ∈R.
The Hermitian inner product on the complex Hilbert space L2 is prescribed in the
usual manner,
⟨f , g ⟩=
 ∞
−∞
f(x) g(x) dx,
(7.60)
so that ∥f ∥2 = ⟨f , f ⟩. The Cauchy–Schwarz inequality
| ⟨f , g ⟩| ≤∥f ∥∥g ∥
(7.61)
ensures that the inner product integral is ﬁnite whenever f, g ∈L2. Observe that the
Fourier transform (7.6) can be regarded as a multiple of the inner product of the function
f(x) with the complex exponential functions:
f(k) =
1
√
2π
 ∞
−∞
f(x) e−i kx dx =
1
√
2π
⟨f(x) , e i kx ⟩.
(7.62)
However, when interpreting this formula, one must bear in mind that the exponentials are
not themselves elements of L2.
Let us state the fundamental result governing the eﬀect of the Fourier transform
on functions in Hilbert space. It can be regarded as a direct analogue of the Pointwise
Convergence Theorem 3.8 for Fourier series.
Theorem 7.15. If f(x) ∈L2 is square-integrable, then its Fourier transform f(k) ∈
L2 is a well-deﬁned, square-integrable function of the frequency variable k.
If f(x) is
continuously diﬀerentiable at a point x, then the inverse Fourier transform integral (7.9)
equals its value f(x).
More generally, if the left- and right-hand limits f(x−), f(x+),
f ′(x−), f ′(x+) exist, then the inverse Fourier transform integral converges to the average
value 1
2

f(x−) + f(x+)

.
Thus, the Fourier transform f = F[f ] deﬁnes a linear transformation from L2 func-
tions of x to L2 functions of k. In fact, the Fourier transform preserves inner products.
This important result is known as Parseval’s formula, whose Fourier series counterpart
appeared in (3.122).
Theorem 7.16. If f(k) = F[f(x)] and g(k) = F[g(x)], then ⟨f , g ⟩= ⟨f , g ⟩, i.e.,
 ∞
−∞
f(x) g(x) dx =
 ∞
−∞
f(k) g(k) dk.
(7.63)
Proof : Let us sketch a formal proof that serves to motivate why this result is valid.
We use the deﬁnition (7.6) of the Fourier transform to evaluate
 ∞
−∞
f(k) g(k) dk =
 ∞
−∞

1
√
2π
 ∞
−∞
f(x) e−i kx dx
 
1
√
2π
 ∞
−∞
g(y) e+ i ky dy

dk
=
 ∞
−∞
 ∞
−∞
f(x) g(y)
 1
2π
 ∞
−∞
e−i k(x−y) dk

dx dy.

286
7 Fourier Transforms
Now according to (7.38), the inner k integral can be replaced by the delta function δ(x−y),
and hence
 ∞
−∞
f(k) g(k) dk =
 ∞
−∞
 ∞
−∞
f(x) g(y) δ(x −y) dx dy =
 ∞
−∞
f(x) g(x) dx.
This completes our “proof”; see [37, 68, 117] for a rigorous version.
Q.E.D.
In particular, orthogonal functions, satisfying ⟨f , g ⟩= 0, will have orthogonal Fourier
transforms, ⟨f , g ⟩= 0. Choosing f = g in Parseval’s formula (7.63) produces Plancherel’s
formula
∥f ∥2 = ∥f ∥2,
or, explicitly,
 ∞
−∞
| f(x) |2 dx =
 ∞
−∞
| f(k) |2 dk.
(7.64)
Thus, the Fourier transform F: L2 →L2 deﬁnes a norm-preserving, or unitary, linear
transformation on Hilbert space, mapping L2 functions of the physical variable x to L2
functions of the frequency variable k.
Quantum Mechanics and the Uncertainty Principle
In its popularized form, the Heisenberg Uncertainty Principle is a by now familiar philo-
sophical concept. First formulated in the 1920s by the German physicist Werner Heisen-
berg, one of the founders of modern quantum mechanics, it states that, in a physical
system, certain quantities cannot be simultaneously measured with complete accuracy.
For instance, the more precisely one measures the position of a particle, the less accuracy
there will be in the measurement of its momentum; conversely, the greater the accuracy
in the momentum, the less certainty in its position. A similar uncertainty couples energy
and time. Experimental veriﬁcation of the uncertainty principle can be found even in fairly
simple situations. Consider a light beam passing through a small hole. The position of the
photons is constrained by the hole; the eﬀect of their momenta is observed in the pattern
of light diﬀused on a screen placed beyond the hole. The smaller the hole, the more con-
strained the photon’s position as it passes through, hence, according to the Uncertainty
Principle, the less certainty there is in the observed momentum, and, consequently, the
wider and more diﬀuse the resulting image on the screen.
This is not the place to discuss the philosophical and experimental consequences of
Heisenberg’s Principle. What we will show is that the Uncertainty Principle is, in fact, a
mathematical property of the Fourier transform! In quantum theory, each of the paired
quantities, e.g., position and momentum, are interrelated by the Fourier transform. Indeed,
Proposition 7.7 says that the Fourier transform of the diﬀerentiation operator representing
momentum is a multiplication operator representing position and vice versa. This Fourier-
transform-based duality between position and momentum, that is, between multiplication
and diﬀerentiation, lies at the heart of the Uncertainty Principle.
In quantum mechanics, the wave functions of a quantum system are characterized as
the elements of unit norm, ∥ϕ ∥= 1, belonging to the underlying state space, which, in
a one-dimensional model of a single particle, is the Hilbert space L2 = L2(R) consisting
of square-integrable complex-valued functions of x. As we already noted in Section 3.5,
the squared modulus of the wave function, | ϕ(x) |2, represents the probability density of
the particle being found at position x. Consequently, the mean or expected value of any

7.4 The Fourier Transform on Hilbert Space
287
function f(x) of the position variable is given by its integral against the system’s probability
density and denoted by
⟨f(x) ⟩=
 ∞
−∞
f(x) | ϕ(x) |2 dx.
(7.65)
In particular,
⟨x ⟩=
 ∞
−∞
x | ϕ(x) |2 dx
(7.66)
is the expected measured position of the particle, while Δx, deﬁned by
(Δx)2 = ⟨

x −⟨x ⟩
2 ⟩= ⟨x2 ⟩−⟨x ⟩2 ,
(7.67)
is the variance, that is, the statistical deviation of the particle’s measured position from
the mean. We note that the next-to-last term equals
⟨x2 ⟩=
 ∞
−∞
x2 | ϕ(x) |2 dx = ∥x ϕ(x) ∥2.
(7.68)
On the other hand, the momentum variable p is related to the Fourier transform
frequency via the de Broglie relation p = ℏk, where
ℏ= h
2π ≈1.055 × 10−34 joule seconds
(7.69)
is Planck’s constant, whose value governs the quantization of physical quantities. There-
fore, the mean, or expected value, of any function of momentum g(p) is given by its integral
against the squared modulus of the Fourier transformed wave function:
⟨g(p) ⟩=
 ∞
−∞
g(ℏk) | ϕ(k) |2 dk.
(7.70)
In particular, the mean of the momentum measurements of the particle is
⟨p ⟩= ℏ
 ∞
−∞
k | ϕ(k) |2 dk = −i ℏ
 ∞
−∞
ϕ′(x) ϕ(x) dx = −i ℏ⟨ϕ′ , ϕ ⟩,
(7.71)
where we used Parseval’s formula (7.63) to convert to an integral over position, and (7.43)
to infer that k ϕ(k) is the Fourier transform of −i ϕ′(x). Similarly,
(Δp)2 = ⟨

p −⟨p ⟩
2 ⟩= ⟨p2 ⟩−⟨p ⟩2
(7.72)
is the squared variance of the momentum, where, by Plancherel’s formula (7.64) and (7.43),
⟨p2 ⟩= ℏ2
 ∞
−∞
k2 | ϕ(k) |2 dk = ℏ2
 ∞
−∞
| i k ϕ(k) |2 dk
= ℏ2
 ∞
−∞
| ϕ′(x) |2 dx = ℏ2 ∥ϕ′(x) ∥2.
(7.73)
With this interpretation, the Uncertainty Principle for position and momentum mea-
surements can be stated.

288
7 Fourier Transforms
Theorem 7.17. If ϕ(x) is a wave function, so ∥ϕ ∥= 1, then the observed variances
in position and momentum satisfy the inequality
Δx Δp ≥1
2 ℏ.
(7.74)
Now, the smaller the variance of a quantity such as position or momentum, the more
accurate will be its measurement. Thus, the Heisenberg inequality (7.74) eﬀectively quan-
tiﬁes the statement that the more accurately we are able to measure the momentum p, the
less accurate will be any measurement of its position x, and vice versa. For more details,
along with physical and experimental consequences, you should consult an introductory
text on mathematical quantum mechanics, e.g., [66, 72].
Proof : For any value of the real parameter t,
0 ≤∥t x ϕ(x) + ϕ′(x) ∥2
= t2 ∥x ϕ(x) ∥2 + t

⟨ϕ′(x) , x ϕ(x) ⟩+ ⟨x ϕ(x) , ϕ′(x) ⟩

+ ∥ϕ′(x) ∥2.
(7.75)
The middle term in the ﬁnal expression can be evaluated as follows:
⟨ϕ′(x) , x ϕ(x) ⟩+ ⟨x ϕ(x) , ϕ′(x) ⟩=
 ∞
−∞

x ϕ′(x) ϕ(x) + x ϕ(x) ϕ′(x)

dx
=
 ∞
−∞
x d
dx | ϕ(x) |2 dx = −
 ∞
−∞
| ϕ(x) |2 dx = −1,
via an integration by parts, noting that the boundary terms vanish, provided ϕ(x) satisﬁes
the L2 decay criterion (7.59). Thus, in view of (7.68) and (7.73), the inequality in (7.75)
reads
⟨x2 ⟩t2 −t + ⟨p2 ⟩
ℏ2
≥0
for all
t ∈R.
The minimum value of the left-hand side occurs at t⋆= 1/(2 ⟨x2 ⟩), where its value is
⟨p2 ⟩
ℏ2
−
1
4 ⟨x2 ⟩≥0,
which implies
⟨x2 ⟩⟨p2 ⟩≥1
4 ℏ2.
To obtain the uncertainty relation (7.74), one performs the selfsame calculation, but with
x −⟨x ⟩replacing x and p −⟨p ⟩replacing p. The result is
1
(x −⟨x ⟩)2 2
t2 −t +
1
(p −⟨p ⟩)2 2
ℏ2
= (Δx)2 t2 −t + (Δp)2
ℏ2
≥0.
(7.76)
Substituting t = 1/(2(Δx)2) produces the Heisenberg inequality (7.74).
Q.E.D.
Exercises
7.4.1.(a) Write out the Plancherel formula for the square wave pulse f(x) =
 1,
| x | < 1,
0,
| x | > 1.
(b) What is
	 ∞
0
sin2 x
x2
dx?

7.4 The Fourier Transform on Hilbert Space
289
7.4.2. Apply the Plancherel formula to the even decaying pulse (7.20) to evaluate
	 ∞
−∞
dx
(a2 + x2)2 . How would you compute this integral using elementary calculus?
♥7.4.3.(a) Find the Fourier transform of the function fn(x) =

−n2 sign x,
| x | < 1
n,
0,
otherwise, where
n is a positive integer. (b) Write out the Plancherel formula for fn(x). (c) Determine the
limit, as n →∞, of the Fourier transform of fn(x). (d) Explain why the limit should be
the Fourier transform of the derivative of the delta function δ ′(x).
7.4.4. Prove that Parseval’s formula is a consequence of Plancherel’s formula. Hint: Use the
identity in Exercise 3.5.34(b).
♦7.4.5. Prove that the Hilbert space L2(R) is a complex vector space.
♦7.4.6. We did not quite tell the truth when we said that L2 functions must decay at large dis-
tances: Prove that the following function is in L2 but does not go to zero as | x | →∞:
f(x) =

1,
n −n−2 < x < n + n−2
for
n = ±1, ±2, ±3, . . . ,
0,
otherwise.
7.4.7. Modify the function in Exercise 7.4.6 to produce a function f ∈L2 that nevertheless
satisﬁes
lim
n →±∞f(n) = ∞for n ∈Z.
♦7.4.8. Suppose f ∈L2 is continuously diﬀerentiable, f ∈C1, and has bounded derivative:
| f′(x) | ≤M for all x ∈R. Prove that f(x) →0 as x →±∞.
7.4.9.(a) Find the constant a > 0 such that ϕ(x) = a e−| x | is a wave function.
(b) Verify the Heisenberg inequality (7.74) for this particular wave function.
7.4.10. Answer Exercise 7.4.9 when
(a) ϕ(x) = a e−x2;
(b) ϕ(x) =
a
1 + x2 .
♦7.4.11. Write out a detailed derivation of the ﬁnal inequality (7.76).

Chapter 8
Linear and Nonlinear Evolution Equations
The term evolution equation refers to a dynamical partial diﬀerential equation that involves
both time t and space x = (x1, . . . , xn) as independent variables and takes the form
∂u
∂t = K[u],
(8.1)
whose left-hand side is just the ﬁrst-order time derivative of the dependent variable u,
while the right-hand side, which can be linear or nonlinear, involves only u and its space
derivatives and, possibly, t and x. Examples already encountered include the linear and
nonlinear transport equations in Chapter 2 and the heat equation. (But not the wave
equation or Laplace equation.) In this chapter, we will analyze several important evolution
equations, both linear and nonlinear, involving a single spatial variable.
Our ﬁrst stop is to revisit the heat equation. We introduce the fundamental solution,
which, for dynamical partial diﬀerential equations, assumes the role of the Green’s function,
in that its initial condition is a concentrated delta impulse. The fundamental solution leads
to an integral superposition formula for the solutions produced by more general initial
conditions or by external forcing. For the heat equation on the entire real line, the Fourier
transform enables us to construct an explicit formula that identiﬁes its fundamental solution
as a Gaussian ﬁlter.
We next present the Maximum Principle that rigorously justiﬁes
the entropic decay of temperature in a heated body and underlies much of the advanced
mathematical analysis of parabolic partial diﬀerential equations. Finally, we discuss the
Black–Scholes equation, the paradigmatic model for investment portfolios, ﬁrst proposed
in the early 1970s and now lying at the heart of the modern ﬁnancial industry. We will
ﬁnd that the Black–Scholes equation can be transformed into the linear heat equation,
whose fundamental solution is applied to establish the celebrated Black–Scholes formula
for option pricing.
The following section provides a brief introduction to symmetry-based solution tech-
niques for linear and nonlinear partial diﬀerential equations. Knowing a symmetry of a
partial diﬀerential equation allows one to readily construct additional solutions from any
known solution. Solutions that remain invariant under a one-parameter family of symme-
tries can be found by solving a reduced ordinary diﬀerential equation. The most important
are the traveling wave solutions, which are invariant under translation symmetries, and
similarity solutions, which are invariant under scaling symmetries.
The next evolution equation to appear is a paradigmatic model of nonlinear diﬀusion
known as Burgers’ equation. It can be regarded as a very simpliﬁed model of ﬂuid dynamics,
combining both nonlinear and viscous eﬀects. We discover a remarkable nonlinear change
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
8
291
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

292
8 Linear and Nonlinear Evolution Equations
of variables that maps Burgers’ equation to the linear heat equation, and thereby facilitates
its analysis, allowing us to construct explicit solutions, and investigate how they converge
to shock wave solutions of the nonlinear transport equation in the inviscid limit.
Next, we turn our attention to the simplest third-order linear evolution equation, which
arises as a model for wave mechanics. Unlike ﬁrst- and second-order wave equations, its
solutions are not simple traveling waves, but instead exhibit dispersion, in which oscillatory
waves of diﬀerent frequencies move at diﬀerent speeds.
As a result, initially localized
disturbances will spread out or disperse, even while they conserve the underlying energy.
Dispersion implies that the individual wave velocities diﬀer from the group velocity, which
measures the speed of propagation of energy in the system. An everyday manifestation of
this phenomenon can be observed in the ripples caused by throwing a rock into a pond:
the individual waves move faster than the overall disturbance.
Finally, we present the
remarkable Talbot eﬀect, only recently discovered, in which solutions having discontinuous
initial data and subject to periodic boundary conditions exhibit radically diﬀerent proﬁles
at rational and irrational times.
Our ﬁnal example is the celebrated Korteweg–de Vries equation, which originally arose
in the work of the nineteenth-century French applied mathematician Joseph Boussinesq as
a model for surface waves on shallow water. It combines the eﬀects of linear dispersion and
nonlinear transport. Unlike the linearly dispersive model, the Korteweg–de Vries equation
admits explicit, localized traveling wave solutions, now known as “solitons”.
Remark-
ably, despite the potentially complicated nonlinear nature of their interaction, two solitons
emerge from a collision with their individual proﬁles preserved, the only residual eﬀect
being a relative phase shift. The Korteweg–de Vries equation is the prototype of a com-
pletely integrable partial diﬀerential equation, whose many remarkable properties were
ﬁrst discovered in the mid 1960s. A surprising number of such completely integrable non-
linear systems appear in a variety of applications, including dynamical models in ﬂuids,
plasmas, optics, and solid mechanics. Their analysis remains an extremely active area of
contemporary research, [2, 36].
8.1 The Fundamental Solution to the Heat Equation
One disadvantage of the Fourier series solution to the heat equation is that it is not nearly
as explicit as one might desire for practical applications, numerical computations, or even
further theoretical investigations and developments. An alternative approach is based on
the idea of the fundamental solution, which plays the role of the Green’s function in solving
initial value problems. The fundamental solution measures the eﬀect of a concentrated,
instantaneous impulse, either in the initial conditions or as an external force on the system.
We restrict our attention to homogeneous boundary conditions — keeping in mind
that these can always be included by use of linear superposition.
The basic idea is to
analyze the case in which the initial data u(0, x) = δξ(x) = δ(x −ξ) is a delta function,
which we can interpret as a highly concentrated unit heat source, e.g., a soldering iron or
laser beam, that is instantaneously applied at a position ξ along a metal bar. The heat
will diﬀuse away from its initial concentration, and the resulting fundamental solution is
denoted by
u(t, x) = F(t, x; ξ),
with
F(0, x; ξ) = δ(x −ξ).
(8.2)
For each ﬁxed ξ, the fundamental solution, considered as a function of t > 0 and x, must

8.1 The Fundamental Solution to the Heat Equation
293
satisfy the underlying partial diﬀerential equation, and so, for the heat equation,
∂F
∂t = γ ∂2F
∂x2 ,
(8.3)
along with the speciﬁed homogeneous boundary conditions.
As with the Green’s function, once we have determined the fundamental solution, we
can then use linear superposition to reconstruct the general solution to the initial-boundary
value problem. Namely, we ﬁrst write the initial data
u(0, x) = f(x) =
 b
a
δ(x −ξ) f(ξ) dξ
(8.4)
as a superposition of delta functions, as in (6.16).
Linearity implies that the solution
can be expressed as the corresponding superposition of the responses to those individual
concentrated delta proﬁles:
u(t, x) =
 b
a
F(t, x; ξ) f(ξ) dξ.
(8.5)
Assuming that we can diﬀerentiate under the integral sign, the fact that F(t, x; ξ) satis-
ﬁes the diﬀerential equation and the homogeneous boundary conditions for each ﬁxed ξ
immediately implies that the integral (8.5) is also a solution with the correct initial and
(homogeneous) boundary conditions.
Unfortunately, most boundary value problems do not have fundamental solutions that
can be written down in closed form. An important exception is the case of an inﬁnitely
long homogeneous bar, which requires solving the heat equation on the entire real line:
∂u
∂t = ∂2u
∂x2 ,
for
−∞< x < ∞,
t > 0.
(8.6)
For simplicity, we have chosen units in which the thermal diﬀusivity is γ = 1. The solution
u(t, x) is deﬁned for all x ∈R, and has initial conditions
u(0, x) = f(x)
for
−∞< x < ∞.
(8.7)
In order to specify the solution uniquely, we shall require that the temperature be square-
integrable, i.e., in L2, at all times, so that
 ∞
−∞
| u(t, x) |2 dx < ∞
for all
t ≥0.
(8.8)
Roughly speaking, square-integrability requires that the temperature be vanishingly small
at large distances, and hence plays the role of boundary conditions in this context.
To solve the initial value problem (8.6–7), we apply the Fourier transform, in the x
variable, to both sides of the diﬀerential equation. In view of the eﬀect of the Fourier
transform on derivatives, cf. (7.43), the result is
∂u
∂t = −k2 u,
(8.9)
where
u(t, k) =
1
√
2π
 ∞
−∞
u(t, x) e−i kx dx
(8.10)

294
8 Linear and Nonlinear Evolution Equations
t = .05
t = .1
t = 1
t = 10
Figure 8.1.
The fundamental solution to the one-dimensional heat equation.

is the Fourier transformed solution. For each ﬁxed k, (8.9) can be viewed as a ﬁrst-order
linear ordinary diﬀerential equation for u(t, k), with initial conditions
u(0, k) = f(k) =
1
√
2π
 ∞
−∞
f(x) e−i kx dx
(8.11)
given by Fourier transforming the initial data (8.7).
The solution to the initial value
problem (8.9, 11) is immediate:
u(t, k) = e−k2 t f(k).
(8.12)
We can thus recover the solution to the initial value problem (8.6–7) by applying the inverse
Fourier transform to (8.12), leading to the explicit integral formula
u(t, x) =
1
√
2π
 ∞
−∞
e i kx u(t, k) dk =
1
√
2π
 ∞
−∞
e i kx−k2 t f(k) dk.
(8.13)
In particular, to construct the fundamental solution, we take the initial temperature
proﬁle to be a delta function δξ(x) = δ(x −ξ) concentrated at x = ξ. According to (7.37),
its Fourier transform is
δξ(k) = e−i kξ
√
2π
.
Plugging this into (8.13), and then referring to our table of Fourier transforms, we are led
to the following explicit formula for the fundamental solution:
F(t, x; ξ) = 1
2π
 ∞
−∞
e i k(x−ξ)−k2 t dk =
1
2
√
π t e−(x−ξ)2/(4t)
for
t > 0.
(8.14)
As you can verify, for each ﬁxed ξ, the function F(t, x; ξ) is indeed a solution to the heat
equation for all t > 0. In addition,
lim
t →0+ F(t, x; ξ) =
 0,
x ̸= ξ,
∞,
x = ξ.

8.1 The Fundamental Solution to the Heat Equation
295
Furthermore, its integral
 ∞
−∞
F(t, x; ξ) dx = 1
(8.15)
is constant — in accordance with the law of conservation of thermal energy; see Exercise
8.1.20.
Therefore, as t →0+, the fundamental solution satisﬁes the original limiting
deﬁnition (6.8–9) of the delta function, and so F(0, x; ξ) = δξ(x) has the desired initial
temperature proﬁle.
In Figure 8.1, we graph F(t, x; 0) at the indicated times.
It starts life as a delta
spike concentrated at the origin, and then immediately smooths out into a tall and narrow
bell-shaped curve, centered at x = 0. As time increases, the solution shrinks and widens,
eventually decaying everywhere to zero. Its amplitude is proportional to t−1/2, while its
overall width is proportional to t1/2. The thermal energy (8.15), which is the area under
the graph, remains ﬁxed while gradually spreading out over the entire real line.
Remark: In probability, these exponentially bell-shaped curves are known as normal or
Gaussian distributions, [39]. The width of the bell curve measures its standard deviation.
For this reason, the fundamental solution to the heat equation is sometimes referred to as
a Gaussian ﬁlter.
Remark: The fact that the fundamental solution depends only on the diﬀerence x−ξ,
and hence has the same proﬁle at all ξ ∈R, is a consequence of the translation invariance
of the heat equation, reﬂecting the fact that it models the thermodynamics of a uniform
medium. See Section 8.2 for additional symmetry properties of the heat equation and its
solutions.
Remark: One of the striking properties of the heat equation is that thermal energy
propagates with inﬁnite speed. Indeed, because, at any t > 0, the fundamental solution
is nonzero for all x, the eﬀect of an initial concentration of heat will immediately be felt
along the entire length of an inﬁnite bar. (The graphs in Figure 8.1 are a little misleading
because they fail to show the extremely small, but still positive, exponentially decreasing
tails.) This eﬀect, while more or less negligible at large distances, is nevertheless in clear
violation of physical intuition — not to mention relativity, which postulates that signals
cannot propagate faster than the speed of light. Despite this non-physical artifact, the heat
equation remains an accurate model for heat propagation and similar diﬀusive phenomena,
and so continues to be successfully used in applications.
With the fundamental solution in hand, we can adapt the linear superposition for-
mula (8.5) to reconstruct the general solution
u(t, x) =
1
2
√
π t
 ∞
−∞
e−(x−ξ)2/(4t) f(ξ) dξ
(8.16)
to our initial value problem (8.6). This solution formula is merely a restatement of (8.13)
combined with the Fourier transform formula (8.11). Comparing with (7.54), we see that
the solutions are obtained by convolution of the initial data with a one-parameter family
of progressively wider and shorter Gaussian ﬁlters:
u(t, x) = F0(t, x) ∗f(x),
where
F0(t, x) = F(t, x; 0) = e−x2/(4t)
2
√
π t
.
Since u(t, x) solves the heat equation, we conclude that Gaussian ﬁlter convolution has the
same smoothing eﬀect on the initial signal f(x). Indeed, the convolution integral (8.16)

296
8 Linear and Nonlinear Evolution Equations
t = 0
t = .1
t = 1
t = 5
t = 30
t = 300
Figure 8.2.
Error function solution to the heat equation.

serves to replace each initial value f(x) by a weighted average of nearby values, the weight
being determined by the Gaussian distribution. This has the eﬀect of smoothing out high-
frequency variations in the signal, and, consequently, the Gaussian convolution formula
(8.16) provides an eﬀective method for denoising rough signals and data.
Example 8.1. An inﬁnite bar is initially heated to unit temperature along a ﬁnite
interval. The initial temperature proﬁle is thus a box function
u(0, x) = f(x) = σ(x −a) −σ(x −b) =
 1,
a < x < b,
0,
otherwise.
The ensuing temperature is provided by the solution to the heat equation obtained by the
integral formula (8.16):
u(t, x) =
1
2
√
π t
 b
a
e−(x−ξ)2/(4t) dξ = 1
2

erf
x −a
2
√
t

−erf
x −b
2
√
t
 
,
(8.17)
where erf denotes the error function, as deﬁned in (2.87). Graphs of the solution (8.17) for
a = −5, b = 5, at the indicated times, are displayed in Figure 8.2. Observe the instanta-
neous smoothing of the sharp interface and instantaneous propagation of the disturbance,
followed by a gradual decay to thermal equilibrium, with u(t, x) →0 as t →∞.
The Forced Heat Equation and Duhamel’s Principle
The fundamental solution approach can be also applied to solve the inhomogeneous heat
equation
ut = uxx + h(t, x),
(8.18)
modeling a bar subject to an external heat source h(t, x), which might depend on both
position and time. We begin by solving the particular case
ut = uxx + δ(t −τ) δ(x −ξ),
(8.19)

8.1 The Fundamental Solution to the Heat Equation
297
whose inhomogeneity represents a heat source of unit magnitude that is concentrated at a
position x = ξ and applied at a single time t = τ > 0. Physically, this models the eﬀect of
instantaneously applying a soldering iron to a single spot on the bar. Let us also impose
homogeneous initial conditions
u(0, x) = 0
(8.20)
as well as homogeneous boundary conditions of one of our standard types. The resulting
solution
u(t, x) = G(t, x; τ, ξ)
(8.21)
will be referred to as the general fundamental solution to the heat equation. Since a heat
source that is applied at time τ will aﬀect the solution only at later times t ≥τ, we expect
that
G(t, x; τ, ξ) = 0
for all
t < τ.
(8.22)
Indeed, since u(t, x) solves the unforced heat equation at all times t < τ subject to ho-
mogeneous boundary conditions and has zero initial temperature, this follows immediately
from the uniqueness of the solution to the initial-boundary value problem.
Once we know the general fundamental solution (8.21), we are able to solve the problem
for a general external heat source (8.18). We ﬁrst write the forcing as a superposition
h(t, x) =
 ∞
0
 b
a
δ(t −τ) δ(x −ξ) h(τ, ξ) dξ dτ
(8.23)
of concentrated instantaneous heat sources. Linearity allows us to conclude that the solu-
tion is given by the self-same superposition formula
u(t, x) =
 t
0
 b
a
G(t, x; τ, ξ) h(τ, ξ) dξ dτ.
(8.24)
The fact that we only need to integrate over times 0 ≤τ ≤t is a consequence of (8.22).
Remark: If we have a nonzero initial condition, u(0, x) = f(x), then, by linear super-
position, the solution
u(t, x) =
 b
a
F(t, x; ξ) f(ξ) dξ +
 t
0
 b
a
G(t, x; τ, ξ) h(τ, ξ) dξ dτ
(8.25)
is a combination of (a) the solution with no external heat source, but nonzero initial
conditions, plus (b) the solution with homogeneous initial conditions but nonzero heat
source.
Let us explicitly solve the forced heat equation on an inﬁnite interval −∞< x < ∞.
We begin by computing the general fundamental solution. As before, we take the Fourier
transform of both sides of the partial diﬀerential equation (8.18) with respect to x. In view
of (7.37, 43), we ﬁnd
∂u
∂t + k2 u =
1
√
2π e−i k ξ δ(t −τ),
(8.26)
which is an inhomogeneous ﬁrst-order ordinary diﬀerential equation for the Fourier trans-
form u(t, k) of u(t, x), while (8.20) implies the initial condition
u(0, k) = 0.
(8.27)

298
8 Linear and Nonlinear Evolution Equations
We solve the initial value problem (8.26–27) by the usual method, [18, 23]. Multiplying
the diﬀerential equation by the integrating factor ek2t yields
∂
∂t ( ek2t u ) =
1
√
2π
ek2t−i k ξ δ(t −τ).
Integrating both sides from 0 to t and using the initial condition, we obtain
u(t, k) =
1
√
2π
e−k2(t−τ)−i k ξ σ(t −τ),
where σ(s) is the usual step function (6.23). Finally, we apply the inverse Fourier transform
formula (7.9), and then (8.14), to deduce that
u(t, x) = G(t, x; τ, ξ) = σ(t −τ)
2π
 ∞
−∞
e−k2(t−τ)+ i k (x−ξ) dk
=
σ(t −τ)
2

π(t −τ)
exp

−(x −ξ)2
4(t −τ)

= σ(t −τ)F(t −τ, x; ξ) .
(8.28)
Thus, the general fundamental solution is obtained by translating the fundamental solution
F(t, x; ξ) for the initial value problem to a starting time of t = τ instead of t = 0. Finally,
the superposition principle (8.24) produces the solution,
u(t, x) =
 t
0
 ∞
−∞
h(τ, ξ)
2

π(t −τ)
exp

−(x −ξ)2
4 (t −τ)

dξ dτ,
(8.29)
to the heat equation with source term and zero initial condition on an inﬁnite bar. A
nonzero initial condition u(0, x) = f(x) leads, as in the superposition formula (8.25), to an
additional term of the form (8.16) in the solution formula.
Remark: The fact that an initial condition has the same aftereﬀect on the temper-
ature as an instantaneous applied heat source of the same magnitude, thus implying the
identiﬁcation (8.28) of the two types of fundamental solution, is known as Duhamel’s Prin-
ciple, named after the nineteenth-century French mathematician Jean–Marie Duhamel.
Duhamel’s Principle remains valid over a broad range of linear evolution equations.
Example 8.2.
An inﬁnitely long bar with unit thermal diﬀusivity starts out uni-
formly at zero degrees.
Beginning at time t = 0, a concentrated heat source of unit
magnitude is continually applied at the origin. The resulting temperature is the solution
u(t, x) to the initial value problem
ut = uxx + δ(x),
u(0, x) = 0,
t > 0,
−∞< x < ∞.
According to (8.29), the solution is given by
u(t, x) =
 t
0
 ∞
−∞
δ(ξ)
2

π(t −τ)
exp

−(x −ξ)2
4 (t −τ)

dξ dτ
=
 t
0
1
2

π(t −τ)
exp

−
x2
4 (t −τ)

dτ =

t
π exp

−x2
4 t

+
x erf
 x
2
√
t

−| x |
2
.
Three snapshots can be seen in Figure 8.3. Observe that the solution is even in x and
monotonically decreasing as | x | →∞. Moreover, it has a corner at the origin with limiting

8.1 The Fundamental Solution to the Heat Equation
299
t = 1
t = 2
t = 3
Figure 8.3.
Eﬀect of a concentrated heat source.

tangent lines of slopes ± 1
2, which implies that its second x derivative produces the delta-
function forcing term. At each time t, the solution can be viewed as the linear superposition
of a continuous family of fundamental solutions, corresponding to the cumulative eﬀect of
individual heat sources applied at each previous time 0 ≤τ ≤t.
Moreover, it is not
diﬃcult to see that, at each ﬁxed x, the temperature is monotonically increasing in t, with
u(t, x) →∞as t →∞, and hence the continuous heat source eventually produces an
unbounded temperature in the entire inﬁnite bar.
The Black–Scholes Equation and Mathematical Finance
The most important and inﬂuential partial diﬀerential equation in ﬁnancial modeling and
investment is the celebrated Black–Scholes equation
∂u
∂t + σ2
2 x2 ∂2u
∂x2 + rx ∂u
∂x −ru = 0,
(8.30)
ﬁrst proposed in 1973 by the American economists Fischer Black and Myron Scholes, [19],
and Robert Merton, [71]. The dependent variable u(t, x) represents the monetary value
of a single ﬁnancial option, meaning a contract to either buy or sell an asset at a speciﬁed
exercise price p at a certain future time t⋆. The value u(t, x) of the option will depend
on the current time t ≤t⋆and the current price x ≥0 of the underlying asset. As with
many ﬁnancial models, one assumes the absence of arbitrage, meaning that there is no
way to make a riskless proﬁt. The constant σ > 0 represents the asset’s volatility, while
r denotes the (assumed ﬁxed) interest rate for bank deposits, where investors could place
their money with a guaranteed rate of return instead of buying the option.
(Investors
borrowing money to buy the asset would use a negative value of r.) The derivation of
the Black–Scholes equation from basic ﬁnancial modeling relies on the theory of stochastic
diﬀerential equations, [83], which would take us too far aﬁeld to explain here; instead, we
refer the interested reader to [123]. The Black–Scholes equation and its generalizations
form the basis of much of the modern ﬁnancial world, and, increasingly, the insurance
industry.
Observe ﬁrst that the Black–Scholes equation is a backwards diﬀusion process, since,
upon solving for
∂u
∂t = −σ2
2 x2 ∂2u
∂x2 −r x ∂u
∂x + r u,
(8.31)
the coeﬃcient of the diﬀusion term uxx is negative. This implies that the initial value
problem is well-posed only when time runs backwards. In other words, given a prescribed

300
8 Linear and Nonlinear Evolution Equations
value of the option at some speciﬁed time in the future, we can use the Black–Scholes
equation to determine its current value. However, ill-posedness implies that we cannot
predict future values from the current worth of the portfolio.
The “ﬁnal value problem” for the Black–Scholes equation is to determine the option’s
value u(t, x) at the current time t and asset value x ≥0, given the ﬁnal condition
u(t⋆, x) = f(x)
(8.32)
at the exercise time t⋆> t. For a so-called European call option, whereby the asset is to
be bought at the exercise price p > 0 at the speciﬁed time, the ﬁnal condition is
u(t⋆, x) = max{x −p, 0},
(8.33)
representing the investor’s proﬁt when x > p, or, when x ≤p, the option not being exercised
so as to avoid a loss. Analogously, for a put option, where the asset is to be sold, the ﬁnal
condition is
u(t⋆, x) = max{p −x, 0}.
(8.34)
The solution u(t, x) will be deﬁned for all t < t⋆and all x > 0, subject to the boundary
conditions
u(t, 0) = 0,
u(t, x) ∼x
as
x →∞,
where the asymptotic boundary condition means that the ratio u(t, x)/x tends to a constant
as x →∞.
Fortunately, the Black–Scholes equation can be solved explicitly by transforming it
into the heat equation. The ﬁrst step is to convert it to a forward diﬀusion process, by
setting
τ = 1
2 σ2 (t⋆−t),
v(τ, x) = u(t⋆−2τ/σ2, x),
so that τ eﬀectively runs forward from 0 as the actual time t runs backwards from t⋆. This
substitution has the eﬀect of converting the ﬁnal condition (8.32) into an initial condition
v(0, x) = f(x). Moreover, a straightforward chain rule computation shows that v satisﬁes
∂v
∂τ = x2 ∂2v
∂x2 + κ x ∂v
∂x −κ v,
where
κ = 2r
σ2 .
The next step is to remove the explicit dependence on the independent variable x. The
hint is that the right-hand side has the form of an Euler ordinary diﬀerential equation,
[23, 89]. According to Exercise 4.3.23, these terms can be placed into constant-coeﬃcient
form by the change of independent variables x = ey. Indeed, writing
w(τ, y) = v(τ, ey) = v(τ, x)
when
x = ey,
we apply the chain rule to compute the derivatives
∂w
∂τ = ∂v
∂τ ,
∂w
∂y = ey ∂v
∂x = x ∂v
∂x ,
∂2w
∂y2 = e2y ∂2v
∂x2 + ey ∂v
∂x = x2 ∂2v
∂x2 + x ∂v
∂x .
As a result, we ﬁnd that w solves the partial diﬀerential equation
∂w
∂τ = ∂2w
∂y2 + (κ −1) ∂w
∂y −κ w.
(8.35)
This is getting closer to the heat equation, and, in fact, can be changed into it by setting
w(τ, y) = eατ+β y z(τ, y)

8.1 The Fundamental Solution to the Heat Equation
301
for suitable constants α, β. Indeed, diﬀerentiating and substituting into (8.35) yields
∂z
∂τ + αz = ∂2z
∂y2 + 2β ∂z
∂y + β2z + (κ −1)
∂z
∂y + β z

−κ z.
The terms involving ∂z/∂y and z are eliminated by setting
α = −1
4 (κ + 1)2,
β = −1
2 (κ −1).
(8.36)
We conclude that the function
z(τ, y) = e(κ+1)2τ/4+(κ−1)y/2 w(τ, y)
(8.37)
satisﬁes the heat equation
∂z
∂τ = ∂2z
∂y2 .
(8.38)
Unwinding the preceding argument, we have managed to prove the following:
Proposition 8.3. If z(τ, y) is the solution to the initial value problem
∂z
∂τ = ∂2z
∂y2 ,
z(0, y) = h(y) = e(κ−1)y/2f(ey),
(8.39)
for τ > 0, −∞< y < ∞, then
u(t, x) = x−(κ−1)/2e−(κ+1)2 σ2(t⋆−t)/8 z
 1
2 σ2(t⋆−t), log x

(8.40)
solves the ﬁnal value problem (8.30, 32) for the Black–Scholes equation for t < t⋆and
0 < x < ∞.
Now, according to (8.16), the solution to the initial value problem (8.39) can be written
as a convolution integral of the initial data with the heat equation’s fundamental solution:
z(τ, y) =
1
2 √π τ
 ∞
−∞
e−(y−η)2/(4τ) h(η) dη =
1
2 √π τ
 ∞
−∞
e−(y−η)2/(4τ)+(κ−1)η/2f(eη) dη.
(8.41)
Combining this formula with (8.40) produces an explicit solution formula for the general
ﬁnal value problem for the Black–Scholes equation. In particular, for the European call
option (8.33), the initial condition is
z(0, y) = h(y) = e(κ−1)y/2 max{ey −p, 0},
and so
z(τ, y) =
1
2 √π τ
 ∞
log p
e−(y−η)2/(4τ)+(κ−1)η/2(eη −p) dη.
The integral can evaluated by completing the square inside the exponential, producing
z(τ, y) = 1
2

e(κ+1)2τ/4+(κ+1)y/2 erfc
log p −(κ + 1)τ −y
2√τ

−p e(κ−1)2τ/4+(κ−1)y/2 erfc
log p −(κ −1)τ −y
2√τ

,
(8.42)

302
8 Linear and Nonlinear Evolution Equations
10
20
10
20
t = 0
10
20
10
20
t = 4
10
20
10
20
t = 8
10
20
10
20
t = 9
10
20
10
20
t = 9.5
10
20
10
20
t = 10
Figure 8.4.
Solution to the Black–Scholes equation.

where
erfc x =
2
√π
 ∞
x
e−z2 dz = 1 −erf x
(8.43)
is the complementary error function, cf. (2.87). Substituting (8.42) into (8.40) results in
the celebrated Black–Scholes formula for a European call option:
u(t, x) = 1
2
$
x erfc

−

r + 1
2 σ2 
(t⋆−t) + log(x/p)

2σ2(t⋆−t)

−p e−r(t⋆−t) erfc

−

r −1
2 σ2 
(t⋆−t) + log(x/p)

2σ2(t⋆−t)
%
.
(8.44)
A graph of the solution for the speciﬁc values t⋆= 10, r = .1, σ = .2, p = 10 appears in
Figure 8.4. Observe that the option’s value slowly decreases as the time gets closer and
closer to the exercise time t⋆, thereby lessening any chances of further proﬁt stemming
from the option’s underlying price volatility.

8.1 The Fundamental Solution to the Heat Equation
303
Exercises
8.1.1. Find the solution to the heat equation ut = uxx on the real line having the following
initial condition at time t = 0. Then sketch graphs of the resulting temperature distribution
at times t = 0, 1, and 5.
(a) e−x2,
(b) the step function σ(x),
(c) e−| x |,
(d)
 1 −| x |,
| x | < 1,
0,
otherwise.
8.1.2. On an inﬁnite bar with unit thermal diﬀusivity, a concentrated unit heat source is in-
stantaneously applied at the origin at time t = 0. A heat sensor measures the resulting
temperature in the bar at position x = 1. Determine the maximum temperature measured
by the sensor. At what time is the maximum achieved?
8.1.3.(a) Find the solution to the heat equation (8.6) whose initial data corresponds to a pair
of unit heat sources placed at positions x = ±1.
(b) Graph the solution at times t =
.1, .25, .5, 1. (c) At what time(s) does the origin experience its maximum overall tempera-
ture? What is the maximum temperature at the origin?
8.1.4.(a) Use the Fourier transform to solve the initial value problem
∂u
∂t = ∂2u
∂x2 ,
u(0, x) = δ ′(x −ξ),
−∞< x < ∞,
t > 0,
whose initial data is the derivative of the delta function at a ﬁxed position ξ.
(b) Show that your solution can be written as the derivative ∂F/∂x of the fundamental solu-
tion F(t, x; ξ). Explain why this observation should be valid.
8.1.5. Suppose that the initial data u(0, x) = f(x) is real. Explain why the Fourier transform
solution formula (8.13) deﬁnes a real function u(t, x) for all t > 0.
8.1.6.(a) What is the maximum value of the fundamental solution at time t?
(b) Can you justify the claim that its width is proportional to
√
t ?
8.1.7. Prove directly that (8.5) is indeed a solution to the heat equation, and, moreover, has
the correct initial and boundary conditions.
8.1.8. Show, by a direct computation, that the ﬁnal formula in (8.14) is a solution to the heat
equation for all t > 0.
♦8.1.9. Justify formula (8.15).
8.1.10. According to Exercises 4.1.11–12, both the t and x partial derivatives of the fundamen-
tal solution solve the heat equation. (a) Write down the initial value problem satisﬁed by
these two solutions. (b) Set ξ = 0 and then sketch graphs of each solution at several se-
lected times. (c) Reconstruct each solution as a Fourier integral.
8.1.11. Let u(t, x) = ∂F
∂x (t, x; 0) denote the x derivative of the fundamental solution (8.14).
(a) Prove that u(t, x) is a solution to the heat equation ut = uxx on the domain
{−∞< x < ∞, t > 0}.
(b) For ﬁxed x, prove that
lim
t →0+ u(t, x) = 0. (c) Explain why,
despite the results in parts (a) and (b), u(t, x) is not a classical solution to the initial value
problem ut = uxx, u(0, x) = 0. What is the classical solution? (d) What initial value
problem does u(t, x) satisfy?
8.1.12. Justify all the statements in Example 8.2.
♥8.1.13.(a) Solve the heat equation on an inﬁnite bar when the initial temperature is equal to 1
for | x | < 1 and 0 elsewhere, while a unit heat source is applied to the same part of the bar
| x | < 1 for a unit time period 0 < t < 1. (b) At what time and what location is the bar
the hottest? (c) What is the ﬁnal equilibrium temperature of the bar?

304
8 Linear and Nonlinear Evolution Equations
8.1.14. An insulated bar 1 meter long, with constant diﬀusivity γ = 1, is taken from a freezer
that is kept at −10◦C, and then has its ends kept at room temperature of 20◦C. A solder-
ing iron with temperature 350◦C is continually held at the midpoint of the bar.
(a) Set up an initial value problem modeling the temperature distribution in the bar.
(b) Find the corresponding equilibrium temperature distribution.
♥8.1.15. Consider the heat equation with unit thermal diﬀusivity on the interval 0 < x < 1
subject to homogeneous Dirichlet boundary conditions.
(a) Find a Fourier series representation for the fundamental solution F(t, x; ξ) that solves
the initial-boundary value problem
ut = uxx,
t > 0,
0 < x < 1,
u(0, x) = δ(x −ξ),
u(t, 0) = 0 = u(t, 1).
Your solution should depend on t, x and the point ξ where the initial delta impulse is
applied.
(b) For the value ξ = .3, use a computer program to sum the ﬁrst few terms in the series
and graph the result at times t = .0001, .001, .01, and .1. Make sure you have included
enough terms to obtain a reasonably accurate graph.
(c) Compare your graphs with those of the fundamental solution F(t, x; .3) on an inﬁnite
interval at the same times. What is the maximum deviation between the two solutions
on the entire interval 0 ≤x ≤1?
(d) Use your fundamental solution F(t, x; ξ) to construct a series solution to the general ini-
tial value problem u(0, x) = f(x). Is your series the same as the usual Fourier series
solution? If not, explain any discrepancy.
8.1.16. True or false: Periodic forcing of the heat equation at a particular frequency can pro-
duce resonance. Justify your answer.
8.1.17. Find the fundamental solution for the cable equation vt = γ vxx −α v on the real line.
Hint: See Exercise 4.1.16.
8.1.18. The partial diﬀerential equation ut + c ux = γ uxx models transport of a diﬀusing pol-
lutant in a ﬂuid ﬂow. Assuming that the speed c is constant, write down a solution to the
initial value problem u(0, x) = f(x) for −∞< x < ∞. Hint: Look at Exercise 4.1.17.
♦8.1.19. Use the Fourier transform to solve the initial value problem i ut = uxx, u(0, x) = f(x),
for the one-dimensional Schr¨odinger equation on the real line −∞< x < ∞.
♦8.1.20. Let u(t, x) be a solution to the heat equation having ﬁnite thermal energy,
E(t) =
	 ∞
−∞u(t, x) dx < ∞, and satisfying ux(t, x) →0 as x →±∞, for all t ≥0. Prove the
law of conservation of thermal energy: E(t) = constant.
8.1.21. Explain in your own words how a function u(t, x) can satisfy u(t, x) →0 uniformly as
t →∞while maintaining the constancy of
	 ∞
−∞u(t, x) dx = 1 for all t. Discuss what this
signiﬁes regarding the interchange of limits and integrals.
8.1.22.(a) Prove that if f(k) ∈L2 is square-integrable, then so is e−ak2 f(k) for any a > 0.
(b) Prove that when the initial data f(x) ∈L2 is square integrable, so is the Fourier inte-
gral solution (8.13) for all t ≥0.
8.1.23. Find the solution to the Black–Scholes equation for a put option (8.34).
8.1.24.(a) If we increase the interest rate r, does the value of a call option (i) increase;
(ii) decrease; (iii) stay the same; (iv) could do any of the above? Justify your answer.
(b) Answer the same question when rate stays ﬁxed, but the volatility σ is increased.
♦8.1.25. Justify formula (8.42).

305
8.2 Symmetry and Similarity
The geometric approach to partial diﬀerential equations enables one to exploit their sym-
metry properties to construct explicit solutions of both mathematical and physical interest.
Unlike separation of variables, which is restricted to special types of linear partial diﬀer-
ential equations,† symmetry methods can also be successfully applied to a broad range of
nonlinear partial diﬀerential equations. While we do not have the mathematical tools to
develop the full range of symmetry techniques, we will learn how to exploit some of the
most basic symmetry properties: translations, leading to traveling wave solutions; scalings,
leading to similarity solutions; and, in subsequent chapters, rotational symmetries.
In general, by a symmetry of an equation, we mean a transformation that takes so-
lutions to solutions. Thus, knowing a symmetry transformation, if we are in possession of
one solution, then we can construct a second solution by applying the symmetry. And,
possibly, a third solution by applying the symmetry yet again. And so on. If we know lots
of symmetries, then we can produce lots of solutions by this simple device.
Remark: General symmetry techniques are founded on the theory of Lie groups,
named after the inﬂuential nineteenth-century Norwegian mathematician Sophus Lie (pro-
nounced “Lee”).
Lie’s theory is a profound synthesis of group theory and diﬀerential
geometry, and provides an algorithm for completely determining all the (continuous) sym-
metries of a given diﬀerential equation.
Although the theory lies beyond the scope of
this introductory text, direct inspection and/or physical intuition will often produce the
most important symmetries of the system, which can then be directly exploited. Modern
applications of Lie’s symmetry methods to partial diﬀerential equations arising in physics
and engineering can be traced back to an inﬂuential book on hydrodynamics by the au-
thor’s thesis advisor, Garrett Birkhoﬀ, [17]. A complete and comprehensive treatment
of Lie symmetry methods can be found in the author’s ﬁrst book [87], and, at a more
introductory level, in the recent books [27, 58], the ﬁrst having a particular emphasis on
applications in ﬂuid mechanics.
The heat equation serves as an excellent testing ground for the general methodology,
since it admits a rich variety of symmetry transformations that take solutions to solutions.
The simplest are the translations.
Moving the space and time coordinates by a ﬁxed
amount,
t −→t + a,
x −→x + b,
(8.45)
where a, b are constants, changes the function u(t, x) into the translated function‡
U(t, x) = u(t −a, x −b).
(8.46)
A simple application of the chain rule proves that the partial derivatives of U with respect
to t and x agree with the corresponding partial derivatives of u, so
∂U
∂t = ∂u
∂t ,
∂U
∂x = ∂u
∂x ,
∂2U
∂x2 = ∂2u
∂x2 ,
†
This is not entirely fair: separation of variables can also be applied to certain nonlinear
partial diﬀerential equations such as Hamilton–Jacobi equations, [73].
‡
The minus signs arise because when we set t = t + a, x = x + b, then the translated function
is U(t, x) = u(t, x) = u(t −a, x −b). Dropping the hats produces the stated formula.
8.2 Symmetry and Similarity

306
8 Linear and Nonlinear Evolution Equations
and so on. In particular, the function U(t, x) is a solution to the heat equation Ut = γ Uxx
whenever u(t, x) also solves ut = γ uxx. Physically, translation symmetry formalizes the
property that the heat equation models a homogeneous medium, and hence the solution
does not depend on the choice of reference point or origin of our coordinate system.
As a consequence, each solution to the heat equation will produce an inﬁnite family
of translated solutions. For example, starting with the separable solution
u(t, x) = e−γ t sin x,
we immediately produce the additional translated solutions
U(t, x) = e−γ (t−a) sin(x −b),
valid for any choice of constants a, b.
Warning: Typically, the symmetries of a diﬀerential equation do not respect initial
or boundary conditions. For instance, if u(t, x) is deﬁned for t ≥0 and in the domain
0 ≤x ≤ℓ, then its translated version (8.46) is deﬁned for t ≥a and in the translated
domain b ≤x ≤ℓ+ b, and so will solve a translated initial-boundary value problem.
A second important class of symmetries consists of the scaling invariances. We already
know that if u(t, x) is a solution, then so is the scalar multiple c u(t, x) for any constant c;
this is a simple consequence of linearity of the heat equation. We can also add an arbitrary
constant to the temperature, noting that
U(t, x) = cu(t, x) + k
(8.47)
is a solution for any choice of constants c, k. Physically, the transformation (8.47) amounts
to a change in the scale used to measure temperature. For instance, if u is measured in
degrees Celsius, and we set c = 9
5 and k = 32, then U = 9
5 u+32 will be measured in degrees
Fahrenheit. Thus, reassuringly, the physical processes described by the heat equation do
not depend on our choice of thermometer.
More interestingly, suppose we rescale the space and time variables:
t −→α t,
x −→β x,
(8.48)
where α, β ̸= 0 are nonzero constants. The eﬀect of such a scaling transformation is to
convert u(t, x) into a rescaled function†
U(t, x) = u(α−1 t, β−1 x).
(8.49)
The derivatives of U are related to those of u according to the formulas
∂U
∂t = 1
α
∂u
∂t ,
∂U
∂x = 1
β
∂u
∂x ,
∂2U
∂x2 = 1
β2
∂2u
∂x2 .
Therefore, if u satisﬁes the heat equation ut = γ uxx, then U satisﬁes the rescaled heat
equation
Ut = 1
α ut = γ
α uxx = β2 γ
α
Uxx,
†
As before, setting t = α t, x = β x, produces the rescaled function U(t, x) = u(t, x) =
u(α−1 t, β−1 x), and we then drop the hats.

8.2 Symmetry and Similarity
307
which we rewrite as
Ut = Γ Uxx,
where
Γ = β2 γ
α
.
(8.50)
Thus, the net eﬀect of scaling space and time is merely to rescale the diﬀusion coeﬃcient.
Physically, the scaling symmetry (8.48) corresponds to a change in the physical units used
to measure time and distance. For instance, to change from minutes to seconds, set α = 60,
and from yards to meters, set β = .9144. The net eﬀect (8.50) on the diﬀusion coeﬃcient
γ is a reﬂection of its physical units, namely distance2/time.
In particular, if we choose
α = γ,
β = 1,
then the rescaled diﬀusion coeﬃcient becomes Γ = 1. This observation has the following
important consequence. If U(t, x) solves the heat equation for a unit diﬀusivity, Γ = 1,
then
u(t, x) = U(γ t, x)
(8.51)
solves the heat equation for the diﬀusivity γ > 0. Thus, the only eﬀect of the diﬀusion
coeﬃcient is to speed up or slow down time. A body with diﬀusivity γ = 2 will cool down
twice as fast as a body (of the same shape subject to similar boundary conditions and initial
conditions) with diﬀusivity γ = 1. Note that this particular rescaling has not altered the
space coordinates, and so U(t, x) is deﬁned on the same spatial domain as u(t, x).
On the other hand, if we set α = β2, then the rescaled diﬀusion coeﬃcient is exactly
the same as the original: Γ = γ. Thus, the transformation
t −→β2 t,
x −→β x,
(8.52)
does not alter the equation, and hence deﬁnes a scaling symmetry — also known as a sim-
ilarity transformation — for the heat equation. Combining (8.52) with the linear rescaling
u →c u, we make the elementary, but important, observation that if u(t, x) is any solution
to the heat equation, then so is the function
U(t, x) = c u(β−2 t, β−1 x),
(8.53)
for the same diﬀusion coeﬃcient γ. For example, rescaling the solution
u(t, x) = e−γ t cos x
leads to the solution
U(t, x) = c e−γ t/β2 cos x
β .
Warning: As in the case of translations, rescaling space by a factor β ̸= 1 will alter
the domain of deﬁnition of the solution. If u(t, x) is deﬁned for a ≤x ≤b, then U(t, x), as
given in (8.53), is deﬁned for β a ≤x ≤β b (or, when β < 0, for β b ≤x ≤β a).
For example, suppose that we have solved the heat equation for the temperature u(t, x)
on a bar of length 1, subject to certain initial and boundary conditions. We are then given
a bar composed of the same material of length 2. Since the diﬀusivity coeﬃcient has not
changed, we can directly construct the new solution U(t, x) by rescaling. Setting β = 2
will serve to double the length. If we also rescale time by a factor α = β2 = 4, then the
rescaled function U(t, x) = u
 1
4 t, 1
2 x

will be a solution of the heat equation on the longer
bar with the same diﬀusivity constant. The net eﬀect is that the rescaled solution will be
evolving four times as slowly as the original, and hence it eﬀectively takes a bar that is
twice the length four times as long to cool down.

308
8 Linear and Nonlinear Evolution Equations
Similarity Solutions
A similarity solution of a partial diﬀerential equation is one that remains unchanged (in-
variant) under a one-parameter family† of scaling symmetryscaling symmetries.
For a
partial diﬀerential equation in two variables — say t and x — the similarity solutions can
be found by solving an ordinary diﬀerential equation.
Suppose our partial diﬀerential equation admits the scaling symmetries
t −→βa t,
x −→βb x,
u −→βc u,
β ̸= 0,
(8.54)
where a, b, c are ﬁxed constants with a, b not both zero. As above, this means that if u(t, x)
is a solution to the diﬀerential equation, so is the rescaled function
U(t, x) = βc u(β−a t, β−b x)
(8.55)
for all values of β ̸= 0. Checking that this indeed deﬁnes a symmetry is a simple matter of
applying the chain rule, which implies that the derivatives scale according to
ut −→βc−a ut,
ux −→βc−b ux,
utt −→βc−2a utt,
uxt −→βc−a−b uxt,
(8.56)
and so on. Products of derivatives scale multiplicatively, e.g., x4 u uxt →β2c−a+3b x4 u uxt.
In order that a (polynomial) diﬀerential equation admit such a scaling symmetry, each of
its terms must scale by the same overall power of β.
By deﬁnition, u(t, x) is called a similarity solution if it remains unchanged (invariant)
under the scaling symmetries (8.54), so that
u(t, x) = βc u(β−a t, β−b x)
(8.57)
for all β > 0. Let us, for speciﬁcity, assume that a ̸= 0, leaving the case a = 0, b ̸= 0,
for the reader to complete in Exercise 8.2.13. Since the left-hand side of (8.57) does not
depend on β, we can ﬁx its value to be‡ β = t1/a, and conclude that the similarity solution
must have the form
u(t, x) = tc/a v(ξ),
where
ξ = x t−b/a
and
v(ξ) = u(1, ξ),
(8.58)
are referred to as the similarity variables, since they remain invariant when subjected to
the scaling transformations (8.54). We then use the chain rule to ﬁnd the formulas for the
partial derivatives of u in terms of the ordinary derivatives of v with respect to ξ. Substi-
tuting these expressions into the scale-invariant partial diﬀerential equation for u(t, x), and
then canceling a common factor of t, will eﬀectively reduce it to an ordinary diﬀerential
equation for the function v(ξ). Each solution to the resulting ordinary diﬀerential equa-
tion then gives rise to a scale-invariant solution to the original partial diﬀerential equation
through the similarity ansatz (8.58).
Example 8.4. As a ﬁrst example, let us return to the nonlinear transport equation
ut + uux = 0,
(8.59)
†
Or, more accurately, a one-parameter group, [87].
‡
This assumes t > 0; for t < 0, just replace t by −t.

8.2 Symmetry and Similarity
309
which we studied in Section 2.3. Under (8.54, 56), the equation rescales to
βc−aut + β2c−buux = 0,
which is unchanged, provided c −a = 2c −b, and hence c = b−a. Setting a = 1, c = b−1,
we conclude that if u(t, x) is any solution, then so is the rescaled function
U(t, x) = βb−1 u(β−1 t, β−b x)
for any b and any β ̸= 0.
To ﬁnd the associated similarity solutions, we use (8.58) to introduce the ansatz
u(t, x) = tb−1 v(ξ),
where
ξ = x t−b.
(8.60)
Diﬀerentiating, we obtain
ut = −b x t−2 v′(ξ) + (b −1) tb−2 v(ξ) = tb−2
−b ξ v′(ξ) + (b −1) v(ξ)

,
ux = t−1 v′(ξ).
Substituting these expressions into the transport equation (8.59) yields
0 = ut + uux = tb−2
(v −b ξ) v′ + (b −1) v

,
and so
(v −b ξ) dv
dξ + (b −1) v = 0.
(8.61)
Any solution to this nonlinear ﬁrst-order ordinary diﬀerential equation will, when substi-
tuted into (8.60), produce a similarity solution to the nonlinear transport equation.
If b = 1, then either v = b ξ, producing the particular similarity solution u(t, x) = x/t
that we earlier used to construct the rarefaction wave (2.54), or v is constant, and so is u.
Otherwise, we can, in fact, linearize (8.61) by treating ξ as a function of v, whence
(b −1) v dξ
dv −b ξ = −v.
The general solution to such a linear ﬁrst-order ordinary diﬀerential equation is found by
the standard method, [18, 23], resulting in
ξ = v + k vb/(b−1),
where k is the constant of integration. Recalling (8.60), we ﬁnd that the similarity solutions
u(t, x) are deﬁned by an implicit equation
x = kub/(b−1) + t u.
For example, if b = 2, the (multi-valued) solution is a sideways-moving parabola:
x = ku2 + tu,
so that
u = −t ±
√
t2 + 4kx
2k
.
Example 8.5. Consider the linear heat equation
ut = uxx.
(8.62)
Under the rescaling (8.54), the equation becomes βc−aut = βc−2buxx, and thus (8.54)
represents a symmetry if and only if a = 2b. Therefore, if u(t, x) is any solution, so is the
rescaled function
U(t, x) = βc u(β−2 t, β−1 x).

310
8 Linear and Nonlinear Evolution Equations
Of course, the initial scaling factor stems from the linearity of the equation.
The scale-invariant solutions are constructed through the similarity ansatz
u(t, x) = tc/2 v(ξ),
where
ξ = x/
√
t .
Diﬀerentiation yields
ut = −1
2 x tc/2−3/2 v′(ξ) + 1
2 ctc/2−1 v(ξ) = tc/2−1
−1
2 ξ v′(ξ) + 1
2 cv(ξ)

,
uxx = tc/2−1 v′′(ξ).
Substituting these expressions into the heat equation and canceling a common power of t,
we ﬁnd that v must satisfy the linear ordinary diﬀerential equation
v′′ + 1
2 ξ v′ −1
2 cv = 0.
(8.63)
If c = 0, then (8.63) is eﬀectively a linear ﬁrst-order ordinary diﬀerential equation for v′(ξ),
which can be readily solved by the usual method, thereby producing the solution
v(ξ) = c1 + c2 erf
 1
2 ξ

,
where c1, c2 are arbitrary constants and erf is the error function (2.87). The corresponding
similarity solution to the heat equation is
u(t, x) = c1 + c2 erf
 x
√
t

.
The error function solutions that we encountered in (8.17) can be built up as a linear
combination of translations of this similarity solution.
If c ̸= 0, most solutions to the ordinary diﬀerential equation (8.63) are not elementary
functions.†
One is in need of more sophisticated techniques, e.g., the method of power
series to be developed in Section 11.3, to understand its solutions, and hence the resulting
similarity solutions to the heat equation.
Exercises
8.2.1. If it takes a 2 cm long insulated bar 23 minutes to cool down to room temperature, how
long does it take a 4 cm bar?
8.2.2. If it takes a 5 centimeter long insulated iron bar 10 minutes to cool down so as not to
burn your hand, how long does it take a 20 centimeter bar made out of the same material
to cool down to the same temperature?
♦8.2.3.(a) Given γ > 0, use a scaling transformation to write down the formula for the funda-
mental solution for the general heat equation ut = γ uxx for x ∈R.
(b) Write down the
corresponding integral formula for the solution to the initial value problem.
†
According to [87; Example 3.3], the general solution can be written in terms of parabolic
cylinder functions, [86].

8.2 Symmetry and Similarity
311
8.2.4. Use scaling to construct the series solution for a heated circular ring of radius r and
thermal diﬀusivity γ. Does scaling also give the correct formulas for the Fourier coeﬃcients
in terms of the initial temperature distribution?
8.2.5. A solution u(t, x) to the heat equation is measured in degrees Fahrenheit. What is the
corresponding temperature in degrees Kelvin? Which symmetry transformation takes the
ﬁrst solution to the second solution, and how does it aﬀect the diﬀusion coeﬃcient?
8.2.6. Is time reversal, t →−t, a symmetry of the heat equation? Write down a physical expla-
nation, and then a mathematical justiﬁcation.
8.2.7. According to Exercise 4.1.17, the partial diﬀerential equation ut + cux = γ uxx models
diﬀusion in a convective ﬂow. Show how to use scaling to place the diﬀerential equation in
the form ut + ux = P −1 uxx, where P is called the P´eclet number, and controls the rate of
mixing. Is there a scaling that will reduce the problem to the case P = 1?
8.2.8. Suppose you know a solution u⋆(t, x) to the heat equation that satisﬁes u⋆(1, x) = f(x).
Explain how to solve the initial value problem with u(0, x) = f(x).
8.2.9. Solve the following initial value problems for the heat equation ut = uxx for x ∈R:
(a) u(0, x) = e−x2/4. Hint: Use Exercise 8.2.8.
(b) u(0, x) = e−4x2.
(c) u(0, x) = x2 e−x2/4. Hint: Use Exercise 4.1.12.
8.2.10. Deﬁne the functions Hn(x) for n = 0, 1, 2, . . . , by the formula
dn
dxn e−x2
= (−1)nHn(x) e−x2
.
(8.64)
(a) Prove that Hn(x) is a polynomial of degree n, known as the nth Hermite polynomial.
(b) Calculate the ﬁrst four Hermite polynomials.
(c) Assuming γ = 1, ﬁnd the solution to the heat equation for −∞< x < ∞and t > 0,
given the initial data u(0, x) = Hn(x) e−x2. Hint: Combine Exercises 4.1.11, 8.2.8.
8.2.11. Find the scaling symmetries and corresponding similarity solutions of the following par-
tial diﬀerential equations:
(a) ut = x2 ux,
(b) ut + u2ux = 0,
(c) utt = uxx.
8.2.12. Show that the wave equation utt = c2uxx has the following invariance properties: if
u(t, x) is a solution, so is (a) any time translate: u(t−a, x), where a is ﬁxed; (b) any space
translate: u(t, x −b), where b is ﬁxed; (c) the dilated function u(β t, β x) for β ̸= 0; (d) any
derivative: say ∂u/∂x or ∂2u/∂t2, provided u is suﬃciently smooth.
♦8.2.13. Suppose a = 0, b ̸= 0 in the scaling transformation (8.57).
(a) Discuss how to reduce the partial diﬀerential equation to an ordinary diﬀerential equa-
tion for the corresponding similarity solutions.
(b) Illustrate your method with the partial diﬀerential equation t ut = u uxx.
8.2.14. True or false: (a) A homogeneous polynomial solution to a partial diﬀerential equa-
tion is always a similarity solution. (b) An inhomogeneous polynomial solution to a partial
diﬀerential equation can never be a similarity solution.
8.2.15.(a) Find all scaling symmetries of the two-dimensional Laplace equation uxx + uyy = 0.
(b) Write down the ordinary diﬀerential equation for the similarity solutions. (c) Can you
ﬁnd an explicit formula for the similarity solutions? Hint: Look at Exercise 8.2.14(a).
♥8.2.16. Besides the translations and scalings, Lie symmetry methods, [87], produce two other
classes of symmetry transformations for the heat equation ut = uxx. Given that u(t, x) is a
solution to the heat equation:
(a) Prove that U(t, x) = ec2 t−cx u(t, x −2ct) is also a solution to the heat equation for any
c ∈R. What solution do you obtain if u(t, x) = a is a constant solution? Remark: This
transformation can be interpreted as the eﬀect of a Galilean boost to a coordinate frame
that is moving with speed c.

312
8 Linear and Nonlinear Evolution Equations
(b) Prove that U(t, x) = e−cx2/(4(1+ct))
√1 + ct
u

t
1 + ct,
x
1 + ct

is a solution to the heat equa-
tion for any c ∈R. What solution do you obtain if u(t, x) = a is a constant?
8.3 The Maximum Principle
We have already noted the temporal decay of temperature, as governed by the heat equa-
tion, to thermal equilibrium. While the temperature at any individual point in a physical
medium can ﬂuctuate — depending on what is happening elsewhere, thermodynamics tells
us that the overall heat content of an isolated body must continually decrease. The Max-
imum Principle is the mathematical formulation of this physical law, and states that the
temperature of a body cannot, in the absence of external heat sources, ever become larger
than its initial or boundary values. This can be viewed as a dynamical counterpart to the
Maximum Principle for the Laplace equation, as formulated in Theorem 4.9, stating that
the maximum temperature of a body in equilibrium is achieved only on its boundary.
The proof of the Maximum Principle will be facilitated if we analyze the more general
situation in which heat energy is being continually extracted throughout the body.
Theorem 8.6. Let γ > 0. Suppose u(t, x) is a solution to the forced heat equation
∂u
∂t = γ ∂2u
∂x2 + F(t, x)
(8.65)
on the rectangular domain
R = {a < x < b, 0 < t < c}.
Assume that the forcing term is nowhere positive: F(t, x) ≤0 for all (t, x) ∈R. Then the
maximum of u(t, x) on the closed rectangle R is attained at t = 0 or x = a or x = b.
In other words, if no new heat is being introduced, the maximum overall temperature
occurs either at the initial time or on the body’s boundary. In particular, in the fully
insulated case F(t, x) ≡0, (8.65) reduces to the heat equation, and Theorem 8.6 applies
as stated.
Proof : First let us ﬁrst prove the result under the stronger assumption F(t, x) < 0,
which implies that
∂u
∂t < γ ∂2u
∂x2
(8.66)
everywhere in the rectangle R. Suppose ﬁrst that u(t, x) has a (local) maximum at a point
(t⋆, x⋆) in the interior of R. Then, by multivariable calculus, [8, 108], its gradient must
vanish there, ∇u(t⋆, x⋆) = 0, and hence
ut(t⋆, x⋆) = ux(t⋆, x⋆) = 0.
(8.67)
Our assumption implies that the scalar function h(x) = u(t⋆, x) has a maximum at x = x⋆.
Thus, by the second derivative test for functions of a single variable,
h′′(x⋆) = uxx(t⋆, x⋆) ≤0.
(8.68)

8.3 The Maximum Principle
313
But the requirements (8.67–68) are clearly incompatible with the initial inequality (8.66).
We conclude that the solution u(t, x) cannot have a local maximum at any point in the
interior of R.
We still need to exclude the possibility of a maximum occurring at a non-corner point
(t⋆, x⋆) = (c, x⋆),
a < x⋆< b, on the right-hand edge of the rectangle. If such were
to occur, then the function g(t) = u(t, x⋆) would be nondecreasing at t = c, and hence
g′(t) = ut(c, x⋆) ≥0 there. The preceding argument also implies that uxx(c, x⋆) ≤0, and
again these two requirements are incompatible with (8.66). We conclude that any (local)
maximum must occur on one of the other three sides of the rectangle, in accordance with
the statement of the theorem.
To generalize the argument to the case F(t, x) ≤0 — which includes the heat equation
— requires a little trick. Starting with the solution u(t, x) to (8.65), we set
v(t, x) = u(t, x) + ε x2,
where
ε > 0.
Then,
∂v
∂t = ∂u
∂t = γ ∂2u
∂x2 + F(t, x) = γ ∂2v
∂x2 −2γ ε + F(t, x) = γ ∂2v
∂x2 + F(t, x),
where, by our original assumption on F(t, x),
F(t, x) = F(t, x) −2γ ε < 0
everywhere in R. Thus, by the previous argument, a local maximum of v(t, x) can occur
only when t = 0 or x = a or x = b. Now we let ε →0 and conclude the same for u. More
rigorously, let M denote the maximum value of u(t, x) on the indicated three sides of the
rectangle. Then
v(t, x) ≤M + ε max{a2, b2 }
there, and hence, by the preceding argument,
u(t, x) ≤v(t, x) ≤M + ε max{a2, b2 }
for all
(t, x) ∈R.
Now, letting ε →0+ proves that u(t, x) ≤M everywhere in R.
Q.E.D.
For the unforced heat equation, we can bound the solution from both above and below
by its boundary and initial temperatures:
Corollary 8.7. Suppose u(t, x) solves the heat equation ut = γ uxx, with γ > 0, for
a < x < b, 0 < t < c. Set
B = { (0, x) | a ≤x ≤b } ∪{ (t, a) | 0 ≤t ≤c } ∪{ (t, b) | 0 ≤t ≤c } ,
and let
M = max { u(t, x) | (t, x) ∈B } ,
m = min { u(t, x) | (t, x) ∈B } ,
(8.69)
be, respectively, the maximum and minimum values for the initial and boundary temper-
atures. Then m ≤u(t, x) ≤M for all a ≤x ≤b, 0 ≤t ≤c.
Proof : The upper bound u(t, x) ≤M follows from the Maximum Principle of Theo-
rem 8.6. To establish the lower bound, we note that u(t, x) = −u(t, x) also solves the heat
equation, satisfying u(t, x) ≤−m on B, and hence, by the Maximum Principle, everywhere
in the rectangle. But this implies u(t, x) = −u(t, x) ≥m.
Q.E.D.

314
8 Linear and Nonlinear Evolution Equations
Remark: Theorem 8.6 is sometimes referred to as the Weak Maximum Principle for
the heat equation.
The Strong Maximum Principle states that, provided the solution
u(t, x) is not constant, its value at any non-initial, non-boundary point (t, x) ∈R =
{a < x < b, 0 < t ≤c} is strictly less than its maximum initial and boundary values; in
other words, u(t, x) < M for (t, x) ∈R, where M is given in (8.69).
Similarly, the
Strong Maximum Principle implies that, for nonconstant solutions to the heat equation,
the inequalities in Corollary 8.7 are strict: m < u(t, x) < M for all (t, x) ∈R. Proofs of
the Strong Maximum Principle are more delicate, and can be found in [38, 61].
One immediate application of the Maximum Principle is to prove uniqueness of solu-
tions to the heat equation.
Theorem 8.8. There is at most one solution to the Dirichlet initial-boundary value
problem for the forced heat equation.
Proof : Suppose u and u are any two solutions with the same initial and boundary
values. Then their diﬀerence v = u −u solves the homogeneous initial-boundary value
problem for the unforced heat equation, with minimum and maximum boundary values
m = 0 ≤v(t, x) ≤0 = M for t = 0, a ≤x ≤b, and also x = a or b, 0 ≤t ≤c. But then
Corollary 8.7 implies that 0 ≤v(t, x) ≤0 everywhere, which implies that u ≡u, thereby
establishing uniqueness.
Q.E.D.
Remark: Existence of the solution follows from the convergence of our Fourier series
— assuming that the initial and boundary data and the forcing function are suﬃciently
nice.
Exercises
8.3.1. True or false: Assuming no external heat source, if the initial and boundary tempera-
tures of a one-dimensional body are always positive, the temperature within the body is
necessarily positive.
8.3.2. Suppose u(t, x) and v(t, x) are two solutions to the heat equation such that u ≤v when
t = 0 and when x = a or x = b. Prove that u(t, x) ≤v(t, x) for all a ≤x ≤b and all t ≥0.
Provide a physical interpretion of this result.
8.3.3. For t > 0, let u(t, x) be a solution to the unforced heat equation on an interval a < x < b,
subject to homogeneous Dirichlet boundary conditions. Prove that
M(t) = max{ u(t, x) | a ≤x ≤b } is a nonincreasing function of t.
8.3.4.(a) State and prove a Maximum Principle for the convection-diﬀusion equation
ut = uxx + ux. (b) Does the equation ut = uxx −ux also admit a Maximum Principle?
8.3.5. Consider the parabolic equation ∂u
∂t = x ∂2u
∂x2 + ∂u
∂x on the interval 1 < x < 2, with initial
and boundary conditions u(0, x) = f(x), u(t, 1) = α(t), u(t, 2) = β(t).
(a) State and prove a version of the Maximum Principle for this problem.
(b) Establish uniqueness of the solution to this initial-boundary value problem.
8.3.6.(a) Show that u(t, x) = −x2 −2xt is a solution to the diﬀusion equation ut = xuxx.
(b) Explain why this diﬀerential equation does not admit a Maximum Principle.

8.4 Nonlinear Diﬀusion
315
8.3.7. Suppose that u(t, x) is a nonconstant solution to the heat equation on the interval
0 < x < ℓ, with homogeneous (a) Dirichlet, (b) Neumann, or (c) mixed boundary condi-
tions. Prove that the function E(t) =
	 ℓ
0 u(t, x)2 dx is everywhere decreasing: E(t1) > E(t2)
whenever t1 < t2.
8.3.8. True or false: The wave equation utt = c2uxx satisﬁes a Maximum Principle. If true,
clearly state the principle; if false, explain why not.
8.4 Nonlinear Diﬀusion
First-order partial diﬀerential equations serve to model conservative wave motion, begin-
ning with the basic one-dimensional scalar transport equations that we studied in Chap-
ter 2, and progressing on to higher-dimensional systems, the equations of gas dynamics,
the full-blown Euler equations of ﬂuid mechanics, and yet more complicated systems of
partial diﬀerential equations modeling plasmas, magneto-hydrodynamics, etc. However,
such systems fail to account for frictional and viscous eﬀects, which are typically modeled
by parabolic diﬀusion equations such as the heat equation and its generalizations, both lin-
ear and nonlinear. In this section, we investigate the consequences of combining nonlinear
wave motion with linear diﬀusion by analyzing the simplest such model. As we will see, the
dissipative term has the eﬀect of smoothing out abrupt shock discontinuities, and the re-
sult is a well-determined, smooth dynamical process with classical solutions. Moreover, in
the inviscid limit, the smooth solutions converge (nonuniformly) to a discontinuous shock
wave, leading to the method of viscosity solutions that has been successfully employed to
analyze such nonlinear dynamical processes.
Burgers’ Equation
The simplest nonlinear diﬀusion equation is known as† Burgers’ equation
ut + uux = γ uxx,
(8.70)
which is obtained by appending a simple linear diﬀusion term to the nonlinear transport
equation (2.31). As with the heat equation, the diﬀusion coeﬃcient γ ≥0 must be nonneg-
ative in order that the initial value problem be well-posed in forwards time. In ﬂuid and
gas dynamics, one interprets the right-hand side as modeling the eﬀect of viscosity, and
so Burgers’ equation represents a very simpliﬁed version of the equations of viscous ﬂuid
ﬂows, including the celebrated and widely applied Navier–Stokes equations (1.4), [122].
When the viscosity coeﬃcient vanishes, γ = 0, Burgers’ equation reduces to the nonlinear
transport equation (2.31), which, as a consequence, is often referred to as the inviscid
Burgers’ equation.
†
The equation is named after the Dutch physicist Johannes Martinus Burgers, [26], and so
the apostrophe goes after the “s”. Burgers’ equation was apparently ﬁrst studied as a physical
model by the British (later American) applied mathematician Harry Bateman, [13], in the early
twentieth century.

316
8 Linear and Nonlinear Evolution Equations
Since Burgers’ equation is of ﬁrst order in t, we expect that its solutions will be
uniquely prescribed by their initial values
u(0, x) = f(x),
−∞< x < ∞.
(8.71)
(For simplicity, we will ignore boundary eﬀects here.) Small, slowly varying solutions —
more speciﬁcally, those for which both | u(t, x) | and | ux(t, x) | are small — tend to act like
solutions to the heat equation, smoothing out and decaying to 0 as time progresses. On the
other hand, when the solution is large or rapidly varying, the nonlinear term tends to play
the dominant role, and we might expect the solution to behave like nonlinear transport
waves, perhaps steepening into some sort of shock. But, as we will learn, the smoothing
eﬀect of the diﬀusion term, no matter how small, ultimately prevents the appearance of a
discontinuous shock wave. Indeed, it can be proved that, under rather mild assumptions
on the initial data, the solution to the initial value problem (8.70–71) remains smooth and
well deﬁned for all subsequent times, [122].
The simplest explicit solutions are the traveling waves, for which
u(t, x) = v(ξ) = v(x −ct),
where
ξ = x −ct,
(8.72)
indicates a ﬁxed proﬁle, moving to the right with constant speed c. By the chain rule,
∂u
∂t = −cv′(ξ),
∂u
∂x = v′(ξ),
∂2u
∂x2 = v′′(ξ).
Substituting these expressions into Burgers’ equation (8.70), we conclude that v(ξ) must
satisfy the nonlinear second-order ordinary diﬀerential equation
−cv′ + vv′ = γ v′′.
This equation can be solved by ﬁrst integrating both sides with respect to ξ, and so
γ v′ = k −cv + 1
2 v2,
where k is a constant of integration.
Following the analysis after Proposition 2.3, as
ξ →±∞, the bounded solutions to such an autonomous ﬁrst-order ordinary diﬀerential
equation tend to one of the ﬁxed points provided by the roots of the quadratic polynomial
on the right-hand side. Therefore, for there to be a bounded traveling-wave solution v(ξ),
the quadratic polynomial must have two real roots, which requires k < 1
2 c2. Assuming
this holds, we rewrite the equation in the form
2γ dv
dξ = (v −a)(v −b),
where
c = 1
2 (a + b),
k = 1
2 ab.
(8.73)
To obtain bounded solutions, we must require a < v < b. Integrating (8.73) by the usual
method, cf. (2.19), we ﬁnd

2γ dv
(v −a)(v −b) =
2γ
b −a log
 b −v
v −a

= ξ −δ,
where δ is another constant of integration. Solving for
v(ξ) = ae(b−a)(ξ−δ)/(2γ) + b
e(b−a)(ξ−δ)/(2γ) + 1 ,

8.4 Nonlinear Diﬀusion
317
γ = .25
γ = .1
γ = .025
Figure 8.5.
Traveling-wave solutions to Burgers’ equation.

and recalling (8.73), we conclude that the bounded traveling-wave solutions to Burgers’
equation all have the explicit form
u(t, x) = ae(b−a)(x−ct−δ)/(2γ) + b
e(b−a)(x−ct−δ)/(2γ) + 1 ,
(8.74)
where a < b and δ are arbitrary constants. Observe that our solution is a monotonically
decreasing function of x, with asymptotic values
lim
x →−∞u(t, x) = b,
lim
x →∞u(t, x) = a,
at large distances. The wave travels to the right, unchanged in form, with speed c = 1
2 (a+b)
equal to the average of its asymptotic values. In particular, if a = −b, the result is a
stationary-wave solution. In Figure 8.5 we graph sample proﬁles, corresponding to a = .1,
b = 1, for three diﬀerent values of the diﬀusion coeﬃcient. Note that the smaller γ is, the
sharper the transition layer between the two asymptotic values of the solution.
In the inviscid limit as the diﬀusion becomes vanishingly small, γ →0, the traveling-
wave solutions (8.74) converge to the step shock-wave solutions (2.51) of the nonlinear
transport equation. Indeed, this can be proved to hold in general: as γ →0, solutions to
Burgers’ equation (8.70) converge to the corresponding solutions to the nonlinear transport
equation (2.31) that are subject to the Rankine–Hugoniot and entropy conditions (2.53, 55).
Thus, the method of vanishing viscosity allows one to monitor solutions to the nonlinear
transport equation as they evolve into regimes where multiple shocks interact and merge.
This approach also reconﬁrms our physical intuition, in that most physical systems retain
a very small dissipative component that serves to mollify abrupt discontinuities that might
appear in a theoretical model that fails to take friction or viscous eﬀects into account. In
the modern theory of partial diﬀerential equations, the resulting viscosity solution method
has been successfully used to characterize the discontinuous solutions to a broad range of
inviscid nonlinear wave equations as limits of classical solutions to a viscously regularized
system. We refer the interested reader to [64, 107, 122] for further details.
The Hopf–Cole Transformation
By a remarkable stroke of good fortune, the nonlinear Burgers’ equation can be con-
verted into the linear heat equation and thereby explicitly solved.
The transformation
that linearizes the nonlinear Burgers’ equation ﬁrst appeared in an obscure exercise in a
nineteenth-century diﬀerential equations textbook, [41; vol. 6, p. 102]. Its rediscovery by

318
8 Linear and Nonlinear Evolution Equations
the applied mathematicians Eberhard Hopf, [56], and Julian Cole, [32], was a milestone
in the modern era of nonlinear partial diﬀerential equations, and it is now named the
Hopf–Cole transformation in their honor.
In general, linearization — that is, converting a given nonlinear diﬀerential equation
into a linear equation — is extremely challenging, and, in most instances, impossible. On
the other hand, the reverse process — “nonlinearizing” a linear equation — is trivial:
any nonlinear change of dependent variables will do the trick!
However, the resulting
nonlinear equation, while evidently linearizable by inverting the change of variables, is
rarely of independent interest. But sometimes there is a lucky accident, and the resulting
linearization of a physically relevant nonlinear diﬀerential equation can have a profound
impact on our understanding of more complicated nonlinear systems.
In the present context, our starting point is the linear heat equation
vt = γ vxx.
(8.75)
Among all possible nonlinear changes of dependent variable, one of the simplest that might
spring to mind is an exponential function. Let us, therefore, investigate the eﬀect of an
exponential change of variables
v(t, x) = eαϕ(t,x),
so
ϕ(t, x) = 1
α log v(t, x),
(8.76)
where α is a nonzero constant. The function ϕ(t, x) is real, provided v(t, x) is a positive
solution to the heat equation.
Fortunately, this is not hard to arrange: if the initial
data v(0, x) > 0 is strictly positive, then, as a consequence of the Maximum Principle in
Corollary 8.7, the resulting solution v(t, x) > 0 is positive for all t > 0.
To determine the diﬀerential equation satisﬁed by the function ϕ, we invoke the chain
and product rules to diﬀerentiate (8.76):
vt = αϕt eαϕ,
vx = αϕx eαϕ,
vxx = (αϕxx + α2 ϕ2
x) eαϕ.
Substituting the ﬁrst and last formulas into the heat equation (8.75) and canceling a com-
mon exponential factor, we conclude that ϕ(t, x) satisﬁes the nonlinear partial diﬀerential
equation
ϕt = γ ϕxx + γ αϕ2
x,
(8.77)
known as the potential Burgers’ equation, for reasons that will soon become apparent.
The second step in the process is to diﬀerentiate the potential Burgers’ equation with
respect to x; the result is
ϕtx = γ ϕxxx + 2γ αϕx ϕxx.
(8.78)
If we now set
∂ϕ
∂x = u,
(8.79)
so that ϕ acquires the status of a potential function, then the resulting partial diﬀerential
equation
ut = γ uxx + 2γ αuux
coincides with Burgers’ equation (8.70) when α = −1/(2γ).
In this manner, we have
arrived at the famous Hopf–Cole transformation.

8.4 Nonlinear Diﬀusion
319
Figure 8.6.
Trignometric solution to Burgers’ equation.

Theorem 8.9.
If v(t, x) > 0 is any positive solution to the linear heat equation
vt = γ vxx, then
u(t, x) = ∂
∂x

−2γ log v(t, x)

= −2γ
vx
v
(8.80)
solves Burgers’ equation ut + uux = γ uxx.
Do all solutions to Burgers’ equation arise in this way? In order to answer this question,
we run the argument in reverse. First, choose a potential function ϕ(t, x) that satisﬁes
(8.79); for example,
ϕ(t, x) =
 x
0
u(t, y) dy.
If u(t, x) is any solution to Burgers’ equation, then ϕ(t, x) satisﬁes (8.78). Integrating both
sides of the latter equation with respect to x, we conclude that
ϕt = γ ϕxx + γ α ϕ 2
x + g(t),
for some integration “constant” g(t).
Thus, unless g(t) ≡0, our potential function ϕ
doesn’t satisfy the potential Burgers’ equation (8.77), but that is because we chose the
“wrong” potential. Indeed, if we deﬁne
ϕ(t, x) = ϕ(t, x) −G(t),
where
G′(t) = g(t),
then
ϕt = ϕt −g(t) = γ ϕxx + γ α ϕ 2
x = γ ϕxx + γ αϕ2
x,
and hence the modiﬁed potential ϕ(t, x) is a solution to the potential Burgers’ equation
(8.77). From this it easily follows that
v(t, x) = e−ϕ(t,x)/(2γ)
(8.81)
is a positive solution to the heat equation, from which the Burgers’ solution u(t, x) can
be recovered through (8.80). We conclude that every solution to Burgers’ equation comes
from a positive solution to the heat equation via the Hopf–Cole transformation.

320
8 Linear and Nonlinear Evolution Equations
Example 8.10. As a simple example, the separable solution
v(t, x) = a + b e−γ ω2 t cos ωx
to the heat equation leads to the following solution to Burgers’ equation:
u(t, x) =
2γ b ω sin ωx
a eγ ω2 t + b cos ωx .
(8.82)
A representative example is plotted in Figure 8.6.
We should require that a > | b | in
order that v(t, x) > 0 be a positive solution to the heat equation for t ≥0; otherwise the
resulting solution to Burgers’ equation will have singularities at the roots of u — as in
the ﬁrst graph in Figure 8.6. This family of solutions is primarily aﬀected by the viscosity
term, and rapidly decays to zero.
To solve the initial value problem (8.70–71) for Burgers’ equation, we note that, under
the Hopf–Cole transformation (8.80),
v(0, x) = exp

−ϕ(0, x)
2γ

= exp

−1
2γ
 x
0
f(y) dy

≡h(x).
(8.83)
Remark: The lower limit of the integral can be changed from 0 to any other convenient
value. The only eﬀect is to multiply v(t, x) by an overall constant, which does not change
the ﬁnal form of u(t, x) in (8.80).
According to formula (8.16) (adapted to general diﬀusivity, as in Exercise 8.2.3), the
solution to the initial value problem (8.75, 83) for the heat equation can be expressed as a
convolution integral with the fundamental solution
v(t, x) =
1
2√πγ t
 ∞
−∞
e−(x−ξ)2/(4γ t) h(ξ) dξ.
Therefore, setting v(t, x) = 2√πγ t v(t, x), the solution to the Burgers’ initial value problem
(8.70–71), valid for t > 0, is given by
u(t, x) = −
2γ
v(t, x)
∂v
∂x ,
where
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
v(t, x) =
 ∞
−∞
e−H(t,x;ξ) dξ,
H(t, x; ξ) = (x −ξ)2
4γ t
+
1
2γ
 ξ
0
f(η) dη.
(8.84)
Example 8.11.
To demonstrate the smoothing eﬀect of the diﬀusion terms, let us
see what happens to the initial data
u(0, x) =
 a,
x < 0,
b,
x > 0,
(8.85)
in the form of a step function. We assume that a > b, which corresponds to a shock wave
in the inviscid limit γ = 0. (In Exercise 8.4.4, the reader is asked to analyze the case a < b,
which corresponds to a rarefaction wave.) In this case,
H(t, x; ξ) = (x −ξ)2
4γ t
+
⎧
⎪
⎪
⎨
⎪
⎪
⎩
aξ
2γ ,
ξ < 0,
bξ
2γ ,
ξ > 0.
(8.86)

8.4 Nonlinear Diﬀusion
321
t = .01
t = .5
t = 1
t = 2
Figure 8.7.
Shock-wave solution to Burgers’ equation.

After some algebraic manipulations, the solution (8.84) is found to have the explicit form
u(t, x) = a +
b −a
1 + exp
b −a
2γ
(x −ct)

erfc
x −at
2 √γ t
 3
erfc
bt −x
2 √γ t
 ,
(8.87)
with c =
1
2 (a + b), where erfc z = 1 −erf z denotes the complementary error function
(8.43).
The solution, for a = 1, b = .1, and γ = .03, is plotted at various times in
Figure 8.7. Observe that, as with the heat equation, the jump discontinuity is immediately
smoothed out, and the solution soon assumes the form of a smoothly varying transition
between its two original heights.
The larger the diﬀusion coeﬃcient in relation to the
jump magnitude, the more pronounced the smoothing eﬀect. Moreover, as γ →0, the
solution u(t, x) converges to the shock-wave solution (2.51) to the transport equation, in
which the speed of the shock is c, the average of the step heights — in accordance with
the Rankine–Hugoniot shock rule. Indeed, in view of (2.88),
lim
z →∞erfc z = 0,
lim
z →−∞erfc z = 2.
(8.88)
Thus, for t > 0, as γ →0, the ratio of the two complementary error functions in (8.87)
tends to ∞when x < bt, to 1 when bt < x < at, and to 0 when x > at. On the other
hand, since a > b, the exponential term tends to ∞when x < ct, and to 0 when x > ct.
Put together, these imply that the solution u(t, x) →a when x < ct, while u(t, x) →b,
when x > ct, thus proving convergence to the shock-wave solution.
Example 8.12.
Consider the case in which the initial data u(0, x) = δ(x) is a
concentrated delta function impulse at the origin. In the solution formula (8.84), starting
the integral for H(t, x; ξ) at 0 is problematic, but as noted earlier, we are free to select any

322
8 Linear and Nonlinear Evolution Equations
5
10
1
t = 1
5
10
1
t = 5
5
10
1
t = 10
5
10
1
t = 50
Figure 8.8.
Triangular-wave solution to Burgers’ equation.

other starting point, e.g., −∞. Thus, we take
H(t, x; ξ) = (x −ξ)2
4γ t
+
1
2γ
 ξ
−∞
δ(η) dη =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
(x −ξ)2
4γ t
,
ξ < 0,
1
2γ + (x −ξ)2
4γ t
,
ξ > 0.
We then evaluate
v(t, x) =
 ∞
−∞
e−H(t,x;ξ) dξ = √π γ t

1 −erf

x
2 √γ t

+ e−1/(2γ)

1 + erf

x
2 √γ t
 
.
Therefore, the solution to the initial value problem is
u(t, x) = −
2γ
v(t, x)
∂v
∂x = 2
 γ
π t
e−x2/(4γ t)
coth
 1
4γ

−erf

x
2 √γ t
 ,
(8.89)
where
coth z = cosh z
sinh z = ez + e−z
ez −e−z = e2z + 1
e2z −1
is the hyperbolic cotangent function. A graph of this solution when γ = .02 and a = 1
appears in Figure 8.8. As you can see, the initial concentration diﬀuses out, but, in contrast
to the heat equation, does not remain symmetric, since the nonlinear advection term causes
the wave to steepen in front. Eventually, as the eﬀect of the diﬀusion accumulates, the
propagating triangular wave becomes vanishingly small.

8.5 Dispersion and Solitons
323
Exercises
8.4.1. Find the solution to Burgers’ equation that has the following initial data:
u(0, x) =
(a) σ(x),
(b) σ(−x),
(c)
 1,
0 < x < 1,
0,
otherwise.
8.4.2. Starting with the heat equation solution v(t, x) = 1 + t−1/2 e−x2/(4γ t), ﬁnd the corre-
sponding solution to Burgers’ equation and discuss its behavior.
8.4.3. Justify the solution formula (8.87).
♦8.4.4.(a) Prove that
lim
z →∞z ez2
erfc z = 1/√π .
(b) Show that when a < b, the Burgers’
solution (8.87) converges to the rarefaction wave (2.54) in the inviscid limit γ →0+.
8.4.5. True or false: If u(t, x) solves Burgers’ equation for the step function initial condition
u(0, x) = σ(x), then v(t, x) = ux(t, x) solves the initial value problem with v(0, x) = δ(x).
8.4.6. True or false: If v(t, x) is as given in (8.84), then
∂v
∂x =
	 ∞
−∞
ξ −x
2γ t e−H(t,x;ξ) dξ,
and hence the solution to the Burgers’ initial value problem (8.70–71) can be written as
u(t, x) =
	 ∞
−∞
x −ξ
t
e−H(t,x;ξ) dξ
	 ∞
−∞e−H(t,x;ξ) dξ
,
where
H(t, x; ξ) = (x −ξ)2
4γ t
+
1
2γ
	 ξ
0 f(η) dη.
8.4.7. Show that if u(t, x) solves Burgers’ equation, then U(t, x) = u(t, x −ct) + c is also a
solution. What is the physical interpretation of this symmetry?
8.4.8.(a) What is the eﬀect of a scaling transformation (t, x, u) −→(αt, β x, λu) on Burgers’
equation?
(b) Use your result to solve the initial value problem for the rescaled Burgers’
equation Ut + ρU Ux = σ Uxx, U(0, x) = F(x).
♥8.4.9.(a) Find all scaling symmetries of Burgers’ equation. (b) Determine the ordinary dif-
ferential equation satisﬁed by the similarity solutions. (c) True or false: The Hopf–Cole
transformation maps similarity solutions of the heat equation to similarity solutions of
Burgers’ equation.
8.4.10. What happens if you nonlinearize the heat equation (8.75) using the change of vari-
ables
(a) v = ϕ2;
(b) v = √ϕ ;
(c) v = log ϕ ?
8.4.11. What partial diﬀerential equation results from applying the exponential change of vari-
ables (8.76) to:
(a) the wave equation vtt = c2vxx?
(b) the Laplace equation vxx + vyy = 0?
8.5 Dispersion and Solitons
In this section, we ﬁnally venture beyond the by now familiar terrain of second-order
partial diﬀerential equations.
While considerably less common than those of ﬁrst and
second order, higher-order equations arise in certain applications, particularly third-order

324
8 Linear and Nonlinear Evolution Equations
dispersive models for wave motion, [2, 122], and fourth-order systems modeling elastic
plates and shells, [7].
We will focus our attention on two basic third-order evolution
equations. The ﬁrst is a simple linear equation with a third derivative term. It arises as
a simpliﬁed model for unidirectional wave motion, and thus has more in common with
ﬁrst-order transport equations than with the second-order dissipative heat equation. The
third-order derivative induces a process of dispersion, in which waves of diﬀerent frequencies
propagate at diﬀerent speeds. Thus, unlike the ﬁrst- and second-order wave equations, in
which waves maintain their initial proﬁle as they move, dispersive waves will spread out and
decay even while conserving energy. Waves on the surface of a liquid are familiar examples
of dispersive waves — an initially concentrated disturbance, caused by, say, throwing a
rock in a pond, spreads out over the surface as its diﬀerent vibrational components move
oﬀat diﬀerent speeds.
Our second example is a remarkable nonlinear third-order evolution equation known
as the Korteweg–de Vries equation, which combines dispersive eﬀects with nonlinear trans-
port. As with Burgers’ equation (but for very diﬀerent mathematical reasons), the dis-
persive term thwarts the tendency for solutions to break into shock waves, and, in fact,
classical solutions exist for all time. Moreover, a general localized initial disturbance will
break up into a ﬁnite number of solitary waves; the taller the wave, the faster it moves.
Even more remarkable are the interactive properties of these solitary waves. One ordinar-
ily expects nonlinearity to induce very complicated and not easily predictable behavior.
However, when two solitary-wave solutions to the Korteweg–de Vries equation collide, they
eventually emerge from the interaction unchanged, save for a phase shift. This unexpected
and remarkable phenomenon was ﬁrst detected through numerical simulations in the 1960s
and distinguished with the neologism soliton. It was then found that solitons appear in
a surprising number of basic nonlinear physical models. The investigation of their mathe-
matical properties has had deep ramiﬁcations, not just within partial diﬀerential equations
and ﬂuid mechanics, but throughout applied mathematics and theoretical physics; it has
even contributed to the solution of long-outstanding problems in complex function theory.
Further development of the modern theory and amazing properties of integrable soliton
equations can be found in [2, 36].
Linear Dispersion
The simplest nontrivial third-order partial diﬀerential equation is the linear equation
ut + uxxx = 0,
(8.90)
which models the unidirectional† propagation of linear dispersive waves. To avoid compli-
cations engendered by boundary conditions, we shall initially look only at solutions on the
entire line, so −∞< x < ∞. Since the equation involves only a ﬁrst-order time derivative,
one expects its solutions to be uniquely speciﬁed by a single initial condition
u(0, x) = f(x),
−∞< x < ∞.
(8.91)
†
Bidirectional propagation, as we saw in the wave equation, requires a second-order time
derivative. As in the d’Alembert solution to the second-order wave equation, the reduction to a
unidirectional model is based on an (approximate) factorization of the bidirectional operator.

8.5 Dispersion and Solitons
325
t = 0
t = .1
t = .5
t = 1
t = 5
t = 30
Figure 8.9.
Gaussian solution to the dispersive wave equation.

In wave mechanics, u(t, x) represents the height of the ﬂuid at time t and position x, and
the initial condition (8.91) speciﬁes the initial disturbance.
As with the heat equation (and, indeed, any linear constant-coeﬃcient evolution equa-
tion), the Fourier transform is an eﬀective tool for solving the initial value problem on the
real line. Assuming that the solution u(t, ·) ∈L2(R) remains square integrable at all times
t (a fact that can be justiﬁed a priori — see Exercise 8.5.18(b)), let
u(t, k) =
1
√
2π
 ∞
−∞
u(t, x) e−i kx dx
be its spatial Fourier transform. Owing to its eﬀect on derivatives, the Fourier transform
converts the partial diﬀerential equation (8.90) into a ﬁrst-order linear ordinary diﬀerential
equation:
∂u
∂t + ( i k)3 u = ∂u
∂t −i k3 u = 0,
(8.92)
in which the frequency variable k appears as a parameter. The corresponding initial con-
ditions
u(0, k) = f(k) =
1
√
2π
 ∞
−∞
f(x) e−i kx dx
(8.93)
are provided by the Fourier transform of (8.91). The solution to the initial value problem
(8.92–93) is
u(t, k) = f(k) e i k3 t.
Inverting the Fourier transform yields the explicit formula for the solution
u(t, x) =
1
√
2π
 ∞
−∞
f(k) e i (kx+k3 t) dk
(8.94)
to the initial value problem (8.90–91) for the dispersive wave equation.

326
8 Linear and Nonlinear Evolution Equations
Example 8.13. Suppose that the initial proﬁle
u(0, x) = f(x) = e−x2
is a Gaussian. According to our table of Fourier transforms (see page 272),
f(k) = e−k2/4
√
2
,
and hence the corresponding solution to the dispersive wave equation (8.90) is
u(t, x) =
1
2√π
 ∞
−∞
e i (kx+k3 t)−k2/4 dk =
1
2√π
 ∞
−∞
e−k2/4 cos(kx + k3 t) dk;
the imaginary part vanishes thanks to the oddness of the integrand. (Indeed, the solution
must be real, since the initial data is real.) A plot of the solution at various times appears
in Figure 8.9. Note the propagation of initially rapid oscillations to the rear (negative x)
of the initial disturbance. The dispersion causes the oscillations to gradually spread out
and decrease in amplitude, with the eﬀect that u(t, x) →0 uniformly as t →∞, even
though, according to Exercise 8.5.7, both the mass M =
 ∞
−∞
u(t, x) dx and the energy
E =
 ∞
−∞
u(t, x)2 dx of the wave are conserved, i.e., are both constant in time.
Example 8.14. The fundamental solution to the dispersive wave equation is gener-
ated by a concentrated initial disturbance:
u(0, x) = δ(x).
The Fourier transform of the delta function is just δ(k) = 1/
√
2π . Therefore, the corre-
sponding solution (8.94) is
u(t, x) = 1
2π
 ∞
−∞
e i (kx+k3 t) dk = 1
π
 ∞
0
cos(kx + k3 t) dk,
(8.95)
since the solution is real (or, equivalently, the imaginary part of the integrand is odd),
while the real part of the integrand is even.
A priori, it appears that the integral (8.95) does not converge, because the integrand
does not go to zero as | k | →∞. However, the increasingly rapid oscillations induced by
the cubic term tend to cancel each other out and allow convergence. To prove this, given
l > 0, we perform a (non-obvious) integration by parts:
 l
0
cos(kx + k3 t) dk =
 l
0
1
x + 3k2 t
d
dk sin(kx + k3 t) dk
(8.96)
= sin(kx + k3 t)
x + 3k2 t

l
k=0
−
 l
0
d
dk

1
x + 3k2 t

sin(kx + k3 t) dk
= sin(lx + l3 t)
x + 3l2 t
+
 l
0
6kt sin(kx + k3 t)
(x + 3k2 t)2
dk.
Provided t ̸= 0, as l →∞, the ﬁrst term on the right goes to zero, while the ﬁnal integral
converges absolutely due to the rapid decay of the integrand.

8.5 Dispersion and Solitons
327
t = .03
t = .1
t = .33333
t = 1
t = 5
t = 20
Figure 8.10.
Fundamental solution to the dispersive wave equation.

While the integral in the solution formula (8.95) cannot be evaluated in terms of
elementary functions, it is related to the integral deﬁning the Airy function
Ai(z) = 1
π
 ∞
0
cos

sz + 1
3 s3
ds,
(8.97)
an important special function, [86], that was ﬁrst employed by the nineteenth-century
British applied mathematician George Airy in his studies of optical caustics (the focusing
of light waves through a lens, e.g., a magnifying glass) and rainbows, [4]. Indeed, applying
the change of variables
s = k
3√
3t ,
z =
x
3√
3t
,
to the Airy function integral (8.97), we deduce that the fundamental solution to the dis-
persive wave equation (8.90) can be written as
u(t, x) =
1
3√
3t
Ai
 x
3√
3t

.
(8.98)
See Figure 8.10 for a graph of the solution at several times; in particular, at t = 1/3
the solution is exactly the Airy function. We see that the immediate eﬀect of the initial
delta impluse is to spawn a highly oscillatory wave trailing oﬀto −∞. (As with the heat
equation, signals propagate with inﬁnite speed.) As time progresses, the dispersive eﬀects
cause the oscillations to spread out, with their overall amplitude decaying in proportion to
t−1/3. On the other hand, as t →0+, the solution becomes more and more oscillatory for
negative x, and so converges weakly to the initial delta function. We also note that (8.98)
has the form of a similarity solution, since it is invariant under the scaling symmetry
(t, x, u) −→(λ−3t, λ−1 x, λ u).
Equation (8.98) gives the response to an initial delta function concentrated at the

328
8 Linear and Nonlinear Evolution Equations
t = .1
t = .2
t = .3
t = .4
t = .5
t = .6
Figure 8.11.
Periodic dispersion at irrational (with respect to π) times.

origin. By translation invariance, we immediately deduce that
F(t, x; ξ) =
1
3√
3t Ai
x −ξ
3√
3t

is the fundamental solution corresponding to an initial delta impulse at x = ξ. Therefore,
we can use linear superposition to ﬁnd an explicit formula for the solution to the initial
value problem that bypasses the Fourier transform. Namely, writing the general initial
data as a superposition of delta functions,
u(0, x) = f(x) =
 ∞
−∞
f(ξ) δ(x −ξ) dξ,
we conclude that the resulting solution is the selfsame combination of fundamental solu-
tions:
u(t, x) =
1
3√
3t
 ∞
−∞
f(ξ) Ai
x −ξ
3√
3t

dξ.
(8.99)
Example 8.15.
Dispersive Quantization.
Let us investigate the periodic initial-
boundary value problem for our basic linear dispersive equation on the interval −π ≤x ≤π:
ut + uxxx = 0,
u(t, −π) = u(t, π),
ux(t, −π) = ux(t, π),
uxx(t, −π) = uxx(t, π),
(8.100)
with initial data u(0, x) = f(x). The Fourier series formula for the resulting solution is
straightforwardly constructed:
u(t, x) =
∞

k=−∞
ck e i (kx+k3t),
(8.101)
where ck are the usual (complex) Fourier coeﬃcients (3.65) of the initial data f(x).

8.5 Dispersion and Solitons
329
t =
1
30 π
t =
1
15 π
t =
1
10 π
t =
2
15 π
t = 1
6 π
t = 1
5 π
Figure 8.12.
Periodic dispersion at rational (with respect to π) times.

Let us take the initial data to be the unit step function: u(0, x) = σ(x). In view of its
Fourier series (3.67), the resulting solution formula (8.101) becomes
u(t, x) = 1
2 −
i
π
∞

l=−∞
e i [(2l+1)x+(2l+1)3t]
2l + 1
= 1
2 + 2
π
∞

l=0
sin

(2l + 1)x + (2l + 1)3t

2l + 1
.
(8.102)
Let us graph this solution. At times uniformly spaced by Δt = .1, the resulting solution
proﬁles are plotted in Figure 8.11. The solution appears to have a continuous but fractal-
like structure, reminiscent of Weierstrass’ continuous but nowhere diﬀerentiable function,
[55; pp. 401–421]. The temporal evolution continues in this fashion until the initial data
are formed again at t = 2π, after which the process periodically repeats.
However, when the times are spaced by Δt =
1
30 π ≈.10472, the resulting solution
proﬁles, as plotted in Figure 8.12, are strikingly diﬀerent! Indeed, as you are asked to
prove in Exercise 8.5.8, at each rational time t = 2π p/q, where p, q are integers, the
solution (8.102) to the initial-boundary value problem is discontinuous but constant on
subintervals of length 2π/q. This remarkable behavior, in which the solution proﬁles of
linearly dispersive periodic boundary value problems have markedly diﬀerent behaviors at
rational and irrational times (with respect to π), was ﬁrst observed, in the 1990’s, in optics
and quantum mechanics by the British physicist Michael Berry, [16, 115], and named the
Talbot eﬀect, after an optical experiment conducted by the inventor of the photographic
negative, William Henry Fox Talbot. While writing this book, I rediscovered the eﬀect,
which I like to call dispersive quantization, [88], and found that it arises in a wide range
of linearly dispersive periodic initial-boundary value problems, [30].

330
8 Linear and Nonlinear Evolution Equations
The Dispersion Relation
As noted earlier, a key feature of the third-order wave equation (8.90) is that waves disperse,
in the sense that those of diﬀerent frequencies move at diﬀerent speeds. Our goal now is
to better understand the dispersion process. To this end, consider a solution whose initial
proﬁle
u(0, x) = e i kx
is a complex oscillatory function. Since the initial data does not decay as | x | →∞, we
cannot use the Fourier integral solution formula (8.94) directly. Instead, anticipating the
induced wave to exhibit temporal oscillations, let us try an exponential solution ansatz
u(t, x) = e i (kx−ω t)
(8.103)
representing a complex oscillatory wave of temporal frequency ω and wave number (spatial
frequency) k. Since
∂u
∂t = −i ω e i (kx−ω t),
∂3u
∂x3 = −i k3 e i (kx−ω t),
(8.103) satisﬁes the partial diﬀerential equation (8.90) if and only if its frequency and wave
number satisfy the dispersion relation
ω = −k3.
(8.104)
Therefore, the exponential solution (8.103) of wave number k takes the form
u(t, x) = e i (kx+k3 t).
(8.105)
Our Fourier transform formula (8.94) for the solution can thus be viewed as a (continu-
ous) linear superposition of these elementary exponential solutions. In general, to ﬁnd the
dispersion relation for a linear constant-coeﬃcient partial diﬀerential equation, one substi-
tutes the exponential ansatz (8.103). On cancellation of the common exponential factors,
the result is an equation expressing the frequency ω as a function of the wave number k.
Any exponential solution (8.103) is automatically in the form of a traveling wave, since
we can write
u(t, x) = e i (kx−ω t) = e i k(x−cp t),
where
cp = ω
k
(8.106)
is the wave speed or, as it is more usually called, the phase velocity. If the dispersion
relation is linear in the wave number, ω = ck, as occurs in the linear transport equation
ut + cux = 0, then all waves move at an identical speed cp = c, and hence localized
disturbances stay localized as they propagate through the medium. In the dispersive case,
ω is no longer a linear function of k, and so waves of diﬀerent spatial frequencies move at
diﬀerent speeds. In the particular case (8.90), those with wave number k move at speed
cp = ω/k = −k2, and so the higher the wave number, the faster the wave propagates to the
left. As the individual exponential constituents separate, the overall eﬀect is the dispersive
decay of an initially localized wave, with slowly diminishing amplitude and increasingly
rapid oscillation as x →−∞.
The general solution to the linear partial diﬀerential equation under consideration is
then built up by linear superposition of the exponential solutions,
u(t, x) =
 ∞
−∞
e i (kx−ω t)g(k) dk,
(8.107)

8.5 Dispersion and Solitons
331
where ω = ω(k) is determined by the relevant dispersion relation. While the evolution of
the individual waves is an immediate consequence of the dispersion relation, the evolution
of the localized wave packet represented by (8.107) is less evident. To determine its speed
of propagation, let us switch to a moving coordinate frame of speed c by setting x = c t+ξ.
The solution formula (8.107) then becomes
u(t, c t + ξ) =
 ∞
−∞
e i (ck−ω)te i k ξ g(k) dk.
(8.108)
For a ﬁxed value of ξ, the integral is of the general oscillatory form
H(t) =
 ∞
−∞
e i ϕ(k) t h(k) dk,
(8.109)
where, in our case, ϕ(k) = ck −ω(k) and h(k) = e i k ξ g(k). We are interested in under-
standing the behavior of such an oscillatory integral as t →∞. Now, if ϕ(k) = k, then
(8.109) is just a Fourier integral, (7.9), and, as we learned in Chapter 7, H(t) →0 as
t →∞, for any reasonable function h(k). Intuitively, the increasingly rapid oscillations of
the exponential factor tend to cancel each other out in the high-frequency limit. A similar
result holds wherever ϕ(k) has no stationary points, i.e., ϕ′(k) ̸= 0, since one can then
perform a local change of variables k = ϕ(k) to convert that part of the oscillatory integral
to Fourier form, and again the increasingly rapid oscillations cause the limit to vanish. In
this fashion, we arrive at the key insight of Stokes and Kelvin that produced the powerful
Method of Stationary Phase. Namely, for large t ≫0, the primary contribution to the
highly oscillatory integral (8.109) occurs at the stationary points of the phase function,
that is, where ϕ′(k) = 0. A rigorous justiﬁcation of the method, along with precise error
bounds, can be found in [85].
In the present context, the Method of Stationary Phase implies that the most signiﬁ-
cant contribution to the integral (8.108) occurs when
0 = d
dk (ω −ck) = dω
dk −c.
(8.110)
Thus, surprisingly, the principal contribution of the components at wave number k is felt
when moving at the group velocity
cg = dω
dk .
(8.111)
Interestingly, unless the dispersion relation is linear in the wave number, the group velocity
(8.111), which determines the speed of propagation of the energy, is not the same as the
phase velocity (8.106), which governs the speed of propagation of an individual oscillatory
wave. For example, in the case of the dispersive wave equation (8.90), ω = −k3, and so
cg = −3k2, which is three times as fast as the phase velocity, cp = ω/k = −k2. Thus, the
energy propagates faster than the individual waves. This can be observed in Figure 8.9:
while the bulk of the disturbance is spreading out rather rapidly to the left, the individual
wave crests are moving slower.
On the other hand, the dispersion relation associated with deep water waves is (ig-
noring physical constants) ω =
√
k , [122]. Now, the phase velocity is cp = ω/k = 1/
√
k ,
whereas the group velocity is cg = dω/dk = 1/(2
√
k ) = 1
2 cp, and so the individual waves
move twice as fast as the speed of propagation of the underlying wave energy. For an ex-
perimental veriﬁcation, just throw a stone in a still pond. An individual wave crest emerges

332
8 Linear and Nonlinear Evolution Equations
in back and then steadily grows as it moves through the disturbance, eventually subsiding
and disappearing into the still water ahead of the expanding wave packet triggered by the
stone. The distinction between group velocity and phase velocity is also well understood
by surfers, who know that the largest waves seen out to sea are not the largest when they
break upon the shore.
Exercises
8.5.1. Sketch a picture of the solution for the initial value problem in Example 8.13 at times
t = −.1, −.5, and −1.
♠8.5.2.(a) Write down an integral formula for the solution to the dispersive wave equation (8.90)
with initial data u(0, x) =
 1,
0 < x < 1,
0,
otherwise.
(b) Use a computer package to plot your
solution at several times and discuss what you observe.
8.5.3.(a) Write down an integral formula for the solution to the initial value problem
ut + ux + uxxx = 0,
u(0, x) = f(x).
(b) Based on the results in Example 8.13, discuss the behavior of the solution to the initial
value problem u(0, x) = e−x2 as t increases.
8.5.4. Find the (i) dispersion relation, (ii) phase velocity, and (iii) group velocity for the fol-
lowing partial diﬀerential equations. Which are dispersive?
(a) ut + ux + uxxx = 0,
(b) ut = uxxxxx, (c) ut + ux −uxxt = 0, (d) utt = c2uxx, (e) utt = uxx −uxxxx.
8.5.5. Find all linear evolution equations for which the group velocity equals the phase velocity.
Justify your answer.
8.5.6. Show that the phase velocity is greater than the group velocity if and only if the phase
velocity is a decreasing function of k for k > 0 and an increasing function of k for k < 0.
How would you observe this in a physical system?
♦8.5.7.(a) Conservation of Mass: Prove that T = u is a density associated with a conservation
law of the dispersive wave equation (8.90). What is the corresponding ﬂux? Under what
conditions is total mass conserved? (b) Conservation of Energy: Establish the same result
for the energy density T = u2.
(c) Is u3 the density of a conservation law?
♦8.5.8. Prove that when t = π p/q, where p, q are integers, the solution (8.102) is constant on
each interval π j/q < x < π(j + 1)/q for integers j ∈Z. Hint: Use Exercise 6.1.29(d).
Remark: The proof that the solution is continuous and fractal at irrational times is consid-
erably more diﬃcult, [90].
♦8.5.9.(a) Find the complex Fourier series representing the fundamental solution F(t, x; ξ) to
the periodic initial-boundary value problem (8.100). (b) Prove that at time t = 2π p/q,
where p, q are relatively prime integers, F(t, x; ξ) is a linear combination of delta functions
based at the points ξ + 2π j/q. Hint: Use Exercise 6.1.29(c). (c) Let u(t, x) be any solution
to (8.100). Prove that u(2π p/q, x) is a linear combination of a ﬁnite number of translates,
f(x −xj), of the initial data.

8.5 Dispersion and Solitons
333
The Korteweg–de Vries Equation
The simplest wave model that combines dispersion with nonlinearity is the celebrated
Korteweg–de Vries equation
ut + uxxx + uux = 0.
(8.112)
It was ﬁrst derived, in 1872, by the French applied mathematician Joseph Boussinesq, [21;
eq. (30)], [22; eqs. (283, 291)], as a model for surface waves on shallow water. Two decades
later, it was rediscovered by the Dutch applied mathematician Diederik Korteweg and his
student Gustav de Vries, [65], and, despite Boussinesq’s priority, it is nowadays named
after them. In the early 1960s, the American mathematical physicists Martin Kruskal and
Norman Zabusky, [125], used the Korteweg–de Vries equation as a continuum model for
a one-dimensional chain of masses interconnected by nonlinear springs: the Fermi–Pasta–
Ulam problem, [40]. Numerical experimentation revealed its many remarkable properties,
which were soon rigorously established. Their work sparked the rapid development of one
of the most remarkable and far-reaching discoveries of the modern era: integrable nonlinear
partial diﬀerential equations, [2, 36].
The most important special solutions to the Korteweg–de Vries equation are the trav-
eling waves. We seek solutions
u = v(ξ) = v(x −ct),
where
ξ = x −ct,
that have a ﬁxed proﬁle while moving with speed c. By the chain rule,
∂u
∂t = −cv′(ξ),
∂u
∂x = v′(ξ),
∂3u
∂x3 = v′′′(ξ).
Substituting these expressions into the Korteweg–de Vries equation (8.112), we conclude
that v(ξ) must satisfy the nonlinear third-order ordinary diﬀerential equation
v′′′ + vv′ −cv′ = 0.
(8.113)
Let us further assume that the traveling wave is localized, meaning that the solution and
its derivatives are vanishingly small at large distances:
lim
x →±∞u(t, x) =
lim
x →±∞
∂u
∂x (t, x) =
lim
x →±∞
∂2u
∂x2 (t, x) = 0.
(8.114)
This implies that we should impose the boundary conditions
lim
ξ →±∞v(ξ) =
lim
ξ →±∞v′(ξ) =
lim
ξ →±∞v′′(ξ) = 0.
(8.115)
The ordinary diﬀerential equation (8.113) can, in fact, be solved in closed form. First,
note that it has the form
d
dξ

v′′ + 1
2 v2 −cv

= 0,
and hence
v′′ + 1
2 v2 −cv = a,
where a indicates the constant of integration. The localizing boundary conditions (8.115)
imply that a = 0. Multiplying the resulting equation by v′ allows us to integrate a second
time:
0 = v′
v′′ + 1
2 v2 −cv

= d
dξ
 1
2 (v′)2 + 1
6 v3 −1
2 cv2 
= 0.

334
8 Linear and Nonlinear Evolution Equations
Figure 8.13.
Solitary wave/soliton.

Thus,
1
2 (v′)2 + 1
6 v3 −1
2 cv2 = b,
where b is a second constant of integration, which, again by the boundary conditions
(8.115), is also zero. Setting b = 0, and solving for v′, we conclude that v(ξ) satisﬁes the
autonomous ﬁrst-order ordinary diﬀerential equation
dv
dξ = v
/
c −1
3 v ,
which is integrated by the standard method:

dv
v
/
c −1
3 v
= ξ + δ,
where δ is constant. Consulting a table of integrals, e.g., [48], and then solving for v, we
conclude that the solution has the form
v(ξ) = 3c sech2 1
2
√c ξ + δ

,
(8.116)
where
sech y =
1
cosh y =
2
ey + e−y
is the hyperbolic secant function. The solution has the form graphed in Figure 8.13. It is
a symmetric, monotone, exponentially decreasing function on either side of its maximum
height of 3c. (Despite its suggestive proﬁle, it is not a Gaussian.) The resulting localized
traveling-wave solutions to the Korteweg–de Vries equation are thus
u(t, x) = 3c sech2  1
2
√c (x −ct) + δ

,
(8.117)
where c > 0 represents the wave speed — which is necessarily positive, and so all such
solutions move to the right — while δ represents an overall phase shift. The amplitude of
the wave is three times its speed, while its width is proportional to 1/√c . Thus, the taller
(and narrower) the wave, the faster it moves.
Localized traveling waves are commonly known as solitary waves. They were ﬁrst
observed in nature by the British engineer J. Scott Russell, [104], who recounts how one was
triggered by the sudden motion of a barge along an Edinburgh canal. Scott Russell ended
up chasing the propagating wave on horseback for several miles — a physical indication
of its stability.
Russell’s observations were dismissed by his contemporary Airy, who,
relying on his linearly dispersive model for surface waves (8.90), claimed that such localized

8.5 Dispersion and Solitons
335
Figure 8.14.
Interaction of two solitons.

disturbances could not exist. Much later, Boussinesq derived the proper nonlinear surface
wave model (8.112), valid for long waves in shallow water, along with its solitary wave
solutions (8.117), thereby fully exonerating Russell’s physical observations and insight.
It took almost a century before all the remarkable properties of these solutions came
to light. The most striking is how two such solitary waves interact. While linear equations
always admit a superposition principle, one cannot na¨ıvely combine two solutions to a
nonlinear equation. However, in the case of the Korteweg–de Vries equation, suppose the
initial data represent a taller solitary wave to the left of a shorter one. As time evolves,
the taller wave will move faster, and eventually catch up to the shorter one. They then
experience a complicated nonlinear interaction, as expected.
But, remarkably, after a
while, they emerge from the interaction unscathed! The smaller wave is now in back and
the larger one in front, and both unchanged in speed, amplitude, and proﬁle. They then

336
8 Linear and Nonlinear Evolution Equations
proceed independently, with the smaller solitary wave lagging farther and farther behind
the faster, taller wave. The only eﬀect of their encounter is an overall phase shift, so that
the taller wave is a bit behind where it would be if it had not encountered the shorter
wave, while the shorter wave is a little ahead of its unhindered position. Figure 8.14 plots
a typical such interaction.
Owing to this “particle-like” behavior under interaction, these solutions were given
a special name: soliton. An explicit formula for a two-soliton solution to the Korteweg–
de Vries equation can be written in the following form:
u(t, x) = 12 ∂2
∂x2 log Δ(t, x),
(8.118)
where
Δ(t, x) = det
⎛
⎜
⎝
1 + ε1(t, x)
2b1
b1 + b2
ε2(t, x)
2b2
b1 + b2
ε1(t, x)
1 + ε2(t, x)
⎞
⎟
⎠,
(8.119)
where 0 < b1 < b2, and
εj(t, x) = exp

bj(x −b2
j t) + dj

,
j = 1, 2.
(8.120)
The constants cj = b2
j represent the wave speeds, while the dj correspond to phase shifts of
the individual solitons. Proving that (8.118) is indeed a solution to the Korteweg–de Vries
equation is a straightforward, albeit tedious, exercise in diﬀerentiation. In Exercise 8.5.14,
the reader is asked to investigate its asymptotic behavior, as t →±∞, and prove that the
solution does, indeed, break up into two solitons, having the same proﬁles, speeds, and
amplitudes in both the distant past and future.
A similar dynamic occurs when there are multiple collisions among solitons. Faster
solitons catch up to slower ones moving to their right. After the various solitons ﬁnish
colliding and interacting, they emerge in order, from smallest to largest, each moving at
its characteristic speed and becoming more and more separated from its peers. An explicit
formula for the n–soliton solution is provided by the same logarithmic derivative (8.118) in
which Δ(t, x) now represents the determinant of an n × n matrix whose ith diagonal entry
is 1 + εi(t, x), while the oﬀ-diagonal (i, j) entry, i ̸= j, is
2bi
bi + bj
εj(t, x), using the same
formula (8.120) for the εj’s, and where 0 < b1 < · · · < bn correspond to the n diﬀerent
soliton wave speeds cj = b2
j. Furthermore, it can be shown that, starting with an arbitrary
localized initial disturbance u(0, x) = f(x) that decays suﬃciently rapidly as | x | →∞, the
resulting solution eventually emits a ﬁnite number of solitons of diﬀerent heights, moving
oﬀat their respective speeds to the right, and so arranged in order from smallest to largest,
followed by a small, asymptotically self-similar dispersive tail that gradually disappears.
The source of these highly non-obvious facts and formulas lies beyond the scope of
this introductory text. Soon after the initial numerical studies, Gardner, Green, Kruskal,
and Miura, [45], discovered a profound connection between the solutions to the Korteweg–
de Vries equation and the eigenvalues λ of the Sturm–Liouville boundary value problem
−d2ψ
dx2 + 6u(t, x) ψ = λ ψ,
−∞< x < ∞,
with
ψ(t, x) −→0
as
| x | −→∞.
(8.121)
Their remarkable result is that whenever u(t, x) is a localized solution to the Korteweg–
de Vries equation (8.112), the eigenvalues of (8.121) are constant, meaning that they do not

8.5 Dispersion and Solitons
337
vary with the time t, while the continuous spectrum has a very simple temporal evolution.
In physical applications of the stationary Schr¨odinger equation (8.121), in which u(t, x)
represents a quantum-mechanical potential, the eigenvalues correspond to bound states,
while the continuous spectrum governs its scattering behavior. The solution to the so-
called inverse scattering problem reconstructs the potential u(t, x) from its spectrum, and
can be viewed as a nonlinear version of the Fourier transform, in that it eﬀectively linearizes
the Korteweg–de Vries equation and thereby reveals its many remarkable properties. In
particular, the eigenvalues are responsible for the preceding determinantal formulae for the
multi-soliton solutions, while, when present, the continuous spectrum governs the dispersive
tail. See [2, 36] for additional details.
Exercises
8.5.10. Justify the statement that the width of a soliton is proportional to the inverse of the
square root of its speed.
8.5.11. Prove that the function (8.116) is a symmetric, monotone, exponentially decreasing
function on either side of its maximum height of 3c.
8.5.12. Let u(t, x) solve the Korteweg–de Vries equation.
(a) Show that U(t, x) = u(t, x −ct) + c is also a solution.
(b) Give a physical interpretation of this symmetry.
8.5.13.(a) Find all scaling symmetries of the Korteweg–de Vries equation.
(b) Write down an ansatz for the similarity solutions, and then ﬁnd the corresponding re-
duced ordinary diﬀerential equation. (Unfortunately, the similarity solutions cannot be
written in terms of elementary functions, [2].)
♥8.5.14.(a) Let u(t, x) be the two-soliton solution deﬁned in (8.118). Let u(t, ξ) = u(t, ξ + ct)
represent the solution as viewed in a coordinate frame moving with speed c. Prove that
lim
t →∞
u(t, ξ) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
3c1 sech2  1
2
√c1 ξ + δ1
 
,
c = c1,
3c2 sech2  1
2
√c2 ξ + δ2
 
,
c = c2,
0,
otherwise,
for suitable constants δ1, δ2. Explain why this justiﬁes the statement that the solution in-
deed breaks up into two individual solitons as t →∞. (b) Explain why u(t, ξ) has a similar
limiting behavior as t →−∞, but with possibly diﬀerent constants δ1, δ2. (c) Use your
formulas to discuss how the solitons are aﬀected by the collision.
8.5.15. Let α, β ̸= 0. Find the soliton solutions to the rescaled Korteweg–de Vries equation
ut + αuxxx + β uux = 0. How are their speed, amplitude, and width interrelated?
8.5.16.(a) Find the solitary wave solutions to the modiﬁed Korteweg–de Vries equation
ut + uxxx + u2ux = 0. (b) Discuss how the amplitude and width of the solitary waves are
related to their speeeds. Note: The modiﬁed Korteweg–de Vries equation is also integrable,
and its solitary wave solutions are solitons, cf. [36].
8.5.17. Answer Exercise 8.5.16 for the Benjamin–Bona–Mahony equation ut −uxxt + u ux = 0,
[14]. Note: The BBM equation is not integrable, and collisions between its solitary waves
produce a small, but measurable, inelastic eﬀect, [1].
♦8.5.18.(a) Show that T1 = u is the density for a conservation law for the Korteweg–de Vries
equation. (b) Show that T2 = u2 is also a conserved density. (c) Find a conserved density
of the form T3 = u2
x + μu3 for a suitable constant μ. Remark: The Korteweg–de Vries

338
8 Linear and Nonlinear Evolution Equations
equation in fact has inﬁnitely many conservation laws, whose densities depend on higher
and higher-order derivatives of the solution, [76, 87]. It was this discovery that unlocked
the door to all its remarkable integrability properties, [2, 36].
8.5.19. Find two conservation laws of
(a) the modiﬁed Korteweg–de Vries equation ut + uxxx + u2ux = 0;
(b) the Benjamin–Bona–Mahony equation ut −uxxt + u ux = 0.

Chapter 9
A General Framework for
Linear Partial Diﬀerential Equations
Before pressing on to the higher-dimensional manifestations of the heat, wave, and Laplace/
Poisson equations, it is worth pausing to develop a general, abstract, linear-algebraic frame-
work that underlies many of the linear partial diﬀerential equations arising throughout the
subject and its applications. The power of mathematical abstraction is that concentrating
on the essential features and not being distracted by the at times messy particular de-
tails enables one to establish, relatively painlessly, very general results that can be applied
throughout the subject and beyond.
Each abstract concept has, as its source, an ele-
mentary ﬁnite-dimensional version valid for linear algebraic systems and matrices, which
is then generalized and extended to include linear boundary value problems and then
initial-boundary value problems governed by diﬀerential equations.
All of the abstract
deﬁnitions and results contained here will be immediately applicable to the boundary and
initial value problems of physical interest, and serve to deepen our understanding of the
underlying commonalities among systems and solution techniques. Nevertheless, a more
applications-oriented reader may prefer to skip ahead to the more concrete developments
contained in the following chapters, referring to the background material presented here as
necessary.
Most equilibrium systems are modeled as boundary value problems involving a linear
diﬀerential operator that satisﬁes the two key conditions of being “self-adjoint” and either
“positive deﬁnite” or, slightly more generally, “positive semi-deﬁnite”. So, our ﬁrst task
is to introduce the adjoint of a linear function in general, and, for our speciﬁc purposes, a
linear diﬀerential operator. The adjoint is a far-reaching generalization of the elementary
matrix transpose. Its formulation relies on the speciﬁcation of inner products on both the
domain and target spaces of the operator, and, when one is dealing with linear diﬀerential
operators, the imposition of suitable homogeneous boundary conditions on the spaces of
allowable functions.
In applications, the relevant inner products are typically dictated
by the underlying physics.
One immediate application of the adjoint is the Fredholm
Alternative, which delineates the constraints required for the existence of solutions to
linear systems, including linear boundary value problems.
A linear operator that equals its own adjoint is called self-adjoint. The simplest exam-
ple is the linear function deﬁned by a symmetric matrix. The most important subclasses
are the positive deﬁnite and positive semi-deﬁnite operators, which are the natural ana-
logues of positive (semi-)deﬁnite matrices.
We will learn how to construct self-adjoint
positive (semi-)deﬁnite operators in a canonical manner. Almost all of the linear diﬀeren-
tial operators studied in this text, including the Laplacian, are, when subject to suitable
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
9
339
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

340
9 A General Framework for Linear Partial Diﬀerential Equations
boundary conditions, self-adjoint and either positive deﬁnite or positive semi-deﬁnite. The
key distinction is that positive deﬁnite linear systems and boundary value problems admit
unique solutions, whereas in the positive semi-deﬁnite case, the solution either does not
exist, since the Fredholm constraints are not satisﬁed, or, when it exists, is not unique. In
their dynamical manifestations, positive deﬁnite operators induce stable vibrational sys-
tems, whereas the positive semi-deﬁnite cases contain unstable modes that can lead to
disastrous physical consequences.
A critically important fact is that the solution to a positive deﬁnite linear system
can be characterized by a minimization principle, provided by a certain quadratic function
or, in the inﬁnite-dimensional function-space version, quadratic functional.
In physical
contexts, the function(al) often represents the potential energy of the system, and the
solution minimizes said energy among all possible conﬁgurations satisfying the prescribed
boundary conditions, thereby quantifying the maxim that Nature is inherently conservative
and seeks to minimize energy. In mathematics, minimization principles underlie advanced
functional-analytic methods used to establish existence theorems, as well as the ﬁnite
element numerical schemes to be presented in Chapter 10.
For linear dynamical systems like the heat and wave equations, separation of variables
leads to an eigenvalue problem for the linear diﬀerential operator governing the corre-
sponding equilibrium system. In the simple one-dimensional cases discussed in Chapter 4,
the eigenfunctions are trigonometric, producing the classical Fourier expansions for the
solutions. The eﬀectuality of the Fourier method relies on the eigenfunctions’ orthogonal-
ity, and we already hinted that this is no accident. Rather, it is a consequence of their
status as the eigenfunctions of a self-adjoint linear operator. Not only are such eigenfunc-
tions automatically mutually orthogonal with respect to the underlying inner product, the
eigenvalues are necessarily real and, when the operator is positive deﬁnite, also positive.
Orthogonality underlies the Fourier-like expansion of quite general functions as series
in the eigenfunctions, whose convergence, in general, requires that the eigenfunctions form
a complete system. For positive deﬁnite boundary value problems on bounded domains,
we will establish completeness by combining the eigenfunction expansion for the associ-
ated Green’s function with a basic minimization principle for the eigenvalues based on the
Rayleigh quotient. On the other hand, problems on unbounded domains do not typically
admit complete systems of eigenfunctions and require the more advanced analytical con-
cepts of continuous spectrum and generalized Fourier transforms that lie beyond the scope
of this text.
The chapter concludes by describing a general framework for dynamics that produces
time-dependent series solutions, in terms of the eigenfunctions of the underlying equilibrium
operator, for diﬀusion equations, vibration equations, and quantum-mechanical systems.
The ﬁnal two chapters will then specialize these general theories and constructions to
analyze initial-boundary value problems for the two- and three-dimensional heat, wave,
and Schr¨odinger equations in simple geometries. More advanced developments and further
applications can be found in higher-level texts, including [35, 38, 44, 61, 99].
9.1 Adjoints
Our starting point is a linear operator
L : U −→V
(9.1)

9.1 Adjoints
341
that maps a vector space U to another vector space V . For most of the development,
we deal with real vector spaces, although the ﬁnal discussion of the Schr¨odinger equation
requires us to venture into the complex realm. For our purposes, L represents a linear
diﬀerential operator, and the elements of the domain space U and the target space V
are suitable scalar- or vector-valued functions. In elastomechanics, the elements of U are
displacements of a deformable body, while the elements of V are the associated strains. In
electromagnetism and gravitation, elements of U represent potentials, and elements of V
are electric or magnetic or gravitational ﬁelds. In thermodynamics, U contains temperature
distributions, while V contains temperature gradients. In ﬂuid mechanics, U is the space
of potential functions, while V is the space of ﬂuid velocities. And so on.
The abstract deﬁnition of the adjoint of a linear operator relies on an inner product
structure on both its domain and target spaces. We distinguish the inner products on U
and V (which may be diﬀerent even when U and V happen to be the same vector space)
by using a single angle bracket
⟨u , u ⟩
to denote the inner product between u, u ∈U,
and a double angle bracket
⟨⟨v , v ⟩⟩
to denote the inner product between v, v ∈V.
In applications, the appropriate inner products are often based on the underlying physics.
Deﬁnition 9.1.
Let U, V be inner product spaces, and let L: U →V be a linear
operator. The adjoint of L is the unique linear operator L∗: V →U that satisﬁes
⟨⟨L[u] , v ⟩⟩= ⟨u , L∗[v] ⟩
for all
u ∈U,
v ∈V.
(9.2)
Observe that the adjoint goes in the reverse direction, that is, from V back to U. To
master the deﬁnition, let us ﬁrst look at the ﬁnite-dimensional case.
Example 9.2.
According to Theorem B.33, every linear function L: Rn →Rm is
given by matrix multiplication, so that L[u] = Au for u ∈Rn, where A is an m × n
matrix. The adjoint function L∗: Rm →Rn is also linear, so it is also represented by
matrix multiplication, L∗[v] = A∗v for v ∈Rm, by an n × m matrix A∗.
Suppose ﬁrst that we impose the ordinary Euclidean dot products
⟨u , u ⟩= u · u = uT u,
u, u ∈Rn,
⟨⟨v , v ⟩⟩= v · v = vT v,
v, v ∈Rm,
as our inner products on both Rn and Rm. Evaluation of both sides of the adjoint identity
(9.2) yields
⟨⟨L[u] , v ⟩⟩= ⟨⟨Au , v ⟩⟩= (Au)T v = uT AT v,
⟨u , L∗[v] ⟩= ⟨u , A∗v ⟩= uT A∗v.
(9.3)
Since these expressions must agree for all u, v, we conclude (see Exercise 9.1.6) that the
matrix A∗representing L∗is equal to the transposed matrix AT . Therefore, the adjoint
of a matrix with respect to the Euclidean dot product is its transpose: A∗= AT . So one
can regard the adjoint as a vast generalization of the elementary operation of transposing
a matrix.
More generally, suppose we take weighted inner products on the domain and target
spaces:
⟨u , u ⟩= uTM u,
u, u ∈Rn,
⟨⟨v , v ⟩⟩= vT C v,
v, v ∈Rm,
(9.4)

342
9 A General Framework for Linear Partial Diﬀerential Equations
where M and C are symmetric, positive deﬁnite matrices of respective sizes n × n and
m × m, cf. Proposition B.13. Then, repeating the previous calculation (9.3), we ﬁnd
⟨⟨L[u] , v ⟩⟩= ⟨⟨Au , v ⟩⟩= (Au)T C v = uTAT C v,
⟨u , L∗[v] ⟩= ⟨u , A∗v ⟩= uTMA∗v.
(9.5)
Comparing these expressions, we conclude that the weighted adjoint matrix is
A∗= M −1AT C.
(9.6)
Therefore, the adjoint does indeed depend on which inner products are being used on both
the domain and target spaces.
Diﬀerential Operators
For applications to linear diﬀerential equations, our attention is focused on adjoints of
diﬀerential operators deﬁned on inﬁnite-dimensional function spaces. Let us begin with
the simplest example.
Example 9.3.
Consider the derivative v = D[u] = du/dx, which deﬁnes a linear
operator D : U →V mapping a vector space U of diﬀerentiable functions u(x) to a vector
space containing their derivatives v(x) = u′(x). We assume that the functions in question
are deﬁned on a ﬁxed bounded interval a ≤x ≤b.
In order to compute its adjoint, we need to impose inner products on both the domain
space U and the target space V . The simplest context is to adopt the standard L2 inner
product on both:
⟨u , u ⟩=
 b
a
u(x) u(x) dx,
⟨⟨v , v ⟩⟩=
 b
a
v(x) v(x) dx.
(9.7)
According to the deﬁning equation (9.2), the adjoint operator D∗: V →U must satisfy the
inner product identity
⟨⟨D[u] , v ⟩⟩= ⟨u , D∗[v] ⟩
for all
u ∈U,
v ∈V.
(9.8)
First, we compute the left-hand side:
⟨⟨D[u] , v ⟩⟩=
44 du
dx , v
55
=
 b
a
du
dx v dx.
(9.9)
On the other hand, the right-hand side should equal
⟨u , D∗[v] ⟩=
 b
a
u D∗[v] dx.
(9.10)
Now, in the latter integral, we see u multiplying the result of applying the linear operator
D∗to v. To identify this integrand with that in (9.9), we need to somehow remove the
derivative from u. The secret is integration by parts, which allows us to rewrite the ﬁrst
integral in the form
 b
a
du
dx v dx =

u(b) v(b) −u(a) v(a)

−
 b
a
u dv
dx dx.
(9.11)

9.1 Adjoints
343
Ignoring the two boundary terms for a moment, we observe that the remaining integral
has the form of an inner product
−
 b
a
u dv
dx dx =
 b
a
u

−dv
dx

dx =
4
u , −dv
dx
5
= ⟨u , −D[v]⟩.
(9.12)
Equating (9.9) and (9.12), we deduce that
⟨⟨D[u] , v ⟩⟩=
44 du
dx , v
55
=
4
u , −dv
dx
5
= ⟨u , −D[v]⟩.
Thus, to satisfy the adjoint equation (9.8), we must have
⟨u , D∗[v] ⟩= ⟨u , −D[v]⟩
for all
u ∈U,
v ∈V,
and so the adjoint of the derivative operator is its negative:
D∗= −D.
(9.13)
However, the preceding argument is valid only if the boundary terms in the integration
by parts formula (9.11) vanish:
u(b) v(b) −u(a) v(a) = 0,
(9.14)
which necessitates imposing suitable boundary conditions on the functions u and v. For
example, imposing Dirichlet boundary conditions
u(a) = 0,
u(b) = 0,
(9.15)
will ensure that (9.14) holds, and therefore validates (9.13). In this case, the domain space
of D: U →V is the vector space
U = { u(x) | u(a) = u(b) = 0 } ,
while no boundary conditions need be imposed on the functions v(x) in the target space
V . An evident alternative is to require that v(a) = v(b) = 0. In this case, the target space
V = { v(x) | v(a) = v(b) = 0 }
consists of all functions that vanish at the endpoints. Since the derivative D: U →V is
required to map a function u(x) ∈U to an allowable function v(x) ∈V , the domain space
now consists of functions satisfying the Neumann boundary conditions:
U = { u(x) | u′(a) = u′(b) = 0 } .
These are evidently not the only two possibilities. Let us list the most important combina-
tions of boundary conditions that imply the vanishing of the boundary terms (9.14), and
so ensure the validity of the adjoint equation (9.13):
(a) Dirichlet boundary conditions:
u(a) = u(b) = 0.
(b) Mixed boundary conditions:
u(a) = u′(b) = 0,
or
u′(a) = u(b) = 0.
(c) Neumann boundary conditions:
u′(a) = u′(b) = 0.
(d) Periodic boundary conditions:
u(a) = u(b)
and
u′(a) = u′(b).
In all cases, the boundary conditions impose restrictions on the domain space U and, in
cases (b–d) when we are identifying v(x) = u′(x), the target space V also.

344
9 A General Framework for Linear Partial Diﬀerential Equations
Remark: In the preceding discussion, we were purposely vague about the required
diﬀerentiability of the functions. In ﬁnite dimensions, every linear function L: Rn →Rm
is given by matrix multiplication L[u] = Au, and hence is deﬁned on all of the underlying
vector space Rn. Linear operators on inﬁnite-dimensional function spaces are typically not
deﬁned on all possible functions. For example, the derivative operator L = D: U →V
requires the function u ∈U to be diﬀerentiable. However, the target function v = D[u] =
u′ is not necessarily as smooth, and so may belong to a diﬀerent function space; for instance
if u ∈C1[a, b], then v = u′ ∈C0[a, b]. On the other hand, the adjoint D∗= −D is deﬁned
only on diﬀerentiable functions v, so if v ∈C1[a, b], then u = −v′ ∈C0[a, b]. Keeping a
detailed account of the various smoothness requirements quickly becomes distracting.
To circumvent this technical annoyance, we will always deal with a ﬁxed class of func-
tions, e.g., continuous functions or, more generally, L2 functions, that are constrained only
by the imposed boundary conditions. When we write L: U →V , we allow the possibil-
ity that the linear operator L may be deﬁned only on a “dense” subspace of the domain
space U. For instance, we will write D: U →V with U = V = C0[a, b], even though
D[u] = u′ ∈V only if u belongs to the dense subspace C1[a, b] ⊂U = C0[a, b]. Similarly,
D∗: V →U is also deﬁned only on the dense subspace C1[a, b] ⊂V = C0[a, b]. The term
dense refers to the fact that any continuous function in the full space U = C0[a, b] can
be arbitrarily closely approximated in norm by a continuously diﬀerentiable function in
the subspace C1[a, b]. Or, to put it another way, given a continuous function u ∈C0[a, b],
there exists a sequence of continuously diﬀerentiable functions u1, u2, u3, . . . ∈C1[a, b] such
that ∥uk −u ∥→0 as k →∞. A similar density result can be proved for U = L2[a, b]; see
[37, 96, 98] for details.
Warning: In more advanced treatments, our notion of adjoint is usually called the
formal adjoint. A true adjoint requires more subtle technical hypotheses on the operator
and its domain, cf. [95].
Example 9.4.
Let us recompute the adjoint of the derivative operator D: U →V ,
this time with respect to the weighted L2 inner products
⟨u , u ⟩=
 b
a
u(x) u(x) ρ(x) dx,
⟨⟨v , v ⟩⟩=
 b
a
v(x) v(x) κ(x) dx,
(9.16)
where ρ(x) > 0 and κ(x) > 0 are strictly positive functions that, physically, might represent
the density and stiﬀness of a nonuniform bar. Now we need to compare
⟨⟨D[u] , v ⟩⟩=
 b
a
du
dx v(x) κ(x) dx,
with
⟨u , D∗[v] ⟩=
 b
a
u(x) D∗[v] ρ(x) dx.
Integrating the ﬁrst expression by parts, we obtain
 b
a
du
dx v κ dx =

u(b)v(b)κ(b) −u(a)v(a)κ(a)

−
 b
a
u d(κ v)
dx
dx
=
 b
a
u

−1
ρ
d(κ v)
dx

ρ dx,
(9.17)
provided that we select our boundary conditions so that
u(b)v(b)κ(b) −u(a)v(a)κ(a) = 0.
(9.18)

9.1 Adjoints
345
As you can check, this follows from any of the listed boundary conditions: Dirichlet,
Neumann, or mixed, as well as periodic, provided κ(a) = κ(b). We conclude that, in such
situations, the weighted adjoint of the derivative operator D is the diﬀerential operator
D∗[v(x)] = −
1
ρ(x)
d
dx

κ(x) v(x)

= −κ(x)
ρ(x)
dv
dx −κ′(x)
ρ(x) v(x).
(9.19)
As with matrices, the adjoint of a diﬀerential operator depends crucially on the speciﬁcation
of inner products.
The following basic results are left as exercises for the reader. The ﬁrst generalizes
the fact that transposing a transposed matrix reverts to the original.
Proposition 9.5. The adjoint of the adjoint is the original operator: L = (L∗)∗.
The second generalizes the fact that the transpose of the product of two matrices is
the product of the transposes, but in the reverse order.
Proposition 9.6. If L: U →V and M: V →W are linear operators on inner product
spaces, with L∗: V →U and M∗: W →V their respective adjoints, then the composite
linear operator M ◦L: U →W has adjoint (M ◦L)∗= L∗◦M∗: W →U.
Example 9.7.
Let us compute the adjoint of the second derivative operator D2 =
D ◦D with respect to the standard L2 inner products on both the domain and target spaces.
According to Proposition 9.6 and equation (9.13), at least on a formal level,
(D2)∗= D∗◦D∗= (−D) ◦(−D) = D2,
(9.20)
and hence D2 equals its own adjoint. However, the validity of (9.13) required that the
functions in the domain and target spaces of both D’s satisfy appropriate boundary con-
ditions. For example, the domain of the ﬁrst D: U →V could be U = { u(x) | u(a) =
u(b) = 0 }, while its target space V is unconstrained; the second D could then map V to
W = { w(x) | w(a) = w(b) = 0 }, which will thus also require that u′′(a) = u′′(b) = 0 in
order that D2 = D ◦D map U to W. Another option would be to impose Neumann condi-
tions on the ﬁrst D, with U = {u′(a) = u′(b) = 0} and thus V = {v(a) = v(b) = 0}, while
W remains unconstrained. Under either these or other suitably compatible constraints,
both adjoint identiﬁcations D∗= −D are valid, thus justifying (9.20). Keep in mind that,
according to our earlier remark, the diﬀerentiation operators are, in fact, deﬁned only on
the dense subspaces containing suﬃciently smooth functions.
Higher–Dimensional Operators
The most natural multi-dimensional analogue of the derivative is the gradient operator,
which, on a two-dimensional space, is given by
∇u = grad u =

∂u/∂x
∂u/∂y

.
The gradient ∇deﬁnes a linear operator that takes a scalar-valued function u(x, y) to
the vector-valued function consisting of its two ﬁrst-order partial derivatives. Thus, the
domain space U consists of scalar-valued functions u(x, y), or scalar ﬁelds, deﬁned for
(x, y) ∈Ω, where the domain Ω ⊂R2 is assumed to be both bounded and connected,

346
9 A General Framework for Linear Partial Diﬀerential Equations
and with a nice boundary ∂Ω. (Similar considerations apply to three- and even higher-
dimensional problems.) The target space V consists of vector-valued functions, or vector
ﬁelds, v(x, y) = ( v1(x, y), v2(x, y) )T deﬁned on Ω. As in the preceding subsection, the gra-
dient operator ∇: U →V is well deﬁned only on the dense subspace C1(Ω) ⊂U consisting
of continuously diﬀerentiable scalar ﬁelds.
In accordance with the general Deﬁnition 9.1, the adjoint of the gradient must go in
the reverse direction,
∇∗: V −→U,
mapping a vector ﬁeld v(x, y) to a scalar ﬁeld w(x, y) = ∇∗v. The deﬁning equation (9.2)
for the adjoint, namely
⟨⟨∇u , v ⟩⟩= ⟨u , ∇∗v ⟩,
(9.21)
relies on the choice of inner products on the two vector spaces. Let us start with the L2
inner product between scalar ﬁelds:
⟨u , u ⟩=
 
Ω
u(x, y) u(x, y) dx dy.
(9.22)
Similarly, the L2 inner product between vector ﬁelds deﬁned on Ω is obtained by integrating
their usual dot product:
⟨⟨v , v ⟩⟩=
 
Ω
v(x, y) · v(x, y) dx dy =
 
Ω

v1(x, y) v1(x, y) + v2(x, y) v2(x, y)

dx dy.
(9.23)
The adjoint identity (9.21) is supposed to hold for all appropriate scalar ﬁelds u and vector
ﬁelds v. For the L2 inner products (9.22, 23), the two sides of the identity read
⟨⟨∇u , v ⟩⟩=
 
Ω
∇u · v dx dy =
 
Ω
 ∂u
∂x v1 + ∂u
∂y v2

dx dy,
⟨u , ∇∗v ⟩=
 
Ω
u ∇∗v dx dy.
Thus, to compare these two double integrals, we must somehow remove the derivatives
from the scalar ﬁeld u. As in the one-dimensional computation (9.8), the mechanism is an
integration by parts formula for double integrals:
 
Ω
∇u · v dx dy =
&
∂Ω
u (v · n) ds −
 
Ω
u (∇· v) dx dy,
(9.24)
which was already noted in (6.83). The left-hand side is just ⟨⟨∇u , v ⟩⟩. If the boundary
line integral vanishes,
&
∂Ω
u (v · n) ds = 0,
(9.25)
then the right-hand side of formula (9.24) reduces to
−
 
Ω
u (∇· v) dx dy = −⟨u , ∇· v ⟩= ⟨u , −∇· v ⟩.
Therefore, subject to the boundary constraint (9.25), we deduce the L2 inner product
identity
⟨⟨∇u , v ⟩⟩= ⟨u , −∇· v ⟩,
(9.26)

9.1 Adjoints
347
which implies that the L2 adjoint of the gradient operator is minus the divergence operator:
∇∗v = −∇· v.
(9.27)
The vanishing of the boundary integral (9.25) will be ensured by the imposition of
suitable homogeneous boundary conditions on the scalar ﬁeld u and/or the vector ﬁeld v.
Clearly the line integral will vanish if either u = 0 or v · n = 0 at each point on the bound-
ary. These possibilities lead immediately to the three principal types of (homogeneous)
boundary conditions. The ﬁrst are the Dirichlet boundary conditions, which require
u = 0
on
∂Ω.
(9.28)
Alternatively, we can set
v · n = 0
on
∂Ω,
(9.29)
which requires that v be everywhere tangent to the boundary. Since ∇must map the
scalar ﬁeld u ∈U to an admissible vector ﬁeld v = ∇u ∈V , the boundary condition (9.29)
requires that u satisfy the homogeneous Neumann boundary conditions
∂u
∂n = ∇u · n = 0
on
∂Ω.
(9.30)
One can evidently also mix the boundary conditions, imposing Dirichlet conditions on part
of the boundary and Neumann conditions on the complementary part:
u = 0
on
D ⊂∂Ω,
v · n = ∂u
∂n = 0
on
N = ∂Ω \ D,
(9.31)
with neither D nor N empty.
More generally, when modeling deﬂections of nonuniform membranes, heat ﬂow through
heterogeneous media, and similar physical equilibria, we replace the L2 inner product be-
tween scalar and vector ﬁelds (9.23) by suitably weighted versions†
⟨u , u ⟩=
 
Ω
u(x, y) u(x, y) ρ(x, y) dx dy,
⟨⟨v , v ⟩⟩=
 
Ω

v1(x, y) v1(x, y) κ1(x, y) + v2(x, y) v2(x, y) κ2(x, y)

dx dy,
(9.32)
in which ρ(x, y), κ1(x, y), κ2(x, y) > 0 are strictly positive functions for (x, y) ∈Ω. In appli-
cations, ρ represents a density, while κ1, κ2 represent stiﬀnesses or thermal conductivities.
To compute the weighted adjoint of the gradient operator, we apply a similar integration
by parts argument based on (6.83):
⟨⟨∇u , v ⟩⟩=
 
Ω

κ1v1
∂u
∂x + κ2v2
∂u
∂y

dx dy
(9.33)
=
&
∂Ω
u

−κ2v2 dx + κ1v1 dy

−
 
Ω
u
 ∂(κ1v1)
∂x
+ ∂(κ2v2)
∂y

dx dy
=
 
Ω
u

−1
ρ
 ∂(κ1v1)
∂x
+ ∂(κ2v2)
∂y
 
ρ dx dy,
†
Exercise 9.2.14 treats an even more general pair of inner products.

348
9 A General Framework for Linear Partial Diﬀerential Equations
provided the boundary integral vanishes. Equating the left-hand side to
⟨u , ∇∗v ⟩=
 
Ω
u (∇∗v) ρ dx dy,
we deduce that the adjoint of the gradient operator with respect to the weighted inner
products (9.32) is minus the “weighted divergence operator”:
∇∗v = −1
ρ
 ∂(κ1v1)
∂x
+ ∂(κ2v2)
∂y

= −κ1
ρ
∂v1
∂x −κ2
ρ
∂v2
∂y −1
ρ
∂κ1
∂x v1 −1
ρ
∂κ2
∂y v2. (9.34)
The vanishing of the boundary integral,
0 =
&
∂Ω
u

−κ2v2 dx + κ1v1 dy

=
&
∂Ω
u v · n ds,
where
v =

κ1v1
κ2v2

,
is ensured if either u = 0 or v·n = 0 on ∂Ω. The former is the usual homogeneous Dirichlet
condition, but the latter is a “weighted” version of the homogeneous Neumann boundary
condition, requiring that ∇u · n = 0 on the boundary, where ∇u =

κ1ux, κ2uy
T repre-
sents a “weighted normal ﬂux vector”.
Example 9.8.
Let us compute the adjoint of the second-order Laplacian operator
Δ = ∂2/∂x2 +∂2/∂y2 with respect to the L2 inner products on both its domain and target
spaces. The computation is a simple consequence of the double integral identity (6.88),
which we rewrite as
⟨Δu , v ⟩=
 
Ω
v Δu dx dy =
&
∂Ω

u ∂v
∂n −v ∂u
∂n

ds +
 
Ω
u Δv dx dy = ⟨u , Δv ⟩.
Thus, provided the boundary integral vanishes, we can conclude that the Laplacian equals
its own adjoint: Δ∗= Δ. This is assured when u ∂v/∂n = v ∂u/∂n at each point in ∂Ω.
For example, the adjoint computation is valid if either u = v = 0 or ∂u/∂n = ∂v/∂n = 0
at every point of the boundary of the domain. Keep in mind that if we require v = 0 on
some or all of ∂Ω, then this imposes the condition Δu = 0 there in order that Δ map u to
an admissible v; similar considerations apply when ∂v/∂n = 0.
Exercises
9.1.1. Choose one from the following list of inner products on R2. Then ﬁnd the adjoint of A =

1
2
−1
3

when your inner product is used on both its domain and target space. (a) The
Euclidean dot product; (b) the weighted inner product ⟨v , w ⟩= 2v1 w1 + 3v2 w2; (c) the
inner product ⟨v , w ⟩= vT C w deﬁned by the symmetric positive deﬁnite matrix C =

2
−1
−1
4

.
9.1.2. From the list in Exercise 9.1.1, choose a diﬀerent inner product on the domain and the
target space, and then determine the adjoint of the matrix A.

9.1 Adjoints
349
9.1.3. Choose one from the following list of inner products on R3 for both the domain and tar-
get space, and ﬁnd the adjoint of A =
⎛
⎜
⎝
1
1
0
−1
0
1
0
−1
2
⎞
⎟
⎠. (a) The Euclidean dot product on
R3; (b) the weighted inner product ⟨v , w ⟩= v1 w1 + 2v2 w2 + 3v3 w3; (c) the inner prod-
uct ⟨v , w ⟩= vT C w deﬁned by the symmetric positive deﬁnite matrix C =
⎛
⎜
⎝
2
1
0
1
2
1
0
1
2
⎞
⎟
⎠.
9.1.4. From the list in Exercise 9.1.3, choose diﬀerent inner products on the domain and target
space, and then compute the adjoint of the matrix A.
9.1.5. Choose an inner product on R2 from the list in Exercise 9.1.1 and an inner product on
R3 from the list in Exercise 9.1.3, and then compute the adjoint of A =
⎛
⎜
⎝
1
3
0
2
−1
1
⎞
⎟
⎠.
♦9.1.6.(a) Let C be an m × n matrix. Suppose uT C v = 0 for all u ∈Rm and v ∈Rn. Prove
that C = O must be the zero matrix. (b) Let A, B be m × n matrices such that uT Av =
uT B v for all u ∈Rm and v ∈Rn. Prove that A = B. (c) Find an n × n matrix C ̸= O
such that uT C u = 0 for all u ∈Rn.
9.1.7. Let U = C0[0, 1]. Find the adjoint I ∗of the identity operator I : U →U under the
weighted inner products (9.16).
9.1.8. Compute the adjoint of the derivative operator v = D[u] = u′ under the weighted inner
products ⟨u , u ⟩=
	 1
0 ex u(x) u(x) dx, ⟨⟨v , v ⟩⟩=
	 1
0 (1 + x) v(x) v(x) dx. Clearly state any
boundary conditions that you are imposing.
9.1.9. Let L[u] = xu′(x) + u(x) and 0 < a < x < b. When subject to homogeneous Dirichlet
boundary conditions u(a) = u(b) = 0, determine the adjoint L∗[v ] with respect to
(a) the L2 inner products (9.7); (b) the weighted inner products (9.16).
9.1.10. Consider the linear operator L[u] =

u′
u

that maps u(x) ∈C1 to the vector-valued
function whose components consist of the function and its ﬁrst derivative. Imposing the
boundary conditions u(0) = u(1), compute the adjoint L∗with respect to the L2 inner
products on both the domain and target spaces.
9.1.11. True or false: The adjoint of the divergence operator ∇· v with respect to the L2 inner
products (9.22, 23) is minus the gradient operator: (∇· )∗u = −∇u. If true, what boundary
conditions do you need to assume? If false, what is the adjoint?
9.1.12. Find the adjoint of the two-dimensional curl operator ∇× v, as deﬁned in (6.73), with
respect to the L2 inner products (9.22, 23). Carefully state any required boundary condi-
tions.
♦9.1.13. Prove that (a) the adjoint of a linear operator is also a linear operator;
(b) the adjoint is unique.
♦9.1.14. Let L, M: U →V be linear operators on the same inner product spaces. Prove that
(a) (L + M)∗= L∗+ M∗,
(b) (cL)∗= cL∗for c ∈R.
♦9.1.15. Prove Proposition 9.5.
♦9.1.16. Prove Proposition 9.6.
9.1.17. True or false: If L: U →U is invertible, then (L−1)∗= (L∗)−1.

350
9 A General Framework for Linear Partial Diﬀerential Equations
The Fredholm Alternative
Given a linear operator L: U →V between inner product spaces U, V , a fundamental
problem is to solve the associated inhomogeneous linear system
L[u] = f
(9.35)
for various forcing functions f ∈V . In ﬁnite dimensions, this reduces to a linear algebraic
system, Au = f, deﬁned by a coeﬃcient matrix A. For the linear ordinary and partial
diﬀerential operators of interest to us, (9.35) represents a linear boundary value problem.
In general, an inhomogeneous linear system will not be solvable unless its right-hand side
satisﬁes certain constraints, ensuring that f belongs to the range of L. These conditions
can be readily characterized using the adjoint operator via the so-called Fredholm Alter-
native, named after the early-twentieth-century Swedish mathematician Ivar Fredholm.
Fredholm’s primary interest was in solving linear integral equations, but his solvability cri-
terion was then recognized to be a completely general property of linear systems, including
linear algebraic systems, linear diﬀerential equations, linear boundary value problems, and
so on.
Recall that the kernel of a linear operator L is the set of solutions to the homogeneous
linear system L[u] = 0.
Deﬁnition 9.9. The cokernel of a linear operator L: U →V between inner product
spaces is deﬁned as the kernel of its adjoint:
cokerL = ker L∗=
-
v ∈V
 L∗[v] = 0
.
.
(9.36)
We can now state and prove the Fredholm Alternative.
Theorem 9.10.
If the linear system L[u] = f has a solution, then the right-hand
side must be orthogonal to the cokernel of L, i.e.,
⟨⟨v , f ⟩⟩= 0
for all
v ∈cokerL.
(9.37)
Proof : If L[u] = f, then, given v ∈cokerL, the adjoint equation (9.2) implies
⟨⟨v , f ⟩⟩= ⟨⟨v , L[u] ⟩⟩= ⟨L∗[v] , u ⟩= 0,
since L∗[v] = 0 by the deﬁnition of the cokernel.
Q.E.D.
Remark: In practice, one needs to check the orthogonality constraints (9.37) only
when v runs through a basis of the cokernel. In particular, if the only solution to the
homogeneous adjoint system L∗[v] = 0 is the trivial solution v = 0, then there are no
constraints, and we expect that the inhomogeneous linear system (9.35) can be solved
for any “reasonable” forcing function f. In ﬁnite dimensions, this is certainly the case,
[89]. For boundary value problems deﬁned by linear diﬀerential operators, one needs to
determine what “reasonable” means, and then prove an appropriate existence theorem.
Although valid for all of the boundary value problems presented here, when subject to
continuous or even piecewise continuous forcing functions f, rigorous proofs of the existence
of solutions for partial diﬀerential equations involve the advanced mathematical machinery
of functional analysis — see, e.g., [38, 44, 61, 99] — and lie beyond the scope of this
introductory text.

9.1 Adjoints
351
Example 9.11. Consider the linear algebraic system
u1 −u3 = f1,
u2 −2u3 = f2,
u1 −2u2 + 3u3 = f3.
(9.38)
Using Gaussian Elimination (or by inspection), one easily sees that (9.38) admits a solution
if and only if the compatibility condition
−f1 + 2f2 + f3 = 0
(9.39)
holds. Moreover, when this occurs, a solution exists but is not unique. To connect this to
the Fredholm Alternative, we write the system in matrix form L[u] = f, where L[u] = Au
represents multiplication by the coeﬃcient matrix
A =
⎛
⎝
1
0
−1
0
1
−2
1
−2
3
⎞
⎠.
Using the dot product on R3, the adjoint linear function L∗[v] = AT v is represented by
the transposed matrix
AT =
⎛
⎝
1
0
1
0
1
−2
−1
−2
3
⎞
⎠.
Therefore, the cokernel is found by solving the homogeneous adjoint linear system AT y = 0,
i.e.,
v1 + v3 = 0,
v2 −2v3 = b2,
−v1 −2v2 + 3v3 = 0,
whose solutions consist of all scalar multiples of v = ( −1, 2, 1 )T . We recognize the com-
patibility condition (9.39) as requiring that the right-hand side be orthogonal (under the
dot product) to the cokernel basis vector,
v · f = −f1 + 2f2 + f3 = 0,
in accordance with the Fredholm Alternative constraint (9.37).
Example 9.12. Let us solve the boundary value problem
u′′ = f(x),
u′(0) = 0,
u′(ℓ) = 0,
(9.40)
modeling the displacement, under an external force, of a uniform elastic bar of length ℓ
both of whose ends are free. Solving the diﬀerential equation by direct integration, we ﬁnd
that
u(x) = ax + b +
 x
0
 y
0
f(z) dz

dy,
where the constants a, b are to be determined by the boundary conditions. Since
u′(x) = a +
 x
0
f(z) dz,
the boundary condition u′(0) = 0 implies that a = 0. The second boundary condition
requires
u′(ℓ) =
 ℓ
0
f(x) dx = 0.
(9.41)

352
9 A General Framework for Linear Partial Diﬀerential Equations
If this fails, then the boundary value problem has no solution.
On the other hand, if
the forcing function f(x) satisﬁes the constraint (9.41), then the resulting solution of the
boundary value problem has the form
u(x) = b +
 x
0
 y
0
f(z) dz

dy,
(9.42)
where the constant b is arbitrary. Thus, when it exists, the solution to the boundary value
problem is not unique. The constant b solves the corresponding homogeneous problem,
and represents a rigid translation of the entire bar by a distance b.
The solvability constraint (9.41) follows from the Fredholm Alternative. Indeed, ac-
cording to Example 9.7, under the L2 inner products and the given boundary conditions,
(D2)∗= D2, and hence the adjoint system is the unforced homogeneous boundary value
problem
v′′ = 0,
v′(0) = 0,
v′(ℓ) = 0,
with solution v(x) = c for any constant c. Thus, the cokernel consists of all scalar multiples
of the constant function v⋆(x) ≡1. The Fredholm Alternative requires that the forcing
function in the original boundary value problem be orthogonal to the cokernel functions,
and so
⟨1 , f ⟩=
 ℓ
0
f(x) dx = 0,
which is precisely the condition (9.41) required for existence of a (nonunique) equilibrium
solution.
Example 9.13.
Consider the homogeneous Neumann boundary value problem for
the Poisson equation on a bounded domain Ω ⊂R2, namely,
−Δu = f
in
Ω,
∂u
∂n = 0
on
∂Ω.
(9.43)
According to Example 9.8, the Laplacian is self-adjoint under the L2 inner product and
the prescribed boundary conditions: Δ∗= Δ. Thus, the homogeneous adjoint system is
merely
−Δv = 0
in
Ω,
∂v
∂n = 0
on
∂Ω.
Theorem 6.15 tells us that the only solutions to the adjoint problem are the constant
functions, v(x, y) ≡c. Thus, a basis for the cokernel consists of the function v(x, y) ≡1,
and so the Fredholm Alternative requires that the forcing function in (9.43) satisfy
⟨1 , f ⟩=
 
Ω
f(x, y) dx dy = 0,
(9.44)
reproducing our earlier constraint (6.90) for the homogeneous Neumann case.
Exercises
9.1.18. Use the Fredholm Alternative to determine whether the following linear systems are
compatible. When compatible, write down the general solution.

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions
353
(a)
2x −4y = −2,
−x + 2y = 3,
(b) 6x −3y + 9z = 6,
2x −y + 3z = 2,
(c)
2x + 3y = −1,
3x + 7y = 1,
x + 4y = 2,
−x + y = 3,
(d)
2x1 −3x2 −x3 = −1,
3x1 −x2 = 1,
4x1 + x2 + x3 = 2,
(e)
2x1 + 3x2 −x4 = −1,
3x1 + 2x3 −x4 = 0,
x1 −x2 + x3 = 1.
9.1.19. Use the Fredholm Alternative to ﬁnd the compatibility conditions for the following sys-
tems of linear equations.
(a) 2x + y = a, x + 4y = b, −3x + 2y = c;
(b) x + 2y + 3z = a, −x + 5y −2z = b, 2x −3y + 5z = c;
(c) x1 + 2x2 + 3x3 = b1, x2 + 2x3 = b2, 3x1 + 5x2 + 7x3 = b3, −2x1 + x2 + 4x3 = b4;
(d) x−3y+2z+w = a, 4x−2y+2z+3w = b, 5x−5y+4z+4w = c, 2x+4y−2z+w = d.
9.1.20. Suppose A is a symmetric matrix. Show that the linear system A x = b has a solution
if and only if b is orthogonal to ker A.
9.1.21. Use the Fredholm Alternative to determine whether there exists a solution to the fol-
lowing boundary value problem: xu′′ + u′ = 1 −2
3 x, u′(0) = u′(1) = 0. If so, write down
all solutions.
9.1.22. Analyze the periodic boundary value problem −u′′ = f(x), u(0) = u(2π), u′(0) = u′(2π),
along the same lines as in Example 9.12. Characterize the forcing functions for which the
problem has a solution. Explain why the constraints, if any, are in accordance with the
Fredholm Alternative. Write down a forcing function f(x) that satisﬁes all your constraints,
and then ﬁnd all corresponding solutions.
9.1.23. Answer Exercise 9.1.22 for the boundary value problems:
(a) u′′′′ = f(x),
u′′(0) = u′′′(0) = 0,
u′′(1) = u′′′(1) = 0;
(b) u′′′′ = f(x),
u′′(0) = u′′′(0) = 0,
u(1) = u′′(1) = 0.
♥9.1.24. Let λ be a real parameter. (a) For which values of λ does the boundary value problem
u′′ + λ u = h(x), u(0) = 0, u(1) = 0, have a unique solution? (b) Construct the Green’s
function for all such λ. (c) In the nonunique cases, use the Fredholm Alternative to ﬁnd
conditions on the forcing function h(x) that are required for the existence of a solution.
9.1.25. Let Ω ⊂R2 be a bounded, connected domain. Using the L2 inner products (9.22, 23)
on scalar and vector ﬁelds, write out the Fredholm Alternative constraints for the solvabil-
ity of the boundary value problem ∇· v = f in Ω, subject to the homogeneous boundary
conditions v · n = 0 on ∂Ω.
9.1.26. Let Ω ⊂R2 be a bounded simply connected domain. Using the L2 inner products (9.22,
23) on scalar and vector ﬁelds on a domain Ω ⊂R2, write out the Fredholm Alternative
constraints for the solvability of the boundary value problem ∇u = f in Ω, subject to the
homogeneous boundary conditions u = 0 on ∂Ω.
9.2 Self–Adjoint and Positive Deﬁnite Linear Functions
In ﬁnite-dimensional linear algebra, there are two particularly important classes of matri-
ces: symmetric, equal their own transpose, and positive deﬁnite, as prescribed by Deﬁni-
tion B.12. The goal of this section is to adapt both concepts to more general linear opera-
tors, paying particular attention to the case of linear diﬀerential operators. The resulting
classes of self-adjoint and positive (semi-)deﬁnite diﬀerential operators are ubiquitous in
applications of ordinary and partial diﬀerential equations.

354
9 A General Framework for Linear Partial Diﬀerential Equations
Self–Adjointness
Throughout this section, U will be a ﬁxed inner product space. We have already seen that
the transpose of a matrix is a very special case of the adjoint operation. Thus, the natural
analogue of a symmetric matrix is a linear operator that equals its own adjoint.
Deﬁnition 9.14. A linear operator S: U →U is called self-adjoint if S∗= S.
Thus, according to (9.2), S is self-adjoint if and only if
⟨S[u] , u ⟩= ⟨u , S[u ] ⟩
for all
u, u ∈U.
(9.45)
Example 9.15.
In the ﬁnite-dimensional case, a linear function S: Rn →Rn is
realized by matrix multiplication: S[u] = K u, where K is a square matrix of size n × n.
If we use the ordinary dot product on Rn, then, according to Example 9.2, the adjoint
function S∗: Rn →Rn is given by multiplication by the transposed matrix: S∗[u] = KT u.
Thus, a linear function is self-adjoint with respect to the dot product if and only if it is
represented by a symmetric matrix: KT = K.
On the other hand, if we adopt the weighted inner product ⟨u , u ⟩= uT C u provided
by the symmetric positive deﬁnite matrix C > 0, then, according to (9.6), the adjoint
function S∗has matrix representative C−1KT C, and hence S is self-adjoint under the
weighted inner product if and only if the matrix K satisﬁes K = C−1KT C.
Example 9.16. In Example 9.7, we argued that the second-order derivative operator
S = D2 is self-adjoint with respect to the L2 inner product, when subject to suitable homo-
geneous boundary conditions. A direct veriﬁcation of this result is instructive. According
to the general adjoint equation (9.2), we need to equate
 b
a
S[u] u dx = ⟨S[u] , u ⟩= ⟨u , S∗[u] ⟩=
 b
a
u S∗[u] dx.
(9.46)
As before, the computation relies on (in this case two) integration by parts:
⟨S[u] , u ⟩=
 b
a
d2u
dx2 u dx = du
dx u

b
x=a
−
 b
a
du
dx
du
dx dx
=
 du
dx u −u du
dx
 
b
x=a
+
 b
a
u d2u
dx2 dx.
Comparing with (9.46), we conclude that S∗= D2 = S, provided the boundary terms
vanish:
 du
dx u −u du
dx
 
b
x=a
=

u′(b) u(b) −u(b) u′(b)

−

u′(a) u(a) −u(a) u′(a)

= 0.
(9.47)
This requires that we impose suitable boundary conditions at the endpoints, which will
serve to characterize the underlying vector space U on which S = D2 acts. One possibility
is to set U = {u(a) = u(b) = 0}, thereby imposing homogeneous Dirichlet boundary condi-
tions. Since u ∈U also, u(a) = u(b) = 0, and hence (9.47) holds, proving self-adjointness.
Alternatively, one can impose homogeneous Neumann, mixed, or periodic boundary con-
ditions to specify the space U and similarly establish self-adjointness of S = D2.

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions
355
Positive Deﬁniteness
Let us turn to the characterization of positive deﬁnite, and, slightly less stringently, positive
semi-deﬁnite linear operators. These serve to extend the notions of positive deﬁnite and
semi-deﬁnite matrices to linear diﬀerential operators deﬁning boundary value problems.
Deﬁnition 9.17.
A linear operator S: U →U on an inner product space is called
positive deﬁnite, written S > 0, if
⟨u , S[u] ⟩> 0
for all
u ̸= 0.
(9.48)
The operator S is positive semi-deﬁnite, written S ≥0, if
⟨u , S[u] ⟩≥0
for all
u.
(9.49)
Observe that, on the ﬁnite-dimensional space U = Rn equipped with the dot product,
the linear function S[u] = K u is positive (semi-)deﬁnite if and only if K is a positive
(semi-)deﬁnite matrix, as per Deﬁnition B.12. (However, changing the inner product on Rn
will result in an alternative notion of positive deﬁniteness for the matrix K; see Exercise
9.2.5.) In the inﬁnite-dimensional situations involving diﬀerential operators, the domain
of the operator may be only a dense subspace of the full inner product space U, and
one imposes the positivity condition (9.48) or (9.49) only on those functions u lying in
the domain of S. Fortunately, this technicality has no serious eﬀect on the subsequent
development.
Example 9.18. Consider the operator S = −D2 acting on the space U consisting of
all C2 functions deﬁned on a bounded interval [a, b] and subject to homogeneous Dirichlet
boundary conditions u(a) = u(b) = 0. To establish positive deﬁniteness, we evaluate
⟨S[u] , u ⟩=
 b
a

−d2u
dx2 u

dx = −du
dx u

b
x=a
+
 b
a
du
dx
2
dx =
 b
a
du
dx
2
dx,
where we integrated by parts and then used the boundary conditions to eliminate the
boundary terms. The ﬁnal expression is clearly ≥0, and hence S is at least positive semi-
deﬁnite. Moreover, since u′(x) is continuous, the only way the ﬁnal integral could vanish is if
u′(x) ≡0, which means u(x) ≡c is constant. However, the only constant function satisfying
the homogeneous Dirichlet boundary conditions is u(x) ≡0. Thus, ⟨S[u] , u ⟩> 0 for all
0 ̸= u ∈U, which implies S > 0. A similar argument implies positive deﬁniteness when the
functions are subject to the mixed boundary conditions u(a) = u′(b) = 0. On the other
hand, any constant function satisﬁes the Neumann boundary conditions u′(a) = u′(b) = 0,
and hence in this case S ≥0 is only positive semi-deﬁnite.
Proposition 9.19. If S > 0, then ker S = {0}. As a consequence, a positive deﬁnite
linear system S[u] = f with f in the range of S, so f ∈rng S, must have a unique solution.
Proof : If S[u] = 0, then ⟨u , S[u] ⟩= 0, which, according to (9.48), is possible only if
u = 0. The second statement follows directly from Theorem 1.6.
Q.E.D.
Thus, in the ﬁnite-dimensional case, positive deﬁniteness implies that the coeﬃcient
matrix of S[u] = K u is nonsingular, and hence existence of a solution is automatic. In
the inﬁnite-dimensional cases of boundary value problems, existence of solutions usually
requires some further analysis, [63].

356
9 A General Framework for Linear Partial Diﬀerential Equations
The most common means of producing self-adjoint, positive (semi-)deﬁnite linear op-
erators is provided by the following general construction. From here on, in order to dis-
tinguish the possibly diﬀerent norms resulting from the inner products on the domain and
target spaces of a linear operator L: U →V , we employ, respectively, the following double
and triple bar notation:
∥u ∥= ⟨u , u ⟩,
u ∈U,
|∥v ∥| = ⟨⟨v , v ⟩⟩,
v ∈V .
(9.50)
Theorem 9.20. Let L: U →V be a linear map between inner product spaces with
adjoint L∗: V →U. Then the composite map
S = L∗◦L : U −→U
is always self-adjoint, S = S∗, and positive semi-deﬁnite, S ≥0, with ker S = ker L.
Moreover, S > 0 is positive deﬁnite if and only if ker L = {0}.
Proof : First, by Propositions 9.5 and 9.6,
S∗= (L∗◦L)∗= L∗◦(L∗)∗= L∗◦L = S,
proving self-adjointness. Furthermore,
⟨u , S[u] ⟩= ⟨u , L∗[L[u]] ⟩= ⟨⟨L[u] , L[u] ⟩⟩= |∥L[u] ∥|2 ≥0
(9.51)
for all u, proving positive semi-deﬁniteness. Moreover, the result is > 0 as long as L[u] ̸= 0.
Thus, if ker L = { u | L[u] = 0 } = {0}, then ⟨u , S[u]⟩> 0 for all u ̸= 0, and hence S
is positive deﬁnite. Finally, the same computation proves that ker S = ker L. Indeed, if
L[u] = 0, then S[u] = L∗[L[u]] = L∗[0] = 0. On the other hand, if S[u] = 0, then
0 = ⟨u , S[u]⟩= |∥L[u] ∥|2, and hence L[u] = 0.
Q.E.D.
We are particularly interested in linear systems that are based on the construction of
Theorem 9.20, namely
S[u] = L∗[L[u]] = f.
(9.52)
We will refer to the system (9.52) as positive deﬁnite or positive semi-deﬁnite according
to the status of its deﬁning operator S. Thus, the system is positive deﬁnite if and only
if ker S = ker L = {0}, i.e., the only solution to the homogeneous system S[z ] = 0 is the
trivial solution z = 0. In this case, the solution to (9.52) (provided it exists) is unique. On
the other hand, if there are nonzero solutions to S[z ] = 0, then (9.52) is only positive semi-
deﬁnite, and does not admit a unique solution. Moreover, unless the Fredholm Alternative
constraints (9.37) hold, then there are no solutions. By Theorem 9.20, we can identify
coker S = ker S∗= ker S = ker L,
(9.53)
which thus implies the following:
Theorem 9.21. Let S = L∗◦L. If the linear system S[u] = f has a solution, then
⟨z , f ⟩= 0 for all z ∈ker L. Moreover, if S[u] = f and S[u ] = f are two solutions to the
same linear system, then u = u + z, where z ∈ker L is any solution to L[z ] = 0.
Example 9.22.
In the ﬁnite-dimensional case, any linear function L: Rn →Rm is
represented by matrix multiplication: L[u] = Au. For the dot product on both the domain
and target spaces, L∗[v] = AT v, and hence the self-adjoint combination S = L∗◦L: Rn →
Rn is represented by the n × n symmetric matrix K = ATA. According to Theorem 9.20,

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions
357
the matrix K is always positive semi-deﬁnite, and is positive deﬁnite if and only if the
only solution to the homogeneous linear system Az = 0 is the trivial solution z = 0. In
the positive semi-deﬁnite case, the Fredholm Alternative of Theorem 9.21 states that the
linear system K u = f has a solution if and only if z · f = 0 for all z ∈ker A. (As noted
before, existence of solutions in the ﬁnite-dimensional case is not an issue.) Moreover, if u
is any solution, so is u = u + z for any z ∈ker A.
More generally, if we adopt the weighted inner products (9.4) on the domain and
target spaces represented by the respective positive deﬁnite matrices M > 0 and C > 0,
then the adjoint map L∗has matrix representative M −1AT C, and hence S = L∗◦L is
given by multiplication by the (not necessarily symmetric) n × n matrix K = M −1AT C A.
Again, K ≥0 in all cases, and K > 0 if and only if ker A = {0}. Now, the Fredholm
Alternative states that the linear system K u = M −1AT C Au = f has a solution if and
only if ⟨z , f ⟩= zTM f = 0 for all z ∈ker A.
See [89, 112] for applications of this
construction in mechanics, electrical networks, and the stability of structures.
Example 9.23.
Consider next the diﬀerentiation operator D[u] = u′. According
to Example 9.3, if we impose suitable homogeneous boundary conditions on the space of
allowable functions — Dirichlet, Neumann, mixed, or periodic — and use the L2 inner
products on both domain and target space, then D∗[v] = −v′. Therefore, the self-adjoint
operator of Theorem 9.20 is given by S = D∗◦D = −D2.
According to Theorem 9.20, the resulting boundary value problem
S[u] = −u′′ = f
is always positive semi-deﬁnite, and is positive deﬁnite if and only if ker D = {0}, i.e.,
the only function that satisﬁes D[u] = u′ = 0 along with the boundary conditions is the
zero function.
Consider ﬁrst the Dirichlet boundary conditions u(a) = u(b) = 0.
On
a connected interval, u′ = 0 if and only if u = c is a constant function. However, the
boundary conditions require that c = 0, and hence only the zero function appears in the
kernel. We conclude that the Dirichlet boundary value problem is positive deﬁnite, and
its solution unique. A similar argument applies to the mixed boundary conditions, e.g.,
u(a) = u′(b) = 0, since the condition at x = a is enough to ensure that the constant function
must be zero. On the other hand, any constant function satisﬁes the Neumann boundary
conditions u′(a) = u′(b) = 0, and hence in this case, ker D consists of all constant functions.
Therefore, the Neumann boundary value problem is only positive semi-deﬁnite. And, as
we saw, the solution, when it exists, is not unique, since we can add any constant function
to a solution and obtain another solution. A similar argument proves that the periodic
boundary value problem, with u(a) = u(b), u′(a) = u′(b), is also positive semi-deﬁnite,
with the same kinds of existence and uniqueness properties.
More generally, if we use weighted inner products (9.16) on the domain and target
spaces, then, again subject to suitable boundary conditions, the adjoint is given by (9.19),
and so the self-adjoint boundary value problem S[u] = D∗◦D[u] = f is based on the more
general diﬀerential equation
S[u] = −
1
ρ(x)
 d
dx κ(x) du
dx

= f(x).
(9.54)
Such boundary value problems model the deformations of a nonuniform elastic bar with
density ρ(x) and stiﬀness κ(x), when subject to the external forcing function f(x). Again,
the positive deﬁniteness of the problem depends on whether ker D = {0}, and so the exact

358
9 A General Framework for Linear Partial Diﬀerential Equations
same classiﬁcation holds as in the unweighted case: the Dirichlet and mixed boundary
value problems are positive deﬁnite and have a unique solution, whereas the Neumann and
periodic boundary value problems are only positive semi-deﬁnite, and the existence of a
solution requires the Fredholm conditions to be satisﬁed.
Self-adjointness underlies the symmetry of the associated Green’s function. As a func-
tion of x, the Green’s function Gξ(x) = G(x; ξ) satisﬁes the boundary value problem with
delta function forcing concentrated at position x = ξ:
S[Gξ ] = δξ,
or, explicitly,
−
1
ρ(x)
∂
∂x

κ(x) ∂G
∂x

= δ(x −ξ),
(9.55)
along with the required homogeneous boundary conditions. Suppose ﬁrst that we are using
the L2 inner product on the interval [a, b], so that ρ(x) ≡1. Using the deﬁnition of the
delta function δξ(x) = δ(x −ξ) and the self-adjointness of S, we have, for any a < x, ξ < b,
G(x; ξ) = Gξ(x) =
 b
a
Gξ(y) δx(y) dy = ⟨Gξ , δx ⟩= ⟨Gξ , S[Gx ] ⟩
= ⟨S[Gξ ] , Gx ⟩= ⟨δξ , Gx ⟩=
 b
a
δξ(y) Gx(y) dy = Gx(ξ) = G(ξ; x).
(9.56)
This establishes† the symmetry equation
G(x; ξ) = G(ξ; x)
(9.57)
for the Green’s function of a self-adjoint boundary value problem under the L2 inner prod-
uct. This can be regarded as the diﬀerential operator version of the fact that the inverse
of a symmetric matrix is also symmetric.
On the other hand, if we adopt a weighted inner product
⟨u , u ⟩=
 b
a
u(y) u(y) ρ(y) dy,
then the preceding argument must be slightly modiﬁed:
ρ(x) G(x; ξ) = ρ(x) Gξ(x) =
 b
a
ρ(y) Gξ(y) δx(y) dy = ⟨Gξ , δx ⟩= ⟨Gξ , S[Gx ] ⟩
= ⟨S[Gξ ] , Gx ⟩= ⟨δξ , Gx ⟩=
 b
a
δξ(y) Gx(y) ρ(y) dy = ρ(ξ) Gx(ξ) = ρ(ξ) G(ξ; x),
and so the Green’s function associated with a weighted self-adjoint boundary value problem
satisﬁes a “weighted symmetry condition”
ρ(x) G(x; ξ) = ρ(ξ) G(ξ; x).
(9.58)
Remark: Equation (9.58) implies that the modiﬁed Green’s function
G(x; ξ) = G(x; ξ)
ρ(ξ)
is genuinely symmetric:
G(x; ξ) = G(ξ; x).
(9.59)
†
Symmetry at the endpoints is a consequence of continuity.

9.2 Self–Adjoint and Positive Deﬁnite Linear Functions
359
The modiﬁed Green’s function also has the advantage of recasting the superposition formula
for the solution to the boundary value problem S[u] = f as the appropriate weighted inner
product:
u(x) =
 b
a
G(x; ξ) f(ξ) dξ =
 b
a
G(x; ξ) f(ξ) ρ(ξ) dξ = ⟨Gx , f ⟩,
where
Gx(ξ) = G(x; ξ).
Two–Dimensional Boundary Value Problems
Let us next apply the self-adjoint formalism to study boundary value problems on a
bounded, connected, two-dimensional domain Ω ⊂R2. We take L = ∇to be the gra-
dient operator, mapping a scalar ﬁeld u to a vector ﬁeld v = ∇u. We impose a suitable
set of homogeneous boundary conditions, i.e., Dirichlet, Neumann, or mixed. According to
the calculation in Section 9.1, if we adopt the basic L2 inner products (9.22, 23) between
scalar and vector ﬁelds, then the adjoint of the gradient is the negative of the divergence:
∇∗v = −∇· v. Therefore, the self-adjoint combination of Theorem 9.20 yields
∇∗◦∇[u] = −∇· (∇u) = −Δu,
where Δ is the Laplacian operator. In this manner, we are able to write the two-dimensional
Poisson equation in self-adjoint form
−Δu = −∇· (∇u) = ∇∗◦∇u = f,
(9.60)
as always subject to the selected boundary conditions.
According to Theorem 9.20, −Δ = ∇∗◦∇is positive deﬁnite if and only if the kernel
of the gradient operator — restricted to the appropriate space of scalar ﬁelds — is trivial:
ker ∇= {0}. Since we are assuming that the domain Ω is connected, Lemma 6.16 tells us
that the only functions that could show up in ker ∇, and thus prevent positive deﬁnite-
ness, are the constants. The boundary conditions will tell us whether this occurs. The
only constant function that satisﬁes either homogeneous Dirichlet or homogeneous mixed
boundary conditions is the zero function, and thus, just as in the one-dimensional case,
the boundary value problem for the Poisson equation subject to Dirichlet or mixed bound-
ary conditions is positive deﬁnite. In particular, this means that its solution is uniquely
deﬁned. On the other hand, any constant function satisﬁes the homogeneous Neumann
boundary condition ∂u/∂n = 0, and hence such boundary value problems are only positive
semi-deﬁnite. Existence of a solution relies on the Fredholm Alternative, as we discussed
in Example 9.13; moreover, when it exists, the solution is no longer unique, because one
can add in any constant without aﬀecting either the equation or the boundary conditions.
More generally, if we impose weighted inner products (9.32) on our spaces of scalar and
vector ﬁelds, then, recalling (9.34), the corresponding self-adjoint boundary value problem
takes the more general form
∇∗◦∇u = −
1
ρ(x, y)
∂
∂x

κ1(x, y) ∂u
∂x

−
1
ρ(x, y)
∂
∂y

κ2(x, y) ∂u
∂y

= f(x, y),
(9.61)
along with the chosen boundary conditions on ∂Ω. Again, the Dirichlet and mixed bound-
ary value problems are positive deﬁnite, with unique solutions, while the (suitably weighted)
Neumann problem is only positive semi-deﬁnite.

360
9 A General Framework for Linear Partial Diﬀerential Equations
The partial diﬀerential equation (9.61) arises in various physical contexts. For exam-
ple, consider a steady-state ﬂuid ﬂow moving in a domain Ω ⊂R2 described by a vector
ﬁeld v. The ﬂow is called irrotational if it has zero curl, ∇× v = 0, and hence, assuming
that Ω is simply connected, is a gradient v = ∇u, where u(x, y) is known as the ﬂuid
velocity potential. The constitutive assumptions connect the ﬂuid velocity with its rate of
ﬂow w = κ v, where κ(x, y) > 0 is the scalar density of the ﬂuid. Conservation of mass
provides the ﬁnal equation, namely ∇· w + f = 0, where f(x, y) represents ﬂuid sources
(f > 0) or sinks (f < 0). Therefore, the basic equilibrium equations take the form
−∇· (κ ∇u) = f,
or
−∂
∂x

κ(x, y) ∂u
∂x

−∂
∂y

κ(x, y) ∂u
∂y

= f(x, y),
(9.62)
which is (9.61) with ρ →1 and κ1, κ2 →κ. The case of a homogeneous (constant density)
ﬂuid thus reduces to the Poisson equation (4.84), with f replaced by f/κ.
Symmetry of the Green’s function for the Poisson equation and the more general
boundary value problems (9.61, 62) follows by an evident adaptation of the one-dimensional
argument presented above. Details are left as Exercise 9.2.17.
Exercises
9.2.1. Which of the following matrices deﬁne self-adjoint linear functions S: R2 →R2 relative
to the dot product?
(a)

1
0
0
1

, (b)

0
3
2
2

, (c)

1
0
2
−5

, (d)

3
2
2
1

.
9.2.2. Answer Exercise 9.2.1 for the inner products
(i) ⟨u , u ⟩= 2u1 u1 + 3u2 u2;
(ii) ⟨u , u ⟩= uT C u, where C =

2
−1
−1
3

.
9.2.3. True or false: Given an inner product ⟨u , v ⟩on Rn:
(a) The inverse of a nonsingular self-adjoint n × n matrix is self-adjoint.
(b) The inverse of a nonsingular positive deﬁnite n × n matrix is positive deﬁnite.
9.2.4. Prove that K > 0 is a positive deﬁnite n × n matrix if and only if J = KT + K is a
symmetric positive deﬁnite matrix.
♦9.2.5.(a) Prove that the n × n matrix K deﬁnes a self-adjoint linear function on Rn with re-
spect to the inner product ⟨u , u ⟩= uT C u for C a symmetric positive deﬁnite matrix if
and only if the matrix J = C K is symmetric, and hence deﬁnes a self-adjoint linear func-
tion with respect to the dot product. (b) Prove that K > 0 under the given inner product
if and only if J > 0 under the dot product.
9.2.6. Let D[u] = u′ be the derivative operator acting on the vector space of C2 scalar func-
tions u(x) deﬁned for 0 ≤x ≤1 and satisfying the boundary conditions u(0) = 0, u(1) = 0.
(a) Given the weighted inner product ⟨u , u ⟩=
	 1
0 u(x) u(x) ex dx on both its domain and
target spaces, determine the corresponding adjoint operator D∗.
(b) Let S = D∗◦D. Write down and solve the boundary value problem S[u] = 2ex.
9.2.7. Let c(x) ∈C0[a, b] be a continuous function. Prove that the linear multiplication op-
erator S[u] = c(x) u(x) is self-adjoint with respect to the L2 inner product. What sort of
boundary conditions need to be imposed?
9.2.8. True or false: The Neumann boundary value problem −u′′ + u = x, u′(0) = u′(π) = 0,
admits a unique solution.

361
9.2.9. Prove that the complex diﬀerential operator L[u] = i du
dx is self-adjoint with respect to
the L2 Hermitian inner product ⟨u , v ⟩=
	 π
−π u(x) v(x) dx on the space of continuously
diﬀerentiable complex-valued 2π–periodic functions: u(x + 2π) = u(x).
9.2.10. Let L = D2. Using the L2 inner products on both the domain and target spaces,
write down a set of homogeneous boundary conditions that makes L∗= D2. Then set
S = L∗◦L = D4. Do your boundary conditions lead to a boundary value problem
S[u] = f that is (i) positive deﬁnite; (ii) positive semi-deﬁnite; or (iii) neither?
9.2.11. Let β be a real constant. True or false: The second derivative operator S[u] = u′′ is
self-adjoint with respect to the L2 inner product on the space of functions
U =
#
u(x) ∈C2[0, 1]
 u(0) = 0, u′(1) + β u(1) = 0
$
subject to Dirichlet boundary conditions at the left-hand endpoint and Robin boundary
conditions at the right-hand endpoint.
♥9.2.12. Let β be a real constant. Consider the diﬀerential operator S[u] = −u′′ acting on the
space of functions
U =
#
u(x) ∈C2[0, 1]
 u(0) = 0, u′(1) + β u(1) = 0
$
subject to Dirichlet boundary conditions at the left-hand endpoint and Robin boundary
conditions at the right-hand endpoint. Prove that S > 0 is positive deﬁnite with respect to
the L2 inner product if and only if β > −1. Hint: Use the analysis following (4.48).
♥9.2.13. The equilibrium equations for a toroidal membrane (an inner tube) lead to the Poisson
equation −uxx −uyy = f(x, y) on a rectangle 0 < x < a, 0 < y < b, subject to periodic
boundary conditions
u(x, 0) = u(x, b),
uy(x, 0) = uy(x, b),
u(0, y) = u(a, y),
ux(0, y) = ux(a, y).
(a) Prove that the toroidal boundary value problem is self-adjoint. (b) Is it positive deﬁ-
nite, positive semi-deﬁnite, or neither? (c) Are there any conditions that must be imposed
on the forcing function f(x, y) in order that a solution exist?
♦9.2.14. Find the adjoint of the gradient operator ∇with respect to the L2 inner product
(9.22) between scalar ﬁelds, and the following weighted inner product between (column)
vector ﬁelds v = ( v1(x, y), v2(x, y) )T ,
v = ( v1(x, y), v2(x, y) )T :
⟨⟨v , v ⟩⟩=
		
Ω v(x, y)T C(x, y) v(x, y) dx dy,
where the 2 × 2 matrix C(x, y) =

α(x, y)
β(x, y)
β(x, y)
γ(x, y)

> 0 is symmetric, positive deﬁnite at
all points (x, y) ∈Ω. What sort of boundary conditions do you need to impose? Write out
the corresponding boundary value problem for the equilibrium equation ∇∗◦∇u = f.
9.2.15. Let Ω ⊂R2 be a bounded domain. Construct a set of homogeneous boundary condi-
tions on ∂Ω that make the biharmonic equation Δ2u = f:
(a) self-adjoint, (b) positive
deﬁnite, (c) positive semi-deﬁnite, but not positive deﬁnite.
9.2.16. Write down the boundary value problem Sξ[ Gξ ] = δξ satisﬁed by the modiﬁed Green’s
function Gξ(x) = G(x; ξ) given in (9.59). Is the underlying linear operator Sξ, which may
depend on ξ, self-adjoint with respect to a suitable inner product?
♦9.2.17. Prove symmetry of the Green’s function, G(ξ; x) = G(x; ξ), for the Poisson equation
on a bounded domain Ω ⊂R2 subject to homogeneous Dirichlet boundary conditions.
Hint: Look at how we established (9.56).
9.2.18. Generalize Exercise 9.2.17 to the partial diﬀerential equation (9.61).
9.2 Self–Adjoint and Positive Deﬁnite Linear Functions

362
9 A General Framework for Linear Partial Diﬀerential Equations
9.3 Minimization Principles
One of the most important features of positive deﬁnite linear problems is that their solution
can be characterized by a quadratic minimization priniciple. In many physical contexts,
equilibrium conﬁguration(s) serve to minimize the potential energy of the system. Think
of a small ball rolling around in a bowl. After frictional eﬀects have stopped its motion,
the ball will be left sitting in equilibrium at the bottom of the bowl — the position that
minimizes the gravitational potential energy.
Minimization priniciples are employed in
functional analytic proofs of existence of solutions, as well as providing a foundation for
the powerful ﬁnite element numerical method to be studied in Chapter 10.
The basic theorem on quadratic minimization principles is as follows.
Theorem 9.24. Let S: U →U be a self-adjoint and positive deﬁnite linear operator
on an inner product space U. Suppose that the linear system
S[u] = f
(9.63)
admits a (necessarily unique) solution u⋆. Then u⋆minimizes the value of the associated
quadratic function(al)
Q[u] = 1
2 ⟨u , S[u]⟩−⟨f , u ⟩,
(9.64)
meaning that Q[u⋆] < Q[u] for all admissible u ̸= u⋆in U.
Proof : We are given that S[u⋆] = f, and so, for any u ∈U,
Q[u] = 1
2 ⟨u , S[u]⟩−⟨u , S[u⋆] ⟩= 1
2 ⟨u −u⋆, S[u −u⋆] ⟩−1
2 ⟨u⋆, S[u⋆] ⟩,
(9.65)
where we used linearity, along with our assumption that S is self-adjoint, to identify the
terms ⟨u , S[u⋆] ⟩= ⟨u⋆, S[u] ⟩. Since S > 0, the ﬁrst term on the right-hand side of (9.65)
is always ≥0; moreover it equals 0 if and only if u = u⋆. On the other hand, the second
term does not depend on u at all. Thus, to minimize Q[u], we must make the ﬁrst term
as small as possible, which is accomplished by setting u = u⋆.
Q.E.D.
Example 9.25. Consider the the problem of minimizing a quadratic function
Q(u1, . . . , un) = 1
2
n

i,j =1
kij ui uj −
n

i=1
fi ui + c,
(9.66)
depending on n variables u = ( u1, u2, . . . , un )T ∈Rn, with ﬁxed real coeﬃcients kij, fi,
and c. Since uiuj = ujui, we can assume, without loss of generality, that the coeﬃcients
of the quadratic terms are symmetric: kij = kji. We rewrite (9.66) in matrix notation as
Q(u) = 1
2 u · K u −f · u + c,
(9.67)
which, apart from the inessential constant term, agrees with (9.64) once we set S[u] = K u
and use the dot product ⟨u , u⟩= u · u as the inner product on Rn. Thus, according to
Theorem 9.24, if K is a symmetric positive deﬁnite matrix, then the quadratic function
(9.67) has a unique minimizer u⋆= (u⋆
1, . . . , u⋆
n)T , which is the solution to the linear system
K u⋆= f.
If the positive deﬁnite linear operator in Theorem 9.24 comes from the self-adjoint
construction of Theorem 9.20, so S = L∗◦L, then, by (9.51), the quadratic term can be
re-expressed as ⟨u , S[u]⟩= |∥L[u] ∥|2, using our notational convention (9.50) for the norm
on the target space V of L. We can thus rephrase the minimization principle as follows:

9.3 Minimization Principles
363
Theorem 9.26. Suppose L: U →V is a linear operator between inner product spaces
with adjoint L∗: V →U. Assume that ker L = {0}, and let S = L∗◦L: U →U be the
associated positive deﬁnite linear operator. If f ∈rng S, then the quadratic function
Q[u] = 1
2 |∥L[u] ∥|2 −⟨f , u ⟩
(9.68)
has a unique minimizer u⋆, which is the solution to the linear system S[u] = f.
Warning: In (9.68), the ﬁrst term |∥L[u] ∥|2 is computed using the norm based on the
inner product on V , while the second term ⟨f , u ⟩employs the inner product on U.
One of the most important applications of minimization is the method of least squares,
which is extensively applied in data analysis and approximation theory.
We refer the
interested reader to [89] for developments in this direction. Here we will concentrate on
applications to diﬀerential equations.
Example 9.27. Consider the boundary value problem
−u′′ = f(x),
u(a) = 0,
u(b) = 0.
(9.69)
The underlying diﬀerential operator S = D∗◦D = −D2, when acting on the space of
functions satisfying the homogeneous Dirichlet boundary conditions, is self-adjoint and, in
fact, positive deﬁnite, since ker D = {0}. Explicitly, positive deﬁniteness requires
⟨S[u] , u ⟩=
 b
a

−u′′(x)u(x)

dx =
 b
a
u′(x)2 dx > 0
(9.70)
for all nonzero u(x) ̸≡0 with u(a) = u(b) = 0. Notice how we used an integration by parts,
invoking the boundary conditions to eliminate the boundary contributions, to expose the
positivity of the integral. The associated quadratic functional is, using (9.68),
Q[u] = 1
2|∥u′ ∥|2 −⟨f , u ⟩=
 b
a
 1
2 u′(x)2 −f(x)u(x)

dx.
Its minimum value, taken over all C2 functions that satisfy the homogeneous Dirichlet
boundary conditions, occurs precisely when u = u⋆is the solution to the boundary value
problem.
Sturm–Liouville Boundary Value Problems
The most important class of boundary value problems governed by second-order ordinary
diﬀerential equations was ﬁrst systematically investigated by the nineteenth-century French
mathematicians Jacques Sturm and Joseph Liouville. A Sturm–Liouville boundary value
problem is based on a second-order ordinary diﬀerential equation of the form
S[u] = −d
dx

p(x) du
dx

+ q(x)u = −p(x) d2u
dx2 −p′(x) du
dx + q(x)u = f(x),
(9.71)
on a bounded interval a ≤x ≤b, supplemented by Dirichlet, Neumann, mixed, or periodic
boundary conditions. To avoid singular points of the diﬀerential equation (although we
will later discover that most cases of interest have one or more singular points), we assume
here that p(x) > 0 and, to ensure positive deﬁniteness, q(x) > 0 for all a ≤x ≤b.

364
9 A General Framework for Linear Partial Diﬀerential Equations
Sturm–Liouville equations and boundary value problems appear in a remarkably broad
range of applications, and particularly in the analysis of partial diﬀerential equations by
the method of separation of variables. Moreover, most of the important special functions,
including Airy functions, Bessel functions, Legendre functions, hypergeometric functions,
and so on, naturally appear as solutions to particular Sturm–Liouville equations, [85, 86].
In the ﬁnal two chapters, the analysis of basic linear partial diﬀerential equations in curvilin-
ear coordinates, in both two and three dimensions, will require us to solve several particular
examples, including the Bessel, Legendre, and Laguerre equations. For now, though, we
concentrate on understanding how Sturm–Liouville boundary value problems ﬁt into our
self-adjoint and positive deﬁnite framework.
Our starting point is the linear operator
L[u] =

u′
u

(9.72)
that maps a scalar function u(x) ∈U to a vector-valued function v(x) = ( v1(x), v2(x) )T ∈
V , whose components are v1 = u′, v2 = u. To compute the adjoint of L: U →V , we use
the standard L2 inner product (9.7) on U, but adopt the following weighted inner product
on V :
⟨⟨v , v ⟩⟩=
 b
a

p(x)v1(x)v1(x) + q(x)v2(x)v2(x)

dx,
v =

v1
v2

,
v =

v1
v2

. (9.73)
The positivity assumptions on the weight functions p, q ensure that the latter is a bona
ﬁde inner product. As usual, the adjoint computation relies on integration by parts. Here,
we only need to manipulate the ﬁrst summand:
⟨⟨L[u] , v ⟩⟩=
 b
a
(p u′v1 + q uv2) dx
= p(b)u(b)v1(b) −p(a)u(a)v1(a) +
 b
a
u

−(pv1)′ + q v2

dx.
The boundary terms will disappear, provided that, at each endpoint, either u or v1 vanishes.
Since for the linear operator v = L[u] given by (9.72), we can identify v1 = u′, we conclude
that any of our usual boundary conditions — Dirichlet, mixed, or Neumann — remain valid
here. Under any of these conditions,
⟨⟨L[u] , v ⟩⟩=
 b
a
u [ −(pv1)′ + q v2 ] dx = ⟨u , L∗[v] ⟩,
and so the adjoint operator is given by
L∗[v] = −d(pv1)
dx
+ q v2 = −pv′
1 −p′ v1 + q v2.
The canonical self-adjoint combination
S[u] = L∗◦L[u] = L∗

u′
u

= −d
dx

p du
dx

+ q u
(9.74)
then reproduces the Sturm–Liouville diﬀerential operator (9.71). Moreover, since ker L =
{0} is trivial (why?), the boundary value problem is positive deﬁnite for all boundary
conditions, not only Dirichlet and mixed, but also Neumann!
A proof of the following general existence theorem can be found in [63].

9.3 Minimization Principles
365
Theorem 9.28. Let p(x) > 0 and q(x) > 0 for a ≤x ≤b. Then, for any choice of
boundary conditions (including Neumann), the Sturm–Liouville boundary value problem
(9.71) admits a unique solution.
Theorem 9.26 tells us that the solution to the Sturm–Liouville boundary value problem
(9.71) can be characterized as the unique minimizer of the quadratic functional
Q[u] = 1
2|∥L[u]∥|2 −⟨f , u ⟩=
 b
a
 1
2 p(x)u′(x)2 + 1
2 q(x)u(x)2 −f(x)u(x)

dx
(9.75)
among all C2 functions satisfying the prescribed homogeneous boundary conditions.
Example 9.29. Let ω > 0. Consider the constant-coeﬃcient Sturm–Liouville prob-
lem
−u′′ + ω2 u = f(x),
u(0) = u(1) = 0,
which we studied earlier in Example 6.10. Theorem 9.28 guarantees the existence of a
unique solution. The solution achieves the minimum possible value for the quadratic func-
tional
Q[u] =
 1
0
 1
2 u′2 + 1
2 ω2 u2 −f u

dx
among all C2 functions satisfying the given boundary conditions.
More generally, suppose we adopt a weighted inner product
⟨u , u ⟩=
 b
a
u(x) u(x) ρ(x) dx
(9.76)
on the domain space U, where ρ(x) > 0 on [a, b]. The same integration by parts compu-
tation proves that, when subject to the homogeneous boundary conditions,
L∗[v] = 1
ρ

−d(pv1)
dx
+ q v2

= −p
ρ v′
1 −p′
ρ v1 + q
ρ v2,
and so the weighted Sturm–Liouville diﬀerential operator is
S[u] = L∗◦L[u] = 1
ρ

−d
dx

p du
dx

+ q u

.
(9.77)
The corresponding weighted Sturm–Liouville equation S[u] = f has the form
S[u] =
1
ρ(x)

−d
dx

p(x) du
dx

+ q(x)u

= −p(x)
ρ(x)
d2u
dx2 −p′(x)
ρ(x)
du
dx + q(x)
ρ(x) u = f(x),
(9.78)
which is, in fact, identical to the ordinary Sturm–Liouville equation (9.71) after we replace
f by ρ f. Be that as it may, the weighted generalization will become important when we
study the associated eigenvalue problems.
Example 9.30.
Let m > 0 be a ﬁxed positive number. Consider the diﬀerential
equation
B[u] = −u′′ −1
x u′ + m2
x2 u = f(x),
(9.79)

366
9 A General Framework for Linear Partial Diﬀerential Equations
where B is known as the Bessel diﬀerential operator of order m. To place it in weighted
Sturm–Liouville form (9.78), we must ﬁnd p(x), q(x), and ρ(x) that satisfy
p(x)
ρ(x) = 1,
p′(x)
ρ(x) = 1
x ,
q(x)
ρ(x) = m2
x2 .
Dividing the second equation by the ﬁrst, we see that p′(x)/p(x) = 1/x, and hence we can
set
p(x) = x,
q(x) = m2
x ,
ρ(x) = x.
Thus, when subject to homogeneous Dirichlet, mixed, or even Neumann boundary condi-
tions on an interval 0 < a ≤x ≤b, the Bessel operator B is positive deﬁnite and self-adjoint
with respect to the weighted inner product
⟨u , u ⟩=
 b
a
u(x) u(x) x dx.
(9.80)
Exercises
9.3.1. Consider the boundary value problem −u′′ = x, u(0) = u(1) = 0. (a) Find the solution.
(b) Write down a minimization principle that characterizes the solution.
(c) What is the
value of the minimized quadratic functional on the solution? (d) Write down at least two
other functions that satisfy the boundary conditions and check that they produce larger
values for the energy.
9.3.2. Answer Exercise 9.3.1 for the boundary value problems
(a)
d
dx

1
1 + x2
du
dx

= x2, u(−1) = u(1) = 0; (b) −(ex u′)′ = e−x, u(0) = u′(1) = 0;
(c) x2 u′′ + 2xu′ = 3x2, u′(1) = u(2) = 0; (d) xu′′ + 3u′ = 1, u(−2) = u(−1) = 0.
9.3.3. Let Q[u] =
	 1
0
 1
2 (u′)2 −5u
 
dx. (a) Find the function u⋆(x) that minimizes Q[u]
among all C2 functions that satisfy u(0) = u(1) = 0.
(b) Test your answer by computing Q[u⋆] and then comparing with the value of Q[u] when
u(x) = (i) x −x2, (ii)
3
2 x −3
2 x3, (iii)
2
3 sin πx, (iv) x2 −x4.
9.3.4. For each of the following functionals and associated boundary conditions: (i) write down
a boundary value problem satisﬁed by the minimizing function, and (ii) ﬁnd the minimiz-
ing function u⋆(x):
(a)
	 1
0
 1
2 (u′)2 −3u
 
dx,
u(0) = u(1) = 0,
(b)
	 1
0
 1
2 (x + 1)(u′)2 −5u
 
dx,
u(0) = u(1) = 0,
(c)
	 3
1

x(u′)2 + 2u
 
dx,
u(1) = u(3) = 0,
(d)
	 1
0
 1
2 ex (u′)2 −(1 + ex)u
 
dx,
u(0) = u(1) = 0,
(e)
	 1
−1
(x2 + 1) (u′)2 + xu
(x2 + 1)2
dx,
u(−1) = u(1) = 0.

9.3 Minimization Principles
367
9.3.5. Which of the following quadratic functionals possess a unique minimizer among all C2
functions satisfying the indicated boundary conditions? Find the minimizer if it exists.
(a)
	 2
1
 1
2 x(u′)2 + 2(x −1)u
 
dx,
u(1) = u(2) = 0;
(b)
	 π
−π
 1
2 x(u′)2 −u cos x
 
dx,
u(−π) = u(π) = 0;
(c)
	 1
−1

(u′)2 cos x −u sin x
 
dx,
u(−1) = u′(1) = 0;
(d)
	 2
−2

(1 −x2) (u′)2 −u
 
dx,
u(−2) = u(2) = 0;
(e)
	 1
0

(x + 1)(u′)2 −u
 
dx,
u′(0) = u′(1) = 0.
9.3.6. Let D[u] = u′ be the derivative operator acting on the vector space of C2 scalar func-
tions u(x) deﬁned for 0 ≤x ≤1 and satisfying the boundary conditions u(0) = 0, u′(1) = 0.
(a) Given the weighted inner product ⟨u , u ⟩=
	 1
0 u(x) u(x) ex dx on both its domain and
target spaces, determine the corresponding adjoint operator D∗.
(b) Let S = D∗◦D. Write down and solve the boundary value problem S[u] = 3ex.
(c) Write down a minimization principle that characterizes the solution you found in part
(b), or explain why none exists.
9.3.7. Solve the Sturm–Liouville boundary value problem −4u′′ + 9u = 1, u(0) = 0, u(2) = 0.
Is your solution unique?
9.3.8. Answer Exercise 9.3.7 for the Neumann boundary conditions u′(0) = 0, u′(2) = 0.
9.3.9. (i) Write the following diﬀerential equations in Sturm–Liouville form.
(ii) If possible,
write down a minimization principle that characterizes the solutions to the Dirichlet bound-
ary value problem on the interval [1, 2]. (a) −ex u′′ −ex u′ = e2x, (b) −xu′′ −u′ + 2u = 1,
(c) −u′′ −2u′ + u = ex, (d) −x2 u′′ + 2xu′ + 3u = 1, (e) xu′′ + (1 −x)u′ + u = 0.
9.3.10. True or false: The Sturm–Liouville operator (9.71) is self-adjoint and positive deﬁnite
when subject to periodic boundary conditions u(a) = u(b), u′(a) = u′(b).
9.3.11. Does the quadratic functional Q[u] =
	 1
0
 1
2 (u′)2 −

x −1
2

u
 
dx have a minimum
value when u(x) is subject to the homogeneous Neumann boundary value conditions u′(0) =
u′(1) = 0? If so, determine the minimum value and ﬁnd all minimizing functions.
♥9.3.12.(a) Determine the adjoint of the diﬀerential operator L[u] = u′ + 2xu with respect
to the L2 inner products on [0, 1] when subject to the ﬁxed boundary conditions u(0) =
u(1) = 0. (b) Is the self-adjoint operator S = L∗◦L positive deﬁnite? Explain your answer.
(c) Write out the boundary value problem represented by S[u] = f. (d) Find the solution
to the boundary value problem when f(x) = ex2. Hint: To integrate the diﬀerential equa-
tion, work with the factored form of the diﬀerential operator. (e) Discuss what happens if
you instead impose the Neumann boundary conditions u′(0) = u′(1) = 0.
9.3.13. Discuss the self-adjointness and positive deﬁniteness of boundary value problems associ-
ated with the Bessel operator (9.79) of order m = 0.
9.3.14. Let u⋆(x) be the solution to the self-adjoint positive deﬁnite boundary value problem
S[u⋆] = f. Prove that if f(x) ̸≡0, then the minimum of the associated quadratic functional
is strictly negative: Q[u⋆] < 0.
9.3.15. Find a function u(x) such that
	 1
0 u′′(x) u(x) dx > 0. How do you reconcile this with
the claimed positivity in (9.70)?
9.3.16. Does the inequality (9.70) hold when u(x) ̸≡0 is subject to the Neumann boundary
conditions u′(a) = u′(b) = 0?

368
9 A General Framework for Linear Partial Diﬀerential Equations
9.3.17. True or false: When subject to homogeneous Dirichlet boundary conditions on an in-
terval [a, b], every nonsingular second-order linear ordinary diﬀerential equation
a(x)u′′ + b(x)u′ + c(x)u = f(x) is (a) self-adjoint, (b) positive deﬁnite, (c) positive semi-
deﬁnite, with respect to some weighted inner product (9.76).
The Dirichlet Principle
Let us now apply these ideas to boundary value problems governed by the Poisson equation
−Δu = ∇∗◦∇u = f.
(9.81)
In the positive deﬁnite cases in which the partial diﬀerential equation is supplemented
by either homogeneous Dirichlet or homogeneous mixed boundary conditions, our general
Minimization Theorem 9.24 implies that the solution can be characterized by the justly
famous Dirichlet Principle.
Theorem 9.31. The function u(x, y) that minimizes the Dirichlet integral
Q[u] = 1
2 |∥∇u ∥|2 −⟨f , u ⟩=
 
Ω
 1
2 u2
x + 1
2 u2
y −f u

dx dy
(9.82)
among all C2 functions that satisfy the prescribed homogeneous Dirichlet or mixed bound-
ary conditions is the solution to the corresponding boundary value problem for the Poisson
equation −Δu = f.
The fact that a minimizer to the Dirichlet integral (9.82) satisﬁes the Poisson equation
is an immediate consequence of our general Minimization Theorem 9.26. On the other
hand, proving the existence of a C2 minimizing function is a nontrivial issue.
Indeed,
the need for a rigorous existence proof was not immediately recognized: arguing from the
ﬁnite-dimensional situation, Dirichlet deemed existence to be self-evident, but it was not
until 50 years later that Hilbert supplied the ﬁrst rigorous proof — which was one of his
primary motivations for introducing the mathematical machinery of Hilbert space.
The Dirichlet principle (9.82) was derived under the assumption that the boundary
conditions are homogeneous — either pure Dirichlet or mixed. As it turns out, the mini-
mization principle, as stated, also applies to the inhomogeneous Dirichlet boundary value
problem. However, the minimizing functional that characterizes the solution to a mixed
boundary value problem with inhomogeneous Neumann conditions on part of the boundary
acquires an additional boundary term.
Theorem 9.32. The solution u(x, y) to the boundary value problem
−Δu = f
in
Ω,
u = h
on
D ⊂∂Ω,
∂u
∂n = k
on
N = ∂Ω \ D,
(9.83)
with D ̸= ∅, is characterized as the unique function that minimizes the modiﬁed Dirichlet
integral
Q[u] =
 
Ω
 1
2 u2
x + 1
2 u2
y −f u

dx dy −

N
k u ds
(9.84)
among all C2 functions that satisfy the prescribed boundary conditions.

9.3 Minimization Principles
369
In particular, the inhomogeneous Dirichlet problem has N = ∅, in which case the
extra boundary integral does not appear.
Proof : Write u(x, y) = u(x, y)+v(x, y), where v is any function that satisﬁes the given
boundary conditions: v = h on D, while ∂v/∂n = k on N. (We speciﬁcally do not require
that v satisfy the Poisson equation.) Their diﬀerence u = u −v satisﬁes the corresponding
homogeneous boundary conditions, along with the modiﬁed Poisson equation
−Δu = f ≡f + Δv
in
Ω,
u = 0
on
D,
∂u
∂n = 0
on
N.
Theorem 9.31 implies that u minimizes the Dirichlet functional
Q[u ] = 1
2 |∥∇u ∥|2 −⟨⟨f , u ⟩⟩=
 
Ω
 1
2 u2
x + 1
2 u2
y −f u

dx dy
among all functions satisfying the homogeneous boundary conditions. We compute
Q[u ] = Q[u −v] = 1
2 |∥∇(u −v) ∥|2 −⟨f + Δv , u −v ⟩
= 1
2 |∥∇u ∥|2 −⟨∇u , ∇v ⟩+ 1
2 |∥∇v ∥|2 −⟨f , u ⟩−⟨Δv , u ⟩+ ⟨f + Δv , v ⟩
= Q[u] −
 
Ω
(∇u · ∇v + uΔv) dx dy + C0,
where
C0 = 1
2 |∥∇v ∥|2 + ⟨f + Δv , v ⟩
does not depend on u. We then apply formula (6.83) to evaluate the middle terms:
 
Ω
(∇u · ∇v + uΔv) dx dy =
&
∂Ω
u ∂v
∂n ds =

D
h ∂v
∂n ds +

N
u k ds.
Thus,
Q[u ] = Q[u] −

N
k u ds + C1 = Q[u] + C1,
where the ﬁnal term C1 = C0 +

D
h ∂v
∂n ds is ﬁxed by the boundary conditions and the
choice of v, and so its value does not change when the function u is varied. We conclude
that u minimizes Q[u ] if and only if u = u + v minimizes Q[u].
Q.E.D.
Exercises
♥9.3.18.(a) Show that the function u(x, y) = 1
2 (−xy + xy2 + x2 y −x2 y2) solves the homoge-
neous Dirichlet boundary value problem for the Poisson equation −Δu = x2 + y2 −x −y
on the unit square S = {0 ≤x ≤1, 0 ≤y ≤1}. (b) Write down the Dirichlet integral
(9.82) for this boundary value problem. What is its value for your solution? (c) Write
down three other functions that satisfy the homogeneous Dirichlet boundary conditions on
S, and check that all three have larger Dirichlet integrals.
9.3.19.(a) Suppose u(x, y) solves the boundary value problem −Δu = f in Ω and u = 0 on
∂Ω, with f(x, y) ̸≡0. Prove that its Dirichlet integral (9.82) is strictly negative: Q[u] < 0.
(b) Does this result hold for the inhomogeneous boundary value problem u = h on ∂Ω?

370
9 A General Framework for Linear Partial Diﬀerential Equations
♥9.3.20. Consider the boundary value problem −Δu = 1, x2 + y2 < 1, u = 0, x2 + y2 = 1.
(a) Find all solutions. (b) Formulate the Dirichlet minimization principle for this prob-
lem. Carefully indicate the function space over which you are minimizing. Make sure your
solution belongs to the function space. (c) Which of the following functions belong to your
function space? (i) 1−x2−y2, (ii) 1 −1
2 x2 −1
2 y2, (iii) x−x3−xy2, (iv) x4−x2 y2+y4,
(v)
1
2 e−x2−y2 −1
2 e−1. (d) For each function in part (c) that does belong to your function
space, verify that its Dirichlet integral is larger than your solution’s value.
9.3.21. Suppose λ > 0. Under what conditions does the inhomogeneous Neumann problem
−Δu + λ u = f in Ω, ∂u/∂n = k on ∂Ω, for the Helmholtz equation have a solution? Is the
solution unique? Hint: Is the boundary value problem positive deﬁnite?
♦9.3.22. Suppose κ(x) > 0 for all a ≤x ≤b.
(a) Prove that the solution u⋆(x) to the inhomogeneous Dirichlet boundary value problem
−d
dx

κ(x) du
dx

= f(x),
u(a) = α,
u(b) = β,
minimizes the functional Q[u] =
	 b
a
 1
2 κ(x) u′(x)2 −f(x) u(x)
 
dx.
Hint: Mimic the proof of Theorem 9.32.
(b) Construct a minimization principle for the mixed boundary value problem
−d
dx

κ(x) du
dx

= f(x),
u(a) = α,
u′(b) = β.
9.3.23. Use the result of Exercise 9.3.22 to ﬁnd the C2 function u⋆(x) that minimizes the inte-
gral Q[u] =
	 2
1
⎡
⎣x
2
du
dx
2
+ x2 u
⎤
⎦dx when subject to the boundary conditions u(1) = 0,
u(2) = 1.
9.3.24. Find the function u(x) that minimizes the integral Q[u] =
	 2
1 [x(u′)2 + x2 u] dx subject
to the boundary conditions u(1) = 1, u′(2) = 0. Hint: Use Exercise 9.3.22(b).
9.3.25. Prove that the functional Q[u] =
	 1
0 (u′)2 dx, when subject to the mixed boundary
conditions u(0) = 0, u′(1) = 1, has no minimizer.
♥9.3.26. Let p1(x, y), p2(x, y), q(x, y) > 0 be strictly positive functions on a closed, bounded,
connected domain Ω ⊂R2. Consider the boundary value problem for the second-order par-
tial diﬀerential equation
−∂
∂x

p1(x, y) ∂u
∂x

−∂
∂y

p2(x, y) ∂u
∂y

+ q(x, y) u = f(x, y),
(x, y) ∈Ω,
(9.85)
subject to homogeneous Dirichlet boundary conditions u = 0 on ∂Ω.
(a) True or false: Equation (9.85) is an elliptic partial diﬀerential equation. (b) Write
the boundary value problem in self-adjoint form L∗◦L[u] = f. Hint: Regard (9.85) as a
“two-dimensional Sturm–Liouville equation”. (c) Prove that this boundary value problem
is positive deﬁnite, and then ﬁnd a minimization principle that characterizes the solution.
(d) Find suitable homogeneous Neumann–type boundary conditions involving the values of
the derivatives of u on ∂Ω that make the resulting boundary value problem for (9.85) self-
adjoint. Is your boundary value problem positive deﬁnite? Why or why not?

9.4 Eigenvalues and Eigenfunctions
371
9.4 Eigenvalues and Eigenfunctions
We have already come to appreciate the value of eigenfunctions for constructing separable
solutions to dynamical partial diﬀerential equations such as the one-dimensional heat and
s. In both cases, the eigenfunctions are trigonometric, and are used to write the solution
to the initial value problem in the form of a Fourier series. The most important feature
is that the Fourier eigenfunctions are orthogonal with respect to the underlying L2 inner
product. As we remarked earlier, orthogonality is not an accident. Rather, it is a direct
consequence of the self-adjointn of the linear diﬀerential operator prescribing the eigenvalue
equation. The goal of this section is, in preparation for extending the eigenfunction method
to higher-dimensional and more general dynamical problems, to establish the orthogonality
property of eigenfunctions in general, discuss how positive (semi-)deﬁniteness aﬀects the
eigenvalues, and present the basic theory of eigenfunction series expansions, thereby sig-
niﬁcantly generalizing basic Fourier series. As an application, we deduce a general formula
for the Green’s function of a positive deﬁnite boundary value problem as an inﬁnite series
in the eigenfunctions, and use this to formulate a condition that guarantees completeness
of the eigenfunctions. Along the way, we also need to introduce an important minimization
principle, the Rayleigh quotient, that characterizes the eigenvalues of a positive deﬁnite
linear system.
We begin with the eigenvalue problem
S[v] = λ v
(9.86)
for a linear operator S: U →U on† a real or complex vector space U. Clearly, v = 0 solves
the eigenvalue equation no matter what the scalar λ is. If the homogeneous linear system
(9.86) admits a nonzero solution 0 ̸= v ∈U, then λ ∈C is called an eigenvalue of the
operator S and v a corresponding eigenvector or eigenfunction, depending on the context.
If λ is an eigenvalue, then the corresponding eigenspace is the subspace
Vλ = ker(S −λ I ) = { v | S[v] = λ v } ⊂U,
(9.87)
consisting of all the eigenvectors/eigenfunctions along with the zero element. To avoid
technical diﬃculties, we will work under the assumption that all the eigenspaces are ﬁnite-
dimensional, and we call 1 ≤dim Vλ < ∞the geometric multiplicity of the eigenvalue
λ. Finite-dimensionality is almost always valid, and indeed, will be later established for
regular boundary value problems on bounded domains.
Self–Adjoint Operators
In the applications considered here, the vector space U comes equipped with an inner
product, and S is a self-adjoint linear operator. In such instances, one can readily establish
the basic orthogonality property of the eigenvectors/eigenfunctions.
Theorem 9.33. If S = S∗is a self-adjoint linear operator on an inner product space
U, then all its eigenvalues are real. Moreover, the eigenvectors/eigenfunctions associated
with diﬀerent eigenvalues are automatically orthogonal.
†
As discussed earlier, in the inﬁnite-dimensional case, the diﬀerential operator S might be
only deﬁned on a dense subspace of U consisting of suﬃciently smooth functions.

372
9 A General Framework for Linear Partial Diﬀerential Equations
Proof : To prove the ﬁrst part of the theorem, suppose λ is a complex eigenvalue,
so that S[v] = λ v for some complex eigenvector/eigenfunction v ̸= 0.
Then, using
the sesquilinearity (B.19) of the underlying Hermitian inner product‡ and self-adjointness
(9.45) of S, we ﬁnd
λ ∥v ∥2 = ⟨λ v , v ⟩= ⟨S[v] , v ⟩= ⟨v , S[v] ⟩= ⟨v , λ v ⟩= λ ∥v ∥2.
Since v ̸= 0, this immediately implies that λ = λ, its complex conjugate, and hence λ must
necessarily be real.
To prove orthogonality, suppose S[u] = λ u and S[v] = μ v. Again by self-adjointness,
λ ⟨u , v ⟩= ⟨λu , v ⟩= ⟨S[u] , v ⟩= ⟨u , S[v] ⟩= ⟨u , μv ⟩= μ ⟨u , v ⟩,
where the ﬁnal equality relies on the fact that the eigenvalue μ is real. Therefore, the
assumption that λ ̸= μ immediately implies orthogonality: ⟨u , v ⟩= 0.
Q.E.D.
Thus, the eigenvalues of self-adjoint linear operators are necessarily real. If, in addi-
tion, the operator is positive deﬁnite, then its eigenvalues must, in fact, be positive.
Theorem 9.34. If S > 0 is a self-adjoint positive deﬁnite linear operator, then all its
eigenvalues are strictly positive: λ > 0. If S ≥0 is self-adjoint and positive semi-deﬁnite,
then its eigenvalues are nonnegative: λ ≥0.
Proof : Self-adjointness assures us that all of the eigenvalues are real. Suppose S[u] =
λ u with u ̸= 0 a real eigenfunction. Then
λ ∥u ∥2 = λ ⟨u , u ⟩= ⟨λu , u ⟩= ⟨S[u] , u ⟩> 0,
by positive deﬁniteness. Since ∥u ∥2 > 0, this immediately implies that λ > 0. The same
argument implies that λ ≥0 in the positive semi-deﬁnite case.
Q.E.D.
All the linear operators to be considered in this text are real, and, at the very least,
self-adjoint, and often either positive deﬁnite or semi-deﬁnite. Thus, we will restrict our
attention from here on (at least until we reach the Schr¨odinger equation in the ﬁnal sub-
section) to real operators deﬁned on real vector spaces, knowing a priori that we are not
overlooking any eigenvalues or eigenfunctions by this restriction.
Example 9.35.
In ﬁnite dimensions, if we equip U = Rn with the dot product,
then any self-adjoint linear function is given by multiplication by an n × n symmetric
matrix: S[u] = K u, where KT = K. Theorem 9.33 implies the well-known result that
a symmetric matrix has only real eigenvalues. Moreover, the eigenvectors associated with
diﬀerent eigenvalues are mutually orthogonal.
In fact, it can be proved that, in general, the eigenvectors of a symmetric matrix are
complete, [89]. In other words, there exists an orthogonal basis v1, . . . , vn of Rn consisting
of eigenvectors of K, so K vj = λjvj for j = 1, . . . , n. If the eigenvalues λ1, . . . , λn are
all simple, so λi ̸= λj for i ̸= j, then the basis eigenvectors are automatically orthogonal.
When K has repeated eigenvalues, this requires selecting an orthogonal basis of each of
the associated eigenspaces Vλ = ker(K −λ I ), e.g., using the Gram–Schmidt process.
‡
We are temporarily working in the vector space of complex-valued functions.
Once we
establish reality of the eigenvalues and eigenfunctions, we can shift our focus back to the real
function space.

9.4 Eigenvalues and Eigenfunctions
373
Completeness implies that the number of linearly independent eigenvectors associated with
an eigenvalue, i.e., its geometric multiplicity, is the same as the eigenvalue’s algebraic
multiplicity. If, furthermore, the matrix K > 0 is symmetric and positive deﬁnite, then
Theorem 9.34 implies that all its eigenvalues are positive: λj > 0. In this case, thanks
to completeness, the converse is also valid: a symmetric matrix is positive deﬁnite if and
only if it has all positive eigenvalues. These results can all be immediately generalized to
self-adjoint matrices under general inner products on Rn.
Example 9.36. Consider the Dirichlet eigenvalue problem
−d2v
dx2 = λ v,
v(0) = 0,
v(ℓ) = 0,
for the diﬀerential operator S = −D2 on an interval of length ℓ> 0. As we know — see,
for instance, Section 4.1 — the eigenvalues and eigenfunctions are
λn =
"n π
ℓ
#2
,
vn(x) = sin nπx
ℓ
,
n = 1, 2, 3, . . ..
We now understand this example in our general framework. The fact that the eigenvalues
are real and positive follows from the fact that the boundary value problem is deﬁned by
the self-adjoint positive deﬁnite operator
S[u] = D∗◦D[u] = −D2[u] = −u′′,
acting on the vector space U = {u(0) = u(ℓ) = 0}, equipped with the L2 inner product:
⟨u , v ⟩=
 ℓ
0
u(x) v(x) dx.
The orthogonality of the Fourier sine functions,
⟨vm , vn ⟩=
 ℓ
0
sin mπx
ℓ
sin nπx
ℓ
dx = 0
for
m ̸= n,
is also an automatic consequence of their status as eigenfunctions of this self-adjoint bound-
ary value problem.
Example 9.37. Similarly, the periodic boundary value problem
−v′′ = λv,
v(−π) = v(π),
v′(−π) = v′(π),
(9.88)
has eigenvalues λ0 = 0, with eigenfunction v0(x) ≡1, and λn = n2, for n = 1, 2, 3, . . ., each
possessing two independent eigenfunctions: vn(x) = cos nx and vn(x) = sin nx. In this
case, a zero eigenvalue appears because S = D∗◦D = −D2 is only positive semi-deﬁnite
on the space of periodic functions. Theorem 9.33 implies the all-important orthogonality of
the Fourier eigenfunctions corresponding to diﬀerent eigenvalues: ⟨vm , vn ⟩= ⟨vm , vn ⟩=
⟨vm , vn ⟩= 0 for m ̸= n, under the L2 inner product on [−π, π ]. However, since they have
the same eigenvalue, the orthogonality of vn(x) = cos nx and vn(x) = sin nx, while true,
is not ensured and must be checked by hand.
Example 9.38. On the other hand, the self-adjoint boundary value problem
−d2u
dx2 = λ u,
lim
x →∞u(x) = 0,
lim
x →−∞u(x) = 0,
(9.89)

374
9 A General Framework for Linear Partial Diﬀerential Equations
on the real line has no eigenvalues: no matter what the value of λ, the only solution
decaying to 0 at both ±∞is the zero solution. Indeed, exponential solutions that decay at
one end become inﬁnitely large at the other. The trigonometric functions u(x) = cos ωx
and sin ωx satisfy the diﬀerential equation when λ = ω2 > 0, but do not go to zero as
| x | →∞, and so do not qualify as bona ﬁde eigenfunctions. Rather, because they are
bounded on the entire line, they represent the “continuous spectrum” of the underlying
self-adjoint diﬀerential operator, [95]. In this particular context, the continuous spectrum
leads directly to the Fourier transform.
Example 9.39. The eigenvalue problem for the Bessel diﬀerential operator of order
m, given in (9.79), is governed by the following diﬀerential equation:
S[u] = −u′′ −1
x u′ + m2
x2 u = λ u,
(9.90)
or, equivalently,
x2 d2u
dx2 + x du
dx + (λ x2 −m2) u = 0,
supplemented by appropriate homogeneous boundary conditions at the endpoints of the
interval 0 ≤a < b. Its eigenfunctions are not elementary, but, as we will learn in Chap-
ter 11, can be expressed in terms of Bessel functions. Nevertheless, no matter what their
eventual formula, Theorem 9.33 guarantees the orthogonality of any two eigenfunctions
v, v associated with distinct eigenvalues λ ̸= λ under the weighted inner product (9.80):
⟨v , v ⟩=
 b
a
v(x) v(x) x dx = 0.
Example 9.40.
According to equation (9.60), on a bounded domain Ω ⊂R2, the
(negative) Laplacian −Δ forms a self-adjoint positive (semi-)deﬁnite operator under the
L2 inner product (9.22) when subject to one of the usual sets of homogeneous boundary
conditions. Let us, for speciﬁcity, concentrate on the Dirichlet case. The eigenfunctions of
the Laplacian are the nonzero solutions to the following boundary value problem:
−Δv = λv
in
Ω,
u = 0
on
∂Ω.
(9.91)
The underlying partial diﬀerential equation, namely
∂2v
∂x2 + ∂2v
∂y2 + λ v = 0,
is known as the Helmholtz equation, named after the inﬂuential and wide-ranging German
applied mathematician Hermann von Helmholtz. As we will see, the Helmholtz equation
plays a central role in the solution of the two-dimensional heat, wave, and Schr¨odinger
equations.
Only in a few special cases, e.g., rectangles and circular disks, can the eigenfunc-
tions and eigenvalues be determined exactly; see Chapter 11 for details.
Nevertheless,
Theorem 9.34 guarantees that, for all domains, the eigenvalues are always nonnegative,
λ ≥0, with λ0 = 0 being an eigenvalue only in positive semi-deﬁnite cases, e.g., Neu-
mann boundary conditions. Moreover, Theorem 9.33 ensures the orthogonality of any two
eigenfunctions,
⟨v , v ⟩=
 
Ω
v(x, y) v(x, y) dx dy = 0,
that are associated with distinct eigenvalues λ ̸= λ .

9.4 Eigenvalues and Eigenfunctions
375
The Rayleigh Quotient
We have already learned how to characterize the solutions of positive deﬁnite boundary
value problems by a minimization principle. One can also characterize their eigenvalues
by a minimization principle, named after the proliﬁc nineteenth-century English applied
mathematician Lord Rayleigh (John Strutt).
Deﬁnition 9.41. Let S: U →U be a self-adjoint linear operator on an inner product
space. The Rayleigh quotient of S is deﬁned as
R[u] = ⟨u , S[u]⟩
∥u ∥2
for
0 ̸= u ∈U.
(9.92)
We are, in fact, primarily interested in the Rayleigh quotient of positive deﬁnite op-
erators, for which R[u] > 0 for all u ̸= 0. If S = L∗◦L, then, using (9.51), we can rewrite
the Rayleigh quotient in the alternative form
R[u] = |∥L[u] ∥|2
∥u ∥2
,
(9.93)
keeping in mind our notational convention (9.50) for the respective norms on U and V .
Theorem 9.42. Let S be a self-adjoint linear operator. Then the minimum value of
its Rayleigh quotient,
λ⋆= min { R[u] | u ̸= 0 } ,
(9.94)
is the smallest eigenvalue of the operator S. Moreover, any 0 ̸= v⋆∈U that achieves this
minimum value, R[v⋆] = λ⋆, is an associated eigenvector/eigenfunction: S[v⋆] = λ⋆v⋆.
Proof : Suppose that v⋆∈U is a minimizing element, and
λ⋆= R[v⋆] = ⟨v⋆, S[v⋆] ⟩
∥v⋆∥2
(9.95)
the minimum value. Given any u ∈U, deﬁne the scalar function†
g(t) = R[v⋆+ t u] = ⟨v⋆+ t u , S[v⋆+ t u] ⟩
∥v⋆+ t u ∥2
= ⟨v⋆, S[v⋆] ⟩+ 2t ⟨u , S[v⋆] ⟩+ t2 ⟨u , S[u]⟩
∥v⋆∥2 + 2t ⟨u , v⋆⟩+ t2 ∥u ∥2
,
where we used the self-adjointness of S and the fact that we are working in a real inner
product space to identify the terms
⟨u , S[v⋆] ⟩= ⟨S[u] , v⋆⟩= ⟨v⋆, S[u] ⟩.
Since
g(0) = R[v⋆] ≤R[v⋆+ t u] = g(t),
the function g(t) will attain its minimum value at t = 0. Elementary calculus tells us that
0 = g′(0) = 2 ⟨u , S[v⋆] ⟩∥v⋆∥2 −⟨v⋆, S[v⋆] ⟩⟨u , v⋆⟩
∥v⋆∥4
.
†
g(t) is not deﬁned if v⋆+ t u = 0, but this does not aﬀect the argument.

376
9 A General Framework for Linear Partial Diﬀerential Equations
Therefore, using (9.95) to replace ⟨v⋆, S[v⋆] ⟩by λ⋆∥v⋆∥2, we must have
⟨u , S[v⋆] ⟩−λ⋆⟨u , v⋆⟩= ⟨u , S[v⋆] −λ⋆v⋆⟩= 0.
(9.96)
The only way the inner product in (9.96) can vanish for all possible u ∈U is if
S[v⋆] = λ⋆v⋆,
(9.97)
which means that 0 ̸= v⋆is an eigenfunction and λ⋆its associated eigenvalue.
On the other hand, if v is any eigenfunction, so S[v] = λv, where, by self-adjointness,
the eigenvalue λ is necessarily real, then the value of its Rayleigh quotient is
R[v] = ⟨v , S[v] ⟩
∥v ∥2
= ⟨v , λv ⟩
∥v ∥2
= λ.
(9.98)
Since λ⋆was, by deﬁnition, the smallest possible value of the Rayleigh quotient, it thus
must necessarily be the smallest eigenvalue.
Q.E.D.
Remark: The existence of a minimizing function is not addressed in this result, and,
indeed, there may be no minimum eigenvalue; the inﬁmum of the set of eigenvalues could be
−∞or, even if ﬁnite, not an eigenvalue. However, for the positive deﬁnite boundary value
problems considered here, the eigenvalues are all strictly positive, and one can, with some
additional analysis, [44], prove the existence of a minimizing eigenfunction, and hence a
smallest positive eigenvalue.
We label the eigenvalues in increasing order, so that, assuming positive deﬁniteness,
0 < λ1 ≤λ2 ≤λ3 ≤· · · , with λ1 the minimum eigenvalue and hence the minimum value
of the Rayleigh quotient. To characterize the other eigenvalues, we need to restrict the
class of functions over which one minimizes. Indeed, since the nth eigenfunction vn must
be orthogonal to all its predecessors v1, . . . , vn−1, it makes sense to try minimizing the
Rayleigh quotient over such elements.
Theorem 9.43. Let v1, . . . , vn−1 be eigenfunctions corresponding to the ﬁrst n −1
eigenvalues 0 < λ1 ≤· · · ≤λn−1 of the positive deﬁnite self-adjoint linear operator S.
Let
Un−1 =
-
u
 ⟨u , v1 ⟩= · · · = ⟨u , vn−1 ⟩= 0
.
⊂U
(9.99)
be the set of functions that are orthogonal to the indicated eigenfunctions.
Then the
minimum value of the Rayleigh quotient function restricted to the subspace Un−1 is the
nth eigenvalue of S, that is,
λn = min
-
R[u]
 0 ̸= u ∈Un−1
.
,
(9.100)
and any minimizer is an associated eigenfunction vn.
Proof : We follow the preceding proof, but now restrict v⋆and u to belong to the sub-
space Un−1. Observe that S[u] ∈Un−1 whenever u ∈Un−1, because, by self-adjointness,
⟨S[u] , vj ⟩= ⟨u , S[vj ] ⟩= λj ⟨u , vj ⟩= 0
for
j = 1, . . ., n −1.
Thus, if 0 ̸= v⋆∈Un−1 minimizes the Rayleigh quotient, then (9.96) holds for arbitrary
u ∈Un−1. In particular, choosing u = S[v⋆] −λ⋆v⋆, we conclude that v⋆satisﬁes the
eigenvalue equation (9.97), and hence must be an eigenfunction that is orthogonal to the
ﬁrst n −1 eigenfunctions. This means that λ⋆= λn must be the next-lowest eigenvalue
and v⋆= vn one of its associated eigenfunctions.
Q.E.D.

9.4 Eigenvalues and Eigenfunctions
377
Example 9.44. Return to the Dirichlet eigenvalue problem on the interval [0, ℓ] for
the self-adjoint (under the L2 inner product) diﬀerential operator −D2 = D∗◦D discussed
in Example 9.36. Its Rayleigh quotient can be written as
R[u] = ⟨u , −u′′ ⟩
∥u ∥2
= −
 ℓ
0
u(x) u′′(x) dx
 ℓ
0
u(x)2 dx
=
 ℓ
0
u′(x)2 dx
 ℓ
0
u(x)2 dx
= |∥u′ ∥|2
∥u ∥2 ,
where the second expression, based on the alternative form (9.93), can be readily deduced
from the ﬁrst via an integration by parts. (Here, both domain and target space of L = D
use the same L2 norm.) According to Theorem 9.42, the minimum value of R[u] over
all nonzero functions u(x) ̸≡0 satisfying the boundary conditions u(0) = u(ℓ) = 0 is the
lowest eigenvalue, namely
λ1 = π2
ℓ2 = min
6
R[u]
 u(0) = u(ℓ) = 0, u(x) ̸≡0
7
,
which is achieved if and only if u(x) is a nonzero constant multiple of sin(πx/ℓ), the
corresponding eigenfunction. The reader is invited to numerically test this result by ﬁxing
a value of ℓ, and then evaluating R[u] on various functions u(x) satisfying the boundary
conditions to check that the numerical value is always larger than π2/ℓ2, the smallest
eigenvalue. The second eigenvalue can be found by minimizing over all nonzero functions
that are orthogonal to the ﬁrst eigenfunction:
λ2 = 4π2
ℓ2
= min

R[u]
 u(0) = u(ℓ) = 0,
 ℓ
0
u(x) sin π
ℓx dx = 0, u(x) ̸≡0
8
,
and similarly for the higher eigenvalues.
Example 9.45. Consider the Helmholtz eigenvalue problem (9.91) on a bounded do-
main Ω ⊂R2, subject to Dirichlet boundary conditions. The associated Rayleigh quotient
(9.93) can be written in the form
R[u] =
|∥∇u ∥|2
∥u ∥2
=
 
Ω
$ ∂u
∂x
2
+
∂u
∂y
2 %
dx dy
 
Ω
u(x, y)2 dx dy
.
(9.101)
Its minimum value among all nonzero functions u(x, y) ̸≡0 subject to the boundary condi-
tions u = 0 on ∂Ω is the smallest eigenvalue λ1, and the minimizing function is any nonzero
constant multiple of the associated eigenfunction v1(x, y). To obtain a higher eigenvalue
λn, one minimizes R[u], where u(x, y) ̸≡0 again satisﬁes the boundary conditions and, in
addition, is orthogonal to the preceding n −1 eigenfunctions:
0 = ⟨u , vk ⟩=
 
Ω
u(x, y) vk(x, y) dx dy,
for
k = 1, . . ., n −1.
It can be proved, [34, 44], that, as long as the domain is bounded with, as always, a
reasonably nice boundary, there is a solution to each of these minimization problems, and
hence the Helmholtz equation admits an inﬁnite sequence of positive eigenvalues 0 < λ1 ≤
λ2 ≤λ3 ≤· · · , with λn →∞becoming arbitrarily large as n →∞; see also Theorem 9.47
below.

378
9 A General Framework for Linear Partial Diﬀerential Equations
Eigenfunction Series
For our applications to dynamical partial diﬀerential equations, we will be particularly
interested in expanding more general functions in terms of the orthogonal eigenfunctions,
the simplest case being the classical Fourier series. To ﬁx notation, we will proceed as
if we were treating a one-dimensional boundary value problem, although the formulas
are equally valid for higher-dimensional problems, e.g., those governed by the Helmholtz
equation. Thus, we consider an eigenvalue problem of the form S[v] = λ v, where S is
a positive deﬁnite or semi-deﬁnite operator that is self-adjoint relative to a weighted L2
inner product
⟨v , v ⟩=
 b
a
v(x) v(x) ρ(x) dx,
(9.102)
with ρ(x) > 0 on the bounded interval a ≤x ≤b.
Let 0 ≤λ1 ≤λ2 ≤λ3 ≤· · · be the eigenvalues, and v1, v2, v3, . . . , the corresponding
eigenfunctions. Theorem 9.33 assures us that those corresponding to diﬀerent eigenvalues
are mutually orthogonal:
⟨vj , vk ⟩= 0,
j ̸= k.
(9.103)
Orthogonality is not automatic if vj and vk belong to the same eigenvalue, but it can be
ensured by selecting an orthogonal basis of each eigenspace Vλ, if necessary by applying
the Gram–Schmidt orthogonalization process, [89].
Let f ∈U be an arbitrary function in our inner product space. The eigenfunction
series of f is, by deﬁnition, its generalized Fourier series:
f ∼

k
ck vk,
where the coeﬃcient
ck = ⟨f , vk ⟩
∥vk ∥2
(9.104)
is found by formally taking the inner product of both sides of (9.104) with the eigenfunction
vk and invoking their mutual orthogonality. (Note that our earlier eigenfunction series
formula (3.108) assumed orthonormality; here, it will be convenient to not necessarily
impose the condition ∥vk ∥= 1.)
For example, in the case covered by Example 9.36,
(9.104) becomes the usual Fourier sine series for the function f, whereas for Example 9.37,
it represents its full periodic Fourier series.
In a similar fashion, Example 9.40 leads
to series in the eigenfunctions of the Laplacian operator on a bounded domain subject
to appropriate homogeneous boundary conditions; explicit examples of the latter can be
found in Chapters 11 and 12.
As we learned in Section 3.5, convergence (in norm) of the series (9.104) requires com-
pleteness of the eigenfunctions. (Pointwise and uniform convergence are then implied by
more restrictive hypotheses on the function and the domain, e.g., f ∈C1.) In the ﬁnite-
dimensional context, when S: Rn →Rn is given by matrix multiplication, S[u] = K u,
there are only ﬁnitely many eigenvectors, and so the summation (9.104) has only ﬁnitely
many terms. There are, hence, no convergence considerations, and completeness is au-
tomatic. For boundary value problems in inﬁnite-dimensional function space, the com-
pleteness of the resulting eigensolutions is a more delicate issue. In Example 9.36, the
eigenvalue problem for S = −D2 subject to homogeneous Dirichlet boundary conditions
on a bounded interval leads to the Fourier sine eigenfunctions, which we know to be com-
plete. On the other hand, the corresponding eigenvalue problem on the real line, treated in
Example 9.38, has no eigenfunctions, and so completeness is out of the question. As we will

9.4 Eigenvalues and Eigenfunctions
379
see, the eigenfunctions associated with regular boundary value problems on bounded do-
mains are automatically complete, whereas singular problems and problems on unbounded
domains require additional analysis.
Whether or not the eigenfunctions are complete, we always have Bessel’s inequality†
(3.117):

k
c2
k ∥vk ∥2 ≤∥f ∥2.
(9.105)
Theorem 3.43 says that the eigenfunctions are complete if and only if Bessel’s inequality
is an equality, which is then the Plancherel formula for the eigenfunction expansion.
Green’s Functions and Completeness
We now combine two of our principal themes. Remarkably, the key to the completeness
of eigenfunctions for boundary value problems lies in the eigenfunction expansion of the
Green’s function!
Assume that S is both self-adjoint and positive deﬁnite.
Thus, by
Theorem 9.34, all its eigenvalues are positive. We index them in increasing order:
0 < λ1 ≤λ2 ≤λ3 ≤· · · ,
(9.106)
where each eigenvalue is repeated according to its multiplicity.
By positive deﬁniteness, the boundary value problem S[u] = f has a unique solution.‡
Therefore, it admits a Green’s function Gξ(x) = G(x; ξ), which satisﬁes the boundary value
problem
S[Gξ ] = δξ,
(9.107)
with a delta function impulse on the right-hand side. For each ﬁxed ξ, let us write down
the eigenfunction series (9.104) for the Green’s function:
G(x; ξ) =
∞

k=1
ck(ξ)vk(x),
where the coeﬃcient
ck(ξ) =
⟨Gξ , vk ⟩
∥vk ∥2
(9.108)
depends on the impulse point ξ. Since S[vk ] = λkvk, the coeﬃcients can be explicitly
evaluated by means of the following calculation:
λk ck(ξ) ∥vk ∥2 = ⟨Gξ , λkvk ⟩= ⟨Gξ , S[vk ] ⟩
= ⟨S[Gξ ] , vk ⟩= ⟨δξ , vk ⟩=
 b
a
δ(x −ξ)vk(x)ρ(x) dx = vk(ξ)ρ(ξ),
where ρ(x) is the weight function of our inner product (9.102), and we invoked the self-
adjointness of S. Solving for
ck(ξ) = vk(ξ)ρ(ξ)
λk ∥vk ∥2
(9.109)
†
Formula (3.117) assumed orthonormality of the functions; here we are stating the analogous
result for orthogonal elements. Moreover, here, the eigenfunctions and hence the coeﬃcients ck
are all real, so we don’t need absolute value signs.
‡
As usual, we are assuming existence of the solution; Proposition 9.19 guarantees uniqueness.

380
9 A General Framework for Linear Partial Diﬀerential Equations
and then substituting back into (9.108), we deduce the explicit eigenfunction series
G(x; ξ) ∼
∞

k=1
vk(x)vk(ξ)ρ(ξ)
λk ∥vk ∥2
(9.110)
for the Green’s function. Observe that this expression is compatible with the weighted
symmetry equation (9.58).
Example 9.46.
According to Example 6.9, the Green’s function for the L2 self-
adjoint boundary value problem
−u′′ = f(x),
u(0) = 0 = u(1),
is
G(x; ξ) =
 x(1 −ξ),
x ≤ξ,
ξ(1 −x),
x ≥ξ.
(9.111)
On the other hand, the eigenfunctions for
−v′′ = λ v,
v(0) = 0 = v(1),
are vk(x) = sin kπx, with corresponding eigenvalues λk = k2π2, for k = 1, 2, 3, . . . . Since
∥vk ∥2 =
 1
0
sin2 kπx dx = 1
2 ,
formula (9.110) implies the eigenfunction expansion
G(x; ξ) =
∞

k=1
2 sin kπx sin kπξ
k2π2
.
(9.112)
This result can be checked by a direct computation of the Fourier sine series of (9.111).
Let us now apply Bessel’s inequality (9.105) to the eigenfunction series (9.108) for the
Green’s function; using (9.109), the result is
n

k=1
ck(ξ)2 ∥vk ∥2 =
n

k=1
vk(ξ)2 ρ(ξ)2
λ2
k ∥vk ∥2
≤∥Gξ ∥2 =
 b
a
G(x; ξ)2ρ(x) dx.
(9.113)
We divide by ρ(ξ) > 0, and then integrate both sides of the resulting inequality from a to
b. On the left-hand side, the integrated summands are
 b
a
vk(ξ)2 ρ(ξ)
λ2
k ∥vk ∥2 dξ =
1
λ2
k ∥vk ∥2
 b
a
vk(ξ)2 ρ(ξ) dξ = 1
λ2
k
.
Substituting back into (9.113) establishes the interesting inequality
n

k=1
1
λ2
k
≤
 b
a
 b
a
G(x; ξ)2 ρ(x)
ρ(ξ) dx dξ.
(9.114)
To make the right-hand side look less strange, we can replace G(x; ξ) by the symmetric

9.4 Eigenvalues and Eigenfunctions
381
modiﬁed Green’s function G(x; ξ) = G(x; ξ)/ρ(ξ) = G(ξ; x), cf. (9.59), whence
 b
a
 b
a
G(x; ξ)2 ρ(x)
ρ(ξ) dx dξ =
 b
a
 b
a
G(x; ξ)2ρ(x)ρ(ξ) dx dξ ≡∥G ∥2,
(9.115)
which we can interpret as a “double weighted L2 norm” of the modiﬁed Green’s function
G(x; ξ). Since the summands in (9.114) are all positive, we can let n →∞, and conclude
that
∞

k=1
1
λ2
k
≤∥G ∥2.
(9.116)
Thus, assuming that the right-hand side of this inequality is ﬁnite, the summation on the
left converges. This implies that its summands must go to zero: λ−2
k
→0 as k →∞. We
have thus proved the ﬁrst statement of the following important result.
Theorem 9.47.
If ∥G ∥2 < ∞, then the eigenvalues of the positive deﬁnite self-
adjoint operator S are unbounded: 0 < λk →∞as k →∞. Moreover, the associated
orthogonal eigenfunctions v1, v2, v3, . . . , are complete.
Proof : Our remaining task is to prove completeness — that is, that the eigenfunction
series (9.104) of any function f ∈U converges in norm. For n = 2, 3, 4, . . . , consider the
function
gn−1 = f −
n−1

k=1
ckvk,
i.e., the diﬀerence between the function f and the (n−1)st partial sum of its eigenfunction
series. Completeness requires that
∥gn−1 ∥−→0
as
n →∞.
(9.117)
We can assume that gn−1 ̸= 0, since otherwise, the eigenfunction series terminates, with
0 = gn−1 = gn = gn+1 = · · · (why?), and so (9.117) holds trivially.
First, note that, for any j = 1, . . . , n −1,
⟨gn−1 , vj ⟩= ⟨f , vj ⟩−
n−1

k=1
ck⟨vk , vj ⟩= ⟨f , vj ⟩−cj∥vj ∥2 = 0,
by the orthogonality of the eigenfunctions combined with the formula (9.104) for the co-
eﬃcient cj. Thus, gn−1 ∈Vn−1, the subspace (9.99) of functions orthogonal to the ﬁrst
n −1 eigenfunctions used in the Rayleigh Minimization Theorem 9.43. Since, according to
(9.100), λn is the minimum value of the Rayleigh quotient among all nonzero elements of
Vn−1, we must have
λn ≤R[gn−1 ] = ⟨gn−1 , S[gn−1 ] ⟩
∥gn−1 ∥2
,

382
9 A General Framework for Linear Partial Diﬀerential Equations
and hence
λn ∥gn−1 ∥2 ≤⟨gn−1 , S[gn−1 ] ⟩
=

f −
n−1

k=1
ckvk , S
$
f −
n−1

k=1
ckvk
% 
=

f −
n−1

k=1
ckvk , S[f ] −
n−1

k=1
ckS[vk ]

=

f −
n−1

k=1
ckvk , S[f ] −
n−1

k=1
ck λkvk

= ⟨f , S[f ] ⟩−
n−1

k=1
λk ck⟨f , vk ⟩−
n−1

k=1
ck⟨vk , S[f ] ⟩+
n−1

k=1
λk c2
k ∥vk ∥2
= ⟨f , S[f ] ⟩−
n−1

k=1
λk
⟨f , vk ⟩2
∥vk ∥2
.
In the ﬁnal equality, we used the self-adjointness of S to identify
⟨vk , S[f ] ⟩= ⟨S[vk ] , f ⟩= λk⟨vk , f ⟩= λk⟨f , vk ⟩,
coupled with the formula in (9.104) for the coeﬃcients ck. Since the summands in the ﬁnal
expression are all positive, we conclude that
∥gn−1 ∥2 ≤⟨f , S[f ] ⟩
λn
.
Since we already know that λn →∞, the right-hand side of the ﬁnal inequality goes to 0
as n →∞. This implies (9.117) and hence establishes completeness.
Q.E.D.
One important corollary of this theorem is that, since each eigenvalue is repeated
according to its geometric multiplicity, the multiplicity cannot be inﬁnite (why?), and
hence each eigenspace of such an S is necessarily ﬁnite-dimensional.
Example 9.48. For the eigenvalue problem considered in Example 9.46, since ρ(x) ≡
1, the double norm of the (modiﬁed) Green’s function G(x; ξ) = G(x; ξ) is
∥G ∥2 =
 1
0
 1
0
G(x; ξ)2 dx dξ = 2
 1
0
 ξ
0
x2(1 −ξ)2 dx dξ = 1
90 < ∞.
Thus, Theorem 9.47 re-establishes the completeness of the sine eigenfunctions, meaning
that the eigenfunction series, which is just the ordinary Fourier sine series on [0, 1], con-
verges in norm.
Indeed, for any regular Sturm–Liouville boundary value problem on a bounded in-
terval, the (modiﬁed) Green’s function is automatically continuous, and hence its double
weighted norm is ﬁnite.
Thus, Theorem 9.47 implies the completeness of the Sturm–
Liouville eigenfunctions. In Chapters 11 and 12, we will extend this result to some impor-
tant singular boundary value problems.

9.4 Eigenvalues and Eigenfunctions
383
Example 9.49. The completeness result of Theorem 9.47 doesn’t directly apply to
the periodic boundary value problem of Example 9.37, because it is not positive deﬁnite,
and hence there is no Green’s function. However, we can convert it into a positive deﬁnite
problem by a simple trick. As you are asked to prove in Exercise 9.4.4, if S ≥0 is any
positive semi-deﬁnite operator and μ > 0 any positive constant, then S = S+μ I is positive
deﬁnite, where I [u] = u is the identity operator. Thus, we replace the original periodic
boundary value problem (9.88) by the following modiﬁcation:
−v′′ + μv = λv,
v(−π) = v(π),
v′(−π) = v′(π).
(9.118)
This does not alter the eigenfunctions, while adding μ to each of the eigenvalues, and
hence the modiﬁed problem has eigenvalues λ0 = μ, with eigenfunction v0(x) ≡1, and
λn = n2 + μ, with two independent eigenfunctions: vn(x) = cos nx and vn(x) = sin nx.
The Green’s function for the periodic boundary value problem
−v′′ + μv = δ(x −ξ),
v(−π) = v(π),
v′(−π) = v′(π),
where μ > 0 is a ﬁxed constant, is derived along the same lines as in Example 6.10. Setting
μ = ω2, the result is
G(x; ξ) = cosh ω

π −| x −ξ |

2 ω sinh πω
.
(9.119)
Its double L2 norm is clearly ﬁnite, and, although unnecessary, can even be computed:
∥G ∥2 =
 π
−π
 π
−π
G(x; ξ)2 dx dξ = π

2πω + sinh 2πω

4ω3 sinh2 πω
< ∞.
As a result, Theorem 9.47 reconﬁrms the completeness of the trigonometric eigenfunctions.
Example 9.50. According to (6.120), the Green’s function G(x; ξ) for the Dirichlet
boundary value problem for the Poisson equation on a domain Ω ⊂R2 is the sum of a
logarithmic potential (6.106) and a harmonic function. Thus G(x; ξ)2 is a sum of three
terms: the ﬁrst two, involving (log r)2 and log r with r = ∥x −ξ ∥, have mild singularities
when x = ξ, while the last term is smooth (indeed analytic) everywhere.
Using this
information, it is not hard to prove that its double L2 norm
∥G ∥2 =
 
Ω
  
Ω
G(x, y; ξ, η)2 dx dy

dξ dη < ∞
is ﬁnite. Indeed, the only problematic point is the logarithmic singularity at x = ξ, but a
polar coordinate computation, similar to that used in the proof of Theorem 6.17, shows that
such logarithmic singularities still have ﬁnite integrals. Therefore, Theorem 9.47 implies
that the Helmholtz eigenvalues λn →∞, and the corresponding Helmholtz eigenfunctions
vn(x, y) form a complete orthogonal system.
Remark: In problems involving unbounded domains, such as the Schr¨odinger equation
for the hydrogen atom to be discussed in Section 12.7, the eigenfunctions are typically not
complete, and one needs to introduce additional solutions corresponding to what is known
as the continuous spectrum of the operator. Functions are now represented by combinations
of discrete Fourier-like sums over the eigenfunctions (the bound states in the quantum-
mechanical system) plus a Fourier integral-like term involving the continuous spectrum
(the scattering states), [66, 72]. A full discussion of completeness and convergence in such
cases must be deferred to an advanced course in analysis, [95].

384
9 A General Framework for Linear Partial Diﬀerential Equations
Exercises
9.4.1. Find the eigenvalues and an orthonormal eigenvector basis for the following symmetric
matrices:
(a)

2
6
6
−7

, (b)

5
−2
−2
5

, (c)

2
−1
−1
5

, (d)
⎛
⎜
⎝
1
0
4
0
1
3
4
3
1
⎞
⎟
⎠, (e)
⎛
⎜
⎝
6
−4
1
−4
6
−1
1
−1
11
⎞
⎟
⎠.
9.4.2. Determine whether the following symmetric matrices are positive deﬁnite by computing
their eigenvalues.
(a)

2
−2
−2
3

(b)

−2
3
3
6

,
(c)
⎛
⎜
⎝
1
−1
0
−1
2
−1
0
−1
1
⎞
⎟
⎠,
(d)
⎛
⎜
⎝
4
−1
−2
−1
4
−1
−2
−1
4
⎞
⎟
⎠.
9.4.3. Suppose S[u] = K u, where K =

0
1
−1
0

. (a) Show that S: R2 →R2 is positive semi-
deﬁnite under the dot product. (b) Find the eigenvalues of S. (c) Explain why your result
in part (b) does not contradict Theorem 9.34.
♦9.4.4. Suppose that S: U →U is a positive semi-deﬁnite linear operator. Let I : U →U be the
identity operator, so I [u] = u. (a) Prove that, for any positive scalar μ > 0, the operator
Sμ = S + μ I is positive deﬁnite.
(b) Show that S and Sμ have the same eigenfunctions.
Do they have the same eigenvalues? If not, how are their eigenvalues related?
9.4.5. Find the minimum value of R[v ] =
	 1
0 v′2 dx
	 1
0 v2 dx
on the space of C2 functions v(x) deﬁned
on 0 ≤x ≤1 that are subject to one of the following pairs of boundary conditions:
(a) v(0) = v(1) = 0,
(b) v(0) = v′(1) = 0,
(c) v′(0) = v′(1) = 0.
9.4.6. Find the minimum value of R[v ] =
	 e
1 x2v′2 dx
	 e
1 v2 dx
on the space of C2 functions deﬁned on
[1, e] subject to the boundary conditions v(1) = v(e) = 0.
9.4.7. Show that the Rayleigh quotient R[v ] has the same value for all nonzero scalar multiples
of an element 0 ̸= v ∈U, i.e., R[cv ] = R[v ] for all c ̸= 0.
9.4.8. Prove that the minimum value of the Rayleigh quotient of a positive semi-deﬁnite, but
not positive deﬁnite, operator is 0.
♥9.4.9.(a) Find the eigenfunctions and eigenvalues for the boundary value problem
−x2u′′ −xu′ = λ u,
u(1) = u(e) = 0.
(b) Under which inner product are the eigenfunctions orthogonal? Justify your answer by
direct computation.
(c) Write down the eigenfunction expansion of a function f(x) deﬁned for 1 ≤x ≤e.
(d) Find the Green’s function for
−x2u′′ −xu′ = f(x),
u(1) = u(e) = 0,
both in closed form and as a series in the eigenfunctions you found in part (a).
(e) Is your Green’s function symmetric? Discuss.
(f ) Prove the completeness of the eigenfunctions.
9.4.10. Discuss completeness of the eigenfunctions of the boundary value problem
−x2u′′ −2xu′ = λ u,
| u(0) | < ∞,
u(1) = 0.

9.5 A General Framework for Dynamics
385
9.4.11. Consider the eigenvalue problem −u′′ = λ u, u(0) = 0, u′(1) = 0. (a) Is the prob-
lem self-adjoint? positive deﬁnite? Which inner product are you referring to? (b) Find all
eigenvalues and eigenfunctions. (c) Write down the explicit formula for the eigenfunction
expansion of a function f(x) deﬁned on [0, 1]. (d) Find the Green’s function and use it to
prove completeness of the eigenfunctions.
♥9.4.12.(a) Find the eigenfunctions and eigenvalues for the Chebyshev boundary value problem
(x2 −1)u′′ + xu′ = λ u,
u(−1) = u(1) = 0.
Hint: Let x = cos θ. (b) Under what inner product are the eigenfunctions orthogonal?
Justify your answer by direct computation. (c) Find the Green’s function for
(x2 −1)u′′ + xu′ = f(x),
u(−1) = u(1) = 0,
both in closed form and as a series in the eigenfunctions you found in part (a).
(d) Discuss completeness of the eigenfunctions.
9.4.13. Consider the diﬀerential operator S[u] = −u′′ + u on the space of C2 functions u(x)
deﬁned for all x and subject to the boundary conditions
lim
x →∞u(x) =
lim
x →−∞u(x) = 0.
(a) Find the Green’s function G(x; ξ). (b) Compute its double L2 norm: ∥G ∥2. What
does this indicate about the completeness of the eigenfunctions of S? (c) Justify your con-
clusion in part (b) by determining the eigenfunctions.
9.4.14. Find all (real and complex) eigenvalues of the ﬁrst-derivative operator D = d/dx on
the interval [0, 1] subject to the single periodic boundary condition v(0) = v(1). Are the
corresponding eigenfunctions orthogonal? For which inner product?
♥9.4.15. Consider the Dirichlet boundary value problem
−Δu = h(x, y),
u(x, 0) = 0,
u(x, 1) = 0,
u(0, y) = 0,
u(1, y) = 0,
0 < x, y < 1,
for the Poisson equation on the unit square. (a) Find the eigenfunction series expansion
for the Green’s function of this problem.
(b) Does your series coincide with that derived
in Exercise 6.3.22? Explain any discrepancies.
(c) For the impulse points (ξ, η) = (.5, .5)
and (.7, .8), graph the result of summing the ﬁrst 9, 25, and 100 terms in your series, and
discuss what you observe in light of what you expect the Green’s function to look like.
9.4.16. Find the eigenfunction series expansion for the Green’s function of the following mixed
boundary value problems:
(a) −Δu = h(x, y),
u(x, 0) = 0,
u(x, 1) = 0,
ux(0, y) = 0,
ux(1, y) = 0,
0 < x, y < 1;
(b) −Δu = h(x, y),
u(x, 0) = 0,
uy(x, 1) = 0,
u(0, y) = 0,
ux(1, y) = 0,
0 < x, y < 1.
9.4.17. Find the eigenfunction series expansion for the Green’s function of the following Helm-
holtz boundary value problem:
−Δu + u = h(x, y),
u(x, 0) = u(x, π) = u(0, y) = u(π, y) = 0,
0 < x, y < π.
♦9.4.18. If the eigenvalues of a self-adjoint linear operator satisfy λn →∞as n →∞, explain
why each eigenspace is necessarily ﬁnite-dimensional.
9.4.19. True or false: If S: Rn →Rn is any linear function, then one can ﬁnd an inner product
on Rn that makes S self-adjoint.
9.5 A General Framework for Dynamics
In this ﬁnal section, we show how to use general eigenfunction expansions to analyze
three important classes of linear dynamical systems: parabolic diﬀusion equations such
as the heat equation, hyperbolic vibration equations such as the wave equation, and the
Schr¨odinger equation, a complex evolution equation that governs the dynamical processes

386
9 A General Framework for Linear Partial Diﬀerential Equations
of quantum mechanics. In all three cases we can, assuming completeness, write the general
solution to the initial-boundary value problem as a convergent eigenfunction series with
time-dependent coeﬃcients, and thereby establish several general properties governing their
dynamics.
Evolution Equations
In all cases, our starting point is the basic equilibrium equation, which is a linear system
of the form
S[u] = f,
(9.120)
where f represents an external forcing. The linear operator S is assumed to be of the usual
self-adjoint form
S = L∗◦L,
(9.121)
which is either positive deﬁnite, when ker L = {0}, or positive semi-deﬁnite, the latter
case being characterized by the existence of null eigenfunctions 0 ̸= v ∈ker L = ker S. In
ﬁnite dimensions, (9.120) represents a linear algebraic system consisting of n equations in n
unknowns with positive (semi-)deﬁnite coeﬃcient matrix. In inﬁnite-dimensional function
space, it represents a self-adjoint positive (semi-)deﬁnite boundary value problem for the
unknown function u.
With the equilibrium operator in hand, there are two principal classical dynamical
systems of importance as physical models. The ﬁrst are the (unforced) diﬀusion processes
modeled by an evolution equation of the form
∂u
∂t = −S[u] = −L∗◦L[u].
(9.122)
In the discrete case, this represents a ﬁrst-order system of ordinary diﬀerential equations,
known as a linear gradient ﬂow. In the continuous case, S is a linear diﬀerential operator
equipped with homogeneous boundary conditions, and (9.122) represents a linear partial
diﬀerential equation for the time-varying function u = u(t, x), the heat equation being
the prototypical example.
(As in the preceding section, the notation employed below
indicates that we are working in a single space dimension, but the methods and results
apply equally well to higher-dimensional problems.) The addition of external forcing to
the diﬀusion process is treated in Exercise 9.5.6.
The basic separation of variables solution technique was already outlined in Section 3.1.
To recap, the separable solutions are of exponential form
u(t, x) = e−λt v(x),
(9.123)
where v ∈U is a ﬁxed function. Since the operator S is linear and does not involve t
diﬀerentiation, we ﬁnd
∂u
∂t = −λ e−λt v,
while
S[u] = e−λt S[v].
Substituting back into (9.122) and canceling the common exponential factors, we are led
to the eigenvalue problem
S[v] = λ v.
(9.124)

9.5 A General Framework for Dynamics
387
Thus, (9.123) deﬁnes a solution if and only if v is an eigenfunction for the linear operator
S, with λ the corresponding eigenvalue.
We let vk(x), k = 1, 2, . . . , be the orthogonal eigenfunctions and 0 ≤λ1 ≤λ2 ≤
λ3 ≤· · · →∞the corresponding eigenvalues. Assuming completeness, the solution to
the initial value problem
u(0, x) = f(x)
(9.125)
can be expanded in terms of the eigensolutions:
u(t, x) =
∞

k=1
e−λkt ck vk(x),
where
ck = ⟨f , vk ⟩
∥vk ∥2
(9.126)
are the eigenfunction coeﬃcients of the initial data. In particular, the fundamental solution
of the diﬀusion equation is deﬁned as the solution u = F(t, x; ξ) to the initial value problem
u(0, x) = δξ(x)
(9.127)
induced by an initial delta impulse at the point ξ. Its eigenfunction coeﬃcients are
ck =
⟨δξ , vk ⟩
∥vk ∥2
=
1
∥vk ∥2
 b
a
δ(x −ξ) vk(x) ρ(x) dx = vk(ξ) ρ(ξ)
∥vk ∥2
.
Thus,
F(t, x; ξ) =
∞

k=1
e−λkt vk(x) vk(ξ) ρ(ξ)
∥vk ∥2
,
(9.128)
where the denominator denotes the appropriately weighted L2 norm of the eigenfunction:
∥vk ∥2 =
 b
a
vk(x)2 ρ(x) dx.
As with the one-dimensional heat equation, if the equilibrium operator is positive def-
inite, S > 0, then all the eigenvalues are strictly positive, and hence, generically, solutions
decay to 0 at the exponential rate prescribed by the smallest eigenvalue, which can be
characterized as the minimum value of the Rayleigh quotient. On the other hand, if S
is only positive semi-deﬁnite, then the solution will tend to a null eigenmode, that is, an
element of ker S = ker L, as its asymptotic equilibrium state. If dim ker S = p, the ﬁrst p
eigenvalues are all 0 = λ1 = · · · = λp < λp+1, and the solution
u(t, x) −→
p

k=1
ckvk(x)
as
t −→∞
will tend to its eventual equilibrium conﬁguration at an exponential rate determined by
the smallest positive eigenvalue λp+1 > 0. In almost all applications, p = 1 and there is a
single, constant null eigenfunction. The Neumann and periodic boundary value problems
for the heat equation are prototypical examples.

388
9 A General Framework for Linear Partial Diﬀerential Equations
Exercises
9.5.1. Find the eigenfunction series of the fundamental solution for the heat equation
ut = γ uxx on the interval 0 ≤x ≤1 subject to homogeneous Dirichlet boundary conditions.
9.5.2. Solve Exercise 9.5.1 for (a) the mixed boundary conditions u(t, 0) = ux(t, 1) = 0;
(b) homogeneous Neumann boundary conditions.
9.5.3. Let D[u] = u′ be the derivative operator acting on the vector space of C1 scalar func-
tions u(x) deﬁned for 0 ≤x ≤1 and satisfying the boundary conditions u(0) = u′(1) = 0.
(a) Given the L2 inner product on its domain space and the weighted inner product
⟨v , v ⟩=
	 1
0 v(x) v(x) x dx on its target space, determine the adjoint operator D∗.
(b) Let S = D∗◦D. Write out the diﬀusion equation ut = −S[u] explicitly, as a partial
diﬀerential equation plus boundary conditions.
(c) Given the initial condition u(0, x) = x −x2, what is the asymptotic equilibrium
u⋆(x) = lim
t →∞u(t, x) of the resulting solution to the diﬀusion equation?
9.5.4. Write down an eigenfunction series for the solution u(t, x) to the initial value problem
u(0, x) = f(x) for the fourth-order evolution equation ut = −uxxxx subject to the boundary
conditions u(t, 0) = uxx(t, 0) = u(t, 1) = uxx(t, 1) = 0. Does your solution tend to an
equilibrium state? If so, at what rate?
9.5.5. Answer Exercise 9.5.4 for the boundary conditions
ux(t, 0) = uxxx(t, 0) = ux(t, 1) = uxxx(t, 1) = 0.
♦9.5.6. Explain how to solve the forced diﬀusion equation ut = −S[u] + f, subject to homoge-
neous boundary conditions, when f(x) does not depend on time t. Does the solution tend
to equilibrium as t →∞? If so, what is the rate of decay, and what is the equilibrium?
9.5.7. Show that if u(t, x) solves the diﬀusion equation (9.122), then ∥u(t, · ) ∥≥∥u(s, · ) ∥
whenever t ≤s.
♦9.5.8. Let S > 0 be a positive deﬁnite operator. Suppose F(t, x; ξ) is the fundamental solu-
tion for the diﬀusion equation (9.122). Prove that G(x; ξ) =
	 ∞
0
F(t, x; ξ) dt is the Green’s
function for the corresponding equilibrium equation S[u] = f.
Vibration Equations
The second important class of dynamical systems comprises the second-order (in time)
vibration equations
∂2u
∂t2 = −S[u],
(9.129)
which we initially analyze in the absence of external forcing. Vibrational systems arise as
a consequence of Newton’s equations of motion in the absence of frictional forces. Their
continuum versions model the propagation of waves in solids and ﬂuids, electromagnetic
waves, plasma waves, and many other related physical systems.
For a general vibration equation, the separable solutions are of trigonometric form
u(t, x) = cos(ωt) v(x)
or
sin(ωt) v(x).
(9.130)

9.5 A General Framework for Dynamics
389
Substituting either ansatz back into (9.129) results in the same eigenvalue problem (9.124)
for v(x) with eigenvalue λ = ω2 equal to the square of the vibrational frequency.
We
conclude that the normal modes or eigensolutions take the form
uk(t, x) = cos(ωk t) vk(x),
uk(t, x) = sin(ωk t) vk(x),
provided λk = ω2
k > 0 is a nonzero eigenvalue and vk an associated eigenfunction. Thus, the
natural vibrational frequencies of the system are the square roots of the nonzero eigenvalues,
a fact that we already observed in the context of the one-dimensional wave equation.
In the positive deﬁnite case, the eigenvalues are all strictly positive, and so the general
solution is built up as a linear combination of vibrational eigenmodes:
u(t, x) =
∞

k=1

ckuk(t, x) + dkuk(t, x)

=
∞

k=1

ck cos(ωk t) + dk sin(ωk t)

vk(x) =
∞

k=1
rk cos(ωk t + δk) vk,
(9.131)
where
rk =
/
c2
k + d2
k ,
δk = tan−1 dk
ck
.
(9.132)
The initial conditions
g(x) = u(0, x) =
∞

k=1
ckvk(x),
h(x) = ut(0, x) =
∞

k=1
dk ωkvk(x),
(9.133)
are used to specify the coeﬃcients:
ck = ⟨g , vk ⟩
∥vk ∥2 ,
dk = ⟨h , vk ⟩
ωk ∥vk ∥2 .
(9.134)
In the unstable, positive semi-deﬁnite cases, any null eigenfunction v0 ∈ker S = ker L
contributes two aperiodic eigensolutions:
u0(t, x) = v0(x),
u0(t, x) = t v0(x),
as can be readily checked. The ﬁrst is constant in time, while the second is an unstable,
linearly growing mode, which is excited if and only if the initial velocity is not orthogonal
to the null eigenfunction: ⟨h , v0 ⟩̸= 0.
If, as occurred in the one-dimensional wave equation, the natural frequencies happen
to be integer multiples of a common frequency, ωk = nk ω⋆for nk ∈N, then the solution
(9.131) is a periodic function of t with period p⋆= 2π/ω⋆. On the other hand, in most
cases the frequencies are not rationally related, and the solution is only quasiperiodic.
Although it is the sum of individually periodic modes, it is not periodic, and never exactly
reproduces its initial behavior; see the illustrative Example 2.20 for additional details.
Forcing and Resonance
Periodically forcing an undamped mechanical structure, modeled by a vibrational system
of ordinary diﬀerential equations, at a frequency that is distinct from its natural vibrational
frequencies, leads, in general, to a quasiperiodic response. The solution is a sum of the

390
9 A General Framework for Linear Partial Diﬀerential Equations
unforced vibrational modes superimposed with an additional component that vibrates at
the forcing frequency. However, if forced at one of its natural frequencies, the system may
experience a catastrophic resonance. See [89; §9.6] for details.
The same type of quasiperiodic/resonant response is also observed in the partial dif-
ferential equations governing the vibrations of continuous media.
Consider the forced
vibrational equation
∂2u
∂t2 = −S[u] + F(t, x),
(9.135)
subject to speciﬁed homogeneous boundary conditions.
The external forcing function
F(t, x) may depend on both time t and position x.
We will be particularly interested
in a periodically varying external force of the form
F(t, x) = cos(ωt) h(x),
(9.136)
where ω is the forcing frequency, while the forcing proﬁle h(x) is unvarying.
As always, the solution to an inhomogeneous linear equation can be written as a
combination,
u(t, x) = u⋆(t, x) + z(t, x),
(9.137)
of a particular solution u⋆(t, x) to the inhomogeneous forced equation combined with the
general solution z(t, x) to the homogeneous equation, namely
∂2z
∂t2 = −S[z ].
(9.138)
The boundary and initial conditions will serve to uniquely prescribe the solution u(t, x),
but there is some ﬂexibility in its two constituents (9.137).
For instance, we may ask
that the particular solution u⋆satisfy the homogeneous boundary conditions along with
zero (homogeneous) initial conditions and thus represent the pure response of the system
to the forcing. The homogeneous solution z(t, x) will then reﬂect the eﬀect of the initial
and boundary conditions unadulterated by the external forcing. The ﬁnal solution is the
combined sum of the two individual responses.
In the case of periodic forcing (9.136), we look for a particular solution
u⋆(t, x) = cos(ωt) v⋆(x)
(9.139)
that vibrates at the forcing frequency.
Substituting the ansatz (9.139) into the equa-
tion (9.135) and canceling the common cosine factors, we discover that v⋆(x) must satisfy
the boundary value problem prescribed by a forced diﬀerential equation
S[v⋆] −ω2 v⋆= h(x),
(9.140)
supplemented by the relevant homogeneous boundary conditions: Dirichlet, Neumann,
mixed, or periodic.
At this juncture, there are two possibilities. If the unforced homogeneous boundary
value problem
S[v] −ω2 v = 0
(9.141)
has only the trivial solution v ≡0, then, according to the Fredholm Alternative Theo-
rem 9.10, a solution to the forced boundary value problem will exist† for any form of the
†
Existence is immediate in ﬁnite-dimensional systems. For boundary value problems, this
relies on an analytic existence theorem, e.g., Theorem 9.28.

9.5 A General Framework for Dynamics
391
forcing function h(x). In other words, if ω2 is not an eigenvalue, then the particular so-
lution (9.139) will vibrate with the forcing frequency, and the general solution will be a
periodic or quasiperiodic combination (9.137) of the natural vibrational modes along with
the vibrational response to the periodic forcing.
On the other hand, if ω2 = λk is an eigenvalue, and so ω = ωk coincides with one of the
natural vibrational frequencies of the homogeneous problem, then (9.141) admits nontrivial
solutions, namely the eigenfunction† vk(x). In such cases, the Fredholm Alternative tells
us that the boundary value problem (9.140) admits a solution if and only if the forcing
function is orthogonal to the eigenfunction:
⟨h , vk ⟩= 0.
(9.142)
If this holds, then the resulting particular solution (9.139) still vibrates with the forcing
frequency, and resonance doesn’t occur.
If we force in a resonant manner — meaning that the Fredholm condition (9.142) does
not hold — then the solution will be a resonantly growing vibration of the form
u⋆(t, x) = at sin(ωk t) vk(x) + cos(ωk t) v⋆(x),
(9.143)
in which a is a constant to be speciﬁed as follows. By direct calculation,
∂2u⋆
∂t2 + S[u⋆] = at sin(ωk t)

S[vk ] −ω2
k vk(x)

+ cos(ωk t)

S[v⋆] −ω2
k v⋆(x) + 2aωk vk(x)

.
The ﬁrst term vanishes, since vk(x) is an eigenfunction with eigenvalue λk = ω2
k. Therefore,
(9.143) satisﬁes the forced boundary value problem if and only if v⋆(x) satisﬁes the forced
boundary value problem
S[v⋆] −ω2
k v⋆(x) = h(x) −2aωk vk(x).
(9.144)
Again, the Fredholm Alternative implies that (9.144) admits a solution v⋆(x) if and only if
0 = ⟨h −2aωkvk , vk ⟩= ⟨h , vk ⟩−2a ∥vk ∥2,
and hence
a = ⟨h , vk ⟩
2 ∥vk ∥2 ,
(9.145)
which serves to ﬁx the value of the constant in the resonant solution ansatz (9.143).
In a real-world situation, such large resonant (or even near resonant) vibrations will, if
unchecked, eventually either leads to a catastrophic breakdown of the system or to a tran-
sition into the nonlinear regime.
Example 9.51. As a speciﬁc example, consider the initial-boundary value problem
modeling the forced vibrations of a uniform string of unit length that is ﬁxed at both ends:
utt = c2 uxx + cos(ωt) h(x),
u(t, 0) = 0 = u(t, 1),
u(0, x) = f(x),
ut(0, x) = g(x).
(9.146)
†
For simplicity, we assume that the eigenvalue λk is simple, and so there is a unique, up to
constant multiple, eigenfunction vk. Modiﬁcations for multiple eigenvalues proceed analogously.

392
9 A General Framework for Linear Partial Diﬀerential Equations
The particular solution u⋆(t, x) will have the nonresonant form (9.139), provided there
exists a solution v⋆(x) to the boundary value problem
S[v⋆] −ω2v⋆= −c2 v′′
⋆−ω2 v⋆= h(x),
v⋆(0) = 0 = v⋆(1).
(9.147)
The natural frequencies and associated eigenfunctions of the unforced Dirichlet boundary
value problem are
ωk = kcπ,
vk(x) = sin kπx,
k = 1, 2, 3, . . . .
Thus, the boundary value problem (9.147) will admit a solution, and hence the forcing is
not resonant, if either ω ̸= ωk is not a natural frequency or ω = ωk for some k but the
forcing proﬁle is orthogonal to the associated eigenfunction:
0 = ⟨h , vk ⟩=
 1
0
h(x) sin kπx dx.
(9.148)
Otherwise, the system will undergo a resonant response.
For example, under periodic forcing of frequency ω with trigonometric sine proﬁle
h(x) ≡sin kπx, for k a positive integer, the particular solution to (9.147) is
v⋆(x) =
sin kπx
ω2 −k2 π2 c2 ,
so that
u⋆(t, x) = cos ωt sin kπx
ω2 −k2 π2 c2
,
(9.149)
which is valid provided ω ̸= ωk = k πc. Observe that we may allow the forcing frequency
to coincide with any of the other natural frequencies, ω = ωn for n ̸= k, because the sine
proﬁles are mutually orthogonal, and so the nonresonance condition (9.148) holds. On the
other hand, if ω = ωk = kπc, then the particular solution
u⋆(t, x) = t sin kπct sin kπx
2kπc
(9.150)
is resonant and grows linearly in time.
To obtain the full solution to the initial-boundary value problem, we write u = u⋆+z,
where z(t, x) must satisfy
ztt −c2zxx = 0,
z(t, 0) = 0 = z(t, 1),
along with the modiﬁed initial conditions
z(0, x) = f(x) −
sin kπx
ω2 −k2 π2 c2 ,
∂z
∂t (0, x) = g(x),
stemming from the fact that the particular solution (9.149) has a nontrivial initial displace-
ment. (In the resonant case (9.150), there is no extra term in the initial data.) Note that
the closer ω is to the resonant frequency, the larger the modiﬁcation of the initial data,
and hence the larger the response of the system to the periodic forcing. As before, the
solution z(t, x) to the homogeneous equation can be written as a Fourier sine series (4.68).
The ﬁnal formulas are left for the reader to write out in detail; see Exercise 9.5.14.

9.5 A General Framework for Dynamics
393
Exercises
9.5.9. Which of the following forcing functions F(t, x) excites resonance in the wave equation
utt = uxx + F(t, x) when subject to homogeneous Dirichlet boundary conditions on the
interval 0 ≤x ≤1?
(a) sin 3t, (b) sin 3πt, (c) sin 3
2 πt, (d) sin πt sin πx,
(e) sin πt sin 2πx, (f ) sin 2πt cos πx, (g) x(1 −x) sin 2πt.
9.5.10. Answer Exercise 9.5.9 when the solution is subject to the mixed boundary conditions
u(t, 0) = ux(t, 1) = 0.
♥9.5.11. Let ω > 0. Find the solution to the initial-boundary value problem
utt = uxx + cos ω t,
u(t, 0) = 0 = u(t, 1),
u(0, x) = 0 = ut(0, x).
9.5.12. Answer Exercise 9.5.11 for homogeneous Neumann boundary conditions.
9.5.13. A piano wire of length 1 m and wave speed c = 2 m/sec can support a maximal de-
ﬂection of 5 cm before breaking. Suppose the wire starts at rest, with both ends ﬁxed, and
then is subject to a uniform periodic force F(t, x) =
1
10 cos ω t sin πx. What range of fre-
quencies will cause the wire to break?
♦9.5.14. Write out the eigenfunction series solution to the initial-boundary value problem in
Example 9.51 with h(x) ≡sin kπx.
9.5.15. How should the solution formulas (9.131, 134) be modiﬁed when there are unstable
modes? Write down explicit conditions on the initial data that prevent an instability from
being excited.
♦9.5.16. Explain how to convert the homogeneous wave equation with inhomogeneous Dirichlet
boundary conditions u(t, 0) = α(t), u(t, ℓ) = β(t), into a homogeneous boundary value
problem for the forced wave equation. Hint: Mimic (4.46).
♥9.5.17. Two children hold a jump rope taut, while one of them periodically shakes their end of
the rope. Use the inhomogeneous boundary value problem
∂2u
∂t2 = ∂2u
∂x2 ,
u(t, 0) = 0,
u(t, 1) = sin ω t,
to model the motion of the rope, adopting units in which the wave speed c = 1.
(a) What are the resonant frequencies of this system?
(b) Apply the method of Exercise 9.5.16 to ﬁnd a particular solution to the boundary value
problem when ω is a nonresonant frequency.
(c) Suppose the rope starts at rest. Find a series solution to the corresponding initial-
boundary value problem when ω is a nonresonant frequency.
(d) Answer parts (b,c) when ω is a resonant frequency. Hint: Use the ansatz (9.143).
9.5.18. Explain how to solve the periodically forced telegrapher’s equation
utt + aut = c2 uxx + h(x) cos ω t
on the interval 0 ≤x ≤1 when subject to homogeneous Dirichlet boundary conditions. At
which frequencies does the forcing function excite a resonant response?
Hint: First solve Exercise 4.2.9.
9.5.19. The fourth-order evolution equation utt = −c2uxxxx, subject to the boundary condi-
tions u(t, 0) = uxx(t, 0) = u(t, 1) = uxx(t, 1) = 0, models the transverse vibrations of a sim-
ply supported uniform thin elastic beam, in which c > 0 represents the wave speed. Write
down an eigenfunction series for the solution to the initial value problem u(0, x) = f(x),
ut(0, x) = 0. Is the solution (i) periodic, (ii) quasiperiodic, (iii) chaotic, (iv) none of the
above?

394
9 A General Framework for Linear Partial Diﬀerential Equations
The Schr¨odinger Equation
The fundamental dynamical system that governs all quantum-mechanical systems is known
as the Schr¨odinger equation, ﬁrst written down by the great twentieth-century German
physicist Erwin Schr¨odinger, one of the preeminent founders of modern quantum physics.
His original series of papers in which, by ﬁts and starts, he arrives at his fundamental
equation, makes for fascinating reading, [101].
Unlike classical mechanics, quantum mechanics is a completely linear theory, gov-
erned by linear systems of partial diﬀerential equations. The abstract form of the linear
Schr¨odinger equation is
i ℏ∂ψ
∂t = S[ψ ],
(9.151)
where S is a linear operator of the usual self-adjoint form (9.121).
In this equation,
i = √−1, while ℏis Planck’s constant (7.69). The operator S is known as the Hamil-
tonian for the quantum-mechanical system, and, typically, represents the quantum energy
operator. For physical systems such as atoms and nuclei, the relevant Hamiltonian op-
erator is constructed from the classical energy through the rather mysterious process of
“quantization”.
At each time t, the solution ψ(t, x) to the Schr¨odinger equation represents the wave
function of the quantum system, and so should be a complex-valued square-integrable
function having unit L2 norm: ∥ψ ∥= 1. (The reader may wish to revisit Sections 3.5
and 7.1 for a discussion of the basics of quantum mechanics and Hilbert space.)
We
interpret the wave function as a probability density on the possible quantum states, and so
the Schr¨odinger equation governs the dynamical evolution of quantum probabilities. The
interested reader should consult a basic text on quantum mechanics, e.g., [66, 72, 115],
for full details on both the physics and underlying mathematics.
Proposition 9.52. If ψ(t, x) is a solution to the Schr¨odinger equation, its Hermitian
L2 norm ∥ψ(t, ·) ∥is ﬁxed for all time.
Proof : Since the solution is complex-valued, we use the sesquilinearity of the under-
lying Hermitian inner product, as in (B.19), to compute
d
dt ∥ψ(t, ·) ∥2 =
4 ∂ψ
∂t , ψ
5
+
4
ψ , ∂ψ
∂t
5
=
4
−i
ℏS[ψ ] , ψ
5
+
4
ψ , −i
ℏS[ψ ]
5
= −i
ℏ⟨S[ψ ] , ψ ⟩+ i
ℏ⟨ψ , S[ψ ] ⟩= 0,
which vanishes because S is self-adjoint. This implies that ∥ψ(t, ·) ∥2 is constant. Q.E.D.
As a result, if the initial data ψ(t0, x) = ψ0(x) is a quantum-mechanical wave function,
meaning that ∥ψ0 ∥= 1, then, at each time t, the solution ψ(t, x) to the Schr¨odinger
equation also has norm 1, and hence remains a wave function throughout the evolutionary
process.
Apart from the extra factor of i ℏ, the Schr¨odinger equation looks like a diﬀusion
equation (9.122). This inspires us to seek separable solutions with an exponential ansatz:
ψ(t, x) = eα t v(x).

9.5 A General Framework for Dynamics
395
Substituting this expression into the Schr¨odinger equation (9.151) and canceling the com-
mon exponential factors reduces us to the usual eigenvalue problem
S[v] = λ v,
with eigenvalue
λ = i ℏα.
By self-adjointness, the eigenvalues are necessarily real.
Let vk denote the normalized
eigenfunction, so ∥vk ∥= 1, associated with the kth eigenvalue λk. The corresponding
eigensolution of the Schr¨odinger equation is the complex-valued function
ψk(t, x) = e−i λkt/ℏvk(x).
Observe that, in contrast to the exponentially decaying solutions to the diﬀusion equation,
the eigensolutions to the Schr¨odinger equation are periodic, with vibrational frequencies
ωk = −λk/ℏproportional to the eigenvalues. (Along with constant solutions corresponding
to the null eigenmodes, if any.)
The general solution is a (quasi)periodic series in the
fundamental eigensolutions,
ψ(t, x) =

k
ckψk(t, x) =

k
ck e−i λkt/ℏvk(x),
(9.152)
whose coeﬃcients are prescribed by the initial conditions. The periodicity of the summands
has the additional implication that, again unlike the diﬀusion equation, the Schr¨odinger
equation can be run backwards in time, i.e., it remains well-posed in the past. Consequently,
we can determine both the past and future behavior of a quantum system from its present
conﬁguration.
The eigenvalues represent the energy levels of the system described by the Schr¨odinger
equation and can be experimentally detected by exciting the system. For instance, when
an excited electron orbiting a nucleus jumps back to a lower energy level, it emits a photon
whose observed electromagnetic spectral line corresponds to the diﬀerence between the
energies of the two quantum levels. This motivates the use of the term spectrum to describe
the eigenvalues of a linear Hamiltonian operator.
Example 9.53.
The simplest version of the Schr¨odinger equation is based on the
derivative operator L = D, leading to the self-adjoint combination S = L∗◦L = −D2
when subject to appropriate boundary conditions. In this case, the Schr¨odinger equation
(9.151) reduces to the second-order partial diﬀerential equation
i ℏ∂ψ
∂t = −∂2ψ
∂x2 .
(9.153)
If we impose the Dirichlet boundary conditions ψ(t, 0) = ψ(t, ℓ) = 0, then the Schr¨odinger
equation (9.153) governs the dynamics of a quantum particle that is conﬁned to the interval
0 < x < ℓ; the boundary conditions imply that there is zero probability of the particle
escaping from the interval.
According to Section 4.1, the eigenfunctions of the Dirichlet eigenvalue problem
v′′ + λv = 0,
v(0) = v(ℓ) = 0,
are
vk(x) =

2
ℓsin kπ
ℓx,
with eigenvalue
λk = k2π2
ℓ2 ,
for
k = 1, 2, . . . ,

396
9 A General Framework for Linear Partial Diﬀerential Equations
where the initial factor ensures that vk has unit L2 norm, and hence is a bona ﬁde wave
function. The corresponding oscillatory eigenmodes are
ψk(t, x) =

2
ℓexp

−i k2π2
ℏℓ2 t

sin kπ
ℓx.
(9.154)
Since the temporal frequencies ωk = −k2π2/(ℏℓ2) depend nonlinearly on the wave number
kπ/ℓ, the Schr¨odinger equation, is, in fact, dispersive, sharing many similarities with the
third-order linear equation (8.90); see, for instance, Exercises 9.5.25, 27.
Exercises
9.5.20.(a) Solve the following initial boundary value problem:
i ℏψt = −ψxx,
ψ(t, 0) = ψ(t, 1) = 0,
ψ(0, x) = 1.
(b) Using your solution formula, verify that ∥ψ(t, · ) ∥= 1 for all t.
9.5.21. Answer Exercise 9.5.20 for the initial condition ψ(0, x) =
√
30 x(1 −x).
9.5.22. Answer Exercise 9.5.20 when the solution is subject to Neumann boundary conditions
ψx(t, 0) = ψx(t, 1) = 0.
9.5.23. Write down the eigenseries solution for the Schr¨odinger equation on a bounded interval
[0, ℓ] when subject to homogeneous Neumann boundary conditions.
9.5.24. Given the solution formula (9.152), and assuming completeness of the eigenfunctions,
prove that ∥ψ(t, · ) ∥2 =

k
| ck |2 for all t.
♦9.5.25.
Write down the dispersion relation, phase velocity, and group velocity for the one-dimensional
Schr¨odinger equation (9.153).
9.5.26. Show that the real and imaginary parts of the solution ψ(t, x) = u(t, x) + i v(t, x) to the
one-dimensional Schr¨odinger equation (9.153) are solutions to the beam equation of Exer-
cise 9.5.19. What is the wave speed?
♦9.5.27. The Talbot eﬀect for the linear Schro¨dinger equation: Let u(t, x) solve the periodic initial-
boundary value problem
i ut = uxx,
u(t, −π) = u(t, π),
ux(t, −π) = ux(t, π),
with initial data u(0, x) = σ(x) given by the unit step function. Prove that when t = π p/q,
where p, q are integers, the solution u(t, x) is constant on each interval ℏπ j/q < x <
ℏπ(j + 1)/q for integers j ∈Z. Hint: Use Exercise 6.1.29(d).
9.5.28. The wave function ψ(t, x) of a one-dimensional free quantum particle of mass m satis-
ﬁes the Schr¨odinger equation i ψt = −ℏψxx/(2m) on the real line −∞< x < ∞. Assum-
ing that ψ and its x derivatives decay reasonably rapidly to zero as | x | →∞, prove that
the particle’s expected position ⟨x⟩=
	 ∞
−∞x | ψ(t, x) |2 dx moves on a straight line.
Hint: Prove that d2⟨x⟩
dt2
= 0.

9.5 A General Framework for Dynamics
397
♥9.5.29. Consider the periodically forced Schr¨odinger equation i ℏψt = −ψxx + e i ω t on the
interval 0 ≤x ≤1, subject to homogeneous Dirichlet boundary conditions. (a) At which
frequencies ω does the forcing function excite a resonant response? (b) Find the solution
to the general initial value problem for a nonresonant forcing frequency. (c) Find the so-
lution to the general initial value problem for a resonant forcing frequency. What are the
conditions on ℏthat ensure that the resulting solution remains a wave function?
♥9.5.30. The Schr¨odinger equation for the harmonic oscillator is i ℏψt = ψxx −x2ψ. Write this
equation in the self-adjoint form (9.151) under a suitable choice of boundary conditions.
Write down the self-adjoint boundary value problem for the eigenfunctions. Remark: The
eigenfunctions are not elementary functions. After studying Section 11.3, you may wish re-
turn here to investigate its solutions.

Chapter 10
Finite Elements and Weak Solutions
In Chapter 5, we studied the oldest, and in many ways the simplest, class of numerical
algorithms for approximating the solutions to partial diﬀerential equations: those based on
ﬁnite diﬀerence approximations. In the present chapter, we introduce the second of the two
major numerical paradigms: the ﬁnite element method. Finite elements are of more recent
vintage, having ﬁrst appeared soon after the Second World War; historical details can be
found in [113]. As a consequence of their ability to adapt to complicated geometries, ﬁnite
elements have, in many situations, become the method of choice for solving equilibrium
boundary value problems governed by elliptic partial diﬀerential equations. Finite elements
can also be adapted to dynamical problems, but lack of space prevents us from pursuing
such extensions in this text.
Finite elements rely on a more sophisticated understanding of the partial diﬀeren-
tial equation, in that, unlike ﬁnite diﬀerences, they are not obtained by simply replacing
derivatives by their numerical approximations. Rather, they are initially founded on an as-
sociated minimization principle that, as we learned in Chapter 9, characterizes the unique
solution to a positive deﬁnite boundary value problem. The basic idea is to restrict the
minimizing functional to an appropriately chosen ﬁnite-dimensional subspace of functions.
Such a restriction produces a ﬁnite-dimensional minimization problem, which can then
be solved by numerical linear algebra. When properly formulated, the restricted ﬁnite-
dimensional minimization problem will have a solution that well approximates the true
minimizer, and hence the solution to the original boundary value problem. To gain fa-
miliarity with the underlying principles, we will ﬁrst illustrate the basic constructions in
the context of boundary value problems for ordinary diﬀerential equations. The following
section extends ﬁnite element analysis to boundary value problems associated with the
two-dimensional Laplace and Poisson equations, thereby revealing the key features used
in applications to the numerical solution of multidimensional equilibrium boundary value
problems.
An alternative approach to the ﬁnite element method, one that can be applied even in
situations in which no minimum principle is available, is founded on the concept of a weak
solution to the diﬀerential equation, a construction of independent analytical importance.
The term “weak” refers to the fact that one is able to relax the diﬀerentiability requirements
imposed on classical solutions. Indeed, as we will show, discontinuous shock wave solutions
as well as the nonsmooth, and hence nonclassical, solutions to the wave equation that we
encountered in Chapters 2 and 4 can all be rigorously characterized through the weak
solution formulation. For the ﬁnite element approximation, rather than impose the weak
solution criterion on the entire inﬁnite-dimensional function space, one again restricts to a
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
10
399
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

400
10 Finite Elements and Weak Solutions
suitably chosen ﬁnite-dimensional subspace. For positive deﬁnite boundary value problems,
which necessarily admit a minimization principle, the weak solution approach leads to the
same ﬁnite element equations.
A rigorous justiﬁcation and proof of convergence of the ﬁnite element approximations
requires further analysis, and we refer the interested reader to more specialized texts,
such as [6, 113, 126]. In this chapter, we shall focus our eﬀort on understanding how to
formulate and implement the ﬁnite element method in practical contexts.
10.1 Minimization and Finite Elements
To explain the principal ideas underpinning the ﬁnite element method, we return to the
abstract framework for boundary value problems that was developed in Chapter 9. Recall
Theorem 9.26, which characterizes the unique solution to a positive deﬁnite linear system
as the minimizer, u⋆∈U, of an associated quadratic functional Q: U →R. For boundary
value problems governed by diﬀerential equations, U is an inﬁnite-dimensional function
space containing all suﬃciently smooth functions that satisfy the prescribed homogeneous
boundary conditions. (Modiﬁcations to deal with inhomogeneous boundary conditions will
be discussed in due course.)
This framework sets the stage for the ﬁrst key idea of the ﬁnite element method. In-
stead of trying to minimize the functional Q[u] over the entire inﬁnite-dimensional function
space, we will seek to minimize it over a ﬁnite-dimensional subspace W ⊂U. The eﬀect
is to reduce a problem in analysis — a boundary value problem for a diﬀerential equation
— to a problem in linear algebra, and hence one that a computer is capable of solving.
On the surface, the idea seems crazy: how could one expect to come close to ﬁnding the
minimizer in a gigantic inﬁnite-dimensional function space by restricting the search to a
mere ﬁnite-dimensional subspace? But this is where the magic of inﬁnite dimensions comes
into play. One can, in fact, approximate all (reasonable) functions arbitrarily closely by
functions belonging to ﬁnite-dimensional subspaces. Indeed, you are already familiar with
two examples: Fourier series, where one approximates rather general periodic functions by
trigonometric polynomials, and interpolation theory, in which one approximates functions
by ordinary polynomials, or, more sophisticatedly, by splines, [89, 102]. Thus, the ﬁnite
element idea perhaps is not as outlandish as it might initially seem.
To be a bit more explicit, let us begin with a linear operator L: U →V between real
inner product spaces, where, as in Section 9.1, ⟨u , u ⟩is used to denote the inner product
in U, and ⟨⟨v , v ⟩⟩the inner product in V . To ensure uniqueness of solutions, we always
assume that L has trivial kernel: ker L = {0}. According to Theorem 9.26, the element
u⋆∈U that minimizes the quadratic function(al)
Q[u] = 1
2 |∥L[u] ∥|2 −⟨f , u ⟩,
(10.1)
where |∥· ∥| denotes the norm in V , is the solution to the linear system
S[u] = f,
where
S = L∗◦L,
(10.2)
with L∗: V →U denoting the adjoint operator. The hypothesis that L has trivial kernel
implies that S is a self-adjoint positive deﬁnite linear operator, which implies that the
solution to (10.2), and hence the minimizer of Q[u], is unique. In our applications, L
is a linear diﬀerential operator between function spaces, e.g., the gradient, while Q[u]
represents a quadratic functional, e.g., the Dirichlet principle, and the associated linear

10.1 Minimization and Finite Elements
401
system (10.2) forms a positive deﬁnite boundary value problem, e.g., the Poisson equation
along with suitable boundary conditions.
To form a ﬁnite element approximation to the solution u⋆∈U, rather than try to
minimize Q[u] on the entire function space U, we now seek to minimize it on a suitably
chosen ﬁnite-dimensional subspace W ⊂U. We will specify W by selecting a set of linearly
independent functions ϕ1, . . . , ϕn ∈U, and letting W be their span. Thus, ϕ1, . . . , ϕn form
a basis of W, whereby dim W = n, and the general element of W is a (uniquely determined)
linear combination
w(x) = c1ϕ1(x) + · · · + cnϕn(x)
(10.3)
of the basis functions. Our goal is to minimize Q[w] over all possible w ∈W; in other
words, we need to determine the coeﬃcients c1, . . . , cn ∈R such that
Q[w] = Q[c1ϕ1 + · · · + cnϕn ]
(10.4)
is as small as possible. Substituting (10.3) back into (10.1) and then expanding, using
the linearity of L and then the bilinearity of the inner product, we ﬁnd that the resulting
expression is the quadratic function
P(c) = 1
2
n

i,j =1
kij ci cj −
n

i=1
bi ci = 1
2 cTK c −cT b,
(10.5)
in which
•
c = ( c1, c2, . . . , cn )T ∈Rn is the vector of unknown coeﬃcients in (10.3);
•
K = (kij) is the symmetric n × n matrix with entries
kij = ⟨⟨L[ϕi ] , L[ϕj ] ⟩⟩,
i, j = 1, . . . , n;
(10.6)
•
b = ( b1, b2, . . . , bn )T is the vector with entries
bi = ⟨f , ϕi ⟩,
i = 1, . . ., n.
(10.7)
Note that formula (10.6) uses the inner product on the target space V , whereas (10.7)
relies on the inner product on the domain space U.
Thus, once we specify the basis functions ϕi, the coeﬃcients kij and bi are all known
quantities.
We have eﬀectively reduced our original problem to the ﬁnite-dimensional
problem of minimizing the quadratic function (10.5) over all possible vectors c ∈Rn. The
symmetric matrix K is, in fact, positive deﬁnite, since, by the preceding computation,
cTK c =
n

i,j =1
kij ci cj = |∥L[c1 ϕ1(x) + · · · + cn ϕn ] ∥|2 = |∥L[w] ∥|2 > 0,
(10.8)
as long as L[w] ̸= 0.
Moreover, our initial assumption tells us that L[w] = 0 if and
only if w = 0, which, by linear independence, occurs only when c = 0. Thus, (10.8) is
indeed positive for all c ̸= 0. We can now invoke the ﬁnite-dimensional minimization result
contained in Example 9.25 to conclude that the unique minimizer to (10.5) is obtained by
solving the associated linear system
K c = b,
whereby
c = K−1b.
(10.9)

402
10 Finite Elements and Weak Solutions
Remark: When of moderate size, the linear system (10.9) can be solved by basic
Gaussian Elimination. When the size (i.e., the dimension, n, of the subspace W) becomes
too large, as is often the case in dealing with partial diﬀerential equations, it is better to
rely on an iterative linear system solver, e.g., Gauss–Seidel or Successive Over–Relaxation
(SOR); see [89, 118] for details.
This summarizes the basic abstract setting for the ﬁnite element method. The key
issue, then, is how to eﬀectively choose the ﬁnite-dimensional subspace W. ndTwo candi-
dates that might spring to mind are the space of polynomials of degree ≤n and the space
of trigonometric polynomials (truncated Fourier series) of degree ≤n. However, for a va-
riety of reasons, neither is well suited to the ﬁnite element method. One constraint is that
the functions in W must satisfy the relevant boundary conditions — otherwise, W would
not be a subspace of U. More importantly, in order to obtain suﬃcient accuracy of the
approximate solution, the linear algebraic system (10.9) will typically — especially when
dealing with partial diﬀerential equations — be quite large, and hence it is desirable that
the coeﬃcient matrix K be as sparse as possible, i.e., have lots of zero entries. Otherwise,
computing the solution may well be too time-consuming to be of much practical value.
With this in mind, the second innovative contribution of the ﬁnite element method is to
ﬁrst (paradoxically) enlarge the space U of allowable functions upon which to minimize the
quadratic functional Q[u]. The governing diﬀerential equation requires its (classical) solu-
tions to have a certain degree of smoothness, whereas the associated minimization principle
typically requires that they possess only half as many derivatives. Thus, for second-order
boundary value problems, the diﬀerential equation requires continuous second-order deriva-
tives, while the quadratic functional Q[u] involves only ﬁrst-order derivatives. It fact, it
can be rigorously shown that, under rather mild hypotheses, the functional retains the
same minimizing solution, even when one allows functions that fail to qualify as classical
solutions to the diﬀerential equation. We will proceed to develop the method in the context
of particular, fairly elementary examples.
Exercises
10.1.1. Let U = { u(x) ∈C2[0, π ] | u(0) = u(π) = 0 } and V = {v(x) ∈C1[0, π ]} both
be equipped with the L2 inner product. Let L: U →V be given by L[u] = D[u] = u′,
and f(x) = x −1. (a) Write out the quadratic functional Q[u] given by (10.1). (b) Write
out the associated boundary value problem (10.2). (c) Find the function u⋆(x) ∈U that
minimizes Q[u]. What is the value of Q[u⋆]? (d) Let W ⊂U be the subspace spanned
by sin x and sin 2x. Write out the corresponding ﬁnite-dimensional minimization problem
(10.8). (e) Find the function w⋆(x) ∈W that minimizes Q[w]. Is Q[w⋆] ≥Q[u⋆]? If not,
why not? How close is your ﬁnite element minimizer w⋆(x) to the actual minimizer u⋆(x)?
10.1.2. Let U = { u(x) ∈C2[0, 1] | u(0) = u(1) = 0 } and V = {v(x) ∈C1[0, 1]} both have
the L2 inner product. Let L: U →V be given by L[u] = u′(x) −u(x), and f(x) = 1
for all x. (a) Write out the quadratic functional Q[u] given by (10.1). (b) Write out the
associated boundary value problem (10.2). (c) Find the function u⋆(x) ∈U that minimizes
Q[u]. What is the value of Q[u⋆]? (d) Let W ⊂U be the subspace containing all cubic
polynomials p(x) that satisfy the boundary conditions: p(0) = p(1) = 0. Find a basis of
W and then write out the corresponding ﬁnite-dimensional minimization problem (10.8).
(e) Find the polynomial p⋆(x) ∈W that minimizes Q[p] for p ∈W. Is Q[p⋆] ≥Q[u⋆]? If
not, why not? How close is your ﬁnite element minimizer p⋆(x) to the minimizer u⋆(x)?

10.2 Finite Elements for Ordinary Diﬀerential Equations
403
10.1.3. Let U = { u(x) ∈C2[1, 2] | u(1) = u(2) = 0 }, V = { (v1(x), v2(x))T | v1, v2 ∈C1[1, 2] },
both be endowed with the L2 inner product. Let L: U →V be given by L[u] =
 xu′(x)
√
2 u(x)

,
and let f(x) = 2 for all 1 ≤x ≤2. (a) Write out the quadratic functional Q[u] given by
(10.1). (b) Write out the associated boundary value problem (10.2). (c) Find the func-
tion u⋆(x) ∈U that minimizes Q[u]. What is the value of Q[u⋆]? (d) Let W ⊂U be
the subspace containing all cubic polynomials p(x) that satisfy the boundary conditions
p(1) = p(2) = 0. Find a basis of W and then write out the corresponding ﬁnite-dimensional
minimization problem (10.8). (e) Find the polynomial p⋆(x) ∈W that minimizes Q[p] for
p ∈W. Is Q[p⋆] ≥Q[u⋆]? If not, why not? How close is your ﬁnite element minimizer
p⋆(x) to the actual minimizer u⋆(x)?
♥10.1.4.(a) Find the solution to the boundary value problem −u′′ = x2 −x, u(−1) = u(1) = 0.
(b) Write down a quadratic functional Q[u] that is minimized by your solution.
(c) Let W be the subspace spanned by the two functions (1−x2), x(1−x2). Find the func-
tion w⋆(x) ∈W that minimizes the restriction of your quadratic functional to W. Compare
w⋆with your solution from part (a). (d) Answer part (c) for the subspace W spanned by
sin π x, sin 2π x. Which of the two approximations is the better?
♥10.1.5.(a) Find the function u⋆(x) that minimizes Q[u] =
	 1
0
 1
2 (x + 1)u′(x)2 −u(x)
 
dx over
the vector space U consisting of C2 functions satisfying u(0) = u(1) = 0. (b) Let W3 ⊂U
be the subspace consisting of all cubic polynomials w(x) that satisfy the same boundary
conditions. Find the function w⋆(x) that minimizes the restriction Q[w] for w ∈W3.
Compare w⋆(x) and u⋆(x): how close are they in the L2 norm? What is the maximal dis-
crepancy | w⋆(x) −u⋆(x) | for 0 ≤x ≤1? (c) Suppose you enlarge your ﬁnite-dimensional
subspace W4 ⊂U to contain all quartic polynomials that satisfy the boundary conditions.
Is your new ﬁnite element approximation better? Discuss.
♥10.1.6.(a) Find the function u⋆(x) that minimizes Q[u] =
	 1
0
 1
2 ex u′(x)2 −3u(x)
 
dx over the
space U consisting of C2 functions satisfying the boundary conditions u(0) = u′(1) = 0.
(b) Let W ⊂U be the subspace containing all cubic polynomials w(x) that satisfy the
boundary conditions. Find the polynomial w⋆(x) that minimizes the restriction Q[w] for
w ∈W. Compare w⋆(x) and u⋆(x): how close are they in the L2 norm? What is the maxi-
mal discrepancy | w⋆(x) −u⋆(x) | for 0 ≤x ≤1?
10.1.7. Consider the Dirichlet boundary value problem
−Δu = x(1 −x) + y (1 −y),
u(x, 0) = u(x, 1) = u(0, y) = u(1, y) = 0,
on the unit square {0 < x, y < 1}.
(a) Find the exact solution u⋆(x, y). Hint: It is a polynomial.
(b) Write down a minimization principle Q[u] that characterizes the solution. Be careful to
specify the function space U over which the minimization takes place.
(c) Let W ⊂U be the subspace spanned by the four functions sin π x sin π y, sin 2π x sin π y,
sin π x sin 2π y, and sin 2π x sin 2π y. Find the function w⋆∈W that minimizes the re-
striction of Q[w] to w ∈W. How close is w⋆to the solution you found in part (a)?
♦10.1.8. Justify the identiﬁcation of (10.4) with the quadratic function (10.5).
10.2 Finite Elements for Ordinary Diﬀerential Equations
To understand the preceding abstract formulation in concrete terms, let us focus our atten-
tion on boundary value problems governed by a second-order ordinary diﬀerential equation.
For example, we might be interested in solving a Sturm–Liouville problem (9.71) subject

404
10 Finite Elements and Weak Solutions
Figure 10.1.
A continuous piecewise aﬃne function.
to, say, homogeneous Dirichlet boundary conditions. Once we understand how the ﬁnite
element constructions work in this relatively simple context, we will be in a good position
to extend the techniques to much more general linear boundary value problems governed
by elliptic partial diﬀerential equations.
For such one-dimensional boundary value problems, a popular and eﬀective choice
of the ﬁnite-dimensional subspace W is to employ continuous, piecewise aﬃne functions.
Recall that a function is aﬃne if its graph is a straight line: f(x) = ax + b. (The function
is linear, in accordance with Deﬁnition B.32, if and only if b = 0.) A function is called
piecewise aﬃne if its graph consists of a ﬁnite number of straight line segments; a typical
example is plotted in Figure 10.1. Continuity requires that the individual segments be
connected together end to end.
Given a boundary value problem on a bounded interval [a, b], let us ﬁx a ﬁnite collec-
tion of nodes
a = x0 < x1 < x2 < · · · < xn−1 < xn = b.
The formulas simplify if one uses equally spaced nodes, but this is not necessary for the
construction to be carried out. Let W denote the vector space consisting of all continu-
ous functions w(x) that are deﬁned on the interval a ≤x ≤b, satisfy the homogeneous
boundary conditions, and are aﬃne when restricted to each subinterval [xj, xj+1]. On each
subinterval, we write
w(x) = cj + bj(x −xj),
for
xj ≤x ≤xj+1,
j = 0, . . ., n −1,
for certain constants cj, bj. Continuity of w(x) requires
cj = w(x+
j ) = w(x−
j ) = cj−1 + bj−1 hj−1,
j = 1, . . ., n −1,
(10.10)
where hj−1 = xj −xj−1 denotes the length of the jth subinterval.
The homogeneous
Dirichlet boundary conditions at the endpoints require
w(a) = c0 = 0,
w(b) = cn−1 + bn−1 hn−1 = 0.
(10.11)
Observe that the function w(x) involves a total of 2n unspeciﬁed coeﬃcients c0, . . . , cn−1,
b0, . . . , bn−1. The continuity conditions (10.10) and the second boundary condition (10.11)
uniquely determine the bj. The ﬁrst boundary condition speciﬁes c0, while the remaining
n −1 coeﬃcients c1 = w(x1), . . . , cn−1 = w(xn−1) are arbitrary, specifying the values of
w(x) at the interior nodes. We conclude that the ﬁnite element subspace W has dimension
n −1, the number of interior nodes.

10.2 Finite Elements for Ordinary Diﬀerential Equations
405
1
2
3
4
5
6
7
1
Figure 10.2.
A hat function.
Remark: Every function w(x) in our subspace has piecewise constant ﬁrst derivative
w′(x). However, the jump discontinuities in w′(x) imply that its second derivative w′′(x)
may well include delta function impulses at the nodes, and hence w(x) is far from being a
solution to the diﬀerential equation. Nevertheless, in practice, the ﬁnite element minimizer
w⋆(x) ∈W will (under suitable assumptions) provide a reasonable approximation to the
actual solution u⋆(x).
The most convenient basis for W consists of the hat functions, which are continuous,
piecewise aﬃne functions satisfying
ϕj(xk) =
 1,
j = k,
0,
j ̸= k,
for
j = 1, . . . , n −1,
k = 0, . . . , n.
(10.12)
The graph of a typical hat function appears in Figure 10.2. The explicit formula is easily
established:
ϕj(x) =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
x −xj−1
xj −xj−1
,
xj−1 ≤x ≤xj,
xj+1 −x
xj+1 −xj
,
xj ≤x ≤xj+1,
0,
x ≤xj−1 or x ≥xj+1,
j = 1, . . . , n −1.
(10.13)
One advantage of using these basis functions is that, thanks to (10.12), the coeﬃcients in
the linear combination
w(x) = c1ϕ1(x) + · · · + cnϕn(x)
coincide with its values at the nodes:
cj = w(xj),
j = 1, . . . , n.
(10.14)
Example 10.1. Let κ(x) > 0 for 0 ≤x ≤ℓ. Consider the equilibrium equations
S[u] = −d
dx

κ(x) du
dx

= f(x),
0 < x < ℓ,
u(0) = u(ℓ) = 0,
for a nonuniform bar with ﬁxed ends and variable stiﬀness κ(x), that is subject to an
external forcing f(x).
In order to ﬁnd a ﬁnite element approximation to the resulting

406
10 Finite Elements and Weak Solutions
displacement u(x), we begin with the minimization principle based on the quadratic func-
tional
Q[u] =
 ℓ
0
 1
2 κ(x)u′(x)2 −f(x)u(x)

dx,
which is a special case of (9.75). We divide the interval [0, ℓ] into n equal subintervals,
each of length h = ℓ/n. The resulting uniform mesh has nodes
xj = j h = j ℓ
n ,
j = 0, . . . , n.
The corresponding ﬁnite element basis hat functions are explicitly given by
ϕj(x) =
⎧
⎨
⎩
(x −xj−1)/h,
xj−1 ≤x ≤xj,
(xj+1 −x)/h,
xj ≤x ≤xj+1,
0,
otherwise,
j = 1, . . ., n −1.
(10.15)
The associated linear system (10.9) has coeﬃcient matrix entries
kij = ⟨⟨ϕ ′
i , ϕ ′
j ⟩⟩=
 ℓ
0
ϕ ′
i(x)ϕ ′
j(x)κ(x) dx,
i, j = 1, . . ., n −1.
Since the function ϕi(x) vanishes except on the interval xi−1 < x < xi+1, while ϕj(x)
vanishes outside xj−1 < x < xj+1, the integral will vanish unless i = j or i = j ± 1.
Moreover,
ϕ ′
i(x) =
⎧
⎨
⎩
1/h,
xj−1 ≤x ≤xj,
−1/h,
xj ≤x ≤xj+1,
0,
otherwise,
j = 1, . . . , n −1.
Therefore, the ﬁnite element coeﬃcient matrix assumes the tridiagonal form
K = 1
h2
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
s0 + s1
−s1
−s1
s1 + s2
−s2
−s2
s2 + s3
−s3
...
...
...
−sn−3
sn−3 + sn−2
−sn−2
−sn−2
sn−2 + sn−1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(10.16)
where
sj =
 xj+1
xj
κ(x) dx
(10.17)
is the total stiﬀness of the jth subinterval. The corresponding right-hand side has entries
bj = ⟨f , ϕj ⟩=
 ℓ
0
f(x)ϕj(x) dx
= 1
h
$  xj
xj−1
(x −xj−1)f(x) dx +
 xj+1
xj
(xj+1 −x)f(x) dx
%
.
(10.18)
In practice, we do not have to explicitly evaluate the integrals (10.17, 18), but may replace
them by suitably close numerical approximations. When the step size h ≪1 is small, then

10.2 Finite Elements for Ordinary Diﬀerential Equations
407
the integrals are taken over small intervals, and so the elementary trapezoid rule, [24, 108],
produces suﬃciently accurate approximations:
sj ≈h
2 [κ(xj) + κ(xj+1)],
bj ≈hf(xj).
(10.19)
The resulting ﬁnite element system K c = b is then solved for c, whose entries, according
to (10.14), coincide with the values of the ﬁnite element approximation to the solution at
the nodes: cj = w(xj) ≈u(xj). Indeed, the tridiagonal Gaussian Elimination algorithm,
[89], will rapidly produce the desired solution. Since the accuracy of the ﬁnite element
solution increases with the number of nodes, this numerical scheme allows us to easily
compute very accurate approximations to the solution to the boundary value problem.
In particular, in the homogeneous case κ(x) ≡1, the coeﬃcient matrix (10.16) reduces
to the special form
K = 1
h
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
2
−1
−1
2
−1
−1
2
−1
...
... ...
−1
2
−1
−1
2
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(10.20)
In this case, the jth equation in the ﬁnite element linear system is, upon dividing by h,
−
cj+1 −2cj + cj−1
h2
= f(xj).
(10.21)
Since cj ≈u(xj), the left-hand side coincides with the standard ﬁnite diﬀerence approx-
imation to minus the second derivative −u′′(xj) at the node xj, cf. (5.5). As a result,
in this particular case the ﬁnite element and ﬁnite diﬀerence numerical solution schemes
happen to coincide.
The sparse tridiagonal nature of the ﬁnite element matrix is a consequence of the
fact that the basis functions are zero on much of the interval, or, in more mathematical
language, that they have small support, in the following sense.
Deﬁnition 10.2.
The support of a function f(x), written supp f, is the closure of
the set where f(x) ̸= 0.
Thus, a point x will belong to the support, provided f is not zero there, or at least
is not zero at nearby points. For example, the support of the hat function (10.13) is the
(small) interval [xj−1, xj+1]. The key property, ensuring sparseness, is that the integral
of the product of two functions will be zero if their supports have empty intersection, or,
slightly more generally, have only a ﬁnite number of points in common.
Example 10.3. Consider the boundary value problem
−d
dx (x + 1) du
dx = 1,
u(0) = 0,
u(1) = 0.
(10.22)
The explicit solution is easily found by direct integration:
u(x) = −x + log(x + 1)
log 2
.
(10.23)

408
10 Finite Elements and Weak Solutions
Figure 10.3.
Finite element solution to (10.22).
It minimizes the associated quadratic functional
Q[u] =
 ℓ
0
 1
2 (x + 1)u′(x)2 −u(x)

dx
(10.24)
over the space of all C2 functions u(x) that satisfy the given boundary conditions. The
ﬁnite element system (10.9) has coeﬃcient matrix given by (10.16) and right-hand side
(10.18), where
sj =
 xj+1
xj
(1 + x) dx = h(1 + xj) + 1
2 h2 = h + h2
j + 1
2

,
bj =
 xj+1
xj
1 dx = h.
The resulting piecewise aﬃne approximation to the solution is plotted in Figure 10.3. The
ﬁrst three graphs contain, respectively, 5, 10, 20 nodes, so that h = .2, .1, .05, while the
last plots the exact solution (10.23). The maximal errors at the nodes are, respectively,
.000298, .000075, .000019, while the maximal overall errors between the exact solution and
its piecewise aﬃne ﬁnite element approximations are .00611, .00166, .00043. (One can more
closely ﬁt the solution curve by employing a cubic spline to interpolate the computed nodal
values, [89, 102], which has the eﬀect of reducing the preceding maximal overall errors by
a factor of, approximately, 20.) Thus, even when computed on rather coarse meshes, the
ﬁnite element approximation gives quite respectable results.
Remark: One can obtain a smoother, and hence more realistic, approximation to the
solution by smoothly interpolating the ﬁnite element approximations cj ≈u(xj) at the
nodes, e.g., by use of cubic splines, [89, 102]. Alternatively, one can require that the ﬁnite
element functions themselves be smoother, e.g., by making the ﬁnite element subspace
consist of piecewise cubic splines that satisfy the boundary conditions.

10.2 Finite Elements for Ordinary Diﬀerential Equations
409
Exercises
♣10.2.1. Use the ﬁnite element method to approximate the solution to the boundary value prob-
lem −d
dx

e−x du
dx

= 1, u(0) = u(2) = 0. Carefully explain how you are setting up
the calculation. Plot the resulting solutions and compare your answer with the exact solu-
tion. You should use an equally spaced mesh, but try at least three diﬀerent mesh spacings
and compare your results. By inspecting the errors in your various approximations, can you
predict how many nodes would be required for six-digit accuracy of the numerical approxi-
mation?
♠10.2.2. For each of the following boundary value problems: (i) Solve the problem exactly.
(ii) Approximate the solution using the ﬁnite element method based on ten equally spaced
nodes. (iii) Compare the graphs of the exact solution and its piecewise aﬃne ﬁnite element
approximation. What is the maximal error in your approximation at the nodes? on the en-
tire interval?
(a) −u′′ =
 1
x > 1,
0,
x < 1,
u(0) = u(2) = 0; (b) −d
dx

(1 + x) du
dx

= 1, u(0) = u(1) = 0;
(c) −d
dx

x2 du
dx

= −x, u(1) = u(3) = 0; (d) −d
dx

ex du
dx

= ex, u(−1) = u(1) = 0.
♣10.2.3.(a) Find the exact solution to the boundary value problem −u′′ = 3x, u(0) = u(1) = 0.
(b) Use the ﬁnite element method based on ﬁve equally spaced nodes to approximate the
solution. (c) Compare the graphs of the exact solution and its piecewise aﬃne ﬁnite ele-
ment approximation. (d) What is the maximal error (i) at the nodes? (ii) on the entire interval?
♣10.2.4. Use ﬁnite elements to approximate the solution to the Sturm–Liouville boundary value
problem −u′′+(x+1)u = xex, u(0) = 0, u(1) = 0, using 5, 10, and 20 equally spaced nodes.
♣10.2.5.(a) Devise a ﬁnite element scheme for numerically approximating the solution to the
mixed boundary value problem
−d
dx

κ(x) du
dx

= f(x),
a < x < b,
u(a) = 0,
u′(b) = 0.
(b) Test your method on the particular boundary value problem
−d
dx

(1 + x) du
dx

= 1,
0 < x < 1,
u(0) = 0,
u′(1) = 0,
using 10 equally spaced nodes. Compare your approximation with the exact solution.
♠10.2.6. Consider the periodic boundary value problem
−u′′ + u = x,
u(0) = u(2π),
u′(0) = u′(2π).
(a) Write down the analytic solution. (b) Write down a minimization principle. (c) Divide
the interval [0, 2π ] into n = 5 equal subintervals, and let Wn denote the subspace consist-
ing of all piecewise aﬃne functions that satisfy the boundary conditions. What is the di-
mension of Wn? Write down a basis. (d) Construct the ﬁnite element approximation to the
solution to the boundary value problem by minimizing the functional from part (b) on the
subspace Wn. Graph the result and compare with the exact solution. What is the maximal
error on the interval? (e) Repeat part (d) for n = 10, 20, and 40 subintervals, and discuss
the convergence of your solutions.
♠10.2.7. Answer Exercise 10.2.6 when the ﬁnite element subspace Wn consists of all periodic
piecewise aﬃne functions of period 1, so w(x + 1) = w(x). Which approximation is better?
♣10.2.8. Use the method of Exercise 10.2.7 to approximate the solution to the following periodic
boundary value problem for the Mathieu equation:
−u′′ + (1 + cos x)u = 1,
u(0) = u(2π),
u′(0) = u′(2π).

410
10 Finite Elements and Weak Solutions
♠10.2.9. Consider the boundary value problem solved in Example 10.3. Let Wn be the sub-
space consisting of all polynomials u(x) of degree ≤n satisfying the boundary conditions
u(0) = u(1) = 0. In this project, we will try to approximate the exact solution to the
boundary value problem by minimizing the functional (10.24) on the polynomial subspace
Wn. For n = 5, 10, and 20: (a) First, determine a basis for Wn. (b) Set up the minimiza-
tion problem as a system of linear equations for the coeﬃcients of the polynomial minimizer
relative to your basis. (c) Solve the polynomial minimization problem and compare your
“polynomial ﬁnite element” solution with the exact solution and the piecewise aﬃne ﬁnite
element solution graphed in Figure 10.3.
♠10.2.10. Consider the boundary value problem −u′′ + λu = x, for 0 < x < π, with u(0) = 0,
u(1) = 0. (a) For what values of λ does the system have a unique solution? (b) For which
values of λ can you ﬁnd a minimization principle that characterizes the solution? Is the
minimizer unique for all such values of λ?
(c) Using n equally spaced nodes, write down
the ﬁnite element equations for approximating the solution to the boundary value problem.
Note: Although the ﬁnite element construction is supposed to work only when there is a
minimization principle, we will consider the resulting linear algebraic system for any value
of λ. (d) Select a value of λ for which the solution can be characterized by a minimization
principle and verify that the ﬁnite element approximation with n = 10 approximates the
exact solution.
(e) Experiment with other values of λ. Does your ﬁnite element solution
give a good approximation to the exact solution when it exists? What happens at values of
λ for which the solution does not exist or is not unique?
10.3 Finite Elements in Two Dimensions
The same basic framework underlies the adaptation of ﬁnite element techniques for nu-
merically approximating the solution to boundary value problems governed by elliptic
partial diﬀerential equations. In this section, we concentrate on the simplest case: the
two-dimensional Poisson equation. Having mastered this, the reader will be well equipped
to carry over the method to more general equations and higher dimensions. As before, we
concentrate on the practical design of the ﬁnite element procedure, and refer the reader
to more advanced texts, e.g., [6, 113, 126], for the analytical details and proofs of conver-
gence. Most of the multi-dimensional complications lie not in the underlying theory, but
rather in the realm of data management and organization.
For speciﬁcity, consider the homogeneous Dirichlet boundary value problem
−Δu = f
in
Ω,
u = 0
on
∂Ω,
(10.25)
on a bounded domain Ω ⊂R2. According to Theorem 9.31, the solution u⋆(x, y) is char-
acterized as the unique minimizer of the Dirichlet functional
Q[u] = 1
2 |∥∇u ∥|2 −⟨u , f ⟩=
 
Ω
 1
2 u2
x + 1
2 u2
y −f u

dx dy
(10.26)
among all C2 functions u(x, y) that satisfy the prescribed boundary conditions.
To construct a ﬁnite element approximation, we restrict the Dirichlet functional to a
suitably chosen ﬁnite-dimensional subspace. As in the one-dimensional version, the most
eﬀective subspaces contain functions that may lack the requisite degree of smoothness that
qualiﬁes them as candidate solutions to the partial diﬀerential equation. Nevertheless, they
will provide good approximations to the actual classical solution. Another important prac-
tical consideration, ensuring sparseness of the ﬁnite element matrix, is to employ functions

10.3 Finite Elements in Two Dimensions
411
Figure 10.4.
Triangulation of a planar domain.
that have small support, meaning that they vanish on most of the domain. Sparseness has
the beneﬁt that the solution to the linear ﬁnite element system can be relatively rapidly
calculated, usually by application of an iterative numerical scheme such as the Gauss–Seidel
or SOR methods discussed in [89, 118].
Triangulation
The ﬁrst step is to introduce a mesh consisting of a ﬁnite number of nodes xl = (xl, yl),
l = 1, . . . , m, usually lying inside the domain Ω ⊂R2. Unlike ﬁnite diﬀerence schemes, ﬁnite
element methods are not tied to a rectangular mesh, thus endowing them with considerably
more ﬂexibility in the allowable discretizations of the domain. We regard the nodes as the
vertices of a triangulation of the domain, consisting of a collection of non-overlapping small
triangles, which we denote by T1, . . . , TN, whose union T⋆=

ν Tν approximates Ω; see
Figure 10.4 for a typical example. The nodes are split into two categories — interior nodes
and boundary nodes, the latter lying on or close to ∂Ω. A curved boundary will thus be
approximated by the polygonal boundary ∂T⋆of the triangulation, whose vertexvertices
are the boundary nodes. Thus, in any practical implementation of a ﬁnite element scheme,
the ﬁrst requirement is a routine that will automatically triangulate a speciﬁed domain in
some “reasonable” manner, as explained below.
As in our one-dimensional construction, the functions w(x, y) in the ﬁnite-dimensional
subspace W will be continuous and piecewise aﬃne, which means that, on each triangle,
the graph of w is a ﬂat plane and hence has the formula†
w(x, y) = αν + βν x + γν y
when
(x, y) ∈Tν,
(10.27)
for certain constants αν, βν, γν. Continuity of w requires that its values on a common edge
between two triangles must agree, and this will impose compatibility constraints on the
coeﬃcients αμ, βμ, γμ and αν, βν, γν associated with adjacent pairs of triangles Tμ and Tν.
The full graph of the piecewise aﬃne function z = w(x, y) forms a connected polyhedral
surface whose triangular faces lie above the triangles Tν; see Figure 10.5 for an illustration.
In addition, we require that the piecewise aﬃne function w(x, y) vanish at the boundary
nodes, which implies that it vanishes on the entire polygonal boundary of the triangulation,
†
Here and subsequently, the index ν is a superscript, not a power.

412
10 Finite Elements and Weak Solutions
Figure 10.5.
Piecewise aﬃne function.
Figure 10.6.
Finite element pyramid function.
∂T⋆, and hence (approximately) satisﬁes the homogeneous Dirichlet boundary conditions
on the curved boundary of the original domain, ∂Ω.
The next step is to choose a basis of the subspace of piecewise aﬃne functions as-
sociated with the given triangulation and subject to the imposed homogeneous Dirichlet
boundary conditions. The analogue of the one-dimensional hat function (10.12) is the pyra-
mid function ϕl(x, y), which has the value 1 at a single node xl = (xl, yl), and vanishes at
all the other nodes:
ϕl(xi, yi) =
 1,
i = l,
0,
i ̸= l.
(10.28)
Because, on any triangle, the pyramid function ϕl(x, y) is uniquely determined by its values
at the vertices, it will be nonzero only on those triangles that have the node xl as one of
their vertices. Hence, as its name implies, the graph of ϕl forms a pyramid of unit height
sitting on a ﬂat plane; a typical example appears in Figure 10.6.
The pyramid functions ϕl(x, y) associated with the interior nodes xl automatically
satisfy the homogeneous Dirichlet boundary conditions on the boundary of the domain —
or, more correctly, on the polygonal boundary of the triangulated domain. Thus, the ﬁnite

10.3 Finite Elements in Two Dimensions
413
element subspace W is the span of the interior node pyramid functions, and so a general
piecewise aﬃne function w ∈W is a linear combination thereof:
w(x, y) =
n

l=1
cl ϕl(x, y),
(10.29)
where the sum ranges over the n interior nodes of the triangulation. Owing to the original
speciﬁcation (10.28) of the pyramid functions, the coeﬃcients
cl = w(xl, yl) ≈u(xl, yl),
l = 1, . . ., n,
(10.30)
are the same as the values of the ﬁnite element approximation w(x, y) at the interior
nodes. This immediately implies linear independence of the pyramid functions, since the
only linear combination that vanishes at all nodes is the trivial one c1 = · · · = cn = 0.
Determining the explicit formulas for the pyramid functions is not diﬃcult. On one of
the triangles Tν that has xl as a vertex, ϕl(x, y) will be the unique aﬃne function (10.27)
that takes the value 1 at the vertex xl and 0 at its other two vertices xi and xj. Thus, we
seek a formula for an aﬃne function or element
ων
l (x, y) = αν
l + βν
l x + γν
l y,
(x, y) ∈Tν,
(10.31)
that takes the prescribed values
ων
l (xi, yi) = αν
l + βν
l xi + γν
l yi = 0,
ων
l (xj, yj) = αν
l + βν
l xj + γν
l yj= 0,
ων
l (xl, yk) = αν
l + βν
l xl + γν
l yl = 1.
(10.32)
Solving this linear system for the coeﬃcients — using either Cramer’s Rule or direct Gauss-
ian Elimination — produces the explicit formulas
αν
l = xi yj −xj yi
Δν
,
βν
l = yi −yj
Δν
,
γν
l = xj −xi
Δν
,
(10.33)
where the denominator
Δν = det
⎛
⎝
1
xi
yi
1
xj
yj
1
xl
yl
⎞
⎠= ±2 area Tν
(10.34)
is, up to sign, twice the area of the triangle Tν; see Exercise 10.3.5.
Example 10.4. Consider an isosceles right triangle T with vertices
x1 = (0, 0),
x2 = (1, 0),
x3 = (0, 1).
Using (10.33–34) (or solving the linear system (10.32) directly), we immediately produce
the three corresponding aﬃne elements
ω1(x, y) = 1 −x −y,
ω2(x, y) = x,
ω3(x, y) = y.
(10.35)
As required, each ωl equals 1 at the vertex xl and is zero at the other two vertices.

414
10 Finite Elements and Weak Solutions
Figure 10.7.
Vertex polygons.
A pyramid function is then obtained by piecing together the individual aﬃne elements:
ϕl(x, y) =
 ων
l (x, y),
if (x, y) ∈Tν and xl is a vertex of Tν,
0,
otherwise.
(10.36)
Continuity of ϕl(x, y) is assured, since the constituent aﬃne elements have the same values
at common vertices, and hence also along common edges. The support of the pyramid
function (10.36) is the vertex polygon
supp ϕl = Pl =

ν
Tν
(10.37)
consisting of all the triangles Tν that have the node xl as a vertex.
In other words,
ϕl(x, y) = 0 whenever (x, y) ̸∈Pl. The node xl lies on the interior of its vertex polygon
Pl, while the vertices of Pl are all the nodes connected to xl by a single edge of the
triangulation. In Figure 10.7, the shaded regions indicate two of the vertex polygons for
the triangulation in Figure 10.4.
Example 10.5. The simplest, and most common, triangulations are based on regular
meshes. For example, suppose that the nodes lie on a square grid, and so are of the form
xi,j = (ih + a, j h + b), where (i, j) run over a collection of integer pairs, h > 0 is the
inter-node spacing, and (a, b) represents an overall oﬀset. If we choose the triangles to all
have the same orientation, as in the ﬁrst picture in Figure 10.8, then the vertex polygons
all have the same shape, consisting of six triangles of total area 3h2 — the shaded region.
On the other hand, if we choose an alternating triangulation, as in the second picture, then
there are two types of vertex polygons. The ﬁrst, consisting of four triangles, has area 2h2,
while the second, containing eight triangles, has twice the area, 4h2. In practice, there are
good reasons to prefer the former triangulation.
In general, to ensure convergence of the ﬁnite element solution to the true minimizer,
one should choose triangulations that satisfy the following properties:
• The three side lengths of any individual triangle should be of comparable size, and
so long, skinny triangles and obtuse triangles should be avoided.
• The areas of nearby triangles Tν should not vary too much.
• The areas of nearby vertex polygons Pl should also not vary too much.

10.3 Finite Elements in Two Dimensions
415
Figure 10.8.
Square mesh triangulations.
While the nearby triangles should be of comparable size, one might very well allow wide
variations over the entire domain, with small triangles in regions where the solution is
changing rapidly, and large triangles in less active regions.
Exercises
10.3.1. Sketch a triangulation of the following domains so that all triangles have side length
at most .5:
(a) a unit square; (b) an isosceles triangle with vertices (−.5, 0), (.5, 0) and
(0, 1); (c) the square {| x |, | y | ≤2} with the hole {| x |, | y | < 1} removed;
(d) the unit disk; (e) the annulus 1 ≤∥x ∥≤2.
10.3.2. Describe the vertex polygons for a triangulation that uses regular equilateral triangles.
10.3.3. Are there any restrictions on the number of sides a vertex polygon can have?
10.3.4. Find the three ﬁnite element functions ω1(x, y), ω2(x, y), ω3(x, y), associated with
(a) the triangle having vertices (1, 0), (0, 1), and (1, 1);
(b) the triangle having vertices (0, 1), (1, −1), and (−1, −1);
(c) an equilateral triangle centered at the origin having one vertex at (1, 0).
♦10.3.5.(a) Prove that the area of a planar triangle T with vertices (a, b), (c, d), (e, f) is equal to
1
2 | Δ |, where Δ = det
⎛
⎜
⎝
1
a
b
1
c
d
1
e
f
⎞
⎟
⎠. (b) Prove that Δ > 0 if and only if the vertices of the
triangle are listed in counterclockwise order.
♦10.3.6. Give a detailed justiﬁcation of the continuity of the pyramid function (10.36).
♥10.3.7. An alternative to triangular elements is to employ piecewise bi-aﬃne functions, mean-
ing ω(x, y) = α + β x + γ y + δ xy, on rectangles. (a) Suppose R is a rectangle with vertices
(x1, y1), (x2, y2), (x3, y3), (x4, y4), whose sides are parallel to the coordinate axes. Prove
that, for each l = 1, . . . , 4, there is a unique bi-aﬃne function ωl(x, y) deﬁned on R that has
the value ωl(xl, yl) = 1 at one vertex while ωl(xi, yi) = 0, i ̸= l, at the other three vertices.

416
10 Finite Elements and Weak Solutions
(b) Write out the four bi-aﬃne functions ω1(x, y), . . . , ω4(x, y), when
(i) R = {0 ≤x, y ≤1}, (ii) R = {−1 ≤x, y ≤1}. (c) Does the result in part (a) hold for
rectangles whose sides are not aligned with the axes? For general quadrilaterals?
The Finite Element Equations
We now seek to approximate the solution to the homogeneous Dirichlet boundary value
problem by restricting the Dirichlet functional (10.26) to the selected ﬁnite element sub-
space W. Using the general framework of Section 10.1, we substitute the formula (10.29)
for a general element of W into the quadratic Dirichlet functional (9.82). Expanding, we
obtain
Q[w] = Q
$
n

i=1
ci ϕi
%
=
 
Ω
$ 
n

i=1
ci ∇ϕi
2
−f(x, y)

n

i=1
ci ϕi
 %
dx dy
= 1
2
n

i,j =1
kij ci cj −
n

i=1
bi ci = 1
2 cT K c −bTc.
(10.38)
Here K = (kij) is a symmetric n × n matrix, while b = ( b1, b2, . . . , bn )T is a vector in Rn,
with respective entries
kij = ⟨⟨∇ϕi , ∇ϕj ⟩⟩=
 
Ω
∇ϕi · ∇ϕj dx dy,
bi = ⟨f , ϕi ⟩=
 
Ω
f ϕi dx dy,
(10.39)
which also follow directly from the general formulas (10.6–7). Thus, the ﬁnite element
approximation (10.29) will minimize the quadratic function
P(c) = 1
2 cT K c −bTc
(10.40)
over all possible choices of coeﬃcients c = ( c1, c2, . . . , cn )T ∈Rn, i.e., over all possible
function values at the interior nodes. As above, the minimizer’s coeﬃcients are obtained
by solving the associated linear system
K c = b,
(10.41)
using either Gaussian Elimination or a suitable iterative linear systems solver.
To ﬁnd explicit formulas for the matrix coeﬃcients kij in (10.39), we begin by noting
that the gradient of the aﬃne element (10.31) is equal to
gν
l = ∇ων
l (x, y) =

∂ων
l /∂x
∂ων
l /∂y

=

βν
l
γν
l

=
1
Δν

yi −yj
xj −xi

,
(x, y) ∈Tν,
(10.42)
which is a constant vector inside the triangle Tν, while ∇ων
l = 0 outside Tν. Therefore,
∇ϕl(x, y) =

gν
l ,
if (x, y) ∈Tν that has xl as a vertex,
0,
otherwise.
(10.43)

10.3 Finite Elements in Two Dimensions
417
Actually, (10.43) is not quite correct, since the gradient is not well deﬁned on the boundary
of a triangle Tν, but this will not cause us any diﬃculty in evaluating the ensuing integrals.
We will approximate integrals over the domain Ω by summing the corresponding in-
tegrals over the individual triangles — which relies on our assumption that the polygonal
boundary of the triangulation ∂T⋆is a reasonably close approximation to the true boundary
∂Ω. In particular,
kij ≈

ν
 
Tν
∇ϕi · ∇ϕj dx dy ≡

ν
kν
ij.
(10.44)
Now, according to (10.43), one or the other gradient in the integrand will vanish on the
entire triangle Tν unless both xi and xj are vertices. Therefore, the only terms contributing
to the sum are those triangles Tν that have both xi and xj as vertices. If i ̸= j, there are
only two such triangles, having a common edge, while if i = j, every triangle in the ith
vertex polygon Pi contributes. The individual summands are easily evaluated, since the
gradients are constant on the triangles, and so, by (10.43),
kν
ij =
 
Tν
gν
i · gν
j dx dy = gν
i · gν
j area Tν = 1
2 gν
i · gν
j | Δν | .
Let Tν have vertices xi, xj, xl. Then, by (10.34, 42, 43),
kν
ij = 1
2
(yj −yl)(yl −yi) + (xl −xj)(xi −xl)
(Δν)2
| Δν | = −(xi −xl) · (xj −xl)
2 | Δν |
,
i ̸= j,
kν
ii = 1
2
(yj −yl)2 + (xl −xj)2
(Δν)2
| Δν | = ∥xj −xl ∥2
2 | Δν |
(10.45)
= −(xi −xl) · (xi −xj) + (xi −xl) · (xj −xl)
2 Δν
= −kν
ij −kν
il .
In this manner, each triangle Tν speciﬁes a collection of six diﬀerent coeﬃcients, kν
ij = kν
ji,
indexed by its vertices, and known as the elemental stiﬀnesses of Tν. Interestingly, the
elemental stiﬀnesses depend only on the three vertex angles in the triangle and not on
its size. Thus, similar triangles have the same elemental stiﬀnesses. Indeed, according to
Exercise 10.3.13,
kν
ii = 1
2(cot θν
j + cot θν
l ),
while
kν
ij = kν
ji = −1
2 cot θν
l ,
i ̸= j,
(10.46)
where 0 < θν
l < π denotes the angle in Tν at the vertex xl.
Example 10.6. The right triangle with vertices x1 = (0, 0), x2 = (1, 0), x3 = (0, 1)
has elemental stiﬀnesses
k11 = 1,
k22 = k33 = 1
2 ,
k12 = k21 = k13 = k31 = −1
2,
k23 = k32 = 0.
(10.47)
The same holds for any other isosceles right triangle, provided its vertices are labeled in
the same manner. Similarly, an equilateral triangle has all 60◦angles, and so its elemental
stiﬀnesses are
k11 = k22 = k33 =
1
√
3 ≈.5774,
k12 = k21 = k13 = k31 = k23 = k32 = −
1
2
√
3 ≈−.2887.
(10.48)

418
10 Finite Elements and Weak Solutions
Exercises
10.3.8. Write down the elemental stiﬀnesses for:
(a) the triangle with vertices (0, 1), (−1, 2),
(0, −1); (b) the triangle with vertices (1, 1), (−1, 1), (0, −2); (c) a 30−60−90 degree right
triangle; (d) a right triangle with side lengths 3, 4, 5; (e) an isosceles triangle of height 3
and base 2; (f ) a “golden” isosceles triangle with angles 36◦, 72◦, 72◦.
♦10.3.9. A rectangular mesh has nodes xi,j = (iΔx + a, j Δy + b), where Δx, Δy > 0 are,
respectively, the horizontal and vertical step sizes. Find the elemental stiﬀnesses for the
triangles associated with such a rectangular mesh.
10.3.10. True or false: Let T be a triangle, and T a triangle obtained by rotating T by 60◦.
Then T and T have the same elemental stiﬀnesses.
xi
xj
xl
aν
l
10.3.11. Prove that the gradient (10.42) of the aﬃne element is equal
to ∇ων
l = ∥aν
l ∥−2 aν
l , where aν
l is the altitude vector that goes
to the vertex xl from its opposite side, as indicated in the ﬁgure.
10.3.12. Explain why the pyramid functions are linearly independent.
♦10.3.13. Prove formulas (10.46).
Assembling the Elements
The elemental stiﬀnesses of each triangle will contribute, through the summation (10.44),
to the ﬁnite element coeﬃcient matrix K. We begin by constructing a larger matrix K,
which we call the full ﬁnite element matrix, of size m × m, where m is the total number
of nodes in our triangulation, including both interior and boundary nodes. The rows and
columns of K are labeled by the nodes x1, . . . , xm. Let Kν = (kν
ij) be the corresponding
m × m matrix containing the elemental stiﬀnesses kν
ij of Tν in the rows and columns
indexed by its vertices, and all other entries equal to 0. Thus, Kν will have (at most) nine
nonzero entries. The resulting m × m matrices are summed together over all the triangles
T1, . . . , TN, whereby
K =
N

ν =1
Kν,
(10.49)
in accordance with (10.44).
The full ﬁnite element matrix K is too large, since its rows and columns include all
the nodes, whereas the ﬁnite element matrix K appearing in (10.41) refers only to the n
interior nodes. The reduced n × n ﬁnite element matrix K is simply obtained from K by
deleting all rows and columns indexed by boundary nodes, retaining only the elements kij
for which both xi and xj are interior nodes. For the homogeneous boundary value problem,
this is all we require. As we will subsequently see, inhomogeneous boundary conditions are
most easily handled by retaining (another part of) the full matrix K.
The easiest way to absorb the construction is by working through a particular example.
Example 10.7.
A metal plate has the shape of an oval running track, consisting
of a rectangle, with side lengths 1 m by 2 m, and two semi-circular disks glued onto its

10.3 Finite Elements in Two Dimensions
419
Figure 10.9.
The oval plate.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Triangles
1
2
3
4
5
6
7
8
9
10
11
12
13
Nodes
Figure 10.10.
A coarse triangulation of the oval plate.
shorter ends, as sketched in Figure 10.9. The plate is subject to a heat source, while its
edges are held at a ﬁxed temperature. The problem is to ﬁnd the equilibrium temperature
distribution within the plate. Mathematically, we must solve the planar Poisson equation,
subject to Dirichlet boundary conditions, for the equilibrium temperature u(x, y).
Let us describe how to set up the ﬁnite element approximation. We begin with a
very coarse triangulation of the plate, which will not give particularly accurate results,
but serves to illustrate how to go about assembling the ﬁnite element matrix. We divide
the rectangular part of the plate into eight right triangles, while each semicircular end
will be approximated by three equilateral triangles. The triangles are numbered from 1
to 14 as indicated in Figure 10.10. There are 13 nodes in all, numbered as in the second
ﬁgure. Only nodes 1, 2, 3 are interior, while the boundary nodes are labeled 4 through 13
in counterclockwise order starting at the top. The full ﬁnite element matrix K will have
size 13 × 13, its rows and columns labeled by all the nodes, while the reduced matrix K
appearing in the ﬁnite element equations (10.41) consists of the upper left 3 × 3 submatrix
of K corresponding to the three interior nodes.
For each ν = 1, . . ., 14, the triangle Tν will contribute its elemental stiﬀnesses, as
indexed by its vertices, to the matrix K through a summand Kν. For example, the ﬁrst
triangle T1 is equilateral, and so has elemental stiﬀnesses (10.48). Its vertices are labeled
1, 5, and 6, and therefore we place the stiﬀnesses in the rows and columns numbered 1, 5, 6

420
10 Finite Elements and Weak Solutions
to form the summand
K1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.5774
0
0
0
−.2887
−.2887
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
−.2887
0
0
0
.5774
−.2887
0
0
. . .
−.2887
0
0
0
−.2887
.5774
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where all the undisplayed entries in the full 13 × 13 matrix are 0. The next triangle T2 has
the same equilateral elemental stiﬀness matrix (10.48), but now its vertices are 1, 6, 7, and
so it will contribute
K2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.5774
0
0
0
0
−.2887
−.2887
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
−.2887
0
0
0
0
.5774
−.2887
0
. . .
−.2887
0
0
0
0
−.2887
.5774
0
. . .
0
0
0
0
0
0
0
0
. . .
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Similarly for K3, with vertices 1, 7, 8. On the other hand, T4 is an isosceles right triangle,
and so has elemental stiﬀnesses (10.47). Its vertices are labeled 1, 4, and 5, with vertex 5
at the right angle. Therefore, its contribution is
K4 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
.5
0
0
0
−.5
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
.5
−.5
0
0
0
. . .
−.5
0
0
−.5
1.0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
0
0
0
0
0
0
0
0
. . .
...
...
...
...
...
...
...
...
...
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Continuing in this manner, we assemble 14 contributions K1, . . . , K14, each with at most
9 nonzero entries. The full ﬁnite element matrix is their sum

10.3 Finite Elements in Two Dimensions
421
%
K = K1 + K2 + · · · + K14
=
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
3.732
−1
0
0
−.7887
−.5774
−.5774
−1
4
−1
−1
0
0
0
0
−1
3.732
0
0
0
0
0
−1
0
2
−.5
0
0
−.7887
0
0
−.5
1.577
−.2887
0
−.5774
0
0
0
−.2887
1.155
−.2887
−.5774
0
0
0
0
−.2887
1.155
−.7887
0
0
0
0
0
−.2887
0
−1
0
0
0
0
0
0
0
−.7887
0
0
0
0
0
0
−.5774
0
0
0
0
0
0
−.5774
0
0
0
0
0
0
−.7887
−.5
0
0
0
(10.50)
−.7887
0
0
0
0
0
0
−1
0
0
0
0
0
0
−.7887
−.5774
−.5774
−.7887
0
0
0
0
0
−.5
0
0
0
0
0
0
0
0
0
0
0
0
−.2887
0
0
0
0
0
1.577
−.5
0
0
0
0
−.5
2
−.5
0
0
0
0
−.5
1.577
−.2887
0
0
0
0
−.2887
1.155
−.2887
0
0
0
0
−.2887
1.155
−.2887
0
0
0
0
−.2887
1.577
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Since nodes 1, 2, 3 are interior, the reduced ﬁnite element matrix
K =
⎛
⎝
3.732
−1
0
−1
4
−1
0
−1
3.732
⎞
⎠
(10.51)
uses only the upper left 3 × 3 block of K. Clearly, it would not be diﬃcult to directly
construct K, bypassing K entirely.
For a ﬁner triangulation, the construction is similar, but the matrices become much
larger. The procedure can, of course, be automated. Fortunately, if we choose a very
regular triangulation, then we do not need to be nearly as meticulous in assembling the
stiﬀness matrices, since many of the entries are the same. The simplest case employs a
uniform square mesh, and so triangulates the domain into isosceles right triangles. This is
accomplished by laying out a relatively dense square grid over the domain Ω ⊂R2. The
interior nodes are the grid points that fall inside the oval domain, while the boundary
nodes are all those grid points lying adjacent to one or more of the interior nodes, and
are near but not necessarily precisely on the boundary ∂Ω. Figure 10.11 shows the nodes
in a square grid with intermesh spacing h = .2. While a bit crude in its approximation
of the boundary of the domain, this procedure does have the advantage of making the
construction of the associated ﬁnite element matrix relatively painless.
For such a mesh, all the triangles are isosceles right triangles, with elemental stiﬀnesses
(10.47). Summing the corresponding matrices Kν over all the triangles, as in (10.49), we
ﬁnd that the rows and columns of K corresponding to the interior nodes all have the same

422
10 Finite Elements and Weak Solutions
Figure 10.11.
A square mesh for the oval plate.
form. Namely, if i labels an interior node, then the corresponding diagonal entry is kii = 4,
while the oﬀ-diagonal entries kij = kji, i ̸= j, are equal to −1 when node i is adjacent to
node j on the grid, and are equal to 0 in all other cases. Node j is allowed to be a boundary
node. (Interestingly, the result does not depend on how one orients the pair of triangles
making up each square of the grid, which plays a role only in the computation of the right-
hand side of the ﬁnite element equation.) Observe that the same computation applies even
to our coarse triangulation. The interior node 2 belongs to all right isosceles triangles, and
the corresponding nonzero entries in (10.50) are k22 = 4 and k21 = k23 = k24 = k29 = −1,
indicating the four adjacent nodes.
Remark: The coeﬃcient matrix constructed from the ﬁnite element method on a
square (or even rectangular) grid is the same as the coeﬃcient matrix arising from a
ﬁnite diﬀerence solution to the Laplace or Poisson equation, as described in Example 5.7.
The ﬁnite element approach has the advantage of readily adapting to much more general
discretizations of the domain, and is not restricted to rectangular grids.
The Coeﬃcient Vector and the Boundary Conditions
So far, we have been concentrating on assembling the ﬁnite element coeﬃcient matrix K.
We also need to compute the forcing vector b = ( b1, b2, . . . , bn )T appearing on the right-
hand side of the fundamental linear equation (10.41). According to (10.39), the entries bi
are found by integrating the product of the forcing function and the ﬁnite element basis
function. As before, we will approximate the integral over the domain Ω by an integral
over the triangles, and so
bi =
 
Ω
f(x, y) ϕi(x, y) dx dy ≈

ν
 
Tν
f(x, y) ων
i (x, y) dx dy ≡

ν
bν
i .
(10.52)
Typically, an exact computation of the various triangular double integrals is not so
convenient, and so we resort to a numerical approximation. Since we are assuming that
the individual triangles are small, we can get away with a very crude numerical integration
scheme. If the function f(x, y) does not vary much over the triangle Tν — which will
certainly be the case if Tν is suﬃciently small — we may approximate f(x, y) ≈cν
i for

10.3 Finite Elements in Two Dimensions
423
Figure 10.12.
Finite element tetrahedron.
(x, y) ∈Tν by a constant. The integral (10.52) is then approximated by
bν
i =
 
Tν
f(x, y) ων
i (x, y) dx dy ≈cν
i
 
Tν
ων
i (x, y) dx dy = 1
3 cν
i area Tν = 1
6 cν
i | Δν |.
(10.53)
The formula for the integral of the aﬃne element ων
i (x, y) follows from solid geometry: it
equals the volume under its graph, a tetrahedron of height 1 and base Tν, as illustrated in
Figure 10.12.
How to choose the constant cν
i ? In practice, the simplest choice is to let cν
i = f(xi, yi)
be the value of the function at the ith vertex. With this choice, the sum in (10.52) becomes
bi ≈

ν
1
3 f(xi, yi) area Tν = 1
3 f(xi, yi) area Pi,
(10.54)
where Pi is the vertex polygon (10.37) corresponding to the node xi. In particular, for the
square mesh with the uniform choice of triangles, as in the ﬁrst plot in Figure 10.8,
area Pi = 3 h2
for all i, and so
bi ≈f(xi, yi) h2
(10.55)
is well approximated by just h2 times the value of the forcing function at the node. This
is the underlying reason to choose the uniform triangulation for the square mesh; the
alternating version would give unequal values for the bi over adjacent nodes, and this could
give rise to unnecessary errors in the ﬁnal approximation.
Example 10.8. For the coarsely triangulated oval plate, the reduced stiﬀness matrix
is (10.51). The Poisson equation
−Δu = 4
models a constant external heat source of magnitude 4◦over the entire plate. If we keep the
edges of the plate ﬁxed at 0◦, then we need to solve the ﬁnite element equation K c = b,
where K is the coeﬃcient matrix (10.51). The entries of b are, by (10.54), equal to 4 (the
right-hand side of the diﬀerential equation) times one-third the area of the corresponding
vertex polygon, which for node 2 is the square consisting of four right triangles, each of
area 1
2, whereas for nodes 1 and 3 it consists of four right triangles of area 1
2 plus three

424
10 Finite Elements and Weak Solutions
Figure 10.13.
Finite element solutions to Poisson’s equation for an oval plate.
equilateral triangles, each of area
√
3
4 ; see Figure 10.10. Thus,
b = 4
3
"
2 + 3
√
3
4 , 2, 2 + 3
√
3
4
#T
= ( 4.3987, 2.6667, 4.3987 )T .
The solution to the ﬁnal linear system K c = b is easily found:
c = ( 1.5672, 1.4503, 1.5672 )T .
Its entries are the values of the ﬁnite element approximation at the three interior nodes.
The piecewise aﬃne ﬁnite element solution is plotted in the ﬁrst illustration in Figure 10.13.
A more accurate approximation, based on a square grid triangulation of size h = .1, appears
in the second ﬁgure. Here, the largest errors are concentrated near the poorly approximated
corners of the oval, and could be improved by a more sophisticated triangulation.
Inhomogeneous Boundary Conditions
So far, we have restricted our attention to problems with homogeneous Dirichlet bound-
ary conditions. According to Theorem 9.32, the solution to the inhomogeneous Dirichlet
problem
−Δu = f
in
Ω,
u = h
on
∂Ω,
is also obtained by minimizing the Dirichlet functional (9.82). However, now the mini-
mization takes place over the set of functions that satisfy the inhomogeneous boundary
conditions. It is not diﬃcult to ﬁt this problem into the ﬁnite element scheme.
The elements corresponding to the interior nodes of our triangulation remain as before,
but now we need to include additional elements to ensure that our approximation satisﬁes
the boundary conditions.
Note that if xl is a boundary node, then the corresponding
boundary element ϕl(x, y) satisﬁes (10.28), and so has the same piecewise aﬃne form
(10.36). The corresponding ﬁnite element approximation
w(x, y) =
m

l=1
cl ϕl(x, y)
(10.56)
has the same form as before, (10.29), but now the sum is over all nodes, both interior
and boundary. As before, the coeﬃcients cl = w(xl, yl) ≈u(xl, yl) are the values of the

10.3 Finite Elements in Two Dimensions
425
ﬁnite element approximation at the nodes. Therefore, in order to satisfy the boundary
conditions, we require
cj = hj = h(xj, yj)
whenever
xj = (xj, yj)
is a boundary node.
(10.57)
If the boundary node xj does not lie precisely on the boundary ∂Ω, then h(xj, yj) is not
deﬁned, and so we need to approximate the value hj appropriately, e.g., using the value of
h(x, y) at a nearby boundary point (x, y) ∈∂Ω.
The derivation of the ﬁnite element equations proceeds as before, but now there are
additional terms arising from the nonzero boundary values. Leaving the intervening details
to Exercise 10.3.23, the ﬁnal outcome can be written as follows. Let K denote the full m×m
ﬁnite element matrix constructed as above. The reduced coeﬃcient matrix K is obtained
by retaining the rows and columns corresponding to only interior nodes, and so will have
size n × n, where n is the number of interior nodes. The boundary coeﬃcient matrix K is
the n × (m −n) matrix consisting of those entries of the interior rows that do not appear
in K, i.e., those lying in the columns indexed by the boundary nodes. For instance, in the
coarse triangulation of the oval plate, the full ﬁnite element matrix is given in (10.50), and
the upper 3 × 3 subblock is the reduced matrix (10.51). The remaining entries of the ﬁrst
three rows form the boundary coeﬃcient matrix
K =
⎛
⎝
0 −.7887 −.5774 −.5774 −.7887
0
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
0
0
0
0
0
0 −.7887 −.5774 −.5774 −.7887
⎞
⎠.
(10.58)
We similarly split the coeﬃcients ci of the ﬁnite element function (10.56) into two groups.
We let c = ( c1, c2, . . . , cn )T ∈Rn denote the as yet unknown coeﬃcients corresponding to
the values of the approximation at the interior nodes xi, while h =

h1, h2, . . . , hm−n
T ∈
Rm−n will be the vector containing the boundary values (10.57). The solution to the ﬁnite
element approximation (10.56) is then obtained by solving the associated linear system
K c + K h = b,
or, equivalently,
K c = f = b −K h.
(10.59)
Example 10.9. For the oval plate discussed in Example 10.7, suppose the right-hand
semicircular edge is held at 10◦, the left-hand semicircular edge at −10◦, while the two
straight edges have a linearly varying temperature distribution ranging from −10◦at the
left to 10◦at the right, as illustrated in Figure 10.14. Our task is to compute its equilibrium
temperature, assuming no internal heat source. Thus, for the coarse triangulation we have
the boundary node values
h = ( h4, . . . , h13 )T = ( 0, −10, −10, −10, −10, 0, 10, 10, 10, 10 )T .
Using the previously computed formulas (10.51, 58) for the interior and boundary coeﬃcient
matrices K, K, we approximate the solution to the Laplace equation by solving (10.59). We
are assuming that there is no external forcing function, f(x, y) ≡0, and hence b = 0, and
so we must solve K c = f = −K h = ( 2.1856, 3.6, 7.6497 )T . The ﬁnite element function
corresponding to the solution c = ( 1.0679, 1.8, 2.5320 )T is plotted in the ﬁrst illustration in
Figure 10.14. Even on such a coarse mesh, the approximation is not too bad, as evidenced
by the second illustration, which plots the ﬁnite element solution for the ﬁner square mesh
of Figure 10.11.

426
10 Finite Elements and Weak Solutions
Figure 10.14.
Solution to the Dirichlet problem for the oval plate.
Exercises
♠10.3.14. Consider the Dirichlet boundary value problem Δu = 0, u(x, 0) = sin x, u(x, π) = 0,
u(0, y) = 0, u(π, y) = 0, on the square S = {0 < x, y < π }. (a) Find the exact solu-
tion. (b) Set up and solve the ﬁnite element equations based on a square mesh with n = 2
squares on each side of S. Write out the reduced ﬁnite element matrix, the boundary coef-
ﬁcient matrix, and the value of your approximation at the middle of the unit square. How
close is this value to the exact solution there? (c) Repeat part (b) for n = 4 squares per
side. Is the value of your approximation at the center of the unit square closer to the true
solution? (d) Use a computer to ﬁnd a ﬁnite element approximation to u
 1
2 π, 1
2 π

using
n = 8 squares per side. Is your approximation converging to the exact solution as the mesh
becomes ﬁner and ﬁner?
♣10.3.15. Approximate the solution to the Dirichlet problem Δu = 0, u(x, 0) = x, u(x, 1) =
1 −x, u(0, y) = y, u(1, y) = 1 −y, by use of ﬁnite elements with mesh sizes Δx = Δy = .25
and .1. Compare your approximations with the solution you obtained in Exercise 4.3.12(d).
What is the maximal error at the nodes in each case?
♠10.3.16. A metal plate has the shape of an equilateral triangle with unit sides. One side is
heated to 100◦, while the other two are kept at 0◦. In order to approximate the equilibrium
temperature distribution, the plate is divided into smaller equilateral triangles, with n tri-
angles on each side, and the corresponding ﬁnite element approximation is then computed.
(a) How many triangles are in the triangulation? How many interior nodes? How many
edge nodes? (b) For n = 2, set up and solve the ﬁnite element linear system to ﬁnd an
approximation to the temperature at the center of the triangle. (c) Answer part (b) when
n = 3. (d) Use a computer to ﬁnd the ﬁnite element approximation to the temperature at
the center when n = 5, 10, and 15. Are your values converging to the actual temperature?
(e) Plot the ﬁnite element approximations you constructed in the previous parts.
10.3.17. Find the equilibrium temperature distribution in a unit equilateral triangle when one
side is heated to 100◦, while the other two are insulated.
♠10.3.18. A metal plate has the shape of a 3 cm square with a 1 cm square hole cut out of the
middle. The plate is heated by ﬁxing the inner edge at temperature 100◦while keeping
the outer edge at 0◦. (a) Find the (approximate) equilibrium temperature using ﬁnite el-
ements with a mesh width of Δx = Δy = .5 cm. Plot your approximate solution using

10.4 Weak Solutions
427
a three-dimensional graphics program. (b) Let C denote the square contour lying midway
between the inner and outer square boundaries of the plate. Using your ﬁnite element ap-
proximation, at what point(s) on C is the temperature a (i) minimum?
(ii) maximum?
(iii) equal to 50◦, the average of the two boundary temperatures?
(c) Repeat part (a)
using a smaller mesh width of h = .2. How much does this aﬀect your answers in part (b)?
♣10.3.19. Answer Exercise 10.3.18 when the plate is additionally subjected to a constant heat
source f(x, y) = 600x + 800y −2400.
♠10.3.20.(a) Construct a ﬁnite element approximation to the solution, using a maximal mesh
size of .1, to the following boundary value problem on the unit disk:
Δu = 0,
x2 + y2 < 1,
u =
⎧
⎨
⎩
1,
x2 + y2 = 1,
y > 0,
0,
x2 + y2 = 1,
y < 0.
(b) Compare your solution with the exact solution given in Example 4.7.
♣10.3.21.(a) Use ﬁnite elements to approximate the solution to the boundary value problem
−Δu + u = 0,
0 < x, y < 1,
u(x, 0) = u(x, 1) = u(0, y) = 0,
u(1, y) = 1.
(b) Compare your result with the ﬁrst 5 and 10 summands in the series solution obtained
via separation of variables.
♦10.3.22.(a) Justify the construction of the ﬁnite element matrix for a square mesh described
in the text. (b) How would you modify the matrix for a rectangular mesh, as in Exercise
10.3.9?
♦10.3.23. Justify the inhomogeneous ﬁnite element construction in the text.
♥10.3.24.(a) Explain how to adapt the ﬁnite element method to a mixed boundary value prob-
lem with inhomogeneous Neumann conditions.
(b) Apply your method to the problem
Δu = 0,
∂u
∂y (x, 0) = x,
u(x, 1) = 0,
u(0, y) = 0,
u(1, y) = 0.
(c) Solve the boundary value problem via separation of variables. Compare the values of
your solutions at the center of the square.
10.4 Weak Solutions
An alternative route to the ﬁnite element method, which avoids the requirement of a
minimization principle, rests upon the notion of a weak solution to a diﬀerential equation
— a concept of considerable independent interest, since it includes many of the nonclassical
solutions that we encountered earlier in this book. In particular, the discontinuous shock
waves of Section 2.3 are, in fact, weak solutions to the nonlinear transport equation, as are
the continuous but only piecewise smooth solutions to the wave equation that resulted from
applying d’Alembert’s formula to nonsmooth initial data. Weak solutions have become an
incredibly powerful idea in the modern theory of partial diﬀerential equations, and we have
space to present only the very basics here. They are particularly appropriate in the study
of discontinuous and nonsmooth physical phenomena, including shock waves, cracks and
dislocations in elastic media, singularities in liquid crystals, and so on. In the mathematical
analysis of partial diﬀerential equations, it is often easier to prove the existence of a weak
solution, for which one can then try to establish suﬃcient smoothness in order that it
qualify as a classical solution. Further developments along with a range of applications can
be found in more advanced texts, including [38, 44, 61, 99, 107, 122].

428
10 Finite Elements and Weak Solutions
Weak Formulations of Linear Systems
The key idea behind the concept of a weak solution begins with a rather trivial observation:
the only element in an inner product space that is orthogonal to every other element is the
zero element.
Lemma 10.10.
Let V be an inner product space with inner product† ⟨⟨· , · ⟩⟩. An
element v⋆∈V satisﬁes ⟨⟨v⋆, v ⟩⟩= 0 for all v ∈V if and only if v⋆= 0.
Proof : In particular, v⋆must be orthogonal to itself, so 0 = ⟨⟨v⋆, v⋆⟩⟩= |∥v⋆∥|2,
which immediately implies v⋆= 0.
Q.E.D.
Thus, one method of solving a linear — or even nonlinear — equation F[u] = 0 is to
write it in the form
⟨⟨F[u] , v ⟩⟩= 0
for all
v ∈V,
(10.60)
where V is the target space of F: U →V .
In particular, for an inhomogeneous linear
system, L[u] = f, with L: U →V a linear operator between inner product spaces, the
condition (10.60) takes the form
0 = ⟨⟨L[u] −f , v ⟩⟩= ⟨⟨L[u] , v ⟩⟩−⟨⟨f , v ⟩⟩
for all
v ∈V,
or, equivalently,
⟨u , L∗[v] ⟩−⟨⟨f , v ⟩⟩= 0
for all
v ∈V,
(10.61)
where L∗: V →U denotes the adjoint of the operator L, as deﬁned in (9.2). We will call
(10.61) the weak formulation of the original linear system.
So far we have not really done anything of substance, and, indeed, for linear systems
of algebraic equations, this more complicated characterization of solutions is of scant help.
However, this is no longer the case for diﬀerential equations, because, thanks to the integra-
tion by parts argument used to determine the adjoint operator, the solution u to the weak
form (10.61) is not restricted by the degree of smoothness required of a classical solution.
A simple example will illustrate the basic construction.
Example 10.11. On a bounded interval a ≤x ≤b, consider the elementary bound-
ary value problem
−d2u
dx2 = f(x),
u(a) = u(b) = 0.
The underlying vector space is U = { u(x) ∈C2[a, b] | u(a) = u(b) = 0 }. To obtain a
weak formulation, we multiply the diﬀerential equation by a test function v(x) ∈U and
integrate:
 b
a

−u′′(x) −f(x)

v(x) dx = 0.
(10.62)
The left-hand integral can be identiﬁed with the L2 inner product between the left-hand
side of the equation L[u] −f = −u′′ −f = 0 and the test function v.
According to
Lemma 10.10, condition (10.62) holds for all v(x) ∈U if and only if u(x) ∈U satisﬁes the
†
Shortly, as in the general framework developed in Chapter 9, V will be identiﬁed as the
target space of a linear operator L: U →V , and hence the choice of notation for its inner product.

10.4 Weak Solutions
429
boundary value problem. However, suppose that we integrate the ﬁrst term by parts once.
The boundary conditions on v imply that the boundary terms vanish, and the result is
 b
a

u′(x) v′(x) −f(x) v(x)

dx = 0.
(10.63)
A function u(x) that satisﬁes the latter integral condition for all smooth test functions v(x)
will be called a weak solution to the original boundary value problem. The key observation
is that the original diﬀerential equation, as well as the integral reformulation (10.62),
requires that u(x) be twice diﬀerentiable, whereas the weak version (10.63) requires only
that its ﬁrst derivative be deﬁned.
Of course, one need not stop at (10.63). Performing another integration by parts on
its ﬁrst term and invoking the boundary conditions on u produces
 b
a

−u(x) v′′(x) −f(x) v(x)

dx = 0.
(10.64)
Now u(x) need only be (piecewise) continuous in order that the integral be deﬁned —
keeping in mind that the test function v(x) is still required to be smooth. Equation (10.64)
is sometimes referred to as the fully weak formulation of the boundary value problem, while
the intermediate integral (10.63), in which the derivatives are evenly distributed among u
and v, is then known as the semi-weak formulation.
Remark: Recall also the Deﬁnition 6.5 of weak convergence, which similarly involves
integrating the standard convergence criterion against a suitable test function. Both are
part and parcel of a general weak analytical framework that plays an essential role in all
of modern advanced analysis, including partial diﬀerential equations.
The preceding example is a particular case of a general construction based on the
abstract formulation of self-adjoint linear systems in Chapter 9. Let L: U →V be a linear
map between inner product spaces, and let S = L∗◦L : U →U be the associated self-
adjoint operator. We further assume that ker L = {0}, which implies that S > 0 is positive
deﬁnite and, provided f ∈rng S, the associated linear system
S[u] = L∗◦L[u] = f
(10.65)
has a unique solution.
In order to construct a weak formulation of the linear system (10.65), we begin by
taking its inner product with a test function v ∈U, whereby
0 = ⟨S[u] −f , v ⟩= ⟨S[u] , v ⟩−⟨f , v ⟩= ⟨L∗◦L[u] , v ⟩−⟨f , v ⟩.
Integration by parts, as in the preceding example, amounts to moving the adjoint operator
so that it acts on the test function v, and in this manner we obtain the weak formulation
⟨⟨L[u] , L[v] ⟩⟩= ⟨f , v ⟩
for all
v ∈U,
(10.66)
where we use our usual notation conventions regarding the inner products on U and V .
Warning: Unlike the minimization principle (10.1), the weak formulation (10.66) does
not have a factor of 1
2 on the left-hand side. Since, in the applications treated here, L is
a diﬀerential operator of order, say, k, the weak formulation requires only that u ∈Ck
be k times diﬀerentiable, whereas, since S has order 2k, the classical formulation (10.65)
requires u ∈C2k to have twice as many derivatives.

430
10 Finite Elements and Weak Solutions
Similarly, the fully weak formulation involves an additional integration by parts, real-
ized in the abstract framework by moving the linear operator L acting on u so as to act
on the test element v, and so
⟨u , L∗◦L[v] ⟩= ⟨u , S[v] ⟩= ⟨f , v ⟩
for all
v ∈U.
(10.67)
In practice, it is often advantageous to restrict the class of test functions in order to
avoid technicalities involving smoothness and boundary behavior. This requires replacing
the simple argument used to establish Lemma 10.10 by a more sophisticated result, named
after the nineteenth-century German analyst Paul du Bois–Reymond.
Lemma 10.12. Let f(x) be a continuous function for a ≤x ≤b. Then
 b
a
f(x) v(x) dx = 0
for every C1 function v(x) with compact support in the open interval (a, b) if and only if
f(x) ≡0.
Proof : Suppose f(x0) > 0 for some a < x0 < b. Then, by continuity, f(x) > 0 for all
x in some interval a < x0 −ε < x < x0 +ε < b around x0. Choose v(x) to be a C1 function
that is atrictly positive in this interval and vanishes outside. An example is
v(x) =
 
(x −x0)2 −ε2 2,
| x −x0 | ≤ε,
0,
otherwise.
(10.68)
Then f(x) v(x) > 0 when | x −x0 | < ε and = 0 everywhere else. This implies
 b
a
f(x) v(x) dx =
 x0+ε
x0−ε
f(x) v(x) dx > 0,
which contradicts the original assumption. An analogous argument rules out f(x0) < 0 for
some a < x0 < b.
Q.E.D.
Finite Elements Based on Weak Solutions
To characterize weak solutions, one imposes the appropriate integral criterion on the entire
inﬁnite-dimensional space of smooth test functions. Thus, an evident approximation strat-
egy is to restrict the criterion to a suitable ﬁnite-dimensional subspace, thereby seeking an
approximate weak solution that belongs to the subspace.
More precisely, concentrating on the self-adjoint framework discussed at the end of the
preceding subsection, we restrict the weak formulation (10.66) of the linear system (10.65)
to a ﬁnite-dimensional subspace W ⊂U, and thus seek w ∈W such that
⟨⟨L[w] , L[v] ⟩⟩= ⟨f , v ⟩
for all
v ∈W.
(10.69)
In this fashion, we characerize the ﬁnite element approximation to the weak solution u as
the element w ∈W such that (10.69) holds for all v ∈W.
To analyze this condition, as in (10.3), we now specify a basis ϕ1, . . . , ϕn of W, and
thus can write both w and v as linear combinations thereof:
w = c1ϕ1 + · · · + cnϕn,
v = d1ϕ1 + · · · + dnϕn.

10.4 Weak Solutions
431
Substituting these expressions into (10.69) produces the bilinear function
B(c, d) =
n

i,j =1
kij cidj −
n

i=1
bidi = cTK d −bT d = (K c −b)Td = 0,
(10.70)
where
kij = ⟨⟨L[ϕi ] , L[ϕj ] ⟩⟩,
bi = ⟨f , ϕi ⟩,
i, j = 1, . . . , n,
(10.71)
are the same as our earlier speciﬁcations (10.6, 7), and we used the fact that KT = K
is a symmetric matrix to arrive at the ﬁnal expression in (10.70).
The condition that
(10.69) hold for all v ∈W is equivalent to the requirement that (10.70) hold for all
d = ( d1, d2, . . . , dn )T ∈Rn, which, in turn, implies that c = ( c1, c2, . . . , cn )T must satisfy
the linear system
K c = b.
But we immediately recognize that this is exactly the same as the ﬁnite element linear sys-
tem (10.9)! We therefore conclude that for a positive deﬁnite linear system constructed as
above, the weak ﬁnite element approximation to the solution is the same as the minimizing
ﬁnite element approximation. In other words, it does not matter whether we characterize
the solutions through the minimization principle or the weak reformulation; the resulting
ﬁnite element approximations are exactly the same. There is thus no need to present any
additional examples illustrating this construction.
In general, while the weak formulation is of much wider applicability, outside of bound-
ary value problems with well-deﬁned minimization principles, the rigorous underpinning
that guarantees that the numerical solution is close to the actual solution is harder to estab-
lish and, in fact, not always valid. Indeed, one can ﬁnd boundary value problems without
analytic solutions that have spurious ﬁnite element numerical solutions, and, conversely,
boundary value problems with solutions for which some ﬁnite element approximations do
not exist because the resulting coeﬃcient matrix is singular, [113, 126].
Shock Waves as Weak Solutions
Finally, let us return to our earlier analysis, in Section 2.3, of shock waves, but now in the
context of weak solutions. We begin by writing the nonlinear transport equation in the
conservative form
∂u
∂t + ∂
∂x
 1
2 u2 
= 0.
(10.72)
Since shock waves are discontinuous functions, they do not qualify as classical solutions.
However, they can be rigorously characterized as weak solutions, a formulation that will,
reassuringly, lead to the Rankine–Hugoniot Equal Area Rule for shock dynamics.
To construct a weak formulation of the nonlinear transport equation, we follow the
general framework, and hence begin by multiplying the equation (10.72) by a smooth test
function v(t, x) and integrating over a domain Ω ⊂R2:
 
Ω
 ∂u
∂t + ∂
∂x
 1
2 u2  
v(t, x) dt dx = 0.
(10.73)
As a direct consequence of the two-dimensional version of the du Bois–Reymond Lemma,
cf. Exercise 10.4.7, if u(t, x) ∈C1 and condition (10.73) holds for all C1 functions v(t, x)

432
10 Finite Elements and Weak Solutions
Ω+
Ω−
n+
n+
n−
n−
C
Figure 10.15.
Integration domain for weak shock-wave solution.
with compact support contained in Ω, then u(t, x) is necessarily a classical solution to the
partial diﬀerential equation (10.72). The next step is to integrate by parts in order to
remove the derivatives from u, and this is accomplished by appealing to Green’s formula
(6.82), which we rewrite in the form
 
Ω

u1
∂v
∂t + u2
∂v
∂x

dt dx =
&
∂Ω
(u · n) v ds −
 
Ω
 ∂u1
∂t + ∂u2
∂x

v dt dx,
(10.74)
where u = ( u1, u2 )T. In our case, we identify the integral in (10.73) with the left-hand
side of (10.74) by setting u1 = u, u2 = 1
2 u2. Since v has compact support, the boundary
integral vanishes, and thus we arrive at the weak formulation of the equation.
Deﬁnition 10.13. A function u(t, x) is said to be a weak solution to the nonlinear
transport equation (10.72) on Ω ⊂R2 if
 
Ω

u ∂v
∂t + 1
2 u2 ∂v
∂x

dt dx = 0
(10.75)
for all C1 functions v(t, x) with compact support: supp v ⊂Ω.
The key point is that, in the weak formulation (10.75), the derivatives are acting solely
on v(t, x), which we assume to be smooth, and not on our prospective solution u(t, x), which
now need not even be continuous for the integral to be well deﬁned.
Let us derive the Rankine–Hugoniot shock condition (2.53) as a consequence of the
weak formulation. Suppose u(t, x) is a weak solution, deﬁned on a domain Ω ⊂R2, that
has a single jump discontinuity along a curve C parametrized by x = σ(t) that separates
Ω into two subdomains, say Ω+ and Ω−, such that its restriction to either subdomain,
denoted by u+ = u | Ω+ and u−= u | Ω−, are each classical solutions on their respective
domains, while the separating curve C = {x = σ(t)} represents a shock-wave discontinuity.
For speciﬁcity, we assume that Ω+ lies above and Ω−lies below C in the (t, x)–plane; see
Figure 10.15.
Let us investigate what the preceding weak formulation implies in this
situation.
We split the integral (10.75) into two parts, and then apply the integration
by parts formula (10.74) to each individual double integral, keeping in mind that, when

10.4 Weak Solutions
433
restricted to Ω+ or Ω−, the integrand is suﬃciently smooth to justify application of the
formula:
0 =
 
Ω

u ∂v
∂t + 1
2 u2 ∂v
∂x

dt dx
=
 
Ω+

u+
∂v
∂t + 1
2 u2
+
∂v
∂x

dt dx +
 
Ω−

u−
∂v
∂t + 1
2 u2
−
∂v
∂x

dt dx
=
&
∂Ω+
(u+ · n+) v ds −
 
Ω+
 ∂u+
∂t + ∂
∂x
 1
2 u2
+
 
v dt dx +
+
&
∂Ω−
(u−· n−) v ds −
 
Ω−
 ∂u−
∂t + ∂
∂x
 1
2 u2
−
 
v dt dx
=

C
(u+ · n+ + u−· n−) v ds.
Here
u+ =
 u+
1
2 u2
+

,
u−=
 u−
1
2 u2
−

,
while n+, n−are the unit outwards normals on, respectively, ∂Ω+ and ∂Ω−. The ﬁnal
equality follows from the fact that the support of v is contained strictly inside Ω, and
hence vanishes on those parts of the boundaries of Ω+ and Ω−that do not lie on the
curve C. In particular, since C is the graph of x = σ(t), the unit normals along it are,
respectively,
n+ =
1

1 +
dσ
dt
2
⎛
⎝
dσ
dt
−1
⎞
⎠,
n−= −n+ =
1

1 +
dσ
dt
2
⎛
⎝−dσ
dt
1
⎞
⎠,
keeping in mind our convention that Ω+ lies above and Ω−lies below C, while
ds =

1 +
dσ
dt
2
dt.
Thus, the ﬁnal line integral reduces to

C

(u−−u+) dσ
dt −1
2 (u2
−−u2
+)

v dt = 0.
(10.76)
Since (10.76) vanishes for all C1 functions v(t, x) with compact support, the du Bois–
Reymond Lemma 10.12 implies that
(u−−u+) dσ
dt = 1
2 (u2
−−u2
+)
on
C,
thereby re-establishing the Rankine–Hugoniot shock condition (2.53). The upshot is that
the shock-wave solutions produced in Section 2.3 are bona ﬁde weak solutions.
Another computation shows that the rarefaction wave (2.54) also qualiﬁes as a weak
solution.
However, so does the non-physical reverse shock solution discussed in Exam-
ple 2.11. Thus, although the weak formulation recovers the Rankine–Hugoniot condition,
it does not address the problem of causality, which must be additionally imposed to single

434
10 Finite Elements and Weak Solutions
out a unique, physically meaningful weak solution. Further developments of these ideas
can be found in more advanced monographs, e.g., [107, 122].
Exercises
10.4.1. Write out semi-weak and fully weak formulations for the following boundary value prob-
lems:
(a) −u′′ + 2u = x −x2, u(0) = u(1) = 0;
(b) ex u′′ + u = cos x, u′(0) = u′(2) = 0;
(c) xu′′ + u′ + xu = 0, u(1) = u(2) = 0.
10.4.2.(a) Write down a weak formulation for the boundary value problem −u′′ + 3u = x,
u(0) = u(1) = 0.
(b) Based on your weak formulation, construct a ﬁnite element approxi-
mation to the solution, using n = 10 nodes.
10.4.3.(a) Write down a weak formulation of the transport equation ut + 3ux = 0 on the real
line.
(b) Solve the initial value problem u(0, x) =
 1 −| x |,
| x | ≤1,
0,
otherwise.
(c) Explain why the result of part (b) is not a classical solution to the wave equation. Is it
a weak solution according to your formulation in part (a)?
10.4.4.(a) Write down a semi-weak formulation of the wave equation utt = 4uxx on the real
line.
(b) Solve the initial value problem u(0, x) = ρ(x), ut(0, x) = 0, where the initial
displacement is a ramp function (6.25).
(c) Explain why the result of part (b) is not a
classical solution to the wave equation. Does it satisfy the semi-weak formulation of part
(a)? Explain your answer.
♦10.4.5.(a) Starting with the nonlinear transport equation written in the alternative conserva-
tive form (2.56), ﬁnd a corresponding weak formulation.
(b) Prove that your weak formulation produces the alternative entropy condition (2.58) for
the motion of a shock discontinuity.
♦10.4.6. Prove that the du Bois–Reymond Lemma 10.12 remains valid even when v(x) ∈C∞is
required to be inﬁnitely diﬀerentiable.
♦10.4.7. The Two-dimensional du Bois–Reymond Lemma: Let Ω ⊂R2 be a domain, and f(t, x)
a continuous function deﬁned thereon. Prove that
		
Ω f(t, x) v(t, x) dt dx = 0 for every C1
function v(t, x) with compact support in Ω if and only if f(t, x) ≡0.
♠10.4.8.(a) Investigate the ability of ﬁnite elements to approximate a solution to the
non-positive-deﬁnite boundary value problem Δu + λ u = 0, 0 < x < π, 0 < y < π,
u(x, 0) = 1, u(x, π) = u(0, y) = u(π, y) = 0, when (i) λ = 1, (ii) λ = 2. Use separation of
variables to ﬁnd a series solution and use it to determine the accuracy of your ﬁnite element
solution in part (a).

Chapter 11
Dynamics of Planar Media
In previous chapters, we studied the equilibrium conﬁgurations of planar media — plates
and membranes — governed by the two-dimensional Laplace and Poisson equations. In
this chapter, we analyze their dynamics, modeled by the two-dimensional heat and wave
equations. The heat equation describes diﬀusion of, say, heat energy in a thin metal plate,
an animal population dispersing over a region, or a pollutant spreading out into a shallow
lake. The wave equation models small vibrations of a two-dimensional membrane such as a
drum. Since both equations ﬁt into the general framework for dynamics that we established
in Section 9.5, their solutions share many of the general qualitative and analytic properties
possessed by their respective one-dimensional counterparts.
Although the increase in dimension may tax our analytical prowess, we have, in fact,
already mastered the principal solution techniques: separation of variables, eigenfunction
series, and fundamental solutions. When applied to partial diﬀerential equations in higher
dimensions, separation of variables in curvilinear coordinates often leads to new linear,
but non-constant-coeﬃcient, ordinary diﬀerential equations, whose solutions are no longer
elementary functions. Rather, they are expressed in terms of a variety of important special
functions, which include the error and Airy functions we encountered earlier; the Bessel
functions, which play a starring role in the present chapter; and the Legendre and Ferrers
functions, spherical harmonics, and spherical Bessel functions arising in three-dimensional
problems. Special functions are ubiquitous in more advanced applications in physics, chem-
istry, mechanics, and mathematics, and, over the last two hundred and ﬁfty years, many
prominent mathematicians have devoted signiﬁcant eﬀort to establishing their fundamen-
tal properties, to the extent that they are now, by and large, well understood, [86]. To
acquire the requisite familiarity with special functions, in preparation for employing them
to solve higher-dimensional partial diﬀerential equations, we must ﬁrst learn basic series
solution techniques for linear second-order ordinary diﬀerential equations.
11.1 Diﬀusion in Planar Media
As we learned in Chapter 4, the equilibrium temperature u(x, y) of a thin, uniform, isotropic
plate is governed by the two-dimensional Laplace equation
Δu = uxx + uyy = 0.
Working by analogy, the dynamical diﬀusion of the plate’s temperature should be modeled
by the two-dimensional heat equation
ut = γ Δu = γ (uxx + uyy).
(11.1)
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
435
11
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

436
11 Dynamics of Planar Media
The coeﬃcient γ > 0, assumed constant, measures the relative speed of diﬀusion of heat
energy throughout the plate; its positivity is required on physical grounds, and also serves
to avoid ill-posedness inherent in running diﬀusion processes backwards in time. In this
model, we are assuming that the plate is uniform and isotropic, and experiences no loss of
heat or external heat sources other than at its edge — which can be arranged by covering
its top and bottom with insulation.
The solution u(t, x) = u(t, x, y) to the heat equation measures the temperature, at
time t, at each point x = (x, y) in the (bounded) domain Ω ⊂R2 occupied by the plate.
To uniquely specify the solution u(t, x, y), we must impose suitable initial and boundary
conditions. The initial data is the temperature of the plate
u(0, x, y) = f(x, y),
(x, y) ∈Ω,
(11.2)
at an initial time, which for simplicity, we take to be t0 = 0. The most important boundary
conditions are as follows:
• Dirichlet boundary conditions: Specifying
u = h
on
∂Ω
(11.3)
ﬁxes the temperature along the edge of the plate.
• Neumann boundary conditions: Let n be the unit outwards normal on the boundary
of the domain. Specifying the normal derivative of the temperature,
∂u
∂n = k
on
∂Ω,
(11.4)
eﬀectively prescribes the heat ﬂux along the boundary. Setting k = 0 corresponds
to an insulated boundary.
• Mixed boundary conditions: More generally, we can impose Dirichlet conditions on
part of the boundary D ⊊∂Ω and Neumann conditions on its complement N =
∂Ω \ D. For instance, homogeneous mixed boundary conditions
u = 0
on
D,
∂u
∂n = 0
on
N,
(11.5)
correspond to freezing a portion of the boundary and insulating the remainder.
• Robin boundary conditions:
∂u
∂n + β u = τ
on
∂Ω,
(11.6)
where the edge of the plate sits in a heat bath at temperature τ.
Under reasonable assumptions on the domain, the initial data, and the boundary data,
a general theorem, [34, 38, 99], guarantees the existence of a unique solution u(t, x, y) to
any of these initial-boundary value problems for all subsequent times t > 0. Our practical
goal is to both compute and understand the behavior of the solution in speciﬁc situations.
Derivation of the Diﬀusion and Heat Equations
The physical derivation of the two-dimensional (and three-dimensional) heat equation relies
on the same basic thermodynamic laws that were used, in Section 4.1, to establish the
one-dimensional version. The ﬁrst principle is that heat energy ﬂows from hot to cold as

437
rapidly as possible. According to multivariable calculus, [8, 108], the negative temperature
gradient −∇u points in the direction of the steepest decrease in the temperature function
u at a point, and so heat energy will ﬂow in that direction. Therefore, the heat ﬂux vector
w, which measures the magnitude and direction of the ﬂow of heat energy, should be
proportional to the temperature gradient:
w(t, x, y) = −κ(x, y) ∇u(t, x, y).
(11.7)
The scalar quantity κ(x, y) > 0 measures the thermal conductivity of the material, so
(11.7) is the multi-dimensional form of Fourier’s Law of Cooling (4.5). We are assuming
that the thermal conductivity depends only on the position (x, y) ∈Ω, which means that
the material in the plate
(a) is not changing in time;
(b) is isotropic, meaning that its thermal conductivity is the same in all directions;
(c) and, moreover, its thermal conductivity is not aﬀected by any change in tempera-
ture.
Dropping either assumption (b) or (c) would result in a considerably more challenging
nonlinear diﬀusion equation.
The second thermodynamic principle is that, in the absence of external heat sources,
heat can enter any subregion R ⊂Ω only through its boundary ∂R. (Keep in mind that
the plate is insulated from above and below.) Let ε(t, x, y) denote the heat energy density
at each time and point in the domain, so that
HR(t) =
 
R
ε(t, x, y) dx dy
represents the total heat energy contained within the subregion R at time t. The amount
of additional heat energy entering R at a boundary point x ∈∂R is given by the normal
component of the heat ﬂux vector, namely −w·n, where, as always, n denotes the outward
unit normal to the boundary ∂R. Thus, the total heat ﬂux entering the region R is ob-
tained by integration along the boundary of R, resulting in the line integral −
&
∂R
w · n ds.
Equating the rate of change of heat energy to the heat ﬂux yields
dHR
dt
=
 
R
∂ε
∂t (t, x, y) dx dy = −
&
∂R
w · n ds = −
 
R
∇· w dx dy,
where we applied the divergence form of Green’s Theorem, (6.80), to convert the ﬂux line
integral into a double integral. Thus,
 
R
 ∂ε
∂t + ∇· w

dx dy = 0.
(11.8)
Keep in mind that this result must hold for any subdomain R ⊂Ω. Now, according to
Exercise 11.1.13, the only way in which an integral of a continuous function can vanish for
all subdomains is if the integrand is identically zero, and so
∂ε
∂t + ∇· w = 0.
(11.9)
In this manner, we arrive at the basic conservation law relating the heat energy density ε
and the heat ﬂux vector w.
11.1 Diﬀusion in Planar Media

438
11 Dynamics of Planar Media
As in our one-dimensional model, cf. (4.3), the heat energy density ε(t, x, y) is propor-
tional to the temperature, so
ε(t, x, y) = σ(x, y) u(t, x, y),
where
σ(x, y) = ρ(x, y) χ(x, y)
(11.10)
is the product of the density ρ and the speciﬁc heat capacity χ of the material at the
point (x, y) ∈Ω. Combining this with the Fourier Law (11.7) and the energy balance
equation (11.10) leads to the general two-dimensional diﬀusion equation
∂u
∂t = 1
σ ∇·

κ ∇u

(11.11)
governing the thermodynamics of an isotropic medium in the absence of external heat
sources or sinks. In full detail, this second-order partial diﬀerential equation is
∂u
∂t =
1
σ(x, y)
 ∂
∂x

κ(x, y) ∂u
∂x

+ ∂
∂y

κ(x, y) ∂u
∂y
 
.
(11.12)
Such diﬀusion equations are also used to model movements of populations, e.g., bacte-
ria in a petri dish or wolves in the Canadian Rockies, [81, 84]. Here the solution u(t, x, y)
represents the population density at position (x, y) at time t, which diﬀuses over the do-
main due to random motions of the individuals.
Similar diﬀusion processes model the
mixing of solutes in liquids, with the diﬀusion induced by the random Brownian motion
from molecular collisions. More generally, diﬀusion processes in the presence of chemical
reactions and convection due to ﬂuid motion are modeled by the more general class of
reaction-diﬀusion and convection-diﬀusion equations, [107].
In particular, if the body (or the environment or the solvent) is uniform, then both
σ and κ are constant, and so (11.11) reduces to the heat equation (11.1) with thermal
diﬀusivity
γ = κ
σ = κ
ρ χ .
(11.13)
Both the heat and more general diﬀusion equations are examples of parabolic partial dif-
ferential equations, the terminology being adapted from Deﬁnition 4.12 to apply to partial
diﬀerential equations in more than two variables. As we will see, all the basic qualitative
features of solutions to the one-dimensional heat equation carry over to parabolic partial
diﬀerential equations in higher dimensions.
Indeed, the general diﬀusion equation (11.12) can be readily ﬁt into the self-adjoint
dynamical framework of Section 9.5, taking the form
ut = −∇∗◦∇u.
(11.14)
The gradient operator ∇maps scalar ﬁelds u to vector ﬁelds v = ∇u; its adjoint ∇∗, which
goes in the reverse direction, is taken with respect to the weighted inner products
⟨u , u ⟩=
 
Ω
u(x, y) u(x, y) σ(x, y) dx dy,
⟨⟨v , v ⟩⟩=
 
Ω
v(x, y) · v(x, y) κ(x, y) dx dy,
(11.15)
between, respectively, scalar and vector ﬁelds. As in (9.33), a straightforward integration
by parts tells us that
∇∗v = −1
σ ∇· (κ v) = −1
σ
 ∂(κ v1)
∂x
+ ∂(κ v2)
∂y

,
when
v =

v1
v2

.
(11.16)

11.1 Diﬀusion in Planar Media
439
Therefore, the right-hand side of (11.14) equals
−∇∗◦∇u = 1
σ ∇· (κ ∇u),
(11.17)
which thereby recovers the general diﬀusion equation (11.11). As always, the validity of
the adjoint formula (11.16) rests on the imposition of suitable homogeneous boundary
conditions: Dirichlet, Neumann, mixed, or Robin.
In particular, to obtain the heat equation, we take σ and κ to be constant, and so
the inner products (11.15) reduce, up to a constant factor, to the usual L2 inner products
between scalar and vector ﬁelds. In this case, the adjoint of the gradient is, up to a scale
factor, minus the divergence: ∇∗= −γ ∇· , where γ = κ/σ. In this scenario, (11.14)
reduces to the two-dimensional heat equation (11.1).
Separation of Variables
Let us now discuss analytical solution techniques. According to Section 9.5, the separable
solutions to any linear evolution equation
ut = −S[u]
(11.18)
are of exponential form
u(t, x, y) = e−λt v(x, y).
(11.19)
Since the linear operator S involves diﬀerentiation with respect to only the spatial variables
x, y, we obtain
∂u
∂t = −λ e−λt v(x, y),
while
S[u] = e−λt S[v].
Substituting back into the diﬀusion equation (11.18) and canceling the exponentials, we
conclude that
S[v] = λ v.
(11.20)
Thus, v(x, y) must be an eigenfunction for the linear operator S, subject to the relevant
homogeneous boundary conditions.
In the case of the heat equation (11.1),
S[u] = −γ Δu,
and hence, as in Example 9.40, the eigenvalue equation (11.20) is the two-dimensional
Helmholtz equation
γ Δv + λ v = 0,
or, in detail,
γ
 ∂2v
∂x2 + ∂2v
∂y2

+ λ v = 0.
(11.21)
According to Theorem 9.34, self-adjointness implies that the eigenvalues are all real and
nonnegative: λ ≥0.
In the positive deﬁnite cases — Dirichlet and mixed boundary
conditions — they are strictly positive, while the Neumann boundary value problem admits
a zero eigenvalue λ0 = 0 corresponding to the constant eigenfunction v0(x, y) ≡1.
Let us index the eigenvalues in increasing order:
0 < λ1 ≤λ2 ≤λ3 ≤· · · ,
(11.22)

440
11 Dynamics of Planar Media
repeated according to their multiplicities, where λ0 = 0 is an eigenvalue only in the Neu-
mann case, and λk →∞as k →∞. For each eigenvalue λk, let vk(x, y) be an independent
eigenfunction. The corresponding separable solution is
uk(t, x, y) = e−λk t vk(x, y).
Those corresponding to positive eigenvalues are exponentially decaying in time, while a
zero eigenvalue produces a constant solution u0(t, x, y) ≡1. The general solution to the
homogeneous boundary value problem can then be built up as an inﬁnite series in these
basic eigensolutions
u(t, x, y) =
∞

k=1
ck uk(t, x, y) =
∞

k=1
ck e−λk t vk(x, y).
(11.23)
The coeﬃcients ck are prescribed by the initial conditions, which require
∞

k=1
ck vk(x, y) = f(x, y).
(11.24)
Since S is self-adjoint, Theorem 9.33 guarantees orthogonality† of the eigenfunctions under
the L2 inner product on the domain Ω:
⟨vj , vk ⟩=
 
Ω
vj(x, y) vk(x, y) dx dy = 0,
j ̸= k.
(11.25)
As a consequence, the coeﬃcients in (11.24) are given by the standard orthogonality formula
(9.104), namely
ck = ⟨f , vk ⟩
∥vk ∥2 =
 
Ω
f(x, y) vk(x, y) dx dy
 
Ω
vk(x, y)2 dx dy
.
(11.26)
(For the more general diﬀusion equation (11.11), one uses the appropriately weighted inner
product.) The exponential decay of the eigenfunction coeﬃcients implies that the resulting
eigensolution series (11.23) converges and thus produces the solution to the initial-boundary
value problem for the diﬀusion equation. See [34; p. 369] for a precise statement and proof
of the general theorem.
Qualitative Properties
Before tackling examples in which we are able to construct explicit formulas for the eigen-
functions and eigenvalues, let us see what the eigenfunction series solution (11.23) can
tell us about general diﬀusion processes. Based on our experience with the case of a one-
dimensional bar, the ﬁnal conclusions will not be especially surprising. Indeed, they also
apply, word for word, to diﬀusion processes in three-dimensional solid bodies. A reader who
is impatient to see the explicit formulas may wish to skip ahead to the following section,
returning here as needed.
†
As usual, in the case of a repeated eigenvalue, one chooses an orthogonal basis of the
associated eigenspace to ensure orthogonality of all the basis eigenfunctions.

11.1 Diﬀusion in Planar Media
441
Keep in mind that we are still dealing with the solution to the homogeneous boundary
value problem. The ﬁrst observation is that all terms in the series solution (11.23), with the
possible exception of a null eigenfunction term that appears in the semi-deﬁnite Neumann
case, are tending to zero exponentially fast. Since most eigenvalues are large, all the higher-
order terms in the series become almost instantaneously negligible, and hence the solution
can be accurately approximated by a ﬁnite sum over the ﬁrst few eigenfunction modes.
As time goes on, more and more of the modes can be neglected, and the solution decays
to thermal equilibrium at an exponentially fast rate. The rate of convergence to thermal
equilibrium is, for most initial data, governed by the smallest positive eigenvalue λ1 > 0
for the Helmholtz boundary value problem on the domain.
In the positive deﬁnite cases of homogeneous Dirichlet or mixed boundary conditions,
thermal equilibrium is u(t, x, y) →u⋆(x, y) ≡0. Here, the equilibrium temperature is equal
to the zero boundary temperature — even if this temperature is ﬁxed on only a small part
of the boundary. The initial heat is eventually dissipated away through the uninsulated
part of the boundary. In the semi-deﬁnite Neumann case, corresponding to a completely
insulated plate, the general solution has the form
u(t, x, y) = c0 +
∞

k=1
ck e−λkt vk(x, y),
(11.27)
where the sum is over the positive eigenmodes, λk > 0. Since all the summands are expo-
nentially decaying, the ﬁnal equilibrium temperature u⋆= c0 is the same as the constant
term in the eigenfunction expansion. We evaluate this term using the orthogonality formula
(11.26), and so, as t →∞,
u(t, x, y) −→c0 = ⟨f , 1 ⟩
∥1 ∥2
=
 
Ω
f(x, y) dx dy
 
Ω
dx dy
=
1
area Ω
 
Ω
f(x, y) dx dy. (11.28)
We conclude that the equilibrium temperature is equal to the average initial temperature
distribution. Thus, when the plate is fully insulated, the heat energy cannot escape, and
instead redistributes itself in a uniform manner over the domain.
Diﬀusion has a smoothing eﬀect on the initial temperature distribution f(x, y). As-
sume that the eigenfunction coeﬃcients are uniformly bounded, so | ck | ≤M for some
constant M. This will certainly be the case if f(x, y) is piecewise continuous or, more gen-
erally, belongs to L2, since Bessel’s inequality, (3.117), which holds for general orthogonal
systems, implies that ck →0 as k →∞. Many distributions, including delta functions,
also have bounded Fourier coeﬃcients. Then, at any time t > 0 after the initial instant,
the coeﬃcients ck e−λkt in the eigenfunction series solution (11.23) are exponentially small
as k →∞, which is enough to ensure smoothness of the solution u(t, x, y) for each t > 0.
Therefore, the diﬀusion process serves to immediately smooth out jumps, corners, and
other discontinuities in the initial data. As time progresses, the local variations in the so-
lution become less and less pronounced, as it asymptotically reaches a constant equilibrium
state.
As a result, diﬀusion processes can be eﬀectively applied to smooth and denoise planar
images. The initial data u(0, x, y) = f(x, y) represents the gray-scale value of the image at
position (x, y), so that 0 ≤f(x, y) ≤1, with 0 representing black and 1 representing white.
As time progresses, the solution u(t, x, y) represents a more and more smoothed version

442
11 Dynamics of Planar Media
            
            
            
            
Figure 11.1.
Smoothing a gray scale image.
of the image. Although this has the eﬀect of removing unwanted high-frequency noise,
there is also a gradual blurring of the actual features. Thus, the “time” or “multiscale”
parameter t needs to be chosen to optimally balance between the two eﬀects — the larger
t is the more noise is removed, but the more noticeable the blurring. A representative
illustration appears in Figure 11.1. The blurring aﬀects small-scale features ﬁrst, then,
gradually, those at larger and larger scales, until eventually the entire image is blurred to
a uniform gray. To further suppress undesirable blurring eﬀects, modern image-processing
ﬁlters are based on anisotropic (and thus nonlinear) diﬀusion equations; see [100] for a
survey of recent progress in this active ﬁeld.
Since the forward heat equation eﬀectively blurs the features in an image, we might be
tempted to reverse “time” in order to sharpen the image. However, the argument presented
in Section 4.1 tells us that the backwards heat equation is ill-posed, and hence cannot be
used directly for this purpose. Various “regularization” strategies have been devised to
circumvent this mathematical barrier, and thereby design eﬀective image enhancement
algorithms, [46].
Inhomogeneous Boundary Conditions and Forcing
Let us next brieﬂy discuss how to incorporate inhomogeneous boundary conditions and
external heat sources into the general solution framework. Consider, as a speciﬁc example,
the forced heat equation
ut = γ Δu + F(x, y)
for
(x, y) ∈Ω,
(11.29)
where F(x, y) represents an unvarying external heat source or sink, subject to inhomoge-
neous Dirichlet boundary conditions
u(x, y) = h(x, y)
for
(x, y) ∈∂Ω,
(11.30)
that ﬁxes the temperature of the plate on its boundary. When the external forcing does
not vary in time, we expect the solution to eventually settle down to an equilibrium con-
ﬁguration: u(t, x, y) →u⋆(x, y) as t →∞. This will be justiﬁed below.
The time-independent equilibrium temperature u⋆(x, y) satisﬁes the equation obtained
by setting ut = 0 in the evolution equation (11.29), which reduces it to the Poisson equation
−γ Δu⋆= F
for
(x, y) ∈Ω.
(11.31)
The equilibrium solution is subject to the same inhomogeneous Dirichlet boundary condi-
tions (11.30). Positive deﬁniteness of the Dirichlet boundary value problem implies that

11.1 Diﬀusion in Planar Media
443
there is a unique equilibrium solution, which can be characterized as the sole minimizer of
the associated Dirichlet principle; for details see Section 9.3.
With the equilibrium solution in hand, we let
v(t, x, y) = u(t, x, y) −u⋆(x, y)
measure the deviation of the dynamical solution u from its eventual equilibrium.
By
linearity v(t, x, y) satisﬁes the unforced heat equation subject to homogeneous boundary
conditions:
vt = γ Δv,
(x, y) ∈Ω,
v = 0,
(x, y) ∈∂Ω.
(11.32)
Therefore, v can be expanded in an eigenfunction series (11.23), and will decay to zero,
v(t, x, y) →0, at an exponentially fast rate prescribed by the smallest eigenvalue λ1 of
the associated homogeneous Helmholtz boundary value problem. (Special initial data can
decay at a faster rate, prescribed by a larger eigenvalue.) Consequently, the solution to the
forced inhomogeneous problem (11.29–30) will approach thermal equilibrium,
u(t, x, y) = v(t, x, y) + u⋆(x, y) −→u⋆(x, y),
at exactly the same exponential rate as its homogeneous counterpart.
The Maximum Principle
Finally, let us state and prove the (Weak) Maximum Principle for the two-dimensional
heat equation. As in the one-dimensional situation described in Section 8.3, it states that
the maximum temperature in a body that is either insulated or having heat removed from
its interior must occur either at the initial time or on its boundary. Observe that there are
no conditions imposed on the boundary temperatures.
Theorem 11.1. Suppose u(t, x, y) is a solution to the forced heat equation
ut = γ Δu + F(t, x, y),
for
(x, y) ∈Ω,
0 < t < c,
where Ω is a bounded domain, and γ > 0. Suppose F(t, x, y) ≤0 for all (x, y) ∈Ω and
0 ≤t ≤c. Then the global maximum of u on the set { (t, x, y) | (x, y) ∈Ω, 0 ≤t ≤c }
occurs either when t = 0 or at a boundary point (x, y) ∈∂Ω.
Proof : First, let us prove the result under the assumption that F(t, x, y) < 0
everywhere.
At a local interior maximum, ut = 0, and, since its Hessian matrix
∇2u =

uxx
uxy
uxy
uyy

must be negative semi-deﬁnite, both diagonal entries uxx, uyy ≤0
there. This would imply that ut −γ Δu ≥0, resulting in a contradiction. If the maximum
were to occur when t = c, then ut ≥0 there, and also uxx, uyy ≤0, leading again to a
contradiction.
To generalize to the case F(t, x, y) ≤0, which includes the heat equation when
F(t, x, y) ≡0, set
v(t, x, y) = u(t, x, y) + ε (x2 + y2),
where
ε > 0.
Then,
∂v
∂t = γ Δv −4γ ε + F(t, x, y) = γ Δv + F(t, x, y),

444
11 Dynamics of Planar Media
where
F(t, x, y) = F(t, x, y) −4γ ε < 0.
Thus, by the previous paragraph, the maximum of v occurs either when t = 0 or at a
boundary point (x, y) ∈∂Ω.
We then let ε →0 and conclude the same for u.
More
precisely, let u(t, x, y) ≤M on t = 0 or (x, y) ∈∂Ω. Then
v(t, x, y) ≤M + C ε,
where
C = max
-
x2 + y2  (x, y) ∈∂Ω
.
< ∞,
since Ω is a bounded domain. Thus,
u(t, x, y) ≤v(t, x, y) ≤M + C ε.
Letting ε →0 proves that u(t, x, y) ≤M at all (x, y) ∈Ω, 0 ≤t ≤c, which completes the
proof.
Q.E.D.
Remark: The preceding proof can be readily adapted to general diﬀusion equations
(11.12) — assuming that the coeﬃcients σ, κ remain strictly positive throughout the do-
main.
Exercises
11.1.1. A homogeneous, isotropic circular metal disk of radius 1 meter has its entire boundary
insulated. The initial temperature at a point is equal to the distance of the point from the
center. Formulate an initial-boundary value problem governing the disk’s subsequent tem-
perature dynamics. What is the eventual equilibrium temperature of the disk?
11.1.2. A homogeneous, isotropic, circular metal disk of radius 2 cm has half its boundary ﬁxed
at 100◦and the other half insulated. Given a prescribed initial temperature distribution,
set up the initial-boundary value problem governing its subsequent temperature proﬁle.
What is the eventual equilibrium temperature of the disk? Does your answer depend on
the initial temperature?
11.1.3. Given the initial temperature distribution f(x, y) = xy (1 −x)(1 −y) on the unit square
Ω = {0 ≤x, y ≤1}, determine the equilibrium temperature when subject to homogeneous
(a) Dirichlet boundary conditions; (b) Neumann boundary conditions.
11.1.4. A square plate with side lengths 1 meter has its right and left edges insulated, its top
edge held at 100◦, and its bottom edge held at 0◦. Assuming that the plate is made out of
a homogeneous, isotropic material, formulate an appropriate initial-boundary value prob-
lem describing the temperature dynamics of the plate. Then ﬁnd its eventual equilibrium
temperature.
11.1.5. A square plate with side lengths 1 meter has initial temperature 5◦throughout, and
evolves subject to the Neumann boundary conditions ∂u/∂n = 1 on its entire boundary.
What is the eventual equilibrium temperature?
♥11.1.6. Let u(t, x, y) be a solution to the heat equation on a bounded domain Ω subject to
homogeneous Neumann conditions on its boundary ∂Ω. (a) Prove that the total heat
H(t) =
		
Ω u(t, x, y) dx dy is conserved, i.e., is constant in time. (b) Use part (a) to prove
that the eventual equilibrium solution is everywhere equal to the average of the initial tem-
perature u(0, x, y). (c) What can you say about the behavior of the total heat for the
homogeneous Dirichlet boundary value problem?
(d) What about an inhomogeneous
Dirichlet boundary value problem?

11.2 Explicit Solutions of the Heat Equation
445
11.1.7. Let u(t, x, y) be a nonconstant solution to the heat equation on a connected, bounded
domain Ω subject to homogeneous Dirichlet boundary conditions on ∂Ω. (a) Prove that its
L2 norm N(t) =
		
Ω u(t, x, y)2 dx dy is a strictly decreasing function of t. (b) Is this also
true for mixed boundary conditions? (c) For Neumann boundary conditions?
11.1.8. Are the conclusions in Exercises 11.1.6 and 11.1.7 valid for the general diﬀusion equa-
tion (11.12)?
♦11.1.9. Write out the eigenvalue equation governing the separable solutions to the general dif-
fusion equation (11.11), subject to appropriate boundary conditions. Given a complete sys-
tem of eigenfunctions, write down the eigenfunction series solution to the initial value prob-
lem u(0, x, y) = f(x, y), including the formulas for the coeﬃcients.
11.1.10. True or false: The equilibrium temperature of a fully insulated nonuniform plate whose
thermodynamics are governed by the general diﬀusion equation (11.12) equals the average
initial temperature.
11.1.11. Let α > 0, and consider the initial-boundary value problem ut = Δu−α u, u(0, x, y) =
f(x, y) on a bounded domain Ω ⊂R2, with boundary conditions ∂u/∂n = 0 on ∂Ω.
(a) Write the equation in self-adjoint form (9.122). Hint: Look at Exercise 9.3.26.
(b) Prove that the problem has a unique equilibrium solution.
11.1.12. Write each of the following linear evolution equations in the self-adjoint form (9.122)
by choosing suitable inner products and a suitable set of homogeneous boundary conditions.
Is the operator you construct positive deﬁnite?
(a) ut = uxx + uyy −u,
(b) ut = y uxx + x uyy,
(c) ut = Δ2u.
♦11.1.13. Prove that if f(x, y) is continuous and
		
R f(x, y) dx dy = 0 for all R ⊂Ω, then
f(x, y) ≡0 for (x, y) ∈Ω. Hint: Adapt the method in Exercise 6.1.23.
11.2 Explicit Solutions of the Heat Equation
Solving the two-dimensional heat equation in series form requires knowing the eigenfunc-
tions for the associated Helmholtz boundary value problem. Unfortunately, as with the
vast majority of partial diﬀerential equations, explicit solution formulas are few and far
between. In this section, we discuss two speciﬁc cases in which the required eigenfunctions
can be found in closed form. The calculations rely on a further separation of variables,
which, as we know, works in only a very limited class of domains. Nevertheless, interesting
solution features can be gleaned from these particular geometries.
The ﬁrst example is a rectangular domain, and the eigensolutions can be expressed in
terms of elementary functions — trigonometric functions and exponentials. We then study
the heating of a circular disk. In this case, the eigenfunctions are no longer elementary
functions, but, rather, are expressed in terms of Bessel functions. Understanding their
basic properties will require us to take a detour to develop the fundamentals of power
series solutions to ordinary diﬀerential equations.
Heating of a Rectangle
A homogeneous rectangular plate
R =
-
0 < x < a, 0 < y < b
.

446
11 Dynamics of Planar Media
is heated to a prescribed initial temperature,
u(0, x, y) = f(x, y),
for
(x, y) ∈R.
(11.33)
Then its top and bottom are insulated, while its sides are held at zero temperature. Our
task is to understand the thermodynamic evolution of the plate’s temperature.
The temperature u(t, x, y) evolves according to the two-dimensional heat equation
ut = γ(uxx + uyy),
for
(x, y) ∈R,
t > 0,
(11.34)
where γ > 0 is the plate’s thermal diﬀusivity, while subject to homogeneous Dirichlet
conditions along the boundary of the rectangle at all subsequent times:
u(t, 0, y) = u(t, a, y) = u(t, x, 0) = u(t, x, b) = 0,
0 < x < a,
0 < y < b,
t > 0.
(11.35)
As in (11.19), the eigensolutions to the heat equation are obtained from the usual expo-
nential ansatz u(t, x, y) = e−λt v(x, y). Substituting this expression into the heat equation,
we conclude that the function v(x, y) solves the Helmholtz eigenvalue problem
γ(vxx + vyy) + λ v = 0,
(x, y) ∈R,
(11.36)
subject to the same homogeneous Dirichlet boundary conditions:
v(0, y) = v(a, y) = v(x, 0) = v(x, b) = 0,
0 < x < a,
0 < y < b.
(11.37)
To tackle the rectangular Helmholtz eigenvalue problem (11.36–37), we shall, as in
(4.89), introduce a further separation of variables, writing the solution
v(x, y) = p(x) q(y)
as the product of functions depending on the individual Cartesian coordinates. Substituting
this expression into the Helmholtz equation (11.36), we ﬁnd
γ p′′(x) q(y) + γ p(x) q′′(y) + λ p(x) q(y) = 0.
To eﬀect the variable separation, we collect all terms involving x on one side and all terms
involving y on the other side of the equation, which is accomplished by dividing by v = pq
and rearranging the terms:
γ p′′(x)
p(x) = −γ q′′(y)
q(y) −λ ≡−μ.
The left-hand side of this equation depends only on x, whereas the middle term depends
only on y. As before, this requires that the expressions equal a common separation constant,
denoted by −μ. (The minus sign is for later convenience.) In this manner, we reduce our
partial diﬀerential equation to a pair of one-dimensional eigenvalue problems
γ d2p
dx2 + μ p = 0,
γ d2q
dy2 + (λ −μ) q = 0,
(11.38)
each of which is subject to homogeneous Dirichlet boundary conditions
p(0) = p(a) = 0,
q(0) = q(b) = 0,
(11.39)

11.2 Explicit Solutions of the Heat Equation
447
stemming from the boundary conditions (11.37). To obtain a nontrivial separable solution
to the Helmholtz equation, we seek nonzero solutions to these two supplementary eigenvalue
problems.
We have already solved these particular two boundary value problems (11.38–39) many
times; see, for instance, (4.21). The eigenfunctions are, respectively,
pm(x) = sin mπx
a
,
m = 1, 2, 3, . . .,
qn(y) = sin nπy
b
,
n = 1, 2, 3, . . .,
with
μ = m2 π2 γ
a2
,
λ −μ = n2 π2 γ
b2
,
so that
λ = m2 π2 γ
a2
+ n2 π2 γ
b2
.
Therefore, the separable eigenfunction solutions to the Helmholtz boundary value problem
(11.35–36) have the doubly trigonometric form
vm,n(x, y) = sin mπx
a
sin nπy
b
,
for
m, n = 1, 2, 3, . . . ,
(11.40)
with associated eigenvalues
λm,n = m2 π2 γ
a2
+ n2 π2 γ
b2
=
 m2
a2 + n2
b2

π2 γ .
(11.41)
Each of these corresponds to an exponentially decaying eigensolution
um,n(t, x, y) = e−λm,n t vm,n(x, y) = exp

−
 m2
a2 + n2
b2

π2 γ t

sin mπx
a
sin nπy
b
(11.42)
to the original rectangular Dirichlet boundary value problem for the heat equation.
Using the fact that the univariate sine functions form a complete system, it is not
hard to prove, [120], that the separable eigenfunction solutions (11.42) are complete, and
so there are no non-separable eigenfunctions.† As a consequence, the general solution to
the initial-boundary value problem can be expressed as a linear combination
u(t, x, y) =
∞

m,n=1
cm,n um,n(t, x, y) =
∞

m,n=1
cm,n e−λm,n t vm,n(x, y)
(11.43)
of the eigenmodes. The coeﬃcients cm,n are prescribed by the initial conditions, which
take the form of a double Fourier sine series
f(x, y) = u(0, x, y) =
∞

m,n=1
cm,nvm,n(x, y) =
∞

m,n=1
cm,n sin mπx
a
sin nπy
b
.
Self-adjointness of the Laplacian operator coupled with the boundary conditions im-
plies that‡ the eigenfunctions vm,n(x, y) are orthogonal with respect to the L2 inner product
†
This appears to be a general fact, true in all known examples, but I know of no general
proof. Theorem 9.47 can be used to establish completeness of the eigenfunctions, but does not
guarantee that they can all be constructed by separation of variables.
‡
Technically, orthogonality is guaranteed only when the eigenvalues are distinct: λm,n ̸= λk,l.
However, by a direct computation, one ﬁnds that orthogonality continues to hold even when the
indicated eigenfunctions are associated with equal eigenvalues. See the ﬁnal subsection of this
chapter for a discussion of when such “accidental degeneracies” arise.

448
11 Dynamics of Planar Media
Figure 11.2.
Heat diﬀusion in a rectangle.
on the rectangle:
⟨vk,l , vm,n ⟩=
 b
0
 a
0
vk,l(x, y) vm,n(x, y) dx dy = 0
unless
k = m
and
l = n.
(The skeptical reader can verify the orthogonality relations directly from the eigenfunction
formulas (11.40).)
Thus, we can appeal to our usual orthogonality formula (11.26) to
evaluate the coeﬃcients
cm,n = ⟨f , vm,n ⟩
∥vm,n ∥2 = 4
ab
 b
0
 a
0
f(x, y) sin mπx
a
sin nπy
b
dx dy,
(11.44)
where the formula for the norms of the eigenfunctions
∥vm,n ∥2 =
 b
0
 a
0
vm,n(x, y)2 dx dy =
 b
0
 a
0
sin2 mπx
a
sin2 nπy
b
dx dy = 1
4 ab
(11.45)
follows from a direct evaluation of the double integral. Unfortunately, while orthogonality
is (mostly) automatic, computation of the norms must inevitably be done “by hand”.
For generic initial temperature distributions, the rectangle approaches thermal equi-
librium at a rate equal to the smallest eigenvalue:
λ1,1 =
 1
a2 + 1
b2

π2 γ,
(11.46)
i.e., the sum of the reciprocals of the squared lengths of its sides multiplied by the diﬀusion
coeﬃcient. The larger the rectangle, or the smaller the diﬀusion coeﬃcient, the smaller the
value of λ1,1, and hence the slower the return to thermal equilibrium. The exponentially
fast decay rate of the Fourier series implies that the solution immediately smooths out any
discontinuites in the initial temperature proﬁle. Indeed, the higher modes, with m and
n large, decay to zero almost instantaneously, and so the solution quickly behaves like a
ﬁnite sum over a few low-order modes. Assuming that c1,1 ̸= 0, the slowest-decaying mode

11.2 Explicit Solutions of the Heat Equation
449
in the Fourier series (11.43) is
c1,1 u1,1(t, x, y) = c1,1 exp

−
 1
a2 + 1
b2

π2 γ t

sin πx
a
sin πy
b .
(11.47)
Thus, in the long run, the temperature becomes entirely of one sign — either positive
or negative depending on the sign of c1,1 — throughout the rectangle. This observation
is, in fact, indicative of the general phenomenon that an eigenfunction associated with
the smallest positive eigenvalue of a self-adjoint elliptic operator is necessarily of one sign
throughout the domain, [34]. A typical solution is plotted at several times in Figure 11.2.
Non-generic initial conditions, with c1,1 = 0, decay more rapidly, and their asymptotic
temperature proﬁles are not of one sign.
Exercises
11.2.1. A rectangle of size 2 cm by 1 cm has initial temperature f(x, y) = sin πx sin πy for
0 ≤x ≤2, 0 ≤y ≤1. All four sides of the rectangle are held at 0◦. Assuming that the
thermal diﬀusivity of the plate is γ = 1, write down a formula for its subsequent tempera-
ture u(t, x, y). What is the rate of decay to thermal equilibrium?
11.2.2. Solve Exercise 11.2.1 when the initial temperature f(x, y) is
(a) x y,
(b)
 1,
0 < x < 1,
0,
1 < x < 2;
(c)

1 −| 1 −x |
  1
2 −
 1
2 −y


.
11.2.3. Solve the initial-boundary value problem for the heat equation ut = 2 Δu on the rectan-
gle −1 < x < 1, 0 < y < 1 when the two short sides are kept at 0◦, the two long sides are
insulated, and the initial temperature distribution is u(0, x, y) =
 −1,
x < 0,
+1,
x > 0,
0 < y < 1.
11.2.4. Answer Exercise 11.2.3 when the two long sides are kept at 0◦and the two short sides
are insulated.
♥11.2.5. A rectangular plate of size 1 meter by 3 meters is made out a metal with unit diﬀusiv-
ity. The plate is taken from a 0◦freezer, and, from then on, one of its long sides is heated
to 100◦, the other is held at 0◦, while its top, bottom, and both of the short sides are fully
insulated. (a) Set up the initial-boundary value problem governing the time-dependent
temperature of the plate. (b) What is the equilibrium temperature? (c) Use your answer
from part (b) to construct an eigenfunction series for the solution. (d) How long until the
temperature of the plate is everywhere within 1◦of its eventual equilibrium?
Hint: Once t is no longer small, you can approximate the series solution by its ﬁrst term.
11.2.6. Among all rectangular plates of a prescribed area, which one returns to thermal equi-
librium the slowest when subject to Dirichlet boundary conditions? The fastest? Use your
physical intuition to explain your answer, but justify it mathematically.
11.2.7. Answer Exercise 11.2.6 for a fully insulated rectangular plate, i.e., subject to Neumann
boundary conditions.
♥11.2.8. A square metal plate is taken from an oven, and then set out to cool, with its top and
bottom insulated. Find the rate of cooling, in terms of the side length and the thermal dif-
fusivity, if (a) all four sides are held at 0◦; (b) one side is insulated and the other three
sides are held at 0◦; (c) two adjacent sides are insulated and the other two are held at 0◦;
(d) two opposite sides are insulated and the other two are held at 0◦; (e) three sides are
insulated and the remaining side is held at 0◦. Order the cooling rates of the plates from
fastest to slowest. Do your results conﬁrm your intuition?

450
11 Dynamics of Planar Media
♥11.2.9. Two square plates are made out of the same homogeneous material, and both are ini-
tially heated to 100◦. All four sides of the ﬁrst plate are held at 0◦, whereas one of the
sides of the second plate is insulated while the other three sides are held at 0◦. Which plate
cools down the fastest? How much faster? Assuming the thermal diﬀusivity γ = 1, how
long do you have to wait until every point on each plate is within 1◦of its equilibrium tem-
perature? Hint: Once t is no longer small, the series solution is well approximated by its
ﬁrst term.
♥11.2.10. Multiple choice: On a unit square that is subject to Dirichlet boundary conditions, the
eigenvalues of the Laplace operator are
(a) all simple, (b) at most double, or (c) can have arbitrarily large multiplicity.
♥11.2.11. The thermodynamics of a thin circular cylindrical shell of radius a and height h, e.g.,
the side of a tin can after its top and bottom are removed, is modeled by the heat equation
∂u
∂t = γ
⎛
⎝1
a2
∂2u
∂θ2 + ∂2u
∂z2
⎞
⎠, in which u(t, θ, z) measures the temperature of the point on
the cylinder at time t > 0, angle −π < θ ≤π, and height 0 < z < h. Keep in mind
that u(t, θ, z) must be a 2π–periodic function of the angular coordinate θ. Assume that the
cylinder is everywhere insulated, while its two circular ends at held at 0◦. Given an initial
temperature distribution at time t = 0, write down a series formula for the cylinder’s tem-
perature at subsequent times. What is the eventual equilibrium temperature? How fast
does the cylinder return to equilibrium?
♥11.2.12. Consider the initial-boundary value problem
ut = uxx + uyy,
u(0, x, y) = 0,
0 < x, y < π,
t > 0,
for the heat equation in a square subject to the Dirichlet conditions
u(0, y) = u(π, y) = 0 = u(x, 0),
u(x, π) = f(x),
0 < x, y < π.
Write out an eigenfunction series formulas for
(a) the equilibrium solution u⋆(x, y) = lim
t →∞u(t, x, y);
(b) the solution u(t, x, y).
11.2.13. Solve Exercise 11.2.1 when one long side of the plate is held at 100◦.
Hint: See Exercise 11.2.12.
Heating of a Disk — Preliminaries
Let us perform a similar analysis of the thermodynamics of a circular disk. For simplicity
(or by choice of suitable physical units), we will assume that the disk
D = {x2 + y2 ≤1 } ⊂R2
has unit radius and unit diﬀusivity γ = 1. We shall solve the heat equation on D subject
to homogeneous Dirichlet boundary values of zero temperature at the circular edge
∂D = C = {x2 + y2 = 1 }.
Thus, the full initial-boundary value problem is
∂u
∂t = ∂2u
∂x2 + ∂2u
∂y2 ,
x2 + y2 < 1,
u(t, x, y) = 0,
x2 + y2 = 1,
u(0, x, y) = f(x, y),
x2 + y2 ≤1.
t > 0,
(11.48)

11.2 Explicit Solutions of the Heat Equation
451
We remark that a simple rescaling of space and time, as outlined in Exercise 11.4.7, can
be used to recover the solution for an arbitrary diﬀusion coeﬃcient and a disk of arbitrary
radius from this particular case.
Since we are working in a circular domain, we instinctively pass to polar coordinates
(r, θ). In view of the polar coordinate formula (4.105) for the Laplace operator, the heat
equation and boundary and initial conditions assume the form
∂u
∂t = ∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2 ,
u(t, 1, θ) = 0,
u(0, r, θ) = f(r, θ),
(11.49)
where the solution u(t, r, θ) is deﬁned for all 0 ≤r ≤1 and t ≥0. To ensure that the
solution represents a single-valued function on the entire disk, it is required to be a 2π–
periodic function of the angular variable:
u(t, r, θ + 2π) = u(t, r, θ).
To obtain the separable solutions
u(t, r, θ) = e−λt v(r, θ),
(11.50)
we need to solve the polar coordinate form of the Helmholtz equation
∂2v
∂r2 + 1
r
∂v
∂r + 1
r2
∂2v
∂θ2 + λ v = 0,
0 ≤r < 1,
−π < θ ≤π,
(11.51)
subject to the boundary conditions
v(1, θ) = 0,
v(r, θ + 2π) = v(r, θ).
(11.52)
To solve the polar Helmholtz boundary value problem (11.51–52), we invoke a further
separation of variables by writing
v(r, θ) = p(r) q(θ).
(11.53)
Substituting this ansatz into (11.51), collecting all terms involving r and all terms involving
θ, and then equating both to a common separation constant, we are led to the pair of
ordinary diﬀerential equations
r2 d2p
dr2 + r dp
dr + (λr2 −μ) p = 0,
d2q
dθ2 + μ q = 0,
(11.54)
where λ is the Helmholtz eigenvalue, and μ the separation constant.
Let us start with the equation for q(θ). The second boundary condition in (11.52)
requires that q(θ) be 2π–periodic. Therefore, the required solutions are the elementary
trigonometric functions
q(θ) = cos mθ
or
sin mθ,
where
μ = m2,
(11.55)
with m = 0, 1, 2, . . . a nonnegative integer.
Substituting the formula for the separation constant, μ = m2, the diﬀerential equation
for p(r) takes the form
r2 d2p
dr2 + r dp
dr + (λ r2 −m2) p = 0,
0 ≤r ≤1.
(11.56)

452
11 Dynamics of Planar Media
Ordinarily, one imposes two boundary conditions in order to pin down a solution to such a
second-order ordinary diﬀerential equation. But our Dirichlet condition, namely p(1) = 0,
speciﬁes its value at only one of the endpoints. The other endpoint is a singular point for
the ordinary diﬀerential equation, because the coeﬃcient of the highest-order derivative,
namely r2, vanishes at r = 0. This situation might remind you of our solution to the Euler
diﬀerential equation (4.111) in the context of separable solutions to the Laplace equation
on the disk. As there, we require that the solution be bounded at r = 0, and so seek
eigensolutions that satisfy the boundary conditions
| p(0) | < ∞,
p(1) = 0.
(11.57)
While (11.56) appears in a variety of applications, it is more challenging than any
ordinary diﬀerential equation we have encountered so far. Indeed, most solutions cannot
be written in terms of the elementary functions (rational functions, trigonometric functions,
exponentials, logarithms, etc.) you see in ﬁrst-year calculus. Nevertheless, owing to their
ubiquity in physical applications, its solutions have been extensively studied and tabulated,
and so are, in a sense, well known, [86, 85, 119].
To simplify the subsequent analysis, we make a preliminary rescaling of the indepen-
dent variable, replacing r by
z =
√
λ r.
(We know the eigenvalue λ > 0, since we are dealing with a positive deﬁnite boundary
value problem.) Note that, by the chain rule,
dp
dr =
√
λ dp
dz ,
d2p
dr2 = λ d2p
dz2 ,
and hence
r dp
dr = z dp
dz ,
r2 d2p
dr2 = z2 d2p
dz2 .
The net eﬀect is to eliminate the eigenvalue parameter λ (or, rather, hide it in the change
of variables), so that (11.56) assumes the slightly simpler form
z2 d2p
dz2 + z dp
dz + (z2 −m2) p = 0.
(11.58)
The resulting ordinary diﬀerential equation (11.58) is known as Bessel’s equation, named
after the early-nineteenth-century German astronomer Wilhelm Bessel, who ﬁrst encoun-
tered its solutions, now known as Bessel functions, in his study of planetary orbits. Special
cases had already appeared in the investigations of Daniel Bernoulli on vibrations of a hang-
ing chain, and in those of Fourier on the thermodynamics of a cylindrical body. To make
further progress, we need to take time out to study their basic properties, and this will re-
quire us to develop the method of power series solutions of ordinary diﬀerential equations.
With this in hand, we can then return to complete our solution to the heat equation on a
disk.
11.3 Series Solutions of Ordinary Diﬀerential Equations
When confronted with a novel ordinary diﬀerential equation, we have several available
options for deriving and understanding its solutions. For instance, the “look-up” method

11.3 Series Solutions of Ordinary Diﬀerential Equations
453
relies on published handbooks. One of the most useful references that collects many solved
diﬀerential equations is the classic German compendium by Kamke, [62]. Two more recent
English-language handbooks are [93, 127]. In addition, many symbolic computer algebra
programs, including Mathematica and Maple, will produce solutions, when expressible
in terms of both elementary and special functions, to a wide range of diﬀerential equations.
Of course, use of numerical integration to approximate solutions, [24, 60, 80], is al-
ways an option.
Numerical methods do, however, have their limitations, and are best
accompanied by some understanding of the underlying theory, coupled with qualitative
or quantitative expectations of how the solutions should behave. Furthermore, numerical
methods provide less than adequate insight into the nature of the special functions that
regularly appear as solutions of the particular diﬀerential equations arising in separation
of variables. A numerical approximation cannot, in itself, establish rigorous mathematical
properties of the solutions of the diﬀerential equation.
A more classical means of constructing and approximating the solutions of diﬀerential
equations is based on their power series expansions, a.k.a. Taylor series. The Taylor ex-
pansion of a solution at a point x0 is found by substituting a general power series into the
diﬀerential equation and equating coeﬃcients of the various powers of x −x0. The initial
conditions at x0 serve to uniquely determine the coeﬃcients and hence all the derivatives of
the solution at the initial point. The Taylor expansion of a special function is an eﬀective
tool for deducing some of its key properties, as well as providing a means of comput-
ing reasonable numerical approximations to its values within the radius of convergence of
the series. (However, serious numerical computations more often rely on nonconvergent
asymptotic expansions, [85].)
In this section, we provide a brief introduction to the basic series solution techniques for
ordinary diﬀerential equations, concentrating on second-order linear diﬀerential equations,
since these form by far the most important class of examples arising in applications. At a
regular point, the method will produce a standard Taylor expansion for the solution, while
so-called regular singular points require a slightly more general type of series expansion.
Generalizations to irregular singular points, higher-order equations, nonlinear equations,
and even linear and nonlinear systems are deferred to more advanced texts, including
[54, 59].
The Gamma Function
Before delving into the machinery of series solutions and special functions, we need to
introduce the gamma function, which eﬀectively generalizes the factorial operation to non-
integers. Recall that the factorial of a nonnegative integer n ≥0 is deﬁned inductively by
the iterative formula
n! = n · (n −1)!,
starting with
0! = 1.
(11.59)
When n is a positive integer, the iteration terminates, yielding the familiar expression
n! = n(n −1)(n −2) · · · 3 · 2 · 1.
(11.60)
However, for more general values of n, the iteration never stops, and it cannot be used to
compute its factorial. Our goal is to circumvent this diﬃculty, and introduce a function
f(x) that is deﬁned for all values of x and will play the role of such a factorial. First,

454
11 Dynamics of Planar Media
mimicking (11.59), the function should satisfy the functional equation
f(x) = x f(x −1)
(11.61)
where deﬁned. If, in addition, f(0) = 1, then we know that f(n) = n! whenever n is a
nonnegative integer, and hence such a function will extend the deﬁnition of the factorial
to more general real and complex numbers.
A moment’s thought should convince the reader that there are many possible ways
to construct such a function; see Exercise 11.3.6 for a nonstandard example. The most
important version is due to Euler. The modern deﬁnition of Euler’s gamma function relies
on an integral formula discovered by the eighteenth-century French mathematician Adrien–
Marie Legendre, who will play a starring role in Chapter 12.
Deﬁnition 11.2. The gamma function is deﬁned by
Γ(x) =
 ∞
0
e−t tx−1 dt.
(11.62)
The ﬁrst fact is that, for real x, the gamma function integral converges only when
x > 0; otherwise the singularity of tx−1 at t = 0 is too severe. The key property that turns
the gamma function into a substitute for the factorial function relies on an elementary
integration by parts:
Γ(x + 1) =
 ∞
0
e−t tx dt = −e−t tx 
∞
t=0 + x
 ∞
0
e−t tx−1 dt.
The boundary terms vanish whenever x > 0, while the ﬁnal integral is merely Γ(x). There-
fore, the gamma function satisﬁes the recurrence relation
Γ(x + 1) = x Γ(x).
(11.63)
If we set f(x) = Γ(x + 1), then (11.63) becomes (11.61). Moreover, by direct integration,
Γ(1) =
 ∞
0
e−t dt = 1.
Combining this with the recurrence relation (11.63), we deduce that
Γ(n + 1) = n!
(11.64)
whenever n ≥0 is a nonnegative integer. Therefore, we can identify x! with the value
Γ(x + 1) whenever x > −1 is any real number.
Remark: The reader may legitimately ask why not replace tx−1 by tx in the deﬁnition
of Γ(z), which would avoid the n + 1 in (11.64). There is no good answer; we are merely
following a well-established precedent set by Legendre and enshrined in all subsequent
works.
Thus, at integer values of x, the gamma function agrees with the elementary factorial.
A few other values can be computed exactly. One important case is at x = 1
2. Using the
substitution t = s2, with dt = 2s ds, we obtain
Γ
 1
2

=
 ∞
0
e−t t−1/2 dt =
 ∞
0
2 e−s2 ds = √π,
(11.65)

11.3 Series Solutions of Ordinary Diﬀerential Equations
455
Figure 11.3.
The gamma function.
where the ﬁnal integral was evaluated in (2.100). Thus, using the identiﬁcation with the
factorial function, we identify this value with

−1
2

! = √π.
The recurrence relation
(11.63) will then produce the value of the gamma function at all half-integers 1
2, 3
2, 5
2, . . . .
For example,
Γ
 3
2

= 1
2 Γ
 1
2

= 1
2
√π,
(11.66)
and hence 1
2 ! = 1
2
√π. The recurrence relation can also be employed to extend the deﬁnition
of Γ(x) to (most) negative values of x. For example, setting x = −1
2 in (11.63), we have
Γ
 1
2

= −1
2 Γ

−1
2

,
so
Γ

−1
2

= −2 Γ
 1
2

= −2 √π .
The only points at which this device fails are the negative integers, and indeed, Γ(x) has
a singularity when x = −1, −2, −3, . . . . A graph† of the gamma function is displayed in
Figure 11.3.
Remark: Most special functions of importance for applications arise as solutions to
fairly simple ordinary diﬀerential equations. The gamma function is a signiﬁcant exception.
Indeed, it can be proved, [11], that the gamma function does not satisfy any algebraic
diﬀerential equation!
Regular Points
We are now ready to develop the method of series solutions to ordinary diﬀerential equa-
tions. Before we proceed to develop the general computational machinery, a na¨ıve calcula-
tion in an elementary example will be enlightening.
†
The axes are at diﬀerent scales; the tick marks are at integer values.

456
11 Dynamics of Planar Media
Example 11.3. Consider the initial value problem
d2u
dx2 + u = 0,
u(0) = 1,
u′(0) = 0.
(11.67)
Let us investigate whether we can construct an analytic solution in the form of a convergent
power series
u(x) = u0 + u1 x + u2 x2 + u3 x3 + · · ·
=
∞

n=0
un xn
(11.68)
that is based at the initial point x0 = 0. Term-by-term diﬀerentiation yields the following
series expansions† for its derivatives:
du
dx = u1 + 2u2 x + 3u3 x2 + 4u4 x3 + · · ·
=
∞

n=0
(n + 1)un+1 xn,
d2u
dx2 = 2u2 + 6u3 x + 12u4x2 + 20u5x3 + · · ·
=
∞

n=0
(n + 1)(n + 2)un+2xn.
(11.69)
The next step is to substitute the series (11.68–69) into the diﬀerential equation and collect
common powers of x:
d2u
dx2 + u = (2u2 + u0) + (6u3 + u1)x + (12u4 + u2)x2 + (20u5 + u3)x3 + · · ·
= 0.
At this point, one focuses attention on the individual coeﬃcients, appealing to the following
basic observation:
Two convergent power series are equal if and only if all their coeﬃcients are equal.
In particular, a power series represents the zero function‡ if and only if all its coeﬃcients
are 0. In this manner we obtain the following inﬁnite sequence of algebraic recurrence
relations among the coeﬃcients:
1
2u2 + u0 = 0,
x
6u3 + u1 = 0,
x2
12u4 + u2 = 0,
x3
20u5 + u3 = 0,
x4
30u6 + u4 = 0,
...
...
xn
(n + 1)(n + 2)un+2 + un = 0.
(11.70)
Now, the initial conditions serve to prescribe the ﬁrst two coeﬃcients:
u(0) = u0 = 1,
u′(0) = u1 = 0.
†
When working with the series in summation form, it helps to re-index in order to display
the term of degree n.
‡
Here it is essential that we work with analytic functions, since this result is not true for
C∞functions! For example, the function e−1/x2 has identically zero power series at x0 = 0; see
Exercise 11.3.21.

11.3 Series Solutions of Ordinary Diﬀerential Equations
457
We then solve the recurrence relations in order: The ﬁrst determines u2 = −1
2 u0 = −1
2;
the second, u3 = −1
6 u1 = 0; next, u4 = −1
12 u2 =
1
24; then u5 = −1
20 u3 = 0; then
u6 = −1
30 u4 = −
1
720; and so on. In general, it is not hard to see that
u2k = (−1)k
(2k)! ,
u2k+1 = 0,
k = 0, 1, 2, . . . .
Hence, the required series solution is
u(x) = 1 −1
2 x2 + 1
24 x3 −
1
720 x6 + · · ·
=
∞

k=0
(−1)k
k!
x2k,
which, by the ratio test, converges for all x. We have thus recovered the well-known Taylor
series for cos x, which is indeed the solution to the initial value problem. Changing the
initial conditions to u(0) = u0 = 0, u′(0) = u1 = 1, will similarly produce the usual
Taylor expansion of sin x. Note that the generation of the Taylor series does not rely on
any a priori knowledge of trigonometric functions or the direct solution method for linear
constant-coeﬃcient ordinary diﬀerential equations.
Building on this experience, let us describe the general method. We shall concentrate
on solving a second-order homogeneous linear diﬀerential equation
p(x) d2u
dx2 + q(x) du
dx + r(x) u = 0.
(11.71)
The coeﬃcients p(x), q(x), r(x) are assumed to be analytic functions on some common
domain. This means that, at a point x0 within the domain, they admit convergent power
series expansions
p(x) = p0 + p1 (x −x0) + p2 (x −x0)2 + · · · ,
q(x) = q0 + q1 (x −x0) + q2 (x −x0)2 + · · · ,
r(x) = r0 + r1 (x −x0) + r2 (x −x0)2 + · · · .
(11.72)
We expect that solutions to the diﬀerential equation are also analytic. This expectation is
justiﬁed, provided that the equation is regular at the point x0, in the following sense.
Deﬁnition 11.4. A point x = x0 is a regular point of a second-order linear ordinary
diﬀerential equation (11.71) if the leading coeﬃcient does not vanish there:
p0 = p(x0) ̸= 0.
A point where p(x0) = 0 is known as a singular point.
In short, at a regular point, the second-order derivative term does not disappear, and
so the equation is “genuinely” of second order.
Remark: The deﬁnition of a singular point assumes that the other two coeﬃcients do
not also vanish there, so that either q(x0) ̸= 0 or r(x0) ̸= 0. If all three functions happen
to vanish at x0, we can cancel any common factor (x −x0)k, and hence, without loss of
generality, assume that at least one of the coeﬃcient functions is nonzero at x0.
Proofs of the basic existence theorem for diﬀerential equations at regular points can
be found in [18, 54, 59].

458
11 Dynamics of Planar Media
Theorem 11.5. Let x0 be a regular point for the second-order homogeneous linear
ordinary diﬀerential equation (11.71). Then there exists a unique, analytic solution u(x)
to the initial value problem
u(x0) = a,
u′(x0) = b.
(11.73)
The radius of convergence of the power series for u(x) is at least as large as the distance
from the regular point x0 to the nearest singular point of the diﬀerential equation in the
complex plane.
Thus, every solution to an analytic diﬀerential equation at a regular point x0 can be
expanded in a convergent power series
u(x) = u0 + u1(x −x0) + u2(x −x0)2 + · · ·
=
∞

n=0
un(x −x0)n.
(11.74)
Since the power series necessarily coincides with the Taylor series for u(x), its coeﬃcients†
un = u(n)(x0)
n!
are multiples of the derivatives of the function at the point x0. In particular, the ﬁrst two
coeﬃcients,
u0 = u(x0) = a,
u1 = u′(x0) = b,
(11.75)
are ﬁxed by the initial conditions. The remaining coeﬃcients will then be uniquely pre-
scribed thanks to the uniqueness of solutions to initial value problems.
Near a regular point, the second-order diﬀerential equation (11.71) admits two linearly
independent analytic solutions, which we denote by u(x) and u(x). The general solution
can be written as a linear combination of the two basis solutions:
u(x) = a u(x) + b u(x).
(11.76)
A convenient choice is to have the ﬁrst satisfy the initial conditions
u(x0) = 1,
u′(x0) = 0,
(11.77)
and the second satisfy
u(x0) = 0,
u′(x0) = 1,
(11.78)
although other conventions may be used depending on the circumstances. Given (11.77–
78), the linear combination (11.76) automatically satisﬁes the initial conditions (11.73).
The basic computational strategy to construct the power series solution to the initial
value problem is a straightforward adaptation of the method used in Example 11.3. One
substitutes the known power series (11.72) for the coeﬃcient functions and the unknown
power series (11.74) for the solution into the diﬀerential equation (11.71). Multiplying out
the formulas and collecting the common powers of x −x0 will result in a (complicated)
power series whose individual coeﬃcients must be equated to zero. The lowest-order terms
are multiples of (x −x0)0 = 1, i.e., the constant terms. They produce a linear relation
u2 = R2(u0, u1) = R2(a, b)
†
Some authors prefer to include the n!’s in the original power series; this is purely a matter
of personal taste.

11.3 Series Solutions of Ordinary Diﬀerential Equations
459
that prescribes the coeﬃcient u2 in terms of the initial data (11.75). The coeﬃcient of
(x −x0) leads to a relation
u3 = R3(u0, u1, u2) = R3(a, b, R2(a, b))
that prescribes u3 in terms of the initial data and the previously computed coeﬃcient u2.
And so on. At the nth stage of the procedure, the coeﬃcient of (x −x0)n produces the
linear recurrence relation
un+2 = Rn(u0, u1, . . . , un+1),
n = 0, 1, 2, . . . ,
(11.79)
that will prescribe the (n + 2)nd order coeﬃcient in terms of the previously computed
coeﬃcients. In this fashion, we will have constructed a formal power series solution to the
diﬀerential equation at a regular point. The one remaining issue is whether the resulting
power series actually converges. The full analysis can be found in [54, 59], and will serve
to complete the proof of the general Existence Theorem 11.5.
Rather than continue on in general, the best way to learn the method is to work
through another, less trivial, example.
The Airy Equation
We will illustrate the procedure by constructing power series solutions to the Airy equation
d2u
dx2 = xu.
(11.80)
This second-order linear ordinary diﬀerential equation, which arises in applications to op-
tics, rainbows, and dispersive waves, has solutions that cannot be expressed in terms of
elementary functions.
For the Airy equation (11.80), the leading coeﬃcient is constant, and so every point
is a regular point. For simplicity, we will look only for power series based at the origin
x0 = 0, and therefore of the form (11.68). Equating the two series
u′′(x) = 2u2 + 6u3 x + 12u4 x2 + 20u5x3 + · · ·
=
∞

n=0
(n + 1)(n + 2)un+2 xn,
x u(x) = u0 x + u1 x2 + u2 x3 + · · ·
=
∞

n=1
un−1 xn,
leads to the following recurrence relations relating the coeﬃcients:
1
2u2 = 0,
x
6u3 = u0,
x2
12u4 = u1,
x3
20u5 = u2,
x4
30u6 = u3,
...
...
xn
(n + 1)(n + 2)un+2 = un−1.

460
11 Dynamics of Planar Media
As before, we solve them in order: The ﬁrst equation determines u2. The second prescribes
u3 = 1
6 u0 in terms of u0. Next, we ﬁnd u4 =
1
12 u1 in terms of u1, followed by u5 =
1
20u2 =
0; then u6 =
1
30 u3 =
1
180 u0 is ﬁrst given in terms of u3, but we already know the latter in
terms of u0. And so on.
Let us now construct two basis solutions. The ﬁrst has the initial conditions
u0 = u(0) = 1,
u1 = u′(0) = 0.
The recurrence relations imply that the only nonzero coeﬃcients cn occur when n = 3k is
a multiple of 3. Moreover,
u3k =
u3k−3
3k(3k −1) .
A straightforward induction proves that
u3k =
1
3k(3k −1)(3k −3)(3k −4) · · ·6 · 5 · 3 · 2 .
The resulting solution is
u(x) = 1+ 1
6x3+
1
180x6+· · · = 1+
∞

k=1
x3k
3k(3k −1)(3k −3)(3k −4) · · ·6 · 5 · 3 · 2 . (11.81)
Note that the denominator is similar to a factorial, except every third term is omitted.
A straightforward application of the ratio test conﬁrms that the series converges for all
(complex) x, in conformity with the general Theorem 11.5, which guarantees an inﬁnite
radius of convergence because the Airy equation has no singular points.
Similarly, starting with the initial conditions
u0 = u(0) = 0,
u1 = u′(0) = 1,
we ﬁnd that the only nonzero coeﬃcients un occur when n = 3k + 1. The recurrence
relation
u3k+1 =
u3k−2
(3k + 1)(3k)
yields
u3k+1 =
1
(3k + 1)(3k)(3k −2)(3k −3) · · · 7 · 6 · 4 · 3 .
The resulting solution is
u(x) = x + 1
12 x4 +
1
504 x7 + · · · = x +
∞

k=1
x3k+1
(3k + 1)(3k)(3k −2)(3k −3) · · · 7 · 6 · 4 · 3 .
(11.82)
Again, the denominator skips every third term in the product. Every solution to the Airy
equation can be written as a linear combination of these two basis power series solutions:
u(x) = a u(x) + b u(x),
where
a = u(0),
b = u′(0).
Both power series (11.81, 82), converge quite rapidly, and so the ﬁrst few terms will provide
a reasonable approximation to the solutions for moderate values of x.
We have, in fact, already encountered another solution to the Airy equation. According
to formula (8.97), the integral
Ai(x) = 1
π
 ∞
0
cos

sx + 1
3 s3
ds
(11.83)

11.3 Series Solutions of Ordinary Diﬀerential Equations
461
deﬁnes the Airy function of the ﬁrst kind. Let us prove that it satisﬁes the Airy diﬀerential
equation (11.80):
d2
dx2 Ai(x) = x Ai(x).
Before diﬀerentiating, we recall the integration by parts argument in (8.96) to re-express
the Airy integral in absolutely convergent form:
Ai(x) = 2
π
 ∞
0
s sin

sx + 1
3 s3
(x + s2)2
ds.
We are now permitted to diﬀerentiate under the integral sign, producing (after some alge-
bra)
d2
dx2 Ai(x) −x Ai(x) = 2
π
 ∞
0
d
ds
$
s(x + s2) cos

sx + 1
3 s3
−sin

sx + 1
3 s3
(x + s2)3
%
ds = 0.
Thus, the Airy function must be a certain linear combination of the two basic series solu-
tions:
Ai(x) = Ai(0) u(x) + Ai′(0) u(x).
Its values at x = 0 are, in fact, given by
Ai(0) = 1
π
 ∞
0
cos
 1
3 s3
ds =
Γ
 1
3

2π 31/6 =
1
32/3 Γ
 2
3
 ≈.355028 ,
Ai′(0) = −1
π
 ∞
0
s sin
 1
3 s3
ds = −31/6 Γ
 2
3

2π
= −
1
31/3 Γ
 1
3
 ≈−.258819.
(11.84)
The second and third expressions involve the gamma function (11.62); a proof, based on
complex integration, can be found in [85; p. 54].
Exercises
11.3.1. Find (a) Γ
 5
2

, (b) Γ
 7
2

, (c) Γ

−3
2

, (d) Γ

−5
2

.
11.3.2. Prove that Γ

n + 1
2

=
√π (2n)!
22n n!
for every positive integer n.
11.3.3. Let x ∈C be complex. (a) Prove that the gamma function integral (11.62) converges,
provided Re x > 0.
(b) Is formula (11.63) valid when x is complex?
♦11.3.4. Prove that Γ(x) =
	 1
0 (−log s)x−1 ds, and hence, for 0 ≤n ∈Z, we have
n! =
	 1
0 (−log s)n ds. Remark: Euler ﬁrst established the latter identity directly, and used
it to deﬁne the gamma function.
11.3.5. Evaluate
	 ∞
0
√x e−x3
dx.
♦11.3.6. Can you construct a function f(x) that satisﬁes the factorial functional equation (11.61)
and has the values f(x) = 1 for 0 ≤x ≤1? If so, is f(x) = Γ(x + 1)?

462
11 Dynamics of Planar Media
11.3.7. Explain how to construct the power series for sin x by solving the diﬀerential equation
(11.67).
11.3.8. Construct two independent power series solutions to the Euler equation x2u′′ −2u = 0
based at the point x0 = 1.
11.3.9. Construct two independent power series solutions to the equation u′′ + x2u = 0 based at
the point x0 = 0.
11.3.10. Consider the ordinary diﬀerential equation u′′ + 2xu′ + 2u = 0. (a) Find two linearly
independent power series solutions in powers of x.
(b) What is the radius of convergence
of your power series?
(c) By inspection of your series, ﬁnd one solution to the equation
expressible in terms of elementary functions. (d) Find an explicit (non-series) formula for
the second independent power series solution.
11.3.11. Answer Exercise 11.3.10 for the equation u′′ + 1
2 xu′ −1
2 u = 0, which is a special case
of equation (8.63).
11.3.12. Consider the ordinary diﬀerential equation u′′ +xu′ +2u = 0. (a) Find two linearly in-
dependent power series solutions based at x0 = 0.
(b) Write down the power series for the
solution to the initial value problem u(0) = 1, u′(0) = −1. (c) What is the radius of con-
vergence of your power series solution in part (a)? Can you justify this by direct inspection
of your power series?
♦11.3.13. The Hermite equation of order n is
d2u
dx2 −2x du
dx + 2nu = 0.
(11.85)
Assuming n ∈N is a nonnegative integer: (a) Find two linearly independent power series
solutions based at x0 = 0, and then show that one of your solutions is a polynomial of de-
gree n. (b) Prove that the Hermite polynomial Hn(x) deﬁned in (8.64) solves the Hermite
equation (11.85) and hence is a multiple of the polynomial solution you found in part (a).
What is the multiple? (c) Prove that the Hermite polynomials are orthogonal with respect
to the inner product ⟨u , v ⟩=
	 ∞
−∞u(x) v(x) e−x2
dx.
11.3.14. Use the ratio test to directly determine the radius of convergence of the series solu-
tions (11.81, 82) to the Airy equation.
11.3.15. Write down the general solution to the following ordinary diﬀerential equations:
(a) u′′ + (x −c) u = 0, where c is a ﬁxed constant;
(b) u′′ = λxu, where λ ̸= 0 is a ﬁxed nonzero constant.
♦11.3.16. The Airy function of the second kind is deﬁned by
Bi(x) = 1
π
	 ∞
0

exp

sx −1
3 s3
+ sin

sx + 1
3 s3  
ds.
(11.86)
(a) Prove that Bi(x) is well deﬁned and a solution to the Airy equation. (b) Given that†
Bi(0) =
1
31/6Γ
 2
3
 ,
Bi′(0) = 31/6
Γ
 1
3
 ,
(11.87)
explain why every solution to the Airy equation can be written as a linear combination of
Ai(x) and Bi(x). (c) Write the two series solutions (11.81, 82) in terms of Ai(x) and Bi(x).
11.3.17. Use the Fourier transform to construct an L2 solution to the Airy equation. Can you
identify your solution?
♦11.3.18. Apply separation of variables to the Tricomi equation (4.137), and write down all sep-
arable solutions. Hint: See Exercise 11.3.15(b) and Exercise 11.3.16.
†
See [85; p. 54] for a proof.

11.3 Series Solutions of Ordinary Diﬀerential Equations
463
♥11.3.19.(a) Show that u(x) =
∞

n=1
(n −1)! xn is a power series solution to the ﬁrst-order linear
ordinary diﬀerential equation x2u′ −u + x = 0. (b) For which x does the series converge?
(c) Find an analytic formula for the general solution to the equation. (d) Find a second-
order homogeneous linear ordinary diﬀerential equation that has this power series as a (for-
mal) solution. Remark: The lesson of this exercise is that not all power series solutions to
ordinary diﬀerential equations converge. Theorem 11.5 guarantees convergence at a regular
point, but in this example the power series is based at the singular point x0 = 0.
11.3.20. True or false: The only function f(x) that has identically zero Taylor series is the zero
function.
♦11.3.21. Deﬁne f(x) =
⎧
⎨
⎩
e−1/x2
,
x ̸= 0,
0,
x = 0.
(a) Prove that f is a C∞function for all x ∈R.
(b) Prove that f(x) is not analytic by showing that its Taylor series at x0 = 0 does not
converge to f(x) when x ̸= 0.
Regular Singular Points
As we have just seen, constructing power series solutions at regular points is a reasonably
straightforward computational exercise: one writes down a power series with arbitrary
coeﬃcients, substitutes into the diﬀerential equation along with a pair of initial conditions,
and recursively solves for the coeﬃcients. Finding a general formula for the coeﬃcients
might be challenging, but producing their successive numerical values, degree by degree, is
a mechanical exercise.
However, at a singular point, the solutions cannot be typically written as an ordinary
power series, and one needs to be cleverer. Of course, you may object — why not just solve
the equation away from the singular point and be done with it. But there are multiple
reasons not to do this. First, one may be unable to discover a general formula for the
power series coeﬃcients at regular points. Second, the most informative and interesting
behavior of solutions is typically found at the singular points, and so series solutions based
at singular points are particularly enlightening. And ﬁnally, one of the boundary conditions
required for us to complete our construction of separable solutions to partial diﬀerential
equations often occurs at a singular point.
Singular points appear in two guises. The easier to handle, and, fortunately, the ones
that arise in almost all applications, are known as “regular singular points”.
Irregular
singular points are nastier, and we will not make any attempt to understand them in this
text; the curious reader is referred to [54, 59].
Deﬁnition 11.6. A second-order linear homogeneous ordinary diﬀerential equation
that can be written the form
(x −x0)2 a(x) d2u
dx2 + (x −x0) b(x) du
dx + c(x) u = 0,
(11.88)
where a(x), b(x), and c(x) are analytic at x = x0 and, moreover, a(x0) ̸= 0, is said to have
a regular singular point at x0.
The simplest example of a second-order equation with a regular singular point at
x0 = 0 is the Euler equation
ax2u′′ + bxu′ + cu = 0,
(11.89)

464
11 Dynamics of Planar Media
with a, b, c all constant and a ̸= 0. Note that all other points are regular points. Euler
equations can be readily solved by substituting the power ansatz u(x) = xr. We ﬁnd
ax2 u′′ + bxu′ + cu = ar(r −1)xr + brxr + cxr = 0,
provided the exponent r satisﬁes the indicial equation
ar(r −1) + br + c = 0.
If this quadratic equation has two distinct roots r1 ̸= r2, we obtain two linearly independent
(possibly complex) solutions u(x) = xr1 and u(x) = xr2. The general solution u(x) =
c1xr1 + c2xr2 is a linear combination thereof. Note that unless r1 or r2 is a nonnegative
integer, all nonzero solutions have a singularity at the singular point x = 0. A repeated root,
r1 = r2, has only one power solution, u(x) = xr1, and requires an additional logarithmic
term, u(x) = xr1 log x, for the second independent solution.
In this case, the general
solution has the form u(x) = c1xr1 + c2xr1 log x.
The series solution method at more general regular singular points is modeled on the
simple example of the Euler equation. One now seeks a solution that has a series expansion
of the form
u(x) = (x−x0)r
∞

n=0
un(x−x0)n = u0(x−x0)r+u1(x−x0)r+1+u2(x−x0)r+2+· · · . (11.90)
The exponent r is known as the index. If r = 0, or, more generally, if r is a positive
integer, then (11.90) is an ordinary power series, but we allow the possibility of a non-
integral, or even complex, index r. We can assume, without any loss of generality, that the
leading coeﬃcient u0 ̸= 0. Indeed, if uk ̸= 0 is the ﬁrst nonzero coeﬃcient, then the series
begins with the term uk(x −x0)r+k, and we merely replace r by r + k to write it in the
form (11.90). Since any scalar multiple of a solution is a solution, we can further assume
that u0 = 1, in which case we call (11.90) a normalized Frobenius series in honor of the
German mathematician Georg Frobenius, who systematically established the calculus of
series solutions at regular singular points in the late 1800s. The index r, and the higher-
order coeﬃcients u1, u2, . . ., are then found by substituting the normalized Frobenius series
into the diﬀerential equation (11.88) and equating the coeﬃcients of the powers of x −x0
to zero.
Warning: Unlike those in ordinary power series expansions, the coeﬃcients u0 = 1
and u1 are not prescribed by the initial conditions at the point x0.
Since
u(x) = (x −x0)r + u1(x −x0)r+1 + · · · ,
(x −x0) u′(x) = r (x −x0)r + (r + 1)u1(x −x0)r+1 + · · · ,
(x −x0)2 u′′(x) = r (r −1) (x −x0)r + (r + 1) ru1(x −x0)r+1 + · · · ,
the terms of lowest order in the equation are multiples of (x −x0)r. Equating their coeﬃ-
cients to zero produces a quadratic equation of the form
s0 r(r −1) + t0 r + r0 = 0,
(11.91)
where
s0 = s(x0) = 1
2 p′′(x0),
t0 = t(x0) = q′(x0),
r0 = r(x0),

11.3 Series Solutions of Ordinary Diﬀerential Equations
465
are the leading coeﬃcients in the power series expansions of the individual coeﬃcient func-
tions. The quadratic equation (11.91) is known as the indicial equation, since it determines
the possible indices r in the Frobenius expansion (11.90) of a solution.
As with the Euler equation, the quadratic indicial equation usually has two roots,
say r1 and r2, which provide two allowable indices, and one thus expects to ﬁnd two
independent Frobenius expansions. Usually, this expectation is realized, but there is an
important exception. The general result is summarized in the following list:
(i) If r2 −r1 is not an integer, then there are two linearly independent solutions u(x) and
u(x), each having convergent normalized Frobenius expansions of the form (11.90).
(ii) If r1 = r2, then there is only one solution u(x) with a normalized Frobenius expansion
(11.90). One can construct a second independent solution of the form
u(x) = log(x −x0) u(x) + v(x),
where
v(x) =
∞

n=1
vn(x −x0)n+r1
(11.92)
is a convergent Frobenius series.
(iii) Finally, if r2 = r1 + k, where k > 0 is a positive integer, then there is a nonzero
solution u(x) with a convergent Frobenius expansion corresponding to the smaller
index r1. One can construct a second independent solution of the form
u(x) = c log(x−x0) u(x)+v(x),
where
v(x) = xr2 +
∞

n=1
vn(x−x0)n+r2
(11.93)
is a convergent Frobenius series, and c is a constant, which may be 0, in which case
the second solution u(x) is also of Frobenius form.
Thus, in every case, the diﬀerential equation has at least one nonzero solution with a con-
vergent Frobenius expansion. If the second independent solution does not have a Frobenius
expansion, then it requires an additional logarithmic term of a well-prescribed form. Rather
than try to develop the general theory in any more detail here, we will content ourselves
to work through a couple of particular examples.
Example 11.7. Consider the second-order ordinary diﬀerential equation
d2u
dx2 +
 1
x + x
2
 du
dx + u = 0.
(11.94)
We look for series solutions based at x = 0.
Note that, upon multiplying by x2, the
equation takes the form
x2u′′ + x

1 + 1
2 x2
u′ + x2u = 0,
and hence x0 = 0 is a regular singular point, with a(x) = 1, b(x) = 1 + 1
2 x2, c(x) = x2.
We thus look for a solution that can be represented by a Frobenius expansion:
u(x) = xr + u1 xr+1 + · · · + un xn+r + · · · ,
x u′(x) = rxr + (r + 1)u1 xr+1 + · · · + (n + r)un xn+r + · · · ,
1
2 x3 u′(x) = 1
2 rxr+2 + 1
2(r + 1)u1 xr+3 + · · · + 1
2 (n + r −2)un−2 xn+r + · · · ,
x2u′′(x) = r(r −1)xr + (r + 1)ru1 xr+1 + · · · + (n + r)(n + r −1)un xn+r + · · · .
(11.95)

466
11 Dynamics of Planar Media
Substituting into the diﬀerential equation, we ﬁnd that the coeﬃcient of xr leads to the
indicial equation
r2 = 0.
There is only one root, r = 0, and hence, even though we are at a singular point, the
Frobenius expansion reduces to an ordinary power series. The coeﬃcient of xr+1 = x tells
us that u1 = 0. The general recurrence relation, for n ≥2, is
n2un + 1
2 nun−2 = 0,
and hence
un = −un−2
2n .
Therefore, the odd coeﬃcients u2k+1 = 0 are all zero, while the even ones are
u2k = −u2k−2
4k
=
u2k−4
4k(4k −4) = −
u2k−6
4k(4k −4)(4k −8) = · · · = (−1)k
4k k! ,
since
u0 = 1.
The resulting power series assumes a recognizable form:
u(x) =
∞

k=1
u2kx2k =
∞

k=1
1
k!

−x2
4
k
= e−x2/4,
which is an explicit elementary solution to the ordinary diﬀerential equation (11.94).
Since there is only one root to the indicial equation, the second solution u(x) will
require a logarithmic term. It can be constructed by a second application of the Frobenius
method using the more complicated form (11.92). Alternatively, since the ﬁrst solution
is known, we can use a well-known reduction trick, [23]. Given one solution u(x) to a
second-order linear ordinary diﬀerential equation, the general solution can be found by
substituting the ansatz
u(x) = v(x) u(x) = v(x) e−x2/4
(11.96)
into the equation. In this case,
u′′ +
 1
x + x
2

u′ + u = v

u′′ +
1
x + x
2

u′ + u

+ v′

2 u′ +
 1
x + x
2

u

+ v′′ u
= e−x2/4

v′′ + v′
x

.
If u is to be a solution, v′ must satisfy a linear ﬁrst-order ordinary diﬀerential equation:
v′′ + v′
x = 0,
and hence
v′ = c
x ,
v = c log x + d,
where c, d are arbitrary constants. We conclude that the general solution to the original
diﬀerential equation is
u(x) = v(x) u(x) = (c log x + d) e−x2/4.
(11.97)
Bessel’s Equation
Perhaps the most important “non-elementary” ordinary diﬀerential equation is
x2 u′′ + x u′ + (x2 −m2) u = 0,
(11.98)

11.3 Series Solutions of Ordinary Diﬀerential Equations
467
known as Bessel’s equation of order m.
We assume here that the order m ≥0 is a
nonnegative real number.
(Exercise 11.3.30 investigates Bessel equations of imaginary
order.)
The Bessel equation arises from separation of variables in a variety of partial
diﬀerential equations, including the Laplace, heat, and wave equations on a disk, a cylinder,
and a spherical ball.
The Bessel equation cannot (except in a few particular instances) be solved in terms
of elementary functions, and so the use of power series is essential. The leading coeﬃcient,
p(x) = x2, is nonzero except when x = 0, and so all points except the origin are regular.
Therefore, at any x0 ̸= 0, the standard power series construction can be used to produce
the solutions of the Bessel equation. However, the recurrence relations for the coeﬃcients
are not particularly easy to solve in closed form. Moreover, applications tend to demand
understanding the behavior of solutions at the singular point x0 = 0.
Comparison with (11.88) immediately shows that x0 = 0 is a regular singular point,
and so we seek solutions in Frobenius form. We substitute the ﬁrst, second, and fourth
expressions in (11.95) into the Bessel equation and then equate the coeﬃcients of the
various powers of x to zero. The lowest power, xr, provides the indicial equation
r(r −1) + r −m2 = r2 −m2 = 0.
It has two solutions, r = ± m, except when m = 0, for which r = 0 is the only index.
The higher powers of x lead to recurrence relations for the coeﬃcients un in the
Frobenius series. Replacing m2 by r2 produces
xr+1 :

(r + 1)2 −r2 
u1 = (2r + 1)u1 = 0,
u1 = 0,
xr+2 :

(r + 2)2 −r2 
u2 + 1 = (4r + 4)u2 + 1 = 0,
u2 = −
1
4r + 4,
xr+3 :

(r + 3)2 −r2 
u3 + u1 = (6r + 9)u3 + u1 = 0,
u3 = −
u1
6r + 9 = 0,
and, in general,
xr+n :

(r + n)2 −r2 
un + un−2 = n(2r + n)un + un−2 = 0.
Thus, the general recurrence relation is
un = −
1
n(2r + n) un−2,
n = 2, 3, 4, . . . .
(11.99)
Starting with u0 = 1, u1 = 0, it is easy to deduce that all un = 0 for all odd n = 2k + 1,
while for even n = 2k,
u2k = −
u2k−2
4k(k + r) =
u2k−4
16k(k −1)(r + k)(r + k −1) = · · ·
=
(−1)k
22k k(k −1) · · ·3 · 2 (r + k)(r + k −1) · · ·(r + 2)(r + 1) .
We have thus found the series solution
u(x) =
∞

k=0
u2k xr+2k =
∞

k=0
(−1)kxr+2k
22k k! (r + k)(r + k −1) · · ·(r + 2)(r + 1) .
(11.100)
So far, we have not paid attention to the precise values of the indices r = ±m. In
order to continue the recurrence, we need to ensure that the denominators in (11.99) are

468
11 Dynamics of Planar Media
never 0. Since n > 0, a vanishing denominator will appear whenever 2r + n = 0, and so
r = −1
2 n is either a negative integer −1, −2, −3, . . . or half-integer −1
2, −3
2, −5
2, . . . . This
will occur when the order m = −r = 1
2 n is either an integer or a half-integer. Indeed,
these are precisely the situations in which the two indices, namely r1 = −m and r2 = m,
diﬀer by an integer, r2 −r1 = n, and so we are in the tricky case (iii) of the Frobenius
method.
There is, in fact, a major diﬀerence between the integral and the half-integral cases.
Recall that the odd coeﬃcients u2k+1 = 0 in the Frobenius series automatically vanish, and
so we only have to worry about the recurrence relation (11.99) for even values of n. When
n = 2k, the factor 2r + n = 2(r + k) = 0 vanishes only when r = −k is a negative integer;
the half-integral values do not, in fact cause problems. Therefore, if the order m ≥0 is not
an integer, then the Bessel equation of order m admits two linearly independent Frobenius
solutions, given by the expansions (11.100) with exponents r = +m and r = −m. On the
other hand, if m is an integer, there is only one Frobenius solution, namely the expansion
(11.100) for the positive index r = +m. The Frobenius recurrence with index r = −m
breaks down, and the second independent solution must include a logarithmic term; details
appear below.
By convention, the standard Bessel function of order m is obtained by multiplying
the Frobenius solution (11.100) with r = m by
1
2m m! ,
or, more generally,
1
2m Γ(m + 1) ,
(11.101)
where the ﬁrst factorial form can be used if m is a nonnegative integer, while the more
general gamma function expression must be employed for non-integral values of m. The
result is
Jm(x) =
∞

k=0
(−1)kxm+2k
22k+m k! (m + k)!
(11.102)
=
1
2m m!

xm −
xm+2
4(m+1) +
xm+4
32(m+1)(m+2) −
xm+6
384(m+1)(m+2)(m+3) + · · ·

.
When m is non-integral, the (m + k)! should be replaced by Γ(m + k + 1), and m! by
Γ(m + 1).
With this convention, the series is well deﬁned for all real m except when
m = −1, −2, −3, . . . is a negative integer. Actually, if m is a negative integer, the ﬁrst
m terms in the series vanish, because, at negative integer values, Γ(−n) = ∞. With this
convention, one can prove that
J−m(x) = (−1)mJm(x),
m = 1, 2, 3, . . . .
(11.103)
A simple application of the ratio test tells us that the power series converges for all
(complex) values of x, and hence Jm(x) is everywhere analytic. Indeed, the convergence
is quite rapid when x is of moderate size, and so summing the series is a reasonably eﬀec-
tive method for computing the Bessel function Jm(x) — although in serious applications
one adopts more sophisticated numerical techniques based on asymptotic expansions and
integral formulas, [85, 86]. In particular, we note that
J0(0) = 1,
Jm(0) = 0,
m > 0.
(11.104)
Figure 11.4 displays graphs of the ﬁrst four Bessel functions for 0 ≤x ≤20; the vertical
axes range from −.5 to 1.0. Most software packages, both symbolic and numeric, include

11.3 Series Solutions of Ordinary Diﬀerential Equations
469
J0(x)
J1(x)
J2(x)
J3(x)
Figure 11.4.
Bessel functions.
routines for accurately evaluating and graphing Bessel functions, and their properties can
be regarded as well known.
Example 11.8. Consider the Bessel equation of order m = 1
2. There are two indices,
r = ± 1
2, and the Frobenius method yields two independent solutions: J1/2(x) and J−1/2(x).
For the ﬁrst, with r = 1
2, the recurrence relation (11.99) takes the form
un = −
un−2
(n + 1)n .
Starting with u0 = 1 and u1 = 0, the general formula is easily found to be
un =
⎧
⎨
⎩
(−1)k
(n + 1)! ,
n = 2k even,
0
n = 2k + 1 odd.
Therefore, the resulting solution is
u(x) = √x
∞

k=0
(−1)k
(2k + 1)! x2k =
1
√x
∞

k=0
(−1)k
(2k + 1)! x2k+1 = sin x
√x .
According to (11.101), the Bessel function of order 1
2 is obtained by dividing this function
by
√
2 Γ
 3
2

=
π
2 ,
where we used (11.66) to evaluate the gamma function at 3
2. Therefore,
J1/2(x) =

2
πx sin x .
(11.105)

470
11 Dynamics of Planar Media
Similarly, for the other index r = −1
2, the recurrence relation
un = −
un−2
n(n −1)
leads to the formula
un =
⎧
⎨
⎩
(−1)k
n!
,
n = 2k even,
0
n = 2k + 1 odd,
for its coeﬃcients, corresponding to the solution
u(x) = x−1/2
∞

k=0
(−1)k
(2k)! x2k = cos x
√x .
Therefore, in view of (11.101) and (11.65), the Bessel function of order −1
2 is
J−1/2(x) =
√
2
Γ
 1
2
 cos x
√x =

2
πx cos x .
(11.106)
As we noted above, if m is not an integer, the two independent solutions to the Bessel
equation of order m are Jm(x) and J−m(x). However, when m is an integer, (11.103)
implies that these two solutions are constant multiples of each other, and so one must look
elsewhere for a second independent solution. One method is to use a generalized Frobenius
expansion involving a logarithmic term, i.e., (11.92) when m = 0 (see Exercise 11.3.33)
or (11.93) when m > 0. A second approach is to employ the reduction procedure used in
Example 11.7. Yet another option relies on the following limiting procedure; see [85, 119]
for full details.
Theorem 11.9.
If m > 0 is not an integer, then the Bessel functions Jm(x) and
J−m(x) provide two linearly independent solutions to the Bessel equation of order m. On
the other hand, if m = 0, 1, 2, 3, . . . is an integer, then a second independent solution,
traditionally denoted by Ym(x) and called the Bessel function of the second kind of order
m, can be found as a limiting case
Ym(x) = lim
ν →m
Jν(x) cosν π −J−ν(x)
sin ν π
(11.107)
of a certain linear combination of Bessel functions of non-integral order ν.
With some further analysis, it can be shown that the Bessel function of the second
kind of order m has the logarithmic Frobenius expansion
Ym(x) = 2
π
"
γ + log x
2
#
Jm(x) +
∞

k=0
bkx2k−m,
m = 0, 1, 2, . . . ,
(11.108)
with coeﬃcients
bk =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
−(m −k −1)!
π 22k−m k! ,
0 ≤k ≤m −1,
(−1)k−m−1(hk−m + hk)
π 22k−m k! (k −m)!
,
k ≥m,

11.3 Series Solutions of Ordinary Diﬀerential Equations
471
Y0(x)
Y1(x)
Y2(x)
Y3(x)
Figure 11.5.
Bessel functions of the second kind.
where
h0 = 0,
hk = 1 + 1
2 + 1
3 + · · · + 1
k ,
k > 0,
while
γ = lim
k →∞

hk −log k

≈.5772156649 . . .
(11.109)
is known as the Euler or Euler–Mascheroni constant. All Bessel functions of the second
kind have a singularity at the origin x = 0; indeed, by inspection of (11.108), we ﬁnd that
the leading asymptotics as x →0 are
Y0(x) ∼2
π log x,
Ym(x) ∼−2m (m −1)!
π xm
,
m > 0.
(11.110)
Figure 11.5 contains graphs of the ﬁrst four Bessel function of the second kind on the
interval 0 < x ≤20; the vertical axis ranges from −1 to 1.
Finally, we show how Bessel functions of diﬀerent orders are interconnected by two
important recurrence relations.
Proposition 11.10. The Bessel functions are related by the following formulae:
dJm
dx + m
x Jm(x) = Jm−1(x),
−dJm
dx + m
x Jm(x) = Jm+1(x).
(11.111)
Proof : Diﬀerentiating the power series
xmJm(x) =
∞

k=0
(−1)kx2m+2k
22k+m k! (m + k)!

472
11 Dynamics of Planar Media
produces
d
dx [ xmJm(x) ] =
∞

k=0
(−1)k 2 (m + k)x2m+2k−1
22k+m k! (m + k)!
= xm
∞

k=0
(−1)kxm−1+2k
22k+m−1 k! (m −1 + k)! = xm Jm−1(x).
(11.112)
Expansion of the left-hand side of this formula leads to
xm dJm
dx + mxm−1Jm(x) = d
dx [ xmJm(x) ] = xmJm−1(x),
which establishes the ﬁrst recurrence formula (11.111). The second is proved by a similar
manipulation involving diﬀerentiation of x−m Jm(x).
Q.E.D.
For example, using the second recurrence formula (11.111) along with (11.105), we
can write the Bessel function of order 3
2 in elementary terms:
J3/2(x) = −
dJ1/2(x)
dx
+ 1
2x J1/2(x)
= −

2
π
 cos x
x1/2 −sin x
2x3/2

+

2
π
sin x
2x3/2 =

2
π
sin x −x cosx
x3/2
.
(11.113)
Iterating, one concludes that Bessel functions of half-integral order, m = ± 1
2 , ± 3
2 , ± 5
2 , . . . ,
are all elementary functions, in that they can be written in terms of trigonometric func-
tions and powers of √x . We will make use of these functions in our treatment of the
three-dimensional heat and wave equations in spherical geometry. On the other hand, all
of the other Bessel functions are non-elementary special functions.
With this, we conclude our brief introduction to the method of Frobenius and the
basics of Bessel functions. The reader interested in delving further into either the general
method or the host of additional properties of Bessel functions is encouraged to consult a
more specialized text, e.g., [59, 85, 119].
Exercises
11.3.22. Consider the ordinary diﬀerential equation 2xu′′ + u′ + xu = 0. (a) Prove that x = 0
is a regular singular point. (b) Find two independent series solutions in powers of x.
♥11.3.23. Consider the diﬀerential equation
u′′
2 −x = u
x2 . (a) Classify all x0 ∈R as either a
(i) regular point; (ii) regular singular point; and/or (iii) irregular singular point. Explain
your answers. (b) Find a series solution to the equation based at the point x0 = 0, or ex-
plain why none exists. What is the radius of convergence of your series?
11.3.24. Consider the diﬀerential equation u′′ +

1 −1
x

u′ + u = 0.
(a) Classify all x0 ∈R as either (i) a regular point; (ii) a regular singular point; (iii) an
irregular singular point; (iv) none of the above. Explain your answers.
(b) Write out the ﬁrst ﬁve nonzero terms in a series solution.

473
11.3.25. Consider the diﬀerential equation 4xu′′ + 2u′ + u = 0. (a) Classify the values of x for
which the equation has regular points, regular singular points, and irregular singular points.
(b) Find two independent series solutions, in powers of x. For what values of x do your
series converge? (c) By inspection of your series, write the general solution to the equation
in terms of elementary functions.
♥11.3.26. The Chebyshev diﬀerential equation is (1 −x2)u′′ −xu′ + m2u = 0. (a) Find all
(i) regular points; (ii) regular singular points; (iii) irregular singular points. (b) Show
that if m is an integer, the equation has a polynomial solution of degree m, known as a
Chebyshev polynomial. Write down the Chebyshev polynomials of degrees 1, 2, and 3.
(c) For m = 1, ﬁnd two linearly independent series solutions based at the point x0 = 1.
11.3.27. Write the following Bessel functions in terms of elementary functions:
(a) J5/2(x),
(b) J7/2(x),
(c) J−3/2(x).
♦11.3.28. Prove the identity (11.103).
11.3.29. Suppose that u(x) solves Bessel’s equation. (a) Find a second order ordinary diﬀeren-
tial equation satisﬁed by the function w(x) = √x u(x). (b) Use this result to rederive the
formulas for J1/2(x) and J−1/2(x).
♦11.3.30. Let m ≥0 be real, and consider the modiﬁed Bessel equation of order m:
x2 u′′ + x u′ −(x2 + m2) u = 0.
(11.114)
(a) Explain why x0 = 0 is a regular singular point.
(b) Use the method of Frobenius to construct a series solution based at x0 = 0. Can you
relate your solutions to the Bessel function Jm(x)?
♦11.3.31.(a) Let a, b, c be constants with b, c ̸= 0. Show that the function u(x) = xaJ0(bxc)
solves the ordinary diﬀerential equation
x2 d2u
dx2 + (1 −2a)x du
dx + (b2c2x2c + a2) u = 0.
What is the general solution to this equation?
(b) Find the general solution to the ordinary diﬀerential equation
x2 d2u
dx2 + αx du
dx + (β x2c + γ) u = 0,
for constants α, β, γ, c with β, c ̸= 0.
♥11.3.32. Let k > 0 be a constant. The ordinary diﬀerential equation d2u
dt2 + e−2t u = 0 describes
the vibrations of a weakening spring whose stiﬀness k(t) = e−2t is exponentially decaying
in time. (a) Show that this equation can be solved in terms of Bessel functions of order 0.
Hint: Perform a change of variables.
(b) Does the solution tend to 0 as t →∞?
♥11.3.33. We know that u(x) = J0(x) is a solution to the Bessel equation of order 0, namely
xu′′ + u′ + xu = 0.
(11.115)
In accordance with the general Frobenius method, construct a second solution of the form
u(x) = J0(x) log x +
∞

n=1
vnxn.
11.3.34. Is it possible to have all solutions to an ordinary diﬀerential equation bounded at a
regular singular point? If not, explain why not. If true, give an example where this hap-
pens.
11.3 Series Solutions of Ordinary Diﬀerential Equations

474
11 Dynamics of Planar Media
11.4 The Heat Equation in a Disk, Continued
Now that we have acquired some familiarity with the solutions to Bessel’s ordinary diﬀer-
ential equation, we are ready to analyze the separable solutions to the heat equation in a
polar geometry. At the end of Section 11.2, we were left with the task of solving the Bessel
equation (11.58) of integer order m. As we now know, there are two independent solutions,
namely the Bessel function of the ﬁrst kind Jm, (11.102), and the more complicated Bessel
function of the second kind Ym, (11.107), and hence the general solution has the form
p(z) = c1Jm(z) + c2Ym(z),
for constants c1, c2. Reverting to our original radial coordinate r = z/
√
λ , we conclude
that every solution to the radial equation (11.56) has the form
p(r) = c1Jm
√
λ r

+ c2Ym
√
λ r

.
Now, the singular point r = 0 represents the center of the disk, and the solutions must
remain bounded there. While this is true for Jm(z), the second Bessel function Ym(z) has,
according to (11.110), a singularity at z = 0 and so is unsuitable for the present purposes.
(On the other hand, it plays a role in other situations, e.g., the heat equation on an annular
ring.) Thus, every separable solution that is bounded at r = 0 comes from the rescaled
Bessel function of the ﬁrst kind of order m:
p(r) = Jm
√
λ r

.
(11.116)
The Dirichlet boundary condition at the disk’s rim r = 1 requires
p(1) = Jm
√
λ

= 0.
Therefore, in order that λ be a bona ﬁde eigenvalue,
√
λ must be a root of the mth order
Bessel function Jm.
Remark: We already know, thanks to the positive deﬁniteness of the Dirichlet bound-
ary value problem, that the Helmholtz eigenvalues must all be positive, λ > 0, and so there
will be no diﬃculty in taking its square root.
The graphs of Jm(z) strongly indicate, and, indeed, it can be rigorously proved,
[85, 119], that as z increases above 0, each Bessel function oscillates, with slowly de-
creasing amplitude, between positive and negative values. In fact, asymptotically,
Jm(z) ∼

2
πz cos

z −
 1
2 m + 1
4

π

as
z −→∞,
(11.117)
and so the oscillations become essentially the same as a (phase-shifted) cosine whose am-
plitude decreases like z−1/2. As a consequence, there exists an inﬁnite sequence of Bessel
roots, which we number in increasing order:
Jm(ζm,n) = 0,
where
0 < ζm,1 < ζm,2 < ζm,3 < · · ·
with
ζm,n −→∞
as
n −→∞.
(11.118)
It is worth emphasizing that the Bessel functions are not periodic, and so their roots
are not evenly spaced. However, as a consequence of (11.117), the large Bessel roots are
asymptotically close to the evenly spaced roots of the shifted cosine:
ζm,n ∼

n + 1
2 m −1
4

π
as
n −→∞.
(11.119)

11.4 The Heat Equation in a Disk, Continued
475
Owing to their physical importance in a wide range of problems, the Bessel roots have
been extensively tabulated. The accompanying table displays all Bessel roots that are < 12
in magnitude. The columns of the table are indexed by m, the order of the Bessel function,
and the rows by n, the root number.
Table of Bessel Roots ζm,n
n
9
m
0
1
2
3
4
5
6
7
. . .
1
2.4048
3.8317
5.1356
6.3802
7.5883
8.7715
9.9361
11.0864
. . .
2
5.5201
7.0156
8.4172
9.7610
11.0647
...
...
...
3
8.6537
10.1735
11.6198
...
...
4
11.7915
...
...
...
...
Remark: According to (11.102),
Jm(0) = 0
for
m > 0,
while
J0(0) = 1.
However, we do not count 0 as a bona ﬁde Bessel root, since it does not lead to a valid
eigenfunction for the Helmholtz boundary value problem.
Summarizing our progress so far, the eigenvalues
λm,n = ζ2
m,n,
n = 1, 2, 3, . . .,
m = 0, 1, 2, . . .,
(11.120)
of the Bessel boundary value problem (11.56–57) are the squares of the roots of the Bessel
function of order m. The corresponding eigenfunctions are
wm,n(r) = Jm(ζm,n r) ,
n = 1, 2, 3, . . .,
m = 0, 1, 2, . . .,
(11.121)
deﬁned for 0 ≤r ≤1. Combining (11.121) with the formula (11.55) for the angular com-
ponents, we conclude that the separable solutions (11.53) to the polar Helmholtz boundary
value problem (11.51) are
v0,n(r) = J0(ζ0,n r),
vm,n(r, θ) = Jm(ζm,n r) cos mθ,
vm,n(r, θ) = Jm(ζm,n r) sin mθ,
where
m, n = 1, 2, 3, . . . .
(11.122)
These solutions deﬁne the normal modes for the unit disk; Figure 11.6 plots the ﬁrst few of
them. The eigenvalues λ0,n are simple, and contribute radially symmetric eigenfunctions,
whereas the eigenvalues λm,n for m > 0 are double, and produce two linearly independent
separable eigenfunctions, with trigonometric dependence on the angular variable.
Recalling the original ansatz (11.50), we have at last produced the basic separable
eigensolutions
u0,n(t, r) = e−ζ2
0,nt v0,n(r) = e−ζ2
0,nt J0(ζ0,n r),
um,n(t, r, θ) = e−ζ2
m,nt vm,n(r, θ) = e−ζ2
m,nt Jm(ζm,n r) cos mθ,
um,n(t, r, θ) = e−ζ2
m,nt vm,n(r, θ) = e−ζ2
m,nt Jm(ζm,n r) sin mθ,
m, n = 1, 2, 3, . . . ,
(11.123)

476
11 Dynamics of Planar Media
Figure 11.6.
Normal modes for a disk.
to the homogeneous Dirichlet boundary value problem for the heat equation on the unit
disk. The general solution is obtained by linear superposition, in the form of an inﬁnite
series
u(t, r, θ) = 1
2
∞

n=1
a0,n u0,n(t, r) +
∞

m,n=1

am,n um,n(t, r, θ) + bm,n um,n(t, r, θ)

, (11.124)
where the initial factor of 1
2 is included, as with ordinary Fourier series, for later conve-

11.4 The Heat Equation in a Disk, Continued
477
nience. As usual, the coeﬃcients am,n, bm,n are determined by the initial condition
u(0, r, θ) = 1
2
∞

n=1
a0,n v0,n(r) +
∞

m,n=1

am,n vm,n(r, θ) + bm,n vm,n(r, θ)

= f(r, θ).
(11.125)
This requires that we expand the initial data into a Fourier–Bessel series in the eigen-
functions. As before, it is possible to prove, [34], that the separable eigenfunctions are
complete — there are no other eigenfunctions — and hence every (reasonable) function
deﬁned on the unit disk can be written as a convergent series in the Bessel eigenfunctions.
Theorem 9.33 gurantees that the eigenfunctions are orthogonal† with respect to the
standard L2 inner product
⟨u , v ⟩=
 
D
u(x, y) v(x, y)dx dy =
 1
0
 π
−π
u(r, θ) v(r, θ) r dθ dr
on the unit disk. (Note the extra factor of r coming from the polar coordinate form of
the area element dx dy = r dr dθ.) The L2 norms of the Fourier–Bessel eigenfunctions are
given by the interesting formulae
∥v0,n ∥= √π
 J1(ζ0,n)
 ,
∥vm,n ∥= ∥vm,n ∥=
π
2
 Jm+1(ζm,n)
 ,
(11.126)
which involve the value of the Bessel function of the next-higher order at the appropriate
Bessel root. A proof of (11.126) can be found in Exercise 11.4.22, while numerical values
are provided in the accompanying table.
Norms of the Fourier–Bessel Eigenfunctions ∥vm,n ∥= ∥vm,n ∥
n
9
m
0
1
2
3
4
5
6
7
1
.9202
.5048
.4257
.3738
.3363
.3076
.2847
.2658
2
.6031
.3761
.3401
.3126
.2906
.2725
.2572
.2441
3
.4811
.3130
.2913
.2736
.2586
.2458
.2347
.2249
4
.4120
.2737
.2589
.2462
.2352
.2255
.2169
.2092
5
.3661
.2462
.2353
.2257
.2171
.2095
.2025
.1962
Orthogonality of the eigenfunctions implies that the coeﬃcients in the Fourier–Bessel
†
For the two independent eigenfunctions corresponding to one of the double eigenvalues,
orthogonality must be veriﬁed by hand, but, in this case, it follows easily from the orthogonality
of their trigonometric components.

478
11 Dynamics of Planar Media
Figure 11.7.
Heat diﬀusion in a disk.

series (11.125) are given by the inner product formulae
a0,n = 2
⟨f , v0,n ⟩
∥v0,n ∥2 =
2
π J1(ζ0,n)2
 1
0
 π
−π
f(r, θ) J0(ζ0,n r) r dθ dr,
am,n = ⟨f , vm,n ⟩
∥vm,n ∥2 =
2
π Jm+1(ζm,n)2
 1
0
 π
−π
f(r, θ) Jm(ζm,n r) r cos mθ dθ dr,
bm,n =
⟨f , vm,n ⟩
∥vm,n ∥2 =
2
π Jm+1(ζm,n)2
 1
0
 π
−π
f(r, θ) Jm(ζm,n r) r sin mθ dθ dr.
(11.127)
In accordance with the general theory, each individual separable solution (11.123) to
the heat equation decays exponentially fast, at a rate λm,n = ζ2
m,n prescribed by the square
of the corresponding Bessel root. In particular, the dominant mode, meaning the one that
persists the longest, is
u0,1(t, r, θ) = e−ζ2
0,1 t J0(ζ0,1 r).
(11.128)
Its decay rate is prescribed by the smallest positive eigenvalue:
ζ2
0,1 ≈5.783,
(11.129)
which is the square of the smallest root of the Bessel function J0(z). Since J0(z) > 0 for
0 ≤z < ζ0,1, the dominant eigenfunction v0,1(r, θ) = J0(ζ0,1 r) > 0 is radially symmet-
ric and strictly positive within the entire disk. Consequently, for most initial conditions

11.4 The Heat Equation in a Disk, Continued
479
(speciﬁcally those for which a0,1 ̸= 0), the disk’s temperature distribution eventually be-
comes entirely of one sign and radially symmetric, while decaying exponentially fast to zero
at the rate given by (11.129). See Figure 11.7 for a plot of a typical solution. Note how,
in accordance with the theory, the solution soon acquires a radial symmetry as it decays
to thermal equilibrium.
Exercises
11.4.1. At the initial time t0 = 0, a concentrated unit heat source is instantaneously applied at
position x = 1
2, y = 0, to a circular metal disk of unit radius and unit thermal diﬀusivity
whose outside edge is held at 0◦. Write down an eigenfunction series for the resulting tem-
perature distribution at time t > 0. Hint: Be careful working with the delta function in
polar coordinates; see Exercise 6.3.6.
11.4.2. Solve Exercise 11.4.1 when the concentrated unit heat source is instantaneously applied
at the center of the disk.
♥11.4.3.(a) Write down the Fourier–Bessel series for the solution to the heat equation on a unit
disk with γ = 1, whose circular edge is held at 0◦and subject to the initial conditions
u(0, x, y) ≡1 for x2 + y2 ≤1. Hint: Use (11.112) to evaluate the integrals for the coeﬃ-
cients. (b) Approximate the time t⋆≥0 after which the temperature of the disk is every-
where ≤.5◦.
♣11.4.4.(a) Write down the ﬁrst three nonzero terms in the Fourier–Bessel series for the solution
to the heat equation on a unit disk with γ = 1 whose circular edge is held at 0◦subject to
the initial conditions u(0, r, θ) = 1 −r for r ≤1. Use numerical integration to evaluate the
coeﬃcients.
(b) Use your approximation to determine at which times t ≥0 the tempera-
ture of the disk is everywhere ≤.5◦.
11.4.5. Prove that every separable eigenfunction of the Dirichlet boundary value problem for
the Helmholtz equation in the unit disk can be written in the form
c Jm(ζm,n r) cos(mθ −α)
for ﬁxed c ̸= 0 and −π < α ≤π.
11.4.6. Suppose the initial data f(r, θ) in (11.49) satisﬁes
	 1
0
	 π
−π f(r, θ) J0(ζ0,1 r) r dθ dr = 0.
(a) What is the decay rate to equilibrium of the resulting heat equation solution u(t, r, θ)?
(b) Prove that, generically, the asymptotic temperature distribution has half the disk above
the equilibrium temperature and the other half below. Can you predict the diameter that
separates the two halves? (c) If you know that a0,1 = 0, and also that the long-time tem-
perature distribution is radially symmetric, what is the (generic) decay rate? What is the
asymptotic temperature distribution?
♦11.4.7. Show how to use a scaling symmetry to solve the heat equation in a disk of radius R
knowing the solution in a disk of radius 1.
11.4.8. Use rescaling, as in Exercise 11.4.7, to produce the solution to the Dirichlet initial-
boundary value problem for a disk of radius 2 with diﬀusion coeﬃcient γ = 5.
11.4.9. If it takes a disk of unit radius 3 minutes to reach (approximate) thermal equilibrium,
how long will it take a disk of radius 2 made out of the same material and subject to the
same homogeneous boundary conditions to reach equilibrium?
11.4.10. Assuming Dirichlet boundary conditions, does a square or a circular disk of the same
area reach thermal equilibrium faster? Use your intuition ﬁrst, and then check using the
explicit formulas.

480
11 Dynamics of Planar Media
11.4.11. Answer Exercise 11.4.10 when the square and circle have the same perimeter.
11.4.12. Which reaches thermal equilibrium faster: a disk whose edge is held at 0◦or a disk of
the same radius that is fully insulated?
11.4.13. A circular metal disk is removed from an oven and then fully insulated.
True or false: (a) The eventual equilibrium temperature is constant.
(b) For large t ≫0, the temperature u(t, x, y) becomes more and more radially symmetric.
If false, what can you say about the temperature proﬁle at large times?
♥11.4.14.(a) Write down an eigenfunction series formula for the temperature dynamics of a disk
of radius 1 that has an insulated boundary. (b) What is the eventual equilibrium temper-
ature? (c) Is the rate of decay to thermal equilibrium (i) faster, (ii) slower, or (iii) the
same as a disk with Dirichlet boundary conditions?
♥11.4.15. Write out a series solution for the temperature in a half-disk of radius 1, subject to
(a) homogeneous Dirichlet boundary conditions on its entire boundary; (b) homogeneous
Dirichlet conditions on the circular part of its boundary and homogeneous Neumann con-
ditions on the straight part. (c) Which of the two boundary conditions results in a faster
return to equilibrium temperature? How much faster?
11.4.16. A large sheet of metal is heated to 100◦. A circular disk and a semi-circular half-disk
of the same radius are cut out of it. Their edges are then held at 0◦, while being fully insu-
lated from above and below.
(a) True or false: The half-disk goes to thermal equilibrium twice as fast as the disk.
(b) If you need to wait 20 minutes for the circular disk to cool down enough to be picked up
in your bare hands, how long do you need to wait to pick up the semi-circular disk?
♣11.4.17. Two identical plates have the shape of an annular ring {1 < r < 2} with inner radius
1 and outer radius 2. The ﬁrst has an insulated inner boundary and outer boundary held
at 0◦, while the second has an insulated outer boundary and inner boundary held at 0◦. If
both start out at the same temperature, which reaches thermal equilibrium faster? Quan-
tify the rates of decay.
♥11.4.18. Let m ≥0 be a nonnegative integer. In this exercise, we investigate the completeness
of the eigenfunctions of the Bessel boundary value problem (11.56–57). To this end, deﬁne
the Sturm–Liouville linear diﬀerential operator
S[u] = −1
x
d
dx

x du
dx

+ m2
x2 u,
subject to the boundary conditions | u′(0) | < ∞, u(1) = 0, and either | u(0) | < ∞when
m = 0, or u(0) = 0 when m > 0.
(a) Show that S is self-adjoint relative to the inner product ⟨f , g ⟩=
	 1
0 f(x) g(x) x dx.
(b) Prove that the eigenfunctions of S are the rescaled Bessel functions Jm(ζm,n x) for n =
1, 2, 3, . . . . What are the orthogonality relations?
(c) Find the Green’s function G(x; ξ) and modiﬁed Green’s function G(x; ξ), cf. (9.59), asso-
ciated with the boundary value problem S[u] = 0.
(d) Use the criterion of Theorem 9.47 to prove that the eigenfunctions are complete.
11.4.19. Determine the Bessel roots ζ1/2,n. Do they satisfy the asymptotic formula (11.119)?
♣11.4.20. Use a numerical root ﬁnder to compute the ﬁrst 10 Bessel roots ζ3/2,n, n = 1, . . . , 10.
Compare your values with the asymptotic formula (11.119).
♦11.4.21. Prove that Jm−1(ζm,n) = −Jm+1(ζm,n).
♦11.4.22. In this exercise, we prove formula (11.126).
(a) First, use the recurrence formulae (11.111) to prove
d
dx

x2
Jm(x)2 −Jm−1(x) Jm+1(x)
  
= 2x Jm(x)2.
(b) Integrate both sides of the previous formula from 0 to the Bessel zero ζm,n and then

481
use Exercise 11.4.21 to show that
	 ζm,n
0
x Jm(x)2 dx = −
ζ2
m,n
2
Jm−1(ζm,n) Jm+1(ζm,n) =
ζ2
m,n
2
Jm+1(ζm,n)2.
(c) Next, use a change of variables to establish the identity
	 1
0 z Jm(ζm,n z)2 dz = 1
2 Jm+1(ζm,n)2.
(d) Finally, use the formulae for vm,n and vm,n to complete the proof of (11.126).
♦11.4.23. Prove directly that the eigenfunctions vm,n(r, θ) and vm,n(r, θ) in (11.122) are orthog-
onal with respect to the L2 inner product on the unit disk.
11.4.24. Establish the following alternative formulae for the eigenfunction norms:
∥v0,n ∥= √π
 J ′
0(ζ0,n)
 ,
∥vm,n ∥= ∥vm,n ∥=
π
2
 J ′
m(ζm,n)
 .
11.5 The Fundamental Solution to the Planar Heat Equation
As we learned in Section 4.1, the fundamental solution to the heat equation measures
the temperature distribution resulting from a concentrated initial heat source, e.g., a hot
soldering iron applied instantaneously at a single point on a metal plate. The physical
problem is modeled mathematically by taking a delta function as the initial data along
with the relevant homogeneous boundary conditions. Once the fundamental solution is
known, one is able to use linear superposition to recover the solution generated by any
other initial data.
As in our one-dimensional analysis, we shall concentrate on the most tractable case,
in which the domain is the entire plane: Ω = R2. Thus, our ﬁrst goal is to solve the initial
value problem
ut = γ Δu,
u(0, x, y) = δ(x −ξ) δ(y −η),
(11.130)
for t > 0 and (x, y) ∈R2. The solution u = F(t, x; ξ) = F(t, x, y; ξ, η) to this initial value
problem is known as the fundamental solution for the heat equation on R2.
The quickest route to the desired formula relies on the following means of combining
solutions of the one-dimensional heat equation to produce solutions of the two-dimensional
version.
Lemma 11.11. Let v(t, x) and w(t, x) be any two solutions to the one-dimensional
heat equation ut = γ uxx. Then their product
u(t, x, y) = v(t, x) w(t, y)
(11.131)
is a solution to the two-dimensional heat equation ut = γ (uxx + uyy).
Proof : Our assumptions imply that vt = γ vxx, while wt = γ wyy when we write
w(t, y) as a function of t and y. Therefore, diﬀerentiating (11.131), we ﬁnd
∂u
∂t = ∂v
∂t w + v ∂w
∂t = γ ∂2v
∂x2 w + γ v ∂2w
∂y2 = γ
 ∂2u
∂x2 + ∂2u
∂y2

,
and hence u(t, x, y) solves the two-dimensional heat equation.
Q.E.D.
11.5 The Fundamental Solution to the Planar Heat Equation

482
11 Dynamics of Planar Media
For example, if
v(t, x) = e−γ α2 t sin αx,
w(t, y) = e−γ β2 t sin β y,
are separable solutions of the one-dimensional heat equation, then
u(t, x, y) = e−γ (α2+β2)t sin αx sin β y
are the separable solutions we used to solve the heat equation on a rectangle. A more
interesting case is to choose
v(t, x) =
1
2 √πγ t e−(x−ξ)2/(4γ t),
w(t, y) =
1
2 √πγ t e−(y−η)2/(4γ t),
(11.132)
to be the fundamental solutions (8.14) to the one-dimensional heat equation at respec-
tive locations x = ξ and y = η. Multiplying these two solutions together produces the
fundamental solution for the two-dimensional problem.
Theorem 11.12.
The fundamental solution to the heat equation ut = γ Δu corre-
sponding to a unit delta function placed at position (ξ, η) ∈R2 at the initial time t0 = 0
is
F(t, x, y; ξ, η) =
1
4πγ t e−[(x−ξ)2+(y−η)2 ]/(4γ t).
(11.133)
Proof : Since we already know that both function (11.132) are solutions to the one-
dimensional heat equation, Lemma 11.11 guarantees that their product, which equals
(11.133), solves the two-dimensional heat equation for t > 0.
Moreover, at the initial
time,
u(0, x, y) = v(0, x) w(0, y) = δ(x −ξ) δ(y −η)
is a product of delta functions, and hence the result follows. Indeed, the total heat
 
u(t, x, y) dx dy =
 ∞
−∞
v(t, x) dx
 ∞
−∞
w(t, y) dy = 1,
t ≥0,
remains constant, while
lim
t→0+ u(t, x, y) =
 ∞,
(x, y) = (ξ, η),
0,
otherwise,
has the standard delta function limit at the initial time instant.
Q.E.D.
Figure 11.8 depicts the evolution of the fundamental solution when γ = 1 at the
indicated times.
Observe that the initially concentrated temperature spreads out in a
radially symmetric manner, while the total amount of heat remains constant.
At any
individual point (x, y) ̸= (0, 0), the initially zero temperature rises slightly at ﬁrst, but
then decays monotonically back to zero at a rate proportional to 1/t.
As in the one-
dimensional case, since the fundamental solution is > 0 for all t > 0, the heat energy has
an inﬁnite speed of propagation.
Both the one- and two-dimensional fundamental solutions have bell-shaped proﬁles
known as Gaussian ﬁlters. The most important diﬀerence is the initial factor. In a one-
dimensional medium, the fundamental solution decays in proportion to 1/
√
t, whereas in
the plane the decay is more rapid, being proportional to 1/t. The physical explanation is
that the heat energy is able to spread out in two independent directions, and hence diﬀuses

11.5 The Fundamental Solution to the Planar Heat Equation
483
Figure 11.8.
Fundamental solution of the planar heat equation.

away from its initial source more rapidly. As we shall see, the decay in three-dimensional
space is more rapid still, being proportional to t−3/2 for similar reasons; see (12.120).
The principal use of the fundamental solution is for solving the general initial value
problem. We express the initial temperature distribution as a superposition of delta func-
tion impulses,
u(0, x, y) = f(x, y) =
 
f(ξ, η) δ(x −ξ, y −η) dξ dη,
where, at the point (ξ, η) ∈R2, the impulse has magnitude f(ξ, η). Linearity implies that
the solution is then given by the same superposition of fundamental solutions.
Theorem 11.13. The solution to the initial value problem
ut = γ Δu,
u(t, x, y) = f(x, y),
(x, y) ∈R2,
for the planar heat equation is given by the linear superposition formula
u(t, x, y) =
1
4πγ t
 
f(ξ, η) e−[(x−ξ)2+(y−η)2 ]/(4γ t) dξ dη.
(11.134)

484
11 Dynamics of Planar Media
Figure 11.9.
Diﬀusion of a disk.

We can interpret the solution formula (11.134) as a two-dimensional convolution
u(t, x, y) = F(t, x, y) ∗f(x, y)
(11.135)
of the initial data with a one-parameter family of progressively wider and shorter Gaussian
ﬁlters
F(t, x, y) = F(t, x, y; 0, 0) =
1
4πγ t e−(x2+y2)/(4γ t).
(11.136)
As in (7.54), such a convolution can be interpreted as a Gaussian weighted averaging of
the function f(x, y), which has the eﬀect of smoothing out the initial data.
Example 11.14.
If our initial temperature distribution is constant on a circular
region, say
u(0, x, y) =
 1
x2 + y2 < 1,
0,
otherwise,
then the solution can be evaluated using (11.134), as follows:
u(t, x, y) =
1
4π t
 
D
e−[(x−ξ)2+(y−η)2 ]/(4t) dξ dη,
where the integral is over the unit disk D = {ξ2 + η2 ≤1}. Unfortunately, the integral
cannot be expressed in terms of elementary functions.
On the other hand, numerical

11.5 The Fundamental Solution to the Planar Heat Equation
485
evaluation of the integral is straightforward. A plot of the resulting radially symmetric
solution appears in Figure 11.9. One could also interpret this solution as the diﬀusion of
an animal population in a uniform isotropic environment or bacteria in a similarly uniform
large petri dish that are initially conﬁned to a small circular region.
Exercises
11.5.1. Solve the following initial value problem: ut = 5(uxx + uyy),
u(0, x, y) = e−(x2+y2).
11.5.2. Write down an integral formula for the solution to the following initial value problem:
ut = 3(uxx + uyy),
u(0, x, y) = (1 + x2 + y2)−2.
11.5.3. At the initial time t = 0, a unit heat source is instantaneously applied at the origin
of the (x, y)–plane. For t > 0, what is the maximum temperature experienced at a point
(x, y) ̸= 0? At what time is the maximum temperature achieved? Does the temperature
approach an equilibrium value as t →∞? If so, how fast?
11.5.4.(a) Find an eigenfunction series representation of the fundamental solution for the heat
equation ut = Δu on the unit square {0 ≤x, y ≤1} when subject to homogeneous Dirich-
let boundary conditions. (b) Write the solution to the initial value problem u(0, x, y) =
f(x, y) in terms of the fundamental solution. (c) Discuss how your formula is related to the
Fourier series solution (11.43).
11.5.5. Let u(t, x, y) be a solution to the heat equation on all of R2 such that u and ∥∇u ∥→0
rapidly as ∥x ∥→∞. (a) Prove that the total heat H(t) =
		
u(t, x, y) dx dy is constant.
(b) Explain how this can be reconciled with the statement that u(t, x, y) →0 as t →∞at
all points (x, y) ∈R2.
♦11.5.6. Consider the initial value problem ut = γ Δu+H(t, x, y), u(0, x, y) = 0, for the inhomo-
geneous heat equation on the entire (x, y)–plane, where H(t, x, y) represents a time-varying
external heat source. Derive an integral formula for its solution. Hint: Mimic the solution
method in Section 8.1.
11.5.7. A ﬂat plate of inﬁnite extent with unit thermal diﬀusivity starts oﬀat 0◦. From then
on, a unit heat source is continually applied at the origin. Find the resulting temperature
distribution. Does the temperature eventually reach a steady state? Hint: Use Exercise
11.5.6.
♥11.5.8. Building on Example 11.14, we model the “diﬀusion” of a set D ⊂R2 as the solution
u(t, x, y) to the heat equation ut = Δu subject to the initial condition u(0, x, y) = χD(x, y),
where χD(x, y) =
 1,
(x, y) ∈D,
0,
(x, y) ̸∈D,
is the characteristic function of the set D.
(a) Write down a formula for the diﬀusion of the set D.
(b) True or false: At each t, the diﬀusion u(t, x, y) is the characteristic function of a set Dt.
(c) Prove that 0 < u(t, x, y) < 1 for all (x, y) and t > 0.
(d) What is
lim
t →∞u(t, x, y)?
(e) Write down a formula for the diﬀusion of a unit square D = {0 ≤x, y ≤1}, and then
plot the result at several times. Discuss what you observe.
11.5.9.(a) Explain why the delta function on R2 satisﬁes the scaling law δ(x, y) = β2 δ(βx, β y),
for β > 0. (b) Verify that the fundamental solution to the heat equation on R2 obeys the
same scaling law: F(t, x, y) = β2 F(β2 t, β x, β y).
(c) Is the fundamental solution a simi-
larity solution?

486
11 Dynamics of Planar Media
11.5.10.(a) Find the fundamental solution on R2 to the cable equation ut = γ Δu −α u, where
α > 0 is constant. (b) Use your solution to write down a formula for the solution to the
general initial value problem u(0, x, y) = f(x, y) for (x, y) ∈R2.
11.5.11.(a) Prove that if v(t, x) and w(t, x) solve the dispersive wave equation (8.90), then
their product u(t, x, y) = v(t, x) w(t, y) solves the two-dimensional dispersive equation
ut + uxxx + uyyy = 0.
(b) What is the fundamental solution on R2 of the latter equation? (c) Write down an in-
tegral formula for the solution to the initial value problem u(0, x, y) = f(x, y) for (x, y) ∈R2.
11.5.12. Deﬁne the two-dimensional convolution f ∗g of functions f(x, y) and g(x, y) so that
equation (11.135) is valid.
11.6 The Planar Wave Equation
Let us next consider the two-dimensional wave equation
∂2u
∂t2 = c2Δu = c2
 ∂2u
∂x2 + ∂2u
∂y2

,
(11.137)
which models the unforced transverse vibrations of a homogeneous membrane, e.g., a drum.
Here, u(t, x, y) represents the vertical displacement of the membrane at time t and position
(x, y) ∈Ω, where the domain Ω ⊂R2, assumed bounded, represents the undeformed shape.
The constant c2 > 0 encapsulates the membrane’s physical properties — density, tension,
stiﬀness, etc.; its square root, c, is called, as in the one-dimensional case, the wave speed,
since it represents the speed of propagation of localized signals.
Remark: In this simpliﬁed model, we are only allowing small, transverse (vertical)
displacements of the membrane.
Large elastic vibrations lead to the nonlinear partial
diﬀerential equations of elastodynamics, [7]. In particular, the bending vibrations of a
ﬂexible elastic plate are governed by a more complicated fourth-order partial diﬀerential
equation.
The solution u(t, x, y) to the wave equation will be uniquely speciﬁed once we impose
suitable boundary and initial conditions. The Dirichlet conditions
u(t, x, y) = h(x, y),
(x, y) ∈∂Ω,
(11.138)
correspond to gluing our membrane to a ﬁxed boundary — a rim; more generally, we can
also allow h to depend on t, modeling a membrane attached to a moving boundary. On
the other hand, the homogeneous Neumann conditions
∂u
∂n (t, x, y) = 0,
(x, y) ∈∂Ω,
(11.139)
represent a free boundary where the membrane is not attached to any support — although
in this model, its edge is allowed to move only in a vertical direction. Mixed boundary
conditions attach part of the boundary and leave the remaining portion free to vibrate:
u = h
on
D ⊊∂Ω,
∂u
∂n = 0
on
N = ∂Ω \ D.
(11.140)

487
Since the wave equation is of second order in time, to uniquely specify the solution we need
to impose two initial conditions,
u(0, x, y) = f(x, y),
∂u
∂t (0, x, y) = g(x, y),
(x, y) ∈Ω.
(11.141)
The ﬁrst speciﬁes the membrane’s initial displacement, while the second prescribes its
initial velocity.
Separation of Variables
Unfortunately, the d’Alembert solution method does not apply to the two-dimensional
wave equation in any obvious manner.
The reason is that, unlike the one-dimensional
version (2.69), one cannot factorize the planar wave operator □= ∂2
t −c2 ∂2
x −c2 ∂2
y, thus
precluding any sort of reduction to a ﬁrst-order partial diﬀerential equation. However, this
is not the end of the story, and we will return to this issue at the end of Section 12.6.
We thus fall back on our universal solution tool for linear partial diﬀerential equations
— separation of variables. According to the general framework established in Section 9.5,
the separable solutions to the wave equation have the trigonometric form
uk(t, x, y) = cos(ωk t) vk(x, y)
and
uk(t, x, y) = sin(ωk t) vk(x, y).
(11.142)
Substituting back into the wave equation, we ﬁnd that vk(x, y) must be an eigenfunction
of the associated Helmholtz boundary value problem
c2
 ∂2u
∂x2 + ∂2u
∂y2

+ λk v = 0,
(11.143)
whose eigenvalue λk = ω2
k equals the square of the vibrational frequency. According to
Theorem 9.47, on a bounded domain, there is an inﬁnite number of such normal modes with
progressively faster vibrational frequencies: ωk →∞as k →∞. In addition, in the positive
semi-deﬁnite case — which occurs under homogeneous Neumann boundary conditions —
there is a single constant null eigenfunction, leading to the additional separable solutions
u0(t, x, y) = 1
and
u0(t, x, y) = t.
(11.144)
The ﬁrst represents a stationary membrane that has been displaced to a ﬁxed height, while
the second represents a membrane that is moving oﬀin the vertical direction with constant
unit speed. (Think of the membrane moving in outer space unaﬀected by any external
gravitational force.) As in Section 9.5, the general solution can be written as an inﬁnite
series in the eigensolutions (11.142). Unfortunately, as we know, the Helmholtz boundary
value problem can be explicitly solved only on a rather restricted class of domains. Here
we will content ourselves with investigating the two most important cases: rectangular and
circular membranes.
Remark: The vibrational frequencies represent the tones and overtones one hears when
the drum membrane vibrates. An interesting question is whether two drums of diﬀerent
shapes can have identical sounds — the exact same vibrational frequencies.
Or, more
descriptively, can one “hear” the shape of a drum? It was not until 1992 that the answer
was shown to be no, but for quite subtle reasons.
See [47] for a discussion and some
examples of diﬀerently shaped drums that have the same vibrational frequencies.
11.6 The Planar Wave Equation

488
11 Dynamics of Planar Media
Vibration of a Rectangular Drum
Let us ﬁrst consider the vibrations of a membrane in the shape of a rectangle
R = {0 < x < a, 0 < y < b},
with side lengths a and b, whose edges are ﬁxed to the (x, y)–plane. Thus, we seek to solve
the wave equation
utt = c2Δu = c2(uxx + uyy),
0 < x < a,
0 < y < b,
(11.145)
subject to the initial and boundary conditions
u(t, 0, y) = u(t, a, y) = 0 = u(t, x, 0) = u(t, x, b),
u(0, x, y) = f(x, y),
ut(0, x, y) = g(x, y),
0 < x < a,
0 < y < b.
(11.146)
As we saw in Section 11.2, the eigenfunctions and eigenvalues for the associated Helmholtz
equation on a rectangle,
c2(vxx + vyy) + λ v = 0,
(x, y) ∈R,
(11.147)
when subject to the homogeneous Dirichlet boundary conditions
v(0, y) = v(a, y) = 0 = v(x, 0) = v(x, b),
0 < x < a,
0 < y < b,
(11.148)
are
vm,n(x, y) = sin mπx
a
sin nπy
b
,
where
λm,n = π2 c2
 m2
a2 + n2
b2

,
(11.149)
with m, n = 1, 2, . . . . The fundamental frequencies of vibration are the square roots of the
eigenvalues, so
ωm,n =
/
λm,n = π c

m2
a2 + n2
b2 ,
m, n = 1, 2, . . . .
(11.150)
The frequencies will depend upon the underlying geometry — meaning the side lengths —
of the rectangle, as well as the wave speed c, which, in turn, is a function of the membrane’s
density and stiﬀness. The higher the wave speed, or the smaller the rectangle, the faster
the vibrations. In layman’s terms, (11.150) quantiﬁes the observation that smaller, stiﬀer
drums made of less-dense material vibrate faster.
According to (11.142), the normal modes of vibration of our rectangle are
um,n(t, x, y) = cos

π c

m2
a2 + n2
b2 t

sin mπx
a
sin nπy
b
,
um,n(t, x, y) = sin

π c

m2
a2 + n2
b2 t

sin mπx
a
sin nπy
b
.
(11.151)
The general solution can then be written as a double Fourier series
u(t, x, y) =
∞

m,n=1

am,n um,n(t, x, y) + bm,n um,n(t, x, y)


11.6 The Planar Wave Equation
489
Figure 11.10.
Vibrations of a square.
in the normal modes.
The coeﬃcients am,n, bm,n are ﬁxed by the initial displacement
u(0, x, y) = f(x, y) and the initial velocity ut(0, x, y) = g(x, y). Indeed, the usual orthogo-
nality relations among the eigenfunctions imply
am,n =
⟨vm,n , f ⟩
∥vm,n ∥2 = 4
a b
 b
0
 a
0
f(x, y) sin mπx
a
sin nπy
b
dx dy,
(11.152)
bm,n =
⟨vm,n , g ⟩
ωm,n ∥vm,n ∥2 =
4
π c
√
m2 b2 + n2 a2
 b
0
 a
0
g(x, y) sin mπx
a
sin nπy
b
dx dy.
Since the fundamental frequencies are not rational multiples of each other, the general
solution is a genuinely quasiperiodic superposition of the various normal modes.
In Figure 11.10, we plot the solution resulting from the initially concentrated displace-
ment†
u(0, x, y) = f(x, y) = e−100[(x−.5)2+(y−.5)2 ]
at the center of a unit square, so a = b = 1, with unit wave speed, c = 1. Note that, unlike
a concentrated displacement of a one-dimensional string, which remains concentrated at
all subsequent times and periodically repeats, the initial displacement here spreads out in
a radially symmetric manner and propagates to the edges of the rectangle, where it reﬂects
†
The alert reader may object that the initial displacement f(x, y) does not exactly satisfy
the Dirichlet boundary conditions on the edges of the rectangle. But this does not prevent the
existence of a well-deﬁned (weak) solution to the initial value problem, whose initial boundary
discontinuities will subsequently propagate into the square. However, here these are so tiny as to
be unnoticeable in the solution graphs.

490
11 Dynamics of Planar Media
and then interacts with itself. Moreover, due to the quasiperiodicity of the solution, the
drum’s motion never exactly repeats, and the initially concentrated displacement never
quite reforms.
Vibration of a Circular Drum
Let us next analyze the vibrations of a circular membrane of unit radius. In polar coordi-
nates, the planar wave equation (11.137) takes the form
∂2u
∂t2 = c2
 ∂2u
∂r2 + 1
r
∂u
∂r + 1
r2
∂2u
∂θ2

.
(11.153)
We will again consider the homogeneous Dirichlet boundary value problem
u(t, 1, θ) = 0,
t ≥0,
−π ≤θ ≤π,
(11.154)
along with initial conditions
u(0, r, θ) = f(r, θ),
∂u
∂t (0, r, θ) = g(r, θ),
(11.155)
representing the initial displacement and velocity of the membrane. As always, we build up
the general solution as a quasiperiodic linear combination of the normal modes as speciﬁed
by the eigenfunctions for the associated Helmholtz boundary value problem.
As we saw in Section 11.2, the eigenfunctions of the Helmholtz equation on a disk
of radius 1, say, subject to homogeneous Dirichlet boundary conditions, are products of
trigonometric and Bessel functions:
v0,n(r, θ) = J0(ζ0,n r),
vm,n(r, θ) = Jm(ζm,nr) cos mθ,
vm,n(r, θ) = Jm(ζm,nr) sin mθ,
m, n = 1, 2, 3, . . . .
(11.156)
Here r, θ are the usual polar coordinates, while ζm,n > 0 denotes the nth (positive) root
of the mth order Bessel function Jm(z), cf. (11.118). The corresponding eigenvalue is its
square, λm,n = ζ2
m,n, and hence the natural frequencies of vibration are equal to the Bessel
roots scaled by the wave speed:
ωm,n = c

λm,n = c ζm,n.
(11.157)
A table of their values (for the case c = 1) can be found in the preceding section. The
Bessel roots do not follow any easily discernible pattern, and are not rational multiples
of each other. This result, known as Bourget’s hypothesis, [119; p. 484], was rigorously
proved by the German pure mathematician Carl Ludwig Siegel in 1929, [106]. Thus, the
vibrations of a circular drum are also truly quasiperiodic, thereby providing a mathematical
explanation of why drums sound dissonant.
The frequencies ω0,n = c ζ0,n correspond to simple eigenvalues, with a single radially
symmetric eigenfunction J0(ζ0,nr), while the “angular modes” ωm,n, for m > 0, are double,
each possessing two linearly independent eigenfunctions (11.156). According to the general

491
Figure 11.11.
Vibrations of a disk.

11.6 The Planar Wave Equation

492
11 Dynamics of Planar Media
formula (11.142), each eigenfunction engenders two independent normal modes of vibration,
having the explicit forms
cos(c ζ0,nt) J0(ζ0,n r),
sin(c ζ0,nt) J0(ζ0,nr),
cos(c ζm,nt) Jm(ζm,n r) cos mθ,
sin(c ζm,nt) Jm(ζm,n r) cos mθ,
cos(c ζm,nt) Jm(ζm,n r) sin mθ,
sin(c ζm,nt) Jm(ζm,n r) sin mθ.
(11.158)
The general solution to (11.153–154) is then expressed as a Fourier–Bessel series:
u(t, r, θ) = 1
2
∞

n=1

a0,n cos(c ζ0,n t) + c0,n sin(c ζ0,n t)

J0(ζ0,n r)
+
∞

m,n=1
 
am,n cos(c ζm,n t) + cm,n sin(c ζm,n t)

cos mθ
+

bm,n cos(c ζm,n t) + dm,n sin(c ζm,n t)

sin mθ

Jm(ζm,n r),
(11.159)
whose coeﬃcients am,n, bm,n, cm,n, dm,n are determined, as usual, by the initial displace-
ment and velocity of the membrane (11.155). In Figure 11.11, the vibrations due to an
initially oﬀ-center concentrated displacement are displayed; the wave speed is c = 1, and the
time interval between successive plots is Δt = .3. Again, the motion is only quasiperiodic
and, no matter how long you wait, never quite returns to its original conﬁguration.
Exercises
11.6.1. Use your physical intuition to decide whether the following statements are true or false.
Then justify your answer.
(a) Increasing the stiﬀness of a membrane increases the wave speed.
(b) Increasing the density of a membrane increases the wave speed.
(c) Increasing the size of a membrane increases the wave speed
11.6.2. Two uniform membranes have the same shape, but are made out of diﬀerent materials.
Assuming that they are both subject to the same homogeneous boundary conditions, how
are their vibrational frequencies related?
11.6.3. List the numerical values of the six lowest vibrational frequencies of a unit square with
wave speed c = 1 when subject to homogeneous Dirichlet boundary conditions. How many
linearly independent normal modes are associated with each of these frequencies?
♥11.6.4. The rectangular membrane R = {−1 < x < 1, 0 < y < 1} has its two short sides at-
tached to the (x, y)–plane, while its long sides are left free. The membrane is initially dis-
placed so that its right half is one unit above, while its left half is one unit below the plane,
and then released with zero initial velocity. (This discontinuous initial data serves to model
a very sharp transition region.) Assume that the physical units are chosen so the wave
speed c = 1. (a) Write down an initial-boundary value problem that governs the vibrations
of the membrane. (b) What are the fundamental frequencies of vibration of the membrane?
(c) Find the eigenfunction series solution that describes the subsequent motion of the mem-
brane. (d) Is the motion (i) periodic? (ii) quasiperiodic? (iii) unstable? (iv) chaotic?
Explain your answer.
11.6.5. Determine the solution to the following initial-boundary value problems for the wave
equation on the rectangle R = {0 < x < 2, 0 < y < 1}:
(a)
 utt = uxx + uyy,
u(t, x, 0) = u(t, x, 1) = u(t, 0, y) = u(t, 2, y) = 0,
u(0, x, y) = sin πy,
ut(0, x, y) = sin πy;

11.6 The Planar Wave Equation
493
(b)
⎧
⎪
⎨
⎪
⎩
utt = uxx + uyy,
u(t, x, 0) = u(t, x, 1) = ∂u
∂x (t, 0, y) = ∂u
∂x (t, 2, y) = 0,
u(0, x, y) = sin πy,
ut(0, x, y) = sin πy;
(c)
⎧
⎪
⎪
⎨
⎪
⎪
⎩
utt = uxx + uyy,
u(t, x, 0) = u(t, x, 1) = u(t, 0, y) = u(t, 2, y) = 0,
u(0, x, y) =
 1,
0 < x < 1,
0,
1 < x < 2,
ut(0, x, y) = 0;
(d)
⎧
⎪
⎪
⎨
⎪
⎪
⎩
utt = 2uxx + 2uyy,
u(t, x, 0) = u(t, x, 1) = u(t, 0, y) = u(t, 2, y) = 0,
u(0, x, y) = 0,
ut(0, x, y) =
 1,
0 < x < 1,
0,
1 < x < 2.
11.6.6. True or false: The more sides of a rectangle that are tied down, the faster it vibrates.
11.6.7. Answer Exercise 11.6.3 when (a) two adjacent sides of the square are tied down and
the other two are left free; (b) two opposite sides of the square are tied down and the other
two are left free; (c) the membrane is freely ﬂoating in outer space.
11.6.8. A square drum has two sides ﬁxed to a support and two sides left free. Does the drum
vibrate faster if the ﬁxed and free sides are adjacent to each other or on opposite sides?
11.6.9. Write down a periodic solution to the wave equation on a unit square, subject to ho-
mogeneous Dirichlet boundary conditions, that is not a normal mode. Does it vibrate at a
fundamental frequency?
11.6.10. A rectangular drum with side lengths 1 cm by 2 cm and unit wave speed c = 1 has its
boundary ﬁxed to the (x, y)–plane while subject to a periodic external forcing of the form
F(t, x, y) = cos(ω t) h(x, y). (a) At which frequencies ω will the forcing incite resonance
in the drum?
(b) If ω is a resonant frequency, write down the condition(s) on h(x, y) that
ensure excitation of a resonant mode.
11.6.11. The right half of a rectangle of side lengths 1 by 2 is initially displaced, while the left half is
quiescent. True or false: The ensuing vibrations are restricted to the right half of the membrane.
♥11.6.12. A torus (inner tube) can be obtained by gluing together each of the two pairs of op-
posite sides of a rubber rectangle. The (small) vibrations of the torus are described by the
following periodic initial-boundary value problem for the wave equation, in which x, y repre-
sent angular variables:
utt = c2Δu = c2(uxx + uyy),
u(0, x, y) = f(x, y),
ut(0, x, y) = g(x, y),
u(t, −π, y) = u(t, π, y),
ux(t, −π, y) = ux(t, π, y),
−π < x < π,
u(t, x, −π) = u(t, x, π),
ux(t, x, −π) = ux(t, x, π),
−π < y < π.
(a) Find the fundamental frequencies and normal modes of vibration. (b) Write down a
series for the solution. (c) Discuss the stability of a vibrating torus. Is the motion
(i) periodic; (ii) quasiperiodic; (iii) chaotic; (iv) none of these?
11.6.13. The forced wave equation utt = c2Δu + F(x, y) on a bounded domain Ω ⊂R2
models a membrane subject to a constant external forcing function F(x, y). Write down
an eigenfunction series solution to the forced wave equation when the membrane is subject
to homogeneous Dirichlet boundary conditions and initial conditions u(0, x, y) = f(x, y),
ut(0, x, y) = g(x, y). Hint: Expand the forcing function in an eigenfunction series.
11.6.14. A circular drum of radius ζ0,1 ≈2.4048 has initial displacement and velocity
u(0, x, y) = 0,
∂u
∂t (0, x, y) = 2 J0

x2 + y2

.
Assuming that the circular edge of the drum is ﬁxed to the (x, y)–plane, describe, both
qualitatively and quantitatively, its subsequent motion.
11.6.15. Write out the integral formulae for the coeﬃcients in the Fourier–Bessel series solution
(11.159) to the wave equation in a circular disk in terms of the initial data
u(0, r, θ) = f(r, θ), ut(0, r, θ) = g(r, θ).
11.6.16. A circular drum at rest is struck with a concentrated blow at its center. Write down

494
11 Dynamics of Planar Media
an eigenfunction series describing the resulting vibration.
♥11.6.17.(a) Set up and solve the initial-boundary value problem for the vibrations of a uniform
circular drum of unit radius that is freely ﬂoating in space. (b) Discuss the stability of the
drum’s motion. (c) Are the vibrations slower or faster than when its edges are ﬁxed to a
plane?
11.6.18. A ﬂat quarter-disk of radius 1 has its circular edge and one of its straight edges at-
tached to the (x, y)–plane, while the other straight edge is left free. At time t = 0 the disk
is struck with a hammer (unit delta function) at its midpoint, i.e., at radius 1
2 and halfway
between the straight edges. (a) Write down an initial-boundary value problem for the sub-
sequent vibrations of the quarter-disk. Hint: Be careful with the form of the delta function
in polar coordinates; see Exercise 6.3.6. (b) Assuming that the physical units are chosen so
that the wave speed c = 1, determine the quarter-disk’s vibrational frequencies. (c) Write
down an eigenfunction series solution for the subsequent motion. (d) Is the motion unsta-
ble? periodic? If so, what is the period?
11.6.19. True or false: Assuming homogeneous Dirichlet boundary conditions, the fundamen-
tal frequencies of a vibrating half-disk are exactly twice those of the full disk of the same
radius.
♥11.6.20. The edge of a circular drum is moved periodically up and down, so u(t, 1, θ) = cos ω t.
Assuming that the drum is initially at rest, discuss its response.
♣11.6.21. A drum is in the shape of a circular annulus with outer radius 1 meter and inner ra-
dius .5 meter. Find numerical values for its ﬁrst three fundamental vibrational frequencies.
♥11.6.22. A homogeneous rope of length 1 and weight 1 is suspended from the ceiling. Taking x
as the vertical coordinate, with x = 1 representing the ﬁxed end and x = 0 the free end, the
planar displacement u(t, x) of the rope satisﬁes the initial-boundary value problem
∂2u
∂t2 = ∂
∂x

x ∂u
∂x

,
| u(t, 0) | < ∞,
u(t, 1) = 0,
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
t > 0,
0 < x < 1.
(a) Find the solution. Hint: Let y = √x .
(b) Are the vibrations periodic or quasiperi-
odic? (c) Describe the behavior of the rope when subject to uniform periodic external forc-
ing
F(t, x) = a cos ω t.
Scaling and Symmetry
Symmetry methods can also be eﬀectively employed in the analysis of the wave equation.
Let us consider the simultaneous rescaling
t −→α t,
x −→β x,
y −→β y,
(11.160)
of time and space, whose eﬀect is to change the function u(t, x, y) into a rescaled version
U(t, x, y) = u(α t, β x, β y).
(11.161)
The chain rule is employed to relate their derivatives:
∂2U
∂t2 = α2 ∂2u
∂t2 ,
∂2U
∂x2 = β2 ∂2u
∂x2 ,
∂2U
∂y2 = β2 ∂2u
∂y2 .
Therefore, if u satisﬁes the wave equation
utt = c2 Δu,

11.6 The Planar Wave Equation
495
then U satisﬁes the rescaled wave equation
Utt = α2 c2
β2
ΔU = C2 ΔU,
where the rescaled wave speed is
C = α c
β .
(11.162)
In particular, rescaling only time by setting α = 1/c, β = 1, results in a unit wave speed
C = 1. In other words, we are free to choose our unit of time measurement so as to ﬁx the
wave speed equal to 1.
If we set α = β, scaling time and space in the same proportion, then the wave speed
does not change, C = c, and so
t −→β t,
x −→β x,
y −→β y,
(11.163)
deﬁnes a symmetry transformation for the wave equation: If u(t, x, y) is any solution to
the wave equation, then so is its rescaled version
U(t, x, y) = u(β t, β x, β y)
(11.164)
for any choice of scale parameter β ̸= 0. Observe that if u(t, x, y) is deﬁned on a domain
Ω, then the rescaled solution U(t, x, y) will be deﬁned on the rescaled domain
Ω = 1
β Ω =
  x
β , y
β
  (x, y) ∈Ω

= { (x, y) | (β x, β y) ∈Ω } .
(11.165)
For instance, setting the scaling parameter β = 2 halves the size of the domain.
The
normal modes for the rescaled domain have the form
Un(t, x, y) = un(β t, β x, β y) = cos(β ωn t) vn(β x, β y),
U n(t, x, y) = un(β t, β x, β y) = sin(β ωn t) vn(β x, β y),
and hence the rescaled vibrational frequencies are Ωn = β ωn. Thus, when β < 1, the
rescaled membrane is larger by a factor 1/β, and its vibrations are slowed down by the
reciprocal factor β. For instance, a drum that is twice as large will vibrate twice as slowly,
and hence have an octave lower overall tone. Musically, this means that all drums of a
similar shape have the same pattern of overtones, diﬀering only in their overall pitch, which
is a function of their size, tautness, and density.
In particular, choosing β = 1/R will rescale the unit disk into a disk of radius R. The
fundamental frequencies of the rescaled disk are
Ωm,n = β ωm,n = c
R ζm,n,
(11.166)
where c is the wave speed and ζm,n are the Bessel roots, deﬁned in (11.118). Observe that
the ratios ωm,n/ωm′,n′ between vibrational frequencies remain the same, independent of
the size of the disk R and the wave speed c. We deﬁne the relative vibrational frequencies
ρm,n = ωm,n
ω0,1
= ζm,n
ζ0,1
,
in proportion to
ω0,1 = c ζ0,1
R
≈2.4 c
R ,
(11.167)
which is the drum’s dominant, or lowest, vibrational frequency. The relative frequencies
ρm,n are independent of the size, stiﬀness or composition of the drum membrane. In the
following table, we display a list of all relative vibrational frequencies (11.167) that are < 6.
Once the lowest frequency ω0,1 has been determined — either theoretically, numerically,

496
11 Dynamics of Planar Media
or experimentally — all the higher overtones ωm,n = ρm,n ω0,1 are simply obtained by
rescaling.
Relative Vibrational Frequencies of a Circular Disk
n
9
m
0
1
2
3
4
5
6
7
8
9
. . .
1
1.000
1.593
2.136
2.653
3.155
3.647
4.132
4.610
5.084
5.553
. . .
2
2.295
2.917
3.500
4.059
4.601
5.131
5.651
...
...
...
3
3.598
4.230
4.832
5.412
5.977
...
...
4
4.903
5.540
...
...
...
...
...
...
Exercises
11.6.23. True or false: Two rectangular membranes, made out of the same material and both
subject to Dirichlet boundary conditions, have the same relative vibrational frequencies if
and only if they are have similar shapes.
11.6.24. True or false: (a) The vibrational frequencies of a square with side lengths a = b = 2
are four times as slow as those of a square with side lengths a = b = 1.
(b) The vibrational frequencies of a rectangle with side lengths a = 2, b = 1, are twice as
slow as those of a square with side lengths a = b = 1.
11.6.25. A vibrating rectangle of unknown size has wave speed c = 1 and is subject to homoge-
neous Dirichlet boundary conditions. How many of its lowest vibrational frequencies do you
need to know in order to determine the size of the rectangle?
11.6.26. Answer Exercise 11.6.25 when the rectangle is subject to homogeneous Neumann bound-
ary conditions.
♣11.6.27. A circular drum has the A above middle C, which has a frequency of 440 Hertz, as its
lowest tone. What notes are the ﬁrst ﬁve overtones nearest? Try playing these on a piano
or guitar. Or, if you have a synthesizer, try assembling notes of these frequencies to see how
closely it reproduces the dissonant sound of a drum.
11.6.28. In an orchestra, a circular snare drum of radius 1 foot sits near a second circular drum
made out of the same material. Vibrations of the ﬁrst drum are observed to excite an unde-
sired resonant vibration in its partner. What are the possible radii of the second drum?
11.6.29. True or false: The relative vibrational frequencies of a half-disk, subject to Dirichlet
boundary conditions, are a subset of the relative vibrational frequencies of a full disk.
11.6.30. True or false: If u(t, x, y) = cos(ω t) v(x, y) is a normal mode of vibration for a unit
square subject to homogeneous Dirichlet boundary conditions, then the function u(t, x, y) =
cos(ω t) v
 1
2 x, 1
3 y

is a normal mode of vibration for a 2 × 3 rectangle that is subject to the
same boundary conditions, but with a possibly diﬀerent wave speed. If true, how are the
wave speeds of the two rectangles related?
11.6.31. Prove that if u(t, x, y) is a solution to the two-dimensional wave equation, so is the
translated function U(t, x, y) = u(t −t0, x −x0, y −y0), for any constants t0, x0, y0.

11.6 The Planar Wave Equation
497
♦11.6.32.(a) Prove that if u(t, x, y) solves the wave equation, so does U(t, x, y) = u(−t, x, y).
Thus, unlike the heat equation, the wave equation is time-reversible, and its solutions can
be unambiguously followed backwards in time. (b) Suppose u(t, x, y) solves the initial value
problem (11.141). Write down the initial value problem satisﬁed by U(t, x, y).
11.6.33.(a) Prove that, on R2, the solution to the pure displacement initial value problem
utt = c2Δu, u(0, x, y) = f(x, y), ut(0, x, y) = 0, is an even function of t.
(b) Prove that the solution to the pure velocity initial value problem utt = c2Δu,
u(0, x, y) = 0, ut(0, x, y) = g(x, y), is an odd function of t. Hint: Use Exercise 11.6.32
and uniqueness of solutions to the initial value problem.
11.6.34. Suppose v(t, x) is any solution to the one-dimensional wave equation vtt = vxx. Prove
that u(t, x, y) = v(t, ax + by), for any constants (a, b) ̸= (0, 0), solves the two-dimensional
wave equation utt = c2(uxx + uyy) for some choice of wave speed. Describe the behavior of
such solutions.
11.6.35. A traveling-wave solution to the two-dimensional wave equation has the form
u(t, x, y) = v(x −at, y −at), where a is a constant. Find the partial diﬀerential equation
satisﬁed by the function v(ξ, η). Is the equation hyperbolic?
11.6.36. Is the counterpart of Lemma 11.11 valid for the wave equation? In other words, if
v(t, x) and w(t, x) are any two solutions to the one-dimensional wave equation, is their prod-
uct u(t, x, y) = v(t, x) w(t, y) a solution to the two-dimensional wave equation?
11.6.37.(a) How would you solve an initial-boundary value problem for the wave equation on a
rectangle that is not aligned with the coordinate axes?
(b) Apply your method to set up
and solve an initial-boundary value problem on the square R = {| x + y | < 1, | x −y | < 1}.
Chladni Figures and Nodal Curves
When a membrane vibrates, its individual atoms typically move up and down in a quasiperi-
odic manner. As such, there is little correlation between their motions at diﬀerent locations.
However, if the membrane is set to vibrate in a pure eigenmode, say
un(t, x, y) = cos(ωn t) vn(x, y),
(11.168)
then all points move up and down at a common frequency ωn =

λn , which is the square
root of the eigenvalue corresponding to the eigenfunction vn(x, y). The exceptions are the
points where the eigenfunction vanishes:
vn(x, y) = 0,
(11.169)
which remain stationary. The set of all points (x, y) ∈Ω that satisfy (11.169) is known as
the nth Chladni ﬁgure of the domain Ω, named in honor of the eighteenth-century German
physicist and musician Ernst Chladni who ﬁrst observed them experimentally by exciting a
metal plate with his violin bow, [43]. The mathematical models governing such vibrating
plates were formulated by the French mathematician Sophie Germain in the early 1800s.
It can be shown that, in general, each Chladni ﬁgure consists of a ﬁnite system of nodal
curves, [34, 43], that partition the membrane into disjoint nodal regions. As the membrane
vibrates, the nodal curves remain stationary, while each nodal region is entirely either
above or below the equilibrium plane, except, momentarily, when the entire membrane
has zero displacement. As Chladni discovered in his original experiments, scattering small

498
11 Dynamics of Planar Media
1.000
1.593
2.136
2.295
2.653
2.917
3.155
3.500
3.598
Figure 11.12.
Nodal curves and relative vibrational
frequencies of a circular membrane.
particles (e.g., ﬁne sand) over a membrane or plate vibrating in an eigenmode will enable
us to visualize the Chladni ﬁgure, because the particles will tend to accumulate along the
stationary nodal curves. Adjacent nodal regions, lying on the opposite sides of a nodal
curve, move in opposing directions — when one is up, its neighbors are down, and then
they switch roles as the membrane becomes momentarily ﬂat. Let us look at a couple of
examples where the Chladni ﬁgures can be readily determined.

11.6 The Planar Wave Equation
499
Example 11.15.
Circular Drums. Since the eigenfunctions (11.156) for a disk are
products of trigonometric functions in the angular variable and Bessel functions of the
radius, the nodal curves for the normal modes of vibrations of a circular membrane are
rays emanating from and circles centered at the origin. Consequently, the nodal regions
are annular sectors. Chladni ﬁgures associated with the ﬁrst nine normal modes, indexed
by their relative frequencies, are plotted in Figure 11.12. Representative displacements of
the membrane in each of the ﬁrst twelve modes can be found earlier, in Figure 11.6. The
dominant (lowest frequency) mode is the only one that has no nodal curves; it has the
form of a radially symmetric bump where the entire membrane ﬂexes up and down. The
next lowest modes vibrate proportionally faster at a relative frequency ρ1,1 ≈1.593. The
most general solution with this vibrational frequency is a linear combination of the two
eigensolutions: αu1,1 + β u1,1. Each such combination has a single diameter as a nodal
curve, whose angle with the horizontal depends on the ratio β/α. The two semicircular
halves of the drum vibrate in opposing directions — when the top half is up, the bottom
half is down and vice versa. The next set of modes have two perpendicular diameters as
nodal curves; the four quadrants of the drum vibrate in tandem, with opposite quadrants
moving in the same direction. Next in increasing order of vibrational frequency is a single
mode, which has a circular nodal curve whose (relative) radius equals the ratio of the
ﬁrst two roots of the order zero Bessel function, ζ0,2/ζ0,1 ≈.43565; see Exercise 11.6.39
for a justiﬁcation. In this case, the inner disk and the outer annulus vibrate in opposing
directions. And so on . . . .
Example 11.16. Rectangular Drums. For most rectangular drums, the Chladni ﬁg-
ures are relatively uninteresting. Since the normal modes (11.151) are separable products
of trigonometric functions in the coordinate variables x, y, the nodal curves are equally
spaced straight lines parallel to the sides of the rectangle.
The internodal regions are
smaller rectangles, of identical size and shape, with adjacent rectangles vibrating in oppo-
site directions.
More interesting ﬁgures appear when the rectangle admits multiple eigenvalues — so-
called accidental degeneracies. Note that two of the eigenvalues (11.149) coincide, λm,n =
λk,l, if and only if
m2
a2 + n2
b2 = k2
a2 + l2
b2 ,
(11.170)
where (m, n) ̸= (k, l) are distinct pairs of positive integers. In such situations, the two
eigenmodes happen to vibrate with a common frequency ω = ωm,n = ωk,l. Consequently,
any linear combination of the eigenmodes, e.g.,
cos(ω t)

α sin mπx
a
sin nπy
b
+ β sin kπx
a
sin lπy
b

,
α, β ∈R,
is also a pure vibration, and hence qualiﬁes as a normal mode. The associated nodal curves,
α sin mπx
a
sin nπy
b
+ β sin kπx
a
sin lπy
b
= 0,
0 ≤x ≤a,
0 ≤y ≤b,
(11.171)
have a more intriguing geometry, which can change dramatically as the coeﬃcients α, β
vary.
For example, on the unit square R =
-
0 < x, y < 1
.
, an accidental degeneracy occurs
whenever
m2 + n2 = k2 + l2
(11.172)

500
11 Dynamics of Planar Media
Figure 11.13.
Some Chladni ﬁgures for a square membrane.
for distinct pairs of positive integers (m, n) ̸= (k, l). The simplest possibility arises when-
ever m ̸= n, in which case we can merely reverse the order, setting k = n, l = m. In
Figure 11.13 we plot three sample nodal curves
α sin 4πx sin πy + sin πx sin 4πy = 0,
corresponding to three diﬀerent linear combinations of the eigenfunctions with m = l = 4,
n = k = 1. The associated vibrational frequency is, in all cases, ω4,1 = c
√
17 π, where c is
the wave speed.
Classifying accidental degeneracies of rectangles takes us into the realm of number
theory, [9, 29]. In the case of a square, equation (11.172) is asking us to locate all integer
points (m, n) ∈Z2 that lie on a common circle.
Remark: Bourget’s hypothesis, mentioned after (11.157), implies that ζm,n ̸= ζk,l
whenever (m, n) ̸= (k, l). This implies that a disk has no accidental degeneracies, and
hence all its nodal curves are concentric circles and diameters.
Exercises
♦11.6.38. Suppose that a membrane is vibrating in a normal mode. Prove that the membrane
lies instantaneously completely ﬂat at regular time intervals.
♦11.6.39. For a vibrating disk of unit radius, determine the radius of the circular nodal curve for
the next-to-lowest circular mode.
11.6.40. Order the ﬁve nodal circles displayed in Figure 11.12 according to their size.
11.6.41. Sketch the Chladni ﬁgures in a unit disk corresponding to the following vibrational
frequencies. Determine numerical values for the radii of any circular nodal curves.
(a) ω4,0,
(b) ω4,2,
(c) ω2,4,
(d) ω3,3,
(e) ω1,5.
11.6.42. True or false: Any diameter of a circular disk is a nodal curve for some normal mode.
11.6.43. True or false: The nodal curves on a semicircular disk are all semicircles and rays em-
anating from the center.
α = β = 1
α = 2, β = 1
α = 5, β = 1

11.6 The Planar Wave Equation
501
11.6.44.(a) Find the smallest distinct pair of positive integers (k, l) ̸= (m, n) satisfying (11.172)
that are not obtained by simply reversing the order, i.e., (k, l) ̸= (n, m).
(b) Find the
next-smallest example.
(c) Plot two or three Chladni ﬁgures arising from such degenerate
eigenfunctions.
♥11.6.45. Let R be a rectangle all of whose sides are ﬁxed to the (x, y)–plane. Suppose that all
its nodal curves are straight lines. What can you say about its side lengths a, b?
11.6.46. True or false: The nodal regions of a vibrating rectangle are similarly shaped rectan-
gles.
♦11.6.47. Prove that any point of intersection (x0, y0) of two nodal curves associated with the
same normal mode is a critical point of the associated eigenfunction: ∇v(x0, y0) = 0.
11.6.48. True or false: The nodal curves on a domain do not depend on the choice of boundary
conditions.

Chapter 12
Partial Diﬀerential Equations in Space
At last we have ascended to the ultimate rung of the dimensional ladder (at least for those
of us living in a three-dimensional universe): partial diﬀerential equations in physical space.
As in the one- and two-dimensional settings developed in the preceding chapters, the main
protagonists are the Laplace and Poisson equations, modeling equilibrium conﬁgurations of
solid bodies; the three-dimensional wave equation, governing vibrations of solids, liquids,
and electromagnetic waves; and the three-dimensional heat equation, modeling spatial
diﬀusion processes. To conclude this chapter — and the book — we will also analyze the
particular three-dimensional Schr¨odinger equation that governs the hydrogen atom, and
thereby characterizes atomic orbitals.
Fortunately, almost everything of importance has already appeared in the previous
chapters, and appending a third dimension is, for the most part, simply a matter of ap-
propriately adapting the constructions.
We have already developed the principal solu-
tion techniques: separation of variables, Green’s functions, and fundamental solutions. In
three-dimensional problems, separation of variables is applicable in a variety of coordinate
systems, including the usual rectangular, cylindrical, and spherical coordinates. The ﬁrst
two do not lead to anything fundamentally new, and are therefore relegated to the exer-
cises. Separation in spherical coordinates requires spherical Bessel functions and spherical
harmonics, which play essential roles in a wide variety of physical systems, both classical
and quantum.
The Green’s function for the three-dimensional Poisson equation in space can be iden-
tiﬁed as the classic Newton (Coulomb) 1/r gravitational (electrostatic) potential.
The
fundamental solution for the three-dimensional heat equation can be easily guessed from
its one- and two-dimensional forms. The three-dimensional wave equation, surprisingly,
has an explicit solution formula, named after Kirchhoﬀ, of electrical fame, but originally
due to Poisson. Counterintuitively, the best way to handle the two-dimensional wave equa-
tion is by “descending” from the simpler(!) three-dimensional Kirchhoﬀformula. Descent
reveals a remarkable diﬀerence between waves in planar and spatial media. Huygens’ Prin-
ciple states that three-dimensional waves emanating from a localized initial disturbance
remain localized as they propagate through space. In contrast, initially concentrated two-
dimensional disturbances leave a slowly decaying remnant that never entirely disappears.
The ﬁnal section is concerned with the Schr¨odinger equation for a hydrogen atom,
that is, the quantum-dynamical system governing the spatial motion of a single electron
around a positively charged nucleus. As we will see, the spherical harmonic eigensolutions
account for the observed quantum energy levels of atoms that underly the periodic table
and hence the foundations of molecular chemistry.
DOI 10.1007/978-3-
-
-0_
319 02099
, © Springer International Publishing Switzerland 2014
503
12
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

504
12 Partial Diﬀerential Equations in Space
12.1 The Three–Dimensional Laplace and Poisson Equations
We begin our investigations, as usual, with systems in equilibrium, deferring dynamics
until later. The prototypical equilibrium system is the three-dimensional Laplace equation
Δu = ∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 = 0,
(12.1)
in which x = ( x, y, z )T represents rectangular coordinates on R3. The solutions u(x, y, z)
continue to be known as harmonic functions.
The Laplace equation models unforced
equilibria; Poisson’s equation is the inhomogeneous version
−Δu = f(x, y, z),
(12.2)
whose right-hand side represents some form of external forcing.
The basic boundary value problem for the Laplace and Poisson equations seeks a
solution inside a bounded domain Ω ⊂R3 subject to either Dirichlet boundary conditions,
prescribing the function values on the domain’s boundary:
u = h
on
∂Ω,
(12.3)
or Neumann boundary conditions, prescribing its normal derivative or ﬂux through the
boundary:
∂u
∂n = k
on
∂Ω,
(12.4)
or mixed boundary conditions, in which one imposes Dirichlet conditions on part of the
boundary and Neumann conditions on the remainder. Keep in mind that the boundary of
the solid domain Ω consists of one or more piecewise smooth closed surfaces, which will be
oriented by use of the outward — meaning exterior to the domain — unit normal n.
The boundary value problems for the three-dimensional Laplace and Poisson equations
govern a wide variety of physical systems, including:
• Heat conduction: The solution u represents the equilibrium temperature in a solid
body. The inhomogeneity f represents some form of internal heat source or sink.
Dirichlet conditions correspond to ﬁxing the temperature on the bounding sur-
face(s), whereas homogeneous Neumann conditions correspond to an insulated
boundary, i.e., one that does not allow any heat ﬂux.
• Ideal ﬂuid ﬂow: Here the solution u to the Laplace equation represents the velocity po-
tential for an incompressible, irrotational steady-state ﬂuid ﬂow inside a container
governed by the velocity vector ﬁeld v = ∇u. Homogeneous Neumann boundary
conditions correspond to a solid boundary that the ﬂuid cannot penetrate.
• Elasticity: In certain restricted contexts, u represents an equilibrium deformation of
a solid body, e.g., the radial deformation of an elastic ball.
• Electrostatics: In applications to electromagnetism, u is the electric potential in a
conducting medium; its gradient ∇u prescribes the electromotive force on a charged
particle. The inhomogeneity f represents an external electrostatic force ﬁeld.
• Gravitation: The Newtonian gravitational potential in ﬂat empty space is also pre-
scribed by the Laplace equation. (In contrast, Einstein’s theory of general rela-
tivity requires a vastly more complicated nonlinear system of partial diﬀerential
equations, [75].)

12.1 The Three–Dimensional Laplace and Poisson Equations
505
Self–Adjoint Formulation and Minimum Principle
The Laplace and Poisson equations naturally ﬁt into the general self-adjoint equilibrium
framework summarized in Chapter 9. We introduce the L2 inner products
⟨u , u ⟩=
  
Ω
u(x, y, z) u(x, y, z) dx dy dz,
⟨v , v ⟩=
  
Ω
v(x, y, z) · v(x, y, z) dx dy dz,
(12.5)
between, respectively, scalar ﬁelds u, u, and vector ﬁelds v, v, which are deﬁned on the
domain Ω ⊂R3. We assume that the functions in question are suﬃciently nice in order
that these inner products be well deﬁned; if Ω is unbounded, this, in essence, requires that
they decay reasonably rapidly to zero at large distances.
When subject to suitable homogeneous boundary conditions, the three-dimensional
Laplace equation can be placed in our standard self-adjoint form
−Δu = −∇· ∇u = ∇∗◦∇u.
(12.6)
This relies on the fact that the adjoint of the gradient operator with respect to the L2 inner
products (12.5) is minus the divergence operator:
∇∗v = −∇· v.
(12.7)
As usual, the determination of the adjoint rests on an integration by parts formula, which,
in three-dimensional space, is a consequence of the Divergence Theorem from multivariable
calculus, [8, 108]:
Theorem 12.1.
Let Ω ⊂R3 be a bounded domain whose boundary ∂Ω consists
of one or more piecewise smooth simple closed surfaces. Let n denote the unit outward
normal to the boundary of Ω. Let v be a C1 vector ﬁeld deﬁned on Ω and continuous
up to its boundary. Then the surface integral, with respect to surface area, of the normal
component of v over the boundary of the domain equals the triple integral of its divergence
over the domain:
 
∂Ω
v · n dS =
  
Ω
∇· v dx dy dz.
(12.8)
Replacing v by the product uv of a scalar ﬁeld u and a vector ﬁeld v yields
  
Ω
(u ∇· v + ∇u · v) dx dy dz =
  
Ω
∇· (u v) dx dy dz =
 
∂Ω
u (v · n) dS. (12.9)
Rearranging the terms produces the desired integration by parts formula for triple integrals:
  
Ω
(∇u · v) dx dy dz =
 
∂Ω
u (v · n) dS −
  
Ω
u (∇· v) dx dy dz.
(12.10)
The boundary surface integral will vanish, provided either u = 0 or v · n = 0 at each point
on ∂Ω. When u = 0 on all of ∂Ω, we have homogeneous Dirichlet conditions. Setting
v ·n = 0 everywhere on ∂Ω results in the homogeneous Neumann boundary value problem
owing to the identiﬁcation of v = ∇u. Finally, the mixed boundary value problem takes
u = 0 on part of ∂Ω and v · n = 0 on the rest. Thus, subject to one of these choices, the
integration by parts formula (12.10) reduces to
⟨∇u , v ⟩= ⟨u , −∇· v ⟩,
(12.11)
which suﬃces to establish the adjoint formula (12.7).

506
12 Partial Diﬀerential Equations in Space
Remark: Adopting more general weighted inner products results in a more general
elliptic boundary value problem. See Exercise 12.1.9 for details.
According to Theorem 9.20, the self-adjoint formulation (12.6) automatically implies
positive semi-deﬁniteness of the boundary value problem, with positive deﬁniteness if
ker ∇= {0}. Since, on a connected domain, only constant functions are annihilated by
the gradient operator — see Lemma 6.16, which also applies to three-dimensional domains
— both the Dirichlet and mixed boundary value problems are positive deﬁnite, while the
Neumann boundary value problem is only positive semi-deﬁnite.
Finally, in the positive deﬁnite cases, Theorem 9.26 implies that the solution can
be characterized by the three-dimensional version of the Dirichlet minimization principle
(9.82).
Theorem 12.2.
The solution u(x, y, z) to the Poisson equation (12.2) subject to
homogeneous Dirichlet or mixed boundary conditions (12.3) is the unique function that
minimizes the Dirichlet integral
1
2 |∥∇u ∥|2 −⟨u , f ⟩=
  
Ω
 1
2 (u2
x + u2
y + u2
z) −f u

dx dy dz
(12.12)
among all C2 functions that satisfy the prescribed boundary conditions.
As in the two-dimensional version discussed in Chapter 9, the Dirichlet minimization
principle continues to hold in the case of the inhomogeneous Dirichlet boundary value
problem. Modiﬁcations for the inhomogeneous mixed boundary value problem appear in
Exercise 12.1.13.
Exercises
12.1.1. Find bases for the following: (a) the space of harmonic polynomials u(x, y, z) of degree
≤2; (b) the space of homogeneous cubic harmonic polynomials u(x, y, z).
12.1.2. True or false: (a) Every harmonic polynomial is homogeneous.
(b) Every homogeneous polynomial is harmonic.
12.1.3. Solve the Poisson boundary value problem −Δu = 1 on the unit ball x2 + y2 + z2 < 1
with homogeneous Dirichlet boundary conditions. Hint: Look for a polynomial solution.
♦12.1.4. Prove that if u(x, y, z) solves the Laplace equation, then so does the translated function
U(x, y, z) = u(x −a, y −b, z −c) for constants a, b, c.
♦12.1.5.(a) Prove that if u(x, y, z) solves Laplace’s equation, so does the rescaled function
U(x, y, z) = u(λx, λy, λz) for any constant λ. (b) More generally, show that
U(x, y, z) = μu(λx, λy, λz) + c solves Laplace’s equation for any constants λ, μ, c.
♦12.1.6. Let A be a constant nonsingular 3 × 3 matrix, u(x) a C1 scalar ﬁeld, and v(x) a C1
vector ﬁeld. Set U(x) = u(A x) and V(x) = v(A x). Prove that
(a) ∇U(x) = AT ∇u(A x),
(b) ∇· V(x) = w(A x), where w(x) = ∇· (Av)(x).
♦12.1.7. Prove that every rotation and reﬂection is a symmetry of the Laplace equation. In
other words, if Q is any 3 × 3 orthogonal matrix, so QT Q = I , and u(x) is a harmonic
function, then so is U(x) = u(Qx). Hint: Use Exercise 12.1.6.

12.2 Separation of Variables for the Laplace Equation
507
♦12.1.8. The Weak Maximum Principle: Let Ω ⊂R2 be a bounded domain. Let u(x, y, z) solve
the Poisson equation −Δu = f(x, y, z), where f(x, y, z) < 0 for all (x, y, z) ∈Ω.
(a) Prove that the maximum value of u occurs on the boundary ∂Ω.
Hint: Explain why u cannot have a local maximum at any interior point in Ω.
(b) Generalize your result to the case f(x, y, z) ≤0.
Hint: Look at vε(x, y, z) = u(x, y, z) + ε (x2 + y2 + z2) and let ε →0+.
♦12.1.9. Find the equilibrium equations corresponding to minimizing |∥∇u ∥|2 subject to homo-
geneous Dirichlet boundary conditions, where the indicated norm is based on the weighted
inner product
⟨⟨v , w ⟩⟩=
			
Ω v(x, y, z) · w(x, y, z) σ(x, y, z) dx dy dz,
with σ(x, y, z) > 0 a positive scalar function.
♦12.1.10. Prove the following vector calculus identities:
(a) ∇· (u v) = ∇u · v + u ∇· v,
(b) ∇× (u v) = ∇u × v + u ∇× v,
(c) ∇· (v × w) = (∇× v) · w −v · (∇× w),
(d) ∇× (∇× v) = ∇(∇· v) −Δv.
(In the ﬁnal term, the Laplacian Δ acts component-wise on the vector ﬁeld v.)
♦12.1.11. Let Ω be a bounded domain with piecewise smooth boundary ∂Ω. Prove the following
identities:
(a)
	 		
Ω Δu dx dy dz =
		
∂Ω
∂u
∂n dS,
(b)
			
Ω u Δu dx dy dz =
	 	
∂Ω u ∂u
∂n dS −
			
Ω |∥∇u ∥|2 dx dy dz.
12.1.12. Suppose the inhomogeneous Neumann boundary value problem (12.1, 4) has a solu-
tion. (a) Prove that
		
∂Ω k dS = 0. (b) Is the solution unique? If not, what is the most
general solution? (c) State and prove an analogous result for the inhomogeneous Poisson
equation −Δu = f(x, y, z). (d) Provide a physical explanation for your answers.
♦12.1.13. Find a minimization principle that characterizes the solution to the inhomogeneous
mixed boundary value problem −Δu = f on Ω, with u = g on D ⊊∂Ω, and ∂u/∂n = h on
N = ∂Ω \ D.
♥12.1.14.(a) Prove that, subject to suitable boundary conditions, the curl ∇× deﬁnes a self-
adjoint operator with respect to the L2 inner product between vector ﬁelds. What kinds
of boundary conditions do you need to impose for your integration by parts argument to be
valid? Hint: Use the identity in Exercise 12.1.10(c). (b) What operator on vector ﬁelds is
given by the self-adjoint composition S = (∇×)∗◦(∇×)?
(c) Choose a set of homoge-
neous boundary conditions that make S self-adjoint. Is the resulting boundary value prob-
lem S[v] = f positive deﬁnite? If not, what does the Fredholm Alternative say about its
solvability?
12.2 Separation of Variables for the Laplace Equation
In this section, we revisit the method of separation of variables in the context of the three-
dimensional Laplace equation. As always, its applicability is unfortunately restricted to
rather special, but important, geometric conﬁgurations, the simplest being rectangular,
cylindrical, and spherical domains. Since the ﬁrst two are straightforward extensions of
their two-dimensional counterparts, we will discuss only spherically separable solutions in
any detail.
The simplest domain to which the separation of variables method applies is a rectan-

508
12 Partial Diﬀerential Equations in Space
gular box:
B = {0 < x < a, 0 < y < b, 0 < z < c}.
For functions of three variables, one begins the separation process by splitting oﬀone of
them, by setting u(x, y, z) = v(x) w(y, z), say. The function v(x) satisﬁes a simple second-
order ordinary diﬀerential equation, while w(y, z) solves the two-dimensional Helmholtz
equation (11.21), which is further separated by writing w(y, z) = p(y) q(z). The resulting
fully separated solutions u(x, y, z) = v(x) p(y) q(z) are (mostly) products of trigonometric
and hyperbolic functions. Implementation of the technique and analysis of the resulting
series solutions are relegated to Exercise 12.2.34.
In the case that the domain is a cylinder, one passes to cylindrical coordinates r, θ, z,
where
x = r cos θ,
y = r sin θ,
z = z,
(12.13)
to eﬀect the separation. Writing u(r, θ, z) = v(r, θ) w(z), one ﬁnds that w(z) satisﬁes a
simple second-order ordinary diﬀerential equation, while v(r, θ) solves the two-dimensional
polar Helmholtz equation (11.51) on a disk. Applying a further separation to v(r, θ), as
in Chapter 11, produces fully separable solutions u(r, θ, z) = p(r) q(θ) w(z) as products of
Bessel functions of the cylindrical radius r, trigonometric functions of the polar angle θ,
and hyperbolic functions of z; see Exercise 12.2.40.
The most interesting case is that of spherical coordinates, which we proceed to analyze
in detail in the following subsection.
Remark: These are just three of the many coordinate systems in which the three-
dimensional Laplace equation separates. See [78, 79] for 37 additional exotic types, in-
cluding ellipsoidal, toroidal, and parabolic spheroidal coordinates. The resulting separable
solutions are written in terms of new classes of special functions that solve interesting
second-order ordinary diﬀerential equations, all of Sturm–Liouville form (9.71).
Laplace’s Equation in a Ball
Suppose a solid ball (e.g., the Earth) is subject to a speciﬁed steady temperature distri-
bution on its spherical boundary. Our task is to determine the equilibrium temperature
within the ball. We assume that the body is composed of an isotropic, uniform medium
and, to slightly simplify the analysis, choose units in which its radius equals 1.
To ﬁnd the equilibrium temperature within the ball, we must solve the Dirichlet bound-
ary value problem
∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 = 0,
x2 + y2 + z2 < 1,
u(x, y, z) = h(x, y, z),
x2 + y2 + z2 = 1,
(12.14)
where h is prescribed on the bounding unit sphere. Problems in spherical geometries are
most naturally analyzed in spherical coordinates r, ϕ, θ. Our convention is to set
x = r sin ϕ cos θ,
y = r sin ϕ sin θ,
z = r cos ϕ,
(12.15)
where −π < θ ≤π is the azimuthal angle or longitude, while 0 ≤ϕ ≤π is the zenith
angle or latitude on the sphere of radius r =

x2 + y2 + z2 . In other words, ϕ measures

12.2 Separation of Variables for the Laplace Equation
509
x
y
z
r
ϕ
θ
(x, y, z)
(x, y, 0)
Figure 12.1.
Spherical coordinates.
the angle between the vector ( x, y, z )T and the positive z–axis, while θ measures the
angle between its projection ( x, y, 0 )T on the (x, y)–plane and the positive x–axis; see
Figure 12.1. On Earth, longitude θ is measured from the Greenwich prime meridian, while
latitude is measured from the equator, and so equals 1
2 π −ϕ (although the everyday units
are degrees, not radians).
Warning: In many books, particularly those in physics, the roles of θ and ϕ are re-
versed, leading to much confusion when one is perusing the literature.
We prefer the
mathematical convention, since the azimuthal angle θ coincides with the cylindrical angle
coordinate (and the polar coordinate on the (x, y)–plane), thus avoiding unnecessary con-
fusion when going from one coordinate system to the other. You must be attentive to the
convention being used when consulting any reference!
In spherical coordinates, the Laplace equation for u(r, ϕ, θ) takes the form
Δu = ∂2u
∂r2 + 2
r
∂u
∂r + 1
r2
∂2u
∂ϕ2 +
cos ϕ
r2 sin ϕ
∂u
∂ϕ +
1
r2 sin2 ϕ
∂2u
∂θ2
= 0.
(12.16)
This important formula is the ﬁnal result of a fairly nasty chain rule computation, whose
details are left to the motivated reader. (Set aside lots of paper and keep an eraser handy!)
To construct separable solutions to the spherical coordinate form (12.16) of the Laplace
equation, we begin by separating oﬀthe radial part of the solution, setting
u(r, ϕ, θ) = v(r) w(ϕ, θ).
(12.17)
Substituting this ansatz into (12.16), multiplying the resulting equation through by r2
vw ,
and then placing all the terms involving r on one side yields
1
v

r2 d2v
dr2 + 2r dv
dr

= −1
w ΔS[w],
(12.18)
where
ΔS[w] = ∂2w
∂ϕ2 + cos ϕ
sin ϕ
∂w
∂ϕ +
1
sin2 ϕ
∂2w
∂θ2 .
(12.19)

510
12 Partial Diﬀerential Equations in Space
The second-order diﬀerential operator ΔS, which involves only the angular components
of the full Laplacian operator Δ, is of particular signiﬁcance. It is known as the spher-
ical Laplacian, and governs the equilibrium and dynamics of thin spherical shells — see
Example 12.15 below.
Returning to equation (12.18), our usual separation argument applies. The left-hand
side depends only on r, while the right-hand side depends only on the angles ϕ, θ. This can
occur only when both sides are equal to a common separation constant, which we denote by
μ. As a consequence, the radial component v(r) satisﬁes the ordinary diﬀerential equation
r2 v′′ + 2rv′ −μv = 0,
(12.20)
which is of Euler type (11.89), and hence can be readily solved. However, let us put this
equation aside for the time being, and concentrate our eﬀorts on the more complicated
angular components.
The second equation in (12.18) assumes the form
ΔS[w] + μ w = ∂2w
∂ϕ2 + cos ϕ
sin ϕ
∂w
∂ϕ +
1
sin2 ϕ
∂2w
∂θ2 + μ w = 0.
(12.21)
This second-order partial diﬀerential equation can be regarded as the eigenvalue equation
for the spherical Laplacian operator ΔS and is known as the spherical Helmholtz equation.
To ﬁnd explicit solutions, we adopt a further separation of angular variables,
w(ϕ, θ) = p(ϕ) q(θ),
(12.22)
which we substitute into (12.21). Dividing the result by the product w = p q, multiplying
by sin2 ϕ, and then rearranging terms, we are led to the separated system
1
p

sin2 ϕ d2p
dϕ2 + cos ϕ sin ϕ dp
dϕ

+ μ sin2 ϕ = −1
q
d2q
dθ2 = ν,
where, by our usual argument, ν is another separation constant. The spherical Helmholtz
equation thereby splits into a pair of ordinary diﬀerential equations
sin2 ϕ d2p
dϕ2 + cos ϕ sin ϕ dp
dϕ + (μ sin2 ϕ −ν) p = 0,
d2q
dθ2 + ν q = 0.
The equation for q(θ) is easy to solve. As one circumnavigates the sphere, the azimuthal
angle θ increases from −π to π, so q(θ) must be a 2π–periodic function. Thus, q(θ) solves
the well-studied periodic boundary value problem treated, for instance, in (4.109). Up to
a constant multiple, nonzero periodic solutions occur only when the separation constant
assumes one of the values ν = m2, where m = 0, 1, 2, . . . is an integer, with
q(θ) = cos mθ
or
sin mθ,
m = 0, 1, 2, . . ..
(12.23)
Each positive ν = m2 > 0 admits two linearly independent 2π–periodic solutions, while
when ν = 0, only the constant solutions are periodic.
The Legendre Equation and Ferrers Functions
With this information, we endeavor to solve the zenith diﬀerential equation
sin2 ϕ d2p
dϕ2 + cos ϕ sin ϕ dp
dϕ + (μ sin2 ϕ −m2) p = 0.
(12.24)

12.2 Separation of Variables for the Laplace Equation
511
This is not so easy, and constructing analytic formulas for its solutions requires some
ingenuity. The motivation behind the following steps may not be so apparent; indeed,
they are the culmination of a long, detailed study of this important diﬀerential equation
by mathematicians over the last 200 years.
As an initial simpliﬁcation, let us get rid of the trigonometric functions, by invoking
the change of variables
t = cos ϕ,
with
p(ϕ) = P(cos ϕ) = P(t).
(12.25)
Since
0 ≤ϕ ≤π,
we have
0 ≤

1 −t2 = sin ϕ ≤1.
According to the chain rule,
dp
dϕ = dP
dt
dt
dϕ = −sin ϕ dP
dt = −

1 −t2 dP
dt ,
d2p
dϕ2 = −sin ϕ d
dt

−

1 −t2 dP
dt

= (1 −t2) d2P
dt2 −t dP
dt .
Substituting these expressions into (12.24), we conclude that P(t) must satisfy
(1 −t2)2 d2P
dt2 −2t (1 −t2) dP
dt +

μ (1 −t2) −m2 
P = 0.
(12.26)
Unfortunately, the resulting diﬀerential equation is still not elementary, but at least its
coeﬃcients are polynomials. It is known as the Legendre diﬀerential equation of order m,
having ﬁrst been employed by Adrien–Marie Legendre to study the gravitational attraction
of ellipsoidal bodies. In the cases of interest to us, the order parameter m is an integer,
while the separation constant μ plays the role of an eigenvalue.
Power series solutions to the Legendre equation can be constructed by the standard
techniques presented in Section 11.3. The most general solution is a new type of special
function, called a Legendre function, [86]. However, it turns out that the solutions we are
actually interested in can all be written in terms of elementary algebraic functions. First
of all, since t = cos ϕ, the solution only needs to be deﬁned on the interval −1 ≤t ≤1,
the so-called cut locus. The endpoints of the cut locus, t = 1 and t = −1, correspond to
the sphere’s north pole, ϕ = 0, and south pole, ϕ = π, respectively. Both endpoints are
singular points for the Legendre equation, since the coeﬃcient (1−t2)2 of the leading-order
derivative vanishes when t = ±1. In fact, both are regular singular points, as you are asked
to show in Exercise 12.2.11. Since ultimately we need the separable solution (12.17) to be a
well-deﬁned function of x, y, z (even at points where the spherical coordinates degenerate,
i.e., on the z–axis), we need p(ϕ) to be well deﬁned at ϕ = 0 and π, and this requires P(t)
to be bounded at the singular points:
| P(−1) | < ∞,
| P(+1) | < ∞.
(12.27)
Let us begin our analysis with the Legendre equation of order m = 0
(1 −t2) d2P
dt2 −2t dP
dt + μ P = 0.
(12.28)
In this case, the eigenfunctions, i.e., solutions to the Legendre boundary value problem
(12.27–28), are the Legendre polynomials
Pn(t) = (−1)n
2n n!
dn
dtn (1 −t2)n.
(12.29)

512
12 Partial Diﬀerential Equations in Space
P0(t)
P1(t)
P2(t)
P3(t)
P4(t)
P5(t)
Figure 12.2.
Legendre polynomials.
(The initial factor is by common convention, [86]; see (12.64) for the explicit formula.)
The ﬁrst few are
P0(t) = 1,
P1(t) = t,
P2(t) = 3
2 t2 −1
2 ,
P3(t) = 5
2 t3 −3
2 t,
P4(t) = 35
8 t4 −15
4 t2 + 3
8 ,
P5(t) = 63
8 t5 −35
4 t3 + 15
8 t,
and are graphed in Figure 12.2.
Each Legendre polynomial clearly satisﬁes the boundary conditions (12.27). To verify
that they are indeed solutions to the diﬀerential equation (12.28), we set
Qn(t) = (1 −t2)n.
By the chain rule, the derivative of Qn(t) is
Q′
n = −2nt(1 −t2)n−1,
and hence
(1 −t2)Q′
n = −2nt(1 −t2)n = −2ntQn.
Diﬀerentiating the latter formula yields
(1 −t2)Q′′
n −2tQ′
n = −2ntQ′
n −2nQn,
or
(1 −t2)Q′′
n = −2(n −1)tQ′
n −2nQn.
A simple induction proves that the kth order derivative Q(k)
n (t) = dkQn
dtk
satisﬁes
(1 −t2)Q(k+2)
n
= −2(n −k −1)tQ(k+1)
n
−2[n + (n −1) + · · · + (n −k)] Q(k)
n
= −2(n −k −1)tQ(k+1)
n
−(k + 1)(2n −k)Q(k)
n .
(12.30)

12.2 Separation of Variables for the Laplace Equation
513
In particular, when k = n, this reduces to
(1 −t2)Q(n+2)
n
= 2tQ(n+1)
n
−n(n + 1)Q(n)
n
= 0,
and so Pn(t) = Q(n)
n (t) satisﬁes
(1 −t2) P ′′
n −2t P ′
n + n(n + 1) Pn = 0,
which is precisely the order 0 Legendre equation (12.28) with eigenvalue parameter μ =
n(n + 1). The Legendre polynomial Pn is a constant multiple of Pn, and hence it too
satisﬁes the order 0 Legendre equation. According to Theorem 12.3 below, the Legendre
polynomials form a complete system of eigenfunctions for the order 0 Legendre boundary
value problem.
When the order m > 0, the eigenfunctions of the Legendre boundary value problem
(12.26–27) are not always polynomials. They are known as the Ferrers functions, named
after the nineteenth-century British mathematician Norman Ferrers, or, more generally, as
associated Legendre functions. They have the explicit formula†
P m
n (t) = (1 −t2)m/2 dm
dtm Pn(t)
= (−1)n (1 −t2)m/2
2n n!
dn+m
dtn+m (1 −t2)n,
n = m, m + 1, . . . ,
(12.31)
which generalizes the formula (12.29) for the Legendre polynomials. In particular P 0
n (t) =
Pn(t). Here is a list of the ﬁrst few Ferrers functions, which, for completeness, includes
Legendre polynomials:
P 0
0 (t) = 1,
P 0
1 (t) = t,
P 1
1 (t) =

1 −t2 ,
P 0
2 (t) = −1
2 + 3
2 t2,
P 1
2 (t) = 3t

1 −t2 ,
P 2
2 (t) = 3(1 −t2),
P 0
3 (t) = −3
2 t + 5
2 t3,
P 1
3 (t) =

−3
2 + 15
2 t2 
1 −t2,
P 2
3 (t) = 15t (1 −t2),
P 3
3 (t) = 15(1 −t2)3/2 ,
(12.32)
P 0
4 (t) = 3
8 −15
4 t2 + 35
8 t4,
P 1
4 (t) =

−15
2 t + 35
2 t3 
1 −t2,
P 2
4 (t) =

−15
2 + 105
2 t2
(1 −t2),
P 3
4 (t) = 105t (1 −t2)3/2 ,
P 4
4 (t) = 105(1 −t2)2.
When m = 2k ≤n is an even integer, P m
n (t) is a polynomial function, while when m =
2k + 1 ≤n is odd, there is an extra factor of
√
1 −t2 . Keep in mind that the square root
is real and positive, since we are restricting our attention to the interval −1 ≤t ≤1. If
m > n, formula (12.31) reduces to the zero function and so is not included in the ﬁnal
tally.
Warning: Even though half of the Ferrers functions are polynomials, only those with
m = 0, i.e., Pn(t) = P 0
n (t), are called Legendre polynomials.
†
Warning: Some authors include a (−1)m factor in the formula, resulting in the opposite sign
when m is odd. Another source of confusion is that many tables deﬁne the associated Legendre
functions using the alternative initial factor (t2 −1)m/2. But this is unsuitable, since we are solely
interested in values of t lying in the interval −1 ≤t ≤1, and this convention would result in a
complex-valued function when m is odd. Following [86], we use the term “Ferrers function” to
refer to the restriction of the associated Legendre function to the cut locus −1 ≤t ≤1.

514
12 Partial Diﬀerential Equations in Space
0 ≤P 1
1 (t) ≤1
−1.5 ≤P 1
2 (t) ≤1.5
0 ≤P 2
2 (t) ≤3
−1.5 ≤P 1
3 (t) ≤2.07
−5.77 ≤P 2
3 (t) ≤5.77
0 ≤P 3
3 (t) ≤15
−2.64 ≤P 1
4 (t) ≤2.64
−7.5 ≤P 2
4 (t) ≤9.64
−34.1 ≤P 3
4 (t) ≤34.1
0 ≤P 4
4 (t) ≤105
Figure 12.3.
Ferrers functions.
Figure 12.3 displays graphs of the Ferrers functions P m
n (t) for 1 ≤m ≤n ≤4.
Pay particular attention to the fact that, owing to the choice of normalization factor, the
graphs have very diﬀerent vertical scales, as indicated by their minimum and maximum
values (rounded to two decimal places) written below each — although one always has the
freedom to rescale the eigenfunctions as desired, e.g., so as to be orthonormal.
To show that the Ferrers functions P m
n (t) satisfy the Legendre diﬀerential equation
(12.26) of order m, we substitute k = m + n in (12.30):
(1 −t2)
d2Rm
n
dt2
−2(m + 1)t
dRm
n
dt
+ (m + n + 1)(n −m)Rm
n = 0,
(12.33)
where
Rm
n (t) = Q(m+n)
n
(t).
This is not the order m Legendre equation, but it can be converted into it by setting
Rm
n (t) = (1 −t2)−m/2 Sm
n (t).

12.2 Separation of Variables for the Laplace Equation
515
Diﬀerentiating, we obtain
dRm
n
dt
= (1 −t2)−m/2 dSm
n
dt
−mt(1 −t2)−m/2−1 Sm
n ,
d2Rm
n
dt2
= (1 −t2)−m/2 d2Sm
n
dt2
−2mt(1 −t2)−m/2−1 dSm
n
dt
+

m + m(m + 1)t2 
(1 −t2)−m/2−2 Sm
n .
Therefore, after a little algebra, equation (12.33) takes the alternative form
(1 −t2)−m/2+1 d2Sm
n
dt2
−2t(1 −t2)−m/2 dSm
n
dt
+

n(n + 1)(1 −t2) −m2 
(1 −t2)−m/2−1 Sm
n = 0,
which, when multiplied by (1−t2)m/2+1, is precisely the order m Legendre equation (12.26)
with eigenvalue parameter μ = n(n + 1). Thus,
Sm
n (t) = (1 −t2)m/2Rm
n (t) = (1 −t2)m/2 dn+m
dtn+m (1 −t2)n,
which is a constant multiple of the Ferrers function P m
n (t), is a solution to the order m
Legendre equation. Moreover, we note that
P m
n (1) = P m
n (−1) = 0,
when
m > 0,
(12.34)
and we conclude that P m
n (t) is an eigenfunction for the order m Legendre boundary value
problem.
The following result states that the Ferrers functions provide a complete list of solu-
tions to the Legendre boundary value problem (12.26–27).
Theorem 12.3.
Let m ≥0 be a nonnegative integer. Then the order m Legendre
boundary value problem prescribed by (12.26–27) has eigenvalues μn = n(n + 1) for n =
0, 1, 2, . . ., and associated eigenfunctions P m
n (t), where m = 0, . . ., n. Moreover, the Ferrers
eigenfunctions form a complete orthogonal system relative to the L2 inner product on the
cut locus [−1, 1].
Returning to the zenith variable ϕ via (12.25), Theorem 12.3 implies that our original
boundary value problem
sin2 ϕ d2p
dϕ2 + cos ϕ sin ϕ dp
dϕ + (μ sin2 ϕ −m2) p = 0,
| p(0) |,
| p(π) | < ∞,
(12.35)
has its eigenvalues and eigenfunctions expressed in terms of the Ferrers functions:
μn = n(n + 1),
pm
n (ϕ) = P m
n (cos ϕ),
for
0 ≤m ≤n.
(12.36)
Since P m
n (t) is either a polynomial or a polynomial multiplied by a power of
√
1 −t2 ,
the eigenfunction pm
n (ϕ) is a trigonometric polynomial of degree n, which we call a trigono-

516
12 Partial Diﬀerential Equations in Space
p0
0(ϕ) ≡1
−1 ≤p0
1(ϕ) ≤1
0 ≤p1
1(ϕ) ≤1
−.5 ≤p0
2(ϕ) ≤1
−1.5 ≤p1
2(ϕ) ≤1.5
0 ≤p2
2(ϕ) ≤3
−1 ≤p0
3(ϕ) ≤1
−1.5 ≤p1
3(ϕ) ≤2.07
−5.77 ≤p2
3(ϕ) ≤5.77
0 ≤p3
3(ϕ) ≤15
−.43 ≤p0
4(ϕ) ≤1
−2.64 ≤p1
4(ϕ) ≤2.64
−7.5 ≤p2
4(ϕ) ≤9.64
−34.1 ≤p3
4(ϕ) ≤34.1
0 ≤p4
4(ϕ) ≤105
Figure 12.4.
Trigonometric Ferrers functions.
metric Ferrers function. Here are the ﬁrst few, written in Fourier form, as in (3.38):
p0
0(ϕ) = 1,
p0
1(ϕ) = cos ϕ,
p1
1(ϕ) = sin ϕ,
p0
2(ϕ) = 1
4 + 3
4 cos 2ϕ,
p1
2(ϕ) = 3
2 sin 2ϕ,
p2
2(ϕ) = 3
2 −3
2 cos 2ϕ,
p0
3(ϕ) = 3
8 cos ϕ + 5
8 cos 3ϕ,
p1
3(ϕ) = 3
8 sin ϕ + 15
8 sin 3ϕ,
p2
3(ϕ) = 15
4 cos ϕ −15
4 cos 3ϕ,
p3
3(ϕ) = 45
4 sin ϕ −15
4 sin 3ϕ,
p0
4(ϕ) =
9
64 + 5
16 cos 2ϕ + 35
64 cos 4ϕ,
p1
4(ϕ) = 5
8 sin 2ϕ + 35
16 sin 4ϕ,
p2
4(ϕ) = 45
16 + 15
4 cos 2ϕ −105
16 cos 4ϕ,
p3
4(ϕ) = 105
4 sin 2ϕ −105
8 sin 4ϕ,
p4
4(ϕ) = 315
8 −105
2 cos 2ϕ + 105
8 cos 4ϕ.
(12.37)
It is also instructive to plot the eigenfunctions in terms of the zenith angle ϕ; see Figure 12.4.
As in Figure 12.3, the vertical scales are not the same, as indicated by the listed minimum
and maximum values.

12.2 Separation of Variables for the Laplace Equation
517
Spherical Harmonics
At this stage, we have determined both angular components of our separable solutions
(12.22). Multiplying the two parts together results in the spherical angle functions
Y m
n (ϕ, θ) = pm
n (ϕ) cos mθ,
Y m
n (ϕ, θ) = pm
n (ϕ) sin mθ,
n = 0, 1, 2, . . .,
m = 0, 1, . . ., n,
(12.38)
known as spherical harmonics. They satisfy the spherical Helmholtz equation
ΔS Y m
n + n(n + 1) Y m
n
= 0 = ΔS Y m
n + n(n + 1) Y m
n ,
(12.39)
and so are eigenfunctions for the spherical Laplacian operator, (12.19), with associated
eigenvalues μn = n(n + 1) for n = 0, 1, 2, . . . . The nth eigenvalue μn admits a (2n + 1)–
dimensional eigenspace, spanned by the spherical harmonics
Y 0
n (ϕ, θ),
Y 1
n (ϕ, θ),
. . . , Y n
n (ϕ, θ),
Y 1
n (ϕ, θ),
. . . , Y n
n (ϕ, θ).
(The omitted function Y 0
n (ϕ, θ) ≡0 is trivial, and so does not contribute.) In Figure 12.5
we plot the ﬁrst few spherical harmonic surfaces r = Y m
n (ϕ, θ). In these graphs, in view of
the spherical coordinate formulae (12.15), points with a negative r coordinate appear on
the opposite side of the origin from their positive r counterparts. Incidentally, the graphs of
the other spherical harmonic surfaces r = Y m
n (ϕ, θ), when m > 0, are obtained by rotation
around the z–axis by 90◦; see Exercise 12.2.20. On the other hand, the graphs of Y 0
n are
cylindrically symmetric (why?), and hence unaﬀected by such a rotation.
Self-adjointness of the spherical Laplacian, as per Exercise 12.2.21, implies that the
spherical harmonics are orthogonal with respect to the L2 inner product
⟨f , g ⟩=
 
S1
f g dS =
 π
−π
 π
0
f(ϕ, θ) g(ϕ, θ) sin ϕ dϕ dθ
(12.40)
given by integrating the product of the functions with respect to the surface area element
dS = sin ϕ dϕ dθ on the unit sphere S1 = {∥x ∥= 1}. More correctly, self-adjointness only
guarantees orthogonality of the harmonics corresponding to distinct eigenvalues: μn ̸= μl.
However, the orthogonality relations
⟨Y m
n , Y k
l ⟩=
 
S1
Y m
n Y k
l dS = 0,
for
(m, n) ̸= (k, l),
⟨Y m
n , Y k
l ⟩=
 
S1
Y m
n
Y k
l dS = 0,
for all
(m, n), (k, l),
⟨Y m
n , Y k
l ⟩=
 
S1
Y m
n
Y k
l dS = 0,
for
(m, n) ̸= (k, l),
(12.41)
do, in fact, hold in full generality; Exercise 12.2.22 asks you to supply the details. Moreover,
their norms can be explicitly computed:
∥Y 0
n ∥2 =
4π
2n + 1 ,
∥Y m
n ∥2 = ∥Y m
n ∥2 =
2π(n + m)!
(2n + 1)(n −m)! ,
m = 1, . . . , n.
(12.42)
Proofs of the latter formulae are outlined in Exercise 12.2.24.
With some further work, it can be shown that the spherical harmonics form a complete
orthogonal system of functions on the unit sphere. This means that any reasonable (e.g.,

518
12 Partial Diﬀerential Equations in Space
Y 0
0 (ϕ, θ)
Y 0
1 (ϕ, θ)
Y 1
1 (ϕ, θ)
Y 0
2 (ϕ, θ)
Y 1
2 (ϕ, θ)
Y 2
2 (ϕ, θ)
Y 0
3 (ϕ, θ)
Y 1
3 (ϕ, θ)
Y 2
3 (ϕ, θ)
Y 3
3 (ϕ, θ)
Y 0
4 (ϕ, θ)
Y 1
4 (ϕ, θ)
Y 2
4 (ϕ, θ)
Y 3
4 (ϕ, θ)
Y 4
4 (ϕ, θ)
Figure 12.5.
Spherical harmonics.

12.2 Separation of Variables for the Laplace Equation
519
piecewise C1 or even L2) function h: S1 →R, can be expanded into a convergent spherical
harmonic series
h(ϕ, θ) = c0,0
2
+
∞

n=1

c0,n
2
Y 0
n (ϕ) +
n

m=1
:
cm,nY m
n (ϕ, θ) + cm,n Y m
n (ϕ, θ)
; 
.
(12.43)
Applying the orthogonality relations (12.41), we ﬁnd that the spherical harmonic coeﬃ-
cients are given by the inner products
c0,n = 2 ⟨h , Y 0
n ⟩
∥Y 0
n ∥2
,
cm,n = ⟨h , Y m
n ⟩
∥Y m
n ∥2 ,
cm,n = ⟨h , Y m
n ⟩
∥Y m
n ∥2 ,
0 ≤n,
1 ≤m ≤n,
or, explicitly, using (12.40) and the formulae (12.42) for the norms,
cm,n = (2n + 1)(n −m)!
2π (n + m)!
 π
−π
 π
0
h(ϕ, θ) pm
n (ϕ) cos mθ sin ϕ dϕ dθ,
cm,n = (2n + 1)(n −m)!
2π (n + m)!
 π
−π
 π
0
h(ϕ, θ) pm
n (ϕ) sin mθ sin ϕ dϕ dθ.
(12.44)
As with an ordinary Fourier series, the extra 1
2 was appended to the c0,n terms in (12.43)
so that equations (12.44) remain valid for all values of m, n. In particular, the constant
term in the spherical harmonic series is the mean of the function h over the unit sphere:
c0,0
2
= 1
4π
 
S1
h dS = 1
4π
 π
−π
 π
0
h(ϕ, θ) sin ϕ dϕ dθ.
(12.45)
Remark: Establishing uniform convergence of a spherical harmonic series (12.43) is
more challenging than in the Fourier series case, because, unlike the trigonometric func-
tions, the orthonormal spherical harmonics are not uniformly bounded. A recent survey of
what is known in this regard can be found in [10].
Remark: An alternative approach is to replace the real trigonometric functions by
complex exponentials, and work with the complex spherical harmonics†
Ym
n (ϕ, θ) = Y m
n (ϕ, θ) + i Y m
n (ϕ, θ) = pm
n (ϕ) e i mθ,
n = 0, 1, 2, . . . ,
m = −n, −n + 1, . . . , n.
(12.46)
The associated orthogonality and expansion formulas are relegated to the exercises.
Harmonic Polynomials
To complete our solution to the Laplace equation on the solid ball, we still need to solve the
ordinary diﬀerential equation (12.20) for the radial component v(r). In view of our analysis
of the spherical Helmholtz equation, the original separation constant is μ = n(n + 1) for
some nonnegative integer n ≥0, and so the radial equation takes the form
r2 v′′ + 2rv′ −n(n + 1)v = 0.
(12.47)
†
Here we use the convention that Y m
n
= Y −m
n
,
Y m
n
= −Y −m
n
, and
Y 0
n ≡0, which is
compatible with their deﬁning formulas (12.38).

520
12 Partial Diﬀerential Equations in Space
To solve this Euler equation, we substitute the power ansatz v(r) = rα, and ﬁnd that the
exponent α must satisfy the quadratic indicial equation
α2 + α −n(n + 1) = 0,
and hence
α = n
or
α = −(n + 1).
Therefore, the two linearly independent solutions are
v1(r) = rn
and
v2(r) = r−n−1.
(12.48)
Since we are currently interested only in solutions that remain bounded at r = 0 — the
center of the ball — we will retain just the ﬁrst solution v(r) = rn for our subsequent
analysis.
At this stage, we have solved all three ordinary diﬀerential equations for the separa-
ble solutions. We combine (12.23, 38, 48) to produce the following spherically separable
solutions to the Laplace equation:
Hm
n = rn Y m
n (ϕ, θ) = rn pm
n (ϕ) cosmθ,
Hm
n = rn Y m
n (ϕ, θ) = rn pm
n (ϕ) sin mθ,
n = 0, 1, 2, . . .,
m = 0, 1, . . ., n.
(12.49)
Although apparently complicated, these solutions are, perhaps surprisingly, elementary
polynomial functions of the rectangular coordinates x, y, z, and hence are harmonic poly-
nomials. The ﬁrst few are
H0
0 = 1,
H0
1 = z,
H0
2 = z2 −1
2 x2 −1
2 y2,
H0
3 = z3 −3
2 x2z −3
2 y2z,
H1
1 = x,
H1
2 = 3xz,
H1
3 = 6xz2 −3
2 x3 −3
2 xy2,
H1
1 = y,
H1
2 = 3y z,
H1
3 = 6y z2 −3
2 x2y −3
2 y3,
H2
2 = 3x2 −3y2,
H2
3 = 15x2z −15y2z,
H2
2 = 6xy,
H2
3 = 30xy z,
H3
3 = 15x3 −45xy2,
H3
3 = 45x2y −15y3.
(12.50)
The polynomials
H0
n,
H1
n, . . . , Hn
n,
H1
n, . . . , Hn
n
are homogeneous of degree n. Orthogonality of the spherical harmonics implies that they
form a basis for the vector space comprised of all homogeneous harmonic polynomials of
degree n, which hence has dimension 2n + 1.
The harmonic polynomials (12.49) form a complete system, and therefore the gen-
eral solution to the Laplace equation inside the unit ball can be written as a harmonic
polynomial series:
u(x, y, z) = c0,0
2
+
∞

n=1

c0,n
2
H0
n(x, y, z) +
n

m=1

cm,nHm
n (x, y, z) + cm,n Hm
n (x, y, z)


,
(12.51)
or equivalently, in spherical coordinates,
u(r, ϕ, θ) = c0,0
2
+
∞

n=1

c0,n
2
rn Y 0
n (ϕ) +
n

m=1

cm,nrn Y m
n (ϕ, θ) + cm,n rn Y m
n (ϕ, θ)


.
(12.52)

12.2 Separation of Variables for the Laplace Equation
521
The coeﬃcients cm,n, cm,n are uniquely prescribed by the boundary conditions. Indeed,
substituting (12.52) into the Dirichlet boundary conditions on the unit sphere r = 1 yields
u(1, ϕ, θ) =
c0,0
2
+
∞

n=1

c0,n
2
Y 0
n (ϕ) +
n

m=1

cm,nY m
n (ϕ, θ) + cm,n Y m
n (ϕ, θ)


= h(ϕ, θ).
(12.53)
Thus, the coeﬃcients cm,n, cm,n are given by the inner product formulae (12.44). If the
terms in the resulting series are uniformly bounded — which occurs for all piecewise con-
tinuous functions h, as well as all L2 functions and many generalized functions such as the
delta function — then the harmonic polynomial series (12.52) converges everywhere, and,
in fact, uniformly on any smaller ball ∥x ∥= r ≤r0 < 1.
Averaging, the Maximum Principle, and Analyticity
In rectangular coordinates, the nth summand of the series (12.51) is a homogeneous polyno-
mial of degree n. Therefore, repeating the argument used in the two-dimensional situation
(4.115), we conclude that the harmonic polynomial series is, in fact, a power series, and
hence provides the Taylor expansion for the harmonic function u(x, y, z) at the origin! In
particular, its convergence for all r < 1 implies that the harmonic function u(x, y, z) is
analytic at x = y = z = 0.
The constant term in such a Taylor series can be identiﬁed with the value of the
function at the origin: u(0, 0, 0) = 1
2 c0,0. On the other hand, since u = h on S1 = ∂Ω, the
coeﬃcient formula (12.45) tells us that
u(0, 0, 0) = c0,0
2
= 1
4π
 
S1
u dS.
(12.54)
Therefore, we have established the three-dimensional counterpart of Theorem 4.8: the value
of a harmonic function u at the center of the sphere is equal to the average of its values
on the sphere’s surface. Moreover, each partial derivative
∂i+j+ku
∂xi∂yj∂zk (0, 0, 0) appears, up
to a factor, as the coeﬃcient of the terms xiyjzk in the Taylor series, and hence can be
expressed as a certain linear combination of the coeﬃcients cm,n, cm,n, which are in turn
given by the integral formulae (12.44).
So far, we have restricted our attention to a ball of unit radius.
A simple scaling
argument serves to establish the general result.
Theorem 12.4. If u(x) is a harmonic function deﬁned on a domain Ω ⊂R3, then u
is analytic inside Ω. Moreover, its value at any x0 ∈Ω is obtained by averaging its values
on any sphere centered at x0:
u(x0) =
1
4πa2
 
∥x−x0 ∥=a
u dS,
(12.55)
provided the enclosed ball lies within its domain of analyticity: {∥x −x0 ∥≤a} ⊂Ω.
Proof : It is easily checked that, under the hypothesis of the theorem, the rescaled and
translated function
U(y) = u(ay + x0) = u(x),
where
y = x −x0
a
,
(12.56)

522
12 Partial Diﬀerential Equations in Space
is harmonic on the unit ball ∥y ∥≤1, and hence solves the boundary value problem (12.14)
with boundary values h(y) = U(y) = u(a y + x0) on ∥y ∥= 1. By the preceding remarks,
U(y) is analytic at y = 0, and so u(x) = U
"x −x0
a
#
is analytic at x = x0. Since x0
can be any point inside Ω, this establishes the analyticity of u everywhere in Ω. Moreover,
according to (12.54),
u(x0) = U(0) = 1
4π
 
∥y ∥=1
U dS =
1
4πa2
 
∥x−x0 ∥=a
u dS,
since the eﬀect of the change of variables (12.56) is just to rescale the spherical surface
integral.
Q.E.D.
Arguing as in the planar case of Theorem 4.9, we readily establish the corresponding
Strong Maximum Principle for harmonic functions of three variables.
Theorem 12.5. A nonconstant harmonic function cannot have a local maximum or
minimum at any interior point of its domain of deﬁnition. Moreover, its global maximum
or minimum (if any) is located on the boundary of the domain.
For instance, the Maximum Principle implies that the maximum and minimum tem-
peratures in a solid body in thermal equilibrium are to be found only on its boundary. In
physical terms, since heat energy must ﬂow away from an internal maximum and towards
an internal minimum, any local temperature extremum inside the body would preclude it
from being in thermal equilibrium.
Example 12.6. In this example, we shall determine the electrostatic potential inside
a hollow sphere when the upper and lower hemispheres are held at diﬀerent constant
potentials. This device is called a spherical capacitor and is realized experimentally by
separating the two charged conducting hemispherical shells by a thin insulating ring at
the equator. A straightforward scaling argument allows us to choose our units so that the
sphere has unit radius, while the potential is set equal to 1 on the upper hemisphere and
equal to 0, i.e., grounded, on the lower hemisphere. The resulting electrostatic potential
satisﬁes the Laplace equation
Δu = 0
inside a solid ball
∥x ∥< 1,
and is subject to Dirichlet boundary conditions
u(x, y, z) = h(x, y, z) ≡
 1,
z > 0,
0,
z < 0,
on the unit sphere
∥x ∥= 1.
(12.57)
The solution will be prescribed by a harmonic polynomial series (12.51) whose coeﬃ-
cients are ﬁxed by the boundary values (12.57). Before tackling the required computation,
let us ﬁrst note that since the boundary data does not depend upon the azimuthal angle
θ, the solution u = u(r, ϕ) will also be independent of θ. Therefore, we need only consider
the θ-independent spherical harmonic polynomials (12.38), which are those with m = 0.
Thus,
u(x, y, z) = 1
2
∞

n=0
cn H0
n(x, y, z) = 1
2
∞

n=0
cn rn Pn(cos ϕ),
(12.58)

12.2 Separation of Variables for the Laplace Equation
523
where we abbreviate cn = c0,n. The boundary conditions (12.57) require
u|r=1 = 1
2
∞

n=0
cnPn(cos ϕ) = h(ϕ) =

1,
0 ≤ϕ < 1
2 π,
0,
1
2 π < ϕ ≤π.
The coeﬃcients are given by (12.44), which, in the case m = 0, reduce to
cn = 2n + 1
2π
 
S1
h Y 0
n dS = (2n + 1)
 π/2
0
Pn(cos ϕ) sin ϕ dϕ = (2n + 1)
 1
0
Pn(t) dt,
(12.59)
since h = 0 when 1
2 π < ϕ ≤π. The ﬁrst few are
c0 = 1,
c1 = 3
2,
c2 = 0,
c3 = −7
8,
c4 = 0,
. . . .
Therefore, the solution has the explicit Taylor expansion
u(x, y, z) = 1
2 + 3
4 r cos ϕ −21
128 r3 cos ϕ −35
128 r3 cos 3ϕ + · · ·
= 1
2 + 3
4 z + 21
32 (x2 + y2) z −7
16 z3 + · · · .
(12.60)
Note in particular that the value u(0, 0, 0) = 1
2 at the center of the sphere is the average
of its boundary values, in accordance with Theorem 12.4. The solution depends only on
the cylindrical coordinates r, z, which is a consequence of the invariance of the Laplace
equation under general rotations, coupled with the invariance of the boundary data under
rotations around the z–axis.
Remark: The same solution u(x, y, z) describes the thermal equilibrium in a solid
sphere whose upper hemisphere is held at temperature 1◦and lower hemisphere at 0◦.
Example 12.7. A closely related problem is to determine the electrostatic potential
outside a spherical capacitor. As in the preceding example, we take our capacitor of radius
1, with electrostatic charge of 1 on the upper hemisphere and 0 on the lower hemisphere.
Here, we need to solve the Laplace equation Δu = 0 in the unbounded domain Ω =
{∥x ∥> 1} — the exterior of the unit sphere — subject to the same Dirichlet boundary
conditions (12.57).
We anticipate that the potential will be vanishingly small at large
distances away from the capacitor: r = ∥x ∥≫1. Therefore, the harmonic polynomial
solutions (12.49) will not help us solve this problem, since (except for the constant case)
they become unboundedly large far away from the origin.
However, revisiting our original separation of variables argument will produce a dif-
ferent class of solutions having the desired decay properties. When we solved the radial
equation (12.47), we discarded the solution v2(r) = r−n−1 because it had a singularity at
the origin. In the present situation, the behavior of the function at r = 0 is irrelevant; our
requirement is that the solution decay as r →∞, and v2(r) has this property. Therefore,
we will utilize the complementary harmonic functions
Km
n (x, y, z) = r−2n−1 Hm
n (x, y, z) = r−n−1 Y m
n (ϕ, θ) = r−n−1pm
n (ϕ) cosmθ,
Km
n (x, y, z) = r−2n−1 Hm
n (x, y, z) = r−n−1 Y m
n (ϕ, θ) = r−n−1pm
n (ϕ) sin mθ,
(12.61)
for solving such exterior problems. For the capacitor problem, we need only those that are
independent of θ, whereby m = 0. We write the resulting solution as a series
u(x, y, z) = 1
2
∞

n=0
cnK0
n(x, y, z) = 1
2
∞

n=0
cn r−n−1 Pn(cos ϕ).
(12.62)

524
12 Partial Diﬀerential Equations in Space
The boundary conditions
u|r=1 = 1
2
∞

n=0
cnPn(cos ϕ) = h(ϕ) ≡

1,
0 ≤ϕ < 1
2 π,
0,
1
2 π < ϕ ≤π,
are identical to those in the previous example. Therefore, the coeﬃcients are given by
(12.59), leading to the series expansion
u(x, y, z) = 1
2r + 3 cos ϕ
4r2
−21 cosϕ + 35 cos3ϕ
128r4
+ · · ·
(12.63)
=
1
2

x2 + y2 + z2 +
3z
4(x2 + y2 + z2)3/2 + 21(x2 + y2)z −14z3
32(x2 + y2 + z2)7/2
+ · · · .
Observe that the higher-order terms become negligible at large distances, and hence the
potential is asymptotic to that associated with a point charge concentrated at the origin
of magnitude 1
2, which is the average of the boundary potential over the sphere. This is
indicative of a general fact, to be explored in Exercise 12.2.32.
Exercises
12.2.1. A solid ball of radius R has its upper hemispherical surface held at temperature T1 and
its lower hemispherical surface held at temperature T0. Find the resulting equilibrium tem-
perature.
12.2.2. A solid ball has its top hemispherical surface insulated and its bottom hemispherical
surface held at a ﬁxed temperature of 10◦. Find its equilibrium temperature.
12.2.3. Find the potential inside a spherical capacitor of radius R when the upper hemisphere
is at potential α and the lower is at β.
12.2.4. Find the potential u(x, y, z) inside a unit spherical capacitor that has the indicated
boundary values on the unit sphere x2+y2+z2 = 1:
(a) x, (b) x2+y2, (c) x3. Hint: The
potential is a polynomial.
12.2.5. Each point on the spherical boundary of a solid ball of radius 1 has temperature equal
to its zenith angle ϕ. (a) Find the value of the equilibrium temperature at the center of the
ball. (b) Find the Taylor polynomial of degree 3, based at the origin, for the equilibrium
temperature distribution.
12.2.6. Solve Exercise 12.2.5 when the boundary temperature equals (a) cos ϕ, (b) cos θ, (c) θ.
12.2.7. A solid spherical container of radius 3 cm contains a hollow spherical cavity of radius
1 cm in its center. The inner cavity is ﬁlled with boiling water at 100◦, while the entire
container is immersed in an ice water bath at 0◦. Assume that the container is in thermal
equilibrium. True or false: The temperature at a point half-way between the container’s in-
ner and outer boundaries is 50◦. If true, explain. If false, what is the temperature at such a
point?
12.2.8. Find the electrostatic potential between two concentric spherical metal shells of respec-
tive radii 1 and 1.2, given that the inner shell is grounded, while the outer shell has poten-
tial equal to 1.
♦12.2.9. Use the chain rule to establish the formula (12.16) for the Laplacian in spherical coordi-
nates.

12.2 Separation of Variables for the Laplace Equation
525
♦12.2.10.(a) Prove that t = ±1 are both regular singular points for the order 0 Legendre dif-
ferential equation (12.28). (b) Prove that the Legendre eigenvalue problem (12.27–28) is
deﬁned by a self-adjoint operator with respect to the L2 inner product on the cut locus
[−1, 1]. (c) Discuss the orthogonality of the Legendre polynomials.
♦12.2.11. Solve Exercise 12.2.10 for the Legendre eigenvalue problem (12.26–27) of order m
along with the relevant Ferrers eigenfunctions.
♦12.2.12. Suppose m > 0. (a) Find the Green’s function for the boundary value problem
(1 −t2) d2P
dt2 −2t dP
dt −
m2
1 −t2 P = f(t),
| P(−1) |, | P(1) | < ∞.
Hint: The homogeneous diﬀerential equation has solutions
 1 + t
1 −t
m
2
and
 1 −t
1 + t
m
2
.
(b) Use part (a) to prove completeness of the Ferrers functions of order m > 0 on [−1, 1].
(c) Explain why there is no Green’s function in the order m = 0 case.
Remark: When m = 0, one can use the trick of Example 9.49 to prove completeness. Al-
though the Green’s function for the modiﬁed operator does not have an explicit elementary
formula, one can prove that it has logarithmic singularities at the endpoints, and hence ﬁ-
nite double L2 norm. See [120; §43] for details.
12.2.13. What happens when n < m in formula (12.31)?
♦12.2.14. Prove that the Legendre polynomial (12.29) has the explicit formula
Pn(t) =

0≤2m≤n
(−1)m
(2n −2m)!
2n (n −m)! m! (n −2m)! tn−2m.
(12.64)
♦12.2.15. Prove the following recurrence relation for the Ferrers functions:
P m+1
n
(t) =

1 −t2 dP m
n
dt
+
m t
√
1 −t2 P m
n (t).
(12.65)
♥12.2.16. In this exercise, we determine the L2 norms of the Ferrers functions. (a) First, prove
that
	 1
−1 (1 −t2)n dt = 22n+1 (n!)2
(2n + 1)! . Hint: Set t = cos θ and then integrate by parts re-
peatedly. (b) Prove that ∥Pn ∥2 =
2
2n + 1 . Hint: Integrate by parts repeatedly and then
use part (a). (c) Prove that ∥P m+1
n
∥2 = (n −m)(n + m + 1) ∥P m
n ∥2. Hint: Use (12.65)
and an integration by parts. (d) Finally, prove that ∥P m
n ∥2 =
2
2n + 1
(n + m)!
(n −m)! .
12.2.17.(a) Prove that P m
n (t) is an even or odd function according to whether m + n is an even
or odd integer. (b) Prove that its Fourier form, pm
n (ϕ), depends only on cos n ϕ, cos(n −2)ϕ,
cos(n −4)ϕ, . . . if m is even, and only on sin n ϕ, sin(n −2)ϕ, sin(n −4)ϕ, . . . if m is odd.
12.2.18. Let m be ﬁxed. Are the functions pm
n (ϕ) for n = 0, 1, 2, . . . mutually orthogonal with
respect to the standard L2 inner product on [0, π ]? If not, is there an inner product that
makes them orthogonal functions?
12.2.19. Prove that the surfaces deﬁned by the ﬁrst three spherical harmonics Y 0
0 , Y 0
1 , and Y 1
1 ,
as in Figure 12.5, are all spheres. Find their centers and radii.
♦12.2.20. Explain why the surface deﬁned by r = Y m
n (ϕ, θ) is obtained by rotating that deﬁned
by r = Y m
n (ϕ, θ) around the z–axis by 90◦.
♦12.2.21. Prove directly that the spherical Laplacian ΔS is a self-adjoint linear operator with
respect to the inner product (12.40).
♦12.2.22.(a) In view of Exercise 12.2.21, which orthogonality relations in (12.41) follow from
their status as eigenfunctions of the spherical Laplacian?
(b) Prove the general orthogonality formulae by direct computation.

526
12 Partial Diﬀerential Equations in Space
♦12.2.23. State and prove the orthogonality of the complex spherical harmonics (12.46). Then
establish the following formula for their norms:
∥Ym
n ∥2 =
		
S1 | Ym
n |2 dS =
4π(n + m)!
(2n + 1)(n −m)!
n = 0, 1, 2, . . . ,
m = −n, −n + 1, . . . , n.
(12.66)
♦12.2.24. Prove the formulae (12.42) for the norms of the spherical harmonics. Hint: Use Exer-
cise 12.2.16.
♦12.2.25. Justify the formulas in (12.50) for (a) H0
1, (b) H0
2, (c) &
H1
2.
12.2.26. Find formulas for the following harmonic polynomials (i) in spherical coordinates;
(ii) in rectangular coordinates:
(a) H0
4, (b) H4
4, (c) &
H4
4.
12.2.27. Explain why every polynomial solution of the Laplace equation is a linear combination
of the harmonic polynomials (12.49). Hint: Look at its Taylor series.
12.2.28.(a) Prove that if u(x, y, z) is any harmonic polynomial, then so are u(y, x, z), u(z, x, y),
and all other functions obtained by permuting the variables x, y, z. (b) Discuss the eﬀect of
such permutations on the basis harmonic polynomials Hm
n (x, y, z) appearing in (12.50).
12.2.29. Find the formulas in rectangular coordinates for the following complementary har-
monic functions:
(a) K0
0, (b) K1
1, (c) K0
2, (d) &
K1
2.
♦12.2.30. Let u(x, y, z) be a harmonic function deﬁned on the unit ball r ≤1. Prove that its
gradient at the center, ∇u(0), equals the average of the vector ﬁeld v(x) = x u(x) over the
unit sphere r = 1.
♦12.2.31.(a) Suppose u(x, y, z) is a solution to the Laplace equation. Prove that the function
U(x, y, z) = r−1 u(x/r2, y/r2, z/r2) obtained by inversion is also a solution. (b) Explain
how inversion can be used to solve boundary value problems on the exterior of a sphere.
(c) Use inversion to relate the solutions to Examples 12.6 and 12.7.
♦12.2.32. Suppose u(r, ϕ, θ) is the potential exterior to a spherical capacitor of unit radius.
(a) Prove that
lim
r →∞ru(r, ϕ, θ) equals the average value of u on the sphere.
(b) Use Exercise 12.2.31 to deduce this result as a consequence of Theorem 12.4.
12.2.33.(a) Write out, using spherical coordinates, formulas for the L2 inner product and norm
for scalar ﬁelds f(r, ϕ, θ) and g(r, ϕ, θ) on a solid ball of unit radius centered at the origin.
(b) Let f(x, y, z) = z and g(x, y, z) = x2 + y2. Find ∥f ∥, ∥g ∥and ⟨f , g ⟩.
(c) Verify the Cauchy–Schwarz and triangle inequalities for these two functions.
♦12.2.34. Use separation of variables to construct a Fourier series solution to the Laplace equa-
tion on a rectangular box, B = {0 < x < a, 0 < y < b, 0 < z < c}, subject to the Dirichlet
boundary conditions u(x, y, z) =
 h(x, y),
z = 0,
0 < x < a,
0 < y < b,
0,
at all other points in ∂B.
12.2.35. Find the equilibrium temperature distribution inside a unit cube that has 100◦tem-
perature on its top face, 0◦on its bottom face, while all four side faces are insulated.
12.2.36. Solve Exercise 12.2.35 when the top face of the cube has temperature u(x, y, 1) =
cos π x cos π y.
♣12.2.37. A solid unit cube is in thermal equilibrium when subject to 100◦temperature on its
top face and 0◦on all other faces. True or false: The temperature at the center equals the
average temperature over the surface of the cube.
12.2.38. Solve the boundary value problem
−∂2u
∂x2 −∂2u
∂y2 −∂2u
∂z2 + u = cos x cos y,
0 < x, y, z < π,
u(x, y, 0) = 1,
∂u
∂z (x, y, π) = ∂u
∂y (x, 0, z) = ∂u
∂y (x, π, z) = ∂u
∂z (0, y, z) = ∂u
∂x (π, y, z) = 0.

12.3 Green’s Functions for the Poisson Equation
527
12.2.39. Let C be the cylinder of height 1 and diameter 1 that sits on the (x, y)–plane centered
on the z–axis. (a) Write out, in cylindrical coordinates, the explicit formula for the L2 in-
ner product and norm on C.
(b) Let f(x, y, z) = z and g(x, y, z) = x2 + y2. Find ∥f ∥, ∥g ∥and ⟨f , g ⟩.
(c) Verify the Cauchy–Schwarz and triangle inequalities for these two functions.
♦12.2.40.(a) Write out the Laplace equation in cylindrical coordinates.
(b) Use separation of variables to construct a series solution to the Laplace equation on the
cylinder C = {x2 + y2 < 1, 0 < z < 1}, subject to the Dirichlet boundary conditions
u(x, y, z) =

h(x, y),
z = 0,
x2 + y2 < 1,
0,
at all other points in ∂C.
12.2.41. A cylinder of radius 1 and height 2 has 100◦temperature on its top face, 0◦on its
bottom face, while its curved side is fully insulated. Find its equilibrium temperature dis-
tribution.
12.2.42. Solve Exercise 12.2.41 if the curved sides are kept at 0◦instead.
12.3 Green’s Functions for the Poisson Equation
We now turn to the inhomogeneous form of the three-dimensional Laplace equation: the
Poisson equation
−Δu = f,
(12.67)
on a solid domain Ω ⊂R3. In order to uniquely specify the solution, we must impose
appropriate boundary conditions: Dirichlet or mixed. (As in the planar version, Neumann
boundary value problems have either inﬁnitely many solutions or no solutions, depending
upon whether the Fredholm conditions are satisﬁed or not.) We only need to discuss the
case of homogeneous boundary conditions, since, by linear superposition, an inhomogeneous
boundary value problem can be split into a homogeneous boundary value problem for the
inhomogeneous Poisson equation along with an inhomogeneous boundary value problem
for the homogeneous Laplace equation.
As in Chapter 6, we begin by analyzing the case of a delta function inhomogeneity
that is concentrated at a single point in the domain. Thus, for each ξ = (ξ, η, ζ) ∈Ω, the
Green’s function G(x; ξ) = G(x, y, z; ξ, η, ζ) is the unique solution to the Poisson equation
−Δu = δ(x −ξ) = δ(x −ξ) δ(y −η) δ(z −ζ)
for all
x ∈Ω,
(12.68)
subject to the chosen homogeneous boundary conditions.
The solution to the general
Poisson equation (12.67) is then obtained by superposition: We write the forcing function
f(x, y, z) =
  
Ω
f(ξ, η, ζ) δ(x −ξ) δ(y −η) δ(z −ζ) dξ dη dζ
(12.69)
as a linear superposition of delta functions. By linearity, the solution
u(x, y, z) =
  
Ω
f(ξ, η, ζ) G(x, y, z; ξ, η, ζ)dξ dη dζ
(12.70)
to the homogeneous boundary value problem for the Poisson equation (12.67) is then given
as the corresponding superposition of the Green’s function solutions.

528
12 Partial Diﬀerential Equations in Space
The Green’s function can also be used to solve the inhomogeneous Dirichlet boundary
value problem
−Δu = 0,
x ∈Ω,
u = h,
x ∈∂Ω.
(12.71)
The same argument that was used in the two-dimensional situation produces the solution
u(x) = −
 
∂Ω
∂G
∂n (x; ξ) h(ξ) dS,
(12.72)
where the normal derivative is taken with respect to the variable ξ ∈∂Ω. In the case that
Ω is a solid ball, this integral formula eﬀectively sums the spherical harmonic series (12.51);
see Theorem 12.12 below.
The Free–Space Green’s Function
Only in a few speciﬁc instances is an explicit formula for the Green’s function known.
Nevertheless, certain general guiding features can be readily established.
The starting
point is to investigate the Poisson equation (12.68) when the domain Ω = R3 is all of
three-dimensional space. We impose boundary constraints by seeking a solution that goes
to zero, u(x) →0, at large distances, ∥x ∥→∞. Since the Laplacian operator is invariant
under translations, we can, without loss of generality, place our delta impulse at the origin,
and concentrate on solving the particular case
−Δu = δ(x) ,
x ∈R3.
Since δ(x) = 0 for all x ̸= 0, the desired solution will, in fact, be a solution to the
homogeneous Laplace equation
Δu = 0,
x ̸= 0,
save, possibly, for a singularity at the origin.
The Laplace equation models the equilibria of a uniform isotropic medium, and so, as
noted in Exercise 12.1.7, is also invariant under three-dimensional rotations. This suggests
that, in any radially symmetric conﬁguration, the solution should depend only on the
distance r = ∥x ∥from the origin. Referring to the spherical coordinate form (12.16) of
the Laplacian operator, if u is a function of r only, then its derivatives with respect to the
angular coordinates ϕ, θ are zero, and so u(r) solves the ordinary diﬀerential equation
d2u
dr2 + 2
r
du
dr = 0.
(12.73)
This equation is, in eﬀect, a ﬁrst-order linear ordinary diﬀerential equation for v = du/dr
and hence is particularly easy to solve:
du
dr = v(r) = −b
r2 ,
and hence
u(r) = a + b
r ,
where a, b are arbitrary constants. The constant solution u(r) = a does not die away at
large distances, nor does it have a singularity at the origin. Therefore, if our intuition is
valid, the desired solution should be of the form
u = b
r =
b
∥x ∥=
b

x2 + y2 + z2 .
(12.74)

12.3 Green’s Functions for the Poisson Equation
529
Indeed, this function is harmonic — solves Laplace’s equation — everywhere away from
the origin and has a singularity at x = 0.
The solution (12.74) is, up to a constant multiple, the three-dimensional Newtonian
gravitational potential due to a point mass at the origin. Its gradient,
f(x) = ∇

b
∥x ∥

= −
b x
∥x ∥3 ,
(12.75)
deﬁnes the gravitational force vector at the point x. When b > 0, the force f(x) points
toward the mass at the origin. Its magnitude
∥f ∥=
b
∥x ∥2 =
b
r2
is proportional to the reciprocal of the squared distance, which is the well-known inverse
square law of three-dimensional Newtonian gravity. Formula (12.75) can also be interpreted
as the electrostatic force due to a concentrated electric charge at the origin, with (12.74)
giving the corresponding Coulomb potential. The constant b is positive when the charges
are of opposite signs, leading to an attractive force, and negative in the repulsive case of
like charges.
Returning to our problem, the remaining task is to ﬁx the multiple b such that the
Laplacian of our candidate solution (12.74) has a delta function singularity at the origin;
equivalently, we must determine a = 1/b such that
−Δ(r−1) = a δ(x).
(12.76)
This equation is certainly valid away from the origin, since δ(x) = 0 when x ̸= 0. To
investigate near the singularity, we integrate both sides of (12.76) over a small solid ball
Bε = {∥x ∥≤ε} of radius ε:
−
  
Bε
Δ(r−1) dx dy dz =
  
Bε
a δ(x) dx dy dz = a,
(12.77)
where we used the deﬁnition of the delta function to evaluate the right-hand side. On the
other hand, since Δ r−1 = ∇· ∇r−1, we can use the divergence theorem (12.8) to evaluate
the left-hand integral, whence
  
Bε
Δ(r−1) dx dy dz =
  
Bε
∇· ∇(r−1) dx dy dz =
 
Sε
∂
∂n
1
r

dS,
where the surface integral is over the bounding sphere Sε = ∂Bε = {∥x ∥= ε}.
The
sphere’s unit normal n points in the radial direction, and hence the normal derivative
coincides with diﬀerentiation with respect to r; in particular,
∂
∂n
1
r

= ∂
∂r
1
r

= −1
r2 .
The surface integral can now be explicitly evaluated:
 
Sε
∂
∂n
1
r

dS = −
 
Sε
1
r2 dS = −
 
Sε
1
ε2 dS = −4π,
since Sε has surface area 4πε2. Substituting this result back into (12.77), we conclude that
a = 4π,
and hence
−Δ r−1 = 4π δ(x).
(12.78)

530
12 Partial Diﬀerential Equations in Space
This is our desired formula! We conclude that a solution to the Poisson equation with a
delta function impulse at the origin is
G(x, y, z) =
1
4π r =
1
4π ∥x ∥=
1
4π

x2 + y2 + z2 ,
(12.79)
which is the three-dimensional Newtonian potential due to a unit point mass situated at
the origin.
If the singularity is concentrated at some other point ξ = (ξ, η, ζ), then we merely
translate the preceding solution. This leads immediately to the free-space Green’s function
G(x; ξ) = G(x −ξ) =
1
4π ∥x −ξ ∥=
1
4π

(x −ξ)2 + (y −η)2 + (z −ζ)2 .
(12.80)
The superposition principle (12.70) implies the following integral formula for the solutions
to the Poisson equation on all of three-dimensional space.
Theorem 12.8.
Assuming that f(x) →0 suﬃciently rapidly as ∥x ∥→∞, a par-
ticular solution to the Poisson equation
−Δu = f,
for
x ∈R3,
(12.81)
is given by
u⋆(x) = 1
4π
  
R3
f(ξ)
∥x −ξ ∥dξ =
1
4π
  
R3
f(ξ, η, ζ) dξ dη dζ

(x −ξ)2 + (y −η)2 + (z −ζ)2 . (12.82)
The general solution is u(x, y, z) = u⋆(x, y, z) + w(x, y, z), where w(x, y, z) is an arbitrary
harmonic function.
Example 12.9.
In this example, we compute the gravitational (or electrostatic)
potential in three-dimensional space due to a uniform solid ball, e.g., a spherical planet
such as the Earth. By rescaling, it suﬃces to consider the case in which the forcing function
is equal to 1 inside a ball of radius 1 and zero outside:
f(x) =
 1,
∥x ∥< 1,
0,
∥x ∥> 1.
The particular solution to the resulting Poisson equation (12.81) is given by the integral
u(x) = 1
4π
  
∥ξ ∥<1
1
∥x −ξ ∥dξ dη dζ.
(12.83)
Clearly, since the forcing function is radially symmetric, the solution u = u(r) is also
radially symmetric. To evaluate the integral, then, we can take x = (0, 0, z) to lie on the
z–axis, so that r = ∥x ∥= | z |. We use cylindrical coordinates ξ = (ρ cosθ, ρ sin θ, ζ), so
that
∥x −ξ ∥=

ρ2 + (z −ζ)2 .
The integral in (12.83) can then be explicitly computed:
1
4π
 1
−1
 √
1−ζ2
0
 2π
0
ρ dθ dρ dζ

ρ2 + (z −ζ)2
= 1
2
 1
−1
" 
1 + z2 −2z ζ −| z −ζ |
#
dζ =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1
3 | z | ,
| z | ≥1,
1
2 −z2
6 ,
| z | ≤1.

12.3 Green’s Functions for the Poisson Equation
531
r
u(r)
1
2
3
4
.25
.5
Figure 12.6.
Solution to Poisson’s equation in a solid ball.
Therefore, by radial symmetry, the solution is
u(x) =
⎧
⎪
⎨
⎪
⎩
1
3r ,
r = ∥x ∥≥1,
1
2 −r2
6 ,
r = ∥x ∥≤1,
(12.84)
plotted, as a function of r = ∥x ∥, in Figure 12.6. Note that, outside the solid ball, the
solution is a Newtonian potential corresponding to a concentrated point mass of magnitude
4
3 π — the total mass of the planet. We have thus demonstrated a well-known result in
gravitation and electrostatics: the exterior potential due to a spherically symmetric mass
(or electrically charged body) is the same as if all the mass (charge) were concentrated at
its center. In the darkness of outer space, if you cannot see a spherical planet, you can
determine only its mass, not its size, by measuring its external gravitational force.
Bounded Domains and the Method of Images
Suppose we now wish to solve the inhomogeneous Poisson equation (12.67) on a bounded
domain Ω ⊂R3. To construct the desired Green’s function, we proceed as follows. The
Newtonian potential (12.80) is a particular solution to the underlying inhomogeneous equa-
tion
−Δu = δ(x −ξ),
x ∈Ω,
(12.85)
but it almost surely does not have the proper boundary values on ∂Ω. By linearity, the
general solution to such an inhomogeneous linear equation must take the form
u(x) =
1
4π ∥x −ξ ∥−v(x),
(12.86)
where the ﬁrst term is a particular solution, while v(x) is an arbitrary solution to the ho-
mogeneous equation Δv = 0, i.e., an arbitrary harmonic function. The solution (12.86) sat-
isﬁes the homogeneous boundary conditions, provided the boundary values of v(x) match
those of the Green’s function. Let us explicitly state the result in the Dirichlet case.
Theorem 12.10.
The Green’s function for the homogeneous Dirichlet boundary
value problem
−Δu = f
for
x ∈Ω,
u = 0
for
∥x ∥∈∂Ω,

532
12 Partial Diﬀerential Equations in Space
0
ξ
η
x
Figure 12.7.
Method of Images for the unit ball.
for the Poisson equation in a domain Ω ⊂R3 has the form
G(x; ξ) =
1
4π ∥x −ξ ∥−v(x; ξ),
x, ξ ∈Ω,
(12.87)
where v(x; ξ) is the harmonic function of x ∈Ω that satisﬁes
v(x; ξ) =
1
4π ∥x −ξ ∥
for all
x ∈∂Ω.
(12.88)
In this manner, we have reduced the determination of the Green’s function to the
solution to a particular family of Laplace boundary value problems, which are parametrized
by the point ξ ∈Ω. In certain domains with simple geometry, the Method of Images can
be used to produce an explicit formula for the Green’s function. As in Section 6.3, the idea
is to match the boundary values of the free-space Green’s function due to a delta impulse
at a point inside the domain with one or more additional Green’s functions corresponding
to impulses at points outside the domain — the “image points”.
The case of a solid ball of radius 1 with Dirichlet boundary conditions is the easiest to
handle. Indeed, the same geometric construction that we used for a planar disk, redrawn
in Figure 12.7, applies here. Although identical to Figure 6.13, we are re-interpreting it as
a three-dimensional diagram, with the circle representing the unit sphere, while the lines
remain lines. The required image point is given by inversion:
η =
ξ
∥ξ ∥2 ,
whereby
∥ξ ∥=
1
∥η ∥.
By the similar triangles argument used before, we have
∥ξ ∥
∥x ∥= ∥x ∥
∥η ∥= ∥x −ξ ∥
∥x −η ∥,
and therefore
∥x ∥= 1.
As a result, the function
v(x, ξ) = 1
4π
∥η ∥
∥x −η ∥= 1
4π
∥ξ ∥
∥ξ −∥ξ ∥2 x ∥
has the same boundary values on the unit sphere as the Newtonian potential:
1
4π
∥η ∥
∥x −η ∥=
1
4π∥x −ξ ∥
whenever
∥x ∥= 1.

12.3 Green’s Functions for the Poisson Equation
533
We conclude that their diﬀerence
G(x; ξ) = 1
4π

1
∥x −ξ ∥−
∥ξ ∥
∥ξ −∥ξ ∥2 x ∥

(12.89)
has the required properties of the Green’s function: it satisﬁes the Laplace equation inside
the unit ball except at the delta function singularity x = ξ, and, moreover, G(x; ξ) = 0
has homogeneous Dirichlet conditions on the spherical boundary ∥x ∥= 1.
With the Green’s function in hand, we can apply the general superposition for-
mula (12.70) to arrive at a solution to the Dirichlet boundary value problem for the Poisson
equation in the unit ball.
Theorem 12.11. The solution to the homogeneous Dirichlet boundary value prob-
lem
−Δu = f
for
∥x ∥< 1,
u = 0
for
∥x ∥= 1,
on the unit ball is given by the integral
u(x) = 1
4π
  
∥ξ ∥≤1

1
∥x −ξ ∥−
∥ξ ∥
∥ξ −∥ξ ∥2 x ∥

f(ξ) dξ dη dζ.
(12.90)
By the same token, formula (12.72) provides a solution to the inhomogeneous Dirichlet
boundary value problem for the Laplace equation on a ball.
Theorem 12.12. The solution to the homogeneous Dirichlet boundary value prob-
lem
−Δu = 0
for
∥x ∥< 1,
u = h
for
∥x ∥= 1,
is given by the following surface integral:
u(x) = 1
4π
 
∥ξ ∥=1
1 −∥x ∥2
∥ξ −x ∥3 h(ξ) dS.
(12.91)
Proof : We start with the explicit formula (12.89) for the Green’s function on the
unit ball.
Since the normal derivative on the unit sphere ∥ξ ∥= 1 can be written as
∂/∂n = ξ · ∇ξ, a short computation demonstrates that
∂G
∂n (x; ξ) = 1
4π

x · ξ −∥ξ ∥2
∥x −ξ ∥3
−∥ξ ∥3
x · ξ −∥ξ ∥2 ∥x ∥2 
∥ξ −∥ξ ∥2 x ∥3

= 1
4π
∥x ∥2 −1
∥ξ −x ∥3 .
The solution formula (12.91) thus immediately follows from (12.72).
Q.E.D.
For example, the series solution (12.60) to the spherical capacitor problem of Exam-
ple 12.6 can thus be re-expressed as a surface integral:
u(x, y, z) = 1
4π
 
{ξ2+η2+ζ2=1, ζ>0}
(1 −x2 −y2 −z2) dS

(ξ −x)2 + (η −y)2 + (ζ −z)2 3/2
=
 π
−π
 π/2
0
(1 −x2 −y2 −z2) sin ϕ dϕ dθ

(cos θ sin ϕ −x)2 + (sin θ sin ϕ −y)2 + (cos ϕ −z)2 3/2 .

534
12 Partial Diﬀerential Equations in Space
Exercises
12.3.1. Find the equilibrium temperature of a sphere of radius 1 whose boundary is held at 0◦
while a concentrated unit heat source is applied at (a) the center; (b) a point half-way be-
tween the center and the boundary.
12.3.2. A hot soldering iron is continually applied to the north pole of a solid spherical ball of
radius 1. Find the equilibrium temperature.
12.3.3. Write down the gravitional potential — both external and internal — due to a spherical
planet of radius R composed out of a uniform material with density ρ.
12.3.4.(a) Find the gravitational potential due to a spherical shell of unit density obtained by
carving out a spherical cavity of radius a from a solid ball of radius b > a. Hint: Use the
solution to Exercise 12.3.3. (b) What is the gravitational force inside the cavity?
(c) Show that outside the shell, the gravitational potential is as if the entire mass were con-
centrated at the origin.
♣12.3.5.(a) Write down an integral formula for the gravitational potential and gravitational
force ﬁeld due to a mass of unit density in the shape of a solid unit cube that is centered
at the origin. (b) Use numerical integration to determine the gravitational force vector at
the points (3, 0, 0) and
√
3 ,
√
3 ,
√
3

. Before doing the calculation, see whether you can
predict which experiences a stronger force, and then check your prediction numerically.
(c) Suppose the mass is re-formed into a sphere. How does this aﬀect the gravitational
force at the two points? First predict whether it will increase, decrease, or stay the same.
Then test your prediction by computing the values and comparing with those you computed
in part (b).
12.3.6. A thin hollow metal sphere of unit radius is grounded. Find the electrostatic potential
inside the sphere due to a small solid metal ball of radius ρ < 1 placed at its center, assum-
ing unit charge density throughout the ball.
12.3.7. A thin straight rod of unit density and length 2ℓis ﬁxed on the z–axis centered at the
origin. Find the induced (a) gravitational potential and (b) gravitational force experienced
by a point (x, y, z) not on the rod.
♥12.3.8.(a) Find the gravitational force due to a thin, uniform straight rod of unit density and
inﬁnite length by letting ℓ→∞in your solution to Exercise 12.3.7(b).
(b) Show that the
force ﬁeld of part (a) has a potential function that can be identiﬁed with the two-dimensional
logarithmic gravitational potential due to a point mass at the origin. Thus, two-dimensional
gravitation can be regarded as a cross-section of three-dimensional gravitation due to in-
ﬁnitely long vertical line masses.
(c) Is your potential function the limit, as ℓ→∞, of the
potential function you found in Exercise 12.3.7(a)? Discuss.
12.3.9. Which well-known solutions to the Laplace equation comes from setting m = n = 0 in
(12.61)?
12.3.10. Use the Fredholm Alternative to analyze the existence and uniqueness of solutions to
the homogeneous Neumann boundary value problem for the Poisson equation on a bounded
domain Ω ⊂R3.
♦12.3.11. Mimic the proof of Theorem 6.19 to establish the solution formula (12.72).
12.3.12. Use the Method of Images to ﬁnd the Green’s function for a solid hemisphere of unit
radius subject to homogeneous Dirichlet boundary conditions.

12.4 The Heat Equation for Three–Dimensional Media
535
12.4 The Heat Equation for Three–Dimensional Media
Thermal diﬀusion in a uniform isotropic solid body Ω ⊂R3 is modeled by the three-
dimensional heat equation
∂u
∂t = γ Δu = γ
 ∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2

,
(x, y, z) ∈Ω.
(12.92)
The positivity of the body’s thermal diﬀusivity, γ > 0, is required on both physical and
mathematical grounds. The physical derivation is exactly the same as that for the two-
dimensional version (11.1), and does not need to be repeated in detail. Brieﬂy, Fourier’s
law expresses the heat ﬂux vector as a multiple of the temperature gradient, w = −κ ∇u,
while energy conservation implies that its divergence is proportional to the rate of change of
temperature: ∇·w = −σ ut. Combining these two physical laws and assuming uniformity,
whereby κ and σ are constant, produces (12.92) with γ = κ/σ.
As always, we must impose suitable boundary conditions:
either Dirichlet condi-
tions u = h that specify the boundary temperature; (homogeneous) Neumann conditions
∂u/∂n = 0 corresponding to an insulated boundary; or a mixture of the two. Given the
body’s temperature
u(t0, x, y, z) = f(x, y, z)
(12.93)
at an initial time t0, it can be proved, [38, 61, 99], that the resulting initial-boundary value
problem is well-posed, which means that there is a unique classical solution u(t, x, y, z),
deﬁned at all subsequent times t > t0, that depends continuously on the initial data.
As in the one- and two-dimensional versions, we begin by restricting our attention to
homogeneous boundary conditions. Separation of variables works as usual, and we quickly
review the basic ideas. One begins by imposing an exponential solution ansatz
u(t, x) = e−λt v(x).
Substituting into the diﬀerential equation and canceling the exponentials, it follows that v
satisﬁes the Helmholtz eigenvalue problem
γ Δv + λ v = 0,
subject to the relevant boundary conditions. For Dirichlet and mixed boundary conditions,
the Laplacian is a positive deﬁnite operator, and hence the eigenvalues are all strictly
positive,
0 < λ1 ≤λ2 ≤· · · ,
with
λn −→∞,
as
n →∞.
Moreover, on a bounded domain, the Helmholtz eigenfunctions are complete, and so linear
superposition implies that the solution can be written as an eigenfunction series
u(t, x) =
∞

n=1
cn e−λn t vn(x).
(12.94)
The coeﬃcients cn are uniquely prescribed by the initial condition (12.93):
u(t0, x) =
∞

n=1
cn e−λn t0 vn(x) = f(x).
(12.95)

536
12 Partial Diﬀerential Equations in Space
Self-adjointness of the boundary value problem implies orthogonality of the eigenfunctions,
and hence the coeﬃcients are obtained via the usual inner product formulae:
cn = eλn t0 ⟨f , vn ⟩
∥vn ∥2 = eλn t0
  
Ω
f(x) vn(x) dx dy dz
  
Ω
vn(x)2 dx dy dz
.
(12.96)
The resulting solution decays exponentially fast to thermal equilibrium, u(t, x) →0
as t →∞, typically at a rate equal to the smallest positive eigenvalue λ1 > 0, although
special solutions, whose initial series coeﬃcients vanish, will decay at a faster rate governed
by a higher eigenvalue. Since the higher modes — the terms with n ≫0 — go to zero
extremely rapidly with increasing t, the solution can be well approximated by the ﬁrst few
terms in its eigenfunction expansion. As a consequence, the heat equation rapidly smooths
out discontinuities and eliminates high-frequency noise in the initial data.
Unfortunately, explicit formulas for the eigenfunctions and eigenvalues are rare. Most
explicit eigensolutions of the Helmholtz boundary value problem require a further separa-
tion of variables. In a rectangular box, one separates the solution into a product of functions
depending upon the individual Cartesian coordinates, and the eigenfunctions are written
as products of trigonometric functions; see Exercise 12.4.1 for details.
In a cylindrical
domain, the separation is eﬀected in cylindrical coordinates, which leads to eigensolutions
involving trigonometric and Bessel functions, as outlined in Exercise 12.4.5. The most in-
teresting and enlightening case is a spherical domain, and we treat this particular problem
in complete detail in the ensuing subsection.
Exercises
♦12.4.1. Let B = {0 < x < a, 0 < y < b, 0 < z < c} be a solid box of size a × b × c.
(a) Write down an initial-boundary value problem for the thermodynamics of the box when
all its sides are all held at 0◦and its initial temperature is f(x, y, z). (b) Use separation
of variables to construct the normal mode solutions. (c) Write down a series representing
the general solution to the initial-boundary value problem. What are the formulas for the
coeﬃcients in your series? (d) What is the equilibrium temperature? How fast does the
temperature in the box decay to equilibrium?
12.4.2. True or false: In the context of Exercise 12.4.1, among all boxes of a given volume V , a
cube decays slowest to thermal equilibrium. What is the cube’s decay rate?
12.4.3. Answer Exercises 12.4.1 and 12.4.2 when the top of the box, where z = c, is insulated.
12.4.4. A rectangular brick of size 1 cm × 2 cm × 3 cm made out of material with diﬀusion
coeﬃcient γ = 6 is insulated on ﬁve sides, while one of its small ends is held at temperature
u(x, y, 0) = cos π x cos 2π y. (a) Find the eventual equilibrium temperature distribution.
(b) If the brick is initially heated in an oven, how fast does it return to equilibrium?
♦12.4.5. Let C =
#
0 ≤

x2 + y2 < a, 0 < z < h
$
be a solid cylinder of radius a and height h.
(a) Write down an initial-boundary value problem in cylindrical coordinates for the thermo-
dynamics of the cylinder when its sides, top, and bottom are all held at 0◦.
(b) Use separation of variables to write down a series representing the general solution to
the initial-boundary value problem. What are the formulas for the coeﬃcients in your
series?

12.4 The Heat Equation for Three–Dimensional Media
537
(c) What is the eventual equilibrium temperature?
(d) How fast does the temperature in the cylinder go to equilibrium?
12.4.6. Find the solution to the initial-boundary value problem in Exercise 12.4.5 when the ini-
tial temperature of the cylinder is uniformly 30◦. Hint: Use (11.112) to evaluate the coeﬃ-
cients.
♥12.4.7. A cylindrical can that contains 355 ml of soda is removed from the refrigerator. Find
the optimal cylindrical shape for such a can in order to keep the soda cold the longest. Is
this the manufactured shape of a standard soda can?
♥12.4.8. True or false: Among all solid cylinders of a given volume, the one that reaches ther-
mal equilibrium the slowest, when subject to homogeneous Dirichlet boundary conditions, is
the one that has the least surface area. Justify your answer.
♥12.4.9. Among all fully insulated solid cylinders of unit volume, which cools down
(i) the slowest? (ii) the fastest?
♦12.4.10. Write down a series for the solution to the homogeneous Neumann boundary value
problem for the heat equation on a bounded domain Ω ⊂R3, corresponding to the thermo-
dynamics of a completely insulated solid body. What is the equilibrium temperature of the
body? Does the solution decay to equilibrium? If so, how fast?
♦12.4.11. Suppose u(t, x, y, z) is a solution to the heat equation on a fully insulated bounded
domain Ω ⊂R3. Use the identities in Exercise 12.1.11 to prove the following:
(a) The total heat H(t) =
			
Ω u(t, x, y, z) dx dy dz is conserved, i.e., is constant. Explain
how this can be used to determine the equilibrium temperature of the body.
(b) If u is a non-equilibrium solution, its squared L2 norm E(t) =
		 	
Ω u(t, x, y, z)2 dx dy dz
is a strictly decreasing function of t.
(c) Use part (b) to prove uniqueness of solutions to the initial value problem.
♦12.4.12. State and prove a Maximum Principle for the three-dimensional heat equation.
Heating of a Ball
Our goal is to study heat propagation in a solid spherical body, e.g., the Earth.†
For
simplicity, we take the diﬀusivity γ = 1, and consider the heat equation on a solid spherical
ball of unit radius, B1 = {∥x ∥< 1}, that is subject to homogeneous Dirichlet boundary
conditions. Once we know how to solve this particular case, an easy scaling argument, as
outlined in Exercise 12.4.16, will allow us to ﬁnd the solution for a ball of arbitrary radius
and general diﬀusivity.
As usual, when dealing with a spherical geometry, we adopt spherical coordinates
r, ϕ, θ, as in (12.15), in terms of which the heat equation takes the form
∂u
∂t = Δu = ∂2u
∂r2 + 2
r
∂u
∂r + 1
r2
∂2u
∂ϕ2 +
cos ϕ
r2 sin ϕ
∂u
∂ϕ +
1
r2 sin2 ϕ
∂2u
∂θ2 ,
(12.97)
where we have used our handy spherical coordinate formula (12.16) for the Laplacian. The
†
In this admittedly simplistic model, we are assuming that the Earth is composed of a
completely uniform and isotropic solid material.

538
12 Partial Diﬀerential Equations in Space
standard diﬀusive separation of variables ansatz
u(t, r, ϕ, θ) = e−λt v(r, ϕ, θ)
requires us to analyze the spherical coordinate form of the Helmholtz equation
Δv + λ v = ∂2v
∂r2 + 2
r
∂v
∂r + 1
r2
∂2v
∂ϕ2 +
cos ϕ
r2 sin ϕ
∂v
∂ϕ +
1
r2 sin2 ϕ
∂2v
∂θ2 + λ v = 0
(12.98)
on the unit ball Ω = {r < 1} under homogeneous Dirichlet boundary conditions. To make
further progress, we invoke a second variable separation, splitting oﬀthe radial coordinate
by setting
v(r, ϕ, θ) = p(r) w(ϕ, θ).
The function w must be 2π–periodic in θ and well deﬁned on the z–axis, i.e., when ϕ = 0, π.
Substituting this ansatz into (12.98), and separating all the r-dependent terms from those
terms depending on the angular variables ϕ, θ leads to a pair of diﬀerential equations
involving a separation constant, denoted by μ. The ﬁrst is an ordinary diﬀerential equation
r2 d2p
dr2 + 2r dp
dr + (λ r2 −μ)p = 0,
(12.99)
for the radial component p(r), while the second is a familiar partial diﬀerential equation
ΔS w + μ w = ∂2w
∂ϕ2 + cos ϕ
sin ϕ
∂w
∂ϕ +
1
sin2 ϕ
∂2w
∂θ2 + μ w = 0,
(12.100)
for its angular counterpart w(ϕ, θ).
The operator ΔS is the spherical Laplacian from
(12.19). In Section 12.2, we showed that its eigenvalues are
μm = m(m + 1)
for
m = 0, 1, 2, 3, . . ..
The mth eigenvalue admits 2m + 1 linearly independent eigenfunctions: the spherical har-
monics Y 0
m, . . . , Y m
m , Y 1
m, . . . , Y m
m deﬁned in (12.38).
Spherical Bessel Functions
The radial ordinary diﬀerential equation (12.99) can be solved by setting
q(r) = √r p(r).
(12.101)
We use the product rule to relate the derivatives of q and p, whereby
p =
q
r1/2 ,
dp
dr =
1
r1/2
dq
dr −
q
2r3/2 ,
d2p
dr2 =
1
r1/2
d2q
dr2 −
1
r3/2
dq
dr +
3q
4r5/2 .
Substituting these expressions back into (12.99) with μ = μm = m(m + 1) and multiplying
the resulting equation by √r, we discover that q(r) must solve the diﬀerential equation
r2 d2q
dr2 + r dq
dr +

λ r2 −

m + 1
2
2 
q = 0,
(12.102)
which we recognize as the rescaled Bessel equation (11.56) of half-integer order m + 1
2 .
Consequently, the solution to (12.102) that remains bounded at r = 0 is (up to a scalar
multiple) the rescaled Bessel function
q(r) = Jm+1/2
√
λ r

.

12.4 The Heat Equation for Three–Dimensional Media
539
The corresponding solution
p(r) = r−1/2 Jm+1/2
√
λ r

(12.103)
to (12.99) is important enough to warrant a special name.
Deﬁnition 12.13.
The spherical Bessel function of order m ≥0 is deﬁned by the
formula
Sm(x) =
 π
2 x Jm+1/2(x).
(12.104)
Remark: The multiplicative factor

π/2 is included in the deﬁnition so as to avoid
annoying factors of √π and
√
2 in the subsequent formulas.
Surprisingly, unlike the Bessel functions of integer order, the spherical Bessel functions
are all elementary functions! Comparing (12.104) with (11.105), we see that the spherical
Bessel function of order 0 is
S0(x) = sin x
x
.
(12.105)
The corresponding explicit formulas for the higher-order spherical Bessel functions can be
obtained through the general recurrence relation
Sm+1(x) = −dSm
dx + m
x Sm(x),
(12.106)
which is a consequence of the Bessel function recurrence formula (11.111). Indeed,
dSm
dx =
 π
2x
dJm+1/2
dx
−1
2
π
2
1
x3/2 Jm+1/2(x)
= −
 π
2x

Jm+3/2(x) + m + 1
2
x
Jm+1/2(x)

−1
2
π
2
1
x3/2 Jm+1/2(x)
= −
 π
2x Jm+3/2(x) + m
x
 π
2x Jm+1/2(x) = −Sm+1(x) + m
x Sm(x).
The next few spherical Bessel functions are, therefore,
S1(x) = −dS0
dx
= −cos x
x
+ sin x
x2
,
S2(x) = −dS1
dx + S1
x
= −sin x
x
−3 cos x
x2
+ 3 sin x
x3
,
S3(x) = −dS2
dx + 2S2
x
= cos x
x
−6 sin x
x2
−15 cosx
x3
+ 15 sin x
x4
,
(12.107)
and so on. Figure 11.4 provides graphs of the ﬁrst four spherical Bessel functions on the
interval 0 ≤x ≤20; the vertical axes range from −.5 to 1.0. We note that
S0(0) = 1,
whereas
Sm(0) = 0
for
m > 0,
(12.108)
whose proof is the task of Exercise 12.4.26. Thus, our radial solution (12.103) is, apart
from an inessential constant multiple, a rescaled spherical Bessel function of order m:
p(r) = Sm
√
λ r

.

540
12 Partial Diﬀerential Equations in Space
S0(x)
S1(x)
S2(x)
S3(x)
Figure 12.8.
Spherical Bessel functions.
So far, we have not taken into account the (homogeneous) Dirichlet boundary condition
at r = 1. This requires
p(1) = 0,
and hence
Sm
√
λ

= 0.
Therefore,
√
λ must be a root of the mth order spherical Bessel function. We introduce the
notation
0 < σm,1 < σm,2 < σm,3 < · · ·
to denote the successive (positive) spherical Bessel roots, satisfying
Sm(σm,n) = 0
for
n = 1, 2, . . . .
(12.109)
In particular the roots of the zeroth order spherical Bessel function S0(x) = x−1 sin x are
just the integer multiples of π:
σ0,n = nπ
for
n = 1, 2, . . . .
The higher-order roots are not expressible in terms of known constants. A table of all
spherical Bessel roots that are < 13 appears below. The columns of the table are indexed
by m, the order, while the rows are indexed by n, the root number.
Re-assembling the individual constituents, we have now demonstrated that the sepa-
rable eigenfunctions of the Helmholtz equation on a solid ball of radius 1, when subject
to homogeneous Dirichlet boundary conditions, are products of spherical Bessel functions
and spherical harmonics,
vk,m,n(r, ϕ, θ) = Sm(σm,n r) Y k
m(ϕ, θ),
vk,m,n(r, ϕ, θ) = Sm(σm,n r) Y k
m(ϕ, θ),
m = 0, 1, 2, . . . ,
k = 0, . . ., m,
n = 1, 2, 3, . . . .
(12.110)

12.4 The Heat Equation for Three–Dimensional Media
541
Spherical Bessel Roots σm,n
n
9
m
0
1
2
3
4
5
6
7
1
3.1416
4.4934
5.7635
8.1826
9.3558
10.5128
11.6570
12.7908
. . .
2
6.2832
7.7253
9.0950
11.7049
12.9665
...
...
...
3
9.4248
10.9041
12.3229
...
...
4
12.5664
...
...
...
...
The corresponding eigenvalues
λm,n = σ2
m,n,
m = 0, 1, 2, . . . ,
n = 1, 2, 3, . . . ,
(12.111)
are the squared spherical Bessel roots.
Since there are 2m + 1 independent spherical
harmonics of order m, the eigenvalue λm,n admits 2m + 1 linearly independent eigenfunc-
tions, namely v0,m,n, . . . , vm,m,n, v1,m,n, . . . , vm,m,n. In particular, the radially symmetric
solutions are the eigenfunctions with k = m = 0:
vn(r) = v0,0,n(r) = S0(σ0,n r) = sin nπ r
nπ r
,
n = 1, 2, . . . .
(12.112)
Further analysis, cf. [34], demonstrates that the separable solutions (12.110) form a com-
plete system of eigenfunctions for the Helmholtz equation on the unit ball with homoge-
neous Dirichlet boundary conditions.
We have thus completely determined the basic separable solutions to the heat equation
on a solid unit ball subject to homogeneous Dirichlet boundary conditions.
They are
products of exponential functions of time, spherical Bessel functions of the radius, and
spherical harmonics:
uk,m,n(t, r, ϕ, θ) = e−σ2
m,nt Sm(σm,n r) Y k
m(ϕ, θ),
uk,m,n(t, r, ϕ, θ) = e−σ2
m,nt Sm(σm,n r) Y k
m(ϕ, θ).
(12.113)
The general solution can be written as an inﬁnite “Fourier–Bessel–spherical harmonic”
series in these fundamental modes:
u(t, r, ϕ, θ) =
∞

m=0
∞

n=1
e−σ2
m,nt Sm(σm,n r)

c0,m,n
2
Y 0
m(ϕ, θ)
+
m

k=1
:
ck,m,nY k
m(ϕ, θ) + ck,m,n Y k
m(ϕ, θ)
;
.
(12.114)
The series’ coeﬃcients are uniquely prescribed by the initial data u(0, r, ϕ, θ) = f(r, ϕ, θ),

542
12 Partial Diﬀerential Equations in Space
and their explicit formulae†
ck,m,n =
(2m + 1)(m −k)!
π (m + k)! Sm+1(σm,n)2
 π
−π
 π
0
 1
0
f(r, ϕ, θ) vk,m,n(r, ϕ, θ) r2 sin ϕ dr dϕ dθ,
ck,m,n =
(2m + 1)(m −k)!
π (m + k)! Sm+1(σm,n)2
 π
−π
 π
0
 1
0
f(r, ϕ, θ) vk,m,n(r, ϕ, θ) r2 sin ϕ dr dϕ dθ,
(12.115)
follow from the usual orthogonality relations among the eigenfunctions, combined with the
formulas
∥v0,m,n ∥=

2π
2m + 1 Sm+1(σm,n),
∥vk,m,n ∥= ∥vk,m,n ∥=

π(m + k)!
(2m + 1)(m −k)! Sm+1(σm,n),
k > 0,
(12.116)
for their norms, to be established in Exercise 12.4.29. In particular, the slowest-decaying
mode is the spherically symmetric function
u0,0,1(t, r) = e−π2 t sin πr
π r
,
(12.117)
corresponding to the smallest eigenvalue λ0,1 = σ2
0,1 = π2. Therefore, typically, the decay
to thermal equilibrium of a unit sphere is at an exponential rate of π2 ≈9.8696, or, to a
very rough approximation, 10.
Exercises
12.4.13. It takes a solid ball of radius 1 cm ten minutes to return to (approximate) thermal
equilibrium. How long does it take a similar ball of radius 2?
12.4.14. If a 200-gram potato served hot from the oven takes 15 minutes until its maximum
temperature is less than 40◦C, how long does it take a 300-gram potato of the same shape
to cool oﬀ?
♥12.4.15. A uniform solid metal ball of radius 1 meter, with diﬀusion coeﬃcient γ = 2, is taken
from a 300◦oven and immersed in a bucket of ice water. (a) Write down an initial-boundary
value problem that describes the temperature of the ball.
(b) Find a series solution for
the temperature. (c) At what time is the temperature ≤50◦throughout the ball?
♦12.4.16. Find the decay rate to thermal equilibrium of a solid spherical ball of radius R and
diﬀusion coeﬃcient γ when subject to homogeneous Dirichlet boundary conditions.
12.4.17. True or false: A heated solid hemisphere placed in a 0◦environment cools down twice
as fast as a solid sphere of the same radius made out of the same material.
12.4.18. A fully insulated solid spherical ball of radius 1 has initial temperature distribution
f(r, ϕ, θ).
(a) Write down a formula for the equilibrium temperature of the ball.
(b) What is the rate of decay of the ball to thermal equilibrium?
†
We use the spherical coordinate form of the L2 inner product on the ball.

12.4 The Heat Equation for Three–Dimensional Media
543
12.4.19. Which cools down to equilibrium faster: a fully insulated solid ball or one whose bound-
ary is held ﬁxed at 0◦? How much faster?
12.4.20. A solid sphere and solid cube are made out of the same material and have the same
volume. Both are heated in an oven and then submerged in a large vat of water. Which
will cool down faster? Explain and justify your answer.
12.4.21. Answer Exercise 12.4.20 when the two solids have the same surface area.
12.4.22. Suppose the solid spherical shell in Exercise 12.2.7 starts oﬀat room temperature. As-
suming that the water in the center remains at 100◦, ﬁnd the rate at which the shell tends
to thermal equilibrium.
♥12.4.23. The thermodynamics of a thin, uniform, spherical shell of unit radius is governed by
the spherical heat equation ut = γ ΔSu, u(0, ϕ, θ) = f(ϕ, θ), in which ΔS is the spheri-
cal Laplacian (12.19). The solution u(t, ϕ, θ) represents the temperature of the point on the
unit sphere with angular coordinates ϕ, θ, while f(ϕ, θ) is the initial temperature distribu-
tion. (a) Find the eigensolutions. (b) Write down the solution to the initial value problem
as a series in eigensolutions. (c) What is the ﬁnal equilibrium temperature of the spheri-
cal shell? (d) What is its rate of decay to equilibrium? (e) Find the solution and the ﬁnal
equilibrium temperature when f(ϕ, θ) = (i) sin ϕ cos θ; (ii) cos 2ϕ.
12.4.24. A spherical potato, of radius R = 7.5 cm and thermal diﬀusivity γ = .3 cm2/sec, is
initially at room temperature, 25◦C, and is placed in a pot of boiling water at 100◦C. The
potato is cooked when it has reached the temperature of at least 90◦C throughout. How
long do you have to wait until the potato is done?
12.4.25.(a) Explain why the spherical Bessel function S1(x) is bounded at x = 0. What is
S1(0)?
(b) Answer the same question for S2(x).
♦12.4.26. Prove the formulae (12.108).
♦12.4.27.(a) Find a recurrence relation expressing the spherical Bessel function Sm−1(x) in
terms of Sm(x). (b) Prove that
d
dx

x3
Sm(x)2 −Sm−1(x) Sm+1(x)
  
= 2x2 Sm(x)2.
♦12.4.28. Let m ≥0 be a ﬁxed integer. (a) Prove that the rescaled spherical Bessel functions
vn(r) = Sm(σm,n r), n = 1, 2, . . . , are mutually orthogonal under the inner product
⟨f , g ⟩=
	 1
0 f(r) g(r) r2 dr. (b) Prove that ∥vn ∥=
1
√
2 | Sm+1(σm,n) |. Hint: Mimic the
method outlined in Exercise 11.4.22, using the identity in Exercise 12.4.27(b).
♦12.4.29.(a) Use the result of Exercise 12.4.28 to prove the formulae (12.116) for the L2 norms
of the eigenfunctions (12.110).
(b) Justify the formulae (12.115).
The Fundamental Solution of the Heat Equation
For the heat equation (as well as more general diﬀusion equations), the fundamental so-
lution measures the response of the body to an instantaneously applied concentrated unit
heat source.
Thus, given a point ξ = (ξ, η, ζ) ∈Ω within the body, the fundamental
solution
u(t, x) = F(t, x; ξ) = F(t, x, y, z; ξ, η, ζ)
solves the initial-boundary value problem
ut = Δu,
u(0, x) = δ(x −ξ),
for
x ∈Ω,
t > 0,
(12.118)

544
12 Partial Diﬀerential Equations in Space
subject to the selected homogeneous boundary conditions — Dirichlet, Neumann, or mixed.
Explicit formulas for the fundamental solution are rare, although in bounded domains
it is possible to construct it as an eigenfunction series, as described in Section 9.5. The
one case amenable to a complete analysis is that in which the heat is distributed over
all of three-dimensional space, so Ω = R3. We recall that Lemma 11.11 showed how to
construct solutions of the two-dimensional heat equation as products of one-dimensional
solutions. In a similar manner, if p(t, x), q(t, x), and r(t, x) are any three solutions to the
one-dimensional heat equation ut = γ uxx, then their product
u(t, x, y, z) = p(t, x) q(t, y) r(t, z)
(12.119)
is a solution to the three-dimensional heat equation
ut = γ (uxx + uyy + uzz).
In particular, choosing
p(t, x) = e−(x−ξ)2/4γ t
2 √πγ t
,
q(t, y) = e−(y−η)2/4γ t
2 √πγ t
,
r(t, z) = e−(z−ζ)2/4γ t
2 √πγ t
,
to all be one-dimensional fundamental solutions, we are immediately led to the fundamental
solution in the form of a three-dimensional Gaussian ﬁlter.
Theorem 12.14. The fundamental solution
F(t, x; ξ) = F(t, x −ξ) = e−∥x−ξ ∥2/(4γ t)
8 (πγ t)3/2
(12.120)
solves the three-dimensional heat equation ut = γ Δu on R3 for t > 0, with an initial
temperature equal to a delta function concentrated at the point x = ξ.
Thus, the initially concentrated heat energy immediately begins to spread out in a
spherically symmetric manner, with a minuscule, but nonzero eﬀect that is felt immediately
arbitrarily far away from the initial concentration. At each individual point x ∈R3, after
an initial warm-up, the temperature decays back to zero at a rate proportional to t−3/2
— more rapidly than in two dimensions, because, intuitively, there are more directions in
which the heat energy can disperse.
To solve a more general initial value problem with the initial temperature distributed
over all of space, we ﬁrst write
u(0, x) = f(x) =
  
f(ξ) δ(x −ξ) dξ dη dζ
as a linear superposition of delta functions. By linearity, the solution to the initial value
problem is given by the corresponding superposition
u(t, x) =
1
8 (πγ t)3/2
  
f(ξ) e−∥x−ξ ∥2/(4γ t) dξ dη dζ
(12.121)
of the fundamental solutions. Since the fundamental solution has exponential decay as
∥x ∥→∞, the superposition formula is valid even for initial temperature distributions
that are moderately increasing at large distances. We remark that the integral (12.121)
has the form of a three-dimensional convolution
u(t, x) = F(t, x) ∗f(x) =
  
f(ξ) F(t, x −ξ) dξ dη dζ
(12.122)

12.5 The Wave Equation for Three–Dimensional Media
545
of the initial data with a one-parameter family of increasingly spread-out Gaussian ﬁlters.
Thus, as before, convolution with a Gaussian ﬁlter has a smoothing eﬀect on the initial
temperature distribution.
Exercises
12.4.30. True or false: In a three-dimensional medium, heat energy propagates at inﬁnite speed.
12.4.31. A solid spherical ball of radius 1 is heated to 100◦and inserted into a three-dimen-
sional medium ﬁlling the rest of R3 with uniform temperature 0◦.
(a) Write down an integral formula for the subsequent temperature distribution over R3 at
time t > 0, assuming a common diﬀusion coeﬃcient γ = 1.
(b) Evaluate the resulting integral using spherical coordinates.
12.4.32.(a) Prove that u(t, r) is a spherically symmetric solution to the three-dimensional heat
equation if and only if w(t, r) = r u(t, r) solves the one-dimensional heat equation: wt = wrr.
(b) True or false: If w(t, r) is the fundamental solution for the one-dimensional heat equa-
tion based at r = 0, then u(t, r) = w(t, r)/r is the fundamental solution for the three-
dimensional heat equation based at the origin.
12.4.33. Construct the solution to the initial value problem in Exercise 12.4.31 using radial
symmetry and Exercise 12.4.32.
♥12.4.34. Suppose that, as Earth orbits the sun, its surface is subject to yearly periodic tem-
perature variations a cos ω t, where the frequency ω is given by (4.56). (a) Assuming, for
simplicity, that the Earth is a homogeneous solid ball, of radius R, formulate an initial-
boundary value problem that governs the temperature ﬂuctuations within the Earth due
to its orbiting the sun. (b) At what depth does the temperature vary out of phase with the
surface, i.e., is the warmest in winter and coldest in summer? Compare your answer with
the root cellar computation at the end of Section 4.1. Hint: Use Exercise 12.4.32.
12.4.35.(a) Prove that if u(t, x) is any (suﬃciently smooth) solution to the heat equation, so is
its time derivative v = ∂u/∂t. (b) Write out the time derivative of the fundamental solu-
tion, and the initial value problem it satisﬁes.
12.4.36. Write down an explicit eigenfunction series for the fundamental solution F(t, x; ξ) to
the heat equation in a unit cube with thermal diﬀusivity γ = 1 that is subject to homoge-
neous Dirichlet boundary conditions.
12.4.37. Write down an explicit eigenfunction series for the fundamental solution F(t, x; ξ) to
the heat equation in a ball of radius 1 that has thermal diﬀusivity γ = 1 and is subject to
homogeneous Dirichlet boundary conditions.
♦12.4.38. Justify the statement that formula (12.119) provides a solution to the three-dimensional
heat equation.
12.4.39. Fill in the details of the proof of Theorem 12.14.
12.5 The Wave Equation for Three–Dimensional Media
The three-dimensional wave equation
utt = c2Δu = c2(uxx + uyy + uzz),
(12.123)

546
12 Partial Diﬀerential Equations in Space
in which c > 0 denotes the speed of light, governs the propagation of waves in a homoge-
neous isotropic three-dimensional medium, e.g., electromagnetic waves (light, X-rays, radio
waves, etc.) in empty space. In this context, while the electric and magnetic vector ﬁelds
E, B are intrinsically coupled by the more complicated system of Maxwell’s equations, each
individual component satisﬁes the wave equation; see Exercise 12.5.14 for details.
The wave equation also models certain restricted classes of vibrations of a uniform
solid body. The solution u(t, x) = u(t, x, y, z) represents a scalar-valued displacement of
the body at time t and position x = (x, y, z) ∈Ω ⊂R3.
For example, u(t, x) might
represent the radial displacement of the body. One imposes suitable boundary conditions,
e.g., Dirichlet, Neumann, or mixed, on ∂Ω, along with a pair of initial conditions
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
x ∈Ω,
(12.124)
that specify the body’s initial displacement and initial velocity. As long as the initial and
boundary data are reasonably nice, there exists a unique classical solution to the initial-
boundary value problem for all −∞< t < ∞, cf. [38, 61, 99]. Thus, in contrast to the
heat equation, one can follow solutions to the wave equation both forwards and backwards
in time.
Let us focus our attention on the homogeneous boundary value problem. The funda-
mental vibrational modes are found by imposing our usual trigonometric ansatz
u(t, x, y, z) = cos(ωt) v(x, y, z)
or
sin(ωt) v(x, y, z).
Substituting into the wave equation (12.123), we discover (yet again) that v(x, y, z) must
be an eigenfunction for the associated Helmholtz eigenvalue problem
Δv + λ v = 0,
where
λ = ω2
c2 ,
(12.125)
coupled to the relevant boundary conditions. In the positive deﬁnite cases, i.e., Dirichlet
and mixed boundary conditions, the eigenvalues λk = ω2
k/c2 > 0 are all positive. Each
eigenfunction vk(x, y, z) yields two normal vibrational modes
uk(t, x, y, z) = cos(ωkt) vk(x, y, z),
uk(t, x, y, z) = sin(ωkt) vk(x, y, z),
of frequency ωk = c
√
λk equal to the square root of the corresponding eigenvalue multiplied
by the wave speed. The general solution is a quasiperiodic linear combination,
u(t, x, y, z) =
∞

k=1

ak cos(ωkt) + bk sin(ωkt)

vk(x, y, z),
(12.126)
of the eigenmodes. The coeﬃcients ak, bk are uniquely prescribed by the initial conditions
(12.124). Thus,
u(0, x, y, z) =
∞

k=1
ak vk(x, y, z) = f(x, y, z),
∂u
∂t (0, x, y, z) =
∞

k=1
ωk bk vk(x, y, z) = g(x, y, z).

12.5 The Wave Equation for Three–Dimensional Media
547
The explicit formulas follow immediately from the orthogonality of the eigenfunctions:
ak = ⟨f , vk ⟩
∥vk ∥2 =
  
Ω
f vk dx dy dz
  
Ω
v2
k dx dy dz
,
bk = 1
ωk
⟨g , vk ⟩
∥vk ∥2 =
  
Ω
g vk dx dy dz
ωk
  
Ω
v2
k dx dy dz
.
(12.127)
In the positive semi-deﬁnite Neumann case, there is an additional zero eigenvalue
λ0 = 0 corresponding to the constant null eigenfunction v0(x, y, z) ≡1. This results in two
additional terms in the eigenfunction expansion — a constant term
a0 =
1
vol Ω
  
Ω
f(x, y, z) dx dy dz
that equals the average initial displacement, and an unstable mode b0 t that grows linearly
in time, whose speed
b0 =
1
vol Ω
  
Ω
g(x, y, z) dx dy dz
is the average initial velocity over the entire body. Thus, the unstable mode will be excited
if and only if there is a nonzero net initial velocity: b0 ̸= 0.
Most of the basic solution techniques we learned in the two-dimensional case apply
here, and we will not dwell on the details. The case of a rectangular box is a particularly
straightforward application of the method of separation of variables, and is outlined in the
exercises. A similar analysis, now in cylindrical coordinates, can be applied to the case of
a vibrating cylinder. The most interesting case is that of a solid spherical ball, which is
the subject of the next subsection.
Vibration of Balls and Spheres
Let us focus on the radial vibrations of a solid ball, as modeled by the three-dimensional
wave equation (12.123). The solution u(t, x, y, z) represents the radial displacement of the
“atom” that is situated at position (x, y, z) when the ball is at rest.
For simplicity, we look at the Dirichlet boundary value problem on the unit ball
B1 = {∥x ∥< 1}. The normal modes of vibration are governed by the Helmholtz equation
(12.125) subject to homogeneous Dirichlet boundary conditions. According to (12.110),
the eigenfunctions are
v0,m,n(r, ϕ, θ) = Sm(σm,n r) Y 0
m(ϕ, θ),
vk,m,n(r, ϕ, θ) = Sn(σn,m r) Y k
m(ϕ, θ),
vk,m,n(r, ϕ, θ) = Sm(σm,n r) Y k
m(ϕ, θ),
for
n = 1, 2, 3, . . . ,
m = 0, 1, 2, . . . ,
k = 1, 2, . . ., m.
(12.128)
Here Sm denotes the mth order spherical Bessel function (12.104), σm,n is its nth root, as
in (12.109), while Y m
n , Y m
n
are the spherical harmonics (12.38). Each eigenvalue
λm,n = σ2
m,n,
m = 0, 1, 2, . . .,
n = 1, 2, 3, . . .,
corresponds to 2m + 1 independent eigenfunctions, namely
vk,m,0(r, ϕ, θ), vk,m,1(r, ϕ, θ), . . . , vk,m,m(r, ϕ, θ), vk,m,1(r, ϕ, θ), . . . , vk,m,m(r, ϕ, θ).

548
12 Partial Diﬀerential Equations in Space
Consequently, the fundamental vibrational frequencies of a solid ball
ωm,n = c

λm,n = c σm,n,
m = 0, 1, 2, . . .,
n = 1, 2, 3, . . .,
(12.129)
are equal to the spherical Bessel roots σm,n multiplied by the wave speed.
There is a
total of 2(2m + 1) independent vibrational modes associated with each distinct frequency
(12.129), namely
u0,m,n(t, r, ϕ, θ) = cos(c σm,nt) Sm(σm,n r) Y 0
m(ϕ, θ),
u0,m,n(t, r, ϕ, θ) = sin(c σm,nt) Sm(σm,n r) Y 0
m(ϕ, θ),
uk,m,n(t, r, ϕ, θ) = cos(c σm,nt) Sm(σm,n r) Y k
m(ϕ, θ),
uk,m,n(t, r, ϕ, θ) = sin(c σm,nt) Sm(σm,n r) Y k
m(ϕ, θ),
uk,m,n(t, r, ϕ, θ) = cos(c σm,nt) Sm(σm,n r) Y k
m(ϕ, θ),
uk,m,n(t, r, ϕ, θ) = sin(c σm,nt) Sm(σm,n r) Y k
m(ϕ, θ),
n = 1, 2, 3, . . . ,
m = 0, 1, 2, . . . ,
k = 1, 2, . . ., m.
(12.130)
In particular, the radially symmetric modes of vibration have, according to (12.105), the
elementary form
u0,0,n(r, ϕ, θ) = cos(c nπ t) S0(nπr) = cos c nπ t sin nπr
r
,
u0,0,n(r, ϕ, θ) = sin(c nπ t) S0(nπr) = sin c nπ t sin nπr
r
,
n = 1, 2, 3, . . . .
(12.131)
Their vibrational frequencies, ω0,n = c nπ, are integral multiples of the lowest frequency
ω0,1 = cπ. Thus, intriguingly, if you excite only the radially symmetric modes, the resulting
motion of the ball is periodic. However, more general vibrations are only quasiperiodic.
Adopting the same scaling argument as in (11.166), we conclude that the fundamental
frequencies for a solid ball of radius R and wave speed c are given by ωm,n = c σm,n/R.
The relative vibrational frequencies
ωm,n
ω0,1
= σm,n
σ0,1
= σm,n
π
(12.132)
are independent of the size of the ball R or the wave speed c. In the accompanying table,
we display all relative vibrational frequencies that are less than 4 in magnitude.
Relative Spherical Bessel Roots σm,n/σ0,1
n
9
m
0
1
2
3
4
6
7
8
. . .
1
1.0000
1.4303
1.8346
2.2243
2.6046
2.9780
3.3463
3.7105
. . .
2
2.0000
2.4590
2.8950
3.3159
3.7258
...
...
...
3
3.0000
3.4709
3.9225
...
...
4
4.0000
...
...
...
...

12.5 The Wave Equation for Three–Dimensional Media
549
The purely radial modes of vibration (12.131) have individual frequencies
ω0,n = nπ c
R
,
so
ω0,n
ω0,1
= n,
which appear in the ﬁrst column of the table. The lowest frequency is ω0,1 = π c/R, cor-
responding to a vibration with period 2π/ω0,1 = 2R/c. In particular, for the Earth, the
radius R ≈6000 km, and the wave speed in rock is, on average, c ≈5 km/sec, so that the
fundamental mode of vibration has period 2 R/c ≈2400 seconds, or 40 minutes. Of course,
we have suppressed almost all interesting terrestrial geology in this very crude approxima-
tion, which has been based on the assumption that the Earth is a uniform spherical body,
globally vibrating only in its radial direction. A more realistic modeling of the vibrations of
the Earth requires an understanding of the basic partial diﬀerential equations of linear and
nonlinear elastodynamics, [7, 49]. Nonuniformities in the Earth lead to scattering of the
vibrational waves, which are then used to locate subterranean geological structures, e.g., oil
and gas deposits. Localized vibrations of the Earth are also known as seismic waves, and,
of course, earthquakes are their most severe manifestation. We refer the interested reader
to [5] for an introduction to mathematical seismology. Understanding terrestrial vibrations
is an issue of critical importance in geophysics and civil engineering, including the design
of structures, buildings, and bridges, requiring the avoidance of potentially catastrophic
resonant frequencies.
Example 12.15. The radial vibrations of a hollow thin spherical shell (e.g., an elastic
balloon) are governed by the diﬀerential equation
∂2u
∂t2 = c2 ΔS[u] = c2
 ∂2u
∂ϕ2 + cos ϕ
sin ϕ
∂u
∂ϕ +
1
sin2 ϕ
∂2u
∂θ2

,
(12.133)
where ΔS denotes the spherical Laplacian (12.19). The radial displacement u(t, ϕ, θ) of a
point on the sphere depends only on time t and the angular coordinates ϕ, θ. The solution
u(t, ϕ, θ) is required to be 2π–periodic in the azimuthal angle θ and bounded at the poles,
where ϕ = 0 and π.
According to (12.38), the nth eigenvalue of the spherical Laplacian, λn = n(n + 1),
possesses 2n + 1 linearly independent eigenfunctions, namely, the spherical harmonics
Y 0
n (ϕ, θ),
Y 1
n (ϕ, θ),
. . . ,
Y n
n (ϕ, θ),
Y 1
n (ϕ, θ),
. . . ,
Y n
n (ϕ, θ).
As a consequence, the fundamental frequencies of vibration for a spherical shell are
ωn = c

λn = c

n(n + 1) ,
n = 1, 2, . . . .
(12.134)
The vibrational solutions are quasiperiodic combinations of the fundamental spherical har-
monic modes
cos

n(n + 1) t

Y m
n (ϕ, θ),
sin

n(n + 1) t

Y m
n (ϕ, θ),
cos

n(n + 1) t
 Y m
n (ϕ, θ),
sin

n(n + 1) t
 Y m
n (ϕ, θ).
(12.135)
Representative graphs can be seen in Figure 12.5.
The smallest positive eigenvalue is
λ1 = 2, yielding a lowest tone of frequency ω1 = c
√
2. The higher-order frequencies are
irrational multiples of the fundamental frequency, implying that a vibrating spherical bell
sounds dissonant to our ears.
One further remark is in order. The spherical Laplacian operator is only positive semi-
deﬁnite, since the lowest mode has eigenvalue λ0 = 0, which corresponds to the constant

550
12 Partial Diﬀerential Equations in Space
null eigenfunction v0(ϕ, θ) = Y 0
0 (ϕ, θ) ≡1. Therefore, the wave equation (12.133) admits
an unstable mode b0,0 t, corresponding to a uniform radial inﬂation; its coeﬃcient
b0,0 = 3
4π
 
S1
∂u
∂t (0, ϕ, θ) dS
represents the shell’s average initial velocity. The existence of such an unstable mode is an
artifact of the simpliﬁed linear model we are using, which fails to account for nonlinearly
elastic eﬀects that serve to constrain the inﬂation of a spherical balloon.
Exercises
12.5.1. Find the eigenfunction series solution to the initial-boundary value problem for the
wave equation utt = Δu on a unit cube C = {0 < x, y, z < 1}, subject to homogeneous
Dirichlet boundary conditions and one of the following sets of initial conditions:
(a) u(0, x, y, z) = 1, ut(0, x, y, z) = 0; (b) u(0, x, y, z) = 0, ut(0, x, y, z) = 1;
(c) u(0, x, y, z) = sin πx sin πy sin πz, ut(0, x, y, z) = 0; (d) u(0, x, y, z) = sin 3πx,
ut(0, x, y, z) = sin 2πy; (e) u(0, x, y, z) = 0, ut(0, x, y, z) = xy z (1 −x)(1 −y)(1 −z).
12.5.2. Suppose the cube in Exercise 12.5.1 is subject to homogeneous Neumann boundary con-
ditions. Which of the preceding initial value problems leads to an unstable motion of the
cube?
12.5.3.(a) Find the separable periodic vibrations of a unit cube subject to homogeneous Dirich-
let boundary conditions.
(b) Can you ﬁnd a periodic mode that is not separable?
12.5.4. Answer Exercise 12.5.3 when one face of the cube is left free, while the other ﬁve faces
are ﬁxed.
12.5.5. Given a material with wave speed c = 1.5 cm/sec, ﬁnd the natural vibrational frequen-
cies of a solid rectangular box of size 1 cm × 2 cm × 3 cm whose sides are held ﬁxed. List
the lowest ﬁve such frequencies in order. Does the box vibrate periodically?
12.5.6. Find the natural vibrational frequencies of a solid cylinder of height 2, radius 1, and
wave speed c = 1, when (a) all sides are ﬁxed; (b) the top and bottom of the cylinder are
free, while the curved side is ﬁxed; (c) the curved side of the cylinder is free, while the top
and bottom are ﬁxed.
12.5.7. Among all solid cylinders of unit volume with ﬁxed boundary, ﬁnd the one that vibrates
the slowest.
12.5.8. Does a solid spherical ball that is subject to homogeneous Neumann boundary condi-
tions vibrate (i) faster, (ii) slower, or (iii) at the same rate as the same ball subject to
homogeneous Dirichlet conditions. If your answer is (i) or (ii), estimate how much faster or
slower.
12.5.9. A solid cube and solid sphere are made of the same material and have the same volume.
Which vibrates faster when subject to homogeneous Dirichlet boundary conditions?
12.5.10. Assuming that they both have the same wave speed and ﬁxed boundaries, which vi-
brates faster: a solid sphere or a circular membrane of the same radius?
12.5.11. A uniform, solid spherical planet is ﬂoating freely in outer space. Find its three slow-
est resonant frequencies.
12.5.12. True or false: Suppose we have two uniform solid bodies composed of the same ma-
terial. If the ﬁrst body cools down to thermal equilibrium the fastest, then it also vibrates
the fastest. Explain your answer.

12.6 Spherical Waves and Huygens’ Principle
551
12.5.13.(a) Deﬁne what is meant by a nodal curve and a nodal region on a vibrating thin spher-
ical shell.
(b) True or false: All the nodal curves are arcs of circles.
♥12.5.14. The propagation of electromagnetic waves (including light) is governed by the electric
ﬁeld E(t, x) and magnetic ﬁeld B(t, x), which are both time-dependent vector ﬁelds deﬁned
for x = (x, y, z) in a domain Ω ⊂R3. In empty space, Maxwell’s equations (as formulated
by Heaviside) are
∇· E = 0,
∇· B = 0,
∂B
∂t = −∇× E,
∂E
∂t =
1
μ0 ϵ0
∇× B,
(12.136)
where μ0, ϵ0 are, respectively, the permeability and permittivity constants. Prove that all in-
dividual components of E and B satisfy the scalar wave equation. What is the wave speed,
i.e., the speed of light in empty space?
12.6 Spherical Waves and Huygens’ Principle
For any dynamical partial diﬀerential equation, the fundamental solution measures the
eﬀect of applying an instantaneous concentrated unit impulse at a single point.
Two
representative physical eﬀects to keep in mind are the light waves emanating from a sudden
concentrated blast, e.g., a lightning bolt or a stellar supernova, and the sound waves
due to an explosion or thunderclap, propagating in air at a much slower speed. Linear
superposition utilizes the fundamental solution to build up more general solutions to initial
value problems. For the wave and other second-order vibrational equations, the impulse
can be applied either to the initial displacement or to the initial velocity, resulting in two
distinct types of fundamental solution. The general solution to the initial value problem will
be obtained by a double superposition. In this section, we derive explicit formulas for the
two fundamental solutions for the three-dimensional wave equation on all of space, leading
to Kirchhoﬀ’s formula for the solution to the general initial value problem. An important
consequence is Huygens’ Principle, which states that, in three-dimensional space, localized
initial disturbances remain localized as they propagate. In the ﬁnal subsection, we apply
the method of descent to our three-dimensional solution formulas in order to solve the
two-dimensional wave equation, for which, surprisingly, Huygens’ Principle is no longer
valid.
Spherical Waves
In a uniform isotropic medium, an initial concentrated blast results in a spherically ex-
panding wave, moving away at the speed of light (or sound) in all directions. Invoking
translation invariance, we will assume, without loss of generality, that the source of the
disturbance is at the origin, and so the solution u(t, x) should depend only on the distance
r = ∥x ∥from the source. We adopt spherical coordinates and look for a solution u = u(t, r)
to the three-dimensional wave equation (12.123) with no angular dependence. Substituting
the formula (12.16) for the spherical Laplacian and setting both angular derivatives to 0,
we are led to the partial diﬀerential equation
∂2u
∂t2 = c2
 ∂2u
∂r2 + 2
r
∂u
∂r

,
(12.137)

552
12 Partial Diﬀerential Equations in Space
which governs the propagation of spherically symmetric waves in three-dimensional space.
Surprisingly, we can explicitly solve (12.137). The secret is to multiply both sides of the
equation by r:
∂2(ru)
∂t2
= r ∂2u
∂t2 = c2

r ∂2u
∂r2 + 2 ∂u
∂r

= c2 ∂2
∂r2 (r u).
Thus, the function
w(t, r) = r u(t, r)
satisﬁes the one-dimensional wave equation
∂2w
∂t2 = c2 ∂2w
∂r2 .
(12.138)
According to Theorem 2.14, the general solution to the one-dimensional wave equa-
tion (12.138) can be written in d’Alembert form
w(t, r) = p(r −ct) + q(r + ct),
where p(ξ) and q(η) are arbitrary functions of a single characteristic variable. Therefore,
spherically symmetric solutions to the three-dimensional wave equation assume the form
u(t, r) = p(r −ct)
r
+ q(r + ct)
r
.
(12.139)
The ﬁrst summand,
u(t, r) = p(r −ct)
r
,
(12.140)
represents a wave moving at speed c in the direction of increasing r, and so describes the
illumination from a variable light source that is concentrated at the origin, e.g., a pulsating
quasar in interstellar space. To highlight this interpretation, let us concentrate on the case
that p(ξ) = δ(ξ −a) is a delta function, keeping in mind that more general solutions can
then be assembled by linear superposition. The induced solution
u(t, r) = δ(r −ct −a)
r
= δ

r −c(t −t0)

r
,
where
t0 = −a
c ,
(12.141)
represents a spherical wave propagating through space. At the instant t = t0, the light is
entirely concentrated at the origin, r = 0. The signal then moves away from the origin
in all directions at speed c. At each later time t > t0, the wave remains concentrated on
the surface of a sphere of radius r = c (t −t0). Its intensity at each point on the sphere,
however, has decreased by a factor 1/r, and so, the farther the light travels away from the
source, the dimmer it becomes. A stationary observer sitting at a ﬁxed point in space will
see only an instantaneous ﬂash of light of intensity 1/r as the spherical wave passes by
at time t = t0 + r/c, where r is the observer’s distance from the light source. A similar
statement holds for sound waves — to an observer, the sound of a distant explosion will
last momentarily. Thunder and lightning are the most familiar examples of this everyday
phenomenon.
On the other hand, for t < t0, the impulse is concentrated at a negative radius r =
c (t −t0) < 0. To interpret this, note that, for spherical coordinates (12.15), replacing r
by −r has the same eﬀect as changing x to the antipodal point −x. Thus, the solution
(12.141) represents a concentrated spherically symmetric light wave arriving from the edges

12.6 Spherical Waves and Huygens’ Principle
553
of the universe at speed c that strengthens in intensity as it collapses into the origin at
t = t0. After collapse, it immediately reappears and expands back out into the universe.
The second solution in the d’Alembert formula (12.139) has, in fact, exactly the same
physical form under the antipodal identiﬁcation. Indeed, if we set
r = −r,
p(ξ) = −q(−ξ),
then
q(r + ct)
r
= p(r −ct)
r
.
Thus, the second d’Alembert solution is redundant, and we only need to consider solutions
of the form (12.140) from now on.
To eﬀectively utilize such spherical wave solutions, we need to understand the nature
of their originating singularity. For simplicity, we set t0 = 0 in (12.141) and concentrate
on the particular solution
u(t, r) = δ(r −ct)
r
,
(12.142)
which apparently has a bad singularity at the origin, r = 0, at the initial time t = 0. We
need to pin down precisely which sort of distribution (generalized function) it represents.
Invoking the limiting deﬁnition is tricky, and it will be easier to work with the dual char-
acterization of a distribution as a linear functional. Thus, at a ﬁxed time t ≥0, we must
evaluate the inner product†
⟨u(t, ·) , f ⟩=
  
u(t, x, y, z) f(x, y, z)dx dy dz
of the solution with a smooth test function f(x) = f(x, y, z). We rewrite the triple integral
in spherical coordinates, whereby
⟨u(t, ·) , f ⟩=
 π
−π
 π
0
 ∞
0
δ(r −ct)
r
f(r, ϕ, θ) r2 sin ϕ dr dϕ dθ.
When t ̸= 0, the r integration can be immediately evaluated, and so
⟨u(t, ·) , f ⟩= c t
 π
−π
 π
0
f(c t, ϕ, θ) sin ϕ dϕ dθ = 4πc t Mct [ f ] ,
(12.143)
where
Mct [ f ] = 1
4π
 π
−π
 π
0
f(c t, ϕ, θ) sin ϕ dϕ dθ =
1
4πc2 t2
 
Sc t
f dS
(12.144)
is the mean or average value of the function f on the sphere Sct =
-
∥x ∥= c t
.
of radius
r = c t and, hence, surface area 4πc2t2. In particular, in the limit as the sphere’s radius
c t →0, by continuity, the mean reduces to just the value of the function at the origin:
lim
t →∞Mct [ f ] = M0 [ f ] = f(0).
(12.145)
Thus, (12.143) implies that
lim
t →∞⟨u(t, ·) , f ⟩= ⟨u(0, ·) , f ⟩= 0
for all functions
f,
†
For ﬁxed t, we use u(t, ·) to indicate the real-valued function (x, y, z) →u(t, x, y, z) on R3.

554
12 Partial Diﬀerential Equations in Space
and hence u(0, x, y, z) ≡0 represents a zero initial displacement. In other words, there is,
in fact, no singularity in the solution at t = 0!
In the absence of any initial displacement, how, then, can the solution (12.142) be
nonzero? Clearly, this must be the result of a nonzero initial velocity. To evaluate ∂u/∂t,
we diﬀerentiate (12.143), whereby
4 ∂u
∂t , f
5
= ∂
∂t ⟨u(t, ·) , f ⟩= ∂
∂t

c t
 π
−π
 π
0
f(c t, ϕ, θ) sin ϕ dϕ dθ

= c
 π
−π
 π
0
f(c t, ϕ, θ) sin ϕ dϕ dθ + c2 t
 π
−π
 π
0
∂f
∂r (c t, ϕ, θ) sin ϕ dϕ dθ
= 4πc Mct [ f ] + 4πc2 t Mct
 ∂f
∂r

.
(12.146)
The result is a linear combination of the means of f and its radial derivative fr over the
sphere of radius c t. In the limit, the second term goes to 0, and so, by (12.145),
lim
t →0 ⟨ut , f ⟩= 4πc M0 [ f ] = 4πc f(0).
Since this holds for all test functions f, we conclude that the initial velocity of our solution
is a multiple of a delta function at the origin:
ut(0, r) = 4πc δ(x).
Dividing through by 4πc, we ﬁnd that the spherical expanding wave
u(t, r) = δ(r −c t)
4πc r
(12.147)
solves the initial value problem
u(0, x) ≡0,
∂u
∂t (0, x) = δ(x),
corresponding to an initial unit-velocity impulse concentrated at the origin. This solution
can be viewed as the three-dimensional version of the hammer-blow solution to the one-
dimensional wave equation discussed in Exercise 6.3.28.
More generally, we use the translational symmetry of the wave equation to conclude
that the function
G(t, x; ξ) = δ

∥x −ξ ∥−c t

4πc ∥x −ξ ∥
,
t ≥0,
(12.148)
is the fundamental solution to the wave equation resulting from a unit-velocity impulse
concentrated at the point ξ at the initial time t = 0:
G(0, x; ξ) = 0,
∂G
∂t (0, x; ξ) = δ(x −ξ).
(12.149)
With this in hand, we can apply linear superposition to solve the zero initial displacement
initial value problem
u(0, x, y, z) = 0,
∂u
∂t (0, x, y, z) = g(x, y, z).
(12.150)

12.6 Spherical Waves and Huygens’ Principle
555
1
t
r
α
x
S x
t
B1
Figure 12.9.
Cross-section of a sphere intersecting a ball.
Namely, we write the initial velocity
g(x) =
  
g(ξ) δ(x −ξ) dξ dη dζ
as a superposition of impulses, and immediately conclude that the relevant solution is the
selfsame superposition of spherical waves:
u(t, x) =
1
4πc
  
g(ξ) δ

∥x −ξ ∥−c t

∥x −ξ ∥
dξ dη dζ
=
1
4πc2 t
 
∥ξ−x ∥=ct
g(ξ) dS = t M x
ct [ g ] .
(12.151)
Thus, the value of our solution at position x and time t > 0 is equal to t times the mean
of the initial velocity function g over the sphere of radius r = c t centered at the point x.
Example 12.16. Let us set the wave speed c = 1. Suppose that the initial velocity
g(x) =
 1,
∥x ∥< 1,
0,
∥x ∥> 1,
is 1 inside the unit ball B1 centered at the origin and 0 outside. To solve the corresponding
initial velocity problem, we must compute the average value of g over a sphere
S x
t = { ξ | ∥ξ −x ∥= t }
of radius t > 0 centered at a point x ∈R3. Since g = 0 outside the unit ball, its average
will be equal to the surface area of that part of the sphere that is contained inside the unit
ball, namely S x
t ∩B1, divided by the total surface area of S x
t , namely 4π t2.
To compute this quantity, let r = ∥x ∥. If t > r + 1 or 0 < t < r −1, then the sphere
of radius t lies entirely outside the unit ball, and so the average is 0; if 0 < t < 1−r, which
requires r < 1 and so x ∈B1, then the sphere lies entirely within the unit ball, and so the
average is 1. Otherwise, referring to Figure 12.9 and Exercise 12.6.7, we see that the area
of the spherical cap S x
t ∩B1 is given by
2π t2(1 −cos α) = 2π t2

1 −r2 + t2 −1
2 r t

= π t
r [1 −(t −r)2 ] ,
(12.152)

556
12 Partial Diﬀerential Equations in Space
where α denotes the angle between the line joining the centers of the two spheres and
the circle formed by their intersection, whose value is prescribed by the Law of Cosines.
Assembling the diﬀerent subcases, we conclude that
M x
ct [ g ] =
⎧
⎪
⎨
⎪
⎩
1,
0 ≤t ≤1 −r,
1 −(t −r)2
4 r t
,
| r −1 | ≤t ≤r + 1,
0,
0 ≤t ≤r −1
or
t ≥r + 1.
(12.153)
The solution (12.151) is obtained by multiplying by t, and hence for t ≥0,
u(t, x) =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
t,
0 ≤t ≤1 −∥x ∥,
1 −

t −∥x ∥
2
4 ∥x ∥
,
 ∥x ∥−1
 ≤t ≤∥x ∥+ 1,
0,
0 ≤t ≤∥x ∥−1
or
t ≥∥x ∥+ 1.
(12.154)
The resulting function is not smooth at the interfaces t =
 ∥x ∥−1
 and ∥x ∥+ 1, and
hence does not qualify as a classical solution. Nevertheless, it can be shown that (12.154)
is a bona ﬁde weak solution to the initial value problem.
The ﬁrst two rows of Figure 12.10 plot the solution as a function of time for several
ﬁxed values of r = ∥x ∥. An observer sitting at the origin will see a linearly increasing
light intensity followed by a sudden blackout. At other points inside the sphere, there
is a similar linear increase, while the subsequent decrease follows a parabolic arc; if the
observer is closer to the edge of the ball than its center, the parabolic portion will continue
to increase for a while before eventually tapering oﬀ. On the other hand, an observer sitting
outside the sphere will experience, after an initially dark period, a symmetric parabolic
increase to a maximal intensity and then a decrease back to dark after a total time lapse
of 2. The second two rows plot the solution as a function of r for various ﬁxed times.
Note that, up until time t = 1, the light spreads out while increasing in intensity near the
origin, after which the solution is of gradually decreasing magnitude, supported within the
domain lying between two concentric spheres of respective radii t −1 and t + 1.
Returning to the general situation, we note that the solution formula (12.151) han-
dles only nonzero initial velocities. What about solutions resulting from a nonzero initial
displacement? Surprisingly, the answer comes from diﬀerentiation! The key observation is
that if u(t, x) is any (suﬃciently smooth) solution to the wave equation, then so is its time
derivative
v(t, x) = ∂u
∂t (t, x).
(12.155)
This follows at once from diﬀerentiating both sides of the wave equation with respect to t
and using the equality of mixed partial derivatives. Physically, this implies that the velocity
of a wave obeys the same evolutionary principle as the wave itself, which is a manifestation
of the linearity and time-independence (autonomy) of the equation.
Now, suppose u has initial conditions
u(0, x) = f(x),
ut(0, x) = g(x).
(12.156)
What are the initial conditions for its derivative v = ut? Clearly, its initial displacement
v(0, x) = ut(0, x) = g(x)
(12.157)

12.6 Spherical Waves and Huygens’ Principle
557
r = 0
r = .3
r = .7
r = 1.3
t = 0
t = .5
t = .9
t = 1.0
t = 1.1
t = 1.5
Figure 12.10.
Wave equation solution u(t, r) due to
an initial velocity of the unit ball.

equals the initial velocity of u. As for its initial velocity, we have
∂v
∂t = ∂2u
∂t2 = c2 Δu,
because we are assuming that u solves the wave equation. Thus, at the initial time, the
velocity,
∂v
∂t (0, x) = c2 Δu(0, x) = c2 Δf(x),
(12.158)
equals c2 times the Laplacian of the initial displacement f. In particular, if u satisﬁes the
initial conditions
u(0, x) = 0,
ut(0, x) = g(x),
(12.159)
then v = ut satisﬁes the initial conditions
v(0, x) = g(x),
vt(0, x) = 0.
(12.160)
Thus, paradoxically, to solve the initial displacement problem we diﬀerentiate the initial
velocity solution (12.151) with respect to t, and hence
v(t, x) = ∂u
∂t (t, x) = ∂
∂t

t M x
ct [ g ]

= M x
ct [ g ] + c t M x
ct
 ∂g
∂n

,
(12.161)

558
12 Partial Diﬀerential Equations in Space
where we have made use of our computation in (12.146). Therefore, v(t, x) is a linear
combination of the mean of the function g and the mean of its normal or radial derivative
∂g/∂n = ∂g/∂r, taken over a sphere of radius c t centered at the point x. In particular, to
obtain the solution corresponding to a concentrated initial displacement,
F(0, x; ξ) = δ(x −ξ),
∂F
∂t (0, x; ξ) = 0,
(12.162)
we diﬀerentiate the solution (12.148), resulting in
F(t, x; ξ) = ∂G
∂t (t, x; ξ) = −δ ′
∥x −ξ ∥−ct

4π ∥ξ −x ∥
,
(12.163)
which is the fundamental solution for the initial displacement problem. Thus, interestingly,
a concentrated initial displacement spawns a spherically expanding doublet, cf. Figure 6.6,
whereas a concentrated initial velocity spawns an expanding spherical singlet or delta wave.
Example 12.17. Let c = 1. Consider the initial conditions
u(0, x) = f(x) =

1,
∥x ∥< 1,
0,
∥x ∥> 1,
∂u
∂t (0, x) = 0,
(12.164)
modeling the eﬀect of an instantaneously illuminated solid ball. To obtain the resulting
solution, we diﬀerentiate (12.154) with respect to t, leading to
u(t, x) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
1,
0 ≤t < 1 −∥x ∥,
∥x ∥−t
2 ∥x ∥,
 ∥x ∥−1
 ≤t ≤∥x ∥+ 1,
0,
0 ≤t < ∥x ∥−1
or
t > 1 + ∥x ∥.
(12.165)
As illustrated in the ﬁrst two rows of Figure 12.11, an observer sitting at the center of
the ball will see a constant light intensity until t = 1, at which time the solution suddenly
goes dark. At other points inside the ball, 0 < r < 1, the downwards jump in intensity
arrives sooner, and even goes below 0, followed by a further linear decrease, and ﬁnally
a jump back to quiescence. An observer placed outside the ball, at radius r = ∥x ∥> 1,
will experience, after an initially dark period, a sudden increase in the light intensity at
time t = r −1, followed by a linear decrease to negative, followed by a jump back up to
darkness at time t = r + 1. The farther away from the source, the fainter the light. In
the second two rows, we plot the same solution as a function of r for diﬀerent values of
t. Note the sudden appearance of a 1/r singularity at the origin at time t = 1, which is
the result of a focusing of the initial discontinuities of u(0, x) = f(x) on the surface of the
unit sphere. Afterwards, the residual radially symmetric disturbance moves oﬀto ∞while
gradually decreasing in intensity. Again, the discontinuities imply that (12.165) is not a
classical solution, but it does qualify as a weak solution to the initial value problem.
Kirchhoﬀ’s Formula and Huygens’ Principle
Linearly combining the two solution formulas (12.151) and (12.161) establishes Kirch-
hoﬀ’s formula (ﬁrst discovered by Poisson), which is the three-dimensional counterpart to
d’Alembert’s solution formula for the wave equation.

12.6 Spherical Waves and Huygens’ Principle
559
r = 0
r = .3
r = .7
r = 1.3
t = 0
t = .5
t = .8
t = 1.0
t = 1.25
t = 1.8
Figure 12.11.
Wave equation solution u(t, r) due to
an initial displacement of the unit ball .


560
12 Partial Diﬀerential Equations in Space
Theorem 12.18. The solution to the initial value problem
utt = c2 Δu,
u(0, x) = f(x),
∂u
∂t (0, x) = g(x),
x ∈R3,
(12.166)
for the wave equation in three-dimensional space is given by
u(t, x) = ∂
∂t

t M x
ct [ f ]

+ t M x
ct [ g ] = M x
ct [ f ] + c t M x
ct
 ∂f
∂n

+ t M x
ct [ g ] , (12.167)
where M x
ct [ f ] denotes the average of the function f over the sphere S x
ct = {∥ξ −x ∥= c t}
of radius c t centered at the point x.
A crucially important consequence of the Kirchhoﬀsolution formula is a celebrated
physical principle ﬁrst set out by the pioneering seventeenth century Dutch scientist Chris-
tiaan Huygens.†
Roughly, Huygens’ Principle states that, in three-dimensional space,
localized solutions to the wave equation remain localized. More concretely, (12.167) im-
plies that the value of the solution at a point x and time t depends only on the values of
the initial displacements and velocities at a distance c t away. Thus, all signals propagate
along the relativistic light cone
c2 t2 = x2 + y2 + z2
in four-dimensional Minkowski space-time. Physically, Huygens’ Principle assures us that
any light that we witness at time t arrived from points that lie a distance exactly d =
c(t −t0) away at an earlier time t0 < t. In particular, a localized initial signal, whether
initial displacement or initial velocity, that is concentrated near a point produces a response
that remains concentrated on an ever expanding sphere surrounding the point. In our three-
dimensional universe, we witness the light from a sudden explosion or lightning bolt for only
a brief moment, after which the view returns to darkness. Similarly, a sharp sound, e.g.,
a thunderclap, remains sharply concentrated with diminishing magnitude as it propagates
through space. Huygens’ Principle is responsible for the important astronomical fact that
the light we now observe from a distant star was generated at a single past time that is
directly proportional to the star’s distance from the Earth. Remarkably, as we will show in
the following subsection, Huygens’ Principle does not hold in a two-dimensional universe!
There, initially concentrated light and sound impulses will spread out as time progresses,
and their eﬀect will be experienced over an extended time range; see below for details.
Exercises
12.6.1. Solve the wave equation in three-dimensional space for the following initial conditions:
(a) u(0, x, y, z) = x + z, ut(0, x, y, z) = 0; (b) u(0, x, y, z) = 0, ut(0, x, y, z) = y;
(c) u(0, x, y, z) = 1/(1 + x2 + y2 + z2), ut(0, x, y, z) = 0,
(d) u(0, x, y, z) = 0, ut(0, x, y, z) = 1/(1 + x2 + y2 + z2).
12.6.2. At what points in space-time does a three-dimensional wave vanish if it vanishes out-
side a sphere of radius R at the initial time t = 0?
†
Don’t even bother trying to pronounce his name correctly unless you are Dutch!

12.6 Spherical Waves and Huygens’ Principle
561
12.6.3. Consider the initial value problem
∂2u
∂t2 = ∂2u
∂x2 + ∂2u
∂y2 + ∂2u
∂z2 ,
u(0, x, y, z) = 0,
∂u
∂t (0, x, y, z) =
 1,
0 < x, y, z < 1,
0,
otherwise,
i.e., the initial velocity is 1 inside a unit cube and 0 outside the cube. We interpret the so-
lution u(t, x, y, z) as the intensity of light at a given point in space-time, measured in units
that make the speed of light c = 1. (a) Write down an integral formula for u(t, x, y, z).
(b) Suppose a light sensor is placed at the point (2, 2, 1). For which values of t > 0 will
the sensor register a nonzero signal? Sketch a rough graph of what the sensor measures.
(You do not need to ﬁnd the precise formula, but explain how you obtained your graph.)
(c) True or false: The solution u(t, x, y, z) ≥0 at all points in space-time.
12.6.4. Is (12.151) a solution to the wave equation for t < 0? If not, write down a solution
formula that is valid for negative t.
12.6.5. True or false: The function u(t, x, y, z) deﬁned by (12.154) is everywhere continuous.
12.6.6. A thermonuclear explosion occurs at the center of the Earth. Would you feel the eﬀect
ﬁrst through a motion at the surface or a change in temperature at the surface? Discuss.
♦12.6.7. Prove that the area of the spherical cap S x
t ∩B1 is given by formula (12.152).
Descent to Two Dimensions
So far, we have found explicit formulas for the solution to the wave equation on the one-
dimensional line, and in three-dimensional space. The two-dimensional case
utt = c2Δu = c2(uxx + uyy)
(12.168)
is, counterintuitively, more complicated! For instance, seeking a radially symmetric solution
u(t, r) requires solving the partial diﬀerential equation
∂2u
∂t2 = c2
 ∂2u
∂r2 + 1
r
∂u
∂r

,
(12.169)
which, unlike its three-dimensional counterpart (12.137), is not so easily integrated.
However, our solution to the three-dimensional problem can be adapted to construct
a solution to the two-dimensional problem using the so-called Method of Descent. Observe
that any solution u(t, x, y) to the two-dimensional wave equation (12.168) can be viewed
as a solution to the three-dimensional wave equation (12.123) that does not depend upon
the vertical z coordinate, whence ∂u/∂z = 0. Clearly, if the three-dimensional initial data
does not depend on z, then the resulting solution u(t, x, y) will also be independent of z.
Consider ﬁrst the zero initial displacement condition
u(0, x, y) = 0,
∂u
∂t (0, x, y) = g(x, y).
(12.170)
In the three-dimensional solution formula (12.151), if g(x, y) does not depend on the z–
coordinate, then the integrals over the upper and lower hemispheres
S+
ct =
-
∥ξ −x ∥= c t,
ζ ≥z
.
,
S−
ct =
-
∥ξ −x ∥= c t,
ζ ≤z
.
,

562
12 Partial Diﬀerential Equations in Space
are identical. To evaluate these integrals, we parametrize the upper hemisphere as the
graph of
ζ = z+

c2 t2 −(ξ −x)2 −(η −y)2
over the disk
D x
ct =
-
(ξ −x)2 + (η −y)2 ≤c2 t2 .
,
concluding that
u(t, x, y) =
1
4πc2t
 
Sc t
g(ξ, η) dS =
1
2πc2 t
 
S+
c t
g(ξ, η) dS
=
1
2πc
 
D x
c t
g(ξ, η)

c2 t2 −(ξ −x)2 −(η −y)2 dξ dη
(12.171)
solves the initial value problem (12.170). In particular, if we take the initial velocity
∂u
∂t (0, x, y) = g(x, y) = δ(x) δ(y)
to be a unit impulse concentrated at the origin, then the resulting solution is
u(t, x, y) =
⎧
⎨
⎩
1
2πc

c2 t2 −x2 −y2 ,
x2 + y2 < c2 t2,
0,
x2 + y2 > c2 t2.
(12.172)
An observer sitting at distance r = ∥x ∥=

x2 + y2 from the origin will ﬁrst witness
a concentrated displacement singularity at time t = r/c.
However, in contrast to the
three-dimensional solution, even after the impulse passes by, there will continue to be a
decreasing, but nonzero, signal of magnitude roughly proportional to 1/t. In Figure 12.12,
we plot the solution (12.172) for unit wave speed c = 1. The ﬁrst row plots intensity as a
function of t at three diﬀerent radii; note that the initial singularity, indicated by a spike in
the graph, is followed by a progressively smaller residual displacement, which never entirely
disappears. The second row shows the displacement at three diﬀerent times as a function
of r = ∥x ∥.
As in the three-dimensional case, the solution to the initial displacement conditions
u(0, x, y) = f(x, y),
∂u
∂t (0, x, y) = 0,
(12.173)
can then be obtained by diﬀerentiation of (12.171) with respect to t, and so
u(t, x, y) =
1
2πc
∂
∂t
 
D x
c t
f(ξ, η)

c2 t2 −(ξ −x)2 −(η −y)2 dξ dη.
(12.174)
As before, starting with a concentrated impulse, an observer will witness, after a time
lapse t = r/c, an abrupt impulse passing by, followed by a progressively decaying residual
wave. The general solution to the two-dimensional wave equation on all of R2 is a linear
combination of these two types of solutions (12.171, 174).
As a consequence of these considerations, we discover that Huygens’ Principle is not
valid in a two-dimensional universe. The solution to the two-dimensional wave equation
at a point x at time t depends on the initial displacement and velocity on the entire disk
of radius c t centered at the point, and not just on the points lying a distance c t away.
So a two-dimensional creature would experience not just an initial eﬀect of a concentrated
sound or light wave, but also an “afterglow” of slowly diminishing magnitude. It would be

12.6 Spherical Waves and Huygens’ Principle
563
r = .5
r = 1
r = 1.5
t = .5
t = 1
t = 2
Figure 12.12.
Solution to the two-dimensional wave equation

for a concentrated impulse.
like living in a permanent echo chamber, and so understanding and acting upon sensory
phenomena would be considerably more challenging. In general, it can be proved that
Huygens’ Principle for the wave equation is valid only in spaces of odd dimension n =
2k + 1 ≥3; see also [15] for recent advances in the classiﬁcation of partial diﬀerential
equations that admit a Huygens’ Principle.
Remark: Since the solutions to the two-dimensional wave equation can be interpreted
as three-dimensional solutions with no z dependence, a concentrated delta impulse for the
two-dimensional wave equation would correspond to a three-dimensional initial impulse
that is concentrated along an entire vertical line, e.g., an instantaneous lightning bolt in
the form of an inﬁnite straight line. An observer ﬁxed in space will ﬁrst encounter the
light ﬂash arriving from the closest point on the line, but will subsequently experience the
gradually decreasing eﬀect of the light emitted by points that lie progressively farther away
along the line. This accounts for the two-dimensional afterglow in formula (12.172).
Exercises
12.6.8. Solve initial value problem for the two-dimensional wave equation with the following
initial data (a) u(0, x, y) = x −y, ut(0, x, y) = 0; (b) u(0, x, y) = 0, ut(0, x, y) = y.

564
12 Partial Diﬀerential Equations in Space
12.6.9.(a) Prove that u(t, x, y) = 1/

x2 + y2 −c2t2 is a solution to the two-dimensional wave
equation on the domain Ω = {x2 + y2 > c2t2 } exterior to the light cone passing through
the origin. What is the corresponding initial data at t = 0? (b) Use part (a) to solve the
initial value problem u(0, x, y) = 0, ut(0, x, y) = 1/

x2 + y2 , on Ω.
12.6.10. Consider the two-dimensional wave equation on R2 with wave speed c = 1. Write
down an integral formula for the solution to the following initial value problems. You need
not evaluate the integrals.
(a) u(0, x, y) = x3 −y3, ut(0, x, y) = 0;
(b) u(0, x, y) = 0, ut(0, x, y) = y2; (c) u(0, x, y) = x2 + y2, ut(0, x, y) = −x2 −y2.
12.6.11.(a) Find the solution to the two-dimensional wave equation whose initial displacement
is a concentrated delta impulse at the origin and whose initial velocity is zero.
(b) Is your expression a classical solution when t > 0?
(c) True or false: The solution tends to 0 uniformly as t →∞.
12.6.12. Use separation of variables to write down an eigenfunction series solution to the par-
tial diﬀerential equation (12.169) when subject to homogeneous Dirichlet boundary condi-
tions at r = 1 and bounded at r = 0.
♦12.6.13. Write down the fundamental solution for the one-dimensional wave equation with
(a) a concentrated initial displacement at the origin; (b) a concentrated initial velocity
at the origin.
(c) Discuss the validity of Huygens’ Principle in a one-dimensional universe.
12.6.14. Discuss how you can construct solutions to the one-dimensional wave equation by de-
scent from the three-dimensional wave equation.
12.7 The Hydrogen Atom
A hydrogen atom consists of a single electron circling an atomic nucleus that contains a
single proton, which, owing to it relatively tiny size, is assumed to be entirely concentrated
at the origin. As a result of quantization of the corresponding classical Coulomb problem,
the Schr¨odinger equation† governing the dynamical behavior of the electron moving around
the nucleus takes the explicit form
i ℏ∂ψ
∂t = −ℏ2
2M Δψ −α2
r ψ = −ℏ2
2M
 ∂2ψ
∂x2 + ∂2ψ
∂y2 + ∂2ψ
∂y2

−
α2 ψ

x2 + y2 + z2 .
(12.175)
Here ψ(t, x, y, z) denotes the electron’s time-dependent wave function, which, at each time
t, prescribes its quantum probability density as it circles the nucleus. In the quantized
Hamiltonian operator K = −1
2 (ℏ2/M) Δ −α2/r, the coeﬃcient of the Laplacian depends
on Planck’s constant ℏand the electron’s mass M. The ﬁnal term represents the three-
dimensional electromagnetic (Coulomb) potential function V (x) = α2/r attracting the
electron to the nucleus, with α representing the electron’s (and proton’s) charge, while r =
∥x ∥is its distance from the nucleus. Incidentally, the quantum-mechanical Schr¨odinger
equation for multi-electron atoms or even molecules is not so diﬃcult to write down, but
its solution, even for, say, the helium atom, is much more diﬃcult, and is still a major
†
The reader is referred to (9.151) and the subsequent discussion for generalities regarding the
Schr¨odinger equation and quantum mechanics.

12.7 The Hydrogen Atom
565
challenge for numerical analysts, even on today’s supercomputers, [116]. Thus, to keep
matters as simple as possible, we will consider only the case of a single electron hydrogen
atom here.
Bound States
According to the analysis in Section 9.5, the normal mode solutions to the Schr¨odinger
equation are of the form
ψ(t, x, y, z) = e i λ t/ℏv(x, y, z),
where v is an eigenfunction of the Hamiltonian operator with eigenvalue λ, and hence
satisﬁes
ℏ2
2M Δv +

λ + α2
r

v = 0.
(12.176)
The bound states of the atom, in which the electron remains trapped by the nucleus, are
represented by the nonzero solutions to the eigenvalue problem (12.176) with unit L2 norm:
∥v ∥2 =
  
| v(x, y, z) |2 dx dy dz = 1.
The eigenvalue λ speciﬁes the bound state’s energy, and is necessarily negative: λ < 0.
Since we are working on an unbounded domain, the bound states do not form a complete
system of eigenfunctions, and so not every wave function ϕ ∈L2(R3) can be approximated
by an eigenfunction series.
The missing data are the so-called scattering states arising
from the continuous spectrum of the Schr¨odinger operator; these represent electrons that
scatter oﬀthe nucleus, and so do not remain trapped in an orbit. (For the classical Kepler
problem of a planet circling a sun, the bound states would correspond to planets following
bounded elliptic orbits, while the scattering states correspond to interstellar comets and
the like moving along unbounded hyperbolic or parabolic trajectories.)
We will leave
the discussion of the quantum-mechanical scattering states and the associated continuous
spectrum to a more advanced treatment of the subject, [72, 95].
To understand the bound states, we will apply the method of separation of variables.
We begin by rewriting the eigenvalue problem (12.176) in spherical coordinates:
ℏ2
2M
 ∂2v
∂r2 + 2
r
∂v
∂r + 1
r2
∂2v
∂ϕ2 +
cos ϕ
r2 sin ϕ
∂v
∂ϕ +
1
r2 sin2 ϕ
∂2v
∂θ2

+

λ + α2
r

v = 0.
(12.177)
We then separate oﬀthe radial coordinate, setting
v(r, ϕ, θ) = p(r) w(ϕ, θ).
The angular component satisﬁes the spherical Helmholtz equation
ΔS w + μ w = ∂2w
∂ϕ2 + cos ϕ
sin ϕ
∂w
∂ϕ +
1
sin2 ϕ
∂2w
∂θ2 + μ w = 0,
which we have already solved; see (12.21) and the ensuing discussion. The eigensolutions
are spherical harmonics, which, because the quantum-mechanical solutions are intrinsically
complex-valued, we take in their complex form (12.46). The associated eigenvalue
μ = l(l + 1),
where the integer
l = 0, 1, 2, . . .
(12.178)

566
12 Partial Diﬀerential Equations in Space
is known as the angular quantum number, admits a total of 2l + 1 linearly independent
eigenfunctions
Ym
l (ϕ, θ) = P m
l (cos ϕ) e i m θ,
m = −l, −l + 1, . . . , l −1, l.
(12.179)
The radial equation with the separation constant (12.178) is
ℏ2
2M
 d2p
dr2 + 2
r
dp
dr

+

λ + α2
r −l(l + 1)
r2

p = 0.
(12.180)
To eliminate the physical parameters, let us rescale the radial coordinate by setting
s = σ r,
where
σ = 2
√
−2M λ
ℏ
,
(12.181)
given that λ < 0. The resulting ordinary diﬀerential equation for the rescaled function
P(s) = p
" s
σ
#
is
d2P
ds2 + 2
s
dP
ds −
 1
4 −n
s + l(l + 1)
s2

P = 0,
(12.182)
where
n = 2M α2
σ ℏ2
= α2
ℏ

−M
2λ .
(12.183)
Equation (12.182) is a version of the generalized Laguerre diﬀerential equation — see Ex-
ercise 12.7.4 below — named after the nineteenth-century French mathematician Edmond
Laguerre, who studied its solutions well before the appearance of quantum mechanics. Since
we are searching for bound states, the relevant solutions should be deﬁned on 0 ≤s < ∞,
remain bounded at s = 0, and go to zero as s →∞:
lim
s →0+ P(s) < ∞,
lim
s →∞P(s) = 0.
(12.184)
The proof of the following key result is outlined in Exercises 12.7.4–5.
Theorem 12.19.
For each pair of nonnegative integers 0 ≤l < n, the boundary
value problem (12.182, 184) has the eigensolution
P n
l (s) = sl e−s/2 L2l+1
n−l−1(s),
(12.185)
where
Lj
k(s) = s−jes
k!
dk
dsk

sj+ke−s 
=
k

i=0
(−1)i
i!
j + k
j + i

si,
j, k = 0, 1, 2, . . . ,
(12.186)
are known as generalized† Laguerre polynomials.
†
The ordinary Laguerre polynomials are Lk(s) = L0
k(s).

12.7 The Hydrogen Atom
567
L0
1(s)
L0
2(s)
L0
3(s)
L1
1(s)
L1
2(s)
L1
3(s)
Figure 12.13.
Generalized Laguerre polynomials.
The ﬁrst few generalized Laguerre polynomials are
L0
0(s) = 1,
L0
1(s) = 1 −s,
L0
2(s) = 1 −2s + 1
2 s2,
L0
3(s) = 1 −3s + 3
2 s2 −1
6 s3,
L1
0(s) = 1,
L1
1(s) = 2 −s,
L1
2(s) = 3 −3s + 1
2 s2,
L1
3(s) = 4 −6s + 2s2 −1
6 s3,
L2
0(s) = 1,
L2
1(s) = 3 −s,
L2
2(s) = 6 −4s + 1
2 s2,
L2
3(s) = 10 −10s + 5
2 s2 −1
6 s3.
Note that Lj
k(s) has degree k.
A few graphs, on the interval 0 ≤t ≤6, appear in
Figure 12.13. See [86] for details on their properties.
Atomic Eigenstates and Quantum Numbers
The integer n, whose physical value was noted in (12.183), is known as the principal
quantum number. We further note that the scaling factor in (12.181) can be written as
σ = 2M α2
n ℏ2
= 2
na ,
where
a =
ℏ2
M α2 ≈.529 × 10−10 meter,
which approximates the radius of the electron’s lowest energy level, is known as the Bohr
radius, in honor of the pioneering Danish quantum physicist Niels Bohr. Reverting to phys-

568
12 Partial Diﬀerential Equations in Space
ical coordinates, the bound state solutions (12.185) become, up to an inessential constant
multiple, the radial wave functions
βn
l (r) =
 2r
na
l
e−r/(na) L2l+1
n−l−1
 2r
na

.
(12.187)
Combining them with the spherical harmonics (12.179) yields the atomic eigenfunctions
or eigenstates
vlmn(r, ϕ, θ) =

(2l + 1) (l −m)! (n −l −1)!
π a3n4(l + m)! (l + n)!
βn
l (r) Ym
l (ϕ, θ),
(12.188)
where the initial factor is selected so as to make ∥vlmn ∥= 1, and hence a bona ﬁde wave
function. (A proof of this fact is outlined in Exercise 12.7.8.) The eigenstates depend on
three integers, which have the following physical designations:
• n = 1, 2, 3, . . . :
the principal quantum number;
• l = 0, 1, . . . , n −1:
the angular quantum number;
• m = −l, −l + 1, . . . , l −1, l :
the magnetic quantum number.
The energy is the associated eigenvalue:
λn = −α4M
2 ℏ2
1
n2 = −α2
2a
1
n2 ,
n = 1, 2, 3, . . . .
(12.189)
The fact that the ratios λn/λ1 = 1/n2 between the energy levels of an atom are inverse
squares of integers was one of the key experimental discoveries that precipitated the dis-
covery of quantum mechanics. Observe that the nth energy level has a total of
n−1

l=0
(2l + 1) = n2
(12.190)
linearly independent bound states (12.188). The dimension of the eigenspace corresponds to
the number of orbital subshells in the atom for the corresponding energy level. The shells
indexed by the angular quantum number, i.e., the order l = 0, 1, 2, . . . of the spherical
harmonic, are traditionally labeled by a letter in the sequence s, p, d, f, g, . . . , where each
successive shell contains 2l + 1 individual subshells, indexed by the magnetic quantum
number m.
The one missing ingredient in this simple model is the electron’s spin. Since electrons
can have one of two possible spins, the Pauli Exclusion Principle, ﬁrst formulated by the
Austrian physicist Wolfgang Pauli, tells us that each atomic energy shell can be occupied
by at most two electrons. Consequently, the atomic shell with angular quantum number l
may contain up to 2(2l + 1) electrons. Keep in mind that, since 0 ≤l < n, the lth shell
appears only when n is suﬃciently large, so that, according to (12.190), the nth energy
level contains up to 2n2 electrons.
The resulting atomic conﬁguration of electronic energy shells is the explanation for
Mendeleev’s periodic table.
Its rows are indexed by the principal quantum number n,
while the columns are labeled by the angular and magnetic quantum numbers l, m, and
the spin. As one moves up the periodic table, the electrons in each successive element’s
atom progressively ﬁll up the lower energy levels, each new shell containing ﬁrst a single
electron, then two electrons with opposite spins. Thus, hydrogen (in its ground state) has
a single electron in the 1s shell. Helium has two electrons in the 1s shell. Lithium has

12.7 The Hydrogen Atom
569
three electrons, with two of them ﬁlling the 1s shell and the third in the 2s shell. Neon
has ten electrons ﬁlling the ﬁrst two energy levels, with two electrons in the 1s shell, two
in the 2s shell, and six in the 2p shell. And so on. The one complication is that, owing
to the orbital’s geometry, as prescribed by the associated spherical harmonic, the angular
and, to a lesser extent, magnetic quantum numbers also aﬀect the physically observed
energy, and this can cause shells to ﬁll later than might initially be expected. For example,
in potassium and calcium, the 4s shell is successively ﬁlled, followed by scandium, which
begins the process of ﬁlling the 3d subshells. The chemical properties of the elements are,
to a very large extent, determined by the placement of their atom’s electrons within the
outermost energy level. The interested reader can consult, for example, [67, 79] for further
details.
Exercises
12.7.1. If the nucleus contains Z protons circled by a single electron, then its atomic potential
V (x) is rescaled accordingly, replacing α2/r by Z α2/r. Discuss the induced eﬀect on the
energy levels of such an atomic ion.
♥12.7.2.(a) Write down the time-dependent wave function for a single electron atom when the
electron is in its ground state, i.e., the lowest energy level. (b) What is the probability
density of the electron? (c) What is the probability of ﬁnding the electron within 1 Bohr
radius of the atom?
(d) Find the distance d (measured in Bohr radii) so that there is a
95% probability of ﬁnding the electron within a distance d of the nucleus.
♦12.7.3. Prove that the two expressions for the Laguerre polynomials in (12.186) agree.
♦12.7.4.(a) Let k = 0, 1, 2, . . . be a nonnegative integer. The Laguerre diﬀerential equation of
order k is
x u′′ + (1 −x) u′ + k u = 0.
(12.191)
Show that x = 0 is a regular singular point. Then prove that the Frobenius solution based
at x = 0 is a polynomial of degree j that coincides with the Laguerre polynomial L0
k(x).
(b) Given nonnegative integers j, k ≥0, use the Frobenius method to prove that the general-
ized Laguerre diﬀerential equation
x u′′ + (j + 1 −x) u′ + k u = 0
(12.192)
has a polynomial solution that can be identiﬁed with the generalized Laguerre polyno-
mial Lj
k(x) in (12.186).
♦12.7.5. Suppose that P(s) solves the ordinary diﬀerential equation (12.182). Prove that
Q(s) = s−l es/2 P(s) solves the diﬀerential equation
s d2Q
ds2 + [2(l + 1) −s] dQ
ds + (n −l −1)Q = 0.
(12.193)
Then apply the result of Exercise 12.7.4 to complete the proof of Theorem 12.19.
♥12.7.6. Suppose f(x) is a polynomial, and let Lj
k(s) denote the generalized Laguerre polynomi-
als (12.186). (a) Prove that, for j, k ≥0,
	 ∞
0
f(s) Lj
k(s) sj e−s ds = (−1)k
k!
	 ∞
0
f(k)(s) sj+k e−s ds.
(b) For ﬁxed j, prove that the generalized Laguerre polynomials Lj
k(s), k = 0, 1, 2, . . . ,
are orthogonal with respect to the weighted inner product ⟨f , g ⟩=
	 ∞
0
f(s) g(s) sj e−s ds.
(c) Prove the formula for their corresponding norms: ∥Lj
k ∥=

(j + k)!
k!
.

570
12 Partial Diﬀerential Equations in Space
♦12.7.7.(a) Prove that the generalized Laguerre polynomials satisfy the following recurrence re-
lation:
(k + 1) Lj
k+1(s) −(j + 2k + 1 −s) Lj
k(s) + (j + k) Lj
k−1(s) = 0.
(12.194)
(b) Prove that
	 ∞
0
sj+1e−s 
Lj
k(s)
 2 ds = (j + 2k + 1) (j + k)!
k!
.
(12.195)
Hint: Use part (a) and Exercise 12.7.6.
♥12.7.8. Prove that the atomic eigenfunctions (12.188) form an orthonormal system of wave
functions with respect to the L2 inner product on R3. Hint: Use Theorem 9.33 and equa-
tion (12.195).

Appendix A
Complex Numbers
The purpose of this short appendix is to review the basics of complex numbers and complex
arithmetic, which are used throughout much of the text.
A complex number is an expression of the form z = x + i y, where x, y ∈R are real
and i = √−1 is the imaginary unit. The set of all complex numbers is denoted by C. We
call x = Rez the real part of z and y = Im z the imaginary part of z = x + i y. (Note: The
imaginary part is the real number y, not i y.) A real number x is merely a complex number
with zero imaginary part, Im z = 0, and so we may regard R ⊂C. Complex addition and
multiplication are based on simple adaptations of the rules of real arithmetic to include
the identity i 2 = −1, and so
(x + i y) + (u + i v) = (x + u) + i (y + v),
(x + i y) (u + i v) = (xu −y v) + i (xv + y u).
(A.1)
Complex numbers enjoy all the usual laws of real addition and multiplication, including
commutativity: z w = wz.
We can identify a complex number x + i y with a vector (x, y) ∈R2 in the real,
two-dimensional plane. For this reason, C is sometimes referred to as the complex plane.
(Although keep in mind that, as a complex vector space, C is only one-dimensional.) Based
on this identiﬁcation, we shall employ the standard terminology of planar vector calculus
— domain, curve, etc. — without alteration. Complex addition (A.1) corresponds to vector
addition, but the vector interpretation of complex multiplication is more obscure.
The complex conjugate of z = x + i y is z = x −i y. Note that Re z = Re z, while
Im z = −Im z. Geometrically, the complex conjugate of z is obtained by reﬂecting the
corresponding vector through the real axis, as illustrated in Figure A.1.
In particular,
z = z if and only if z is real. In general,
Re z = z + z
2
,
Im z = z −z
2 i
.
(A.2)
Complex conjugation is compatible with complex arithmetic:
z + w = z + w,
z w = z w.
In particular, the product of a complex number and its conjugate,
z z = (x + i y) (x −i y) = x2 + y2,
(A.3)
is real and nonnegative. Its square root is known as the modulus or norm of the complex
number z = x + i y, and written
| z | =

x2 + y2 .
(A.4)
DOI 10.1007/978-3-
-
-0
319 02099
, © Springer International Publishing Switzerland 2014
571
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

572
A Complex Numbers
r
θ
z
z
x
y
Figure A.1.
Complex numbers.
Note that | z | ≥0, with | z | = 0 if and only if z = 0. The modulus | z | generalizes the
absolute value of a real number and coincides with the standard Euclidean norm in the
(x, y)–plane. This implies the validity of the triangle inequality
| z + w | ≤| z | + | w |.
(A.5)
Equation (A.3) can be rewritten in terms of the modulus as
z z = | z |2.
(A.6)
Rearranging the factors, we deduce the formula for the reciprocal of a nonzero complex
number:
1
z =
z
| z |2 ,
z ̸= 0,
or, equivalently,
1
x + i y = x −i y
x2 + y2 .
(A.7)
The general formula for complex division,
w
z = w z
| z |2
or
u + i v
x + i y = (xu + y v) + i (xv −y u)
x2 + y2
,
(A.8)
is an immediate consequence.
The modulus of a complex number,
r = | z | =

x2 + y2 ,
is one component of its polar coordinate representation
x = r cos θ,
y = r sin θ
or
z = r(cos θ + i sin θ).
(A.9)
The polar angle θ, which measures the angle that the line connecting z to the origin makes
with the horizontal axis, is known as the phase, and written
θ = ph z.
(A.10)
As such, the phase is deﬁned only up to an integer multiple of 2π. The unique principal
value of the phase is restricted to −π < ph z ≤π. A more common term for the polar

A Complex Numbers
573
angle is the argument of z, written arg z = ph z. However, in conformity with [85, 86], we
prefer to use “phase” here, in part to avoid confusion with the argument z of a function
f(z).
Euler’s celebrated formula for the complex exponential,
e i θ = cos θ + i sin θ,
(A.11)
can be used to compactly rewrite the polar form (A.9) of a complex number as
z = r e i θ,
where
r = | z |,
θ = ph z.
(A.12)
Consequently, the complex logarithm has the form
log z = log(r e i θ) = log r + log e i θ = log r + i θ = log | z | + i ph z.
(A.13)
More generally, the complex exponential is given by
ez = ex cos y + i ex sin y,
for
z = x + i y.
(A.14)
We note that the modulus and phase of a product of complex numbers can be readily
computed:
| z w | = | z | | w |,
ph(z w) = ph z + ph w,
(A.15)
the latter formula requiring that we allow multiply valued phases; the formula does not
hold as stated for all z, w when the principal value of the phase is used. Similarly, the
modulus and phase of the reciprocal of a nonzero complex number are

1
z
 =
1
| z | ,
ph
1
z

= −ph z.
(A.16)
On the other hand, complex conjugation preserves the modulus, but negates the phase:
| z | = | z |,
ph z = −ph z.
(A.17)
The latter formula is not valid for the principal value of the phase when z lies on the
negative real axis.

Appendix B
Linear Algebra
In this appendix, we collect basic results and deﬁnitions from linear algebra that are used
in our study of partial diﬀerential equations. The reader is referred to [89] for the proofs
and further details.
B.1 Vector Spaces and Subspaces
Vector spaces and their ancillary structures provide the common language of linear alge-
bra. The basic deﬁnition is modeled on the prototypical ﬁnite-dimensional example: the
Euclidean space Rn, which is the set of all real (column) vectors with n entries, equipped
with the operations of vector addition and scalar multiplication. More generally:
Deﬁnition B.1. A (real) vector space is a set V equipped with two operations:
(i) Addition: adding any pair of elements v, w ∈V produces another vector v + w ∈V .
(ii) Scalar Multiplication: multiplying an element v ∈V by a scalar c ∈R produces a
vector cv ∈V .
These are subject to the following axioms: for all u, v, w ∈V and all scalars c, d ∈R,
(a) Commutativity of Addition: v + w = w + v.
(b) Associativity of Addition: u + (v + w) = (u + v) + w.
(c) Additive Identity: There is a zero element 0 ∈V satisfying v + 0 = v = 0 + v.
(d) Additive Inverse: For each v ∈V there is an element −v ∈V such that
v + (−v) = 0 = (−v) + v.
(e) Distributivity: (c + d)v = (cv) + (dv), and c(v + w) = (cv) + (cw).
(f ) Associativity of Scalar Multiplication: c(dv) = (cd)v.
(g) Unit for Scalar Multiplication: the scalar 1 ∈R satisﬁes 1v = v.
Complex vector spaces are deﬁned in an identical manner, the only diﬀerence being
that the scalars are allowed to be complex numbers. In this case, the prototype is the space
Cn consisting of column vectors with n complex entries.
While ﬁnite-dimensional vector spaces play a signiﬁcant role in the study of partial
diﬀerential equations, particularly in the design of numerical solution schemes, for us the
more important examples are inﬁnite-dimensional vector spaces whose elements (“vectors”)
are functions. The main example is the following:
DOI 10.1007/978-3-
-
-0
319 02099
, © Springer International Publishing Switzerland 2014
575
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

576
B Linear Algebra
Example B.2.
Let I ⊂R be an interval. The function space F = F(I), whose
elements are all real-valued functions f(x) deﬁned for x ∈I, has the structure of a vector
space. Addition of functions in F is deﬁned in the usual manner: (f + g)(x) = f(x) + g(x)
for all x ∈I. Multiplication by scalars c ∈R is the same as multiplication by constants,
(c f)(x) = c f(x). The zero element is the constant function that is identically 0 for all
x ∈I. With these operations, all the vector space axioms listed in Deﬁnition B.1 are valid,
and hence F(I) is a real vector space.
More generally, if Ω ⊂Rn is any subset of n-dimensional Euclidean space, the function
space F(Ω) is deﬁned as the set of all real-valued functions f(x1, . . . , xn) deﬁned for all
x = (x1, . . . , xn) ∈Ω.
Addition and scalar (constant) multiplication of functions are
deﬁned in the same manner.
A subspace of a vector space V is a subset W ⊂V that is a vector space in its own
right. In particular, a subspace W must contain the zero element of V .
Proposition B.3. A nonempty subset W ⊂V of a vector space is a subspace if and
only if
(a) for every v, w ∈W, the sum v + w ∈W, and
(b) for every v ∈W and every c ∈R, the scalar product cv ∈W.
For example, a complete list of subspaces of V = R3 is (i) the origin {0}; (ii) every
line through the origin; (iii) every plane through the origin; (iv) all of R3.
Example B.4. Here are some examples of subspaces of the function space F(I).
(a) The space P(n) of polynomials of degree ≤n.
(b) The space C0(I) of all continuous functions on the interval I.
(c) The space Cn(I) consisting of all functions f(x) that have n continuous derivatives
f ′(x), f ′′(x), . . . , f (n)(x) on† I.
(d) The space C∞(I) =

n≥0 Cn(I) of inﬁnitely diﬀerentiable, or smooth, functions is
also a subspace.
(e) The space A(I) of analytic functions. Recall that a function f(x) is called analytic
at a point a if it is smooth, and, moreover, its Taylor series
f(a) + f ′(a) (x −a) + 1
2 f ′′(a) (x −a)2 + · · ·
=
∞

n=0
f (n)(a)
n!
(x −a)n
(B.1)
converges to f(x) for all x suﬃciently close to a. (The series is not required to
converge on the entire interval I.) Not every smooth function is analytic, and so
A(I) ⊊C∞(I); see Exercise 11.3.21 for an explicit example.
B.2 Bases and Dimension
Deﬁnition B.5. Let v1, . . . , vk belong to a vector space V . A sum of the form
c1v1 + c2v2 + · · · + ckvk =
k

i=1
civi,
(B.2)
†
We use one-sided derivatives at any endpoint that belongs to the interval.

577
where the coeﬃcients c1, c2, . . . , ck are any scalars, is known as a linear combination of the
elements v1, . . . , vk. Their span is the subspace W = span {v1, . . . , vk} ⊂V consisting of
all possible linear combinations.
Deﬁnition B.6. The elements v1, . . . , vk ∈V are called linearly dependent if there
exist scalars c1, . . . , ck, not all zero, such that
c1v1 + · · · + ckvk = 0.
(B.3)
Elements that are not linearly dependent are called linearly independent.
In particular, a collection of functions f1(x), . . ., fn(x) is linearly dependent if and
only if there exist constants c1, . . . , cn, not all zero, such that the linear combination
c1f1(x) + · · · + cnfn(x) ≡0
(B.4)
is identically zero.
Conversely, if the only choice of constants for which (B.4) holds is
c1 = · · · = cn = 0, then the functions are linearly independent.
Deﬁnition B.7.
A basis of a vector space V is a ﬁnite collection of elements
v1, . . . , vn ∈V that (a) spans V , and (b) is linearly independent.
The simplest example is the standard basis of Rn, consisting of the n vectors
e1 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
0
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
e2 =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
1
0
...
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
. . . ,
en =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
0
0
...
0
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(B.5)
so that ei is the vector with 1 in the ith slot and 0’s elsewhere. However, there are many
other bases of Rn; indeed, any n linearly independent vectors v1, . . . , vn ∈Rn form a basis.
Lemma B.8. The elements v1, . . . , vn form a basis of V if and only if every v ∈V
can be written uniquely as a linear combination of the basis elements:
v = c1 v1 + · · · + cn vn =
n

i=1
ci vi.
(B.6)
The coeﬃcients (c1, . . . , cn) are called the coordinates of the vector v with respect to the
given basis.
Theorem B.9. Suppose the vector space V has a basis v1, . . . , vn. Then every other
basis of V has the same number of elements in it. This number is called the dimension of
V , and written dim V = n.
On the other hand, if the vector space contains inﬁnitely many linearly independent
elements, then it does not have a basis in the sense of Deﬁnition B.7, and is thus inﬁnite-
dimensional. All of the function spaces and subspaces listed above are inﬁnite-dimensional
vector spaces. An example of a ﬁnite-dimensional function space is the space P(n) ⊂F(R)
consisting of all polynomials p(x) = a0 + a1x + · · · + anxn of degree ≤n. The monomials
1, x, x2, . . . , xn form a basis, and hence P(n) has dimension n + 1. (On the other hand, the
vector space containing all polynomials is inﬁnite-dimensional.)
B.2 Bases and Dimension

578
B Linear Algebra
B.3 Inner Products and Norms
The dot product on Euclidean space Rn plays an essential role in geometry, analysis, and
mechanics.
Its basic properties inspire the general deﬁnition of an inner product on a
vector space.
Deﬁnition B.10. An inner product on the real vector space V is a pairing that takes
two elements v, w ∈V and produces a real number ⟨v , w ⟩∈R, subject to the following
three axioms for all u, v, w ∈V , and scalars c, d ∈R.
(i) Bilinearity:
⟨c u + d v , w ⟩= c ⟨u , w ⟩+ d ⟨v , w ⟩,
⟨u , c v + d w ⟩= c ⟨u , v ⟩+ d ⟨u , w ⟩.
(B.7)
(ii) Symmetry:
⟨v , w ⟩= ⟨w , v ⟩.
(B.8)
(iii) Positivity:
⟨v , v ⟩> 0
whenever
v ̸= 0,
while
⟨0 , 0 ⟩= 0.
(B.9)
Given an inner product, the associated norm of an element v ∈V is deﬁned as the
positive square root of its inner product with itself:
∥v ∥=

⟨v , v ⟩.
(B.10)
Bilinearity of the inner product implies that
∥c v ∥= | c | ∥v ∥
for any scalar c.
The positivity axiom implies that ∥v ∥≥0 is real and nonnegative,
and equals 0 if and only if v = 0 is the zero element. A vector space norm induces a
notion of distance between elements v, w ∈V , with dist(v, w) = ∥v −w ∥. In particular,
dist(v, w) = 0 if and only if v = w.
Example B.11. The most familiar example of an inner product is the dot product†
⟨v , w ⟩= v · w = vT w = v1 w1 + v2 w2 + · · · + vn wn
(B.11)
on the Euclidean space Rn. The associated Euclidean norm
∥v ∥= √v · v =

v2
1 + v2
2 + · · · + v2
n
(B.12)
conforms to our usual notion of distance between points in Euclidean space.
To ﬁnd the most general inner product on Rn, we need to introduce the important
class of positive deﬁnite matrices.
Deﬁnition B.12.
An n × n matrix C is called positive deﬁnite if it satisﬁes the
positivity condition
vT C v > 0
for all
0 ̸= v ∈Rn.
(B.13)
We will sometimes write C > 0 to mean that C is a positive deﬁnite matrix.
†
The elements v ∈Rn are to be regarded as column vectors, while the transpose, written
vT , is the corresponding row vector.

579
Warning: The condition C > 0 does not mean that all the entries of C are positive.
For example,

3
−1
−1
1

is positive deﬁnite, whereas

1
2
2
1

is not.
Many authors, including [89], require that a positive deﬁnite matrix also be symmetric.
We will not impose this condition here a priori. However, most of the positive deﬁnite
matrices we will encounter in applications will be symmetric (or, more generally, self-
adjoint — as in Example 9.15). For a symmetric matrix, the most useful test for positive
deﬁniteness is to perform Gaussian Elimination on C, which is positive deﬁnite if and only
if no row interchanges are needed, and all the pivots are positive, [89].
Proposition B.13. Every inner product on Rn is given by
⟨v , w ⟩= vT C w
for
v, w ∈Rn,
(B.14)
where C > 0 is a symmetric positive deﬁnite matrix.
The next example is of particular signiﬁcance in Fourier analysis and partial diﬀerential
equations.
Example B.14. Let [a, b] ⊂R be a bounded closed interval. The integral
⟨f , g ⟩=
 b
a
f(x) g(x) dx
(B.15)
deﬁnes an inner product on the space C0[a, b] of continuous functions. The associated
norm
∥f ∥=
 b
a
f(x)2 dx
(B.16)
is known as the L2 norm of the function f over the interval [a, b]. The positivity of the
norm: ∥f ∥> 0 for f ̸= 0, follows from the fact that the only continuous nonnegative
function g(x) ≥0 that satisﬁes
 b
a
g(x) dx = 0 is the zero function g(x) ≡0. Extending
this construction to spaces containing discontinuous functions is trickier, since there are
discontinuous functions that are not identically zero, but nevertheless have zero norm
integral. An example is a function that is zero except at a single point. Further discussion
can be found in Section 3.5.
The two most important inequalities in mathematical analysis apply to any inner
product space.
Theorem B.15.
Every inner product satisﬁes the Cauchy–Schwarz and triangle
inequalities
| ⟨v , w ⟩| ≤∥v ∥∥w ∥,
∥v + w ∥≤∥v ∥+ ∥w ∥,
for all
v, w ∈V.
(B.17)
Equality holds if and only if v and w are parallel, i.e., scalar multiples of each other.
Proof : We begin with the Cauchy–Schwarz inequality: | ⟨v , w ⟩| ≤∥v ∥∥w ∥. The
case w = 0 is trivial, and so we assume w ̸= 0. Let t ∈R be an arbitrary scalar. Using
the three inner product axioms, we have
0 ≤∥v + t w ∥2 = ⟨v + t w , v + t w ⟩= ∥v ∥2 + 2 t ⟨v , w ⟩+ t2 ∥w ∥2,
(B.18)
B.3 Inner Products and Norms

580
B Linear Algebra
with equality holding if and only if v = −t w, which requires v and w to be parallel vectors.
We ﬁx v and w, and consider the right-hand side of (B.18) as a quadratic function of t.
Its minimum value occurs when t = ∥w ∥−2 ⟨v , w ⟩. Substituting this value into (B.18),
we obtain
0 ≤∥v ∥2 −2 ⟨v , w ⟩2
∥w ∥2
+ ⟨v , w ⟩2
∥w ∥2
= ∥v ∥2 −⟨v , w ⟩2
∥w ∥2
,
and hence ⟨v , w ⟩2 ≤∥v ∥2 ∥w ∥2, which, upon taking the square root, establishes the
Cauchy–Schwarz inequality. Again, as noted above, equality holds if and only if v and w
are parallel.
To establish the triangle inequality, we compute
∥v + w ∥2 = ⟨v + w , v + w ⟩= ∥v ∥2 + 2 ⟨v , w ⟩+ ∥w ∥2
≤∥v ∥2 + 2 ∥v ∥∥w ∥+ ∥w ∥2 =

∥v ∥+ ∥w ∥
2,
where the middle inequality follows from the Cauchy–Schwarz inequality (which clearly
also holds if the absolute value is removed.) Taking square roots of both sides completes
the proof.
Q.E.D.
We will also have occasion to use inner products on complex vector spaces. To ensure
that the associated norm remains positive, the real deﬁnition must be modiﬁed.
The
complex conjugate of a complex scalar c = a + i b, with a, b ∈R, will be indicated by an
overbar: c = a −i b. When dealing with a complex inner product space, one must pay
careful attention to complex conjugation.
Deﬁnition B.16. An inner product on the complex vector space V is a pairing that
takes two vectors v, w ∈V and produces a complex number ⟨v , w ⟩∈C, subject to the
following requirements, for u, v, w ∈V , and c, d ∈C:
(i) Sesquilinearity:
⟨c u + d v , w ⟩= c ⟨u , w ⟩+ d ⟨v , w ⟩,
⟨u , c v + d w ⟩= c ⟨u , v ⟩+ d ⟨u , w ⟩.
(B.19)
(ii) Conjugate Symmetry:
⟨v , w ⟩= ⟨w , v ⟩.
(B.20)
(iii) Positivity:
∥v ∥2 = ⟨v , v ⟩≥0,
and
⟨v , v ⟩= 0
if and only if
v = 0.
(B.21)
Example B.17. The simplest example is the Hermitian dot product
z · w = zT w = z1 w1 + z2 w2 + · · · + zn wn,
for
z =
⎛
⎜
⎜
⎜
⎝
z1
z2
...
zn
⎞
⎟
⎟
⎟
⎠,
w =
⎛
⎜
⎜
⎜
⎝
w1
w2
...
wn
⎞
⎟
⎟
⎟
⎠, (B.22)
between complex vectors v, w ∈Cn.
Example B.18.
Let C0[−π, π ] denote the complex vector space consisting of all
complex-valued continuous functions f(x) = u(x) + i v(x) depending on the real variable
−π ≤x ≤π. The L2 Hermitian inner product on C0[−π, π ] is deﬁned as
⟨f , g ⟩=
 π
−π
f(x) g(x) dx ,
(B.23)

B.4 Orthogonality
581
i.e., the integral of f times the complex conjugate of g, with corresponding norm
∥f ∥=
 π
−π
| f(x) |2 dx =
 π
−π

u(x)2 + v(x)2 
dx .
(B.24)
Inner products on complex vector spaces also satisfy the Cauchy–Schwarz and triangle
inequalities (B.17). The proof is left as an exercise for the reader; see [89; Exercise 3.6.46].
B.4 Orthogonality
Deﬁnition B.19.
Two elements v, w ∈V of an inner product space V are called
orthogonal if their inner product vanishes: ⟨v , w ⟩= 0.
For ordinary Euclidean space equipped with the dot product, two vectors are orthog-
onal if and only if they are perpendicular, i.e., meet at a right angle.
Deﬁnition B.20. A basis u1, . . . , un of an inner product space V is called orthogonal
if ⟨ui , uj ⟩= 0 for all i ̸= j. The basis is called orthonormal if, in addition, each vector
has unit length: ∥ui ∥= 1, for all i = 1, . . ., n.
For example, the standard basis vectors (B.5) form an orthonormal basis of Rn with
respect to the dot product, but they are not orthonormal for any other inner product
thereon.
Theorem B.21.
If v1, . . . , vn form an orthogonal basis, then the corresponding
coordinates of a vector
v = a1 v1 + · · · + an vn
are given by
ai = ⟨v , vi ⟩
∥vi ∥2 .
(B.25)
Moreover, the vector’s norm can be computed using the formula
∥v ∥2 =
n

i=1
a2
i ∥vi ∥2 =
n

i=1
⟨v , vi ⟩
∥vi ∥
2
.
(B.26)
Proof : We compute the inner product of (B.25) with one of the basis vectors. By
orthogonality,
⟨v , vi ⟩=

n

j =1
aj vj , vi

=
n

j =1
aj ⟨uj , ui ⟩= ai ∥vi ∥2.
To prove formula (B.26), we similarly expand
∥v ∥2 = ⟨v , v ⟩=
n

i,j =1
ai aj ⟨vi , vj ⟩=
n

i=1
a2
i ∥vi ∥2.
Q.E.D.
In the case of an orthonormal basis, the formulas (B.25–26) simplify to
v = c1 u1 + · · · + cn un,
where
ci = ⟨v , ui ⟩,
∥v ∥= c2
1 + · · · + c2
n.
(B.27)

582
B Linear Algebra
Example B.22.
A particularly important orthogonal basis is provided by the fol-
lowing vectors lying in Cn:
ωk =

1, ζk, ζ2k, ζ3k, . . . , ζ(n−1)k T
=

1, e2kπ i /n, e4kπ i /n, . . . , e2(n−1)kπ i /n T ,
k = 0, . . ., n −1,
(B.28)
where
ζ = e2π i /n.
(B.29)
Orthogonality relies on the fact that its powers, ζk = e2kπ i /n, k = 0, . . ., n −1, are the
complex roots of the elementary polynomial
zn −1 = (z −1)(1 + z + z2 + · · · + zn−1),
(B.30)
while
ζ = e−2π i /n = ζ−1.
Since when 0 < k ≤n −1, the complex number ζk ̸= 1 is a root of the polynomial (B.30),
it must also be a root of the second factor. This implies that
1 + ζk + ζ2k + ζ3k + · · · + ζ(n−1)k =
 n,
k ≡0 mod n,
0,
k ̸≡0 mod n,
where the former case k ≡0 mod n follows by direct substitution of ζk = 1. Thus, the
Hermitian inner products of the vectors (B.28) equal
⟨ωk , ωl ⟩=
n−1

j =0
ζj k ζj l =
n−1

j =0
ζj (k−l) =
 n,
k = l,
0,
k ̸= l,
(B.31)
provided 0 ≤k, l ≤n −1, thereby establishing orthogonality. These vectors are the dis-
crete analogues of the orthogonal complex exponential functions that are used to construct
complex Fourier series. They are the basis of the discrete Fourier transform, [89; §5.7],
and their orthogonality is the key to modern signal processing.
B.5 Eigenvalues and Eigenvectors
The eigenvalues and eigenvectors of a matrix ﬁrst appear when solving linear systems
of ordinary diﬀerential equations.
But their essential importance extends across all of
mathematics and its manifold applications. Extensions of the eigenvalue method to linear
operators on function spaces are critical to the analysis of partial diﬀerential equations.
Deﬁnition B.23. Let A be an n × n matrix. A scalar λ is called an eigenvalue of A
if there is a nonzero vector v ̸= 0, called an associated eigenvector, such that
Av = λv.
(B.32)
In particular, a matrix has λ = 0 as an eigenvalue if and only if it has a null eigenvector
v ̸= 0, satisfying Av = 0, and hence is a singular (non-invertible) matrix, with vanishing
determinant: det A = 0.
An eigenvalue is called simple if it admits only one linearly
independent eigenvalue; more generally, the multiplicity of an eigenvalue is deﬁned as the

B.6 Linear Iteration
583
dimension of the eigenspace consisting of all solutions to the eigenequation (B.32), including
0. Thus, a simple eigenvalue has multiplicity 1.
Even if A is a real matrix, we must allow the possibility of complex eigenvectors.
Matrices with a “complete” set of eigenvectors are the most common, and also the easiest
to deal with.
Deﬁnition B.24.
An n × n real or complex matrix A is called complete if there
exists a basis of Cn consisting of its (complex) eigenvectors.
It is not hard to show that eigenvectors corresponding to diﬀerent eigenvalues are
necessarily linearly independent. This means that matrices with all distinct (and hence
simple) eigenvalues are necessarily complete:
Proposition B.25. Any n × n matrix with n distinct eigenvalues is complete.
Unfortunately, not all matrices with repeated eigenvalues are complete. For instance,

1
0
0
1

is complete, since, for instance,

1
0

and

0
1

form an eigenvector basis of
C2, whereas

1
1
0
1

is not, since it has only one independent eigenvector, namely

1
0

.
Incomplete matrices are much more challenging to deal with, both theoretically and nu-
merically. Fortunately, we can safely ignore the incomplete cases in this text.
The most common way for orthogonal bases to arise is as eigenvector bases of sym-
metric matrices. (Orthogonality is with respect to the standard dot product on Rn.) The
extension of this result to “self-adjoint” operators on function space forms the foundation
of Fourier analysis and its generalizations.
Theorem B.26. Let A = AT be a real symmetric n × n matrix. Then
(a) All the eigenvalues of A are real.
(b) Eigenvectors corresponding to distinct eigenvalues are orthogonal.
(c) There is an orthonormal basis of Rn consisting of n eigenvectors of A.
Let us demonstrate orthogonality, leaving the remaining steps in the proof to [89;
Theorem 8.20]. If
Av = λv,
Aw = μ w,
where λ ̸= μ are distinct real eigenvalues, then, by symmetry of A,
λ v · w = (Av) · w = (Av)T w = vT Aw = v · (Aw) = v · (μ w) = μ v · w,
and hence
(λ −μ) v · w = 0.
Since λ ̸= μ, this implies that the eigenvectors v, w are necessarily orthogonal.
B.6 Linear Iteration
For numerical applications, we will require some basic results on iteration of linear systems.
Consider ﬁrst a homogeneous linear iterative system of the form
u(k+1) = Au(k),
u(0) = u0,
(B.33)

584
B Linear Algebra
in which A is an n × n matrix and u0 ∈Rn or Cn. The solution to such a system is
evidently obtained by repeatedly multiplying the initial vector u0 by the matrix A, and so
u(k) = Aku0.
(B.34)
Deﬁnition B.27.
A matrix A is called convergent if every solution to the homo-
geneous linear iterative system (B.33) tends to zero in the limit: u(k) →0 as k →∞.
Equivalently, A is convergent if and only if its powers converge to the zero matrix: Ak →O
as k →∞.
The solution formula (B.34), while elementary, is not particularly enlightening. An
alternative approach is to recognize that if λj is an eigenvalue of A and vj a corresponding
eigenvector, then
u(k)
j
= λk
j vj
(B.35)
is a solution, since
Au(k)
j
= λk
j Avj = λk+1
j
vj = u(k+1)
j
.
Moreover, linear combinations of such eigensolutions are also solutions. In particular, if A
is complete, then we can write down the general solution to (B.33) as a linear combination
of the independent eigensolutions:
u(k) = c1 λk
1 v1 + c2 λk
2 v2 + · · · + cn λk
n vn,
(B.36)
where {v1, . . . , vn} is the eigenvector basis. The coeﬃcients c1, . . . , cn are uniquely deter-
mined by the initial conditions,
u(0) = c1 v1 + c2 v2 + · · · + cn vn = u0,
which relies on the fact that the eigenvectors v1, . . . , vn form a basis. Now, A is convergent
if and only if all solutions u(k) →0. The individual eigensolution (B.35) goes to zero if and
only if its associated eigenvalue is strictly less than 1 in modulus: | λj | < 1. This proves
the following result for complete matrices. The proof in the incomplete case relies on the
Jordan canonical form, [89; Chapter 10].
Theorem B.28. The matrix A is convergent if and only if all its eigenvalues satisfy
| λ | < 1.
Deﬁnition B.29. The spectral radius of a matrix A is deﬁned as the maximal mod-
ulus of all of its real and complex eigenvalues: ρ(A) = max { | λ1 |, . . . , | λk | }.
Corollary B.30. The matrix A is convergent if and only if ρ(A) < 1.
Indeed, the spectral radius essentially governs the rate of convergence of the iterative
system — the closer it is to 0, the faster the convergence rate.
Next, consider the inhomogeneous linear iterative system
v(k+1) = Av(k) + b,
v(0) = v0,
(B.37)
where b a ﬁxed vector. A ﬁxed point is a vector v⋆that satisﬁes
v⋆= Av⋆+ b,
or, equivalently,
( I −A)v⋆= b,
(B.38)
where I is the identity matrix of the same size as A. Thus, if 1 is not an eigenvalue of
A (which cannot happen when A is convergent), then I −A is nonsingular, and so the
iterative system has a unique ﬁxed point.

B.7 Linear Functions and Systems
585
Theorem B.31.
Assume that 1 is not an eigenvalue of A. Then all solutions to
(B.37) converge to the ﬁxed point, v(k) →v⋆as k →∞if and only if A is a convergent
matrix.
Proof : Let u(k) = v(k) −v⋆, so that v(k) →v⋆if and only if u(k) →0. Now,
u(k+1) = v(k+1) −v⋆= (Av(k) + b) −(Av⋆+ b) = A(v(k) −v⋆) = Au(k),
and hence u(k) solves the homogeneous version (B.33). Thus, the result is an immediate
consequence of Deﬁnition B.27.
Q.E.D.
B.7 Linear Functions and Systems
The most basic structural features of linear diﬀerential equations, both ordinary and
partial, linear boundary value problems, etc., are founded on the concept of a linear function
between vector spaces.
Deﬁnition B.32. Let U and V be real vector spaces. A function L: U →V is called
linear if it obeys two basic rules:
L[u + v] = L[u] + L[v],
L[c u] = c L[u],
(B.39)
for all u, v ∈U and all scalars c.
We will refer to U as the domain space of the function L, and V as the target space.
The latter is to emphasize the fact that the range of L, namely
rng L = { v ∈V | v = L[u] for some u ∈U } ,
(B.40)
may very well be a proper subspace of the target space V .
Theorem B.33.
Every linear function L: Rn →Rm is given by matrix multiplica-
tion, L[v] = Av, where A is an m × n matrix.
Proving that matrix multiplication satisﬁes the linearity conditions (B.39) is easy. The
converse is established by seeing what the linear function does to the basis vectors of Rn;
see [89; Theorem 7.5].
Corollary B.34. Every linear function L: Rn →R is given by taking the dot product
with a ﬁxed vector a ∈Rn:
L[v] = a · v.
(B.41)
When U is a function space, a linear function is also referred to as a linear operator
in order to avoid confusion with the elements of U. If the target space V = R, then the
term linear functional is also often used for L: U →R.
Here are some representative examples that arise in applications.
Example B.35.
(a) Evaluation of a function at a point, namely L[f ] = f(x0),
deﬁnes a linear operator L: C0[a, b] →R.
(b) Integration,
I[f ] =
 b
a
f(x) dx,
(B.42)
also deﬁnes a linear functional I: C0[a, b] →R.

586
B Linear Algebra
(c) The operation Ma[f(x)] = a(x) f(x) of multiplication by a continuous function
a deﬁnes a linear operator Ma: C0[a, b] →C0[a, b].
(d) Diﬀerentiation of functions, D[f ] = f ′, serves to deﬁne a linear operator
D: C1[a, b] →C0[a, b].
(e) A general linear ordinary diﬀerential operator of order n,
L = an(x)Dn + an−1(x)Dn−1 + · · · + a1(x)D + a0(x),
(B.43)
is obtained by summing such operators. If the coeﬃcient functions a0(x), . . ., an(x) are
continuous, then
L[u] = an(x) dnu
dxn + an−1(x) dn−1u
dxn−1 + · · · + a1(x) du
dx + a0(x)u
(B.44)
deﬁnes a linear operator from Cn[a, b] to C0[a, b].
Linear partial diﬀerential equations are based on linear partial diﬀerential operators,
which are discussed in Chapter 1. They are particular examples of the general concept of
a linear system.
Deﬁnition B.36. A linear system is an equation of the form
L[u] = f,
(B.45)
in which L: U →V is a linear function, f ∈V , while the desired solution u ∈U. The
system is homogeneous if f = 0; otherwise, it is called inhomogeneous.
Note that, by the deﬁnition (B.40) of the range of L, the linear system (B.45) will
have a solution if and only if f ∈rng L. In particular, a homogeneous linear system always
has a solution, namely u = 0. However, it may possibly admit other, nonzero, solutions.
Theorem B.37. If z1, . . . , zk are all solutions to the same homogeneous linear system
L[z] = 0,
(B.46)
then any linear combination c1 z1 + · · · + ck zk, for any scalars c1, . . . , ck, is also a solution.
In other words, the set of solutions to a homogeneous linear system (B.46) forms a
subspace of the domain space U, known as the kernel of the linear function L:
ker L = { z ∈U | L[z] = 0 } .
(B.47)
Theorem B.38.
If the inhomogeneous linear system L[u] = f has a particular
solution u⋆, which requires f ∈rng L, then the general solution is u = u⋆+ z, where
z ∈ker L is any solution to the corresponding homogeneous system L[z] = 0.
The Superposition Principle for inhomogeneous linear systems allows us to combine
solutions corresponding to diﬀerent right-hand sides.
Theorem B.39. Suppose that for each i = 1, . . ., k, we know a particular solution
u⋆
i to the inhomogeneous linear system L[u] = fi for some fi ∈rng L. Then, given scalars
c1, . . . , ck, a particular solution to the combined inhomogeneous system
L[u] = c1 f1 + · · · + ck fk
(B.48)

B.7 Linear Functions and Systems
587
is the corresponding linear combination
u⋆= c1 u⋆
1 + · · · + ck u⋆
k
(B.49)
of particular solutions. The general solution to the inhomogeneous system (B.48) is
u = u⋆+ z = c1 u⋆
1 + · · · + ck u⋆
k + z,
(B.50)
where z ∈ker L is an arbitrary solution to the associated homogeneous system L[z] = 0.

References
[1] Abdulloev, K. O., Bogolubsky, I. L., and Makhankov, V. G., One more example of
inelastic soliton interaction, Phys. Lett. A 56 (1976), 427–428.
[2] Ablowitz, M. J., and Clarkson, P. A., Solitons, Nonlinear Evolution Equations and
the Inverse Scattering Transform, L.M.S. Lecture Notes in Math., vol. 149,
Cambridge University Press, Cambridge, 1991.
[3] Abraham, R., Marsden, J. E., and Ratiu, T., Manifolds, Tensor Analysis, and
Applications, Springer–Verlag, New York, 1988.
[4] Airy, G. B., On the intensity of light in the neighborhood of a caustic, Trans.
Cambridge Phil. Soc. 6 (1838), 379–402.
[5] Aki, K., and Richards, P. G., Quantitative Seismology, W. H. Freeman, San
Francisco, 1980.
[6] Ames, W. F., Numerical Methods for Partial Diﬀerential Equations, 3rd ed.,
Academic Press, New York, 1992.
[7] Antman, S. S., Nonlinear Problems of Elasticity, Appl. Math. Sci., vol. 107,
Springer–Verlag, New York, 1995.
[8] Apostol, T. M., Calculus, Blaisdell Publishing Co., Waltham, Mass., 1967–1969.
[9] Apostol, T. M., Introduction to Analytic Number Theory, Springer–Verlag, New
York, 1976.
[10] Atkinson, K., and Han, W., Spherical Harmonics and Approximations on the Unit
Sphere: An Introduction, Lecture Notes in Math., vol. 2044, Springer, Berlin,
2012.
[11] Bank, S. B., and Kaufman, R. P., A note on H¨older’s theorem concerning the
gamma function, Math. Ann. 232 (1978), 115–120.
[12] Batchelor, G. K., An Introduction to Fluid Dynamics, Cambridge University Press,
Cambridge, 1967.
[13] Bateman, H., Some recent researches on the motion of ﬂuids, Monthly Weather
Rev. 43 (1915), 63–170.
[14] Benjamin, T. B., Bona, J. L., and Mahony, J. J., Model equations for long waves
in nonlinear dispersive systems, Phil. Trans. Roy. Soc. London A 272 (1972),
47–78.
[15] Berest, Y., and Winternitz, P., Huygens’ principle and separation of variables,
Rev. Math. Phys. 12 (2000), 159–180.
[16] Berry, M. V., Marzoli, I., and Schleich, W., Quantum carpets, carpets of light,
Physics World 14(6) (2001), 39–44.
[17] Birkhoﬀ, G., Hydrodynamics — A Study in Logic, Fact and Similitude, 2nd ed.,
Princeton University Press, Princeton, 1960.
[18] Birkhoﬀ, G., and Rota, G.–C., Ordinary Diﬀerential Equations, Blaisdell Publ. Co.,
Waltham, Mass., 1962.
DOI 10.1007/978-3-
-
-0
319 02099
, © Springer International Publishing Switzerland 2014
589
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

590
References
[19] Black, F., and Scholes, M., The pricing of options and corporate liabilities, J.
Political Economy 81 (1973), 637–654.
[20] Blanchard, P., Devaney, R. L., and Hall, G. R., Diﬀerential Equations, Brooks–Cole
Publ. Co., Paciﬁc Grove, Calif., 1998.
[21] Boussinesq, J., Th´eorie des ondes et des remous qui se propagent le long d’un
canal rectangulaire horizontal, en communiquant au liquide contenu dans ce
canal des vitesses sensiblement pareilles de la surface au fond, J. Math. Pures
Appl. 17 (2) (1872), 55–108.
[22] Boussinesq, J., Essai sur la th´eorie des eaux courants, M´em. Acad. Sci. Inst. Nat.
France 23 (1) (1877), 1–680.
[23] Boyce, W. E., and DiPrima, R. C., Elementary Diﬀerential Equations and Boundary
Value Problems, 7th ed., John Wiley & Sons, Inc., New York, 2001.
[24] Bradie, B., A Friendly Introduction to Numerical Analysis, Prentice–Hall, Inc.,
Upper Saddle River, N.J., 2006.
[25] Bronstein, M., Symbolic integration I : Transcendental Functions, Springer–Verlag,
New York, 1997.
[26] Burgers, J. M., A mathematical model illustrating the theory of turbulence, Adv.
Appl. Mech. 1 (1948), 171–199.
[27] Cantwell, B. J., Introduction to Symmetry Analysis, Cambridge University Press,
Cambridge, 2003.
[28] Carleson, L., On the convergence and growth of partial sums of Fourier series,
Acta Math. 116 (1966), 135–157.
[29] Carmichael, R., The Theory of Numbers, Dover Publ., New York, 1959.
[30] Chen, G., and Olver, P. J., Dispersion of discontinuous periodic waves, Proc. Roy.
Soc. London 469 (2012), 20120407.
[31] Coddington, E. A., and Levinson, N., Theory of Ordinary Diﬀerential Equations,
McGraw–Hill, New York, 1955.
[32] Cole, J. D., On a quasilinear parabolic equation occurring in aerodynamics, Q.
Appl. Math. 9 (1951), 225–236.
[33] Courant, R., Friedrichs, K. O., and Lewy, H., ¨Uber die partiellen Diﬀerenzen-
gleichungen der mathematischen Physik, Math. Ann. 100 (1928), 32–74.
[34] Courant, R., and Hilbert, D., Methods of Mathematical Physics, vol. I,
Interscience Publ., New York, 1953.
[35] Courant, R., and Hilbert, D., Methods of Mathematical Physics, vol. II,
Interscience Publ., New York, 1953.
[36] Drazin, P. G., and Johnson, R. S., Solitons: An Introduction, Cambridge University
Press, Cambridge, 1989.
[37] Dym, H., and McKean, H. P., Fourier Series and Integrals, Academic Press, New
York, 1972.
[38] Evans, L. C., Partial Diﬀerential Equations, Grad. Studies Math. vol. 19, Amer.
Math. Soc., Providence, R.I., 1998.
[39] Feller, W., An Introduction to Probability Theory and Its Applications, 3rd ed., J.
Wiley & Sons, New York, 1968.
[40] Fermi, E., Pasta, J., and Ulam, S., Studies of nonlinear problems. I., preprint,
Los Alamos Report LA 1940, 1955; in: Nonlinear Wave Motion, A. C. Newell,
ed., Lectures in Applied Math., vol. 15, American Math. Soc., Providence,
R.I., 1974, pp. 143–156.
[41] Forsyth, A. R., The Theory of Diﬀerential Equations, Cambridge University Press,
Cambridge, 1890, 1900, 1902, 1906.
[42] Fourier, J., The Analytical Theory of Heat, Dover Publ., New York, 1955.

References
591
[43] Gander, M. J., and Kwok, F., Chladni ﬁgures and the Tacoma bridge: motivating
PDE eigenvalue problems via vibrating plates, SIAM Review 54 (2012),
573–596.
[44] Garabedian, P., Partial Diﬀerential Equations, 2nd ed., Chelsea Publ. Co., New
York, 1986.
[45] Gardner, C. S., Greene, J. M., Kruskal, M. D., and Miura, R. M., Method for
solving the Korteweg–deVries equation, Phys. Rev. Lett. 19 (1967), 1095–1097.
[46] Gonzalez, R. C., and Woods, R. E., Digital Image Processing, 2nd ed.,
Prentice–Hall, Inc., Upper Saddle River, N.J., 2002.
[47] Gordon, C., Webb, D. L., and Wolpert, S., One cannot hear the shape of a drum,
Bull. Amer. Math. Soc. 27 (1992), 134–138.
[48] Gradshteyn, I. S., and Ryzhik, I. W., Table of Integrals, Series and Products,
Academic Press, New York, 1965.
[49] Gurtin, M. E., An Introduction to Continuum Mechanics, Academic Press, New
York, 1981.
[50] Haberman, R., Elementary Applied Partial Diﬀerential Equations, 3rd ed.,
Prentice–Hall, Inc., Upper Saddle River, NJ, 1998.
[51] Hairer, E., Lubich, C., and Wanner, G., Geometric Numerical Integration,
Springer–Verlag, New York, 2002.
[52] Hale, J. K., Ordinary Diﬀerential Equations, 2nd ed., R.E. Krieger Pub. Co.,
Huntington, N.Y., 1980.
[53] Henrici, P., Applied and Computational Complex Analysis, vol. 1, J. Wiley &
Sons, New York, 1974.
[54] Hille, E., Ordinary Diﬀerential Equations in the Complex Domain, John Wiley &
Sons, New York, 1976.
[55] Hobson, E. W., The Theory of Functions of a Real Variable and the Theory of
Fourier’s Series, Dover Publ., New York, 1957.
[56] Hopf, E., The partial diﬀerential equation ut + uux = μu, Commun. Pure Appl.
Math. 3 (1950), 201–230.
[57] Howison, S., Practical Applied Mathematics: Modelling, Analysis, Approximation,
Cambridge University Press, Cambridge, 2005.
[58] Hydon, P. E., Symmetry Methods for Diﬀerential Equations, Cambridge Texts in
Appl. Math., Cambridge University Press, Cambridge, 2000.
[59] Ince, E. L., Ordinary Diﬀerential Equations, Dover Publ., New York, 1956.
[60] Iserles, A., A First Course in the Numerical Analysis of Diﬀerential Equations,
Cambridge University Press, Cambridge, 1996.
[61] Jost, J., Partial Diﬀerential Equations, Graduate Texts in Mathematics, vol. 214,
Springer–Verlag, New York, 2007.
[62] Kamke, E., Diﬀerentialgleichungen L¨osungsmethoden und L¨osungen, vol. 1, Chelsea,
New York, 1971.
[63] Keller, H. B., Numerical Methods for Two-Point Boundary-Value Problems, Blaisdell,
Waltham, MA, 1968.
[64] Knobel, R., An Introduction to the Mathematical Theory of Waves, American
Mathematical Society, Providence, RI, 2000.
[65] Korteweg, D. J., and de Vries, G., On the change of form of long waves
advancing in a rectangular channel, and on a new type of long stationary
waves, Phil. Mag. (5) 39 (1895), 422–443.
[66] Landau, L. D., and Lifshitz, E. M., Quantum Mechanics (Non-relativistic Theory),
Course of Theoretical Physics, vol. 3, Pergamon Press, New York, 1977.
[67] Levine, I. N., Quantum Chemistry, 5th ed., Prentice–Hall, Inc., Upper Saddle
River, N.J., 2000.

592
References
[68] Lighthill, M. J., Introduction to Fourier Analysis and Generalised Functions,
Cambridge University Press, Cambridge, 1970.
[69] Lin, C. C., and Segel, L. A., Mathematics Applied to Deterministic Problems in the
Natural Sciences, SIAM, Philadelphia, 1988.
[70] McOwen, R. C., Partial Diﬀerential Equations: Methods and Applications,
Prentice–Hall, Inc., Upper Saddle River, N.J., 2002.
[71] Merton, R. C., Theory of rational option pricing, Bell J. Econ. Management Sci. 4
(1973), 141–183.
[72] Messiah, A., Quantum Mechanics, John Wiley & Sons, New York, 1976.
[73] Miller, W., Jr., Symmetry and Separation of Variables, Encyclopedia of
Mathematics and Its Applications, vol. 4, Addison–Wesley Publ. Co., Reading,
Mass., 1977.
[74] Milne–Thompson, L. M., The Calculus of Finite Diﬀerences, Macmillan and Co.,
Ltd., London, 1951.
[75] Misner, C. W., Thorne, K. S., and Wheeler, J. A., Gravitation, W. H. Freeman, San
Francisco, 1973.
[76] Miura, R. M., Gardner, C. S., and Kruskal, M. D., Korteweg–deVries equation
and generalizations. II. Existence of conservation laws and constants of the
motion, J. Math. Phys. 9 (1968), 1204–1209.
[77] Moon, F. C., Chaotic Vibrations, John Wiley & Sons, New York, 1987.
[78] Moon, P., and Spencer, D. E., Field Theory Handbook, Springer-Verlag, New York,
1971.
[79] Morse, P. M., and Feshbach, H., Methods of Theoretical Physics, McGraw–Hill, New
York, 1953.
[80] Morton, K. W., and Mayers, D. F., Numerical Solution of Partial Diﬀerential
Equations, 2nd ed., Cambridge University Press, Cambridge, 2005.
[81] Murray, J. D., Mathematical Biology, 3rd ed., Springer-Verlag, New York,
2002–2003.
[82] Oberhettinger, F., Tables of Fourier Transforms and Fourier Transforms of
Distributions, Springer-Verlag, New York, 1990.
[83] Øksendal, B., Stochastic Diﬀerential Equations: An Introduction with Applications,
Springer–Verlag, New York, 1985.
[84] Okubo, A., Diﬀusion and Ecological Problems: Mathematical Models,
Springer-Verlag, New York, 1980.
[85] Olver, F. W. J., Asymptotics and Special Functions, Academic Press, New York,
1974.
[86] Olver, F. W. J., Lozier, D. W., Boisvert, R. F., and Clark, C. W., eds., NIST
Handbook of Mathematical Functions, Cambridge University Press, Cambridge,
2010.
[87] Olver, P. J., Applications of Lie Groups to Diﬀerential Equations, 2nd ed.,
Graduate Texts in Mathematics, vol. 107, Springer–Verlag, New York, 1993.
[88] Olver, P. J., Dispersive quantization, Amer. Math. Monthly 117 (2010), 599–610.
[89] Olver, P. J., and Shakiban, C., Applied Linear Algebra, Prentice–Hall, Inc., Upper
Saddle River, N.J., 2005.
[90] Oskolkov, K. I., A class of I. M. Vinogradov’s series and its applications in
harmonic analysis, in: Progress in Approximation Theory , Springer Ser.
Comput. Math., 19, Springer, New York, 1992, pp. 353–402.
[91] Pinchover, Y., and Rubinstein, J., An Introduction to Partial Diﬀerential
Equations, Cambridge University Press, Cambridge, 2005.
[92] Pinsky, M. A., Partial Diﬀerential Equations and Boundary–Value Problems with
Applications, 3rd ed., McGraw–Hill, New York, 1998.

References
593
[93] Polyanin, A. D., and Zaitsev, V. F., Handbook of Exact Solutions for Ordinary
Diﬀerential Equations, 2nd ed., Chapman & Hall/CRC, Boca Raton, Fl.,
2003.
[94] Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P., Numerical
Recipes: The Art of Scientiﬁc Computing, 3rd ed., Cambridge University
Press, Cambridge, 2007.
[95] Reed, M., and Simon, B., Methods of Modern Mathematical Physics, Academic
Press, New York, 1972.
[96] Royden, H. L., and Fitzpatrick, P. M., Real Analysis, 4th ed., Pearson Education
Inc., Boston, MA, 2010.
[97] Rudin, W., Principles of Mathematical Analysis, 3rd ed., McGraw–Hill, New York,
1976.
[98] Rudin, W., Real and Complex Analysis, 3rd ed., McGraw–Hill, New York, 1987.
[99] Salsa, S., Partial Diﬀerential Equations in Action: From Modelling to Theory,
Springer–Verlag, New York, 2008.
[100] Sapiro, G., Geometric Partial Diﬀerential Equations and Image Analysis,
Cambridge University Press, Cambridge, 2001.
[101] Schr¨odinger, E., Collected Papers on Wave Mechanics, Chelsea Publ. Co., New
York, 1982.
[102] Schumaker, L. L., Spline Functions: Basic Theory, John Wiley & Sons, New York,
1981.
[103] Schwartz, L., Th´eorie des distributions, Hermann, Paris, 1957.
[104] Scott Russell, J., On waves, in: Report of the 14th Meeting, British Assoc. Adv.
Sci., 1845, pp. 311–390.
[105] Sethares, W. A., Tuning, Timbre, Spectrum, Scale, Springer–Verlag, New York,
1999.
[106] Siegel, C. L., ¨Uber einige Anwendungen diophantischer Approximationen, in:
Gesammelte Abhandlungen, vol. 1, Springer–Verlag, New York, 1966, pp.
209–266.
[107] Smoller, J., Shock Waves and Reaction–Diﬀusion Equations, 2nd ed.,
Springer-Verlag, New York, 1994.
[108] Stewart, J., Calculus: Early Transcendentals, vols. 1 & 2, 7th ed., Cengage
Learning, Mason, OH, 2012.
[109] Stokes, G. G., On a diﬃculty in the theory of sound, Phil. Mag. 33(3) (1848),
349–356.
[110] Stokes, G. G., Mathematical and Physical Papers, Cambridge University Press,
Cambridge, 1880–1905.
[111] Stokes, G. G., Mathematical and Physical Papers, 2nd ed., Johnson Reprint Corp.,
New York, 1966.
[112] Strang, G., Introduction to Applied Mathematics, Wellesley Cambridge Press,
Wellesley, Mass., 1986.
[113] Strang, G., and Fix, G. J., An Analysis of the Finite Element Method,
Prentice–Hall, Inc., Englewood Cliﬀs, N.J., 1973.
[114] Strauss, W. A., Partial Diﬀerential Equations: An Introduction, John Wiley &
Sons, New York, 1992.
[115] Thaller, B., Visual Quantum Mechanics, Springer–Verlag, New York, 2000.
[116] Thijssen, J., Computational Physics, Cambridge University Press, Cambridge, 1999.
[117] Titchmarsh, E. C., Theory of Functions, Oxford University Press, London, 1968.
[118] Varga, R. S., Matrix Iterative Analysis, 2nd ed., Springer–Verlag, New York, 2000.

594
[119] Watson, G. N., A Treatise on the Theory of Bessel Functions, Cambridge
University Press, Cambridge, 1952.
[120] Weinberger, H. F., A First Course in Partial Diﬀerential Equations, Dover Publ.,
New York, 1995.
[121] Wiener, N., I Am a Mathematician, Doubleday, Garden City, N.Y., 1956.
[122] Whitham, G. B., Linear and Nonlinear Waves, John Wiley & Sons, New York,
1974.
[123] Wilmott, P., Howison, S., and Dewynne, J., The Mathematics of Financial
Derivatives, Cambridge University Press, Cambridge, 1995.
[124] Yong, D., Strings, chains, and ropes, SIAM Review 48 (2006), 771–781.
[125] Zabusky, N. J., and Kruskal, M. D., Interaction of “solitons” in a collisionless
plasma and the recurrence of initial states, Phys. Rev. Lett. 15 (1965),
240–243.
[126] Zienkiewicz, O. C., and Taylor, R. L., The Finite Element Method, 4th ed.,
McGraw–Hill, New York, 1989.
[127] Zwillinger, D., Handbook of Diﬀerential Equations, Academic Press, Boston, 1992.
[128] Zygmund, A., Trigonometric Series, 3rd ed., Cambridge University Press,
Cambridge, 2002.
References

Symbol Index
Symbol
Meaning
Page(s)
c + d
addition of scalars
575
z + w
complex addition
571
A + B
addition of matrices
575
v + w
addition of vectors
575
f + g
addition of functions
575
z w
complex multiplication
571
z/w
complex division
572
cv, cA, cf
scalar multiplication
575
z
complex conjugate
571
Ω
closure of subset or domain
243
0
zero vector
xvi, 575
> 0
positive deﬁnite
355, 578
≥0
positive semi-deﬁnite
355
f −1
inverse function
xvi
A−1
inverse matrix
xvi
f(x+), f(x−)
one-sided limits
xvi
n!
factorial
163, 453
n
k

binomial coeﬃcient
163
| · |
absolute value, modulus
94, 225, 571
∥· ∥
norm
73, 89, 106, 284, 356,
578, 579, 581
∥· ∥
double norm
380
|∥· ∥|
norm
356
v · w
dot product
578
z · w
Hermitian dot product
580
⟨· ⟩
expected value
287
⟨· , · ⟩
inner product
73, 89, 107, 285, 341,
578, 579, 581
⟨⟨· , · ⟩⟩
inner product
341
[0, 1]
closed interval
xvi
{ f | C }
set
xvi
∈
element of
xvi
595
DOI 10.1007/978-3-
-
-0
319 02099
, © Springer International Publishing Switzerland 2014
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

596
Symbol Index
̸∈
not element of
xvi
⊂, ⊊
subset
xvi
∪
union
xvi
∩
intersection
xvi
\
set theoretic diﬀerence
xvi
:=
deﬁnition of symbol
xvi
≡
identical equality of functions
xvi
≡
equivalence in modular arithmetic
xvi
◦
composition
xvi
∗
convolution
95, 281
L∗
adjoint operator
341
∼
Fourier series representation
74
∼
asymptotic equality
300
f: X →Y
function
xvi
xn →x
convergent sequence
xvi
fn ⇀f
weak convergence
230
f(x+), f(x−)
one-sided limits
41, 79
u′, u′′, . . .
space derivatives
xvii
u,
u, . . .
time derivatives
xvii
ux, uxx, utx, . . .
partial derivatives
xvii, 1
du
dx , d2u
dx2 , . . .
ordinary derivatives
xvii, 1
∂
partial derivative
xvii, 1
∂
boundary of domain
5, 152, 504
∂u
∂x , ∂2u
∂x2 , ∂2u
∂t ∂x , . . .
partial derivatives
xvii, 1
∂x, ∂
∂x
partial derivative operator
2
∂
∂n
normal derivative
153, 244, 504
∇
gradient
150, 242, 345, 505
∇·
divergence
242, 347, 505
∇×
curl
242
∇2
Laplacian
243
□
wave operator
50
n

i=1
summation
xvi

f(x) dx
indeﬁnite integral
xvii
 b
a
f(x) dx
deﬁnite integral
xvii

Symbol Index
597
 ∞
−∞
−
f(x) dx
principal value integral
283
 
Ω
f(x, y) dx dy
double integral
243
  
Ω
f(x, y, z) dx dy dz
triple integral
505

C
f(s) ds
line integral with respect to arc length
244

C
v dx
line integral
243
&
C
v dx
line integral around closed curve
243
 
∂Ω
f dS
surface integral
505
a
Bohr radius
567
A
space of analytic functions
576
ak
Fourier coeﬃcient
74, 89
Ai
Airy function
327, 460
arg
argument (see phase)
xvi, 573
b
ﬁnite element vector
401
B
magnetic ﬁeld
551
bk
Fourier coeﬃcient
74, 89
Bi
Airy function of the second kind
462
c
wave speed
19, 24, 50, 486, 546
c
ﬁnite element coeﬃcient vector
401
C
complex numbers
xv, 571
cg
group velocity
331
ck
complex Fourier coeﬃcient
89
ck
eigenfunction series coeﬃcient
378
cp
phase velocity
330
C0
space of continuous functions
108, 576
Cn
space of diﬀerentiable functions
5, 576
C∞
space of smooth functions
576
Cn
n-dimensional complex space
xv, 575
coker
cokernel
350
cos
cosine
6, 89
cosh
hyperbolic cosine
88
coth
hyperbolic cotangent
91, 317
csc
cosecant
230
curl
curl (see also ∇×)
242
d
ordinary derivative
xvii, 1
D
derivative operator
342, 585
D
domain
5

598
Symbol Index
det
determinant
582
dim
dimension
577
div
divergence (see also ∇·)
242
ds
arc length element
244
dS
surface area element
505
e
base of natural logarithm
xvi
E
energy
61, 132, 151
E
electric ﬁeld
551
ex
exponential
5
ez
complex exponential
573
ei
standard basis vector
216, 577
erf
error function
55
erfc
complementary error function
302
f
periodic extension
77
F
function space
575
F
Fourier transform
264
F −1
inverse Fourier transform
265
F(t, x; ξ)
fundamental solution
292, 387, 481, 543
G(x; ξ), Gξ(x)
Green’s function
234, 240, 248, 527
G(t, x; τ, ξ)
general fundamental solution
297
h
step size
182
ℏ
Planck’s constant
6, 287, 394
Hn
Hermite polynomial
311
Hm
n , Hm
n
harmonic polynomial
520
i =
√
−1
imaginary unit
571
I
identity matrix
575
Im
imaginary part
571
Jm
Bessel function
468
k
frequency variable
264
k
wave number
330
K
ﬁnite element matrix
401
K[u]
right hand side of evolution equation
291
kν
ij
elemental stiﬀness
417
Km
n , Km
n
complementary harmonic function
523
ker
kernel
350, 577
l
angular quantum number
568
L2
Hilbert space
106, 284
Lk
Laguerre polynomial
566
Lj
k
generalized Laguerre polynomial
566
L[u]
linear function/operator
10, 64, 585
lim
x →a ,
lim
n →∞
limits
xvi

Symbol Index
599
lim
x →a−,
lim
x →a+
one-sided limits
xvi
log
natural or complex logarithm
xvi, 573
m
mass
6
m
magnetic quantum number
568
M
electron mass
564
Mr, Mx
r
spherical mean
553
max
maximum
xvi
min
minimum
xvi
mod
modular arithmetic
xvi
n
principal quantum number
568
n
unit normal
153, 244, 505
N
natural numbers
xv
O
zero matrix
575
O(h)
Big Oh notation
182
p
pressure
3
p
option exercise price
299
P
P´eclet number
311
Pn
Legendre polynomial
511, 525
pm
n
trigonometric Ferrers function
515
P m
n
Ferrers (associated Legendre) function
513
P(n)
space of polynomials of degree ≤n
577
ph
phase (argument)
xvi, 572
Q[u]
quadratic function(al)
362
r
radial coordinate
xv, 3, 160, 572
r
cylindrical radius
xv, 3, 508
r
spherical radius
xv, 3, 508
r
interest rate
299
R
real numbers
xv
Rn
n-dimensional Euclidean space
xv, 575
R[u]
Rayleigh quotient
375
Re
real part
571
rng
range
576
s
arc length
244
S
surface area
505
Sm
spherical Bessel function
539
sn
partial sum
75, 113
Sr, Sx
r
sphere of radius r
553, 555
sech
hyperbolic secant
334
sign
sign function
94, 225
sin
sine
6, 89
sinh
hyperbolic sine
88

600
Symbol Index
span
span
576
supp
support
407
t
time
xv, 3
T
conserved density
38, 256
AT
transpose of matrix
341, 578
Tν
ﬁnite element triangle
411
tan
tangent
1
tanh
hyperbolic tangent
135
u
dependent variable
xv, 3
ux, uxx, . . .
partial derivative
1
v
dependent variable
xv, 3
v
eigenvector/eigenfunction
371
v
vector
xv, 575
v
eigenvector
66, 582
v
vector ﬁeld
3, 242
V
vector space
575
V
potential function
6
v⊥
perpendicular vector
244
vlmn
atomic eigenfunction
568
Vλ
eigenspace
371
w
dependent variable
xv, 3
w
heat ﬂux
122
w
heat ﬂux vector
437
x
Cartesian space coordinate
xv, 3, 152, 504
x
real part of complex number
571
X
ﬂux
38, 256
y
Cartesian space coordinate
xv, 3, 152, 504
y
imaginary part of complex number
571
Y
ﬂux
256
Ym
Bessel function of the second kind
470
Y m
n , Y m
n
spherical harmonic
517
Ym
n
complex spherical harmonic
519
z
Cartesian space coordinate
xv, 3, 504
z
cylindrical coordinate
xv, 3, 508
z
complex number
571
Z
integers
xv
α
electron charge
564
βn
l
radial wave function
568
γ
thermal diﬀusivity
124, 438, 535
γ
Euler–Mascheroni constant
471
Γ
gamma function
454

Symbol Index
601
δ, δξ
delta function
217, 219, 246, 247, 527
δ
periodically extended delta function
229
δ ′, δ ′
ξ
derivative of delta function
225, 226
Δ
Laplacian
4, 152, 161, 243,
504, 509
Δ
discriminant
172, 173
Δx
step size
186
Δx
variance
287
ΔS
spherical Laplacian
509
ε
thermal energy density
122, 437
ϵ0
permittivity constant
551
ζm,n
Bessel root
474
η
characteristic variable
51
θ
polar angle
xv, 3, 160, 572
θ
cylindrical angle
xv, 3, 508
θ
azimuthal angle
xv, 3, 508
ζ
root of unity
582
κ
thermal conductivity
65, 123, 437
κ
stiﬀness or tension
49
λ
eigenvalue
66, 371, 573
λ
magniﬁcation factor
189
μ0
permeability constant
551
ν
viscosity
3
ξ
characteristic variable
19, 25, 32, 51
π
area of unit circle
5
ρ
density
49, 122, 438
ρ
spectral radius
584
ρ, ρξ
ramp function
91, 223
ρn, ρn,ξ
nth order ramp function
95, 223
ρm,n
relative vibrational frequency
495
σ
shock position
41
σ
heat capacity
65, 122, 438
σ
volatility
299
σ, σξ
unit step function
61, 80, 222
σm,n
spherical Bessel root
540
ϕ
zenith angle
xv, 3, 508
ϕ
wave function
286
ϕk
orthogonal or orthonormal system
109
ϕk
basis for ﬁnite element subspace
401
χ
speciﬁc heat capacity
122, 431
χD
characteristic function
485

602
Symbol Index
ψ
time-dependent wave function
394, 564
ω
frequency
59, 330
Ω
domain
152, 242, 504

Author Index
Abdulloev, K. O.
337, [1]
Ablowitz, M. J.
283, 292, 324, 333, 337,
338, [2]
Abraham, R.
161, [3]
Airy, G. B.
327, 334, [4]
Aki, K.
549, [5]
Ames, W. F.
181, 213, 400, 410, [6]
Antman, S. S.
324, 486, 549, [7]
Apostol, T. M.
5, 20, 76, 87, 100, 105,
169, 182, 236, 242, 245, 267, 312,
437, 500, 505, [8], [9]
Atkinson, K.
519, [10]
Bank, S. B.
455, [11]
Batchelor, G. K.
3, [12]
Bateman, H.
315, [13]
Benjamin, T. B.
337, [14]
Berest, Y.
563, [15]
Bernoulli, J.
xvii, 452
Berry, M. V.
329, [16]
Bessel, F. W.
111, 452
Birkhoﬀ, G.
ix, 2, 11, 29, 67, 298, 305,
309, 457, [17], [18]
Black, F.
299, [19]
Blanchard, P.
ix, 2, 11, 22, 25, 29, 65,
68, [20]
Bogolubsky, I. L.
337, [1]
Bohr, N. H. D.
567
Boisvert, R. F.
xvi, 55, 310, 327, 364,
435, 452, 468, 511, 512, 513, 567,
573, [86]
Bona, J. L.
337, [14]
Bourget, J.
490
Boussinesq, J.
292, 333, 335, [21], [22]
Boyce, W. E.
ix, 2, 11, 22, 25, 29, 65,
67, 68, 162, 169, 263, 298, 300,
309, 466, [23]
Bradie, B.
ix, 135, 185, 407, 453, [24]
Bronstein, M.
267, [25]
Burgers, J. M.
315, [26]
Cantor, G. F. L. P.
xvii, 64
Cantwell, B. J.
305, [27]
Carleson, L.
117, 231, [28]
Carmichael, R.
500, [29]
Cauchy, A. L.
xvii, 175, 215
Chen, G.
329, [30]
Chladni, E. F. F.
497
Clark, C. W.
xvi, 55, 310, 327, 364, 435,
452, 468, 511, 512, 513, 567, 573,
[86]
Clarkson, P. A.
283, 292, 324, 333, 337,
338, [2]
Coddington, E. A.
2, [31]
Cole, J. D.
318, [32]
Courant, R.
xvii, 4, 175, 177, 178, 197,
246, 255, 340, 377, 436, 440, 449,
477, 497, 541, [33], [34], [35]
Crank, J.
192
d’Alembert, J. L. R.
xvii, 15, 50, 140,
149, 558
de Broglie, L. V. P. R.
287
de Coulomb, C.–A.
252, 503
Devaney, R. L.
ix, 2, 11, 22, 25, 29, 65,
68, [20]
de Vries, G.
333, [65]
Dewynne, J.
299, [123]
DiPrima, R. C.
ix, 2, 11, 22, 25, 29, 65,
67, 68, 162, 169, 263, 298, 300,
309, 466, [23]
Dirac, P. A. M.
217
Dirichlet, J. P. G. L.
7, 368
Drazin, P. G.
38, 292, 324, 333, 337,
338, [36]
du Bois–Reymond, P. D. G.
430
Duhamel, J. M. C.
298
Dym, H.
76, 99, 107, 115, 117, 263, 265,
275, 286, 344, [37]
Einstein, A.
19, 31, 63, 149, 504
Euler, L.
xvii, 3, 454, 461, 573
Evans, L. C.
xvii, 4, 314, 340, 350, 427,
436, 535, 546, [38]
Feller, W.
55, 295, [39]
Fermi, E.
333, [40]
Ferrers, N. M.
513
Feshbach, H.
169, 508, 569, [79]
Fitzpatrick, P. M.
76, 100, 102, 107,
108, 119, 217, 219, 344, [96]
Fix, G. J.
399, 400, 410, 431, [113]
Flannery, B. P.
ix, 135, 181, [94]
Forsyth, A. R.
317, [41]
Fourier, J.
xvii, 63, 64, 71, 114, 123,
149, 437, 452, 535, [42]
Fredholm, E. I.
350
DOI 10.1007/978-3-
-
-0
319 02099
, © Springer International Publishing Switzerland 2014
603
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

604
Author Index
Friedrichs, K. O.
197, [33]
Frobenius, F. G.
464
Galilei, G.
20
Gander, M. J.
497, [43]
Garabedian, P.
xvii, 4, 172, 175, 246,
340, 350, 376, 377, 427, [44]
Gardner, C. S.
336, 338, [45], [76]
Gauss, J. C. F.
63
Germain, M.–S.
497
Gibbs, J. W.
84
Gonzalez, R. C.
442, [46]
Gordon, C.
487, [47]
Gradshteyn, I. S.
334, [48]
Green, G.
3, 215, 243
Greene, J. M.
336, [45]
Gregory, J.
78
Gurtin, M. E.
256, 549, [49]
Haberman, R.
xvii, [50]
Hairer, E.
181, [51]
Hale, J. K.
ix, 2, 11, 29, 67, [52]
Hall, G. R.
ix, 2, 11, 22, 25, 29, 65, 68,
[20]
Han, W.
519, [10]
Heaviside, O.
215, 217, 551
Heisenberg, W. K.
286, 288
Henrici, P.
256, [53]
Hilbert, D.
xvii, 4, 106, 175, 177, 178,
246, 255, 340, 368, 377, 436, 440,
449, 477, 497, 541, [34], [35]
Hille, E.
ix, 2, 453, 457, 459, 463, [54]
Hobson, E. W.
329, [55]
Hopf, E.
318, [56]
Howison, S.
vii, viii, 43, 46, 299, [57],
[123]
Hugoniot, P. H.
40
Huygens, C.
560
Hydon, P. E.
305, [58]
Ince, E. L.
ix, 2, 453, 457, 459, 463, 472,
[59]
Iserles, A.
ix, 181, 185, 453, [60]
Johnson, R. S.
38, 292, 324, 333, 337,
338, [36]
Jost, J.
xvii, 4, 246, 255, 314, 340, 350,
427, 535, 546, [61]
Kamke, E.
453, [62]
Kaufman, R. P.
455, [11]
Keller, H. B.
185, 355, 364, [63]
Kelvin, L. (Thomson, W.)
3, 41, 139,
331
Kirchhoﬀ, G. R.
503, 558
Knobel, R.
317, [64]
Korteweg, D. J.
333, [65]
Kovalevskaya, S. V.
175
Kruskal, M. D.
333, 336, 338, [45], [76],
[125]
Kwok, F.
497, [43]
Lagrange, J.–L.
xvii, 182
Laguerre, E. N.
566
Landau, L. D.
108, 278, 288, 383, 394,
[66]
Laplace, P.–S.
xvii, 152
Lebesgue, H. L.
107, 112
Legendre, A.–M.
454, 511
Leibniz, G. W.
1, 78
Levine, I. N.
569, [67]
Levinson, N.
2, [31]
Lewy, H.
197, [33]
Lie, M. S.
305
Lifshitz, E. M.
108, 278, 288, 383, 394,
[66]
Lighthill, M. J.
76, 233, 263, 286, [68]
Lin, C. C.
vii, viii, [69]
Liouville, J.
363
Lozier, D. W.
xvi, 55, 310, 327, 364,
435, 452, 468, 511, 512, 513, 567,
573, [86]
Lubich, C.
181, [51]
Mahony, J. J.
337, [14]
Makhankov, V. G.
337, [1]
Marsden, J. E.
161, [3]
Marzoli, I.
329, [16]
Maxwell, J. C.
551
Mayers, D. F.
181, 200, 213, 453, [80]
McKean, H. P.
76, 99, 107, 115, 117,
263, 265, 275, 286, 344, [37]
McOwen, R. C.
xvii, 56, 122, 178, 246,
255, [70]
Mendeleev, D. I.
568
Merton, R. C.
299, [71]
Messiah, A.
108, 278, 288, 383, 394,
565, [72]
Miller, W., Jr.
305, [73]
Milne–Thompson, L. M.
185, [74]
Minkowski, H.
56, 560
Misner, C. W.
56, 504, [75]
Miura, R. M.
336, 338, [45], [76]
Moon, F. C.
60, [77]
Moon, P.
169, 508, [78]
Morse, P. M.
169, 508, 569, [79]
Morton, K. W.
181, 200, 213, 453, [80]
Murray, J. D.
438, [81]
Navier, C. L. M. H.
3
Neumann, C. G.
7
Newton, I.
vii, 49, 63, 135, 149, 182,
252, 388, 503
Nicolson, P.
192

605
Oberhettinger, F.
273, [82]
Okubo, A.
438, [84]
Olver, F. W. J.
xvi, 55, 310, 327, 331,
364, 435, 452, 453, 461, 462, 468,
470, 472, 474, 511, 512, 513, 567,
573, [85], [86]
Olver, P. J.
ix, xi, xv, xviii, 11, 27, 38,
39, 65, 66, 67, 73, 75, 98, 110,
162, 191, 210, 212, 300, 305, 308,
310, 311, 329, 338, 350, 357, 363,
372, 378, 390, 400, 402, 407, 408,
411, 575, 579, 581, 582, 583, 584,
585, [30], [87], [88], [89]
Oskolkov, K. I.
332, [90]
Øksendal, B.
viii, 299, [83]
Parseval des Chˆenes, M.–A.
114
Pasta, J.
333, [40]
Pauli, W. E.
568
Pinchover, Y.
xvii, [91]
Pinsky, M. A.
xvii, [92]
Plancherel, M.
114
Planck, M. K. E. L.
287, 394
Poisson, S. D.
31, 152, 503, 558
Polyanin, A. D.
453, [93]
Press, W. H.
ix, 135, 181, [94]
Rankine, W. J. M.
40
Ratiu, T.
161, [3]
Rayleigh, L. (Strutt, J.W.)
40, 375
Reed, M.
344, 374, 383, 565, [95]
Richards, P. G.
549, [5]
Richardson, L. F.
194
Riemann, G. F. B.
xvii, 31, 63, 87, 112
Robin, V. G.
123
Rota, G.–C.
ix, 2, 11, 29, 67, 298, 309,
457, [18]
Royden, H. L.
76, 100, 102, 107, 108,
119, 217, 219, 344, [96]
Rubinstein, J.
xvii, [91]
Rudin, W.
5, 76, 100, 102, 105, 107,
108, 119, 169, 182, 217, 219, 256,
263, 267, 344, [97], [98]
Ryzhik, I. W.
334, [48]
Salsa, S.
xvii, 4, 122, 340, 350, 427, 436,
535, 546, [99]
Sapiro, G.
442, [100]
Schleich, W.
329, [16]
Scholes, M.
299, [19]
Schr¨odinger, E.
394, [101]
Schumaker, L. L.
210, 400, 408, [102]
Schwartz, L.
217, [103]
Scott Russell, J.
334, [104]
Segel, L. A.
vii, viii, [69]
Sethares, W. A.
144, [105]
Shakiban, C.
ix, xv, xviii, 11, 27, 65,
66, 67, 73, 75, 98, 110, 162, 191,
210, 212, 300, 350, 357, 363, 372,
378, 390, 400, 402, 407, 408, 411,
575, 579, 581, 582, 583, 584, 585,
[89]
Siegel, C. L.
490, [106]
Simon, B.
344, 374, 383, 565, [95]
Smoller, J.
317, 427, 434, 438, [107]
Spencer, D. E.
169, 508, [78]
Stewart, J.
5, 20, 105, 236, 242, 245,
312, 407, 437, 505, [108]
Stokes, G. G.
3, 41, 331, [109], [110],
[111]
Strang, G.
xvii, 357, 399, 400, 410, 431,
[112], [113]
Strauss, W. A.
xvii, [114]
Strutt, J.W. see Rayleigh, L.
Sturm, F. O. R.
363
Talbot, W. H. F.
329
Taylor, B.
75
Taylor, R. L.
400, 410, 431, [126]
Teukolsky, S. A.
ix, 135, 181, [94]
Thaller, B.
108, 329, 394, [115]
Thijssen, J.
565, [116]
Thomson, W. see Kelvin, L.
Thorne, K. S.
56, 504, [75]
Titchmarsh, E. C.
263, 265, 275, 286,
[117]
Ulam, S.
333, [40]
Varga, R. S.
213, 402, 411, [118]
Vetterling, W. T.
ix, 135, 181, [94]
Victoria, Q.
139
von Helmholtz, H. L. F.
374
von Neumann, J.
190
Wanner, G.
181, [51]
Watson, G. N.
452, 470, 472, 474, 490,
[119]
Webb, D. L.
487, [47]
Weierstrass, K. T. W.
xvii, 100, 329
Weinberger, H. F.
xvii, 50, 447, 525,
[120]
Wheeler, J. A.
56, 504, [75]
Whitham, G. B.
31, 44, 46, 178, 315,
316, 317, 324, 331, 427, 434, [122]
Wiener, N.
254, [121]
Wilmott, P.
299, [123]
Winternitz, P.
563, [15]
Wolpert, S.
487, [47]
Woods, R. E.
442, [46]
Yong, D.
50, [124]
Zabusky, N. J.
333, [125]
Author Index

606
Zaitsev, V. F.
453, [93]
Zienkiewicz, O. C.
400, 410, 431, [126]
Zwillinger, D.
453, [127]
Zygmund, A.
76, 99, 102, 105, 115, 117,
[128]
Author Index

Subject Index
absolute
convergence
101
value
86, 94, 105, 225
zero
139
abstraction
339
acceleration
7, 49
accidental degeneracy
499
acoustics
2, 15, 31
acoustic wave
15
Acta Numerica
181
addition
571, 575
identity
575
inverse
575
adjoint
339, 341, 344, 428, 438, 505
formal
344
system
350
weighted
342
advection
322
aerodynamics
172
aﬃne
404
element
414, 416
piecewise
404, 411
afterglow
562, 563
air
15, 174, 551
airplane
15, 173, 174
Airy
diﬀerential equation
281, 459
function
327, 364, 435, 461
second kind
462
algebra
263, 273, 275, 453
linear
viii, ix, 63, 215, 221, 234, 339,
353, 400, 575
numerical linear
ix, 399
algebraic
diﬀerential equation
455
equation
1, 8, 11, 215, 428
function
511
multiplicity
373
algorithm
399
altitude vector
418
amplitude
334
analysis
63, 76, 99, 400, 578
complex
31, 175, 256, 263
functional
340, 350, 362
numerical
ix, 181
real
ix, 219
vector
viii
analytic
76, 105, 158, 168, 175, 181,
521, 576
function
98, 456, 463
solution
431
angle
509
azimuthal
508, 522, 549
cylindrical
509
polar
572
right
73, 581
zenith
508, 515
angular
coordinate
130
quantum number
566, 568
animal population
2, 435, 485
anisotropic
442
annulus
170, 415, 474, 480, 494, 499
ansatz
66, 124, 161, 330, 390, 394, 466,
475, 535, 538
exponential
67, 330
power
162, 464, 520
trigonometric
546
approximation
vii, 9, 110, 181, 363, 406
arbitrage
299
arbitrary function
6, 21
arc
551, 556
arc-length
244
area
39, 244, 413, 414, 415, 477
equal
40, 46, 47, 431
surface
517, 529, 537, 543, 553, 555
argument (see phase)
xvi, 573
arithmetic
184, 571
ﬂoating-point
ix, 184
single-precision
184
asset
299
associated Legendre function
513
associativity
281, 575
astronomy
560
asymptotics
69, 284, 453, 468
atom
37, 279, 394, 497, 547, 568
eigenfunction
568, 570
orbital
503
audio
63
autonomous
24, 29, 556
average
40, 92, 131, 167, 252, 285, 521,
523, 553, 560
weighted
213
DOI 10.1007/978-3-
-
-0
319 02099
, © Springer International Publishing Switzerland 2014
607
, 
P
 Olver Introduction to Partial Differential Equations, Undergraduate Texts in Mathematics,
.J.

608
Subject Index
axis
horizontal
18
vertical
18
azimuthal angle
508, 522, 549
B spline
283
back substitution
212
backward diﬀerence
182
backwards heat equation
129, 299, 442
bacteria
438, 485
ball
362, 467, 508, 528, 530, 533, 534,
537, 540, 542, 545, 547, 550, 555
balloon
549
bank
299
bar
2, 15, 49, 64, 65, 95, 122, 124, 132,
134, 138, 140, 144, 170, 234, 239,
241, 292, 293, 298, 307, 344, 351,
357, 405, 440
barge
334
barrier
15, 173
basis
ix, 112, 350, 401, 405, 430, 577,
583
eigenvector
66, 583, 584
orthogonal
73, 112, 372, 581
solution
458
standard
216, 577, 581
bath
123, 134, 154, 436
beam
2, 146, 393
equation
146, 396
light
286
bell curve
295
bend
486
Benjamin–Bona–Mahony equation
337
Bessel
boundary value problem
475, 480
diﬀerential operator
366, 374
equation
364, 452, 467, 474, 538
modiﬁed
473
function
109, 364, 374, 435, 445, 452,
468, 474, 490, 499, 508, 536, 538
second kind
470, 474
spherical
435, 503, 539, 540, 543
inequality
111, 113, 379, 380, 441
operator
367
root
474, 490, 499
spherical
540, 541, 548
best approximation
110
bi-aﬃne function
415
bidiagonal
211
bidirectional
5, 324
big Oh
182
biharmonic equation
361
bilinearity
281, 578
binomial
coeﬃcient
164
formula
163
biology
vii, 4, 15, 63, 121, 130
black
441
Black–Scholes
equation
vii, viii, 291, 299
formula
302
block
144
bidiagonal
211
tridiagonal
211
blow-up
23, 32, 37
blurring
129, 442
body
121, 341
solid
2, 15, 65, 144, 440, 503, 504,
522, 546
Bohr radius
567, 569
boiling water
138, 170, 524, 543
boost
20, 311
boundary
ix, 5, 152, 220, 243, 245, 254,
436, 437, 522
coeﬃcient matrix
425
condition
4, 7, 68, 217, 293, 306, 339,
400, 521
Dirichlet
7, 123, 141, 147, 153, 166,
186, 202, 343, 345, 347, 359, 364,
368, 404, 412, 436, 439, 441, 446,
486, 488, 504, 521, 522, 527, 535,
537, 540, 544, 546
impedance
154
mixed
124, 343, 345, 347, 359, 364,
368, 436, 439, 441, 486, 504, 527,
535, 544, 546
Neumann
7, 123, 144, 153, 343,
345, 347, 359, 364, 368, 374, 436,
439, 441, 486, 504, 527, 535, 544,
546
periodic
69, 124, 130, 292, 343, 345,
361
Robin
123, 134, 154, 361, 436, 439
curved
411, 412
data
207
element
424
free
486
integral
347
moving
486
node
188, 207, 411, 418, 422, 425
point
153, 168
polygonal
411, 417
solid
154, 504
term
343
value problem
ix, 2, 6, 7, 74, 121,
152, 172, 215, 217, 234, 237, 255,
263, 278, 336, 339, 350, 355, 371,
376, 386, 399, 401, 402, 410, 585
Bessel
475, 480
Chebyshev
385

Subject Index
609
boundary value problem (continued )
Dirichlet
125, 132, 160, 207, 213,
245, 254, 383, 410, 416, 442, 450,
474, 508, 528, 531, 533, 547
elliptic
207, 216, 256, 506
Helmholtz
160, 441, 443, 445, 475,
487, 490
Legendre
511, 515
mixed
7, 134, 154, 214, 241, 245,
505, 507
Neumann
148, 238, 245, 352, 387,
534
periodic
69, 373, 383, 387, 510
regular
379
singular
379
Sturm–Liouville
363, 382
bounded
ix, 152, 345, 359, 504, 519, 531
bound state
278, 337, 383, 565, 568
Bourget’s hypothesis
490, 500
bow
497
bowl
362
box
508, 526, 536, 547, 550
function
60, 88, 149, 266, 283, 296
brick
536
bridge
549
Brownian motion
438
building
549
bump
168
Burgers’ equation
2, 9, 291, 315, 317
inviscid
315
potential
318
Cn
5, 456, 463, 576
piecewise
80–2, 94, 223, 233
cable equation
139, 304
calcium
569
calculus
vii, viii, 63, 76, 98, 182, 221,
223, 233, 263, 275, 452
of ﬁnite diﬀerences
182, 185
multivariable
215, 437, 505
of variations
255
vector
242, 507, 571
call option
304
can
537
canal
334
cap
555, 561
capacity
65
capacitor
522, 523, 526, 533
Carleson’s Theorem
231
Cartesian coordinate
viii, 242, 446, 536
Cauchy data
175
–Kovalevskaya Theorem
175
problem
175
–Schwarz inequality
ix, 107, 175, 285,
526, 579, 581
sequence
107, 119
causality
15, 43, 433
cavity
524, 534
CD
63
ceiling
494
cell phone
63
cellar
136, 137, 545
Celsius
306
center
252
centered diﬀerence
184, 198
CFL condition
197, 205
chain
333, 452
rule
viii, 20, 24, 32, 57, 161, 167, 170,
300, 305, 308, 333, 494, 509, 511,
524
change of variables
viii, 51, 58, 174,
179, 318, 511
chaos
60, 393, 492
characteristic
56, 175
coordinate
57
curve
15, 24, 30, 31, 47, 49, 176–8
lifted
49
equation
177
function
485
line
21, 45, 195
variable
20, 22, 25, 30, 32, 51, 52, 552
charge
249, 252, 256, 529, 531, 564
Chebyshev
boundary value problem
385
equation
385, 473
polynomial
473
chemical
2
diﬀusion
123
reaction
vii, 31, 438
chemistry
vii, 435, 569, 503
Chladni ﬁgure
497
chromatography
15, 31
circle
167, 258, 450, 500, 551
civil engineering
549
clarinet
144
classical
mechanics
394, 503
solution
5, 7, 17, 51, 144, 255, 399,
410, 427, 428, 432, 535, 546, 556,
558
clockwise
243
closed
ix
curve
152, 243
surface
505
coeﬃcient
constant
ix, 2, 9
diﬀusion
129, 307
Fourier
71, 72, 75, 85, 102, 107, 228,
264, 441
matrix
188, 191, 350, 386
transfer
156

610
cokernel
350
cold
436
collision
54, 292, 324, 336, 337, 438
column vector
216, 578
comb
229, 233
comet
565
commutativity
575
compact
ix
support
230, 430, 432
complementary
error function
277, 302, 321
harmonic function
523
complete
66, 107, 113, 119, 340, 371,
372, 378, 379, 381, 382, 383, 447,
477, 480, 517, 535, 583
system
520, 541, 565
complex
addition
571
analysis
31, 175, 256, 263
arithmetic
571
change of variables
174
conjugate
571, 580
division
572
eigenvalue
372
exponential
ix, 88, 107, 109, 136, 181,
189, 265, 271, 285, 519, 573
Fourier coeﬃcient
89, 229, 284
Fourier series
89, 582
function
284, 324
inner product
118
integral
266, 461
logarithm
164, 573
multiplication
571
number
viii, ix, xv, 571
plane
xv, 571
solution
6
spherical harmonic
519, 526, 565
variable
163, 571
vector space
289, 571, 575, 580
zero
87
compressible
37
compression wave
36, 44
computer
184, 190, 400
algebra
273, 453
arithmetic
ix
graphics
63
concentration
34
conditionally stable
190
conduction
504
conducting medium
504
conductivity
124
thermal
65, 123, 437
conductor
123
cone
56, 560, 564
conformal mapping
256
conic
172
conjugate symmetry
580
connected
17, 141, 245, 345, 359
simply
243
conservation law
15, 38, 46, 131, 201,
255, 256, 295, 304, 332, 337, 431,
437
energy
61, 151, 535
heat energy
122, 304
mass
41, 47, 360
conserved density
38, 46, 201, 256
constant
coeﬃcient
ix, 2, 9
Euler–Mascheroni
471
function
285, 506
Planck
6, 287, 394, 564
separation
141, 156, 446, 510
constitutive assumption
122
contaminant
123
continuous
viii, 7, 63, 80, 82, 94, 102,
231
dependence
130
function
99, 108, 117, 219, 220, 344,
576, 579
Lipschitz
29
media
390
piecewise
79, 81, 108, 127, 441
spectrum
337, 340, 374, 383, 565
continuum mechanics/physics
viii, 38
contract
299
control
9, 263
convection
438
-diﬀusion equation
139, 311, 314, 438
convective ﬂow
311
convergence
viii, xvi, 63, 72, 75, 76, 98,
109, 213, 383, 400, 410
in norm
98, 99, 109, 110, 231
nonuniform
267
pointwise
99, 109, 115, 117, 231, 285,
378
radius of
453, 458
test
integral
viii, 105
ratio
viii, 75, 462, 468
root
viii, 75
theorem
82
uniform
99, 100–102, 104, 378, 519
weak
99, 230, 270, 327, 429
convergent
584, 585
convolution
242, 281, 282, 295, 484, 544
integral
301
periodic
95
summation
284
theorem
284
coordinate
Subject Index

Subject Index
611
angular
130
Cartesian
viii, 242, 446, 536
characteristic
57
curvilinear
364, 435
cylindrical
viii, 3, 503, 508, 523, 527,
530, 536, 547
ellipsoidal
508
moving
15, 19
parabolic
174
parabolic spheroidal
508
polar
viii, 3, 62, 160, 161, 170, 250,
383, 451, 477, 479, 490, 509, 572
rectangular
viii, 161, 503
separable
508
space
3
spherical
viii, 3, 503, 508, 520, 524,
528, 537, 551, 553, 565
toroidal
508
corner
81, 193, 441
node
209
corpse
129
cosine transform
274
Coulomb
potential
503, 529, 564
problem
564
counterclockwise
243, 415
crack
256, 427
Cramer’s Rule
413
Crank–Nicolson method
192
crest
331
critical point
501
cross product
viii
cube
526, 534, 536, 543, 545, 550, 561
cubic
283, 408
curl
viii, 13, 242, 243, 349, 507
curve
172, 175, 254, 571
bell
295
characteristic
15, 24, 30, 31, 47, 49,
176, 177, 178
lifted
49
closed
152, 243
simple
152
nodal
497, 551
curved boundary
411, 412
curvilinear coordinate
364, 435
cusp
254
cut locus
511, 515, 525
cylinder
450, 452, 467, 508, 527, 536,
537, 547, 550
cylindrical
angle
509
coordinates
viii, 3, 503, 508, 523, 527,
530, 536, 547
shell
450
symmetry
517
cymbal
144
d’Alembert
formula
16, 121, 140, 146, 260, 427,
552, 558
solution
53, 201, 324, 487
dam
61
damped
62, 200
heat equation
65
transport equation
49
data
363, 410
de Broglie relation
287
death
129
decay
22, 105, 276, 285, 387, 441, 443,
479, 482, 536, 544
entropic
291
exponential
127
decimal expansion
108
deep water
331
deﬂection
249
deformable body
341
deformation
121
degree
306, 509
delta
comb
229, 233
distribution
215, 217
function
63, 215, 217, 221, 223, 225,
228, 233, 246, 249, 270, 276, 277,
280, 281, 292, 293, 321, 326, 358,
379, 405, 441, 479, 481, 483, 485,
521, 544, 552, 554
three-dimensional
527
two-dimensional
246, 255
impulse
221, 234, 248, 291, 292, 387
wave
558
denoise
128, 296, 441
dense
107, 344
subspace
344, 346, 371
density
49, 122, 124, 132, 142, 253, 344,
357, 438, 486, 488, 492, 495
conserved
38, 46, 201, 256
momentum
61
probability
108, 286, 564
dependent variable
3
deposit
299
depression
31
derivative
1, 5, 105, 182, 223, 236, 275,
342, 576
of delta function
233, 277
of Fourier series
94
left-hand
81
logarithmic
336
normal
7, 153, 245, 250, 436, 504,
528, 529
one-sided
576
partial
viii, 1, 3, 521

612
derivative (continued )
mixed
5, 8, 50, 242, 556
radial
554
right-hand
81
of series
101
operator
354
ordinary
1, 3
space
291
descent
503, 551, 561, 564
determinant
ix
Jacobian
58, 255
deviation
287, 295
diameter
499, 500
diﬀerence
backward
182
centered
184, 198
ﬁnite
vii, 9, 181, 182, 185, 186, 213,
214, 399, 407, 411, 422
forward
182
quotient
182
diﬀerentiable
80, 285
inﬁnitely
5, 75, 105, 128, 158
nowhere
329
diﬀerential
equation
1, 181, 342, 350, 427, 428,
585
stochastic
viii, 299
system of
2, 66
zenith
510
see also ordinary diﬀerential equation,
partial diﬀerential equation
geometry
63
operator
9, 64, 339, 350, 371
Bessel
366, 374
partial
2, 50
Sturm–Liouville
364, 365, 480
diﬀerentiation
viii, 233, 263, 276, 556
implicit
49
numerical
181
operator
286
diﬀusion
171, 299, 311, 315, 385, 388,
435, 436, 503, 535
chemical
123
coeﬃcient
129, 307
equation
123, 315, 340, 395, 438, 439,
543
nonlinear
2, 122, 291, 437, 442
of set
485
process
121, 129, 386
diﬀusive transport equation
194
diﬀusivity
307, 537
thermal
124, 134, 186, 293, 298, 438,
535
Dilation Theorem
271, 274
dimension
2, 112, 577
eigenspace
583
ﬁnite
ix, 11, 98, 109, 215, 220, 400,
410, 430
inﬁnite
ix, 11, 99, 109, 215, 340, 342,
371, 400, 577
Dirac
comb
229
delta function
217
equation
vii
direct method
255
Dirichlet
boundary condition
7, 123, 141, 147,
153, 166, 186, 202, 343, 345, 347,
359, 364, 368, 404, 412, 436, 439,
441, 446, 486, 488, 504, 521, 522,
527, 535, 537, 540, 544, 546
boundary value problem
125, 132,
160, 207, 213, 245, 254, 383, 410,
416, 442, 450, 474, 508, 528, 531,
533, 547
eigenvalue
373, 377
functional
410, 416, 424
integral
368, 506
principle
368, 400, 443, 506
disconnected
17
discontinuity
37, 193, 223, 441
jump
80, 81, 82, 96, 164, 223, 233,
236, 405, 432
removable
80
discontinuous
15, 76, 102, 427
initial data
292
discrete Fourier transform
582
discriminant
172, 173
disease
123
disk
121, 160, 167, 249, 251, 253, 374,
415, 427, 444, 445, 450, 467, 479,
490, 499, 500, 508
half
260, 480, 494, 496
metal
166, 479
quarter
494
semi-circular
170, 418
unit
155, 166
dislocation
256, 427
dispersion
vii, 292, 324, 330
relation
330
dispersive
329, 396
equation
328, 486
medium
2
quantization
328, 329
tail
337
wave
2, 324, 459
displacement
49, 216, 341, 351, 486
initial
53, 59, 145, 487, 546, 547, 551,
554, 556, 557, 560, 561, 562
radial
546
Subject Index

Subject Index
613
dissipative
315
dissonant
144, 490, 496, 549
distance
307, 578
distribution
ix, 215, 217, 553
Gaussian
295
distributive
575
disturbance
15, 22, 178, 292
diverge
98
divergence
viii, 13, 242, 244, 359, 437,
439, 505, 535
operator
347
theorem
viii, 505, 529
division
263
domain
ix, 5, 17, 152, 207, 243, 245,
248, 253, 339, 341, 345, 359, 486,
504, 571
of dependence
59, 197, 199
of inﬂuence
56
irregular
213
rescaled
495
space
401, 585, 586
dominant
frequency
495
mode
499
dot product
viii, 73, 341, 346, 354, 372,
578, 585
Hermitian
580
double
Fourier series
488
integral
viii, 58, 243, 245, 248, 251,
346, 422, 432, 437, 448
weighted L2 norm
381
L2 norm
383, 525
doublet
558
doubly inﬁnite series
89, 91
driver
44
drum
63, 144, 152, 153, 214, 486, 487,
490, 495, 496
circular
154, 160, 490, 494, 496, 499
rectangular
499
square
493
du Bois–Reymond lemma
431, 434
duality
221, 247, 286, 553
Duhamel’s principle
298
DVD
63
dynamical
3
partial diﬀerential equation
291, 551
process
7, 15, 171
system
340, 385
dynamics
46, 47, 49, 340, 435
ﬂuid
291, 315
gas
15, 31, 315
shock
15, 431
ear
144
Earth
136, 137, 508, 530, 537, 545, 549,
560, 561
earthquake
549
echo chamber
563
ecology
15
economics
vii, 4
eigenequation
66, 67, 371, 439, 583
eigenfunction
67, 74, 110, 125, 190, 340,
371, 374, 375, 376, 378, 387, 395,
439, 445, 515, 517, 535, 540, 546,
547, 549, 565, 566
atomic
568, 570
expansion
340, 379
Fourier–Bessel
477
null
70, 132, 145, 386, 387, 389, 441,
487, 547, 550
series
109, 371, 378, 386, 435, 441,
443, 535, 544
Sturm–Liouville
382
eigenmode
389, 497, 499, 546
eigensolution
66, 67, 125, 140, 389, 395,
447, 475, 487, 566, 584
series
440
eigenspace
371, 382, 517, 568, 583
eigenstate
568
eigenvalue
ix, 2, 66, 67, 125, 190, 336,
340, 371, 376, 378, 387, 389, 395,
487, 517, 535, 541, 546, 549, 565,
568, 582–4
complex
372
equation
66, 67, 371, 439, 582
Dirichlet
373, 377
Helmholtz
377, 383, 446, 474, 535,
546
multiplicity
582
null
131, 439, 582
problem
130, 371, 446
simple
372, 391, 582
zero
131, 439, 582
eigenvector
ix, 66, 371, 372, 375, 582,
583, 584
basis
66, 583, 584
null
582
eikonal equation
vii
Einstein equation
vii
elastic
550
ball
504
bar
15, 49, 234, 241, 351, 357
beam
146, 393
media
427
plate
324, 486
vibration
486
wave
121
elasticity
vii, 2, 121, 175, 504
elastodynamics
486, 549

614
elastomechanics
341
electric
charge
256, 529, 531, 564
ﬁeld
341, 504, 546, 551
potential
504
electromagnetic
254, 395
potential
2, 564
vibration
2
wave
15, 121, 388, 503, 546, 551
electromagnetism
vii, 121, 154, 341, 504
electromotive force
504
electron
6, 108, 249, 279, 395, 503, 564,
567, 569
electronic music
63
electrostatic
242, 249, 531
attraction
279
force
504, 529
potential
152, 249, 252, 256, 503, 522,
534
element
413, 568
aﬃne
414, 416
boundary
424
ﬁnite
viii, 9, 181, 207, 340, 362, 399,
410, 411, 416, 427, 430, 431
zero
428
elemental stiﬀness
417, 418
elementary function
xvi, 9, 55, 267, 310,
337, 435, 445, 452, 453, 459, 467,
539
elevation
31
ellipse
172
ellipsoid
511
ellipsoidal coordinates
508
elliptic
2, 122, 171, 172, 177, 212, 399,
404, 410
boundary value problem
207, 216,
256, 506
orbit
565
energy
39, 286, 292, 324, 565, 568, 569
conservation
61, 151, 535
density
61, 122
heat
122, 246, 304, 435–7, 482
kinetic
61
level
395, 503, 567
operator
394
potential
6, 61, 152, 242, 318, 340,
362
thermal
121, 122, 132, 134, 139, 168,
295, 304
total
61, 151
engineering
vii, 2, 4, 63, 217, 263, 305,
549
enhancement
129, 442
entropic decay
291
entropy
317
condition
43, 46
envelope
230
Equal Area Rule
40, 46, 47, 431
equation
Airy
281, 459
algebraic
1, 8, 428
algebraic diﬀerential
455
backwards heat
129, 299, 442
beam
146, 396
Benjamin–Bona–Mahony
337
Bessel
364, 452, 467, 474, 538
modiﬁed
473
biharmonic
361
Black–Scholes
vii, viii, 291, 299
Burgers
2, 9, 291, 315, 317
inviscid
315
potential
318
cable
139, 304
characteristic
177
Chebyshev
385, 473
convection-diﬀusion
139, 311, 314,
438
damped heat
65
damped transport
49
diﬀerential
1, 181, 342, 350, 427, 428,
585
diﬀusion
123, 194, 315, 340, 395, 438,
439, 543
forced
388
diﬀusive transport
194
Dirac
vii
dispersive
328, 486
eigenvalue
66, 67, 371, 439, 582
eikonal
vii
Einstein
vii
equilibrium
3, 5, 386
Euler
vii, 3, 162, 169, 300, 315, 452,
463, 465, 510, 520
evolution
2, 64, 67, 291, 298, 324,
325, 385, 439, 442
Fitz-Hugh–Nagumo
vii
functional
454
Ginzburg–Landau
vii
Hamilton–Jacobi
305
heat
vii, 2, 5, 9, 11, 12, 64, 69, 121,
124, 140, 152, 171, 172, 177, 181,
186, 291, 292, 295, 301, 305, 309,
311, 315, 318, 324, 325, 339, 340,
371, 374, 385, 387, 435, 438, 443,
445, 446, 467, 474, 503, 535, 537,
543, 544
backwards
129, 299, 442
forced
134, 296, 312, 314, 442
generalized
65
lossy
139
spherical
543
Subject Index

Subject Index
615
equation (continued )
Helmholtz
178, 370, 374, 378, 439,
488, 490, 508, 538, 540, 547
polar
451, 508
spherical
510, 517, 519, 565
Hermite
462
integral
350
implicit
26
indicial
464, 465, 520
integral
9, 284
Kadomstev–Petviashvili
vii
Kolmogorov–Petrovsky–Piskounov
vii
Korteweg–de Vries
vii, 2, 292, 324,
333, 336
modiﬁed
337
Laguerre
364, 566, 569
Lam´e
vii
Laplace
2, 8, 13, 121, 152, 155, 166,
171, 174, 177, 181, 207, 213, 243,
249, 256, 291, 311, 312, 339, 399,
422, 435, 467, 503, 504, 507, 509,
520, 522, 527
Legendre
364, 511, 514, 525
linear
9, 11, 215, 216, 428
Mathieu
409
Maxwell
vii, 546, 551
Maxwell–Bloch
vii
Navier–Stokes
vii, 3, 9, 315
ordinary diﬀerential
ix, 2, 6, 8, 11,
22, 65, 67, 121, 130, 140, 185, 215,
217, 263, 291, 308, 333, 363, 399,
435, 445, 452, 455, 463, 585
ﬁrst-order
ix, 297
linear
11, 585
second-order
ix, 297, 363, 459, 463
separable
ix
partial diﬀerential
vii, ix, 2, 63, 67,
130, 175, 215, 263, 305, 317, 339,
350, 394, 399, 429, 487, 582
dynamical
291, 551
elliptic
2, 122, 171, 172, 177, 212,
399, 404, 410
ﬁrst-order
175, 178, 195
fourth-order
486
hyperbolic
2, 121, 171, 172, 177,
178, 195, 385, 497
ill-posed
129, 174, 436, 442
nonlinear
2, 12, 31, 222, 305, 315,
333, 486
second-order
121, 172
third-order
2, 324
type
171, 172
well-posed
136, 395, 535
equation (continued )
Poisson
2, 121, 152, 171, 172, 181,
207, 215, 242, 245, 248, 253, 255,
256, 339, 352, 359, 361, 368, 383,
399, 401, 410, 422, 435, 442, 503,
504, 507, 527, 530, 532, 533, 534
Poisson–Darboux
62
quadratic
172, 464, 465
reaction-diﬀusion
438
Schr¨odinger
vii, 6, 146, 304, 340, 372,
374, 383, 385, 394, 396, 397, 503,
564
nonlinear
vii
stochastic diﬀerential
viii, 299
Sturm–Liouville
336, 364, 370, 403,
508
telegrapher
62, 146, 393
transcendental
1, 134
transport
2, 19, 31, 51, 65, 181, 195,
291, 315, 324, 330, 434
vibration
340, 388, 390
wave
2, 8, 13, 15, 50, 64, 121, 140,
149, 151, 152, 169, 171, 172, 177,
181, 195, 202, 291, 315, 324, 339,
340, 371, 374, 385, 389, 427, 434,
435, 467, 486, 488, 494, 503, 545,
551, 552, 554, 556, 558, 561
forced
56, 58, 393, 493
nonlinear
317, 333
vibration
340, 388
Vlasov
vii
von Karman
vii
zenith
510
equator
509, 522
equilateral triangle
260, 415, 417, 419,
424, 426
equilibrium
2, 131, 132, 171, 216, 239,
399, 435, 442, 503, 504
equation
3, 5, 386
mechanics
121
solution
132, 443
system
339, 340
temperature
133, 154, 214, 246, 435,
537
thermal
127, 152, 153, 168, 441, 448,
479, 522, 536
equivalence class
108
error
182, 184
bound
331
function
55, 277, 296, 310, 435
oscillatory
193
round-oﬀ
ix, 184, 190
Euclidean
norm
169, 572, 578
space
ix, 575, 578

616
Euler
equation
162, 169, 300, 452, 463, 465,
510, 520
equations
vii, 3, 315
–Mascheroni constant
471
formula
ix, 6, 88, 267, 573
European call option
300, 301
even
85, 149, 229, 268, 275, 497, 525
evolution equation
64, 67, 291, 298,
325, 385, 439, 442
third-order
2, 324
exercise
price
299
time
302
existence
ix, 2, 3, 29, 130, 246, 254, 350,
355, 368, 376, 390, 436
theorem
67, 364, 457
expected value
286, 287
explicit scheme
188
explosion
551, 560, 561
exponential
445, 452
ansatz
67, 330
complex
ix, 88, 107, 109, 136, 181,
189, 265, 271, 285, 519, 573
decay
127
function
66, 90, 140
integral
267
pulse
275
external
force
12, 56, 58, 152, 215, 216, 234,
291, 292, 351, 386, 390, 504
heat source
12, 152, 246, 296, 297,
312, 436, 437, 442, 485
factorial
453, 454, 468
factorization
15, 50, 62, 243, 324, 487
Fahrenheit
306, 311
feature
128
Fermi–Pasta–Ulam problem
333
Ferrers function
435, 513, 515, 525
trigonometric
516
ﬁeld
electric
341, 504, 546, 551
force
6, 252
gravitational
341
magnetic
341, 546, 551
scalar
242, 345, 359, 439, 505
vector
242, 243, 346, 359, 360, 439,
505, 507, 526
ﬁgure
Chladni
497
ﬁlter
291, 295, 482, 544, 545
ﬁnal
condition
300
value problem
300
ﬁnance
vii, 63, 291, 299
ﬁngerprint
63
ﬁnite diﬀerence
vii, 9, 181, 182, 185,
186, 213, 214, 399, 407, 411, 422
ﬁnite-dimensional
ix, 11, 98, 109, 215,
220
subspace
400, 410, 430
ﬁnite element
viii, 9, 181, 207, 340, 362,
399, 410, 411, 427, 430
linear system
431
matrix
418
subspace
416
ﬁre
254
ﬁrst-order
19, 182
ordinary diﬀerential equation
ix, 297
partial diﬀerential equation
175, 178,
195
Fitz-Hugh–Nagumo equation
vii
ﬁxed point
25, 29, 30, 584
ﬂoating-point
ix, 184
ﬂood wave
15, 31
ﬂow
3, 19, 22, 139, 242, 243, 244, 304,
311, 360, 504
compressible
37
gradient
386
ideal
504
incompressible
3, 243, 244, 504
ﬂuctuation
137
ﬂuid
2, 15, 37, 152, 292, 388, 438, 504
dynamics
291, 315
ﬂux
153, 244
mechanics
vii, 3, 121, 152, 153, 305,
315, 324, 341
source
360
velocity
341
ﬂute
144
ﬂux
38, 46, 201, 256, 504
ﬂuid
153, 244
heat
7, 122, 123, 139, 153, 154, 242,
246, 436, 437, 504, 535
focusing
178
food
136
force
49, 216, 218
electromotive
504
electrostatic
504, 529
external
12, 56, 58, 152, 215, 216,
234, 291, 292, 351, 386, 390, 504
ﬁeld
6, 252, 504
gravitational
154, 242, 487, 529, 531
restoring
168
forced
diﬀusion equation
388
heat equation
134, 296, 312, 314, 442
vibrational equation
390
wave equation
56, 58, 393, 493
Subject Index

Subject Index
617
forcing
4, 206
frequency
390
function
215, 255, 350, 357, 422
periodic
304, 390, 397
forensics
129
formal adjoint
344
formula
binomial
163
Black–Scholes
302
d’Alembert
16, 121, 140, 146, 260,
427, 552, 558
Euler
ix, 6, 88, 267, 573
Green
244, 250, 251, 254, 432
integral
468, 521, 530
Kirchhoﬀ
503, 551, 558
Lagrange
182
Parseval
114, 118, 285, 287, 289
Plancherel
113, 118, 286, 287, 289,
379
Poisson
165, 171, 259
superposition
248, 291, 297
Taylor
75
forward
diﬀerence
182
substitution
212
foundations
64
Fourier
analysis
vii, ix, 11, 181, 233, 579
–Bessel eigenfunction
477
–Bessel series
477, 492, 493
–Bessel–spherical harmonic series
541
coeﬃcient
71, 72, 75, 85, 102, 107,
228, 264, 441
complex
89, 229, 284
integral
284, 331, 383
law
123, 437, 535
mode
142, 189, 193
series
viii, 71, 74, 76, 82, 102, 109,
113, 121, 131, 163, 228, 231, 233,
263, 285, 328, 340, 371, 378, 402,
519
complex
89, 582
cosine
85, 87, 144, 148, 233
derivative of
94
double
488
generalized
109, 378
rescaled
495
sine
85, 87, 126, 142, 147, 157, 233,
382
transform
263, 264, 265, 269, 273,
274, 278, 282, 284, 285, 293, 297,
325, 330, 337, 340, 374, 462
discrete
582
inverse
263, 265, 269
Table
272
two-dimensional
274, 278
fractal
329, 332
Fredholm Alternative
239, 339, 350,
356, 390, 507, 527, 534
free
boundary
486
end
239
-space Green’s function
249, 530
freezer
304, 449
freezing
436
frequency
144, 263, 265, 292, 330, 546
dominant
495
forcing
390
high
127, 128, 193, 231, 296, 331, 442,
536
low
128
resonant
60, 279, 304, 391, 397, 493,
549
space
263, 284
variable
286
vibrational
389, 395, 487, 495, 548
friction
62, 315, 317, 388
Frobenius series
464, 569
fully
nonlinear
173
weak
429, 430
function
xvi, 63, 76, 108, 215, 220, 263,
575
Airy
327, 364, 435, 461
second kind
462
algebraic
327, 364, 435, 461
analytic
98, 456, 463
arbitrary
6, 21
associated Legendre
513
Bessel
109, 364, 374, 435, 445, 452,
468, 474, 490, 499, 508, 536, 538
second kind
470, 474
spherical
435, 503, 539, 540, 543
bi-aﬃne
415
box
60, 88, 149, 266, 283, 296
C∞
456, 463
characteristic
485
complementary error
277, 302, 321
complementary harmonic
523
complex
284, 324
constant
285, 506
continuous
99, 108, 117, 219, 220,
344, 576, 579
delta
63, 215, 217, 221, 223, 225, 228,
233, 246, 249, 270, 276, 277, 280,
281, 292, 293, 321, 326, 358, 379,
405, 441, 479, 481, 483, 485, 521,
544, 552, 554
three-dimensional
527
two-dimensional
246, 255
elementary
xvi, 9, 55, 267, 310, 337,
435, 445, 452, 453, 459, 467, 539

618
function (continued )
error
55, 277, 296, 310, 435
even
85, 149, 229, 268, 275, 497, 525
exponential
66, 90, 140
Ferrers
435, 513, 515, 525
trigonometric
516
forcing
215, 255, 350, 357, 422
gamma
454, 455, 461, 468
Gaussian
227, 247, 273, 277, 326, 334
generalized
ix, 63, 215, 223, 228, 233,
263, 270, 275, 553
Green’s
vii, 3, 215, 234, 236, 239, 240,
242, 248, 253, 256, 258, 280, 291,
292, 340, 358, 360, 371, 379, 383,
388, 480, 503, 525, 527, 528, 531,
533
free-space
249, 530
modiﬁed
358, 381, 480
harmonic
121, 152, 156, 168, 383,
504, 521, 530, 531
hat
88, 227, 274, 283, 405, 407, 412
hyperbolic
88, 91, 157, 241, 322, 334,
508
hypergeometric
364
inverse
xvi
L2
286, 344
Legendre
364, 435, 511, 513
linear
220, 339, 341, 344, 585
odd
85, 93, 147, 268, 275, 497, 525
parabolic cylinder
310
pyramid
412, 414, 418
quadratic
340, 362, 401, 416
ramp
82, 91, 95, 223, 235, 239
rational
218, 279, 452
rescaled
96, 506
rotated
155
sawtooth
78
series of
100
sign
94, 225, 270
space
63, 74, 99, 109, 215, 220, 340,
342, 344, 386, 400, 576
special
vii, 2, 9, 327, 364, 435, 453,
455, 472, 508, 511
step
29, 40, 42, 61, 105, 276, 320
unit
80, 83, 90, 102, 222, 232, 266,
329, 396
symmetric
236
trigonometric
60, 63, 70, 72, 74, 89,
109, 113, 125, 130, 189, 231, 264,
273, 374, 445, 451, 452, 457, 499,
508, 511, 519, 536
wave
108, 286, 288, 394, 396, 564,
565, 568
weight
379
zero
114, 219, 231, 281, 456, 463
zeta
87
functional
220, 399
analysis
340, 350, 362
Dirichlet
368, 400, 410, 416, 424, 443,
506
equation
454
linear
220, 553, 585
quadratic
340, 362, 400, 402
fundamental
solution
291, 292, 294, 301, 304, 326,
328, 387, 388, 435, 481, 482, 503,
543, 551, 554, 564
general
297
theorem
viii, 16, 39, 223, 236, 243,
245, 267
future
130
Galilean boost
20, 311
gamma function
454, 455, 461, 468
gas
37, 123, 129, 549
dynamics
15, 31, 315
Gauss–Seidel method
212, 402, 411
Gaussian
distribution
295
Elimination
ix, 210, 213, 351, 402,
407, 413, 416, 579
ﬁlter
291, 295, 482, 544, 545
function
227, 247, 273, 277, 326, 334
kernel
283
general
fundamental solution
297
relativity
31, 63, 504
solution
12, 67, 253, 390
generalized
Fourier series
109, 378
function
ix, 63, 215, 223, 228, 233,
263, 270, 275, 553
heat equation
65
Laguerre equation
566, 569
Laguerre polynomial
567, 569
geology
549
geometric
multiplicity
371, 373, 382
sum
116
geometry
31, 63, 121, 156, 181, 423,
488, 578
geophysics
549
Gibbs phenomenon
84, 88, 99, 267
Ginzburg–Landau equation
vii
glacier
15
glass
123
gradient
viii, 13, 152, 171, 242, 245,
252, 278, 345, 359, 400, 416, 438,
504, 505, 526, 529
ﬂow
386
temperature
341, 437, 535
Gram–Schmidt process
372, 378
Subject Index

Subject Index
619
gravitation
341, 362, 504, 511, 529
ﬁeld
341
force
154, 242, 487, 529, 531
potential
152, 249, 252, 503, 529, 534
gray-scale
441, 442
Green’s
formula
244, 250, 251, 254, 432
function
vii, 3, 215, 234, 236, 239,
240, 242, 248, 253, 256, 258, 280,
291, 292, 340, 358, 360, 371, 379,
383, 388, 480, 503, 525, 527, 528,
531, 533
free-space
249, 530
modiﬁed
358, 381, 480
identity
244
theorem
viii, 3, 215, 243, 437
Greenwich
509
Gregory’s series
78, 85, 86
grid
414, 421, 422
grounded
524
group
305, 308
velocity
292, 331
guitar
496
half
disk
260, 480, 494, 496
plane
260
Hamilton–Jacobi equation
305
Hamiltonian
394, 395, 564
hammer
55, 261, 494, 554
handbook
453
harmonic
function
121, 152, 156, 168, 383, 504,
521, 530, 531
oscillator
397
part
256
polynomial
163, 171, 506, 520, 522,
526
spherical
109, 435, 503, 517, 519, 528,
538, 540, 549, 565, 568
harmonious
144
hat function
88, 227, 274, 283, 405, 407,
412
heat
63
bath
123, 134, 154, 436
capacity
65
conduction
504
energy
122, 246, 304, 435–7, 482
equation
vii, 2, 5, 9, 11, 12, 64, 69,
121, 124, 140, 152, 171, 172, 177,
181, 186, 291, 292, 295, 301, 305,
309, 311, 315, 318, 324, 325, 339,
340, 371, 374, 385, 387, 435, 438,
443, 445, 446, 467, 474, 503, 535,
537, 543, 544
backwards
129, 299, 442
heat equation (continued )
forced
312, 314, 442
generalized
65
lossy
139
spherical
543
ﬂux
7, 122, 123, 139, 153, 154, 242,
246, 436, 437, 504, 535
sensor
303
sink
504
source
292, 438, 442, 504, 543
external
12, 152, 246, 296, 297, 312,
436, 437, 442, 485
speciﬁc
122, 124, 132, 438
total
444, 485, 537
Heisenberg Uncertainty Principle
286–7
helium
564, 568
helix
164
Helmholtz
boundary value problem
160, 441,
443, 445, 475, 487, 490
eigenvalue
377, 383, 446, 474, 535,
546
equation
178, 370, 374, 378, 439, 488,
490, 508, 538, 540, 547
polar
451, 508
spherical
510, 517, 519, 565
hemisphere
522, 523, 534, 542, 561
Hermite
equation
462
polynomial
311, 462
Hermitian
dot product
580
inner product
89, 107, 118, 271, 285,
361, 372, 394, 580
norm
89, 394
Hessian matrix
443
high frequency
127, 128, 193, 231, 296,
331, 442, 536
highway
38
Hilbert
space
ix, 106, 108, 112, 119, 263, 284,
286, 368, 394
transform
283
hole
214, 243, 286, 426
homogeneous
2, 9, 10, 172, 356, 390,
583, 586
polynomial
168
Hopf–Cole transformation
318
Hˆopital’s Rule
117, 229
horizontal axis
18
horse
334
Huygens’ Principle
vii, 503, 551, 560,
562, 564
hydrodynamics
305
hydrogen
383, 503, 564, 568
hyperbola
172

620
hyperbolic
2, 121, 171, 172, 177, 178,
195, 385, 497
cosine
241
cotangent
91, 322
function
88, 508
secant
334
sine
157, 241
trajectory
565
hypergeometric function
364
hypersurface
175
ice
138, 524, 542
ideal ﬂuid ﬂow
504
identity
additive
575
matrix
584
operator
383, 384
ill-posed
129, 174, 436, 442
image
xvi, 63, 441
enhancement
129, 442
method
256, 532, 534
mirror-
143, 148, 149
point
532
processing
2, 63, 129, 263
imaginary
part
ix, 571
unit
571
impedance
154
implicit
diﬀerentiation
49
equation
26
function theorem
49
scheme
191
impulse
221, 234, 248, 291, 292, 387
boundary value problem
236
point
236
unit
215–218, 234, 280, 551, 562
incomplete matrix
583, 584
incompressible
3, 243, 244, 504
independent variable
3, 6, 15, 291, 300
index
464
indicial equation
464, 465, 520
inelastic
337
inequality
Bessel
111, 113, 379, 380, 441
Cauchy–Schwarz
ix, 107, 175, 285,
526, 579, 581
triangle
ix, 107, 526, 572, 579, 581
inﬁnite-dimensional
ix, 11, 99, 109, 215,
340, 342, 371, 400, 577
inﬁnitely diﬀerentiable
5, 75, 105, 128,
158
inﬂation
550
inhomogeneous
2, 12, 133, 215, 217,
234, 350, 424, 427, 442, 507, 584,
586
initial
-boundary value problem
7, 121, 172,
293, 297, 306, 314, 339, 386, 440,
535, 546
periodic
328, 493
condition
4, 7, 53, 291, 292, 297, 306,
389, 453, 458, 487, 535
data
127, 175, 292, 295, 436, 481, 535
displacement
53, 59, 145, 487, 546,
547, 551, 554, 556, 557, 560, 561,
562
position
50
temperature
131, 544
value problem
ix, 2, 6, 19, 50, 53,
172, 174, 263, 292, 325, 328, 371,
458, 544, 551, 560
velocity
50, 55, 59, 145, 487, 546, 547,
551, 554, 557, 560, 562
inner product
ix, 107, 112, 118, 114,
339, 341, 371, 379, 400, 428, 578,
580
Hermitian
89, 107, 118, 271, 285, 361,
372, 394, 580
L2
73, 220, 230, 247, 342, 346, 358,
359, 371, 439, 477, 505, 515, 517,
525
space
350, 354, 400, 428, 429, 578
complex
118, 580
weighted
341, 344, 354, 358, 359, 365,
378, 438, 506, 569
inner tube
361, 493
insulated
122, 153, 154, 436, 441, 443,
504, 522, 535
insurance
299
integer
xv, 453
integrable
127, 292, 324, 333, 337
square-
106, 108, 231, 284, 285, 293,
394
integral
viii, 219, 236, 264, 406
boundary
347
complex
266, 461
convergence test
viii, 105
convolution
301
double
viii, 58, 243, 245, 248, 251,
346, 422, 432, 437, 448
equation
9, 284, 350
exponential
267
formula
468, 521, 530
Fourier
284, 331, 383
line
viii, 39, 243, 245, 251, 256, 433,
437
Poisson
165, 171, 259
of series
101
surface
viii, 505, 522, 529, 533
triple
viii, 505, 553
integrating factor
22
Subject Index

Subject Index
621
integration
viii, 63, 92, 101, 263, 276,
585
by parts
viii, 226, 245, 288, 342, 346,
428, 430, 432, 454, 461, 505
Lebesgue
ix, 107, 217, 219, 263, 270
numerical
185, 407, 422, 453
of series
92, 101
Riemann
217, 219, 270
interest
299, 304
interior
maximum
443
node
188, 207, 209, 404, 411, 412,
418, 422, 425
point
168
interpolation
27, 210, 400
interstellar space
552
interval
17, 73, 95, 102, 217, 220, 264,
284, 576
invariant
42, 291
inverse
additive
575
Fourier transform
263, 265, 269
function
xvi
matrix
xvi, 217
scattering
337
square law
252, 529
tangent
277
inversion
258, 526, 532
investment
291, 299
inviscid
Burgers’ equation
315
limit
292, 315, 317, 320
irrational
60, 107, 144, 389
time
292, 329, 332
irregular
domain
213
singular point
453, 463
irreversibility
130
irrotational
242, 360, 504
isosceles
413, 415, 417, 421
isotropic
435, 436, 437, 485, 508, 528,
535, 537
iterative
187, 453, 583
numerical method
ix, 203, 212, 411
Gaussian–Seidel
212, 402, 411
Jacobi
212
successive over-relaxation (SOR)
212, 402, 411
system
190, 402, 416
Jacobi method
212
Jacobian determinant
58, 255
Jordan canonical form
67, 584
joule
287
jump
441
discontinuity
80, 81, 82, 96, 164, 223,
233, 236, 405, 432
magnitude
80, 223
rope
148, 393
Kadomstev–Petviashvili equation
vii
Kelvin
311
Kepler problem
565
kernel
350, 356, 400, 586
Gaussian
283
Poisson
165, 259
kinetic energy
61
Kirchhoﬀ’s formula
503, 551, 558
Kolmogorov–Petrovsky–Piskounov equa-
tion
vii
Korteweg–de Vries equation
vii, 2, 292,
324, 333, 336
modiﬁed
337
L2
function
286, 344
Hermitian inner product
89, 107, 580
Hermitian norm
89
inner product
73, 220, 230, 247, 342,
346, 358, 359, 371, 439, 477, 505,
515, 517, 525
norm
89, 106, 284, 383, 394, 445, 477,
525, 579
ladder
503
lag
137
Lagrange’s formula
182
Laguerre
equation
364, 566, 569
polynomial
109, 566, 567, 569
Lam´e equation
vii
Laplace
equation
2, 8, 13, 121, 152, 155, 166,
171, 174, 177, 181, 207, 213, 243,
249, 256, 291, 311, 312, 339, 399,
422, 435, 467, 503, 504, 507, 509,
520, 522, 527
transform
263
Laplacian
4, 13, 152, 170, 243, 250, 339,
359, 374, 447, 451, 507, 524, 528,
535, 537, 557, 564
spherical
510, 517, 525, 538, 543, 549,
551
laser
292
latitude
508
law
of cosines
258, 556
Fourier
123, 437, 535
inverse square
252, 529
Newton’s second
49
Lax–Wendroﬀscheme
200
least squares
110, 363

622
Lebesgue integration
ix, 107, 217, 219,
263, 270
left-hand
derivative
81
limit
79, 81
Legendre
boundary value problem
511, 515
equation
364, 511, 514, 525
function
364, 435, 511, 513
polynomial
511, 513, 525
lemma
du Bois–Reymond
431, 434
Riemann–Lebesgue
112, 117, 231, 276
Lie group
305
lifted characteristic curve
49
light
15, 149, 546, 551, 552, 560, 562
beam
286
cone
56, 560, 564
ray
178
sensor
561
speed
295, 551
wave
551
lightning
551, 552, 560, 563
limit
viii, 63, 98, 107, 218, 219, 221, 247
inviscid
292, 315, 317, 320
left-hand
79, 81
one-sided
viii
right-hand
79, 81
line
32, 396, 404, 563, 576
characteristic
21, 45, 195
integral
viii, 39, 243, 245, 251, 256,
433, 437
shock
40, 173
spectral
395
real
xv, 107, 284
linear
10, 64, 306, 394, 404, 585
algebra
viii, ix, 63, 215, 221, 234, 339,
353, 400, 575
numerical
ix, 399
combination
11, 12, 156, 216, 221,
401, 413, 430, 577
dependence
577
diﬀerential operator
9, 10, 64, 339,
350, 371, 586
equation
9, 11, 215, 216, 428
function
220, 339, 341, 344, 585
functional
220, 553, 585
independence
ix, 112, 401, 577
operator
110, 284, 339, 340, 350, 354,
355, 386, 400, 428, 585
superposition
11, 51, 59, 67, 71, 234,
242, 293, 328, 330, 476, 481, 483,
535, 544, 551, 552, 554
system
ix, 187, 191, 216, 339, 350,
386, 400, 428, 582, 586
transformation
285
linearization
317, 318
Lipschitz continuity
29
liquid
121, 123, 324, 438, 503
crystal
427
lithium
568
local maximum
507
localized
333, 503, 551
disturbance
178, 292
solution
560
logarithm
250, 452, 464
complex
164, 573
logarithmic
derivative
336
potential
253, 256, 383
singularity
383, 525
term
468
logic
64
longitude
508
lossy
convection-diﬀusion equation
139
diﬀusion equation
194
heat equation
139
low frequency
128
lower triangular matrix
211
M–test
100, 105
Mach number
174
magnetic
ﬁeld
341, 546, 551
quantum number
568, 569
magneto-hydrodynamics
vii, 315
magniﬁcation factor
189, 192, 199, 205
magnitude
80, 221, 223
manufactured solution
210
Maple
273, 453
mass
6, 37, 39, 49, 95, 216, 333, 396,
531, 564
conservation
41, 47, 360
ﬂux
38, 47
point
249, 252, 256, 529
-spring system
95
materials science
175
Mathematica
273, 453
mathematics
vii, ix, 2, 16, 64, 435
Mathieu equation
409
matrix
66, 216, 339, 582
boundary coeﬃcient
425
coeﬃcient
188, 191, 350, 386
convergent
584, 585
ﬁnite element
418
Hessian
443
identity
584
incomplete
583, 584
inverse
xvi, 217
lower triangular
211
multiplication
341, 344, 356, 585
Subject Index

Subject Index
623
matrix (continued )
orthogonal
506
positive deﬁnite
342, 353, 362
self-adjoint
354, 373
symmetric
66, 74, 342, 353, 354, 362,
372, 401
tridiagonal
188, 191, 211, 407
upper triangular
211
weighted adjoint
342
zero
583
maximum
168, 507, 522
principle
vii, 213, 291, 312, 318, 443,
537
strong
167, 314, 522
weak
314, 507
Maxwell’s equations
vii, 546, 551
Maxwell–Bloch equation
vii
mean
92, 286, 287, 519, 553, 555
temperature
136
zero
78, 92
measure
ix, 63, 102
zero
102, 104, 108, 114, 117
mechanics
viii, 435, 578
classical
394, 503
continuum
viii, 38
equilibrium
121
ﬂuid
vii, 3, 121, 152, 153, 305, 315,
324, 341
Newtonian
7
quantum
vii, viii, x, 6, 108, 149, 263,
278, 284, 286, 288, 329, 337, 340,
383, 386, 394, 503, 564, 566, 568
solid
121, 292
medium
2, 64, 390, 427, 435, 504
membrane
15, 121, 152, 153, 154, 158,
168, 178, 249, 361, 435, 486, 487,
490, 492, 550
meridian
509
mesh
186, 195, 207, 411, 414, 418, 421,
423
metal
ball
534, 542
bar
64, 138, 292
disk
166, 479
pipe
170
plate
214, 418, 426, 435, 481, 497
shell
524
sphere
534
method
of images
256, 532, 534
Newton
ix, 135
Perron
255
Richardson
194
Runge–Kutta
ix
stationary phase
331
microwave
15
minimal surface
153, 174
minimization principle
255, 340, 371,
399, 427, 429, 507
minimizer
399, 400, 401, 443
minimum
168, 522
Minneapolis
137
minute
307
mirror-image
143, 148, 149
mixed
boundary condition
124, 343, 345,
347, 359, 364, 368, 436, 439, 441,
486, 504, 527, 535, 544, 546
boundary value problem
7, 134, 154,
214, 241, 245, 505, 507
partial derivatives
5, 8, 50, 242, 556
mode
Fourier
142, 189, 193
normal
389, 475, 487, 488, 489, 492,
499, 546, 565
unstable
145, 340, 547
modeling
viii
modiﬁed
Bessel equation
473
Dirichlet integral
368
Green’s function
358, 381, 480
Korteweg–de Vries equation
337
modulus
ix, 571
molecule
438, 503, 564
momentum
39, 61, 286, 287
money
299
monomial
577
monotone
29, 30
Monte Carlo
181
movie
xv, 63
moving
boundary
486
coordinate
15, 19
multigrid
181
multiplication
263
complex
571
matrix
341, 344, 356, 585
operator
286, 586
scalar
575
unit
575
multiplicity
371, 373, 382
multiply valued solution
36, 37
multipole
181
multiscale
442
multi-soliton solution
337
multivariable calculus
215, 437, 505
music
63, 144
N–wave
48
natural number
xv
Nature
340

624
Navier–Stokes equations
vii, 3, 9, 315
negative semi-deﬁnite
443
neon
569
Neumann
boundary condition
7, 123, 144, 153,
343, 345, 347, 359, 364, 368, 374,
436, 439, 441, 486, 504, 527, 535,
544, 546
boundary value problem
148, 238,
245, 352, 387, 534
neuron
139
neuroscience
vii
Newton
mechanics
7
potential
503, 504, 529, 530, 532
method
ix, 135
Second Law
49
nodal
curve
497, 551
region
497, 551
node
186, 187, 404, 411
boundary
188, 207, 411, 418, 422, 425
corner
209
interior
188, 207, 209, 404, 411, 412,
418, 422, 425
no-ﬂux
153
noise
193, 442, 536
non-characteristic
175
nonlinear
vii, 60, 222, 324, 428, 549
diﬀusion
2, 122, 291, 437, 442
fully
173
partial diﬀerential equation
2, 12, 31,
222, 305, 315, 333, 486
Schr¨odinger equation
vii
transport
38, 46, 130, 292, 308, 315,
317, 427, 431
wave equation
317, 333
nonlinearizing
318
nonsingular
211, 216
nonsmooth
129, 399, 427
nonuniform
10
convergence
267
transport
24
norm
ix, 98, 109, 344, 356, 400, 571,
578
convergence in
98, 99, 109, 110, 231
double weighted L2
381
Euclidean
169, 572, 578
L2
89, 106, 284, 383, 394, 445, 477,
525, 579
-preserving
286
unit
108, 286, 394
normal
ix, 295
derivative
7, 153, 245, 250, 436, 504,
528, 529
mode
389, 475, 487, 488, 489, 492,
499, 546, 565
unit
153, 244, 433, 436, 437, 504, 505
north pole
511, 534
nowhere diﬀerentiable
329
nucleus
279, 394, 395, 503, 564, 569
null
eigenfunction
70, 132, 145, 386, 387,
389, 441, 487, 547, 550
eigenvalue
131, 439, 582
eigenvector
582
number
complex
viii, ix, xv, 571
Mach
174
natural
xv
P´eclet
311
quantum
566–9
rational
59, 107
real
xv, 98, 571
theory
31, 121, 500
wave
330
numerical
algorithm
399
analysis
ix, 181
approximation
vii, 9, 181, 406
Crank–Nicolson method
192
diﬀerentiation
181
domain of dependence
197, 199
explicit scheme
188
geometric method
181
implicit scheme
191
integration
185, 407, 422, 453
iterative method
ix, 203, 212, 411
Gaussian–Seidel
212, 402, 411
Jacobi
212
successive over-relaxation (SOR)
212, 402, 411
Lax–Wendroﬀscheme
200
linear algebra
ix, 399
multigrid method
181
multipole method
181
Monte Carlo method
181
pseudo-spectral method
181
Richardson’s method
194
Runge–Kutta method
ix
simulation
324
spectral method
181
stability
181, 190, 205
symplectic method
181
upwind scheme
200
observer
552, 556, 558, 562
obtuse
414
Subject Index

Subject Index
625
odd
85, 93, 147, 268, 275, 497, 525
oil
129, 549
one-sided
derivative
576
limit
viii
open
ix, 152
operator
Bessel
367
derivative
354
diﬀerential
9, 64, 339, 350, 371
partial
2, 50
divergence
347
identity
383, 384
linear
110, 284, 339, 340, 350, 354,
355, 386, 400, 428, 585
multiplication
286, 586
Schr¨odinger
565
Sturm–Liouville
364, 365, 480
wave
50, 487
optics
vii, 178, 292, 329, 459
option
291, 299, 300–2, 304
orbit
452, 503, 565
order
1, 3, 366, 467, 511, 569
ordinary
derivative
1, 3
diﬀerential equation
ix, 2, 6, 8, 11,
22, 65, 67, 121, 130, 140, 185, 215,
217, 263, 291, 308, 333, 363, 399,
435, 445, 452, 455, 463, 585
ﬁrst-order
ix, 297
linear
11, 585
second-order
ix, 297, 363, 459, 463
separable
ix
orientation
243, 504
orthogonal
ix, 73, 271, 286, 340, 350,
371, 378, 387, 428, 440, 441, 447,
462, 477, 489, 517, 525, 536, 547,
569, 581, 583
basis
73, 112, 372, 581
matrix
506
system
73, 113, 441, 515
orthonormal
109, 378, 514
basis
112, 113, 581, 583
system
73, 113, 570
oscillation
230, 267, 397, 474
oscillatory
error
193
wave
292, 327
outer space
487, 493, 531, 550
out of phase
137
oval
418, 423, 425
oven
138, 449, 480, 542
overdamped
146
overtone
63, 487, 495, 496
parabola
172, 309
parabolic
2, 122, 171, 172, 177, 193,
291, 315, 385, 438
arc
556
coordinates
174
cylinder function
310
spheroidal coordinates
508
trajectory
565
parallel
579
parameter
viii, 66
relaxation
213
Parseval’s formula
114, 118, 285, 287,
289
partial
derivative
viii, 1, 3, 521
mixed
5, 8, 50, 242, 556
diﬀerential equation
vii, ix, 2, 63, 67,
130, 175, 215, 263, 305, 317, 339,
350, 394, 399, 429, 487, 582
dynamical
291, 551
elliptic
2, 122, 171, 172, 177, 212,
399, 404, 410
ﬁrst-order
175, 178, 195
fourth-order
486
hyperbolic
2, 121, 171, 172, 177,
178, 195, 385, 497
ill-posed
129, 174, 436, 442
nonlinear
2, 12, 31, 222, 305, 315,
333, 486
second-order
121, 172
third-order
2, 324
type
171, 172
well-posed
136, 395, 535
diﬀerential operator
2, 50
fraction
279
sum
75, 100, 110, 113, 229
particle
149, 286
quantum
6, 396
subatomic
108, 149, 279
-wave duality
55, 149
particular solution
12, 253, 390
passenger
19, 20
path-independent
256
Pauli Exclusion Principle
568
P´eclet number
311
percussion
144
periodic
59, 82, 144, 147, 389, 393, 395,
489, 548
boundary condition
69, 124, 130, 292,
343, 345, 361
boundary value problem
69, 373, 383,
387, 510
convolution
95
eigenvalue problem
130
extension
77, 96, 102, 229
force
304, 390, 397
function
60, 130, 278

626
periodic (continued )
initial-boundary value problem
328,
493
Table
503, 568
permeability
551
permittivity
551
perpendicular
244, 581
Perron method
255
petri dish
438, 485
phase
ix, xvi, 137, 572
lag
137
shift
292, 324, 334
stationary
331
velocity
330
photoelectric eﬀect
149
photon
108, 149, 286, 395
physical
space
263, 284, 503
variable
286
physicist
217
physics
vii, viii, x, 37, 38, 55, 63, 305,
324, 339, 341, 394, 435, 509
piano
63, 144, 145, 261, 393, 496
piecewise
aﬃne
404, 411
continuous
79, 81, 108, 127, 441
cubic
408
smooth
254, 427, 505
C1
80, 82, 94, 223, 233
C2
94
Cn
81
pie wedge
170
pipe
31, 170
piston
37
pitch
495
pivot
579
Plancherel formula
113, 118, 286, 287,
289, 379
Planck’s constant
6, 287, 394, 564
plane
411, 571, 576
complex
xv, 571
curve
172
half
260
quarter
24
square
255
planet
452, 530, 534, 550, 565
plasma
vii, 2, 292, 388, 315
plate
2, 65, 152, 153, 246, 253, 324, 435,
441, 442, 486, 497
rectangular
154, 445
semi-circular
155
point
boundary
153, 168
charge
249, 252
point (continued )
critical
501
ﬁxed
25, 29, 30, 584
ﬂoating
ix, 184
image
532
interior
168
mass
249, 252, 256, 529
regular
453, 457, 463
set topology
viii
singular
2, 283, 363, 452, 457, 463
irregular
453, 463
regular
453, 463, 511, 569
pointwise convergence
99, 109, 115, 117,
231, 285, 378
Poisson
–Darboux equation
62
equation
2, 121, 152, 171, 172, 181,
207, 215, 242, 245, 248, 253, 255,
256, 339, 352, 359, 361, 368, 383,
399, 401, 410, 422, 435, 442, 503,
504, 507, 527, 530, 532, 533, 534
integral
165, 171, 259
kernel
165, 259
polar
angle
572
coordinates
viii, 3, 62, 160, 161, 170,
250, 383, 451, 477, 479, 490, 509,
572
Helmholtz equation
451, 508
pole
511, 534, 549
pollutant
2, 15, 19, 23, 139, 304, 435
polygon
boundary
411, 417
vertex
414, 423
polyhedral surface
411
polynomial
1, 50, 75, 119, 400, 402, 576,
577
Chebyshev
473
Laguerre
109, 566, 567, 569
harmonic
163, 171, 506, 520, 522, 526
Hermite
311, 462
homogeneous
168
Laguerre
109, 566, 569
Legendre
511, 513, 525
quadratic
316
trigonometric
71, 75, 91, 108, 111,
119, 400, 402
pond
292, 324, 331
population
2, 121, 123, 435, 438, 485
portfolio
291, 300
posed
ill
129, 174, 436, 442
well
136, 395, 535
position
50, 286, 287, 396
Subject Index

Subject Index
627
positive
578, 580
deﬁnite
339, 340, 355, 356, 362, 363,
371, 372, 373, 375, 376, 378, 383,
386, 387, 389, 400, 401, 429, 431,
439, 441, 442, 474, 506, 535, 546,
578
matrix
342, 353, 362
semi-deﬁnite
339, 355, 356, 371, 372,
378, 386, 387, 389, 441, 487, 506,
547, 549
potassium
569
potato
542, 543
potential
341
Burgers’ equation
318
Coulomb
503, 529, 564
electric
504
electromagnetic
2, 564
electrostatic
152, 249, 252, 256, 503,
522, 534
energy
6, 61, 152, 242, 318, 340, 362
gravitational
152, 249, 252, 503, 529,
534
logarithmic
253, 256, 383
Newtonian
503, 504, 529, 530, 532
theory
121
velocity
360, 504
power
ansatz
162, 464, 520
series
viii, 75, 92, 98, 168, 169, 453,
456, 458, 464, 511, 521
solution
445, 452, 463
pressure
3, 35
price
299
prime meridian
509
principal
quantum number
567, 568
value
283, 572
principle
Dirichlet
368, 400, 410, 416, 424, 443,
506
Duhamel
298
Heisenberg Uncertainty
286, 287
Huygens
vii, 503, 551, 560, 562, 564
maximum
vii, 213, 291, 312, 318, 443,
537
strong
167, 314, 522
weak
314, 507
minimization
255, 340, 371, 399, 427,
429, 507
Pauli Exclusion
568
superposition
9, 11, 12, 215, 217, 235,
335, 586
symmetry
269, 271, 275
probability
109, 121, 181, 263, 394
density
108, 286, 564
process
diﬀusion
121, 129, 386
dynamical
7, 15, 171
Gram–Schmidt
372, 378
product
282
convolution
281
cross
viii
dot
viii, 73, 341, 346, 354, 372, 578,
580, 585
inner
ix, 107, 112, 118, 114, 339, 341,
371, 379, 400, 428, 578
Hermitian
89, 107, 118, 271, 285,
361, 372, 394, 580
L2
73, 220, 230, 247, 342, 346, 358,
359, 371, 439, 477, 505, 515, 517,
525
weighted
341, 344, 354, 358, 359,
365, 378, 438, 506, 569
rule
viii
proﬁt
299, 302
program
453
proof
ix
proton
564, 569
pseudo-spectral
181
pulse
54, 266, 267, 275
put option
300, 304
pyramid function
412, 414, 418
quadrant
499
quadratic
equation
172, 464, 465
function
340, 362, 401, 416
functional
340, 362, 400, 402
polynomial
316
quadrilateral
416
quantization
287, 394, 564
dispersive
328, 329
quantum
-dynamical system
503
mechanics
vii, viii, x, 6, 55, 108, 149,
263, 278, 284, 286, 288, 329, 337,
340, 383, 386, 394, 503, 564, 566,
568
number
566–9
particle
6, 396
quasar
552
quasilinear
173
quasiperiodic
60, 144, 389, 393, 395,
489, 492, 497, 546, 548, 549
quotient
Rayleigh
340, 371, 375, 376, 377, 381,
384, 387
rule
viii

628
radial
derivative
554
displacement
546
Gaussian function
247
symmetry
249, 479, 528, 531, 541,
548
variable
249
vibration
547, 549
wave function
568
radian
509
radiation
65
radio
15, 63, 546
radioactive
22
radius
Bohr
567, 569
of convergence
453, 458
spectral
584
rainbow
459
ramp function
82, 91, 95, 223, 235, 239
higher order
95, 223
random
60, 129
range
585, 586
Rankine–Hugoniot condition
40, 46, 47,
317, 321, 431, 433
rarefaction wave
34, 43, 44, 309, 320,
323, 433
ratio test
viii, 75, 462, 468
rational
59, 389
function
218, 279, 452
number
59, 107
time
292, 329
ray
178, 257, 499
Rayleigh quotient
340, 371, 375, 376,
377, 381, 384, 387
reaction
vii, 31, 438
-diﬀusion equation
438
real
6, 371, 571
analysis
ix, 219
line
xv, 107, 284
number
xv, 98, 571
part
ix
reciprocal
572
rectangle
121, 156, 214, 248, 255, 312,
361, 374, 415, 418, 445, 482, 488,
493, 496, 497, 499
rectangular
box
508, 526, 536, 547, 550
coordinates
viii, 161, 503
drum
499
grid
422
mesh
186, 195, 207, 411, 418
plate
154, 445
pulse
266
recurrence relation
454, 456, 459, 467,
471, 525, 539, 543, 570
reduced ﬁnite element matrix
418
reﬂection
506, 571
refraction
178
refrigeration
136, 537
regular
457
boundary value problem
379
mesh
414
point
453, 457, 463
singular point
453, 463, 511, 569
regularization
317, 442
relative vibrational frequency
495, 548
relativity
vii, 2, 31, 56, 63, 295, 504,
560
relaxation parameter
213
remainder term
182
removable discontinuity
80
reparametrization
179
repulsive
529
rescaled
domain
495
Fourier series
96, 264
function
96, 506
reservoir
123
resonant
60, 279, 304, 391, 397, 493,
549
restoring force
168
reverse shock
433
Richardson’s method
194
Riemann
hypothesis
87
geometry
31
integration
217, 219, 270
–Lebesgue Lemma
112, 117, 231, 276
sum
265
zeta function
87
right
angle
73, 581
triangle
413, 417, 419, 423
right-hand
derivative
81
limit
79, 81
rim
486
ring
69, 124, 130, 146
ripple
292
river
31
Robin boundary condition
123, 134,
154, 361, 436, 439
rock
292, 324, 549
rod
534
root
ix, 316, 464, 474, 582
Bessel
474, 490, 499
spherical
540, 541, 548
cellar
136, 137, 545
test
viii, 75
rope
148, 393, 494
Subject Index

Subject Index
629
rotated function
155
rotation
305, 506, 517, 528
rough
284, 296
round-oﬀerror
ix, 184, 190
row vector
216, 578
rubber
493
rule
chain
viii, 20, 24, 32, 57, 161, 167,
170, 300, 305, 308, 333, 494, 509,
511, 524
product
viii
quotient
viii
trapezoid
407
Runge–Kutta method
ix
sample
27, 187, 189
sand
498
sawtooth function
78
saxophone
144
scalar
ﬁeld
242, 345, 359, 439, 505
multiplication
575
scaling
155, 305, 306, 307, 308, 323,
494, 521
symmetry
291, 307, 308, 327, 337,
479
scandium
569
scattering
337, 383, 549, 565
Schr¨odinger
equation
vii, 6, 146, 304, 340, 372,
374, 383, 385, 394, 396, 397, 503,
564
operator
565
science
vii, 2, 4
screen
286
sea
332
secant
182, 185
second-order
iterative scheme
203
ordinary diﬀerential equation
ix, 297,
363, 459, 463
partial diﬀerential equation
121, 172
ramp function
95
segment
404
seismology
63, 129, 549
self-adjoint
74, 110, 237, 243, 249, 339,
340, 354, 356, 362, 371, 378, 386,
400, 429, 438, 447, 505, 517, 525,
536, 579
matrix
354, 373
semi-
circle
480, 499
circular disk
170, 418
circular plate
155
inﬁnite bar
136
weak formulation
429
semi-deﬁnite
negative
443
positive
339, 355, 356, 371, 372, 378,
386, 387, 389, 441, 487, 506, 547,
549
sensor
23, 303, 561
separable
coordinates
508
ordinary diﬀerential equation
ix
solution
68, 121, 124, 141, 156, 163,
306, 320, 386, 388, 439, 463, 482,
509, 517, 520
separation
constant
141, 156, 446, 510
of variables
vii, ix, 25, 68, 121, 140,
155, 161, 215, 256, 305, 340, 364,
386, 435, 447, 451, 467, 487, 503,
507, 527, 535, 538, 547, 565
sequence
viii, xvi, 98, 104
Cauchy
107, 119
series
viii, 2, 9, 11, 67, 68, 75, 100, 121,
156, 256, 435, 455
complex Fourier
89, 582
diﬀerentiated
101
doubly inﬁnite
89, 91
eigenfunction
109, 340, 371, 378, 379,
386, 435, 441, 443, 535, 544
eigensolution
440
Fourier
viii, 71, 74, 76, 82, 102, 109,
113, 121, 131, 163, 228, 231, 233,
263, 285, 328, 340, 371, 378, 402,
519
–Bessel
477, 492, 493
–Bessel–spherical harmonic
541
complex
89, 582
cosine
85, 87, 144, 148, 233
derivative of
94
double
488
generalized
109, 378
rescaled
495
sine
85, 87, 126, 142, 147, 157, 233,
382
of functions
100
Frobenius
464, 569
Gregory
78, 85, 86
integrated
101
power
viii, 75, 92, 98, 168, 169, 453,
456, 458, 464, 511, 521
solution
445, 452, 463
Taylor
75, 169, 171, 182, 183, 193,
453, 458, 463, 521, 576
sesquilinear
372, 394, 580
set
theory
64
diﬀusion of
485
shallow water
292, 333, 435

630
sharpen
442
shell
324, 450, 524, 568
spherical
510, 534, 543, 549, 551
shift
phase
292, 324, 334
theorem
271, 274, 284
shock
39, 45, 47
dynamics
15, 431
line
40, 173
reverse
433
wave
vii, 2, 5, 15, 31, 37, 130, 195,
292, 315, 316, 317, 320, 324, 399,
427, 431, 433
shore
332
signal
15, 22, 23, 128, 178, 295, 486,
552, 560
processing
63, 263, 582
sign function
94, 225, 270
silver
123
similar triangle
257, 532
similarity
solution
vii, 42, 291, 305, 308, 323,
327, 337, 485
transformation
307
variable
308
simple
closed curve
152
eigenvalue
372, 391, 582
simply
connected
243
supported
146, 393
simulation
324
sine transform
274
single-precision
184
singlet
558
singular
172, 582
boundary value problem
379
point
2, 283, 363, 452, 457, 463
irregular
453, 463
regular
453, 463, 511, 569
singularity
251, 256, 427, 454, 553
logarithmic
383, 525
sink
360, 438, 442, 504
slope
21, 37, 182, 185
Smith Prize
3
smooth
5, 284, 441, 536, 576
piecewise
254, 427, 505
smoothing
128, 296, 441, 545
smoothness
7, 105, 276, 402, 410, 428,
430
snare drum
496
soap
153
soda
537
soil
137
soldering iron
292, 297, 304, 481, 534
solid
121, 388
body
2, 15, 65, 144, 440, 503, 504,
522, 546
boundary
154, 504
geometry
423
mechanics
121, 292
solitary wave
324, 334
soliton
292, 324, 336, 337
solute
22, 121, 438
solution
5, 6, 11, 181, 291, 350, 452
analytic
431
basis
458
classical
5, 7, 17, 51, 144, 255, 399,
410, 427, 428, 432, 535, 546, 556,
558
complex
6
d’Alembert
53, 201, 324, 487
equilibrium
132, 443
fundamental
291, 292, 294, 297, 301,
304, 326, 328, 387, 388, 435, 481,
482, 503, 543, 551, 554, 564
general
12, 67, 253, 390
localized
560
manufactured
210
multiply valued
36, 37
multi-soliton
337
particular
12, 253, 390
power series
445, 452, 463
separable
68, 121, 124, 141, 156, 163,
306, 320, 386, 388, 439, 463, 482,
509, 517, 520
similarity
vii, 42, 291, 305, 308, 323,
327, 337, 485
weak
5, 43, 53, 144, 399, 427, 429,
431, 432, 433, 489, 556, 558
sonic boom
173
SOR
212, 402, 411
sound
560, 562
barrier
15, 173
speed
174
wave
121, 551, 552
source
194, 292, 360, 438, 442, 504, 543,
552
space
6, 15, 18, 121, 291, 305, 494, 504
coordinates
3
derivative
291
Euclidean
ix, 575, 578
frequency
263, 284
function
63, 74, 99, 109, 215, 220,
340, 342, 344, 386, 400, 576
Hilbert
ix, 106, 108, 112, 119, 263,
284, 286, 368, 394
inner product
118, 350, 354, 400, 428,
429, 578, 580
outer
487, 493, 531, 550, 552
Subject Index

Subject Index
631
space (continued )
physical
263, 284, 503
three-dimensional
503, 528, 544, 552
-time
19, 56, 560
two-dimensional
252
vector
ix, 11, 98, 112, 220, 341, 371,
575, 578
span
ix, 112, 401, 413, 577
sparse
402, 407, 410
special
function
vii, 2, 9, 327, 364, 435, 453,
455, 472, 508, 511
relativity
56
speciﬁc heat
122, 124, 132, 438
spectral
181
line
395
radius
584
spectrum
395
continuous
337, 340, 374, 383, 565
speech
63
speed
331, 334
of light
295, 551
of sound
174
wave
19, 22, 24, 50, 51, 65, 140, 195,
197, 292, 330, 334, 396, 486, 488,
492, 495, 549
sphere
108, 508, 511, 517, 521, 522, 523,
526, 532, 534, 552, 555, 560
spherical
Bessel function
435, 503, 539, 540,
543
Bessel root
540, 541, 548
cap
555, 561
capacitor
522, 523, 526, 533
cavity
524, 534
coordinates
viii, 3, 503, 508, 520, 524,
528, 537, 551, 553, 565
harmonic
109, 435, 503, 517, 519,
528, 538, 540, 549, 565, 568
complex
519, 526, 565
Fourier–Bessel
541
heat equation
543
Helmholtz equation
510, 517, 519,
565
Laplacian
510, 517, 525, 538, 543,
549, 551
shell
510, 534, 543, 549, 551
symmetry
531, 552
wave
552, 554
spike
218, 224, 229
spin
568
spiral
164
spline
27, 210, 283, 400, 408
spring
95, 154, 216, 333, 473
square
369, 385, 415, 426, 444, 449, 479,
489, 492, 496, 499
drum
493
grid
414, 421
hole
426
-integrable
106, 108, 231, 284, 285,
293, 394
least
110, 363
mesh
421, 423
plate
255
unit
155, 158, 260, 485
stable
181, 190, 205, 340
conditionally
190
unconditionally
192, 193
von Neumann criterion
198, 199, 205
standard
basis
216, 577, 581
deviation
295
standing wave
55
star
560
state space
286
stationary
20
phase
331
point
331
wave
17
statistics
vii
steepest decrease
437
step
function
29, 40, 42, 61, 105, 276, 320
unit
80, 83, 90, 102, 222, 232, 266,
329, 396
size
182, 187, 406
time
197
stiﬀness
49, 142, 234, 344, 357, 473,
486, 488, 492, 495
elemental
417, 418
stochastic diﬀerential equation
viii, 299
stone
331
stopping time
128
St. Elmo’s ﬁre
254
strain
341
string
2, 13, 15, 49, 62, 96, 140, 142,
261, 391, 489
Strong Maximum Principle
167, 314,
522
structure
549
Sturm–Liouville
boundary value problem
363, 382
eigenfunction
382
equation
336, 364, 370, 403, 508
operator
364, 365, 480
subatomic particle
108, 149, 279

632
subsolution
255
subsonic
173, 174
subspace
576
dense
344, 346, 371
ﬁnite-dimensional
400, 410, 430
ﬁnite element
416
subterranean
129
successive over-relaxation (SOR)
212,
402, 411
sum
convolution
284
geometric
116
partial
75, 100, 110, 113, 229
Riemann
265
trigonometric
116
summer
136, 137, 545
sun
545, 565
supercomputer
565
superconductivity
vii
supernova
551
superposition
11, 51, 59, 67, 71, 234,
242, 293, 328, 330, 476, 481, 483,
535, 544, 551, 552, 554
formula
248, 291, 297
principle
9, 11, 12, 215, 217, 235, 335,
586
supersonic
15, 173, 174
support
284, 407, 411, 414
compact
230, 430, 432
surface
viii, 324, 504
area
517, 529, 537, 543, 553, 555
curve
505
integral
viii, 505, 522, 529, 533
minimal
153, 174
polyhedral
411
wave
292, 333
surfer
332
symbolic program
453
symmetric
188, 211, 249, 358, 579, 583
function
236
matrix
66, 74, 342, 353, 354, 362, 372,
401
symmetry
vii, ix, 169, 237, 263, 281,
291, 305, 358, 360, 494, 578
conjugate
580
cylindrical
517
principle
269, 271, 275
radial
249, 479, 528, 531, 541, 548
rotational
305
scaling
291, 307, 308, 327, 337, 479
spherical
531, 552
transformation
495
weighted
358, 380
symplectic
181
synthesizer
63, 496
system
adjoint
350
complete
520, 541, 565
of diﬀerential equations
2, 66
dynamical
340, 385
equilibrium
339, 340
linear
ix, 187, 191, 216, 339, 350, 386,
400, 428, 582, 586
ﬁnite element
431
orthogonal
73, 113, 570
quantum-dynamical
503
table
of Fourier transforms
272
periodic
503, 568
tail
337
Talbot eﬀect
292, 329, 396
tangent
153, 182, 277
target
xvi, 339, 341, 401, 428, 585
tautness
495
Taylor
series
75, 169, 171, 182, 183, 193, 453,
458, 463, 521, 576
formula
75
theorem
viii, 203
telegrapher’s equation
62, 146, 393
television
63
temperature
7, 68, 122, 129, 136, 152,
153, 291, 312, 341, 436, 449, 504,
508, 522, 535
equilibrium
133, 154, 214, 246, 435,
537
ﬂuctuation
137
gradient
341, 437, 535
initial
131, 544
maximum
168
minimum
168
mean
136
tension
49, 142, 486
test
function
220, 225, 230, 429, 553
integral
viii, 105
ratio
viii, 75, 462, 468
root
viii, 75
Weierstrass M
100, 105
tetrahedron
423
theorem
ix
Carleson
231
Cauchy–Kovalevskaya
175
convergence
82
convolution
284
dilation
271, 274
divergence
viii, 505, 529
existence
67, 364, 457
fundamental
viii, 16, 39, 223, 236,
243, 245, 267
Subject Index

Subject Index
633
theorem (continued )
Green
viii, 3, 215, 243, 437
implicit function
49
shift
271, 274, 284
Taylor
viii, 203
uniqueness
62, 67, 151
thermal
conductivity
65, 123, 437
diﬀusivity
124, 134, 186, 293, 298,
438, 535
energy
121, 122, 132, 134, 139, 168,
295, 304
equilibrium
127, 152, 153, 168, 441,
448, 479, 522, 536
reservoir
123
thermodynamics
vii, viii, 2, 7, 12, 123,
130, 139, 242, 295, 312, 341, 436,
452
thermomechanics
121, 153
thermometer
306
thermonuclear explosion
561
third-order evolution equation
324
three-dimensional
delta function
527
space
503, 528, 544, 552
thunder
551, 552, 560
timbre
63
time
2, 3, 6, 7, 15, 18, 39, 121, 130, 286,
291, 299, 305, 307, 395, 442, 494
exercise
302
irrational
292, 329, 332
rational
292, 329
-reversible
497
space-
19, 56, 560
step
197
stopping
128
tone
63, 487
topology
viii
toroidal
coordinates
508
membrane
361
torus
493
total
energy
61, 151
heat
444, 485, 537
traﬃc
15, 31, 38, 44
train
19, 20
trajectory
565
transatlantic cable
139
transcendental equation
1, 134
transfer coeﬃcient
156
transform
cosine
274
Fourier
263, 264, 265, 269, 273, 274,
278, 282, 284, 285, 293, 297, 325,
330, 337, 340, 374, 462
discrete
582
inverse
263, 265, 269
table
272
two-dimensional
274, 278
Hilbert
283
Laplace
263
sine
274
transformation
Hopf–Cole
318
linear
285
similarity
307
symmetry
495
translation
21, 155, 291, 295, 305, 328,
506, 528, 551
transport
equation
2, 19, 31, 51, 65, 181, 195,
291, 315, 324, 330, 434
nonlinear
38, 46, 130, 292, 308, 315,
317, 427, 431
nonuniform
24
transpose
216, 339, 341, 353, 578
transverse vibration
393
trapezoid rule
407
traveling wave
21, 51, 195, 291, 292,
305, 316, 330, 333, 334, 497
triangle
40, 58, 144, 197, 199, 205, 210,
257, 411, 414, 415, 416, 418
equilateral
260, 415, 417, 419, 424,
426
inequality
ix, 107, 526, 572, 579, 581
isosceles
413, 415, 417, 421
obtuse
414
right
413, 417, 419, 423
similar
257, 532
wave
44, 48, 322
triangulation
411, 414, 417
Tricomi equation
172, 178, 462
tridiagonal
188, 191, 211, 407
trigonometric
340, 371, 388, 487, 490
ansatz
546
Ferrers function
516
function
60, 63, 70, 72, 74, 89, 109,
113, 125, 130, 189, 231, 264, 273,
374, 445, 451, 452, 457, 499, 508,
511, 519, 536
polynomial
71, 75, 91, 108, 111, 119,
400, 402
sum
116
triple integral
viii, 505, 553
trumpet
63, 144

634
two-dimensional
delta function
246, 255
Fourier transform
274, 278
space
252
type
171, 172
Uncertainty Principle
286, 287
unconditionally stable
192, 193
underdamped
146
unidirectional
19, 324
uniform
50, 99, 104, 435, 436, 438, 485,
508, 528, 535, 537
convergence
99, 100–102, 104, 378,
519
uniqueness
ix, 2, 29, 130, 168, 245, 314,
340, 355, 400, 429, 458
theorem
62, 67, 151
unit
217, 281
circle
167, 258
disk
155, 166
imaginary
571
impulse
215–218, 234, 280, 551, 562
multiplicative
575
norm
108, 286, 394
normal
153, 244, 433, 436, 437, 504,
505
sphere
108
square
155, 158, 260, 485
step function
80, 83, 90, 102, 222,
232, 266, 329, 396
unitary
286
universe
130, 503, 553, 560, 564
unstable
136, 190, 389
mode
145, 340, 547
upper triangular matrix
211
upwind scheme
200
value
302
variable
1
change of
viii, 51, 58, 174, 179, 318,
511
characteristic
20, 22, 25, 30, 32, 51,
52, 552
complex
163, 571
dependent
3
frequency
286
independent
3, 6, 15, 291, 300
physical
286
radial
249
separation of
vii, ix, 25, 68, 121, 140,
155, 161, 215, 256, 305, 340, 364,
386, 435, 447, 451, 467, 487, 503,
507, 527, 535, 538, 547, 565
similarity
308
variance
287
vector
66, 98, 571, 575
addition
571
analysis
viii
calculus
242, 507, 571
column
216, 578
ﬁeld
242, 243, 346, 359, 360, 439, 505,
507, 526
electric
341, 546, 551
magnetic
341, 546, 551
velocity
3, 152, 244, 504
row
216, 578
space
ix, 11, 98, 112, 220, 341, 371,
575, 578
complex
289, 571, 575, 580
ﬁnite-dimensional
ix, 11, 98, 109,
215, 220
inﬁnite-dimensional
ix, 11, 99, 109,
215, 340, 342, 371, 400, 577
vehicle
38, 44
velocity
7, 19, 35
ﬂuid
341
group
292, 331
initial
50, 55, 59, 145, 487, 546, 547,
551, 554, 557, 560, 562
phase
330
potential
360, 504
vector ﬁeld
3, 152, 244, 504
vertex
411, 415, 416
polygon
414, 423
vertical axis
18
vibration
10, 15, 49, 60, 121, 140, 142,
149, 171, 340, 385, 389, 503, 546
elastic
486
electromagnetic
2
equation
340, 388, 390
frequency
389, 395, 487, 495, 548
radial
547, 549
transverse
393
video
63
violin
13, 15, 49, 63, 144, 497
viscosity
3, 291, 315, 317
Vlasov equation
vii
volatility
299, 302, 304
volume
243, 543
von Karman equation
vii
von Neumann stability
198, 199, 205
water
2, 3, 15, 121, 149, 243, 331
deep
331
shallow
292, 333, 435
wave
2, 15, 54, 121, 149, 292, 324, 388
acoustic
15
compression
36, 44
delta
558
dispersive
2, 324, 459
elastic
121
Subject Index

Subject Index
635
wave (continued )
electromagnetic
15, 121, 388, 503,
546, 551
equation
2, 8, 13, 15, 50, 64, 121, 140,
149, 151, 152, 169, 171, 172, 177,
181, 195, 202, 291, 315, 324, 339,
340, 371, 374, 385, 389, 427, 434,
435, 467, 486, 488, 494, 503, 545,
551, 552, 554, 556, 558, 561
ﬂood
15, 31
function
108, 286, 288, 394, 396, 564,
565, 568
light
551
N–
48
number
330
operator
50, 487
oscillatory
292, 327
packet
331
-particle duality
55, 149
rarefaction
34, 43, 44, 309, 320, 323,
433
shock
vii, 2, 5, 15, 31, 37, 130, 195,
292, 315, 316, 317, 320, 324, 399,
427, 431, 433
solitary
324, 334
sound
121, 551, 552
speed
19, 22, 24, 50, 51, 65, 140, 195,
197, 292, 330, 334, 396, 486, 488,
492, 495, 549
spherical
552, 554
standing
55
surface
292, 333
traveling
21, 51, 195, 291, 292, 305,
316, 330, 333, 334, 497
triangle
44, 48, 322
weak
convergence
99, 230, 270, 327, 429
formulation
428, 429, 430, 432
maximum principle
314, 507
solution
5, 43, 53, 144, 399, 427, 429,
431, 432, 433, 489, 556, 558
weakening spring
473
wedge
35, 43
Weierstrass M–test
100, 105
weight function
379
weighted
adjoint matrix
342
average
213
inner product
341, 344, 354, 358, 359,
365, 378, 438, 506, 569
Sturm–Liouville diﬀerential operator
365
symmetry condition
358, 380
well-posed
136, 395, 535
white
441
width
334
wind instrument
15, 144
winter
136, 137, 545
wire
153, 158, 160, 168, 393
wolf
438
X-ray
546
xylophone
144
yard
307
zenith
angle
508, 515
diﬀerential equation
510
zero
complex
87
eigenvalue
131, 439, 582
element
428
function
114, 219, 231, 281, 456, 463
matrix
583
mean
78, 92
measure
102, 104, 108, 114, 117
zeta function
87


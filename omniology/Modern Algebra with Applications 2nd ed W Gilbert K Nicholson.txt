MODERN ALGEBRA
WITH APPLICATIONS

PURE AND APPLIED MATHEMATICS
A Wiley-Interscience Series of Texts, Monograph, and Tracts
Founded by RICHARD COURANT
Editors: MYRON B. ALLEN III, DAVID A. COX, PETER LAX
Editors Emeriti: PETER HILTON, HARRY HOCHSTADT, JOHN TOLAND
A complete list of the titles in this series appears at the end of this volume.

MODERN ALGEBRA
WITH APPLICATIONS
Second Edition
WILLIAM J. GILBERT
University of Waterloo
Department of Pure Mathematics
Waterloo, Ontario, Canada
W. KEITH NICHOLSON
University of Calgary
Department of Mathematics and Statistics
Calgary, Alberta, Canada
A JOHN WILEY & SONS, INC., PUBLICATION

Cover: Still image from the applet KaleidoHedron, Copyright 2000 by Greg Egan, from his
website http://www.netspace.net.au/∼gregegan/. The pattern has the symmetry of the icosahedral
group.
Copyright 2004 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any
form or by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise,
except as permitted under Section 107 or 108 of the 1976 United States Copyright Act, without
either the prior written permission of the Publisher, or authorization through payment of the
appropriate per-copy fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers,
MA 01923, 978-750-8400, fax 978-750-4470, or on the web at www.copyright.com. Requests to
the Publisher for permission should be addressed to the Permissions Department, John Wiley &
Sons, Inc., 111 River Street, Hoboken, NJ 07030, (201) 748-6011, fax (201) 748-6008, e-mail:
permreq@wiley.com.
Limit of Liability/Disclaimer of Warranty: While the publisher and author have used their best
efforts in preparing this book, they make no representations or warranties with respect to the
accuracy or completeness of the contents of this book and speciﬁcally disclaim any implied
warranties of merchantability or ﬁtness for a particular purpose. No warranty may be created or
extended by sales representatives or written sales materials. The advice and strategies contained
herein may not be suitable for your situation. You should consult with a professional where
appropriate. Neither the publisher nor author shall be liable for any loss of proﬁt or any other
commercial damages, including but not limited to special, incidental, consequential, or other
damages.
For general information on our other products and services please contact our Customer Care
Department within the U.S. at 877-762-2974, outside the U.S. at 317-572-3993 or
fax 317-572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in
print, however, may not be available in electronic format.
Library of Congress Cataloging-in-Publication Data:
Gilbert, William J., 1941–
Modern algebra with applications / William J. Gilbert, W. Keith Nicholson.—2nd ed.
p. cm.—(Pure and applied mathematics)
Includes bibliographical references and index.
ISBN 0-471-41451-4 (cloth)
1. Algebra, Abstract. I. Nicholson, W. Keith. II. Title. III. Pure and applied
mathematics (John Wiley & Sons : Unnumbered)
QA162.G53 2003
512—dc21
2003049734
Printed in the United States of America.
10 9 8 7 6 5 4 3 2 1

CONTENTS
Preface to the First Edition
ix
Preface to the Second Edition
xiii
List of Symbols
xv
1
Introduction
1
Classical Algebra,
1
Modern Algebra,
2
Binary Operations,
2
Algebraic Structures,
4
Extending Number Systems,
5
2
Boolean Algebras
7
Algebra of Sets,
7
Number of Elements in a Set,
11
Boolean Algebras,
13
Propositional Logic,
16
Switching Circuits,
19
Divisors,
21
Posets and Lattices,
23
Normal Forms and Simpliﬁcation of Circuits,
26
Transistor Gates,
36
Representation Theorem,
39
Exercises,
41
3
Groups
47
Groups and Symmetries,
48
Subgroups,
54
v

vi
CONTENTS
Cyclic Groups and Dihedral Groups,
56
Morphisms,
60
Permutation Groups,
63
Even and Odd Permutations,
67
Cayley’s Representation Theorem,
71
Exercises,
71
4
Quotient Groups
76
Equivalence Relations,
76
Cosets and Lagrange’s Theorem,
78
Normal Subgroups and Quotient Groups,
82
Morphism Theorem,
86
Direct Products,
91
Groups of Low Order,
94
Action of a Group on a Set,
96
Exercises,
99
5
Symmetry Groups in Three Dimensions
104
Translations and the Euclidean Group,
104
Matrix Groups,
107
Finite Groups in Two Dimensions,
109
Proper Rotations of Regular Solids,
111
Finite Rotation Groups in Three Dimensions,
116
Crystallographic Groups,
120
Exercises,
121
6
P´olya–Burnside Method of Enumeration
124
Burnside’s Theorem,
124
Necklace Problems,
126
Coloring Polyhedra,
128
Counting Switching Circuits,
130
Exercises,
134
7
Monoids and Machines
137
Monoids and Semigroups,
137
Finite-State Machines,
142
Quotient Monoids and the Monoid of a Machine,
144
Exercises,
149
8
Rings and Fields
155
Rings,
155
Integral Domains and Fields,
159
Subrings and Morphisms of Rings,
161

CONTENTS
vii
New Rings from Old,
164
Field of Fractions,
170
Convolution Fractions,
172
Exercises,
176
9
Polynomial and Euclidean Rings
180
Euclidean Rings,
180
Euclidean Algorithm,
184
Unique Factorization,
187
Factoring Real and Complex Polynomials,
190
Factoring Rational and Integral Polynomials,
192
Factoring Polynomials over Finite Fields,
195
Linear Congruences and the Chinese Remainder Theorem,
197
Exercises,
201
10
Quotient Rings
204
Ideals and Quotient Rings,
204
Computations in Quotient Rings,
207
Morphism Theorem,
209
Quotient Polynomial Rings That Are Fields,
210
Exercises,
214
11
Field Extensions
218
Field Extensions,
218
Algebraic Numbers,
221
Galois Fields,
225
Primitive Elements,
228
Exercises,
232
12
Latin Squares
236
Latin Squares,
236
Orthogonal Latin Squares,
238
Finite Geometries,
242
Magic Squares,
245
Exercises,
249
13
Geometrical Constructions
251
Constructible Numbers,
251
Duplicating a Cube,
256
Trisecting an Angle,
257
Squaring the Circle,
259
Constructing Regular Polygons,
259

viii
CONTENTS
Nonconstructible Number of Degree 4,
260
Exercises,
262
14
Error-Correcting Codes
264
The Coding Problem,
266
Simple Codes,
267
Polynomial Representation,
270
Matrix Representation,
276
Error Correcting and Decoding,
280
BCH Codes,
284
Exercises,
288
Appendix 1: Proofs
293
Appendix 2: Integers
296
Bibliography and References
306
Answers to Odd-Numbered Exercises
309
Index
323

PREFACE TO THE
FIRST EDITION
Until recently the applications of modern algebra were mainly conﬁned to other
branches of mathematics. However, the importance of modern algebra and dis-
crete structures to many areas of science and technology is now growing rapidly.
It is being used extensively in computing science, physics, chemistry, and data
communication as well as in new areas of mathematics such as combinatorics.
We believe that the fundamentals of these applications can now be taught at the
junior level. This book therefore constitutes a one-year course in modern algebra
for those students who have been exposed to some linear algebra. It contains
the essentials of a ﬁrst course in modern algebra together with a wide variety of
applications.
Modern algebra is usually taught from the point of view of its intrinsic inter-
est, and students are told that applications will appear in later courses. Many
students lose interest when they do not see the relevance of the subject and often
become skeptical of the perennial explanation that the material will be used later.
However, we believe that by providing interesting and nontrivial applications as
we proceed, the student will better appreciate and understand the subject.
We cover all the group, ring, and ﬁeld theory that is usually contained in a
standard modern algebra course; the exact sections containing this material are
indicated in the table of contents. We stop short of the Sylow theorems and Galois
theory. These topics could only be touched on in a ﬁrst course, and we feel that
more time should be spent on them if they are to be appreciated.
In Chapter 2 we discuss boolean algebras and their application to switching
circuits. These provide a good example of algebraic structures whose elements
are nonnumerical. However, many instructors may prefer to postpone or omit this
chapter and start with the group theory in Chapters 3 and 4. Groups are viewed
as describing symmetries in nature and in mathematics. In keeping with this view,
the rotation groups of the regular solids are investigated in Chapter 5. This mate-
rial provides a good starting point for students interested in applying group theory
to physics and chemistry. Chapter 6 introduces the P´olya–Burnside method of
enumerating equivalence classes of sets of symmetries and provides a very prac-
tical application of group theory to combinatorics. Monoids are becoming more
ix

x
PREFACE TO THE FIRST EDITION
important algebraic structures today; these are discussed in Chapter 7 and are
applied to ﬁnite-state machines.
The ring and ﬁeld theory is covered in Chapters 8–11. This theory is motivated
by the desire to extend the familiar number systems to obtain the Galois ﬁelds and
to discover the structure of various subﬁelds of the real and complex numbers.
Groups are used in Chapter 12 to construct latin squares, whereas Galois ﬁelds are
used to construct orthogonal latin squares. These can be used to design statistical
experiments. We also indicate the close relationship between orthogonal latin
squares and ﬁnite geometries. In Chapter 13 ﬁeld extensions are used to show
that some famous geometrical constructions, such as the trisection of an angle
and the squaring of the circle, are impossible to perform using only a straightedge
and compass. Finally, Chapter 14 gives an introduction to coding theory using
polynomial and matrix techniques.
We do not give exhaustive treatments of any of the applications. We only go so
far as to give the ﬂavor without becoming too involved in technical complications.
Introduction
Groups
Boolean
Algebras
Pólya–Burnside
Method of
Enumeration
Symmetry
Groups in Three
Dimensions
Quotient
Groups
Monoids
and
Machines
Rings
and
Fields
Polynomial
and Euclidean
Rings
Quotient
Rings
Field
Extensions
Latin
Squares
Geometrical
Constructions
Error-Correcting
Codes
1
2
3
4
5
6
7
8
9
10
11
12
13
14
Figure P.1.
Structure of the chapters.

PREFACE TO THE FIRST EDITION
xi
The interested reader may delve further into any topic by consulting the books
in the bibliography.
It is important to realize that the study of these applications is not the only
reason for learning modern algebra. These examples illustrate the varied uses to
which algebra has been put in the past, and it is extremely likely that many more
different applications will be found in the future.
One cannot understand mathematics without doing numerous examples. There
are a total of over 600 exercises of varying difﬁculty, at the ends of chapters.
Answers to the odd-numbered exercises are given at the back of the book.
Figure P.1 illustrates the interdependence of the chapters. A solid line indicates
a necessary prerequisite for the whole chapter, and a dashed line indicates a
prerequisite for one section of the chapter. Since the book contains more than
sufﬁcient material for a two-term course, various sections or chapters may be
omitted. The choice of topics will depend on the interests of the students and the
instructor. However, to preserve the essence of the book, the instructor should be
careful not to devote most of the course to the theory, but should leave sufﬁcient
time for the applications to be appreciated.
I would like to thank all my students and colleagues at the University of
Waterloo, especially Harry Davis, D. ˇZ. Djokovi´c, Denis Higgs, and Keith Rowe,
who offered helpful suggestions during the various stages of the manuscript. I am
very grateful to Michael Boyle, Ian McGee, Juris Step´rans, and Jack Weiner
for their help in preparing and proofreading the preliminary versions and the
ﬁnal draft. Finally, I would like to thank Sue Cooper, Annemarie DeBrusk, Lois
Graham, and Denise Stack for their excellent typing of the different drafts, and
Nadia Bahar for tracing all the ﬁgures.
Waterloo, Ontario, Canada
WILLIAM J. GILBERT
April 1976

PREFACE TO THE
SECOND EDITION
In addition to improvements in exposition, the second edition contains the fol-
lowing new items:
ž New shorter proof of the parity theorem using the action of the symmetric
group on the discriminant polynomial
ž New proof that linear isometries are linear, and more detail about their
relation to orthogonal matrices
ž Appendix on methods of proof for beginning students, including the def-
inition of an implication, proof by contradiction, converses, and logical
equivalence
ž Appendix on basic number theory covering induction, greatest common divi-
sors, least common multiples, and the prime factorization theorem
ž New material on the order of an element and cyclic groups
ž More detail about the lattice of divisors of an integer
ž New historical notes on Fermat’s last theorem, the classiﬁcation theorem
for ﬁnite simple groups, ﬁnite afﬁne planes, and more
ž More detail on set theory and composition of functions
ž 26 new exercises, 46 counting parts
ž Updated symbols and notation
ž Updated bibliography
February 2003
WILLIAM J. GILBERT
W. KEITH NICHOLSON
xiii

LIST OF SYMBOLS
A
Algebraic numbers, 233
An
Alternating group on n elements, 70
C
Complex numbers, 4
C∗
Nonzero complex numbers, 48
Cn
Cyclic group of order n, 58
C[0, ∞)
Continuous real valued functions on [0, ∞), 173
Dn
Dihedral group of order 2n, 58
Dn
Divisors of n, 22
d(u, v)
Hamming distance between u and v, 269
deg
Degree of a polynomial, 166
e
Identity element of a group or monoid, 48, 137
eG
Identity element in the group G, 61
E(n)
Euclidean group in n dimensions, 104
F
Field, 4, 160
Fn
Switching functions of n variables, 28
Fixg
Set of elements ﬁxed under the action of g, 125
FM(A)
Free monoid on A, 140
gcd(a, b)
Greatest common divisor of a and b, 184, 299
GF(n)
Galois ﬁeld of order n, 227
GL(n, F)
General linear group of dimension n over F, 107
H
Quaternions, 177
I
Identity matrix, 4
Ik
k × k identity matrix, 277
Imf
Image of f , 87
Kerf
Kernel of f , 86
lcm(a, b)
Least common multiple of a and b, 184, 303
L(Rn, Rn)
Linear transformations from Rn to Rn, 163
Mn(R)
n × n matrices with entries from R, 4, 166
N
Nonnegative integers, 55
NAND
NOT-AND, 28, 36
NOR
NOT-OR, 28, 36
O(n)
Orthogonal group of dimension n, 105
Orb x
Orbit of x, 97
xv

xvi
LIST OF SYMBOLS
P
Positive integers, 3
P(X)
Power set of X, 8
Q
Rational numbers, 6
Q∗
Nonzero rational numbers, 48
Q
Quaternion group, 73
R
Real numbers, 2
R∗
Nonzero real numbers, 48
R+
Positive real numbers, 5
S(X)
Symmetric group of X, 50
Sn
Symmetric group on n elements, 63
SO(n)
Special orthogonal group of dimension n, 108
Stab x
Stabilizer of x, 97
SU(n)
Special unitary group of dimension n, 108
T(n)
Translations in n dimensions, 104
U(n)
Unitary group of dimension n, 108
Z
Integers, 5
Zn
Integers modulo n, 5, 78
Z∗
n
Integers modulo n coprime to n, 102
δ(x)
Dirac delta function, or remainder in general
division algorithm, 172, 181

Null sequence, 140
∅
Empty set, 7
φ(n)
Euler φ-function, 102
⋆
General binary operation or concatenation, 2, 140
*
Convolution, 168, 173
Ž
Composition, 49

Symmetric difference, 9, 29
−
Difference, 9
∧
Meet, 14
∨
Join, 14
⊆
Inclusion, 7
⩽
Less than or equal, 23
⇒
Implies, 17, 293
⇔
If and only if, 18, 295
∼=
Isomorphic, 60, 172
≡mod n
Congruent modulo n, 77
≡mod H
Congruent modulo H, 79
|X|
Number of elements in X, 12, 56
|G : H|
Index of H in G, 80
R∗
Invertible elements in the ring R, 188
a′
Complement of a in a boolean algebra, 14, 28
a−1
Inverse of a, 3, 48
A
Complement of the set A, 8
∩
Intersection of sets, 8
∪
Union of sets, 8

LIST OF SYMBOLS
xvii
∈
Membership in a set, 7
A–B
Set difference, 9
||v||
Length of v in Rn, 105
v · w
Inner product in Rn, 105
V T
Transpose of the matrix V , 104
□
End of a proof or example, 9
(a)
Ideal generated by a, 204
(a1a2 . . . an)
n-cycle, 64

1 2 . . . n
a1a2 . . . an

Permutation, 63

n
r

Binomial coefﬁcient n!/r!(n −r)!, 129
F(a)
Smallest ﬁeld containing F and a, 220
F(a1, . . . , an)
Smallest ﬁeld containing F and a1, . . . , an, 220
(n, k)-code
Code of length n with messages of length k, 266
(X, ⋆)
Group or monoid, 5, 48, 137
(R, +, ·)
Ring, 156
(K, ∧, ∨, ′)
Boolean algebra, 14
[x]
Equivalence class containing x, 77
[x]n
Congruence class modulo n containing x, 100
R[x]
Polynomials in x with coefﬁcients from R, 167
R[[x]]
Formal power series in x with coefﬁcients from R, 169
R[x1, . . . , xn]
Polynomials in x1, . . . , xn with coefﬁcients from R, 168
[K : F]
Degree of K over F, 219
XY
Set of functions from Y to X, 138
RN
Sequences of elements from R, 168
⟨ai⟩
Sequence whose ith term is ai, 168
G × H
Direct product of G and H, 91
S × S
Direct product of sets, 2
S/E
Quotient set, 77
G/H
Quotient group or set of right cosets, 83
R/I
Quotient ring, 206
a|b
a divides b, 21, 184, 299
l//m
l is parallel to m, 242
Ha
Right coset of H containing a, 79
aH
Left coset of H containing a, 82
I + r
Coset of I containing r, 205

1
INTRODUCTION
Algebra can be deﬁned as the manipulation of symbols. Its history falls into two
distinct parts, with the dividing date being approximately 1800. The algebra done
before the nineteenth century is called classical algebra, whereas most of that
done later is called modern algebra or abstract algebra.
CLASSICAL ALGEBRA
The technique of introducing a symbol, such as x, to represent an unknown
number in solving problems was known to the ancient Greeks. This symbol could
be manipulated just like the arithmetic symbols until a solution was obtained.
Classical algebra can be characterized by the fact that each symbol always
stood for a number. This number could be integral, real, or complex. However,
in the seventeenth and eighteenth centuries, mathematicians were not quite sure
whether the square root of −1 was a number. It was not until the nineteenth
century and the beginning of modern algebra that a satisfactory explanation of
the complex numbers was given.
The main goal of classical algebra was to use algebraic manipulation to solve
polynomial equations. Classical algebra succeeded in producing algorithms for
solving all polynomial equations in one variable of degree at most four. However,
it was shown by Niels Henrik Abel (1802–1829), by modern algebraic methods,
that it was not always possible to solve a polynomial equation of degree ﬁve
or higher in terms of nth roots. Classical algebra also developed methods for
dealing with linear equations containing several variables, but little was known
about the solution of nonlinear equations.
Classical algebra provided a powerful tool for tackling many scientiﬁc prob-
lems, and it is still extremely important today. Perhaps the most useful math-
ematical tool in science, engineering, and the social sciences is the method of
solution of a system of linear equations together with all its allied linear algebra.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
1

2
1
INTRODUCTION
MODERN ALGEBRA
In the nineteenth century it was gradually realized that mathematical symbols did
not necessarily have to stand for numbers; in fact, it was not necessary that they
stand for anything at all! From this realization emerged what is now known as
modern algebra or abstract algebra.
For example, the symbols could be interpreted as symmetries of an object, as
the position of a switch, as an instruction to a machine, or as a way to design
a statistical experiment. The symbols could be manipulated using some of the
usual rules for numbers. For example, the polynomial 3x2 + 2x −1 could be
added to and multiplied by other polynomials without ever having to interpret
the symbol x as a number.
Modern algebra has two basic uses. The ﬁrst is to describe patterns or sym-
metries that occur in nature and in mathematics. For example, it can describe
the different crystal formations in which certain chemical substances are found
and can be used to show the similarity between the logic of switching circuits
and the algebra of subsets of a set. The second basic use of modern algebra is
to extend the common number systems naturally to other useful systems.
BINARY OPERATIONS
The symbols that are to be manipulated are elements of some set, and the manipu-
lation is done by performing certain operations on elements of that set. Examples
of such operations are addition and multiplication on the set of real numbers.
As shown in Figure 1.1, we can visualize an operation as a “black box” with
various inputs coming from a set S and one output, which combines the inputs
in some speciﬁed way. If the black box has two inputs, the operation combines
two elements of the set to form a third. Such an operation is called a binary
operation. If there is only one input, the operation is called unary. An example
of a unary operation is ﬁnding the reciprocal of a nonzero real number.
If S is a set, the direct product S × S consists of all ordered pairs (a, b)
with a, b ∈S. Here the term ordered means that (a, b) = (a1, b1) if and only if
a = a1 and b = b1. For example, if we denote the set of all real numbers by R,
then R × R is the euclidean plane.
Using this terminology, a binary operation, ⋆, on a set S is really just a
particular function from S × S to S. We denote the image of the pair (a, b)
a
b
a ∗ b
c
c′
Binary operation
Unary operation
Figure 1.1

BINARY OPERATIONS
3
under this function by a ⋆b. In other words, the binary operation ⋆assigns to
any two elements a and b of S the element a ⋆b of S. We often refer to an
operation ⋆as being closed to emphasize that each element a ⋆b belongs to
the set S and not to a possibly larger set. Many symbols are used for binary
operations; the most common are +, ·, −, Ž , ÷, ∪, ∩, ∧, and ∨.
A unary operation on S is just a function from S to S. The image of c under
a unary operation is usually denoted by a symbol such as c′, c, c−1, or (−c).
Let P = {1, 2, 3, . . .} be the set of positive integers. Addition and multipli-
cation are both binary operations on P, because, if x, y ∈P, then x + y and
x · y ∈P. However, subtraction is not a binary operation on P because, for
instance, 1 −2 /∈P. Other natural binary operations on P are exponentiation and
the greatest common divisor, since for any two positive integers x and y, xy and
gcd(x, y) are well-deﬁned elements of P.
Addition, multiplication, and subtraction are all binary operations on R because
x + y, x · y, and x −y are real numbers for every pair of real numbers x and y.
The symbol −stands for a binary operation when used in an expression such as
x −y, but it stands for the unary operation of taking the negative when used in
the expression −x. Division is not a binary operation on R because division by
zero is undeﬁned. However, division is a binary operation on R −{0}, the set of
nonzero real numbers.
A binary operation on a ﬁnite set can often be presented conveniently by
means of a table. For example, consider the set T = {a, b, c}, containing three
elements. A binary operation ⋆on T is deﬁned by Table 1.1. In this table, x ⋆y
is the element in row x and column y. For example, b ⋆c = b and c ⋆b = a.
One important binary operation is the composition of symmetries of a given
ﬁgure or object. Consider a square lying in a plane. The set S of symmetries
of this square is the set of mappings of the square to itself that preserve dis-
tances. Figure 1.2 illustrates the composition of two such symmetries to form a
third symmetry.
Most of the binary operations we use have one or more of the following
special properties. Let ⋆be a binary operation on a set S. This operation is called
associative if a ⋆(b ⋆c) = (a ⋆b) ⋆c for all a, b, c ∈S. The operation ⋆is called
commutative if a ⋆b = b ⋆a for all a, b ∈S. The element e ∈S is said to be
an identity for ⋆if a ⋆e = e ⋆a = a for all a ∈S.
If ⋆is a binary operation on S that has an identity e, then b is called the
inverse of a with respect to ⋆if a ⋆b = b ⋆a = e. We usually denote the
TABLE 1.1. Binary Operation
on {a, b, c}
⋆
a
b
c
a
b
a
a
b
c
a
b
c
c
a
b

4
1
INTRODUCTION
1
2
3
4
4
1
2
3
1
4
3
2
Square in its
original position
Rotation
through p/2
Flip about
the vertical
axis
Flip about a diagonal axis
Figure 1.2.
Composition of symmetries of a square.
inverse of a by a−1; however, if the operation is addition, the inverse is denoted
by −a.
If ⋆and Ž are two binary operations on S, then Ž is said to be distributive over
⋆if a Ž (b ⋆c) = (a Ž b) ⋆(a Ž c) and (b ⋆c) Ž a = (b Ž a) ⋆(c Ž a) for all a, b, c ∈
S.
Addition and multiplication are both associative and commutative operations
on the set R of real numbers. The identity for addition is 0, whereas the mul-
tiplicative identity is 1. Every real number, a, has an inverse under addition,
namely, its negative, −a. Every nonzero real number a has a multiplicative
inverse, a−1. Furthermore, multiplication is distributive over addition because
a · (b + c) = (a · b) + (a · c) and (b + c) · a = (b · a) + (c · a); however, addi-
tion is not distributive over multiplication because a + (b · c) ̸= (a + b) · (a + c)
in general.
Denote the set of n × n real matrices by Mn(R). Matrix multiplication is an
associative operation on Mn(R), but it is not commutative (unless n = 1). The
matrix I, whose (i, j)th entry is 1 if i = j and 0 otherwise, is the multiplicative
identity. Matrices with multiplicative inverses are called nonsingular.
ALGEBRAIC STRUCTURES
A set, together with one or more operations on the set, is called an algebraic
structure. The set is called the underlying set of the structure. Modern algebra
is the study of these structures; in later chapters, we examine various types of
algebraic structures. For example, a ﬁeld is an algebraic structure consisting of
a set F together with two binary operations, usually denoted by + and ·, that
satisfy certain conditions. We denote such a structure by (F, +, ·).
In order to understand a particular structure, we usually begin by examining its
substructures. The underlying set of a substructure is a subset of the underlying
set of the structure, and the operations in both structures are the same. For
example, the set of complex numbers, C, contains the set of real numbers, R, as
a subset. The operations of addition and multiplication on C restrict to the same
operations on R, and therefore (R, +, ·) is a substructure of (C, +, ·).

EXTENDING NUMBER SYSTEMS
5
Two algebraic structures of a particular type may be compared by means of
structure-preserving functions called morphisms. This concept of morphism is
one of the fundamental notions of modern algebra. We encounter it among every
algebraic structure we consider.
More precisely, let (S, ⋆) and (T, Ž ) be two algebraic structures consisting of
the sets S and T , together with the binary operations ⋆on S and Ž on T . Then a
function f : S →T is said to be a morphism from (S, ⋆) to (T, Ž ) if for every
x, y ∈S,
f (x ⋆y) = f (x) Ž f (y).
If the structures contain more than one operation, the morphism must preserve
all these operations. Furthermore, if the structures have identities, these must be
preserved, too.
As an example of a morphism, consider the set of all integers, Z, under the
operation of addition and the set of positive real numbers, R+, under multiplica-
tion. The function f : Z →R+ deﬁned by f (x) = ex is a morphism from (Z, +)
to (R+, ·). Multiplication of the exponentials ex and ey corresponds to addition
of their exponents x and y.
A vector space is an algebraic structure whose underlying set is a set of
vectors. Its operations consist of the binary operation of addition and, for each
scalar λ, a unary operation of multiplication by λ. A function f : S →T , between
vector spaces, is a morphism if f (x + y) = f (x) + f (y) and f (λx) = λf (x) for
all vectors x and y in the domain S and all scalars λ. Such a vector space
morphism is usually called a linear transformation.
A morphism preserves some, but not necessarily all, of the properties of the
domain structure. However, if a morphism between two structures is a bijective
function (that is, one-to-one and onto), it is called an isomorphism, and the
structures are called isomorphic. Isomorphic structures have identical properties,
and they are indistinguishable from an algebraic point of view. For example, two
vector spaces of the same ﬁnite dimension over a ﬁeld F are isomorphic.
One important method of constructing new algebraic structures from old ones
is by means of equivalence relations. If (S, ⋆) is a structure consisting of the set
S with the binary operation ⋆on it, the equivalence relation ∼on S is said to be
compatible with ⋆if, whenever a ∼b and c ∼d, it follows that a ⋆c ∼b ⋆d.
Such a compatible equivalence relation allows us to construct a new structure
called the quotient structure, whose underlying set is the set of equivalence
classes. For example, the quotient structure of the integers, (Z, +, ·), under the
congruence relation modulo n, is the set of integers modulo n, (Zn, +, ·) (see
Appendix 2).
EXTENDING NUMBER SYSTEMS
In the words of Leopold Kronecker (1823–1891), “God created the natural num-
bers; everything else was man’s handiwork.” Starting with the set of natural

6
1
INTRODUCTION
numbers under addition and multiplication, we show how this can be extended
to other algebraic systems that satisfy properties not held by the natural numbers.
The integers (Z, +, ·) is the smallest system containing the natural numbers, in
which addition has an identity (the zero) and every element has an inverse under
addition (its negative). The integers have an identity under multiplication (the
element 1), but 1 and −1 are the only elements with multiplicative inverses. A
standard construction will produce the ﬁeld of fractions of the integers, which is
the rational number system (Q, +, ·), and we show that this is the smallest ﬁeld
containing (Z, +, ·). We can now divide by nonzero elements in Q and solve
every linear equation of the form ax = b (a ̸= 0). However, not all quadratic
equations have solutions in Q; for example, x2 −2 = 0 has no rational solution.
The next step is to extend the rationals to the real number system (R, +, ·).
The construction of the real numbers requires the use of nonalgebraic concepts
such as Dedekind cuts or Cauchy sequences, and we will not pursue this, being
content to assume that they have been constructed. Even though many polynomial
equations have real solutions, there are some, such as x2 + 1 = 0, that do not.
We show how to extend the real number system by adjoining a root of x2 + 1
to obtain the complex number system (C, +, ·). The complex number system
is really the end of the line, because Carl Friedrich Gauss (1777–1855), in his
doctoral thesis, proved that any nonconstant polynomial with real or complex
coefﬁcients has a root in the complex numbers. This result is now known as the
fundamental theorem of algebra.
However, the classical number system can be generalized in a different way.
We can look for ﬁelds that are not subﬁelds of (C, +, ·). An example of such a
ﬁeld is the system of integers modulo a prime p, (Zp, +, ·). All the usual oper-
ations of addition, subtraction, multiplication, and division by nonzero elements
can be performed in Zp. We show that these ﬁelds can be extended and that
for each prime p and positive integer n, there is a ﬁeld (GF(pn), +, ·) with pn
elements. These ﬁnite ﬁelds are called Galois ﬁelds after the French mathemati-
cian ´Evariste Galois. We use Galois ﬁelds in the construction of orthogonal latin
squares and in coding theory.

2
BOOLEAN ALGEBRAS
A boolean algebra is a good example of a type of algebraic structure in which the
symbols usually represent nonnumerical objects. This algebra is modeled after
the algebra of subsets of a set under the binary operations of union and inter-
section and the unary operation of complementation. However, boolean algebra
has important applications to switching circuits, where each symbol represents a
particular electrical circuit or switch. The origin of boolean algebra dates back
to 1847, when the English mathematician George Boole (1815–1864) published
a slim volume entitled The Mathematical Analysis of Logic, which showed how
algebraic symbols could be applied to logic. The manipulation of logical propo-
sitions by means of boolean algebra is now called the propositional calculus.
At the end of this chapter, we show that any ﬁnite boolean algebra is equivalent
to the algebra of subsets of a set; in other words, there is a boolean algebra
isomorphism between the two algebras.
ALGEBRA OF SETS
In this section, we develop some properties of the basic operations on sets. A set
is often referred to informally as a collection of objects called the elements of
the set. This is not a proper deﬁnition—collection is just another word for set.
What is clear is that there are sets, and there is a notion of being an element
(or member) of a set. These fundamental ideas are the primitive concepts of
set theory and are left undeﬁned.∗The fact that a is an element of a set X is
denoted a ∈X. If every element of X is also an element of Y, we write X ⊆Y
(equivalently, Y ⊇X) and say that X is contained in Y, or that X is a subset
of Y. If X and Y have the same elements, we say that X and Y are equal sets
and write X = Y. Hence X = Y if and only if both X ⊆Y and Y ⊆X. The set
with no elements is called the empty set and is denoted as Ø.
∗Certain basic properties of sets must also be assumed (called the axioms of the theory), but it is
not our intention to go into this here.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
7

8
2
BOOLEAN ALGEBRAS
Let X be any set. The set of all subsets of X is called the power
set of X and is denoted by P(X). Hence P(X) = {A|A ⊆X}. Thus if
X = {a, b}, then P(X) = {Ø, {a}, {b}, X}. If X = {1, 2, 3}, then P(X) =
{Ø, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, X}.
If A and B are subsets of a set X, their intersection A ∩B is deﬁned to be
the set of elements common to A and B, and their union A ∪B is the set of
elements in A or B (or both). More formally,
A ∩B = {x|x ∈A and x ∈B}
and
A ∪B = {x|x ∈A or x ∈B}.
The complement of A in X is A = {x|x ∈X and x /∈A} and is the set of
elements in X that are not in A. The shaded areas of the Venn diagrams in
Figure 2.1 illustrate these operations.
Union and intersection are both binary operations on the power set P(X),
whereas complementation is a unary operation on P(X). For example, with
X = {a, b}, the tables for the structures (P(X), ∩), (P(X), ∪) and (P(X), −)
are given in Table 2.1, where we write A for {a} and B for {b}.
Proposition 2.1. The following are some of the more important relations involv-
ing the operations ∩, ∪, and −, holding for all A, B, C ∈P(X).
(i) A ∩(B ∩C) = (A ∩B) ∩C.
(ii) A ∪(B ∪C) = (A ∪B) ∪C.
(iii) A ∩B = B ∩A.
(iv) A ∪B = B ∪A.
(v) A ∩(B ∪C)
= (A ∩B) ∪(A ∩C).
(vi) A ∪(B ∩C)
= (A ∪B) ∩(A ∪C).
(vii) A ∩X = A.
(viii) A ∪Ø = A.
(ix) A ∩A = Ø.
(x) A ∪A = X.
(xi) A ∩Ø = Ø.
(xii) A ∪X = X.
(xiii) A ∩(A ∪B) = A.
(xiv) A ∪(A ∩B) = A.
A
B
X
X
A
B
X
A
A ∩ B
A ∪ B
A–
Figure 2.1.
Venn diagrams.
TABLE 2.1. Intersection, Union, and Complements in P({a, b})
∩
Ø
A
B
X
∪
Ø
A
B
X
Subset
Complement
Ø
Ø
Ø
Ø
Ø
Ø
Ø
A
B
X
Ø
X
A
Ø
A
Ø
A
A
A
A
X
X
A
B
B
Ø
Ø
B
B
B
B
X
B
X
B
A
X
Ø
A
B
X
X
X
X
X
X
X
Ø

ALGEBRA OF SETS
9
(xv) A ∩A = A.
(xvi) A ∪A = A.
(xvii) (A ∩B) = A ∪B.
(xviii) (A ∪B) = A ∩B.
(xix) X = Ø.
(xx) Ø = X.
(xxi) A = A.
Proof. We shall prove relations (v) and (x) and leave the proofs of the others
to the reader.
(v) A ∩(B ∪C) = {x|x ∈A and x ∈B ∪C}
= {x|x ∈A and (x ∈B or x ∈C)}
= {x|(x ∈A and x ∈B) or (x ∈A and x ∈C)}
= {x|x ∈A ∩B or x ∈A ∩C}
= (A ∩B) ∪(A ∩C).
The Venn diagrams in Figure 2.2 illustrate this result.
(x) A ∪A = {x|x ∈A or x ∈A}
= {x|x ∈A or (x ∈X and x /∈A)}
= {x|(x ∈X and x ∈A) or (x ∈X and x /∈A)}, since A ⊆X
= {x|x ∈X and (x ∈A or x /∈A)}
= {x|x ∈X}, since it is always true that x ∈A or x /∈A
= X.
□
Relations (i)–(iv), (vii), and (viii) show that ∩and ∪are associative and
commutative operations on P(X) with identities X and Ø, respectively. The
only element with an inverse under ∩is its identity X, and the only element with
an inverse under ∪is its identity Ø.
Note the duality between ∩and ∪. If these operations are interchanged in any
relation, the resulting relation is also true.
Another operation on P(X) is the difference of two subsets. It is deﬁned by
A −B = {x|x ∈A and x /∈B} = A ∩B.
Since this operation is neither associative nor commutative, we introduce another
operation AB, called the symmetric difference, illustrated in Figure 2.3,
A
B
C
A ∩ (B ∪ C)
(A ∩ B) ∪ (A ∩ C)
A
B
C
Figure 2.2.
Venn diagrams illustrating a distributive law.

10
2
BOOLEAN ALGEBRAS
X
A
B
A − B
A ∆ B
X
A
B
Figure 2.3.
Difference and symmetric difference of sets.
deﬁned by
AB = (A ∩B) ∪(A ∩B) = (A ∪B) −(A ∩B) = (A −B) ∪(B −A).
The symmetric difference of A and B is the set of elements in A or B, but not
in both. This is often referred to as the exclusive OR function of A and B.
Example 2.2. Write down the table for the structure (P(X), ) when X =
{a, b}.
Solution. The table is given in Table 2.2, where we write A for {a} and B
for {b}.
□
Proposition 2.3. The operation  is associative and commutative on P(X); it
has an identity Ø, and each element is its own inverse. That is, the following
relations hold for all A, B, C ∈P(X):
(i) A(BC) = (AB)C.
(ii) AB = BA.
(iii) AØ = A.
(iv) AA = Ø.
Three further properties of the symmetric difference are:
(v) AX = A.
(vi) AA = X.
(vii) A ∩(BC) = (A ∩B)(A ∩C).
Proof. (ii) follows because the deﬁnition of AB is symmetric in A and B.
To prove (i) observe ﬁrst that Proposition 2.1 gives
BC = (B ∩C) ∪(B ∩C) = (B ∪C) ∩(B ∪C)
= (B ∩B) ∪(B ∩C) ∪(C ∩B) ∪(C ∩C)
= (B ∩C) ∪(B ∩C).
TABLE 2.2. Symmetric Difference in P({a, b})

Ø
A
B
X
Ø
Ø
A
B
X
A
A
Ø
X
B
B
B
X
Ø
A
X
X
B
A
Ø

NUMBER OF ELEMENTS IN A SET
11
A ∆ (B ∆ C) = (A ∆ B) ∆ C
A ∩ (B ∆ C) = (A ∩ B) ∆ (A ∩ C)
A
B
C
A
B
C
Figure 2.4.
Venn diagrams.
A ∪ (B ∆ C)
A
B
C
(A ∪ B) ∆ (A ∪ C)
A
B
C
Figure 2.5.
Venn diagrams of unequal expressions.
Hence
A(BC) = {A ∩(BC)} ∪{A ∩(BC)}
= {A ∩[(B ∩C) ∪(B ∩C)]} ∪{A ∩[(B ∩C) ∪(B ∩C)]}
= (A ∩B ∩C) ∪(A ∩B ∩C) ∪(A ∩B ∩C) ∪(A ∩B ∩C)).
This expression is symmetric in A, B, and C, so (ii) gives
A(BC) = C(AB) = (AB)C.
We leave the proof of the other parts to the reader. Parts (i) and (vii) are
illustrated in Figure 2.4.
□
Relation (vii) of Proposition 2.3 is a distributive law and states that ∩is
distributive over . It is natural to ask whether ∪is distributive over .
Example 2.4. Is it true that A ∪(BC) = (A ∪B)(A ∪C) for all A, B, C ∈
P(X)?
Solution. The Venn diagrams for each side of the equation are given in
Figure 2.5. If the shaded areas are not the same, we will be able to ﬁnd a
counter example. We see from the diagrams that the result will be false if
A is nonempty. If A = X and B = C = Ø, then A ∪(BC) = A, whereas
(A ∪B)(A ∪C) = Ø; thus union is not distributive over symmetric difference.
□
NUMBER OF ELEMENTS IN A SET
If a set X contains two or three elements, we have seen that P(X) contains 22
or 23 elements, respectively. This suggests the following general result on the
number of subsets of a ﬁnite set.

12
2
BOOLEAN ALGEBRAS
Theorem 2.5. If X is a ﬁnite set with n elements, then P(X) contains 2n
elements.
Proof. Each of the n elements of X is either in a given subset A or not in A.
Hence, in choosing a subset of X, we have two choices for each element, and
these choices are independent. Therefore, the number of choices is 2n, and this
is the number of subsets of X.
If n = 0, then X = Ø and P(X) = {Ø}, which contains one element.
□
Denote the number of elements of a set X by |X|. If A and B are ﬁnite
disjoint sets (that is, A ∩B = Ø), then
|A ∪B| = |A| + |B|.
Proposition 2.6. For any two ﬁnite sets A and B,
|A ∪B| = |A| + |B| −|A ∩B|.
Proof. We can express A ∪B as the disjoint union of A and B −A; also,
B can be expressed as the disjoint union of B −A and A ∩B as shown in
Figure 2.6. Hence |A ∪B| = |A| + |B −A| and |B| = |B −A| + |A ∩B|. It fol-
lows that |A ∪B| = |A| + |B| −|A ∩B|.
□
Proposition 2.7. For any three ﬁnite sets A, B, and C,
|A ∪B ∪C| = |A| + |B| + |C| −|A ∩B| −|A ∩C|
−|B ∩C| + |A ∩B ∩C|.
Proof. Write A ∪B ∪C as (A ∪B) ∪C. Then, by Proposition 2.6,
|A ∪B ∪C| = |A ∪B| + |C| −|(A ∪B) ∩C|
= |A| + |B| −|A ∩B| + |C| −|(A ∩C) ∪(B ∩C)|
= |A| + |B| + |C| −|A ∩B| −|A ∩C| −|B ∩C|
+ |(A ∩C) ∩(B ∩C)|.
The result follows because (A ∩C) ∩(B ∩C) = A ∩B ∩C.
□
A
B − A
B − A
A ∩ B
Figure 2.6

BOOLEAN ALGEBRAS
13
W
B
C
520
60
120
10
20
110
200
Figure 2.7.
Different classes of commuters.
Example 2.8. A survey of 1000 commuters reported that 850 sometimes used a
car, 200 a bicycle, and 350 walked, whereas 130 used a car and a bicycle, 220
used a car and walked, 30 used a bicycle and walked, and 20 used all three. Are
these ﬁgures consistent?
Solution. Let C, B, and W be the sets of commuters who sometimes used a
car, a bicycle, and walked, respectively. Then
|C ∪B ∪W| = |C| + |B| + |W| −|C ∩B| −|C ∩W|
−|B ∩W| + |C ∩B ∩W|
= 850 + 200 + 350 −130 −220 −30 + 20
= 1040.
Since this number is greater than 1000, the ﬁgures must be inconsistent. The
breakdown of the reported ﬁgures into their various classes is illustrated in
Figure 2.7. The sum of all these numbers is 1040.
□
Example 2.9. If 47% of the people in a community voted in a local election and
75% voted in a federal election, what is the least percentage that voted in both?
Solution. Let L and F be the sets of people who voted in the local and federal
elections, respectively. If n is the total number of voters in the community, then
|L| + |F| −|L ∩F| = |L ∪F| ⩽n. It follows that
|L ∩F| ⩾|L| + |F| −n =
 47
100 + 75
100 −1

n = 22
100n.
Hence at least 22% voted in both elections.
□
BOOLEAN ALGEBRAS
We now give the deﬁnition of an abstract boolean algebra in terms of a set
with two binary operations and one unary operation on it. We show that various
algebraic structures, such as the algebra of sets, the logic of propositions, and

14
2
BOOLEAN ALGEBRAS
the algebra of switching circuits are all boolean algebras. It then follows that
any general result derived from the axioms will hold in all our examples of
boolean algebras.
It should be noted that this axiom system is only one of many equivalent ways
of deﬁning a boolean algebra. Another common way is to deﬁne a boolean algebra
as a lattice satisfying certain properties (see the section “Posets and Lattices”).
A boolean algebra (K, ∧, ∨, ′) is a set K together with two binary operations
∧and ∨, and a unary operation ′ on K satisfying the following axioms for all
A, B, C ∈K:
(i) A ∧(B ∧C) = (A ∧B) ∧C.
(ii) A ∨(B ∨C) = (A ∨B) ∨C.
(associative laws)
(iii) A ∧B = B ∧A.
(iv) A ∨B = B ∨A.
(commutative laws)
(v) A ∧(B ∨C)
= (A ∧B) ∨(A ∧C).
(vi) A ∨(B ∧C)
= (A ∨B) ∧(A ∨C).
(distributive laws)
(vii) There is a zero element 0 in K such that A ∨0 = A.
(viii) There is a unit element 1 in K such that A ∧1 = A.
(ix) A ∧A′ = 0.
(x) A ∨A′ = 1.
We call the operations ∧and ∨, meet and join, respectively. The element A′
is called the complement of A.
The associative axioms (i) and (ii) are redundant in the system above because
with a little effort they can be deduced from the other axioms. However, since
associativity is such an important property, we keep these properties as axioms.
It follows from Proposition 2.1 that (P(X), ∩, ∪,
−) is a boolean algebra
with Ø as zero and X as unit. When X = Ø, this boolean algebra of subsets
contains one element, and this is both the zero and unit. It can be proved (see
Exercise 2.17) that if the zero and unit elements are the same, the boolean algebra
must have only one element.
We can deﬁne a two-element boolean algebra ({0, 1}, ∧, ∨,
′) by means of
Table 2.3.
Proposition 2.10. If the binary operation ⋆on the set K has an identity e such
that a ⋆e = e ⋆a = a for all a ∈K, then this identity is unique.
TABLE 2.3. Two-Element Boolean Algebra
A
B
A ∧B
A ∨B
0
0
0
0
0
1
0
1
1
0
0
1
1
1
1
1
A
A′
0
1
1
0

BOOLEAN ALGEBRAS
15
Proof. Suppose that e and e′ are both identities. Then e = e ⋆e′, since e′ is
an identity, and e ⋆e′ = e′ since e is an identity. Hence e = e′, so the identity
must be unique.
□
Corollary 2.11. The zero and unit elements in a boolean algebra are unique.
Proof. This follows directly from the proposition above, because the zero
and unit elements are the identities for the join and meet operations, respec-
tively.
□
Proposition 2.12. The complement of an element in a boolean algebra is unique;
that is, for each A ∈K there is only one element A′ ∈K satisfying axioms (ix)
and (x): A ∧A′ = 0 and A ∨A′ = 1.
Proof. Suppose that B and C are both complements of A, so that A ∧B =
0, A ∨B = 1, A ∧C = 0, and A ∨C = 1. Then
B = B ∨0 = B ∨(A ∧C) = (B ∨A) ∧(B ∨C)
= (A ∨B) ∧(B ∨C) = 1 ∧(B ∨C) = B ∨C.
Similarly, C = C ∨B and so B = B ∨C = C ∨B = C.
□
If we interchange ∧and ∨and interchange 0 and 1 in the system of axioms
for a boolean algebra, we obtain the same system. Therefore, if any proposition
is derivable from the axioms, so is the proposition obtained by interchanging
∧and ∨and interchanging 0 and 1. This is called the duality principle. For
example, in the following proposition, there are four pairs of dual statements. If
one member of each pair can be proved, the other will follow directly from the
duality principle.
If (K, ∧, ∨, ′) is a boolean algebra with 0 as zero and 1 as unit, then (K, ∨, ∧, ′)
is also a boolean algebra with 1 as zero and 0 as unit.
Proposition 2.13. If A, B, and C are elements of a boolean algebra (K, ∧, ∨, ′),
the following relations hold:
(i) A ∧0 = 0.
(ii) A ∨1 = 1.
(iii) A ∧(A ∨B) = A.
(iv) A ∨(A ∧B) = A.
(absorption laws)
(v) A ∧A = A.
(vi) A ∨A = A. (idempotent laws)
(vii) (A ∧B)′ = A′ ∨B′.
(viii) (A ∨B)′ = A′ ∧B′.
(De Morgan’s laws)
(ix) (A′)′ = A.
Proof. Note ﬁrst that relations (ii), (iv), (vi), and (viii) are the duals of relations
(i), (iii), (v), and (vii), so we prove the last four, and relation (ix). We use the
axioms for a boolean algebra several times.

16
2
BOOLEAN ALGEBRAS
(i) A ∧0 = A ∧(A ∧A′) = (A ∧A) ∧A′ = A ∧A′ = 0.
(iii) A ∧(A ∨B) = (A ∨0) ∧(A ∨B) = A ∨(0 ∧B) = A ∨0 = A.
(v) A = A ∧1 = A ∧(A ∨A′) = (A ∧A) ∨(A ∧A′)
= (A ∧A) ∨0 = A ∧A.
Relations (vii) follows from Proposition 2.12 if we can show that A′ ∨B′ is
a complement of A ∧B [then it is the complement (A ∧B)′]. Now using part
(i) of this proposition,
(A ∧B) ∧(A′ ∨B′) = [(A ∧B) ∧A′] ∨[(A ∧B) ∧B′]
= [(A ∧A′) ∧B] ∨[A ∧(B ∧B′)]
= [0 ∧B] ∨[A ∧0]
= 0 ∨0
= 0.
Similarly, part (ii) gives
(A ∧B) ∨(A′ ∨B′) = [A ∨(A′ ∨B′)] ∧[B ∨(A′ ∨B′)]
= [(A ∨A′) ∨B′] ∧[(B ∨B′) ∨A′]
= [1 ∨B′] ∧[1 ∨A′]
= 1 ∧1
= 1.
To prove relation (ix), by deﬁnition we have A′ ∧A = 0 and A′ ∨A = 1.
Therefore, A is a complement of A′, and since the complement is unique,
A = (A′)′.
□
PROPOSITIONAL LOGIC
We now show brieﬂy how boolean algebra can be applied to the logic of propo-
sitions. Consider two sentences “A” and “B”, which may either be true or false.
For example, “A” could be “This apple is red,” and “B” could be “This pear
is green.” We can combine these to form other sentences, such as “A and B,”
which would be “This apple is red, and this pear is green.” We could also form
the sentence “not A,” which would be “This apple is not red.” Let us now com-
pare the truth or falsity of the derived sentences with the truth or falsity of the
original ones. We illustrate the relationship by means of a diagram called a truth
table. Table 2.4 shows the truth tables for the expressions “A and B,” “A or B,”
and “not A.” In these tables, T stands for “true” and F stands for “false.” For

PROPOSITIONAL LOGIC
17
TABLE 2.4. Truth Tables
A
B
A and B
A or B
F
F
F
F
F
T
F
T
T
F
F
T
T
T
T
T
A
Not A
F
T
T
F
example, if the statement “A” is true while “B” is false, the statement “A and
B” will be false, and the statement “A or B” will be true.
We can have two seemingly different sentences with the same meaning; for
example, “This apple is not red or this pear is not green” has the same meaning
as “It is not true that this apple is red and that this pear is green.” If two
sentences, P and Q, have the same meaning, we say that P and Q are logically
equivalent, and we write P = Q. The example above concerning apples and
pears implies that
(not A) or (not B) = not (A and B).
This equation corresponds to De Morgan’s law in a boolean algebra.
It appears that a set of sentences behaves like a boolean algebra. To be more
precise, let us consider a set of sentences that are closed under the operations of
“and,” “or,” and “not.” Let K be the set, each element of which consists of all
the sentences that are logically equivalent to a particular sentence. Then it can
be veriﬁed that (K, and, or, not) is indeed a boolean algebra. The zero element
is called a contradiction, that is, a statement that is always false, such as “This
apple is red and this apple is not red.” The unit element is called a tautology, that
is, a statement that is always true, such as “This apple is red or this apple is not
red.” This allows us to manipulate logical propositions using formulas derived
from the axioms of a boolean algebra.
An important method of combining two statements, A and B, in a sentence is
by a conditional, such as “If A, then B,” or equivalently, “A implies B,” which
we shall write as “A ⇒B.” How does the truth or falsity of such a conditional
depend on that of A and B? Consider the following sentences:
1. If x > 4, then x2 > 16.
2. If x > 4, then x2 = 2.
3. If 2 = 3, then 0.2 = 0.3.
4. If 2 = 3, then the moon is made of green cheese.
Clearly, if A is true, then B must also be true for the sentence “A ⇒B”
to be true. However, if A is not true, then the sentence “If A, then B” has
no standard meaning in everyday language. Let us take “A ⇒B” to mean that
we cannot have A true and B not true. This implies that the truth value of the

18
2
BOOLEAN ALGEBRAS
TABLE 2.5. Truth tables for Conditional and
Biconditional Statements
A
B
A ⇒B
A ⇐B
A ⇔B
F
F
T
T
T
F
T
T
F
F
T
F
F
T
F
T
T
T
T
T
statement “A ⇒B” is the same as that of “not (A and not B).” Let us write
∧, ∨, and ′ for “and,” “or,” and “not,” respectively. Then “A ⇒B” is equivalent
to (A ∧B′)′ = A′ ∨B. Thus “A ⇒B” is true if A is false or if B is true.
Using this deﬁnition, statements 1, 3, and 4 are all true, whereas statement 2
is false.
We can combine two conditional statements to form a biconditional statement
of the form “A if and only if B” or “A ⇔B.” This has the same truth value as
“(A ⇒B) and (B ⇒A)” or, equivalently, (A ∧B) ∨(A′ ∧B′). Another way
of expressing this biconditional is to say that “A is a necessary and sufﬁcient
condition for B.” It is seen from Table 2.5 that the statement “A ⇔B” is true if
either A and B are both true or A and B are both false.
Example 2.14. Apply this propositional calculus to determine whether a certain
politician’s arguments are consistent. In one speech he states that if taxes are
raised, the rate of inﬂation will drop if and only if the value of the dollar does
not fall. On television, he says that if the rate of inﬂation decreases or the value
of the dollar does not fall, taxes will not be raised. In a speech abroad, he states
that either taxes must be raised or the value of the dollar will fall and the rate of
inﬂation will decrease. His conclusion is that taxes will be raised, but the rate of
inﬂation will decrease, and the value of the dollar will not fall.
Solution. We write
A to mean “Taxes will be raised,”
B to mean “The rate of inﬂation will decrease,”
C to mean “The value of the dollar will not fall.”
The politician’s three statements can be written symbolically as
(i) A ⇒(B ⇔C).
(ii) (B ∨C) ⇒A′.
(iii) A ∨(C′ ∧B).
His conclusion is (iv) A ∧B ∧C.
The truth values of the ﬁrst two statements are equivalent to those of the
following:

SWITCHING CIRCUITS
19
TABLE 2.6. Truth Tables for the Politician’s Arguments
A
B
C
(i)
(ii)
(iii)
(i) ∧(ii) ∧(iii)
(iv)
(i) ∧(ii) ∧(iii) ⇒(iv)
F
F
F
T
T
F
F
F
T
F
F
T
T
T
F
F
F
T
F
T
F
T
T
T
T
F
F
F
T
T
T
T
F
F
F
T
T
F
F
T
T
T
T
F
F
T
F
T
F
F
T
F
F
T
T
T
F
F
F
T
F
F
T
T
T
T
T
F
T
F
T
T
(i) A′ ∨((B ∧C) ∨(B′ ∧C′)).
(ii) (B ∨C)′ ∨A′.
It follows from Table 2.6 that (i) ∧(ii) ∧(iii) ⇒(iv) is not a tautology; that
is, it is not always true. Therefore, the politician’s arguments are incorrect. They
break down when A and C are false and B is true, and when B and C are false
and A is true.
□
SWITCHING CIRCUITS
In this section we use boolean algebra to analyze some simple switching circuits.
A switch is a device with two states; state 1 is the “on” state, and state 0 the “off”
state. An ordinary household light switch is such a device, but the theory holds
equally well for more sophisticated electronic or magnetic two-state devices. We
analyze circuits with two terminals: The circuit is said to be closed if current can
pass between the terminals, and open if current cannot pass.
We denote a switch A by the symbol in Figure 2.8. We assign the value 1 to
A if the switch A is closed and the value 0 if it is open. We denote two switches
by the same letter if they open and close simultaneously. If B is a switch that is
always in the opposite position to A (that is, if B is open when A is closed, and
B is closed when A is open), denote switch B by A′.
The two switches A and B in Figure 2.9 are said to be connected in series. If
we connect this circuit to a power source and a light as in Figure 2.10, we see
that the light will be on if and only if A and B are both switched on; we denote
this series circuit by A ∧B. Its effect is shown in Table 2.7.
The switches A and B in Figure 2.11 are said to be in parallel, and this circuit
is denoted by A ∨B because the circuit is closed if either A or B is switched on.
A
Figure 2.8.
Switch A.
A
B
Figure 2.9.
Switches in series.

20
2
BOOLEAN ALGEBRAS
A
B
Power source
Light
Figure 2.10.
Series circuit.
A
B
Figure 2.11.
Switches in parallel.
TABLE 2.7. Effect of the Series Circuit
Switch A
Switch B
Circuit A ∧B
Light
0 (off)
0 (off)
0 (open)
off
0 (off)
1 (on)
0 (open)
off
1 (on)
0 (off)
0 (open)
off
1 (on)
1 (on)
1 (closed)
on
A
A′
B′
B
Figure 2.12.
Series-parallel circuit.
The reader should be aware that many books on switching theory use the
notation + and · instead of ∨and ∧, respectively.
Series and parallel circuits can be combined to form circuits like the one in
Figure 2.12. This circuit would be denoted by (A ∨(B ∧A′)) ∧B′. Such circuits
are called series-parallel switching circuits.
In actual practice, the wiring diagram may not look at all like Figure 2.12,
because we would want switches A and A′ together and B and B′ together.
Figure 2.13 illustrates one particular form that the wiring diagram could take.
Two circuits C1 and C2 involving the switches A, B, . . . are said to be equiv-
alent if the positions of the switches A, B, . . ., which allow current to pass,
Switch A
Switch B
Figure 2.13.
Wiring diagram of the circuit.

DIVISORS
21
A
B
C
A
B
A
C
A ∧ (B ∨ C)
(A ∧ B) ∨ (A ∧ C)
=
Figure 2.14.
Distributive law.
are the same for both circuits. We write C1 = C2 to mean that the circuits are
equivalent. It can be veriﬁed that all the axioms for a boolean algebra are valid
when interpreted as series-parallel switching circuits. For example, Figure 2.14
illustrates a distributive law. The zero corresponds to a circuit that is always
open, and the unit corresponds to a circuit that is always closed. The com-
plement C′ of a circuit C is open whenever C is closed and closed when C
is open.
DIVISORS
As a last example, we are going to construct boolean algebras based on the
divisibility relation on the set P of positive integers. Given two integers d and
a in P, we write d|a (and call d a divisor of a) if a = qd for some q ∈P. If
p ⩾2 in P, and the only divisors of p are 1 and p, then p is called a prime.
Thus, the ﬁrst few primes are 2, 3, 5, 7, 11, . . .. A fundamental fact about P is
the prime factorization theorem: Every number a ∈P is uniquely a product
of primes.∗
For example, the prime factorizations of 110 and 12 are 110 = 2 · 5 · 11 and
12 = 22 · 3. If a = pa1
1 pa2
2 · · · par
r
is the prime factorization of a ∈P where the
pi are distinct primes, the divisors d of a can be described as follows:
d|a
if and only if
d = pd1
1 pd2
2 · · · pdr
r
where
0 ⩽di ⩽ai for each i.
Hence the divisors of 12 = 2231 in P are 1 = 2030, 2 = 2130, 4 = 2230, 3 =
2031, 6 = 2131, and 12 = 2231.
Given a and b in P, let p1, p2, . . . , pr denote the distinct primes that are divisors
of either a or b. Hence we can write a = pa1
1 pa2
2 · · · par
r and b = pb1
1 pb2
2 · · · pbr
r ,
where ai ⩾0 and bi ⩾0 for each i. Then the greatest common divisor d =
gcd(a, b) and the least common multiple m = lcm(a, b) of a and b are deﬁned by
d = pmin(a,b)
1
pmin(a,b)
2
· · · pmin(a,b)
r
and
m = pmax(a,b)
1
pmax(a,b)
2
· · · pmax(a,b)
r
.
∗See Appendix 2 for a proof of the prime factorization theorem.

22
2
BOOLEAN ALGEBRAS
It follows that d is the unique integer in P that is a divisor of both a and b, and
is a multiple of every such common divisor (hence the name). Similarly, m is
the unique integer in P that is a multiple of both a and b, and is a divisor of
every such common multiple. For example, gcd(2, 3) = 1 and gcd(12, 28) = 4,
while lcm(2, 3) = 6 and lcm(12, 28) = 84.
With this background, we can describe some new examples of boolean alge-
bras. Given n ∈P, let
Dn = {d ∈P|d divides n}.
It is clear that gcd and lcm are commutative binary operations on Dn, and it is
easy to verify that the zero is 1 and the unit is n. To prove the distributive laws,
let a, b, and c be elements of Dn, and write
a = pa1
1 pa2
2 · · · par
r ,
b = pb1
1 pb2
2 · · · pbr
r ,
and
c = pc1
1 pc2
2 · · · pcr
r ,
where p1, p2, . . . , pr are the distinct primes dividing at least one of a, b, and c,
and where ai ⩾0, bi ⩾0, and ci ⩾0 for each i. Then the ﬁrst distributive law
states that
gcd(a, lcm(b, c)) = lcm(gcd(a, b), gcd(a, c)).
If we write out the prime factorization of each side in terms of the primes pi, this
holds if and only if for each i, the powers of pi are equal on both sides, that is,
min(ai, max(bi, ci)) = max(min(ai, bi), min(ai, ci)).
To verify this, observe ﬁrst that we may assume that bi ⩽ci (bi and ci can be
interchanged without changing either side), and then check the three cases ai ⩽
bi, bi ⩽ai ⩽ci, and ci ⩽ai separately. Hence the ﬁrst distributive law holds; the
other distributive law and the associative laws are veriﬁed similarly. Thus (Dn,
gcd, lcm) satisﬁes all the axioms for a boolean algebra except for the existence
of a complement.
But complements need not exist in general: For example, 6 has no comple-
ment in D18 = {1, 2, 3, 6, 9, 18}. Indeed, if 6 has a complement 6′ in D18, then
gcd(6, 6′) = 1, so we must have 6′ = 1. But then lcm(6, 6′) = 6 and this is not
the unit of D18. Hence 6 has no complement, so D18 is not a boolean alge-
bra. However, all is not lost. The problem in D18 is that the prime factorization
18 = 2 · 32 has a repeated prime factor. An integer n ∈P is called square-free
if it is a product of distinct primes with none repeated (for example, every prime
is square-free, as are 6 = 2 · 3, 10 = 2 · 5, 30 = 2 · 3 · 5, etc.) If n is square-free,
it is routine to verify that the complement of d ∈Dn is d′ = n/d, and we have
Example 2.15. If n ∈P is square-free, then (Dn, gcd, lcm, ′) is a boolean alge-
bra where d′ = n/d for each d ∈Dn.
The interpretations of the various boolean algebra terms are given in Table 2.8.

POSETS AND LATTICES
23
TABLE 2.8. Dictionary of Boolean Algebra Terms
Boolean
Algebra
P(X)
Switching
Circuits
Propositional
Logic
Dn
∧
∩
Series
And
gcd
∨
∪
Parallel
Or
lcm
′
−
Opposite
Not
a′ = n/a
0
Ø
Open
Contradiction
1
1
X
Closed
Tautology
n
=
=
Equivalent circuit
Logically equivalent
=
POSETS AND LATTICES
Boolean algebras were derived from the algebra of sets, and there is one important
relation between sets that we have neglected to generalize to boolean algebras,
namely, the inclusion relation. This relation can be deﬁned in terms of the union
operation by
A ⊆B
if and only if
A ∩B = A.
We can deﬁne a corresponding relation ⩽on any boolean algebra (K, ∧, ∨, ′)
using the meet operation:
A ⩽B
if and only if
A ∧B = A.
If the boolean algebra is the algebra of subsets of X, this relation is the usual
inclusion relation.
Proposition 2.16. A ∧B = A if and only if A ∨B = B. Hence either of the
these conditions will deﬁne the relation ⩽.
Proof. If A ∧B = A, then it follows from the absorption law that A ∨B =
(A ∧B) ∨B = B. Similarly, if A ∨B = B, it follows that A ∧B = A.
□
Proposition 2.17. If A, B, and C are elements of a boolean algebra, K, the
following properties of the relation ⩽hold.
(i) A ⩽A.
(reﬂexivity)
(ii) If A ⩽B and B ⩽A, then A = B.
(antisymmetry)
(iii) If A ⩽B and B ⩽C, then A ⩽C.
(transitivity)
Proof
(i) A ∧A = A is an idempotent law.
(ii) If A ∧B = A and B ∧A = B, then A = A ∧B = B ∧A = B.
(iii) If A ∧B = A and B ∧C = B, then A ∧C = (A ∧B) ∧C
= A ∧(B ∧C) = A ∧B = A.
□

24
2
BOOLEAN ALGEBRAS
TABLE 2.9. Partial Order Relation in Various Boolean Algebras
Boolean
Algebra
Algebra of
Subsets
Series-Parallel
Switching
Circuits
Propositional
Logic
Divisors of a
Square-Free
Integer
A ∧B = A
A ∩B = A
A ∧B = A
(A and B) = A
gcd(a, b) = a
A ⩽B
A ⊆B
A ⇒B
A ⇒B
a|b
A is less than
or equal to B
A is a subset
of B
If A is closed,
then B is
closed
A implies B
a divides b
A relation satisfying the three properties in Proposition 2.17 is called a partial
order relation, and a set with a partial order on it is called a partially ordered
set or poset for short. The interpretation of the partial order in various boolean
algebras is given in Table 2.9.
A partial order on a ﬁnite set K can be displayed conveniently in a poset
diagram in which the elements of K are represented by small circles. Lines are
drawn connecting these elements so that there is a path from A to B that is always
directed upward if and only if A ⩽B. Figure 2.15 illustrates the poset diagram
of the boolean algebra of subsets (P({a, b}, )∩, ∪,
−). Figure 2.16 illustrates
the boolean algebra D110 = {1, 2, 5, 11, 10, 22, 55, 110} of positive divisors of
110 = 2 · 5 · 11. The partial order relation is divisibility, so that there is an upward
path from a to b if and only if a divides b.
The following proposition shows that ⩽has properties similar to those of the
inclusion relation in sets.
Proposition 2.18. If A, B, C are elements of a boolean algebra (K, ∧, ∨,
′),
then the following relations hold:
(i) A ∧B ⩽A.
(ii) A ⩽A ∨B.
(iii) A ⩽C and B ⩽C implies that A ∨B ⩽C.
{a, b}
{a}
{b}
f
Figure 2.15.
Poset diagram of P({a, b}).
110
22
55
11
5
1
2
10
Figure 2.16.
Poset diagram of D110.

POSETS AND LATTICES
25
(iv) A ⩽B if and only if A ∧B′ = 0.
(v) 0 ⩽A and A ⩽1 for all A.
Proof
(i) (A ∧B) ∧A = (A ∧A) ∧B = A ∧B so A ∧B ⩽A.
(ii) A ∧(A ∨B) = A, so A ⩽A ∨B.
(iii) (A ∨B) ∧C = (A ∧C) ∨(B ∧C) = A ∨B.
(iv) If A ⩽B, then A ∧B = A and A ∧B′ = A ∧B ∧B′ = A ∧0 = 0. On
the other hand, if A ∧B′ = 0, then A ⩽B because
A = A ∧1 = A ∧(B ∨B′) = (A ∧B) ∨(A ∧B′)
= (A ∧B) ∨0 = A ∧B.
(v) 0 ∧A = 0 and A ∧1 = A.
□
Not all posets are derived from boolean algebras. A boolean algebra is an
extremely special kind of poset. We now determine conditions which ensure that
a poset is indeed a boolean algebra. Given a partial order ⩽on a set K, we have
to ﬁnd two binary operations that correspond to the meet and join.
An element d is said to be the greatest lower bound of the elements a and
b in a partially ordered set if d ⩽a, d ⩽b, and x is another element, for which
x ⩽a, x ⩽b, then x ⩽d. We denote the greatest lower bound of a and b by
a ∧b. Similarly, we can deﬁne the least upper bound and denote it by ∨. It
follows from the antisymmetry of the partial order relation that each pair of
elements a and b can have at most one greatest lower bound and at most one
least upper bound.
A lattice is a partially ordered set in which every two elements have a greatest
lower bound and a least upper bound. Thus Dn is a lattice for every integer n ∈P,
so by the discussion preceding Example 2.15, D18 is a lattice that is not a boolean
algebra (see Figure 2.17).
We can now give an alternative deﬁnition of a boolean algebra in terms of a
lattice: A boolean algebra is a lattice that has universal bounds (that is, elements
0 and 1 such that 0 ⩽a and a ⩽1 for all elements a) and is distributive and
complemented (that is, the distributive laws for ∧and ∨hold, and complements
exist). It can be veriﬁed that this deﬁnition is equivalent to our original one.
In Figure 2.18, the elements c and d have a least upper bound b but no greatest
lower bound.
We note in passing that the discussion preceding Example 2.15 shows that for
each n ∈P, the poset Dn is a lattice in which the distributive laws hold, but it is
not a boolean algebra unless n is square-free. For further reading on lattices in
applied algebra, consult, Davey and Priestley [16] or Lidl and Pilz [10].

26
2
BOOLEAN ALGEBRAS
1
2
6
3
9
18
Figure 2.17.
Lattice that is not a boolean algebra.
c
a
b
d
e
Figure 2.18.
Poset that is not a lattice.
NORMAL FORMS AND SIMPLIFICATION OF CIRCUITS
If we have a complicated switching circuit represented by a boolean expression,
such as
(A ∧(B ∨C′)′) ∨((B ∧C′) ∨A′),
we would like to know if we can build a simpler circuit that would perform the
same function. In other words, we would like to reduce this boolean expression
to a simpler form. In actual practice, it is usually desirable to reduce the circuit to
the one that is cheapest to build, and the form this takes will depend on the state
of the technology at the time; however, for our purposes we take the simplest
form to mean the one with the fewest switches. It is difﬁcult to ﬁnd the simplest
form for circuits with many switches, and there is no one method that will lead to
that form. However, we do have methods for determining whether two boolean
expressions are equivalent. We can reduce the expressions to a certain normal
form, and the expressions will be the same if and only if their normal forms are
the same. We shall look at one such form, called the disjunctive normal form.
In the boolean algebra of subsets of a set, every subset can be expressed as
a union of singleton sets, and this union is unique to within the ordering of the
terms. We shall obtain a corresponding result for arbitrary ﬁnite boolean algebras.
The elements that play the role of singleton sets are called atoms. Here an atom
in a boolean algebra (K, ∧, ∨, ′) is a nonzero element B for which
B ∧Y = B
or
B ∧Y = 0
for each Y ∈K.

NORMAL FORMS AND SIMPLIFICATION OF CIRCUITS
27
Thus B is an atom if Y ⩽B implies that Y = 0 or Y = B. This implies that the
atoms are the elements immediately above the zero element in the poset diagram.
In the case of the algebra of divisors of a square-free integer, the atoms are the
primes, because the deﬁnition of b being prime is that y|b implies that y = 1 or
y = b.
We now give a more precise description of the algebra of switching circuits.
The atoms of the algebra and the disjunctive normal form of an expression will
become clear from this description.
An n-variable switching circuit can be viewed as a black box containing
n independent switches A1, A2, . . . , An, as shown in Figure 2.19, where each
switch can be either on or off. The effect of such a circuit can be tested by trying
all the 2n different combinations of the n switches and observing when the box
allows current to pass. In this way, each circuit deﬁnes a function of n variables
A1, A2, . . . , An:
f : {0, 1}n →{0, 1},
which we call the switching function of the circuit. Two circuits give rise to
the same switching function if and only if they are equivalent.
For example, the circuit in Figure 2.20, corresponding to the expression
(A ∨B′) ∧(C ∨A′), gives rise to the switching function f : {0, 1}3 →{0, 1}
given in Table 2.10.
A1
A2
An
•
•
•
Figure 2.19.
n-Variable switching circuit.
A
C
A′
B′
Figure 2.20.
Circuit (A ∨B′) ∧(C ∨A′).
TABLE 2.10. Switching Function
A
B
C
f = (A ∨B′) ∧(C ∨A′)
0
0
0
1
0
0
1
1
0
1
0
0
0
1
1
0
1
0
0
0
1
0
1
1
1
1
0
0
1
1
1
1

28
2
BOOLEAN ALGEBRAS
Denote the set of all n-variable switching functions from {0, 1}n to {0, 1} by
Fn. Each of the 2n elements in the domain of such a function can be mapped to
either of the two elements in the codomain. Therefore, the number of different
n-variable switching functions, and hence the number of different circuits with
n switches, is 22n.
Let f and g be the switching functions of two circuits of the n-variables
A1, A2, . . . , An. When these circuits are connected in series or in parallel, they
give rise to the switching functions f ∧g or f ∨g, respectively, where
(f ∧g)(A1, . . . , An) = f (A1, . . . , An) ∧g(A1, . . . , An)
and
(f ∨g)(A1, . . . , An) = f (A1, . . . , An) ∨g(A1, . . . , An).
The switching function of the opposite circuit to that deﬁning f is f ′, where
f ′(A1, . . . , An) = (f (A1, . . . , An))′.
Theorem 2.19. The set of n-variable switching functions forms a boolean alge-
bra (Fn, ∧, ∨, ′) that contains 22n elements.
Proof. It can be veriﬁed that (Fn, ∧, ∨,′ ) satisﬁes all the axioms of a boolean
algebra. The zero element is the function whose image is always 0, and the unit
element is the function whose image is always 1.
□
The boolean algebra of switching functions of two variables contains 16 ele-
ments, which are displayed in Table 2.11. For example, f6(A, B) = 0 if A = B,
and 1 if A ̸= B. This function is the exclusive OR function or a modulo 2 adder.
It is also the symmetric difference function, where the symmetric difference of
A and B in a boolean algebra is deﬁned by
AB = (A ∧B′) ∨(A′ ∧B).
The operations NAND and NOR stand for “not and” and “not or,” respectively;
these are discussed further in the section “Transistor Gates.”
As an example of the operations in the boolean algebra F2, we calculate the
meet and join of f10 and f7, and the complement of f10 in Table 2.12. We
see that f10 ∧f7 = f2, f10 ∨f7 = f15 and f ′
10 = f5. These correspond to the
relations B′ ∧(A ∨B) = A ∧B′, B′ ∨(A ∨B) = 1, and (B′)′ = B.
In the boolean algebra Fn, f ⩽g if and only if f ∧g = f , which happens if
g(A1, . . . , An) = 1 whenever f (A1, . . . , An) = 1. The atoms of Fn are therefore
the functions whose image contains precisely one nonzero element. Fn contains
2n atoms, and the expressions that realize these atoms are of the form
Aα1
1 ∧Aα2
2 ∧· · · ∧Aαn
n , where each Aαi
i = Ai or A′
i.

NORMAL FORMS AND SIMPLIFICATION OF CIRCUITS
29
TABLE 2.11. Two-Variable Switching Functions
A
0
0
1
1
Expressions in A and B
B
0
1
0
1
Representing the Function
f0
0
0
0
0
0
f1
0
0
0
1
A ∧B
f2
0
0
1
0
A ∧B′ or A ̸⇒B
f3
0
0
1
1
A
f4
0
1
0
0
A′ ∧B or A ̸⇐B
f5
0
1
0
1
B
f6
0
1
1
0
AB or Exclusive OR(A, B)
f7
0
1
1
1
A ∨B
f8
1
0
0
0
A′ ∧B′ or NOR(A, B)
f9
1
0
0
1
AB′ or A ⇔B
f10
1
0
1
0
B′
f11
1
0
1
1
A ∨B′ or A ⇐B
f12
1
1
0
0
A′
f13
1
1
0
1
A′ ∨B or A ⇒B
f14
1
1
1
0
A′ ∨B′ or NAND(A, B)
f15
1
1
1
1
1
TABLE 2.12. Some Operations in F 2
A
B
f10
f7
f10 ∧f7
f10 ∨f7
f ′
10
0
0
1
0
0
1
0
0
1
0
1
0
1
1
1
0
1
1
1
1
0
1
1
0
1
0
1
1
The 16 elements of F2 are illustrated in Figure 2.21, and the four atoms are
f1, f2, f4, and f8, which are deﬁned in Table 2.11.
To show that every element of a ﬁnite boolean algebra can be written as a
join of atoms, we need three preliminary lemmas.
Lemma 2.20. If A, B1, . . . , Br are atoms in a boolean algebra, then
A ⩽(B1 ∨· · · ∨Br) if and only if A = Bi, for some i with 1 ⩽i ⩽r.
Proof. If A ⩽(B1 ∨· · · ∨Br), then A ∧(B1 ∨· · · ∨Br) = A; thus (A ∧B1) ∨
· · · ∨(A ∧Br) = A. Since each Bi is an atom, A ∧Bi = Bi or 0. Not all the
elements A ∧Bi can be 0, for this would imply that A = 0. Hence there is some i,
with 1 ⩽i ⩽r, for which A ∧Bi = Bi. But A is also an atom, so A = A ∧Bi =
Bi.
The implication the other way is straightforward
□
Lemma 2.21. If Z is a nonzero element of a ﬁnite boolean algebra, there exists
an atom B with B ⩽Z.

30
2
BOOLEAN ALGEBRAS
f15
f11
f13
f7
f3
f1
f5
f9
f6
f2
f0
f4
f10
f14
f12
f8
Figure 2.21.
Poset diagram of the boolean algebra of two-variable switching functions.
Proof. If Z is an atom, take B = Z. If not, then it follows from the deﬁnition
of atoms that there exists a nonzero element Z1, different from Z, with Z1 ⩽Z.
If Z1 is not an atom, we continue in this way to obtain a sequence of distinct
nonzero elements · · · ⩽Z3 ⩽Z2 ⩽Z1 ⩽Z, which, because the algebra is ﬁnite,
must terminate in an atom B.
□
Lemma 2.22. If B1, . . . , Bn are all the atoms of a ﬁnite boolean algebra, then
Y = 0 if and only if Y ∧Bi = 0 for all i such that 1 ⩽i ⩽n.
Proof. Suppose that Y ∧Bi = 0 for each i. If Y is nonzero, it follows from the
previous lemma that there is an atom Bj with Bj ⩽Y. Hence Bj = Y ∧Bj = 0,
which is a contradiction, so Y = 0. The converse implication is trivial.
□
Theorem 2.23. Disjunctive Normal Form. Each element X of a ﬁnite boolean
algebra can be written as a join of atoms
X = Bα ∨Bβ ∨· · · ∨Bω.
Moreover, this expression is unique up to the order of the atoms.
Proof. Let Bα, Bβ, . . . , Bω be all the atoms less than or equal to X in the
partial order. It follows from Proposition 2.18(iii) that the join Y = Bα ∨Bβ ∨
· · · ∨Bω ⩽X.
We will show that X ∧Y ′ = 0, which, by Proposition 2.18(iv), is equivalent
to X ⩽Y. We have
X ∧Y ′ = X ∧B′
α ∧· · · ∧B′
ω.
If B is an atom in the join Y, say B = Bα, it follows that X ∧Y ′ ∧B = 0,
since B′
α ∧Bα = 0. If B is an atom that is not in Y, then X ∧Y ′ ∧B = 0 also,

NORMAL FORMS AND SIMPLIFICATION OF CIRCUITS
31
because X ∧B = 0. Therefore, by Lemma 2.22, X ∧Y ′ = 0, which is equivalent
to X ⩽Y. The antisymmetry of the partial order relation implies that X = Y.
To show uniqueness, suppose that X can be written as the join of two sets
of atoms
X = Bα ∨· · · ∨Bω = Ba ∨· · · ∨Bz.
Now Bα ⩽X; thus, by Lemma 2.20, Bα is equal to one of the atoms on the
right-hand side, Ba, . . . , Bz. Repeating this argument, we see that the two sets of
atoms are the same, except possibly for their order.
□
In the boolean algebra of n-variable switching functions, the atoms are realized
by expressions of the form Aα1
1 ∧Aα2
2 ∧· · · ∧Anαn, where the αi’s are 0 or 1 and
Aiαi = Ai, if αi = 1, whereas Aiαi = A′
i, if αi = 0. The expression Aα1
1 ∧Aα2
2 ∧
· · · ∧Anαn is included in the disjunctive normal form of the function f if and
only if f (α1, α2, . . . , αn) = 1. Hence there is one atom in the disjunctive normal
form for each time the element 1 occurs in the image of the switching function.
Example 2.24. Find the disjunctive normal form for the expression
(B ∨(A ∧C)) ∧((A ∨C) ∧B)′, and check the result by using the axioms to
reduce the expression to that form.
Solution. We see from the values of the switching function in Table 2.13 that
the disjunctive normal form is (A′ ∧B ∧C′) ∨(A ∧B′ ∧C).
From the axioms, we have
(B ∨(A ∧C)) ∧((A ∨C) ∧B)′ = (B ∨(A ∧C)) ∧((A′ ∧C′) ∨B′)
= ((B ∨(A ∧C)) ∧(A′ ∧C′))∨
((B ∨(A ∧C)) ∧B′)
= (B ∧A′ ∧C′) ∨(A ∧C ∧A′ ∧C′)∨
(B ∧B′) ∨(A ∧C ∧B′)
= (A′ ∧B ∧C′) ∨0 ∨0 ∨(A ∧B′ ∧C)
= (A′ ∧B ∧C′) ∨(A ∧B′ ∧C).
□
TABLE 2.13. Switching Function
A
B
C
(B ∨(A ∧C)) ∧((A ∨C) ∧B)′
0
0
0
0
0
0
1
0
0
1
0
1
0
1
1
0
1
0
0
0
1
0
1
1
1
1
0
0
1
1
1
0

32
2
BOOLEAN ALGEBRAS
TABLE 2.14. Switching Function
A
B
(A ∨B) ∧B′
(A ∨B) ∧(A ∧B)′
(A ∧B)′ ∧(A ∧B′)
0
0
0
0
0
0
1
0
1
0
1
0
1
1
1
1
1
0
0
0
Example 2.25. Determine whether any of the three expressions
(A ∨B) ∧B′, (A ∨B) ∧(A ∧B)′, and (A ∧B)′ ∧(A ∧B′) equivalent.
Solution. We see from Table 2.14 that (A ∨B) ∧B′ = (A ∧B)′ ∧(A ∧B′)
and that these are both equal to A ∧B′.
□
The atoms in the boolean algebra F2 are realized by the expressions
A′ ∧B′, A′ ∧B, A ∧B′, and A ∧B. These atoms partition the Venn diagram
in Figure 2.22 into four disjoint regions. The disjunctive normal form for any
boolean expression involving the variables A and B can be calculated by shading
the region of the Venn diagram corresponding to the expression and then taking
the join of the atoms in the shaded region. Figure 2.23 illustrates the eight regions
of the corresponding Venn diagram for three variables.
A
A ∧ B′ A ∧ B A′ ∧ B
A′ ∧ B′
B
Figure 2.22.
Venn diagram for F2.
A ∧ B′ ∧ C′
A′ ∧ B ∧ C′
A ∧ B ∧ C′
A ∧ B ∧ C
A′ ∧ B′ ∧ C
A′ ∧ B′ ∧ C′
A ∧ B′ ∧ C
A′ ∧ B ∧ C
A
B
C
Figure 2.23.
Venn diagram for F3.

NORMAL FORMS AND SIMPLIFICATION OF CIRCUITS
33
By looking at the shaded region of a Venn diagram corresponding to a boolean
expression, it is often possible to see how to simplify the expression. Furthermore,
the disjunctive normal form provides a method of proving hypotheses derived
from these Venn diagrams.
However, Venn diagrams become too complicated and impractical for func-
tions of more than four variables. For other general methods of simplifying
circuits, consult a book on boolean algebras such as Mendelson [21].
Example 2.26. Find the disjunctive normal form and simplify the circuit in
Figure 2.24.
Solution. This circuit is represented by the boolean expression
f = A ∨((B′ ∨C) ∧(A ∨B ∨C)).
The boolean function f : {0, 1}3 →{0, 1} that this expression deﬁnes is given in
Table 2.15. It follows that the disjunctive normal form is
(A′ ∧B′ ∧C) ∨(A′ ∧B ∧C) ∨(A ∧B′ ∧C′) ∨(A ∧B′ ∧C)
∨(A ∧B ∧C′) ∨(A ∧B ∧C),
which is certainly not simpler than the original. However, by looking at the Venn
diagram in Figure 2.25, we see that this expression is equivalent to just A ∨C;
thus a simpler equivalent circuit is given in Figure 2.25.
□
A
A
B
C
B′
C
Figure 2.24.
Series-parallel circuit.
TABLE 2.15. Switching
Function
A
B
C
f
0
0
0
0
0
0
1
1
0
1
0
0
0
1
1
1
1
0
0
1
1
0
1
1
1
1
0
1
1
1
1
1

34
2
BOOLEAN ALGEBRAS
A
A
C
B
C
Figure 2.25.
Venn diagram and simpliﬁed circuit.
In building a computer, one of the most important pieces of equipment needed
is a circuit that will add two numbers in binary form. Consider the problem of
adding the numbers 15 and 5. Their binary forms are 1111 and 101, respectively.
The binary and decimal additions are shown below. In general, if we add the
number . . . a2a1a0 to . . . b2b1b0, we have to carry the digits . . . c2c1 to obtain the
sum . . . s2s1s0.
1111
15
. . . a2a1a0
101
5
. . . b2b1b0
1111
←carry digits →
1
. . . c2c1
10100
20
. . . s2s1s0
binary addition
decimal addition
Let us ﬁrst design a circuit to add a0 and b0 to obtain s0 and the carry digit
c1. This is called a half adder. The digits s0 and c1 are functions of a0 and b0
which are given by Table 2.16. For example, in binary arithmetic, 1 + 1 = 10,
which means that if a0 = 1 and b0 = 1, s0 = 0, and we have to carry c1 = 1.
We see from Table 2.16 that c1 = a0 ∧b0 and s0 = (a′
0 ∧b0) ∨(a0 ∧b′
0).
These circuits are shown in Figure 2.26.
TABLE 2.16. Switching
Functions for the Half
Adder
a0
b0
c1
s0
0
0
0
0
0
1
0
1
1
0
0
1
1
1
1
0
a′0
b0
b0
c1
s0
a0
a0
b′0
Figure 2.26.
Circuits for the half adder.

NORMAL FORMS AND SIMPLIFICATION OF CIRCUITS
35
TABLE 2.17. Switching Functions
for a Full Adder
ai
bi
ci
ci+1
si
0
0
0
0
0
0
0
1
0
1
0
1
0
0
1
0
1
1
1
0
1
0
0
0
1
1
0
1
1
0
1
1
0
1
0
1
1
1
1
1
ai
bi
ci +1 shaded
si  shaded
ci
ai
bi
ci
Figure 2.27.
Venn diagrams for a full adder.
a′i
b′i
c′i
c′i
ci
ci
si
ci +1
ci
ci
ai
ai
bi
b′i
bi
bi
bi
Figure 2.28.
Circuits for a full adder.
A circuit that adds ai, bi, and the carry digit, ci, to obtain si, with ci+1 to
carry, is called a full adder. The functions ci+1 and si are deﬁned by Table 2.17,
and their Venn diagrams are given in Figure 2.27. Notice that si = aibici.
Suitable expressions for a full adder are as follows. The corresponding circuits
are shown in Figure 2.28.

36
2
BOOLEAN ALGEBRAS
si = (a′
i ∧b′
i ∧ci) ∨(a′
i ∧bi ∧c′
i) ∨(ai ∧b′
i ∧c′
i) ∨(ai ∧bi ∧ci)
= (a′
i ∧((b′
i ∧ci) ∨(bi ∧c′
i))) ∨(ai ∧((b′
i ∧c′
i) ∨(bi ∧ci))).
ci+1 = (ai ∧bi) ∨(ai ∧ci) ∨(bi ∧ci)
= (ai ∧(bi ∨ci)) ∨(bi ∧ci).
Using one half adder and (n −1) full adders, we can design a circuit that will
add two numbers that in binary form have n or fewer digits (that is, numbers
less than 2n).
TRANSISTOR GATES
The switches we have been dealing with so far have been simple two-state
devices. Transistor technology, however, allows us to construct basic switches
with multiple inputs. These are called transistor gates. Transistor gates can be
used to implement the logical operations AND, OR, NOT, and modulo 2 addi-
tion (that is, exclusive OR). Gates for the composite operations NOT-AND and
NOT-OR are also easily built from transistors; these are called NAND and NOR
gates, respectively. Figure 2.29 illustrates the symbols and outputs for these gates
when there are two inputs. However, any number of inputs is possible. Note that
the inversion operation is indicated by a small circle.
Transistor gates can be combined in series and in parallel to form more com-
plex circuits. Any circuit with n inputs and one output deﬁnes an n-variable
switching function. The set of all such n-variable functions again forms the
boolean algebra (Fn, ∧, ∨, ′).
It follows from the disjunctive normal form that any boolean function can
be constructed from AND, OR, and NOT gates. What is not so obvious is that
any boolean function can be constructed solely from NOR gates (or solely from
NAND gates). This is of interest because with certain types of transistors, it is
easier to build NOR gates (and NAND gates) than it is to build the basic opera-
tions. Figure 2.30 illustrates how two-input NOR gates can be used to construct
two-input AND and OR gates as well as a NOT gate.
a
b
a
a
a′
b
a
b
AND
OR
NOT
a ∧ b
a ∨ b
a
b
a
b
NAND
NOR
(a ∧ b)′ = a′ ∨ b′ 
(a ∨ b)′ = a′ ∧ b′ 
Exclusive OR(a, b) =
(a ∧ b′) ∨ (a′ ∧ b)
Figure 2.29.
Transistor gates.

TRANSISTOR GATES
37
a
a
b
a′
NOR
NOR
NOR
a
a′
NOR
(a′ ∨ b′)′ = a ∧ b
a′ ∧ b′
a ∨ b
NOR
b
b′
NOR
Figure 2.30.
Basic operations constructed from NOR gates.
NOR
NOR
NOR
NOR
NOR
NOR
NOR
NOR
NOR
ci
si
cj +1
ai
bi
Figure 2.31.
Full adder using NOR gates.
Example 2.27. Verify that the circuit in Figure 2.31 is indeed a full adder.
Solution. We analyze this circuit by breaking it up into component parts as
illustrated in Figure 2.32. Consider the subcircuit consisting of four NOR gates
in Figure 2.32 with inputs a and b and outputs l and m. If u and v are the
intermediate functions as shown in the ﬁgure, then
m = NOR(a, b) = a′ ∧b′
u = NOR(a, a′ ∧b′) = a′ ∧(a′ ∧b′)′ = a′ ∧(a ∨b)
= (a′ ∧a) ∨(a′ ∧b) = 0 ∨(a′ ∧b) = a′ ∧b,
NOR
NOR
TRANSISTOR GATES
NOR
NOR
NOR
a
b
u
ci
ai
li
mi
si
ci +1
bi
v
l
m
Figure 2.32.
Component parts of the full adder.

38
2
BOOLEAN ALGEBRAS
and v = a ∧b′, similarly. Therefore,
l = NOR(u, v) = (a′ ∧b)′ ∧(a ∧b′)′ = (a ∨b′) ∧(a′ ∨b)
= (a ∧a′) ∨(a ∧b) ∨(b′ ∧a′) ∨(b′ ∧b) = 0 ∨(a ∧b) ∨(b′ ∧a′) ∨0
= (a ∧b) ∨(a′ ∧b′) = ab′.
The entire circuit can now be constructed from two of these identical sub-
circuits together with one NOR gate, as shown in Figure 2.32. The switching
functions for the subcircuit and the full adder are calculated in Table 2.18.
We have ci+1 = NOR(mi, NOR(ci, li)), while si = cil′
i. We see from
Table 2.18 that the circuits do perform the addition of ai, bi, and ci correctly.□
TABLE 2.18. Switching Functions for the NOR
Circuit
a
b
l
m
0
0
1
1
0
1
0
0
1
0
0
0
1
1
1
0
ai
bi
ci
li
mi
NOR(ci, li)
ci+1
si
0
0
0
1
1
0
0
0
0
0
1
1
1
0
0
1
0
1
0
0
0
1
0
1
0
1
1
0
0
0
1
0
1
0
0
0
0
1
0
1
1
0
1
0
0
0
1
0
1
1
0
1
0
0
1
0
1
1
1
1
0
0
1
1
Figure 2.33.
Photomicrograph of the IBM POWER4 chip containing 174 million transistors. (Cour-
tesy of the IBM Journal of Research and Development.)

REPRESENTATION THEOREM
39
Instead of using many individual transistors, circuits are now made on a single
semiconductor “chip,” such as the one in Figure 2.33. This chip may contain
millions of gates and several layers of semiconductor. Simpliﬁcation of a circuit
may not mean the reduction of the circuit to the smallest number of gates. It
could mean simpliﬁcation to standard modules, or the reduction of the numbers
of layers in the chip. In the design of high-speed computers, it is important to
reduce the time a circuit will take to perform a given set of operations.
REPRESENTATION THEOREM
A boolean algebra is a generalization of the notion of the algebra of sets. How-
ever, we now show that every ﬁnite boolean algebra is in fact essentially the
same as the algebra of subsets of some ﬁnite set. To be more precise about
what we mean by algebras being essentially the same, we introduce the notion
of morphism and isomorphism of boolean algebras. A morphism between two
boolean algebras is a function between their elements that preserves the two
binary operations and the unary operation.
More precisely, if (K, ∧, ∨,
′) and (L, ∩, ∪,
−) are two boolean algebras,
the function f : K →L is called a boolean algebra morphism if the following
conditions hold for all A, B ∈K:
(i) f (A ∧B) = f (A) ∩f (B).
(ii) f (A ∨B) = f (A) ∪f (B).
(iii) f (A′) = f (A).
A boolean algebra isomorphism is a bijective boolean algebra morphism.
Isomorphic boolean algebras have identical properties. For example, their poset
diagrams are the same, except for the labeling of the elements. Furthermore, the
atoms of one algebra must correspond to the atoms in the isomorphic algebra.
If we wish to ﬁnd an isomorphism between any boolean algebra, K, and an
algebra of sets, the atoms of K must correspond to the singleton elements of the
algebra of sets. This suggests that we try to deﬁne an isomorphism from K to
the algebra P(A) of subsets of the set A of atoms of K. The following theorem
shows that if K is ﬁnite, we can set up such an isomorphism.
Theorem 2.28. Representation Theorem for Finite Boolean Algebras. Let A
be the set of atoms of the ﬁnite boolean algebra (K, ∧, ∨,
′). Then there is a
boolean algebra isomorphism between (K, ∧, ∨,
′) and the algebra of subsets
(P(A), ∩, ∪, −).
Proof. We already have a natural correspondence between the atoms of K
and the atoms of P(A). We use the disjunctive normal form (Theorem 2.23) to
extend this correspondence to all the elements of K.

40
2
BOOLEAN ALGEBRAS
By the disjunctive normal form, any element of K can be written as a join of
atoms of K, say Bα ∨· · · ∨Bω. Deﬁne the function f : K →P(A) by
f (Bα ∨· · · ∨Bω) = {Bα} ∪· · · ∪{Bω}.
The uniqueness of the normal form implies that each element of K has a unique
image in P(A) and that f is a bijection.
We still have to show that f is a morphism of boolean algebras. If X and
Y are two elements of K, the atoms in the normal forms of X ∨Y and X ∧Y
are, respectively, the atoms in the forms of X or Y and the atoms common to
the forms of X and Y. Therefore, f (X ∨Y) = f (X) ∪f (Y), and f (X ∧Y) =
f (X) ∩f (Y). An atom B is in the normal form for X′ if and only if B ⩽X′,
which, by Proposition 2.18(iv), happens if and only if B ∧X = 0. Therefore, the
atoms in X′ are all the atoms that are not in X and f (X′) = f (X). This proves
that f is a boolean algebra isomorphism.
□
COROLLARY 2.29. If (K, ∧, ∨, ′) is a ﬁnite boolean algebra, then K has 2n
elements, where n is the number of atoms in K.
Proof. This follows from Theorem 2.5.
□
Consider the representation theorem (Theorem 2.28) applied to the boolean
algebra D110, which consists of the divisors of 110. The atoms of this algebra
are the prime divisors 2, 5, and 11. Theorem 2.28 deﬁnes a boolean algebra
isomorphism to the algebra of subsets of {2, 5, 11}. This isomorphism, f , maps
a number onto the subset consisting of its prime divisors; for example, f (11) =
{11} and f (10) = {2, 5}.
Example 2.30. Do the divisors of 12 form a boolean algebra under gcd and lcm?
Solution. The set of divisors of 12 is {1, 2, 3, 4, 6, 12}. Since the number of
elements is not a power of 2, it cannot form a boolean algebra.
□
Example 2.31. Do the divisors of 24 form a boolean algebra under gcd and lcm?
Solution. There are 8 = 23 divisors of 24, namely 1, 2, 3, 4, 6, 8, 12, and
24. However, the poset diagram in Figure 2.34 shows that 2 and 3 are the only
atoms. Hence, by Corollary 2.29, it cannot be a boolean algebra because it does
not have 22 = 4 elements.
□
An inﬁnite boolean algebra is not necessarily isomorphic to the algebra of all
subsets of a set, but is isomorphic to the algebra of some subsets of a set. This
result is known as Stone’s representation theorem, and a proof can be found in
Mendelson [21, Sec. 5.7].

EXERCISES
41
1
3
6
2
4
12
24
8
Figure 2.34.
Poset diagram of the divisors of 24.
EXERCISES
If A, B, and C are subsets of a set X, under what conditions do the equalities in
Exercises 2.1 to 2.6 hold?
2.1. A ∪B = AB(A ∩B).
2.2. A ∩(B ∪C) = (A ∩B) ∪C.
2.3. A −(B ∪C) = (A −B) ∪(A −C).
2.4. A(B ∩C) = (AB) ∩(AC).
2.5. A(B ∪C) = (AB) ∪(AC).
2.6. A ∪(B ∩A) = A.
2.7. Prove the remaining parts of Proposition 2.1.
2.8. Prove the remaining parts of Proposition 2.3.
2.9. Prove Theorem 2.5 by induction on n. That is, if X is a ﬁnite set with n
elements, prove that P(X) contains 2n elements.
2.10. Prove or give a counterexample to the following statements.
(a) P(X) ∩P(Y) = P(X ∩Y).
(b) P(X) ∪P(Y) = P(X ∪Y).
2.11. (Cantor’s theorem) Prove that there is no surjective (onto) function from
X to P(X) for any ﬁnite or inﬁnite set X. This shows that P(X) always
contains more elements than X.
[Hint: If f : X →P(X), consider {x ∈X|x /∈f (x)}.]
2.12. Write down the table for (P(X), −), under the difference operation, when
X = {a, b}.
2.13. If A, B, C, and D are ﬁnite sets, ﬁnd an expression for |A ∪B ∪C ∪D|
in terms of the number of elements in their intersections.
2.14. Of the Chelsea pensioners who returned from a war, at least 70% had lost
an eye, 75% an ear, 80% an arm, and 85% a leg. What percentage, at least
must have lost all four? (From Lewis Carroll, A Tangled Tale.)
2.15. One hundred students were questioned about their study habits. Seventy
said they sometimes studied during the day, 55 said they sometimes studied
during the night, and 45 said they sometimes studied during the weekend.

42
2
BOOLEAN ALGEBRAS
Also, 36 studied during the day and night, 24 during the day and at week-
ends, 17 during the night and at weekends, and 3 during the day, night,
and weekends. How many did not study at all?
2.16. Prove that the associative laws in deﬁned in the section “Boolean Algebras”
follow from the other axioms of a boolean algebra.
2.17. If the zero element is the same as the unit element in a boolean algebra,
prove that the algebra has only one element. Is this algebra isomorphic to
the algebra of subsets of some set?
2.18. Draw the poset diagram for F1, the boolean algebra of switching functions
of one variable.
If A, B, and C are elements of a boolean algebra (K, ∧, ∨, ′) and ⩽is the related
partial order, prove the assertions in Exercises 2.19 to 2.24 from the axioms and
Propositions 2.13, 2.17, and 2.18.
2.19. 0′ = 1.
2.20. A ∧(A′ ∨B) = A ∧B.
2.21. (A ∧B) ∨(B ∧C) ∨(C ∧A) = (A ∨B) ∧(B ∨C) ∧(C ∨A).
2.22. A ⩽B ∧C implies that A ⩽B and A ⩽C.
2.23. (A ∧B′) ∨C = (B ∧C) ∨(B′ ∧(A ∨C)).
2.24. A ⩽B if and only if B′ ⩽A′.
2.25. Write down the truth tables for the following propositions. Which of these
propositions are equivalent?
(a) A ⇒B.
(b) B′ ⇒A′.
(c) (A ∧B) ⇔B.
(d) (A ∨B) ⇔A.
2.26. Is the proposition [(A′ ⇒B) ∧(B ⇔C)′] equivalent to [B ∨(A ∧C)] or
[(C ⇒B) ∨(B ⇒(A ∧C))]?
2.27. Which of the following are tautologies, and which are contradictions?
(a) (A ∧B) ⇔(A ⇒B′).
(b) A ⇒(B ⇒A).
(c) (A ∧B′) ⇔(A ⇒B)′.
(d) (A ⇒B) ⇒
((B ⇒C) ⇒(A ⇒C)).
2.28. Harry broke the window if and only if he ran away and John was lying.
John said that either Harry broke the window or Harry did not run away.
If Harry ran away, then he did not break the window. What conclusions
can you come to?
Draw circuits to realize the expressions in Exercises 2.29 and 2.30.
2.29. (A ∧(B ∨C′ ∨D)) ∨B′.
2.30. (A ∧B′ ∧C′) ∨(A′ ∧B ∧C) ∨(A ∧B ∧C).
2.31. Simplify the following expression and then draw a circuit for it.
((A ∧B) ∨C′) ∧(B′ ∨(C ∧A′)) ∨(A′ ∧B′ ∧C′).

EXERCISES
43
Give a boolean expression for each of the circuits in Exercises 2.32 to 2.36, ﬁnd
their disjunctive normal forms, and then try to simplify the circuits.
2.32.
A
A
A′
2.33.
A
B
A′
2.34.
A
B
C
A′
B′
C
A′
C′
2.35.
A
B
B
A′
B′
A′
2.36.
A
C
B′
B
B
C
A′
A′
C′
2.37. By looking at all the possible paths through the bridge circuit in Figure 2.35,
show that it corresponds to the boolean expression
(A ∧D) ∨(B ∧E) ∨(A ∧C ∧E) ∨(B ∧C ∧D).
A
B
C
D
E
Figure 2.35
2.38. Find a series-parallel circuit that is equivalent to the bridge circuit in
Figure 2.36 and simplify your circuit.
A
B
C
C
B
A′
Figure 2.36
2.39. A hall light is controlled by two switches, one upstairs and one downstairs.
Design a circuit so that the light can be switched on or off from the upstairs
or the downstairs.

44
2
BOOLEAN ALGEBRAS
2.40. A large room has three separate entrances, and there is a light switch by
each entrance. Design a circuit that will allow the lights to be turned on
or off by throwing any one switch.
2.41. A voting machine for three people contains three YES–NO switches and
allows current to pass if and only if there is a majority of YES votes.
Design and simplify such a machine.
2.42. Design and simplify a voting machine for ﬁve people.
2.43. Design a circuit for a light that is controlled by two independent switches
A and B and a master switch C. C must always be able to turn the light
on. When C is off, the light should be able to be turned on and off using
A or B.
2.44. A committee consists of a chairman A, and three other members, B, C,
and D. If B, C, and D, are not unanimous in their voting, the chairman
decides the vote. Design a voting machine for this committee and simplify
it as much as possible.
2.45. Verify that the Venn diagram in Figure 2.37 illustrates the 16 atoms for a
boolean expression in four variables. Then use the diagram to simplify the
circuit in Figure 2.37.
A
B
C
D
A
C
B′
A′
A
D
B
A′
Figure 2.37
2.46. Design four series-parallel circuits to multiply two numbers in binary form
that have at most two digits each.
2.47. Design a circuit that will turn an orange light on if exactly one of the four
switches A, B, C, and D is on and a green light when all four are on.
2.48. Five switches are set to correspond to a number in binary form that has at
most ﬁve digits. Design and simplify a circuit that will switch a light on
if and only if the binary number is a perfect square.
2.49. In Chapter 11 we construct a ﬁnite ﬁeld F = {0, 1, α, β} whose multipli-
cation table is given in Table 2.19. Writing 00 for 0, 01 for 1, 10 for α,
and 11 for β, design and simplify circuits to perform this multiplication.
2.50. A swimming pool has four relay switches that open when the water tem-
perature is above the maximum allowable, when the water temperature is
below the minimum, when the water level is too high, and when the level

EXERCISES
45
TABLE 2.19. Multiplication in
a Four-Element Field
·
0
1
α
β
0
0
0
0
0
1
0
1
α
β
α
0
α
β
1
β
0
β
1
α
is too low. These relays are used to control the valves that add cold water,
that let water out, and that heat the water in the pool. Design and simplify
a circuit that will perform the following tasks. If the temperature is correct
but the level too high, it is to let water out. If the temperature is correct but
the level too low, it is to let in cold water and heat the water. If the pool is
too warm, add cold water and, if the level is also too high, let water out at
the same time. If the pool is too cold but the level correct, heat the water;
if the level is too low, heat the water and add cold water, and, if the level
is too high, just let out the water.
2.51. In a dual fashion to the disjunctive normal form, every boolean expression
in n-variables can be written in its conjunctive normal form. What are the
conjunctive normal forms for AB and A ∧B′?
Draw poset diagrams for the sets given in Exercises 2.52 to 2.57 with divisi-
bility as the partial order and determine whether the systems are lattices or
boolean algebras.
2.52. {1, 2, 3, 4, 5, 6}.
2.53. {2, 4, 6, 12}.
2.54. D54
2.55. {1, 2, 4, 8}.
2.56. D42
2.57. {1, 2, 3, 5, 6, 10, 30, 60}.
2.58. Prove that (Dn, |) is a poset for each n ⩾2, and prove the distributive laws.
2.59. Let n = p1p2 · · · pr, where the pi are distinct primes. Describe the atoms
in the boolean algebra (Dn, gcd, lcm, ′).
2.60. Prove that an element A ̸= 0 in a boolean algebra is an atom if and only
if for each B in the algebra, either A ⩽B or A ⩽B′.
2.61. Suppose that A and B are elements of a boolean algebra. If an element X
in the algebra exists such that A ∧X = B ∧X and A ∨X = B ∨X, show
that A = B. [Hint: A = A ∧(A ∨X).]
2.62. Let K = {x ∈R|0 ⩽x ⩽1} and let x ∧y and x ∨y be the smaller and
larger of x and y, respectively. Show that it is not possible to deﬁne a
complement ′ on K so that (K, ∧, ∨,
′) is a boolean algebra. However,
if we deﬁne x′ = 1 −x, which of the properties deﬁned in the section
“Boolean Algebras” and in Proposition 2.13 remain true? This is the kind
of algebraic model that would be required to deal with transistor switching
gates under transient conditions. The voltage or current varies continuously
between the levels 0 and 1, while an AND gate performs the operation
x ∧y, an OR gate performs x ∨y, and a NOT gate produces x′.

46
2
BOOLEAN ALGEBRAS
2.63. If f is a boolean algebra morphism from K to L, prove that f (0K) = 0L
and f (1K) = 1L, where 0K, 0L, 1K, and 1L are the respective zero and
unit elements.
2.64. Write down the tables for the NOR and NAND operations on the set
P({a, b, c}).
2.65. Can every switching circuit be built out of AND and NOT gates?
2.66. (a) Design a half adder using ﬁve NOR gates.
(b) Design a half adder using ﬁve NAND gates.
2.67. Analyze the effect of the circuit in Figure 2.38.
NAND
NOR
a
b
c
d
Figure 2.38
2.68. Design a NOR circuit that will produce a parity check symbol for four
binary input digits; that is, the circuit must produce a 0 if the inputs contain
an even number of 1’s, and it must produce 1 otherwise.
2.69. One of the basic types of components of a digital computer is a ﬂip-ﬂop.
This is a device that can be in one of two states (corresponding to outputs
0 and 1) and it will remain in a particular state Q until an input changes
the state to the next state Q∗. One important use of a ﬂip-ﬂop is to store
a binary digit. An RS ﬂip-ﬂop is a circuit with two inputs, R and S, and
one output, Q, corresponding to the state of the ﬂip-ﬂop. An input R = 1
resets the next state Q∗to 0, and an input S = 1 sets the next state to 1. If
both R and S are 0, the next state is the same as the previous state Q. It
is assumed that R and S cannot both be 1 simultaneously. Verify that the
NOR circuit in Figure 2.39 is indeed an RS ﬂip-ﬂop. To eliminate spurious
effects due to the time it takes a transistor to operate, this circuit should
be controlled by a “clock.” The output Q should be read only when the
clock “ticks,” whereas the inputs are free to change between ticks.
NOR
NOR
S
R
Q
Figure 2.39.
RS ﬂip-ﬂop.
2.70. A JK ﬂip-ﬂop is similar to an RS ﬂip-ﬂop except that both inputs are
allowed to be 1 simultaneously, and in this case the state Q changes to its
opposite state. Design a JK ﬂip-ﬂop using NOR and NAND gates.

3
GROUPS
Symmetries and permutations in nature and in mathematics can be described
conveniently by an algebraic object called a group. In Chapter 5, we use group
theory to determine all the symmetries that can occur in two- or three-dimensional
space. This can be used, for example, to classify all the forms that chemical
crystals can take. If we have a large class of objects, some of which are equiva-
lent under permutations or symmetries, we show, in Chapter 6, how groups can
be used to count the nonequivalent objects. For example, we count the num-
ber of different switching functions of n variables if we allow permutations of
the inputs.
Historically, the basic ideas of group theory arose with the investigation of
permutations of ﬁnite sets in the theory of equations. One of the aims of mathe-
maticians at the beginning of the nineteenth century was to ﬁnd methods for
solving polynomial equations of degree 5 and higher. Algorithms, involving
the elementary arithmetical operations and the extraction of roots, were already
known for solving all polynomial equations of degree less than 5; the formulas
for solving quadratic equations had been known since Babylonian times, and
cubic and quartic equations had been solved by various Italian mathematicians
in the sixteenth century. However, in 1829, using the rudiments of group theory,
the Norwegian Niels Abel (1802–1829) showed that some equations of the ﬁfth
degree could not be solved by any such algorithm. Just before he was mortally
wounded in a duel, at the age of 20, the brilliant mathematician ´Evariste Galois
(1811–1832) developed an entire theory that connected the solvability of an
equation with the permutation group of its roots. This theory, now called Galois
theory, is beyond the scope of this book, but interested students should look at
Stewart [35] after reading Chapter 11.
It was not until the 1880s that the abstract deﬁnition of a group that we
use today began to emerge. However, Cayley’s theorem, proved at the end of
this chapter, shows that every abstract group can be considered as a group of
permutations. It was soon discovered that this concept of a group was so universal
that it cropped up in many different branches of mathematics and science.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
47

48
3
GROUPS
GROUPS AND SYMMETRIES
A group (G, ·) is a set G together with a binary operation · satisfying the
following axioms.
(i) G is closed under the operation ·; that is, a · b ∈G for all a, b ∈G.
(ii) The operation · is associative; that is, (a · b) · c = a · (b · c) for all
a, b, c ∈G.
(iii) There is an identity element e ∈G such that e · a = a · e = a for all
a ∈G.
(iv) Each element a ∈G has an inverse element a−1 ∈G such that a−1 · a =
a · a−1 = e.
The closure axiom is already implied by the deﬁnition of a binary operation;
however, it is included because it is often overlooked otherwise.
If the operation is commutative, that is, if a · b = b · a for all a, b ∈G,
the group is called commutative or abelian, in honor of the mathematician
Niels Abel.
Let G be the set of complex numbers {1, −1, i, −i} and let · be the standard
multiplication of complex numbers. Then (G, ·) is an abelian group. The product
of any two of these elements is an element of G; thus G is closed under the
operation. Multiplication is associative and commutative in G because multipli-
cation of complex numbers is always associative and commutative. The identity
element is 1, and the inverse of each element a is the element 1/a. Hence
1−1 = 1, (−1)−1 = −1, i−1 = −i, and (−i)−1 = i. The multiplication of any two
elements of G can be represented by Table 3.1.
The set of all rational numbers, Q, forms an abelian group (Q, +) under addi-
tion. The identity is 0, and the inverse of each element is its negative. Similarly,
(Z, +), (R, +), and (C, +) are all abelian groups under addition.
If Q∗, R∗, and C∗denote the set of nonzero rational, real, and complex
numbers, respectively, (Q∗, ·), (R∗, ·), and (C∗, ·) are all abelian groups under
multiplication.
For any set X, (P(X), ) is an abelian group. The group axioms follow
from Proposition 2.3; the empty set, Ø, is the identity, and each element is its
own inverse.
TABLE 3.1. Group
{1, −1, i, −i}
·
1
−1
i
−i
1
1
−1
i
−i
−1
−1
1
−i
i
i
i
−i
−1
1
−i
−i
i
1
−1

GROUPS AND SYMMETRIES
49
Every group must have at least one element, namely, its identity, e. A group
with only this one element is called trivial. A trivial group takes the form ({e}, ·),
where e · e = e.
Many important groups consist of functions. Given functions f : X →Y and
g: Y →Z, their composite g Ž f : X →Z is deﬁned by
(g Ž f )(x) = g(f (x))
for all x ∈X.
Composition is associative; that is, if h: Z →W, then h Ž (g Ž f ) = (h Ž g) Ž f .
Indeed,
(h Ž (g Ž f ))(x) = h(g(f (x))) = ((h Ž g) Ž f )(x)
for all x ∈X, as is readily veriﬁed. In particular, if X is a set, then
Ž is an
associative binary operation on the set of all functions f : X →X. Moreover,
this operation has an identity: The identity function 1X: X →X is deﬁned by
1X(x) = x
for all x ∈X.
Then 1X Ž f = f = f Ž 1X for all f : X →X. Hence, we say that a function
f ′: X →X is an inverse of f : X →X if
f ′ Ž f = 1X
and
f Ž f ′ = 1X;
equivalently if f ′(f (x)) = x and f (f ′(x)) = x for all x ∈X. This inverse is
unique when it exists. For if f ′′ is another inverse of f , then
f ′ = f ′ Ž 1X = f ′ Ž (f Ž f ′′) = (f ′ Ž f ) Ž f ′′ = 1X Ž f ′′ = f ′′.
When it exists (see Theorem 3.3) the inverse of f is denoted f −1.
Example 3.1. A translation of the plane R2 in the direction of the vector (a, b)
is a function f : R2 →R2 deﬁned by f (x, y) = (x + a, y + b). The composition
of this translation with a translation g in the direction of (c, d) is the function
f Ž g: R2 →R2, where
f Ž g(x, y) = f (g(x, y)) = f (x + c, y + d) = (x + c + a, y + d + b).
This is a translation in the direction of (c + a, d + b). It can easily be veriﬁed
that the set of all translations in R2 forms an abelian group, (T(2), Ž ), under
composition. The identity is the identity transformation 1R2: R2 →R2, and the
inverse of the translation in the direction (a, b) is the translation in the opposite
direction (−a, −b).
A function f : X →Y is called injective or one-to-one if f (x1) = f (x2)
implies that x1 = x2. In other words, an injective function never takes two dif-
ferent points to the same point. The function f : X →Y is called surjective or

50
3
GROUPS
onto if for any y ∈Y, there exists x ∈X with y = f (x), that is, if the image
f (X) is the whole set Y. A bijective function or one-to-one correspondence is
a function that is both injective and surjective. A permutation or symmetry of
a set X is a bijection from X to itself.
Lemma 3.2. If f : X →Y and g: Y →Z are two functions, then:
(i) If f and g are injective, g Ž f is injective.
(ii) If f and g are surjective, g Ž f is surjective.
(iii) If f and g are bijective, g Ž f is bijective.
Proof. (i) Suppose that (g Ž f )(x1) = (g Ž f )(x2). Then g(f (x1)) = g(f (x2))
so, since g is injective, f (x1) = f (x2). Since f is also injective, x1 = x2, proving
that g Ž f is injective.
(ii) Let z ∈Z. Since g is surjective, there exists y ∈Y with g(y) = z, and
since f is also surjective, there exists x ∈X with f (x) = y. Hence (g Ž f )(x) =
g(f (x)) = g(y) = z, so g Ž f is surjective.
(iii) This follows from parts (i) and (ii).
□
The following theorem gives a necessary and sufﬁcient condition for a function
to have an inverse.
Theorem 3.3. Inversion Theorem. The function f : X →Y has an inverse if
and only if f is bijective.
Proof. Suppose that h: Y →X is an inverse of f . The function f is injective
because if f (x1) = f (x2), it follows that (h Ž f )(x1) = (h Ž f )(x2), and so x1 =
x2. The function f is surjective because if y is any element of Y and x = h(y),
it follows that f (x) = f (h(y)) = y. Therefore, f is bijective.
Conversely, suppose that f is bijective. We deﬁne the function h: Y →X as
follows. For any y ∈Y, there exists x ∈X with y = f (x). Since f is injective,
there is only one such element x. Deﬁne h(y) = x. This function h is an inverse
to f because f (h(y)) = f (x) = y, and h(f (x)) = h(y) = x.
□
Theorem 3.4. If S(X) is the set of bijections from any set X to itself, then
(S(X), Ž ) is a group under composition. This group is called the symmetric
group or permutation group of X.
Proof. It follows from Lemma 3.1 that the composition of two bijections is a
bijection; thus S(X) is closed under composition. The composition of functions is
always associative, and the identity of S(X) is the identity function 1X: X →X.
The inversion theorem (Theorem 3.3) proves that any bijective function f ∈
S(X) has an inverse f −1 ∈S(X). Therefore, (S(X), Ž ) satisﬁes all the axioms
for a group.
□

GROUPS AND SYMMETRIES
51
TABLE 3.2. Symmetry
Group of {a, b}
Ž
1X
f
1X
1X
f
f
f
1X
For example, if X = {a, b} is a two-element set, the only bijections from X to
itselfaretheidentity1X andthesymmetryf : X →X,deﬁnedbyf (a) = b, f (b) =
a, that interchanges the two elements. The use of the term symmetry to describe the
bijection f agrees with one of our everyday uses of the word. In the phrase “the
boolean expression (a ∧b) ∨(a′ ∧b′) is symmetrical in a and b” we mean that the
expression is unchanged when we interchange a and b. The symmetric group of
X, S(X) = {1X, f } and its group table is given in Table 3.2. The composition f Ž f
interchanges the two elements a and b twice; thus it is the identity.
Since the composition of functions is not generally commutative, S(X) is not
usually an abelian group. Consider the elements f and g in the permutation
group of {1, 2, 3}, where f (1) = 2, f (2) = 3, f (3) = 1 and g(1) = 1, g(2) =
3, g(3) = 2. Then f Ž g(1) = 2, f Ž g(2) = 1, f Ž g(3) = 3, while g Ž f (1) = 3,
g Ž f (2) = 2, g Ž f (3) = 1; hence f Ž g ̸= g Ž f , and S({1, 2, 3}) is not abelian.
A nonsingular linear transformation of the plane is a bijective function of the
form f : R2 →R2, where f (x, y) = (a11x + a12y, a21x + a22y) with the deter-
minant a11a22 −a12a21 ̸= 0. It can be veriﬁed that the composition of two such
linear transformations is again of the same type. The set of all nonsingular linear
transformations, L, forms a non-abelian group (L, Ž ).
Besides talking about the symmetries of a distinct set of elements, we often
refer, in everyday language, to a geometric object or ﬁgure as being symmetrical.
We now make this notion more mathematically precise.
If F is a ﬁgure in the plane or in space, a symmetry of the ﬁgure F or
isometry of F is a bijection f : F →F which preserves distances; that is, for
all points p, q ∈F, the distance from f (p) to f (q) must be the same as the
distance from p to q.
One can visualize this operation by imagining F to be a solid object that
can be picked up and turned in some manner so that it assumes a conﬁguration
identical to the one it had in its original position. For example, the design on
the left of Figure 3.1 has two symmetries: the identity and a half turn about a
vertical axis, called an axis of symmetry. The design in the center of Figure 3.1
Figure 3.1.
Symmetrical designs.

52
3
GROUPS
has three symmetries: the identity and rotations of one-third and two-thirds of a
revolution about its center.
However, both the one-third rotation and interchanging two vertices are sym-
metries of the equilateral triangle on the right in Figure 3.1, but there is a subtle
difference: The rotation can be performed as a physical motion within the plane
of the triangle (and so is called a proper symmetry or a proper rotation),
while the reﬂection can only be accomplished as a physical motion by moving
the triangle outside its plane (an improper symmetry or an improper rotation).
The set of all symmetries of a geometric ﬁgure forms a group under compo-
sition because the composition and inverse of two distance-preserving functions
is distance preserving.
Example 3.5. Write down the table for the group of symmetries of a rectangle
with unequal sides.
Solution. Label the corners of the rectangle 1, 2, 3, and 4 as in Figure 3.2.
Any symmetry of the rectangle will send corner points to corner points and so
will permute the corners among themselves. Denote the (improper) symmetry
obtained by reﬂecting the rectangle in the horizontal axis through the center,
by a; then a(1) = 4, a(2) = 3, a(3) = 2, and a(4) = 1. This symmetry can also
be considered as a rotation of the rectangle through half a revolution about this
horizontal axis. There is a similar symmetry, b, about the vertical axis through
the center. A third (proper) symmetry, c, is obtained by rotating the rectangle in
its plane through half a revolution about its center. Finally, the identity map, e,
is a symmetry. These are the only symmetries because it can be veriﬁed that any
other bijection between the corners will not preserve distances.
The group of symmetries of the rectangle is ({e, a, b, c}, Ž ), and its table, as
shown in Table 3.3, can be calculated as follows. The symmetries a, b, and c
are all half turns, so a Ž a, b Ž b, and c Ž c are full turns and are therefore equal to
the identity. The function a Ž b acts on the corner points by a Ž b(1) = a(b(1)) =
a(2) = 3, a Ž b(2) = 4, a Ž b(3) = 1, and a Ž b(4) = 2. Therefore, a Ž b = c. The
other products can be calculated similarly.
□
This group of symmetries of a rectangle is sometimes called the Klein 4-
group, after the German geometer Felix Klein (1849–1925).
We have seen that the group operation can be denoted by various symbols, the
most common being multiplication, composition, and addition. It is conventional
b
a
c
1
2
4
3
Figure 3.2.
Symmetries of a rectangle.

GROUPS AND SYMMETRIES
53
TABLE 3.3. Symmetry Group of
a Rectangle
Ž
e
a
b
c
e
e
a
b
c
a
a
e
c
b
b
b
c
e
a
c
c
b
a
e
to use addition only for abelian groups. Furthermore, the identity under addition
is usually denoted by 0 and the inverse of a by −a. Hence expressions of the
form a · b−1 and an = a · · · a, in multiplicative notation, would be written as
a −b and na = a + · · · + a, respectively, in additive notation.
In propositions and theorems concerning groups in general, it is conventional
to use multiplicative notation and also to omit the dot in writing a product;
therefore, a · b is just written as ab.
Whenever the operation in a group is clearly understood, we denote the group
just by its underlying set. Therefore, the groups (Z, +), (Q, +), (R, +), and
(C, +) are usually denoted just by Z, Q, R, and C, respectively. This should
cause no confusion because Z, Q, R, and C are not groups under multiplication
(since the element 0 has no multiplicative inverse). The symmetric group of X is
denoted just by S(X), the operation of composition being understood. Moreover,
if we refer to a group G without explicitly deﬁning the group or the operation,
it can be assumed that the operation in G is multiplication.
We now prove two propositions that will enable us to manipulate the elements
of a group more easily. Recall from Proposition 2.10 that the identity of any
binary operation is unique. We ﬁrst show that the inverse of any element of a
group is unique.
Proposition 3.6. Let ⋆be an associative binary operation on a set S that has
identity e. Then, if an element a has an inverse, this inverse is unique.
Proof. Suppose that b and c are both inverses of a; thus a ⋆b = b ⋆a = e,
and a ⋆c = c ⋆a = e. Now, since e is the identity and ⋆is associative,
b = b ⋆e = b ⋆(a ⋆c) = (b ⋆a) ⋆c = e ⋆c = c.
Hence the inverse of a is unique.
□
Note that if ab = e in a group G with identity e, then a−1 = b and b−1 = a.
Indeed, b has an inverse b−1 in G, so b−1 = eb−1 = (ab)b−1 = ae = a. Simi-
larly, a−1 = b.
Proposition 3.7. If a, b, and c are elements of a group G, then
(i) (a−1)−1 = a.
(ii) (ab)−1 = b−1a−1.
(iii) ab = ac or ba = ca implies that b = c.
(cancellation law)

54
3
GROUPS
Proof. (i) The inverse of a−1 is an element b such that a−1b = ba−1 = e. But
a is such an element, and by Proposition 3.6 we know that the inverse is unique.
Hence (a−1)−1 = a.
(ii) Using associativity, we have
(ab)(b−1a−1) = a((bb−1)a−1) = a(ea−1) = aa−1 = e.
Hence b−1a−1 is the unique inverse of ab.
(iii) Suppose that ab = ac. Then a−1(ab) = a−1(ac), so (a−1a)b = (a−1a)c.
That is, eb = ec and b = c. Similarly, ba = ca implies that b = c.
□
Notice in part (ii) that the order of multiplication is reversed. This should
be familiar from the particular case of the group of invertible n × n matrices
under multiplication. A more everyday example is the operation of putting on
socks and shoes. To reverse the procedure, the shoes are taken off ﬁrst and then
the socks.
If a is an element in any group G, we write a1 = a, a2 = aa, a3 = aaa, and
so on, just as for numbers. Hence, if k ⩾1, we deﬁne ak to be the product of a
with itself k times. Similarly, we deﬁne a−k = (a−1)k for k ⩾1. Finally, we set
a0 to be the identity, again as for numbers. Thus we have deﬁned ak for every
k ∈Z, and it is a routine veriﬁcation that the laws of exponents hold:
akam = ak+m,
(ak)m = ak+m
for all a ∈G and all k, m ∈Z.
Moreover:
If ab = ba in G, then (ab)k = akbk
for all k ∈Z.
However, this need not hold if ab ̸= ba: For example, if a =

0
−1
1
0

and
b =

1
−1
1
0

in the group of invertible 2 × 2 matrices, then (ab)2 ̸= a2b2.
SUBGROUPS
It often happens that some subset of a group will also form a group under the
same operation. Such a group is called a subgroup. For example, (R, +) is a
subgroup of (C, +), and the group of translations of R2 in Example 3.1 is a
subgroup of the group of all isometries of R2.
If (G, ·) is a group and H is a nonempty subset of G, then (H, ·) is called a
subgroup of (G, ·) if the following conditions hold:
(i) a · b ∈H for all a, b ∈H.
(closure)
(ii) a−1 ∈H for all a ∈H.
(existence of inverses)

SUBGROUPS
55
Proposition 3.8. If H is a subgroup of (G, ·), then (H, ·) is also a group.
Proof. If H is a subgroup of (G, ·), we show that (H, ·) satisﬁes all the group
axioms. The deﬁnition above implies that H is closed under the operation; that is,
· is a binary operation on H. If a, b, c ∈H, then (a · b) · c = a · (b · c) in (G, ·)
and hence also in (H, ·). Since H is nonempty, it contains at least one element,
say h. Now h−1 ∈H and h · h−1, which is the identity, is in H. The deﬁnition
of subgroup implies that (H, ·) contains inverses. Therefore, (H, ·) satisﬁes all
the axioms of a group.
□
Conditions (i) and (ii) are equivalent to the single condition:
(iii) a · b−1 ∈H for all a, b ∈H.
However, when H is ﬁnite, the following result shows that it is sufﬁcient just to
check condition (i).
Proposition 3.9. If H is a nonempty ﬁnite subset of a group G and ab ∈H for
all a, b ∈H, then H is a subgroup of G.
Proof. We have to show that for each element a ∈H, its inverse is also
in H. All the elements, a, a2 = aa, a3 = aaa, . . . belong to H so, since H is
ﬁnite, these cannot all be distinct. Therefore, ai = aj for some 1 ⩽i < j. By
Proposition 3.7(iii), we can cancel ai from each side to obtain e = aj−1, where
j −i > 0. Therefore, e ∈H and this equation can be written as e = a(aj−i−1) =
(aj−i−1)a. Hence a−1 = aj−i−1, which belongs to H, since j −i −1 ⩾0.
□
In the group ({1, −1, i, −i}, ·), the subset {1, −1} forms a subgroup because
this subset is closed under multiplication. In the group of translations of the plane
(Example 3.1), the set of translations in the horizontal direction forms a subgroup
because compositions and inverses of horizontal translations are still horizontal
translations.
The group Z is a subgroup of Q, Q is a subgroup of R, and R is a subgroup
of C. (Remember that addition is the operation in all these groups.)
However, the set N = {0, 1, 2, . . .} of nonnegative integers is a subset of Z but
not a subgroup, because the inverse of 1, namely, −1, is not in N. This example
shows that Proposition 3.9 is false if we drop the condition that H be ﬁnite.
The relation of being a subgroup is transitive. In fact, for any group G, the
inclusion relation between the subgroups of G is a partial order relation.
Example 3.10. Draw the poset diagram of the subgroups of the group of sym-
metries of a rectangle.
Solution. By looking at the table of this group in Table 3.3, we see that Ž is
a binary operation on {e, a}; thus {e, a} is a subgroup. Also, {e, b} and {e, c} are
subgroups. If a subgroup contains a and b, it must contain a Ž b = c, so it is the

56
3
GROUPS
{e, a, b, c}
{e, c}
{e, a}
{e, b}
{e}
Figure 3.3.
Subgroups of the group of symmetries of a rectangle.
whole group. Similarly, subgroups containing a and c or b and c must be the
whole group. The poset diagram of subgroups is given in Figure 3.3.
□
CYCLIC GROUPS AND DIHEDRAL GROUPS
The number of elements in a group G is written |G| and is called the order of
the group. G is called a ﬁnite group if |G| is ﬁnite, and G is called an inﬁnite
group otherwise.
An important class of groups consists of those for which every element can be
written as a power (positive or negative) of some ﬁxed element. More precisely,
a group (G, ·) is called cyclic if there exists an element g ∈G such that G =
{gn|n ∈Z}. The element g is called a generator of the cyclic group.
Every cyclic group is abelian because gr · gs = gr+s = gs · gr.
The group ({1, −1, i, −i}, ·) is a cyclic group of order 4 generated by i because
i0 = 1, i1 = i, i2 = −1, i3 = −i, i4 = 1, i5 = i, and so on. Hence the group can
be written as ({1, i, i2, i3}, ·).
In additive notation, the group (G, +) is cyclic if G = {ng|n ∈Z} for some
g ∈G. The group (Z, +) is an inﬁnite cyclic group with generator 1 (or −1).
The order of an element g in a group (G, ·) is the least positive integer r
such that gr = e. If no such r exists, the order of the element is said to be inﬁnite.
Note the difference between the order of an element and the order of a group.
We are going to ﬁnd connections between these two orders and later prove
Lagrange’s theorem, which implies that in a ﬁnite group, the order of every
element divides the order of the group.
For example, in ({1, −1, i, −i}, ·), the identity 1 has order 1, −1 has order 2
because (−1)2 = 1, whereas i and −i both have order 4. The group has order 4.
Let Q∗= Q −{0} be the set of nonzero rational numbers. Then (Q∗, ·) is a
group under multiplication. The order of the identity element 1 is 1, and the
order of −1 is 2. The order of every other element is inﬁnite, because the only
solutions to qr = 1 with q ∈Q∗, r ⩾1 are q = ±1. The group has inﬁnite order.
However, it is not cyclic, because there is no rational number r such that every
nonzero rational can be written as rn for some n ∈Z.
The next two results show how the division algorithm for integers (see Appen-
dix 2) is used in group theory.
Proposition 3.11. Let a be an element of order r in a group G. Then for k ∈
Z, gk = e if and only if r divides k.

CYCLIC GROUPS AND DIHEDRAL GROUPS
57
Proof. If k = rm, m ∈Z, then ak = (ar)m = em = e. Conversely, if ak = e,
write k = qr + s, where q and s are in Z and 0 ⩽s < r. Then as = ak−qr =
ak(ar)−q = e · e−q = e. Since 0 ⩽s < r and r is the smallest positive integer
such that ar = e, it follows that s = 0. But then k = qr, as required.
□
Proposition 3.12. Every subgroup of a cyclic group is cyclic.
Proof. Suppose that G is cyclic with generator g and that H ⊆G is a sub-
group. If H = {e}, it is cyclic with generator e. Otherwise, let gk ∈H with
k ̸= 0. Since g−k = (gk)−1 is in H, we have gm ∈H for some m > 0, and we
choose m to be the smallest such positive integer. Write h = gm; we claim that
h generates H. Certainly, hk ∈H for every k ∈Z because h ∈H; we must
show that every element a in H is a power of h. Since a ∈G we have a =
gs, s ∈Z. By the division algorithm, write s = qm + r, where 0 ⩽r < m. Then
ar = gs−qm = gs(gm)−q = a · h−q ∈H, so r = 0 by the choice of m. Hence
a = gs = gqm = (gm)q = hq, as we wanted.
□
For any element g in a group (G, ·) we can look at all the powers of this
element, namely, {gr|r ∈Z}. This may not be the whole group, but it will be
a subgroup.
Proposition 3.13. If g is any element of order k in a group (G, ·), then H =
{gr|r ∈Z} is a subgroup of order k in (G, ·). This is called the cyclic subgroup
generated by g.
Proof. We ﬁrst check that H is a subgroup of (G, ·). This follows from the
fact that gr · gs = gr+s ∈H and (gr)−1 = g−r ∈H for all r, s ∈Z.
If the order of the element g is inﬁnite, we show that the elements gr are
all distinct. Suppose that gr = gs, where r > s. Then gr−s = e with r −s > 0,
which contradicts the fact that g has inﬁnite order. In this case, |H| is inﬁnite.
If the order of the element g is k, which is ﬁnite, we show that H = {g0 =
e, g1, g2, . . . , gk−1}. Suppose that gr = gs, where 0 ⩽s < r ⩽k −1. Multiply
both sides by g−s so that gr−s = e with 0 < r −s < k. This contradicts the fact
that k is the order of g. Hence the elements g0, g1, g2, . . . , gk−1 are all distinct.
For any other element, gt, we can write t = qk + r, where 0 ⩽r < k by the
division algorithm. Hence
gt = gqk+r = (gk)q(gr) = (eq)(gr) = gr.
Hence H = {g0, g1, g2, . . . , gk−1} and |H| = k.
□
For example, in (Z, +), the subgroup generated by 3 is {. . . , −3, 0, 3, 6, 9, . . .},
an inﬁnite subgroup that we write as 3Z = {3r|r ∈Z}.
Theorem 3.14. If the ﬁnite group G is of order n and has an element g of order
n, then G is a cyclic group generated by g.

58
3
GROUPS
Proof. From the previous proposition we know that H, the subgroup of G
generated by g, has order n. Therefore, H is a subset of the ﬁnite set G with
the same number of elements. Hence G = H and G is a cyclic group gener-
ated by g.
□
Example 3.15. Show that the Klein 4-group of symmetries of a rectangle, descri-
bed in Example 3.5, is not cyclic.
Solution. In the Klein 4-group, the identity has order 1, whereas all the other
elements have order 2. As it has no element of order 4, it cannot be cyclic.
□
All the elements of the Klein 4-group can be written in terms of a and b. We
therefore say that this group can be generated by the two elements a and b.
Example 3.16. Show that the group of proper rotations of a regular n-gon in the
plane is a cyclic group of order n generated by a rotation of 2π/n radians. This
group is denoted by Cn.
Solution. This is the group of those symmetries of the regular n-gon that can
be performed in the plane, that is, without turning the n-gon over.
Label the vertices 1 through n as in Figure 3.4. Under any symmetry, the
center must be ﬁxed, and the vertex 1 can be taken to any of the n vertices. The
image of 1 determines the rotation; hence the group is of order n.
Let g be the counterclockwise rotation of the n-gon through 2π/n. Then
g has order n, and by Theorem 3.14, the group is cyclic of order n. Hence
Cn = {e, g, g2, . . . , gn−1}.
□
Let us now consider the group of all symmetries (both proper and improper
rotations) of the regular n-gon. We call this group the dihedral group and denote
it by Dn.
Example 3.17. Show that the dihedral group, Dn, is of order 2n and is not cyclic.
Solution. Label the vertices 1 to n in a counterclockwise direction around
the n-gon. Let g be a counterclockwise rotation through 2π/n, and let h be the
improper rotation of the n-gon about an axis through the center and vertex 1, as
indicated in Figure 3.5. The element g generates the group Cn, which is a cyclic
subgroup of Dn. The element h has order 2 and generates a subgroup {e, h}.
3
2
1
n
n − 1
g
g2
Figure 3.4.
Elements of Cn.
3
2
1
n
n − 1
g
h
Figure 3.5.
Elements of Dn.

CYCLIC GROUPS AND DIHEDRAL GROUPS
59
Any symmetry will ﬁx the origin and is determined by the image of two
adjacent vertices, say 1 and 2. The vertex 1 can be taken to any of the n vertices,
and then 2 must be taken to one of the two vertices adjacent to the image of 1.
Hence Dn has order 2n.
If the image of 1 is r + 1, the image of 2 must be r + 2 or r. If the image
of 2 is r + 2, the symmetry is gr. If the image of 2 is r, the symmetry is grh.
Figure 3.6 shows that the symmetries grh and hg−r have the same effect and
therefore imply the relation grh = hg−r = hgn−r.
Hence the dihedral group is
Dn = {e, g, g2, . . . , gn−1, h, gh, g2h, . . . , gn−1h}.
Note that if n ⩾3, then gh ̸= hg; thus Dn is a noncommutative group. There-
fore, this group cannot be cyclic.
□
D2 can be deﬁned as the symmetries of the ﬁgure in Figure 3.7. Hence D2 =
{e, g, h, gh}, and each nonidentity element has order 2.
Example 3.18. Draw the group table for C4 and D4.
Solution. D4 is the group of symmetries of the square, and its table, which
is calculated using the relation grh = hg4−r, is given in Table 3.4. For example,
(g2h)(gh) = g2(hg)h = g2(g3h)h = g5h2 = g. Since C4 is a subgroup of D4,
the table for C4 appears inside the dashed lines in the top left corner.
□
Note that the order of each of the elements h, gh, g2h, and g3h in D4 is 2.
In general, the element grh in Dn is a reﬂection in the line through the center
h
h
2
n
1
2
n
1
g−r
gr
n − r
n − r + 1
n − r + 2
r + 2
r + 1
r
Figure 3.6.
Relation grh = hg−r in Dn.
h
1
2
g
Figure 3.7.
Symmetries of a 2-gon.

60
3
GROUPS
TABLE 3.4. Group D4
•
e
g
g2
g3
h
gh
g2h
g 3h
e
e
g
g2
g3
h
gh
g2h
g 3h
g
g
g2
g3
e
gh
g2h
g 3h
h
g2
g2
g3
e
g
g2h
g 3h
h
gh
g3
g3
e
g
g2
g3h
h
gh
g2h
h
h
g 3h
g 2h
gh
e
g3
g2
g
gh
gh
h
g 3h
g 2h
g
e
g3
g2
g2h
g2h
gh
h
g 3h
g 2
g
e
g3
g3h
g3h
g 2h
gh
h
g 3
g2
g
e
of the n-gon bisecting the angle between vertices 1 and r + 1. Therefore, grh
always has order 2.
MORPHISMS
Recall that a morphism between two algebraic structures is a function that pre-
serves their operations. For instance, in Example 3.5, each element of the group
K of symmetries of the rectangle induces a permutation of the vertices 1, 2, 3,
4. This deﬁnes a function f : K →S({1, 2, 3, 4}) with the property that the com-
position of two symmetries of the rectangle corresponds to the composition of
permutations of the set {1, 2, 3, 4}. Since this function preserves the operations,
it is a morphism of groups.
Two groups are isomorphic if their structures are essentially the same. For
example, the group tables of the cyclic group C4 and ({1, −1, i, −i}, ·) would be
identical if we replaced a rotation through nπ/2 by in. We would therefore say
that (C4, Ž ) and ({1, −1, i, −1}, ·) are isomorphic.
If (G, ·) and (H, ·) are two groups, the function f : G →H is called a group
morphism if
f (a · b) = f (a) · f (b)
for all a, b ∈G.
If the groups have different operations, say they are (G, ·) and (H, ⋆), the con-
dition would be written as
f (a · b) = f (a) ⋆f (b).
We often use the notation f : (G, ·) →(H, ⋆) for such a morphism. Many authors
use homomorphism instead of morphism but we prefer the simpler terminology.
A group isomorphism is a bijective group morphism. If there is an isomor-
phism between the groups (G, ·) and (H, ⋆), we say that (G, ·) and (H, ⋆) are
isomorphic and write (G, ·) ∼= (H, ⋆).
If G and H are any two groups, the trivial function that maps every element
of G to the identity of H is always a morphism. If i: Z →Q is the inclusion
map, i is a group morphism from (Z, +) to (Q, +). In fact, if H is a subgroup
of G, the inclusion map H →G is always a group morphism.

MORPHISMS
61
Let f : Z →{1, −1} be the function deﬁned by f (n) = 1 if n is even, and
f (n) = −1 if n is odd. Then it can be veriﬁed that f (m + n) = f (m) · f (n) for
any m, n ∈Z, so this deﬁnes a group morphism f : (Z, +) →({1, −1}, ·).
Let GL(2, R) be the set of 2 × 2 invertible real matrices. The one-to-one
correspondence between the set, L, of invertible linear transformations of the
plane and the 2 × 2 coefﬁcient matrices is an isomorphism between the groups
(L, Ž ) and (GL(2, R), ·).
Isomorphic groups share exactly the same properties, and we sometimes iden-
tify the groups via the isomorphism and give them the same name. If f : G →H
is an isomorphism between ﬁnite groups, the group table of H is the same as
that of G, when each element g ∈G is replaced by f (g) ∈H.
Besides preserving the operations of a group, the following result shows that
morphisms also preserve the identity and inverses.
Proposition 3.19. Let f : G →H be a group morphism, and let eG and eH be
the identities of G and H, respectively. Then
(i) f (eG) = eH.
(ii) f (a−1) = f (a)−1 for all a ∈G.
Proof. (i) Since f
is a morphism, f (eG)f (eG) = f (eG · eG) = f (eG) =
f (eG)eH. Hence (i) follows by cancellation in H (Proposition 3.7).
(ii) f (a) · f (a−1) = f (a · a−1) = f (eG) = eH by (i). Hence f (a−1) is the
unique inverse of f (a); that is f (a−1) = f (a)−1.
□
Theorem 3.20. Cyclic groups of the same order are isomorphic.
Proof. Let G = {gr|r ∈Z} and H = {hr|r ∈Z} be cyclic groups. If G and
H are inﬁnite, then g has inﬁnite order, so for r, s ∈Z, gr = gs if and only if
r = s (see Proposition 3.13). Hence the function f : G →H deﬁned by f (gr) =
hr, r ∈Z, is a bijection, and
f (grgs) = f (gr+s) = hr+s = hrhs = f (gr)f (gs)
for all r, s ∈Z, so f is a group isomorphism.
If |G| = n = |H|, then G = {e, g, g2, . . . , gn−1}, where these powers of
g
are all distinct (see the proof of Proposition 3.13). Similarly, H =
{e, h, h2, . . . , hn−1}. Then the function f : G →H deﬁned by f (gr) = hr, is
again a bijection. To see that it is a morphism, suppose that 0 ⩽r, s ⩽n −1,
and let r + s = kn + l, where 0 ⩽l ⩽n −1. Then
f (gr · gs) = f (gr+s) = f (gkn+l) = f ((gn)k · gl) = f (ek · gl) = f (gl) = hl
and
f (gr) · f (gs) = hr · hs = hr+s = hkn+l = (hn)k · hl = ek · hl = hl,
so f is an isomorphism.
□

62
3
GROUPS
Hence every cyclic group is isomorphic to either (Z, +) or (Cn, ·) for some n.
In the next chapter, we see that another important class of cyclic groups consists
of the integers modulo n, (Zn, +). Of course, the theorem above implies that
(Zn, +) ∼= (Cn, ·).
Any morphism, f : G →H, from a cyclic group G to any group H is deter-
mined just by the image of a generator. If g generates G and f (g) = h, it follows
from the deﬁnition of a morphism that f (gr) = f (g)r = hr for all r ∈Z.
Proposition 3.21. Corresponding elements under a group isomorphism have the
same order.
Proof. Let f : G →H be an isomorphism, and let f (g) = h. Suppose that g
and h have orders m and n, respectively, where m is ﬁnite. Then hm = f (g)m =
f (gm) = f (e) = e. So n is also ﬁnite, and n ⩽m, since n is the least positive
integer with the property hn = e.
On the other hand, if n is ﬁnite then f (gn) = f (g)n = hn = e = f (e). Since
f is bijective, gn = e, and hence m is ﬁnite and m ⩽n.
Therefore, either m and n are both ﬁnite and m = n, or m and n are both
inﬁnite.
□
Example 3.22. Is D2 isomorphic to C4 or the Klein 4-group of symmetries of
a rectangle?
Solution. Compare the orders of the elements given in Table 3.5. By Propo-
sition 3.21 we see that D2 cannot be isomorphic to C4 but could possibly be
isomorphic to the Klein 4-group.
In the Klein 4-group we can write c = a Ž b and we can obtain a bijection, f ,
from D2 to the Klein 4-group, by deﬁning f (g) = a and f (h) = b. Table 3.6
for the two groups show that this is an isomorphism.
□
TABLE 3.5
D2
C4
Klein 4-Group
Element
Order
Element
Order
Element
Order
e
1
e
1
e
1
g
2
g
4
a
2
h
2
g2
2
b
2
gh
2
g3
4
c
2
TABLE 3.6. Isomorphic Groups
Group D2
Klein 4-Group
·
e
g
h
gh
Ž
e
a
b
c
e
e
g
h
gh
e
e
a
b
c
g
g
e
gh
h
a
a
e
c
b
h
h
gh
e
g
b
b
c
e
a
gh
gh
h
g
e
c
c
b
a
e

PERMUTATION GROUPS
63
PERMUTATION GROUPS
A permutation of n elements is a bijective function from the set of the n elements
to itself. The permutation groups of two sets, with the same number of elements,
are isomorphic. We denote the permutation group of X = {1, 2, 3, . . . , n} by
(Sn, Ž ) and call it the symmetric group on n elements. Hence Sn ∼= S(Y) for
any n element set Y.
Proposition 3.23. |Sn| = n!
Proof. The order of Sn is the number of bijections from {1, 2, . . . , n} to itself.
There are n possible choices for the image of 1 under a bijection. Once the
image of 1 has been chosen, there are n −1 choices for the image of 2. Then
there are n −2 choices for the image of 3. Continuing in this way, we see that
|Sn| = n(n −1)(n −2) · · · 2 · 1 = n!
□
If π: {1, 2, . . . , n} →{1, 2, . . . , n} is a permutation, we denote it by

1
2
· · ·
n
π(1)
π(2)
· · ·
π(n)

.
For example, the permutation of {1, 2, 3} that interchanges 1 and 3 is written

1
2
3
3
2
1

. We think of this as


1
2
3
↓
↓
↓
3
2
1

.
We can write S2 =
	
1
2
1
2

,

1
2
2
1

, which has two elements and
S3 =
	
1
2
3
1
2
3

,

1
2
3
3
1
2

,

1
2
3
2
3
1

,

1
2
3
1
3
2

,

1
2
3
2
1
3

,

1
2
3
3
2
1

,
which is of order 3! = 6.
If π, ρ ∈Sn are two permutations, their product π Ž ρ is the permutation obtai-
ned by applying ρ ﬁrst and then π. This agrees with our notion of composition of
functions because (π Ž ρ)(x) = π(ρ(x)). (However, the reader should be aware
that some authors use the opposite convention in which π is applied ﬁrst and
then ρ.)
Example 3.24. If π =

1
2
3
3
1
2

and ρ =

1
2
3
3
2
1

are two elements of
S3, calculate π Ž ρ and ρ Ž π.

64
3
GROUPS
Solution. π Ž ρ =

1
2
3
3
1
2

Ž

1
2
3
3
2
1

. To calculate this, we start at
the right and trace the image of each element under the composition.
1    2
3
3    1
2
1    2
3
3    2
1
=
1    2
3
2
↓
↓
↓
°
Under ρ, 1 is mapped to 3, and under π, 3 is mapped to 2; thus under π Ž ρ, 1
is mapped to 2. Tracing the images of 2 and 3, we see that
π Ž ρ =

1
2
3
3
1
2

Ž

1
2
3
3
2
1

=

1
2
3
2
1
3

.
In a similar way we can show that
ρ Ž π =

1
2
3
3
2
1

Ž

1
2
3
3
1
2

=

1
2
3
1
3
2

.
Note that π Ž ρ ̸= ρ Ž π and so S3 is not commutative.
□
The permutation π =

1
2
3
3
1
2

has the effect of moving the elements
around in a cycle. This is called a cycle of length 3, and we write this as
 1
3
2 
. We think of this as 1
3
2
(
)
→
→
. The permutation π could
also be written as
 3
2
1 
or
 2
1
3 
in cycle notation.
In general, if a1, a2, . . . , ar are distinct elements of {1, 2, 3, . . . , n}, the per-
mutation π ∈Sn, deﬁned by
π(a1) = a2,
π(a2) = a3, . . . , π(ar−1) = ar,
π(ar) = a1
and π(x) = x if x /∈{a1, a2, . . . , ar}, is called a cycle of length r or an r-cycle.
We denote it by (a1a2 · · · ar).
Note
that
the
value
of
n
does
not
appear
in
the
cycle
notation.
For
example,

1
2
3
4
3
1
4
2

=
 1
3
4
2 
,
is
a
4-cycle
in
S4,
whereas

1
2
3
4
5
6
3
1
4
2
5
6

=
 1
3
4
2 
is a 4-cycle in S6, and

1
2
3
4
5
6
1
5
3
2
4
6

=
 2
5
4 
, is a 3-cycle in S6.
Proposition 3.25. An r-cycle in Sn has order r.
Proof. If π = (a1a2 · · · ar) is an r-cycle in Sn, then π(a1) = a2, π2(a1) =
a3, π3(a1) = a4, . . . and πr(a1) = a1. Similarly, πr(ai) = ai for i = 1, 2, . . . , r.

PERMUTATION GROUPS
65
Since πr ﬁxes all the other elements, it is the identity permutation. But none
of the permutations π, π2, . . . , πr−1 equal the identity permutation because they
all move the element a1. Hence the order of π is r.
□
Example 3.26. Write down π =
 1
3
4
2 
, ρ =
 1
3 
, and σ =
 1
2 
Ž  3
4 
as permutations in S4. Calculate π Ž ρ Ž σ.
Solution.
 1
3
4
2 
=

1
2
3
4
3
1
4
2

,
 1
3 
=

1
2
3
4
3
2
1
4

,
 1
2 
Ž  3
4 
=

1
2
3
4
2
1
4
3

.
We can either calculate a product of cycles from the permutation representation
or we can use the cycle representation directly. Let us calculate π Ž ρ Ž σ from
their cycles. Remember that a cycle in S4 is a bijection from {1, 2, 3, 4} to itself,
and a product of cycles is a composition of functions. In calculating such a
composition, we begin at the right and work our way left. Consider the effect of
π Ž ρ Ž σ on each of the elements 1, 2, 3, and 4.
π Ž ρ Ž σ =
 1
3
4
2 
Ž  1
3 
Ž  1
2 
Ž  3
4 
1 ←−−−−−−−
2
←−
2
←−
1
←−1
4 ←−−−−−−−
3
←−
1
←−
2
←−2
2 ←−−−−−−−
4
←−
4
←−
4
←−3
3 ←−−−−−−−
1
←−
3
←−
3
←−4
For example, 2 is left unchanged by
 3
4 
; then 2 is sent to 1 under
 1
2 
,
1 is sent to 3 under
 1
3 
, and ﬁnally, 3 is sent to 4 under
 1
3
4
2 
.
Hence π Ž ρ Ž σ sends 2 to 4. The permutation π Ž ρ Ž σ also sends 4 to 3, 3 to 2,
and ﬁxes 1. Therefore, π Ž ρ Ž σ =
 2
4
3 
.
□
Permutations that are not cycles can be split up into two or more cycles as
follows. If π is a permutation in Sn and a ∈{1, 2, 3, . . . , n}, the orbit of a
under π consists of the distinct elements a, π(a), π2(a), π3(a), . . .. We can split
a permutation up into its different orbits, and each orbit will give rise to a cycle.
Consider the permutation π =

1
2
3
4
5
6
7
8
3
2
8
1
5
7
6
4

∈S8. Here
π(1) = 3, π2(1) = π(3) = 8, π3(1) = 4, and π4(1) = 1; thus the orbit of 1 is
{1, 3, 8, 4}. This is also the orbit of 3, 4, and 8. This orbit gives rise to the cycle
 1
3
8
4 
. Since π leaves 2 and 5 ﬁxed, their orbits are {2} and {5}. The
orbit of 6 and 7 is {6, 7}, which gives rise to the 2-cycle
 6
7 
. We can picture
the orbits and their corresponding cycles as in Figure 3.8.

66
3
GROUPS
2
6
7
5
1
8
3
4
Figure 3.8.
Disjoint cycle decomposition.
It can be veriﬁed that π =  1
3
8
4 
Ž (2) Ž (5) Ž  6
7 . Since no num-
ber is in two different cycles, these cycles are called disjoint. If a permutation
is written as a product of disjoint cycles, it does not matter in which order
we write the cycles. We could write π = (5) Ž  6
7 
Ž (2) Ž  1
3
8
4 .
When writing down a product of cycles, we often omit the 1-cycles and write
π =  1
3
8
4 
Ž  6
7 . The identity permutation is usually just written
as (1).
Proposition 3.27. Every permutation can be written as a product of disjoint
cycles.
Proof. Let π be a permutation and let γ1, . . . , γk be the cycles obtained as
described above from the orbits of π. Let a1 be any number in the domain
of π, and let π(a1) = a2. If γi is the cycle containing a1, we can write γi =
(a1a2 · · · ar); the other cycles will not contain any of the elements a1, a2, . . . , ar
and hence will leave them all ﬁxed. Therefore, the product γ1 Ž γ2 Ž · · · Ž γk
will map a1 to a2, because the only cycle to move a1 or a2 is γi. Hence
π = γ1 Ž γ2 Ž · · · Ž γk, because they both have the same effect on all the numbers
in the domain of π.
□
Corollary 3.28. The order of a permutation is the least common multiple of the
lengths of its disjoint cycles.
Proof. If π is written in terms of disjoint cycles as γ1 Ž γ2 Ž · · · Ž γk, the order
of the cycles can be changed because they are disjoint. Therefore, for any integer
m, γ m = γ m
1
Ž γ m
2
Ž · · · Ž γ m
k . Because the cycles are disjoint, this is the identity
if and only if γ m
i
is the identity for each i. The least such integer is the least
common multiple of the orders of the cycles.
□
Example 3.29. Find the order of the permutation
π =

1
2
3
4
5
6
7
8
3
5
8
7
1
4
6
2

.
Solution. We can write this permutation in terms of disjoint cycles as
π =
 1
3
8
2
5 
Ž  4
7
6 
.
Hence the order of π is lcm (5, 3) = 15. Of course, we could calculate
π2, π3, π4, . . . until we obtained the identity, but this would take much
longer.
□

EVEN AND ODD PERMUTATIONS
67
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1
2
3
1 2 3
1 2 3
= (1) = e
1 2 3
2 3 1
= (1 2 3) = g
1 2 3
1 3 2
= (2 3) = h
1 2 3
2 1 3
= (1 2) = gh
1 2 3
3 2 1
= (1 3) = g2h
1 2 3
3 1 2
= (1 3 2) = g2
Figure 3.9.
Symmetries of an equilateral triangle.
TABLE 3.7. Group S3
Ž
(1)
(123)
(132)
(23)
(12)
(13)
(1)
(1)
(123)
(132)
(23)
(12)
(13)
(123)
(123)
(132)
(1)
(12)
(13)
(23)
(132)
(132)
(1)
(123)
(13)
(23)
(12)
(23)
(23)
(13)
(12)
(1)
(132)
(123)
(12)
(12)
(23)
(13)
(123)
(1)
(132)
(13)
(13)
(12)
(23)
(132)
(123)
(1)
Example 3.30. Show that D3 is isomorphic to S3 and write out the table for the
latter group.
Solution. D3 is the group of symmetries of an equilateral triangle, and any
symmetry induces a permutation of the vertices. This deﬁnes a function f : D3 →
S3. If σ, τ ∈D3, then f (σ Ž τ) is the induced permutation on the vertices, which
is the same as f (σ) Ž f (τ). Hence f is a morphism. Figure 3.9 illustrates the six
elements of D3 and their corresponding permutations. We shade the underside
of the triangle and mark the corner near vertex 1 to illustrate how the triangle
moves. To visualize this, imagine a triangular jigsaw puzzle piece and consider
all possible ways of ﬁtting this piece into a triangular hole. Any proper rotation
will leave the white side uppermost, whereas an improper rotation will leave the
shaded side uppermost.
The six permutations are all distinct; thus f is a bijection and an isomorphism
between D3 and S3. The group table for S3 is given in Table 3.7.
□
EVEN AND ODD PERMUTATIONS
We are going to show that every permutation can be given a parity, even or
odd. The deﬁnition derives from an action of each permutation σ in Sn on a

68
3
GROUPS
polynomial f (x1, x2, . . . , xn) in n variables by permuting the variables:
σf (x1, x2, . . . , xn) = f (xσ1, xσ2, . . . , xσn).
For example, if σ =
 1
2
3 
in S4 and f (x1, x2, x3, x4) = 2x1x4 −3x2
2 +
x2x3
3, then σf = 2x2x4 −3x2
3 + x3x3
1.
Our use of this action involves a particular polynomial D = D(x1, x2, . . . , xn)
called the discriminant, deﬁned to be the product of all terms (xi −xj), where
i < j. More formally,
D =

0⩽i<j⩽n
(xi −xj).
For example, if n = 3, then D = (x1 −x2)(x1 −x3)(x2 −x3). Given a permuta-
tion σ ∈Sn, we have
σD =

0⩽i<j⩽n
(xσi −xσj).
Thus if n = 3 and σ = (12) ∈S3, then σD = (x2 −x1)(x2 −x3)(x1 −x3) =
−D. In fact, σD = ±D for every σ ∈Sn, and we say that
σ is even if σD = D
and
σ is odd if σD = −D.
We are going to determine an easy method for deciding which is the case. A 2-
cycle is called a transposition, and surprisingly, much of the discussion centers
around determining the parity of these transpositions. We are going to show that
every transposition is odd.
Let D denote the discriminant in n variables x1, x2, . . . , xn, and deﬁne
Dk/m = the product of all terms in D involving xk, except (xk −xm)
Dk,m = the product of all terms in D involving neither xk nor xm.
For example, if n = 5, we have
D2/4 = (x1 −x2)(x2 −x3)(x2 −x5)
D4/2 = (x1 −x4)(x3 −x4)(x4 −x5)
D2,4 = (x1 −x3)(x1 −x5)(x3 −x5).
Then D factors as follows:
D = (xk −xm)Dk/mDm/kDk,m.
Now ﬁx a transposition τ = (k m) in Sn, where k < m. Since τ interchanges k
and m, we see that
τDk/m = uDm/k
where u = 1 or u = −1.

EVEN AND ODD PERMUTATIONS
69
Since τ 2 is the identity permutation, we have
Dk/m = τ 2Dk/m = τ(τDk/m) = τ(uDm/k) = u(τDm/k).
Because u2 = 1, it follows that
τDm/k = uDk/m.
Since τDk,m = Dk,m, applying τ to D gives
τD = τ(xk −xm) · τDk/m · τDm/k · τDk,m
= (xm −xk) · uDm/k · uDk/m · Dk,m
= −D
because u2 = 1. Hence τ is odd, and we have proved:
Proposition 3.31. Every transposition is odd.
With this we can determine the parity of an arbitrary permutation σ in Sn. The
idea is to factor σ as a product of transpositions, and we begin with the cycles.
The proof of the next result is a straightforward veriﬁcation.
Proposition 3.32. Every r-cycle is a product of r −1 transpositions (not neces-
sarily disjoint); in fact,
 a1
a2
· · ·
ar

=
 a1
a2

Ž  a2
a3

Ž · · · Ž  ar−1
ar

.
Since every permutation σ is a product of disjoint cycles by Proposition 3.27, it
follows that σ is a product of transpositions. This gives us the desired parity test.
Theorem 3.33. Parity Theorem. Every permutation σ ∈Sn is a product of
transpositions. Moreover, if σ is a product of m transpositions in any way at
all, the parity of σ equals the parity of m. That is, σ is even if m is even, and σ
is odd if m is odd.
Proof. Write σ = τ1τ2 · · · τm, where the τi are transpositions. If D is the
discriminant in n variables, then τiD = −D for each i by Proposition 3.31.
Hence the effect of σ = τ1τ2 · · · τm on D is to change the sign m times. Thus
σD = (−1)mD, and the result follows.
□
The following result is now a consequence of Proposition 3.32.
Corollary 3.34. An n-cycle is an even permutation if n is odd and an odd per-
mutation if n is even.

70
3
GROUPS
Example 3.35. Write the permutation
π =

1
2
3
4
5
6
7
8
4
1
8
2
7
3
6
5

as a product of disjoint cycles and determine its order and parity.
Solution. As disjoint cycles π =  1
4
2 
Ž  3
8
5
7
6 . Hence the
order of π is lcm (3, 5) = 15. The parity of the 3-cycle
 1
4
2 
is even, and
the parity of the 5-cycle  3
8
5
7
6  is even; therefore, the parity of π is
(even ) Ž (even ) = even.
□
Denote the set of even permutations on n elements by An. It follows from
Theorem 3.33 that An is a subgroup of Sn, called the alternating group on n
elements. For example,
A4 =



(1 2) Ž (3 4),
(1),
(1 3) Ž (2 4),
(1 2 3),
(1 2 4),
(1 3 4),
(2 3 4),
(1 4) Ž (2 3),
(1 3 2),
(1 4 2),
(1 4 3),
(2 4 3),


,
a group of 12 elements. In fact, we have
Theorem 3.36. |An| = 1
2n! for every n ⩾2.
Proof. Let On denote the set of odd permutations in Sn, so that Sn = An ∪On
and An ∩On = Ø. Hence n! = |Sn| = |An| + |On|, so it sufﬁces to show that
|An| = |On|. We do this by ﬁnding a bijection f : An →On. To this end, write
τ =
 1
2 
and deﬁne f by f (σ) = τ Ž σ for all σ in An (τ Ž σ is odd because
σ is even and τ is odd). Then f is injective because f (σ) = f (σ1) implies that
τ Ž σ = τ Ž σ1, so σ = σ1 by cancellation in Sn. To see that f is surjective, let
λ ∈On. Then τ Ž λ ∈An and f (τ Ž λ) = τ Ž (τ Ž λ) = λ because τ Ž τ = ε. Thus
f is surjective, as required.
□
Proposition 3.37. Every even permutation can be written as a product of 3-cycles
(not necessarily disjoint).
Proof. By Theorem 3.33, an even permutation can be written as a product of
an even number of transpositions. We show that any product of two transpositions
is a product of 3-cycles. If these two transpositions are identical, their product
is the identity. If the two transpositions have one element in common, say (ab)
and (bc), their product (ab) Ž (bc) = (abc), a 3-cycle. If the two transpositions
have no elements in common, say (ab) and (cd), we can write their product as
(ab) Ž (cd) = (ab) Ž (bc) Ž (bc) Ž (cd) = (abc) Ž (bcd),
a product of two 3-cycles.
□

EXERCISES
71
Theorem 3.33 and Proposition 3.37 show, respectively, that Sn is generated
by the 2-cycles and An is generated by the 3-cycles.
CAYLEY’S REPRESENTATION THEOREM
At the beginning of the nineteenth century, groups appeared only in a very
concrete form, such as symmetry groups or permutation groups. Arthur Cayley
(1821–1895) was the ﬁrst mathematician to deal with groups abstractly in terms
of axioms, but he showed that any abstract group can be considered as a subgroup
of a symmetric group. Hence, in some sense, if you know all about symmetry
groups and permutation groups, you know all about group theory. This result is
analogous to Stone’s representation theorem for boolean algebras, which proves
that any abstract boolean algebra can be considered as an algebra of subsets of
a set.
Theorem 3.38. Cayley’s Theorem. Every group (G, ·) is isomorphic to a sub-
group of its symmetric group (S(G), Ž ).
Proof. For each element g ∈G, deﬁne πg: G →G by πg(x) = g · x. We show
that πg is a bijection. It is surjective because, for any y ∈G, πg(g−1 · y) = g ·
(g−1 · y) = y. It is injective because πg(x) = πg(y) implies that g · x = g · y,
and so, by Proposition 3.7, x = y. Hence πg ∈S(G).
Let H = {πg ∈S(G)|g ∈G}. We show that (H, Ž ) is a subgroup of (S(G), Ž )
isomorphic to (G, ·). In fact, we show that the function ψ: G →H by ψ(g) = πg
is a group isomorphism. This is clearly surjective. It is also injective because
ψ(g) = ψ(h) implies that πg = πh, and πg(e) = πh(e) implies that g = h.
It remains to show that ψ
preserves the group operation. If g, h ∈
G, πg·h(x) = (g · h)(x) = g · (h · x) = πg(h · x) = (πg Ž πh)(x) and πg·h = πg Ž
πh. Also, πh−1 Ž πh = πh−1·h = πe; thus (πh)−1 = πh−1 ∈H. Hence H is a
subgroup of S(G), and ψ(g · h) = ψ(g) Ž ψ(h).
□
Corollary 3.39. If G is a ﬁnite group of order n, then G is isomorphic to a
subgroup of Sn.
Proof. This follows because S(G) is isomorphic to Sn.
□
This is not of very much practical value, however, because Sn has order n!,
which is much larger than the order of G, in general.
EXERCISES
Construct tables for the groups designated in Exercises 3.1 to 3.4.
3.1. C5.
3.2 D4.
3.3. (P(X), ), where X = {a, b, c}.
3.4 A4.

72
3
GROUPS
For Exercises 3.5 to 3.16, state which are groups and which are abelian groups.
Give reasons for your answers.
3.5. (Mn(R), +).
3.6. ({1, 2, 3, 4, 6, 12}, gcd).
3.7. ({a + b
√
2|a, b ∈Q}, +).
3.8. ({a/b ∈Q|a, b ∈Z, b odd}, +).
3.9. ({z ∈C||z| = 1}, +).
3.10. ({z ∈C||z| = 1}, ·).
3.11.
	
1
0
0
1

,

−1
0
0
1

,

1
0
0
−1

,

−1
0
0
−1

, ·

.
3.12. (Mn(R) −{0}, ·), where 0 is the n × n zero matrix.
3.13. ({1, ζ, ζ 2, ζ 3, . . . , ζ n−1}, ·), where ζ is a complex nth root of 1.
3.14. ({e, a}, ⋆), where e ⋆e = e and e ⋆a = a ⋆e = a ⋆a = a.
3.15. (R∗, ∼), where R∗= R −{0} and x ∼y is xy if x > 0, and x/y if x < 0.
3.16. (Z, ⋆), where m ⋆n is m + n if m is even, and m −n if m is odd.
3.17. Prove that in any group (G, ·), (a1 · · · an)−1 = a−1
n · · · a−1
1 .
3.18. If k is an integer (positive or negative), prove that (a−1ba)k = a−1bka in
any group (G, ·).
3.19. If G is a group in which a2 = e, the identity for all a ∈G, show that G
is abelian.
3.20. Prove that G is abelian if and only if (ab)2 = a2b2 for all a, b ∈G.
3.21. If a is not the identity in a group and a4b = ba5, prove that ab ̸= ba.
3.22. Prove that the order of the element g−1 is the same as the order of g.
3.23. Prove that the order of the element ab is the same as the order of ba.
3.24. Prove that every image of a cyclic group is cyclic.
3.25. The gaussian integers comprise the set of complex numbers Z[i] =
{a + ib|a, b ∈Z}. Is the group (Z[i], +) a cyclic group?
For Exercises 3.26 to 3.33, describe the symmetry groups of the ﬁgures.
3.26.
3.27.
3.28.
3.29.
3.30.
3.31.
3.32.
3.33.
For Exercises 3.34 and 3.35, describe the symmetry groups of the frieze patterns.
These patterns are repeated indeﬁnitely in both directions.
3.34.
3.35.
3.36. Prove that the relation of being a subgroup is a partial order on the set of
subgroups of a group G.
3.37. Draw the poset diagram of the subgroups of C6.
3.38. Draw the poset diagram of the subgroups of S3.

EXERCISES
73
3.39. If H and K are subgroups of a group G, prove that H ∩K is also a
subgroup of G. Is H ∪K necessarily a subgroup of G?
3.40. If f : G →H and g: H →K are group morphisms, show that g Ž f : G →
K is also a group morphism.
3.41. Find all the group morphisms from (Z, +) to (Q, +).
3.42. Show that the set {f1, f2, f3, f4, f5, f6} of functions R −{0, 1} →R −
{0, 1} under composition is isomorphic to S3, where
f1(x) = x,
f2(x) = 1 −x,
f3(x) = 1
x ,
f4(x) = 1 −1
x ,
f5(x) =
1
1 −x ,
f6(x) =
x
x −1.
3.43. Is (Z, +) isomorphic to (Q∗, ·), where Q∗= Q −{0}? Give reasons.
3.44. Is (R, +) isomorphic to (R+, ·), where R+ = {x ∈R|x > 0}? Give reasons.
3.45. Find the orders of all the elements in A4.
3.46. Is A4 ∼= D6? Give reasons.
3.47. Draw the table and ﬁnd the order of all the elements in the group
({±1, ±i, ±j, ±k}, ·), where i2 = j 2 = k2 = −1, ij = k = −ji, jk = i =
−kj, and ki = j = −ik. This is called the quaternion group Q of order 8.
3.48. Let G be the group generated by the matrices

0
1
−1
0

and

0
1
1
0

under matrix multiplication. Show that G is a non-abelian group of order
8. Is it isomorphic to D4 or the quaternion group Q?
3.49. Show that Dk is isomorphic to the group generated by

0
1
1
0

and
 ζ
0
0
ζ −1

under matrix multiplication, where ζ = exp(2πi/k), a
complex kth root of unity.
3.50. Construct the table for the group generated by g and h, where g and h
satisfy the relations g3 = h2 = e and gh = hg2.
3.51. Prove that a group of even order always has at least one element of order 2.
3.52. Find a subgroup of S7 of order 10.
3.53. Find a subgroup of S5 of order 3.
3.54. Find a subgroup of A4 isomorphic to the Klein 4-group.
Multiply out the permutations in Exercises 3.55 to 3.58.
3.55.

1
2
3
4
2
4
3
1

Ž

1
2
3
4
4
3
2
1

.
3.56.

1
2
3
4
5
6
4
5
2
6
3
1
3
.
3.57.  1
2
3
4
5 
Ž  2
3
4 .
3.58.
 3
6
2 
Ž  1
5 
Ž  4
2 
.

74
3
GROUPS
For Exercises 3.59 to 3.62, write the permutations as a product of disjoint cycles.
Find the order of each permutation and state whether the permutation is even
or odd.
3.59.

1
2
3
4
5
6
6
1
2
3
4
5

.
3.60.

1
2
3
4
5
6
7
2
4
6
1
5
7
3

.
3.61.

1
2
3
4
5
6
5
6
4
3
2
1

.
3.62.

1
2
3
4
5
6
7
8
9
8
9
4
2
7
3
5
1
6

.
Find the permutations for Exercises 3.63 to 3.66.
3.63.

1
2
3
4
5
5
1
2
3
4
−1
.
3.64.

1
2
3
4
5
6
2
1
6
5
3
4
−1
.
3.65.
 1
2
3 −1.
3.66.
 1
2
4
6
5
7 −2.
For each polynomial in Exercises 3.67 to 3.69, ﬁnd the permutations of the
subscripts that leave the value of the polynomial unchanged. These will form
subgroups of S4, called the symmetry groups of the polynomials.
3.67. (x1 + x2)(x3 + x4).
3.68. (x1 −x2)(x3 −x4).
3.69. (x1 −x2)2 + (x2 −x3)2 + (x3 −x4)2 + (x4 −x1)2.
3.70. Describe the group of proper rotations of the tetrahedron with vertices
(0, 0, 0), (1, 0, 0), (0, 1, 0), and (0, 0, 1) in R3.
3.71. Write
G = {1, −1},
a
multiplicative
subgroup
of
R+,
and
deﬁne
f : Sn →G by f (σ) =
	
1
if σ is even
−1
if σ is odd . Prove that f is an onto
group morphism.
3.72. If g and h are elements in a group G, show that g and h−1gh have the
same order.
3.73. What is the number of generators of the cyclic group Cn?
3.74. Express (123) Ž (456) as the power of a single cycle in S6. Can you gener-
alize this result?
3.75. A perfect interlacing shufﬂe of a deck of 2n cards is the permutation

1
2
3
· · ·
n
n + 1
n + 2
· · ·
2n
2
4
6
· · ·
2n
1
3
· · ·
2n −1

. What is the least
number of perfect shufﬂes that have to be performed on a deck of 52
cards before the cards are back in their original position? If there were 50
cards, what would be the least number?
3.76. The center of a group G is the set Z(G) = {x ∈G|xg = gx for all g ∈
G}. Show that Z(G) is an abelian subgroup of G.
3.77. Find the center of D3.
3.78. Find the center of D4.
3.79. Prove that Sn is generated by the elements (12), (23), (34), . . . , (n −1 n).

EXERCISES
75
3.80. Prove that Sn is generated by the elements (123 · · · n) and (12).
3.81. Prove that An is generated by the set {(12r)|r = 3, 4, . . . , n}.
3.82. The well-known 15-puzzle consists of a shallow box ﬁlled with 16 small
squares in a 4 × 4 array. The bottom right corner square is removed, and the
other squares are labeled as in Figure 3.10. By sliding the squares around
(without lifting them up), show that the set of possible permutations that
can be obtained with the bottom right square blank is precisely A15. (There
is no known easy proof that all elements in A15 must occur.)
1
2
3
4
5
6
7
8
9 10 11 12
13 14 15
Initial position
4
3
2
1
5
6
7
8
12 11 10 9
13 14 15
(1)
10 9
8
7
11 2
1
6
12 3
4
5
13 14 15
(2)
8 14 11 3
12 2 15 9
6
4 13 1
5
7 10
(3)
Figure 3.10.
The 15-puzzle.
3.83. Which of the positions of the 15-puzzle shown in Figure 3.10 can
be achieved?
3.84. An automorphism of a group G is an isomorphism from G to itself. Prove
that the set of all automorphisms of G forms a group under composition.
3.85. Find the automorphism group of the Klein 4-group.
3.86. Find the automorphism group of C3.
3.87. Find the automorphism group of C4.
3.88. Find the automorphism group of S3.
3.89. A word on {x, y} is a ﬁnite string of the symbols x, x−1, y, y−1, where x
and x−1 cannot be adjacent and y and y−1 cannot be adjacent; for example,
xxy−1x and x−1x−1yxy are words. Let F be the set of such words together
with the empty word, which is denoted by 1. The operation of concatenation
places one word after another. Show that F is a group under concatenation,
where any strings of the form xx−1, x−1x, yy−1, y−1y are deleted in a
concatenated word. F is called the free group on two generators. Is F
abelian? What is the inverse of x−1x−1yxy?

4
QUOTIENT GROUPS
Certain techniques are fundamental to the study of algebra. One such technique
is the construction of the quotient set of an algebraic object by means of an
equivalence relation on the underlying set. For example, if the object is the
group of integers (Z, +), the congruence relation modulo n on Z will deﬁne the
quotient group of integers modulo n.
This quotient construction can be applied to numerous algebraic structures,
including groups, boolean algebras, and vector spaces.
In this chapter we introduce the concept of an equivalence relation and go on
to apply this to groups. We obtain Lagrange’s theorem, which states that the order
of a subgroup divides the order of the group, and we also obtain the morphism
theorem for groups. We study the implications of these two theorems and classify
the groups of low order.
EQUIVALENCE RELATIONS
Relations are one of the basic building blocks of mathematics (as well as of the
rest of the world). A relation R from a set S to a set T is a subset of S × T .
We say that a is related to b under R if the pair (a, b) belongs to the subset,
and we write this as aRb. If (a, b) does not belong to the subset, we say that a
is not related to b, and write aR

b. This deﬁnition even covers many relations in
everyday life, such as “is the father of,” “is richer than,” and “goes to the same
school as” as well as mathematical relations such as “is equal to,” “is a member
of,” and “is similar to.” A relation R from S to T has the property that for any
elements a in S, and b in T , either aRb or aR

b.
Any function f : S →T gives rise to a relation R from S to T by taking aRb
to mean f (a) = b. The subset R of S × T is the graph of the function. However,
relations are much more general than functions. One element can be related to
many elements or to no elements at all.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
76

EQUIVALENCE RELATIONS
77
A relation from a set S to itself is called a relation on S. Any partial order
on a set, such as “⩽” on the real numbers, or “is a subset of” on a power set
P(X), is a relation on that set. “Equals” is a relation on any set S and is deﬁned
by the subset {(a, a)|a ∈S} of S × S. An equivalence relation is a relation that
has the most important properties of the “equals” relation.
A relation E on a set S is called an equivalence relation if the following
conditions hold.
(i) aEa for all a ∈S.
(reﬂexive condition)
(ii) If aEb, then bEa.
(symmetric condition)
(iii) If aEb and bEc, then aEc.
(transitive condition)
If E is an equivalence relation on S and a ∈S, then [a] = {x ∈S|xEa} is
called the equivalence class containing a. The set of all equivalence classes is
called the quotient set of S by E and is denoted by S/E. Hence
S/E = {[a]|a ∈S}.
Proposition 4.1. If E is an equivalence relation on a set S, then
(i) If aEb, then [a] = [b].
(ii) If aE

b, then [a] ∩[b] = Ø.
(iii) S is the disjoint union of all the distinct equivalence classes.
Proof. (i) If aEb, let x be any element of [a]. Then xEa and so xEb by
transitivity. Hence x ∈[b] and [a] ⊆[b]. The symmetry of E implies that bEa,
and an argument similar to the above shows that [b] ⊆[a]. This proves that
[a] = [b].
(ii) Suppose that aE

b. If there was an element x ∈[a] ∩[b], then xEa, xEb,
so aEb by symmetry and transitivity. Hence [a] ∩[b] = Ø.
(iii) Parts (i) and (ii) show that two equivalence classes are either the same or
disjoint. The reﬂexivity of E implies that each element a ∈S is in the equivalence
class [a]. Hence S is the disjoint union of all the equivalence classes.
□
A collection of nonempty subsets is said to partition a set S if the union of
the subsets is S and any two subsets are disjoint. The previous proposition shows
that any equivalence relation partitions the set into its equivalence classes. Each
element of the set belongs to one and only one equivalence class.
It can also be shown that every partition of a set gives rise to an equivalence
relation whose classes are precisely the subsets in the partition.
Example 4.2. Let n be a ﬁxed positive integer and a and b any two integers.
We say that a is congruent to b modulo n if n divides a −b. We denote
this by
a ≡b mod n.

78
4
QUOTIENT GROUPS
Show that this congruence relation modulo n is an equivalence relation on Z. The
set of equivalence classes is called the set of integers modulo n and is denoted
by Zn.
Solution. Write “n|m” for “n divides m,” which means that there is some
integer k such that m = nk. Hence a ≡b mod n if and only if n|(a −b).
(i) For all a ∈Z, n|(a −a), so a ≡a mod n and the relation is reﬂexive.
(ii) If a ≡b mod n, then n|(a −b), so n| −(a −b). Hence n|(b −a) and
b ≡a mod n.
(iii) If a ≡b mod n and b ≡c mod n, then n|(a −b) and n|(b −c), so
n|(a −b) + (b −c). Therefore, n|(a −c) and a ≡c mod n.
Hence congruence modulo n is an equivalence relation on Z.
□
In the congruence relation modulo 3, we have the following equivalence classes:
[0] = {. . . , −3, 0, 3, 6, 9, . . .}
[1] = {. . . , −2, 1, 4, 7, 10, . . .}
[2] = {. . . , −1, 2, 5, 8, 11, . . .}
[3] = {. . . , 0, 3, 6, 9, 12, . . .} = [0]
Any equivalence class must be one of [0], [1], or [2], so Z3 = {[0], [1], [2]}.
In general, Zn = {[0], [1], [2], . . . , [n −1]}, since any integer is congruent
modulo n to its remainder when divided by n.
One set of equivalence classes that is introduced in elementary school is the
set of rational numbers. Students soon become used to the fact that
1
2 and 3
6
represent the same rational number. We need to use the concept of equivalence
class to deﬁne a rational number precisely. Deﬁne the relation E on Z × Z∗
(where Z∗= Z −{0}) by (a, b) E (c, d) if and only if ad = bc. This is an
equivalence relation on Z × Z∗, and the equivalence classes are called rational
numbers. We denote the equivalence class [(a, b)] by a
b. Therefore, since (1, 2)
E(3, 6), it follows that 1
2 = 3
6.
Two series-parallel circuits involving the switches A1, A2, . . . , An are said to
be equivalent if they both are open or both are closed for any position of the n
switches. This is an equivalence relation, and the equivalence classes are the 22n
distinct types of circuits controlled by n switches.
Any permutation π on a set S induces an equivalence relation, ∼, on S where
a ∼b if and only if b = πr(a), for some r ∈Z. The equivalence classes are the
orbits of π. In the decomposition of the permutation π into disjoint cycles, the
elements in each cycle constitute one orbit.
COSETS AND LAGRANGE’S THEOREM
The congruence relation modulo n on Z can be deﬁned by a ≡b mod n if and
only if a −b ∈nZ, where nZ is the subgroup of Z consisting of all multiples

COSETS AND LAGRANGE’S THEOREM
79
of n. We now generalize this notion and deﬁne congruence in any group modulo
one of its subgroups. We are interested in the equivalence classes, which we call
cosets.
Let (G, ·) be a group with subgroup H. For a, b ∈G, we say that a is
congruent to b modulo H, and write a ≡b mod H if and only if ab−1 ∈H.
Proposition 4.3. The relation a ≡b mod H is an equivalence relation on G. The
equivalence class containing a can be written in the form Ha = {ha|h ∈H}, and
it is called a right coset of H in G. The element a is called a representative of
the coset Ha.
Proof. (i) For all a ∈G, aa−1 = e ∈H; thus the relation is reﬂexive.
(ii) If a ≡b mod H, then ab−1 ∈H; thus ba−1 = (ab−1)−1 ∈H. Hence b ≡
a mod H, and the relation is symmetric.
(iii) If a ≡b and b ≡c mod H, then ab−1 and bc−1 ∈H. Hence ac1 =
(ab−1)(bc−1) ∈H and a ≡c mod H. The relation is transitive. Hence ≡is an
equivalence relation. The equivalence class containing a is
{x ∈G|x ≡a mod H} = {x ∈G|xa−1 = h ∈H}
= {x ∈G|x = ha, where h ∈H}
= {ha|h ∈H},
which we denote by Ha.
□
Example 4.4. Find the right cosets of A3 in S3.
Solution. One coset is the subgroup itself A3 = {(1), (123), (132)}. Take any
element not in the subgroup, say (12). Then another coset is
A3(12) = {(12), (123) Ž (12), (132) Ž (12)} = {(12), (13), (23)}.
Since the right cosets form a partition of S3 and the two cosets above contain all
the elements of S3, it follows that these are the only two cosets.
In fact, A3 = A3(123) = A3(132) and A3(12) = A3(13) = A3(23).
□
Example 4.5. Findtherightcosetsof H = {e, g4, g8}inC12 = {e, g, g2, . . . , g11}.
Solution. H itself is one coset. Another is Hg = {g, g5, g9}. These two cosets
have not exhausted all the elements of C12, so pick an element, say g2, which
is not in H or Hg. A third coset is Hg2 = {g2, g6, g10} and a fourth is Hg3 =
{g3, g7, g11}.
Since C12 = H ∪Hg ∪Hg2 ∪Hg3, these are all the cosets.
□
As the examples above suggest, every coset contains the same number of
elements. We use this result to prove the famous theorem of Joseph Lagrange
(1736–1813).

80
4
QUOTIENT GROUPS
Lemma 4.6. There is a bijection between any two right cosets of H in G.
Proof. Let Ha be a right coset of H in G. We produce a bijection between
Ha and H, from which it follows that there is a bijection between any two
right cosets.
Deﬁne ψ: H →Ha by ψ(h) = ha. Then ψ is clearly surjective. Now suppose
that ψ(h1) = ψ(h2), so that h1a = h2a. Multiplying each side by a−1 on the
right, we obtain h1 = h2. Hence ψ is a bijection.
□
Theorem 4.7. Lagrange’s Theorem. If G is a ﬁnite group and H is a subgroup
of G, then |H| divides |G|.
Proof. The right cosets of H in G form a partition of G, so G can be written
as a disjoint union
G = Ha1 ∪Ha2 ∪· · · ∪Hak for a ﬁnite set of elements a1, a2, . . . , ak ∈G.
By Lemma 4.6, the number of elements in each coset is |H|. Hence, counting
all the elements in the disjoint union above, we see that |G| = k|H|. Therefore,
|H| divides |G|.
□
If H is a subgroup of G, the number of distinct right cosets of H in G is
called the index of H in G and is written |G : H|. The following is a direct
consequence of the proof of Lagrange’s theorem.
Corollary 4.8. If G is a ﬁnite group with subgroup H, then
|G : H| = |G|/|H|.
Corollary 4.9. If a is an element of a ﬁnite group G, then the order of a divides
the order of G.
Proof. Let H = {ar|r ∈Z} be the cyclic subgroup generated by a. By Propo-
sition 3.13, the order of the subgroup H is the same as the order of the element
a. Hence, by Lagrange’s theorem, the order of a divides the order of G.
□
Corollary 4.10. If a is an element of the ﬁnite group G, then a|G| = e.
Proof. If m is the order of a, then |G| = mk for some integer k. Hence a|G| =
amk = (am)k = ek = e.
□
Corollary 4.11. If G is a group of prime order, then G is cyclic.
Proof. Let |G| = p, a prime number. By Corollary 4.9, every element has
order 1 or p. But the only element of order 1 is the identity. Therefore, all the

COSETS AND LAGRANGE’S THEOREM
81
other elements have order p, and there is at least one because |G| ⩾2. Hence
by Theorem 3.14, G is a cyclic group.
□
The converse of Lagrange’s theorem is false, as the following example shows.
That is, if k is a divisor of the order of G, it does not necessarily follow that G
has a subgroup of order k.
Example 4.12. A4 is a group of order 12 having no subgroup of order 6.
Solution. A4 contains one identity element, eight 3-cycles of the form (abc),
and three pairs of transpositions of the form (ab) Ž (cd), where a, b, c, and
d are distinct elements of {1, 2, 3, 4}. If a subgroup contains a 3-cycle (abc),
it must also contain its inverse (acb). If a subgroup of order 6 exists, it must
contain the identity and a product of two transpositions, because the odd number
of nonidentity elements cannot be made up of 3-cycles and their inverses. A
subgroup of order 6 must also contain at least two 3-cycles because A4 only
contains four elements that are not 3-cycles.
Without loss of generality, suppose that a subgroup of order 6 contains the
elements (abc) and (ab) Ž (cd). Then it must also contain the elements (abc)−1 =
(acb), (abc) Ž (ab) Ž (cd) = (acd), (ab) Ž (cd) Ž (abc) = (bdc), and (acd)−1 =
(adc), which, together with the identity, gives more than six elements. Hence A4
contains no subgroup of order 6.
□
The next proposition strengthens Lagrange’s theorem in the case of ﬁnite
cyclic groups. The following lemma, of interest in its own right, will be needed.
Lemma 4.13. Let g be an element of order n in a group, and let m ⩾1.
(i) If gcd(n, m) = d, then gm has order n/d.
(ii) In particular, if m divides n, then gm has order n/m.
Proof. (i). We have (gm)n/d = (gn)m/d = em/d = e. If (gm)k = e, we must
show that n
d divides k. We have gmk = e, so n divides mk by Proposition 3.11.
Hence n
d divides m
d k. But n
d and m
d
are relatively prime by Theorem 11,
Appendix 2, so n
d divides k (by the same theorem).
(ii). If m divides n, then gcd(n, m) = m, so (i) implies (ii).
□
Proposition 4.14. If G is a cyclic group of order n, and if k divides n, then
G has exactly one subgroup H of order k. In fact, if g generates G, then H is
generated by gn/k.
Proof. Let H denote the subgroup generated by gn/k. Then |H| = k because
gn/k has order k by Lemma 4.13

with m = n
k

. Now let K be any subgroup of

82
4
QUOTIENT GROUPS
G of order k. By Proposition 3.12, K is generated by gm for some m ∈Z. Then
gm has order |K| = k by Proposition 3.13. But if d = gcd(m, n), then gm also
has order n/d by Lemma 4.13. Thus k = n/d, so d = n/k. Write d = xm +
yn, x, y ∈Z (by Theorem 8, Appendix 2). Then gn/k = gd = (gm)x(gn)y =
(gm)x ∈K. Since gn/k generates H, it follows that H ⊆K, so H = K because
|H| = |K|.
□
NORMAL SUBGROUPS AND QUOTIENT GROUPS
Let G be a group with subgroup H. The right cosets of H in G are equivalence
classes under the relation a ≡b mod H, deﬁned by ab−1 ∈H. We can also
deﬁne the relation L on G so that aLb if and only if b−1a ∈H. This relation,
L, is an equivalence relation, and the equivalence class containing a is the left
coset aH = {ah|h ∈H}. As the following example shows, the left coset of an
element does not necessarily equal the right coset.
Example 4.15. Find the left and right cosets of H = A3 and K = {(1), (12)}
in S3.
Solution. We calculated the right cosets of H = A3 in Example 4.4.
Right Cosets
Left Cosets
H
= {(1), (123), (132)}
H
= {(1), (123), (132)}
H(12) = {(12), (13), (23)}
(12)H = {(12), (23), (13)}
In this case, the left and right cosets of H are the same.
However, the left and right cosets of K are not all the same.
Right Cosets
Left Cosets
K
= {(1), (12)}
K
= {(1), (12)}
K(13) = {(13), (132)}
(13)K = {(13), (123)}
K(23) = {(23), (123)}
(23)K = {(23), (132)}
□
Since a ≡b mod H is an equivalence relation for any subgroup H of a group
G and the quotient set is the set of right cosets {Ha|a ∈G}, it is natural to ask
whether this quotient set is also a group with a multiplication induced by the
multiplication in G. We show that this is the case if and only if the right cosets
of H equal the left cosets.
A subgroup H of a group G is called a normal subgroup of G if g−1hg ∈H
for all g ∈G and h ∈H.
Proposition 4.16. Hg = gH, for all g ∈G, if and only if H is a normal sub-
group of G.

NORMAL SUBGROUPS AND QUOTIENT GROUPS
83
Proof. Suppose that Hg = gH. Then, for any element h ∈H, hg ∈Hg =
gH. Hence hg = gh1 for some h1 ∈H and g−1hg = g−1gh1 = h1 ∈H. There-
fore, H is a normal subgroup.
Conversely, if H is normal, let hg ∈Hg and g−1hg = h1 ∈H. Then hg =
gh1 ∈gH and Hg ⊆gH. Also, ghg−1 = (g−1)−1hg−1 = h2 ∈H, since H is
normal, so gh = h2g ∈Hg. Hence, gH ⊆Hg, and so Hg = gH.
□
Therefore, A3 is a normal subgroup of S3 by Example 4.13, whereas {(1), (12)}
is not.
Proposition 4.17. Any subgroup of an abelian group is normal.
Proof. If H is a subgroup of an abelian group, G, then g−1hg = hg−1g =
h ∈H for all g ∈G, h ∈H. Hence H is normal.
□
If N is a normal subgroup of a group G, the left cosets of N in G are the
same as the right cosets of N in G, so there will be no ambiguity in just talking
about the cosets of N in G.
Theorem 4.18. If N is a normal subgroup of (G, ·), the set of cosets G/N =
{Ng|g ∈G} forms a group (G/N, ·), where the operation is deﬁned by
(Ng1) · (Ng2) = N(g1 · g2). This group is called the quotient group or factor
group of G by N.
Proof. The operation of multiplying two cosets, Ng1 and Ng2, is deﬁned in
terms of particular elements, g1 and g2, of the cosets. For this operation to make
sense, we have to verify that, if we choose different elements, h1 and h2, in
the same cosets, the product coset N(h1 · h2) is the same as N(g1 · g2). In other
words, we have to show that multiplication of cosets is well deﬁned.
Since h1 is in the same coset as g1, we have h1 ≡g1 mod N. Similarly,
h2 ≡g2 mod N. We show that Nh1h2 = Ng1g2. We have h1g−1
1
= n1 ∈
N and h2g−1
2
= n2 ∈N, so h1h2(g1g2)−1 = h1h2g−1
2 g−1
1
= n1g1n2g2g−1
2 g−1
1
=
n1g1n2g−1
1 . Now N is a normal subgroup, so g1n2g−1
1
∈N and n1g1n2g−1
1
∈N.
Hence h1h2 ≡g1g2 mod N and Nh1h2 = Ng1g2. Therefore, the operation is
well deﬁned.
The operation is associative because (Ng1 · Ng2) · Ng3 = N(g1g2) · Ng3 =
N(g1g2)g3
and
also
Ng1 · (Ng2 · Ng3) = Ng1 · N(g2g3) = Ng1(g2g3) =
N(g1g2)g3.
Since Ng · Ne = Nge = Ng and Ne · Ng = Ng, the identity is Ne = N. The
inverse of Ng is Ng−1 because Ng · Ng−1 = N(g · g−1) = Ne = N and also
Ng−1 · Ng = N.
Hence (G/N, ·) is a group.
□
The order of G/N is the number of cosets of N in G. Hence
|G/N| = |G : N| = |G|/|N|.

84
4
QUOTIENT GROUPS
TABLE 4.1. Quotient
Group S3/A3
Ž
H
H(12)
H
H
H(12)
H(12)
H(12)
H
We have seen in Example 4.15 that A3 is a normal subgroup of S3; therefore,
S3/A3 is a quotient group. If H = A3, the elements of this group are the cosets
H and H(12), and its multiplication table is given in Table 4.1.
Example 4.19. (Zn, +) is the quotient group of (Z, +) by the subgroup nZ =
{nz|z ∈Z}.
Solution. Since (Z, +) is abelian, every subgroup is normal. The set nZ can
be veriﬁed to be a subgroup, and the relationship a ≡b mod nZ is equivalent
to a −b ∈nZ and to n|a −b. Hence a ≡b mod nZ is the same relation as
a ≡b mod n. Therefore, Zn is the quotient group Z/nZ, where the operation on
congruence classes is deﬁned by [a] + [b] = [a + b].
□
(Zn, +) is a cyclic group with 1 as a generator, and therefore, by Theorem 3.25,
is isomorphic to Cn. The group (Z5, +) is shown in Table 4.2.
When there is no confusion, we write the elements of Zn as 0, 1, 2, 3, . . . ,
n −1 instead of [0], [1], [2], [3], . . . , [n −1].
Proposition 4.20. If H is a subgroup of index 2 in G, so that |G : H| = 2, then
H is a normal subgroup of G, and G/H is cyclic group of order 2.
Proof. Since |G : H| = 2, there are only two right cosets of H in G. One
must be H and the other can be written as Hg, where g is any element of G that
is not in H. To show that H is a normal subgroup of G, we need to show that
g−1hg ∈H for all g ∈G and h ∈H. If g is an element of H, it is clear that
g−1hg ∈H for all h ∈H. If g is not an element of H, suppose that g−1hg /∈H.
In this case, g−1hg must be an element of the other right coset Hg, and we
can write g−1hg = h1g, for some h1 ∈H. It follows that g = hh−1
1
∈H, which
contradicts the fact that g /∈H. Hence g−1hg ∈H for all g ∈G and h ∈H; in
other words, H is normal in G.
□
TABLE 4.2. Group (Z5, +)
+
[0]
[1]
[2]
[3]
[4]
[0]
[0]
[1]
[2]
[3]
[4]
[1]
[1]
[2]
[3]
[4]
[0]
[2]
[2]
[3]
[4]
[0]
[1]
[3]
[3]
[4]
[0]
[1]
[2]
[4]
[4]
[0]
[1]
[2]
[3]

NORMAL SUBGROUPS AND QUOTIENT GROUPS
85
Theorem 4.21. If G is a ﬁnite abelian group and the prime p divides the order
of G, then G contains an element of order p and hence a subgroup of order p.
Proof. We prove this result by induction on the order of G. For a particular
prime p, suppose that all abelian groups of order less than k, whose order is
divisible by p, contain an element of order p. The result is vacuously true for
groups of order 1. Now suppose that G is a group of order k. If p divides k,
choose any nonidentity element g ∈G. Let t be the order of the element g.
Case 1. If p divides t, say t = pr, then gr is an element of order p. This follows
because gr is not the identity, but (gr)p = gt = e, and p is a prime.
Case 2. On the other hand, if p does not divide t, let K be the subgroup generated
by g. Since G is abelian, K is normal, and the quotient group G/K has order
|G|/t, which is divisible by p. Therefore, by the induction hypothesis, G/K has
an element of order p, say Kh. If u is the order of h in G, then hu = e and
(Kh)u = Khu = K. Since Kh has order p in G/K, u is a multiple of p, and we
are back to case 1.
The result now follows from the principle of mathematical induction.
□
This result is a partial converse to Lagrange’s theorem. It is a special case of
some important results, in more advanced group theory, known as the Sylow the-
orems. These theorems give information on the subgroups of prime power order,
and they can be found in books such as Herstein [9], Hall [30], or Nicholson [11].
Example 4.22. Show that A5 has no proper normal subgroups.
Solution. It follows from Corollary 3.34 that A5 contains three types of non-
identity elements: 3-cycles, 5-cycles, and pairs of disjoint transpositions. Suppose
that N is a normal subgroup of A5 that contains more than one element.
Case 1. Suppose that N contains the 3-cycle (abc). From the deﬁnition of nor-
mal subgroup, g−1 Ž (abc) Ž g ∈N for all g ∈A5. If we take g = (ab) Ž (cd),
we obtain
(ab) Ž (cd) Ž (abc) Ž (ab) Ž (cd) = (adb) ∈N
and also (adb)−1 = (abd) ∈N. In a similar way, we can show that N contains
every 3-cycle. Therefore, by Proposition 3.37, N must be the entire alternat-
ing group.
Case 2. Suppose that N contains the 5-cycle (abcde). Then
and (abc)−1 Ž (abcde) Ž (abc) = (acb) Ž (abcde) Ž (abc) = (abdec) ∈N
(abcde) Ž (abdec)−1 = (abcde) Ž (acedb) = (adc) ∈N.
We are now back to case 1, and hence N = A5.

86
4
QUOTIENT GROUPS
Case 3. Suppose that N contains the pair of disjoint transpositions (ab) Ž (cd).
Then, if e is the element of {1, 2, 3, 4, 5} not appearing in these transpositions,
we have
(abe)−1 Ž (ab) Ž (cd) Ž (abe) = (aeb) Ž (ab) Ž (cd) Ž (abe) = (ae) Ž (cd) ∈N.
Also, (ab) Ž (cd) Ž (ae) Ž (cd) = (aeb) ∈N, and again we are back to case 1.
We have shown that any normal subgroup of A5 containing more than one
element must be A5 itself.
□
A group without any proper normal subgroups is called a simple group. The
term simple must be understood in the technical sense that it cannot be broken
down, because it cannot have any nontrivial quotient groups. This is analogous to
a prime number, which has no nontrivial quotients. Apart from the cyclic groups
of prime order, which have no proper subgroups of any kind, simple groups are
comparatively rare.
The group A5 is of great interest to mathematicians because it is used in Galois
theory to show that there is an equation of the ﬁfth degree that cannot be solved
by any algebraic formula.
It can be shown that every alternating group An, n ⩾5, is simple. The cyclic
groups Cp, p a prime, are another inﬁnite series of simple groups (the abelian
ones), and other series have been known for decades. But it was not until 1981
that the ﬁnite simple groups were completely classiﬁed. This was the culmina-
tion of more than 30 years of effort by hundreds of mathematicians, yielding
thousands of pages of published work, and was one of the great achievements of
twentieth-century mathematics. One spectacular landmark came in 1963, when
J. G. Thompson and W. Feit veriﬁed a long-standing conjecture of W. Burnside
(1852–1927) that every ﬁnite, non-abelian, simple group has even order (the
proof is more than 250 pages long!). The main difﬁculty in the classiﬁcation was
the existence of sporadic ﬁnite simple groups, not belonging to any of the known
families. The largest of these, known as the monster, has order approximately
2 × 1053. The complete classiﬁcation encompasses several inﬁnite families and
exactly 26 sporadic groups.
MORPHISM THEOREM
The morphism theorem is a basic result of group theory that describes the rela-
tionship between morphisms, normal subgroups, and quotient groups. There is an
analogous result for most algebraic systems, including rings and vector spaces.
If f : G →H is a group morphism, the kernel of f , denoted by Kerf , is
deﬁned to be the set of elements of G that are mapped by f to the identity of
H. That is, Kerf = {g ∈G|f (g) = eH}.
Proposition 4.23. Let f : G →H be a group morphism. Then:
(i) Kerf is a normal subgroup of G.
(ii) f is injective if and only if Kerf = {eG}.

MORPHISM THEOREM
87
Proof. (i) We ﬁrst show that Kerf is a subgroup of G. Let a, b ∈Kerf so
that f (a) = f (b) = eH. Then
f (ab) = f (a)f (b) = eHeH = eH,
so
ab ∈Kerf
and
f (a−1) = f (a)−1 = e−1
H = eH,
so
a−1 ∈Kerf.
Therefore, Kerf is a subgroup of G.
If a ∈Kerf and g ∈G, then
f (g−1ag) = f (g−1)f (a)f (g) = f (g)−1eHf (g) = f (g)−1f (g) = eH.
Hence g−1ag ∈Kerf , and Kerf is a normal subgroup of G.
(ii) If f is injective, only one element maps to the identity of H. Hence
Kerf = {eG}. Conversely, if Kerf = {eG}, suppose that f (g1) = f (g2). Then
f (g1g−1
2 ) = f (g1)f (g2)−1 = eH so g1g−1
2
∈Kerf = {eG}. Hence g1 = g2, and
f is injective.
□
Proposition 4.24. For any group morphism f : G →H, the image of f , Imf =
{f (g)|g ∈G}, is a subgroup of H (although not necessarily normal).
Proof. Let f (g1), f (g2) ∈Imf . Then eH = f (eG) ∈Imf, f (g1)f (g2) =
f (g1g2) ∈Imf , and f (g1)−1 = f (g−1
1 ) ∈Imf . Hence Imf is a subgroup of H.
□
Theorem 4.25. Morphism Theorem for Groups. Let K be the kernel of the
group morphism f : G →H. Then G/K is isomorphic to the image of f , and
the isomorphism ψ: G/K →Imf is deﬁned by ψ(Kg) = f (g).
This result is also known as the ﬁrst isomorphism theorem; the second and
third isomorphism theorems are given in Exercises 4.43 and 4.44.
Proof. The function ψ is deﬁned on a coset by using one particular element
in the coset, so we have to check that ψ is well deﬁned; that is, it does not matter
which element we use. If Kg = Kg′, then g′ ≡g mod K so g′g−1 = k ∈K =
Kerf . Hence g′ = kg and so
f (g′) = f (kg) = f (k)f (g) = eHf (g) = f (g).
Thus ψ is well deﬁned on cosets.
The function ψ is a morphism because
ψ(Kg1Kg2) = ψ(Kg1g2) = f (g1g2) = f (g1)f (g2) = ψ(Kg1)ψ(Kg2).
If ψ(Kg) = eH, then f (g) = eH and g ∈K. Hence the only element in the
kernel of ψ is the identity coset K, and ψ is injective. Finally, Im ψ = Imf ,

88
4
QUOTIENT GROUPS
by the deﬁnition of ψ. Therefore, ψ is the required isomorphism between G/K
and Imf .
□
Conversely, note that if K is any normal subgroup of G, the map g 
→Kg is
a morphism from G to G/K, whose kernel is precisely K.
By taking f to be the identity morphism from G to itself, the morphism
theorem implies that G/{e} ∼= G.
The function f : Z →Zn, deﬁned by f (x) = [x], has nZ as its kernel, and
therefore the morphism theorem yields the fact that Z/nZ ∼= Zn.
If a and b are generators of the cyclic groups C12 = ⟨a⟩and C6 = ⟨b⟩, respec-
tively, consider the function f : C12 →C6 given by f (ar) = b2r. This is well
deﬁned. In fact, if ar = ar1, then 12 divides r −r1 by Proposition 3.11, so cer-
tainly b2r = b2r1. It is easily veriﬁed that f is morphism, and the kernel is
K = {e, a3, a6, a9} because if ar is in K, then b2r = 0, so 6 divides 2r, whence
3 divides r. Thus C12/K ∼= C6, and this isomorphism is obtained by mapping
the coset Kar to b2r.
The alternating group An is of index 2 in the symmetric group Sn (by The-
orem 3.36) and so is a normal subgroup by Proposition 4.20. It is instructive
to obtain this same conclusion from the morphism theorem. If σ is a per-
mutation in Sn, recall that σ is called even or odd according as σD = D or
σD = −D, where D is the discriminant in n variables (see the discussion lead-
ing to Theorem 3.33). Consider the multiplicative group {1, −1}, and deﬁne a
function f : Sn →{1, −1} by f (σ) =
 1
if σD = D
−1
if σD = −D . Then f is a surjec-
tive morphism (verify) and the kernel is the group An of even permutations. Since
|Sn| = n!, the morphism theorem and Corollary 4.8 give the following result (and
reprove Theorem 3.36).
Proposition 4.26. An is a normal subgroup of Sn, Sn/An ∼= C2, and |An| = 1
2n!.
Example 4.27. Show that the quotient group R/Z, of real numbers modulo 1 is
isomorphic to the circle group W = {eiθ ∈C|θ ∈R}.
Solution. The set W consists of points on the circle of complex numbers
of unit modulus, and forms a group under multiplication. Deﬁne the function
f : R →W by f (x) = e2πix. This is a morphism from (R, +) to (W, ·) because
f (x + y) = e2πi(x+y) = e2πix · e2πiy = f (x) · f (y).
This function can be visualized in Figure 4.1 as wrapping the real line around
and around the circle.
The morphism f is clearly surjective, and its kernel is {x ∈R|e2πix = 1} = Z.
Therefore, the morphism theorem implies that R/Z ∼= W. The quotient space R/Z
is the set of equivalence classes of R under the relation deﬁned by x ≡y mod Z
if and only if the real numbers x and y differ by an integer. This quotient space
is called the group of real numbers modulo 1.
□

MORPHISM THEOREM
89
−2
−1
0
1
2
3
−1
−i
i
f
W
1
Figure 4.1.
Morphism f : R →W.
Proposition 4.28. If G and H are ﬁnite groups whose orders are relatively prime,
there is only one morphism from G to H, the trivial one.
Proof. Let K be the kernel of a morphism f from G to H. Then G/K ∼= Imf ,
a subgroup of H. Now |G/K| = |G|/|K|, which is a divisor of |G|. But by
Lagrange’s theorem, |Imf | is a divisor of |H|. Since |G| and |H| are relatively
prime, we must have |G/K| = |Imf | = 1. Therefore K = G, so f : G →H is
the trivial morphism deﬁned by f (g) = eH for all g ∈G.
□
Example 4.29. Find all the subgroups and quotient groups of D4, the symmetry
group of a square, and draw the poset diagram of its subgroups.
Solution. Any symmetry of the square induces a permutation of its vertices.
Thus, as in Example 3.30, this deﬁnes a group morphism f : D4 →S4. How-
ever, unlike the case of the symmetries of an equilateral triangle, this is not an
isomorphism because |D4| = 8, whereas |S4| = 24. The kernel of f consists of
symmetries ﬁxing the vertices and so consists of the identity only. Therefore, by
the morphism theorem, D4 is isomorphic to the image of f in S4. We equate
an element of D4 with its image in S4. All the elements of D4 are shown in
Figure 4.2. The corner by the vertex 1 is blocked in, and the reverse side of the
square is shaded to illustrate the effect of the symmetries. The order of each
symmetry is given in Table 4.3.
2
1
3
4
2
1
3
4
2
1
3
4
2
1
3
4
(1) = e
(1234) = g
(13)°(24) = g2
(1432) = g3
2
1
3
4
2
1
3
4
2
1
3
4
2
1
3
4
(24) = h
(12) ° (34) = gh
(13) = g2h
(14) ° (23) = g3h
Figure 4.2.
Symmetries of the square.

90
4
QUOTIENT GROUPS
TABLE 4.3. Orders of the Symmetries of a Square
Elements of D4
e
g
g2
g3
h
gh
g2h
g3h
Order of Element
1
4
2
4
2
2
2
2
e, g2h
e
e, h
e, g3h
e, gh
D4
C4
e, g2
= L
e, g2,h, g2h
K1 =
e, g2, gh, g3h = K2
Figure 4.3.
Poset diagram of subgroups of D4.
The cyclic subgroups generated by the elements are {e}, C4 = {e, g, g2, g3},
{e, g2}, {e, h}, {e, gh}, {e, g2h}, and {e, g3h}.
By Lagrange’s theorem, any proper subgroup must have order 2 or 4. Since
any group of order 2 is cyclic, the only proper subgroups that are not cyclic are
of order 4 and contain elements of order 1 and 2. There are two such subgroups,
K1 = {e, g2, h, g2h} and K2 = {e, g2, gh, g3h}. All the subgroups are illustrated
in Figure 4.3.
To ﬁnd all the quotient groups, we must determine which subgroups are
normal.
The trivial group {e} and the whole group D4 are normal subgroups. Since
C4, K1, and K2 have index 2 in D4, they are normal by Proposition 4.20, and
their quotient groups are cyclic of order 2.
Subgroup H
Left Coset gH
Right Coset Hg
{e, h}
{g, gh}
{g, hg} = {g, g3h}
{e, g2h}
{g, g3h}
{g, g2hg} = {g, gh}
{e, gh}
{g, g2h}
{g, ghg} = {g, h}
{e, g3h}
{g, h}
{g, g3hg} = {g, g2h}
For each of the subgroups above, the left and right cosets containing g are
different; therefore, none of the subgroups are normal.
Left Cosets of L
Right Cosets of L
L = {e, g2}
L = {e, g2}
gL = {g, g3}
Lg = {g, g3}
hL = {h, hg2} = {h, g2h}
Lh = {h, g2h}
ghL = {gh, ghg2} = {gh, g3h}
Lgh = {gh, g3h}
The table above shows that L = {e, g2} is a normal subgroup. The multipli-
cation table for D4/L given in Table 4.4 shows that it is isomorphic to the Klein
4-group.
□

DIRECT PRODUCTS
91
TABLE 4.4. Group D4/L
·
L
Lh
Lg
Lgh
L
L
Lh
Lg
Lgh
Lh
Lh
L
Lgh
Lg
Lg
Lg
Lgh
L
Lh
Lgh
Lgh
Lg
Lh
L
DIRECT PRODUCTS
Given two sets, S and T , we can form their Cartesian product, S × T = {(s, t)|s ∈
S, t ∈T }, whose elements are ordered pairs. For example, the product of the real
line, R, with itself is the plane, R × R = R2. We now show how to deﬁne the
product of any two groups; the underlying set of the product is the Cartesian
product of the underlying sets of the original groups.
Proposition 4.30. If (G, Ž ) and (H, ⋆) are two groups, then (G × H, ·) is a
group under the operation · deﬁned by
(g1, h1) · (g2, h2) = (g1 Ž g2, h1 ⋆h2).
The group (G × H, ·) is called the direct product of the groups (G, Ž ) and (H, ⋆).
Proof. All the group axioms follow from the axioms for (G, Ž ) and (H, ⋆).
The identity of G × H is (eG, eH), and the inverse of (g, h) is (g−1, h−1).
□
This construction can be iterated any ﬁnite number of times to obtain the
direct product of n groups.
Sometimes the direct product of two groups G and H is called the direct
sum and is denoted by G ⊕H. (The direct sum of a ﬁnite number of groups is
the same as the direct product. It is possible to deﬁne a direct sum and direct
product of an inﬁnite number of groups; these are different. An element of the
direct product is obtained by taking one element from each group, while an
element of the direct sum is obtained by taking one element from each group,
but with only a ﬁnite number different from the identity.)
Example 4.31. Write down the table for the direct product of C2 with itself.
Solution. Let C2 = {e, g}, so that C2 × C2 = {(e, e), (e, g), (g, e), (g, g)}. Its
table is given in Table 4.5. We see that this group C2 × C2 is isomorphic to the
Klein 4-group of symmetries of a rectangle.
□
Theorem 4.32. If gcd(m, n) = 1, then Cmn ∼= Cm × Cn.

92
4
QUOTIENT GROUPS
TABLE 4.5. Group C2 × C2
·
(e, e)
(e, g)
(g, e)
(g, g)
(e, e)
(e, e)
(e, g)
(g, e)
(g, g)
(e, g)
(e, g)
(e, e)
(g, g)
(g, e)
(g, e)
(g, e)
(g, g)
(e, e)
(e, g)
(g, g)
(g, g)
(g, e)
(e, g)
(e, e)
Proof. Let g, h, and k be the generators of Cmn, Cm, and Cn, respectively.
Deﬁne
f : Cmn →Cm × Cn
by f (gr) = (hr, kr) for r ∈Z. This is well deﬁned for all integers r because if
gr = gr′, then r −r′ is a multiple of mn, so r −r′ is a multiple of m and of n.
Hence hr = hr′ and kr = kr′. Now f is a group morphism because
f (gr · gs) = f (gr+s) = (hr+s, kr+s) = (hr · hs, kr · ks) = (hr, kr) · (hs, ks)
= f (gr) · f (gs).
If gr ∈Kerf , then hr = e and kr = e. Therefore, r is divisible by m and n, and
since gcd(m, n) = 1, r is divisible by mn. Hence Kerf = {e}, and the image of f
is isomorphic to Cmn. However, |Cmn| = mn and |Cm × Cn| = |Cm| · |Cn| = mn;
hence Imf = Cm × Cn, and f is an isomorphism.
□
The following is an easy consequence of this result.
Corollary 4.33. Let n = pα1
1 pα2
2 . . . pαr
r where p1, p2, . . . , pr are distinct primes.
Then Cn ∼= Cp
α1
1 × Cp
α2
2 × · · · × Cpαr
r .
□
If m and n are not coprime, then Cmn is never isomorphic to Cm × Cn. For
example, C2 × C2 is not isomorphic to C4 because the direct product contains
no element of order 4. In general, the order of the element (h, k) in H × K is
the least common multiple of the orders of h and k, because (h, k)r = (hr, kr) =
(e, e) if and only if hr = e and kr = e. Hence, if gcd(m, n) > 1, the order of
(h, k) in Cm × Cn is always less than mn.
Direct products can be used to classify all ﬁnite abelian groups. It can be
shown that any ﬁnite abelian group is isomorphic to a direct product of cyclic
groups. For example, see Nicholson [11] or Baumslag and Chandler [25]. The
results above can be used to sort out those products of cyclic groups that are
isomorphic to each other. For example, there are three nonisomorphic abelian
groups with 24 elements, namely,
C8 × C3 ∼= C24
C2 × C4 × C3 ∼= C6 × C4 ∼= C2 × C12
C2 × C2 × C2 × C3 ∼= C2 × C2 × C6.

DIRECT PRODUCTS
93
Theorem 4.34. If (G, ·) is a ﬁnite group for which every element g ∈G satisﬁes
g2 = e, then |G| = 2n for some n ⩾0, and G is isomorphic to the n-fold direct
product Cn
2 = C2 × C2 × · · · × C2.
Proof. Every element in G has order 1 or 2, and the identity is the only
element of order 1. Therefore, every element of G is its own inverse. The group
G is abelian because for any g, h ∈G, gh = (gh)−1 = h−1g−1 = hg.
Choose the elements a1, a2, . . . , an ∈G so that ai ̸= e and ai cannot be written
as a product of powers of a1, . . . , ai−1. Furthermore, choose n maximal, so that
every element can be written in terms of the elements ai. If C2 is generated by
g, we show that the function
f : Cn
2 →G, deﬁned by f (gr1, gr2, . . . , grn) = ar1
1 ar2
2 . . . arn
n
is an isomorphism. It is well deﬁned for all integers ri, because if gri = gqi, then
ari
i = aqi
i . Now
f ((gr1, . . . , grn) · (gs1, . . . , gsn)) = f (gr1+s1, . . . , grn+sn) = ar1+s1
1
. . . arn+sn
n
= ar1
1 . . . arn
n .as1
1 . . . asn
n
because G is abelian
= f (gr1, . . . , grn) · f (gs1, . . . , gsn).
Hence f is a group morphism.
Let (gr1, . . . , grn) ∈Kerf . Suppose that ri is the last odd exponent, so that
ri+1, ri+2, . . . , rn are all even. Then ar1...
1 ari−1
i−1ai = e and
ai = a−1
i
= ar1...
1 ari−1
i−1,
which is a contradiction. Therefore, all the exponents are even, and f is injective.
The choice of the elements ai guarantees that f is surjective. Hence f is the
required isomorphism.
□
Example 4.35. Describe all the group morphisms from C10 to C2 × C5. Which
of these are isomorphisms?
□
Solution. Since C10 is a cyclic group, generated by g, for example, a morphism
from C10 is determined by the image of g. Let h and k be generators of C2 and C5,
respectively, and consider the function fr,s: C10 →C2 × C5 which maps g to the
element (hr, ks) ∈C2 × C5. Then, if fr,s is a morphism, fr,s(gn) = (hrn, ksn) for
0 ⩽n⩽9. However, this would also be true for all integers n, because if gn = gm,
then 10|n −m. Hence 2|n −m and 5|n −m and hrn = hrm and ksn = ksm.
We now verify that fr,s is a morphism for any r and s. We have
fr,s(gagb) = fr,s(ga+b) = (h(a+b)r, k(a+b)s) = (har, kas)(hbr, kbs)
= fr,s(ga)fr,s(gb).

94
4
QUOTIENT GROUPS
Therefore, there are ten morphisms, fr,s, from C10 to C2 × C5 corresponding to
the ten elements (hr, ks) of C2 × C5.
Now
Kerfr,s = {gn|(hrn, ksn) = (e, e)} = {gn|rn ≡0 mod 2 and sn ≡0 mod 5}.
Hence Kerfr,s = {e} if (r, s) = (1, 1), (1, 2), (1, 3), or (1, 4), while Kerf0,0 =
C10, Kerf1,0 = {e, g2, g4, g6, g8}, and Kerf0,s = {e, g5}, if s = 1, 2, 3, or 4. If
Kerfr,s contains more than one element, fr,s is not an injection and cannot be an
isomorphism. By the morphism theorem,
|C10|/|Kerfr,s| = |Imfr,s|,
and if Kerfr,s = {e}, then | Im fr,s| = 10, so fr,s is surjective also. Therefore, the
isomorphisms are f1,1, f1,2, f1,3, and f1,4.
□
GROUPS OF LOW ORDER
We ﬁnd all possible isomorphism classes of groups with eight or fewer elements.
Lemma 4.36. Suppose that a and b are elements of coprime orders r and s,
respectively, in an abelian group. Then ab has order rs.
Proof. Let A and B denote the subgroups generated by a and b, respectively.
Since ab = ba, we have (ab)rs = arsbrs = (ar)s(bs)r = eser = e. Suppose that
(ab)k = e; we must show that rs divides k. Observe that ak = b−k ∈A ∩B.
Since A ∩B is a subgroup of both A and B, its order divides |A| = r and
|B| = s by Lagrange’s theorem. Since r and s are coprime, this implies that
|A ∩B| = 1. It follows that ak = e and b−k = e, so r divides k and s divides
k. Hence rs divides k by Theorem 11, Appendix 2 (again because r and s are
coprime), as required.
□
With this we can describe the groups of order eight or less.
Order 1. Every trivial group is isomorphic to {e}.
Order 2. By Corollary 4.11, every group of order 2 is cyclic.
Order 3. By Corollary 4.11, every group of order 3 is cyclic.
Order 4. Each element has order 1, 2, or 4.
Case (i). If there is an element of order 4, the group is cyclic.
Case (ii). If not, every element has order 1 or 2 and, by Theorem 4.34, the
group is isomorphic to C2 × C2.
Order 5. By Corollary 4.11, every group of order 5 is cyclic.
Order 6. Each element has order 1, 2, 3, or 6.
Case (i). If there is an element of order 6, the group is cyclic.

GROUPS OF LOW ORDER
95
Case (ii). If not, the elements have orders 1, 2, or 3. By Theorem 4.34, all
the elements in a group of order 6 cannot have orders 1 and 2. Hence
there is an element, say a, of order 3. The subgroup H = {e, a, a2} has
index 2, and if b /∈H, the underlying set of the group is then H ∪Hb =
{e, a, a2, b, ab, a2b}. By Proposition 4.20, H is normal, and the quotient
group of H is cyclic of order 2. Hence
br ∈Hbr = (Hb)r =
H
if r is even
Hb
if r is odd .
Therefore, b has even order. It cannot be 6, so it must be 2. As H
is normal, bab−1 ∈H. We cannot have bab−1 = e, because a ̸= e. If
bab−1 = a, then ba = ab, and we can prove that the entire group is
abelian. This cannot happen because by Lemma 4.36, ab would have
order 6. Therefore, bab−1 = a2, and the group is generated by a and b
with relations a3 = b2 = e and ba = a2b. This group is isomorphic to
D3 and S3.
Order 7. Every group of order 7 is cyclic.
Order 8. Each element has order 1, 2, 4, or 8.
Case (i). If there is an element of order 8, the group is cyclic.
Case (ii). If all elements have order 1 or 2, the group is isomorphic to
C2 × C2 × C2 by Theorem 4.34.
Case (iii). Otherwise, there is an element of order 4, say a. The subgroup
H = {e, a, a2, a3} is of index 2 and therefore normal. If b /∈H, the
underlying set of the group is H ∪Hb = {e, a, a2, a3, b, ab, a2b, a3b}.
Now b2 ∈H, but b2 cannot have order 4; otherwise, b would have
order 8. Therefore, b2 = e or a2. As H is normal, bab−1 ∈H and has
the same order as a because (bab−1)k = bakb−1.
Case (iiia). If bab−1 = a, then ba = ab, and the whole group can be proved
to be abelian. If b2 = e, each element can be written uniquely in the form
arbs, where 0 ⩽r ⩽3 and 0 ⩽s ⩽1. Hence the group is isomorphic
to C4 × C2 by mapping arbs to (ar, bs). If b2 = a2, let c = ab, so that
c2 = a2b2 = a4 = e. Each element of the group can now be written
uniquely in the form arcs, where 0 ⩽r ⩽3, 0 ⩽s ⩽1, and the group
is still isomorphic to C4 × C2.
Case (iiib). If bab−1 = a3 and b2 = e, the group is generated by a and
b with the relations a4 = b2 = e, ba = a3b. This is isomorphic to the
dihedral group D4.
Case (iiic). If bab−1 = a3 and b2 = a2, then the group is isomorphic to the
quaternion group Q, described in Exercise 3.47. The isomorphism maps
arbs to irj s.
Any group with eight or fewer elements is isomorphic to exactly one group
in Table 4.6.

96
4
QUOTIENT GROUPS
TABLE 4.6. Groups of Low Order
Order
1
2
3
4
5
6
7
8
Abelian groups
{e}
C2
C3
C4
C5
C6
C7
C8
C2 × C2
C4 × C2
C2 × C2 × C2
Non-abelian groups
S3
D4
Q
ACTION OF A GROUP ON A SET
The concept of a group acting on a set X is a slight generalization of the group
of symmetries of X. It is equivalent to considering a subgroup of S(X). This
concept is useful for determining the order of the symmetry groups of solids
in three dimensions, and it is indispensable in Chapter 6, when we look at the
P´olya–Burnside method of enumerating sets with symmetries.
The group (G, ·) acts on the set X if there is a function
ψ: G × X →X
such that when we write g(x) for ψ(g, x), we have:
(i) (g1g2)(x) = g1(g2(x)) for all g1, g2 ∈G, x ∈X.
(ii) e(x) = x if e is the identity of G and x ∈X.
Proposition 4.37. If g is an element of a group G acting on the set X, then
the function g: X →X, which maps x to g(x), is a bijection. This deﬁnes
a morphism
χ: G →S(X)
from G to the group of symmetries of X.
Proof. The function g: X →X is injective because if g(x) = g(y), then
g−1g(x) = g−1g(y), and e(x) = e(y) or x = y. It is surjective because if z ∈X,
g(g−1(z)) = gg−1(z) = e(z) = z.
Hence g is bijective, and g can be considered as an element of S(X), the group
of symmetries of X.
The function χ: G →S(X), which takes the element g ∈G to the bijection
g: X →X, is a group morphism because χ(g1g2) is the function from X
to X deﬁned by χ(g1g2)(x) = (g1g2)(x) = (g1(g2(x)) = χ(g1) Ž χ(g2)(x); thus
χ(g1g2) = χ(g1) Ž χ(g2).
□
If Kerχ = {e}, then χ is injective, and the group G is said to act faithfully
on the set X. G acts faithfully on X if the only element of G, which ﬁxes every

ACTION OF A GROUP ON A SET
97
1
2
3
6
h
4
5
Figure 4.4.
C2 acting on a hexagon.
8
3
4
1
2
7
6
g
5
Figure 4.5.
C3 acting on a cube.
element of X, is the identity e ∈G. In this case, we identify G with Imχ and
regard G as a subgroup of S(X).
For example, consider the cyclic group of order 2, C2 = {e, h}, acting on
the regular hexagon in Figure 4.4, where h reﬂects the hexagon about the line
joining vertex 3 to vertex 6. Then C2 acts faithfully and can be identiﬁed with
the subgroup {(1), (15) Ž (24)} of D6.
The cyclic group C3 = {e, g, g2} acts faithfully on the cube in Figure 4.5,
where g rotates the cube through one-third of a revolution about a line join-
ing two opposite vertices. This group action can be considered as the subgroup
{(1), (163) Ž (457), (136) Ž (475)} of the symmetry group of the cube.
Proposition 4.38. If G acts on a set X and x ∈X, then
Stab x = {g ∈G|g(x) = x}
is a subgroup of G, called the stabilizer of x. It is the set of elements of G that
ﬁx x.
Proof. Stab x is a subgroup because
(i) If g1, g2 ∈Stab x, then (g1g2)(x) = g1(g2(x)) = g1(x) = x, so g1g2 ∈
Stab x.
(ii) If g ∈Stab x, then g−1(x) = x, so g−1 ∈Stab x.
□
The set of all images of an element x ∈X under the action of a group G is
called the orbit of x under G and is denoted by
Orb x = {g(x)|g ∈G}.
The orbit of x is the equivalence class of x under the equivalence relation on X
in which x is equivalent to y if and only if y = g(x) for some g ∈G. If π is
a permutation in Sn, the subgroup generated by π acts on the set {1, 2, . . . , n},
and this deﬁnition of orbit agrees with our previous one.
A graphic illustration of orbits can be obtained by looking at the group
of matrices
SO(2) =

cos θ
−sin θ
sin θ
cos θ
 θ ∈R
	

98
4
QUOTIENT GROUPS
under matrix multiplication. This group is called the special orthogonal group
and is isomorphic to the circle group W. SO(2) acts on R2 as follows. The matrix
M ∈SO(2) takes the vector x ∈R2 to the vector Mx. The orbit of any element
x ∈R2 is the circle through x with center at the origin. Since the origin is the
only ﬁxed point for any of the nonidentity transformations, the stabilizer of the
origin is the whole group, whereas the stabilizer of any other element is the
subgroup consisting of the identity matrix only.
The orbits of the cyclic group C2 acting on the hexagon in Figure 4.4 are
{1, 5}, {2, 4}, {3}, and {6}.
There is an important connection between the number of elements in the orbit
of a point x and the stabilizer of that point.
Lemma 4.39. If G acts on X, then for each x ∈X
|G: Stab x| = |Orb x|.
Proof. Let H = Stab x and deﬁne the function
ξ: G/H →Orb x
by ξ(Hg) = g−1(x). This is well deﬁned on cosets because if Hg = Hk, then
k = hg for some h ∈H, so k−1(x) = (hg)−1(x) = g−1h−1(x) = g−1(x), since
h−1 ∈H = Stab x.
The function ξ is surjective by the deﬁnition of the orbit of x. It is also injec-
tive, because ξ(Hg1) = ξ(Hg2) implies that g−1
1 (x) = g−1
2 (x), so g2g−1
1 (x) = x
and g2g−1
1
∈Stab x = H. Therefore, ξ is a bijection, and the result follows. □
Note that ξ is not a morphism. G/Stab x is just a set of cosets because Stab
x is not necessarily normal. Furthermore, we have placed no group structure on
Orb x.
Theorem 4.40. If the ﬁnite group G acts on a set X, then for each x ∈X,
|G| = |Stab x||Orb x|.
Proof. This follows from Lemma 4.39 and Corollary 4.8.
□
Example 4.41. Find the number of proper rotations of a cube.
Solution. Let G be the group of proper rotations of a cube; that is, rotations
that can be carried out in three dimensions. The stabilizer of the vertex 1 in
Figure 4.6 is Stab 1 = {(1), (245) Ž (386), (254) Ž (368)}. The orbit of 1 is the set
of all the vertices, because there is an element of G that will take 1 to any other
vertex. Therefore, by Theorem 4.40,
|G| = |Stab 1| |Orb 1| = 3 · 8 = 24.
□

ACTION OF A GROUP ON A SET
99
8
3
4
7
1
2
6
5
Figure 4.6.
Cube.
Figure 4.7.
Reﬂection in a plane.
The full symmetry group of the cube would include improper rotations such as
the reﬂection in the plane as shown in Figure 4.7. This induces the permutation
(24) Ž (68) on the vertices, and it cannot be obtained by physically rotating the
cube in three dimensions. Under this group
Stab 1 = {(1), (245) Ž (368), (254) Ž (386), (24) Ž (68), (25) Ž (38), (45) Ž (36)},
so the order of the full symmetry group of the cube is
|Stab 1||Orb 1| = 6.8 = 48.
Therefore, there are 24 proper and 24 improper rotations of the cube.
The article by Shapiro [32] contains many applications, mainly to group the-
ory, of the actions of a group on a set.
We conclude by mentioning the action of the symmetric group Sn on the set
of polynomials in n variables x1, x2, . . . , xn. A permutation σ ∈Sn acts on a
polynomial f = f (x1, x2, . . . , xn) by permuting the variables:
σ(f ) = f (xσ1, xσ2, . . . , xσn).
This was the action we used in Chapter 3 to deﬁne the parity of a permutation
and prove the parity theorem. It has historical interest as well. It is the context in
which Lagrange proved Lagrange’s theorem—in essence, what he actually did
is prove Theorem 4.40 for this action. Moreover, this is the group action that
launched Galois theory, about which we say more in Chapter 11.

100
4
QUOTIENT GROUPS
EXERCISES
In Exercises 4.1 to 4.4, which of the relations are equivalence relations? Describe
the equivalence classes of those relations which are equivalence relations.
4.1. The relation ∼on P × P deﬁned by (a, b) ∼(c, d) if and only if a + d =
b + c.
4.2. The relation T on the set of continuous functions from R to R, where fTg
if and only if f (3) = g(3).
4.3. The inclusion relation on the power set P(X).
4.4. The relation C on a group G, where aCb if and only if ab = ba.
4.5. Find the left and right cosets of H = {(1), (12), (34), (12) Ž (34)} in S4.
4.6. Let H be the subgroup of A4 that ﬁxes 1. Find the left and right cosets of
H in A4. Is H normal? Describe the left cosets in terms of their effect on
the element 1. Can you ﬁnd a similar description for the right cosets?
In Exercises 4.7 to 4.12, verify that each of the functions is well deﬁned. Determine
which are group morphisms, and ﬁnd the kernels and images of all the morphisms.
The element of Zn containing x is denoted by [x]n.
4.7. f : Z12 →Z12, where f ([x]12) = [x + 1]12.
4.8. f : C12 →C12, where f (g) = g3.
4.9. f : Z →Z2 × Z4, where f (x) = ([x]2, [x]4).
4.10. f : Z8 →Z2, where, f ([x]8) = [x]2.
4.11. f : C2 × C3 →C3, where f (hr, ks) = (12)r Ž (123)s.
4.12. f : Sn →Sn+1, where f (π) is the permutation on {1, 2, . . . , n + 1} deﬁned
by f (π)(i) = π(i) if i ⩽n, and f (π)(n + 1) = n + 1.
4.13. If H is a subgroup of an abelian group G, prove that the quotient group
G/H is abelian.
4.14. If H is a subgroup of G, show that g−1Hg = {g−1hg|h ∈H} is a subgroup
for each g ∈G.
4.15. Prove that the subgroup H is normal in G if and only if g−1Hg = H for
all g ∈G.
4.16. If H is the only subgroup of a given order in a group G, prove that H is
normal in G.
4.17. Let H be any subgroup of a group G. Prove that there is a one-to-one
correspondence between the set of left cosets of H in G and the set of
right cosets of H in G.
4.18. Is the cyclic subgroup {(1), (123), (132)} normal in S4?
4.19. Is the cyclic subgroup {(1), (123), (132)} normal in A4?
4.20. Is {(1), (1234), (13) Ž (24), (1432), (13), (24), (14) Ž (23), (12) Ž (34)} nor-
mal in S4?
4.21. Find all the group morphisms from C3 to C4.
4.22. Find all the group morphisms from Z to Z4.

EXERCISES
101
4.23. Find all the group morphisms from C6 to C6.
4.24. Find all the group morphisms from Z to D4.
In Exercises 4.25 to 4.29, which of the pairs of groups are isomorphic? Give
reasons.
4.25. C60 and C10 × C6.
4.26. (P{a, b, c}, 	) and C2 × C2 × C2.
4.27. Dn and Cn × C2.
4.28. D6 and A4.
4.29. Z4 × Z2 and ({±1, ±i, ±(1 + i)/
√
2, ±(1 −i)/
√
2}, ·).
4.30. If G × H is cyclic, prove that G and H are cyclic.
4.31. If π is an r-cycle in Sn, prove that ρ−1 Ž π Ž ρ is also an r-cycle for each
ρ ∈Sn.
4.32. Find four different subgroups of S4 that are isomorphic to S3.
4.33. Find all the isomorphism classes of groups of order 10.
4.34. Find all the ten subgroups of A4 and draw the poset diagram under inclu-
sion. Which of the subgroups are normal?
4.35. For any groups G and H, prove that (G × H)/G′ ∼= H and (G × H)/H ′ ∼=
G, where G′ = {(g, e) ∈G × H|g ∈G} and H ′ = {(e, h) ∈G × H|h ∈
H}.
4.36. Show that Q/Z is an inﬁnite group but that every element has ﬁnite order.
4.37. If G is a subgroup of Sn and G contains an odd permutation, prove that
G contains a normal subgroup of index 2.
4.38. In any group (G, ·) the element a−1b−1ab is called the commutator of
a and b. Let G′ be the subset of G consisting of all ﬁnite products of
commutators. Show that G′ is a normal subgroup of G. This is called the
commutator subgroup. Also prove that G/G′ is abelian.
4.39. Let C∗be the group of nonzero complex numbers under multiplication and
let W be the multiplicative group of complex numbers of unit modulus.
Describe C∗/W.
4.40. Show that K = {(1), (12) Ž (34), (13) Ž (24), (14) Ž (23)} is a subgroup of S4
isomorphic to the Klein 4-group. Prove that K is normal and that S4/K ∼=
S3.
4.41. If K is the group given in Exercise 4.40, prove that K is normal in A4 and
that A4/K ∼= C3. This shows that A4 is not a simple group.
4.42. The cross-ratio of the four distinct real numbers x1, x2, x3, x4 in that order
is the ratio λ = (x2 −x4)(x3 −x1)/(x2 −x1)(x3 −x4). Find the subgroup
K of S4, of all those permutations of the four numbers that preserve the
value of the cross-ratio. Show that if λ is the cross-ratio of four numbers
taken in a certain order, the cross-ratio of these numbers in any other order
must belong to the set

λ, 1 −λ, 1
λ, 1 −1
λ,
1
1 −λ,
λ
λ −1
	
.

102
4
QUOTIENT GROUPS
Furthermore, show that all permutations in the same coset of K in S4 give
rise to the same cross-ratio. In other words, prove that the quotient group
S4/K is isomorphic to the group of functions given in Exercise 3.42. The
cross-ratio is very useful in projective geometry because it is preserved
under projective transformations.
4.43. (Second Isomorphism Theorem) Let N be a normal subgroup of G, and
let H be any subgroup of G. Show that HN = {hn|h ∈H, n ∈N} is a
subgroup of G and that H ∩N is a normal subgroup of H. Also prove that
H/(H ∩N) ∼= HN/N.
4.44. (Third isomorphism theorem) Let M and N be normal subgroups of
G, and N be a normal subgroup of M. Show that φ: G/N →G/M is a
well-deﬁned morphism if φ(Ng) = Mg, and prove that
(G/N)/(M/N) ∼= G/M.
4.45. If a ﬁnite group contains no nontrivial subgroups, prove that it is either
trivial or cyclic of prime order.
4.46. If d is a divisor of the order of a ﬁnite cyclic group G, prove that G
contains a subgroup of order d.
4.47. If G is a ﬁnite abelian group and p is a prime such that gp = e, for all
g ∈G, prove that G is isomorphic to Zn
p, for some integer n.
4.48. What is the symmetry group of a rectangular box with sides of length 2,
3, and 4 cm?
4.49. Let
Gp =

a
b
c
d

∈M2(Zp)|ad −bc = 1 in Zp
	
.
If p is prime, show that (Gp, ·) is a group of order p(p2 −1), and ﬁnd a
group isomorphic to G2.
4.50. Show that (R∗, ·) acts on Rn+1 by scalar multiplication. What are the
orbits under this action? The set of orbits, excluding the origin, form the
n-dimensional real projective space.
4.51. Let G be a group of order n, and let gcd(n, m) = 1. Show that every
element h in G has an mth root; that is, h = gm for some g ∈G.
4.52. Let G′ denote the commutator subgroup of a group G (see Exercise 4.38).
If K is a subgroup of G, show that G′ ⊆H if and only if K is normal in
G and G/K is abelian.
4.53. Call a group G metabelian if it has a normal subgroup K such that both
K and G/K are abelian.
(a) Show that every subgroup and factor group of a metabelian group is
metabelian. (Exercises 4.43 and 4.44 are useful.)
(b) Show that G is metabelian if and only if the commutator group G′ is
abelian (see Exercise 4.38).

EXERCISES
103
4.54. Recall (Exercise 3.76) that the center Z(G) of a group G is deﬁned by
Z(G) = {z ∈G|zg = gz for all g ∈G}. Let K ⊆Z(G) be a subgroup.
(a) Show that K is normal in G.
(b) If G/K is cyclic, show that G is abelian.
For Exercises 4.55 to 4.61, let Zm∗= {[x] ∈Zm|gcd(x, m) = 1}. The number of
elements in this set is denoted by φ(m) and is called the Euler φ-function. For
example, φ(4) = 2, φ(6) = 2, and φ(8) = 4.
4.55. Show that φ(pr) = pr −pr−1 if p is a prime.
4.56. Show that φ(mn) = φ(m)φ(n) if gcd(m, n) = 1.
4.57. Prove that (Zm∗, ·) is an abelian group.
4.58. Write out the multiplication table for (Z8∗, ·).
4.59. Prove that (Z6∗, ·) and (Z17∗, ·) are cyclic and ﬁnd generators.
4.60. Find groups in Table 4.6 that are isomorphic to (Z8∗, ·), (Z9∗, ·), (Z10∗, ·),
and (Z15∗, ·) and describe the isomorphisms.
4.61. Prove that if gcd(a, m) = 1, then aφ(m) ≡1 mod m. [This result was known
to Leonhard Euler (1707–1783).]
4.62. Prove that if p is a prime, then for any integer a, ap ≡a modp. [This
result was known to Pierre de Fermat (1601–1665).]
4.63. If G is a group of order 35 acting on a set with 13 elements, show that G
must have a ﬁxed point, that is, a point x ∈S such that g(x) = x for all
g ∈G.
4.64. If G is a group of order pr acting on a set with m elements, show that G
has a ﬁxed point if p does not divide m.

5
SYMMETRY GROUPS
IN THREE DIMENSIONS
In this chapter we determine the symmetry groups that can be realized in two-
and three-dimensional space. We rely heavily on geometric intuition, not only to
simplify arguments but also to give geometric ﬂavor to the group theory. Because
we live in a three-dimensional world, these symmetry groups play a crucial role
in the application of modern algebra to physics and chemistry.
We ﬁrst show how the group of isometries of Rn can be broken down into
translations and orthogonal transformations ﬁxing the origin. Since the orthog-
onal transformations can be represented as a group of matrices, we look at the
properties of matrix groups. We then use these matrix groups to determine all the
ﬁnite rotation groups in two and three dimensions, and we ﬁnd polyhedra that
realize these symmetry groups.
TRANSLATIONS AND THE EUCLIDEAN GROUP
Euclidean geometry in n dimensions is concerned with those properties that are
preserved under isometries (rigid motions) of euclidean n-space, that is, bijections
α: Rn →Rn that preserve distance. The group of all isometries of Rn is called the
euclidean group in n dimensions and is denoted E(n). Given w ∈Rn, the map
Rn →Rn with v →v + w is called translation by w, and we begin by showing
that the group T (n) of all translations is a normal subgroup of E(n), and that
the factor group is isomorphic to the group of all orthogonal n × n matrices (that
is, matrices A such that A−1 = AT , the transpose of A—reﬂection of A in its
main diagonal).
Recall that a function λ: Rn →Rn is called a linear transformation if λ(av +
bw) = aλ(v) + bλ(w) for all a, b ∈R and all (column) vectors v, w ∈Rn. Let
{e1, e2, . . . , en} denote the standard basis of Rn, that is, the columns of the
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
104

TRANSLATIONS AND THE EUCLIDEAN GROUP
105
n × n identity matrix. Then the action of λ is matrix multiplication λ(v) =
Av for all v in Rn, where the matrix A is given in terms of its columns by
A = [λ(e1)λ(e2) · · · λ(en)] and is called the standard matrix of α. Moreover,
the correspondence λ ↔A is a bijection that preserves addition, multiplica-
tion, and the identity. So we may (and sometimes shall) identify λ with the
matrix A.
If v and w are vectors in Rn, let vžw = vT w denote their inner product. Then
||v|| = √vžv is the length of v, and ||v −w|| is the distance between v and w.
Thus a function α: Rn →Rn is an isometry if
||α(v) −α(w)|| = ||v −w||
for all v, w ∈Rn.
(∗)
Since ||v −w||2 = ||v||2 + 2(vžw) + ||w||2 for any v, w ∈Rn, it follows from (∗)
that every isometry α preserves inner products in the sense that
α(v)žα(w) = vžw
for all v, w ∈Rn.
(∗∗)
Lemma 5.1. If α: Rn →Rn is an isometry such that α(0) = 0, then α is linear.
Proof. It follows from (∗∗) that {α(e1), α(e2), . . . , α(en)} is an orthonormal
basis of Rn. If a ∈R and v ∈Rn, then (∗∗) implies that
[α(av) −aα(v)]žα(ei) = (av)žei −a(vžei) = 0
for each i.
Hence α(av) = aα(v), and α(v + w) = α(v) + α(w) follows in the same way
for all v, w ∈Rn.
□
Hence the isometries of Rn that ﬁx the origin are precisely the linear isometries.
An n × n matrix A is called orthogonal if it is invertible and A−1 = AT ,
equivalently if the columns of A are an orthonormal basis of Rn. These matrices
form a subgroup of the group of all invertible matrices, called the orthogonal
group and denoted O(n).
Proposition 5.2. Let λ: Rn →Rn be a linear transformation with standard
matrix A.
(i) λ is an isometry if and only if A is an orthogonal matrix.
(ii) The group of linear isometries of Rn is isomorphic to O(n).
Proof. If A is orthogonal, then for all v, w ∈Rn,
||λ(v) −λ(w)||2 = A(v −w)žA(v −w) = (v −w)T AT A(v −w) = ||v −w||2
and it follows that λ is an isometry. Conversely, if λ is an isometry and
{e1, e2, . . . , en} is the standard basis of Rn, then (∗∗) gives
eižej = λ(ei)žλ(ej) = AeižAej = eT
i (AT A)ej = the (i, j)-entry of A

106
5
SYMMETRY GROUPS IN THREE DIMENSIONS
for all i and j. It follows that AT A = I, so A is orthogonal, proving (i). But then
the correspondence λ ↔A between the linear transformation λ and its standard
matrix A induces a group isomorphism between the (linear) isometries ﬁxing the
origin and the orthogonal matrices. This proves (ii).
□
Given a vector w ∈Rn, deﬁne τw: Rn →Rn by τw(v) = v −w for all v ∈Rn.
Thus τw is the unique translation that carries w to 0. Because
τw Ž τw′ = τw+w′
for all w, w′ ∈Rn,
the correspondence w ↔τw is a group isomorphism (Rn, +) ∼= T (n). In partic-
ular, T (n) is an abelian group.
Theorem 5.3. For each n ⩾1, T (n) is an abelian normal subgroup of E(n) and
E(n)/T (n) ∼= O(n). In fact, the map E(n) →O(n) given by
α →the standard matrix of τα(0) Ž α
is a surjective group morphism E(n) →O(n) with kernel T (n).
Proof. Write G(n) for the group of all linear isometries of Rn. Observe that
if α ∈E(n), then τα(0) Ž α is linear (it is an isometry that ﬁxes 0), so we have
a map
φ: E(n) →G(n)
given by
φ(α) = τα(0) Ž α
for all α ∈E(n).
By Proposition 5.2 it sufﬁces to show that φ is a surjective group morphism with
kernel T (n). To see that φ is a group morphism, observe ﬁrst that
α Ž τw = τα(w) Ž α
for all w ∈Rn.
(∗∗∗)
[Indeed,
(α Ž τw)(v) = α(v −w) = α(v) −α(w) = (τα(w) Ž α)(v)
for
all
v.]
Hence, given α and β in E(n), we have
φ(α) Ž φ(β) = τα(0) Ž α Ž τβ(0) Ž β = τα(0) Ž (α Ž τβ(0) Ž α−1) Ž (α Ž β),
so it sufﬁces to show that τα(0) Ž (α Ž τβ(0) Ž α−1) = τ(α Ž β)(0). But this follows
because α Ž τβ(0) Ž α−1 is a translation by (∗∗∗), so τα(0) Ž (α Ž τβ(0) Ž α−1) is the
unique translation that carries (α Ž β)(0) to 0. Hence φ is a group morphism.
Moreover, φ is surjective because φ(λ) = λ for every λ ∈G(n), and Ker(φ) =
T (n) because φ(α) = 1Rn if and only if α(v) = v + α(0) for all v ∈Rn, that is,
if and only if α = τ−α(0).
□
If α ∈E(n) and α(0) = w, the proof of Theorem 5.3 shows that τw Ž α ∈G(n).
Hence every isometry α of Rn is the composition of a linear isometry τ −1
w
Ž α
followed by a translation τw.

MATRIX GROUPS
107
Proposition 5.4. Every ﬁnite subgroup of isometries of n-dimensional space
ﬁxes at least one point.
Proof. Let G be a ﬁnite subgroup of isometries, and let x be any point of
n-dimensional space. The orbit of x consists of a ﬁnite number of points that are
permuted among themselves by any element of G. Since all the elements of G
are rigid motions, the centroid of Orb x must always be sent to itself. Therefore,
the centroid is a ﬁxed point under G.
□
If the ﬁxed point of any ﬁnite subgroup G of isometries is taken as the origin,
then G is a subgroup of O(n), and all its elements can be written as orthogonal
matrices. We now look at the structure of groups whose elements can be written
as matrices.
MATRIX GROUPS
In physical sciences and in mathematical theory, we frequently encounter mul-
tiplicative group structures whose elements are n × n complex matrices. Such a
group is called a matrix group if its identity element is the n × n identity matrix
I. To investigate these groups, we have at our disposal, and shall freely apply,
the machinery of linear algebra.
For example, if
Ak =

cos(2πk/m)
−sin(2πk/m)
sin(2πk/m)
cos(2πk/m)

,
then ({A0, A1, . . . , Am−1}, ·) is a real matrix group of order m isomorphic to
Cm. The matrix Ak represents a counterclockwise rotation of the plane about the
origin through an angle (2πk/m).
The matrices

1
0
0
1

,

−1
0
0
−1

,

−i
0
0
i

,

i
0
0
−i

,

0
1
−1
0

,

0
−1
1
0

,

0
−i
−i
0

,
and

0
i
i
0

form a group under matrix multiplication. This is a complex matrix group of
order 8 that is, in fact, isomorphic to the quaternion group Q of Exercise 3.47.
Since the identity of any matrix group is the identity matrix I and every
element of a matrix group must have an inverse, every element must be a non-
singular matrix. All the nonsingular n × n matrices over a ﬁeld F form a group
(GL(n, F), ·) called the general linear group of dimension n over F. Any
matrix group over the ﬁeld F must be a subgroup of GL(n, F).
Proposition 5.5. The determinant of any element of a ﬁnite matrix group must
be an integral root of unity.

108
5
SYMMETRY GROUPS IN THREE DIMENSIONS
Proof. Let A be an element of a matrix group of order m. Then, by Corol-
lary 4.10, Am = I. Hence (detA)m = detAm = detI = 1.
□
Hence, if G is a real matrix group, the determinant of any element of G is
either +1 or −1. If G is a complex matrix group, the determinant of any element
is of the form e2πik/m.
The orthogonal group O(n) is a real matrix group, and therefore any element
must have determinant +1 or −1. The determinant function
det: O(n) →{1, −1}
is a group morphism from (O(n), ·) to ({1, −1}, ·). The kernel, consisting of
orthogonal matrices with determinant +1, is called the special orthogonal group
of dimension n and is denoted by
SO(n) = {A ∈O(n)|detA = +1}.
This is a normal subgroup of O(n) of index 2. The elements of SO(n) are
called proper rotations, whereas the elements in the other coset of O(n)
by SO(n), consisting of orthogonal matrices with determinant −1, are called
improper rotations.
An n × n complex matrix A is called unitary if it is invertible and A−1 is
the conjugate transpose of A. Thus the real unitary matrices are precisely the
orthogonal matrices. Indeed, if ⟨x, y⟩= xT y denotes the inner product in Cn,
the matrix A is unitary if and only if it preserves inner products in Cn (that
is, ⟨Ax, Ay⟩= ⟨x, y⟩for all x, y ∈Cn), if and only if the columns of A are
orthonormal.
The unitary group of dimension n, U(n), consists of all n × n complex uni-
tary matrices under multiplication. The special unitary group, SU(n), is the
subgroup of U(n) consisting of those matrices with determinant +1. The group
SU(3) received some publicity in 1964 when the Brookhaven National Labo-
ratory discovered the fundamental particle called the omega-minus baryon. The
existence and properties of this particle had been predicted by a theory that used
SU(3) as a symmetry group of elementary particles.
Proposition 5.6. If λ ∈C is an eigenvalue of any unitary matrix, then |λ| = 1.
Proof. Let λ be an eigenvalue and x a corresponding nonzero eigenvector of
the unitary matrix A. Then Ax = λx, and since A preserves distances, ||x|| =
||Ax|| = ||λx|| = |λ|||x||. Since x is nonzero, it follows that |λ| = 1.
□
The group {Ak|k = 0, 1, . . . , m −1} of rotations of the plane is a subgroup of
SO(2), and the eigenvalues that occur are e±(2πik/m). The matrix group isomorphic
to the quaternion group Q is a subgroup of SU(2), and the eigenvalues that occur
are ±1 and ±i.

FINITE GROUPS IN TWO DIMENSIONS
109
Cayley’s theorem (Theorem 3.38) showed that any group could be represented
by a group of permutations. Another way to represent groups is by means of
matrices. A matrix representation of a group G is a group morphism φ: G →
GL(n, F). This is equivalent to an action of G on an n-dimensional vector
space over the ﬁeld F, by means of linear transformations. The representation
is called faithful if the kernel of φ is the identity. In this case, φ is injective
and G is isomorphic to Imφ, a subgroup of the general linear group. Matrix
representations provide powerful tools for studying groups because they lend
themselves readily to calculation. As a result, most physical applications of group
theory use representations.
It is possible to prove that any representation of a ﬁnite group over the real or
complex ﬁeld may be changed by a similarity transformation into a representation
that uses only orthogonal or unitary matrices, respectively. Therefore, a real or
complex faithful representation allows us to view a group as a subgroup of O(n)
or U(n), respectively.
FINITE GROUPS IN TWO DIMENSIONS
We determine all the ﬁnite subgroups of rotations (proper and improper) of the
plane R2. That is, we ﬁnd all the ﬁnite matrix subgroups of SO(2) and O(2).
This was essentially done by Leonardo da Vinci when he determined the possible
symmetries of a central building with chapels attached. See Field and Golubit-
sky [29], where they construct interesting symmetric patterns in the plane using
chaotic maps.
Proposition 5.7
(i) The set of proper rotations in two dimensions is
SO(2) =

cos θ
−sin θ
sin θ
cos θ
 θ ∈R

.
(ii) The set of improper rotations in two dimensions is

cos θ
sin θ
sin θ
−cos θ
 θ ∈R

.
(iii) The eigenvalues of the proper rotation

cos θ
−sin θ
sin θ
cos θ

are e±iθ and
those of any improper rotation are ±1.
Proof. (i) Let A =

p
q
r
s

∈SO(2), so that A

1
0

=

p
r

and A

0
1

=

q
s

. Since A preserves distances, p2 + r2 = 1, and q2 + s2 = 1; thus there

110
5
SYMMETRY GROUPS IN THREE DIMENSIONS
exists angles θ and φ such that p = cos θ, r = sin θ, q = sin φ, and s =
cos φ. Therefore,
detA = ps −qr = cos θ cos φ −sin θ sin φ = cos(θ + φ).
If A is proper, detA = 1, so θ + φ = 2nπ. Hence φ = 2nπ −θ, and A is of the
form

cos θ
−sin θ
sin θ
cos θ

. Conversely, if A is of this form, then AAT = I and
A ∈O(2). Since detA = +1, A is a proper rotation, and A ∈SO(2).
(ii) One improper rotation in R2 is

1
0
0
−1

, so the coset of improper rota-
tions is
SO(2)

1
0
0
−1

=

cos θ
sin θ
sin θ
−cos θ
 θ ∈R

.
(iii) If λ is an eigenvalue of

cos θ
−sin θ
sin θ
cos θ

,
then det

(cos θ) −λ
−sin θ
sin θ
(cos θ) −λ

= 0.
Therefore, λ2 −2λ cos θ + 1 = 0 and λ = cos θ ± i sin θ = e±iθ.
If λ is an eigenvalue of the improper rotation

cos θ
sin θ
sin θ
−cos θ

, then
det

(cos θ) −λ
sin θ
sin θ
−(cos θ) −λ

= 0. Hence λ2 −1 = 0 and λ = ±1.
□
The improper rotation B =

cos θ
sin θ
sin θ
−cos θ

always has an eigenvalue 1
and hence leaves an axis through the origin invariant because, for any corre-
sponding eigenvector x, Bx = x. It can be veriﬁed that this axis of eigenvectors,
corresponding to the eigenvalue 1, is a line through the origin making an angle
θ/2 with the ﬁrst coordinate axis. The matrix B corresponds to a reﬂection of
the plane about this axis.
Hence we see that an improper rotation is a reﬂection about a line through
the origin, and conversely, it is easy to see that a reﬂection about a line through
the origin is an improper rotation.
Theorem 5.8. If G is a ﬁnite subgroup of SO(2), then G is cyclic, and so is
isomorphic to Cn for some n ∈P.
Proof. By Proposition 5.6, every element A ∈G ⊂SO(2) is a counter-
clockwise rotation through an angle θ(A), where 0 ⩽θ(A) < 2π. Since G is
ﬁnite, we can choose an element B ∈G so that θ(B) is the smallest positive
angle. For any A ∈G, there exists an integer r ⩾0 such that rθ(B) ⩽θ(A) <
(r + 1)θ(B). Since θ(AB−r) = θ(A) −rθ(B), it follows that 0 ⩽θ(AB−r) <
θ(B). Therefore, θ(AB−r) = 0, AB−r = I, and A = Br.

PROPER ROTATIONS OF REGULAR SOLIDS
111
Hence G = {I, B, B2, . . . , Br, . . . , Bn−1}, and G is a ﬁnite cyclic group that
must be isomorphic to Cn for some integer n.
□
Theorem 5.9. If G is a ﬁnite subgroup of O(2), then G is isomorphic to either
Cn or Dn for some n ∈P.
Proof. The kernel of the morphism det: G →{1, −1} is a normal subgroup,
H, of index 1 or 2 consisting of the proper rotations in G. By the previous
theorem, H is a cyclic group of order n, generated by B, for example.
If G contains no improper rotations, then G = H ∼= Cn. If G does contain an
improper rotation A, then
G = H ∪HA = {I, B, B2, . . . , Bn−1, A, BA, B2A, . . . , Bn−1A}.
Since A and BkA are reﬂections, A = A−1 and BkA = (BkA)−1 = A−1B−k =
ABn−k. These relations completely determine the multiplication in G, and it is
now clear that G is isomorphic to the dihedral group Dn by an isomorphism that
takes B to a rotation through 2π/n and A to a reﬂection.
□
Theorem 5.9 shows that the only possible types of ﬁnite symmetries, ﬁx-
ing one point, of any geometric ﬁgure in the plane are the cyclic and dihedral
groups. Examples of such symmetries abound in nature; the symmetry group of
a snowﬂake is usually D6, and many ﬂowers have ﬁve petals with symmetry
group C5.
We have found all the possible ﬁnite symmetries in the plane that ﬁx one point.
However, there are ﬁgures in the plane that have inﬁnite symmetry groups that
ﬁx one point; one example is the circular disk. The group of proper symmetries
of this disk is the group SO(2), whereas the group of all symmetries is the whole
of O(2).
PROPER ROTATIONS OF REGULAR SOLIDS
One class of symmetries that we know occurs in three dimensions is the class
of symmetry groups of the regular solids: the tetrahedron, cube, octahedron,
dodecahedron, and icosahedron. In this section, we determine the proper rotation
groups of these solids. These will all be subgroups of SO(3). We restrict our
consideration to proper rotations because these are the only ones that can be
physically performed on models in three dimensions; to physically perform an
improper symmetry on a solid, we would require four dimensions!
Theorem 5.10. Every element A ∈SO(3) has a ﬁxed axis, and A is a rotation
about that axis.
Proof. Let λ1, λ2, and λ3 be the eigenvalues of A. These are the roots of the
cubic characteristic polynomial with real coefﬁcients. Hence, at least one eigen-
value is real and if a second one is complex, the third is its complex conjugate.

112
5
SYMMETRY GROUPS IN THREE DIMENSIONS
By Proposition 5.6, |λ1| = |λ2| = |λ3| = 1. Since detA = λ1λ2λ3 = 1, we can
relabel the eigenvalues, if necessary, so that one of the following cases occurs:
(i) λ1 = λ2 = λ3 = 1.
(ii) λ1 = 1, λ2 = λ3 = −1.
(iii) λ1 = 1, λ2 = λ3 = eiθ
(where θ ̸= nπ).
In all cases there is an eigenvalue equal to 1. If x is a corresponding eigenvec-
tor, then Ax = x, and A ﬁxes the axis along the vector x. We can change the
coordinate axes so that A can be written in one of the following three forms:
(i)


1
0
0
0
1
0
0
0
1


(ii)


1
0
0
0
−1
0
0
0
−1


(iii)


1
0
0
0
cos θ
−sin θ
0
sin θ
cos θ

.
The ﬁrst matrix is the identity, the second is a rotation through π, and the third
is a rotation through θ about the ﬁxed axis.
□
A regular solid is a polyhedron in which all faces are congruent regular
polygons and all vertices are incident with the same number of faces. There are
ﬁve such solids, and they are illustrated in Figure 5.1; their structure is given
in Table 5.1. The reader interested in making models of these polyhedra should
consult Cundy and Rollett [28].
Given any polyhedron, we can construct its dual polyhedron in the following
way. The vertices of the dual are the centers of the faces of the original poly-
hedron. Two centers are joined by an edge if the corresponding faces meet in
Tetrahedron
Cube
Octahedron
Dodecahedron
Icosahedron
Figure 5.1.
Regular solids.
TABLE 5.1. Regular Solids
Polyhedron
Number of
Vertices
Number of
Edges
Number of
Faces
Faces
Number of Faces at
Each Vertex
Tetrahedron
4
6
4
Triangles
3
Cube
8
12
6
Squares
3
Octahedron
6
12
8
Triangles
4
Dodecahedron
20
30
12
Pentagons
3
Icosahedron
12
30
20
Triangles
5

PROPER ROTATIONS OF REGULAR SOLIDS
113
an edge. The dual of a regular tetrahedron is another regular tetrahedron. The
dual of a cube is an octahedron, and the dual of an octahedron is a cube. The
dodecahedron and icosahedron are also duals of each other. Any symmetry of
a polyhedron will induce a symmetry on its dual and vice versa. Hence dual
polyhedra will have the same rotation group.
Theorem 5.11. The group of proper rotations of a regular tetrahedron is isomor-
phic to A4.
Proof. Label the vertices of the tetrahedron 1, 2, 3, and 4. Then any rotation
of the tetrahedron will permute these vertices. So if G is the rotation group of the
tetrahedron, we have a group morphism f : G →S4 whose kernel contains only
the identity element. Hence, by the morphism theorem, G is isomorphic to Imf .
We can use Theorem 4.40 to count the number of elements of G. The stabilizer
of the vertex 1 is the set of elements ﬁxing 1 and is {(1), (234), (243)}. The vertex
1 can be taken to any of the four vertices under G, so the orbit of 1 is the set of
four vertices. Hence |G| = |Stab 1| |Orb 1| = 3.4 = 12.
There are two types of nontrivial elements in G that are illustrated in
Figures 5.2 and 5.3. There are rotations of order 3 about axes, each of which
joins a vertex to the center of the opposite face. These rotations perform an even
permutation of the vertices because each ﬁxes one vertex and permutes the other
three cyclically. There are also rotations of order 2 about axes, each of which joins
the midpoints of a pair of opposite edges. (Two edges in a tetrahedron are said
to be opposite if they do not meet.) The corresponding permutations interchange
two pairs of vertices and, being products of two transpositions, are even.
Hence Imf
consists of 12 permutations, all of which are even, and
Imf = A4.
□
The alternating group A4 is sometimes called the tetrahedral group.
There are many different ways of counting the number of elements of the
rotation group G of the tetrahedron. One other way is as follows. Consider
the tetrahedron sitting on a table, and shade in an equilateral triangle on the
table where the bottom face rests, as in Figure 5.4. Any symmetry in G can
be performed by picking up the tetrahedron, turning it, and replacing it on the
table so that one face of the tetrahedron lies on top of the shaded equilateral
triangle. Any of the four faces of the tetrahedron can be placed on the table, and
each face can be placed on top of the shaded triangle in three different ways.
2
4
3
1
Figure 5.2.
Element (2 3 4).
4
2
3
1
Figure 5.3.
Element (1 2) Ž (3 4).

114
5
SYMMETRY GROUPS IN THREE DIMENSIONS
Figure 5.4
Hence |G| = 4 · 3 = 12. This really corresponds to applying Theorem 4.40 to the
stabilizer and orbit of a face of the tetrahedron.
Theorem 5.12. The group of proper rotations of a regular octahedron and cube
is isomorphic to S4.
Proof. The regular octahedron is dual to the cube, so it has the same rotation
group. There are four diagonals in a cube that join opposite vertices. Label these
diagonals 1, 2, 3, and 4 as in Figure 5.5. Any rotation of the cube will permute
these diagonals, and this deﬁnes a group morphism f : G →S4, where G is the
rotation group of the cube.
The stabilizer of any vertex of the cube is a cyclic group of order 3 that
permutes the three adjacent vertices. The orbit of any vertex is the set of eight
vertices. Hence, by Theorem 4.40, |G| = 3 · 8 = 24.
Consider the rotation of order 2 about the line joining A to A′ in Figure 5.5.
The corresponding permutation is the transposition (12). Similarly, any other
transposition is in Imf . Therefore, by Proposition 3.35, Imf = S4.
By the morphism theorem, G/Kerf ∼= S4 and |G|/|Kerf | = |S4|. Since |G| =
|S4| = 24, it follows that |Kerf | = 1, and f is an isomorphism.
□
The symmetric group S4 is sometimes called the octahedral group.
Theorem 5.13. The group of proper rotations of a regular dodecahedron and a
regular icosahedron is isomorphic to A5.
Proof. A regular dodecahedron is dual to the icosahedron, so it has the same
rotation group.
1
4
3
2
2
A′
A
3
4
1
Figure 5.5.
Diagonals of the cube.

PROPER ROTATIONS OF REGULAR SOLIDS
115
There are 30 edges of an icosahedron, and there are 15 lines through the center
joining the midpoints of opposite edges. (The reﬂection of each edge in the center
of the icosahedron is a parallel edge, called the opposite edge.) Given any one
of these 15 lines, there are exactly two others that are perpendicular both to the
ﬁrst line and to each other. We call three such mutually perpendicular lines a
triad. The 15 lines fall into ﬁve sets of triads. Label these triads 1, 2, 3, 4, and
5. Figure 5.6 shows the top half of an icosahedron, where we have labeled the
endpoints of each triad. (The existence of mutually perpendicular triads and the
labeling of the diagram can best be seen by actually handling a model of the
icosahedron.)
A rotation of the icosahedron permutes the ﬁve triads among themselves, and
this deﬁnes a group morphism f : G →S5, where G is the rotation group of the
icosahedron.
The stabilizer of any vertex of the icosahedron is a group of order 5 that
cyclically permutes the ﬁve adjacent vertices. The orbit of any vertex is the set
of all 12 vertices. Hence, by Theorem 4.40, |G| = 5 · 12 = 60.
There are three types of nontrivial elements in G. There are rotations of order
5 about axes through a vertex. The rotations about the vertex A in Figure 5.6
correspond to multiples of the cyclic permutation (12345), all of which are even.
There are rotations of order 3 about axes through the center of a face. The rota-
tions about an axis through the point B, in Figure 5.6, are multiples of (142)
and are therefore even permutations. Finally, there are rotations of order 2 about
the 15 lines joining midpoints of opposite edges. The permutation correspond-
ing to a rotation about an axis through C, in Figure 5.6, is (23) Ž (45), which
is even.
Every 3-cycle occurs in the image of f so, by Proposition 3.37, Imf = A5.
Since G and A5 both have 60 elements, the morphism theorem implies that G is
isomorphic to A5.
□
The alternating group A5 is sometimes called the icosahedral group.
3
4
3
5
2
1
3
4
1
5
2
2
4
3
4
2
1
5
5
A
C
B
1
Figure 5.6.
Ends of the triads of the icosahedron.

116
5
SYMMETRY GROUPS IN THREE DIMENSIONS
FINITE ROTATION GROUPS IN THREE DIMENSIONS
We now proceed to show that the only ﬁnite proper rotation groups in three
dimensions are the three symmetry groups of the regular solids, A4, S4, and A5
together with the cyclic and dihedral groups, Cn and Dn.
The unit sphere S2 = {x ∈R3 ||x|| = 1} is mapped to itself by every element
of O(3). Every rotation group ﬁxing the origin is determined by its action on the
unit sphere S2. By Theorem 5.10, every nonidentity element A ∈SO(3) leaves
precisely two antipodal points on S2 ﬁxed. That is, there exists x ∈S2 such that
A(x) = x and A(−x) = −x. The points x and −x are called the poles of A. Let
P be the set of poles of the nonidentity elements of a ﬁnite subgroup G of SO(3).
Proposition 5.14. G acts on the set, P , of poles of its nonidentity elements.
Proof. We show that G permutes the poles among themselves. Let A, B, be
nonidentity elements of G, and let x be a pole of A. Then (BAB−1)B(x) =
BA(x) = B(x), so that B(x) is a pole of BAB−1. Therefore, the image of any
pole is another pole, and G acts on the set of poles.
□
We classify the rotation groups by considering the number of elements in the
stabilizers and orbits of the poles. Recall that the stabilizer of a pole x, Stab x =
{A ∈G|A(x) = x}, is a subgroup of G, and that the orbit of x, Orb x = {B(x)|B ∈
G}, is a subset of the set P of poles. In Table 5.2 we look at the stabilizers and
orbits of the poles of the rotation groups we have already discussed.
TABLE 5.2. Poles of the Finite Rotation Groups
Group G =
|G| =
Symmetries of
Cn
n
n-agonal cone
Looking down
on the pole, x
Dn
2n
n-agonal cylinder
A4
12
tetrahedron
n
1
2
12
3
8
4
6
or
or
or
2
30
5
12
or
or
3
20
or
n
1
2
n
2
n
n
2
2
6
3
4
3
4
|Stab x|        =
|Orb x|         =
Group G =
|G| =
Symmetries of
S4
24
cube
Looking down
on the pole, x
A5
60
dodecahedron
or icosahedron
or octahedron
|Stab x|        =
|Orb x|         =

FINITE ROTATION GROUPS IN THREE DIMENSIONS
117
p
x
2p
3
4p
3
or
x
p
2
p
or
x
x
x
x
Rotation:
Looking down on
the pole x:
Order of the rotation:
2
3
4 or 2
Figure 5.7.
Rotations of the cube.
We take Cn to be the rotation group of a regular n-agonal cone whose base
is a regular n-gon. (The sloping edges of the cone must not be equal to the base
edges if n = 3.) Dn is the rotation group of a regular n-agonal cylinder whose
base is a regular n-gon. (The vertical edges must not be equal to the base edges
if n = 4.)
Each stabilizer group, Stab x, is a cyclic subgroup of rotations of the solid
about the axis through x. The orbit of x, Orb x, is the set of poles of the same
type as x. As a check on the number of elements in the stabilizers and orbits, we
have |G| = |Stab x||Orb x| for each pole x.
For example, the cube has three types of poles and four types of nontrivial
elements in its rotation group; these are illustrated in Figure 5.7.
Theorem 5.15. Any ﬁnite subgroup of SO(3) is isomorphic to one of
Cn(n ⩾1), Dn(n ⩾2), A4, S4 or A5.
Proof. Let G be a ﬁnite subgroup of SO(3). Choose a set of poles x1, . . . , xr,
one from each orbit. Let pi = |Stab xi| and qi = |Orb xi|, so that piqi = n = |G|.
Each nonidentity element of G has two poles; thus the total number of poles,
counting repetitions, is 2(n −1). The pole xi occurs as a pole of a nonidentity
element pi −1 times. There are qi poles of the same type as xi. Therefore, the
total number of poles, counting repetitions, is
2(n −1) =
r

i=1
qi(pi −1) =
r

i=1
(n −qi),
so
2 −2
n =
r

i=1

1 −1
pi

.
(∗)

118
5
SYMMETRY GROUPS IN THREE DIMENSIONS
If G is not the trivial group, n ⩾2 and 1 ⩽2 −2
n < 2. Since xi is a pole of
some nonidentity element, Stab xi contains a nonidentity element, and pi ⩾2.
Therefore, 1
2 ⩽1 −1
pi
< 1. It follows from (*) that the number of orbits, r, must
be 2 or 3.
If there are just two orbits, it follows that
2 −2
n = 1 −1
p1
+ 1 −1
p2
and 2 = n
p1
+ n
p2
= q1 + q2. Hence q1 = q2 = 1, and p1 = p2 = n. This means
that x1 = x2, and there is just one axis of rotation. Therefore, G is isomorphic
to the cyclic group Cn.
If there are three orbits, it follows that
2 −2
n = 1 −1
p1
+ 1 −1
p2
+ 1 −1
p3
1 + 2
n = 1
p1
+ 1
p2
+ 1
p3
.
(**)
Suppose that p1 ⩽p2 ⩽p3. If p1 ⩾3, we would have
1
p1
+ 1
p2
+ 1
p3
⩽1
3 + 1
3 + 1
3 = 1,
which is a contradiction, since 2
n > 0. Hence p1 = 2 and q1 = n
2.
Now 1 + 2
n = 1
2 + 1
p2
+ 1
p3
, so 1
2 + 2
n = 1
p2
+ 1
p3
. If p2 ⩾4, we would
have 1
p2
+ 1
p3
⩽1
4 + 1
4 = 1
2, which is a contradiction, since 2
n > 0. Hence p2
is 2 or 3. The only possibilities are the following.
Case (i). p1 = 2, p2 = 2, p3 = n
2, n is even and n ⩾4, q1 = n
2, q2 = n
2, and
q3 = 2.
Case (ii). p1 = 2, p2 = 3, p3 = 3, n = 12, q1 = 6, q2 = 4, and q3 = 4.
Case (iii). p1 = 2, p2 = 3, p3 = 4, n = 24, q1 = 12, q2 = 8, and q3 = 6.
Case (iv). p1 = 2, p2 = 3, p3 = 5, n = 60, q1 = 30, q2 = 20, and q3 = 12.
If p2 = 2 and p3 ⩾6, 1
p2
+ 1
p3
⩽1
3 + 1
6 = 1
2, which contradicts (**), since
2
n > 0.

FINITE ROTATION GROUPS IN THREE DIMENSIONS
119
Case (i). Let H = Stab x3. This is a group of rotations about one axis, and it
is a cyclic group of order n/2. Any other element A that is not in H is of
order 2 and is a half turn. Therefore, G = H ∪HA, and G is isomorphic
to Dn/2 of order n.
Case (ii). Let y1, y2, y3, and y4 be the four poles in Orb x2. Now p2 =
|Stab yi| = 3; thus Stab y1 permutes y2, y3, and y4 as in Figure 5.8. There-
fore, ||y2 −y1|| = ||y3 −y1|| = ||y4 −y1||. We have similar results for Stab
y2 and Stab y3. Hence y1, y2, y3, and y4 are the vertices of a regular tetra-
hedron, and G is a subgroup of the symmetries of this tetrahedron. Since
|G| = 12, G must be the whole rotation group, A4.
Case (iii). Let y1, y2, . . . , y6 be the six poles in Orb x3. Since p3 = 4, a rota-
tion in Stab yi must ﬁx two of the poles and rotate the other four cyclically.
Hence y1, y2, . . . , y6 must lie at the vertices of a regular octahedron. Again,
since |G| = 24, G must be the whole rotation group, S4, of this octahedron.
Case (iv). Let y1, y2, . . . , y12 be the 12 poles in Orb x3. Any element of
order 5 in G must permute these poles and hence must ﬁx two poles
and permute the others, as in Figure 5.9, in two disjoint 5-cycles, say
 2
3
4
5
6 
Ž  7
8
9
10
11 
, where we denote the pole yi
by i. The points y2, y3, y4, y5, and y6 form a regular pentagon and their
distances from y1 are all equal. Using similar results for rotations of order
5 about the other poles, we see that the poles are the vertices of an
icosahedron, and the group G is the proper rotation group, A5, of this
icosahedron.
□
Throughout this section we have considered only proper rotations. However,
if we allow improper rotations as well, it can be shown that a ﬁnite subgroup
y2
y3
y1
y4
Stab y1
Figure 5.8
1
2
3
4
5
6
7
8
9
10
11
12
Stab y1
Figure 5.9

120
5
SYMMETRY GROUPS IN THREE DIMENSIONS
of O(3) is isomorphic to one of the groups in Theorem 5.15 or contains one of
these groups as a normal subgroup of index 2. See Coxeter [27, Sec. 15.5] for a
more complete description of these improper rotation groups.
CRYSTALLOGRAPHIC GROUPS
This classiﬁcation of ﬁnite symmetries in R3 has important applications in crys-
tallography. Many chemical substances form crystals and their structures take
the forms of crystalline lattices. A crystal lattice is always ﬁnite, but in order to
study its symmetries, we create a mathematical model by extending this crystal
lattice to inﬁnity. We deﬁne an ideal crystalline lattice to be an inﬁnite set of
points in R3 of the form
n1a1 + n2a2 + n3a3,
where a1, a2, and a3 form a basis of R3 and n1, n2, n3 ∈Z. Common salt forms
a cubic crystalline lattice in which a1, a2, and a3 are orthogonal vectors of the
same length. Figure 5.10 illustrates a crystalline lattice.
This use of the term lattice is not the same as that in Chapter 2, where a
lattice referred to a special kind of partially ordered set. To avoid confusion, we
always use the term crystalline lattice here.
A subgroup of O(3) that leaves a crystalline lattice invariant is called a crys-
tallographic point group. This is a ﬁnite subgroup of O(3) because there are
only a ﬁnite number of crystalline lattice points that can be the images of a1, a2,
and a3 when the origin is ﬁxed.
However, not all ﬁnite subgroups of O(3) are crystallographic point groups.
Suppose that A ∈SO (3) leaves a crystalline lattice L invariant. Then, by The-
orem 5.10, A is a rotation through an angle θ, and the trace of A is 1 + 2 cos θ.
If we choose a basis for R3 consisting of the vectors a1, a2, a3 of the crystalline
lattice L, the matrix representing A will have integer entries. The trace is invari-
ant under change of basis, so the trace of A must be an integer. Hence 2 cos θ
a3
a2
a1
Figure 5.10.
Crystalline lattice.

EXERCISES
121
must be integral, and θ must be either kπ/2 or kπ/3, where k ∈Z. It follows
that every element of a crystallographic point group in SO(3) can only contain
elements of order 1, 2, 3, 4, or 6.
It can be shown that every crystallographic point group in SO(3) is isomorphic
to one of C1, C2, C3, C4, C6, D2, D3, D4, D6, A4, or S4.
If we allow reﬂections, the only other such groups in O(3) must contain one
of these groups as a normal subgroup of index 2. Every one of these groups
occurs in nature as the point group of at least one chemical crystal. See Coxeter
[27, Sec. 15.6] or Lomont [31, Chap. 4, Sec. 4].
EXERCISES
Find the group of proper rotations and the group of all rotations of the ﬁgures in
Exercises 5.1 to 5.7.
5.1.
5.2.
5.3.
5.4.
5.5.
5.6.
5.7.
5.8. Let G be the subgroup of O(2) isomorphic to Dn. Find two matrices A and
B so that any element of G can be written as a product of A’s and B’s.
5.9. What is the group of proper rotations of a rectangular box of length 3 cm,
depth 2 cm, and height 2 cm?
Find the proper rotation group of the 13 Archimedean solids in Exercises 5.10
to 5.22. All the faces of these solids are regular polygons and all the vertices are
similar. (See Cundy and Rollet [28] for methods on how to construct these solids.)
5.10.
5.11.
5.12.
5.13.
5.14.
5.15.
5.16.
5.17.
5.18.
5.19.

122
5
SYMMETRY GROUPS IN THREE DIMENSIONS
5.20.
5.21.
5.22.
5.23. It is possible to inscribe ﬁve cubes in a regular dodecahedron. One such
cube is shown in Figure 5.11. Use these cubes to show that the rotation
group of the dodecahedron is A5.
Figure 5.11.
Cube inside a dodecahedron.
5.24. What is the proper symmetry group of a cube in which three faces, coming
together at one vertex, are painted green and the other faces are red?
5.25. Find the group of all rotations (both proper and improper) of a regular
tetrahedron.
5.26. Let G be the full symmetry group of the cube. Deﬁne f : G →S4 as in
Theorem 5.12. Find the kernel of f and the order of G.
5.27. Let the vertices of a tetrahedron be (1, 1, 1), (−1, −1, 1), (−1, 1, −1),
and (1, −1, −1). Find matrices in SO(3) of orders 2 and 3 that leave the
tetrahedron invariant.
5.28. Let the vertices of a cube be (±1, ±1, ±1). Find matrices in SO(3) of
orders 2, 3, and 4 that leave the cube invariant.
Find the symmetry groups of the chemical molecules in Exercises 5.29 to 5.31.
(Assume that all of the C-C bonds are equivalent.)
5.29.
C
C
C
C
C
C
H
H
H
H
H
H
Benzene
5.30.
C
C
C
C
C
C
H
H
CH3
CH3
H
H
Xylene
5.31.
C
C
C
C
C
C
NO2
CH3
NO2
H
H
NO2
Trinitrotoluene (TNT)

EXERCISES
123
Find matrices in SO(3) that preserve the crystalline lattices described in Exercises
5.32 to 5.34, and ﬁnd their crystallographic point groups. The points of the crys-
talline lattice are n1a1 + n2a2 + n3a3, where ni ∈Z and the basis vectors ai
are given below.
5.32. a1 = (1, 1, 0), a2 = (−1, 1, 0), a3 = (0, 1, 1).
5.33. a1 = (1, 0, 0), a2 = (0, 1, 0), a3 = (0, 0, 2).
5.34. a1 = (1, 0, 0), a2 = (0, −3
√
3, 3), a3 = (0, 3
√
3, 3).
5.35. Let G(n) denote the group of linear isometries of Rn.
(a) Show that E(n) = T (n)G(n) = G(n)T (n), where the product of sub-
groups is as deﬁned in Exercise 4.43.
(b) Show that T (n) ∩G(n) = {1Rn}.
(c) Use parts (a) and (b) to prove that E(n)/T (n) ∼= G(n).

6
P ´OLYA–BURNSIDE METHOD
OF ENUMERATION
This chapter provides an introduction to the P´olya–Burnside method of counting
the number of orbits of a set under the action of a symmetry group. If a group
G acts on a set X and we know the number of elements of X, this method will
enable us to count the number of different types of elements of X under the
action of G.
For example, how many different chemical compounds can be obtained by
attaching a CH3 or H radical to each carbon atom in the benzene ring of
Figure 6.3? There are 26 different ways of attaching a CH3 or H radical on paper,
but these do not all give rise to different compounds because many are equivalent
under a symmetry. There are six different ways of attaching one CH3 radical and
ﬁve H radicals, but they all give rise to the same compound. The dihedral group
D6 acts on the 26 ways of attaching the radicals, and the number of different
compounds is the number of orbits under the action of D6, that is, the number of
formulas that cannot be obtained from each other by any rotation or reﬂection.
We have seen that the number of different switching circuits that can be
obtained with n switches is 22n. This number grows very quickly as n becomes
large. Table 2.11 gives the 16 switching functions of two variables; when n = 3,
there are 256 different circuits, and when n = 4, there are 65,536 different cir-
cuits. However, many of these circuits are equivalent if we change the labels of
the switches. That is, the symmetric group, Sn, acts on the 22n different circuits
by permuting the labels of the switches. The number of nonequivalent circuits is
the number of orbits under the action of Sn.
BURNSIDE’S THEOREM
Let G be a ﬁnite group that acts on a ﬁnite set X. The following theorem
describes the number of orbits in terms of the number of elements left ﬁxed
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
124

BURNSIDE’S THEOREM
125
by each element of G. It was ﬁrst proved by W. Burnside in 1911 and was
called Burnside’s lemma; it was not until 1937 that its applicability to many
combinatorial problems was discovered by G. P´olya.
Theorem 6.1. Burnside’s Theorem. Let G be a ﬁnite group that acts on the
elements of a ﬁnite set X. For each g ∈G, let Fix g = {x ∈X|g(x) = x}, the
set of elements of X left ﬁxed by g. If N is the number of orbits of X under
G, then
N =
1
|G|

g∈G
| Fix g|.
Proof. We count the set S = {(x, g) ∈X × G|g(x) = x} in two different ways.
Consider Table 6.1, whose columns are indexed by the elements of X and whose
rows are indexed by the elements of G. Put a value of 1 in the (x, g) position if
g(x) = x; otherwise, let the entry be 0.
The sum of the entries in row g is the number |Fix g| of elements left ﬁxed
by g. The sum of the entries in column x is |Stab x|, the number of elements of
G that ﬁx x.
We can count the number of elements of S either by totaling the row sums or
by totaling the column sums. Hence
|S| =

g∈G
|Fix g| =

x∈X
|Stab x|.
Choose a set of representatives, x1, x2, . . . , xN, one from each orbit of X under
G. If x is in the same orbit as xi, then Orb x = Orb xi, and by Theorem 4.40,
|Stab x| = |Stab xi|. Hence

g∈G
|Fix g| =
N

i=1
·

x∈Orb xi
|Stab x| =
N

i=1
|Orb xi||Stab xi| = N · |G|
by Theorem 4.40. The theorem now follows.
□
TABLE 6.1. Elements of S Correspond to the 1’s in
This Table
Elements of X
Row
x
Sums
↓
...
1
Elements
g
. . .1
0
1
1
1
0. . .
|Fix g|
of G
0
...
Column →
|Stab x|
Sums

126
6
P ´OLYA–BURNSIDE METHOD OF ENUMERATION
NECKLACE PROBLEMS
Example 6.2. Three black and six white beads are strung onto a circular wire.
This necklace can be rotated in its plane, and turned over. How many differ-
ent types of necklaces can be made assuming that beads of the same color are
indistinguishable?
Solution. Position the three black and six white beads at the vertices of a
regular 9-gon. If the 9-gon is ﬁxed, there are 9 · 8 · 7/3! = 84 different ways of
doing this. Two such arrangements are equivalent if there is an element of the
symmetry group of the regular 9-gon, D9, which takes one arrangement into the
other. The group D9 permutes the different arrangements, and the number of
nonequivalent arrangements is the number, N, of orbits under D9. We can now
use the Burnside theorem to ﬁnd N.
Table 6.2 lists all the different types of elements of D9 and the number of
ﬁxed points for each type. For example, consider the reﬂection g ∈D9 about the
line joining vertex 2 to the center of the circle, which is illustrated in Figure 6.1.
Then the arrangements that are ﬁxed under g occur when the black beads are
at vertices 1 2 3, 9 2 4, 8 2 5, or 7 2 6. Hence |Fix g| = 4. There are nine
reﬂections about a line, one through each vertex. Therefore, the total number of
ﬁxed points contributed by these types of elements is 9 · 4 = 36. A rotation of
order 3 in D9 will ﬁx an arrangement if the black beads are at vertices 1 4 7, 2 5
8, or 3 6 9; hence there are three arrangements that are ﬁxed. If an arrangement
is ﬁxed under a rotation of order 9, all the beads must be the same color; hence
|Fix g| = 0 if g has order 9. Table 6.2 shows that the sum of all the numbers of
ﬁxed points is 126. By Theorem 6.1, N =
1
|D9|

g∈D9 |Fix g| = 126
18 = 7, and
there are seven different types of necklaces.
□
In this example, it is easy to determine all the seven types. They are illustrated
in Figure 6.2.
TABLE 6.2. Action of D9 on the Necklaces
Type of Element,
g ∈D9
Order of g
Number, s, of
Such Elements
|Fix g|
s · |Fix g|
Identity
1
1
84
84
Reﬂection about a line
2
9
4
36
Rotation through 2π/3 or 4π/3
3
2
3
6
Other rotations
9
6
0
0
 = 126

NECKLACE PROBLEMS
127
1
2
3
4
5
6
7
8
9
g
Figure 6.1.
D9 acting on the necklace.
Figure 6.2.
Seven types of necklaces.
C
C
C
C
C
C
4
1
6
5
3
2
– – – – – – –
– – – –
g
Figure 6.3.
Benzene ring.
TABLE 6.3. Action of D6 on the Compounds
Type of Element,
g ∈D6
Order of g
Number, s, of
Such Elements
|Fix g|
s · |Fix g|
Identity
1
1
26
64
Reﬂection in a line through
opposite vertices
[e.g., (26) Ž (35) Ž (1) Ž (4)]
2
3
24
48
Reﬂection in a line through
midpoints of opposite sides
[e.g., (56) Ž (14) Ž (23)]
2
3
23
24
Rotation through ±π/3
[e.g., (123456)]
6
2
2
4
Rotation through ±2π/3
[e.g., (135) Ž (246)]
3
2
22
8
Rotation through π,
(14) Ž (25) Ž (36)
2
1
23
8
|D6| = 12
 = 156

128
6
P ´OLYA–BURNSIDE METHOD OF ENUMERATION
Example 6.3. Find the number of different chemical compounds that can be
obtained by attaching CH3 or H radicals to a benzene ring.
Solution. The carbon atoms are placed at the six vertices of a regular hexagon,
and there are 26 ways of attaching CH3 or H radicals. The dihedral group D6
acts on these 26 ways, and we wish to ﬁnd the number of orbits.
Consider a reﬂection, g, about a line through opposite vertices. The order of
g is 2, and there are three such reﬂections, through the three opposite pairs of
vertices. |Fix g| can be determined by looking at Figure 6.3. If a conﬁguration is
ﬁxed by g, the radical in place 2 must be the same as the radical in place 6, and
also the radicals in places 3 and 5 must be equal. Hence the radicals in places 1,
2, 3, and 4 can be chosen arbitrarily, and this can be done in 24 ways.
The number of conﬁgurations left ﬁxed by each element of D6 is given in
Table 6.3. To check that we have not omitted any elements, we add the column
containing the numbers of elements, and this should equal the order of the group
D6. It follows from the Burnside theorem that the number of orbits is 156/|D6| =
156/12 = 13. Hence there are 13 different types of molecules obtainable.
□
COLORING POLYHEDRA
Example 6.4. How many ways is it possible to color the vertices of a cube if n
colors are available?
Solution. If the cube is ﬁxed, the eight vertices can each be colored in n ways,
giving a total of n8 colorings. The rotation group of the cube, S4, permutes these
colorings among themselves, and the number of orbits is the number of distinct
colorings taking the rotations into account. We can calculate the number of orbits
using the Burnside theorem.
There are ﬁve types of elements in the rotation group of the cube. We take an
element g of each type and determine the vertices that must have the same color
in order that the coloring be invariant under g.
Figure 6.4 illustrates the different types of rotations; vertices that have to have
the same color are shaded in the same way. Table 6.4 gives the number of ﬁxed
colorings, with column totals.
By the Burnside theorem, the number of orbits and hence the number of
colorings is
1
|S4|

g∈S4
|Fix g| =
1
24(n8 + 17n4 + 6n2).
p
g1
2p/3
g2
p/2
g3
p
g4
Figure 6.4.
Types of rotations of the cube.

COLORING POLYHEDRA
129
TABLE 6.4. Colorings of the Vertices of the Cube
Type of
Element, gi
(Figure 6.4)
Order of
gi
Number,
s, of
Such Elements
Number,
r, of Choices
of Colors
|Fix gi|
Which Is
nr
s · |Fix gi|
Identity
1
1
8
n8
n8
g1
2
6 · 1 = 6
4
n4
6n4
g2
3
4 · 2 = 8
4
n4
8n4
g3
4
3 · 2 = 6
2
n2
6n2
g4
2
3 · 1 = 3
4
n4
3n4
|S4| = 24
n8 + 17n4 + 6n2
This shows, incidentally, that n8 + 17n4 + 6n2 is divisible by 24 for all
n ∈P.
□
Example 6.5. In how many ways is it possible to color a regular dodecahedron
so that ﬁve of its faces are black and the other seven are white?
Solution. The number of ways† of choosing ﬁve faces of a ﬁxed dodecahedron
to be colored black is

12
5

= 12 · 11 · 10 · 9 · 8
5!
= 792.
The different types of elements in the rotation group, A5, of the dodecahedron
are shown in Figure 6.5. The numbers of elements of a given type, in Table 6.5,
are calculated as follows. An element of order 3 is a rotation about an axis
through opposite vertices. Since there are 20 vertices, there are ten such axes.
There are two nonidentity rotations of order 3 about each axis; thus the total
number of elements of order 3 is 10 · 2 = 20. The elements of orders 2 and 5
can be counted in a similar way.
If g2 ∈A5 is of order 3, we can calculate |Fix g2| as follows. The element
g2 does not ﬁx any face and permutes the faces in disjoint 3-cycles. Now ﬁve
black faces cannot be permuted by disjoint 3-cycles without ﬁxing two faces, so
|Fix g2| = 0. Similarly, |Fix g1| = 0 if g1 is a 2-cycle. If g3 is of order 5, then
g3 is a rotation about an axis through the centers of two opposite faces, and these
two faces are ﬁxed. The other ten faces are permuted in two disjoint 5-cycles;
either of these 5-cycles can be black; thus |Fix g3| = 2.
It follows from Table 6.5 and from the Burnside theorem that the number of
different colorings is 840/60 = 14.
□
Any face coloring of the dodecahedron corresponds to a vertex coloring of its
dual, the icosahedron.
† A set of n elements has

n
k

subsets of k ⩽n elements where

n
k

=
n!
k!(n −k)! is the binomial
coefﬁcient.

130
6
P ´OLYA–BURNSIDE METHOD OF ENUMERATION
g1
g2
g3
Figure 6.5.
Types of rotations of a dodecahedron.
TABLE 6.5. Colorings of the Dodecahedron
Type of Element, gi
(Figure 6.5)
Order of gi
Number, s, of
Such Elements
|Fix gi|
s · |Fix gi|
Identity
1
1
792
792
g1
2
15
0
0
g2
3
10 · 2 = 20
0
0
g3
5
6 · 4 = 24
2
48
|A5| = 60
 = 840
COUNTING SWITCHING CIRCUITS
The Burnside theorem can still be applied when the sets to be enumerated do not
have any geometric symmetry. In this case, the symmetry group is usually the
full permutation group Sn.
Consider the different switching circuits obtained by using three switches. We
can think of these as black boxes with three binary inputs x1, x2, and x3 and one
binary output f (x1, x2, x3), as in Figure 6.6. Two circuits, f and g, are called
equivalent if there is a permutation π of the variables so that f (x1, x2, x3) =
g(xπ1, xπ2, xπ3). Equivalent circuits can be obtained from each other by just
permuting the wires outside the black boxes, as in Figure 6.7.
x1
x2
x3
f
f(x1, x2, x3)
Figure 6.6.
Switching circuit.
f
f(x2, x3, x1)
x1
x2
x3
Figure 6.7.
Permutation of the inputs.

COUNTING SWITCHING CIRCUITS
131
Example 6.6. Find the number of switching circuits using three switches that
are not equivalent under permutations of the inputs.
Solution. There are eight possible inputs using three binary variables and
hence there are 28 = 256 circuits to consider. The symmetric group S3 acts
on these 256 circuits, and we wish to ﬁnd the number of different equivalence
classes, that is, the number of orbits.
Table 6.6 lists the number of circuits left ﬁxed by the different types of ele-
ments in S3. For example, if the switching function f (x1, x2, x3) is ﬁxed by
the transposition (12) of the input variables, then f (0, 1, 0) = f (1, 0, 0) and
f (0, 1, 1) = f (1, 0, 1). The values of f for the inputs (0, 0, 0), (0, 0, 1), (0, 1,
0), (0, 1, 1), (1, 1, 0), and (1, 1, 1) can be chosen arbitrarily in 26 ways.
By Burnside’s theorem and Table 6.6, the number of nonequivalent circuits is
480/|S3| = 480/6 = 80.
□
However, this number can be reduced further if we allow permutations and
complementation of the three variables. In a circuit consisting of two-state switches,
the variable xi can be complemented by simply reversing each of the switches
controlled by xi. The resulting circuit is just as simple and the cost is the same as
the original one. In transistor networks, we can just permute the input wires and
add NOT gates as in Figure 6.8.
The eight input values of a three-variable switching circuit can be considered
as the vertices of a three-dimensional cube, as shown in Figure 6.9. The six faces
of this cube are deﬁned by the equations x1 = 0, x1 = 1, x2 = 0, x2 = 1, x3 = 0,
x3 = 1. The group that permutes and complements the variables takes each face
to another face and takes opposite faces to opposite faces. Hence the group is the
complete symmetry group, G, of the cube. There is a morphism ψ: G →{1, −1},
TABLE 6.6. Action of S3 on the Inputs of the Switches
Type of Element,
g ∈S3
Number, s, of
Such Elements
|Fix g|
s · |Fix g|
Identity
1
28
28 = 256
Transposition
3
26
3 · 26 = 192
3-Cycle
2
24
2 · 24 = 32
|S3| = 6
480
f
f(x2, x1′, x3′)
x3
x2
x1
NOT
NOT
Figure 6.8.
Permutation and complementation of inputs.

132
6
P ´OLYA–BURNSIDE METHOD OF ENUMERATION
111
011
001
000
010
101
110
100
Figure 6.9.
Cube of input values.
which sends proper rotations to 1 and improper rotations to −1; the kernel of ψ
is the group of proper rotations of the cube which, by the morphism theorem,
must be a normal subgroup of index 2. Therefore, the order of G is 2 · 24 = 48.
Example 6.7. Find the number of switching circuits involving three switches
that are nonequivalent under permutation and complementation of the variables.
Solution. Each boolean function in three variables deﬁnes a coloring of the
vertices of the cube of input values. A vertex is colored black if the function is
1 for the corresponding input value. It is colored white if the function takes the
value 0 at that input value.
We can represent the complete symmetry group, G, of the cube by means of
permutations of the vertices labeled 0, 1, 2, 3, 4, 5, 6, 7 in Figure 6.10. Since
the group of proper rotations of the cube is a normal subgroup of index 2 in G,
every element of G can be written as a proper rotation π or as π Ž ρ, where ρ is
the reﬂection of the cube in its center.
There are 28 different switching functions of three variables, and Table 6.7
describes the number of circuits that are ﬁxed by the action of each element
of the group G on the eight inputs. For example, consider the element g =
(01) Ž (67) Ž (34) Ž (25). If a switching function f is ﬁxed under the action of g,
then the images of the input values corresponding to the vertices 0 and 1 must be
the same; that is, f (0, 0, 0) = f (0, 0, 1). Similarly, the images of the input values
corresponding to the vertices 6 and 7 are the same, and f (1, 1, 0) = f (1, 1, 1).
3
7
6
5
2
1
0
4
Figure 6.10.
Labeling of the cube.

COUNTING SWITCHING CIRCUITS
133
TABLE 6.7. Symmetries of a Cube Acting on the Three-Variable
Switching Functions
Type of Element,
g, in the Symmetry
Group of the Cube
Order of g
Number, s, of
Such Elements |Fix g| s · |Fix g|
Proper rotations
Identity
1
1
28
256
Rotation about a line joining
midpoints of opposite edges
[e.g., (01) Ž (67) Ž (34) Ž (25)]
2
6
24
96
Rotation about a line joining
opposite vertices
[e.g., (124) Ž (365) Ž (0) Ž (7)]
3
8
24
128
Rotation about a line joining centers
of opposite faces
[e.g., (0264) Ž (1375)]
4
6
22
24
Rotation about a line joining centers
of opposite faces
[e.g., (06) Ž (24) Ž (17) Ž (35)]
2
3
24
48
Improper rotations
Reﬂection in the center
[ρ = (07) Ž (16) Ž (25) Ž (34)]
2
1
24
16
Reﬂection in a diagonal plane
[e.g., (01) Ž (67) Ž (34) Ž (25) Ž ρ =
(06) Ž (17) Ž (2) Ž (3) Ž (4) Ž (5)]
2
6
26
384
Reﬂection and rotation
[e.g., (124) Ž (365) Ž ρ =
(07) Ž (154623)]
6
8
22
32
Reﬂection and rotation
[e.g., (0264) Ž (1375) Ž ρ =
(0563) Ž (1472)]
4
6
22
24
Reﬂection in a central plane
[e.g., (06) Ž (24) Ž (17) Ž (35) Ž ρ =
(01) Ž (23) Ž (45) Ž (67)]
2
3
24
48
|G| = 48
1056
Also, f (0, 1, 1) = f (1, 0, 0) and f (0, 1, 0) = f (1, 0, 1). Hence the values of
f (0, 0, 0), f (1, 1, 0), f (0, 1, 1), and f (0, 1, 0) can be chosen arbitrarily in 24
ways, and |Fix g| = 24. In general, if the function f is ﬁxed under g, the images
of the input values, corresponding to the vertices in any one cycle of g, must be
the same. Hence |Fix g| is 2r, where r is the number of disjoint cycles in the
permutation representation of g.
It follows from Table 6.7 and the Burnside theorem that the number of non-
equivalent circuits is 1056/|G| = 1056/48 = 22.
□

134
6
P ´OLYA–BURNSIDE METHOD OF ENUMERATION
TABLE 6.8. Number of Types of Switching Functions
Number of Switches, n
1
2
3
4
5
Number of boolean
functions, 22n
4
16
256
65,536
4,294,967,296
Nonequivalent functions
under permutations of
inputs
4
12
80
3,984
37,333,248
Nonequivalent functions
under permutation and
complementation of
inputs
3
6
22
402
1,228,158
Nonequivalent functions
under permutation and
complementation of
inputs and outputs
2
4
14
222
616,126
We can reduce this number slightly more by complementing the function as
well as the variables; this corresponds to adding a NOT gate to the output. The
group acting is now a combination of a cyclic group of order 2 with the complete
symmetry groups of the cube.
The numbers of nonequivalent circuits for ﬁve or fewer switches given in
Table 6.8 can be computed as in Example 6.7.
In 1951, the Harvard Computing Laboratory laboriously calculated all the
nonequivalent circuits using four switches and the best way to design each of
them. It was not until later that it was realized that the P´olya theory could be
applied to this problem.
In many examples, it is quite difﬁcult to calculate |Fix g| for every element
g of the group G. P´olya’s most important contribution to this theory of enumer-
ation was to show how |Fix g| can be calculated, using what are called cycle
index polynomials. This saves much individual calculation, and the results on
nonequivalent boolean functions in Table 6.8 can easily be calculated. However,
it is still a valuable exercise to tackle a few enumeration problems without using
cycle index polynomials, since this gives a better understanding of the P´olya
theory. For example, we see in Tables 6.3, 6.4, and 6.7 that |Fix g| is always of
the form nr, where r is the number of disjoint cycles in g.
Further information on the P´olya theory can be obtained from Biggs [15], Lidl
and Pilz [10], or Stone [22].

EXERCISES
135
EXERCISES
Find the number of different types of circular necklaces that could be made from
the sets of beads described in Exercises 6.1 to 6.4, assuming that all the beads
are used on one necklace.
6.1. Three black and three white beads.
6.2. Four black, three white, and one red bead.
6.3. Seven black and ﬁve white beads.
6.4. Five black, six white, and three red beads.
6.5. How many different circular necklaces containing ten beads can be made
using beads of at most two colors?
6.6. Five neutral members and two members from each of two warring factions
are to be seated around a circular armistice table. In how many nonequiv-
alent ways, under the action of D9, can they be seated if no two members
of opposing factions sit next to each other?
6.7. How many different chemical compounds can be made by attaching H,
CH3, C2H5, or Cl radicals to the four bonds of a carbon atom? The radicals
lie at the vertices of a regular tetrahedron, and the group is the tetrahedral
group A4.
6.8. How many different chemical compounds can be made by attaching H,
CH3, or OH radicals to each of the carbon atoms in the benzene ring of
Figure 6.3? (Assume that all of the C–C bonds in the ring are equivalent.)
6.9. How many ways can the vertices of a cube be colored using, at most,
three colors?
6.10. How many ways can the vertices of a regular tetrahedron be colored using,
at most, n colors?
6.11. How many different tetrahedra can be made from n types of resistors when
each edge contains one resistor?
6.12. How many ways can the faces of a regular dodecahedron be colored using,
at most, n colors?
Find the number of different colorings of the faces of the solids described in
Exercises 6.13 to 6.16.
6.13. A regular tetrahedron with two white faces and two black faces.
6.14. A cube with two white, one black, and three red faces.
6.15. A regular icosahedron with four black faces and 16 white faces.
6.16. A regular dodecahedron with ﬁve black faces, two white faces, and ﬁve
green faces.
6.17. How many ways can the faces of a cube be colored with six different
colors, if all the faces are to be a different color?
6.18. (a) Find the number of binary relations, on a set with four elements, that
are not equivalent under permutations of the four elements.

136
6
P ´OLYA–BURNSIDE METHOD OF ENUMERATION
Switching
device
Output
Input plug
x1
x2
x3
Figure 6.11
Figure 6.12
(b) Find the number of equivalence relations, on a set with four elements,
that are not equivalent under permutations of the four elements.
6.19. How many different patchwork quilts, four patches long and three patches
wide, can be made from ﬁve red and seven blue squares, assuming that the
quilts cannot be turned over?
6.20. If the quilts in Exercise 6.19 could be turned over, how many different
patterns are possible?
6.21. Find the number of ways of distributing three blue balls, two red balls, and
four green balls into three piles.
6.22. If the cyclic group Cn, generated by g, operates on a set S, show that the
number of orbits is
1
n

d/n
|Fix gn/d| · φ(d),
where the Euler φ-function, φ(d), is the number of integers from 1 to d
that are relatively prime to d. (See Exercises 4.55 and 4.56.)
6.23. Some transistor switching devices are sealed in a can with three input
sockets at the vertices of an equilateral triangle. The three input wires
are connected to a plug that will ﬁt into the input sockets as shown in
Figure 6.11. How many different cans are needed to produce any boolean
function of three input variables?
6.24. How many different ways can the elements of the poset in Figure 6.12 be
colored using, at most, n colors?
6.25. Verify that the number of nonequivalent switching functions of four vari-
ables, under permutation of the inputs, is 3984.

7
MONOIDS AND MACHINES
For many purposes, a group is too restrictive an algebraic concept, and we need
a more general object. In the theory of machines, or automata theory, and in
the mathematical study of languages and programming, algebraic objects arise
naturally that have a single binary operation that is associative and has an iden-
tity. These are called monoids. The instructions to a digital machine consist of
a sequence of input symbols that is fed into the machine. Two such sequences
can be combined by following one by the other and, since this operation is asso-
ciative, these input sequences form a monoid; the identity is the empty sequence
that leaves the machine alone. Even though inverses do not necessarily exist in
monoids, many of the general notions from group theory can be applied to these
objects; for example, we can deﬁne subobjects, morphisms, and quotient objects.
MONOIDS AND SEMIGROUPS
A monoid (M, ⋆) consists of a set M together with a binary operation ⋆on M
such that
(i) a ⋆(b ⋆c) = (a ⋆b) ⋆c for all a, b, c ∈M.
(associativity)
(ii) There exists an identity e ∈M such that a ⋆e = e ⋆a = a for all a ∈M.
All groups are monoids. However, more general objects such as (N, +) and
(N, ·), which do not have inverses, are also monoids.
A monoid (M, ⋆) is called commutative if the operation ⋆is commutative.
The algebraic objects (N, +), (N, ·), (Z, +), (Z, ·), (Q, +), (Q, ·), (R, +), (R, ·),
(C, +), (C, ·), (Zn, +), and (Zn, ·) are all commutative monoids.
However, (Z, −) is not a monoid because subtraction is not associative. In
general, (a −b) −c ̸= a −(b −c).
Sometimes an algebraic object would be a monoid but for the fact that it lacks
an identity element; such an object is called a semigroup. Hence a semigroup
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
137

138
7
MONOIDS AND MACHINES
(S, ⋆) is just a set S together with an associative binary operation, ⋆. For example,
(P, +) is a semigroup, but not a monoid, because the set of positive integers, P,
does not contain zero.
Just as one of the basic examples of a group consists of the permutations of
any set, a basic example of a monoid is the set of transformations of any set. A
transformation is just a function (not necessarily a bijection) from a set to itself.
In fact, the analogue of Cayley’s theorem holds for monoids, and it can be shown
that every monoid can be represented as a transformation monoid.
Proposition 7.1. Let X be any set and let XX = {f : X →X} be the set of all
functions from X to itself. Then (XX, Ž ) is a monoid, called the transformation
monoid of X.
Proof. If f, g ∈XX, then the composition f Ž g ∈XX. Composition of func-
tions is always associative, because if f, g, h ∈XX, then
(f Ž (g Ž h))(x) = f (g(h(x)))
and
((f Ž g) Ž h)(x) = f (g(h(x)))
for all x ∈X. The identity function 1X: X →X deﬁned by 1X(x) = x is the
identity for composition. Hence (XX, Ž ) is a monoid.
□
Example 7.2. If X = {0, 1}, write out the table for the transformation monoid
(XX, Ž ).
Solution. XX has four elements, e, f, g, h, deﬁned as follows.
e(0) = 0
f (0) = 0
g(0) = 1
h(0) = 1
e(1) = 1
f (1) = 0
g(1) = 0
h(1) = 1
The table for (XX, Ž ) is shown in Table 7.1. For example, g Ž f (0) = g(f (0)) =
g(0) = 1, and g Ž f (1) = g(f (1)) = g(0) = 1. Therefore, g Ž f = h. The other
compositions can be calculated in a similar manner.
□
Example 7.3. Prove that (Z, ⋆) is a commutative monoid, where x ⋆y =
6 −2x −2y + xy for x, y ∈Z.
TABLE 7.1. Transformation Monoid of {0, 1}
Ž
e
f
g
h
e
e
f
g
h
f
f
f
f
f
g
g
h
e
f
h
h
h
h
h

MONOIDS AND SEMIGROUPS
139
Solution. For any x, y ∈Z, x ⋆y ∈Z, and x ⋆y = y ⋆x, so that ⋆is a com-
mutative binary operation on Z. Now
x ⋆(y ⋆z) = x ⋆(6 −2y −2z + yz) = 6 −2x + (−2 + x)(6 −2y −2z + yz)
= −6 + 4x + 4y + 4z −2xy −2xz −2yz + xyz.
Also,
(x ⋆y) ⋆z = (6 −2x −2y + xy) ⋆z = 6 + (−2 + z)(6 −2x −2y + xy) −2z
= −6 + 4x + 4y + 4z −2xy −2xz −2yz + xyz
= x ⋆(y ⋆z).
Hence ⋆is associative.
Suppose that e ⋆x = x. Then 6 −2e −2x + ex = x, and 6 −2e −3x + ex =
0. This implies that (x −2)(e −3) = 0. Hence e ⋆x = x for all x ∈Z if and only
if e = 3. Therefore, (Z, ⋆) is a commutative monoid with 3 as the identity.
□
Since the operation in a monoid, (M, ⋆), is associative, we can omit the paren-
theses when writing down a string of symbols combined by ⋆. We write the
element x1 ⋆(x2 ⋆x3) = (x1 ⋆x2) ⋆x3 simply as x1 ⋆x2 ⋆x3.
In any monoid (M, ⋆) with identity e, the powers of any element a ∈M are
deﬁned by
a0 = e,
a1 = a,
a2 = a ⋆a,
. . . ,
an = a ⋆an−1
for n ∈N.
The monoid (M, ⋆) is said to be generated by the subset A if every element of
M can be written as a ﬁnite combination of the powers of elements of A. That
is, each element m ∈M can be written as
m = ar1
1 ⋆ar2
2 ⋆· · · ⋆arn
n
for some a1, a2, . . . , an ∈A.
For example, the monoid (P, ·) is generated by all the prime numbers. The
monoid (N, +) is generated by the single element 1, since each element can be
written as the sum of n copies of 1, where n ∈N. A monoid generated by one
element is called a cyclic monoid.
A ﬁnite cyclic group is also a cyclic monoid. However, the inﬁnite cyclic
group (Z, +) is not a cyclic monoid; it needs at least two elements to gener-
ate it, for example, 1 and −1. Not all ﬁnite cyclic monoids are groups. For
example, extending the notation of Chapter 3, let σ =

1
2
3
4
1
1
4
3

∈XX
where X = {1, 2, 3, 4}. Then M = {ε, σ, σ 2, σ 3} is a cyclic monoid that is not a
group because σ 4 = σ 2. More generally, the points in Figure 7.1 correspond to
the elements of a cyclic monoid, and the arrows correspond to multiplication by
the element c.

140
7
MONOIDS AND MACHINES
c
e
c2
c3
ck
…
ck − m − 1
ck + m − 2
ck + 1
ck + 2
Figure 7.1.
Finite cyclic monoid.
A computer receives its information from an input terminal that feeds in a
sequence of symbols, usually binary digits consisting of 0’s and 1’s. If one
sequence is fed in after another, the computer receives one long sequence that is
the concatenation (or juxtaposition) of the two sequences. These input sequences
together with the binary operation of concatenation form a monoid that is called
the free monoid generated by the input symbols.
Let A be any set (sometimes called the alphabet), and let An be the set
of n-tuples of elements in A. In this chapter, we write an n-tuple as a string
of elements of A without any symbols between them. The elements of An are
called words of length n from A. A word of length 0 is an empty string; this
empty word is denoted by . For example, if A = {a, b}, then baabbaba ∈A8,
A0 = {}, and
A3 = {aaa, aab, aba, abb, baa, bab, bba, bbb}.
Let FM(A) denote the set of all words from A, more formally
FM(A) = A0 ∪A ∪A2 ∪A3 ∪· · · =
∞

n=0
An.
Then (FM(A), ⋆) is called the free monoid generated by A, where the operation
⋆is concatenation, and the identity is the empty word . Another common
notation for FM(A) is A∗.
If we do not include the empty word, , we obtain the free semigroup
generated by A; this is often denoted by A+.
If α and β are words of length m and n, then α ⋆β is the word of length
m + n obtained by placing α to the left of β.
If
A
consists
of
a
single
element,
a,
then
the
monoid
FM(A) =
{, a, aa, aaa, aaaa, . . .} and, for example, aaa ⋆aa = aaaaa. This free monoid,
generated by one element, is commutative.
If A = {0, 1}, then FM(A) consists of all the ﬁnite sequences of 0’s and 1’s,
FM(A) = {, 0, 1, 00, 01, 10, 11, 000, 001, . . .}.
We have 010 ⋆1110 = 0101110 and 1110 ⋆010 = 1110010, so FM(A) is not
commutative.

MONOIDS AND SEMIGROUPS
141
If A = {a, b, c, d, . . . , y, z, □, .}, the letters of the alphabet together with a
space, □, and a period, then
the□sky ∈FM(A)
and
the□sky ⋆is□b ⋆lue. = the□sky is□blue.
Of course, any nonsense string of letters is also in FM(A); for example,
pqb.a□..□xxu ∈FM(A).
There is an important theorem that characterizes free monoids in terms of
monoid morphisms. If (M, ⋆) and (N, ·) are two monoids, with identities eM
and eN, respectively, then the function f : M →N is a monoid morphism from
(M, ⋆) to (N, ·) if
(i) f (x ⋆y) = f (x) · f (y) for all x, y ∈M.
(ii) f (eM) = eN.
A monoid isomorphism is simply a bijective monoid morphism.
For example, f : (N, +) →(P, ·) deﬁned by f (n) = 2n is a monoid mor-
phism because
f (n + m) = 2n+m = 2n · 2m = f (n) · f (m)
for all m, n ∈N.
However, f : N →N deﬁned by f (x) = x2 is not a monoid morphism from
(N, +) to (N, +). We have f (x + y) = (x + y)2, whereas f (x) + f (y) = x2 +
y2. Hence f (1 + 1) = 4, whereas f (1) + f (1) = 2.
Theorem 7.4. Let (FM(A), ⋆) be the free monoid generated by A and let i: A →
FM(A) be the function that maps each element a of A into the corresponding
word of length 1, so that i(a) = a.
Then if l: A →M is any function into the underlying set of any monoid (M, ·),
there is a unique monoid morphism h: (FM(A), ⋆) →(M, ·) such that h Ž i = l.
This is illustrated in Figure 7.2.
Proof. If h satisﬁes h Ž i = l, then h must be deﬁned on words of length 1 by
h(a) = l(a). Once a morphism has been deﬁned on its generators, it is determined
completely as follows. Let α be a word of length n ⩾2 in FM(A). Write α as
β ⋆c, where β is of length n −1 and c is of length 1. Then we have
h(α) = h(β ⋆c) = h(β) · h(c) = h(β) · l(c). Hence h can be determined by using
induction on the word length. In fact, if α = a1a2 · · · an, where ai ∈A, then
h(α) = l(a1) · l(a2) . . . l(an). Finally, let h() be the identity of M.
□
i
A
M
h
FM(A)
l
Figure 7.2.
The function l factors through the free monoid FM(A).

142
7
MONOIDS AND MACHINES
FINITE-STATE MACHINES
We now look at mathematical models of sequential machines. These are machines
that accept a ﬁnite set of inputs in sequential order. At any one time, the machine
can be in one of a ﬁnite set of internal conﬁgurations or states. There may be
a ﬁnite set of outputs. These outputs and internal states depend not only on the
previous input but also on the stored information in the machine, that is, on the
previous state of the machine. A pushbutton elevator is an example of such a
machine. A digital computer is a very complex ﬁnite-state machine. It can be
broken down into its component parts, each of which is also a machine. The
RS and JK ﬂip-ﬂops, discussed in Exercises 2.69 and 2.70, are examples of two
widely used components.
For simplicity, we only consider machines with a ﬁnite set of inputs and a
ﬁnite set of states. We do not mention any outputs explicitly, because the state set
can be enlarged, if necessary, to include any outputs. The states can be arranged
so that a particular state always gives rise to a certain output.
A ﬁnite-state machine, (S, I, m) consists of a set of states S = {s1, s2, . . . , sn},
a set of input values I = {i1, i2, . . . , it}, and a transition function
m: I × S →S,
which describes how each input value changes the states. If the machine is in
state sp and an input iq is applied, the machine will change to state m(iq, sp).
For example, consider a pushbutton elevator that travels between two levels,
1 and 2, and stops at the lower level 1 when not in use. We take the time for the
elevator to travel from one level to the other to be the basic time interval, and
the controlling machine can change states at the end of each interval. We allow
the machine three inputs, so that I = {0, 1, 2}.
input =



0
if no button is pressed in the preceding time interval
1
if button 1 only is pressed in the preceding time interval
2
if button 2 or both buttons are pressed
in the preceding time interval.
Since the elevator is to stop at the bottom when not in use, we only consider
states that end with the elevator going down. Let the set of states be
S = {stop, down, up–down, down–up–down}.
For example, in the “up–down” state, the elevator is traveling up, but must
remember to come down. If no button is pressed or just button 1 is pressed while it
is going up, the machine will revert to the “down” state when the elevator reaches
level 2. On the other hand, if someone arrives at level 1 and presses button 2, the
machine will change to the “down–up–down” state when the elevator reaches
level 2.

FINITE-STATE MACHINES
143
Up-down
Down
Stop
Down-up-down
1, 2
0, 1
1, 2
0
0
2
0, 1, 2
Figure 7.3.
State diagram of the elevator.
The machine can be pictured by the state diagram in Figure 7.3. If the input i
causes the machine to change from state sp to state sq, we draw an arrow labeled
i from sp to sq in the diagram.
As another example, consider the following machine that checks the parity
of the number of 1’s fed into it. The set of states is S = {start, even, odd}, and
the set of input values is I = {0, 1}. The function m: I × S →S is described by
Table 7.2, and the state diagram is given in Figure 7.4. If any sequence of 0’s
and 1’s is fed into this machine, it will be in the even state if there is an even
number of 1s in the sequence, and in an odd state otherwise.
Let I be the set of input values for any ﬁnite-state machine with state set S
and function m: I × S →S. Each input value deﬁnes a function from the set of
states to itself, the image of any state being the subsequent state produced by the
given input. Hence we have a function
˜m: I →SS,
where SS is the set of functions from S to itself, and ˜m(i): S →S is deﬁned by
[ ˜m(i)](s) = m(i, s).
TABLE 7.2. Transition Function
of the Parity Checker
Next State
Initial
Input
State
0
1
Start
Even
Odd
Even
Even
Odd
Odd
Odd
Even
0
0
1
1
1
0
Even
Odd
Start
Figure 7.4.
State diagram of the parity checker.

144
7
MONOIDS AND MACHINES
i1
i2
i3
ir
•
•
•
Machine
Figure 7.5.
Input sequence being fed into a machine.
Any set of input values can be fed into the machine in sequence. The set
of all such input sequences is the underlying set of the free monoid of input
values, FM(I). By Theorem 7.4, the function ˜m: I →SS can be extended to a
monoid morphism
h: (FM(I), ⋆) →(SS, Ž ),
where h(i1i2 . . . ir) = ˜m(i1) Ž ˜m(i2) Ž · · · Ž ˜m(ir). Note that the input value ir is fed
into the machine ﬁrst, and we can visualize this feeding of the input sequence
in Figure 7.5. (The reader should be aware that many authors use the opposite
convention in which the left input is fed into the machine ﬁrst.)
For example, in the machine that checks the parity of the number of 1s in a
sequence, the state set is S = {start, even, odd} with functions
˜m: {0, 1} →SS
and
h: FM({0, 1}) →SS.
The morphism h is deﬁned by
h(sequence) =



˜m(0)
if the sequence contains an
even number of 1’s
˜m(1)
if the sequence contains an
odd number of 1’s
identity function on S
if the sequence is empty.
QUOTIENT MONOIDS AND THE MONOID OF A MACHINE
We have seen that different input sequences may have the same effect on a
machine. For example, in the machine that checks the parity of the number of
1’s in a sequence,
h(0101101) = h(0000) = h(11) = h(0);
thus the sequences 0101101, 0000, 11, and 0 cannot be distinguished by the
machine.
In any machine with n states, the input sequences can have at most |SS| = nn
different effects. Since there are an inﬁnite number of sequences in FM(I), there
must always be many different input sequences that have the same effect.
The effect that an input has on a ﬁnite-state machine deﬁnes an equivalence
relation on the input monoid FM(I). The monoid of a machine will be the quotient

QUOTIENT MONOIDS AND THE MONOID OF A MACHINE
145
monoid of FM(I) by this relation. It will always be a ﬁnite monoid with, at most,
nn elements. We ﬁrst deﬁne the notion of a quotient monoid.
Suppose that R is an equivalence relation on a monoid (M, ⋆). Then R is
called a congruence relation on M if aRb implies that (a ⋆c)R(b ⋆c) and
(c ⋆a)R(c ⋆b) for all c ∈M. The congruence class containing the element
a ∈M is the set
[a] = {x ∈M|xRa}.
Proposition 7.5. If R is a congruence relation on the monoid (M, ⋆), the quotient
set M/R = {[a]|a ∈M} is a monoid under the operation deﬁned by
[a] ⋆[b] = [a ⋆b].
This monoid is called the quotient monoid of M by R.
Proof. We ﬁrst have to verify that the operation is well deﬁned on congruence
classes. Suppose that [a] = [a′] and [b] = [b′] so that aRa′ and bRb′. Then
(a ⋆b)R(a ⋆b′) and (a ⋆b′)R(a′ ⋆b′). Since R is transitive, (a ⋆b)R(a′ ⋆b′) so
[a ⋆b] = [a′ ⋆b′]. This shows that ⋆is well deﬁned on M/R. The associativity
of ⋆in M/R follows from the associativity of ⋆in M. If e is the identity of M,
then [e] is the identity of M/R. Hence (M/R, ⋆) is a monoid.
□
Let (S, I, m) be a ﬁnite-state machine and let the effect of an input sequence
be given by
h: FM(I) →SS.
Deﬁne the relation R on FM(I) by
αRβ
if and only if
h(α) = h(β).
This is easily veriﬁed to be an equivalence relation. Furthermore, it is a congru-
ence relation on the free monoid (FM(I), ⋆), because if αRβ, then h(α) = h(β),
and h(α ⋆γ ) = h(α) Ž h(γ ) = h(β) Ž h(γ ) = h(β ⋆γ ); thus (α ⋆γ )R(β ⋆γ ), and
similarly, (γ ⋆α)R(γ ⋆β).
The quotient monoid (FM(I)/R, ⋆) is called the monoid of the machine
(S, I, m).
We can apply the same construction to the free semigroup of input sequences
to obtain the semigroup of the machine.
The monoid of a machine reﬂects the capability of the machine to respond
to the input sequences. There are an inﬁnite number of sequences in FM(I),
whereas the number of elements in the quotient monoid is less than or equal to
nn. Two sequences are in the same congruence class if and only if they have the
same effect on the machine.
A morphism theorem for monoids can be proved in a similar way to the
morphism theorem for groups (Theorem 4.25; see Exercise 7.24). Applying this

146
7
MONOIDS AND MACHINES
to the monoid morphism h: FM(I) →SS, it follows that the quotient monoid
FM(I)/R is isomorphic to Im h. This isomorphism assigns to each congruence
class a unique transition between states.
Example 7.6. Draw the state diagram and ﬁnd the monoid of the following
machine (S, I, m). The machine has two states, s0 and s1, and two input sym-
bols, 0 and 1. The effects of the input symbols are given by the functions
h(0), h(1): S →S, deﬁned in Table 7.3.
Solution. Let us calculate the effect of inputs of length 2. We have h(ij) =
h(i) Ž h(j), where j is fed into the machine ﬁrst. It follows from Tables 7.3
and 7.4 that h(00) = h(01) = h(0) and [00] = [01] = [0] in the monoid of the
machine. There are only four functions from {s0, s1} to {s0, s1}, and these are
h(0), h(1), h(10), and h(11). Hence the monoid of the machine consists of the
four congruence classes [0], [1], [10], and [11]. The table of this quotient monoid
is given in Table 7.5, and the state diagram is given in Figure 7.6. For example,
TABLE 7.3
Initial
Next State
State
h(0)
h(1)
s0
s0
s1
s1
s0
s0
TABLE 7.4
Initial
End State
State
h(00)
h(01)
h(10)
h(11)
s0
s0
s0
s1
s0
s1
s0
s0
s1
s1
TABLE 7.5. Monoid of the Machine
⋆
[0]
[1]
[10]
[11]
[0]
[0]
[0]
[0]
[0]
[1]
[10]
[11]
[0]
[1]
[10]
[10]
[10]
[10]
[10]
[11]
[0]
[1]
[10]
[11]
0, 1
s1
s0
1
0
Figure 7.6.
State diagram.

QUOTIENT MONOIDS AND THE MONOID OF A MACHINE
147
[1] ⋆[10] = [110]. Since h(110)(s0) = s0 and h(110)(s1) = s0, it follows that
[110] = [0]. Notice that [11] is the identity; thus, in the monoid of the machine,
[] = [11].
□
Example 7.7. Describe the monoid of the machine ({start, even, odd}, {0, 1}, m)
that determines the parity of the number of 1’s in the input.
Solution. We have already seen that any input sequence with an even number
of 1’s has the same effect as 0 and that any sequence with an odd number of
1’s has the same effect as 1. It follows from Table 7.6 that the monoid of the
machine contains the three elements [], [0], and [1]. The table for this monoid
is given in Table 7.7.
□
Finite-state machines can easily be designed to recognize certain types of input
sequences. For example, most numbers inside a computer are in binary form and
have a check digit attached to them so that there is always an even number of
1’s in each sequence. This is used to detect any machine errors (see Chapter 14).
A ﬁnite-state machine like Example 7.7 can be used to perform a parity check
on all the sequences of numbers in the computer. The machine can be designed
to signal a parity check error whenever it ends in the “odd” state.
Let us now look at a machine that will recognize the pattern 010 in any binary
input sequence that is fed into the machine. Figure 7.7 is the state diagram of
such a machine. If the machine is initiated in state s1, it will be in state s4 if and
only if the preceding inputs were 010, and in this case, the machine sends an
output signal.
This machine has four states; thus the total possible number of different func-
tions between states is 44 = 256. Table 7.8 shows that the input sequences of
length 0, 1, and 2 all have different effects on the various states. However, seven
of the eight sequences of length 3 have the same effect as sequences of length
TABLE 7.6
Initial
Next State
State
h()
h(0)
h(1)
Start
Start
Even
Odd
Even
Even
Even
Odd
Odd
Odd
Odd
Even
TABLE 7.7. Monoid of the Parity Checker Machine
⋆
[]
[0]
[1]
[]
[]
[0]
[1]
[0]
[0]
[0]
[1]
[1]
[1]
[1]
[0]

148
7
MONOIDS AND MACHINES
Sends an
output signal
s4
0
0
0
0
1
s1
s2
s3
1
1
1
Figure 7.7.
State diagram of a machine that recognizes the sequence 010.
TABLE 7.8. Effects of the Input Sequences on the States of the Machine
Initial
End State
State
 0
1 00 01 10 11
000 001 010 011 100 101 110 111 0010 1010
s1
s1 s2 s1 s2 s2 s3 s1
s2
s2
s4
s2
s3
s3
s1
s1
s2
s3
s2
s2 s2 s3 s2 s4 s3 s1
s2
s2
s4
s2
s3
s3
s1
s1
s2
s3
s3
s3 s4 s1 s2 s2 s3 s1
s2
s2
s4
s2
s3
s3
s1
s1
s2
s3
s4
s4 s2 s3 s2 s4 s3 s1
s2
s2
s4
s2
s3
s3
s1
s1
s2
s3
00
0
10
110
000
100
010
001
101
011
111
11
1
0010
1010
01
Λ
Figure 7.8.
Tree diagram of input sequences.
2. The only input sequence with a different effect is 010, the sequence that the
machine is designed to recognize. Therefore, the only sequences of length 4 that
we check are those whose initial inputs are 010, namely, 0010 and 1010.
We can use the tree diagram in Figure 7.8 to check that we have covered all the
possible transition functions obtainable by any input sequence. We label the nodes
of the tree by input sequences. At any node α, there will be two upward branches
ending in the nodes 0 ⋆α and 1 ⋆α, corresponding to the two input symbols. We
prune the tree at node α, if α gives rise to the same transition function as another

EXERCISES
149
TABLE 7.9. Monoid of the Machine That Recognizes 010
⋆
[]
[0]
[1]
[00]
[01]
[10]
[11]
[010]
[]
[]
[0]
[1]
[00]
[01]
[10]
[11]
[010]
[0]
[0]
[00]
[01]
[00]
[00]
[010]
[00]
[00]
[1]
[1]
[10]
[11]
[10]
[10]
[11]
[11]
[10]
[00]
[00]
[00]
[00]
[00]
[00]
[00]
[00]
[00]
[01]
[01]
[010]
[00]
[010]
[010]
[00]
[00]
[010]
[10]
[10]
[10]
[10]
[10]
[10]
[10]
[10]
[10]
[11]
[11]
[11]
[11]
[11]
[11]
[11]
[11]
[11]
[010]
[010]
[010]
[010]
[010]
[010]
[010]
[010]
[010]
node β in the tree. The tree must eventually stop growing because there are only
a ﬁnite number of transition functions. Every input sequence has the same effect
as one of the solid black nodes in Figure 7.8. These nodes provide a complete
set of representatives for the monoid of the machine.
Therefore, the monoid of the machine that recognizes the sequence 010 con-
tains only eight elements: [], [0], [1], [00], [01], [10], [11], and [010], out of a
possible 256 transition functions between states. Its table is given in Table 7.9.
For further reading on the mathematical structure of ﬁnite-state machines and
automata see Hopcroft et al. [18], Kolman [20], or Stone [22].
EXERCISES
Are the structures described in Exercises 7.1 to 7.13 semigroups or monoids or
neither? Give the identity of each monoid.
7.1. (N, gcd).
7.2. (Z, [), where a[b = a.
7.3. (R, ⋆), where x ⋆y =

x2 + y2.
7.4. (R, ⋆), where x ⋆y =
3
x3 + y3.
7.5. (Z3, −).
7.6. (R, | |), where | | is the absolute value.
7.7. (Z, max), where max (m, n) is the larger of m and n.
7.8. (Z, ⋆), where x ⋆y = x + y + xy.
7.9. (S, gcd), where S = {1, 2, 3, 4, 5, 6}.
7.10. (X, max), where X is the set of real-valued functions on the unit interval
[0,1] and if f, g ∈X, then max (f, g) is the function on X deﬁned by
max(f, g)(x) = max(f (x), g(x)).
7.11. (T , lcm) where T = {1, 2, 4, 5, 10, 20}.

150
7
MONOIDS AND MACHINES
7.12. The set of all relations on a set X, where the composition of two relations
R and S is the relation RS deﬁned by xRSz if and only if for some y ∈X,
xRy and ySz.
7.13. ({a, b, c}, ⋆), where the table for ⋆is given in Table 7.10.
TABLE 7.10
⋆
a
b
c
a
a
b
c
b
b
a
a
c
c
a
a
Write out the tables for the monoids and semigroups described in Exercises 7.14
to 7.17.
7.14. (S, gcd), where S = {1, 2, 3, 4, 6, 8, 12, 24}.
7.15. (T , gcd), where T = {1, 2, 3, 4}.
7.16. (XX, Ž ), where X = {1, 2, 3}.
7.17. ({e, c, c2, c3, c4}, ·), where multiplication by c is indicated by an arrow in
Figure 7.9.
c4
c2
c3
c
e
Figure 7.9
7.18. Find all the commutative monoids on the set S = {e, a, b} with identity e.
7.19. Are all the elements of the free semigroup generated by {0, 1, 2, 3, 4, 5, 6,
7, 8, 9} simply the nonnegative integers written in the base 10?
7.20. A submonoid of a monoid (M, ·) is a subset N of M containing the
identity and such that x · y ∈N, for all x, y ∈N. Find all the submonoids
of the monoid given in Exercise 7.17.
7.21. Prove that there is a monoid isomorphism between (FM({a}), ⋆) and (N, +).
7.22. (Representation theorem for monoids) Prove that any monoid (M, ⋆) is
isomorphic to a submonoid of (MM, Ž ). This gives a representation of any
monoid as a monoid of transformations.
7.23. Prove that any cyclic monoid is either isomorphic to (N, +) or is isomor-
phic to a monoid of the form shown in Figure 7.1, for some values of k
and m.
7.24. (Morphism theorem for monoids) Let f : (M, ⋆) →(N, ·) be a morphism
of monoids. Let R be the relation on M deﬁned by m1Rm2 if and only if

EXERCISES
151
f (m1) = f (m2). Prove that the quotient monoid (M/R, ⋆) is isomorphic
to the submonoid (Imf, ·) of (N, ·). (See Exercise 7.20.)
7.25. An automorphism of a monoid M is an isomorphism from M to itself.
Prove that the set of all automorphisms of a monoid M forms a group
under composition.
7.26. A machine has three states, s1, s2, and s3 and two input symbols, α and β.
The effect of the input symbols on the states is given by Table 7.11. Draw
the state diagram and ﬁnd the monoid of this machine.
TABLE 7.11
Initial
Next State
State
h(α)
h(β)
s1
s1
s1
s2
s3
s1
s3
s2
s1
7.27. Prove that every ﬁnite monoid is the monoid of some ﬁnite-state machine.
For Exercises 7.28 to 7.30, draw state diagrams of machines with the given input
set, I, that will recognize the given sequence.
7.28. 1101, where I = {0, 1}.
7.29. 0101, where I = {0, 1}.
7.30. 2131, where I = {1, 2, 3}.
Which of the relations described in Exercises 7.31 to 7.34 are congruence rela-
tions on the monoid (N, +)? Find the quotient monoid when the relation is a
congruence relation.
7.31. aRb if a −b is even.
7.32. aRb if a > b.
7.33. aRb if a = 2rb for some r ∈Z.
7.34. aRb if 10|(a −b).
The machines in Tables 7.12, 7.13, and 7.14 have state set S = {s1, s2, s3} and
input set I = {0, 1}.
7.35. Draw the table of the monoid of the machine deﬁned by Table 7.12.
TABLE 7.12
Initial
Next State
State
h(0)
h(1)
s1
s2
s1
s2
s1
s2
s3
s3
s2

152
7
MONOIDS AND MACHINES
7.36. Draw the table of the monoid of the machine deﬁned by Table 7.13.
TABLE 7.13
Initial
Next State
State
h(0)
h(1)
s1
s2
s1
s2
s3
s1
s3
s3
s2
7.37. Find the number of elements in the monoid of the machine deﬁned by
Table 7.14.
TABLE 7.14
Initial
Next State
State
h(0)
h(1)
s1
s2
s1
s2
s3
s3
s3
s1
s1
7.38. Find the number of elements in the semigroup of the machine, given by
Figure 7.3, that controls the elevator.
7.39. Find the monoid of the machine in Figure 7.10.
s1
s2
s3
a, b
a, b
b
g
g
g
a
Figure 7.10
7.40. A serial adder, illustrated in Figure 7.11, is a machine that adds two
numbers in binary form. The two numbers are fed in together, one digit
at a time, starting from the right end. Their sum appears as the output.
The machine has input symbols 00, 01, 10, and 11, corresponding to the
rightmost digits of the numbers. Figure 7.12 gives the state diagram of
such a machine, where the symbol “sij/j” indicates that the machine is in
state sij and emits an output j. The carry digit is the number i of the state
sij. Find the monoid of this machine.

EXERCISES
153
Serial
adder
Figure 7.11
s00 0
0
s10
s01
s11
1
1
11
11
00
10, 01
10, 01
00
00
10, 01
11
11
00
10, 01
Figure 7.12.
State diagram of the serial adder.
The circuits in Exercises 7.41 to 7.44 represent the internal structures of some
ﬁnite-state machines constructed from transistor circuits. These circuits are con-
trolled by a clock, and the rectangular boxes denote delays of one time unit. The
input symbols are 0 and 1 and are fed in at unit time intervals. The internal
states of the machines are described by the contents of the delays. Draw the state
diagram and ﬁnd the elements in the semigroup of each machine.
7.41.
Delay
y
AND
7.42.
Delay
y2
Delay
y1
OR
NAND
7.43.
Delay
y1
Delay
y2
AND
7.44.
Delay
y2
Delay
y1
NOR

154
7
MONOIDS AND MACHINES
7.45. In the spring, a plant bud has to have the right conditions in order to
develop. One particular bud has to have a rainy day followed by two
warm days, without being interrupted by cool or freezing days, in order to
develop. Furthermore, if a freezing day occurs after the bud has developed,
the bud dies. Draw a state diagram for such a bud using the input symbols
R, W, C, F to stand for rainy, warm, cool, and freezing days, respectively.
What is the number of elements in the resulting monoid of this bud?
7.46. A dog can either be passive, angry, frightened, or angry and frightened,
in which case he bites. If you give him a bone, he becomes passive. If
you remove one of his bones, he becomes angry, and, if he is already
frightened, he will bite you. If you threaten him, he becomes frightened,
but, if he is already angry, he will bite. Write out the table of the monoid
of the dog.

8
RINGS AND FIELDS
The familiar number systems of the real or complex numbers contain two basic
binary operations, addition and multiplication. Group theory is not sufﬁcient to
capture all of the algebraic structure of these number systems, because a group
deals with only one binary operation. It is possible to consider the integers as a
group (Z, +) and the nonzero integers as a monoid (Z∗, ·), but this still neglects
the relation between addition and multiplication, namely, the fact that multiplica-
tion is distributive over addition. We therefore consider algebraic structures with
two binary operations modeled after these number systems. A ring is a structure
that has the minimal properties we would expect of addition and multiplication.
A ﬁeld is a more specialized ring in which division by nonzero elements is
always possible.
In this chapter we look at the basic properties of rings and ﬁelds and con-
sider many examples. In later chapters we construct new number systems with
properties similar to the familiar systems.
RINGS
A ring (R, +, ·) is a set R, together with two binary operations + and · on R
satisfying the following axioms. For any elements a, b, c ∈R,
(i) (a + b) + c = a + (b + c).
(associativity of addition)
(ii) a + b = b + a.
(commutativity of addition)
(iii) there exists 0 ∈R, called the
zero, such that a + 0 = a.
(existence of an additive identity)
(iv) there exists (−a) ∈R such that
a + (−a) = 0.
(existence of an additive inverse)
(v) (a · b) · c = a · (b · c).
(associativity of multiplication)
(vi) there exists 1 ∈R such that
1 · a = a · 1 = a.
(existence of multiplicative identity)
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
155

156
8
RINGS AND FIELDS
(vii) a · (b + c) = a · b + a · c and
(b + c) · a = b · a + c · a.
(distributivity)
Axioms (i)–(iv) are equivalent to saying that (R, +) is an abelian group, and
axioms (v) and (vi) are equivalent to saying that (R, ·) is a monoid.
The ring (R, +, ·) is called a commutative ring if, in addition,
(viii) a · b = b · a for all a, b ∈R.
(commutativity of multiplication)
The integers under addition and multiplication satisfy all of the axioms above,
so that (Z, +, ·) is a commutative ring. Also, (Q, +, ·), (R, +, ·), and (C, +, ·)
are all commutative rings. If there is no confusion about the operations, we write
only R for the ring (R, +, ·). Therefore, the rings above would be referred to as
Z, Q, R, or C. Moreover, if we refer to a ring R without explicitly deﬁning its
operations, it can be assumed that they are addition and multiplication.
Many authors do not require a ring to have a multiplicative identity, and
most of the results we prove can be veriﬁed to hold for these objects as well.
Exercise 8.49 shows that such an object can always be embedded in a ring that
does have a multiplicative identity.
The set of all n × n square matrices with real coefﬁcients forms a ring
(Mn(R), +, ·), which is not commutative if n > 1, because matrix multiplication
is not commutative.
The elements “even” and “odd” form a commutative ring ({even, odd}, +, ·)
where the operations are given by Table 8.1. “Even” is the zero of this ring, and
“odd” is the multiplicative identity. This is really a special case of the following
example when n = 2.
Example 8.1. Show that (Zn, +, ·) is a commutative ring, where addition and
multiplication on congruence classes, modulo n, are deﬁned by the equations
[x] + [y] = [x + y] and [x] · [y] = [xy].
Solution. It follows from Example 4.19, that (Zn, +) is an abelian group.
Since multiplication on congruence classes is deﬁned in terms of represen-
tatives, it must be veriﬁed that it is well deﬁned. Suppose that [x] = [x′] and
[y] = [y′], so that x ≡x′ and y ≡y′ mod n. This implies that x = x′ + kn
and y = y′ + ln for some k, l ∈Z. Now x · y = (x′ + kn) · (y′ + ln) = x′ · y′ +
(ky′ + lx′ + kln)n, so x · y ≡x′ · y′ mod n and hence [x · y] = [x′ · y′]. This
shows that multiplication is well deﬁned.
TABLE 8.1. Ring of Odd and Even Integers
+
Even
Odd
Even
even
odd
Odd
odd
even
·
Even
Odd
Even
even
even
Odd
even
odd

RINGS
157
The remaining axioms now follow from the deﬁnitions of addition and mul-
tiplication and from the properties of the integers. The zero is [0], and the unit
is [1]. The left distributive law is true, for example, because
[x] · ([y] + [z]) = [x] · [y + z] = [x · (y + z)]
= [x · y + x · z]
by distributivity in Z
= [x · y] + [x · z] = [x] · [y] + [x] · [z].
□
Example 8.2. Construct the addition and multiplication tables for the ring
(Z5, +, ·).
Solution. We denote the congruence class [x] just by x. The tables are given
in Table 8.2.
□
Example 8.3. Show that (Q(
√
2), +, ·) is a commutative ring where Q(
√
2) =
{a + b
√
2 ∈R|a, b ∈Q}.
Solution. The set Q(
√
2) is a subset of R, and the addition and multiplication
is the same as that of real numbers. First, we check that + and · are binary
operations on Q(
√
2). If a, b, c, d ∈Q, we have
(a + b
√
2) + (c + d
√
2) = (a + c) + (b + d)
√
2 ∈Q(
√
2)
since (a + c) and (b + d) ∈Q. Also,
(a + b
√
2) · (c + d
√
2) = (ac + 2bd) + (ad + bc)
√
2 ∈Q(
√
2)
since (ac + 2bd) and (ad + bc) ∈Q.
We now check that axioms (i)–(viii) of a commutative ring are valid in Q(
√
2).
(i) Addition of real numbers is associative.
(ii) Addition of real numbers is commutative.
(iii) The zero is 0 = 0 + 0
√
2 ∈Q(
√
2).
(iv) The additive inverse of a + b
√
2 is (−a) + (−b)
√
2 ∈Q(
√
2), since
(−a) and (−b) ∈Q.
TABLE 8.2. Ring (Z5, +, ·)
+
0
1
2
3
4
0
0
1
2
3
4
1
1
2
3
4
0
2
2
3
4
0
1
3
3
4
0
1
2
4
4
0
1
2
3
·
0
1
2
3
4
0
0
0
0
0
0
1
0
1
2
3
4
2
0
2
4
1
3
3
0
3
1
4
2
4
0
4
3
2
1

158
8
RINGS AND FIELDS
(v) Multiplication of real numbers is associative.
(vi) The multiplicative identity is 1 = 1 + 0
√
2 ∈Q(
√
2).
(vii) The distributive axioms hold for real numbers and hence hold for ele-
ments of Q(
√
2).
(viii) Multiplication of real numbers is commutative.
□
We have already investigated one algebraic system with two binary operations:
a boolean algebra. The boolean algebra of subsets of a set is not a ring under
the operations of union and intersection, because neither of these operations has
inverses. However, the symmetric difference does have an inverse, and we can
make a boolean algebra into a ring using this operation and the operation of
intersection.
Example 8.4. (P(X), , ∩) is a commutative ring for any set X.
Solution. The axioms (i)–(viii) of a commutative ring follow from Proposi-
tions 2.1 and 2.3. The zero is Ø, and the identity is X.
□
In the ring above, A ∩A = A for every element A in the ring. Such rings
are called boolean rings, since they are all derivable from boolean algebras (see
Exercise 8.13).
Example 8.5. Construct the tables for the ring (P(X), , ∩), where X =
{a, b, c}.
Solution. Let A = {a}, B = {b}, and C = {c}, so that A = {b, c}, B = {a, c},
and C = {a, b}. Therefore, P(X) = {Ø, A, B, C, A, B, C, X}. The tables for the
symmetric difference and intersection are given in Table 8.3.
□
The following properties are useful in manipulating elements of any ring.
Proposition 8.6. If (R, +, ·) is a ring, then for all a, b ∈R:
(i) a · 0 = 0 · a = 0.
(ii) a · (−b) = (−a) · b = −(a · b).
(iii) (−a) · (−b) = a · b.
(iv) (−1) · a = −a.
(v) (−1) · (−1) = 1.
Proof. (i) By distributivity, a · 0 = a · (0 + 0) = a · 0 + a · 0. Adding
−(a · 0) to each side, we obtain 0 = a · 0. Similarly, 0 · a = 0.
(ii) Compute a · (−b) + a · b = a · (−b + b) = a · 0 = 0, using (i). Therefore,
a · (−b) = −(a · b). Similarly, (−a) · b = −(a · b).
(iii) We have (−a) · (−b) = −(a · (−b)) = −(−(a · b)) = a · b by (ii) and
Proposition 3.7.

INTEGRAL DOMAINS AND FIELDS
159
TABLE 8.3. Ring P({a, b, c})

Ø
A
B
C
A
B
C
X
Ø
Ø
A
B
C
A
B
C
X
A
A
Ø
C
B
X
C
B
A
B
B
C
Ø
A
C
X
A
B
C
C
B
A
Ø
B
A
X
C
A
A
X
C
B
Ø
C
B
A
B
B
C
X
A
C
Ø
A
B
C
C
B
A
X
B
A
Ø
C
X
X
A
B
C
A
B
C
Ø
∩
Ø
A
B
C
A
B
C
X
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Ø
Ø
A
Ø
A
Ø
Ø
Ø
A
A
A
B
Ø
Ø
B
Ø
B
Ø
B
B
C
Ø
Ø
Ø
C
C
C
Ø
C
A
Ø
Ø
B
C
A
C
B
A
B
Ø
A
Ø
C
C
B
A
B
C
Ø
A
B
Ø
B
A
C
C
X
Ø
A
B
C
A
B
C
X
(iv) By (ii), (−1) · a = −(1 · a) = −a.
(v) By (iii), (−1) · (−1) = 1 · 1 = 1.
□
Proposition 8.7. If 0 = 1, the ring contains only one element and is called the
trivial ring. All other rings are called nontrivial.
Proof. For any element, a, in a ring in which 0 = 1, we have a = a · 1 =
a · 0 = 0. Therefore, the ring contains only the element 0. It can be veriﬁed that
this forms a ring with the operations deﬁned by 0 + 0 = 0 and 0 · 0 = 0.
□
INTEGRAL DOMAINS AND FIELDS
One very useful property of the familiar number systems is the fact that if ab = 0,
then either a = 0 or b = 0. This property allows us to cancel nonzero elements
because if ab = ac and a ̸= 0, then a(b −c) = 0, so b = c. However, this prop-
erty does not hold for all rings. For example, in Z4, we have [2] · [2] = [0], and
we cannot always cancel since [2] · [1] = [2] · [3], but [1] ̸= [3].
If (R, +, ·) is a commutative ring, a nonzero element a ∈R is called a zero
divisor if there exists a nonzero element b ∈R such that a · b = 0. A nontrivial
commutative ring is called an integral domain if it has no zero divisors. Hence

160
8
RINGS AND FIELDS
a nontrivial commutative ring is an integral domain if a · b = 0 always implies
that a = 0 or b = 0.
As the name implies, the integers form an integral domain. Also, Q, R, and C
are integral domains. However, Z4 is not, because [2] is a zero divisor. Neither
is (P(X), , ∩), because every nonempty proper subset of X is a zero divisor.
Mn(R) is not an integral domain (for example,

0
1
0
0
2
= 0).
Proposition 8.8. If a is a nonzero element of an integral domain R and
a · b = a · c, then b = c.
Proof. If a · b = a · c, then a · (b −c) = a · b −a · c = 0. Since R is an inte-
gral domain, it has no zero divisors. Since a ̸= 0, it follows that (b −c) = 0.
Hence b = c.
□
Generally speaking, it is possible to add, subtract, and multiply elements in a
ring, but it is not always possible to divide. Even in an integral domain, where
elements can be canceled, it is not always possible to divide by nonzero elements.
For example, if x, y ∈Z, then 2x = 2y implies that x = y, but not all elements
in Z can be divided by 2.
The most useful number systems are those in which we can divide by nonzero
elements. A ﬁeld is a ring in which the nonzero elements form an abelian group
under multiplication. In other words, a ﬁeld is a nontrivial commutative ring R
satisfying the following extra axiom.
(ix) For each nonzero element a ∈R there exists a−1 ∈R such that a · a−1 = 1.
The rings Q, R, and C are all ﬁelds, but the integers do not form a ﬁeld.
Proposition 8.9. Every ﬁeld is an integral domain; that is, it has no zero divisors.
Proof. Let a · b = 0 in a ﬁeld F. If a ̸= 0, there exists an inverse a−1 ∈F
and b = (a−1 · a) · b = a−1(a · b) = a−1 · 0 = 0. Hence either a = 0 or b = 0,
and F is an integral domain.
□
Theorem 8.10. A ﬁnite integral domain is a ﬁeld.
Proof. Let D = {x0, x1, x2, . . . , xn} be a ﬁnite integral domain with x0 as 0
and x1 as 1. We have to show that every nonzero element of D has a multiplica-
tive inverse.
If xi is nonzero, we show that the set xiD = {xix0, xix1, xix2, . . . , xixn} is the
same as the set D. If xixj = xixk, then, by the cancellation property, xj = xk. Hence
all the elements xix0, xix1, xix2, . . . , xixn are distinct, and xiD is a subset of D with
the same number of elements. Therefore, xiD = D. But then there is some element,
xj, such that xixj = x1 = 1. Hence xj = x−1
i , and D is a ﬁeld.
□

SUBRINGS AND MORPHISMS OF RINGS
161
Note that Z is an inﬁnite integral domain that is not a ﬁeld.
Theorem 8.11. Zn is a ﬁeld if and only if n is prime.
Proof. Suppose that n is prime and that [a] · [b] = [0] in Zn. Then n|ab. So
n|a or n|b by Euclid’s Lemma (Theorem 12, Appendix 2). Hence [a] = [0] or
[b] = [0], and Zn is an integral domain. Since Zn is also ﬁnite, it follows from
Theorem 8.10 that Zn is a ﬁeld.
Suppose that n is not prime. Then we can write n = rs, where r and s are
integers such that 1 < r < n and 1 < s < n. Now [r] ̸= [0] and [s] ̸= [0] but
[r] · [s] = [rs] = [0]. Therefore, Zn has zero divisors and hence is not a ﬁeld.
□
Example 8.12. Is (Q(
√
2), +, ·) an integral domain or a ﬁeld?
Solution. From Example 8.3 we know that Q(
√
2) is a commutative ring. Let
a + b
√
2 be a nonzero element, so that at least one of a and b is not zero. Hence
a −b
√
2 ̸= 0 (because
√
2 is not in Q), so we have
1
a + b
√
2
=
a −b
√
2
(a + b
√
2)(a −b
√
2)
=
a
a2 −2b2 −

b
a2 −2b2
 √
2.
This is an element of Q(
√
2), and so is the inverse of a + b
√
2. Hence Q(
√
2)
is a ﬁeld (and an integral domain).
□
SUBRINGS AND MORPHISMS OF RINGS
If (R, +, ·) is a ring, a nonempty subset S of R is called a subring of R if for
all a, b ∈S:
(i) a + b ∈S.
(ii) −a ∈S.
(iii) a · b ∈S.
(iv) 1 ∈S.
Conditions (i) and (ii) imply that (S, +) is a subgroup of (R, +) and can be
replaced by the condition a −b ∈S.
Proposition 8.13. If S is a subring of (R, +, ·), then (S, +, ·) is a ring.
Proof. Conditions (i) and (iii) of the deﬁnition above guarantee that S is closed
under addition and multiplication. Condition (iv) shows that 1 ∈S. It follows
from Proposition 3.8 that (S, +) is a group. (S, +, ·) satisﬁes the remaining
axioms for a ring because they hold in (R, +, ·).
□

162
8
RINGS AND FIELDS
For example, Z, Q, and R are all subrings of C. Let D be the set of n × n
real diagonal matrices. Then D is a subring of the ring of all n × n real matri-
ces, Mn(R), because the sum, difference, and product of two diagonal matrices
is another diagonal matrix. Note that D is commutative even though Mn(R)
is not.
Example 8.14. Show that Q(
√
2) = {a + b
√
2|a, b ∈Q} is a subring of R.
Solution. Let a + b
√
2, c + d
√
2 ∈Q(
√
2). Then
(i) (a + b
√
2) + (c + d
√
2) = (a + c) + (b + d)
√
2 ∈Q(
√
2).
(ii) −(a + b
√
2) = (−a) + (−b)
√
2 ∈Q(
√
2).
(iii) (a + b
√
2) · (c + d
√
2) = (ac + 2bd) + (ad + bc)
√
2 ∈Q(
√
2).
(iv) 1 = 1 + 0
√
2 ∈Q(
√
2).
□
A morphism between two rings is a function between their underlying sets
that preserves the two operations of addition and multiplication and also the
element 1. Many authors use the term homomorphism instead of morphism.
More precisely, let (R, +, ·) and (S, +, ·) be two rings. The function f : R →S
is called a ring morphism if for all a, b ∈R:
(i) f (a + b) = f (a) + f (b).
(ii) f (a · b) = f (a) · f (b).
(iii) f (1) = 1.
If the operations in the two rings are denoted by different symbols, for
example, if the rings are (R, +, ·) and (S, ⊕,Ž), then the conditions for
f : R →S to be a ring morphism are:
(i) f (a + b) = f (a) ⊕f (b).
(ii) f (a · b) = f (a)Žf (b).
(iii) f (1R) = 1S where 1R and 1S are the respective identities.
A ring isomorphism is a bijective ring morphism. If there is an isomorphism
between the rings R and S, we say R and S are isomorphic rings and write
R ∼= S.
A ring morphism, f , from (R, +, ·) to (S, +, ·) is, in particular, a group
morphism from (R, +) to (S, +). Therefore, by Proposition 3.19, f (0) = 0 and
f (−a) = −f (a) for all a ∈R.
The inclusion function, i: S →R, of any subring S into a ring R is always
a ring morphism. The function f : Z →Zn, deﬁned by f (x) = [x], which maps
an integer to its equivalence class modulo n, is a ring morphism from (Z, +, ·)
to (Zn, +, ·).

SUBRINGS AND MORPHISMS OF RINGS
163
Example 8.15. If X is a one element set, show that f : P(X) →Z2 is a
ring isomorphism between (P(X), , ∩) and (Z2, +, ·), where f (Ø) = [0] and
f (X) = [1].
Solution. We can check that f is a morphism by testing all the possibilities
for f (AB) and f (A ∩B). Since the rings are commutative, they are
f (ØØ) = f (Ø) = [0] = f (Ø) + f (Ø)
f (ØX) = f (X) = [1] = f (Ø) + f (X)
f (XX) = f (Ø) = [0] = f (X) + f (X)
f (Ø ∩Ø) = f (Ø) = [0] = f (Ø) · f (Ø)
f (Ø ∩X) = f (Ø) = [0] = f (Ø) · f (X)
f (X ∩X) = f (X) = [1] = f (X) · f (X).
Both rings contain only two elements, and f is a bijection; therefore, f is an
isomorphism.
□
If f : R →S is an isomorphism between two ﬁnite rings, the addition and
multiplication tables of S will be the same as those of R if we replace each
a ∈R by f (a) ∈S. For example, Tables 8.4 and 8.5 illustrate the isomorphism
of Example 8.15.
The following ring isomorphism between linear transformations and matrices
is the crux of much of linear algebra.
Example 8.16. The linear transformations from Rn to itself form a ring,
(L(Rn, Rn), +, Ž ), under addition and composition. Show that the function
f : L(Rn, Rn) →Mn(R)
TABLE 8.4. Ring P(X)
When X Is a Point

Ø
X
Ø
Ø
X
X
X
Ø
∩
Ø
X
Ø
Ø
Ø
X
Ø
X
TABLE 8.5. Ring Z2
+
[0]
[1]
[0]
[0]
[1]
[1]
[1]
[0]
·
[0]
[1]
[0]
[0]
[0]
[1]
[0]
[1]

164
8
RINGS AND FIELDS
is a ring morphism, where f assigns to each linear transformation its standard
matrix, that is, its n × n coefﬁcient matrix with respect to the standard basis
of Rn.
Solution. If α is a linear transformation from Rn to itself, then
α


x1
...
xn

=


a11x1 + · · · + a1nxn
...
...
an1x1 + · · · + annxn

and f (α) =


a11 . . . a1n
...
...
an1 . . . ann

.
Matrix addition and multiplication is deﬁned so that f (α + β) = f (α) + f (β)
and f (α Ž β) = f (α) · f (β). Also, if ι is the identity linear transformation then
f (ι) is the identity matrix.
Any matrix deﬁnes a linear transformation, so that f is surjective. Fur-
thermore, f is injective, because any matrix can arise from only one linear
transformation. In fact, the jth column of the matrix must be the image of the
jth basis vector. Hence f is an isomorphism.
□
Example 8.17. Show that f : Z24 →Z4, deﬁned by f ([x]24) = [x]4 is a ring
morphism.
Proof. Since the function is deﬁned in terms of representatives of equiva-
lence classes, we ﬁrst check that it is well deﬁned. If [x]24 = [y]24,
then x ≡y mod 24 and 24|(x −y). Hence 4|(x −y) and [x]4 = [y]4, which
shows that f is well deﬁned.
We now check the conditions for f to be a ring morphism.
(i) f ([x]24 + [y]24) = f ([x + y]24) = [x + y]4 = [x]4 + [y]4.
(ii) f ([x]24 · [y]24) = f ([xy]24) = [xy]4 = [x]4 · [y]4.
(iii) f ([1]24) = [1]4.
□
NEW RINGS FROM OLD
This section introduces various methods for constructing new rings from given
rings. These include the direct product of rings, matrix rings, polynomial rings,
rings of sequences, and rings of formal power series. Perhaps the most impor-
tant class of rings constructible from given rings is the class of quotient rings.
Their construction is analogous to that of quotient groups and is discussed in
Chapter 10.
If (R, +, ·) and (S, +, ·) are two rings, their product is the ring (R × S, +, ·)
whose underlying set is the cartesian product of R and S and whose operations
are deﬁned component-wise by
(r1, s1) + (r2, s2) = (r1 + r2, s1 + s2)
and
(r1, s1) · (r2, s2) = (r1 · r2, s1 · s2).

NEW RINGS FROM OLD
165
It is readily veriﬁed that these operations do indeed deﬁne a ring structure on
R × S whose zero is (0R, 0S), where 0R and 0S are the zeros of R and S, and
whose multiplicative identity is (1R, 1S), where 1R and 1S are the identities in R
and S.
The product construction can be iterated any number of times. For example,
(Rn, +, ·) is a commutative ring, where Rn is the n-fold cartesian product of R
with itself.
Example 8.18. Write down the addition and multiplication tables for Z2 × Z3.
Solution. Let Z2 = {0, 1} and Z3 = {0, 1, 2}. Then Z2 × Z3 = {(0, 0), (0, 1),
(0, 2), (1, 0), (1, 1), (1, 2)}. The addition and multiplication tables are given in
Table 8.6. In calculating these, it must be remembered that addition and mul-
tiplication are performed modulo 2 in the ﬁrst coordinate and modulo 3 in the
second coordinate.
□
We know that Z2 × Z3 and Z6 are isomorphic as groups; we now show that
they are isomorphic as rings.
Theorem 8.19. Zm × Zn is isomorphic as a ring to Zmn if and only if
gcd(m, n) = 1.
Proof. If
gcd(m, n) = 1, it follows from Theorems 4.32 and 3.20 that the
function
f : Zmn →Zm × Zn
TABLE 8.6. Ring Z2 × Z3
+
(0, 0)
(0, 1)
(0, 2)
(1, 0)
(1, 1)
(1, 2)
(0, 0)
(0, 0)
(0, 1)
(0, 2)
(1, 0)
(1, 1)
(1, 2)
(0, 1)
(0, 1)
(0, 2)
(0, 0)
(1, 1)
(1, 2)
(1, 0)
(0, 2)
(0, 2)
(0, 0)
(0, 1)
(1, 2)
(1, 0)
(1, 1)
(1, 0)
(1, 0)
(1, 1)
(1, 2)
(0, 0)
(0, 1)
(0, 2)
(1, 1)
(1, 1)
(1, 2)
(1, 0)
(0, 1)
(0, 2)
(0, 0)
(1, 2)
(1, 2)
(1, 0)
(1, 1)
(0, 2)
(0, 0)
(0, 1)
·
(0, 0)
(0, 1)
(0, 2)
(1, 0)
(1, 1)
(1, 2)
(0, 0)
(0, 0)
(0, 0)
(0, 0)
(0, 0)
(0, 0)
(0, 0)
(0, 1)
(0, 0)
(0, 1)
(0, 2)
(0, 0)
(0, 1)
(0, 2)
(0, 2)
(0, 0)
(0, 2)
(0, 1)
(0, 0)
(0, 2)
(0, 1)
(1, 0)
(0, 0)
(0, 0)
(0, 0)
(1, 0)
(1, 0)
(1, 0)
(1, 1)
(0, 0)
(0, 1)
(0, 2)
(1, 0)
(1, 1)
(1, 2)
(1, 2)
(0, 0)
(0, 2)
(0, 1)
(1, 0)
(1, 2)
(1, 1)

166
8
RINGS AND FIELDS
deﬁned by f ([x]mn) = ([x]m, [x]n) is a group isomorphism. However, this func-
tion also preserves multiplication because
f ([x]mn · [y]mn) = f ([xy]mn) = ([xy]m, [xy]n) = ([x]m[y]m, [x]n[y]n)
= ([x]m, [x]n) · ([y]m, [y]n) = f ([x]mn) · f ([y]mn).
Also, f ([1]mn) = ([1]m, [1]n); thus f is a ring isomorphism.
It was shown in the discussion following Corollary 4.33 that if gcd(m, n) ̸=
1, Zm × Zn and Zmn are not isomorphic as groups, and hence they cannot be
isomorphic as rings.
□
We can extend this result by induction to show the following.
Theorem 8.20. Let m = m1 · m2 · · · mr, where gcd(mi, mj) = 1 if i ̸= j. Then
Zm1 × Zm2 × · · · × Zmr is a ring isomorphic to Zm.
Corollary 8.21. Let n = pα1
1 pα2
2 · · · pαr
r be a decomposition of the integer n into
powers of distinct primes. Then Zn ∼= Zp
α1
1 × Zp
α2
2 × · · · × Zpαr
r
as rings.
If R is a commutative ring, we can construct the ring of n × n matrices with
entries from R, (Mn(R), +, ·). Addition and multiplication are performed as in
real matrices.
For example, (Mn(Z2), +, ·) is the ring of n × n matrices with 0 and 1 entries.
Addition and multiplication is performed modulo 2. This is a noncommutative
ring with 2(n2) elements.
If R is a commutative ring, a polynomial p(x) in the indeterminate x over
the ring R is an expression of the form
p(x) = a0 + a1x + a2x2 + · · · + anxn,
where a0, a1, a2, . . . , an ∈R and n ∈N. The element ai is called the coefﬁcient
of xi in p(x). If the coefﬁcient of xi is zero, the term 0xi may be omitted, and
if the coefﬁcient of xi is one, 1xi may be written simply as xi.
Two polynomials f (x) and g(x) are called equal when they are identical, that
is, when the coefﬁcient of xn is the same in each polynomial for every n ⩾0.
In particular,
a0 + a1x + a2x2 + · · · + anxn = 0
is the zero polynomial if and only if a0 = a1 = a2 = · · · = an = 0.
If n is the largest integer for which an ̸= 0, we say that p(x) has degree n and
write degp(x) = n. If all the coefﬁcients of p(x) are zero, then p(x) is called
the zero polynomial, and its degree is not deﬁned.
For example, 4x2 −
√
3 is a polynomial over R of degree 2, ix4 −(2 + i)x3 +
3x is a polynomial over C of degree 4, and x7 + x5 + x4 + 1 is a polynomial
over Z2 of degree 7. The number 5 is a polynomial over Z of degree 0; the zero

NEW RINGS FROM OLD
167
polynomial and the polynomials of degree 0 are called constant polynomials
because they contain no x terms.
The set of all polynomials in x with coefﬁcients from the commutative ring
R is denoted by R[x]. That is,
R[x] = {a0 + a1x + a2x2 + · · · + anxn|ai ∈R, n ∈N}.
This forms a ring (R[x], +, ·) called the polynomial ring with coefﬁcients from
R when addition and multiplication of the polynomials
p(x) =
n
	
i=0
aixi
and
q(x) =
m
	
i=0
bixi
are deﬁned by
p(x) + q(x) =
max(m,n)
	
i=0
(ai + bi)xi
and
p(x) · q(x) =
m+n
	
k=0
ckxk
where
ck =
	
i+j=k
aibj.
With a little effort, it can be veriﬁed that (R[x], +, ·) satisﬁes all the axioms
for a commutative ring. The zero is the zero polynomial, and the multiplicative
identity is the constant polynomial 1.
For example, in Z5[x], the polynomial ring with coefﬁcients in the integers
modulo 5, we have
(2x3 + 2x2 + 1) + (3x2 + 4x + 1) = 2x3 + 4x + 2
and
(2x3 + 2x2 + 1) · (3x2 + 4x + 1) = x5 + 4x4 + 4x + 1.
When working in Zn[x], the coefﬁcients, but not the exponents, are reduced
modulo n.
Proposition 8.22. If R is an integral domain and p(x) and q(x) are nonzero
polynomials in R[x], then
deg(p(x) · q(x)) = deg p(x) + deg q(x).
Proof. Let deg p(x) = n, deg q(x) = m and let p(x) = a0 + · · · + anxn,
q(x) = b0 + · · · + bmxm, where an ̸= 0, bm ̸= 0. Then the coefﬁcient of the high-
est power of x in p(x) · q(x) is anbm, which is nonzero since R has no zero
divisors. Hence deg(p(x) · q(x)) = m + n.
□

168
8
RINGS AND FIELDS
If the coefﬁcient ring is not an integral domain, the degree of a product may be
less than the sum of the degrees. For example, (2x3 + x) · (3x) = 3x2 in Z6[x].
Corollary 8.23. If R is an integral domain, so is R[x].
Proof. If p(x) and q(x) are nonzero elements of R[x], then p(x) · q(x) is
also nonzero by Proposition 8.22. Hence R[x] has no zero divisors.
□
The construction of a polynomial ring can be iterated to obtain the ring of
polynomials in n variables x1, . . . , xn, with coefﬁcients from R. We deﬁne induc-
tively R[x1, . . . , xn] = R[x1, . . . , xn−1][xn]. For example, consider a polynomial
f in R[x, y] = R[x][y], say
f = f0 + f1y + f2y2 + · · · + fnyn,
where each fi = fi(x) is in R[x]. If we write fi = a0i + a1ix + a2ix2 · · · for
each i, then
f (x, y) = a00 + a10x + a01y + a20x2 + a11xy + a02y2 + · · · .
Clearly, we can prove by induction from Corollary 8.22 that R[x1, . . . , xn] is
an integral domain if R is an integral domain.
Proposition 8.24. Let R be a commutative ring and denote the inﬁnite sequence
of elements of R, ⟨a0, a1, a2, . . .⟩, by ⟨ai⟩. Deﬁne addition, +, and convolution,
∗, of two such sequences by
⟨ai⟩+ ⟨bi⟩= ⟨ai + bi⟩
and
⟨ai⟩∗⟨bi⟩=

 	
j+k=i
ajbk

= ⟨a0bi + a1bi−1 + · · · + aib0⟩.
The set of all such sequences forms a commutative ring (RN, +, ∗) called the
ring of sequences in R. If R is an integral domain, so is RN.
Proof. Addition is clearly associative and commutative. The zero element is
the zero sequence ⟨0⟩= ⟨0, 0, 0, . . .⟩, and the negative of ⟨ai⟩is ⟨−ai⟩. Now
(⟨ai⟩∗⟨bi⟩) ∗⟨ci⟩=

 	
j+k=i
ajbk

∗⟨ci⟩
=

 	
l+m=i

	
j+k=m
ajbk

cl

=

 	
j+k+l=i
ajbkcl

.

NEW RINGS FROM OLD
169
Similarly, bracketing the sequences in the other way, we obtain the same result,
which shows that convolution is associative.
Convolution is clearly commutative and the distributive laws hold because
⟨ai⟩∗(⟨bi⟩+ ⟨ci⟩) =

 	
j+k=i
aj(bk + ck)

=

 	
j+k=i
ajbk

+

 	
j+k=i
ajck

= ⟨ai⟩∗⟨bi⟩+ ⟨ai⟩∗⟨ci⟩.
The identity in the ring of sequences is ⟨1, 0, 0, . . .⟩because
⟨1, 0, 0, . . .⟩∗⟨a0, a1, a2, . . .⟩= ⟨1a0, 1a1 + 0a0, 1a2 + 0a1 + 0a0, . . .⟩
= ⟨a0, a1, a2, . . .⟩.
Therefore, (RN, +, ∗) is a commutative ring.
Suppose that aq and br are the ﬁrst nonzero elements in the nonzero sequences
⟨ai⟩and ⟨bi⟩, respectively. Then the element in the (q + r)th position of their
convolution is

j+k=q+r
ajbk = a0bq+r + a1bq+r−1 + · · · + aqbr + aq+1br−1 + · · · + aq+rb
= 0
+
0
+ · · · + aqbr +
0
+ · · · + 0 = aqbr.
Hence, if R is an integral domain, this element is not zero and the ring of
sequences has no zero divisors.
□
The ring of sequences cannot be a ﬁeld because ⟨0, 1, 0, 0, . . .⟩has no inverse.
In fact, for any sequence ⟨bi⟩, ⟨0, 1, 0, 0, . . .⟩∗⟨b0, b1, b2, . . .⟩= ⟨0, b0, b1, . . .⟩,
which can never be the identity in the ring.
A formal power series in x with coefﬁcients from a commutative ring R is
an expression of the form.
a0 + a1x + a2x2 + · · · =
∞
	
i=0
aixi
where
ai ∈R.
In contrast to a polynomial, these power series can have an inﬁnite number of
nonzero terms.
We denote the set of all such formal power series by R[[x]]. The term formal is
used to indicate that questions of convergence of these series are not considered.
Indeed, over many rings, such as Zn, convergence would not be meaningful.
Motivated by RN, addition and multiplication are deﬁned in R[[x]] by
 ∞
	
i=0
aixi

+
 ∞
	
i=0
bixi

=
∞
	
i=0
(ai + bi)xi

170
8
RINGS AND FIELDS
and
 ∞
	
i=0
aixi

·
 ∞
	
i=0
bixi

=
∞
	
i=0

	
j+k=i
ajbk

xi.
It can be veriﬁed that these formal power series do form a ring, (R[[x]], +, ·),
and that the polynomial ring, R[x], is the subring consisting of those power
series with only a ﬁnite number of nonzero terms. In fact, the ring of sequences
(RN, +, ∗) is isomorphic to the ring of formal power series (R[[x]], +, ·). The
function f : RN →R[[x]] that is deﬁned by f (⟨a0, a1, a2, · · ·⟩) = a0 + a1x +
a2x2 + · · · is clearly a bijection. It follows from the deﬁnitions of addition, mul-
tiplication, and convolution in these rings, that f is a ring morphism.
FIELD OF FRACTIONS
We can always add, subtract, and multiply elements in any ring, but we cannot
always divide. However, if the ring is an integral domain, it is possible to enlarge
it so that division by nonzero elements is possible. In other words, we can con-
struct a ﬁeld containing the given ring as a subring. This is precisely what we did
following Example 4.2 when constructing the rational numbers from the integers.
If the original ring did have zero divisors or was noncommutative, it could
not possibly be a subring of any ﬁeld, because ﬁelds cannot contain zero divisors
or pairs of noncommutative elements.
Theorem 8.25. If R is an integral domain, it is possible to construct a ﬁeld Q,
so that the following hold:
(i) R is isomorphic to a subring, R′, of Q.
(ii) Every element of Q can be written as p · q−1 for suitable p, q ∈R′.
Q is called the ﬁeld of fractions of R (or sometimes the ﬁeld of quotients of R).
Proof. Consider the set R × R∗= {(a, b)|a, b ∈R, b ̸= 0}, consisting of pairs
of elements of R, the second being nonzero. Motivated by the fact that a
b = c
d
in Q if and only if ad = bc, we deﬁne a relation ∼on R × R∗by
(a, b) ∼(c, d)
if and only if
ad = bc in R.
We verify that this is an equivalence relation.
(i) (a, b) ∼(a, b), since ab = ba.
(ii) If (a, b) ∼(c, d), then ad = bc. This implies that cb = da and hence
that (c, d) ∼(a, b).

FIELD OF FRACTIONS
171
(iii) If (a, b) ∼(c, d) and (c, d) ∼(e, f ), then ad = bc and cf = de. This
implies that (af −be)d = (ad)f −b(ed) = bcf −bcf = 0. Since R has
no zero divisors and d ̸= 0, it follows that af = be and (a, b) ∼(e, f ).
Hence the relation ∼is reﬂexive, symmetric, and transitive.
Denote the equivalence class containing (a, b) by a/b and the set of equiva-
lence classes by Q. As in Q, deﬁne addition and multiplication in Q by
a
b + c
d = ad + bc
bd
and
a
b · c
d = ac
bd .
These operations on equivalence classes are deﬁned in terms of particular repre-
sentatives, so it must be checked that they are well deﬁned. If a/b = a′/b′ and
c/d = c′/d′, then ab′ = a′b and cd′ = c′d. Hence
(ad + bc)(b′d′) = (ab′)dd′ + bb′(cd′) = (a′b)dd′ + bb′(c′d)
= (a′d′ + b′c′)(bd)
and therefore ad + bc
bd
= a′d′ + b′c′
b′d′
, which shows that addition is well deﬁned.
Also, acb′d′ = a′c′bd; thus ac
bd = a′c′
b′d′ , which shows that multiplication is well
deﬁned.
It can now be veriﬁed that (Q, +, ·) is a ﬁeld. The zero is 0/1, and the identity
is 1/1. For example, the distributive laws hold because
a
b ·
 c
d + e
f

= a
b · cf + de
df
= a(cf + de)
bdf
= a(cf + de)
bdf
· b
b = ac
bd + ae
bf
= a
b · c
d + a
b · e
f .
The inverse of any nonzero element a
b is b
a . The remaining axioms for a ﬁeld
are straightforward to check.
The ring R is isomorphic to the subring R′ =
r
1
r ∈R

of Q by an iso-
morphism that maps r to r
1. Any element a
b in the ﬁeld Q can be written as
a
b = a
1 · 1
b = a
1
b
1
−1
where a
1 and b
1 are in R′.
□
If we take R = Z to be integers in the above construction, we obtain the
rational numbers Q as the ﬁeld of fractions.

172
8
RINGS AND FIELDS
If R is an integral domain, the ﬁeld of fractions of the polynomial ring R[x]
is called the ﬁeld of rational functions with coefﬁcients in R. Its elements can
be considered as fractions of one polynomial over a nonzero polynomial.
A (possibly noncommutative) ring is called a domain if ab = 0 if and only
if a = 0 or b = 0. Thus the commutative domains are precisely the integral
domains. In 1931, Oystein Ore (1899–1968) extended Theorem 8.25 to a class
of domains (now called left Ore domains) for which a ring of left fractions
can be constructed that is a skew ﬁeld (that is, a ﬁeld that is not necessarily
commutative). On the other hand, in 1937, A. I. Mal’cev (1909–1967) discov-
ered an example of a domain that cannot be embedded in any skew ﬁeld. The
simplest example of a noncommutative skew ﬁeld is the ring of quaternions
(see Exercise 8.36). It has inﬁnitely many elements, in agreement with a famous
theorem of J. H. M. Wedderburn, proved in 1905, asserting that any ﬁnite skew
ﬁeld is necessarily commutative.
CONVOLUTION FRACTIONS
We now present an application of the ﬁeld of fractions that has important implica-
tions in analysis. This example is of a different type than most of the applications
in this book. It can be omitted, without loss of continuity, by those readers not
interested in analysis or applied mathematics.
We construct the ﬁeld of fractions of a set of continuous functions, and use
it to explain two mathematical techniques that have been used successfully by
engineers and physicists for many years, but were at ﬁrst mistrusted by math-
ematicians because they did not have a ﬁrm mathematical basis. One such
technique was introduced by O. Heaviside in 1893 in dealing with electrical
circuits; this is called the operational calculus, and it enabled him to solve par-
tial differential equations by manipulating differential operators as if they were
algebraic quantities. The second such technique is the use of impulse functions
in applied mathematics and mathematical physics. In 1926, when solving prob-
lems in relativistic quantum mechanics, P. Dirac introduced his delta function,
δ(x), which has the property that
δ(x) = 0
if x ̸= 0
and
 ∞
−∞
δ(x) dx = 1.
If we use the usual deﬁnition of functions, no such object exists. However, it
can be pictured in Figure 8.1 as the limit, as k tends to zero, of the functions
δk(x), where
δk(x) =
1/k
if 0 ⩽x ⩽k.
0
otherwise
Each function δk(x) vanishes outside the interval 0 ⩽x ⩽k and has the prop-
erty that
 ∞
−∞
δk(x) dx = 1.

CONVOLUTION FRACTIONS
173
x
x
d1
d1/2
x
d1/4
Figure 8.1.
The Dirac delta “function” is the limit of δk as k tends to zero.
Consider the set, C[0, ∞), of real-valued functions that are continuous in the
interval 0 ⩽x < ∞. We deﬁne the operations of addition and convolution on this
set so that the algebraic structure (C[0, ∞), +, ∗) is nearly an integral domain:
convolution does not have an identity so the structure fails to satisfy Axiom
(vi) of a ring. However, it is still possible to embed this structure in its ﬁeld
of fractions. The Polish mathematician Jan Mikusinski constructed this ﬁeld of
fractions and called such elements operators or generalized functions. The Dirac
delta function is a generalized function and is in fact the identity for convolution
in the ﬁeld of fractions.
Deﬁne addition and convolution of two functions f and g in C[0, ∞) by
(f + g)(x) = f (x) + g(x)
and
(f ∗g)(x) =
 x
0
f (t)g(x −t) dt.
This convolution of functions is the continuous analogue of convolution of
sequences, as can be seen by writing the ith term of the sequence
⟨ai⟩∗⟨bi⟩
as
i	
t=0
atbi−t.
It is clear that addition is associative and commutative, and the zero function
is the additive identity. Also, the negative of f (x) is −f (x).
Convolution is commutative because
(f ∗g)(x) =
 x
0
f (t)g(x −t) dt
= −
 0
x
f (x −u)g(u) du
substituting u = x −t
=
 x
0
g(u)f (x −u) du = (g ∗f )(x).
Convolution is associative because
(f ∗(g ∗h))(x) =
 x
0
f (t)(g ∗h)(x −t) dt
=
 x
0
f (t)
 x−t
0
g(u)h(x −t −u) du

dt
=
 x
0
f (t)
 x
t
g(w −t)h(x −w) dw

dt,

174
8
RINGS AND FIELDS
t
x
x
T
w
Figure 8.2
putting u = w −t. This integration is over the triangle in Figure 8.2 so, changing
the order of integration,
(f ∗(g ∗h))(x) =
 x
0
 w
0
f (t)g(w −t)h(x −w) dt dw,
=
 x
0
(f ∗g)(w)h(x −w) dw = ((f ∗g) ∗h)(x).
The distributive laws follow because
((f + g) ∗h)(x) =
 x
0
(f (t) + g(t))h(x −t) dt
=
 x
0
f (t)h(x −t) dt +
 x
0
g(t)h(x −t) dt
= (f ∗h)(x) + (g ∗h)(x).
If f is a function that is the identity under convolution, then f ∗h = h for
all functions h. If we take h to be the function deﬁned by h(x) = 1 for all
0 ⩽x < ∞, then
(f ∗h)(x) =
 x
0
f (t) dt = 1
for all x ⩾0.
There is no function f in C[0, ∞) with this property, although the Dirac delta
“function” does have this property. Hence (C[0, ∞), +, ∗) satisﬁes all the axioms
for a commutative ring except for the existence of an identity under convolution.
Furthermore, there are no zero divisors under convolution; that is, f ∗g = 0
implies that f = 0 or g = 0. This is a hard result in analysis, which is known as
Titchmarsh’s theorem. Proofs can be found in Erdelyi [36] or Marchand [37].
However, we can still construct the ﬁeld of fractions of this algebraic object in
exactly the same way as we did in Theorem 8.25. For example, the even integers
under addition and multiplication, (2Z, +, ·) is also an algebraic object that sat-
isﬁes all the axioms for an integral domain except for the fact that multiplication
has no identity. The ﬁeld of fractions of 2Z is the set of rational numbers; every
rational number can be written in the form 2r/2s, where 2r, 2s ∈2Z.

CONVOLUTION FRACTIONS
175
The ﬁeld of fractions of (C[0, ∞), +, ∗) is called the ﬁeld of convolution
fractions, and its elements are sometimes called generalized functions, distri-
butions, or operators. Elements of this ﬁeld are the abstract entities f/g, where
f and g are functions. There is a bijection between the set of elements of the
form f ∗g/g and the set C[0, ∞). It is possible to interpret other convolution
fractions as impulse functions, discontinuous functions, and even differential or
integral operators. The Dirac delta function can be deﬁned to be the identity of
this ﬁeld under convolution; therefore, δ = f/f , for any nonzero function f .
The Heaviside step function illustrated in Figure 8.3 is deﬁned by h(x) = 1
if x ⩾0, and h(x) = 0 if x < 0. The function is continuous when restricted
to the nonnegative numbers and, in some sense, is the integral of the Dirac
delta function. Convolution by h acts as an integral operator on any continuous
function because
(h ∗f )(x) = (f ∗h)(x) =
 x
0
f (t)h(x −t) dt =
 x
0
f (t) dt.
Hence h ∗f is the integral of f . We can use this to deﬁne integration of any
generalized function. Take the integral of the convolution fraction f/g to be the
fraction (h ∗f )/g.
Denote the inverse of the Heaviside step function by s, so that s = h/h ∗
h. This element s is not a genuine function, but only a convolution fraction.
Convolution by s acts, in some sense, as a differential operator in the ﬁeld
of convolution fractions. It is not exactly the usual differential operator, because
convolution by s and by h must commute, and s ∗h = h ∗s must be the identity.
If f (x) is a continuous function, we know from the calculus, that the derivative of
 x
0 f (t) dt is just f (x); however,
 x
0 f ′(t) dt is not just f (x) but is f (x) −f (0).
In fact, if the function f has a derivative,
(s ∗f )(x) = f ′(x) + f (0)δ(x)
where δ(x) is the identity in the ﬁeld of convolution fractions. Now, when we
calculate h ∗s ∗f , which is equivalent to integrating s ∗f from 0 to x, we
obtain the function f back again.
By repeated convolution with s or h, a generalized function can be differenti-
ated or integrated any number of times, the result being another generalized func-
tion. We can even differentiate or integrate a fractional number of times. These
h
x
Figure 8.3.
Heaviside step function.

176
8
RINGS AND FIELDS
operations s and h can be used to explain Heaviside’s operational calculus, in
which differential and integral operators are manipulated like algebraic symbols.
For further information on the algebraic aspects of generalized functions and
distributions, see Erdelyi [36] or Marchand [37].
EXERCISES
8.1. Write out the tables for the ring Z4.
8.2. Write out the tables for the ring Z2 × Z2.
Which of the systems described in Exercises 8.3 to 8.12 are rings under addition
and multiplication? Give reasons.
8.3. {a + b
√
5|a, b ∈Z}.
8.4. N.
8.5. {a + b
√
2 + c
√
3|a, b, c ∈Z}.
8.6. {a +
3√
2b|a, b ∈Q}.
8.7. All 2 × 2 real matrices with zero determinant.
8.8. All rational numbers that can be written with denominator 2.
8.9. All rational numbers that can be written with an odd denominator.
8.10. (Z, +, ×), where + is the usual addition and a × b = 0 for all a, b ∈Z.
8.11. The set A = {a, b, c} with tables given in Table 8.7.
TABLE 8.7
+
a
b
c
a
a
b
c
b
b
c
a
c
c
a
b
·
a
b
c
a
a
a
a
b
a
b
c
c
a
c
c
8.12. The set A = {a, b, c} with tables given in Table 8.8.
TABLE 8.8
+
a
b
c
a
a
b
c
b
b
c
a
c
c
a
b
·
a
b
c
a
a
a
a
b
a
c
b
c
a
b
c
8.13. A ring R is called a boolean ring if a2 = a for all a ∈R.
(a) Show that (P(X), , ∩) is a boolean ring for any set X.
(b) Show that Z2 and Z2 × Z2 are boolean rings.
(c) Prove that if R is boolean, then 2a = 0 for all a ∈R.

EXERCISES
177
(d) Prove that any boolean ring is commutative.
(e) If (R, ∧, ∨, ′) is any boolean algebra, show that (R, , ∧) is a boolean
ring where ab = (a ∧b′) ∨(a′ ∧b).
(f) If (R, +, ·) is a boolean ring, show that (R, ∧, ∨, ′) is a boolean algebra
where a ∧b = a · b, a ∨b = a + b + a · b and a′ = 1 + a.
This shows that there is a one-to-one correspondence between boolean
algebras and boolean rings.
8.14. If A and B are subrings of a ring R, prove that A ∩B is also a subring of R.
8.15. Prove that the only subring of Zn is itself.
Which of the sets described in Exercises 8.16 to 8.20 are subrings of C? Give
reasons.
8.16. {0 + ib|b ∈R}.
8.17. {a + ib|a, b ∈Q}.
8.18. {a + b√−7|a, b ∈Z}.
8.19. {z ∈C| |z| ⩽1}.
8.20. {a + ib|a, b ∈Z}.
Which of the rings described in Exercises 8.21 to 8.26 are integral domains and
which are ﬁelds?
8.21. Z2 × Z2.
8.22. (P({a}), , ∩).
8.23. {a + bi|a, b ∈Q}.
8.24. Z × R.
8.25. {a + b
√
2|a, b ∈Z}.
8.26. R[x].
8.27. Prove that the set C(R) of continuous real-valued functions deﬁned on the
real line forms a ring (C(R), +, ·), where addition and multiplication of
two functions f, g ∈C(R) is given by
(f + g)(x) = f (x) + g(x)
and
(f · g)(x) = f (x) · g(x).
Find all the zero divisors in the rings described in Exercises 8.28 to 8.33.
8.28. Z4.
8.29. Z10.
8.30. Z4 × Z2.
8.31. (P(X), , ∩).
8.32. M2(Z2).
8.33. Mn(R).
8.34. Let (R, +, ·) be a ring in which (R, +) is a cyclic group. Prove that
(R, +, ·) is commutative ring.
8.35. Show that S =

a
b
−b
a
 a, b ∈R

is a subring of M2(R) isomorphic
to C.
8.36. Show that H =
 α
β
−β
α
 α, β ∈C

is a subring of M2(C), where α
is the conjugate of α. This is called the ring of quaternions and gen-
eralizes the complex numbers in the following way: If I =

1
0
0
1

, ˆi =

i
0
0
−i

, ˆj =

0
1
−1
0

, and
ˆk =

0
i
i
0

in H, show that every

178
8
RINGS AND FIELDS
quaternion q has a unique representation in the form q = aI +bˆi +c ˆj +d ˆk,
where a, b, c, d ∈R. Show further that ˆi2 = ˆj 2 = ˆk2 = ˆi ˆj ˆk = −I and that
these relations determine the multiplication in H. If 0 ̸= q ∈H, show that
q−1 =
1
a2 + b2 + c2 + d2 q∗, where q∗= aI −bˆi −c ˆj −d ˆk, so that H is
a noncommutative skew ﬁeld.
8.37. Find all the ring morphisms from Z to Z6.
8.38. Find all the ring morphisms from Z15 to Z3.
8.39. Find all the ring morphisms from Z × Z to Z × Z.
8.40. Find all the ring morphisms from Z7 to Z4.
8.41. If (A, +) is an abelian group, the set of endomorphisms of A, End(A), con-
sists of all the group morphisms from A to itself. Show that (End(A), +, Ž )
is a ring under addition and composition, where (f + g)(a) = f (a) +
g(a), for f, g ∈End(A). This is called the endomorphism ring of A.
8.42. Describe the endomorphism ring End(Z2 × Z2). Is it commutative?
8.43. Prove that 10n ≡1 mod 9 for all n ∈N. Then prove that an integer is
divisible by 9 if and only if the sum of its digits is divisible by 9.
8.44. Find the number of nonisomorphic rings with three elements.
8.45. Prove that R[x] ∼= R[y].
8.46. Prove that R[x, y] ∼= R[y, x].
8.47. Let (R, +, ·) be a ring. Deﬁne the operations ⊕and Ž on R by
r ⊕s = r + s + 1
and
rŽs = r · s + r + s.
(a) Prove that (R, ⊕,Ž) is a ring.
(b) What are the additive and multiplicative identities of (R, ⊕,Ž)?
(c) Prove that (R, ⊕,Ž) is isomorphic to (R, +, ·).
8.48. Let a and b be elements of a commutative ring. For each positive integer
n, prove the binomial theorem:
(a + b)n = an +

n
1

an−1b + · · · +

n
k

an−kbk + · · · + bn.
8.49. Let (R, +, ·) be an algebraic object that satisﬁes all the axioms for a ring
except for the multiplicative identity. Deﬁne addition and multiplication in
R × Z by
(a, n) + (b, m) = (a + b, n + m)
and
(a, n) · (b, m) = (ab + ma + nb, nm).
Show that (R × Z, +, ·) is a ring that contains a subset in one-to-one
correspondence with R that has all the properties of the algebraic object
(R, +, ·).

EXERCISES
179
8.50. If R and S are commutative rings, prove that the ring of sequences (R ×
S)N is isomorphic to RN × SN.
8.51. If F is a ﬁeld, show that the ﬁeld of fractions of F is isomorphic to F.
8.52. Describe the ﬁeld of fractions of the ring ({a + ib|a, b ∈Z}, +, ·).
8.53. Let (S, ∗) be a commutative semigroup that satisﬁes the cancellation law;
that is, a ∗b = a ∗c implies that b = c. Show that (S, ∗) can be embedded
in a group.
8.54. Let T = {f : R →R|f (x) = a cos x + b sin x, a, b ∈R}. Deﬁne addition
of two such trigonometric functions in the usual way and deﬁne convolu-
tion by
(f ∗g)(x) =
 2π
0
f (t)g(x −t) dt.
Show that (T, +, ∗) is a ﬁeld.
8.55. Let Tn =

f : R →R|f (x) = a0
2 +
n
r=1
(ar cos rx + br sin rx), ar, br ∈R

.
Show that (Tn, +, ∗) is a commutative ring where addition and convolution
are deﬁned as in Exercise 8.54. What is the multiplicative identity? Is the
ring an integral domain?
8.56. If R is any ring, deﬁne R(i) = {a + bi|a, b ∈R} to be the set of all formal
sums a + bi, where a and b are in R. As in C, we declare that a + bi =
a1 + b1i if and only if a = a1 and b = b1. If we insist that i2 = −1 and
ai = ia for all a ∈R, then the ring axioms determine the addition and
multiplication in R(i):
(r + si) + (r1 + s1i) = (r + r1) + (s + s1)i
(r + si)(r1 + s1i) = (rr1 −ss1) + (rs1 + sr1)i.
Thus, for example, R(i) = C.
(a) Show that R(i) is a ring, commutative if R is commutative.
(b) If R is commutative, show that a + bi is has an inverse in R(i) if and
only if a2 + b2 has an inverse in R.
(c) Show that Z3(i) is a ﬁeld of nine elements.
(d) Is C(i) a ﬁeld? Is Z5(i) a ﬁeld? Give reasons.
8.57. If R is a ring call e ∈R an idempotent if e2 = e. Call R “tidy” if some
positive power of every element is an idempotent.
(a) Show that every ﬁnite ring is tidy. [Hint: If a ∈R, show that am+n =
am for some n ⩾1.]
(b) If R is tidy, show that uv = 1 in R implies that vu = 1.
(c) If R is a commutative tidy ring, show that every element of R is either
invertible or a zero divisor.

9
POLYNOMIAL AND
EUCLIDEAN RINGS
Polynomial functions and the solution of polynomial equations are a basic part
of mathematics. One of the important uses of ring and ﬁeld theory is to extend
a ﬁeld to a larger ﬁeld so that a given polynomial has a root. For example,
the complex number ﬁeld can be obtained by enlarging the real ﬁeld so that all
quadratic equations will have solutions.
Before we are able to extend ﬁelds, we need to investigate the ring of poly-
nomials, F[x], with coefﬁcients in a ﬁeld F. This polynomial ring has many
properties in common with the ring of integers; both F[x] and Z are integral
domains, but not ﬁelds. Moreover, both rings have division and euclidean algo-
rithms. These algorithms are extremely useful, and rings with such algorithms
are called euclidean rings.
EUCLIDEAN RINGS
Long division of integers gives a method for dividing one integer by another to
obtain a quotient and a remainder. The fact that this is always possible is stated
formally in the division algorithm.
Theorem 9.1. Division Algorithm for Integers. If a and b are integers and b
is nonzero, then there exist unique integers q and r such that
a = qb + r
and
0 ⩽r < |b|.
Proof. If b > 0, then |b| = b, so this restates Theorem 7 in Appendix 1. If b <
0,
then
−b > 0,
so
the
same
theorem
gives
a = q(−b) + r,
where
0 ⩽r < (−b). Since |b| = −b in this case, this gives a = (−q)b + r, where
0 ⩽r < |b|.
□
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
180

EUCLIDEAN RINGS
181
The integer r is called the remainder in the division of a by b, and q is called
the quotient.
What other rings, besides the integers, have a division algorithm? In a ﬁeld, we
can always divide any element exactly by a nonzero element. If a ring contains
zero divisors, the cancellation property does not hold, and we cannot expect to
obtain a unique quotient. This leaves integral domains, and the following kinds
contain a useful generalization of the division algorithm.
An integral domain R is called a euclidean ring if for each nonzero element
a ∈R, there exists a nonnegative integer δ(a) such that:
(i) If a and b are nonzero elements of R, then δ(a) ⩽δ(ab).
(ii) For every pair of elements a, b ∈R with b ̸= 0, there exist elements
q, r ∈R such that
a = qb + r
where
r = 0
or
δ(r) < δ(b).
(division algorithm)
Theorem 9.1 shows that the ring Z of integers is a euclidean ring if we take
δ(b) = |b|, the absolute value of b, for all b ∈Z. A ﬁeld is trivially a euclidean
ring when δ(a) = 1 for all nonzero elements a of the ﬁeld. We now show that
the ring of polynomials, with coefﬁcients in a ﬁeld, is a euclidean ring when we
take δ(g(x)) to be the degree of the polynomial g(x).
Theorem 9.2. Division Algorithm for Polynomials. Let f (x), g(x) be ele-
ments of the polynomial ring F[x], with coefﬁcients in the ﬁeld F. If g(x) is
not the zero polynomial, there exist unique polynomials q(x), r(x) ∈F[x] such
that
f (x) = q(x) · g(x) + r(x)
where either r(x) is the zero polynomial or deg r(x) < deg g(x).
Proof. If f (x) is the zero polynomial or deg f (x) < deg g(x), then writing
f (x) = 0 · g(x) + f (x), we see that the requirements of the algorithm are ful-
ﬁlled.
If deg f (x) = deg g(x) = 0, then f (x) and g(x) are nonzero constant poly-
nomials a0 and b0, respectively. Now f (x) = (a0b−1
0 )g(x), and the algorithm
holds.
We prove the other cases by induction on the degree of f (x). Suppose that,
when we divide by a ﬁxed polynomial g(x), the division algorithm holds for
polynomials of degree less than n. Let f (x) = a0 + · · · + anxn and g(x) = b0 +
· · · + bmxm where an ̸= 0, bm ̸= 0. If n < m, we have already shown that the
algorithm holds.
Suppose that n ⩾m and put
f1(x) = f (x) −anb−1
m xn−mg(x)

182
9
POLYNOMIAL AND EUCLIDEAN RINGS
so that deg f1(x) < n. By the induction hypothesis
f1(x) = q1(x) · g(x) + r(x)
where either r(x) = 0
or
deg r(x) < deg g(x).
Hence
f (x) = anb−1
m xn−mg(x) + f1(x) = {anb−1
m xn−m + q1(x)} · g(x) + r(x),
which is a representation of the required form. The algorithm now follows by
induction, starting with n = m −1 if m ̸= 0, or with n = 0 if m = 0.
The uniqueness of the quotient, g(x), and of the remainder, r(x), follows in
a similar way to the uniqueness of the quotient and remainder in the division
algorithm for integers (Theorem 7, Appendix 2).
□
The quotient and remainder polynomials can be calculated by long division
of polynomials.
Example 9.3. Divide x3 + 2x2 + x + 2 by x2 + 2 in Z3[x].
Solution. Write Z3 = {0, 1, 2} for convenience.
x + 2
x2 + 2
x3+2x2+ x+2
x3+
+2x
2x2+2x+2
2x2
+1
2x+1
Hence x3 + 2x2 + x + 2 = (x + 2)(x2 + 2) + (2x + 1).
□
If we divide by a polynomial of degree 1, the remainder must be a constant.
This constant can be found as follows.
Theorem 9.4. Remainder Theorem. The remainder when the polynomial f (x)
is divided by (x −α) in F[x] is f (α).
Proof. By the division algorithm, there exist q(x), r(x) ∈F[x] with f (x) =
q(x)(x −α) + r(x), where r(x) = 0 or deg r(x) < 1. The remainder is therefore
a constant r0 ∈F and f (x) = q(x)(x −α) + r0. Substituting α for x, we obtain
the result f (α) = r0.
□
Theorem 9.5. Factor Theorem. The polynomial (x −α) is a factor of f (x) in
F[x] if and only if f (α) = 0.
Proof. We can write f (x) = q(x)(x −α) for some q(x) ∈F[x] if and only
if f (x) has remainder 0 when divided by (x −α). By the remainder theorem,
this happens if and only if f (α) = 0.
□

EUCLIDEAN RINGS
183
An element α is called a root of a polynomial f (x) if f (α) = 0. The fac-
tor theorem shows that (x −α) is a factor of f (x) if and only if α is a root
of f (x).
Theorem 9.6. A polynomial of degree n over a ﬁeld F has at most n roots in F.
Proof. We prove the theorem by induction on the degree n. A polynomial of
degree 0 consists of only a nonzero constant and therefore has no roots.
Assume that the theorem is true for polynomials of degree n −1 and let
f (x) ∈F[x] be a polynomial of degree n. If f (x) has no roots, the theorem
holds. If f (x) does have roots, let α be one such root. By the factor theorem,
we can write
f (x) = (x −α)g(x),
and by Proposition 8.22, deg g(x) = n −1.
Since the ﬁeld F has no zero divisors, f (β) = 0 if and only if (β −α) = 0
or g(β) = 0. Therefore, any root of f (x) is either equal to α or is a root of g(x).
By the induction hypothesis, g(x) has, at most, n −1 roots, so f (x) has, at most,
n roots.
□
Example 9.7. Show that the ring of gaussian integers, Z[i] = {a + ib|a, b ∈Z},
is a euclidean ring with δ(a + ib) = a2 + b2.
Solution. Z[i] is a subring of the complex numbers, C, and therefore is an
integral domain.
If z ∈Z[i], then δ(z) = zz, where z is the conjugate of z in the complex
numbers. For any nonzero complex number z, δ(z) > 0, and for two nonzero
gaussian integers z and w, δ(z · w) = δ(z) · δ(w).
To
prove
the
division
algorithm
in
Z[i],
let
z
and
w
be
gaus-
sian
integers
where
w ̸= 0.
Then
z/w
is
a
complex
number,
c +
id,
where
c, d ∈Q.
Choose
integers,
a,
b
as
in
Figure 9.1
so
that
|c −a| ⩽1
2
and |d −b| ⩽1
2. Then z/w = a + ib + [(c −a) + i(d −b)] so
z = (a + ib)w + [(c −a) + i(d −b)]w.
Now
δ([(c −a) + i(d −b)]w) =
−1
0
1
2
−1 + i
i
a + ib
 = c + id
z
w
Figure 9.1.
Complex numbers with the elements of Z[i] circled.

184
9
POLYNOMIAL AND EUCLIDEAN RINGS
δ((c −a) + i(d −b))δ(w)=[(c −a)2 + (d −b)2]δ(w) ⩽( 1
4 + 1
4)δ(w) < δ(w).
Hence Z[i] is a euclidean ring.
□
EUCLIDEAN ALGORITHM
The division algorithm allows us to generalize the concepts of divisors and
greatest common divisors to any euclidean ring. Furthermore, we can produce a
euclidean algorithm that will enable us to calculate greatest common divisors.
If a, b, q are three elements in an integral domain such that a = qb, we say that
b divides a or that b is a factor of a and write b|a. For example, (2 + i)|(7 + i)
in the gaussian integers, Z[i], because 7 + i = (3 −i)(2 + i).
Proposition 9.8. Let a, b, c be elements in an integral domain R.
(i) If a|b and a|c, then a|(b + c).
(ii) If a|b, then a|br for any r ∈R.
(iii) If a|b and b|c, then a|c.
Proof. These results follow immediately from the deﬁnition of divisibility. □
By analogy with Z, if a and b are elements in an integral domain R, then the
element g ∈R is called a greatest common divisor of a and b, and is written
g = gcd(a, b), if the following hold:
(i) g|a and g|b.
(ii) If c|a and c|b, then c|g.
The element l ∈R is called a least common multiple of a and b, and is
written l = lcm(a, b), if the following hold:
(i) a|l and b|l.
(ii) If a|k and b|k, then l|k.
For example, 4 and −4 are greatest common divisors, and 60 and −60 are
least common multiples, of 12 and 20 in Z. Note that in Z it is customary to
choose the positive value in each case to make it unique (see Appendix 2).
Theorem 9.9. Let R be a euclidean ring. Any two elements a and b in R have
a greatest common divisor g. Moreover, there exist s, t ∈R such that
g = sa + tb.
Proof. If a and b are both zero, their greatest common divisor is zero, because
r|0 for any r ∈R.

EUCLIDEAN ALGORITHM
185
Suppose that at least one of a and b is nonzero. By the well-ordering axiom
(Appendix 2), let g be a nonzero element for which δ(g) is minimal in the set
I = {xa + yb|x, y ∈R}. We can write g = sa + tb for some s, t ∈R.
Since R is a euclidean ring, a = hg + r, where r = 0 or δ(r) < δ(g). There-
fore, r = a −hg = a −h(sa + tb) = (1 −hs)a −htb ∈I. Since g was an
element for which δ(g) was minimal in I, it follows that r must be zero, and
g|a. Similarly, g|b.
If c|a and c|b, so that a = kc and b = lc, then g = sa + tb = skc + tlc =
(sk + tl)c and c|g. Therefore, g = gcd(a, b).
□
Theorem 9.9 shows that greatest common divisors exist in any euclidean ring,
but does not give a method for ﬁnding them. In fact, they can be computed using
the following general euclidean algorithm.
Theorem 9.10. Euclidean Algorithm. Let a, b be elements of a euclidean ring
R and let b be nonzero. By repeated use of the division algorithm, we can write
a = bq1 + r1
where
δ(r1) < δ(b)
b = r1q2 + r2
where
δ(r2) < δ(r1)
r1 = r2q3 + r3
where
δ(r3) < δ(r2)
...
...
rk−2 = rk−1qk + rk
where
δ(rk) < δ(rk−1)
rk−1 = rkqk+1 + 0.
If r1 = 0, then b = gcd(a, b); otherwise, rk = gcd(a, b).
Furthermore, elements s, t ∈R such that
gcd(a, b) = sa + tb
can be found by starting with the equation rk = rk−2 −rk−1qk and successively
working up the sequence of equations above, each time replacing ri in terms of
ri−1 and ri−2.
Proof. This algorithm must terminate, because δ(b), δ(r1), δ(r2), . . . is a de-
creasing sequence of nonnegative integers; thus, rk+1 = 0 for some k + 1. The
proof of the algorithm follows as in the proof of Theorem 10 in Appendix 2. □
Example 9.11. Find the greatest common divisor of 713 and 253 in Z and ﬁnd
two integers s and t such that
713s + 253t = gcd(713, 253).
Solution. By the division algorithm, we have

186
9
POLYNOMIAL AND EUCLIDEAN RINGS
(i) 713 = 2 · 253 + 207
a = 713, b = 253, r1 = 207
(ii) 253 = 1 · 207 + 46
r2 = 46
(iii) 207 = 4 · 46 + 23
r3 = 23
46 = 2 · 23 + 0.
r4 = 0
The
last
nonzero
remainder
is
the
greatest
common
divisor.
Hence
gcd(713, 253) = 23.
We can ﬁnd the integers s and t by using equations (i)–(iii). We have
23 = 207 −4 · 46
from equation (iii)
= 207 −4(253 −207)
from equation (ii)
= 5 · 207 −4 · 253
= 5 · (713 −2 · 253) −4 · 253
from equation (i)
= 5 · 713 −14 · 253.
Therefore, s = 5 and t = −14.
□
Example 9.12. Find a greatest common divisor, g(x), of a(x) = 2x4 + 2 and
b(x) = x5 + 2 in Z3[x], and ﬁnd s(x), t(x) ∈Z3[x], so that
g(x) = s(x) · (2x4 + 2) + t(x) · (x5 + 2).
Solution. By repeated use of the division algorithm (see below), we have:
(i) x5 + 2 = (2x)(2x4 + 2) + (2x + 2).
(ii) 2x4 + 2 = (x3 + 2x2 + x + 2)(2x + 2) + 1.
(iii) 2x + 2 = (2x + 2) · 1 + 0.
Hence gcd(a(x), b(x)) = 1. From equation (ii) we have
1 = 2x4 + 2 −(x3 + 2x2 + x + 2)(2x + 2)
= 2x4 + 2 −(x3 + 2x2 + x + 2){x5 + 2 −(2x)(2x4 + 2)}
from equation (i)
= (2x4 + x3 + 2x2 + x + 1)(2x4 + 2) + (2x3 + x2 + 2x + 1)(x5 + 2).
Therefore,
s(x) = 2x4 + x3 + 2x2 + x + 1
and
t(x) = 2x3 + x2 + 2x + 1.

UNIQUE FACTORIZATION
187
2x
2x4 + 2
x5+0x4+0x3+0x2+0x+2
x5
+ x
2x+2
x3 + 2x2 + x + 2
2x + 2
2x4+0x3+0x2+0x+2
2x4+2x3
x3
+2
x3+ x2
2x2
+2
2x2+2x
x+2
x+1
1
□
Example 9.13. Find a greatest common divisor of a(x) = x4 + x3 + 3x −9 and
b(x) = 2x3 −x2 + 6x −3 in Q[x].
Solution. By the division algorithm we have (computation below)
a(x) =
 1
2x + 3
4

b(x) −9
4x2 −27
4
and
b(x) =

−8
9x + 4
9
 
−9
4x2 −27
4

.
Hence
gcd(a(x), b(x)) = −9
4x2 −27
4 .
1
2x + 3
4
2x3 −x2 + 6x −3
x4+ x3+0x2+3x−9
x4−1
2x3+3x2−3
2x
3
2x3−3x2+ 9
2x−9
3
2x3−3
4x2+ 9
2x−9
4
−9
4x2
−27
4
−8
9x + 4
9
−9
4x2 −27
4
2x3−x2+6x−3
2x3
+6x
−x2
−3
−x2
−3
0
□
UNIQUE FACTORIZATION
One important property of the integers, commonly known as the fundamental
theorem of arithmetic, states that every integer greater than 1 can be written as

188
9
POLYNOMIAL AND EUCLIDEAN RINGS
a ﬁnite product of prime numbers, and furthermore, this product is unique up to
the ordering of the primes (see Theorem 13 in Appendix 2). In this section, we
prove a similar result for any euclidean ring.
Let R be a commutative ring. An element u is called an invertible element
(or unit) of R if there exists an element v ∈R such that uv = 1. The invertible
elements in a ring R are those elements with multiplicative inverses in R. Denote
the set of invertible elements of R by R∗. If R is a ﬁeld, every nonzero element
is invertible and R∗= R −{0}.
The invertible elements in the integers are ±1. If F is a ﬁeld, the invertible
polynomials in F[x] are the nonzero constant polynomials, that is, the poly-
nomials of degree 0. The set of invertible elements in the gaussian integers is
Z[i]∗= {±1, ±i}.
Proposition 9.14. For any commutative ring R, the invertible elements form an
abelian group, (R∗, ·), under multiplication.
Proof. Let u1, u2 ∈R∗and let u1v1 = u2v2 = 1. Then (u1u2)(v1v2) = 1; thus
u1u2 ∈R∗. The group axioms follow immediately.
□
Two elements in a euclidean ring may have many greatest common divisors.
For example, in Q[x], x + 1, 2x + 2, and 1
3x + 1
3 are all greatest common divisors
of x2 + 2x + 1 and x2 −1. However, they can all be obtained from one another
by multiplying by invertible elements.
Lemma 9.15. If a|b and b|a in an integral domain R, then a = ub, where u is
an invertible element.
Proof. Since a|b, b = va for v ∈R so if a = 0, then b = 0 and a = b. If a ̸=
0, then a = ub for u ∈R since b|a. Therefore, a = ub = uva; thus a(uv −1) =
0. As a ̸= 0 and R has no zero divisors, uv = 1 and u is invertible.
□
Lemma 9.16. If g2 is a greatest common divisor of a and b in the euclidean ring
R, then g1 is also a greatest common divisor of a and b if and only if g1 = ug2,
where u is invertible.
Proof. If g1 = ug2 where uv = 1, then g2 = vg1. Hence g2|g1 and g1|g2 if
and only if g1 = ug2. The result now follows from the deﬁnition of a greatest
common divisor.
□
Lemma 9.17. If a and b are elements in a euclidean ring R, then δ(a) = δ(ab)
if and only if b is invertible. Otherwise, δ(a) < δ(ab).
Proof. If b is invertible and bc = 1, then δ(a) ⩽δ(ab) ⩽δ(abc) = δ(a).
Hence δ(a) = δ(ab).
If b is not invertible, ab does not divide a and a = qab + r, where δ(r) <
δ(ab). Now r = a(1 −qb); thus δ(a) ⩽δ(r). Therefore, δ(a) < δ(ab).
□

UNIQUE FACTORIZATION
189
A noninvertible element p in a euclidean ring R is said to be irreducible if,
whenever p = ab, either a or b is invertible in R. The irreducible elements in
the integers are the prime numbers together with their negatives.
Lemma 9.18. Let R be a euclidean ring. If a, b, c ∈R, gcd(a, b) = 1 and a|bc,
then a|c.
Proof. By Theorem 9.9, we can write 1 = sa + tb, where s, t ∈R. Therefore
c = sac + tbc, so a|c because a|bc.
□
Proposition 9.19. If p is irreducible in the euclidean ring R and p|ab, then p|a
or p|b.
Proof. For any a ∈R, write d = gcd(a, p). Then d|p, say p = d · h. Since p
is irreducible, either d or h is invertible, and so either d = 1 or p. Hence if p
does not divide a, then d = 1, and it follows from Lemma 9.18 that p|b.
□
Theorem 9.20. Unique Factorization Theorem. Every nonzero element in a
euclidean ring R is either an invertible element or can be written as the prod-
uct of a ﬁnite number of irreducibles. In such a product, the irreducibles are
uniquely determined up to the order of the factors and up to multiplication by
invertible elements.
Proof. We proceed by induction on δ(a) for a ∈R. The least value of δ(a) for
nonzero a is δ(1), because 1 divides any other element. Suppose that δ(a) = δ(1).
Then δ(1 · a) = δ(1) and, by Lemma 9.17, a is invertible.
By the induction hypothesis, suppose that all elements x ∈R, with δ(x) <
δ(a), are either invertible or can be written as a product of irreducibles. We now
prove this for the element a.
If a is irreducible, there is nothing to prove. If not, we can write a = bc,
where neither b nor c is invertible. By Lemma 9.17, δ(b) < δ(bc) = δ(a) and
δ(c) < δ(bc) = δ(a). By the induction hypothesis, b and c can each be written
as a product of irreducibles, and hence a can also be written as a product of
irreducibles.
To prove the uniqueness, suppose that
a = p1p2 · · · pn = q1q2 · · · qm,
where each pi and qj is irreducible. Now p1|a and so p1|q1q2 · · · qm. By an
extension of Proposition 9.19 to m factors, p1 divides some qi. Rearrange the qi,
if necessary, so that p1|q1. Therefore, q1 = u1p1 where u1 is invertible, because
p1 and q1 are both irreducible.
Now a = p1p2 · · · pn = u1p1q2 · · · qm; thus p2 · · · pn = u1q2 · · · qm. Proceed
inductively to show that pi = uiqi for all i, where each ui is invertible.

190
9
POLYNOMIAL AND EUCLIDEAN RINGS
If m < n, we would obtain the relation pm+1 · · · pn = u1u2 · · · um, which is
impossible because irreducibles cannot divide an invertible element. If m > n,
we would obtain
1 = u1u2 · · · unqn+1 · · · qm,
which is again impossible because an irreducible cannot divide 1. Hence m = n,
and the primes p1, p2, . . . , pn are the same as q1, q2, . . . , qm up to a rearrange-
ment and up to multiplication by invertible elements.
□
When the euclidean ring is the integers, the theorem above yields the funda-
mental theorem of arithmetic referred to earlier. The ring of polynomials over
a ﬁeld and the gaussian integers also have this unique factorization property
enjoyed by the integers. However, the integral domain
Z[
√
−3] = {a + b
√
−3 |a, b ∈Z},
which is a subring of C, does not have the unique factorization property. For
example,
4 = 2 · 2 = (1 +
√
−3) · (1 −
√
−3),
whereas 2, 1 + √−3, and 1 −√−3 are all irreducible. Therefore, Z[√−3] cannot
be a euclidean ring.
FACTORING REAL AND COMPLEX POLYNOMIALS
The question of whether or not a polynomial is irreducible will be crucial in
Chapter 10 when we extend number ﬁelds by adjoining roots of a polynomial.
We therefore investigate different methods of factoring polynomials over various
coefﬁcient ﬁelds.
A polynomial f (x) of positive degree is said to be reducible over the ﬁeld F
if it can be factored into two polynomials of positive degree in F[x]. If it cannot
be so factored, f (x) is called irreducible over F, and f (x) is an irreducible
element of the ring F[x]. It is important to note that reducibility depends on the
ﬁeld F. The polynomial x2 + 1 is irreducible over R but reducible over C.
The following basic theorem, ﬁrst proved by Gauss in his doctoral thesis in
1799, enables us to determine which polynomials are irreducible in C[x] and in
R[x].
Theorem 9.21. Fundamental Theorem of Algebra. If f (x) is a polynomial in
C[x] of positive degree, then f (x) has a root in C.
A proof of this theorem is given in Nicholson [11] using the fact from analysis
that a cubic real polynomial has a real root.
The following useful theorem shows that the complex roots of real polyno-
mials occur in conjugate pairs.

FACTORING REAL AND COMPLEX POLYNOMIALS
191
Theorem 9.22. (i) If z = a + ib is a complex root of the real polynomial f (x) ∈
R[x], then its conjugate z = a −ib is also a root. Thus the real polynomial
(x −z)(x −z) = x2 −2ax + (a2 + b2) is a factor of f (x).
(ii) If a, b, c ∈Q and a + b√c is an irrational root of the rational polyno-
mial f (x) ∈Q[x], then a −b√c is also a root, and the rational polynomial
x2 −2ax + (a2 −b2c) is a factor of f (x).
Proof. (i) Let g(x) = x2 −2ax + a2 + b2 = (x −z)(x −z). By the division
algorithm in R[x], there exist real polynomials q(x) and r(x) such that
f (x) = q(x)g(x) + r(x)
where
r(x) = 0
or
deg r(x) < 2.
Hence r(x) = r0 + r1x where r0, r1 ∈R. Now z = a + ib is a root of f (x) and
of g(x); therefore, it is also a root of r(x), so 0 = r0 + r1(a + ib). Equating real
and imaginary parts, we have r0 + r1a = 0 and r1b = 0. But then
r(z) = r(a −ib) = r0 + r1(a −ib) = r0 + r1a −ir1b = 0.
Since z is a root of r(x) and g(x), it must be a root of f (x).
If z is complex and not real, then b ̸= 0. In this case r1 = 0 and r0 = 0; thus
g(x)|f (x).
(ii) This can be proved in a similar way to part (i).
□
Theorem 9.23. (i) The irreducible polynomials in C[x] are the polynomials of
degree 1.
(ii) The irreducible polynomials in R[x] are the polynomials of degree 1
together with the polynomials of degree 2 of the form ax2 + bx + c, where
b2 < 4ac.
Proof. (i) The polynomials of degree 0 are the invertible elements of C[x].
By the fundamental theorem of algebra, any polynomial of positive degree has a
root in C and hence a linear factor. Therefore, all polynomials of degree greater
than 1 are reducible and those of degree 1 are the irreducibles.
(ii) The polynomials of degree 0 are the invertible elements of R[x]. By part (i)
and the unique factorization theorem, every real polynomial of positive degree
can be factored into linear factors in C[x]. By Theorem 9.22 (i), its nonreal
roots fall into conjugate pairs, whose corresponding factors combine to give a
quadratic factor in R[x] of the form ax2 + bx + c, where b2 < 4ac. Hence any
real polynomial can be factored into real linear factors and real quadratic factors
of the form above.
□
Example 9.24. Find the kernel and image of the ring morphism ψ: Q[x] →R
deﬁned by ψ(f (x)) = f (
√
2).
Solution. If p(x) = a0 + a1x + · · · + anxn ∈Q[x], then
ψ(p(x)) = a0 + a1
√
2 + · · · + an(
√
2)n
= (a0 + 2a2 + 4a4 + · · ·) +
√
2 (a1 + 2a3 + 4a5 + · · ·),

192
9
POLYNOMIAL AND EUCLIDEAN RINGS
so ψ(p(x)) ∈Q(
√
2) = {a + b
√
2|a, b ∈Q}, where Q(
√
2) is the subring of
R deﬁned in Example 8.3. Hence Im ψ ⊆Q(
√
2), and Im ψ = Q(
√
2) because
ψ(a + bx) = a + b
√
2.
If p(x) ∈Kerψ, then p(
√
2) = 0; therefore, by Theorem 9.22(ii), p(−
√
2) =
0, and p(x) contains a factor (x2 −2). Conversely, if p(x) contains a factor
(x2 −2), then p(
√
2) = 0 and p(x) ∈Kerψ. Hence Kerψ = {(x2 −2)q(x)|q(x)
∈Q[x]}, that is, the set of all polynomials in Q[x] with (x2 −2) as a factor. □
FACTORING RATIONAL AND INTEGRAL POLYNOMIALS
A rational polynomial can always be reduced to an integer polynomial by mul-
tiplying it by the least common multiple of the denominators of its coefﬁcients.
We now give various methods for determining whether an integer polynomial
has rational roots or is irreducible over Q.
Theorem 9.25. Rational Roots Theorem. Let p(x) = a0 + a1x + · · · + anxn ∈
Z[x]. If r/s is a rational root of p(x) and gcd(r, s) = 1, then:
(i) r|a0.
(ii) s|an.
Proof. If p(r/s) = 0, then a0 + a1(r/s) + · · · + an−1(r/s)n−1 + an(r/s)n =
0, whence a0sn + a1rsn−1 + · · · + an−1rn−1s + anrn = 0. Therefore, a0sn =
−r(a1sn−1 + · · · + an−1rn−2s + anrn−1); thus r|a0sn. Since gcd(r, s) = 1, it
follows from Lemma 9.18 that r|a0. Similarly, s|an.
□
Example 9.26. Factor p(x) = 2x3 + 3x2 −1 in Q[x].
Solution. If p(r/s) = 0, then, by Theorem 9.25, r|(−1) and s|2. Hence r =
±1 and s = ±1 or ±2, and the only possible values of r/s are ±1, ±1/2. Instead
of testing all these values, we sketch the graph of p(x) to ﬁnd approximate roots.
Differentiating, we have p′(x) = 6x2 + 6x = 6x(x + 1), so p(x) has turning val-
ues at 0 and −1.
We see from the graph in Figure 9.2 that −1 is a double root and that there
is one more positive root. If it is rational, it can only be 1
2. Checking this in
Table 9.1, we see that 1
2 is a root; hence p(x) factors as (x + 1)2(2x −1).
□
Example 9.27. Prove that
5√
2 is irrational.
TABLE 9.1
x
−1
0
1/2
1
2
p(x)
0
−1
0
4
27

FACTORING RATIONAL AND INTEGRAL POLYNOMIALS
193
p(x)
x
−1
−1
1
Figure 9.2.
Graph of p(x) = 2x3 + 3x2 −1.
TABLE 9.2
x
−2
−1
1
2
x5 −2
−34
−3
−1
30
Solution. Observe that
5√
2 is a root of x5 −2. If this polynomial has a rational
root r/s, in its lowest terms, it follows from Theorem 9.25 that r|(−2) and
s|1. Hence the only possible rational roots are ±1, ±2. We see from Table 9.2
that none of these are roots, so all the roots of the polynomial x5 −2 must be
irrational.
□
Theorem 9.28. Gauss’ Lemma. Let P (x) = a0 + · · · + anxn ∈Z[x]. If P (x)
can be factored in Q[x] as P (x) = q(x)r(x) with q(x), r(x) ∈Q[x], then P (x)
can also be factored in Z[x].
Proof. Express the rational coefﬁcients of q(x) in their lowest terms and let
u be the least common multiple of their denominators. Then q(x) = (1/u)Q(x),
where Q(x) ∈Z[x]. Let s be the greatest common divisor of all the coefﬁcients
of Q(x); write q(x) = (s/u)Q(x), where Q(x) ∈Z[x], and the greatest common
divisor of its coefﬁcients is 1. Write r(x) = (t/v)R(x) in a similar way.
Now
P (x) = q(x)r(x) = s
uQ(x) t
v R(x) = st
uv Q(x)R(x),
so
uvP (x) =
stQ(x)R(x). To prove the theorem, we show that uv|st by proving that no
prime p in uv can divide all the coefﬁcients of Q(x)R(x).
Let Q(x) = b0 + · · · + bkxk and R(x) = c0 + · · · + clxl. Choose a prime p
and let bi and cj be the ﬁrst coefﬁcients of Q(x) and R(x), respectively, that p
fails to divide. These exist because gcd(b0, . . . , bk) = 1 and gcd(c0, . . . , cl) = 1.
The coefﬁcient of xi+j in Q(x)R(x) is
bi+jc0 + bi+j−1c1 + · · · + bi+1cj−1 + bicj + bi−1cj+1 + · · · + b0ci+j.

194
9
POLYNOMIAL AND EUCLIDEAN RINGS
Now p|c0, p|c1, . . . , p|cj−1, p|bi−1, p|bi−2, . . . , p|b0 but p ̸ |bicj so this coefﬁ-
cient is not divisible by p. Hence the greatest common divisor of the coefﬁcients
of Q(x)R(x) is 1; therefore, uv|st and P (x) can be factored in Z[x].
□
Example 9.29. Factor p(x) = x4 −3x2 + 2x + 1 into irreducible factors in Q[x].
Solution. By Theorem 9.25, the only possible rational roots are ±1. However,
these are not roots, so p(x) has no linear factors.
Therefore, if it does factor, it must factor into two quadratics, and by Gauss’
lemma these factors can be chosen to have integral coefﬁcients. Suppose that
x4 −3x2 + 2x + 1 = (x2 + ax + b)(x2 + cx + d)
= x4 + (a + c)x3 + (b + d + ac)x2 + (bc + ad)x + bd.
Thus we have to solve the following system for integer solutions:
a + c = 0,
b + d + ac = −3,
bc + ad = 2,
and
bd = 1.
Therefore, b = d = ±1 and b(a + c) = 2. Hence a + c = ±2, which is a con-
tradiction. The polynomial cannot be factored into two quadratics and therefore
is irreducible in Q[x].
□
Theorem 9.30. Eisenstein’s Criterion. Let f (x) = a0 + a1x + · · · + anxn ∈
Z[x]. Suppose that the following conditions all hold for some prime p:
(i) p|a0, p|a1, . . . , p|an−1.
(ii) p̸ |an.
(iii) p2̸ |a0.
Then f (x) is irreducible over Q.
Proof. Suppose that f (x) is reducible. By Gauss’ lemma, it factors as two
polynomials in Z[x]; that is,
f (x) = (b0 + · · · + brxr)(c0 + · · · + csxs),
where bi, cj ∈Z, s > 0, and r + s = n. Comparing coefﬁcients, we see that a0 =
b0c0. Now p|a0, but p2 ̸ |a0, so p must divide b0 or c0 but not both. Without
loss of generality, suppose that p|b0 and p ̸ |c0. Now p cannot divide all of
b0, b1, . . . , br, for then p would divide an. Let t be the smallest integer for
which p ̸ |bt; thus 1 ⩽t ⩽r < n. Then at = btc0 + bt−1c1 + · · · + b1ct−1 + b0ct
and p|at, p|b0, p|b1, . . . , p|bt−1. Hence p|btc0. However, p̸ |bt and p̸ |c0, so we
have a contradiction, and the theorem is proved.
□

FACTORING POLYNOMIALS OVER FINITE FIELDS
195
For example, Eisenstein’s criterion can be used to show that x5 −2,
x7 + 2x3 + 12x2 −2 and 2x3 + 9x −3 are all irreducible over Q.
Example 9.31. Show that φ(x) = xp−1 + xp−2 + · · · + x + 1 is irreducible over
Q for any prime p. This is called a cyclotomic polynomial and can be written
φ(x) = (xp −1)/(x −1).
Solution. We cannot apply Eisenstein’s criterion to φ(x) as it stands. However,
if we put x = y + 1, we obtain
φ(y + 1) = 1
y [(y + 1)p −1]
= yp−1 +

p
p −1

yp−2 +

p
p −2

yp−3 + · · · +

p
2

y + p
where

p
k

=
p!
k!(p −k)!
is the binomial coefﬁcient. Hence p
divides
k!(p −k)!

p
k

. If 1 ⩽k ⩽p −1, the prime p does not divide k!(p −k)!, so
it must divide

p
k

. Hence φ(y + 1) is irreducible by Eisenstein’s criterion, so
φ(x) is irreducible.
□
FACTORING POLYNOMIALS OVER FINITE FIELDS
The roots of a polynomial in Zp[x] can be found by trying all the p possi-
ble values.
Example 9.32. Does x4 + 4 ∈Z7[x] have any roots in Z7?
Solution. We see from Table 9.3 that x4 + 4 is never zero and therefore has
no roots in Z7.
□
Proposition 9.33. A polynomial in Z2[x] has a factor x + 1 if and only if it has
an even number of nonzero coefﬁcients.
Proof. Let p(x) = a0 + a1x + · · · + anxn ∈Z2[x]. By the factor theorem,
(x + 1) is a factor of p(x) if and only if p(1) = 0. (Remember that x −1 = x + 1
TABLE 9.3. Values Modulo 7
x
0
1
2
3
4
5
6
x4
0
1
2
4
4
2
1
x4 + 4
4
5
6
1
1
6
5

196
9
POLYNOMIAL AND EUCLIDEAN RINGS
in Z2[x].) Now p(1) = a0 + a1 + · · · + an, which is zero in Z2 if and only if p(x)
has an even number of nonzero coefﬁcients.
□
Example 9.34. Find all the irreducible polynomials of degree less than or equal
to 4 over Z2.
Solution. Degree 1 polynomials are irreducible; in Z2[x] we have x and x + 1.
Let p(x) = a0 + · · · + anxn ∈Z2[x]. If p(x) has degree n, then an is nonzero,
so an = 1. The only possible roots are 0 and 1. The element 0 is a root if and
only if a0 = 0, and 1 is a root if and only if p(x) has an even number of nonzero
terms. Hence the following are the polynomials of degrees 2, 3, and 4 in Z2[x]
with no linear factors:
x2 + x + 1
(degree 2)
x3 + x + 1, x3 + x2 + 1
(degree 3)
x4 + x + 1, x4 + x2 + 1, x4 + x3 + 1, x4 + x3 + x2 + x + 1
(degree 4).
If a polynomial of degree 2 or 3 is reducible, it must have a linear factor; hence
the polynomials of degree 2 and 3 above are irreducible. If a polynomial of degree
4 is reducible, it either has a linear factor or is the product of two irreducible
quadratic factors. Now there is only one irreducible quadratic in Z2[x], and its
square (x2 + x + 1)2 = x4 + x2 + 1 is reducible.
Hence the irreducible polynomials of degree ⩽4 over Z2 are x, x + 1,
x2 + x + 1, x3 + x + 1, x3 + x2 + 1, x4 + x + 1, x4 + x3 + 1, and
x4 + x3 + x2 + x + 1.
□
For example, the polynomials of degree 4 in Z2[x] factorize into irreducible
factors as follows.
x4
= x4
x4 + 1
= (x + 1)4
x4 + x
= x(x + 1)(x2 + x + 1)
x4 + x + 1
is irreducible
x4 + x2
= x2(x + 1)2
x4 + x2 + 1
= (x2 + x + 1)2
x4 + x2 + x
= x(x3 + x + 1)
x4 + x2 + x + 1
= (x + 1)(x3 + x2 + 1)
x4 + x3
= x3(x + 1)
x4 + x3 + 1
is irreducible
x4 + x3 + x
= x(x3 + x2 + 1)
x4 + x3 + x + 1
= (x + 1)2(x2 + x + 1)
x4 + x3 + x2
= x2(x2 + x + 1)
x4 + x3 + x2 + 1
= (x + 1)(x3 + x + 1)
x4 + x3 + x2 + x
= x(x + 1)3
x4 + x3 + x2 + x + 1
is irreducible

LINEAR CONGRUENCES AND THE CHINESE REMAINDER THEOREM
197
LINEAR CONGRUENCES AND THE CHINESE
REMAINDER THEOREM
The euclidean algorithm for integers can be used to solve linear congruences. We
ﬁrst ﬁnd the conditions for a single congruence to have a solution and then show
how to ﬁnd all its solutions, if they exist. We then present the Chinese remainder
theorem, which gives conditions under which many simultaneous congruences,
with coprime moduli, have solutions. These solutions can again be found by
using the euclidean algorithm.
First let us consider a linear congruence of the form
ax ≡b mod n.
This has a solution if and only if the equation
ax + ny = b
has integer solutions for x and y. The congruence is also equivalent to the
equation [a][x] = [b] in Zn.
Theorem 9.35. The equation ax + ny = b has solutions for x, y ∈Z if and only
if gcd(a, n)|b.
Proof. Write d = gcd(a, n). If ax + ny = b has a solution, then d|b because
d|a and d|n. Conversely, let d|b, say b = k · d. By Theorem 9.9, there exist
s, t ∈Z such that as + nt = d. Hence ask + ntk = k · d and x = sk, y = tk is
a solution to ax + ny = b.
□
The euclidean algorithm gives a practical way to ﬁnd the integers s and t in
Theorem 9.35. These can then be used to ﬁnd one solution to the equation.
Theorem 9.36. The congruence ax ≡b mod n has a solution if and only if d|b,
where d = gcd(a, n). Moreover, if this congruence does have at least one solu-
tion, the number of noncongruent solutions modulo n is d; that is, if [a][x] = [b]
has a solution in Zn, then it has d different solutions in Zn.
Proof. The condition for the existence of a solution follows immediately from
Theorem 9.35. Now suppose that x0 is a solution, so that ax0 ≡b mod n. Let
d = gcd(a, n) and a = da′, n = dn′. Then gcd(a′, n′) = 1, so the following state-
ments are all equivalent.
(i) x is a solution to the congruence ax ≡b mod n.
(ii) x is a solution to the congruence a(x −x0) ≡0 mod n.
(iii) n|a(x −x0).
(iv) n′|a′(x −x0).

198
9
POLYNOMIAL AND EUCLIDEAN RINGS
(v) n′|(x −x0).
(vi) x = x0 + kn′ for some k ∈Z.
Now x0, x0 + n′, x0 + 2n′, . . . , x0 + (d −1)n′ form a complete set of noncon-
gruent solutions modulo n, and there are d such solutions.
□
Example 9.37. Find the inverse of [49] in the ﬁeld Z53.
Solution. Let [x] = [49]−1 in Z53. Then [49] · [x] = [1]; that is, 49x ≡1
mod 53. We can solve this congruence by solving the equation 49x −1 = 53y,
where y ∈Z. By using the euclidean algorithm we have
53 = 1 · 49 + 4
and
49 = 12 · 4 + 1.
Hence
gcd(49, 53) = 1 = 49 −12 · 4 = 49 −12(53 −49) = 13 · 49 −12 · 53.
Therefore, 13 · 49 ≡1 mod 53 and [49]−1 = [13] in Z53.
□
Theorem 9.38. Chinese Remainder Theorem. Let m = m1m2 · · · mr, where
gcd(mi, mj) = 1 if i ̸= j. Then the system of simultaneous congruences
x ≡a1 mod m1,
x ≡a2 mod m2,
. . . ,
x ≡ar mod mr
always has an integral solution. Moreover, if b is one solution, the complete
solution is the set of integers satisfying x ≡b mod m.
Proof. This result follows from the ring isomorphism
f : Zm →Zm1 × Zm2 × · · · × Zmr
of Theorem 8.20 deﬁned by f ([x]m) = ([x]m1, [x]m2, . . . , [x]mr). The integer
x is a solution of the simultaneous congruences if and only if f ([x]m) =
([a1]m1, [a2]m2, . . . , [ar]mr). Therefore, there is always a solution, and the solution
set consists of exactly one congruence class modulo m.
□
One method of ﬁnding the solution to a set of simultaneous congruences is to
use the euclidean algorithm repeatedly.
Example 9.39. Solve the simultaneous congruences

x ≡36 mod 41
x ≡5 mod 17 .
Proof. Any solution to the ﬁrst congruence is of the form x = 36 + 41t where
t ∈Z. Substituting this into the second congruence, we obtain
36 + 41t ≡5 mod 17
that is,
41t ≡−31 mod 17.

LINEAR CONGRUENCES AND THE CHINESE REMAINDER THEOREM
199
Reducing modulo 17, we have 7t ≡3 mod 17. Solving this by the euclidean
algorithm, we have
17 = 2 · 7 + 3
and
7 = 2 · 3 + 1.
Therefore, 1 = 7 −2(17 −2 · 7) = 7 · 5 −17 · 2 and 7 · 5 ≡1 mod 17. Hence
7 · 15 ≡3 mod 17, so t ≡15 mod 17 is the solution to 7t ≡3 mod 17.
We have shown that if x = 36 + 41t is a solution to both congruences, then
t = 15 + 17u, where u ∈Z. That is,
x = 36 + 41t = 36 + 41(15 + 17u) = 651 + 697u
or x ≡651 mod 697 is the complete solution.
□
Example 9.40. Find the smallest positive integer that has remainders 4, 3, and
1 when divided by 5, 7, and 9, respectively.
Solution. We have to solve the three simultaneous congruences
x ≡4 mod 5,
x ≡3 mod 7,
and
x ≡1 mod 9.
The ﬁrst congruence implies that x = 4 + 5t, where t ∈Z. Substituting into the
second congruence, we have
4 + 5t ≡3 mod 7.
Hence 5t ≡−1 mod 7. Now 5−1 = 3 in Z7, so t ≡3 · (−1) ≡4 mod 7. There-
fore, t = 4 + 7u, where u ∈Z, and any integer satisfying the ﬁrst two congru-
ences is of the form
x = 4 + 5t = 4 + 5(4 + 7u) = 24 + 35u.
Substituting this into the third congruence, we have 24 + 35u ≡1 mod 9 and
−u ≡−23 mod 9. Thus u ≡5 mod 9 and u = 5 + 9v for some v ∈Z.
Hence any solution of the three congruences is of the form
x = 24 + 35u = 24 + 35(5 + 9v) = 199 + 315v.
The smallest positive solution is x = 199.
□
The Chinese remainder theorem was known to ancient Chinese astronomers,
who used it to date events from observations of various periodic astronomical
phenomena. It is used in this computer age as a tool for ﬁnding integer solutions
to integer equations and for speeding up arithmetic operations in a computer.
Addition of two numbers in conventional representation has to be carried out
sequentially on the digits in each position; the digits in the ith position have to

200
9
POLYNOMIAL AND EUCLIDEAN RINGS
be added before the digit to be carried over to the (i + 1)st position is known.
One method of speeding up addition on a computer is to perform addition using
residue representation, since this avoids delays due to carry digits.
Let m = m1m2 · · · mr, where the integers mi are coprime in pairs. The residue
representation or modular representation of any number x in Zm is the r-tuple
(a1, a2, . . . , ar), where x ≡ai mod mi.
For example, every integer from 0 to 29 can be uniquely represented by its
residues modulo 2, 3, and 5 in Table 9.4.
This residue representation corresponds exactly to the isomorphism
Z30 →Z2 × Z3 × Z5.
Since this is a ring isomorphism, addition and multiplication are performed simply
by adding and multiplying each residue separately.
For example, to add 4 and 7 using residue representation, we have
(0, 1, 4) + (1, 1, 2) = (0 + 1, 1 + 1, 4 + 2) = (1, 2, 1).
Similarly, multiplying 4 and 7, we have
(0, 1, 4) · (1, 1, 2) = (0 · 1, 1 · 1, 4 · 2) = (0, 1, 3).
Fast adders can be designed using residue representation, because all the
residues can be added simultaneously. Numbers can be converted easily into
residue form; however, the reverse procedure of ﬁnding a number with a given
residue representation requires the Chinese remainder theorem. See Knuth [19,
Sec. 4.3.2] for further discussion of the use of residue representations in com-
puters.
TABLE 9.4. Residue Representation of the Integers
from 0 to 29
Residues
Residues
Residues
Modulo:
Modulo:
Modulo:
x
2
3
5
x
2
3
5
x
2
3
5
0
0
0
0
10
0
1
0
20
0
2
0
1
1
1
1
11
1
2
1
21
1
0
1
2
0
2
2
12
0
0
2
22
0
1
2
3
1
0
3
13
1
1
3
23
1
2
3
4
0
1
4
14
0
2
4
24
0
0
4
5
1
2
0
15
1
0
0
25
1
1
0
6
0
0
1
16
0
1
1
26
0
2
1
7
1
1
2
17
1
2
2
27
1
0
2
8
0
2
3
18
0
0
3
28
0
1
3
9
1
0
4
19
1
1
4
29
1
2
4

EXERCISES
201
EXERCISES
For Exercises 9.1 to 9.6 calculate the quotients and remainders.
9.1. Divide 3x4 + 4x3 −x2 + 5x −1
by 2x2 + x + 1 in Q[x].
9.2. Divide x6 + x4 −4x3 + 5x
by x3 + 2x2 + 1 in R[x].
9.3. Divide x7 + x6 + x4 + x + 1
by x3 + x + 1 in Z2[x].
9.4. Divide 2x5 + x4 + 2x3 + x2 + 2
by x3 + 2x + 2 in Z3[x].
9.5. Divide 17 + 11i
by 3 + 4i in Z[i].
9.6. Divide 20 + 8i
by 7 −2i in Z[i].
For Exercises 9.7 to 9.13, ﬁnd the greatest common divisors of the elements a, b
in the given euclidean ring, and ﬁnd elements s, t in the ring so that as + bt =
gcd(a, b).
9.7. a = 33, b = 42 in Z.
9.8. a = 2891, b = 1589 in Z.
9.9. a = 2x3 −4x2 −8x + 1, b = 2x3 −5x2 −5x + 2 ∈Q[x].
9.10. a = x6 −x3 −16x2 + 12x −2, b = x5 −2x2 −16x + 8 ∈Q[x].
9.11. a = x4 + x + 1, b = x3 + x2 + x ∈Z3[x].
9.12. a = x4 + 2, b = x3 + 3 ∈Z5[x].
9.13. a = 4 −i, b = 1 + i ∈Z[i].
For Exercises 9.14 to 9.17, ﬁnd one solution to each equation with x, y ∈Z.
9.14. 15x + 36y = 3.
9.15. 24x + 29y = 1.
9.16. 24x + 29y = 6.
9.17. 11x + 31y = 1.
For Exercises 9.18 to 9.21, ﬁnd the inverse to the element in the given ﬁeld.
9.18. [4] in Z7.
9.19. [24] in Z29.
9.20. [35] in Z101.
9.21. [11] in Z31.
Find all integral solutions to the equations in Exercises 9.22 to 9.24.
9.22. 27x + 15y = 13.
9.23. 12x + 20y = 14.
9.24. 28x + 20y = 16.
Factor the polynomials in Exercises 9.25 to 9.36 into irreducible factors in the
given ring.
9.25. x5 −1 in Q[x].
9.26. x5 + 1 in Z2[x].
9.27. x4 + 1 in Z5[x].
9.28. 2x3 + x2 + 4x + 2 in Q[x].
9.29. x4 −9x + 3 in Q[x].
9.30. 2x3 + x2 + 4x + 2 in C[x].
9.31. x3 −4x + 1 in Q[x].
9.32. x4 + 3x3 + 9x −9 in Q[x].
9.33. x8 −16 in C[x].
9.34. x8 −16 ∈R[x].
9.35. x8 −16 in Q[x].
9.36. x8 −16 ∈Z17[x].
9.37. Find all irreducible polynomials of degree 5 over Z2.

202
9
POLYNOMIAL AND EUCLIDEAN RINGS
9.38. Find an irreducible polynomial of degree 2 over Z5.
9.39. Find an irreducible polynomial of degree 3 over Z7.
9.40. Find the kernel and image of the ring morphism ψ: R[x] →C deﬁned by
ψ(p(x)) = p(i), where i =
√
−1.
9.41. Find the kernel and image of the ring morphism ψ: R[x] →C deﬁned by
ψ(p(x)) = p(1 +
√
3i).
In Exercises 9.42 to 9.47, are the polynomials irreducible in the given ring?
Give reasons.
9.42. x3 + x2 + x + 1 in Q[x].
9.43. 3x8 −4x6 + 8x5 −10x + 6 in Q[x].
9.44. x4 + x2 −6 in Q[x].
9.45. 4x3 + 3x2 + x + 1 in Z5[x].
9.46. x5 + 15 in Q[x].
9.47. x4 −2x3 + x2 + 1 in R[x].
9.48. Is Z[x] a euclidean ring when δ(f (x)) = degf (x) for any nonzero poly-
nomial? Is Z[x] a euclidean ring with any other deﬁnition of δ(f (x))?
9.49. Can you deﬁne a division algorithm in R[x, y]? How would you divide
x3 + 3xy + y + 4 by xy + y3 + 2?
9.50. Let Lp be the set of all linear functions f : Zp →Zp of the form f (x) =
ax + b, where a ̸= 0 in Zp. Show that (Lp, Ž ) is a group of order p(p −1)
under composition.
9.51. If p is a prime, prove that (x −a)|(xp−1 −1) in Zp[x] for all nonzero a
in Zp. Hence prove that
xp−1 −1 = (x −1)(x −2) · · · (x −p + 1)
in Zp[x].
9.52. (Wilson’s theorem) Prove that (n −1)! ≡−1 mod n if and only if n
is prime.
9.53. Prove that
√
2/
3√
5 is irrational.
9.54. Find a polynomial in Q[x] with
√
2 +
√
3 as a root. Then prove that
√
2 +
√
3 is irrational.
9.55. Is 5 irreducible in Z[i]?
9.56. Show that Z[
√
−5] = {a + b
√
−5|a, b ∈Z} does not have the unique fac-
torization property.
9.57. Prove that a gaussian integer is irreducible if and only if it is an invertible
element times one of the following gaussian integers:
(1) any prime p in Z with p ≡3 mod 4.
(2) 1 + i.
(3) a + bi, where a is positive and even, and a2 + b2 = p, for some prime
p in Z such that p ≡1 mod 4.
9.58. If r/s is a rational root, in its lowest terms, of a polynomial p(x) with
integral coefﬁcients, show that p(x) = (sx −r)g(x) for some polynomial
g(x) with integral coefﬁcients.

EXERCISES
203
9.59. Prove that r/s, in its lowest terms, cannot be a root of the integral poly-
nomial p(x) unless (s −r)|p(1). This can be used to shorten the list of
possible rational roots of an integral polynomial.
9.60. Let m = m1m2 · · · mr and Mi = m/mi. If gcd(mi, mj) = 1 for i ̸= j, each
of the congruences Miy ≡1 mod mi has a solution y ≡bi mod mi. Prove
that the solution to the simultaneous congruences
x ≡a1 mod m1,
x ≡a2 mod m2,
. . . ,
x ≡ar mod mr
is x ≡r
i=1 Mibiai mod m.
For Exercises 9.61 to 9.64, solve the simultaneous congruences.
9.61. x ≡5 mod 7
9.62. x ≡41 mod 65
x ≡4 mod 6.
x ≡35 mod 72.
9.63. x ≡0 mod 2
9.64. x ≡9 mod 12
x ≡1 mod 3
x ≡3 mod 13
x ≡2 mod 5.
x ≡6 mod 25.
9.65. Prove that det


320
461
5264
72
702
1008
−967
−44
−91
2333
46
127
164
−216
1862
469

is nonzero.
9.66. Solve the following simultaneous equations:
26x −141y = −697
55x −112y = 202
(a) in Z2, (b) in Z3, and (c) in Z5. Then use the Chinese remainder theorem
to solve them in Z assuming they have a pair of integral solutions between
0 and 29.
9.67. The value of det


676
117
522
375
65
290
825
143
639

is positive and less than 100.
Find its value without using a calculator. (If you get tired of doing arith-
metic, calculate its value mod 10 and mod 11 and then use the Chinese
remainder theorem.)
9.68. The polynomial x3 + 5x ∈Z6[x] has six roots. Does this contradict Theo-
rem 9.6?
9.69. If R is an integral domain and R[x] is euclidean, show that R must be
a ﬁeld.
9.70. Assume that R is a euclidean domain in which δ(a + b) ⩽max{δ(a), δ(b)}
whenever a, b, and a + b are all nonzero. Show that the quotient and
remainder in the division algorithm are uniquely determined.

10
QUOTIENT RINGS
In this chapter we deﬁne a quotient ring in a way similar to our deﬁnition of
a quotient group. The analogue of a normal subgroup is called an ideal, and
a quotient ring consists of the set of cosets of the ring by one of its ideals.
As in groups, we have a morphism theorem connecting morphisms, ideals, and
quotient rings. We discover under what conditions quotient rings are ﬁelds. This
will enable us to fulﬁll our long-range goal of extending the number systems by
deﬁning new ﬁelds using quotient rings of some familiar rings.
IDEALS AND QUOTIENT RINGS
If (R, +, ·) is any ring and (S, +) is any subgroup of the abelian group (R, +),
then the quotient group (R/S, +) has already been deﬁned. However, R/S does
not have a ring structure induced on it by R unless S is a special kind of subgroup
called an ideal.
A nonempty subset I of a ring R is called an ideal of R if the following
conditions are satisﬁed for all x, y ∈I and r ∈R:
(i) x −y ∈I.
(ii) x · r and r · x ∈I.
Condition (i) implies that (I, +) is a subgroup of (R, +). In any ring R, R itself
is an ideal, and {0} is an ideal.
Proposition 10.1. Let a be an element of a commutative ring R. The set
{ar|r ∈R} of all multiples of a is an ideal of R called the principal ideal
generated by a. This ideal is denoted by (a).
Proof. Let ar, as ∈(a) and t ∈R. Then ar −as = a(r −s) ∈(a) and (ar)t =
a(rt) ∈(a). Hence (a) is an ideal of R.
□
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
204

IDEALS AND QUOTIENT RINGS
205
For example, (n) = nZ, consisting of all integer multiples of n, is the principal
ideal generated by n in Z.
The set of all polynomials in Q[x] that contain x2 −2 as a factor is the
principal ideal (x2 −2) = {(x2 −2) · p(x)|p(x) ∈Q[x]} generated by x2 −2 in
Q[x]. The set of all real polynomials that have zero constant term is the principal
ideal (x) = {x · p(x)|p(x) ∈R[x]} generated by x in R[x]. It is also the set of
real polynomials with 0 as a root.
The set of all real polynomials, in two variables x and y, that have a zero
constant term is an ideal of R[x, y]. However, this ideal is not principal (see
Exercise 10.30).
However, every ideal is principal in many commutative rings; these are called
principal ideal rings.
Theorem 10.2. A euclidean ring is a principal ideal ring.
Proof. Let I be any ideal of the euclidean ring R. If I = {0}, then I = (0),
the principal ideal generated by 0. Otherwise, I contains nonzero elements. Let
b be a nonzero element of I for which δ(b) is minimal. If a is any other element
in I, then, by the division algorithm, there exist q, r ∈R such that
a = q · b + r
where
r = 0 or δ(r) < δ(b).
Now r = a −q · b ∈I. Since b is a nonzero element of I for which δ(b) is
minimal, it follows that r must be zero and a = q · b. Therefore, a ∈(b) and
I ⊆(b).
Conversely, any element of (b) is of the form q · b for some q ∈R, so q · b ∈
I. Therefore, I ⊇(b), which proves that I = (b). Hence R is a principal ideal
ring.
□
Corollary 10.3. Z is a principal ideal ring, so is F[x], if F is a ﬁeld.
Proof. This follows because Z and F[x] are euclidean rings.
□
Proposition 10.4. Let I be ideal of the ring R. If I contains the identity 1, then
I is the entire ring R.
Proof. Let 1 ∈I and r ∈R. Then r = r · 1 ∈I, so I = R.
□
Let I be any ideal in a ring R. Then (I, +) is a normal subgroup of (R, +),
and we denote the coset of I in R that contains r by I + r. Hence
I + r = {i + r ∈R|i ∈I}.
The cosets of I in R are the equivalence classes under the congruence relation
modulo I. We have
r1 ≡r2 modI
if and only if
r1 −r2 ∈I.

206
10
QUOTIENT RINGS
By Theorem 4.18, the set of cosets R/I = {I + r|r ∈R} is an abelian group
under the operation deﬁned by
(I + r1) + (I + r2) = I + (r1 + r2).
In fact, we get a ring structure in R/I.
Theorem 10.5. Let I be an ideal in the ring R. Then the set of cosets forms a
ring (R/I, +, ·) under the operations deﬁned by
(I + r1) + (I + r2) = I + (r1 + r2)
and
(I + r1)(I + r2) = I + (r1r2).
This ring (R/I, +, ·) is called the quotient ring (or factor ring) of R by I.
Proof. As mentioned above, (R/I, +) is an abelian group; thus we only have
to verify the axioms related to multiplication.
We ﬁrst show that multiplication is well deﬁned on cosets. Let I + r′
1 = I + r1
and I + r′
2 = I + r2, so that r′
1 −r1 = i1 ∈I and r′
2 −r2 = i2 ∈I. Then
r′
1r′
2 = (i1 + r1)(i2 + r2) = i1i2 + r1i2 + i1r2 + r1r2.
Now, since I is an ideal, i1i2, r1i2 and i1r2 ∈I. Hence r′
1r′
2 −r1r2 ∈I, so
I + r′
1r′
2 = I + r1r2, which shows that multiplication is well deﬁned on R/I.
Multiplication is associative and distributive over addition. If r1, r2, r3 ∈
R, then
(I + r1){(I + r2)(I + r3)} = (I + r1)(I + r2r3) = I + r1(r2r3) = I + (r1r2)r3
= (I + r1r2)(I + r3) = {(I + r1)(I + r2)}(I + r3).
Also,
(I + r1){(I + r2) + (I + r3)} = (I + r1){I + (r2 + r3)} = I + r1(r2 + r3)
= I + (r1r2 + r1r3) = (I + r1r2) + (I + r1r3)
= {(I + r1)(I + r2)} + {(I + r1)(I + r3)}.
The other distributive law can be proved similarly. The multiplicative identity is
I + 1. Hence (R/I, +, ·) is a ring.
□

COMPUTATIONS IN QUOTIENT RINGS
207
TABLE 10.1. Quotient Ring Z6/{0, 2, 4}
+
I
I + 1
·
I
I + 1
I
I
I + 1
I
I
I
I + 1
I + 1
I
I + 1
I
I + 1
For example, the quotient ring of Z by (n) is Z/(n) = Zn, the ring of integers
modulo n. A coset (n) + r = {nz + r|z ∈Z} is the equivalent class modulo n
containing r.
If R is commutative, so is the quotient ring R/I, because
(I + r1)(I + r2) = I + r1r2 = I + r2r1 = (I + r2)(I + r1).
Example 10.6. If I = {0, 2, 4} is the ideal generated by 2 in Z6, ﬁnd the tables
for the quotient ring Z6/I.
Solution. There are two cosets of Z6 by I: namely, I = {0, 2, 4} and I + 1 =
{1, 3, 5}. Hence
Z6/I = {I, I + 1}.
The addition and multiplication tables given in Table 10.1 show that the quotient
ring Z6/I is isomorphic to Z2.
□
COMPUTATIONS IN QUOTIENT RINGS
If F is a ﬁeld, the quotient rings of the polynomial ring F[x] form an important
class of rings that will be used to construct new ﬁelds. Recall that F[x] is a
principal ideal ring, so that any quotient ring is of the form F[x]/(p(x)), for some
polynomial p(x) ∈F[x]. We now look at the structure of such a quotient ring.
The elements of the ring F[x]/(p(x)) are equivalence classes under the rela-
tion on F[x] deﬁned by
f (x) ≡g(x) mod(p(x))
if and only if
f (x) −g(x) ∈(p(x)).
Lemma 10.7. f (x) ≡g(x) mod(p(x)) if and only if f (x) and g(x) have the
same remainder when divided by p(x).
Proof. Let f (x) = q(x) · p(x) + r(x) and g(x) = s(x) · p(x) + t(x), where
r(x) and t(x) are zero or have degrees less than that of p(x). Now the lemma
follows because the following statements are equivalent:
(i) f (x) ≡g(x) mod(p(x)).
(ii) f (x) −g(x) ∈(p(x)).
(iii) p(x)|f (x) −g(x).

208
10
QUOTIENT RINGS
(iv) p(x)|[{q(x) −s(x)} · p(x) + (r(x) −t(x))].
(v) p(x)|[r(x) −t(x)].
(vi) r(x) = t(x).
□
Hence every coset of F[x] by (p(x)) contains the zero polynomial or a poly-
nomial of degree less than that of p(x).
Theorem 10.8. If F is a ﬁeld, let P be the ideal (p(x)) in F[x] generated by
the polynomial p(x) of degree n > 0. The different elements of F[x]/(p(x)) are
precisely those of the form
P + a0 + a1x + · · · + an−1xn−1
where
a0, a1, . . . , an−1 ∈F.
Proof. Let P + f (x) be any element of F[x]/(p(x)) and let r(x) be the
remainder when f (x) is divided by p(x). Then, by Lemma 10.7, P + f (x) =
P + r(x), which is of the required form.
Suppose that P + r(x) = P + t(x), where r(x) and t(x) are zero or have
degree less than n. Then
r(x) ≡t(x) mod(p(x)),
and by Lemma 10.7, r(x) = t(x).
□
Example 10.9. Write down the tables for Z2[x]/(x2 + x + 1).
Solution. Let P = (x2 + x + 1), so that
Z2[x]/(x2 + x + 1) = {P + a0 + a1x|a0, a1 ∈Z2}
= {P, P + 1, P + x, P + x + 1}.
The tables for the quotient ring are given in Table 10.2. The addition table is
straightforward to calculate. Multiplication is computed as follows:
(P + x)2 = P + x2 = P + (x2 + x + 1) + (x + 1) = P + x + 1
and
(P + x)(P + x + 1) = P + x2 + x = P + (x2 + x + 1) + 1 = P + 1.
□
Example 10.10. Let P = x2 −2 be the principal ideal of Q[x] generated by
x2 −2. Find the sum and product of P + 3x + 4 and P + 5x −6 in the ring
Q[x]/(x2 −2) = {P + a0 + a1x|a0, a1 ∈Q}.
Solution. (P + 3x + 4) + (P + 5x −6) = P + (3x + 4) + (5x −6) = P +
8x −2. (P + 3x + 4)(P + 5x −6) = P + (3x + 4)(5x −6) = P + 15x2 + 2x
−24. By the division algorithm, 15x2 + 2x −24 = 15(x2 −2) + 2x + 6. Hence,
by Lemma 10.7, P + 15x2 + 2x −24 = P + 2x + 6.
□

MORPHISM THEOREM
209
TABLE 10.2. Ring Z2[x]/(x 2 + x + 1)
+
P
P + 1
P + x
P + x + 1
P
P
P + 1
P + x
P + x + 1
P + 1
P + 1
P
P + x + 1
P + x
P + x
P + x
P + x + 1
P
P + 1
P + x + 1
P + x + 1
P + x
P + 1
P
·
P
P + 1
P + x
P + x + 1
P
P
P
P
P
P + 1
P
P + 1
P + x
P + x + 1
P + x
P
P + x
P + x + 1
P + 1
P + x + 1
P
P + x + 1
P + 1
P + x
There are often easier ways of ﬁnding the remainder of f (x) when divided by
p(x) than by applying the division algorithm directly. If deg p(x) = n and P =
(p(x)), the problem of ﬁnding the remainder reduces to the problem of ﬁnding
a polynomial r(x) of degree less than n such that f (x) ≡r(x) modP . This can
often be solved by manipulating congruences, using the fact that p(x) ≡0 modP .
Consider Example 10.10, in which P is the ideal generated by x2 −2. Then
x2 −2 ≡0 modP and x2 ≡2 modP . Hence, in any congruence modulo P , we
can always replace x2 by 2. For example,
15x2 + 2x −24 ≡15(2) + 2x −24 modP
≡2x + 6 modP,
so P + 15x2 + 2x −24 = P + 2x + 6.
In Example 10.9, P = (x2 + x + 1), so x2 + x + 1 ≡0 modP and x2 ≡x +
1 modP . (Remember + 1 = −1 in Z2.) Therefore, in multiplying two elements
in Z2[x]/P , we can always replace x2 by x + 1. For example,
P + x2 = P + x + 1
and
P + x(x + 1) = P + x2 + x = P + 1.
We have usually written the elements of Zn = Z/(n) simply as 0, 1, . . . ,
n −1 instead of as [0], [1], . . . , [n −1] or as (n) + 0, (n) + 1, . . . , (n) + n −1.
In a similar way, when there is no confusion, we henceforth write the elements
of F[x]/(p(x)) simply as a0 + a1x + · · · + an−1xn−1 instead of (p(x)) + a0 +
a1x + · · · + an−1xn−1.
MORPHISM THEOREM
Proposition 10.11. If f : R →S is a ring morphism, then Kerf is an ideal of R.
Proof. Since any ring morphism is a group morphism, it follows from Propo-
sition 4.23 that Kerf is a subgroup of (R, +). If x ∈Kerf and r ∈R, then

210
10
QUOTIENT RINGS
f (xr) = f (x)f (r) = 0 · f (r) = 0 and xr ∈Kerf . Similarly, rx ∈Kerf , so Kerf
is an ideal of R.
□
Furthermore, any ideal I of a ring R is the kernel of a morphism, for example,
the ring morphism π: R →R/I deﬁned by π(r) = I + r.
The image of a morphism f : R →S can easily be veriﬁed to be a subring
of S.
Theorem 10.12. Morphism Theorem for Rings. If f : R →S is a ring mor-
phism, then R/Kerf is isomorphic to Imf .
This result is also known as the ﬁrst isomorphism theorem for rings; the
second and third isomorphism theorems are given in Exercises 10.19 and 10.20.
Proof. Let K = Kerf . It follows from the morphism theorem for groups
(Theorem 4.23), that ψ: R/K →Imf , deﬁned by ψ(K + r) = f (r), is a group
isomorphism. Hence we need only prove that ψ is a ring morphism. We have
ψ{(K + r)(K + s)} = ψ{K + rs} = f (rs) = f (r)f (s)
= ψ(K + r)ψ(K + s).
□
Example 10.13. Prove that Q[x]/(x2 −2) ∼= Q(
√
2).
Solution. Consider the ring morphism ψ: Q[x] →R deﬁned by ψ(f (x)) =
f (
√
2) in Example 9.24. The kernel is the set of polynomials containing x2 −2
as a factor, that is, the principal ideal (x2 −2). The image of ψ is Q(
√
2) so by
the morphism theorem for rings, Q[x]/(x2 −2) ∼= Q(
√
2).
□
In this isomorphism, the element a0 + a1x ∈Q[x]/(x2 −2) is mapped to
a0 + a1
√
2 ∈Q(
√
2). Addition and multiplication of the elements a0 + a1x and
b0 + b1x in Q[x]/(x2 −2) correspond to the addition and multiplication of the
real numbers a0 + a1
√
2 and b0 + b1
√
2.
Example 10.14. Prove that R[x]/(x2 + 1) ∼= C.
Solution. Deﬁne the ring morphism ψ: R[x] →C by ψ(f (x)) = f (i), where
i =
√
−1. Any polynomial in Kerψ has i as a root, and therefore, by Theo-
rem 9.22, also has −i as a root and contains the factor x2 + 1. Hence Kerψ =
(x2 + 1).
Now ψ(a + bx) = a + ib; thus ψ is surjective. By the morphism theorem for
rings, R[x]/(x2 + 1) ∼= C.
□
QUOTIENT POLYNOMIAL RINGS THAT ARE FIELDS
We now determine when a quotient of a polynomial ring is a ﬁeld. This result
allows us to construct many new ﬁelds.

QUOTIENT POLYNOMIAL RINGS THAT ARE FIELDS
211
Theorem 10.15. Let a be an element of the euclidean ring R. The quotient ring
R/(a) is a ﬁeld if and only if a is irreducible in R.
Proof. Suppose that a is an irreducible element of R and let (a) + b be a
nonzero element of R/(a). Then b is not a multiple of a, and since a is irreducible,
gcd(a, b) = 1. By Theorem 9.9, there exist s, t ∈R such that
sa + tb = 1.
Now sa ∈(a), so [(a) + t] · [(a) + b] = (a) + 1, the identity of R/(a). Hence
(a) + t is the inverse of (a) + b in R/(a) and R/(a) is a ﬁeld.
Now suppose that a is not irreducible in R so that there exist elements s and
t, which are not invertible, with st = a. By Lemma 9.17, δ(s) < δ(st) = δ(a)
and δ(t) < δ(st) = δ(a). Hence s is not divisible by a, and s /∈(a). Similarly,
t /∈(a), and neither (a) + s nor (a) + t is the zero element of R/(a). However,
[(a) + s] · [(a) + t] = (a) + st = (a),
the zero element of R/(a).
Therefore, the ring R/(a) has zero divisors and cannot possibly be a ﬁeld.
□
For example, in the quotient ring Q[x]/P , where P = (x2 −1), the elements
P + x + 1 and P + x −1 are zero divisors because
(P + x + 1) · (P + x −1) = P + x2 −1 = P,
the zero element.
Corollary 10.16. Zp = Z/(p) is a ﬁeld if and only if p is prime.
Proof. This result,
which we proved in Theorem 8.11, follows from
Theorem 10.15 because the irreducible elements in Z are the primes (and
their negatives).
□
Another particular case of Theorem 10.15 is the following important theorem.
Theorem 10.17. The ring F[x]/(p(x)) is a ﬁeld if and only if p(x) is irreducible
over the ﬁeld F. Furthermore, the ring F[x]/(p(x)) always contains a subring
isomorphic to the ﬁeld F.
Proof. The ﬁrst part of the theorem is just Theorem 10.15. Let F = {(p(x)) +
r|r ∈F}. This can be veriﬁed to be a subring of F[x]/(p(x)), which is isomor-
phic to the ﬁeld F
by the isomorphism that takes r ∈F
to (p(x))+
r ∈F[x]/(p(x)).
□
Example 10.18. Show that Z2[x]/(x2 + x + 1) is a ﬁeld with four elements.
Solution. We showed in Example 9.34 that x2 + x + 1 is irreducible over Z2
and in Example 10.9 that the quotient ring has four elements. Hence the quotient
ring is a ﬁeld containing four elements. Its tables are given in Table 10.2.
□

212
10
QUOTIENT RINGS
Example 10.19. Write down the multiplication table for the ﬁeld Z3[x]/(x2 + 1).
Solution. If x = 0, 1, or 2 in Z3, then x2 + 1 = 1, 2, or 2; thus, by the factor
theorem, x2 + 1 has no linear factors. Hence x2 + 1 is irreducible over Z3 and,
by Theorem 10.17, the quotient ring Z3[x]/(x2 + 1) is a ﬁeld. By Theorem 10.8,
the elements of this ﬁeld can be written as
Z3[x]/(x2 + 1) = {a0 + a1x|a0, a1 ∈Z3}.
Hence the ﬁeld contains nine elements. Its multiplication table is given in
Table 10.3. This can be calculated by multiplying the polynomials in Z3[x] and
replacing x2 by −1 or 2, since x2 ≡−1 ≡2mod (x2 + 1).
□
Example 10.20. Show that Q[x]/(x3 −5) = {a0 + a1x + a2x2|ai ∈Q} is a ﬁeld
and ﬁnd the inverse of the element x + 1.
Solution. By the rational roots theorem (Theorem 9.25), (x3 −5) has no linear
factors and hence is irreducible over Q. Therefore, by Theorem 10.17,
Q[x]/(x3 −5) is a ﬁeld.
If s(x) is the inverse of x + 1, then (x + 1)s(x) ≡1 mod(x3 −5); that is,
(x + 1)s(x) + (x3 −5)t(x) = 1 for some t(x) ∈Q[x].
We can ﬁnd such polynomials s(x) and t(x) by the euclidean algorithm. We
have (see below)
x3 −5 = (x2 −x + 1)(x + 1) −6,
so
6 ≡(x2 −x + 1)(x + 1) mod(x3 −5)
and
1 ≡1
6(x2 −x + 1)(x + 1) mod(x3 −5).
TABLE 10.3. Multiplication in Z3[x]/(x 2 + 1)
·
0
1
2
x
x + 1
x + 2
2x
2x + 1
2x + 2
0
0
0
0
0
0
0
0
0
0
1
0
1
2
x
x + 1
x + 2
2x
2x + 1
2x + 2
2
0
2
1
2x
2x + 2
2x + 1
x
x + 2
x + 1
x
0
x
2x
2
x + 2
2x + 2
1
x + 1
2x + 1
x + 1
0
x + 1
2x + 2
x + 2
2x
1
2x + 1
2
x
x + 2
0
x + 2
2x + 1
2x + 2
1
x
x + 1
2x
2
2x
0
2x
x
1
2x + 1
x + 1
2
2x + 2
x + 2
2x + 1
0
2x + 1
x + 2
x + 1
2
2x
2x + 2
x
1
2x + 2
0
2x + 2
x + 1
2x + 1
x
2
x + 2
1
2x

QUOTIENT POLYNOMIAL RINGS THAT ARE FIELDS
213
Hence (x + 1)−1 = 1
6x2 −1
6x + 1
6 in Q[x]/(x3 −5).
x2 −x + 1
x + 1
x3+ 0+0 −5
x3+x2
−x2
−5
−x2−x
x
−5
x
+1
−6
□
Example 10.21. Show that Z3[x]/(x3 + 2x + 1) is a ﬁeld with 27 elements and
ﬁnd the inverse of the element x2.
Solution. If x = 0, 1, or 2 in Z3, then x3 + 2x + 1 = 1; hence x3 + 2x + 1
has no linear factors and is irreducible. Therefore,
Z3[x]/(x3 + 2x + 1) = {a0 + a1x + a2x2|ai ∈Z3}
is a ﬁeld that has 33 = 27 elements.
As in Example 10.20, to ﬁnd the inverse of x2, we apply the euclidean algo-
rithm to x3 + 2x + 1 and x2 in Z3[x].
We have x3 + 2x + 1 = x(x2) + (2x + 1) and x2 = (2x + 2)(2x + 1) + 1.
Hence
1 = x2 −(2x + 2){(x3 + 2x + 1) −x · x2}
= x2(2x2 + 2x + 1) −(2x + 2)(x3 + 2x + 1),
so
1 ≡x2(2x2 + 2x + 1) mod(x3 + 2x + 1)
and the inverse of x2 in Z3[x]/(x3 + 2x + 1) is 2x2 + 2x + 1.
x
x2
x3+0+2x+1
x3
2x+1
2x + 2
2x + 1
x2+ 0+0
x2+2x
x+0
x+2
1
□

214
10
QUOTIENT RINGS
We cannot use Theorem 10.15 directly on a ﬁeld to obtain any new quotient
ﬁelds, because the only ideals of a ﬁeld are the zero ideal and the entire ﬁeld. In
fact, the following result shows that a ﬁeld can be characterized by its ideals.
Theorem 10.22. The nontrivial commutative ring R is a ﬁeld if and only if (0)
and R are its only ideals.
Proof. Let I be an ideal in the ﬁeld R. Suppose that I ̸= (0), so that there is
a nonzero element a ∈I. Since a−1 ∈R, a · a−1 = 1 ∈I. Therefore, by Propo-
sition 10.4, I = R. Hence R has only trivial ideals.
Conversely, suppose that (0) and R are the only ideals in the ring R. Let
a be a nonzero element of R and consider (a) the principal ideal generated
by a. Since 1 · a ∈(a), (a) ̸= (0), and hence (a) = R. Hence 1 ∈R = (a), so
there must exist some b ∈R such that a · b = 1. Therefore, b = a−1 and R is
a ﬁeld.
□
EXERCISES
For Exercises 10.1 to 10.6, ﬁnd all the ideals in the rings.
10.1. Z2 × Z2.
10.2. Z18.
10.3. Q.
10.4. Z7.
10.5. C[x].
10.6. Z[i].
For Exercises 10.7 to 10.10, construct addition and multiplication tables for the
rings. Find all the zero divisors in each ring. Which of these rings are ﬁelds?
10.7. Z6/(3).
10.8. Z2[x]/(x3 + 1).
10.9. Z3 × Z3/((1, 2)).
10.10. Z3[x]/(x2 + 2x + 2).
For Exercises 10.11 to 10.14, compute the sum and product of the elements in the
given quotient rings.
10.11. 3x + 4 and 5x −2 in Q[x]/(x2 −7).
10.12. x2 + 3x + 1 and −2x2 + 4 in Q[x]/(x3 + 2).
10.13. x2 + 1 and x + 1 in Z2[x]/(x3 + x + 1).
10.14. ax + b and cx + d in R[x]/(x2 + 1), where a, b, c, d ∈R.
10.15. If U and V are ideals in a ring R, prove that U ∩V is also an ideal in
R.
10.16. Show, by example, that if U and V are ideals in a ring R, then U ∪V is
not necessarily an ideal in R. But prove that U + V = {u + v|u ∈U, v ∈
V } is always an ideal in R.
10.17. Find a generator of the following ideals in the given ring and prove a
general result for the intersection of two ideals in a principal ideal ring.
(a) (2) ∩(3) in Z.
(b) (12) ∩(18) in Z.
(c) (x2 −1) ∩(x + 1) in Q[x].

EXERCISES
215
10.18. Find a generator of the following ideals in the given ring and prove a
general result for the sum of two ideals in a principal ideal ring.
(a) (2) + (3) in Z.
(b) (9) + (12) in Z.
(c) (x2 + x + 1) + (x2 + 1) in Z2[x].
10.19. (Second isomorphism theorem for rings) If I and J are ideals of the
ring R, prove that
I/(I ∩J) ∼= (I + J)/J.
10.20. (Third isomorphism theorem for rings) Let I and J be two ideals of
the ring R, with J ⊆I. Prove that I/J is an ideal of R/J and that
(R/J)/(I/J) ∼= R/I.
For Exercises 10.21 to 10.29, prove the isomorphisms.
10.21. R[x]/(x2 + 5) ∼= C.
10.22. Z[x]/(x2 + 1) ∼= Z[i] = {a + ib|a, b ∈Z}.
10.23. Q[x]/(x2 −7) ∼= Q(
√
7) = {a + b
√
7|a, b ∈Q}.
10.24. Z[x]/(2x −1) ∼= {a/b ∈Q|a ∈Z, b = 2r, r ⩾0}, a subring of Q.
10.25. Z14/(7) ∼= Z7.
10.26. Z14/(2) ∼= Z2.
10.27. R[x, y]/(x + y) ∼= R[y].
10.28. (R × S)/((1, 0)) ∼= S.
10.29. P(X)/P(X −Y) ∼= P(Y), where Y is a subset of X and the operations
in these boolean rings are symmetric difference and intersection.
10.30. Let I be the set of all polynomials with no constant term in R[x, y]. Find
a ring morphism from R[x, y] to R whose kernel is the ideal I. Prove
that I is not a principal ideal.
10.31. Let I = {p(x) ∈Z[x]| 5|p(0)}. Prove that I is an ideal of Z[x] by ﬁnding
a ring morphism from Z[x] to Z5 with kernel I. Prove that I is not a
principal ideal.
10.32. Let I ⊆P(X) with the property that, if A ∈I, then all the subsets of A
are in I, and also if A and B are disjoint sets in I, then A ∪B ∈I. Prove
that I is an ideal in the boolean ring (P(X), , ∩).
10.33. Is {p(x) ∈Q[x]|p(0) = 3} an ideal of Q[x]?
10.34. Is

a
0
b
0

∈M2(Z)|a, b ∈Z

an ideal of M2(Z)?
10.35. What is the smallest ideal in M2(Z) containing

1
0
0
0

?
10.36. Let a, b be elements of a euclidean ring R. Prove that
(a) ⊆(b)
if and only if
b|a.
For the rings in Exercises 10.37 and 10.38, ﬁnd all the ideals and draw the poset
diagrams of the ideals under inclusion.
10.37. Z8.
10.38. Z20.

216
10
QUOTIENT RINGS
Which of the elements in Exercises 10.39 to 10.46 are irreducible in the given
ring? If an element is irreducible, ﬁnd the corresponding quotient ﬁeld modulo
the ideal generated by that element.
10.39. 11 in Z.
10.40. 10 in Z.
10.41. x2 −2 in R[x].
10.42. x3 + x2 + 2 in Z3[x].
10.43. x4 −2 in Q[x].
10.44. x7 + 4x3 −3ix + 1 in C[x].
10.45. x2 −3 in Q(
√
2)[x].
10.46. 3x5 −4x3 + 2 in Q[x].
Which of the rings in Exercises 10.47 to 10.56 are ﬁelds? Give reasons.
10.47. Z2 × Z2.
10.48. Z4.
10.49. Z17.
10.50. R3.
10.51. Q[x]/(x3 −3).
10.52. Z7[x]/(x2 + 1).
10.53. Z5[x]/(x2 + 1).
10.54. R[x]/(x2 + 7).
10.55. Q(
4√
11) = {a + b11
1
4 + c11
1
2 + d11
3
4 |a, b, c, d ∈Q}.
10.56. Mn(R).
10.57. An ideal I ̸= R is said to be a maximal ideal in the commutative ring
R if, whenever U is an ideal of R such that I ⊆U ⊆R, then U = I or
U = R. Show that the nonzero ideal (a) of a euclidean ring R is maximal
if and only if a is irreducible in R.
10.58. If I is an ideal in a commutative ring R, prove that R/I is a ﬁeld if and
only if I is a maximal ideal of R.
10.59. Find all the ideals in the ring of formal power series, R[[x]]. Which of
the ideals are maximal?
10.60. Let C[0, 1] = {f : [0, 1] →R|f is continuous}, the ring of real-valued
continuous functions on the interval [0, 1]. Prove that Ia = {f ∈
C[0, 1]|f (a) = 0} is a maximal ideal in C[0, 1] for each a ∈[0, 1]. (Every
maximal ideal is, in fact, of this form, but this is much harder to prove.)
10.61. If P is an ideal in a commutative ring R, show that R/P is an integral
domain if and only if ab ∈P can only happen if a ∈P or b ∈P . The
ideal P is called a prime ideal in this case.
10.62. Let R be a commutative ring. An element a ∈R is called nilpotent if
an = 0 for some n ⩾0 in Z. The set N(R) of all nilpotents in R is called
the radical of R.
(a) Show that N(R) is an ideal of R (the binomial theorem is useful).
(b) Show that N[R/N(R)] = {N(R)}.
(c) Show that N(R) is contained in the intersection of all prime ideals
of R (see Exercise 10.61). In fact, N(R) equals the intersection of
all prime ideals of R.
10.63. A commutative ring R is called local if the set J(R) of all non-invertible
elements forms an ideal of R.
(a) Show that every ﬁeld is local as is Zpn for each prime p and n ⩾1,
but that Z6 is not local.

EXERCISES
217
(b) Show that Z(p) =
r
s ∈Q|p does not divide s

is a local subring of
Q for each prime p.
(c) If R is local show that R/J(R) is a ﬁeld.
(d) If R/N(R) is a ﬁeld, show that R is local (if a is nilpotent, then 1 −a
is invertible).
10.64. If R is a (possibly noncommutative) ring, an additive subgroup L of R is
called a left ideal if rx ∈L for all r ∈R and x ∈L. Show that the only
left ideals of R are {0} and R if and only if every nonzero element of R
has an inverse (then R is called a skew ﬁeld).
10.65. If F is a ﬁeld, show that R = M2(F) has exactly two ideals: 0 and R.
(Because of this, R is called a simple ring.) Conclude that Theorem 10.22
fails if the ring is not commutative.

11
FIELD EXTENSIONS
We proved in Chapter 10 that if p(x) is an irreducible polynomial over the ﬁeld
F, the quotient ring K = F[x]/(p(x)) is a ﬁeld. This ﬁeld K contains a subring
isomorphic to F; thus K can be considered to be an extension of the ﬁeld F. We
show that the polynomial p(x) now has a root α in this extension ﬁeld K, even
though p(x) was irreducible over F. We say that K can be obtained from F by
adjoining the root α. We can construct the complex numbers C in this way, by
adjoining a root of x2 + 1 to the real numbers R.
Another important achievement is the construction of a ﬁnite ﬁeld with pn
elements for each prime p. Such a ﬁeld is called a Galois ﬁeld of order pn and
is denoted by GF(pn). We show how this ﬁeld can be constructed as a quotient
ring of the polynomial ring Zp[x], by an irreducible polynomial of degree n.
FIELD EXTENSIONS
A subﬁeld of a ﬁeld K is a subring F that is also a ﬁeld. In this case, the ﬁeld
K is called an extension of the ﬁeld F. For example, Q is a subﬁeld of R; thus
R is an extension of the ﬁeld Q.
Example 11.1. Let p(x) be a polynomial of degree n irreducible over the ﬁeld
F, so that the quotient ring
K = F[x]/(p(x)) = {a0 + a1x + · · · + an−1xn−1|ai ∈F}
is a ﬁeld. Then K is an extension ﬁeld of F.
Solution. This follows from Theorem 10.17 when we identify the coset
(p(x)) + a0 containing the constant term a0 with the element a0 of F.
□
Proposition 11.2. Let K be an extension ﬁeld of F. Then K is a vector space
over F.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
218

FIELD EXTENSIONS
219
Proof. K is an abelian group under addition. Elements of K can be multiplied
by elements of F. This multiplication satisﬁes the following properties:
(i) If 1 is the identity element of F then 1k = k for all k ∈K.
(ii) If λ ∈F and k, l ∈K, then λ(k + l) = λk + λl.
(iii) If λ, µ ∈F and k ∈K, then (λ + µ)k = λk + µK.
(iv) If λ, µ ∈F and k ∈K, then (λµ)k = λ(µk).
Hence K is a vector space over F.
□
The fact that a ﬁeld extension K is a vector space over F tells us much
about the structure of K. The elements of K can be written uniquely as a linear
combination of certain elements called basis elements. Furthermore, if the vector
space K has ﬁnite dimension n over the ﬁeld F, there will be n basis elements,
and the construction of K is particularly simple.
The degree of the extension K of the ﬁeld F, written [K : F], is the dimension
of K as a vector space over F. The ﬁeld K is called a ﬁnite extension if [K : F]
is ﬁnite.
Example 11.3. [C : R] = 2.
Solution. C = {a + ib|a, b ∈R}; therefore, 1 and i span the vector space C
over R. Now 1 and i are linearly independent since, if λ, µ ∈R, then λ1 + µi = 0
implies
that
λ = µ = 0.
Hence
{1, i}
is
a
basis
for
C
over
R
and
[C : R] = 2.
□
Example 11.4. If K = Z5[x]/(x3 + x + 1), then [K: Z5] = 3.
Solution. {1, x, x2} is a basis for K over Z5 because by Theorem 10.8, every
element of K can be written uniquely as the coset containing a0 + a1x + a2x2,
where ai ∈Z5. Hence [K: Z5] = 3.
□
Example 11.4 is a special case of the following theorem.
Theorem 11.5. If p(x) is an irreducible polynomial of degree n over the ﬁeld
F, and K = F[x]/(p(x)), then [K : F] = n.
Proof. By Theorem 10.8, K = {a0 + a1x + · · · + an−1xn−1|ai ∈F}, and such
expressions for the elements of K are unique. Hence {1, x, x2, . . . , xn−1} is a
basis for K over F, and [K : F] = n.
□
Theorem 11.6. Let L be a ﬁnite extension of K and K a ﬁnite extension of F.
Then L is a ﬁnite extension of F and [L : F] = [L : K][K : F].
Proof. We have three ﬁelds, F, K, L, with L ⊇K ⊇F. We prove the theorem
by taking bases for L over K, and K over F, and constructing a basis for L
over F.

220
11
FIELD EXTENSIONS
Let [L : K] = m and let {u1, . . . , um} be a basis for L over K. Let [K : F] = n
and let {v1, . . . , vn} be a basis for K over F. We show that
B = {vjui|i = 1, . . . , m, j = 1, . . . , n} is a basis for L over F.
If x ∈L, then x = m
i=1 λiui, for some λi ∈K. Now each element λi can be
written as λi = n
j=1 µijvj, for some µij ∈F. Hence x = m
i=1
n
j=1 µijvjui,
and B spans L over F.
Now suppose that m
i=1
n
j=1 µijvjui = 0, where µij ∈F. Then, since
u1, . . . , um are linearly independent over K, it follows that n
j=1 µijvj = 0 for
each i = 1, . . . , m. But v1, . . . , vn are linearly independent over F so µij = 0
for each i and each j.
Hence the elements of B are linearly independent, and B is a basis for L over
F. Therefore, [L : F] = m · n = [L : K][K : F].
□
Example 11.7. Show that there is no ﬁeld lying strictly between Q and L =
Q[x]/(x3 −2).
Solution. The constant polynomials in L are identiﬁed with Q. Suppose that
K is a ﬁeld such that L ⊇K ⊇Q. Then [L : Q] = [L : K][K : Q], by Theo-
rem 11.6. But, by Theorem 11.5, [L : Q] = 3, so [L : K] = 1, or [K : Q] = 1.
If [L : K] = 1, then L is a vector space over K, and {1}, being linearly
independent, is a basis. Hence L = K. If [K : Q] = 1, then K = Q. Hence there
is no ﬁeld lying strictly between L and Q.
□
Given a ﬁeld extension K of F and an element a ∈K, deﬁne F(a) to be
the intersection of all subﬁelds of K that contain F and a. This is the smallest
subﬁeld of K containing F and a, and is called the ﬁeld obtained by adjoining
a to F.
For example, the smallest ﬁeld containing R and i is the whole of the complex
numbers, because this ﬁeld must contain all elements of the form a + ib where
a, b ∈R. Hence R(i) = C.
In a similar way, the ﬁeld obtained by adjoining a1, . . . , an ∈K to F is
denoted by F(a1, . . . , an) and is deﬁned to be the smallest subﬁeld of F con-
taining a1, . . . , an and F. It follows that F(a1, . . . , an) = F(a1, . . . , an−1)(an).
Example 11.8. Q(
√
2) is equal to the subﬁeld F = {a + b
√
2|a, b ∈Q} of R.
Solution. Q(
√
2) must contain all rationals and
√
2. Hence Q(
√
2) must con-
tain all real numbers of the form b
√
2 for b ∈Q and also a + b
√
2 for a, b ∈Q.
Therefore, F ⊆Q(
√
2). But Q(
√
2) is the smallest ﬁeld containing Q and
√
2.
Since F is another such ﬁeld, F ⊇Q(
√
2) and so F = Q(
√
2).
□
If R is an integral domain and x is an indeterminate, then
R(x) =
 a0 + a1x + · · · + anxn
b0 + b1x + · · · + bmxm
ai, bj ∈R; not all the bj’s are zero

,

ALGEBRAIC NUMBERS
221
which is the ﬁeld of rational functions in R. Any ﬁeld containing R and x must
contain the polynomial ring R[x], and the smallest ﬁeld containing R[x] is its
ﬁeld of fractions R(x).
ALGEBRAIC NUMBERS
If K is a ﬁeld extension of F, the element k ∈K is called algebraic over F if
there exist a0, a1, . . . , an ∈F, not all zero, such that
a0 + a1k + · · · + ankn = 0.
In other words, k is the root of a nonzero polynomial in F[x]. Elements that are
not algebraic over F are called transcendental over F.
For example, 5,
√
3, i,
n√
7 + 3 are all algebraic over Q because they are roots
of the polynomials x −5, x2 −3, x2 + 1, (x −3)n −7, respectively.
Example 11.9. Find a polynomial in Q[x] with
3√
2 +
√
5 as a root.
Solution. Let x =
3√
2 +
√
5. We have to eliminate the square and cube roots
from this equation. We have x −
√
5 =
3√
2, so (x −
√
5)3 = 2 or
x3 −3
√
5x2 + 15x −5
√
5 = 2. Hence x3 + 15x −2 =
√
5(3x2 + 5), so
(x3 + 15x −2)2 = 5(3x2 + 5)2. Therefore,
3√
2 +
√
5 is a root of
x6 −15x4 −4x3 + 75x2 −60x −121 = 0.
□
Not all real and complex numbers are algebraic over Q. The numbers π and
e can be proven to be transcendental over Q (see Stewart [35]). Since π is
transcendental, we have
Q(π) =
 a0 + a1π + · · · + anπn
b0 + b1π + · · · + bmπm
ai, bj ∈Q; not all the bjs are zero

,
the ﬁeld of rational functions in π with coefﬁcients in Q. Q(π) must contain all
the powers of π and hence any polynomial in π with rational coefﬁcients. Any
nonzero element of Q(π) must have its inverse in Q(π); thus Q(π) contains the
set of rational functions in π. The number b0 + b1π + · · · + bmπm is never zero
unless b0 = b1 = · · · = bm = 0 because π is not the root of any polynomial with
rational coefﬁcients. This set of rational functions in π can be shown to be a
subﬁeld of R.
Those readers acquainted with the theory of inﬁnite sets can prove that the set
of rational polynomials, Q[x], is countable. Since each polynomial has only a
ﬁnite number of roots in C, there are only a countable number of real or complex
numbers algebraic over Q. Hence there must be an uncountable number of real
and complex numbers transcendental over Q.
Example 11.10. Is cos(2π/5) algebraic or transcendental over Q?

222
11
FIELD EXTENSIONS
Solution. We know from De Moivre’s theorem that
(cos 2π/5 + i sin 2π/5)5 = cos 2π + i sin 2π = 1.
Taking real parts and writing c = cos 2π/5 and s = sin 2π/5, we have
c5 −10s2c3 + 5s4c = 1.
Since s2 + c2 = 1, we have c5 −10(1 −c2)c3 + 5(1 −c2)2c = 1. That is,
16c5 −20c3 + 5c −1 = 0 and hence c = cos 2π/5 is algebraic over Q.
□
Theorem 11.11. Let α be algebraic over F and let p(x) be an irreducible poly-
nomial of degree n over F with α as a root. Then
F(α) ∼= F[x]/(p(x)),
and the elements of F(α) can be written uniquely in the form
c0 + c1α + c2α2 + · · · + cn−1αn−1
where
ci ∈F.
Proof. Deﬁne the ring morphism f : F[x] →F(α) by f (q(x)) = q(α). The
kernel of f is an ideal of F[x]. By Corollary 10.3, all ideals in F[x] are principal;
thus Kerf = (r(x)) for some r(x) ∈F[x]. Since p(α) = 0, p(x) ∈Kerf , and
so r(x)|p(x). Since p(x) is irreducible, p(x) = kr(x) for some nonzero element
k of F. Therefore, Kerf = (r(x)) = (p(x)).
By the morphism theorem,
F[x]/(p(x)) ∼= Imf ⊆F(α).
Now, by Theorem 10.17, F[x]/(p(x)) is a ﬁeld; thus Imf is a subﬁeld of F(α)
that contains F and α. Since Imf cannot be a smaller ﬁeld than F(α), it follows
that Imf = F(α) and F[x]/(p(x)) ∼= F(α).
The unique form for the elements of F(α) follows from the isomorphism
above and Theorem 10.8.
□
Corollary 11.12. If α is a root of the polynomial p(x) of degree n, irreducible
over F, then [F(α): F] = n.
Proof. By Theorems 11.11 and 11.5, [F(α) : F] = [F[x]/(p(x)) : F] = n.
□
For example, Q(
√
2) ∼= Q[x]/(x2 −2) and [Q(
√
2) : Q] = 2. Also, Q(
4√
7i) ∼=
Q[x]/(x4 −7) and [Q(
4√
7i) : Q] = 4 because
4√
7 i is a root of x4 −7, which
is irreducible over Q, by Eisenstein’s criterion (Theorem 9.30).

ALGEBRAIC NUMBERS
223
Lemma 11.13. Let p(x) be an irreducible polynomial over the ﬁeld F. Then F
has a ﬁnite extension ﬁeld K in which p(x) has a root.
Proof. Let p(x) = a0 + a1x + a2x2 + · · · + anxn and denote the ideal (p(x))
by P . By Theorem 11.5, K = F[x]/P is a ﬁeld extension of F of degree n
whose elements are cosets of the form P + f (x). The element P + x ∈K is a
root of p(x) because
a0 + a1(P + x) + a2(P + x)2 + · · · + an(P + x)n
= a0 + (P + a1x) + (P + a2x2) + · · · + (P + anxn)
= P + (a0 + a1x + a2x2 + · · · + anxn) = P + p(x)
= P + 0,
and this is the zero element of the ﬁeld K.
□
Theorem 11.14. If f (x) is any polynomial over the ﬁeld F, there is an extension
ﬁeld K of F over which f (x) splits into linear factors.
Proof. We prove this by induction on the degree of f (x). If deg f (x) ⩽1,
there is nothing to prove.
Suppose that the result is true for polynomials of degree n −1. If f (x) has
degree n, we can factor f (x) as p(x)q(x), where p(x) is irreducible over F.
By Lemma 11.13, F has a ﬁnite extension K′ in which p(x) has a root, say α.
Hence, by the factor theorem,
f (x) = (x −α)g(x)
where
g(x) is of degree n −1 in K′[x].
By the induction hypothesis, the ﬁeld K′ has a ﬁnite extension, K, over which
g(x) splits into linear factors. Hence f (x) also splits into linear factors over K
and, by Theorem 11.6, K is a ﬁnite extension of F.
□
Let us now look at the development of the complex numbers from the real
numbers. The reason for constructing the complex numbers is that certain equ-
ations, such as x2 + 1 = 0, have no solution in R. Since x2 + 1 is a quadratic
polynomial in R[x] without roots, it is irreducible over R. In the above manner,
we can extend the real ﬁeld to
R[x]/(x2 + 1) = {a + bx|a, b ∈R}.
In this ﬁeld extension
(0 + 1x)2 = −1
since
x2 ≡−1
mod(x2 + 1).
Denote the element 0 + 1x by i, so that i2 = −1 and i is a root of the equation
x2 + 1 = 0 in this extension ﬁeld. The ﬁeld of complex numbers, C, is deﬁned
to be R(i) and, by Theorem 11.11, there is an isomorphism
ψ: R[x]/(x2 + 1) →R(i)

224
11
FIELD EXTENSIONS
deﬁned by ψ(a + bx) = a + bi. Since
(a + bx) + (c + dx) ≡(a + c) + (b + d)x
mod(x2 + 1)
and
(a + bx)(c + dx) ≡ac + (ad + bc)x + bdx2
mod(x2 + 1)
≡(ac −bd) + (ad + bc)x
mod(x2 + 1),
addition and multiplication in C = R(i) are deﬁned in the standard way by
(a + bi) + (c + di) = (a + c) + (b + d)i
and
(a + bi)(c + di) = (ac −bd) + (ad + bc)i.
Example 11.15. Find [Q(cos 2π/5) : Q].
Solution. We know from Example 11.10 that cos 2π/5 is algebraic over Q
and is a root of the polynomial 16x5 −20x3 + 5x −1. Using the same methods,
we can show that cos 2kπ/5 is also a root of this equation for each k ∈Z.
Hence we see from Figure 11.1 that its roots are 1, cos 2π/5 = cos 8π/5, and
cos 4π/5 = cos 6π/5. Therefore, (x −1) is a factor of the polynomial and
16x5 −20x3 + 5x −1 = (x −1)(16x4 + 16x3 −4x2 −4x + 1)
= (x −1)(4x2 + 2x −1)2.
It follows that cos 2π/5 and cos 4π/5 are roots of the quadratic 4x2 + 2x −
1 so by the quadratic formula, these roots are (−1 ±
√
5)/4. Since cos 2π/5
is positive,
cos 2π/5 = (
√
5 −1)/4
and
cos 4π/5 = (−
√
5 −1)/4.
Therefore, Q(cos 2π/5) ∼= Q[x]/(4x2 + 2x −1) because 4x2 + 2x −1 is irre-
ducible over Q. By Corollary 11.12, [Q(cos 2π/5) : Q] = 2.
□
q = 4p
5
q = 6p
5
q = 8p
5
q = 2p
5
q = 0
cos q
1
Figure 11.1.
Values of cos(2kπ/5).

GALOIS FIELDS
225
Proposition 11.16. If [K : F] = 2, where F ⊇Q, then K = F(√γ ) for some
γ ∈F.
Proof. K is a vector space of dimension 2 over F. Extend {1} to a basis {1, α}
for K over F, so that K = {aα + b|a, b ∈F}.
Now K is a ﬁeld, so α2 ∈K and α2 = aα + b for some a, b ∈F. Hence
(α −(a/2))2 = b + (a2/4). Put β = α −(a/2). Then {1, β} is also a basis for
K over F, and K = F(β) where β2 = b + (a2/4) = γ ∈F. Hence K = F(√γ ).
□
Proposition 11.17. If F is an extension ﬁeld of R of ﬁnite degree, then F is
isomorphic to R or C.
Proof. Let [F : R] = n. If n = 1, then F is equal to R. Otherwise, n > 1
and F contains some element α not in R. Now {1, α, α2, . . . , αn} is a linearly
dependent set of elements of F over R, because it contains more than n elements;
hence there exist real numbers λ0, λ1, . . . , λn, not all zero, such that
λ0 + λ1α + · · · + λnαn = 0.
The element α is therefore algebraic over R so since the only irreducible poly-
nomials over R have degree 1 or 2, α must satisfy a linear or quadratic equation
over R. If it satisﬁes a linear equation, then α ∈R, contrary to our hypothesis.
Therefore, [R(α) : R] = 2, and, by Proposition 11.16, R(α) = R(√γ ). In this
case γ must be negative because R contains all positive square roots; hence
√γ = ir, where r ∈R and R(α) = R(i) ∼= C.
Therefore, the ﬁeld F contains a subﬁeld C = R(α) isomorphic to the complex
numbers and [F : C] = [F : R]/2, which is ﬁnite. By an argument similar to
the above, any element of C is the root of an irreducible polynomial over C.
However, the only irreducible polynomials over C are the linear polynomials,
and all their roots lie in C. Hence F = C is isomorphic to C.
□
Example 11.18. [R : Q] is inﬁnite.
Solution. The real number
n√
2 is a root of the polynomial xn −2, which, by
Eisenstein’s criterion, is irreducible over Q. If [R : Q] were ﬁnite, we could use
Theorem 11.6 and Corollary 11.12 to show that
[R : Q] = [R : Q(
n√
2)][Q(
n√
2) : Q] = [R : Q(
n√
2)]n.
This is a contradiction, because no ﬁnite integer is divisible by every integer n.
Hence [R : Q] must be inﬁnite.
□
GALOIS FIELDS
In this section we investigate the structure of ﬁnite ﬁelds; these ﬁelds are called
Galois ﬁelds in honor of the mathematician ´Evariste Galois (1811–1832).

226
11
FIELD EXTENSIONS
We show that the element 1 in any ﬁnite ﬁeld generates a subﬁeld isomorphic
to Zp, for some prime p called the characteristic of the ﬁeld. Hence a ﬁnite ﬁeld
is some ﬁnite extension of the ﬁeld Zp and so must contain pm elements, for
some integer m.
The characteristic can be deﬁned for any ring, and we give the general deﬁ-
nition here, even though we are mainly interested in its application to ﬁelds.
For any ring R, deﬁne the ring morphism f : Z →R by f (n) = n · 1R where
1R is the identity of R. The kernel of f is an ideal of the principal ideal ring Z;
hence Kerf = (q) for some q ⩾0. The generator q ⩾0 of Kerf is called the
characteristic of the ring R. If a ∈R then qa = q(1Ra) = (q1R)a = 0, Hence
if q > 0 the characteristic of R is the least integer q > 0 for which qa = 0, for
all a ∈R. If no such number exists, the characteristic of R is zero. For example,
the characteristic of Z is 0, and the characteristic of Zn is n.
Proposition 11.19. The characteristic of an integral domain is either zero or
prime.
Proof. Let q be the characteristic of an integral domain D. By applying the
morphism theorem to f : Z →D, deﬁned by f (1) = 1, we see that
f (Z) ∼=

Zq
if q ̸= 0
Z
if q = 0.
But f (Z) is a subring of an integral domain; therefore, it has no zero divisors,
and, by Theorem 8.11, q must be zero or prime.
□
The characteristic of the ﬁeld Zp is p, while Q, R, and C have zero charac-
teristic.
Proposition 11.20. If the ﬁeld F has prime characteristic p, then F contains a
subﬁeld isomorphic to Zp. If the ﬁeld F has zero characteristic, then F contains
a subﬁeld isomorphic to the rational numbers, Q.
Proof. From the proof of Proposition 11.19 we see that F contains the sub-
ring f (Z), which is isomorphic to Zp if F has prime characteristic p. If the
characteristic of F is zero, f : Z →f (Z) is an isomorphism. We show that F
contains the ﬁeld of fractions of f (Z) and that this is isomorphic to Q.
Let Q = {xy−1 ∈F|x, y ∈f (Z)}, a subring of F. Deﬁne the function
˜f : Q →Q
by ˜f (a/b) = f (a) · f (b)−1. Since rational numbers are deﬁned as equivalence
classes, we have to check that ˜f is well deﬁned. We can show that ˜f (a/b) =
˜f (c/d) if a/b = c/d. Furthermore, it can be veriﬁed that ˜f is a ring isomorphism.
Hence Q is isomorphic to Q.
□

GALOIS FIELDS
227
Corollary 11.21. The characteristic of a ﬁnite ﬁeld is nonzero.
Theorem 11.22. If F is a ﬁnite ﬁeld, it has pm elements for some prime p and
some integer m.
Proof. By the previous results, F has characteristic p, for some prime p, and
contains a subﬁeld isomorphic to Zp. We identify this subﬁeld with Zp so that F
is a ﬁeld extension of Zp. The degree of this extension must be ﬁnite because F
is ﬁnite. Let [F : Zp] = m and let {f1, . . . , fm} be a basis of F over Zp, so that
F = {λ1f1 + · · · + λmfm|λi ∈Zp}.
There are p choices for each λi; therefore, F contains pm elements.
□
A ﬁnite ﬁeld with pm elements is called a Galois ﬁeld of order pm and is
denoted by GF(pm). It can be shown that for a given prime p and positive integer
m, a Galois ﬁeld GF(pm) exists and that all ﬁelds of order pm are isomorphic.
See Stewart [35] or Nicholson [11] for a proof of these facts. For m = 1, the
integers modulo p, Zp, is a Galois ﬁeld of order p.
From Theorem 11.22 it follows that GF(pm) is a ﬁeld extension of Zp of
degree m. Each ﬁnite ﬁeld GF(pm) can be constructed by ﬁnding a polynomial
q(x) of degree m, irreducible in Zp[x], and deﬁning
GF(pm) = Zp[x]/(q(x)).
By Lemma 11.13 and Corollary 11.12, there is an element α in GF(pm), such
that q(α) = 0, and GF(pm) = Zp(α), the ﬁeld obtained by adjoining α to Zp.
For example, GF(4) = Z2[x]/(x2 + x + 1) = Z2(α) = {0, 1, α, α + 1}, where
α2 + α + 1 = 0. Rewriting Table 10.2, we obtain Table 11.1 for GF(4).
Example 11.23. Construct a ﬁeld GF(125).
Solution. Since 125 = 53, we can construct such a ﬁeld if we can ﬁnd an
irreducible polynomial of degree 3 over Z5.
A reducible polynomial of degree 3 must have a linear factor. Therefore, by
the factor theorem, p(x) = x3 + ax2 + bx + c is irreducible in Z5[x] if and only
if p(n) ̸= 0 for n = 0, 1, 2, 3, 4 in Z5.
TABLE 11.1. Galois Field GF(4)
+
0
1
α
α + 1
0
0
1
α
α + 1
1
1
0
α + 1
α
α
α
α + 1
0
1
α + 1
α + 1
α
1
0
·
0
1
α
α + 1
0
0
0
0
0
1
0
1
α
α + 1
α
0
α
α + 1
1
α + 1
0
α + 1
1
α

228
11
FIELD EXTENSIONS
By trial and error, we ﬁnd that the polynomial p(x) = x3 + x + 1 is irre-
ducible because p(0) = 1, p(1) = 3, p(2) = 11 = 1, p(3) = 31 = 1, and p(4) =
p(−1) = −1 = 4 in Z5. Hence
GF(125) = Z5[x]/(x3 + x + 1).
□
Note that (x3 + x + 1) is not the only irreducible polynomial of degree 3
over Z5. For example, (x3 + x2 + 1) is also irreducible. But Z5[x]/(x3 + x + 1)
is isomorphic to Z5[x]/(x3 + x2 + 1).
PRIMITIVE ELEMENTS
The elements of a Galois ﬁeld GF(pm) can be written as

a0 + a1α + · · · + am−1αm−1|ai ∈Zp

where α is a root of a polynomial q(x) of degree m irreducible over Zp. Addition
is easily performed using this representation, because it is simply addition of
polynomials in Zp[α]. However, multiplication is more complicated and requires
repeated use of the relation q(α) = 0. We show that by judicious choice of α,
the elements of GF(pm) can be written as

0, 1, α, α2, α3, . . . , αpm−2
where
αpm−1 = 1.
This element α is called a primitive element of GF(pm), and multiplication is
easily calculated using powers of α; however, addition is much harder to perform
using this representation.
For example, in GF(4) = Z2(α) = {0, 1, α, α2} where α + 1 = α2 and α3 = 1,
and the tables are given in Table 11.2.
If F is any ﬁeld and F ∗= F −{0}, we know that (F ∗, ·) is an abelian group
under multiplication. We now show that the nonzero elements of a ﬁnite ﬁeld
form a cyclic group under multiplication; the generators of this cyclic group
are the primitive elements of the ﬁeld. To prove this theorem, we need some
preliminary results about the orders of elements in an abelian group.
TABLE 11.2. Galois Field GF(4) in Terms of a
Primitive Element
+
0
1
α
α2
0
0
1
α
α2
1
1
0
α2
α
α
α
α2
0
1
α2
α2
α
1
0
·
0
1
α
α2
0
0
0
0
0
1
0
1
α
α2
α
0
α
α2
1
α2
0
α2
1
α

PRIMITIVE ELEMENTS
229
Lemma 11.24. If g and h are elements of an abelian group of orders a and b,
respectively, there exists an element of order lcm(a, b).
Proof. Let a = pa1
1 pa2
2 · · · pas
s and b = pb1
1 pb2
2 · · · pbs
s , where the pi are distinct
primes and ai ⩾0, bi ⩾0 for each i. For each i deﬁne
xi =

ai
if ai ⩾bi
0
if ai < bi
and
yi =

0
if ai ⩾bi
bi
if ai < bi .
If x = px1
1 px2
2 · · · pxs
s and y = py1
1 py2
2 · · · pys
s , then x|a and y|b, so ga/x has order
x and hb/y has order y. Moreover, gcd(x, y) = 1, so ga/xhb/y has order xy
by Lemma 4.36. But xy = lcm(a, b) by Theorem 15 of Appendix 2 because
xi + yi = max(ai, bi) for each i.
□
Lemma 11.25. If the maximum order of the elements of an abelian group G is
r, then xr = e for all x ∈G.
□
Proof. Let g ∈G be an element of maximal order r. If h is an element of order
t, there is an element of order lcm(r, t) by Lemma 11.24. Since lcm(r, t) ⩽r,
t divides r. Therefore, hr = e.
□
Theorem 11.26. Let GF(q)∗be the set of nonzero elements in the Galois ﬁeld
GF(q). Then (GF(q)∗, ·) is a cyclic group of order q −1.
Proof. Let r be the maximal order of elements of (GF(q)∗, ·). Then, by
Lemma 11.25,
xr −1 = 0
for all x ∈GF(q)∗.
Hence every nonzero element of the Galois ﬁeld GF(q) is a root of the polynomial
xr −1 and, by Theorem 9.6, a polynomial of degree r can have at most r roots
over any ﬁeld; therefore, r ⩾q −1. But, by Lagrange’s theorem, r|(q −1); it
follows that r = q −1.
(GF(q)∗, ·) is therefore a group of order q −1 containing an element of order
q −1 and hence must be cyclic.
□
A generator of the cyclic group (GF(q)∗, ·) is called a primitive element
of GF(q). For example, in GF(4) = Z2(α), the multiplicative group of nonzero
elements, GF(4)∗, is a cyclic group of order 3, and both nonidentity elements α
and α + 1 are primitive elements.
If α is a primitive element in the Galois ﬁeld GF(q), where q is the power of
a prime p, then GF(q) is the ﬁeld extension Zp(α) and
GF(q)∗= {1, α, α2, . . . , αq−2}.
Hence
GF(q) = {0, 1, α, α2, . . . , αq−2}.

230
11
FIELD EXTENSIONS
Example 11.27. Find all the primitive elements in GF(9) = Z3(α), where
α2 + 1 = 0.
Solution. Since x2 + 1 is irreducible over Z3, we have
GF(9) = Z3[x]/(x2 + 1) = {a + bx|a, b ∈Z3}.
The nonzero elements form a cyclic group GF(9)∗of order 8; hence the multi-
plicative order of each element is either 1, 2, 4, or 8.
In calculating the powers of each element, we use the relationship α2 = −1 =
2. From Table 11.3, we see that 1 + α, 2 + α, 1 + 2α, and 2 + 2α are the primi-
tive elements of GF(9).
□
Proposition 11.28.
(i) zq−1 = 1 for all elements z ∈GF(q)∗.
(ii) zq = z for all elements z ∈GF(q).
(iii) If GF(q) = {α1, α2, . . . , αq}, then zq −z factors over GF(q) as
(z −α1)(z −α2) · · · (z −αq).
Proof. We have already shown that (i) is implied by Lemma 11.25. Part (ii)
follows immediately because 0 is the only element of GF(q) that is not in GF(q)∗.
The polynomial zq −z, of degree q, can have at most q roots over any ﬁeld.
By (ii), all elements of GF(q) are roots over GF(q); hence zq −z factors into
q distinct linear factors over GF(q).
□
For example, in GF(4) = Z2[x]/(x2 + x + 1) = {0, 1, α, α + 1}, and we have
(z + 0)(z + 1)(z + α)(z + α + 1) = (z2 + z)(z2 + z + α2 + α)
= (z2 + z)(z2 + z + 1)
= z4 + z = z4 −z.
TABLE 11.3. Nonzero Elements of GF(9)
Element x
x2
x4
x8
Order
Primitive
1
1
1
1
1
No
2
1
1
1
2
No
α
2
1
1
4
No
1 + α
2α
2
1
8
Yes
2 + α
α
2
1
8
Yes
2α
2
1
1
4
No
1 + 2α
α
2
1
8
Yes
2 + 2α
2α
2
1
8
Yes

PRIMITIVE ELEMENTS
231
An irreducible polynomial g(x), of degree m over Zp, is called a primitive
polynomial if g(x)|(xk −1) for k = pm −1 and for no smaller k.
Proposition 11.29. The irreducible polynomial g(x) ∈Zp[x] is primitive if and
only if x is a primitive element in Zp[x]/(g(x)) = GF(pm).
Proof. The following statements are equivalent:
(i) x is a primitive element in GF(pm) = Zp[x]/(g(x)).
(ii) xk = 1 in GF(pm) for k = pm −1 and for no smaller k.
(iii) xk −1 ≡0 mod g(x) for k = pm −1 and for no smaller k.
(iv) g(x)|(xk −1) for k = pm −1 and for no smaller k.
□
For example, x2 + x + 1 is primitive in Z2[x]. From Example 11.27, we see
that x2 + 1 is not primitive in Z3[x]. However, 1 + α and 1 + 2α = 1 −α are
primitive elements, and they are roots of the polynomial
(x −1 −α)(x −1 + α) = (x −1)2 −α2 = x2 + x + 2 ∈Z3[x].
Hence x2 + x + 2 is a primitive polynomial in Z3[x]. Also, x2 + 2x + 2 is
another primitive polynomial in Z3[x] with roots 2 + α and 2 + 2α = 2 −α.
Example 11.30. Let α be a root of the primitive polynomial x4 + x + 1 ∈Z2[x].
Show how the nonzero elements of GF(16) = Z2(α) can be represented by the
powers of α.
Solution. The representation is given in Table 11.4.
□
Arithmetic in GF(16) can very easily be performed using Table 11.4. Addition
is performed by representing elements as polynomials in α of degree less than 4,
whereas multiplication is performed using the representation of nonzero elements
as powers of α. For example,
1 + α + α3
1 + α2 + α3 + α + α2 = α7
α13 + α + α2
= α−6 + α + α2
= α9 + α + α2
since α15 = 1
= α + α3 + α + α2
= α2 + α3.
The concept of primitive polynomials is useful in designing feedback shift
registers with a long cycle length. Consider the circuit in Figure 11.2, in which
the square boxes are delays of one unit of time, and the circle with a cross inside
represents a modulo 2 adder.

232
11
FIELD EXTENSIONS
TABLE 11.4. Representation of GF(16)
Element
α0
α1
α2
α3
0
= 0
0
0
0
0
α0 = 1
1
0
0
0
α1 =
α
0
1
0
0
α2 =
α2
0
0
1
0
α3 =
α3
0
0
0
1
α4 = 1 + α
1
1
0
0
α5 =
α + α2
0
1
1
0
α6 =
α2 + α3
0
0
1
1
α7 = 1 + α
+ α3
1
1
0
1
α8 = 1
+ α2
1
0
1
0
α9 =
α
+ α3
0
1
0
1
α10 = 1 + α + α2
1
1
1
0
α11 =
α + α2 + α3
0
1
1
1
α12 = 1 + α + α2 + α3
1
1
1
1
α13 = 1
+ α2 + α3
1
0
1
1
α14 = 1
+ α3
1
0
0
1
α15 = 1
a0
a1
a2
a3
a4 = 1 + a
Figure 11.2.
Feedback shift register.
If the delays are labeled by a representation of the elements of GF(16), a
single shift corresponds to multiplying the element of GF(16) by α. Hence, if
the contents of the delays are not all zero initially, this shift register will cycle
through 15 different states before repeating itself. In general, it is possible to
construct a shift register with n delay units that will cycle through 2n −1 different
states before repeating itself. The feedback connections have to be derived from a
primitive polynomial of degree n over Z2. Such feedback shift registers are useful
in designing error-correcting coders and decoders, random number generators,
and radar transmitters. See Chapter 14 of this book and, Lidl and Niederreiter [34,
Chap. 6], or Stone [22, Chap. 9].

EXERCISES
233
EXERCISES
For Exercises 11.1 to 11.4, write out the addition and multiplication tables for
the ﬁelds.
11.1. GF(5).
11.2. GF(7).
11.3. GF(9).
11.4. GF(8).
For Exercises 11.5 to 11.10, in each case ﬁnd, if possible, an irreducible polyno-
mial of degree n over F.
11.5. n = 3, F = Z11.
11.6. n = 3, F = Q.
11.7. n = 4, F = R.
11.8. n = 3, F = GF(4).
11.9. n = 2, F = Q(i).
11.10. n = 5, F = Z3.
For Exercises 11.11 to 11.13, in each case, ﬁnd a polynomial in F[x] with r as
a root.
11.11. r =
√
2 +
√
6, F = Q.
11.12. r = π + ei, F = R.
11.13. r =
3√
3/
√
2, F = Q.
11.14. Show that θ = 2kπ/7 satisﬁes the equation cos 4θ −cos 3θ = 0 for each
integer k. Hence ﬁnd an irreducible polynomial over Q with cos(2π/7)
as a root.
11.15. Prove that the algebraic numbers
A = {x ∈C|x is algebraic over Q}
form a subﬁeld of C.
11.16. Assuming the fundamental theorem of algebra, prove that every polyno-
mial in A has a root in A.
For Exercises 11.17 to 11.25, Calculate the degrees.
11.17. [Q(
3√
7) : Q].
11.18. [C : Q].
11.19. [Q(i, 3i) : Q].
11.20. [C : R(√−7)].
11.21. [Z3[x]/(x2 + x + 2) : Z3].
11.22. [Q(i,
√
2) : Q].
11.23. [A : Q].
11.24. [C : A].
11.25. [Z3(t) : Z3], where Z3(t) is the ﬁeld of rational functions in t over Z3.
11.26. Prove that x2 −2 is irreducible over Q(
√
3).
For Exercises 11.27 to 11.32, ﬁnd the inverses of the elements in the given ﬁelds.
Each ﬁeld is a ﬁnite extension F(α). Express your answers in the form a0 + a1α +
· · · + an−1αn−1, where ai ∈F and [F(α) : F] = n.
11.27. 1 +
3√
2 in Q(
3√
2).
11.28.
4√
5 +
√
5 in Q(
4√
5).
11.29. 5 + 6ω in Q(ω), where ω is a complex cube root of 1.

234
11
FIELD EXTENSIONS
11.30. 2 −3i in Q(i).
11.31. α in GF(32) = Z2(α), where α5 + α2 + 1 = 0.
11.32. α in GF(27) = Z3(α), where α3 + α2 + 2 = 0.
For Exercises 11.33 to 11.40, ﬁnd the characteristic of the rings. Which of these
are ﬁelds?
11.33. Z2 × Z2.
11.34. Z3 × Z4.
11.35. GF(49).
11.36. Z × Z2.
11.37. Q(
3√
7).
11.38. M2(Z5).
11.39. Q × Z3.
11.40. GF(4)[x].
11.41. Let R be any ring and n a positive integer. Prove that In = {na|a ∈R}
is an ideal of R and that the characteristic of R/In divides n.
11.42. Let M be a ﬁnite subgroup of the multiplicative group F ∗of any inﬁnite
ﬁeld F. Prove that M is cyclic, and give an example to show that F ∗
need not be cyclic.
11.43. For what values of m is (Z∗
m, ·) cyclic? (This is a difﬁcult problem; see
Exercises 4.55 to 4.62 for other results on Z∗
m.)
11.44. Let GF(4) = Z2(α), where α2 + α + 1 = 0. Find an irreducible quadratic
in GF(4)[x]. If β is the root of such a polynomial, show that GF(4)(β)
is a Galois ﬁeld of order 16.
11.45. (a) Show that there are (p2 −p)/2 monic irreducible polynomials of
degree 2 over GF(p). (A polynomial is monic if the coefﬁcient of
the highest power of the variable is 1.)
(b) Prove that there is a ﬁeld with p2 elements for every prime p.
11.46. (a) How many monic irreducible polynomials of degree 3 are there over
GF(p)?
(b) Prove that there is a ﬁeld with p3 elements for every prime p.
11.47. Find an element α such that Q(
√
2,
√
−3) = Q(α).
11.48. Find all primitive
elements in
GF(16) = Z2(α),
where α4 + α +
1 = 0.
11.49. Find all the primitive elements in GF(32).
For Exercises 11.50 and 11.51, ﬁnd a primitive polynomial of degree n over the
ﬁeld F.
11.50. n = 2, F = Z5.
11.51. n = 3, F = Z2.
11.52. Let g(x) be a polynomial of degree m over Zp. If g(x)|(xk −1) for
k = pm −1 and for no smaller k, show that g(x) is irreducible over Zp.
11.53. Prove that x8 + x ∈Z2[x] will split into linear factors over GF(8) but not
over any smaller ﬁeld.
11.54. Let f (x) = 2x3 + 5x2 + 7x + 6 ∈Q[x]. Find a ﬁeld, smaller than the
complex numbers, in which f (x) splits into linear factors.
11.55. If α and β are roots of x3 + x + 1 and x3 + x2 + 1 ∈Z2[x], respectively,
prove that the Galois ﬁelds Z2(α) and Z2(β) are isomorphic.

EXERCISES
235
11.56. (a) If p(x) ∈Z2[x], prove that [p(x)]2 = p(x2).
(b) If β is a root of p(x) ∈Z2[x], prove that β2l is a root for all l ∈N.
(c) Let GF(16) = Z2(α) where α4 + α + 1 = 0. Find an irreducible poly-
nomial in Z2[x] with α3 as a root.
For Exercises 11.57 and 11.58, solve the simultaneous linear equations in
GF(4) = Z2(α).
11.57. αx + (α + 1)y = α + 1
11.58. (α + 1)x + y = α
x + αy = 1.
x + (α + 1)y = α + 1.
11.59. Solve the quadratic equation αx2 + (1 + α)x + 1 = 0 over the ﬁeld
GF(4) = Z2(α).
11.60. Let R be any commutative ring of characteristic p, where p is a prime.
(a) Show that (a + b)p = ap + bp for all a, b in R. [Hint: If

p
r

denotes the binomial coefﬁcient, show that

p
r

= p
r

p −1
r −1

whenever 1 ⩽r ⩽p.]
(b) If σ : R →R is deﬁned by σ(a) = ap for all a ∈R, show that σ is
a ring morphism.
(c) If R = GF(pn) show that σ is an isomorphism of R (called the Frobe-
nius automorphism).
11.61. If F is a ﬁeld and F ∗is cyclic, show that F is ﬁnite.
11.62. Design a feedback shift register using six delays that has a cycle length
of 63.
11.63. What is the cycle length of the feedback shift register in Figure 11.3?
Figure 11.3
11.64. Design a feedback shift register that has a cycle length of 21.
11.65. Describe the output sequence of the feedback shift register in Figure 11.4
when the registers initially contain the elements shown.
1
0
1
Output
Figure 11.4
11.66. If a feedback shift register with n delays has a cycle length of 2n −1,
show that the feedback connections must be derived from a primitive
irreducible polynomial of degree n over Z2.

12
LATIN SQUARES
Latin squares ﬁrst arose with parlor games such as the problem of arranging the
jacks, queens, kings, and aces of a pack of cards in a 4 × 4 array so that each row
and each column contains one card from each suit and one card from each rank.
In 1779, Leonard Euler posed the following famous problem of the 36 ofﬁcers
from six ranks and six regiments. He claimed that it was impossible to arrange
these ofﬁcers on parade in a 6 × 6 square so that each row and each column
contains one ofﬁcer from each rank and one from each regiment.
Recently, statisticians have found latin squares useful in designing experi-
ments, and mathematicians have found close connections between latin squares
and ﬁnite geometries.
LATIN SQUARES
Let S be a set with n elements. Then a latin square L = (lij), of order n based
on S, is an n × n array of the elements of S such that each element appears
exactly once in each row and once in each column.
For example, Table 12.1 illustrates a latin square of order 3 based on {a, b, c}.
Theorem 12.1. The table for any ﬁnite group (G, +) of order n is a latin square
of order n based on G.
Proof. We write the operation in G as addition, even though the result still
holds if G is not commutative.
Suppose that two elements in one row are equal. Then xi + xj = xi + xk
for some xi, xj, xk ∈G. Since G is a group, xi has an inverse (−xi) such
that (−xi) + xi = 0. Hence (−xi) + (xi + xj) = (−xi) + (xi + xk), and since the
operation is associative, we have xj = xk. Therefore, an element cannot appear
twice in the same row. Similarly, an element cannot appear twice in the same
column, and the table is a latin square.
□
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
236

LATIN SQUARES
237
TABLE 12.1. Latin
Square
a
b
c
c
a
b
b
c
a
TABLE 12.2. Latin Squares of Order 4
(0, 0)
(0, 1)
(1, 0)
(1, 1)
(0, 1)
(0, 0)
(1, 1)
(1, 0)
(1, 0)
(1, 1)
(0, 0)
(0, 1)
(1, 1)
(1, 0)
(0, 1)
(0, 0)
c
b
a
d
d
a
b
c
a
d
c
b
b
c
d
a
Given any latin square, we can permute the rows among themselves and also
the columns among themselves and we still have a latin square. For example, the
addition table for Z2 × Z2 is a latin square of order 4. If we interchange the ﬁrst
and third columns and replace (0, 0) by a, (0, 1) by b, (1, 0) by c, and (1, 1)
by d, we obtain another latin square of order 4 based on {a, b, c, d}. These are
illustrated in Table 12.2.
Latin squares are useful in designing statistical experiments because they can
show how an experiment can be arranged so as to reduce the errors without
making the experiment too large or too complicated. See Laywine and Mullen
[40] for more complete details.
Suppose that you wanted to compare the yields of three varieties of hybrid
corn. You have a rectangular test plot, but you are not sure that the fertility of the
soil is the same everywhere. You could divide up the land into nine rectangular
regions and plant the three varieties, a, b, and c, in the form of the latin square
in Table 12.1. Then if one row were more fertile than the others, the latin square
would reduce the error that this might cause. In fact, if the soil fertility was a
linear function of the coordinates of the plot, the latin square arrangement would
minimize the error.
Of course, the error could be reduced by subdividing the plot into a large
number of pieces and planting the varieties at random. But this would make it
much more difﬁcult to sow and harvest.
Example 12.2. A smoking machine is used to test the tar content of four brands
of cigarettes; the machine has four ports so that four cigarettes can be smoked
simultaneously. However, the four ports might not be identical and that might
affect the measurements of the tar content. Also, if four runs were made on the
machine, testing one brand at a time, the humidity could change, thus affecting
the results.
Show how to reduce the errors due to the different ports and the different runs
by using a latin square to design the experiment.

238
12
LATIN SQUARES
TABLE 12.3. Design of the
Smoking Experiment
Ports
1
2
3
4
↓
↓
↓
↓
1 →
c
b
a
d
Runs
2 →
d
a
b
c
3 →
a
d
c
b
4 →
b
c
d
a
TABLE 12.4
A
B
C
D
E
B
A
E
C
D
C
D
A
E
B
D
E
B
A
C
E
C
D
B
A
TABLE 12.5
+
..p
..q..
.
.
r
A
B
.
s
B
A
.
.
Solution. If a, b, c, d are the four brands, we can use one of the latin squares
of order 4 that we have constructed. Table 12.3 illustrates which brand should
be tested at each port during each of the four runs.
□
Not all latin squares can be obtained from a group table, even if we allow
permutations of the rows and columns.
Example 12.3. Show that the latin square illustrated in Table 12.4 cannot be
obtained from a group table.
Solution. By Corollary 4.11, all groups of order 5 are cyclic and are isomor-
phic to (Z5, +). Suppose that the latin square in Table 12.4 could be obtained
from the addition table of Z5. Since permutations are reversible, it follows that the
rows and columns of this square could be permuted to obtain the table of Z5. The
four elements in the left-hand top corner would be taken into four elements form-
ing a rectangle in Z5, as shown in Table 12.5. Then we would have p + r = A,
q + r = B, p + s = B, and q + s = A for some p, q, r, s ∈Z5, where p ̸= q
and r ̸= s. Hence p + r = A = q + s and q + r = B = p + s. Adding, we have
p + q + 2r = p + q + 2s and 2r = 2s. Therefore, 6r = 6s, which implies that
r = s in Z5, which is a contradiction.
□
ORTHOGONAL LATIN SQUARES
Suppose that in our cornﬁeld, besides testing the yields of three varieties of corn,
we also wanted to test the effects of three fertilizers on the corn. We could do

ORTHOGONAL LATIN SQUARES
239
TABLE 12.6
a
b
c
c
a
b
b
c
a
A
B
C
B
C
A
C
A
B
TABLE 12.7
aA
bB
cC
cB
aC
bA
bC
cA
aB
this in the same experiment by arranging the fertilizers on the nine plots so that
each of the three fertilizers was used once on each variety of corn and so that
the different fertilizers themselves were arranged in a latin square of order 3.
Let a, b, c be three varieties of corn and A, B, C be three types of fertilizer.
Then the two latin squares in Table 12.6 could be superimposed to form the
design in Table 12.7. In this table, each variety of corn and each type of fertilizer
appears exactly once in each row and in each column. Furthermore, each type
of fertilizer is used exactly once with each variety of corn. This table could be
used to design the experiment. For example, in the top left section of our test
plot, we would plant variety a and use fertilizer A.
Two latin squares of order n are called orthogonal if when the squares
are superimposed, each element of the ﬁrst square occurs exactly once with
each element of the second square. Thus the two latin squares in Table 12.6
are orthogonal.
Although it is easy to construct latin squares of any order, the construction
of orthogonal latin squares can be a difﬁcult problem. At this point the reader
should try to construct two orthogonal latin squares of order 4.
Going back to our ﬁeld of corn and fertilizers, could we use the same trick
again to test the effect of three insecticides by choosing another latin square of
order 3 orthogonal to the ﬁrst two? It can be proved that it is impossible to
ﬁnd such a latin square (see Exercise 12.5). However, if we have four types of
corn, fertilizer, and insecticide, we show using Theorem 12.5 how they could
be distributed on a 4 × 4 plot using three latin squares of order 4 orthogonal to
each other.
If L1, . . . , Lr are latin squares of order n such that Li is orthogonal to Lj
for all i ̸= j, then {L1, . . . , Lr} is called a set of r mutually orthogonal latin
squares of order n.
We show how to construct n −1 mutually orthogonal latin squares of order
n from a ﬁnite ﬁeld with n elements. We know that a ﬁnite ﬁeld has a prime
power number of elements, and we are able to construct such squares for n =
2, 3, 4, 5, 7, 8, 9, 11, 13, 16, 17, . . . etc.
Let GF(n) = {x0, x1, x2, . . . , xn−1} be a ﬁnite ﬁeld of order n = pm, where
x0 = 0 and x1 = 1. Let L1 = (a1
ij) be the latin square of order n that is the
addition table of GF(n). Then
a1
ij = xi + xj
for
0 ⩽i ⩽n −1
and
0 ⩽j ⩽n −1.
Proposition 12.4. Deﬁne the squares Lk = (ak
ij) for 1 ⩽k ⩽n −1 by
ak
ij = xk · xi + xj
for
0 ⩽i ⩽n −1
and
0 ⩽j ⩽n −1.

240
12
LATIN SQUARES
Then Lk is a latin square of order n for 1 ⩽k ⩽n −1 based on GF(n).
Proof. The difference between two elements in the ith row is
ak
ij −ak
iq = (xk · xi + xj) −(xk · xi + xq)
= xj −xq ̸= 0
if j ̸= q.
Hence each row is a permutation of GF(n).
The difference between two elements in the jth column is
ak
ij −ak
rj = (xk · xi + xj) −(xk · xr + xj)
= xk · (xi −xr) ̸= 0
if i ̸= r
since xk ̸= 0
and
xi ̸= xr.
Hence each column is a permutation of GF(n) and Lk is a latin square of order n.
□
Theorem 12.5. With the notation in Proposition 12.4, {L1, L2, . . . , Ln−1} is a
mutually orthogonal set of latin squares of order n = pm.
Proof. We have to prove that Lk is orthogonal to Ll for all k ̸= l.
Suppose that when Lk is superimposed on Ll, the pair of elements in the
(i, j)th position is the same as the pair in the (r, q)th position. That is, (ak
ij, al
ij) =
(ak
rq, al
rq) or ak
ij = ak
rq and al
ij = al
rq. Hence xk · xi + xj = xk · xr + xq and
xl · xi + xj = xl · xr + xq. Subtracting, we have (xk −xl) · xi = (xk −xl) · xr or
(xk −xl) · (xi −xr) = 0. Now the ﬁeld GF(n) has no zero divisors; thus either
xk = xl or xi = xr. Hence either k = l or i = r. But k ̸= l and we know from
Proposition 12.4 that two elements in the same row of Lk or Ll cannot be equal;
therefore, i ̸= r.
This contradiction proves that when Lk and Ll are superimposed, all the pairs
of elements occurring are different. Each element of the ﬁrst square appears n
times and hence must occur with all the n different elements of the second square.
Therefore, Lk is orthogonal to Ll, if k ̸= l.
□
If we start with Z3 and perform the construction above, we obtain the two
mutually orthogonal latin squares of order 3 given in Table 12.8.
Example 12.6. Construct three mutually orthogonal latin squares of order 4.
TABLE 12.8. Two Orthogonal
Latin Squares
L1
0
1
2
1
2
0
2
0
1
L2
0
1
2
2
0
1
1
2
0

ORTHOGONAL LATIN SQUARES
241
TABLE 12.9. Three Mutually Orthogonal Latin Squares of Order 4
L1
0
1
α
α2
1
0
α2
α
α
α2
0
1
α2
α
1
0
L2
0
1
α
α2
α
α2
0
1
α2
α
1
0
1
0
α2
α
L3
0
1
α
α2
α2
α
1
0
1
0
α2
α
α
α2
0
1
TABLE 12.10. Superimposed Latin
Squares
aaa
bbb
ccc
ddd
bcd
adc
dab
cba
cdb
dca
abd
bac
dbc
cad
bda
acb
TABLE 12.11. Sixteen Court Cards
A♠
K♦
Q♥
J♣
Q♣
J♥
A♦
K♠
J♦
Q♠
K♣
A♥
K♥
A♣
J♠
Q♦
Solution. Apply the method given in Proposition 12.4 to the Galois ﬁeld
GF(4) = Z2(α) = {0, 1, α, α2}, where α2 = α + 1.
L1 is simply the addition table for GF(4). From the way the square Lk was
constructed in Proposition 12.4, we see that its rows are a permutation of the
rows of L1. Hence L2 can be obtained by multiplying the ﬁrst column of L1 by
α and then permuting the rows of L1 so that they start with the correct element.
L3 is also obtained by permuting the rows of L1 so that the ﬁrst column is α2
times the ﬁrst column of L1. These are illustrated in Table 12.9.
□
If we write a for 0, b for 1, c for α, and d for α2, and superimpose the three
latin squares, we obtain Table 12.10. Example 12.6 also allows us to solve the
parlor game of laying out the 16 cards that was mentioned at the beginning of the
chapter. One solution, using the squares L2 and L3 in Table 12.9, is illustrated
in Table 12.11.
Example 12.7. A drug company wishes to produce a new cold remedy by com-
bining a decongestant, an antihistamine, and a pain reliever. It plans to test various
combinations of three decongestants, three antihistamines, and three pain reliev-
ers on four groups of subjects each day from Monday to Thursday. Furthermore,
each type of ingredient should also be compared with a placebo. Design this test
so as to reduce the effects due to differences between the subject groups and the
different days.
Solution. We can use the three mutually orthogonal latin squares constructed
in Example 12.6 to design this experiment—see Table 12.9.
Make up the drugs given to each group using Table 12.12. The letter in the
ﬁrst position refers to the decongestant, the second to the antihistamine, and the
third to the pain reliever. The letter a refers to a placebo, and b, c, and d refer
to the three different types of ingredients.
□

242
12
LATIN SQUARES
TABLE 12.12. Testing Three Different
Drugs
Mon.
Tues.
Wed.
Thurs.
A
aaa
bbb
ccc
ddd
Subject
B
bcd
adc
dab
cba
Group
C
cdb
dca
abd
bac
D
dbc
cad
bda
acb
We recognize Euler’s problem of the 36 ofﬁcers on parade mentioned at the
beginning of the chapter as the problem of constructing two orthogonal latin
squares of order 6. Euler not only conjectured that this problem was impossible
to solve, but he also conjectured that it was impossible to ﬁnd two orthogonal
latin squares of order n, whenever n ≡2 mod 4.
Theorem 12.5 cannot be used to construct two such squares with order congruent
to 2 modulo 4 because the only prime power of this form is 2, and then the theorem
only gives one latin square. In 1899, G. Tarry, by exhaustive enumeration, proved
that the problem of the 36 ofﬁcers was insoluble. However, in 1959, Euler’s general
conjecture was shown to be false, and in fact, Bose, Shrikhande, and Parker proved
that there exist at least two orthogonal latin squares of order n, for any n > 6.
Hence Proposition 12.4 is by no means the only way of constructing orthogonal
latin squares. Laywine and Mullen [40] give a comprehensive survey of all the
known results on latin squares up to the time of the book’s publication in 1998.
FINITE GEOMETRIES
The construction in Theorem 12.5 of n −1 mutually orthogonal latin squares of
order n, when n is a prime power, was ﬁrst discovered by the American mathe-
matician E. H. Moore in 1896, and was rediscovered by the Indian mathematical
statistician R. C. Bose in 1938. (See Section 2.2 of Laywine and Mullin [40].)
Bose also showed that there is a very close connection between orthogonal latin
squares and geometries with a ﬁnite number of points and lines. These geometries
are called afﬁne planes.
An afﬁne plane consists of a set, P , of points, together with a set, L, of
subsets of P called lines. The points and lines must satisfy the following inci-
dence axioms.
(i) Any two distinct points lie on exactly one line.
(ii) For each line l and point x not on l, there exists a unique line m containing
x and not meeting l.
(iii) There exist three points not lying on a line.
We can deﬁne an equivalence relation of parallelism, / /, on the set of lines L,
by deﬁning l//m if l = m or l and m contain no common point. Axiom (ii) then
states that through each point there is a unique line parallel to any other line. The

FINITE GEOMETRIES
243
points and lines in the euclidean plane R2 form such a geometry with an inﬁnite
number of points.
If the geometry has only a ﬁnite number of points, it can be shown that there
exists an integer n such that the geometry contains n2 points and n2 + n lines,
and that each line contains n points, while each point lies on n + 1 lines. Such a
ﬁnite geometry is called an afﬁne plane of order n. In an afﬁne plane of order
n there are n + 1 parallelism classes (see Exercises 12.12 and 12.13).
Figure 12.1 shows an afﬁne plane of order 2 in which P = {a, b, c, d} and
L = {{a, b}, {c, d}, {a, c}, {b, c}, {b, d}, {a, d}}.
Bose showed that an afﬁne plane of order n produces a complete set of n −1
mutually orthogonal latin squares of order n, and conversely, that each set of
n −1 mutually orthogonal latin squares of order n deﬁnes an afﬁne plane of
order n.
Theorem 12.8. There exists an afﬁne plane of order n if and only if there exist
n −1 mutually orthogonal latin squares of order n.
Proof. Suppose that there exists an afﬁne plane of order n. We coordinatize
the points as follows. Take any line and label the n points as 0, 1, 2, . . . , n −1.
This is called the x-axis, and the point labeled 0 is called the origin. Choose
any other line through the origin and label the n points 0, 1, 2, . . . , n −1 with
0 at the origin. This line is called the y-axis. A point of the plane is said to
have coordinates (a, b) if the unique lines through the point parallel to the y and
x-axes meet the axes in points labeled a and b, respectively. This is illustrated
in Figure 12.2.
a
b
c
d
Figure 12.1.
Afﬁne plane with four points.
n − 1
b
(a, b)
a
n − 1
x
y
1
1
0
Figure 12.2.
Coordinates in an afﬁne plane.

244
12
LATIN SQUARES
There are n2 ordered pairs (a, b) corresponding to the n2 points of the plane.
These points also correspond to the n2 cells of an n × n square where (a, b)
refers to the cell in the ath row and bth column. We ﬁll these cells with numbers
in n −1 different ways to produce n −1 mutually orthogonal latin squares of
order n.
Consider any complete set of parallel lines that are not parallel to either axis.
Label the n parallel lines 0, 1, 2, . . . , n −1 in any manner. Through each point,
there is exactly one of these lines. In the cell (a, b) place the number of the
unique line on which the point (a, b) is found. The numbers in these cells form
a latin square of order n on {0, 1, . . . , n −1}. No two numbers in the same row
can be the same, because there is only one line through two points in the same
row, namely, the line parallel to the x-axis. Hence each number appears exactly
once in each row and, similarly, once in each column.
There are n −1 sets of parallelism classes that are not parallel to the axes; each
of these gives rise to a latin square. These n −1 squares are mutually orthogonal
because each line of one parallel system meets all n of the lines of any other
system. Hence, when two squares are superimposed, each number of one square
occurs once with each number of the second square.
Conversely, suppose that there exists a set of n −1 mutually orthogonal latin
squares of order n. We can relabel the elements, if necessary, so that these squares
are based on S = {0, 1, 2, . . . , n −1}. We deﬁne an afﬁne plane with S2 as the
set of points. A set of n points is said to lie on a line if there is a latin square
with the same number in each of the n cells corresponding to these points, or if
the n points all have one coordinate the same. It is straightforward to check that
this is an afﬁne plane of order n.
□
Corollary 12.9. There exists an afﬁne plane of order n whenever n is the power
of a prime.
Proof. This follows from Theorem 12.5.
□
The only known afﬁne planes have prime power order. Because of the impos-
sibility of solving Euler’s ofﬁcer problem, there are no orthogonal latin squares
of order 6, and hence there is no afﬁne plane of order 6. In 1988, by means of
a massive computer search, Lam, Thiel, and Swiercz showed that there was no
afﬁne plane of order 10. See Lam [39] for the story behind this two-decade search.
By Theorem 12.8, there cannot exist nine mutually orthogonal latin squares of
order 10. However, two mutually orthogonal latin squares of order 10 have been
found, but not three such squares. Computers have also been used to search for
many sets of mutually orthogonal latin squares of low order. See Chapter 2 of
Laywine and Mullen [40] for further results on mutually orthogonal latin squares.
By the method of Theorem 12.8, we can construct an afﬁne plane of order n
from the Galois ﬁeld GF(n) whenever n is a prime power. The set of points is
P = GF(n)2 = {(x, y)|x, y ∈GF(n)}.

MAGIC SQUARES
245
(0, a2)
(1, a2)
(a, a2)
(a2, a2)
(a2, a)
(a2, 1)
(a2, 0)
(0, a)
(0, 1)
(0, 0)
(1, 0)
(a, 0)
(a, 1)
(a, a)
(1, a)
(1, 1)
Figure 12.3.
Afﬁne plane of order 4 with the points of the line y = αx + α2 enlarged.
It follows from Proposition 12.4 that a line consists of points satisfying a linear
equation in x and y with coefﬁcients in GF(n). The slope of a line is deﬁned in
the usual way and is an element of GF(n) or is inﬁnite. Two lines are parallel if
and only if they have the same slope.
For example, if GF(4) = Z2(α) = {0, 1, α, α2}, the 16 points of the afﬁne
plane of order 4 are shown in Figure 12.3. The horizontal lines are of the form
y = constant
and have slope 0, whereas the vertical lines are of the form
x = constant
and have inﬁnite slope. The line
y = αx + α2
has slope α and contains the points (0, α2), (1, 1), (α, 0) and (α2, α). This line
is parallel to the lines y = αx, y = αx + 1, and y = αx + α.
Given an afﬁne plane of order n, it is possible to construct a projective plane
of order n by adding a “line at inﬁnity” containing n + 1 points corresponding
to each parallelism class, so that parallel lines intersect on the line at inﬁnity.
The projective plane of order n has n2 + n + 1 points and n2 + n + 1 lines.
Furthermore, any projective plane gives rise to an afﬁne plane by taking one line
to be the line at inﬁnity. Hence the existence of a projective plane of order n is
equivalent to the existence of an afﬁne plane of the same order.
MAGIC SQUARES
Magic squares have been known for thousands of years, and in times when
particular numbers were associated with mystical ideas, it was natural that a

246
12
LATIN SQUARES
Figure 12.4.
“Melancholia,” an engraving by Albrecht D¨urer. In the upper right there is a magic
square of order 4 with the date of the engraving, 1514, in the middle of the bottom row. [Courtesy
of Staatliche Museen Kupferstichkabinett, photo by Walter Steinkopf.]
square that displays such symmetry should have been deemed to have magical
properties. Figure 12.4 illustrates an engraving by D¨urer, made in 1514, that
contains a magic square. Magic squares have no applications, and this section is
included for amusement only.
Publisher's Note:
Permission to reproduce this image
online was not granted by the
copyright holder. Readers are kindly
requested to refer to the printed version
of this article.

MAGIC SQUARES
247
A magic square of order n consists of the integers 1 to n2 arranged in an
n × n square array so that the row sums, column sums, and corner diagonal sums
are all the same.
The sum of each row must be n(n2 + 1)/2, which is 1/n times the sum of all
the integers from 1 to n2. For example, in D¨urer’s magic square of Figure 12.4,
the sum of each row, column, and diagonal is 34.
It is an interesting exercise to try to construct such squares. We show how to
construct some magic squares from certain pairs of orthogonal latin squares. See
Ball et al. [38] and Laywine and Mullen [40] for other methods of constructing
magic squares.
Let K = (kij) and L = (lij) be two orthogonal latin squares of order n on the
set S = {0, 1, . . . , n −1}. Superimpose these two squares to form a square M =
(mij) in which the elements of M are numbers in the base n, whose ﬁrst digit is
taken from K and whose second digit is taken from L. That is, mij = n · kij + lij.
Since K and L are orthogonal, all possible combinations of two elements from
S occur exactly once in M. In other words, all the numbers from 0 to n2 −1
occur in M.
Now add 1 to every element of M to obtain the square M′ = m′
ij, where
m′
ij = mij + 1.
Lemma 12.10. The square M′ contains all the numbers between 1 and n2 and
is row and column magic; that is, the sums of each row and of each column are
the same.
Proof. In any row or column of M, each number from 0 to n −1 occurs
exactly once as the ﬁrst digit and exactly once as the second digit. Hence the
sum is
(0 + 1 + · · · + n −1)n + (0 + 1 + · · · + n −1)
= (n + 1)(n −1)n/2 = n(n2 −1)/2.
Therefore, each row or column sum of M′ is n(n2 −1)/2 + n = n(n2 + 1)/2.□
Example 12.11. Construct the square M′ from the two orthogonal latin squares,
K and L, in Table 12.13.
Solution. Table 12.13 illustrates the superimposed square M in base 3 and in
base 10. By adding one to each element, we obtain the magic square M′.
□
Theorem 12.12. If K and L are orthogonal latin squares of order n on the
set {0, 1, 2, . . . , n −1} and the sum of each of the diagonals of K and L is
n(n −1)/2, then the square M′ derived from K and L is a magic square of
order n.

248
12
LATIN SQUARES
TABLE 12.13. Construction of a Magic
Square of Order 3
1
2
0
0
1
2
2
0
1
0
2
1
2
1
0
1
0
2
10
22
01
02
11
20
21
00
12
K
L
M (in base 3)
3
8
1
2
4
6
7
0
5
4
9
2
3
5
7
8
1
6
M (in base 10)
M′
Proof. Lemma 12.10 shows that the sum of each row and each column is
n(n2 + 1)/2. A similar argument shows that the sum of each diagonal is also
n(n2 + 1)/2.
□
There are two common ways in which the sum of the diagonal elements of
K and L can equal n(n −1)/2.
(i) The diagonal is a permutation of {0, 1, . . . , n −1}.
(ii) If n is odd, every diagonal element is (n −1)/2.
Both these situations occur in the squares K and L of Table 12.13; thus the
square M′, which is constructed from these, is a magic square.
Example 12.13. Construct a magic square of order 4 from two orthogonal latin
squares in Table 12.9.
Solution. By replacing 0, 1, α, α2 by 0, 1, 2, 3, in any order, the squares
L2 and L3 in Table 12.9 satisfy the conditions of Theorem 12.12, because the
TABLE 12.14. Construction of a Magic
Square of Order 4
0
1
2
3
2
3
0
1
3
2
1
0
1
0
3
2
3
2
0
1
1
0
2
3
2
3
1
0
0
1
3
2
L′
2
L′
3
03
12
20
31
21
30
02
13
32
23
11
00
10
01
33
22
4
7
9
14
10
13
3
8
15
12
6
1
5
2
16
11
M (in base 4)
M′

EXERCISES
249
diagonal elements are all different. However, L1 will not satisfy the conditions
of Theorem 12.12, whatever substitutions we make. In L2, replace 0, 1, α, α2 by
0, 1, 2, 3, respectively, and in L3 replace 0, 1, α, α2 by 3, 2, 0, 1, respectively, to
obtain the squares L′
2 and L′
3 in Table 12.14. Combine these to obtain the square
M with entries in base 4. Add 1 to each entry and convert to base 10 to obtain
the magic square M′ in Table 12.14.
□
EXERCISES
12.1. Construct a latin square of order 7 on {a, b, c, d, e, f, g}.
12.2. Construct four mutually orthogonal latin squares of order 5.
12.3. Construct four mutually orthogonal latin squares of order 8.
12.4. Construct two mutually orthogonal latin squares of order 9.
12.5. Prove that there are at most (n −1) mutually orthogonal latin squares of
order n. (You can always relabel each square so that the ﬁrst rows are
the same.)
12.6. Let L = (lij) be a latin square of order l on {1, 2, . . . , l} and M = (mij)
be a latin square of order m on {1, 2, . . . , m}. Describe how to construct
a latin square of order lm on {1, 2, . . . , l} × {1, 2, . . . , m} from L and M.
12.7. Is the latin square of Table 12.15 the multiplication table for a group of
order 6 with identity A?
TABLE 12.15
A
B
C
D
E
F
B
A
F
E
C
D
C
F
B
A
D
E
D
C
E
B
F
A
E
D
A
F
B
C
F
E
D
C
A
B
12.8. A chemical company wants to test a chemical reaction using seven differ-
ent levels of catalyst, a, b, c, d, e, f , g. In the manufacturing process, the
raw material comes from the previous stage in batches, and the catalyst
must be added immediately. If there are seven reactors, A, B, C, D, E,
F, G, in which the catalytic reaction can take place, show how to design
the experiment using seven batches of raw material so as to minimize the
effect of the different batches and of the different reactors.
12.9. A supermarket wishes to test the effect of putting cereal on four shelves
at different heights. Show how to design such an experiment lasting four
weeks and using four brands of cereal.

250
12
LATIN SQUARES
12.10. A manufacturer has ﬁve types of toothpaste. He would like to test these
on ﬁve subjects by giving each subject a different type each week for ﬁve
weeks. Each type of toothpaste is identiﬁed by a different color—red,
blue, green, white, or purple—and the manufacturer changes the color
code each week to reduce the psychological effect of the color. Show
how to design this experiment.
12.11. Quality control would like to ﬁnd the best type of music to play to its
assembly line workers in order to reduce the number of faulty products.
As an experiment, a different type of music is played on four days in
a week, and on the ﬁfth day no music at all is played. Design such an
experiment to last ﬁve weeks that will reduce the effect of the different
days of the week.
12.12. The relation of parallelism, //, on the set of lines of an afﬁne plane is
deﬁned by l//m if and only if l = m or l ∩m = Ø. Prove that // is an
equivalence relation.
12.13. Let P be the set of points and L be the set of lines of a ﬁnite afﬁne plane.
(a) Show that the number of points on a line l equals the number of lines
in any parallelism class not containing l.
(b) Deduce that all the lines contain the same number of points.
(c) If each line contains n points, show that the plane contains n2 points
and n2 + n lines, each point lying on n + 1 lines. Show also that there
are n + 1 parallelism classes.
12.14. Find all the lines in the afﬁne plane of order 3 whose point set is Z2
3.
Exercises 12.15 to 12.17 refer to the afﬁne plane of order 9 obtained from GF(9) =
Z3(α), where α2 + 1 = 0.
12.15. Find the line through (2α, 1) that is parallel to the line y = αx + 2 + α.
12.16. Find the point of intersection of the lines y = x + α and y = (α + 1)x +
2α.
12.17. Find the equation of the line through (0, 2α) and (2, α + 1).
12.18. Prove that a magic square of order 3 must have 5 at its center.
12.19. Prove that 1 cannot be a corner element of a magic square of order 3.
12.20. How many different magic squares of order 3 are there?
12.21. How many essentially different magic squares of order 3 are there, that
is, magic squares that cannot be obtained from each other by a symmetry
of the square?
12.22. Is there a magic square of order 2?
12.23. Find two magic squares of order 4 different from the square in
Example 12.18.
12.24. Find a magic square of order 5.
12.25. Find a magic square of order 8.
12.26. Can you construct a magic square with the present year in the last two
squares of the bottom row?

13
GEOMETRICAL
CONSTRUCTIONS
The only geometric instruments used by the ancient Greeks were a straightedge
and a compass. They did not possess reliable graduated rulers or protractors.
However, with these two instruments, they could still perform a wide variety of
constructions; they could divide a line into any number of equal parts, and they
could bisect angles and construct parallel and perpendicular lines. There were
three famous problems that the Greeks could not solve using these methods:
(1) duplication of the cube; that is, given one edge of a cube, construct the edge
of a cube whose volume is double that of the given cube; (2) trisection of any
given angle; and (3) squaring of the circle; that is, given any circle, construct a
square whose area is the same as that of the circle. For centuries, the solution
to these problems eluded mathematicians, despite the fact that large prizes were
offered for their discovery.
It was not until the nineteenth century that mathematicians suspected and, in
fact, proved that these constructions were impossible. In the beginning of that
century, nonexistence proofs began appearing in algebra; it was proved that the
general polynomial equation of degree 5 could not be solved in terms of nth roots
and rational operations. Similar algebraic methods were then applied to these
geometric problems. The geometric problems could be converted into algebraic
problems by determining which multiples of a given length could be constructed
using only straightedge and compass. Some of the classical constructions are
illustrated in Figure 13.1.
CONSTRUCTIBLE NUMBERS
We are interested in those lengths that can be constructed from a given length.
For convenience, we choose our unit of length to be the given length. We see
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
251

252
13
GEOMETRICAL CONSTRUCTIONS
Dividing a length r into
n equal segments
r
n
r
r
r
√2r
Constructing √2 times
a given length r
P
Constructing a line through P
parallel to a line l
P
l
l
Erecting a perpendicular
from P to a line l
Figure 13.1.
Geometrical constructions using straightedge and compass.
that we can divide a length into any number of equal parts, and hence we can
construct any rational multiple. However, we can do more than this; we can
construct irrational multiples such as
√
2 by using right-angled triangles.
Given any line segment in the plane, choose rectangular coordinates so that
the line’s end points are (0, 0) and (1, 0). Any point in the plane that can be
constructed from this line segment by using a straightedge and compass is called
a constructible point. A real number is called constructible if it occurs as one
coordinate of a constructible point. Points can be constructed by performing the
following allowable operations a ﬁnite number of times. We can:
1. Draw a line through two previously constructed points.
2. Draw a circle with center at a previously constructed point and with radius
equal to the distance between two previously constructed points.
3. Mark the point of intersection of two straight lines.
4. Mark the points of intersection of a straight line and a circle.
5. Mark the points of intersection of two circles.
Theorem 13.1. The set of constructible numbers, K, is a subﬁeld of R.
Proof. K is a subset of R, so we have to show that it is a ﬁeld. That is, if
a, b ∈K, we have to show that a + b, a −b, ab, and if b ̸= 0, a/b ∈K.

CONSTRUCTIBLE NUMBERS
253
X
A
O
C
B
b
x
a
c
Figure 13.2.
Constructing products and quotients.
If a, b ∈K, we can mark off lengths a and b on a line to construct lengths
a + b and a −b.
If a, b, c ∈K, mark off a segment OA of length a on one line and mark off
segments OB and OC of length b and c on another line through O as shown
in Figure 13.2. Draw a line through B parallel to CA and let it meet OA in X.
Triangles OAC and OXB are similar, and if OX = x, then x/a = b/c and so
x = ab/c.
By taking c = 1, we can construct ab, and by taking b = 1, we can construct
a/c. Hence K is a subﬁeld of R.
□
Corollary 13.2. K is an extension ﬁeld of Q.
Proof. Since 1 ∈K and sums and differences of constructible numbers are
constructible, it follows that Z ⊆K. Since quotients of constructible numbers
are constructible, Q ⊆K.
□
Proposition 13.3. If k ∈K and k > 0, then
√
k ∈K.
Proof. Mark off segments AB and BC of lengths k and 1 on a line. Draw the
circle with diameter AC and construct the perpendicular to AC at B as shown
in Figure 13.3. Let it meet the circle at D and E. Then, by a standard theorem
in geometry, AB · BC = DB · BE: thus BD = BE =
√
k.
□
Example 13.4.
4√
2 is constructible.
Solution. We apply the construction of Proposition 13.3 twice to construct
√
2
and then
√
2 =
4√
2.
□
We can construct any number that can be written in terms of rational numbers,
+, −, ·, ÷, and √
signs. For example, the numbers 1 + 4
√
5,
√
2 + √4/5, and

3 −
√
7 are all constructible. If k1 is a positive rational number, all the elements

254
13
GEOMETRICAL CONSTRUCTIONS
E
D
C
A
B
k
1
√k
√k
Figure 13.3.
Constructing square roots.
of the extension ﬁeld K1 = Q(√k1) are constructible. K1 has degree 1 or 2 over
Q depending on whether √k1 is rational or irrational. If k2 is a positive element
of K1, all the elements of K2 = K1(√k2) = Q(√k1, √k2) are constructible, and
[K2 : K1] = 1 or 2, depending on whether or not √k2 is an element of K1. We
now show that every constructible number lies in a ﬁeld obtained by repeating
the extensions above.
Theorem 13.5. The number α is constructible if and only if there exists a
sequence of real ﬁelds K0, K1, . . . , Kn such that α ∈Kn ⊇Kn−1 ⊇· · · ⊇K0 =
Q and [Ki : Ki−1] = 2 for 1 ⩽i ⩽n.
Proof. Suppose that α ∈Kn ⊇Kn−1 ⊇· · · ⊇K0 = Q, where [Ki : Ki−1] =
2. By Proposition 11.16, Ki = Ki−1(√γi−1) for γi−1 ∈Ki−1, and since Ki is
real, γi−1 > 0. Therefore, by repeated application of Proposition 13.3, it can be
shown that every element of Kn is constructible.
Conversely, suppose that α ∈K; thus α appears as the coordinate of a point
constructible from (0, 0) and (1, 0) by a ﬁnite number of the operations 1 to 5
preceding Theorem 13.1. We prove the result by induction on m, the number of
constructible numbers used in reaching α.
Suppose that Xk = {x1, . . . , xk} is a set of numbers that have already been
constructed, that is, have appeared as coordinates of constructible points. When
the next number xk+1 is constructed, we show that [Q(Xk+1) : Q(Xk)] = 1 or 2,
where Q(Xk+1) = Q(x1, . . . , xk, xk+1).
We ﬁrst show that if we perform either operation 1 or 2 using previously
constructed numbers in Xk, the coefﬁcients in the equation of the line or circle
remain in Q(Xk). If we perform operation 3, the newly constructed numbers
remain in Q(Xk), and if we perform operation 4 or 5, the newly constructed
numbers are either in Q(Xk) or an extension ﬁeld of degree 2 over Q(Xk).
Operation 1. The line through (α1, β1) and (α2, β2) is (y −β1)/(β2 −β1) =
(x −α1)/(α2 −α1), and if α1, α2, β1, β2 ∈Xk, the coefﬁcients in the equation
of this line lie in Q(Xk).

CONSTRUCTIBLE NUMBERS
255
Operation 2. The circle with center (α1, β1) and radius equal to the distance from
(α2, β2) to (α3, β3) is (x −α1)2 + (y −β1)2 = (α2 −α3)2 + (β2 −β3)2, and all
the coefﬁcients in this equation lie in Q(Xk).
Operation 3. Let αij, βj ∈Q(Xk). Then the lines
α11x + α12y = β1
α21x + α22y = β2
meet in the point (x, y), where using Cramer’s rule,
x = det

β1
α12
β2
α22
 
det

α11
α12
α21
α22

and
y = det

α11
β1
α21
β2
 
det

α11
α12
α21
α22

as long as they are not parallel. Both of these coordinates are in Q(Xk).
Operation 4. To obtain the points of intersection of a circle and line with coef-
ﬁcients in Q(Xk), we eliminate y from the equations to obtain an equation of
the form
αx2 + βx + γ = 0
where
α, β, γ ∈Q(Xk).
The line and circle intersect if β2 −4αγ ⩾0 and the x coordinates of
the intersection points are x = −β ±

β2 −4αγ
2α
, which are in Q(Xk) or
Q(Xk)(

β2 −4αγ ). Similarly, the y coordinates are in Q(Xk) or in an extension
ﬁeld of degree 2 over Q(Xk).
Operation 5. The intersection of the two circles
x2 + y2 + α1x + β1y + γ1 = 0
x2 + y2 + α2x + β2y + γ2 = 0
is the same as the intersection of one of them with the line
(α1 −α2)x + (β1 −β2)y + (γ1 −γ2) = 0.
This is now the same situation as in operation (4).
Initially, m = 2, X2 = {0, 1}, and Q(X2) = Q. It follows by induction on m,
the number of constructible points used, that
α ∈Q(Xm) ⊇Q(Xm−1) ⊇· · · ⊇Q(X3) ⊇Q(X2) = Q,

256
13
GEOMETRICAL CONSTRUCTIONS
where [Q(Xk+1) : Q(Xk)] = 1 or 2 for 2 ⩽k ⩽m −1. Furthermore, each exten-
sion ﬁeld Q(Xk) is a subﬁeld of R because Q and Xk are sets of real numbers. By
dropping each ﬁeld Q(Xi) that is a trivial extension of Q(Xi−1), it follows that
α ∈Kn ⊇Kn−1 ⊇· · · ⊇K0 = Q
where [Ki : Ki−1] = 2 for 1 ⩽i ⩽n.
□
Corollary 13.6. If α is constructible, then [Q(α) : Q] = 2r for some r ⩾0.
Proof. If α is constructible, then α ∈Kn ⊇Kn−1 ⊇· · · ⊇K0 = Q, where Ki
is an extension ﬁeld of degree 2 over Ki−1. By Theorem 11.6,
[Kn : Q(α)][Q(α) : Q] = [Kn : Q]
= [Kn : Kn−1][Kn−1 : Kn−2] · · · [K1 : Q] = 2n.
Hence [Q(α) : Q]|2n; thus [Q(α) : Q] = 2r for some r ⩾0.
□
Corollary 13.7. If [Q(α) : Q] ̸= 2r for some r ⩾0, then α is not constructible.
Corollary 13.6 does not give a sufﬁcient condition for α to be constructible,
as shown in Example 13.17 below.
Example 13.8. Can a root of the polynomial x5 + 4x + 2 be constructed using
straightedge and compass?
Solution. Let α be a root of x5 + 4x + 2. By Eisenstein’s criterion, x5 + 4x +
2 is irreducible over Q; thus, by Corollary 11.12, [Q(α) : Q] = 5. Since 5 is not
a power of 2, it follows from Corollary 13.7 that α is not constructible.
□
Example 13.9. Can a root of the polynomial x4 −3x2 + 1 be constructed using
straightedge and compass?
Solution. Solving the equation x4 −3x2 + 1 = 0, we obtain x2 = (3 ±
√
5)/2
and x = ±

(3 ± √5)/2. It follows from Theorem 13.5 that all these roots can
be constructed.
□
DUPLICATING A CUBE
Let l be the length of the sides of a given cube so that its volume is l3. A cube
with double the volume will have sides of length
3√
2 l.
Proposition 13.10.
3√
2 is not constructible.

TRISECTING AN ANGLE
257
Proof.
3√
2 is a root of x3 −2 which, by the rational roots theorem (Theo-
rem 9.25), is irreducible over Q. Hence, by Corollary 11.12, [Q(
3√
2) : Q] = 3
so, by Corollary 13.7,
3√
2 is not constructible.
□
Since we cannot construct a length of
3√
2 l starting with a length l, the ancient
problem of duplicating the cube is insoluble.
TRISECTING AN ANGLE
Certain angles can be trisected using straightedge and compass. For example,
π, π/2, 3π/4 can be trisected because π/3, π/6, and π/4 can be constructed.
However, we show that not all angles are trisectable by proving that π/3 cannot
be trisected.
If we are given the angle φ, we can drop a perpendicular from a point a
unit distance from the angle to construct the lengths cos φ and sin φ, as shown
in Figure 13.4. Conversely, if either cos φ or sin φ is constructible, it is pos-
sible to construct the angle φ. Hence, if we are given an angle φ, we can
construct all numbers in the extension ﬁeld Q(cos φ). Of course, if cos φ ∈Q,
then Q(cos φ) = Q.
We can now consider those numbers that are constructible from Q(cos φ). This
notion of constructibility is similar to our previous notion, and similar results
hold, except that the starting ﬁeld is Q(cos φ) instead of Q.
Theorem 13.11. The angle φ can be trisected if and only if the polynomial
4x3 −3x −cos φ is reducible over Q(cos φ).
Proof. Let θ = φ/3. The angle θ can be constructed from φ if and only if
cos θ can be constructed from cos φ. It follows from De Moivre’s theorem and
the binomial theorem that
cos φ = cos 3θ = 4 cos3 θ −3 cos θ.
Hence cos θ is a root of f (x) = 4x3 −3x −cos φ.
If f (x) is reducible over Q(cos φ), then cos θ is a root of a polynomial of
degree 1 or 2 over Q(cos φ); thus [Q(cos φ, cos θ) : Q(cos φ)] = 1 or 2. Hence,
by Propositions 11.16 and 13.3, cos θ is constructible from Q(cos φ).
cos f
sin f
f
1
Figure 13.4.
Constructing sin φ and cos φ from the angle φ.

258
13
GEOMETRICAL CONSTRUCTIONS
If f (x) is irreducible over Q(cos φ), then [Q(cos φ, cos θ) : Q(cos φ)] = 3,
and it follows, by a proof similar to that of Theorem 13.5, that cos θ cannot be
constructed from Q(cos φ) by using straightedge and compass.
□
Corollary 13.12. If cos φ ∈Q, then the angle φ can be trisected if and only if
4x3 −3x −cos φ is reducible over Q.
For example, if φ = π/2, then φ can be trisected because the polynomial
4x3 −3x + 0 is reducible over Q.
Proposition 13.13. π/3 cannot be trisected by straightedge and compass.
Proof. The polynomial f (x) = 4x3 −3x −cos(π/3) = 4x3 −3x −1
2. Now,
by the rational roots theorem (Theorem 9.25), the only possible roots of 2f (x) =
8x3 −6x −1 are ±1, ± 1
2, ± 1
4, or ± 1
8. We see from the graph of f (x) in Fig-
ure 13.5 that none of these are roots, except possibly −1
4 or −1
8. However,
f (−1
4) =
3
16 and f (−1
8) = −17
128; thus f (x) has no rational roots. Hence f (x) is
irreducible over Q, and by Corollary 13.12, π/3 cannot be trisected by straight-
edge and compass.
□
Example 13.14. Archimedes showed that, if we are allowed to mark our straight-
edge, it is possible to trisect any angle.
Construction. Let AOB be the angle φ we are to trisect. Draw a circle with
center O and any radius r and let this circle meet OA and OB in P and Q. Mark
two points X and Y on our straightedge of distance r apart. Now move this
straightedge through Q, keeping X on OA until Y lies on the circle, as shown
in Figure 13.6. Then we claim that the angle OXY is φ/3, and hence the angle
AOB is trisected.
Solution. Let angle OXY = θ. Since triangle XYO is isosceles, the angle XOY =
θ. Now
angle OYQ = angle OXY + angle XOY = 2θ.
f(x)
0
1
−1
−1
x
Figure 13.5.
Graph of f (x) = 4x3 −3x −1
2.

CONSTRUCTING REGULAR POLYGONS
259
f
q
q
2q
Y
r
X
2q Q
B
O
r
P
A
Figure 13.6.
Trisection of the angle φ using a marked ruler.
Triangle YOQ is isosceles, so angle OQY = 2θ. Also,
φ = angle AOB = angle OXQ + angle OQX = θ + 2θ = 3θ.
Hence θ = φ/3.
□
SQUARING THE CIRCLE
Given any circle of radius r, its area is πr2, so that a square with the same
area has sides of length √π r. We can square the circle if and only if √π is
constructible.
Proposition 13.15. [Q(√π) : Q] is inﬁnite, and hence √π is not constructible.
Proof. The proof of this depends on the fact that π is transcendental over
Q; that is, π does not satisfy any polynomial equation with rational coefﬁcients.
This was mentioned in Chapter 11, and a proof is given in Stewart [35].
Q(π) is a subﬁeld of Q(√π) because π = (√π)2 ∈Q(√π). Since π is tran-
scendental, π, π2, π3, . . . are linearly independent over Q, and [Q(π) : Q] is
inﬁnite. Therefore,
[Q(√π) : Q] = [Q(√π) : Q(π)][Q(π) : Q]
is also inﬁnite. Hence, by Corollary 13.7, √π is not constructible.
□
Hence the circle cannot be squared by straightedge and compass.
CONSTRUCTING REGULAR POLYGONS
Another problem that has been of great interest to mathematicians from the time
of the ancient Greeks is that of constructing a regular n-gon, that is, a regular
polygon with n sides. This is equivalent to constructing the angle 2π/n or the

260
13
GEOMETRICAL CONSTRUCTIONS
number cos(2π/n). The Greeks knew how to construct regular polygons with
three, four, ﬁve, and six sides, but were unable to construct a regular 7-gon.
It is well known how to construct an equilateral triangle and a square using
straightedge and compass. We proved in Example 11.15 that cos(2π/5) =
(
√
5 −1)/4; thus a regular pentagon can be constructed. Furthermore, if a regu-
lar n-gon is constructible, so is a regular 2n-gon, because angles can be bisected
using straightedge and compass. Proposition 13.13 shows that π/9 cannot be
constructed; hence 2π/9 and a regular 9-gon cannot be constructed.
In 1796, at the age of 19, Gauss discovered that a regular 17-gon could be
constructed and later showed the only regular n-gons that are constructible are the
ones for which n = 2kp1 · · · pr, where k ⩾0 and p1, . . . , pr are distinct primes
of the form 22m + 1. Prime numbers of the form 22m + 1 are called Fermat
primes. Pierre de Fermat (1601–1665) conjectured that all numbers of the form
22m + 1 are prime. When m = 0, 1, 2, 3, and 4, the numbers are 3, 5, 17, 257, and
65,537, respectively, and they are all prime. However, in 1732, Euler discovered
that 225 + 1 is divisible by 641. Computers have checked many of these numbers
for m > 5, and they have all been composite. In fact, no more Fermat primes are
known today.
A complete proof of Gauss’s result is beyond the scope of this book, since it
requires more group and ﬁeld theory than we have covered (see Stewart [35]).
However, we can prove the following.
Theorem 13.16. If p is a prime for which a regular p-gon is constructible, then
p is a Fermat prime.
Proof. Let ξp = cos(2π/p) + i sin(2π/p), a pth root of unity. If a regular
p-gon can be constructed, cos(2π/p) and sin(2π/p) are constructible numbers
and [Q(cos(2π/p), sin(2π/p)) : Q] = 2r for some integer r. Hence
[Q(cos(2π/p), sin(2π/p), i) : Q] = 2r+1.
Now Q(ξp) ⊆Q(cos(2π/p), sin(2π/p), i) and so, by Theorem 11.6,
[Q(ξp) : Q] = 2k for some integer k ⩽r + 1.
The pth root of unity, ξp, is a root of the cyclotomic polynomial φ(x) =
xp−1 + xp−2 + · · · + x + 1 which, by Example 9.31, is irreducible over Q. Hence
[Q(ξp) : Q] = p −1, and therefore p −1 = 2k.
The number p = 2k + 1 is a prime only if k = 0 or k is a power of 2. Suppose
that k contains an odd factor b > 1 and that k = a · b. Then 2a + 1 divides
(2a)b + 1, since x + 1 divides xb + 1 if b is odd. Hence 2ab + 1 cannot be prime.
The case p = 2 gives rise to the degenerate 2-gon. Otherwise, p is a Fermat
prime, 22m + 1, for some integer m ⩾0.
□
NONCONSTRUCTIBLE NUMBER OF DEGREE 4
This next example shows that Corollary 13.6 does not give a sufﬁcient condition
for a number to be constructible.

NONCONSTRUCTIBLE NUMBER OF DEGREE 4
261
Example 13.17. There is a real root ri, of the irreducible polynomial x4 −4x + 2,
that is not constructible, even though [Q(ri) : Q] = 22.
Solution. By Eisenstein’s criterion, x4 −4x + 2 is irreducible over Q, so that
[Q(ri) : Q] = 4 for each root ri. However, we can factor this polynomial into
two quadratics over R, say
x4 −4x + 2 = (x2 + ax + b)(x2 + cx + d).
Comparing coefﬁcients, and then using equation (i), we have
(i)
0 = a + c
and
c = −a
(ii)
0 = b + d + ac
and
b + d = a2
(iii) −4 = bc + ad
and
−4 = a(d −b)
(iv)
2 = bd.
Let t = b + d, so that 16 = a2{(b + d)2 −4bd} = t(t2 −8). This number t sat-
isﬁes the equation
(v) t3 −8t −16 = 0.
By Theorem 9.25, this equation (v) has no rational roots; thus t3 −8t −16 is irre-
ducible over Q. We see from Figure 13.7 that the equation does have a real root ρ
between 3 and 4, and the coefﬁcients a, b, c, d can be expressed in terms of ρ.
Either a or c is positive. Without loss of generality suppose that c > 0; thus
b + d = t = ρ, a = −c = −√ρ, and d −b = 4/√ρ. Therefore, we have b =
ρ/2 −2/√ρ and d = ρ/2 + 2/√ρ, and the roots of x2 + ax + b are
−a ±
√
a2 −4b
2
= 1
2

√ρ ±

−ρ +
8
√ρ

,
which are real, since ρ < 4. These are the roots r1 and r2 in Figure 13.8.
−2
2
r
4
t = b + d
t 3 − 8t − 16
0
16
−16
Figure 13.7.
Graph of t3 −8t −16.

262
13
GEOMETRICAL CONSTRUCTIONS
1
x
r2
r1
x 4 − 4x + 2
0
1
3
2
−1
Figure 13.8.
Graph of x4 −4x + 2.
If both these roots of x4 −4x + 2 are constructible, then (r1 + r2)2 = ρ is
also constructible. But this is impossible, since ρ is a root of the irreducible
polynomial t3 −8t −16 and [Q(ρ) : Q] = 3.
Hence x4 −4x + 2 has a real root that is not constructible.
□
This example was adapted from the article by Kalmanson [42].
EXERCISES
For Exercises 13.1 to 13.6, which of the numbers are constructible?
13.1.
4
5 + √2.
13.2.
6√
2.
13.3.
2
1 +
√
7
.
13.4. (1 −4
√
7)3.
13.5. 1 −
5√
27.
13.6.
3
7 −5√2.
13.7. Is Q(cos φ) = Q(sin φ) for every angle φ?
13.8. If tan φ is constructible, show how to construct the angle φ.
13.9. Prove that all the constructible numbers are algebraic over Q.
For the values of cos φ given in Exercises 13.10 to 13.13, determine whether you
can trisect the angle φ by straightedge and compass.
13.10. cos φ = 1/4.
13.11. cos φ = −9/16.
13.12. cos φ = 1/
√
2.
13.13. cos φ =
√
2/8.
13.14. By writing π/15 in terms of π/5 and π/3, show that it is possible to
trisect π/5 and also possible to construct a regular 15-gon.
13.15. Can π/7 be trisected?
13.16. Construct a regular pentagon using straightedge and compass only.
13.17. Prove that cos(2π/7) is a root of 8x3 + 4x2 −4x −1 and that 2 cos(2π/7)
is a root of x3 + x2 −2x −1. Hence show that a regular septagon is not
constructible.

EXERCISES
263
13.18. If the regular n-gon is constructible and n = qr, show that the regular
q-gon is also constructible.
13.19. Let ξ = cos(2π/p2) + i sin(2π/p2). Show that ξ is a root of
f (x) = 1 + xp + x2p + · · · + x(p−1)p.
Prove that f (x) is irreducible over Q by applying Eisenstein’s criterion
to f (1 + x).
13.20. Using Exercises 13.18 and 13.19, prove that if a regular n-gon is
constructible, then n = 2kp1 · · · pr where p1, . . . , pr are distinct Fermat
primes.
13.21. Prove that a regular 17-gon is constructible.
For Exercises 13.22 to 13.33, can you construct a root of the polynomials?
13.22. x2 −7x −13.
13.23. x4 −5.
13.24. x8 −16.
13.25. x3 −10x2 + 2.
13.26. x4 + x3 −12x2 + 7x −1.
13.27. x5 −9x3 + 3.
13.28. x6 + x3 −1.
13.29. 3x6 −8x4 + 1.
13.30. 4x4 −x2 + 2x + 1.
13.31. x4 + x −1.
13.32. x48 −1.
13.33. x4 −4x3 + 4x2 −2.

14
ERROR-CORRECTING CODES
With the increased use of electronic instrumentation and computers, there is a
growing need for methods of transmitting information quickly and accurately
over radio and telephone lines and to and from digital storage devices. In fact,
CDs and DVDs use error-correcting codes.
Over any transmission line, there is liable to be noise, that is, extraneous
signals that can alter the transmitted information. This is not very noticeable in
listening to the radio or even in reading a telegram, because normal English is
about 20% redundant. However, in transmissions from satellites and in computer
link-ups, the redundancy is usually zero; thus we would like to detect, and possi-
bly correct, any errors in the transmitted message. We can do this by putting the
message into a code that will detect and correct most of the errors. (These are
not the sorts of codes useful to a spy. Secret codes are made deliberately hard to
break, whereas error-correcting codes are designed to be easily decoded.)
One familiar code is the parity check digit that is usually attached to each
number inside a computer. A number is written in binary form and a check digit
is added that is the sum modulo 2 of the other digits. The sum of the digits
of any number and its check digit is always even unless an error has occurred.
This check digit will detect any odd number of errors but not an even number
of errors. This is useful if the probability of two errors occurring in the same
word is very small. When a parity check failure occurs in reading words from
a computer memory, the computer automatically rereads the faulty word. If a
parity check failure occurs inside the arithmetic unit, the program usually has to
be rerun.
All the codes we construct are obtained by adding a certain number of check
digits to each block of information. Codes can either be used simply to detect
errors or can be used to correct errors. A code that will detect 2t or fewer errors
can be used to correct t or fewer errors.
Error-detecting codes are used when it is relatively easy to send the original
message again, whenever an error is detected. The single parity check code in a
computer is an example of an error-detecting code.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
264

ERROR-CORRECTING CODES
265
Sometimes it is impossible or too expensive to retransmit the original message
when an error is detected. Error-correcting codes then have to be employed.
These are used, for example, in transmissions from satellites and space probes
(see Figure 14.1). The extra equipment needed to store and retransmit messages
from a satellite would add unnecessary weight to the payload. Error-correcting
codes are also used when transmitting data from computer memories to storage
Figure 14.1.
In 1969 the Mariners 6 and 7 space probes sent back over 200 close-up photographs
of Mars. Each photograph was divided into 658,240 pixels and each pixel was given a brightness
level ranging from 1 to 28. Therefore, each photograph required about 5 million bits of information.
These bits were encoded, using an error-correcting code, and transmitted at a rate of 16,200 bits
per second back to Earth, where they were received and decoded into photographs. (Courtesy of
NASA/JPL/Caltech.)

266
14
ERROR-CORRECTING CODES
devices. If a message containing an error is stored on a device, it may be weeks
before it is read and the error detected; by this time the original data might
be lost.
THE CODING PROBLEM
In most digital computers and many communication systems, information is han-
dled in binary form; that is, messages are formed from the symbols 0 and 1.
Therefore, in this chapter, we discuss only binary codes. However, most of the
results generalize to codes whose symbols come from any ﬁnite ﬁeld.
We assume that when a message is transmitted over a channel, the probability
of the digit 1 being changed into 0 is the same as that of 0 being changed into 1.
Such channels are called binary symmetric. Figure 14.2 illustrates what might
happen to a message over a noisy channel.
To transmit a message over a noisy channel, we break up the message into
blocks of k digits and we encode each block by attaching n −k check digits to
obtain a code word consisting of n digits, as shown in Figure 14.3. Such a code
is referred to as an (n, k)-code.
The code words can now be transmitted over the noisy channel, and after
being received, they can be processed in one of two ways. The code can be used
to detect errors by checking whether or not the received word is a code word.
If the received word is a code word, it is assumed to be the transmitted word. If
Transmitter
Transmission channel
Encoder
Original message
Receiver
Error detector
or
corrector
Decoded message
noise
1 0 1 1 0
1 0 1 0 0
Figure 14.2.
Block diagram for error detection or correction.
k information digits
(n, k)–
encoder
Message of k digits
k information digits
n − k check digits
Code word of n digits
Figure 14.3.
Encoding a block of k digits.

SIMPLE CODES
267
the received word is not a code word, an error must have occurred during trans-
mission, and the receiver can request that the word be retransmitted. However,
the code could also be used to correct errors. In this case, the decoder chooses
the transmitted code word that is most likely to produce each received word.
Whether a code is used as an error-detecting or error-correcting code depends
on each individual situation. More equipment is required to correct errors, and
fewer errors can be corrected than could be detected; on the other hand, when
a code only detects errors, there is the trouble of stopping the decoding process
and requesting retransmission every time an error occurs.
In an (n, k)-code, the original message is k digits long and there are 2k different
possible messages and hence 2k code words. The received words have n digits;
hence there are 2n possible words that could be received, only 2k of which are
code words.
The extra n −k check digits that are added to produce the code word are
called redundant digits because they carry no new information but only allow
the existing information to be transmitted more accurately. The ratio R = k/n is
called the code rate or information rate.
For each particular communications channel, it is a major problem to design
a code that will transmit useful information as fast as possible and, at the same
time, as reliably as possible.
It was proved by C. E. Shannon in 1948 that each channel has a deﬁnite
capacity C, and for any rate R < C, there exist codes of rate R such that the
probability of erroneous decoding is arbitrarily small. In other words, by increas-
ing the code length n and keeping the code rate R below the channel capacity
C, it is possible to make the probability of erroneous decoding as small as we
please. However, this theory provides no useful method of ﬁnding such codes.
For codes to be efﬁcient, they usually have to be very long; they may contain
2100 messages and many times that number of possible received words. To be
able to encode and decode such long codes effectively, we look at codes that
have a strong algebraic structure.
SIMPLE CODES
We now compare two very simple codes of length 3. The ﬁrst is the (3, 2)-code
that attaches a single parity check to a message of length 2. The parity check
is the sum modulo 2 of the digits in the message. Hence a received word is a
code word if and only if it contains an even number of 1’s. The code words are
given in Table 14.1. The second code is the (3, 1)-code that repeats a message,
consisting of a single digit, three times. Its two code words are illustrated in
Table 14.2.
If one error occurs in the (3, 2) parity check code during transmission, say
101 is changed to 100, then this would be detected because there would be an
odd number of 1’s in the received word. However, this code will not correct any
errors; the received word 100 is just as likely to have come from 110 or 000 as
from 101. This code will not detect two errors either. If 101 was the transmitted

268
14
ERROR-CORRECTING CODES
TABLE 14.1. (3, 2)
Parity Check Code
Message
Code Word
00
000
01
101
10
110
11
011
↑
parity check
TABLE 14.2. (3, 1)
Repeating Code
Message
Code Word
0
000
1
111
code word and errors occurred in the ﬁrst two positions, the received word would
be 011, and this would be erroneously decoded as 11.
The decoder ﬁrst performs a parity check on the received word. If there are
an even number of 1’s in the word, the word passes the parity check, and the
message is the last two digits of the word. If there are an odd number of 1’s in
the received work, it fails the parity check, and the decoder registers an error.
Examples of this decoding are shown in Table 14.3.
The (3, 1) repeating code can be used as an error-detecting code, and it will
detect one or two transmission errors but, of course, not three errors. This same
code can also be used as an error-correcting code. If the received word contains
more 1’s than 0’s, the decoder assumes that the message is 1; otherwise, it
assumes that the message is 0. This will correctly decode messages containing
one error, but will erroneously decode messages containing more than one error.
Examples of this decoding are shown in Table 14.4.
One useful way to discover the error-detecting and error-correcting capabili-
ties of a code is by means of the Hamming distance. The Hamming distance
between two words u and v of the same length is deﬁned to be the number of
positions in which they differ. This distance is denoted by d(u, v). For example,
d(101, 100) = 1, d(101, 010) = 3, and d(010, 010) = 0.
TABLE 14.3. (3, 2) Parity Check Code Used to
Detect Errors
Received Word
101
111
100
000
110
Parity Check
Passes
Fails
Fails
Passes
Passes
Received Message
01
Error
Error
00
10
TABLE 14.4. (3, 1) Repeating Code Used
to Correct Errors
Received Word
111
010
011
000
Decoded Message
1
0
1
0

SIMPLE CODES
269
011
010
000
100
101
111
110
001
Figure 14.4.
The code words of the (3,2)
parity check code are shown as large dots.
011
010
000
100
101
111
110
001
Figure 14.5.
The code words of the (3,1)
repeating code are shown as large dots.
The Hamming distance between two words is the number of single errors
needed to change one word into the other. In an (n, k)-code, the 2n received
words can be thought of as placed at the vertices of an n-dimensional cube with
unit sides. The Hamming distance between two words is the shortest distance
between their corresponding vertices along the edges of the n-cube. The 2k code
words form a subset of the 2n vertices, and the code has better error-correcting
and error-detecting capabilities the farther apart these code words are. Figure 14.4
illustrates the (3,2) parity check code whose code words are at Hamming distance
2 apart. Figure 14.5 illustrates the (3,1) repeating code whose code words are at
Hamming distance 3 apart.
Proposition 14.1. A code will detect all sets of t or fewer errors if and only if
the minimum Hamming distance between code words is at least t + 1.
Proof. If r errors occur when the code word u is transmitted, the received
word v is at Hamming distance r from u. These transmission errors will be
detected if and only if v is not another code word. Hence all sets of t or fewer
errors in the code word u will be detected if and only if the Hamming distance
of u from all the other code words is at least t + 1.
□
Proposition 14.2. A code is capable of correcting all sets of t or fewer errors
if and only if the minimum Hamming distance between code words is at least
2t + 1.
Proof. Suppose that the code contains two code words u1 and u2 at Hamming
distance 2t or closer. Then there exists a received word v that differs from u1
and u2 in t or fewer positions. This received word v could have originated from
u1 or u2 with t or fewer errors and hence would not be correctly decoded in both
these situations.
Conversely, any code whose code words are at least 2t + 1 apart is capable
of correcting up to t errors. This can be achieved in decoding by choosing the
code word that is closest to each received word.
□
Table 14.5 summarizes these results.

270
14 ERROR-CORRECTING CODES
TABLE 14.5. Detection Capabilities of Various Codes
Code
Minimum Distance
between
Code Words
Number of
Errors
Detectable
Number of
Errors
Correctable
Information
Rate
(3,2) parity check code
2
1
0
2/3
(3,1) repeating code
3
2
1
1/3
General (n, k) code
d
d −1
⩽(d −1)/2
k/n
POLYNOMIAL REPRESENTATION
There are various ways that a word of n binary digits can be represented alge-
braically. One convenient way is by means of a polynomial in Z2[x] of degree
less than n. The word a0a1 · · · an−1 can be represented by the polynomial
a0 + a1x + · · · + an−1xn−1 ∈Z2[x].
We now use this representation to show how codes can be constructed.
Let p(x) ∈Z2[x] be a polynomial of degree n −k. The polynomial code
generated by p(x) is an (n, k)-code whose code words are precisely those poly-
nomials, of degree less than n, which are divisible by p(x).
A message of length k is represented by a polynomial m(x), of degree less
than k. In order that the higher-order coefﬁcients in a code polynomial carry the
message digits, we multiply m(x) by xn−k. This has the effect of shifting the
message n −k places to the right. To encode the message polynomial m(x), we
divide xn−km(x) by p(x) and add the remainder, r(x), to xn−km(x) to form the
code polynomial
v(x) = r(x) + xn−km(x).
This code polynomial is always a multiple of p(x) because, by the division
algorithm,
xn−km(x) = q(x) · p(x) + r(x)
where
deg r(x) < n −k
or
r(x) = 0;
thus
v(x) = r(x) + xn−km(x) = −r(x) + xn−km(x) = q(x) · p(x).
(Remember r(x) = −r(x) in Z2[x].) The polynomial xn−km(x) has zeros in the
n −k lowest-order terms, whereas the polynomial r(x) is of degree less than
n −k; hence the k highest-order coefﬁcients of the code polynomial v(x) are
the message digits, and the n −k lowest-order coefﬁcients are the check digits.
These check digits are precisely the coefﬁcients of the remainder r(x).

POLYNOMIAL REPRESENTATION
271
For example, let p(x) = 1 + x2 + x3 + x4 be the generator polynomial of a
(7, 3)-code. We encode the message 101 as follows:
message = 1
0
1
m(x) = 1
+ x2
x4m(x) =
x4
+ x6
r(x) = 1 + x
v(x) = r(x) + x4m(x) = 1 + x
+ x4
+ x6
code word = 1
1
0 0
1 0
1



check digits



message
x2 + x + 1
x4 + x3 + x2 + 0 + 1
x6
+x4
x6+x5+x4
+x2
x5
+x2
x5+x4+x3
+x
x4+x3+x2+x
x4+x3+x2
+1
x+1
The generator polynomial p(x) = a0 + a1x + · · · + an−kxn−k is always cho-
sen so that a0 = 1 and an−k = 1, since this avoids wasting check digits. If a0 = 0,
any code polynomial would be divisible by x and the ﬁrst digit of the code word
would always be 0; if an−k = 0, the coefﬁcient of xn−k−1 in the code polynomial
would always be 0.
Example 14.3. Write down all the code words for the code generated by the
polynomial p(x) = 1 + x + x3 when the message length k is 3.
Solution. Since deg p(x) = 3, there will be three check digits, and since the
message length k is 3, the code word length n will be 6. The number of messages
is 2k = 8.
x + 1
x3 + 0 + x + 1
x4+x3
x4
+x2+x
x3+x2+x
x3
x+1
x2
+1
Consider the message 110, which is represented by the polynomial m(x) =
1 + x. Its check digits are the coefﬁcients of the remainder r(x) = 1 + x2,

272
14
ERROR-CORRECTING CODES
obtained by dividing x3m(x) = x3 + x4 by p(x). Hence the code polynomial
is v(x) = r(x) + x3m(x) = 1 + x2 + x3 + x4, and the code word is 101110.
Table 14.6 shows all the code words.
□
A received message can be checked for errors by testing whether it is divisible
by the generator polynomial p(x). If the remainder is nonzero when the received
polynomial u(x) is divided by p(x), an error must have occurred during trans-
mission. If the remainder is zero, the received polynomial u(x) is a code word,
and either no error has occurred or an undetectable error has occurred.
Example 14.4. If the generator polynomial is p(x) = 1 + x + x3, test whether
the following received words contain detectable errors: (i) 100011, (ii) 100110,
(iii) 101000.
Solution. The received polynomials are 1 + x4 + x5, 1 + x3 + x4, and 1 + x2,
respectively. These contain detectable errors if and only if they have nonzero
remainders when divided by p(x) = 1 + x + x3.
x2 + x + 1
x3 + x + 1
x5+x4+ 0+ 0+0+1
x5
+x3+x2
x4+x3+x2
+1
x4
+x2+x
x3
+x+1
x3
+x+1
0
x + 1
x3 + x + 1
x4+x3+ 0+0+1
x4
+x2+x
x3+x2+x+1
x3
+x+1
x2
0
x3 + x + 1
x2+0+1
0
x2
+1
Hence 1 + x4 + x5 is divisible by p(x), but 1 + x3 + x4 and 1 + x2 are not.
Therefore, errors have occurred in the latter two words but are unlikely to have
occurred in the ﬁrst.
□
Table 14.6 lists all the code words for this code. Hence, in Example 14.4 we
can tell at a glance whether a word is a code word simply by noting whether it is
on this list. However, in practice, the list of code words is usually so large that
it is easier to calculate the remainder when the received polynomial is divided
by the generator polynomial.

POLYNOMIAL REPRESENTATION
273
TABLE 14.6. (6, 3) Code Generated by 1 + x + x 3
Code Word
Message
Check Digits
Message Digits
0
0
0
0
0
0
0
0
0
1
0
0
1
1
0
1
0
0
0
1
0
0
1
1
0
1
0
0
0
1
1
1
1
0
0
1
1
1
0
1
0
1
1
1
0
1
0
1
0
0
1
1
0
1
0
1
1
1
0
0
0
1
1
1
1
1
0
1
0
1
1
1
↑
↑
↑
↑
↑
↑
↑
↑
↑
1
x
x2
1
x
x2
x3
x4
x5
Furthermore, this remainder can easily be computed using shift registers.
Figure 14.6 shows a shift register for dividing by 1 + x + x3. The square boxes
represent unit delays, and the circle with a cross inside denotes a modulo 2 adder
(or exclusive OR gate).
The delays are initially zero, and a polynomial u(x) is fed into this shift register
with the high-order coefﬁcients ﬁrst. When all the coefﬁcients of u(x) have been
fed in, the delays contain the remainder of u(x) when divided by 1 + x + x3. If
these are all zero, the polynomial u(x) is a code word; otherwise, a detectable
error has occurred. Table 14.7 illustrates this shift register in operation.
The register in Figure 14.6 could be modiﬁed to encode messages, because
the check digits for m(x) are the coefﬁcients of the remainder when x3m(x)
x0
x1
x 2
x3 = 1 + x
u(x)
(high order first)
Figure 14.6.
Shift register for dividing by 1 + x + x3.
x1
x 2
x0
OR
1
2
Encoded
message
Message
m(x)
v(x)
Figure 14.7.
Encoding circuit for a code generated by 1 + x + x3.

274
14
ERROR-CORRECTING CODES
TABLE 14.7. Contents of the Shift Register When
1 + x 3 + x 4 Is Divided by 1 + x + x 3
Received Polynomial
Register
Waiting to Enter
Contents
Stage
Register
x0
x1
x2
0
1
0
0
1
1
0
0
0
0
←register initially zero
1
1
0
0
1
1
0
0
0
2
1
0
0
1
1
0
0
3
1
0
0
1
1
0
4
1
0
0
1
1
5
1
1
1
1
6
0
0
1
←remainder is x2
is divided by 1 + x + x3. However, the circuit in Figure 14.7 is more efﬁcient
for encoding. Here the message m(x) is fed simultaneously to the shift register
and the output. While m(x) is being fed in, the switch is in position 1 and the
remainder is calculated by the register. Then the switch is changed to position 2,
and the check digits are let out to immediately follow the message.
This encoding circuit could also be used for error detection. When u(x) is fed
into the encoding circuit with the switch in position 1, the register calculates the
remainder of x3u(x) when divided by p(x). However, u(x) is divisible by p(x)
if and only if x3u(x) is divisible by p(x), assuming that p(x) does not contain a
factor x.
How is the generator polynomial chosen so that the code has useful properties
without adding too many check digits? We now give some examples.
Proposition 14.5. The polynomial p(x) = 1 + x generates the (n, n −1) parity
check code.
Proof. By Proposition 9.33, a polynomial in Z2[x] is divisible by 1 + x if and
only if it contains an even number of nonzero coefﬁcients. Hence the code words
of a code generated by 1 + x are those words containing an even number of 1’s.
The check digit for the message polynomial m(x) is the remainder when xm(x) is
divided by 1 + x. Therefore, by the remainder theorem, Theorem 9.4, the check
digit is m(1), the parity of the number of 1’s in the message. This code is the
parity check code.
□
The (3, 1) code that repeats the single message digit three times has code
words 000 and 111, and is generated by the polynomial 1 + x + x2.
We now give one method, using primitive polynomials, of ﬁnding a generator
for a code that will always detect single, double, or triple errors. Furthermore,
the degree of the generator polynomial will be as small as possible so that the
check digits are reduced to a minimum. Recall (see Proposition 11.29) that an
irreducible polynomial p(x) of degree m over Z2 is primitive if p(x)|(1 + xk)
for k = 2m −1 and for no smaller k.

POLYNOMIAL REPRESENTATION
275
Theorem 14.6. If p(x) is a primitive polynomial of degree m, then the
(n, n −m)-code generated by p(x) detects all single and double errors whenever
n ⩽2m −1.
Proof. Let v(x) be a transmitted code word and u(x) = v(x) + e(x) be the
received word. The polynomial e(x) is called the error polynomial. An error is
detectable if and only if p(x)̸ |u(x). Since p(x) does divide the code word v(x),
an error e(x) will be detectable if and only if p(x)̸ |e(x).
If a single error occurs, the error polynomial contains a single term, say xi,
where 0 ⩽i < n. Since p(x) is irreducible, it does not have 0 as a root; therefore,
p(x)̸ |xi, and the error xi is detectable.
If a double error occurs, the error polynomial e(x) is of the form xi + xj
where 0 ⩽i < j < n. Hence e(x) = xi(1 + xj−i), where 0 < j −i < n. Now
p(x)̸ |xi, and since p(x) is primitive, p(x)̸ |(1 + xj−i) if j −i < 2m −1. Since
p(x) is irreducible, p(x)̸ |xi(1 + xj−i) whenever n ⩽2m −1, and all double
errors are detectable.
□
Corollary 14.7. If p1(x) is a primitive polynomial of degree m, the
(n, n −m −1)-code generated by p(x) = (1 + x)p1(x) detects all double errors
and any odd number of errors whenever n ⩽2m −1.
Proof. The code words in the code generated by p(x) must be divisible by
p1(x) and by (1 + x). The factor (1 + x) has the effect of adding an overall
parity check digit to the code. By Proposition 9.33, all the code words have an
even number of terms, and the code will detect any odd number of errors. Since
the code words are divisible by the primitive polynomial p1(x), the code will
detect all double errors if n ⩽2m −1.
□
Some primitive polynomials of low degree are given in Table 14.8. For
example, by adding 11 check digits to a message of length 1012 or less, using the
generator polynomial (1 + x)(1 + x3 + x10) = 1 + x + x3 + x4 + x10 + x11, we
can detect single, double, triple, and any odd number of errors. Furthermore, the
TABLE 14.8. Short Table of Primitive Polynomials
in Z2[x]
Primitive Polynomial
Degree m
2m −1
1 + x
1
1
1 + x + x2
2
3
1 + x + x3
3
7
1 + x + x4
4
15
1 + x2 + x5
5
31
1 + x + x6
6
63
1 + x3 + x7
7
127
1 + x2 + x3 + x4 + x8
8
255
1 + x4 + x9
9
511
1 + x3 + x10
10
1023

276
14
ERROR-CORRECTING CODES
encoding and detecting can be done by a small shift register using only 11 delay
units. The number of different messages of length 1012 is 21012, an enormous
ﬁgure! When written out in base 10, it would contain 305 digits.
MATRIX REPRESENTATION
Another natural way to represent a word a1a2 . . . an of length n is by the element
(a1, a2, . . . , an)T of the vector space Zn
2 = Z2 × Z2 × · · · × Z2 of dimension n
over Z2. We denote the elements of our vector spaces as column vectors, and
(a1, a2, . . . , an)T denotes the transpose of (a1, a2, . . . , an). In an (n, k)-code, the
2k possible messages of length k are all the elements of the vector space Zk
2,
whereas the 2n possible received words of length n form the vector space Zn
2.
An encoder is an injective function
γ : Zk
2 →Zn
2
that assigns to each k digit message an n-digit code word.
An (n, k)-code is called a linear code if the encoding function is a linear
transformation from Zk
2 to Zn
2. Nearly all block codes in use are linear codes, and
in particular, all polynomial codes are linear.
Proposition 14.8. Let p(x) be a polynomial of degree n −k that generates an
(n, k)-code. Then this code is linear.
Proof. Let γ : Zk
2 →Zn
2 be the encoding function deﬁned by the generator
polynomial p(x). Let m1(x) and m2(x) be two message polynomials of degree
less than k and let m1 and m2 be the same messages considered as vectors
in Zk
2. The code vector γ (mi) corresponds to the code polynomial vi(x) =
ri(x) + xn−kmi(x), where ri(x) is the remainder when xn−kmi(x) is divided by
p(x). Now
v1(x) + v2(x) = r1(x) + r2(x) + xn−k[m1(x) + m2(x)],
and r1(x) + r2(x) has degree less than n −k; therefore, r1(x) + r2(x) is the
remainder when xn−km1(x) + xn−km2(x) is divided by p(x). Hence v1(x) +
v2(x) corresponds to the code vector γ (m1 + m2) and
γ (m1 + m2) = γ (m1) + γ (m2).
Since the only scalars are 0 and 1, this implies that γ is a linear trans-
formation.
□
Let {e1, e2, . . . , en} be the standard basis of the vector space Zn
2, that is, ei
contains a 1 in the ith position and 0’s elsewhere. Let G be the n × k matrix that
represents, with respect to the standard basis, the transformation γ : Zk
2 →Zn
2,
deﬁned by an (n, k) linear code. This matrix G is called the generator matrix
or encoding matrix of the code.

MATRIX REPRESENTATION
277
If m is a message vector, its code word is v = Gm. The code vectors are the
vectors in the image of γ , and they form a vector subspace of Zn
2 of dimension k.
The columns of G are a basis for this subspace, and therefore, a vector is a code
vector if and only if it is a linear combination of the columns of the generator
matrix G.
(Most coding theorists write the elements of their vector spaces as row vectors
instead of column vectors, as used here. In this case, their generator matrix is
the transpose of ours, and it operates on the right of the message vector.)
In the (3,2) parity check code, a vector m = (m1, m2)T is encoded as v =
(c, m1, m2)T , where the parity check c = m1 + m2. Hence the generator matrix is
G =


1
1
1
0
0
1


because


1
1
1
0
0
1


	
m1
m2

=


c
m1
m2

.
If the code word is to contain the message digits in its last k positions, the
generator matrix must be of the form G =
	P
Ik

, where P is an (n −k) × k
matrix and Ik is the k × k identity matrix.
Example 14.9. Find the generator matrix for the (6,3)-code of Example 14.3 that
is generated by the polynomial 1 + x + x3.
Solution. The columns of the generator matrix G are the code vectors corre-
sponding to messages consisting of basis elements e1 = (1, 0, 0)T , e2 = (0, 1, 0)T ,
and e3 = (0, 0, 1)T . We see from Table 14.6 that the generator matrix is
G =


1
0
1
1
1
1
0
1
1
1
0
0
0
1
0
0
0
1


.
□
Any message vector, m, in the (6,3)-code of Example 14.9 can be encoded by
calculating Gm. However, given any received vector u it is not easy to determine
from the generator matrix G whether or not u is a code vector. The code vectors
form a subspace, Im γ , of dimension k in Zn
2, generated by the columns of G.
We now ﬁnd a linear transformation η: Zn
2 →Zn−k
2
, represented by a matrix H,
whose kernel is precisely Im γ . Hence a vector u will be a code vector if and
only if Hu = 0. This proves (ii) in the following theorem.
Theorem 14.10. Let γ : Zk
2 →Zn
2 be the encoding function for a linear (n, k)-
code with generator matrix G =
P
Ik

, where P is an (n −k) × k matrix and Ik
is the k × k identity matrix. Then the linear transformation
η: Zn
2 →Zn−k
2

278
14
ERROR-CORRECTING CODES
deﬁned by the (n −k) × n matrix H = (In−k|P ) has the following properties:
(i) Ker η = Im γ .
(ii) A received vector u is a code vector if and only if Hu = 0.
Proof. The composition η Ž γ : Zk
2 →Zn−k
2
is the zero transformation because
HG = (In−k|P )
P
Ik

= (In−kP + P Ik) = P + P = 0
using block multiplication of matrices over the ﬁeld Z2. Hence Im γ ⊆Ker η.
Since the ﬁrst n −k columns of H consist of the standard basis vectors in
Zn−k
2
, Im η spans Zn−k
2
and contains 2n−k elements. By the morphism theorem
for groups,
|Ker η| =
|Zn
2|
|Im η| =
2n
2n−k = 2k.
But Im γ also contains 2k elements, and therefore Im γ must equal Ker η.
□
The (n −k) × n matrix H in Theorem 14.10 is called the parity check matrix
of the (n, k)-code.
The parity check matrix of the (3, 2) parity check code is the 1 × 3 matrix
H =
 1
1
1 
. A received vector u = (u1, u2, u3)T is a code vector if and
only if
Hu =
 1
1
1 


u1
u2
u3

= u1 + u2 + u3 = 0.
The parity check matrix of the (3, 1)-code that repeats the message three
times is the 2 × 3 matrix H =
	
1
0
1
0
1
1

. A received vector u = (u1, u2, u3)T
is a code vector if and only if Hu = 0, that is, if and only if u1 + u3 = 0 and
u2 + u3 = 0. In Z2, this is equivalent to u1 = u2 = u3.
The parity check matrix for the (6, 3)-code of Examples 14.3 and 14.9 is
H =


1
0
0
1
0
1
0
1
0
1
1
1
0
0
1
0
1
1

.
The received vector u = (u1, . . . , u6)T is a code vector if and only if
u1
+
u4
+
u6
=
0
u2
+
u4
+
u5
+
u6
=
0
u3
+
u5
+
u6
=
0.

MATRIX REPRESENTATION
279
That is, if and only if
u1
=
u4
+
u6
u2
=
u4
+
u5
+
u6
u3
=
u5
+
u6.
In this code, the three digits on the right, u4, u5, and u6, are the message
digits, whereas u1, u2, and u3 are the check digits. For each code vector u, the
equation Hu = 0 expresses each check digit in terms of the message digits. This
is why H is called the parity check matrix.
Example 14.11. Find the generator matrix and parity check matrix for the
(9, 4)-code generated by p(x) = (1 + x)(1 + x + x4) = 1 + x2 + x4 + x5. Then
use the parity check matrix to determine whether the word 110110111 is a
code word.
Solution. The check digits attached to a message polynomial m(x) are the
coefﬁcients of the remainder when x5m(x) is divided by p(x). The message
polynomials are linear combinations of 1, x, x2, and x3. We can calculate the
remainders when x5, x6, x7, and x8 are divided by p(x) as follows. [This is just
like the action of a shift register that divides by p(x).]
x5 ≡1 + x2 + x4 mod p(x)
x6 ≡x + x3 + x5 ≡1 + x + x2 + x3 + x4 mod p(x)
x7 ≡x + x2 + x3 + x4 + x5 ≡1 + x + x3 mod p(x)
x8 ≡x + x2 + x4 mod p(x).
Therefore, every code polynomial is a linear combination of the following basis
polynomials:
1
+
x2
+
x4
+
x5
1
+
x
+
x2
+
x3
+
x4
+
x6
1
+
x
+
x3
+
x7
x
+
x2
+
x4
+
x8.
The generator matrix G is obtained from the coefﬁcients of the polynomials
above, and the parity check matrix H is obtained from G. Hence
G =


1
1
1
0
0
1
1
1
1
1
0
1
0
1
1
0
1
1
0
1
1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1


and
H =


1
0
0
0
0
1
1
1
0
0
1
0
0
0
0
1
1
1
0
0
1
0
0
1
1
0
1
0
0
0
1
0
0
1
1
0
0
0
0
0
1
1
1
0
1

.

280
14
ERROR-CORRECTING CODES
If
the
received
vector
is
u = ( 1
1
0
1
1
0
1
1
1 )T , Hu =
( 1
0
0
1
1 )T and hence u is not a code vector.
□
Summing up, if G =
P
Ik

is the generator matrix of an (n, k)-code, then
H = (In−k|P ) is the parity check matrix. We encode a message m by calculating
Gm, and we can detect errors in a received vector u by calculating Hu. A linear
code is determined by either giving its generator matrix or by giving its parity
check matrix.
ERROR CORRECTING AND DECODING
We would like to ﬁnd an efﬁcient method for correcting errors and decoding. One
crude method would be to calculate the Hamming distance between a received
word and each code word. The code word closest to the received word would
be assumed to be the most likely transmitted word. However, the magnitude of
this task becomes enormous as soon as the message length is quite large.
Consider an (n, k) linear code with encoding function γ : Zk
2 →Zn
2. Let V =
Im γ be the subspace of code vectors. If the code vector v ∈V is sent through a
channel and an error e ∈Zn
2 occurs during transmission, the received vector will
be u = v + e. The decoder receives the vector u and has to determine the most
likely transmitted code vector v by ﬁnding the most likely error pattern e. This
error is e = −v + u = v + u. The decoder does not know what the code vector
v is, but knows that the error e lies in the coset V + u.
The most likely error pattern in each coset of Zn
2 by V is called the
coset leader.
The coset leader will usually be the element of the coset containing the smallest
number of 1’s. If two or more error patterns are equally likely, one is chosen
arbitrarily. In many transmission channels, errors such as those caused by a
stroke of lightning tend to come in bursts that affect several adjacent digits. In
these cases, the coset leaders are chosen so that the 1’s in each error pattern are
bunched together as much as possible.
The cosets of Zn
2 by the subspace V can be characterized by means of the parity
check matrix H. The subspace V is the kernel of the transformation η: Zn
2 →
Zn−k
2
; therefore, by the morphism theorem, the set of cosets Zn
2/V is isomorphic
to Im η, where the isomorphism sends the coset V + u to η(u) = Hu. Hence the
coset V + u is characterized by the vector Hu.
If H is an (n −k) × n parity check matrix and u ∈Zn
2, then the (n −k)-
dimensional vector Hu is called the syndrome of u. (Syndrome is a medical
term meaning a pattern of symptoms that characterizes a condition or disease.)
Every element of Zn−k
2
is a syndrome; thus there are 2n−k different cosets and
2n−k different syndromes.
Theorem 14.12. Two vectors are in the same coset of Zn
2 by V if and only if
they have the same syndrome.

ERROR CORRECTING AND DECODING
281
Proof. If u1, u2 ∈Zn
2, then the following statements are equivalent:
(i) V + u1 = V + u2,
(ii) u1 −u2 ∈V ,
(iii) H(u1 −u2) = 0,
(iv) Hu1 = Hu2.
□
We
can
decode
received
words
to
correct
errors
by
using
the
following procedure:
1. Calculate the syndrome of the received word.
2. Find the coset leader in the coset corresponding to this syndrome.
3. Subtract the coset leader from the received word to obtain the most likely
transmitted word.
4. Drop the check digits to obtain the most likely message.
For a polynomial code generated by p(x), the syndrome of a received poly-
nomial u(x) is the remainder obtained by dividing u(x) by p(x). This is because
the jth column of H is the remainder obtained by dividing xj−1 by p(x). Hence
the syndrome of elements in a polynomial code can easily be calculated by means
of a shift register that divides by the generator polynomial.
Example 14.13. Write out the cosets and syndromes for the (6,3)-code with
parity check matrix
H =


1
0
0
1
0
1
0
1
0
1
1
1
0
0
1
0
1
1

.
Solution. Each of the rows in Table 14.9 forms a coset with its corresponding
syndrome. The top row is the set of code words.
The element in each coset that is most likely to occur as an error pattern is
chosen as coset leader and placed at the front of each row. In the top row 000000
is clearly the most likely error pattern to occur. This means that any received
word in this row is assumed to contain no errors. In each of the next six rows,
TABLE 14.9. Syndromes and All Words of a (6,3) Code
Syndrome
Coset
Leader
Words
000
000000
110100
011010
111001
101110
001101
100011
010111
100
100000
010100
111010
011001
001110
101101
000011
110111
010
010000
100100
001010
101001
111110
011101
110011
000111
001
001000
111100
010010
110001
100110
000101
101011
011111
110
000100
110000
011110
111101
101010
001001
100111
010011
011
000010
110110
011000
111011
101100
001111
100001
010101
111
000001
110101
011011
111000
101111
001100
100010
010110
101
000110
110010
011100
111111
101000
001011
100101
010001

282
14
ERROR-CORRECTING CODES
there is one element containing precisely one nonzero digit; these are chosen as
coset leaders. Any received word in one of these rows is assumed to have one
error corresponding to the nonzero digit in its coset leader. In the last row, every
word contains at least two nonzero digits. We choose 000110 as coset leader.
We could have chosen 101000 or 010001, since these also contain two nonzero
digits; however, if the errors occur in bursts, then 000110 is a more likely error
pattern. Any received word in this last row must contain at least two errors. In
decoding with 000110 as coset leader, we are assuming that the two errors occur
in the fourth and ﬁfth digits.
Each word in Table 14.9 can be constructed by adding its coset leader to the
code word at the top of its column.
□
A word could be decoded by looking it up in the table and taking the code
word at the top of the column in which it appears. When the code is large, this
decoding table is enormous, and it would be impossible to store it in a computer.
However, in order to decode, all we really need is the parity check matrix to
calculate the syndromes, and the coset leaders corresponding to each syndrome.
Example 14.14. Decode 111001, 011100, 000001, 100011, and 101011 using
Table 14.10, which contains the syndromes and coset leaders. The parity check
matrix is
H =


1
0
0
1
0
1
0
1
0
1
1
1
0
0
1
0
1
1

.
Solution. Table 14.11 shows the calculation of the syndromes and the decod-
ing of the received words.
□
Example 14.15. Calculate the table of coset leaders and syndromes for the (9,4)
polynomial code of Example 14.11, which is generated by p(x) = 1 + x2 +
x4 + x5.
□
TABLE 14.10. Syndromes
and Coset Leaders for a
(6,3) Code
Syndrome
Coset Leader
000
000000
100
100000
010
010000
001
001000
110
000100
011
000010
111
000001
101
000110

ERROR CORRECTING AND DECODING
283
TABLE 14.11. Decoding Using Syndromes and Coset Leaders
Word received u
111001
011100
000001
100011
101011
Syndrome Hu
000
101
111
000
001
Coset leader e
000000
000110
000001
000000
001000
Code word u + e
111001
011010
000000
100011
100011
Message
001
010
000
011
011
Solution. There is no simple algorithm for ﬁnding all the coset leaders. One
method of ﬁnding them is as follows.
We write down, in Table 14.12, the 25 possible syndromes and try to ﬁnd their
corresponding coset leaders. We start ﬁlling in the table by ﬁrst entering the error
patterns, with zero or one errors, next to their syndromes. These will be the most
likely errors to occur. The error pattern with one error in the jth position is the jth
standard basis vector in Z9
2 and its syndrome is the jth column of the parity check
matrix H, given in Example 14.11. So, for instance, H(000000001) = 01101, the
last column of H.
The next most likely errors to occur are those with two adjacent errors. We
enter all these in the table. For example,
H(000000011) = H(000000010) + H(000000001)
= 11010 + 01101, the last two columns of H
= 10111.
This still does not ﬁll the table. We now look at each syndrome without a
coset leader and ﬁnd the simplest way the syndrome can be constructed from the
columns of H. Most of them come from adding two columns, but some have to
be obtained by adding three columns.
□
TABLE 14.12. Syndromes and Their Coset Leaders for a (9,4) Code
Syndrome
Coset Leader
Syndrome
Coset Leader
Syndrome
Coset Leader
00000
000000000
01011
000011100
10110
000111000
00001
000010000
01100
011000000
10111
000000011
00010
000100000
01101
000000001
11000
110000000
00011
000110000
01110
011100000
11001
110010000
00100
001000000
01111
000001010
11010
000000010
00101
000000110
10000
100000000
11011
000010010
00110
001100000
10001
001001000
11100
111000000
00111
001110000
10010
000000101
11101
000100100
01000
010000000
10011
001101000
11110
000010100
01001
010010000
10100
000011000
11111
000000100
01010
000001100
10101
000001000

284
14
ERROR-CORRECTING CODES
TABLE 14.13. Decoding Using Syndromes and Coset Leaders
Word received u
100110010
100100101
111101100
000111110
Syndrome Hu
01000
00000
10111
10011
Coset leader e
010000000
000000000
000000011
001101000
Code word u + e
110110010
100100101
111101111
001010110
Message
0010
0101
1111
0110
The (9,4)-code in Example 14.15 will, by Corollary 14.7, detect single, dou-
ble, and triple errors. Hence it will correct any single error. It will not detect all
errors involving four digits or correct all double errors, because 000000000 and
100001110 are two code words of Hamming distance 4 apart. For example, if
the received word is 100001000, whose syndrome is 00101, Table 14.12 would
decode this as 100001110 rather than 000000000; both these code words differ
from the received word by a double error.
Example 14.16. Decode 100110010, 100100101, 111101100, and 000111110
using the parity check matrix in Example 14.11 and the coset leaders in
Table 14.12.
Solution. Table 14.13 illustrates the decoding process.
□
BCH CODES
The most powerful class of error-correcting codes known to date were
discovered around 1960 by Hocquenghem and independently by Bose and
Chaudhuri. For any positive integers m and t, with t < 2m−1, there exists a
Bose–Chaudhuri–Hocquenghem (BCH) code of length n = 2m −1 that will
correct any combination of t or fewer errors. These codes are polynomial codes
with a generator p(x) of degree ⩽mt and have message length at least n −mt.
A t-error-correcting BCH code of length n = 2m −1 has a generator poly-
nomial p(x) that is constructed as follows. Take a primitive element α in the
Galois ﬁeld GF(2m). Let pi(x) ∈Z2[x] be the irreducible polynomial with αi as
a root, and deﬁne
p(x) = lcm(p1(x), p2(x), . . . , p2t(x)).
It is clear that α, α2, α3, . . . , α2t are all roots of p(x). By Exercise 11.56,
[pi(x)]2 = pi(x2) and hence α2i is a root of pi(x). Therefore,
p(x) = lcm(p1(x), p3(x), . . . , p2t−1(x)).
Since GF(2m) is a vector space of degree m over Z2, for any β = αi, the
elements 1, β, β2, . . . , βm are linearly dependent. Hence β satisﬁes a polynomial
of degree at most m in Z2[x], and the irreducible polynomial pi(x) must also

BCH CODES
285
have degree at most m. Therefore,
deg p(x) ⩽deg p1(x) · deg p3(x) · · · deg p2t−1(x) ⩽mt.
Example 14.17. Find the generator polynomials of the t-error-correcting BCH
codes of length n = 15 for each value of t less than 8.
Solution. Let α be a primitive element of GF(16), where α4 + α + 1 = 0.
We repeatedly refer back to the elements of GF(16) given in Table 11.4 when
performing arithmetic operations in GF(16) = Z2(α).
We ﬁrst calculate the irreducible polynomials pi(x) that have αi as roots. We
only need to look at the odd powers of α. The element α itself is the root of
x4 + x + 1. Therefore, p1(x) = x4 + x + 1.
If the polynomial p3(x) contains α3 as a root, it also contains
(α3)2 = α6,
(α6)2 = α12,
(α12)2 = α24 = α9,
and
(α9)2 = α18 = α3.
Hence
p3(x) = (x −α3)(x −α6)(x −α12)(x −α9)
= (x2 + (α3 + α6)x + α9)(x2 + (α12 + α9)x + α21)
= (x2 + α2x + α9)(x2 + α8x + α6)
= x4 + (α2 + α8)x3 + (α9 + α10 + α6)x2 + (α17 + α8)x + α15
= x4 + x3 + x2 + x + 1.
The polynomial p5(x) has roots α5, α10, and α20 = α5. Hence
p5(x) = (x −α5)(x −α10)
= x2 + x + 1.
The polynomial p7(x) has roots α7, α14, α28 = α13, α26 = α11, and α22 = α7.
Hence
p7(x) = (x −α7)(x −α14)(x −α13)(x −α11)
= (x2 + αx + α6)(x2 + α4x + α9)
= x4 + x3 + 1.
Now every power of α is a root of one of the polynomials p1(x), p3(x), p5(x),
or p7(x). For example, p9(x) contains α9 as a root, and therefore, p9(x) = p3(x).
The BCH code that corrects one error is generated by p(x) = p1(x) = x4 +
x + 1.
The BCH code that corrects two errors is generated by
p(x) = lcm(p1(x), p3(x)) = (x4 + x + 1)(x4 + x3 + x2 + x + 1).

286
14
ERROR-CORRECTING CODES
This least common multiple is the product because p1(x) and p3(x) are different
irreducible polynomials. Hence p(x) = x8 + x7 + x6 + x4 + 1.
The BCH code that corrects three errors is generated by
p(x) = lcm(p1(x), p3(x), p5(x))
= (x4 + x + 1)(x4 + x3 + x2 + x + 1)(x2 + x + 1)
= x10 + x8 + x5 + x4 + x2 + x + 1.
The BCH code that corrects four errors is generated by
p(x) = lcm(p1(x), p3(x), p5(x), p7(x))
= p1(x) · p3(x) · p5(x) · p7(x)
= x15 + 1
x + 1 =
14

i=0
xi.
This polynomial contains all the elements of GF(16) as roots, except for 0 and 1.
Since p9(x) = p3(x), the ﬁve-error-correcting BCH code is generated by
p(x) = lcm(p1(x), p3(x), p5(x), p7(x), p9(x))
= (x15 + 1)/(x + 1),
and this is also the generator of the six- and seven-error-correcting BCH codes.
These results are summarized in Table 14.14.
□
For example, the two-error-correcting BCH code is a (15, 7)-code with gen-
erator polynomial x8 + x7 + x6 + x4 + 1. It contains seven message digits and
eight check digits.
The seven-error-correcting code generated by (x15 + 1)/(x + 1) has message
length 1, and the two code words are the sequence of 15 zeros and the sequence
of 15 ones. Each received word can be decoded by majority rule to give the
TABLE 14.14. Construction of t-Error-Correcting BCH Codes of Length 15
t
Roots of
p2t−1(x)
Degree,
p2t−1(x)
p(x)
Degp(x) = 15 −k
Message
Length, k
1
α, α2, α4, α8
4
p1(x)
4
11
2
α3, α6, α12, α9
4
p1(x)p3(x)
8
7
3
α5, α10
2
p1(x)p3(x)p5(x)
10
5
4
α7, α14, α13, α11
4
(x15 + 1)/(x + 1)
14
1
5
α9, α3, α6, α12
4
(x15 + 1)/(x + 1)
14
1
6
α11, α7, α14, α13
4
(x15 + 1)/(x + 1)
14
1
7
α13, α11, α7, α14
4
(x15 + 1)/(x + 1)
14
1

BCH CODES
287
message 1, if the word contains more 1’s than 0’s, and to give the message 0
otherwise. It is clear that this will correct up to seven errors.
We now show that the BCH code given at the beginning of this section does
indeed correct t errors.
Lemma 14.18. The minimum Hamming distance between code words of a linear
code is the minimum number of ones in the nonzero code words.
Proof. If v1 and v2 are code words, then, since the code is linear, v1 −v2
is also a code word. The Hamming distance between v1 and v2 is equal to the
number of 1’s in v1 −v2. The result now follows because the zero word is always
a code word, and its Hamming distance from any other word is the number of
1’s in that word.
□
Theorem 14.19. If t < 2m−1, the minimum distance between code words in the
BCH code given in at the beginning of this section is at least 2t + 1, and hence
this code corrects t or fewer errors.
Proof. Suppose that the code contains a code polynomial with fewer than
2t + 1 nonzero terms,
v(x) = v1xr1 + · · · + v2txr2t
where
r1 < · · · < r2t.
This code polynomial is divisible by the generator polynomial p(x) and hence
has roots α, α2, α3, . . . , α2t. Therefore, if 1 ⩽i ⩽2t,
v(αi) = v1αir1 + · · · + v2tαir2t
= αir1(v1 + · · · + v2tαir2t−ir1).
Put
si = ri −r1
so
that
the
elements
v1, . . . , v2t
satisfy
the
following
linear equations:
v1
+
v2αs2
+
· · ·
+
v2tαs2t
=
0
v1
+
v2α2s2
+
· · ·
+
v2tα2s2t
=
0
...
...
...
v1
+
v2α2ts2
+
· · ·
+
v2tα2ts2t
=
0.
The coefﬁcient matrix is nonsingular because its determinant is the Vandermonde
determinant:
det


1
αs2
· · ·
αs2t
1
α2s2
α2s2t
...
...
1
α2ts2
· · ·
α2ts2t

=

2t⩾i>j⩾2
(αsi −αsj ) ̸= 0.

288
14
ERROR-CORRECTING CODES
This determinant is nonzero because α, α2, . . . , α2t are all different if t < 2m−1.
[The expression for the Vandermonde determinant can be veriﬁed as follows.
When the jth column is subtracted from the ith column, each term contains
a factor (αsi −αsj); hence the determinant contains this factor. Both sides are
polynomials in αs2, . . . , αs2t of the same degree and hence must differ by a
multiplicative constant. By looking at the leading diagonal, we see that this
constant is 1.]
The linear equations above must have the unique solution v1 = v2 = · · · =
v2t = 0. Therefore, there are no nonzero code words with fewer than 2t + 1
ones, and, by Lemma 14.18 and Proposition 14.2, the code will correct t or
fewer errors.
□
There is, for example, a BCH (127,92)-code that will correct up to ﬁve errors.
This code adds 35 check digits to the 92 information digits and hence contains
235 syndromes. It would be impossible to store all these syndromes and their
coset leaders in a computer, so decoding has to be done by other methods. The
errors in BCH codes can be found by algebraic means without listing the table
of syndromes and coset leaders.
In fact, any code with a relatively high information rate must be long and
consequently, to be useful, must possess a simple algebraic decoding algorithm.
Further details of the BCH and other codes can be found in Roman [46], Lidl
and Pilz [10], and Lidl and Niederreiter [34].
EXERCISES
14.1. Which of the following received words contain detectable errors when
using the (3, 2) parity check code?
110, 010, 001, 111, 101, 000.
14.2. Decode the following words using the (3, 1) repeating code to cor-
rect errors:
111, 011, 101, 010, 000, 001.
Which of the words contain detectable errors?
14.3. An ancient method of detecting errors when performing the arithmeti-
cal operations of addition, multiplication, and subtraction is the method
known as casting out nines. For each number occurring in a calcula-
tion, a check digit is found by adding together the digits in the number
and casting out any multiples of nine. The original calculation is then
performed on these check digits instead of on the original numbers. The
answer obtained, after casting out nines, should equal the check digit of
the original answer. If not, an error has occurred. For example, check
the following:
9642 × (425 −163) = 2526204.

EXERCISES
289
Add the digits of each number; 9 + 6 + 4 + 2 = 21 = 2 × 9 + 3, 4 + 2 +
5 = 9 + 2, 1 + 6 + 3 = 9 + 1. Cast out the nines and perform the calcu-
lation on these check digits:
3 × (2 −1) = 3.
Now 3 is the check digit for the answer because 2 + 5 + 2 + 6 +
2 + 0 + 4 = 2 × 9 + 3; hence this calculation checks. Why does this
method work?
14.4. Find the redundancy of the English language. Copy a paragraph from a
book leaving out every nth letter, and ask a friend to try to read the para-
graph. (Try n = 2, 3, 4, 5, 6. If a passage with every ﬁfth letter missing
can usually be read, the redundancy is at least 1
5 or 20%.)
14.5. Each recent book, when published, is given an International Standard
Book Number (ISBN) consisting of ten digits, for example, 0-471-29891-
3. The ﬁrst digit is a code for the language group, the second set of digits
is a code for the publisher, and the third group is the publisher’s number
for the book. The last digit is one of 0, 1, 2, . . . , 9, X and is a check digit.
Have a look at some recent books and discover how this check digit is
calculated. What is the 1 × 10 parity check matrix? How many errors
does this code detect? Will it correct any?
14.6. Is 1 + x3 + x4 + x6 + x7 or x + x2 + x3 + x6 a code word in the (8,4)
polynomial code generated by p(x) = 1 + x2 + x3 + x4?
14.7. Write down all the code words in the (6,3)-code generated by p(x) =
1 + x2 + x3.
14.8. Design a code for messages of length 20, by adding as few check digits
as possible, that will detect single, double, and triple errors. Also give a
shift register encoding circuit for your code.
14.9. Decode the following, using the (6,3)-code given in Table 14.9:
000101, 011001, 110000.
14.10. A (7,4) linear code is deﬁned by the equations
u1 = u4 + u5 + u7, u2 = u4 + u6 + u7, u3 = u4 + u5 + u6,
where u4, u5, u6, u7 are the message digits and u1, u2, u3 are the check
digits. Write down the generator and parity check matrices for this code.
Decode the received words 0000111 and 0001111 to correct any errors.
14.11. Find the minimum Hamming distance between the code words of the code
with generator matrix G, where
GT =


0
0
1
0
1
1
0
0
0
0
1
0
1
0
0
1
0
0
1
0
1
0
0
0
0
1
0
0
1
1
0
1
0
0
0
1

.

290
14
ERROR-CORRECTING CODES
Discuss the error-detecting and error-correcting capabilities of this code,
and write down the parity check matrix.
14.12. Encode the following messages using the generator matrix of the (9,4)-
code of Example 14.11:
1101, 0111, 0000, 1000.
For Exercises 14.13 to 14.15, ﬁnd the generator and parity check matrices for the
polynomial codes.
14.13. The (4,1)-code generated by 1 + x + x2 + x3.
14.14. The (7,3)-code generated by (1 + x)(1 + x + x3).
14.15. The (9,4)-code generated by 1 + x2 + x5.
14.16. Find the syndromes of all the received words in the (3,2) parity
check code.
14.17. Using the parity check matrix in Example 14.14 and the syndromes in
Table 14.10, decode the following words:
101110, 011000, 001011, 111111, 110011.
14.18. Using the parity check matrix in Example 14.11 and the syndromes in
Table 14.12, decode the following words:
110110110, 001001101, 111111111, 000000111.
For Exercises 14.19 to 14.22, construct a table of coset leaders and syndromes
for each code.
14.19. The (3,1)-code generated by 1 + x + x2.
14.20. The (7,4)-code with parity check matrix
H =


1
0
0
1
1
1
0
0
1
0
1
1
0
1
0
0
1
1
0
1
1

.
14.21. The (9,4)-code generated by 1 + x2 + x5.
14.22. The (7,3)-code generated by (1 + x)(1 + x + x3).
14.23. Consider the (63,56)-code generated by (1 + x)(1 + x + x6).
(a) What is the number of digits in the message before coding?
(b) What is the number of check digits?
(c) How many different syndromes are there?
(d) What is the information rate?
(e) What sort of errors will it detect?
(f) How many errors will it correct?

EXERCISES
291
14.24. One method of encoding a rectangular array of digits is to add a parity
check digit to each of the rows and then add a parity check digit to each
of the columns (including the column of row checks). For example, in
the array in Figure 14.8, the check digits are shaded and the check on
checks is crosshatched. This idea is sometimes used when transferring
information to and from magnetic tape. The same principle is used in
accounting. Show that one error can be corrected and describe how to
correct that error. Will it correct two errors? What is the maximum number
of errors that it will detect?
1
0 1
0
0
1 1
0
1
1 0
0
Figure 14.8
14.25. Let V be a vector space over Zp, where p is a prime. Show that every
subgroup is a subspace. Is this result true for a vector space over any
Galois ﬁeld?
14.26. Show that the Hamming distance between vectors has the follow-
ing properties:
(1) d(u, v) = d(v, u).
(2) d(u, v) + d(v, w) ⩾d(u, w).
(3) d(u, v) ⩾0 with equality if and only if u = v.
(This shows that d is a metric on the vector space.)
14.27. We can use elements from a ﬁnite ﬁeld GF(q), instead of binary digits,
to construct codes. If G =
P
Ik

is a generator matrix, show that
H = (In−k| −P ) is the parity check matrix.
14.28. Using elements of Z5, ﬁnd the parity check matrix of the (7,4)-code
generated by 1 + 2x + x3 ∈Z5[x].
14.29. Find the generators of the two- and three-error-correcting BCH codes
of length 15 by starting with the primitive element β in GF(16), where
β4 = 1 + β3.
14.30. Find the generator polynomial of a single-error-correcting BCH code of
length 7.
14.31. Let α be a primitive element in GF(32), where α5 = 1 + α2. Find an
irreducible polynomial in Z2[x] with α3 as a root.
14.32. Find the generator polynomial of a double-error-correcting BCH code of
length 31.

292
14
ERROR-CORRECTING CODES
14.33. A linear code is called cyclic if a cyclic shift of a code word is still a code
word; in other words, if a1a2 · · · an is a code word, then ana1a2 · · · an−1
is also a code word. Show that a binary (n, k) linear code is cyclic if
and only if the code words, considered as polynomials, form an ideal in
Z2[x]/(xn −1).
14.34. Let F be a ﬁeld. Given f (x) = a0 + a1x + a2x2 + · · · + anxn in F[x],
deﬁne the derivative f ′(x) of f (x) by f ′(x) = a1 + 2a2x + · · · + nanxn.
Show that the usual rules of differentiation hold:
(a) [af (x)]′ = af ′(x).
(b) [f (x) + g(x)]′ = f ′(x) + g′(x).
(c) [f (x)g(x)]′ = f (x)g′(x) + f ′(x)g(x).
(d) {f [g(x)]}′ = f ′[g(x)]g′(x).
[Hint for (c) and (d): Let x and y be two indeterminants over F, and write
F(x, y) for the ﬁeld of fractions of the integral domain F[x, y]. Given
f (x) in F[x], let f0(x, y) be the unique polynomial in F[x, y] such that
f (x) −f (y)
x −y
= f0(x, y) in F(x, y). Show that f ′(x) = f0(x, x).]
14.35. Let a be an element of a ﬁeld F, and let f (x) ∈F[x]. Show that (x −a)2
divides f (x) in F[x] if and only if (x −a) divides both f (x) and f ′(x).
[Hint: See Exercise 14.34.]
14.36. In n is odd, show that xn −1 is square-free when factored into irreducibles
in Z2[x]. [Hint: See Exercise 14.35.]
14.37. Write Bn = Z2[x]/(xn −1) for the factor ring, and write the coset
x + (xn −1) as t = x + (xn −1). Hence binary linear codes are written
as follows:
Bn(t) = {a0 + a2t + · · · + an−1tn−1|ai ∈Z2, tn = 1}
= {f (t)|f (x) ∈F[x], tn = 1}.
Let C denote a cyclic code (see Exercise 14.33).
(a) Show that
C = (g(t)) = {q(t)g(t)|q(t) ∈Bn(t)}
= {f (t)|g(x)dividesf (x)in Z2[x]}.
(b) If n is odd, show that C = (e(t)) where [e(t)]2 = e(t) in Bn(t).
Show further that e(t) is uniquely determined by C; it is called
the idempotent generator of C. [Hint: By Exercise 14.36 write
xn −1 = g(x)h(x), where g(x) and h(x) are relatively prime in
Z2[x].]

Appendix 1
PROOFS
If p and q denote statements, mathematical theorems usually take the form of
an implication: “If p is true, then q is true”. We write this in symbols as
p ⇒q
and read it as “p implies q.” Here p is called the hypothesis, q is called the
conclusion, and the veriﬁcation that p ⇒q is valid is called the proof of the
implication.
Example 1. If n is an odd integer, show that n2 is odd.
Proof. We proceed by assuming that n is odd, and using that information to
show that n2 is also odd. If n is odd it has the form n = 2k + 1, where k is some
integer. Hence n2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1 is also odd.
□
This is called the direct method of proof, where the truth of the hypothesis is
used directly to establish the truth of the conclusion.
Note that the computation that n2 = 2(2k2 + 2k) + 1 in Example 1 depends
on other properties of arithmetic that we did not prove. In fact, proofs that p ⇒
q usually proceed by establishing a sequence p ⇒p1 ⇒p2 ⇒· · · ⇒pn−1 ⇒
pn ⇒q of implications leading from p to q. Many of the intervening implications
are part of an established part of mathematics, and are not stated explicitly.
Another method is proof by reduction to cases. Here is an illustration.
Example 2. Show that n2 −n is even for every integer n.
Proof. This proposition may not appear to be an implication, but it can be
reformulated as: If “n is an integer,” then “n2 −n is even.” Given n, the idea is
to separate the proof into the two cases that n is even or odd. Since n is even in
the ﬁrst case and n −1 is even in the second case, we see that n2 −n = n(n −1)
is even in either case.
□
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
293

294
PROOFS
Note that it is important in Example 2 that every integer n is even or odd,
so that the two cases considered cover every possibility. Of course, a proof
can proceed by reduction to more than two cases, at least one of which must
always hold.
The statements used in mathematics are chosen so that they are either true
or false. This leads to another method of proof of an implication p ⇒q called
proof by contradiction. Since q is either true or false, the idea is to show that it
cannot happen that both p is true and q is false. We accomplish this by showing
that the assumption that p is true and q is false leads to a contradiction.
Example 3. If r is a rational number (that is, a fraction), show that r2 ̸= 2.
Proof. Here we want to prove that p ⇒q, where p is the statement that “r is
a fraction” and q is the statement that r2 ̸= 2. The idea is to show that assuming
that p is true and q is false leads to a contradiction. So assume that r = m
n is a
fraction and r2 = 2. Write m
n in lowest terms, so, in particular, m and n are not
both even. The statement r2 = 2 leads to m2 = 2n2, so m2 is even. Hence m is
even (by Example 1), say m = 2k, where k is an integer. But then the equation
m2 = 2n2 becomes 4k2 = 2n2, so n2 = 2k2 is even. Hence n is even (again by
Example 1), so we have shown that m and n are both even, contradicting the
choice of m and n. This completes the proof.
□
As in Example 3, proof by contradiction often provides the simplest veriﬁca-
tion of an implication.
To provide another example, we need the following concept. An integer greater
than 1 is called a prime (and we say that it is a prime number) if it cannot be
factored as the product of two smaller integers both greater than 1. Hence the ﬁrst
few primes are 2, 3, 5, 7, 11, 13, . . ., but 6 = 2 · 3 and 35 = 5 · 7 are not primes.
Example 4. If 2n −1 is a prime number, show that n is prime.
Proof. We must show that p ⇒q where p is the statement “2n −1 is prime”
and q is the statement that “n is prime.” Suppose that q is false, so that n = ab
where a ⩾2 and b ⩾2 are integers. For convenience, write k = 2a. Then 2n =
2ab = (2a)b = kb, and we verify that
2n −1 = kb −1 = (k −1)(kb−1 + kb−2 + · · · + k2 + k + 1).
Since k ⩾4 this is a factorization of 2n −1 as a product of integers greater than
2, a contradiction.
□
The next example illustrates one way to verify that an implication is not valid.
Example 5. Show that the implication “n is a prime” ⇒“2n −1 is a prime”
is false.

PROOFS
295
Proof. The ﬁrst few primes are n = 2, 3, 5, and 7, and the corresponding
values 2n −1 = 3, 7, 31, and 127 are all prime, as the reader can verify. However,
the next prime is n = 11, and 211 −1 = 2047 = 23 · 89 is not prime.
□
We say that n = 11 is a counterexample to the (proposed) implication in
Example 5. Note that it is enough to ﬁnd even one example in which an impli-
cation is not valid to show that the implication is false. Hence it is in a sense
easier to disprove an implication than to prove it.
The implications in Examples 4 and 5 are closely related. They have the form
p ⇒q and q ⇒p, respectively, where p is the statement “2n −1 is a prime”
and q is the statement “n is a prime.” In general, each of the statements p ⇒q
and q ⇒p is called the converse of the other, and these examples show that an
implication can be valid even though its converse is not valid.
If both p ⇒q and q ⇒p are valid, we say that p and q are logically equiv-
alent. We write this as
p ⇔q,
and read it as “p if and only if q.” Many of the most satisfying theorems assert
that two statements, ostensibly quite different, are in fact logically equivalent.
Example 6. If n is an integer, show that “n is odd” ⇔“n2 is odd.”
Proof. The proof that “n is odd” ⇒“n2 is odd” is given in Example 1. If
n2 is odd, suppose that n is not odd. Then n is even, say n = 2k, where k is
an integer. But then n2 = 2(2k) is even, a contradiction. Hence the implication
q ⇒p has been proved by contradiction.
□
Every mathematics book is full of examples of proofs of implications. This
is because of the importance of the axiomatic method. This procedure arises as
follows: In the course of studying various examples, it is observed that they all
have certain properties in common. This leads to the study of a general, abstract
system where these common properties are assumed to hold. The properties are
then called axioms in the abstract system, and the mathematician proceeds by
deducing other properties (called theorems) from these axioms using the methods
introduced in this appendix. These theorems are then true in all the concrete
examples because the axioms hold in each case. The body of theorems is called
a mathematical theory, and many of the greatest mathematical achievements
take this form. Two of the best examples are number theory and group theory,
which derive a wealth of theorems from 5 and 4 axioms, respectively.
The axiomatic method is not new: Euclid ﬁrst used it in about 300 B.C.E. to
derive all the propositions of (euclidean) geometry from a list of 10 axioms. His
book, The Elements, is one of the enduring masterpieces of mathematics.

Appendix 2
INTEGERS
The set Z = {0, ±1, ±2, ±3, . . .} of integers is essential to all of algebra, and
has been studied for centuries. In this short section we derive the basic properties
of Z, focusing on the idea of the greatest common divisor, and culminating in
the prime factorization theorem. We assume a basic knowledge of the addition
and multiplication of integers, and of their ordering.
INDUCTION
The following principle is an axiom for the set P = {1, 2, . . .} of positive num-
bers.
Well-Ordering Axiom. Every nonempty subset of P has a smallest member.
Our ﬁrst deduction from the axiom gives a very useful method for proving
sequences of statements are true.
Theorem 1. Induction Principle. Let p1, p2, . . . be statements such that:
(i) p1 is true.
(ii) If pk is true for some value of k ⩾1, then pk+1 is true.
Then pn is true for every n ⩾1.
Proof. Let X = {n ⩾1|pn is false}. If X is nonempty, let m denote the small-
est member of X (by the well-ordering axiom). Then m ̸= 1 by (i), so if we write
n = m −1, then n ⩾1 and pn is true because n /∈X. But then pm = pn+1 is true
by (ii), a contradiction. So X is empty, as required.
□
Example 2. Prove Gauss’ formula: 1 + 2 + · · · + n = 1
2n(n + 1) for n ⩾1.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
296

INTEGERS
297
Proof. If pn denotes the statement 1 + 2 + · · · + n = 1
2n(n + 1) then p1 is
true. If pk holds, pk+1 is true because 1 + 2 + · · · + k + (k + 1) = 1
2k(k + 1) +
(k + 1) = 1
2(k + 1)(k + 2). Hence every pk is true by the induction principle.
□
There is nothing special about 1 in the induction principle. In fact, the list of
statements can be started from any integer b.
Corollary 3. Extended Induction. If b is an integer, let pb, pb+1, . . . be state-
ments such that:
(i) pb is true.
(ii) If pk is true for some value of k ⩾b, then pk+1 is true.
Then pn is true for every n ⩾b.
Proof. Apply the induction principle to show that the statements q1, q2, . . .
are all true, where qk is the statement that “pb+k−1 is true.” This means that
pb, pb+1, . . . are all true, as desired.
□
Sometimes it is convenient to be able to replace the inductive assumption
that “pk is true” in Corollary 3 (ii) with the stronger assumption that “each of
pb, pb+1, . . . , pk is true.” This is valid by the next theorem.
Theorem 4. Strong Induction. If b is an integer, let pb, pb+1, . . . be statements
such that:
(i) pb is true.
(ii) If pb, pb+1, . . . , pk are all true for some value of k ⩾b then pk+1 is true.
Then pn is true for every n ⩾b.
Proof. Apply extended induction to the new statements qb, qb+1, . . ., where
qk is the statement that “each of pb, pb+1, . . . , pk is true.”
□
An integer p is called a prime if p ⩾2 and p cannot be written as a product
of positive integers apart from p = 1 · p. Hence the ﬁrst few primes are 2, 3, 5,
7, 11, 13, . . .. With a little experimentation, you can convince yourself that every
integer n ⩾2 is a product of (one or more) primes. Strong induction is needed
to prove it.
Theorem 5. Every integer n ⩾2 is a product of primes.
Proof. Let pn denote the statement of the theorem. Then p2 is clearly true.
If p2, p3, . . . , pk are all true, consider the integer k + 1. If k + 1 is a prime,
there is nothing to prove. Otherwise, k + 1 = ab, where 2 ⩽a, b ⩽k. But then
each of a and b are products of primes because pa and pb are both true by the
(strong) induction assumption. Hence ab = k + 1 is also a product of primes, as
required.
□

298
INTEGERS
Corollary 6. Euclid’s Theorem. There are inﬁnitely many primes.
Proof. Suppose on the contrary that p1, p2, . . . , pm are all the primes. In
that case, consider the integer n = 1 + p1p2 · · · pm. It is a product of primes by
Theorem 5, so is a multiple of pi for some i = 1, 2, . . . , m. But then 1 is an
integral multiple of pi, a contradiction.
□
Another famous question concerns twin primes, that is, consecutive odd num-
bers that are both primes: 3 and 5, 5 and 7, 11 and 13, . . .. The question is
whether there are inﬁnitely many twin primes. One curious fact which suggests
that there may be only ﬁnitely many is that the series   1
p
p a twin prime

of
reciprocals of twin primes is convergent, whereas the series   1
p
p a prime

of all prime reciprocals is known to be divergent. But the question remains open.
Euclid’s theorem certainly implies that there are inﬁnitely many odd primes,
that is, primes of the form 2k + 1, where k ⩾1. A natural question is whether
if a and b are positive integers, there are inﬁnitely many primes of the form
ak + b, k ⩾1. This clearly cannot happen if a and b are both multiples of some
integer greater than 1. But it is true if 1 is the only positive common divisor of
a and b, a famous theorem ﬁrst proved by P. G. L. Dirichlet (1805–1859).
DIVISORS
When we write fractions like 22 1
7 we are using the fact that 22 = 3 · 7 + 1; that
is, when 22 is divided by 7 there is a remainder of 1. The general form of this
observation is fundamental to the study of Z.
Theorem 7. Division Algorithm∗. Let n and d ⩾1 be integers. There exist
uniquely determined integers q and r such that
n = qd + r
and
0 ⩽r < d.
Proof. Let X = {n −td|t ∈Z, n −td ⩾0}. Then X is nonempty (if n ⩾0,
then n ∈X; if n < 0, then n(1 −d) ∈X). Hence let r be the smallest member
of X (by the well-ordering axiom). Then r = n −qd for some q ∈Z, and it
remains to show that r < d. But if r ⩾d, then 0 ⩽r −d = n −(q + 1)d, so
r −d is in X contrary to the minimality of r.
As to uniqueness, suppose that n = q′d + r′, where 0 ⩽r′ < d. We may
assume that r ⩽r′ (a similar argument works if r′ ⩽r). Then 0 ⩽r′ −r =
(q′ −q)d, so (q′ −q)d is a nonnegative multiple of d that is less than d (because
r′ −r ⩽r′ < d). The only possibility is (q′ −q)d = 0, so q′ = q, and hence
r′ = r.
□
∗This as not an algorithm at all, it is a theorem, but the name is well established.

INTEGERS
299
Given n and d ⩾1, the integers q and r in Theorem 7 are called, respectively,
the quotient and remainder when n is divided by d. For example, if we divide
n = −29 by d = 7, we ﬁnd that −29 = (−5) · 7 + 6, so the quotient is −5 and
remainder is 6.
The usual process of long division is a procedure for ﬁnding the quotient and
remainder for a given n and d ⩾1. However, they can easily be found with a
calculator. For example, if n = 3196 and d = 271 then n
d = 11.79 approximately,
so q = 11. Then r = n −qd = 215, so 3196 = 11 · 271 + 215, as desired.
If d and n are integers, we say that d divides n, or that d is a divisor of n, if
n = qd for some integer q. We write d|n when this is the case. Thus, a positive
integer p is prime if and only if p has no positive divisors except 1 and p. The
following properties of the divisibility relation | are easily veriﬁed:
(i) n|n for every n.
(ii) If d|m and m|n, then d|n.
(iii) If d|n and n|d, then d = ±n.
(iv) If d|n and d|m, then d|(xm + yn) for all integers x and y.
These facts will be used frequently below (usually without comment).
Given positive integers m and n, an integer d is called a common divisor of
m and n if d|m and d|n. The set of common divisors of m and n clearly has
a maximum element; what is surprising is that this largest common divisor is
actually a multiple of every common divisor. With this in mind, we make the
following deﬁnition: If m and n are integers, not both zero, we say that d is the
greatest common divisor of m and n, and write d = gcd(m, n), if the following
three conditions are satisﬁed:
(i) d ⩾1.
(ii) d|m and d|n.
(iii) If k|m and k|n, then k|d.
In other words, d = gcd(m, n) is a positive common divisor that is a multiple of
every common divisor. It is routine to use conditions (i) to (iii) to show that d
is unique if it exists. Note that d does not exist if m = 0 = n, but it does exist
in every other case (although this is not apparent). In fact, even more is true:
Theorem 8. Let m and n be integers, not both zero. Then d = gcd(m, n) exists,
and d = xm + yn for some integers x and y.
Proof. Let X = {sm + tn|s, t ∈Z; sm + tn ⩾1}. Then X is not empty since
m2 + n2 is in X, so let d be the smallest member of X (by the well-ordering
axiom). Since d ∈X we have d ⩾1 and d = xm + ym for integers x and y, prov-
ing conditions (i) and (iii) in the deﬁnition of the gcd. Hence it remains to show
that d|m and d|n. We show that d|n; the other is similar. By the division algorithm

300
INTEGERS
write n = qd + r, where 0 ⩽r < d. Then r = n −q(xm + yn) = (−qx)m +
(1 −qy)n. Hence, if r ⩾1, then r ∈X, contrary to the minimality of d. So
r = 0 and we have d|n.
□
When gcd(m, n) = xm + yn where x and y are integers, we say that gcd(m, n)
is a linear combination of m and n. There is an efﬁcient way of computing x and
y using the division algorithm. The following example illustrates the method.
Example 9. Find gcd(37, 8) and express it as a linear combination of 37 and 8.
Proof. It is clear that gcd(37, 8) = 1 because 37 is a prime; however, no linear
combination is apparent. Dividing 37 by 8, and then dividing each successive
divisor by the preceding remainder, gives the ﬁrst set of equations. The last
37 = 4 · 8 + 5
1 = 3 −1 · 2 = 3 −1(5 −1 · 3)
8 = 1 · 5 + 3
= 2 · 3 −5 = 2(8 −1 · 5) −5
5 = 1 · 3 + 2
= 2 · 8 −3 · 5 = 2 · 8 −3(37 −4 · 8)
3 = 1 · 2 + 1
= 14 · 8 −3 · 37
2 = 2 · 1
nonzero remainder is 1, the greatest common divisor, and this turns out always
to be the case. Eliminating remainders from the bottom up (as in the second set
of equations) gives 1 = 14 · 8 −3 · 37.
□
The method in Example 9 works in general.
Theorem 10. Euclidean Algorithm. Given integers m and n ⩾1, use the divi-
sion algorithm repeatedly:
m = q1n + r1
0 ⩽r1 < n
n = q2r1 + r2
0 ⩽r2 < r1
r1 = q3r2 + r3
0 ⩽r3 < r2
...
...
rk−2 = qkrk−1 + rk
0 ⩽rk < rk−1
rk−1 = qk+1rk
where in each equation the divisor at the preceding stage is divided by the
remainder. These remainders decrease
r1 > r2 > · · · ⩾0

INTEGERS
301
so the process eventually stops when the remainder becomes zero. If r1 = 0,
then gcd(m, n) = n. Otherwise, rk = gcd(m, n), where rk is the last nonzero
remainder and can be expressed as a linear combination of m and n by eliminat-
ing remainders.
Proof. Express rk as a linear combination of m and n by eliminating remain-
ders in the equations from the second last equation up. Hence every common
divisor of m and n divides rk. But rk is itself a common divisor of m and n (it
divides every ri —work up through the equations). Hence rk = gcd(m, n).
□
Two integers m and n are called relatively prime if gcd(m, n) = 1. Hence
12 and 35 are relatively prime, but this is not true for 12 and 15 because
gcd(12, 15) = 3. Note that 1 is relatively prime to every integer m. The following
theorem collects three basic properties of relatively prime integers.
Theorem 11. If m and n are integers, not both zero:
(i) m and n are relatively prime if and only if 1 = xm + yn for some integers
x and y.
(ii) If d = gcd(m, n), then m
d and n
d are relatively prime.
(iii) Suppose that m and n are relatively prime.
(a) If m|k and n|k, where k ∈Z, then mn|k.
(b) If m|kn for some k ∈Z, then m|k.
Proof. (i) If 1 = xm + yn with x, y ∈Z, then every divisor of both m and n
divides 1, so must be 1 or −1. It follows that gcd(m, n) = 1. The converse is by
the euclidean algorithm.
(ii). By Theorem 8, write d = xm + yn, where x, y ∈Z. Then 1 = x m
d + y n
d ,
and (ii) follows from (i).
(iii). Write 1 = xm + yn, where x, y ∈Z. If k = am and k = bn, a, b ∈Z,
then k = kxm + kyn = (xb + ya)mn, and (a) follows. As to (b), suppose that
kn = qm, q ∈Z. Then k = kxm + kyn = (kx + qn)m, so m|k.
□
PRIME FACTORIZATION
Recall that an integer p is called a prime if:
(i) p ⩾2.
(ii) The only positive divisors of p are 1 and p.
The reason for not regarding 1 as a prime is that we want the factorization of
every integer into primes (as in Theorem 5) to be unique. The following result
is needed.

302
INTEGERS
Theorem 12. Euclid’s Lemma. Let p denote a prime.
(i) If p|mn where m, n ∈Z, then either p|m or p|n.
(ii) If p|m1m2 · · · mr where each mi ∈Z, then p|mi for some i.
Proof. (i) Write d = gcd(m, p). Then d|p, so as p is a prime, either d =
p or d = 1. If d = p, then p|m; if d = 1, then since p|mn, we have p|n by
Theorem 11.
(ii) This follows from (i) using induction on r.
□
By Theorem 5, every integer n ⩾2 can be written as a product of (one or
more) primes. For example, 12 = 22 · 3, 15 = 3 · 5, 225 = 32 · 52. This factoriza-
tion is unique.
Theorem 13. Prime Factorization Theorem. Every integer n ⩾2 can be writ-
ten as a product of (one or more) primes. Moreover, this factorization is unique
except for the order of the factors. That is, if
n = p1p2 · · · pr
and
n = q1q2 · · · qs,
where the pi and qj are primes, then r = s and the qj can be relabeled so that
pi = qi for each i.
Proof. The existence of such a factorization was shown in Theorem 5. To
prove uniqueness, we induct on the minimum of r and s. If this is 1, then n is
a prime and the uniqueness follows from Euclid’s lemma. Otherwise, r ⩾2 and
s ⩾2. Since p1|n = q1q2 · · · qs Euclid’s lemma shows that p1 divides some qj,
say p1|q1 (after possible relabeling of the qj). But then p1 = q1 because q1 is
a prime. Hence
n
p1 = p2p3 · · · pr = q2q3 · · · qs, so, by induction, r −1 = s −1
and q2, q3, . . . , qs can be relabeled such that pi = qi for all i = 2, 3, . . . , r. The
theorem follows.
□
It follows that every integer n ⩾2 can be written in the form
n = pn1
1 pn2
2 · · · pnr
r ,
where p1, p2, . . . , pr are distinct primes, ni ⩾1 for each i, and the pi and ni are
determined uniquely by n. If every ni = 1, we say that n is square-free, while
if n has only one prime divisor, we call n a prime power.
If the prime factorization n = pn1
1 pn2
2 · · · pnr
r of an integer n is given, and if d
is a positive divisor of n, then these pi are the only possible prime divisors of d
(by Euclid’s lemma). It follows that
Corollary 14. If the prime factorization of n is n = pn1
1 pn2
2 · · · pnr
r , then the pos-
itive divisors d of n are given as follows:
d = pd1
1 pd2
2 · · · pdr
r
where
0 ⩽di ⩽ni
for each i.

INTEGERS
303
This gives another characterization of the greatest common divisor of two
positive integers m and n. In fact, let p1, p2, . . . , pr denote the distinct primes
that divide one or the other of m and n. If we allow zero exponents, these numbers
can be written in the form
n = pn1
1 pn2
2 · · · pnr
r
ni ⩾0
m = pm1
1 pm2
2 · · · pmr
r
mi ⩾0.
It follows from Corollary 14 that the positive common divisors d of m and n
have the form
d = pd1
1 pd2
2 · · · pdr
r
where 0 ⩽di ⩽min(mi, ni) for each i. [Here min(mi, ni) denotes the smaller
of the integers mi and ni.] Clearly then, we obtain gcd(m, n) if we set di =
min(mi, ni) for each i. Before recording this observation (in Theorem 15 below),
we ﬁrst consider a natural question: What if we use max(mi, ni) for each expo-
nent? [Here max(mi, ni) is the larger of the integers mi and ni.] This leads to
the dual of the notion of a greatest common divisor.
If m and n are positive integers, write n = pn1
1 pn2
2 · · · pnr
r
and m =
pm1
1 pm2
2 · · · pmr
r
where, as before, the pi are distinct primes and we have mi ⩾0
and ni ⩾0 for each i. We deﬁne the least common multiple of m and n, denoted
lcm(m, n), by
lcm(m, n) = pmax(m1,n1)
1
pmax(m2,n2)
2
· · · pmax(mr,nr)
r
.
It is clear by Corollary 14 that lcm(m, n) is a common multiple of m and n,
and that it is a divisor of any such common multiple. Hence lcm(m, n) is indeed
playing a role dual to that of the greatest common divisor. This discussion is
summarized in
Theorem 15. Suppose that m and n are positive integers, and write
n = pn1
1 pn2
2 · · · pnr
r
ni ⩾0
m = pm1
1 pm2
2 · · · pmr
r
mi ⩾0,
where the pi are distinct primes. Then:
gcd(m, n) = pmin(m1,n1)
1
pmin(m2,n2)
2
· · · pmin(mr,nr)
r
lcm(m, n) = pmax(m1,n1)
1
pmax(m2,n2)
2
· · · pmax(mr,nr)
r
.
The fact that max(m, n) + min(m, n) = m + n for any integers m and n gives
immediately:
Corollary 16. mn = gcd(m, n)lcm(m, n) for all positive integers m and n.

304
INTEGERS
Example 17. Find gcd(600, 294) and lcm(600, 294).
Proof. We have 600 = 23 · 3 · 52 and 294 = 3 · 2 · 72 so, as above, write
600 = 23315270
294 = 21315072.
Then gcd(600, 294) = 21315070 = 6, while lcm(600, 294) = 23315272 = 29,400.
Note that Corollary 16 is veriﬁed by the fact that 600 · 294 = 6 · 29,400.
□
Of course, using Theorem 15 requires ﬁnding the prime factorizations of the
integers m and n, and that is not easy. One useful observation is that if n ⩾2 is
not a prime, then it has a prime factor p ⩽√n (it cannot have two factors greater
than √n), so when looking for prime divisors of n it is only necessary to test
the primes p ⩽√n. But for large integers, this is difﬁcult, if not impossible. The
euclidean algorithm (and Corollary 16) is a better method for ﬁnding greatest
common divisors and least common multiples.
Note that this all generalizes: Given a ﬁnite collection a, b, c, . . . of positive
integers, write them as
a = pa1
1 pa2
2 · · · par
r
ai ⩾0
b = pb1
1 pb2
2 · · · pbr
r
bi ⩾0
c = pc1
1 pc2
2 · · · pcr
r
ci ⩾0,
...
...
where the pi are the distinct primes that divide at least one of a, b, c, . . .. Then
deﬁne their greatest common divisor and least common multiple as follows:
gcd(a, b, c, . . .) = pmin(a1,b1,c1,···)
1
pmin(a2,b2,c2,···)
2
· · · pmin(ar,br,cr,···)
r
lcm(a, b, c, . . .) = pmax(a1,b1,c1,···)
1
pmax(a2,b2,c2,···)
2
· · · pmax(ar,br,cr,···)
r
.
Then Theorem 15 extends as follows: gcd(a, b, c, . . .) is the common divisor of
a, b, c, . . ., that is, a multiple of every such common divisor, and lcm(a, b, c, . . .)
is the common multiple of a, b, c, . . ., that is, a divisor of every such com-
mon multiple.
This is as far as we go into number theory, the study of the integers, a subject
that has fascinated mathematicians for centuries. There remain many unanswered
questions, among them the celebrated Goldbach conjecture that every even
number greater than 2 is the sum of two primes. This appears to be very difﬁcult,
but it is known that every sufﬁciently large even number is the sum of a prime
and a number that is the product of at most two primes.
However, the twentieth century brought one resounding success. The fact that
32 + 42 = 52 shows that the equation ak + bk = ck has integer solutions if k = 2.

INTEGERS
305
However, Fermat asserted that there are no positive integer solutions if k ⩾3. He
wrote a note in his copy of Arithmetica by Diophantus that “I have discovered
a truly remarkable proof but the margin is to small to contain it.” The result
became known as Fermat’s last theorem and remained open for 300 years. But
in 1997, Andrew Wiles proved the result: He related Fermat’s conjecture to a
problem in geometry, which he solved.

BIBLIOGRAPHY AND
REFERENCES
Proofs in Mathematics
1. Bloch, Ethan D., Proofs and Fundamentals: A First Course in Abstract Mathematics. Boston:
Birkhauser, 2000.
2. Schumacher, Carol, Chapter Zero: Fundamental Notions of Abstract Mathematics, 2nd ed. Read-
ing, Mass.: Addison-Wesley, 2000.
3. Solow, Daniel, How to Read and Do Proofs: An Introduction to Mathematical Thought Pro-
cesses, 3rd ed. New York: Wiley, 2002.
Modern Algebra in General
4. Artin, Michael, Algebra. Upper Saddle River, N.J.: Prentice Hall, 1991.
5. Birkhoff, Garrett, and Thomas C. Bartee, Modern Applied Algebra. New York: McGraw-Hill,
1970.
6. Birkhoff, Garrett, and Saunders Maclane, A Survey of Modern Algebra, 4th ed. New York:
Macmillan, 1977.
7. Durbin, John R., Modern Algebra: An Introduction, 4th ed. New York: Wiley, 2000.
8. Gallian, Joseph A., Contemporary Abstract Algebra, 5th ed. Boston: Houghton Mifﬂin, 2002.
9. Herstein, I. N., Topics in Algebra, 2nd ed. New York: Wiley, 1973.
10. Lidl, Rudolf, and Gunter Pilz, Applied Abstract Algebra, 2nd ed. New York: Springer-Verlag,
1997.
11. Nicholson, W. Keith, Introduction to Abstract Algebra, 2nd ed. New York: Wiley, 1999.
12. Weiss, Edwin, First Course in Algebra and Number Theory. San Diego, Calif.: Academic Press,
1971.
History of Modern Algebra
13. Kline, Morris, Mathematical Thought from Ancient to Modern Times, Vol. 3. New York: Oxford
University Press, 1990 (Chap. 49).
14. Stillwell, John, Mathematics and Its History, 2nd ed. New York: Springer-Verlag, 2002.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
306

BIBLIOGRAPHY AND REFERENCES
307
Connections to Computer Science and Combinatorics
15. Biggs, Norman L., Discrete Mathematics, 2nd ed. Oxford: Oxford University Press, 2003.
16. Davey, B. A., and H. A. Priestley, Introduction to Lattices and Order, 2nd ed. Cambridge:
Cambridge University Press, 2002.
17. Gathen, Joachim von zur, and J¨urgen Gerhard, Modern Computer Algebra, 2nd ed. Cambridge:
Cambridge University Press, 2003.
18. Hopcroft, John E., Rajeev Motwani, and Jeffrey D. Ullman, Introduction to Automata Theory,
Languages, and Computation, 2nd ed. Reading, Mass.: Addison-Wesley, 2000.
19. Knuth, Donald E., The Art of Computer Programming, Vol. 2, Seminumerical Algorithms, 3rd
ed. Reading, Mass.: Addison-Wesley, 1998.
20. Kolman, Bernard, Robert C. Busby, and Sharon Cutler Ross, Discrete Mathematical Structures,
4th ed. Upper Saddle River, N.J.: Prentice Hall, 1999.
21. Mendelson, Elliott, Schaum’s Outline of Theory and Problems of Boolean Algebra and Switching
Circuits. New York: McGraw-Hill, 1970.
22. Stone, Harold S., Discrete Mathematical Structures and Their Applications. Chicago: Science
Research Associates, 1973.
23. Whitesitt, J. Eldon, Boolean Algebra and Its Applications. New York: Dover, 1995.
Groups and Symmetry
24. Armstrong, Mark Anthony, Groups and Symmetry. New York: Springer-Verlag, 1988.
25. Baumslag, Benjamin, and Bruce Chandler, Schaum’s Outline of Group Theory. New York:
McGraw-Hill, 1968.
26. Budden, F. J., The Fascination of Groups. Cambridge: Cambridge University Press, 1972.
27. Coxeter, H. S. M., Introduction to Geometry, 2nd ed. New York: Wiley, 1989.
28. Cundy, H. Martyn, and A. P. Rollett, Mathematical Models, 3rd ed. Stradbroke, Norfolk, Eng-
land: Tarquin, 1981.
29. Field, Michael, and Martin Golubitsky, Symmetry in Chaos: A Search for Pattern in Mathemat-
ics, Art and Nature. Oxford: Oxford University Press, 1992.
30. Hall, Marshall, Jr., The Theory of Groups. New York: Macmillan, 1959 (reprinted by the Amer-
ican Mathematical Society, 1999).
31. Lomont, John S., Applications of Finite Groups. New York: Dover, 1993.
32. Shapiro, Louis W., Finite groups acting on sets with applications. Mathematics Magazine, 46
(1973), 136–147.
Rings and Fields
33. Cohn, P. M., Introduction to Ring Theory. New York: Springer-Verlag, 2000.
34. Lidl, Rudolf, and Harald Niederreiter, Introduction to Finite Fields and Their Applications, rev.
ed. Cambridge: Cambridge University Press, 1994.
35. Stewart, Ian, Galois Theory, 3rd ed. Boca Raton, Fla.: CRC Press, 2003.
Convolution Fractions
36. Erdelyi, Arthur, Operational Calculus and Generalized Functions. New York: Holt, Rinehart
and Winston, 1962.
37. Marchand, Jean Paul, Distributions: An Outline. Amsterdam: North-Holland, 1962.
Latin Squares
38. Ball, W. W. Rouse, and H. S. M. Coxeter, Mathematical Recreations and Essays. New York:
Dover, 1987.

308
BIBLIOGRAPHY AND REFERENCES
39. Lam, C. W. H., The search for a ﬁnite projective plane of order 10. American Mathematical
Monthly, 98(1991), 305–318.
40. Laywine, Charles F., and Gary L. Mullen, Discrete Mathematics Using Latin Squares. New
York: Wiley, 1998.
Geometrical Constructions
41. Courant, Richard, Herbert Robbins, and Ian Stewart, What Is Mathematics? New York: Oxford
University Press, 1996.
42. Kalmanson, Kenneth, A familiar constructibility criterion. American Mathematical Monthly,
79(1972), 277–278.
43. Kazarinoff, Nicholas D., Ruler and the Round. New York: Dover, 2003.
44. Klein, Felix, Famous Problems of Elementary Geometry. New York: Dover, 1956.
Coding Theory
45. Kirtland, Joseph, Identiﬁcation Numbers and Check Digit Schemes. Washington, D.C.: Mathe-
matical Association of America, 2001.
46. Roman, Steven, Introduction to Coding and Information Theory. New York: Springer-Verlag,
1997.

ANSWERS TO THE
ODD-NUMBERED
EXERCISES
CHAPTER 2
2.1. Always true.
2.3. When A ∩(BC) = ∅.
2.5. When A ∩(BC) = ∅.
2.13. |A ∪B ∪C ∪D| = |A| + |B| + |C| + |D| −|A ∩B|
−|A ∩C| −|A ∩D| −|B ∩C| −|B ∩D| −|C ∩D|
+|A ∩B ∩C| + |A ∩B ∩D| + |A ∩C ∩D| + |B ∩C ∩D|
−|A ∩B ∩C ∩D|.
2.15. 4.
2.17. Yes; P(∅).
2.25.
A
B
(a)
(b)
(c)
(d)
T
T
T
T
T
T
T
F
F
F
T
T
F
T
T
T
F
F
F
F
T
T
T
T
(a) and (b) are equivalent and (c) and (d) are equivalent.
2.27. (a) is a contradiction and (b), (c) and (d) are tautologies.
2.29.
B′
C′
A
B
C′
D
B′
2.31. B′ ∧C′.
2.33. A ∨(B ∧A′); (A ∧B) ∨(A ∧B′) ∨(A′ ∧B); A ∨B.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
309

310
ANSWERS TO THE ODD-NUMBERED EXERCISES
2.35. (A ∨B) ∧(A′ ∨B) ∧(A′ ∨B′); A′ ∧B; A′ ∧B.
2.39.
D
D′
U
U′
2.41. (A ∧(B ∨C)) ∨(B ∧C).
2.43. (A ∧B) ∨(A′ ∧B′) ∨C.
2.45. A ∨C ∨D.
2.47. Orange:
(A′ ∧B′ ∧((C′ ∧D) ∨(C ∧D′))) ∨(((A′ ∧B) ∨(A ∧B′)) ∧
C′ ∧D′). Green: A ∧B ∧C ∧D.
2.49. Let the result of multiplying AB by CD be EF. Then the circuit for E
is (A′ ∧B ∧C) ∨(A ∧((B ∨C′) ∧D) ∨(B′ ∧C ∧D′)), and the circuit
for F is ((A ∧C) ∨(B ∧D)) ∧(A′ ∨B′ ∨C′ ∨D′).
2.51. (A ∨B) ∧(A′ ∨B′); (A ∨B) ∧(A ∨B′) ∧(A′ ∨B′).
2.53.
6
2
Boolean algebra
12
4
2.55.
8
4
2
1
Lattice
2.57.
2
1
Lattice
30
6
3
5
10
60
2.59. The primes pi.
2.65. Yes.
2.67. d = a ∧b′ ∧c′.
CHAPTER 3
3.1.
·
e
g
g2
g3
g4
e
e
g
g2
g3
g4
g
g
g2
g3
g4
e
g2
g2
g3
g4
e
g
g3
g3
g4
e
g
g2
g4
g4
e
g
g2
g3
3.3. See Table 8.3.
3.5. Abelian group.
3.7. Abelian group.
3.9. Not a group; the operation is not closed.
3.11. Abelian group.
3.13. Abelian group.
3.15. Group.
3.25. No.
3.27. D2.
3.29. D6.
3.31. C6.
3.33. This is the group O(2) we meet in Chapter 5.
3.35. Z, generated by a glide reﬂection.

ANSWERS TO THE ODD-NUMBERED EXERCISES
311
3.37.
C6
e
e, g2, g4
e, g3
3.39. No.
3.41. For any c ∈Q, f : Z →Q deﬁned by f (n) = cn for all n ∈Z.
3.43. No; Q∗has an element of order 2, whereas Z does not.
3.45. The identity has order 1; (12) Ž (34), (13) Ž (24), (14) Ž (23) have order 2,
and all the other elements have order 3.
3.47.
·
1
−1
i
−i
j
−j
k
−k
1
1
−1
i
−i
j
−j
k
−k
−1
−1
1
−i
i
−j
j
−k
k
i
i
−i
−1
1
k
−k
−j
j
−i
−i
i
1
−1
−k
k
j
−j
j
j
−j
−k
k
−1
1
i
−i
−j
−j
j
k
−k
1
−1
−i
i
k
k
−k
j
−j
−i
i
−1
1
−k
−k
k
−j
j
i
−i
1
−1
The identity, 1, has order 1; −1 has order 2; all the other elements have
order 4.
3.53. {(1), (123), (132)}.
3.55.

1
2
3
4
1
3
4
2

.
3.57. (12435).
3.59. (165432) is of order 6 and is odd.
3.61. (1526) Ž (34) is of order 4 and is even.
3.63.

1
2
3
4
5
2
3
4
5
1

.
3.65. (132).
3.67. {(1), (12), (34), (12) Ž (34), (13) Ž (24), (14) Ž (23), (1324), (1423)}.
3.69. {(1), (13), (24), (13) Ž (24), (12) Ž (34), (14) Ž (23), (1234), (1432)}.
3.73. φ(n), the number of positive integers less than n that are relatively prime
to n.
3.75. 52; 8.
3.77. {e}.
3.83. (1) Achievable; (3) achievable.
3.85. S3.
3.87. S2.
3.89. F is not abelian; y−1x−1y−1xx.
CHAPTER 4
4.1. Equivalence relation whose equivalence classes are the integers.
4.3. Not an equivalence relation.

312
ANSWERS TO THE ODD-NUMBERED EXERCISES
4.5.
Left Cosets
Right Cosets
H
= {(1), (12), (34), (12) Ž (34)}
H
= {(1), (12), (34), (12) Ž (34)}
(13)H
= {(13), (123), (134), (1234)}
H(13) = {(13), (132), (143), (1432)}
(14)H
= {(14), (124), (143), (1243)}
H(14) = {(14), (142), (134), (1342)}
(23)H
= {(23), (132), (234), (1342)}
H(23) = {(23), (123), (243), (1243)}
(24)H
= {(24), (142), (243), (1432)}
H(24) = {(24), (124), (234), (1234)}
(1324)H
= {(1324), (14) Ž (23),
H(1324) = {(1324), (13) Ž (24),
(13) Ž (24), (1423)}
(14) Ž (23), (1423)}
4.7. Not a morphism.
4.9. A morphism; Kerf = 4Z, and Imf = {(0, 0), (1, 1), (0, 2), (1, 3)}.
4.11. Not a morphism.
4.19. No.
4.21. f : C3 →C4 deﬁned by f (gr) = e.
4.23. fk: C6 →C6 deﬁned by fk(gr) = gkr for k = 0, 1, 2, 3, 4, 5.
4.25. Not isomorphic; C60 contains elements of order 4, whereas C10 × C6
does not.
4.27. Not isomorphic; Cn × C2 is commutative, whereas Dn is not.
4.29. Not isomorphic; (1 + i)/
√
2 has order 8, whereas Z4 × Z2 contains no
element of order 8.
4.33. C10 and D5.
4.39. (R+, ·).
4.49. G2 ∼= S3.
4.59. 5 is a generator of Z∗
6, and 3 is a generator of Z∗
17.
CHAPTER 5
5.1. C2 and C2.
5.3. C2 and C2 × C2.
5.5. C3 and D3.
5.7. C9 and C9.
5.9. D4.
5.11. S4.
5.13. S4.
5.15. S4.
5.17. A5.
5.19. A5.
5.21. A5.
5.25. S4.
5.27.


−1
0
0
0
−1
0
0
0
1

and


0
0
1
1
0
0
0
1
0

.
5.29. D6.
5.31. C2.
5.33. D4 generated by


−1
0
0
0
1
0
0
0
−1

and


0
−1
0
1
0
0
0
0
1

.
CHAPTER 6
6.1. 3.
6.3. 38.
6.5. 78.
6.7. 35.

ANSWERS TO THE ODD-NUMBERED EXERCISES
313
6.9. 333.
6.11. (n6 + 3n4 + 8n2)/12.
6.13. 1.
6.15. 96.
6.17. 30.
6.19. 396.
6.21. 126.
6.23. 96.
CHAPTER 7
7.1. Monoid with identity 0.
7.3. Semigroup.
7.5. Neither.
7.7. Semigroup.
7.9. Semigroup.
7.11. Monoid with identity 1.
7.13. Neither.
7.15.
gcd
1
2
3
4
1
1
1
1
1
2
1
2
1
2
3
1
1
3
1
4
1
2
1
4
7.17.
·
e
c
c2
c3
c4
e
e
c
c2
c3
c4
c
c
c2
c3
c4
c2
c2
c2
c3
c4
c2
c3
c3
c3
c4
c2
c3
c4
c4
c4
c2
c3
c4
c2
7.19. No; 01 ̸= 1 in the free semigroup.
7.29.
s1
s2
s3
s4
s5
Sends an
output signal
1
0
0
0
1
1
1
0
0
1
7.31. A congruence relation with quotient semigroup = {2N, 2N + 1}.
7.33. Not a congruence relation.
7.35.
⋆
[0]
[1]
[00]
[10]
[01]
[010]
[0]
[00]
[01]
[0]
[010]
[1]
[10]
[1]
[10]
[1]
[1]
[10]
[01]
[010]
[00]
[0]
[1]
[00]
[10]
[01]
[010]
[10]
[1]
[01]
[10]
[010]
[1]
[10]
[01]
[010]
[01]
[01]
[010]
[1]
[10]
[010]
[01]
[1]
[010]
[10]
[01]
[010]
7.37. 24.

314
ANSWERS TO THE ODD-NUMBERED EXERCISES
7.39.
⋆
[]
[α]
[β]
[γ ]
[αβ]
[αγ ]
[]
[]
[α]
[β]
[γ ]
[αβ]
[αγ ]
[α]
[α]
[]
[αβ]
[αγ ]
[β]
[γ ]
[β]
[β]
[β]
[β]
[γ ]
[β]
[γ ]
[γ ]
[γ ]
[γ ]
[γ ]
[β]
[γ ]
[β]
[αβ]
[αβ]
[αβ]
[αβ]
[αγ ]
[αβ]
[αγ ]
[αγ ]
[αγ ]
[αγ ]
[αγ ]
[αβ]
[αγ ]
[αβ]
7.41.
s1
0
1
s0 0, 1
7.43.
s01
0
s10
0
s00
0
1
0
1
1
s11
1
{[0], [1]}.
{[0], [1], [10]}.
7.45. The monoid contains 27 elements.
Dormant
1
WCF
Dormant
2
R
Dormant
3
R
R
CF
CF
W
Buds
RWC
W
Dead
RWCF
F
CHAPTER 8
8.1.
+
0
1
2
3
0
0
1
2
3
1
1
2
3
0
2
2
3
0
1
3
3
0
1
2
·
0
1
2
3
0
0
0
0
0
1
0
1
2
3
2
0
2
0
2
3
0
3
2
1
8.3. A ring.
8.5. Not a ring; not closed under multiplication.
8.7. Not a ring; not closed under addition.
8.9. A ring.
8.11. Not a ring; distributive laws do not hold.
8.17. A subring.
8.19. Not a subring; not closed under addition.
8.21. Neither.
8.23. Both.
8.25. Integral domain.
8.29. [2], [4], [5], [6], [8].

ANSWERS TO THE ODD-NUMBERED EXERCISES
315
8.31. Any nonempty proper subset of X.
8.33. Nonzero matrices with zero determinant.
8.37. f (x) = [x]6.
8.39. f (x, y) = (x, y), (y, x), (x, x) or (y, y).
8.47. (b) −1 and 0.
8.55. The identity is Dn(x) = (1/2π) + (1/π)(cos x + cos 2x + · · · + cos nx).
The ring is not an integral domain.
CHAPTER 9
9.1.
3
2x2 + 5
4x −15
8 and 45
8 x + 7
8.
9.3. x4 + x3 + x2 + x and 1.
9.5. 3 −i and 4 + 2i, or 4 −i and 1 −2i, or 4 −2i and −3 + i.
9.7. gcd(a, b) = 3, s = −5, t = 4.
9.9. gcd(a, b) = 1, s = −(2x + 1)/3, t = (2x + 2)/3.
9.11. gcd(a, b) = 2x + 1, s = 1, t = 2x + 1.
9.13. gcd(a, b) = 1, s = 1, t = −1 + 2i.
9.15. x = −6, y = 5.
9.17. x = −14, y = 5.
9.19. [23].
9.21. [17].
9.23. No solutions.
9.25. (x −1)(x4 + x3 + x2 + x + 1).
9.27. (x2 + 2)(x2 + 3).
9.29. x4 −9x + 3.
9.31. x3 −4x + 1.
9.33. (x −
√
2)(x +
√
2)(x −i
√
2)(x + i
√
2)(x −1 −i)(x −1 + i)
(x + 1 −i)(x + 1 + i).
9.35. (x2 −2)(x2 + 2)(x2 −2x + 2)(x2 + 2x + 2).
9.37. x5 + x3 + 1, x5 + x2 + 1, x5 + x4 + x3 + x2 + 1, x5 + x4 + x3 + x + 1,
x5 + x4 + x2 + x + 1, x5 + x3 + x2 + x + 1.
9.39. x3 + 2.
9.41. Kerψ = {q(x) · (x2 −2x + 4)|q(x) ∈Q[x]} and Im ψ = Q(
√
3i) =
{a + b
√
3i|a, b ∈Q}.
9.43. Irreducible by Eisenstein’s Criterion.
9.45. Irreducible, since it has no linear factors.
9.47. Reducible; any polynomial of degree >2 in R[x] is reducible.
9.49. No.
9.55. No.
9.61. x ≡40 mod 42.
9.63. x ≡22 mod 30.
9.67. 65.
CHAPTER 10
10.1. ((0, 0)), ((0, 1)), ((1, 0)), Z2 × Z2.
10.3. (0) and Q.

316
ANSWERS TO THE ODD-NUMBERED EXERCISES
10.5. (p(x)) where p(x) ∈C[x].
10.7. The quotient ring is a ﬁeld.
+
(3)
(3) + 1
(3) + 2
(3)
(3)
(3) + 1
(3) + 2
(3) + 1
(3) + 1
(3) + 2
(3)
(3) + 2
(3) + 2
(3)
(3) + 1
·
(3)
(3) + 1
(3) + 2
(3)
(3)
(3)
(3)
(3) + 1
(3)
(3) + 1
(3) + 2
(3) + 2
(3)
(3) + 2
(3) + 1
10.9. The ideal ((1, 2)) is the whole ring Z3 × Z3. The quotient ring is not a ﬁeld.
+
((1, 2))
((1, 2))
((1, 2))
·
((1, 2))
((1, 2))
((1, 2))
10.11. 8x + 2 and 14x + 97.
10.13. x2 + x and x2.
10.17. (a) 6; (b) 36; (c) x2 −1, (a) ∩(b) = (lcm(a, b)).
10.33. No.
10.35. The whole ring.
10.37.
Z8
|
([2]8)
|
([4]8)
|
([0]8)
10.39. Irreducible; Z11.
10.41. Reducible.
10.43. Irreducible; Q(
4√
2).
10.45. Irreducible; Q(
√
2,
√
3).
10.47. Not a ﬁeld; contains zero divisors.
10.49. A ﬁeld by Corollary 10.16.
10.51. A ﬁeld by Theorem 10.17.
10.53. Not a ﬁeld; x2 + 1 = (x + 2)(x + 3) in Z5[x].
10.55. A ﬁeld isomorphic to Q[x]/(x4 −11).
10.59. (0) and (xn) for n⩾0; (x) is maximal.

ANSWERS TO THE ODD-NUMBERED EXERCISES
317
CHAPTER 11
11.1. GF(5) = Z5 = {0, 1, 2, 3, 4}.
+
0
1
2
3
4
0
0
1
2
3
4
1
1
2
3
4
0
2
2
3
4
0
1
3
3
4
0
1
2
4
4
0
1
2
3
·
0
1
2
3
4
0
0
0
0
0
0
1
0
1
2
3
4
2
0
2
4
1
3
3
0
3
1
4
2
4
0
4
3
2
1
11.3. GF(9) = Z3[x]/(x2 + 1) = {aα + b|a, b ∈Z3, α2 + 1 = 0}.
+
0
1
2
α
α +1
α +2
2α
2α +1
2α +2
0
0
1
2
α
α +1
α +2
2α
2α +1
2α +2
1
1
2
0
α +1
α +2
α
2α +1
2α +2
2α
2
2
0
1
α +2
α
α +1
2α +2
2α
2α +1
α
α
α +1
α +2
2α
2α +1
2α +2
0
1
2
α +1
α +1
α +2
α
2α +1
2α +2
2α
1
2
0
α +2
α +2
α
α +1
2α +2
2α
2α +1
2
0
1
2α
2α
2α +1
2α +2
0
1
2
α
α +1
α +2
2α +1
2α +1
2α +2
2α
1
2
0
α +1
α +2
α
2α +2
2α +2
2α
2α +1
2
0
1
α +2
α
α +1
·
0
1
2
α
α +1
α +2
2α
2α +1
2α +2
0
0
0
0
0
0
0
0
0
0
1
0
1
2
α
α +1
α +2
2α
2α +1
2α +2
2
0
2
1
2α
2α +2
2α +1
α
α +2
α +1
α
0
α
2α
2
α +2
2α +2
1
α +1
2α +1
α +1
0
α +1
2α +2
α +2
2α
1
2α +1
2
α
α +2
0
α +2
2α +1
2α +2
1
α
α +1
2α
2
2α
0
2α
α
1
2α +1
α +1
2
2α +2
α +2
2α +1
0
2α +1
α +2
α +1
2
2α
2α +2
α
1
2α +2
0
2α +2
α +1
2α +1
α
2
α +2
1
2α
11.5. x3 + x + 4.
11.7. Impossible.
11.9. x2 + 2.
11.11. x4 −16x2 + 16.
11.13. 8x6 −9.
11.17. 3.

318
ANSWERS TO THE ODD-NUMBERED EXERCISES
11.19. 2.
11.21. 2.
11.23. ∞.
11.25. ∞.
11.27. (1 −
3√
2 +
3√
4)/3.
11.29. −(1 + 6ω)/31.
11.31. α4 + α.
11.33. 2; not a ﬁeld.
11.35. 7; a ﬁeld
11.37. 0; a ﬁeld.
11.39. 0; not a ﬁeld.
11.43. m = 2, 4, pr or 2pr, where p is an odd prime; see Weiss [12, Th. 4–6–10].
11.47. α =
√
2 +
√
−3.
11.49. All elements of GF(32) except 0 and 1 are primitive.
11.51. x3 + x + 1.
11.57. No solutions.
11.59. x = 1 or α + 1.
11.63. 5.
11.65. The output has cycle length 7 and repeats the sequence 1101001, starting
at the right.
CHAPTER 12
12.1.
a
b
c
d
e
f
g
b
c
d
e
f
g
a
c
d
e
f
g
a
b
d
e
f
g
a
b
c
e
f
g
a
b
c
d
f
g
a
b
c
d
e
g
a
b
c
d
e
f
12.3. Use GF(8) = {0, 1, α, 1 + α, α2, 1 + α2, α + α2, 1 + α + α2} where
α3 = α + 1.
12.7. No.
12.9.
Week
1
2
3
4
↓
↓
↓
↓
→
A
B
C
D
Shelf
→
B
C
D
A
height
→
C
D
A
B
→
D
A
B
C
A, B, C, and D are the four brands of cereal.

ANSWERS TO THE ODD-NUMBERED EXERCISES
319
12.11.
Week
1
2
3
4
5
↓
↓
↓
↓
↓
M
→
A
B
C
D
0
T
→
B
C
D
0
A
W
→
C
D
0
A
B
T
→
D
0
A
B
C
F
→
0
A
B
C
D
A, B, C, and D are the four different types of music, and 0 refers to
no music.
12.15. y = αx.
12.17. y = (α + 2)x + 2α.
12.21. 1.
12.23.
1
6
11
16
12
15
2
5
14
9
8
3
7
4
13
10
1
6
11
16
15
12
5
2
8
3
14
9
10
13
4
7
12.25.
1
10
19
28
37
46
55
64
35
44
49
58
7
16
21
30
29
22
15
8
57
50
43
36
63
56
45
38
27
20
9
2
52
59
34
41
24
31
6
13
18
25
4
11
54
61
40
47
48
39
62
53
12
3
26
17
14
5
32
23
42
33
60
51
CHAPTER 13
13.1. Constructible.
13.3. Constructible.
13.5. Not constructible.
13.7. No.
13.11. Yes.
13.13. No.
13.15. Yes; π
21 = 1
4
π
3 −π
7

.
13.23. Yes.
13.25. No.
13.27. No.
13.29. Yes.
13.31. No.
13.33. Yes.
CHAPTER 14
14.1. 010, 001, 111.

320
ANSWERS TO THE ODD-NUMBERED EXERCISES
14.3. The checking is done modulo 9, using the fact that any integer is congruent
to the sum of its digits modulo 9.
14.5. (1 2 3 4 5 6 7 8 9 10) modulo 11. It will detect one error but not
correct any.
14.7. 000000, 110001, 111010, 001011, 101100, 011101, 010110, 100111.
14.9. 101, 001, 100.
14.11. Minimum distance = 3. It detects two errors and corrects one error.
H =


1
0
0
0
0
0
0
1
0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
1
0
1
1
0
0
0
1
0
0
1
0
0
0
0
0
0
1
1
0
0
1

.
14.13. GT =
 1
1
1
1 
, H =


1
0
0
1
0
1
0
1
0
0
1
1

.
14.15. GT =


1
0
1
0
0
1
0
0
0
0
1
0
1
0
0
1
0
0
0
0
1
0
1
0
0
1
0
1
0
1
1
0
0
0
0
1

,
H =


1
0
0
0
0
1
0
0
1
0
1
0
0
0
0
1
0
0
0
0
1
0
0
1
0
1
1
0
0
0
1
0
0
1
0
1
0
0
0
0
1
0
0
1
0

.
14.17. 110, 010, 101, 001, 011.
14.19. Syndrome
Coset Leader
00
000
01
010
10
100
11
001

ANSWERS TO THE ODD-NUMBERED EXERCISES
321
14.21.
Coset
Coset
Coset
Syndrome
Leader
Syndrome
Leader
Syndrome
Leader
00000
000000000
01011
000010100
10110
000000001
00001
000010000
01100
011000000
10111
000010001
00010
000100000
01101
010000010
11000
110000000
00011
000110000
01110
001000100
11001
000000111
00100
001000000
01111
000000110
11010
100000100
00101
000000010
10000
100000000
11011
000001110
00110
001100000
10001
000001010
11100
000000101
00111
000100010
10010
100100000
11101
000010101
01000
010000000
10011
000000011
11110
000001100
01001
010010000
10100
000001000
11111
000011100
01010
000000100
10101
000011000
14.23. (a) 56; (b) 7; (c) 27 = 128, (d) 8/9; (e) it will detect single, double, triple,
and any odd number of errors; (f) 1.
14.25. No.
14.29. x8 + x4 + x2 + x + 1 and x10 + x9 + x8 + x6 + x5 + x2 + 1.
14.31. x5 + x4 + x3 + x2 + 1.

INDEX
Abel, N. H., 1, 47, 48
Abelian group, 48
ﬁnite, 92
Absorption laws, 15
Abstract algebra, 2
Action of a group, 96
Adder, full, 35
half, 34
modulo 2, 28
serial, 152
Addition modulo m, 84
Adjoining an element, 220
Afﬁne plane, 242
Algebra, abstract, 2
boolean, 7, 14, 25
classical, 1
modern, 2
Algebraic numbers, 221, 233
Algebraic structure, 4
Algebra of sets, 7
Alternating group, 70, 81, 85, 88
AND gate, 36
Angle trisection, 251, 257
Antisymmetry, 23
Archimedean solids, 121
Archimedes, 258
Associativity, 3, 14, 48, 137, 155
Atom, 26
Automorphism, 75, 151
Frobenius, 235
Axiomatic method, 295
Axioms, 295
Axis of symmetry, 51
BCH code, 284
Benzene, 126
Biconditional, 18
Bijective, 50, 63
Binary code, 266
Binary operation, 2
Binary symmetric channel, 266
Binomial theorem, 178
coefﬁcients, 129
Boole, G., 7
Boolean algebra, 7, 14, 25
isomorphism, 39
morphism, 39
representation theorem, 39
Boolean expression, 26
Boolean ring, 158, 176
Bose, R. C., 242, 284
Bridge circuit, 43
Burnside, W., 125
Burnside’s theorem (lemma), 125
Cancellation, 53
Cantor’s theorem, 41
Carrol, L., 41
Casting out nines, 288
Cauchy sequences, 6
Cayley, A., 47, 71
Cayley’s theorem, 71
CD, 264
Center of a group, 74
Characteristic of a ring, 226
Check digit, 266
Chinese remainder theorem, 198
Circle group, 88, 98
Circle squaring, 251, 259
Circuit, see Switching circuits
Classical algebra, 1
Closed switch, 19
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.
323

324
INDEX
Closure of an operation, 3, 48
Code, 264
BCH, 284
binary, 266
cyclic, 292
error-correcting, 265
error-detecting, 264
linear, 276
(n, k)-, 266
parity check, 267, 274
polynomial, 270
repeating, 268
Code rate, 267
Code word, 266
Coefﬁcient, 166
Colorings, 128
Common divisor, 184, 299
multiple, 184, 303
Commutativity, 3, 14, 48, 148, 167
Commutator, 101
Complement, 8, 14
Complex numbers, 4, 223
Complex roots, 191
Composition of relations, 150
of functions, 49
of relations, 150
Conclusion, 293
Concatenation, 140
Conditional, 17
Cone, n-agonal, 116
Congruence, 77, 79, 145
linear, 197
Congruence class, 77, 145
Conjugate, 191
Conjunctive normal form, 45
Constant polynomial, 167
Constructible number, 252
Constructible point, 252
Construction of polygons, 259
Contradiction, 17
proof by, 294
Converse, 295
Convolution fraction, 175
Convolution of functions, 173
of sequences, 168
Coset, 79, 82
Coset leader, 280
Countable, 221
Counterexample, 295
Cross-ratio, 101
Crystalline lattice, 120
Crystallographic group, 120
Cube, duplication of, 251, 256
Cube, rotation group of, 114
Cycle, 64
disjoint, 66
Cyclic code, 291
Cyclic group, 56
Cyclic monoid, 139, 150
Cyclic subgroup, 57
Cyclotomic polynomial, 195
Cylinder, n-agonal, 116
Da Vinci, L., 109
Decode, 280
Dedekind cuts, 6
Degree, of an extension, 219
of a polynomial, 166
Delta function, 172
De Morgan’s laws, 15
Derivative, 292
Detecting errors, 280
Determinant, 110, 112, 287
Diagram, state, 143
tree, 148
Venn, 8, 32
Difference, 9
symmetric, 9
Dihedral group, 58, 90
Dirac delta function, 172
Direct product, 2, 91, 164
Direct sum, 91
Disjoint cycles, 66
Disjunctive normal form, 30
Distributions, 175
Distributivity, 4, 14, 156
Division, 184, 299
Division algorithm, 180, 181, 298
Divisor of zero, 159
Dodecahedron, 112
rotation group, 114
Domain, 172
left Ore, 172
Domain, integral, 159
Duality in boolean algebras, 15
Duality of regular solids, 112
Duplication of the cube, 251, 256
D¨urer, A., 246
DVD, 264
Eigenvalue, 108, 109, 111
Eigenvector, 108, 110
Eisenstein’s criterion, 194
Element of a set, 7
Empty set, 7
Encode, 266, 280
Encoding matrix, 276
Endomorphism ring, 178

INDEX
325
Equivalence class, 77
Equivalence, logical, 295
Equivalence relation, 5, 77
Equivalent circuits, 20
Error-correcting code, 264
Error-detecting code, 264
Error polynomial, 275
Euclidean algorithm, 185, 300
Euclidean group, 104
Euclidean ring, 181
Euclid’s lemma, 302
Euclid’s theorem, 298
Euler, L., 103, 236, 242, 260
Euler φ-function, 103
Even permutation, 68
Exclusive OR, 10, 29
Extension ﬁeld, 218
degree of, 219
ﬁnite, 219
Factor, 184, see also Quotient
Factor theorem, 182
Faithful action of a group, 96
Faithful representation, 109
Feedback shift register, 231, 272
Fermat, P., 103, 260
Fermat primes, 260
Fermat’s last theorem, 305
Field, 4, 160
ﬁnite, 6, 45, 225
Galois, 6, 227
primitive element in, 229
skew, 172
Field extension, 218
Field of, convolution fractions, 175
fractions, 170
quotients, 170
rational functions, 172, 221
Fifteen puzzle, 74
Finite, abelian group, 92
extension, 219
ﬁeld, 6, 45, 225
geometry, 242
group, 56
groups in three dimensions, 116
groups in two dimensions, 109
Finite-state machine, 142
First isomorphism theorem, 87, 210
Fixed points, 125
Flip-ﬂop, 46
Formal power series, 169
Fractional differentiation and integration, 175
Fractions, ﬁeld of, 170
left, 172
Free, group, 75
monoid, 140
semigroup, 140
Full adder, 35
Frobenius automorphism, 235
Function, bijective, 50, 63
composition of, 49
delta, 172
generalized, 175
Heaviside, 175
impulse, 172
injective, 49
inverse, 49
one-to-one, 49
onto, 50
surjective, 49
transition, 142
Fundamental theorem of, algebra, 6, 190
arithmetic, 187
Galois, ´E., 6, 47, 225
Galois ﬁeld, 6, 227
Galois theory, 47,
Gate, 36
Gauss, C. F., 6, 190, 260
Gaussian integers, 72, 183
Gauss’ lemma, 193
Generalized function, 175
General linear group, 107
Generator, group, 56
idempotent, 292
matrix, 276
of a monoid, 139
polynomial, 270
Geometry, 242
Goldbach conjecture, 304
Graeco-Latin square, see Orthogonal latin
squares
Greatest common divisor, 21, 184, 299
Greatest lower bound, 25
Group, 48
abelian, 48
alternating, 70, 82, 86, 88
automorphism, 75
center, 74
circle, 88, 98
commutative, 48
crystallographic, 120
cyclic, 56
dihedral, 58, 90
euclidean, 104
factor, 83
ﬁnite, 56
free, 75
general linear, 107

326
INDEX
Group (continued)
generator, 56
icosahedral, 115
inﬁnite, 56
isomorphism, 60
Klein, 52, 90
matrix, 107
metabelian, 102
morphism, 60
noncommutative, 59
octahedral, 114
of a polynomial, 80
of a rectangle, 53, 55
of a square, 89
of low order, 94
of prime order, 80
order, 56
orthogonal, 105
permutation, 50
quaternion, 73, 95, 107
quotient, 83
simple, 86
special orthogonal, 98, 108
special unitary, 108
sporadic, 86
symmetric, 50, 63
symmetries, 51
tetrahedral, 113
translation, 49, 104
trivial, 49
unitary, 108
Group acting on a set, 96
Group isomorphism, 60
Group morphism, 60
Half adder, 34
Hamming distance, 268
Heaviside, O., 172, 175
Homomorphism, see Morphism
Hypothesis, 293
Icosahedral group, 115
Icosahedron, 112
rotation group, 114
Ideal, 204
left, 217
maximal, 216
prime, 216
principal, 204
Ideal crystalline lattice, 120
Idempotent element, 179
generator, 292
laws, 15
Identity, 4, 48, 137
function, 49
If and only if, 18, 295
Image, 87
Implication, 17, 293
Improper rotation, 52, 58, 108
Impulse functions, 172
Inclusion, 7
Index of a subgroup, 80
Induction, 296
extended, 297
strong, 297
Inﬁnite group, 56
Information rate, 267
Injective, 49, 86
Input values, 142
Integers, 5, 156, 296
gaussian, 72, 183
Integers modulo m, 78
Integral domain, 159
Interlacing shufﬂe, 74
International standard book number, 289
Intersection of sets, 8
Inverse, 3, 48, 188
function, 49
Inversion theorem, 50
Invertible element, 3, 48, 188
Irrational roots, 192
Irreducible, 189
polynomial, 190
Isometry, 51, 112
Isomorphism, 5
boolean algebra, 39
group, 60
monoid, 141
ring, 162
Isomorphism theorems for groups, 87, 101,
102
Isomorphism theorems for rings, 210, 215
Join, 14
Juxtaposition, 140
Kernel, 86
Klein, F., 52
Klein 4-group, 52, 90
Kronecker, L., 5
Lagrange, J., 79
Lagrange’s theorem, 80
Latin square, 236
orthogonal, 239
Lattice, 25
crystalline, 120

INDEX
327
Least common multiple, 21, 184, 303
Least upper bound, 25
Left coset, 82
Left ideal, 217
Linear code, 276
cyclic, 291
Linear congruences, 197,
Linear transformation, 5, 104
Lines in a geometry, 242
Length of a vector, 105
Local ring, 216
Logically equivalent, 17, 295
Logic of propositions, 16
Machine, 142
monoid of, 145
parity checker, 143
semigroup of, 142
Magic square, 247
Mathematical theory, 295
Matrix
eigenvalue of, 108
eigenvector of, 108
encoding, 276
generator, 276
nonsingular, 4
orthogonal, 105
parity check, 278
unitary, 108
Matrix group, 107
Matrix representation, 109
of codes, 276
Matrix ring, 166
Maximal ideal, 216
Meet, 14
Metabelian group, 102
Metric, 291
Mikusinski, J., 173
Modern algebra, 2
Modular representation, 200
Modulo m, 78
Modulo 2 adder, 28
Monic polynomial, 234
Monoid, 137
automorphism, 151
cyclic, 139, 150
free, 140
generator, 139
morphism, 141
morphism theorem, 150
quotient, 145
representation theorem, 150
transformations, 138
Monoid isomorphism, 141
Monoid morphism, 141
Monoid of a machine, 145
Monoid of transformations, 138
Morphism, 5
boolean algebra, 39
group, 60
monoid, 141
ring, 172
Morphism theorem for, groups, 87, 101, 102
monoids, 150
rings, 210
Mutually orthogonal squares, 239
NAND gate, 28, 36
Necklace problems, 126
Network, see Switching circuits
Nilpotent element, 216
Nonsingular matrix, 4
NOR gate, 28, 36
Normal form, conjunctive, 45
disjunctive, 30
Normal subgroup, 82
NOT gate, 36
Octahedral group, 114
Octahedron, 112
rotation group, 114
Odd permutation, 68
One-to-one, 49
One-to-one correspondence, 50
Onto, 50
Open switch, 19
Operation, 2
Operational calculus, 172
Operator, 175
Orbit, 65, 78, 97
Order of a group, 56
Order of an element, 56
OR gate, 36
Orthogonal group, 105
Orthogonal latin squares, 239
Orthogonal matrix, 105
Output, 142
Parallel circuit, 19
Parallelism, 242
Parity check, 278
code, 267, 274
machine, 143
matrix, 278
Parity of a permutation, 69
Partial order, 24
Partition, 77
Pattern recognition, 147

328
INDEX
Permutation, 50, 63
even, 68
odd, 68
parity, 69
Permutation group, 50, 63
Phi function, 103
Plane, afﬁne, 242
projective, 245
Points of a geometry, 242
Pole of a rotation, 116
P´olya, G., 125, 134
P´olya-Burnside enumeration, 124
Polygons, construction, 259
Polyhedron, dual, 112
Polynomial, 166
coefﬁcients, 166
constant, 167
cyclotomic, 195
degree, 166
equality, 166
group of symmetries, 74
irreducible, 189, 190
monic, 234
primitive, 231, 275
reducible, 190
zero, 166
Polynomial code, 270
Polynomial equations, 1, 47
Polynomial representation of codes, 270
Polynomial ring, 167
Poset, 24
Positive integers, 3
Power series, 169
Power set, 8
Prime, 189, 294, 297
factorization theorem, 21, 302
Fermat, 260
ideal, 216
Prime order group, 80
Primitive element, 229
Primitive polynomial, 231, 275
Principal ideal, 204
Principal ideal ring, 205
Product group, 91
Product ring, 164
Projective plane, 245
Projective space, 102
Proof, 293
by contradiction, 294
by reduction to cases, 293
direct, 293
Proper rotation, 52, 58, 108
symmetry, 52
Propositional calculus, 18
Quaternion, group, 73, 95, 107
ring, 172, 177
Quotient, 181, 299
ﬁeld of, 170
group, 83
monoid, 145
ring, 206
set, 72
structure, 5
Radical of a ring, 216
Rational functions, 172, 221
Rational number, 6, 48, 78, 170
Rational roots theorem, 192
Real numbers, 6, 48, 156
Real numbers modulo one, 88
Real projective space, 102
Rectangle, symmetries of, 53, 55
Reducible, 190
Redundant digits, 267
Reﬂexivity, 23, 77
Register, shift, 231, 272
Regular n-gon, rotations, 58
Regular polygons, construction, 259
Regular solid, 112
Relation, 76
composition of, 150
congruence, 77, 145
equivalence, 77
partial order, 24
Relatively prime, 301
Remainder, 181, 299
Remainder theorem, 182
Repeating code, 268
Representation, faithful, 109
matrix, 109
modular, 200
residue, 200
Representation theorem for, boolean
algebras, 39, 40
groups, 71
monoids, 150
Representative of a coset, 79
Residue representation, 200
Right coset, 79
Ring, 155
boolean, 158, 176
characteristic, 226
commutative, 156
endomorphism, 178
euclidean, 181
factor, 206
ﬁeld, 160
ideal of, 204

INDEX
329
integral domain, 159
local, 216
matrix, 166
morphism, 172
morphism theorem, 210
nontrivial, 159
polynomial, 166
principal ideal, 205
product, 164
quotient, 206
radical of, 216
simple, 217
subring of, 161
trivial, 159
Ring isomorphism, 162
Ring morphism, 162, 210
Ring of, formal power series, 169
matrices, 166
polynomials, 166
quaternions, 172, 177
sequences, 168
Roots, 183
complex, 190, 191
irrational, 191
rational, 192
Rotations, 52, 58, 108
Rotations of, a cube, 114
a dodecahedron, 114
an icosahedron, 114
an n-gon, 58
an octahedron, 114
a tetrahedron, 113
Ruler and compass, 251
Second isomorphism theorem, 102, 215
Semigroup, 137
free, 140
Semigroup of a machine, 145
Sequences, ring of, 168
Serial adder, 152
Series, power, 169
Series circuit, 19
Series-parallel circuit, 20
Set, 7
Sets, algebra of, 7
Shannon, C. E., 267
Shift register, 231, 272
Shufﬂe, interlacing, 74
Simple, group, 86
ring, 217
Simpliﬁcation of circuits, 26
Skew ﬁeld, 172
Smallest subﬁeld, 220
Solids, Archimedean, 121
Solids, regular, 112
Space, projective, 102
Special orthogonal group, 98, 108
Special unitary group, 108
Sphere, 116
Sporadic groups, 86
Square, latin, 236
magic, 247
orthogonal latin, 239
Square-free integer, 22, 302
Squaring the circle, 251, 259
Stabilizer, 97
Standard basis, 104
Standard matrix, 105, 164
State, 142
diagram, 143
Step function, 175
Stone’s representation theorem, 40
Straight-edge and compass constructions, 251
Structure, algebraic, 4
Structure, quotient, 5
Subﬁeld, 218
smallest, 220
Subgroup, 54
commutator, 101
cyclic, 57
index of, 80
normal, 92
Submonoid, 150
Subring, 161
Subset, 7
Substructure, 4
Sum, direct, 91
Surjective, 49
Switch, 19
Switching circuits, 20
bridge, 43
number of, 130
n-variable, 28
series-parallel, 28
Switching function, 27
Sylow theorems, 85
Symmetric condition, 23, 77
Symmetric difference, 9, 28
Symmetric group, 50, 63
Symmetries of a
ﬁgure, 3, 51
polynomial, 74
rectangle, 53, 55
set, 50
square, 89
Symmetry, proper, 52
Syndrome, 280

330
INDEX
Table, 3
truth, 16
Tarry, G., 242
Tautology, 17
Tetrahedral group, 113
Tetrahedron, 112
rotation group, 113
Third isomorphism theorem, 102, 215
Titchmarsh’s theorem, 174
Transcendental, 221
Transformation monoid, 138
Transient conditions in circuits, 49
Transistor gates, 36
Transition function, 142
Transitivity, 23, 77
Translation, 49, 104
Transpose of a matrix, 104
Transposition, 68
Tree diagram, 148
Trisection of an angle, 251, 257
Trivial group, 49
Trivial ring, 159
Truth table, 16
Unary operation, 2, 8, 14
Underlying set, 4
Union of sets, 8
Unique factorization theorem, 189
Unitary group, 108
Unit element, 188. See also Identity;
Invertible element
Unity, see Identity
Universal bounds, 25
Vandermonde determinant, 287
Vector space, 5
Venn diagram, 8, 32
Well ordering axiom, 296
Wilson’s theorem, 202
Words, 140
Zero, 14, 155
Zero divisor, 159
Zero polynomial, 166

PURE AND APPLIED MATHEMATICS
A Wiley-Interscience Series of Texts, Monographs, and Tracts
Founded by RICHARD COURANT
Editors: MYRON B. ALLEN III, DAVID A. COX, PETER LAX
Editors Emeriti: PETER HILTON, HARRY HOCHSTADT, JOHN TOLAND
AD ´AMEK, HERRLICH, and STRECKER—Abstract and Concrete Categories
ADAMOWICZ and ZBIERSKI—Logic of Mathematics
AINSWORTH and ODEN—A Posteriori Error Estimation in Finite Element Analysis
AKIVIS and GOLDBERG—Conformal Differential Geometry and Its Generalizations
ALLEN and ISAACSON—Numerical Analysis for Applied Science
∗ARTIN—Geometric Algebra
AUBIN—Applied Functional Analysis, Second Edition
AZIZOV and IOKHVIDOV—Linear Operators in Spaces with an Indeﬁnite Metric
BERG—The Fourier-Analytic Proof of Quadratic Reciprocity
BERMAN, NEUMANN, and STERN—Nonnegative Matrices in Dynamic Systems
BERKOVITZ—Convexity and Optimization in Rn
BOYARINTSEV—Methods of Solving Singular Systems of Ordinary Differential
Equations
BURK—Lebesgue Measure and Integration: An Introduction
∗CARTER—Finite Groups of Lie Type
CASTILLO, COBO, JUBETE, and PRUNEDA—Orthogonal Sets and Polar Methods in
Linear Algebra: Applications to Matrix Calculations, Systems of Equations,
Inequalities, and Linear Programming
CASTILLO, CONEJO, PEDREGAL, GARCI ´A, and ALGUACIL—Building and Solving
Mathematical Programming Models in Engineering and Science
CHATELIN—Eigenvalues of Matrices
CLARK—Mathematical Bioeconomics: The Optimal Management of Renewable
Resources, Second Edition
†COX—Primes of the Form x2 + ny2: Fermat, Class Field Theory, and Complex
Multiplication
∗CURTIS and REINER—Representation Theory of Finite Groups and Associative
Algebras
∗CURTIS and REINER—Methods of Representation Theory: With Applications to Finite
Groups and Orders, Volume I
CURTIS and REINER—Methods of Representation Theory: With Applications to Finite
Groups and Orders, Volume II
DINCULEANU—Vector Integration and Stochastic Integration in Banach Spaces
∗DUNFORD and SCHWARTZ—Linear Operators
Part 1—General Theory
Part 2—Spectral Theory, Self Adjoint Operators in
Hilbert Space
Part 3—Spectral Operators
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in paperback.
Modern Algebra with Applications, Second Edition, by William J. Gilbert and W. Keith Nicholson
ISBN 0-471-41451-4
Copyright 2004 John Wiley & Sons, Inc.

FARINA and RINALDI—Positive Linear Systems: Theory and Applications
FOLLAND—Real Analysis: Modern Techniques and Their Applications
FR ¨OLICHER and KRIEGL—Linear Spaces and Differentiation Theory
GARDINER—Teichm¨uller Theory and Quadratic Differentials
GILBERT and NICHOLSON—Modern Algebra with Applications, Second Edition
GREENE and KRANTZ—Function Theory of One Complex Variable
∗GRIFFITHS and HARRIS—Principles of Algebraic Geometry
GRILLET—Algebra
GROVE—Groups and Characters
GUSTAFSSON, KREISS and OLIGER—Time Dependent Problems and Difference
Methods
HANNA and ROWLAND—Fourier Series, Transforms, and Boundary Value Problems,
Second Edition
∗HENRICI—Applied and Computational Complex Analysis
Volume 1, Power Series—Integration—Conformal Mapping—Location
of Zeros
Volume 2, Special Functions—Integral Transforms—Asymptotics—
Continued Fractions
Volume 3, Discrete Fourier Analysis, Cauchy Integrals, Construction
of Conformal Maps, Univalent Functions
∗HILTON and WU—A Course in Modern Algebra
∗HOCHSTADT—Integral Equations
JOST—Two-Dimensional Geometric Variational Procedures
KHAMSI and KIRK—An Introduction to Metric Spaces and Fixed Point Theory
∗KOBAYASHI and NOMIZU—Foundations of Differential Geometry, Volume I
∗KOBAYASHI and NOMIZU—Foundations of Differential Geometry, Volume II
KOSHY—Fibonacci and Lucas Numbers with Applications
LAX—Functional Analysis
LAX—Linear Algebra
LOGAN—An Introduction to Nonlinear Partial Differential Equations
McCONNELL and ROBSON—Noncommutative Noetherian Rings
MORRISON—Functional Analysis: An Introduction to Banach Space Theory
NAYFEH—Perturbation Methods
NAYFEH and MOOK—Nonlinear Oscillations
PANDEY—The Hilbert Transform of Schwartz Distributions and Applications
PETKOV—Geometry of Reﬂecting Rays and Inverse Spectral Problems
∗PRENTER—Splines and Variational Methods
RAO—Measure Theory and Integration
RASSIAS and SIMSA—Finite Sums Decompositions in Mathematical Analysis
RENELT—Elliptic Systems and Quasiconformal Mappings
RIVLIN—Chebyshev Polynomials: From Approximation Theory to Algebra and Number
Theory, Second Edition
ROCKAFELLAR—Network Flows and Monotropic Optimization
ROITMAN—Introduction to Modern Set Theory
∗RUDIN—Fourier Analysis on Groups
SENDOV—The Averaged Moduli of Smoothness: Applications in Numerical Methods
and Approximations
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
†Now available in paperback.

SENDOV and POPOV—The Averaged Moduli of Smoothness
∗SIEGEL—Topics in Complex Function Theory
Volume 1—Elliptic Functions and Uniformization Theory
Volume 2—Automorphic Functions and Abelian Integrals
Volume 3—Abelian Functions and Modular Functions of Several Variables
SMITH and ROMANOWSKA—Post-Modern Algebra
STAKGOLD—Green’s Functions and Boundary Value Problems, Second Edition
∗STOKER—Differential Geometry
∗STOKER—Nonlinear Vibrations in Mechanical and Electrical Systems
∗STOKER—Water Waves: The Mathematical Theory with Applications
WATKINS—Fundamentals of Matrix Computations, Second Edition
WESSELING—An Introduction to Multigrid Methods
†WHITHAM—Linear and Nonlinear Waves
†ZAUDERER—Partial Differential Equations of Applied Mathematics, Second Edition
∗Now available in a lower priced paperback edition in the Wiley Classics Library.
† Now available in paperback.


c
⃝2000 by Chapman & Hall/CRC
CRC
standard 
probability
and
Statistics tables 
and formulae

CHAPMAN & HALL/CRC
DANIEL ZWILLINGER
Rensselaer Polytechnic Institute 
Troy, New York 
STEPHEN KOKOSKA
Bloomsburg University 
Bloomsburg, Pennsylvania
Boca Raton   London   New York   Washington, D.C.
standard 
probability
and
Statistics tables 
and formulae
CRC

This book contains information obtained from authentic and highly regarded sources. Reprinted material
is quoted with permission, and sources are indicated. A wide variety of references are listed. Reasonable
efforts have been made to publish reliable data and information, but the author and the publisher cannot
assume responsibility for the validity of all materials or for the consequences of their use.
Neither this book nor any part may be reproduced or transmitted in any form or by any means, electronic
or mechanical, including photocopying, microﬁlming, and recording, or by any information storage or
retrieval system, without prior permission in writing from the publisher.
The consent of CRC Press LLC does not extend to copying for general distribution, for promotion, for
creating new works, or for resale. Speciﬁc permission must be obtained in writing from CRC Press LLC
for such copying.
Direct all inquiries to CRC Press LLC, 2000 N.W. Corporate Blvd., Boca Raton, Florida 33431. 
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are
used only for identiﬁcation and explanation, without intent to infringe.
Visit the CRC Press Web site at www.crcpress.com
© 2000 by Chapman & Hall/CRC  
No claim to original U.S. Government works
International Standard Book Number 1-58488-059-7
Library of Congress Card Number 99-045786
Printed in the United States of America          2  3  4  5  6  7  8  9  0
Printed on acid-free paper
Library of Congress Cataloging-in-Publication Data
Zwillinger, Daniel, 1957-
CRC standard probability and statistics tables and formulae / Daniel Zwillinger, Stephen Kokoska.
p. cm.
Includes bibliographical references and index.
ISBN 1-58488-059-7 (alk. paper)
1. Probabilities—Tables. 2. Mathematical statistics—Tables. I. Kokoska, Stephen.
II. Title.
QA273.3 .Z95 1999
519.2′02′1—dc21
99-045786
 
 

Preface
It has long been the established policy of CRC Press to publish, in handbook
form, the most up-to-date, authoritative, logically arranged, and readily us-
able reference material available. This book ﬁlls the need in probability and
statistics.
Prior to the preparation of this book the contents of similar books were con-
sidered. It is easy to ﬁll a statistics reference book with many hundred pages
of tables—indeed, some large books contain statistical tables for only a single
test. The authors of this book focused on the basic principles of statistics.
We have tried to ensure that each topic had an understandable textual in-
troduction as well as easily understood examples. There are more than 80
examples; they usually follow the same format: start with a word problem,
interpret the words as a statistical problem, ﬁnd the solution, interpret the
solution in words.
We have organized this reference in an eﬃcient and useful format. We believe
both students and researchers will ﬁnd this reference easy to read and under-
stand. Material is presented in a multi-sectional format, with each section
containing a valuable collection of fundamental reference material—tabular
and expository. This Handbook serves as a guide for determining appropriate
statistical procedures and interpretation of results. We have assembled the
most important concepts in probability and statistics, as experienced through
our own teaching, research, and work in industry.
For most topics, concise yet useful tables were created. In most cases, the
tables were re-generated and veriﬁed against existing tables. Even very mod-
est statistical software can generate many of the tables in the book—often to
more decimal places and for more values of the parameters. The values in
this book are designed to illustrate the range of possible values and act as a
handy reference for the most commonly needed values.
This book also contains many useful topics from more advanced areas of statis-
tics, but these topics have fewer examples. Also included are a large collection
of short topics containing many classical results and puzzles. Finally, a section
on notation used in the book and a comprehensive index are also included.
c
⃝2000 by Chapman & Hall/CRC

In line with the established policy of CRC Press, this Handbook will be kept
as current and timely as is possible. Revisions and anticipated uses of newer
materials and tables will be introduced as the need arises. Suggestions for the
inclusion of new material in subsequent editions and comments concerning
the accuracy of stated information are welcomed.
If any errata are discovered for this book, they will be posted to
http://vesta.bloomu.edu/~skokoska/prast/errata.
Many people have helped in the preparation of this manuscript. The authors
are especially grateful to our families who have remained lighthearted and
cheerful throughout the process. A special thanks to Janet and Kent, and to
Joan, Mark, and Jen.
Daniel Zwillinger
zwillinger@alum.mit.edu
Stephen Kokoska
skokoska@planetx.bloomu.edu
ACKNOWLEDGMENTS
Plans 6.1–6.6, 6A.1–6A.6, and 13.1–13.5 (appearing on pages 331–337) originally appeared
on pages 234–237, 276–279, and 522–523 of W. G. Cochran and G. M. Cox, Experimental
Designs, Second Edition, John Wiley & Sons, Inc, New York, 1957. Reprinted by permission
of John Wiley & Sons, Inc.
The tables of Bartlett’s critical values (in section 10.6.2) are from D. D. Dyer and J. P.
Keating, “On the Determination of Critical Values for Bartlett’s Test”, JASA, Volume 75,
1980, pages 313–319. Reprinted with permission from the Journal of American Statistical
Association. Copyright 1980 by the American Statistical Association. All rights reserved.
The tables of Cochran’s critical values (in section 10.7.1) are from C. Eisenhart, M. W.
Hastay, and W. A. Wallis, Techniques of Statistical Analysis, McGraw-Hill Book Com-
pany, 1947, Tables 15.1 and 15.2 (pages 390-391). Reprinted courtesy of The McGraw-Hill
Companies.
The tables of Dunnett’s critical values (in section 12.1.4.5) are from C. W. Dunnett, “A
Multiple Comparison Procedure for Comparing Several Treatments with a Control”, JASA,
Volume 50, 1955, pages 1096–1121. Reprinted with permission from the Journal of Amer-
ican Statistical Association. Copyright 1980 by the American Statistical Association. All
rights reserved.
The tables of Duncan’s critical values (in section 12.1.4.3) are from L. Hunter, “Critical
Values for Duncan’s New Multiple Range Test”, Biometrics, 1960, Volume 16, pages 671–
685.
Reprinted with permission from the Journal of American Statistical Association.
Copyright 1960 by the American Statistical Association. All rights reserved.
Table 15.1 is reproduced, by permission, from ASTM Manual on Quality Control of Mate-
rials, American Society for Testing and Materials, Philadelphia, PA, 1951.
The table in section 15.1.2 and much of Chapter 18 originally appeared in D. Zwillinger,
Standard Mathematical Tables and Formulae, 30th edition, CRC Press, Boca Raton, FL,
1995. Reprinted courtesy of CRC Press, LLC.
Much of section 17.17 is taken from the URL http://members.aol.com/johnp71/javastat.html
Permission courtesy of John C. Pezzullo.
c
⃝2000 by Chapman & Hall/CRC

Contents
1
Introduction
1.1
Background
1.2
Data sets
1.3
References
2
Summarizing Data
2.1
Tabular and graphical procedures
2.2
Numerical summary measures
3
Probability
3.1
Algebra of sets
3.2
Combinatorial methods
3.3
Probability
3.4
Random variables
3.5
Mathematical expectation
3.6
Multivariate distributions
3.7
Inequalities
4
Functions of Random Variables
4.1
Finding the probability distribution
4.2
Sums of random variables
4.3
Sampling distributions
4.4
Finite population
4.5
Theorems
4.6
Order statistics
4.7
Range and studentized range
© 2000 by Chapman & Hall/CRC

5
Discrete Probability Distributions
5.1
Bernoulli distribution
5.2
Beta binomial distribution
5.3
Beta Pascal distribution
5.4
Binomial distribution
5.5
Geometric distribution
5.6
Hypergeometric distribution
5.7
Multinomial distribution
5.8
Negative binomial distribution
5.9
Poisson distribution
5.10
Rectangular (discrete uniform) distribution
6
Continuous Probability Distributions
6.1
Arcsin distribution
6.2
Beta distribution
6.3
Cauchy distribution
6.4
Chi–square distribution
6.5
Erlang distribution
6.6
Exponential distribution
6.7
Extreme–value distribution
6.8
F distribution
6.9
Gamma distribution
6.10
Half–normal distribution
6.11
Inverse Gaussian (Wald) distribution
6.12
Laplace distribution
6.13
Logistic distribution
6.14
Lognormal distribution
6.15
Noncentral chi–square distribution
6.16
Noncentral F distribution
6.17
Noncentral t distribution
6.18
Normal distribution
6.19
Normal distribution: multivariate
6.20
Pareto distribution
6.21
Power function distribution
6.22
Rayleigh distribution
6.23
t distribution
c
⃝2000 by Chapman & Hall/CRC

6.24
Triangular distribution
6.25
Uniform distribution
6.26
Weibull distribution
6.27
Relationships among distributions
7
Standard Normal Distribution
7.1
Density function and related functions
7.2
Critical values
7.3
Tolerance factors for normal distributions
7.4
Operating characteristic curves
7.5
Multivariate normal distribution
7.6
Distribution of the correlation coeﬃcient
7.7
Circular normal probabilities
7.8
Circular error probabilities
8
Estimation
8.1
Deﬁnitions
8.2
Cram´er–Rao inequality
8.3
Theorems
8.4
The method of moments
8.5
The likelihood function
8.6
The method of maximum likelihood
8.7
Invariance property of MLEs
8.8
Diﬀerent estimators
8.9
Estimators for small samples
8.10
Estimators for large samples
9
Conﬁdence Intervals
9.1
Deﬁnitions
9.2
Common critical values
9.3
Sample size calculations
9.4
Summary of common conﬁdence intervals
9.5
Conﬁdence intervals: one sample
9.6
Conﬁdence intervals: two samples
9.7
Finite population correction factor
10
Hypothesis Testing
c
⃝2000 by Chapman & Hall/CRC

10.1
Introduction
10.2
The Neyman–Pearson lemma
10.3
Likelihood ratio tests
10.4
Goodness of ﬁt test
10.5
Contingency tables
10.6
Bartlett’s test
10.7
Cochran’s test
10.8
Number of observations required
10.9
Critical values for testing outliers
10.10
Signiﬁcance test in 2 × 2 contingency tables
10.11
Determining values in Bernoulli trials
11
Regression Analysis
11.1
Simple linear regression
11.2
Multiple linear regression
11.3
Orthogonal polynomials
12
Analysis of Variance
12.1
One-way anova
12.2
Two-way anova
12.3
Three-factor experiments
12.4
Manova
12.5
Factor analysis
12.6
Latin square design
13
Experimental Design
13.1
Latin squares
13.2
Graeco–Latin squares
13.3
Block designs
13.4
Factorial experimentation: 2 factors
13.5
2r Factorial experiments
13.6
Confounding in 2n factorial experiments
13.7
Tables for design of experiments
13.8
References
14
Nonparametric Statistics
14.1
Friedman test for randomized block design
c
⃝2000 by Chapman & Hall/CRC

14.2
Kendall’s rank correlation coeﬃcient
14.3
Kolmogorov–Smirnoﬀtests
14.4
Kruskal–Wallis test
14.5
The runs test
14.6
The sign test
14.7
Spearman’s rank correlation coeﬃcient
14.8
Wilcoxon matched-pairs signed-ranks test
14.9
Wilcoxon rank–sum (Mann–Whitney) test
14.10
Wilcoxon signed-rank test
15
Quality Control and Risk Analysis
15.1
Quality assurance
15.2
Acceptance sampling
15.3
Reliability
15.4
Risk analysis and decision rules
16
General Linear Models
16.1
Notation
16.2
The general linear model
16.3
Summary of rules for matrix operations
16.4
Quadratic forms
16.5
General linear hypothesis of full rank
16.6
General linear model of less than full rank
17
Miscellaneous Topics
17.1
Geometric probability
17.2
Information and communication theory
17.3
Kalman ﬁltering
17.4
Large deviations (theory of rare events)
17.5
Markov chains
17.6
Martingales
17.7
Measure theoretical probability
17.8
Monte Carlo integration techniques
17.9
Queuing theory
17.10
Random matrix eigenvalues
17.11
Random number generation
17.12
Resampling methods
c
⃝2000 by Chapman & Hall/CRC

17.13
Self-similar processes
17.14
Signal processing
17.15
Stochastic calculus
17.16
Classic and interesting problems
17.17
Electronic resources
17.18
Tables
18
Special Functions
18.1
Bessel functions
18.2
Beta function
18.3
Ceiling and ﬂoor functions
18.4
Delta function
18.5
Error functions
18.6
Exponential function
18.7
Factorials and Pochhammer’s symbol
18.8
Gamma function
18.9
Hypergeometric functions
18.10
Logarithmic functions
18.11
Partitions
18.12
Signum function
18.13
Stirling numbers
18.14
Sums of powers of integers
18.15
Tables of orthogonal polynomials
18.16
References
Notation
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 1
Introduction
Contents
1.1
Background
1.2
Data sets
1.3
References
1.1
BACKGROUND
The purpose of this book is to provide a modern set of tables and a com-
prehensive list of deﬁnitions, concepts, theorems, and formulae in probability
and statistics. While the numbers in these tables have not changed since they
were ﬁrst computed (in some cases, several hundred years ago), the presenta-
tion format here is modernized. In addition, nearly all table values have been
re-computed to ensure accuracy.
Almost every table is presented along with a textual description and at least
one example using a value from the table. Most concepts are illustrated with
examples and step-by-step solutions. Several data sets are described in this
chapter; they are used in this book in order for users to be able to check
algorithms.
The emphasis of this book is on what is often called basic statistics. Most
real-world statistics users will be able to refer to this book in order to quickly
verify a formula, deﬁnition, or theorem. In addition, the set of tables here
should make this a complete statistics reference tool. Some more advanced
useful and current topics, such as Brownian motion and decision theory are
also included.
1.2
DATA SETS
We have established a few data sets which we have used in examples through-
out this book. With these, a user can check a local statistics program by
verifying that it returns the same values as given in this book. For exam-
ple, the correlation coeﬃcient between the ﬁrst 100 elements of the sequence
of integers {1, 2, 3 . . . } and the ﬁrst 100 elements of the sequence of squares
{1, 4, 9 . . . } is 0.96885. Using this value is an easy way to check for correct
computation of a computer program. These data sets may be obtained from
http://vesta.bloomu.edu/~skokoska/prast/data.
c
⃝2000 by Chapman & Hall/CRC

Ticket data: Forty random speeding tickets were selected from the courthouse
records in Columbia County. The speed indicated on each ticket is given in
the table below.
58
72
64
65
67
92
55
51
69
73
64
59
65
55
75
56
89
60
84
68
74
67
55
68
74
43
67
71
72
66
62
63
83
64
51
63
49
78
65
75
Swimming pool data: Water samples from 35 randomly selected pools in Bev-
erly Hills were tested for acidity. The following table lists the PH for each
sample.
6.4
6.6
6.2
7.2
6.2
8.1
7.0
7.0
5.9
5.7
7.0
7.4
6.5
6.8
7.0
7.0
6.0
6.3
5.6
6.3
5.8
5.9
7.2
7.3
7.7
6.8
5.2
5.2
6.4
6.3
6.2
7.5
6.7
6.4
7.8
Soda pop data: A new soda machine placed in the Mathematics Building on
campus recorded the following sales data for one week in April.
Soda
Number of cans
Pepsi
72
Wild Cherry Pepsi
60
Diet Pepsi
85
Seven Up
54
Mountain Dew
32
Lipton Ice Tea
64
1.3
REFERENCES
Gathered here are some of the books referenced in later sections; each has a
broad coverage of the topics it addresses.
1. W. G. Cochran and G. M. Cox, Experimental Designs, Second Edition,
John Wiley & Sons, Inc., New York, 1957.
2. C. J. Colbourn and J. H. Dinitz, CRC Handbook of Combinatorial De-
signs, CRC Press, Boca Raton, FL, 1996.
3. L. Devroye, Non-Uniform Random Variate Generation, Springer–Verlag,
New York, 1986.
4. W. Feller, An Introduction to Probability Theory and Its Applications,
Volumes 1 and 2, John Wiley & Sons, New York, 1968.
5. C. W. Gardiner, Handbook of Stochastic Methods, Second edition, Springer–
Verlag, New York, 1985.
6. D. J. Sheskin, Handbook of Parametric and Nonparametric Statistical
Procedures, CRC Press LLC, Boca Raton, FL, 1997.
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 2
Summarizing Data
Contents
2.1
Tabular and graphical procedures
2.1.1
Stem-and-leaf plot
2.1.2
Frequency distribution
2.1.3
Histogram
2.1.4
Frequency polygons
2.1.5
Chernoﬀfaces
2.2
Numerical summary measures
2.2.1
(Arithmetic) mean
2.2.2
Weighted (arithmetic) mean
2.2.3
Geometric mean
2.2.4
Harmonic mean
2.2.5
Mode
2.2.6
Median
2.2.7
p% trimmed mean
2.2.8
Quartiles
2.2.9
Deciles
2.2.10
Percentiles
2.2.11
Mean deviation
2.2.12
Variance
2.2.13
Standard deviation
2.2.14
Standard errors
2.2.15
Root mean square
2.2.16
Range
2.2.17
Interquartile range
2.2.18
Quartile deviation
2.2.19
Box plots
2.2.20
Coeﬃcient of variation
2.2.21
Coeﬃcient of quartile variation
2.2.22
Z score
2.2.23
Moments
2.2.24
Measures of skewness
c
⃝2000 by Chapman & Hall/CRC

2.2.25
Measures of kurtosis
2.2.26
Data transformations
2.2.27
Sheppard’s corrections for grouping
Numerical descriptive statistics and graphical techniques may be used to sum-
marize information about central tendency and/or variability.
2.1
TABULAR AND GRAPHICAL PROCEDURES
2.1.1
Stem-and-leaf plot
A stem-and-leaf plot is a a graphical summary used to describe a set of ob-
servations (as symmetric, skewed, etc.). Each observation is displayed on the
graph and should have at least two digits. Split each observation (at the same
point) into a stem (one or more of the leading digit(s)) and a leaf (remaining
digits). Select the split point so that there are 5–20 total stems. List the
stems in a column to the left, and write each leaf in the corresponding stem
row.
Example 2.1:
Construct a stem-and-leaf plot for the Ticket Data (page 2).
Solution:
Stem
Leaf
4
3 9
5
1 1 5 5 5 6 8 9
6
0 2 3 3 4 4 4 5 5 5 6 7 7 7 8 8 9
7
1 2 2 3 4 4 5 5 8
8
3 4 9
9
2
Stem = 10, Leaf = 1
Figure 2.1: Stem–and–leaf plot for Ticket Data.
2.1.2
Frequency distribution
A frequency distribution is a tabular method for summarizing continuous or
discrete numerical data or categorical data.
(1) Partition the measurement axis into 5–20 (usually equal) reasonable
subintervals called classes, or class intervals.
Thus, each observation
falls into exactly one class.
(2) Record, or tally, the number of observations in each class, called the
frequency of each class.
(3) Compute the proportion of observations in each class, called the relative
frequency.
(4) Compute the proportion of observations in each class and all preceding
classes, called the cumulative relative frequency.
c
⃝2000 by Chapman & Hall/CRC

Example 2.2:
Construct a frequency distribution for the Ticket Data (page 2).
Solution:
(S1) Determine the classes. It seems reasonable to use 40 to less than 50, 50 to less
than 60, . . . , 90 to less than 100.
Note: For continuous data, one end of each class must be open. This ensures
that each observation will fall into only one class. The open end of each class
may be either the left or right, but should be consistent.
(S2) Record the number of observations in each class.
(S3) Compute the relative frequency and cumulative relative frequency for each class.
(S4) The resulting frequency distribution is in Figure 2.2.
Cumulative
Relative
relative
Class
Frequency
frequency
frequency
[40, 50)
2
0.050
0.050
[50, 60)
8
0.200
0.250
[60, 70)
17
0.425
0.625
[70, 80)
9
0.225
0.900
[80, 90)
3
0.075
0.975
[90, 100)
1
0.025
1.000
Figure 2.2: Frequency distribution for Ticket Data.
2.1.3
Histogram
A histogram is a graphical representation of a frequency distribution. A (rela-
tive) frequency histogram is a plot of (relative) frequency versus class interval.
Rectangles are constructed over each class with height proportional (usually
equal) to the class (relative) frequency. A frequency and relative frequency
histogram have the same shape, but diﬀerent scales on the vertical axis.
Example 2.3:
Construct a frequency histogram for the Ticket Data (page 2).
Solution:
(S1) Using the frequency distribution in Figure 2.2, construct rectangles above each
class, with height equal to class frequency.
(S2) The resulting histogram is in Figure 2.3.
Note: A probability histogram is constructed so that the area of each rectangle
equals the relative frequency. If the class widths are unequal, this histogram
presents a more accurate description of the distribution.
2.1.4
Frequency polygons
A frequency polygon is a line plot of points with x coordinate being class
midpoint and y coordinate being class frequency. Often the graph extends to
c
⃝2000 by Chapman & Hall/CRC

Figure 2.3: Frequency histogram for Ticket Data.
an additional empty class on both ends. The relative frequency may be used
in place of frequency.
Example 2.4:
Construct a frequency polygon for the Ticket Data (page 2).
Solution:
(S1) Using the frequency distribution in Figure 2.2, plot each point and connect the
graph.
(S2) The resulting frequency polygon is in Figure 2.4.
Figure 2.4: Frequency polygon for Ticket Data.
An ogive, or cumulative frequency polygon, is a plot of cumulative fre-
quency versus the upper class limit. Figure 2.5 is an ogive for the Ticket Data
(page 2).
Another type of frequency polygon is a more-than cumulative frequency poly-
gon. For each class this plots the number of observations in that class and
every class above versus the lower class limit.
c
⃝2000 by Chapman & Hall/CRC

Figure 2.5: Ogive for Ticket Data.
A bar chart is often used to graphically summarize discrete or categorical
data. A rectangle is drawn over each bin with height proportional to frequency.
The chart may be drawn with horizontal rectangles, in three dimensions, and
may be used to compare two or more sets of observations. Figure 2.6 is a bar
chart for the Soda Pop Data (page 2).
Figure 2.6: Bar chart for Soda Pop Data.
A pie chart is used to illustrate parts of the total. A circle is divided into
slices proportional to the bin frequency. Figure 2.7 is a pie chart for the Soda
Pop Data (page 2).
2.1.5
Chernoﬀfaces
Chernoﬀfaces are used to illustrate trends in multidimensional data. They
are eﬀective because people are used to diﬀerentiating between facial features.
Chernoﬀfaces have been used for cluster, discriminant, and time-series anal-
yses. Facial features that might be controllable by the data include:
(a) ear: level, radius
(b) eyebrow: height, slope, length
(c) eyes: height, size, separation, eccentricity, pupil position or size
c
⃝2000 by Chapman & Hall/CRC

Figure 2.7: Pie chart for Soda Pop Data.
(d) face: width, half-face height, lower or upper eccentricity
(e) mouth: position of center, curvature, length, openness
(f) nose: width, length
The Chernoﬀfaces in Figure 2.8 come from data about this book. For the
even chapters:
(a) eye size is proportional to the approximate number of pages
(b) mouth size is proportional to the approximate number of words
(c) face shape is proportional to the approximate number of occurrences of
the word “the”
The data are as follows:
Chapter
2
4
6
8
10
12
14
16
18
Number of pages
18
30
56
8
36
40
40
26
23
Number of words
4514 5426 12234 2392 9948 18418 8179 11739 5186
Occurrences of “the”
159
147
159
47
153
118
264
223
82
An interactive program for creating Chernoﬀfaces is available at http://
www.hesketh.com/schampeo/projects/Faces/interactive.shtml. See H.
Chernoﬀ, “The use of faces to represent points in a K-dimensional space
graphically,” Journal of the American Statistical Association, Vol. 68, No. 342,
1973, pages 361–368.
2.2
NUMERICAL SUMMARY MEASURES
The following conventions will be used in the deﬁnitions and formulas in this
section.
(C1) Ungrouped data: Let x1, x2, x3, . . . , xn be a set of observations.
(C2) Grouped data: Let x1, x2, x3, . . . , xk be a set of class marks from a fre-
quency distribution, or a representative set of observations, with corre-
c
⃝2000 by Chapman & Hall/CRC

Figure 2.8: Chernoﬀfaces for chapter data.
sponding frequencies f1, f2, f3, . . . , fk. The total number of observations
is n =
k
i=1
fi. Let c denote the (constant) width of each bin and xo one
of the class marks selected to be the computing origin. Each class mark,
xi, may be coded by ui = (xi −xo)/c. Each ui will be an integer and
the bin mark taken as the computing origin will be coded as a 0.
2.2.1
(Arithmetic) mean
The (arithmetic) mean of a set of observations is the sum of the observations
divided by the total number of observations.
(1) Ungrouped data:
x = 1
n
n

i=1
xi = x1 + x2 + x3 + · · · + xn
n
(2.1)
(2) Grouped data:
x = 1
n
k

i=1
fixi = f1x1 + f2x2 + f3x3 + · · · + fnxn
n
(2.2)
c
⃝2000 by Chapman & Hall/CRC

(3) Coded data:
x = xo + c ·
k
i=1
fiui
n
(2.3)
2.2.2
Weighted (arithmetic) mean
Let wi ≥0 be the weight associated with observation xi. The total weight is
given by
n
i=1
wi and the weighted mean is
xw =
n
i=1
wixi
n
i=1
wi
= w1x1 + w2x2 + w3x3 + · · · + wnxn
w1 + w2 + w3 + · · · + wn
.
(2.4)
2.2.3
Geometric mean
For ungrouped data such that xi > 0, the geometric mean is the nth root of
the product of the observations:
GM =
n√x1 · x2 · x3 · · · xn .
(2.5)
In logarithmic form:
log(GM) = 1
n
n

i=1
log xi = log x1 + log x2 + log x3 + · · · + log xn
n
.
(2.6)
For grouped data with each class mark xi > 0:
GM =
n
xf1
1 · xf2
2 · xf3
3 · · · xfk
k .
(2.7)
In logarithmic form:
log(GM) = 1
n
k

i=1
fi log(xi)
(2.8)
= f1 log(x1) + f2 log(x2) + f3 log(x3) + · · · + fk log(xk)
n
.
2.2.4
Harmonic mean
For ungrouped data the harmonic mean is given by
HM =
n
n
i=1
1
xi
=
n
1
x1
+ 1
x2
+ 1
x3
+ · · · + 1
xn
.
(2.9)
c
⃝2000 by Chapman & Hall/CRC

For grouped data:
HM =
n
k
i=1
fi
xi
=
n
f1
x1
+ f2
x2
+ f3
x3
+ · · · + fk
xk
.
(2.10)
Note: The equation involving the arithmetic, geometric, and harmonic mean
is
HM ≤GM ≤x .
(2.11)
Equality holds if all n observations are equal.
2.2.5
Mode
For ungrouped data, the mode, Mo, is the value that occurs most often, or with
the greatest frequency. A mode may not exist, for example, if all observations
occur with the same frequency. If the mode does exist, it may not be unique,
for example, if two observations occur with the greatest frequency.
For grouped data, select the class containing the largest frequency, called
the modal class. Let L be the lower boundary of the modal class, dL the
diﬀerence in frequencies between the modal class and the class immediately
below, and dH the diﬀerence in frequencies between the modal class and the
class immediately above. The mode may be approximated by
Mo ≈L + c ·
dL
dL + dH
.
(2.12)
2.2.6
Median
The median, ˜x, is another measure of central tendency, resistant to outliers.
For ungrouped data, arrange the observations in order from smallest to largest.
If n is odd, the median is the middle value. If n is even, the median is the
mean of the two middle values.
For grouped data, select the class containing the median (median class). Let
L be the lower boundary of the median class, fm the frequency of the median
class, and CF the sum of frequencies for all classes below the median class (a
cumulative frequency). The median may be approximated by
˜x ≈L + c ·
n
2 −CF
fm
.
(2.13)
Note: If x > ˜x the distribution is positively skewed. If x < ˜x the distribution
is negatively skewed. If x ≈˜x the distribution is approximately symmetric.
2.2.7
p% trimmed mean
A trimmed mean is a measure of central tendency and a compromise between
a mean and a median. The mean is more sensitive to outliers, and the median
is less sensitive to outliers. Order the observations from smallest to largest.
c
⃝2000 by Chapman & Hall/CRC

Delete the smallest p% and the largest p% of the observations.
The p%
trimmed mean, xtr(p), is the arithmetic mean of the remaining observations.
Note: If p% of n (observations) is not an integer, several (computer) algo-
rithms exist for interpolating at each end of the distribution and for deter-
mining xtr(p).
Example 2.5:
Using the Swimming Pool data (page 2) ﬁnd the mean, median, and
mode. Compute the geometric mean and the harmonic mean, and verify the relationship
between these three measures.
Solution:
(S1) x = 1
35(6.4 + 6.6 + 6.2 + · · · + 7.8) = 6.5886
(S2) ˜x = 6.5, the middle values when the observations are arranged in order from
smallest to largest.
(S3) Mo = 7.0, the observation that occurs most often.
(S4) GM =
35
(6.4)(6.6)(6.2) · · · (7.8) = 6.5513
(S5) HM =
35
(1/6.4) + (1/6.6) + (1/6.2) + · · · + (1/7.8) = 6.5137
(S6) To verify the inequality: 6.5137
  	
HM
≤6.5513
  	
GM
≤6.5886
  	
x
2.2.8
Quartiles
Quartiles split the data into four parts.
For ungrouped data, arrange the
observations in order from smallest to largest.
(1) The second quartile is the median: Q2 = ˜x.
(2) If n is even:
The ﬁrst quartile, Q1, is the median of the smallest n/2 observations;
and the third quartile, Q3, is the median of the largest n/2 observations.
(3) If n is odd:
The ﬁrst quartile, Q1, is the median of the smallest (n + 1)/2 observa-
tions; and the third quartile, Q3, is the median of the largest (n + 1)/2
observations.
For grouped data, the quartiles are computed by applying equation (2.13) for
the median. Compute the following:
L1 = the lower boundary of the class containing Q1.
L3 = the lower boundary of the class containing Q3.
f1 = the frequency of the class containing the ﬁrst quartile.
f3 = the frequency of the class containing the third quartile.
CF1 = cumulative frequency for classes below the one containing Q1.
CF3 = cumulative frequency for classes below the one containing Q3.
c
⃝2000 by Chapman & Hall/CRC

The (approximate) quartiles are given by
Q1 = L1 + c ·
n
4 −CF1
f1
Q3 = L3 + c ·
3n
4 −CF3
f3
.
(2.14)
2.2.9
Deciles
Deciles split the data into 10 parts.
(1) For ungrouped data, arrange the observations in order from smallest to
largest. The ith decile, Di (for i = 1, 2, . . . , 9), is the i(n + 1)/10th ob-
servation. It may be necessary to interpolate between successive values.
(2) For grouped data, apply equation (2.13) (as in equation (2.14)) for the
median to ﬁnd the approximate deciles. Di is in the class containing
the i n/10th largest observation.
2.2.10
Percentiles
Percentiles split the data into 100 parts.
(1) For ungrouped data, arrange the observations in order from smallest to
largest. The ith percentile, Pi (for i = 1, 2, . . . , 99), is the i(n+1)/100th
observation. It may be necessary to interpolate between successive val-
ues.
(2) For grouped data, apply equation (2.13) (as in equation (2.14)) for the
median to ﬁnd the approximate percentiles. Pi is in the class containing
the i n/100th largest observation.
2.2.11
Mean deviation
The mean deviation is a measure of variability based on the absolute value of
the deviations about the mean or median.
(1) For ungrouped data:
MD = 1
n
n

i=1
|xi −x|
or
MD = 1
n
n

i=1
|xi −˜x| .
(2.15)
(2) For grouped data:
MD = 1
n
k

i=1
fi|xi −x|
or
MD = 1
n
k

i=1
fi|xi −˜x| .
(2.16)
2.2.12
Variance
The variance is a measure of variability based on the squared deviations about
the mean.
c
⃝2000 by Chapman & Hall/CRC

(1) For ungrouped data:
s2 =
1
n −1
n

i=1
(xi −x)2.
(2.17)
The computational formula for s2:
s2 =
1
n −1


n

i=1
x2
i −1
n
 n

i=1
xi
2
=
1
n −1
 n

i=1
x2
i −nx2

. (2.18)
(2) For grouped data:
s2 =
1
n −1
k

i=1
fi(xi −x)2.
(2.19)
The computational formula for s2:
s2 =
1
n −1


k

i=1
fix2
i −1
n
 k

i=1
fixi
2

=
1
n −1
 k

i=1
fix2
i −nx2

.
(2.20)
(3) For coded data:
s2 =
c
n −1


k

i=1
fiu2
i −1
n
 k

i=1
fiui
2
.
(2.21)
2.2.13
Standard deviation
The standard deviation is the positive square root of the variance: s =
√
s2.
The probable error is 0.6745 times the standard deviation.
2.2.14
Standard errors
The standard error of a statistic is the standard deviation of the sampling dis-
tribution of that statistic. The standard error of a statistic is often designated
by σ with a subscript indicating the statistic.
2.2.14.1
Standard error of the mean
The standard error of the mean is used in hypothesis testing and is an indi-
cation of the accuracy of the estimate x.
SEM = s/√n .
(2.22)
c
⃝2000 by Chapman & Hall/CRC

2.2.15
Root mean square
(1) For ungrouped data:
RMS =

1
n
n

i=1
x2
i
1/2
.
(2.23)
(2) For grouped data:
RMS =

1
n
k

i=1
fix2
i
1/2
.
(2.24)
2.2.16
Range
The range is the diﬀerence between the largest and smallest values.
R = max{x1, x2, . . . , xn} −min{x1, x2, . . . , xn} = x(n) −x(1) .
(2.25)
2.2.17
Interquartile range
The interquartile range, or fourth spread, is the diﬀerence between the third
and ﬁrst quartile.
IQR = Q3 −Q1 .
(2.26)
2.2.18
Quartile deviation
The quartile deviation, or semi-interquartile range, is half the interquartile
range.
QD = Q3 −Q1
2
.
(2.27)
2.2.19
Box plots
Box plots, also known as quantile plots, are graphics which display the center
portions of the data and some information about the range of the data. There
are a number of variations and a box plot may be drawn with either a hori-
zontal or vertical scale. The inner and outer fences are used in constructing
a box plot and are markers used in identifying mild and extreme outliers.
Inner Fences: Q1 −1.5 · IQR, Q1 + 1.5 · IQR
Outer Fences: Q3 −3 · IQR,
Q3 + 3 · IQR
(2.28)
c
⃝2000 by Chapman & Hall/CRC

A general description:
Multiple box plots on the same measurement axis may be used to compare
the center and spread of distributions. Figure 2.9 presents box plots for ran-
domly selected August residential electricity bills for three diﬀerent parts of
the country.
Figure 2.9: Example of multiple box plots.
c
⃝2000 by Chapman & Hall/CRC

2.2.20
Coeﬃcient of variation
The coeﬃcient of variation is a measure of relative variability. Reported as
percentage it is deﬁned as:
CV = 100 s
x .
(2.29)
2.2.21
Coeﬃcient of quartile variation
The coeﬃcient of quartile variation is a measure of variability.
CQV = 100 Q3 −Q1
Q3 + Q1
.
(2.30)
2.2.22
Z score
The z score, or standard score, associated with an observation is a measure
of relative standing.
z = xi −x
s
(2.31)
2.2.23
Moments
Moments are used to characterize a set of observations.
(1) For ungrouped data:
The rth moment about the origin:
m′
r = 1
n
n

i=1
xr
i .
(2.32)
The rth moment about the mean x:
mr = 1
n
n

i=1
(xi −x)r =
r

j=0
r
j

(−1)jm′
r−jxj .
(2.33)
(2) For grouped data:
The rth moment about the origin:
m′
r = 1
n
k

i=1
fixr
i .
(2.34)
The rth moment about the mean x:
mr = 1
n
k

i=1
fi(xi −¯x)r =
r

j=0
r
j

(−1)jm′
r−j ¯xj.
(2.35)
(3) For coded data:
m′
r = cr
n
n

i=1
fiur
i .
(2.36)
c
⃝2000 by Chapman & Hall/CRC

2.2.24
Measures of skewness
The following descriptive statistics measure the lack of symmetry. Larger val-
ues (in magnitude) indicate more skewness in the distribution of observations.
2.2.24.1
Coeﬃcient of skewness
g1 =
m3
m3/2
2
(2.37)
2.2.24.2
Coeﬃcient of momental skewness
g1
2 =
m3
2m3/2
2
(2.38)
2.2.24.3
Pearson’s ﬁrst coeﬃcient of skewness
Sk1 = 3(x −Mo)
s
(2.39)
2.2.24.4
Pearson’s second moment of skewness
Sk2 = 3(x −˜x)
s
(2.40)
2.2.24.5
Quartile coeﬃcient of skewness
Skq = Q3 −2˜x + Q1
Q3 −Q1
(2.41)
Example 2.6:
Using the Swimming Pool data (page 2) ﬁnd the coeﬃcient of skew-
ness, coeﬃcient of momental skewness, Pearson’s ﬁrst coeﬃcient of skewness, Pearson’s
second moment of skewness, and the quartile coeﬃcient of skewness.
Solution:
(S1) ¯x = 6.589, ˜x = 6.5, s = 0.708, Q1 = 6.2, Q3 = 7.0, Mo = 7.0
(S2) m2 = 1
n
35

i=1
(xi −¯x)2 = 0.4867
m3 = 1
n
35

i=1
(xi −¯x)3 = 0.0126
(S3) g1 = 0.0126/(0.4867)3/2 = 0.0371 ,
g1/2 = 0.0372/2 = 0.0186
(S4) Sk1 = 3(6.589 −7)
0.708
= −1.7415 ,
Sk2 = 3(6.589 −6.5)
0.708
= 0.3771
(S5) Skq = 7.0 −2(6.5) + 6.2
7.0 −6.2
= 0.25
2.2.25
Measures of kurtosis
The following statistics describe the extent of the peak in a distribution.
Smaller values (in magnitude) indicate a ﬂatter, more uniform distribution.
c
⃝2000 by Chapman & Hall/CRC

2.2.25.1
Coeﬃcient of kurtosis
g2 = m4
m2
2
(2.42)
2.2.25.2
Coeﬃcient of excess kurtosis
g2 −3 = m4
m2
2
−3
(2.43)
2.2.26
Data transformations
Suppose yi = axi + b for i = 1, 2, . . . , n. The following summary statistics for
the distribution of y’s are related to summary statistics for the distribution
of x’s.
y = ax + b ,
s2
y = a2s2
x ,
sy = |a|sx
(2.44)
2.2.27
Sheppard’s corrections for grouping
For grouped data, suppose every class interval has width c. If both tails of the
distribution are very ﬂat and close to the measurement axis, the grouped data
approximation to the sample variance may be improved by using Sheppard’s
correction, −c2/12:
corrected variance = grouped data variance −c2
12
(2.45)
There are similar corrected sample moments, denoted m′
rc and mrc:
m′
1c = m′
1
m1c = m1
m′
2c = m′
2 −c2
12
m2c = m2 −c2
12
m′
3c = m′
3 −c2
4 m′
1
m3c = m3
m′
4c = m′
4 −c2
2 m′
1 + 7c2
240
m4c = m4 −c2
2 m2 + 7c2
240
(2.46)
Example 2.7:
Consider the grouped Ticket Data (page 2) as presented in the fre-
quency distribution in Example 2.2 (on page 5). Find the corrected sample variance and
corrected sample moments.
Solution:
(S1) ¯x = 66.5 ,
s2 = 115.64 (for grouped data), c = 10
(S2) corrected variance = 115.64 −(102/12) = 107.31
(S3) r
m′
r
m′
rc
mr
mrc
1
66.5
66.5
0.0
0.0
2
4535.0
4526.7
112.8
104.4
3
316962.5
315300.0
389.3
389.3
4
22692125.0
22688802.9
40637.3
35002.7
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 3
Probability
Contents
3.1
Algebra of sets
3.2
Combinatorial methods
3.2.1
The product rule for ordered pairs
3.2.2
The generalized product rule for k-tuples
3.2.3
Permutations
3.2.4
Circular permutations
3.2.5
Combinations (binomial coeﬃcients)
3.2.6
Sample selection
3.2.7
Balls into cells
3.2.8
Multinomial coeﬃcients
3.2.9
Arrangements and derangements
3.3
Probability
3.3.1
Relative frequency concept of probability
3.3.2
Axioms of probability (discrete sample space)
3.3.3
The probability of an event
3.3.4
Probability theorems
3.3.5
Probability and odds
3.3.6
Conditional probability
3.3.7
The multiplication rule
3.3.8
The law of total probability
3.3.9
Bayes’ theorem
3.3.10
Independence
3.4
Random variables
3.4.1
Discrete random variables
3.4.2
Continuous random variables
3.4.3
Random functions
3.5
Mathematical expectation
3.5.1
Expected value
3.5.2
Variance
3.5.3
Moments
3.5.4
Generating functions
c
⃝2000 by Chapman & Hall/CRC

3.6
Multivariate distributions
3.6.1
Discrete case
3.6.2
Continuous case
3.6.3
Expectation
3.6.4
Moments
3.6.5
Marginal distributions
3.6.6
Independent random variables
3.6.7
Conditional distributions
3.6.8
Variance and covariance
3.6.9
Correlation coeﬃcient
3.6.10
Moment generating function
3.6.11
Linear combination of random variables
3.6.12
Bivariate distribution
3.7
Inequalities
3.1
ALGEBRA OF SETS
Properties of and operations on sets are important since events may be
thought of as sets. Some set facts:
(1) A set A is a collection of objects called the elements of the set.
a ∈A means a is an element of the set A.
a ̸∈A means a is not an element of the set A.
A = {a, b, c} is used to denote the elements of the set A.
(2) The null set, denoted by φ or { }, is the empty set; the set that contains
no elements.
(3) Two sets A and B are equal, written A = B, if
1) every element of A is an element of B, and
2) every element of B is an element of A.
(4) The set A is a subset of the set B if every element of A is also in B;
written A ⊂B (or B ⊃A). For every set A, φ ⊂A.
(5) If A ⊂B and B ⊂A then A = B and A is an improper subset of B.
If A ⊂B and there is at least one element of B not in A then A is
a proper subset of B. The subset symbol ⊂is often used to denote a
proper subset while the symbol ⊆indicates an improper subset.
(6) Let S be the universal set, the set consisting of all elements of interest.
For any set A, A ⊂S.
(7) The complement of the set A, denoted A′, is the set consisting of all
elements in S but not in A (Figure 3.1).
(8) For any two sets A and B:
The union of A and B, denoted A∪B, is the set consisting of all elements
in A, or B, or both (Figure 3.2).
The intersection of A and B, denoted A ∩B, is the set consisting of all
elements in both A and B (Figure 3.3).
c
⃝2000 by Chapman & Hall/CRC

(9) A and B are disjoint or mutually exclusive if A ∩B = φ (Figure 3.4).
Figure 3.1: Shaded region =
A′.
Figure 3.2: Shaded region =
A ∪B.
Figure 3.3: Shaded region =
A ∩B.
Figure 3.4: Mutually exclu-
sive sets.
For the following properties, suppose A, B, and C are sets. It is necessary
to assume these sets lie in a universal set S only in those properties that
explicitly involve S.
(1) Closure
(a) There is a unique set A ∪B.
(b) There is a unique set A ∩B.
(2) Commutative laws
(a) A ∪B = B ∪A
(b) A ∩B = B ∩A
(3) Associative laws
(a) (A ∪B) ∪C = A ∪(B ∪C)
(b) (A ∩B) ∩C = A ∩(B ∩C)
(4) Distributive laws
(a) A ∪(B ∩C) = (A ∪B) ∩(A ∪C)
(b) A ∩(B ∪C) = (A ∩B) ∪(A ∩C)
(5) Idempotent laws
(a) A ∪A = A
(b) A ∩A = A
(6) Properties of S and φ
c
⃝2000 by Chapman & Hall/CRC

(a) A ∩S = A
(b) A ∪φ = A
(c) A ∩φ = φ
(d) A ∪S = S
(7) Properties of ⊂
(a) A ⊂(A ∪B)
(b) (A ∩B) ⊂A
(c) A ⊂S
(d) φ ⊂A
(e) If A ⊂B, then A ∪B = B and A ∩B = A.
(8) Properties of ′ (set complement)
(a) For every set A, there is a unique set A′.
(b) A ∪A′ = S
(c) A ∩A′ = φ
(d)
(A ∪B)′ = A′ ∩B′
(A ∩B)′ = A′ ∪B′

DeMorgan’s laws
(9) Some generalizations
Suppose A1, A2, A3, . . . , An is a collection of sets.
(a) The generalized union, A1 ∪A2 ∪· · · ∪An, is the set consisting of
all elements in at least one Ai.
(b) The generalized intersection, A1∩A2∩· · ·∩An, is the set consisting
of all elements in every Ai.
(c)

n∪
i=1 Ai
′
= (A1 ∪· · · ∪An)′ = A′
1 ∩· · · ∩A′
n =
n∩
i=1 A′
i
(d)

n∩
i=1 Ai
′
= (A1 ∩· · · ∩An)′ = A′
1 ∪· · · ∪A′
n =
n∪
i=1 A′
i
3.2
COMBINATORIAL METHODS
In an equally likely outcome experiment, computing the probability of an
event involves counting. The following techniques are useful for determining
the number of outcomes in an event and/or the sample space.
3.2.1
The product rule for ordered pairs
If the ﬁrst element of an ordered pair can be selected in n1 ways, and for each
of these n1 ways the second element of the pair can be selected in n2 ways,
then the number of possible pairs is n1n2.
c
⃝2000 by Chapman & Hall/CRC

3.2.2
The generalized product rule for k-tuples
Suppose a sample space, or set, consists of ordered collections of k-tuples.
If there are n1 choices for the ﬁrst element, and for each choice of the ﬁrst
element there are n2 choices for the second element, . . . , and for each of the
ﬁrst k −1 elements there are nk choices for the kth element, then there are
n1n2 · · · nk possible k-tuples.
3.2.3
Permutations
The number of permutations of n distinct objects taken k at a time is
P(n, k) =
n!
(n −k)! .
(3.1)
A table of values is on page 500.
3.2.4
Circular permutations
The number of permutations of n distinct objects arranged in a circle is (n −
1)!.
3.2.5
Combinations (binomial coeﬃcients)
The binomial coeﬃcient
n
k

is the number of combinations of n distinct ob-
jects taken k at a time without regard to order:
C(n, k) =
n
k

=
n!
k!(n −k)! = P(n, k)
k!
.
(3.2)
A table of values is on page 500. Other formulas involving binomial coeﬃ-
cients:
(a)
n
k

= n(n −1) · · · (n −k + 1)
k!
=

n
n −k

(b)
n
0

=
n
n

= 1
and
n
1

= n
(c)
n
k

=
n −1
k

+
n −1
k −1

(d)
n
0

+
n
1

+ · · · +
n
n

= 2n
(e)
n
0

−
n
1

+ · · · + (−1)n
n
n

= 0
(f)
2n
n

= 2n(2n−1)(2n−3)···3·1
n!
(g)
n
n

+
n+1
n

+
n+2
n

+ · · · +
n+m
n

=
n+m+1
n+1

(h)
n
0

+
n
2

+
n
4

+ · · · + = 2n−1 (last term in sum is
 n
n−1

or
n
n

)
(i)
n
1

+
n
3

+
n
5

+ · · · + = 2n−1 (last term in sum is
 n
n−1

or
n
n

)
c
⃝2000 by Chapman & Hall/CRC

(j)
n
0
2 +
n
1
2 + · · · +
n
n
2 =
2n
n

(k)
m
0
n
p

+
m
1
 n
p−1

+ · · · +
m
p
n
0

=
m+n
p

(l) 1
n
1

+ 2
n
2

+ · · · + n
n
n

= n2n−1
(m) 1
n
1

−2
n
2

+ · · · + (−1)n+1n
n
n

= 0
Example 3.8:
For the 5 element set {a, b, c, d, e} ﬁnd the number of subsets containing
exactly 3 elements.
Solution:
(S1) There are

5
3

=
5!
3!2! = 10 subsets containing exactly 3 elements.
(S2) The subsets are
(a, b, c)
(a, b, d)
(a, b, e)
(a, c, d)
(a, c, e)
(a, d, e)
(b, c, d)
(b, c, e)
(b, d, e)
(c, d, e)
3.2.6
Sample selection
There are 4 ways in which a sample of k elements can be obtained from a set
of n distinguishable objects.
Order
Repetitions
The sample
Number of ways to
counts?
allowed?
is called a
choose the sample
No
No
k-combination
C(n, k)
Yes
No
k-permutation
P(n, k)
No
Yes
k-combination
with replacement
CR(n, k)
Yes
Yes
k-permutation
with replacement
P R(n, k)
where
C(n, k) =
n
k

=
n!
k! (n −k)!
P(n, k) = (n)k = nk =
n!
(n −k)!
CR(n, k) = C(n + k −1, k) = (n + k −1)!
k!(n −1)!
P R(n, k) = nk
(3.3)
Example 3.9:
There are 4 ways in which to choose a 2 element sample from the set
{a, b}:
combination
C(2, 2) = 1
ab
permutation
P(2, 2) = 2
ab and ba
combination with replacement
CR(2, 2) = 3
aa, ab, and bb
permutation with replacement
P R(2, 2) = 4
aa, ab, ba, and bb
c
⃝2000 by Chapman & Hall/CRC

3.2.7
Balls into cells
There are 8 diﬀerent ways in which n balls can be placed into k cells.
Distinguish
Distinguish
Can cells
Number of ways to
balls?
cells?
be empty?
place n balls into k cells
Yes
Yes
Yes
kn
Yes
Yes
No
k!
 n
k

No
Yes
Yes
C(k + n −1, n) =
k+n−1
n

No
Yes
No
C(n −1, k −1) =
n−1
k−1

Yes
No
Yes
 n
1

+
 n
2

+ · · · +
 n
k

Yes
No
No
 n
k

No
No
Yes
p1(n) + p2(n) + · · · + pk(n)
No
No
No
pk(n)
where
 n
k

is the Stirling cycle number (see page 525) and pk(n) is the number
of partitions of the number n into exactly k integer pieces (see page 523).
Given n distinguishable balls and k distinguishable cells, the number of ways
in which we can place n1 balls into cell 1, n2 balls into cell 2, . . . , nk balls
into cell k, is given by the multinomial coeﬃcient

n
n1,n2,...,nk

.
3.2.8
Multinomial coeﬃcients
The multinomial coeﬃcient,

n
n1,n2,...,nk

= C(n; n1, n2, . . . , nk), is the number
of ways of choosing n1 objects, then n2 objects, . . . , then nk objects from a
collection of n distinct objects without regard to order. This requires that
k
j=1 nj = n.
Other ways to interpret the multinomial coeﬃcient:
(1) Permutations (all objects not distinct): Given n1 objects of one kind,
n2 objects of a second kind, . . . , nk objects of a kth kind, and n1 +
n2 + · · · + nk = n. The number of permutations of the n objects is

n
n1,n2,...,nk

.
(2) Partitions: The number of ways of partitioning a set of n distinct objects
into k subsets with n1 objects in the ﬁrst subset, n2 objects in the second
subset, . . . , and nk objects in the kth subset is

n
n1,n2,...,nk

.
The multinomial symbol is numerically evaluated as

n
n1, n2, . . . , nk

=
n!
n1! n2! · · · nk!
(3.4)
Example 3.10:
The number of ways to choose 2 objects, then 1 object, then 1 object
from the set {a, b, c, d} is 
4
2,1,1

= 12; they are as follows (commas separate the ordered
c
⃝2000 by Chapman & Hall/CRC

selections):
{ab, c, d}
{ab, d, c}
{ac, b, d}
{ac, d, b}
{ad, b, c}
{ad, c, b}
{bc, a, d}
{bc, d, a}
{bd, a, c}
{bd, c, a}
{cd, a, b}
{cd, b, a}
3.2.9
Arrangements and derangements
(a) The number of ways to arrange n distinct objects in a row is n!; this is
the number of permutations of n objects.
Example 3.11:
For the three objects {a, b, c} the number of arrangements is
3! = 6. These permutations are {abc, bac, cab, acb, bca, cba}.
(b) The number of ways to arrange n non-distinct objects (assuming that
there are k types of objects, and ni copies of each object of type i) is
the multinomial coeﬃcient

n
n1,n2,...,nk

.
Example 3.12:
For the set {a, a, b, c} the parameters are n = 4, k = 3, n1 = 2,
n2 = 1, and n3 = 1. Hence, there are 
4
2,1,1

=
4!
2! 1! 1! = 12 arrangements, they
are:
aabc
aacb
abac
abca
acab
acba
baac
baca
bcaa
caab
caba
cbaa
(c) A derangement is a permutation of objects, in which object i is not in
the ith location.
Example 3.13:
All the derangements of {1, 2, 3, 4} are:
2143
2341
2413
3142
3412
3421
4123
4312
4321
The number of derangements of n elements, Dn, satisﬁes the recursion
relation: Dn = (n −1) (Dn−1 + Dn−2), with the initial values D1 = 0
and D2 = 1. Hence,
Dn = n!

1 −1
1! + 1
2! −1
3! + · · · + (−1)n 1
n!

The numbers Dn are also called subfactorials and rencontres numbers.
For large values of n, Dn/n! ∼e−1 ≈0.37. Hence, more than one of
every three permutations is a derangement.
n
1
2
3
4
5
6
7
8
9
10
Dn
0
1
2
9
44
265
1854
14833
133496
1334961
3.3
PROBABILITY
The sample space of an experiment, denoted S, is the set of all possible out-
comes. Each outcome of the sample space is also called an element of the
sample space or a sample point. An event is any collection of outcomes con-
tained in the sample space. A simple event consists of exactly one outcome
and a compound event consists of more than one outcome.
c
⃝2000 by Chapman & Hall/CRC

3.3.1
Relative frequency concept of probability
Suppose an experiment is conducted n identical and independent times and
n(A) is the number of times the event A occurs. The quotient n(A)/n is the
relative frequency of occurrence of the event A. As n increases, the relative
frequency converges to the limiting relative frequency of the event A. The
probability of the event A, Prob [A], is this limiting relative frequency.
3.3.2
Axioms of probability (discrete sample space)
(1) For any event A, Prob [A] ≥0.
(2) Prob [S] = 1.
(3) If A1, A2, A3, . . . , is a ﬁnite or inﬁnite collection of pairwise mutually
exclusive events of S, then
Prob [A1 ∪A2 ∪A3 ∪· · ·] = Prob [A1] + Prob [A2] + Prob [A3] + · · ·
(3.5)
3.3.3
The probability of an event
The probability of an event A is the sum of Prob [ai] for all sample points ai
in the event A:
Prob [A] =

ai∈A
Prob [ai] .
(3.6)
If all of the outcomes in S are equally likely:
Prob [A] = n(A)
n(S) = number of outcomes in A
number of outcomes in S .
(3.7)
3.3.4
Probability theorems
(1) Prob [φ] = 0 for any sample space S.
(2) If A and A′ are complementary events, Prob [A] + Prob [A′] = 1.
(3) For any events A and B, if A ⊂B then Prob [A] ≤Prob [B].
(4) For any events A and B,
Prob [A ∪B] = Prob [A] + Prob [B] −Prob [A ∩B] .
(3.8)
If A and B are mutually exclusive events, Prob [A ∩B] = 0 and
Prob [A ∪B] = Prob [A] + Prob [B] .
(3.9)
(5) For any events A and B,
Prob [A] = Prob [A ∩B] + Prob [A ∩B′] .
(3.10)
c
⃝2000 by Chapman & Hall/CRC

(6) For any events A, B, and C,
Prob [A ∪B ∪C] =Prob [A] + Prob [B] + Prob [C]
−Prob [A ∩B] −Prob [A ∩C] −Prob [B ∩C]
+ Prob [A ∩B ∩C].
(3.11)
(7) For any events A1, A2, . . . , An,
Prob

n∪
i=1 Ai

≤
n

i=1
Prob [Ai] .
(3.12)
Equality holds if the events are pairwise mutually exclusive.
3.3.5
Probability and odds
If the probability of an event A is Prob [A] then
odds for A = Prob [A]/Prob [A′],
Prob [A′] ̸= 0
odds against A = Prob [A′]/Prob [A],
Prob [A] ̸= 0.
(3.13)
If the odds for the event A are a:b, then Prob [A] = a/(a + b).
Example 3.14:
The odds of a fair coin coming up heads are 1:1; that it, is has a
probability of 1/2.
The odds of a die showing a “1” are 5:1 against; that it, there is a probability of 5/6
that a “1” does not appear.
3.3.6
Conditional probability
The conditional probability of A given the event B has occurred is
Prob [A|B] = Prob [A ∩B]
Prob [B]
,
Prob [B] > 0 .
(3.14)
(1) If Prob [A1 ∩A2 ∩· · · ∩An−1] > 0 then
Prob [A1 ∩A2 ∩· · · ∩An] =Prob [A1] · Prob [A2 |A1]
· Prob [A3 |A1 ∩A2]
· · · Prob [An |A1 ∩A2 ∩· · · ∩An−1].
(3.15)
(2) If A ⊂B, then Prob [A|B] = Prob [A]/Prob [B] and
Prob [B |A] = 1.
(3) Prob [A′ |B] = 1 −Prob [A|B].
Example 3.15:
A local bank oﬀers loans for three purposes: home (H), automobile
(A), and personal (P), and two diﬀerent types: ﬁxed rate (FR) and adjustable rate
(ADJ). The joint probability table given below presents the proportions for the various
c
⃝2000 by Chapman & Hall/CRC

categories of loan and type:
Loan Purpose
H
A
P
Type
FR
.27
.19
.14
ADJ
.13
.09
.18
Suppose a person who took out a loan at this bank is selected at random.
(a) What is the probability the person has an automobile loan and it is ﬁxed rate?
(b) Given the person has an adjustable rate loan, what is the probability it is for a
home?
(c) Given the person does not have a personal loan, what is the probability it is
adjustable rate?
Solution:
(S1) Prob [A ∩FR] = .19
(S2) Prob [H | ADJ] = Prob [H ∩ADJ]/Prob [ADJ] = .13/.4 = .325
(S3) Prob 
ADJ | P′
= Prob 
ADJ ∩P′
/Prob 
P′
= .22/.68 = .3235
3.3.7
The multiplication rule
Prob [A ∩B] = Prob [A|B] · Prob [B] ,
Prob [B] ̸= 0
= Prob [B |A] · Prob [A] ,
Prob [A] ̸= 0
(3.16)
3.3.8
The law of total probability
Suppose A1, A2, . . . , An is a collection of mutually exclusive, exhaustive events,
Prob [Ai] ̸= 0, i = 1, 2, . . . , n. For any event B:
Prob [B] =
n

i=1
Prob [B |Ai] · Prob [Ai] .
(3.17)
Example 3.16:
A ball drawing strategy. There are two urns. A marked ball may be in
urn 1 (with probability p) or urn 2 (with probability 1 −p). The probability of drawing
the marked ball from the urn it is in is r (with r < 1). After a ball is drawn from an
urn, it is replaced. What is the best way to use n draws of balls from any urn so that
the probability of drawing the marked ball is largest?
Solution:
(S1) Let the event of selecting the marked ball be A.
(S2) Let Hi be the hypothesis that the marked ball is in urn i.
(S3) By assumption, Prob [H1] = p and Prob [H2] = 1 −p.
(S4) Choose m balls from urn 1, and n −m balls from urn 2. The conditional proba-
bilities are then:
Prob [A | H1] = 1 −(1 −r)m,
Prob [A | H2] = 1 −(1 −r)n−m
(3.18)
c
⃝2000 by Chapman & Hall/CRC

so that (using the law of total probability)
Prob [A] = Prob [H1] · Prob [A | H1] + Prob [H2] · Prob [A | H2]
= p [1 −(1 −r)m] + (1 −p) 
1 −(1 −r)n−m
.
(3.19)
(S5) Diﬀerentiating this with respect to m, and setting dProb[A]
dm
= 0 results in (1 −
r)2m−n = (1 −p)/p or
m = n
2 +
ln

1−p
p

2 ln(1 −r).
(3.20)
3.3.9
Bayes’ theorem
Suppose A1, A2, . . . , An is a collection of mutually exclusive, exhaustive events,
Prob [Ai] ̸= 0, i = 1, 2, . . . , n. For any event B such that Prob [B] ̸= 0:
Prob [Ak |B] = Prob [Ak ∩B]
Prob [B]
=
Prob [B |Ak] · Prob [Ak]
n
i=1
Prob [B |Ai] · Prob [Ai]
,
(3.21)
for k = 1, 2, . . . , n.
Example 3.17:
A large manufacturer uses three diﬀerent trucking companies (A, B,
and C) to deliver products. The probability a randomly selected shipment is delivered
by each company is
Prob [A] = .60,
Prob [B] = .25,
Prob [C] = .15
Occasionally, a shipment is damaged (D) in transit.
Prob [D | A] = .01,
Prob [D | B] = .005,
Prob [D | C] = .015
Suppose a shipment is selected at random.
(a) Find the probability the shipment is sent by trucking company B and is damaged.
(b) Find the probability the shipment is damaged.
(c) Suppose a shipment arrives damaged. What is the probability it was shipped by
company B?
Solution:
(S1) Prob [B ∩D] = Prob [B] · Prob [D | B] = (.25)(.005) = .00125
(S2) Prob [D] = Prob [A ∩D] + Prob [B ∩D] + Prob [C ∩D]
= Prob [A] · Prob [D | A] + Prob [B] · Prob [D | B]
+ Prob [B] · Prob [D | B]
= (.60)(.01) + (.25)(.005) + (.15)(.015) = .0095
(S3) Prob [B | D] = Prob [B ∩D]/Prob [D] = .00125/.0095 = .1316
3.3.10
Independence
(1) A and B are independent events if Prob [A|B] = Prob [A] or, equiva-
lently, if Prob [B |A] = Prob [B].
c
⃝2000 by Chapman & Hall/CRC

(2) A and B are independent events if and only if Prob [A ∩B] = Prob [A] ·
Prob [B].
(3) A1, A2, . . . , An are pairwise independent events if
Prob [Ai ∩Aj] = Prob [Ai] · Prob [Aj]
for every pair i, j with i ̸= j.
(3.22)
(4) A1, A2, . . . , An are mutually independent events if for every k, k =
2, 3, . . . , n, and every subset of indices i1, i2, . . . , ik,
Prob [Ai1 ∩Ai2 ∩· · · ∩Aik] = Prob [Ai1] · Prob [Ai2] · · · Prob [Aik] .
(3.23)
3.4
RANDOM VARIABLES
Given a sample space S, a random variable is a function with domain S and
range some subset of the real numbers. A random variable is discrete if it can
assume only a ﬁnite or countably inﬁnite number of values. A random variable
is continuous if its set of possible values is an entire interval of numbers.
Random variables are denoted by upper-case letters, for example X.
3.4.1
Discrete random variables
3.4.1.1
Probability mass function
The probability distribution or probability mass function (pmf), p(x), of a
discrete random variable is a rule deﬁned for every number x by p(x) =
Prob [X = x] such that
(1) p(x) ≥0; and
(2) 
x
p(x) = 1
3.4.1.2
Cumulative distribution function
The cumulative distribution function (cdf), F(x), for a discrete random vari-
able X with pmf p(x) is deﬁned for every number x:
F(x) = Prob [X ≤x] =

y|y≤x
p(y) .
(3.24)
(1)
lim
x→−∞F(x) = 0
(2)
lim
x→∞F(x) = 1
(3) If a and b are real numbers such that a < b, then F(a) ≤F(b).
(4) Prob [a ≤X ≤b] = Prob [X ≤b] −Prob [X < a] = F(b) −F(a−) where
a−is the ﬁrst value X assumes less than a. Valid for a, b, ∈R and a < b.
c
⃝2000 by Chapman & Hall/CRC

3.4.2
Continuous random variables
3.4.2.1
Probability density function
The probability distribution or probability density function (pdf) of a contin-
uous random variable X is a real-valued function f(x) such that
Prob [a ≤X ≤b] =
 b
a
f(x) dx,
a, b ∈R,
a ≤b.
(3.25)
(1) f(x) ≥0
for
−∞< x < ∞
(2)
 ∞
−∞
f(x) dx = 1
(3) Prob [X = c] = 0
for
c ∈R.
3.4.2.2
Cumulative distribution function
The cumulative distribution function (cdf), F(x), for a continuous random
variable X is deﬁned by
F(x) = Prob [X ≤x] =
 x
−∞
f(y) dy
−∞< x < ∞.
(3.26)
(1)
lim
x→−∞F(x) = 0
(2)
lim
x→∞F(x) = 1
(3) If a and b are real numbers such that a < b, then F(a) ≤F(b).
(4) Prob [a ≤X ≤b] = Prob [X ≤b] −Prob [X < a] = F(b) −F(a),
a, b, ∈R and a < b.
(5) The pdf f(x) may be found from the cdf:
f(x) = dF(x)
dx
whenever the derivative exists.
(3.27)
3.4.3
Random functions
A random function of a real variable t is a function, denoted X(t), that is a
random variable for each value of t. If the variable t can assume any value in
an interval, then X(t) is called a stochastic process; if the variable t can only
assume discrete values then X(t) is called a random sequence.
3.5
MATHEMATICAL EXPECTATION
3.5.1
Expected value
(1) If X is a discrete random variable with pmf p(x):
(a) The expected value of X is
E [X] = µ =

x
xp(x) ,
(3.28)
c
⃝2000 by Chapman & Hall/CRC

(b) The expected value of a function g(X) is
E [g(X)] = µg(X) =

x
g(x)p(x) .
(3.29)
(2) If X is a continuous random variable with pdf f(x):
(a) The expected value of X is
E [X] = µ =
 ∞
−∞
xf(x) dx ,
(3.30)
(b) The expected value of a function g(X) is
E [g(X)] = µg(X) =
 ∞
−∞
g(x)f(x) dx .
(3.31)
(3) Jensen’s inequality
Let h(x) be a function such that d2
dx2 [h(x)] ≥0, then
E [h(X)] ≥h(E [X]).
(4) Theorems:
(a) E [aX + bY ] = aE [X] + bE [Y ]
(b) E [X · Y ] = E [X] · E [Y ] if X and Y are independent.
3.5.2
Variance
The variance of a random variable X is
σ2 = E

(X −µ)2
=








x
(x −µ)2p(x)
if X is discrete
 ∞
−∞
(x −µ)2f(x) dx
if X is continuous
(3.32)
The standard deviation of X is σ =
√
σ2 .
3.5.2.1
Theorems
Suppose X is a random variable, and a, b are constants.
(1) σ2
X = E

X2
−(E [X])2 .
(2) σ2
aX = a2 · σ2
X ,
σaX = |a| · σX .
(3) σ2
X+b = σ2
X .
(4) σ2
aX+b = a2 · σ2
X ,
σaX+b = |a| · σX .
3.5.3
Moments
3.5.3.1
Moments about the origin
The moments about the origin completely characterize a probability distribu-
tion. The rth moment about the origin, r = 0, 1, 2, . . . , of a random variable
c
⃝2000 by Chapman & Hall/CRC

X is
µ′
r = E [Xr] =








x
xrp(x)
if X is discrete
 ∞
−∞
xrf(x) dx
if X is continuous
(3.33)
The ﬁrst moment about the origin is the mean of the random variable: µ′
1 =
E [X] = µ.
3.5.3.2
Moments about the mean
The rth moment about the mean, r = 0, 1, 2, . . . , of a random variable X is
µr = E [(X −µ)r] =








x
(x −µ)rp(x)
if X is discrete
 ∞
−∞
(x −µ)rf(x) dx
if X is continuous
(3.34)
The second moment about the mean is the variance of the random variable:
µ2 = E [(X −µ)r] = σ2 = µ′
2 −µ2.
3.5.3.3
Factorial moments
The rth factorial moment, r = 0, 1, 2, . . . , of a random variable is
µ[r] = E
"
X[r]#
=








x
x[r]p(x)
if X is discrete
 ∞
−∞
x[r]f(x) dx
if X is continuous
(3.35)
where x[r] is the factorial expression
x[r] = x(x −1)(x −2) · · · (x −r + 1) .
(3.36)
3.5.4
Generating functions
3.5.4.1
Moment generating function
The moment generating function (mgf) of a random variable X, where it
exists, is
mX(t) = E

etX
=








x
etxp(x)
if X is discrete
 ∞
−∞
etxf(x) dx
if X is continuous
(3.37)
c
⃝2000 by Chapman & Hall/CRC

The moment generating function mX(t) is the expected value of etX and may
be written as
mX(t) = E

etX
= E

1 + Xt + (Xt)2
2!
+ (Xt)3
3!
+ · · ·

= 1 + µ′
1t + µ′
2
t2
2! + µ′
3
t3
3! + · · ·
(3.38)
The moments µ′
r are the coeﬃcients of tr/r! in equation (3.38). Therefore,
mX(t) generates the moments since the rth derivative of mX(t) evaluated at
t = 0 yields µ′
r:
µ′
r = m(r)
x (0) = drmX(t)
dtr
$$$$
t=0
(3.39)
Theorems: Suppose mX(t) is the moment generating function for the random
variable X and a, b are constants.
(1) maX(t) = mX(at)
(2) mX+b(t) = ebt · mX(t)
(3) m(X+b)/a(t) = e(b/a)t · mX(t/a)
(4) If X1, X2, . . . , Xn are independent random variables and Y = X1+X2+
· · · + Xn, then mY (t) = [mX(t)]n.
The moment generating function for X −µ is
mX−µ(t) = e−µt · mX(t) .
(3.40)
Equation (3.40) may be used to generate the moments about the mean for
the random variable X:
µr = m(r)
X−µ(0) = dr (e−µt · mX(t))
dtr
$$$$
t=0
(3.41)
3.5.4.2
Factorial moment generating functions
The factorial moment generating function of a random variable X is
P(t) = E

tX
=








x
txp(x)
if X is discrete
 ∞
−∞
txf(x) dx
if X is continuous
(3.42)
The rth derivative of the function P (in equation (3.42)) with respect to t,
evaluated at t = 1 is the rth factorial moment. Therefore, the function P
c
⃝2000 by Chapman & Hall/CRC

generates the factorial moments:
µ[r] = P (r)(1) = drP(t)
dtr
$$$$
t=1
.
(3.43)
In particular:
1 = P(1)
“conservation of probability”
µ = P ′(1)
σ2 = P ′′(1) + P ′(1) −[P ′(1)]2
(3.44)
3.5.4.3
Factorial moment generating function theorems
Theorems: Suppose PX(t) is the factorial moment generating function for the
random variable X and a, b are constants.
(1) PaX(t) = PX(ta)
(2) PX+b(t) = tb · PX(t)
(3) P(X+b)/a(t) = tb/a · PX(t1/a)
(4) PX(t) = mX(ln t), where mx(t) is the moment generating function for X.
(5) If X1, X2, . . . , Xn are independent random variables with factorial mo-
ment generating function PX(t) and Y = X1 + X2 + · · · + Xn, then
PY (t) = [PX(t)]n.
3.5.4.4
Cumulant generating function
Let mX(t) be a moment generating function. If ln mX(t) can be expanded in
the form
c(t) = ln mX(t) = κ1t + κ2
t2
2! + κ3
t3
3! + · · · + κr
tr
r! + · · · ,
(3.45)
then c(t) is the cumulant generating function (or semi–invariant generating
function). The constants κr are the cumulants (or semi–invariants) of the
distribution. The rth derivative of c with respect to t, evaluated at 0 is the
rth cumulant. The function c generates the cumulants:
κr = c(r)(0) = drc(t)
dtr
$$$$
t=0
.
(3.46)
Marcienkiewicz’s theorem states that either all but the ﬁrst two cumulants
vanish (i.e., it is a normal distribution) or there are an inﬁnite number of
non-vanishing cumulants.
c
⃝2000 by Chapman & Hall/CRC

3.5.4.5
Characteristic function
The characteristic function exists for every random variable X and is deﬁned
by
φ(t) = E

eitX
=








x
eitxp(x)
if X is discrete
 ∞
−∞
eitxf(x) dx
if X is continuous
(3.47)
where t is a real number and i2 = −1. The rth derivative of φ with respect
to t, evaluated at t = 0 is irµ′
r. Therefore, the characteristic function also
generates the moments:
irµ′
r = φ(r)(0) = drφ(t)
dtr
$$$$
t=0
.
(3.48)
3.6
MULTIVARIATE DISTRIBUTIONS
Note that the specialization to bivariate distributions is on page 45.
3.6.1
Discrete case
A n-dimensional random variable (X1, X2, . . . , Xn) is n-dimensional discrete
if it can assume only a ﬁnite or countably inﬁnite number of values. The joint
probability distribution, joint probability mass function, or joint density, for
(X1, X2, . . . , Xn) is
p(x1, x2, . . . , xn) = Prob [X1 = x1, X2 = x2, . . . , Xn = xn]
∀(x1, x2, . . . , xn) .
(3.49)
Suppose E is a subset of values the random variable may assume. The prob-
ability the event E occurs is
Prob [E] = Prob [(X1, X2, . . . , Xn) ∈E]
=
 
· · ·

(x1,x2,...,xn)∈E
p(x1, x2, . . . , xn) .
(3.50)
The cumulative distribution function for (X1, X2, . . . , Xn) is
F(x1, x2, . . . , xn) =

t1|t1≤x1

t2|t2≤x2
· · ·

tn|tn≤xn
p(x1, x2, . . . , xn) .
(3.51)
3.6.2
Continuous case
The continuous random variables X1, X2, . . . , Xn are jointly distributed if
there exists a function f such that f(x1, x2, . . . , xn) ≥0 for −∞< xi < ∞,
c
⃝2000 by Chapman & Hall/CRC

i = 1, 2, . . . , n, and for any event E
Prob [E] = Prob [(X1, X2, . . . , Xn) ∈E]
=
 
· · ·

E
f(x1, x2, . . . , xn) dxn · · · dx1
(3.52)
where f is the joint distribution function or joint probability density func-
tion for the random variables X1, X2, . . . , Xn. The cumulative distribution
function for X1, X2, . . . , Xn is
F(x1, x2, . . . , xn) =
 x1
−∞
 x2
−∞
· · ·
 xn
−∞
f(x1, x2, . . . , xn) dxn · · · dx1 .
(3.53)
Given the cumulative distribution function, F, the probability density func-
tion may be found by
f(x1, x2, . . . , xn) =
∂n
∂x1 ∂x2 · · · ∂xn
F(x1, x2, . . . , xn)
(3.54)
wherever the partials exist.
3.6.3
Expectation
Let g(X1, X2, . . . , Xn) be a function of the random variables X1, . . . , Xn. The
expected value of g(X1, X2, . . . , Xn) is
E [g(X1, X2, . . . , Xn)] =

x1

x2
· · ·

xn
g(x1, x2, . . . , xn)p(x1, x2, . . . , xn)
(3.55)
if X1, X2, . . . , Xn are discrete, and
E [g(X1, X2, . . . , Xn)] =
 ∞
−∞
 ∞
−∞
· · ·
 ∞
−∞
g(x1, . . . , xn)f(x1, . . . , xn) dxn · · · dx1
(3.56)
if X1, X2, . . . , Xn are continuous.
If c1, c2, . . . , cn are constants, then
E
% n

i=1
cigi(X1, X2, . . . , Xn)
&
=
n

i=1
ciE [gi(X1, X2, . . . , Xn)] .
(3.57)
3.6.4
Moments
If X1, X2, . . . , Xn are jointly distributed, the rth moment of Xi is
E [Xr
i ] =

x1

x2
· · ·

xn
xr
i p(x1, x2, . . . , xn)
(3.58)
if X1, X2, . . . , Xn are discrete, and
E [Xr
i ] =
 ∞
−∞
 ∞
−∞
· · ·
 ∞
−∞
xr
i f(x1, x2, . . . , xn) dxn · · · dx1
(3.59)
c
⃝2000 by Chapman & Hall/CRC

if X1, X2, . . . , Xn are continuous.
The joint (product) moments about the origin are
E [Xr1
1 Xr2
2 · · · Xrn
n ] =

x1

x2
· · ·

xn
xr1
1 xr2
2 · · · xrn
n p(x1, x2, . . . , xn)
(3.60)
if X1, X2, . . . , Xn are discrete, and
E [Xr1
1 Xr2
2 · · · Xrn
n ]
=
 ∞
−∞
 ∞
−∞
· · ·
 ∞
−∞
xr1
1 xr2
2 · · · xrn
n f(x1, x2, . . . , xn) dxn · · · dx1
(3.61)
if X1, X2, . . . , Xn are continuous. The value r = r1 +r2 +· · ·+rn is the order
of the moment.
If E [Xi] = µi, then the joint moments about the mean are
E [(X1 −µ1)r1(X2 −µ2)r2 · · · (Xn −µn)rn] =

x1

x2
· · ·

xn
(x1 −µ1)r1(x2 −µ2)r1 · · · (xn −µn)rnp(x1, x2, . . . , xn)
(3.62)
if the X1, X2, . . . , Xn are discrete, and
E [(X1 −µ1)r1(X2 −µ2)r2 · · · (Xn −µn)rn] =
 ∞
−∞
 ∞
−∞
· · ·
 ∞
−∞
(x1 −µ1)r1 · · · (xn −µn)rnf(x1, . . . , xn) dxn · · · dx1 ,
(3.63)
if the X1, X2, . . . , Xn are continuous.
3.6.5
Marginal distributions
Let X1, X2, . . . , Xn be a collection of random variables. The marginal dis-
tribution of a subset of the random variables X1, X2, . . . , Xk (with (k < n))
is
g(x1, x2, . . . , xk) =

xk+1

xk+2
· · ·

xn
p(x1, x2, . . . , xn)
(3.64)
if X1, X2, . . . , Xn are discrete, and
g(x1, x2, . . . , xk) =
 ∞
−∞
 ∞
−∞
· · ·
 ∞
−∞
f(x1, x2, . . . , xn) dxk+1dxk+2 · · · dxn
(3.65)
if X1, X2, . . . , Xn are continuous.
Example 3.18:
The joint density functions g(x, y) = x+y and h(x, y) = (x+ 1
2)(y+ 1
2)
when 0 ≤x ≤1 and 0 ≤y ≤1 have the same marginal distributions. Using equation
c
⃝2000 by Chapman & Hall/CRC

(3.65):
gx(x) =
 1
0
g(x, y) dy =

xy + y2
2
$$$$
y=1
y=0
= x + 1
2
hx(x) =
 1
0
h(x, y) dy =

x + 1
2
 y2
2 + y
2
$$$$
y=1
y=0
= x + 1
2
(3.66)
and, by symmetry, gy(y) has the same form as gx(x) (likewise for hy(y) and hx(x)).
3.6.6
Independent random variables
Let X1, X2, . . . , Xn be a collection of discrete random variables with joint
probability distribution function p(x1, x2, . . . , xn). Let gXi(xi) be the marginal
distribution for Xi. The random variables X1, X2, . . . , Xn are independent if
and only if
p(x1, x2, . . . , xn) = gX1(x1) · gX2(x2) · · · gXn(xn) .
(3.67)
Let X1, X2, . . . , Xn be a collection of continuous random variables with joint
probability distribution function f(x1, x2, . . . , xn). Let gXi(xi) be the marginal
distribution for Xi. The random variables X1, X2, . . . , Xn are independent if
and only if
f(x1, x2, . . . , xn) = gX1(x1) · gX2(x2) · · · gXn(xn) .
(3.68)
Example 3.19:
Suppose X1, X2, and X3 are independent random variables with
probability density functions given by
gX1(x1) =
' e−x1 x1 > 0
0
elsewhere
gX2(x2) =
' 3e−3x2 x2 > 0
0
elsewhere
gX3(x3) =
' 7e−7x3 x3 > 0
0
elsewhere
Using equation (3.68), the joint probability distribution for X1, X2, X3 is
f(x1, x2, x3) = gX1(x1) · gX2(x2) · gX3(x3)
= (e−x1) · (3e−3x2) · (7e−7x3)
= 21e−x1−3x2−7x3
[x1 > 0, x2 > 0, x3 > 0].
3.6.7
Conditional distributions
Let X1, X2, . . . , Xn be a collection of random variables. The conditional dis-
tribution of any subset of the random variables X1, X2, . . . , Xk given Xk+1 =
xk+1, Xk+2 = xk+2, . . . , Xn = xn is
p(x1, x2, . . . , xk |xk+1, xk+2 . . . , xn) =
p(x1, x2, . . . , xn)
g(xk+1, xk+2, . . . , xn)
(3.69)
c
⃝2000 by Chapman & Hall/CRC

if X1, X2, . . . , Xn are discrete with joint distribution function
p(x1, x2, . . . , xn) and Xk+1, Xk+2, . . . , Xn have marginal distribution
g(xk+1, xk+2, . . . , xn) ̸= 0, and
f(x1, x2, . . . , xk |xk+1, xk+2, . . . , xn) =
f(x1, x2, . . . , xn)
g(xk+1, xk+2, . . . , xn)
(3.70)
if X1, X2, . . . , Xn are continuous with joint distribution function
f(x1, x2, . . . , xn) and Xk+1, Xk+2, . . . , Xn have marginal distribution
g(xk+1, xk+2, . . . , xn) ̸= 0.
Example 3.20:
Suppose X1, X2, X3 have a joint distribution function given by
f(x1, x2, x3) =
(
(x1 + x2)e−x3
when 0 < x1 < 1, 0 < x2 < 1, x3 > 0
0
elsewhere
The marginal distribution of X2 is
g(x2) =
 1
0
 ∞
0
(x1 + x2)e−x3 dx3 dx1
=
 1
0
(x1 + x2) dx1 = 1
2 + x2,
0 < x2 < 1.
The conditional distribution of X1, X3 given X2 = x2 is
f(x1, x3 | x2) = f(x1, x2, x3)
g(x2)
= (x1 + x2)e−x3
1
2 + x2
.
If X2 = 3/4, then
f(x1, x3 | 3/4) =

x1 + 3
4

e−x3
1
2 + 3
4
= 4
5

x1 + 3
4

e−x3,
0 < x1 < 1, x3 > 0.
3.6.8
Variance and covariance
Let X1, X2, . . . , Xn be a collection of random variables. The variance, σii, of
Xi is
σii = σ2
i = E

(Xi −µi)2
(3.71)
and the covariance, σij, of Xi and Xj is
σij = ρijσiσj = E [(Xi −µi)(Xj −µj)]
(3.72)
where ρij is the correlation coeﬃcient and σi and σj are the standard devia-
tions of Xi and Xj, respectively.
Theorems:
(1) If X1, X2, . . . , Xn are independent, then
E [X1X2 · · · Xn] = E [X1]E [X2] · · · E [Xn] .
(3.73)
c
⃝2000 by Chapman & Hall/CRC

(2) For two random variables Xi and Xj:
σij = E [XiXj] −E [Xi]E [Xj] .
(3.74)
(3) If Xi and Xj are independent random variables, then σij = 0.
(4) Two variables may be dependent and have zero covariance. For example,
let X take the four values {−2, −1, 1, 2} with equal probability. If Y =
X2 then the covariance of X and Y is zero.
The correlation function of a random function (see page 34) is
KX(t1, t2) = E [[X∗(t1) −µX∗(t1)] [X(t2) −µX(t2)]]
(3.75)
where ∗denotes the complex conjugate. If X(t) is stationary then
KX(t1, t2) = KX(t1 −t2)
and
µX(t) = constant.
(3.76)
3.6.9
Correlation coeﬃcient
The correlation coeﬃcient, deﬁned by (see equation (3.72))
ρij = σij
σiσj
(3.77)
is no greater than one in magnitude: |ρij| ≤1. Figure 3.5 contains 4 data
sets of 100 points each; the correlation coeﬃcients vary from −0.7 to 0.99.
···
·
·
······
····
·
·
··
···
···
·
·
·
·
·
·
··
···
··
·
·
·
·
·
·
·
·
···
··
·
·
··
·
·
··
··
·
··
·
··
····
·
·
·
·
···
··
·
·
··
···
·
·
···
·
·
·
·
··
··
ρ=−0.71
··
·
···
·
··
·
···
·
·
···
··
·
·
·
··
···
·
·
·
··
··
·
·····
·
·
·
·
··
········
·
·
····
·
·
··
··
·
·
·
·
·
·
·
··
··
·
·
·
·
··
·
·
···
·
·
·
··
·
··
·
·
·
ρ=0.09
···
·
·
·
·
·
·
··
·
·
··
·
·
·
··
·
·
·
·
·
···
·
···
·
··
·
··
···
···
·
·
··
·
·
·
··
·
·
·
··
·
···
·····
·
·
·
·
·
·
··
···
·
·
·
·
·
·
·
·
··
·
··
··
·
··
···
·
ρ=0.28
····································································································
ρ=0.99
Figure 3.5: Data sets illustrating diﬀerent correlation coeﬃcients.
Example 3.21:
The correlation coeﬃcient of the ﬁrst 100 integers {1, 2, 3 . . . } and
the ﬁrst 100 squares {1, 4, 9 . . . } is 0.96885.
3.6.10
Moment generating function
Let X1, X2, . . . , Xn be a collection of random variables. The joint moment
generating function is
m(t1, t2, . . . , tn) = m(t) = E

et1X1+t2X2+···+tnXn
= E

et·X
(3.78)
c
⃝2000 by Chapman & Hall/CRC

if it exists for all values of ti such that |ti| < h2 (for some value h).
The rth moment of Xi may be obtained (generated) by diﬀerentiating m(t1, t2, . . . , tn)
r times with respect to ti, and then evaluating the result with all t’s equal to
zero:
E [Xr
i ] = ∂rm(t1, t2, . . . , tn)
∂tr
i
$$$$
(t1,t2,...,tn)=(0,0,...,0)
(3.79)
The rth joint moment, r = r1+r2+· · ·+rn, may be obtained by diﬀerentiating
m(t1, t2, . . . , tn) r1 times with respect to t1, r2 times with respect to t2, . . . ,
and rn times with respect to tn, and then evaluating the result with all t’s
equal to zero:
E [Xr1
1 Xr2
2 · · · Xrn
n ] = ∂rm(t1, t2, . . . , tn)
∂tr1
1 ∂tr2
2 · · · ∂trn
n
$$$$
(t1,t2,...,tn)=(0,0,...,0)
(3.80)
3.6.11
Linear combination of random variables
Let X1, X2, . . . , Xm and Y1, Y2, . . . , Yn be random variables, let a1, a2, . . . , am
and b1, b2, . . . , bn be constants, and let U and V be the linear combinations
U =
m

i=1
aiXi ,
V =
n

j=1
bjYj .
(3.81)
Theorems:
(1) E [U] =
m

i=1
aiE [Xi] .
(2) σ2
i =
m

i=1
a2
i σ2
i + 2
 
i<j
aiajσij,
where the double sum extends over all pairs (i, j) with i < j.
(3) If the random variables X1, X2, . . . , Xm are independent, then
σ2
U =
m

i=1
a2
i σ2
i .
(4) σUV =
m

i=1
n

j=1
aibjσij.
3.6.12
Bivariate distribution
3.6.12.1
Joint probability distribution
(a) Discrete case
Let X and Y be discrete random variables. The joint probability distri-
bution for X and Y is
p(x, y) = Prob [X = x, Y = y]
∀(x, y) .
(3.82)
c
⃝2000 by Chapman & Hall/CRC

(1) For any subset E consisting of pairs (x, y),
Prob [(X, Y ) ∈E] =
 
(x,y)∈E
p(x, y).
(3.83)
(2) p(x, y) ≥0
∀(x, y).
(3)

x

y
p(x, y) = 1.
(b) Continuous case
Let X and Y be continuous random variables. The joint probability
distribution for X and Y is a function f(x, y) such that for any two-
dimensional set E
Prob [(X, Y ) ∈E] =
 
E
f(x, y) dx dy .
(3.84)
(1) If E is a rectangle {(x, y) | a ≤x ≤b, c ≤y ≤d}, then
Prob [(X, Y ) ∈E] = Prob [a ≤X ≤b, c ≤Y ≤d]
=
 b
a
 d
c
f(x, y) dy dx .
(3.85)
(2) f(x, y) ≥0
∀(x, y).
(3)
 ∞
−∞
 ∞
−∞
f(x, y) dx dy = 1.
3.6.12.2
Cumulative distribution function
For any two random variables X and Y the cumulative distribution function
is F(x, y) = Prob [X ≤x, Y ≤y]:
F(a, b) =










x|x≤a

y|y≤b
p(x, y)
if X and Y are discrete
 a
−∞
 b
−∞
f(x, y) dy dx
if X and Y are continuous
(3.86)
Properties:
(1)
lim
(x,y)→(−∞,−∞) F(x, y) =
lim
x→−∞F(x, y) =
lim
y→−∞F(x, y) = 0.
(2)
lim
(x,y)→(∞,∞) F(x, y) = 1.
(3) If a ≤b and c ≤d, then
Prob [a < X ≤b, c < Y ≤d] = F(b, d) −F(b, c) −F(a, d) + F(a, c)
≥0.
(3.87)
(4) Given the cumulative distribution function, F, for the continuous ran-
dom variables X and Y , the probability density function may be found
c
⃝2000 by Chapman & Hall/CRC

by
f(x, y) =
∂2
∂x∂y F(x, y)
(3.88)
wherever the partials exist.
3.6.12.3
Marginal distributions
Let X and Y be discrete random variables with joint distribution p(x, y). The
marginal distributions for X and Y are
pX(x) =

y
p(x, y) ,
pY (y) =

x
p(x, y) .
(3.89)
Let X and Y be continuous random variables with joint distribution f(x, y).
The marginal distributions for X and Y are
fX(x) =
 ∞
−∞
f(x, y) dy,
−∞< x < ∞
fY (y) =
 ∞
−∞
f(x, y) dx,
−∞< y < ∞.
(3.90)
3.6.12.4
Conditional distributions
Let X and Y be discrete random variables with joint distribution p(x, y) and
let pY (y) be the marginal distribution for Y . The conditional distribution for
X given Y = y is
p(x|y) = p(x, y)
pY (y) ,
pY (y) ̸= 0.
(3.91)
Let pX(x) be the conditional distribution for X. The conditional distribution
for Y given X = x is
p(y|x) = p(x, y)
pX(x) ,
pX(x) ̸= 0.
(3.92)
Let X and Y be continuous random variables with joint distribution f(x, y)
and let fY (y) be the marginal distribution for Y . The conditional distribution
for X given Y = y is
f(x|y) = f(x, y)
fY (y) ,
fY (y) ̸= 0.
(3.93)
Let fX(x) be the conditional distribution for X. The conditional distribution
for Y given X = x is
f(y|x) = f(x, y)
fX(x) ,
fX(x) ̸= 0.
(3.94)
c
⃝2000 by Chapman & Hall/CRC

3.6.12.5
Conditional expectation
Let X and Y be random variables and let g(X) be a function of X. The
conditional expectation of g(X) given Y = y is
E [g(X)|y] =








x
g(x)p(x|y)
if X and Y are discrete
 ∞
−∞
g(x)f(x|y) dx
if X and Y are continuous
(3.95)
Properties:
(1) The conditional mean, or conditional expectation, of X given Y = y is
µX|y = E [X |y] =








x
xp(x|y)
if X and Y are discrete
 ∞
−∞
xf(x|y) dx
if X and Y are continuous
(3.96)
(2) The conditional variance of X given Y = y is
σ2
X|y = E

(X −µX|y)2 |y

= E

X2 |y

−µ2
X|y.
(3.97)
(3) E [X] = E [E [X |Y ]]
3.7
INEQUALITIES
1. Bienaym´e–Chebyshev’s inequality:
If E [|X|r] < ∞for all r > 0 (r not
necessarily an integer) then, for every a > 0
Prob [|X| ≥a] ≤E [|X|r]
ar
(3.98)
2. Bienaym´e–Chebyshev’s inequality (generalized):
Let g(x) be a non-
decreasing nonnegative function deﬁned on (0, ∞). Then, for a ≥0,
Prob [|X| ≥a] ≤E [g(|X|)]
g(a)
(3.99)
3. Cauchy–Schwartz inequality:
Let X and Y be random variables in
which E

Y 2
and E

Z2
exist, then
(E [Y Z])2 ≤E

Y 2
E

Z2
(3.100)
4. Chebyshev inequality: Let c be any real number and let X be a random
variable for which E

(X −c)2
is ﬁnite.
Then for every ϵ > 0 the
following holds
Prob [|X −c| ≥ϵ] ≤1
ϵ2 E

(X −c)2
(3.101)
c
⃝2000 by Chapman & Hall/CRC

5. Chebyshev inequality (one-sided):
Let X be a random variable with
zero mean (i.e., E [X] = 0) and variance σ2. Then for any positive a
Prob [X > a] ≤
σ2
σ2 + a2
(3.102)
6. Chernoﬀbound:
This bound is useful for sums of random variables.
Let Yn = n
i=1 Xi where each of the Xi is iid. Let mX(t) = E

etX
be the common moment generating function for the {Xi}, and deﬁne
c(t) = log mX(t). Then
Prob [Yn ≥nc′(t)] ≤e−n[tc′(t)−c(t)]
if t ≥0
Prob [Yn ≤nc′(t)] ≤e−n[tc′(t)−c(t)]
if t ≤0
(3.103)
7. Jensen’s inequality: If E [X] exists, and if f(x) is a convex ∪(“convex
cup”) function, then
E [f(X)] ≥f(E [X])
(3.104)
8. Kolmogorov’s inequality: Let X1, X2, . . . , Xn be n independent random
variables such that E [Xi] = 0 and Var(Xi) = σ2
Xi is ﬁnite. Then, for all
a > 0,
Prob

max
i=1,...,n |X1 + X2 + · · · + Xi| > a

≤
n

i=1
σ2
i
a2
(3.105)
9. Kolmogorov’s inequality: Let X1, X2, . . . , Xn be n mutually indepen-
dent random variables with expectations µi = E [Xi] and variances σ2
k.
Deﬁne the sums Sk = X1 +· · ·+Xk so that mk = E [Sk] = µ1 +· · ·+µk
and s2
k = Var [Sk] = σ2
1 + · · · + σ2
k. For every t > 0, the probability of
the simulteneous realization of the n inequalities
|Sk −mk| < t sk
(3.106)
is at least 1 −t−2. (When n = 1 this is Chebyshev’s inequality.)
10. Markov’s inequality: If X is random variable which takes only nonneg-
ative values, then for any a > 0
Prob [X ≥a] ≤E [X]
a
.
(3.107)
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 4
Functions of Random
Variables
Contents
4.1
Finding the probability distribution
4.1.1
Method of distribution functions
4.1.2
Method of transformations (one variable)
4.1.3
Method of transformations (many variables)
4.1.4
Method of moment generating functions
4.2
Sums of random variables
4.2.1
Deterministic sums of random variables
4.2.2
Random sums of random variables
4.3
Sampling distributions
4.3.1
Deﬁnitions
4.3.2
The sample mean
4.3.3
Central limit theorem
4.3.4
The law of large numbers
4.3.5
Laws of the iterated logarithm
4.4
Finite population
4.5
Theorems
4.5.1
Theorems: the chi–square distribution
4.5.2
Theorems: the t distribution
4.5.3
Theorems: the F distribution
4.6
Order statistics
4.6.1
Deﬁnition
4.6.2
The ﬁrst order statistic
4.6.3
The nth order statistic
4.6.4
The median
4.6.5
Joint distributions
4.6.6
Midrange and range
4.6.7
Uniform distribution: order statistics
4.6.8
Normal distribution: order statistics
4.7
Range and studentized range
4.7.1
Probability integral of the range
4.7.2
Percentage points, studentized range
c
⃝2000 by Chapman & Hall/CRC

Let X1, X2, . . . , Xn be a collection of random variables with joint probability
mass function p(x1, x2, . . . , xn) (if the collection is discrete) or joint density
function f(x1, x2, . . . , xn) (if the collection is continuous). Suppose the ran-
dom variable Y = Y (X1, X2, . . . , Xn) is a function of X1, X2, . . . , Xn. Meth-
ods for ﬁnding the distribution of Y are presented below and sampling distri-
butions are discussed in the following section.
4.1
FINDING THE PROBABILITY DISTRIBUTION
The following techniques may be used to determine the probability distribu-
tion for Y = Y (X1, X2, . . . , Xn).
4.1.1
Method of distribution functions
Let X1, X2, . . . , Xn be a collection of continuous random variables.
(1) Determine the region Y = y.
(2) Determine the region Y ≤y.
(3) Compute F(y) = Prob [Y ≤y] by integrating the joint density function
f(x1, x2, . . . , xn) over the region Y ≤y.
(4) Compute the probability density function for Y , f(y), by diﬀerentiating
F(y):
f(y) = dF(y)
dy
.
(4.1)
Example 4.22:
Suppose the joint density function of X1 and X2 is given by
f(x1, x2) =
(
4x1x2e−(x2
1+x2
2)
for x1 > 0, x2 > 0
0
elsewhere
and Y =

X2
1 + X2
2. Find the cumulative distribution function for Y and the proba-
bility density function for Y .
Solution:
(S1) The region Y ≤y is a quarter circle in quadrant I, shown shaded in Figure 4.1.
Figure 4.1: Integration region for example 4.22.
c
⃝2000 by Chapman & Hall/CRC

(S2) The cumulative distribution function for Y is given by
F(y) =
 y
0
 √
y2−x2
1
0
4x1x2e−(x2
1+x2
2) dx2 dx1
=
 y
0
2x1(e−x2
1 −e−y2) dx1
= 1 −(1 + y2)e−y2
(4.2)
(S3) The probability density function for Y is given by
f(y) = F ′(y) = −[(2y)e−y2 + (1 + y2)(−2y)e−y2]
= 2y3e−y2,
when y > 0
(4.3)
4.1.2
Method of transformations (one variable)
Let X be a continuous random variable with probability density function
fX(x). If u(x) is diﬀerentiable and either increasing or decreasing, then Y =
u(X) has probability density function
fY (y) = fX(w(y)) · |w′(y)|,
u′(x) ̸= 0
(4.4)
where x = w(y) = u−1(y).
Example 4.23:
Let X be a standard normal random variable, and let Y = X2. What
is the distribution of Y ?
Solution:
(S1) Since X can be both positive and negative, two regions of X correspond to the
same value of Y .
(S2) The computation is
fy(y) =

fx(x) + fx(−x)
 $$$$
dy
dx
$$$$
=
%
e−x2/2
√
2π
+ e−(−x)2/2
√
2π
+
&
1
2√y
=
1
√2πy e−x2/2
=
1
√2πy e−y/2
(4.5)
which is the probability density function for a chi–square random variable with
one degree of freedom.
Example 4.24:
Given two independent random variables X and Y with joint probability
density f(x, y), let U = X/Y be the ratio distribution. The probability density is:
fU(u) =
 ∞
−∞
|x| f(x, ux) dx
(4.6)
c
⃝2000 by Chapman & Hall/CRC

If X and Y are normally distributed, then U has a Cauchy distribution. If X and Y are
uniformly distributed on [0, 1], then
fU(u) =





0
for u < 0
1/2
for 0 ≤u ≤1
1
2u2
for u > 1
(4.7)
4.1.3
Method of transformations (two or more variables)
Let X1 and X2 be continuous random variables with joint density function
f(x1, x2). Let the functions y1 = u1(x1, x2) and y2 = u2(x1, x2) represent a
one–to–one transformation from the x’s to the y’s and let the partial deriva-
tives with respect to both x1 and x2 exist.
The joint density function of
Y1 = u1(X1, X2) and Y2 = u2(X1, X2) is
g(y1, y2) = f(w1(y1, y2), w2(y1, y2)) · |J|
(4.8)
where y1 = u1(x1, x2) and y2 = u2(x1, x2) are uniquely solved for x1 =
w1(y1, y2) and x2 = w2(y1, y2), and J is the determinant of the Jacobian
J =
$$$$$$$$
∂x1
∂y1
∂x1
∂y2
∂x2
∂y1
∂x2
∂y2
$$$$$$$$
.
(4.9)
This method of transformations may be extended to functions of n random
variables. Let X1, X2, . . . , Xn be continuous random variables with joint den-
sity function f(x1, x2, . . . , xn). Let the functions y1 = u1(x1, x2, . . . , xn), y2 =
u2(x1, x2, . . . , xn), . . . , yn = un(x1, x2, . . . , xn) represent a one–to–one trans-
formation from the x’s to the y’s and let the partial derivatives with respect
to x1, x2, . . . , xn exist. The joint density function of Y1 = u1(X1, X2, . . . , Xn),
Y2 = u2(X1, X2, . . . , Xn), . . . , Yn = un(X1, X2, . . . , Xn) is
g(y1, y2, . . . , yn) = f(w1(y1, . . . , yn), . . . , wn(y1, . . . , yn)) · |J|
(4.10)
where the functions y1 = u1(x1, x2, . . . , xn), y2 = u2(x1, x2, . . . , xn), . . . ,
yn = un(x1, x2, . . . , xn) are uniquely solved for x1 = w1(y1, y2, . . . , yn), x2 =
w2(y1, y2, . . . , yn), . . . , xn = wn(y1, y2, . . . , yn) and J is the determinant of
the Jacobian
J =
$$$$$$$$$$$$$$$$
∂x1
∂y1
∂x1
∂y2
· · ·
∂x1
∂yn
∂x2
∂y1
∂x2
∂y2
· · ·
∂x2
∂yn
...
...
...
...
∂xn
∂y1
∂xn
∂y2
· · ·
∂xn
∂yn
$$$$$$$$$$$$$$$$
(4.11)
c
⃝2000 by Chapman & Hall/CRC

Example 4.25:
Suppose the random variables X and Y are independent with prob-
ability density functions fX(x) and fY (y), then the probability density of their sum,
Z = X + Y , is given by
fZ(z) =
 ∞
−∞
fX(t)fY (z −t) dt
(4.12)
Example 4.26:
Suppose the random variables X and Y are independent with proba-
bility density functions fX(x) and fY (y), then the probability density of their product,
Z = XY , is given by
fZ(z) =
 ∞
−∞
1
|t|fX(t)fY
z
t

dt
(4.13)
Example 4.27:
Two random variables X and Y have a joint normal distribution. The
probability density is f(x, y) =
1
2πσ2 exp
x2 + y2
2σ2

. Find the probability density of
the system (R, Φ) if
X = R cos Φ
Y = R sin Φ
(4.14)
Solution:
(S1) Use f(r, φ) = f 
x(r, φ), y(r, φ) $$$ ∂(x,y)
∂(r,φ)
$$$ and
$$$ ∂(x,y)
∂(r,φ)
$$$ = r.
(S2) Then
f(r, φ) =
r
2πσ2 exp

−r2 cos2 φ + r2 sin2 φ
2σ2

=
1
2π
	
fΦ(φ)
r
σ2 exp

−r2
2σ2



	
fR(r)
(4.15)
where fR(r) is a Rayleigh distribution and fΦ(φ) is a uniform distribution.
4.1.4
Method of moment generating functions
To determine the distribution of Y :
(1) Determine the moment generating function for Y , mY (t).
(2) Compare mY (t) with known moment generating functions. If mY (t) =
mU(t) for all t, then Y and U have identical distributions.
Theorems:
(1) Let X and Y be random variables with moment generating functions
mX(t) and mY (t), respectively. If mX(t) = mY (t) for all t, then X and
Y have the same probability distributions.
c
⃝2000 by Chapman & Hall/CRC

(2) Let X1, X2, . . . , Xn be independent random variables and let Y = X1 +
X2 + · · · + Xn, then
mY (t) =
n
)
i=1
mXi(t) .
(4.16)
4.2
SUMS OF RANDOM VARIABLES
4.2.1
Deterministic sums of random variables
If Y = X1 + X2 + · · · + Xn and
(a) the X1, X2, . . . , Xn are independent random variables with factorial mo-
ment generating functions PXi(t), then
PY (t) =
n
)
i=1
PXi(t)
(4.17)
(b) the X1, X2, . . . , Xn are independent random variables with the same
factorial moment generating function PX(t), then
PY (t) = [PX(t)]n
(4.18)
(c) the X1, X2, . . . , Xn are independent random variables with characteris-
tic functions φXi(t), then
φY (t) =
n
)
i=1
φXi(t)
(4.19)
(d) the X1, X2, . . . , Xn are independent random variables with the same
characteristic function φX(t), then
φY (t) = [φX(t)]n
(4.20)
Example 4.28:
What is the distribution of the sum of two normal random variables?
Solution:
(S1) Let X1 be N(µ1, σ1) and let X2 be N(µ2, σ2).
(S2) The characteristic functions are (see page 148) φX1(t) = exp

µ1it −
σ2
1t
2

and
φX2(t) = exp

µ2it −
σ2
2t
2

.
(S3) From equation (4.19) the characteristic function for Y = X1 + X2 is
φY (t) = φX1(t) · φX2(t) = exp

(µ1 + µ2)it −(σ2
1 + σ2
2)t
2

(4.21)
(S4) This last expression is the characteristic function for a normal random variable
with mean µY = µ1 + µ2 and variance of σ2
Y = σ2
1 + σ2
2.
(S5) Conclusion: the distribution of the sum of two normal random variables is normal;
the means add and the variances add.
See section 3.6.11 for linear combinations of random variables.
c
⃝2000 by Chapman & Hall/CRC

4.2.2
Random sums of random variables
If T = N
i=1 Xi where N is an integer valued random variable with factorial
generating function PN(t), the {Xi} are discrete independent and identically
distributed random variables with factorial generating function PX(t), and
the {Xi} are independent of N, then the factorial generating function for T
is
PT (t) = PN(PX(t))
(4.22)
(If the {Xi} are continuous random variables, then φT (t) = PN(φX(t)).)
Hence (using equation (3.44))
µT = µNµX
σ2
T = µNσ2
X + µXσ2
N
(4.23)
Example 4.29:
A game is played as follows: There are two coins used to play the
game. The probability of a head on the ﬁrst coin is p1 and the probability of a head
on the second coin is p2. The ﬁrst coin is tossed. If the resulting toss is a head, the
game is over. If the outcome is a tail, then the second coin is tossed. If the second coin
lands head up, a $1.00 payoﬀis made. There is no payoﬀfor a tail. The ﬁrst coin is
tossed again and the game continues in this manner. What is the expected payoﬀfor
this game?
Solution:
(S1) In this game the number of rounds, N, has a geometric distribution, so that
PN(t) =
p1t
1 −(1 −p1)t.
(S2) Let the random variable X be the payoﬀat each round.
X has a Bernoulli
distribution: PX(t) = (1 −p2) + p2t.
(S3) The generating function for the payoﬀis
PT (t) = PN(PX(t)) =
p1[(1 −p2) + p2t]
1 −(1 −p1)[(1 −p2) + p2t].
(4.24)
(S4) Using PT (t) in equation (3.44) or using equation (4.23) (with µN = 1/p1 and
µX = p2) results in µT = p2/p1.
4.3
SAMPLING DISTRIBUTIONS
4.3.1
Deﬁnitions
(1) The random variables X1, X2, . . . , Xn are a random sample of size n from
an inﬁnite population if X1, X2, . . . , Xn are independent and identically
distributed (iid).
(2) If X1, X2, . . . , Xn are a random sample, then the sample total and sam-
ple mean are
T =
n

i=1
Xi
and
X = 1
n
n

i=1
Xi ,
(4.25)
c
⃝2000 by Chapman & Hall/CRC

respectively. The sample variance is
S2 =
1
n −1
n

i=1
(Xi −X)2.
(4.26)
4.3.2
The sample mean
Consider an inﬁnite population with mean µ, variance σ2, skewness γ1, and
kurtosis γ2. Using a sample of size n, the parameters describing the sample
mean are:
µx = µ
σ2
X = σ2
n
σX =
σ
√n
γ1,X = γ1
√n
γ2,X = γ2
n
(4.27)
When the population is ﬁnite and of size M,
µ(M)
x
= µ
σ2(M)
x
= σ2
N
M −N
M −1
(4.28)
If the underlying population is normal, then the sample mean X is normally
distributed.
4.3.3
Central limit theorem
Let X1, X2, . . . , Xn be a random sample from an inﬁnite population with mean
µ and variance σ2. The limiting distribution of
Z = X −µ
σ/√n
(4.29)
as n →∞is the standard normal distribution. The limiting distribution of
T =
n

i=1
Xi
(4.30)
as n →∞is normal with mean nµ and variance nσ2.
4.3.4
The law of large numbers
Let X1, X2, . . . , Xn be a random sample from an inﬁnite population with mean
µ and variance σ2. For any positive constant c, the probability the sample
mean is within c units of µ is at least 1 −σ2
nc2 :
Prob

µ −c < X < µ + c

≥1 −σ2
nc2 .
(4.31)
c
⃝2000 by Chapman & Hall/CRC

As n →∞the probability approaches 1. (See Chebyshev inequality on page
48.)
4.3.5
Laws of the iterated logarithm
Laws of the iterated logarithm (the following hold “a.s.”, or “almost surely”):
lim sup
t↓0
Wt

2t ln ln(1/t)
= 1
lim sup
t→∞
Wt
√
2t ln ln t
= 1
lim inf
t↓0
Wt

2t ln ln(1/t)
= −1
lim inf
t→∞
Wt
√
2t ln ln t
= −1
where W is a Brownian motion.
4.4
FINITE POPULATION
Let {c1, c2, . . . , cN} be a collection of numbers representing a ﬁnite population
of size N and assume the sampling from this population is done without
replacement.
Let the random variable Xi be the ith observation selected
from the population. The collection X1, X2, . . . , Xn is a random sample of
size n from the ﬁnite population if the joint probability mass function for
X1, X2, . . . , Xn is
p(x1, x2, . . . , xn) =
1
N(N −1) · · · (N −n + 1).
(4.32)
(1) The marginal probability distribution for the random variable Xi, i =
1, 2, . . . , n is
pXi(xi) = 1
N
for
xi = c1, c2, . . . , cN.
(4.33)
(2) The mean and the variance of the ﬁnite population are
µ =
N

i=1
ci
1
N
and
σ2 =
N

i=1
(ci −µ)2 1
N .
(4.34)
(3) The joint marginal probability mass function for any two random vari-
ables in the collection X1, X2, . . . , Xn is
p(xi, xj) =
1
N(N −1).
(4.35)
(4) The covariance between any two random variables in the collection
X1, X2, . . . , Xn is
Cov [Xi, Xj] = −
σ2
N −1.
(4.36)
c
⃝2000 by Chapman & Hall/CRC

(5) Let X be the sample mean of the random sample of size n. The expected
value and variance of X are
E

X

= µ
and
Var

X

= σ2
n · N −n
N −1 .
(4.37)
The quantity (N −n)/(N −1) is the ﬁnite population correction factor.
4.5
THEOREMS
4.5.1
Theorems: the chi–square distribution
(1) Let Z be a standard normal random variable, then Z2 has a chi–square
distribution with 1 degree of freedom.
(2) Let Z1, Z2, . . . , Zn be independent standard normal random variables.
The random variable Y =
n
i=1
Z2
i has a chi–square distribution with n
degrees of freedom.
(3) Let X1, X2, . . . , Xn be independent random variables such that Xi has a
chi–square distribution with νi degrees of freedom. The random variable
Y =
n
i=1
Xi has a chi–square distribution with ν = ν1 + ν2 + · · · + νn
degrees of freedom.
(4) Let U have a chi–square distribution with ν1 degrees of freedom, U
and V be independent, and U + V have a chi–square distribution with
ν > ν1 degrees of freedom. The random variable V has a chi–sqaure
distribution with ν −ν1 degrees of freedom.
(5) Let X1, X2, . . . , Xn be a random sample from a normal population with
mean µ and variance σ2. Then
(a) The sample mean, X, and the sample variance, S2, are indepen-
dent, and
(b) The random variable (n −1)S2
σ2
has a chi–square distribution with
n −1 degrees of freedom.
4.5.2
Theorems: the t distribution
(1) Let Z have a standard normal distribution, X have a chi–square distri-
bution with ν degrees of freedom, and X and Z be independent. The
random variable
T =
Z

X/ν
(4.38)
has a t distribution with ν degrees of freedom.
c
⃝2000 by Chapman & Hall/CRC

(2) Let X1, X2, . . . , Xn be a random sample from a normal population with
mean µ and variance σ2. The random variable
T = X −µ
S/√n
(4.39)
has a t distribution with n −1 degrees of freedom.
4.5.3
Theorems: the F distribution
(1) Let U have a chi–square distribution with ν1 degrees of freedom, V have
a chi–square distribution with ν2 degrees of freedom, and U and V be
independent. The random variable
F = U/ν1
V/ν2
(4.40)
has an F distribution with ν1 and ν2 degrees of freedom.
(2) Let X1, X2, . . . , Xm and Y1, Y2, . . . , Yn be random samples from nor-
mal populations with variances σ2
X and σ2
Y , respectively. The random
variable
F = S2
X/σ2
X
S2y/σ2
Y
(4.41)
has an F distribution with m −1 and n −1 degrees of freedom.
(3) Let Fα,ν1,ν2 be a critical value for the F distribution deﬁned by
Prob [F ≥Fα,ν1,ν2] = α. Then F1−α,ν1,ν2 = 1/Fα,ν2,ν1.
4.6
ORDER STATISTICS
4.6.1
Deﬁnition
Let X1, X2, . . . , Xn be independent continuous random variables with proba-
bility density function f(x) and cumulative distribution function F(x). The
order statistic, X(i), i = 1, 2, . . . , n, is a random variable deﬁned to be the ith
largest of the set {X1, X2, . . . , Xn}. Therefore,
X(1) ≤X(2) ≤· · · ≤X(n)
(4.42)
and in particular
X(1) = min{X1, X2, . . . , Xn}
and
X(n) = max{X1, X2, . . . , Xn}.
(4.43)
The cumulative distribution function for the ith order statistic is
FX(i)(x) = Prob

X(i) ≤x

= Prob [i or more observations are ≤x]
=
n

j=i
n
j

[F(x)]j[1 −F(x)]n−j
(4.44)
c
⃝2000 by Chapman & Hall/CRC

and the probability density function is
fX(i)(x) = n
n −1
i −1

[F(x)]i−1[1 −F(x)]n−if(x)
=
n!
(i −1)!(n −i)![F(x)]i−1f(x)[1 −F(x)]n−i.
(4.45)
4.6.2
The ﬁrst order statistic
The probability density function, fX(1)(x), and the cumulative distribution
function, FX(1)(x), for X(1) are
fX(1)(x) = n[1 −F(x)]n−1f(x)
FX(1)(x) = 1 −[1 −F(x)]n.
(4.46)
4.6.3
The nth order statistic
The probability density function, f(n)(x), and the cumulative distribution
function, F(n)(x), for X(n) are
fX(n)(x) = n[F(x)]n−1f(x)
FX(n)(x) = [F(x)]n.
(4.47)
4.6.4
The median
If the number of observations is odd, the median is the middle observation
when the observations are in numerical order. If the number of observations
is even, the median is (arbitrarily) deﬁned as the average of the middle two
of the ordered observations.
median =
(
X(k)
if n is odd and n = 2k −1
1
2[X(k) + X(k+1)]
if n is even and n = 2k
(4.48)
4.6.5
Joint distributions
The joint density function for X(1), X(2), . . ., X(n) is
g(x1, x2, . . . , xn) = n!f(x1)f(x2) · · · f(xn).
(4.49)
The joint density function for the ith and jth (i < j) order statistics is
fij(x, y) =
n!
(i −1)!(j −i −1)!(n −j)!f(x)f(y)
×[F(x)]i−1[1 −F(y)]n−j[F(y) −F(x)]j−i−1.
(4.50)
The joint distribution function for X(1) and X(n) is
F1n(x, y) = Prob

X(1) ≤x
and
X(n) ≤y

=
(
[F(y)]n −[F(y) −F(x)]n
if x ≤y
[F(y)]n
if x > y
(4.51)
c
⃝2000 by Chapman & Hall/CRC

and the joint density function is
f1n(x, y) =
(
n(n −1)f(x)f(y)[F(y) −F(x)]n−2
if x ≤y
0
if x > y
(4.52)
4.6.6
Midrange and range
The midrange is deﬁned to be A = 1
2

X(1) + X(n)

. Using f1n(x, y) for the
joint density function of X(1) and X(n) results in
fA(x) = 2
 x
−∞
f1n(t, 2x −t) dt
= 2n(n −1)
 x
−∞
f(t)f(2x −t) [F(2x −t) −F(t)]n−2 dt
(4.53)
The range is the diﬀerence between the largest and smallest observations:
R = X(n) −X(1).
The random variable R is used in the construction of
tolerance intervals.
fR(r) =
 ∞
−∞
f1n(t, t + r) dt
=





n(n −1)
 ∞
−∞
f(t)f(t + r)[F(t + r) −F(t)]n−2 dt
if r > 0
0
if r ≤0
(4.54)
4.6.7
Uniform distribution: order statistics
If X is uniformly distributed on the interval [0, 1] then the density function
for X(i) is
fi(x) = n
n −1
i −1

xi−1(1 −x)n−i,
0 ≤x ≤1
(4.55)
which is a beta distribution with parameters i and n −i + 1.
(1) E

X(i)

=
 1
0
fk(t) dt =
i
n + 1.
(2) The expected value of the largest of n observations is
n
n + 1.
(3) The expected value of the smallest of n observations is
1
n + 1.
(4) The density function of the midrange is
fA(x) =
(
n2n−1xn−1
if 0 < x ≤1
2
n2n−1(1 −x)n−1
if 1
2 ≤x < 1
(4.56)
c
⃝2000 by Chapman & Hall/CRC

(5) The density function of the range is
fR(r) =
(
n(n −1)(1 −r)rn−2
if 0 < r < 1
0
otherwise
(4.57)
4.6.7.1
Tolerance intervals
In many applications, we need to estimate an interval in which a certain
proportion of the population lies, with given probability. A tolerance interval
may be constructed using the results relating to order statistics and the range.
A table of required sample sizes for varying ranges and probabilities is in the
following table.
Example 4.30:
Assume a sample is drawn from a uniform population. Find a sample
size n such that at least 99% of the sample population, with probability .95, lies between
the smallest and largest observations. This problem may be written as a probability
statement:
0.95 = Prob [F(Zn) −F(Z1) > 0.99]
= Prob [R > 0.99]
= n(n −1)
 1
0.99
(1 −r)rn−2 dr
= 1 −(0.99)n−1(0.01n + 0.99)
(4.58)
Solving this results in the value n ≈473.
Tolerance intervals, uniform distribution
This fraction of the total population is within the range
Probability
0.500
0.750
0.900
0.950
0.975
0.990
0.995
0.999
0.500
3
7
17
34
67
168
336
1679
0.750
5
10
26
53
107
269
538
2692
0.900
6
14
38
77
154
388
777
3889
0.950
8
17
46
93
188
473
947
4742
0.975
9
20
54
109
221
555
1112
5570
0.990
10
24
64
130
263
661
1325
6636
0.995
11
26
71
145
294
740
1483
7427
For tolerance intervals for normal samples, see section 7.3.
4.6.8
Normal distribution: order statistics
When the {Xi} come from a standard normal distribution, the {X(i)} are
called standard order statistics.
4.6.8.1
Expected value of normal order statistics
The tables on pages 65–66 gives expected values of standard order statistics
E

X(i)

= n
n −1
i −1
  ∞
−∞
tf(t)[F(t)]i−1[1 −F(t)]n−i dt
(4.59)
c
⃝2000 by Chapman & Hall/CRC

when f(x) = e−x2/2
√
2π
and F(x) =
 x
−∞
e−t2/2
√
2π dt. Missing values (indicated
by a dash) may be obtained from E

X(i)

= −E

X(n−i+1)

.
Example 4.31:
If an average person takes ﬁve intelligence tests (each test having a
normal distribution with a mean of 100 and a standard deviation of 20), what is the
expected value of the largest score?
Solution:
(S1) We need to obtain the expected value of the largest normal order statistic when
n = 5.
(S2) Using n = 5 and i = 5 in the table on page 65 yields (use j = 1) E 
X(5)
$$
n=5 =
1.1630.
(S3) The expected value of the largest score is 100 + (1.1630)(20) ≈123.
Expected value of the ith normal order statistic (use j = n −i + 1)
j
n = 2
3
4
5
6
7
8
9
1
0.5642 0.8463 1.0294 1.1629 1.2672 1.3522 1.4236 1.4850
2
—
0.0000 0.2970 0.4950 0.6418 0.7574 0.8522 0.9323
3
—
—
—
0.0000 0.2015 0.3527 0.4728 0.5720
4
—
—
—
—
—
0.0000 0.1526 0.2745
5
—
—
—
—
—
—
—
0.0000
j
n = 10
11
12
13
14
15
16
17
18
19
1
1.5388 1.5865 1.6292 1.6680 1.7034 1.7359 1.7660 1.7939 1.8200 1.8445
2
1.0014 1.0619 1.1157 1.1641 1.2079 1.2479 1.2848 1.3188 1.3504 1.3800
3
0.6561 0.7288 0.7929 0.8498 0.9011 0.9477 0.9903 1.0295 1.0657 1.0995
4
0.3757 0.4619 0.5368 0.6028 0.6618 0.7149 0.7632 0.8074 0.8481 0.8859
5
0.1227 0.2249 0.3122 0.3883 0.4556 0.5157 0.5700 0.6195 0.6648 0.7066
6
—
0.0000 0.1025 0.1905 0.2672 0.3353 0.3962 0.4513 0.5016 0.5477
7
—
—
—
0.0000 0.0882 0.1653 0.2337 0.2952 0.3508 0.4016
8
—
—
—
—
—
0.0000 0.0772 0.1459 0.2077 0.2637
9
—
—
—
—
—
—
—
0.0000 0.0688 0.1307
10
—
—
—
—
—
—
—
—
—
0.0000
j
n = 20
21
22
23
24
25
26
27
28
29
1
1.8675 1.8892 1.9097 1.9292 1.9477 1.9653 1.9822 1.9983 2.0137 2.0285
2
1.4076 1.4336 1.4581 1.4813 1.5034 1.5243 1.5442 1.5632 1.5814 1.5988
3
1.1310 1.1605 1.1883 1.2145 1.2393 1.2628 1.2851 1.3064 1.3268 1.3462
4
0.9210 0.9538 0.9846 1.0136 1.0409 1.0668 1.0914 1.1147 1.1370 1.1582
5
0.7454 0.7816 0.8153 0.8470 0.8769 0.9051 0.9318 0.9571 0.9812 1.0042
6
0.5903 0.6298 0.6667 0.7012 0.7336 0.7641 0.7929 0.8202 0.8462 0.8709
7
0.4483 0.4915 0.5316 0.5690 0.6040 0.6369 0.6679 0.6973 0.7251 0.7515
8
0.3149 0.3620 0.4056 0.4461 0.4839 0.5193 0.5527 0.5841 0.6138 0.6420
9
0.1869 0.2384 0.2857 0.3296 0.3704 0.4086 0.4443 0.4780 0.5098 0.5398
10
0.0620 0.1183 0.1699 0.2175 0.2616 0.3026 0.3410 0.3770 0.4109 0.4430
11
—
0.0000 0.0564 0.1081 0.1558 0.2000 0.2413 0.2798 0.3160 0.3501
12
—
—
—
0.0000 0.0518 0.0995 0.1439 0.1852 0.2239 0.2602
13
—
—
—
—
—
0.0000 0.0478 0.0922 0.1336 0.1724
14
—
—
—
—
—
—
—
0.0000 0.0444 0.0859
15
—
—
—
—
—
—
—
—
—
0.0000
c
⃝2000 by Chapman & Hall/CRC

Expected value of the ith normal order statistic (use j = n −i + 1)
j
n = 30
31
32
33
34
35
36
37
38
39
1
2.0427 2.0564 2.0696 2.0824 2.0947 2.1066 2.1181 2.1292 2.1401 2.1505
2
1.6156 1.6316 1.6471 1.6620 1.6763 1.6902 1.7036 1.7165 1.7291 1.7413
3
1.3648 1.3827 1.3999 1.4164 1.4323 1.4476 1.4624 1.4768 1.4906 1.5040
4
1.1786 1.1980 1.2167 1.2347 1.2520 1.2686 1.2847 1.3002 1.3151 1.3296
5
1.0262 1.0472 1.0673 1.0866 1.1052 1.1230 1.1402 1.1568 1.1729 1.1884
6
0.8944 0.9169 0.9385 0.9591 0.9789 0.9979 1.0163 1.0339 1.0510 1.0674
7
0.7767 0.8007 0.8236 0.8456 0.8666 0.8868 0.9063 0.9250 0.9430 0.9604
8
0.6689 0.6944 0.7188 0.7420 0.7644 0.7857 0.8063 0.8261 0.8451 0.8634
9
0.5683 0.5954 0.6213 0.6460 0.6695 0.6921 0.7138 0.7346 0.7547 0.7740
10
0.4733 0.5020 0.5294 0.5555 0.5804 0.6043 0.6271 0.6490 0.6701 0.6904
11
0.3823 0.4129 0.4418 0.4694 0.4957 0.5208 0.5449 0.5679 0.5900 0.6113
12
0.2945 0.3268 0.3575 0.3867 0.4144 0.4409 0.4662 0.4904 0.5136 0.5359
13
0.2088 0.2432 0.2757 0.3065 0.3358 0.3637 0.3903 0.4157 0.4401 0.4635
14
0.1247 0.1613 0.1957 0.2283 0.2592 0.2886 0.3166 0.3433 0.3689 0.3934
15
0.0415 0.0804 0.1170 0.1515 0.1842 0.2151 0.2446 0.2727 0.2995 0.3252
16
—
0.0000 0.0389 0.0755 0.1101 0.1428 0.1739 0.2034 0.2316 0.2585
17
—
—
—
0.0000 0.0367 0.0713 0.1040 0.1351 0.1647 0.1929
18
—
—
—
—
—
0.0000 0.0346 0.0674 0.0986 0.1282
19
—
—
—
—
—
—
—
0.0000 0.0328 0.0640
20
—
—
—
—
—
—
—
—
—
0.0000
4.6.8.2
Variances and covariances of order statistics
Given n observations of independent standard normal variables, arrange the
sample in ascending order of magnitude X(1), X(2), . . . , X(n). The variances
and covariances for expected values and product moments may be found from
E

X(i)

= n
n −1
n −i
  ∞
−∞
tf(t)F i−1(t)[1 −F(t)]n−i dt
E
"
X2
(i)
#
= n
n −1
n −i
  ∞
−∞
t2f(t)F i−1(t)[1 −F(t)]n−i dt
E

X(i)X(j)

= n
n −1
n −i
  ∞
−∞
 y
−∞
tyf(t)f(y)
× [F(t)]i−1[1 −F(y)]n−j[F(y) −F(t)]j−i−1 dt dy
(4.60)
where f(x) = e−x2/2
√
2π
and F(x) =
 x
−∞
e−x2/2
√
2π dx.
The following table gives the variances and covariances of order statistics in
samples of sizes up to 10 from a standard normal distribution. Missing values
may be obtained from E

X(i)X(j)

= E

X(j)X(i)

= E

X(n−i+1)X(n−j+1)

See G. L. Tietjen, D. K. Kahaner, and R. J. Beckman, “Variances and covari-
ances of the normal order statistics for samples sizes 2 to 50”, Selected Tables
in Mathematical Statistics, 5, American Mathematical Society, Providence,
RI, 1977.
c
⃝2000 by Chapman & Hall/CRC

Variances and covariances of normal order statistics
E

X(i)X(j)

is shown for samples of size n
(use k = n −i + 1 and ℓ= n −j + 1)
n
k
ℓ
value
2
1
1
.6817
2
.3183
2
2
.6817
3
1
1
.5595
2
.2757
3
.1649
2
2
.4487
4
1
1
.4917
2
.2456
3
.1580
4
.1047
2
2
.3605
3
.2359
5
1
1
.4475
2
.2243
3
.1481
4
.1058
5
.0742
2
2
.3115
3
.2084
4
.1499
3
3
.2868
6
1
1
.4159
2
.2085
3
.1394
4
.1024
5
.0774
6
.0563
2
2
.2796
3
.1890
4
.1397
5
.1059
3
3
.2462
4
.1833
7
1
1
.3919
2
.1962
3
.1321
4
.0985
5
.0766
6
.0599
7
.0448
2
2
.2567
3
.1745
4
.1307
5
.1020
6
.0800
n
k
ℓ
value
7
3
3
.2197
4
.1656
5
.1296
4
4
.2104
8
1
1
.3729
2
.1863
3
.1260
4
.0947
5
.0748
6
.0602
7
.0483
8
.0368
2
2
.2394
3
.1632
4
.1233
5
.0976
6
.0787
7
.0632
3
3
.2008
4
.1524
5
.1210
6
.0978
4
4
.1872
5
.1492
9
1
1
.3574
2
.1781
3
.1207
4
.0913
5
.0727
6
.0595
7
.0491
8
.0401
9
.0311
2
2
.2257
3
.1541
4
.1170
5
.0934
6
.0765
7
.0632
8
.0517
3
3
.1864
4
.1421
5
.1138
6
.0934
7
.0772
n
k
ℓ
value
9
4
4
.1706
5
.1370
6
.1127
5
5
.1661
10
1
1
.3443
2
.1713
3
.1163
4
.0882
5
.0707
6
.0584
7
.0489
8
.0411
9
.0340
10
.0267
2
2
.2145
3
.1466
4
.1117
5
.0897
6
.0742
7
.0622
8
.0523
9
.0434
3
3
.1750
4
.1338
5
.1077
6
.0892
7
.0749
8
.0630
4
4
.1579
5
.1275
6
.1058
7
.0889
5
5
.1511
6
.1256
c
⃝2000 by Chapman & Hall/CRC

4.7
RANGE AND STUDENTIZED RANGE
4.7.1
Probability integral of the range
Let {X1, X2, . . . , Xn} denote a random sample of size n from a population
with standard deviation σ, density function f(x), and cumulative distribution
function F(x). Let {X(1), X(2), . . . , X(n)} denote the same values in ascending
order of magnitude. The sample range R is deﬁned by
R = X(n) −X(1)
(4.61)
In standardized form
W = R
σ = X(n) −X(1)
σ
(4.62)
The probability that the range exceeds some value R, for a sample of size n,
is (see equation (4.54))
Prob

range exceeds R for
a sample of size n

=
 ∞
R
fR(r) dr
= n
 ∞
−∞

F(t + R) −F(t)
n−1
f(t) dt
(4.63)
The following tables provide values of this probability for the normal density
function f(x) =
1
√
2π e−x2/2 for various values of n and W. (Note that since
σ = 1 for this case R = W.)
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 2
3
4
5
6
7
8
9
10
0.00
0.0000 0.0000
0.05
0.0282 0.0007 0.0000
0.10
0.0564 0.0028 0.0001
0.15
0.0845 0.0062 0.0004 0.0000
0.20
0.1125 0.0110 0.0010 0.0001
0.25
0.1403 0.0171 0.0020 0.0002 0.0000
0.30
0.1680 0.0245 0.0034 0.0004 0.0001
0.35
0.1955 0.0332 0.0053 0.0008 0.0001
0.40
0.2227 0.0431 0.0079 0.0014 0.0002 0.0000
0.45
0.2497 0.0543 0.0111 0.0022 0.0004 0.0001
0.50
0.2763 0.0666 0.0152 0.0033 0.0007 0.0002 0.0000
0.55
0.3027 0.0800 0.0200 0.0048 0.0011 0.0003 0.0001
0.60
0.3286 0.0944 0.0257 0.0068 0.0017 0.0004 0.0001 0.0000
0.65
0.3542 0.1099 0.0322 0.0092 0.0026 0.0007 0.0002 0.0001
0.70
0.3794 0.1263 0.0398 0.0121 0.0036 0.0011 0.0003 0.0001
0.75
0.4041 0.1436 0.0483 0.0157 0.0050 0.0016 0.0005 0.0002 0.0000
0.80
0.4284 0.1616 0.0578 0.0200 0.0068 0.0023 0.0008 0.0002 0.0001
0.85
0.4522 0.1805 0.0682 0.0250 0.0090 0.0032 0.0011 0.0004 0.0001
0.90
0.4755 0.2000 0.0797 0.0308 0.0117 0.0044 0.0016 0.0006 0.0002
0.95
0.4983 0.2201 0.0922 0.0375 0.0150 0.0059 0.0023 0.0009 0.0003
1.00
0.5205 0.2407 0.1057 0.0450 0.0188 0.0078 0.0032 0.0013 0.0005
1.05
0.5422 0.2618 0.1201 0.0535 0.0234 0.0101 0.0043 0.0018 0.0008
1.10
0.5633 0.2833 0.1355 0.0629 0.0287 0.0129 0.0058 0.0025 0.0011
1.15
0.5839 0.3052 0.1517 0.0733 0.0348 0.0163 0.0076 0.0035 0.0016
1.20
0.6039 0.3272 0.1688 0.0847 0.0417 0.0203 0.0098 0.0047 0.0022
1.25
0.6232 0.3495 0.1867 0.0970 0.0495 0.0249 0.0125 0.0062 0.0030
1.30
0.6420 0.3719 0.2054 0.1104 0.0583 0.0304 0.0157 0.0080 0.0041
1.35
0.6602 0.3943 0.2248 0.1247 0.0680 0.0366 0.0195 0.0103 0.0054
1.40
0.6778 0.4168 0.2448 0.1400 0.0787 0.0437 0.0240 0.0131 0.0071
1.45
0.6948 0.4392 0.2654 0.1562 0.0904 0.0516 0.0292 0.0164 0.0092
1.50
0.7112 0.4614 0.2865 0.1733 0.1031 0.0606 0.0353 0.0204 0.0117
1.55
0.7269 0.4835 0.3080 0.1913 0.1168 0.0705 0.0421 0.0250 0.0148
1.60
0.7421 0.5053 0.3299 0.2101 0.1315 0.0814 0.0499 0.0304 0.0184
1.65
0.7567 0.5269 0.3521 0.2296 0.1473 0.0934 0.0587 0.0366 0.0227
1.70
0.7707 0.5481 0.3745 0.2498 0.1639 0.1064 0.0684 0.0437 0.0278
1.75
0.7841 0.5690 0.3970 0.2706 0.1815 0.1204 0.0792 0.0517 0.0336
1.80
0.7969 0.5894 0.4197 0.2920 0.2000 0.1355 0.0910 0.0607 0.0403
1.85
0.8092 0.6094 0.4423 0.3138 0.2193 0.1516 0.1039 0.0707 0.0479
1.90
0.8209 0.6290 0.4649 0.3361 0.2394 0.1686 0.1178 0.0818 0.0565
1.95
0.8321 0.6480 0.4874 0.3587 0.2602 0.1867 0.1329 0.0939 0.0661
2.00
0.8427 0.6665 0.5096 0.3816 0.2816 0.2056 0.1489 0.1072 0.0768
2.05
0.8528 0.6845 0.5317 0.4046 0.3035 0.2254 0.1661 0.1216 0.0886
2.10
0.8624 0.7019 0.5534 0.4277 0.3260 0.2460 0.1842 0.1371 0.1015
2.15
0.8716 0.7187 0.5748 0.4508 0.3489 0.2673 0.2032 0.1536 0.1155
2.20
0.8802 0.7349 0.5957 0.4739 0.3720 0.2893 0.2232 0.1712 0.1307
2.25
0.8884 0.7505 0.6163 0.4969 0.3955 0.3118 0.2440 0.1899 0.1470
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 11
12
13
14
15
16
17
18
19
20
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
0.55
0.60
0.65
0.70
0.75
0.80
0.85
0.0000
0.90
0.0001
0.95
0.0001 0.0000
1.00
0.0002 0.0001 0.0000
1.05
0.0003 0.0001 0.0001
1.10
0.0005 0.0002 0.0001 0.0000
1.15
0.0007 0.0003 0.0001 0.0001 0.0000
1.20
0.0010 0.0005 0.0002 0.0001 0.0001
1.25
0.0015 0.0007 0.0004 0.0002 0.0001 0.0000
1.30
0.0021 0.0010 0.0005 0.0003 0.0001 0.0001 0.0000
1.35
0.0028 0.0015 0.0008 0.0004 0.0002 0.0001 0.0001
1.40
0.0038 0.0021 0.0011 0.0006 0.0003 0.0002 0.0001 0.0000
1.45
0.0051 0.0028 0.0016 0.0009 0.0005 0.0003 0.0001 0.0001 0.0000
1.50
0.0067 0.0038 0.0022 0.0012 0.0007 0.0004 0.0002 0.0001 0.0001 0.0000
1.55
0.0087 0.0051 0.0030 0.0017 0.0010 0.0006 0.0003 0.0002 0.0001 0.0001
1.60
0.0111 0.0067 0.0040 0.0024 0.0014 0.0008 0.0005 0.0003 0.0002 0.0001
1.65
0.0140 0.0086 0.0053 0.0032 0.0020 0.0012 0.0007 0.0004 0.0003 0.0002
1.70
0.0176 0.0111 0.0070 0.0044 0.0027 0.0017 0.0011 0.0007 0.0004 0.0003
1.75
0.0217 0.0140 0.0090 0.0058 0.0037 0.0023 0.0015 0.0010 0.0006 0.0004
1.80
0.0266 0.0175 0.0115 0.0075 0.0049 0.0032 0.0021 0.0014 0.0009 0.0006
1.85
0.0323 0.0217 0.0145 0.0097 0.0065 0.0043 0.0029 0.0019 0.0013 0.0008
1.90
0.0388 0.0266 0.0182 0.0124 0.0084 0.0057 0.0039 0.0026 0.0018 0.0012
1.95
0.0463 0.0323 0.0225 0.0156 0.0108 0.0075 0.0052 0.0036 0.0024 0.0017
2.00
0.0548 0.0389 0.0276 0.0195 0.0137 0.0097 0.0068 0.0048 0.0033 0.0023
2.05
0.0643 0.0465 0.0335 0.0241 0.0173 0.0124 0.0088 0.0063 0.0045 0.0032
2.10
0.0748 0.0550 0.0403 0.0295 0.0215 0.0156 0.0114 0.0082 0.0060 0.0043
2.15
0.0866 0.0646 0.0481 0.0357 0.0265 0.0196 0.0144 0.0106 0.0078 0.0058
2.20
0.0994 0.0753 0.0569 0.0429 0.0323 0.0242 0.0182 0.0136 0.0102 0.0076
2.25
0.1134 0.0872 0.0669 0.0511 0.0390 0.0297 0.0226 0.0172 0.0130 0.0099
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 2
3
4
5
6
7
8
9
10
2.25
0.8884 0.7505 0.6163 0.4969 0.3955 0.3118 0.2440 0.1899 0.1470
2.30
0.8961 0.7655 0.6363 0.5196 0.4190 0.3348 0.2656 0.2095 0.1645
2.35
0.9034 0.7799 0.6559 0.5421 0.4427 0.3582 0.2878 0.2300 0.1829
2.40
0.9103 0.7937 0.6748 0.5643 0.4663 0.3820 0.3107 0.2514 0.2025
2.45
0.9168 0.8069 0.6932 0.5861 0.4899 0.4059 0.3341 0.2735 0.2229
2.50
0.9229 0.8195 0.7110 0.6075 0.5132 0.4300 0.3579 0.2963 0.2443
2.55
0.9286 0.8315 0.7282 0.6283 0.5364 0.4541 0.3820 0.3198 0.2665
2.60
0.9340 0.8429 0.7448 0.6487 0.5592 0.4782 0.4064 0.3437 0.2894
2.65
0.9390 0.8537 0.7607 0.6685 0.5816 0.5022 0.4310 0.3680 0.3130
2.70
0.9438 0.8640 0.7759 0.6877 0.6036 0.5259 0.4555 0.3927 0.3372
2.75
0.9482 0.8737 0.7905 0.7063 0.6252 0.5494 0.4801 0.4175 0.3617
2.80
0.9523 0.8828 0.8045 0.7242 0.6461 0.5725 0.5045 0.4425 0.3867
2.85
0.9561 0.8915 0.8177 0.7415 0.6665 0.5952 0.5286 0.4675 0.4119
2.90
0.9597 0.8996 0.8304 0.7581 0.6863 0.6174 0.5525 0.4923 0.4372
2.95
0.9630 0.9073 0.8424 0.7739 0.7055 0.6391 0.5760 0.5171 0.4625
3.00
0.9661 0.9145 0.8537 0.7891 0.7239 0.6601 0.5991 0.5415 0.4878
3.05
0.9690 0.9212 0.8645 0.8036 0.7416 0.6806 0.6216 0.5656 0.5129
3.10
0.9716 0.9275 0.8746 0.8174 0.7587 0.7003 0.6436 0.5892 0.5378
3.15
0.9741 0.9334 0.8842 0.8305 0.7750 0.7194 0.6649 0.6124 0.5623
3.20
0.9763 0.9388 0.8931 0.8429 0.7905 0.7377 0.6856 0.6350 0.5864
3.25
0.9784 0.9439 0.9016 0.8546 0.8053 0.7553 0.7055 0.6569 0.6099
3.30
0.9804 0.9487 0.9095 0.8657 0.8194 0.7721 0.7248 0.6782 0.6329
3.35
0.9822 0.9531 0.9168 0.8761 0.8327 0.7881 0.7432 0.6988 0.6553
3.40
0.9838 0.9572 0.9237 0.8859 0.8454 0.8034 0.7609 0.7186 0.6769
3.45
0.9853 0.9610 0.9302 0.8951 0.8573 0.8179 0.7778 0.7376 0.6978
3.50
0.9867 0.9644 0.9361 0.9037 0.8685 0.8316 0.7938 0.7558 0.7180
3.55
0.9879 0.9677 0.9417 0.9117 0.8790 0.8446 0.8091 0.7732 0.7373
3.60
0.9891 0.9706 0.9468 0.9192 0.8889 0.8568 0.8236 0.7898 0.7558
3.65
0.9901 0.9734 0.9516 0.9261 0.8981 0.8683 0.8372 0.8055 0.7735
3.70
0.9911 0.9759 0.9560 0.9326 0.9067 0.8790 0.8501 0.8204 0.7903
3.75
0.9920 0.9782 0.9600 0.9386 0.9147 0.8891 0.8622 0.8345 0.8062
3.80
0.9928 0.9803 0.9637 0.9441 0.9222 0.8985 0.8736 0.8477 0.8212
3.85
0.9935 0.9822 0.9672 0.9493 0.9291 0.9073 0.8842 0.8602 0.8355
3.90
0.9942 0.9840 0.9703 0.9540 0.9355 0.9155 0.8941 0.8718 0.8488
3.95
0.9948 0.9856 0.9732 0.9583 0.9415 0.9230 0.9034 0.8827 0.8614
4.00
0.9953 0.9870 0.9758 0.9623 0.9469 0.9300 0.9120 0.8929 0.8731
4.05
0.9958 0.9883 0.9782 0.9660 0.9520 0.9365 0.9199 0.9024 0.8841
4.10
0.9963 0.9895 0.9804 0.9693 0.9566 0.9425 0.9273 0.9112 0.8943
4.15
0.9967 0.9906 0.9824 0.9724 0.9608 0.9480 0.9341 0.9193 0.9038
4.20
0.9970 0.9916 0.9842 0.9752 0.9647 0.9530 0.9404 0.9268 0.9126
4.25
0.9973 0.9925 0.9859 0.9777 0.9682 0.9576 0.9461 0.9338 0.9208
4.30
0.9976 0.9933 0.9874 0.9800 0.9715 0.9619 0.9514 0.9402 0.9283
4.35
0.9979 0.9941 0.9887 0.9821 0.9744 0.9657 0.9562 0.9460 0.9352
4.40
0.9981 0.9947 0.9899 0.9840 0.9771 0.9692 0.9607 0.9514 0.9416
4.45
0.9983 0.9953 0.9910 0.9857 0.9795 0.9724 0.9647 0.9563 0.9474
4.50
0.9985 0.9958 0.9920 0.9873 0.9817 0.9754 0.9684 0.9608 0.9527
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 11
12
13
14
15
16
17
18
19
20
2.25
0.1134 0.0872 0.0669 0.0511 0.0390 0.0297 0.0226 0.0172 0.0130 0.0099
2.30
0.1286 0.1003 0.0779 0.0604 0.0468 0.0361 0.0279 0.0214 0.0165 0.0127
2.35
0.1450 0.1145 0.0902 0.0709 0.0556 0.0435 0.0340 0.0265 0.0207 0.0161
2.40
0.1624 0.1299 0.1036 0.0825 0.0655 0.0519 0.0411 0.0325 0.0256 0.0202
2.45
0.1810 0.1466 0.1183 0.0953 0.0766 0.0615 0.0493 0.0394 0.0315 0.0251
2.50
0.2007 0.1643 0.1342 0.1094 0.0890 0.0722 0.0585 0.0474 0.0383 0.0309
2.55
0.2213 0.1833 0.1513 0.1247 0.1025 0.0842 0.0690 0.0565 0.0462 0.0377
2.60
0.2429 0.2032 0.1696 0.1413 0.1174 0.0974 0.0807 0.0668 0.0552 0.0455
2.65
0.2653 0.2243 0.1891 0.1590 0.1335 0.1119 0.0937 0.0783 0.0654 0.0545
2.70
0.2885 0.2462 0.2096 0.1780 0.1509 0.1278 0.1080 0.0911 0.0768 0.0647
2.75
0.3124 0.2690 0.2311 0.1981 0.1696 0.1449 0.1236 0.1053 0.0896 0.0761
2.80
0.3368 0.2926 0.2536 0.2194 0.1894 0.1632 0.1405 0.1208 0.1037 0.0889
2.85
0.3618 0.3169 0.2770 0.2416 0.2103 0.1828 0.1587 0.1376 0.1191 0.1031
2.90
0.3870 0.3417 0.3011 0.2647 0.2323 0.2036 0.1782 0.1557 0.1360 0.1186
2.95
0.4125 0.3670 0.3258 0.2887 0.2553 0.2255 0.1989 0.1752 0.1541 0.1355
3.00
0.4382 0.3927 0.3511 0.3134 0.2792 0.2484 0.2207 0.1959 0.1736 0.1537
3.05
0.4639 0.4186 0.3769 0.3387 0.3039 0.2723 0.2436 0.2177 0.1944 0.1733
3.10
0.4895 0.4446 0.4029 0.3645 0.3292 0.2969 0.2675 0.2407 0.2163 0.1942
3.15
0.5150 0.4706 0.4291 0.3907 0.3551 0.3223 0.2922 0.2646 0.2394 0.2163
3.20
0.5401 0.4965 0.4554 0.4171 0.3814 0.3483 0.3177 0.2894 0.2634 0.2395
3.25
0.5649 0.5222 0.4817 0.4437 0.4080 0.3748 0.3438 0.3151 0.2884 0.2638
3.30
0.5893 0.5475 0.5078 0.4703 0.4348 0.4016 0.3704 0.3413 0.3142 0.2890
3.35
0.6131 0.5725 0.5337 0.4967 0.4617 0.4286 0.3974 0.3681 0.3407 0.3150
3.40
0.6363 0.5970 0.5592 0.5230 0.4885 0.4557 0.4246 0.3953 0.3676 0.3416
3.45
0.6589 0.6209 0.5842 0.5489 0.5150 0.4827 0.4519 0.4227 0.3950 0.3688
3.50
0.6807 0.6442 0.6087 0.5744 0.5413 0.5096 0.4792 0.4502 0.4226 0.3964
3.55
0.7017 0.6668 0.6326 0.5994 0.5672 0.5362 0.5063 0.4777 0.4504 0.4242
3.60
0.7220 0.6886 0.6558 0.6237 0.5926 0.5624 0.5332 0.5051 0.4781 0.4522
3.65
0.7414 0.7096 0.6782 0.6474 0.6173 0.5881 0.5597 0.5322 0.5056 0.4801
3.70
0.7600 0.7298 0.6999 0.6704 0.6414 0.6132 0.5856 0.5588 0.5329 0.5078
3.75
0.7776 0.7491 0.7206 0.6925 0.6648 0.6376 0.6110 0.5850 0.5598 0.5352
3.80
0.7944 0.7675 0.7406 0.7138 0.6874 0.6613 0.6357 0.6106 0.5861 0.5622
3.85
0.8103 0.7850 0.7596 0.7342 0.7090 0.6842 0.6596 0.6355 0.6118 0.5887
3.90
0.8254 0.8016 0.7777 0.7537 0.7298 0.7062 0.6827 0.6596 0.6369 0.6145
3.95
0.8395 0.8173 0.7948 0.7723 0.7497 0.7273 0.7050 0.6829 0.6611 0.6397
4.00
0.8528 0.8321 0.8111 0.7899 0.7686 0.7474 0.7263 0.7053 0.6845 0.6640
4.05
0.8653 0.8460 0.8264 0.8066 0.7866 0.7666 0.7466 0.7268 0.7070 0.6874
4.10
0.8769 0.8590 0.8408 0.8223 0.8036 0.7848 0.7660 0.7472 0.7285 0.7099
4.15
0.8878 0.8712 0.8543 0.8371 0.8196 0.8021 0.7844 0.7667 0.7491 0.7315
4.20
0.8978 0.8826 0.8669 0.8509 0.8347 0.8183 0.8018 0.7852 0.7686 0.7520
4.25
0.9072 0.8931 0.8787 0.8639 0.8488 0.8336 0.8182 0.8027 0.7871 0.7715
4.30
0.9158 0.9029 0.8896 0.8760 0.8620 0.8479 0.8336 0.8191 0.8046 0.7899
4.35
0.9238 0.9120 0.8998 0.8872 0.8744 0.8613 0.8480 0.8346 0.8210 0.8074
4.40
0.9312 0.9204 0.9092 0.8976 0.8858 0.8737 0.8615 0.8490 0.8364 0.8237
4.45
0.9379 0.9281 0.9178 0.9073 0.8964 0.8853 0.8740 0.8625 0.8508 0.8391
4.50
0.9441 0.9352 0.9258 0.9162 0.9062 0.8960 0.8856 0.8750 0.8643 0.8534
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 2
3
4
5
6
7
8
9
10
4.50
0.9985 0.9958 0.9920 0.9873 0.9817 0.9754 0.9684 0.9608 0.9527
4.55
0.9987 0.9963 0.9929 0.9887 0.9837 0.9780 0.9717 0.9649 0.9576
4.60
0.9989 0.9967 0.9937 0.9899 0.9855 0.9804 0.9747 0.9686 0.9620
4.65
0.9990 0.9971 0.9944 0.9911 0.9871 0.9825 0.9775 0.9719 0.9660
4.70
0.9991 0.9974 0.9951 0.9921 0.9885 0.9845 0.9799 0.9750 0.9696
4.75
0.9992 0.9977 0.9956 0.9930 0.9898 0.9862 0.9822 0.9777 0.9729
4.80
0.9993 0.9980 0.9962 0.9938 0.9910 0.9878 0.9842 0.9802 0.9759
4.85
0.9994 0.9982 0.9966 0.9945 0.9920 0.9892 0.9860 0.9824 0.9786
4.90
0.9995 0.9985 0.9970 0.9952 0.9930 0.9904 0.9876 0.9844 0.9810
4.95
0.9995 0.9986 0.9974 0.9958 0.9938 0.9916 0.9890 0.9862 0.9832
5.00
0.9996 0.9988 0.9977 0.9963 0.9945 0.9926 0.9903 0.9878 0.9851
5.05
0.9996 0.9990 0.9980 0.9967 0.9952 0.9935 0.9915 0.9893 0.9869
5.10
0.9997 0.9991 0.9982 0.9971 0.9958 0.9942 0.9925 0.9906 0.9884
5.15
0.9997 0.9992 0.9985 0.9975 0.9963 0.9950 0.9934 0.9917 0.9898
5.20
0.9998 0.9993 0.9987 0.9978 0.9968 0.9956 0.9942 0.9927 0.9911
5.25
0.9998 0.9994 0.9988 0.9981 0.9972 0.9961 0.9949 0.9936 0.9922
5.30
0.9998 0.9995 0.9990 0.9983 0.9975 0.9966 0.9956 0.9944 0.9931
5.35
0.9998 0.9995 0.9991 0.9985 0.9979 0.9971 0.9961 0.9951 0.9940
5.40
0.9999 0.9996 0.9992 0.9987 0.9981 0.9974 0.9966 0.9957 0.9948
5.45
0.9999 0.9997 0.9993 0.9989 0.9984 0.9978 0.9971 0.9963 0.9954
5.50
0.9999 0.9997 0.9994 0.9990 0.9986 0.9981 0.9974 0.9968 0.9960
5.55
0.9999 0.9997 0.9995 0.9992 0.9988 0.9983 0.9978 0.9972 0.9965
5.60
0.9999 0.9998 0.9996 0.9993 0.9989 0.9985 0.9981 0.9976 0.9970
5.65
0.9999 0.9998 0.9996 0.9994 0.9991 0.9987 0.9983 0.9979 0.9974
5.70
0.9999 0.9998 0.9997 0.9995 0.9992 0.9989 0.9986 0.9982 0.9977
5.75
1.0000 0.9999 0.9997 0.9995 0.9993 0.9991 0.9988 0.9984 0.9980
5.80
0.9999 0.9998 0.9996 0.9994 0.9992 0.9989 0.9986 0.9983
5.85
0.9999 0.9998 0.9997 0.9995 0.9993 0.9991 0.9988 0.9985
5.90
0.9999 0.9998 0.9997 0.9996 0.9994 0.9992 0.9990 0.9988
5.95
0.9999 0.9998 0.9997 0.9996 0.9995 0.9993 0.9991 0.9989
6.00
0.9999 0.9999 0.9998 0.9997 0.9996 0.9994 0.9993 0.9991
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 11
12
13
14
15
16
17
18
19
20
4.50
0.9441 0.9352 0.9258 0.9162 0.9062 0.8960 0.8856 0.8750 0.8643 0.8534
4.55
0.9498 0.9417 0.9332 0.9244 0.9153 0.9060 0.8964 0.8867 0.8768 0.8667
4.60
0.9550 0.9476 0.9399 0.9319 0.9236 0.9151 0.9064 0.8975 0.8884 0.8791
4.65
0.9597 0.9530 0.9460 0.9388 0.9313 0.9235 0.9155 0.9074 0.8991 0.8906
4.70
0.9639 0.9579 0.9516 0.9451 0.9382 0.9312 0.9240 0.9165 0.9089 0.9012
4.75
0.9678 0.9624 0.9567 0.9508 0.9446 0.9383 0.9317 0.9249 0.9180 0.9110
4.80
0.9713 0.9665 0.9614 0.9560 0.9505 0.9447 0.9387 0.9326 0.9263 0.9199
4.85
0.9745 0.9702 0.9656 0.9608 0.9557 0.9505 0.9452 0.9396 0.9339 0.9281
4.90
0.9774 0.9735 0.9694 0.9650 0.9605 0.9559 0.9510 0.9460 0.9409 0.9356
4.95
0.9799 0.9765 0.9728 0.9689 0.9649 0.9607 0.9563 0.9518 0.9472 0.9424
5.00
0.9822 0.9791 0.9759 0.9724 0.9688 0.9650 0.9611 0.9571 0.9529 0.9486
5.05
0.9843 0.9816 0.9786 0.9756 0.9723 0.9690 0.9655 0.9618 0.9581 0.9543
5.10
0.9862 0.9837 0.9811 0.9784 0.9755 0.9725 0.9694 0.9661 0.9628 0.9593
5.15
0.9878 0.9856 0.9833 0.9809 0.9783 0.9757 0.9729 0.9700 0.9670 0.9639
5.20
0.9893 0.9874 0.9853 0.9832 0.9809 0.9785 0.9760 0.9735 0.9708 0.9681
5.25
0.9906 0.9889 0.9871 0.9852 0.9832 0.9811 0.9789 0.9766 0.9742 0.9718
5.30
0.9917 0.9903 0.9887 0.9870 0.9852 0.9833 0.9814 0.9794 0.9773 0.9751
5.35
0.9928 0.9915 0.9901 0.9886 0.9870 0.9854 0.9836 0.9819 0.9800 0.9781
5.40
0.9937 0.9925 0.9913 0.9900 0.9886 0.9872 0.9856 0.9841 0.9824 0.9807
5.45
0.9945 0.9935 0.9924 0.9913 0.9900 0.9888 0.9874 0.9860 0.9846 0.9831
5.50
0.9952 0.9943 0.9934 0.9924 0.9913 0.9902 0.9890 0.9878 0.9865 0.9852
5.55
0.9958 0.9951 0.9942 0.9933 0.9924 0.9914 0.9904 0.9893 0.9882 0.9870
5.60
0.9964 0.9957 0.9950 0.9942 0.9934 0.9925 0.9916 0.9907 0.9897 0.9887
5.65
0.9969 0.9963 0.9956 0.9950 0.9943 0.9935 0.9927 0.9919 0.9910 0.9901
5.70
0.9973 0.9968 0.9962 0.9956 0.9950 0.9944 0.9937 0.9930 0.9922 0.9914
5.75
0.9976 0.9972 0.9967 0.9962 0.9957 0.9951 0.9945 0.9939 0.9932 0.9925
5.80
0.9980 0.9976 0.9972 0.9967 0.9963 0.9958 0.9952 0.9947 0.9941 0.9935
5.85
0.9982 0.9979 0.9976 0.9972 0.9968 0.9963 0.9959 0.9954 0.9949 0.9944
5.90
0.9985 0.9982 0.9979 0.9976 0.9972 0.9968 0.9964 0.9960 0.9956 0.9952
5.95
0.9987 0.9985 0.9982 0.9979 0.9976 0.9973 0.9969 0.9966 0.9962 0.9958
6.00
0.9989 0.9987 0.9984 0.9982 0.9979 0.9977 0.9974 0.9971 0.9967 0.9964
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 2
3
4
5
6
7
8
9
10
6.00
1.0000 0.9999 0.9999 0.9998 0.9997 0.9996 0.9994 0.9993 0.9991
6.05
0.9999 0.9999 0.9998 0.9997 0.9996 0.9995 0.9994 0.9992
6.10
1.0000 0.9999 0.9998 0.9998 0.9997 0.9996 0.9995 0.9993
6.15
0.9999 0.9999 0.9998 0.9997 0.9996 0.9995 0.9994
6.20
0.9999 0.9999 0.9998 0.9998 0.9997 0.9996 0.9995
6.25
0.9999 0.9999 0.9999 0.9998 0.9997 0.9997 0.9996
6.30
1.0000 0.9999 0.9999 0.9998 0.9998 0.9997 0.9996
6.35
0.9999 0.9999 0.9999 0.9998 0.9998 0.9997
6.40
0.9999 0.9999 0.9999 0.9998 0.9998 0.9997
6.45
0.9999 0.9999 0.9999 0.9999 0.9998 0.9998
6.50
1.0000 0.9999 0.9999 0.9999 0.9999 0.9998
6.55
0.9999 0.9999 0.9999 0.9999 0.9998
6.60
1.0000 0.9999 0.9999 0.9999 0.9999
6.65
0.9999 0.9999 0.9999 0.9999
6.70
1.0000 0.9999 0.9999 0.9999
6.75
1.0000 0.9999 0.9999
6.80
0.9999 0.9999
6.85
1.0000 0.9999
6.90
1.0000
6.95
7.00
7.05
7.10
7.15
7.20
7.25
c
⃝2000 by Chapman & Hall/CRC

Probability integral of the range
W
n = 11
12
13
14
15
16
17
18
19
20
6.00
0.9989 0.9987 0.9984 0.9982 0.9979 0.9977 0.9974 0.9971 0.9967 0.9964
6.05
0.9990 0.9989 0.9987 0.9985 0.9982 0.9980 0.9977 0.9975 0.9972 0.9969
6.10
0.9992 0.9990 0.9989 0.9987 0.9985 0.9983 0.9981 0.9978 0.9976 0.9973
6.15
0.9993 0.9992 0.9990 0.9989 0.9987 0.9985 0.9983 0.9981 0.9979 0.9977
6.20
0.9994 0.9993 0.9992 0.9990 0.9989 0.9987 0.9986 0.9984 0.9982 0.9980
6.25
0.9995 0.9994 0.9993 0.9992 0.9990 0.9989 0.9988 0.9986 0.9985 0.9983
6.30
0.9996 0.9995 0.9994 0.9993 0.9992 0.9991 0.9990 0.9988 0.9987 0.9986
6.35
0.9996 0.9996 0.9995 0.9994 0.9993 0.9992 0.9991 0.9990 0.9989 0.9988
6.40
0.9997 0.9996 0.9996 0.9995 0.9994 0.9993 0.9992 0.9992 0.9991 0.9990
6.45
0.9997 0.9997 0.9996 0.9996 0.9995 0.9994 0.9994 0.9993 0.9992 0.9991
6.50
0.9998 0.9997 0.9997 0.9996 0.9996 0.9995 0.9995 0.9994 0.9993 0.9993
6.55
0.9998 0.9998 0.9997 0.9997 0.9996 0.9996 0.9995 0.9995 0.9994 0.9994
6.60
0.9998 0.9998 0.9998 0.9997 0.9997 0.9997 0.9996 0.9996 0.9995 0.9995
6.65
0.9999 0.9998 0.9998 0.9998 0.9997 0.9997 0.9997 0.9996 0.9996 0.9995
6.70
0.9999 0.9999 0.9998 0.9998 0.9998 0.9998 0.9997 0.9997 0.9997 0.9996
6.75
0.9999 0.9999 0.9999 0.9998 0.9998 0.9998 0.9998 0.9997 0.9997 0.9997
6.80
0.9999 0.9999 0.9999 0.9999 0.9998 0.9998 0.9998 0.9998 0.9998 0.9997
6.85
0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9998 0.9998 0.9998 0.9998
6.90
0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9998 0.9998 0.9998
6.95
1.0000 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9998
7.00
1.0000 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999
7.05
1.0000 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999
7.10
1.0000 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999
7.15
1.0000 1.0000 0.9999 0.9999 0.9999 0.9999
7.20
1.0000 0.9999 0.9999 0.9999
7.25
1.0000 1.0000 0.9999
4.7.2
Percentage points, studentized range
The standardized range is W = R/σ as deﬁned in the previous section. If the
population standard deviation σ is replaced by the sample standard deviation
s (computed from another sample from the same population), then the stu-
dentized range Q is given by Q = R/S. Here, R is the range of the sample
of size n and S is the independent of R and has ν degrees of freedom. The
probability integral for the studentized range is given by
Prob [Q ≤q] = Prob
R
S ≤q

=
 ∞
0
21−ν/2νν/2sν−1e−νs2/2f(qs)
Γ(ν/2)
ds (4.64)
where f is the probability integral of the range for samples of size n.
The following tables provide values of the studentized range for the normal
density function f(x) =
1
√
2π e−x2/2.
c
⃝2000 by Chapman & Hall/CRC

Upper 1% points of the studentized range
The entries are q.01 where Prob [Q < q.01] = .99.
c
⃝2000 by Chapman & Hall/CRC
ν n = 2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1 77.75129.44147.54170.27188.43202.60215.08225.53234.69242.85250.15255.43261.60238.57243.92248.69253.42257.88262.10
2 13.58 18.33 22.09 24.66 26.66 28.28 29.68 30.87 30.63 31.76 32.78 33.71 34.57 35.36 36.10 36.79 37.44 38.05 38.63
3 8.10 10.54 12.18 13.35 13.96 14.86 15.63 16.29 16.88 17.40 17.87 18.31 18.70 19.07 19.41 19.73 20.04 20.32 20.71
4 6.33
8.11
9.20
9.75 10.46 11.03 11.52 11.94 12.31 12.64 12.94 13.22 13.47 13.70 13.92 14.13 14.32 14.50 14.67
5 5.64
6.98
7.82
8.28
8.81
9.23
9.59
9.91 10.18 10.43 10.65 10.86 11.05 11.22 11.38 11.53 11.67 11.81 11.94
6 5.23
6.34
7.05
7.44
7.87
8.22
8.51
8.77
8.99
9.19
9.37
9.54
9.69
9.83
9.96 10.09 10.20 10.31 10.42
7 4.94
5.93
6.55
6.91
7.28
7.57
7.83
8.05
8.38
8.56
8.72
8.87
9.01
9.13
9.25
9.36
9.47
9.57
9.66
8 4.75
5.65
6.12
6.54
6.87
7.13
7.48
7.69
7.87
8.04
8.18
8.32
8.44
8.56
8.66
8.77
8.86
8.95
9.03
9 4.60
5.44
5.89
6.27
6.57
6.92
7.14
7.33
7.50
7.65
7.79
7.91
8.03
8.13
8.23
8.33
8.42
8.50
8.58
10 4.49
5.20
5.71
6.07
6.35
6.68
6.88
7.06
7.22
7.36
7.49
7.60
7.71
7.81
7.91
7.99
8.08
8.15
8.23
11 4.40
5.09
5.57
5.91
6.17
6.48
6.68
6.85
7.00
7.13
7.25
7.36
7.46
7.56
7.65
7.73
7.81
7.88
7.95
12 4.33
5.00
5.46
5.78
6.03
6.33
6.51
6.67
6.82
6.95
7.06
7.17
7.26
7.35
7.44
7.52
7.59
7.66
7.73
13 4.27
4.93
5.37
5.68
5.92
6.20
6.38
6.53
6.67
6.80
6.90
7.01
7.10
7.19
7.27
7.34
7.42
7.48
7.55
14 4.22
4.86
5.29
5.59
5.82
6.09
6.26
6.41
6.55
6.67
6.77
6.87
6.96
7.05
7.12
7.20
7.27
7.33
7.39
15 4.18
4.81
5.22
5.51
5.74
6.00
6.17
6.31
6.44
6.56
6.66
6.76
6.84
6.93
7.00
7.07
7.14
7.20
7.26
16 4.08
4.76
5.16
5.45
5.67
5.92
6.08
6.23
6.35
6.47
6.57
6.66
6.74
6.82
6.90
6.97
7.03
7.09
7.15
17 4.05
4.72
5.12
5.39
5.61
5.79
6.01
6.15
6.27
6.38
6.48
6.57
6.66
6.73
6.81
6.87
6.94
6.99
7.05
18 4.03
4.69
5.07
5.35
5.56
5.73
5.95
6.08
6.20
6.31
6.41
6.50
6.58
6.65
6.72
6.79
6.85
6.91
6.97
19 4.01
4.66
5.03
5.30
5.51
5.68
5.89
6.03
6.14
6.25
6.34
6.43
6.51
6.58
6.65
6.72
6.78
6.84
6.89
20 3.99
4.63
5.00
5.27
5.47
5.64
5.84
5.97
6.09
6.19
6.29
6.37
6.45
6.52
6.59
6.66
6.71
6.77
6.82
25 3.92
4.52
4.87
5.12
5.32
5.48
5.61
5.73
5.89
5.99
6.07
6.15
6.23
6.29
6.36
6.42
6.47
6.52
6.57
30 3.87
4.45
4.79
5.03
5.22
5.37
5.50
5.61
5.71
5.80
5.94
6.01
6.08
6.14
6.20
6.26
6.31
6.36
6.41
40 3.82
4.37
4.70
4.93
5.10
5.25
5.37
5.48
5.57
5.66
5.73
5.80
5.87
5.93
5.98
6.03
6.08
6.13
6.17
60 3.76
4.29
4.60
4.82
4.99
5.13
5.25
5.35
5.44
5.52
5.60
5.66
5.72
5.78
5.83
5.88
5.93
5.97
6.01
1203w.71
4.20
4.50
4.71
4.87
5.00
5.12
5.21
5.30
5.37
5.44
5.50
5.56
5.61
5.66
5.70
5.75
5.79
5.82
1000 3.64
4.13
4.42
4.62
4.78
4.91
5.02
5.11
5.20
5.27
5.34
5.40
5.46
5.51
5.56
5.61
5.65
5.69
5.73

Upper 5% points of the studentized range
The entries are q.05 where Prob [Q < q.05] = .95.
c
⃝2000 by Chapman & Hall/CRC
ν n = 2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1 17.79 26.70 32.79 37.07 39.84 42.67 45.05 47.10 48.89 50.49 51.91 53.14 54.28 55.36 56.34 57.22 58.05 58.83 59.56
2
6.10
8.31
9.81 10.89 11.70 12.52 13.25 13.87 14.42 14.91 15.35 15.75 16.13 16.47 16.79 17.09 17.38 17.64 17.90
3
4.50
5.91
6.83
7.46
8.03
8.50
8.90
9.24
9.56
9.84 10.09 10.31 10.52 10.72 10.90 11.06 11.22 11.37 11.52
4
3.93
5.04
5.76
6.26
6.68
7.03
7.32
7.58
7.80
8.00
8.19
8.35
8.50
8.65
8.78
8.90
9.02
9.12
9.23
5
3.63
4.60
5.22
5.65
6.04
6.33
6.58
6.80
7.00
7.17
7.33
7.47
7.60
7.72
7.83
7.94
8.03
8.12
8.21
6
3.46
4.34
4.88
5.28
5.63
5.90
6.12
6.32
6.49
6.65
6.79
6.92
7.04
7.14
7.24
7.34
7.43
7.51
7.59
7
3.35
4.15
4.67
5.05
5.36
5.61
5.82
6.00
6.16
6.30
6.43
6.55
6.66
6.76
6.85
6.94
7.02
7.10
7.17
8
3.26
4.03
4.53
4.88
5.15
5.40
5.60
5.77
5.92
6.06
6.18
6.29
6.39
6.48
6.57
6.65
6.73
6.80
6.87
9
3.20
3.95
4.42
4.75
5.01
5.25
5.43
5.60
5.74
5.87
5.98
6.09
6.19
6.28
6.36
6.44
6.51
6.58
6.64
10
3.15
3.88
4.33
4.65
4.90
5.11
5.31
5.46
5.60
5.72
5.83
5.94
6.03
6.12
6.19
6.27
6.34
6.41
6.47
11
3.11
3.83
4.27
4.58
4.82
5.02
5.20
5.35
5.49
5.61
5.71
5.81
5.90
5.99
6.06
6.14
6.20
6.27
6.33
12
3.08
3.78
4.21
4.52
4.75
4.94
5.11
5.27
5.40
5.51
5.62
5.71
5.80
5.88
5.95
6.02
6.09
6.15
6.21
13
3.06
3.75
4.17
4.46
4.69
4.88
5.04
5.18
5.32
5.43
5.53
5.63
5.71
5.79
5.86
5.93
6.00
6.06
6.11
14
3.04
3.72
4.13
4.42
4.65
4.83
4.99
5.12
5.24
5.35
5.46
5.56
5.64
5.71
5.79
5.85
5.92
5.97
6.03
15
3.02
3.69
4.10
4.38
4.61
4.79
4.94
5.08
5.19
5.30
5.39
5.48
5.56
5.64
5.70
5.79
5.85
5.90
5.96
16
3.00
3.67
4.07
4.35
4.57
4.75
4.90
5.03
5.15
5.26
5.35
5.44
5.51
5.59
5.66
5.72
5.78
5.84
5.89
17
2.99
3.65
4.05
4.33
4.54
4.72
4.87
5.00
5.11
5.22
5.31
5.39
5.47
5.55
5.61
5.68
5.73
5.79
5.84
18
2.97
3.63
4.02
4.30
4.52
4.69
4.84
4.97
5.08
5.18
5.28
5.36
5.44
5.51
5.57
5.64
5.70
5.75
5.80
19
2.95
3.62
4.00
4.28
4.49
4.67
4.81
4.94
5.05
5.15
5.24
5.33
5.40
5.48
5.54
5.60
5.66
5.72
5.77
20
2.94
3.60
3.99
4.26
4.47
4.65
4.79
4.92
5.03
5.13
5.22
5.30
5.38
5.45
5.52
5.58
5.63
5.69
5.74
25
2.91
3.55
3.92
4.19
4.39
4.56
4.71
4.83
4.94
5.03
5.12
5.20
5.28
5.35
5.41
5.47
5.53
5.58
5.63
30
2.88
3.51
3.88
4.14
4.34
4.51
4.65
4.77
4.88
4.97
5.06
5.14
5.21
5.28
5.34
5.40
5.46
5.51
5.56
40
2.86
3.47
3.83
4.09
4.28
4.44
4.58
4.70
4.80
4.90
4.98
5.06
5.13
5.20
5.26
5.30
5.35
5.40
5.45
60
2.83
3.43
3.78
4.03
4.22
4.38
4.52
4.63
4.73
4.81
4.89
4.97
5.04
5.10
5.16
5.22
5.27
5.32
5.36
120
2.80
3.36
3.68
3.91
4.10
4.25
4.38
4.49
4.59
4.67
4.75
4.83
4.89
4.95
5.01
5.07
5.12
5.16
5.20
1000
2.77
3.35
3.68
3.92
4.11
4.25
4.37
4.48
4.58
4.67
4.75
4.82
4.88
4.95
5.00
5.06
5.11
5.15
5.20

Upper 10% points of the studentized range
The entries are q.10 where Prob [Q < q.10] = .90.
c
⃝2000 by Chapman & Hall/CRC
ν n = 2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1
8.94 13.43 16.37 18.43 20.12 21.49 22.63 23.61 24.47 25.23 25.91 26.52 26.81 27.46 28.06 28.62 29.15 29.65 30.18
2
4.13
5.73
6.77
7.54
8.14
8.63
9.05
9.41
9.73 10.01 10.26 10.49 10.70 10.89 11.07 11.24 11.40 11.54 11.63
3
3.33
4.47
5.20
5.74
6.16
6.51
6.81
7.06
7.29
7.49
7.67
7.83
7.98
8.12
8.25
8.37
8.48
8.58
8.68
4
3.01
3.98
4.59
5.03
5.39
5.68
5.92
6.14
6.33
6.49
6.64
6.78
6.91
7.02
7.13
7.23
7.33
7.41
7.50
5
2.85
3.72
4.26
4.66
4.98
5.24
5.46
5.65
5.81
5.96
6.10
6.22
6.33
6.44
6.53
6.62
6.71
6.79
6.86
6
2.75
3.56
4.07
4.44
4.73
4.97
5.17
5.34
5.50
5.64
5.76
5.87
5.98
6.07
6.16
6.25
6.32
6.40
6.46
7
2.68
3.45
3.93
4.28
4.55
4.78
4.97
5.14
5.28
5.41
5.53
5.64
5.73
5.82
5.91
5.99
6.06
6.13
6.19
8
2.63
3.37
3.83
4.17
4.43
4.65
4.83
4.99
5.12
5.25
5.36
5.46
5.56
5.64
5.72
5.80
5.87
5.93
6.00
9
2.59
3.32
3.76
4.08
4.34
4.54
4.72
4.87
5.01
5.13
5.23
5.33
5.42
5.50
5.58
5.65
5.72
5.79
5.84
10
2.56
3.27
3.70
4.02
4.26
4.47
4.64
4.78
4.91
5.03
5.13
5.23
5.32
5.40
5.47
5.54
5.61
5.67
5.73
11
2.54
3.23
3.66
3.97
4.20
4.40
4.57
4.71
4.84
4.95
5.05
5.15
5.23
5.31
5.38
5.45
5.51
5.57
5.63
12
2.52
3.20
3.62
3.92
4.16
4.35
4.51
4.65
4.78
4.89
4.99
5.08
5.16
5.24
5.31
5.37
5.44
5.49
5.55
13
2.51
3.18
3.59
3.89
4.12
4.30
4.46
4.60
4.72
4.83
4.93
5.02
5.10
5.18
5.25
5.31
5.37
5.43
5.48
14
2.49
3.16
3.56
3.85
4.08
4.27
4.42
4.56
4.68
4.79
4.88
4.97
5.05
5.12
5.19
5.26
5.32
5.37
5.43
15
2.48
3.14
3.54
3.83
4.05
4.24
4.39
4.52
4.64
4.75
4.84
4.93
5.01
5.08
5.15
5.21
5.27
5.32
5.38
16
2.47
3.12
3.52
3.81
4.03
4.21
4.36
4.49
4.61
4.71
4.81
4.89
4.97
5.04
5.11
5.17
5.23
5.28
5.33
17
2.46
3.11
3.50
3.78
4.00
4.18
4.33
4.46
4.58
4.68
4.77
4.86
4.93
5.01
5.07
5.13
5.19
5.24
5.30
18
2.45
3.10
3.49
3.77
3.98
4.16
4.31
4.44
4.55
4.66
4.75
4.83
4.91
4.98
5.04
5.10
5.16
5.21
5.26
19
2.44
3.09
3.47
3.75
3.97
4.14
4.29
4.42
4.53
4.63
4.72
4.80
4.88
4.95
5.01
5.07
5.13
5.18
5.23
20
2.43
3.08
3.46
3.74
3.95
4.12
4.27
4.40
4.51
4.61
4.70
4.78
4.86
4.92
4.99
5.05
5.10
5.16
5.21
25
2.41
3.04
3.42
3.68
3.89
4.06
4.20
4.32
4.43
4.53
4.62
4.69
4.77
4.83
4.89
4.95
5.00
5.06
5.10
30
2.40
3.02
3.39
3.65
3.85
4.02
4.16
4.28
4.38
4.47
4.56
4.64
4.71
4.77
4.83
4.89
4.94
4.99
5.04
40
2.38
2.99
3.35
3.61
3.80
3.96
4.10
4.22
4.32
4.41
4.49
4.56
4.63
4.69
4.75
4.81
4.86
4.91
4.95
60
2.36
2.96
3.31
3.56
3.76
3.91
4.04
4.16
4.25
4.34
4.42
4.49
4.56
4.62
4.68
4.73
4.78
4.82
4.87
120
2.34
2.93
3.28
3.52
3.71
3.86
3.99
4.10
4.19
4.28
4.35
4.42
4.49
4.54
4.60
4.65
4.69
4.74
4.78
1000
2.33
2.90
3.24
3.48
3.66
3.81
3.93
4.04
4.13
4.21
4.28
4.35
4.41
4.48
4.53
4.59
4.64
4.68
4.73

CHAPTER 5
Discrete Probability
Distributions
Contents
5.1
Bernoulli distribution
5.1.1
Properties
5.1.2
Variates
5.2
Beta binomial distribution
5.2.1
Properties
5.2.2
Variates
5.3
Beta Pascal distribution
5.3.1
Properties
5.4
Binomial distribution
5.4.1
Properties
5.4.2
Variates
5.4.3
Tables
5.5
Geometric distribution
5.5.1
Properties
5.5.2
Variates
5.5.3
Tables
5.6
Hypergeometric distribution
5.6.1
Properties
5.6.2
Variates
5.6.3
Tables
5.7
Multinomial distribution
5.7.1
Properties
5.7.2
Variates
5.8
Negative binomial distribution
5.8.1
Properties
5.8.2
Variates
5.8.3
Tables
5.9
Poisson distribution
5.9.1
Properties
c
⃝2000 by Chapman & Hall/CRC

5.9.2
Variates
5.9.3
Tables
5.10
Rectangular (discrete uniform) distribution
5.10.1
Properties
This chapter presents some common discrete probability distributions along
with their properties. Relevant numerical tables are also included.
Notation used throughout this chapter:
Probability mass function (pmf)
p(x) = Prob [X = x]
Mean
µ = E [X]
Variance
σ2 = E

(X −µ)2
Coeﬃcient of skewness
β1 = E

(X −µ)3
Coeﬃcient of kurtosis
β2 = E

(X −µ)4
Moment generating function (mgf)
m(t) = E

etX
Characteristic function (char function)
φ(t) = E

eitX
Factorial moment generating function (fact mgf)
P(t) = E

tX
5.1
BERNOULLI DISTRIBUTION
A Bernoulli distribution is used to describe an experiment in which there are
only two possible outcomes, typically a success or a failure.
This type of
experiment is called a Bernoulli trial, or simply a trial. The probability of a
success is p and a sequence of Bernoulli trials is referred to as repeated trials.
5.1.1
Properties
pmf
p(x) =
(
q
x = 0
p
x = 1
(or pxq1−x for x = 0, 1)
0 ≤p ≤1,
q = 1 −p
mean
µ = p
variance
σ2 = pq
skewness
β1 = 1 −2p
√pq
kurtosis
β2 = 3 + 1 −6pq
pq
mgf
m(t) = q + pet
c
⃝2000 by Chapman & Hall/CRC

char function
φ(t) = q + peit
fact mgf
P(t) = q + pt
5.1.2
Variates
(1) Let X1, X2, . . . , Xn be independent, identically distributed (iid) Bernoulli
random variables with probability of a success p. The random variable
Y = X1 + X2 + · · · + Xn has a binomial distribution with parameters n
and p.
5.2
BETA BINOMIAL DISTRIBUTION
The beta binomial distribution is also known as the negative hypergeometric
distribution, inverse hypergeometric distribution, hypergeometric waiting–time
distribution, and Markov–P´olya distribution.
5.2.1
Properties
pmf
p(x) =
−a
n
 −b
n −x
 *−a −b
n

x = 0, 1, . . . , n
a, b, n > 0, n an integer
mean
µ =
an
a + b
variance
σ2 =
abn(a + b + n)
(a + b)2(a + b + 1)
skewness
β1 =
+
(a + b + 1)
abn(a + b + n)
(a −b)(a + b + 2n)
(a + b + 2)
kurtosis
β2 =
(a + b)2(a + b + 1)
abn(a + b + n)(a + b + 2)(a + b + 3)×

(a + b)(a + b + 1 + 6n) + 3ab(n −2) + 6n2
−3abn(6 −n)
a + b
−18abn2
(a + b)2

+ 3
mgf
m(t) = 2F1(a, −n; a + b; −et)
char function
φ(t) = 2F1(a, −n; a + b; e−it)
fact mgf
P(t) = 2F1(a, −n; a + b; −t)
where pFq is the generalized hypergeometric function deﬁned in Chapter 18
(see page 520).
c
⃝2000 by Chapman & Hall/CRC

5.2.2
Variates
Let X be a beta binomial random variable with parameters a, b, and n.
(1) If a = b = 1 and n is reduced by 1, then X is a rectangular (discrete
uniform) random variable.
(2) As n →∞, X is approximately a binomial random variable with pa-
rameters n and p = a/b.
5.3
BETA PASCAL DISTRIBUTION
The beta Pascal distribution arises from a special case of the urn scheme.
5.3.1
Properties
pmf
p(x) =
Γ(x)Γ(ν)Γ(ρ + ν)Γ(ν + x −(ρ + r))
Γ(r)Γ(x −r + 1)Γ(ρ)Γ(ν −ρ)Γ(ν + x)
x = r, r + 1, . . . ,
r ∈N,
ν > ρ > 0
mean
µ = rν −1
ρ −1,
ρ > 1
variance
σ2 = r(r + ρ −1) (ν −1)(ν −ρ)
(ρ −1)2(ρ −2),
ρ > 2
where Γ(x) is the gamma function deﬁned in Chapter 18 (see page 515).
5.4
BINOMIAL DISTRIBUTION
The binomial distribution is used to characterize the number of successes in n
Bernoulli trials. It is used to model some very common experiments in which
a sample of size n is taken from an inﬁnite population such that each element
is selected independently and has the same probability, p, of having a speciﬁed
attribute.
5.4.1
Properties
pmf
p(x) =
n
x

pxqn−x x = 0, 1, 2, . . . , n
0 ≤p ≤1,
q = 1 −p
mean
µ = np
variance
σ2 = npq
skewness
β1 = 1 −2p
√npq
kurtosis
β2 = 3 + 1 −6pq
npq
c
⃝2000 by Chapman & Hall/CRC

mgf
m(t) = (q + pet)n
char function
φ(t) = (q + peit)n
fact mgf
P(t) = (q + pt)n
5.4.2
Variates
Let X be a binomial random variable with parameters n and p.
(1) If n = 1, then X is a Bernoulli random variable with probability of
success p.
(2) As n →∞if np ≥5 and n(1 −p) ≥5, then X is approximately normal
with parameters µ = np and σ2 = np(1 −p).
(3) As n →∞if p < 0.1 and np < 10, then X is approximately a Poisson
random variable with parameter λ = np.
(4) Let X1, . . . , Xk be independent, binomial random variables with param-
eters ni and p, respectively. The random variable Y = X1+X2+· · ·+Xk
has a binomial distribution with parameters n = n1+n2+· · ·+nk and p.
5.4.3
Tables
The following tables only contain values of p up to p = 1/2. By symmetry
(replacing p with 1 −p and replacing x with n −x) values for p > 1/2 can be
reduced to the present tables.
Example 5.32:
A biased coin has a probability of heads of .75; what is the probability
of obtaining 5 or more heads in 8 ﬂips?
Solution:
(S1) The answer is given by looking in cumulative distribution tables with n = 8,
x = 5, and p = 0.75.
(S2) Making the substitutions mentioned above this is the same as n = 8, x = 3 and
p = 0.25.
(S3) This value is in the tables and is equal to 0.8862. Hence, 89% of the time 5 or
more heads would be likely to occur.
(S4) This result can be interpreted as the likelihood of ﬂipping a biased coin that has
a probability of tails equal to 25% and asking how likely it is to have 3 or fewer
tails.
(S5) Note: The following tables are for the cumulative distribution function, not the
probability mass function. The probability of obtaining exactly 5 heads in 8 ﬂips
is 8
5

(0.75)5(.25)3 = .2076.
Example 5.33:
The probability a randomly selected home in Columbia County will
lose power during a summer storm is .25. Suppose 14 homes in this county are selected
at random. What is the probability exactly 4 homes will lose power, more than 6 will
lose power, and between 2 and 7 (inclusive) will lose power?
c
⃝2000 by Chapman & Hall/CRC

Solution:
(S1) Let X be the number of homes (out of 14) that will lose power. The random
variable X has a binomial distribution with parameters n = 14 and p = 0.25.
Use the cumulative terms for the binomial distribution to answer each probability
question.
(S2) Prob [X = 4] = Prob [X ≤4] −Prob [X ≤3]
= 0.7415 −0.5213 = 0.2202
(S3) Prob [X > 6] = 1 −Prob [X ≤6] = 1 −0.9617 = 0.0383
(S4) Prob [2 ≤X ≤7] = Prob [X ≤7] −Prob [X ≤1]
= 0.9897 −0.1010 = 0.8887
c
⃝2000 by Chapman & Hall/CRC

Cumulative probability, Binomial distribution
n
x
p =0.05
0.10
0.15
0.20
0.25
0.30
0.40
0.50
2
0
0.9025
0.8100
0.7225
0.6400
0.5625
0.4900
0.3600
0.2500
1
0.9975
0.9900
0.9775
0.9600
0.9375
0.9100
0.8400
0.7500
3
0
0.8574
0.7290
0.6141
0.5120
0.4219
0.3430
0.2160
0.1250
1
0.9928
0.9720
0.9393
0.8960
0.8438
0.7840
0.6480
0.5000
2
0.9999
0.9990
0.9966
0.9920
0.9844
0.9730
0.9360
0.8750
4
0
0.8145
0.6561
0.5220
0.4096
0.3164
0.2401
0.1296
0.0625
1
0.9860
0.9477
0.8905
0.8192
0.7383
0.6517
0.4752
0.3125
2
0.9995
0.9963
0.9880
0.9728
0.9492
0.9163
0.8208
0.6875
3
1.0000
0.9999
0.9995
0.9984
0.9961
0.9919
0.9744
0.9375
5
0
0.7738
0.5905
0.4437
0.3277
0.2373
0.1681
0.0778
0.0313
1
0.9774
0.9185
0.8352
0.7373
0.6328
0.5282
0.3370
0.1875
2
0.9988
0.9914
0.9734
0.9421
0.8965
0.8369
0.6826
0.5000
3
1.0000
0.9995
0.9978
0.9933
0.9844
0.9692
0.9130
0.8125
4
1.0000
1.0000
0.9999
0.9997
0.9990
0.9976
0.9898
0.9688
6
0
0.7351
0.5314
0.3771
0.2621
0.1780
0.1177
0.0467
0.0156
1
0.9672
0.8857
0.7765
0.6554
0.5339
0.4202
0.2333
0.1094
2
0.9978
0.9841
0.9527
0.9011
0.8306
0.7443
0.5443
0.3438
3
0.9999
0.9987
0.9941
0.9830
0.9624
0.9295
0.8208
0.6563
4
1.0000
1.0000
0.9996
0.9984
0.9954
0.9891
0.9590
0.8906
5
1.0000
1.0000
1.0000
0.9999
0.9998
0.9993
0.9959
0.9844
7
0
0.6983
0.4783
0.3206
0.2097
0.1335
0.0824
0.0280
0.0078
1
0.9556
0.8503
0.7166
0.5767
0.4450
0.3294
0.1586
0.0625
2
0.9962
0.9743
0.9262
0.8520
0.7564
0.6471
0.4199
0.2266
3
0.9998
0.9973
0.9879
0.9667
0.9294
0.8740
0.7102
0.5000
4
1.0000
0.9998
0.9988
0.9953
0.9871
0.9712
0.9037
0.7734
5
1.0000
1.0000
0.9999
0.9996
0.9987
0.9962
0.9812
0.9375
6
1.0000
1.0000
1.0000
1.0000
0.9999
0.9998
0.9984
0.9922
8
0
0.6634
0.4305
0.2725
0.1678
0.1001
0.0576
0.0168
0.0039
1
0.9428
0.8131
0.6572
0.5033
0.3671
0.2553
0.1064
0.0352
2
0.9942
0.9619
0.8948
0.7969
0.6785
0.5518
0.3154
0.1445
3
0.9996
0.9950
0.9787
0.9437
0.8862
0.8059
0.5941
0.3633
4
1.0000
0.9996
0.9971
0.9896
0.9727
0.9420
0.8263
0.6367
5
1.0000
1.0000
0.9998
0.9988
0.9958
0.9887
0.9502
0.8555
6
1.0000
1.0000
1.0000
0.9999
0.9996
0.9987
0.9915
0.9648
7
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9993
0.9961
9
0
0.6302
0.3874
0.2316
0.1342
0.0751
0.0403
0.0101
0.0019
1
0.9288
0.7748
0.5995
0.4362
0.3003
0.1960
0.0705
0.0195
2
0.9916
0.9470
0.8591
0.7382
0.6007
0.4628
0.2318
0.0898
3
0.9994
0.9917
0.9661
0.9144
0.8343
0.7297
0.4826
0.2539
4
1.0000
0.9991
0.9944
0.9804
0.9511
0.9012
0.7334
0.5000
5
1.0000
0.9999
0.9994
0.9969
0.9900
0.9747
0.9006
0.7461
6
1.0000
1.0000
1.0000
0.9997
0.9987
0.9957
0.9750
0.9102
7
1.0000
1.0000
1.0000
1.0000
0.9999
0.9996
0.9962
0.9805
8
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
0.9980
c
⃝2000 by Chapman & Hall/CRC

Cumulative probability, Binomial distribution
n
x
p =0.05
0.10
0.15
0.20
0.25
0.30
0.40
0.50
10
0
0.5987
0.3487
0.1969
0.1074
0.0563
0.0283
0.0060
0.0010
1
0.9139
0.7361
0.5443
0.3758
0.2440
0.1493
0.0464
0.0107
2
0.9885
0.9298
0.8202
0.6778
0.5256
0.3828
0.1673
0.0547
3
0.9990
0.9872
0.9500
0.8791
0.7759
0.6496
0.3823
0.1719
4
0.9999
0.9984
0.9901
0.9672
0.9219
0.8497
0.6331
0.3770
5
1.0000
0.9999
0.9986
0.9936
0.9803
0.9526
0.8338
0.6230
6
1.0000
1.0000
0.9999
0.9991
0.9965
0.9894
0.9452
0.8281
7
1.0000
1.0000
1.0000
0.9999
0.9996
0.9984
0.9877
0.9453
8
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9983
0.9893
9
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9990
11
0
0.5688
0.3138
0.1673
0.0859
0.0422
0.0198
0.0036
0.0005
1
0.8981
0.6974
0.4922
0.3221
0.1971
0.1130
0.0302
0.0059
2
0.9848
0.9104
0.7788
0.6174
0.4552
0.3127
0.1189
0.0327
3
0.9984
0.9815
0.9306
0.8389
0.7133
0.5696
0.2963
0.1133
4
0.9999
0.9972
0.9841
0.9496
0.8854
0.7897
0.5328
0.2744
5
1.0000
0.9997
0.9973
0.9883
0.9657
0.9218
0.7535
0.5000
6
1.0000
1.0000
0.9997
0.9980
0.9924
0.9784
0.9006
0.7256
7
1.0000
1.0000
1.0000
0.9998
0.9988
0.9957
0.9707
0.8867
8
1.0000
1.0000
1.0000
1.0000
0.9999
0.9994
0.9941
0.9673
9
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9993
0.9941
10
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9995
12
0
0.5404
0.2824
0.1422
0.0687
0.0317
0.0138
0.0022
0.0002
1
0.8816
0.6590
0.4435
0.2749
0.1584
0.0850
0.0196
0.0032
2
0.9804
0.8891
0.7358
0.5584
0.3907
0.2528
0.0834
0.0193
3
0.9978
0.9744
0.9078
0.7946
0.6488
0.4925
0.2253
0.0730
4
0.9998
0.9957
0.9761
0.9274
0.8424
0.7237
0.4382
0.1938
5
1.0000
0.9995
0.9954
0.9806
0.9456
0.8821
0.6652
0.3872
6
1.0000
1.0000
0.9993
0.9961
0.9858
0.9614
0.8418
0.6128
7
1.0000
1.0000
0.9999
0.9994
0.9972
0.9905
0.9427
0.8062
8
1.0000
1.0000
1.0000
0.9999
0.9996
0.9983
0.9847
0.9270
9
1.0000
1.0000
1.0000
1.0000
1.0000
0.9998
0.9972
0.9807
10
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
0.9968
11
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9998
13
0
0.5133
0.2542
0.1209
0.0550
0.0238
0.0097
0.0013
0.0001
1
0.8646
0.6213
0.3983
0.2336
0.1267
0.0637
0.0126
0.0017
2
0.9755
0.8661
0.6920
0.5017
0.3326
0.2025
0.0579
0.0112
3
0.9969
0.9658
0.8820
0.7473
0.5843
0.4206
0.1686
0.0461
4
0.9997
0.9935
0.9658
0.9009
0.7940
0.6543
0.3530
0.1334
5
1.0000
0.9991
0.9925
0.9700
0.9198
0.8346
0.5744
0.2905
6
1.0000
0.9999
0.9987
0.9930
0.9757
0.9376
0.7712
0.5000
7
1.0000
1.0000
0.9998
0.9988
0.9943
0.9818
0.9023
0.7095
8
1.0000
1.0000
1.0000
0.9998
0.9990
0.9960
0.9679
0.8666
9
1.0000
1.0000
1.0000
1.0000
0.9999
0.9993
0.9922
0.9539
10
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9987
0.9888
11
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9983
12
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
c
⃝2000 by Chapman & Hall/CRC

Cumulative probability, Binomial distribution
n
x
p =0.05
0.10
0.15
0.20
0.25
0.30
0.40
0.50
14
0
0.4877
0.2288
0.1028
0.0440
0.0178
0.0068
0.0008
0.0001
1
0.8470
0.5846
0.3567
0.1979
0.1010
0.0475
0.0081
0.0009
2
0.9699
0.8416
0.6479
0.4481
0.2811
0.1608
0.0398
0.0065
3
0.9958
0.9559
0.8535
0.6982
0.5213
0.3552
0.1243
0.0287
4
0.9996
0.9908
0.9533
0.8702
0.7415
0.5842
0.2793
0.0898
5
1.0000
0.9985
0.9885
0.9562
0.8883
0.7805
0.4859
0.2120
6
1.0000
0.9998
0.9978
0.9884
0.9617
0.9067
0.6925
0.3953
7
1.0000
1.0000
0.9997
0.9976
0.9897
0.9685
0.8499
0.6047
8
1.0000
1.0000
1.0000
0.9996
0.9979
0.9917
0.9417
0.7880
9
1.0000
1.0000
1.0000
1.0000
0.9997
0.9983
0.9825
0.9102
10
1.0000
1.0000
1.0000
1.0000
1.0000
0.9998
0.9961
0.9713
11
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9994
0.9935
12
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9991
13
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
15
0
0.4633
0.2059
0.0873
0.0352
0.0134
0.0047
0.0005
0.0000
1
0.8290
0.5490
0.3186
0.1671
0.0802
0.0353
0.0052
0.0005
2
0.9638
0.8159
0.6042
0.3980
0.2361
0.1268
0.0271
0.0037
3
0.9945
0.9444
0.8227
0.6482
0.4613
0.2969
0.0905
0.0176
4
0.9994
0.9873
0.9383
0.8358
0.6865
0.5155
0.2173
0.0592
5
1.0000
0.9978
0.9832
0.9389
0.8516
0.7216
0.4032
0.1509
6
1.0000
0.9997
0.9964
0.9819
0.9434
0.8689
0.6098
0.3036
7
1.0000
1.0000
0.9994
0.9958
0.9827
0.9500
0.7869
0.5000
8
1.0000
1.0000
0.9999
0.9992
0.9958
0.9848
0.9050
0.6964
9
1.0000
1.0000
1.0000
0.9999
0.9992
0.9963
0.9662
0.8491
10
1.0000
1.0000
1.0000
1.0000
0.9999
0.9993
0.9907
0.9408
11
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9981
0.9824
12
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
0.9963
13
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9995
14
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
16
0
0.4401
0.1853
0.0742
0.0282
0.0100
0.0033
0.0003
0.0000
1
0.8108
0.5147
0.2839
0.1407
0.0635
0.0261
0.0033
0.0003
2
0.9571
0.7893
0.5614
0.3518
0.1971
0.0994
0.0183
0.0021
3
0.9930
0.9316
0.7899
0.5981
0.4050
0.2459
0.0651
0.0106
4
0.9991
0.9830
0.9210
0.7983
0.6302
0.4499
0.1666
0.0384
5
0.9999
0.9967
0.9765
0.9183
0.8104
0.6598
0.3288
0.1051
6
1.0000
0.9995
0.9944
0.9733
0.9204
0.8247
0.5272
0.2273
7
1.0000
0.9999
0.9989
0.9930
0.9729
0.9256
0.7161
0.4018
8
1.0000
1.0000
0.9998
0.9985
0.9925
0.9743
0.8577
0.5982
9
1.0000
1.0000
1.0000
0.9998
0.9984
0.9929
0.9417
0.7728
10
1.0000
1.0000
1.0000
1.0000
0.9997
0.9984
0.9809
0.8949
11
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
0.9951
0.9616
12
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9991
0.9894
13
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9979
14
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
15
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
c
⃝2000 by Chapman & Hall/CRC

Cumulative probability, Binomial distribution
n
x
p =0.05
0.10
0.15
0.20
0.25
0.30
0.40
0.50
17
0
0.4181
0.1668
0.0631
0.0225
0.0075
0.0023
0.0002
0.0000
1
0.7922
0.4818
0.2525
0.1182
0.0501
0.0193
0.0021
0.0001
2
0.9497
0.7618
0.5198
0.3096
0.1637
0.0774
0.0123
0.0012
3
0.9912
0.9174
0.7556
0.5489
0.3530
0.2019
0.0464
0.0064
4
0.9988
0.9779
0.9013
0.7582
0.5739
0.3887
0.1260
0.0245
5
0.9999
0.9953
0.9681
0.8943
0.7653
0.5968
0.2639
0.0717
6
1.0000
0.9992
0.9917
0.9623
0.8929
0.7752
0.4478
0.1661
7
1.0000
0.9999
0.9983
0.9891
0.9598
0.8954
0.6405
0.3145
8
1.0000
1.0000
0.9997
0.9974
0.9876
0.9597
0.8011
0.5000
9
1.0000
1.0000
1.0000
0.9995
0.9969
0.9873
0.9081
0.6855
10
1.0000
1.0000
1.0000
0.9999
0.9994
0.9968
0.9652
0.8338
11
1.0000
1.0000
1.0000
1.0000
0.9999
0.9993
0.9894
0.9283
12
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9975
0.9755
13
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9996
0.9936
14
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9988
15
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
16
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
18
0
0.3972
0.1501
0.0537
0.0180
0.0056
0.0016
0.0001
0.0000
1
0.7735
0.4503
0.2240
0.0991
0.0395
0.0142
0.0013
0.0001
2
0.9419
0.7338
0.4797
0.2713
0.1353
0.0600
0.0082
0.0007
3
0.9891
0.9018
0.7202
0.5010
0.3057
0.1646
0.0328
0.0038
4
0.9984
0.9718
0.8794
0.7164
0.5187
0.3327
0.0942
0.0154
5
0.9998
0.9936
0.9581
0.8671
0.7175
0.5344
0.2088
0.0481
6
1.0000
0.9988
0.9882
0.9487
0.8610
0.7217
0.3743
0.1189
7
1.0000
0.9998
0.9973
0.9837
0.9431
0.8593
0.5634
0.2403
8
1.0000
1.0000
0.9995
0.9958
0.9807
0.9404
0.7368
0.4073
9
1.0000
1.0000
0.9999
0.9991
0.9946
0.9790
0.8653
0.5927
10
1.0000
1.0000
1.0000
0.9998
0.9988
0.9939
0.9424
0.7597
11
1.0000
1.0000
1.0000
1.0000
0.9998
0.9986
0.9797
0.8811
12
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
0.9942
0.9519
13
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9987
0.9846
14
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9998
0.9962
15
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9993
16
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
17
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
c
⃝2000 by Chapman & Hall/CRC

Cumulative probability, Binomial distribution
n
x
p =0.05
0.10
0.15
0.20
0.25
0.30
0.40
0.50
19
0
0.3774
0.1351
0.0456
0.0144
0.0042
0.0011
0.0001
0.0000
1
0.7547
0.4203
0.1985
0.0829
0.0310
0.0104
0.0008
0.0000
2
0.9335
0.7054
0.4413
0.2369
0.1113
0.0462
0.0055
0.0004
3
0.9868
0.8850
0.6842
0.4551
0.2631
0.1332
0.0230
0.0022
4
0.9980
0.9648
0.8556
0.6733
0.4654
0.2822
0.0696
0.0096
5
0.9998
0.9914
0.9463
0.8369
0.6678
0.4739
0.1629
0.0318
6
1.0000
0.9983
0.9837
0.9324
0.8251
0.6655
0.3081
0.0835
7
1.0000
0.9997
0.9959
0.9767
0.9225
0.8180
0.4878
0.1796
8
1.0000
1.0000
0.9992
0.9933
0.9712
0.9161
0.6675
0.3238
9
1.0000
1.0000
0.9999
0.9984
0.9911
0.9675
0.8139
0.5000
10
1.0000
1.0000
1.0000
0.9997
0.9977
0.9895
0.9115
0.6762
11
1.0000
1.0000
1.0000
1.0000
0.9995
0.9972
0.9648
0.8204
12
1.0000
1.0000
1.0000
1.0000
0.9999
0.9994
0.9884
0.9165
13
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9969
0.9682
14
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9994
0.9904
15
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9999
0.9978
16
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9996
17
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
18
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
20
0
0.3585
0.1216
0.0388
0.0115
0.0032
0.0008
0.0000
0.0000
1
0.7358
0.3917
0.1756
0.0692
0.0243
0.0076
0.0005
0.0000
2
0.9245
0.6769
0.4049
0.2061
0.0913
0.0355
0.0036
0.0002
3
0.9841
0.8670
0.6477
0.4114
0.2252
0.1071
0.0160
0.0013
4
0.9974
0.9568
0.8298
0.6297
0.4148
0.2375
0.0510
0.0059
5
0.9997
0.9888
0.9327
0.8042
0.6172
0.4164
0.1256
0.0207
6
1.0000
0.9976
0.9781
0.9133
0.7858
0.6080
0.2500
0.0577
7
1.0000
0.9996
0.9941
0.9679
0.8982
0.7723
0.4159
0.1316
8
1.0000
0.9999
0.9987
0.9900
0.9591
0.8867
0.5956
0.2517
9
1.0000
1.0000
0.9998
0.9974
0.9861
0.9520
0.7553
0.4119
10
1.0000
1.0000
1.0000
0.9994
0.9961
0.9829
0.8725
0.5881
11
1.0000
1.0000
1.0000
0.9999
0.9991
0.9949
0.9435
0.7483
12
1.0000
1.0000
1.0000
1.0000
0.9998
0.9987
0.9790
0.8684
13
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
0.9935
0.9423
14
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9984
0.9793
15
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9997
0.9941
16
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9987
17
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
0.9998
18
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
19
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
1.0000
c
⃝2000 by Chapman & Hall/CRC

5.5
GEOMETRIC DISTRIBUTION
In a series of Bernoulli trials with probability of success p, a geometric random
variable is the number of the trial on which the ﬁrst success occurs. Hence,
this is a waiting time distribution. The geometric distribution, sometimes
called the Pascal distribution, is often thought of as the discrete version of an
exponential distribution.
5.5.1
Properties
pmf
p(x) = pqx−1 x = 1, 2, 3, . . . ,
0 ≤p ≤1,
q = 1 −p
mean
µ = 1/p
variance
σ2 = q/p2
skewness
β1 = 2 −p
√q
kurtosis
β2 = p2 + 6q
q
mgf
m(t) =
pet
1 −qet
char function
φ(t) =
peit
1 −qeit
fact mgf
P(t) =
pt
1 −qt
5.5.2
Variates
Let X1, X2, . . . , Xn be independent, identically distributed geometric random
variables with parameter p.
(1) The random variable Y = X1 + X2 + · · · + Xn has a negative binomial
distribution with parameters n and p.
(2) The random variable Y = min(X1, X2, . . . , Xn) has a geometric distri-
bution with parameter p.
5.5.3
Tables
Example 5.34:
When ﬂipping a biased coin (so that heads occur only 30% of the
time), what is the probability that the ﬁrst head occurs on the 10th ﬂip?
Solution:
(S1) Using the probability mass table below with x = 10 and p = 0.3 results in 0.0121.
(S2) Hence, this is likely to occur only about 1% of the time.
c
⃝2000 by Chapman & Hall/CRC

Example 5.35:
The probability a randomly selected customer has the correct change
when making a purchase at the local donut shop is 0.1. What is the probability the ﬁrst
person to have correct change will be the ﬁfth customer? What is the probability the
ﬁrst person with correct change will be at least the sixth customer?
Solution:
(S1) Let X be the number of the ﬁrst customer with correct change. The random
variable X has a geometric distribution with parameter p = 0.1. Use the table
for cumulative terms of the geometric probabilities to answer each question.
(S2) Prob [X = 5] = 0.0656
(S3) Prob [X ≥6] = 1 −Prob [X ≤5]
= 1 −(0.1000 + 0.0900 + 0.0810 + 0.0729 + 0.656)
= 1 −0.4095 = 0.5905
Probability mass, Geometric distribution
x
p = 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1000
0.2000
0.3000
0.4000
0.5000
0.6000
0.7000
0.8000
0.9000
2
0.0900
0.1600
0.2100
0.2400
0.2500
0.2400
0.2100
0.1600
0.0900
3
0.0810
0.1280
0.1470
0.1440
0.1250
0.0960
0.0630
0.0320
0.0090
4
0.0729
0.1024
0.1029
0.0864
0.0625
0.0384
0.0189
0.0064
0.0009
5
0.0656
0.0819
0.0720
0.0518
0.0313
0.0154
0.0057
0.0013
0.0001
6
0.0590
0.0655
0.0504
0.0311
0.0156
0.0061
0.0017
0.0003
7
0.0531
0.0524
0.0353
0.0187
0.0078
0.0025
0.0005
0.0001
8
0.0478
0.0419
0.0247
0.0112
0.0039
0.0010
0.0002
9
0.0430
0.0336
0.0173
0.0067
0.0020
0.0004
10
0.0387
0.0268
0.0121
0.0040
0.0010
0.0002
11
0.0349
0.0215
0.0085
0.0024
0.0005
0.0001
12
0.0314
0.0172
0.0059
0.0015
0.0002
13
0.0282
0.0137
0.0042
0.0009
0.0001
14
0.0254
0.0110
0.0029
0.0005
0.0001
15
0.0229
0.0088
0.0020
0.0003
20
0.0135
0.0029
0.0003
Cumulative probability, Geometric distribution
x
p = 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0.1000
0.2000
0.3000
0.4000
0.5000
0.6000
0.7000
0.8000
0.9000
2
0.1900
0.3600
0.5100
0.6400
0.7500
0.8400
0.9100
0.9600
0.9900
3
0.2710
0.4880
0.6570
0.7840
0.8750
0.9360
0.9730
0.9920
0.9990
4
0.3439
0.5904
0.7599
0.8704
0.9375
0.9744
0.9919
0.9984
0.9999
5
0.4095
0.6723
0.8319
0.9222
0.9688
0.9898
0.9976
0.9997
1
6
0.4686
0.7379
0.8824
0.9533
0.9844
0.9959
0.9993
0.9999
1
7
0.5217
0.7903
0.9176
0.9720
0.9922
0.9984
0.9998
1
8
0.5695
0.8322
0.9424
0.9832
0.9961
0.9993
0.9999
1
9
0.6126
0.8658
0.9596
0.9899
0.9980
0.9997
1
10
0.6513
0.8926
0.9718
0.9940
0.9990
0.9999
1
11
0.6862
0.9141
0.9802
0.9964
0.9995
1
12
0.7176
0.9313
0.9862
0.9978
0.9998
1
13
0.7458
0.9450
0.9903
0.9987
0.9999
1
14
0.7712
0.9560
0.9932
0.9992
0.9999
1
15
0.7941
0.9648
0.9953
0.9995
1
20
0.8784
0.9885
0.9992
1
c
⃝2000 by Chapman & Hall/CRC

5.6
HYPERGEOMETRIC DISTRIBUTION
In a ﬁnite population of size N suppose there are M successes (and N −M
failures). The hypergeometric distribution is used to describe the number of
successes, X, in n trials (n observations drawn from the population). Unlike
a binomial distribution, the probability of a success does not remain constant
from trial to trial.
5.6.1
Properties
pmf
p(x) =
M
x
N−M
n−x

N
n

x = 0, 1, . . . , n
x ≤M
n −x ≤N −M,
n, M, N ∈N,
1 ≤n ≤N
1 ≤M ≤N,
N = 1, 2, . . .
mean
µ = nM
N
variance
σ2 =
N −n
N −1

nM
N

1 −M
N

skewness
β1 =
(N −2M)(N −2n)
√
N −1
(N −2)

nM(N −M)(N −n)
kurtosis
β2 =
N 2(N −1)
(N −2)(N −3)nM(N −m)(N −n)×
"
N(N + 1) −6n(N −n) + 3 M
N 2 (N −M)×
[N 2(n −2) −Nn2 + 6n(N −n)]
#
mgf
m(t) = 2F1(−n, −M; −N; 1 −et)
char function
φ(t) = 2F1(−n, −M; −N; 1 −eit)
fact mgf
P(t) = 2F1(−n, −M; −N; 1 −t)
where pFq is the generalized hypergeometric function deﬁned in Chapter 18
(see page 520).
5.6.2
Variates
Let X be a hypergeometric random variable with parameters n, m, and N.
(1) As N →∞if n/N < 0.1 then X is approximately a binomial random
variable with parameters n and p = M/N.
(2) As n, M, and N all tend to inﬁnity, if M/N is small then X has ap-
proximately a Poisson distribution with parameter λ = nM/N.
c
⃝2000 by Chapman & Hall/CRC

5.6.3
Tables
Let X be a hypergeometric random variable with parameters n, M, and N.
The probability mass function p(x) = f(x; n, M, N) is the probability of ex-
actly x successes in a sample of n items. The cumulative distribution function
F(x; n, M, N) =
x

r=0
f(r; n, M, N) =
x

r=0
M
r
N −M
n −r

N
n

(5.1)
is the probability of x or fewer successes in the sample of n items.
The
following table contains values for f(x; n, M, N) and F(x; n, M, N) for various
values of x, n, M, and N.
Example 5.36:
A New York City transportation company has 10 taxis, 3 of which
have broken meters. Suppose 4 taxis are selected at random. What is the probability
exactly 1 will have a broken meter, fewer than 2 will have a broken meter, all will have
working meters?
Solution:
(S1) Let X be the number of taxis selected with broken meters. The random variable
X has a hypergeometric distribution with N = 10, n = 4, and M = 3.
(S2) Prob [X = 1] =
M
1
N−M
n−1

N
n

=
3
1
7
3

10
4

= 3 · 35
210 = 0.5
(S3) Prob [X = 0] =
M
0
N−M
n

N
n

=
3
0
7
4

10
4

= 1 · 35
210 = 0.16667
(S4) Prob [X < 2] = Prob [X ≤1] = Prob [X = 0] + Prob [X = 1] = 0.66667
c
⃝2000 by Chapman & Hall/CRC

Hypergeometric probability and distribution functions
N
n
M
x
F(x)
f(x)
N
n
M
x
F(x)
f(x)
2
1
1
0
0.50000
0.50000
6
2
2
2
1.00000
0.06667
2
1
1
1
1.00000
0.50000
6
3
1
0
0.50000
0.50000
3
1
1
0
0.66667
0.66667
6
3
1
1
1.00000
0.50000
3
1
1
1
1.00000
0.33333
6
3
2
0
0.20000
0.20000
3
2
1
0
0.33333
0.33333
6
3
2
1
0.80000
0.60000
3
2
1
1
1.00000
0.66667
6
3
2
2
1.00000
0.20000
3
2
2
1
0.66667
0.66667
6
3
3
0
0.05000
0.05000
3
2
2
2
1.00000
0.33333
6
3
3
1
0.50000
0.45000
4
1
1
0
0.75000
0.75000
6
3
3
2
0.95000
0.45000
4
1
1
1
1.00000
0.25000
6
3
3
3
1.00000
0.05000
4
2
1
0
0.50000
0.50000
6
4
1
0
0.33333
0.33333
4
2
1
1
1.00000
0.50000
6
4
1
1
1.00000
0.66667
4
2
2
0
0.16667
0.16667
6
4
2
0
0.06667
0.06667
4
2
2
1
0.83333
0.66667
6
4
2
1
0.60000
0.53333
4
2
2
2
1.00000
0.16667
6
4
2
2
1.00000
0.40000
4
3
1
0
0.25000
0.25000
6
4
3
1
0.20000
0.20000
4
3
1
1
1.00000
0.75000
6
4
3
2
0.80000
0.60000
4
3
2
1
0.50000
0.50000
6
4
3
3
1.00000
0.20000
4
3
2
2
1.00000
0.50000
6
4
4
2
0.40000
0.40000
4
3
3
2
0.75000
0.75000
6
4
4
3
0.93333
0.53333
4
3
3
3
1.00000
0.25000
6
4
4
4
1.00000
0.06667
5
1
1
0
0.80000
0.80000
6
5
1
0
0.16667
0.16667
5
1
1
1
1.00000
0.20000
6
5
1
1
1.00000
0.83333
5
2
1
0
0.60000
0.60000
6
5
2
1
0.33333
0.33333
5
2
1
1
1.00000
0.40000
6
5
2
2
1.00000
0.66667
5
2
2
0
0.30000
0.30000
6
5
3
2
0.50000
0.50000
5
2
2
1
0.90000
0.60000
6
5
3
3
1.00000
0.50000
5
2
2
2
1.00000
0.10000
6
5
4
3
0.66667
0.66667
5
3
1
0
0.40000
0.40000
6
5
4
4
1.00000
0.33333
5
3
1
1
1.00000
0.60000
6
5
5
4
0.83333
0.83333
5
3
2
0
0.10000
0.10000
6
5
5
5
1.00000
0.16667
5
3
2
1
0.70000
0.60000
7
1
1
0
0.85714
0.85714
5
3
2
2
1.00000
0.30000
7
1
1
1
1.00000
0.14286
5
3
3
1
0.30000
0.30000
7
2
1
0
0.71429
0.71429
5
3
3
2
0.90000
0.60000
7
2
1
1
1.00000
0.28571
5
3
3
3
1.00000
0.10000
7
2
2
0
0.47619
0.47619
5
4
1
0
0.20000
0.20000
7
2
2
1
0.95238
0.47619
5
4
1
1
1.00000
0.80000
7
2
2
2
1.00000
0.04762
5
4
2
1
0.40000
0.40000
7
3
1
0
0.57143
0.57143
5
4
2
2
1.00000
0.60000
7
3
1
1
1.00000
0.42857
5
4
3
2
0.60000
0.60000
7
3
2
0
0.28571
0.28571
5
4
3
3
1.00000
0.40000
7
3
2
1
0.85714
0.57143
5
4
4
3
0.80000
0.80000
7
3
2
2
1.00000
0.14286
5
4
4
4
1.00000
0.20000
7
3
3
0
0.11429
0.11429
6
1
1
0
0.83333
0.83333
7
3
3
1
0.62857
0.51429
6
1
1
1
1.00000
0.16667
7
3
3
2
0.97143
0.34286
6
2
1
0
0.66667
0.66667
7
3
3
3
1.00000
0.02857
6
2
1
1
1.00000
0.33333
7
4
1
0
0.42857
0.42857
6
2
2
0
0.40000
0.40000
7
4
1
1
1.00000
0.57143
6
2
2
1
0.93333
0.53333
7
4
2
0
0.14286
0.14286
c
⃝2000 by Chapman & Hall/CRC

Hypergeometric probability and distribution functions
N
n
M
x
F(x)
f(x)
N
n
M
x
F(x)
f(x)
7
4
2
1
0.71429
0.57143
8
3
3
2
0.98214
0.26786
7
4
2
2
1.00000
0.28571
8
3
3
3
1.00000
0.01786
7
4
3
0
0.02857
0.02857
8
4
1
0
0.50000
0.50000
7
4
3
1
0.37143
0.34286
8
4
1
1
1.00000
0.50000
7
4
3
2
0.88571
0.51429
8
4
2
0
0.21429
0.21429
7
4
3
3
1.00000
0.11429
8
4
2
1
0.78571
0.57143
7
4
4
1
0.11429
0.11429
8
4
2
2
1.00000
0.21429
7
4
4
2
0.62857
0.51429
8
4
3
0
0.07143
0.07143
7
4
4
3
0.97143
0.34286
8
4
3
1
0.50000
0.42857
7
4
4
4
1.00000
0.02857
8
4
3
2
0.92857
0.42857
7
5
1
0
0.28571
0.28571
8
4
3
3
1.00000
0.07143
7
5
1
1
1.00000
0.71429
8
4
4
0
0.01429
0.01429
7
5
2
0
0.04762
0.04762
8
4
4
1
0.24286
0.22857
7
5
2
1
0.52381
0.47619
8
4
4
2
0.75714
0.51429
7
5
2
2
1.00000
0.47619
8
4
4
3
0.98571
0.22857
7
5
3
1
0.14286
0.14286
8
4
4
4
1.00000
0.01429
7
5
3
2
0.71429
0.57143
8
5
1
0
0.37500
0.37500
7
5
3
3
1.00000
0.28571
8
5
1
1
1.00000
0.62500
7
5
4
2
0.28571
0.28571
8
5
2
0
0.10714
0.10714
7
5
4
3
0.85714
0.57143
8
5
2
1
0.64286
0.53571
7
5
4
4
1.00000
0.14286
8
5
2
2
1.00000
0.35714
7
5
5
3
0.47619
0.47619
8
5
3
0
0.01786
0.01786
7
5
5
4
0.95238
0.47619
8
5
3
1
0.28571
0.26786
7
5
5
5
1.00000
0.04762
8
5
3
2
0.82143
0.53571
7
6
1
0
0.14286
0.14286
8
5
3
3
1.00000
0.17857
7
6
1
1
1.00000
0.85714
8
5
4
1
0.07143
0.07143
7
6
2
1
0.28571
0.28571
8
5
4
2
0.50000
0.42857
7
6
2
2
1.00000
0.71429
8
5
4
3
0.92857
0.42857
7
6
3
2
0.42857
0.42857
8
5
4
4
1.00000
0.07143
7
6
3
3
1.00000
0.57143
8
5
5
2
0.17857
0.17857
7
6
4
3
0.57143
0.57143
8
5
5
3
0.71429
0.53571
7
6
4
4
1.00000
0.42857
8
5
5
4
0.98214
0.26786
7
6
5
4
0.71429
0.71429
8
5
5
5
1.00000
0.01786
7
6
5
5
1.00000
0.28571
8
6
1
0
0.25000
0.25000
7
6
6
5
0.85714
0.85714
8
6
1
1
1.00000
0.75000
7
6
6
6
1.00000
0.14286
8
6
2
0
0.03571
0.03571
8
1
1
0
0.87500
0.87500
8
6
2
1
0.46429
0.42857
8
1
1
1
1.00000
0.12500
8
6
2
2
1.00000
0.53571
8
2
1
0
0.75000
0.75000
8
6
3
1
0.10714
0.10714
8
2
1
1
1.00000
0.25000
8
6
3
2
0.64286
0.53571
8
2
2
0
0.53571
0.53571
8
6
3
3
1.00000
0.35714
8
2
2
1
0.96429
0.42857
8
6
4
2
0.21429
0.21429
8
2
2
2
1.00000
0.03571
8
6
4
3
0.78571
0.57143
8
3
1
0
0.62500
0.62500
8
6
4
4
1.00000
0.21429
8
3
1
1
1.00000
0.37500
8
6
5
3
0.35714
0.35714
8
3
2
0
0.35714
0.35714
8
6
5
4
0.89286
0.53571
8
3
2
1
0.89286
0.53571
8
6
5
5
1.00000
0.10714
8
3
2
2
1.00000
0.10714
8
6
6
4
0.53571
0.53571
8
3
3
0
0.17857
0.17857
8
6
6
5
0.96429
0.42857
8
3
3
1
0.71429
0.53571
8
6
6
6
1.00000
0.03571
c
⃝2000 by Chapman & Hall/CRC

Hypergeometric probability and distribution functions
N
n
M
x
F(x)
f(x)
N
n
M
x
F(x)
f(x)
8
7
1
0
0.12500
0.12500
9
5
3
1
0.40476
0.35714
8
7
1
1
1.00000
0.87500
9
5
3
2
0.88095
0.47619
8
7
2
1
0.25000
0.25000
9
5
3
3
1.00000
0.11905
8
7
2
2
1.00000
0.75000
9
5
4
0
0.00794
0.00794
8
7
3
2
0.37500
0.37500
9
5
4
1
0.16667
0.15873
8
7
3
3
1.00000
0.62500
9
5
4
2
0.64286
0.47619
8
7
4
3
0.50000
0.50000
9
5
4
3
0.96032
0.31746
8
7
4
4
1.00000
0.50000
9
5
4
4
1.00000
0.03968
8
7
5
4
0.62500
0.62500
9
5
5
1
0.03968
0.03968
8
7
5
5
1.00000
0.37500
9
5
5
2
0.35714
0.31746
8
7
6
5
0.75000
0.75000
9
5
5
3
0.83333
0.47619
8
7
6
6
1.00000
0.25000
9
5
5
4
0.99206
0.15873
8
7
7
6
0.87500
0.87500
9
5
5
5
1.00000
0.00794
8
7
7
7
1.00000
0.12500
9
6
1
0
0.33333
0.33333
9
1
1
0
0.88889
0.88889
9
6
1
1
1.00000
0.66667
9
1
1
1
1.00000
0.11111
9
6
2
0
0.08333
0.08333
9
2
1
0
0.77778
0.77778
9
6
2
1
0.58333
0.50000
9
2
1
1
1.00000
0.22222
9
6
2
2
1.00000
0.41667
9
2
2
0
0.58333
0.58333
9
6
3
0
0.01191
0.01191
9
2
2
1
0.97222
0.38889
9
6
3
1
0.22619
0.21429
9
2
2
2
1.00000
0.02778
9
6
3
2
0.76191
0.53571
9
3
1
0
0.66667
0.66667
9
6
3
3
1.00000
0.23810
9
3
1
1
1.00000
0.33333
9
6
4
1
0.04762
0.04762
9
3
2
0
0.41667
0.41667
9
6
4
2
0.40476
0.35714
9
3
2
1
0.91667
0.50000
9
6
4
3
0.88095
0.47619
9
3
2
2
1.00000
0.08333
9
6
4
4
1.00000
0.11905
9
3
3
0
0.23810
0.23810
9
6
5
2
0.11905
0.11905
9
3
3
1
0.77381
0.53571
9
6
5
3
0.59524
0.47619
9
3
3
2
0.98809
0.21429
9
6
5
4
0.95238
0.35714
9
3
3
3
1.00000
0.01191
9
6
5
5
1.00000
0.04762
9
4
1
0
0.55556
0.55556
9
6
6
3
0.23810
0.23810
9
4
1
1
1.00000
0.44444
9
6
6
4
0.77381
0.53571
9
4
2
0
0.27778
0.27778
9
6
6
5
0.98809
0.21429
9
4
2
1
0.83333
0.55556
9
6
6
6
1.00000
0.01191
9
4
2
2
1.00000
0.16667
9
7
1
0
0.22222
0.22222
9
4
3
0
0.11905
0.11905
9
7
1
1
1.00000
0.77778
9
4
3
1
0.59524
0.47619
9
7
2
0
0.02778
0.02778
9
4
3
2
0.95238
0.35714
9
7
2
1
0.41667
0.38889
9
4
3
3
1.00000
0.04762
9
7
2
2
1.00000
0.58333
9
4
4
0
0.03968
0.03968
9
7
3
1
0.08333
0.08333
9
4
4
1
0.35714
0.31746
9
7
3
2
0.58333
0.50000
9
4
4
2
0.83333
0.47619
9
7
3
3
1.00000
0.41667
9
4
4
3
0.99206
0.15873
9
7
4
2
0.16667
0.16667
9
4
4
4
1.00000
0.00794
9
7
4
3
0.72222
0.55556
9
5
1
0
0.44444
0.44444
9
7
4
4
1.00000
0.27778
9
5
1
1
1.00000
0.55556
9
7
5
3
0.27778
0.27778
9
5
2
0
0.16667
0.16667
9
7
5
4
0.83333
0.55556
9
5
2
1
0.72222
0.55556
9
7
5
5
1.00000
0.16667
9
5
2
2
1.00000
0.27778
9
7
6
4
0.41667
0.41667
9
5
3
0
0.04762
0.04762
9
7
6
5
0.91667
0.50000
c
⃝2000 by Chapman & Hall/CRC

Hypergeometric probability and distribution functions
N
n
M
x
F(x)
f(x)
N
n
M
x
F(x)
f(x)
9
7
6
6
1.00000
0.08333
10
5
1
0
0.50000
0.50000
9
7
7
5
0.58333
0.58333
10
5
1
1
1.00000
0.50000
9
7
7
6
0.97222
0.38889
10
5
2
0
0.22222
0.22222
9
7
7
7
1.00000
0.02778
10
5
2
1
0.77778
0.55556
9
8
1
0
0.11111
0.11111
10
5
2
2
1.00000
0.22222
9
8
1
1
1.00000
0.88889
10
5
3
0
0.08333
0.08333
9
8
2
1
0.22222
0.22222
10
5
3
1
0.50000
0.41667
9
8
2
2
1.00000
0.77778
10
5
3
2
0.91667
0.41667
9
8
3
2
0.33333
0.33333
10
5
3
3
1.00000
0.08333
9
8
3
3
1.00000
0.66667
10
5
4
0
0.02381
0.02381
9
8
4
3
0.44444
0.44444
10
5
4
1
0.26190
0.23810
9
8
4
4
1.00000
0.55556
10
5
4
2
0.73809
0.47619
9
8
5
4
0.55556
0.55556
10
5
4
3
0.97619
0.23810
9
8
5
5
1.00000
0.44444
10
5
4
4
1.00000
0.02381
9
8
6
5
0.66667
0.66667
10
5
5
0
0.00397
0.00397
9
8
6
6
1.00000
0.33333
10
5
5
1
0.10318
0.09921
9
8
7
6
0.77778
0.77778
10
5
5
2
0.50000
0.39682
9
8
7
7
1.00000
0.22222
10
5
5
3
0.89682
0.39682
9
8
8
7
0.88889
0.88889
10
5
5
4
0.99603
0.09921
9
8
8
8
1.00000
0.11111
10
5
5
5
1.00000
0.00397
10
1
1
0
0.90000
0.90000
10
6
1
0
0.40000
0.40000
10
1
1
1
1.00000
0.10000
10
6
1
1
1.00000
0.60000
10
2
1
0
0.80000
0.80000
10
6
2
0
0.13333
0.13333
10
2
1
1
1.00000
0.20000
10
6
2
1
0.66667
0.53333
10
2
2
0
0.62222
0.62222
10
6
2
2
1.00000
0.33333
10
2
2
1
0.97778
0.35556
10
6
3
0
0.03333
0.03333
10
2
2
2
1.00000
0.02222
10
6
3
1
0.33333
0.30000
10
3
1
0
0.70000
0.70000
10
6
3
2
0.83333
0.50000
10
3
1
1
1.00000
0.30000
10
6
3
3
1.00000
0.16667
10
3
2
0
0.46667
0.46667
10
6
4
0
0.00476
0.00476
10
3
2
1
0.93333
0.46667
10
6
4
1
0.11905
0.11429
10
3
2
2
1.00000
0.06667
10
6
4
2
0.54762
0.42857
10
3
3
0
0.29167
0.29167
10
6
4
3
0.92857
0.38095
10
3
3
1
0.81667
0.52500
10
6
4
4
1.00000
0.07143
10
3
3
2
0.99167
0.17500
10
6
5
1
0.02381
0.02381
10
3
3
3
1.00000
0.00833
10
6
5
2
0.26190
0.23810
10
4
1
0
0.60000
0.60000
10
6
5
3
0.73809
0.47619
10
4
1
1
1.00000
0.40000
10
6
5
4
0.97619
0.23810
10
4
2
0
0.33333
0.33333
10
6
5
5
1.00000
0.02381
10
4
2
1
0.86667
0.53333
10
6
6
2
0.07143
0.07143
10
4
2
2
1.00000
0.13333
10
6
6
3
0.45238
0.38095
10
4
3
0
0.16667
0.16667
10
6
6
4
0.88095
0.42857
10
4
3
1
0.66667
0.50000
10
6
6
5
0.99524
0.11429
10
4
3
2
0.96667
0.30000
10
6
6
6
1.00000
0.00476
10
4
3
3
1.00000
0.03333
10
7
1
0
0.30000
0.30000
10
4
4
0
0.07143
0.07143
10
7
1
1
1.00000
0.70000
10
4
4
1
0.45238
0.38095
10
7
2
0
0.06667
0.06667
10
4
4
2
0.88095
0.42857
10
7
2
1
0.53333
0.46667
10
4
4
3
0.99524
0.11429
10
7
2
2
1.00000
0.46667
10
4
4
4
1.00000
0.00476
10
7
3
0
0.00833
0.00833
c
⃝2000 by Chapman & Hall/CRC

Hypergeometric probability and distribution functions
N
n
M
x
F(x)
f(x)
N
n
M
x
F(x)
f(x)
10
7
3
1
0.18333
0.17500
10
9
5
4
0.50000
0.50000
10
7
3
2
0.70833
0.52500
10
9
5
5
1.00000
0.50000
10
7
3
3
1.00000
0.29167
10
9
6
5
0.60000
0.60000
10
7
4
1
0.03333
0.03333
10
9
6
6
1.00000
0.40000
10
7
4
2
0.33333
0.30000
10
9
7
6
0.70000
0.70000
10
7
4
3
0.83333
0.50000
10
9
7
7
1.00000
0.30000
10
7
4
4
1.00000
0.16667
10
9
8
7
0.80000
0.80000
10
7
5
2
0.08333
0.08333
10
9
8
8
1.00000
0.20000
10
7
5
3
0.50000
0.41667
10
9
9
8
0.90000
0.90000
10
7
5
4
0.91667
0.41667
10
9
9
9
1.00000
0.10000
10
7
5
5
1.00000
0.08333
11
1
1
0
0.90909
0.90909
10
7
6
3
0.16667
0.16667
11
1
1
1
1.00000
0.09091
10
7
6
4
0.66667
0.50000
11
2
1
0
0.81818
0.81818
10
7
6
5
0.96667
0.30000
11
2
1
1
1.00000
0.18182
10
7
6
6
1.00000
0.03333
11
2
2
0
0.65455
0.65455
10
7
7
4
0.29167
0.29167
11
2
2
1
0.98182
0.32727
10
7
7
5
0.81667
0.52500
11
2
2
2
1.00000
0.01818
10
7
7
6
0.99167
0.17500
11
3
1
0
0.72727
0.72727
10
7
7
7
1.00000
0.00833
11
3
1
1
1.00000
0.27273
10
8
1
0
0.20000
0.20000
11
3
2
0
0.50909
0.50909
10
8
1
1
1.00000
0.80000
11
3
2
1
0.94546
0.43636
10
8
2
0
0.02222
0.02222
11
3
2
2
1.00000
0.05455
10
8
2
1
0.37778
0.35556
11
3
3
0
0.33939
0.33939
10
8
2
2
1.00000
0.62222
11
3
3
1
0.84849
0.50909
10
8
3
1
0.06667
0.06667
11
3
3
2
0.99394
0.14546
10
8
3
2
0.53333
0.46667
11
3
3
3
1.00000
0.00606
10
8
3
3
1.00000
0.46667
11
4
1
0
0.63636
0.63636
10
8
4
2
0.13333
0.13333
11
4
1
1
1.00000
0.36364
10
8
4
3
0.66667
0.53333
11
4
2
0
0.38182
0.38182
10
8
4
4
1.00000
0.33333
11
4
2
1
0.89091
0.50909
10
8
5
3
0.22222
0.22222
11
4
2
2
1.00000
0.10909
10
8
5
4
0.77778
0.55556
11
4
3
0
0.21212
0.21212
10
8
5
5
1.00000
0.22222
11
4
3
1
0.72121
0.50909
10
8
6
4
0.33333
0.33333
11
4
3
2
0.97576
0.25455
10
8
6
5
0.86667
0.53333
11
4
3
3
1.00000
0.02424
10
8
6
6
1.00000
0.13333
11
4
4
0
0.10606
0.10606
10
8
7
5
0.46667
0.46667
11
4
4
1
0.53030
0.42424
10
8
7
6
0.93333
0.46667
11
4
4
2
0.91212
0.38182
10
8
7
7
1.00000
0.06667
11
4
4
3
0.99697
0.08485
10
8
8
6
0.62222
0.62222
11
4
4
4
1.00000
0.00303
10
8
8
7
0.97778
0.35556
11
5
1
0
0.54546
0.54546
10
8
8
8
1.00000
0.02222
11
5
1
1
1.00000
0.45454
10
9
1
0
0.10000
0.10000
11
5
2
0
0.27273
0.27273
10
9
1
1
1.00000
0.90000
11
5
2
1
0.81818
0.54546
10
9
2
1
0.20000
0.20000
11
5
2
2
1.00000
0.18182
10
9
2
2
1.00000
0.80000
11
5
3
0
0.12121
0.12121
10
9
3
2
0.30000
0.30000
11
5
3
1
0.57576
0.45454
10
9
3
3
1.00000
0.70000
11
5
3
2
0.93939
0.36364
10
9
4
3
0.40000
0.40000
11
5
3
3
1.00000
0.06061
10
9
4
4
1.00000
0.60000
11
5
4
0
0.04546
0.04546
c
⃝2000 by Chapman & Hall/CRC

5.7
MULTINOMIAL DISTRIBUTION
The multinomial distribution is a generalization of the binomial distribution.
Suppose there are n independent trials, and each trial results in exactly one
of k possible distinct outcomes. For i = 1, 2, . . . , k let pi be the probability
that outcome i occurs on any given trial (with k
i=1 pi = 1). The multinomial
random variable is the random vector X = [X1, X2, . . . , Xk]T where Xi is the
number of times outcome i occurs.
5.7.1
Properties
pmf
p(x1, x2, . . . , xk) = n!
k
)
i=1
pxi
i
xi! ,
k

i=1
xi = n
mean of Xi
µi = npi
variance of Xi
σ2
i = npi(1 −pi)
Cov[Xi, Xj]
σij = −npipj,
i ̸= j
joint mgf
m(t1, t2, . . . , tk) = (p1et1 + p2et2 + · · · + pketk)n
joint char function
φ(t1, t2, . . . , tk) = (p1eit1 + p2eit2 + · · · + pkeitk)n
joint fact mgf
P(t1, t2, . . . , tk) = (p1t1 + p2t2 + · · · + pktk)n
5.7.2
Variates
Let X be a multinomial random variable with parameters n (number of trials)
and p1, p2, . . . , pk.
(1) The marginal distribution of Xi is binomial with parameters n and pi.
(2) If k = 2 and p1 = p, then the multinomial random variable corresponds
to the binomial random variable with parameters n and p.
5.8
NEGATIVE BINOMIAL DISTRIBUTION
Consider a sequence of Bernoulli trials with probability of success p. The
negative binomial distribution is used to describe the number of failures, X,
before the nth success.
5.8.1
Properties
pmf
p(x) =
x + n −1
n −1

pnqx x = 0, 1, 2, . . . ,
n = 1, 2, . . .
0 ≤p ≤1,
q = 1 −p
mean
µ = nq
p
c
⃝2000 by Chapman & Hall/CRC

variance
σ2 = nq
p2
skewness
β1 = 2 −p
√nq
kurtosis
β2 = 3 + p2 + 6q
nq
mgf
m(t) =

p
1 −qet
n
char function
φ(t) =

p
1 −qeit
n
fact mgf
P(t) =

p
1 −qt
n
Using p = k/(m + k) and n = k, there is the following alternative characteri-
zation.
5.8.1.1
Alternative characterization
pmf
p(x) = Γ(k + x)
x!Γ(k)

k
m + k
k 
m
m + k
x
x = 0, 1, 2, . . . ,
m, k > 0
mean
µ = m
variance
σ2 = m + m2/k
skewness
β1 =
2m + k

mk(m + k)
kurtosis
β2 = 3 + 6m2 + 6mk + k2
mk(m + k)
mgf
m(t) =

1 −m
k (et −1)
−k
char function
φ(t) =

1 −m
k (eit −1)
−k
fact mgf
P(t) =

1 −m
k (t −1)
−k
where Γ(x) is the gamma function deﬁned in Chapter 18 (see page 515).
5.8.2
Variates
Let X be a negative binomial random variable with parameters n and p.
(1) If n = 1 then X is a geometric random variable with probability of
success p.
c
⃝2000 by Chapman & Hall/CRC

(2) As n →∞and p →1 with n(1 −p) held constant, X is approximately
a Poisson random variable with λ = n(1 −p).
(3) Let X1, X2, . . . , Xk be independent negative binomial random variables
with parameters ni and p, respectively. The random variable Y = X1 +
X2 + · · · + Xk has a negative binomial distribution with parameters
n = n1 + n2 + · · · + nk and p.
5.8.3
Tables
Example 5.37:
Suppose a biased coin has probability of heads 0.3.
What is the
probability that the 5th head occurs after the 8th tail?
Solution:
(S1) Recognizing that n = 5 and x = 8 with p = 0.3 and q = 1 −p = 0.7, the
probability is
Prob [X = 8] =

5 + 8 −1
5 −1

(0.3)5(0.7)8 = 495(0.3)5(0.7)8 = 0.0693
(S2) This value is in the table below with n = 5, x = 8, and p = 0.3.
Probability mass, Negative binomial distribution
(n, x)
p = 0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
(1,2)
0.0810
0.1280
0.1470
0.1440
0.1250
0.0960
0.0630
0.0320
0.00900
(1,5)
0.0590
0.0655
0.0504
0.0311
0.0156
0.0061
0.0017
0.0003
(1,8)
0.0430
0.0336
0.0173
0.0067
0.0020
0.0004
(1,10)
0.0349
0.0215
0.0085
0.0024
0.0005
0.0001
(3,2)
0.0049
0.0307
0.0794
0.1382
0.1875
0.2074
0.1852
0.1229
0.04370
(3,5)
0.0124
0.0551
0.0953
0.1045
0.0820
0.0464
0.0175
0.0034
0.00020
(3,8)
0.0194
0.0604
0.0700
0.0484
0.0220
0.0064
0.0010
0.0001
(3,10)
0.0230
0.0567
0.0503
0.0255
0.0081
0.0015
0.0001
(5,2)
0.0001
0.0031
0.0179
0.0553
0.1172
0.1866
0.2269
0.1966
0.08860
(5,5)
0.0007
0.0132
0.0515
0.1003
0.1230
0.1003
0.0515
0.0132
0.00070
(5,8)
0.0021
0.0266
0.0693
0.0851
0.0604
0.0252
0.0055
0.0004
(5,10)
0.0035
0.0344
0.0687
0.0620
0.0305
0.0082
0.0010
(8,2)
0.0001
0.0012
0.0085
0.0352
0.0967
0.1868
0.2416
0.15500
(8,5)
0.0007
0.0087
0.0404
0.0967
0.1362
0.1109
0.0425
0.00340
(8,8)
0.0028
0.0243
0.0708
0.0982
0.0708
0.0243
0.0028
(8,10)
0.0001
0.0053
0.0360
0.0771
0.0742
0.0343
0.0066
0.0003
5.9
POISSON DISTRIBUTION
The Poisson, or rare event, distribution is completely described by a single
parameter, λ. This distribution is used to model the number of successes,
X, in a speciﬁed time interval or given region. It is assumed the numbers
of successes occurring in diﬀerent time intervals or regions are independent,
the probability of a success in a time interval or region is very small and
proportional to the length of the time interval or the size of the region, and
the probability of more than one success during any one time interval or region
is negligible.
c
⃝2000 by Chapman & Hall/CRC

5.9.1
Properties
pmf
p(x) = e−λλx
x!
x = 0, 1, 2, . . . ,
λ > 0
mean
µ = λ
variance
σ2 = λ
skewness
β1 = 1/
√
λ
kurtosis
β2 = 3 + (1/λ)
mgf
m(t) = exp[λ(et −1)]
char function
φ(t) = exp[λ(eit −1)]
fact mgf
P(t) = exp[λ(t −1)]
Note that the waiting time between Poisson arrivals is exponentially dis-
tributed.
5.9.2
Variates
Let X be a Poisson random variable with parameter λ.
(1) As λ →∞, X is approximately normal with parameters µ = λ and
σ2 = λ.
(2) Let X1, X2, . . . , Xn be independent Poisson random variables with pa-
rameters λi, respectively. The random variable Y = X1 +X2 +· · ·+Xn
has a Poisson distribution with parameter λ = λ1 + λ2 + · · · + λn.
5.9.3
Tables
Example 5.38:
The number of black bear sightings in Northeastern Pennsylvania
during a given week has a Poisson distribution with λ = 3. For a randomly selected
week, what is the probability of exactly 2 sightings, more than 5 sightings, between 4
and 7 sightings (inclusive)?
Solution:
(S1) Let X be the random variable representing the number of black bear sightings
during any given week; X is Poisson with λ = 3. Use the table below to answer
the probability questions.
(S2) Prob [X = 2] = Prob [X ≤2] −Prob [X ≤1] = 0.423 −0.199 = 0.224
(S3) Prob [X > 5] = 1 −Prob [X ≤4] = 1 −0.815 = 0.185
(S4) Prob [4 ≤X ≤7] = Prob [X ≤7] −Prob [X ≤3] = .988 −.647 = .341
c
⃝2000 by Chapman & Hall/CRC

Cumulative probability, Poisson distribution
λ
x =0
1
2
3
4
5
6
7
8
9
0.02
0.980
1.000
0.04
0.961
0.999
1.000
0.06
0.942
0.998
1.000
0.08
0.923
0.997
1.000
0.10
0.905
0.995
1.000
0.15
0.861
0.990
1.000
1.000
0.20
0.819
0.983
0.999
1.000
0.25
0.779
0.974
0.998
1.000
0.30
0.741
0.963
0.996
1.000
0.35
0.705
0.951
0.995
1.000
0.40
0.670
0.938
0.992
0.999
1.000
0.45
0.638
0.925
0.989
0.999
1.000
0.50
0.607
0.910
0.986
0.998
1.000
0.55
0.577
0.894
0.982
0.998
1.000
0.60
0.549
0.878
0.977
0.997
1.000
0.65
0.522
0.861
0.972
0.996
0.999
1.000
0.70
0.497
0.844
0.966
0.994
0.999
1.000
0.75
0.472
0.827
0.960
0.993
0.999
1.000
0.80
0.449
0.809
0.953
0.991
0.999
1.000
0.85
0.427
0.791
0.945
0.989
0.998
1.000
0.90
0.407
0.772
0.937
0.987
0.998
1.000
0.95
0.387
0.754
0.929
0.984
0.997
1.000
1.00
0.368
0.736
0.920
0.981
0.996
0.999
1.000
1.1
0.333
0.699
0.900
0.974
0.995
0.999
1.000
1.2
0.301
0.663
0.879
0.966
0.992
0.999
1.000
1.3
0.273
0.627
0.857
0.957
0.989
0.998
1.000
1.4
0.247
0.592
0.834
0.946
0.986
0.997
0.999
1.000
1.5
0.223
0.558
0.809
0.934
0.981
0.996
0.999
1.000
1.6
0.202
0.525
0.783
0.921
0.976
0.994
0.999
1.000
1.7
0.183
0.493
0.757
0.907
0.970
0.992
0.998
1.000
1.8
0.165
0.463
0.731
0.891
0.964
0.990
0.997
0.999
1.000
1.9
0.150
0.434
0.704
0.875
0.956
0.987
0.997
0.999
1.000
2.0
0.135
0.406
0.677
0.857
0.947
0.983
0.996
0.999
1.000
2.2
0.111
0.355
0.623
0.819
0.927
0.975
0.993
0.998
1.000
2.4
0.091
0.308
0.570
0.779
0.904
0.964
0.988
0.997
0.999
1.000
2.6
0.074
0.267
0.518
0.736
0.877
0.951
0.983
0.995
0.999
1.000
2.8
0.061
0.231
0.469
0.692
0.848
0.935
0.976
0.992
0.998
0.999
3.0
0.050
0.199
0.423
0.647
0.815
0.916
0.967
0.988
0.996
0.999
3.2
0.041
0.171
0.380
0.603
0.781
0.895
0.955
0.983
0.994
0.998
3.4
0.033
0.147
0.340
0.558
0.744
0.871
0.942
0.977
0.992
0.997
3.6
0.027
0.126
0.303
0.515
0.706
0.844
0.927
0.969
0.988
0.996
3.8
0.022
0.107
0.269
0.473
0.668
0.816
0.909
0.960
0.984
0.994
continued on next page
c
⃝2000 by Chapman & Hall/CRC

continued from previous page
λ
x =0
1
2
3
4
5
6
7
8
9
4.0
0.018
0.092
0.238
0.433
0.629
0.785
0.889
0.949
0.979
0.992
4.2
0.015
0.078
0.210
0.395
0.590
0.753
0.868
0.936
0.972
0.989
4.4
0.012
0.066
0.185
0.359
0.551
0.720
0.844
0.921
0.964
0.985
4.6
0.010
0.056
0.163
0.326
0.513
0.686
0.818
0.905
0.955
0.981
4.8
0.008
0.048
0.142
0.294
0.476
0.651
0.791
0.887
0.944
0.975
5.0
0.007
0.040
0.125
0.265
0.441
0.616
0.762
0.867
0.932
0.968
5.2
0.005
0.034
0.109
0.238
0.406
0.581
0.732
0.845
0.918
0.960
5.4
0.004
0.029
0.095
0.213
0.373
0.546
0.702
0.822
0.903
0.951
5.6
0.004
0.024
0.082
0.191
0.342
0.512
0.670
0.797
0.886
0.941
5.8
0.003
0.021
0.071
0.170
0.313
0.478
0.638
0.771
0.867
0.929
6.0
0.003
0.017
0.062
0.151
0.285
0.446
0.606
0.744
0.847
0.916
6.2
0.002
0.015
0.054
0.134
0.259
0.414
0.574
0.716
0.826
0.902
6.4
0.002
0.012
0.046
0.119
0.235
0.384
0.542
0.687
0.803
0.886
6.6
0.001
0.010
0.040
0.105
0.213
0.355
0.511
0.658
0.780
0.869
6.8
0.001
0.009
0.034
0.093
0.192
0.327
0.480
0.628
0.755
0.850
7.0
0.001
0.007
0.030
0.082
0.173
0.301
0.450
0.599
0.729
0.831
7.2
0.001
0.006
0.025
0.072
0.155
0.276
0.420
0.569
0.703
0.810
7.4
0.001
0.005
0.022
0.063
0.140
0.253
0.392
0.539
0.676
0.788
7.6
0.001
0.004
0.019
0.055
0.125
0.231
0.365
0.510
0.648
0.765
7.8
0.000
0.004
0.016
0.049
0.112
0.210
0.338
0.481
0.620
0.741
8.0
0.000
0.003
0.014
0.042
0.100
0.191
0.313
0.453
0.593
0.717
8.5
0.000
0.002
0.009
0.030
0.074
0.150
0.256
0.386
0.523
0.653
9.0
0.000
0.001
0.006
0.021
0.055
0.116
0.207
0.324
0.456
0.587
9.5
0.000
0.001
0.004
0.015
0.040
0.088
0.165
0.269
0.392
0.522
10.0
0.000
0.001
0.003
0.010
0.029
0.067
0.130
0.220
0.333
0.458
10.5
0.000
0.000
0.002
0.007
0.021
0.050
0.102
0.178
0.279
0.397
11.0
0.000
0.000
0.001
0.005
0.015
0.037
0.079
0.143
0.232
0.341
11.5
0.000
0.000
0.001
0.003
0.011
0.028
0.060
0.114
0.191
0.289
12.0
0.000
0.000
0.001
0.002
0.008
0.020
0.046
0.089
0.155
0.242
12.5
0.000
0.000
0.000
0.002
0.005
0.015
0.035
0.070
0.125
0.201
13.0
0.000
0.000
0.000
0.001
0.004
0.011
0.026
0.054
0.100
0.166
13.5
0.000
0.000
0.000
0.001
0.003
0.008
0.019
0.042
0.079
0.135
14.0
0.000
0.000
0.000
0.001
0.002
0.005
0.014
0.032
0.062
0.109
14.5
0.000
0.000
0.000
0.000
0.001
0.004
0.011
0.024
0.048
0.088
15.0
0.000
0.000
0.000
0.000
0.001
0.003
0.008
0.018
0.037
0.070
Cumulative probability, Poisson distribution
λ
x =10
11
12
13
14
15
16
17
18
19
2.8
1.000
3.0
1.000
3.2
1.000
3.4
0.999
1.000
3.6
0.999
1.000
3.8
0.998
0.999
1.000
4.0
0.997
0.999
1.000
continued on next page
c
⃝2000 by Chapman & Hall/CRC

continued from previous page
λ
x =10
11
12
13
14
15
16
17
18
19
4.2
0.996
0.999
1.000
4.4
0.994
0.998
0.999
1.000
4.6
0.992
0.997
0.999
1.000
4.8
0.990
0.996
0.999
1.000
5.0
0.986
0.995
0.998
0.999
1.000
5.2
0.982
0.993
0.997
0.999
1.000
5.4
0.978
0.990
0.996
0.999
1.000
5.6
0.972
0.988
0.995
0.998
0.999
1.000
5.8
0.965
0.984
0.993
0.997
0.999
1.000
6.0
0.957
0.980
0.991
0.996
0.999
1.000
1.000
6.2
0.949
0.975
0.989
0.995
0.998
0.999
1.000
6.4
0.939
0.969
0.986
0.994
0.997
0.999
1.000
6.6
0.927
0.963
0.982
0.992
0.997
0.999
1.000
1.000
6.8
0.915
0.955
0.978
0.990
0.996
0.998
0.999
1.000
7.0
0.901
0.947
0.973
0.987
0.994
0.998
0.999
1.000
7.2
0.887
0.937
0.967
0.984
0.993
0.997
0.999
1.000
7.4
0.871
0.926
0.961
0.981
0.991
0.996
0.998
0.999
1.000
7.6
0.854
0.915
0.954
0.976
0.989
0.995
0.998
0.999
1.000
7.8
0.835
0.902
0.945
0.971
0.986
0.993
0.997
0.999
1.000
8.0
0.816
0.888
0.936
0.966
0.983
0.992
0.996
0.998
0.999
1.000
8.5
0.763
0.849
0.909
0.949
0.973
0.986
0.993
0.997
0.999
1.000
9.0
0.706
0.803
0.876
0.926
0.959
0.978
0.989
0.995
0.998
0.999
9.5
0.645
0.752
0.836
0.898
0.940
0.967
0.982
0.991
0.996
0.998
10.0
0.583
0.697
0.792
0.865
0.916
0.951
0.973
0.986
0.993
0.997
10.5
0.521
0.639
0.742
0.825
0.888
0.932
0.960
0.978
0.989
0.994
11.0
0.460
0.579
0.689
0.781
0.854
0.907
0.944
0.968
0.982
0.991
11.5
0.402
0.520
0.633
0.733
0.815
0.878
0.924
0.954
0.974
0.986
12.0
0.347
0.462
0.576
0.681
0.772
0.844
0.899
0.937
0.963
0.979
12.5
0.297
0.406
0.519
0.628
0.725
0.806
0.869
0.916
0.948
0.969
13.0
0.252
0.353
0.463
0.573
0.675
0.764
0.836
0.890
0.930
0.957
13.5
0.211
0.304
0.409
0.518
0.623
0.718
0.797
0.861
0.908
0.942
14.0
0.176
0.260
0.358
0.464
0.570
0.669
0.756
0.827
0.883
0.923
14.5
0.145
0.220
0.311
0.412
0.518
0.619
0.711
0.790
0.853
0.901
15.0
0.118
0.185
0.268
0.363
0.466
0.568
0.664
0.749
0.820
0.875
Cumulative probability, Poisson distribution
λ
x =20
21
22
23
24
25
26
27
28
29
8.5
1.000
9.0
1.000
9.5
0.999
1.000
10.0
0.998
0.999
1.000
10.5
0.997
0.999
0.999
1.000
11.0
0.995
0.998
0.999
1.000
11.5
0.993
0.996
0.998
0.999
1.000
12.0
0.988
0.994
0.997
0.999
0.999
1.000
continued on next page
c
⃝2000 by Chapman & Hall/CRC

continued from previous page
λ
x =20
21
22
23
24
25
26
27
28
29
12.5
0.983
0.991
0.995
0.998
0.999
0.999
1.000
13.0
0.975
0.986
0.992
0.996
0.998
0.999
1.000
13.5
0.965
0.980
0.989
0.994
0.997
0.998
0.999
1.000
14.0
0.952
0.971
0.983
0.991
0.995
0.997
0.999
0.999
1.000
14.5
0.936
0.960
0.976
0.986
0.992
0.996
0.998
0.999
1.000
1.000
15.0
0.917
0.947
0.967
0.981
0.989
0.994
0.997
0.998
0.999
1.000
Cumulative probability, Poisson distribution
λ
x =5
6
7
8
9
10
11
12
13
14
16
0.001
0.004
0.010
0.022
0.043
0.077
0.127
0.193
0.275
0.367
17
0.001
0.002
0.005
0.013
0.026
0.049
0.085
0.135
0.201
0.281
18
0.000
0.001
0.003
0.007
0.015
0.030
0.055
0.092
0.143
0.208
19
0.000
0.001
0.002
0.004
0.009
0.018
0.035
0.061
0.098
0.150
20
0.000
0.000
0.001
0.002
0.005
0.011
0.021
0.039
0.066
0.105
21
0.000
0.000
0.000
0.001
0.003
0.006
0.013
0.025
0.043
0.072
22
0.000
0.000
0.000
0.001
0.002
0.004
0.008
0.015
0.028
0.048
23
0.000
0.000
0.000
0.000
0.001
0.002
0.004
0.009
0.017
0.031
24
0.000
0.000
0.000
0.000
0.000
0.001
0.003
0.005
0.011
0.020
25
0.000
0.000
0.000
0.000
0.000
0.001
0.001
0.003
0.006
0.012
26
0.000
0.000
0.000
0.000
0.000
0.000
0.001
0.002
0.004
0.008
27
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.001
0.002
0.005
28
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.001
0.001
0.003
29
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.001
0.002
30
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.001
Cumulative probability, Poisson distribution
λ
x =15
16
17
18
19
20
21
22
23
24
16
0.467
0.566
0.659
0.742
0.812
0.868
0.911
0.942
0.963
0.978
17
0.371
0.468
0.564
0.655
0.736
0.805
0.862
0.905
0.937
0.959
18
0.287
0.375
0.469
0.562
0.651
0.731
0.799
0.855
0.899
0.932
19
0.215
0.292
0.378
0.469
0.561
0.647
0.726
0.793
0.849
0.893
20
0.157
0.221
0.297
0.381
0.470
0.559
0.644
0.721
0.787
0.843
21
0.111
0.163
0.227
0.302
0.384
0.471
0.558
0.640
0.716
0.782
22
0.077
0.117
0.169
0.233
0.306
0.387
0.472
0.556
0.637
0.712
23
0.052
0.082
0.123
0.175
0.238
0.310
0.389
0.472
0.555
0.635
24
0.034
0.056
0.087
0.128
0.180
0.243
0.314
0.392
0.473
0.554
25
0.022
0.038
0.060
0.092
0.134
0.185
0.247
0.318
0.394
0.473
26
0.014
0.025
0.041
0.065
0.097
0.139
0.191
0.252
0.321
0.396
27
0.009
0.016
0.027
0.044
0.069
0.102
0.144
0.195
0.256
0.324
28
0.005
0.010
0.018
0.030
0.048
0.073
0.106
0.148
0.200
0.260
29
0.003
0.006
0.011
0.020
0.033
0.051
0.077
0.110
0.153
0.204
30
0.002
0.004
0.007
0.013
0.022
0.035
0.054
0.081
0.115
0.157
c
⃝2000 by Chapman & Hall/CRC

Cumulative probability, Poisson distribution
λ
x =25
26
27
28
29
30
31
32
33
34
16
0.987
0.993
0.996
0.998
0.999
0.999
1.000
17
0.975
0.985
0.991
0.995
0.997
0.999
0.999
1.000
18
0.955
0.972
0.983
0.990
0.994
0.997
0.998
0.999
1.000
19
0.927
0.951
0.969
0.981
0.988
0.993
0.996
0.998
0.999
0.999
20
0.888
0.922
0.948
0.966
0.978
0.987
0.992
0.995
0.997
0.999
21
0.838
0.883
0.917
0.944
0.963
0.976
0.985
0.991
0.995
0.997
22
0.777
0.832
0.877
0.913
0.940
0.960
0.974
0.983
0.990
0.994
23
0.708
0.772
0.827
0.873
0.908
0.936
0.956
0.971
0.981
0.988
24
0.632
0.704
0.768
0.823
0.868
0.904
0.932
0.953
0.969
0.979
25
0.553
0.629
0.700
0.763
0.818
0.863
0.900
0.928
0.950
0.966
26
0.474
0.552
0.627
0.697
0.759
0.813
0.859
0.896
0.925
0.947
27
0.398
0.474
0.551
0.625
0.694
0.755
0.809
0.855
0.892
0.921
28
0.327
0.400
0.475
0.550
0.623
0.690
0.751
0.805
0.851
0.888
29
0.264
0.330
0.401
0.475
0.549
0.621
0.687
0.748
0.801
0.847
30
0.208
0.267
0.333
0.403
0.476
0.548
0.619
0.684
0.744
0.797
Cumulative probability, Poisson distribution
λ
x =35
36
37
38
39
40
41
42
43
44
16
1.000
17
1.000
18
1.000
19
1.000
20
0.999
1.000
21
0.998
0.999
1.000
1.000
22
0.996
0.998
0.999
0.999
1.000
23
0.993
0.996
0.997
0.999
0.999
1.000
24
0.987
0.992
0.995
0.997
0.998
0.999
1.000
1.000
25
0.978
0.985
0.991
0.994
0.997
0.998
0.999
0.999
1.000
26
0.964
0.976
0.984
0.990
0.994
0.996
0.998
0.999
0.999
1.000
27
0.944
0.961
0.974
0.983
0.989
0.993
0.996
0.997
0.998
0.999
28
0.918
0.941
0.959
0.972
0.981
0.988
0.992
0.995
0.997
0.998
29
0.884
0.914
0.938
0.956
0.970
0.980
0.986
0.991
0.994
0.997
30
0.843
0.880
0.911
0.935
0.954
0.968
0.978
0.985
0.990
0.994
c
⃝2000 by Chapman & Hall/CRC

5.10
RECTANGULAR (DISCRETE UNIFORM) DISTRIBUTION
A general rectangular distribution is used to describe a random variable, X,
that can assume n diﬀerent values with equal probabilities. In the special
case presented here, we assume the random variable can assume the ﬁrst n
positive integers.
5.10.1
Properties
pmf
p(x) = 1/n,
x = 1, 2, . . . , n,
n ∈N
mean
µ = (n + 1)/2
variance
σ2 = (n2 −1)/12
skewness
β1 = 0
kurtosis
β2 = 3
5

3 −
4
n2 −1

mgf
m(t) = et(1 −ent)
n(1 −et)
char function
φ(t) = eit(1 −enit)
n(1 −eit)
fact mgf
P(t) = t(1 −tn)
n(1 −t)
Example 5.39:
A new family game has a special 12-sided numbered die, manufactured
so that each side is equally likely to occur. Find the mean and variance of the number
rolled, and the probability of rolling a 2, 3, or 12.
Solution:
(S1) Let X be the number on the side facing up; X has a discrete uniform distribution
with n = 12.
(S2) Using the properties given above:
µ = (n + 1)/2 = (12 + 1)/2 = 13/2 = 6.5
σ2 = (n2 −1)/12 = (122 −1)/12 = 143/12 = 11.9167
(S3) Prob [X = 2, 3, 12] = 1
12 + 1
12 + 1
12 = 3
12 = 0.25
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 6
Continuous Probability
Distributions
Contents
6.1
Arcsin distribution
6.1.1
Properties
6.1.2
Probability density function
6.2
Beta distribution
6.2.1
Properties
6.2.2
Probability density function
6.2.3
Related distributions
6.3
Cauchy distribution
6.3.1
Properties
6.3.2
Probability density function
6.3.3
Related distributions
6.4
Chi–square distribution
6.4.1
Properties
6.4.2
Probability density function
6.4.3
Related distributions
6.4.4
Critical values for chi–square distribution
6.4.5
Percentage points, chi–square over dof
6.5
Erlang distribution
6.5.1
Properties
6.5.2
Probability density function
6.5.3
Related distributions
6.6
Exponential distribution
6.6.1
Properties
6.6.2
Probability density function
6.6.3
Related distributions
6.7
Extreme–value distribution
6.7.1
Properties
6.7.2
Probability density function
6.7.3
Related distributions
c
⃝2000 by Chapman & Hall/CRC

6.8
F distribution
6.8.1
Properties
6.8.2
Probability density function
6.8.3
Related distributions
6.8.4
Critical values for the F distribution
6.9
Gamma distribution
6.9.1
Properties
6.9.2
Probability density function
6.9.3
Related distributions
6.10
Half–normal distribution
6.10.1
Properties
6.10.2
Probability density function
6.11
Inverse Gaussian (Wald) distribution
6.11.1
Properties
6.11.2
Probability density function
6.11.3
Related distributions
6.12
Laplace distribution
6.12.1
Properties
6.12.2
Probability density function
6.12.3
Related distributions
6.13
Logistic distribution
6.13.1
Properties
6.13.2
Probability density function
6.13.3
Related distributions
6.14
Lognormal distribution
6.14.1
Properties
6.14.2
Probability density function
6.14.3
Related distributions
6.15
Noncentral chi–square distribution
6.15.1
Properties
6.15.2
Probability density function
6.15.3
Related distributions
6.16
Noncentral F distribution
6.16.1
Properties
6.16.2
Probability density function
6.16.3
Related distributions
6.17
Noncentral t distribution
6.17.1
Properties
6.17.2
Probability density function
6.17.3
Related distributions
6.18
Normal distribution
6.18.1
Properties
c
⃝2000 by Chapman & Hall/CRC

6.18.2
Probability density function
6.18.3
Related distributions
6.19
Normal distribution: multivariate
6.19.1
Properties
6.19.2
Probability density function
6.20
Pareto distribution
6.20.1
Properties
6.20.2
Probability density function
6.20.3
Related distributions
6.21
Power function distribution
6.21.1
Properties
6.21.2
Probability density function
6.21.3
Related distributions
6.22
Rayleigh distribution
6.22.1
Properties
6.22.2
Probability density function
6.22.3
Related distributions
6.23
t distribution
6.23.1
Properties
6.23.2
Probability density function
6.23.3
Related distributions
6.23.4
Critical values for the t distribution
6.24
Triangular distribution
6.24.1
Properties
6.24.2
Probability density function
6.25
Uniform distribution
6.25.1
Properties
6.25.2
Probability density function
6.25.3
Related distributions
6.26
Weibull distribution
6.26.1
Properties
6.26.2
Probability density function
6.26.3
Related distributions
6.27
Relationships among distributions
6.27.1
Other relationships among distributions
This chapter presents some common continuous probability distributions along
with their properties. Relevant numerical tables are also included.
Notation used throughout this chapter:
c
⃝2000 by Chapman & Hall/CRC

Probability density function
f(x)
Prob [a ≤X ≤b] =
 b
a
f(x) dx
(pdf)
Cumulative distrib function
F(x) = Prob [X ≤x] =
 x
−∞
f(x) dx
(cdf)
Mean
µ = E [X]
Variance
σ2 = E

(X −µ)2
Coeﬃcient of skewness
β1 = E

(X −µ)3
/σ3
Coeﬃcient of kurtosis
β2 = E

(X −µ)4
/σ4
Moment generating function
m(t) = E

etX
(mgf)
Characteristic function
φ(t) = E

eitX
(char function)
6.1
ARCSIN DISTRIBUTION
6.1.1
Properties
pdf
f(x) =
1
π

x(1 −x)
,
0 < x < 1
mean
µ = 1/2
variance
σ2 = 1/8
skewness
β1 = 0
kurtosis
β2 = 3/2
mgf
m(t) = et/2I0(t/2)
char function
φ(t) = J0(t/2) cos(t/2) + iJ0(t/2) sin(t/2)
where Jn(x) is the Bessel function of the ﬁrst kind and Ip(x) is the modiﬁed
Bessel function of the ﬁrst kind deﬁned in Chapter 18 (see page 506).
6.1.2
Probability density function
The probability density function is “U” shaped. As x →0+ and as x →1−,
f(x) →∞.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.1: Probability density function for an arcsin random variable.
6.2
BETA DISTRIBUTION
6.2.1
Properties
pdf
f(x) = Γ(α + β)
Γ(α)Γ(β)xα−1(1 −x)β−1 = xα−1(1 −x)β−1
B(α, β)
0 ≤x ≤1,
α, β > 0
mean
µ =
α
α + β
variance
σ2 =
αβ
(α + β)2(α + β + 1)
skewness
β1 = 2(β −α)√α + β + 1
√αβ(α + β + 2)
kurtosis
β2 = 3(α + β + 1)[2(α + β)2 + αβ(α + β −6)]
αβ(α + β + 2)(α + β + 3)
mgf
m(t) = 1F1(α; β; t)
char function
φ(t) =
1
α + β

iat 2F3
'1
2 + α
2 , 1 + α
2
,
;
'3
2, 1
2 + α
2 + β
2 , 1 + α
2 + β
2
,
; −t2
4

+ 2F3
'1
2 + α
2 , α
2
,
;
'1
2, α
2 + β
2 , 1
2 + α
2 + β
2
,
; −t2
4

where Γ(x) is the gamma function, B(a, b) is the beta function, and pFq is
the generalized hypergeometric function deﬁned in Chapter 18 (see pages 515,
c
⃝2000 by Chapman & Hall/CRC

511, and 520).
The rth moment about the origin is
µ′
r = Γ(α + β)Γ(α + r)
Γ(α)Γ(α + β + r)
(6.1)
6.2.2
Probability density function
If α < 1 and β < 1 the probability density function is “U” shaped. If the
product (α−1)(β −1) < 0 the probability density function is “J” shaped. Let
f(x; α, β) denote the probability density function for a beta random variable
with parameters α and β.
If both α > 1 and β > 1 then f(x; α, β) and
f(x; β, α) are symmetric with respect to the line x = .5.
Figure 6.2: Probability density functions for a beta random variable, various
shape parameters.
6.2.3
Related distributions
Let X be a beta random variable with parameters α and β.
(1) If α = β = 1/2, then X is an arcsin random variable.
(2) If α = β = 1, then X is a uniform random variable with parameters
a = 0 and b = 1.
(3) If β = 1, then X is a power function random variable with parameters
b = 1 and c = α.
(4) As α and β tend to inﬁnity such that α/β is constant, X tends to a
standard normal random variable.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.3: Probability density functions for a beta random variable, example
of symmetry.
6.3
CAUCHY DISTRIBUTION
6.3.1
Properties
pdf
f(x) =
1
bπ

1 +
 x−a
b
2,
x ∈R, a ∈R, b > 0
mean
µ = does not exist
variance
σ2 = does not exist
skewness
β1 = does not exist
kurtosis
β2 = does not exist
mgf
m(t) = does not exist
char function
φ(t) = eait−b|t|
6.3.2
Probability density function
The probability density function for a Cauchy random variable is unimodal
and symmetric about the parameter a. The tails are heavier than those of a
normal random variable.
6.3.3
Related distributions
Let X be a Cauchy random variable with parameters a and b.
(1) If a = 0 and b = 1 then X is a standard Cauchy random variable.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.4: Probability density functions for a Cauchy random variable.
(2) The random variable 1/X is also a Cauchy random variable with pa-
rameters a/(a2 + b2) and b/(a2 + b2).
(3) Let Xi (for i = 1, 2, . . . , n) be independent, Cauchy random variables
with parameters ai and bi, respectively.
The random variable Y =
X1 + X2 + · · · + Xn has a Cauchy distribution with parameters a =
a1 + a2 + · · · + an and b = b1 + b2 + · · · + bn.
6.4
CHI–SQUARE DISTRIBUTION
6.4.1
Properties
pdf
f(x) = e−x/2x(ν/2)−1
2ν/2Γ(ν/2) ,
x ≥0, ν ∈N
mean
µ = ν
variance
σ2 = 2ν
skewness
β1 = 2

2/ν
kurtosis
β2 = 3 + 12
ν
mgf
m(t) = (1 −2t)−ν/2,
t < 1/2
char function
φ(t) = (1 −2it)−ν/2
where Γ(x) is the gamma function (see page 515).
A chi–square(χ2) distribution is completely characterized by the parameter
ν, the degrees of freedom.
c
⃝2000 by Chapman & Hall/CRC

6.4.2
Probability density function
The probability density function for a chi–square random variable is positively
skewed. As ν tends to inﬁnity, the density function becomes more bell–shaped
and symmetric.
Figure 6.5: Probability density functions for a chi–square random variable.
6.4.3
Related distributions
(1) If X is a chi–square random variable with ν = 2, then X is an exponen-
tial random variable with λ = 1/2.
(2) If X1 and X2 are independent chi–square random variables with param-
eters ν1 and ν2, then the random variable (X1/ν1)/(X2/ν2) has an F
distribution with ν1 and ν2 degrees of freedom.
(3) If X1 and X2 are independent chi–square random variables with param-
eters ν1 = ν2 = ν, the random variable
Y =
√ν
2
X1 −X2
√X1X2
(6.2)
has a t distribution with ν degrees of freedom.
(4) Let Xi (for i = 1, 2, . . . , n) be independent chi–square random variables
with parameters νi. The random variable Y = X1 + X2 + · · · + Xn has
a chi–square distribution with ν = ν1 +ν2 +· · ·+νn degrees of freedom.
(5) If X is a chi–square random variable with ν degrees of freedom, the
random variable
√
X has a chi distribution with parameter ν.
Properties of a chi random variable:
c
⃝2000 by Chapman & Hall/CRC

pdf
f(x) =
xn−1e−x2/2
2(n/2)−1Γ(n/2),
x ≥0, n ∈N
mean
µ = Γ
 n+1
2

Γ
 n
2

variance
σ2 = Γ
 n+2
2

Γ
 n
2

−
%
Γ
 n+1
2

Γ
 n
2

&2
where Γ(x) is the gamma function (see page 515).
If X is a chi random variable with parameter n = 2, then X is a Rayleigh
random variable with σ = 1.
6.4.4
Critical values for chi–square distribution
The following tables give values of χ2
α,ν such that
1 −α = F

χ2
α,ν

=
 χ2
α,ν
0
1
2ν/2 Γ(ν/2)x(ν−2)/2e−x/2 dx
(6.3)
where ν, the number of degrees of freedom, varies from 1 to 10,000 and α
varies from 0.0001 to 0.9999.
(a) For ν > 30, the expression

2χ2−√2ν −1 is approximately a standard
normal distribution. Hence, χ2
α,ν is approximately
χ2
α,ν ≈1
2

zα +
√
2ν −1
2
for ν ≫1
(6.4)
(b) For even values of ν, F

χ2
α,ν

can be written as
1 −F

χ2
α,ν

=
x′−1

x=0
e−λλx
x!
(6.5)
with λ = χ2
α,ν/2 and x′ = ν/2. Hence, the cumulative chi–square dis-
tribution is related to the cumulative Poisson distribution.
Example 6.40:
Use the table on page 121 to ﬁnd the values χ2
.99,36 and χ2
.05,20.
Solution:
(S1) The left–hand column of the table on page 121 contains entries for the num-
ber of degrees of freedom and the top row lists values for α. The intersection
of the ν degrees of freedom row and the α column contains χ2
α,ν such that
Prob 
χ2 ≥χ2
α,ν

= α.
(S2) χ2
.99,36 = 19.2327
=⇒
Prob 
χ2 ≥19.2327
= .99
χ2
.05,20 = 31.4104
=⇒
Prob 
χ2 ≥31.4104
= .05
c
⃝2000 by Chapman & Hall/CRC

Critical values for the chi–square distribution χ2
α,ν.
α
ν
.9999
.9995
.999
.995
.99
.975
.95
.90
1
.07157
.06393
.05157
.04393
.0002
.0010
.0039
.0158
2
.0002
.0010
.0020
.0100
.0201
.0506
.1026
.2107
3
.0052
.0153
.0243
.0717
.1148
.2158
.3518
.5844
4
.0284
.0639
.0908
.2070
.2971
.4844
.7107
1.0636
5
.0822
.1581
.2102
.4117
.5543
.8312
1.1455
1.6103
6
.1724
.2994
.3811
.6757
.8721
1.2373
1.6354
2.2041
7
.3000
.4849
.5985
.9893
1.2390
1.6899
2.1673
2.8331
8
.4636
.7104
.8571
1.3444
1.6465
2.1797
2.7326
3.4895
9
.6608
.9717
1.1519
1.7349
2.0879
2.7004
3.3251
4.1682
10
.8889
1.2650
1.4787
2.1559
2.5582
3.2470
3.9403
4.8652
11
1.1453
1.5868
1.8339
2.6032
3.0535
3.8157
4.5748
5.5778
12
1.4275
1.9344
2.2142
3.0738
3.5706
4.4038
5.2260
6.3038
13
1.7333
2.3051
2.6172
3.5650
4.1069
5.0088
5.8919
7.0415
14
2.0608
2.6967
3.0407
4.0747
4.6604
5.6287
6.5706
7.7895
15
2.4082
3.1075
3.4827
4.6009
5.2293
6.2621
7.2609
8.5468
16
2.7739
3.5358
3.9416
5.1422
5.8122
6.9077
7.9616
9.3122
17
3.1567
3.9802
4.4161
5.6972
6.4078
7.5642
8.6718 10.0852
18
3.5552
4.4394
4.9048
6.2648
7.0149
8.2307
9.3905 10.8649
19
3.9683
4.9123
5.4068
6.8440
7.6327
8.9065 10.1170 11.6509
20
4.3952
5.3981
5.9210
7.4338
8.2604
9.5908 10.8508 12.4426
21
4.8348
5.8957
6.4467
8.0337
8.8972 10.2829 11.5913 13.2396
22
5.2865
6.4045
6.9830
8.6427
9.5425 10.9823 12.3380 14.0415
23
5.7494
6.9237
7.5292
9.2604 10.1957 11.6886 13.0905 14.8480
24
6.2230
7.4527
8.0849
9.8862 10.8564 12.4012 13.8484 15.6587
25
6.7066
7.9910
8.6493 10.5197 11.5240 13.1197 14.6114 16.4734
26
7.1998
8.5379
9.2221 11.1602 12.1981 13.8439 15.3792 17.2919
27
7.7019
9.0932
9.8028 11.8076 12.8785 14.5734 16.1514 18.1139
28
8.2126
9.6563 10.3909 12.4613 13.5647 15.3079 16.9279 18.9392
29
8.7315 10.2268 10.9861 13.1211 14.2565 16.0471 17.7084 19.7677
30
9.2581 10.8044 11.5880 13.7867 14.9535 16.7908 18.4927 20.5992
31
9.7921 11.3887 12.1963 14.4578 15.6555 17.5387 19.2806 21.4336
32
10.3331 11.9794 12.8107 15.1340 16.3622 18.2908 20.0719 22.2706
33
10.8810 12.5763 13.4309 15.8153 17.0735 19.0467 20.8665 23.1102
34
11.4352 13.1791 14.0567 16.5013 17.7891 19.8063 21.6643 23.9523
35
11.9957 13.7875 14.6878 17.1918 18.5089 20.5694 22.4650 24.7967
36
12.5622 14.4012 15.3241 17.8867 19.2327 21.3359 23.2686 25.6433
37
13.1343 15.0202 15.9653 18.5858 19.9602 22.1056 24.0749 26.4921
38
13.7120 15.6441 16.6112 19.2889 20.6914 22.8785 24.8839 27.3430
39
14.2950 16.2729 17.2616 19.9959 21.4262 23.6543 25.6954 28.1958
40
14.8831 16.9062 17.9164 20.7065 22.1643 24.4330 26.5093 29.0505
c
⃝2000 by Chapman & Hall/CRC

Critical values for the chi–square distribution χ2
α,ν.
α
ν
.9999
.9995
.999
.995
.99
.975
.95
.90
41
15.48
17.54
18.58
21.42
22.91
25.21
27.33
29.91
42
16.07
18.19
19.24
22.14
23.65
26.00
28.14
30.77
43
16.68
18.83
19.91
22.86
24.40
26.79
28.96
31.63
44
17.28
19.48
20.58
23.58
25.15
27.57
29.79
32.49
45
17.89
20.14
21.25
24.31
25.90
28.37
30.61
33.35
46
18.51
20.79
21.93
25.04
26.66
29.16
31.44
34.22
47
19.13
21.46
22.61
25.77
27.42
29.96
32.27
35.08
48
19.75
22.12
23.29
26.51
28.18
30.75
33.10
35.95
49
20.38
22.79
23.98
27.25
28.94
31.55
33.93
36.82
50
21.01
23.46
24.67
27.99
29.71
32.36
34.76
37.69
60
27.50
30.34
31.74
35.53
37.48
40.48
43.19
46.46
70
34.26
37.47
39.04
43.28
45.44
48.76
51.74
55.33
80
41.24
44.79
46.52
51.17
53.54
57.15
60.39
64.28
90
48.41
52.28
54.16
59.20
61.75
65.65
69.13
73.29
100
55.72
59.90
61.92
67.33
70.06
74.22
77.93
82.36
200
134.02
140.66
143.84
152.24
156.43
162.73
168.28
174.84
300
217.33
225.89
229.96
240.66
245.97
253.91
260.88
269.07
400
303.26
313.43
318.26
330.90
337.16
346.48
354.64
364.21
500
390.85
402.45
407.95
422.30
429.39
439.94
449.15
459.93
600
479.64
492.52
498.62
514.53
522.37
534.02
544.18
556.06
700
569.32
583.39
590.05
607.38
615.91
628.58
639.61
652.50
800
659.72
674.89
682.07
700.73
709.90
723.51
735.36
749.19
900
750.70
766.91
774.57
794.47
804.25
818.76
831.37
846.07
1000
842.17
859.36
867.48
888.56
898.91
914.26
927.59
943.13
1500 1304.80 1326.30 1336.42 1362.67 1375.53 1394.56 1411.06 1430.25
2000 1773.30 1798.42 1810.24 1840.85 1855.82 1877.95 1897.12 1919.39
2500 2245.54 2273.86 2287.17 2321.62 2338.45 2363.31 2384.84 2409.82
3000 2720.44 2751.65 2766.32 2804.23 2822.75 2850.08 2873.74 2901.17
3500 3197.36 3231.23 3247.14 3288.25 3308.31 3337.92 3363.53 3393.22
4000 3675.88 3712.22 3729.29 3773.37 3794.87 3826.60 3854.03 3885.81
4500 4155.71 4194.37 4212.52 4259.39 4282.25 4315.96 4345.10 4378.86
5000 4636.62 4677.48 4696.67 4746.17 4770.31 4805.90 4836.66 4872.28
5500 5118.47 5161.42 5181.58 5233.60 5258.96 5296.34 5328.63 5366.03
6000 5601.13 5646.08 5667.17 5721.59 5748.11 5787.20 5820.96 5860.05
6500 6084.50 6131.36 6153.35 6210.07 6237.70 6278.43 6313.60 6354.32
7000 6568.49 6617.20 6640.05 6698.98 6727.69 6769.99 6806.52 6848.80
7500 7053.05 7103.53 7127.22 7188.28 7218.03 7261.85 7299.69 7343.48
8000 7538.11 7590.32 7614.81 7677.94 7708.68 7753.98 7793.08 7838.33
8500 8023.63 8077.51 8102.78 8167.91 8199.63 8246.35 8286.68 8333.34
9000 8509.57 8565.07 8591.09 8658.17 8690.83 8738.94 8780.46 8828.50
9500 8995.90 9052.97 9079.73 9148.70 9182.28 9231.74 9274.42 9323.78
10000 9482.59 9541.19 9568.67 9639.48 9673.95 9724.72 9768.53 9819.19
c
⃝2000 by Chapman & Hall/CRC

Critical values for the chi–square distribution χ2
α,ν.
α
ν
.10
.05
.025
.01
.005
.001
.0005
.0001
1
2.7055
3.8415
5.0239
6.6349
7.8794 10.8276 12.1157 15.1367
2
4.6052
5.9915
7.3778
9.2103 10.5966 13.8155 15.2018 18.4207
3
6.2514
7.8147
9.3484 11.3449 12.8382 16.2662 17.7300 21.1075
4
7.7794
9.4877 11.1433 13.2767 14.8603 18.4668 19.9974 23.5127
5
9.2364 11.0705 12.8325 15.0863 16.7496 20.5150 22.1053 25.7448
6
10.6446 12.5916 14.4494 16.8119 18.5476 22.4577 24.1028 27.8563
7
12.0170 14.0671 16.0128 18.4753 20.2777 24.3219 26.0178 29.8775
8
13.3616 15.5073 17.5345 20.0902 21.9550 26.1245 27.8680 31.8276
9
14.6837 16.9190 19.0228 21.6660 23.5894 27.8772 29.6658 33.7199
10
15.9872 18.3070 20.4832 23.2093 25.1882 29.5883 31.4198 35.5640
11
17.2750 19.6751 21.9200 24.7250 26.7568 31.2641 33.1366 37.3670
12
18.5493 21.0261 23.3367 26.2170 28.2995 32.9095 34.8213 39.1344
13
19.8119 22.3620 24.7356 27.6882 29.8195 34.5282 36.4778 40.8707
14
21.0641 23.6848 26.1189 29.1412 31.3193 36.1233 38.1094 42.5793
15
22.3071 24.9958 27.4884 30.5779 32.8013 37.6973 39.7188 44.2632
16
23.5418 26.2962 28.8454 31.9999 34.2672 39.2524 41.3081 45.9249
17
24.7690 27.5871 30.1910 33.4087 35.7185 40.7902 42.8792 47.5664
18
25.9894 28.8693 31.5264 34.8053 37.1565 42.3124 44.4338 49.1894
19
27.2036 30.1435 32.8523 36.1909 38.5823 43.8202 45.9731 50.7955
20
28.4120 31.4104 34.1696 37.5662 39.9968 45.3147 47.4985 52.3860
21
29.6151 32.6706 35.4789 38.9322 41.4011 46.7970 49.0108 53.9620
22
30.8133 33.9244 36.7807 40.2894 42.7957 48.2679 50.5111 55.5246
23
32.0069 35.1725 38.0756 41.6384 44.1813 49.7282 52.0002 57.0746
24
33.1962 36.4150 39.3641 42.9798 45.5585 51.1786 53.4788 58.6130
25
34.3816 37.6525 40.6465 44.3141 46.9279 52.6197 54.9475 60.1403
26
35.5632 38.8851 41.9232 45.6417 48.2899 54.0520 56.4069 61.6573
27
36.7412 40.1133 43.1945 46.9629 49.6449 55.4760 57.8576 63.1645
28
37.9159 41.3371 44.4608 48.2782 50.9934 56.8923 59.3000 64.6624
29
39.0875 42.5570 45.7223 49.5879 52.3356 58.3012 60.7346 66.1517
30
40.2560 43.7730 46.9792 50.8922 53.6720 59.7031 62.1619 67.6326
31
41.4217 44.9853 48.2319 52.1914 55.0027 61.0983 63.5820 69.1057
32
42.5847 46.1943 49.4804 53.4858 56.3281 62.4872 64.9955 70.5712
33
43.7452 47.3999 50.7251 54.7755 57.6484 63.8701 66.4025 72.0296
34
44.9032 48.6024 51.9660 56.0609 58.9639 65.2472 67.8035 73.4812
35
46.0588 49.8018 53.2033 57.3421 60.2748 66.6188 69.1986 74.9262
36
47.2122 50.9985 54.4373 58.6192 61.5812 67.9852 70.5881 76.3650
37
48.3634 52.1923 55.6680 59.8925 62.8833 69.3465 71.9722 77.7977
38
49.5126 53.3835 56.8955 61.1621 64.1814 70.7029 73.3512 79.2247
39
50.6598 54.5722 58.1201 62.4281 65.4756 72.0547 74.7253 80.6462
40
51.8051 55.7585 59.3417 63.6907 66.7660 73.4020 76.0946 82.0623
c
⃝2000 by Chapman & Hall/CRC

Critical values for the chi–square distribution χ2
α,ν.
α
ν
.10
.05
.025
.01
.005
.001
.0005
.0001
41
52.95
56.94
60.56
64.95
68.05
74.74
77.46
83.47
42
54.09
58.12
61.78
66.21
69.34
76.08
78.82
84.88
43
55.23
59.30
62.99
67.46
70.62
77.42
80.18
86.28
44
56.37
60.48
64.20
68.71
71.89
78.75
81.53
87.68
45
57.51
61.66
65.41
69.96
73.17
80.08
82.88
89.07
46
58.64
62.83
66.62
71.20
74.44
81.40
84.22
90.46
47
59.77
64.00
67.82
72.44
75.70
82.72
85.56
91.84
48
60.91
65.17
69.02
73.68
76.97
84.04
86.90
93.22
49
62.04
66.34
70.22
74.92
78.23
85.35
88.23
94.60
50
63.17
67.50
71.42
76.15
79.49
86.66
89.56
95.97
60
74.40
79.08
83.30
88.38
91.95
99.61
102.69
109.50
70
85.53
90.53
95.02
100.43
104.21
112.32
115.58
122.75
80
96.58
101.88
106.63
112.33
116.32
124.84
128.26
135.78
90
107.57
113.15
118.14
124.12
128.30
137.21
140.78
148.63
100
118.50
124.34
129.56
135.81
140.17
149.45
153.17
161.32
200
226.02
233.99
241.06
249.45
255.26
267.54
272.42
283.06
300
331.79
341.40
349.87
359.91
366.84
381.43
387.20
399.76
400
436.65
447.63
457.31
468.72
476.61
493.13
499.67
513.84
500
540.93
553.13
563.85
576.49
585.21
603.45
610.65
626.24
600
644.80
658.09
669.77
683.52
692.98
712.77
720.58
737.46
700
748.36
762.66
775.21
789.97
800.13
821.35
829.71
847.78
800
851.67
866.91
880.28
895.98
906.79
929.33
938.21
957.38
900
954.78
970.90
985.03
1001.63
1013.04
1036.83
1046.19
1066.40
1000
1057.72
1074.68
1089.53
1106.97
1118.95
1143.92
1153.74
1174.93
1500
1570.61
1591.21
1609.23
1630.35
1644.84
1674.97
1686.81
1712.30
2000
2081.47
2105.15
2125.84
2150.07
2166.66
2201.16
2214.68
2243.81
2500
2591.04
2617.43
2640.47
2667.43
2685.89
2724.22
2739.25
2771.57
3000
3099.69
3128.54
3153.70
3183.13
3203.28
3245.08
3261.45
3296.66
3500
3607.64
3638.75
3665.87
3697.57
3719.26
3764.26
3781.87
3819.74
4000
4115.05
4148.25
4177.19
4211.01
4234.14
4282.11
4300.88
4341.22
4500
4622.00
4657.17
4687.83
4723.63
4748.12
4798.87
4818.73
4861.40
5000
5128.58
5165.61
5197.88
5235.57
5261.34
5314.73
5335.62
5380.48
5500
5634.83
5673.64
5707.45
5746.93
5773.91
5829.81
5851.68
5898.63
6000
6140.81
6181.31
6216.59
6257.78
6285.92
6344.23
6367.02
6415.98
6500
6646.54
6688.67
6725.36
6768.18
6797.45
6858.05
6881.74
6932.61
7000
7152.06
7195.75
7233.79
7278.19
7308.53
7371.35
7395.90
7448.62
7500
7657.38
7702.58
7741.93
7787.86
7819.23
7884.18
7909.57
7964.06
8000
8162.53
8209.19
8249.81
8297.20
8329.58
8396.59
8422.78
8479.00
8500
8667.52
8715.59
8757.44
8806.26
8839.60
8908.62
8935.59
8993.48
9000
9172.36
9221.81
9264.85
9315.05
9349.34
9420.30
9448.03
9507.53
9500
9677.07
9727.86
9772.05
9823.60
9858.81
9931.67
9960.13 10021.21
10000 10181.66 10233.75 10279.07 10331.93 10368.03 10442.73 10471.91 10534.52
c
⃝2000 by Chapman & Hall/CRC

6.4.5
Percentage points, chi–square over degrees of freedom
distribution
The following table gives the percentage points of the sampling distribution
of s2/σ2, referred to as the percentage points of the χ2/d.f. distribution (read
“chi–square over degrees of freedom”). These percentage points are a function
of the sample size.
Probability in percent
Probability in percent
ν
0.05
0.1
0.5
1.0
2.5
5.0
95
97.5
99
99.5
99.9
99.95
1 0.000 0.000 0.000 0.000 0.001 0.004
3.841 5.024 6.635 7.879 10.828 12.116
2 0.001 0.001 0.005 0.010 0.025 0.051
2.996 3.689 4.605 5.298
6.908
7.601
3 0.005 0.008 0.024 0.038 0.072 0.117
2.605 3.116 3.782 4.279
5.422
5.910
4 0.016 0.023 0.052 0.074 0.121 0.178
2.372 2.786 3.319 3.715
4.617
4.999
5 0.032 0.042 0.082 0.111 0.166 0.229
2.214 2.567 3.017 3.350
4.103
4.421
6 0.050 0.064 0.113 0.145 0.206 0.273
2.099 2.408 2.802 3.091
3.743
4.017
7 0.069 0.085 0.141 0.177 0.241 0.310
2.010 2.288 2.639 2.897
3.475
3.717
8 0.089 0.107 0.168 0.206 0.272 0.342
1.938 2.192 2.511 2.744
3.266
3.484
9 0.108 0.128 0.193 0.232 0.300 0.369
1.880 2.114 2.407 2.621
3.097
3.296
10 0.126 0.148 0.216 0.256 0.325 0.394
1.831 2.048 2.321 2.519
2.959
3.142
15 0.207 0.232 0.307 0.349 0.417 0.484
1.666 1.833 2.039 2.187
2.513
2.648
20 0.270 0.296 0.372 0.413 0.480 0.543
1.571 1.708 1.878 2.000
2.266
2.375
25 0.320 0.346 0.421 0.461 0.525 0.584
1.506 1.626 1.773 1.877
2.105
2.198
30 0.360 0.386 0.460 0.498 0.560 0.616
1.459 1.566 1.696 1.789
1.990
2.072
40 0.423 0.448 0.518 0.554 0.611 0.663
1.394 1.484 1.592 1.669
1.835
1.902
50 0.469 0.493 0.560 0.594 0.647 0.695
1.350 1.428 1.523 1.590
1.733
1.791
75 0.548 0.570 0.629 0.660 0.706 0.747
1.283 1.345 1.419 1.470
1.581
1.626
100 0.599 0.619 0.673 0.701 0.742 0.779
1.243 1.296 1.358 1.402
1.494
1.532
150 0.663 0.681 0.728 0.751 0.787 0.818
1.197 1.239 1.288 1.322
1.395
1.424
200 0.703 0.719 0.761 0.782 0.814 0.841
1.170 1.205 1.247 1.276
1.338
1.362
500 0.810 0.816 0.845 0.859 0.880 0.898
1.111 1.128 1.153 1.170
1.207
1.221
1000 0.859 0.868 0.889 0.899 0.914 0.928
1.075 1.090 1.107 1.119
1.144
1.154
6.5
ERLANG DISTRIBUTION
6.5.1
Properties
pdf
f(x) = xn−1 e−x/β
βn(n −1)! ,
x ≥0, β > 0, n ∈N
mean
µ = nβ
variance
σ2 = nβ2
skewness
β1 = 2/√n
kurtosis
β2 = 3 + 6
n
mgf
m(t) = (1 −βt)−n
char function
φ(t) = (1 −βit)−n
c
⃝2000 by Chapman & Hall/CRC

6.5.2
Probability density function
The probability density function is skewed to the right with n as the shape
parameter.
Figure 6.6: Probability density functions for an Erlang random variable.
6.5.3
Related distributions
If X is an Erlang random variable with parameters β and n = 1, then X is
an exponential random variable with parameter λ = 1/β.
6.6
EXPONENTIAL DISTRIBUTION
6.6.1
Properties
pdf
f(x) = λe−λx,
x ≥0, λ > 0
mean
µ = 1/λ
variance
σ2 = 1/λ2
skewness
β1 = 2
kurtosis
β2 = 9
mgf
m(t) =
λ
λ −t
char function
φ(t) =
λ
λ −it
6.6.2
Probability density function
The probability density function is skewed to the right. The tail of the distri-
bution is heavier for larger values of λ.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.7: Probability density functions for an exponential random variable.
6.6.3
Related distributions
Let X be an exponential random variable with parameter λ.
(1) If λ = 1/2, then X is a chi–square random variable with ν = 2.
(2) The random variable
√
X has a Rayleigh distribution with parameter
σ =

1/(2λ).
(3) The random variable Y = X1/α has a Weibull distribution with param-
eters α and λ−1/α.
(4) The random variable Y = e−X has a power function distribution with
parameters b = 1 and c = λ.
(5) The random variable Y = aeX has a Pareto distribution with parame-
ters a and θ = λ.
(6) The random variable Y = α −ln X has an extreme–value distribution
with parameters α and β = 1/λ.
(7) Let X1, X2, . . . , Xn be independent exponential random variables each
with parameter λ.
(a) The random variable Y = min(X1, X2, . . . , Xn) has an exponential
distribution with parameter nλ.
(b) The random variable Y = X1 + X2 + · · · + Xn has an Erlang
distribution with parameters β = 1/λ and n.
(8) Let X1 and X2 be independent exponential random variables each with
parameter λ. The random variable Y = X1 −X2 has a Laplace distri-
bution with parameters 0 and 1/λ.
(9) Let X be an exponential random variable with parameter λ = 1. The
random variable Y = −ln[e−X/(1 + e−X)] has a (standard) logistic
c
⃝2000 by Chapman & Hall/CRC

distribution with parameters α = 0 and β = 1.
(10) Let X1 and X2 be independent exponential random variables with pa-
rameter λ = 1.
(a) The random variable Y = X1/(X1 + X2) has a (standard) uniform
distribution with parameters a = 0 and b = 1.
(b) The random variable W = −ln(X1/X2) has a (standard) logistic
distribution with parameters α = 0 and β = 1.
6.7
EXTREME–VALUE DISTRIBUTION
6.7.1
Properties
pdf
f(x) = (1/β)e−(x−α)/βe[−e−(x−α)/β]
x, α ∈R, β > 0
mean
µ = α + γβ,
γ = 0.5772156649 . . . (Euler’s constant)
variance
σ2 = π2β2
6
skewness
β1 = −6
√
6 ϕ′′(1)
π3
kurtosis
β2 = 27/5 = 5.4
mgf
m(t) = eαtΓ(1 −βt),
t < 1/β
char function
φ(t) = eαitΓ(1 −βit)
where Γ(x) is the gamma function and ϕ(x) is the digamma function (see
pages 515 and 518).
6.7.2
Probability density function
The probability density function is skewed slightly to the right with location
parameter α.
6.7.3
Related distributions
(1) The standard extreme–value distribution has α = 0 and β = 1.
(2) If X is an extreme–value random variable with parameters α and β,
then the random variable Y = (X −α)/β has a (standard) extreme–
value distribution with parameters 0 and 1.
(3) If X is a (standard) extreme–value random variable with parameters
α = 0 and β = 1, then the random variable Y = e(−e−X/c) has a power
function distribution with parameters b = 0 and c.
(4) If X is a extreme–value random variable with parameters α = 0 and
β = 1, then the random variable Y = a
"
1 −e(−e−X)#1/θ
has a Pareto
distribution with parameters a and θ.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.8: Probability density functions for an extreme–value random vari-
able.
(5) Let X1 and X2 be independent extreme–value random variables with
parameters α and β. The random variable Y = X1 −X2 has a logistic
distribution with parameters 0 and β.
6.8
F DISTRIBUTION
6.8.1
Properties
pdf
f(x) = Γ
 ν1+ν2
2

ν
ν1
2
1 ν
ν2
2
2
Γ(ν1/2)Γ(ν2/2) x(ν1/2)−1 (ν2 + ν1x)−(ν1+ν2)/2
x > 0, ν1, ν2 > 0
mean
µ =
ν2
ν2 −2,
ν2 ≥3
variance
σ2 =
2ν2
2(ν1 + ν2 −2)
ν1(ν2 −2)2(ν2 −4),
ν2 ≥5
skewness
β1 = (2ν1 + ν2 −2)

8(ν2 −4)
√ν1(ν2 −6)√ν1 + ν2 −2 ,
ν2 ≥7
kurtosis
β2 = 3 +
12[(ν2 −2)2(ν2 −4)+ν1(ν1 + ν2 −2)(5ν2 −22)]
ν1(ν2 −6)(ν2 −8)(ν1 + ν2 −2)
ν2 ≥9
c
⃝2000 by Chapman & Hall/CRC

mgf
m(t) = does not exist
char function
φ(t) = Γ
ν1 + ν2
2

Γ
ν2
2

ψ
ν1
2 , 1 −ν2
2 ; itν2
ν1

where Γ(x) is the gamma function and ψ is the conﬂuent hypergeometric
function of the second kind (see pages 515 and 521).
6.8.2
Probability density function
The probability density function is skewed to the right with shape parameters
ν1 and ν2. For ﬁxed ν2, the tail becomes lighter as ν1 increases.
Figure 6.9: Probability density functions for an F random variable.
6.8.3
Related distributions
(1) If X has an F distribution with ν1 and ν2 degrees of freedom, then the
random variable Y = 1/X has an F distribution with ν2 and ν1 degrees
of freedom.
(2) If X has an F distribution with ν1 and ν2 degrees of freedom, the ran-
dom variable ν1X tends to a chi–square distribution with ν1 degrees of
freedom as ν2 →∞.
(3) Let X1 and X2 be independent F random variables with ν1 = ν2 = ν
degrees of freedom. The random variable
Y =
√ν
2

X1 −

X2

(6.6)
has a t distribution with ν degrees of freedom.
c
⃝2000 by Chapman & Hall/CRC

(4) If X has an F distribution with parameters ν1 and ν2, the random
variable
Y = ν1X/ν2
1 + ν1X
ν2
(6.7)
has a beta distribution with parameters α = ν2/2 and β = ν1/2.
6.8.4
Critical values for the F distribution
Given values of ν1, ν2, and α, the tables on pages 132–137 contain values of
Fα,ν1,ν2 such that
1 −α =
 Fα,ν1,ν2
0
f(x) dx
=
 Fα,ν1,ν2
0
Γ
 ν1+ν2
2

ν
ν1
2
1 ν
ν2
2
2
Γ(ν1/2)Γ(ν2/2) x(ν1/2)−1 (ν2 + ν1x)−(ν1+ν2)/2 dx
(6.8)
Note that F1−α for ν1 and ν2 degrees of freedom is the reciprocal of Fα for ν2
and ν1 degrees of freedom. For example,
F.05,4,7 =
1
F.95,7,4
=
1
6.09 = .164
(6.9)
Example 6.41:
Use the following tables to ﬁnd the values F.1,4,9 and F.95,12,15.
Solution:
(S1) The top rows of the tables on pages 132–137 contain entries for the numera-
tor degrees of freedom and the left–hand column contains the denominator de-
grees of freedom.
The intersection of the ν1 degrees of freedom column and
the ν2 row may be used to ﬁnd critical values of the form Fα,ν1,ν2 such that
Prob [F ≥Fα,ν1,ν2] = α.
(S2) F.1,4,9 = 2.69
=⇒
Prob [F ≥2.69] = .1
F.95,12,15 =
1
F.05,15,12 =
1
2.62 = .3817
=⇒
Prob [F ≥.3817] = .95
(S3) Illustrations:
c
⃝2000 by Chapman & Hall/CRC

Critical values for the F distribution
For given values of ν1 and ν2, the following table contains values of F0.1,ν1,ν2;
deﬁned by Prob [F ≥F0.1,ν1,ν2] = α = 0.1.
ν2
ν1 =1
2
3
4
5
6
7
8
9
10
50
100
∞
1
39.86
49.50
53.59
55.83
57.24
58.20
58.91
59.44
59.86
60.19
62.69
63.01
63.33
2
8.53
9.00
9.16
9.24
9.29
9.33
9.35
9.37
9.38
9.39
9.47
9.48
9.49
3
5.54
5.46
5.39
5.34
5.31
5.28
5.27
5.25
5.24
5.23
5.15
5.14
5.13
4
4.54
4.32
4.19
4.11
4.05
4.01
3.98
3.95
3.94
3.92
3.80
3.78
3.76
5
4.06
3.78
3.62
3.52
3.45
3.40
3.37
3.34
3.32
3.30
3.15
3.13
3.10
6
3.78
3.46
3.29
3.18
3.11
3.05
3.01
2.98
2.96
2.94
2.77
2.75
2.72
7
3.59
3.26
3.07
2.96
2.88
2.83
2.78
2.75
2.72
2.70
2.52
2.50
2.47
8
3.46
3.11
2.92
2.81
2.73
2.67
2.62
2.59
2.56
2.54
2.35
2.32
2.29
9
3.36
3.01
2.81
2.69
2.61
2.55
2.51
2.47
2.44
2.42
2.22
2.19
2.16
10
3.29
2.92
2.73
2.61
2.52
2.46
2.41
2.38
2.35
2.32
2.12
2.09
2.06
11
3.23
2.86
2.66
2.54
2.45
2.39
2.34
2.30
2.27
2.25
2.04
2.01
1.97
12
3.18
2.81
2.61
2.48
2.39
2.33
2.28
2.24
2.21
2.19
1.97
1.94
1.90
13
3.14
2.76
2.56
2.43
2.35
2.28
2.23
2.20
2.16
2.14
1.92
1.88
1.85
14
3.10
2.73
2.52
2.39
2.31
2.24
2.19
2.15
2.12
2.10
1.87
1.83
1.80
15
3.07
2.70
2.49
2.36
2.27
2.21
2.16
2.12
2.09
2.06
1.83
1.79
1.76
16
3.05
2.67
2.46
2.33
2.24
2.18
2.13
2.09
2.06
2.03
1.79
1.76
1.72
17
3.03
2.64
2.44
2.31
2.22
2.15
2.10
2.06
2.03
2.00
1.76
1.73
1.69
18
3.01
2.62
2.42
2.29
2.20
2.13
2.08
2.04
2.00
1.98
1.74
1.70
1.66
19
2.99
2.61
2.40
2.27
2.18
2.11
2.06
2.02
1.98
1.96
1.71
1.67
1.63
20
2.97
2.59
2.38
2.25
2.16
2.09
2.04
2.00
1.96
1.94
1.69
1.65
1.61
25
2.92
2.53
2.32
2.18
2.09
2.02
1.97
1.93
1.89
1.87
1.61
1.56
1.52
50
2.81
2.41
2.20
2.06
1.97
1.90
1.84
1.80
1.76
1.73
1.44
1.39
1.34
100
2.76
2.36
2.14
2.00
1.91
1.83
1.78
1.73
1.69
1.66
1.35
1.29
1.20
∞
2.71
2.30
2.08
1.94
1.85
1.77
1.72
1.67
1.63
1.60
1.24
1.17
1.00
c
⃝2000 by Chapman & Hall/CRC

Critical values for the F distribution
For given values of ν1 and ν2, the following table contains values of
F0.05,ν1,ν2; deﬁned by Prob [F ≥F0.05,ν1,ν2] = α = 0.05.
ν2
ν1 =1
2
3
4
5
6
7
8
9
10
50
100
∞
1
161.4
199.5
215.7
224.6
230.2
234.0
236.8
238.9
240.5
241.9
251.8
253.0
254.3
2
18.51
19.00
19.16
19.25
19.30
19.33
19.35
19.37
19.38
19.40
19.48
19.49
19.50
3
10.13
9.55
9.28
9.12
9.01
8.94
8.89
8.85
8.81
8.79
8.58
8.55
8.53
4
7.71
6.94
6.59
6.39
6.26
6.16
6.09
6.04
6.00
5.96
5.70
5.66
5.63
5
6.61
5.79
5.41
5.19
5.05
4.95
4.88
4.82
4.77
4.74
4.44
4.41
4.36
6
5.99
5.14
4.76
4.53
4.39
4.28
4.21
4.15
4.10
4.06
3.75
3.71
3.67
7
5.59
4.74
4.35
4.12
3.97
3.87
3.79
3.73
3.68
3.64
3.32
3.27
3.23
8
5.32
4.46
4.07
3.84
3.69
3.58
3.50
3.44
3.39
3.35
3.02
2.97
2.93
9
5.12
4.26
3.86
3.63
3.48
3.37
3.29
3.23
3.18
3.14
2.80
2.76
2.71
10
4.96
4.10
3.71
3.48
3.33
3.22
3.14
3.07
3.02
2.98
2.64
2.59
2.54
11
4.84
3.98
3.59
3.36
3.20
3.09
3.01
2.95
2.90
2.85
2.51
2.46
2.40
12
4.75
3.89
3.49
3.26
3.11
3.00
2.91
2.85
2.80
2.75
2.40
2.35
2.30
13
4.67
3.81
3.41
3.18
3.03
2.92
2.83
2.77
2.71
2.67
2.31
2.26
2.21
14
4.60
3.74
3.34
3.11
2.96
2.85
2.76
2.70
2.65
2.60
2.24
2.19
2.13
15
4.54
3.68
3.29
3.06
2.90
2.79
2.71
2.64
2.59
2.54
2.18
2.12
2.07
16
4.49
3.63
3.24
3.01
2.85
2.74
2.66
2.59
2.54
2.49
2.12
2.07
2.01
17
4.45
3.59
3.20
2.96
2.81
2.70
2.61
2.55
2.49
2.45
2.08
2.02
1.96
18
4.41
3.55
3.16
2.93
2.77
2.66
2.58
2.51
2.46
2.41
2.04
1.98
1.92
19
4.38
3.52
3.13
2.90
2.74
2.63
2.54
2.48
2.42
2.38
2.00
1.94
1.88
20
4.35
3.49
3.10
2.87
2.71
2.60
2.51
2.45
2.39
2.35
1.97
1.91
1.84
25
4.24
3.39
2.99
2.76
2.60
2.49
2.40
2.34
2.28
2.24
1.84
1.78
1.71
50
4.03
3.18
2.79
2.56
2.40
2.29
2.20
2.13
2.07
2.03
1.60
1.52
1.45
100
3.94
3.09
2.70
2.46
2.31
2.19
2.10
2.03
1.97
1.93
1.48
1.39
1.28
∞
3.84
3.00
2.60
2.37
2.21
2.10
2.01
1.94
1.88
1.83
1.35
1.25
1.00
c
⃝2000 by Chapman & Hall/CRC

Critical values for the F distribution
For given values of ν1 and ν2, the following table contains values of
F0.025,ν1,ν2; deﬁned by Prob [F ≥F0.025,ν1,ν2] = α = 0.025.
ν2
ν1 =1
2
3
4
5
6
7
8
9
10
50
100
∞
1
647.8
799.5
864.2
899.6
921.8
937.1
948.2
956.7
963.3
968.6
1008
1013
1018
2
38.51
39.00
39.17
39.25
39.30
39.33
39.36
39.37
39.39
39.40
39.48
39.49
39.50
3
17.44
16.04
15.44
15.10
14.88
14.73
14.62
14.54
14.47
14.42
14.01
13.96
13.90
4
12.22
10.65
9.98
9.60
9.36
9.20
9.07
8.98
8.90
8.84
8.38
8.32
8.26
5
10.01
8.43
7.76
7.39
7.15
6.98
6.85
6.76
6.68
6.62
6.14
6.08
6.02
6
8.81
7.26
6.60
6.23
5.99
5.82
5.70
5.60
5.52
5.46
4.98
4.92
4.85
7
8.07
6.54
5.89
5.52
5.29
5.12
4.99
4.90
4.82
4.76
4.28
4.21
4.14
8
7.57
6.06
5.42
5.05
4.82
4.65
4.53
4.43
4.36
4.30
3.81
3.74
3.67
9
7.21
5.71
5.08
4.72
4.48
4.32
4.20
4.10
4.03
3.96
3.47
3.40
3.33
10
6.94
5.46
4.83
4.47
4.24
4.07
3.95
3.85
3.78
3.72
3.22
3.15
3.08
11
6.72
5.26
4.63
4.28
4.04
3.88
3.76
3.66
3.59
3.53
3.03
2.96
2.88
12
6.55
5.10
4.47
4.12
3.89
3.73
3.61
3.51
3.44
3.37
2.87
2.80
2.72
13
6.41
4.97
4.35
4.00
3.77
3.60
3.48
3.39
3.31
3.25
2.74
2.67
2.60
14
6.30
4.86
4.24
3.89
3.66
3.50
3.38
3.29
3.21
3.15
2.64
2.56
2.49
15
6.20
4.77
4.15
3.80
3.58
3.41
3.29
3.20
3.12
3.06
2.55
2.47
2.40
16
6.12
4.69
4.08
3.73
3.50
3.34
3.22
3.12
3.05
2.99
2.47
2.40
2.32
17
6.04
4.62
4.01
3.66
3.44
3.28
3.16
3.06
2.98
2.92
2.41
2.33
2.25
18
5.98
4.56
3.95
3.61
3.38
3.22
3.10
3.01
2.93
2.87
2.35
2.27
2.19
19
5.92
4.51
3.90
3.56
3.33
3.17
3.05
2.96
2.88
2.82
2.30
2.22
2.13
20
5.87
4.46
3.86
3.51
3.29
3.13
3.01
2.91
2.84
2.77
2.25
2.17
2.09
25
5.69
4.29
3.69
3.35
3.13
2.97
2.85
2.75
2.68
2.61
2.08
2.00
1.91
50
5.34
3.97
3.39
3.05
2.83
2.67
2.55
2.46
2.38
2.32
1.75
1.66
1.54
100
5.18
3.83
3.25
2.92
2.70
2.54
2.42
2.32
2.24
2.18
1.59
1.48
1.37
∞
5.02
3.69
3.12
2.79
2.57
2.41
2.29
2.19
2.11
2.05
1.43
1.27
1.00
c
⃝2000 by Chapman & Hall/CRC

Critical values for the F distribution
For given values of ν1 and ν2, the following table contains values of
F0.01,ν1,ν2; deﬁned by Prob [F ≥F0.01,ν1,ν2] = α = 0.01.
ν2
ν1 =1
2
3
4
5
6
7
8
9
10
50
100
∞
1
4052
5000
5403
5625
5764
5859
5928
5981
6022
6056
6303
6334
6336
2
98.50
99.00
99.17
99.25
99.30
99.33
99.36
99.37
99.39
99.40
99.48
99.49
99.50
3
34.12
30.82
29.46
28.71
28.24
27.91
27.67
27.49
27.35
27.23
26.35
26.24
26.13
4
21.20
18.00
16.69
15.98
15.52
15.21
14.98
14.80
14.66
14.55
13.69
13.58
13.46
5
16.26
13.27
12.06
11.39
10.97
10.67
10.46
10.29
10.16
10.05
9.24
9.13
9.02
6
13.75
10.92
9.78
9.15
8.75
8.47
8.26
8.10
7.98
7.87
7.09
6.99
6.88
7
12.25
9.55
8.45
7.85
7.46
7.19
6.99
6.84
6.72
6.62
5.86
5.75
5.65
8
11.26
8.65
7.59
7.01
6.63
6.37
6.18
6.03
5.91
5.81
5.07
4.96
4.86
9
10.56
8.02
6.99
6.42
6.06
5.80
5.61
5.47
5.35
5.26
4.52
4.41
4.31
10
10.04
7.56
6.55
5.99
5.64
5.39
5.20
5.06
4.94
4.85
4.12
4.01
3.91
11
9.65
7.21
6.22
5.67
5.32
5.07
4.89
4.74
4.63
4.54
3.81
3.71
3.60
12
9.33
6.93
5.95
5.41
5.06
4.82
4.64
4.50
4.39
4.30
3.57
3.47
3.36
13
9.07
6.70
5.74
5.21
4.86
4.62
4.44
4.30
4.19
4.10
3.38
3.27
3.17
14
8.86
6.51
5.56
5.04
4.69
4.46
4.28
4.14
4.03
3.94
3.22
3.11
3.00
15
8.68
6.36
5.42
4.89
4.56
4.32
4.14
4.00
3.89
3.80
3.08
2.98
2.87
16
8.53
6.23
5.29
4.77
4.44
4.20
4.03
3.89
3.78
3.69
2.97
2.86
2.75
17
8.40
6.11
5.18
4.67
4.34
4.10
3.93
3.79
3.68
3.59
2.87
2.76
2.65
18
8.29
6.01
5.09
4.58
4.25
4.01
3.84
3.71
3.60
3.51
2.78
2.68
2.57
19
8.18
5.93
5.01
4.50
4.17
3.94
3.77
3.63
3.52
3.43
2.71
2.60
2.49
20
8.10
5.85
4.94
4.43
4.10
3.87
3.70
3.56
3.46
3.37
2.64
2.54
2.42
25
7.77
5.57
4.68
4.18
3.85
3.63
3.46
3.32
3.22
3.13
2.40
2.29
2.17
50
7.17
5.06
4.20
3.72
3.41
3.19
3.02
2.89
2.78
2.70
1.95
1.82
1.70
100
6.90
4.82
3.98
3.51
3.21
2.99
2.82
2.69
2.59
2.50
1.74
1.60
1.45
∞
6.63
4.61
3.78
3.32
3.02
2.80
2.64
2.51
2.41
2.32
1.53
1.32
1.00
c
⃝2000 by Chapman & Hall/CRC

Critical values for the F distribution
For given values of ν1 and ν2, the following table contains values of
F0.005,ν1,ν2; deﬁned by Prob [F ≥F0.005,ν1,ν2] = α = 0.005.
ν2
ν1 =1
2
3
4
5
6
7
8
9
10
50
100
∞
1
16211
20000
21615
22500
23056
23437
23715
23925
24091
24224
25211
25337
25465
2
198.5
199.0
199.2
199.2
199.3
199.3
199.4
199.4
199.4
199.4
199.5
199.5
199.5
3
55.55
49.80
47.47
46.19
45.39
44.84
44.43
44.13
43.88
43.69
42.21
42.02
41.83
4
31.33
26.28
24.26
23.15
22.46
21.97
21.62
21.35
21.14
20.97
19.67
19.50
19.32
5
22.78
18.31
16.53
15.56
14.94
14.51
14.20
13.96
13.77
13.62
12.45
12.30
12.14
6
18.63
14.54
12.92
12.03
11.46
11.07
10.79
10.57
10.39
10.25
9.17
9.03
8.88
7
16.24
12.40
10.88
10.05
9.52
9.16
8.89
8.68
8.51
8.38
7.35
7.22
7.08
8
14.69
11.04
9.60
8.81
8.30
7.95
7.69
7.50
7.34
7.21
6.22
6.09
5.95
9
13.61
10.11
8.72
7.96
7.47
7.13
6.88
6.69
6.54
6.42
5.45
5.32
5.19
10
12.83
9.43
8.08
7.34
6.87
6.54
6.30
6.12
5.97
5.85
4.90
4.77
4.64
11
12.23
8.91
7.60
6.88
6.42
6.10
5.86
5.68
5.54
5.42
4.49
4.36
4.23
12
11.75
8.51
7.23
6.52
6.07
5.76
5.52
5.35
5.20
5.09
4.17
4.04
3.90
13
11.37
8.19
6.93
6.23
5.79
5.48
5.25
5.08
4.94
4.82
3.91
3.78
3.65
14
11.06
7.92
6.68
6.00
5.56
5.26
5.03
4.86
4.72
4.60
3.70
3.57
3.44
15
10.80
7.70
6.48
5.80
5.37
5.07
4.85
4.67
4.54
4.42
3.52
3.39
3.26
16
10.58
7.51
6.30
5.64
5.21
4.91
4.69
4.52
4.38
4.27
3.37
3.25
3.11
17
10.38
7.35
6.16
5.50
5.07
4.78
4.56
4.39
4.25
4.14
3.25
3.12
2.98
18
10.22
7.21
6.03
5.37
4.96
4.66
4.44
4.28
4.14
4.03
3.14
3.01
2.87
19
10.07
7.09
5.92
5.27
4.85
4.56
4.34
4.18
4.04
3.93
3.04
2.91
2.78
20
9.94
6.99
5.82
5.17
4.76
4.47
4.26
4.09
3.96
3.85
2.96
2.83
2.69
25
9.48
6.60
5.46
4.84
4.43
4.15
3.94
3.78
3.64
3.54
2.65
2.52
2.38
50
8.63
5.90
4.83
4.23
3.85
3.58
3.38
3.22
3.09
2.99
2.10
1.95
1.81
100
8.24
5.59
4.54
3.96
3.59
3.33
3.13
2.97
2.85
2.74
1.84
1.68
1.51
∞
7.88
5.30
4.28
3.72
3.35
3.09
2.90
2.74
2.62
2.52
1.60
1.36
1.00
c
⃝2000 by Chapman & Hall/CRC

Critical values for the F distribution
For given values of ν1 and ν2, the following table contains values of
F0.001,ν1,ν2; deﬁned by Prob [F ≥F0.001,ν1,ν2] = α = 0.001.
ν2
ν1 =1
2
3
4
5
6
7
8
9
10
50
100
∞
2
998.5
999.0
999.2
999.2
999.3
999.3
999.4
999.4
999.4
999.4
999.5
999.5
999.5
3
167.0
148.5
141.1
137.1
134.6
132.8
131.6
130.6
129.9
129.2
124.7
124.1
123.5
4
74.14
61.25
56.18
53.44
51.71
50.53
49.66
49.00
48.47
48.05
44.88
44.47
44.05
5
47.18
37.12
33.20
31.09
29.75
28.83
28.16
27.65
27.24
26.92
24.44
24.12
23.79
6
35.51
27.00
23.70
21.92
20.80
20.03
19.46
19.03
18.69
18.41
16.31
16.03
15.75
7
29.25
21.69
18.77
17.20
16.21
15.52
15.02
14.63
14.33
14.08
12.20
11.95
11.70
8
25.41
18.49
15.83
14.39
13.48
12.86
12.40
12.05
11.77
11.54
9.80
9.57
9.33
9
22.86
16.39
13.90
12.56
11.71
11.13
10.70
10.37
10.11
9.89
8.26
8.04
7.81
10
21.04
14.91
12.55
11.28
10.48
9.93
9.52
9.20
8.96
8.75
7.19
6.98
6.76
11
19.69
13.81
11.56
10.35
9.58
9.05
8.66
8.35
8.12
7.92
6.42
6.21
6.00
12
18.64
12.97
10.80
9.63
8.89
8.38
8.00
7.71
7.48
7.29
5.83
5.63
5.42
13
17.82
12.31
10.21
9.07
8.35
7.86
7.49
7.21
6.98
6.80
5.37
5.17
4.97
14
17.14
11.78
9.73
8.62
7.92
7.44
7.08
6.80
6.58
6.40
5.00
4.81
4.60
15
16.59
11.34
9.34
8.25
7.57
7.09
6.74
6.47
6.26
6.08
4.70
4.51
4.31
16
16.12
10.97
9.01
7.94
7.27
6.80
6.46
6.19
5.98
5.81
4.45
4.26
4.06
17
15.72
10.66
8.73
7.68
7.02
6.56
6.22
5.96
5.75
5.58
4.24
4.05
3.85
18
15.38
10.39
8.49
7.46
6.81
6.35
6.02
5.76
5.56
5.39
4.06
3.87
3.67
19
15.08
10.16
8.28
7.27
6.62
6.18
5.85
5.59
5.39
5.22
3.90
3.71
3.51
20
14.82
9.95
8.10
7.10
6.46
6.02
5.69
5.44
5.24
5.08
3.77
3.58
3.38
25
13.88
9.22
7.45
6.49
5.89
5.46
5.15
4.91
4.71
4.56
3.28
3.09
2.89
50
12.22
7.96
6.34
5.46
4.90
4.51
4.22
4.00
3.82
3.67
2.44
2.25
2.06
100
11.50
7.41
5.86
5.02
4.48
4.11
3.83
3.61
3.44
3.30
2.08
1.87
1.65
∞
10.83
6.91
5.42
4.62
4.10
3.74
3.47
3.27
3.10
2.96
1.75
1.45
1.00
c
⃝2000 by Chapman & Hall/CRC

6.9
GAMMA DISTRIBUTION
6.9.1
Properties
pdf
f(x) = xα−1 e−x/β
βαΓ(α)
mean
µ = αβ
variance
σ2 = αβ2
skewness
β1 = 2/√α
kurtosis
β2 = 3

1 + 2
α

mgf
m(t) = (1 −βt)−α
char function
φ(t) = (1 −iβt)−α
where Γ(x) is the gamma function (see page 515).
6.9.2
Probability density function
The probability density function is skewed to the right. For ﬁxed β the tail
becomes heavier as α increases.
Figure 6.10: Probability density functions for a gamma random variable.
6.9.3
Related distributions
Let X be a gamma random variable with parameters α and β.
(1) The random variable X has a standard gamma distribution if α = 1.
(2) If α = 1 and β = 1/λ, then X has an exponential distribution with
parameter λ.
c
⃝2000 by Chapman & Hall/CRC

(3) If α = ν/2 and β = 2, then X has a chi–square distribution with ν
degrees of freedom.
(4) If α = n is an integer, then X has an Erlang distribution with parameters
β and n.
(5) If α = ν/2 and β = 1, then the random variable Y = 2X has a chi–
square distribution with ν degrees of freedom.
(6) As α →∞, X tends to a normal distribution with parameters µ = αβ
and σ2 = αβ2.
(7) Suppose X1 is a gamma random variable with parameters α = 1 and
β = β1, X2 is a gamma random variable with parameters α = 1 and
β = β2, and X1 and X2 are independent. The random variable Y =
X1/(X1 + X2) has a beta distribution with parameters β1 and β2.
(8) Let X1, X2, . . . , Xn be independent gamma random variables with pa-
rameters α and βi for i = 1, 2, . . . , n.
The random variable Y
=
X1 + X2 + · · · + Xn has a gamma distribution with parameters α and
β = β1 + β2 + · · · + βn.
6.10
HALF–NORMAL DISTRIBUTION
6.10.1
Properties
pdf
f(x) = 2θ
π
exp

−θ2x2
π2

,
x ≥0, θ > 0
mean
µ = 1/θ
variance
σ2 = π −2
2θ2
skewness
β1 =
√
2(4 −π)
(π −2)3/2
kurtosis
β2 = 3π2 −4π −12
(π −2)2
mgf
m(t) = exp
πt2
4θ2
 
1 + erf
√πt
2θ

char function
φ(t) = exp

−πt2
4θ2
 
1 + erf
√πit
2θ

where erf(x) is the error function (see page 512).
6.10.2
Probability density function
The probability density function is skewed to the right. As θ increases the
tail becomes lighter.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.11: Probability density functions for a half–normal random variable.
6.11
INVERSE GAUSSIAN (WALD) DISTRIBUTION
6.11.1
Properties
pdf
f(x) =
-
λ
2πx3 exp
−λ(x −µ)2
2µ2x

x, µ, λ > 0
mean
µ = µ
variance
σ2 = µ3/λ
skewness
β1 = 3

µ/λ
kurtosis
β2 = 3 + 15µ
λ
mgf
m(t) = exp

λ
µ
%
1 −
-
1 −2µ2t
λ
&
char function
φ(t) = exp

λ
µ
%
1 −
-
1 −2µ2it
λ
&
6.11.2
Probability density function
The probability density function is skewed right. For ﬁxed µ the probability
density function becomes more bell–shaped as λ increases.
6.11.3
Related distributions
If X is an inverse Gaussian random variable with parameters µ and λ, the
random variable Y = λ(X −µ)2
µ2X
has a chi–square distribution with 1 degree
c
⃝2000 by Chapman & Hall/CRC

Figure 6.12: Probability density functions for an inverse Gaussian random
variable.
of freedom.
6.12
LAPLACE DISTRIBUTION
6.12.1
Properties
pdf
f(x) = 1
2β exp

−|x −α|
β

,
x ∈R, α ∈R, β > 0
mean
µ = α
variance
σ2 = 2β2
skewness
β1 = 0
kurtosis
β2 = 6
mgf
m(t) =
eαt
1 −β2t2
char function
φ(t) =
eαit
1 + β2t2
6.12.2
Probability density function
The probability density function is symmetric about the parameter α. For
ﬁxed α the tails become heavier as β increases.
6.12.3
Related distributions
(1) Let X be a Laplace random variable with parameters α and β. The
random variable Y = |X −α| has an exponential distribution with pa-
c
⃝2000 by Chapman & Hall/CRC

Figure 6.13: Probability density functions for a Laplace random variable.
rameter λ = β. The random variable W = |X −α|/β has an exponential
distribution with parameter λ = 1.
(2) Let X1 and X2 be independent Laplace random variables with pa-
rameters α = 0, and β1 and β2, respectively.
The random variable
Y = |X1/X2| has an F distribution with parameters ν1 = ν2 = 2.
6.13
LOGISTIC DISTRIBUTION
6.13.1
Properties
pdf
f(x) =
e−(x−α)/β
β(1 + e−(x−α)/β)2 ,
x ∈R, α ∈R, β ∈R
mean
µ = α
variance
σ2 = β2π2/3
skewness
β1 = 0
kurtosis
β2 = 21/5
mgf
m(t) = πβteαt/ sin(πβt)
char function
φ(t) = πβteiαt/ sinh(πβt)
6.13.2
Probability density function
The probability density function is symmetric about the parameter α. For
ﬁxed α the tails become heavier as β increases.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.14: Probability density functions for a logistic random variable.
6.13.3
Related distributions
(1) The random variable X has a standard logistic distribution if α = 0 and
β = 1.
(2) If X is a logistic random variable with parameters α and β, then the
random variable Y = (X −α)/β has a (standard) logistic distribution
with parameters 0 and 1.
6.14
LOGNORMAL DISTRIBUTION
6.14.1
Properties
pdf
f(x) =
1
√
2π σx exp

−1
2σ2 (ln x −µ)2

x > 0, µ ∈R, σ > 0
mean
µ = eµ+σ2/2
variance
σ2 = e2µ+σ2(eσ2 −1)
skewness
β1 = (eσ2 + 2)

eσ2 −1
kurtosis
β2 = e4σ2 + 2e3σ2 + 3e2σ2
mgf
m(t) = does not exist
char function
φ(t) = does not exist
c
⃝2000 by Chapman & Hall/CRC

6.14.2
Probability density function
The probability density function is skewed to the right. The scale parameter
is µ and the shape parameter is σ.
Figure 6.15: Probability density functions for a lognormal random variable.
6.14.3
Related distributions
(1) If X is a lognormal random variable with parameters µ and σ, then the
random variable Y = ln X has a normal distribution with mean µ and
variance σ2.
(2) If X is a lognormal random variable with parameters µ and σ and a and
b are constants, then the random variable Y = eaXb has a lognormal
distribution with parameters a + bµ and bσ.
(3) Let X1 and X2 be independent lognormal random variables with param-
eters µ1, σ1 and µ2, σ2, respectively. The random variable Y = X1/X2
has a lognormal distribution with parameters µ1 −µ2 and σ1 + σ2.
(4) Let X1, X2, . . . , Xn be independent lognormal random variables with
parameters µi and σi for i = 1, 2, . . . , n.
The random variable Y =
X1 · X2 · · · Xn has a lognormal distribution with parameters µ = µ1 +
µ2 + · · · + µn and σ = σ1 + σ2 + · · · + σn.
(5) Let X1, X2, . . . , Xn be independent lognormal random variables with
parameters µ and σ.
The random variable Y =
n√X1 · · · · Xn has a
lognormal distribution with parameters µ and σ/n.
c
⃝2000 by Chapman & Hall/CRC

6.15
NONCENTRAL CHI–SQUARE DISTRIBUTION
6.15.1
Properties
pdf
f(x) = e[ 1
2 (x+λ)]
2ν/2
∞

j=1
x(ν/2)+j−1λj
Γ
 ν
2 + j

22jj!
x, λ > 0, ν ∈N
mean
µ = ν + λ
variance
σ2 = 2ν + 4λ
skewness
β1 = 2
√
2(ν + 3λ)
(ν + 2λ)3/2
kurtosis
β2 = 3 + 12(ν + 4λ)
(ν + 2λ)2
mgf
m(t) = (1 −2t)−ν/2 exp

λt
1 −2t

char function
φ(t) = (1 −2it)−ν/2 exp

λit
1 −2it

where Γ(x) is the gamma function (see page 515).
6.15.2
Probability density function
The probability density function is skewed to the right. For ﬁxed ν the tail
becomes heavier as the noncentrality parameter λ increases.
Figure 6.16: Probability density functions for a noncentral chi–square random
variable.
c
⃝2000 by Chapman & Hall/CRC

6.15.3
Related distributions
(1) If X is a noncentral chi–square random variable with parameters ν and
λ = 0, then X is a chi–square random variable with ν degrees of freedom.
(2) If X1 is a noncentral chi–square random variable with parameters ν1 and
λ, X2 is a chi–square random variable with parameter ν2, and X1 and
X2 are independent, then the random variable Y = (X1/ν1)/(X2/ν2)
has a noncentral F distribution with parameters ν1, ν2, and λ.
(3) Let X1, X2, . . . , Xn be independent noncentral chi–square random vari-
ables with parameters νi and λi (for i = 1, 2, . . . , n). The random vari-
able Y = X1 + X2 + · · · + Xn has a noncentral chi–square distribution
with parameters ν = ν1 + ν2 + · · · + νn and λ = λ1 + λ2 + · · · + λn.
6.16
NONCENTRAL F DISTRIBUTION
6.16.1
Properties
pdf
f(x) = e−λ/2νν1/2
1
νν2/2
2
x
1
2 (ν1−2)(ν1x + ν2)−1
2 (ν1+ν2)
B(ν1/2, ν2/2)
×
1F1
ν1 + ν2
2
, ν1
2 ,
ν1λx
2(ν1x + ν2)

x > 0, ν1, ν2 ∈N, λ > 0
mean
µ = ν2(ν1 + λ)
ν1(ν2 −2) ,
ν2 > 2
variance
σ2 = 2ν2
2 ((ν1 + λ) + (ν2 −2)(ν1 + 2λ))
ν2
1(ν2 −4)(ν2 −2)2
,
ν2 > 4
skewness
β1 = does not exist
kurtosis
β2 = does not exist
mgf
m(t) = does not exist
char function
φ(t) = does not exist
where B(a, b) is the beta function and pFq is the generalized hypergeometric
function deﬁned in Chapter 18 (see pages 511 and 520).
6.16.2
Probability density function
The probability density function is skewed to the right. The parameters ν1
and ν2 are the shape parameters and λ is the noncentrality parameter.
6.16.3
Related distributions
If X has a noncentral F distribution with parameters ν1, ν2, and λ, then the
random variable X tends to an F distribution as λ →0.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.17: Probability density functions for a noncentral F random variable.
6.17
NONCENTRAL t DISTRIBUTION
6.17.1
Properties
pdf
f(x) =
νν/2e−λ2/2
√π Γ(ν/2)(ν + x2)(ν+1)/2 ×
∞

j=0
Γ
ν + j + 1
2
 λj
j!
  2x2
ν + x2
j/2
x, λ ∈R, ν ∈N
where Γ(x) is the gamma function (see page 515).
The moments about the origin are
µ′
r = cr
νr/2Γ[(ν −r)/2]
2r/2Γ(ν/2)
,
ν > r
(6.10)
where
c2r−1 =
r

j=1
(2r −1)!λ2r−1
(2j −1)(r −j)!2r−j ,
r = 1, 2, 3, . . .
c2r =
r

j=0
(2r)!λ2j
(2j)!(r −j)!2r−j ,
r = 1, 2, 3, . . .
(6.11)
6.17.2
Probability density function
The probability density function is skewed to the right. The shape parame-
ter is ν and the noncentrality parameter is λ. For ﬁxed ν the tail becomes
heavier as λ increases. For large values of ν, the probability density function
is approximately symmetric.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.18: Probability density functions for a noncentral t random variable.
6.17.3
Related distributions
If X has a noncentral t distribution with parameters ν and λ = 0, then X has
a t distribution with ν degrees of freedom.
6.18
NORMAL DISTRIBUTION
6.18.1
Properties
pdf
f(x) =
1
σ
√
2π e−(x−µ)2/2σ2,
x ∈R, µ ∈R, σ > 0
mean
µ = µ
variance
σ2 = σ2
skewness
β1 = 0
kurtosis
β2 = 3
mgf
m(t) = exp

µt + σ2t2
2

char function
φ(t) = exp

µit −σ2t2
2

See Chapter 7 for more details.
6.18.2
Probability density function
The probability density function is symmetric and bell–shaped about the lo-
cation parameter µ. For small values of the scale parameter σ the probability
density function is more compact.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.19: Probability density functions for a normal random variable.
6.18.3
Related distributions
(1) The random variable X has a standard normal distribution if µ = 0 and
σ = 1.
(2) If X is a normal random variable with parameters µ and σ, the random
variable Y = (X −µ)/σ has a (standard) normal distribution with
parameters 0 and 1.
(3) If X is a normal random variable with parameters µ and σ, the random
variable Y = eX has a lognormal distribution with parameters µ and σ.
(4) If X is a normal random variable with parameters µ = 0 and σ = 1,
then the random variable Y = eµ+σX has a lognormal distribution with
parameters µ and σ.
(5) If X is a normal random variable with parameters µ and σ, and a and
b are constants, then the random variable Y = a + bX has a normal
distribution with parameters a + bµ and bσ.
(6) If X1 and X2 are independent standard normal random variables, the
random variable Y = X1/X2 has a Cauchy distribution with parameters
a = 0 and b = 1.
(7) If X1 and X2 are independent normal random variables with parameters
µ = 0 and σ, then the random variable Y =

X2
1 + X2
2 has a Rayleigh
distribution with parameter σ.
(8) Let Xi (for i = 1, 2, . . . , n) be independent, normal random variables
with parameters µi and σi, and let ci be any constants. The random
variable Y =
n
i=1
ciXi has a normal distribution with parameters µ =
n
i=1
ciµi and σ2 =
n
i=1
c2
i σ2
i .
c
⃝2000 by Chapman & Hall/CRC

(9) Let Xi (for i = 1, 2, . . . , n) be independent, normal random variables
with parameters µ and σ, then the random variable Y = X1 + X2 +
· · · + Xn has a normal distribution with mean nµ and variance nσ2.
(10) Let Xi (for i = 1, 2, . . . , n) be independent standard normal random
variables. The random variable Y =
n
i=1
X2
i has a chi–square distribu-
tion with ν = n degrees of freedom. If µi = λi > 0 (σi = 1), then
the random variable Y has a noncentral chi–square distribution with
parameters ν = n and noncentrality parameter λ =
n
i=1
λ2
i .
6.19
NORMAL DISTRIBUTION: MULTIVARIATE
6.19.1
Properties
pdf
f(x) =
1
(2π)n/2
det(Σ)
exp

−(x −µ)TΣ−1(x −µ)
2

mean
µ
covariance matrix
Σ
char function
φ(t) = exp

itTµ −1
2tTΣt

where x =

x1, x2, . . . , xn
T (with xi ∈R) and Σ is a positive semi-deﬁnite
matrix.
Section 7.6 discusses the bivariate normal.
6.19.2
Probability density function
The probability density function is smooth and unimodal. Figure 6.20 shows
two views of a bivariate normal with µ =

1 0
T and Σ =

1 2
0 4

.
Figure 6.20: Two views of the probability density for a bivariate normal.
c
⃝2000 by Chapman & Hall/CRC

6.20
PARETO DISTRIBUTION
6.20.1
Properties
pdf
f(x) = θaθ
xθ+1 ,
x ≥a, θ > 0, a > 0
mean
µ =
aθ
θ −1,
θ > 1
variance
σ2 =
a2θ
(θ −1)2(θ −2),
θ > 2
skewness
β1 = 2(θ + 1)
√
θ −2
(θ −3)
√
θ
,
θ > 3
kurtosis
β2 = 3(θ −2)(3θ2 + θ + 2)
θ(θ −3)(θ −4)
,
θ > 4
mgf
m(t) = does not exist
char function
φ(t) = −aθtθ cos(πθ/2)Γ(1 −θ) +
1F2

−θ
2

,
 1
2, 1 −θ
2

, −1
4a2t2
−
1
1−θ

atiθ1F2
 1
2 −θ
2

,
 3
2, 3
2 −θ
2

, −1
4a2t2
sgn(t)

+
iaθtθΓ(1 −θ) sgn(t) sin(πθ/2)
where pFq is the generalized hypergeometric function and sgn(t) is the signum
function deﬁned in Chapter 18 (see pages 520 and 523).
6.20.2
Probability density function
The probability density function is skewed to the right. The shape parameter
is θ and the location parameter is a.
6.20.3
Related distributions
(1) Let X be a Pareto random variable with parameters a and θ.
(a) The random variable Y = ln(X/a) has an exponential distribution
with parameter λ = 1/θ.
(b) The random variable Y = 1/X has a power function distribution
with parameters 1/a and θ.
(c) The random variable Y = −ln

(X/a)θ −1

has a logistic distri-
bution with parameters α = 0 and β = 1.
(2) Let Xi (for i = 1, 2, . . . , n) be independent Pareto random variables
with parameters a and θ. The random variable Y = 2a
n
i=1
ln(Xi/θ) has
a chi–square distribution with ν = 2n.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.21: Probability density functions for a Pareto random variable.
6.21
POWER FUNCTION DISTRIBUTION
6.21.1
Properties
pdf
f(x) = cxc−1
bc
,
0 ≤x ≤b, b > 0, c > 0
mean
µ =
bc
c + 1
variance
σ2 =
b2c
(c + 1)2(c + 2)
skewness
β1 = 2(1 −c)√c + 2
(c + 3)√c
kurtosis
β2 = 3(c + 2)(3c2 −c + 2)
c(c + 3)(c + 4)
mgf
m(t) = does not exist
char function
φ(t) = 1F2
 c
2

,
 1
2, 1 + c
2

, −1
4b2t2
+
1
c+1

ibct1F2
 1
2 + c
2

,
 3
2, 3
2 + c
2

, −1
4b2t2
sgn(t)

where pFq is the generalized hypergeometric function and sgn(t) is the signum
function deﬁned in Chapter 18 (see pages 520 and 523).
6.21.2
Probability density function
The probability density function is “J” shaped for c < 1 and is skewed left for
c > 1.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.22: Probability density functions for a power function random vari-
able.
6.21.3
Related distributions
(1) Let X be a power function random variable with parameters b and c.
(a) The random variable Y = 1/X has a power function distribution
with parameters 1/b and c.
(b) If b = 1:
(1) The random variable X has a beta distribution with parame-
ters α = c and β = 1.
(2) The random variable Y = −ln X has an exponential distribu-
tion with parameter λ = c.
(3) The random variable Y = 1/X has a Pareto distribution with
parameters a = 0 and θ = c.
(4) The random variable Y = −ln(X−c −1) has a logistic distri-
bution with parameters α = 0 and β = 1.
(5) The random variable Y = (−ln Xc)1/k has a Weibull distribu-
tion with parameters α = k and β = 1.
(6) The random variable Y = −ln(−c ln X) has an extreme–value
distribution with parameters α = 0 and β = 1.
(c) If c = 1 then X has a uniform distribution with parameters a = 0
and b.
(7) Let X1, X2 be independent power function random variables with pa-
rameters b = 1 and c. The random variable Y = −c ln(X1/X2) has a
Laplace distribution with parameters α = 0 and β = 1.
c
⃝2000 by Chapman & Hall/CRC

6.22
RAYLEIGH DISTRIBUTION
6.22.1
Properties
pdf
f(x) = x
σ2 exp

−x2
2σ2

,
x ≥0, σ > 0
mean
µ = σ

π/2
variance
σ2 = σ2 
2 −π
2

skewness
β1 = (π −3)

π/2

2 −π
2
3/2
kurtosis
β2 = 32 −3π2
(4 −π)2
mgf
m(t) = 1
2

2 +
√
2π σ t eσ2t2/2

1 + erf
 σt
√
2

char function
φ(t) = 1 + ie−σ2t2/2
-π
2 σ t

1 −erf

−iσt
√
2

where erf(x) is the error function (see page 512).
6.22.2
Probability density function
The probability density function is skewed to the right. For large values of σ
the tail is heavier.
Figure 6.23: Probability density functions for a Rayleigh random variable.
c
⃝2000 by Chapman & Hall/CRC

6.22.3
Related distributions
(1) If X is a Rayleigh random variable with parameter σ = 1, then X is a
chi random variable with parameter n = 2.
(2) If X is a Rayleigh random variable with parameter σ, then the random
variable Y = X2 has an exponential distribution with parameter λ =
1/(2σ2).
6.23
t DISTRIBUTION
6.23.1
Properties
pdf
f(x) =
1
√πν
Γ
 ν+1
2

Γ
 ν
2


1 + x2
ν
−(ν+1)/2
x ∈R, ν ∈N
mean
µ = 0,
ν ≥2
variance
σ2 =
ν
ν −2,
ν ≥3
skewness
β1 = 0,
ν ≥4
kurtosis
β2 = 3 +
6
ν −4,
ν ≥5
mgf
m(t) = does not exist
char function
φ(t) = 21−ν
2 νν/4|t|ν/2Kν/2(√ν|t|)
Γ(ν/2)
where Kn(x) is a modiﬁed Bessel function and Γ(x) is the gamma function
deﬁned in Chapter 18 (see pages 506 and 18.8).
6.23.2
Probability density function
The probability density function is symmetric and bell–shaped centered about 0.
As the degrees of freedom, ν, increases the distribution becomes more com-
pact.
6.23.3
Related distributions
(1) If X is a t random variable with parameter ν, then the random variable
Y = X2 has an F distribution with 1 and ν degrees of freedom.
(2) If X is a t random variable with parameter ν = 1, then X has a Cauchy
distribution with parameters a = 0 and b = 1.
(3) If X is a t random variable with parameter ν, as ν tends to inﬁnity X
tends to a standard normal distribution. The approximation is reason-
able for ν ≥30.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.24: Probability density functions for a t random variable.
6.23.4
Critical values for the t distribution
For a given value of ν, the number of degrees of freedom, the table on page
157 contains values of tα,ν such that
Prob [t ≥tα,ν] = α
(6.12)
Example 6.42:
Use the table on page 157 to ﬁnd the values t.05,11 and −t.01,24.
Solution:
(S1) The top row of the following table contains cumulative probability and the left–
hand column contains the degrees of freedom. The values in the body of the
table may be used to ﬁnd critical values.
(S2) t.05,11 = 1.7959 since F(1.7959; 11) = .95
=⇒
Prob [t ≥1.7959] = .05
(S3) −t.01,24 = −2.4922 since F(2.4922; 24) = .99
=⇒
Prob [t ≤−2.4922] = .01
(S4) Illustrations:
c
⃝2000 by Chapman & Hall/CRC

Critical values for the t distribution.
ν
α = 0.1
0.05
0.025
0.01
0.005
0.0025
0.001
1
3.078
6.314
12.706
31.821
63.657
318.309
636.619
2
1.886
2.920
4.303
6.965
9.925
22.327
31.599
3
1.638
2.353
3.182
4.541
5.841
10.215
12.924
4
1.533
2.132
2.776
3.747
4.604
7.173
8.610
5
1.476
2.015
2.571
3.365
4.032
5.893
6.869
6
1.440
1.943
2.447
3.143
3.707
5.208
5.959
7
1.415
1.895
2.365
2.998
3.499
4.785
5.408
8
1.397
1.860
2.306
2.896
3.355
4.501
5.041
9
1.383
1.833
2.262
2.821
3.250
4.297
4.781
10
1.372
1.812
2.228
2.764
3.169
4.144
4.587
11
1.363
1.796
2.201
2.718
3.106
4.025
4.437
12
1.356
1.782
2.179
2.681
3.055
3.930
4.318
13
1.350
1.771
2.160
2.650
3.012
3.852
4.221
14
1.345
1.761
2.145
2.624
2.977
3.787
4.140
15
1.341
1.753
2.131
2.602
2.947
3.733
4.073
16
1.337
1.746
2.120
2.583
2.921
3.686
4.015
17
1.333
1.740
2.110
2.567
2.898
3.646
3.965
18
1.330
1.734
2.101
2.552
2.878
3.610
3.922
19
1.328
1.729
2.093
2.539
2.861
3.579
3.883
20
1.325
1.725
2.086
2.528
2.845
3.552
3.850
21
1.323
1.721
2.080
2.518
2.831
3.527
3.819
22
1.321
1.717
2.074
2.508
2.819
3.505
3.792
23
1.319
1.714
2.069
2.500
2.807
3.485
3.768
24
1.318
1.711
2.064
2.492
2.797
3.467
3.745
25
1.316
1.708
2.060
2.485
2.787
3.450
3.725
26
1.315
1.706
2.056
2.479
2.779
3.435
3.707
27
1.314
1.703
2.052
2.473
2.771
3.421
3.69o
28
1.313
1.701
2.048
2.467
2.763
3.408
3.674
29
1.311
1.699
2.045
2.462
2.756
3.396
3.659
30
1.310
1.697
2.042
2.457
2.750
3.385
3.646
35
1.306
1.690
2.030
2.438
2.724
3.340
3.591
40
1.303
1.684
2.021
2.423
2.704
3.307
3.551
45
1.301
1.679
2.014
2.412
2.690
3.281
3.520
50
1.299
1.676
2.009
2.403
2.678
3.261
3.496
100
0.290
1.660
1.984
2.364
2.626
3.174
3.390
∞
1.282
1.645
1.960
2.326
2.576
3.091
3.291
c
⃝2000 by Chapman & Hall/CRC

6.24
TRIANGULAR DISTRIBUTION
6.24.1
Properties
pdf
f(x) =









0
x ≤a
4(x −a)/(b −a)2 a < x ≤(a + b)/2
4(b −x)/(b −a)2 (a + b)/2 < x < b
0
x ≥b
a < b ∈R
mean
µ = a + b
2
variance
σ2 = (b −a)2
24
skewness
β1 = 0
kurtosis
β2 = 12/5
mgf
m(t) = 4(eat/2 −ebt/2)2
(b −a)2t2
char function
φ(t) = −4(eait/2 −ebit/2)2
(b −a)2t2
6.24.2
Probability density function
The probability density function is symmetric about the mean and consists
of two line segments.
Figure 6.25: Probability density function for a triangular random variable.
c
⃝2000 by Chapman & Hall/CRC

6.25
UNIFORM DISTRIBUTION
6.25.1
Properties
pdf
f(x) =
1
b −a,
a ≤x ≤b, a < b ∈R
mean
µ = a + b
2
variance
σ2 = (b −a)2
12
skewness
β1 = 0
kurtosis
β2 = 9/5
mgf
m(t) = ebt −eat
(b −a)t
char function
φ(t) = ebit −eait
(b −a)it
6.25.2
Probability density function
The probability density function is a horizontal line segment between a and b
at 1/(b −a).
Figure 6.26: Probability density functions for a uniform random variable.
6.25.3
Related distributions
(1) The random variable X has a standard uniform distribution if a = 0
and b = 1.
(2) If X is a uniform random variable with parameters a = 0 and b = 1, the
random variable Y = −(ln X)/λ has an exponential distribution with
parameter λ.
c
⃝2000 by Chapman & Hall/CRC

(3) Let X1 and X2 be independent uniform random variables with param-
eters a = 0 and b = 1. The random variable Y = (X1 + X2)/2 has a
triangular distribution with parameters 0 and 1.
(4) If X is a uniform random variable with parameters a = −π/2 and
b = π/2, then the random variable Y = tan X has a Cauchy distribution
with parameters a = 0 and b = 1.
6.26
WEIBULL DISTRIBUTION
6.26.1
Properties
pdf
f(x) = α
βα xα−1e−(x/β)α
mean
µ = βΓ

1 + 1
α

variance
σ2 = β2

Γ

1 + 2
α

−Γ2

1 + 1
α

skewness
β1 = 2Γ3
1 + 1
α

−3Γ

1 + 1
α

Γ

1 + 2
α

+ Γ

1 + 3
α


Γ

1 + 2
α

−Γ2
1 + 1
α
3/2
kurtosis
β2 =
−3Γ4
1 + 1
α

+ 6Γ2
1 + 1
α

Γ

1 + 2
α

−4Γ

1 + 1
α

Γ

1 + 3
α

+ Γ

1 + 4
α


Γ

1 + 2
α

−Γ2
1 + 1
α
2
mgf
m(t) = does not exist
char function
φ(t) = does not exist
where Γ(x) is the gamma function (see page 515).
6.26.2
Probability density function
The probability density function is skewed to the right. For ﬁxed β the tail
becomes lighter and the distribution becomes more bell–shaped as α increases.
6.26.3
Related distributions
Suppose X is a Weibull random variable with parameters α and β.
(1) The random variable X has a standard Weibull distribution if β = 1.
(2) If α = 1 then X has an exponential distribution with parameter λ = 1/β.
(3) The random variable Y = Xα has an exponential distribution with
parameter λ = β.
(4) If α = 2 then X has a Rayleigh distribution with parameter σ = β/
√
2.
(5) The random variable Y = −α ln(X/β) has a (standard) extreme–value
distribution with parameters α = 0 and β = 1.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.27: Probability density functions for a Weibull random variable.
6.27
RELATIONSHIPS AMONG DISTRIBUTIONS
Figure 6.27 presents some of the relationships among common univariate dis-
tributions.
The ﬁrst line of each box is the name of the distribution and
the second line lists the parameters that characterize the distribution. The
random variable X is used to represent each distribution. The three types of
relationships presented in the ﬁgure are transformations (independent random
variables are assumed) and special cases (both indicated with a solid arrow),
and limiting distributions (indicated with a dashed arrow).
6.27.1
Other relationships among distributions
(1) If X1 has a standard normal distribution, X2 has a chi–square distribu-
tion with ν degrees of freedom, and X1 and X2 are independent, then
the random variable
Y =
X1

X2/ν
(6.13)
has a t distribution with ν degrees of freedom.
(2) Let X1, X2, . . . , Xn be independent normal random variables with pa-
rameters µ and σ, and deﬁne
X = 1
n
n

i=1
Xi
and
S2 = 1
n
n

i=1
(Xi −X)2.
(6.14)
(a) The random variable Y = nS2/σ2 has a chi–square distribution
with n −1 degrees of freedom.
c
⃝2000 by Chapman & Hall/CRC

Figure 6.28: Relationships among distributions (see page 161).
c
⃝2000 by Chapman & Hall/CRC

(b) The random variable
W =
X −µ
S/√n −1
(6.15)
has a t distribution with n −1 degrees of freedom.
(3) Let X1, X2, . . . , Xn be independent normal random variables with pa-
rameters µ and σ, and deﬁne
X = 1
n
n

i=1
Xi
and
S2 =
1
n −1
n

i=1
(Xi −X)2.
(6.16)
The random variable
Y = X −µ
S/√n
(6.17)
has a t distribution with n −1 degrees of freedom.
(4) Let X1, X2, . . . , Xn1 be independent normal random variables with pa-
rameters µ1 and σ, and Y1, Y2, . . . , Yn2 be independent normal random
variables with parameters µ2 and σ. Deﬁne
X = 1
n1
n1

i=1
Xi
S2
1 = 1
n1
n1

i=1
(Xi −X)2
Y = 1
n2
n2

i=1
Yi
S2
2 = 1
n2
n2

i=1
(Yi −Y )2
(6.18)
(a) The random variable Y = (n1S2
1 + n2S2
2)/σ2 has a chi–square dis-
tribution with n1 + n2 −2 degrees of freedom.
(b) The random variable
W =
(X −Y ) −(µ1 −µ2)

1
n1 +
1
n2

n1S2
1+n2S2
2
n1+n2−2
(6.19)
has a t distribution with n1 + n2 −2 degrees of freedom.
(5) Let X1, X2, . . . , Xn1 be independent normal random variables with pa-
rameters µ1 and σ1, and Y1, Y2, . . . , Yn2 be independent normal random
variables with parameters µ2 and σ2. Deﬁne
X = 1
n1
n1

i=1
Xi
S2
1 = 1
n1
n1

i=1
(Xi −X)2
Y = 1
n2
n2

i=1
Yi
S2
2 = 1
n2
n2

i=1
(Yi −Y )2
(6.20)
The random variable
Y =
n1S2
1
(n1 −1)σ2
1
*
n2S2
2
(n2 −1)σ2
2
(6.21)
c
⃝2000 by Chapman & Hall/CRC

has an F distribution with n1 and n2 degrees of freedom.
(6) Let X1 be a normal random variable with parameters µ = λ and σ = 1,
X2 a chi–square random variable with parameter ν, and X1 and X2 be
independent. The random variable Y = X1/

X2/ν has a noncentral t
distribution with parameters ν and λ.
(7) Let X be a continuous random variable with cumulative distribution
function F(x).
(a) The random variable Y = F(X) has a (standard) uniform distri-
bution with parameters a = 0 and b = 1.
(b) The random variable Y = −ln[1 −F(X)] has a (standard) expo-
nential distribution with parameter λ = 1.
Let X be a continuous random variable with probability density function
f(x). The random variable Y = |X| has probability density function
g(y) given by
g(y) =
(
f(y) + f(−y) if y > 0
0
elsewhere
(6.22)
If X has a standard normal distribution (µ = 0, σ = 1) then g(y) =
2f(y).
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 7
Standard Normal
Distribution
Contents
7.1
Density function and related functions
7.2
Critical values
7.3
Tolerance factors for normal distributions
7.3.1
Tables of tolerance intervals
7.4
Operating characteristic curves
7.4.1
One-sample Z test
7.4.2
Two-sample Z test
7.5
Multivariate normal distribution
7.6
Distribution of the correlation coeﬃcient
7.6.1
Normal approximation
7.6.2
Zero coeﬃcient for bivariate normal
7.7
Circular normal probabilities
7.8
Circular error probabilities
7.1
THE PROBABILITY DENSITY FUNCTION AND
RELATED FUNCTIONS
Let Z be a standard normal random variable (µ = 0, σ = 1). The probability
density function is given by
f(z) =
1
√
2π e−z2/2.
(7.1)
The following tables contain values for:
(1) f(z)
(2) F(z) = Prob [Z ≤z] =
 z
−∞
1
√
2π e−t2/2 dt.
= the cumulative distribution function
c
⃝2000 by Chapman & Hall/CRC

Figure 7.1: Cumulative distribution function for a standard normal random
variable.
Note:
(1) For all z, f(−z) = f(z).
(2) For all z, F(−z) = 1 −F(z)
(3) For all z, Prob [|Z| ≤z] = F(z) −F(−z)
(4) For all z, Prob [|Z| ≥z] = 1 −F(z) + F(−z)
(5) The function Φ(z) = F(z) is often used to represent the normal distri-
bution function.
(6) f ′(x) = −
x
√
2πe−x2/2 = −x f(x)
(7) f ′′(x) =

x2 −1

f(x)
(8) f ′′′(x) =

3x −x3
f(x)
(9) f (4)(x) =

x4 −6x2 + 3

f(x)
(10) For large values of x:
%
e−x2/2
√
2π
 1
x −1
x3
&
< 1 −Φ(x) <
%
e−x2/2
√
2π
 1
x
&
(7.2)
(11) Order statistics for the normal distribution may be found in section 4.6.8
on page 64.
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
−4.00
0.0001
0.0000
1.0000
−3.50
0.0009
0.0002
0.9998
−3.99
0.0001
0.0000
1.0000
−3.49
0.0009
0.0002
0.9998
−3.98
0.0001
0.0000
1.0000
−3.48
0.0009
0.0003
0.9998
−3.97
0.0001
0.0000
1.0000
−3.47
0.0010
0.0003
0.9997
−3.96
0.0002
0.0000
1.0000
−3.46
0.0010
0.0003
0.9997
−3.95
0.0002
0.0000
1.0000
−3.45
0.0010
0.0003
0.9997
−3.94
0.0002
0.0000
1.0000
−3.44
0.0011
0.0003
0.9997
−3.93
0.0002
0.0000
1.0000
−3.43
0.0011
0.0003
0.9997
−3.92
0.0002
0.0000
1.0000
−3.42
0.0011
0.0003
0.9997
−3.91
0.0002
0.0001
1.0000
−3.41
0.0012
0.0003
0.9997
−3.90
0.0002
0.0001
1.0000
−3.40
0.0012
0.0003
0.9997
−3.89
0.0002
0.0001
1.0000
−3.39
0.0013
0.0003
0.9997
−3.88
0.0002
0.0001
1.0000
−3.38
0.0013
0.0004
0.9996
−3.87
0.0002
0.0001
1.0000
−3.37
0.0014
0.0004
0.9996
−3.86
0.0002
0.0001
0.9999
−3.36
0.0014
0.0004
0.9996
−3.85
0.0002
0.0001
0.9999
−3.35
0.0015
0.0004
0.9996
−3.84
0.0003
0.0001
0.9999
−3.34
0.0015
0.0004
0.9996
−3.83
0.0003
0.0001
0.9999
−3.33
0.0016
0.0004
0.9996
−3.82
0.0003
0.0001
0.9999
−3.32
0.0016
0.0004
0.9996
−3.81
0.0003
0.0001
0.9999
−3.31
0.0017
0.0005
0.9995
−3.80
0.0003
0.0001
0.9999
−3.30
0.0017
0.0005
0.9995
−3.79
0.0003
0.0001
0.9999
−3.29
0.0018
0.0005
0.9995
−3.78
0.0003
0.0001
0.9999
−3.28
0.0018
0.0005
0.9995
−3.77
0.0003
0.0001
0.9999
−3.27
0.0019
0.0005
0.9995
−3.76
0.0003
0.0001
0.9999
−3.26
0.0020
0.0006
0.9994
−3.75
0.0003
0.0001
0.9999
−3.25
0.0020
0.0006
0.9994
−3.74
0.0004
0.0001
0.9999
−3.24
0.0021
0.0006
0.9994
−3.73
0.0004
0.0001
0.9999
−3.23
0.0022
0.0006
0.9994
−3.72
0.0004
0.0001
0.9999
−3.22
0.0022
0.0006
0.9994
−3.71
0.0004
0.0001
0.9999
−3.21
0.0023
0.0007
0.9993
−3.70
0.0004
0.0001
0.9999
−3.20
0.0024
0.0007
0.9993
−3.69
0.0004
0.0001
0.9999
−3.19
0.0025
0.0007
0.9993
−3.68
0.0005
0.0001
0.9999
−3.18
0.0025
0.0007
0.9993
−3.67
0.0005
0.0001
0.9999
−3.17
0.0026
0.0008
0.9992
−3.66
0.0005
0.0001
0.9999
−3.16
0.0027
0.0008
0.9992
−3.65
0.0005
0.0001
0.9999
−3.15
0.0028
0.0008
0.9992
−3.64
0.0005
0.0001
0.9999
−3.14
0.0029
0.0008
0.9992
−3.63
0.0006
0.0001
0.9999
−3.13
0.0030
0.0009
0.9991
−3.62
0.0006
0.0001
0.9999
−3.12
0.0031
0.0009
0.9991
−3.61
0.0006
0.0001
0.9999
−3.11
0.0032
0.0009
0.9991
−3.60
0.0006
0.0002
0.9998
−3.10
0.0033
0.0010
0.9990
−3.59
0.0006
0.0002
0.9998
−3.09
0.0034
0.0010
0.9990
−3.58
0.0007
0.0002
0.9998
−3.08
0.0035
0.0010
0.9990
−3.57
0.0007
0.0002
0.9998
−3.07
0.0036
0.0011
0.9989
−3.56
0.0007
0.0002
0.9998
−3.06
0.0037
0.0011
0.9989
−3.55
0.0007
0.0002
0.9998
−3.05
0.0038
0.0011
0.9989
−3.54
0.0008
0.0002
0.9998
−3.04
0.0039
0.0012
0.9988
−3.53
0.0008
0.0002
0.9998
−3.03
0.0040
0.0012
0.9988
−3.52
0.0008
0.0002
0.9998
−3.02
0.0042
0.0013
0.9987
−3.51
0.0008
0.0002
0.9998
−3.01
0.0043
0.0013
0.9987
−3.50
0.0009
0.0002
0.9998
−3.00
0.0044
0.0014
0.9987
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
−3.00
0.0044
0.0014
0.9987
−2.50
0.0175
0.0062
0.9938
−2.99
0.0046
0.0014
0.9986
−2.49
0.0180
0.0064
0.9936
−2.98
0.0047
0.0014
0.9986
−2.48
0.0184
0.0066
0.9934
−2.97
0.0049
0.0015
0.9985
−2.47
0.0189
0.0068
0.9932
−2.96
0.0050
0.0015
0.9985
−2.46
0.0194
0.0069
0.9930
−2.95
0.0051
0.0016
0.9984
−2.45
0.0198
0.0071
0.9929
−2.94
0.0053
0.0016
0.9984
−2.44
0.0203
0.0073
0.9927
−2.93
0.0054
0.0017
0.9983
−2.43
0.0208
0.0076
0.9925
−2.92
0.0056
0.0018
0.9982
−2.42
0.0213
0.0078
0.9922
−2.91
0.0058
0.0018
0.9982
−2.41
0.0219
0.0080
0.9920
−2.90
0.0060
0.0019
0.9981
−2.40
0.0224
0.0082
0.9918
−2.89
0.0061
0.0019
0.9981
−2.39
0.0229
0.0084
0.9916
−2.88
0.0063
0.0020
0.9980
−2.38
0.0235
0.0087
0.9913
−2.87
0.0065
0.0021
0.9980
−2.37
0.0241
0.0089
0.9911
−2.86
0.0067
0.0021
0.9979
−2.36
0.0246
0.0091
0.9909
−2.85
0.0069
0.0022
0.9978
−2.35
0.0252
0.0094
0.9906
−2.84
0.0071
0.0023
0.9977
−2.34
0.0258
0.0096
0.9904
−2.83
0.0073
0.0023
0.9977
−2.33
0.0264
0.0099
0.9901
−2.82
0.0075
0.0024
0.9976
−2.32
0.0271
0.0102
0.9898
−2.81
0.0077
0.0025
0.9975
−2.31
0.0277
0.0104
0.9896
−2.80
0.0079
0.0026
0.9974
−2.30
0.0283
0.0107
0.9893
−2.79
0.0081
0.0026
0.9974
−2.29
0.0290
0.0110
0.9890
−2.78
0.0084
0.0027
0.9973
−2.28
0.0296
0.0113
0.9887
−2.77
0.0086
0.0028
0.9972
−2.27
0.0303
0.0116
0.9884
−2.76
0.0089
0.0029
0.9971
−2.26
0.0310
0.0119
0.9881
−2.75
0.0091
0.0030
0.9970
−2.25
0.0317
0.0122
0.9878
−2.74
0.0094
0.0031
0.9969
−2.24
0.0325
0.0126
0.9875
−2.73
0.0096
0.0032
0.9968
−2.23
0.0332
0.0129
0.9871
−2.72
0.0099
0.0033
0.9967
−2.22
0.0339
0.0132
0.9868
−2.71
0.0101
0.0034
0.9966
−2.21
0.0347
0.0135
0.9865
−2.70
0.0104
0.0035
0.9965
−2.20
0.0355
0.0139
0.9861
−2.69
0.0107
0.0036
0.9964
−2.19
0.0363
0.0143
0.9857
−2.68
0.0110
0.0037
0.9963
−2.18
0.0371
0.0146
0.9854
−2.67
0.0113
0.0038
0.9962
−2.17
0.0379
0.0150
0.9850
−2.66
0.0116
0.0039
0.9961
−2.16
0.0387
0.0154
0.9846
−2.65
0.0119
0.0040
0.9960
−2.15
0.0396
0.0158
0.9842
−2.64
0.0122
0.0042
0.9959
−2.14
0.0404
0.0162
0.9838
−2.63
0.0126
0.0043
0.9957
−2.13
0.0413
0.0166
0.9834
−2.62
0.0129
0.0044
0.9956
−2.12
0.0422
0.0170
0.9830
−2.61
0.0132
0.0045
0.9955
−2.11
0.0431
0.0174
0.9826
−2.60
0.0136
0.0047
0.9953
−2.10
0.0440
0.0179
0.9821
−2.59
0.0139
0.0048
0.9952
−2.09
0.0449
0.0183
0.9817
−2.58
0.0143
0.0049
0.9951
−2.08
0.0459
0.0188
0.9812
−2.57
0.0147
0.0051
0.9949
−2.07
0.0468
0.0192
0.9808
−2.56
0.0151
0.0052
0.9948
−2.06
0.0478
0.0197
0.9803
−2.55
0.0155
0.0054
0.9946
−2.05
0.0488
0.0202
0.9798
−2.54
0.0158
0.0055
0.9945
−2.04
0.0498
0.0207
0.9793
−2.53
0.0163
0.0057
0.9943
−2.03
0.0508
0.0212
0.9788
−2.52
0.0167
0.0059
0.9941
−2.02
0.0519
0.0217
0.9783
−2.51
0.0171
0.0060
0.9940
−2.01
0.0529
0.0222
0.9778
−2.50
0.0175
0.0062
0.9938
−2.00
0.0540
0.0227
0.9772
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
−2.00
0.0540
0.0227
0.9772
−1.50
0.1295
0.0668
0.9332
−1.99
0.0551
0.0233
0.9767
−1.49
0.1315
0.0681
0.9319
−1.98
0.0562
0.0238
0.9761
−1.48
0.1334
0.0694
0.9306
−1.97
0.0573
0.0244
0.9756
−1.47
0.1354
0.0708
0.9292
−1.96
0.0584
0.0250
0.9750
−1.46
0.1374
0.0722
0.9278
−1.95
0.0596
0.0256
0.9744
−1.45
0.1394
0.0735
0.9265
−1.94
0.0608
0.0262
0.9738
−1.44
0.1415
0.0749
0.9251
−1.93
0.0619
0.0268
0.9732
−1.43
0.1435
0.0764
0.9236
−1.92
0.0632
0.0274
0.9726
−1.42
0.1456
0.0778
0.9222
−1.91
0.0644
0.0281
0.9719
−1.41
0.1476
0.0793
0.9207
−1.90
0.0656
0.0287
0.9713
−1.40
0.1497
0.0808
0.9192
−1.89
0.0669
0.0294
0.9706
−1.39
0.1518
0.0823
0.9177
−1.88
0.0681
0.0301
0.9699
−1.38
0.1540
0.0838
0.9162
−1.87
0.0694
0.0307
0.9693
−1.37
0.1561
0.0853
0.9147
−1.86
0.0707
0.0314
0.9686
−1.36
0.1582
0.0869
0.9131
−1.85
0.0721
0.0322
0.9678
−1.35
0.1604
0.0885
0.9115
−1.84
0.0734
0.0329
0.9671
−1.34
0.1626
0.0901
0.9099
−1.83
0.0748
0.0336
0.9664
−1.33
0.1647
0.0918
0.9082
−1.82
0.0761
0.0344
0.9656
−1.32
0.1669
0.0934
0.9066
−1.81
0.0775
0.0352
0.9648
−1.31
0.1691
0.0951
0.9049
−1.80
0.0790
0.0359
0.9641
−1.30
0.1714
0.0968
0.9032
−1.79
0.0804
0.0367
0.9633
−1.29
0.1736
0.0985
0.9015
−1.78
0.0818
0.0375
0.9625
−1.28
0.1759
0.1003
0.8997
−1.77
0.0833
0.0384
0.9616
−1.27
0.1781
0.1020
0.8980
−1.76
0.0848
0.0392
0.9608
−1.26
0.1804
0.1038
0.8962
−1.75
0.0863
0.0401
0.9599
−1.25
0.1827
0.1056
0.8943
−1.74
0.0878
0.0409
0.9591
−1.24
0.1849
0.1075
0.8925
−1.73
0.0893
0.0418
0.9582
−1.23
0.1872
0.1094
0.8907
−1.72
0.0909
0.0427
0.9573
−1.22
0.1895
0.1112
0.8888
−1.71
0.0925
0.0436
0.9564
−1.21
0.1919
0.1131
0.8869
−1.70
0.0940
0.0446
0.9554
−1.20
0.1942
0.1151
0.8849
−1.69
0.0957
0.0455
0.9545
−1.19
0.1965
0.1170
0.8830
−1.68
0.0973
0.0465
0.9535
−1.18
0.1989
0.1190
0.8810
−1.67
0.0989
0.0475
0.9525
−1.17
0.2012
0.1210
0.8790
−1.66
0.1006
0.0485
0.9515
−1.16
0.2036
0.1230
0.8770
−1.65
0.1023
0.0495
0.9505
−1.15
0.2059
0.1251
0.8749
−1.64
0.1040
0.0505
0.9495
−1.14
0.2083
0.1271
0.8729
−1.63
0.1057
0.0515
0.9485
−1.13
0.2107
0.1292
0.8708
−1.62
0.1074
0.0526
0.9474
−1.12
0.2131
0.1314
0.8686
−1.61
0.1091
0.0537
0.9463
−1.11
0.2155
0.1335
0.8665
−1.60
0.1109
0.0548
0.9452
−1.10
0.2178
0.1357
0.8643
−1.59
0.1127
0.0559
0.9441
−1.09
0.2203
0.1379
0.8621
−1.58
0.1145
0.0570
0.9429
−1.08
0.2226
0.1401
0.8599
−1.57
0.1163
0.0582
0.9418
−1.07
0.2251
0.1423
0.8577
−1.56
0.1182
0.0594
0.9406
−1.06
0.2275
0.1446
0.8554
−1.55
0.1200
0.0606
0.9394
−1.05
0.2299
0.1469
0.8531
−1.54
0.1219
0.0618
0.9382
−1.04
0.2323
0.1492
0.8508
−1.53
0.1238
0.0630
0.9370
−1.03
0.2347
0.1515
0.8485
−1.52
0.1257
0.0643
0.9357
−1.02
0.2371
0.1539
0.8461
−1.51
0.1276
0.0655
0.9345
−1.01
0.2396
0.1563
0.8438
−1.50
0.1295
0.0668
0.9332
−1.00
0.2420
0.1587
0.8413
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
−1.00
0.2420
0.1587
0.8413
−0.50
0.3521
0.3085
0.6915
−0.99
0.2444
0.1611
0.8389
−0.49
0.3538
0.3121
0.6879
−0.98
0.2468
0.1635
0.8365
−0.48
0.3555
0.3156
0.6844
−0.97
0.2492
0.1660
0.8340
−0.47
0.3572
0.3192
0.6808
−0.96
0.2516
0.1685
0.8315
−0.46
0.3589
0.3228
0.6772
−0.95
0.2541
0.1711
0.8289
−0.45
0.3605
0.3264
0.6736
−0.94
0.2565
0.1736
0.8264
−0.44
0.3621
0.3300
0.6700
−0.93
0.2589
0.1762
0.8238
−0.43
0.3637
0.3336
0.6664
−0.92
0.2613
0.1788
0.8212
−0.42
0.3653
0.3372
0.6628
−0.91
0.2637
0.1814
0.8186
−0.41
0.3668
0.3409
0.6591
−0.90
0.2661
0.1841
0.8159
−0.40
0.3683
0.3446
0.6554
−0.89
0.2685
0.1867
0.8133
−0.39
0.3697
0.3483
0.6517
−0.88
0.2709
0.1894
0.8106
−0.38
0.3711
0.3520
0.6480
−0.87
0.2732
0.1921
0.8078
−0.37
0.3725
0.3557
0.6443
−0.86
0.2756
0.1949
0.8051
−0.36
0.3739
0.3594
0.6406
−0.85
0.2780
0.1977
0.8023
−0.35
0.3752
0.3632
0.6368
−0.84
0.2803
0.2004
0.7995
−0.34
0.3765
0.3669
0.6331
−0.83
0.2827
0.2033
0.7967
−0.33
0.3778
0.3707
0.6293
−0.82
0.2850
0.2061
0.7939
−0.32
0.3790
0.3745
0.6255
−0.81
0.2874
0.2090
0.7910
−0.31
0.3802
0.3783
0.6217
−0.80
0.2897
0.2119
0.7881
−0.30
0.3814
0.3821
0.6179
−0.79
0.2920
0.2148
0.7852
−0.29
0.3825
0.3859
0.6141
−0.78
0.2943
0.2177
0.7823
−0.28
0.3836
0.3897
0.6103
−0.77
0.2966
0.2207
0.7793
−0.27
0.3847
0.3936
0.6064
−0.76
0.2989
0.2236
0.7764
−0.26
0.3857
0.3974
0.6026
−0.75
0.3011
0.2266
0.7734
−0.25
0.3867
0.4013
0.5987
−0.74
0.3034
0.2296
0.7703
−0.24
0.3876
0.4052
0.5948
−0.73
0.3056
0.2327
0.7673
−0.23
0.3885
0.4091
0.5909
−0.72
0.3079
0.2358
0.7642
−0.22
0.3894
0.4129
0.5871
−0.71
0.3101
0.2389
0.7611
−0.21
0.3902
0.4168
0.5832
−0.70
0.3123
0.2420
0.7580
−0.20
0.3910
0.4207
0.5793
−0.69
0.3144
0.2451
0.7549
−0.19
0.3918
0.4247
0.5754
−0.68
0.3166
0.2482
0.7518
−0.18
0.3925
0.4286
0.5714
−0.67
0.3187
0.2514
0.7486
−0.17
0.3932
0.4325
0.5675
−0.66
0.3209
0.2546
0.7454
−0.16
0.3939
0.4364
0.5636
−0.65
0.3230
0.2579
0.7421
−0.15
0.3945
0.4404
0.5596
−0.64
0.3251
0.2611
0.7389
−0.14
0.3951
0.4443
0.5557
−0.63
0.3271
0.2643
0.7357
−0.13
0.3956
0.4483
0.5517
−0.62
0.3292
0.2676
0.7324
−0.12
0.3961
0.4522
0.5478
−0.61
0.3312
0.2709
0.7291
−0.11
0.3965
0.4562
0.5438
−0.60
0.3332
0.2742
0.7258
−0.10
0.3970
0.4602
0.5398
−0.59
0.3352
0.2776
0.7224
−0.09
0.3973
0.4641
0.5359
−0.58
0.3372
0.2810
0.7190
−0.08
0.3977
0.4681
0.5319
−0.57
0.3391
0.2843
0.7157
−0.07
0.3980
0.4721
0.5279
−0.56
0.3411
0.2877
0.7123
−0.06
0.3982
0.4761
0.5239
−0.55
0.3429
0.2912
0.7088
−0.05
0.3984
0.4801
0.5199
−0.54
0.3448
0.2946
0.7054
−0.04
0.3986
0.4840
0.5160
−0.53
0.3467
0.2981
0.7019
−0.03
0.3988
0.4880
0.5120
−0.52
0.3485
0.3015
0.6985
−0.02
0.3989
0.4920
0.5080
−0.51
0.3503
0.3050
0.6950
−0.01
0.3989
0.4960
0.5040
−0.50
0.3521
0.3085
0.6915
0.00
0.3989
0.5000
0.5000
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
0.00
0.3989
0.5000
0.5000
0.50
0.3521
0.6915
0.3085
0.01
0.3989
0.5040
0.4960
0.51
0.3503
0.6950
0.3050
0.02
0.3989
0.5080
0.4920
0.52
0.3485
0.6985
0.3015
0.03
0.3988
0.5120
0.4880
0.53
0.3467
0.7019
0.2981
0.04
0.3986
0.5160
0.4840
0.54
0.3448
0.7054
0.2946
0.05
0.3984
0.5199
0.4801
0.55
0.3429
0.7088
0.2912
0.06
0.3982
0.5239
0.4761
0.56
0.3411
0.7123
0.2877
0.07
0.3980
0.5279
0.4721
0.57
0.3391
0.7157
0.2843
0.08
0.3977
0.5319
0.4681
0.58
0.3372
0.7190
0.2810
0.09
0.3973
0.5359
0.4641
0.59
0.3352
0.7224
0.2776
0.10
0.3970
0.5398
0.4602
0.60
0.3332
0.7258
0.2742
0.11
0.3965
0.5438
0.4562
0.61
0.3312
0.7291
0.2709
0.12
0.3961
0.5478
0.4522
0.62
0.3292
0.7324
0.2676
0.13
0.3956
0.5517
0.4483
0.63
0.3271
0.7357
0.2643
0.14
0.3951
0.5557
0.4443
0.64
0.3251
0.7389
0.2611
0.15
0.3945
0.5596
0.4404
0.65
0.3230
0.7421
0.2579
0.16
0.3939
0.5636
0.4364
0.66
0.3209
0.7454
0.2546
0.17
0.3932
0.5675
0.4325
0.67
0.3187
0.7486
0.2514
0.18
0.3925
0.5714
0.4286
0.68
0.3166
0.7518
0.2482
0.19
0.3918
0.5754
0.4247
0.69
0.3144
0.7549
0.2451
0.20
0.3910
0.5793
0.4207
0.70
0.3123
0.7580
0.2420
0.21
0.3902
0.5832
0.4168
0.71
0.3101
0.7611
0.2389
0.22
0.3894
0.5871
0.4129
0.72
0.3079
0.7642
0.2358
0.23
0.3885
0.5909
0.4091
0.73
0.3056
0.7673
0.2327
0.24
0.3876
0.5948
0.4052
0.74
0.3034
0.7703
0.2296
0.25
0.3867
0.5987
0.4013
0.75
0.3011
0.7734
0.2266
0.26
0.3857
0.6026
0.3974
0.76
0.2989
0.7764
0.2236
0.27
0.3847
0.6064
0.3936
0.77
0.2966
0.7793
0.2207
0.28
0.3836
0.6103
0.3897
0.78
0.2943
0.7823
0.2177
0.29
0.3825
0.6141
0.3859
0.79
0.2920
0.7852
0.2148
0.30
0.3814
0.6179
0.3821
0.80
0.2897
0.7881
0.2119
0.31
0.3802
0.6217
0.3783
0.81
0.2874
0.7910
0.2090
0.32
0.3790
0.6255
0.3745
0.82
0.2850
0.7939
0.2061
0.33
0.3778
0.6293
0.3707
0.83
0.2827
0.7967
0.2033
0.34
0.3765
0.6331
0.3669
0.84
0.2803
0.7995
0.2004
0.35
0.3752
0.6368
0.3632
0.85
0.2780
0.8023
0.1977
0.36
0.3739
0.6406
0.3594
0.86
0.2756
0.8051
0.1949
0.37
0.3725
0.6443
0.3557
0.87
0.2732
0.8078
0.1921
0.38
0.3711
0.6480
0.3520
0.88
0.2709
0.8106
0.1894
0.39
0.3697
0.6517
0.3483
0.89
0.2685
0.8133
0.1867
0.40
0.3683
0.6554
0.3446
0.90
0.2661
0.8159
0.1841
0.41
0.3668
0.6591
0.3409
0.91
0.2637
0.8186
0.1814
0.42
0.3653
0.6628
0.3372
0.92
0.2613
0.8212
0.1788
0.43
0.3637
0.6664
0.3336
0.93
0.2589
0.8238
0.1762
0.44
0.3621
0.6700
0.3300
0.94
0.2565
0.8264
0.1736
0.45
0.3605
0.6736
0.3264
0.95
0.2541
0.8289
0.1711
0.46
0.3589
0.6772
0.3228
0.96
0.2516
0.8315
0.1685
0.47
0.3572
0.6808
0.3192
0.97
0.2492
0.8340
0.1660
0.48
0.3555
0.6844
0.3156
0.98
0.2468
0.8365
0.1635
0.49
0.3538
0.6879
0.3121
0.99
0.2444
0.8389
0.1611
0.50
0.3521
0.6915
0.3085
1.00
0.2420
0.8413
0.1587
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
1.00
0.2420
0.8413
0.1587
1.50
0.1295
0.9332
0.0668
1.01
0.2396
0.8438
0.1563
1.51
0.1276
0.9345
0.0655
1.02
0.2371
0.8461
0.1539
1.52
0.1257
0.9357
0.0643
1.03
0.2347
0.8485
0.1515
1.53
0.1238
0.9370
0.0630
1.04
0.2323
0.8508
0.1492
1.54
0.1219
0.9382
0.0618
1.05
0.2299
0.8531
0.1469
1.55
0.1200
0.9394
0.0606
1.06
0.2275
0.8554
0.1446
1.56
0.1182
0.9406
0.0594
1.07
0.2251
0.8577
0.1423
1.57
0.1163
0.9418
0.0582
1.08
0.2226
0.8599
0.1401
1.58
0.1145
0.9429
0.0570
1.09
0.2203
0.8621
0.1379
1.59
0.1127
0.9441
0.0559
1.10
0.2178
0.8643
0.1357
1.60
0.1109
0.9452
0.0548
1.11
0.2155
0.8665
0.1335
1.61
0.1091
0.9463
0.0537
1.12
0.2131
0.8686
0.1314
1.62
0.1074
0.9474
0.0526
1.13
0.2107
0.8708
0.1292
1.63
0.1057
0.9485
0.0515
1.14
0.2083
0.8729
0.1271
1.64
0.1040
0.9495
0.0505
1.15
0.2059
0.8749
0.1251
1.65
0.1023
0.9505
0.0495
1.16
0.2036
0.8770
0.1230
1.66
0.1006
0.9515
0.0485
1.17
0.2012
0.8790
0.1210
1.67
0.0989
0.9525
0.0475
1.18
0.1989
0.8810
0.1190
1.68
0.0973
0.9535
0.0465
1.19
0.1965
0.8830
0.1170
1.69
0.0957
0.9545
0.0455
1.20
0.1942
0.8849
0.1151
1.70
0.0940
0.9554
0.0446
1.21
0.1919
0.8869
0.1131
1.71
0.0925
0.9564
0.0436
1.22
0.1895
0.8888
0.1112
1.72
0.0909
0.9573
0.0427
1.23
0.1872
0.8907
0.1094
1.73
0.0893
0.9582
0.0418
1.24
0.1849
0.8925
0.1075
1.74
0.0878
0.9591
0.0409
1.25
0.1827
0.8943
0.1056
1.75
0.0863
0.9599
0.0401
1.26
0.1804
0.8962
0.1038
1.76
0.0848
0.9608
0.0392
1.27
0.1781
0.8980
0.1020
1.77
0.0833
0.9616
0.0384
1.28
0.1759
0.8997
0.1003
1.78
0.0818
0.9625
0.0375
1.29
0.1736
0.9015
0.0985
1.79
0.0804
0.9633
0.0367
1.30
0.1714
0.9032
0.0968
1.80
0.0790
0.9641
0.0359
1.31
0.1691
0.9049
0.0951
1.81
0.0775
0.9648
0.0352
1.32
0.1669
0.9066
0.0934
1.82
0.0761
0.9656
0.0344
1.33
0.1647
0.9082
0.0918
1.83
0.0748
0.9664
0.0336
1.34
0.1626
0.9099
0.0901
1.84
0.0734
0.9671
0.0329
1.35
0.1604
0.9115
0.0885
1.85
0.0721
0.9678
0.0322
1.36
0.1582
0.9131
0.0869
1.86
0.0707
0.9686
0.0314
1.37
0.1561
0.9147
0.0853
1.87
0.0694
0.9693
0.0307
1.38
0.1540
0.9162
0.0838
1.88
0.0681
0.9699
0.0301
1.39
0.1518
0.9177
0.0823
1.89
0.0669
0.9706
0.0294
1.40
0.1497
0.9192
0.0808
1.90
0.0656
0.9713
0.0287
1.41
0.1476
0.9207
0.0793
1.91
0.0644
0.9719
0.0281
1.42
0.1456
0.9222
0.0778
1.92
0.0632
0.9726
0.0274
1.43
0.1435
0.9236
0.0764
1.93
0.0619
0.9732
0.0268
1.44
0.1415
0.9251
0.0749
1.94
0.0608
0.9738
0.0262
1.45
0.1394
0.9265
0.0735
1.95
0.0596
0.9744
0.0256
1.46
0.1374
0.9278
0.0722
1.96
0.0584
0.9750
0.0250
1.47
0.1354
0.9292
0.0708
1.97
0.0573
0.9756
0.0244
1.48
0.1334
0.9306
0.0694
1.98
0.0562
0.9761
0.0238
1.49
0.1315
0.9319
0.0681
1.99
0.0551
0.9767
0.0233
1.50
0.1295
0.9332
0.0668
2.00
0.0540
0.9772
0.0227
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
2.00
0.0540
0.9772
0.0227
2.50
0.0175
0.9938
0.0062
2.01
0.0529
0.9778
0.0222
2.51
0.0171
0.9940
0.0060
2.02
0.0519
0.9783
0.0217
2.52
0.0167
0.9941
0.0059
2.03
0.0508
0.9788
0.0212
2.53
0.0163
0.9943
0.0057
2.04
0.0498
0.9793
0.0207
2.54
0.0158
0.9945
0.0055
2.05
0.0488
0.9798
0.0202
2.55
0.0155
0.9946
0.0054
2.06
0.0478
0.9803
0.0197
2.56
0.0151
0.9948
0.0052
2.07
0.0468
0.9808
0.0192
2.57
0.0147
0.9949
0.0051
2.08
0.0459
0.9812
0.0188
2.58
0.0143
0.9951
0.0049
2.09
0.0449
0.9817
0.0183
2.59
0.0139
0.9952
0.0048
2.10
0.0440
0.9821
0.0179
2.60
0.0136
0.9953
0.0047
2.11
0.0431
0.9826
0.0174
2.61
0.0132
0.9955
0.0045
2.12
0.0422
0.9830
0.0170
2.62
0.0129
0.9956
0.0044
2.13
0.0413
0.9834
0.0166
2.63
0.0126
0.9957
0.0043
2.14
0.0404
0.9838
0.0162
2.64
0.0122
0.9959
0.0042
2.15
0.0396
0.9842
0.0158
2.65
0.0119
0.9960
0.0040
2.16
0.0387
0.9846
0.0154
2.66
0.0116
0.9961
0.0039
2.17
0.0379
0.9850
0.0150
2.67
0.0113
0.9962
0.0038
2.18
0.0371
0.9854
0.0146
2.68
0.0110
0.9963
0.0037
2.19
0.0363
0.9857
0.0143
2.69
0.0107
0.9964
0.0036
2.20
0.0355
0.9861
0.0139
2.70
0.0104
0.9965
0.0035
2.21
0.0347
0.9865
0.0135
2.71
0.0101
0.9966
0.0034
2.22
0.0339
0.9868
0.0132
2.72
0.0099
0.9967
0.0033
2.23
0.0332
0.9871
0.0129
2.73
0.0096
0.9968
0.0032
2.24
0.0325
0.9875
0.0126
2.74
0.0094
0.9969
0.0031
2.25
0.0317
0.9878
0.0122
2.75
0.0091
0.9970
0.0030
2.26
0.0310
0.9881
0.0119
2.76
0.0089
0.9971
0.0029
2.27
0.0303
0.9884
0.0116
2.77
0.0086
0.9972
0.0028
2.28
0.0296
0.9887
0.0113
2.78
0.0084
0.9973
0.0027
2.29
0.0290
0.9890
0.0110
2.79
0.0081
0.9974
0.0026
2.30
0.0283
0.9893
0.0107
2.80
0.0079
0.9974
0.0026
2.31
0.0277
0.9896
0.0104
2.81
0.0077
0.9975
0.0025
2.32
0.0271
0.9898
0.0102
2.82
0.0075
0.9976
0.0024
2.33
0.0264
0.9901
0.0099
2.83
0.0073
0.9977
0.0023
2.34
0.0258
0.9904
0.0096
2.84
0.0071
0.9977
0.0023
2.35
0.0252
0.9906
0.0094
2.85
0.0069
0.9978
0.0022
2.36
0.0246
0.9909
0.0091
2.86
0.0067
0.9979
0.0021
2.37
0.0241
0.9911
0.0089
2.87
0.0065
0.9980
0.0021
2.38
0.0235
0.9913
0.0087
2.88
0.0063
0.9980
0.0020
2.39
0.0229
0.9916
0.0084
2.89
0.0061
0.9981
0.0019
2.40
0.0224
0.9918
0.0082
2.90
0.0060
0.9981
0.0019
2.41
0.0219
0.9920
0.0080
2.91
0.0058
0.9982
0.0018
2.42
0.0213
0.9922
0.0078
2.92
0.0056
0.9982
0.0018
2.43
0.0208
0.9925
0.0076
2.93
0.0054
0.9983
0.0017
2.44
0.0203
0.9927
0.0073
2.94
0.0053
0.9984
0.0016
2.45
0.0198
0.9929
0.0071
2.95
0.0051
0.9984
0.0016
2.46
0.0194
0.9930
0.0069
2.96
0.0050
0.9985
0.0015
2.47
0.0189
0.9932
0.0068
2.97
0.0049
0.9985
0.0015
2.48
0.0184
0.9934
0.0066
2.98
0.0047
0.9986
0.0014
2.49
0.0180
0.9936
0.0064
2.99
0.0046
0.9986
0.0014
2.50
0.0175
0.9938
0.0062
3.00
0.0044
0.9987
0.0014
c
⃝2000 by Chapman & Hall/CRC

Normal distribution
z
f(z)
F(z)
1 −F(z)
z
f(z)
F(z)
1 −F(z)
3.00
0.0044
0.9987
0.0014
3.50
0.0009
0.9998
0.0002
3.01
0.0043
0.9987
0.0013
3.51
0.0008
0.9998
0.0002
3.02
0.0042
0.9987
0.0013
3.52
0.0008
0.9998
0.0002
3.03
0.0040
0.9988
0.0012
3.53
0.0008
0.9998
0.0002
3.04
0.0039
0.9988
0.0012
3.54
0.0008
0.9998
0.0002
3.05
0.0038
0.9989
0.0011
3.55
0.0007
0.9998
0.0002
3.06
0.0037
0.9989
0.0011
3.56
0.0007
0.9998
0.0002
3.07
0.0036
0.9989
0.0011
3.57
0.0007
0.9998
0.0002
3.08
0.0035
0.9990
0.0010
3.58
0.0007
0.9998
0.0002
3.09
0.0034
0.9990
0.0010
3.59
0.0006
0.9998
0.0002
3.10
0.0033
0.9990
0.0010
3.60
0.0006
0.9998
0.0002
3.11
0.0032
0.9991
0.0009
3.61
0.0006
0.9999
0.0001
3.12
0.0031
0.9991
0.0009
3.62
0.0006
0.9999
0.0001
3.13
0.0030
0.9991
0.0009
3.63
0.0006
0.9999
0.0001
3.14
0.0029
0.9992
0.0008
3.64
0.0005
0.9999
0.0001
3.15
0.0028
0.9992
0.0008
3.65
0.0005
0.9999
0.0001
3.16
0.0027
0.9992
0.0008
3.66
0.0005
0.9999
0.0001
3.17
0.0026
0.9992
0.0008
3.67
0.0005
0.9999
0.0001
3.18
0.0025
0.9993
0.0007
3.68
0.0005
0.9999
0.0001
3.19
0.0025
0.9993
0.0007
3.69
0.0004
0.9999
0.0001
3.20
0.0024
0.9993
0.0007
3.70
0.0004
0.9999
0.0001
3.21
0.0023
0.9993
0.0007
3.71
0.0004
0.9999
0.0001
3.22
0.0022
0.9994
0.0006
3.72
0.0004
0.9999
0.0001
3.23
0.0022
0.9994
0.0006
3.73
0.0004
0.9999
0.0001
3.24
0.0021
0.9994
0.0006
3.74
0.0004
0.9999
0.0001
3.25
0.0020
0.9994
0.0006
3.75
0.0003
0.9999
0.0001
3.26
0.0020
0.9994
0.0006
3.76
0.0003
0.9999
0.0001
3.27
0.0019
0.9995
0.0005
3.77
0.0003
0.9999
0.0001
3.28
0.0018
0.9995
0.0005
3.78
0.0003
0.9999
0.0001
3.29
0.0018
0.9995
0.0005
3.79
0.0003
0.9999
0.0001
3.30
0.0017
0.9995
0.0005
3.80
0.0003
0.9999
0.0001
3.31
0.0017
0.9995
0.0005
3.81
0.0003
0.9999
0.0001
3.32
0.0016
0.9996
0.0004
3.82
0.0003
0.9999
0.0001
3.33
0.0016
0.9996
0.0004
3.83
0.0003
0.9999
0.0001
3.34
0.0015
0.9996
0.0004
3.84
0.0003
0.9999
0.0001
3.35
0.0015
0.9996
0.0004
3.85
0.0002
0.9999
0.0001
3.36
0.0014
0.9996
0.0004
3.86
0.0002
0.9999
0.0001
3.37
0.0014
0.9996
0.0004
3.87
0.0002
1.0000
0.0001
3.38
0.0013
0.9996
0.0004
3.88
0.0002
1.0000
0.0001
3.39
0.0013
0.9997
0.0003
3.89
0.0002
1.0000
0.0001
3.40
0.0012
0.9997
0.0003
3.90
0.0002
1.0000
0.0001
3.41
0.0012
0.9997
0.0003
3.91
0.0002
1.0000
0.0001
3.42
0.0011
0.9997
0.0003
3.92
0.0002
1.0000
0.0000
3.43
0.0011
0.9997
0.0003
3.93
0.0002
1.0000
0.0000
3.44
0.0011
0.9997
0.0003
3.94
0.0002
1.0000
0.0000
3.45
0.0010
0.9997
0.0003
3.95
0.0002
1.0000
0.0000
3.46
0.0010
0.9997
0.0003
3.96
0.0002
1.0000
0.0000
3.47
0.0010
0.9997
0.0003
3.97
0.0001
1.0000
0.0000
3.48
0.0009
0.9998
0.0003
3.98
0.0001
1.0000
0.0000
3.49
0.0009
0.9998
0.0002
3.99
0.0001
1.0000
0.0000
3.50
0.0009
0.9998
0.0002
4.00
0.0001
1.0000
0.0000
c
⃝2000 by Chapman & Hall/CRC

7.2
CRITICAL VALUES
Table 7.1 lists common critical values for a standard normal random variable,
zα, deﬁned by (see Figure 7.2):
Prob [Z ≥zα] = α.
(7.3)
Figure 7.2: Critical values for a normal random variable.
α
zα
α
zα
α
zα
.10
1.2816
.00009
3.7455
.000001
4.75
.05
1.6449
.00008
3.7750
.0000001
5.20
.025
1.9600
.00007
3.8082
.00000001
5.61
.01
2.3263
.00006
3.8461
.000000001
6.00
.005
2.5758
.00005
3.8906
.0000000001
6.36
.0025
2.8070
.00004
3.9444
.001
3.0902
.00003
4.0128
.0005
3.2905
.00002
4.1075
.0001
3.7190
.00001
4.2649
Table 7.1: Common critical values
7.3
TOLERANCE FACTORS FOR NORMAL DISTRIBUTIONS
Suppose X1, X2, . . . , Xn is a random sample of size n from a normal popula-
tion with mean µ and standard deviation σ. Using the summary statistics x
and s, a tolerance interval [L, U] may be constructed to capture 100P% of the
population with probability 1 −α. The following procedures may be used.
(1) Two-sided tolerance interval: A 100(1 −α)% tolerance interval that
captures 100P% of the population has as endpoints
[L, U] = x ± Kα,n,P · s
(7.4)
c
⃝2000 by Chapman & Hall/CRC

(2) One-sided tolerance interval, upper tailed: A 100(1 −α)% tolerance
interval bounded below has
L = x −kα,n,P · s
U = ∞
(7.5)
(3) One-sided tolerance interval, lower tailed: A 100(1 −α)% tolerance in-
terval bounded above has
L = −∞
U = x + kα,n,P · s
(7.6)
where Kα,n,P is the tolerance factor given in section 7.3.1 and kα,n,P is com-
puted using the formula below.
Values of Kα,n,P are given in section 7.3.1 for P = 0.75, 0.90, 0.95, 0.99,
0.999, α = 0.75, 0.90, 0.95, 0.99, and various values of n. The value of kα,n,P
is given by
kα,n,P = z1−P +

(z1−P )2 −ab
a
a = 1 −
(zα)2
2(n −1)
b = z2
1−P −z2
α
n
(7.7)
where z1−P and zα are critical values for a standard normal random variable
(see page 175).
Example 7.43:
Suppose a sample of size n = 30 from a normal distribution has
x = 10.02 and s = 0.13. Find tolerance intervals with a conﬁdence level 95% (α = .05)
and P = .90.
Solution:
(S1) Two-sided interval:
1. From the tables in section 7.3.1 we ﬁnd K.05,30,.90 = 2.413.
2. The interval is x ± K · s = 10.02 ± 0.31; or I = [9.71, 10.33].
3. We conclude: in each sample of size 30, at least 90% of the normal popu-
lation being sampled will be in the interval I, with probability 95%.
(S2) One-sided intervals:
1. The critical values used in equation (7.7) are z1−P = z.10 = 1.282 and
zα = z.05 = 1.645.
Using this equation: a = 1 −(1.645)2
2(29)
= 0.9533,
b = (1.282)2 −(1.645)2
30
= 1.553, and k.05,30,.90 = 1.768
2. The lower bound is L = x −k · s = 9.79.
3. The upper bound is U = x + k · s = 10.25.
4. We conclude:
(a) In each sample of size 30, at least 90% of the normal population being
sampled will be greater than L, with probability 95%.
(b) In each sample of size 30, at least 90% of the normal population being
sampled will be smaller than U, with probability 95%.
c
⃝2000 by Chapman & Hall/CRC

7.3.1
Tables of tolerance intervals for normal distributions
Tolerance factors for normal distributions
P = .90
n α = .10
.05
.01
.001
n α = .10
.05
.01
.001
2
15.978 18.800 24.167 30.227
20
2.152 2.564 3.368 4.300
3
5.847
6.919
8.974 11.309
25
2.077 2.474 3.251 4.151
4
4.166
4.943
6.440
8.149
30
2.025 2.413 3.170 4.049
5
3.494
4.152
5.423
6.879
40
1.959 2.334 3.066 3.917
6
3.131
3.723
4.870
6.188
50
1.916 2.284 3.001 3.833
7
2.902
3.452
4.521
5.750
75
1.856 2.211 2.906 3.712
8
2.743
3.264
4.278
5.446
100
1.822 2.172 2.854 3.646
9
2.626
3.125
4.098
5.220
500
1.717 2.046 2.689 3.434
10
2.535
3.018
3.959
5.046
1000
1.695 2.019 2.654 3.390
15
2.278
2.713
3.562
4.545
∞
1.645 1.960 2.576 3.291
Tolerance factors for normal distributions
P = .95
n α = .10
.05
.01
.001
n α = .10
.05
.01
.001
2
32.019 37.674 48.430 60.573
20
2.310 2.752 3.615 4.614
3
8.380
9.916 12.861 16.208
25
2.208 2.631 3.457 4.413
4
5.369
6.370
8.299 10.502
30
2.140 2.549 3.350 4.278
5
4.275
5.079
6.634
8.415
40
2.052 2.445 3.213 4.104
6
3.712
4.414
5.775
7.337
50
1.996 2.379 3.126 3.993
7
3.369
4.007
5.248
6.676
75
1.917 2.285 3.002 3.835
8
3.136
3.732
4.891
6.226
100
1.874 2.233 2.934 3.748
9
2.967
3.532
4.631
5.899
500
1.737 2.070 2.721 3.475
10
2.839
3.379
4.433
5.649
1000
1.709 2.036 2.676 3.418
15
2.480
2.954
3.878
4.949
∞
1.645 1.960 2.576 3.291
Tolerance factors for normal distributions
P = .99
n α = .10
.05
.01
.001
n α = .10
.05
.01
.001
2 160.193 188.491 242.300 303.054
20
2.659 3.168 4.161 5.312
3
18.930
22.401
29.055
36.616
25
2.494 2.972 3.904 4.985
4
9.398
11.150
14.527
18.383
30
2.385 2.841 3.733 4.768
5
6.612
7.855
10.260
13.015
40
2.247 2.677 3.518 4.493
6
5.337
6.345
8.301
10.548
50
2.162 2.576 3.385 4.323
7
4.613
5.488
7.187
9.142
75
2.042 2.433 3.197 4.084
8
4.147
4.936
6.468
8.234
100
1.977 2.355 3.096 3.954
9
3.822
4.550
5.966
7.600
500
1.777 2.117 2.783 3.555
10
3.582
4.265
5.594
7.129 1000
1.736 2.068 2.718 3.472
15
2.945
3.507
4.605
5.876
∞
1.645 1.960 2.576 3.291
c
⃝2000 by Chapman & Hall/CRC

7.4
OPERATING CHARACTERISTIC CURVES
7.4.1
One-sample Z test
Consider a one-sample hypothesis test on a population mean of a normal
distribution with known standard deviation σ (see section 10.2). The general
form of the hypothesis test (for each possible alternative hypothesis) is:
H0 : µ = µ0
Ha : µ > µ0,
µ < µ0,
µ ̸= µ0
TS: Z =
¯X −µ0
σ/√n
RR: Z ≥zα,
Z ≤−zα,
|Z| ≥zα/2
Let α be the probability of a Type I error, β the probability of a Type II error,
and µa an alternative mean. For ∆= |µa −µ0|/σ the operating characteristic
curve returns the probability of not rejecting the null hypothesis given µ = µa.
The curves may be used to determine the appropriate sample size for given
values of α, β, and ∆.
7.4.2
Two-sample Z test
Consider a two-sample hypothesis test for comparing population means from
normal distributions with known standard deviations σ1 and σ2 (see section
10.3). The general form of the hypothesis test for testing the equality of means
(for each possible alternative hypothesis) is:
H0 : µ1 −µ2 = 0
Ha : µ1 −µ2 > 0,
µ1 −µ2 < 0,
µ1 −µ2 ̸= 0
TS: Z =
¯X1 −¯X2

σ2
1
n1 + σ2
2
n2
RR: Z ≥zα,
Z ≤−zα,
|Z| ≥zα/2
Let α be the probability of a Type I error and β the probability of a Type II
error. For given values of α, and ∆= |µ1 −µ2|

σ2
1 + σ2
2
the operating characteristic
curve returns the probability of not rejecting the null hypothesis. The curves
may be used to determine an appropriate sample size (n = n1 = n2) for
desired levels of α, β, and ∆.
c
⃝2000 by Chapman & Hall/CRC

Figure 7.3: Operating characteristic curves, various values of n, Z test, two-
sided alternative, α = .05.
Figure 7.4: Operating characteristic curves, various values of n, Z test, two-
sided alternative, α = .01.
c
⃝2000 by Chapman & Hall/CRC

Figure 7.5: Operating characteristic curves, various values of n, Z test, one-
sided alternative, α = .05.
Figure 7.6: Operating characteristic curves, various values of n, Z test, one-
sided alternative, α = .01.
c
⃝2000 by Chapman & Hall/CRC

7.5
MULTIVARIATE NORMAL DISTRIBUTION
Let each {Xi} (for i = 1, . . . , n) be a normal random variable with mean
µi and variance σii. If the covariance of Xi and Xj is σij, then the joint
probability density of the {Xi} is:
f(x) =
1
(2π)n/2
det(Σ)
exp

−1
2(x −µ)TΣ−1(x −µ)

(7.8)
where
(a) x =

x1 x2 . . . xn
T
(b) µ =

µ1 µ2 . . . µn
T
(c) Σ is an n × n matrix with elements σij
The corresponding characteristic function is
φ(t) = exp

iµTt −1
2tTΣt

(7.9)
The form of the characteristic function implies that all cumulants of higher
order than 2 vanish (see Marcienkiewicz’s theorem). Therefore, all moments
of order higher than 2 may be expressed in terms of those of order 1 and 2.
If µ = 0 then the odd moments vanish and the (2n)th moment satisﬁes
E

XiXjXkXl · · ·


	
2n terms

= (2n)!
n!2n {σijσkl · · ·}sym
(7.10)
where the subscript “sym” means the symmetrized form of the product of
the σ’s.
Example 7.44:
For n = 2 we can compute fourth moments
E [X1X2X3X4] =
4!
2! · 22
1
3 (σ12σ34 + σ41σ23 + σ13σ24)

= σ12σ34 + σ41σ23 + σ13σ24
E 
X4
1

=
4!
2! · 22

σ2
11

= 3σ2
11
(7.11)
See C. W. Gardiner Handbook of Stochastic Methods, Springer–Verlag, New
York, 1985, pages 36–37.
c
⃝2000 by Chapman & Hall/CRC

7.6
DISTRIBUTION OF THE CORRELATION COEFFICIENT
FOR A BIVARIATE NORMAL
The bivariate normal probability function is given by
f(x, y) =
1
2πσxσy

1 −ρ2 exp

−
1
2(1 −ρ2)
×
%x −µx
σx
2
−2ρ
x −µx
σx
 y −µy
σy

+
y −µy
σy
2&
(7.12)
where µx = mean of x
µy = mean of y
σx = standard deviation of x
σy = standard deviation of y
ρ = correlation coeﬃcient between x and y
Given a sample {(x1, y1), . . . , (xn, yn)} of size n, the sample correlation coef-
ﬁcient, an estimate of ρ, is
r =
n
i=1
(xi −x) (yi −y)
+ n
i=1
(xi −x)2
  n
i=1
(yi −y)2

(7.13)
where x = (n
i=1 xi) /n and y = (n
i=1 yi) /n. For given n, the distribution
of r is independent of {µx, µy, σx, σy}, but depends on ρ. For −1 ≤ρ ≤1,
the density function for r is fn(r; ρ):
fn(r; ρ) = 1
π (n −2)(1 −r2)(n−4)/2 
1 −ρ2(n−1)/2  ∞
0
dz
(cosh z −ρr)n−1
= 1
π (n −2)(1 −r2)(n−4)/2 
1 −ρ2(n−1)/2
-π
2
Γ(n −1)
Γ(n −1/2)
× (1 −ρr)−(n−3/2)
2F1
1
2, 1
2; 2n −1
2
; ρr + 1
2

(7.14)
c
⃝2000 by Chapman & Hall/CRC

where Γ(x) is the gamma function and 2F1 is the hypergeometric function
deﬁned in Chapter 18 (see pages 515 and 520). The moments are given by
µr = ρ −ρ(1 −ρ2)
(n + 1)
σ2
r = (1 −ρ2)2
n + 1

1 +
11ρ2
2(n + 1) + . . .

γ1 =
6ρ
√n + 1

1 + 77ρ2 −30
12(n + 1) + . . .

γ2 =
6
n + 1

12ρ2 −1

+ . . .
(7.15)
7.6.1
Normal approximation
If r is the sample correlation coeﬃcient (deﬁned in equation (7.13)), the ran-
dom variable
Z = tanh−1 r = 1
2 ln 1 + r
1 −r
(7.16)
is approximately normally distributed with parameters
µZ = 1
2 ln
1 + ρ
1 −ρ

= tanh−1 ρ and σ2
Z =
1
n −3
(7.17)
7.6.2
Zero correlation coeﬃcient for bivariate normal
In the special case where ρ = 0, the density function of r becomes
fn(r; 0) =
1
√π
Γ ((n −1)/2)
Γ ((n −2)/2)

1 −r2(n−4)/2
(7.18)
Under the transformation
r2 =
t2
t2 + ν
(7.19)
fn(r; 0), as given by equation (7.18), has a t-distribution with ν = n−1 degrees
of freedom. The following table gives percentage points of the distribution of
the correlation coeﬃcient when ρ = 0.
c
⃝2000 by Chapman & Hall/CRC

Percentage points of the correlation coeﬃcient, when ρ = 0
Prob [r ≤tabulated value] = 1 −α
α =
0.05
0.025
0.01
0.005
0.0025
0.0005
2α =
0.1
0.05
0.02
0.01
0.005
0.001
ν = 1
0.988
0.997
0.93507
0.93877
0.9469
0.96
2
0.900
0.950
0.980
0.990
0.995
0.999
3
0.805
0.878
0.934
0.959
0.974
0.991
4
0.729
0.811
0.882
0.917
0.942
0.974
5
0.669
0.754
0.833
0.875
0.906
0.951
6
0.621
0.707
0.789
0.834
0.870
0.925
7
0.582
0.666
0.750
0.798
0.836
0.898
8
0.549
0.632
0.715
0.765
0.805
0.872
9
0.521
0.602
0.685
0.735
0.776
0.847
10
0.497
0.576
0.658
0.708
0.750
0.823
11
0.476
0.553
0.634
0.684
0.726
0.801
12
0.458
0.532
0.612
0.661
0.703
0.780
13
0.441
0.514
0.592
0.641
0.683
0.760
14
0.426
0.497
0.574
0.623
0.664
0.742
15
0.412
0.482
0.558
0.606
0.647
0.725
16
0.400
0.468
0.543
0.590
0.631
0.708
17
0.389
0.456
0.529
0.575
0.616
0.693
18
0.378
0.444
0.516
0.561
0.602
0.679
19
0.369
0.433
0.503
0.549
0.589
0.665
20
0.360
0.423
0.492
0.537
0.576
0.652
25
0.323
0.381
0.445
0.487
0.524
0.597
30
0.296
0.349
0.409
0.449
0.484
0.554
35
0.275
0.325
0.381
0.418
0.452
0.519
40
0.257
0.304
0.358
0.393
0.425
0.490
45
0.243
0.288
0.338
0.372
0.403
0.465
50
0.231
0.273
0.322
0.354
0.384
0.443
60
0.211
0.250
0.295
0.325
0.352
0.408
70
0.195
0.232
0.274
0.302
0.327
0.380
80
0.183
0.217
0.257
0.283
0.307
0.357
90
0.173
0.205
0.242
0.267
0.290
0.338
100
0.164
0.195
0.230
0.254
0.276
0.321
Use the α value for a single-tail test. For a two-tail test, use the 2α value. If
r is computed from n paired observations, enter the table with ν = n−2. For
partial correlations, enter the table with ν = n−2−k, where k is the number
of variables held constant.
c
⃝2000 by Chapman & Hall/CRC

7.7
CIRCULAR NORMAL PROBABILITIES
The joint probability density of two independent and normally distributed
random variables X and Y (each of mean zero and variance σ2) is
f(x, y) =
1
2πσ2 exp

−1
2σ2

x2 + y2
(7.20)
The following table gives the probability that a sample value of X and Y is
obtained inside a circle C of radius R at a distance d from the origin:
F
 d
σ , R
σ

=
1
2πσ2

C
exp

−1
2σ2

x2 + y2
dx dy
(7.21)
Circular normal probabilities
R/σ
d/σ
0.2
0.4
0.6
0.8
1.0
1.5
2.0
2.5
3.0
0.2
0.020
0.077
0.164
0.273
0.392
0.674
0.863
0.955
0.989
0.3
0.019
0.075
0.162
0.269
0.387
0.668
0.859
0.953
0.988
0.4
0.019
0.074
0.158
0.264
0.380
0.659
0.852
0.950
0.987
0.5
0.018
0.071
0.153
0.256
0.370
0.647
0.843
0.945
0.985
0.6
0.017
0.068
0.147
0.246
0.357
0.631
0.831
0.938
0.982
0.7
0.017
0.065
0.140
0.235
0.342
0.612
0.816
0.930
0.979
0.8
0.016
0.061
0.132
0.222
0.326
0.591
0.799
0.920
0.975
0.9
0.014
0.057
0.123
0.209
0.307
0.566
0.779
0.909
0.970
1.0
0.013
0.052
0.114
0.194
0.288
0.540
0.756
0.895
0.964
1.2
0.012
0.048
0.104
0.179
0.267
0.512
0.731
0.879
0.956
1.4
0.0097 0.038
0.085
0.148
0.225
0.451
0.674
0.841
0.937
1.6
0.0075 0.030
0.067
0.119
0.184
0.388
0.610
0.795
0.912
1.8
0.0056 0.022
0.051
0.092
0.145
0.325
0.541
0.739
0.878
2.0
0.0040 0.016
0.037
0.069
0.111
0.264
0.469
0.676
0.837
2.2
0.0027 0.011
0.026
0.050
0.082
0.209
0.396
0.606
0.786
2.4
0.0018 0.0075 0.018
0.034
0.059
0.161
0.327
0.532
0.726
2.6
0.0011 0.0048 0.012
0.023
0.040
0.120
0.262
0.456
0.659
2.8
0.0007 0.0030 0.0074 0.015
0.027
0.086
0.204
0.381
0.586
3.0
0.0004 0.0018 0.0045 0.0094 0.017
0.060
0.154
0.311
0.510
3.2
0.0002 0.0010 0.0027 0.0057 0.011
0.041
0.113
0.246
0.433
3.4
0.0001 0.0006 0.0015 0.0033 0.0065 0.027
0.080
0.189
0.358
3.6
0.0001 0.0003 0.0008 0.0018 0.0038 0.017
0.055
0.141
0.288
3.8
0.0002 0.0004 0.0010 0.0021 0.010
0.037
0.102
0.225
4.0
0.0001 0.0002 0.0005 0.0011 0.006
0.024
0.072
0.171
4.2
0.0001 0.0003 0.0019 0.0088 0.0320 0.090
4.4
0.0001 0.0001 0.0010 0.0051 0.0200 0.062
4.6
0.0001 0.0005 0.0029 0.0120 0.041
4.8
0.0003 0.0015 0.0073 0.027
5.0
0.0001 0.0008 0.0041 0.017
5.2
0.0001 0.0004 0.0023 0.0100
5.4
0.0002 0.0012 0.0058
5.6
0.0001 0.0006 0.0033
5.8
0.0003 0.0018
c
⃝2000 by Chapman & Hall/CRC

7.8
CIRCULAR ERROR PROBABILITIES
The joint probability density of two independent and normally distributed
random variables X and Y , each of mean zero and having variances σ2
x and
σ2
y, is
f(x, y) =
1
2πσxσy
exp
%
−1
2
( x
σx
2
+
 y
σy
2&
(7.22)
The probability that a sample value of X and Y will lie within a circle with
center at the origin and radius Kσx is given by
P(K, σx, σy) =

R
f(x, y) dx dy
(7.23)
where R is the region

x2 + y2 < Kσx.
For various values of K and c = σx/σy (for convenience we assume that
σy ≤σx) the following table gives the value of P.
Circular error probabilities
K
c = 0.0
0.2
0.4
0.6
0.8
1.0
0.1
0.0797
0.0242
0.0124
0.0083
0.0062
0.0050
0.2
0.1585
0.0885
0.0482
0.0327
0.0247
0.0198
0.3
0.2358
0.1739
0.1039
0.0719
0.0547
0.0440
0.4
0.3108
0.2635
0.1742
0.1238
0.0951
0.0769
0.5
0.3829
0.3482
0.2533
0.1857
0.1444
0.1175
0.6
0.4515
0.4256
0.3357
0.2548
0.2010
0.1647
0.7
0.5161
0.4961
0.4171
0.3280
0.2629
0.2173
0.8
0.5763
0.5604
0.4942
0.4026
0.3283
0.2739
0.9
0.6319
0.6191
0.5652
0.4759
0.3953
0.3330
1.0
0.6827
0.6724
0.6291
0.5461
0.4621
0.3935
1.2
0.7699
0.7630
0.7359
0.6714
0.5893
0.5132
1.4
0.8385
0.8340
0.8170
0.7721
0.7008
0.6247
1.6
0.8904
0.8875
0.8769
0.8478
0.7917
0.7220
1.8
0.9281
0.9263
0.9197
0.9019
0.8613
0.8021
2.0
0.9545
0.9534
0.9494
0.9388
0.9116
0.8647
2.2
0.9722
0.9715
0.9692
0.9631
0.9459
0.9111
2.4
0.9836
0.9832
0.9819
0.9785
0.9683
0.9439
2.6
0.9907
0.9905
0.9897
0.9879
0.9821
0.9660
2.8
0.9949
0.9948
0.9944
0.9934
0.9903
0.9802
3.0
0.9973
0.9972
0.9970
0.9965
0.9949
0.9889
3.2
0.9986
0.9986
0.9985
0.9982
0.9974
0.9940
3.4
0.9993
0.9993
0.9993
0.9991
0.9988
0.9969
3.6
0.9997
0.9997
0.9997
0.9996
0.9994
0.9985
3.8
0.9999
0.9999
0.9998
0.9998
0.9997
0.9993
4.0
0.9999
0.9999
0.9999
0.9999
0.9999
0.9997
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 8
Estimation
Contents
8.1
Deﬁnitions
8.2
Cram´er–Rao inequality
8.3
Theorems
8.4
The method of moments
8.5
The likelihood function
8.6
The method of maximum likelihood
8.7
Invariance property of MLEs
8.8
Diﬀerent estimators
8.9
Estimators for small samples
8.10
Estimators for large samples
A nonconstant function of a set of random variables is a statistic.
It is a
function of observable random variables, which does not contain any unknown
parameters. A statistic is itself an observable random variable.
Let θ be a parameter appearing in the density function for the random vari-
able X. Let g be a function that returns an approximate value .θ of θ from
a given sample {x1, . . . , xn}. Then .θ = g(x1, x2, . . . , xn) may be considered
a single observation of the random variable .Θ = g(X1, X2, . . . , Xn).
The
random variable .Θ is an estimator for the parameter θ.
8.1
DEFINITIONS
(1) .Θ is an unbiased estimator for θ if E [.Θ] = θ.
(2) The bias of the estimator .Θ is B [.Θ] = E [.Θ] −θ.
(3) The mean square error of .Θ is
MSE [.Θ] = E

(.Θ −θ)2
= Var [.Θ] + B [.Θ]2 .
(4) The error of estimation is ϵ = |.Θ −θ|.
(5) Let .Θ1 and .Θ2 be unbiased estimators for θ.
(a) If Var [.Θ1] < Var [.Θ2] then the estimator .Θ1 is relatively more
eﬃcient than the estimator .Θ2.
c
⃝2000 by Chapman & Hall/CRC

(b) The eﬃciency of .Θ2 relative to .Θ1 is
Eﬃciency = Var [.Θ1]
Var [.Θ2].
(6) .Θ is a consistent estimator for θ if for every ϵ > 0,
lim
n→∞Prob [|.Θ −θ| ≤ϵ] = 1
or, equivalently
lim
n→∞Prob [|.Θ −θ| > ϵ] = 0.
(7) .Θ is a suﬃcient estimator for θ if for each value of .Θ the conditional
distribution of X1, X2, . . . , Xn given .Θ = θ0 is independent of θ.
(8) Let .Θ be an estimator for the parameter θ and suppose .Θ has sampling
distribution g(.Θ). Then .Θ is a complete statistic if for all θ, E [h (.Θ)] =
0 implies h (.Θ) = 0 for all functions h (.Θ).
(9) An estimator .Θ for θ is a minimum variance unbiased estimator
(MVUE) if it is unbiased and has the smallest possible variance.
Example 8.45:
To determine if X
2 is an unbiased estimator of µ2, consider the
following expected value:
E
"
X
2#
= E
%
1
n
n

i=1
Xi
2&
= 1
n2 E


n

i=1
X2
i +
n

i=1,j=1,i̸=j
XiXj


= 1
n2

n(σ2 + µ2) + (n2 −n)µ2

= µ2 + σ2
n
> µ2
(8.1)
This shows X
2 is a biased estimator of µ2.
8.2
CRAM´ER–RAO INEQUALITY
Let {X1, X2, . . . , Xn} be a random sample from a population with probability
density function f(x). Let .Θ be an unbiased estimator for θ. Under very
general conditions it can be shown that
Var [.Θ] ≥
1
n · E

∂ln f(X)
∂θ
2 =
1
−n · E
"
∂2 ln f(X)
∂θ2
#.
(8.2)
If equality holds then .Θ is a minimum variance unbiased estimator (MVUE)
for θ.
c
⃝2000 by Chapman & Hall/CRC

Example 8.46:
The probability density function for a normal random variable with
unknown mean θ and known variance σ2 is f(x; θ) =
1
√
2πσ
exp

−(x −θ)2
2σ2

. Use the
Cram´er–Rao inequality to show the minimum variance of any unbiased estimator, .Θ, for
θ is at least σ2/n.
Solution:
(S1)
∂
∂θ ln f(X; θ) = (X −θ)
σ2
(S2)
∂ln f(X; θ)
∂θ
2
= (X −θ)2
σ4
(S3) E
(X −θ)2
σ4

=
 ∞
−∞
(x −θ)2
σ4

1
√
2πσ
e−(x−θ)2/2σ2
dx = 1
σ2
(S4) Var [.Θ] ≥
1
n 1
σ2
= σ2
n
8.3
THEOREMS
(1) .Θ is a consistent estimator for θ if
(a) .Θ is unbiased, and
(b)
lim
n→∞Var [.Θ] = 0.
(2) .Θ is a suﬃcient estimator for the parameter θ if the joint distribution
of {X1, X2, . . . , Xn} can be factored into
f(x1, x2, . . . , xn; θ) = g(.Θ, θ) · h(x1, x2, . . . , xn)
(8.3)
where g(.Θ, θ) depends only on the estimate .θ and the parameter θ, and
h(x1, x2, . . . , xn) does not depend on the parameter θ.
(3) Unbiased estimators:
(a) An unbiased estimator may not exist.
(b) An unbiased estimator is not unique.
(c) An unbiased estimator may be meaningless.
(d) An unbiased estimator is not necessarily consistent.
(4) Consistent estimators:
(a) A consistent estimator is not unique.
(b) A consistent estimator may be meaningless.
(c) A consistent estimator is not necessarily unbiased.
(5) Maximum likelihood estimators (MLE):
(a) A MLE need not be consistent.
(b) A MLE may not be unbiased.
(c) A MLE is not unique.
c
⃝2000 by Chapman & Hall/CRC

(d) If a single suﬃcient statistic T exists for the parameter θ, the MLE
of θ must be a function of T.
(e) Let .Θ be a MLE of θ. If τ(·) is a function with a single-valued
inverse, then a MLE of τ(θ) is τ(.Θ).
(6) Method of moments (MOM) estimators:
(a) MOM estimators are not uniquely deﬁned.
(b) MOM estimators may not be functions of suﬃcient or complete
statistics.
(7) A single suﬃcient estimator may not exist.
8.4
THE METHOD OF MOMENTS
The moment estimators are the solutions to the systems of equations
µ′
r = E [Xr] = 1
n
n

i=1
xr
i = m′
r,
r = 1, 2, . . . , k
where k is the number of parameters.
Example 8.47:
Suppose X1, X2, . . . , Xn is a random sample from a population having
a gamma distribution with parameters α and β. Use the method of moments to obtain
estimators for the parameters α and β.
Solution:
(S1) The system of equations to solve: µ′
1 = m′
1 ;
µ′
2 = m′
2
(S2) µ′
1 = E [X] = αβ ;
µ′
2 = E 
X2
= α(α + 1)β2
(S3) Solve αβ = m′
1 and α(α + 1)β2 = m′
2 for α and β.
(S4) .α =
(m′
1)2
m′
2 −(m′
1)2 ;
.β = m′
2 −(m′
1)2
m′
1
(S5) Given m′
1 = x = 1
n
n

i=1
xi and m′
2 = 1
n
n

i=1
x2
i , then
.α =
nx2
n
i=1
(xi −x)2
and .β =
n
i=1
(xi −x)2
nx
8.5
THE LIKELIHOOD FUNCTION
Let x1, x2, . . . , xn be the values of a random sample from a population char-
acterized by the parameters θ1, θ2, . . . , θr. The likelihood function of the
sample is
(1) the joint probability mass function evaluated at (x1, x2, . . . , xn) if (X1,
X2, . . . , Xn) are discrete,
L(θ1, θ2, . . . , θr) = p(x1, x2, . . . , xn; θ1, θ2, . . . , θr)
(8.4)
c
⃝2000 by Chapman & Hall/CRC

(2) the joint probability density function evaluated at (x1, x2, . . . , xn) if
(X1, X2, . . . , Xn) are continuous.
L(θ1, θ2, . . . , θr) = f(x1, x2, . . . , xn; θ1, θ2, . . . , θr)
(8.5)
8.6
THE METHOD OF MAXIMUM LIKELIHOOD
The maximum likelihood estimators (MLEs) are those values of the parame-
ters that maximize the likelihood function of the sample: L(θ1, . . . , θr). In
practice, it is often easier to maximize ln L(θ1, . . . , θr). This is equivalent to
maximizing the likelihood function, L(θ1, . . . , θr), since ln L(θ1, . . . , θr) is a
monotonic function of L(θ1, . . . , θr).
Example 8.48:
Suppose X1, X2, . . . , Xn is a random sample from a population having
a Poisson distribution with parameter λ. Find the maximum likelihood estimator for the
parameter λ.
Solution:
(S1) The probability mass function for a Poisson random variable is
f(x; λ) = e−λλx
x!
(S2) We compute
L(θ) =
e−λλx1
x1!
 e−λλx2
x2!

· · ·
e−λλxn
xn!

= e−nλλx1+x2+···+xn
x1!x2! · · · xn!
ln L(θ) = −nλ + (x1 + x2 + · · · + xn) ln λ + ln (x1!x2! · · · xn!)
(8.6)
(S3) ∂ln L(λ)
∂λ
= −n + x1 + x2 + · · · + xn
λ
= 0
(S4) Solving for λ:
.λ = x1 + x2 + · · · + xn
n
= x is the MLE for λ.
8.7
INVARIANCE PROPERTY OF MAXIMUM LIKELIHOOD
ESTIMATORS
Let .Θ1, .Θ2, . . . , .Θr be the maximum likelihood estimators for θ1, θ2, . . . , θr and
let h(θ1, θ2, . . . , θr) be a function of θ1, θ2, . . . , θr. The maximum likelihood es-
timator of the parameter h(θ1, θ2, . . . , θr) is .h(θ1, θ2, . . . , θr) = h(.Θ1, .Θ2, . . . , .Θr).
8.8
DIFFERENT ESTIMATORS
Assume {x1, x2, . . . , xn} is a set of observations. Let UMV stand for uniformly
minimum variance unbiased and let MLE stand for maximum likelihood esti-
mator.
(1) Normal distribution: N(µ, σ2)
(a) When σ is known:
1.  xi is necessary, suﬃcient, and complete.
c
⃝2000 by Chapman & Hall/CRC

2. Point estimate for µ: .µ = x = 1
n

xi is UMV, MLE.
(b) When µ is known:
1. (xi −µ)2 is necessary, suﬃcient, and complete.
2. Point estimate for σ2: /
σ2 =
(xi −µ)2
n
is UMV, MLE.
(c) When µ and σ are unknown:
1. { xi, (xi −x)2} are necessary, suﬃcient, and complete.
2. Point estimate for µ: .µ = x = 1
n

xi is UMV, MLE.
3. Point estimate for σ2: /
σ2 =
(xi −x)2
n
is MLE.
4. Point estimate for σ2: /
σ2 =
(xi −x)2
n −1
is UMV.
5. Point estimate for σ: .σ = Γ [(n −1)/2]
√
2Γ(n/2)
-(xi −x)2
n −1
is UMV.
(2) Poisson distribution with parameter λ:
(a)  xi is necessary, suﬃcient, and complete.
(b) Point estimate for λ: .λ = 1
n

xi is UMV, MLE.
(3) Uniform distribution on an interval:
(a) Interval is [0, θ]
1. max(xi) is necessary, suﬃcient, and complete.
2. Point estimate for θ: .Θ = max(xi) is MLE.
3. Point estimate for θ: .Θ = n + 1
n
max(xi) is UMV.
(b) Interval is [α, β]
1. {min(xi), max(xi)} are necessary, suﬃcient, and complete.
2. Point estimate for α: .α = n min(xi) −max(xi)
n −1
is UMV.
3. Point estimate for α: .α = min(xi) is MLE.
4. Point estimate for α+β
2 :
0
α + β
2
= min(xi) + max(xi)
2
is UMV.
(c) Interval is

θ −1
2, θ + 1
2

1. {min(xi), max(xi)} are necessary and suﬃcient.
2. Point estimate for θ: .Θ = min(xi) + max(xi)
2
is MLE.
c
⃝2000 by Chapman & Hall/CRC

8.9
ESTIMATORS FOR MEAN AND STANDARD DEVIATION
IN SMALL SAMPLES
In all cases below, the variance of an estimate must be multiplied by the the
true variance of the sample, σ2.
Diﬀerent estimators for the mean
Median
Midrange
Average of best two
1
n−2
n−1
i=2 xi

n
Var
Eﬀ
Var
Eﬀ
Statistic
Var
Eﬀ
Var
Eﬀ
2 .500 1.000 .500 1.000
1
2(x1 + x2)
.500 1.000
3 .449
.743 .362
.920
1
2(x1 + x3)
.362
.920
.449
.743
4 .298
.838 .298
.838
1
2(x2 + x3)
.298
.838
.298
.838
5 .287
.697 .261
.767
1
2(x2 + x4)
.231
.867
.227
.881
10 .138
.723 .186
.539
1
2(x3 + x8)
.119
.840
.105
.949
15 .102
.656 .158
.422
1
2(x4 + x12)
.081
.825
.069
.969
20 .073
.681 .143
.350
1
2(x6 + x15)
.061
.824
.051
.978
∞
1.57
n
.637
.000
1
2(P25 + P75)
1.24
n
.808
1.000
Estimating standard deviation σ from the sample range w
n
Estimator
Variance
Eﬃciency
2
.886w
.571
1.000
3
.591w
.275
.992
4
.486w
.183
.975
5
.430w
.138
.955
6
.395w
.112
.933
7
.370w
.095
.911
8
.351w
.083
.890
9
.337w
.074
.869
10
.325w
.067
.850
15
.288w
.047
.766
20
.268w
.038
.700
Best linear estimate of the standard deviation σ
n
Estimator
Eﬃciency
2
.8862(x2 −x1)
1.000
3
.5908(x3 −x1)
.992
4
.4539(x4 −x1) + .1102(x3 −x2)
.989
5
.3724(x5 −x1) + .1352(x3 −x2)
.988
6
.3175(x6 −x1) + .1386(x5 −x2) + .0432(x4 −x3)
.988
7
.2778(x7 −x1) + .1351(x6 −x2) + .0625(x5 −x3)
.989
c
⃝2000 by Chapman & Hall/CRC

8.10
ESTIMATORS FOR MEAN AND STANDARD DEVIATION
IN LARGE SAMPLES
Percentile estimates may be used to estimate both the mean and the standard
deviation.
Estimators for the mean
Number of terms
Estimator using percentiles
Eﬃciency
1
P50
.64
2
1/2 (P25 + P75)
.81
3
1/3 (P17 + P50 + P83)
.88
4
1/4 (P12.5 + P37.5 + P62.5 + P87.5)
.91
5
1/5 (P10 + P30 + P50 + P70 + P90)
.93
Estimators for the standard deviation
Number of terms
Estimator using percentiles
Eﬃciency
2
.3388 (P93 −P7)
.65
4
.1714 (P97 + P85 −P15 −P3)
.80
6
.1180 (P98 + P91 + P80 −P20 −P9 −P2)
.87
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 9
Conﬁdence Intervals
Contents
9.1
Deﬁnitions
9.2
Common critical values
9.3
Sample size calculations
9.4
Summary of common conﬁdence intervals
9.5
Conﬁdence intervals: one sample
9.5.1
Mean of normal population, known variance
9.5.2
Mean of normal population, unknown var
9.5.3
Variance of normal population
9.5.4
Success in binomial experiments
9.5.5
Conﬁdence interval for percentiles
9.5.6
Conﬁdence interval for medians
9.5.7
Conﬁdence interval for Poisson distribution
9.5.8
Conﬁdence interval for binomial distribution
9.6
Conﬁdence intervals: two samples
9.6.1
Diﬀerence in means, known variances
9.6.2
Diﬀerence in means, equal unknown var
9.6.3
Diﬀerence in means, unequal unknown var
9.6.4
Diﬀerence in means, paired observations
9.6.5
Ratio of variances
9.6.6
Diﬀerence in success probabilities
9.6.7
Diﬀerence in medians
9.7
Finite population correction factor
9.1
DEFINITIONS
A simple point estimate .θ of a parameter θ serves as a best guess for the
value of θ, but conveys no sense of conﬁdence in the estimate. A conﬁdence
interval I, based on .θ, is used to make statements about θ when the sample
size, the underlying distribution of θ, and the conﬁdence coeﬃcient 1 −α
are known. We make statements of the form:
The probability that θ is in a speciﬁed interval is 1 −α.
(9.1)
c
⃝2000 by Chapman & Hall/CRC

To construct a conﬁdence interval for a parameter θ, the conﬁdence coeﬃcient
must be speciﬁed. The usual procedure is to specify the conﬁdence coeﬃcient
1 −α and then determine the conﬁdence interval. A typical value is α = 0.05
(also written as α = 5%); so that 1 −α = 0.95 (or 95% conﬁdence).
The conﬁdence interval may be denoted (θlow, θhigh). There are many ways
to specify θlow and θhigh, depending on the parameter θ and the underlying
distribution. The bounds on the conﬁdence interval are usually deﬁned to
satisfy
The probability that θ < θlow is α/2: Prob [θ < θlow] = α/2
The probability that θ > θhigh is α/2: Prob [θ > θhigh] = α/2
(9.2)
so that Prob [θlow ≤θ ≤θhigh] = 1 −α.
It is also possible to construct one-sided conﬁdence intervals. For these,
(1) θlow = −∞and Prob [θ > θhigh] = α or Prob [θ ≤θhigh] = 1 −α
(one-sided, lower-tailed conﬁdence interval), or
(2) θhigh = ∞and Prob [θ < θlow] = α or Prob [θ ≥θlow] = 1 −α
(one-sided, upper-tailed conﬁdence interval).
Notes:
(1) When the sample size is at least 5% of the total population, a ﬁnite
population correction factor may be used to modify a conﬁdence interval.
See section (9.7).
(2) If the test statistic is signiﬁcant in an ANOVA, conﬁdence intervals may
then be used to determine which pairs of means diﬀer.
9.2
COMMON CRITICAL VALUES
The formulas for common conﬁdence intervals usually involve critical values
from the normal distribution, the t distribution, or the chi–square distribution;
see Tables 9.3 and 9.4. More extensive critical value tables for the normal
distribution are given on page 175, for the t distribution on page 156, for the
chi–square distribution on page 6.4.4, and for the F distribution on page 131.
9.3
SAMPLE SIZE CALCULATIONS
In order to construct a conﬁdence interval of speciﬁed width, a priori pa-
rameter estimates and a bound on the error of estimation may be used to
determine the necessary sample size. For a 100(1 −α)% conﬁdence interval,
let E = error of estimation (half the width of the conﬁdence interval). Table
9.2 presents some common sample size calculations.
Example 9.49:
A researcher would like to estimate the probability of a success,
p, in a binomial experiment.
How large a sample is necessary in order to estimate
c
⃝2000 by Chapman & Hall/CRC

α
Distribution
0.10
0.05
0.01
0.001
0.0001
t distribution
tα/2,10
1.8125
2.2281
3.1693
4.5869
6.2111
tα/2,100
1.6602
1.9840
2.6259
3.3905
4.0533
tα/2,1000
1.6464
1.9623
2.5808
3.3003
3.9063
Normal distribution
zα/2
1.6449
1.9600
2.5758
3.2905
3.8906
χ2 distribution
χ2
1−α/2,10
3.9403
3.2470
2.1559
1.2650
0.7660
χ2
α/2,10
18.3070
20.4832
25.1882
31.4198
37.3107
χ2
1−α/2,100
77.9295
74.2219
67.3276
59.8957
54.1129
χ2
α/2,100
124.3421
129.5612
140.1695
153.1670
164.6591
χ2
1−α/2,1000
927.5944
914.2572
888.5635
859.3615
835.3493
χ2
α/2,1000
1074.6790
1089.5310
1118.9480
1153.7380
1183.4920
0.90
0.95
0.99
0.999
0.9999
1 −α
Table 9.1: Common critical values used with conﬁdence intervals
Parameter
Estimate
Sample size
µ
x
n =
zα/2 · σ
E
2
(1)
p
.p
n = (zα/2)2 · pq
E2
(2)
µ2 −µ2
x1 −x2
n1 = n2 = (zα/2)2(σ2
1 + σ2
2)
E2
(3)
p1 −p2
.p1 −.p2
n1 = n2 = (zα/2)2(p1q1 + p2q2)
E2
(4)
Table 9.2: Common sample size calculations
this proportion to within .05 with 99% conﬁdence, i.e., ﬁnd a value of n such that
Prob [|ˆp −p| ≤0.05] ≥0.99.
Solution:
(S1) Since no a priori estimate of p is available, use p = .5. The bound on the error
of estimation is E = .05 and 1 −α = .99.
c
⃝2000 by Chapman & Hall/CRC

(S2) From Table 9.2, n = z2
.005 · pq
E2
= (2.5758)(.5)(.5)
.052
= 663.47
(S3) This formula produces a conservative value for the necessary sample size (since
no a priori estimate of p is known). A sample size of at least 664 should be used.
9.4
SUMMARY OF COMMON CONFIDENCE INTERVALS
Table 9.3 presents a summary of common conﬁdence intervals for one sample,
Table 9.4 is for two samples. For each population parameter, the assumptions
and formula for a 100(1 −α)% conﬁdence interval are given.
Parameter
Assumptions (reference)
100(1 −α)% Conﬁdence interval
µ
n large, σ2 known, or
normality, σ2 known
(§9.5.1)
x ± zα/2 · σ
√n
(1)
µ
normality, σ2 unknown
(§9.5.2)
x ± tα/2,n−1 ·
s
√n
(2)
σ2
normality (§9.5.3)

(n −1)s2
χ2
α/2,n−1
, (n −1)s2
χ2
1−α/2,n−1

(3)
p
binomial experiment,
n large (§9.5.4)
.p ± zα/2 ·
-
.p(1 −.p)
n
(4)
Table 9.3: Summary of common conﬁdence intervals: one sample
9.5
CONFIDENCE INTERVALS: ONE SAMPLE
Let x1, x2, . . . , xn be a random sample of size n.
9.5.1
Conﬁdence interval for mean of normal population, known
variance
Find a 100(1−α)% conﬁdence interval for the mean µ of a normal population
with known variance σ2, or
Find a 100(1 −α)% conﬁdence interval for the mean µ of a population with
known variance σ2 where n is large.
(a) Compute the sample mean x.
(b) Determine the critical value zα/2 such that Φ(zα/2) = 1 −α/2, where
Φ(z) is the standard normal cumulative distribution function. That is,
zα/2 is deﬁned so that Prob

Z ≥zα/2

= α/2.
(c) Compute the constant k = σ zα/2/√n.
(Table 9.5 on page 200 has
common values of zα/2/√n.)
(d) A 100(1 −α)% conﬁdence interval for µ is given by (x −k, x + k).
c
⃝2000 by Chapman & Hall/CRC

Parameter
Assumptions (reference)
100(1 −α)% Conﬁdence interval
µ1 −µ2
normality, independence,
σ2
1, σ2
2 known
or n1, n2 large,
independence,
σ2
1, σ2
2 known, (§9.6.1)
(x1 −x2) ± zα/2 ·
+
σ2
1
n1 + σ2
2
n2
(1)
µ1 −µ2
normality, independence,
σ2
1 = σ2
2 unknown (§9.6.2)
(x1 −x2) ±
t α
2 ,n1+n2−2 · sp
-
1
n1 + 1
n2
s2
p = (n1 −1)s2
1 + (n2 −1)s2
2
n1 + n2 −2
(2)
µ1 −µ2
normality, independence,
σ2
1 ̸= σ2
2 unknown (§9.6.3)
(x1 −x2) ± tα/2,ν ·
+
s2
1
n1 + s2
2
n2
ν ≈

s2
1
n1 +
s2
2
n2
2
(s2
1/n1)2
n1−1
+
(s2
2/n2)2
n2−1
(3)
µ1 −µ2
normality, n pairs,
dependence (§9.6.4)
d ± tα/2,n−1 · sd
√n
(4)
σ2
1/σ2
2
normality, independence
(§9.6.5)

s2
1
s2
2
·
1
F α
2 ,n1−1,n2−1
,
s2
1
s2
2
·
1
F1−α
2 ,n1−1,n2−1

(5)
p1 −p2
binomial experiments,
n1, n2 large,
independence (§9.6.6)
(.p1 −.p2) ±
zα/2 ·
-
.p1(1 −.p1)
n1
+ .p2(1 −.p2)
n2
(6)
Table 9.4: Summary of common conﬁdence intervals: two samples
9.5.2
Conﬁdence interval for mean of normal population, unknown
variance
Find a 100(1−α)% conﬁdence interval for the mean µ of a normal population
with unknown variance σ2.
(a) Compute the sample mean x and the sample standard deviation s.
(b) Determine the critical value tα/2,n−1 such that F(tα/2,n−1) = 1 −α/2,
where F(t) is the cumulative distribution function for a t distribution
with n −1 degrees of freedom.
That is, tα/2,n−1 is deﬁned so that
Prob

T ≥tα/2,n−1

= α/2.
c
⃝2000 by Chapman & Hall/CRC

zα/2/√n when
n
α = 0.05
α = 0.01
2
8.99
45.0
3
2.48
5.73
4
1.59
2.92
5
1.24
2.06
6
1.05
1.65
7
0.925
1.40
8
0.836
1.24
9
0.769
1.12
10
0.715
1.03
zα/2/√n when
n
α = 0.05
α = 0.01
15
0.554
0.769
20
0.468
0.640
25
0.413
0.559
30
0.373
0.503
40
0.320
0.428
50
0.284
0.379
100
0.198
0.263
200
0.139
0.184
500
0.088
0.116
Table 9.5: Common values of zα/2/√n
(c) Compute the constant k = tα/2,n−1 s/√n.
(d) A 100(1 −α)% conﬁdence interval for µ is given by (x −k, x + k).
Example 9.50:
A software company conducted a survey on the size of a typical word
processing ﬁle. For n = 23 randomly selected ﬁles, x = 4822 kb and s = 127. Find a
95% conﬁdence interval for the true mean size of word processing ﬁles.
Solution:
(S1) The underlying population, the size of word processing ﬁles, is assumed to be
normal. The conﬁdence interval for µ is based on a t distribution.
(S2) 1 −α = .95 ; α = .05 ; α/2 = .025 ; tα/2,n−1 = t.025,22 = 2.0739
(S3) k = (2.0739)(127)/
√
23 = 54.92
(S4) A 99% conﬁdence interval for µ: (x −k, x + k) = (4767.08, 4876.92)
9.5.3
Conﬁdence interval for variance of normal population
Find a 100(1 −α)% conﬁdence interval for the variance σ2 of a normal pop-
ulation.
(a) Compute the sample variance s2.
(b) Determine the critical values χ2
α/2,n−1 and χ2
1−α/2,n−1 such that
Prob
"
χ2 ≥χ2
α/2,n−1
#
= Prob
"
χ2 ≤χ2
1−α/2,n−1
#
= α/2.
(c) Compute the constants k1 = (n −1)s2
χ2
α/2,n−1
and k2 = (n −1)s2
χ2
1−α/2,n−1
.
(d) A 100(1 −α)% conﬁdence interval for σ2 is given by (k1, k2).
(e) A 100(1 −α)% conﬁdence interval for σ is given by (√k1, √k2).
c
⃝2000 by Chapman & Hall/CRC

9.5.4
Conﬁdence interval for the probability of a success in a
binomial experiment
Find a 100(1 −α)% conﬁdence interval for the probability of a success p in a
binomial experiment where n is large.
(a) Compute the sample proportion of successes .p.
(b) Determine the critical value zα/2 such that Prob

Z ≥zα/2

= α/2.
(c) Compute the constant k = zα/2
-
.p(1 −.p)
n
.
(d) A 100(1 −α)% conﬁdence interval for p is given by (.p −k, .p + k).
9.5.5
Conﬁdence interval for percentiles
Find an approximate 100(1 −α)% conﬁdence interval for the pth percentile,
ξp, where n is large.
(1) Compute the order statistics {x(1), x(2), . . . , x(n)}.
(2) Determine the critical value zα/2 such that Prob

Z ≥zα/2

= α/2.
(3) Compute the constants k1 =
1
np −zα/2

np(1 −p)
2
and
k2 =
3
np + zα/2

np(1 −p)
4
.
(4) A 100(1 −α)% conﬁdence interval for ξp is given by (x(k1), x(k2)).
9.5.6
Conﬁdence interval for medians
Find an approximate 100(1 −α)% conﬁdence interval for the median ˜µ where
n is large (based on the Wilcoxon one-sample statistic).
(1) Compute the order statistics {w(1), w(2), . . . , w(N)} of the N =
n
2

=
n(n−1)
2
averages (xi + xj)/2, for 1 ≤i < j ≤n.
(2) Determine the critical value zα/2 such that Prob

Z ≥zα/2

= α/2.
(3) Compute the constants k1 =
5N
2 −zα/2N
√
3n
6
and
k2 =
7N
2 + zα/2N
√
3n
8
.
(4) A 100(1 −α)% conﬁdence interval for ˜µ is given by (w(k1), w(k2)). (See
Table 9.6.)
9.5.6.1
Table of conﬁdence interval for medians
If the n observations {x1, x2, . . . , xn} are arranged in ascending order {x(1),
x(2), . . . , x(n)}, a 100(1 −α)% conﬁdence interval for the median of the
population can be found.
Table 9.7 lists values of l and u such that the
probability the median is between x(l) and x(u) is (1 −α).
c
⃝2000 by Chapman & Hall/CRC

α = .05
α = .01
n
k1
k2
k1
k2
7
1
20
8
2
26
9
4
32
10
6
39
1
44
11
8
47
2
53
12
11
55
4
62
13
14
64
6
72
14
17
74
9
82
15
21
84
12
93
16
26
94
15
105
17
30
106
18
118
18
35
118
22
131
19
41
130
27
144
20
46
144
31
159
Table 9.6: Conﬁdence interval for median (see section 9.5.6)
9.5.7
Conﬁdence interval for parameter in a Poisson distribution
The probability distribution function for a Poisson random variable is given
by (see page 103)
f(x; λ) = e−λλx
x!
for x = 0, 1, 2, . . .
(9.3)
For any value of x′ and α < 1, lower and upper values of λ (λlower and λupper)
may be determined such that λlower < λupper and
x′

x=0
e−λlowerλx
lower
x!
= α
2
and
∞

x=x′
e−λupperλx
upper
x!
= α
2
(9.4)
Table 9.8 lists λlower and λupper for α = 0.01 and α = 0.05. For x′ > 50,
λupper and λlower may be approximated by
λupper = χ2
1−α,n
2
where 1 −F(χ2
1−α,n) = α, and n = 2(x′ + 1)
λlower = χ2
α,n
2
where F(χ2
α,n) = α, and n = 2x′
(9.5)
where F(χ2) is the cumulative distribution function for a chi–square random
variable with n degrees of freedom.
Example 9.51:
In a Poisson process, 5 outcomes are observed during a speciﬁed time
interval. Find a 95% and a 99% conﬁdence interval for the parameter λ in this Poisson
process.
c
⃝2000 by Chapman & Hall/CRC

n
l
u
actual α ≤0.05
l
u
actual α ≤0.01
6
1
6
0.031
7
1
7
0.016
8
1
8
0.008
1
8
0.008
9
2
8
0.039
1
9
0.004
10
2
9
0.021
1
10
0.002
11
2
10
0.012
1
11
0.001
12
3
10
0.039
2
11
0.006
13
3
11
0.022
2
12
0.003
14
3
12
0.013
2
13
0.002
15
4
12
0.035
3
13
0.007
16
4
13
0.021
3
14
0.004
17
5
13
0.049
3
15
0.002
18
5
14
0.031
4
15
0.008
19
5
15
0.019
4
16
0.004
20
6
15
0.041
4
17
0.003
21
6
16
0.027
5
17
0.007
22
6
17
0.017
5
18
0.004
23
7
17
0.035
5
19
0.003
24
7
18
0.023
6
19
0.007
25
8
18
0.043
6
20
0.004
26
8
19
0.029
7
20
0.009
27
8
20
0.019
7
21
0.006
28
9
20
0.036
7
22
0.004
29
9
21
0.024
8
22
0.008
30
10
21
0.043
8
23
0.005
35
12
24
0.041
10
26
0.006
40
14
27
0.038
12
29
0.006
50
18
33
0.033
16
35
0.007
60
22
39
0.027
20
41
0.006
70
27
44
0.041
24
47
0.006
75
29
47
0.037
26
50
0.005
80
31
50
0.033
29
52
0.010
90
36
55
0.045
33
58
0.008
100
40
61
0.035
37
64
0.007
110
45
66
0.045
42
69
0.010
120
49
72
0.035
46
75
0.008
Table 9.7: Conﬁdence intervals for medians
Solution:
(S1) Using Table 9.8 with an observed count of 5 and a 95% signiﬁcance level (α =
0.05), the conﬁdence interval bounds are λlower = 1.6 and λupper = 11.7.
(S2) Hence, the probability is .95 that the interval (1.6, 11.7) contains the true value
of λ.
c
⃝2000 by Chapman & Hall/CRC

Signiﬁcance level
Observed
α = 0.01
α = 0.05
count
λlower λhigher λlower λhigher
0
0.0
5.3
0.0
3.7
1
0.0
7.4
0.0
5.6
2
0.1
9.3
0.2
7.2
3
0.3
11.0
0.6
8.8
4
0.7
12.6
1.1
10.2
5
1.1
14.1
1.6
11.7
6
1.5
15.7
2.2
13.1
7
2.0
17.1
2.8
14.4
8
2.6
18.6
3.5
15.8
9
3.1
20.0
4.1
17.1
10
3.7
21.4
4.8
18.4
11
4.3
22.8
5.5
19.7
12
4.9
24.1
6.2
21.0
13
5.6
25.5
6.9
22.2
14
6.2
26.8
7.7
23.5
15
6.9
28.2
8.4
24.7
16
7.6
29.5
9.1
26.0
17
8.3
30.8
9.9
27.2
18
8.9
32.1
10.7
28.4
19
9.6
33.4
11.4
29.7
20
10.4
34.7
12.2
30.9
21
11.1
35.9
13.0
32.1
22
11.8
37.2
13.8
33.3
23
12.5
38.5
14.6
34.5
24
13.3
39.7
15.4
35.7
25
14.0
41.0
16.2
36.9
Signiﬁcance level
Observed
α = 0.01
α = 0.05
count
λlower λhigher λlower λhigher
26
14.7
42.3
17.0
38.1
27
15.5
43.5
17.8
39.3
28
16.2
44.7
18.6
40.5
29
17.0
46.0
19.4
41.6
30
17.8
47.2
20.2
42.8
31
18.5
48.4
21.1
44.0
32
19.3
49.7
21.9
45.2
33
20.1
50.9
22.7
46.3
34
20.9
52.1
23.5
47.5
35
21.6
53.3
24.4
48.7
36
22.4
54.5
25.2
49.8
37
23.2
55.7
26.1
51.0
38
24.0
57.0
26.9
52.2
39
24.8
58.2
27.7
53.3
40
25.6
59.4
28.6
54.5
41
26.4
60.6
29.4
55.6
42
27.2
61.8
30.3
56.8
43
28.0
63.0
31.1
57.9
44
28.8
64.1
32.0
59.1
45
29.6
65.3
32.8
60.2
46
30.4
66.5
33.7
61.4
47
31.2
67.7
34.5
62.5
48
32.0
68.9
35.4
63.6
49
32.8
70.1
36.3
64.8
50
33.7
71.3
37.1
65.9
Table 9.8: Conﬁdence limits for the parameter in a Poisson distribution
(S3) Using Table 9.8 with an observed count of 5 and a 99% signiﬁcance level (α =
0.01), the conﬁdence interval bounds are λlower = 1.1 and λupper = 14.1.
(S4) Hence, the probability is .99 that the interval (1.1, 14.1) contains the true value
of λ.
c
⃝2000 by Chapman & Hall/CRC

9.5.8
Conﬁdence interval for parameter in a binomial distribution
The probability distribution function of a binomial random variable is given
by (see page 84)
f(x; n, p) =
n
x

px(1 −p)n−x
for x = 0, 1, 2, . . . , n
(9.6)
For known n, any value of x′ less than n, and α < 1, lower and upper values
of p (plower and pupper) may be determined such that plower < pupper and
x′

x=0
f(x; n, plower) = α
2
and
n

x=x′
f(x; n, pupper) = α
2
(9.7)
The tables on pages 206–221 list plower and pupper for α = 0.01 and α = 0.05.
Example 9.52:
In a binomial experiment with n = 30, x = 8 successes are observed.
Determine a 95% and a 99% conﬁdence interval for the probability of a success p.
Solution:
(S1) The table on page 210 may be used to construct a 95% conﬁdence interval
(α = 0.05). Using this Table with n −x = 22 and x = 8 the bounds on the
conﬁdence interval are plower = 0.123 and pupper = 0.459.
(S2) Hence, the probability is .95 that the interval (0.123, 0.459) contains the true
value of p.
(S3) Using the Table on page 218 for a 99% conﬁdence level (α = 0.01), the bounds
on the conﬁdence interval are plower = 0.093 and pupper = 0.516.
(S4) Hence, the probability is .99 that the interval (0.093, 0.516) contains the true
value of p.
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
1
2
3
4
5
6
7
8
9
0
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.975
0.842
0.708
0.602
0.522
0.459
0.410
0.369
0.336
1
0.013
0.008
0.006
0.005
0.004
0.004
0.003
0.003
0.003
0.987
0.906
0.806
0.716
0.641
0.579
0.527
0.482
0.445
2
0.094
0.068
0.053
0.043
0.037
0.032
0.028
0.025
0.023
0.992
0.932
0.853
0.777
0.710
0.651
0.600
0.556
0.518
3
0.194
0.147
0.118
0.099
0.085
0.075
0.067
0.060
0.055
0.994
0.947
0.882
0.816
0.755
0.701
0.652
0.610
0.572
4
0.284
0.223
0.184
0.157
0.137
0.122
0.109
0.099
0.091
0.995
0.957
0.901
0.843
0.788
0.738
0.692
0.651
0.614
5
0.359
0.290
0.245
0.212
0.187
0.168
0.152
0.139
0.128
0.996
0.963
0.915
0.863
0.813
0.766
0.723
0.684
0.649
6
0.421
0.349
0.299
0.262
0.234
0.211
0.192
0.177
0.163
0.996
0.968
0.925
0.878
0.832
0.789
0.749
0.711
0.677
7
0.473
0.400
0.348
0.308
0.277
0.251
0.230
0.213
0.198
0.997
0.972
0.933
0.891
0.848
0.808
0.770
0.734
0.701
8
0.518
0.444
0.390
0.349
0.316
0.289
0.266
0.247
0.230
0.997
0.975
0.940
0.901
0.861
0.823
0.787
0.753
0.722
9
0.555
0.482
0.428
0.386
0.351
0.323
0.299
0.278
0.260
0.997
0.977
0.945
0.909
0.872
0.837
0.802
0.770
0.740
10
0.587
0.516
0.462
0.419
0.384
0.354
0.329
0.308
0.289
0.998
0.979
0.950
0.916
0.882
0.848
0.816
0.785
0.756
11
0.615
0.545
0.492
0.449
0.413
0.383
0.357
0.335
0.315
0.998
0.981
0.953
0.922
0.890
0.858
0.827
0.797
0.769
12
0.640
0.572
0.519
0.476
0.440
0.410
0.384
0.361
0.340
0.998
0.982
0.957
0.927
0.897
0.867
0.837
0.809
0.782
13
0.661
0.595
0.544
0.501
0.465
0.435
0.408
0.384
0.364
0.998
0.983
0.960
0.932
0.903
0.874
0.846
0.819
0.793
14
0.680
0.617
0.566
0.524
0.488
0.457
0.430
0.407
0.385
0.998
0.984
0.962
0.936
0.909
0.881
0.854
0.828
0.803
15
0.698
0.636
0.586
0.544
0.509
0.478
0.451
0.427
0.406
0.998
0.985
0.964
0.940
0.913
0.887
0.861
0.836
0.812
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
1
2
3
4
5
6
7
8
9
16
0.713
0.653
0.604
0.563
0.528
0.498
0.471
0.447
0.425
0.998
0.986
0.966
0.943
0.918
0.893
0.868
0.844
0.820
17
0.727
0.669
0.621
0.581
0.546
0.516
0.489
0.465
0.443
0.999
0.987
0.968
0.945
0.922
0.898
0.874
0.851
0.828
18
0.740
0.683
0.637
0.597
0.563
0.533
0.506
0.482
0.460
0.999
0.988
0.970
0.948
0.925
0.902
0.879
0.857
0.835
19
0.751
0.696
0.651
0.612
0.578
0.549
0.522
0.498
0.477
0.999
0.988
0.971
0.951
0.929
0.906
0.884
0.862
0.841
20
0.762
0.708
0.664
0.626
0.593
0.564
0.537
0.513
0.492
0.999
0.989
0.972
0.953
0.932
0.910
0.889
0.868
0.847
22
0.780
0.730
0.688
0.651
0.619
0.591
0.565
0.541
0.520
0.999
0.990
0.975
0.956
0.937
0.917
0.897
0.877
0.858
24
0.796
0.749
0.708
0.673
0.642
0.614
0.589
0.566
0.545
0.999
0.991
0.977
0.960
0.942
0.923
0.904
0.885
0.867
26
0.810
0.765
0.727
0.693
0.663
0.636
0.611
0.588
0.567
0.999
0.991
0.978
0.962
0.945
0.928
0.910
0.893
0.875
28
0.822
0.779
0.742
0.710
0.681
0.655
0.631
0.608
0.588
0.999
0.992
0.980
0.965
0.949
0.932
0.916
0.899
0.882
30
0.833
0.792
0.757
0.726
0.697
0.672
0.648
0.627
0.607
0.999
0.992
0.981
0.967
0.952
0.936
0.920
0.904
0.889
35
0.855
0.818
0.786
0.758
0.732
0.708
0.686
0.666
0.647
0.999
0.993
0.983
0.971
0.958
0.944
0.930
0.916
0.902
40
0.871
0.838
0.809
0.783
0.759
0.737
0.717
0.698
0.680
0.999
0.994
0.985
0.975
0.963
0.951
0.938
0.925
0.912
45
0.885
0.855
0.828
0.804
0.782
0.761
0.742
0.724
0.707
0.999
0.995
0.987
0.977
0.967
0.956
0.944
0.933
0.921
50
0.896
0.868
0.843
0.821
0.800
0.781
0.763
0.746
0.730
0.999
0.995
0.988
0.979
0.970
0.960
0.949
0.939
0.928
60
0.912
0.888
0.867
0.848
0.830
0.813
0.797
0.781
0.767
1.000
0.996
0.990
0.983
0.975
0.966
0.957
0.948
0.939
80
0.933
0.915
0.898
0.883
0.868
0.854
0.841
0.829
0.817
1.000
0.997
0.992
0.987
0.981
0.974
0.967
0.960
0.953
100
0.946
0.931
0.917
0.904
0.892
0.881
0.870
0.859
0.849
1.000
0.998
0.994
0.989
0.984
0.979
0.973
0.967
0.962
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
10
11
12
13
14
15
16
17
18
0
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.309
0.285
0.265
0.247
0.232
0.218
0.206
0.195
0.185
1
0.002
0.002
0.002
0.002
0.002
0.002
0.002
0.001
0.001
0.413
0.385
0.360
0.339
0.320
0.302
0.287
0.273
0.260
2
0.021
0.019
0.018
0.017
0.016
0.015
0.014
0.013
0.012
0.484
0.455
0.428
0.405
0.383
0.364
0.347
0.331
0.317
3
0.050
0.047
0.043
0.040
0.038
0.036
0.034
0.032
0.030
0.538
0.508
0.481
0.456
0.434
0.414
0.396
0.379
0.363
4
0.084
0.078
0.073
0.068
0.064
0.060
0.057
0.055
0.052
0.581
0.551
0.524
0.499
0.476
0.456
0.437
0.419
0.403
5
0.118
0.110
0.103
0.097
0.091
0.087
0.082
0.078
0.075
0.616
0.587
0.560
0.535
0.512
0.491
0.472
0.454
0.437
6
0.152
0.142
0.133
0.126
0.119
0.113
0.107
0.102
0.098
0.646
0.617
0.590
0.565
0.543
0.522
0.502
0.484
0.467
7
0.184
0.173
0.163
0.154
0.146
0.139
0.132
0.126
0.121
0.671
0.643
0.616
0.592
0.570
0.549
0.529
0.511
0.494
8
0.215
0.203
0.191
0.181
0.172
0.164
0.156
0.149
0.143
0.692
0.665
0.639
0.616
0.593
0.573
0.553
0.535
0.518
9
0.244
0.231
0.218
0.207
0.197
0.188
0.180
0.172
0.165
0.711
0.685
0.660
0.636
0.615
0.594
0.575
0.557
0.540
10
0.272
0.257
0.244
0.232
0.221
0.211
0.202
0.194
0.186
0.728
0.702
0.678
0.655
0.634
0.613
0.594
0.576
0.559
11
0.298
0.282
0.268
0.256
0.244
0.234
0.224
0.215
0.207
0.743
0.718
0.694
0.672
0.651
0.631
0.612
0.594
0.577
12
0.322
0.306
0.291
0.278
0.266
0.255
0.245
0.235
0.227
0.756
0.732
0.709
0.687
0.666
0.647
0.628
0.611
0.594
13
0.345
0.328
0.313
0.299
0.287
0.275
0.264
0.255
0.245
0.768
0.744
0.722
0.701
0.680
0.661
0.643
0.626
0.609
14
0.366
0.349
0.334
0.320
0.306
0.294
0.283
0.273
0.264
0.779
0.756
0.734
0.713
0.694
0.675
0.657
0.640
0.623
15
0.387
0.369
0.353
0.339
0.325
0.313
0.302
0.291
0.281
0.789
0.766
0.745
0.725
0.706
0.687
0.669
0.653
0.637
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
10
11
12
13
14
15
16
17
18
16
0.406
0.388
0.372
0.357
0.343
0.331
0.319
0.308
0.298
0.798
0.776
0.755
0.736
0.717
0.698
0.681
0.665
0.649
17
0.424
0.406
0.389
0.374
0.360
0.347
0.335
0.324
0.314
0.806
0.785
0.765
0.745
0.727
0.709
0.692
0.676
0.660
18
0.441
0.423
0.406
0.391
0.377
0.363
0.351
0.340
0.329
0.814
0.793
0.773
0.755
0.736
0.719
0.702
0.686
0.671
19
0.457
0.439
0.422
0.406
0.392
0.379
0.367
0.355
0.344
0.821
0.801
0.782
0.763
0.745
0.728
0.712
0.696
0.681
20
0.472
0.454
0.437
0.421
0.407
0.393
0.381
0.369
0.358
0.827
0.808
0.789
0.771
0.753
0.737
0.721
0.705
0.690
22
0.500
0.482
0.465
0.449
0.435
0.421
0.408
0.396
0.385
0.839
0.820
0.803
0.785
0.769
0.753
0.737
0.722
0.707
24
0.525
0.507
0.490
0.475
0.460
0.446
0.433
0.421
0.410
0.849
0.831
0.814
0.798
0.782
0.766
0.751
0.737
0.723
26
0.548
0.530
0.513
0.498
0.483
0.469
0.456
0.444
0.433
0.858
0.841
0.825
0.809
0.794
0.779
0.764
0.750
0.737
28
0.569
0.551
0.535
0.519
0.504
0.491
0.478
0.465
0.454
0.866
0.850
0.834
0.819
0.804
0.790
0.776
0.762
0.749
30
0.588
0.570
0.554
0.539
0.524
0.510
0.497
0.485
0.473
0.873
0.858
0.843
0.828
0.814
0.800
0.786
0.773
0.760
35
0.629
0.612
0.596
0.582
0.567
0.554
0.541
0.529
0.517
0.888
0.874
0.861
0.847
0.834
0.821
0.809
0.797
0.785
40
0.663
0.647
0.632
0.617
0.603
0.590
0.578
0.566
0.555
0.900
0.887
0.875
0.862
0.850
0.839
0.827
0.816
0.805
45
0.691
0.676
0.661
0.647
0.634
0.621
0.609
0.598
0.586
0.909
0.898
0.886
0.875
0.864
0.853
0.842
0.831
0.821
50
0.715
0.700
0.686
0.673
0.660
0.648
0.636
0.625
0.614
0.917
0.906
0.896
0.885
0.875
0.865
0.855
0.845
0.835
60
0.753
0.740
0.727
0.715
0.703
0.692
0.681
0.670
0.660
0.929
0.920
0.911
0.902
0.893
0.883
0.875
0.866
0.857
80
0.805
0.794
0.783
0.773
0.763
0.753
0.743
0.734
0.725
0.945
0.938
0.931
0.923
0.916
0.909
0.902
0.894
0.887
100
0.839
0.830
0.820
0.811
0.803
0.794
0.786
0.778
0.770
0.956
0.950
0.943
0.937
0.931
0.925
0.919
0.913
0.907
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
19
20
22
24
26
28
30
35
40
0
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.176
0.168
0.154
0.143
0.132
0.123
0.116
0.100
0.088
1
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.249
0.238
0.220
0.204
0.190
0.178
0.167
0.145
0.129
2
0.012
0.011
0.010
0.009
0.009
0.008
0.008
0.007
0.006
0.304
0.292
0.270
0.251
0.235
0.221
0.208
0.182
0.162
3
0.029
0.028
0.025
0.023
0.022
0.020
0.019
0.017
0.015
0.349
0.336
0.312
0.292
0.273
0.258
0.243
0.214
0.191
4
0.049
0.047
0.044
0.040
0.038
0.035
0.033
0.029
0.025
0.388
0.374
0.349
0.327
0.307
0.290
0.274
0.242
0.217
5
0.071
0.068
0.063
0.058
0.055
0.051
0.048
0.042
0.037
0.422
0.407
0.381
0.358
0.337
0.319
0.303
0.268
0.241
6
0.094
0.090
0.083
0.077
0.072
0.068
0.064
0.056
0.049
0.451
0.436
0.409
0.386
0.364
0.345
0.328
0.292
0.263
7
0.116
0.111
0.103
0.096
0.090
0.084
0.080
0.070
0.062
0.478
0.463
0.435
0.411
0.389
0.369
0.352
0.314
0.283
8
0.138
0.132
0.123
0.115
0.107
0.101
0.096
0.084
0.075
0.502
0.487
0.459
0.434
0.412
0.392
0.373
0.334
0.302
9
0.159
0.153
0.142
0.133
0.125
0.118
0.111
0.098
0.088
0.523
0.508
0.480
0.455
0.433
0.412
0.393
0.353
0.320
10
0.179
0.173
0.161
0.151
0.142
0.134
0.127
0.112
0.100
0.543
0.528
0.500
0.475
0.452
0.431
0.412
0.371
0.337
11
0.199
0.192
0.180
0.169
0.159
0.150
0.142
0.126
0.113
0.561
0.546
0.518
0.493
0.470
0.449
0.430
0.388
0.353
12
0.218
0.211
0.197
0.186
0.175
0.166
0.157
0.139
0.125
0.578
0.563
0.535
0.510
0.487
0.465
0.446
0.404
0.368
13
0.237
0.229
0.215
0.202
0.191
0.181
0.172
0.153
0.138
0.594
0.579
0.551
0.525
0.502
0.481
0.461
0.418
0.383
14
0.255
0.247
0.231
0.218
0.206
0.196
0.186
0.166
0.150
0.608
0.593
0.565
0.540
0.517
0.496
0.476
0.433
0.397
15
0.272
0.263
0.247
0.234
0.221
0.210
0.200
0.179
0.161
0.621
0.607
0.579
0.554
0.531
0.509
0.490
0.446
0.410
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
19
20
22
24
26
28
30
35
40
16
0.288
0.279
0.263
0.249
0.236
0.224
0.214
0.191
0.173
0.633
0.619
0.592
0.567
0.544
0.522
0.503
0.459
0.422
17
0.304
0.295
0.278
0.263
0.250
0.238
0.227
0.203
0.184
0.645
0.631
0.604
0.579
0.556
0.535
0.515
0.471
0.434
18
0.319
0.310
0.293
0.277
0.263
0.251
0.240
0.215
0.195
0.656
0.642
0.615
0.590
0.567
0.546
0.527
0.483
0.445
19
0.334
0.324
0.307
0.291
0.277
0.264
0.252
0.227
0.206
0.666
0.652
0.626
0.601
0.579
0.557
0.538
0.494
0.456
20
0.348
0.338
0.320
0.304
0.289
0.276
0.264
0.238
0.217
0.676
0.662
0.636
0.612
0.589
0.568
0.548
0.504
0.467
22
0.374
0.364
0.346
0.329
0.314
0.300
0.287
0.260
0.237
0.693
0.680
0.654
0.631
0.608
0.587
0.568
0.524
0.487
24
0.399
0.388
0.369
0.352
0.337
0.322
0.309
0.281
0.257
0.709
0.696
0.671
0.648
0.626
0.605
0.586
0.543
0.505
26
0.421
0.411
0.392
0.374
0.358
0.343
0.330
0.300
0.276
0.723
0.711
0.686
0.663
0.642
0.622
0.603
0.560
0.522
28
0.443
0.432
0.413
0.395
0.378
0.363
0.350
0.319
0.294
0.736
0.724
0.700
0.678
0.657
0.637
0.618
0.575
0.538
30
0.462
0.452
0.432
0.414
0.397
0.382
0.368
0.337
0.311
0.748
0.736
0.713
0.691
0.670
0.650
0.632
0.590
0.553
35
0.506
0.496
0.476
0.457
0.440
0.425
0.410
0.378
0.351
0.773
0.762
0.740
0.719
0.700
0.681
0.663
0.622
0.586
40
0.544
0.533
0.513
0.495
0.478
0.462
0.447
0.414
0.386
0.794
0.783
0.763
0.743
0.724
0.706
0.689
0.649
0.614
45
0.576
0.565
0.546
0.528
0.511
0.495
0.480
0.447
0.418
0.811
0.801
0.782
0.763
0.745
0.728
0.711
0.673
0.639
50
0.604
0.594
0.575
0.557
0.540
0.524
0.510
0.476
0.447
0.825
0.816
0.798
0.780
0.763
0.747
0.731
0.694
0.660
60
0.650
0.641
0.622
0.605
0.589
0.574
0.560
0.526
0.497
0.849
0.840
0.824
0.808
0.792
0.777
0.763
0.728
0.697
80
0.717
0.708
0.692
0.676
0.662
0.648
0.634
0.603
0.575
0.880
0.873
0.860
0.846
0.833
0.820
0.808
0.778
0.750
100
0.762
0.754
0.740
0.726
0.712
0.700
0.687
0.658
0.632
0.901
0.895
0.883
0.872
0.861
0.849
0.839
0.812
0.787
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italics, upper limit in roman)
x
45
50
60
80
100
∞
0
0.000
0.000
0.000
0.000
0.000
0.000
0.079
0.071
0.060
0.045
0.036
0.000
1
0.001
0.001
0.000
0.000
0.000
0.000
0.115
0.104
0.088
0.067
0.054
0.000
2
0.005
0.005
0.004
0.003
0.002
0.000
0.145
0.132
0.112
0.085
0.069
0.000
3
0.013
0.012
0.010
0.008
0.006
0.000
0.172
0.157
0.133
0.102
0.083
0.000
4
0.023
0.021
0.017
0.013
0.011
0.000
0.196
0.179
0.152
0.117
0.096
0.000
5
0.033
0.030
0.025
0.019
0.016
0.000
0.218
0.200
0.170
0.132
0.108
0.000
6
0.044
0.040
0.034
0.026
0.021
0.000
0.239
0.219
0.187
0.146
0.119
0.000
7
0.056
0.051
0.043
0.033
0.027
0.000
0.258
0.237
0.203
0.159
0.130
0.000
8
0.067
0.061
0.052
0.040
0.033
0.000
0.276
0.254
0.219
0.171
0.141
0.000
9
0.079
0.072
0.061
0.047
0.038
0.000
0.293
0.270
0.233
0.183
0.151
0.000
10
0.091
0.083
0.071
0.055
0.044
0.000
0.309
0.285
0.247
0.195
0.161
0.000
11
0.102
0.094
0.080
0.062
0.050
0.000
0.324
0.300
0.260
0.206
0.170
0.000
12
0.114
0.104
0.089
0.069
0.057
0.000
0.339
0.314
0.273
0.217
0.180
0.000
13
0.125
0.115
0.098
0.077
0.063
0.000
0.353
0.327
0.285
0.227
0.189
0.000
14
0.136
0.125
0.107
0.084
0.069
0.000
0.366
0.340
0.297
0.237
0.197
0.000
15
0.147
0.135
0.117
0.091
0.075
0.000
0.379
0.352
0.308
0.247
0.206
0.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .95)
Denominator minus numerator: n −x
(Lower limit in italics, upper limit in roman)
x
45
50
60
80
100
∞
16
0.158
0.145
0.125
0.098
0.081
0.000
0.391
0.364
0.319
0.257
0.214
0.000
17
0.169
0.155
0.134
0.106
0.087
0.000
0.402
0.375
0.330
0.266
0.222
0.000
18
0.179
0.165
0.143
0.113
0.093
0.000
0.414
0.386
0.340
0.275
0.230
0.000
19
0.189
0.175
0.151
0.120
0.099
0.000
0.424
0.396
0.350
0.283
0.238
0.000
20
0.199
0.184
0.160
0.127
0.105
0.000
0.435
0.406
0.359
0.292
0.246
0.000
22
0.218
0.202
0.176
0.140
0.117
0.000
0.454
0.425
0.378
0.308
0.260
0.000
24
0.237
0.220
0.192
0.154
0.128
0.000
0.472
0.443
0.395
0.324
0.274
0.000
26
0.255
0.237
0.208
0.167
0.139
0.000
0.489
0.460
0.411
0.338
0.288
0.000
28
0.272
0.253
0.223
0.180
0.151
0.000
0.505
0.476
0.426
0.352
0.300
0.000
30
0.289
0.269
0.237
0.192
0.161
0.000
0.520
0.490
0.440
0.366
0.313
0.000
35
0.327
0.306
0.272
0.222
0.188
0.000
0.553
0.524
0.474
0.397
0.342
0.000
40
0.361
0.340
0.303
0.250
0.213
0.000
0.582
0.553
0.503
0.425
0.368
0.000
45
0.393
0.370
0.332
0.276
0.236
0.000
0.607
0.579
0.529
0.451
0.392
0.000
50
0.421
0.398
0.359
0.301
0.259
0.000
0.630
0.602
0.552
0.474
0.415
0.000
60
0.471
0.448
0.407
0.345
0.300
0.000
0.668
0.641
0.593
0.515
0.455
0.000
80
0.549
0.526
0.485
0.420
0.371
0.000
0.724
0.699
0.655
0.580
0.520
0.000
100
0.608
0.585
0.545
0.480
0.429
0.000
0.764
0.741
0.700
0.629
0.571
0.000
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
1
2
3
4
5
6
7
8
9
0
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.995
0.929
0.829
0.734
0.653
0.586
0.531
0.484
0.445
1
0.003
0.002
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.997
0.959
0.889
0.815
0.746
0.685
0.632
0.585
0.544
2
0.041
0.029
0.023
0.019
0.016
0.014
0.012
0.011
0.010
0.998
0.971
0.917
0.856
0.797
0.742
0.693
0.648
0.608
3
0.111
0.083
0.066
0.055
0.047
0.042
0.037
0.033
0.030
0.999
0.977
0.934
0.882
0.830
0.781
0.735
0.693
0.655
4
0.185
0.144
0.118
0.100
0.087
0.077
0.069
0.062
0.057
0.999
0.981
0.945
0.900
0.854
0.809
0.767
0.727
0.691
5
0.254
0.203
0.170
0.146
0.128
0.114
0.103
0.094
0.087
0.999
0.984
0.953
0.913
0.872
0.831
0.791
0.755
0.720
6
0.315
0.258
0.219
0.191
0.169
0.152
0.138
0.127
0.117
0.999
0.986
0.958
0.923
0.886
0.848
0.811
0.777
0.744
7
0.368
0.307
0.265
0.233
0.209
0.189
0.172
0.159
0.147
0.999
0.988
0.963
0.931
0.897
0.862
0.828
0.795
0.764
8
0.415
0.352
0.307
0.273
0.245
0.223
0.205
0.190
0.176
0.999
0.989
0.967
0.938
0.906
0.873
0.841
0.810
0.781
9
0.456
0.392
0.345
0.309
0.280
0.256
0.236
0.219
0.205
0.999
0.990
0.970
0.943
0.913
0.883
0.853
0.824
0.795
10
0.491
0.427
0.379
0.342
0.312
0.287
0.266
0.247
0.232
1.000
0.991
0.972
0.947
0.920
0.891
0.863
0.835
0.808
11
0.523
0.459
0.411
0.373
0.341
0.315
0.293
0.274
0.257
1.000
0.992
0.974
0.951
0.925
0.899
0.872
0.845
0.819
12
0.551
0.488
0.440
0.401
0.369
0.342
0.319
0.299
0.282
1.000
0.992
0.976
0.955
0.930
0.905
0.879
0.854
0.829
13
0.576
0.514
0.466
0.427
0.394
0.367
0.343
0.323
0.305
1.000
0.993
0.978
0.957
0.935
0.910
0.886
0.862
0.838
14
0.598
0.537
0.490
0.451
0.418
0.390
0.366
0.345
0.326
1.000
0.993
0.979
0.960
0.938
0.915
0.892
0.869
0.846
15
0.619
0.559
0.512
0.473
0.440
0.412
0.388
0.366
0.347
1.000
0.994
0.980
0.962
0.942
0.920
0.898
0.875
0.854
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
1
2
3
4
5
6
7
8
9
16
0.637
0.578
0.532
0.493
0.461
0.433
0.408
0.386
0.367
1.000
0.994
0.981
0.964
0.945
0.924
0.903
0.881
0.860
17
0.654
0.596
0.550
0.512
0.480
0.452
0.427
0.405
0.385
1.000
0.994
0.982
0.966
0.947
0.927
0.907
0.887
0.866
18
0.669
0.613
0.568
0.530
0.498
0.470
0.445
0.422
0.403
1.000
0.995
0.983
0.968
0.950
0.931
0.911
0.891
0.872
19
0.683
0.628
0.584
0.547
0.515
0.486
0.461
0.439
0.419
1.000
0.995
0.984
0.969
0.952
0.934
0.915
0.896
0.877
20
0.696
0.642
0.599
0.562
0.530
0.502
0.477
0.455
0.435
1.000
0.995
0.985
0.971
0.954
0.936
0.918
0.900
0.881
22
0.719
0.668
0.626
0.590
0.559
0.531
0.507
0.484
0.464
1.000
0.996
0.986
0.973
0.958
0.941
0.924
0.907
0.890
24
0.738
0.690
0.649
0.615
0.584
0.557
0.533
0.511
0.491
1.000
0.996
0.987
0.975
0.961
0.945
0.930
0.913
0.897
26
0.755
0.709
0.670
0.637
0.607
0.581
0.557
0.535
0.515
1.000
0.996
0.988
0.977
0.963
0.949
0.934
0.919
0.904
28
0.770
0.726
0.689
0.656
0.627
0.602
0.578
0.557
0.537
1.000
0.997
0.989
0.978
0.966
0.952
0.938
0.924
0.909
30
0.784
0.741
0.705
0.674
0.646
0.621
0.597
0.576
0.557
1.000
0.997
0.989
0.980
0.968
0.955
0.942
0.928
0.914
35
0.811
0.773
0.740
0.711
0.685
0.661
0.639
0.619
0.600
1.000
0.997
0.991
0.982
0.972
0.961
0.949
0.937
0.924
40
0.832
0.797
0.767
0.741
0.716
0.694
0.673
0.654
0.636
1.000
0.997
0.992
0.984
0.975
0.965
0.955
0.944
0.933
45
0.849
0.817
0.789
0.765
0.742
0.721
0.702
0.683
0.666
1.000
0.998
0.993
0.986
0.978
0.969
0.959
0.949
0.939
50
0.863
0.834
0.808
0.785
0.763
0.744
0.725
0.708
0.692
1.000
0.998
0.993
0.987
0.980
0.972
0.963
0.954
0.945
60
0.884
0.859
0.836
0.816
0.797
0.780
0.763
0.747
0.733
1.000
0.998
0.995
0.989
0.983
0.976
0.969
0.961
0.953
80
0.912
0.892
0.874
0.858
0.842
0.828
0.814
0.801
0.789
1.000
0.999
0.996
0.992
0.987
0.982
0.976
0.970
0.964
100
0.929
0.912
0.898
0.884
0.871
0.859
0.847
0.836
0.826
1.000
0.999
0.997
0.993
0.990
0.985
0.981
0.976
0.971
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
10
11
12
13
14
15
16
17
18
0
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.411
0.382
0.357
0.335
0.315
0.298
0.282
0.268
0.255
1
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.509
0.477
0.449
0.424
0.402
0.381
0.363
0.346
0.331
2
0.009
0.008
0.008
0.007
0.007
0.006
0.006
0.006
0.005
0.573
0.541
0.512
0.486
0.463
0.441
0.422
0.404
0.387
3
0.028
0.026
0.024
0.022
0.021
0.020
0.019
0.018
0.017
0.621
0.589
0.560
0.534
0.510
0.488
0.468
0.450
0.432
4
0.053
0.049
0.045
0.043
0.040
0.038
0.036
0.034
0.032
0.658
0.627
0.599
0.573
0.549
0.527
0.507
0.488
0.470
5
0.080
0.075
0.070
0.065
0.062
0.058
0.055
0.053
0.050
0.688
0.659
0.631
0.606
0.582
0.560
0.539
0.520
0.502
6
0.109
0.101
0.095
0.090
0.085
0.080
0.076
0.073
0.069
0.713
0.685
0.658
0.633
0.610
0.588
0.567
0.548
0.530
7
0.137
0.128
0.121
0.114
0.108
0.102
0.097
0.093
0.089
0.734
0.707
0.681
0.657
0.634
0.612
0.592
0.573
0.555
8
0.165
0.155
0.146
0.138
0.131
0.125
0.119
0.113
0.109
0.753
0.726
0.701
0.677
0.655
0.634
0.614
0.595
0.578
9
0.192
0.181
0.171
0.162
0.154
0.146
0.140
0.134
0.128
0.768
0.743
0.718
0.695
0.674
0.653
0.633
0.615
0.597
10
0.218
0.206
0.195
0.185
0.176
0.168
0.161
0.154
0.148
0.782
0.758
0.734
0.712
0.690
0.670
0.651
0.633
0.616
11
0.242
0.229
0.218
0.207
0.197
0.189
0.181
0.173
0.167
0.794
0.771
0.748
0.726
0.706
0.686
0.667
0.649
0.632
12
0.266
0.252
0.240
0.228
0.218
0.209
0.200
0.192
0.185
0.805
0.782
0.760
0.739
0.719
0.700
0.681
0.664
0.647
13
0.288
0.274
0.261
0.249
0.238
0.228
0.219
0.211
0.203
0.815
0.793
0.772
0.751
0.732
0.713
0.695
0.677
0.661
14
0.310
0.294
0.281
0.268
0.257
0.247
0.237
0.228
0.220
0.824
0.803
0.782
0.762
0.743
0.724
0.707
0.690
0.674
15
0.330
0.314
0.300
0.287
0.276
0.265
0.255
0.246
0.237
0.832
0.811
0.791
0.772
0.753
0.735
0.718
0.701
0.685
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
10
11
12
13
14
15
16
17
18
16
0.349
0.333
0.319
0.305
0.293
0.282
0.272
0.262
0.253
0.839
0.819
0.800
0.781
0.763
0.745
0.728
0.712
0.696
17
0.367
0.351
0.336
0.323
0.310
0.299
0.288
0.278
0.269
0.846
0.827
0.808
0.789
0.772
0.754
0.738
0.722
0.706
18
0.384
0.368
0.353
0.339
0.326
0.315
0.304
0.294
0.284
0.852
0.833
0.815
0.797
0.780
0.763
0.747
0.731
0.716
19
0.401
0.384
0.369
0.355
0.342
0.330
0.319
0.308
0.299
0.858
0.840
0.822
0.804
0.787
0.771
0.755
0.740
0.725
20
0.417
0.400
0.384
0.370
0.357
0.345
0.333
0.323
0.313
0.863
0.845
0.828
0.811
0.794
0.778
0.763
0.748
0.733
22
0.446
0.429
0.413
0.399
0.385
0.372
0.361
0.350
0.340
0.873
0.856
0.839
0.823
0.807
0.792
0.777
0.763
0.748
24
0.472
0.455
0.439
0.425
0.411
0.398
0.386
0.375
0.364
0.881
0.865
0.849
0.834
0.819
0.804
0.789
0.776
0.762
26
0.496
0.479
0.463
0.449
0.435
0.422
0.410
0.398
0.388
0.888
0.873
0.858
0.843
0.829
0.815
0.801
0.787
0.774
28
0.518
0.502
0.486
0.471
0.457
0.444
0.432
0.420
0.409
0.894
0.880
0.866
0.852
0.838
0.824
0.811
0.798
0.785
30
0.539
0.522
0.506
0.492
0.478
0.465
0.452
0.441
0.430
0.900
0.886
0.873
0.859
0.846
0.833
0.820
0.807
0.795
35
0.583
0.567
0.551
0.537
0.523
0.510
0.498
0.486
0.475
0.912
0.900
0.887
0.875
0.863
0.851
0.839
0.828
0.816
40
0.620
0.604
0.589
0.575
0.561
0.549
0.536
0.525
0.514
0.921
0.910
0.899
0.888
0.877
0.866
0.855
0.844
0.833
45
0.650
0.635
0.621
0.607
0.594
0.582
0.570
0.558
0.548
0.929
0.919
0.908
0.898
0.888
0.878
0.867
0.857
0.848
50
0.676
0.662
0.648
0.635
0.622
0.610
0.599
0.587
0.577
0.935
0.926
0.916
0.906
0.897
0.888
0.878
0.869
0.860
60
0.719
0.705
0.692
0.680
0.668
0.657
0.646
0.636
0.626
0.945
0.937
0.928
0.920
0.912
0.903
0.895
0.887
0.879
80
0.777
0.766
0.755
0.744
0.734
0.724
0.714
0.705
0.696
0.957
0.951
0.944
0.938
0.931
0.924
0.918
0.911
0.905
100
0.815
0.806
0.796
0.787
0.778
0.769
0.761
0.752
0.744
0.965
0.960
0.955
0.949
0.944
0.938
0.933
0.927
0.921
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
19
20
22
24
26
28
30
35
40
0
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.243
0.233
0.214
0.198
0.184
0.172
0.162
0.140
0.124
1
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.000
0.317
0.304
0.281
0.262
0.245
0.230
0.216
0.189
0.168
2
0.005
0.005
0.004
0.004
0.004
0.003
0.003
0.003
0.003
0.372
0.358
0.332
0.310
0.291
0.274
0.259
0.227
0.203
3
0.016
0.015
0.014
0.013
0.012
0.011
0.011
0.009
0.008
0.416
0.401
0.374
0.351
0.330
0.311
0.295
0.260
0.233
4
0.031
0.029
0.027
0.025
0.023
0.022
0.020
0.018
0.016
0.453
0.438
0.410
0.385
0.363
0.344
0.326
0.289
0.259
5
0.048
0.046
0.042
0.039
0.037
0.034
0.032
0.028
0.025
0.485
0.470
0.441
0.416
0.393
0.373
0.354
0.315
0.284
6
0.066
0.064
0.059
0.055
0.051
0.048
0.045
0.039
0.035
0.514
0.498
0.469
0.443
0.419
0.398
0.379
0.339
0.306
7
0.085
0.082
0.076
0.070
0.066
0.062
0.058
0.051
0.045
0.539
0.523
0.493
0.467
0.443
0.422
0.403
0.361
0.327
8
0.104
0.100
0.093
0.087
0.081
0.076
0.072
0.063
0.056
0.561
0.545
0.516
0.489
0.465
0.443
0.424
0.381
0.346
9
0.123
0.119
0.110
0.103
0.096
0.091
0.086
0.076
0.067
0.581
0.565
0.536
0.509
0.485
0.463
0.443
0.400
0.364
10
0.142
0.137
0.127
0.119
0.112
0.106
0.100
0.088
0.079
0.599
0.583
0.554
0.528
0.504
0.482
0.461
0.417
0.380
11
0.160
0.155
0.144
0.135
0.127
0.120
0.114
0.100
0.090
0.616
0.600
0.571
0.545
0.521
0.498
0.478
0.433
0.396
12
0.178
0.172
0.161
0.151
0.142
0.134
0.127
0.113
0.101
0.631
0.616
0.587
0.561
0.537
0.514
0.494
0.449
0.411
13
0.196
0.189
0.177
0.166
0.157
0.148
0.141
0.125
0.112
0.645
0.630
0.601
0.575
0.551
0.529
0.508
0.463
0.425
14
0.213
0.206
0.193
0.181
0.171
0.162
0.154
0.137
0.123
0.658
0.643
0.615
0.589
0.565
0.543
0.522
0.477
0.439
15
0.229
0.222
0.208
0.196
0.185
0.176
0.167
0.149
0.134
0.670
0.655
0.628
0.602
0.578
0.556
0.535
0.490
0.451
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italic type, upper limit in roman type)
x
19
20
22
24
26
28
30
35
40
16
0.245
0.237
0.223
0.211
0.199
0.189
0.180
0.161
0.145
0.681
0.667
0.639
0.614
0.590
0.568
0.548
0.502
0.464
17
0.260
0.252
0.237
0.224
0.213
0.202
0.193
0.172
0.156
0.692
0.677
0.650
0.625
0.602
0.580
0.559
0.514
0.475
18
0.275
0.267
0.252
0.238
0.226
0.215
0.205
0.184
0.167
0.701
0.687
0.660
0.636
0.612
0.591
0.570
0.525
0.486
19
0.289
0.281
0.265
0.251
0.239
0.227
0.217
0.195
0.177
0.711
0.697
0.670
0.646
0.623
0.601
0.581
0.536
0.497
20
0.303
0.295
0.279
0.264
0.251
0.239
0.229
0.206
0.187
0.719
0.705
0.679
0.655
0.632
0.611
0.591
0.546
0.507
22
0.330
0.321
0.304
0.289
0.275
0.263
0.251
0.227
0.207
0.735
0.721
0.696
0.672
0.650
0.629
0.609
0.565
0.526
24
0.354
0.345
0.328
0.312
0.298
0.285
0.273
0.247
0.226
0.749
0.736
0.711
0.688
0.666
0.646
0.626
0.582
0.543
26
0.377
0.368
0.350
0.334
0.319
0.306
0.293
0.266
0.244
0.761
0.749
0.725
0.702
0.681
0.661
0.642
0.598
0.560
28
0.399
0.389
0.371
0.354
0.339
0.325
0.313
0.285
0.262
0.773
0.761
0.737
0.715
0.694
0.675
0.656
0.613
0.575
30
0.419
0.409
0.391
0.374
0.358
0.344
0.331
0.303
0.279
0.783
0.771
0.749
0.727
0.707
0.687
0.669
0.626
0.589
35
0.464
0.454
0.435
0.418
0.402
0.387
0.374
0.343
0.318
0.805
0.794
0.773
0.753
0.734
0.715
0.697
0.657
0.620
40
0.503
0.493
0.474
0.457
0.440
0.425
0.411
0.380
0.353
0.823
0.813
0.793
0.774
0.756
0.738
0.721
0.682
0.647
45
0.537
0.527
0.508
0.491
0.474
0.459
0.445
0.413
0.386
0.838
0.829
0.810
0.792
0.775
0.758
0.742
0.704
0.670
50
0.567
0.557
0.538
0.521
0.505
0.489
0.475
0.443
0.415
0.851
0.842
0.824
0.807
0.791
0.775
0.759
0.723
0.690
60
0.616
0.607
0.589
0.572
0.556
0.541
0.527
0.495
0.466
0.871
0.863
0.847
0.832
0.817
0.802
0.788
0.755
0.724
80
0.687
0.679
0.662
0.647
0.632
0.618
0.605
0.574
0.547
0.898
0.892
0.879
0.866
0.853
0.841
0.829
0.800
0.773
100
0.736
0.729
0.714
0.700
0.686
0.674
0.661
0.632
0.606
0.916
0.910
0.899
0.888
0.878
0.867
0.857
0.832
0.807
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italics, upper limit in roman)
x
45
50
60
80
100
∞
0
0.000
0.000
0.000
0.000
0.000
0.000
0.111
0.101
0.085
0.064
0.052
0.000
1
0.000
0.000
0.000
0.000
0.000
0.000
0.151
0.137
0.116
0.088
0.071
0.000
2
0.002
0.002
0.002
0.001
0.001
0.000
0.183
0.166
0.141
0.108
0.088
0.000
3
0.007
0.007
0.005
0.004
0.003
0.000
0.211
0.192
0.164
0.126
0.102
0.000
4
0.014
0.013
0.011
0.008
0.007
0.000
0.235
0.215
0.184
0.142
0.116
0.000
5
0.022
0.020
0.017
0.013
0.010
0.000
0.258
0.237
0.203
0.158
0.129
0.000
6
0.031
0.028
0.024
0.018
0.015
0.000
0.279
0.256
0.220
0.172
0.141
0.000
7
0.041
0.037
0.031
0.024
0.019
0.000
0.298
0.275
0.237
0.186
0.153
0.000
8
0.051
0.046
0.039
0.030
0.024
0.000
0.317
0.292
0.253
0.199
0.164
0.000
9
0.061
0.055
0.047
0.036
0.029
0.000
0.334
0.308
0.267
0.211
0.174
0.000
10
0.071
0.065
0.055
0.043
0.035
0.000
0.350
0.324
0.281
0.223
0.185
0.000
11
0.081
0.074
0.063
0.049
0.040
0.000
0.365
0.338
0.295
0.234
0.194
0.000
12
0.092
0.084
0.072
0.056
0.045
0.000
0.379
0.352
0.308
0.245
0.204
0.000
13
0.102
0.094
0.080
0.062
0.051
0.000
0.393
0.365
0.320
0.256
0.213
0.000
14
0.112
0.103
0.088
0.069
0.056
0.000
0.406
0.378
0.332
0.266
0.222
0.000
15
0.122
0.112
0.097
0.076
0.062
0.000
0.418
0.390
0.343
0.276
0.231
0.000
16
0.133
0.122
0.105
0.082
0.067
0.000
0.430
0.401
0.354
0.286
0.239
0.000
17
0.143
0.131
0.113
0.089
0.073
0.000
0.442
0.413
0.364
0.295
0.248
0.000
c
⃝2000 by Chapman & Hall/CRC

Conﬁdence limits of proportions (conﬁdence coeﬃcient .99)
Denominator minus numerator: n −x
(Lower limit in italics, upper limit in roman)
x
45
50
60
80
100
∞
18
0.152
0.140
0.121
0.095
0.079
0.000
0.452
0.423
0.374
0.304
0.256
0.000
19
0.162
0.149
0.129
0.102
0.084
0.000
0.463
0.433
0.384
0.313
0.264
0.000
20
0.171
0.158
0.137
0.108
0.090
0.000
0.473
0.443
0.393
0.321
0.271
0.000
22
0.190
0.176
0.153
0.121
0.101
0.000
0.492
0.462
0.411
0.338
0.286
0.000
24
0.208
0.193
0.168
0.134
0.112
0.000
0.509
0.479
0.428
0.353
0.300
0.000
26
0.225
0.209
0.183
0.147
0.122
0.000
0.526
0.495
0.444
0.368
0.314
0.000
28
0.242
0.225
0.198
0.159
0.133
0.000
0.541
0.511
0.459
0.382
0.326
0.000
30
0.258
0.241
0.212
0.171
0.143
0.000
0.555
0.525
0.473
0.395
0.339
0.000
35
0.296
0.277
0.245
0.200
0.168
0.000
0.587
0.557
0.505
0.426
0.368
0.000
40
0.330
0.310
0.276
0.227
0.193
0.000
0.614
0.585
0.534
0.453
0.394
0.000
45
0.362
0.341
0.305
0.253
0.216
0.000
0.638
0.609
0.559
0.478
0.418
0.000
50
0.391
0.369
0.332
0.277
0.238
0.000
0.659
0.631
0.581
0.501
0.440
0.000
60
0.441
0.419
0.380
0.321
0.278
0.000
0.695
0.668
0.620
0.541
0.479
0.000
80
0.522
0.499
0.459
0.396
0.349
0.000
0.747
0.723
0.679
0.604
0.543
0.000
100
0.582
0.560
0.521
0.457
0.407
0.000
0.784
0.762
0.722
0.651
0.593
0.000
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

9.6
CONFIDENCE INTERVALS: TWO SAMPLES
Let x1, x2, . . . , xn1 be a random sample of size n1 from population 1 and
y1, y2, . . . , yn2 a random sample of size n2 from population 2.
9.6.1
Conﬁdence interval for diﬀerence in means, known variances
Find a 100(1 −α)% conﬁdence interval for the diﬀerence in means µ1 −µ2
if the populations are normal, σ2
1 and σ2
2 are known, and the samples are
independent, or
Find a 100(1 −α)% conﬁdence interval for the diﬀerence in means µ1 −µ2 if
n1 and n2 are large, σ2
1 and σ2
2 are known, and the samples are independent.
(a) Compute the sample means x1 and x2.
(b) Determine the critical value zα/2 such that Prob

Z ≥zα/2

= α/2.
(c) Compute the constant k = zα/2
+
σ2
1
n1
+ σ2
2
n2
.
(d) A 100(1 −α)% conﬁdence interval for µ1 −µ2 is given by ((x1 −x2) −
k, (x1 −x2) + k).
9.6.2
Conﬁdence interval for diﬀerence in means, equal unknown
variances
Find a 100(1 −α)% conﬁdence interval for the diﬀerence in means µ1 −µ2 if
the populations are normal, the samples are independent, and the variances
are unknown but assumed equal (σ2
1 = σ2
2 = σ2).
(a) Compute the sample means x1 and x2, the sample variances s2
1 and s2
2,
and the pooled estimate of the common variance σ2
s2
p = (n1 −1)s2
1 + (n2 −1)s2
2
n1 + n2 −2
.
(9.8)
(b) Determine the critical value tα/2,n1+n2−2 such that
Prob

T ≥tα/2,n1+n2−2

= α/2.
(c) Compute the constant k = tα/2,n1+n2−2 · sp
-
1
n1
+ 1
n2
,
(d) A 100(1 −α)% conﬁdence interval for µ1 −µ2 is given by ((x1 −x2) −
k, (x1 −x2) + k).
9.6.3
Conﬁdence interval for diﬀerence in means, unequal
unknown variances
Find an approximate 100(1 −α)% conﬁdence interval for the diﬀerence in
means µ1 −µ2 if the populations are normal, the samples are independent,
and the variances are unknown and unequal.
c
⃝2000 by Chapman & Hall/CRC

(a) Compute the sample means x1 and x2, the sample variances s2
1 and s2
2,
and the approximate degrees of freedom
ν =

s2
1
n1 + s2
2
n2
2
(s2
1/n1)2
n1−1
+ (s2
2/n2)2
n2−1
.
(9.9)
Round ν to the nearest integer.
(b) Determine the critical value tα/2,ν such that Prob

T ≥tα/2,ν

= α/2.
(c) Compute the constant k = tα/2,ν ·
+
s2
1
n1
+ s2
2
n2
.
(d) An approximate 100(1 −α)% conﬁdence interval for µ1 −µ2 is given by
((x1 −x2) −k, (x1 −x2) + k).
9.6.4
Conﬁdence interval for diﬀerence in means, paired
observations
Find a 100(1 −α)% conﬁdence interval for the diﬀerence in means µ1 −µ2 if
the populations are normal and the observations are paired (dependent).
(a) Compute the paired diﬀerences x1 −y1, x2 −y2, . . . , xn −yn, the sample
mean for the diﬀerences d, and the sample variance for the diﬀerences
s2
d.
(b) Determine the critical value tα/2,n−1 such that Prob

T ≥tα/2,n−1

=
α/2.
(c) Compute the constant k = tα/2,n−1sd/√n.
(d) A 100(1 −α)% conﬁdence interval for µ1 −µ2 is given by (d −k, d + k).
9.6.5
Ratio of variances
Find a 100(1 −α)% conﬁdence interval for the ratio of variances σ2
1/σ2
2 if the
populations are normal and the samples are independent.
(a) Compute the sample variances s2
1 and s2
2.
(b) Determine the critical values Fα/2,n1−1,n2−1 and
F1−α/2,n1−2,n2−1 such that Prob

F ≥Fα/2,n1−1,n2−1

= 1 −α/2 and
Prob

F ≤F1−α/2,n1−2,n2−1

= α/2.
(c) Compute the constants k1 = 1/Fα/2,n1−1,n2−1 and
k2 = 1/F1−α/2,n1−1,n2−1.
(d) A 100(1 −α)% conﬁdence interval for σ2
1
σ2
2 is
s2
1
s2
2
k1, s2
1
s2
2
k2

.
9.6.6
Diﬀerence in success probabilities
Find a 100(1 −α)% conﬁdence interval for the diﬀerence in success probabil-
ities p1 −p2 if the samples are from a binomial experiment, the sample sizes
are large, and the samples are independent.
c
⃝2000 by Chapman & Hall/CRC

(a) Compute the proportion of success for each sample .p1 and .p2.
(b) Determine the critical value zα/2 such that Prob

Z ≥zα/2

= α/2.
(c) Compute the constant k = zα/2
+
.p1(1 −.p1)
n1
+ .p2(1 −.p2)
n2
.
(d) A 100(1 −α)% conﬁdence interval for .p1 −.p2 is given by
((.p1 −.p2) −k, (.p1 −.p2) + k).
Example 9.53:
A researcher would like to compare the quality of incoming students
at a public college and a private college. One measure of the strength of a class is the
proportion of students who took an Advanced Placement test in High School. Random
samples were selected at each institution. For the public college, .p1 = 190/500 = .38
and for the private college, .p2 = 215/500 = .43. Find a 95% conﬁdence interval for the
diﬀerence in proportions of students who took an AP test in high school.
Solution:
(S1) The samples are assumed to be from binomial experiments and independent, and
the samples are large.
(S2) 1 −α = .95 ; α = .05 ; α/2 = .025 ; zα/2 = z.025 = 1.96
(S3) k = (1.96)
-
(.38)(.62)
500
+ (.43)(.57)
500
= .0608
(S4) A 95% conﬁdence interval for p1−p2: ((.p1−.p2)−k, (.p1−.p2)+k) = (−.1108, .0108)
9.6.7
Diﬀerence in medians
The following technique, based on the Mann–Whitney–Wilcoxon procedure,
may be used to ﬁnd an approximate 100(1 −α)% conﬁdence interval for the
diﬀerence in medians, ˜µ1 −˜µ2. Assume the sample sizes are large and the
samples are independent.
(1) Compute the order statistics {w(1), w(2), . . . , w(N)} for the N = n1n2
diﬀerences xi −yj, for 1 ≤i ≤n1 and 1 ≤j ≤n2.
(2) Determine the critical value zα/2 such that Prob

Z ≥zα/2

= α/2.
(3) Compute the constants
k1 =
9
n1n2
2
+ 0.5 −zα/2
-
n1n2(n1 + n2 + 1)
12
:
and
k2 =
;
n1n2
2
+ 0.5 + zα/2
-
n1n2(n1 + n2 + 1)
12
<
.
(4) An approximate 100(1 −α)% conﬁdence interval for ˜µ1 −˜µ2 is given by
(w(k1), w(k2)).
c
⃝2000 by Chapman & Hall/CRC

9.7
FINITE POPULATION CORRECTION FACTOR
Suppose a sample of size n is taken without replacement from a (ﬁnite) popu-
lation of size N. If n is large or a signiﬁcant portion of the population then, in-
tuitively, a point estimate based on this sample should be more accurate than
if the population were inﬁnite. In such cases, therefore, the standard devia-
tion of the sample mean and the standard deviation of the sample proportion
are corrected (multiplied) by the ﬁnite population correction factor:
-
N −n
N −1 .
(9.10)
When constructing a conﬁdence interval, the critical distance is multiplied
by this function of n and N to yield a more accurate interval estimate. If
the sample size is less than 5% of the total population, the ﬁnite population
correction factor is usually not applied.
Conﬁdence intervals constructed using the ﬁnite population correction factor:
(1) Suppose a random sample of size n is taken from a population of size N.
If the population is assumed normal, the endpoints for a 100(1 −α)%
conﬁdence interval for the population mean µ are
x ± zα/2 ·
s
√n ·
-
N −n
N −1 .
(9.11)
(2) In a binomial experiment, suppose a random sample of size n is taken
from a population of size N. The endpoints for a 100(1−α)% conﬁdence
interval for the population proportion p are
.p ± zα/2 ·
-
.p(1 −.p)
n
·
-
N −n
N −1 .
(9.12)
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 10
Hypothesis Testing
Contents
10.1
Introduction
10.1.1
Tables
10.2
The Neyman–Pearson lemma
10.3
Likelihood ratio tests
10.4
Goodness of ﬁt test
10.5
Contingency tables
10.6
Bartlett’s test
10.6.1
Approximate test procedure
10.6.2
Tables for Bartlett’s test
10.7
Cochran’s test
10.7.1
Tables for Cochran’s test
10.8
Number of observations required
10.9
Critical values for testing outliers
10.10
Signiﬁcance test in 2 × 2 contingency tables
10.11
Determining values in Bernoulli trials
10.1
INTRODUCTION
A hypothesis test is a formal procedure used to investigate a claim about
one or more population parameters. Using the information in the sample the
claim is either rejected or not rejected.
There are four parts to every hypothesis test:
(1) The null hypothesis, H0, is a claim about the value of one or more
population parameters; assumed to be true.
(2) The alternative, or (research), hypothesis, Ha, is an opposing state-
ment; believed to be true if the null hypothesis is false.
(3) The test statistic, TS, is a quantity computed from the sample and
used to decide whether or not to reject the null hypothesis.
(4) The rejection region, RR, is a set or interval of numbers selected in
such a way that if the value of the test statistic lies in the rejection region
the null hypothesis is rejected. One or more critical values separate
the rejection region from the remaining values of the test statistic.
c
⃝2000 by Chapman & Hall/CRC

Decision
Do not
reject H0
Reject H0
Nature
H0 True
Correct decision
Type I error: α
H0 False
Type II error: β
Correct decision
Table 10.1: Hypothesis test errors
There are two error probabilities associated with hypothesis testing; they are
illustrated in Table 10.1 and described below.
(1) A type I error occurs if the null hypothesis is rejected when it is really
true. The probability of a type I error is usually denoted by α, so that
Prob [type I error] = α. Common values of α include 0.05, 0.01, and
0.001.
(2) A type II error occurs if the null hypothesis is accepted when it is
really false. The probability of a type II error depends upon the true
value of the population parameter(s) and is usually denoted by β (or
β(θ)), so that Prob [type II error] = β. The power of the hypothesis
test is 1 −α.
Note:
(1) α is the signiﬁcance level of the hypothesis test. The test statistic is
signiﬁcant if it lies in the rejection region.
(2) The values α and β are inversely related, that is, when α increases then
β decreases, and conversely.
(3) To decrease both α and β, increase the sample size.
The p-value is the smallest value of α (the smallest signiﬁcance level) that
would result in rejecting the null hypothesis. A p-value for a hypothesis test
is often reported rather than whether or not the value of the test statistic lies
in the rejection region.
10.1.1
Tables
Tables 10.2 and 10.3 contain hypothesis tests for one and two samples. The
small numbers on the right-hand side of each table are for referencing these
tests.
Example 10.54:
A breakfast cereal manufacturer claims each box is ﬁlled with 24
ounces of cereal. To check this claim, a consumer group randomly selected 17 boxes
and carefully weighed the contents. The summary statistics: x = 23.55 and s = 1.5. Is
there any evidence to suggest the cereal boxes are underﬁlled? Use α = .05.
Solution:
(S1) This is a question about a population mean µ. The distribution of cereal box
weights is assumed normal and the population variance is unknown. A one-sample
c
⃝2000 by Chapman & Hall/CRC

Null hypothesis,
Alternative
Test
Rejection
assumptions
hypotheses
statistic
regions
µ = µ0,
µ > µ0
Z = X −µ0
σ/√n
Z ≥zα
(1)
n large, σ2 known, or
µ < µ0
Z ≤−zα
(2)
normality, σ2 known
µ ̸= µ0
|Z| ≥zα/2
(3)
µ = µ0,
µ > µ0
T = X −µ0
S/√n
T ≥tα,n−1
(4)
normality,
µ < µ0
T ≤−tα,n−1
(5)
σ2 unknown
µ ̸= µ0
|T| ≥tα/2,n−1
(6)
σ2 = σ2
0,
σ2 > σ2
0
χ2 = (n −1)S2
σ2
0
χ2 ≥χ2
α,n−1
(7)
normality
σ2 < σ2
0
χ2 ≤χ2
1−α,n−1
(8)
σ2 ̸= σ2
0
χ2 ≤χ2
1−α/2,n−1, or
χ2 ≥χ2
α/2,n−1
(9)
p = p0,
p > p0
Z =
.p −p0

p0(1 −p0)/n
Z ≥zα
(10)
binomial experiment,
p < p0
Z ≤−zα
(11)
n large
p ̸= p0
|Z| ≥zα/2
(12)
Table 10.2: Hypothesis tests: one sample
t test is appropriate (Table 10.2, number (5)).
(S2) The four parts to the hypothesis test are:
H0: µ = 24 = µ0
Ha: µ < 24
TS: T = X −µ0
S/√n
RR: T ≤−tα,n−1 = −t.05,16 = −1.7459
(S3) T = 23.55 −24
1.5/
√
17
= −1.2369
(S4) Conclusion: The value of the test statistic does not lie in the rejection region
(equivalently, p = .1170 > .05). There is no evidence to suggest the population
mean is less than 24 ounces.
Example 10.55:
A newspaper article claimed the proportion of local residents in favor
of a property tax increase to fund new educational programs is .45. A school board
member selected 192 random residents and found 65 were in favor of the tax increase.
Is there any evidence to suggest the proportion reported in the newspaper article is
wrong? Use α = 0.1.
Solution:
(S1) This is a question about a population proportion p. A binomial experiment is
assumed and n is large. A one-sample test based on a Z statistic is appropriate
(Table 10.2, number (12)).
(S2) The four parts to the hypothesis test are:
c
⃝2000 by Chapman & Hall/CRC

Assumptions
Null
Alternative
Test
Rejection
hypothesis
hypotheses
statistic
regions
n1, n2 large, independence, σ2
1, σ2
2 known, or
normality, independence, σ2
1, σ2
2 known
µ1 −µ2 = ∆0
µ1 −µ2 > ∆0
Z = (X1 −X2) −∆0
-
σ2
1
n1 +
σ2
2
n2
Z ≥zα
(1)
µ1 −µ2 < ∆0
Z ≤−zα
(2)
µ1 −µ2 ̸= ∆0
|Z| ≥zα/2
(3)
normality, independence, σ2
1 = σ2
2 unknown
µ1 −µ2 = ∆0
µ1 −µ2 > ∆0
T = (X1 −X2) −∆0
Sp

1
n1 +
1
n2
T ≥tα,n1+n2−2
(4)
µ1 −µ2 < ∆0
T ≤−tα,n1+n2−2
(5)
µ1 −µ2 ̸= ∆0
Sp = (n1−1)S2
1 + (n2−1)S2
2
n1 + n2 −2
|T| ≥t α
2 ,n1+n2−2
(6)
normality, independence, σ2
1, σ2
2 unknown, σ2
1 ̸= σ2
2
µ1 −µ2 = ∆0
µ1 −µ2 > ∆0
T ′ = (X1 −X2) −∆0
-
S2
1
n1 +
S2
2
n2
T ′ ≥tα,ν
(7)
µ1 −µ2 < ∆0
T ′ ≤−tα,ν
(8)
µ1 −µ2 ̸= ∆0
ν ≈

s2
1
n1 + s2
2
n2
2
(s2
1/n1)2
n1−1
+
(s2
2/n2)2
n2−1
|T ′| ≥tα/2,ν
(9)
normality, n pairs, dependence
µD = ∆0
µD > ∆0
T = D −∆0
SD/√n
T ≥tα,n−1
(10)
µD < ∆0
T ≤−tα,n−1
(11)
µD ̸= ∆0
|T| ≥tα/2,n−1
(12)
normality, independence
σ2
1 = σ2
2
σ2
1 > σ2
2
F ≥Fα,n1−1,n2−1
(13)
σ2
1 < σ2
2
F = S2
1/S2
2
F ≤F1−α,n1−1,n2−1
(14)
σ2
1 ̸= σ2
2
F ≤F1−α
2 ,n1−1,n2−1
or
F ≥F α
2 ,n1−1,n2−1
(15)
binomial experiments, n1, n2 large, independence
p1 = p2 = 0
p1 −p2 > 0
Z =
.p1 −.p2

.p.q(1/n1 + 1/n2)
Z ≥zα
(16)
p1 −p2 < 0
Z ≤−zα
(17)
p1 −p2 ̸= 0
.p = X1 + X2
n1 + n2
,
.q = 1 −.p
|Z| ≥zα/2
(18)
binomial experiments, n1, n2 large, independence
p1 −p2 = ∆0
p1 −p2 > ∆0
Z =
(.p1 −.p2) −∆0

.p1(1−.p1)
n1
+ .p2(1−.p2)
n2
Z ≥zα
(19)
p1 −p2 < ∆0
Z ≤−zα
(20)
p1 −p2 ̸= ∆0
|Z| ≥zα/2
(21)
Table 10.3: Hypothesis tests: two samples
c
⃝2000 by Chapman & Hall/CRC

H0: p = .45 = p0
Ha: p ̸= .45
TS: Z =
.p −p0

p0(1 −p0)/n
RR: |Z| ≥zα/2 = z.005 = 2.5758
(S3) .p = 65
192 = .3385 ;
Z =
.3385 −.45

(.3385)(.6615)/192
= −3.1044
(S4) Conclusion: The value of the test statistic lies in the rejection region (equivalently,
p = .0019 < .005). There is evidence to suggest the true proportion of residents
in favor of the property tax increase is diﬀerent from .45.
Example 10.56:
An automobile parts seller claims a new product when attached
to an engine’s air ﬁlter will signiﬁcantly improve gas mileage.
To test this claim, a
consumer group randomly selected 10 cars and drivers. The miles per gallon for each
automobile was recorded without the product and then using the new product. The
summary statistics for the diﬀerences (before −after) were: d = −1.2 and sD = 3.5. Is
there any evidence to suggest the new product improves gas mileage? Use α = .01.
Solution:
(S1) This is a question about a diﬀerence in population means, µD. The data are
assumed to be from a normal distribution and the observations are dependent. A
paired t test is appropriate (Table 10.3, number (5)).
(S2) The four parts to the hypothesis test are:
H0: µD = 0 = ∆0
Ha: µD < 0
TS: T = D −∆0
Sd/√n
RR: T ≤−tα,n−1 = −t.01,9 = −2.8214
(S3) T = −1.2 −0
3.5/
√
10
= −1.0842
(S4) Conclusion: The value of the test statistic does not lie in the rejection region
(equivalently, p = .1532 > .01). There is no evidence to suggest the new product
improves gas mileage.
10.2
THE NEYMAN–PEARSON LEMMA
Given the null hypothesis H0: θ = θ0 versus the alternative hypothesis Ha:
θ = θa, let L(θ) be the likelihood function evaluated at θ. For a given α, the
test that maximizes the power at θa has a rejection region determined by
L(θ0)
L(θa) < k
(10.1)
This statistical test is the most powerful test of H0 versus Ha.
c
⃝2000 by Chapman & Hall/CRC

10.3
LIKELIHOOD RATIO TESTS
Given the null hypothesis H0: θ ∈Ω0 versus the alternative hypothesis Ha:
θ ∈Ωa with Ω0 ∩Ωa = φ and Ω= Ω0 ∪Ωa. Let L(.Ω0) be the likelihood
function with all unknown parameters replaced by their maximum likelihood
estimators subject to the constraint θ ∈Ω0, and let L(.Ω) be deﬁned similarly
so that θ ∈Ω. Deﬁne
λ = L(.Ω0)
L(.Ω)
.
(10.2)
A likelihood ratio test of H0 versus Ha uses λ as a test statistic and has a
rejection region given by λ ≤k (for 0 < k < 1).
Under very general conditions and for large n, −2 ln λ has approximately
a chi–square distribution with degrees of freedom equal to the number of
parameters or functions of parameters assigned speciﬁc values under H0.
10.4
GOODNESS OF FIT TEST
Let ni be the number of observations falling into the ith category (for i =
1, 2, . . . , k) and let n = n1 + n2 + · · · + nk.
H0: p1 = p10, p2 = p20, . . . , pk = pk0
Ha: pi ̸= pi0 for at least one i
TS: χ2 =
k

i=1
(observed −estimated expected)2
estimated expected
=
k

i=1
(ni −npi0)2
npi0
Under the null hypothesis χ2 has approximately a chi–square dis-
tribution with k −1 degrees of freedom. The approximation is
satisfactory if n pi0 ≥5 for all i.
RR: χ2 ≥χ2
α,k−1
Example 10.57:
The bookstore at a large university stocks four brands of graphing
calculators.
Recent sales ﬁgures indicated 55% of all graphing calculator sales were
Texas Instruments (TI), 25% were Hewlett Packard (HP), 15% were Casio, and 5%
were Sharp. This semester 200 graphing calculators were sold according to the table
given below. Is there any evidence to suggest the sales proportions have changed? Use
α = .05.
Calculator Sales
TI
HP
Casio
Sharp
120
47
21
12
c
⃝2000 by Chapman & Hall/CRC

Solution:
(S1) There are k = 4 categories (of calculators) with unequal expected frequencies.
The bookstore would like to determine if sales are consistent with previous results.
This problem involves a goodness of ﬁt test based on a chi–square distribution.
(S2) The four parts to the hypothesis test are:
H0: p1 = .55, p2 = .25, p3 = .15, p4 = .05.
Ha: pi ̸= pi0 for at least one i
TS: χ2 =
4

i=1
(observed −estimated expected)2
estimated expected
=
4

i=1
(ni −npi0)2
npi0
RR: χ2 ≥χ2
α,k−1 = χ2
.05,3 = 7.8147
(S3) χ2 = (120 −110)2
110
+ (50 −47)2
50
+ (30 −21)2
30
+ 10 −12)2
10
= 4.1891
(S4) Conclusion: The value of the test statistic does not lie in the rejection region
(equivalently, p = .2418 > .05). There is no evidence to suggest the proportions
of graphing calculator sales have changed.
If k = 2, this test is equivalent to a one proportion Z test, Table 10.2, number
(3). This result follows from section 6.18.3 (page 149): If Z is a standard
normal random variable, then Z2 has a chi–square distribution with 1 degree
of freedom.
10.5
CONTINGENCY TABLES
The general I × J contingency table has the form:
Treatment 1
Treatment 1
. . .
Treatment J
Totals
Sample 1
n11
n12
. . .
n1J
n1.
Sample 2
n21
n22
. . .
n2J
n2.
...
...
...
...
...
...
Sample I
nI1
nI2
. . .
nIJ
nI.
Totals
n.1
n.2
. . .
n.J
n
where nk. = J
j=1 nkj and n.k = I
i=1 nik.
If complete independence is
assumed, then the probability of any speciﬁc conﬁguration, given the row and
column totals {n.k, nk.}, is
Prob [n11, . . . , nIJ | n1., . . . , n.J] = (ΠI
i ni.!)(ΠJ
j n.j!)
n! ΠI
i ΠJ
j nij!
(10.3)
Let a contingency table contain I rows and J columns, let nij be the count in
the (i, j)th cell, and let .ϵij be the estimated expected count in that cell. The
test statistic is
χ2 =

all cells
(observed −estimated expected)2
estimated expected
=
I

i=1
J

j=1
(nij −ˆϵij)2
ˆϵij
(10.4)
c
⃝2000 by Chapman & Hall/CRC

where
ˆϵij = (ith row total)(jth column total)
grand total
= ni.n.j
n
(10.5)
Under the null hypothesis χ2 has approximately a chi–square distribution with
(I −1)(J −1) degrees of freedom. The approximation is satisfactory if ˆϵij ≥5
for all i and j.
Example 10.58:
Recent reports indicate meals served during ﬂights are rated similar
regardless of airline. A survey given to randomly selected passengers asked each to rate
the quality of in-ﬂight meals. The results are given in the table below.
Airline
A
B
C
D
Poor
42
35
22
23
Acceptable
50
75
33
28
Good
10
17
21
18
Is there any evidence to suggest the quality of meals diﬀers by airline? Use α = .01.
Solution:
(S1) The contingency table has I = 3 rows and J = 4 columns. To determine if the
meal ratings diﬀer by airline, a contingency table analysis is appropriate. The test
statistic is based on a chi–square distribution.
(S2) The four parts to the hypothesis test are:
H0: Airline and meal ratings are independent
Ha: Airline and meal ratings are dependent
TS: χ2 =
3

i=1
4

j=1
(nij −ˆϵij)2
ˆϵij
RR: χ2 ≥χ2
.01,6 = 18.5476
(S3) χ2 = (42 −33.27)2
33.27
+ (35 −41.43)2
41.43
+ (22 −24.79)2
24.79
+ (23 −22.51)2
22.51
+ (50 −50.73)2
50.73
+ (75 −63.16)2
63.16
+ (33 −37.80)2
37.80
+ (28 −34.32)2
34.32
+ (10 −18.00)2
18.00
+ (17 −22.41)2
22.41
+ (21 −13.41)2
13.41
+ (18 −12.18)2
12.18
= 19.553
(S4) The value of the test statistic lies in the rejection region (equivalently, p = .003 <
.01). There is evidence to suggest the meal rating proportions diﬀer by airline.
10.6
BARTLETT’S TEST
Let there be k independent samples with ni (for i = 1, 2, . . . , k) observations
in each sample, N = n1 +n2 +· · ·+nk, and let S2
i be the ith sample variance.
c
⃝2000 by Chapman & Hall/CRC

H0: σ2
1 = σ2
2 = · · · = σ2
k
Ha: the variances are not all equal
TS: B =

(S2
1)n1−1(S2
2)n2−1 · · · (S2
k)nk−11/(N−k)
S2p
S2
p =
k
i=1
(ni −1)S2
i
N −k
RR: B ≤bα,k,n
(n1 = n2 = · · · = nk = n)
B ≤bα,k,n1,n2,...,nk
(when sample sizes are unequal)
where bα,k,n1,n2,...,nk ≈n1bα,k,n1 + n2bα,k,n2 + · · · + nkbα,k,nk
N
Here bα,k,n is a critical value for Bartlett’s test with α being the signiﬁcance
level, k is the number of populations, and n is the sample size from each
population. A table of values is in section 10.6.2.
10.6.1
Approximate test procedure
Let νi = ni −1
TS: χ2 = M/C
where
M =
 k

i=1
νi

ln S
2 −
k

i=1
νi ln S2
i ,
S
2 =
k

i=1
νiS2
i
= k

i=1
νi
C = 1 +
1
3(k −1)
% k

i=1
1
νi
−

1
= k

i=1
νi
&
Under the null hypothesis χ2 has approximately a chi–square dis-
tribution with k −1 degrees of freedom.
RR: χ2 ≥χ2
α,k−1
10.6.2
Tables for Bartlett’s test
These tables contain critical values, bα,k,n, for Bartlett’s test where α is the
signiﬁcance level, k is the number of populations, and n is the sample size
from each population. These tables are from D. D. Dyer and J. P. Keating,
“On the Determination of Critical Values for Bartlett’s Test”, JASA, Volume
75, 1980, pages 313–319.
Reprinted with permission from the Journal of
American Statistical Association. Copyright 1980 by the American Statistical
Association. All rights reserved.
c
⃝2000 by Chapman & Hall/CRC

Critical values for Bartlett’s test, bα,k,n
α = .05
k
n
2
3
4
5
6
7
8
9
10
3
.3123
.3058
.3173
.3299
∗
∗
∗
∗
∗
4
.4780
.4699
.4803
.4921
.5028
.5122
.5204
.5277
.5341
5
.5845
.5762
.5850
.5952
.6045
.6126
.6197
.6260
.6315
6
.6563
.6483
.6559
.6646
.6727
.6798
.6860
.6914
.6961
7
.7075
.7000
.7065
.7142
.7213
.7275
.7329
.7376
.7418
8
.7456
.7387
.7444
.7512
.7574
.7629
.7677
.7719
.7757
9
.7751
.7686
.7737
.7798
.7854
.7903
.7946
.7984
.8017
10
.7984
.7924
.7970
.8025
.8076
.8121
.8160
.8194
.8224
11
.8175
.8118
.8160
.8210
.8257
.8298
.8333
.8365
.8392
12
.8332
.8280
.8317
.8364
.8407
.8444
.8477
.8506
.8531
13
.8465
.8415
.8450
.8493
.8533
.8568
.8598
.8625
.8648
14
.8578
.8532
.8564
.8604
.8641
.8673
.8701
.8726
.8748
15
.8676
.8632
.8662
.8699
.8734
.8764
.8790
.8814
.8834
16
.8761
.8719
.8747
.8782
.8815
.8843
.8868
.8890
.8909
17
.8836
.8796
.8823
.8856
.8886
.8913
.8936
.8957
.8975
18
.8902
.8865
.8890
.8921
.8949
.8975
.8997
.9016
.9033
19
.8961
.8926
.8949
.8979
.9006
.9030
.9051
.9069
.9086
20
.9015
.8980
.9003
.9031
.9057
.9080
.9100
.9117
.9132
21
.9063
.9030
.9051
.9078
.9103
.9124
.9143
.9160
.9175
22
.9106
.9075
.9095
.9120
.9144
.9165
.9183
.9199
.9213
23
.9146
.9116
.9135
.9159
.9182
.9202
.9219
.9235
.9248
24
.9182
.9153
.9172
.9195
.9217
.9236
.9253
.9267
.9280
25
.9216
.9187
.9205
.9228
.9249
.9267
.9283
.9297
.9309
26
.9246
.9219
.9236
.9258
.9278
.9296
.9311
.9325
.9336
27
.9275
.9249
.9265
.9286
.9305
.9322
.9337
.9350
.9361
28
.9301
.9276
.9292
.9312
.9330
.9347
.9361
.9374
.9385
29
.9326
.9301
.9316
.9336
.9354
.9370
.9383
.9396
.9406
30
.9348
.9325
.9340
.9358
.9376
.9391
.9404
.9416
.9426
40
.9513
.9495
.9506
.9520
.9533
.9545
.9555
.9564
.9572
50
.9612
.9597
.9606
.9617
.9628
.9637
.9645
.9652
.9658
60
.9677
.9665
.9672
.9681
.9690
.9698
.9705
.9710
.9716
80
.9758
.9749
.9754
.9761
.9768
.9774
.9779
.9783
.9787
100
.9807
.9799
.9804
.9809
.9815
.9819
.9823
.9827
.9830
c
⃝2000 by Chapman & Hall/CRC

Critical values for Bartlett’s test, bα,k,n
α = .01
k
n
2
3
4
5
6
7
8
9
10
3
.1411
.1672
∗
∗
∗
∗
∗
∗
∗
4
.2843
.3165
.3475
.3729
.3937
.4110
∗
∗
∗
5
.3984
.4304
.4607
.4850
.5046
.5207
.5343
.5458
.5558
6
.4850
.5149
.5430
.5653
.5832
.5978
.6100
.6204
.6293
7
.5512
.5787
.6045
.6248
.6410
.6542
.6652
.6744
.6824
8
.6031
.6282
.6518
.6704
.6851
.6970
.7069
.7153
.7225
9
.6445
.6676
.6892
.7062
.7197
.7305
.7395
.7471
.7536
10
.6783
.6996
.7195
.7352
.7475
.7575
.7657
.7726
.7786
11
.7063
.7260
.7445
.7590
.7703
.7795
.7871
.7935
.7990
12
.7299
.7483
.7654
.7789
.7894
.7980
.8050
.8109
.8160
13
.7501
.7672
.7832
.7958
.8056
.8135
.8201
.8256
.8303
14
.7674
.7835
.7985
.8103
.8195
.8269
.8330
.8382
.8426
15
.7825
.7977
.8118
.8229
.8315
.8385
.8443
.8491
.8532
16
.7958
.8101
.8235
.8339
.8421
.8486
.8541
.8586
.8625
17
.8076
.8211
.8338
.8436
.8514
.8576
.8627
.8670
.8707
18
.8181
.8309
.8429
.8523
.8596
.8655
.8704
.8745
.8780
19
.8275
.8397
.8512
.8601
.8670
.8727
.8773
.8811
.8845
20
.8360
.8476
.8586
.8671
.8737
.8791
.8835
.8871
.8903
21
.8437
.8548
.8653
.8734
.8797
.8848
.8890
.8926
.8956
22
.8507
.8614
.8714
.8791
.8852
.8901
.8941
.8975
.9004
23
.8571
.8673
.8769
.8844
.8902
.8949
.8988
.9020
.9047
24
.8630
.8728
.8820
.8892
.8948
.8993
.9030
.9061
.9087
25
.8684
.8779
.8867
.8936
.8990
.9034
.9069
.9099
.9124
26
.8734
.8825
.8911
.8977
.9029
.9071
.9105
.9134
.9158
27
.8781
.8869
.8951
.9015
.9065
.9105
.9138
.9166
.9190
28
.8824
.8909
.8988
.9050
.9099
.9138
.9169
.9196
.9219
29
.8864
.8946
.9023
.9083
.9130
.9167
.9198
.9224
.9246
30
.8902
.8981
.9056
.9114
.9159
.9195
.9225
.9250
.9271
40
.9175
.9235
.9291
.9335
.9370
.9397
.9420
.9439
.9455
50
.9339
.9387
.9433
.9468
.9496
.9518
.9536
.9551
.9564
60
.9449
.9489
.9527
.9557
.9580
.9599
.9614
.9626
.9637
80
.9586
.9617
.9646
.9668
.9685
.9699
.9711
.9720
.9728
100
.9669
.9693
.9716
.9734
.9748
.9759
.9769
.9776
.9783
c
⃝2000 by Chapman & Hall/CRC

10.7
COCHRAN’S TEST
Let there be k independent samples with n observations in each sample, and
let S2
i be the ith sample variance (for i = 1, 2, . . . , k).
H0: σ2
1 = σ2
2 = · · · = σ2
k
Ha: the variances are not all equal
TS: G = largest S2
i
= k

i=1
S2
i
RR: G ≥gα,k,n
Here gα,k,n is a critical value for Cochran’s test with α being the signiﬁcance
level, k is the number of populations, and n is the sample size from each
population. A table of values is in section 10.7.1.
10.7.1
Tables for Cochran’s test
These tables contain critical values, gα,k,n, for Cochran’s test where α is the
signiﬁcance level, k is the number of independent estimates of variance, each
of which is based on n degrees of freedom. These tables are from C. Eisenhart,
M. W. Hastay, and W. A. Wallis, Techniques of Statistical Analysis, McGraw-
Hill Book Company, 1947, Tables 15.1 and 15.2 (pages 390-391). Reprinted
courtesy of The McGraw-Hill Companies.
c
⃝2000 by Chapman & Hall/CRC

Critical values for Cochran’s test, gα,k,n
c
⃝2000 by Chapman & Hall/CRC
α = .05
n
k
1
2
3
4
5
6
7
8
9
10
16
36
144
∞
2
.9985
.9750
.9392
.9057
.8772
.8534
.8332
.8159
.8010
.7880
.7341
.6602
.5813
.5000
3
.9669
.8709
.7977
.7457
.7071
.6771
.6530
.6333
.6167
.6025
.5466
.4748
.4031
.3333
4
.9065
.7679
.6841
.6287
.5895
.5598
.5365
.5175
.5017
.4884
.4366
.3720
.3093
.2500
5
.8412
.6838
.5981
.5441
.5065
.4783
.4564
.4387
.4241
.4118
.3645
.3066
.2513
.2000
6
.7808
.6161
.5321
.4803
.4447
.4184
.3980
.3817
.3682
.3568
.3135
.2612
.2119
.1667
7
.7271
.5612
.4800
.4307
.3974
.3726
.3535
.3384
.3259
.3154
.2756
.2278
.1833
.1429
8
.6798
.5157
.4377
.3910
.3595
.3362
.3185
.3043
.2926
.2829
.2462
.2022
.1616
.1250
9
.6385
.4775
.4027
.3584
.3286
.3067
.2901
.2768
.2659
.2568
.2226
.1820
.1446
.1111
10
.6020
.4450
.3733
.3311
.3029
.2823
.2666
.2541
.2439
.2353
.2032
.1655
.1308
.1000
12
.5410
.3924
.3264
.2880
.2624
.2439
.2299
.2187
.2098
.2020
.1737
.1403
.1100
.0833
15
.4709
.3346
.2758
.2419
.2195
.2034
.1911
.1815
.1736
.1671
.1429
.1144
.0889
.0667
20
.3894
.2705
.2205
.1921
.1735
.1602
.1501
.1422
.1357
.1303
.1108
.0879
.0675
.0500
24
.3434
.2354
.1907
.1656
.1493
.1374
.1286
.1216
.1160
.1113
.0942
.0743
.0567
.0417
30
.2929
.1980
.1593
.1377
.1237
.1137
.1061
.1002
.0958
.0921
.0771
.0604
.0457
.0333
40
.2370
.1576
.1259
.1082
.0968
.0887
.0827
.0780
.0745
.0713
.0595
.0462
.0347
.0250
60
.1737
.1131
.0895
.0765
.0682
.0623
.0583
.0552
.0520
.0497
.0411
.0316
.0234
.0167
120
.0998
.0632
.0495
.0419
.0371
.0337
.0312
.0292
.0279
.0266
.0218
.0165
.0120
.0083
∞
0
0
0
0
0
0
0
0
0
0
0
0
0
0

Critical values for Cochran’s test, gα,k,n
c
⃝2000 by Chapman & Hall/CRC
α = .05
n
k
1
2
3
4
5
6
7
8
9
10
16
36
144
∞
2
.9999
.9950
.9794
.9586
.9373
.9172
.8988
.8823
.8674
.8539
.7949
.7067
.6062
.5000
3
.9933
.9423
.8831
.8335
.7933
.7606
.7335
.7107
.6912
.6743
.6059
.5153
.4230
.3333
4
.9676
.8643
.7814
.7212
.6761
.6410
.6129
.5897
.5702
.5536
.4884
.4057
.3251
.2500
5
.9279
.7885
.6957
.6329
.5875
.5531
.5259
.5037
.4854
.4697
.4094
.3351
.2644
.2000
6
.8828
.7218
.6258
.5635
.5195
.4866
.4608
.4401
.4229
.4084
.3529
.2858
.2229
.1667
7
.8376
.6644
.5685
.5080
.4659
.4347
.4105
.3911
.3751
.3616
.3105
.2494
.1929
.1429
8
.7945
.6152
.5209
.4627
.4226
.3932
.3704
.3522
.3373
.3248
.2779
.2214
.1700
.1250
9
.7544
.5727
.4810
.4251
.3870
.3592
.3378
.3207
.3067
.2950
.2514
.1992
.1521
.1111
10
.7175
.5358
.4469
.3934
.3572
.3308
.3106
.2945
.2813
.2704
.2297
.1811
.1376
.1000
12
.6528
.4751
.3919
.3428
.3099
.2861
.2680
.2535
.2419
.2320
.1961
.1535
.1157
.0833
15
.5747
.4069
.3317
.2882
.2593
.2386
.2228
.2104
.2002
.1918
.1612
.1251
.0934
.0667
20
.4799
.3297
.2654
.2288
.2048
.1877
.1748
.1646
.1567
.1501
.1248
.0960
.0709
.0500
24
.4247
.2871
.2295
.1970
.1759
.1608
.1495
.1406
.1338
.1283
.1060
.0810
.0595
.0417
30
.3632
.2412
.1913
.1635
.1454
.1327
.1232
.1157
.1100
.1054
.0867
.0658
.0480
.0333
40
.2940
.1915
.1508
.1281
.1135
.1033
.0957
.0898
.0853
.0816
.0668
.0503
.0363
.0250
60
.2151
.1371
.1069
.0902
.0796
.0722
.0668
.0625
.0594
.0567
.0461
.0344
.0245
.0167
120
.1225
.0759
.0585
.0489
.0429
.0387
.0357
.0334
.0316
.0302
.0242
.0178
.0125
.0083
∞
0
0
0
0
0
0
0
0
0
0
0
0
0
0

10.8
NUMBER OF OBSERVATIONS REQUIRED FOR THE
COMPARISON OF A POPULATION VARIANCE WITH A
STANDARD VALUE USING THE CHI–SQUARE TEST
Suppose x1, x2, . . . , xn+1 is a random sample from a population with variance
sigma2
1. The sample variance, s2
1 has n degrees of freedom, and may be used
to test the hypothesis that σ2
1 = σ2
0. Let R be the ratio of the variances σ2
0
and σ2
1. The table below shows the value of the ratio R for which a chi-square
test, with signiﬁcance level α, will not be able to detect the diﬀerence in the
variances with probability β. Note that when R is far from one few samples
will be required to distinguish σ2
0 from σ2
1, while for R near one large samples
will be required.
Example 10.59:
Testing for an increase in variance. Let α = 0.05, β = 0.01, and
R = 4. Using the table below with these values the value R = 4 occurs between the
rows corresponding to n = 15 and n = 20. Using rough, linear, interpolation, the table
indicates that the estimate of variance should be based on 19 degrees of freedom.
Example 10.60:
Testing for an decrease in variance. Let α = 0.05, β = 0.01, and
R = 0.33. Using the table below with α′ = β = 0.01, β′ = α = 0.05 and R′ = 1/R = 3,
the value R = 3 occurs between the rows corresponding to n = 24 and n = 30. Using
rough, linear, interpolation, the table indicates that the estimate of variance should be
based on 26 degrees of freedom.
Values of R given n, α, and β
α = 0.01
α = 0.05
n
β = 0.01 β = 0.05 β = 0.1 β = 0.5
β = 0.01 β = 0.05 β = 0.1 β = 0.5
1 42236.852 1687.350 420.176
14.584 24454.206
976.938 243.272
8.444
2
458.211
89.781
43.709
6.644
298.073
58.404
28.433
4.322
3
98.796
32.244
19.414
4.795
68.054
22.211
13.373
3.303
4
44.686
18.681
12.483
3.955
31.933
13.349
8.920
2.827
5
27.217
13.170
9.369
3.467
19.972
9.665
6.875
2.544
6
19.278
10.280
7.627
3.144
14.438
7.699
5.713
2.354
7
14.911
8.524
6.521
2.911
11.353
6.490
4.965
2.217
8
12.202
7.352
5.757
2.736
9.418
5.675
4.444
2.112
9
10.377
6.516
5.198
2.597
8.103
5.088
4.059
2.028
10
9.072
5.890
4.770
2.484
7.156
4.646
3.763
1.960
15
5.847
4.211
3.578
2.133
4.780
3.442
2.925
1.743
20
4.548
3.462
3.019
1.943
3.803
2.895
2.524
1.624
25
3.845
3.033
2.690
1.821
3.267
2.577
2.286
1.547
30
3.403
2.752
2.471
1.735
2.927
2.367
2.125
1.492
40
2.874
2.403
2.192
1.619
2.516
2.103
1.919
1.418
50
2.564
2.191
2.021
1.544
2.272
1.942
1.791
1.368
75
2.150
1.898
1.779
1.431
1.945
1.716
1.609
1.294
100
1.938
1.743
1.649
1.367
1.775
1.596
1.510
1.252
150
1.715
1.575
1.506
1.297
1.594
1.464
1.400
1.206
∞
1.000
1.000
1.000
1.000
1.000
1.000
1.000
1.000
c
⃝2000 by Chapman & Hall/CRC

10.9
CRITICAL VALUES FOR TESTING OUTLIERS
Tests for outliers may be based on the largest deviation
max
i=1,2,...(xi −x) of the
observations from their mean (which has to be normalized by the standard
deviation or an estimate of the standard deviation). An alternative technique
is to look at ratios of approximations to the range.
(a) To determine if the smallest element in a sample, x(1), is an outlier
compute
r10 = x(2) −x(1)
x(n) −x(1)
(10.6)
Equivalently, to determine if the largest element in a sample, x(n), is an
outlier compute
r10 = x(n) −x(n−1)
x(n) −x(1)
(10.7)
(b) To determine if the smallest element in a sample, x(1), is an outlier, and
the value x(n) is not to be used, then compute
r11 =
x(2) −x(1)
x(n−1) −x(1)
(10.8)
Equivalently, to determine if the largest element in a sample, x(n), is an
outlier, without using the value x(1), compute
r11 = x(n) −x(n−1)
x(n) −x(2)
(10.9)
(c) To determine if the smallest element in a sample, x(1), is an outlier, and
the value x(2) is not to be used, then compute
r20 = x(3) −x(1)
x(n) −x(1)
(10.10)
Equivalently, to determine if the largest element in a sample, x(n), is an
outlier, without using the value x(n−1), compute
r20 = x(n) −x(n−2)
x(n) −x(2)
(10.11)
The following tables contain critical values for r10, r11, and r20. See W. J.
Dixon, Annals of Mathematical Statistics, 22, 1951, pages 68–78.
c
⃝2000 by Chapman & Hall/CRC

Percentage values for r10
(Prob [r10 > R] = α)
n
α = .005
.01
.02
.05
.10
.50
.90
.95
3
.994
.988
.976
.941
.886
.500
.114
.059
4
.926
.889
.846
.745
.679
.324
.065
.033
5
.821
.780
.729
.642
.557
.250
.048
.023
6
.740
.698
.644
.560
.482
.210
.038
.018
7
.680
.637
.586
.507
.434
.184
.032
.016
8
.634
.590
.543
.468
.399
.166
.029
.014
9
.598
.555
.510
.437
.370
.152
.026
.013
10
.568
.527
.483
.412
.349
.142
.025
.012
15
.475
.438
.399
.338
.285
.111
.019
.010
20
.425
.391
.356
.300
.252
.096
.017
.008
25
.393
.362
.329
.277
.230
.088
.015
.008
30
.372
.341
.309
.260
.215
.082
.014
.007
Percentage values for r11
(Prob [r11 > R] = α)
n
α = .005
.01
.02
.05
.10
.50
.90
.95
4
.995
.991
.981
.955
.910
.554
.131
.069
5
.937
.916
.876
.807
.728
.369
.078
.039
6
.839
.805
.763
.689
.609
.288
.056
.028
7
.782
.740
.689
.610
.530
.241
.045
.022
8
.725
.683
.631
.554
.479
.210
.037
.019
9
.677
.635
.587
.512
.441
.189
.033
.016
10
.639
.597
.551
.477
.409
.173
.030
.014
15
.522
.486
.445
.381
.323
.129
.023
.011
20
.464
.430
.392
.334
.282
.110
.019
.010
25
.426
.394
.359
.394
.255
.098
.017
.009
30
.399
.369
.336
.283
.236
.090
.016
.008
Percentage values for r20
(Prob [r20 > R] = α)
n
α = .005
.01
.02
.05
.10
.50
.90
.95
4
.996
.992
.987
.967
.935
.676
.321
.235
5
.950
.929
.901
.845
.782
.500
.218
.155
6
.865
.836
.800
.736
.670
.411
.172
.126
7
.814
.778
.732
.661
.596
.355
.144
.099
8
.746
.719
.670
.607
.545
.317
.125
.085
9
.700
.667
.627
.565
.505
.288
.114
.077
10
.664
.632
.592
.531
.474
.268
.104
.070
15
.554
.522
.486
.430
.382
.209
.079
.052
20
.494
.464
.430
.372
.333
.179
.067
.046
25
.456
.428
.395
.343
.304
.161
.060
.041
30
.428
.402
.372
.322
.285
.149
.056
.039
c
⃝2000 by Chapman & Hall/CRC

10.10
TEST OF SIGNIFICANCE IN 2 × 2 CONTINGENCY
TABLES
A 2×2 contingency table (see section 10.5) is a special case that occurs often.
Suppose n elements are simultaneously classiﬁed as having either property
1 or 2 and as having property I or II. The 2 × 2 contingency table may be
written as:
I
II
Totals
1
a
A −a
A
2
b
B −b
B
Totals
r
n −r
n
If the marginal totals are ﬁxed, the probability of a given conﬁguration may
be written as
f(a | r, A, B) =
A
a
B
b

n
r

=
A! B! r! (n −r)!
n! a! b! (A −a)! (B −b)!
(10.12)
The following tables are designed to be used in conducting a hypothesis test
concerning the diﬀerence between observed and expected frequencies in a 2×2
contingency table. For given values of a, A, and B, table entries show the
largest value of b (in bold type, with b < a) for which there is a signiﬁcant
diﬀerence (between observed and expected frequencies, or equivalently, be-
tween a/A and b/B). Critical values of b (probability levels) are presented for
α = .05, .025, .01, and .005. The tables also satisfy the following conditions:
(1) Categories 1 and 2 are determined so that A ≥B.
(2) a
A ≥b
B or, aB ≥bA.
(3) If b is less than or equal to the integer in bold type, then a/A is sig-
niﬁcantly greater than b/B (for a one tailed–test) at the probability
level (α) indicated by the column heading. For a two-tailed test the
signiﬁcance level is 2α.
(4) A dash in the body of the table indicates no 2 × 2 table may show a
signiﬁcant eﬀect at that probability level and combination of a, A, and
B.
(5) For a given r, the probability b is less than the integer in bold type is
shown in small type following an entry.
Note that as A and B get large, this test may be approximated by a two-
sample Z test of proportions.
c
⃝2000 by Chapman & Hall/CRC

Example 10.61:
In order to compare the probability of a success in two populations,
the following 2 × 2 contingency table was obtained.
Success
Failure
Totals
Sample from population 1
7
2
9
Sample from population 2
3
3
6
Totals
10
5
15
Is there any evidence to suggest the two population proportions are diﬀerent?
Use
α = .05.
Solution:
(S1) In this 2 × 2 contingency table, a = 7, A = 9, and B = 6. For α = .05 the table
entry is 1.035.
(S2) The critical value for b is 1. If b ≤1 then the null hypothesis H0: p1 = p2 is
rejected.
(S3) Conclusion: The value of the test statistic does not lie in the rejection region,
b = 3. There is no evidence to suggest the population proportions are diﬀerent.
(S4) Note there are six 2 × 2 tables with the same marginal totals as the table in this
example (that is, A = 9, B = 6, and r = 10):
9
0
1
5
8
1
2
4
7
2
3
3
6
3
4
2
5
4
5
1
4
5
6
0
Assuming independence, the probability of obtaining each of these six tables (using
equation (10.12), rounded) is {.002, .045, .24, .42, .25, .042}. That is, the ﬁrst
conﬁguration is the least likely, and the fourth conﬁguration is the most likely.
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 3 B = 3 3 0.050
−
−
−
A = 4 B = 4 4 0.014 0.014
−
−
B = 3 4 0.029
−
−
−
A = 5 B = 5 5 1.024 1.024 0.004 0.004
4 0.024 0.024
−
−
B = 4 5 1.048 0.008 0.008
−
4 0.040
−
−
−
B = 3 5 0.018 0.018
−
−
B = 2 5 0.048
−
−
−
A = 6 B = 6 6 2.030 1.008 1.008 0.001
5 1.039 0.008 0.008
−
4 0.030
−
−
−
B = 5 6 1.015 1.015 0.002 0.002
5 0.013 0.013
−
−
4 0.045
−
−
−
B = 4 6 1.033 0.005 0.005 0.005
5 0.024 0.024
−
−
B = 3 6 0.012 0.012
−
−
5 0.048
−
−
−
B = 2 6 0.036
−
−
−
A = 7 B = 7 7 3.035 2.010 1.002 1.002
6 2.049 1.014 0.002 0.002
5 1.049 0.010
−
−
4 0.035
−
−
−
B = 6 7 2.021 2.021 1.005 1.005
6 1.024 1.024 0.004 0.004
5 0.016 0.016
−
−
4 0.049
−
−
−
B = 5 7 2.045 1.010 0.001 0.001
6 1.044 0.008 0.008
−
5 0.027
−
−
−
B = 4 7 1.024 1.024 0.003 0.003
6 0.015 0.015
−
−
5 0.045
−
−
−
B = 3 7 0.008 0.008 0.008
−
6 0.033
−
−
−
B = 2 7 0.028
−
−
−
A = 8 B = 8 8 4.038 3.013 2.003 2.003
7 2.020 2.020 1.005 1.005
6 1.020 1.020 0.003 0.003
5 0.013 0.013
−
−
4 0.038
−
−
−
B = 7 8 3.026 2.007 2.007 1.001
7 2.034 1.009 1.009 0.001
6 1.030 0.006 0.006
−
5 0.019 0.019
−
−
B = 6 8 2.015 2.015 1.003 1.003
7 1.016 1.016 0.002 0.002
6 1.049 0.009 0.009
−
5 0.028
−
−
−
B = 5 8 2.035 1.007 1.007 0.001
7 1.031 0.005 0.005 0.005
a
Probability
0.05 0.025 0.01 0.005
A = 8
B = 5
6 0.016 0.016
−
−
5 0.044
−
−
−
B = 4
8 1.018 1.018 0.002 0.002
7 0.010 0.010
−
−
6 0.030
−
−
−
B = 3
8 0.006 0.006 0.006
−
7 0.024 0.024
−
−
B = 2
8 0.022 0.022
−
−
A = 9
B = 9
9 5.041 4.015 3.005 3.005
8 3.024 3.024 2.007 1.002
7 2.027 1.007 1.007 0.001
6 1.024 1.024 0.005 0.005
5 0.015 0.015
−
−
4 0.041
−
−
−
B = 8
9 4.029 3.009 3.009 2.002
8 3.041 2.013 1.003 1.003
7 2.041 1.012 0.002 0.002
6 1.035 0.007 0.007
−
5 0.020 0.020
−
−
B = 7
9 3.019 3.019 2.005 2.005
8 2.024 2.024 1.006 0.001
7 1.020 1.020 0.003 0.003
6 0.010 0.010
−
−
5 0.029
−
−
−
B = 6
9 3.044 2.011 1.002 1.002
8 2.045 1.011 0.001 0.001
7 1.034 0.006 0.006
−
6 0.017 0.017
−
−
5 0.042
−
−
−
B = 5
9 2.027 1.005 1.005 1.005
8 1.022 1.022 0.003 0.003
7 0.010 0.010
−
−
6 0.028
−
−
−
B = 4
9 1.014 1.014 0.001 0.001
8 0.007 0.007 0.007
−
7 0.021 0.021
−
−
6 0.049
−
−
−
B = 3
9 1.045 0.005 0.005 0.005
8 0.018 0.018
−
−
7 0.045
−
−
−
B = 2
9 0.018 0.018
−
−
A = 10 B = 10 10 6.043 5.016 4.005 3.002
9 4.027 3.010 3.010 2.003
8 3.032 2.011 1.003 1.003
7 2.032 1.010 1.010 0.002
6 1.027 0.005 0.005
−
5 0.016 0.016
−
−
4 0.043
−
−
−
B = 9
10 5.033 4.011 3.003 3.003
9 4.046 3.017 2.005 2.005
8 2.018 2.018 1.004 1.004
7 2.047 1.014 0.002 0.002
6 1.038 0.008 0.008
−
5 0.022 0.022
−
−
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 10 B = 8
10 4.023 4.023 3.007 2.002
9 3.030 2.009 2.009 1.002
8 2.029 1.007 1.007 0.001
7 1.022 1.022 0.004 0.004
6 0.011 0.011
−
−
5 0.029
−
−
−
B = 7
10 3.015 3.015 2.003 2.003
9 2.017 2.017 1.004 1.004
8 2.049 1.013 0.002 0.002
7 1.035 0.006 0.006
−
6 0.017 0.017
−
−
5 0.041
−
−
−
B = 6
10 3.036 2.008 2.008 1.001
9 2.034 1.007 1.007 0.001
8 1.024 1.024 0.003 0.003
7 0.010 0.010
−
−
6 0.026
−
−
−
B = 5
10 2.022 2.022 1.004 1.004
9 1.017 1.017 0.002 0.002
8 1.045 0.007 0.007
−
7 0.019 0.019
−
−
6 0.042
−
−
−
B = 4
10 1.011 1.011 0.001 0.001
9 1.040 0.005 0.005 0.005
8 0.015 0.015
−
−
7 0.035
−
−
−
B = 3
10 1.038 0.003 0.003 0.003
9 0.014 0.014
−
−
8 0.035
−
−
−
B = 2
10 0.015 0.015
−
−
9 0.045
−
−
−
A = 11 B = 11 11 7.045 6.018 5.006 4.002
10 5.030 4.011 3.004 3.004
9 4.036 3.014 2.004 2.004
8 3.039 2.014 1.004 1.004
7 2.036 1.011 0.002 0.002
6 1.030 0.006 0.006
−
5 0.018 0.018
−
−
4 0.045
−
−
−
B = 10 11 6.035 5.012 4.004 4.004
10 4.020 4.020 3.006 2.002
9 3.022 3.022 2.007 1.002
8 2.021 2.021 1.006 0.001
7 1.016 1.016 0.003 0.003
6 1.040 0.009 0.009
−
5 0.023 0.023
−
−
B = 9
11 5.026 4.008 4.008 3.002
10 4.036 3.012 2.003 2.003
9 3.037 2.012 1.003 1.003
8 2.032 1.009 1.009 0.001
7 1.024 1.024 0.004 0.004
6 0.012 0.012
−
−
5 0.030
−
−
−
B = 8
11 4.018 4.018 3.005 3.005
10 3.023 3.023 2.006 1.001
a
Probability
0.05 0.025 0.01 0.005
A = 11 B = 8
9 2.020 2.020 1.005 1.005
8 1.014 1.014 0.002 0.002
7 1.035 0.007 0.007
−
6 0.017 0.017
−
−
5 0.040
−
−
−
B = 7
11 4.043 3.011 2.002 2.002
10 3.045 2.012 1.002 1.002
9 2.036 1.009 1.009 0.001
8 1.024 1.024 0.004 0.004
7 0.010 0.010
−
−
6 0.025 0.025
−
−
B = 6
11 3.029 2.006 2.006 1.001
10 2.027 1.005 1.005 0.001
9 1.017 1.017 0.002 0.002
8 1.041 0.007 0.007
−
7 0.017 0.017
−
−
6 0.037
−
−
−
B = 5
11 2.018 2.018 1.003 1.003
10 1.013 1.013 0.001 0.001
9 1.034 0.005 0.005 0.005
8 0.013 0.013
−
−
7 0.029
−
−
−
B = 4
11 1.009 1.009 1.009 0.001
10 1.032 0.004 0.004 0.004
9 0.011 0.011
−
−
8 0.026
−
−
−
B = 3
11 1.033 0.003 0.003 0.003
10 0.011 0.011
−
−
9 0.027
−
−
−
B = 2
11 0.013 0.013
−
−
10 0.038
−
−
−
A = 12 B = 12 12 8.047 7.019 6.007 5.002
11 6.032 5.013 4.005 4.005
10 5.040 4.017 3.006 2.002
9 4.044 3.018 2.006 1.001
8 3.044 2.017 1.005 1.005
7 2.040 1.013 0.002 0.002
6 1.032 0.007 0.007
−
5 0.019 0.019
−
−
4 0.047
−
−
−
B = 11 12 7.037 6.014 5.005 5.005
11 5.023 5.023 4.008 3.002
10 4.027 3.010 3.010 2.003
9 3.027 2.009 2.009 1.002
8 2.024 2.024 1.007 0.001
7 1.018 1.018 0.003 0.003
6 1.041 0.009 0.009
−
5 0.024 0.024
−
−
B = 10 12 6.029 5.010 5.010 4.003
11 5.041 4.015 3.005 3.005
10 4.043 3.016 2.005 2.005
9 3.041 2.014 1.003 1.003
8 2.034 1.010 1.010 0.002
7 1.025 1.025 0.005 0.005
6 0.012 0.012
−
−
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 12 B = 10 5 0.030
−
−
−
B = 9
12 5.021 5.021 4.006 3.002
11 4.028 3.009 3.009 2.002
10 3.027 2.008 2.008 1.002
9 2.022 2.022 1.006 0.001
8 1.015 1.015 0.002 0.002
7 1.035 0.007 0.007
−
6 0.017 0.017
−
−
5 0.039
−
−
−
B = 8
12 5.049 4.014 3.004 3.004
11 3.017 3.017 2.004 2.004
10 3.048 2.015 1.003 1.003
9 2.037 1.010 1.010 0.001
8 1.024 1.024 0.004 0.004
7 0.010 0.010
−
−
6 0.024 0.024
−
−
B = 7
12 4.036 3.009 3.009 2.002
11 3.036 2.009 2.009 1.002
10 2.028 1.006 1.006 0.001
9 1.017 1.017 0.002 0.002
8 1.038 0.007 0.007
−
7 0.016 0.016
−
−
6 0.034
−
−
−
B = 6
12 3.025 3.025 2.005 2.005
11 2.021 2.021 1.004 1.004
10 1.012 1.012 0.002 0.002
9 1.030 0.005 0.005 0.005
8 0.011 0.011
−
−
7 0.025 0.025
−
−
6 0.050
−
−
−
B = 5
12 2.015 2.015 1.002 1.002
11 1.010 1.010 1.010 0.001
10 1.027 0.003 0.003 0.003
9 0.009 0.009 0.009
−
8 0.020 0.020
−
−
7 0.041
−
−
−
B = 4
12 2.050 1.007 1.007 0.001
11 1.026 0.003 0.003 0.003
10 0.008 0.008 0.008
−
9 0.019 0.019
−
−
8 0.038
−
−
−
B = 3
12 1.029 0.002 0.002 0.002
11 0.009 0.009 0.009
−
10 0.022 0.022
−
−
9 0.044
−
−
−
B = 2
12 0.011 0.011
−
−
11 0.033
−
−
−
A = 13 B = 13 13 9.048 8.020 7.007 6.003
12 7.034 6.014 5.005 4.002
11 6.043 5.019 4.007 3.002
10 5.048 4.021 3.008 2.002
9 4.049 3.021 2.007 1.002
8 3.048 2.019 1.005 0.001
7 2.043 1.014 0.003 0.003
6 1.034 0.007 0.007
−
a
Probability
0.05 0.025 0.01 0.005
A = 13 B = 13 5 0.020 0.020
−
−
4 0.048
−
−
−
B = 12 13 8.039 7.015 6.005 5.002
12 6.025 6.025 5.010 4.003
11 5.030 4.012 3.004 3.004
10 4.032 3.012 2.004 2.004
9 3.030 2.011 1.003 1.003
8 2.026 1.008 1.008 0.001
7 1.019 1.019 0.004 0.004
6 1.043 0.010 0.010
−
5 0.024 0.024
−
−
B = 11 13 7.031 6.011 5.003 5.003
12 6.045 5.017 4.006 3.002
11 5.049 4.020 3.007 2.002
10 4.048 3.019 2.006 1.001
9 3.044 2.016 1.004 1.004
8 2.036 1.011 0.002 0.002
7 1.026 0.005 0.005 0.005
6 0.013 0.013
−
−
5 0.030
−
−
−
B = 10 13 6.024 6.024 5.007 4.002
12 5.032 4.011 3.003 3.003
11 4.033 3.011 2.003 2.003
10 3.030 2.010 2.010 1.002
9 2.024 2.024 1.006 0.001
8 1.016 1.016 0.003 0.003
7 1.035 0.007 0.007
−
6 0.017 0.017
−
−
5 0.038
−
−
−
B = 9
13 5.017 5.017 4.005 4.005
12 4.022 4.022 3.006 2.001
11 3.020 3.020 2.006 1.001
10 3.048 2.016 1.004 1.004
9 2.036 1.010 1.010 0.001
8 1.023 1.023 0.004 0.004
7 1.048 0.010
−
−
6 0.023 0.023
−
−
5 0.049
−
−
−
B = 8
13 5.042 4.012 3.003 3.003
12 4.045 3.013 2.003 2.003
11 3.038 2.011 1.002 1.002
10 2.027 1.006 1.006 0.001
9 1.016 1.016 0.002 0.002
8 1.035 0.006 0.006
−
7 0.015 0.015
−
−
6 0.032
−
−
−
B = 7
13 4.031 3.007 3.007 2.001
12 3.029 2.007 2.007 1.001
11 2.021 2.021 1.004 1.004
10 2.048 1.012 0.002 0.002
9 1.027 0.004 0.004 0.004
8 0.010 0.010
−
−
7 0.022 0.022
−
−
6 0.044
−
−
−
B = 6
13 3.021 3.021 2.004 2.004
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05
0.025 0.01 0.005
A = 13 B = 6
12 2.017 2.017 1.003 1.003
11 2.043 1.009 1.009 0.001
10 1.023 1.023 0.003 0.003
9
1.046 0.008 0.008
−
8
0.017 0.017
−
−
7
0.034
−
−
−
B = 5
13 2.012 2.012 1.002 1.002
12 2.042 1.008 1.008 0.001
11 1.021 1.021 0.002 0.002
10 1.045 0.007 0.007
−
9
0.015 0.015
−
−
8
0.029
−
−
−
B = 4
13 2.044 1.006 1.006 0.000
12 1.022 1.022 0.002 0.002
11 0.006 0.006 0.006
−
10 0.015 0.015
−
−
9
0.029
−
−
−
B = 3
13 1.025 1.025 0.002 0.002
12 0.007 0.007 0.007
−
11 0.018 0.018
−
−
10 0.036
−
−
−
B = 2
13 0.010 0.010 0.010
−
12 0.029
−
−
−
A = 14 B = 14 14 10.049 9.020 8.008 7.003
13 8.036 7.015 6.006 5.002
12 7.045 6.021 5.008 4.003
11 5.024 5.024 4.010 3.003
10 4.025 4.025 3.010 2.003
9
3.024 3.024 2.008 1.002
8
2.021 2.021 1.006 0.001
7
2.045 1.015 0.003 0.003
6
1.036 0.008 0.008
−
5
0.020 0.020
−
−
4
0.049
−
−
−
B = 13 14 9.041 8.016 7.006 6.002
13 7.027 6.011 5.004 5.004
12 6.033 5.014 4.005 4.005
11 5.036 4.015 3.005 2.001
10 4.036 3.014 2.004 2.004
9
3.033 2.012 1.003 1.003
8
2.028 1.008 1.008 0.001
7
1.020 1.020 0.004 0.004
6
1.044 0.010
−
−
5
0.025 0.025
−
−
B = 12 14 8.033 7.012 6.004 6.004
13 7.048 6.020 5.007 4.002
12 5.023 5.023 4.008 3.003
11 4.023 4.023 3.008 2.002
10 3.021 3.021 2.007 1.002
9
3.046 2.017 1.005 1.005
8
2.037 1.012 0.002 0.002
7
1.026 0.005 0.005
−
6
0.013 0.013
−
−
5
0.030
−
−
−
B = 11 14 7.026 6.009 6.009 5.003
a
Probability
0.05 0.025 0.01 0.005
A = 14 B = 11 13 6.037 5.013 4.004 4.004
12 5.039 4.015 3.005 3.005
11 4.037 3.013 2.004 2.004
10 3.032 2.011 1.002 1.002
9 2.025 2.025 1.007 0.001
8 1.016 1.016 0.003 0.003
7 1.035 0.007 0.007
−
6 0.017 0.017
−
−
5 0.038
−
−
−
B = 10 14 6.020 6.020 5.006 4.002
13 5.026 4.008 4.008 3.002
12 4.026 3.008 3.008 2.002
11 3.022 3.022 2.007 1.001
10 3.048 2.017 1.004 1.004
9 2.036 1.010 0.002 0.002
8 1.023 1.023 0.004 0.004
7 1.047 0.010 0.010
−
6 0.022 0.022
−
−
5 0.047
−
−
−
B = 9
14 6.047 5.014 4.004 4.004
13 4.017 4.017 3.005 3.005
12 4.047 3.016 2.004 2.004
11 3.037 2.011 1.002 1.002
10 2.027 1.007 1.007 0.001
9 1.016 1.016 0.002 0.002
8 1.033 0.006 0.006
−
7 0.014 0.014
−
−
6 0.030
−
−
−
B = 8
14 5.036 4.010 4.010 3.002
13 4.037 3.011 2.002 2.002
12 3.030 2.008 2.008 1.001
11 2.020 2.020 1.005 1.005
10 2.043 1.011 0.002 0.002
9 1.025 1.025 0.004 0.004
8 1.048 0.009 0.009
−
7 0.020 0.020
−
−
6 0.040
−
−
−
B = 7
14 4.026 3.006 3.006 2.001
13 3.024 3.024 2.005 1.001
12 2.016 2.016 1.003 1.003
11 2.038 1.009 1.009 0.001
10 1.020 1.020 0.003 0.003
9 1.040 0.007 0.007
−
8 0.015 0.015
−
−
7 0.030
−
−
−
B = 6
14 3.018 3.018 2.003 2.003
13 2.014 2.014 1.002 1.002
12 2.035 1.007 1.007 0.001
11 1.017 1.017 0.002 0.002
10 1.036 0.005 0.005
−
9 0.012 0.012
−
−
8 0.024 0.024
−
−
7 0.044
−
−
−
B = 5
14 2.010 2.010 1.001 1.001
13 2.036 1.006 1.006 0.001
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05
0.025
0.01 0.005
A = 14 B = 5
12 1.017
1.017 0.002 0.002
11 1.036
0.005 0.005 0.005
10 0.011
0.011
−
−
9
0.022
0.022
−
−
8
0.040
−
−
−
B = 4
14 2.039
1.005 1.005 1.005
13 1.018
1.018 0.002 0.002
12 1.042
0.005 0.005 0.005
11 0.011
0.011
−
−
10 0.023
0.023
−
−
9
0.041
−
−
−
B = 3
14 1.022
1.022 0.001 0.001
13 0.006
0.006 0.006
−
12 0.015
0.015
−
−
11 0.029
−
−
−
B = 2
14 0.008
0.008 0.008
−
13 0.025
0.025
−
−
12 0.050
−
−
−
A = 15 B = 15 15 11.050 10.021 9.008 8.003
14 9.037
8.016 7.007 6.002
13 8.047
7.022 6.010 5.004
12 6.026
5.011 4.004 4.004
11 5.028
4.012 3.004 3.004
10 4.028
3.011 2.004 2.004
9
3.026
2.010 2.010 1.002
8
2.022
2.022 1.007 0.001
7
2.047
1.016 0.003 0.003
6
1.037
0.008 0.008
−
5
0.021
0.021
−
−
4
0.050
−
−
−
B = 14 15 10.042 9.017 8.006 7.002
14 8.029
7.012 6.004 6.004
13 7.036
6.016 5.006 4.002
12 6.039
5.018 4.007 3.002
11 5.040
4.018 3.006 2.002
10 4.039
3.016 2.005 1.001
9
3.035
2.013 1.003 1.003
8
2.029
1.009 1.009 0.001
7
1.021
1.021 0.004 0.004
6
1.045
0.011
−
−
5
0.025
−
−
−
B = 13 15 9.035
8.013 7.005 7.005
14 7.022
7.022 6.008 5.003
13 6.026
5.010 4.003 4.003
12 5.027
4.011 3.003 3.003
11 4.026
3.010 3.010 2.003
10 3.023
3.023 2.008 1.002
9
3.047
2.018 1.005 1.005
8
2.038
1.012 0.002 0.002
7
1.027
0.005 0.005
−
6
0.013
0.013
−
−
5
0.031
−
−
−
B = 12 15 8.028
7.010 7.010 6.003
14 7.040
6.016 5.005 4.002
13 6.044
5.018 4.006 3.002
a
Probability
0.05 0.025 0.01 0.005
A = 15 B = 12 12 5.043 4.017 3.006 2.001
11 4.039 3.015 2.004 2.004
10 3.033 2.011 1.003 1.003
9 2.025 1.007 1.007 0.001
8 1.016 1.016 0.003 0.003
7 1.035 0.007 0.007
−
6 0.017 0.017
−
−
5 0.037
−
−
−
B = 11 15 7.022 7.022 6.007 5.002
14 6.030 5.011 4.003 4.003
13 5.031 4.011 3.003 3.003
12 4.028 3.010 3.010 2.003
11 3.023 3.023 2.007 1.002
10 3.048 2.017 1.004 1.004
9 2.036 1.010 0.002 0.002
8 1.023 1.023 0.004 0.004
7 1.045 0.010 0.010
−
6 0.022 0.022
−
−
5 0.046
−
−
−
B = 10 15 6.017 6.017 5.005 5.005
14 5.021 5.021 4.007 3.002
13 4.020 4.020 3.006 2.001
12 4.047 3.017 2.005 2.005
11 3.037 2.012 1.003 1.003
10 2.026 1.007 1.007 0.001
9 1.015 1.015 0.002 0.002
8 1.031 0.006 0.006
−
7 0.013 0.013
−
−
6 0.028
−
−
−
B = 9
15 6.042 5.012 4.003 4.003
14 5.044 4.014 3.004 3.004
13 4.038 3.012 2.003 2.003
12 3.029 2.008 2.008 1.002
11 2.020 2.020 1.005 1.005
10 2.040 1.011 0.002 0.002
9 1.023 1.023 0.004 0.004
8 1.044 0.009 0.009
−
7 0.019 0.019
−
−
6 0.037
−
−
−
B = 8
15 5.032 4.008 4.008 3.002
14 4.031 3.008 3.008 2.002
13 3.024 3.024 2.006 1.001
12 2.016 2.016 1.003 1.003
11 2.033 1.008 1.008 0.001
10 1.018 1.018 0.003 0.003
9 1.035 0.006 0.006
−
8 0.013 0.013
−
−
7 0.026
−
−
−
6 0.050
−
−
−
B = 7
15 4.023 4.023 3.005 3.005
14 3.020 3.020 2.004 2.004
13 3.049 2.013 1.002 1.002
12 2.030 1.006 1.006 0.001
11 1.015 1.015 0.002 0.002
10 1.030 0.005 0.005 0.005
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05
0.025
0.01
0.005
A = 15 B = 7
9
0.010
0.010
−
−
8
0.020
0.020
−
−
7
0.038
−
−
−
B = 6
15 3.015
3.015
2.003 2.003
14 2.011
2.011
1.002 1.002
13 2.029
1.005
1.005 0.001
12 1.013
1.013
0.002 0.002
11 1.028
0.004
0.004 0.004
10 0.009
0.009
0.009
−
9
0.017
0.017
−
−
8
0.032
−
−
−
B = 5
15 2.009
2.009
2.009 1.001
14 2.031
1.005
1.005 1.005
13 1.014
1.014
0.001 0.001
12 1.029
0.004
0.004 0.004
11 0.008
0.008
0.008
−
10 0.016
0.016
−
−
9
0.030
−
−
−
B = 4
15 2.035
1.004
1.004 1.004
14 1.015
1.015
0.001 0.001
13 1.036
0.004
0.004 0.004
12 0.009
0.009
0.009
−
11 0.018
0.018
−
−
10 0.033
−
−
−
B = 3
15 1.020
1.020
0.001 0.001
14 0.005
0.005
0.005 0.005
13 0.012
0.012
−
−
12 0.025
0.025
−
−
11 0.043
−
−
−
B = 2
15 0.007
0.007
0.007
−
14 0.022
0.022
−
−
13 0.044
−
−
−
A = 16 B = 16 16 11.022 11.022 10.009 9.003
15 10.038 9.017
8.007 7.003
14 9.049
8.024
6.004 6.004
13 7.028
6.013
5.005 4.002
12 6.031
5.014
4.006 3.002
11 5.032
4.014
3.005 2.002
10 4.031
3.013
2.004 2.004
9
3.028
2.011
1.003 1.003
8
2.024
2.024
1.007 0.001
7
2.049
1.017
0.003 0.003
6
1.038
0.009
0.009
−
5
0.022
0.022
−
−
B = 15 16 11.043 10.018 9.007 8.002
15 9.030
8.013
7.005 6.002
14 8.038
7.017
6.007 5.003
13 7.043
6.020
5.008 4.003
12 6.044
5.021
4.008 3.003
11 5.044
4.020
3.007 2.002
10 4.041
3.018
2.006 1.001
9
3.037
2.014
1.004 1.004
8
2.030
1.010
1.010 0.002
7
1.022
1.022
0.004 0.004
6
1.046
0.011
−
−
a
Probability
0.05
0.025 0.01 0.005
A = 16 B = 15 5
0.026
−
−
−
B = 14 16 10.037 9.014 8.005 7.002
15 8.024 8.024 7.009 6.003
14 7.029 6.012 5.004 5.004
13 6.031 5.013 4.005 4.005
12 5.030 4.013 3.004 3.004
11 4.028 3.011 2.003 2.003
10 3.024 3.024 2.008 1.002
9
3.048 2.019 1.005 0.001
8
2.039 1.013 0.002 0.002
7
1.027 0.006 0.006
−
6
0.013 0.013
−
−
5
0.031
−
−
−
B = 13 16 9.030 8.011 7.004 7.004
15 8.043 7.018 6.006 5.002
14 7.048 6.021 5.008 4.002
13 6.048 5.021 4.008 3.002
12 5.045 4.019 3.007 2.002
11 4.040 3.016 2.005 1.001
10 3.034 2.012 1.003 1.003
9
2.026 1.007 1.007 0.001
8
1.017 1.017 0.003 0.003
7
1.035 0.007 0.007
−
6
0.017 0.017
−
−
5
0.037
−
−
−
B = 12 16 8.024 8.024 7.008 6.002
15 7.034 6.012 5.004 5.004
14 6.036 5.014 4.005 4.005
13 5.034 4.013 3.004 3.004
12 4.030 3.011 2.003 2.003
11 3.024 3.024 2.008 1.002
10 3.047 2.017 1.004 1.004
9
2.035 1.010 0.002 0.002
8
1.022 1.022 0.004 0.004
7
1.044 0.010 0.010
−
6
0.021 0.021
−
−
5
0.044
−
−
−
B = 11 16 7.019 7.019 6.006 5.002
15 6.025 6.025 5.008 4.002
14 5.025 5.025 4.008 3.002
13 4.022 4.022 3.007 2.002
12 4.046 3.017 2.005 2.005
11 3.036 2.012 1.003 1.003
10 2.025 1.007 1.007 0.001
9
2.048 1.015 0.002 0.002
8
1.030 0.006 0.006
−
7
0.013 0.013
−
−
6
0.027
−
−
−
B = 10 16 7.046 6.014 5.004 5.004
15 5.018 5.018 4.005 3.001
14 5.046 4.016 3.005 3.005
13 4.038 3.013 2.003 2.003
12 3.028 2.008 2.008 1.002
11 2.019 2.019 1.005 1.005
10 2.037 1.010 0.002 0.002
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 16 B = 10 9 1.022 1.022 0.004 0.004
8 1.041 0.008 0.008
−
7 0.017 0.017
−
−
6 0.035
−
−
−
B = 9
16 6.037 5.010 5.010 4.002
15 5.038 4.011 3.003 3.003
14 4.031 3.009 3.009 2.002
13 3.023 3.023 2.006 1.001
12 3.047 2.015 1.003 1.003
11 2.030 1.008 1.008 0.001
10 1.016 1.016 0.002 0.002
9 1.031 0.006 0.006
−
8 0.012 0.012
−
−
7 0.024 0.024
−
−
6 0.045
−
−
−
B = 8
16 5.028 4.007 4.007 3.001
15 4.026 3.007 3.007 2.001
14 3.019 3.019 2.005 2.005
13 3.043 2.012 1.002 1.002
12 2.026 1.006 1.006 0.001
11 2.049 1.013 0.002 0.002
10 1.026 0.004 0.004 0.004
9 1.047 0.009 0.009
−
8 0.017 0.017
−
−
7 0.033
−
−
−
B = 7
16 4.020 4.020 3.004 3.004
15 3.017 3.017 2.003 2.003
14 3.042 2.010 1.002 1.002
13 2.024 2.024 1.005 1.005
12 2.047 1.011 0.001 0.001
11 1.023 1.023 0.003 0.003
10 1.041 0.007 0.007
−
9 0.014 0.014
−
−
8 0.026
−
−
−
7 0.047
−
−
−
B = 6
16 3.013 3.013 2.002 2.002
15 3.044 2.009 2.009 1.001
14 2.024 2.024 1.004 1.004
13 2.049 1.011 0.001 0.001
12 1.022 1.022 0.003 0.003
11 1.041 0.006 0.006
−
10 0.012 0.012
−
−
9 0.023 0.023
−
−
8 0.040
−
−
−
B = 5
16 3.048 2.008 2.008 1.001
15 2.027 1.004 1.004 1.004
14 1.011 1.011 0.001 0.001
13 1.024 1.024 0.003 0.003
12 1.045 0.006 0.006
−
11 0.012 0.012
−
−
10 0.023 0.023
−
−
9 0.039
−
−
−
B = 4
16 2.032 1.004 1.004 1.004
15 1.013 1.013 0.001 0.001
14 1.031 0.003 0.003 0.003
a
Probability
0.05
0.025
0.01
0.005
A = 16 B = 4
13 0.007
0.007
0.007
−
12 0.014
0.014
−
−
11 0.026
−
−
−
10 0.043
−
−
−
B = 3
16 1.018
1.018
0.001
0.001
15 1.050
0.004
0.004
0.004
14 0.010
0.010
−
−
13 0.021
0.021
−
−
12 0.036
−
−
−
B = 2
16 0.007
0.007
0.007
−
15 0.020
0.020
−
−
14 0.039
−
−
−
A = 17 B = 17 17 12.022 12.022 11.009 10.004
16 11.039 10.018 9.008
8.003
15 9.025
8.012
7.005
7.005
14 8.030
7.014
6.006
5.002
13 7.033
6.016
5.007
4.002
12 6.035
5.016
4.007
3.002
11 5.035
4.016
3.006
2.002
10 4.033
3.014
2.005
2.005
9
3.030
2.012
1.003
1.003
8
2.025
1.008
1.008
0.001
7
1.018
1.018
0.004
0.004
6
1.039
0.009
0.009
−
5
0.022
0.022
−
−
B = 16 17 12.044 11.018 10.007 9.003
16 10.032 9.014
8.006
7.002
15 9.040
8.019
7.008
6.003
14 8.045
7.022
6.010
5.004
13 7.048
6.023
4.004
4.004
12 6.048
5.023
4.010
3.003
11 5.046
4.022
3.008
2.003
10 4.043
3.019
2.007
1.002
9
3.038
2.015
1.004
1.004
8
2.032
1.010
0.002
0.002
7
1.022
1.022
0.005
0.005
6
1.046
0.011
−
−
5
0.026
−
−
−
B = 15 17 11.038 10.015 9.006
8.002
16 9.025
8.010
7.004
7.004
15 8.031
7.014
6.005
5.002
14 7.034
6.015
5.006
4.002
13 6.034
5.015
4.006
3.002
12 5.033
4.014
3.005
3.005
11 4.030
3.012
2.004
2.004
10 3.025
2.009
2.009
1.002
9
3.049
2.020
1.006
0.001
8
2.040
1.013
0.002
0.002
7
1.028
0.006
0.006
−
6
0.014
0.014
−
−
5
0.031
−
−
−
B = 14 17 10.032 9.012
8.004
8.004
16 9.046
8.019
7.007
6.003
15 7.023
7.023
6.009
5.003
14 6.024
6.024
5.010
4.003
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 17 B = 14 13 5.023 5.023 4.009 3.003
12 5.047 4.021 3.007 2.002
11 4.041 3.017 2.005 1.001
10 3.034 2.013 1.003 1.003
9 2.026 1.008 1.008 0.001
8 2.050 1.017 0.003 0.003
7 1.035 0.007 0.007
−
6 0.017 0.017
−
−
5 0.036
−
−
−
B = 13 17 9.026 8.009 8.009 7.003
16 8.037 7.014 6.005 6.005
15 7.040 6.016 5.006 4.002
14 6.039 5.016 4.006 3.002
13 5.035 4.014 3.005 3.005
12 4.030 3.011 2.003 2.003
11 3.024 3.024 2.008 1.002
10 3.046 2.018 1.005 1.005
9 2.035 1.011 0.002 0.002
8 1.022 1.022 0.004 0.004
7 1.043 0.010 0.010
−
6 0.021 0.021
−
−
5 0.043
−
−
−
B = 12 17 8.021 8.021 7.007 6.002
16 7.028 6.010 5.003 5.003
15 6.029 5.011 4.003 4.003
14 5.027 4.010 4.010 3.003
13 4.023 4.023 3.008 2.002
12 4.045 3.018 2.005 1.001
11 3.035 2.012 1.003 1.003
10 2.025 2.025 1.007 0.001
9 2.046 1.015 0.002 0.002
8 1.029 0.006 0.006
−
7 0.012 0.012
−
−
6 0.026
−
−
−
B = 11 17 7.016 7.016 6.005 6.005
16 6.021 6.021 5.007 4.002
15 5.020 5.020 4.006 3.002
14 5.045 4.017 3.005 2.001
13 4.037 3.013 2.003 2.003
12 3.027 2.008 2.008 1.002
11 2.018 2.018 1.004 1.004
10 2.035 1.010 1.010 0.001
9 1.020 1.020 0.004 0.004
8 1.039 0.008 0.008
−
7 0.016 0.016
−
−
6 0.033
−
−
−
B = 10 17 7.041 6.012 5.003 5.003
16 6.044 5.014 4.004 4.004
15 5.039 4.013 3.003 3.003
14 4.030 3.010 3.010 2.002
13 3.022 3.022 2.006 1.001
12 3.043 2.014 1.003 1.003
11 2.028 1.007 1.007 0.001
10 1.015 1.015 0.002 0.002
9 1.029 0.005 0.005
−
a
Probability
0.05 0.025 0.01 0.005
A = 17 B = 10 8 0.011 0.011
−
−
7 0.022 0.022
−
−
6 0.042
−
−
−
B = 9
17 6.032 5.008 5.008 4.002
16 5.033 4.009 4.009 3.002
15 4.026 3.007 3.007 2.002
14 3.018 3.018 2.005 2.005
13 3.038 2.011 1.002 1.002
12 2.023 2.023 1.005 0.001
11 2.043 1.012 0.002 0.002
10 1.023 1.023 0.004 0.004
9 1.041 0.008 0.008
−
8 0.016 0.016
−
−
7 0.030
−
−
−
B = 8
17 5.024 5.024 4.006 3.001
16 4.022 4.022 3.005 2.001
15 3.016 3.016 2.004 2.004
14 3.035 2.009 2.009 1.002
13 2.020 2.020 1.004 1.004
12 2.039 1.010 1.010 0.001
11 1.019 1.019 0.003 0.003
10 1.035 0.006 0.006
−
9 0.012 0.012
−
−
8 0.022 0.022
−
−
7 0.040
−
−
−
B = 7
17 4.017 4.017 3.003 3.003
16 3.014 3.014 2.003 2.003
15 3.035 2.008 2.008 1.001
14 2.019 2.019 1.004 1.004
13 2.038 1.008 1.008 0.001
12 1.017 1.017 0.002 0.002
11 1.032 0.005 0.005 0.005
10 0.010 0.010 0.010
−
9 0.019 0.019
−
−
8 0.033
−
−
−
B = 6
17 3.011 3.011 2.002 2.002
16 3.038 2.008 2.008 1.001
15 2.020 2.020 1.003 1.003
14 2.042 1.008 1.008 0.001
13 1.017 1.017 0.002 0.002
12 1.032 0.005 0.005 0.005
11 0.009 0.009 0.009
−
10 0.017 0.017
−
−
9 0.030
−
−
−
8 0.050
−
−
−
B = 5
17 3.043 2.006 2.006 1.001
16 2.023 2.023 1.003 1.003
15 1.009 1.009 1.009 0.001
14 1.020 1.020 0.002 0.002
13 1.037 0.005 0.005 0.005
12 0.010 0.010 0.010
−
11 0.018 0.018
−
−
10 0.030
−
−
−
9 0.049
−
−
−
B = 4
17 2.029 1.003 1.003 1.003
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05
0.025
0.01
0.005
A = 17 B = 4
16 1.011
1.011
0.001
0.001
15 1.027
0.003
0.003
0.003
14 0.006
0.006
0.006
−
13 0.012
0.012
−
−
12 0.021
0.021
−
−
11 0.035
−
−
−
B = 3
17 1.016
1.016
0.001
0.001
16 1.045
0.004
0.004
0.004
15 0.009
0.009
0.009
−
14 0.018
0.018
−
−
13 0.031
−
−
−
12 0.049
−
−
−
B = 2
17 0.006
0.006
0.006
−
16 0.018
0.018
−
−
15 0.035
−
−
−
A = 18 B = 18 18 13.023 13.023 12.010 11.004
17 12.040 11.019 10.008 9.003
16 10.026 9.012
8.005
7.002
15 9.032
8.015
7.007
6.003
14 8.035
7.017
6.008
5.003
13 7.037
6.019
5.008
4.003
12 6.038
5.019
4.008
3.003
11 5.037
4.017
3.007
2.002
10 4.035
3.015
2.005
1.001
9
3.032
2.012
1.003
1.003
8
2.026
1.008
1.008
0.001
7
1.019
1.019
0.004
0.004
6
1.040
0.010
0.010
−
5
0.023
0.023
−
−
B = 17 18 13.045 12.019 11.008 10.003
17 11.033 10.015 9.006
8.002
16 10.042 9.020
8.009
7.004
15 9.048
8.024
6.004
6.004
14 7.026
6.012
5.005
5.005
13 6.026
5.012
4.004
4.004
12 5.025
4.011
3.004
3.004
11 5.049
4.023
3.009
2.003
10 4.045
3.020
2.007
1.002
9
3.040
2.016
1.005
1.005
8
2.032
1.011
0.002
0.002
7
1.023
1.023
0.005
0.005
6
1.047
0.011
−
−
5
0.026
−
−
−
B = 16 18 12.039 11.016 10.006 9.002
17 10.027 9.011
8.004
8.004
16 9.033
8.015
7.006
6.002
15 8.037
7.017
6.007
5.003
14 7.038
6.018
5.007
4.003
13 6.037
5.017
4.007
3.002
12 5.035
4.015
3.006
2.002
11 4.031
3.013
2.004
2.004
10 3.026
2.010
2.010
1.002
9
3.050
2.020
1.006
0.001
8
2.040
1.013
0.002
0.002
7
1.028
0.006
0.006
−
a
Probability
0.05
0.025
0.01 0.005
A = 18 B = 16 6
0.014
0.014
−
−
5
0.031
−
−
−
B = 15 18 11.033 10.013 9.005 9.005
17 10.049 9.021 8.008 7.003
16 8.026
7.011 6.004 6.004
15 7.027
6.012 5.004 5.004
14 6.027
5.011 4.004 4.004
13 5.025
5.025 3.003 3.003
12 5.048
4.022 3.008 2.002
11 4.042
3.018 2.006 1.001
10 3.035
2.013 1.003 1.003
9
2.026
1.008 1.008 0.001
8
2.050
1.017 0.003 0.003
7
1.034
0.007 0.007
−
6
0.017
0.017
−
−
5
0.036
−
−
−
B = 14 18 10.028 9.010 9.010 8.003
17 9.040
8.016 7.006 6.002
16 8.044
7.019 6.007 5.002
15 7.043
6.019 5.007 4.002
14 6.041
5.018 4.006 3.002
13 5.036
4.015 3.005 2.001
12 4.031
3.012 2.004 2.004
11 3.025
3.025 2.008 1.002
10 3.046
2.018 1.005 1.005
9
2.034
1.011 0.002 0.002
8
1.022
1.022 0.004 0.004
7
1.042
0.009 0.009
−
6
0.020
0.020
−
−
5
0.043
−
−
−
B = 13 18 9.023
9.023 8.008 7.002
17 8.031
7.012 6.004 6.004
16 7.033
6.013 5.004 5.004
15 6.032
5.012 4.004 4.004
14 5.028
4.011 3.003 3.003
13 4.023
4.023 3.008 2.002
12 4.044
3.018 2.005 1.001
11 3.034
2.012 1.003 1.003
10 2.024
2.024 1.007 0.001
9
2.045
1.014 0.002 0.002
8
1.028
0.006 0.006
−
7
0.012
0.012
−
−
6
0.025
−
−
−
B = 12 18 8.018
8.018 7.006 6.002
17 7.024
7.024 6.008 5.002
16 6.024
6.024 5.008 4.003
15 5.022
5.022 4.007 3.002
14 5.044
4.018 3.006 2.001
13 4.035
3.013 2.004 2.004
12 3.026
2.008 2.008 1.002
11 3.048
2.018 1.004 1.004
10 2.033
1.010 1.010 0.001
9
1.019
1.019 0.003 0.003
8
1.037
0.007 0.007
−
7
0.016
0.016
−
−
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 18 B = 12 6 0.031
−
−
−
B = 11 18 8.045 7.014 6.004 6.004
17 6.018 6.018 5.005 4.001
16 6.045 5.016 4.005 3.001
15 5.038 4.013 3.004 3.004
14 4.029 3.010 3.010 2.002
13 3.021 3.021 2.006 1.001
12 3.039 2.013 1.003 1.003
11 2.026 1.007 1.007 0.001
10 2.046 1.014 0.002 0.002
9 1.027 0.005 0.005 0.005
8 1.048 0.010
−
−
7 0.020 0.020
−
−
6 0.039
−
−
−
B = 10 18 7.037 6.010 5.003 5.003
17 6.038 5.012 4.003 4.003
16 5.033 4.010 3.003 3.003
15 4.025 4.025 3.007 2.002
14 4.049 3.017 2.005 2.005
13 3.034 2.010 1.002 1.002
12 2.021 2.021 1.005 1.005
11 2.038 1.010 0.001 0.001
10 1.020 1.020 0.003 0.003
9 1.037 0.007 0.007
−
8 0.014 0.014
−
−
7 0.027
−
−
−
6 0.049
−
−
−
B = 9
18 6.029 5.007 5.007 4.002
17 5.028 4.008 4.008 3.002
16 4.022 4.022 3.006 2.001
15 4.046 3.015 2.003 2.003
14 3.030 2.008 2.008 1.002
13 2.018 2.018 1.004 1.004
12 2.033 1.008 1.008 0.001
11 1.016 1.016 0.002 0.002
10 1.030 0.005 0.005
−
9 0.010 0.010
−
−
8 0.020 0.020
−
−
7 0.036
−
−
−
B = 8
18 5.022 5.022 4.005 4.005
17 4.019 4.019 3.004 3.004
16 4.047 3.013 2.003 2.003
15 3.029 2.007 2.007 1.001
14 2.016 2.016 1.003 1.003
13 2.031 1.007 1.007 0.001
12 1.014 1.014 0.002 0.002
11 1.026 0.004 0.004 0.004
10 1.045 0.008 0.008
−
9 0.016 0.016
−
−
8 0.028
−
−
−
7 0.048
−
−
−
B = 7
18 4.015 4.015 3.003 3.003
17 4.050 3.012 2.002 2.002
16 3.030 2.007 2.007 1.001
15 2.016 2.016 1.003 1.003
a
Probability
0.05
0.025
0.01
0.005
A = 18 B = 7
14 2.031
1.007
1.007
0.001
13 1.013
1.013
0.002
0.002
12 1.025
1.025
0.004
0.004
11 1.043
0.007
0.007
−
10 0.013
0.013
−
−
9
0.024
0.024
−
−
8
0.040
−
−
−
B = 6
18 3.010
3.010
3.010
2.001
17 3.034
2.006
2.006
1.001
16 2.017
2.017
1.003
1.003
15 2.035
1.007
1.007
0.001
14 1.014
1.014
0.002
0.002
13 1.026
0.003
0.003
0.003
12 1.045
0.007
0.007
−
11 0.013
0.013
−
−
10 0.022
0.022
−
−
9
0.037
−
−
−
B = 5
18 3.040
2.006
2.006
1.001
17 2.020
2.020
1.003
1.003
16 2.045
1.008
1.008
0.001
15 1.017
1.017
0.002
0.002
14 1.031
0.004
0.004
0.004
13 0.007
0.007
0.007
−
12 0.014
0.014
−
−
11 0.024
0.024
−
−
10 0.038
−
−
−
B = 4
18 2.026
1.003
1.003
1.003
17 1.010
1.010
1.010
0.001
16 1.023
1.023
0.002
0.002
15 1.044
0.005
0.005
0.005
14 0.010
0.010
0.010
−
13 0.017
0.017
−
−
12 0.029
−
−
−
11 0.045
−
−
−
B = 3
18 1.014
1.014
0.001
0.001
17 1.041
0.003
0.003
0.003
16 0.008
0.008
0.008
−
15 0.015
0.015
−
−
14 0.026
−
−
−
13 0.042
−
−
−
B = 2
18 0.005
0.005
0.005
−
17 0.016
0.016
−
−
16 0.032
−
−
−
A = 19 B = 19 19 14.023 14.023 13.010 12.004
18 13.041 12.020 11.009 10.004
17 11.027 10.013 9.006
8.002
16 10.033 9.017
8.008
7.003
15 9.037
8.019
7.009
6.004
14 8.040
7.020
6.009
5.004
13 7.041
6.021
5.009
4.004
12 6.041
5.020
4.009
3.003
11 5.040
4.019
3.008
2.002
10 4.037
3.017
2.006
1.001
9
3.033
2.013
1.004
1.004
8
2.027
1.009
1.009
0.002
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05
0.025
0.01
0.005
A = 19 B = 19 7
1.020
1.020
0.004
0.004
6
1.041
0.010
0.010
−
5
0.023
0.023
−
−
B = 18 19 14.046 13.020 12.008 11.003
18 12.034 11.016 10.007 9.003
17 11.044 10.021 9.010
8.004
16 10.050 8.012
7.005
6.002
15 8.028
7.013
6.006
5.002
14 7.029
6.014
5.006
4.002
13 6.029
5.013
4.005
3.002
12 5.027
4.012
3.004
3.004
11 4.025
4.025
2.003
2.003
10 4.046
3.021
2.008
1.002
9
3.041
2.017
1.005
1.005
8
2.033
1.011
0.002
0.002
7
1.023
1.023
0.005
0.005
6
1.047
0.012
−
−
5
0.027
−
−
−
B = 17 19 13.040 12.016 11.006 10.002
18 11.028 10.012 9.005
9.005
17 10.035 9.016
8.007
7.003
16 9.039
8.019
7.008
6.003
15 8.041
7.020
6.009
5.003
14 7.041
6.020
5.008
4.003
13 6.039
5.019
4.008
3.003
12 5.036
4.016
3.006
2.002
11 4.032
3.014
2.004
2.004
10 3.027
2.010
1.003
1.003
9
2.021
2.021
1.006
0.001
8
2.040
1.014
0.002
0.002
7
1.028
0.006
0.006
−
6
0.014
0.014
−
−
5
0.031
−
−
−
B = 16 19 12.035 11.013 10.005 10.005
18 10.023 10.023 9.009
8.003
17 9.028
8.012
7.005
7.005
16 8.030
7.013
6.005
5.002
15 7.030
6.013
5.005
4.002
14 6.029
5.013
4.005
4.005
13 5.026
4.011
3.004
3.004
12 5.049
4.023
3.009
2.003
11 4.042
3.018
2.006
1.001
10 3.035
2.013
1.004
1.004
9
2.027
1.008
1.008
0.001
8
2.049
1.017
0.003
0.003
7
1.034
0.007
0.007
−
6
0.017
0.017
−
−
5
0.036
−
−
−
B = 15 19 11.029 10.011 9.004
9.004
18 10.042 9.018
8.007
7.002
17 9.047
8.021
7.008
6.003
16 8.048
7.022
6.009
5.003
15 7.045
6.021
5.008
4.003
14 6.042
5.019
4.007
3.002
13 5.037
4.016
3.006
2.002
a
Probability
0.05
0.025
0.01 0.005
A = 19 B = 15 12 4.031
3.012 2.004 2.004
11 3.025
3.025 2.009 1.002
10 3.045
2.018 1.005 1.005
9
2.034
1.011 0.002 0.002
8
1.022
1.022 0.004 0.004
7
1.042
0.009 0.009
−
6
0.020
0.020
−
−
5
0.042
−
−
−
B = 14 19 10.024 10.024 9.008 8.003
18 9.034
8.013 7.005 7.005
17 8.037
7.015 6.006 5.002
16 7.036
6.015 5.005 4.002
15 6.033
5.014 4.005 4.005
14 5.028
4.011 3.004 3.004
13 4.023
4.023 3.008 2.002
12 4.043
3.018 2.006 1.001
11 3.034
2.012 1.003 1.003
10 2.024
2.024 1.007 0.001
9
2.043
1.014 0.002 0.002
8
1.027
0.005 0.005
−
7
0.012
0.012
−
−
6
0.024
0.024
−
−
5
0.049
−
−
−
B = 13 19 9.020
9.020 8.006 7.002
18 8.027
7.010 7.010 6.003
17 7.028
6.010 5.003 5.003
16 6.026
5.010 5.010 4.003
15 5.022
5.022 4.008 3.002
14 5.043
4.018 3.006 2.002
13 4.034
3.013 2.004 2.004
12 3.025
2.008 2.008 1.002
11 3.046
2.017 1.004 1.004
10 2.032
1.009 1.009 0.001
9
1.019
1.019 0.003 0.003
8
1.035
0.007 0.007
−
7
0.015
0.015
−
−
6
0.030
−
−
−
B = 12 19 9.049
8.016 7.005 7.005
18 7.020
7.020 6.007 5.002
17 6.020
6.020 5.007 4.002
16 6.044
5.017 4.006 3.002
15 5.036
4.014 3.004 3.004
14 4.028
3.010 3.010 2.003
13 3.020
3.020 2.006 1.001
12 3.037
2.013 1.003 1.003
11 2.024
2.024 1.006 0.001
10 2.043
1.013 0.002 0.002
9
1.025
1.025 0.005 0.005
8
1.045
0.010 0.010
−
7
0.019
0.019
−
−
6
0.037
−
−
−
B = 11 19 8.041
7.012 6.003 6.003
18 7.044
6.015 5.004 5.004
17 6.039
5.013 4.004 4.004
16 5.031
4.011 3.003 3.003
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 19 B = 11 15 4.023 4.023 3.007 2.002
14 4.044 3.016 2.004 2.004
13 3.031 2.010 2.010 1.002
12 2.019 2.019 1.005 1.005
11 2.035 1.010 1.010 0.001
10 1.019 1.019 0.003 0.003
9 1.034 0.006 0.006
−
8 0.013 0.013
−
−
7 0.025 0.025
−
−
6 0.046
−
−
−
B = 10 19 7.033 6.009 6.009 5.002
18 6.034 5.010 4.003 4.003
17 5.028 4.008 4.008 3.002
16 4.020 4.020 3.006 2.001
15 4.041 3.013 2.003 2.003
14 3.027 2.008 2.008 1.001
13 3.048 2.016 1.003 1.003
12 2.029 1.007 1.007 0.001
11 1.015 1.015 0.002 0.002
10 1.027 0.005 0.005 0.005
9 1.046 0.009 0.009
−
8 0.018 0.018
−
−
7 0.032
−
−
−
B = 9
19 6.026 5.006 5.006 4.001
18 5.024 5.024 4.006 3.001
17 4.018 4.018 3.005 3.005
16 4.039 3.012 2.003 2.003
15 3.025 3.025 2.006 1.001
14 3.045 2.014 1.003 1.003
13 2.026 1.006 1.006 0.001
12 2.045 1.012 0.002 0.002
11 1.022 1.022 0.004 0.004
10 1.039 0.007 0.007
−
9 0.013 0.013
−
−
8 0.024 0.024
−
−
7 0.043
−
−
−
B = 8
19 5.019 5.019 4.004 4.004
18 4.016 4.016 3.004 3.004
17 4.040 3.011 2.002 2.002
16 3.024 3.024 2.006 1.001
15 3.046 2.013 1.002 1.002
14 2.025 2.025 1.005 0.001
13 2.044 1.011 0.001 0.001
12 1.020 1.020 0.003 0.003
11 1.035 0.006 0.006
−
10 0.011 0.011
−
−
9 0.020 0.020
−
−
8 0.034
−
−
−
B = 7
19 4.013 4.013 3.002 3.002
18 4.044 3.010 2.002 2.002
17 3.026 2.005 2.005 1.001
16 2.013 2.013 1.002 1.002
15 2.026 1.005 1.005 0.001
14 2.046 1.011 0.001 0.001
13 1.020 1.020 0.003 0.003
a
Probability
0.05
0.025
0.01
0.005
A = 19 B = 7
12 1.034
0.005
0.005
−
11 0.010
0.010
0.010
−
10 0.017
0.017
−
−
9
0.030
−
−
−
8
0.048
−
−
−
B = 6
19 4.050
3.009
3.009
2.001
18 3.030
2.005
2.005
1.001
17 2.014
2.014
1.002
1.002
16 2.030
1.005
1.005
0.000
15 1.011
1.011
0.001
0.001
14 1.021
1.021
0.003
0.003
13 1.037
0.005
0.005
−
12 0.010
0.010
0.010
−
11 0.017
0.017
−
−
10 0.028
−
−
−
9
0.045
−
−
−
B = 5
19 3.036
2.005
2.005
2.005
18 2.018
2.018
1.002
1.002
17 2.040
1.006
1.006
0.000
16 1.014
1.014
0.001
0.001
15 1.026
0.003
0.003
0.003
14 1.044
0.006
0.006
−
13 0.011
0.011
−
−
12 0.019
0.019
−
−
11 0.030
−
−
−
10 0.047
−
−
−
B = 4
19 2.024
2.024
1.002
1.002
18 1.009
1.009
1.009
0.001
17 1.020
1.020
0.002
0.002
16 1.038
0.004
0.004
0.004
15 0.008
0.008
0.008
−
14 0.014
0.014
−
−
13 0.024
0.024
−
−
12 0.037
−
−
−
B = 3
19 1.013
1.013
0.001
0.001
18 1.037
0.003
0.003
0.003
17 0.006
0.006
0.006
−
16 0.013
0.013
−
−
15 0.023
0.023
−
−
14 0.036
−
−
−
B = 2
19 0.005
0.005
0.005
0.005
18 0.014
0.014
−
−
17 0.029
−
−
−
16 0.048
−
−
−
A = 20 B = 20 20 15.024 15.024 13.004 13.004
19 14.042 13.020 12.009 11.004
18 12.028 11.014 10.006 9.003
17 11.034 10.018 9.008
8.004
16 10.039 9.020
8.010
7.004
15 9.041
8.022
6.005
6.005
14 8.043
7.023
5.005
5.005
13 7.044
6.023
4.004
4.004
12 6.043
5.022
4.010
3.004
11 5.041
4.020
3.008
2.003
10 4.039
3.018
2.006
1.002
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05
0.025
0.01
0.005
A = 20 B = 20 9
3.034
2.014
1.004
1.004
8
2.028
1.009
1.009
0.002
7
1.020
1.020
0.004
0.004
6
1.042
0.010
−
−
5
0.024
0.024
−
−
B = 19 20 15.047 14.020 13.008 12.003
19 13.035 12.016 11.007 10.003
18 12.045 11.023 9.004
9.004
17 10.027 9.013
8.006
7.002
16 9.030
8.015
7.006
6.003
15 8.031
7.015
6.007
5.003
14 7.031
6.015
5.007
4.002
13 6.031
5.014
4.006
3.002
12 5.029
4.013
3.005
3.005
11 4.026
3.011
2.004
2.004
10 4.047
3.022
2.008
1.002
9
3.042
2.017
1.005
0.001
8
2.034
1.011
0.002
0.002
7
1.024
1.024
0.005
−
6
1.048
0.012
−
−
5
0.027
−
−
−
B = 18 20 14.041 13.017 12.007 11.003
19 12.029 11.013 10.005 9.002
18 11.037 10.018 9.008
8.003
17 10.041 9.020
8.009
7.004
16 9.044
8.022
7.010
6.004
15 8.044
7.022
5.004
5.004
14 7.043
6.021
5.009
4.004
13 6.041
5.020
4.008
3.003
12 5.038
4.017
3.007
2.002
11 4.033
3.014
2.005
2.005
10 3.028
2.010
1.003
1.003
9
2.021
2.021
1.006
0.001
8
2.041
1.014
0.003
0.003
7
1.029
0.006
0.006
−
6
0.014
0.014
−
−
5
0.031
−
−
−
B = 17 20 13.036 12.014 11.005 10.002
19 11.024 11.024 9.004
9.004
18 10.030 9.013
8.005
7.002
17 9.032
8.015
7.006
6.002
16 8.033
7.015
6.006
5.002
15 7.032
6.015
5.006
4.002
14 6.030
5.014
4.005
3.002
13 5.027
4.012
3.004
3.004
12 5.049
4.023
3.009
2.003
11 4.043
3.019
2.006
1.002
10 3.035
2.014
1.004
1.004
9
2.027
1.008
1.008
0.001
8
2.049
1.017
0.003
0.003
7
1.034
0.008
0.008
−
6
0.017
0.017
−
−
5
0.036
−
−
−
B = 16 20 12.031 11.012 10.004 10.004
19 11.045 10.019 9.008
8.003
a
Probability
0.05
0.025
0.01
0.005
A = 20 B = 16 18 9.023
9.023
8.010 7.004
17 8.024
8.024
6.004 6.004
16 8.050
7.024
5.004 5.004
15 7.047
6.022
5.009 4.003
14 6.042
5.020
4.008 3.003
13 5.037
4.016
3.006 2.002
12 4.031
3.013
2.004 2.004
11 3.025
3.025
2.009 1.002
10 3.045
2.018
1.005 1.005
9
2.034
1.011
0.002 0.002
8
1.021
1.021
0.004 0.004
7
1.041
0.009
0.009
−
6
0.020
0.020
−
−
5
0.041
−
−
−
B = 15 20 11.026 10.009 10.009 9.003
19 10.037 9.015
8.005 7.002
18 9.040
8.017
7.007 6.002
17 8.040
7.018
6.007 5.002
16 7.037
6.016
5.006 4.002
15 6.033
5.014
4.005 3.002
14 5.029
4.012
3.004 3.004
13 4.023
4.023
3.009 2.003
12 4.042
3.018
2.006 1.001
11 3.033
2.012
1.003 1.003
10 2.023
2.023
1.007 0.001
9
2.042
1.014
0.002 0.002
8
1.027
0.005
0.005
−
7
1.049
0.012
−
−
6
0.024
0.024
−
−
5
0.048
−
−
−
B = 14 20 10.022 10.022 9.007 8.002
19 9.030
8.011
7.004 7.004
18 8.031
7.012
6.004 6.004
17 7.030
6.012
5.004 5.004
16 6.027
5.010
4.003 4.003
15 5.022
5.022
4.008 3.003
14 5.042
4.018
3.006 2.002
13 4.033
3.013
2.004 2.004
12 3.025
3.025
2.008 1.002
11 3.044
2.016
1.004 1.004
10 2.031
1.009
1.009 0.001
9
1.018
1.018
0.003 0.003
8
1.034
0.007
0.007
−
7
0.014
0.014
−
−
6
0.029
−
−
−
B = 13 20 9.017
9.017
8.005 7.002
19 8.023
8.023
7.008 6.002
18 7.023
7.023
6.008 5.003
17 6.021
6.021
5.008 4.002
16 6.043
5.018
4.006 3.002
15 5.035
4.014
3.004 3.004
14 4.027
3.010
3.010 2.003
13 4.048
3.019
2.006 1.001
12 3.035
2.012
1.003 1.003
11 2.023
2.023
1.006 0.001
c
⃝2000 by Chapman & Hall/CRC

Contingency tables: 2 × 2
a
Probability
0.05 0.025 0.01 0.005
A = 20 B = 13 10 2.041 1.012 0.002 0.002
9 1.024 1.024 0.004 0.004
8 1.042 0.009 0.009
−
7 0.018 0.018
−
−
6 0.035
−
−
−
B = 12 20 9.044 8.014 7.004 7.004
19 8.049 7.017 6.005 5.002
18 7.045 6.017 5.005 4.001
17 6.038 5.014 4.004 4.004
16 5.030 4.011 3.003 3.003
15 4.022 4.022 3.007 2.002
14 4.041 3.015 2.004 2.004
13 3.028 2.009 2.009 1.002
12 3.049 2.018 1.004 1.004
11 2.032 1.009 1.009 0.001
10 1.017 1.017 0.003 0.003
9 1.031 0.006 0.006
−
8 0.012 0.012
−
−
7 0.023 0.023
−
−
6 0.043
−
−
−
B = 11 20 8.037 7.010 6.003 6.003
19 7.039 6.013 5.004 5.004
18 6.033 5.011 4.003 4.003
17 5.026 4.008 4.008 3.002
16 4.019 4.019 3.006 2.001
15 4.036 3.012 2.003 2.003
14 3.024 3.024 2.007 1.001
13 3.043 2.014 1.003 1.003
12 2.026 1.007 1.007 0.001
11 2.045 1.013 0.002 0.002
10 1.024 1.024 0.004 0.004
9 1.042 0.008 0.008
−
8 0.016 0.016
−
−
7 0.029
−
−
−
B = 10 20 7.030 6.008 6.008 5.002
19 6.029 5.008 5.008 4.002
18 5.024 5.024 4.007 3.002
17 5.049 4.017 3.005 3.005
16 4.034 3.011 2.003 2.003
15 3.022 3.022 2.006 1.001
14 3.039 2.012 1.002 1.002
13 2.022 2.022 1.005 0.001
12 2.039 1.011 0.001 0.001
11 1.019 1.019 0.003 0.003
10 1.034 0.006 0.006
−
9 0.012 0.012
−
−
8 0.022 0.022
−
−
7 0.038
−
−
−
B = 9
20 6.023 6.023 5.005 4.001
19 5.021 5.021 4.005 3.001
18 4.015 4.015 3.004 3.004
17 4.033 3.010 3.010 2.002
16 3.020 3.020 2.005 1.001
15 3.038 2.011 1.002 1.002
14 2.021 2.021 1.004 1.004
a
Probability
0.05 0.025 0.01 0.005
A = 20 B = 9 13 2.036 1.009 1.009 0.001
12 1.017 1.017 0.002 0.002
11 1.029 0.005 0.005 0.005
10 1.048 0.009 0.009
−
9 0.017 0.017
−
−
8 0.029
−
−
−
7 0.050
−
−
−
B = 8 20 5.017 5.017 4.003 4.003
19 4.014 4.014 3.003 3.003
18 4.035 3.009 3.009 2.002
17 3.021 3.021 2.005 2.005
16 3.039 2.010 1.002 1.002
15 2.020 2.020 1.004 1.004
14 2.036 1.008 1.008 0.001
13 1.015 1.015 0.002 0.002
12 1.027 0.004 0.004 0.004
11 1.044 0.008 0.008
−
10 0.014 0.014
−
−
9 0.024 0.024
−
−
8 0.041
−
−
−
B = 7 20 4.012 4.012 3.002 3.002
19 4.040 3.009 3.009 2.001
18 3.022 3.022 2.004 2.004
17 3.045 2.011 1.002 1.002
16 2.022 2.022 1.004 1.004
15 2.039 1.008 1.008 0.001
14 1.016 1.016 0.002 0.002
13 1.027 0.004 0.004 0.004
12 1.044 0.007 0.007
−
11 0.013 0.013
−
−
10 0.022 0.022
−
−
9 0.036
−
−
−
B = 6 20 4.046 3.008 3.008 2.001
19 3.027 2.005 2.005 2.005
18 2.012 2.012 1.002 1.002
17 2.026 1.004 1.004 1.004
16 2.047 1.009 1.009 0.001
15 1.018 1.018 0.002 0.002
14 1.030 0.004 0.004 0.004
13 1.048 0.007 0.007
−
12 0.013 0.013
−
−
11 0.022 0.022
−
−
10 0.035
−
−
−
B = 5 20 3.033 2.004 2.004 2.004
19 2.016 2.016 1.002 1.002
18 2.036 1.005 1.005 0.000
17 1.012 1.012 0.001 0.001
16 1.022 1.022 0.002 0.002
15 1.038 0.005 0.005 0.005
14 0.009 0.009 0.009
−
13 0.015 0.015
−
−
12 0.024 0.024
−
−
11 0.038
−
−
−
B = 4 20 2.022 2.022 1.002 1.002
19 1.008 1.008 1.008 0.000
c
⃝2000 by Chapman & Hall/CRC

10.11
DETERMINING VALUES IN BERNOULLI TRIALS
Suppose the probability of heads for a biased coin is either (H0) p = α or
(H1) p = β (with α < β). Assume the values α and β are known and the
purpose of an experiment is to determine the true value of p. Toss the coin
repeatedly and let the random variable Y be the number of tosses until the
rth head appears. Let 1 −θ be the probability the identiﬁcation is correct
under either hypothesis and deﬁne N by
1 −θ ≈Prob [Y ≤N | p = β]
≈Prob [Y > N | p = α]
(10.13)
This hypothesis test has signiﬁcance level θ and power 1 −θ.
The random variable Y has a negative binomial distribution with mean r/p
and variance r(1 −p)/p2 but may be approximated by a normal distribution.
If ξ is deﬁned by Φ(ξ) = θ, then
r ≈

ξ
β −α
>
α

1 −β + β
√
1 −α
?2
N ≈r −ξ

r(1 −α)
α
(10.14)
Example 10.62:
If θ = 0.05, equation (10.14) yields the values below. In this table,
E [T] is the expected number of tosses required to reach a decision.
α
β
r
N
E [T]
0.1
0.2
21
139
122
0.3
0.4
87
247
232
0.5
0.6
148
268
257
0.7
0.8
153
203
197
0.1
0.6
4
10
8
0.4
0.9
7
9
8
See G. J. Manas and D. H. Meyer, “On a problem of coin identiﬁcation,” SIAM
Review, 31, Number 1, March 1989, SIAM, Philadelphia, pages 114–117.
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 11
Regression Analysis
Contents
11.1
Simple linear regression
11.1.1
Least squares estimates
11.1.2
Sum of squares
11.1.3
Inferences regarding regression coeﬃcients
11.1.4
The mean response
11.1.5
Prediction interval
11.1.6
Analysis of variance table
11.1.7
Test for linearity of regression
11.1.8
Sample correlation coeﬃcient
11.1.9
Example
11.2
Multiple linear regression
11.2.1
Least squares estimates
11.2.2
Sum of squares
11.2.3
Inferences regarding regression coeﬃcients
11.2.4
The mean response
11.2.5
Prediction interval
11.2.6
Analysis of variance table
11.2.7
Sequential sum of squares
11.2.8
Partial F test
11.2.9
Residual analysis
11.2.10 Example
11.3
Orthogonal polynomials
11.3.1
Tables for orthogonal polynomials
11.1
SIMPLE LINEAR REGRESSION
Let (x1, y1), (x2, y2), . . . , (xn, yn) be n pairs of observations such that yi is an
observed value of the random variable Yi. Assume there exist constants β0
and β1 such that
Yi = β0 + β1xi + ϵi
(11.1)
c
⃝2000 by Chapman & Hall/CRC

where ϵ1, ϵ2, . . . , ϵn are independent, normal random variables having mean 0
and variance σ2.
Assumptions
In terms of ϵi’s
In terms of Yi’s
ϵi’s are normally distributed
Yi’s are normally distributed
E [ϵi] = 0
E [Yi] = β0 + β1xi
Var [ϵi] = σ2
Var [Yi] = σ2
Cov [ϵi, ϵj] = 0, i ̸= j
Cov [Yi, Yj] = 0, i ̸= j
Principle of least squares: The sum of squared deviations about the true
regression line is
S(β0, β1) =
n

i=1
[yi −(β0 + β1xi)]2.
(11.2)
The point estimates of β0 and β1, denoted by .β0 and .β1, are those values that
minimize S(β0, β1). The estimates .β0 and .β1 are called the least squares
estimates. The estimated regression line or least squares line is .y = .β0+ .β1x.
The normal equations for .β0 and .β1 are
 n

i=1
yi

= .β0 n + .β1
 n

i=1
xi

 n

i=1
xiyi

= .β0
 n

i=1
xi

+ .β1
 n

i=1
x2
i

(11.3)
Notation:
Sxx =
n

i=1
(xi −x)2 =
n

i=1
x2
i −1
n
 n

i=1
xi
2
Syy =
n

i=1
(yi −y)2 =
n

i=1
y2
i −1
n
 n

i=1
yi
2
Sxy =
n

i=1
(xi −x)(yi −y)2 =
n

i=1
xiyi −1
n
 n

i=1
xi
  n

i=1
yi

c
⃝2000 by Chapman & Hall/CRC

11.1.1
Least squares estimates
.β1 = Sxy
Sxx
=
n
 n
i=1
xiyi

−
 n
i=1
xi
  n
i=1
yi

n
 n
i=1
x2
i

−
 n
i=1
xi
2
.β0 =
n
i=1
yi −.β1
n
i=1
xi
n
= y −.β1x
(11.4)
The ith predicted (ﬁtted) value: .yi = .β0 + .β1xi (for i = 1, 2, . . . , n).
The ith residual: ei = yi −.yi (for i = 1, 2, . . . , n).
Properties:
(1) E [.β1] = β1,
Var [.β1] =
σ2
n
i=1
(xi −x)2
= σ2
Sxx
(2) E [.β0] = β0,
Var [.β0] =
σ2
n
i=1
xi
n
n
i=1
(xi −x)2
=
σ2
n
i=1
xi
nSxx
= σ2x
Sxx
(3) .β0 and .β1 are normally distributed.
11.1.2
Sum of squares
n

i=1
(yi −y)2


	
SST
=
n

i=1
(.yi −y)2


	
SSR
+
n

i=1
(yi −.yi)2


	
SSE
SST = total sum of squares = Syy
SSR = sum of squares due to regression = .β2Sxy
SSE = sum of squares due to error
=
n

i=1
[yi −(.β0 + .β1xi)]2 =
n

i=1
y2
i −.β0
n

i=1
yi −.β1
n

i=1
xiyi
= Syy −2.β1Sxy + .β2
1Sxx = Syy −.β2
1Sxx = Syy −.β1Sxy
.σ2 = s2 = SSE
n −2,
E

S2
= σ2
Sample coeﬃcient of determination: r2 = SSR
SST = 1 −SSE
SST
c
⃝2000 by Chapman & Hall/CRC

11.1.3
Inferences concerning the regression coeﬃcients
The parameter .β1
(1) T =
.β1 −β1
S/√Sxx
=
.β1 −β1
S.β1
has a t distribution with n −2 degrees of freedom, where
S.β1 = S/√Sxx is an estimate for the standard deviation of .β1.
(2) A 100(1 −α)% conﬁdence interval for β1 has as endpoints
.β1 ± tα/2,n−2 · s ˆβ1
(3) Hypothesis test:
Null
Alternative
Test
Rejection
hypothesis
hypotheses
statistic
regions
β1 = β10
β1 > β10
T =
.β1 −β10
S.β1
T ≥tα,n−2
(1)
β1 < β10
T ≤−tα,n−2
(2)
β1 ̸= β10
|T| ≥tα/2,n−2
(3)
The parameter .β0
(1) T =
.β0 −β0
S
+
n
i=1
x2
i /nSxx
=
.β0 −β0
S.β0
has a t distribution with n −2 degrees of freedom, where
S.β0 denotes the estimate for the standard deviation of .β0.
(2) A 100(1 −α)% conﬁdence interval for β1 has as endpoints
.β1 ± tα/2,n−2 · s ˆβ0
(3) Hypothesis test:
Null
Alternative
Test
Rejection
hypothesis
hypotheses
statistic
regions
β0 = β00
β0 > β00
T =
.β0 −β00
S.β0
T ≥tα,n−2
(1)
β0 < β00
T ≤−tα,n−2
(2)
β0 ̸= β00
|T| ≥tα/2,n−2
(3)
11.1.4
The mean response
The mean response of Y given x = x0 is µY |x0 = β0 + β1x0. The random
variable .Y0 = .β0 + .β1x0 is used to estimate µY |x0.
c
⃝2000 by Chapman & Hall/CRC

(1) E [.Y0] = β0 + β1x0
(2) Var [.Y0] = σ2
 1
n + (x0 −x)2
Sxx

(3) .Y0 has a normal distribution.
(4) T =
.Y0 −µY |x0
S

(1/n) + [(x0 −x)2/Sxx]
=
.Y0 −µY |x0
S.Y0
has a t distribution with n −2 degrees of freedom, where
S.Y0 denotes the estimate for the standard deviation of .Y0.
(5) A 100(1 −α)% conﬁdence interval for µY |x0 has as endpoints
.y0 ± tα/2,n−2 · s.Y0.
(6) Hypothesis test:
Null
Alternative
Test
Rejection
hypothesis
hypotheses
statistic
regions
β0 + β1x0 = y0 = µ0
y0 > µ0
T =
.Y0 −µ0
S.Y0
T ≥tα,n−2
(1)
y0 < µ0
T ≤−tα,n−2
(2)
y0 ̸= µ0
|T| ≥tα/2,n−2
(3)
11.1.5
Prediction interval
A prediction interval for a value y0 of the random variable Y0 = β0 +β1x0 +ϵ0
is obtained by considering the random variable .Y0 −Y0.
(1) E [.Y0 −Y0] = 0
(2) Var [.Y0 −Y0] = σ2

1 + 1
n + (x0 −x)2
Sxx

(3) .Y0 −Y0 has a normal distribution.
(4) T =
.Y0 −Y0
S

1 + (1/n) + [(x0 −x)2/Sxx]
=
.Y0 −Y0
S.Y0−Y0
has a t distribution with n −2 degrees of freedom.
(5) A 100(1 −α)% prediction interval for y0 has as endpoints
.y0 ± tα/2,n−2 · s.Y0−Y0
c
⃝2000 by Chapman & Hall/CRC

11.1.6
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Regression
SSR
1
MSR = SSR
1
MSR/MSE
Error
SSE
n −2
MSE = SSE
n −2
Total
SST
n −1
Hypothesis test of signiﬁcant regression:
Null
Alternative
Test
Rejection
hypothesis
hypothesis
statistic
region
β1 = 0
β1 ̸= 0
F = MSR/MSE
F ≥Fα,1,n−2
11.1.7
Test for linearity of regression
Suppose there are k distinct values of x, {x1, x2, . . . , xk}, ni observations for
xi, and n = n1 + n2 + · · · + nk.
Deﬁnitions:
(1) yij = the jth observation on the random variable Yi.
(2) Ti =
ni

j=1
yij,
yi. = Ti/ni
(3) SSPE = sum of squares due to pure error
=
k

i=1
ni

j=1
(yij −yi.)2 =
k

i=1
ni

j=1
y2
ij −
k

i=1
T 2
i
ni
(4) SSLF = Sum of squares due to lack of ﬁt = SSE −SSPE
Hypothesis test:
Null
Alternative
Test
Rejection
hypothesis
hypothesis
statistic
region
Linear regression
Lack of ﬁt
F = SSLF/(k −2)
SSPE/(n −k)
F ≥Fα,k−2,n−k
11.1.8
Sample correlation coeﬃcient
The sample correlation coeﬃcient is a measure of linear association and is
deﬁned by
r = .β1
+
Sxx
Syy
=
Sxy

SxxSyy
.
(11.5)
c
⃝2000 by Chapman & Hall/CRC

Hypothesis tests:
Null
Alternative
Test
Rejection
hypothesis
hypothesis
statistic
region
ρ = 0
ρ > 0
T = R√n −2
√
1 −R2 =
.β1
S.β1
T ≥tα,n−2
(1)
ρ < 0
T ≤−tα,n−2
(2)
ρ ̸= 0
|T| ≥tα/2,n−2
(3)
If X and Y have a bivariate normal distribution:
ρ = ρ0
ρ > ρ0
Z =
√n −3
2
Z ≥zα
(4)
ρ < ρ0
× ln
(1 + R)(1 −ρ0)
(1 −R)(1 + ρ0)

Z ≤−zα
(5)
ρ ̸= ρ0
|Z| ≥zα/2
(6)
11.1.9
Example
Example 11.63:
A recent study at a manufacturing facility examined the relationship
between the noon temperature (Fahrenheit) inside the plant and the number of defective
items produced during the day shift. The data are given in the following table.
Noon
Number
Noon
Number
temperature (x)
defective (y)
temperature (x)
defective (y)
68
27
74
48
78
52
65
33
71
39
72
45
69
22
73
51
66
21
67
29
75
66
77
65
(1) Find the regression equation using temperature as the independent variable and
construct the anova table.
(2) Test for a signiﬁcant regression. Use α = .05.
(3) Find a 95% conﬁdence interval for the mean number of defective items produced
when the temperature is 68◦F.
(4) Find a 99% prediction interval for a temperature of 75◦F.
Solution:
(S1) .β1 = Sxy
Sxx = 640.5
204.25 = 3.1359
.β0 = y −.β1x = 41.5 −(3.1359)(71.25) = −181.93
Regression line: .y = −181.93 + 3.1359x
(S2)
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Regression
2008.5
1
2008.5
31.16
Error
644.5
10
64.4
Total
2653.0
11
c
⃝2000 by Chapman & Hall/CRC

(S3) Hypothesis test of signiﬁcant regression:
H0: β1 = 0
Ha: β1 ̸= 0
TS: F = MSR/MSE
RR: F ≥F.05,1,10 = 4.96
Conclusion: The value of the test statistic (F = 31.16) lies in the rejection region.
There is evidence to suggest a signiﬁcant regression. Note: this test is equivalent
to the t test in section 11.1.3 with β10 = 0.
(S4) A 95% conﬁdence interval for µY |68:
.y0 ± t.025,10 · s .Y0 = 31.31 ± (2.228)(2.9502) = (24.74, 37.88)
(S5) A 99% prediction interval for y0 = −181.93 + (3.1359)(75) = 53.26
.y0 ± t.005,10 · s .Y0−Y0 = 53.26 ± (3.169)(8.6172) = (25.95, 80.56)
11.2
MULTIPLE LINEAR REGRESSION
Let there be n observations of the form (x1i, x2i, . . . , xki, yi) such that yi is
an observed value of the random variable Yi. Assume there exist constants
β0, β1, . . . , βk such that
Yi = β0 + β1x1i + · · · + βkxki + ϵi
(11.6)
where ϵ1, ϵ2, . . . , ϵn are independent, normal random variables having mean 0
and variance σ2.
Assumptions
In terms of ϵi’s
In terms of Yi’s
ϵi’s are normally distributed
Yi’s are normally distributed
E [ϵi] = 0
E [Yi] = β0 + β1x1i + · · · + βkxki
Var [ϵi] = σ2
Var [Yi] = σ2
Cov [ϵi, ϵj] = 0, i ̸= j
Cov [Yi, Yj] = 0, i ̸= j
Notation:
Let Y be the random vector of responses, y be the vector of observed re-
sponses, β be the vector of regression coeﬃcients, ϵ be the vector of random
errors, and let X be the design matrix:
Y=


Y1
Y2
...
Yn

y=


y1
y2
...
yn

β=


β0
β1
...
βk

ϵ=


ϵ1
ϵ2
...
ϵn

X=


1 x11 x21 · · · xk1
1 x12 x22 · · · xk2
...
...
...
...
...
1 x1n x2n · · · xkn


(11.7)
c
⃝2000 by Chapman & Hall/CRC

The model can now be written as Y = Xβ + ϵ where ϵ ∼Nn(0, σ2In) or
equivalently Y ∼Nn(Xβ, σ2In).
Principle of least squares: The sum of squared deviations about the true
regression line is
S(β) =
n

i=1
[yi −(β0 + β1x1i + · · · + βkxki)]2 = ∥y −Xβ∥2.
(11.8)
The vector .β = [.β0, .β1, . . . , .βk]T that minimizes S(β) is the vector of least
squares estimates. The estimated regression line or least squares line is y =
.β0 + .β1x1 + · · · + .βkxk.
The normal equations may be written as

XTX
 .β = XTy.
11.2.1
Least squares estimates
If the matrix XTX is non–singular, then .β = (XTX)−1XTy.
The ith predicted (ﬁtted) value: .yi = .β0 + .β1x1i + · · · + .βkxki
(for i = 1, 2, . . . , n), .y = X.β.
The ith residual: ei = yi −.yi, i = 1, 2, . . . , n, e = y −.y.
Properties: For i = 0, 1, 2, , . . . , k and j = 0, 1, 2, . . . , k:
(1) E [.βi] = βi.
(2) Var [.βi] = ciiσ2, where cij is the value in the ith row and jth column of
the matrix (XTX)−1.
(3) .βi is normally distributed.
(4) Cov[.βi, .βj] = cijσ2, i ̸= j.
11.2.2
Sum of squares
n

i=1
(yi −y)2


	
SST
=
n

i=1
(.yi −y)2


	
SSR
+
n

i=1
(yi −.yi)2


	
SSE
SST = total sum of squares = ||y −y1||2 = yTy −ny2
SSR = sum of squares due to regression = ||X.β −y1||2 = .β
TXTy −ny2
SSE = sum of squares due to error = ||y −X.β||2 = yTy −.βXTy
where 1 = [1, 1, . . . , 1]T is a column vector of all 1’s.
(1) .σ2 = s2 =
SSE
n −k −1,
E

S2
= σ2
(2) (n −k −1)S2
σ2
has a chi–square distribution with n −k −1 degrees of
freedom, and S2 and .βi are independent.
c
⃝2000 by Chapman & Hall/CRC

(3) The coeﬃcient of multiple determination:
R2 = SSR
SST = 1 −SSE
SST
(4) Adjusted coeﬃcient of multiple determination:
R2
a = 1 −

n −1
n −k −1
 SSE
SST = 1 −(1 −R2)

n −1
n −k −1

11.2.3
Inferences concerning the regression coeﬃcients
(1) T =
.βi −βi
S√cii
has a t distribution with n−k−1 degrees of freedom.
(2) A 100(1 −α)% conﬁdence interval for βi has as endpoints
.βi ± tα/2,n−k−1 · s√cii
(3) Hypothesis test for βi:
Null
Alternative
Test
Rejection
hypothesis
hypotheses
statistic
regions
βi = βi0
βi > βi0
T =
.βi −βi0
S√cii
T ≥tα,n−k−1
(1)
βi < βi0
T ≤−tα,n−k−1
(2)
βi ̸= βi0
|T| ≥tα/2,n−k−1
(3)
11.2.4
The mean response
The mean response of Y given x = x0 = [1, x10, x20, . . . , xk0]T is
µY |x10,x20,...,xk0 = β0 + β1x10 + · · · + βkxk0. The random variable
.Y0 = .β
Tx0 = .β0 + .β1x10 + · · · + .βkxk0 is used to estimate µY |x10,x20,...,xk0.
(1) E [.Y0] = β0 + β1x10 + · · · + βkxk0
(2) Var [.Y0] = σ2xT
0 (XTX)−1x0
(3) .Y0 has a normal distribution.
(4) T =
.Y0 −µY |x10,x20,...,xk0
S

xT
0 (XTX)−1x0
has a t distribution with n −k −1 degrees of freedom.
(5) A 100(1 −α)% conﬁdence interval for µY |x10,x20,...,xk0 has as endpoints
.y0 ± tα/2,n−k−1 · s

xT
0 (XTX)−1x0. ”
(6) Hypothesis test:
c
⃝2000 by Chapman & Hall/CRC

Null
Alternative Test
Rejection
hypothesis hypotheses
statistic
regions
β0 + β1x10 + · · · + βkxk0 = y0 = µ0
y0 > µ0
T =
.Y0 −µ0
S

xT
0 (XTX)−1x0
T ≥tα,n−k−1
(1)
y0 < µ0
T ≤−tα,n−k−1
(2)
y0 ̸= µ0
|T| ≥tα/2,n−k−1
(3)
11.2.5
Prediction interval
A prediction interval for a value y0 of the random variable Y0 = β0 + β1x10 +
· · · + βkxk0 + ϵ0 is obtained by considering the random variable .Y0 −Y0.
(1) E [.Y0 −Y0] = 0
(2) Var [.Y0 −Y0] = σ2 
1 + xT
0 (XTX)−1x0

(3) .Y0 −Y0 has a normal distribution.
(4) T =
.Y0 −Y0
S

1 + xT
0 (XTX)−1x0
has a t distribution with n −k −1 degrees of freedom.
(5) A 100(1 −α)% prediction interval for y0 has as endpoints
.y0 ± tα/2,n−2 · s

1 + xT
0 (XTX)−1X0
11.2.6
Analysis of variance table
Source of
Sum of Degrees of Mean
variation
squares freedom
square
Computed F
Regression SSR
k
MSR = SSR
k
MSR/MSE
Error
SSE
n −k −1
MSE =
SSE
n −k −1
Total
SST
n −1
Hypothesis test of signiﬁcant regression:
H0: β1 = β2 = · · · = βk = 0
Ha: βi ̸= 0 for some i
TS: F = MSR/MSE
RR: F ≥Fα,k,n−k−1
c
⃝2000 by Chapman & Hall/CRC

11.2.7
Sequential sum of squares
Deﬁne
g =


g0
g1
...
gk

= XTy =


n

i=1
yi
n

i=1
x1iyi
...
n

i=1
xkiyi


(11.9)
SSR =
k

j=0
.βjgj −ny2
SS(β1, β2, . . . , βr) = the sum of squares due to β1, β2, . . . , βr
=
r

j=1
.βjgj −ny2
SS(β1) = the regression sum of squares due to x1
=
1

j=0
.βjgj −ny2
SS(β2|β1) = the regression sum of squares due to x2 given
x1 is in the model
= SS(β1, β2) −SS(β1) = .β2g2
SS(β3|β1, β2) = the regression sum of squares due to x3 given
x1, x2 are in the model
= SS(β1, β2, β3) −SS(β1, β2) = .β3g3
...
SS(βr|β1, . . . , βr−1) = the regression sum of squares due to xr given
x1, . . . , xr−1 are in the model
= SS(β1, . . . , βr) −SS(β1, . . . , βr−1) = .βrgr
11.2.8
Partial F test
Deﬁnitions:
(1) Full model:
yi = β0 + β1x1i + · · · + βrxri + βr+1x(r+1)i + · · · + βkxki + ϵi
(2) SSE(F) = sum of squares due to error in the full model.
=
n

i=1
(yi −.yi)2 where
c
⃝2000 by Chapman & Hall/CRC

.yi = ˆβ0 + ˆβ1x1i + · · · + ˆβrxri + ˆβr+1x(r+1)i + · · · + ˆβkxki
(3) Reduced model: yi = β0 + β1x1i + · · · + βrxri + ϵi
(4) SSE(R) = sum of squares due to error in the reduced model.
=
n

i=1
(yi −.yi)2 where .yi = ˆβ0 + ˆβ1x1i + · · · + ˆβrxri
(5) SS(βr+1, . . . , |β1, . . . , βr) = regression sum of squares due to
xr+1, . . . , xk given x1, . . . , xr are in the model. It is given by:
SS(β1, . . . , βr, βr+1, . . . , βk) −SS(β1, . . . , βr) =
k

j=r+1
.βjgj
Hypothesis test:
H0: βr+1 = βr+2 = · · · = βk = 0
Ha: βm ̸= 0 for some m = r + 1, r + 2, . . . , k
TS: F = [SSE(R) −SSE(F)]/(k −r)
SSE(F)/(n −k −1)
= SS(βr+1, . . . , βk|β1, . . . , βr)/(k −r)
SSE(R)/(n −k −1)
RR: F ≥Fα,k−r,n−k−1
11.2.9
Residual analysis
Let hii be the diagonal entries of the HAT matrix deﬁned by
H = X(XTX)−1XT.
Standardized residuals:
ei
√
MSE
= ei
s ,
i = 1, 2, . . . , n
Studentized residuals: e∗
i =
ei
s√1 −hii
,
i = 1, 2, . . . , n
Deleted studentized residuals:
d∗
i = ei
+
n −k −2
s2(1 −hii) −e2
i
,
i = 1, 2, . . . , n
Cook’s distance: Di =
e2
i
(k + 1)s2

hii
(1 −hii)2

,
i = 1, 2, . . . , n
Press residuals: δi = yi −.yi,−i =
ei
1 −hii
,
i = 1, 2, . . . , n
where .yi,−i is the ith predicted value by the model without using the ith
observation in calculating the regression coeﬃcients.
c
⃝2000 by Chapman & Hall/CRC

Prediction sum of squares = PRESS =
n

i=1
δ2
i
n

i=1
|δi|: is used for cross validation, it is less sensitive to large press residuals.
11.2.10
Example
Example 11.64:
A university foundation oﬃce recently investigated factors that
might contribute to alumni donations. Fifteen years were randomly selected and the
total donations (in millions of dollars), United States savings rate, the unemployment
rate, and the number of games won by the school basketball team are given in the table
below.
Donations
Savings
Unemployment
Games
(y)
rate (%) (x1)
rate (%) (x2)
won
27.80
15.1
4.7
26
17.41
11.3
5.5
14
18.51
13.6
5.6
24
30.09
16.1
4.9
15
34.22
17.8
5.1
17
20.28
13.3
5.6
18
26.98
15.5
4.6
20
27.32
16.7
4.7
9
18.74
13.8
5.8
18
35.52
17.4
4.5
17
15.52
10.9
6.1
19
17.75
12.5
5.7
16
22.94
12.9
5.0
21
41.47
18.8
4.2
19
22.95
16.1
5.4
18
(1) Find the regression equation using donation as the dependent variable and con-
struct the anova table.
(2) Test for a signiﬁcant regression. Use α = .05.
(3) Is there any evidence to suggest the number of games won by the school basketball
team aﬀects alumni donations. Use α = .10.
Solution:
(S1) Regression line: .y = 23.89 −5.54x1 + 1.95x2 + .057x3
(S2)
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Regression
755.64
3
251.88
38.87
Error
71.28
11
6.48
Total
826.92
14
(S3) Hypothesis test of signiﬁcant regression:
c
⃝2000 by Chapman & Hall/CRC

H0: β1 = β2 = β3 = 0
Ha: βi ̸= 0 for some i
TS: F = MSR/MSE
RR: F ≥F.05,3,11 = 3.58
Conclusion: The value of the test statistic (F = 38.87) lies in the rejection region.
There is evidence to suggest a signiﬁcant regression.
(S4) Hypothesis test for β3:
H0: β3 = 0
Ha: β3 ̸= 0
TS: T =
.β3 −0
S√cii
RR: |T| ≥t.05,11 = 1.796
t = .057/.1719 = .33
Conclusion: The value of the test statistic does not lie in the rejection region.
There is no evidence to suggest the number of games won by the basketball team
aﬀects alumni donations.
11.3
ORTHOGONAL POLYNOMIALS
Polynomial regression models may contain several independent variables and
each independent variable may appear in the model to various powers. Let
(x1, y1), (x2, y2), . . . , (xn, yn) be n pairs of observations such that yi is an
observed value of the random variable Yi.
Assume there exist constants
β0, β1, . . . , βp such that
Yi = β0 + β1xi + β2x2
i + · · · + βpxp
i + ϵi
(11.10)
where ϵ1, ϵ2, . . . , ϵn are independent, normal random variables having mean 0
and variance σ2.
In order to determine the best polynomial model, the regression coeﬃcients
{βj} must be recalculated for various values of p.
If the values {xi} are
evenly spaced, then orthogonal polynomials are often used to determine the
best model. This technique presents certain computational advantages and
quickly isolates signiﬁcant eﬀects.
The orthogonal polynomial model is
Yi = α0ξ0(x) + α1ξ1(x) + · · · + αpξp(x) + ϵi
(11.11)
where ξi(x) are orthogonal polynomials in x of degree i.
The orthogonal
polynomials have the property
n

i=1
ξh(xi) · ξj(xi) = 0 , when h ̸= j and xi = x0 + iδ
(11.12)
c
⃝2000 by Chapman & Hall/CRC

The least square estimator for αj is given by
.αj =
n
i=1
yiξj(xi)
n
i=1
[ξj(xi)]2
(11.13)
The estimator .αj (for j = 1, 2, . . . , n −1) is a normal random variable with
mean αj if j ≤p and mean 0 if j > p, and variance σ2
= n

i=1
[ξj(xi)]2 . An
estimate of σ2 is given by
s2 =
n
i=1
y2
i −
p
j=0
(.αj)2
 n
i=1
[ξj(xi)]2

n −p −1
.
(11.14)
Each ratio
(αj −.αj)
-
i
[ξj(xi)]2
s
(with .αj = 0 for j > p) has a t distribution.
The contribution of each term may be tested; if it is not signiﬁcant then the
term may be discarded without recalculating the previously obtained coeﬃ-
cients (i.e., the tests for signiﬁcance eﬀects are isolated).
In order to tabulate the values of the orthogonal polynomials for repeated use,
the values of xi are assumed to be one unit apart, and ξ′
j(x) is deﬁned to be
a multiple, λj, of ξ′
j(x) so that ξj(x) has a leading coeﬃcient of unity. This
adjustment makes all the tabulated values ξ′
j(xi) integers. Thus, in particular:
ξ′
1(x) = λ1ξ1(x) = λ1(x −x)
ξ′
2(x) = λ2ξ2(x) = λ2

(x −x)2 −n2 −1
12

ξ′
3(x) = λ3ξ3(x) = λ3

(x −x)3 −(x −x)
3n2 −7
20

ξ′
4(x) = λ4ξ4(x) = λ4
'
(x −x)4
−(x −x)2
3n2 −13
14

+ 3(n2 −1)(n2 −9)
560
,
(11.15)
and, in general, ξ′
r(x) = λrξr with the recursion relation
ξ0 = 1,
ξ1 = (x −x)
ξr+1 = ξ1ξr −r2(n2 −r2)
4(4r2 −1) ξr−1
(11.16)
The tables below provide values of {ξ′
j(xi)} for various values of n and j.
When n > 10 only half of the table is shown (the other half can be found
c
⃝2000 by Chapman & Hall/CRC

by symmetry). The two values at the bottom of each column are the values
Dj =
n
i=1
[ξj(xi)]2 and λj.
Example 11.65:
Consider n = 11 observations of data from a quadratic with noise:
speciﬁcally the values are {1, 4, 9, . . . , 121} + 0.1{n1, n2, . . . , n11} where each {ni} is
a standard normal random variable. In order to ﬁt a polynomial regression we must
assume some maximum polynomial power of interest; we presume that terms up to
degree three might be of interest. The {ξ′
j(xi)} table for n = 11 (see page 279) gives
the values for x ≤0; the values for x > 0 are given by symmetry. From the data and
the {ξ′
j(xi)} table we compute
xi
yi
ξ′
1
ξ′
2
ξ′
3
1
1.53
−5
15
−30
2
4.27
−4
6
6
3
8.94
−3
−1
22
4
15.92
−2
−6
23
5
24.99
−1
−9
14
6
35.94
0
−10
0
7
48.77
1
−9
−14
8
64.13
2
−6
−23
9
80.80
3
−1
−22
10
99.89
4
6
−6
11
120.99
5
15
30
Di =  ξ2
i
. . .
110
858
4290
λi
. . .
1
1
5/6
 yiξi
. . .
1315
869.5
−12.26
The values for .αi are found by:
x = 6
y = 46.01
.α1 = 1315
110 = 11.96
.α2 = 869.5
858
= 1.013
.α3 = −12.26
4290
= −.00029
(11.17)
Using the value of n and {λi} in equation (11.15), the orthogonal polynomials are
ξ′
1(x) = x −8
ξ′
2(x) = x2 −12x + 26
ξ′
3(x) = 1
6(5x3 −90x2 + 451x −546)
(11.18)
The regression equation is .y = y+ .α1ξ′
1(x)+ .α2ξ′
2(x)+ .α3ξ′
3(x) = .85−.42x+1.06x2 −
.002x3. The predicted values are {1.49, 4.22, 9.04, . . . , 120.92}.
c
⃝2000 by Chapman & Hall/CRC

The signiﬁcance of the coeﬃcients may be determined by computing:
Error
Sum of
Degrees
Mean
mean
Computed
Quantity
squares
freedom
square
square
F statistic
(1) n  y2
i −( yi)2
n
16615
10
(2) linear regression
15733
1
15733
881.4
17.9

yiξ′
1
2* 
ξ2
1
(3) quadratic regression
881.2
1
881.2
0.15
5958

yiξ′
2
2* 
ξ2
2
(3) cubic regression
0.035
1
0.035
0.11
0.31

yiξ′
3
2* 
ξ2
3
residual sum of squares:
0.018
7
0.0026
0.095
0.027
(1) −(2) −(3) −(4)
where the computed F statistic is given by F = (mean square)/(error mean square).
Conclusion: The cubic term should probably not be included in the model. The regres-
sion equation then becomes .y = y+.α1ξ′
1(x)+.α2ξ′
2(x)+0·ξ′
3(x) = .59−.20x+1.0134x2.
The predicted values are then {1.40, 4.24, 9.11, . . . , 121.00}.
11.3.1
Tables for orthogonal polynomials
n = 3 points
ξ′
1
ξ′
2
−1
1
0
−2
1
1
D 2
6
λ 1
3
n = 4 points
ξ′
1 ξ′
2
ξ′
3
−3
1
−1
−1 −1
3
1 −1
−3
3
1
1
D 20
4
20
λ 2
1
10/3
n = 5 points
ξ′
1 ξ′
2 ξ′
3
ξ′
4
−2
2 −1
1
−1 −1
2
−4
0 −2
0
6
1 −1 −2
−4
2
2
1
1
D 10 14 10
70
λ 1
1 5/6 35/12
n = 6 points
ξ′
1 ξ′
2
ξ′
3
ξ′
4
ξ′
5
−5
5 −5
1
−1
−3 −1
7 −3
5
−1 −4
4
2 −10
1 −4 −4
2
10
3 −1 −7 −3
−5
5
5
5
1
1
D 70 84 180
28
252
λ 2 3/2
5/3 7/12 21/10
n = 7 points
ξ′
1 ξ′
2 ξ′
3
ξ′
4 ξ′
5
−3
5 −1
3 −1
−2
0
1 −7
4
−1 −3
1
1 −5
0 −4
0
6
0
1 −3 −1
1
5
2
0 −1 −7 −4
3
5
1
3
1
D 28 84
6 154
84
λ 1
1 1/6 7/12 7/20
n = 8 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−7
7 −7
7
−7
−5
1
5 −13
23
−3 −3
7
−3 −17
−1 −5
3
9 −15
1 −5 −3
9
15
3 −3 −7
−3
17
5
1 −5 −13 −23
7
7
7
7
7
D 168 168 264 616 2184
λ
2
1
2/3 7/12
7/10
n = 9 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−4
28 −14
14
−4
−3
7
7 −21
11
−2
−8
13 −11
−4
−1 −17
9
9
−9
0 −20
0
18
0
1 −17
−9
9
9
2
−8 −13 −11
4
3
7
−7 −21 −11
4
28
14
14
4
D 60 2772 990 2002 468
λ 1
3
5/6
7/12 3/20
c
⃝2000 by Chapman & Hall/CRC

n = 10 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−9
6 −42
18
−6
−7
2
14 −22
14
−5 −1
35 −17
−1
−3 −3
31
3 −11
−1 −4
12
18
−6
1 −4 −12
18
6
3 −3 −31
3
11
5 −1 −35 −17
1
7
2 −14 −22 −14
9
6
42
18
6
D 330 132 8580 2860 780
λ
2
1/2
5/3
5/12 1/10
n = 11 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4 ξ′
5
−5
15 −30
6 −3
−4
6
6 −6
6
−3
−1
22 −6
1
−2
−6
23 −1 −4
−1
−9
14
4 −4
0 −10
0
6
0
D 110 858 4290 286 156
λ
1
1
5/6 1/12 1/40
n = 12 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−11
55 −33
33
−33
−9
25
3 −27
57
−7
1
21 −33
21
−5
−17
25 −13
−29
−3
−29
19
12
−44
−1
−35
7
28
−20
D 572 12012 5148 8008 15912
λ
2
3
2/3
7/24
3/20
n = 13 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−6
22 −11
99 −22
−5
11
0
−66
33
−4
2
6
−96
18
−3
−5
8
−54 −11
−2 −10
7
11 −26
−1 −13
4
64 −20
0 −14
0
84
0
D 182 2002 572 68068 6188
λ
1
1
1/6
7/12 7/120
n = 14 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−13
13 −143
143
−143
−11
7
−11
−77
187
−9
2
66
−132
132
−7 −2
98
−92
−28
−5 −5
95
−13
−139
−3 −7
67
63
−145
−1 −8
24
108
−60
D 910 728 97240 136136 235144
λ
2
1/2
5/3
7/12
7/30
n = 15 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−7
91
−91
1001
−1001
−6
52
−13
−429
1144
−5
19
35
−869
979
−4
−8
58
−704
44
−3
−29
61
−249
−751
−2
−44
49
251
−1000
−1
−53
27
621
−675
0
−56
0
756
0
D 280 37128 39780 6466460 10581480
λ
1
3
5/6
35/12
21/20
n = 16 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−15
35
−455
273
−143
−13
21
−91
−91
143
−11
9
143
−221
143
−9
−1
267
−201
33
−7
−9
301
−101
−77
−5 −15
265
23
−131
−3 −19
179
129
−115
−1 −21
63
189
−45
D 1360 5712 1007760 470288 201552
λ
2
1
10/3
7/12
1/10
n = 17 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−8
40 −28
52
−104
−7
25
−7
−13
91
−6
12
7
−39
104
−5
1
15
−39
39
−4
−8
18
−24
−36
−3 −15
17
−3
−83
−2 −20
13
17
−88
−1 −23
7
31
−55
0 −24
0
36
0
D 408 7752 3876 16796 100776
λ
1
1
1/6
1/12
1/20
n = 18 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−17
68
−68
68
−884
−15
44
−20
−12
676
−13
23
13
−47
871
−11
5
33
−51
429
−9
−10
42
−36
−156
−7
−22
42
−12
−588
−5
−31
35
13
−733
−3
−37
23
33
−583
−1
−40
8
44
−220
D 1938 23256 23256 28424 6953544
λ
2
3/2
1/3
1/12
3/10
c
⃝2000 by Chapman & Hall/CRC

n = 19 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−9
51
−204
612 −102
−8
34
−68
−68
68
−7
19
28
−388
98
−6
6
89
−453
58
−5
−5
120
−354
−3
−4
−14
126
−168
−54
−3
−21
112
42
−79
−2
−26
83
227
−74
−1
−29
44
352
−44
0
−30
0
396
0
D 570 13566 213180 2288132 89148
λ
1
1
5/6
7/12
1/40
n = 20 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−19
57
−969
1938
−1938
−17
39
−357
−102
1122
−15
23
85
−1122
1802
−13
9
377
−1402
1222
−11
−3
539
−1187
187
−9
−13
591
−687
−771
−7
−21
553
−77
−1351
−5
−27
445
503
−1441
−3
−31
287
948
−1076
−1
−33
99
1188
−396
D 2660 17556 4903140 22881320 31201800
λ
2
1
10/3
35/24
7/20
n = 21 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−10
190
−285
969
−3876
−9
133
−114
0
1938
−8
82
12
−510
3468
−7
37
98
−680
2618
−6
−2
149
−615
788
−5
−35
170
−406
−1063
−4
−62
166
−130
−2354
−3
−83
142
150
−2819
−2
−98
103
385
−2444
−1
−107
54
540
−1404
0
−110
0
594
0
D 770 201894 432630 5720330 121687020
λ
1
3
5/6
7/12
21/40
n = 22 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−21
35 −133
1197
−2261
−19
25
−57
57
969
−17
16
0
−570
1938
−15
8
40
−810
1598
−13
1
65
−775
663
−11
−5
77
−563
−363
−9 −10
78
−258
−1158
−7 −14
70
70
−1554
−5 −17
55
365
−1509
−3 −19
35
585
−1079
−1 −20
12
702
−390
D 3542 7084 96140 8748740 40562340
λ
2
1/2
1/3
7/12
7/30
n = 23 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−11
77
−77
1463
−209
−10
56
−35
133
76
−9
37
−3
−627
171
−8
20
20
−950
152
−7
5
35
−955
77
−6
−8
43
−747
−12
−5
−19
45
−417
−87
−4
−28
42
−42
−132
−3
−35
35
315
−141
−2
−40
25
605
−116
−1
−43
13
793
−65
0
−44
0
858
0
D 1012 35420 32890 13123110 340860
λ
1
1
1/6
7/12
1/60
n = 24 points
ξ′
1
ξ′
2
ξ′
3
ξ′
4
ξ′
5
−23
253
−1771
253
−4807
−21
187
−847
33
1463
−19
127
−133
−97
3743
−17
73
391
−157
3553
−15
25
745
−165
2071
−13
−17
949
−137
169
−11
−53
1023
−87
−1551
−9
−83
987
−27
−2721
−7
−107
861
33
−3171
−5
−125
665
85
−2893
−3
−137
419
123
−2005
−1
−143
143
143
−715
D 4600 394680 17760600 394680 177928920
λ
2
3
10/3
1/12
3/10
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 12
Analysis of Variance
Contents
12.1
One-way anova
12.1.1
Sum of squares
12.1.2
Properties
12.1.3
Analysis of variance table
12.1.4
Multiple comparison procedures
12.1.5
Contrasts
12.1.6
Example
12.2
Two-way anova
12.2.1
One observation per cell
12.2.2
Analysis of variance table
12.2.3
Nested classiﬁcations with equal samples
12.2.4
Nested classiﬁcations with unequal samples
12.2.5
Two-factor experiments
12.2.6
Example
12.3
Three-factor experiments
12.3.1
Models and assumptions
12.3.2
Sum of squares
12.3.3
Mean squares and properties
12.3.4
Analysis of variance table
12.4
Manova
12.5
Factor analysis
12.6
Latin square design
12.6.1
Models and assumptions
12.6.2
Sum of squares
12.6.3
Mean squares and properties
12.6.4
Analysis of variance table
c
⃝2000 by Chapman & Hall/CRC

12.1
ONE-WAY ANOVA
Let there be k treatments, or populations, and independent random samples
of size ni (for i = 1, 2, . . . , k) from each population, and let N = n1 + n2 +
· · · + nk. Let Yij be the jth random observation in the ith treatment group.
Assume each population is normally distributed with mean µi and common
variance σ2. In a ﬁxed eﬀects, or model I, experiment the treatment
levels are predetermined. In a random eﬀects, or model II, experiment,
the treatment levels are selected at random.
Notation:
(1) Dot notation is used to indicate a sum over all values of the selected
subscript.
(2) The random error term is ϵij and an observed value is eij.
Fixed eﬀects experiment:
Model:
Yij = µi + ϵij = µ + αi + ϵij
i = 1, 2, . . . , k,
j = 1, 2, . . . , ni
Assumptions:
The ϵij’s are independent, normally distributed with
mean 0 and variance σ2 (ϵij
ind
∼N(0, σ2)),
k

i=1
αi = 0
Random eﬀects experiment:
Model:
Yij = µi + ϵij = µ + Ai + ϵij
i = 1, 2, . . . , k,
j = 1, 2, . . . , ni
Assumptions:
ϵij
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α)
The ϵij’s are independent of the Ai’s.
12.1.1
Sum of squares
k

i=1
ni

j=1
(yij −¯y..)2


	
SST
=
k

i=1
ni(yi. −y..)2


	
SSA
+
k

i=1
ni

j=1
(yij −yi.)2


	
SSE
(12.1)
Notation:
yij = observed value of Yij
yi. = 1
ni
ni

j=1
yij
= mean of the observations in the ith sample
y.. = 1
N
k

i=1
ni

j=1
yij = mean of all observations
c
⃝2000 by Chapman & Hall/CRC

Ti. =
ni

j=1
yij
= sum of the observations in the ith sample
T.. =
k

i=1
ni

j=1
yij = sum of all observations
SST = total sum of squares =
k

i=1
ni

j=1
(yij −y..)2 =
k

i=1
ni

j=1
y2
ij −T 2
..
N
SSA = sum of squares due to treatment
=
k

i=1
ni(yi. −y..)2 =
k

i=1
T 2
i.
ni
−T 2
..
N
SSE = sum of squares due to error =
k

i=1
ni

j=1
(yij −yi.)2 = SST −SSA
12.1.2
Properties
Expected value
Mean square
Fixed model
Random model
MSA = S2
A = SSA
k −1
σ2 +
k
i=1
niα2
i
k −1
σ2 +
1
k −1


n −
k
i=1
n2
i
n


σ2
α
MSE = S2 =
SSE
N −k
σ2
σ2
F = S2
A/S2 has an F distribution with k −1 and N −k degrees of freedom.
12.1.3
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Treatments
SSA
k −1
MSA
MSA/MSE
(between groups)
Error
SSE
N −k
MSE
(within groups)
Total
SST
N −1
Hypothesis test of signiﬁcant regression:
c
⃝2000 by Chapman & Hall/CRC

H0: µ1 = µ2 = · · · = µk
(Fixed eﬀects model: α1 = α2 = · · · = αk = 0)
(Random eﬀects model: σ2
α = 0)
Ha: at least two of the means are unequal
(Fixed eﬀects model: αi ̸= 0 for some i)
(Random eﬀects model: σ2
α ̸= 0)
TS: F = S2
A/S2
RR: F ≥Fα,k−1,N−k
12.1.4
Multiple comparison procedures
12.1.4.1
Tukey’s procedure
Equal sample sizes:
Let n = n1 = n2 = · · · = nk and let Qα,ν1,ν2 be a critical value of the Studen-
tized Range distribution (see page 76). The set of intervals with endpoints
(yi. −yj.) ± Qα,k,k(n−1) ·

s2/n for all i and j, i ̸= j
(12.2)
is a collection of simultaneous 100(1 −α)% conﬁdence intervals for the diﬀer-
ences between the true treatment means, µi −µj. Each conﬁdence interval
that does not include zero suggests µi ̸= µj at the α signiﬁcance level.
Unequal sample sizes:
The set of conﬁdence intervals with endpoints (N = n1 + n2 + · · · + nk)
(yi. −yj.) ± 1
√
2 Qα,k,N−k · s
+
1
ni
+ 1
nj
for all i and j, i ̸= j
(12.3)
is a collection of simultaneous 100(1 −α)% conﬁdence intervals for the diﬀer-
ences between the true treatment means, µi −µj.
12.1.4.2
Duncan’s multiple range test
Let n = n1 = n2 = · · · = nk and let rα,ν1,ν2 be a critical value for Dun-
can’s multiple range test (see page 285). Duncan’s procedure for determining
signiﬁcant diﬀerences between each treatment group at the joint signiﬁcance
level α is:
(1) Deﬁne Rp = rα,p,k(n−1) ·
-
s2
n
for p = 2, 3, . . . , k.
(2) List the sample means in increasing order.
(3) Compare the range of every subset of p sample means (for p = 2, 3, . . . , k)
in the ordered list with Rp.
(4) If the range of a p–subset is less than Rp then that subset of ordered
means in not signiﬁcantly diﬀerent.
c
⃝2000 by Chapman & Hall/CRC

12.1.4.3
Duncan’s multiple range test
These tables contain critical values for the least signiﬁcant studentized ranges,
rα,p,ν, for Duncan’s multiple range test where α is the signiﬁcance level, p is
the number of successive values from an ordered list of k means of equal
sample sizes (p = 2, 3, . . . , k), and n is the degrees of freedom for the inde-
pendent estimate s2. These tables are from L. Hunter, “Critical Values for
Duncan’s New Multiple Range Test”, Biometrics, 1960, Volume 16, pages
671–685. Reprinted with permission from the Journal of American Statistical
Association.
Copyright 1960 by the American Statistical Association.
All
rights reserved.
c
⃝2000 by Chapman & Hall/CRC

Critical values for Duncan’s test, rα,p,n, for α = .05
c
⃝2000 by Chapman & Hall/CRC
n p = 2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97 17.97
2 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085 6.085
3 4.501 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516 4.516
4 3.927 4.013 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033 4.033
5 3.635 3.749 3.797 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814 3.814
6 3.461 3.587 3.649 3.680 3.694 3.697 3.697 3.697 3.697 3.697 3.697 3.697 3.697 3.697 3.697 3.697 3.697 3.697 3.697
7 3.344 3.477 3.548 3.588 3.611 3.622 3.626 3.626 3.626 3.626 3.626 3.626 3.626 3.626 3.626 3.626 3.626 3.626 3.626
8 3.261 3.399 3.475 3.521 3.549 3.566 3.575 3.579 3.579 3.579 3.579 3.579 3.579 3.579 3.579 3.579 3.579 3.579 3.579
9 3.199 3.339 3.420 3.470 3.502 3.523 3.536 3.544 3.547 3.547 3.547 3.547 3.547 3.547 3.547 3.547 3.547 3.547 3.547
10 3.151 3.293 3.376 3.430 3.465 3.489 3.505 3.516 3.522 3.525 3.526 3.526 3.526 3.526 3.526 3.526 3.526 3.526 3.526
11 3.113 3.256 3.342 3.397 3.435 3.462 3.480 3.493 3.501 3.506 3.509 3.510 3.510 3.510 3.510 3.510 3.510 3.510 3.510
12 3.082 3.225 3.313 3.370 3.410 3.439 3.459 3.474 3.484 3.491 3.496 3.498 3.499 3.499 3.499 3.499 3.499 3.499 3.499
13 3.055 3.200 3.289 3.348 3.389 3.419 3.442 3.458 3.470 3.478 3.484 3.488 3.490 3.490 3.490 3.490 3.490 3.490 3.490
14 3.033 3.178 3.268 3.329 3.372 3.403 3.426 3.444 3.457 3.467 3.474 3.479 3.482 3.484 3.484 3.485 3.485 3.485 3.485
15 3.014 3.160 3.250 3.312 3.356 3.389 3.413 3.432 3.446 3.457 3.465 3.471 3.476 3.478 3.480 3.481 3.481 3.481 3.481
16 2.998 3.144 3.235 3.298 3.343 3.376 3.402 3.422 3.437 3.449 3.458 3.465 3.470 3.473 3.477 3.478 3.478 3.478 3.478
17 2.984 3.130 3.222 3.285 3.331 3.366 3.392 3.412 3.429 3.441 3.451 3.459 3.465 3.469 3.473 3.475 3.476 3.476 3.476
18 2.971 3.118 3.210 3.274 3.321 3.356 3.383 3.405 3.421 3.435 3.445 3.454 3.460 3.465 3.470 3.472 3.474 3.474 3.474
19 2.960 3.107 3.199 3.264 3.311 3.347 3.375 3.397 3.415 3.429 3.440 3.449 3.456 3.462 3.467 3.470 3.472 3.473 3.474
20 2.950 3.097 3.190 3.255 3.303 3.339 3.368 3.391 3.409 3.424 3.436 3.445 3.453 3.459 3.464 3.467 3.470 3.472 3.473
24 2.919 3.066 3.160 3.226 3.276 3.315 3.345 3.370 3.390 3.406 3.420 3.432 3.441 3.449 3.456 3.461 3.465 3.469 3.471
30 2.888 3.035 3.131 3.199 3.250 3.290 3.322 3.349 3.371 3.389 3.405 3.418 3.430 3.439 3.447 3.454 3.460 3.466 3.470
40 2.858 3.006 3.102 3.171 3.224 3.266 3.300 3.328 3.352 3.373 3.390 3.405 3.418 3.429 3.439 3.448 3.456 3.463 3.469
60 2.829 2.976 3.073 3.143 3.198 3.241 3.277 3.307 3.333 3.355 3.374 3.391 3.406 3.419 3.431 3.442 3.451 3.460 3.467
120 2.800 2.947 3.045 3.116 3.172 3.217 3.254 3.287 3.314 3.337 3.359 3.377 3.394 3.409 3.423 3.435 3.446 3.457 3.466
∞2.772 2.918 3.017 3.089 3.146 3.193 3.232 3.265 3.294 3.320 3.343 3.363 3.382 3.399 3.414 3.428 3.442 3.454 3.466

Critical values for Duncan’s test, rα,p,n, for α = .01
c
⃝2000 by Chapman & Hall/CRC
n p = 2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03 90.03
2 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04 14.04
3 8.261 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321 8.321
4 6.512 6.677 6.740 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756 6.756
5 5.702 5.893 5.989 6.040 6.065 6.074 6.074 6.074 6.074 6.074 6.074 6.074 6.074 6.074 6.074 6.074 6.074 6.074 6.074
6 5.243 5.439 5.549 5.614 5.655 5.680 5.694 5.701 5.703 5.703 5.703 5.703 5.703 5.703 5.703 5.703 5.703 5.703 5.703
7 4.949 5.145 5.260 5.334 5.383 5.416 5.439 5.454 5.464 5.470 5.472 5.472 5.472 5.472 5.472 5.472 5.472 5.472 5.472
8 4.746 4.939 5.057 5.135 5.189 5.227 5.256 5.276 5.291 5.302 5.309 5.314 5.316 5.317 5.317 5.317 5.317 5.317 5.317
9 4.596 4.787 4.906 4.986 5.043 5.086 5.118 5.142 5.160 5.174 5.185 5.193 5.199 5.203 5.205 5.206 5.206 5.206 5.206
10 4.482 4.671 4.790 4.871 4.931 4.975 5.010 5.037 5.058 5.074 5.088 5.098 5.106 5.112 5.117 5.120 5.122 5.124 5.124
11 4.392 4.579 4.697 4.780 4.841 4.887 4.924 4.952 4.975 4.994 5.009 5.021 5.031 5.039 5.045 5.050 5.054 5.057 5.059
12 4.320 4.504 4.622 4.706 4.767 4.815 4.852 4.883 4.907 4.927 4.944 4.958 4.969 4.978 4.986 4.993 4.998 5.002 5.006
13 4.260 4.442 4.560 4.644 4.706 4.755 4.793 4.824 4.850 4.872 4.889 4.904 4.917 4.928 4.937 4.944 4.950 4.956 4.960
14 4.210 4.391 4.508 4.591 4.654 4.704 4.743 4.775 4.802 4.824 4.843 4.859 4.872 4.884 4.894 4.902 4.910 4.916 4.921
15 4.168 4.347 4.463 4.547 4.610 4.660 4.700 4.733 4.760 4.783 4.803 4.820 4.834 4.846 4.857 4.866 4.874 4.881 4.887
16 4.131 4.309 4.425 4.509 4.572 4.622 4.663 4.696 4.724 4.748 4.768 4.786 4.800 4.813 4.825 4.835 4.844 4.851 4.858
17 4.099 4.275 4.391 4.475 4.539 4.589 4.630 4.664 4.693 4.717 4.738 4.756 4.771 4.785 4.797 4.807 4.816 4.824 4.832
18 4.071 4.246 4.362 4.445 4.509 4.560 4.601 4.635 4.664 4.689 4.711 4.729 4.745 4.759 4.772 4.783 4.792 4.801 4.808
19 4.046 4.220 4.335 4.419 4.483 4.534 4.575 4.610 4.639 4.665 4.686 4.705 4.722 4.736 4.749 4.761 4.771 4.780 4.788
20 4.024 4.197 4.312 4.395 4.459 4.510 4.552 4.587 4.617 4.642 4.664 4.684 4.701 4.716 4.729 4.741 4.751 4.761 4.769
24 3.956 4.126 4.239 4.322 4.386 4.437 4.480 4.516 4.546 4.573 4.596 4.616 4.634 4.651 4.665 4.678 4.690 4.700 4.710
30 3.889 4.056 4.168 4.250 4.314 4.366 4.409 4.445 4.477 4.504 4.528 4.550 4.569 4.586 4.601 4.615 4.628 4.640 4.650
40 3.825 3.988 4.098 4.180 4.244 4.296 4.339 4.376 4.408 4.436 4.461 4.483 4.503 4.521 4.537 4.553 4.566 4.579 4.591
60 3.762 3.922 4.031 4.111 4.174 4.226 4.270 4.307 4.340 4.368 4.394 4.417 4.438 4.456 4.474 4.490 4.504 4.518 4.530
120 3.702 3.858 3.965 4.044 4.107 4.158 4.202 4.239 4.272 4.301 4.327 4.351 4.372 4.392 4.410 4.426 4.442 4.456 4.469
∞3.643 3.796 3.900 3.978 4.040 4.091 4.135 4.172 4.205 4.235 4.261 4.285 4.307 4.327 4.345 4.363 4.379 4.394 4.408

Critical values for Duncan’s test, rα,p,n, for α = .001
c
⃝2000 by Chapman & Hall/CRC
n p = 2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
1 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3 900.3
2 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69 44.69
3 18.28 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45 18.45
4 12.18 12.52 12.67 12.73 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75 12.75
5 9.714 10.05 10.24 10.35 10.42 10.46 10.48 10.49 10.49 10.49 10.49 10.49 10.49 10.49 10.49 10.49 10.49 10.49 10.49
6 8.427 8.743 8.932 9.055 9.139 9.198 9.241 9.272 9.294 9.309 9.319 9.325 9.328 9.329 9.329 9.329 9.329 9.329 9.329
7 7.648 7.943 8.127 8.252 8.342 8.409 8.460 8.500 8.530 8.555 8.574 8.589 8.600 8.609 8.616 8.621 8.624 8.626 8.627
8 7.130 7.407 7.584 7.708 7.799 7.869 7.924 7.968 8.004 8.033 8.057 8.078 8.094 8.108 8.119 8.129 8.137 8.143 8.149
9 6.762 7.024 7.195 7.316 7.407 7.478 7.535 7.582 7.619 7.652 7.679 7.702 7.722 7.739 7.753 7.766 7.777 7.786 7.794
10 6.487 6.738 6.902 7.021 7.111 7.182 7.240 7.287 7.327 7.361 7.390 7.415 7.437 7.456 7.472 7.487 7.500 7.511 7.522
11 6.275 6.516 6.676 6.791 6.880 6.950 7.008 7.056 7.097 7.132 7.162 7.188 7.211 7.231 7.250 7.266 7.280 7.293 7.304
12 6.106 6.340 6.494 6.607 6.695 6.765 6.822 6.870 6.911 6.947 6.978 7.005 7.029 7.050 7.069 7.086 7.102 7.116 7.128
13 5.970 6.195 6.346 6.457 6.543 6.612 6.670 6.718 6.759 6.795 6.826 6.854 6.878 6.900 6.920 6.937 6.954 6.968 6.982
14 5.856 6.075 6.223 6.332 6.416 6.485 6.542 6.590 6.631 6.667 6.699 6.727 6.752 6.774 6.794 6.812 6.829 6.844 6.858
15 5.760 5.974 6.119 6.225 6.309 6.377 6.433 6.481 6.522 6.558 6.590 6.619 6.644 6.666 6.687 6.706 6.723 6.739 6.753
16 5.678 5.888 6.030 6.135 6.217 6.284 6.340 6.388 6.429 6.465 6.497 6.525 6.551 6.574 6.595 6.614 6.631 6.647 6.661
17 5.608 5.813 5.953 6.056 6.138 6.204 6.260 6.307 6.348 6.384 6.416 6.444 6.470 6.493 6.514 6.533 6.551 6.567 6.582
18 5.546 5.748 5.886 5.988 6.068 6.134 6.189 6.236 6.277 6.313 6.345 6.373 6.399 6.422 6.443 6.462 6.480 6.497 6.512
19 5.492 5.691 5.826 5.927 6.007 6.072 6.127 6.174 6.214 6.250 6.281 6.310 6.336 6.359 6.380 6.400 6.418 6.434 6.450
20 5.444 5.640 5.774 5.873 5.952 6.017 6.071 6.117 6.158 6.193 6.225 6.254 6.279 6.303 6.324 6.344 6.362 6.379 6.394
24 5.297 5.484 5.612 5.708 5.784 5.846 5.899 5.945 5.984 6.020 6.051 6.079 6.105 6.129 6.150 6.170 6.188 6.205 6.221
30 5.156 5.335 5.457 5.549 5.622 5.682 5.734 5.778 5.817 5.851 5.882 5.910 5.935 5.958 5.980 6.000 6.018 6.036 6.051
40 5.022 5.191 5.308 5.396 5.466 5.524 5.574 5.617 5.654 5.688 5.718 5.745 5.770 5.793 5.814 5.834 5.852 5.869 5.885
60 4.894 5.055 5.166 5.249 5.317 5.372 5.420 5.461 5.498 5.530 5.559 5.586 5.610 5.632 5.653 5.672 5.690 5.707 5.723
120 4.771 4.924 5.029 5.109 5.173 5.226 5.271 5.311 5.346 5.377 5.405 5.431 5.454 5.476 5.496 5.515 5.532 5.549 5.565
∞4.654 4.798 4.898 4.974 5.034 5.085 5.128 5.166 5.199 5.229 5.256 5.280 5.303 5.324 5.343 5.361 5.378 5.394 5.409

12.1.4.4
Dunnett’s procedure
Let n = n0 = n1 = n2 = · · · = nk where treatment 0 is the control group and
let dα,ν1,ν2 be a critical value for Dunnett’s procedure (see page 289). Dun-
nett’s procedure for determining signiﬁcant diﬀerences between each treat-
ment and the control at the joint signiﬁcance level α is given in the following
table. For i = 1, 2, . . . , k:
Null
Alternative
Test
Rejection
hypothesis
hypotheses
statistic
regions
µ0 = µi
µ0 > µi
Di = yi. −y0.

2S2/n
Di ≥dα,k,k(n−1)
(1)
µ0 < µi
Di ≤−dα,k,k(n−1)
(2)
µ0 ̸= µi
|Di| ≥dα,k,k(n−1)
(3)
12.1.4.5
Tables for Dunnett’s procedure
This table contains critical values dα/2,k,ν and dα,k,n for simultaneous com-
parisons of each treatment group with a control group; α is the signiﬁcance
level, k is the number of treatment groups, and n is the degrees of freedom
of the independent estimate s2. These tables are from C. W. Dunnett, “A
Multiple Comparison Procedure for Comparing Several Treatments with a
Control”, JASA, Volume 50, 1955, pages 1096–1121. Reprinted with permis-
sion from the Journal of American Statistical Association. Copyright 1980 by
the American Statistical Association. All rights reserved.
Values of dα/2,k,n for two–sided comparisons (α = .05)
c
⃝2000 by Chapman & Hall/CRC
n k = 1
2
3
4
5
6
7
8
9
5
2.57
3.03 3.39 3.66 3.88 4.06 4.22 4.36 4.49
6
2.45
2.86 3.18 3.41 3.60 3.75 3.88 4.00 4.11
7
2.36
2.75 3.04 3.24 3.41 3.54 3.66 3.76 3.86
8
2.31
2.67 2.94 3.13 3.28 3.40 3.51 3.60 3.68
9
2.26
2.61 2.86 3.04 3.18 3.29 3.39 3.48 3.55
10
2.23
2.57 2.81 2.97 3.11 3.21 3.31 3.39 3.46
11
2.20
2.53 2.76 2.92 3.05 3.15 3.24 3.31 3.38
12
2.18
2.50 2.72 2.88 3.00 3.10 3.18 3.25 3.32
13
2.16
2.48 2.69 2.84 2.96 3.06 3.14 3.21 3.27
14
2.14
2.46 2.67 2.81 2.93 3.02 3.10 3.17 3.23
15
2.13
2.44 2.64 2.79 2.90 2.99 3.07 3.13 3.19
16
2.12
2.42 2.63 2.77 2.88 2.96 3.04 3.10 3.16
17
2.11
2.41 2.61 2.75 2.85 2.94 3.01 3.08 3.13
18
2.10
2.40 2.59 2.73 2.84 2.92 2.99 3.05 3.11
19
2.09
2.39 2.58 2.72 2.82 2.90 2.97 3.04 3.09
20
2.09
2.38 2.57 2.70 2.81 2.89 2.96 3.02 3.07
24
2.06
2.35 2.53 2.66 2.76 2.84 2.91 2.96 3.01
30
2.04
2.32 2.50 2.62 2.72 2.79 2.86 2.91 2.96
40
2.02
2.29 2.47 2.58 2.67 2.75 2.81 2.86 2.90
60
2.00
2.27 2.43 2.55 2.63 2.70 2.76 2.81 2.85
120
1.98
2.24 2.40 2.51 2.59 2.66 2.71 2.76 2.80
∞
1.96
2.21 2.37 2.47 2.55 2.62 2.67 2.71 2.75

Values of dα/2,k,n for two–sided comparisons (α = .01)
Values of dα/2,k,n for one–sided comparisons (α = .05)
c
⃝2000 by Chapman & Hall/CRC
n k = 1
2
3
4
5
6
7
8
9
5
2.02
2.44 2.68 2.85 2.98 3.08 3.16 3.24 3.30
6
1.94
2.34 2.56 2.71 2.83 2.92 3.00 3.07 3.12
7
1.89
2.27 2.48 2.62 2.73 2.82 2.89 2.95 3.01
8
1.86
2.22 2.42 2.55 2.66 2.74 2.81 2.87 2.92
9
1.83
2.18 2.37 2.50 2.60 2.68 2.75 2.81 2.86
10
1.81
2.15 2.34 2.47 2.56 2.64 2.70 2.76 2.81
11
1.80
2.13 2.31 2.44 2.53 2.60 2.67 2.72 2.77
12
1.78
2.11 2.29 2.41 2.50 2.58 2.64 2.69 2.74
13
1.77
2.09 2.27 2.39 2.48 2.55 2.61 2.66 2.71
14
1.76
2.08 2.25 2.37 2.46 2.53 2.59 2.64 2.69
15
1.75
2.07 2.24 2.36 2.44 2.51 2.57 2.62 2.67
16
1.75
2.06 2.23 2.34 2.43 2.50 2.56 2.61 2.65
17
1.74
2.05 2.22 2.33 2.42 2.49 2.54 2.59 2.64
18
1.73
2.04 2.21 2.32 2.41 2.48 2.53 2.58 2.62
19
1.73
2.03 2.20 2.31 2.40 2.47 2.52 2.57 2.61
20
1.72
2.03 2.19 2.30 2.39 2.46 2.51 2.56 2.60
24
1.71
2.01 2.17 2.28 2.36 2.43 2.48 2.53 2.57
30
1.70
1.99 2.15 2.25 2.33 2.40 2.45 2.50 2.54
40
1.68
1.97 2.13 2.23 2.31 2.37 2.42 2.47 2.51
60
1.67
1.95 2.10 2.21 2.28 2.35 2.39 2.44 2.48
120
1.66
1.93 2.08 2.18 2.26 2.32 2.37 2.41 2.45
∞
1.64
1.92 2.06 2.16 2.23 2.29 2.34 2.38 2.42
n k = 1
2
3
4
5
6
7
8
9
5
4.03
4.63 5.09 5.44 5.73 5.97 6.18 6.36 6.53
6
3.71
4.22 4.60 4.88 5.11 5.30 5.47 5.61 5.74
7
3.50
3.95 4.28 4.52 4.71 4.87 5.01 5.13 5.24
8
3.36
3.77 4.06 4.27 4.44 4.58 4.70 4.81 4.90
9
3.25
3.63 3.90 4.09 4.24 4.37 4.48 4.57 4.65
10
3.17
3.53 3.78 3.95 4.10 4.21 4.31 4.40 4.47
11
3.11
3.45 3.68 3.85 3.98 4.09 4.18 4.26 4.33
12
3.05
3.39 3.61 3.76 3.89 3.99 4.08 4.15 4.22
13
3.01
3.33 3.54 3.69 3.81 3.91 3.99 4.06 4.13
14
2.98
3.29 3.49 3.64 3.75 3.84 3.92 3.99 4.05
15
2.95
3.25 3.45 3.59 3.70 3.79 3.86 3.93 3.99
16
2.92
3.22 3.41 3.55 3.65 3.74 3.82 3.88 3.93
17
2.90
3.19 3.38 3.51 3.62 3.70 3.77 3.83 3.89
18
2.88
3.17 3.35 3.48 3.58 3.67 3.74 3.80 3.85
19
2.86
3.15 3.33 3.46 3.55 3.64 3.70 3.76 3.81
20
2.85
3.13 3.31 3.43 3.53 3.61 3.67 3.73 3.78
24
2.80
3.07 3.24 3.36 3.45 3.52 3.58 3.64 3.69
30
2.75
3.01 3.17 3.28 3.37 3.44 3.50 3.55 3.59
40
2.70
2.95 3.10 3.21 3.29 3.36 3.41 3.46 3.50
60
2.66
2.90 3.04 3.14 3.22 3.28 3.33 3.38 3.42
120
2.62
2.84 2.98 3.08 3.15 3.21 3.25 3.30 3.33
∞
2.58
2.79 2.92 3.01 3.08 3.14 3.18 3.22 3.25

Values of dα/2,k,n for one–sided comparisons (α = .01)
12.1.5
Contrasts
A contrast L is a linear combination of the means µi such that the coeﬃcients
ci sum to zero:
L =
k

i=1
ciµi
where
k

i=1
ci = 0.
(12.4)
Let .L =
k

i=1
ciyi., then
(1) .L has a normal distribution, E
"
.L
#
=
k

i=1
ciµi, Var
"
.L
#
= σ2
k

i=1
c2
i
ni
.
(2) A 100(1 −α)% conﬁdence interval for L has as endpoints
.l ± tα/2,N−k · s
F
G
G
H
k

i=1
c2
i /ni.
(12.5)
c
⃝2000 by Chapman & Hall/CRC
n k = 1
2
3
4
5
6
7
8
9
5
3.37
3.90 4.21 4.43 4.60 4.73 4.85 4.94 5.03
6
3.14
3.61 3.88 4.07 4.21 4.33 4.43 4.51 4.59
7
3.00
3.42 3.66 3.83 3.96 4.07 4.15 4.23 4.30
8
2.90
3.29 3.51 3.67 3.79 3.88 3.96 4.03 4.09
9
2.82
3.19 3.40 3.55 3.66 3.75 3.82 3.89 3.94
10
2.76
3.11 3.31 3.45 3.56 3.64 3.71 3.78 3.83
11
2.72
3.06 3.25 3.38 3.48 3.56 3.63 3.69 3.74
12
2.68
3.01 3.19 3.32 3.42 3.50 3.56 3.62 3.67
13
2.65
2.97 3.15 3.27 3.37 3.44 3.51 3.56 3.61
14
2.62
2.94 3.11 3.23 3.32 3.40 3.46 3.51 3.56
15
2.60
2.91 3.08 3.20 3.29 3.36 3.42 3.47 3.52
16
2.58
2.88 3.05 3.17 3.26 3.33 3.39 3.44 3.48
17
2.57
2.86 3.03 3.14 3.23 3.30 3.36 3.41 3.45
18
2.55
2.84 3.01 3.12 3.21 3.27 3.33 3.38 3.42
19
2.54
2.83 2.99 3.10 3.18 3.25 3.31 3.36 3.40
20
2.53
2.81 2.97 3.08 3.17 3.23 3.29 3.34 3.38
24
2.49
2.77 2.92 3.03 3.11 3.17 3.22 3.27 3.31
30
2.46
2.72 2.87 2.97 3.05 3.11 3.16 3.21 3.24
40
2.42
2.68 2.82 2.92 2.99 3.05 3.10 3.14 3.18
60
2.39
2.64 2.78 2.87 2.94 3.00 3.04 3.08 3.12
120
2.36
2.60 2.73 2.82 2.89 2.94 2.99 3.03 3.06
∞
2.33
2.56 2.68 2.77 2.84 2.89 2.93 2.97 3.00

(3) Single degree of freedom test: ith sample
H0:
k

i=1
ciµi = c
Ha:
k

i=1
ciµi > c,
k

i=1
ciµi < c,
k

i=1
ciµi ̸= c
TS: T =
L −c
s
+
k
i=1
c2
i /ni
or F = T 2 =
(L −c)2
s2
k
i=1
c2
i /ni
RR: T ≥tα,N−k,
T ≤−tα,N−k,
|T| ≥tα/2,N−k
or F ≥Fα,1,N−k
(4) The set of conﬁdence intervals with endpoints
.l ±

(k −1)Fα,k−1,N−k · s
F
G
G
H
k

i=1
c2
i /ni
(12.6)
is the collection of simultaneous 100(1 −α)% conﬁdence intervals for all
possible contrasts.
(5) Let n = ni, i = 1, 2, . . . , k, then the contrast sum of squares, SSL, is
given by
SSL =
 k
i=1
ciTi.
2
n
k
i=1
c2
i
.
(12.7)
(6) Two contrasts
L1 =
k
i=1
biµi
and
L2 =
k
i=1
ciµi
are orthogonal if
k

i=1
bici
ni
= 0.
12.1.6
Example
Example 12.66:
A telephone company recently surveyed the length of long distance
calls originating in four diﬀerent parts of the country. The length of each randomly
c
⃝2000 by Chapman & Hall/CRC

selected call (in minutes) is given in the table below.
North:
11.0
9.5
10.3
8.7
10.6
7.9
11.1
10.7
8.4
10.8
South:
13.6
12.2
12.5
17.5
9.7
16.4
13.0
15.6
12.1
10.2
14.6
16.2
13.6
17.2
13.1
13.3
11.9
12.0
Midwest:
15.0
14.2
11.9
14.5
12.7
17.1
10.6
13.8
16.8
11.4
15.7
11.0
13.4
10.9
11.1
11.0
10.3
10.4
West:
12.0
12.7
13.0
13.3
11.6
11.4
12.9
15.2
14.9
11.4
11.3
Is there any evidence to suggest the mean lengths of long distance calls from these four
parts of the country are diﬀerent? Use α = .05.
Solution:
(S1) There are k = 4 treatments and each sample is assumed to be independent and
randomly selected. Each population is assumed to be normally distributed with a
mean of µi (for i = 1, 2, 3, 4) and common variance σ2.
(S2) Summary statistics:
T1. = 99,
T2. = 244.7,
T3. = 231.8,
T4. = 139.7,
T.. = 715.2
(S3) Sum of squares:
SST =
4

i=1
ni

j=1
y2
ij −T 2
..
N = 9269.7 −715.22/57 = 295.82
SSA =
4

i=1
T 2
i.
ni −T 2
..
N =
992
10 + 244.72
18
+ 231.82
18
+ 139.72
11

−715.22/57
= 9065.92 −8973.88 = 92.04
SSE = SST −SSA = 295.82 −92.04 = 203.78
(S4) The analysis of variance table:
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Treatments
92.04
3
30.68
7.98
Error
203.78
53
3.84
Total
295.82
56
(S5) Hypothesis test for signiﬁcant regression (see section 12.1.3):
H0: µ1 = µ2 = µ3 = µ4 = 0
Ha: at least two of the means are unequal
TS: F = S2
A/S2
RR: F ≥F.05,3,53 = 2.78
Conclusion: The value of the test statistic lies in the rejection region. There is
evidence to suggest at least two of the mean lengths are diﬀerent.
c
⃝2000 by Chapman & Hall/CRC

12.2
TWO-WAY ANOVA
12.2.1
One observation per cell
12.2.1.1
Models and assumptions
Let Yij be the random observation in the ith row and the jth column for
i = 1, 2, . . . , r and j = 1, 2, . . . , c.
Fixed eﬀects experiment:
Model:
Yij = µ + αi + βj + ϵij
Assumptions:
ϵij
ind
∼N(0, σ2),
r

i=1
αi =
c

j=1
βj = 0
Random eﬀects experiment:
Model:
Yij = µ + Ai + Bj + ϵij
Assumptions:
ϵij
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
Bj
ind
∼N(0, σ2
β)
The ϵij’s, Ai’s, and Bj’s are independent.
Mixed Eﬀects Experiment:
Model:
Yij = µ + Ai + βj + ϵij
Assumptions:
ϵij
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
c

j=1
βj = 0
The ϵij’s and Ai’s are independent.
12.2.1.2
Sum of squares
Dots in the subscript of y and T indicate the mean and sum of yij, respectively,
over the appropriate subscript(s).
SST =
r

i=1
c

j=1
(yij −y..)2 =
r

i=1
c

j=1
y2
ij −T 2
..
rc
SSR = c
r

i=1
(yi. −y..)2 =
r
i=1
T 2
i.
c
−T 2
..
rc
SSC = r
c

j=1
(y.j −y..)2 =
c
j=1
T 2
.j
r
−T 2
..
rc
SSE =
r

i=1
c

j=1
(yij −yi. −y.j + y..)2 = SST −SSR −SSC
c
⃝2000 by Chapman & Hall/CRC

12.2.1.3
Mean squares and properties
MSR = SSR
r −1
= S2
R = mean square due to rows
MSC = SSC
c −1
= S2
C = mean square due to columns
MSE =
SSE
(r −1)(c −1) = S2 = mean square due to error
Mean
Expected value
Square
Fixed model
Random model
Mixed model
MSR
σ2 + c


r
i=1
α2
i
r −1


σ2 + cσ2
α
σ2 + cσ2
α
MSC
σ2 + r


c
j=1
β2
i
c −1


σ2 + rσ2
β
σ2 + r


c
j=1
β2
i
c −1


MSE
σ2
σ2
σ2
(1) F = S2
R/S2 has an F distribution with r −1 and (r −1)(c −1) degrees
of freedom.
(2) F = S2
C/S2 has an F distribution with c −1 and (r −1)(c −1) degrees
of freedom.
12.2.2
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Rows
SSR
r −1
MSR
MSR/MSE
Columns
SSC
c −1
MSC
MSC/MSE
Error
SSE
(r −1)(c −1)
MSE
Total
SST
rc −1
Hypothesis tests:
(1) Test for signiﬁcant row eﬀect
H0: There is no eﬀect due to rows
(Fixed eﬀects model: α1 = α2 = · · · = αr = 0)
(Random eﬀects model: σ2
α = 0)
(Mixed eﬀects model: σ2
α = 0)
c
⃝2000 by Chapman & Hall/CRC

Ha: There is an eﬀect due to rows
(Fixed eﬀects model: αi ̸= 0 for some i)
(Random eﬀects model: σ2
α ̸= 0)
(Mixed eﬀects model: σ2
α ̸= 0)
TS: F = S2
R/S2
RR: F ≥Fα,r−1,(r−1)(c−1)
(2) Test for signiﬁcant column eﬀect
H0: There is no eﬀect due to columns
(Fixed eﬀects model: β1 = β2 = · · · = βc = 0)
(Random eﬀects model: σ2
β = 0)
(Mixed eﬀects model: β1 = β2 = · · · = βc = 0)
Ha: There is an eﬀect due to columns
(Fixed eﬀects model: βj ̸= 0 for some j)
(Random eﬀects model: σ2
β ̸= 0)
(Mixed eﬀects model: βj ̸= 0 for some j)
TS: F = S2
C/S2
RR: F ≥Fα,c−1,(r−1)(c−1)
12.2.3
Nested classiﬁcations with equal samples
12.2.3.1
Models and assumptions
Let Yijk be the kth random observation for the ith level of factor A and the
jth level of factor B. There are n observations for each factor combination:
i = 1, 2, . . . , a, j = 1, 2, . . . , b, and k = 1, 2, . . . , n.
Fixed eﬀects experiment:
Model:
Yijk = µ + αi + βj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2),
a

i=1
αi = 0,
b

j=1
βj(i) = 0 for all i
Random eﬀects experiment:
Model:
Yijk = µ + Ai + Bj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
Bj(i)
ind
∼N(0, σ2
β)
The ϵk(ij)’s, Ai’s, and Bj(i)’s are independent.
c
⃝2000 by Chapman & Hall/CRC

Mixed eﬀects experiment (α):
Model:
Yijk = µ + αi + Bj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2),
a

i=1
αi = 0,
Bj(i)
ind
∼N(0, σ2
β)
The ϵk(ij)’s and Bj(i)’s are independent.
Mixed eﬀects experiment (β):
Model:
Yijk = µ + Ai + βj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2), Ai
ind
∼N(0, σ2
α),
b

j=1
βj(i) = 0 for all i
The ϵk(ij)’s and Ai’s are independent.
12.2.3.2
Sum of squares
Dots in the subscript of y and T indicate the mean and sum of yijk, respec-
tively, over the appropriate subscript(s).
SST =
a

i=1
b

j=1
n

k=1
(yijk −y...)2 =
a

i=1
b

j=1
n

k=1
y2
ijk −T 2
...
abn
SSA =
a

i=1
ni.(yi.. −y...)2 =
a
i=1
T 2
i..
ni.
−T 2
...
abn
SSB(A) =
a

i=1
b

j=1
nij(yij. −yi..)2 =
a

i=1
b

j=1
T 2
ij.
nij
−
a

i=1
T 2
i..
ni.
SSE =
a

i=1
b

j=1
n

k=1
(yijk −yij.)2 = SST −SSA −SSB(A)
12.2.3.3
Mean squares and properties
MSA = SSA
a −1
= S2
A
= mean square due to factor A
MSB(A) = SSB(A)
a(b −1)
= S2
B(A) = mean square due to factor B
MSE =
SSE
ab(n −1) = S2
= mean square due to error
c
⃝2000 by Chapman & Hall/CRC

Expected value
Mean
Fixed model
Random model
square
Mixed model (α)
Mixed model (β)
MSA
σ2 + bn
a
i=1
α2
i
a −1
σ2 + bnσ2
α + nσ2
β
σ2 + nσ2
β + bn


a
i=1
α2
i
a −1


σ2 + bnσ2
α
MSB(A)
σ2 + n
a
i=1
b
j=1
β2
j(i)
a(b −1)
σ2 + nσ2
β
σ2 + nσ2
β
σ2 + n
a
i=1
b
j=1
β2
j(i)
a(b −1)
MSE
σ2
σ2
σ2
σ2
(1) F = S2
A/S2 has an F distribution with a −1 and ab(n −1) degrees of
freedom.
(2) F = S2
B(A)/S2 has an F distribution with a(b−1) and ab(n−1) degrees
of freedom.
12.2.3.4
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Between main
groups
SSA
a −1
MSA
MSA
MSE
Subgroups within
main groups
SSB(A)
a(b −1)
MSB(A)
MSB(A)
MSE
Error
SSE
ab(n −1)
MSE
Total
SST
abn −1
Hypothesis tests:
(1) Test for signiﬁcant factor A main eﬀect
c
⃝2000 by Chapman & Hall/CRC

H0: There is no eﬀect due to factor A
(Fixed eﬀects model: α1 = α2 = · · · = αa = 0)
(Random eﬀects model: σ2
α = 0)
(Mixed eﬀects model (α): α1 = α2 = · · · = αa = 0)
(Mixed eﬀects model (β): σ2
α = 0)
Ha: There is an eﬀect due to factor A
(Fixed eﬀects model: αi ̸= 0 for some i)
(Random eﬀects model: σ2
α ̸= 0)
(Mixed eﬀects model (α): αi ̸= 0 for some i)
(Mixed eﬀects model (β): σ2
α ̸= 0)
TS: F = S2
A/S2
RR: F ≥Fα,a−1,ab(n−1)
(2) Test for signiﬁcant factor B speciﬁc eﬀect
H0: There is no eﬀect due to factor B
(Fixed eﬀects model: all βj(i) = 0)
(Random eﬀects model: σ2
β = 0)
(Mixed eﬀects model (α): σ2
β = 0)
(Mixed eﬀects model (β): all βj(i) = 0)
Ha: There is an eﬀect due to factor B
(Fixed eﬀects model: not all βj(i) = 0)
(Random eﬀects model: σ2
β ̸= 0)
(Mixed eﬀects model (α): σ2
β ̸= 0)
(Mixed eﬀects model (β): not all βj(i) = 0)
TS: F = S2
B(A)/S2
RR: F ≥Fα,a(b−1),ab(n−1)
12.2.4
Nested classiﬁcations with unequal samples
12.2.4.1
Models and assumptions
Let Yijk be the kth random observation for the ith level of factor A and the
jth level of factor B. There are nij observations for each factor combination:
i = 1, 2, . . . , a, j = 1, 2, . . . , mi, k = 1, 2, . . . , nij, and
a
i=1
mi

j=1
nij = n.
c
⃝2000 by Chapman & Hall/CRC

Fixed eﬀects experiment:
Model:
Yijk = µ + αi + βj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2),
a

i=1
αi = 0,
mi

j=1
βj(i) = 0 for all i
Random eﬀects experiment:
Model:
Yijk = µ + Ai + Bj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
Bj(i)
ind
∼N(0, σ2
β)
The ϵk(ij)’s, Ai’s, and Bj(i)’s are independent.
Mixed eﬀects experiment (α):
Model:
Yijk = µ + αi + Bj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2),
a

i=1
αi = 0,
Bj(i)
ind
∼N(0, σ2
β)
The ϵk(ij)’s and Bj(i)’s are independent.
Mixed eﬀects experiment (β):
Model:
Yijk = µ + Ai + βj(i) + ϵk(ij)
Assumptions: ϵk(ij)
ind
∼N(0, σ2), Ai
ind
∼N(0, σ2
α),
mi

j=1
βj(i) = 0 for all i
The ϵk(ij)’s and Ai’s are independent.
12.2.4.2
Sum of squares
Dots in the subscript of y and T indicate the mean and sum of yijk, respec-
tively, over the appropriate subscript(s).
SST =
a

i=1
mi

j=1
nij

k=1
(yijk −y...)2 =
a

i=1
mi

j=1
nij

k=1
y2
ijk −T 2
...
n
SSA =
a

i=1
ni.(yi.. −y...)2 =
a
i=1
T 2
i..
ni.
−T 2
...
n
SSB(A) =
a

i=1
mi

j=1
nij(yij. −yi..)2 =
a

i=1
mi

j=1
T 2
ij.
nij
−
a

i=1
T 2
i..
ni.
SSE =
a

i=1
mi

j=1
nij

k=1
(yijk −yij.)2 = SST −SSA −SSB(A)
c
⃝2000 by Chapman & Hall/CRC

12.2.4.3
Mean squares and properties
MSA = SSA
a −1
= S2
A
= mean square due to factor A
MSB(A) = SSB(A)
a
i=1
mi
= S2
B(A) = mean square due to factor B
MSE =
SSE
n −
a
i=1
mi
= S2
= mean square due to error
Expected value
Mean
Fixed model
Random model
square
Mixed model (α)
Mixed model (β)
MSA
σ2 +
a
i=1
miα2
i
a −1
σ2 + c1σ2
α + c2σ2
β
σ2 + c2σ2
β +
a
i=1
miα2
i
a −1
σ2 + c1σ2
α
MSB(A)
σ2 +
a
i=1
mi

j=1
nijβ2
j(i)
a
i=1
mi −a
σ2 + c3σ2
β
σ2 + c3σ2
β
σ2 +
a
i=1
mi

j=1
nijβ2
j(i)
a
i=1
mi −a
MSE
σ2
σ2
σ2
σ2
where
c1 =
a
i=1
mi

j=1
n2
ij
mi
−
a

i=1
mi

j=1
n2
ij
n
a −1
,
c2 = n −
a

i=1
m2
i
n
a −1
,
c3 =
n −
a
i=1
mi

j=1
n2
ij
mi
a
i=1
mi −k
.
(1) F = S2
A/S2 has an F distribution with a −1 and n −
a
i=1
mi degrees of
freedom.
c
⃝2000 by Chapman & Hall/CRC

(2) F = S2
B(A)/S2 has an F distribution with
a
i=1
mi −a and n −
a
i=1
mi
degrees of freedom.
12.2.4.4
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Between main
groups
SSA
a −1
MSA
MSA
MSE
Subgroups within
main groups
SSB(A)
a
i=1
mi −a
MSB(A)
MSB(A)
MSE
Error
SSE
n −
a
i=1
mi
MSE
Total
SST
n −1
Hypothesis tests:
(1) Test for signiﬁcant factor A main eﬀect
H0: There is no eﬀect due to factor A
(Fixed eﬀects model: α1 = α2 = · · · = αa = 0)
(Random eﬀects model: σ2
α = 0)
(Mixed eﬀects model (α): α1 = α2 = · · · = αa = 0)
(Mixed eﬀects model (β): σ2
α = 0)
Ha: There is an eﬀect due to factor A
(Fixed eﬀects model: αi ̸= 0 for some i)
(Random eﬀects model: σ2
α ̸= 0)
(Mixed eﬀects model (α): αi ̸= 0 for some i)
(Mixed eﬀects model (β): σ2
α ̸= 0)
TS: F = S2
A/S2
RR: F ≥F
α,a−1,n−
a

i=1
mi
(2) Test for signiﬁcant factor B speciﬁc eﬀect
c
⃝2000 by Chapman & Hall/CRC

H0: There is no eﬀect due to factor B
(Fixed eﬀects model: all βj(i) = 0)
(Random eﬀects model: σ2
β = 0)
(Mixed eﬀects model (α): σ2
β = 0)
(Mixed eﬀects model (β): all βj(i) = 0)
Ha: There is an eﬀect due to factor B
(Fixed eﬀects model: not all βj(i) = 0)
(Random eﬀects model: σ2
β ̸= 0)
(Mixed eﬀects model (α): σ2
β ̸= 0)
(Mixed eﬀects model (β): not all βj(i) = 0)
TS: F = S2
B(A)/S2
RR: F ≥F
α,
a

i=1
mi−a,n−
a

i=1
mi
12.2.5
Two-factor experiments
12.2.5.1
Models and assumptions
Let Yijk be the kth random observation for the ith level of factor A and the
jth level of factor B. There are n observations for each factor combination:
i = 1, 2, . . . , a, j = 1, 2, . . . , b, and k = 1, 2, . . . , n.
Fixed eﬀects experiment:
Model:
Yijk = µ + αi + βj + (αβ)ij + ϵijk
Assumptions: ϵijk
ind
∼N(0, σ2),
a

i=1
αi = 0,
b

j=1
βj = 0
a

i=1
(αβ)ij =
b

j=1
(αβ)ij = 0
Random eﬀects experiment:
Model:
Yijk = µ + Ai + Bj + (AB)ij + ϵijk
Assumptions: ϵijk
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
Bj
ind
∼N(0, σ2
β)
(AB)ij
ind
∼N(0, σ2
αβ)
The ϵijk’s, Ai’s, Bj’s, and (AB)ij’s are independent.
Mixed eﬀects experiment (α):
Model:
Yijk = µ + αi + Bj + (αB)ij + ϵijk
Assumptions: ϵijk
ind
∼N(0, σ2),
a

i=1
αi = 0,
a

i=1
(αB)ij = 0 for all j
Bj
ind
∼N(0, σ2
β),
(αB)ij
ind
∼N(0, a−1
a σ2
αβ)
The ϵijk’s, Bj’s, and (αB)ij’s are independent.
c
⃝2000 by Chapman & Hall/CRC

12.2.5.2
Sum of squares
Dots in the subscript of y and T indicate the mean and sum of yijk, respec-
tively, over the appropriate subscript(s).
SST =
a

i=1
b

j=1
n

k=1
(yijk −y...)2 =
a

i=1
b

j=1
n

k=1
y2
ijk −T 2
...
abn
SSA = bn
a

i=1
(yi.. −y...)2 =
a
i=1
T 2
i..
bn
−T 2
...
abn
SSB = an
b

j=1
(y.j. −y...)2 =
b
j=1
T 2
.j.
an
−T 2
...
abn
SS(AB) = n
a

i=1
b

j=1
(yij. −yi.. −y.j. + y...)2
=
a
i=1
b
j=1
T 2
ij.
n
−
a
i=1
T 2
i..
bn
−
b
j=1
T 2
.j.
an
+ T 2
...
abn
SSE =
a

i=1
b

j=1
n

k=1
(yijk −yij.)2 = SST −SSA −SSB −SS(AB)
12.2.5.3
Mean squares and properties
MSA = SSA
a −1
= S2
A
= mean square due to factor A
MSB = SSB
b −1
= S2
B
= mean square due to factor B
MS(AB) =
SS(AB)
(a −1)(b −1) = S2
AB = mean square due to interaction
MSE =
SSE
ab(n −1)
= S2
= mean square due to error
c
⃝2000 by Chapman & Hall/CRC

Mean square
Expected value
Fixed model
Random model
Mixed model (α)
MSA
σ2 + nb
a
i=1
α2
i
a −1
σ2 + nbσ2
α + σ2
αβ
σ2 + nb
a
i=1
α2
i
a −1 + nσ2
αβ
MSB
σ2 + na
b
j=1
β2
j
b −1
σ2 + naσ2
β + nσ2
αβ
σ2 + naσ2
β
MS(AB)
σ2 + n
a
i=1
b
j=1
(αβ)2
ij
(a −1)(b −1)
σ2 + nσ2
αβ
σ2 + nσ2
αβ
MSE
σ2
σ2
σ2
(1) F = S2
A/S2 has an F distribution with a −1 and ab(n −1) degrees of
freedom.
(2) F = S2
B/S2 has an F distribution with b −1 and ab(n −1) degrees of
freedom.
(3) F = S2
AB/S2 has an F distribution with (a −1)(b −1) and ab(n −1)
degrees of freedom.
12.2.5.4
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Factor A
SSA
a −1
MSA
MSA
MSE
Factor B
SSB
b −1
MSB
MSB
MSE
Interaction AB
SS(AB)
(a−1)(b−1)
MS(AB)
MS(AB)
MSE
Error
SSE
ab(n −1)
MSE
Total
SST
abn −1
Hypothesis tests:
(1) Test for signiﬁcant factor A main eﬀect
c
⃝2000 by Chapman & Hall/CRC

H0: There is no eﬀect due to factor A
(Fixed eﬀects model: α1 = α2 = · · · = αa = 0)
(Random eﬀects model: σ2
α = 0)
(Mixed eﬀects model (α): α1 = α2 = · · · = αa = 0)
Ha: There is an eﬀect due to factor A
(Fixed eﬀects model: αi ̸= 0 for some i)
(Random eﬀects model: σ2
α ̸= 0)
(Mixed eﬀects model (α): αi ̸= 0 for some i)
TS: F = S2
A/S2
RR: F ≥Fα,a−1,ab(n−1)
(2) Test for signiﬁcant factor B main eﬀect
H0: There is no eﬀect due to factor B
(Fixed eﬀects model: β1 = β2 = · · · = βb = 0)
(Random eﬀects model: σ2
β = 0)
(Mixed eﬀects model (α): σ2
β = 0)
Ha: There is an eﬀect due to factor B
(Fixed eﬀects model: βj ̸= 0 for some j)
(Random eﬀects model: σ2
β ̸= 0)
(Mixed eﬀects model (α): σ2
β ̸= 0)
TS: F = S2
B/S2
RR: F ≥Fα,b−1,ab(n−1)
(3) Test for signiﬁcant AB interaction eﬀect
H0: There is no eﬀect due to interaction
(Fixed eﬀects model: (αβ)11 = (αβ)12 = · · · = (αβ)ab = 0 )
(Random eﬀects model: σ2
αβ = 0
(Mixed eﬀects model (α): σ2
αβ = 0
Ha: There is an eﬀect due to interaction
(Fixed eﬀects model: (αβ)ij ̸= 0 for some ij
(Random eﬀects model: σ2
αβ ̸= 0
(Mixed eﬀects model (α): σ2
αβ ̸= 0
TS: F = S2
AB/S2
RR: F ≥Fα,(a−1)(b−1),ab(n−1)
12.2.6
Example
Example 12.67:
An electrical engineer believes the brand of battery and the style of
music played most often may have an eﬀect on the lifetime of batteries in a portable
CD player. Random samples were selected and the lifetime of each battery (in hours)
c
⃝2000 by Chapman & Hall/CRC

was recorded. The data are given in the table below.
Battery Brand
A
B
C
D
Easy listening 61.1 58.3 60.3 68.8 58.0 59.9 55.7 48.9
61.3 69.2 69.5 61.9 66.4 60.3 64.1 60.5
Style Country
62.2 55.9 64.0 53.4 64.5 59.2 61.8 59.0
63.1 67.0 64.3 64.8 66.0 56.4 54.1 50.5
Rock
64.2 57.0 58.7 61.1 62.8 63.0 58.7 57.3
59.1 48.7 62.3 70.8 65.1 64.1 60.9 63.0
Is there any evidence to suggest a diﬀerence in battery life due to brand or music style?
Solution:
(S1) A ﬁxed eﬀects experiment is assumed. There are i = 3 styles of music, j = 4
battery brands, and k = 4 observations for each factor combination.
(S2) The analysis of variance table:
Source of
Sum of Degrees of Mean
variation
squares freedom
square Computed F
Style
10.2
2
5.1
0.22
Brand
199.7
3
66.6
2.91
Interaction
125.8
6
21.0
0.92
Error
822.5
36
22.8
Total
1158.1
47
(S3) Test for signiﬁcant interaction eﬀect:
H0: There is no eﬀect due to interaction.
Ha: There is an eﬀect due to interaction.
TS: F = S2
AB/S2
RR: F ≥F.05,6,36 = 2.36
Conclusion: The value of the test statistic (F = 0.92) does not lie in the rejection
region. There is no evidence to suggest an interaction eﬀect. Tests for main eﬀects
may be analyzed as though there were no interaction.
(S4) Test for signiﬁcant eﬀect due to style:
H0: There is no eﬀect due to style.
Ha: There is an eﬀect due to style.
TS: F = S2
A/S2
RR: F ≥F.05,2,36 = 3.26
Conclusion: The value of the test statistic (F = 0.22) does not lie in the rejection
region. There is no evidence to suggest a diﬀerence in battery life due to style of
music.
(S5) Test for signiﬁcant eﬀect due to brand:
H0: There is no eﬀect due to brand.
Ha: There is an eﬀect due to brand.
c
⃝2000 by Chapman & Hall/CRC

TS: F = S2
B/S2
RR: F ≥F.05,3,36 = 2.86
Conclusion: The value of the test statistic (F = 2.91) lies in the rejection region.
There is some evidence to suggest a diﬀerence in battery life due to brand.
12.3
THREE-FACTOR EXPERIMENTS
12.3.1
Models and assumptions
Let Yijkl be the lth random observation for the ith level of factor A, the jth
level of factor B, and the kth level of factor C. There are n observations for
each factor combination: i = 1, 2, . . . , a, j = 1, 2, . . . , b, and k = 1, 2, . . . , c,
l = 1, 2, . . . , n.
Fixed eﬀects experiment:
Model:
Yijkl = µ + αi + βj + γk + (αβ)ij + (αγ)ik + (βγ)jk + (αβγ)ijk + ϵijkl
Assumptions:
ϵijkl
ind
∼N(0, σ2),
a

i=1
αi = 0,
b

j=1
βj = 0,
c

k=1
γk = 0,
a

i=1
(αβ)ij =
b

j=1
(αβ)ij =
c

i=1
(αγ)ik =
c

k=1
(αγ)ik =
b

j=1
(βγ)jk =
c

k=1
(βγ)jk =0,
a

i=1
(αβγ)ijk =
b

j=1
(αβγ)ijk =
c

k=1
(αβγ)ijk = 0
Random eﬀects experiment:
Model:
Yijkl =µ+Ai+Bj+Ck+(AB)ij+(AC)ik+(BC)jk+(ABC)ijk+ϵijkl
Assumptions:
ϵijkl
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
Bj
ind
∼N(0, σ2
β),
Ck
ind
∼N(0, σ2
γ)
(AB)ij
ind
∼N(0, σ2
αβ),
(AC)ik
ind
∼N(0, σ2
αγ),
(BC)jk
ind
∼N(0, σ2
βγ),
(ABC)ijk
ind
∼N(0, σ2
αβγ).
The ϵijkl’s are independent of the other random components.
c
⃝2000 by Chapman & Hall/CRC

Mixed eﬀects experiment (α):
Model:
Yijkl =µ+αi+Bj+Ck+(αB)ij+(αC)ik+(BC)jk+(αBC)ijk+ϵijkl
Assumptions:
ϵijk
ind
∼N(0, σ2),
a

i=1
αi =
a

i=1
(αB)ij =
a

i=1
(αC)ik =
a

i=1
(αBC)ijk = 0 ,
Bj
ind
∼N(0, σ2
β),
Ck
ind
∼N(0, σ2
γ),
(αB)ij
ind
∼N(0, σ2
αβ)
(αC)ik
ind
∼N(0, σ2
αγ),
(BC)jk
ind
∼N(0, σ2
βγ),
(αBC)ijk
ind
∼N(0, σ2
αβγ)
The ϵijkl’s are independent of the other random components.
Mixed eﬀects experiment (α, β):
Model:
Yijkl =µ+αi+βj+Ck+(αβ)ij+(αC)ik+(βC)jk+(αβC)ijk+ϵijkl
Assumptions:
ϵijkl
ind
∼N(0, σ2),
a

i=1
αi =
b

j=1
βj =
a

i=1
(αC)ik =
b

j=1
(βC)jk = 0,
a

i=1
(αβ)ij =
b

j=1
(αβ)ij =
a

i=1
(αβC)ijk =
b

j=1
(αβC)ijk = 0,
Ck
ind
∼N(0, σ2
γ),
(αC)ik
ind
∼N(0, σ2
αγ),
(βC)jk
ind
∼N(0, σ2
βγ),
(αβC)ijk
ind
∼N(0, σ2
αβγ)
The ϵijkl’s are independent of the other random components.
12.3.2
Sum of squares
Dots in the subscript of y and T indicate the mean and sum of yijkl, respec-
tively, over the appropriate subscript(s).
c
⃝2000 by Chapman & Hall/CRC

SST =
a

i=1
b

j=1
c

k=1
n

l=1
(yijkl −y....)2 =
a

i=1
b

j=1
c

k=1
n

l=1
y2
ijkl −T 2
....
abcn
SSA = bcn
a

i=1
(yi... −y....)2 =
a
i=1
T 2
i...
bn
−T 2
....
abcn
SSB = acn
b

j=1
(y.j.. −y....)2 =
b
j=1
T 2
.j..
acn
−T 2
....
abcn
SSC = abn
c

k=1
(y..k. −y....)2 =
c
k=1
T 2
..k.
abn
−T 2
....
abcn
SS(AB) = cn
a

i=1
b

j=1
(yij.. −yi... −y.j.. + y....)2
=
a
i=1
b
j=1
T 2
ij..
cn
−
a
i=1
T 2
i...
bcn
−
b
j=1
T 2
.j..
acn
+ T 2
....
abcn
SS(AC) = bn
a

i=1
c

k=1
(yi.k. −yi... −y..k. + y....)2
=
a
i=1
c
k=1
T 2
i.k.
bn
−
a
i=1
T 2
i...
bcn
−
c
k=1
T 2
..k.
abn
+ T 2
....
abcn
SS(BC) = an
b

j=1
c

k=1
(y.jk. −y.j.. −y..k. + y....)2
=
b
j=1
c
k=1
T 2
.jk.
an
−
b
j=1
T 2
.j..
acn
−
c
k=1
T 2
..k.
abn
+ T 2
....
abcn
SS(ABC) =
a

i=1
b

j=1
c

k=1
(yijk.−yij..−yi.k.−y.jk.+yi...+y.j..+y..k.−y....)2
=
a
i=1
b
j=1
c
k=1
T 2
ijk.
n
−
a
i=1
b
j=1
T 2
ij..
cn
−
a
i=1
b
j=1
T 2
i.k.
bn
−
b
j=1
c
k=1
T 2
.jk.
an
+
a
i=1
T 2
i...
bcn
+
b
j=1
T 2
.j..
acn
+
c
k=1
T 2
..k.
abn
+ T 2
....
abcn
c
⃝2000 by Chapman & Hall/CRC

SSE =
a

i=1
b

j=1
c

k=1
n

l=1
(yijkl −yijk.)2
= SST −SSA −SSB −SSC −SS(AB) −SS(AC) −SS(BC)
−SS(ABC)
12.3.3
Mean squares and properties
MSA = SSA
a −1 = S2
A = mean square due to factor A
MSB = SSB
b −1 = S2
B = mean square due to factor B
MSC = SSC
c −1 = S2
C = mean square due to factor C
MS(AB) =
SS(AB)
(a −1)(b −1)
= S2
AB = mean square due to AB interaction
MS(AC) =
SS(AC)
(a −1)(c −1)
= S2
AC = mean square due to AC interaction
MS(BC) =
SS(BC)
(b −1)(c −1)
= S2
BC = mean square due to BC interaction
MS(ABC) =
SS(ABC)
(a −1)(b −1)(c −1)
= S2
ABC = mean square due to ABC interaction
MSE =
SSE
abc(n −1) = S2 = mean square due to error
c
⃝2000 by Chapman & Hall/CRC

Mean square
Expected value
Fixed model
Random model
MSA
σ2 + bcn
a
i=1
α2
i
a −1
σ2 + bcnσ2
α + cnσ2
αβ + bnσ2
αγ + nσ2
αβγ
MSB
σ2 + acn
b
j=1
β2
j
b −1
σ2 + acnσ2
β + cnσ2
αβ + anσ2
βγ + nσ2
αβγ
MSC
σ2 + abn
c
k=1
γ2
k
c −1
σ2 + abnσ2
γ + bnσ2
αγ + anσ2
βγ + nσ2
αβγ
MS(AB)
σ2 + cn
a
i=1
b
j=1
(αβ)2
ij
(a −1)(b −1)
σ2 + cnσ2
αβ + nσ2
αβγ
MS(AC)
σ2 + bn
a
i=1
c
k=1
(αγ)2
ik
(a −1)(c −1)
σ2 + bnσ2
αγ + nσ2
αβγ
MS(BC)
σ2 + an
b
j=1
c
k=1
(βγ)2
jk
(b −1)(c −1)
σ2 + anσ2
βγ + nσ2
αβγ
MS(ABC)
σ2 + n
a
i=1
b
j=1
c
k=1
(αβγ)2
ijk
(a −1)(b −1)(c −1)
σ2 + nσ2
αβγ
MSE
σ2
σ2
c
⃝2000 by Chapman & Hall/CRC

Mean square
Expected value
Mixed model (α)
Mixed model (α, β)
MSA
σ2 + bcn
a
i=1
α2
i
a −1 + cnσ2
αβ
σ2 + bcn
a
i=1
α2
i
a −1 + bnσ2
αγ
+ bnσ2
αγ + nσ2
αβγ
MSB
σ2 + acnσ2
β + anσ2
βγ
σ2 + acn
b
j=1
β2
j
b −1 + anσ2
βγ
MSC
σ2 + abnσ2
γ + anσ2
βγ
σ2 + abnσ2
γ
MS(AB)
σ2 + cnσ2
αβ + nσ2
αβγ
σ2 + cn
a
i=1
b
j=1
(αβ)2
ij
(a −1)(b −1) + nσ2
αβγ
MS(AC)
σ2 + bnσ2
αγ + nσ2
αβγ
σ2 + bnσ2
αγ
MS(BC)
σ2 + anσ2
βγ
σ2 + anσ2
βγ
MS(ABC)
σ2 + nσ2
αβγ
σ2 + nσ2
αβγ
MSE
σ2
σ2
The following statistics have F distributions with the stated degrees of free-
dom.
Statistic
Numerator df
Denominator df
S2
A/S2
a −1
abc(n −1)
S2
B/S2
b −1
abc(n −1)
S2
C/S2
c −1
abc(n −1)
S2
AB/S2
(a −1)(b −1)
abc(n −1)
S2
AC/S2
(a −1)(c −1)
abc(n −1)
S2
BC/S2
(b −1)(c −1)
abc(n −1)
S2
ABC/S2 (a −1)(b −1)(c −1) abc(n −1)
c
⃝2000 by Chapman & Hall/CRC

12.3.4
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Factor A
SSA
a −1
MSA
MSA
MSE
Factor B
SSB
b −1
MSB
MSB
MSE
Factor C
SSC
c −1
MSC
MSC
MSE
A × B
SS(AB)
(a−1)(b−1)
MS(AB)
MS(AB)
MSE
A × C
SS(AC)
(a−1)(c−1)
MS(AC)
MS(AC)
MSE
B × C
SS(BC)
(b−1)(c−1)
MS(BC)
MS(BC)
MSE
A × B × C
SS(ABC)
(a−1)(b−1)(c−1)
MS(ABC)
MS(ABC)
MSE
Error
SSE
abc(n −1)
MSE
Total
SST
abcn −1
Hypothesis tests:
(1) Test for signiﬁcant factor A main eﬀect
H0: There is no eﬀect due to factor A
(Fixed eﬀects model: α1 = α2 = · · · = αa = 0)
(Random eﬀects model: σ2
α = 0)
(Mixed eﬀects model (α): α1 = α2 = · · · = αa = 0)
(Mixed eﬀects model (α, β): α1 = α2 = · · · = αa = 0)
Ha: There is an eﬀect due to factor A
(Fixed eﬀects model: αi ̸= 0 for some i)
(Random eﬀects model: σ2
α ̸= 0)
(Mixed eﬀects model (α): αi ̸= 0 for some i)
(Mixed eﬀects model (α, β): αi ̸= 0 for some i)
TS: F = S2
A/S2
RR: F ≥Fα,a−1,abc(n−1)
(2) Test for signiﬁcant factor B main eﬀect
c
⃝2000 by Chapman & Hall/CRC

H0: There is no eﬀect due to factor B
(Fixed eﬀects model: β1 = β2 = · · · = βb = 0)
(Random eﬀects model: σ2
β = 0)
(Mixed eﬀects model (α): σ2
β = 0)
(Mixed eﬀects model (α, β): β1 = β2 = · · · = βb = 0)
Ha: There is an eﬀect due to factor B
(Fixed eﬀects model: βj ̸= 0 for some j)
(Random eﬀects model: σ2
β ̸= 0)
(Mixed eﬀects model (α): σ2
β ̸= 0)
(Mixed eﬀects model (α, β): βj ̸= 0 for some j)
TS: F = S2
B/S2
RR: F ≥Fα,b−1,abc(n−1)
(3) Test for signiﬁcant factor C main eﬀect
H0: There is no eﬀect due to factor C
(Fixed eﬀects model: γ1 = γ2 = · · · = γc = 0)
(Random eﬀects model: σ2
γ = 0)
(Mixed eﬀects model (α): σ2
γ = 0)
(Mixed eﬀects model (α, β): σ2
γ = 0)
Ha: There is an eﬀect due to factor C
(Fixed eﬀects model: γk ̸= 0 for some k)
(Random eﬀects model: σ2
γ ̸= 0)
(Mixed eﬀects model (α): σ2
γ ̸= 0)
(Mixed eﬀects model (α, β): σ2
γ ̸= 0)
TS: F = S2
C/S2
RR: F ≥Fα,c−1,abc(n−1)
(4) Test for signiﬁcant AB interaction eﬀect
H0: There is no eﬀect due to interaction
(Fixed eﬀects model: (αβ)11 = · · · = (αβ)ab = 0)
(Random eﬀects model: σ2
αβ = 0)
(Mixed eﬀects model (α): σ2
αβ = 0)
(Mixed eﬀects model (α, β): (αβ)11 = · · · = (αβ)ab = 0)
Ha: There is an eﬀect due to AB interaction
(Fixed eﬀects model: (αβ)ij ̸= 0 for some ij)
(Random eﬀects model: σ2
αβ ̸= 0)
(Mixed eﬀects model (α): σ2
αβ ̸= 0)
(Mixed eﬀects model (α, β): (αβ)ij ̸= 0 for some ij)
TS: F = S2
AB/S2
RR: F ≥Fα,(a−1)(b−1),abc(n−1)
(5) Test for signiﬁcant AC interaction eﬀect
c
⃝2000 by Chapman & Hall/CRC

H0: There is no eﬀect due to interaction
(Fixed eﬀects model: (αγ)11 = · · · = (αγ)ac = 0)
(Random eﬀects model: σ2
αγ = 0)
(Mixed eﬀects model (α): σ2
αγ = 0)
(Mixed eﬀects model (α, β): σ2
αγ = 0)
Ha: There is an eﬀect due to AC interaction
(Fixed eﬀects model: (αγ)ik ̸= 0 for some ik)
(Random eﬀects model: σ2
αγ ̸= 0)
(Mixed eﬀects model (α): σ2
αγ ̸= 0)
(Mixed eﬀects model (α, β): σ2
αγ ̸= 0)
TS: F = S2
AC/S2
RR: F ≥Fα,(a−1)(c−1),abc(n−1)
(6) Test for signiﬁcant BC interaction eﬀect
H0: There is no eﬀect due to interaction
(Fixed eﬀects model: (βγ)11 = · · · = (βγ)bc = 0)
(Random eﬀects model: σ2
βγ = 0)
(Mixed eﬀects model (α): σ2
βγ = 0)
(Mixed eﬀects model (α, β): σ2
βγ = 0)
Ha: There is an eﬀect due to BC interaction
(Fixed eﬀects model: (βγ)jk ̸= 0 for some jk
(Random eﬀects model: σ2
βγ ̸= 0)
(Mixed eﬀects model (α): σ2
βγ ̸= 0)
(Mixed eﬀects model (α, β): σ2
βγ ̸= 0)
TS: F = S2
BC/S2
RR: F ≥Fα,(b−1)(c−1),abc(n−1)
(7) Test for signiﬁcant ABC interaction eﬀect
H0: There is no eﬀect due to interaction
(Fixed eﬀects model: (αβγ)111 = · · · = (αβγ)abc = 0)
(Random eﬀects model: σ2
αβγ = 0)
(Mixed eﬀects model (α): σ2
αβγ = 0)
(Mixed eﬀects model (α, β): σ2
αβγ = 0)
Ha: There is an eﬀect due to BC interaction
(Fixed eﬀects model: (αβγ)ijk ̸= 0 for some ijk)
(Random eﬀects model: σ2
αβγ ̸= 0)
(Mixed eﬀects model (α): σ2
αβγ ̸= 0)
(Mixed eﬀects model (α, β): σ2
αβγ ̸= 0)
TS: F = S2
ABC/S2
RR: F ≥Fα,(a−1)(b−1)(c−1),abc(n−1)
c
⃝2000 by Chapman & Hall/CRC

12.4
MANOVA
Manova means multiple anova, used if there are multiple dependent variables
to be analyzed simultaneously.
The use of repeated measurement is a subset of manova. Using multiple one-
way anovas to do this will raise the probability of a Type I error. Manova
controls the experiment-wide error rate.
(While it may seem that several
simultaneous anovas raise power, the Type I error rate increases also.)
Manova assumptions:
(a) Usual anova assumptions (normality, independence, HOV).
(b) Linearity or multicollinearity of dependent variables.
(c) Manova does not have the compound symmetry requirement that the
one-factor repeated measures anova model requires.
Manova advantages:
(a) Manova is a “gateway” test. If the multivariate F test is signiﬁcant,
then individual univariate analyses may be considered.
(b) Manova may be used with assorted dependent variables, or with re-
peated measures. This is an important feature of the model if the factors
cannot be collapsed because they are all essentially diﬀerent.
(c) Manova may detect combined diﬀerences not found by univariate anal-
yses if there is multicollinearity (a linear combination of the dependent
variables).
Manova limitations:
(a) Manova may be very sensitive to outliers, for small sample sizes.
(b) Manova assumes a linear relationship between dependent variables.
(c) Manova cannot give the interaction eﬀects between the main eﬀect and
a repeated factor.
12.5
FACTOR ANALYSIS
The purpose of factor analysis is to examine the covariance, or correlation,
relationships among all of the variables.
This technique is used to group
variables that tend to move together into an unobservable, random quantity
called a factor.
Suppose highly correlated observable variables are grouped together, i.e.,
grouped by correlations.
Variables within a group tend to move together
and have very little correlation with variables outside their group. It is pos-
sible that each group of variables may be represented by, and depend on, a
single, unobserved factor. Factor analysis attempts to discover this model
structure so that each factor has a large correlation with a few variables and
little correlation with the remaining variables.
c
⃝2000 by Chapman & Hall/CRC

Let X be a p×1 random vector with mean µ and variance-covariance matrix Σ.
Assume X is a linear function of a set of unobservable factors, F1, F2, . . . , Fm,
and p error terms, ϵ1, ϵ2, . . . , ϵp. The factor analysis model may be written as
X1 −µ1 = ℓ11F1 + ℓ12F2 + · · · + ℓ1mFm + ϵ1
X2 −µ2 = ℓ21F1 + ℓ22F2 + · · · + ℓ2mFm + ϵ2
...
...
Xp −µp = ℓp1F1 + ℓp2F2 + · · · + ℓpmFm + ϵp
(12.8)
In matrix notation, the factor analysis model may be written as
X −µ =
L
F
+
ϵ
(p × 1)
(p × m) (m × 1)
(p × 1)
(12.9)
where L is the matrix of factor loadings and ℓij is the loading of the ith
variable on the jth factor. There are additional model assumptions involving
the unobservable random vectors F and ϵ:
(1) F and ϵ are independent.
(2) E [F] = 0, Cov [F] = I.
(3) E [ϵ] = 0, Cov [ϵ] = Ψ, where Ψ is a diagonal matrix.
The orthogonal factor analysis model with m common factors (equation (12.8)
and these assumptions) implies the following covariance structure for the ran-
dom vector X:
(1) Cov [X] = LLT + Ψ or
Var [Xi] = ℓ2
i1 + · · · + ℓ2
im + Ψi
(12.10)
Cov [Xi, Xk] = ℓi1ℓk1 + · · · + ℓimℓkm
(12.11)
(2) Cov [X, F] = L, or Cov [Xi, Fj] = ℓij
The variance of the ith variable, σii, is the sum of two terms: the ith commu-
nality and the speciﬁc variance.
σii
	
Var[Xi]
= ℓ2
i1 + ℓ2
i2 + · · · + ℓ2
im


	
communality
+
Ψi
	
speciﬁc variance
(12.12)
Factor analysis is most useful when the number of unobserved factors, m, is
small relative to the number of observed random variables, p. The objective
of the factor analysis model is to provide a simpler explanation for the rela-
tionships in X rather than referring to the complete variance-covariance Σ. A
problem with this procedure is that most variance-covariance matrices cannot
be written as in equation (12.11) with m much less than p.
If m > 1, there are additional conditions necessary in order to obtain unique
estimates of L and Ψ. The estimate of the loading matrix L is determined
only up to an orthogonal (rotation) matrix. The rotation matrix is usually
constructed so that the model may be realistically interpreted.
c
⃝2000 by Chapman & Hall/CRC

There are two common procedures used to estimate the parameters ℓij and Ψi:
the method of principal components and the method of maximum likelihood.
Each of these solutions may be rotated in order to more appropriately interpret
the model. See, for example, R. A. Johnson and D. W. Wichern, Applied
Multivariate Statistical Analysis, Fourth Edition, Prentice-Hall, Inc., Upper
Saddle River, NJ, 1998.
12.6
LATIN SQUARE DESIGN
12.6.1
Models and assumptions
Let Yij(k) be the random observation corresponding to the ith row, the jth
column, and the kth treatment. The parentheses in the subscripts are used to
denote the one value k assumes for each ij combination: i, j, k = 1, 2, . . . , r.
It is assumed there are no interactions among these three factors.
Fixed eﬀects experiment:
Model:
Yij(k) = µ + αi + βj + γk + ϵij(k)
Assumptions: ϵij(k)
ind
∼N(0, σ2),
r

i=1
αi =
r

j=1
βj =
r

k=1
γk = 0
Random eﬀects experiment:
Model:
Yij(k) = µ + Ai + Bj + Ck + ϵij(k)
Assumptions:
ϵij(k)
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
Bj
ind
∼N(0, σ2
β),
Ck
ind
∼N(0, σ2
γ)
The ϵij(k)’s are independent of the other random components.
Mixed eﬀects experiment (γ):
Model:
Yij(k) = µ + Ai + Bj + γk + ϵij(k)
Assumptions:
ϵij(k)
ind
∼N(0, σ2),
Ai
ind
∼N(0, σ2
α),
Bj
ind
∼N(0, σ2
β),
r

k=1
γk = 0
The ϵij(k)’s are independent of the other random components.
Mixed eﬀects experiment (α, γ):
Model:
Yij(k) = µ + αi + Bj + γk + ϵij(k)
Assumptions:
ϵij(k)
ind
∼N(0, σ2),
Bj
ind
∼N(0, σ2
β),
r

i=1
αi =
r

k=1
γk = 0
The ϵij(k)’s and the Bj’s are independent.
c
⃝2000 by Chapman & Hall/CRC

12.6.2
Sum of squares
Dots in the subscript of y and T indicate the mean and sum of yij(k), respec-
tively, over the appropriate subscript(s).
SST =
r

i=1
r

j=1
(yij(k) −y...)2 =
r

i=1
r

j=1
y2
ij(k) −T 2
...
r2
SSR = r
r

i=1
(yi.. −y...)2 =
r
i=1
T 2
i..
r
−T 2
...
r2
SSC = r
r

j=1
(y.j. −y...)2 =
r
j=1
T 2
.j.
r
−T 2
...
r2
SSTr = r
r

k=1
(y..k −y...)2 =
r
k=1
T 2
..k
r
−T 2
...
r2
SSE =
r

i=1
r

j=1
(yij(k) −yi.. −y.j. −y..k + 2y...)2
= SST −SSR −SSC −SSTr
12.6.3
Mean squares and properties
MSR = SSR
r −1
= S2
R = mean square due to rows
MSC = SSC
r −1
= S2
C = mean square due to columns
MSTr = SSTr
r −1
= S2
Tr = mean square due to treatments
MSE =
SSE
(r −1)(r −2) = S2 = mean square due to error
c
⃝2000 by Chapman & Hall/CRC

Mean square
Expected value
Fixed
Random
Mixed model
Mixed model
model
model
(γ)
(α, γ)
MSR
σ2 + r
r
i=1
α2
i
r −1
σ2 + rσ2
α
σ2 + rσ2
α
σ2 + r
r
i=1
α2
i
r −1
MSC
σ2 + r
r
j=1
β2
j
r −1
σ2 + rσ2
β
σ2 + rσ2
β
σ2 + rσ2
β
MSTr
σ2 + r
r
k=1
γ2
k
r −1
σ2 + rσ2
γ
σ2 + r
r
k=1
γ2
k
r −1
σ2 + r
r
k=1
γ2
k
r −1
MSE
σ2
σ2
σ2
σ2
(1) F = S2
R/S2 has an F distribution with r −1 and (r −1)(r −2) degrees
of freedom.
(2) F = S2
C/S2 has an F distribution with r −1 and (r −1)(r −2) degrees
of freedom.
(3) F = S2
Tr/S2 has an F distribution with r −1 and (r −1)(r −2) degrees
of freedom.
12.6.4
Analysis of variance table
Source of
Sum of
Degrees of
Mean
variation
squares
freedom
square
Computed F
Rows
SSR
r −1
MSA
MSR
MSE
Columns
SSC
r −1
MSB
MSC
MSE
Treatments
SSTr
r −1
MSC
MSTr
MSE
Error
SSE
(r −1)(r −2)
MSE
Total
SST
r2 −1
Hypothesis tests:
(1) Test for signiﬁcant row eﬀect
c
⃝2000 by Chapman & Hall/CRC

H0: There is no eﬀect due to rows
(Fixed eﬀects model: α1 = α2 = · · · = αr = 0)
(Random eﬀects model: σ2
α = 0)
(Mixed eﬀects model (γ): σ2
α = 0)
(Mixed eﬀects model (α, γ): α1 = α2 = · · · = αr = 0)
Ha: There is an eﬀect due to rows
(Fixed eﬀects model: αi ̸= 0 for some i)
(Random eﬀects model: σ2
α ̸= 0)
(Mixed eﬀects model (γ): σ2
α ̸= 0)
(Mixed eﬀects model (α, γ): αi ̸= 0 for some i)
TS: F = S2
R/S2
RR: F ≥Fα,r−1,(r−1)(r−2)
(2) Test for signiﬁcant column eﬀect
H0: There is no eﬀect due to columns
(Fixed eﬀects model: β1 = β2 = · · · = βr = 0)
(Random eﬀects model: σ2
β = 0)
(Mixed eﬀects model (γ): σ2
β = 0)
(Mixed eﬀects model (α, γ): σ2
β = 0)
Ha: There is an eﬀect due to columns
(Fixed eﬀects model: βj ̸= 0 for some j)
(Random eﬀects model: σ2
β ̸= 0)
(Mixed eﬀects model (γ): σ2
β ̸= 0)
(Mixed eﬀects model (α, γ): σ2
β ̸= 0)
TS: F = S2
C/S2
RR: F ≥Fα,r−1,(r−1)(r−2)
(3) Test for signiﬁcant treatment eﬀect
H0: There is no eﬀect due to treatments
(Fixed eﬀects model: γ1 = γ2 = · · · = γr = 0)
(Random eﬀects model: σ2
γ = 0)
(Mixed eﬀects model (γ): γ1 = γ2 = · · · = γr = 0)
(Mixed eﬀects model (α, γ): γ1 = γ2 = · · · = γr = 0)
Ha: There is an eﬀect due to treatments
(Fixed eﬀects model: γk ̸= 0 for some k)
(Random eﬀects model: σ2
γ ̸= 0)
(Mixed eﬀects model (γ): γk ̸= 0 for some k)
(Mixed eﬀects model (α, γ): γk ̸= 0 for some k)
TS: F = S2
Tr/S2
RR: F ≥Fα,r−1,(r−1)(r−2)
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 13
Experimental Design
Contents
13.1
Latin squares
13.2
Graeco–Latin squares
13.3
Block designs
13.4
Factorial experimentation: 2 factors
13.5
2r Factorial experiments
13.6
Confounding in 2n factorial experiments
13.7
Tables for design of experiments
13.7.1
Plans of factorial experiments confounded
13.7.2
Plans of 2n factorials in fractional replication
13.7.3
Plans of incomplete block designs
13.7.4
Interactions in factorial designs
13.8
References
13.1
LATIN SQUARES
A Latin square of order n is an n×n array in which each cell contains a single
element from an n-set, such that each element occurs exactly once in each
row and exactly once in each column. A Latin square is in standard form if
in the ﬁrst row and column the elements occur in natural order. The number
of Latin squares in standard form are:
n
1
2
3
4
5
6
7
8
number
1
1
1
4
56
9,408
16,942,080
535,281,401,856
The unique Latin squares of order 1 and 2 are
A and A B
B A . There are 4
Latin squares of order 4.
3 × 3
A B C
B C A
C A B
4 × 4
a
b
c
d
A B C D
A B C D
A B C D
A B C D
B A D C
B C D A
B D A C
B A D C
C D B A
C D A B
C
A D B
C D A B
D C
A B
D A B C
D C B A
D C B A
c
⃝2000 by Chapman & Hall/CRC

5 × 5
A B C D E
B A E C D
C D A E B
D E B A C
E C D B A
6 × 6
A B C D E F
B F
D C
A E
C D E F
B A
D A F
E C B
E C
A B F
D
F
E B A D C
7 × 7
A B C D E F
G
B C D E F
G A
C D E F
G A B
D E F
G A B C
E F
G A B C D
F
G A B C D E
G A B C D E F
8 × 8
A B
C
D E
F
G H
B
C
D E
F
G H
A
C
D E
F
G H
A B
D E
F
G H
A B
C
E
F
G H
A B
C
D
F
G H
A B
C
D E
G H
A B
C
D E
F
H
A B
C
D E
F
G
9 × 9
A B
C
D E
F
G H
I
B
C
D E
F
G H
I
A
C
D E
F
G H
I
A B
D E
F
G H
I
A B
C
E
F
G H
I
A B
C
D
F
G H
I
A B
C
D E
G H
I
A B
C
D E
F
H
I
A B
C
D E
F
G
I
A B
C
D E
F
G H
10 × 10
A B
C
D E
F
G H
I
J
B
C
D E
F
G H
I
J
A
C
D E
F
G H
I
J
A B
D E
F
G H
I
J
A B
C
E
F
G H
I
J
A B
C
D
F
G H
I
J
A B
C
D E
G H
I
J
A B
C
D E
F
H
I
J
A B
C
D E
F
G
I
J
A B
C
D E
F
G H
J
A B
C
D E
F
G H
I
11 × 11
A
B
C
D E
F
G H
I
J
K
B
C
D E
F
G H
I
J
K
A
C
D E
F
G H
I
J
K
A
B
D E
F
G H
I
J
K
A
B
C
E
F
G H
I
J
K
A
B
C
D
F
G H
I
J
K
A
B
C
D E
G H
I
J
K
A
B
C
D E
F
H
I
J
K
A
B
C
D E
F
G
I
J
K
A
B
C
D E
F
G H
J
K
A
B
C
D E
F
G H
I
K
A
B
C
D E
F
G H
I
J
13.2
GRAECO–LATIN SQUARES
Two Latin squares K and L of order n are orthogonal if K(a, b) = K(c, d)
and L(a, b) = L(c, d) implies a = c and b = d. Equivalently, all of the n2
pairs (Ki,j, Li,j) are distinct. A pair of orthogonal Latin squares are called
Graeco–Latin squares.There is a pair of orthogonal Latin squares of order n
for all n > 1 except n = 2 or 6.
A set of Latin squares L1, . . . , Lm are mutually orthogonal if for every 1 ≤
i < j ≤m, the Latin squares Li and Lj are orthogonal. Three mutually
orthogonal Latin squares of size 4 are


A B C D
B A D C
C D A B
D C B A




1 3 4 2
2 4 3 1
3 1 2 4
4 2 1 3




a d b c
b c a d
c b d a
d a c b


(13.1)
c
⃝2000 by Chapman & Hall/CRC

3 × 3
A1 B3 C2
B2 C1 A3
C3 A2 B1
4 × 4
A1 B3
C4 D2
B2 A4 D3 C1
C3 D1 A2 B4
D4 C2
B1 A3
5 × 5
A1 B3
C5 D2 E4
B2
C4 D1 E3
A5
C3 D5 E2
A4 B1
D4 E1
A3 B5
C2
E5
A2 B4
C1 D3
There are no 6 × 6 Graeco–Latin squares
7 × 7
A1 B5
C2 D6 E3
F7
G4
B2
C6 D3 E7
F4
G1 A5
C3 D7 E4
F1
G5 A2 B6
D4 E1
F5
G2 A6 B3
C7
E5
F2
G6 A3 B7
C4 D1
F6
G3 A7 B4
C1 D5 E2
G7 A4 B1
C5 D2 E6
F3
8 × 8
A1 B5
C2 D3 E7
F4
G8 H6
B2 A8 G1
F7
H3 D6 C5
E4
C3
G4 A7
E1 D2 H5 B6
F8
D4
F3
E6
A5
C8
B1 H7 G2
E5 H1 D8 C4
A6 G3
F2
B7
F6
D7 H4 B8 G5 A2
E3
C1
G7
C6
B3 H2
F1
E8
A4 D5
H8 E2
F5
G6 B4
C7 D1 A3
9 × 9
A1 B3
C2 D7 E9
F8
G4 H6
I5
B2
C1
A3
E8
F7
D9 H5
I4
G6
C3
A2 B1
F8
D8 E7
I6
G5 H4
D4 E6
F5
G1 H3
I2
A7 B9
C8
E5
F4
D6 H2
I1
G3 B8
C7
A9
F6
D5 E4
I3
G2 H1 C9
A8 B7
G7 H9
I8
A4 B6 A5 D1 E3
F2
H8
I7
G9 B5
C4
A6
E2
F1
D3
I9
G8 H7 C6
A5 B4
F3
D2 E1
10 × 10
A1
B8
C9
D10
E2
F4
G6
H3
I5
J7
G7
H2
A8
B9
C10
E3
F5
I4
J6
D1
F6
G1
I3
H8
A9
B10
E4
J5
D7
C2
E5
F7
G2
J4
I8
H9
A10
D6
C1
B3
H10 E6
F1
G3
D5
J8
I9
C7
B2
A4
J9
I10
E7
F2
G4
C6
D8
B1
A3
H5
C8
D9 J10
E1
F3
G5
B7
A2
H4
I6
I2
J3
D4
C5
B6
A7
H1
G8
F9
E10
D3
C4
B5
A6
H7
I1
J2
F10
E8
G9
B4
A5
H6
I7
J1
D2
C3
E9
G10
F8
13.3
BLOCK DESIGNS
A balanced incomplete block design (BIBD) is a pair (V, B) where V is a v-set
and B is a collection of b subsets of V (each subset containing k elements)
such that each element of V is contained in exactly r blocks and any 2-subset
of V is contained in exactly λ blocks. The numbers v, b, r, k, λ are parameters
of the BIBD.
c
⃝2000 by Chapman & Hall/CRC

The parameters are necessarily related by vr = bk and r(k −1) = λ(v −1).
BIBDs are usually described by specifying (v, k, λ); this is a (v, k, λ)-design.
From these values r = λ(v−1)
k−1
and b = vr
k = vλ(v−1)
k(k−1) .
The complement of a design for (V, B) is a design for (V, B) where B =
(V \B | B ∈B).
The complement of a design with parameters (v, b, r, k, λ)
is a design with parameters (v, b, b −r, v −k, b −2r + λ). For this reason,
tables are usually given for v ≥2k (the designs for v < 2k are then obtained
by taking complements).
The example designs given below are from Colbourn and Dinitz. To conserve
space, designs are displayed in a k × b array in which each column contains
the elements forming a block.
(a) The unique (6,3,2) design is
0 0 0 0 0 1 1 1 2 2
1 1 2 3 4 2 3 4 3 3
2 3 4 5 5 5 4 5 4 5
Here there are v = 6 elements (numbered 0, 1, . . . , 5), k = 3 elements
per block, and each pair is in λ = 2 blocks. The other parameters are
r = 5 and b = 10.
As an illustration of how to interpret this design, note that the pair
(0,1) appears in columns 1 and 2, and in no other columns. Note that
the pair (0,2) appears in columns 1 and 3, and in no other columns, etc.
(b) One of the 4 nonisomorphic (7,3,2) designs is
0 0 0 0 0 0 1 1 1 1 2 2 2 2
1 1 3 3 5 5 3 3 4 4 3 3 4 4
2 2 4 4 6 6 5 5 6 6 6 6 5 5
(c) One of the 10 nonisomorphic (7,3,3) designs is
0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 2 2 2 2 2 2
1 1 1 3 3 3 5 5 5 3 3 3 4 4 4 3 3 3 4 4 4
2 2 2 4 4 4 6 6 6 5 5 5 6 6 6 6 6 6 5 5 5
(d) One of the 4 nonisomorphic (8,4,3) designs is
0 0 0 0 0 0 0 1 1 1 1 2 2 2
1 1 1 2 3 3 4 2 3 3 4 3 3 4
2 2 5 5 4 6 6 6 4 5 5 4 5 5
3 4 6 7 5 7 7 7 6 7 7 7 6 6
(e) The unique (9,3,1) design is
0 0 0 0 1 1 1 2 2 2 3 6
1 3 4 5 3 4 5 3 4 5 4 7
2 6 8 7 8 7 6 7 6 8 5 8
(f) One of the 36 nonisomorphic (9,3,2) designs is
0 0 0 0 0 0 0 0 1 1 1 1 1 1 2 2 2 2 2 2 3 3 4 4
1 1 3 3 5 5 7 7 3 3 4 4 6 6 3 3 4 4 5 5 6 6 5 5
2 2 4 4 6 6 8 8 5 5 7 7 8 8 8 8 6 6 7 7 7 7 8 8
c
⃝2000 by Chapman & Hall/CRC

(g) One of the 11 nonisomorphic (9,4,3) designs is
0 0 0 0 0 0 0 0 1 1 1 1 1 2 2 2 2 3
1 1 1 2 3 3 5 6 2 3 3 4 4 3 3 4 4 4
2 2 5 5 4 4 7 7 7 5 5 6 6 6 6 5 5 5
3 4 6 6 7 8 8 8 8 7 8 7 8 7 8 7 8 6
(h) One of the 3 nonisomorphic (10,4,2) designs is
0 0 0 0 0 0 1 1 1 1 2 2 2 3 3
1 1 2 3 5 6 2 3 4 5 3 4 5 4 4
2 4 4 7 7 8 7 6 7 6 5 8 6 5 6
3 5 6 8 9 9 8 9 9 8 9 9 7 8 7
(i) The unique (11,5,2) design is
0 0 0 0 0 1 1 1 2 2 3
1 1 2 3 4 1 3 6 3 5 4
2 4 5 6 7 4 5 7 4 6 5
3 5 8 8 9 8 9 8 6 7 7
7 6 9 a a a a 9 9 a 8
(j) The unique (13,4,1) design is
0 0 0 0 1 1 1 2 2 3 3 4 5
1 2 4 6 2 5 7 3 6 4 7 8 9
3 8 5 a 4 6 b 5 7 6 8 9 a
9 c 7 b a 8 c b 9 c a b c
13.4
FACTORIAL EXPERIMENTATION: 2 FACTORS
If two factors A and B are to be investigated at a levels and b levels, respec-
tively, and if there are ab experimental conditions (treatments) corresponding
to all possible combinations of the levels of the two factors, the resulting ex-
periment is called a complete a × b factorial experiment. Assume the entire
set of ab experimental conditions are repeated r times.
Let yijk be the observation in the kth replicate taken at the ith factor of A
and the jth factor of B. The model has the form
yijk = µ + αi + βj + (αβ)ij + ρk + ϵijk
(13.2)
for i = 1, 2, . . . , a, j = 1, 2, . . . , b, and k = 1, 2, . . . , r. Here
• µ is the grand mean
• αi is the eﬀect of the ith level of factor A
• βj is the eﬀect of the jth level of factor B
• (αβ)ij is the interaction eﬀect or joint eﬀect of the ith level of factor A
and the jth level of factor B
• ρk is the eﬀect of the kth replicate
• ϵijk are independent normally distributed random variables with mean
zero and variance σ2
The following conditions are also imposed
a

i=1
αi =
b

j=1
βj =
a

i=1
(αβ)ij =
b

j=1
(αβ)ij =
r

k=1
ρk = 0
(13.3)
c
⃝2000 by Chapman & Hall/CRC

In the usual way
SST

	

a

i=1
b

j=1
r

k=1
(yijk −y...)2 =
SS(Tr)

	

r
a

i=1
b

j=1

yij. −y...
2
+ ab
r

k=1
(y..k −y...)2
+
a

i=1
b

j=1
r

k=1

yijk −yij. −y..k + y...
2


	
SSE
(13.4)
SST is the total sum of squares, SS(Tr) is the treatment sum of squares, SSE
is the error sum of squares.
The distinguishing feature of a factorial experiment is that the treatment sum
of squares can be further subdivided into components corresponding to the
various factorial eﬀects. Here:
SS(Tr)

	

r
a

i=1
b

j=1
(yij. −y...)2 =
SS(A)

	

rb
a

i=1
(yi.. −y...)2 +
SS(B)

	

ra
b

j=1

y.j. −y...
2
+ r
a

i=1
b

j=1

yij. −yi.. −y.j. + y...
2


	
SS(AB)
(13.5)
SS(A) is the factor A sum of squares, SS(B) is the factor B sum of squares,
SS(AB) is the interaction sum of squares.
13.5
2r FACTORIAL EXPERIMENTS
A factorial experiment in which there are r factors, each at only two levels, is
a 2r factorial experiment. The two levels are often denoted as high and low,
or 0 and 1. A complete 2r factorial experiment includes observations for every
combination of factor and level, for a total of 2r observations. A 23 factorial
experiment has 8 treatment combinations, and the model is given by
Yijkl = µ + αi + βj + γk + (αβ)ij + (αγ)ik + (βγ)jk
+ (αβγ)ijk + ϵijkl
(13.6)
where i = 0, 1, j = 0, 1, k = 0, 1, and l = 1, 2, . . . , n. The assumptions are
ϵij
ind
∼N(0, σ2),
α1 = −α0,
β1 = −β0,
γ1 = −γ0,
(αβ)10 = (αβ)01 = −(αβ)11 = −(αβ)00, . . .
(13.7)
c
⃝2000 by Chapman & Hall/CRC

A 2n factorial experiment requires 2n experimental conditions. These con-
ditions are listed in a standard order using a special notation. Factor A at
the low level, or level 0, is denoted by “1”, at the high level, or level 1, by
“a”. The levels of factor B are represented by “1” and “b”, etc. In a 23
factorial experiment, the treatment combination that consists of high levels of
factors A and C, and a low level of factor B, is denoted by ac. The treatment
combination of all low levels is denoted simply by 1.
The treatment combinations are given by a binary expansion of the factor
levels. For n = 2 the standard order of combinations is {1, a, b, ab}. For n = 3
the standard form is:
Experimental
Level of factor
condition
A
B
C
1
0
0
0
a
1
0
0
b
0
1
0
ab
1
1
0
c
0
0
1
ac
1
0
1
bc
0
1
1
abc
1
1
1
The symbols for the ﬁrst four experimental conditions are like those for a
two–factor experiment, and the second four are obtained by multiplying each
of the ﬁrst four symbols by c.
Deﬁne (1), (a), (b), (ab), (c), . . . , to be the treatment totals corresponding to
the experimental conditions 1, a, b, ab, c, . . . . For example, in a 23 factorial
experiment,
(1) =
r

ℓ=1
y000ℓ
(a) =
r

ℓ=1
y100ℓ
. . .
(bc) =
r

ℓ=1
y011ℓ
(abc) =
r

ℓ=1
y111ℓ
(13.8)
Certain linear combinations of these totals result in the sum of squares for
the main eﬀects and interaction eﬀects. Deﬁne the eﬀect total for factor A:
[A] = −(1) + (a) −(b) + (ab) −(c) + (ac) −(bc) + (abc).
(13.9)
The sum of squares due to each factor may be obtained from its eﬀect total.
[1] is the total eﬀect.
The linear combination for each experimental condition eﬀect total may be
presented in a table of signs (a larger table is on page 338):
c
⃝2000 by Chapman & Hall/CRC

Experimental
Eﬀect total
condition
[1]
[A]
[B]
[AB]
[C]
[AC]
[BC]
[ABC]
1
+
−
−
+
−
+
+
−
a
+
+
−
−
−
−
+
+
b
+
−
+
−
−
+
−
+
ab
+
+
+
+
−
−
−
−
c
+
−
−
+
+
−
−
+
ac
+
+
−
−
+
+
−
−
bc
+
−
+
−
+
−
+
−
abc
+
+
+
+
+
+
+
+
To compute the sum of squares: SSA = [A]2/(8r), SSB = [B]2/(8r), SSC =
[C]2/(8r), SS(AB) = [AB]2/(8r), . . . .
Note: The expression for a single eﬀect total may be found by expanding
an algebraic expression. Consider the eﬀect total [AB] in a 23 factorial ex-
periment. Take the expression (a ± 1)(b ± 1)(c ± 1) and use a “−” if the
corresponding letter appears in the symbol for the main eﬀect, and use a “+”
if the letter does not appear. Expand the expression and add parentheses.
For [AB] the calculation is given by
(a −1)(b −1)(c + 1) = abc −ac −bc + c + ab −a −b + 1
= (1) −(a) −(b) + (ab) + (c) −(ac) −(bc) + (abc)
(13.10)
13.6
CONFOUNDING IN 2N FACTORIAL EXPERIMENTS
Sometimes it is impossible to run all the required experiments in a single
block. When experimental conditions are distributed over several blocks, one
or more of the eﬀects may become confounded (i.e., inseparable) with possible
block eﬀects, that is, between-block diﬀerences.
For example, in a 23 factorial experiment, let 1, b, c, and bc be in one block,
and let a, ab, ac, and abc be in another block. The “block eﬀect”, the diﬀerence
between the two block totals, is given by
[(a) + (ab) + (ac) + (abc)] −[(1) + (b) + (c) + (bc)]
This happens to be equal to [A]. Hence, the main eﬀect of A is confounded
with blocks.
Using instead a block of a, b, c, abc and a block of 1, ab, ac, bc would result
in ABC being confounded with blocks.
If the number of blocks is 2p then a total of 2p −1 eﬀects are confounded by
blocks.
13.7
TABLES FOR DESIGN OF EXPERIMENTS
The following tables present combinatorial patterns that may be used as ex-
perimental designs. The plans and plan numbers are from a more numerous
c
⃝2000 by Chapman & Hall/CRC

set of patterns in W. G. Cochran and G. M. Cox, Experimental Designs,
Second Edition, John Wiley & Sons, Inc, New York, 1957.
Reprinted by
permission of John Wiley & Sons, Inc.
Plans 6.1–6.6 are plans of factorial experiments confounded in randomized
incomplete blocks. Plans 6A.1–6A.6 are plans of 2n factorials in fractional
replication. Plans 13.1–13.5 are plans of incomplete block designs.
13.7.1
Plans of factorial experiments confounded in randomized
incomplete blocks
Plan 6.1: 23 factorial, 4 unit blocks
Rep. I, ABC confounded
abc
ab
a
ac
b
bc
c
(1)
Plan 6.2: 24 factorial, 8 unit blocks
Rep. I, ABCD confounded
a
(1)
b
ab
c
ac
d
bc
abc
ad
abd
bd
acd
cd
bcd
abcd
Plan 6.3: 26 factorial, 16 unit blocks
Rep. I, ABCD, ABEF, CDEF confounded
a
c
ab
ac
b
d
cd
ad
acd
abc
(1)
bc
bcd
abd
abcd
bd
ce
ae
ace
abe
de
be
ade
cde
abce
acde
bce
e
abde
bcde
bde
abcde
cf
af
acf
abf
df
bf
adf
cdf
abcf
acdf
bcf
f
abdf
bcdf
bdf
abcdf
aef
cef
abef
acef
bef
def
cdef
adef
acdef
abcef
ef
bcef
bcdef
abdef
abcdef
bdef
c
⃝2000 by Chapman & Hall/CRC

Plan 6.4: Balanced group of sets for 24 factorial, 4 unit blocks
Two–factor interactions are confounded in 1 replication and three–factor interactions
are confounded in 3 replications. The columns are the blocks.
Rep. I, AB, ACD,
BCD confounded
(1)
ab
a
b
abc
c
bc
ac
abd
d
bd
ad
cd
abcd
acd
bcd
Rep. II, AC, ABD, BCD
(1)
ac
a
c
abc
b
bc
ab
acd
d
cd
ad
bd
abcd
abd
bcd
Rep. III, AD, ABC, BCD
(1)
ad
a
d
abd
b
bd
ab
acd
c
cd
ac
bc
abcd
abc
bcd
Rep. IV, BC, ABD, ACD
(1)
bc
b
c
abc
a
ac
ab
bcd
d
cd
bd
ad
abcd
abd
acd
Rep. V, BD, ABC, ACD
(1)
bd
b
d
abd
a
ad
ab
bcd
c
cd
bc
ac
abcd
abc
acd
Rep. VI, CD, ABC, ABD
(1)
cd
c
d
acd
a
ad
ac
bcd
b
bd
bc
ab
abcd
abc
abd
Plan 6.5: Balanced group of sets for 25 factorial, 8 unit blocks
Three– and four–factor interactions are confounded in 1 replication.
Rep. I, ABC, ADE, BCDE confounded
(1)
ab
a
b
bc
ac
abc
c
abd
d
bd
ad
acd
bcd
be
abcd
abe
e
ce
ae
ace
bce
ade
abce
de
abde
abcde
bde
bcde
acde
cd
cde
Rep. II, ABD, BCE, ACDE
(1)
ab
a
b
ad
bd
d
abd
abc
c
bc
ac
bcd
acd
abcd
cd
abe
e
be
ae
bde
ade
abde
de
ce
abce
ace
bce
acde
bcde
cde
abcde
Rep. III, ACE, BCD, ABDE
(1)
ac
a
c
ae
ce
e
ace
abc
b
bc
ab
bce
abe
abce
be
acd
d
cd
ad
cde
ade
acde
de
bd
abcd
abd
bcd
abde
bcde
bde
abcde
Rep. IV, ACD, BDE, ABCE
(1)
ad
a
d
ac
cd
c
acd
abd
b
bd
ab
bcd
abc
abcd
bc
ade
e
de
ae
cde
ace
acde
ce
be
abde
abe
bde
abce
bcde
bce
abcde
Rep. V, ABE, CDE, ABCD
(1)
ae
a
e
ab
be
b
abe
ace
c
ce
ac
bce
abc
abce
bc
ade
d
de
ad
bde
abd
abde
bd
cd
acde
acd
cde
abcd
bcde
bcd
abcde
c
⃝2000 by Chapman & Hall/CRC

Plan 6.6: Balanced group of sets for 26 factorial, 8 unit blocks
All three– and four–factor interactions are confounded in 2 replications.
Rep. I, ABC, CDE, ADF, BEF, ABDE, BCDF, ACEF confounded
abc
a
b
(1)
bc
ac
c
ab
bd
cd
abcd
acd
abd
d
ad
bcd
ae
abce
ce
bce
e
abe
be
ace
cde
bde
ade
abde
acde
bcde
abcde
de
cf
bf
af
abf
acf
bcf
abcf
f
adf
abcdf
cdf
bcdf
df
abdf
bdf
acdf
bef
cef
abcef
acef
abef
ef
aef
bcef
abcdef
adef
bdef
def
bcdef
acdef
cdef
abdef
Rep. II, ABD, DEF, BCF, ACE, ABEF, ACDF, BCDE
abd
b
a
(1)
ad
bd
d
ab
cd
ac
bc
abc
bcd
acd
abcd
c
be
abde
de
ade
e
abe
ae
bde
ace
cde
abcde
bcde
abce
ce
bce
acde
af
df
abdf
bdf
abf
f
bf
adf
bcf
abcdf
cdf
acdf
cf
abcf
acf
bcdf
def
aef
bef
abef
bdef
adef
abdef
ef
abcdef
bcef
acef
cef
acdef
bcdef
cdef
abcef
Rep. III, ABE, BDF, ACD, CEF, ADEF, BCDE, ABCF
bc
a
ac
(1)
abc
ab
b
c
acd
bd
bcd
abd
cd
d
ad
abcd
abe
cd
d
ace
be
bce
abce
ae
de
abcde
abde
bcde
ade
acde
cde
bde
af
bcf
bf
abcf
f
cf
acf
abf
bdf
acdf
adf
cdf
abdf
abcdf
bcdf
df
cef
abef
abcef
bef
acef
aef
ef
bcef
abcdef
def
cdef
adef
bcdef
bdef
abdef
acdef
Rep. IV, ABF, CDF, ADE, BCE, ABCD, BDEF, ACEF
ac
a
b
(1)
c
abc
bc
ab
bd
bcd
acd
abcd
abd
d
ad
cd
bce
be
ae
abe
abce
ce
ace
e
ade
acde
bcde
cde
de
abde
bde
abcde
abf
abcf
cf
bcf
bf
af
f
acf
cdf
df
abdf
adf
acdf
bcdf
abcdf
bdf
ef
cef
abcef
acef
aef
bef
abef
bcef
abcdef
abdef
def
bdef
bcdef
acdef
cdef
adef
Rep. V, ACF, BCD, ADE, BEF, ABDF, CDEF, ABCE
ab
a
bc
(1)
b
ac
c
abc
bcd
cd
abd
acd
abcd
d
ad
bd
ce
bce
ae
abce
ace
be
abe
e
ade
abde
cde
bde
de
abcde
bcde
acde
acf
abcf
f
bcf
cf
abf
bf
af
df
bdf
acdf
abdf
adf
bcdf
abcdf
cdf
bef
ef
abcef
aef
abef
cef
acef
bcef
abcdef
acdef
bdef
cdef
bcdef
adef
def
abdef
c
⃝2000 by Chapman & Hall/CRC

Note:
(1) Replication VI, ABC, BDE, ADF, CEF, ACDE, BCDF, ABEF.
Interchange B and C in replication I.
(2) Replication VII, ABF, DEF, BCD, ACE, ABDE, ACDF, BCEF.
Interchange F and D in replication II.
(3) Replication VIII, ABE, BDF, CDE, ACF, ADEF, ABCD, BCEF.
Interchange A and E in replication III.
(4) Replication IX, ABD, CDF, AEF, BCE, ABCF, BDEF, ACDE.
Interchange F and D in replication IV.
(5) Replication X, AEF, BDE, ACD, BCF, ABDF, CDEF, ABCE.
Interchange E and C in replication V.
13.7.2
Plans of 2n factorials in fractional replication
Plan 6A.1: 24 factorial in 8 units (1/2 replicate)
Deﬁning contrast: ABCD
Estimable 2-factor interactions: AB = CD, AC = BD, AD = BC
(1)
ab
ac
ad
bc
bd
cd
abcd
Eﬀect
df
Main
4
2-factor
3
Total
7
Plan 6A.2: 25 factorial in 8 units (1/4 replicate)
Deﬁning contrast: ABE, CDE, ABCD
Main eﬀects have 2-factors as aliases. The only estimatable 2-factors are AC = BD
and AD = BC.
(1)
ab
cd
ace
bce
ade
bde
abcd
Eﬀect
df
Main
5
2-factor
2
Total
7
Plan 6A.3: 25 factorial in 16 units (1/2 replicate)
Deﬁning contrast: ABCDE
1. Blocks of 4 units
Estimatable 2-factors: All except CD, CE, DE (confounded with blocks)
Blocks
(1)
(2)
(3)
(4)
(1)
ac
ae
ad
ab
bc
be
bd
acde
de
cd
ce
bcde
abde
abcd
abce
CD, CE, DE confounded
Eﬀect
df
Block
3
Main
5
2-factor
7
Total
15
c
⃝2000 by Chapman & Hall/CRC

2. Blocks of 8 units
(a) Estimatable 2-factors: All except DE
(b) Combine blocks 1 and 2; and blocks 3
and 4. DE confounded.
Eﬀect
df
Block
1
Main
5
2-factor
9
Total
15
3. Blocks of 16 units
(a) Estimatable 2-factors: All
(b) Combine blocks 1–4
Eﬀect
df
Main
5
2-factor
10
Total
15
Plan 6A.4: 26 factorial in 8 units (1/8 replicate)
Deﬁning contrasts: ACE, ADF, BCF, BDE, ABCD, ABEF, CDEF
Main eﬀects have 2-factors as aliases. The only estimable 2-factor is the set AB =
CD = EF
(1)
acf
ade
bce
bdf
abcd
abef
cdef
Eﬀect
df
Main
6
2-factor (AB = CD = EF)
1
Total
7
Plan 6A.5: 26 factorial in 16 units (1/4 replicate)
Deﬁning contrasts: ABCE, ABDF, CDEF
1. Blocks of 4 units
Estimatable 2-factors: The alias sets AC = BD, AD = BF, AE = BC,
AF = BD, CD = EF, CF = DE
Blocks
(1)
(2)
(3)
(4)
(1)
acd
ab
acf
abce
aef
ce
ade
abdf
bcf
df
bcd
cdef
bde
abcdef
bef
AB, ACF, BCF confounded
Eﬀect
df
Block
3
Main
6
2-factor
6
Total
15
2. Blocks of 8 units
(a) Estimatable 2-factors: Same as in
blocks of 4 units, plus the set
AB = CE = DF.
(b) Combine blocks 1 and 2; and blocks 3
and 4. ACF confounded.
Eﬀect
df
Block
1
Main
6
2-factor
7
3-factor
1
Total
15
3. Blocks of 16 units
(a) Estimatable 2-factors: Same as in
blocks of 8 units
(b) Combine blocks 1–4
Eﬀect
df
Main
6
2-factor
7
3-factor
2
Total
15
c
⃝2000 by Chapman & Hall/CRC

Plan 6A.6: 26 factorial in 32 units (1/2 replicate)
Deﬁning contrast: ABCDEF
1. Blocks of 4 units
Estimatable 2-factors: All except AE, BF, and CD (confounded with blocks)
Blocks
(1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(1)
ab
ac
bc
ae
af
ad
bd
abef
ef
de
df
bf
be
ce
cf
acde
acdf
abdf
acef
cd
abcd
abcf
abce
bcdf
bcde
bcef
abde
abcdef
cdef
bdef
adef
AE, BF, CD, ABD, ACF, ADF confounded
Eﬀect
df
Block
7
Main
6
2-factor
12
Higher order
6
Total
31
2. Blocks of 8 units
(a) Estimatable 2-factors: All except CD
(b) Combine blocks 1 and 2; blocks 3 and
4; blocks 5 and 6; and blocks 7 and 8;
CD, ABC, ABD confounded.
Eﬀect
df
Block
3
Main
6
2-factor
14
Higher order
8
Total
31
3. Blocks of 16 units
(a) Estimatable 2-factors: All
(b) Estimatable 3-factors: ABC = DEF is
lost by confounding. The others are in
alias pairs, e.g., ABD = CEF.
(c) Combine blocks 1–4; and blocks 5–8.
ABC confounded.
Eﬀect
df
Block
1
Main
6
2-factor
15
3-factor
9
Total
31
4. Blocks of 32 units
(a) Estimatable 2-factors: All
(b) Estimatable 3-factors: These are
arranged in 10 alias pairs.
(c) Combine blocks 1–8.
Eﬀect
df
Main
6
2-factor
15
3-factor
10
Total
31
13.7.3
Plans of incomplete block designs
Plan 13.1: t = 7, k = 3, r = 3, b = 7, λ = 1 E = .78, Type II
Reps.
Block
I
II
III
(1)
7
1
3
(2)
1
2
4
(3)
2
3
5
(4)
3
4
6
Reps.
Block
I
II
III
(5)
4
5
7
(6)
5
6
1
(7)
6
7
2
c
⃝2000 by Chapman & Hall/CRC

Plan 13.2: t = 7, k = 4, r = 4, b = 7, λ = 2 E = .88, Type II
Reps.
Block
I
II
III
IV
(1)
3
5
6
7
(2)
4
6
7
1
(3)
5
7
1
2
(4)
6
1
2
3
Reps.
Block
I
II
III
IV
(5)
7
2
3
4
(6)
1
3
4
5
(7)
2
4
5
6
Plan 13.3: t = 11, k = 5, r = 5, b = 11, λ = 2 E = .88, Type I
Reps.
Block
I
II
III
IV
V
(1)
1
2
3
4
5
(2)
7
1
6
10
3
(3)
9
8
1
6
2
(4)
11
9
7
1
4
(5)
10
11
5
8
1
(6)
8
7
2
3
11
Reps.
Block
I
II
III
IV
V
(7)
2
6
4
11
10
(8)
6
3
11
5
9
(9)
3
4
10
9
8
(10)
5
10
9
2
7
(11)
4
5
8
7
6
Plan 13.4: t = 11, k = 6, r = 6, b = 11, λ = 3 E = .92, Type I
Reps.
Block
I
II
III
IV
V
VI
(1)
6
7
8
9
10
11
(2)
5
8
4
11
2
9
(3)
4
5
7
3
11
10
(4)
3
10
2
6
5
8
(5)
2
3
9
7
4
6
(6)
1
6
10
4
9
5
Reps.
Block
I
II
III
IV
V
VI
(7)
9
1
3
5
8
7
(8)
8
2
1
10
7
4
(9)
7
11
5
1
6
2
(10)
11
4
6
8
1
3
(11)
10
9
11
2
3
1
Plan 13.5: t = 13, k = 4, r = 4, b = 13, λ = 1 E = .81, Type I
Reps.
Block
I
II
III
IV
(1)
13
1
3
9
(2)
1
2
4
10
(3)
2
3
5
11
(4)
3
4
6
12
(5)
4
5
7
13
(6)
5
6
8
1
(7)
6
7
9
2
Reps.
Block
I
II
III
IV
(8)
7
8
10
3
(9)
8
9
11
4
(10)
9
10
12
5
(11)
10
11
13
6
(12)
11
12
1
7
(13)
12
13
2
8
c
⃝2000 by Chapman & Hall/CRC

13.7.4
Main eﬀect and interactions in factorial designs
Main eﬀect and interactions in 22, 23, 24, 25, and 26 factorial designs
(1)
+ −−+ −+ + −−+ + −+ −−+ −+ + −+ −−+ + −−+ −+ + −
a
+ + −−−−+ + −−+ + + + −−−−+ + + + −−+ + −−−−+ +
b
+ −+ −−+ −+ −+ −+ + −+ −−+ −+ + −+ −+ −+ −−+ −+
ab
+ + + + −−−−−−−−+ + + + −−−−+ + + + + + + + −−−−
c
+ −−+ + −−+ −+ + −−+ + −−+ + −−+ + −+ −−+ + −−+
ac
+ + −−+ + −−−−+ + −−+ + −−+ + −−+ + + + −−+ + −−
bc
+ −+ −+ −+ −−+ −+ −+ −+ −+ −+ −+ −+ + −+ −+ −+ −
abc
+ + + + + + + + −−−−−−−−−−−−−−−−+ + + + + + + +
d
+ −−+ −+ + −+ −−+ −+ + −−+ + −+ −−+ −+ + −+ −−+
ad
+ + −−−−+ + + + −−−−+ + −−+ + + + −−−−+ + + + −−
bd
+ −+ −−+ −+ + −+ −−+ −+ −+ −+ + −+ −−+ −+ + −+ −
abd
+ + + + −−−−+ + + + −−−−−−−−+ + + + −−−−+ + + +
cd
+ −−+ + −−+ + −−+ + −−+ −+ + −−+ + −−+ + −−+ + −
acd
+ + −−+ + −−+ + −−+ + −−−−+ + −−+ + −−+ + −−+ +
bcd
+ −+ −+ −+ −+ −+ −+ −+ −−+ −+ −+ −+ −+ −+ −+ −+
abcd + + + + + + + + + + + + + + + + −−−−−−−−−−−−−−−−
e
+ −−+ −+ + −−+ + −+ −−+ + −−+ −+ + −−+ + −+ −−+
ae
+ + −−−−+ + −−+ + + + −−+ + −−−−+ + −−+ + + + −−
be
+ −+ −−+ −+ −+ −+ + −+ −+ −+ −−+ −+ −+ −+ + −+ −
abe
+ + + + −−−−−−−−+ + + + + + + + −−−−−−−−+ + + +
ce
+ −−+ + −−+ −+ + −−+ + −+ −−+ + −−+ −+ + −−+ + −
ace
+ + −−+ + −−−−+ + −−+ + + + −−+ + −−−−+ + −−+ +
bce
+ −+ −+ −+ −−+ −+ −+ −+ + −+ −+ −+ −−+ −+ −+ −+
abce
+ + + + + + + + −−−−−−−−+ + + + + + + + −−−−−−−−
de
+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −
ade
+ + −−−−+ + + + −−−−+ + + + −−−−+ + + + −−−−+ +
bde
+ −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+
abde + + + + −−−−+ + + + −−−−+ + + + −−−−+ + + + −−−−
cde
+ −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+
acde
+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−
bcde
+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −
abcde + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +
c
⃝2000 by Chapman & Hall/CRC
(T)
A
AB
C
AC
BC
ABC
D
AD
BD
ABD
CD
ACD
BCD
ABCD
E
AE
BE
ABE
CE
ACE
BCE
ABCE
DE
ADE
BDE
ABDE
CDE
ACDE
BCDE
ABCDE

Main eﬀect and interactions in 22, 23, 24, 25, and 26 factorial designs
(1)
−+ + −+ −−+ + −−+ −+ + −+ −−+ −+ + −−+ + −+ −−+
a
−−+ + + + −−+ + −−−−+ + + + −−−−+ + −−+ + + + −−
b
−+ −+ + −+ −+ −+ −−+ −+ + −+ −−+ −+ −+ −+ + −+ −
ab
−−−−+ + + + + + + + −−−−+ + + + −−−−−−−−+ + + +
c
−+ + −−+ + −+ −−+ + −−+ + −−+ + −−+ −+ + −−+ + −
ac
−−+ + −−+ + + + −−+ + −−+ + −−+ + −−−−+ + −−+ +
bc
−+ −+ −+ −+ + −+ −+ −+ −+ −+ −+ −+ −−+ −+ −+ −+
abc
−−−−−−−−+ + + + + + + + + + + + + + + + −−−−−−−−
d
−+ + −+ −−+ −+ + −+ −−+ + −−+ −+ + −+ −−+ −+ + −
ad
−−+ + + + −−−−+ + + + −−+ + −−−−+ + + + −−−−+ +
bd
−+ −+ + −+ −−+ −+ + −+ −+ −+ −−+ −+ + −+ −−+ −+
abd
−−−−+ + + + −−−−+ + + + + + + + −−−−+ + + + −−−−
cd
−+ + −−+ + −−+ + −−+ + −+ −−+ + −−+ + −−+ + −−+
acd
−−+ + −−+ + −−+ + −−+ + + + −−+ + −−+ + −−+ + −−
bcd
−+ −+ −+ −+ −+ −+ −+ −+ + −+ −+ −+ −+ −+ −+ −+ −
abcd −−−−−−−−−−−−−−−−+ + + + + + + + + + + + + + + +
e
−+ + −+ −−+ + −−+ −+ + −−+ + −+ −−+ + −−+ −+ + −
ae
−−+ + + + −−+ + −−−−+ + −−+ + + + −−+ + −−−−+ +
be
−+ −+ + −+ −+ −+ −−+ −+ −+ −+ + −+ −+ −+ −−+ −+
abe
−−−−+ + + + + + + + −−−−−−−−+ + + + + + + + −−−−
ce
−+ + −−+ + −+ −−+ + −−+ −+ + −−+ + −+ −−+ + −−+
ace
−−+ + −−+ + + + −−+ + −−−−+ + −−+ + + + −−+ + −−
bce
−+ −+ −+ −+ + −+ −+ −+ −−+ −+ −+ −+ + −+ −+ −+ −
abce −−−−−−−−+ + + + + + + + −−−−−−−−+ + + + + + + +
de
−+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+
ade
−−+ + + + −−−−+ + + + −−−−+ + + + −−−−+ + + + −−
bde
−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −
abde −−−−+ + + + −−−−+ + + + −−−−+ + + + −−−−+ + + +
cde
−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −
acde
−−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ +
bcde −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+
abcde −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
c
⃝2000 by Chapman & Hall/CRC
F
AF
BF
ABF
CF
ACF
BCF
ABCF
DF
ADF
BDF
ABDF
CDF
ACDF
BCDF
ABCDF
EF
AEF
BEF
ABEF
CEF
ACEF
BCEF
ABCEF
DEF
ADEF
BDEF
ABDEF
CDEF
ACDEF
BCDEF
ABCDEF

Main eﬀect and interactions in 22, 23, 24, 25, and 26 factorial designs
f
+ −−+ −+ + −−+ + −+ −−+ −+ + −+ −−+ + −−+ −+ + −
af
+ + −−−−+ + −−+ + + + −−−−+ + + + −−+ + −−−−+ +
bf
+ −+ −−+ −+ −+ −+ + −+ −−+ −+ + −+ −+ −+ −−+ −+
abf
+ + + + −−−−−−−−+ + + + −−−−+ + + + + + + + −−−−
cf
+ −−+ + −−+ −+ + −−+ + −−+ + −−+ + −+ −−+ + −−+
acf
+ + −−+ + −−−−+ + −−+ + −−+ + −−+ + + + −−+ + −−
bcf
+ −+ −+ −+ −−+ −+ −+ −+ −+ −+ −+ −+ + −+ −+ −+ −
abcf
+ + + + + + + + −−−−−−−−−−−−−−−−+ + + + + + + +
df
+ −−+ −+ + −+ −−+ −+ + −−+ + −+ −−+ −+ + −+ −−+
adf
+ + −−−−+ + + + −−−−+ + −−+ + + + −−−−+ + + + −−
bdf
+ −+ −−+ −+ + −+ −−+ −+ −+ −+ + −+ −−+ −+ + −+ −
abdf
+ + + + −−−−+ + + + −−−−−−−−+ + + + −−−−+ + + +
cdf
+ −−+ + −−+ + −−+ + −−+ −+ + −−+ + −−+ + −−+ + −
acdf
+ + −−+ + −−+ + −−+ + −−−−+ + −−+ + −−+ + −−+ +
bcdf
+ −+ −+ −+ −+ −+ −+ −+ −−+ −+ −+ −+ −+ −+ −+ −+
abcdf + + + + + + + + + + + + + + + + −−−−−−−−−−−−−−−−
ef
+ −−+ −+ + −−+ + −+ −−+ + −−+ −+ + −−+ + −+ −−+
aef
+ + −−−−+ + −−+ + + + −−+ + −−−−+ + −−+ + + + −−
bef
+ −+ −−+ −+ −+ −+ + −+ −+ −+ −−+ −+ −+ −+ + −+ −
abef
+ + + + −−−−−−−−+ + + + + + + + −−−−−−−−+ + + +
cef
+ −−+ + −−+ −+ + −−+ + −+ −−+ + −−+ −+ + −−+ + −
acef
+ + −−+ + −−−−+ + −−+ + + + −−+ + −−−−+ + −−+ +
bcef
+ −+ −+ −+ −−+ −+ −+ −+ + −+ −+ −+ −−+ −+ −+ −+
abcef
+ + + + + + + + −−−−−−−−+ + + + + + + + −−−−−−−−
def
+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −
adef
+ + −−−−+ + + + −−−−+ + + + −−−−+ + + + −−−−+ +
bdef
+ −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+
abdef + + + + −−−−+ + + + −−−−+ + + + −−−−+ + + + −−−−
cdef
+ −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+
acdef
+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−
bcdef
+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −
abcdef + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +
c
⃝2000 by Chapman & Hall/CRC
(T)
A
B
AB
C
AC
BC
ABC
D
AD
BD
ABD
CD
ACD
BCD
ABCD
E
AE
BE
ABE
CE
ACE
BCE
ABCE
DE
ADE
BDE
ABDE
CDE
ACDE
BCDE
ABCDE

Main eﬀect and interactions in 22, 23, 24, 25, and 26 factorial designs
f
+ −−+ −+ + −−+ + −+ −−+ −+ + −+ −−+ + −−+ −+ + −
af
+ + −−−−+ + −−+ + + + −−−−+ + + + −−+ + −−−−+ +
bf
+ −+ −−+ −+ −+ −+ + −+ −−+ −+ + −+ −+ −+ −−+ −+
abf
+ + + + −−−−−−−−+ + + + −−−−+ + + + + + + + −−−−
cf
+ −−+ + −−+ −+ + −−+ + −−+ + −−+ + −+ −−+ + −−+
acf
+ + −−+ + −−−−+ + −−+ + −−+ + −−+ + + + −−+ + −−
bcf
+ −+ −+ −+ −−+ −+ −+ −+ −+ −+ −+ −+ + −+ −+ −+ −
abcf
+ + + + + + + + −−−−−−−−−−−−−−−−+ + + + + + + +
df
+ −−+ −+ + −+ −−+ −+ + −−+ + −+ −−+ −+ + −+ −−+
adf
+ + −−−−+ + + + −−−−+ + −−+ + + + −−−−+ + + + −−
bdf
+ −+ −−+ −+ + −+ −−+ −+ −+ −+ + −+ −−+ −+ + −+ −
abdf
+ + + + −−−−+ + + + −−−−−−−−+ + + + −−−−+ + + +
cdf
+ −−+ + −−+ + −−+ + −−+ −+ + −−+ + −−+ + −−+ + −
acdf
+ + −−+ + −−+ + −−+ + −−−−+ + −−+ + −−+ + −−+ +
bcdf
+ −+ −+ −+ −+ −+ −+ −+ −−+ −+ −+ −+ −+ −+ −+ −+
abcdf + + + + + + + + + + + + + + + + −−−−−−−−−−−−−−−−
ef
+ −−+ −+ + −−+ + −+ −−+ + −−+ −+ + −−+ + −+ −−+
aef
+ + −−−−+ + −−+ + + + −−+ + −−−−+ + −−+ + + + −−
bef
+ −+ −−+ −+ −+ −+ + −+ −+ −+ −−+ −+ −+ −+ + −+ −
abef
+ + + + −−−−−−−−+ + + + + + + + −−−−−−−−+ + + +
cef
+ −−+ + −−+ −+ + −−+ + −+ −−+ + −−+ −+ + −−+ + −
acef
+ + −−+ + −−−−+ + −−+ + + + −−+ + −−−−+ + −−+ +
bcef
+ −+ −+ −+ −−+ −+ −+ −+ + −+ −+ −+ −−+ −+ −+ −+
abcef + + + + + + + + −−−−−−−−+ + + + + + + + −−−−−−−−
def
+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −
adef
+ + −−−−+ + + + −−−−+ + + + −−−−+ + + + −−−−+ +
bdef
+ −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+ + −+ −−+ −+
abdef + + + + −−−−+ + + + −−−−+ + + + −−−−+ + + + −−−−
cdef
+ −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+
acdef
+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−+ + −−
bcdef + −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −+ −
abcdef + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + + +
13.8
REFERENCES
1. W. G. Cochran and G. M. Cox, Experimental Designs, Second Edition, John
Wiley & Sons, Inc, New York, 1957.
2. C. J. Colbourn and J. H. Dinitz, CRC Handbook of Combinatorial Designs,
CRC Press, Boca Raton, FL, 1996, pages 578–581.
c
⃝2000 by Chapman & Hall/CRC
F
AF
BF
ABF
CF
ACF
BCF
ABCF
DF
ADF
BDF
ABDF
CDF
ACDF
BCDF
ABCDF
EF
AEF
BEF
ABEF
CEF
ACEF
BCEF
ABCEF
DEF
ADEF
BDEF
ABDEF
CDEF
ACDEF
BCDEF
ABCDEF

CHAPTER 14
Nonparametric Statistics
Contents
14.1
Friedman test for randomized block design
14.2
Kendall’s rank correlation coeﬃcient
14.2.1
Kendall rank correlation coeﬃcient table
14.3
Kolmogorov–Smirnoﬀtests
14.3.1
One-sample Kolmogorov–Smirnoﬀtest
14.3.2
Two-sample Kolmogorov–Smirnoﬀtest
14.3.3
Tables for Kolmogorov–Smirnoﬀtests
14.4
Kruskal–Wallis test
14.4.1
Tables for Kruskal–Wallis test
14.5
The runs test
14.5.1
Tables for the runs test
14.6
The sign test
14.6.1
Critical values for the sign test
14.7
Spearman’s rank correlation coeﬃcient
14.7.1
Tables for Spearman’s rank correlation
14.8
Wilcoxon matched-pairs signed-ranks test
14.9
Wilcoxon rank–sum (Mann–Whitney) test
14.9.1
Tables for Wilcoxon U statistic
14.9.2
Critical values for Wilcoxon U statistic
14.10
Wilcoxon signed-rank test
Nonparametric, or distribution–free, statistical procedures generally as-
sume very little about the underlying population(s). The test statistic used
in each procedure is usually easy to compute and may involve qualitative
measurements or measurements made on an ordinal scale.
If both a parametric and nonparametric test are applicable, the nonparametric
test is less eﬃcient because it does not utilize all of the information in the
sample. A larger sample size is required in order for the nonparametric test
to have the same probability of a type II error.
c
⃝2000 by Chapman & Hall/CRC

14.1
FRIEDMAN TEST FOR RANDOMIZED BLOCK DESIGN
Assumptions: Let there be k independent random samples (treatments) from
continuous distributions and b blocks.
Hypothesis test:
H0: the k samples are from identical populations.
Ha: at least two of the populations diﬀer in location.
Rank each observation from 1 (smallest) to k (largest) within each
block.
Equal observations are assigned the mean rank for their
positions. Let Ri be the rank sum of the ith sample (treatment).
TS: Fr =

12
bk(k + 1)
k

i=1
R2
i

−3b(k + 1)
RR: Fr ≥χ2
α,k−1
14.2
KENDALL’S RANK CORRELATION COEFFICIENT
Given two sets containing ranked elements of the same size, consider each of
the
n
2

= n(n−1)
2
pairs of elements from within each set. Associate with each
pair (a) a score of +1 if the relative ranking of both samples is the same, or
(b) a score of −1 if the relative rankings are diﬀerent. Kendall’s score, St, is
deﬁned as the total of these
n
2

individual scores. St will have a maximum
value of
n(n−1)
2
if the two rankings are identical and a minimum value of
−n(n−1)
2
if the sets are ranked in exactly the opposite order. Kendall’s Tau is
deﬁned as
τ = St
I n(n −1)
2

(14.1)
and has the range −1 ≤τ ≤1.
The table on page 345 may be used to determine the exact probability asso-
ciated with an occurrence (one-tailed) of a speciﬁc value of St. In this case
the null hypothesis is the existence of an association between the two sets
as extreme as an observed St. The tabled value is the probability that St is
equaled or exceeded.
Example 14.68:
Consider the sets of ranked elements: a = {4, 12, 6, 10} and b =
{8, 7, 16, 2}. Kendall’s score is St = 0 for these sets since
c
⃝2000 by Chapman & Hall/CRC

For the (1, 2) term (with a1 < a2 and b1 > b2) score is −1
For the (1, 3) term (with a1 < a3 and b1 < b3) score is +1
For the (1, 4) term (with a1 < a4 and b1 > b4) score is −1
For the (2, 3) term (with a2 > a3 and b2 > b3) score is +1
For the (2, 4) term (with a2 > a4 and b2 > b4) score is +1
For the (3, 4) term (with a3 < a4 and b3 > b4) score is −1
Total is
0
Using the table on page 345 with n = 4 we ﬁnd Prob [St ≥0] = .625. That is, an St
value of 0 or larger would be expected 62.5% of the time.
14.2.1
Tables for Kendall rank correlation coeﬃcient
The following table may be used to determine the exact probability associated
with an occurrence (one-tailed) of a speciﬁc value of St. In this case the null
hypothesis is the existence of an association between the two sets as extreme
as an observed St. The tabled value is the probability that St is equaled or
exceeded.
Distribution of Kendall’s rank correlation coeﬃcient
in random rankings
St
n = 3
4
5
6
7
8
9
10
0
0.5000
0.6250
0.5917
0.5000
0.5000
0.5476
0.5403
0.5000
2
0.1667
0.3750
0.4083
0.3597
0.3863
0.4524
0.4597
0.4309
4
0.1667
0.2417
0.2347
0.2810
0.3598
0.3807
0.3637
6
0.0417
0.1167
0.1361
0.1907
0.2742
0.3061
0.3003
8
0.0417
0.0681
0.1194
0.1994
0.2384
0.2422
10
0.0083
0.0278
0.0681
0.1375
0.1792
0.1904
12
0.0083
0.0345
0.0894
0.1298
0.1456
14
0.0014
0.0151
0.0543
0.0901
0.1082
16
0.0054
0.0305
0.0597
0.0779
18
0.0014
0.0156
0.0376
0.0542
20
0.0002
0.0071
0.0223
0.0363
22
0.0028
0.0124
0.0233
24
0.0009
0.0063
0.0143
26
0.0002
0.0029
0.0083
28
0.0012
0.0046
30
0.0004
0.0023
32
0.0001
0.0011
34
0.0005
36
0.0002
38
0.0001
Note that each distribution is symmetric about St = 0: e.g., for n = 4,
Prob [St = 2] = Prob [St = −2] = 0.375. Note also that St can only assume
values with the same parity as n (for example, if n is even then Prob [St = odd] =
0); e.g., for n = 4, Prob [St = ±1] = Prob [St = ±3] = Prob [St = ±5] = 0.
c
⃝2000 by Chapman & Hall/CRC

14.3
KOLMOGOROV–SMIRNOFF TESTS
A one-sample Kolmogorov–Smirnoﬀtest is used to compare an observed cu-
mulative distribution function (computed from a sample) to a speciﬁc con-
tinuous distribution function.
This is a special test of goodness of ﬁt.
A
two-sample Kolmogorov–Smirnoﬀtest is used to compare two observed cumu-
lative distribution functions; the null hypothesis is that the two independent
samples come from identical continuous distributions.
14.3.1
One-sample Kolmogorov–Smirnoﬀtest
Suppose a sample of size n is drawn from a population with known cumulative
distribution function F(x).
The empirical distribution function, Fn(x), is
deﬁned by the sample and is a step function given by
Fn(x) = k
n
when
x(i) ≤x < x(i+1)
(14.2)
where k is the number of observations less than or equal to x and the {x(i)}
are the order statistics. If the sample is drawn from the hypothesized dis-
tribution, then the empirical distribution function, Fn(x), should be close to
F(x). Deﬁne the maximum absolute diﬀerence between the two distributions
to be
D = max
$$$$Fn(x) −F(x)
$$$$
(14.3)
For a two-tailed test the table on page 348 gives critical values for the sam-
pling distribution of D under the null hypothesis.
One should reject the
hypothetical distribution F(x) if the value D exceeds the tabulated value.
A corresponding one-tailed test is provided by the statistic
D+ = max

Fn(x) −F(x)

(14.4)
Example 14.69:
The values {.5, .75, .9, .1} are observed from data that are presumed
to be uniformly distributed on the interval (0, 1). Since the presumed distribution is
uniform, we have F(x) = x. Figure 14.1 shows Fn(x) and F(x). To determine D, only
the values of |Fn(x) −F(x)| for x at the endpoints (x = 0 and x = 1) and on each
side of the sample values (since Fn(x) has discontinuities at the sample values) need to
be considered. Constructing Table 14.1 results in D = .25. If α = .05 and n = 4 the
table on page 348 yields a critical value of c = .624. Since D < c, the null hypothesis
is not rejected.
14.3.2
Two-sample Kolmogorov–Smirnoﬀtest
Suppose two independent samples of sizes n1 and n2 are drawn from a pop-
ulation with cumulative distribution function F(x). For each sample j an
c
⃝2000 by Chapman & Hall/CRC

x
Fn(x)
F(x) = x
|Fn(x) −F(x)|
x = 0
0
0
|0 −0| = 0
x = .1−
0
.10
|0 −.10| = .10
x = .1+
.25
.10
|.25 −.10| = .15
x = .5−
.25
.50
|.25 −.50| = .25
x = .5+
.50
.50
|.50 −.50| = 0
x = .75−
.50
.75
|.50 −.75| = .25
x = .75+
.75
.75
|.75 −.75| = 0
x = .90−
.75
.90
|.75 −.90| = .15
x = .90+
.90
.90
|.90 −.90| = 0
x = 1
1
1
|1 −1| = 0
Table 14.1: Table for Kolmogorov–Smirnoﬀcomputation.
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
Figure 14.1: Comparison of the sample distribution function (dotted curve)
with the distribution function (solid line) for a uniform random variable on
the interval (0, 1).
empirical distribution function Fnj(x) is given by the step function,
Fnj(x) = k
n
when
xsample j
(i)
≤x < xsample j
(i+1)
(14.5)
where k is the number of observations less than or equal to x and the {xsample j
(i)
}
are the order statistics for the jth sample (for j = 1 or j = 2).
If the two samples have been drawn from the same population, or from pop-
ulations with the same distribution (the null hypothesis), then Fn1(x) should
be close to Fn2(x). Deﬁne the maximum absolute diﬀerence between the two
empirical distributions to be
D = max
$$$$Fn1(x) −Fn2(x)
$$$$
(14.6)
c
⃝2000 by Chapman & Hall/CRC

For a two-tailed test the table on page 350 gives critical values for the sampling
distribution of D under the null hypothesis. The null hypothesis is rejected if
the value of D exceeds the tabulated value.
A corresponding one-tailed test is provided by the statistic
D+ = max

Fn1(x) −Fn2(x)

(14.7)
14.3.3
Tables for Kolmogorov–Smirnoﬀtests
14.3.3.1
Critical values, one-sample Kolmogorov–Smirnoﬀtest
Critical values, one-sample Kolmogorov–Smirnov test
One-sided test
α = 0.10
0.05
0.025
0.01
0.005
Two-sided test
α = 0.20
0.10
0.05
0.02
0.01
n = 1
0.900
0.950
0.975
0.990
0.995
2
0.684
0.776
0.842
0.900
0.929
3
0.565
0.636
0.708
0.785
0.829
4
0.493
0.565
0.624
0.689
0.734
5
0.447
0.509
0.563
0.627
0.669
6
0.410
0.468
0.519
0.577
0.617
7
0.381
0.436
0.483
0.538
0.576
8
0.358
0.410
0.454
0.507
0.542
9
0.339
0.387
0.430
0.480
0.513
10
0.323
0.369
0.409
0.457
0.489
11
0.308
0.352
0.391
0.437
0.468
12
0.296
0.338
0.375
0.419
0.449
13
0.285
0.325
0.361
0.404
0.432
14
0.275
0.314
0.349
0.390
0.418
15
0.266
0.304
0.338
0.377
0.404
16
0.258
0.295
0.327
0.366
0.392
17
0.250
0.286
0.318
0.355
0.381
18
0.244
0.279
0.309
0.346
0.371
19
0.237
0.271
0.301
0.337
0.361
20
0.232
0.265
0.294
0.329
0.352
21
0.226
0.259
0.287
0.321
0.344
22
0.221
0.253
0.281
0.314
0.337
23
0.216
0.247
0.275
0.307
0.330
24
0.212
0.242
0.269
0.301
0.323
25
0.208
0.238
0.264
0.295
0.317
c
⃝2000 by Chapman & Hall/CRC

Critical values, one-sample Kolmogorov–Smirnov test
One-sided test
α = 0.10
0.05
0.025
0.01
0.005
Two-sided test
α = 0.20
0.10
0.05
0.02
0.01
26
0.204
0.233
0.259
0.290
0.311
27
0.200
0.229
0.254
0.284
0.305
28
0.197
0.225
0.250
0.279
0.300
29
0.193
0.221
0.246
0.275
0.295
30
0.190
0.218
0.242
0.270
0.290
31
0.187
0.214
0.238
0.266
0.285
32
0.184
0.211
0.234
0.262
0.281
33
0.182
0.208
0.231
0.258
0.277
34
0.179
0.205
0.227
0.254
0.273
35
0.177
0.202
0.224
0.251
0.269
36
0.174
0.199
0.221
0.247
0.265
37
0.172
0.196
0.218
0.244
0.262
38
0.170
0.194
0.215
0.241
0.258
39
0.168
0.191
0.213
0.238
0.255
40
0.165
0.189
0.210
0.235
0.252
Approximation
for n > 40:
1.07
√n
1.22
√n
1.36
√n
1.52
√n
1.63
√n
14.3.3.2
Critical values, two-sample Kolmogorov–Smirnoﬀtest
Given the null hypothesis that the two distributions are the same (H0: F1(x) =
F2(x)), compute D = max |Fn1(x) −Fn2(x)|.
(a) Reject H0 if D exceeds the value in the table on page 350.
(b) Where ∗appears in the table on page 350, do not reject H0 at the given
signiﬁcance level.
(c) For large values of n1 and n2, and various values of α, the approximate
critical value of D is given in the table below.
Level of signiﬁcance
Approximate critical value
α = 0.10
1.22

n1+n2
n1n2
α = 0.05
1.36

n1+n2
n1n2
α = 0.025
1.48

n1+n2
n1n2
α = 0.01
1.63

n1+n2
n1n2
α = 0.005
1.73

n1+n2
n1n2
α = 0.001
1.95

n1+n2
n1n2
c
⃝2000 by Chapman & Hall/CRC

The entries in the following table are expressed as rational numbers since all
critical values of D are an integer divided by n1n2. For example, if n1 = 6
and n2 = 5, then
.108225 = Prob

D ≥20
30

= Prob

D ≥21
30

= Prob

D ≥22
30

= Prob

D ≥23
30

.047619 = Prob

D ≥24
30

(least value of D for which α < 0.05)
.025974 = Prob

D ≥25
30

= Prob

D ≥26
30

= Prob

D ≥27
30

= Prob

D ≥28
30

= Prob

D ≥29
30

.004329 = Prob

D ≥30
30

(least value of D for which α < 0.01)
(14.8)
See P. J. Kim and R. I. Jennrich, Tables of the exact sampling distribution
of the two-sample Kolmogorov–Smirnov criterion, Dmn, m ≤n, pages 79–
170, in H. L. Harter and D. B. Brown (ed.), Selected Tables in Mathematical
Statistics, Volume 1, American Mathematical Society, Providence, RI, 1973.
Critical values for the Kolmogorov–Smirnov test of F1(x) = F2(x)
(upper value for α ≤.05, lower value for α ≤.01)
Sample
Sample size n1
size n2 3
4
5
6
7
8
9
10
11
12
1
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
2
∗
∗
∗
∗
∗
16/16 18/18
20/20
22/22
24/24
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
3
∗
∗
15/15 18/18 21/21 21/24 24/27
27/30
30/33
30/36
∗
∗
∗
∗
∗
24/24 27/27
30/30
33/33
36/35
4
16/16 20/20 20/24 24/28 28/32 28/36
30/40
33/44
36/48
∗
∗
24/24 28/28 32/32 32/36
36/40
40/44
44/48
5
∗
24/30 30/35 30/40 35/45
40/50
39/55
43/60
∗
30/30 35/35 35/40 40/45
45/50
45/55
50/60
6
30/36 30/42 34/48 39/54
40/60
43/66
48/72
36/36 36/42 40/48 45/54
48/60
54/66
60/72
7
42/49 40/56 42/63
46/70
48/77
53/84
42/49 48/56 49/63
53/70
59/77
60/84
8
48/64 46/72
48/80
53/88
60/96
56/64 55/72
60/80
64/88
68/96
9
54/81
53/90
59/99
63/108
63/81
70/90
70/99
75/108
10
70/100 60/110 66/120
80/100 77/110 80/120
11
77/121 72/132
88/121 86/132
12
96/144
84/144
c
⃝2000 by Chapman & Hall/CRC

14.4
KRUSKAL–WALLIS TEST
Assumptions: Suppose there are k > 2 independent random samples from
continuous distributions, let ni (for i = 1, 2, . . . , k) be the number of observa-
tions in each sample, and let n = n1 + n2 + · · · + nk.
Hypothesis test:
H0: the k samples are from identical populations.
Ha: at least two of the populations diﬀer.
Rank all n observations from 1 (smallest) to n (largest). Equal
observations are assigned the mean rank for their positions. Let
Rij be the rank assigned to the jth observation in the ith sample,
and let Ri be the total of the ranks in the ith sample.
TS: H =
%
12
n(n + 1)
k

i=1
R2
i
ni
&
−3(n + 1)
RR: H ≥h
where h is the critical value for the Kruskal–Wallis statistic (see
table on page 352) such that Prob [H ≥h] ≈α.
Note:
(1) The Kruskal–Wallis procedure is equivalent to an analysis of variance of
the ranks. Deﬁne the variance ratio as
VR =
k
i=1
ni

Ri−R
2
k−1
k
i=1
ni

j=1
(Rij−Ri)
2
n−k
(14.9)
where Ri = Ri/ni is the mean of the ranks assigned to the ith sample
and R = (n+1)/2 is the overall mean. The Kruskal–Wallis test statistic,
H, and VR are related by the equations
VR =
H(n −k)
(k −1)(n −1 −H),
H =
(n −1)(k −1)VR
(n −k) + (k −1)VR.
(14.10)
(2) As n →∞and each ni/n →λi > 0, H has approximately a chi–square
distribution with k −1 degrees of freedom. Practically, if H0 is true,
and either
(a) k = 3,
ni ≥6,
i = 1, 2, 3 or
(b) k > 3,
ni ≥5,
i = 1, 2, . . . , k
then H has a chi–square distribution with k −1 degrees of freedom.
(3) The variance ratio, VR, has approximately an F distribution with k −1
and n −k degrees of freedom.
c
⃝2000 by Chapman & Hall/CRC

Example 14.70:
Suppose that k = 3 treatments (A, B, and C) result in the following
observations {1.2, 1.8, 1.7}, {0.9, 0.7}, and {1.0, 0.8}. (Therefore, n1 = 3, n2 = 2,
n3 = 2, n = 7.) Ranking these values:
Treatment
A
B
C
Sample size, ni
3
2
2
5
3
4
Ranks
7
1
2
6
Rank sums, Ri
18
4
6
Hence, H =
12
7(8)

182
3 + 42
2 + 62
2

−3(8) = 33
7 ≈4.714. From the table on page 352
with {ni} = {3, 2, 2}, we observe that Prob [H ≥4.714] = .0476. At the α = .05 level
of signiﬁcance, there is evidence to suggest at least two of the populations diﬀer.
See R. L. Iman, D. Quade, and D. A. Alexander, Exact probability levels for
the Kruskal–Wallis test, Selected Tables in Mathematical Statistics, Volume
3, American Mathematical Society, Providence, RI, 1975.
14.4.1
Tables for Kruskal–Wallis test
{ni} = {2, 1, 1}
h
P(H ≥h)
2.700
0.5000
{ni} = {2, 2, 1}
h
P(H ≥h)
3.600
0.2000
{ni} = {2, 2, 2}
h
P(H ≥h)
4.571
0.0667
3.714
0.2000
{ni} = {3, 2, 1}
h
P(H ≥h)
4.286
0.1000
3.857
0.1333
{ni} = {3, 2, 2}
h
P(H ≥h)
5.357
0.0286
4.714
0.0476
4.500
0.0667
4.464
0.1048
3.929
0.1810
3.750
0.2190
3.607
0.2381
{ni} = {3, 3, 1}
h
P(H ≥h)
5.143
0.0429
4.571
0.1000
4.000
0.1286
3.286
0.1571
3.143
0.2429
2.571
0.3286
2.286
0.4857
{ni} = {3, 3, 2}
h
P(H ≥h)
6.250
0.0107
5.556
0.0250
5.361
0.0321
5.139
0.0607
5.000
0.0750
4.694
0.0929
4.556
0.1000
{ni} = {3, 3, 3}
h
P(H ≥h)
7.200
0.0036
6.489
0.0107
5.956
0.0250
5.689
0.0286
5.600
0.0500
5.067
0.0857
4.622
0.1000
{ni} = {4, 2, 1}
h
P(H ≥h)
4.821
0.0571
4.500
0.0762
4.018
0.1143
3.750
0.1333
3.696
0.1714
3.161
0.1905
2.893
0.2667
2.786
0.2857
{ni} = {4, 2, 2}
h
P(H ≥h)
6.000
0.0143
5.500
0.0238
5.333
0.0333
5.125
0.0524
4.500
0.0905
4.458
0.1000
4.167
0.1048
4.125
0.1524
{ni} = {4, 3, 1}
h
P(H ≥h)
5.833
0.0214
5.389
0.0357
5.208
0.0500
5.000
0.0571
4.764
0.0714
4.208
0.0786
4.097
0.0857
4.056
0.0929
{ni} = {4, 3, 2}
h
P(H ≥h)
7.000
0.0048
6.444
0.0079
6.300
0.0111
6.111
0.0206
5.800
0.0302
5.500
0.0397
5.400
0.0508
4.444
0.1016
c
⃝2000 by Chapman & Hall/CRC

{ni} = {4, 3, 3}
h
P(H ≥h)
8.018
0.0014
7.000
0.0062
6.745
0.0100
6.564
0.0171
6.018
0.0267
5.982
0.0343
5.727
0.0505
5.436
0.0619
5.064
0.0705
4.845
0.0810
4.700
0.1010
{ni} = {4, 4, 1}
h
P(H ≥h)
6.667
0.0095
6.167
0.0222
6.000
0.0286
5.667
0.0349
5.100
0.0413
4.967
0.0476
4.867
0.0540
4.267
0.0698
4.167
0.0825
4.067
0.1016
3.900
0.1079
{ni} = {4, 4, 2}
h
P(H ≥h)
7.855
0.0019
6.873
0.0108
6.545
0.0203
5.945
0.0279
5.645
0.0394
5.236
0.0521
4.991
0.0648
4.691
0.0800
4.555
0.0978
4.445
0.1029
{ni} = {4, 4, 3}
h
P(H ≥h)
8.909
0.0005
7.144
0.0097
7.136
0.0107
6.659
0.0201
6.182
0.0296
6.167
0.0306
6.000
0.0400
5.576
0.0507
4.712
0.0902
4.477
0.1022
14.5
THE RUNS TEST
A run is a maximal subsequence of elements with a common property.
Hypothesis test:
H0: the sequence is random.
Ha: the sequence is not random.
TS: V = the total number of runs
RR: V ≥v1 or V ≤v2
where v1 and v2 are critical values for the runs test (see page 354)
such that Prob [V ≥v1] ≈α/2 and Prob [V ≤v2] ≈α/2.
The normal approximation: Let m be the number of elements with the prop-
erty that occurs least and n be the number of elements with the other property.
As m and n increase, V has approximately a normal distribution with
µV = 2mn
m + n + 1
and
σ2
V = 2mn(2mn −m −n)
(m + n)2(m + n + 1).
(14.11)
The random variable
Z = V −µV
σV
(14.12)
has approximately a standard normal distribution.
Example 14.71:
Suppose the following sequence of heads (H) and tails (T) was
obtained from ﬂipping a coin: {H, H, T, T, H, T, H, T, T, T, T, H}. Is there any
evidence to suggest the coin is biased?
Solution:
(S1) Place vertical bars at the end of each run. The data set may be written to easily
count the number of runs.
HH | TT | H | T | H | TTTT | H |
c
⃝2000 by Chapman & Hall/CRC

(S2) Using this notation, there are 5 H’s, 7 T’s, and 7 runs.
(S3) The table on page 356 (using m = 5 and n = 7) indicates that 65% of the time
one would expect there to be 7 runs or fewer.
(S4) The table on page 356 (using m = 5 and n = 6) indicates that 42% of the time
one would expect there be 6 runs or fewer. Alternatively, 58% (since 1 −0.42 =
0.58) of the time there would be 7 runs or more.
(S5) In neither case is there any evidence to suggest the coin is biased.
14.5.1
Tables for the runs test
Runs can be used to test data for randomness or to test the hypothesis that
two samples come from the same distribution. A run is deﬁned as a succession
of identical elements which are followed and preceded by diﬀerent elements or
by no elements at all. Let m be the number of elements of one kind and n
be the number of elements of the other kind. Let v equal the total number of
runs among the n + m elements. The probability that exactly v runs occur is
given by
Prob [v runs ] =













2

n−1
(k−2)/2

m−1
(k−2)/2

n+m
n

if k is even

n−1
(k−3)/2

m−1
(k−1)/2

+

n−1
(k−1)/2

m−1
(k−3)/2

n+m
n

if k is odd
(14.13)
The following tables give the sampling distribution for v for values of m and
n less than or equal to 20. That is, the values listed in this table give the
probability that v or fewer runs will occur.
The table on page 364 gives percentage points of the distribution for larger
sample sizes when m = n. The columns headed with 0.5%, 1%, 2.5%, 5%
indicate the values of v such that v or fewer runs occur with probability less
than that indicated; the columns headed with 97.5%, 99%, 99.5% indicate
values of v for which the probability of v or more runs is less than 2.5%,
1%, 0.5%.
For large values of m and n, particularly for m = n greater
than 10, a normal approximation may be used, with the parameters given in
equation (14.11).
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 2
3
4
5
6
7
8
9
10
2, 2
0.3333 0.6667 1.0000
2, 3
0.2000 0.5000 0.9000 1.0000
2, 4
0.1333 0.4000 0.8000 1.0000
2, 5
0.0952 0.3333 0.7143 1.0000
2, 6
0.0714 0.2857 0.6429 1.0000
2, 7
0.0556 0.2500 0.5833 1.0000
2, 8
0.0444 0.2222 0.5333 1.0000
2, 9
0.0364 0.2000 0.4909 1.0000
2, 10
0.0303 0.1818 0.4545 1.0000
2, 11
0.0256 0.1667 0.4231 1.0000
2, 12
0.0220 0.1538 0.3956 1.0000
2, 13
0.0190 0.1429 0.3714 1.0000
2, 14
0.0167 0.1333 0.3500 1.0000
2, 15
0.0147 0.1250 0.3309 1.0000
2, 16
0.0131 0.1176 0.3137 1.0000
2, 17
0.0117 0.1111 0.2982 1.0000
2, 18
0.0105 0.1053 0.2842 1.0000
2, 19
0.0095 0.1000 0.2714 1.0000
2, 20
0.0087 0.0952 0.2597 1.0000
3, 3
0.1000 0.3000 0.7000 0.9000 1.0000
3, 4
0.0571 0.2000 0.5429 0.8000 0.9714 1.0000
3, 5
0.0357 0.1429 0.4286 0.7143 0.9286 1.0000
3, 6
0.0238 0.1071 0.3452 0.6429 0.8810 1.0000
3, 7
0.0167 0.0833 0.2833 0.5833 0.8333 1.0000
3, 8
0.0121 0.0667 0.2364 0.5333 0.7879 1.0000
3, 9
0.0091 0.0545 0.2000 0.4909 0.7455 1.0000
3, 10
0.0070 0.0455 0.1713 0.4545 0.7063 1.0000
3, 11
0.0055 0.0385 0.1484 0.4231 0.6703 1.0000
3, 12
0.0044 0.0330 0.1297 0.3956 0.6374 1.0000
3, 13
0.0036 0.0286 0.1143 0.3714 0.6071 1.0000
3, 14
0.0029 0.0250 0.1015 0.3500 0.5794 1.0000
3, 15
0.0025 0.0221 0.0907 0.3309 0.5539 1.0000
3, 16
0.0021 0.0196 0.0815 0.3137 0.5304 1.0000
3, 17
0.0018 0.0175 0.0737 0.2982 0.5088 1.0000
3, 18
0.0015 0.0158 0.0669 0.2842 0.4887 1.0000
3, 19
0.0013 0.0143 0.0610 0.2714 0.4701 1.0000
3, 20
0.0011 0.0130 0.0559 0.2597 0.4529 1.0000
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 2
3
4
5
6
7
8
9
10
4, 4
0.0286 0.1143 0.3714 0.6286 0.8857 0.9714 1.0000
4, 5
0.0159 0.0714 0.2619 0.5000 0.7857 0.9286 0.9921 1.0000
4, 6
0.0095 0.0476 0.1905 0.4048 0.6905 0.8810 0.9762 1.0000
4, 7
0.0061 0.0333 0.1424 0.3333 0.6061 0.8333 0.9545 1.0000
4, 8
0.0040 0.0242 0.1091 0.2788 0.5333 0.7879 0.9293 1.0000
4, 9
0.0028 0.0182 0.0853 0.2364 0.4713 0.7455 0.9021 1.0000
4, 10
0.0020 0.0140 0.0679 0.2028 0.4186 0.7063 0.8741 1.0000
4, 11
0.0015 0.0110 0.0549 0.1758 0.3736 0.6703 0.8462 1.0000
4, 12
0.0011 0.0088 0.0451 0.1538 0.3352 0.6374 0.8187 1.0000
4, 13
0.0008 0.0071 0.0374 0.1357 0.3021 0.6071 0.7920 1.0000
4, 14
0.0007 0.0059 0.0314 0.1206 0.2735 0.5794 0.7663 1.0000
4, 15
0.0005 0.0049 0.0266 0.1078 0.2487 0.5539 0.7417 1.0000
4, 16
0.0004 0.0041 0.0227 0.0970 0.2270 0.5304 0.7183 1.0000
4, 17
0.0003 0.0035 0.0195 0.0877 0.2080 0.5088 0.6959 1.0000
4, 18
0.0003 0.0030 0.0170 0.0797 0.1913 0.4887 0.6746 1.0000
4, 19
0.0002 0.0026 0.0148 0.0727 0.1764 0.4701 0.6544 1.0000
4, 20
0.0002 0.0023 0.0130 0.0666 0.1632 0.4529 0.6352 1.0000
5, 5
0.0079 0.0397 0.1667 0.3571 0.6429 0.8333 0.9603 0.9921 1.0000
5, 6
0.0043 0.0238 0.1104 0.2619 0.5216 0.7381 0.9113 0.9762 0.9978
5, 7
0.0025 0.0152 0.0758 0.1970 0.4242 0.6515 0.8535 0.9545 0.9924
5, 8
0.0016 0.0101 0.0536 0.1515 0.3473 0.5758 0.7933 0.9293 0.9837
5, 9
0.0010 0.0070 0.0390 0.1189 0.2867 0.5105 0.7343 0.9021 0.9720
5, 10
0.0007 0.0050 0.0290 0.0949 0.2388 0.4545 0.6783 0.8741 0.9580
5, 11
0.0005 0.0037 0.0220 0.0769 0.2005 0.4066 0.6264 0.8462 0.9423
5, 12
0.0003 0.0027 0.0170 0.0632 0.1698 0.3654 0.5787 0.8187 0.9253
5, 13
0.0002 0.0021 0.0133 0.0525 0.1450 0.3298 0.5352 0.7920 0.9076
5, 14
0.0002 0.0016 0.0106 0.0441 0.1246 0.2990 0.4958 0.7663 0.8893
5, 15
0.0001 0.0013 0.0085 0.0374 0.1078 0.2722 0.4600 0.7417 0.8709
5, 16
.04983 0.0010 0.0069 0.0320 0.0939 0.2487 0.4276 0.7183 0.8524
5, 17
.04759 0.0008 0.0057 0.0276 0.0823 0.2281 0.3982 0.6959 0.8341
5, 18
.04594 0.0007 0.0047 0.0239 0.0724 0.2098 0.3715 0.6746 0.8161
5, 19
.04471 0.0006 0.0040 0.0209 0.0641 0.1937 0.3473 0.6544 0.7984
5, 20
.04376 0.0005 0.0033 0.0184 0.0570 0.1793 0.3252 0.6352 0.7811
6, 6
0.0022 0.0130 0.0671 0.1753 0.3918 0.6082 0.8247 0.9329 0.9870
6, 7
0.0012 0.0076 0.0425 0.1212 0.2960 0.5000 0.7331 0.8788 0.9662
6, 8
0.0007 0.0047 0.0280 0.0862 0.2261 0.4126 0.6457 0.8205 0.9371
6, 9
0.0004 0.0030 0.0190 0.0629 0.1748 0.3427 0.5664 0.7622 0.9021
6, 10
0.0002 0.0020 0.0132 0.0470 0.1369 0.2867 0.4965 0.7063 0.8636
6, 11
0.0002 0.0014 0.0095 0.0357 0.1084 0.2418 0.4357 0.6538 0.8235
6, 12
0.0001 0.0010 0.0069 0.0276 0.0869 0.2054 0.3832 0.6054 0.7831
6, 13
.04737 0.0007 0.0051 0.0217 0.0704 0.1758 0.3379 0.5609 0.7434
6, 14
.04516 0.0005 0.0039 0.0173 0.0575 0.1514 0.2990 0.5204 0.7049
6, 15
.04369 0.0004 0.0030 0.0139 0.0475 0.1313 0.2655 0.4835 0.6680
6, 16
.04268 0.0003 0.0023 0.0114 0.0395 0.1146 0.2365 0.4499 0.6329
6, 17
.04198 0.0002 0.0018 0.0093 0.0331 0.1005 0.2114 0.4195 0.5998
6, 18
.04149 0.0002 0.0014 0.0078 0.0280 0.0886 0.1896 0.3917 0.5685
6, 19
.04113 0.0001 0.0012 0.0065 0.0238 0.0785 0.1706 0.3665 0.5392
6, 20
.05869 0.0001 0.0009 0.0055 0.0203 0.0698 0.1540 0.3434 0.5118
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 11
12
13
14
15
16
17
18
19
20
21
4, 4
4, 5
4, 6
4, 7
4, 8
4, 9
4, 10
4, 11
4, 12
4, 13
4, 14
4, 15
4, 16
4, 17
4, 18
4, 19
4, 20
5, 5
5, 6
1.0000
5, 7
1.0000
5, 8
1.0000
5, 9
1.0000
5, 10
1.0000
5, 11
1.0000
5, 12
1.0000
5, 13
1.0000
5, 14
1.0000
5, 15
1.0000
5, 16
1.0000
5, 17
1.0000
5, 18
1.0000
5, 19
1.0000
5, 20
1.0000
6, 6
0.9978 1.0000
6, 7
0.9924 0.9994 1.0000
6, 8
0.9837 0.9977 1.0000
6, 9
0.9720 0.9944 1.0000
6, 10
0.9580 0.9895 1.0000
6, 11
0.9423 0.9830 1.0000
6, 12
0.9253 0.9751 1.0000
6, 13
0.9076 0.9659 1.0000
6, 14
0.8893 0.9557 1.0000
6, 15
0.8709 0.9447 1.0000
6, 16
0.8524 0.9329 1.0000
6, 17
0.8341 0.9207 1.0000
6, 18
0.8161 0.9081 1.0000
6, 19
0.7984 0.8952 1.0000
6, 20
0.7811 0.8822 1.0000
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 2
3
4
5
6
7
8
9
10
7, 7
0.0006 0.0041 0.0251 0.0775 0.2086 0.3834 0.6166 0.7914 0.9225
7, 8
0.0003 0.0023 0.0154 0.0513 0.1492 0.2960 0.5136 0.7040 0.8671
7, 9
0.0002 0.0014 0.0098 0.0350 0.1084 0.2308 0.4266 0.6224 0.8059
7, 10
0.0001 0.0009 0.0064 0.0245 0.0800 0.1818 0.3546 0.5490 0.7433
7, 11
.04628 0.0006 0.0043 0.0175 0.0600 0.1448 0.2956 0.4842 0.6821
7, 12
.04397 0.0004 0.0030 0.0128 0.0456 0.1165 0.2475 0.4276 0.6241
7, 13
.04258 0.0003 0.0021 0.0095 0.0351 0.0947 0.2082 0.3785 0.5700
7, 14
.04172 0.0002 0.0015 0.0072 0.0273 0.0777 0.1760 0.3359 0.5204
7, 15
.04117 0.0001 0.0011 0.0055 0.0216 0.0642 0.1496 0.2990 0.4751
7, 16
.05816 .04938 0.0008 0.0043 0.0172 0.0536 0.1278 0.2670 0.4340
7, 17
.05578 .04693 0.0006 0.0034 0.0138 0.0450 0.1097 0.2392 0.3969
7, 18
.05416 .04520 0.0005 0.0027 0.0112 0.0381 0.0947 0.2149 0.3634
7, 19
.05304 .04395 0.0004 0.0022 0.0092 0.0324 0.0820 0.1937 0.3332
7, 20
.05225 .04304 0.0003 0.0018 0.0075 0.0278 0.0714 0.1751 0.3060
8, 8
0.0002 0.0012 0.0089 0.0317 0.1002 0.2145 0.4048 0.5952 0.7855
8, 9
.04823 0.0007 0.0053 0.0203 0.0687 0.1573 0.3186 0.5000 0.7016
8, 10
.04457 0.0004 0.0033 0.0134 0.0479 0.1170 0.2514 0.4194 0.6209
8, 11
.04265 0.0003 0.0021 0.0090 0.0341 0.0882 0.1994 0.3522 0.5467
8, 12
.04159 0.0002 0.0014 0.0063 0.0246 0.0674 0.1591 0.2966 0.4800
8, 13
.05983 0.0001 0.0009 0.0044 0.0181 0.0521 0.1278 0.2508 0.4211
8, 14
.05625 .04688 0.0006 0.0032 0.0134 0.0408 0.1034 0.2129 0.3695
8, 15
.05408 .04469 0.0004 0.0023 0.0101 0.0322 0.0842 0.1816 0.3245
8, 16
.05272 .04326 0.0003 0.0017 0.0077 0.0257 0.0690 0.1556 0.2856
8, 17
.05185 .04231 0.0002 0.0013 0.0060 0.0207 0.0570 0.1340 0.2518
8, 18
.05128 .04166 0.0002 0.0010 0.0047 0.0169 0.0473 0.1159 0.2225
8, 19
.06901 .04122 0.0001 0.0008 0.0037 0.0138 0.0395 0.1006 0.1971
8, 20
.06643 .05901 .04946 0.0006 0.0029 0.0114 0.0332 0.0878 0.1751
9, 9
.04411 0.0004 0.0030 0.0122 0.0445 0.1090 0.2380 0.3992 0.6008
9, 10
.04217 0.0002 0.0018 0.0076 0.0294 0.0767 0.1786 0.3186 0.5095
9, 11
.04119 0.0001 0.0011 0.0049 0.0199 0.0549 0.1349 0.2549 0.4300
9, 12
.05680 .04714 0.0007 0.0032 0.0137 0.0399 0.1028 0.2049 0.3621
9, 13
.05402 .04442 0.0004 0.0022 0.0096 0.0294 0.0789 0.1656 0.3050
9, 14
.05245 .04281 0.0003 0.0015 0.0068 0.0220 0.0612 0.1347 0.2572
9, 15
.05153 .04184 0.0002 0.0010 0.0049 0.0166 0.0478 0.1102 0.2174
9, 16
.06979 .04122 0.0001 0.0007 0.0036 0.0127 0.0377 0.0907 0.1842
9, 17
.06640 .05832 .04903 0.0005 0.0027 0.0099 0.0299 0.0751 0.1566
9, 18
.06427 .05576 .04638 0.0004 0.0020 0.0077 0.0240 0.0626 0.1336
9, 19
.06290 .05405 .04458 0.0003 0.0015 0.0061 0.0193 0.0524 0.1144
9, 20
.06200 .05290 .04333 0.0002 0.0012 0.0048 0.0157 0.0441 0.0983
10, 10
.04108 0.0001 0.0010 0.0045 0.0185 0.0513 0.1276 0.2422 0.4141
10, 11
.05567 .04595 0.0006 0.0027 0.0119 0.0349 0.0920 0.1849 0.3350
10, 12
.05309 .04340 0.0003 0.0017 0.0078 0.0242 0.0670 0.1421 0.2707
10, 13
.05175 .04201 0.0002 0.0011 0.0053 0.0170 0.0493 0.1099 0.2189
10, 14
.05102 .04122 0.0001 0.0007 0.0036 0.0122 0.0367 0.0857 0.1775
10, 15
.06612 .05765 .04847 0.0005 0.0025 0.0088 0.0275 0.0673 0.1445
10, 16
.06377 .05489 .04557 0.0003 0.0018 0.0065 0.0209 0.0533 0.1180
10, 17
.06237 .05320 .04373 0.0002 0.0013 0.0048 0.0160 0.0425 0.0968
10, 18
.06152 .05213 .04255 0.0002 0.0009 0.0036 0.0124 0.0341 0.0798
10, 19
.07999 .05145 .04176 0.0001 0.0007 0.0028 0.0096 0.0276 0.0661
10, 20
.07666 .06999 .04124 .04864 0.0005 0.0021 0.0076 0.0225 0.0550
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 11
12
13
14
15
16
17
18
19
20
21
7, 7
0.9749 0.9959 0.9994 1.0000
7, 8
0.9487 0.9879 0.9977 0.9998 1.0000
7, 9
0.9161 0.9748 0.9944 0.9993 1.0000
7, 10
0.8794 0.9571 0.9895 0.9981 1.0000
7, 11
0.8405 0.9355 0.9830 0.9962 1.0000
7, 12
0.8009 0.9109 0.9751 0.9935 1.0000
7, 13
0.7616 0.8842 0.9659 0.9898 1.0000
7, 14
0.7233 0.8561 0.9557 0.9852 1.0000
7, 15
0.6864 0.8273 0.9447 0.9799 1.0000
7, 16
0.6512 0.7982 0.9329 0.9738 1.0000
7, 17
0.6178 0.7692 0.9207 0.9669 1.0000
7, 18
0.5862 0.7407 0.9081 0.9595 1.0000
7, 19
0.5565 0.7128 0.8952 0.9516 1.0000
7, 20
0.5286 0.6857 0.8822 0.9433 1.0000
8, 8
0.8998 0.9683 0.9911 0.9988 0.9998 1.0000
8, 9
0.8427 0.9394 0.9797 0.9958 0.9993 1.0000 1.0000
8, 10
0.7822 0.9031 0.9636 0.9905 0.9981 0.9998 1.0000
8, 11
0.7217 0.8618 0.9434 0.9823 0.9962 0.9994 1.0000
8, 12
0.6634 0.8174 0.9201 0.9714 0.9935 0.9987 1.0000
8, 13
0.6084 0.7718 0.8944 0.9580 0.9898 0.9976 1.0000
8, 14
0.5573 0.7263 0.8672 0.9423 0.9852 0.9960 1.0000
8, 15
0.5103 0.6818 0.8390 0.9248 0.9799 0.9939 1.0000
8, 16
0.4674 0.6389 0.8104 0.9057 0.9738 0.9913 1.0000
8, 17
0.4285 0.5981 0.7818 0.8855 0.9669 0.9881 1.0000
8, 18
0.3931 0.5595 0.7536 0.8645 0.9595 0.9844 1.0000
8, 19
0.3611 0.5232 0.7258 0.8429 0.9516 0.9803 1.0000
8, 20
0.3322 0.4893 0.6988 0.8210 0.9433 0.9757 1.0000
9, 9
0.7620 0.8910 0.9555 0.9878 0.9970 0.9996 1.0000 1.0000
9, 10
0.6814 0.8342 0.9233 0.9742 0.9924 0.9986 0.9998 1.0000 1.0000
9, 11
0.6050 0.7731 0.8851 0.9551 0.9851 0.9965 0.9994 0.9999 1.0000
9, 12
0.5350 0.7111 0.8431 0.9311 0.9751 0.9931 0.9987 0.9998 1.0000
9, 13
0.4721 0.6505 0.7991 0.9031 0.9625 0.9880 0.9976 0.9996 1.0000
9, 14
0.4164 0.5928 0.7545 0.8721 0.9477 0.9813 0.9960 0.9991 1.0000
9, 15
0.3674 0.5389 0.7104 0.8390 0.9309 0.9729 0.9939 0.9985 1.0000
9, 16
0.3245 0.4892 0.6675 0.8047 0.9125 0.9629 0.9913 0.9976 1.0000
9, 17
0.2871 0.4437 0.6264 0.7699 0.8929 0.9515 0.9881 0.9963 1.0000
9, 18
0.2545 0.4024 0.5872 0.7351 0.8724 0.9388 0.9844 0.9948 1.0000
9, 19
0.2261 0.3650 0.5503 0.7008 0.8513 0.9250 0.9803 0.9930 1.0000
9, 20
0.2013 0.3313 0.5155 0.6672 0.8298 0.9103 0.9757 0.9908 1.0000
10, 10
0.5859 0.7578 0.8724 0.9487 0.9815 0.9955 0.9990 0.9999 1.0000 1.0000
10, 11
0.5000 0.6800 0.8151 0.9151 0.9651 0.9896 0.9973 0.9996 0.9999 1.0000
10, 12
0.4250 0.6050 0.7551 0.8751 0.9437 0.9804 0.9942 0.9988 0.9998 1.0000 1.0000
10, 13
0.3607 0.5351 0.6950 0.8307 0.9180 0.9678 0.9896 0.9974 0.9996 0.9999 1.0000
10, 14
0.3062 0.4715 0.6369 0.7839 0.8889 0.9519 0.9834 0.9952 0.9991 0.9999 1.0000
10, 15
0.2602 0.4146 0.5818 0.7361 0.8574 0.9330 0.9755 0.9920 0.9985 0.9997 1.0000
10, 16
0.2216 0.3641 0.5303 0.6886 0.8243 0.9115 0.9661 0.9879 0.9976 0.9994 1.0000
10, 17
0.1893 0.3197 0.4828 0.6423 0.7904 0.8880 0.9551 0.9826 0.9963 0.9991 1.0000
10, 18
0.1621 0.2809 0.4393 0.5978 0.7562 0.8629 0.9429 0.9763 0.9948 0.9985 1.0000
10, 19
0.1392 0.2470 0.3997 0.5554 0.7223 0.8367 0.9296 0.9689 0.9930 0.9978 1.0000
10, 20
0.1200 0.2175 0.3638 0.5155 0.6889 0.8097 0.9153 0.9606 0.9908 0.9969 1.0000
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 2
3
4
5
6
7
8
9
10
11, 11
.05284 .04312 0.0003 0.0016 0.0073 0.0226 0.0635 0.1349 0.2599
11, 12
.05148 .04170 0.0002 0.0010 0.0046 0.0150 0.0443 0.0992 0.2017
11, 13
.06801 .05961 0.0001 0.0006 0.0030 0.0101 0.0313 0.0736 0.1569
11, 14
.06449 .05561 .04639 0.0004 0.0019 0.0069 0.0223 0.0551 0.1224
11, 15
.06259 .05337 .04396 0.0002 0.0013 0.0048 0.0161 0.0416 0.0960
11, 16
.06153 .05207 .04251 0.0002 0.0009 0.0034 0.0118 0.0317 0.0757
11, 17
.07931 .05130 .04162 0.0001 0.0006 0.0025 0.0087 0.0244 0.0600
11, 18
.07578 .06838 .04107 .04721 0.0004 0.0018 0.0065 0.0189 0.0478
11, 19
.07366 .06549 .05714 .04500 0.0003 0.0013 0.0049 0.0148 0.0383
11, 20
.07236 .06366 .05485 .04351 0.0002 0.0010 0.0037 0.0116 0.0308
12, 12
.06740 .05888 .04984 0.0005 0.0028 0.0095 0.0296 0.0699 0.1504
12, 13
.06385 .05481 .04556 0.0003 0.0017 0.0061 0.0201 0.0498 0.1126
12, 14
.06207 .05269 .04323 0.0002 0.0011 0.0040 0.0138 0.0358 0.0847
12, 15
.06115 .05155 .04193 0.0001 0.0007 0.0027 0.0096 0.0260 0.0640
12, 16
.07657 .06920 .04118 .04769 0.0005 0.0018 0.0068 0.0191 0.0487
12, 17
.07385 .06559 .05734 .04497 0.0003 0.0013 0.0048 0.0142 0.0373
12, 18
.07231 .06347 .05467 .04328 0.0002 0.0009 0.0035 0.0106 0.0288
12, 19
.07142 .06220 .05303 .04220 0.0001 0.0006 0.0025 0.0080 0.0223
12, 20
.08886 .06142 .05199 .04150 .04983 0.0005 0.0019 0.0061 0.0175
13, 13
.06192 .05250 .04302 0.0002 0.0010 0.0038 0.0131 0.0341 0.0812
13, 14
.07997 .05135 .04169 0.0001 0.0006 0.0024 0.0087 0.0236 0.0589
13, 15
.07534 .06748 .05972 .04636 0.0004 0.0016 0.0058 0.0165 0.0430
13, 16
.07295 .06427 .05573 .04389 0.0002 0.0010 0.0040 0.0117 0.0316
13, 17
.07167 .06251 .05346 .04243 0.0002 0.0007 0.0027 0.0084 0.0234
13, 18
.08970 .06150 .05213 .04155 0.0001 0.0005 0.0019 0.0061 0.0175
13, 19
.08576 .07921 .05134 .04100 .04682 0.0003 0.0014 0.0045 0.0132
13, 20
.08349 .07576 .06853 .05662 .04460 0.0002 0.0010 0.0033 0.0100
14, 14
.07499 .06698 .05912 .04597 0.0004 0.0015 0.0056 0.0157 0.0412
14, 15
.07258 .06374 .05507 .04344 0.0002 0.0009 0.0036 0.0107 0.0291
14, 16
.07138 .06206 .05289 .04203 0.0001 0.0006 0.0024 0.0073 0.0207
14, 17
.08754 .06117 .05169 .04123 .04829 0.0004 0.0016 0.0051 0.0149
14, 18
.08424 .07679 .05101 .05757 .04526 0.0002 0.0011 0.0035 0.0108
14, 19
.08244 .07403 .06612 .05476 .04339 0.0002 0.0007 0.0025 0.0079
14, 20
.08144 .07244 .06379 .05304 .04222 0.0001 0.0005 0.0018 0.0058
15, 15
.07129 .06193 .05272 .04191 0.0001 0.0006 0.0023 0.0070 0.0199
15, 16
.08665 .06103 .05150 .04109 .04745 0.0003 0.0014 0.0046 0.0137
15, 17
.08354 .07566 .06848 .05639 .04450 0.0002 0.0009 0.0031 0.0095
15, 18
.08193 .07318 .06491 .05382 .04277 0.0001 0.0006 0.0021 0.0067
15, 19
.08108 .07183 .06290 .05233 .04173 .04873 0.0004 0.0014 0.0047
15, 20
.09616 .07108 .06175 .05144 .04110 .04573 0.0003 0.0010 0.0034
16, 16
.08333 .07532 .06802 .05604 .04427 0.0002 0.0009 0.0030 0.0092
16, 17
.08171 .07283 .06440 .05342 .04250 0.0001 0.0006 0.0019 0.0062
16, 18
.09907 .07154 .06247 .05198 .04149 .04754 0.0004 0.0013 0.0042
16, 19
.09493 .08862 .06142 .05117 .05909 .04473 0.0002 0.0008 0.0029
16, 20
.09274 .08493 .07829 .06707 .05562 .04302 0.0002 0.0006 0.0020
17, 17
.09857 .07146 .06234 .05188 .04142 .04718 0.0003 0.0012 0.0041
17, 18
.09441 .08771 .06128 .05106 .05825 .04430 0.0002 0.0008 0.0027
17, 19
.09233 .08419 .07712 .06607 .05488 .04262 0.0001 0.0005 0.0018
17, 20
.09126 .08233 .07406 .06356 .05294 .04163 .04845 0.0003 0.0012
18, 18
.09220 .08397 .07677 .06577 .05465 .04250 0.0001 0.0005 0.0017
18, 19
.09113 .08209 .07367 .06322 .05268 .04148 .04776 0.0003 0.0011
18, 20
.010596 .08113 .07204 .06184 .05157 .05896 .04482 0.0002 0.0007
19, 19
.010566 .08108 .07194 .06175 .05150 .05856 .04462 0.0002 0.0007
19, 20
.010290 .09566 .07105 .07973 .06857 .05503 .04280 0.0001 0.0005
20, 20
.010145 .09290 .08553 .07527 .06477 .05288 .04165 .04710 0.0003
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 11
12
13
14
15
16
17
18
19
20
21
11, 11
0.4100 0.5900 0.7401 0.8651 0.9365 0.9774 0.9927 0.9984 0.9997 1.0000 1.0000
11, 12
0.3350 0.5072 0.6650 0.8086 0.9008 0.9594 0.9850 0.9960 0.9990 0.9999 1.0000
11, 13
0.2735 0.4334 0.5933 0.7488 0.8598 0.9360 0.9740 0.9919 0.9978 0.9996 0.9999
11, 14
0.2235 0.3690 0.5267 0.6883 0.8154 0.9078 0.9598 0.9857 0.9958 0.9991 0.9999
11, 15
0.1831 0.3137 0.4660 0.6293 0.7692 0.8758 0.9424 0.9774 0.9930 0.9981 0.9997
11, 16
0.1504 0.2665 0.4116 0.5728 0.7225 0.8410 0.9224 0.9669 0.9891 0.9967 0.9994
11, 17
0.1240 0.2265 0.3632 0.5199 0.6765 0.8043 0.9002 0.9542 0.9841 0.9948 0.9991
11, 18
0.1027 0.1928 0.3205 0.4708 0.6317 0.7666 0.8763 0.9395 0.9781 0.9922 0.9985
11, 19
0.0853 0.1644 0.2830 0.4257 0.5888 0.7286 0.8510 0.9231 0.9711 0.9889 0.9978
11, 20
0.0712 0.1404 0.2500 0.3846 0.5480 0.6908 0.8247 0.9051 0.9631 0.9849 0.9969
12, 12
0.2632 0.4211 0.5789 0.7368 0.8496 0.9301 0.9704 0.9905 0.9972 0.9995 0.9999
12, 13
0.2068 0.3475 0.5000 0.6642 0.7932 0.8937 0.9502 0.9816 0.9939 0.9985 0.9997
12, 14
0.1628 0.2860 0.4296 0.5938 0.7345 0.8518 0.9251 0.9691 0.9886 0.9968 0.9992
12, 15
0.1286 0.2351 0.3681 0.5277 0.6759 0.8062 0.8958 0.9528 0.9813 0.9940 0.9984
12, 16
0.1020 0.1933 0.3149 0.4669 0.6189 0.7585 0.8632 0.9330 0.9718 0.9899 0.9971
12, 17
0.0813 0.1591 0.2693 0.4118 0.5646 0.7101 0.8283 0.9101 0.9602 0.9844 0.9953
12, 18
0.0651 0.1312 0.2304 0.3626 0.5137 0.6621 0.7919 0.8847 0.9465 0.9774 0.9929
12, 19
0.0524 0.1085 0.1973 0.3189 0.4665 0.6153 0.7548 0.8572 0.9311 0.9690 0.9898
12, 20
0.0424 0.0900 0.1693 0.2803 0.4231 0.5703 0.7176 0.8281 0.9140 0.9590 0.9860
13, 13
0.1566 0.2772 0.4179 0.5821 0.7228 0.8434 0.9188 0.9659 0.9869 0.9962 0.9990
13, 14
0.1189 0.2205 0.3475 0.5056 0.6525 0.7880 0.8811 0.9446 0.9764 0.9921 0.9976
13, 15
0.0906 0.1753 0.2883 0.4365 0.5847 0.7299 0.8388 0.9182 0.9623 0.9858 0.9952
13, 16
0.0695 0.1396 0.2389 0.3751 0.5212 0.6714 0.7934 0.8873 0.9446 0.9771 0.9917
13, 17
0.0535 0.1113 0.1980 0.3215 0.4628 0.6141 0.7465 0.8529 0.9238 0.9658 0.9868
13, 18
0.0415 0.0890 0.1643 0.2752 0.4098 0.5592 0.6992 0.8159 0.9001 0.9520 0.9805
13, 19
0.0324 0.0714 0.1365 0.2353 0.3623 0.5074 0.6525 0.7772 0.8742 0.9358 0.9728
13, 20
0.0254 0.0575 0.1138 0.2012 0.3200 0.4592 0.6072 0.7377 0.8465 0.9174 0.9635
14, 14
0.0871 0.1697 0.2798 0.4266 0.5734 0.7202 0.8303 0.9129 0.9588 0.9843 0.9944
14, 15
0.0642 0.1306 0.2247 0.3576 0.5000 0.6519 0.7753 0.8749 0.9358 0.9727 0.9893
14, 16
0.0476 0.1007 0.1804 0.2986 0.4336 0.5854 0.7183 0.8322 0.9081 0.9574 0.9820
14, 17
0.0355 0.0779 0.1450 0.2487 0.3745 0.5226 0.6614 0.7863 0.8765 0.9382 0.9721
14, 18
0.0266 0.0604 0.1167 0.2068 0.3227 0.4643 0.6058 0.7386 0.8418 0.9155 0.9598
14, 19
0.0202 0.0471 0.0942 0.1720 0.2776 0.4110 0.5527 0.6903 0.8049 0.8898 0.9450
14, 20
0.0153 0.0368 0.0763 0.1432 0.2387 0.3630 0.5027 0.6425 0.7667 0.8616 0.9281
15, 15
0.0457 0.0974 0.1749 0.2912 0.4241 0.5759 0.7088 0.8251 0.9026 0.9543 0.9801
15, 16
0.0328 0.0728 0.1362 0.2362 0.3576 0.5046 0.6424 0.7710 0.8638 0.9305 0.9672
15, 17
0.0237 0.0546 0.1061 0.1912 0.3005 0.4393 0.5781 0.7147 0.8210 0.9020 0.9505
15, 18
0.0173 0.0412 0.0830 0.1546 0.2519 0.3806 0.5174 0.6581 0.7754 0.8693 0.9303
15, 19
0.0127 0.0312 0.0650 0.1251 0.2109 0.3286 0.4610 0.6026 0.7285 0.8334 0.9068
15, 20
0.0094 0.0237 0.0512 0.1014 0.1766 0.2831 0.4095 0.5493 0.6813 0.7952 0.8806
16, 16
0.0228 0.0528 0.1028 0.1862 0.2933 0.4311 0.5689 0.7067 0.8138 0.8972 0.9472
16, 17
0.0160 0.0385 0.0778 0.1465 0.2397 0.3659 0.5000 0.6420 0.7603 0.8584 0.9222
16, 18
0.0113 0.0282 0.0591 0.1153 0.1956 0.3091 0.4369 0.5789 0.7051 0.8155 0.8928
16, 19
0.0080 0.0207 0.0450 0.0908 0.1594 0.2603 0.3801 0.5188 0.6498 0.7697 0.8596
16, 20
0.0058 0.0153 0.0345 0.0716 0.1300 0.2188 0.3297 0.4628 0.5959 0.7224 0.8237
17, 17
0.0109 0.0272 0.0572 0.1122 0.1907 0.3028 0.4290 0.5710 0.6972 0.8093 0.8878
17, 18
0.0075 0.0194 0.0422 0.0859 0.1514 0.2495 0.3659 0.5038 0.6341 0.7567 0.8486
17, 19
0.0052 0.0139 0.0313 0.0659 0.1202 0.2049 0.3108 0.4418 0.5728 0.7022 0.8057
17, 20
0.0036 0.0100 0.0233 0.0506 0.0955 0.1680 0.2631 0.3854 0.5146 0.6474 0.7604
18, 18
0.0050 0.0134 0.0303 0.0640 0.1171 0.2004 0.3046 0.4349 0.5651 0.6954 0.7996
18, 19
0.0034 0.0094 0.0219 0.0479 0.0906 0.1606 0.2525 0.3729 0.5000 0.6338 0.7475
18, 20
0.0023 0.0066 0.0159 0.0359 0.0701 0.1285 0.2088 0.3182 0.4398 0.5736 0.6940
19, 19
0.0022 0.0064 0.0154 0.0349 0.0683 0.1256 0.2044 0.3127 0.4331 0.5669 0.6873
19, 20
0.0015 0.0044 0.0109 0.0255 0.0516 0.0981 0.1650 0.2610 0.3729 0.5033 0.6271
20, 20
0.0009 0.0029 0.0075 0.0182 0.0380 0.0748 0.1301 0.2130 0.3143 0.4381 0.5619
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 22
23
24
25
26
27
28
29
11, 11
11, 12
1.0000
11, 13
1.0000
11, 14
1.0000 1.0000
11, 15
1.0000 1.0000
11, 16
0.9999 1.0000
11, 17
0.9998 1.0000
11, 18
0.9996 1.0000
11, 19
0.9994 1.0000
11, 20
0.9991 1.0000
12, 12
1.0000
12, 13
1.0000 1.0000
12, 14
0.9999 1.0000 1.0000
12, 15
0.9997 1.0000 1.0000
12, 16
0.9993 0.9999 1.0000 1.0000
12, 17
0.9987 0.9998 1.0000 1.0000
12, 18
0.9978 0.9996 0.9999 1.0000
12, 19
0.9966 0.9994 0.9999 1.0000
12, 20
0.9950 0.9991 0.9998 1.0000
13, 13
0.9998 1.0000 1.0000
13, 14
0.9995 0.9999 1.0000 1.0000
13, 15
0.9988 0.9997 1.0000 1.0000
13, 16
0.9975 0.9994 0.9999 1.0000 1.0000
13, 17
0.9957 0.9989 0.9997 1.0000 1.0000
13, 18
0.9930 0.9981 0.9995 0.9999 1.0000 1.0000
13, 19
0.9894 0.9969 0.9991 0.9999 1.0000 1.0000
13, 20
0.9848 0.9954 0.9986 0.9998 1.0000 1.0000
14, 14
0.9985 0.9996 0.9999 1.0000
14, 15
0.9967 0.9991 0.9998 1.0000 1.0000
14, 16
0.9938 0.9981 0.9995 0.9999 1.0000 1.0000
14, 17
0.9894 0.9965 0.9990 0.9998 1.0000 1.0000
14, 18
0.9834 0.9941 0.9982 0.9996 0.9999 1.0000 1.0000
14, 19
0.9756 0.9909 0.9970 0.9992 0.9998 1.0000 1.0000
14, 20
0.9660 0.9867 0.9952 0.9987 0.9997 1.0000 1.0000
15, 15
0.9930 0.9977 0.9994 0.9999 1.0000 1.0000
15, 16
0.9872 0.9954 0.9987 0.9997 0.9999 1.0000 1.0000
15, 17
0.9789 0.9918 0.9974 0.9992 0.9998 1.0000 1.0000
15, 18
0.9678 0.9866 0.9953 0.9985 0.9996 0.9999 1.0000 1.0000
15, 19
0.9540 0.9798 0.9923 0.9975 0.9993 0.9998 1.0000 1.0000
15, 20
0.9375 0.9712 0.9881 0.9959 0.9987 0.9997 0.9999 1.0000
16, 16
0.9772 0.9908 0.9970 0.9991 0.9998 1.0000 1.0000
16, 17
0.9634 0.9840 0.9942 0.9981 0.9995 0.9999 1.0000 1.0000
16, 18
0.9457 0.9747 0.9900 0.9964 0.9989 0.9997 0.9999 1.0000
16, 19
0.9244 0.9626 0.9840 0.9938 0.9980 0.9994 0.9999 1.0000
16, 20
0.8996 0.9479 0.9761 0.9903 0.9965 0.9989 0.9997 0.9999
17, 17
0.9428 0.9728 0.9891 0.9959 0.9988 0.9997 0.9999 1.0000
17, 18
0.9172 0.9578 0.9816 0.9925 0.9975 0.9992 0.9998 1.0000
17, 19
0.8872 0.9391 0.9714 0.9876 0.9954 0.9985 0.9996 0.9999
17, 20
0.8534 0.9168 0.9584 0.9808 0.9924 0.9972 0.9992 0.9998
18, 18
0.8829 0.9360 0.9697 0.9866 0.9950 0.9983 0.9995 0.9999
18, 19
0.8438 0.9094 0.9540 0.9781 0.9911 0.9966 0.9990 0.9997
18, 20
0.8010 0.8788 0.9345 0.9670 0.9856 0.9941 0.9980 0.9994
19, 19
0.7956 0.8744 0.9317 0.9651 0.9846 0.9936 0.9978 0.9993
19, 20
0.7444 0.8350 0.9048 0.9484 0.9756 0.9891 0.9959 0.9985
20, 20
0.6857 0.7870 0.8699 0.9252 0.9620 0.9818 0.9925 0.9971
c
⃝2000 by Chapman & Hall/CRC

Distribution of total number of runs v in samples of size (m, n)
m, n
v = 30
31
32
33
34
35
36
37
11, 11
11, 12
11, 13
11, 14
11, 15
11, 16
11, 17
11, 18
11, 19
11, 20
12, 12
12, 13
12, 14
12, 15
12, 16
12, 17
12, 18
12, 19
12, 20
13, 13
13, 14
13, 15
13, 16
13, 17
13, 18
13, 19
13, 20
14, 14
14, 15
14, 16
14, 17
14, 18
14, 19
14, 20
15, 15
15, 16
15, 17
15, 18
15, 19
15, 20
16, 16
16, 17
16, 18
1.0000
16, 19
1.0000
16, 20
1.0000 1.0000
17, 17
1.0000
17, 18
1.0000
17, 19
1.0000 1.0000
17, 20
0.9999 1.0000
18, 18
1.0000 1.0000
18, 19
0.9999 1.0000 1.0000
18, 20
0.9998 1.0000 1.0000
19, 19
0.9998 1.0000 1.0000
19, 20
0.9996 0.9999 1.0000 1.0000
20, 20
0.9991 0.9997 0.9999 1.0000 1.0000
c
⃝2000 by Chapman & Hall/CRC

The values listed in the previous tables indicate the probability that v or
fewer runs will occur. For example, for two samples of size 4, the probability
of three or fewer runs is 0.114. For sample size m = n, and m larger than
10, the following table can be used. The columns headed 0.5, 1, 2.5, and 5
give values of v such that v or fewer runs occur with probability less than the
indicated percentage. For example, for m = n = 12, the probability of 8 or
fewer runs is approximately 5%. The columns headed 95, 97.5, 99, and 99.5
give values of v for which the probability of v or more runs is less than 5, 2.5,
1, or 0.5 percent.
Distribution of the total number of
runs v in samples of size m = n.
m = n 0.5 1.0 2.5 5.0 95.0 97.5 99.0 99.5 mean var (σ2)
s.d. (σ)
11
5
6
7
7
16
16
17
18
12
5.24
2.29
12
6
7
7
8
17
18
18
19
13
5.74
2.40
13
7
7
8
9
18
19
20
20
14
6.24
2.50
14
7
8
9
10
19
20
21
22
15
6.74
2.60
15
8
9
10
11
20
21
22
23
16
7.24
2.69
16
9
10
11
11
22
22
23
24
17
7.74
2.78
17
10
10
11
12
23
24
25
25
18
8.24
2.87
18
11
11
12
13
24
25
26
26
19
8.74
2.96
19
11
12
13
14
25
26
27
28
20
9.24
3.04
20
12
13
14
15
26
27
28
29
21
9.74
3.12
25
16
17
18
19
32
33
34
35
26
12.24
3.50
30
20
21
23
24
37
38
40
41
31
14.75
3.84
35
24
25
27
28
43
44
46
47
36
17.25
4.15
40
29
30
31
33
48
50
51
52
41
19.75
4.44
45
33
34
36
37
54
55
57
58
46
22.25
4.72
50
37
38
40
42
59
61
63
64
51
24.75
4.97
55
42
43
45
46
65
66
68
69
56
27.25
5.22
60
46
47
49
51
70
72
74
75
61
29.75
5.45
65
50
52
54
56
75
77
79
81
66
32.25
5.68
70
55
56
58
60
81
83
85
86
71
34.75
5.89
75
59
61
63
65
86
88
90
92
76
37.25
6.10
80
64
65
68
70
91
93
96
97
81
39.75
6.30
85
68
70
72
74
97
99
101
103
86
42.25
6.50
90
73
74
77
79
102
104
107
108
91
44.75
6.69
95
77
79
82
84
107
109
112
114
96
47.25
6.87
100
82
84
86
88
113
115
117
119
101
49.75
7.05
c
⃝2000 by Chapman & Hall/CRC

14.6
THE SIGN TEST
Assumptions: Let X1, X2, . . . , Xn be a random sample from a continuous
distribution.
Hypothesis test:
H0: ˜µ = ˜µ0
Ha: ˜µ > ˜µ0,
˜µ < ˜µ0,
˜µ ̸= ˜µ0
TS: Y = the number of Xi’s greater than ˜µ0.
Under the null hypothesis, Y has a binomial distribution with pa-
rameters n and p = .5.
RR: Y ≥c1,
Y ≤c2,
Y ≥c or Y ≤n −c
The critical values c1, c2, and c are obtained from the binomial
distribution with parameters n and p = .5 to yield the desired
signiﬁcance level α. (See the table on page 366.)
Sample values equal to ˜µ0 are excluded from the analysis and the
sample size is reduced accordingly.
The normal approximation: When n ≥10 and np ≥5 the binomial distribu-
tion can be approximated by a normal distribution with
µY = np
and
σ2
Y = np(1 −p)
(14.14)
The random variable
Z = Y −µY
σY
=
Y −np

np(1 −p)
(14.15)
has approximately a standard normal distribution when H0 is true and n ≥10
and np ≥5.
14.6.1
Table of critical values for the sign test
Let X1, X2, . . . , Xn be a random sample from a continuous distribution with
hypothesized median ˜µ0. The test statistic is Y , the number of Xi’s greater
than ˜µ0. If the null hypothesis is true, the probability Xi is greater than the
median is 1/2. The probability distribution for Y is given by the binomial
probability function
Prob [Y = y] = f(y) =
n
y
 1
2
n
.
(14.16)
The following table contains critical values k such that
Prob [Y ≤k] =
k

y=0
n
y
 1
2
n
< α
2 .
(14.17)
c
⃝2000 by Chapman & Hall/CRC

For a one-tailed test with signiﬁcance level α, enter the table in the column
headed by 2α.
For larger values of n, approximate critical values may be found using equation
(14.15).
k =
1
np +

np(1 −p)zα/2
2
(14.18)
where zα is the critical value for the normal distribution.
Critical values for the sign test
probability
probability
n
.01
.025
.05
.10
n
.01
.025
.05
.10
1
0
0
0
0
26
6
6
7
8
2
0
0
0
0
27
6
7
7
8
3
0
0
0
0
28
6
7
8
9
4
0
0
0
0
29
7
8
8
9
5
0
0
0
0
30
7
8
9
10
6
0
0
0
0
31
7
8
9
10
7
0
0
0
0
32
8
9
9
10
8
0
0
0
1
33
8
9
10
11
9
0
0
1
1
34
9
10
10
11
10
0
1
1
1
35
9
10
11
12
11
0
1
1
2
36
9
10
11
12
12
1
1
2
2
37
10
11
12
13
13
1
2
2
3
38
10
11
12
13
14
1
2
2
3
39
11
12
12
13
15
2
2
3
3
40
11
12
13
14
16
2
3
3
4
41
11
12
13
14
17
2
3
4
4
42
12
13
14
15
18
3
3
4
5
43
12
13
14
15
19
3
4
4
5
44
13
14
15
16
20
3
4
5
5
45
13
14
15
16
21
4
4
5
6
46
13
14
15
16
22
4
5
5
6
47
14
15
16
17
23
4
5
6
7
48
14
15
16
17
24
5
6
6
7
49
15
16
17
18
25
5
6
7
7
50
15
16
17
18
14.7
SPEARMAN’S RANK CORRELATION COEFFICIENT
Suppose there are n pairs of observations from continuous distributions. Rank
the observations in the two samples separately from smallest to largest. Equal
observations are assigned the mean rank for their positions. Let ui be the
rank of the ith observation in the ﬁrst sample and let vi be the rank of the ith
observation in the second sample. Spearman’s rank correlation coeﬃcient, rS,
is a measure of the correlation between ranks, calculated by using the ranks in
place of the actual observations in the formula for the correlation coeﬃcient r.
c
⃝2000 by Chapman & Hall/CRC

rS =
SSuv
√SSuu SSvv
=
n
n
i=1
uivi −
 n
i=1
ui
  n
i=1
vi

F
G
G
H
%
n
n
i=1
u2
i −
 n
i=1
ui
2& %
n
n
i=1
v2
i −
 n
i=1
vi
2&
= 1 −
6
n
i=1
d2
i
n(n2 −1)
where di = ui −vi.
(14.19)
The shortcut formula for rS that only uses the diﬀerences {di} is not exact
when there are tied measurements.
The approximation is good when the
number of ties is small in comparison to n.
Hypothesis test:
H0: ρS = 0 (no population correlation between ranks)
Ha: ρS > 0,
ρS < 0,
ρS ̸= 0
TS: rS
RR: rS ≥rS,α,
rS ≤−rS,α,
|rS| ≥rS,α/2
where rS,α is a critical value for Spearman’s rank correlation coef-
ﬁcient test (see page 367).
The normal approximation: When H0 is true rS has approximately a normal
distribution with
µrS = 0
and
σ2
rS =
1
n −1.
(14.20)
The random variable
Z = rS −µrS
σrS
=
rS −0
1/√n −1 = rS
√
n −1
(14.21)
has approximately a standard normal distribution as n increases.
14.7.1
Tables for Spearman’s rank correlation coeﬃcient
Spearman’s coeﬃcient of rank correlation, rS, measures the correspondence
between two rankings; see equation (14.19). The table below gives critical
values for rS assuming the samples are independent; their derivation comes
from the subsequent table.
c
⃝2000 by Chapman & Hall/CRC

Critical values of Spearman’s rank correlation coeﬃcient
n
α = 0.10
α = 0.05
α = 0.01
α = 0.001
4
0.8000
0.8000
−
−
5
0.7000
0.8000
0.9000
−
6
0.6000
0.7714
0.8857
−
7
0.5357
0.6786
0.8571
0.9643
8
0.5000
0.6190
0.8095
0.9286
9
0.4667
0.5833
0.7667
0.9000
10
0.4424
0.5515
0.7333
0.8667
11
0.4182
0.5273
0.7000
0.8364
12
0.3986
0.4965
0.6713
0.8182
13
0.3791
0.4780
0.6429
0.7912
14
0.3626
0.4593
0.6220
0.7670
15
0.3500
0.4429
0.6000
0.7464
20
0.2977
0.3789
0.5203
0.6586
25
0.2646
0.3362
0.4654
0.5962
30
0.2400
0.3059
0.4251
0.5479
Let  m represents the mean value of the sum of squares. Then the following
tables give the probability that  d2 ≥S for S ≥ m, or that  d2 ≤S for
S ≤ m. The tables for n = 9 and n = 10 can be completed by symmetry.
The values in the next table create the critical values in the last table. For
example, taking n = 9 we note that (a) S = 26 (corresponding to a Spearman
rank correlation coeﬃcient of 1 −
26
120 ≈0.7833) has a probability of 0.0086;
and (b) S = 28 (corresponding a Spearman rank correlation coeﬃcient of
1 −
28
120 ≈0.7667) has a probability of 0.0107. Hence, the critical value for
n = 9 and α = 0.01, the least value of the coeﬃcient whose probability is less
that 0.01, is 0.7667.
c
⃝2000 by Chapman & Hall/CRC

Exact values for Spearman’s rank correlation coeﬃcient
n = 2
3
4
5
6
7
8
9
10
S
 m = 1
4
10
20
35
56
84
120
165
0
0.5000 0.1667 0.0417 0.0083 0.0014 0.0002 0.0000 0.0000 0.0000
2
0.5000 0.5000 0.1667 0.0417 0.0083 0.0014 0.0002 0.0000 0.0000
4
0.5000 0.2083 0.0667 0.0167 0.0034 0.0006 0.0001 0.0000
6
0.5000 0.3750 0.1167 0.0292 0.0062 0.0011 0.0002 0.0000
8
0.1667 0.4583 0.1750 0.0514 0.0119 0.0023 0.0004 0.0001
10
0.5417 0.2250 0.0681 0.0171 0.0036 0.0007 0.0001
12
0.4583 0.2583 0.0875 0.0240 0.0054 0.0010 0.0002
14
0.3750 0.3417 0.1208 0.0331 0.0077 0.0015 0.0003
16
0.2083 0.3917 0.1486 0.0440 0.0109 0.0023 0.0004
18
0.1667 0.4750 0.1778 0.0548 0.0140 0.0030 0.0006
20
0.0417 0.5250 0.2097 0.0694 0.0184 0.0041 0.0008
22
0.4750 0.2486 0.0833 0.0229 0.0054 0.0011
24
0.3917 0.2819 0.1000 0.0288 0.0069 0.0014
26
0.3417 0.3292 0.1179 0.0347 0.0086 0.0019
28
0.2583 0.3569 0.1333 0.0415 0.0107 0.0024
30
0.2250 0.4014 0.1512 0.0481 0.0127 0.0029
32
0.1750 0.4597 0.1768 0.0575 0.0156 0.0036
34
0.1167 0.5000 0.1978 0.0661 0.0184 0.0044
36
0.0667 0.5000 0.2222 0.0756 0.0216 0.0053
38
0.0417 0.5000 0.2488 0.0855 0.0252 0.0063
40
0.0083 0.4597 0.2780 0.0983 0.0294 0.0075
42
0.4014 0.2974 0.1081 0.0333 0.0087
44
0.3569 0.3308 0.1215 0.0380 0.0101
46
0.3292 0.3565 0.1337 0.0429 0.0117
48
0.2819 0.3913 0.1496 0.0484 0.0134
50
0.2486 0.4198 0.1634 0.0540 0.0153
52
0.2097 0.4532 0.1799 0.0603 0.0173
54
0.1778 0.4817 0.1947 0.0664 0.0195
56
0.1486 0.5183 0.2139 0.0738 0.0219
58
0.1208 0.4817 0.2309 0.0809 0.0245
60
0.0875 0.4532 0.2504 0.0888 0.0272
62
0.0681 0.4198 0.2682 0.0969 0.0302
64
0.0514 0.3913 0.2911 0.1063 0.0334
66
0.0292 0.3565 0.3095 0.1149 0.0367
68
0.0167 0.3308 0.3323 0.1250 0.0403
70
0.0083 0.2974 0.3517 0.1348 0.0441
72
0.0014 0.2780 0.3760 0.1456 0.0481
74
0.2488 0.3965 0.1563 0.0524
76
0.2222 0.4201 0.1681 0.0569
78
0.1978 0.4410 0.1793 0.0616
80
0.1768 0.4674 0.1927 0.0667
c
⃝2000 by Chapman & Hall/CRC

Exact values for Spearman’s rank correlation coeﬃcient
n = 2
3
4
5
6
7
8
9
10
S
 m = 1
4
10
20
35
56
84
120
165
80
0.1768 0.4674 0.1927 0.0667
82
0.1512 0.4884 0.2050 0.0720
84
0.1333 0.5116 0.2183 0.0774
86
0.1179 0.4884 0.2315 0.0831
88
0.1000 0.4674 0.2467 0.0893
90
0.0833 0.4410 0.2603 0.0956
92
0.0694 0.4201 0.2759 0.1022
94
0.0548 0.3965 0.2905 0.1091
96
0.0440 0.3760 0.3067 0.1163
98
0.0331 0.3517 0.3218 0.1237
100
0.0240 0.3323 0.3389 0.1316
102
0.0171 0.3095 0.3540 0.1394
104
0.0119 0.2911 0.3718 0.1478
106
0.0062 0.2682 0.3878 0.1564
108
0.0034 0.2504 0.4050 0.1652
110
0.0014 0.2309 0.4216 0.1744
112
0.0002 0.2139 0.4400 0.1839
114
0.1947 0.4558 0.1935
116
0.1799 0.4742 0.2035
118
0.1634 0.4908 0.2135
120
0.1496 0.5092 0.2241
122
0.1337 0.4908 0.2349
124
0.1215 0.4742 0.2459
126
0.1081 0.4558 0.2567
128
0.0983 0.4400 0.2683
130
0.0855 0.4216 0.2801
132
0.0756 0.4050 0.2918
134
0.0661 0.3878 0.3037
136
0.0575 0.3718 0.3161
138
0.0481 0.3540 0.3284
140
0.0415 0.3389 0.3410
142
0.0347 0.3218 0.3536
144
0.0288 0.3067 0.3665
146
0.0229 0.2905 0.3795
148
0.0184 0.2759 0.3925
150
0.0140 0.2603 0.4056
152
0.0109 0.2467 0.4191
154
0.0077 0.2315 0.4326
156
0.0054 0.2183 0.4458
158
0.0036 0.2050 0.4592
160
0.0023 0.1927 0.4730
c
⃝2000 by Chapman & Hall/CRC

14.8
WILCOXON MATCHED-PAIRS SIGNED-RANKS TEST
Assume we have a matched set of n observations {xi, yi}. Let di denote the
diﬀerences di = xi −yi.
Hypothesis test:
H0: there is no diﬀerence in the distribution of the xi’s and the yi’s
Ha: there is a diﬀerence
Rank all of the di’s without regard to sign: the least value of |di|
gets rank 1, the next largest value gets rank 2, etc. After deter-
mining the ranking, aﬃx the signs of the diﬀerences to each rank.
TS: T = the smaller sum of the like-signed ranks.
RR: T ≥c
where c is found from the table on page 372.
Example 14.72:
Suppose n = 10 values are as shown in the ﬁrst two columns of the
following table:
xi
yi
di = xi −yi
rank of |di|
signed rank of |di|
9
8
1
2
2
2
2
0
–
–
1
3
−2
4.5
−4.5
4
2
2
4.5
4.5
6
3
3
7
7
4
0
4
9
9
7
4
3
7
7
8
5
3
7
7
5
4
1
2
2
1
0
1
2
2
 R+ = 40.5
 R−= −4.5
The subsequent columns show the diﬀerences, the ranks (note how ties are handled),
and the signed ranks. The smaller of the two sums is T = 4.5. From the following table
(with n = 10) we conclude that there is evidence of a diﬀerence in distributions at the
.005 signiﬁcance level.
See D. J. Sheskin, Handbook of Parametric and Nonparametric Statistical
Procedures, CRC Press LLC, Boca Raton, FL, 1997, pages 291–301, 681.
c
⃝2000 by Chapman & Hall/CRC

Critical values for the Wilcoxon signed-ranks test
and the matched-pairs signed-ranks test
One sided Two sided
n = 5
6
7
8
9
10
11
12
13
14
α = .05
α = .10
0
2
3
5
8
10
13
17
21
25
α = .025
α = .05
0
2
3
5
8
10
13
17
21
α = .01
α = .02
0
1
3
5
7
9
12
15
α = .005
α = .01
0
1
3
5
7
9
12
One sided Two sided n = 15
16
17
18
19
20
21
22
23
24
α = .05
α = .10
30
35
41
47
53
60
67
75
83
91
α = .025
α = .05
25
29
34
40
46
52
58
65
73
81
α = .01
α = .02
19
23
27
32
37
43
49
55
62
69
α = .005
α = .01
15
19
23
27
32
37
42
48
54
61
One sided Two sided n = 25
26
27
28
29
30
31
32
33
34
α = .05
α = .10
100
110 119 130 140 151 163 175 187 200
α = .025
α = .05
89
98
107 116 126 137 147 159 170 182
α = .01
α = .02
76
84
92
101 110 120 130 140 151 162
α = .005
α = .01
68
75
83
91
100 109 118 128 138 148
14.9
WILCOXON RANK–SUM (MANN–WHITNEY) TEST
Assumptions: Let X1, X2, . . . , Xm and Y1, Y2, . . . , Yn (with m ≤n) be inde-
pendent random samples from continuous distributions.
Hypothesis test:
H0: ˜µ1 −˜µ2 = ∆0
Ha: ˜µ1 −˜µ2 > ∆0,
˜µ1 −˜µ2 < ∆0,
˜µ1 −˜µ2 ̸= ∆0
Subtract ∆0 from each Xi. Combine the (Xi −∆0)’s and the Yj’s
into one sample and rank all of the observations. Equal diﬀerences
are assigned the mean rank for their positions.
TS: W =
m

i=1
Ri
where Ri is the rank of (Xi −∆0) in the combined sample.
RR: W ≥c1,
W ≤c2,
W ≥c or W ≤m(m + n + 1) −c
where c1, c2, and c are critical values for the Wilcoxon rank–sum
statistic such that Prob [W ≥c1] ≈α, Prob [W ≤c2] ≈α, and
Prob [W ≥c] ≈α/2. (In practice, we convert from W to U via
equation (14.24) and look up U values.)
The normal approximation: When both m and n are greater than 8, W has
approximately a normal distribution with
µW = m(m + n + 1)
2
and
σ2
W = mn(m + n + 1)
12
.
(14.22)
c
⃝2000 by Chapman & Hall/CRC

The random variable
Z = W −µW
σW
(14.23)
has approximately a standard normal distribution.
The Mann–Whitney U statistic: The rank–sum test may also be based on the
test statistic
U = m(m + 2n + 1)
2
−W.
(14.24)
When both m and n are greater than 8, U has approximately a normal dis-
tribution with
µU = mn
2
and
σ2
U = mn(m + n + 1)
12
.
(14.25)
The random variable
Z = U −µU
σU
(14.26)
has approximately a standard normal distribution.
Note that there are two tests commonly called the Mann–Whitney U test: one
developed by Mann and Whitney and one developed by Wilcoxon. Although
they employ diﬀerent equations and diﬀerent tables, the two versions yield
comparable results.
Example 14.73:
The Pennsylvania State Police theorize that cars travel faster during
the evening rush hour versus the morning rush hour.
Randomly selected cars were
selected during each rush hour and there speeds were computed using radar. The data
is given in the table below.
Morning:
68
65
80
61
64
64
63
73
75
71
Evening:
70
70
71
72
72
71
75
74
81
72
74
71
Use the Mann–Whitney U test to determine if there is any evidence to suggest the
median speeds are diﬀerent. Use α = .05.
Solution:
(S1) Computations:
m = 10, n = 12, W = 88, U = 87
(S2) Using the tables, the critical value for a two sided test with α = .05 is 29.
(S3) The value of the test statistic is not in the rejection region. There is no evidence
to suggest the median speeds are diﬀerent.
14.9.1
Tables for Wilcoxon (Mann–Whitney) U statistic
Given two sample of sizes m and n (with m ≤n) the Mann–Whitney U-
statistic (see equation (14.24)) is used to test the hypothesis that the two
c
⃝2000 by Chapman & Hall/CRC

samples are from populations with the same median. Rank all of the obser-
vations in ascending order of magnitude. Let W be the sum of the ranks
assigned to the sample of size m. Then U is deﬁned as
U = m(m + 2n + 1)
2
−W
(14.27)
The following tables present cumulative probability and are used to determine
exact probabilities associated with this test statistic. If the null hypothesis is
true, the body of the tables contains probabilities such that Prob [U ≤u].
Only small values of u are shown in the tables since the probability distribu-
tion for U is symmetric. For example, for n = 3 and m = 2 the probability
distribution of U values is:
Prob [U = 0] = Prob [U = 1] = Prob [U = 5] = Prob [U = 6] = 0.1
Prob [U = 2] = Prob [U = 3] = Prob [U = 4] = 0.2
so that the distribution function is:
Prob [U ≤0] = 0.1,
Prob [U ≤1] = 0.2,
Prob [U ≤2] = 0.4,
Prob [U ≤3] = 0.6,
Prob [U ≤4] = 0.8,
Prob [U ≤5] = 0.9,
Prob [U ≤6] = 1
Example 14.74:
Consider the two samples {13, 9} (m = 2) and {12, 16, 14} (n = 3).
Arrange the combined samples in rank order and box the values from the ﬁrst sample:
rank
9
1
12
2
13
3
14
4
16
5
(14.28)
Compute the U statistic:
(a) W = 1 + 3 = 4
(b) U = 2(2 + 2 · 3 + 1
2
−4 = 5
Using the tables below (and the comment above): Prob [U ≤5] = .9. There is little
evidence to suggest the medians are diﬀerent.
n = 3
u
m = 1
2
3
0
0.250
0.100
0.050
1
0.500
0.200
0.100
2
0.750
0.400
0.200
3
0.600
0.350
4
0.500
5
0.650
n = 4
u
m = 1
2
3
4
0
0.200
0.067
0.029
0.014
1
0.400
0.133
0.057
0.029
2
0.600
0.267
0.114
0.057
3
0.400
0.200
0.100
4
0.600
0.314
0.171
5
0.429
0.243
6
0.571
0.343
7
0.443
8
0.557
c
⃝2000 by Chapman & Hall/CRC

n = 5
u
m = 1
2
3
4
5
0
0.167
0.048
0.018
0.008
0.004
1
0.333
0.095
0.036
0.016
0.008
2
0.500
0.190
0.071
0.032
0.016
3
0.667
0.286
0.125
0.056
0.028
4
0.429
0.196
0.095
0.048
5
0.571
0.286
0.143
0.075
6
0.393
0.206
0.111
7
0.500
0.278
0.155
8
0.607
0.365
0.210
9
0.452
0.274
10
0.548
0.345
11
0.421
12
0.500
13
0.579
n = 6
u
m = 1
2
3
4
5
6
0
0.143
0.036
0.012
0.005
0.002
0.001
1
0.286
0.071
0.024
0.010
0.004
0.002
2
0.429
0.143
0.048
0.019
0.009
0.004
3
0.571
0.214
0.083
0.033
0.015
0.008
4
0.321
0.131
0.057
0.026
0.013
5
0.429
0.190
0.086
0.041
0.021
6
0.571
0.274
0.129
0.063
0.032
7
0.357
0.176
0.089
0.047
8
0.452
0.238
0.123
0.066
9
0.548
0.305
0.165
0.090
10
0.381
0.214
0.120
11
0.457
0.268
0.155
12
0.543
0.331
0.197
13
0.396
0.242
14
0.465
0.294
15
0.535
0.350
16
0.409
17
0.469
18
0.531
c
⃝2000 by Chapman & Hall/CRC

n = 7
u
m = 1
2
3
4
5
6
7
0
0.125
0.028
0.008
0.003
0.001
0.001
0.000
1
0.250
0.056
0.017
0.006
0.003
0.001
0.001
2
0.375
0.111
0.033
0.012
0.005
0.002
0.001
3
0.500
0.167
0.058
0.021
0.009
0.004
0.002
4
0.625
0.250
0.092
0.036
0.015
0.007
0.003
5
0.333
0.133
0.055
0.024
0.011
0.006
6
0.444
0.192
0.082
0.037
0.017
0.009
7
0.556
0.258
0.115
0.053
0.026
0.013
8
0.333
0.158
0.074
0.037
0.019
9
0.417
0.206
0.101
0.051
0.027
10
0.500
0.264
0.134
0.069
0.036
11
0.583
0.324
0.172
0.090
0.049
12
0.394
0.216
0.117
0.064
13
0.464
0.265
0.147
0.082
14
0.536
0.319
0.183
0.104
15
0.378
0.223
0.130
16
0.438
0.267
0.159
17
0.500
0.314
0.191
18
0.562
0.365
0.228
19
0.418
0.267
20
0.473
0.310
21
0.527
0.355
22
0.402
23
0.451
24
0.500
25
0.549
c
⃝2000 by Chapman & Hall/CRC

n = 8
u
m = 1
2
3
4
5
6
7
8
0
0.111
0.022
0.006
0.002
0.001
0.000
0.000
0.000
1
0.222
0.044
0.012
0.004
0.002
0.001
0.000
0.000
2
0.333
0.089
0.024
0.008
0.003
0.001
0.001
0.000
3
0.444
0.133
0.042
0.014
0.005
0.002
0.001
0.001
4
0.556
0.200
0.067
0.024
0.009
0.004
0.002
0.001
5
0.267
0.097
0.036
0.015
0.006
0.003
0.001
6
0.356
0.139
0.055
0.023
0.010
0.005
0.002
7
0.444
0.188
0.077
0.033
0.015
0.007
0.003
8
0.556
0.248
0.107
0.047
0.021
0.010
0.005
9
0.315
0.141
0.064
0.030
0.014
0.007
10
0.388
0.184
0.085
0.041
0.020
0.010
11
0.461
0.230
0.111
0.054
0.027
0.014
12
0.539
0.285
0.142
0.071
0.036
0.019
13
0.341
0.177
0.091
0.047
0.025
14
0.404
0.218
0.114
0.060
0.032
15
0.467
0.262
0.141
0.076
0.041
16
0.533
0.311
0.172
0.095
0.052
17
0.362
0.207
0.116
0.065
18
0.416
0.245
0.140
0.080
19
0.472
0.286
0.168
0.097
20
0.528
0.331
0.198
0.117
21
0.377
0.232
0.139
22
0.426
0.268
0.164
23
0.475
0.306
0.191
24
0.525
0.347
0.221
25
0.389
0.253
26
0.433
0.287
27
0.478
0.323
28
0.522
0.360
29
0.399
30
0.439
31
0.480
32
0.520
14.9.2
Critical values for Wilcoxon (Mann–Whitney) statistic
The following tables give critical values for U for signiﬁcance levels of 0.00005,
0.0001, 0.005, 0.01, 0.025, 0.05, and 0.10 for a one-tailed test. For a two-tailed
test, the signiﬁcance levels are doubled. If an observed U is equal to or less
than the tabular value, the null hypothesis may be rejected at the level of
signiﬁcance indicated at the head of the table.
c
⃝2000 by Chapman & Hall/CRC

Critical values of U in the Mann–Whitney test
Critical values for the α = 0.10 level of signiﬁcance
m
n = 1 2
3
4
5
6
7
8
9 10 11 12 13
14
15
16
17
18
19
20
1
2
0
0
1
1
1
2
2
3
3
4
4
5
5
5
6
6
7
7
3
0
1
1
2
3
4
5
5
6
7
8
9
10
10
11
12
13
14
15
4
0
1
3
4
5
6
7
9 10 11 12 13
15
16
17
18
20
21
22
5
1
2
4
5
7
8 10 12 13 15 17 18
20
22
23
25
27
28
30
6
1
3
5
7
9 11 13 15 17 19 21 23
25
27
29
31
34
36
38
7
1
4
6
8 11 13 16 18 21 23 26 28
31
33
36
38
41
43
46
8
2
5
7 10 13 16 19 22 24 27 30 33
36
39
42
45
48
51
54
9
2
5
9 12 15 18 22 25 28 31 35 38
41
45
48
52
55
58
62
10
3
6 10 13 17 21 24 28 32 36 39 43
47
51
54
58
62
66
70
11
3
7 11 15 19 23 27 31 36 40 44 48
52
57
61
65
69
73
78
12
4
8 12 17 21 26 30 35 39 44 49 53
58
63
67
72
77
81
86
13
4
9 13 18 23 28 33 38 43 48 53 58
63
68
74
79
84
89
94
14
5 10 15 20 25 31 36 41 47 52 58 63
69
74
80
85
91
97 102
15
5 10 16 22 27 33 39 45 51 57 63 68
74
80
86
92
98 104 110
16
5 11 17 23 29 36 42 48 54 61 67 74
80
86
93
99 106 112 119
17
6 12 18 25 31 38 45 52 58 65 72 79
85
92
99 106 113 120 127
8
6 13 20 27 34 41 48 55 62 69 77 84
91
98 106 113 120 128 135
19
7 14 21 28 36 43 51 58 66 73 81 89
97 104 112 120 128 135 143
20
7 15 22 30 38 46 54 62 70 78 86 94 102 110 119 127 135 143 151
Critical values of U in the Mann–Whitney test
Critical values for the α = 0.05 level of signiﬁcance
m
n = 1 2
3
4
5
6
7
8
9 10 11 12 13 14
15
16
17
18
19
20
1
2
0
0
0
1
1
1
1
2
2
3
3
3
3
4
4
4
3
0
0
1
2
2
3
4
4
5
5
6
7
7
8
9
9
10
11
4
0
1
2
3
4
5
6
7
8
9 10 11
12
14
15
16
17
18
5
0
1
2
4
5
6
8
9 11 12 13 15 16
18
19
20
22
23
25
6
0
2
3
5
7
8 10 12 14 16 17 19 21
23
25
26
28
30
32
7
0
2
4
6
8 11 13 15 17 19 21 24 26
28
30
33
35
37
39
8
1
3
5
8 10 13 15 18 20 23 26 28 31
33
36
39
41
44
47
9
1
4
6
9 12 15 18 21 24 27 30 33 36
39
42
45
48
51
54
10
1
4
7 11 14 17 20 24 27 31 34 37 41
44
48
51
55
58
62
11
1
5
8 12 16 19 23 27 31 34 38 42 46
50
54
57
61
65
69
12
2
5
9 13 17 21 26 30 34 38 42 47 51
55
60
64
68
72
77
13
2
6 10 15 19 24 28 33 37 42 47 51 56
61
65
70
75
80
84
14
3
7 11 16 21 26 31 36 41 46 51 56 61
66
71
77
82
87
92
15
3
7 12 18 23 28 33 39 44 50 55 61 66
72
77
83
88
94 100
16
3
8 14 19 25 30 36 42 48 54 60 65 71
77
83
89
95 101 107
17
3
9 15 20 26 33 39 45 51 57 64 70 77
83
89
96 102 109 115
18
4
9 16 22 28 35 41 48 55 61 68 75 82
88
95 102 109 116 123
19
4 10 17 23 30 37 44 51 58 65 72 80 87
94 101 109 116 123 130
20
4 11 18 25 32 39 47 54 62 69 77 84 92 100 107 115 123 130 138
c
⃝2000 by Chapman & Hall/CRC

Critical values of U in the Mann–Whitney test
Critical values for the α = 0.025 level of signiﬁcance
m
n = 1 2 3
4
5
6
7
8
9 10 11 12 13 14 15 16
17
18
19
20
1
2
0
0
0
0
1
1
1
1
1
2
2
2
2
3
0
1
1
2
2
3
3
4
4
5
5
6
6
7
7
8
4
0
1
2
3
4
4
5
6
7
8
9 10 11
11
12
13
14
5
0
1
2
3
5
6
7
8
9 11 12 13 14 15
17
18
19
20
6
1
2
3
5
6
8 10 11 13 14 16 17 19 21
22
24
25
27
7
1
3
5
6
8 10 12 14 16 18 20 22 24 26
28
30
32
34
8
0 2
4
6
8 10 13 15 17 19 22 24 26 29 31
34
36
38
41
9
0 2
4
7 10 12 15 17 20 23 26 28 31 34 37
39
42
45
48
10
0 3
5
8 11 14 17 20 23 26 29 33 36 39 42
45
48
52
55
11
0 3
6
9 13 16 19 23 26 30 33 37 40 44 47
51
55
58
62
12
1 4
7 11 14 18 22 26 29 33 37 41 45 49 53
57
61
65
69
13
1 4
8 12 16 20 24 28 33 37 41 45 50 54 59
63
67
72
76
14
1 5
9 13 17 22 26 31 36 40 45 50 55 59 64
69
74
78
83
15
1 5 10 14 19 24 29 34 39 44 49 54 59 64 70
75
80
85
90
16
1 6 11 15 21 26 31 37 42 47 53 59 64 70 75
81
86
92
98
17
2 6 11 17 22 28 34 39 45 51 57 63 69 75 81
87
93
99 105
18
2 7 12 18 24 30 36 42 48 55 61 67 74 80 86
93
99 106 112
19
2 7 13 19 25 32 38 45 52 58 65 72 78 85 92
99 106 113 119
20
2 8 14 20 27 34 41 48 55 62 69 76 83 90 98 105 112 119 127
Critical values of U in the Mann–Whitney test
Critical values for the α = 0.01 level of signiﬁcance
m
n = 1 2 3
4
5
6
7
8
9 10 11 12 13 14 15 16 17
18
19
20
1
2
0
0
0
0
0
0
1
1
3
0
0
1
1
1
2
2
2
3
3
4
4
4
5
4
0
1
1
2
3
3
4
5
5
6
7
7
8
9
9
10
5
0
1
2
3
4
5
6
7
8
9 10 11 12 13
14
15
16
6
1
2
3
4
6
7
8
9 11 12 13 15 16 18
19
20
22
7
0
1
3
4
6
7
9 11 12 14 16 17 19 21 23
24
26
28
8
0
2
4
6
7
9 11 13 15 17 20 22 24 26 28
30
32
34
9
1
3
5
7
9 11 14 16 18 21 23 26 28 31 33
36
38
40
10
1
3
6
8 11 13 16 19 22 24 27 30 33 36 38
41
44
47
11
1
4
7
9 12 15 18 22 25 28 31 34 37 41 44
47
50
53
12
2
5
8 11 14 17 21 24 28 31 35 38 42 46 49
53
56
60
13
0 2
5
9 12 16 20 23 27 31 35 39 43 47 51 55
59
63
67
14
0 2
6 10 13 17 22 26 30 34 38 43 47 51 56 60
65
69
73
15
0 3
7 11 15 19 24 28 33 37 42 47 51 56 61 66
70
75
80
16
0 3
7 12 16 21 26 31 36 41 46 51 56 61 66 71
76
82
87
17
0 4
8 13 18 23 28 33 38 44 49 55 60 66 71 77
82
88
93
18
0 4
9 14 19 24 30 36 41 47 53 59 65 70 76 82
88
94 100
19
1 4
9 15 20 26 32 38 44 50 56 63 69 75 82 88
94 101 107
20
1 5 10 16 22 28 34 40 47 53 60 67 73 80 87 93 100 107 114
c
⃝2000 by Chapman & Hall/CRC

Critical values of U in the Mann–Whitney test
Critical values for the α = 0.005 level of signiﬁcance
m
n = 1 2 3 4
5
6
7
8
9 10 11 12 13 14 15 16 17 18 19
20
1
2
0
0
3
0
0
0
1
1
1
2
2
2
2
3
3
4
0
0
1
1
2
2
3
3
4
5
5
6
6
7
8
5
0
1
1
2
3
4
5
6
7
7
8
9 10 11 12
13
6
0
1
2
3
4
5
6
7
9 10 11 12 13 15 16 17
18
7
0
1
3
4
6
7
9 10 12 13 15 16 18 19 21 22
24
8
1
2
4
6
7
9 11 13 15 17 18 20 22 24 26 28
30
9
0 1
3
5
7
9 11 13 16 18 20 22 24 27 29 31 33
36
10
0 2
4
6
9 11 13 16 18 21 24 26 29 31 34 37 39
42
11
0 2
5
7 10 13 16 18 21 24 27 30 33 36 39 42 45
48
12
1 3
6
9 12 15 18 21 24 27 31 34 37 41 44 47 51
54
13
1 3
7 10 13 17 20 24 27 31 34 38 42 45 49 53 57
60
14
1 4
7 11 15 18 22 26 30 34 38 42 46 50 54 58 63
67
15
2 5
8 12 16 20 24 29 33 37 42 46 51 55 60 64 69
73
16
2 5
9 13 18 22 27 31 36 41 45 50 55 60 65 70 74
79
17
2 6 10 15 19 24 29 34 39 44 49 54 60 65 70 75 81
86
18
2 6 11 16 21 26 31 37 42 47 53 58 64 70 75 81 87
92
19
0 3 7 12 17 22 28 33 39 45 51 57 63 69 74 81 87 93
99
20
0 3 8 13 18 24 30 36 42 48 54 60 67 73 79 86 92 99 105
Critical values of U in the Mann–Whitney test
Critical values for the α = 0.001 level of signiﬁcance
m
n = 1 2 3 4 5
6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
1
2
3
0
0
0
0
4
0
0
0
1
1
1
2
2
3
3
3
5
0
1
1
2
2
3
3
4
5
5
6
7
7
6
0
1
2
3
4
4
5
6
7
8
9 10 11 12
7
0
1
2
3
5
6
7
8
9 10 11 13 14 15 16
8
0
1
2
4
5
6
8
9 11 12 14 15 17 18 20 21
9
1
2
3
5
7
8 10 12 14 15 17 19 21 23 25 26
10
0 1
3
5
6
8 10 12 14 17 19 21 23 25 27 29 32
11
0 2
4
6
8 10 12 15 17 20 22 24 27 29 32 34 37
12
0 2
4
7
9 12 14 17 20 23 25 28 31 34 37 40 42
13
1 3
5
8 11 14 17 20 23 26 29 32 35 38 42 45 48
14
1 3
6
9 12 15 19 22 25 29 32 36 39 43 46 50 54
15
1 4
7 10 14 17 21 24 28 32 36 40 43 47 51 55 59
16
2 5
8 11 15 19 23 27 31 35 39 43 48 52 56 60 65
17
0 2 5
9 13 17 21 25 29 34 38 43 47 52 57 61 66 70
18
0 3 6 10 14 18 23 27 32 37 42 46 51 56 61 66 71 76
19
0 3 7 11 15 20 25 29 34 40 45 50 55 60 66 71 77 82
20
0 3 7 12 16 21 26 32 37 42 48 54 59 65 70 76 82 88
c
⃝2000 by Chapman & Hall/CRC

Critical values of U in the Mann–Whitney test
Critical values for the α = 0.0005 level of signiﬁcance
m
n = 1 2 3 4 5 6
7
8
9 10 11 12 13 14 15 16 17 18 19 20
1
2
3
4
0
0
0
1
1
1
2
2
5
0
0
1
1
2
2
3
3
4
4
5
5
6
0
1
2
2
3
4
5
5
6
7
8
8
9
7
0
1
2
3
4
5
6
7
8
9 10 11 13 14
8
0
1
2
4
5
6
7
9 10 11 13 14 15 17 18
9
0 1
2
4
5
7
8 10 11 13 15 16 18 20 21 23
10
0 2
3
5
7
8 10 12 14 16 18 20 22 24 26 28
11
1 2
4
6
8 10 12 15 17 19 21 24 26 28 31 33
12
1 3
5
7 10 12 15 17 20 22 25 27 30 33 35 38
13
0 2 4
6
9 11 14 17 20 23 25 28 31 34 37 40 43
14
0 2 5
7 10 13 16 19 22 25 29 32 35 39 42 45 49
15
0 3 5
8 11 15 18 21 25 28 32 36 39 43 46 50 54
16
1 3 6
9 13 16 20 24 27 31 35 39 43 47 51 55 59
17
1 4 7 10 14 18 22 26 30 34 39 43 47 51 56 60 65
18
1 4 8 11 15 20 24 28 33 37 42 46 51 56 61 65 70
19
2 5 8 13 17 21 26 31 35 40 45 50 55 60 65 70 76
20
2 5 9 14 18 23 28 33 38 43 49 54 59 65 70 76 81
14.10
WILCOXON SIGNED-RANK TEST
Assumptions: Let X1, X2, . . . , Xn be a random sample from a continuous
symmetric distribution.
Hypothesis test:
H0: ˜µ = ˜µ0
Ha: ˜µ > ˜µ0,
˜µ < ˜µ0,
˜µ ̸= ˜µ0
Rank the absolute diﬀerences |X1 −˜µ0|, |X2 −˜µ0|, . . . , |Xn −˜µ0|.
Equal absolute diﬀerences are assigned the mean rank for their
positions.
TS: T+ = the sum of the ranks corresponding to the positive diﬀerences
(Xi −˜µ0).
RR: T+ ≥c1,
T+ ≤c2,
T+ ≥c or T+ ≤n(n + 1) −c
where c1, c2, and c are critical values for the Wilcoxon signed-rank
statistic (see table on page 372) such that Prob [T+ ≥c1] ≈α,
Prob [T+ ≤c2] ≈α, and Prob [T+ ≥c] ≈α/2.
Any observed diﬀerence (xi −˜µ0) = 0 is excluded from the test and
the sample size is reduced accordingly.
c
⃝2000 by Chapman & Hall/CRC

The normal approximation: When n ≥20, T+ has approximately a normal
distribution with
µT+ = n(n + 1)
4
and
σ2
T+ = n(n + 1)(2n + 1)
24
.
(14.29)
The random variable
Z = T+ −µT+
σT+
(14.30)
has approximately a standard normal distribution when H0 is true.
See D. J. Sheskin, Handbook of Parametric and Nonparametric Statistical
Procedures, CRC Press LLC, Boca Raton, FL, 1997, pages 83–94.
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 15
Quality Control and Risk
Analysis
Contents
15.1
Quality assurance
15.1.1
Control charts
15.1.2
Abnormal distributions of points
15.2
Acceptance sampling
15.2.1
Sequential sampling
15.3
Reliability
15.3.1
Failure time distributions
15.4
Risk analysis and decision rules
15.1
QUALITY ASSURANCE
15.1.1
Control charts
Expression
Meaning
x chart
control chart for means
R chart
control chart for sample ranges
σ chart
control chart for standard deviations
p chart
fraction defective chart
c chart
number of defects chart
LCL
lower control limit
UCL
upper control limit
Suppose the population mean µ and the population standard deviation σ are
unknown. For k samples of size n let xi and Ri be the sample mean and
sample range for the ith sample, respectively. The average mean x and the
average range R are deﬁned by
x = 1
k
k

i=1
xi
R = 1
k
k

i=1
Ri
(15.1)
Suppose the population proportion p or the number of defects is unknown.
For k samples assume sample i has ni items, ci defects, and ei defective items.
c
⃝2000 by Chapman & Hall/CRC

Note that a single defective item may have many defects. Deﬁne p and c by
p = e1 + e2 + · · · + ek
n1 + n2 + · · · + nk
c = 1
k
k

i=1
ci
(15.2)
Type of chart
central line
lower control limit (LCL)
upper control limit (UCL)
x chart (µ, σ known)
µ
µ −Aσ
µ + Aσ
x chart (µ, σ unknown)
x
x −A2R
x + A2R
R chart (σ known)
d2σ
D1σ
D2σ
R chart (σ unknown)
R
D3R
D4R
p chart (based on past data)
p
min

0, p −3

p(1−p)
n

p + 3

p(1−p)
n
number-of-defectives chart
np
np −3

np(1 −p)
np + 3

np(1 −p)
c chart
c
c −3
√
c
c + 3
√
c
where n is the sample size, A = 3/√n, D1 = d2 −3d3, D2 = d2 + 3d3,
D3 = D1/d2, and D4 = D2/d2. Values of {A, A2, d2, d3, D1, D2, D3, D4} are
given in Table 15.11.
1This table reproduced, by permission, from ASTM Manual on Quality Control of Ma-
terials, American Society for Testing and Materials, Philadelphia, PA, 1951.
c
⃝2000 by Chapman & Hall/CRC

Chart for averages
Chart for standard deviations
Chart for ranges
n
A
A1
A2
c2
B1
B2
B3
B4
d2
D1
D2
D3
D4
2
2.121
3.760
1.880
0.5642
0
1.843
0
3.267
1.128
0
3.686
0
3.267
3
1.732
2.394
1.023
0.7236
0
1.858
0
2.568
1.693
0
4.358
0
2.575
4
1.500
1.880
0.729
0.7979
0
1.808
0
2.266
2.059
0
4.698
0
2.282
5
1.342
1.596
0.577
0.8407
0
1.756
0
2.089
2.326
0
4.918
0
2.115
6
1.225
1.410
0.483
0.8686
0.026
1.711
0.030
1.970
2.534
0
5.078
0
2.004
7
1.134
1.277
0.419
0.8882
0.105
1.672
0.118
1.882
2.704
0.205
5.203
0.076
1.924
8
1.061
1.175
0.373
0.9027
0.167
1.638
0.185
1.815
2.847
0.387
5.307
0.136
1.864
9
1.000
1.094
0.337
0.9139
0.219
1.609
0.239
1.761
2.970
0.546
5.394
0.184
1.816
10
0.949
1.028
0.308
0.9227
0.262
1.584
0.284
1.716
3.078
0.687
5.469
0.223
1.777
11
0.905
0.973
0.285
0.9300
0.299
1.561
0.321
1.679
3.173
0.812
5.534
0.256
1.744
12
0.866
0.925
0.266
0.9359
0.331
1.541
0.354
1.646
3.258
0.924
5.592
0.284
1.716
13
0.832
0.884
0.249
0.9410
0.359
1.523
0.382
1.618
3.336
1.026
5.646
0.308
1.692
14
0.802
0.848
0.235
0.9453
0.384
1.507
0.406
1.594
3.407
1.121
5.693
0.329
1.671
15
0.775
0.816
0.223
0.9490
0.406
1.492
0.428
1.572
3.472
1.207
5.737
0.348
1.652
c
⃝2000 by Chapman & Hall/CRC
Table 15.1: Parameter values for control charts (reproduced by permission)

15.1.2
Abnormal distributions of points in control charts
Abnormality
Description
Sequence
Seven or more consecutive points on one side
of the center line. Denotes the average value
has shifted.
Bias
Fewer than seven consecutive points on one
side of the center line, but most of the points
are on that side.
• 10 of 11 consecutive points
• 12 or more of 14 consecutive points
• 14 or more of 17 consecutive points
• 16 or more of 20 consecutive points
Trend
Seven or more consecutive rising or falling
points.
Approaching the limit
Two out of three or three or more out of seven
consecutive points are more than two thirds
the distance between the center line and a
control limit.
Periodicity
The data points vary in a regular periodic
pattern.
15.2
ACCEPTANCE SAMPLING
Expression
Meaning
AQL
acceptable quality level
AOQ
average outgoing quality
AOQL
average outgoing quality limit (maximum value of
AOQ for varying incoming quality)
LTPD
lot tolerance percent defective
producer’s risk
Type I error (percentage of “good” lots rejected)
consumer’s risk
Type II error (percentage of “bad” lots accepted)
Military standard 105 D is a widely used sampling plan.
There are three
general levels of inspection corresponding to diﬀerent consumer’s risks. (In-
spection level II is usually chosen; level I uses smaller sample sizes and level II
uses larger sample sizes.) There are also three types of inspections: normal,
tightened, and reduced. Tables are available for single, double, and multiple
sampling.
To use MIL-STD-105 D for single sampling, determine the sample size code
letter from Table 15.2. Using this sample size code letter ﬁnd the sample size
and the acceptance and rejection numbers from Table 15.3.
c
⃝2000 by Chapman & Hall/CRC

Lot or batch size
general inspection levels
I
II
III
2
to
8
A
A
B
9
to
15
A
B
C
16
to
25
B
C
D
26
to
50
C
D
E
51
to
90
C
E
F
91
to
150
D
F
G
151
to
280
E
G
H
281
to
500
F
H
J
501
to
1,200
G
J
K
1,201
to
3,200
H
K
L
3,201
to
10,000
J
L
M
10,001
to
35,000
K
M
N
35,001
to
150,000
L
N
P
150,001
to
500,000
M
P
Q
500,001
and
over
N
Q
R
Table 15.2: Sample size code letters for MIL-STD-105 D
Example 15.75:
Suppose that MIL-STD-105D is to be used with incoming lots of
1,000 items, inspection level II is to be used in conjunction with normal inspection, and
an AQL of 2.5 percent is desired. How should the inspections be carried out?
Solution:
(S1) From table 15.2 the sample size code letter is J.
(S2) From the table on page 388, for column J, the lot size should be 80. Using the
row labeled 2.5 the acceptance number is 5 and the rejection number is 6.
(S3) Thus, if a single sample of size 80 (selected randomly from each lot of 1,000
items) contains 5 or fewer defectives then the lot is to be accepted. If it contains
6 or more defectives, then the lot is to be rejected.
15.2.1
Sequential sampling
We need to inspect a lot. Suppose the two sequences of numbers {an} (for
accept) and {rn} (for reject) are given. Elements from the lot are sequen-
tially taken (n = 1, 2, . . . ); after each element is selected the total number of
defective elements is determined. The lot is accepted after n elements if the
number of defectives is less than or equal to an. The lot is rejected after n
elements if the number of defectives is greater than or equal to rn. Sampling
continues as long as the number of defectives in the sample of size n falls
between an and rn.
c
⃝2000 by Chapman & Hall/CRC

AQL = Acceptable quality level (normal inspection).
Ac|Re = Accept if Ac or fewer are found, reject if Re or more are found.
←= Use ﬁrst sampling procedure to left.
→= Use ﬁrst sampling procedure to right. If sample size equals, or exceeds, lot or
batch size, do 100 percent inspection.
Table 15.3: Master table for single sampling inspection (normal inspection)
MIL-STD-105 D.
Sample size code letter and sample size
A
B
C
D
E
F
G
H
J
K
L
M
N
P
Q
R
AQL
2
3
5
8
13
20
32
50
80
125
200
315
500
800
1250 2000
0.010
→
→
→
→
→
→
→
→
→
→
→
→
→
→
0|1
←
0.015
→
→
→
→
→
→
→
→
→
→
→
→
→
0|1
←
←
0.025
→
→
→
→
→
→
→
→
→
→
→
→
0|1
←
→
1|2
0.040
→
→
→
→
→
→
→
→
→
→
→
0|1
←
→
1|2
2|3
0.065
→
→
→
→
→
→
→
→
→
→
0|1
←
→
1|2
2|3
3|4
0.10
→
→
→
→
→
→
→
→
→
0|1
←
→
1|2
2|3
3|4
5|6
0.15
→
→
→
→
→
→
→
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
0.25
→
→
→
→
→
→
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11
0.40
→
→
→
→
→
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15
0.65
→
→
→
→
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
1.0
→
→
→
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
1.5
→
→
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
2.5
→
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
4.0
→
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
←
6.5
0|1
←
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
←
←
10
→
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
←
←
←
15
→
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
←
←
←
←
25
1|2
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
←
←
←
←
←
40
2|3
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
←
←
←
←
←
←
65
3|4
5|6
7|8
10|11 14|15 21|22
←
←
←
←
←
←
←
←
←
←
100
5|6
7|8
10|11 14|15 21|22
←
←
←
←
←
←
←
←
←
←
←
150
7|8
10|11 14|15 21|22 30|31
←
←
←
←
←
←
←
←
←
←
←
250
10|11 14|15 21|22 30|31 44|45
←
←
←
←
←
←
←
←
←
←
←
400
14|15 21|22 30|31 44|45
←
←
←
←
←
←
←
←
←
←
←
←
650
21|22 30|31 44|45
←
←
←
←
←
←
←
←
←
←
←
←
←
1000
30|31 44|45
←
←
←
←
←
←
←
←
←
←
←
←
←
←
c
⃝2000 by Chapman & Hall/CRC

A sequential sampling plan with AQL= p0, LTPD= p1, producer’s risk α, and
consumer’s risk β is given by
an =

log

β
1−α

+ n log

1−p0
1−p1

log

p1
p0

−log

1−p1
1−p0


rn =


log

1−β
α

+ n log

1−p0
1−p1

log

p1
p0

−log

1−p1
1−p0



(15.3)
15.2.1.1
Sequential probability ratio tests
A sequential probability ratio test is a sequential sampling strategy in which
the ratio of probabilities (based on the hypotheses) is used. Given two simple
hypotheses (H0 and H1) and m observations, compute
(a) P0m = Prob (observations | H0)
(b) P1m = Prob (observations | H1)
(c) vm = P1m/P0m
and then follow the decision rule given by:
(a) If vm ≥1 −β
α
then reject H0.
(b) If vm ≤
β
1 −α then reject H1.
(c) If
β
1 −α < vm < 1 −β
α
then make another observation.
Note that the number of samples taken is not ﬁxed a priori, but is determined
as sampling occurs.
Example 15.76:
Lot acceptance
Let θ denote the fraction of defective items. Two simple hypotheses are H0: θ = θ0 =
0.05 and H1: θ = θ1 = 0.15. Choose α = .05 and β = .10 (i.e., reject lot with θ = θ0
about 5% of the time, accept lot with θ = θ1 about 10% of the time). If, after m
observations, there are d defective items, then
Pim =

m
d

θd
i (1 −θi)m−d
and
vm =
θ1
θ0
d 1 −θ1
1 −θ0
m−d
or vm = 3d(0.895)m−d using the above numbers. The critical values are
β
1−α = 0.105
and 1−β
α
= 18. The decision to perform another observation depends on whether or not
0.105 ≤3d(0.895)m−d ≤18
Taking logarithms, a (m −d, d) control chart can be drawn with the the following lines:
d = 0.101(m −d) −2.049 and d = 0.101(m −d) + 2.63. On the ﬁgure below, a sample
path leading to rejection of H0 has been indicated:
c
⃝2000 by Chapman & Hall/CRC

✻
✲
✑✑✑✑✑✑✑✑✑✑✑
✑✑✑✑✑✑✑✑✑✑✑✑✑✑
reject H0
reject H1
d
sample
path
(defectives)
decision boundaries
m −d (non-defectives)
15.2.1.2
Two-sided mean test
Let X be normally distributed with unknown mean µ and known standard
deviation σ. Consider the two simple hypotheses H0: µ = µ0 and H1: µ = µ1.
If Y is the sum of the ﬁrst m observations of X, then a (Y, m) control chart
is constructed with the two lines:
Y = µ0 + µ1
2
m +
σ2
µ1 −µ0
log
β
1 −α
Y = µ0 + µ1
2
m +
σ2
µ1 −µ0
log 1 −β
α
(15.4)
15.2.1.3
One-sided variance test
Consider testing the null hypothesis that σ = σ1 against the alternative that
σ = σ2 > σ1. If α is the assigned risk of rejecting the null hypothesis when σ
equals σ1 and β is the assigned risk of accepting the null hypothesis when σ
equal σ2, then the sequential plan is as follows:
1. Deﬁne the quantities
g = 0.43429
 1
σ2
1
−1
σ2
2

a = log10
1 −β
α
h2 = 2a
g
b = log10
1 −α
β
h1 = 2b
g
s = log10(σ2
2/σ2
1)
g
(15.5)
2. Deﬁne the acceptance and rejection lines as
Z(n)
1
= −h1 + s(n −1)
(lower)
Z(n)
2
=
h2 + s(n −1)
(upper)
(15.6)
The sequential test is carried out as follows:
c
⃝2000 by Chapman & Hall/CRC

1. Let n stand for the number of sample items inspected
2. Let Z stand for the sum of squared deviations from the sample mean:
Z =
n

i=1
(xi −x)2 =
n
n
i=1
x2
i −
 n
i=1
xi
2
n
(15.7)
3. Test against limits
(a) If Z < Z(n)
1
then accept the null hypothesis.
(b) If Z > Z(n)
2
then reject the null hypothesis.
(c) If neither inequality is true, then take another sample.
15.3
RELIABILITY
1. The reliability of a product is the probability that the product will
function within speciﬁed limits for at least a speciﬁed period of time.
2. A series system is one in which the entire system will fail if any of its
components fail.
3. A parallel system is one in which the entire system will fail only if all of
its components fail.
4. Let Ri denote the reliability of the ith component.
5. Let Rs denote the reliability of a series system.
6. Let Rp denote the reliability of a parallel system.
The product law of reliabilities states
Rs =
n
)
i=1
Ri
(15.8)
The product law of unreliabilities states
Rp = 1 −
n
)
i=1
(1 −Ri)
(15.9)
15.3.1
Failure time distributions
1. Let the probability of a component failing between times t and t + ∆t
be f(t)∆t.
2. The probability that a component will fail on the interval from 0 to t is
F(t) =
 t
0
f(x) dx
(15.10)
3. The reliability function is the probability that a component survives to
time t
R(t) = 1 −F(t)
(15.11)
c
⃝2000 by Chapman & Hall/CRC

4. The instantaneous failure rate, Z(t), is the average rate of failure in the
interval from t to t + ∆t, given that the component survived to time t
Z(t) = f(t)
R(t) =
f(t)
1 −F(t)
(15.12)
Note the relationships:
R(t) = e−J t
0 Z(x) dx
f(t) = Z(t)e−J t
0 Z(x) dx
(15.13)
Example 15.77:
If f(t) = αβtβ−1eαtβ with α > 0 and β > 0, the probability
distribution function for a Weibull random variable, then the failure rate is Z(t) =
αβtβ−1 and R(t) = e−αtβ. Note that failure rate decreases with time if β < 1 and
increases with time if β > 1.
15.3.1.1
Use of the exponential distribution
If the failure rate is a constant Z(t) = α (with α > 0) then f(t) = αe−αt (for
t > 0) which is the probability density function for an exponential random
variable.
If a failed component is replaced with another having the same
constant failure rate α, then the occurrence of failures is a Poisson process.
The constant 1/α is called the mean time between failures (MTBF). The
reliability function is R(t) = e−αt.
If a series system has n components, each with constant failure rate {αi},
then
Rs(t) = exp

−
n

i=1
αi

(15.14)
The MTBF for the series system is µs
µs =
1
1
µ1 +
1
µ2 + · · · +
1
µn
(15.15)
If a parallel system has n components, each with identical constant failure
rate α, then the MTBF for the parallel system is µp
µp = 1
α

1 + 1
2 + · · · + 1
n

(15.16)
15.4
RISK ANALYSIS AND DECISION RULES
Suppose knowledge of a speciﬁc state of a system is desired, and those states
can be delineated as {θ1, θ2, . . . }. For example, in a weather application,
the states might be θ1 for rain and θ2 for no rain. Decision rules are actions
that may be taken based on the state of a system. For example, in deciding
whether to go on a trip, there are the decision rules: stay home, go with an
umbrella, and go without an umbrella.
c
⃝2000 by Chapman & Hall/CRC

Possible actions
System state
θ1 (rain)
θ2 (no rain)
Stay home
a1
4
4
Go without an umbrella
a2
5
0
Go with an umbrella
a3
2
5
Table 15.4: An example loss function ℓ(θ, a)
A loss function is an arbitrary function that depends on a speciﬁc state and
a decision rule. For example, consider the loss function ℓ(θ, a) given in Ta-
ble 15.4. It is possible to determine the “best” decision, under diﬀerent mod-
els, even without obtaining any data.
• Minimax principle
With this principle one should expect and prepare for the worst. That
is, for each action it is possible to determine the minimum possible loss
that may be incurred. This loss is assigned to each action; the action
with the smallest (or minimum) maximum loss is the action chosen.
For the data in Table 15.4 the maximum loss is 4 for action a1 and 5 for
either of the actions a2 or a3. Under a minimax principle, the chosen
action would be a1 and the minimax loss would be 4.
• Minimax principle for mixed actions
It is possible to minimize the maximum loss when the action taken is
a statistical distribution, p, of actions. Assume that action ai is taken
with probability pi (with p1+p2+p3 = 1). Then the expected loss L(θi)
is given by L(θi) = Ea[ℓ(θi, a)] = p1ℓ(θi, a1) + p2ℓ(θi, a2) + p3ℓ(θi, a3).
The data in Table 15.4 result in the following expected losses:

L(θ1)
L(θ2)

= p1

4
4

+ p2

5
0

+ p3

2
5

(15.17)
It can be shown that the minimax point of this mixed action case has
to satisfy L(θ1) = L(θ2). Solving equation (15.17) with this constraint
leads to 5p2 = 3p3. Using this and p1 + p2 + p3 = 1 in equation (15.17)
results in L(θ1) = L(θ2) = 4 −7p3/5. This indicates that p3 should
be as large as possible. Hence, the maximum value is obtained by the
mixed distribution p = ( 0
8, 3
8, 5
8).
Hence, if action a2 is chosen 3/8’s of the time, and action a3 is chosen
5/8’s of the time, then the minimax loss is equal to L = 25/8. This is a
smaller loss than using a pure strategy of only choosing a single action.
c
⃝2000 by Chapman & Hall/CRC

• Bayes actions
If the probability distribution of the states {θ1, θ2, . . . } is given by the
density function g(θi), then the loss has a known distribution with an
expectation of B(a) = Ei[ℓ(θi, a)] = 
i g(θi)ℓ(θi, a). This quantity is
known as the Bayes loss for action a. A Bayes action is an action that
minimizes the Bayes loss.
For example, assuming that the prior distribution is given by g(θ1) = 0.4
and g(θ2) = 0.6, then B(a1) = 4, B(a2) = 2, and B(a3) = 3.8. This
leads to the choice of action a2.
A course of action can also be based on data about the states of interest.
For example, a weather report Z will give data for the predictions of rain
and no rain. Continuing the example, assume that the correctness of these
predictions is given as follows:
θ1 (rain)
θ2 (no rain)
Predict rain
z1
0.8
0.1
Predict no rain
z2
0.2
0.9
That is, when it will rain, then the prediction is correct 80% of the time.
A decision function is an assignment of data to actions. Since there are ﬁnitely
many possible actions and ﬁnitely many possible values of Z, the number of
decision functions is ﬁnite. In the example there are 32 = 9 possible decision
functions, {d1, d2, . . . , d9}; they are deﬁned to be:
Decision functions
d1
d2
d3
d4
d5
d6
d7
d8
d9
Predict z1, take action
a1
a2
a3
a1
a2
a1
a3
a2
a3
Predict z2, take action
a1
a2
a3
a2
a1
a3
a1
a3
a2
The risk function R(θ, di) is the expected value of the loss when a speciﬁc de-
cision function is being used: R(θ, di) = EZ[ℓ(θ, di(Z))]. It is straightforward
to compute the risk function for all values of {di} and {aj}. This results in
the following values:
Risk function evaluation
Decision Function
θ1 (rain)
θ2 (no rain)
d1
4
4
d2
5
0
d3
2
5
d4
4.2
0.4
d5
4.8
3.6
d6
3.6
4.9
d7
2.4
4.1
d8
4.4
4.5
d9
2.6
0.5
c
⃝2000 by Chapman & Hall/CRC

θ1 (rain)
θ2 (no rain)
a1
2
4
a2
3
0
a3
0
5
Table 15.5: The regret function r(θ, a) corresponding to the loss function in
Table 15.4
This array can now be treated as though it gave the loss function in a no–
data problem. The minimax principle for mixed action results in the “best”
solution being rule d3 for
7
17’s of the time and rule d9 for 10
17’s of the time. This
leads to a minimax loss of 40
17. Before the data Z is received, the minimax loss
was 25
8 . Hence, the data Z is “worth” 25
8 −40
17 = 105
136 in using the minimax
approach.
The regret function (also called the opportunity loss function) r(θ, a) is the
loss, ℓ(θ, a), minus the minimum loss for the given θ: r(θ, a) = ℓ(θ, a) −
minb ℓ(θ, b). For each state, the least loss is determined if that state were
known to be true. This is the contribution to loss that even a good decision
cannot avoid. The quantity r(θ, a) represents the loss that could have been
avoided had the state been known—hence the term regret.
For the loss function example in Table 15.4, the minimum loss for θ = θ1 is 2,
and the minimum loss for θ = θ2 is 0. Hence, the regret function is as given
in Table 15.5.
Most of the computations performed for a loss function could also be per-
formed with the risk function. If the minimax principle is used to determine
the “best” action, then, in this example, the “best” action is a2.
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 16
General Linear Models
Contents
16.1
Notation
16.2
The general linear model
16.2.1
The simple linear regression model
16.2.2
Multiple linear regression
16.2.3
One-way analysis of variance
16.2.4
Two-way analysis of variance
16.2.5
Analysis of covariance
16.3
Summary of rules for matrix operations
16.3.1
Linear combinations
16.3.2
Determinants
16.3.3
Inverse of a partitioned matrix
16.3.4
Eigenvalues
16.3.5
Diﬀerentiation involving vectors/matrices
16.3.6
Additional deﬁnitions and properties
16.4
Quadratic forms
16.4.1
Multivariate distributions
16.4.2
The principle of least squares
16.4.3
Minimum variance unbiased estimates
16.5
General linear hypothesis of full rank
16.5.1
Notation
16.5.2
Simple linear regression
16.5.3
Analysis of variance, one-way anova
16.5.4
Multiple linear regression
16.5.5
Randomized blocks (one observation per cell)
16.5.6
Quadratic form due to hypothesis
16.5.7
Sum of squares due to error
16.5.8
Summary
16.5.9
Computation procedure for hypothesis testing
16.5.10 Regression signiﬁcance test
16.5.11 Alternate form of the distribution
16.6
General linear model of less than full rank
16.6.1
Estimable function and estimability
16.6.2
Linear hypothesis model of less than full rank
16.6.3
Constraints and conditions
c
⃝2000 by Chapman & Hall/CRC

16.1
NOTATION
In this chapter, matrices are denoted by bold–face capital letters; for example,
if the matrix A has m rows and n columns, then A = Amn and
A =


a11 a12 · · · a1n
a21 a22 · · · a2n
...
...
...
...
am1 am2 · · · amn

.
In general, column vectors will be denoted by lower–case bold–face letters.
For example,
xT = [x1 x2 · · · xn],
βT = [β1 β2 · · · βk].
(16.1)
If necessary, the number of rows in a column vector is indicated with a sub-
script, for example, xn has n rows.
(1) Some special column vectors are T (vector of treatment totals), B (vec-
tor of block totals), 1 (vector of all ones), and 0 (vector of all zeros).
(2) 1TA is a row vector whose entries are the column sums of A, and A1
denotes a column vector whose entries are the row sums of A. 1TA1
denotes the sum of all the elements in the matrix A.
(3) AT denotes the transpose of A.
(4) (A)ij = aij denotes the element in the ith row and the jth column of A.
(5) The identity matrix is denoted by I. The order of the identity matrix
may be indicated by a subscript, for example, In denotes an n × n
identity matrix.
(6) Dx denotes a diagonal matrix with entries x1, x2, . . . , xn (the subscript
indicates the terms in the diagonal).
(7) A tilde placed above a matrix indicates the matrix is triangular. The
matrix KT is a lower triangular matrix and KTT is an upper triangular
matrix:
KT =


t11 0
0
· · · 0
t21 t22 0
· · · 0
t31 t32 t33 · · · 0
...
...
...
... ...
tn1 tn2 tn3 · · · tnn


,
KTT =


t11 t21 t31 · · · tn1
0
t22 t32 · · · tn2
0
0
t33 · · · tn3
...
...
...
... ...
0
0
0
· · · tnn


16.2
THE GENERAL LINEAR MODEL
16.2.1
The simple linear regression model
Let (x1, y1), (x2, y2), . . . , (xn, yn) be n pairs of observations such that yi is an
observed value of the random variable Yi. Assume there exist constants β0
c
⃝2000 by Chapman & Hall/CRC

and β1 such that
Yi = β0 + β1xi + ϵi
(16.2)
where ϵ1, ϵ2, . . . , ϵn are independent, normal random variables having mean 0
and variance σ2.
Assumptions
In terms of ϵi’s
In terms of Yi’s
ϵi’s are normally distributed
Yi’s are normally distributed
E [ϵi] = 0
E [Yi] = β0 + β1xi
Var [ϵi] = σ2
Var [Yi] = σ2
Cov [ϵi, ϵj] = 0, i ̸= j
Cov [Yi, Yj] = 0, i ̸= j
Using equation (16.2):
Y1 = β0 + β1x1 + ϵ1
Y2 = β0 + β1x2 + ϵ2
...
Yn = β0 + β1xn + ϵn
(16.3)
This set of equations may be written in matrix form:


Y1
Y2
Y3
...
Yn


=


1
x1
1
x2
1
x3
...
...
1
xn



β0
β1

+


ϵ1
ϵ2
ϵ3
...
ϵn


Y
=
X
β
+
ϵ
The matrix X is the design matrix and may also be written as X = [1, x],
where 1 is a column vector containing all 1’s and x is the column vector
containing the xi’s.
The simple linear regression model, equation (16.2), is often written in the
form
Yi = µ + β1(xi −x) + ϵi
(16.4)
c
⃝2000 by Chapman & Hall/CRC

where µ = β0 + β1x. This model may also be written in matrix form:


Y1
Y2
Y3
...
Yn


=


1
(x1 −x)
1
(x2 −x)
1
(x3 −x)
...
...
1
(xn −x)



µ
β1

+


ϵ1
ϵ2
ϵ3
...
ϵn


Y
=
X
β
+
ϵ
where X = [1, (x −x1)].
16.2.2
Multiple linear regression
Let there be n observations of the form (x1i, x2i, . . . , xki, yi) such that yi is
an observed value of the random variable Yi. Assume there exist constants
β0, β1, . . . , βk such that
Yi = β0 + β1x1i + · · · + βkxki + ϵi
(16.5)
where ϵ1, ϵ2, . . . , ϵn are independent, normal random variables having mean 0
and variance σ2.
Assumptions
In terms of ϵi’s
In terms of Yi’s
ϵi’s are normally distributed
Yi’s are normally distributed
E [ϵi] = 0
E [Yi] = β0 + β1x1i + · · · + βkxki
Var [ϵi] = σ2
Var [Yi] = σ2
Cov [ϵi, ϵj] = 0, i ̸= j
Cov [Yi, Yj] = 0, i ̸= j
Using equation (16.5):
Y1 = β0 + β1x11 + β2x21 + · · · + βkxk1 + ϵ1
Y2 = β0 + β1x12 + β2x22 + · · · + βkxk2 + ϵ2
...
Yn = β0 + β1x1n + β2x2n + · · · + βkxkn + ϵn
(16.6)
This set of equations may be written in matrix form:


Y1
Y2
...
Yn

=


1 x11 x21 x31 · · · xk1
1 x21 x22 x32 · · · xk2
...
...
...
...
...
...
1 x1n x2n x3n · · · xkn




β0
β1
β2
...
βk


+


ϵ1
ϵ2
ϵ3
...
ϵn


Y
=
X
β
+
ϵ
c
⃝2000 by Chapman & Hall/CRC

where the design matrix X = [1, Xnk] and Xnk is the matrix of observations
on the independent variables.
16.2.3
One-way analysis of variance
Let there be k treatments (or populations), independent random samples of
size ni (where i = 1, 2, . . . , k), from each population, and let N = n1 + n2 +
· · · + nk. Let Yij be the jth random observation in the ith treatment group.
Assume a ﬁxed eﬀects experiment model:
Yij = µ + αi + ϵij
(16.7)
where µ is the grand mean, αi is the ith treatment eﬀect, and ϵij is the random
error term. The ϵij’s are assumed to be independent, normally distributed,
with mean 0 and variance σ2.
Using equation (16.7):
Y11
= µ + α1
+ ϵ11
Y12
= µ + α1
+ ϵ12
...
...
...
Y1n1 = µ + α1
+ ϵ1n1
Y21
= µ
+ α2
+ ϵ21
Y22
= µ
+ α2
+ ϵ22
...
...
...
...
Y2n2 = µ
+ α2
+ ϵ2n2
...
...
...
...
Yk1
= µ
+ αk + ϵk1
Yk2
= µ
+ αk + ϵk2
...
...
...
...
Yknk = µ
+ αk + ϵknk
c
⃝2000 by Chapman & Hall/CRC

This set of equations may be written in matrix form:


Y11
Y12
...
Y1n1
Y21
Y22
...
Y2n2
...
Yk1
Yk2
...
Yknk


=


1 1 0 0 · · · 0 0
1 1 0 0 · · · 0 0
...
...
...
... ... ...
...
1 1 0 0 · · · 0 0
1 0 1 0 · · · 0 0
1 0 1 0 · · · 0 0
...
...
...
... ... ...
...
1 0 1 0 · · · 0 0
...
...
...
...
...
...
...
1 0 0 0 · · · 0 1
1 0 0 0 · · · 0 1
...
...
...
... ... ...
...
1 0 0 0 · · · 0 1




µ
α1
α2
...
αk


+


ϵ11
ϵ12
...
ϵ1n1
ϵ21
ϵ22
...
ϵ2n2
...
ϵk1
ϵk2
...
ϵknk


Y
=
X
β
+
ϵ
The design matrix X may be written as X = [1N, XNk] where
XNk =


1n1 0n1 0n1 · · · 0n1
0n2 1n2 0n2 · · · 0n2
...
...
...
...
0nk 0nk 0nk · · · 1nk


(16.8)
and the parameter vector may be written as β =

µ α
T.
16.2.4
Two-way analysis of variance
Let Yijk be the kth random observation for the ith level of factor A and the
jth level of factor B.
Assume there are nij observations for the ij factor
combination: i = 1, 2, . . . , a, j = 1, 2, . . . , b, k = 1, 2, . . . , nij.
For simplicity, consider a ﬁxed eﬀects experiment model:
Yijk = µ + αi + βj + (αβ)ij + ϵijk
(16.9)
where µ is the grand mean, αi is the level i factor A eﬀect, βj is the level j
factor B eﬀect, (αβ)ij is the level ij interaction eﬀect, and ϵijk is the random
error term. The ϵijk’s are assumed to be independent, normally distributed,
with mean 0 and variance σ2.
c
⃝2000 by Chapman & Hall/CRC

Suppose a = 2 and b = 3. The model may be written in matrix form:


y11
y12
y13
y21
y22
y23


=


1n11 1 0 1 0 0 1 0 0 0 0 0
1n12 1 0 0 1 0 0 1 0 0 0 0
1n13 1 0 0 0 1 0 0 1 0 0 0
1n21 0 1 1 0 0 0 0 0 1 0 0
1n22 0 1 0 1 0 0 0 0 0 1 0
1n23 0 1 0 0 1 0 0 0 0 0 1




µ
α1
α2
β1
β2
β3
(αβ)11
(αβ)12
(αβ)13
(αβ)21
(αβ)22
(αβ)23


+


ϵ111
...
ϵ11n11
ϵ121
...
ϵ12n12
...
ϵ231
...
ϵ23n23


Y
=
X
β
+
ϵ
16.2.5
Analysis of covariance
The analysis of covariance procedure is a combination of analysis of variance
and regression analysis. For example, consider a one-way classiﬁcation with
one independent variable. Let Yij be the jth random observation in the ith
treatment group: i = 1, 2, . . . , a, j = 1, 2, . . . , ni. The model is
Yij = µ + αi + βxij + ϵij
(16.10)
where µ is the grand mean, αi is the ith treatment eﬀect, β is the regression
coeﬃcient of Y on X, and ϵij is the random error term. The ϵij’s are assumed
to be independent, normally distributed, with mean 0 and variance σ2.
Suppose a = 3, using equation (16.10):
Y11
= µ + α1
+ βx11
+ ϵ11
Y12
= µ + α1
+ βx12
+ ϵ12
...
...
...
...
...
Y1n1 = µ + α1
+ βx1n1 + ϵ1n1
Y21
= µ
+ α2
+ βx21
+ ϵ21
...
...
...
...
...
Y2n2 = µ
+ α2
+ βx2n2 + ϵ2n2
Y31
= µ
+ α3 + βx31
+ ϵ31
...
...
...
...
...
Y3n3 = µ
+ α3 + βx3n3 + ϵ3n3
c
⃝2000 by Chapman & Hall/CRC

If x1, x2, and x3 denote the observations on the independent variable in each
treatment group, this set of equations may be written in matrix form:


Y11
Y12
...
Y1n1
Y21
...
Y2n2
Y31
...
Y3n3


=


1n1 1 0 0 x1
1n2 0 1 0 x2
1n3 0 0 1 x3




µ
α1
α2
α3
β


+


ϵ11
ϵ12
...
ϵ1n1
ϵ21
...
ϵ2n2
ϵ31
...
ϵ3n3


Y
=
X
β
+
ϵ
16.3
SUMMARY OF RULES FOR MATRIX OPERATIONS
16.3.1
Linear combinations
Suppose X is a random vector: a vector whose elements are random variables.
Let µ be the vector of means and let Σ be the variance–covariance matrix,
denoted
E [X] = µ,
Cov [X] = E

XXT
= Σ.
(16.11)
For any conforming matrix C, the linear combinations Y = CX have
E [Y] = E [CX] = Cµ,
Cov [Y] = Cov [CX] = CΣCT.
(16.12)
The linear combinations Z = XTC have
E [Z] = E

XTC

= µTC,
Cov [Z] = Cov

XTC

= CTΣC.
(16.13)
16.3.2
Determinants and partitioning of determinants
The determinant of a square matrix X, denoted by |X| or det (X), is a scalar
function of X deﬁned as
det(X) =

σ
sgn(σ)x1,σ(1) x2,σ(2) · · · xn,σ(n)
(16.14)
where the sum is taken over all permutations σ of {1, 2, . . . , n}. The signum
function sgn(σ) is the number of successive transpositions required to change
the permutation σ to the identity permutation. Note the properties of deter-
minants: |A| |B| = |AB| and |A| = |AT|.
Omitting the signum function in equation (16.14) yields the deﬁnition of the
permanent of X, given by per X = 
σ x1,σ(1) · · · xn,σ(n).
c
⃝2000 by Chapman & Hall/CRC

Suppose the matrix X can be partitioned, written as
X =

X11 X12
X21 X22

.
(16.15)
The determinant of X may be computed by
$$$$
X11 X12
X21 X22
$$$$ = |X11|
$$X22 −X21X−1
11 X12
$$
if X−1
11 exists
= |X22|
$$X11 −X12X−1
22 X21
$$
if X−1
22 exists.
(16.16)
16.3.3
Inverse of a partitioned matrix
Suppose the matrix X can be partitioned as in equation (16.15). The inverse
of the matrix X may be written as
%
X11 X12
X21 X22
&−1
=
%
A B
C D
&
where
A = [X11 −X12X−1
22 X21]−1
D = [X22 −X21X−1
11 X12]−1
B = −X−1
11 X12D
C = −X−1
22 X21A
16.3.3.1
Symmetric case
%
X11 X12
XT
12 X22
&−1
=
%
A
B
BT D
&
where
A = [X11 −X12X−1
22 XT
12]−1
D = [X22 −XT
12X−1
11 X12]−1
B = −AX12X−1
22 = −X−1
11 X12D
16.3.4
Eigenvalues
If A is a k × k square matrix and I is the k × k identity matrix, then the
scalers λ1, λ2, . . . , λk that satisfy the polynomial equation |A −λI| are the
eigenvalues (or characteristic roots) of the matrix A. The equation |A −
λI| is a function of λ and is the characteristic equation.
Let ch(A) denote the characteristic roots of the matrix A and tr(A) denote
the trace of A.
(1) ch(AB) = ch(BA) except possibly for zero roots.
(2) tr(AB) = tr(BA)
(3) If ch(A) = {λ}n
i=1, then ch(A−1) = 1/λi and ch(I ± A) = 1 ± λi for
i = 1, 2, . . . , n.
c
⃝2000 by Chapman & Hall/CRC

16.3.5
Diﬀerentiation involving vectors/matrices
16.3.5.1
Deﬁnitions
(1) Let f be a real–valued function of x1, x2, . . . , xn. The symbol ∂f/∂x
denotes a column vector whose ith element is ∂f/∂xi.
(2) Let f be a real–valued function of x11, x12, . . . , x1n, x21, . . . , x2n, . . . ,
xm1, xm2, . . . , xmn. The symbol ∂f/∂X denotes a matrix whose (i, j)
entry is ∂f/∂xij.
Note: If there are functional relationships between the elements of X (for
example, in a symmetric matrix) these relationships are disregarded in
the deﬁnition above. If xij = xji = yij (yij is the symbol for the distinct
variable that occurs in two places in X) then ∂f/∂yij = (∂f/∂X)ij +
(∂f/∂X)ij.
(3) If y1, y2, . . . , yn are functions of x, then ∂y/∂x denotes the column vector
whose ith entry is ∂yi/∂x.
(4) If y11, y12, . . . , y1n, y21, . . . , y2n, . . . , ym1, . . . , ymn are functions of x, then
∂Y/∂x denotes the matrix whose (i, j) entry is ∂yij/∂x.
(5) If each of the quantities y1, y2, . . . , yn is a function of the variables
x1, x2, . . . , xm, then ∂yT/∂x denotes an m×n matrix whose (i, j) entry
is ∂yj/∂xi.
16.3.5.2
Properties
Suppose a, b, e, x, y, and z are column vectors, and A, Q, X, and Y are
matrices.
(1) ∂(xTx)
∂x
= 2x
(2) ∂(xTQx)
∂x
= Qx + QTx
(3) ∂(xTQx)/∂x = 2Qx if Q is symmetric.
(4) ∂(aTx)/∂x = a
(5) ∂(aTQx)/∂x = QTa
(6) ∂tr(AX)/∂X = AT
(7) ∂tr(XA)/∂X = AT
(8) ∂ln |X|/∂X = (XT)−1 if X is square and nonsingular.
(9) ∂y
∂x = ∂zT
∂x · ∂y
∂z
(Chain rule 1)
(10) ∂(xTA)
∂x
= A
c
⃝2000 by Chapman & Hall/CRC

(11) If e = B −ATx, then
∂(eTe)
∂x
= ∂eT
∂x · ∂(eTe)
∂e
(using property 9)
= −2ATe
(using properties 1 and 10)
(12) If the scalar z is related to a scalar x through the variables yij, i =
1, 2, . . . , m; j = 1, 2, . . . , n, then
∂z
∂x = tr
 ∂z
∂Y · ∂YT
∂x

= tr
 ∂z
∂YT · ∂Y
∂x

(16.17)
This second chain rule is correct regardless of any functional relation-
ships that may exist between the elements of Y.
16.3.6
Additional deﬁnitions and properties
(1) If KT is lower triangular, then KT−1 is also lower triangular.
(2) A matrix A is
(a) positive deﬁnite if xTAx > 0 for all x ̸= 0.
(b) positive semi-deﬁnite if xTAx ≥0 for all x.
(3) For any matrix Q, the dimension of the row (column) space of Q is the
row (column) rank of Q. (The row (column) rank of a matrix is also
the number of linearly independent rows (columns) of that matrix.) The
row rank and the column rank of any matrix Q are equal, and are the
rank of the matrix Q.
(4) If Q is a symmetric, positive-deﬁnite matrix, then there exists a unique
real matrix KT with positive diagonal entries such that Q = KTKTT. The
matrix KT may be obtained by using the forward Doolittle procedure: In
each cycle, divide every element of the next–to–last row (the row which
is immediately above the one beginning with 1) by the square–root of
the leading (ﬁrst) element. This technique produces KTT on the left and
KT−1 on the right–hand side.
(5) If Q is an n×n symmetric, positive semi-deﬁnite matrix of rank r, then
the matrix KT obtained via the forward Doolittle procedure will have
zeros to the right of the rth column. Q may be written as
Q =
KT1
T2
 "
KTT
1 TT
2
#
(16.18)
where only KT1 is triangular. This computational procedure is important
when determining characteristic roots. Suppose A and B are symmetric,
and A is of rank r < n. To ﬁnd the largest characteristic root of AB
ﬁrst obtain the representation
A =
KT1
T2
 "
KTT
1 TT
2
#
(16.19)
c
⃝2000 by Chapman & Hall/CRC

using the forward Doolittle procedure. The characteristic roots of AB
may be found using
ch(AB) = ch
"
KTT
1 TT
2
#
B
KT1
T2

(16.20)
where the right–hand matrix is of small order and symmetric.
16.4
PRINCIPLE OF MINIMIZING QUADRATIC FORMS AND
GAUSS MARKOV THEOREM
16.4.1
Multivariate distributions
Suppose X is a random variable with mean µ and variance σ2:
E [X] = µ,
Var [X] = σ2.
(16.21)
(1) The standardized random variable Y = (X −µ)/σ has mean 0 and
variance 1:
E [Y ] = 0,
Var [Y ] = 1.
(16.22)
(2) If X is a normal random variable, then the random variable Z2 = (X −
µ)2/σ2 has a chi–square distribution with one degree of freedom.
Suppose X is a random vector consisting of the p random variables {X1, X2, . . . , Xp}:
that is XT = [X1, X2, . . . , Xp]. Let µ be the vector of means and Σ be the
variance–covariance matrix:
E [X] = µ,
Cov [X] = Σ.
(16.23)
Let Σ = KA KAT where KA is the lower triangular matrix obtained using the for-
ward Doolittle analysis. Let Y = KA−1(X−µ) and note Σ−1 = ( KAT)−1 KA−1 =
( KA−1)T KA−1.
(1) E [Y] = KA−1E [X −µ] = 0.
(2) Cov [Y] = KA−1Cov [X −µ]( KA−1)T = KA−1Cov [X −µ]( KAT)−1
= KA−1Cov [X]( KAT)−1
= KA−1 KA KAT( KAT)−1 = I
(3) The expression
YTY = (X −µ)T( L
bfA
−1)T KA−1(X −µ)
= (X −µ)TΣ−1(X −µ)
(16.24)
is the standard quadratic form.
If X has a multivariate normal
distribution, then YTY has a chi–square distribution with p degrees of
freedom.
c
⃝2000 by Chapman & Hall/CRC

16.4.2
The principle of least squares
Let Y be the random vector of responses, y be the vector of observed re-
sponses, β be the vector of regression coeﬃcients, ϵ be the vector of random
errors, and let X be the design matrix:
Y=


Y1
Y2
...
Yn

y=


y1
y2
...
yn

β=


β0
β1
...
βk

ϵ=


ϵ1
ϵ2
...
ϵn

X=


1 x11 x21 · · · xk1
1 x12 x22 · · · xk2
...
...
...
...
1 x1n x2n · · · xkn


The model may now be written as Y = Xβ + ϵ where ϵ ∼Nn(0, σ2In) or
equivalently Y ∼Nn(Xβ, σ2In).
The sum of squared deviations about the true regression line is
S(β) =
n

i=1
[yi −(β0 + β1x1i + · · · + βkxki)]2
= (yT −βTXT)(y −βX) = eTe
(16.25)
where e is the vector of observed errors. To minimize equation (16.25):
∂(eTe)
∂β
= −2XT(y −Xβ)
(16.26)
The vector .β
T = (.β0, .β1, . . . , .βk) that minimizes S(β) is the vector of least
squares estimates. Setting equation (16.26) equal to zero:
XT 
y −X.β

= 0
(XTX).β = XTy.
(16.27)
The normal equations are given by equation (16.27). If the matrix XTX is
non–singular, then
.β = (XTX)−1XTy.
(16.28)
16.4.3
Minimum variance unbiased estimates
The minimum variance, unbiased, linear estimate of β is obtained by using a
general form of the Gauss Markov theorem. Suppose
Y = Xβ + ϵ,
E [ϵ] = 0,
Cov [Y] = Cov [ϵ] = σ2V
(16.29)
where V is a square, symmetric, non–singular, n × n matrix with known
entries.
Therefore, the variance of Yi (for i = 1, 2, . . . , n) is known and
Cov [Yi, Yj], for all i ̸= j, is known except for an arbitrary scalar multiple.
c
⃝2000 by Chapman & Hall/CRC

The best linear estimate of an arbitrary linear function cTβ is cT.β where .β
minimizes the quadratic form ϵTV−1ϵ. The standard quadratic form for ϵ is
(ϵ −E [ϵ])T(Cov [ϵ])−1(ϵ −E [ϵ]) = (ϵ −0)T(σ2V)−1(ϵ −0)
= 1
σ2 ϵTV−1ϵ.
(16.30)
Minimizing the standard quadratic form is equivalent to minimizing ϵTV−1ϵ,
as stated in the Gauss Markov Theorem. In this case the normal equations
are XTV−1X.β = XTV−1y.
16.5
GENERAL LINEAR HYPOTHESIS OF FULL RANK
This section is concerned with the problem of testing hypotheses about certain
parameters and the associated probability distributions.
16.5.1
Notation
A general null hypothesis is stated as Cβ = k where
(1) C = Cnhm, (nh ≤m), is the hypothesis matrix and is of rank nh.
(2) β is an n×1 column vector of parameters as deﬁned in the general linear
model.
(3) k is a vector of nh known elements, usually equal to 0.
(4) nh is the number of degrees of freedom due to hypothesis; the
number of rows in the hypothesis matrix C; the number of nonredundant
statements in the null hypothesis.
(5) ne is the number of degrees of freedom due to error and is equal to
the number of observations minus the eﬀective number of parameters.
Note: A composite hypothesis should not contain:
(1) contradictory statements like
H0: β1 = β2
and
β1 = 2β2
simultaneously,
(2) redundant statements like
H0: β1 = β2
and
3β1 = 3β2.
16.5.2
Simple linear regression
Model: Let (x1, y1), (x2, y2), . . . , (xn, yn) be n pairs of observations such that
yi is an observed value of the random variable Yi. Assume there exist constants
β0 and β1 such that
Yi = β0 + β1xi + ϵi,
parameter vector β = [β0, β1]T
(16.31)
where ϵ1, ϵ2, . . . , ϵn are independent, normal random variables having mean 0
and variance σ2.
Examples:
c
⃝2000 by Chapman & Hall/CRC

(1) H0: β0 = 0
Ha: β0 ̸= 0
General linear hypothesis: nh = 1
[1,
0]

β0
β1

= 0
C
β
= 0
(2) H0: β1 = 0
Ha: β1 ̸= 0
General linear hypothesis: nh = 1
[0,
1]

β0
β1

= 0
C
β
= 0
(3) H0: β0 = β1 = 0 simultaneously
Ha: βi ̸= 0 for some i
General linear hypothesis: nh = 2

1 0
0 1
 
β0
β1

=

0
0

C
β
=
0
(4) H0: β0 = β1
Ha: β0 ̸= β1
General linear hypothesis: nh = 1
[1,
−1]

β0
β1

= 0
C
β
= 0
16.5.3
Analysis of variance, one-way anova
Model: Let there be k treatments, or populations, independent random sam-
ples of size ni, i = 1, 2, . . . , k, from each population, and let N = n1 + n2 +
· · · + nk. Let Yij be the jth random observation in the ith treatment group.
Assume a ﬁxed eﬀects experiment:
Yij = µ + αi + ϵij,
i = 1, 2, . . . , k,
j = 1, 2, . . . , ni
parameter vector β = [µ, α1, α2, . . . , αk]T
Examples:
(1) H0: α1 = α2 = · · · = αk
Ha: αi ̸= αj for some i ̸= j
General linear hypothesis: nh = k −1
c
⃝2000 by Chapman & Hall/CRC



0
1 −1
0
0
· · ·
0
0
1
0 −1
0
· · ·
0
0
1
0
0 −1
· · ·
0
...
...
...
...
...
...
...
0
1
0
0
0
· · · −1




µ
α1
α2
α3
...
αk


=


0
0
0
0
...
0


C(k−1)(k+1)
β
= 0k−1
(2) H0: α1 = α2 = · · · = αk = 0
Ha: αi ̸= 0 for at least one i
General linear hypothesis: nh = k


0
1
0
0
0
· · ·
0
0
0
1
0
0
· · ·
0
0
0
0
1
0
· · ·
0
0
0
0
0
1
· · ·
0
...
...
...
...
...
...
...
0
0
0
0
0
· · ·
1




µ
α1
α2
α3
...
αk


=


0
0
0
0
...
0


C(k)(k+1)
β
=
0k
(3) Suppose i = 1, 2, 3, 4.
H0: −α1 + 2α2 −α3 = 0 (Quadratic contrast of three eﬀects)
Ha: −α1 + 2α2 −α3 ̸= 0
General linear hypothesis: nh = 1
[0,
−1,
+2,
−1,
0]


µ
α1
α2
α3
α4


= 0
C
β
= 0
16.5.4
Multiple linear regression
Model: Let there be n observations of the form (x1i, x2i, . . . , xki, yi) such
that yi is an observed value of the random variable Yi. Assume there exist
constants β0, β1, . . . , βk such that
Yi = β0 + β1x1i + · · · + βkxki + ϵi
parameter vector β = [β0, β1, β2, . . . , βk]T
where ϵ1, ϵ2, . . . , ϵn are independent, normal random variables having mean 0
and variance σ2.
c
⃝2000 by Chapman & Hall/CRC

Examples:
(1) H0: β1 = 0
Ha: β1 ̸= 0
General linear hypothesis: nh = 1
[0,
1,
0,
0, · · · ,
0]


β0
β1
β2
...
βk


= 0
C
β
= 0
(2) H0: β1 = β2 = β3 = · · · = βk = 0
Ha: βi ̸= 0 for some i
General linear hypothesis: nh = k


0
1
0
0
0
· · ·
0
0
0
1
0
0
· · ·
0
0
0
0
1
0
· · ·
0
0
0
0
0
1
· · ·
0
...
...
...
...
...
...
...
0
0
0
0
0
· · ·
1




β0
β1
β2
β3
...
βk


=


0
0
0
0
...
0


C(k)(k+1)
β
=
0k
(3) H0: β1 = β2 = 0
Ha: βi ̸= 0 for some i
General linear hypothesis: nh = 2

0
1
0
0
0
· · ·
0
0
0
1
0
0
· · ·
0



β0
β1
β2
...
βk


=

0
0

C
β
=
0
16.5.5
Randomized blocks (one observation per cell)
Model: Let Yij be the random observation in the ith row and the jth column,
i = 1, 2, 3 and j = 1, 2, 3, 4. Assume a ﬁxed eﬀects model:
Yij = µ + αi + βj + ϵij
parameter vector β = [µ, α1, α2, α3, β1, β2, β3, β4]T
c
⃝2000 by Chapman & Hall/CRC

Examples:
(1) H0: α1 = α2 = α3
Ha: αi ̸= αj for some i ̸= j
General linear hypothesis: nh = 2

0
1
−1
0
0
0
0
0
0
1
0
−1
0
0
0
0



µ
α1
α2
α3
β1
β2
β3
β4


=

0
0

C
β
=
0
(2) H0: −α1 + 2α2 −α3 = 0 (quadratic contrast)
Ha: −α1 + 2α2 −α3 ̸= 0
General linear hypothesis: nh = 1
[0, −1,
2, −1,
0,
0,
0,
0]


µ
α1
α2
α3
β1
β2
β3
β4


= 0
C
β
= 0
16.5.6
Quadratic form due to hypothesis
For the general linear model
Y = Xβ + ϵ,
E [Y] = Xβ,
Var [Y] = σ2I
(16.32)
the normal equations are given by
(XTX).β = XTY.
(16.33)
If the model is of full rank ((XTX) has an inverse) then the estimate of β is
.β = (XTX)−1XTY
(16.34)
c
⃝2000 by Chapman & Hall/CRC

and the variance–covariance matrix of .β is given by
Var
"
.β
#
= (XTX)−1 Var[XTY](XTX)−1
= (XTX)−1XT Var [Y]X(XTX)−1
= σ2(XTX)−1XTX(XTX)−1
= σ2(XTX)−1
(16.35)
If the null hypothesis is H0: Cβ = 0 then C.β is an unbiased estimate of Cβ
and
E
"
C.β
#
= Cβ = 0
(16.36)
Var
"
C.β
#
= C Var[.β]CT
= σ2C(XTX)−1CT,
(16.37)
"
Var
"
C.β
##−1
= 1
σ2 [C(XTX)−1CT]−1.
(16.38)
Under the null hypothesis, the standard quadratic form is
SSH = 1
σ2 .β
TCT[C(XTX)−1CT]−1C.β.
(16.39)
The expression in equation (16.39) is the sum of squares due to hypothesis
(denoted SSH). If Y has a multivariate normal distribution, then SSH/σ2 has
a chi–square distribution with nh degrees of freedom.
16.5.7
Sum of squares due to error
For the general linear model Y = Xβ + ϵ let e = Y −X.β, the error of
estimation. The sum of squares due to error is given by
SSE =
n

i=1
e2
i = eTe
= (Y −X.β)T(Y −X.β)
= YTY −2.β
TXTY + .β
TXTX.β
= YTY −2.β
TXTY + .β
TXTX(XTX)−1XTY
= YTY −2.β
TXTY + .β
TIXTY
= YTY −.β
TXTY.
(16.40)
Thus, SSE is obtained by computing the sum of squares of all observations
(YTY) and subtracting the scalar product of the vector of estimates of .β and
the vector on the right–hand side of the normal equations.
c
⃝2000 by Chapman & Hall/CRC

The sum of squares due to error may depend only on the model, and is
determined once the model is stated. SSE is independent of any hypothesis
which may be stated or tested.
If Y has a multivariate normal distribution, then SSE/σ2 has a chi–square
distribution with ne degrees of freedom and is independent of any SSH.
16.5.8
Summary
For the general linear model
Y = Xβ + ϵ,
E [Y] = Xβ
(16.41)
suppose the model is of full rank (XTX is non–singular and thus has an
inverse). If
Var[Y] = σ2I
(homoscedasticity and independence)
(16.42)
then the normal equations are given by
(XTX).β
T = XTY
(16.43)
and the estimate of β is
.β = (XTX)−1XTY.
(16.44)
If the elements of Y are normally distributed, then the following hypothesis
test may be conducted:
H0 : Cβ = 0
Ha : Cβ = d (̸= 0)
(16.45)
This hypothesis matrix has nh rows.
If H0 is consistent and contains no
redundancies then nh is the degrees of freedom due to hypothesis.
16.5.9
Computation procedure for hypothesis testing
A procedure for testing a hypothesis in a general linear model (equation
(16.45)):
(1) Obtain the sum of squares due to hypothesis:
SSH = .β
TCT 
C(XTX)−1CT−1 C.β.
(16.46)
(2) Obtain the sum of squares due to error:
SSE = YTY −.β
TXTY.
(16.47)
(3) Let ne = n −np = (sample size) −(number of eﬀective parameters in
the model).
(4) If the null hypothesis, H0, is true, then
SSH/nk
SSE/ne
(16.48)
c
⃝2000 by Chapman & Hall/CRC

has an F distribution with nk and ne degrees of freedom.
16.5.10
Regression signiﬁcance test
For the general linear model
Y = Xβ + ϵ,
E [Y] = Xβ,
Var[Y] = σ2I
(16.49)
the normal equations, SSE, and an estimate of β are given by
XTX.β = XTY
SSE = YTY −.β
TXTY
.β = (XTX)−1XTY.
(16.50)
Suppose the reduced model is given by
Y = Xβr + ϵ,
Cβr = 0.
(16.51)
The sum of squares due to hypothesis is given by
SSH = SSE(R) −SSE.
(16.52)
16.5.11
Alternate form of the distribution
The quotient
B =
SSE
SSE + SSH
(16.53)
has a beta distribution with parameters ne/2 and nh/2. Using this distribution,
the hypothesis tests are lower-tailed; reject H0 if the value of the test statistic
B is smaller than the critical value. Thus the rejection region is given by
B =
SSE
SSE(R) ≤β
ne
2 , nh
2

(usual notation) or,
≤β∗(nh, ne)
(beta percentage point), or
≤I
ne
2 , nh
2

(incomplete beta function).
16.6
GENERAL LINEAR MODEL OF LESS THAN FULL RANK
A singular general linear model is not of full rank. For a general linear model
Y = Xβ + ϵ
(16.54)
with normal equations
XTX.β = XTy
(16.55)
suppose the rank of the design matrix X is r (with r < m). Then the matrix
(XTX) is singular and has no inverse; there are no unbiased estimates for
each βi. However, there may exist unbiased estimates for certain functions of
the βi’s.
c
⃝2000 by Chapman & Hall/CRC

16.6.1
Estimable function and estimability
Suppose wTβ is a function of the βi’s where wT is a given vector of weights.
An estimator for wTβ is a linear function of the Y ’s, cTY = wT.β, such that
E

cTY

= wTβ for all β
(16.56)
and the variance is a minimum, i.e., Var

cTy

= minimum. The unbiasedness
constraints are
E

cTY

= wTβ
cTE [Y] = wTβ
cTXβ = wTβ
for all β
cTX = wT
cTX −wT = 0.
(16.57)
Therefore, the variance, Var

cTY

= σ2cTc, must be a minimum subject to
the constraints cTX = wT.
The criterion function Φ is
Φ = 1
2cTc −(cTX −wT)λ
and
(16.58)
∂Φ
∂c = c −Xλ.
(16.59)
In equation (16.59), set the derivative equal to zero to obtain
Xλ = ˆc.
(16.60)
Premultiply by XT:
XTXλ = XTˆc
(16.61)
which is equal to w under the constraints. Therefore,
XTXλ = w.
(16.62)
Equations (16.60) and (16.62) are the conjugate normal equations. If X
has rank r (with r < m) there will always be r columns that form a basis
with the remaining m−r columns as an extension, linear combinations of the
basis.
For the general linear model
Y = Xβ + ϵ
(16.63)
partition the elements of β and the columns of X such that
βT = [βT
1 , βT
2 ]
(16.64)
X = [X1, X2]
(16.65)
with dimensions given by: β1 is r × 1, β2 is (m −r) × 1, X1 is r × r, and X2
is (m −r) × r, such that X1 is a basis for X. The columns of X2 must be
c
⃝2000 by Chapman & Hall/CRC

linear combinations of those in X1. Therefore, there exists a matrix Qr(m−r)
such that X2 = X1Q. If X1 and X2 are given, suppose
X2 = X1Q
and
(16.66)
XT
1 X2 = XT
1 X1Q
Q = (XT
1 X1)−1XT
1 X2.
(16.67)
Often, Q may be found by inspection.
Using equation (16.66) the matrix X may be written as
X =

X1 X2

=

X1 X1Q

= X1

Ir Q

(16.68)
and the conjugate normal equation (16.62) may be written as

XT
1
QTXT
1
 
X1 X1Q

λ =

XT
1 X1
XT
1 X1Q
QTXT
1 X1 QTXT
1 X1Q

λ =

w1
w2

(16.69)
where w1 is r × 1 and w2 is (m −r) × 1. Equating components:

XT
1 X1 XT
1 X1Q

λ = w1
(16.70)

QTXT
1 X1 QTXT
1 X1Q

λ = w2.
(16.71)
Premultiply equation (16.70) by QT to obtain

QTXT
1 X1 QTXT
1 X1Q

λ = QTw1.
(16.72)
In order for the system to be consistent, the condition
w2 = QTw1
(16.73)
must be true. Therefore, in the function wβ, the weight vector w must be of
the form wT = [wT
1 , wT
2 ] where
wT
2 = wT
1 Q.
(16.74)
If the vector of weights w is of this form then there is a linear unbiased (and
mathematically consistent) estimate for the function wβ.
Equation (16.74) is the condition of estimability of a linear function. A
function wTβ is estimable if wT can be written as wT =

wT
1 , wT
2

where
wT
2 is related to wT
2 in the same way as X2 is related to X1.
A parametric function is linearly estimable if there exists a linear combina-
tion of the observations whose expected value is equal to the function, i.e., if
there exists an unbiased estimate.
If the function wTβ is estimable, equation (16.70) may be written as
(XT
1 X1)

I Q

λ = w1.
(16.75)
The equation involving w2 may be disregarded since w2 is completely deter-
mined by the relation w2 = QTw1. Therefore
I Q
λ = (XT
1 X1)−1w1.
(16.76)
c
⃝2000 by Chapman & Hall/CRC

Rewriting the ﬁrst conjugate normal equation (16.60) yields
Xλ = ˆc

X1 X2

λ = ˆc
X1

I Q

λ = ˆc.
(16.77)
Using equation (16.76):
X1(XT
1 X1)−1w1 = ˆc
and
(16.78)
ˆcy = 0
wTβ = w1(XT
1 X1)−1XT
1 y
(16.79)
which is of the same form as in the non–singular case, except that X has been
replaced by its basis X1 and in w only the ﬁrst r elements are considered:
w1.
The normal equations in the method of least squares,
(XTX).β
T = XTy
(16.80)
may be used formally in the reduced statement
(XT
1 X1).β
T
1 = XT
1 y.
(16.81)
16.6.2
General linear hypothesis model of less than full rank
For a general linear model of less than full rank:
Y = Xβ + ϵ
=

X1X2
 
β1
β2

+ ϵ
=

X1X1Q
 
β1
β2

+ ϵ
= X1β1 + X1Qβ2 + ϵ
= X1(β1 + Qβ2) + ϵ.
(16.82)
Therefore, this general linear model may be written in the form Y = X1β∗+ϵ
where β∗= β1 + Qβ2.
16.6.2.1
Sum of squares due to error
Since ϵ has not changed in this model, the normal equations are
(XT
1 X1).β
∗= XT
1 y,
and the sum of squares due to error is
SSE = eTe = yTy −[.β
∗]TXT
1 y
= yTy −(XT
1 X1)−1XT
1 yXT
1 y.
The expression SSE/σ2 has a chi–square distribution with n −r degrees of
freedom where r is the rank of X. The eﬀective number of parameters in the
c
⃝2000 by Chapman & Hall/CRC

singular model is only r, while the remaining m−r parameters are determined
in terms of the ﬁrst r by the estimability condition.
16.6.2.2
Sum of squares due to hypothesis
Suppose the null hypothesis is given by
H0 : Cβ = 0
where
C =

C1 C2

(16.83)
and C1 has dimension r × r and C2 has dimension r × (m −r). Equation
(16.83) implies

C1 C2
 
β1
β2

=

0
0

.
(16.84)
The left–hand side of this equation must represent an estimable function,
therefore
C2 = C1Q.
(16.85)
Equation (16.85) is the condition of testability: if
C =


cT
1
cT
2
...
cT
nh


where (cT
i β) is an estimable function (i = 1, 2, . . . , nh), then the null hypoth-
esis
H0 : C1β1 + C2β2 = 0
may be written as
H0 : C1β1 + C1Qβ2 = 0
or simply
H0 : C1β∗= 0,
where β∗= (β1 + Qβ2).
Therefore, a null hypothesis H0: Cβ = 0 is testable if Cβ consists of nh
estimable functions, i.e., if C2 = C1Q, where C = [C1, C2]. The sum of
squares due to hypothesis is given by
SSH = (.β
∗)TCT
1 [C1(XT
1 X1)−1CT
1 ]−1C1.β
∗
where .β
∗= (XT
1 X1)−1XT
1 y. The expression SSH/σ2 has a chi–square dis-
tribution with nh degrees of freedom.
If the null hypothesis is true, then
SSH/nk
SSE/ne
has an F distribution with nk and ne degrees of freedom.
c
⃝2000 by Chapman & Hall/CRC

16.6.3
Constraints and conditions
If the general linear model is singular of rank r < m, then (m−r) constraints
on the .βi’s (the estimates) may be arbitrarily introduced, for example
ˆβr+1 = 0, . . . , ˆβm = 0
or
(16.86)
m

i=1
ˆβi = 0,
m

i=1
ni ˆβi = 0.
(16.87)
This procedure reparameterizes the model. The constraining functions are
fairly arbitrary, but they must not be estimable functions, otherwise the re-
sulting model will still be singular.
To apply the constraints in equation
(16.86), delete the last (m−r) rows and columns of XTX and the last (m−r)
elements of XTy. To apply the constraints in equation (16.87), add a constant
to all elements of XTX. This has no eﬀect on the value of estimable functions
or test statistics.
A diﬀerent situation arises if conditions are placed on the parameters in a
model, especially on interaction terms. In a two-factor, ﬁxed eﬀects experi-
ment, the model is given by
Yijk = µ + αi + βj + (αβ)ij + ϵijk
(16.88)
with assumptions on the interaction terms
a

i=1
(αβ)ij =
b

j=1
(αβ)ij = 0.
(16.89)
The assumptions in equation (16.89) are often called natural constraints
(even though the are neither natural nor constraints).
These assumptions
represent a set of conditions on the interactions, minimizing this eﬀect (making
SSH for interaction a minimum). Given these assumptions, the model is still
singular, but can be made nonsingular by introducing the arbitrary constraints
a

i=1
ˆαi = 0,
b

j=1
ˆβj = 0.
(16.90)
Using the diﬀerent assumptions
All αi’s = 0,
All βj’s = 0
(16.91)
would simplify the model to a one-way anova.
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 17
Miscellaneous Topics
Contents
17.1
Geometric probability
17.2
Information and communication theory
17.2.1
Discrete entropy
17.2.2
Continuous entropy
17.2.3
Channel capacity
17.2.4
Shannon’s theorem
17.3
Kalman ﬁltering
17.3.1
Extended Kalman ﬁltering
17.4
Large deviations (theory of rare events)
17.4.1
Theory
17.4.2
Sample rate functions
17.4.3
Example: Insurance company
17.5
Markov chains
17.5.1
Transition function
17.5.2
Transition matrix
17.5.3
Recurrence
17.5.4
Stationary distributions
17.5.5
Random walks
17.5.6
Ehrenfest chain
17.6
Martingales
17.6.1
Examples of martingales
17.7
Measure theoretical probability
17.8
Monte Carlo integration techniques
17.8.1
Importance sampling
17.8.2
Hit-or-miss Monte Carlo method
17.9
Queuing theory
17.9.1
M/M/1 queue
17.9.2
M/M/1/K queue
17.9.3
M/M/2 queue
17.9.4
M/M/c queue
17.9.5
M/M/c/c queue
c
⃝2000 by Chapman & Hall/CRC

17.9.6
M/M/c/K queue
17.9.7
M/M/∞queue
17.9.8
M/Ek/1 queue
17.9.9
M/D/1 queue
17.10
Random matrix eigenvalues
17.10.1
Random matrix products
17.11
Random number generation
17.11.1
Pseudorandom number generation
17.11.2
Generating nonuniform random variables
17.11.3
References
17.12
Resampling methods
17.13
Self-similar processes
17.13.1
Deﬁnitions
17.13.2
Self-similar processes
17.14
Signal processing
17.14.1
Estimation
17.14.2
Matched ﬁltering (Wiener ﬁlter)
17.14.3
Median ﬁlter
17.14.4
Mean ﬁlter
17.14.5
Spectral decompositions
17.15
Stochastic calculus
17.15.1
Brownian motion (Wiener processes)
17.15.2
Brownian motion expectations
17.15.3
Itˆo lemma
17.15.4
Stochastic integration
17.15.5
Stochastic diﬀerential equations
17.15.6
Motion in a domain
17.15.7
Option Pricing
17.16
Classic and interesting problems
17.16.1
Approximating a distribution
17.16.2
Averages over vectors
17.16.3
Bertrand’s box “paradox”
17.16.4
Bertrand’s circle “paradox”
17.16.5
Bingo cards: nontransitive
17.16.6
Birthday problem
17.16.7
Buﬀon’s needle problem
17.16.8
Card problems
17.16.9
Coin problems
17.16.10 Coupon collectors problem
17.16.11 Dice problems
17.16.12 Ehrenfest urn model
17.16.13 Envelope problem “paradox”
c
⃝2000 by Chapman & Hall/CRC

17.16.14 Gambler’s ruin problem
17.16.15 Gender distributions
17.16.16 Holtzmark distribution: stars in the galaxy
17.16.17 Large-scale testing
17.16.18 Leading digit distribution
17.16.19 Lotteries
17.16.20 Match box problem
17.16.21 Maximum entropy distributions
17.16.22 Monte Hall problem
17.16.23 Multi-armed bandit problem
17.16.24 Parking problem
17.16.25 Passage problems
17.16.26 Proofreading mistakes
17.16.27 Raisin cookie problem
17.16.28 Random sequences
17.16.29 Random walks
17.16.30 Relatively prime integers
17.16.31 Roots of a random polynomial
17.16.32 Roots of a random quadratic
17.16.33 Simpson paradox
17.16.34 Secretary call problem
17.16.35 Waiting for a bus
17.17
Electronic resources
17.17.1
Statlib
17.17.2
Uniform resource locators
17.17.3
Interactive demonstrations and tutorials
17.17.4
Textbooks, manuals, and journals
17.17.5
Free statistical software packages
17.17.6
Demonstration statistical software packages
17.18
Tables
17.18.1
Random deviates
17.18.2
Permutations
17.18.3
Combinations
17.1
GEOMETRIC PROBABILITY
1. Two points on a ﬁnite line:
If A and B are uniformly chosen from the interval [0, 1), and X is the
distance between A and B (that is, X = |A −B|) then the probability
density of X if fX(x) = 2(1 −x) for 0 ≤x ≤1.
c
⃝2000 by Chapman & Hall/CRC

2. Many points on a ﬁnite line:
Suppose n −1 values are randomly selected from a uniform distribution
on the interval [0, 1). These n −1 values determine n intervals.
Pk(x) = Probability (exactly k intervals have length larger than x)
=
n
k
'
[1 −kx]n−1 −
n −1
1

[1 −(k + 1)x]n−1 +
· · · + (−1)s
n −k
s

[1 −(k + s)x]n−1
,
(17.1)
where s =
5 1
x −k
6
. Using this, the probability that the largest interval
length exceeds x is (for 0 ≤x ≤1):
1 −P0(x) =
n
1

(1 −x)n−1 −
n
2

(1 −2x)n−1 + . . .
(17.2)
3. Points in the plane:
Suppose the number of points in any region of area A of the plane is
a Poisson random variable with mean λA (i.e., λ is the density of the
points). Given a ﬁxed point P deﬁne R1, R2, . . . , to be the distance
to the point nearest to P, second nearest to P, etc. The probability
density function for Rs is (for 0 ≤r ≤∞):
fRs(r) = 2(λπ)s
(s −1)!r2s−1e−λπr2
(17.3)
4. Points on a checkerboard:
Consider the unit squares on a checkerboard and select one point uni-
formly in each square. The following results concern the distance be-
tween points, on average.
(a) For adjacent squares (such as a black and white square) the mean
distance between points is 1.088.
(b) For diagonal squares (such as between two white squares) the mean
between points is 1.473.
5. Points in three-dimensional space:
Suppose the number of points in any volume V is a Poisson random
variable with mean λV (i.e., λ is the density of the points). Given a
ﬁxed point P deﬁne R1, R2, . . . , to be the distance to the point nearest
to P, second nearest to P, etc. The probability density function for Rs
is (for 0 ≤r ≤∞):
fRs(r) = 3
 4
3λπ
s
(s −1)! r3s−1e−4
3 λπr3
(17.4)
c
⃝2000 by Chapman & Hall/CRC

6. Points in a cube:
Choose two points uniformly in a unit cube. The distance between these
points has mean 0.66171 and standard deviation 0.06214.
7. Points in n-dimensional cubes:
Let two points be selected randomly from a unit n-dimensional cube.
The expected distance between the points, ∆(n), is
∆(1) = 1/3
∆(2) ≈0.54141 . . .
∆(3) ≈0.66171 . . .
∆(4) ≈0.77766 . . .
∆(5) ≈0.87852 . . .
∆(6) ≈0.96895 . . .
∆(7) ≈1.05159 . . .
∆(8) ≈1.12817 . . .
8. Points on a circle:
Select three points at random on a unit circle. These points determine
a triangle with area A. The mean and variance of area are:
µA = 3
2π ≈0.4775
σ2
A = 3

π2 −6

8π2
≈0.1470
(17.5)
9. Particle in a box:
A particle is bouncing randomly in a square box with unit side. On
average, how far does it travel between bounces?
Suppose the particle is initially at some random position in the box
and is traveling in a straight line in a random direction and rebounds
normally at the edges. Let θ be the angle of the point’s initial vector.
After traveling a distance r (where r ≫1; think of many adjacent boxes
and the particle exits each box and enters the next box), the point
has moved r cos θ horizontally and r sin θ vertically, and thus has struck
r(sin θ + cos θ) + O(1) walls. Hence the average distance between walls
is 1/(sin θ + cos θ). Averaging this over all angles results in
2
π
 π/2
0
dθ
sin θ + cos θ = 2
√
2
π
ln(1 +
√
2) ≈0.793515
(17.6)
See J. G. Berryman, Random close packing of hard spheres and disks, Physical
Review A, 27, pages 1053–1061, 1983 and H. Solomon, Geometric Probability,
SIAM, Philadelphia, PA, 1978.
17.2
INFORMATION AND COMMUNICATION THEORY
17.2.1
Discrete entropy
Suppose X is a discrete random variable that assumes n distinct values. Let
pX be the probability distribution for X and Prob [X = x] = px. The entropy
c
⃝2000 by Chapman & Hall/CRC

Figure 17.1: Binary entropy function
of the distribution is
H(pX) = −

x
px log2 px.
(17.7)
The units for entropy is bits. The maximum value of H(pX) is log2 n and is
obtained when X is a discrete uniform random variable that assumes n values.
Entropy measures how much information is gained from observing the value
of X.
If X assumes only two values, pX = (p, 1 −p), and
H(pX) = H(p) = −p log2 p −(1 −p) log2(1 −p)
(17.8)
with a maximum at p = 0.5. A plot of H(p) is in Figure 17.1.
Given two discrete random variables X and Y , pX×Y is the joint distribution
of X and Y . The mutual information of X and Y is deﬁned by
I(X, Y ) = H(pX) + H(pY ) −H(pX×Y )
(17.9)
Note that I(X, Y ) ≥0 and that I(X, Y ) = 0 if and only if X and Y are
independent. Mutual information gives the amount of information obtained
about X after observing a value of Y (and vice versa).
Example 17.78:
A coin weighing problem.
There are 12 coins of which one is
counterfeit, diﬀering from the others by its weight. Using a balance but no weights, how
many weighings are necessary to identify the counterfeit coin?
Solution:
(S1) Any of the 12 coins may turn out to be the counterfeit one, and it may be heavier
or lighter than the genuine ones. Hence, there are 24 possible outcomes. For
equal probabilities of these 24 outcomes, the entropy of the unknown result is
then log2 24 ≈4.58.
(S2) Each weighing process has three outcomes (equal weight, left side heavier, right
side heavier).
Using an assumption of equal probabilities gives an entropy of
log2 3 ≈1.58 per weighing. (Note that other assumptions will produce a smaller
entropy.)
(S3) Therefore the minimal number of weighings cannot be less that 4.58/1.58 ≈2.90.
Hence 3 weighings are needed. (In fact, 3 weighings are suﬃcient.)
c
⃝2000 by Chapman & Hall/CRC

17.2.2
Continuous entropy
For a d-dimensional continuous random variable X, the entropy is
h(X) = −

Rd p(x) log p(x) dx
(17.10)
Continuous entropy is not the the limiting case of the entropy of a discrete
random variable. In fact, if X is the limit of the one-dimensional discrete
random variable {Xn}, and the entropy of X is ﬁnite, then
lim
n→∞(H(Xn) −n log 2) = h(X)
(17.11)
If X and Y are continuous d-dimensional random variables with density func-
tions p(x) and q(y), then the relative entropy is
H(X, Y) =

Rd p(x) log p(x)
q(x) dx
(17.12)
A d-dimensional Gaussian random variable N(a, Γ) has the density function
g(x) =
1
(2π)d/2
|Γ|
exp

−1
2(x −a)TΓ−1(x −a)

(17.13)
where a is the vector of means and Γ is the positive deﬁnite covariance matrix.
1. If X = (X1, X2, . . . , Xd) is a d-dimensional Gaussian random vector
with distribution N(a, Γ) then
h(X) = 1
2 log

(2πe)d |Γ|

(17.14)
2. If X and Y are d-dimensional Gaussian random vectors with distribu-
tions N(a, Γ) and N(b, ∆) then
H(X, Y) = 1
2

log |∆|
|Γ| + tr

Γ

∆−1 −Γ−1
+ (a −b)T∆−1(a −b)

(17.15)
3. If X is a d-dimensional Gaussian random vector with distribution N(a, Γ),
and if Y is a d-dimensional random vector with a continuous probability
distribution having the same covariance matrix Γ, then
h(X) ≤h(Y)
(17.16)
17.2.3
Channel capacity
The transition probabilities are deﬁned by tx,y = Prob [X = x | Y = y]. The
distribution pX determines pY by py = 
x tx,ypx. The matrix T = (tx,y) is
c
⃝2000 by Chapman & Hall/CRC

the transition matrix. The matrix T deﬁnes a channel given by a transition
diagram (input is X, output is Y ). For example (here X and Y only assume
two values):
t
x1
t
x0
ty1
ty0
✟✟✟✟✟✟✟✟✟✟
✯
✲
❍❍❍❍❍❍❍❍❍❍
❥
✲
tx1,y1
tx0,y0
tx0,y1
tx1,y0
The capacity of the channel is deﬁned as
C = max
pX I(X, Y )
(17.17)
A channel is symmetric if each row is a permutation of the ﬁrst row and the
transition matrix is a symmetric matrix. The capacity of a symmetric channel
is C = log2 n −H(p), where p is the ﬁrst row. The capacity of a symmetric
channel is achieved with equally likely inputs. The channel below on the left
is symmetric; both channels achieve capacity with equally likely inputs.
Binary symmetric channel
Binary erasure channel
t
1
t
0
t1
t0
✟✟✟✟✟✟✟✟✟✟
✯
✲
❍❍❍❍❍❍❍❍❍❍
❥
✲
1 −p
1 −p
p
p
t
1
t
0
t1
t?
t0
✘✘✘✘✘✘✘✘✘✘
✿
✲

z
✲
1 −p
1 −p
p
p
C = 1 −H(p)
C = 1 −p
17.2.4
Shannon’s theorem
Let both X and Y be discrete random variables with values in an alphabet A.
A code is a set of n-tuples (codewords) with entries from A that is in one-
to-one correspondence with M messages. The rate R of the code is deﬁned
as 1
n log2 M. Assume that the codeword is sent via a channel with transition
matrix T by sending each vector element independently. Deﬁne
e =
max
all codewords Prob [codeword incorrectly decoded].
(17.18)
Then Shannon’s coding theorem states:
(a) If R < C, then there is a sequence of codes with n →∞such that
e →0.
(b) If R ≥C, then e is always bounded away from 0.
c
⃝2000 by Chapman & Hall/CRC

17.3
KALMAN FILTERING
In the following model for k ≥0:
xk+1 = Fkxk + Gkwk
zk = HT
k xk + vk
(17.19)
with the conditions:
1. The initial state x0 is a Gaussian random variable with mean x0 and
covariance P0, independent of {vk} and {wk}.
2. The {vk} and {wk} are independent, zero mean, Gaussian white pro-
cesses with
E

vkvT
l

= Rkδkl
and
E

wkwT
l

= Qkδkl
(17.20)
an estimate of xk from observation of the zi’s is desired.
1. Deﬁne Zk−1 to be the sequence of observed values {z0, z1, . . . , zk−1}
2. Deﬁne the estimate of xk, conditioned on the z values (up to the (k−1)th
value) to be .xk/k−1 = E [xk | Zk−1]. Similarly, deﬁne the estimate of xk,
conditioned on the z values (up to the kth value) to be .xk/k = E [xk | Zk].
3. Deﬁne the error covariance matrix to be
Σk/k−1 = E
"
xk −.xk/k−1
 
xk −.xk/k−1
T | Zk−1
#
. // Deﬁne Σk/k in
a similar way.
4. By convention, deﬁne .xk/k−1 for k = 0 (i.e., .x0/−1) to be x0 = E [x0],
i.e., the expected value of x0 given no measurements. Similarly, take
Σ0/−1 to be P0.
The solution is given by (the intermediate matrix Kk is called the gain matrix)
.x0/−1 = x0
Σ0/−1 = P0
Ωk = HT
k Σk/k−1Hk + Rk
Kk = FkΣk/k−1HkΩ−1
k
.xk+1/k =

Fk −KkHT
k
 .xk/k−1 + Kzzk
.xk/k = .xk/k−1 + Σk/k−1HkΩ−1
k

zk −HT
k .xk/k−1

Σk/k = Σk/k−1 −Σk/k−1HkΩ−1
k HT
k Σk/k−1
Σk+1/k = FkΣk/kF T
k + GkQkGT
k
(17.21)
See B. D. O. Anderson and J. B. Moore, Optimal Filtering, Prentice–Hall,
Inc., Englewood Cliﬀs, NJ, 1979.
c
⃝2000 by Chapman & Hall/CRC

17.3.1
Extended Kalman ﬁltering
We have the following model for k ≥0:
xk+1 = fk(xk) + gk(xk)wk
zk = hk(xk) + vk
(17.22)
with the usual assumptions. We presume the nonlinear functions {fk, gk, hk}
are suﬃciently smooth and they can be expanded in Taylor series about the
conditional means .xk/k and .xk/k−1 as
fk(xk) = fk(.xk/k) + Fk(xk −.xk/k) + . . .
gk(xk) = gk(.xk/k) + · · · = Gk + . . .
hk(xk) = hk(.xk/k−1) + HT
k (xk −.xk/k−1) + . . .
(17.23)
Neglecting higher order terms and assuming knowledge of .xk/k and .xk/k−1
enables us to approximate the original system as
xk+1 = Fkxk + Gkwk + uk
zk = HT
k xk + vk + yk
(17.24)
where uk and yk are calculated from
uk = fk(.xk/k) −Fk.xk/k
and
yk = hk(.xk/k−1) −HT
k .xk/k−1
(17.25)
The Kalman ﬁlter for this approximate signal model is:
.x0/−1 = x0
Σ0/−1 = P0
Ωk = HT
k Σk/k−1Hk + Rk
Lk = Σk/k−1HkΩ−1
k
.xk/k = .xk/k−1 + Lk

zk −hk(.xk/k−1)

.xk+1/k = fk(.xk/k)
Σk/k = Σk/k−1 −Σk/k−1HkΩ−1
k HT
k Σk/k−1
Σk+1/k = FkΣk/kF T
k + GkQkGT
k
(17.26)
See B. D. O. Anderson and J. B. Moore, Optimal Filtering, Prentice–Hall,
Inc., Englewood Cliﬀs, NJ, 1979.
17.4
LARGE DEVIATIONS (THEORY OF RARE EVENTS)
17.4.1
Theory
1. Cram´ers Theorem: Let {Xi} be a sequence of bounded, independent,
identically distributed random variables with common mean m. Deﬁne
c
⃝2000 by Chapman & Hall/CRC

Mn to be the sample mean of the ﬁrst n random variables:
Mn = 1
n (X1 + X2 + · · · + Xn)
(17.27)
The tails of the probability distribution for Mn decay exponentially, as
n →∞, at a rate given by the convex rate function I(x).
Prob [Mn > x] ∼e−nI(x)
for x > m
Prob [Mn < x] ∼e−nI(x)
for x < m
(17.28)
2. Chernoﬀ’s Formula: The rate-function I(x) is related to the cumulant
generating function λ(θ) (see page 38) via
I(x) = max
θ
{xθ −λ(θ)} .
(17.29)
3. Contraction Principle: If {Xn} satisﬁes a large deviation principle
with rate function I and f is a continuous function, then {f(Xn)} sat-
isﬁes a large deviation principle with rate function J, where J is given
by
J(y) = min [I(x) | f(x) = y] .
(17.30)
17.4.2
Sample rate functions
1. Let {Xi} be a sequence of Bernoulli random variables where p is the
probability of obtaining a “1” and (1−p) is the probability of obtaining
a “0”. Then λ(θ) = ln

p · eθ + (1 −p) · 1

and therefore
I(x) = x ln x
p + (1 −x) ln 1 −x
1 −p
(17.31)
(The maximum value of I occurs when θ is θ = ln x
p −ln(1−x)
1−p ).
2. If the random variables in the sequence {Xi} are all N(µ, σ2) then
I(x) = 1
2
x −µ
σ
2
.
(17.32)
17.4.3
Example: Insurance company
Suppose an insurance company collects daily premiums as a constant rate
p, and has daily claims total Z ∼N(µ, σ2).
The company would like to,
naturally, avoid going bankrupt. The probability that the payments exceed
income after T days is the probability that T
k=1 Zk exceeds pT. For T large
Prob
%
1
T
T

k=1
Zk

> p
&
∼e−T I(p)
(17.33)
c
⃝2000 by Chapman & Hall/CRC

If an acceptable amount of risk is e−r, then e−T I(p) = e−r, or I(p) = r/T.
Using the rate function for a normal random variable, r = T
2
 p−µ
σ
2, or
p = µ + σ
-
2r
T .
(17.34)
The safety loading is deﬁned by
p −µ
µ



	
safety loading
=
σ
µ

  	
size of ﬂuctuations
-
2r
T



	
ﬁxed by regulators
(17.35)
17.5
MARKOV CHAINS
A discrete parameter stochastic process is a collection of random variables
{X(t), t = 0, 1, 2, . . . }.
The values of X(t) are called the states of the
process. The collection of states is called the state space. The values of t
usually represent points in time.
The number of states is either ﬁnite or
countably inﬁnite. A discrete parameter stochastic process is called a Markov
chain if, for any set of n time points t1 < t2 < · · · < tn, the conditional
distribution of X(tn) given values for X(t1), X(t2), . . . , X(tn−1) depends
only on X(tn−1). That is,
Prob [X(tn) ≤xn | X(t1) = x1, . . . , X(tn−1) = xn−1]
= Prob [X(tn) ≤xn | X(tn−1) = xn−1].
(17.36)
A Markov chain is said to be stationary if the value of the conditional prob-
ability P [X(tn+1) = xn+1 | X(tn) = xn] is independent of n. The following
discussion will be restricted to stationary Markov chains.
17.5.1
Transition function
Let x and y be states and let {tn} be time points in T = {0, 1, 2, . . . }. The
transition function, P(x, y), is deﬁned by
P(x, y) = Pn,n+1(x, y) = Prob [X(tn+1) = y | X(tn) = x]
(17.37)
P(x, y) is the probability that a Markov chain in state x at time n will be
in state y at time n + 1.
Some properties of the transition function are:
P(x, y) ≥0 and 
y P(x, y) = 1. The values of P(x, y) are commonly called
the one-step transition probabilities.
The function π0(x) = P(X(0) = x), with π0(x) ≥0 and 
x π0(x) = 1,
is called the initial distribution of the Markov chain. It is the probability
distribution when the chain is started. Thus,
P [X(0) = x0, X(1) = x1, . . . , X(n) = xn]
= π0(x0)P0,1(x0, x1)P1,2(x1, x2) · · · Pn−1,n(xn−1, xn).
(17.38)
c
⃝2000 by Chapman & Hall/CRC

17.5.2
Transition matrix
A convenient way to summarize the transition function of a Markov chain is
by using the one-step transition matrix. It is deﬁned as
P =


P(0, 0) P(0, 1) . . . P(0, n) . . .
P(1, 0) P(1, 1) . . . P(1, n) . . .
...
...
...
...
P(n, 0) P(n, 1) . . . P(n, n) . . .
...
...
...


.
(17.39)
Deﬁne the n–step transition matrix by P (n) to be the matrix with entries
P (n)(x, y) = Prob [X(tm+n) = y | X(tm) = x].
(17.40)
This can be written using the one-step transition matrix as P (n) = P n.
Suppose the state space is ﬁnite. The one-step transition matrix is said to be
regular if, for some positive power m, all of the elements of P m are strictly
positive.
Theorem 1 (Chapman–Kolmogorov equation) Let P(x, y) be the one-step
transiton function of a Markov chain and deﬁne P 0(x, y) = 1, if x = y, and
0, otherwise. Then, for any pair of nonnegative integers s and t such that
s + t = n,
P n(x, y) =

z
P s(x, z)P t(z, y).
(17.41)
17.5.3
Recurrence
Deﬁne the probability that a Markov chain starting in state x returns to state
x for the ﬁrst time after n steps by
f n(x, x) = Prob [X(tn) = x, X(tn−1) ̸= x, . . . , X(t1) ̸= x | X(t0) = x].
(17.42)
It follows that P n(x, x) = n
k=0 f k(x, x)P n−k(x, x). A state x is said to be
recurrent if ∞
n=0 f n(x, x) = 1. This means that a state x is recurrent if,
after starting in x, the probability of returning to x after some ﬁnite length
of time is one. A state which is not recurrent is said to be transient.
Theorem 2 A state x of a Markov chain is recurrent if and only if
∞
n=1 P n(x, x) = ∞.
Two states, x and y, are said to communicate if, for some n ≥0, P n(x, y) > 0.
This theorem implies that if x is a recurrent state and x communicates with
y, then y is also a recurrent state. A Markov chain is said to be irreducible if
every state communicates with every other state and with itself.
c
⃝2000 by Chapman & Hall/CRC

Let x be a recurrent state and deﬁne Tx the (return time) to be the number
of stages for a Markov chain to return to state x, having begun there. A
recurrent state x is said to be null recurrent if E [Tx] = ∞. A recurrent state
that is not null recurrent is said to be positive recurrent.
17.5.4
Stationary distributions
Let {X(t), t = 0, 1, 2, . . . } be a Markov chain having a one-step transition
function of P(x, y). A function π(x) where each π(x) is nonnegative, 
x π(x)P(x, y) =
π(y), and 
y π(y) = 1, is called a stationary distribution. If a Markov chain
has a stationary distribution and limn→∞P n(x, y) = π(y), then regardless of
the initial distribution, π0(x), the distribution of X(tn) approaches π(x) as n
becomes inﬁnite. When this happens π(x) is often referred to as the steady-
state distribution. The following categorizes those Markov chains that have
stationary distributions.
Theorem 3 Let XP denote the set of positive recurrent states of a Markov
chain.
1. If XP is empty, the chain has no stationary distribution.
2. If XP is a nonempty irreducible set, the chain has a unique stationary
distribution.
3. If XP is nonempty but not irreducible, the chain has an inﬁnite number
of distinct stationary distributions.
The period of a state x is denoted by d(x) and is deﬁned to be the greatest
common divisor of all integers n ≥1 for which P n(x, x) > 0. If P n(x, x) = 0
for all n ≥1 then deﬁne d(x) = 0. If each state of a Markov chain has d(x) = 1
the chain is said to be aperiodic. If each state has period d > 1 the chain is said
to be periodic with period d. The vast majority of Markov chains encountered
in practice are aperiodic. An irreducible, positive recurrent, aperiodic Markov
chain always possesses a steady-state distribution.
An important special case occurs when the state space is ﬁnite. In this case,
suppose that X = {1, 2, . . . , K}. Let π0 = {π0(1), π0(2), . . . , π0(K)}.
Theorem 4 Let P be a regular one-step transition matrix and π0 be an
arbitrary vector of initial probabilities. Then limn→∞π0(x)P n = y, where
yP = y, and K
i=1 π0(ti) = 1.
Example 17.79:
A Markov chain having three states, {0, 1, 2}, with a one-step transition matrix of
P =


1/2 0
1/2
1/4
3/4 0
0
3/4
1/4

is diagrammed as follows:
c
⃝2000 by Chapman & Hall/CRC

The one-step transition matrix gives a two–step transition matrix of
P (2) = P 2 =


1/4
3/8
3/8
5/16
9/16
1/8
3/16
9/16
1/8


The one-step transition matrix is regular. This Markov chain is irreducible, and all three
states are recurrent. In addition, all three states are positive recurrent. Since all states
have period 1, the chain is aperiodic.
The steady-state distribution is π(0) =
3/11,
π(1) = 6/11, and π(2) = 2/11.
17.5.5
Random walks
Let {η(t1), η(t2), . . . } be independent random variables having a common den-
sity f(x), and let t1, t2, . . . be integers.
Let X(t0) be an integer–valued
random variable that is independent of η(t1), η(t2), . . . , and X(tn) = X0 +
n
i=1 η(ti). The sequence {X(ti), i = 0, 1, . . . } is called a random walk. An
important special case is a simple random walk. It is deﬁned by the following.
P(x, y) =





p
if y = x −1
r
if y = x
q
if y = x + 1
,
where
p + q + r = 1,
P(0, 0) = p + r.
(17.43)
Here, an object begins at a certain point in a lattice and at each step either
stays at that point or moves to a neighboring lattice point. In the case of a
one– or two–dimensional lattice it turns out that if a random walk begins at
a lattice point, x, it will return to that point with probability 1. In the case
of a three–dimensional lattice the probability that it will return to its starting
point is approximately 0.3405.
17.5.6
Ehrenfest chain
A simple model of gas exchange between two isolated bodies is as follows.
Suppose that there are two boxes, Box I and Box II, where Box I contains K
molecules numbered 1, 2, . . . , K and Box II contains N −K molecules num-
bered K +1, K +2, . . . , N. A number is chosen at random from {1, 2, . . . , N},
c
⃝2000 by Chapman & Hall/CRC

and the molecule with that number is transferred from its box to the other
one. Let X(tn) be the number of molecules in Box I after n trials. Then the
sequence {X(tn), n = 0, 1, . . . } is a Markov chain with one–stage transition
function of
P(x, y) =







x
K
y = x −1,
1 −x
K
y = x + 1,
0
otherwise
(17.44)
17.6
MARTINGALES
A stochastic process {Zn | n ≥1} with E [|Zn|] < ∞for all n is a
(a) martingale process if E [Zn+1 | Z1, Z2, . . . , Zn] = Zn
(b) submartingale process if E [Zn+1 | Z1, Z2, . . . , Zn] ≥Zn
(c) supermartingale process if E [Zn+1 | Z1, Z2, . . . , Zn] ≤Zn
Azuma’s inequality: Let {Zn} be a martingale process with mean µ =
E [Zn]. Let Z0 = µ and suppose that −αi ≤(Zi −Zi−1) ≤βi for nonnegative
constants {αi, βi} and i ≥1. Then, for any n ≥0 and a > 0:
(a) Prob [Xn −µ ≥a] ≤exp

−2a2
* n
i=1
(αi + βi)2

(b) Prob [Xn −µ ≤−a] ≤exp

−2a2
* n
i=1
(αi + βi)2

17.6.1
Examples of martingales
(a) If {Xi} are independent, mean zero random variables, and Zn =
n
i=1
Xi,
then {Zn} is a martingale.
(b) If {Xi} are independent random variables with E [Xi] = 1, and Zn =
nM
i=1
Xi, then {Zn} is a martingale.
(c) If {X, Yi} are arbitrary random variables with E [|X|] < ∞, and Zn =
E [X | Y1, Y2, . . . , Yn], then {Zn} is a martingale.
17.7
MEASURE THEORETICAL PROBABILITY
1. A σ-ﬁeld of subsets of a set Ωis a collection F of subsets of Ωthat
contains φ (the empty set) as a member and is closed under complements
and countable unions. If Ωis a topological space, the σ-ﬁeld generated
by the open subsets of Ωis called the Borel σ-ﬁeld.
2. A probability measure P on a σ-ﬁeld F of subsets of a set Ωis a function
from F to the unit interval [0, 1] such that P(Ω) = 1 and P of a countable
union of disjoint sets {Ai} equals the sum of P(Ai).
c
⃝2000 by Chapman & Hall/CRC

3. A probability space is a triple (Ω, F, P), where Ωis a set, F is a σ-ﬁeld
of subsets of Ω, and P is a probability measure on F.
4. Given a probability space (Ω, F, P) and a measurable space (Ψ, G), a
random variable from (Ω, F, P) to (Ψ, G) is a measurable function from
(Ω, F, P) to (Ψ, G).
5. A random variable X from (Ω, F, P) to (Ψ, G) induces a probability
measure on Ψ. The measure of a set A in G is simply P(X−1(A)). This
induced measure is called the distribution of X.
6. A real-valued function F deﬁned on the set of real numbers R is called
a distribution function for R if it is increasing and right-continuous and
satisﬁes limx→−∞F(x) = 0 and limx→∞F(x) = 1.
Let Q be the distribution of X where X is a real valued random variable.
Then the function F: x →Q((−∞, x]) is a distribution function. We
call F the distribution function of X.
17.8
MONTE CARLO INTEGRATION TECHNIQUES
Random numbers may be used to approximate the value of a deﬁnite integral.
Let g be an integrable function and deﬁne the integral I by
I =

B
g(x) dx,
(17.45)
where B is a bounded region that may be enclosed in a rectangular paral-
lelepiped R with volume V (R). If 1B(x) represents the indicator function of
B,
1B(x) =
(
1
if x ∈B
0
if x ̸∈B
(17.46)
then the integral I may be written as
I =

R

g(x)1B(x)

dx =
1
V (R)

R

g(x)1B(x)V (R)

dx
(17.47)
Equation (17.47) may be interpreted as an expected value of the function
h(X) = g(X)1B(X)V (R) where the random variable X is uniformly dis-
tributed on the parallelepiped R (i.e., it has density function 1/V (R)).
The expected value of h(X) may be obtained by simulating random deviates
from X, evaluating h at these points, and then computing the mean of the h
values. If N trials are used, then the following estimate is obtained:
I ≈.I = 1
N
N

i=1
h(xi) = V (R)
N
N

i=1
g(xi)1B(xi)
(17.48)
where each xi is uniformly distributed in R.
c
⃝2000 by Chapman & Hall/CRC

See J. M. Hammersley and D. C. Handscomb, Monte Carlo Methods, John
Wiley, 1965.
17.8.1
Importance sampling
Importance sampling is the term given to sampling from a non-uniform distri-
bution so as to minimize the variance of the estimate for I in equation (17.45).
Suppose a sample is selected from a distribution with density function f(x).
The integral I may be written as
I =

B
 g(x)
f(x)

f(x) dx = Ef
 g(x)
f(x)

(17.49)
where Ef [·] denotes the expectation taken with respect to the density f(x).
That is, I is the mean of g(x)/f(x) with respect to the distribution f(x).
Associated with this mean is the variance:
σ2
f = Ef
%' g(x)
f(x) −I
,2&
= Ef
 g2
f 2

−I2 =

B
g2(x)
f(x) dx −I2
(17.50)
Approximations to I obtained by sampling from f(x) will have errors that
scale with σf.
A minimum variance estimator may be obtained by ﬁnding the density func-
tion f(x) such that σ2
f is minimized.
Using the calculus of variations the
density function for the minimal estimator is
fopt(x) = C|g(x)| =
|g(x)|
J
B|g(x)| dx
(17.51)
where the constant C is chosen so that fopt(x) is appropriately normalized.
(Since fopt(x) is a density function, it must integrate to unity.) While ﬁnding
fopt(x) is as diﬃcult as determining the original integral I, equation (17.51)
indicates that fopt(x) should have the same general behavior as |g(x)|.
17.8.2
Hit-or-miss Monte Carlo method
The hit-or-miss Monte Carlo method is very ineﬃcient but is easy to under-
stand. Suppose that 0 ≤f(x) ≤1 when 0 ≤x ≤1. Deﬁning
g(x, y) =
(
0
if f(x) < y,
1
if f(x) > y,
(17.52)
then I =
J 1
0 f(x) dx =
J 1
0
J 1
0 g(x, y) dy dx. This integral may be estimated by
I ≈.I = 1
n
n

i=1
g(ξ2i−1, ξ2i) = n∗
n
(17.53)
where the {ξi} are chosen independently and uniformly from the interval [0, 1].
The summation in equation (17.53) reduces to the number of points in the
c
⃝2000 by Chapman & Hall/CRC

unit square which are below the curve y = f(x) (this deﬁnes n∗) divided by
the total number of sample points (i.e., n).
17.9
QUEUING THEORY
The following diagram and notation are used to deﬁne a queue.
A queue is represented as A/B/c/K/m/Z where
(a) A and B represent the interarrival times and service times:
D deterministic (constant) interarrival or service time distribution.
Ek Erlang–k interarrival or service time distribution (a gamma distri-
bution with α = (k −1), β = 1/λk and density function f(t) =
λk(λkt)k−1e−λkt/(k −1)! for t > 0.
GI general independent interarrival time.
G general service time distribution.
Hk k–stage hyperexponential interarrival or service time distribution
(density function is f(t) =
k
i=1
αiµie−µit for t ≥0).
M exponential interarrival or service time distribution.
(b) c is the number of identical servers.
(c) K is the system capacity.
(d) m is the number in the source.
(e) Z is the queue discipline:
FCFS ﬁrst come, ﬁrst served (also known as FIFO).
LIFO last in, ﬁrst out.
RSS service in random order.
PRI priority service.
If all variables are not present, the last three above have the default values:
K = ∞, m = ∞, and Z is RSS.
Note: The system includes both the queue and the service facility.
The variables of interest are:
c
⃝2000 by Chapman & Hall/CRC

(a) an: proportion of customers that ﬁnd n customers already in the
system when they arrive.
(b) c: number of servers in the service facility.
(c) dn: proportion of customers leaving behind n customers in the system.
(d) K: maximum number of customers allowed in queueing system.
(e) L: mean number of customers in the steady-state system, L = E [N].
(f) Lq: mean number of customers in the steady-state queue, Lq = E [Nq].
(g) λ: mean arrival rate of customers to the system (number per unit
time), λ = 1/E [τ].
(h) µ: mean service rate per server (number per unit time), µ = 1/E [s].
(i) N: random number of customers in system in steady state.
(j) Na: random number of customers receiving service in steady state.
(k) Nq: random number of customers in queue in steady state.
(l) pn: proportion of time the system contains n customers.
(m) πq(r): the queueing time that r percent of the customers do not
exceed.
(n) πw(r): the system time that r percent of the customers do not exceed.
(o) q: random time a customer waits in the queue in order to begin service.
(p) qn: probability that there are n customers in the system just before a
customer enters.
(q) ρ: server utilization, the probability that any particular server is busy.
(r) s: random service time for one customer, E [s] = 1/µ.
(s) τ: random interarrival time, E [τ] = 1/λ.
(t) u: traﬃc intensity (units are erlangs) u = λ/µ.
(u) W: mean time of customers in the system in steady state, W = E [w].
(v) w: total waiting time in the system, including queue and service times,
w = q + s.
(w) Wq: mean time for customer in the queue in steady state, Wq = E [q].
Relationships between variables:
(a) Little’s formula: L = λW and Lq = λWq.
(b) For Poisson arrivals: pn = an.
(c) If customers arrive one at a time, and are served one at a time: an = dn.
(d) N = Nq + Ns
c
⃝2000 by Chapman & Hall/CRC

(e) W = Wq + Ws
17.9.1
M/M/1 queue
Assume λ < µ:
(a) ρ = u/c = (λ/µ)/c
(b) pn = (1 −ρ)ρn for n = 0, 1, 2, . . .
(c) L = ρ/(1 −ρ)
(d) Lq = ρ2/(1 −ρ)
(e) W = 1/µ(1 −ρ)
(f) Wq = ρ/µ(1 −ρ)
(g) πq(r) = max
"
0, W log

100ρ
100−r
#
(h) πw(r) = max
"
0, W log

100
100−r
#
17.9.2
M/M/1/K queue
Assume K ≥1 and N ≤K:
(a) ρ = (1 −pK)u
(b) pn =
( (1−u)un
1−uK+1
if λ ̸= µ and n = 0, 1, . . . , K
1/(K + 1)
if λ = µ and n = 0, 1, . . . , K
(c) L =



u[1−(K+1)uK+KuK+1]
(1−u)(1−uK+1)
if λ ̸= µ
K/2
if λ = µ
(d) Lq = L −(1 −p0)
(e) λa = (1 −pK)λ is the actual arrival rate at which customers enter the
system.
(f) W = L/λa
(g) Wq = Lq/λa
(h) qn = pn/(1 −pK) for n = 0, 1, . . . , K −1
Note: pK is the probability that an arriving customer is lost since there is no
room in the queue.
17.9.3
M/M/2 queue
(a) ρ = u/2
(b) p0 = (1 −ρ)/(1 + ρ)
(c) pn = 2(1 −ρ)ρn/(1 + ρ) for n = 1, 2, 3, . . . , c
(d) L = 2ρ/(1 −ρ2)
(e) Lq = 2ρ3/(1 −ρ2)
(f) W = 1/µ(1 −ρ2)
c
⃝2000 by Chapman & Hall/CRC

(g) Wq = ρ2/µ(1 −ρ2)
17.9.4
M/M/c queue
Erlang’s C formula is the probability that all c servers are busy
C(c, u) =
%
1 + c!(1 −ρ)
uc
c−1

n=0
un
n!
&−1
(17.54)
(a) ρ = u/c
(b) u = λ/µ
(c) p0 = c!(1 −ρ)
uc
C(c, u) =
%
uc
c!(1 −ρ) +
c−1

n=0
un
n!
&−1
(d) pn =
( un
n! p0
for n = 0, 1, . . . , c
un
c!cn−c p0
for n ≥c
(e) L = Lq + u
(f) Lq = uC(c,U)
c(1−ρ)
(g) W = Wq + 1/µ
(h) Wq =
C(c,u)
cµ(1−ρ)
(i) πq(90) = max
>
0, ln[10 C(c,U)]
cµ(1−ρ)
?
(j) πq(95) = max
>
0, ln[20 C(c,U)]
cµ(1−ρ)
?
17.9.5
M/M/c/c queue
Erlang’s B formula is the probability that all servers are busy
B(c, u) =
%
c!
uc
c

n=0
un
n!
&−1
(17.55)
(a) pn =

n!
un
c
n=0
un
n!
−1
for n = 0, 1, . . . , c
(b) λa = λ(1−B(c, u)) is the average traﬃc rate experienced by the system.
(c) ρ = λa/µc
(d) L = u [1 −B(c, u)]
(e) W = 1/µ
17.9.6
M/M/c/K queue
(a) p0 =
%
c

n=0
un
n! + uc
c!
K−c

n=1
u
c
n
&−1
c
⃝2000 by Chapman & Hall/CRC

(b) pn =
( un
n! p0
for n = 0, 1, . . . , c
uc
c!
 u
c
n−c p0
for n = c + 1, . . . , K
(c) λa = λ(1 −pK)
(d) ρ = (1 −pK)u/c
(e) L = Lq +
c−1

n=0
npn + c

1 −
c−1

n=0
pn

(f) Lq =
ucp0u/c
c!(1 −u/c)2

1 −
u
c
K−c+1
−(K −c + 1)
u
c
K−c
1 −u
c

(g) W = L/λa
(h) Wq = Lq/λa
17.9.7
M/M/∞queue
(a) pn = e−nun/n! for n = 0, 1, 2, . . .
(b) L = u
(c) Lq = 0
17.9.8
M/Ek/1 queue
(a) pn = (1 −ρ)
n

j=0
(−1)n−jrn−j−1
1 + ρ
k
kj kj
n −j

r +

kj
n −j −1

for
n = 0, 1, . . . , where r = ρ/(k + ρ)
(b) L = Lq + ρ
(c) Lq = λWq
(d) W = Wq + 1/µ
(e) Wq =
ρ
µ(1−ρ)

1+1/k
2

17.9.9
M/D/1 queue
(a) p0 = (1 −ρ)
(b) p1 = (1 −ρ) (eρ −1)
(c) pn = (1 −ρ)
n

j=0
(−1)n−j
(jρ)n−j−1(jρ + n −j)ejρ
(n −j)!

for n = 2, 3, . . .
(d) L = λW = Lq + ρ
(e) Lq = λWq =
ρ2
2(1−ρ)
(f) W = Wq + 1
µ
(g) Wq =
ρ
2µ(1−ρ)
c
⃝2000 by Chapman & Hall/CRC

17.10
RANDOM MATRIX EIGENVALUES
1. Let A be a n×n matrix whose entries are independent standard normal
deviates.
(a) The probability pn,k that A has k real eigenvalues has the form
r + s
√
2, where r and s are rational. In particular, the probability
that A has all real eigenvalues is
pn,n = 2−n(n−1)/4
(17.56)
n
k
pn,k
1
1
1
1
2
2
1
2
√
2
≈0.707
0
1 −1
2
√
2
≈0.293
3
3
1
4
√
2
≈0.354
1
1 −1
4
√
2
≈0.646
4
4
1
8
0.125
2
−1
4 + 11
16
√
2
≈0.722
0
9
8 −11
16
√
2
≈0.153
(b) The expected number of real eigenvalues of A is
En =













√
2
n/2−1

k=0
(4k −1)!
(4k)!!
when n is even
1 +
√
2
(n−1)/2

k=1
(4k −3)!
(4k −2)!!
when n is odd
(17.57)
and En ∼

2n
π

1 −
3
8n −
3
128n2 + . . .

as n →∞.
(c) If λn denotes a real eigenvalue of A, then its marginal probability
density fn(λ) is given by
fn(λ) = 1
En

1
√
2π
Γ(n −1, λ2)
Γ(n −1)

+
$$λn−1$$ e−λ2/2
Γ(n/2)2n/2
γ((n −1)/2, λ2/2)
Γ((n −1)/2)

(17.58)
where γ(a, x) and Γ(a, x) are incomplete gamma functions (see page
519).
c
⃝2000 by Chapman & Hall/CRC

(d) If the elements of A have mean 0 and variance 1, and z is a scalar,
then
E

det

A2 + z2I

= E
"
det (A + zI)2#
= per

J + z2I

= n! en

z2
= n! 1F1

−1; 1
2n; −1
2z2I

(17.59)
where J is the matrix of all ones, “per” refers to the permanent
of a matrix, en(x) = n
k=0
xk
k! is the truncated Taylor series for
ex, and the hypergeometric function has a scalar multiple of the
identity as its argument.
2. Let A be a random n×n matrix with randomly selected integer entries.
Let P(p, n) be the probability that det(A) is not congruent to 0 modulo
p. Then
P(p, n) =
n
)
k=1

1 −p−k
(17.60)
3. Let A be a random n × n complex matrix uniformly distributed on the
sphere 1 = ∥A∥F = 
i,j |Aij|2. Then,
E

det

AHA

| σ2
min = λ

=
n−1

r=0
λn−r(1 −nλ)n2+r−1
×
Γ(n2)Γ(n + 1)Γ(n + 2)
Γ(r + 1)Γ(n −r)Γ(n2 + r −1)Γ(n + 2 −r)
(17.61)
and if En = E

det

AHA

then En =

n
n2+n−1
−1. The ﬁrst few values
of 1/En are {1, 10, 165, 3876, 118755, . . . }.
4. Deﬁne the following matrices where K
N(µ, σ2) refers to the distribution
X + iY where both X and Y are N(µ, σ2):
(a) Gaussian matrix: G(m, n), an m × n random matrix with iid ele-
ments which are N(0, 1).
(b) Wishart matrix: W(m, n), symmetric random matrix AAT where
A is G(m, n).
(c) Gaussian orthogonal ensemble (GOE): an m × m random matrix
(A + AT)/2 where A is G(m, m).
(d) Complex Gaussian matrix: KG(m, n), an m×n random matrix with
iid elements which are K
N(0, 1).
(e) Complex Wishart matrix:
N
W(m, n), symmetric random matrix
AAH where A is KG(m, n).
c
⃝2000 by Chapman & Hall/CRC

W(m, n) :
πm2
Γm(m/2)Γm(n/2) exp

−1
2

λi
 )
λ(n−m−1)/2
i
)
i<j
(λi −λj)
GOE :
1
2n/2 Mn
i=1 Γ(i/2) exp

−1
2

λ2
i
 )
i<j
(λi −λj)
N
W(m, n) :
2−mnπm(m−1)
KΓm(n)KΓm(m)
exp

−1
2

λi
 )
λn−m
i
)
i<j
(λi −λj)2
GUE :
2n(n−1)/2
πn/2 Mn
i=1 Γ(i) exp

−

λ2
i
 )
i<j
(λi −λj)
Table 17.1: Distribution functions for eigenvalues
(f) Gaussian unitary ensemble (GUE): an m×m random matrix (A+
AH)/
√
8 where A is KG(m, m).
Then
(a) The joint densities of the eigenvalues λ1 ≥· · · ≥λm for the above
random matrices are in Table 17.1 where the unlabeled sums and
products run from i = 1 to m and the complex multivariate gamma
function is deﬁned by
KΓm(a) = πm(m−1)/2
m
)
i=1
Γ(a −i + 1)
(17.62)
For the Wishart cases, this is the joint distribution for the non-
negative eigenvalues. For the Gaussian cases, this is the joint dis-
tribution for the eigenvalues which may be anywhere on the real
line.
(b) The probability density function of the smallest eigenvalue of a
matrix from W(m, m) is
fλmin(λ) =
m
√
2π Γ
m + 1
2

λ−1/2e−λm/2U
m −1
2
, −1
2, λ
2

(17.63)
Here, U(a, b, z) is the Tricomi function (see section 18.9.3).
c
⃝2000 by Chapman & Hall/CRC

(c) If κ is the condition number of a matrix from G(n, n), then the
probability density function of κ/n converges to
f(x) = 2x + 4
x3
e−2/x−2/x2
(17.64)
and
E [log κ] = log n + c + o(1)
(17.65)
as n →∞where c ≈1.537.
See A. Edelman, How many eigenvalues of a random matrix are real?, J.
Amer. Math. Soc., 7, 1994, pages 247–267, and A. Edelman, Eigenvalues and
condition numbers of random matrices, SIAM J. Matrix Anal. Appl., 9, 1988,
pages 543–560.
17.10.1
Random matrix products
Let || · || be a non-negative real-valued function of matrices that satisﬁes
||AB|| ≤||A|| · ||B| when A and B are d × d matrices (i.e,, || · || could be a
matrix norm). Let {Ai} be iid random d×d matrices in which all d2 elements
of each Ai are iid normal random variables with mean 0 and variance 1. Then
lim
t→∞log ||A1A2 · · · At||
t
= 1
2

1 + ψ
d
2

(17.66)
where ψ is the digamma function (see page 518).
See J. E. Cohen and C. M. Newman, The stability of large matrices and their
products, Ann. Probab., 1984, 12, pages 283–310.
17.10.1.1
Vibonacci numbers
The Fibonacci numbers are deﬁned by fn = fn−1 + fn−2 and f1 = f2 = 1.
This formula can represented as the matrix product:

fn−1
fn

=

0 1
1 1
 
fn−2
fn−1

(17.67)
As n →∞, the ratio fn+1
fn
∼φ ≈1.61803 . . . (the golden mean).
The vibonacci numbers are deﬁned by vn = vn−1 ± vn−1 where the ± sign is
chosen randomly (50% of the time it is −and 50% of the time it is +). This
formula can represented as the matrix product:
vn−1
vn

=













%
0 1
1 1
& %
vn−2
vn−1
&
50% of the time
%
0 1
1 −1
& %
vn−2
vn−1
&
50% of the time
(17.68)
As n →∞, the ratio vn+1
vn
∼1.1319882 . . . (Viswanath’s constant).
c
⃝2000 by Chapman & Hall/CRC

See B. Hayes, The vibonacci numbers, American Scientist, 87, July–August
1999, pages 296–301.
17.11
RANDOM NUMBER GENERATION
17.11.1
Methods of pseudorandom number generation
Depending on the application, either integers in some range or ﬂoating point
numbers in [0, 1) are the desired output from a pseudorandom number gen-
erator (PRNG). Since most PRNGs use integer recursions, a conversion into
integers in a desired range or into a ﬂoating point number in [0, 1) is required.
If xn is an integer produced by some PRNG in the range 0 ≤xn ≤M −1,
then an integer in the range 0 ≤xn ≤N −1, with N ≤M, is given by
yn =
O Nxn
M
P
. If N ≪M, then yn = xn (mod N) may be used. Alternately,
if a ﬂoating point value in [0, 1) is desired, let yn = xn/M.
17.11.1.1
Linear congruential generators
Perhaps the oldest generator still in use is the linear congruential generator
(LCG). The underlying integer recursion for LCGs is:
xn = axn−1 + b
(mod M).
(17.69)
Equation (17.69) deﬁnes a periodic sequence of integers modulo M starting
with x0, the initial seed. The constants of the recursion are referred to as
the modulus, M, multiplier, a, and additive constant, b. If M = 2m, a very
eﬃcient implementation is possible. Alternately, there are theoretical reasons
why choosing M prime is optimal. Hence, the only moduli that are used in
practical implementations are M = 2m or the prime M = 2p −1 (i.e., M is
a Mersenne prime). With a Mersenne prime, modular multiplication can be
implemented at about twice the computational cost of multiplication modulo
2p.
Equation (17.69) yields a sequence {xn} whose period, denoted Per(xn), de-
pends on M, a, and b. The values of the maximal period for the three most
common cases used and the conditions required to obtain them are:
a
b
M
Per(xn)
primitive root of M
anything
prime
M −1
3 or 5 (mod 8)
0
2m
2m−2
1 (mod 4)
1 (mod 2)
2m
2m
A major shortcoming of LCGs modulo a power-of-two compared with prime
modulus LCGs derives from the following theorem for LCGs:
Theorem 5 Deﬁne the following LCG sequence: xn = axn−1 +b (mod M1).
If M2 divides M1 and if yn = xn (mod M2), then yn satisﬁes yn = ayn−1 + b
(mod M2).
c
⃝2000 by Chapman & Hall/CRC

Theorem 5 implies that the k least-signiﬁcant bits of any power-of-two mod-
ulus LCG with Per(xn) = 2m = M has Per(yn) = 2k, 0 < k ≤m. Since a
long period is crucial in PRNGs, when these types of LCGs are employed in a
manner that makes use of only a few least-signiﬁcant bits, their quality may
be compromised. When M is prime, no such problem arises.
Since LCGs are in such common usage, the table below contains a list of
parameter values mentioned in the literature. The Park–Miller LCG is widely
considered to be a minimally acceptable PRNG.
a
b
M
Source
75
0
231 −1
Park–Miller
131
0
235
Neave
16333
25887
215
Oakenfull
3432
6789
9973
Oakenfull
171
0
30269
Wichman–Hill
17.11.1.2
Shift register generators
Another popular method of generating pseudorandom numbers is to use bi-
nary shift register sequences to produce pseudorandom bits. A binary shift
register sequence (SRS) is deﬁned by a binary recursion of the type:
xn = xn−j1 ⊕xn−j2 ⊕· · · ⊕xn−jk,
j1 < j2 < · · · < jk = ℓ.
(17.70)
where ⊕is the exclusive “or” operation. Note that x ⊕y ≡x + y (mod 2).
Thus the new bit, xn, is produced by adding k previously computed bits
together modulo 2. The implementation of this recurrence requires keeping
the last ℓbits from the sequence in a shift register, hence the name. The
longest possible period is equal to the number of nonzero ℓ-dimensional binary
vectors, namely, 2ℓ−1.
A suﬃcient condition for achieving Per(xn) = 2ℓ−1 is that the characteristic
polynomial corresponding to equation (17.70) be primitive modulo 2. Since
primitive trinomials of nearly all degrees of interest have been found, SRSs
are usually implemented using two-term recursions of the form:
xn = xn−k ⊕xn−ℓ,
0 < k < ℓ.
(17.71)
In these two-term recursions, k is the lag and ℓis the register length. Proper
choice of the pair (ℓ, k) leads to SRSs with Per(xn) = 2ℓ−1. Here is a list
with suitable (ℓ, k) pairs:
Primitive trinomial exponents
(5,2)
(7,1)
(7,3)
(17,3)
(17,5)
(17,6)
(31,3)
(31,6)
(31,7)
(31,13
(127,1)
(521,32)
17.11.1.3
Lagged-Fibonacci generators
Another way of producing pseudorandom numbers is to use the lagged-Fibonacci
methods. The term “lagged-Fibonacci” refers to two term recurrences of the
c
⃝2000 by Chapman & Hall/CRC

form:
xn = xn−k ⋄xn−ℓ,
0 < k < ℓ,
(17.72)
where ⋄refers to three common methods of combination: (1) addition modulo
2m, (2) multiplication modulo 2m, or (3) bit-wise exclusive ‘OR’ing of m-
long bit vectors.
Combination method (3) can be thought of as a special
implementation of a two-term SRS.
Using combination method (1) leads to additive lagged-Fibonacci sequences
(ALFSs). If xn satisﬁes:
xn = xn−k + xn−ℓ
(mod 2m),
0 < k < ℓ,
(17.73)
then the maximal period is Per(xn) = (2ℓ−1)2m−1.
ALFS are especially suitable for producing ﬂoating point deviates using the
real valued recursion yn = yn−k + yn−ℓ(mod 1). This circumvents the need
to convert from integers to ﬂoating point values and allows ﬂoating point
hardware to be used. One caution with ALFS is that theorem 5 holds, and
so the low-order bits have periods that are shorter than the maximal period.
However, this is not nearly the problem as in the LCG case. With ALFSs
the j least-signiﬁcant bits will have period (2ℓ−1)2j−1, so if ℓis large there
really is no problem. Note that one can use the table of primitive trinomial
exponents to ﬁnd (ℓ, k) pairs that give maximal period ALF sequences.
17.11.2
Generating nonuniform random variables
In order to select random observations from an arbitrary distribution, suppose
X has probability density function f(x) and distribution function F(x) =
J x
−∞f(u) du. In the following we write “Y is U[0, 1)” to mean Y is uniformly
distributed on the interval [0, 1).
Two general techniques for converting uniform random variables into those
from other distributions are:
1. The inverse transform method.
If Y is U[0, 1), then the random variable X = F −1(Y ) has probability
density function f(x).
2. The acceptance–rejection method.
Suppose the density can be written as f(x) = Ch(x)g(x) where h(x) is
the density of a computable random variable, g(x) satisﬁes the inequality
0 < g(x) ≤1, and C−1 =
J ∞
−∞h(u)g(u) du is a normalization constant.
If X is U[0, 1), Y has density h(x), and if X < g(Y ), then X has density
f(x). Therefore, generate {X, Y } pairs, reject both if X ≥g(Y ) and
return X if X < g(Y ).
Examples of the inverse transform method:
c
⃝2000 by Chapman & Hall/CRC

1. (Exponential distribution) The exponential distribution with rate λ has
f(x) = λe−λx (for x ≥0) and F(x) = 1 −e−λx. Thus u = F(x) can be
solved to give x = F −1(u) = −λ−1 ln(1 −u). If U is U[0, 1) then so is
1 −U. Hence X = −λ−1 ln U is exponentially distributed with rate λ.
2. (Normal distribution) Let Zi be normally distributed with f(z) =
1
√
2πe−z2/2.
The polar transformation produces random variables R =

Z2
1 + Z2
2
(exponentially distributed with λ = 2) and Θ = arctan(Z2/Z1) (uni-
formly distributed on [0, 2π)). Inverting: Z1 = √−2 ln X1 cos 2πX2 and
Z2 = √−2 ln X1 sin 2πX2 are normally distributed when X1 and X2 are
U[0, 1). (This is the Box–Muller technique.)
Examples of the rejection method:
1. (Exponential distribution with λ = 1)
(a) Generate random numbers {Ui}N
i=1 uniform on [0, 1]), stopping at
N = min{n | U1 ≥U2 ≥Un−1 < Un}.
(b) If N is even accept that run, and go to step (c). If N is odd reject
the run, and return to step (a).
(c) Set X equal to the number of failed runs plus the ﬁrst random
number in the successful run.
2. (Normal distribution)
(a) Select two random variables (V1, V2) from U[0, 1). Form R = V 2
1 +
V 2
2 .
(b) If R > 1 then reject the (V1, V2) pair and select another pair.
(c) If R < 1 then X = V1
-
−2ln R
R
has a N(0, 1) distribution.
3. (Normal distribution)
(a) Select two exponentially distributed random variables with rate 1:
(V1, V2).
(b) If V2 ≥(V1 −1)2/2, then reject the (V1, V2) pair and select another
pair.
(c) Otherwise, V1 has a a N(0, 1) distribution.
4. (Cauchy distribution) To generate values of X from f(x) =
1
π(1+x2) on
−∞< x < ∞:
(a) Generate random numbers U1, U2 (uniform on [0, 1)) and set Y1 =
U1 −1
2, Y2 = U2 −1
2.
(b) If Y 2
1 +Y 2
2 ≤1
4 then return X = Y1/Y2. Otherwise return to step 1.
To generate values of X from a Cauchy distribution with parameters β
and θ; f(x) =
β
π [β2 + (x −θ)2] for −∞< x < ∞; construct X as above
and then use βX + θ.
c
⃝2000 by Chapman & Hall/CRC

17.11.2.1
Discrete random variables
In general, the density function of a discrete random variable can be rep-
resented as a vector p = (p0, p1, . . . , pn−1, pn) by deﬁning the probabilities
Prob [x = j] = pj (for j = 0, . . . , n). The distribution function can be deﬁned
by the vector c = (c0, c1, . . . , cn−1, 1) where cj = j
i=0 pi. Given this rep-
resentation of F(x) we can apply the inverse transform by computing X to
be U[0, 1), and then ﬁnding the index j so that cj ≤X < cj+1. In this case
event j will have occurred. For example:
1. (Binomial distribution) The binomial distribution with n trials, with
each trial having probability of success p, has pj =
n
j

pj(1 −p)n−j for
j = 0, . . . , n.
(a) As an example, consider the result of ﬂipping a fair coin. In 2 ﬂips,
the probability of obtaining (0, 1, 2) heads are p = ( 1
4, 1
2, 1
4). Hence
c = ( 1
4, 3
4, 1). If x (chosen from U[0, 1)) turns out to be say, 0.4,
then “1 head” is returned (since 1
4 ≤0.4 < 1
2).
(b) Note that when n is large it is costly to compute the density and
distribution vectors. When n is large and relatively few binomially
distributed pseudorandom numbers are desired, an alternative is
to use the normal approximation to the binomial.
(c) Alternately, one can form the sum n
i=1 ⌊Ui + p⌋, where each Ui
is U[0, 1).
2. (Geometric distribution) To simulate a value from Prob [X = i] = p(1−
p)i−1 for i ≥1; use X = 1 +
7
log U
log(1 −p)
8
.
3. (Poisson distribution) The Poisson distribution with mean λ has pj =
λjeλ/j! for j ≥0. The Poisson distribution counts the number of events
in a unit time interval if the times are exponentially distributed with
rate λ. Thus if the times {Ti} are exponentially distributed with rate
λ, then J will be Poisson distributed with mean λ when J
i=0 Ti ≤
1 ≤J+1
i=0 Ti. Since Ti = −λ−1 ln Ui, where Ui is U[0, 1), the previous
equation may be written as MJ
i=0 Ui ≥e−λ ≥MJ+1
i=0 Ui.
Hence, we
can compute Poisson random variables by iteratively computing PJ =
MJ
i=0 Ui until PJ < e−λ. The ﬁrst such J that makes this inequality
true will have a Poisson distribution.
Random variables can be simulated using the following table (each U and Ui
are uniform on the interval [0, 1)):
c
⃝2000 by Chapman & Hall/CRC

Using uniform random deviates to create random
deviates from diﬀerent distributions
Distribution
Density
Formula for deviate
Binomial
pj =
n
j

pj(1 −p)n−j
n

i=1
⌊Ui + p⌋
Cauchy
f(x) =
σ
π(x2 + σ2)
σ tan(πU)
Exponential
f(x) = λe−λx
−λ−1 ln U
Pareto
f(x) = aba
xa+1
b
U 1/a
Rayleigh
x
σ e−x2/2σ2
σ
√
−ln U
17.11.2.2
Testing pseudorandom numbers
The prudent way to check a complicated computation that makes use of pseu-
dorandom numbers is to run it several times with diﬀerent types of pseudo-
random number generators and see if the results appear consistent across the
generators. The fact that this is not always possible or practical has led re-
searchers to develop statistical tests of randomness that should be passed by
general purpose pseudorandom number generators. Some common tests are
the spectral test, the equidistribution test, the serial test, the runs test, the
coupon collector test (section 17.16.10), and the birthday spacing test (section
17.16.6).
17.11.3
References
1. L. Devroye, Non-Uniform Random Variate Generation, Springer–Verlag,
New York, 1986.
2. S. K. Park and K. W. Miller, Random number generators: good ones
are hard to ﬁnd, Communications of the ACM, October 1988, Volume
31, Number 10, pages 1192–1201.
17.12
RESAMPLING METHODS
Assume the set S = {x1, x2, . . . , xn} contains n sample values of the random
deviate X. Resampling techniques use the values in S repeatedly to obtain an
estimate of a statistic, and also the variance of that estimate. For example,
when computing the sample mean of the values in S, the value of the sample
standard deviation indicates the possible range of values for the true mean.
However, when computing the sample median, there is no natural way (other
than resampling) to ﬁnd the variance of the estimate.
c
⃝2000 by Chapman & Hall/CRC

Suppose an estimate of the statistic θ(X) is desired. Let .θ be a procedure
(estimator) used for estimating θ.
1. Bootstrap
To apply the bootstrap technique, m random sets of the same size are
drawn from the set S with replacement, and .θ is calculated for each
sample. The bootstrapped estimate for θ(X) is the mean of the values
of .θ for each random set.
Example 17.80:
Consider a sample of four data points: {1, 3, 5, 9}.
The
estimated median from this sample is 4.
To estimate the variability of the estimate of the median, repeatedly sample with
replacement from the four data points. For example:
(a) {5, 9, 9, 9}, median is 9
(b) {1, 5, 1, 3}, median is 2
(c) {9, 3, 3, 9}, median is 6
(d) {3, 1, 5, 1}, median is 2
(e) {5, 1, 9, 5}, median is 5
(f) {3, 9, 9, 5}, median is 7
(g) {5, 3, 9, 1}, median is 4
(h) {3, 3, 9, 3}, median is 3
(i) {9, 5, 3, 3}, median is 4
(j) {3, 9, 9, 1}, median is 6
These 10 estimates of the median have a mean of 4.8 and a standard deviation
2.3. This indicates the approximate variability of the estimate of the median.
2. Jackknife
To apply the jackknife technique, .θ is computed on the set S and also
on the n−1 sets obtained from S by sequentially deleting each element.
Hence, .θ is computed for 1 sample of size n and n samples of size n −1.
The jackknife estimate of θ(X) is the mean of the n values of .θ that
have been obtained.
See B. Efron, ”Bootstrap methods, another look at the jackknife,” Annals
of Statistics, 7, 1979, pages 1–26 and B. Efron, The Jackknife, the Bootstrap
and Other Resampling Plans, Society for Industrial and Applied Mathematics,
Philadelphia, 1982.
17.13
SELF-SIMILAR PROCESSES
17.13.1
Deﬁnitions
(a) A function L(x) is slowly varying if, for all x > x0,
lim
t→∞
L(tx)
L(t) = 1.
(17.74)
Slowly varying functions include L(x) = c+o(1) for x > 0, L(x) = log x
for x > 1, and L(x) = 1/ log x for x > 1.
c
⃝2000 by Chapman & Hall/CRC

(b) A random variable X has a heavy tailed distribution if Prob [X > x] =
x−αL(x) for α > 0 and x > x0 where L(x) is a slowly varying function.
17.13.2
Self-similar processes
A process {Xt}t=0,1,2,... is asymptotically self-similar if the autocorrelation
function, r(k), has the form
r(k) ∼k−(2−2H)L(k)
as k →∞
(17.75)
where L(x) is a slowly varying function and the Hurst parameter H satisﬁes
1/2 < H < 1. The process is exactly self-similar if
r(k) = 1
2

(k + 1)2H −2k2H + (k −1)2H
.
(17.76)
Note: White noise has r(k) = 0, which corresponds to H = 1/2.
For any process {Xt}t=0,1,2,..., the aggregated version {X(m)
t
}t=0,1,2,... is con-
structed by partitioning {Xt} into non–overlapping blocks of m sequential
elements and constructing a single element X(m)
t
from the mean:
X(m)
t
= 1
m
tm

i=tm−m+1
Xi
(17.77)
Note: {X(m)
t
} represents viewing {Xt} on a time scale that is a factor of m
coarser.
For a typical process, as m increases the autocorrelation of {X(m)
t
} decreases
until, in the limit, the elements of {X(m)
t
} are uncorrelated. For a self-similar
process, the processes {Xt} and {X(m)
t
} have the same autocorrelation func-
tion.
17.14
SIGNAL PROCESSING
17.14.1
Estimation
Let {et} be a white noise process (so that E [et] = µ, Var [et] = σ2, and
Cov [et, es] = 0 for s ̸= t).
Suppose that {Xt} is a time series.
A non-
anticipating linear model presumes that ∞
u=0 huXt−u = et, where the {hu}
are constants. This can be written H(z)Xt = et where H(z) = ∞
u=0 huzu
and znXt = Xt−n. Alternately, Xt = H−1(z)et. In practice, several types of
models are used:
1. AR(k): autoregressive model of order k. This assumes that
H(z) = 1 + a1z + · · · + akzk and so
Xt + a1Xt−1 + · · · + akXt−k = et
(17.78)
2. MA(l): moving average of order l. This assumes that
c
⃝2000 by Chapman & Hall/CRC

H−1(z) = 1 + b1z + · · · + bkzk and so
Xt = et + b1et−1 + · · · + blet−l
(17.79)
3. ARMA(k, l): mixed autoregressive/moving average of order (k, l). This
assumes that H−1(z) = 1+b1z+···+bkzk
1+a1z+···+akzk and so
Xt + a1Xt−1 + · · · + akXt−k = et + b1et−1 + · · · + blet−l
(17.80)
17.14.2
Matched ﬁltering (Wiener ﬁlter)
Let S(t) represent a signal to be recovered, let N(t) represent noise, and let
Y (t) = S(t) + N(t) represent the observable. A prediction of the signal is
Sp(t) =
 ∞
0
K(z)Y (t−z) dz, where K(z) is a ﬁlter. The mean square error is
E

(S(t) −Sp(t))2
; this is minimized by the optimal (Wiener) ﬁlter Kopt(z).
When X and Y are stationary, deﬁne their autocorrelation functions to be
RXX(t −s) = E [X(t)X(s)] and RY Y (t −s) = E [Y (t)Y (s)]. If F represents
the Fourier transform, then the optimal ﬁlter is given by
F [Kopt(t)] = 1
2π
F [RXX(t)]
F [RY Y (t)]
(17.81)
For example, if X and N are uncorrelated, then
F [Kopt(t)] = 1
2π
F [RXX(t)]
F [RXX(t)] + F [RNN(t)]
(17.82)
In the case of no noise: F [Kopt(t)] =
1
2π, Kopt(t) = δ(t), and Sp(t) = Y (t).
17.14.3
Median ﬁlter
A median ﬁlter replaces a value in a data set with the median of the entries
surrounding that value. In the one-dimensional case it consists of sliding a
window of an odd number of elements along the signal, replacing the center
sample by the median of the samples in the window.
The median is a stronger “central indicator” than the average. The median is
hardly aﬀected by one or two discrepant values among the data values in the
region. Consequently, median ﬁltering is very eﬀective at removing various
kinds of noise. In two-dimensional data sets (such as images) median ﬁltering
is a nonlinear signal enhancement technique for the smoothing of signals, the
suppression of impulse noise, and preserving of edges.
17.14.4
Mean ﬁlter
A mean ﬁlter or averaging ﬁlter replaces the values in a data set with the
average of the entries surrounding that value. Thought of as a convolution
ﬁlter, it is represented by a kernel which represents the shape and size of the
neighborhood to be sampled when calculating the mean. In two-dimensional
data sets (such as images) a 3 × 3 square kernel is often used:
c
⃝2000 by Chapman & Hall/CRC

1/9
1/9
1/9
1/9
1/9
1/9
1/9
1/9
1/9
A problem with the mean ﬁlter is that it blurs edges and other sharp details.
An alternative is to use a median ﬁlter.
17.14.5
Spectral decomposition of stationary random functions
Any stationary function X(t) can be written as
X(t) −µX =
 ∞
−∞
eiωt dΦ(ω)
(17.83)
If the correlation function satisﬁes the equation
 ∞
−∞
|KX(t)| dt < ∞then the
increments dΦ(ω) satisfy
E [dΦ(ω)] = 0
E [dΦ∗(ω1) dΦ(ω2)] = SX(ω)δ(ω1 −ω2) dω1 dω2
(17.84)
where ∗denotes the complex conjugate. Here, SX(ω) is the spectral density
of X(t) and δ(x) denotes the δ-function.
The correlation function and the spectral density are related by mutually
inverse Fourier transforms:
KX(t) =
 ∞
−∞
eiωtSX(ω) dω,
and
SX(ω) = 1
2π
 ∞
−∞
e−iωtKX(t) dt
(17.85)
Note that:
(a) If X(t) is a real function, then SX(ω) = SX(−ω).
(b) The spectral density of X′(t) is related to SX(ω) by:
SX′(ω) = ω2SX(ω)
(17.86)
17.15
STOCHASTIC CALCULUS
17.15.1
Brownian motion (Wiener processes)
Brownian motion W(t) is a Gaussian random process that has a mean given by
its starting point, E [W(t)] = W0 = W(t0), a variance of E

(W(t) −W0)2
=
t −t0, and a covariance of E [W(t)W(s)] = min(t, s). The sample paths of
W(t) are continuous but not diﬀerentiable. Brownian motion is also called
a Wiener process. Formally: Let (Ω, B, Pr) be a Lebesgue probability space,
and let (R, F, m) represent the real numbers with Lebesgue measure m. Then
a Brownian motion is a function X(t, ω) : R+ × Ω→R satisfying three
conditions:
c
⃝2000 by Chapman & Hall/CRC

1. For any 0 < s < t, (X(t, ω) −X(s, ω)) has a Gaussian distribution with
mean zero and variance m([s, t));
2. If t0 < t1 < · · · < tk, then (X(tj, ω) −X(tj−1, ω))j=1,2,...,k is an inde-
pendent system;
3. X(0, ω) = 0 for all ω ∈Ω.
17.15.2
Brownian motion expectations
Deﬁne the following types of Brownian motions:
1. Brownian motion Ws
2. Brownian motion with drift W (µ)
s
= µs + Ws
3. Reﬂecting Brownian motion |Ws|
For each, let the Brownian motion start at the location x. Deﬁne the following
types of stopping times at which a process X will stop:
1. An exponential stopping time is the time τ given by Prob [τ > t] = e−λt
for λ > 0. (Here τ is independent of the process X).
2. A ﬁrst hitting time is the ﬁrst time that a process reaches some value;
for example, Hz = min{s | Ws = z} is the ﬁrst time when a Brownian
motion reaches the value z.
3. A ﬁrst exit time is the ﬁrst time that a process leaves a region; for
example, Ha,b = min{s | Ws ̸∈(a, b)} is the ﬁrst time that a Brownian
motion exits the interval (a, b).
We have the following expectations involving Brownian motion:
1. Brownian motion Ws:
(a) Unconstrained
i. E

eiβWt
= exp

iβx −β2t
2

ii. E

exp

−γ sup
0<s<t
Ws

= exp

−γx + γ2t
2

erfc

γ
-
t
2

iii. E

exp

γ inf
0<s<t Ws

= exp

γx + γ2t
2

erfc

γ
-
t
2

(b) Exponential stopping times:
i. E

eiβWτ 
=
2λ
2λ + β2 eiβx
ii. E

exp

−γ sup
0<s<τ
Ws

=
√
2λ
γ +
√
2λ
e−γx
iii. E

exp

γ
inf
0<s<τ Ws

=
√
2λ
γ +
√
2λ
eγx
c
⃝2000 by Chapman & Hall/CRC

(c) First hitting time:
i. E

e−αHz
= e−|x−z|
√
2α
ii. E

exp

−γ
sup
0<s<Hz
Ws

= e−γx
%
1 −γ(x −z)
∞
J
x−z
e−γv
v
dv
&
for z ≤x
iii. E

exp

γ
inf
0<s<Hz Ws

= eγx
%
1 −γ(z −x)
∞
J
z−x
e−γv
v
dv
&
for
x ≤z
iv. E
%
exp

−γ
Hz
J
0
e2βWs ds
&
=











I0
 √2γ
|β| eβx
I0
 √2γ
|β| eβz ,
(z −x)β ≥0
K0
 √2γ
|β| eβx
K0
 √2γ
|β| eβz ,
(z −x)β ≤0
(d) First exit time
i. E

e−αHa,b
=
cosh

(b−2x+a)√
α/2

cosh

(b−a)√
α/2

ii. E
"
eβWHa,b
#
= b−x
b−aeiβa + x−a
b−a eiβb
2. Brownian motion with drift W (µ)
s
= µs + Ws
(a) Unconstrained
i. E
"
eiβW (µ)
t
#
= exp

iβ(x + µt) −β2t
2

(b) Exponential stopping times
i. E
"
eiβW (µ)
τ
#
=
2λ
2λ −2iβµ + β2 eiβx
ii. E

exp

−γ sup
0<s<τ
W (µ)
s

=

2λ + µ2 −µ
γ +

2λ + µ2 −µ
e−γx
(c) First hitting time (Hz = min{s | W (µ)
s
= z})
i. E

e−αHz
= exp

µ(z −x) −|z −x|

2α + µ2

(d) First exit time (Ha,b = min{s | W (µ)
s
̸∈(a, b)})
i. E

e−αHz
= eµ(a−x) sinh((b−x)η)+eµ(b−x) sinh((x−a)η)
sinh((b−a)η)
where η =

2α + µ2
ii. E

e
iW (µ)
Ha,b

= sinh ((b −x)|µ|)
sinh ((b −a)|µ|)eµ(a−x)+iβa
+ sinh ((x −a)|µ|)
sinh ((b −a)|µ|) eµ(b−x)+iβb
3. Reﬂecting Brownian motion |Ws|
(a) Exponential stopping times
i. E
"
e−β|Wτ |#
=
√
2λ
2λ −β2
√
2λe−βx −βe−
√
2λx
for 0 ≤x
c
⃝2000 by Chapman & Hall/CRC

See A. N. Borodin and P. Salminen, Handbook of Brownian Motion — Facts
and Formulae, Birkh¨auser Verlag, Boston, 1996.
17.15.3
Itˆo lemma
Consider the process f(W) = {F(Wt) | t ≥0} where f is a given smooth
function and W is a Brownian motion. When f has two derivatives, the Itˆo
formula is given by:
f(Wt) −f(W0) =
 t
0
f ′(Ws) dWs + 1
2
 t
0
f ′′(Ws) ds
(17.87)
17.15.4
Stochastic integration
If W(t) is a Wiener process and G(t, W(t)) is an arbitrary function, then
the stochastic integral I =
J t
t0 G(s, W(s)) dW(s) is deﬁned as a limiting sum.
Divide the interval [t0, t] into n sub-intervals: t0 ≤t1 ≤· · · ≤tn−1 ≤tn = t,
and choose points {τi} that lie in each sub-interval: ti−1 ≤τi ≤ti. The
stochastic integral I is deﬁned as the limit of partial sums, I = limn→∞Sn,
with Sn = n
i=1 G(τi, W(τi))[W(ti) −W(ti−1)].
Consider, for example, the special case of G(t) = W(t). Then the expectation
of Sn is
E [Sn] = E
% n

i=1
W(τi)[W(ti) −W(ti−1)]
&
=
n

i=1
[min(τi, ti) −min(τi, ti−1)]
=
n

i=1
(τi −ti−1).
(17.88)
If τi = αti + (1 −α)ti−1 (where 0 < α < 1), then E [Sn] = n
i=1(ti −ti−1)α =
(t−t0)α. Hence, the value of Sn depends on α. For consistency, some speciﬁc
choice must be made for the points {τi}.
1. For the Itˆo stochastic integral (indicated by I
J
), we choose τi = ti−1
(i.e., α = 0 in the above). That is:
I
 t
t0
G(s, W(s)) dW(s)
= ms-lim
n→∞
( n

i=1
G(ti−1, W(ti−1))[W(ti) −W(ti−1)]

,
(17.89)
where “ms-lim” refers to the mean square limit.
c
⃝2000 by Chapman & Hall/CRC

2. For the Stratonovich stochastic integral (indicated by S
J
), we choose
τi = (ti + ti−1)/2 (i.e., α = 1/2 in the above). That is:
S
 t
t0
G(W(s), x) dW(s)
= ms-lim
n→∞
( n

i=1
G

ti−1, w
ti + ti−1
2

[W(ti) −W(ti−1)]

.
(17.90)
Example 17.81:
Consider the integral J t
t0 W(s) dW(s). The Itˆo and Stratonovich
evaluations are
1.
I
J t
t0 W(s) dW(s) = 
W 2(t) −W 2(t0) −(t −t0)
/2 .
2.
S
J t
t0 W(s) dW(s) = 
W 2(t) −W 2(t0)
/2.
17.15.5
Stochastic diﬀerential equations
If a diﬀerential equation contains random terms, then the solution can only
be described statistically.
A linear ordinary diﬀerential equation for, say,
x(t) with linearly appearing white Gaussian noise terms can be converted
to a parabolic partial diﬀerential equation whose solution is the probability
density of x(t). This equation is called a Fokker–Planck equation or a forward
Kolmogorov equation.
Consider the linear diﬀerential system for the m component vector X(t)
d
dtX(t) = b(t, X) + σ(t, X) N(t),
X(t0) = y,
(17.91)
where σ(t, X) is a real m × n matrix and N(t) is a vector of n independent
white noise terms. That is,
E [Ni(t)] = 0,
E [Ni(t)Nj(t + τ)] = δijδ(τ),
(17.92)
where δij is the Kronecker delta, and δ(τ) is the delta function. The Fokker–
Planck equation corresponding to equation (17.91) is
∂P
∂t = −
m

i=1
∂
∂xi
(biP) + 1
2
m

i,j=1
∂2
∂xi∂xj
(aijP) ,
(17.93)
where P = P(t, x) is a probability density and the matrix A = (aij) is deﬁned
by A(t, x) = σ(t, x)σT(t, x).
Any statistical information about X(t) that
could be ascertained from equation (17.91) may be derived from P(t, x).
In one dimension, the stochastic diﬀerential equation
dX
dt = f(X) + g(X)N(t),
(17.94)
c
⃝2000 by Chapman & Hall/CRC

with X(0) = z, corresponds to the Fokker–Planck equation
∂P
∂t = −∂
∂x(f(x)P) + 1
2
∂2
∂x2 (g2(x)P),
(17.95)
for P(t, x) with P(0, x) = δ(x −z).
Example 17.82:
The Langevin equation
X′′ + βX′ = N(t),
(17.96)
with the initial conditions X(0) = 0 and X′(0) = u0 can be written as the vector system
d
dt
X
U

=
 U
−βU

+
0 0
0 1
 N1(t)
N2(t)

,
X
U

t=0
=
 0
u0

.
(17.97)
The Fokker–Planck equation for P(t, x, u), the joint probability density of X and U at
time t, is
∂P
∂t = −∂
∂x(uP) + ∂
∂u(βuP) + 1
2
∂2P
∂u2 ,
P(0, x, u) = δ(x)δ(u −u0).
(17.98)
This equation has the solution
P(t, x, u) =
1
det D exp

−
x −µx
u −µu

D
x −µx
u −µu
T
,
(17.99)
where D =
σxx σxu
σxu σuu

, and the parameters {µx, µu, σxx, σxu, σuu} are given by
µx = u0
β

1 −e−βt
,
µu = u0e−βt,
σ2
xx = t
β2 −2
β3

1 −e−βt
+
1
2β3

1 −e−2βt
,
σ2
xu = 1
β2

1 −e−βt
−
1
2β2

1 −e−2βt
,
σ2
uu = 1
2β

1 −e−2βt
.
(17.100)
17.15.6
Motion in a domain
Consider a particle starting at y and randomly moving in a domain Ω. If the
probability density of the location evolves according to
∂P
∂t = L[P] = −
m

i=1
bi(y)∂P
∂yi
+ 1
2
m

i,j=1
aij(y) ∂2P
∂yi∂yj
,
(17.101)
then
c
⃝2000 by Chapman & Hall/CRC

(a) The expectation of the exit time w(y) is the solution of L[w] = −1 in
Ω, with w = 0 on ∂Ω.
(b) The probability u(y) that the exit occurs on the boundary segment Γ is
the solution of L[u] = 0 in Ωwith
u(y) =
(
1
for y ∈Γ
0
for y ∈Ω/Γ.
17.15.7
Option Pricing
Let S represent the price of a share of stock, and presume that S follows
a geometric Brownian motion dS = µS dt + σS dω, where t is time, µ is a
constant, and σ is a constant called the volatility. Let V (S, t) be the value of
a derivative security whose payoﬀis solely a function of S and t. Construct a
portfolio consisting of V and ∆shares of stock. The value P of this portfolio
is P = V + S∆. The random component of the portfolio increment (dP) can
be removed by choosing ∆= −∂V/∂S. The concept of arbitrage says that
dP = rP dt, where r is the (constant) risk-free bank interest rate. Together
this results in the Black–Scholes equation for option pricing (note that no
transaction costs are included):
∂V
∂t + rS ∂V
∂S + 1
2σ2S2 ∂2V
∂S2 −rV = 0
(17.102)
If the asset pays a continuous dividend of DS dt (i.e., this is proportional to
the asset value S during the time period dt), then the equation is modiﬁed to
become
∂V
∂t + (r −D)S ∂V
∂S + 1
2σ2S2 ∂2V
∂S2 −rV = 0
(17.103)
If E is the exercise price of the option, and T is the expiry date (the only
date on which the option can be exercised), then the solution of the modiﬁed
Black–Scholes equation is
V (S, t) = e−D(T −t)SN(d1) −Ee−r(T −t)Φ(d2)
(17.104)
where
d1 = log(S/E) + (r −D + σ2/2)(T −t)
σ
√
T −t
,
d2 = d1 −σ
√
T −t,
(17.105)
and Φ is the cumulative probability distribution for the normal distribution.
c
⃝2000 by Chapman & Hall/CRC

17.16
CLASSIC AND INTERESTING PROBLEMS
17.16.1
Approximating a distribution
Given the moments of a distribution (µ, σ, and {γi}), the asymptotic proba-
bility density function is given by
f(t) = φ(x) −γ1
6 φ(3)(x) +
γ2
24φ(4)(x) + γ2
1
72φ(6)(x)

−
 γ3
120φ(5)(x) + γ1γ2
144 φ(7)(x) +
γ3
1
1296φ(9)(x)

+ . . .
(17.106)
where φ(x) =
1
σ
√
2π e−(x−µ)2/2σ2 is the normal density function.
17.16.2
Averages over vectors
Let f(n) denote the average of the function f as the unit vector n varies
uniformly in all directions in three dimensions. If a, b, c, and d are constant
three-dimensional vectors, then
|a · n|2 = |a|2 /3
(a · n)(b · n) = (a · b)/3
(a · n)n = a/3
|a × n|2 = 2 |a|2 /3
(a × n) · (b × n) = 2a · b/3
(a · n)(b · n)(c · n)(d · n) = [(a · b)(c · d) + (a · c)(b · d)
+ (a · d)(b · c)]/15
(17.107)
Now let f(n) denote the average of the function f as the unit vector n varies
uniformly in all directions in two dimensions. If a and b are constant two-
dimensional vectors, then
|a · n|2 = |a|2 /2
(a · n)(b · n) = (a · b)/2
(a · n)n = a/2
(17.108)
17.16.3
Bertrand’s box “paradox”
Suppose there are three small boxes, each with two drawers, each containing a
coin. One box has two gold coins, another has two silver coins, and the third
has one gold and one silver coin. Suppose a box and a drawer are selected at
random, and a gold coin is discovered. What is the probability that the other
drawer also contains a gold coin?
c
⃝2000 by Chapman & Hall/CRC

If a gold coin is found, even though the box selected is not the one with two
silver coins, it is not equally likely to be one of the remaining two boxes. Given
a gold coin has been found, there are two ways this could happen if selecting
a drawer from the G/G box, and only one way if selecting from the the G/S
box. Therefore, the probability the box is G/G is 2/3 and the probability the
other coin in the selected box is gold is 2/3.
17.16.4
Bertrand’s circle “paradox”
Consider the following problem:
Given a circle of unit radius, ﬁnd the probability that a randomly
chosen chord will be longer than the side of an inscribed equilateral
triangle.
There are at least three solutions to this problem:
(a) Randomly choose two points on a circle and measure the distance be-
tween them.
The result depends only on the position of the second
point relative to the ﬁrst one (i.e., the ﬁrst point can be ﬁxed). Con-
sider chords that emanate from a ﬁxed ﬁrst point; 1/3 of the resulting
chords will be longer that the side of an equilateral triangle. Therefore,
the probability is 1/3.
(b) A chord is completely speciﬁed by its midpoint. If the length of a chord
exceeds the side of an equilateral triangle, then the midpoint must be
inside a smaller circle of radius 1/2. The area of the smaller circle is 1/4
of the area of the original (unit) circle. Therefore, the probability is 1/4.
(c) A chord is completely speciﬁed by its midpoint.
If the length of a
chord exceeds the side of an equilateral triangle, then the midpoint is
within 1/2 of the center of the circle. If the midpoints are distributed
uniformly over the radius (instead of over the area, as assumed in ((b)),
the probability is 1/2.
It all depends on how randomly is deﬁned. See M. Kac and S. M. Ulam,
Mathematics and Logic, Dover Publications, NY, 1968.
17.16.5
Bingo cards: nontransitive
Consider the 4 bingo cards shown below (labeled A, B, C, and D), which
were created by D. E. Knuth. Two players each select a bingo card. Numbers
from 1 to 6 are randomly drawn without replacement, as they are in standard
bingo. If a selected number is on a card, it is marked with a bean. The ﬁrst
player to complete a horizontal row wins. It can be shown that, statistically,
card A beats card B, card B beats card C, card C beats card D, and card D
beats card A.
A
B
C
D
1
2
3
4
2
4
5
6
1
3
4
5
1
5
2
6
c
⃝2000 by Chapman & Hall/CRC

17.16.6
Birthday problem
The probability that n people have diﬀerent birthdays is
qn =
364
365

·
363
365

· · ·
366 −n
365

(17.109)
Let pn = 1 −qn. For 23 people the probability of at least two people having
the same birthday is more than half (p23 = 1 −q23 > 1/2).
n
10
20
23
30
40
50
pn
0.117
0.411
0.507
0.706
0.891
0.970
The number of people needed to have a 50% chance of two people having the
same birthday is 23. The number of people needed to have a 50% chance of
three people having the same birthday is 88. For four, ﬁve, and six people
having the same birthday the number of people necessary is 187, 313, and
460.
The number of people needed so that there is a 50% chance that two people
have a birthday within one day of each other is 14. In general, in a n-day
year the probability that p people all have birthdays at least k days apart (so
k = 1 is the original birthday problem) is
probability =
n −p(k −1) −1
p −1
(p −1)!
np−1
(17.110)
The non-equiprobable case was solved in M. Klamkin and D. Newman, Ex-
tensions of the birthday surprise, J. Comb. Theory, 3 (1967), pages 279–282.
17.16.7
Buﬀon’s needle problem
A needle of length L is placed at random on a plane on which are drawn
parallel lines a distance D apart.
Assume that L < D so that only one
intersection is possible. The probability P that the needle intersects a line is
P = 2L
πD
(17.111)
If a convex object of perimeter s, with a maximum diameter less than D, is
randomly placed on the ruled plane, then the probability P that the object
intersects a line is P = s/πD.
If a needle of length L is dropped on a rectilinear ruled grid, with line separa-
tions of a and b (with L < a and L < b), then the probability that the needle
will land on a line is 2L(a + b) −L2
abπ
.
17.16.8
Card problems
17.16.8.1
Shuﬄing cards
A riﬄe shuﬄe takes a deck of n cards, splits it into two stacks, and then
recombines them. We assume that the probability that the split occurs after
c
⃝2000 by Chapman & Hall/CRC

the kth card has a binomial distribution. The recombination occurs by taking
cards, one by one, oﬀof the bottom of the two stacks, and placing them onto
one stack; if there are k1 and k2 cards in the two stacks, then the probability
of taking the next card from the ﬁrst (resp. second) stack is
k1
k1+k2 (resp.
k2
k1+k2 ).
Let Ωn = {π1, π2, . . . , πn!} denote the possible permutations of n cards. Let
fX(πi) be the probability that operator X (a riﬄe shuﬄe) produces the or-
dering πi. The variation distance between the process X and a uniform dis-
tribution on Ωn is
||fX −u|| = 1
2

π∈Ωn
$$$$fX(π) −1
n!
$$$$
(17.112)
where the 1
2 is used so that value falls in the interval [0, 1]. For the riﬄe shuﬄe
equation (17.112) becomes
||fX −u|| = 1
2
n

r=1
An,r
$$$$
2k + n −r
n
 1
2nk −1
n!
$$$$
(17.113)
where the Eulerian numbers {An,r} are deﬁned by An,1 = 1 and
An,a = an −
a−1

r=1
n + a −r
n

An,r
(17.114)
Table 17.2 contains numerical values for a n = 52 card deck. Until 5 shuﬄes
have occurred, the output of X is very far from random. After 5 shuﬄes,
the distance from the random process is essentially halved each time a shuﬄe
occurs.
See D. Bayer and P. Dianconis, Trailing the dovetail shuﬄe to its lair, Annals
of Applied Probability, 2(2), 1992, pages 294–313.
17.16.8.2
Card games
(a) Poker hands
The number of distinct 13-card poker hands is
52
5

= 2,598,960.
Hand
Probability
Odds
royal ﬂush
1.54 × 10−6
649,739:1
straight ﬂush
1.39 × 10−5
72,192:1
four of a kind
2.40 × 10−4
4,164:1
full house
1.44 × 10−3
693:1
ﬂush
1.97 × 10−3
508:1
straight
3.92 × 10−3
254:1
three of a kind
0.0211
46:1
two pair
0.0475
20:1
one pair
0.423
1.37:1
c
⃝2000 by Chapman & Hall/CRC

Number of riﬄe shuﬄes
Variation distance
1
1
2
1
3
1
4
0.99999953
5
0.924
6
0.614
7
0.334
8
0.167
9
0.0854
10
0.0429
11
0.0215
12
0.0108
13
0.00538
14
0.00269
Table 17.2: Number of riﬄe shuﬄes versus variation distance
(b) Bridge hands
The number of distinct 13-card bridge hands is
52
13

= 635,013,559,600.
In bridge, the honors are the ten, jack, queen, king, or ace. Obtaining
the three top cards (ace, king, and queen) of three suits and the ace, king,
queen, and jack of the remaining suit is called 13 top honors. Obtaining
all cards of the same suit is called a 13-card suit. Obtaining 12 cards
of the same suit with ace high and the 13th card not an ace is called a
12-card suit, ace high. Obtaining no honors is called a Yarborough.
Hand
Probability
Odds
13 top honors
6.30 × 10−12
158,753,389,899:1
13-card suit
6.30 × 10−12
158,753,389,899:1
12-card suit, ace high
2.72 × 10−9
367,484,698:1
Yarborough
5.47 × 10−4
1,827:1
four aces
2.64 × 10−3
378:1
nine honors
9.51 × 10−3
104:1
17.16.9
Coin problems
17.16.9.1
Even odds from a biased coin
If you have a coin biased toward heads, it is possible to get the equivalent of
a fair coin with several tosses of the unfair coin.
Toss the biased coin twice. If both tosses give the same result, repeat this
process (throw out the two tosses and start again). Otherwise, take the ﬁrst
of the two results as the toss of an unbiased coin.
c
⃝2000 by Chapman & Hall/CRC

17.16.9.2
Two heads in a row
What is the probability in n ﬂips of a fair coin that there will be two heads
in a row?
Consider strings of n H’s and T’s.
We count the number of strings that
do not contain HH and subtract it from the total number of such strings
(which is 2n). There must be no more than n/2 H’s; otherwise two heads
would be adjacent. If a string contains i H’s, with no two of them in a row,
then these H’s must be placed between (or around) the (n −i) T’s present:
there are
n−i+1
i

ways to do this. Hence, the total number of strings that do
not contain HH is
n/2

i=0
n −i + 1
i

= Fn+2 (where Fm is the mth Fibonacci
number). Hence, the probability that n coin tosses will contain a HH is:
2n −Fn+2
2n
.
17.16.10
Coupon collectors problem
There are n coupons that can be collected. At each time a random coupon
is selected, with replacement.
How long must one wait until they have a
speciﬁed collection of coupons?
Deﬁne Wn,j to be the number of time steps required until j diﬀerent coupons
are seen; then
E [Wn,j] = n
j

i=1
1
n −i + 1
Var [Wn,j] = n
j

i=1
i −1
(n −i + 1)2
(17.115)
When j = n, then all coupons are being collected and E [Wn,n] = nHn with
Hn = 1+1/2+1/3+· · ·+1/n. As n →∞, E [Wn,n] ∼n log n. Typical values
are shown below.
n
E [Wn,n]

Var [Wn,n]
2
3
1.41
5
11.4
5.02
10
29.3
11.2
50
225
62
100
519
126
200
1,176
254
c
⃝2000 by Chapman & Hall/CRC

In the unequal probabilities case, with pi being the probability of obtaining
coupon i,
E [Wn,n] =
 ∞
0
%
1 −
n
)
i=1

1 −e−pit
&
dt
(17.116)
17.16.11
Dice problems
17.16.11.1
Dice: nontransitive (Efron)
(a) Let A have the two six-sided dice with sides of {0, 0, 4, 4, 4, 4} and
{2, 3, 3, 9, 10, 11}
(b) Let B have the two six-sided dice with sides of {3, 3, 3, 3, 3, 3} and
{0, 1, 7, 8, 8, 8}
(c) Let C have the two six-sided dice with sides of {2, 2, 2, 2, 6, 6} and
{5, 5, 6, 6, 6, 6}
(d) Let D have the two six-sided dice with sides of {1, 1, 1, 5, 5, 5} and
{4, 4, 4, 4, 12, 12}
Then the odds of
(a) A winning against B
(b) B winning against C
(c) C winning against D
(d) D winning against A
are all 2:1.
17.16.11.2
Dice: distribution of sums
When rolling two dice, the probability distribution of the sum is
Prob [sum of s] = 6 −|s −7|
36
for 2 ≤s ≤12
(17.117)
When rolling three dice, the probability distribution of the sum is
Prob [sum of s] =
1
216





1
2(s −1)(s −2)
for 3 ≤s ≤8
−s2 + 21s −83
for 9 ≤s ≤14
[1
2 (19 −s)(20 −s)
for 15 ≤s ≤18
(17.118)
For 2 dice, the most common roll is a 7 (probability 1/6). For 3 dice, the
most common rolls are 10 and 11 (probability 1/8 each). For 4 dice, the most
common roll is a 14 (probability 73/648).
17.16.11.3
Dice: same distribution
Ordinary six-sided dice have the numbers {1, 2, 3, 4, 5, 6} on the sides. Rolling
two dice and summing the numbers face up creates a distribution of values
from 2 to 12 (see equation (17.117)). That same distribution of sums can be
obtained by rolling two cubes, with the following numbers on their sides:
c
⃝2000 by Chapman & Hall/CRC

• {0, 1, 1, 2, 2, 3} and {2, 4, 5, 6, 7, 9}
• {0, 2, 3, 4, 5, 7} and {2, 3, 3, 4, 4, 5}
• {1, 2, 2, 3, 3, 4} and {1, 3, 4, 5, 6, 8} (Sicherman dice)
17.16.12
Ehrenfest urn model
Suppose there are two urns and 2R numbered balls, and suppose that there
are R + a balls in urn I and R −a balls in urn II. Each second a ball is
chosen at random and moved from its urn into the other urn. The procedure
is repeated.
Consider the case of R = 10, 000. If a = 0, then the expected time to return
to the initial conﬁguration is 175 seconds. If a = R, then the expected time
to return to the initial conﬁguration is approximately 106000 years.
17.16.13
Envelope problem “paradox”
The envelope exchange problem is
“Someone has prepared two envelopes containing money.
One
contains twice as much money as the other. You have decided to
pick one envelope, but then the following argument occurs to you:
‘Suppose my chosen envelope contains X, then the other
envelope either contains X/2 or 2X.
Both cases are
equally likely, so my expectation if I take the other en-
velope is 1
2
 X
2

+ 1
2 (2X) = 1.25X, which is higher than
my current X. Hence, I should change my mind and
take the other envelope.’
But then I can apply the argument all over again. There must be
sometime wrong.”
There is, in fact, no contradiction. Switching the envelope or not switching the
envelope results in the same expected return. See R. Christensen and J. Utts,
Bayesian resolution of the ‘Exchange Paradox’, The American Statistician, 46
(4), 1992, pages 274–276.
17.16.14
Gambler’s ruin problem
A gambler starts with z dollars. For each gamble: with probability p she wins
one dollar, with probability q she loses one dollar. Gambling stops when she
has either zero dollars, or a dollars.
c
⃝2000 by Chapman & Hall/CRC

If qz denotes the probability of eventually stopping at z = a (“gambler’s
success”) then
qz =









(q/p)a −(q/p)z
(q/p)a −1
if p ̸= q
1 −z
a
if p = q = 1
2
(17.119)
For example:
p
q
z
a
qz
fair
0.5
0.5
9
10
.900
game
0.5
0.5
90
100
.900
0.5
0.5
900
1000
.900
0.5
0.5
9000
10000
.900
biased
0.4
0.6
90
100
.017
game
0.4
0.6
90
99
.667
17.16.15
Gender distributions
For these problems, there is a 50/50 chance of male or female on each birth.
1. Hospital deliveries: Every day a large hospital delivers 1000 babies and
a small hospital delivers 100 babies. Which hospital has a better chance
of having the same number of boys as girls?
The small one. If 2n babies are born, then the probability of an even
split is
2n
n

22n . This is a decreasing function of n. As n goes to inﬁnity
the probability of an even split approaches zero (although it is still the
most likely event).
2. Family planning
Suppose that in large society of people, every family continues to have
children until they have a girl, then they stop having children. After
many generations of families, what is the ratio of males to females?
The ratio will be 50-50; half of all conceptions will be male, half female.
3. If a person has two children and
(a) The older one is a girl, then the probability that both children are
girls is 1/2.
(b) At least one is a girl, then the probability that both children are
girls is 1/3.
c
⃝2000 by Chapman & Hall/CRC

17.16.16
Holtzmark distribution: stars in the galaxy
The force f on a unit mass due to a star of mass m at a location r is f = Gm
|r|3 r.
The probability density for the magnitude of the force, f = |f|, due to a
uniform distribution of stars in the galaxy, with random masses, with λ stars
per unit volume, is given by
p(f) =
1
4πa2β2 H(β)
a = 4λ
15 (2πG)3/2 E
"
m3/2#
β = f/a2/3
H(β) = 2
πβ
 ∞
0
x e−(x/β)3/2 sin x dx
(17.120)
Note that
H(β) ∼



4β2
3π + O(β4)
as β →0
15
8

2
πβ−5/2 + O(β−4)
as β →∞
(17.121)
See S. Chandrasekhar, Stochastic problems in physics and astronomy, Reviews
of Modern Physics, 15, number 1, January 1943, pages 1–89.
17.16.17
Large-scale testing
17.16.17.1
Infrequent success
Suppose that a disease occurs in one person out of every 1000.
Suppose
that a test for this disease has a type I and a type II error of .01 (that is,
α = β = 0.01). Imagine that 100,000 people are tested. Of the 100 people
who have the disease, 99 will be diagnosed as having it. Of the 99,900 people
who do not have the disease, 999 will be diagnosed as having it. Hence, only
99
1098 ≈9% of the people who test positive for the disease actually have it.
17.16.17.2
Pooling of blood samples
A large population of persons is to be screened for the presence of some
condition using a blood test which registers positive if a sample contains
blood from a person having the condition and registers negative otherwise.
To implement the screening, the blood specimens of groups of k persons are
pooled, and the pooled blood samples are tested. If a pooled sample registers
negative, the group is cleared; otherwise each person in the group is tested.
If the probability of a random person having a positive test is p, what group
size k∗minimizes the expected number of tests per person? If f(k) is the
c
⃝2000 by Chapman & Hall/CRC

expected number of tests per person, then
f(k) =
(
1
if k = 1
1 −(1 −p)k + 1/k
if k ≥2
(17.122)
If {{x}} = x −⌊x⌋denotes that fractional part of x, then
(a) If p > 1 −
 1
3
1/3 ≈0.3066 then k∗= 1.
(b) If p < 1 −
 1
3
1/3 then either
• k∗= 1 +
O
p−1/2P
; or
• k∗= 2 +
O
p−1/2P
.
(c) If
>>
p−1/2??
<
O
p−1/2P
2
O
p−1/2P
+

p−1/2, then k∗= 1 +
O
p−1/2P
.
(d) It is never the case that k∗= 2.
Some values of k∗:
p
.0001
.0005
.001
.005
.01
.05
.1
.2
.3
.4
.5
k∗
101
45
32
15
11
5
4
3
1
1
1
See S. M. Samuels, The exact solution to the two-stage group-testing problem,
Technometrics, 1978, 20, pages 497–500.
17.16.18
Leading digit distribution
17.16.18.1
Ratio of uniform numbers
If X and Y are chosen uniformly from the interval (0, 1], what is the proba-
bility that the ratio Y/X starts with a 1? What is the probability that the
ratio Y/X starts with a 9?
(a) If Y/X has a leading digit of 1, then Y/X must lie in one of the intervals
{. . . , [0.1, 0.2), [1, 2), [10, 20), . . . }. This corresponds to Y/X lying in
one of several triangles with height 1 and bases on either the right or top
edges of the square. The triangles are deﬁned by the lines (Y = 0.1X,
Y = 0.2X, and X = 1), (Y = X, Y = X, and Y = 1), (Y = 10X,
Y = 20Xl, and Y = 1), . . . .
The bases along the right edge have
lengths 0.1+0.01+· · · = 1/9. The bases along the top edge have lengths
0.5+0.05+· · · = 5/9. For a total base length of 6/9 and a height of 1, the
total area is 1/3. The total area of the square is 1, hence the probability
that Y/X starts with a 1 is 1/3.
(b) In this case, similar triangles can be drawn; on the right edge the total
length is 1/9. On the top edge the length is 1/90 + 1/900 + · · · = 1/81.
Total base length is 10/81 and the total area (and hence probability of a
leading 9) is 5/81 ≈0.061728.
c
⃝2000 by Chapman & Hall/CRC

17.16.18.2
Benford’s law
Let Pd be the probability that the leading digit of a set of numbers is d.
Assuming that the probability distribution of leading digits is scale-invariant
(common, for example, for data that are dimensional), then Pd is approxi-
mately given by Pd ≈log10
 d+1
d

. Hence,
P1
P2
P3
P4
P5
P6
P7
P8
P9
0.301
0.176
0.125
0.097
0.079
0.067
0.058
0.051
0.048
17.16.19
Lotteries
Consider a collection of n numbers. A player chooses a subset T, called a
ticket, of these numbers of size k. The house chooses a subset S of these
numbers of size p (these are the winning numbers). The payoﬀdepends on
the size of the intersection between S and T. If the intersection is i elements,
then the payoﬀis wi.
If n, p, and k are ﬁxed, then
1. There are
n
p

possible tickets.
2. The number of ways that i of the player’s k choices are among the
selected p values is
k
i
n−k
p−i

.
3. The probability that i of the player’s k choices are among the selected
p values is (
k
i)(
n−k
p−i)
(
n
p)
.
4. The expected return is
p

i=0
wi
k
i
n−k
p−i

n
p

.
5. Each i-subset has the same probability (
p
i)
(
n
i) of appearing in the p-subset
chosen.
It is possible to obtain a large number of tickets and obtain few matches. If
n = 49 and p = 6 then there are 13,983,816 possible tickets.
1. There are
6
0
43
6

= 6, 096, 454 tickets that have no matches with the
winning numbers.
2. There are
6
0
43
6

+
6
1
43
5

= 11, 872, 042 tickets that have, at most,
only a 1-match with the winning numbers.
3. There are
6
0
43
6

+
6
1
43
5

+
6
2
43
4

= 13, 723, 192 tickets that have, at
most, only a 2-match with the winning numbers.
A lottery wheel is a system of buying multiple tickets to guarantee a minimum
match. For example, if n = 49 and p = 6 then it is possible to obtain 174
tickets and guarantee at least a 3-match.
See C. J. Colbourn and J. H. Dinitz, CRC Handbook of Combinatorial Designs,
CRC Press, Boca Raton, FL, 1996, pages 578–581.
c
⃝2000 by Chapman & Hall/CRC

17.16.20
Match box problem
A certain mathematician carries one match box in his right pocket and one in
his left pocket. When he wants a match, he selects a pocket at random. Each
box initially contains N matches. The probability ur that a selected box is
empty on the rth trial is
ur =
N −r
N

2−2N+r
(17.123)
17.16.21
Maximum entropy distributions
Deﬁne the entropy of a probability density function to be
J = −
J
f(x) log[f(x)] dx.
(a) Among all probability density functions restricted to the interval [−1, 1],
the one of minimum entropy has the form f(x) = C; that is, it is the
uniform distribution: f(x) =
(
1
2
for −1 ≤x ≤1
0
otherwise
(b) Among all probability density functions restricted to the interval [−1, 1]
and having a given mean of µ, the one of minimum entropy has the
form f(x) = Ceλx; that it f(x) =
(
λ
2 sinh λeλx
for −1 ≤x ≤1
0
otherwise
where
µ = λ−tanh λ
λ tanh λ .
(c) Among all probability density functions restricted to the interval [−1, 1]
that have a mean of zero and a given variance, the one of minimum
entropy has the form f(x) = Ceλ1x+λ2x2
See F. E. Udwadia, Some results on maximum entropy distributions for pa-
rameters known to lie in certain intervals, SIAM Review, 31, Number 1, March
1989, SIAM, Philadelphia, pages 103–109.
17.16.22
Monte Hall problem
Consider a game in which there are three doors: one door has a prize, two
doors have no prize. A player selects one of the three doors. Suppose someone
opens one of the two unselected doors that contains no prize. The player is
then asked if he would like to exchange the originally selected door for the
remaining unopened door.
To increase the chance of winning, the player
should switch doors: without switching the probability of winning is 1/3; by
switching the probability of winning is 2/3.
17.16.23
Multi-armed bandit problem
In the multi-armed bandit problem, a gambler must decide which arm of K
non-identical slot machines to play in a sequence of trials so as to maximize
his reward. This problem has received much attention because of the simple
model it provides of the trade-oﬀbetween exploration (trying out each arm
c
⃝2000 by Chapman & Hall/CRC

to ﬁnd the best one) and exploitation (playing the arm believed to give the
best payoﬀ).
17.16.24
Parking problem
Let E(x) denote the expected number of cars of length 1 which can be parked
on a block of length x if cars park randomly (and with a uniform distribution
in the available space). From the integral equation
E(x) = 1 +
2
x −1
 x−1
0
E(t) dt
(17.124)
consider the Laplace transform
 ∞
0
e−sxE(x) dx = e−s
s
 ∞
s
exp

−2
 t
s
1 −e−u
u
du

dt.
(17.125)
This implies E(x) ∼cx as x →∞(where c ≈0.7476).
17.16.25
Passage problems
Given a random function X(t), a passage (time) at a given level a is when
a graph of X(t) crosses the horizontal line X = A from below. Deﬁne the
derivative of X to be V (t) =
dX(t)
dt , and let f(x, v | t) be the probability
density of x and v at time t (deﬁne the probability density for X to be f(x)).
The probability that a passage (time) lies in the time interval dt about the
time t is p(a | t) dt and
p(a | t) =
 ∞
0
f(a, v | t) v dv
(17.126)
(a) For normal functions
p(a | t) = 1
2π exp

−(a −x)2
2σ2x
 +
1
Kx(t, t)
∂2Kx(t1, t2)
∂t1 ∂t2
$$$$
t1=t2=t
(17.127)
where Kx is the correlation function.
(b) For normal stationary functions
p(a | t) = p(a) = 1
2π
σv
σx
exp

−(a −x)2
2σ2x

(17.128)
(c) The number of passages of a stationary function during a time interval
T is N a = Tp(a).
(d) The average duration τa of a passage of a stationary function is
τa =
J ∞
a f(x) dx
p(a)
(17.129)
c
⃝2000 by Chapman & Hall/CRC

For normal stationary functions:
τa = π σx
σv
exp

−(a −x)2
2σ2x
 
1 −Φ
a −x
σx

(17.130)
(e) Finding the average number of minima of a random diﬀerentiable func-
tion can be reduced to passage problems since a maxima is achieved
when the ﬁrst derivative passes through zero from above. For a small
average number of passages during a time interval T, the approxi-
mate probability Q for non-occurrence of any run during this interval
is Q = e−Na (i.e., the number of passages in the given interval can be
approximated as a Poisson distribution).
17.16.26
Proofreading mistakes
Suppose that proofreader A ﬁnds a mistakes, and proofreader B ﬁnds b mis-
takes. Assume that there are c overlaps, mistakes that both A and B found.
The approximate number of total mistakes is ab
c and the approximate number
of mistakes missed by both A and B is (a −c)(b −c)
c
.
17.16.27
Raisin cookie problem
A baker creates enough cookie dough for 1000 raisin cookies. The number of
raisins to be added to the dough, R, is to be determined.
1. In order to be 99% certain that the ﬁrst cookie will have at least one
raisin, then 1 −
 999
1000
R ≥0.99, or R ≥4603.
2. In order to be 99% certain that every cookie will have at least one
raisin, then P(C, R) ≥0.99, where C is the number of cookies and
P(C, R) = C−R C
i=0
C
i

(−1)i(C −i)R. Or, R ≥11508.
17.16.28
Random sequences
17.16.28.1
Long runs
For a biased coin with a probability of heads p, let Rn denote that length of
the longest run of heads in the ﬁrst n tosses of the coin. It has been shown:
Prob
%
lim
n→∞
Rn
log1/p(n) = 1
&
= 1
(17.131)
See R. Arratia and M. S. Waterman, An Erd¨os–R´enyi law with shifts, Ad-
vances in Mathematics, 55, 1985, pages 13–23.
c
⃝2000 by Chapman & Hall/CRC

17.16.28.2
Waiting times: two types of characters
In a Bernoulli process with a probability of success p and a probability of
failure q = 1 −p, the probability that a run of α consecutive successes occurs
before a run of β consecutive failures is
probability = pα−1 
1 −qβ
pα−1 −qβ−1
(17.132)
The expected waiting time until either run occurs is
expected waiting time = (1 −pα)

1 −qβ
pαq + pqβ −pαqβ
(17.133)
The waiting times for strings of two characters and for strings of many types
of characters can be very diﬀerent.
17.16.28.3
Waiting times: many types of characters
Assume there are N diﬀerent characters, each equally likely to occur. Let
S = (a1, a2, · · · , an) be a sequence of characters, some of which may be equal.
If Sm = (a1, a2, . . . , am), for 1 ≤m ≤n, appears both at the beginning and at
the end of S, then Sm is an overlapping subsequence of S. The mean waiting
time until S appears depends on the lengths of the overlapping subsequences.
If the lengths of the overlapping subsequences are {k1, k2, . . . , kr} then,
expected waiting time =
r

i=0
N kr
(17.134)
Example 17.83:
Choosing letters randomly from the alphabet, it will take on the
average,
(a) 263 letters to get APE
(b) 263 + 26 letters to get DAD
(c) 2611 + 264 + 26 letters to get ABRACADABRA.
Example 17.84:
Flipping a fair coin, it will take on the average,
(a) 24 ﬂips to get HHTT
(b) 24 + 21 ﬂips to get HTHH
(c) 24 + 22 ﬂips to get TH TH
(d) 24 + 23 + 22 + 21 ﬂips to get HHHH
See G. Blom, On the mean number of random digits until a given sequence
occurs, J. Appl. Prob., 19, 1982, pages 136–143 and G. Blom and D. Thorurn,
How many random digits are required until given sequences are obtained?, J.
Appl. Prob., 19, 1982, pages 518–531.
c
⃝2000 by Chapman & Hall/CRC

[AA] = 1 0 1 0 = 10
A = T H T H
A = T H T H
[AB] = 0 1 0 1 = 5
A = T H T H
B = H T H H
[BB] = 1 0 0 1 = 9
B = H T H H
B = H T H H
[BA] = 0 0 0 0 = 0
B = H T H H
A = T H T H
Figure 17.2: Computation of leading numbers
17.16.28.4
First random sequence
Let a fair coin be ﬂipped until a given sequence appears. Let two competitors,
P and Q, choose sequences of heads and tails; the winner is the one whose
sequence appears ﬁrst. Conway’s leading number algorithm indicates who is
likely to win:
Given two sequences A = (a1, . . . , am) and B = (b1, . . . , bm), de-
ﬁne the leading number of A over B as a binary integer ϵnϵn−1 · · · ϵ1
via
ϵi =



1
if 1 ≤i ≤min(m, n) and the two sequence
(am−i+1, . . . , am) and (b1, . . . , bi) are identical
0
otherwise
(17.135)
Then, let [AB] denote the leading number of A over B. The odds
for B to precede A in a symmetric Bernoulli process are
([AA] −[AB]) : ([BB] −[BA])
(17.136)
Example 17.85:
Consider the two diﬀerent 4-tuples A =THTH and B =HTHH.
Figure 17.2 contains the needed leading number computations so that ([AA] −[AB]) :
([BB] −[BA]) (in equation 17.136) becomes (10 −5) : (9 −0) or 5 : 9. Hence the
odds that A occurs before B is 9/14 (see section 3.3.5).
Note that sequence A = THTH has a waiting time (see section 17.16.28.2) of 20 while
the sequence B = HTHH has a waiting time of 18. Hence, in this case, an event that is
less frequent in the long run is likely to happen before a more frequent event.
Table 17.3 has the probability of Q winning in a double game. Table 17.4
has the probability of Q winning in a triplet game. When playing a triplet
game, note that whatever triplet the ﬁrst player chooses, the second player
can choose a better one.
c
⃝2000 by Chapman & Hall/CRC

P \ Q
HH
HT
TH
TT
HH
—
1/2
1/4
1/2
HT
1/2
—
1/2
3/4
TH
3/4
1/2
—
1/2
TT
1/2
1/4
1/2
—
Table 17.3: Probability of P winning in a double game
P \ Q
HHH
HHT
HTH
HTT
THH
THT
TTH
TTT
HHH
—
1/2
2/5
2/5
1/8
5/12
3/10
1/2
HHT
1/2
—
2/3
2/3
1/4
5/8
1/2
7/10
HTH
3/5
1/3
—
1/2
1/2
1/2
3/8
7/12
HTT
3/5
1/3
1/2
—
1/2
1/2
3/4
7/8
THH
7/8
3/4
1/2
1/2
—
1/2
1/3
3/5
THT
7/12
3/8
1/2
1/2
1/2
—
1/3
3/5
TTH
7/10
1/2
5/8
1/4
2/3
2/3
—
1/2
TTT
1/2
3/10
5/12
1/8
2/5
2/5
1/2
—
Table 17.4: Probability of P winning in a triplet game
17.16.29
Random walks
17.16.29.1
Random walk on a grid
Consider a random walk of uniform step lengths on a one-, two-, or three-
dimensional grid. Each step goes in any of the 2, 4, or 8 directions randomly.
In one and two dimensions, the random walk will, with probability 1, return
to the starting location. In three dimensions the probability of a return to
the origin is approximately 0.34053.
The probability of return to the origin in d dimensions (for d = 4, 5, . . . ) is
p(d) = 1 −
1
u(d) where u(d) =
J ∞
0

I0
 t
d
d e−t dt. The approximate probabil-
ities of returning to the origin are as follows:
dimension
4
5
6
7
8
probability
0.20
0.136
0.105
0.0858
0.0729
17.16.29.2
Random walk in two dimensions (Rayleigh problem)
Consider a two-dimensional random walk in which each step, xi, is of length
a and is taken in a random direction. The position after n steps is (X1, X2) =
n
i=1 Xi. The radial distance after n steps is Rn =

X2
1 + X2
2. If p(x) is the
probability density for a single step and pn(x) is the probability density after
c
⃝2000 by Chapman & Hall/CRC

n steps, then
p(x) =
1
2πaδ(|x| −a)
φ(ξ) =
 ∞
−∞
 ∞
−∞
p(x) eiξ·x dx = J0 (a |ξ|)
pn(x) = 1
2π
 ∞
0
ρ J0 (ρ |x|) [J0(ρa)]n dρ
Prob [RN < r] = r
 ∞
0
J1(ru) [J0(au)]n du
(17.137)
Note that
E [X1] = E [X2] = 0
E [X1X2] = 0
E

X2
1

= E

X2
2

= na2
2
E

R2
n

= na2
(17.138)
If, instead, the ith step has length ai, then
Prob [RN < r] = r
 ∞
0
J1(ru)

J0(a1u) · J0(a2u) · · · J0(anu)

du
(17.139)
17.16.29.3
Random walk in three dimensions
Consider a three-dimensional random walk in which each step, Xi, is of length
a and is taken in a random direction. If p(x) is the probability density for a
single step and pn(x) is the probability density after n steps, then
p(x) =
1
4πa2 δ(|x| −a)
φ(ξ) =
 ∞
−∞
 ∞
−∞
 ∞
−∞
px(x)eiξ·x dx =
1
a |ξ| sin(aξ)
pn(x) =
1
2π2
 ∞
0
ρ2
sin(aρ)
aρ
n sin(|x| ρ)
|x| ρ
dρ
(17.140)
Note that
p2(x) =



1
8πa2 |x|
for |x| ≤2a
0
for |x| > 2a
(17.141)
c
⃝2000 by Chapman & Hall/CRC

p3(x) =











1
8πa3
for 0 ≤|x| ≤a
3a −|x|
16πa3 |x|
for a < |x| ≤3a
0
for 3a < |x|
(17.142)
17.16.29.4
Self-avoiding walks
Consider a random walk of n unit length steps on a d-dimensional grid. A
walk in which no grid point is visited twice is called a self-avoiding walk;
the number of such walks is denoted c(n). Known values include: c(0) = 1,
c(1) = 2d, and c(2) = 2d(d −1). For each d there is a non-zero constant
µd (the connective constant) such that c(n) ∼(µd)1/n as n →∞. Current
bounds include:
2.62 < µ2 < 2.70
4.57 < µ3 < 4.75
6.74 < µ4 < 6.82
8.82 < µ5 < 8.87
10.87 < µ6 < 10.89
(17.143)
See J. Noonan, New upper bounds for the connective constants of self-avoiding
walks, J. Statist. Physics, 91, 1998, pages 871–888.
17.16.30
Relatively prime integers
Given two integers chosen at random, the probability that they are relatively
prime is [ζ(2)]−1 = 6/π2 ≈0.608. Given three integers chosen at random, the
probability they have no common factor other than 1 is [ζ(3)]−1 ≈1.202−1 ≈
0.832.
17.16.31
Roots of a random polynomial
Consider the polynomial f(x; a) = a0+a1x+· · ·+an−1xn−1 where the {ai} are
chosen randomly. To determine the expected number of real roots, suppose:
(a) The {ai} are chosen uniformly distributed on the interval (−1, 1); or
(b) The {ai} are chosen equal to +1 and −1 with equal probability; or
(c) The {ai} are chosen to be independent standard normal coeﬃcients
then the expected number of real roots is
E [number of real roots] = 4
π
 1
0
+
1
(1 −t2)2 −(n + 1)2t2n
(1 −t2n+2)2 dt
∼2
π log n + C + 2
nπ + O
 1
n2

as n →∞
(17.144)
c
⃝2000 by Chapman & Hall/CRC

where C = 0.6257358072 . . . .
Consider a random polynomial of degree n with coeﬃcients that are indepen-
dent and identically distributed normal random variables. Deﬁne m ̸= 0 to be
the mean divided by the standard deviation (m = µ/σ). Then the expected
number of real roots as n →∞is
1
π log n + C + 1
2
−2 + γ
π
−2
π log |m| + O
 1
n

(17.145)
where γ is Euler’s constant.
See M. Kac, On the average number of real roots of a random algebraic
equation, Bulletin of the AMS, 49, pages 314–320, 1943, and A. T. Bharucha-
Reid and M. Sambandham, Random Polynomials, Academic Press, New York,
Chapter 4, pages 49–102, 1986.
17.16.32
Roots of a random quadratic
If A, B, and C are independent random variables uniformly distributed on
(0, 1) then the probability that Ax2 + Bx + C = 0 has real roots is (5 +
3 ln 4)/36.
17.16.33
Simpson paradox
Consider the comparison of a new treatment with an old treatment, and sup-
pose that the following results are obtained:
All patients
improved
not improved
percent improved
New treatment
20
20
50%
Old treatment
24
16
60%
On the basis of these data, one would be inclined to say that the new treatment
is not better than the old treatment. Now consider a disaggregation of these
data into the following two sub-groups:
Young patients
improved
not improved
percent improved
New treatment
12
18
40%
Old treatment
3
7
30%
Old patients
improved
not improved
percent improved
New treatment
8
2
80%
Old treatment
21
9
70%
From these data, one would be inclined to say that the new treatment is better
than the old treatment for both young and old patients. This is an example
of Simpson’s paradox, and it is not a small sample eﬀect.
Simpson’s paradox reﬂects the fact that it is possible for all three of the
following inequalities to hold simultaneously:
c
⃝2000 by Chapman & Hall/CRC

Prob [I | A, B] > Prob [I | A, B′]
Prob [I | A′, B] > Prob [I | A′, B′]
Prob [I | B] < Prob [I | B′]
(17.146)
However, if Prob [A | B] = Prob [A | B′] then it is not possible for the 3 equa-
tions in (17.146) to hold. Practically, this means that the relative proportions
(new treatment versus old treatment) should be the same for both new and
old patients.
17.16.34
Secretary call problem
Suppose a secretary must be hired and n applicants are in line. Each applicant
will be interviewed one at a time. Following each interview an immediate
decision is made to hire or reject the candidate. Once a candidate has left,
he cannot be called back. The following strategy may be used to ﬁnd the best
secretary.
For suﬃciently large n, the interviewer should consider n/e candi-
dates, and then select the next candidate that is better than any
seen previously. Using this strategy, the probability of selecting
the best candidate is approximately 1/e.
See J. S. Rose, The secretary problem with a call option, Operations Research
Letters, Vol. 3, No. 5, December 1984.
17.16.35
Waiting for a bus
Suppose buses pass a certain corner with an average time between them of 20
minutes. What is the average time that one would expect to wait for a bus?
If the buses are exactly 20 minutes apart, then the average waiting time
is 10 minutes. If the buses’ arrival pattern is a Poisson distribution, then
the average waiting time is 20 minutes.
If the buses arrival pattern is a
hyperexponential distribution, then the average waiting time may exceed 20
minutes.
17.17
ELECTRONIC RESOURCES
Thanks are extended to John C. Pezzullo for supplying the source for much
of this section.
17.17.1
Statlib
Statlib, located at http://lib.stat.cmu.edu is a system for distributing
statistical software, datasets, and information by electronic mail, FTP and
WWW. The main web page includes pointers to:
(a) The apstat collection; a nearly complete set of algorithms published in
Applied Statistics.
c
⃝2000 by Chapman & Hall/CRC

(b) The CMLIB collection of non-proprietary Fortran subprograms solving
a variety of mathematical and statistical problems (originally produced
by NIST).
(c) The DASL library of dataﬁles and stories that illustrate the use of basic
statistics methods.
(d) A collection of interesting datasets, from classics like the Stanford heart
transplant data, to the complete data from several textbooks. For exam-
ple, over 6Mb of data and descriptions are available from Case Studies
in Biometry, by N. Lange, L. Ryan, L. Billard, and D. Brillinger, John
Wiley & Sons, New York, 1994.
(e) A collection of designs, programs, and algorithms for creating designs
for statistical experiments.
(f) Algorithms from Applied Statistics Algorithms by P. Griﬃths and I. D.
Hill, Ellis Horwood, Chichester, 1985.
(g) A collection of software related to articles published in the Journal of
the American Statistical Association.
(h) The jcgs archive of contributed datasets and software and abstracts from
the Journal of Computational Graphics and Statistics.
(i) The jqt collection of algorithms from articles published in the Journal
of Quality Technology.
(j) P-Stat functions and related software.
(k) Software and macros for the Genstat language.
(l) Macros, software, and algorithms for the GLIM statistical package.
(m) Software and extensions for the S (Splus) language. Over 130 separate
packages including many novel statistical ideas.
(n) The Sapaclisp collection of Lisp functions that can be used for compu-
tations described in Spectral Analysis for Physical Applications: Multi-
taper and Conventional Univariate Techniques, by D. B. Percival and
A. T. Walden, Cambridge University Press, Cambridge, England, 1993.
(o) The MacAnova statistical system (for Mac, Windows, and Unix).
(p) Macros for Minitab.
(q) Materials related to the Stata statistical package.
(r) Lisp-Stat, which is an extensible environment for statistical computing
and dynamic graphics based on Lisp.
17.17.2
Uniform resource locators
The following is a list of interesting Uniform Resource Locators (URL):
(a) http://www.stat.berkeley.edu/~stark/SticiGui/Text/
gloss.htm
A very useful glossary.
c
⃝2000 by Chapman & Hall/CRC

(b) http://www.yahoo.com/Science/Mathematics/
A very large list of useful sites relating to mathematics. It is perhaps
the best place to start researching an arbitrary mathematical question
not covered elsewhere in this list.
(c) http://daisy.uwaterloo.ca/~alopez-o/math-faq/
math-faq.html The FAQ (frequently asked questions) listing from the
newsgroup sci.math.
(d) http://e-math.ams.org/
The American Mathematical Society home page, with information about
AMS-TEX, the Combined Membership List of the AMS, Math Reviews
subject classiﬁcations, preprints, etc.
(e) http://www.siam.org/
The Society for Industrial and Applied Mathematics.
(f) http://gams.cam.nist.gov/
The Guide to Available Mathematical Software (GAMS).
(g) http://www.york.ac.uk/depts/maths/histstat/welcome.htm
The early history of statistics: a collection of seminal papers by Bayes,
Pascal, Laplace, Legendre, and others.
(h) http://www-sci.lib.uci.edu/HSG/RefCalculators.html#STAT
Martindale’s reference desk—calculators on–line—statistics: the largest
compendia of calculating web pages.
(i) http://www.fedstats.gov
FedStats: master page maintained by the Federal Interagency Council
on Statistical Policy to provide easy access to the full range of statistics
and information produced by more than 70 agencies in the United States
Government.
(j) http://members.aol.com/johnp71/javastat.html
A collection of web pages with links to many statistical sites on the web,
maintained by John C. Pezzullo.
17.17.3
Interactive demonstrations and tutorials
(a) Statistics tutorials
http://204.215.60.174/tutindex.html
These brieﬂy explain the use and interpretation of standard statistical
analysis techniques, using the WINKS program from TexaSoft. Tutori-
als include:
• descriptive statistics program
• Friedman’s test
• independent group t-test
• Kruskal–Wallis test
• Mann–Whitney test
• one-way ANOVA
• paired t-test
• Pearson’s correlation coeﬃcient
c
⃝2000 by Chapman & Hall/CRC

• simple linear regression
• single sample t-test
(b) Simulation and demonstrations
http://www.ruf.rice.edu/~lane/stat_sim/index.html
From the creator of the HyperStat Online statistics book. More than a
dozen applets; including:
• 2 × 2 contingency tables
• a small eﬀect size can make a
large diﬀerence
• bin widths
• chi–square test of deviations
from expected frequencies
• components of r
• conﬁdence interval for a
proportion
• conﬁdence intervals
• cross validation
• histograms
• mean and median
• normal approximation to
binomial distribution
• regression by eye
• reliability and regression
analysis
• repeated measures
• restriction of range
• sampling distribution
simulation
(c) Virtual ﬂy lab
http://vflylab.calstatela.edu/edesktop/VirtApps/VflyLab/
IntroVflyLab.html
Simulate classic drosophila genetics experiments: Specify characteristics
of a hypothetical male and female ﬂy; simulate the oﬀspring of a mating;
select a pair of oﬀspring to breed; simulate the oﬀspring of their mat-
ing; and compare the observed frequencies with those predicted from
Mendelian genetics.
http://www.users.on.net/zhcchz/java/quincunx/quincunx.1.html
Illustration of the central limit theorem in Java.
(d) http://www.math.csusb.edu/faculty/stanton/m262/
probstat.html
Miscellaneous Java demos.
(e) http://www.ms.uky.edu/~mai/java/AppletIndex.html
Five demonstrations of how samples of increasing size approach a theo-
retical distribution:
• Empirical
• Kaplan–Meier
• Nelson–Aalen
• interval censored
• doubly-censored data
(f) http://www.ms.uky.edu/~mai/java/AppletIndex.html
Three demonstrations of famous random processes:
• 1-dimensional and 2-dimensional Brownian motion
• Buﬀon needle experiment
• Galton’s ball–drop quincunx
(g) http://www.ms.uky.edu/~lancastr/java/javapage.html
Three bootstrap demonstrations all using the Exp(1) distribution:
c
⃝2000 by Chapman & Hall/CRC

• central limit theorem
• parametric bootstrap of sample mean
• nonparametric bootstrap of sample mean
(h) http://www-stat.stanford.edu/~naras/jsm/NormalDensity/
NormalDensity.html
Experiments with the normal distribution.
(i) http://www.stat.uiuc.edu/~stat100/java/guess/
PPApplet.html
Linear regression and correlation demo in Java—
click a bunch of points onto the screen; as each is entered the computer
immediately computes and displays an adjusted regression line (with
equation and correlation coeﬃcient).
(j) http://stat-www.berkeley.edu/users/stark/Java/
Correlation.html
Similar to above, but lets you simulate points
from distributions with known correlation coeﬃcient.
(k) http://www.ruf.rice.edu/~lane/stat_sim/sampling_dist/
sampling_demo.html
Simulation of various sampling distributions in Java.
(l) http://www.stat.uiuc.edu/~stat100/java/box/BoxApplet.html
Simulation in Java of drawing objects (with replacement) from a “box”
whose contents you can deﬁne.
(m) http://www.stat.sc.edu/~west/javahtml/epid.html
Simulate the course of an epidemic in Java.
(n) http://huizen.dds.nl/~berrie/
A collection of QuickTime movies illustrating various statistical con-
cepts.
(o) http://members.aol.com/Trane64/java/CardPowersim.html
Monte–Carlo p versus sample size simulation for survey questionnaire
results, with graphical output (in Java).
17.17.4
Online textbooks, reference manuals, and journals
(a) Glossary of statistical terms used in evidence-based medicine
http://www.musc.edu/muscid/glossary.html
Can be printed out into a handy 12-page mini-handbook of statistical
concepts and terminology.
(b) HyperStat statistics textbook
http://ruf.rice.edu/~lane/hyperstat/contents.html
A well-designed and well-constructed “hyper-text-book.”
(c) Statistics at Square One
http://www.bmj.com/collections/statsbk/index.shtml
An excellent online textbook.
c
⃝2000 by Chapman & Hall/CRC

(d) Electronic Statistics Textbook
http://www.statsoft.com/textbook/stathome.html
(by StatSoft) Very extensive and well-organized (can also be downloaded
for quicker access from your hard drive).
(e) Instat Guide
http://www.graphpad.com/instatman
Instat Guide to Choosing and Interpreting Statistical Tests, from Graph-
pad.
(f) Glossary of over 30 statistical terms
http://www.animatedsoftware.com/statglos/statglos.htm
Explanations vary from short deﬁnitions to extensive explanations, with
graphics and hyperlinks.
(g) Introduction to Probability
http://www.geom.umn.edu/docs/snell/chance/teaching_aids/
probability_book/pdf.html
A complete book by Grinstead and Snell in Adobe Acrobat and Postscript
format. It can be downloaded all at once, or chapter by chapter.
(h) Graphpad web site of statistical resources
http://www.graphpad.com/www/
Short articles, book chapters, bibliographies, and (commercial) software.
Well-written, down-to-earth, and helpful.
(i) InterStat (Statistics on the internet)
http://interstat.stat.vt.edu/intersta.htm/
An online journal where one can publish or read about any aspect of
statistical research or innovative method. Abstracts of articles can be
viewed; from there you can read or download the article (or any com-
ments written about it) in pdf and ps formats.
(j) Journal of Statistics Education
http://www.stat.ncsu.edu/info/jse/
A refereed electronic journal on postsecondary teaching of statistics.
(k) http://www.psychstat.smsu.edu/sbk00.htm
Introductory Statistics: Concepts, Models, and Applications by David
W. Stockburger. In-depth coverage, with extensive use of web technol-
ogy (animated graphics, interactive calculating pages).
(l) http://www.psychstat.smsu.edu/multibook/mlt00.htm
Multivariate Statistics: Concepts, Models, and Applications, by David
W. Stockburger In-depth coverage, with extensive use of web technology
(animated graphics, interactive calculating pages).
(m) http://trochim.human.cornell.edu/kb/kbhome.htm
The Knowledge Base— An Online Research Methods Textbook, by William
M. K. Trochim. An online textbook for an introductory course in re-
search methods.
c
⃝2000 by Chapman & Hall/CRC

(n) http://www.cern.ch/Physics/DataAnalysis/BriefBook/
Data Analysis BriefBook: A condensed handbook, or an extended glos-
sary, written in encyclopedic format, covering subjects in statistics, com-
puting, analysis, and related ﬁelds; meant to be both introduction and
reference for data analysts, scientists and engineers (from CERN).
(o) http://www.cne.gmu.edu/modules/dau/stat/
Statistics refresher tutorial.
(p) http://www.wrightslaw.com/advoc/articles/
tests_measurements.html
Statistics of performance measurement, designed for parents of children
with special educational needs, but also worthwhile for a good introduc-
tory presentation of basic statistical concepts.
(q) http://nimitz.mcs.kent.edu/~blewis/stat/scon.html
Guide to Basic Laboratory Statistics (in Java).
(r) http://duke.usask.ca/~rbaker/stats.html
Basic Principles of Statistical Analysis.
17.17.5
Free statistical software packages
A selection of free software packages.
(a) ViSta
http://forrest.psych.unc.edu/research/ViSta.html
A VIsual STAtistics program for Windows, Mac, and Unix. Features a
structured desktop.
(b) WebStat
http://www.stat.sc.edu/~west/webstat/
Java-based statistical computing environment for the web. Requires a
browser, but can be run oﬄine.
(c) MacANOVA
http://www.stat.umn.edu/~gary/macanova/macanova.home.html
Available for Windows and Mac. This is a complete programming lan-
guage.
(d) Scilab
http://www-rocq.inria.fr/scilab/
Available for Windows, Mac, and Unix. This is a programming language
with MatLab-like syntax, hundreds of built-in functions and libraries,
3D graphics, and symbolic capabilities through a Maple interface.
(e) SISA
http://home.clara.net/sisa/pasprog.htm
Simple Interactive Statistical Analysis for DOS from Daan Uitenbroek.
Collection of modules for many types of hypothesis testing, sample size
calculations, and survey design.
Includes many analyses not usually
found elsewhere.
c
⃝2000 by Chapman & Hall/CRC

(f) Anderson statistical archives
http://odin.mdacc.tmc.edu/anonftp/page_2.html
A large collection of statistical programs for DOS and Mac with For-
tran and C source, from the Biomathematics Department of the M. D.
Anderson Cancer Center.
(g) STPLAN
http://odin.mdacc.tmc.edu/anonftp/page_2.html
Performs power, sample size, and related calculations needed to plan
studies.
Covers a wide variety of situations, including studies whose
outcomes involve the binomial, poisson, normal, and log-normal distri-
butions, or are survival times or correlation coeﬃcients. For DOS and
Mac with Fortran and C source.
(h) EpiInfo
http://www.cdc.gov/epo/epi/epiinfo.htm
A set of programs for word processing, data management, and epidemi-
ologic analysis, designed for public health professionals. Consists of Epi
Info (forms design, data entry, data management), Epi Map (generated
geographical, map-based output), SSS1 (Box–Jenkins time series anal-
ysis, MMWR graphs, trend analysis, and 2-source comparisons).
(i) Rweb
http://www.math.montana.edu/Rweb/
A Web based interface to the ”R” statistical programming language
(similar to S or S-plus).
(j) ARIMA
ftp://ftp.census.gov/pub/ts/x12a/
A seasonal adjustment program for PC and Unix, developed by the
Census Bureau.
(k) G*Power
http://www.psychologie.uni-trier.de:8000/projects/gpower.html
A general Power Analysis program for DOS and Macintosh. Performs
high-precision analysis for t-tests, F-tests, chi–square tests. Computes
power, sample sizes, alpha, beta, and alpha/beta ratios. Contains web-
based tutorial and reference manual.
(l) JDB
http://www.isi.edu/~johnh/SOFTWARE/JDB/index.html
Relational database and elementary statistics for unix. Useful for ma-
nipulating experimental data (joining ﬁles, cleaning data, reformatting
for input into other programs). Computes basic statistics (mean, std.
dev., conﬁdence intervals, quartiles, n-tiles, percentiles, histograms, cor-
relations, z-scores, t-scores).
(m) EasyMA
http://www.spc.univ-lyon1.fr/~mcu/easyma/
c
⃝2000 by Chapman & Hall/CRC

DOS program for the meta-analysis of clinical trials results. Developed
to help physicians and medical researchers to synthesize evidence in
clinical or therapeutic research.
(n) Meta-analysis 5.3
http://www.yorku.ca/faculty/academic/schwarze/meta_e.htm
DOS software for meta-analysis, perhaps the most frequently used meta-
analysis software in the world. Can select the analysis of exact p values
or eﬀect sizes (d or r, with a cluster size option). Plots a stem-and-leaf
display of correlation coeﬃcients. A utility menu allows various trans-
formations and preliminary computations that are typically required
before meta-analysis can be performed.
(o) First Bayes
http://www.maths.nott.ac.uk/personal/aoh/1b.html
A windows application for elementary Bayesian Statistics.
Performs
most standard, elementary Bayesian analyses, including: plotting and
summarizing distributions, deﬁning and examining arbitrary mixtures
of distributions, analysis of two kinds of linear model (one or more nor-
mal samples with common but unknown variance, and simple linear
regression), examination of marginal distributions for arbitrary linear
combinations of the location parameters, and the generation of predic-
tive distributions.
(p) MANET
http://www1.math.uni-augsburg.de/Manet/
Missings Are Now Equally Treated: Mac software for interactive graph-
ics tools for data sets with missing values. Generates missing values
chart, histograms and barcharts, boxplots and dotplots, scatterplots,
mosaic plots, polygon plots, highlighted boxplots, interactive trellis dis-
plays, traces, context-sensitive interrogation, cues, redframing, selection
sequences.
(q) WinSAAM
http://www.winsaam.com/#WinSAAM
Windows version of SAAM (System Analysis and Modeling Software).
Lets you create mathematical models, design and simulate experiments,
and analyze data. Models can contain diﬀerential equations. Graphic
and tabular output is provided.
(r) Boomer
http://www.cpb.uokhsc.edu/common/anonymous/MFB/toc.html
Non-linear regression program for analysis of pharmacokinetic and phar-
macodynamic data.
Includes normal ﬁtting, Bayesian estimation, or
simulation-only, with integrated or diﬀerential equation models. Allows
selection of weighting schemes and methods for numerical integration.
Available for Macintosh and PC. Online manual, tutorial, and sample
data sets.
c
⃝2000 by Chapman & Hall/CRC

(s) Serpik Graph http://www.chat.ru/~mserpik/index.htm
A very sophisticated function graphing program.
17.17.6
Demonstration statistical software packages
The following are a selection of “demonstration versions” or “student versions”
of commercial packages.
They can be freely downloaded, but are usually
restricted or limited in some way.
(a) AssiStat
http://users.aol.com/micrometr/assistat.htm
A Windows package, designed to complement a typical statistical pack-
age. It covers topics not normally in a primary analysis package such
as correction of correlations for restriction in range or less-than-perfect
reliability.
(b) CART
http://www.salford-systems.com/
Salford Systems ﬂagship decision-tree software, combines an easy-to-use
GUI with advanced features for data mining, data pre-processing, and
predictive modeling.
(c) InStat
http://www.graphpad.com/instat3/instatdemo.htm
INstant STATistics: A package from GraphPad Software for Windows
and Mac. Demo version disables printing, saving, and exporting capa-
bilities.
(d) MiniTab
http://www.minitab.com/products/minitab/rel12/index.htm
aA program for Windows. Good coverage of industrial quality control
analyses.
(e) Prism
http://www.graphpad.com/prism/pdemo.htm
A package from from GraphPad Software for Windows and Mac. Per-
forms basic biostatistics, ﬁts curves and creates publication quality sci-
entiﬁc graphs in one complete package.
(f) QuickStat
http://www.camcode.com/arcus.html
A program for Windows. Does not assume statistical knowledge. Gives
advice on experimental design, analysis, and interpretation from the
integrated statistical help system. Results presented in plain language.
For the more experienced user, Arcus is an extremely useful statistical
toolbox which covers biomedical statistical methods.
(g) Rasch
http://mesa.spc.uchicago.edu/
Measurement software deals with the various nuances of constructing
optimal rating scales from a number of (usually) dichotomous measure-
c
⃝2000 by Chapman & Hall/CRC

ments, such as responses to questions in a survey or test.
(h) STATISTICA demo
http://www.statsoft.com/download.html#demo
Has many features of the “Basic Statistics” module of STATISTICA.
(i) Statlets
http://www.statlets.com/
A Java statistics program.
(j) SPSS demos
http://www.spss.com/cool/index.htm
Numerous demo packages from products acquired by SPSS (all for the
PC except one):
• allCLEAR 3.5 and 4.5
• GOLDMineR
• DeltaGraph (Mac)
• LogXact 2.1
• PeakFit 4.06
• QI Analyst 3.5
• Remark Oﬃce OMR 3.0
• SamplePower 1.2
• SigmaGel 1.0
• SPSS Diamond
• SigmaPlot 4.0
• SigmaScan Pro 4.0
• SigmaStat 2.0
• SmartViewer
• StatXact 3.1
• SYSTAT 7.0
• TableCurve 2D 4.07
• TableCurve 3D 3.01
• WesVar Complex
Samples
17.18
TABLES
17.18.1
Random deviates
Consider the two data sequences Scontinuous unif and Sstd normal deﬁned by:
(a) Continuous uniform random numbers on the interval (0, 1) are gen-
erated by a linear congruential generator with parameters: x0 = 1,
a = 99, b = 0, and M = 220; see section 17.11.1.1.
The sequence
begins {1, 99, 9801, 970299, 639185, 364755, . . . }. Dividing this sequence
by M yields numbers on the interval (0, 1). Hence, Scontinuous unif =
{.00000095, .000094, .0093, .9253, .6095, .3478, . . . }
(b) Sstd normal is then obtained by using the Box–Muller method (see sec-
tion 17.11.2). Speciﬁcally, if the values in Scontinuous uniform are writ-
ten as {u1, v1, u2, v2, . . . } then standard normal variables are given by
√−2 ln ui cos 2πvi. Hence, Sstd normal = {5.266, 2.727, −.574, −.763,
−.140, . . . }.
The following random units are obtained from the third digit of the ﬁrst 2,500
values in Scontinuous unif. The standard normal values are the ﬁrst 500 values
in Sstd normal.
c
⃝2000 by Chapman & Hall/CRC

2,500 Random units
Line
Col = (1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
1
00959
77100
36484
06101
34734
18924
28001
87674
16684
07027
2
08149
50395
26971
29957
25235
85468
85629
57477
68311
33626
3
49324
30579
12667
28910
42885
78563
10340
77508
96233
84084
4
03207
57615
90513
02808
68168
12164
81531
90806
01306
94806
5
43855
79617
46906
90009
48557
21458
31805
99078
15841
12303
6
56825
52535
70234
98035
96147
93443
08513
81271
04047
29078
7
91822
62169
10152
29526
48393
07763
12402
43920
06443
44055
8
96171
27730
38041
12364
85443
55515
26148
85117
49046
28379
9
81089
47387
46900
96147
68169
37767
82207
74778
21357
54056
10
00214
98330
43526
10401
39687
16238
43098
63975
59317
90397
11
77471
67351
83417
65376
79343
15268
37451
36632
52965
29032
12
69857
73393
24342
43527
49655
46460
12109
36747
80829
80614
13
91127
97088
07052
71213
52345
20899
50674
29653
04919
99244
14
17793
53476
95434
42814
82423
29613
77770
99328
90929
23462
15
44797
23004
91138
43018
14720
10925
53861
22495
14266
06873
16
80021
02262
28002
01277
17361
81989
28191
10545
83231
47679
17
23856
96770
76740
42257
70503
07524
93800
19562
32113
34636
18
56168
45893
93645
99333
25071
39446
51115
57579
68558
23032
19
72348
46166
92883
40513
01003
86613
28231
06276
40051
70357
20
17468
91307
08718
73452
48267
56730
11798
23490
06958
14224
21
44671
49100
23878
73037
99519
35718
60484
31475
64635
55380
22
86905
30471
97545
47616
05116
03575
52029
32216
41781
57265
23
20038
92416
54157
50748
30886
01292
12454
75935
00909
19524
24
10567
25587
13156
25831
92058
37872
42508
41351
08577
58216
25
15270
23258
16824
62411
69868
29323
31010
81249
49061
48966
26
76791
13967
66020
83633
02426
45880
19989
10388
97013
10831
27
55235
76804
47756
62552
49756
71619
83809
06429
62513
28251
28
57021
01659
32822
72473
47212
10174
98107
61187
58584
86515
29
03229
35982
13383
95360
59148
56016
53365
46634
65098
90531
30
75332
71330
68576
04241
75405
24229
22471
98754
66221
90688
31
38634
66207
19246
60714
07599
82754
80271
84538
74552
84207
32
97519
96693
14402
82045
52534
92484
09281
89645
15765
63189
33
68223
30041
74131
58640
27944
02945
05826
02617
61112
86956
34
61103
01559
62880
84410
72932
44660
38281
62393
36801
73695
35
17932
68475
38125
31372
72691
00476
93257
16191
06430
83422
36
89736
05257
17573
50608
65491
50562
43426
61796
57616
95798
37
57969
44305
67666
37122
42042
64781
47188
44139
16099
13088
38
83902
88463
16427
12809
71391
83774
07016
99953
55540
55620
39
60705
47476
55277
62116
24849
32422
78322
78196
26035
00704
40
01078
04490
01831
63010
66789
79496
89329
23308
23914
60209
41
51923
25930
42663
19843
63039
37580
26048
50867
89446
58697
42
00304
09740
80005
82576
33389
36698
97231
84403
83824
01422
43
56659
01069
57000
39995
11739
63783
76384
46473
73722
43182
44
02302
91182
53872
17030
03905
55025
76676
22339
83255
42673
45
17892
88588
76728
89583
58411
98924
22062
35871
27748
06631
46
00996
60460
33299
81480
14693
89001
78743
23702
98548
29161
47
24141
34012
93370
57766
01847
60396
75707
17819
46328
88234
48
51133
50180
02363
88535
44723
75027
82942
64448
14215
49756
49
05261
99025
29539
35100
86954
23969
84766
55919
91772
54978
50
23374
37461
32685
96100
85808
20952
17928
94719
50170
04314
c
⃝2000 by Chapman & Hall/CRC

500 Random normal numbers with µ = 0, σ = 1
Line Col = (1)
(2)
(3)
(4)
(5)
(6)
(7)
(8)
(9)
(10)
1
5.266
2.727 −0.574 −0.763 −0.140 −0.645 −2.263
1.252 −2.044 −1.055
2
−0.099
0.811
1.207 −0.287
1.809
0.839
3.572
0.078
0.773
0.795
3
−0.436
0.022 −1.077
0.766
0.556 −1.075
0.706 −0.572
0.980 −1.021
4
1.261 −0.685
0.364 −1.045
1.151 −0.747 −0.050 −2.057
0.564 −1.714
5
−1.395 −2.157 −0.494 −1.670
0.032 −0.128
0.561
0.181
0.612 −0.099
6
−0.424
0.894
1.277 −0.047
0.709
1.871 −2.185 −1.179 −0.121 −0.882
7
0.664 −1.257
0.612
0.025 −0.210
2.123 −0.214
2.638
0.754 −1.035
8
−1.042
1.385
0.677
0.307 −1.218
0.135
1.399
0.124
0.722 −0.719
9
0.432 −2.265 −0.226
1.120
0.216
0.620
0.008 −1.319 −0.816
0.897
10
−0.976
0.057 −1.004 −0.585
1.426 −0.330 −0.506
0.934 −0.169 −1.126
11
−1.350
1.221
0.131
0.317 −0.977
0.360 −1.105
0.914
0.240
0.335
12
−0.400 −0.595 −1.737 −0.814 −0.234
1.423
0.693 −0.289
0.107
0.564
13
0.127 −1.512
0.066 −0.005
2.384
1.103 −0.333 −0.288 −0.098
1.524
14
0.388 −0.366 −0.157 −0.200 −0.565
0.372 −0.981
0.175 −0.193
0.612
15
1.729 −0.169
0.907
0.183 −0.270 −0.947
0.215 −1.625
0.176
0.242
16
0.036 −1.458 −0.821
0.945 −0.046
1.573
1.968 −0.464 −0.882
1.741
17
0.548 −0.325 −0.599 −0.795
0.586
1.610
1.032 −0.189 −0.252 −0.699
18
−1.009 −0.102 −0.064 −0.661
0.366
0.212 −0.880
1.825 −1.637
1.538
19
−1.053 −1.050 −0.036 −0.482 −0.175
0.579
1.041
0.316
0.298 −0.838
20
1.076 −0.520
0.626 −0.187 −0.107 −0.267 −0.922
0.009 −0.510
0.352
21
−0.839 −1.043 −0.938
0.528 −1.290
0.692 −1.089 −0.968
0.435 −0.102
22
−1.528
0.303
0.526 −1.717 −0.545 −1.138 −0.428
0.558
0.335
0.254
23
−0.043 −0.391 −1.238 −0.449
1.081 −0.375 −0.829
0.635 −0.098
0.126
24
0.384 −1.481
0.037
0.038 −0.669 −0.698 −0.070
0.061
0.041
0.751
25
−2.603
0.156
0.611 −0.138 −1.676 −0.073
1.491 −1.177
0.620 −0.914
26
−1.072
1.243 −0.714 −1.498
0.232 −1.446 −0.415
0.711
1.109 −1.220
27
−1.650
1.330
0.299
0.408 −2.151
1.257
0.267 −0.678 −0.010 −0.541
28
0.052 −1.202 −1.635
0.495 −0.384
0.417 −0.316
0.493 −0.528
0.479
29
1.458
0.704 −0.689 −0.580
0.034
1.022 −0.161 −0.122
0.532
0.763
30
0.094
1.669
0.377 −1.093
1.349
0.284
0.530 −0.967
1.496 −0.688
31
2.339
1.534 −1.522 −0.078
0.633
0.225 −0.959
0.934 −0.889 −0.602
32
−0.153 −0.535 −0.026 −1.107 −1.187
0.917
0.045 −0.301
0.356
1.042
33
1.131 −0.313
0.444
0.451 −0.051
1.719
0.744 −0.137
0.989 −1.067
34
−0.641 −0.402
0.184 −0.815
0.678
0.045 −1.626 −0.413
0.383 −0.505
35
0.700
0.826 −0.609 −0.971
0.017 −0.359
0.165
2.421
1.404
0.462
36
−1.902 −1.028 −1.636 −0.384 −0.900
0.466
1.761 −0.412 −0.471
0.077
37
2.053 −0.333
0.200
0.138 −0.205 −1.281
0.292 −0.614 −0.412
2.186
38
−0.938 −0.357 −0.764
0.742
0.251 −0.938 −0.803 −0.301 −0.789
0.723
39
0.951
0.229 −0.107 −0.707
0.469
0.069
0.859 −0.674 −0.406 −0.835
40
1.644 −0.247 −1.016 −0.761 −0.881 −1.342 −0.737 −0.534 −1.414
0.008
41
1.601 −1.484 −0.114 −0.064
0.076
0.694
0.788 −1.181 −0.114 −0.098
42
−1.666
0.998 −0.323
0.360
1.075 −0.912
0.227
2.022
0.693
0.584
43
1.307
0.553
1.461
1.297 −0.085 −0.201
0.155 −0.895 −2.334 −1.281
44
0.548 −0.623 −1.971 −0.993 −0.350 −0.195
0.923
0.118
1.602
0.858
45
0.476
1.318
0.146 −1.564 −0.432 −1.733
0.609 −1.462 −0.473
1.087
46
−1.303 −0.567
0.325
0.725 −0.026
0.322 −1.127
0.942 −0.339 −0.336
47
0.150
2.147
0.652
0.910 −0.970 −0.560
0.885 −0.997
0.826 −1.420
48
−2.351 −0.813
1.751 −0.980 −0.291
1.930 −0.320
0.337
0.121
1.108
49
0.309
0.720 −0.304 −0.190 −3.205 −0.408
1.503
0.718
0.194
0.526
50
1.672 −1.693 −1.064 −0.623
0.201
2.392 −0.255 −1.015
1.252
1.796
c
⃝2000 by Chapman & Hall/CRC

17.18.2
Permutations
This table contains the number of permutations of n distinct things taken m
at a time, given by (see section 3.2.3):
P(n, m) =
n!
(n −m)! = n(n −1) · · · (n −m + 1)
(17.147)
Permutations P(n, m)
n
m = 0
1
2
3
4
5
6
7
8
0
1
1
1
1
2
1
2
2
3
1
3
6
6
4
1
4
12
24
24
5
1
5
20
60
120
120
6
1
6
30
120
360
720
720
7
1
7
42
210
840
2520
5040
5040
8
1
8
56
336
1680
6720
20160
40320
40320
9
1
9
72
504
3024
15120
60480
181440
362880
10
1
10
90
720
5040
30240
151200
604800
1814400
11
1
11
110
990
7920
55440
332640
1663200
6652800
12
1
12
132
1320
11880
95040
665280
3991680
19958400
13
1
13
156
1716
17160
154440
1235520
8648640
51891840
14
1
14
182
2184
24024
240240
2162160
17297280
121080960
15
1
15
210
2730
32760
360360
3603600
32432400
259459200
Permutations P(n, m)
n
m = 9
10
11
12
13
9
362880
10
3628800
3628800
11
19958400
39916800
39916800
12
79833600
239500800
479001600
479001600
13
259459200
1037836800
3113510400
6227020800
6227020800
14
726485760
3632428800
14529715200
43589145600
87178291200
15
1816214400
10897286400
54486432000
217945728000
653837184000
17.18.3
Combinations
This table contains the number of combinations of n distinct things taken m
at a time, given by (see section 3.2.5):
C(n, m) =
n
m

=
n!
m!(n −m)!
(17.148)
c
⃝2000 by Chapman & Hall/CRC

Combinations C(n, m)
n
m = 0
1
2
3
4
5
6
7
1
1
1
2
1
2
1
3
1
3
3
1
4
1
4
6
4
1
5
1
5
10
10
5
1
6
1
6
15
20
15
6
1
7
1
7
21
35
35
21
7
1
8
1
8
28
56
70
56
28
8
9
1
9
36
84
126
126
84
36
10
1
10
45
120
210
252
210
120
11
1
11
55
165
330
462
462
330
12
1
12
66
220
495
792
924
792
13
1
13
78
286
715
1287
1716
1716
14
1
14
91
364
1001
2002
3003
3432
15
1
15
105
455
1365
3003
5005
6435
16
1
16
120
560
1820
4368
8008
11440
17
1
17
136
680
2380
6188
12376
19448
18
1
18
153
816
3060
8568
18564
31824
19
1
19
171
969
3876
11628
27132
50388
20
1
20
190
1140
4845
15504
38760
77520
21
1
21
210
1330
5985
20349
54264
116280
22
1
22
231
1540
7315
26334
74613
170544
23
1
23
253
1771
8855
33649
100947
245157
24
1
24
276
2024
10626
42504
134596
346104
25
1
25
300
2300
12650
53130
177100
480700
26
1
26
325
2600
14950
65780
230230
657800
27
1
27
351
2925
17550
80730
296010
888030
28
1
28
378
3276
20475
98280
376740
1184040
29
1
29
406
3654
23751
118755
475020
1560780
30
1
30
435
4060
27405
142506
593775
2035800
31
1
31
465
4495
31465
169911
736281
2629575
32
1
32
496
4960
35960
201376
906192
3365856
33
1
33
528
5456
40920
237336
1107568
4272048
34
1
34
561
5984
46376
278256
1344904
5379616
35
1
35
595
6545
52360
324632
1623160
6724520
36
1
36
630
7140
58905
376992
1947792
8347680
37
1
37
666
7770
66045
435897
2324784
10295472
38
1
38
703
8436
73815
501942
2760681
12620256
39
1
39
741
9139
82251
575757
3262623
15380937
40
1
40
780
9880
91390
658008
3838380
18643560
41
1
41
820
10660
101270
749398
4496388
22481940
42
1
42
861
11480
111930
850668
5245786
26978328
43
1
43
903
12341
123410
962598
6096454
32224114
44
1
44
946
13244
135751
1086008
7059052
38320568
45
1
45
990
14190
148995
1221759
8145060
45379620
46
1
46
1035
15180
163185
1370754
9366819
53524680
47
1
47
1081
16215
178365
1533939
10737573
62891499
48
1
48
1128
17296
194580
1712304
12271512
73629072
49
1
49
1176
18424
211876
1906884
13983816
85900584
50
1
50
1225
19600
230300
2118760
15890700
99884400
c
⃝2000 by Chapman & Hall/CRC

Combinations C(n, m)
n
m = 8
9
10
11
12
8
1
9
9
1
10
45
10
1
11
165
55
11
1
12
495
220
66
12
1
13
1287
715
286
78
13
14
3003
2002
1001
364
91
15
6435
5005
3003
1365
455
16
12870
11440
8008
4368
1820
17
24310
24310
19448
12376
6188
18
43758
48620
43758
31824
18564
19
75582
92378
92378
75582
50388
20
125970
167960
184756
167960
125970
21
203490
293930
352716
352716
293930
22
319770
497420
646646
705432
646646
23
490314
817190
1144066
1352078
1352078
24
735471
1307504
1961256
2496144
2704156
25
1081575
2042975
3268760
4457400
5200300
26
1562275
3124550
5311735
7726160
9657700
27
2220075
4686825
8436285
13037895
17383860
28
3108105
6906900
13123110
21474180
30421755
29
4292145
10015005
20030010
34597290
51895935
30
5852925
14307150
30045015
54627300
86493225
31
7888725
20160075
44352165
84672315
141120525
32
10518300
28048800
64512240
129024480
225792840
33
13884156
38567100
92561040
193536720
354817320
34
18156204
52451256
131128140
286097760
548354040
35
23535820
70607460
183579396
417225900
834451800
36
30260340
94143280
254186856
600805296
1251677700
37
38608020
124403620
348330136
854992152
1852482996
38
48903492
163011640
472733756
1203322288
2707475148
39
61523748
211915132
635745396
1676056044
3910797436
40
76904685
273438880
847660528
2311801440
5586853480
41
95548245
350343565
1121099408
3159461968
7898654920
42
118030185
445891810
1471442973
4280561376
11058116888
43
145008513
563921995
1917334783
5752004349
15338678264
44
177232627
708930508
2481256778
7669339132
21090682613
45
215553195
886163135
3190187286
10150595910
28760021745
46
260932815
1101716330
4076350421
13340783196
38910617655
47
314457495
1362649145
5178066751
17417133617
52251400851
48
377348994
1677106640
6540715896
22595200368
69668534468
49
450978066
2054455634
8217822536
29135916264
92263734836
50
536878650
2505433700
10272278170
37353738800
121399651100
c
⃝2000 by Chapman & Hall/CRC

Combinations C(n, m)
n
m = 13
14
15
16
17
13
1
14
14
1
15
105
15
1
16
560
120
16
1
17
2380
680
136
17
1
18
8568
3060
816
153
18
19
27132
11628
3876
969
171
20
77520
38760
15504
4845
1140
21
203490
116280
54264
20349
5985
22
497420
319770
170544
74613
26334
23
1144066
817190
490314
245157
100947
24
2496144
1961256
1307504
735471
346104
25
5200300
4457400
3268760
2042975
1081575
26
10400600
9657700
7726160
5311735
3124550
27
20058300
20058300
17383860
13037895
8436285
28
37442160
40116600
37442160
30421755
21474180
29
67863915
77558760
77558760
67863915
51895935
30
119759850
145422675
155117520
145422675
119759850
31
206253075
265182525
300540195
300540195
265182525
32
347373600
471435600
565722720
601080390
565722720
33
573166440
818809200
1037158320
1166803110
1166803110
34
927983760
1391975640
1855967520
2203961430
2333606220
35
1476337800
2319959400
3247943160
4059928950
4537567650
36
2310789600
3796297200
5567902560
7307872110
8597496600
37
3562467300
6107086800
9364199760
12875774670
15905368710
38
5414950296
9669554100
15471286560
22239974430
28781143380
39
8122425444
15084504396
25140840660
37711260990
51021117810
40
12033222880
23206929840
40225345056
62852101650
88732378800
41
17620076360
35240152720
63432274896
103077446706
151584480450
42
25518731280
52860229080
98672427616
166509721602
254661927156
43
36576848168
78378960360
151532656696
265182149218
421171648758
44
51915526432
114955808528
229911617056
416714805914
686353797976
45
73006209045
166871334960
344867425584
646626422970
1103068603890
46
101766230790
239877544005
511738760544
991493848554
1749695026860
47
140676848445
341643774795
751616304549
1503232609098
2741188875414
48
192928249296
482320623240
1093260079344
2254848913647
4244421484512
49
262596783764
675248872536
1575580702584
3348108992991
6499270398159
50
354860518600
937845656300
2250829575120
4923689695575
9847379391150
c
⃝2000 by Chapman & Hall/CRC

CHAPTER 18
Special Functions
Contents
18.1
Bessel functions
18.1.1
Diﬀerential equation
18.1.2
Series expansions
18.1.3
Recurrence relations
18.1.4
Behavior as z →0
18.1.5
Integral representations
18.1.6
Fourier expansion
18.1.7
Asymptotic expansions
18.1.8
Half-order Bessel functions
18.1.9
Modiﬁed Bessel functions
18.2
Beta function
18.2.1
Other integrals
18.2.2
Properties
18.3
Ceiling and ﬂoor functions
18.4
Delta function
18.5
Error functions
18.5.1
Expansions
18.5.2
Special values
18.6
Exponential function
18.6.1
Exponentiation
18.6.2
Deﬁnition of ez
18.6.3
Derivative and integral of ez
18.6.4
Circular functions and exponentials
18.6.5
Hyperbolic functions
18.7
Factorials and Pochhammer’s symbol
18.8
Gamma function
18.8.1
Other integrals for the gamma function
18.8.2
Properties
18.8.3
Expansions
18.8.4
Special values
18.8.5
Digamma function
c
⃝2000 by Chapman & Hall/CRC

18.8.6
Incomplete gamma functions
18.9
Hypergeometric functions
18.9.1
Generalized hypergeometric function
18.9.2
Gauss hypergeometric function
18.9.3
Conﬂuent hypergeometric functions
18.10
Logarithmic functions
18.10.1 Deﬁnition of the natural log
18.10.2 Special values
18.10.3 Logarithms to a base other than e
18.10.4 Relation of the logarithm to the exponential
18.10.5 Identities
18.10.6 Series expansions for the natural logarithm
18.10.7 Derivative and integration formulae
18.11
Partitions
18.12
Signum function
18.13
Stirling numbers
18.13.1 Stirling numbers
18.13.2 Stirling cycle numbers
18.14
Sums of powers of integers
18.15
Tables of orthogonal polynomials
18.16
References
The following notation is used throughout this chapter:
(a) N denotes the natural numbers.
(b) R denotes the real numbers.
(c) Re(z) denotes the real part of the complex quantity z.
18.1
BESSEL FUNCTIONS
18.1.1
Diﬀerential equation
The Bessel diﬀerential equation is
z2y′′ + zy′ + (z2 −ν2)y = 0
(18.1)
It has a regular singularity at z = 0 and an irregular singularity at z = ∞.
The solutions are denoted by Jν(z) and Yν(z); these are the ordinary Bessel
functions. Other solutions are J−ν(z) and Y−ν(z). If ν is an integer
J−n(z) = (−1)nJn(z),
n = 0, 1, 2, . . .
(18.2)
Complete solutions to Bessel’s equation may be written as
c1Jν(z) + c2J−ν(z),
if ν is not an integer,
c1Jν(z) + c2Yν(z),
for any value of ν.
(18.3)
c
⃝2000 by Chapman & Hall/CRC

Figure 18.1: Bessel functions J0(x), J1(x), Y0(x), Y1(x), for 0 ≤x ≤12.
18.1.2
Series expansions
For any complex number z
Jν(z) =
1
2 z
ν
∞

n=0
(−1)n ( 1
2 z)2n
Γ(n + ν + 1) n!
J0(z) = 1 −
1
2 z
2
+
1
2! 2!
1
2 z
4
−
1
3! 3!
1
2 z
6
+ . . .
J1(z) = 1
2 z
%
1 −
1
1! 2!
1
2 z
2
+
1
2! 3!
1
2 z
4
−
1
3! 4!
1
2 z
6
+ . . .
&
(18.4)
18.1.3
Recurrence relations
The term cylinder function refers to any of the functions Jν(z), Yν(z), H(1)
ν (z),
or H(2)
ν (z). If Cν(z) denotes any of the cylinder functions, then
Cν−1(z) + Cν+1(z) = 2ν
z Cν(z),
Cν−1(z) −Cν+1(z) = 2C′
ν(z),
C′
ν(z) = Cν−1(z) −ν
z Cν(z),
C′
ν(z) = −Cν+1(z) + ν
z Cν(z).
(18.5)
c
⃝2000 by Chapman & Hall/CRC

x
J0(x)
J1(x)
Y0(x)
Y1(x)
0.0
1.00000000
0.00000000
−∞
−∞
0.2
0.99002497
0.09950083
−1.08110532
−3.32382499
0.4
0.96039823
0.19602658
−0.60602457
−1.78087204
0.6
0.91200486
0.28670099
−0.30850987
−1.26039135
0.8
0.84628735
0.36884205
−0.08680228
−0.97814418
1.0
0.76519769
0.44005059
0.08825696
−0.78121282
1.2
0.67113274
0.49828906
0.22808350
−0.62113638
1.4
0.56685512
0.54194771
0.33789513
−0.47914697
1.6
0.45540217
0.56989594
0.42042690
−0.34757801
1.8
0.33998641
0.58151695
0.47743171
−0.22366487
2.0
0.22389078
0.57672481
0.51037567
−0.10703243
2.2
0.11036227
0.55596305
0.52078429
0.00148779
2.4
0.00250768
0.52018527
0.51041475
0.10048894
2.6
−0.09680495
0.47081827
0.48133059
0.18836354
2.8
−0.18503603
0.40970925
0.43591599
0.26354539
3.0
−0.26005195
0.33905896
0.37685001
0.32467442
3.2
−0.32018817
0.26134325
0.30705325
0.37071134
3.4
−0.36429560
0.17922585
0.22961534
0.40101529
3.6
−0.39176898
0.09546555
0.14771001
0.41539176
3.8
−0.40255641
0.01282100
0.06450325
0.41411469
4.0
−0.39714981
−0.06604333
−0.01694074
0.39792571
4.2
−0.37655705
−0.13864694
−0.09375120
0.36801281
4.4
−0.34225679
−0.20277552
−0.16333646
0.32597067
4.6
−0.29613782
−0.25655284
−0.22345995
0.27374524
4.8
−0.24042533
−0.29849986
−0.27230379
0.21356517
5.0
−0.17759677
−0.32757914
−0.30851763
0.14786314
Table 18.1: Values of the Bessel functions J0, J1, Y0, and Y1.
18.1.4
Behavior as z →0
If Re(ν) > 0, then
Jν(z) ∼
 1
2z
ν
Γ(ν + 1),
Yν(z) ∼−1
π Γ(ν)
2
z
ν
(18.6)
The same relations hold as Re(ν) →∞, with z ﬁxed.
18.1.5
Integral representations
Let Re(z) > 0 and ν be any complex number
Jν(z) = 1
π
 π
0
cos(νθ −z sin θ) dθ −sin νπ
π
 ∞
0
e−νt −z sinh t dt,
Yν(z) = 1
π
 π
0
sin(z sin θ −νθ) dθ −
 ∞
0

eνt + e−νt cos νπ

e−z sinh t dt.
(18.7)
When ν = n (integer) the second integral in the ﬁrst relation is zero.
c
⃝2000 by Chapman & Hall/CRC

18.1.6
Fourier expansion
For any complex z
e−iz sin t =
∞

n=−∞
e−intJn(z)
(18.8)
with Parseval relation
∞

n=−∞
J2
n(z) = 1.
(18.9)
18.1.7
Asymptotic expansions
For large positive values of x:
Jν(x) =
-
2
πx

cos

x −1
2 νπ −1
4 π

+ O(x−1)

,
Yν(x) =
-
2
πx

sin

x −1
2 νπ −1
4 π

+ O(x−1)

.
(18.10)
18.1.8
Half-order Bessel functions
For integer values of n let
jn(z) =

π/(2z) Jn+ 1
2 (z),
yn(z) =

π/(2z) Yn+ 1
2 (z).
(18.11)
Then
j0(z) = y−1(z) = sin z
z
,
y0(z) = −j−1(z) = −cos z
z
,
(18.12)
and for n = 0, 1, 2, . . .
jn(z) = (−z)n
1
z
d
dz
n sin z
z
,
yn(z) = −(−z)n
1
z
d
dz
n cos z
z
. (18.13)
The functions jn(z) and yn(z) both satisfy
z[fn−1(z) + fn+1(z)] = (2n + 1)fn(z)
nfn−1(z) −(n + 1)fn+1(z) = (2n + 1)f ′
n(z)
(18.14)
18.1.9
Modiﬁed Bessel functions
The diﬀerential equation for the modiﬁed Bessel functions is
z2y′′ + zy′ −(z2 + ν2)y = 0
(18.15)
This diﬀerential equation has solutions Iν(z) and Kν(z) deﬁned by
Iν(z) =
z
2
ν
∞

n=0
(z/2)2n
Γ(n + ν + 1) n!,
Kν(z) = π
2
I−ν(z) −Iν(z)
sin νπ
,
(18.16)
c
⃝2000 by Chapman & Hall/CRC

x
e−xI0(x)
e−xI1(x)
exK0(x)
exK1(x)
0.0
1.00000000
0.00000000
∞
∞
0.2
0.82693855
0.08228312
2.14075732
5.83338603
0.4
0.69740217
0.13676322
1.66268209
3.25867388
0.6
0.59932720
0.17216442
1.41673762
2.37392004
0.8
0.52414894
0.19449869
1.25820312
1.91793030
1.0
0.46575961
0.20791042
1.14446308
1.63615349
1.2
0.41978208
0.21525686
1.05748453
1.44289755
1.4
0.38306252
0.21850759
0.98807000
1.30105374
1.6
0.35331500
0.21901949
0.93094598
1.19186757
1.8
0.32887195
0.21772628
0.88283353
1.10480537
2.0
0.30850832
0.21526929
0.84156822
1.03347685
2.2
0.29131733
0.21208773
0.80565398
0.97377017
2.4
0.27662232
0.20848109
0.77401814
0.92291367
2.6
0.26391400
0.20465225
0.74586824
0.87896728
2.8
0.25280553
0.20073741
0.72060413
0.84053006
3.0
0.24300035
0.19682671
0.69776160
0.80656348
3.2
0.23426883
0.19297862
0.67697511
0.77628028
3.4
0.22643140
0.18922985
0.65795227
0.74907206
3.6
0.21934622
0.18560225
0.64045596
0.72446066
3.8
0.21290013
0.18210758
0.62429158
0.70206469
4.0
0.20700192
0.17875084
0.60929767
0.68157595
4.2
0.20157738
0.17553253
0.59533899
0.66274241
4.4
0.19656556
0.17245023
0.58230127
0.64535587
4.6
0.19191592
0.16949973
0.57008720
0.62924264
4.8
0.18758620
0.16667571
0.55861332
0.61425660
5.0
0.18354081
0.16397227
0.54780756
0.60027386
Table 18.2: Values of the Bessel functions I0, I1, K0, and K1.
where the right-hand side should be determined by a limiting process when ν
assumes integer values. When n = 0, 1, 2, . . .
Kn(z) =(−1)n+1In(z) ln z
2 + 1
2
2
z
n n−1

k=0
(n −k −1)!
k!

−z2
4
k
+ (−1)n
2
z
2
n ∞

k=0
[ψ(k + 1) + ψ(n + k + 1)]
(z/2)2k
k! (n + k)!.
(18.17)
18.1.9.1
Relation to ordinary Bessel functions
Iν(z) =





e−1
2 νπiJν

ze
1
2 πi
,
for −π < arg z ≤1
2 π
e
3
2 νπiJν

ze−3
2 πi
,
for
1
2 π < arg z ≤π
(18.18)
For n = 0, 1, 2, . . .
In(z) = i−nJn(iz),
I−n(z) = In(z)
(18.19)
c
⃝2000 by Chapman & Hall/CRC

For any ν
K−ν(z) = Kν(z)
(18.20)
18.1.9.2
Recursion relations
Iν−1(z) −Iν+1(z) = 2ν
z Iν(z),
Kν+1(z) −Kν−1(z) = 2ν
z Kν(z)
Iν−1(z) + Iν+1(z) = 2I′
ν(z),
Kν−1(z) + Kν+1(z) = −2K′
ν(z)
(18.21)
18.1.9.3
Integrals
Iν(z) = 1
π
 π
0
ez cos θcos(νθ) dθ −sin νπ
π
 ∞
0
e−νt −z cosh t dt
Kν(z) =
 ∞
0
e−z cosh t cosh(νt) dt
(18.22)
When ν = n (integer) the second integral in the ﬁrst relation disappears.
18.2
BETA FUNCTION
The beta function is deﬁned by
B(p, q) =
 1
0
tp−1(1 −t)q−1 dt,
Re(p) > 0,
Re(q) > 0
(18.23)
The beta function is related to the gamma function as follows:
B(p, q) = Γ(p) Γ(q)
Γ(p + q)
(18.24)
Values of the beta function are in Table 18.3.
18.2.1
Other integrals
Other integral representations for the beta function include (in all cases
Re(p) > 0, Re(q) > 0):
(a) B(p, q) = 2
J π/2
0
sin2p−1 θ cos2q−1 θ dθ
(b) B(p, q) =
J ∞
0
tp−1
(t+1)p+q dt
(c) B(p, q) =
J ∞
0
e−pt (1 −e−t)q−1 dt
(d) B(p, q) = rq(r + 1)p J 1
0
tp−1(1−t)q−1
(r+t)p+q
dt,
r > 0
18.2.2
Properties
The beta function has the properties:
(a) B(p, q) = B(q, p)
(b) B(p, q + 1) = q
p B(p + 1, q) =
q
p+q B(p, q)
c
⃝2000 by Chapman & Hall/CRC

p q = 0.100
0.200
0.300
0.400
0.500
0.600
0.700
0.800
0.900
1.000
0.1
19.715 14.599 12.831 11.906 11.323 10.914 10.607 10.365 10.166 10.000
0.2
14.599
9.502
7.748
6.838
6.269
5.872
5.576
5.345
5.157
5.000
0.3
12.831
7.748
6.010
5.112
4.554
4.169
3.883
3.661
3.482
3.333
0.4
11.906
6.838
5.112
4.226
3.679
3.303
3.027
2.813
2.641
2.500
0.5
11.323
6.269
4.554
3.679
3.142
2.775
2.506
2.299
2.135
2.000
0.6
10.914
5.872
4.169
3.303
2.775
2.415
2.154
1.954
1.796
1.667
0.7
10.607
5.576
3.883
3.027
2.506
2.154
1.899
1.705
1.552
1.429
0.8
10.365
5.345
3.661
2.813
2.299
1.954
1.705
1.517
1.369
1.250
0.9
10.166
5.157
3.482
2.641
2.135
1.796
1.552
1.369
1.226
1.111
1.0
10.000
5.000
3.333
2.500
2.000
1.667
1.429
1.250
1.111
1.000
1.2
9.733
4.751
3.099
2.279
1.791
1.468
1.239
1.069
0.938
0.833
1.4
9.525
4.559
2.921
2.113
1.635
1.321
1.101
0.938
0.813
0.714
1.6
9.355
4.404
2.779
1.982
1.513
1.208
0.994
0.837
0.718
0.625
1.8
9.213
4.276
2.663
1.875
1.415
1.117
0.909
0.758
0.644
0.556
2.0
9.091
4.167
2.564
1.786
1.333
1.042
0.840
0.694
0.585
0.500
2.2
8.984
4.072
2.480
1.710
1.264
0.979
0.783
0.641
0.536
0.455
2.4
8.890
3.989
2.406
1.644
1.205
0.925
0.734
0.597
0.495
0.417
2.6
8.805
3.915
2.340
1.586
1.153
0.878
0.692
0.558
0.460
0.385
2.8
8.728
3.848
2.282
1.534
1.107
0.837
0.655
0.525
0.430
0.357
3.0
8.658
3.788
2.230
1.488
1.067
0.801
0.622
0.496
0.403
0.333
Table 18.3: Values of the beta function B(p, q).
(c) B(p, q) B(p + q, r) = Γ(p) Γ(q) Γ(r)
Γ(p+q+r)
18.3
CEILING AND FLOOR FUNCTIONS
The ceiling function of x, denoted ⌈x⌉, is the least integer that is not smaller
than x. For example, ⌈π⌉= 4, ⌈5⌉= 5, and ⌈−1.5⌉= −1.
The ﬂoor function of x, denoted ⌊x⌋, is the largest integer that is not larger
than x. For example, ⌊π⌋= 3, ⌊5⌋= 5, and ⌊−1.5⌋= −2.
18.4
DELTA FUNCTION
The delta function of x, denoted δ(x), is deﬁned by δ(x) = 0 when x ̸= 0, and
J ∞
−∞δ(x) dx = 1.
18.5
ERROR FUNCTIONS
The error function, erf(x), and the complementary error function, erfc(x), are
deﬁned by:
erfx =
2
√π
 x
0
e−t2 dt
erfcx =
2
√π
 ∞
x
e−t2 dt
(18.25)
These functions have the properties:
(a) erf x + erfc x = 1,
c
⃝2000 by Chapman & Hall/CRC

(b) erf(−x) = −erf x,
(c) erfc(−x) = 2 −erfc x
The error function is related to the normal probability distribution function
as follows:
Φ(x) = 1
2

1 + erf
 x
√
2

(18.26)
18.5.1
Expansions
The error function has the series expansion
erf(x) =
2
√π
∞

n=0
(−1)n x2n+1
(2n + 1) n!
=
2
√π

x −x3
3 + 1
2!
x5
5 −1
3!
x7
7 + . . .

=
2
√π
∞

n=0
Γ( 3
2) e−x2
Γ(n + 3
2) x2n+1 =
2
√π e−x2 
x + 2
3 x3 + 4
15x5 . . .

(18.27)
and the asymptotic expansion (for z →∞, with |arg z| < 3
4π):
erfc(z) ∼
2
√π
e−z2
2z
∞

n=0
(−1)n (2n)!
n!(2z)2n
∼
2
√π
e−z2
2z

1 −1
z2 + 6
z4 −15
8 z6 + . . .

(18.28)
18.5.2
Special values
Special values of the error function include:
(a) erf(±∞) = ±1
(b) erfc(−∞) = 2
(c) erfc(∞) = 0
(d) erf(0) = 0
(e) erfc(0) = 1
Note that erf(x0) = erfc(x0) = 1
2 for x0 = 0.476936 . . .
18.6
EXPONENTIAL FUNCTION
18.6.1
Exponentiation
For a any real number and m a positive integer, the exponential am is deﬁned
as
am = a · a · a · · · a


	
m terms
(18.29)
The three laws of exponents are:
c
⃝2000 by Chapman & Hall/CRC

1. an · am = am+n
2. am
an =







am−n
if m > n,
1
if m = n,
1
an−m
if m < n.
3. (am)n = a(mn)
The n-th root function is deﬁned as the inverse of the n-th power function.
That is
if bn = a, then b =
n√a = a(1/n).
(18.30)
If n is odd, there will be a unique real number satisfying the above deﬁnition
of
n√a, for any real value of a. If n is even, for positive values of a there will
be two real values for
n√a, one positive and one negative. By convention, the
symbol
n√a is understood to mean the positive value. If n is even and a is
negative, then there are no real values for
n√a.
To extend the deﬁnition to include at (for t not necessarily an integer), in
such a way as to maintain the laws of exponents, the following deﬁnitions are
required (where we now restrict a to be positive, p to be an odd number, and
q to be an even number):
a0 = 1
ap/q =
q√
ap
a−t = 1
at
(18.31)
With these restrictions, the second law of exponents can be written as: am
an =
am−n.
If a > 1 then the function ax is monotone increasing while, if 0 < a < 1 then
the function ax is monotone decreasing.
18.6.2
Deﬁnition of ez
exp(z) = ez = lim
m→∞

1 + z
m
m
= 1 + z + z2
2! + z3
3! + z4
4! + . . .
(18.32)
18.6.3
Derivative and integral of ez
The derivative of ez is dex
dx = ez. The integral of ez is

ex dx = ez + C.
18.6.4
Circular functions and exponentials
cos z = eiz + e−iz
2
,
eiz = cos z + i sin z
(18.33)
sin z = eiz −e−iz
2i
e−iz = cos z −i sin z
(18.34)
c
⃝2000 by Chapman & Hall/CRC

Figure 18.2: Graphs of ex and e−x
If z = x + iy, then
ez = exeiy = ex(cos y + i sin y)
(18.35)
18.6.5
Hyperbolic functions
cosh z = ez + e−z
2
,
ez = cosh z + sinh z
(18.36)
sinh z = ez −e−z
2
e−z = cosh z −sinh z
(18.37)
18.7
FACTORIALS AND POCHHAMMER’S SYMBOL
The factorial of n, denoted n!, is the product of all integers less than or equal
to n: n! = n · (n −1) · (n −2) · · · 2 · 1. The double factorial of n, denoted n!!,
is the product of every other integer: n!! = n · (n −2) · (n −4) · · · , where the
last element in the product is either 2 or 1, depending on whether n is even or
odd. The generalization of the factorial function is the gamma function (see
section 18.8). When n is an integer, Γ(n) = (n −1)!. A table of values is in
Table 18.4.
The shifted factorial (also called the rising factorial or Pochhammer’s symbol)
is denoted by (n)k (sometimes nk), and is deﬁned as
(n)k = n · (n + 1) · (n + 2) · · ·


	
k terms
= (n + k −1)!
(n −1)!
= Γ(n + k)
Γ(n)
(18.38)
18.8
GAMMA FUNCTION
The gamma function is deﬁned by
Γ(z) =
 ∞
0
tz−1 e−t dt,
z = x + iy,
x > 0
(18.39)
c
⃝2000 by Chapman & Hall/CRC

n
n!
log10 n!
n!!
log10 n!!
0
1
0.00000
1
0.00000
1
1
0.00000
1
0.00000
2
2
0.30103
2
0.30103
3
6
0.77815
3
0.47712
4
24
1.38021
8
0.90309
5
120
2.07918
15
1.17609
6
720
2.85733
48
1.68124
7
5040
3.70243
105
2.02119
8
40320
4.60552
384
2.58433
9
3.6288 × 105
5.55976
945
2.97543
10
3.6288 × 106
6.55976
3840
3.58433
11
3.9917 × 107
7.60116
10395
4.01682
12
4.7900 × 108
8.68034
46080
4.66351
13
6.2270 × 109
9.79428
1.3514 × 105
5.13077
14
8.7178 × 1010
10.94041
6.4512 × 105
5.80964
15
1.3077 × 1012
12.11650
2.0270 × 106
6.30686
16
2.0923 × 1013
13.32062
1.0322 × 107
7.01376
17
3.5569 × 1014
14.55107
3.4459 × 107
7.53731
18
6.4024 × 1015
15.80634
1.8579 × 108
8.26903
19
1.2165 × 1017
17.08509
6.5473 × 108
8.81606
20
2.4329 × 1018
18.38612
3.7159 × 109
9.57006
25
1.5511 × 1025
25.19065
7.9059 × 1012
12.89795
50
3.0414 × 1064
64.48307
5.2047 × 1032
32.71640
100
9.3326 × 10157
157.97000
3.4243 × 1079
79.53457
150
5.7134 × 10262
262.75689
9.3726 × 10131
131.97186
500
1.2201 × 101134
1134.0864
5.8490 × 10567
567.76709
1000
4.0239 × 102567
2567.6046
3.9940 × 101284
1284.6014
Table 18.4: Values of the factorial function (n!) and the double factorial func-
tion (n!!).
The gamma function can also be deﬁned by products:
(a) Γ(z) = lim
n→∞
n! nz
z(z + 1) · · · (z + n)
(b)
1
Γ(z) = z eγz
∞
)
n=1
"
1 + z
n

e−z/n#
, where γ is Euler’s constant
18.8.1
Other integrals for the gamma function
Other integral representations for the gamma function include:
(a) Γ(z) cos
1
2πz

=
 ∞
0
tz−1 cos t dt for 0 < Re(z) < 1
(b) Γ(z) sin
1
2πz

=
 ∞
0
tz−1 sin t dt for −1 < Re(z) < 1
c
⃝2000 by Chapman & Hall/CRC

Figure 18.3: Graphs of Γ(x) and 1/Γ(x), for x real.
18.8.2
Properties
Γ′(1) =
 ∞
0
ln t e−t dt = −γ
(18.40)
Multiplication formula:
Γ(2z) = π−1
2 22z−1 Γ(z) Γ

z + 1
2

(18.41)
Reﬂection formulas:
Γ(z) Γ(1 −z) =
π
sin πz ,
Γ
1
2 + z

Γ
1
2 −z

=
π
cos πz ,
Γ(z −n) = (−1)nΓ(z)
Γ(1 −z)
Γ(n + 1 −z) =
(−1)n π
sin πz Γ(n + 1 −z)
(18.42)
The gamma function has the recursion formula:
Γ(z + 1) = z Γ(z)
(18.43)
The relation Γ(z) = Γ(z + 1)/z can be used to deﬁne the gamma function in
the left half plane, z ̸= 0, −1, −2, . . .
The gamma function has simple poles at z = −n, (for n = 0, 1, 2, . . . ), with
the respective residues (−1)n/n!. That is,
lim
z→−n(z + n)Γ(z) = (−1)n
n!
(18.44)
c
⃝2000 by Chapman & Hall/CRC

18.8.3
Expansions
The gamma function has the series expansion, for z →∞, |arg z| < π:
Γ(z) ∼
-
2π
z zz e−z

1 +
1
12 z +
1
288 z2 −
139
51 840 z3 + . . .

ln Γ(z) ∼ln
-
2π
z zze−z

+
∞

n=1
B2n
2n (2n −1)
1
z2n−1 ,
∼ln
-
2π
z zze−z

+
1
12z −
1
360z3 +
1
1 260z5 −
1
1 680z7 + . . . ,
(18.45)
where Bn are the Bernoulli numbers.
If z = n, a large positive integer, then
a useful approximation for n! is given by Stirling’s formula:
n! ∼
√
2πn nn e−n,
n →∞
(18.46)
18.8.4
Special values
Γ(n + 1) = n!
if n = 0, 1, 2, . . . , where 0! = 1,
Γ(1) = 1,
Γ(2) = 1,
Γ(3) = 2,
Γ
1
2

= √π,
Γ

m + 1
2

= 1 · 3 · 5 · · · (2m −1)
2m
√π,
m = 1, 2, 3, . . . ,
Γ

−m + 1
2

=
(−1)m2m
1 · 3 · 5 · · · (2m −1)
√π,
m = 1, 2, 3, . . .
Γ( 1
4) = 3.62560 99082
Γ( 1
3) = 2.67893 85347
Γ( 1
2) = √π = 1.77245 38509
Γ( 2
3) = 1.35411 79394
Γ( 3
4) = 1.22541 67024
Γ( 3
2) = √π/2 = 0.88622 69254
See table 18.5.
18.8.5
Digamma function
The digamma function ϕ(x) is deﬁned by
ϕ(x) = d ln Γ(z)
dz
= Γ′(z)
Γ(z)
(18.47)
c
⃝2000 by Chapman & Hall/CRC

n
Γ(n)
n
Γ(n)
n
Γ(n)
n
Γ(n)
1.00
1.0000
1.25
.9064
1.50
.8862
1.75
.9191
1.01
.9943
1.26
.9044
1.51
.8866
1.76
.9214
1.02
.9888
1.27
.9025
1.52
.8870
1.77
.9238
1.03
.9835
1.28
.9007
1.53
.8876
1.78
.9262
1.04
.9784
1.29
.8990
1.54
.8882
1.79
.9288
1.05
.9735
1.30
.8975
1.55
.8889
1.80
.9314
1.06
.9687
1.31
.8960
1.56
.8896
1.81
.9341
1.07
.9642
1.32
.8946
1.57
.8905
1.82
.9368
1.08
.9597
1.33
.8934
1.58
.8914
1.83
.9397
1.09
.9555
1.34
.8922
1.59
.8924
1.84
.9426
1.10
.9514
1.35
.8912
1.60
.8935
1.85
.9456
1.11
.9474
1.36
.8902
1.61
.8947
1.86
.9487
1.12
.9436
1.37
.8893
1.62
.8959
1.87
.9518
1.13
.9399
1.38
.8885
1.63
.8972
1.88
.9551
1.14
.9364
1.39
.8879
1.64
.8986
1.89
.9584
1.15
.9330
1.40
.8873
1.65
.9001
1.90
.9618
1.16
.9298
1.41
.8868
1.66
.9017
1.91
.9652
1.17
.9267
1.42
.8864
1.67
.9033
1.92
.9688
1.18
.9237
1.43
.8860
1.68
.9050
1.93
.9724
1.19
.9209
1.44
.8858
1.69
.9068
1.94
.9761
1.20
.9182
1.45
.8857
1.70
.9086
1.95
.9799
1.21
.9156
1.46
.8856
1.71
.9106
1.96
.9837
1.22
.9131
1.47
.8856
1.72
.9126
1.97
.9877
1.23
.9108
1.48
.8857
1.73
.9147
1.98
.9917
1.24
.9085
1.49
.8859
1.74
.9168
1.99
.9958
1.25
.9064
1.50
.8862
1.75
.9191
2.00
1.0000
Table 18.5: Values of the gamma function.
18.8.6
Incomplete gamma functions
The incomplete gamma functions are deﬁned by
γ(a, x) =
 x
0
e−tta−1 dt
Γ(a, x) = Γ(a) −γ(a, x) =
 ∞
x
e−tta−1 dt
(18.48)
c
⃝2000 by Chapman & Hall/CRC

18.9
HYPERGEOMETRIC FUNCTIONS
Recall the geometric series and the binomial expansion (|z| < 1):
(1 −z)−1 =
∞

n=0
zn,
(1 −z)−a =
∞

n=0
−a
n

(−z)n =
∞

n=0
(a)n
zn
n!
(18.49)
where Pochhammer’s symbol, (a)n, is deﬁned in section 18.7.
18.9.1
Generalized hypergeometric function
The generalized hypergeometric function is deﬁned by:
pFq

a1, a2, . . . , ap; b1, b2, . . . , bq; x

= pFq

a1, . . . , ap; x
b1, . . . , bq

=
∞

k=0
Mp
i=1(ai)k
Mq
i=1(bi)k
xk
k!
=
∞

k=0
(a1)k · · · (ap)k
(b1)k · · · (bq)k
xk
k!
(18.50)
where (n)k represents Pochhammer’s symbol. Usually, 2F1(a, b; c; x) is called
“the” hypergeometric function; this is also called the Gauss hypergeometric
function.
18.9.2
Gauss hypergeometric function
The Gauss hypergeometric function is
F(a, b; c; x) = 2F1(a, b; c; x) =
∞

n=0
(a)n(b)n
(c)n
xn
n!
(18.51)
18.9.2.1
Special cases
F(a, b; b; x) = (1 −x)−a,
F(1, 1; 2; x) = −ln(1 −x)
x
,
F(1/2, 1; 3/2; x2) = 1
2x ln
1 + x
1 −x

,
F(1/2, 1; 3/2; −x2) = arctan x
x
,
F(1/2, 1/2; 3/2; x2) = arcsin x
x
,
F(1/2, 1/2; 3/2; −x2) = ln(x +
√
1 + x2)
x
.
(18.52)
c
⃝2000 by Chapman & Hall/CRC

18.9.2.2
Functional relations
F(a, b; c; x) = (1 −x)−aF

a, c −b; c;
x
x −1

= (1 −x)−bF

c −a, b; c;
x
x −1

= (1 −x)c−a−bF(c −a, c −b; c; x).
(18.53)
18.9.3
Conﬂuent hypergeometric functions
The conﬂuent hypergeometric functions, M and U, are deﬁned by
M(a, c, z) = lim
z→∞F

a, b; c; z
b

U(a, c, z) =
Γ(1 −c)
Γ(a −c + 1)M(a, c, z) + Γ(c −1)
Γ(a)
z1−cM(a −c, 2 −c, z)
(18.54)
Sometimes the notation ψ is used for M.
Sometimes U(a, b, z) is called the Tricomi function; it is the unique solution
to zw′′ + (b −z)w′ −aw = 0 with U(a, b, 0) = Γ(1 −b)/Γ(1 + a −b) and
U(a, b, ∞) = 0.
18.10
LOGARITHMIC FUNCTIONS
18.10.1
Deﬁnition of the natural log
The natural logarithm (also known as the Naperian logarithm) of z is written
as ln z or as loge z. It is sometimes written log z (this is also used to represent
a “generic” logarithm, a logarithm to any base). One deﬁnition is
ln z =
 z
1
dt
t ,
(18.55)
where the integration path from 1 to z does not cross the origin or the negative
real axis.
For complex values of z the natural logarithm, as deﬁned above, can be rep-
resented in terms of it’s magnitude and phase. If z = x + iy = reiθ, then
ln z = ln r + iθ, where r =

x2 + y2, x = r cos θ, and y = r sin θ.
18.10.2
Special values
lim
ϵ→0 (ln ϵ) = −∞
ln 1 = 0
ln e = 1
ln (−1) = iπ + 2πik
ln (±i) = ±iπ
2 + 2πik
c
⃝2000 by Chapman & Hall/CRC

18.10.3
Logarithms to a base other than e
The logarithmic function to the base a, written loga, is deﬁned as
loga z = logb z
logb a = ln z
ln a
(18.56)
Note the properties:
(a) loga ap = p
(b) loga b =
1
logb a
(c) log10 z = ln z
ln 10 = (log10 e) ln z ≈(0.4342944819 . . . ) ln z
(d) ln z = (ln 10) log10 z ≈(2.3025850929 . . . ) log10 z
18.10.4
Relation of the logarithm to the exponential
For real values of z the logarithm is a monotonic function, as is the exponen-
tial. Any monotonic function has a single–valued inverse function; the natural
logarithm is the inverse of the exponential. That is, if x = ey, then y = ln x
and x = eln x. The same inverse relations exist for bases other than e. For
example, if u = aw, then w = loga u and u = aloga u.
18.10.5
Identities
loga z1z2 = loga z1 + loga z2
for (−π < arg z1 + arg z2 < π)
loga
z1
z2
= loga z1 −loga z2
for (−π < arg z1 −arg z2 < π)
loga zn = n loga z
for (−π < n arg z < π), when n is an integer
18.10.6
Series expansions for the natural logarithm
ln (1 + z) = z −1
2z2 + 1
3z3 −· · · ,
for |z| < 1
ln z =
z −1
z

+ 1
2
z −1
z
2
+ 1
3
z −1
z
3
+ · · ·
for Re(z) ≥1
2
18.10.7
Derivative and integration formulae
d ln z
dz
= 1
z
 dz
z = ln |z| + C

ln z dz = z ln |z| −z + C
(18.57)
c
⃝2000 by Chapman & Hall/CRC

n
1
2
3
4
5
6
7
8
9
10
p(n)
1
2
3
5
7
11
15
22
30
42
n
11
12
13
14
15
16
17
18
19
20
p(n)
56
77
101
135
176
231
297
385
490
627
n
21
22
23
24
25
26
27
28
29
30
p(n)
792
1002
1255
1575
1958
2436
3010
3718
4565
5604
n
31
32
33
34
35
40
45
50
p(n)
6842
8349
10143
12310
14883
37338
89134
204226
Table 18.6: Values of the partition function.
18.11
PARTITIONS
A partition of a number n is a representation of n as the sum of any number
of positive integral parts (for example: 5 = 4 + 1 = 3 + 2 = 3 + 1 + 1 =
2+2+1 = 2+1+1+1 = 1+1+1+1+1 ). The number of partitions of n is
p(n) (for example, p(5) = 7). The number of partitions of n into at most m
parts is equal to the number of partitions of n into parts which do not exceed
m; this is denoted pm(n) (for example, p3(5) = 5 and p2(5) = 3).
The generating functions for p(n) and pm(n) are
1 +
∞

n=1
p(n)xn =
1
(1 −x)(1 −x2)(1 −x3) · · ·
(18.58)
1 +
∞

n=1
n

m=1
pm(n)xntm =
1
(1 −tx)(1 −tx2)(1 −tx3) · · ·
(18.59)
18.12
SIGNUM FUNCTION
The signum function indicates whether the argument is greater than or less
than zero:
sgn(x) =
(
1
x > 0
−1
x < 0
(18.60)
18.13
STIRLING NUMBERS
There are two types of Stirling numbers.
18.13.1
Stirling numbers
The number (−1)n−m  n
m

is the number of permutations of n symbols which
have exactly m cycles. The term
 n
m

is called a Stirling number. It can be
c
⃝2000 by Chapman & Hall/CRC

n
m = 1
2
3
4
5
6
7
1
1
2
−1
1
3
2
−3
1
4
−6
11
−6
1
5
24
−50
35
−10
1
6
−120
274
−225
85
−15
1
7
720
−1764
1624
−735
175
−21
1
8
−5040
13068
−13132
6769
−1960
322
−28
9
40320
−109584
118124
−67284
22449
−4536
546
10
−362880
1026576
−1172700
723680
−269325
63273
−9450
Table 18.7: Table of Stirling numbers
 n
m

.
numerically evaluated as (see Table 18.7):
" n
m
#
=
n−m

k=0
(−1)k
 n −1 + k
n −m + k
 2n −m
n −m −k
 'n −m −k
k
,
(18.61)
where
>
n−m−k
k
?
is a Stirling cycle number (see Table 18.8).
(1) There is the recurrence relation:
 n+1
m

=
"
n
m−1
#
−n
 n
m

.
(2) The factorial polynomial is deﬁned as x(n) = x(1−x) · · · (x−n+1) with
x(0) = 1 by deﬁnition. If n > 0, then
x(n) =
"n
1
#
x +
"n
2
#
x2 + · · · +
"n
n
#
xn
(18.62)
For example: x(3) = x(x −1)(x −2) = 2x −3x2 + x3 =
 3
1

x +
 3
2

x2 +
 3
2

x3
(3) Stirling numbers satisfy ∞
n=m
 n
m
 xn
n! = (log(1+x))m
m!
for |x| < 1.
Example 18.86:
For the 4 element set {a, b, c, d} there are  4
2

= 11 permutations
containing exactly 2 cycles. They are:
1
2
2
3
3
1
4
4

= (123)(4),
1
3
2
1
3
2
4
4

= (132)(4),
1
3
2
2
3
4
4
1

= (134)(2),
1
4
2
2
3
1
4
3

= (143)(2),
1
2
2
4
3
3
4
1

= (124)(3),
1
4
2
1
3
3
4
2

= (142)(3),
1
1
2
3
3
4
4
2

= (234)(1),
1
1
2
4
3
2
4
3

= (243)(1),
1
2
2
1
3
4
4
3

= (12)(34),
1
3
2
4
3
1
4
2

= (13)(24),
1
4
2
3
3
2
4
1

= (14)(23).
c
⃝2000 by Chapman & Hall/CRC

n
m = 1
2
3
4
5
6
7
1
1
2
1
1
3
1
3
1
4
1
7
6
1
5
1
15
25
10
1
6
1
31
90
65
15
1
7
1
63
301
350
140
21
1
8
1
127
966
1701
1050
266
28
9
1
255
3025
7770
6951
2646
462
10
1
511
9330
34105
42525
22827
5880
11
1
1023
28501
145750
246730
179487
63987
12
1
2047
86526
611501
1379400
1323652
627396
13
1
4095
261625
2532530
7508501
9321312
5715424
14
1
8191
788970
10391745
40075035
63436373
49329280
15
1
16383
2375101
42355950
210766920
420693273
408741333
Table 18.8: Table of Stirling cycle numbers
 n
m

.
18.13.2
Stirling cycle numbers
The Stirling cycle number,
 n
m

, is the number of ways to partition n into m
blocks. (Equivalently, it is the number of ways that n distinguishable balls
can be placed in m indistinguishable cells, with no cell empty.) The Stirling
cycle numbers can be numerically evaluated as (see Table 18.8):
> n
m
?
= 1
m!
m

i=0
(−1)m−i
m
i

in
(18.63)
Ordinary powers can be expanded in terms of factorial polynomials. If n > 0,
then
xn =
>n
1
?
x(1) +
>n
2
?
x(2) + · · · +
>n
n
?
x(n)
(18.64)
For example: x3 =
 3
1

x(1) +
 3
2

x(2) +
 3
2

x(3)
Example 18.87:
Placing the 4 distinguishable balls {a, b, c, d} into 2 distinguishable
cells, so that no cell is empty, can be done in  4
2

= 7 ways. These are (vertical bars
delineate the cells):
| ab | cd |
| ad | bc |
| ac | bd |
| a | bcd |
| b | acd |
| c | abd |
| d | abc |
18.14
SUMS OF POWERS OF INTEGERS
Deﬁne sk(n) = 1k + 2k + · · · + nk =
n

m=1
mk. Properties include:
(a) sk(n) = (k + 1)−1 [Bk+1(n + 1) −Bk+1(0)], where the Bk are Bernoulli
polynomials
c
⃝2000 by Chapman & Hall/CRC

n
n

k=1
k
n

k=1
k2
n

k=1
k3
n

k=1
k4
n

k=1
k5
1
1
1
1
1
1
2
3
5
9
17
33
3
6
14
36
98
276
4
10
30
100
354
1300
5
15
55
225
979
4425
6
21
91
441
2275
12201
7
28
140
784
4676
29008
8
36
204
1296
8772
61776
9
45
285
2025
15333
120825
10
55
385
3025
25333
220825
11
66
506
4356
39974
381876
12
78
650
6084
60710
630708
13
91
819
8281
89271
1002001
14
105
1015
11025
127687
1539825
15
120
1240
14400
178312
2299200
16
136
1496
18496
243848
3347776
17
153
1785
23409
327369
4767633
18
171
2109
29241
432345
6657201
19
190
2470
36100
562666
9133300
20
210
2870
44100
722666
12333300
Table 18.9: Sums of powers of integers
(b) Writing sk(n) as
k+1

m=1
amnk−m+2 there is the recursion formula:
sk+1(n) =
k + 1
k + 2

a1nk+2 + · · · +
k + 1
k

a3nk
+ · · · +
k + 1
2

ak+1n2 +
%
1 −(k + 1)
k+1

m=1
am
k + 3 −m
&
n
(18.65)
s1(n) = 1 + 2 + 3 + · · · + n = 1
2n(n + 1)
s2(n) = 12 + 22 + 32 + · · · + n2 = 1
6n(n + 1)(2n + 1)
s3(n) = 13 + 23 + 33 + · · · + n3 = 1
4(n2(n + 1)2) = [s1(n)]2
s4(n) = 14 + 24 + 34 + · · · + n4 = 1
5(3n2 + 3n −1)s2(n)
c
⃝2000 by Chapman & Hall/CRC

s5(n) = 15 + 25 + 35 + · · · + n5 = 1
12n2(n + 1)2(2n2 + 2n −1)
s6(n) = n
42(n + 1)(2n + 1)(3n4 + 6n3 −3n + 1)
s7(n) = n2
24(n + 1)2(3n4 + 6n3 −n2 −4n + 2)
s8(n) = n
90(n + 1)(2n + 1)(5n6 + 15n5 + 5n4 −15n3 −n2 + 9n −3)
s9(n) = n2
20(n + 1)2(2n6 + 6n5 + n4 −8n3 + n2 + 6n −3)
s10(n) = n
66(n + 1)(2n + 1)(3n8 + 12n7 + 8n6 −18n5
−10n4 + 24n3 + 2n2 −15n + 5)
18.15
TABLES OF ORTHOGONAL POLYNOMIALS
In the following:
• Hn are Hermite polynomials
• Ln are Laguerre polynomials
• Pn are Legendre polynomials
• Tn are Chebyshev polynomials
• Un are Chebyshev polynomials
H0 = 1
x10 = (30240H0 + 75600H2 + 25200H4 + 2520H6 + 90H8 + H10)/1024
H1 = 2x
x9 = (15120H1 + 10080H3 + 1512H5 + 72H7 + H9)/512
H2 = 4x2 −2
x8 = (1680H0 + 3360H2 + 840H4 + 56H6 + H8)/256
H3 = 8x3 −12x
x7 = (840H1 + 420H3 + 42H5 + H7)/128
H4 = 16x4 −48x2 + 12
x6 = (120H0 + 180H2 + 30H4 + H6)/64
H5 = 32x5 −160x3 + 120x
x5 = (60H1 + 20H3 + H5)/32
H6 = 64x6 −480x4 + 720x2 −120
x4 = (12H0 + 12H2 + H4)/16
H7 = 128x7 −1344x5 + 3360x3 −1680x
x3 = (6H1 + H3)/8
H8 = 256x8 −3584x6 + 13440x4 −13440x2 + 1680
x2 = (2H0 + H2)/4
H9 = 512x9 −9216x7 + 48384x5 −80640x3 + 30240x
x = (H1)/2
H10 = 1024x10 −23040x8 + 161280x6 −403200x4 + 302400x2 −30240
1 = H0
L0 = 1
x6 = 720L0 −4320L1 + 10800L2 −14400L3 + 10800L4 −4320L5 + 720L6
L1 = −x + 1
x5 = 120L0 −600L1 + 1200L2 −1200L3 + 600L4 −120L5
L2 = (x2 −4x + 2)/2
x4 = 24L0 −96L1 + 144L2 −96L3 + 24L4
L3 = (−x3 + 9x2 −18x + 6)/6
x3 = 6L0 −18L1 + 18L2 −6L3
L4 = (x4 −16x3 + 72x2 −96x + 24)/24
x2 = 2L0 −4L1 + 2L2
L5 = (−x5 + 25x4 −200x3 + 600x2 −600x + 120)/120
x = L0 −L1
L6 = (x6 −36x5 + 450x4 −2400x3 + 5400x2 −4320x + 720)/720
1 = L0
P0 = 1
x10 = (4199P0 + 16150P2 + 15504P4 + 7904P6 + 2176P8 + 256P10)/46189
P1 = x
x9 = (3315P1 + 4760P3 + 2992P5 + 960P7 + 128P9)/12155
c
⃝2000 by Chapman & Hall/CRC

P2 = (3x2 −1)/2
x8 = (715P0 + 2600P2 + 2160P4 + 832P6 + 128P8)/6435
P3 = (5x3 −3x)/2
x7 = (143P1 + 182P3 + 88P5 + 16P7)/429
P4 = (35x4 −30x2 + 3)/8
x6 = (33P0 + 110P2 + 72P4 + 16P6)/231
P5 = (63x5 −70x3 + 15x)/8
x5 = (27P1 + 28P3 + 8P5)/63
P6 = (231x6 −315x4 + 105x2 −5)/16
x4 = (7P0 + 20P2 + 8P4)/35
P7 = (429x7 −693x5 + 315x3 −35x)/16
x3 = (3P1 + 2P3)/5
P8 = (6435x8 −12012x6 + 6930x4 −1260x2 + 35)/128
x2 = (P0 + 2P2)/3
P9 = (12155x9 −25740x7 + 18018x5 −4620x3 + 315x)/128
x = P1
P10 = (46189x10 −109395x8 + 90090x6 −30030x4 + 3465x2 −63)/256
1 = P0
T0 = 1
x10 = (126T0 + 210T2 + 120T4 + 45T6 + 10T8 + T10)/512
T1 = x
x9 = (126T1 + 84T3 + 36T5 + 9T7 + T9)/256
T2 = 2x2 −1
x8 = (35T0 + 56T2 + 28T4 + 8T6 + T8)/128
T3 = 4x3 −3x
x7 = (35T1 + 21T3 + 7T5 + T7)/64
T4 = 8x4 −8x2 + 1
x6 = (10T0 + 15T2 + 6T4 + T6)/32
T5 = 16x5 −20x3 + 5x
x5 = (10T1 + 5T3 + T5)/16
T6 = 32x6 −48x4 + 18x2 −1
x4 = (3T0 + 4T2 + T4)/8
T7 = 64x7 −112x5 + 56x3 −7x
x3 = (3T1 + T3)/4
T8 = 128x8 −256x6 + 160x4 −32x2 + 1
x2 = (T0 + T2)/2
T9 = 256x9 −576x7 + 432x5 −120x3 + 9x
x = T1
T10 = 512x10 −1280x8 + 1120x6 −400x4 + 50x2 −1
1 = T0
U0 = 1
x10 = (42U0 + 90U2 + 75U4 + 35U6 + 9U8 + U10)/1024
U1 = 2x
x9 = (42U1 + 48U3 + 27U5 + 8U7 + U9)/512
U2 = 4x2 −1
x8 = (14U0 + 28U2 + 20U4 + 7U6 + U8)/256
U3 = 8x3 −4x
x7 = (14U1 + 14U3 + 6U5 + U7)/128
U4 = 16x4 −12x2 + 1
x6 = (5U0 + 9U2 + 5U4 + U6)/64
U5 = 32x5 −32x3 + 6x
x5 = (5U1 + 4U3 + U5)/32
U6 = 64x6 −80x4 + 24x2 −1
x4 = (2U0 + 3U2 + U4)/16
U7 = 128x7 −192x5 + 80x3 −8x
x3 = (2U1 + U3)/8
U8 = 256x8 −448x6 + 240x4 −40x2 + 1
x2 = (U0 + U2)/4
U9 = 512x9 −1024x7 + 672x5 −160x3 + 10x
x = (U1)/2
U10 = 1024x10 −2304x8 + 1792x6 −560x4 + 60x2 −1
1 = U0
18.16
REFERENCES
1. M. Abramowitz and I. A. Stegun, Handbook of mathematical functions,
NIST, Washington, DC, 1964,
2. A. Erd´elyi (ed.), Bateman Manuscript Project, Tables of integral trans-
forms, in 3 volumes, McGraw–Hill, New York, 1954.
3. N. M. Temme, Special functions: An introduction to the classical func-
tions of mathematical physics, John Wiley & Sons, New York, 1996.
c
⃝2000 by Chapman & Hall/CRC

List of Notation
Symbols
!!: double factorial
!: factorial
′: complement of a set
(1): treatment totals
(a): treatment totals
(ab): treatment totals
(b): treatment totals
(n)k: Pochhammer’s symbol
n
k

: binomial coeﬃcient

n
n1,...,nk

: multinomial coeﬃcient
.: dot notation
=: set equality
T: transpose
[A]: eﬀect total for factor a
⌈⌉: ceiling function
⌊⌋: ﬂoor function
 n
m

: Stirling numbers
{ }: empty set
 n
m

: Stirling cycle numbers
¯: mean
∩: set intersection
∪: set union
∈: in set
̸∈: not in set
|: conditional probability
⊕: exclusive or
∼: distribution similarity
⊂: subset
⊆: improper subset
⊃: superset
˜: median
˜: triangular matrix
Greek Letters
α: Weibull parameter
α: conﬁdence coeﬃcient
α: type I error
(αβ)ij: level ij eﬀect
αi: ith treatment eﬀect
αi: level i factor A eﬀect
β: Weibull parameter
β: type II error
βj: level j factor B eﬀect
δi: Press residuals
δ(x): delta function
ϵ: error of estimation
ϵij: error term
ˆϵij: estimated expected count
ϵijk: error term
γ: Euler’s constant
Γ(a, x): incomplete gamma function
γ(a, x): incomplete gamma function
Γ(x): gamma function
κr: cumulant
λ: parameter
exponential distribution
noncentral chi–square
noncentrality
of a BIBD
Poisson distribution
λ: test statistic
λj: scaling factor
µ: vector of means
µ: parameter
location
noncentral chi–square
scale
µr: moment about the mean
µ[r]: factorial moment
µ′
r: moment about the origin
ν: parameter
t distribution
chi distribution
chi–squared distribution
shape
c
⃝2000 by Chapman & Hall/CRC

ν1: parameter
F distribution
shape
ν2: parameter
F distribution
shape
φ: characteristic function
φ: empty set
ϕ(x): digamma function
Φ(z): normal distribution function
166
ρij: correlation coeﬃcient
Σ: variance–covariance matrix
σ: parameter
Rayleigh distribution
scale
shape
σ: standard deviation
σ chart
σ-ﬁeld
σ2: variance
σ2
X|y: conditional variance
σi: standard deviation
σii: variance
σij: covariance
Σk/k−1: error covariance matrix
τ: Kendall’s Tau
θ: distribution parameter
θ: shape parameter
ξi(x): orthogonal polynomials
ξ′
i(x): scaled orthogonal polynomials
276
Numbers
0: vector of all zeros
1: vector of all ones
A
A: interarrival time
A: midrange
a: location parameter
A/B/c/K/m/Z: queue
representation
ALFS: additive lagged-Fibonacci
sequence
an: proportion of customers
anova: analysis of variance
AOQ: average outgoing quality
AOQL: average outgoing quality
limit
AQL: acceptable quality level
AR(k): autoregressive model
ARMA(k, l): mixed model
B
B: service time
B: vector of block totals
b: parameter of a BIBD
b: power function parameter
B(a, b): beta function
B[ ]: bias
BIBD: balanced incomplete block
design
C
C: channel capacity
b: power function parameter
c: bin width
c: number of identical servers
c chart
C(n, k): k-combination
C(n; n1, . . . , nk): multinomial
coeﬃcient
cdf: cumulative distribution function
33
CF: cumulative frequency
ch: characteristic roots
cosh(x): hyperbolic function
cos(x): circular function
CQV: coeﬃcient of quartile variation
17
CR(n, k): k-combination with
replacement
c(t): cumulant generating function
38
CV: coeﬃcient of variation
D
D: Kolmogorov–Smirnoﬀstatistic
346, 348
D: constant service time
D+: Kolmogorov–Smirnoﬀstatistic
346, 348
δij: Kronecker delta
det(X): determinant of matrix X
404
Di: Cook’s distance
c
⃝2000 by Chapman & Hall/CRC

Dn: derangement
dn: proportion of customers
Dx: diagonal matrix
E
eij: observed value
E[ ]: expectation
ei: residual
Ek: Erlang-k service time
erf: error function
erfc: complementary error function
512
F
F: Fourier transform
FCFS: ﬁrst come, ﬁrst served
FIFO: ﬁrst in, ﬁrst out
fk: frequency
F(x): cumulative distribution
function
F(x1, x2, . . . , xn): cumulative
distribution function
f(x1, x2, . . . , xn): probability density
function
f(x | y): conditional probability
G
G: general service time distribution
441
g1: coeﬃcient of skewness
g2: coeﬃcient of skewness
GI: general interarrival time
GM: geometric mean
H
H(pX): entropy
H0: null hypothesis
Ha: alternative hypothesis
Hk: k-stage hyperexponential service
time
HM: harmonic mean
Hn(x): Hermite polynomial
I
I: identity matrix
I(X, Y ): mutual information
iid: independent and identically
distributed
Iν(z): Bessel function
IQR: interquartile range
J
J: determinant of the Jacobian
jn(z): half order Bessel function
Jν(z): Bessel function
K
K: system capacity
k: parameter of a BIBD
Kk: Kalman gain matrix
Kν(z): Bessel function . . . . . . . . . .509
KX(t1, t2): correlation function
L
L: average number of customers
L(θ): expected loss function
L(θ): likelihood function
ℓ(θ, a): loss function
λ: average arrival rate
λlower: conﬁdence interval
λupper: conﬁdence interval
LCG: linear congruential generator
450
LCL: lower control limit
LIFO: last in, ﬁrst out
Ln(x): Laguerre polynomial
ln: logarithm
log: logarithm
logb: logarithm to base b
Lq: average number of customers
LTPD: lot tolerance percent
defective
M
M: exponential service time
M: hypergeometric function
m: number in the source
MA(l): moving average
M/D/1: queue
MD: mean deviation
M/Ek/1: queue
mgf: moment generating function
MLE: maximum likelihood estimator
M/M/1: queue
M/M/1/K: queue
M/M/2: queue
M/M/c: queue
c
⃝2000 by Chapman & Hall/CRC

M/M/c/c: queue
M/M/c/K: queue
M/M/∞: queue
Mo: mode
mr: moment about the mean
m′
r: moment about the origin
ms-lim: mean square limit
MSE: mean square—error
MSR: mean square—regression
MTBF: mean time between failures
µ: average service rate
µp: MTBF for parallel system
µs: MTBF for series system
MVUE: minimum variance unbiased
estimator
mX(t): moment generating function
37
µX|y: conditional mean
N
N: natural numbers
n: shape parameter
ne: degrees of freedom—errors
nh: degrees of freedom—hypothesis
410
P
p chart
p(n) partitions
P(n, k): k-permutation
P(x, y): Markov transition function
p-value
∂()/∂(): derivative
pdf: probability density function
π(x): probability distribution
pm(n) restricted partitions
pmf: probability mass function
pn: proportion of time
Pn(x): Legendre polynomial
P n(x, y): n-step Markov transition
matrix
P R(n, k): k-permutation with
replacement
Per(xn): period of a sequence
PRESS: prediction sum of squares
PRI: priority service
PRNG: pseudorandom number
generator
Prob[ ]: probability
P(t): factorial moment generating
function
p(x): probability mass function
pX×Y : joint probability distribution
p(x | y): conditional probability
Q
QD: quartile deviation
Qi: ith quartile
R
R: range
R: rate of a code
R: real numbers
r: parameter of a BIBD
r: sample correlation coeﬃcient
R chart
R(t): reliability function
r(θ, a): regret function
R(θ, di): risk function
R2: coeﬃcient multiple
determination
R2
a: adjusted coeﬃcient multiple
determination
Re: real part
ρ: server utilization
Ri: reliability of a component
RMS: root mean square
Rp: reliability of parallel system
RR: rejection region
Rs: reliability of series system
rS: Spearman’s rank correlation
coeﬃcient
rS,α: Spearman’s rank correlation
coeﬃcient
RSS: random service
S
S: sample space
S: universal set
s: sample standard deviation
S2: mean square—error
S2: sample variance
s2 sample variance
c
⃝2000 by Chapman & Hall/CRC

S2
C: mean square—columns
SEM: standard error—mean
sgn: signum function
sinh(x): hyperbolic function
sin(x): circular function
Sk1: coeﬃcient of skewness
Sk2: coeﬃcient of skewness
Skq: coeﬃcient of skewness
SPRT: sequential probability ratio
test
S2
R mean square—rows
S2
R: mean square—rows
SRS: shift register sequence
SSA: sum of squares—treatment
SSE(F): sum of squares—error—full
model
SSE(F): sum of
squares—error—reduced
model
SSE: sum of squares—error 263, 269,
283, 415
SSH: sum of squares—hypothesis 415
SSLF: sum of squares—lack of ﬁt 266
SSPE: sum of squares—pure error
266
SSR: sum of squares—regression 263,
269
SST: sum of squares—total 263, 269,
283
St: Kendall’s score
S2
Tr: mean square—treatments
Sxx
Sxy
Syy
T
T: sample total
T: vector of treatment totals
T..: sum of all observations
Ti.: sum of ith observations
Tn(x): Chebyshev polynomial
tr: trace of a matrix
TS: test statistic
tx,y transition probabilities
U
U: Mann–Whitney U statistic
U: hypergeometric function
u: traﬃc intensity
UCL: upper control limit
ui: coded class mark
UMV: uniformly minimum variance
unbiased
Un(x): Chebyshev polynomial
V
v: parameter of a BIBD
vk: Gaussian white process
VR: variance ratio
W
W: average time
W: range, standardized
W: range, studentized
W(t): Brownian motion
wi: weight
wk: Gaussian white process
Wq: average time
X
X: design matrix
X: random vector
x: column vector
X(i): order statistic
X: sample mean
x: mean of x
x chart
xi: class mark
.xk/k−1: estimate of xk
xo: computing origin
˜x: median of x
xtr(p): trimmed mean of x
Y
y..: mean of all observations
yi.: mean of ith observation
yij: observed value of Yij
yn(z): half order Bessel function
Yν(z): Bessel function
Z
Z: queue discipline
Z(t): instantaneous failure rate
Zk−1: sequence of observed values
c
⃝2000 by Chapman & Hall/CRC


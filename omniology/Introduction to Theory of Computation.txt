Introduction to Theory of Computation
Anil Maheshwari
Michiel Smid
School of Computer Science
Carleton University
Ottawa
Canada
{anil,michiel}@scs.carleton.ca
April 11, 2016

ii
Contents

Contents
Preface
vi
1
Introduction
1
1.1
Purpose and motivation
. . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Complexity theory
. . . . . . . . . . . . . . . . . . . .
2
1.1.2
Computability theory . . . . . . . . . . . . . . . . . . .
2
1.1.3
Automata theory . . . . . . . . . . . . . . . . . . . . .
3
1.1.4
This course
. . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Mathematical preliminaries
. . . . . . . . . . . . . . . . . . .
4
1.3
Proof techniques
. . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.1
Direct proofs
. . . . . . . . . . . . . . . . . . . . . . .
8
1.3.2
Constructive proofs . . . . . . . . . . . . . . . . . . . .
9
1.3.3
Nonconstructive proofs . . . . . . . . . . . . . . . . . .
10
1.3.4
Proofs by contradiction . . . . . . . . . . . . . . . . . .
11
1.3.5
The pigeon hole principle . . . . . . . . . . . . . . . . .
12
1.3.6
Proofs by induction . . . . . . . . . . . . . . . . . . . .
13
1.3.7
More examples of proofs . . . . . . . . . . . . . . . . .
15
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2
Finite Automata and Regular Languages
21
2.1
An example: Controling a toll gate . . . . . . . . . . . . . . .
21
2.2
Deterministic ﬁnite automata
. . . . . . . . . . . . . . . . . .
23
2.2.1
A ﬁrst example of a ﬁnite automaton . . . . . . . . . .
26
2.2.2
A second example of a ﬁnite automaton
. . . . . . . .
28
2.2.3
A third example of a ﬁnite automaton
. . . . . . . . .
29
2.3
Regular operations . . . . . . . . . . . . . . . . . . . . . . . .
31
2.4
Nondeterministic ﬁnite automata
. . . . . . . . . . . . . . . .
35
2.4.1
A ﬁrst example . . . . . . . . . . . . . . . . . . . . . .
35

iv
Contents
2.4.2
A second example . . . . . . . . . . . . . . . . . . . . .
37
2.4.3
A third example . . . . . . . . . . . . . . . . . . . . . .
38
2.4.4
Deﬁnition of nondeterministic ﬁnite automaton
. . . .
39
2.5
Equivalence of DFAs and NFAs . . . . . . . . . . . . . . . . .
41
2.5.1
An example . . . . . . . . . . . . . . . . . . . . . . . .
44
2.6
Closure under the regular operations
. . . . . . . . . . . . . .
48
2.7
Regular expressions . . . . . . . . . . . . . . . . . . . . . . . .
52
2.8
Equivalence of regular expressions and regular languages . . .
56
2.8.1
Every regular expression describes a regular language .
57
2.8.2
Converting a DFA to a regular expression
. . . . . . .
60
2.9
The pumping lemma and nonregular languages . . . . . . . . .
67
2.9.1
Applications of the pumping lemma . . . . . . . . . . .
69
2.10 Higman’s Theorem . . . . . . . . . . . . . . . . . . . . . . . .
76
2.10.1 Dickson’s Theorem . . . . . . . . . . . . . . . . . . . .
76
2.10.2 Proof of Higman’s Theorem . . . . . . . . . . . . . . .
77
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
3
Context-Free Languages
91
3.1
Context-free grammars . . . . . . . . . . . . . . . . . . . . . .
91
3.2
Examples of context-free grammars . . . . . . . . . . . . . . .
94
3.2.1
Properly nested parentheses . . . . . . . . . . . . . . .
94
3.2.2
A context-free grammar for a nonregular language . . .
95
3.2.3
A context-free grammar for the complement of a non-
regular language
. . . . . . . . . . . . . . . . . . . . .
97
3.2.4
A context-free grammar that veriﬁes addition
. . . . .
98
3.3
Regular languages are context-free . . . . . . . . . . . . . . . . 100
3.3.1
An example . . . . . . . . . . . . . . . . . . . . . . . . 102
3.4
Chomsky normal form
. . . . . . . . . . . . . . . . . . . . . . 104
3.4.1
An example . . . . . . . . . . . . . . . . . . . . . . . . 109
3.5
Pushdown automata
. . . . . . . . . . . . . . . . . . . . . . . 112
3.6
Examples of pushdown automata . . . . . . . . . . . . . . . . 116
3.6.1
Properly nested parentheses . . . . . . . . . . . . . . . 116
3.6.2
Strings of the form 0n1n
. . . . . . . . . . . . . . . . . 117
3.6.3
Strings with b in the middle . . . . . . . . . . . . . . . 118
3.7
Equivalence of pushdown automata and context-free grammars 120
3.8
The pumping lemma for context-free languages
. . . . . . . . 124
3.8.1
Proof of the pumping lemma . . . . . . . . . . . . . . . 125
3.8.2
Applications of the pumping lemma . . . . . . . . . . . 128

Contents
v
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
4
Turing Machines and the Church-Turing Thesis
137
4.1
Deﬁnition of a Turing machine . . . . . . . . . . . . . . . . . . 137
4.2
Examples of Turing machines
. . . . . . . . . . . . . . . . . . 141
4.2.1
Accepting palindromes using one tape
. . . . . . . . . 141
4.2.2
Accepting palindromes using two tapes . . . . . . . . . 142
4.2.3
Accepting anbncn using one tape . . . . . . . . . . . . . 143
4.2.4
Accepting anbncn using tape alphabet {a, b, c, 2} . . . . 145
4.2.5
Accepting ambncmn using one tape . . . . . . . . . . . . 147
4.3
Multi-tape Turing machines . . . . . . . . . . . . . . . . . . . 148
4.4
The Church-Turing Thesis . . . . . . . . . . . . . . . . . . . . 151
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
5
Decidable and Undecidable Languages
157
5.1
Decidability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
5.1.1
The language ADFA . . . . . . . . . . . . . . . . . . . . 158
5.1.2
The language ANFA . . . . . . . . . . . . . . . . . . . . 159
5.1.3
The language ACFG . . . . . . . . . . . . . . . . . . . . 159
5.1.4
The language ATM
. . . . . . . . . . . . . . . . . . . . 160
5.1.5
The Halting Problem . . . . . . . . . . . . . . . . . . . 162
5.2
Countable sets . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
5.2.1
The Halting Problem revisited . . . . . . . . . . . . . . 167
5.3
Rice’s Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 169
5.3.1
Proof of Rice’s Theorem . . . . . . . . . . . . . . . . . 170
5.4
Enumerability . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
5.4.1
Hilbert’s problem . . . . . . . . . . . . . . . . . . . . . 174
5.4.2
The language ATM
. . . . . . . . . . . . . . . . . . . . 175
5.5
Where does the term “enumerable” come from? . . . . . . . . 176
5.6
Most languages are not enumerable . . . . . . . . . . . . . . . 179
5.6.1
The set of enumerable languages is countable
. . . . . 180
5.6.2
The set of all languages is not countable . . . . . . . . 181
5.6.3
There are languages that are not enumerable . . . . . . 183
5.7
The relationship between decidable and enumerable languages 184
5.8
A language A such that both A and A are not enumerable . . 186
5.8.1
EQTM is not enumerable . . . . . . . . . . . . . . . . . 186
5.8.2
EQTM is not enumerable . . . . . . . . . . . . . . . . . 188
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

vi
Contents
6
Complexity Theory
197
6.1
The running time of algorithms . . . . . . . . . . . . . . . . . 197
6.2
The complexity class P . . . . . . . . . . . . . . . . . . . . . . 199
6.2.1
Some examples . . . . . . . . . . . . . . . . . . . . . . 199
6.3
The complexity class NP . . . . . . . . . . . . . . . . . . . . . 202
6.3.1
P is contained in NP . . . . . . . . . . . . . . . . . . . 208
6.3.2
Deciding NP-languages in exponential time
. . . . . . 208
6.3.3
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . 210
6.4
Non-deterministic algorithms
. . . . . . . . . . . . . . . . . . 211
6.5
NP-complete languages
. . . . . . . . . . . . . . . . . . . . . 213
6.5.1
Two examples of reductions . . . . . . . . . . . . . . . 215
6.5.2
Deﬁnition of NP-completeness . . . . . . . . . . . . . . 220
6.5.3
An NP-complete domino game
. . . . . . . . . . . . . 222
6.5.4
Examples of NP-complete languages . . . . . . . . . . 231
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
7
Summary
239

Preface
This is a free textbook for an undergraduate course on the Theory of Com-
putation, which we have been teaching at Carleton University since 2002.
Until the 2011/2012 academic year, this course was oﬀered as a second-year
course (COMP 2805) and was compulsory for all Computer Science students.
Starting with the 2012/2013 academic year, the course has been downgraded
to a third-year optional course (COMP 3803).
We have been developing this book since we started teaching this course.
Currently, we cover most of the material from Chapters 2–5 during a 12-week
term with three hours of classes per week.
The material from Chapter 6, on Complexity Theory, is taught in the
third-year course COMP 3804 (Design and Analysis of Algorithms). In the
early years of COMP 2805, we gave a two-lecture overview of Complexity
Theory at the end of the term. Even though this overview has disappeared
from the course, we decided to keep Chapter 6. This chapter has not been
revised/modiﬁed for a long time.
The course as we teach it today has been inﬂuenced by the following two
textbooks:
• Introduction to the Theory of Computation (second edition), by Michael
Sipser, Thomson Course Technnology, Boston, 2006.
• Einf¨uhrung in die Theoretische Informatik, by Klaus Wagner, Springer-
Verlag, Berlin, 1994.
Besides reading this text, we recommend that you also take a look at
these excellent textbooks, as well as one or more of the following ones:
• Elements of the Theory of Computation (second edition), by Harry
Lewis and Christos Papadimitriou, Prentice-Hall, 1998.

viii
• Introduction to Languages and the Theory of Computation (third edi-
tion), by John Martin, McGraw-Hill, 2003.
• Introduction to Automata Theory, Languages, and Computation (third
edition), by John Hopcroft, Rajeev Motwani, Jeﬀrey Ullman, Addison
Wesley, 2007.
Please let us know if you ﬁnd errors, typos, simpler proofs, comments,
omissions, or if you think that some parts of the book “need improvement”.

Chapter 1
Introduction
1.1
Purpose and motivation
This course is on the Theory of Computation, which tries to answer the
following questions:
• What are the mathematical properties of computer hardware and soft-
ware?
• What is a computation and what is an algorithm? Can we give rigorous
mathematical deﬁnitions of these notions?
• What are the limitations of computers?
Can “everything” be com-
puted? (As we will see, the answer to this question is “no”.)
Purpose of the Theory of Computation: Develop formal math-
ematical models of computation that reﬂect real-world computers.
This ﬁeld of research was started by mathematicians and logicians in the
1930’s, when they were trying to understand the meaning of a “computation”.
A central question asked was whether all mathematical problems can be
solved in a systematic way. The research that started in those days led to
computers as we know them today.
Nowadays, the Theory of Computation can be divided into the follow-
ing three areas: Complexity Theory, Computability Theory, and Automata
Theory.

2
Chapter 1.
Introduction
1.1.1
Complexity theory
The main question asked in this area is “What makes some problems com-
putationally hard and other problems easy?”
Informally, a problem is called “easy”, if it is eﬃciently solvable. Exam-
ples of “easy” problems are (i) sorting a sequence of, say, 1,000,000 numbers,
(ii) searching for a name in a telephone directory, and (iii) computing the
fastest way to drive from Ottawa to Miami. On the other hand, a problem is
called “hard”, if it cannot be solved eﬃciently, or if we don’t know whether
it can be solved eﬃciently. Examples of “hard” problems are (i) time table
scheduling for all courses at Carleton, (ii) factoring a 300-digit integer into
its prime factors, and (iii) computing a layout for chips in VLSI.
Central Question in Complexity Theory: Classify problems ac-
cording to their degree of “diﬃculty”.
Give a rigorous proof that
problems that seem to be “hard” are really “hard”.
1.1.2
Computability theory
In the 1930’s, G¨odel, Turing, and Church discovered that some of the fun-
damental mathematical problems cannot be solved by a “computer”. (This
may sound strange, because computers were invented only in the 1940’s).
An example of such a problem is “Is an arbitrary mathematical statement
true or false?” To attack such a problem, we need formal deﬁnitions of the
notions of
• computer,
• algorithm, and
• computation.
The theoretical models that were proposed in order to understand solvable
and unsolvable problems led to the development of real computers.
Central Question in Computability Theory: Classify problems
as being solvable or unsolvable.

1.1.
Purpose and motivation
3
1.1.3
Automata theory
Automata Theory deals with deﬁnitions and properties of diﬀerent types of
“computation models”. Examples of such models are:
• Finite Automata. These are used in text processing, compilers, and
hardware design.
• Context-Free Grammars. These are used to deﬁne programming lan-
guages and in Artiﬁcial Intelligence.
• Turing Machines.
These form a simple abstract model of a “real”
computer, such as your PC at home.
Central Question in Automata Theory: Do these models have
the same power, or can one model solve more problems than the
other?
1.1.4
This course
In this course, we will study the last two areas in reverse order: We will start
with Automata Theory, followed by Computability Theory. The ﬁrst area,
Complexity Theory, will be covered in COMP 3804.
Actually, before we start, we will review some mathematical proof tech-
niques. As you may guess, this is a fairly theoretical course, with lots of
deﬁnitions, theorems, and proofs. You may guess this course is fun stuﬀfor
math lovers, but boring and irrelevant for others. You guessed it wrong, and
here are the reasons:
1. This course is about the fundamental capabilities and limitations of
computers. These topics form the core of computer science.
2. It is about mathematical properties of computer hardware and software.
3. This theory is very much relevant to practice, for example, in the design
of new programming languages, compilers, string searching, pattern
matching, computer security, artiﬁcial intelligence, etc., etc.
4. This course helps you to learn problem solving skills. Theory teaches
you how to think, prove, argue, solve problems, express, and abstract.

4
Chapter 1.
Introduction
5. This theory simpliﬁes the complex computers to an abstract and simple
mathematical model, and helps you to understand them better.
6. This course is about rigorously analyzing capabilities and limitations
of systems.
Where does this course ﬁt in the Computer Science Curriculum at Car-
leton University? It is a theory course that is the third part in the series
COMP 1805, COMP 2804, COMP 3803, COMP 3804, and COMP 4804.
This course also widens your understanding of computers and will inﬂuence
other courses including Compilers, Programming Languages, and Artiﬁcial
Intelligence.
1.2
Mathematical preliminaries
Throughout this course, we will assume that you know the following mathe-
matical concepts:
1. A set is a collection of well-deﬁned objects. Examples are (i) the set of
all Dutch Olympic Gold Medallists, (ii) the set of all pubs in Ottawa,
and (iii) the set of all even natural numbers.
2. The set of natural numbers is N = {1, 2, 3, . . .}.
3. The set of integers is Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.
4. The set of rational numbers is Q = {m/n : m ∈Z, n ∈Z, n ̸= 0}.
5. The set of real numbers is denoted by R.
6. If A and B are sets, then A is a subset of B, written as A ⊆B, if every
element of A is also an element of B. For example, the set of even
natural numbers is a subset of the set of all natural numbers. Every
set A is a subset of itself, i.e., A ⊆A. The empty set is a subset of
every set A, i.e., ∅⊆A.
7. If B is a set, then the power set P(B) of B is deﬁned to be the set of
all subsets of B:
P(B) = {A : A ⊆B}.
Observe that ∅∈P(B) and B ∈P(B).

1.2.
Mathematical preliminaries
5
8. If A and B are two sets, then
(a) their union is deﬁned as
A ∪B = {x : x ∈A or x ∈B},
(b) their intersection is deﬁned as
A ∩B = {x : x ∈A and x ∈B},
(c) their diﬀerence is deﬁned as
A \ B = {x : x ∈A and x ̸∈B},
(d) the Cartesian product of A and B is deﬁned as
A × B = {(x, y) : x ∈A and y ∈B},
(e) the complement of A is deﬁned as
A = {x : x ̸∈A}.
9. A binary relation on two sets A and B is a subset of A × B.
10. A function f from A to B, denoted by f : A →B, is a binary relation
R, having the property that for each element a ∈A, there is exactly
one ordered pair in R, whose ﬁrst component is a. We will also say
that f(a) = b, or f maps a to b, or the image of a under f is b. The
set A is called the domain of f, and the set
{b ∈B : there is an a ∈A with f(a) = b}
is called the range of f.
11. A function f : A →B is one-to-one (or injective), if for any two distinct
elements a and a′ in A, we have f(a) ̸= f(a′). The function f is onto
(or surjective), if for each element b ∈B, there exists an element a ∈A,
such that f(a) = b; in other words, the range of f is equal to the set
B. A function f is a bijection, if f is both injective and surjective.
12. A binary relation R ⊆A × A is an equivalence relation, if it satisﬁes
the following three conditions:

6
Chapter 1.
Introduction
(a) R is reﬂexive: For every element in a ∈A, we have (a, a) ∈R.
(b) R is symmetric: For all a and b in A, if (a, b) ∈R, then also
(b, a) ∈R.
(c) R is transitive: For all a, b, and c in A, if (a, b) ∈R and (b, c) ∈R,
then also (a, c) ∈R.
13. A graph G = (V, E) is a pair consisting of a set V , whose elements are
called vertices, and a set E, where each element of E is a pair of distinct
vertices. The elements of E are called edges. The ﬁgure below shows
some well-known graphs: K5 (the complete graph on ﬁve vertices), K3,3
(the complete bipartite graph on 2 × 3 = 6 vertices), and the Peterson
graph.
K5
K3,3
Peterson graph
The degree of a vertex v, denoted by deg(v), is deﬁned to be the number
of edges that are incident on v.
A path in a graph is a sequence of vertices that are connected by edges.
A path is a cycle, if it starts and ends at the same vertex. A simple
path is a path without any repeated vertices. A graph is connected, if
there is a path between every pair of vertices.
14. In the context of strings, an alphabet is a ﬁnite set, whose elements
are called symbols. Examples of alphabets are Σ = {0, 1} and Σ =
{a, b, c, . . . , z}.
15. A string over an alphabet Σ is a ﬁnite sequence of symbols, where each
symbol is an element of Σ. The length of a string w, denoted by |w|, is
the number of symbols contained in w. The empty string, denoted by

1.3.
Proof techniques
7
ǫ, is the string having length zero. For example, if the alphabet Σ is
equal to {0, 1}, then 10, 1000, 0, 101, and ǫ are strings over Σ, having
lengths 2, 4, 1, 3, and 0, respectively.
16. A language is a set of strings.
17. The Boolean values are 1 and 0, that represent true and false, respec-
tively. The basic Boolean operations include
(a) negation (or NOT), represented by ¬,
(b) conjunction (or AND), represented by ∧,
(c) disjunction (or OR), represented by ∨,
(d) exclusive-or (or XOR), represented by ⊕,
(e) equivalence, represented by ↔or ⇔,
(f) implication, represented by →or ⇒.
The following table explains the meanings of these operations.
NOT
AND
OR
XOR
equivalence
implication
¬0 = 1
0 ∧0 = 0
0 ∨0 = 0
0 ⊕0 = 0
0 ↔0 = 1
0 →0 = 1
¬1 = 0
0 ∧1 = 0
0 ∨1 = 1
0 ⊕1 = 1
0 ↔1 = 0
0 →1 = 1
1 ∧0 = 0
1 ∨0 = 1
1 ⊕0 = 1
1 ↔0 = 0
1 →0 = 0
1 ∧1 = 1
1 ∨1 = 1
1 ⊕1 = 0
1 ↔1 = 1
1 →1 = 1
1.3
Proof techniques
In mathematics, a theorem is a statement that is true. A proof is a sequence
of mathematical statements that form an argument to show that a theorem is
true. The statements in the proof of a theorem include axioms (assumptions
about the underlying mathematical structures), hypotheses of the theorem
to be proved, and previously proved theorems. The main question is “How
do we go about proving theorems?” This question is similar to the question
of how to solve a given problem. Of course, the answer is that ﬁnding proofs,
or solving problems, is not easy; otherwise life would be dull! There is no
speciﬁed way of coming up with a proof, but there are some generic strategies
that could be of help. In this section, we review some of these strategies,
that will be suﬃcient for this course. The best way to get a feeling of how
to come up with a proof is by solving a large number of problems. Here are

8
Chapter 1.
Introduction
some useful tips. (You may take a look at the book How to Solve It, by G.
P´olya).
1. Read and completely understand the statement of the theorem to be
proved. Most often this is the hardest part.
2. Sometimes, theorems contain theorems inside them.
For example,
“Property A if and only if property B”, requires showing two state-
ments:
(a) If property A is true, then property B is true (A ⇒B).
(b) If property B is true, then property A is true (B ⇒A).
Another example is the theorem “Set A equals set B.” To prove this,
we need to prove that A ⊆B and B ⊆A. That is, we need to show
that each element of set A is in set B, and that each element of set B
is in set A.
3. Try to work out a few simple cases of the theorem just to get a grip on
it (i.e., crack a few simple cases ﬁrst).
4. Try to write down the proof once you have it. This is to ensure the
correctness of your proof. Often, mistakes are found at the time of
writing.
5. Finding proofs takes time, we do not come prewired to produce proofs.
Be patient, think, express and write clearly and try to be precise as
much as possible.
In the next sections, we will go through some of the proof strategies.
1.3.1
Direct proofs
As the name suggests, in a direct proof of a theorem, we just approach the
theorem directly.
Theorem 1.3.1 If n is an odd positive integer, then n2 is odd as well.

1.3.
Proof techniques
9
Proof. An odd positive integer n can be written as n = 2k + 1, for some
integer k ≥0. Then
n2 = (2k + 1)2 = 4k2 + 4k + 1 = 2(2k2 + 2k) + 1.
Since 2(2k2 + 2k) is even, and “even plus one is odd”, we can conclude that
n2 is odd.
Theorem 1.3.2 Let G = (V, E) be a graph. Then the sum of the degrees of
all vertices is an even integer, i.e.,
X
v∈V
deg(v)
is even.
Proof. If you do not see the meaning of this statement, then ﬁrst try it out
for a few graphs. The reason why the statement holds is very simple: Each
edge contributes 2 to the summation (because an edge is incident on exactly
two distinct vertices).
Actually, the proof above proves the following theorem.
Theorem 1.3.3 Let G = (V, E) be a graph. Then the sum of the degrees of
all vertices is equal to twice the number of edges, i.e.,
X
v∈V
deg(v) = 2|E|.
1.3.2
Constructive proofs
This technique not only shows the existence of a certain object, it actually
gives a method of creating it. Here is how a constructive proof looks like:
Theorem 1.3.4 There exists an object with property P.
Proof. Here is the object: [. . .]
And here is the proof that the object satisﬁes property P: [. . .]
Here is an example of a constructive proof. A graph is called 3-regular, if
each vertex has degree three.

10
Chapter 1.
Introduction
Theorem 1.3.5 For every even integer n ≥4, there exists a 3-regular graph
with n vertices.
Proof. Deﬁne
V = {0, 1, 2, . . ., n −1},
and
E = {{i, i+1} : 0 ≤i ≤n−2}∪{{n−1, 0}}∪{{i, i+n/2} : 0 ≤i ≤n/2−1}.
Then the graph G = (V, E) is 3-regular.
Convince yourself that this graph is indeed 3-regular. It may help to draw
the graph for, say, n = 8.
1.3.3
Nonconstructive proofs
In a nonconstructive proof, we show that a certain object exists, without
actually creating it. Here is an example of such a proof:
Theorem 1.3.6 There exist irrational numbers x and y such that xy is ra-
tional.
Proof. There are two possible cases.
Case 1:
√
2
√
2 ∈Q.
In this case, we take x = y =
√
2. In Theorem 1.3.9 below, we will prove
that
√
2 is irrational.
Case 2:
√
2
√
2 ̸∈Q.
In this case, we take x =
√
2
√
2 and y =
√
2. Since
xy =
√
2
√
2√
2
=
√
2
2 = 2,
the claim in the theorem follows.
Observe that this proof indeed proves the theorem, but it does not give
an example of a pair of irrational numbers x and y such that xy is rational.

1.3.
Proof techniques
11
1.3.4
Proofs by contradiction
This is how a proof by contradiction looks like:
Theorem 1.3.7 Statement S is true.
Proof. Assume that statement S is false. Then, derive a contradiction (such
as 1 + 1 = 3).
In other words, show that the statement “¬S ⇒false” is true. This is
suﬃcient, because the contrapositive of the statement “¬S ⇒false” is the
statement “true ⇒S”. The latter logical formula is equivalent to S, and
that is what we wanted to show.
Below, we give two examples of proofs by contradiction.
Theorem 1.3.8 Let n be a positive integer. If n2 is even, then n is even.
Proof. We will prove the theorem by contradiction. So we assume that n2
is even, but n is odd. Since n is odd, we know from Theorem 1.3.1 that n2
is odd. This is a contradiction, because we assumed that n2 is even.
Theorem 1.3.9
√
2 is irrational, i.e.,
√
2 cannot be written as a fraction of
two integers m and n.
Proof. We will prove the theorem by contradiction. So we assume that
√
2
is rational. Then
√
2 can be written as a fraction of two integers,
√
2 = m/n,
where m ≥1 and n ≥1.
We may assume that m and n are not both
even, because otherwise we can get rid of the common factors. By squaring
√
2 = m/n, we get 2n2 = m2.
This implies that m2 is even.
Then, by
Theorem 1.3.8, m is even, which means that we can write m as m = 2k, for
some positive integer k. It follows that 2n2 = m2 = 4k2, which implies that
n2 = 2k2. Hence, n2 is even. Again by Theorem 1.3.8, it follows that n is
even.
We have shown that m and n are both even. But we know that m and
n are not both even. Hence, we have a contradiction. Our assumption that
√
2 is rational is wrong. Thus, we can conclude that
√
2 is irrational.
There is a nice discussion of this proof in the book My Brain is Open:
The Mathematical Journeys of Paul Erd˝os by B. Schechter.

12
Chapter 1.
Introduction
1.3.5
The pigeon hole principle
This is a simple principle with surprising consequences.
Pigeon Hole Principle: If n + 1 or more objects are placed into n
boxes, then there is at least one box containing two or more objects.
In other words, if A and B are two sets such that |A| > |B|, then
there is no one-to-one function from A to B.
Theorem 1.3.10 Let n be a positive integer. Every sequence of n2 + 1 dis-
tinct real numbers contains a subsequence of length n + 1 that is either in-
creasing or decreasing.
Proof. For example consider the sequence (20, 10, 9, 7, 11, 2, 21, 1, 20, 31) of
10 = 32 + 1 numbers. This sequence contains an increasing subsequence of
length 4 = 3 + 1, namely (10, 11, 21, 31).
The proof of this theorem is by contradiction, and uses the pigeon hole
principle.
Let (a1, a2, . . . , an2+1) be an arbitrary sequence of n2 + 1 distinct real
numbers.
For each i with 1 ≤i ≤n2 + 1, let inci denote the length of
the longest increasing subsequence that starts at ai, and let deci denote the
length of the longest decreasing subsequence that starts at ai.
Using this notation, the claim in the theorem can be formulated as follows:
There is an index i such that inci ≥n + 1 or deci ≥n + 1.
We will prove the claim by contradiction. So we assume that inci ≤n
and deci ≤n for all i with 1 ≤i ≤n2 + 1.
Consider the set
B = {(b, c) : 1 ≤b ≤n, 1 ≤c ≤n},
and think of the elements of B as being boxes. For each i with 1 ≤i ≤n2+1,
the pair (inci, deci) is an element of B. So we have n2+1 elements (inci, deci),
which are placed in the n2 boxes of B. By the pigeon hole principle, there
must be a box that contains two (or more) elements. In other words, there
exist two integers i and j such that i < j and
(inci, deci) = (incj, decj).
Recall that the elements in the sequence are distinct. Hence, ai ̸= aj. We
consider two cases.

1.3.
Proof techniques
13
First assume that ai < aj. Then the length of the longest increasing
subsequence starting at ai must be at least 1+incj, because we can append ai
to the longest increasing subsequence starting at aj. Therefore, inci ̸= incj,
which is a contradiction.
The second case is when ai > aj. Then the length of the longest decreasing
subsequence starting at ai must be at least 1+decj, because we can append ai
to the longest decreasing subsequence starting at aj. Therefore, deci ̸= decj,
which is again a contradiction.
1.3.6
Proofs by induction
This is a very powerful and important technique for proving theorems.
For each positive integer n, let P(n) be a mathematical statement that
depends on n. Assume we wish to prove that P(n) is true for all positive
integers n. A proof by induction of such a statement is carried out as follows:
Basis: Prove that P(1) is true.
Induction step: Prove that for all n ≥1, the following holds: If P(n) is
true, then P(n + 1) is also true.
In the induction step, we choose an arbitrary integer n ≥1 and assume
that P(n) is true; this is called the induction hypothesis. Then we prove that
P(n + 1) is also true.
Theorem 1.3.11 For all positive integers n, we have
1 + 2 + 3 + . . . + n = n(n + 1)
2
.
Proof. We start with the basis of the induction. If n = 1, then the left-hand
side is equal to 1, and so is the right-hand side. So the theorem is true for
n = 1.
For the induction step, let n ≥1 and assume that the theorem is true for
n, i.e., assume that
1 + 2 + 3 + . . . + n = n(n + 1)
2
.

14
Chapter 1.
Introduction
We have to prove that the theorem is true for n + 1, i.e., we have to prove
that
1 + 2 + 3 + . . . + (n + 1) = (n + 1)(n + 2)
2
.
Here is the proof:
1 + 2 + 3 + . . . + (n + 1)
=
1 + 2 + 3 + . . . + n
|
{z
}
= n(n+1)
2
+(n + 1)
=
n(n + 1)
2
+ (n + 1)
=
(n + 1)(n + 2)
2
.
By the way, here is an alternative proof of the theorem above: Let S =
1 + 2 + 3 + . . . + n. Then,
S
=
1
+
2
+
3
+
. . .
+
(n −2)
+
(n −1)
+
n
S
=
n
+
(n −1)
+
(n −2)
+
. . .
+
3
+
2
+
1
2S
=
(n + 1)
+
(n + 1)
+
(n + 1)
+
. . .
+
(n + 1)
+
(n + 1)
+
(n + 1)
Since there are n terms on the right-hand side, we have 2S = n(n + 1). This
implies that S = n(n + 1)/2.
Theorem 1.3.12 For every positive integer n, a −b is a factor of an −bn.
Proof. A direct proof can be given by providing a factorization of an −bn:
an −bn = (a −b)(an−1 + an−2b + an−3b2 + . . . + abn−2 + bn−1).
We now prove the theorem by induction. For the basis, let n = 1. The claim
in the theorem is “a −b is a factor of a −b”, which is obviously true.
Let n ≥1 and assume that a −b is a factor of an −bn. We have to prove
that a −b is a factor of an+1 −bn+1. We have
an+1 −bn+1 = an+1 −anb + anb −bn+1 = an(a −b) + (an −bn)b.
The ﬁrst term on the right-hand side is divisible by a −b. By the induction
hypothesis, the second term on the right-hand side is divisible by a −b as
well. Therefore, the entire right-hand side is divisible by a −b. Since the
right-hand side is equal to an+1 −bn+1, it follows that a −b is a factor of
an+1 −bn+1.
We now give an alternative proof of Theorem 1.3.3:

1.3.
Proof techniques
15
Theorem 1.3.13 Let G = (V, E) be a graph with m edges. Then the sum
of the degrees of all vertices is equal to twice the number of edges, i.e.,
X
v∈V
deg(v) = 2m.
Proof. The proof is by induction on the number m of edges. For the basis of
the induction, assume that m = 0. Then the graph G does not contain any
edges and, therefore, P
v∈V deg(v) = 0. Thus, the theorem is true if m = 0.
Let m ≥0 and assume that the theorem is true for every graph with m
edges. Let G be an arbitrary graph with m+1 edges. We have to prove that
P
v∈V deg(v) = 2(m + 1).
Let {a, b} be an arbitrary edge in G, and let G′ be the graph obtained
from G by removing the edge {a, b}. Since G′ has m edges, we know from
the induction hypothesis that the sum of the degrees of all vertices in G′ is
equal to 2m. Using this, we obtain
X
v∈G
deg(v) =
X
v∈G′
deg(v) + 2 = 2m + 2 = 2(m + 1).
1.3.7
More examples of proofs
Recall Theorem 1.3.5, which states that for every even integer n ≥4, there
exists a 3-regular graph with n vertices. The following theorem explains why
we stated this theorem for even values of n.
Theorem 1.3.14 Let n ≥5 be an odd integer. There is no 3-regular graph
with n vertices.
Proof. The proof is by contradiction. So we assume that there exists a
graph G = (V, E) with n vertices that is 3-regular. Let m be the number of
edges in G. Since deg(v) = 3 for every vertex, we have
X
v∈V
deg(v) = 3n.
On the other hand, by Theorem 1.3.3, we have
X
v∈V
deg(v) = 2m.

16
Chapter 1.
Introduction
It follows that 3n = 2m, which can be rewritten as m = 3n/2. Since m is an
integer, and since gcd(2, 3) = 1, n/2 must be an integer. Hence, n is even,
which is a contradiction.
Let Kn be the complete graph on n vertices. This graph has a vertex set
of size n, and every pair of distinct vertices is joined by an edge.
If G = (V, E) is a graph with n vertices, then the complement G of G is
the graph with vertex set V that consists of those edges of Kn that are not
present in G.
Theorem 1.3.15 Let n ≥2 and let G be a graph on n vertices. Then G is
connected or G is connected.
Proof. We prove the theorem by induction on the number n of vertices. For
the basis, assume that n = 2. There are two possibilities for the graph G:
1. G contains one edge. In this case, G is connected.
2. G does not contain an edge. In this case, the complement G contains
one edge and, therefore, G is connected.
So for n = 2, the theorem is true.
Let n ≥2 and assume that the theorem is true for every graph with n
vertices. Let G be graph with n + 1 vertices. We have to prove that G is
connected or G is connected. We consider three cases.
Case 1: There is a vertex v whose degree in G is equal to n.
Since G has n+1 vertices, v is connected by an edge to every other vertex
of G. Therefore, G is connected.
Case 2: There is a vertex v whose degree in G is equal to 0.
In this case, the degree of v in the graph G is equal to n. Since G has n+1
vertices, v is connected by an edge to every other vertex of G. Therefore, G
is connected.
Case 3: For every vertex v, the degree of v in G is in {1, 2, . . . , n −1}.
Let v be an arbitrary vertex of G.
Let G′ be the graph obtained by
deleting from G the vertex v, together with all edges that are incident on v.
Since G′ has n vertices, we know from the induction hypothesis that G′ is
connected or G′ is connected.

1.3.
Proof techniques
17
Let us ﬁrst assume that G′ is connected. Then the graph G is connected
as well, because there is at least one edge in G between v and some vertex
of G′.
If G′ is not connected, then G′ must be connected. Since we are in Case 3,
we know that the degree of v in G is in the set {1, 2, . . . , n −1}. It follows
that the degree of v in the graph G is in this set as well. Hence, there is at
least one edge in G between v and some vertex in G′. This implies that G is
connected.
The previous theorem can be rephrased as follows:
Theorem 1.3.16 Let n ≥2 and consider the complete graph Kn on n ver-
tices. Color each edge of this graph as either red or blue. Let R be the graph
consisting of all the red edges, and let B be the graph consisting of all the
blue edges. Then R is connected or B is connected.
A graph is said to be planar, if it can be drawn (a better term is “embed-
ded”) in the plane in such a way that no two edges intersect, except possibly
at their endpoints.
An embedding of a planar graph consists of vertices,
edges, and faces. In the example below, there are 11 vertices, 18 edges, and
9 faces (including the unbounded face).
The following theorem is known as Euler’s theorem for planar graphs.
Apparently, this theorem was discovered by Euler around 1750. Legendre
gave the ﬁrst proof in 1794, see
http://www.ics.uci.edu/~eppstein/junkyard/euler/
Theorem 1.3.17 (Euler) Consider an embedding of a planar graph G. Let
v, e, and f be the number of vertices, edges, and faces (including the single

18
Chapter 1.
Introduction
unbounded face) of this embedding, respectively. Moreover, let c be the number
of connected components of G. Then
v −e + f = c + 1.
Proof. The proof is by induction on the number of edges of G. To be more
precise, we start with a graph having no edges, and prove that the theorem
holds for this case. Then, we add the edges one by one, and show that the
relation v −e + f = c + 1 is maintained.
So we ﬁrst assume that G has no edges, i.e., e = 0. Then the embedding
consists of a collection of v points. In this case, we have f = 1 and c = v.
Hence, the relation v −e + f = c + 1 holds.
Let e > 0 and assume that Euler’s formula holds for a subgraph of G
having e −1 edges. Let {u, v} be an edge of G that is not in the subgraph,
and add this edge to the subgraph. There are two cases depending on whether
this new edge joins two connected components or joins two vertices in the
same connected component.
Case 1: The new edge {u, v} joins two connected components.
In this case, the number of vertices and the number of faces do not change,
the number of connected components goes down by 1, and the number of
edges increases by 1. It follows that the relation in the theorem is still valid.
Case 2: The new edge {u, v} joins two vertices in the same connected com-
ponent.
In this case, the number of vertices and the number of connected com-
ponents do not change, the number of edges increases by 1, and the number
of faces increases by 1 (because the new edge splits one face into two faces).
Therefore, the relation in the theorem is still valid.
Euler’s theorem is usually stated as follows:
Theorem 1.3.18 (Euler) Consider an embedding of a connected planar
graph G. Let v, e, and f be the number of vertices, edges, and faces (in-
cluding the single unbounded face) of this embedding, respectively. Then
v −e + f = 2.
If you like surprising proofs of various mathematical results, you should
read the book Proofs from THE BOOK by Aigner and Ziegler.

Exercises
19
Exercises
1.1 Use induction to prove that every positive integer can be written as a
product of prime numbers.
1.2 For every prime number p, prove that √p is irrational.
1.3 Let n be a positive integer that is not a perfect square. Prove that √n
is irrational.
1.4 Prove by induction that n4 −4n2 is divisible by 3, for all integers n ≥1.
1.5 Prove that
n
X
i=1
1
i2 < 2 −1/n,
for every integer n ≥2.
1.6 Prove that 9 divides n3 + (n + 1)3 + (n + 2)3, for every integer n ≥0.
1.7 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are
always two numbers that are consecutive.
1.8 Prove that in any set of n + 1 numbers from {1, 2, . . . , 2n}, there are
always two numbers such that one divides the other.

20
Chapter 1.
Introduction

Chapter 2
Finite Automata and Regular
Languages
In this chapter, we introduce and analyze the class of languages that are
known as regular languages. Informally, these languages can be “processed”
by computers having a very small amount of memory.
2.1
An example: Controling a toll gate
Before we give a formal deﬁnition of a ﬁnite automaton, we consider an
example in which such an automaton shows up in a natural way. We consider
the problem of designing a “computer” that controls a toll gate.
When a car arrives at the toll gate, the gate is closed. The gate opens as
soon as the driver has payed 25 cents. We assume that we have only three
coin denominations: 5, 10, and 25 cents. We also assume that no excess
change is returned.
After having arrived at the toll gate, the driver inserts a sequence of coins
into the machine. At any moment, the machine has to decide whether or not
to open the gate, i.e., whether or not the driver has paid 25 cents (or more).
In order to decide this, the machine is in one of the following six states, at
any moment during the process:
• The machine is in state q0, if it has not collected any money yet.
• The machine is in state q1, if it has collected exactly 5 cents.
• The machine is in state q2, if it has collected exactly 10 cents.

22
Chapter 2.
Finite Automata and Regular Languages
• The machine is in state q3, if it has collected exactly 15 cents.
• The machine is in state q4, if it has collected exactly 20 cents.
• The machine is in state q5, if it has collected 25 cents or more.
Initially (when a car arrives at the toll gate), the machine is in state q0.
Assume, for example, that the driver presents the sequence (10,5,5,10) of
coins.
• After receiving the ﬁrst 10 cents coin, the machine switches from state
q0 to state q2.
• After receiving the ﬁrst 5 cents coin, the machine switches from state
q2 to state q3.
• After receiving the second 5 cents coin, the machine switches from state
q3 to state q4.
• After receiving the second 10 cents coin, the machine switches from
state q4 to state q5. At this moment, the gate opens. (Remember that
no change is given.)
The ﬁgure below represents the behavior of the machine for all possible
sequences of coins. State q5 is represented by two circles, because it is a
special state: As soon as the machine reaches this state, the gate opens.
q0
q1
q2
q3
q4
q5
5
5
5
5
10
10
10
25
25
25
10, 25
5, 10, 25
5, 10
25
start
Observe that the machine (or computer) only has to remember which
state it is in at any given time. Thus, it needs only a very small amount
of memory: It has to be able to distinguish between any one of six possible
cases and, therefore, it only needs a memory of ⌈log 6⌉= 3 bits.

2.2.
Deterministic ﬁnite automata
23
2.2
Deterministic ﬁnite automata
Let us look at another example. Consider the following state diagram:
q1
q2
q3
0
0
1
1
0,1
We say that q1 is the start state and q2 is an accept state. Consider the
input string 1101. This string is processed in the following way:
• Initially, the machine is in the start state q1.
• After having read the ﬁrst 1, the machine switches from state q1 to
state q2.
• After having read the second 1, the machine switches from state q2 to
state q2. (So actually, it does not switch.)
• After having read the ﬁrst 0, the machine switches from state q2 to
state q3.
• After having read the third 1, the machine switches from state q3 to
state q2.
After the entire string 1101 has been processed, the machine is in state q2,
which is an accept state. We say that the string 1101 is accepted by the
machine.
Consider now the input string 0101010. After having read this string
from left to right (starting in the start state q1), the machine is in state q3.
Since q3 is not an accept state, we say that the machine rejects the string
0101010.
We hope you are able to see that this machine accepts every binary string
that ends with a 1. In fact, the machine accepts more strings:
• Every binary string having the property that there are an even number
of 0s following the rightmost 1, is accepted by this machine.

24
Chapter 2.
Finite Automata and Regular Languages
• Every other binary string is rejected by the machine. Observe that each
such string is either empty, consists of 0s only, or has an odd number
of 0s following the rightmost 1.
We now come to the formal deﬁnition of a ﬁnite automaton:
Deﬁnition 2.2.1 A ﬁnite automaton is a 5-tuple M = (Q, Σ, δ, q, F), where
1. Q is a ﬁnite set, whose elements are called states,
2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,
3. δ : Q × Σ →Q is a function, called the transition function,
4. q is an element of Q; it is called the start state,
5. F is a subset of Q; the elements of F are called accept states.
You can think of the transition function δ as being the “program” of the
ﬁnite automaton M = (Q, Σ, δ, q, F). This function tells us what M can do
in “one step”:
• Let r be a state of Q and let a be a symbol of the alphabet Σ. If
the ﬁnite automaton M is in state r and reads the symbol a, then it
switches from state r to state δ(r, a). (In fact, δ(r, a) may be equal to
r.)
The “computer” that we designed in the toll gate example in Section 2.1
is a ﬁnite automaton. For this example, we have Q = {q0, q1, q2, q3, q4, q5},
Σ = {5, 10, 25}, the start state is q0, F = {q5}, and δ is given by the following
table:
5
10
25
q0
q1
q2
q5
q1
q2
q3
q5
q2
q3
q4
q5
q3
q4
q5
q5
q4
q5
q5
q5
q5
q5
q5
q5
The example given in the beginning of this section is also a ﬁnite automa-
ton. For this example, we have Q = {q1, q2, q3}, Σ = {0, 1}, the start state
is q1, F = {q2}, and δ is given by the following table:

2.2.
Deterministic ﬁnite automata
25
0
1
q1
q1
q2
q2
q3
q2
q3
q2
q2
Let us denote this ﬁnite automaton by M. The language of M, denoted
by L(M), is the set of all binary strings that are accepted by M. As we have
seen before, we have
L(M) = {w : w contains at least one 1 and ends with an even number of 0s}.
We now give a formal deﬁnition of the language of a ﬁnite automaton:
Deﬁnition 2.2.2 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton and let w =
w1w2 . . . wn be a string over Σ. Deﬁne the sequence r0, r1, . . . , rn of states, in
the following way:
• r0 = q,
• ri+1 = δ(ri, wi+1), for i = 0, 1, . . . , n −1.
1. If rn ∈F, then we say that M accepts w.
2. If rn ̸∈F, then we say that M rejects w.
In this deﬁnition, w may be the empty string, which we denote by ǫ, and
whose length is zero; thus in the deﬁnition above, n = 0. In this case, the
sequence r0, r1, . . . , rn of states has length one; it consists of just the state
r0 = q. The empty string is accepted by M if and only if the start state q
belongs to F.
Deﬁnition 2.2.3 Let M = (Q, Σ, δ, q, F) be a ﬁnite automaton. The lan-
guage L(M) accepted by M is deﬁned to be the set of all strings that are
accepted by M:
L(M) = {w : w is a string over Σ and M accepts w }.
Deﬁnition 2.2.4 A language A is called regular, if there exists a ﬁnite au-
tomaton M such that A = L(M).

26
Chapter 2.
Finite Automata and Regular Languages
We ﬁnish this section by presenting an equivalent way of deﬁning the
language accepted by a ﬁnite automaton. Let M = (Q, Σ, δ, q, F) be a ﬁnite
automaton. The transition function δ : Q × Σ →Q tells us that, when M
is in state r ∈Q and reads symbol a ∈Σ, it switches from state r to state
δ(r, a). Let Σ∗denote the set of all strings over the alphabet Σ. (Σ∗includes
the empty string ǫ.) We extend the function δ to a function
δ : Q × Σ∗→Q,
that is deﬁned as follows. For any state r ∈Q and for any string w over the
alphabet Σ,
δ(r, w) =
 r
if w = ǫ,
δ(δ(r, v), a)
if w = va, where v is a string and a ∈Σ.
What is the meaning of this function δ? Let r be a state of Q and let w be
a string over the alphabet Σ. Then
• δ(r, w) is the state that M reaches, when it starts in state r, reads the
string w from left to right, and uses δ to switch from state to state.
Using this notation, we have
L(M) = {w : w is a string over Σ and δ(q, w) ∈F}.
2.2.1
A ﬁrst example of a ﬁnite automaton
Let
A = {w : w is a binary string containing an odd number of 1s}.
We claim that this language A is regular. In order to prove this, we have to
construct a ﬁnite automaton M such that A = L(M).
How to construct M? Here is a ﬁrst idea: The ﬁnite automaton reads the
input string w from left to right and keeps track of the number of 1s it has
seen. After having read the entire string w, it checks whether this number
is odd (in which case w is accepted) or even (in which case w is rejected).
Using this approach, the ﬁnite automaton needs a state for every integer
i ≥0, indicating that the number of 1s read so far is equal to i. Hence,
to design a ﬁnite automaton that follows this approach, we need an inﬁnite

2.2.
Deterministic ﬁnite automata
27
number of states. But, the deﬁnition of ﬁnite automaton requires the number
of states to be ﬁnite.
A better, and correct approach, is to keep track of whether the number
of 1s read so far is even or odd. This leads to the following ﬁnite automaton:
• The set of states is Q = {qe, qo}. If the ﬁnite automaton is in state qe,
then it has read an even number of 1s; if it is in state qo, then it has
read an odd number of 1s.
• The alphabet is Σ = {0, 1}.
• The start state is qe, because at the start, the number of 1s read by the
automaton is equal to 0, and 0 is even.
• The set F of accept states is F = {qo}.
• The transition function δ is given by the following table:
0
1
qe
qe
qo
qo
qo
qe
This ﬁnite automaton M = (Q, Σ, δ, qe, F) can also be described by its state
diagram, which is given in the ﬁgure below. The arrow that comes “out of
the blue” and enters the state qe, indicates that qe is the start state. The
state depicted with double circles indicates the accept state.
qe
qo
0
0
1
1
We have constructed a ﬁnite automaton M that accepts the language A.
Therefore, A is a regular language.

28
Chapter 2.
Finite Automata and Regular Languages
2.2.2
A second example of a ﬁnite automaton
Deﬁne the language A as
A = {w : w is a binary string containing 101 as a substring}.
Again, we claim that A is a regular language. In other words, we claim that
there exists a ﬁnite automaton M that accepts A, i.e., A = L(M).
The ﬁnite automaton M will do the following, when reading an input
string from left to right:
• It skips over all 0s, and stays in the start state.
• At the ﬁrst 1, it switches to the state “maybe the next two symbols are
01”.
– If the next symbol is 1, then it stays in the state “maybe the next
two symbols are 01”.
– On the other hand, if the next symbol is 0, then it switches to the
state “maybe the next symbol is 1”.
∗If the next symbol is indeed 1, then it switches to the accept
state (but keeps on reading until the end of the string).
∗On the other hand, if the next symbol is 0, then it switches
to the start state, and skips 0s until it reads 1 again.
By deﬁning the following four states, this process will become clear:
• q1: M is in this state if the last symbol read was 1, but the substring
101 has not been read.
• q10: M is in this state if the last two symbols read were 10, but the
substring 101 has not been read.
• q101: M is in this state if the substring 101 has been read in the input
string.
• q: In all other cases, M is in this state.
Here is the formal description of the ﬁnite automaton that accepts the
language A:
• Q = {q, q1, q10, q101},

2.2.
Deterministic ﬁnite automata
29
• Σ = {0, 1},
• the start state is q,
• the set F of accept states is equal to F = {q101}, and
• the transition function δ is given by the following table:
0
1
q
q
q1
q1
q10
q1
q10
q
q101
q101
q101
q101
The ﬁgure below gives the state diagram of the ﬁnite automaton M =
(Q, Σ, δ, q, F).
q
q1
q10
q101
0
1
1
0
0
1
0,1
This ﬁnite automaton accepts the language A consisting of all binary
strings that contain the substring 101. As an exercise, how would you obtain
a ﬁnite automaton that accepts the complement of A, i.e., the language
consisting of all binary strings that do not contain the substring 101?
2.2.3
A third example of a ﬁnite automaton
The ﬁnite automata we have seen so far have exactly one accept state. In
this section, we will see an example of a ﬁnite automaton having more accept
states.

30
Chapter 2.
Finite Automata and Regular Languages
Let A be the language
A = {w ∈{0, 1}∗: w has a 1 in the third position from the right},
where {0, 1}∗is the set of all binary strings, including the empty string ǫ. We
claim that A is a regular language. To prove this, we have to construct a ﬁnite
automaton M such that A = L(M). At ﬁrst sight, it seems diﬃcult (or even
impossible?) to construct such a ﬁnite automaton: How does the automaton
“know” that it has reached the third symbol from the right? It is, however,
possible to construct such an automaton. The main idea is to remember the
last three symbols that have been read. Thus, the ﬁnite automaton has eight
states qijk, where i, j, and k range over the two elements of {0, 1}. If the
automaton is in state qijk, then the following hold:
• If M has read at least three symbols, then the three most recently read
symbols are ijk.
• If M has read only two symbols, then these two symbols are jk; more-
over, i = 0.
• If M has read only one symbol, then this symbol is k; moreover, i =
j = 0.
• If M has not read any symbol, then i = j = k = 0.
The start state is q000 and the set of accept states is {q100, q110, q101, q111}.
The transition function of M is given by the following state diagram.
q000
q100
q010
q110
q001
q101
q011
q111
0
1
0
1
0
1
0
1
0
1
1
0
1
0
1
0

2.3.
Regular operations
31
2.3
Regular operations
In this section, we deﬁne three operations on languages. Later, we will answer
the question whether the set of all regular languages is closed under these
operations. Let A and B be two languages over the same alphabet.
1. The union of A and B is deﬁned as
A ∪B = {w : w ∈A or w ∈B}.
2. The concatenation of A and B is deﬁned as
AB = {ww′ : w ∈A and w′ ∈B}.
In words, AB is the set of all strings obtained by taking an arbitrary
string w in A and an arbitrary string w′ in B, and gluing them together
(such that w is to the left of w′).
3. The star of A is deﬁned as
A∗= {u1u2 . . . uk : k ≥0 and ui ∈A for all i = 1, 2, . . . , k}.
In words, A∗is obtained by taking any ﬁnite number of strings in A, and
gluing them together. Observe that k = 0 is allowed; this corresponds
to the empty string ǫ. Thus, ǫ ∈A∗.
To give an example, let A = {0, 01} and B = {1, 10}. Then
A ∪B = {0, 01, 1, 10},
AB = {01, 010, 011, 0110},
and
A∗= {ǫ, 0, 01, 00, 001, 010, 0101, 000, 0001, 00101, . . .}.
As another example, if Σ = {0, 1}, then Σ∗is the set of all binary strings
(including the empty string). Observe that a string always has a ﬁnite length.
Before we proceed, we give an alternative (and equivalent) deﬁnition of
the star of the language A: Deﬁne
A0 = {ǫ}

32
Chapter 2.
Finite Automata and Regular Languages
and, for k ≥1,
Ak = AAk−1,
i.e., Ak is the concatenation of the two languages A and Ak−1. Then we have
A∗=
∞
[
k=0
Ak.
Theorem 2.3.1 The set of regular languages is closed under the union op-
eration, i.e., if A and B are regular languages over the same alphabet Σ, then
A ∪B is also a regular language.
Proof.
Since A and B are regular languages, there are ﬁnite automata
M1 = (Q1, Σ, δ1, q1, F1) and M2 = (Q2, Σ, δ2, q2, F2) that accept A and B,
respectively. In order to prove that A ∪B is regular, we have to construct a
ﬁnite automaton M that accepts A ∪B. In other words, M must have the
property that for every string w ∈Σ∗,
M accepts w ⇔M1 accepts w or M2 accepts w.
As a ﬁrst idea, we may think that M could do the following:
• Starting in the start state q1 of M1, M “runs” M1 on w.
• If, after having read w, M1 is in a state of F1, then w ∈A, thus
w ∈A ∪B and, therefore, M accepts w.
• On the other hand, if, after having read w, M1 is in a state that is not
in F1, then w ̸∈A and M “runs” M2 on w, starting in the start state
q2 of M2. If, after having read w, M2 is in a state of F2, then we know
that w ∈B, thus w ∈A ∪B and, therefore, M accepts w. Otherwise,
we know that w ̸∈A ∪B, and M rejects w.
This idea does not work, because the ﬁnite automaton M can read the input
string w only once. The correct approach is to run M1 and M2 simulta-
neously. We deﬁne the set Q of states of M to be the Cartesian product
Q1 × Q2. If M is in state (r1, r2), this means that
• if M1 would have read the input string up to this point, then it would
be in state r1, and

2.3.
Regular operations
33
• if M2 would have read the input string up to this point, then it would
be in state r2.
This leads to the ﬁnite automaton M = (Q, Σ, δ, q, F), where
• Q = Q1 × Q2 = {(r1, r2) : r1 ∈Q1 and r2 ∈Q2}.
Observe that
|Q| = |Q1| × |Q2|, which is ﬁnite.
• Σ is the alphabet of A and B (recall that we assume that A and B are
languages over the same alphabet).
• The start state q of M is equal to q = (q1, q2).
• The set F of accept states of M is given by
F = {(r1, r2) : r1 ∈F1 or r2 ∈F2} = (F1 × Q2) ∪(Q1 × F2).
• The transition function δ : Q × Σ →Q is given by
δ((r1, r2), a) = (δ1(r1, a), δ2(r2, a)),
for all r1 ∈Q1, r2 ∈Q2, and a ∈Σ.
To ﬁnish the proof, we have to show that this ﬁnite automaton M indeed
accepts the language A∪B. Intuitively, this should be clear from the discus-
sion above. The easiest way to give a formal proof is by using the extended
transition functions δ1 and δ2. (The extended transition function has been
deﬁned after Deﬁnition 2.2.4.) Here we go: Recall that we have to prove that
M accepts w ⇔M1 accepts w or M2 accepts w,
i.e,
M accepts w ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.
In terms of the extended transition function δ of the transition function δ of
M, this becomes
δ((q1, q2), w) ∈F ⇔δ1(q1, w) ∈F1 or δ2(q2, w) ∈F2.
(2.1)
By applying the deﬁnition of the extended transition function, as given after
Deﬁnition 2.2.4, to δ, it can be seen that
δ((q1, q2), w) = (δ1(q1, w), δ2(q2, w)).

34
Chapter 2.
Finite Automata and Regular Languages
The latter equality implies that (2.1) is true and, therefore, M indeed accepts
the language A ∪B.
What about the closure of the regular languages under the concatenation
and star operations? It turns out that the regular languages are closed under
these operations. But how do we prove this?
Let A and B be two regular languages, and let M1 and M2 be ﬁnite
automata that accept A and B, respectively. How do we construct a ﬁnite
automaton M that accepts the concatenation AB? Given an input string
u, M has to decide whether or not u can be broken into two strings w and
w′ (i.e., write u as u = ww′), such that w ∈A and w′ ∈B. In words, M
has to decide whether or not u can be broken into two substrings, such that
the ﬁrst substring is accepted by M1 and the second substring is accepted by
M2. The diﬃculty is caused by the fact that M has to make this decision by
scanning the string u only once. If u ∈AB, then M has to decide, during
this single scan, where to break u into two substrings. Similarly, if u ̸∈AB,
then M has to decide, during this single scan, that u cannot be broken into
two substrings such that the ﬁrst substring is in A and the second substring
is in B.
It seems to be even more diﬃcult to prove that A∗is a regular language,
if A itself is regular. In order to prove this, we need a ﬁnite automaton that,
when given an arbitrary input string u, decides whether or not u can be
broken into substrings such that each substring is in A. The problem is that,
if u ∈A∗, the ﬁnite automaton has to determine into how many substrings,
and where, the string u has to be broken; it has to do this during one single
scan of the string u.
As we mentioned already, if A and B are regular languages, then both
AB and A∗are also regular. In order to prove these claims, we will introduce
a more general type of ﬁnite automaton.
The ﬁnite automata that we have seen so far are deterministic.
This
means the following:
• If the ﬁnite automaton M is in state r and if it reads the symbol a,
then M switches from state r to the uniquely deﬁned state δ(r, a).
From now on, we will call such a ﬁnite automaton a deterministic ﬁnite
automaton (DFA). In the next section, we will deﬁne the notion of a nonde-
terministic ﬁnite automaton (NFA). For such an automaton, there are zero
or more possible states to switch to. At ﬁrst sight, nondeterministic ﬁnite

2.4.
Nondeterministic ﬁnite automata
35
automata seem to be more powerful than their deterministic counterparts.
We will prove, however, that DFAs have the same power as NFAs. As we will
see, using this fact, it will be easy to prove that the class of regular languages
is closed under the concatenation and star operations.
2.4
Nondeterministic ﬁnite automata
We start by giving three examples of nondeterministic ﬁnite automata. These
examples will show the diﬀerence between this type of automata and the
deterministic versions that we have considered in the previous sections. After
these examples, we will give a formal deﬁnition of a nondeterministic ﬁnite
automaton.
2.4.1
A ﬁrst example
Consider the following state diagram:
q1
q2
q3
q4
0,1
1
0,ε
1
0,1
You will notice three diﬀerences with the ﬁnite automata that we have
seen until now. First, if the automaton is in state q1 and reads the symbol 1,
then it has two options: Either it stays in state q1, or it switches to state q2.
Second, if the automaton is in state q2, then it can switch to state q3 without
reading a symbol; this is indicated by the edge having the empty string ǫ as
label. Third, if the automaton is in state q3 and reads the symbol 0, then it
cannot continue.
Let us see what this automaton can do when it gets the string 010110 as
input. Initially, the automaton is in the start state q1.
• Since the ﬁrst symbol in the input string is 0, the automaton stays in
state q1 after having read this symbol.
• The second symbol is 1, and the automaton can either stay in state q1
or switch to state q2.

36
Chapter 2.
Finite Automata and Regular Languages
– If the automaton stays in state q1, then it is still in this state after
having read the third symbol.
– If the automaton switches to state q2, then it again has two op-
tions:
∗Either read the third symbol in the input string, which is 0,
and switch to state q3,
∗or switch to state q3, without reading the third symbol.
If we continue in this way, then we see that, for the input string 010110,
there are seven possible computations. All these computations are given in
the ﬁgure below.
q1
q1
0
1
q1
q1
0
1
1
q1
q2
1
1
q1
q2
q1
0
0
ε
q3
q3
hang
hang
ε
q3
q4
1
0
q4
1
q2
0
ε
q3
q3
hang
1
q4
1
q4
q4
0
Consider the lowest path in the ﬁgure above:
• When reading the ﬁrst symbol, the automaton stays in state q1.
• When reading the second symbol, the automaton switches to state q2.
• The automaton does not read the third symbol (equivalently, it “reads”
the empty string ǫ), and switches to state q3. At this moment, the

2.4.
Nondeterministic ﬁnite automata
37
automaton cannot continue: The third symbol is 0, but there is no
edge leaving q3 that is labeled 0, and there is no edge leaving q3 that
is labeled ǫ. Therefore, the computation hangs at this point.
From the ﬁgure, you can see that, out of the seven possible computations,
exactly two end in the accept state q4 (after the entire input string 010110 has
been read). We say that the automaton accepts the string 010110, because
there is at least one computation that ends in the accept state.
Now consider the input string 010. In this case, there are three possible
computations:
1. q1
0→q1
1→q1
0→q1
2. q1
0→q1
1→q2
0→q3
3. q1
0→q1
1→q2
ǫ→q3 →hang
None of these computations ends in the accept state (after the entire input
string 010 has been read). Therefore, we say that the automaton rejects the
input string 010.
The state diagram given above is an example of a nondeterministic ﬁnite
automaton (NFA). Informally, an NFA accepts a string, if there exists at least
one path in the state diagram that (i) starts in the start state, (ii) does not
hang before the entire string has been read, and (iii) ends in an accept state.
A string for which (i), (ii), and (iii) does not hold is rejected by the NFA.
The NFA given above accepts all binary strings that contain 101 or 11 as
a substring. All other binary strings are rejected.
2.4.2
A second example
Let A be the language
A = {w ∈{0, 1}∗: w has a 1 in the third position from the right}.
The following state diagram deﬁnes an NFA that accepts all strings that are
in A, and rejects all strings that are not in A.
q1
q2
q3
q4
0,1
1
0,1
0,1

38
Chapter 2.
Finite Automata and Regular Languages
This NFA does the following. If it is in the start state q1 and reads the
symbol 1, then it either stays in state q1 or it “guesses” that this symbol
is the third symbol from the right in the input string. In the latter case,
the NFA switches to state q2, and then it “veriﬁes” that there are indeed
exactly two remaining symbols in the input string. If there are more than
two remaining symbols, then the NFA hangs (in state q4) after having read
the next two symbols.
Observe how this guessing mechanism is used: The automaton can only
read the input string once, from left to right. Hence, it does not know when
it reaches the third symbol from the right. When the NFA reads a 1, it can
guess that this is the third symbol from the right; after having made this
guess, it veriﬁes whether or not the guess was correct.
In Section 2.2.3, we have seen a DFA for the same language A. Observe
that the NFA has a much simpler structure than the DFA.
2.4.3
A third example
Consider the following state diagram, which deﬁnes an NFA whose alphabet
is {0}.
ε
ε
0
0
0
0
0
This NFA accepts the language
A = {0k : k ≡0 mod 2 or k ≡0 mod 3},
where 0k is the string consisting of k many 0s. (If k = 0, then 0k = ǫ.)
Observe that A is the union of the two languages
A1 = {0k : k ≡0 mod 2}

2.4.
Nondeterministic ﬁnite automata
39
and
A2 = {0k : k ≡0 mod 3}.
The NFA basically consists of two DFAs: one of these accepts A1, whereas the
other accepts A2. Given an input string w, the NFA has to decide whether
or not w ∈A, which is equivalent to deciding whether or not w ∈A1 or
w ∈A2. The NFA makes this decision in the following way: At the start, it
“guesses” whether (i) it is going to check whether or not w ∈A1 (i.e., the
length of w is even), or (ii) it is going to check whether or not w ∈A2 (i.e.,
the length of w is a multiple of 3). After having made the guess, it veriﬁes
whether or not the guess was correct. If w ∈A, then there exists a way of
making the correct guess and verifying that w is indeed an element of A (by
ending in an accept state). If w ̸∈A, then no matter which guess is made,
the NFA will never end in an accept state.
2.4.4
Deﬁnition of nondeterministic ﬁnite automaton
The previous examples give you an idea what nondeterministic ﬁnite au-
tomata are and how they work. In this section, we give a formal deﬁnition
of these automata.
For any alphabet Σ, we deﬁne Σǫ to be the set
Σǫ = Σ ∪{ǫ}.
Recall the notion of a power set: For any set Q, the power set of Q, denoted
by P(Q), is the set of all subsets of Q, i.e.,
P(Q) = {R : R ⊆Q}.
Deﬁnition 2.4.1 A nondeterministic ﬁnite automaton (NFA) is a 5-tuple
M = (Q, Σ, δ, q, F), where
1. Q is a ﬁnite set, whose elements are called states,
2. Σ is a ﬁnite set, called the alphabet; the elements of Σ are called symbols,
3. δ : Q × Σǫ →P(Q) is a function, called the transition function,
4. q is an element of Q; it is called the start state,
5. F is a subset of Q; the elements of F are called accept states.

40
Chapter 2.
Finite Automata and Regular Languages
As for DFAs, the transition function δ can be thought of as the “program”
of the ﬁnite automaton M = (Q, Σ, δ, q, F):
• Let r ∈Q, and let a ∈Σǫ. Then δ(r, a) is a (possibly empty) subset of
Q. If the NFA M is in state r, and if it reads a (where a may be the
empty string ǫ), then M can switch from state r to any state in δ(r, a).
If δ(r, a) = ∅, then M cannot continue and the computation hangs.
The example given in Section 2.4.1 is an NFA, where Q = {q1, q2, q3, q4},
Σ = {0, 1}, the start state is q1, the set of accept states is F = {q4}, and the
transition function δ is given by the following table:
0
1
ǫ
q1
{q1}
{q1, q2}
∅
q2
{q3}
∅
{q3}
q3
∅
{q4}
∅
q4
{q4}
{q4}
∅
Deﬁnition 2.4.2 Let M = (Q, Σ, δ, q, F) be an NFA, and let w ∈Σ∗. We
say that M accepts w, if w can be written as w = y1y2 . . . ym, where yi ∈Σǫ
for all i with 1 ≤i ≤m, and there exists a sequence r0, r1, . . . , rm of states
in Q, such that
• r0 = q,
• ri+1 ∈δ(ri, yi+1), for i = 0, 1, . . . , m −1, and
• rm ∈F.
Otherwise, we say that M rejects the string w.
The NFA in the example in Section 2.4.1 accepts the string 01100. This
can be seen by taking
• w = 01ǫ100 = y1y2y3y4y5y6, and
• r0 = q1, r1 = q1, r2 = q2, r3 = q3, r4 = q4, r5 = q4, and r6 = q4.
Deﬁnition 2.4.3 Let M = (Q, Σ, δ, q, F) be an NFA. The language L(M)
accepted by M is deﬁned as
L(M) = {w ∈Σ∗: M accepts w }.

2.5.
Equivalence of DFAs and NFAs
41
2.5
Equivalence of DFAs and NFAs
You may have the impression that nondeterministic ﬁnite automata are more
powerful than deterministic ﬁnite automata. In this section, we will show
that this is not the case.
That is, we will prove that a language can be
accepted by a DFA if and only if it can be accepted by an NFA. In order
to prove this, we will show how to convert an arbitrary NFA to a DFA that
accepts the same language.
What about converting a DFA to an NFA? Well, there is (almost) nothing
to do, because a DFA is also an NFA. This is not quite true, because
• the transition function of a DFA maps a state and a symbol to a state,
whereas
• the transition function of an NFA maps a state and a symbol to a set
of zero or more states.
The formal conversion of a DFA to an NFA is done as follows: Let M =
(Q, Σ, δ, q, F) be a DFA. Recall that δ is a function δ : Q × Σ →Q. We
deﬁne the function δ′ : Q × Σǫ →P(Q) as follows. For any r ∈Q and for
any a ∈Σǫ,
δ′(r, a) =

{δ(r, a)}
if a ̸= ǫ,
∅
if a = ǫ.
Then N = (Q, Σ, δ′, q, F) is an NFA, whose behavior is exactly the same as
that of the DFA M; the easiest way to see this is by observing that the state
diagrams of M and N are equal. Therefore, we have L(M) = L(N).
In the rest of this section, we will show how to convert an NFA to a DFA:
Theorem 2.5.1 Let N = (Q, Σ, δ, q, F) be a nondeterministic ﬁnite automa-
ton. There exists a deterministic ﬁnite automaton M, such that L(M) =
L(N).
Proof.
Recall that the NFA N can (in general) perform more than one
computation on a given input string. The idea of the proof is to construct a
DFA M that runs all these diﬀerent computations simultaneously. (We have
seen this idea already in the proof of Theorem 2.3.1.) To be more precise,
the DFA M will have the following property:
• the state that M is in after having read an initial part of the input
string corresponds exactly to the set of all states that N can reach
after having read the same part of the input string.

42
Chapter 2.
Finite Automata and Regular Languages
We start by presenting the conversion for the case when N does not
contain ǫ-transitions. In other words, the state diagram of N does not contain
any edge that has ǫ as a label. (Later, we will extend the conversion to the
general case.) Let the DFA M be deﬁned as M = (Q′, Σ, δ′, q′, F ′), where
• the set Q′ of states is equal to Q′ = P(Q); observe that |Q′| = 2|Q|,
• the start state q′ is equal to q′ = {q}; so M has the “same” start state
as N,
• the set F ′ of accept states is equal to the set of all elements R of Q′
having the property that R contains at least one accept state of N, i.e.,
F ′ = {R ∈Q′ : R ∩F ̸= ∅},
• the transition function δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each
R ∈Q′ and for each a ∈Σ,
δ′(R, a) =
[
r∈R
δ(r, a).
Let us see what the transition function δ′ of M does. First observe that,
since N is an NFA, δ(r, a) is a subset of Q. This implies that δ′(R, a) is the
union of subsets of Q and, therefore, also a subset of Q. Hence, δ′(R, a) is
an element of Q′.
The set δ(r, a) is equal to the set of all states of the NFA N that can be
reached from state r by reading the symbol a. We take the union of these
sets δ(r, a), where r ranges over all elements of R, to obtain the new set
δ′(R, a). This new set is the state that the DFA M reaches from state R, by
reading the symbol a.
In this way, we obtain the correspondence that was given in the beginning
of this proof.
After this warming-up, we can consider the general case. In other words,
from now on, we allow ǫ-transitions in the NFA N. The DFA M is deﬁned as
above, except that the start state q′ and the transition function δ′ have to be
modiﬁed. Recall that a computation of the NFA N consists of the following:
1. Start in the start state q and make zero or more ǫ-transitions.
2. Read one “real” symbol of Σ and move to a new state (or stay in the
current state).

2.5.
Equivalence of DFAs and NFAs
43
3. Make zero or more ǫ-transitions.
4. Read one “real” symbol of Σ and move to a new state (or stay in the
current state).
5. Make zero or more ǫ-transitions.
6. Etc.
The DFA M will simulate this computation in the following way:
• Simulate 1. in one single step. As we will see below, this simulation is
implicitly encoded in the deﬁnition of the start state q′ of M.
• Simulate 2. and 3. in one single step.
• Simulate 4. and 5. in one single step.
• Etc.
Thus, in one step, the DFA M simulates the reading of one “real” symbol of
Σ, followed by making zero or more ǫ-transitions.
To formalize this, we need the notion of ǫ-closure. For any state r of the
NFA N, the ǫ-closure of r, denoted by Cǫ(r), is deﬁned to be the set of all
states of N that can be reached from r, by making zero or more ǫ-transitions.
For any state R of the DFA M (hence, R ⊆Q), we deﬁne
Cǫ(R) =
[
r∈R
Cǫ(r).
How do we deﬁne the start state q′ of the DFA M? Before the NFA N
reads its ﬁrst “real” symbol of Σ, it makes zero or more ǫ-transitions. In
other words, at the moment when N reads the ﬁrst symbol of Σ, it can be
in any state of Cǫ(q). Therefore, we deﬁne q′ to be
q′ = Cǫ(q) = Cǫ({q}).
How do we deﬁne the transition function δ′ of the DFA M? Assume that
M is in state R, and reads the symbol a. At this moment, the NFA N would
have been in any state r of R. By reading the symbol a, N can switch to
any state in δ(r, a), and then make zero or more ǫ-transitions. Hence, the

44
Chapter 2.
Finite Automata and Regular Languages
NFA can switch to any state in the set Cǫ(δ(r, a)). Based on this, we deﬁne
δ′(R, a) to be
δ′(R, a) =
[
r∈R
Cǫ(δ(r, a)).
To summarize, the NFA N = (Q, Σ, δ, q, F) is converted to the DFA
M = (Q′, Σ, δ′, q′, F ′), where
• Q′ = P(Q),
• q′ = Cǫ({q}),
• F ′ = {R ∈Q′ : R ∩F ̸= ∅},
• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each
a ∈Σ,
δ′(R, a) =
[
r∈R
Cǫ(δ(r, a)).
The results proved until now can be summarized in the following theorem.
Theorem 2.5.2 Let A be a language. Then A is regular if and only if there
exists a nondeterministic ﬁnite automaton that accepts A.
2.5.1
An example
Consider the NFA N = (Q, Σ, δ, q, F), where Q = {1, 2, 3}, Σ = {a, b}, q = 1,
F = {2}, and δ is given by the following table:
a
b
ǫ
1
{3}
∅
{2}
2
{1}
∅
∅
3
{2}
{2, 3}
∅
The state diagram of N is as follows:

2.5.
Equivalence of DFAs and NFAs
45
1
2
3
a
a
ǫ
b
a, b
We will show how to convert this NFA N to a DFA M that accepts the
same language. Following the proof of Theorem 2.5.1, the DFA M is speciﬁed
by M = (Q′, Σ, δ′, q′, F ′), where each of the components is deﬁned below.
• Q′ = P(Q). Hence,
Q′ = {∅, {1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}}.
• q′ = Cǫ({q}). Hence, the start state q′ of M is the set of all states of
N that can be reached from N’s start state q = 1, by making zero or
more ǫ-transitions. We obtain
q′ = Cǫ({q}) = Cǫ({1}) = {1, 2}.
• F ′ = {R ∈Q′ : R ∩F ̸= ∅}. Hence, the accept states of M are those
states that contain the accept state 2 of N. We obtain
F ′ = {{2}, {1, 2}, {2, 3}, {1, 2, 3}}.
• δ′ : Q′ × Σ →Q′ is deﬁned as follows: For each R ∈Q′ and for each
a ∈Σ,
δ′(R, a) =
[
r∈R
Cǫ(δ(r, a)).

46
Chapter 2.
Finite Automata and Regular Languages
In this example δ′ is given by
δ′(∅, a) = ∅
δ′(∅, b) = ∅
δ′({1}, a) = {3}
δ′({1}, b) = ∅
δ′({2}, a) = {1, 2}
δ′({2}, b) = ∅
δ′({3}, a) = {2}
δ′({3}, b) = {2, 3}
δ′({1, 2}, a) = {1, 2, 3}
δ′({1, 2}, b) = ∅
δ′({1, 3}, a) = {2, 3}
δ′({1, 3}, b) = {2, 3}
δ′({2, 3}, a) = {1, 2}
δ′({2, 3}, b) = {2, 3}
δ′({1, 2, 3}, a) = {1, 2, 3}
δ′({1, 2, 3}, b) = {2, 3}
The state diagram of the DFA M is as follows:
/0
{1}
{2}
{3}
{1,2}
{2,3}
{1,3}
{1,2,3}
a,b
b
a
b
a
a
b
a,b
a
b
b
a
b
a
We make the following observations:

2.6.
Closure under the regular operations
47
• The states {1} and {1, 3} do not have incoming edges. Therefore, these
two states cannot be reached from the start state {1, 2}.
• The state {3} has only one incoming edge; it comes from the state
{1}. Since {1} cannot be reached from the start state, {3} cannot be
reached from the start state.
• The state {2} has only one incoming edge; it comes from the state
{3}. Since {3} cannot be reached from the start state, {2} cannot be
reached from the start state.
Hence, we can remove the four states {1}, {2}, {3}, and {1, 3}. The
resulting DFA accepts the same language as the DFA above.
This leads
to the following state diagram, which depicts a DFA that accepts the same
language as the NFA N:
/0
{1,2}
{2,3}
{1,2,3}
a,b
a
b
b
a
b
a

48
Chapter 2.
Finite Automata and Regular Languages
2.6
Closure under the regular operations
In Section 2.3, we have deﬁned the regular operations union, concatenation,
and star. We proved in Theorem 2.3.1 that the union of two regular lan-
guages is a regular language. We also explained why it is not clear that the
concatenation of two regular languages is regular, and that the star of a reg-
ular language is regular. In this section, we will see that the concept of NFA,
together with Theorem 2.5.2, can be used to give a simple proof of the fact
that the regular languages are indeed closed under the regular operations.
We start by giving an alternative proof of Theorem 2.3.1:
Theorem 2.6.1 The set of regular languages is closed under the union op-
eration, i.e., if A1 and A2 are regular languages over the same alphabet Σ,
then A1 ∪A2 is also a regular language.
Proof.
Since A1 is regular, there is, by Theorem 2.5.2, an NFA M1 =
(Q1, Σ, δ1, q1, F1), such that A1 = L(M1). Similarly, there is an NFA M2 =
(Q2, Σ, δ2, q2, F2), such that A2 = L(M2). We may assume that Q1 ∩Q2 = ∅,
because otherwise, we can give new “names” to the states of Q1 and Q2.
From these two NFAs, we will construct an NFA M = (Q, Σ, δ, q0, F), such
that L(M) = A1 ∪A2. The construction is illustrated in Figure 2.1. The
NFA M is deﬁned as follows:
1. Q = {q0} ∪Q1 ∪Q2, where q0 is a new state.
2. q0 is the start state of M.
3. F = F1 ∪F2.
4. δ : Q × Σǫ →P(Q) is deﬁned as follows: For any r ∈Q and for any
a ∈Σǫ,
δ(r, a) =







δ1(r, a)
if r ∈Q1,
δ2(r, a)
if r ∈Q2,
{q1, q2}
if r = q0 and a = ǫ,
∅
if r = q0 and a ̸= ǫ.

2.6.
Closure under the regular operations
49
q1
M1
M2
q2
q0
q1
q2
ε
ε
M
Figure 2.1: The NFA M accepts L(M1) ∪L(M2).
Theorem 2.6.2 The set of regular languages is closed under the concatena-
tion operation, i.e., if A1 and A2 are regular languages over the same alphabet
Σ, then A1A2 is also a regular language.
Proof.
Let M1 = (Q1, Σ, δ1, q1, F1) be an NFA, such that A1 = L(M1).
Similarly, let M2 = (Q2, Σ, δ2, q2, F2) be an NFA, such that A2 = L(M2).
As in the proof of Theorem 2.6.1, we may assume that Q1 ∩Q2 = ∅. We
will construct an NFA M = (Q, Σ, δ, q0, F), such that L(M) = A1A2. The
construction is illustrated in Figure 2.2. The NFA M is deﬁned as follows:
1. Q = Q1 ∪Q2.
2. q0 = q1.
3. F = F2.

50
Chapter 2.
Finite Automata and Regular Languages
q1
M1
M2
q2
q2
ε
ε
ε
q0
M
Figure 2.2: The NFA M accepts L(M1)L(M2).
4. δ : Q × Σǫ →P(Q) is deﬁned as follows: For any r ∈Q and for any
a ∈Σǫ,
δ(r, a) =







δ1(r, a)
if r ∈Q1 and r ̸∈F1,
δ1(r, a)
if r ∈F1 and a ̸= ǫ,
δ1(r, a) ∪{q2}
if r ∈F1 and a = ǫ,
δ2(r, a)
if r ∈Q2.
Theorem 2.6.3 The set of regular languages is closed under the star oper-
ation, i.e., if A is a regular language, then A∗is also a regular language.
Proof. Let Σ be the alphabet of A and let N = (Q1, Σ, δ1, q1, F1) be an
NFA, such that A = L(N). We will construct an NFA M = (Q, Σ, δ, q0, F),
such that L(M) = A∗. The construction is illustrated in Figure 2.3. The
NFA M is deﬁned as follows:

2.6.
Closure under the regular operations
51
q1
N
q1
q0
ε
ε
ε
ε
M
Figure 2.3: The NFA M accepts (L(N))∗.
1. Q = {q0} ∪Q1, where q0 is a new state.
2. q0 is the start state of M.
3. F = {q0} ∪F1. (Since ǫ ∈A∗, q0 has to be an accept state.)
4. δ : Q × Σǫ →P(Q) is deﬁned as follows: For any r ∈Q and for any
a ∈Σǫ,
δ(r, a) =











δ1(r, a)
if r ∈Q1 and r ̸∈F1,
δ1(r, a)
if r ∈F1 and a ̸= ǫ,
δ1(r, a) ∪{q1}
if r ∈F1 and a = ǫ,
{q1}
if r = q0 and a = ǫ,
∅
if r = q0 and a ̸= ǫ.
In the ﬁnal theorem of this section, we mention (without proof) two more
closure properties of the regular languages:
Theorem 2.6.4 The set of regular languages is closed under the complement
and intersection operations:
1. If A is a regular language over the alphabet Σ, then the complement
A = {w ∈Σ∗: w ̸∈A}
is also a regular language.

52
Chapter 2.
Finite Automata and Regular Languages
2. If A1 and A2 are regular languages over the same alphabet Σ, then the
intersection
A1 ∩A2 = {w ∈Σ∗: w ∈A1 and w ∈A2}
is also a regular language.
2.7
Regular expressions
In this section, we present regular expressions, which are a means to describe
languages. As we will see, the class of languages that can be described by
regular expressions coincides with the class of regular languages.
Before formally deﬁning the notion of a regular expression, we give some
examples. Consider the expression
(0 ∪1)01∗.
The language described by this expression is the set of all binary strings
1. that start with either 0 or 1 (this is indicated by (0 ∪1)),
2. for which the second symbol is 0 (this is indicated by 0), and
3. that end with zero or more 1s (this is indicated by 1∗).
That is, the language described by this expression is
{00, 001, 0011, 00111, . . ., 10, 101, 1011, 10111, . . .}.
Here are some more examples (in all cases, the alphabet is {0, 1}):
• The language {w : w contains exactly two 0s} is described by the ex-
pression
1∗01∗01∗.
• The language {w : w contains at least two 0s} is described by the ex-
pression
(0 ∪1)∗0(0 ∪1)∗0(0 ∪1)∗.
• The language {w : 1011 is a substring of w} is described by the ex-
pression
(0 ∪1)∗1011(0 ∪1)∗.

2.7.
Regular expressions
53
• The language {w : the length of w is even} is described by the expres-
sion
((0 ∪1)(0 ∪1))∗.
• The language {w : the length of w is odd} is described by the expres-
sion
(0 ∪1) ((0 ∪1)(0 ∪1))∗.
• The language {1011, 0} is described by the expression
1011 ∪0.
• The language {w :
the ﬁrst and last symbols of w are equal} is de-
scribed by the expression
0(0 ∪1)∗0 ∪1(0 ∪1)∗1 ∪0 ∪1.
After these examples, we give a formal (and inductive) deﬁnition of regular
expressions:
Deﬁnition 2.7.1 Let Σ be a non-empty alphabet.
1. ǫ is a regular expression.
2. ∅is a regular expression.
3. For each a ∈Σ, a is a regular expression.
4. If R1 and R2 are regular expressions, then R1 ∪R2 is a regular expres-
sion.
5. If R1 and R2 are regular expressions, then R1R2 is a regular expression.
6. If R is a regular expression, then R∗is a regular expression.
You can regard 1., 2., and 3. as being the “building blocks” of regular
expressions.
Items 4., 5., and 6. give rules that can be used to combine
regular expressions into new (and “larger”) regular expressions. To give an
example, we claim that
(0 ∪1)∗101(0 ∪1)∗
is a regular expression (where the alphabet Σ is equal to {0, 1}). In order
to prove this, we have to show that this expression can be “built” using the
“rules” given in Deﬁnition 2.7.1. Here we go:

54
Chapter 2.
Finite Automata and Regular Languages
• By 3., 0 is a regular expression.
• By 3., 1 is a regular expression.
• Since 0 and 1 are regular expressions, by 4., 0∪1 is a regular expression.
• Since 0∪1 is a regular expression, by 6., (0∪1)∗is a regular expression.
• Since 1 and 0 are regular expressions, by 5., 10 is a regular expression.
• Since 10 and 1 are regular expressions, by 5., 101 is a regular expression.
• Since (0 ∪1)∗and 101 are regular expressions, by 5., (0 ∪1)∗101 is a
regular expression.
• Since (0 ∪1)∗101 and (0 ∪1)∗are regular expressions, by 5., (0 ∪
1)∗101(0 ∪1)∗is a regular expression.
Next we deﬁne the language that is described by a regular expression:
Deﬁnition 2.7.2 Let Σ be a non-empty alphabet.
1. The regular expression ǫ describes the language {ǫ}.
2. The regular expression ∅describes the language ∅.
3. For each a ∈Σ, the regular expression a describes the language {a}.
4. Let R1 and R2 be regular expressions and let L1 and L2 be the lan-
guages described by them, respectively. The regular expression R1∪R2
describes the language L1 ∪L2.
5. Let R1 and R2 be regular expressions and let L1 and L2 be the languages
described by them, respectively. The regular expression R1R2 describes
the language L1L2.
6. Let R be a regular expression and let L be the language described by
it. The regular expression R∗describes the language L∗.
We consider some examples:
• The regular expression (0∪ǫ)(1∪ǫ) describes the language {01, 0, 1, ǫ}.

2.7.
Regular expressions
55
• The regular expression 0 ∪ǫ describes the language {0, ǫ}, whereas the
regular expression 1∗describes the language {ǫ, 1, 11, 111, . . .}. There-
fore, the regular expression (0 ∪ǫ)1∗describes the language
{0, 01, 011, 0111, . . ., ǫ, 1, 11, 111, . . .}.
Observe that this language is also described by the regular expression
01∗∪1∗.
• The regular expression 1∗∅describes the empty language, i.e., the lan-
guage ∅. (You should convince yourself that this is correct.)
• The regular expression ∅∗describes the language {ǫ}.
Deﬁnition 2.7.3 Let R1 and R2 be regular expressions and let L1 and L2
be the languages described by them, respectively. If L1 = L2 (i.e., R1 and
R2 describe the same language), then we will write R1 = R2.
Hence, even though (0∪ǫ)1∗and 01∗∪1∗are diﬀerent regular expressions,
we write
(0 ∪ǫ)1∗= 01∗∪1∗,
because they describe the same language.
In Section 2.8.2, we will show that every regular language can be described
by a regular expression. The proof of this fact is purely algebraic and uses
the following algebraic identities involving regular expressions.
Theorem 2.7.4 Let R1, R2, and R3 be regular expressions. The following
identities hold:
1. R1∅= ∅R1 = ∅.
2. R1ǫ = ǫR1 = R1.
3. R1 ∪∅= ∅∪R1 = R1.
4. R1 ∪R1 = R1.
5. R1 ∪R2 = R2 ∪R1.
6. R1(R2 ∪R3) = R1R2 ∪R1R3.

56
Chapter 2.
Finite Automata and Regular Languages
7. (R1 ∪R2)R3 = R1R3 ∪R2R3.
8. R1(R2R3) = (R1R2)R3.
9. ∅∗= ǫ.
10. ǫ∗= ǫ.
11. (ǫ ∪R1)∗= R∗
1.
12. (ǫ ∪R1)(ǫ ∪R1)∗= R∗
1.
13. R∗
1(ǫ ∪R1) = (ǫ ∪R1)R∗
1 = R∗
1.
14. R∗
1R2 ∪R2 = R∗
1R2.
15. R1(R2R1)∗= (R1R2)∗R1.
16. (R1 ∪R2)∗= (R∗
1R2)∗R∗
1 = (R∗
2R1)∗R∗
2.
We will not present the (boring) proofs of these identities, but urge you
to convince yourself informally that they make perfect sense. To give an
example, we mentioned above that
(0 ∪ǫ)1∗= 01∗∪1∗.
We can verify this identity in the following way:
(0 ∪ǫ)1∗
=
01∗∪ǫ1∗
(by identity 7)
=
01∗∪1∗
(by identity 2)
2.8
Equivalence of regular expressions and reg-
ular languages
In the beginning of Section 2.7, we mentioned the following result:
Theorem 2.8.1 Let L be a language. Then L is regular if and only if there
exists a regular expression that describes L.
The proof of this theorem consists of two parts:

2.8.
Equivalence of regular expressions and regular languages 57
• In Section 2.8.1, we will prove that every regular expression describes
a regular language.
• In Section 2.8.2, we will prove that every DFA M can be converted to
a regular expression that describes the language L(M).
These two results will prove Theorem 2.8.1.
2.8.1
Every regular expression describes a regular lan-
guage
Let R be an arbitrary regular expression over the alphabet Σ. We will prove
that the language described by R is a regular language. The proof is by
induction on the structure of R (i.e., by induction on the way R is “built”
using the “rules” given in Deﬁnition 2.7.1).
The ﬁrst base case: Assume that R = ǫ.
Then R describes the lan-
guage {ǫ}. In order to prove that this language is regular, it suﬃces, by
Theorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q, F) that accepts this
language. This NFA is obtained by deﬁning Q = {q}, q is the start state,
F = {q}, and δ(q, a) = ∅for all a ∈Σǫ. The ﬁgure below gives the state
diagram of M:
q
The second base case: Assume that R = ∅. Then R describes the language
∅. In order to prove that this language is regular, it suﬃces, by Theorem 2.5.2,
to construct an NFA M = (Q, Σ, δ, q, F) that accepts this language. This
NFA is obtained by deﬁning Q = {q}, q is the start state, F = ∅, and
δ(q, a) = ∅for all a ∈Σǫ. The ﬁgure below gives the state diagram of M:
q
The third base case: Let a ∈Σ and assume that R = a. Then R describes
the language {a}. In order to prove that this language is regular, it suﬃces,
by Theorem 2.5.2, to construct an NFA M = (Q, Σ, δ, q1, F) that accepts

58
Chapter 2.
Finite Automata and Regular Languages
this language. This NFA is obtained by deﬁning Q = {q1, q2}, q1 is the start
state, F = {q2}, and
δ(q1, a)
=
{q2},
δ(q1, b)
=
∅for all b ∈Σǫ \ {a},
δ(q2, b)
=
∅for all b ∈Σǫ.
The ﬁgure below gives the state diagram of M:
q1
q2
a
The ﬁrst case of the induction step: Assume that R = R1 ∪R2, where
R1 and R2 are regular expressions. Let L1 and L2 be the languages described
by R1 and R2, respectively, and assume that L1 and L2 are regular. Then R
describes the language L1 ∪L2, which, by Theorem 2.6.1, is regular.
The second case of the induction step: Assume that R = R1R2, where
R1 and R2 are regular expressions. Let L1 and L2 be the languages described
by R1 and R2, respectively, and assume that L1 and L2 are regular. Then R
describes the language L1L2, which, by Theorem 2.6.2, is regular.
The third case of the induction step: Assume that R = (R1)∗, where
R1 is a regular expression.
Let L1 be the language described by R1 and
assume that L1 is regular. Then R describes the language (L1)∗, which, by
Theorem 2.6.3, is regular.
This concludes the proof of the claim that every regular expression de-
scribes a regular language.
To give an example, consider the regular expression
(ab ∪a)∗,
where the alphabet is {a, b}. We will prove that this regular expression de-
scribes a regular language, by constructing an NFA that accepts the language
described by this regular expression. Observe how the regular expression is
“built”:
• Take the regular expressions a and b, and combine them into the regular
expression ab.

2.8.
Equivalence of regular expressions and regular languages 59
• Take the regular expressions ab and a, and combine them into the
regular expression ab ∪a.
• Take the regular expression ab ∪a, and transform it into the regular
expression (ab ∪a)∗.
First, we construct an NFA M1 that accepts the language described by
the regular expression a:
a
M1
Next, we construct an NFA M2 that accepts the language described by
the regular expression b:
M2
b
Next, we apply the construction given in the proof of Theorem 2.6.2 to
M1 and M2. This gives an NFA M3 that accepts the language described by
the regular expression ab:
M3
a
ε
b
Next, we apply the construction given in the proof of Theorem 2.6.1 to
M3 and M1. This gives an NFA M4 that accepts the language described by
the regular expression ab ∪a:
a
ε
b
a
ε
ε
M4
Finally, we apply the construction given in the proof of Theorem 2.6.3
to M4. This gives an NFA M5 that accepts the language described by the
regular expression (ab ∪a)∗:

60
Chapter 2.
Finite Automata and Regular Languages
a
ε
b
a
ε
ε
ε
ε
ε
M5
2.8.2
Converting a DFA to a regular expression
In this section, we will prove that every DFA M can be converted to a regular
expression that describes the language L(M). In order to prove this result,
we need to solve recurrence relations involving languages.
Solving recurrence relations
Let Σ be an alphabet, let B and C be “known” languages in Σ∗such that
ǫ ̸∈B, and let L be an “unknown” language such that
L = BL ∪C.
Can we “solve” this equation for L? That is, can we express L in terms of
B and C?
Consider an arbitrary string u in L. We are going to determine how u
looks like. Since u ∈L and L = BL ∪C, we know that u is a string in
BL ∪C. Hence, there are two possibilities for u.
1. u is an element of C.
2. u is an element of BL. In this case, there are strings b ∈B and v ∈L
such that u = bv. Since ǫ ̸∈B, we have b ̸= ǫ and, therefore, |v| < |u|.
(Recall that |v| denotes the length, i.e., the number of symbols, of the
string v.) Since v is a string in L, which is equal to BL ∪C, v is a
string in BL ∪C. Hence, there are two possibilities for v.

2.8.
Equivalence of regular expressions and regular languages 61
(a) v is an element of C. In this case,
u = bv, where b ∈B and v ∈C; thus, u ∈BC.
(b) v is an element of BL. In this case, there are strings b′ ∈B and
w ∈L such that v = b′w. Since ǫ ̸∈B, we have b′ ̸= ǫ and,
therefore, |w| < |v|. Since w is a string in L, which is equal to
BL∪C, w is a string in BL∪C. Hence, there are two possibilities
for w.
i. w is an element of C. In this case,
u = bb′w, where b, b′ ∈B and w ∈C; thus, u ∈BBC.
ii. w is an element of BL. In this case, there are strings b′′ ∈B
and x ∈L such that w = b′′x. Since ǫ ̸∈B, we have b′′ ̸= ǫ
and, therefore, |x| < |w|. Since x is a string in L, which is
equal to BL ∪C, x is a string in BL ∪C. Hence, there are
two possibilities for x.
A. x is an element of C. In this case,
u = bb′b′′x, where b, b′, b′′ ∈B and x ∈C; thus, u ∈BBBC.
B. x is an element of BL. Etc., etc.
This process hopefully convinces you that any string u in L can be written
as the concatenation of zero or more strings in B, followed by one string in
C. In fact, L consists of exactly those strings having this property:
Lemma 2.8.2 Let Σ be an alphabet, and let B, C, and L be languages in
Σ∗such that ǫ ̸∈B and
L = BL ∪C.
Then
L = B∗C.
Proof. First, we show that B∗C ⊆L. Let u be an arbitrary string in B∗C.
Then u is the concatenation of k strings of B, for some k ≥0, followed by
one string of C. We proceed by induction on k.
The base case is when k = 0. In this case, u is a string in C. Hence, u is
a string in BL ∪C. Since BL ∪C = L, it follows that u is a string in L.

62
Chapter 2.
Finite Automata and Regular Languages
Now let k ≥1. Then we can write u = vwc, where v is a string in B,
w is the concatenation of k −1 strings of B, and c is a string of C. Deﬁne
y = wc. Observe that y is the concatenation of k −1 strings of B followed
by one string of C. Therefore, by induction, the string y is an element of L.
Hence, u = vy, where v is a string in B and y is a string in L. This shows
that u is a string in BL. Hence, u is a string in BL ∪C. Since BL ∪C = L,
it follows that u is a string in L. This completes the proof that B∗C ⊆L.
It remains to show that L ⊆B∗C. Let u be an arbitrary string in L,
and let ℓbe its length (i.e., ℓis the number of symbols in u). We prove by
induction on ℓthat u is a string in B∗C.
The base case is when ℓ= 0. Then u = ǫ. Since u ∈L and L = BL ∪C,
u is a string in BL ∪C. Since ǫ ̸∈B, u cannot be a string in BL. Hence, u
must be a string in C. Since C ⊆B∗C, it follows that u is a string in B∗C.
Let ℓ≥1. If u is a string in C, then u is a string in B∗C and we are done.
So assume that u is not a string in C. Since u ∈L and L = BL ∪C, u is a
string in BL. Hence, there are strings b ∈B and v ∈L such that u = bv.
Since ǫ ̸∈B, the length of b is at least one; hence, the length of v is less than
the length of u. By induction, v is a string in B∗C. Hence, u = bv, where
b ∈B and v ∈B∗C. This shows that u ∈B(B∗C). Since B(B∗C) ⊆B∗C,
it follows that u ∈B∗C.
Note that Lemma 2.8.2 holds for any language B that does not contain
the empty string ǫ. As an example, assume that B = ∅. Then the language
L satisﬁes the equation
L = BL ∪C = ∅L ∪C.
Using Theorem 2.7.4, this equation becomes
L = ∅∪C = C.
We now show that Lemma 2.8.2 also implies that L = C: Since ǫ ̸∈B,
Lemma 2.8.2 implies that L = B∗C, which, using Theorem 2.7.4, becomes
L = B∗C = ∅∗C = ǫC = C.
The conversion
We will now use Lemma 2.8.2 to prove that every DFA can be converted to
a regular expression.

2.8.
Equivalence of regular expressions and regular languages 63
Let M = (Q, Σ, δ, q, F) be an arbitrary deterministic ﬁnite automaton.
We will show that there exists a regular expression that describes the lan-
guage L(M).
For each state r ∈Q, we deﬁne
Lr = {w ∈Σ∗:
the path in the state diagram of M that starts
in state r and that corresponds to w ends in a
state of F }.
In words, Lr is the language accepted by M, if r were the start state.
We will show that each such language Lr can be described by a regular
expression. Since L(M) = Lq, this will prove that L(M) can be described by
a regular expression.
The basic idea is to set up equations for the languages Lr, which we then
solve using Lemma 2.8.2. We claim that
Lr =
[
a∈Σ
a · Lδ(r,a)
if r ̸∈F.
(2.2)
Why is this true? Let w be a string in Lr. Then the path P in the state
diagram of M that starts in state r and that corresponds to w ends in a
state of F. Since r ̸∈F, this path contains at least one edge. Let r′ be the
state that follows the ﬁrst state (i.e., r) of P. Then r′ = δ(r, b) for some
symbol b ∈Σ. Hence, b is the ﬁrst symbol of w. Write w = bv, where v is
the remaining part of w. Then the path P ′ = P \ {r} in the state diagram
of M that starts in state r′ and that corresponds to v ends in a state of F.
Therefore, v ∈Lr′ = Lδ(r,b). Hence,
w ∈b · Lδ(r,b) ⊆
[
a∈Σ
a · Lδ(r,a).
Conversely, let w be a string in S
a∈Σ a· Lδ(r,a). Then there is a symbol b ∈Σ
and a string v ∈Lδ(r,b) such that w = bv. Let P ′ be the path in the state
diagram of M that starts in state δ(r, b) and that corresponds to v. Since
v ∈Lδ(r,b), this path ends in a state of F. Let P be the path in the state
diagram of M that starts in r, follows the edge to δ(r, b), and then follows P ′.
This path P corresponds to w and ends in a state of F. Therefore, w ∈Lr.
This proves the correctness of (2.2).

64
Chapter 2.
Finite Automata and Regular Languages
Similarly, we can prove that
Lr = ǫ ∪
 [
a∈Σ
a · Lδ(r,a)
!
if r ∈F.
(2.3)
So we now have a set of equations in the “unknowns” Lr, for r ∈Q. The
number of equations is equal to the size of Q. In other words, the number
of equations is equal to the number of unknowns. The regular expression for
L(M) = Lq is obtained by solving these equations using Lemma 2.8.2.
Of course, we have to convince ourselves that these equations have a so-
lution for any given DFA. Before we deal with this issue, we give an example.
An example
Consider the deterministic ﬁnite automaton M = (Q, Σ, δ, q0, F), where Q =
{q0, q1, q2}, Σ = {a, b}, q0 is the start state, F = {q2}, and δ is given in the
state diagram below. We show how to obtain the regular expression that
describes the language accepted by M.
q0
q1
q2
a
a
a
b
b
b
For this case, (2.2) and (2.3) give the following equations:



Lq0
=
a · Lq0 ∪b · Lq2
Lq1
=
a · Lq0 ∪b · Lq1
Lq2
=
ǫ ∪a · Lq1 ∪b · Lq0

2.8.
Equivalence of regular expressions and regular languages 65
In the third equation, Lq2 is expressed in terms of Lq0 and Lq1. Hence, if we
substitute the third equation into the ﬁrst one, and use Theorem 2.7.4, then
we get
Lq0
=
a · Lq0 ∪b · (ǫ ∪a · Lq1 ∪b · Lq0)
=
(a ∪bb) · Lq0 ∪ba · Lq1 ∪b.
We obtain the following set of equations.
 Lq0
=
(a ∪bb) · Lq0 ∪ba · Lq1 ∪b
Lq1
=
b · Lq1 ∪a · Lq0
Let L = Lq1, B = b, and C = a · Lq0. Then ǫ ̸∈B and the second equation
reads L = BL ∪C. Hence, by Lemma 2.8.2,
Lq1 = L = B∗C = b∗a · Lq0.
If we substitute Lq1 into the ﬁrst equation, then we get (again using Theo-
rem 2.7.4)
Lq0
=
(a ∪bb) · Lq0 ∪ba · b∗a · Lq0 ∪b
=
(a ∪bb ∪bab∗a)Lq0 ∪b.
Again applying Lemma 2.8.2, this time with L = Lq0, B = a∪bb∪bab∗a and
C = b, gives
Lq0 = (a ∪bb ∪bab∗a)∗b.
Thus, the regular expression that describes the language accepted by M is
(a ∪bb ∪bab∗a)∗b.
Completing the correctness of the conversion
It remains to prove that, for any DFA, the system of equations (2.2) and (2.3)
can be solved. This will follow from the following (more general) lemma.
(You should verify that the equations (2.2) and (2.3) are in the form as
speciﬁed in this lemma.)

66
Chapter 2.
Finite Automata and Regular Languages
Lemma 2.8.3 Let n ≥1 be an integer and, for 1 ≤i ≤n and 1 ≤j ≤n,
let Bij and Ci be regular expressions such that ǫ ̸∈Bij. Let L1, L2, . . . , Ln be
languages that satisfy
Li =
 n[
j=1
BijLj
!
∪Ci for 1 ≤i ≤n.
Then L1 can be expressed as a regular expression only involving the regular
expressions Bij and Ci.
Proof. The proof is by induction on n. The base case is when n = 1. In
this case, we have
L1 = B11L1 ∪C1.
Since ǫ ̸∈B11, it follows from Lemma 2.8.2 that L1 = B∗
11C1. This proves
the base case.
Let n ≥2 and assume the lemma is true for n −1. We have
Ln
=
 n[
j=1
BnjLj
!
∪Cn
=
BnnLn ∪
 n−1
[
j=1
BnjLj
!
∪Cn.
Since ǫ ̸∈Bnn, it follows from Lemma 2.8.2 that
Ln
=
B∗
nn
  n−1
[
j=1
BnjLj
!
∪Cn
!
=
B∗
nn
 n−1
[
j=1
BnjLj
!
∪B∗
nnCn
=
 n−1
[
j=1
B∗
nnBnjLj
!
∪B∗
nnCn

2.9.
The pumping lemma and nonregular languages
67
By substituting this equation for Ln into the equations for Li, 1 ≤i ≤n−1,
we obtain
Li
=
 n[
j=1
BijLj
!
∪Ci
=
BinLn ∪
 n−1
[
j=1
BijLj
!
∪Ci
=
 n−1
[
j=1
(BinB∗
nnBnj ∪Bij) Lj
!
∪BinB∗
nnCn ∪Ci.
Thus, we have obtained n −1 equations in L1, L2, . . . , Ln−1.
Since ǫ ̸∈
BinB∗
nnBnj ∪Bij, it follows from the induction hypothesis that L1 can be
expressed as a regular expression only involving the regular expressions Bij
and Ci.
2.9
The pumping lemma and nonregular lan-
guages
In the previous sections, we have seen that the class of regular languages is
closed under various operations, and that these languages can be described by
(deterministic or nondeterministic) ﬁnite automata and regular expressions.
These properties helped in developing techniques for showing that a language
is regular. In this section, we will present a tool that can be used to prove
that certain languages are not regular. Observe that for a regular language,
1. the amount of memory that is needed to determine whether or not a
given string is in the language is ﬁnite and independent of the length
of the string, and
2. if the language consists of an inﬁnite number of strings, then this lan-
guage should contain inﬁnite subsets having a fairly repetitive struc-
ture.
Intuitively, languages that do not follow 1. or 2. should be nonregular. For
example, consider the language
{0n1n : n ≥0}.

68
Chapter 2.
Finite Automata and Regular Languages
This language should be nonregular, because it seems unlikely that a DFA can
remember how many 0s it has seen when it has reached the border between
the 0s and the 1s. Similarly the language
{0n : n is a prime number}
should be nonregular, because the prime numbers do not seem to have any
repetitive structure that can be used by a DFA. To be more rigorous about
this, we will establish a property that all regular languages must possess.
This property is called the pumping lemma. If a language does not have this
property, then it must be nonregular.
The pumping lemma states that any suﬃciently long string in a regular
language can be pumped, i.e., there is a section in that string that can be
repeated any number of times, so that the resulting strings are all in the
language.
Theorem 2.9.1 (Pumping Lemma for Regular Languages) Let A be
a regular language. Then there exists an integer p ≥1, called the pumping
length, such that the following holds: Every string s in A, with |s| ≥p, can
be written as s = xyz, such that
1. y ̸= ǫ (i.e., |y| ≥1),
2. |xy| ≤p, and
3. for all i ≥0, xyiz ∈A.
In words, the pumping lemma states that by replacing the portion y in s
by zero or more copies of it, the resulting string is still in the language A.
Proof. Let Σ be the alphabet of A. Since A is a regular language, there
exists a DFA M = (Q, Σ, δ, q, F) that accepts A. We deﬁne p to be the
number of states in Q.
Let s = s1s2 . . . sn be an arbitrary string in A such that n ≥p. Deﬁne
r1 = q, r2 = δ(r1, s1), r3 = δ(r2, s2), . . ., rn+1 = δ(rn, sn). Thus, when the
DFA M reads the string s from left to right, it visits the states r1, r2, . . . , rn+1.
Since s is a string in A, we know that rn+1 belongs to F.
Consider the ﬁrst p + 1 states r1, r2, . . . , rp+1 in this sequence. Since the
number of states of M is equal to p, the pigeonhole principle implies that
there must be a state that occurs twice in this sequence. That is, there are
indices j and ℓsuch that 1 ≤j < ℓ≤p + 1 and rj = rℓ.

2.9.
The pumping lemma and nonregular languages
69
q = r1
rn+1
r j = rℓ
read x
read y
read z
We deﬁne x = s1s2 . . . sj−1, y = sj . . . sℓ−1, and z = sℓ. . . sn. Since j < ℓ,
we have y ̸= ǫ, proving the ﬁrst claim in the theorem. Since ℓ≤p + 1, we
have |xy| = ℓ−1 ≤p, proving the second claim in the theorem. To see that
the third claim also holds, recall that the string s = xyz is accepted by M.
While reading x, M moves from the start state q to state rj. While reading
y, it moves from state rj to state rℓ= rj, i.e., after having read y, M is again
in state rj. While reading z, M moves from state rj to the accept state rn+1.
Therefore, the substring y can be repeated any number i ≥0 of times, and
the corresponding string xyiz will still be accepted by M. It follows that
xyiz ∈A for all i ≥0.
2.9.1
Applications of the pumping lemma
First example
Consider the language
A = {0n1n : n ≥0}.
We will prove by contradiction that A is not a regular language.
Assume that A is a regular language. Let p ≥1 be the pumping length,
as given by the pumping lemma. Consider the string s = 0p1p. It is clear
that s ∈A and |s| = 2p ≥p. Hence, by the pumping lemma, s can be
written as s = xyz, where y ̸= ǫ, |xy| ≤p, and xyiz ∈A for all i ≥0.
Observe that, since |xy| ≤p, the string y contains only 0s. Moreover,
since y ̸= ǫ, y contains at least one 0. But now we are in trouble: None of
the strings xy0z = xz, xy2z = xyyz, xy3z = xyyyz, . . . , is contained in A.
However, by the pumping lemma, all these strings must be in A. Hence, we
have a contradiction and we conclude that A is not a regular language.

70
Chapter 2.
Finite Automata and Regular Languages
Second example
Consider the language
A = {w ∈{0, 1}∗: the number of 0s in w equals the number of 1s in w}.
Again, we prove by contradiction that A is not a regular language.
Assume that A is a regular language. Let p ≥1 be the pumping length,
as given by the pumping lemma. Consider the string s = 0p1p. Then s ∈A
and |s| = 2p ≥p. By the pumping lemma, s can be written as s = xyz,
where y ̸= ǫ, |xy| ≤p, and xyiz ∈A for all i ≥0.
Since |xy| ≤p, the string y contains only 0s. Since y ̸= ǫ, y contains at
least one 0. Therefore, the string xy2z = xyyz contains more 0s than 1s,
which implies that this string is not contained in A. But, by the pumping
lemma, this string is contained in A. This is a contradiction and, therefore,
A is not a regular language.
Third example
Consider the language
A = {ww : w ∈{0, 1}∗}.
We prove by contradiction that A is not a regular language.
Assume that A is a regular language. Let p ≥1 be the pumping length,
as given by the pumping lemma. Consider the string s = 0p10p1. Then s ∈A
and |s| = 2p + 2 ≥p. By the pumping lemma, s can be written as s = xyz,
where y ̸= ǫ, |xy| ≤p, and xyiz ∈A for all i ≥0.
Since |xy| ≤p, the string y contains only 0s. Since y ̸= ǫ, y contains at
least one 0. Therefore, the string xy2z = xyyz is not contained in A. But,
by the pumping lemma, this string is contained in A. This is a contradiction
and, therefore, A is not a regular language.
You should convince yourself that by choosing s = 02p (which is a string
in A whose length is at least p), we do not obtain a contradiction. The reason
is that the string y may have an even length. Thus, 02p is the “wrong” string
for showing that A is not regular. By choosing s = 0p10p1, we do obtain
a contradiction; thus, this is the “correct” string for showing that A is not
regular.

2.9.
The pumping lemma and nonregular languages
71
Fourth example
Consider the language
A = {0m1n : m > n ≥0}.
We prove by contradiction that A is not a regular language.
Assume that A is a regular language. Let p ≥1 be the pumping length,
as given by the pumping lemma. Consider the string s = 0p+11p. Then s ∈A
and |s| = 2p + 1 ≥p. By the pumping lemma, s can be written as s = xyz,
where y ̸= ǫ, |xy| ≤p, and xyiz ∈A for all i ≥0.
Since |xy| ≤p, the string y contains only 0s. Since y ̸= ǫ, y contains at
least one 0. Consider the string xy0z = xz. The number of 1s in this string
is equal to p, whereas the number of 0s is at most equal to p. Therefore, the
string xy0z is not contained in A. But, by the pumping lemma, this string
is contained in A. This is a contradiction and, therefore, A is not a regular
language.
Fifth example
Consider the language
A = {1n2 : n ≥0}.
We prove by contradiction that A is not a regular language.
Assume that A is a regular language. Let p ≥1 be the pumping length,
as given by the pumping lemma. Consider the string s = 1p2. Then s ∈A
and |s| = p2 ≥p. By the pumping lemma, s can be written as s = xyz,
where y ̸= ǫ, |xy| ≤p, and xyiz ∈A for all i ≥0.
Observe that
|s| = |xyz| = p2
and
|xy2z| = |xyyz| = |xyz| + |y| = p2 + |y|.
Since |xy| ≤p, we have |y| ≤p. Since y ̸= ǫ, we have |y| ≥1. It follows that
p2 < |xy2z| ≤p2 + p < (p + 1)2.
Hence, the length of the string xy2z is strictly between two consecutive
squares.
It follows that this length is not a square and, therefore, xy2z
is not contained in A. But, by the pumping lemma, this string is contained
in A. This is a contradiction and, therefore, A is not a regular language.

72
Chapter 2.
Finite Automata and Regular Languages
Sixth example
Consider the language
A = {1n : n is a prime number}.
We prove by contradiction that A is not a regular language.
Assume that A is a regular language. Let p ≥1 be the pumping length,
as given by the pumping lemma. Let n ≥p be a prime number, and consider
the string s = 1n. Then s ∈A and |s| = n ≥p. By the pumping lemma, s
can be written as s = xyz, where y ̸= ǫ, |xy| ≤p, and xyiz ∈A for all i ≥0.
Let k be the integer such that y = 1k. Since y ̸= ǫ, we have k ≥1. For
each i ≥0, n + (i −1)k is a prime number, because xyiz = 1n+(i−1)k ∈A.
For i = n + 1, however, we have
n + (i −1)k = n + nk = n(1 + k),
which is not a prime number, because n ≥2 and 1 + k ≥2.
This is a
contradiction and, therefore, A is not a regular language.
Seventh example
Consider the language
A = {w ∈{0, 1}∗:
the number of occurrences of 01 in w is equal to
the number of occurrences of 10 in w }.
Since this language has the same ﬂavor as the one in the second example,
we may suspect that A is not a regular language. This is, however, not true:
As we will show, A is a regular language.
The key property is the following one: Let w be an arbitrary string in
{0, 1}∗. Then
the absolute value of the number of occurrences of 01 in w minus
the number of occurrences of 10 in w is at most one.
This property holds, because between any two consecutive occurrences of
01, there must be exactly one occurrence of 10. Similarly, between any two
consecutive occurrences of 10, there must be exactly one occurrence of 01.
We will construct a DFA that accepts A. This DFA uses the following
ﬁve states:

2.9.
The pumping lemma and nonregular languages
73
• q: start state; no symbol has been read.
• q01: the last symbol read was 1; in the part of the string read so far, the
number of occurrences of 01 is one more than the number of occurrences
of 10.
• q10: the last symbol read was 0; in the part of the string read so far, the
number of occurrences of 10 is one more than the number of occurrences
of 01.
• q0
equal: the last symbol read was 0; in the part of the string read so far,
the number of occurrences of 01 is equal to the number of occurrences
of 10.
• q1
equal: the last symbol read was 1; in the part of the string read so far,
the number of occurrences of 01 is equal to the number of occurrences
of 10.
The set of accept states is equal to {q, q0
equal, q1
equal}. The state diagram of
the DFA is given below.
q0
equal
q1
equal
q01
q10
q
0
0
1
1
0
1
0
0
1
1
In fact, the key property mentioned above implies that the language A
consists of the empty string ǫ and all non-empty binary strings that start

74
Chapter 2.
Finite Automata and Regular Languages
and end with the same symbol. As a result, A is the language described by
the regular expression
ǫ ∪0 ∪1 ∪0(0 ∪1)∗0 ∪1(0 ∪1)∗1.
This gives an alternative proof for the fact that A is a regular language.
Eighth example
Consider the language
L = {w ∈{0, 1}∗: w is the binary representation of a prime number}.
We assume that for any positive integer, the leftmost bit in its binary repre-
sentation is 1. In other words, we assume that there are no 0’s added to the
left of such a binary representation. Thus,
L = {10, 11, 101, 111, 1011, 1101, 10001, . . .}.
We will prove that L is not a regular language.
Assume that L is a regular language. Let p ≥1 be the pumping length.
Let N > 2p be a prime number and let s ∈{0, 1}∗be the binary representa-
tion of N. Observe that |s| ≥p + 1. Also, the leftmost and rightmost bits of
s are 1.
Since s ∈L and |s| ≥p + 1 ≥p, the Pumping Lemma implies that we
can write s = xyz, such that
1. |y| ≥1,
2. |xy| ≤p (and, thus, |z| ≥1), and
3. for all i ≥0, xyiz ∈L, i.e., xyiz is the binary representation of a prime
number.
Deﬁne A, B, and C to be the integers whose binary representations are
x, y, and z, respectively. Note that both y and z may have leading 0’s. In
fact, y may be a string consisting of 0’s only, in which case B = 0. However,
since the rightmost bit of z is 1, we have C ≥1. Observe that
N = C + B · 2|z| + A · 2|z|+|y|.
(2.4)

2.9.
The pumping lemma and nonregular languages
75
Let i = N, consider the bitstring xyiz = xyNz, and let M be the prime
number whose binary representation is given by this bitstring. Then,
M
=
C +
N−1
X
k=0
B · 2|z|+k|y| + A · 2|z|+N|y|
=
C + B · 2|z|
N−1
X
k=0
2k|y| + A · 2|z|+N|y|.
Let
T =
N−1
X
k=0
2k|y|.
Then
 2|y| −1

T = 2N|y| −1.
(2.5)
By Fermat’s Little Theorem, we have
2N ≡2
(mod N),
implying that
2N|y| −1 =
 2N|y| −1 ≡2|y| −1
(mod N).
Thus, (2.5) implies that
 2|y| −1

T ≡2|y| −1
(mod N).
(2.6)
Observe that 2|y| ≤2p < N, because |y| ≤|xy| ≤p. Also, 2|y| ≥2, because
y ̸= ǫ. It follows that
1 ≤2|y| −1 < N,
implying that
2|y| −1 ̸≡0
(mod N).
This, together with (2.6), implies that
T ≡1
(mod N).
Since
M = C + B · 2|z| · T + A · 2|z|+N|y|,

76
Chapter 2.
Finite Automata and Regular Languages
it follows that
M ≡C + B · 2|z| + A · 2|z|+|y|
(mod N).
This, together with (2.4), implies that
M ≡0
(mod N),
i.e., N divides M. Since M > N, we conclude that M is not a prime number,
which is a contradiction. Thus, the language L is not regular.
2.10
Higman’s Theorem
Let Σ be a ﬁnite alphabet. For any two strings x and y in Σ∗, we say that x
is a subsequence of y, if x can be obtained by deleting zero or more symbols
from y. For example, 10110 is a subsequence of 0010010101010001. For any
language L ⊆Σ∗, we deﬁne
SUBSEQ(L) := {x : there exists a y ∈L such that x is a subsequence of y}.
That is, SUBSEQ(L) is the language consisting of the subsequences of all
strings in L. In 1952, Higman proved the following result:
Theorem 2.10.1 (Higman) For any ﬁnite alphabet Σ and for any lan-
guage L ⊆Σ∗, the language SUBSEQ(L) is regular.
2.10.1
Dickson’s Theorem
Our proof of Higman’s Theorem will use a theorem that was proved in 1913
by Dickson.
Recall that N denotes the set of positive integers. Let n ∈N. For any
two points p = (p1, p2, . . . , pn) and q = (q1, q2, . . . , qn) in Nn, we say that p is
dominated by q, if pi ≤qi for all i with 1 ≤i ≤n.
Theorem 2.10.2 (Dickson) Let S ⊆Nn, and let M be the set consisting of
all elements of S that are minimal in the relation “is dominated by”. Thus,
M = {q ∈S : there is no p in S \ {q} such that p is dominated by q}.
Then, the set M is ﬁnite.

2.10.
Higman’s Theorem
77
We will prove this theorem by induction on the dimension n. If n = 1,
then either M = ∅(if S = ∅) or M consists of exactly one element (if S ̸= ∅).
Therefore, the theorem holds if n = 1. Let n ≥2 and assume the theorem
holds for all subsets of Nn−1. Let S be a subset of Nn and consider the set
M of minimal elements in S. If S = ∅, then M = ∅and, thus, M is ﬁnite.
Assume that S ̸= ∅. We ﬁx an arbitrary element q in M. If p ∈M \ {q},
then q is not dominated by p. Therefore, there exists an index i such that
pi ≤qi −1. It follows that
M \ {q} ⊆
n[
i=1
 Ni−1 × [1, qi −1] × Nn−i
.
For all i and k with 1 ≤i ≤n and 1 ≤k ≤qi −1, we deﬁne
Sik = {p ∈S : pi = k}
and
Mik = {p ∈M : pi = k}.
Then,
M \ {q} =
n[
i=1
qi−1
[
k=1
Mik.
(2.7)
Lemma 2.10.3 Mik is a subset of the set of all elements of Sik that are
minimal in the relation “is dominated by”.
Proof. Let p be an element of Mik, and assume that p is not minimal in
Sik. Then there is an element r in Sik, such that r ̸= p and r is dominated
by p. Since p and r are both elements of S, it follows that p ̸∈M. This is a
contradiction.
Since the set Sik is basically a subset of Nn−1, it follows from the induction
hypothesis that Sik contains ﬁnitely many minimal elements. This, combined
with Lemma 2.10.3, implies that Mik is a ﬁnite set. Thus, by (2.7), M \ {q}
is the union of ﬁnitely many ﬁnite sets. Therefore, the set M is ﬁnite.
2.10.2
Proof of Higman’s Theorem
We give the proof of Theorem 2.10.1 for the case when Σ = {0, 1}. If L = ∅
or SUBSEQ(L) = {0, 1}∗, then SUBSEQ(L) is obviously a regular language.

78
Chapter 2.
Finite Automata and Regular Languages
Hence, we may assume that L is non-empty and SUBSEQ(L) is a proper
subset of {0, 1}∗.
We ﬁx a string z of length at least two in the complement SUBSEQ(L) of
the language SUBSEQ(L). Observe that this is possible, because SUBSEQ(L)
is an inﬁnite language. We insert 0s and 1s into z, such that, in the result-
ing string z′, 0s and 1s alternate. For example, if z = 0011101011, then
z′ = 01010101010101. Let n = |z′| −1, where |z′| denotes the length of z′.
Then, n ≥|z| −1 ≥1.
A (0, 1)-alternation in a binary string x is any occurrence of 01 or 10 in x.
For example, the string 1101001 contains four (0, 1)-alternations. We deﬁne
A = {x ∈{0, 1}∗: x has at most n many (0, 1)-alternations}.
Lemma 2.10.4 SUBSEQ(L) ⊆A.
Proof. Let x ∈SUBSEQ(L) and assume that x ̸∈A. Then, x has at least
n + 1 = |z′| many (0, 1)-alternations and, therefore, z′ is a subsequence of x.
In particular, z is a subsequence of x. Since x ∈SUBSEQ(L), it follows that
z ∈SUBSEQ(L), which is a contradiction.
Lemma 2.10.5 SUBSEQ(L) =

A ∩SUBSEQ(L)

∪A.
Proof. Follows from Lemma 2.10.4.
Lemma 2.10.6 The language A is regular.
Proof.
The complement A of A is the language consisting of all binary
strings with at least n + 1 many (0, 1)-alternations. If, for example, n = 3,
then A is described by the regular expression
(00∗11∗00∗11∗0(0 ∪1)∗) ∪(11∗00∗11∗00∗1(0 ∪1)∗) .
This should convince you that the claim is true for any value of n.
For any b ∈{0, 1} and for any k ≥0, we deﬁne Abk to be the language
consisting of all binary strings that start with a b and have exactly k many
(0, 1)-alternations. Then, we have
A = {ǫ} ∪
 1[
b=0
n[
k=0
Abk
!
.

2.10.
Higman’s Theorem
79
Thus, if we deﬁne
Fbk = Abk ∩SUBSEQ(L),
and use the fact that ǫ ∈SUBSEQ(L) (which is true because L ̸= ∅), then
A ∩SUBSEQ(L) =
1[
b=0
n[
k=0
Fbk.
(2.8)
For any b ∈{0, 1} and for any k ≥0, consider the relation “is a subse-
quence of” on the language Fbk. We deﬁne Mbk to be the language consisting
of all strings in Fbk that are minimal in this relation. Thus,
Mbk = {x ∈Fbk : there is no x′ in Fbk \ {x} such that x′ is a subsequence of x}.
It is clear that
Fbk =
[
x∈Mbk
{y ∈Fbk : x is a subsequence of y}.
If x ∈Mbk, y ∈Abk, and x is a subsequence of y, then y must be in
SUBSEQ(L) and, therefore, y must be in Fbk. To prove this, assume that
y ∈SUBSEQ(L).
Then, x ∈SUBSEQ(L), contradicting the fact that
x ∈Mbk ⊆Fbk ⊆SUBSEQ(L). It follows that
Fbk =
[
x∈Mbk
{y ∈Abk : x is a subsequence of y}.
(2.9)
Lemma 2.10.7 Let b ∈{0, 1} and 0 ≤k ≤n, and let x be an element of
Mbk. Then, the language
{y ∈Abk : x is a subsequence of y}
is regular.
Proof. We will prove the claim by means of an example. Assume that b = 1,
k = 3, and x = 11110001000. Then, the language
{y ∈Abk : x is a subsequence of y}
is described by the regular expression
11111∗0000∗11∗0000∗.
This should convince you that the claim is true in general.

80
Chapter 2.
Finite Automata and Regular Languages
Lemma 2.10.8 For each b ∈{0, 1} and each 0 ≤k ≤n, the set Mbk is
ﬁnite.
Proof. Again, we will prove the claim by means of an example. Assume
that b = 1 and k = 3. Any string in Fbk can be written as 1a0b1c0d, for some
integers a, b, c, d ≥1. Consider the function ϕ : Fbk →N4 that is deﬁned by
ϕ(1a0b1c0d) = (a, b, c, d). Then, ϕ is an injective function, and the following
is true, for any two strings x and x′ in Fbk:
x is a subsequence of x′ if and only if ϕ(x) is dominated by ϕ(x′).
It follows that the elements of Mbk are in one-to-one correspondence with
those elements of ϕ(Fbk) that are minimal in the relation “is dominated by”.
The lemma thus follows from Dickson’s Theorem.
Now we can complete the proof of Higman’s Theorem:
• It follows from (2.9) and Lemmas 2.10.7 and 2.10.8, that Fbk is the
union of ﬁnitely many regular languages. Therefore, by Theorem 2.3.1,
Fbk is a regular language.
• It follows from (2.8) that A∩SUBSEQ(L) is the union of ﬁnitely many
regular languages. Therefore, again by Theorem 2.3.1, A∩SUBSEQ(L)
is a regular language.
• Since A ∩SUBSEQ(L) is regular and, by Lemma 2.10.6, A is regular,
it follows from Lemma 2.10.5 that SUBSEQ(L) is the union of two reg-
ular languages. Therefore, by Theorem 2.3.1, SUBSEQ(L) is a regular
language.
• Since SUBSEQ(L) is regular, it follows from Theorem 2.6.4 that the
language SUBSEQ(L) is regular as well.
Exercises
2.1 For each of the following languages, construct a DFA that accepts the
language. In all cases, the alphabet is {0, 1}.
1. {w : the length of w is divisible by three}

Exercises
81
2. {w : 110 is not a substring of w}
3. {w : w contains at least ﬁve 1s}
4. {w : w contains the substring 1011}
5. {w : w contains at least two 1s and at most two 0s}
6. {w : w contains an odd number of 1s or exactly two 0s}
7. {w : w begins with 1 and ends with 0}
8. {w : every odd position in w is 1}
9. {w : w has length at least 3 and its third symbol is 0}
10. {ǫ, 0}
2.2 For each of the following languages, construct an NFA, with the speciﬁed
number of states, that accepts the language. In all cases, the alphabet is
{0, 1}.
1. The language {w : w ends with 10} with three states.
2. The language {w : w contains the substring 1011} with ﬁve states.
3. The language {w : w contains an odd number of 1s or exactly two 0s}
with six states.
2.3 For each of the following languages, construct an NFA that accepts the
language. In all cases, the alphabet is {0, 1}.
1. {w : w contains the substring 11001}
2. {w : w has length at least 2 and does not end with 10}
3. {w : w begins with 1 or ends with 0}
2.4 Convert the following NFA to an equivalent DFA.

82
Chapter 2.
Finite Automata and Regular Languages
1
2
a
b
a, b
2.5 Convert the following NFA to an equivalent DFA.
1
3
2
a
a
b
a
ε,b
2.6 Convert the following NFA to an equivalent DFA.
0
1
2
3
a, ǫ
b
a
ǫ
b
2.7 In the proof of Theorem 2.6.3, we introduced a new start state q0, which
is also an accept state. Explain why the following is not a valid proof of
Theorem 2.6.3:
Let N = (Q1, Σ, δ1, q1, F1) be an NFA, such that A = L(N). Deﬁne the
NFA M = (Q1, Σ, δ, q1, F), where

Exercises
83
1. F = {q1} ∪F1.
2. δ : Q1 × Σǫ →P(Q1) is deﬁned as follows: For any r ∈Q1 and for any
a ∈Σǫ,
δ(r, a) =



δ1(r, a)
if r ∈Q1 and r ̸∈F1,
δ1(r, a)
if r ∈F1 and a ̸= ǫ,
δ1(r, a) ∪{q1}
if r ∈F1 and a = ǫ.
Then L(M) = A∗.
2.8 Prove Theorem 2.6.4.
2.9 Let A be a language over the alphabet Σ = {0, 1} and let A be the
complement of A. Thus, A is the language consisting of all binary strings
that are not in A.
Assume that A is a regular language. Let M = (Q, Σ, δ, q, F) be a non-
deterministic ﬁnite automaton (NFA) that accepts A.
Consider the NFA N = (Q, Σ, δ, q, F), where F = Q\F is the complement
of F. Thus, N is obtained from M by turning all accept states into nonaccept
states, and turning all nonaccept states into accept states.
1. Is it true that the language accepted by N is equal to A?
2. Assume now that M is a deterministic ﬁnite automaton (DFA) that
accepts A. Deﬁne N as above; thus, turn all accept states into nonac-
cept states, and turn all nonaccept states into accept states. Is it true
that the language accepted by N is equal to A?
2.10 Recall the alternative deﬁnition for the star of a language A that we
gave just before Theorem 2.3.1.
In Theorems 2.3.1 and 2.6.2, we have shown that the class of regular
languages is closed under the union and concatenation operations.
Since
A∗= S∞
k=0 Ak, why doesn’t this imply that the class of regular languages is
closed under the star operation?
2.11 Let A and B be two regular languages over the same alphabet Σ. Prove
that the diﬀerence of A and B, i.e., the language
A \ B = {w : w ∈A and w ̸∈B}
is a regular language.

84
Chapter 2.
Finite Automata and Regular Languages
2.12 For each of the following regular expressions, give two strings that are
members and two strings that are not members of the language described by
the expression. The alphabet is Σ = {a, b}.
1. a(ba)∗b.
2. (a ∪b)∗a(a ∪b)∗b(a ∪b)∗a(a ∪b)∗.
3. (a ∪ba ∪bb)(a ∪b)∗.
2.13 Give regular expressions describing the following languages.
In all
cases, the alphabet is {0, 1}.
1. {w : w contains at least three 1s}.
2. {w : w contains at least two 1s and at most one 0},
3. {w : w contains an even number of 0s and exactly two 1s}.
4. {w : w contains exactly two 0s and at least two 1s}.
5. {w : w contains an even number of 0s and each 0 is followed by at least one 1}.
6. {w : every odd position in w is 1}.
2.14 Convert each of the following regular expressions to an NFA.
1. (0 ∪1)∗000(0 ∪1)∗
2. (((10)∗(00)) ∪10)∗
3. ((0 ∪1)(11)∗∪0)∗
2.15 Convert the following DFA to a regular expression.

Exercises
85
1
2
3
a
a
b
b
a
b
2.16 Convert the following DFA to a regular expression.
1
2
3
a, b
a
a
b
b
2.17 Convert the following DFA to a regular expression.
a, b
2.18
1. Let A be a non-empty regular language. Prove that there exists
an NFA that accepts A and that has exactly one accept state.

86
Chapter 2.
Finite Automata and Regular Languages
2. For any string w = w1w2 . . . wn, we denote by wR the string obtained
by reading w backwards, i.e., wR = wnwn−1 . . . w2w1. For any language
A, we deﬁne AR to be the language obtained by reading all strings in
A backwards, i.e.,
AR = {wR : w ∈A}.
Let A be a non-empty regular language. Prove that the language AR
is also regular.
2.19 If n ≥1 is an integer and w = a1a2 . . . an is a string, then for any i
with 0 ≤i < n, the string a1a2 . . . ai is called a proper preﬁx of w. (If i = 0,
then a1a2 . . . ai = ǫ.)
For any language L, we deﬁne MIN(L) to be the language
MIN(L) = {w ∈L : no proper preﬁx of w belongs to L}.
Prove the following claim: If L is a regular language, then MIN(L) is regular
as well.
2.20 Use the pumping lemma to prove that the following languages are not
regular.
1. {anbmcn+m : n ≥0, m ≥0}.
2. {anbnc2n : n ≥0}.
3. {anbman : n ≥0, m ≥0}.
4. {a2n : n ≥0}. (Remark: a2n is the string consisting of 2n many a’s.)
5. {anbmck : n ≥0, m ≥0, k ≥0, n2 + m2 = k2}.
6. {uvu : u ∈{a, b}∗, u ̸= ǫ, v ∈{a, b}∗}.
2.21 Prove that the language
{ambn : m ≥0, n ≥0, m ̸= n}
is not regular. (Using the pumping lemma for this one is a bit tricky. You
can avoid using the pumping lemma by combining results about the closure
under regular operations.)

Exercises
87
2.22
1. Give an example of a regular language A and a non-regular lan-
guage B for which A ⊆B.
2. Give an example of a non-regular language A and a regular language
B for which A ⊆B.
2.23 Let A be a language consisting of ﬁnitely many strings.
1. Prove that A is a regular language.
2. Let n be the maximum length of any string in A. Prove that every
deterministic ﬁnite automaton (DFA) that accepts A has at least n+ 1
states. (Hint: How is the pumping length chosen in the proof of the
pumping lemma?)
2.24 Let L be a regular language, let M be a DFA whose language is equal
to L, and let p be the number of states of M. Prove that L ̸= ∅if and only
if L contains a string of length less than p.
2.25 Let L be a regular language, let M be a DFA whose language is equal
to L, and let p be the number of states of M. Prove that L is an inﬁnite
language if and only if L contains a string w with p ≤|w| ≤2p −1.
2.26 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,
L ⊆Σ∗. We deﬁne a binary relation RL on Σ∗× Σ∗, in the following way:
For any two strings u and u′ in Σ∗,
uRLu′ if and only if (∀v ∈Σ∗: uv ∈L ⇔u′v ∈L) .
Prove that RL is an equivalence relation.
2.27 Let Σ = {0, 1}, let
L = {w ∈Σ∗: |w| is odd},
and consider the relation RL deﬁned in Exercise 2.26.
1. Prove that for any two strings u and u′ in Σ∗,
uRLu′ ⇔|u| −|u′| is even.

88
Chapter 2.
Finite Automata and Regular Languages
2. Determine all equivalence classes of the relation RL.
2.28 Let Σ be a non-empty alphabet, and let L be a language over Σ, i.e.,
L ⊆Σ∗. Recall the equivalence relation RL that was deﬁned in Exercise 2.26.
1. Assume that L is a regular language, and let M = (Q, Σ, δ, q0, F) be
a DFA that accepts L. Let u and u′ be strings in Σ∗. Let q be the
state reached, when following the path in the state diagram of M, that
starts in q0 and that is obtained by reading the string u. Similarly, let
q′ be the state reached, when following the path in the state diagram
of M, that starts in q0 and that is obtained by reading the string u′.
Prove the following: If q = q′, then uRLu′.
2. Prove the following claim: If L is a regular language, then the equiva-
lence relation RL has a ﬁnite number of equivalence classes.
2.29 Let L be the language deﬁned by
L = {uuR : u ∈{0, 1}∗}.
In words, a string is in L if and only if its length is even, and the second half
is the reverse of the ﬁrst half. Consider the equivalence relation RL that was
deﬁned in Exercise 2.26.
1. Let m and n be two distinct positive integers and consider the two
strings u = 0m1 and u′ = 0n1. Prove that ¬(uRLu′).
2. Prove that L is not a regular language, without using the pumping
lemma.
3. Use the pumping lemma to prove that L is not a regular language.
2.30 In this exercise, we will show that the converse of the pumping lemma
does, in general, not hold. Consider the language
A = {ambncn : m ≥1, n ≥0} ∪{bnck : n ≥0, k ≥0}.
1. Show that A satisﬁes the conclusion of the pumping lemma for p = 1.
Thus, show that every string s in A whose length is at least p can be
written as s = xyz, such that y ̸= ǫ, |xy| ≤p, and xyiz ∈A for all
i ≥0.

Exercises
89
2. Consider the equivalence relation RA that was deﬁned in Exercise 2.26.
Let n and n′ be two distinct non-negative integers and consider the two
strings u = abn and u′ = abn′. Prove that ¬(uRAu′).
3. Prove that A is not a regular language.

90
Chapter 2.
Finite Automata and Regular Languages

Chapter 3
Context-Free Languages
In this chapter, we introduce the class of context-free languages.
As we
will see, this class contains all regular languages, as well as some nonregular
languages such as {0n1n : n ≥0}.
The class of context-free languages consists of languages that have some
sort of recursive structure. We will see two equivalent methods to obtain this
class. We start with context-free grammars, which are used for deﬁning the
syntax of programming languages and their compilation. Then we introduce
the notion of (nondeterministic) pushdown automata, and show that these
automata have the same power as context-free grammars.
3.1
Context-free grammars
We start with an example. Consider the following ﬁve (substitution) rules:
S
→
AB
A
→
a
A
→
aA
B
→
b
B
→
bB
Here, S, A, and B are variables, S is the start variable, and a and b are
terminals. We use these rules to derive strings consisting of terminals (i.e.,
elements of {a, b}∗), in the following manner:
1. Initialize the current string to be the string consisting of the start
variable S.

92
Chapter 3.
Context-Free Languages
2. Take any variable in the current string and take any rule that has this
variable on the left-hand side. Then, in the current string, replace this
variable by the right-hand side of the rule.
3. Repeat 2. until the current string only contains terminals.
For example, the string aaaabb can be derived in the following way:
S
⇒
AB
⇒
aAB
⇒
aAbB
⇒
aaAbB
⇒
aaaAbB
⇒
aaaabB
⇒
aaaabb
This derivation can also be represented using a parse tree, as in the ﬁgure
below:
S
A
A
A
A
a
a
a
a
b
b
B
B
The ﬁve rules in this example constitute a context-free grammar. The
language of this grammar is the set of all strings that

3.1.
Context-free grammars
93
• can be derived from the start variable and
• only contain terminals.
For this example, the language is
{ambn : m ≥1, n ≥1},
because every string of the form ambn, for some m ≥1 and n ≥1, can be
derived from the start variable, whereas no other string over the alphabet
{a, b} can be derived from the start variable.
Deﬁnition 3.1.1 A context-free grammar is a 4-tuple G = (V, Σ, R, S),
where
1. V is a ﬁnite set, whose elements are called variables,
2. Σ is a ﬁnite set, whose elements are called terminals,
3. V ∩Σ = ∅,
4. S is an element of V ; it is called the start variable,
5. R is a ﬁnite set, whose elements are called rules. Each rule has the
form A →w, where A ∈V and w ∈(V ∪Σ)∗.
In our example, we have V = {S, A, B}, Σ = {a, b}, and
R = {S →AB, A →a, A →aA, B →b, B →bB}.
Deﬁnition 3.1.2 Let G = (V, Σ, R, S) be a context-free grammar. Let A be
an element in V and let u, v, and w be strings in (V ∪Σ)∗such that A →w
is a rule in R. We say that the string uwv can be derived in one step from
the string uAv, and write this as
uAv ⇒uwv.
In other words, by applying the rule A →w to the string uAv, we obtain
the string uwv. In our example, we see that aaAbb ⇒aaaAbb.
Deﬁnition 3.1.3 Let G = (V, Σ, R, S) be a context-free grammar. Let u
and v be strings in (V ∪Σ)∗. We say that v can be derived from u, and write
this as u
∗⇒v, if one of the following two conditions holds:

94
Chapter 3.
Context-Free Languages
1. u = v or
2. there exist an integer k ≥2 and a sequence u1, u2, . . . , uk of strings in
(V ∪Σ)∗, such that
(a) u = u1,
(b) v = uk, and
(c) u1 ⇒u2 ⇒. . . ⇒uk.
In other words, by starting with the string u and applying rules zero or
more times, we obtain the string v. In our example, we see that aaAbB
∗⇒
aaaabbbB.
Deﬁnition 3.1.4 Let G = (V, Σ, R, S) be a context-free grammar.
The
language of G is deﬁned to be the set of all strings in Σ∗that can be derived
from the start variable S:
L(G) = {w ∈Σ∗: S
∗⇒w}.
Deﬁnition 3.1.5 A language L is called context-free, if there exists a context-
free grammar G such that L(G) = L.
3.2
Examples of context-free grammars
3.2.1
Properly nested parentheses
Consider the context-free grammar G = (V, Σ, R, S), where V = {S}, Σ =
{a, b}, and
R = {S →ǫ, S →aSb, S →SS}.
We write the three rules in R as
S →ǫ|aSb|SS,
where you can think of “|” as being a short-hand for “or”.

3.2.
Examples of context-free grammars
95
By applying the rules in R, starting with the start variable S, we obtain,
for example,
S
⇒
SS
⇒
aSbS
⇒
aSbSS
⇒
aSSbSS
⇒
aaSbSbSS
⇒
aabSbSS
⇒
aabbSS
⇒
aabbaSbS
⇒
aabbabS
⇒
aabbabaSb
⇒
aabbabab
What is the language L(G) of this context-free grammar G? If we think
of a as being a left-parenthesis “(”, and of b as being a right-parenthesis “)”,
then L(G) is the language consisting of all strings of properly nested paren-
theses. Here is the explanation: Any string of properly nested parentheses is
either
• empty (which we derive from S by the rule S →ǫ),
• consists of a left-parenthesis, followed by an arbitrary string of properly
nested parentheses, followed by a right-parenthesis (these are derived
from S by ﬁrst applying the rule S →aSb), or
• consists of an arbitrary string of properly nested parentheses, followed
by an arbitrary string of properly nested parentheses (these are derived
from S by ﬁrst applying the rule S →SS).
3.2.2
A context-free grammar for a nonregular lan-
guage
Consider the language L1 = {0n1n : n ≥0}. We have seen in Section 2.9.1
that L1 is not a regular language. We claim that L1 is a context-free language.

96
Chapter 3.
Context-Free Languages
In order to prove this claim, we have to construct a context-free grammar
G1 such that L(G1) = L1.
Observe that any string in L1 is either
• empty or
• consists of a 0, followed by an arbitrary string in L1, followed by a 1.
This leads to the context-free grammar G1 = (V1, Σ, R1, S1), where V1 =
{S1}, Σ = {0, 1}, and R1 consists of the rules
S1 →ǫ|0S11.
Hence, R1 = {S1 →ǫ, S1 →0S11}.
To derive the string 0n1n from the start variable S1, we do the following:
• Starting with S1, apply the rule S1 →0S11 exactly n times. This gives
the string 0nS11n.
• Apply the rule S1 →ǫ. This gives the string 0n1n.
It is not diﬃcult to see that these are the only strings that can be derived
from the start variable S1. Thus, L(G1) = L1.
In a symmetric way, we see that the context-free grammar G2 = (V2, Σ, R2, S2),
where V2 = {S2}, Σ = {0, 1}, and R2 consists of the rules
S2 →ǫ|1S20,
has the property that L(G2) = L2, where L2 = {1n0n : n ≥0}. Thus, L2 is
a context-free language.
Deﬁne L = L1 ∪L2, i.e.,
L = {0n1n : n ≥0} ∪{1n0n : n ≥0}.
The context-free grammar G = (V, Σ, R, S), where V = {S, S1, S2}, Σ =
{0, 1}, and R consists of the rules
S
→
S1|S2
S1
→
ǫ|0S11
S2
→
ǫ|1S20,
has the property that L(G) = L. Hence, L is a context-free language.

3.2.
Examples of context-free grammars
97
3.2.3
A context-free grammar for the complement of
a nonregular language
Let L be the (nonregular) language L = {0n1n : n ≥0}. We want to prove
that the complement L of L is a context-free language. Hence, we want to
construct a context-free grammar G whose language is equal to L. Observe
that a binary string w is in L if and only if
1. w = 0m1n, for some integers m and n with 0 ≤m < n, or
2. w = 0m1n, for some integers m and n with 0 ≤n < m, or
3. w contains 10 as a substring.
Thus, we can write L as the union of the languages of all strings of type 1.,
type 2., and type 3.
Any string of type 1. is either
• the string 1,
• consists of a string of type 1., followed by one 1, or
• consists of one 0, followed by an arbitrary string of type 1., followed by
one 1.
Thus, using the rules
S1 →1|S11|0S11,
we can derive, from S1, all strings of type 1.
Similarly, using the rules
S2 →0|0S2|0S21,
we can derive, from S2, all strings of type 2.
Any string of type 3.
• consists of an arbitrary binary string, followed by the string 10, followed
by an arbitrary binary string.
Using the rules
X →ǫ|0X|1X,

98
Chapter 3.
Context-Free Languages
we can derive, from X, all binary strings. Thus, by combining these with
the rule
S3 →X10X,
we can derive, from S3, all strings of type 3.
We arrive at the context-free grammar G = (V, Σ, R, S), where V =
{S, S1, S2, S3, X}, Σ = {0, 1}, and R consists of the rules
S
→
S1|S2|S3
S1
→
1|S11|0S11
S2
→
0|0S2|0S21
S3
→
X10X
X
→
ǫ|0X|1X
To summarize, we have
S1
∗⇒0m1n, for all integers m and n with 0 ≤m < n,
S2
∗⇒0m1n, for all integers m and n with 0 ≤n < m,
X
∗⇒u, for each string u in {0, 1}∗,
and
S3
∗⇒w, for every binary string w that contains 10 as a substring.
From these observations, it follows that that L(G) = L.
3.2.4
A context-free grammar that veriﬁes addition
Consider the language
L = {anbmcn+m : n ≥0, m ≥0}.
Using the pumping lemma for regular languages (Theorem 2.9.1), it can
be shown that L is not a regular language. We will construct a context-
free grammar G whose language is equal to L, thereby proving that L is a
context-free language.
First observe that ǫ ∈L. Therefore, we will take S →ǫ to be one of the
rules in the grammar.
Let us see how we can derive all strings in L from the start variable S:

3.3.
Regular languages are context-free
99
1. Every time we add an a, we also add a c. In this way, we obtain all
strings of the form ancn, where n ≥0.
2. Given a string of the form ancn, we start adding bs. Every time we add
a b, we also add a c. Observe that every b has to be added between
the as and the cs. Therefore, we use a variable B as a “pointer” to
the position in the current string where a b can be added: Instead of
deriving ancn from S, we derive the string anBcn. Then, from B, we
derive all strings of the form bmcm, where m ≥0.
We obtain the context-free grammar G = (V, Σ, R, S), where V = {S, A, B},
Σ = {a, b, c}, and R consists of the rules
S
→
ǫ|A
A
→
ǫ|aAc|B
B
→
ǫ|bBc
The facts that
• A
∗⇒anBcn, for every n ≥0,
• B
∗⇒bmcm, for every m ≥0,
imply that the following strings can be derived from the start variable S:
• S
∗⇒anBcn
∗⇒anbmcmcn = anbmcn+m, for all n ≥0 and m ≥0.
In fact, no other strings in {a, b, c}∗can be derived from S. Therefore, we
have L(G) = L. Since
S ⇒A ⇒B ⇒ǫ,
we can simplify this grammar G, by eliminating the rules S →ǫ and A →ǫ.
This gives the context-free grammar G′ = (V, Σ, R′, S), where V = {S, A, B},
Σ = {a, b, c}, and R′ consists of the rules
S
→
A
A
→
aAc|B
B
→
ǫ|bBc
Finally, observe that we do not need S; instead, we can use A as start
variable. This gives our ﬁnal context-free grammar G′′ = (V, Σ, R′′, A), where
V = {A, B}, Σ = {a, b, c}, and R′′ consists of the rules
A
→
aAc|B
B
→
ǫ|bBc

100
Chapter 3.
Context-Free Languages
3.3
Regular languages are context-free
We mentioned already that the class of context-free languages includes the
class of regular languages. In this section, we will prove this claim.
Theorem 3.3.1 Let Σ be an alphabet and let L ⊆Σ∗be a regular language.
Then L is a context-free language.
Proof.
Since L is a regular language, there exists a deterministic ﬁnite
automaton M = (Q, Σ, δ, q, F) that accepts L.
To prove that L is context-free, we have to deﬁne a context-free grammar
G = (V, Σ, R, S), such that L = L(M) = L(G). Thus, G must have the
following property: For every string w ∈Σ∗,
w ∈L(M) if and only if w ∈L(G),
which can be reformulated as
M accepts w if and only if S
∗⇒w.
We will deﬁne the context-free grammar G in such a way that the following
correspondence holds for any string w = w1w2 . . . wn:
• Assume that M is in state A just after it has read the substring
w1w2 . . . wi.
• Then in the context-free grammar G, we have S
∗⇒w1w2 . . . wiA.
In the next step, M reads the symbol wi+1 and switches from state A to,
say, state B; thus, δ(A, wi+1) = B. In order to guarantee that the above
correspondence still holds, we have to add the rule A →wi+1B to G.
Consider the moment when M has read the entire string w. Let A be the
state M is in at that moment. By the above correspondence, we have
S
∗⇒w1w2 . . . wnA = wA.
Recall that G must have the property that
M accepts w if and only if S
∗⇒w,
which is equivalent to
A ∈F if and only if S
∗⇒w.

3.3.
Regular languages are context-free
101
We guarantee this property by adding to G the rule A →ǫ for every accept
state A of M.
We are now ready to give the formal deﬁnition of the context-free gram-
mar G = (V, Σ, R, S):
• V = Q, i.e., the variables of G are the states of M.
• S = q, i.e., the start variable of G is the start state of M.
• R consists of the rules
A →aB, where A ∈Q, a ∈Σ, B ∈Q, and δ(A, a) = B,
and
A →ǫ, where A ∈F.
In words,
• every transition δ(A, a) = B of M (i.e., when M is in the state A and
reads the symbol a, it switches to the state B) corresponds to a rule
A →aB in the grammar G,
• every accept state A of M corresponds to a rule A →ǫ in the grammar
G.
We claim that L(G) = L. In order to prove this, we have to show that
L(G) ⊆L and L ⊆L(G).
We prove that L ⊆L(G). Let w = w1w2 . . . wn be an arbitrary string
in L. When the ﬁnite automaton M reads the string w, it visits the states
r0, r1, . . . , rn, where
• r0 = q, and
• ri+1 = δ(ri, wi+1) for i = 0, 1, . . . , n −1.
Since w ∈L = L(M), we know that rn ∈F.
It follows from the way we deﬁned the grammar G that
• for each i = 0, 1, . . . , n −1, ri →wi+1ri+1 is a rule in R, and
• rn →ǫ is a rule in R.

102
Chapter 3.
Context-Free Languages
Therefore, we have
S = q = r0 ⇒w1r1 ⇒w1w2r2 ⇒. . . ⇒w1w2 . . . wnrn ⇒w1w2 . . . wn = w.
This proves that w ∈L(G).
The proof of the claim that L(G) ⊆L is left as an exercise.
In Sections 2.9.1 and 3.2.2, we have seen that the language {0n1n : n ≥
0} is not regular, but context-free. Therefore, the class of all context-free
languages properly contains the class of regular languages.
3.3.1
An example
Let L be the language deﬁned as
L = {w ∈{0, 1}∗: 101 is a substring of w}.
In Section 2.2.2, we have seen that L is a regular language. In that section,
we constructed the following deterministic ﬁnite automaton M that accepts
L (we have renamed the states):
0
1
1
0
0
1
0,1
S
A
B
C
We apply the construction given in the proof of Theorem 3.3.1 to convert
M to a context-free grammar G whose language is equal to L. According
to this construction, we have G = (V, Σ, R, S), where V = {S, A, B, C},
Σ = {0, 1}, the start variable S is the start state of M, and R consists of the
rules
S
→
0S|1A
A
→
0B|1A
B
→
0S|1C
C
→
0C|1C|ǫ

3.4.
Chomsky normal form
103
Consider the string 010011011, which is an element of L. When the ﬁnite
automaton M reads this string, it visits the states
S, S, A, B, S, A, A, B, C, C.
In the grammar G, this corresponds to the derivation
S
⇒
0S
⇒
01A
⇒
010B
⇒
0100S
⇒
01001A
⇒
010011A
⇒
0100110B
⇒
01001101C
⇒
010011011C
⇒
010011011.
Hence,
S
∗⇒010011011,
implying that the string 010011011 is in the language L(G) of the context-free
grammar G.
The string 10011 is not in the language L. When the ﬁnite automaton
M reads this string, it visits the states
S, A, B, S, A, A,
i.e., after the string has been read, M is in the non-accept state A. In the
grammar G, reading the string 10011 corresponds to the derivation
S
⇒
1A
⇒
10B
⇒
100S
⇒
1001A
⇒
10011A.
Since A is not an accept state in M, the grammar G does not contain the
rule A →ǫ. This implies that the string 10011 cannot be derived from the
start variable S. Thus, 10011 is not in the language L(G) of G.

104
Chapter 3.
Context-Free Languages
3.4
Chomsky normal form
The rules in a context-free grammar G = (V, Σ, R, S) are of the form
A →w,
where A is a variable and w is a string over the alphabet V ∪Σ. In this
section, we show that every context-free grammar G can be converted to a
context-free grammar G′, such that L(G) = L(G′), and the rules of G′ are of
a restricted form, as speciﬁed in the following deﬁnition:
Deﬁnition 3.4.1 A context-free grammar G = (V, Σ, R, S) is said to be in
Chomsky normal form, if every rule in R has one of the following three forms:
1. A →BC, where A, B, and C are elements of V , B ̸= S, and C ̸= S.
2. A →a, where A is an element of V and a is an element of Σ.
3. S →ǫ, where S is the start variable.
You should convince yourself that, for such a grammar, R contains the
rule S →ǫ if and only if ǫ ∈L(G).
Theorem 3.4.2 Let Σ be an alphabet and let L ⊆Σ∗be a context-free lan-
guage. There exists a context-free grammar in Chomsky normal form, whose
language is L.
Proof. Since L is a context-free language, there exists a context-free gram-
mar G = (V, Σ, R, S), such that L(G) = L. We will transform G into a
grammar that is in Chomsky normal form and whose language is equal to
L(G). The transformation consists of ﬁve steps.
Step 1: Eliminate the start variable from the right-hand side of the rules.
We deﬁne G1 = (V1, Σ, R1, S1), where S1 is the start variable (which is a
new variable), V1 = V ∪{S1}, and R1 = R ∪{S1 →S}. This grammar has
the property that
• the start variable S1 does not occur on the right-hand side of any rule
in R1, and
• L(G1) = L(G).

3.4.
Chomsky normal form
105
Step 2: An ǫ-rule is a rule that is of the form A →ǫ, where A is a variable
that is not equal to the start variable. In the second step, we eliminate all
ǫ-rules from G1.
We consider all ǫ-rules, one after another. Let A →ǫ be one such rule,
where A ∈V1 and A ̸= S1. We modify G1 as follows:
1. Remove the rule A →ǫ from the current set R1.
2. For each rule in the current set R1 that is of the form
(a) B →A, add the rule B →ǫ to R1, unless this rule has already
been deleted from R1; observe that in this way, we replace the two-
step derivation B ⇒A ⇒ǫ by the one-step derivation B ⇒ǫ;
(b) B →uAv (where u and v are strings that are not both empty),
add the rule B →uv to R1; observe that in this way, we replace
the two-step derivation B ⇒uAv ⇒uv by the one-step derivation
B ⇒uv;
(c) B →uAvAw (where u, v, and w are strings), add the rules B →
uvw, B →uAvw, and B →uvAw to R1; if u = v = w = ǫ and
the rule B →ǫ has already been deleted from R1, then we do not
add the rule B →ǫ;
(d) treat rules in which A occurs more than twice on the right-hand
side in a similar fashion.
We repeat this process until all ǫ-rules have been eliminated.
Let R2
be the set of rules, after all ǫ-rules have been eliminated. We deﬁne G2 =
(V2, Σ, R2, S2), where V2 = V1 and S2 = S1. This grammar has the property
that
• the start variable S2 does not occur on the right-hand side of any rule
in R2,
• R2 does not contain any ǫ-rule (it may contain the rule S2 →ǫ), and
• L(G2) = L(G1) = L(G).
Step 3: A unit-rule is a rule that is of the form A →B, where A and B are
variables. In the third step, we eliminate all unit-rules from G2.

106
Chapter 3.
Context-Free Languages
We consider all unit-rules, one after another. Let A →B be one such
rule, where A and B are elements of V2. We know that B ̸= S2. We modify
G2 as follows:
1. Remove the rule A →B from the current set R2.
2. For each rule in the current set R2 that is of the form B →u, where
u ∈(V2 ∪Σ)∗, add the rule A →u to the current set R2, unless this is
a unit-rule that has already been eliminated.
Observe that in this way, we replace the two-step derivation A ⇒B ⇒
u by the one-step derivation A ⇒u.
We repeat this process until all unit-rules have been eliminated.
Let
R3 be the set of rules, after all unit-rules have been eliminated. We deﬁne
G3 = (V3, Σ, R3, S3), where V3 = V2 and S3 = S2. This grammar has the
property that
• the start variable S3 does not occur on the right-hand side of any rule
in R3,
• R3 does not contain any ǫ-rule (it may contain the rule S3 →ǫ),
• R3 does not contain any unit-rule, and
• L(G3) = L(G2) = L(G1) = L(G).
Step 4: Eliminate all rules having more than two symbols on the right-hand
side.
For each rule in the current set R3 that is of the form A →u1u2 . . . uk,
where k ≥3 and each ui is an element of V3 ∪Σ, we modify G3 as follows:
1. Remove the rule A →u1u2 . . . uk from the current set R3.
2. Add the following rules to the current set R3:
A
→
u1A1
A1
→
u2A2
A2
→
u3A3
...
Ak−3
→
uk−2Ak−2
Ak−2
→
uk−1uk

3.4.
Chomsky normal form
107
where A1, A2, . . . , Ak−2 are new variables that are added to the current
set V3.
Observe that in this way, we replace the one-step derivation A ⇒
u1u2 . . . uk by the (k −1)-step derivation
A ⇒u1A1 ⇒u1u2A2 ⇒. . . ⇒u1u2 . . . uk−2Ak−2 ⇒u1u2 . . . uk.
Let R4 be the set of rules, and let V4 be the set of variables, after all rules
with more than two symbols on the right-hand side have been eliminated. We
deﬁne G4 = (V4, Σ, R4, S4), where S4 = S3. This grammar has the property
that
• the start variable S4 does not occur on the right-hand side of any rule
in R4,
• R4 does not contain any ǫ-rule (it may contain the rule S4 →ǫ),
• R4 does not contain any unit-rule,
• R4 does not contain any rule with more than two symbols on the right-
hand side, and
• L(G4) = L(G3) = L(G2) = L(G1) = L(G).
Step 5: Eliminate all rules of the form A →u1u2, where u1 and u2 are not
both variables.
For each rule in the current set R4 that is of the form A →u1u2, where
u1 and u2 are elements of V4 ∪Σ, but u1 and u2 are not both contained in
V4, we modify G3 as follows:
1. If u1 ∈Σ and u2 ∈V4, then replace the rule A →u1u2 in the current
set R4 by the two rules A →U1u2 and U1 →u1, where U1 is a new
variable that is added to the current set V4.
Observe that in this way, we replace the one-step derivation A ⇒u1u2
by the two-step derivation A ⇒U1u2 ⇒u1u2.
2. If u1 ∈V4 and u2 ∈Σ, then replace the rule A →u1u2 in the current
set R4 by the two rules A →u1U2 and U2 →u2, where U2 is a new
variable that is added to the current set V4.
Observe that in this way, we replace the one-step derivation A ⇒u1u2
by the two-step derivation A ⇒u1U2 ⇒u1u2.

108
Chapter 3.
Context-Free Languages
3. If u1 ∈Σ, u2 ∈Σ, and u1 ̸= u2, then replace the rule A →u1u2 in the
current set R4 by the three rules A →U1U2, U1 →u1, and U2 →u2,
where U1 and U2 are new variables that are added to the current set
V4.
Observe that in this way, we replace the one-step derivation A ⇒u1u2
by the three-step derivation A ⇒U1U2 ⇒u1U2 ⇒u1u2.
4. If u1 ∈Σ, u2 ∈Σ, and u1 = u2, then replace the rule A →u1u2 = u1u1
in the current set R4 by the two rules A →U1U1 and U1 →u1, where
U1 is a new variable that is added to the current set V4.
Observe that in this way, we replace the one-step derivation A ⇒
u1u2 = u1u1 by the three-step derivation A ⇒U1U1 ⇒u1U1 ⇒u1u1.
Let R5 be the set of rules, and let V5 be the set of variables, after Step 5
has been completed. We deﬁne G5 = (V5, Σ, R5, S5), where S5 = S4. This
grammar has the property that
• the start variable S5 does not occur on the right-hand side of any rule
in R5,
• R5 does not contain any ǫ-rule (it may contain the rule S5 →ǫ),
• R5 does not contain any unit-rule,
• R5 does not contain any rule with more than two symbols on the right-
hand side,
• R5 does not contain any rule of the form A →u1u2, where u1 and u2
are not both variables of V5, and
• L(G5) = L(G4) = L(G3) = L(G2) = L(G1) = L(G).
Since the grammar G5 is in Chomsky normal form, the proof is complete.

3.4.
Chomsky normal form
109
3.4.1
An example
Consider the context-free grammar G = (V, Σ, R, A), where V = {A, B},
Σ = {0, 1}, A is the start variable, and R consists of the rules
A
→
BAB|B|ǫ
B
→
00|ǫ
We apply the construction given in the proof of Theorem 3.4.2 to convert
this grammar to a context-free grammar in Chomsky normal form whose
language is the same as that of G. Throughout the construction, upper case
letters will denote variables.
Step 1: Eliminate the start variable from the right-hand side of the rules.
We introduce a new start variable S, and add the rule S →A. This gives
the following grammar:
S
→
A
A
→
BAB|B|ǫ
B
→
00|ǫ
Step 2: Eliminate all ǫ-rules.
We take the ǫ-rule A →ǫ, and remove it. Then we consider all rules that
contain A on the right-hand side. There are two such rules:
• S →A; we add the rule S →ǫ;
• A →BAB; we add the rule A →BB.
This gives the following grammar:
S
→
A|ǫ
A
→
BAB|B|BB
B
→
00|ǫ
We take the ǫ-rule B →ǫ, and remove it. Then we consider all rules that
contain B on the right-hand side. There are three such rules:
• A →BAB; we add the rules A →AB, A →BA, and A →A;
• A →B; we do not add the rule A →ǫ, because it has already been
removed;

110
Chapter 3.
Context-Free Languages
• A →BB; we add the rule A →B, but not the rule A →ǫ (because it
has already been removed).
At this moment, we have the following grammar:
S
→
A|ǫ
A
→
BAB|B|BB|AB|BA|A
B
→
00
Since all ǫ-rules have been eliminated, this completes Step 2. (Observe that
the rule S →ǫ is allowed, because S is the start variable.)
Step 3: Eliminate all unit-rules.
We take the unit-rule A →A. We can remove this rule, without adding
any new rule. At this moment, we have the following grammar:
S
→
A|ǫ
A
→
BAB|B|BB|AB|BA
B
→
00
We take the unit-rule S →A, remove it, and add the rules
S →BAB|B|BB|AB|BA.
This gives the following grammar:
S
→
ǫ|BAB|B|BB|AB|BA
A
→
BAB|B|BB|AB|BA
B
→
00
We take the unit-rule S →B, remove it, and add the rule S →00. This
gives the following grammar:
S
→
ǫ|BAB|BB|AB|BA|00
A
→
BAB|B|BB|AB|BA
B
→
00
We take the unit-rule A →B, remove it, and add the rule A →00. This
gives the following grammar:
S
→
ǫ|BAB|BB|AB|BA|00
A
→
BAB|BB|AB|BA|00
B
→
00

3.5.
Pushdown automata
111
Since all unit-rules have been eliminated, this concludes Step 3.
Step 4: Eliminate all rules having more than two symbols on the right-hand
side. There are two such rules:
• We take the rule S →BAB, remove it, and add the rules S →BA1
and A1 →AB.
• We take the rule A →BAB, remove it, and add the rules A →BA2
and A2 →AB.
This gives the following grammar:
S
→
ǫ|BB|AB|BA|00|BA1
A
→
BB|AB|BA|00|BA2
B
→
00
A1
→
AB
A2
→
AB
Step 4 is now completed.
Step 5: Eliminate all rules, whose right-hand side contains exactly two
symbols, which are not both variables. There are three such rules:
• We replace the rule S →00 by the rules S →A3A3 and A3 →0.
• We replace the rule A →00 by the rules A →A4A4 and A4 →0.
• We replace the rule B →00 by the rules B →A5A5 and A5 →0.
This gives the following grammar, which is in Chomsky normal form:
S
→
ǫ|BB|AB|BA|BA1|A3A3
A
→
BB|AB|BA|BA2|A4A4
B
→
A5A5
A1
→
AB
A2
→
AB
A3
→
0
A4
→
0
A5
→
0

112
Chapter 3.
Context-Free Languages
3.5
Pushdown automata
In this section, we introduce nondeterministic pushdown automata. As we
will see, the class of languages that can be accepted by these automata is
exactly the class of context-free languages.
We start with an informal description of a deterministic pushdown au-
tomaton. Such an automaton consists of the following, see also Figure 3.1.
1. There is a tape which is divided into cells. Each cell stores a symbol
belonging to a ﬁnite set Σ, called the tape alphabet. There is a special
symbol 2 that is not contained in Σ; this symbol is called the blank
symbol. If a cell contains 2, then this means that the cell is actually
empty.
2. There is a tape head which can move along the tape, one cell to the
right per move. This tape head can also read the cell it currently scans.
3. There is a stack containing symbols from a ﬁnite set Γ, called the stack
alphabet. This set contains a special symbol $.
4. There is a stack head which can read the top symbol of the stack. This
head can also pop the top symbol, and it can push symbols of Γ onto
the stack.
5. There is a state control, which can be in any one of a ﬁnite number
of states. The set of states is denoted by Q. The set Q contains one
special state q, called the start state.
The input for a pushdown automaton is a string in Σ∗. This input string
is stored on the tape of the pushdown automaton and, initially, the tape head
is on the leftmost symbol of the input string. Initially, the stack only contains
the special symbol $, and the pushdown automaton is in the start state q.
In one computation step, the pushdown automaton does the following:
1. Assume that the pushdown automaton is currently in state r. Let a be
the symbol of Σ that is read by the tape head, and let A be the symbol
of Γ that is on top of the stack.
2. Depending on the current state r, the tape symbol a, and the stack
symbol A,

3.5.
Pushdown automata
113
state control
a a b a b b a b a b 2
tape
6
$
A
A
B
A
stack
-
Figure 3.1: A pushdown automaton.
(a) the pushdown automaton switches to a state r′ of Q (which may
be equal to r),
(b) the tape head either moves one cell to the right or stays at the
current cell, and
(c) the top symbol A is replaced by a string w that belongs to Γ∗. To
be more precise,
i. if w = ǫ, then A is popped from the stack, whereas
ii. if w = B1B2 . . . Bk, with k ≥1 and B1, B2, . . . , Bk ∈Γ, then
A is replaced by w, and Bk becomes the new top symbol of
the stack.
Later, we will specify when the pushdown automaton accepts the input
string.
We now give a formal deﬁnition of a deterministic pushdown automaton.
Deﬁnition 3.5.1 A deterministic pushdown automaton is a 5-tuple M =
(Σ, Γ, Q, δ, q), where

114
Chapter 3.
Context-Free Languages
1. Σ is a ﬁnite set, called the tape alphabet; the blank symbol 2 is not
contained in Σ,
2. Γ is a ﬁnite set, called the stack alphabet; this alphabet contains the
special symbol $,
3. Q is a ﬁnite set, whose elements are called states,
4. q is an element of Q; it is called the start state,
5. δ is called the transition function, which is a function
δ : Q × (Σ ∪{2}) × Γ →Q × {N, R} × Γ∗.
The transition function δ can be thought of as being the “program” of the
pushdown automaton. This function tells us what the automaton can do in
one “computation step”: Let r ∈Q, a ∈Σ ∪{2}, and A ∈Γ. Furthermore,
let r′ ∈Q, σ ∈{R, N}, and w ∈Γ∗be such that
δ(r, a, A) = (r′, σ, w).
(3.1)
This transition means that if
• the pushdown automaton is in state r,
• the tape head reads the symbol a, and
• the top symbol on the stack is A,
then
• the pushdown automaton switches to state r′,
• the tape head moves according to σ: if σ = R, then it moves one cell
to the right; if σ = N, then it does not move, and
• the top symbol A on the stack is replaced by the string w.
We will write the computation step (3.1) in the form of the instruction
raA →r′σw.
We now specify the computation of the pushdown automaton M = (Σ, Γ, Q, δ, q).

3.6.
Examples of pushdown automata
115
Start conﬁguration: Initially, the pushdown automaton is in the start state
q, the tape head is on the leftmost symbol of the input string a1a2 . . . an, and
the stack only contains the special symbol $.
Computation and termination: Starting in the start conﬁguration, the
pushdown automaton performs a sequence of computation steps as described
above. It terminates at the moment when the stack becomes empty. (Hence,
if the stack never gets empty, the pushdown automaton does not terminate.)
Acceptance: The pushdown automaton accepts the input string a1a2 . . . an ∈
Σ∗, if
1. the automaton terminates on this input, and
2. at the time of termination (i.e., at the moment when the stack gets
empty), the tape head is on the cell immediately to the right of the cell
containing the symbol an (this cell must contain the blank symbol 2).
In all other cases, the pushdown automaton rejects the input string. Thus,
the pushdown automaton rejects this string if
1. the automaton does not terminate on this input (i.e., the computation
“loops forever”) or
2. at the time of termination, the tape head is not on the cell immediately
to the right of the cell containing the symbol an.
We denote by L(M) the language accepted by the pushdown automaton
M. Thus,
L(M) = {w ∈Σ∗: M accepts w}.
The pushdown automaton described above is deterministic. For a non-
deterministic pushdown automata, the current computation step may not
be uniquely deﬁned, but the automaton can make a choice out of a ﬁnite
number of possibilities. In this case, the transition function δ is a function
δ : Q × (Σ ∪{2}) × Γ →Pf(Q × {N, R} × Γ∗),
where Pf(K) is the set of all ﬁnite subsets of the set K.
We say that a nondeterministic pushdown automaton M accepts an input
string, if there exists an accepting computation, in the sense as described for
deterministic pushdown automata. We say that M rejects an input string, if
every computation on this string is rejecting. As before, we denote by L(M)
the set of all strings in Σ∗that are accepted by M.

116
Chapter 3.
Context-Free Languages
3.6
Examples of pushdown automata
3.6.1
Properly nested parentheses
We will show how to construct a deterministic pushdown automaton, that
accepts the set of all strings of properly nested parentheses. Observe that a
string w in {(, )}∗is properly nested if and only if
• in every preﬁx of w, the number of “(” is greater than or equal to the
number of “)”, and
• in the complete string w, the number of “(” is equal to the number of
“)”.
We will use the tape symbol a for “(”, and the tape symbol b for “)”.
The idea is as follows. Recall that initially, the stack only contains the
special symbol $. The pushdown automaton reads the input string from left
to right. For every a it reads, it pushes the symbol S onto the stack, and
for every b it reads, it pops the top symbol from the stack. In this way, the
number of symbols S on the stack will always be equal to the number of as
that have been read minus the number of bs that have been read; additionally,
the bottom of the stack will contain the special symbol $. The input string
is properly nested if and only if (i) this diﬀerence is always non-negative and
(ii) this diﬀerence is zero once the entire input string has been read. Hence,
the input string is accepted if and only if during this process, (i) the stack
always contains at least the special symbol $ and (ii) at the end, the stack
only contains the special symbol $ (which will then be popped in the ﬁnal
step).
Based on this discussion, we obtain the deterministic pushdown automa-
ton M = (Σ, Γ, Q, δ, q), where Σ = {a, b}, Γ = {$, S}, Q = {q}, and the
transition function δ is speciﬁed by the following instructions:

3.6.
Examples of pushdown automata
117
qa$ →qR$S
because of the a, S is pushed onto the stack
qaS →qRSS
because of the a, S is pushed onto the stack
qbS →qRǫ
because of the b, the top element is popped
from the stack
qb$ →qNǫ
the number of bs read is larger than the number
of as read; the stack is made empty (hence,
the computation terminates before the entire
string has been read), and the input string is rejected
q2$ →qNǫ
the entire input string has been read; the stack is
made empty, and the input string is accepted
q2S →qNS
the entire input string has been read, it contains
more as than bs; no changes are made (thus, the
automaton does not terminate), and the input string
is rejected
3.6.2
Strings of the form 0n1n
We construct a deterministic pushdown automata that accepts the language
{0n1n : n ≥0}.
The automaton uses two states q0 and q1, where q0 is the start state.
Initially, the automaton is in state q0.
• For each 0 that it reads, the automaton pushes one symbol S onto the
stack and stays in state q0.
• When the ﬁrst 1 is read, the automaton switches to state q1. From that
moment,
– for each 1 that is read, the automaton pops the top symbol from
the stack and stays in state q1;
– if a 0 is read, the automaton does not make any change and,
therefore, does not terminate.
Based on this discussion, we obtain the deterministic pushdown automa-
ton M = (Σ, Γ, Q, δ, q0), where Σ = {0, 1}, Γ = {$, S}, Q = {q0, q1}, q0 is
the start state, and the transition function δ is speciﬁed by the following
instructions:

118
Chapter 3.
Context-Free Languages
q00$ →q0R$S
push S onto the stack
q00S →q0RSS
push S onto the stack
q01$ →q0N$
ﬁrst symbol in the input is 1; loop forever
q01S →q1Rǫ
ﬁrst 1 is encountered
q02$ →q0Nǫ
input string is empty; accept
q02S →q0NS
input only consists of 0s; loop forever
q10$ →q1N$
0 to the right of 1; loop forever
q10S →q1NS
0 to the right of 1; loop forever
q11$ →q1N$
too many 1s; loop forever
q11S →q1Rǫ
pop top symbol from the stack
q12$ →q1Nǫ
accept
q12S →q1NS
too many 0s; loop forever
3.6.3
Strings with b in the middle
We will construct a nondeterministic pushdown automaton that accepts the
set L of all strings in {a, b}∗having an odd length and whose middle symbol
is b, i.e.,
L = {vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|}.
The idea is as follows. The automaton uses two states q and q′, where q
is the start state. These states have the following meaning:
• If the automaton is in state q, then it has not reached the middle symbol
b of the input string.
• If the automaton is in state q′, then it has read the middle symbol b.
Observe that since the automaton can only make one single pass over the
input string, it has to “guess” (i.e., use nondeterminism) when it reaches the
middle of the string.
• If the automaton is in state q, then, when reading the current tape
symbol,
– it either pushes one symbol S onto the stack and stays in state q
– or, in case the current tape symbol is b, it “guesses” that it has
reached the middle of the input string, by switching to state q′.
• If the automaton is in state q′, then, when reading the current tape
symbol, it pops the top symbol S from the stack and stays in state q′.

3.7.
Equivalence of PDA’s and CFG’s
119
In this way, the number of symbols S on the stack will always be equal to the
diﬀerence of (i) the number of symbols in the part to the left of the middle
symbol b that have been read and (ii) the number of symbols in the part
to the right of the middle symbol b that have been read; additionally, the
bottom of the stack will contain the special symbol $.
The input string is accepted if and only if, at the moment when the blank
symbol 2 is read, the automaton is in state q′ and the top symbol on the
stack is $. In this case, the stack is made empty and, thus, the computation
terminates.
We obtain the nondeterministic pushdown automaton M = (Σ, Γ, Q, δ, q),
where Σ = {a, b}, Γ = {$, S}, Q = {q, q′}, q is the start state, and the
transition function δ is speciﬁed by the following instructions:
qa$ →qR$S
push S onto the stack
qaS →qRSS
push S onto the stack
qb$ →q′R$
reached the middle
qb$ →qR$S
did not reach the middle; push S onto the stack
qbS →q′RS
reached the middle
qbS →qRSS
did not reach the middle; push S onto the stack
q2$ →qN$
input string is empty; loop forever
q2S →qNS
loop forever
q′a$ →q′Nǫ
stack is empty; terminate, but reject, because
the entire input string has not been read
q′aS →q′Rǫ
pop top symbol from stack
q′b$ →q′Nǫ
stack is empty; terminate, but reject, because
the entire input string has not been read
q′bS →q′Rǫ
pop top symbol from stack
q′2$ →q′Nǫ
accept
q′2S →q′NS
loop forever
Remark 3.6.1 It can be shown that there is no deterministic pushdown
automaton that accepts the language L. The reason is that a deterministic
pushdown automaton cannot determine when it reaches the middle of the
input string. Thus, unlike as for ﬁnite automata, nondeterministic pushdown
automata are more powerful than their deterministic counterparts.

120
Chapter 3.
Context-Free Languages
3.7
Equivalence of pushdown automata and
context-free grammars
The main result of this section is that nondeterministic pushdown automata
and context-free grammars are equivalent in power:
Theorem 3.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then
A is context-free if and only if there exists a nondeterministic pushdown
automaton that accepts A.
We will only prove one direction of this theorem. That is, we will show
how to convert an arbitrary context-free grammar to a nondeterministic push-
down automaton.
Let G = (V, Σ, R, $) be a context-free grammar, where V is the set of
variables, Σ is the set of terminals, R is the set of rules, and $ is the start
variable. By Theorem 3.4.2, we may assume that G is in Chomsky normal
form. Hence, every rule in R has one of the following three forms:
1. A →BC, where A, B, and C are variables, B ̸= $, and C ̸= $.
2. A →a, where A is a variable and a is a terminal.
3. $ →ǫ.
We will construct a nondeterministic pushdown automaton M that ac-
cepts the language L(G) of this grammar G. Observe that M must have the
following property: For every string w = a1a2 . . . an ∈Σ∗,
w ∈L(G) if and only if M accepts w.
This can be reformulated as follows:
$
∗⇒a1a2 . . . an
if and only if there exists a computation of M that starts in the initial
conﬁguration
a1 · · ·
ai · · · an 2
6
$
-

3.7.
Equivalence of PDA’s and CFG’s
121
and ends in the conﬁguration
a1 · · ·
ai · · · an 2
6
∅
-
where ∅indicates that the stack is empty.
Assume that $
∗⇒a1a2 . . . an. Then there exists a derivation (using the
rules of R) of the string a1a2 . . . an from the start variable $. We may assume
that in each step in this derivation, a rule is applied to the leftmost variable
in the current string. Hence, because the grammar G is in Chomsky normal
form, at any moment during the derivation, the current string has the form
a1a2 . . . ai−1AkAk−1 . . . A1,
(3.2)
for some integers i and k with 1 ≤i ≤n + 1 and k ≥0, and variables
A1, A2, . . . , Ak. (In particular, at the start of the derivation, we have i = 1
and k = 1, and the current string is Ak = $. At the end of the derivation,
we have i = n + 1 and k = 0, and the current string is a1a2 . . . an.)
We will deﬁne the pushdown automaton M in such a way that the current
string (3.2) corresponds to the conﬁguration
a1 · · ·
ai · · · an 2
6
A1
...
Ak
-
Based on this discussion, we obtain the nondeterministic pushdown au-
tomaton M = (Σ, V, {q}, δ, q), where
• the tape alphabet is the set Σ of terminals of G,
• the stack alphabet is the set V of variables of G,
• the set of states consists of one state q, which is the start state, and
• the transition function δ is obtained from the rules in R, in the following
way:

122
Chapter 3.
Context-Free Languages
– For each rule in R that is of the form A →BC, with A, B, C ∈V ,
the pushdown automaton M has the instructions
qaA →qNCB, for all a ∈Σ.
– For each rule in R that is of the form A →a, with A ∈V and
a ∈Σ, the pushdown automaton M has the instruction
qaA →qRǫ.
– If R contains the rule $ →ǫ, then the pushdown automaton M
has the instruction
q2$ →qNǫ.
This concludes the deﬁnition of M. It remains to prove that L(M) =
L(G), i.e., the language of the nondeterministic pushdown automaton M is
equal to the language of the context-free grammar G. Hence, we have to
show that for every string w ∈Σ∗,
w ∈L(G) if and only if w ∈L(M),
which can be rewritten as
$
∗⇒w if and only if M accepts w.
Claim 3.7.2 Let a1a2 . . . an be a string in Σ∗, let A1, A2, . . . , Ak be variables
in V , and let i and k be integers with 1 ≤i ≤n + 1 and k ≥0. Then the
following holds:
$
∗⇒a1a2 . . . ai−1AkAk−1 . . . A1
if and only if there exists a computation of M from the initial conﬁguration
a1 · · ·
ai · · · an 2
6
$
-
to the conﬁguration

3.7.
Equivalence of PDA’s and CFG’s
123
a1 · · ·
ai · · · an 2
6
A1
...
Ak
-
Proof. The claim can be proved by induction. Let
w = a1a2 . . . ai−1AkAk−1 . . . A1.
Assume that k ≥1 and assume that the claim is true for the string w. Then
we have to show that the claim is still true after applying a rule in R to the
leftmost variable Ak in w. Since the grammar is in Chomsky normal form,
the rule to be applied is either of the form Ak →BC or of the form Ak →ai.
In both cases, the property mentioned in the claim is maintained.
We now use Claim 3.7.2 to prove that L(M) = L(G). Let w = a1a2 . . . an
be an arbitrary string in Σ∗. By applying Claim 3.7.2, with i = n + 1 and
k = 0, we see that w ∈L(G), i.e.,
$
∗⇒a1a2 . . . an,
if and only if there exists a computation of M from the initial conﬁguration
a1 · · ·
ai · · · an 2
6
$
-
to the conﬁguration
a1 · · ·
ai · · · an 2
6
∅
-
But this means that w ∈L(G) if and only if the automaton M accepts the
string w.
This concludes the proof of the fact that every context-free grammar can
be converted to a nondeterministic pushdown automaton.
As mentioned
already, we will not give the conversion in the other direction. We ﬁnish this
section with the following observation:

124
Chapter 3.
Context-Free Languages
Theorem 3.7.3 Let Σ be an alphabet and let A ⊆Σ∗be a context-free lan-
guage. Then there exists a nondeterministic pushdown automaton that ac-
cepts A and has only one state.
Proof. Since A is context-free, there exists a context-free grammar G0 such
that L(G0) = A. By Theorem 3.4.2, there exists a context-free grammar G
that is in Chomsky normal form and for which L(G) = L(G0). The construc-
tion given above converts G to a nondeterministic pushdown automaton M
that has only one state and for which L(M) = L(G).
3.8
The pumping lemma for context-free lan-
guages
In Section 2.9, we proved the pumping lemma for regular languages and
used it to prove that certain languages are not regular. In this section, we
generalize the pumping lemma to context-free languages.
The idea is to
consider the parse tree (see Section 3.1) that describes the derivation of a
suﬃciently long string in the context-free language L. Since the number of
variables in the corresponding context-free grammar G is ﬁnite, there is at
least one variable, say Aj, that occurs more than once on the longest root-
to-leaf path in the parse tree. The subtree which is sandwiched between two
occurrences of Aj on this path can be copied any number of times. This will
result in a legal parse tree and, hence, in a “pumped” string that is in the
language L.
Theorem 3.8.1 (Pumping Lemma for Context-Free Languages) Let
L be a context-free language. Then there exists an integer p ≥1, called the
pumping length, such that the following holds: Every string s in L, with
|s| ≥p, can be written as s = uvxyz, such that
1. |vy| ≥1 (i.e., v and y are not both empty),
2. |vxy| ≤p, and
3. uvixyiz ∈L, for all i ≥0.

3.8.
The pumping lemma for context-free languages
125
3.8.1
Proof of the pumping lemma
The proof of the pumping lemma will use the following result about parse
trees:
Lemma 3.8.2 Let G be a context-free grammar in Chomsky normal form,
let s be a non-empty string in L(G), and let T be a parse tree for s. Let ℓbe
the height of T, i.e., ℓis the number of edges on a longest root-to-leaf path
in T. Then
|s| ≤2ℓ−1.
Proof. The claim can be proved by induction on ℓ. By looking at some
small values of ℓand using the fact that G is in Chomsky normal form, you
should be able to verify the claim.
Now we can start with the proof of the pumping lemma. Let L be a
context-free language and let Σ be the alphabet of L. By Theorem 3.4.2, there
exists a context-free grammar in Chomsky normal form, G = (V, Σ, R, S),
such that L = L(G).
Deﬁne r to be the number of variables of G and deﬁne p = 2r. We will
prove that the value of p can be used as the pumping length. Consider an
arbitrary string s in L such that |s| ≥p, and let T be a parse tree for s. Let
ℓbe the height of T. Then, by Lemma 3.8.2, we have
|s| ≤2ℓ−1.
On the other hand, we have
|s| ≥p = 2r.
By combining these inequalities, we see that 2r ≤2ℓ−1, which can be rewrit-
ten as
ℓ≥r + 1.
Consider the nodes on a longest root-to-leaf path in T.
Since this path
consists of ℓedges, it consists of ℓ+ 1 nodes. The ﬁrst ℓof these nodes store
variables, which we denote by A0, A1, . . . , Aℓ−1 (where A0 = S), and the last
node (which is a leaf) stores a terminal, which we denote by a.
Since ℓ−1 −r ≥0, the sequence
Aℓ−1−r, Aℓ−r, . . . , Aℓ−1

126
Chapter 3.
Context-Free Languages
of variables is well-deﬁned.
Observe that this sequence consists of r + 1
variables. Since the number of variables in the grammar G is equal to r,
the pigeonhole principle implies that there is a variable that occurs at least
twice in this sequence. In other words, there are indices j and k, such that
ℓ−1 −r ≤j < k ≤ℓ−1 and Aj = Ak. Refer to the ﬁgure below for an
illustration.
S
Aj
Ak
u
v
x
y
z
s
A0 = S
A1
Aℓ−1−r
Aℓ−r
Aℓ−2
Aℓ−1
a
r +1
variables
Recall that T is a parse tree for the string s. Therefore, the terminals
stored at the leaves of T, in the order from left to right, form s. As indicated
in the ﬁgure above, the nodes storing the variables Aj and Ak partition s
into ﬁve substrings u, v, x, y, and z, such that s = uvxyz.

3.8.
The pumping lemma for context-free languages
127
It remains to prove that the three properties stated in the pumping lemma
hold. We start with the third property, i.e., we prove that
uvixyiz ∈L, for all i ≥0.
In the grammar G, we have
S
∗⇒uAjz.
(3.3)
Since Aj
∗⇒vAky and Ak = Aj, we have
Aj
∗⇒vAjy.
(3.4)
Finally, since Ak
∗⇒x and Ak = Aj, we have
Aj
∗⇒x.
(3.5)
From (3.3) and (3.5), it follows that
S
∗⇒uAjz
∗⇒uxz,
which implies that the string uxz is in the language L. Similarly, it follows
from (3.3), (3.4), and (3.5) that
S
∗⇒uAjz
∗⇒uvAjyz
∗⇒uvvAjyyz
∗⇒uvvxyyz.
Hence, the string uv2xy2z is in the language L. In general, for each i ≥0,
the string uvixyiz is in the language L, because
S
∗⇒uAjz
∗⇒uviAjyiz
∗⇒uvixyiz.
This proves that the third property in the pumping lemma holds.
Next we show that the second property holds. That is, we prove that
|vxy| ≤p.
Consider the subtree rooted at the node storing the variable
Aj.
The path from the node storing Aj to the leaf storing the terminal
a is a longest path in this subtree.
(Convince yourself that this is true.)
Moreover, this path consists of ℓ−j edges. Since Aj
∗⇒vxy, this subtree
is a parse tree for the string vxy (where Aj is used as the start variable).
Therefore, by Lemma 3.8.2, we can conclude that |vxy| ≤2ℓ−j−1. We know
that ℓ−1 −r ≤j, which is equivalent to ℓ−j −1 ≤r. It follows that
|vxy| ≤2ℓ−j−1 ≤2r = p.

128
Chapter 3.
Context-Free Languages
Finally, we show that the ﬁrst property in the pumping lemma holds.
That is, we prove that |vy| ≥1. Recall that
Aj
∗⇒vAky.
Let the ﬁrst rule used in this derivation be Aj →BC. (Since the variables
Aj and Ak, even though they are equal, are stored at diﬀerent nodes of the
parse tree, and since the grammar G is in Chomsky normal form, this ﬁrst
rule exists.) Then
Aj ⇒BC
∗⇒vAky.
Observe that the string BC has length two. Moreover, by applying rules of
a grammar in Chomsky normal form, strings cannot become shorter. (Here,
we use the fact that the start variable does not occur on the right-hand side
of any rule.) Therefore, we have |vAky| ≥2. But this implies that |vy| ≥1.
This completes the proof of the pumping lemma.
3.8.2
Applications of the pumping lemma
First example
Consider the language
A = {anbncn : n ≥0}.
We will prove by contradiction that A is not a context-free language.
Assume that A is a context-free language. Let p ≥1 be the pumping
length, as given by the pumping lemma. Consider the string s = apbpcp.
Observe that s ∈A and |s| = 3p ≥p. Hence, by the pumping lemma, s can
be written as s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all
i ≥0.
Observe that the pumping lemma does not tell us the location of the
substring vxy in the string s, it only gives us an upper bound on the length
of this substring. Therefore, we have to consider three cases, depending on
the location of vxy in s.
Case 1: The substring vxy does not contain any c.
Consider the string uv2xy2z = uvvxyyz.
Since |vy| ≥1, this string
contains more than p many as or more than p many bs. Since it contains
exactly p many cs, it follows that this string is not in the language A. This
is a contradiction because, by the pumping lemma, the string uv2xy2z is in
A.

3.8.
The pumping lemma for context-free languages
129
Case 2: The substring vxy does not contain any a.
Consider the string uv2xy2z = uvvxyyz.
Since |vy| ≥1, this string
contains more than p many bs or more than p many cs. Since it contains
exactly p many as, it follows that this string is not in the language A. This
is a contradiction because, by the pumping lemma, the string uv2xy2z is in
A.
Case 3: The substring vxy contains at least one a and at least one c.
Since s = apbpcp, this implies that |vxy| > p, which again contradicts the
pumping lemma.
Thus, in all of the three cases, we have obtained a contradiction. There-
fore, we have shown that the language A is not context-free.
Second example
Consider the languages
A = {wwR : w ∈{a, b}∗},
where wR is the string obtained by writing w backwards, and
B = {ww : w ∈{a, b}∗}.
Even though these languages look similar, we will show that A is context-free
and B is not context-free.
Consider the following context-free grammar, in which S is the start vari-
able:
S →ǫ|aSa|bSb.
It is easy to see that the language of this grammar is exactly the language A.
Therefore, A is context-free. Alternatively, we can show that A is context-
free, by constructing a (nondeterministic) pushdown automaton that accepts
A. This automaton has two states q and q′, where q is the start state. If the
automaton is in state q, then it did not yet ﬁnish reading the leftmost half of
the input string; it pushes all symbols read onto the stack. If the automaton
is in state q′, then it is reading the rightmost half of the input string; for each
symbol read, it checks whether it is equal to the symbol on top of the stack
and, if so, pops the top symbol from the stack. The pushdown automaton
uses nondeterminism to “guess” when to switch from state q to state q′ (i.e.,
when it has completed reading the leftmost half of the input string).

130
Chapter 3.
Context-Free Languages
At this point, you should convince yourself that the two approaches above,
which showed that A is context-free, do not work for B. The reason why
they do not work is that the language B is not context-free, as we will prove
now.
Assume that B is a context-free language. Let p ≥1 be the pumping
length, as given by the pumping lemma. At this point, we must choose a
string s in B, whose length is at least p, and that does not satisfy the three
properties stated in the pumping lemma. Let us try the string s = apbapb.
Then s ∈B and |s| = 2p + 2 ≥p. Hence, by the pumping lemma, s can be
written as s = uvxyz, where (i) |vy| ≥1, (ii) |vxy| ≤p, and (iii) uvixyiz ∈B
for all i ≥0. It may happen that p ≥3, u = ap−1, v = a, x = b, y = a,
and z = ap−1b. If this is the case, then properties (i), (ii), and (iii) hold,
and, thus, we do not get a contradiction. In other words, we have chosen
the “wrong” string s. This string is “wrong”, because there is only one b
between the as. Because of this, v can be in the leftmost block of as, and
y can be in the rightmost block of as. Observe that if there were at least p
many bs between the as, then this would not happen, because |vxy| ≤p.
Based on the discussion above, we choose s = apbpapbp. Observe that
s ∈B and |s| = 4p ≥p. Hence, by the pumping lemma, s can be written as
s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈B for all i ≥0. Based
on the location of vxy in the string s, we distinguish three cases:
Case 1: The substring vxy overlaps both the leftmost half and the rightmost
half of s.
Since |vxy| ≤p, the substring vxy is contained in the “middle” part of s,
i.e., vxy is contained in the block bpap. Consider the string uv0xy0z = uxz.
Since |vy| ≥1, we know that at least one of v and y is non-empty.
• If v ̸= ǫ, then v contains at least one b from the leftmost block of bs in
s, whereas y does not contain any b from the rightmost block of bs in s.
Therefore, in the string uxz, the leftmost block of bs contains fewer bs
than the rightmost block of bs. Hence, the string uxz is not contained
in B.
• If y ̸= ǫ, then y contains at least one a from the rightmost block of
as in s, whereas v does not contain any a from the leftmost block of
as in s. Therefore, in the string uxz, the leftmost block of as contains
more as than the rightmost block of as. Hence, the string uxz is not
contained in B.

3.8.
The pumping lemma for context-free languages
131
In both cases, we conclude that the string uxz is not an element of the
language B. But, by the pumping lemma, this string is contained in B.
Case 2: The substring vxy is in the leftmost half of s.
In this case, none of the strings uxz, uv2xy2z, uv3xy3z, uv4xy4z, etc.,
is contained in B.
But, by the pumping lemma, each of these strings is
contained in B.
Case 3: The substring vxy is in the rightmost half of s.
This case is symmetric to Case 2: None of the strings uxz, uv2xy2z,
uv3xy3z, uv4xy4z, etc., is contained in B. But, by the pumping lemma, each
of these strings is contained in B.
To summarize, in each of the three cases, we have obtained a contradic-
tion. Therefore, the language B is not context-free.
Third example
We have seen in Section 3.2.4 that the language
{ambncm+n : m ≥0, n ≥0}
is context-free. Using the pumping lemma for regular languages, it is easy to
prove that this language is not regular. In other words, context-free gram-
mars can verify addition, whereas ﬁnite automata are not powerful enough
for this. We now consider the problem of verifying multiplication: Let A be
the language deﬁned as
A = {ambncmn : m ≥0, n ≥0}.
We will prove by contradiction that A is not a context-free language.
Assume that A is context-free. Let p ≥1 be the pumping length, as
given by the pumping lemma. Consider the string s = apbpcp2. Then, s ∈A
and |s| = 2p + p2 ≥p. Hence, by the pumping lemma, s can be written as
s = uvxyz, where |vy| ≥1, |vxy| ≤p, and uvixyiz ∈A for all i ≥0.
There are three possible cases, depending on the locations of v and y in
the string s.
Case 1: The substring v does not contain any a and does not contain any
b, and the substring y does not contain any a and does not contain any b.

132
Chapter 3.
Context-Free Languages
Consider the string uv2xy2z.
Since |vy| ≥1, this string consists of p
many as, p many bs, but more than p2 many cs. Therefore, this string is not
contained in A. But, by the pumping lemma, it is contained in A.
Case 2: The substring v does not contain any c and the substring y does
not contain any c.
Consider again the string uv2xy2z. This string consists of p2 many cs.
Since |vy| ≥1, in this string,
• the number of as is at least p + 1 and the number of bs is at least p, or
• the number of as is at least p and the number of bs is at least p + 1.
Therefore, the number of as multiplied by the number of bs is at least p(p+1),
which is larger than p2. Therefore, uv2xy2z is not contained in A. But, by
the pumping lemma, this string is contained in A.
Case 3: The substring v contains at least one b and the substring y contains
at least one c.
Since |vxy| ≤p, the substring vy does not contain any a. Thus, we can
write vy = bjck, where j ≥1 and k ≥1. Consider the string uxz. We can
write this string as uxz = apbp−jcp2−k. Since, by the pumping lemma, this
string is contained in A, we have p(p−j) = p2−k, which implies that jp = k.
Thus,
|vxy| ≥|vy| = j + k = j + jp ≥1 + p.
But, by the pumping lemma, we have |vxy| ≤p.
Observe that, since |vxy| ≤p, the above three cases cover all possibilities
for the locations of v and y in the string s. In each of the three cases, we
have obtained a contradiction. Therefore, the language A is not context-free.
Exercises
3.1 Construct context-free grammars that generate the following languages.
In all cases, Σ = {0, 1}.
• {02n1n : n ≥0}
• {w : w contains at least three 1s}
• {w : the length of w is odd and its middle symbol is 0}

Exercises
133
• {w : w is a palindrome}.
A palindrome is a string w having the property that w = wR, i.e.,
reading w from left to right gives the same result as reading w from
right to left.
• {w : w starts and ends with the same symbol}
• {w : w starts and ends with diﬀerent symbols}
3.2 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},
Σ = {0, 1}, S is the start variable, and R consists of the rules
S
→
0S|1A|ǫ
A
→
0B|1S
B
→
0A|1B
Deﬁne the following language L:
L = {w ∈{0, 1}∗:
w is the binary representation of a non-negative
integer that is divisible by three } ∪{ǫ}.
Prove that L = L(G). (Hint: The variables S, A, and B are used to
remember the remainder after division by three.)
3.3 Let G = (V, Σ, R, S) be the context-free grammar, where V = {A, B, S},
Σ = {a, b}, S is the start variable, and R consists of the rules
S
→
aB|bA
A
→
a|aS|BAA
B
→
b|bS|ABB
• Prove that ababba ∈L(G).
• Prove that L(G) is the set of all non-empty strings w over the alphabet
{a, b} such that the number of as in w is equal to the number of bs in
w.
3.4 Let A and B be context-free languages over the same alphabet Σ.
• Prove that the union A ∪B of A and B is also context-free.
• Prove that the concatenation AB of A and B is also context-free.

134
Chapter 3.
Context-Free Languages
• Prove that the star A∗of A is also context-free.
3.5 Deﬁne the following two languages A and B:
A = {ambncn : m ≥0, n ≥0}
and
B = {ambmcn : m ≥0, n ≥0}.
• Prove that both A and B are context-free, by constructing two context-
free grammars, one that generates A and one that generates B.
• We have seen in Section 3.8.2 that the language
{anbncn : n ≥0}
is not context-free. Explain why this implies that the intersection of
two context-free languages is not necessarily context-free.
• Use De Morgan’s Law to conclude that the complement of a context-
free language is not necessarily context-free.
3.6 Let A be a context-free language and let B be a regular language.
• Prove that the intersection A ∩B of A and B is context-free.
• Prove that the set-diﬀerence
A \ B = {w : w ∈A, w ̸∈B}
of A and B is context-free.
• Is the set-diﬀerence of two context-free languages necessarily context-
free?
3.7 Let L be the language consisting of all non-empty strings w over the
alphabet {a, b} such that
• the number of as in w is equal to the number of bs in w,
• w does not contain the substring abba, and
• w does not contain the substring bbaa.

Exercises
135
In this exercise, you will prove that L is context-free.
Let A be the language consisting of all non-empty strings w over the
alphabet {a, b} such that the number of as in w is equal to the number of bs
in w. In Exercise 3.3, you have shown that A is context-free.
Let B be the language consisting of all strings w over the alphabet {a, b}
such that
• w does not contain the substring abba, and
• w does not contain the substring bbaa.
1. Give a regular expression that describes the complement of B.
2. Argue that B is a regular language.
3. Use Exercise 3.6 to argue that L is a context-free language.
3.8 Construct (deterministic or nondeterministic) pushdown automata that
accept the following languages.
1. {02n1n : n ≥0}.
2. {0n1m0n : n ≥1, m ≥1}.
3. {w ∈{0, 1}∗: w contains more 1s than 0s}.
4. {wwR : w ∈{0, 1}∗}.
(If w = w1 . . . wn, then wR = wn . . . w1.)
5. {w ∈{0, 1}∗: w is a palindrome}.
3.9 Let L be the language
L = {ambn : 0 ≤m ≤n ≤2m}.
1. Prove that L is context-free, by constructing a context-free grammar
whose language is equal to L.
2. Prove that L is context-free, by constructing a nondeterministic push-
down automaton that accepts L.
3.10 Prove that the following languages are not context-free.

136
Chapter 3.
Context-Free Languages
• {an b a2n b a3n : n ≥0}.
• {anbnanbn : n ≥0}.
• {ambnck : m ≥0, n ≥0, k = max(m, n)}.
• {w#x : w is a substring of x, and w, x ∈{a, b}∗}.
For example, the string aba#abbababbb is in the language, whereas the
string aba#baabbaabb is not in the language. The alphabet is {a, b, #}.
•
{ w ∈{a, b, c}∗
:
w contains more b’s than a’s and
w contains more c’s than a’s }.
• {1n : n is a prime number}.
• {(abn)n : n ≥0}. (The parentheses are not part of the alphabet; thus,
the alphabet is {a, b, }.)
3.11 Let L be a language consisting of ﬁnitely many strings. Show that L
is regular and, therefore, context-free. Let k be the maximum length of any
string in L.
• Prove that every context-free grammar in Chomsky normal form that
generates L has more than log k variables. (The logarithm is in base
2.)
• Prove that there is a context-free grammar that generates L and that
has only one variable.
3.12 Let L be a context-free language. Prove that there exists an integer
p ≥1, such that the following is true: For every string s in L with |s| ≥p,
there exists a string s′ in L such that |s| < |s′| ≤|s| + p.

Chapter 4
Turing Machines and the
Church-Turing Thesis
In the previous chapters, we have seen several computational devices that
can be used to accept or generate regular and context-free languages. Even
though these two classes of languages are fairly large, we have seen in Sec-
tion 3.8.2 that these devices are not powerful enough to accept simple lan-
guages such as A = {ambncmn : m ≥0, n ≥0}. In this chapter, we introduce
the Turing machine, which is a simple model of a real computer. Turing ma-
chines can be used to accept all context-free languages, but also languages
such as A. We will argue that every problem that can be solved on a real
computer can also be solved by a Turing machine (this statement is known
as the Church-Turing Thesis). In Chapter 5, we will consider the limitations
of Turing machines and, hence, of real computers.
4.1
Deﬁnition of a Turing machine
We start with an informal description of a Turing machine. Such a machine
consists of the following, see also Figure 4.1.
1. There are k tapes, for some ﬁxed k ≥1. Each tape is divided into
cells, and is inﬁnite both to the left and to the right. Each cell stores
a symbol belonging to a ﬁnite set Γ, which is called the tape alphabet.
The tape alphabet contains the blank symbol 2. If a cell contains 2,
then this means that the cell is actually empty.

138
Chapter 4.
Turing Machines and the Church-Turing Thesis
state control
. . . 2 2 2 a a b a b b a b a b 2 2 2
. . .
?
. . . 2 2 2 b a a b 2 a b 2 2 2
. . .
?
Figure 4.1: A Turing machine with k = 2 tapes.
2. Each tape has a tape head which can move along the tape, one cell
per move. It can also read the cell it currently scans and replace the
symbol in this cell by another symbol.
3. There is a state control, which can be in any one of a ﬁnite number of
states. The ﬁnite set of states is denoted by Q. The set Q contains
three special states: a start state, an accept state, and a reject state.
The Turing machine performs a sequence of computation steps. In one
such step, it does the following:
1. Immediately before the computation step, the Turing machine is in a
state r of Q, and each of the k tape heads is on a certain cell.
2. Depending on the current state r and the k symbols that are read by
the tape heads,
(a) the Turing machine switches to a state r′ of Q (which may be
equal to r),
(b) each tape head writes a symbol of Γ in the cell it is currently
scanning (this symbol may be equal to the symbol currently stored
in the cell), and

4.1.
Deﬁnition of a Turing machine
139
(c) each tape head either moves one cell to the left, moves one cell to
the right, or stays at the current cell.
We now give a formal deﬁnition of a deterministic Turing machine.
Deﬁnition 4.1.1 A deterministic Turing machine is a 7-tuple
M = (Σ, Γ, Q, δ, q, qaccept, qreject),
where
1. Σ is a ﬁnite set, called the input alphabet; the blank symbol 2 is not
contained in Σ,
2. Γ is a ﬁnite set, called the tape alphabet; this alphabet contains the
blank symbol 2, and Σ ⊆Γ,
3. Q is a ﬁnite set, whose elements are called states,
4. q is an element of Q; it is called the start state,
5. qaccept is an element of Q; it is called the accept state,
6. qreject is an element of Q; it is called the reject state,
7. δ is called the transition function, which is a function
δ : Q × Γk →Q × Γk × {L, R, N}k.
The transition function δ is basically the “program” of the Turing ma-
chine. This function tells us what the machine can do in “one computation
step”: Let r ∈Q, and let a1, a2, . . . , ak ∈Γ.
Furthermore, let r′ ∈Q,
a′
1, a′
2, . . . , a′
k ∈Γ, and σ1, σ2, . . . , σk ∈{L, R, N} be such that
δ(r, a1, a2, . . . , ak) = (r′, a′
1, a′
2, . . . , a′
k, σ1, σ2, . . . , σk).
(4.1)
This transition means that if
• the Turing machine is in state r, and
• the head of the i-th tape reads the symbol ai, 1 ≤i ≤k,
then

140
Chapter 4.
Turing Machines and the Church-Turing Thesis
• the Turing machine switches to state r′,
• the head of the i-th tape replaces the scanned symbol ai by the symbol
a′
i, 1 ≤i ≤k, and
• the head of the i-th tape moves according to σi, 1 ≤i ≤k: if σi = L,
then the tape head moves one cell to the left; if σi = R, then it moves
one cell to the right; if σi = N, then the tape head does not move.
We will write the computation step (4.1) in the form of the instruction
ra1a2 . . . ak →r′a′
1a′
2 . . . a′
kσ1σ2 . . . σk.
We now specify the computation of the Turing machine
M = (Σ, Γ, Q, δ, q, qaccept, qreject).
Start conﬁguration: The input is a string over the input alphabet Σ.
Initially, this input string is stored on the ﬁrst tape, and the head of this
tape is on the leftmost symbol of the input string. Initially, all other k −1
tapes are empty, i.e., only contain blank symbols, and the Turing machine is
in the start state q.
Computation and termination: Starting in the start conﬁguration, the
Turing machine performs a sequence of computation steps as described above.
The computation terminates at the moment when the Turing machine en-
ters the accept state qaccept or the reject state qreject. (Hence, if the Turing
machine never enters the states qaccept and qreject, the computation does not
terminate.)
Acceptance: The Turing machine M accepts the input string w ∈Σ∗, if the
computation on this input terminates in the state qaccept. If the computation
on this input terminates in the state qreject, then M rejects the input string
w.
We denote by L(M) the language accepted by the Turing machine M.
Thus, L(M) is the set of all strings in Σ∗that are accepted by M.
Observe that a string w ∈Σ∗does not belong to L(M) if and only if on
input w,
• the computation of M terminates in the state qreject or
• the computation of M does not terminate.

4.2.
Examples of Turing machines
141
4.2
Examples of Turing machines
4.2.1
Accepting palindromes using one tape
We will show how to construct a Turing machine with one tape, that decides
whether or not any input string w ∈{a, b}∗is a palindrome. Recall that the
string w is called a palindrome, if reading w from left to right gives the same
result as reading w from right to left. Examples of palindromes are abba,
baabbbbaab, and the empty string ǫ.
Start of the computation: The tape contains the input string w, the tape
head is on the leftmost symbol of w, and the Turing machine is in the start
state q0.
Idea: The tape head reads the leftmost symbol of w, deletes this symbol
and “remembers” it by means of a state.
Then the tape head moves to
the rightmost symbol and tests whether it is equal to the (already deleted)
leftmost symbol.
• If they are equal, then the rightmost symbol is deleted, the tape head
moves to the new leftmost symbol, and the whole process is repeated.
• If they are not equal, the Turing machine enters the reject state, and
the computation terminates.
The Turing machine enters the accept state as soon as the string currently
stored on the tape is empty.
We will use the input alphabet Σ = {a, b} and the tape alphabet Γ =
{a, b, 2}. The set Q of states consists of the following eight states:
q0 :
start state; tape head is on the leftmost symbol
qa :
leftmost symbol was a; tape head is moving to the right
qb :
leftmost symbol was b; tape head is moving to the right
q′
a :
reached rightmost symbol; test whether it is equal to a, and delete it
q′
b :
reached rightmost symbol; test whether it is equal to b, and delete it
qL :
test was positive; tape head is moving to the left
qaccept :
accept state
qreject :
reject state

142
Chapter 4.
Turing Machines and the Church-Turing Thesis
The transition function δ is speciﬁed by the following instructions:
q0a →qa2R
qaa →qaaR
qba →qbaR
q0b →qb2R
qab →qabR
qbb →qbbR
q02 →qaccept
qa2 →q′
a2L
qb2 →q′
b2L
q′
aa →qL2L
q′
ba →qreject
qLa →qLaL
q′
ab →qreject
q′
bb →qL2L
qLb →qLbL
q′
a2 →qaccept
q′
b2 →qaccept
qL2 →q02R
You should go through the computation of this Turing machine for some
sample inputs, for example abba, b, abb and the empty string (which is a
palindrome).
4.2.2
Accepting palindromes using two tapes
We again consider the palindrome problem, but now we use a Turing machine
with two tapes.
Start of the computation: The ﬁrst tape contains the input string w and
the head of the ﬁrst tape is on the leftmost symbol of w. The second tape is
empty and its tape head is at an arbitrary position. The Turing machine is
in the start state q0.
Idea: First, the input string w is copied to the second tape. Then the head
of the ﬁrst tape moves back to the leftmost symbol of w, while the head of
the second tape stays at the rightmost symbol of w. Finally, the actual test
starts: The head of the ﬁrst tape moves to the right and, at the same time,
the head of the second tape moves to the left. While moving, the Turing
machine tests whether the two tape heads read the same symbol in each
step.
The input alphabet is Σ = {a, b} and the tape alphabet is Γ = {a, b, 2}.
The set Q of states consists of the following ﬁve states:
q0 :
start state; copy w to the second tape
q1 :
w has been copied; head of ﬁrst tape moves to the left
q2 :
head of ﬁrst tape moves to the right; head of second tape moves
to the left; until now, all tests were positive
qaccept :
accept state
qreject :
reject state

4.2.
Examples of Turing machines
143
The transition function δ is speciﬁed by the following instructions:
q0a2 →q0aaRR
q1aa →q1aaLN
q0b2 →q0bbRR
q1ab →q1abLN
q022 →q122LL
q1ba →q1baLN
q1bb →q1bbLN
q12a →q22aRN
q12b →q22bRN
q122 →qaccept
q2aa →q2aaRL
q2ab →qreject
q2ba →qreject
q2bb →q2bbRL
q222 →qaccept
Again, you should run this Turing machine for some sample inputs.
4.2.3
Accepting anbncn using one tape
We will construct1 a Turing machine with one tape that accepts the language
{anbncn : n ≥0}.
Recall that we have proved in Section 3.8.2 that this language is not context-
free.
Start of the computation: The tape contains the input string w and the
tape head is on the leftmost symbol of w. The Turing machine is in the start
state.
Idea: In the previous examples, the tape alphabet Γ was equal to the union
of the input alphabet Σ and {2}. In this example, we will add one symbol
d to the tape alphabet. As we will see, this simpliﬁes the construction of
the Turing machine. Thus, the input alphabet is Σ = {a, b, c} and the tape
alphabet is Γ = {a, b, c, d, 2}. Recall that the input string w belongs to Σ∗.
The general approach is to split the computation into two stages.
1Thanks to Michael Fleming for pointing out an error in a previous version of this
construction.

144
Chapter 4.
Turing Machines and the Church-Turing Thesis
Stage 1: In this stage, we check if the string w is in the language described
by the regular expression a∗b∗c∗. If this is the case, then we walk back to
the leftmost symbol. For this stage, we use the following states, besides the
states qaccept and qreject:
qa :
start state; we are reading the block of a’s
qb :
we are reading the block of b’s
qc :
we are reading the block of c’s
qL :
walk to the leftmost symbol
Stage 2: In this stage, we repeat the following: Walk along the string from
left to right, replace the leftmost a by d, replace the leftmost b by d, replace
the leftmost c by d, and walk back to the leftmost symbol.
For this stage, we use the following states:
q′
a :
start state of Stage 2; search for the leftmost a
q′
b :
leftmost a has been replaced by d;
search for the leftmost b
q′
c :
leftmost a has been replaced by d;
leftmost b has been replaced by d;
search for the leftmost c
q′
L :
leftmost a has been replaced by d;
leftmost b has been replaced by d;
leftmost c has been replaced by d;
walk to the leftmost symbol
The transition function δ is speciﬁed by the following instructions:
qaa →qaaR
qba →qreject
qab →qbbR
qbb →qbbR
qac →qccR
qbc →qccR
qad →cannot happen
qbd →cannot happen
qa2 →qL2L
qb2 →qL2L
qca →qreject
qLa →qLaL
qcb →qreject
qLb →qLbL
qcc →qccR
qLc →qLcL
qcd →cannot happen
qLd →cannot happen
qc2 →qL2L
qL2 →q′
a2R

4.2.
Examples of Turing machines
145
q′
aa →q′
bdR
q′
ba →q′
baR
q′
ab →qreject
q′
bb →q′
cdR
q′
ac →qreject
q′
bc →qreject
q′
ad →q′
adR
q′
bd →q′
bdR
q′
a2 →qaccept
q′
b2 →qreject
q′
ca →qreject
q′
La →q′
LaL
q′
cb →q′
cbR
q′
Lb →q′
LbL
q′
cc →q′
LdL
q′
Lc →q′
LcL
q′
cd →q′
cdR
q′
Ld →q′
LdL
q′
c2 →qreject
q′
L2 →q′
a2R
We remark that Stage 1 is really necessary for this Turing machine: If we
omit this stage, and use only Stage 2, then the string aabcbc will be accepted.
4.2.4
Accepting anbncn using tape alphabet {a, b, c, 2}
We consider again the language {anbncn : n ≥0}. In the previous section,
we presented a Turing machine that uses an extra symbol d. The reader may
wonder if we can construct a Turing machine for this language that does not
use any extra symbols. We will show below that this is indeed possible.
Start of the computation: The tape contains the input string w and the
tape head is on the leftmost symbol of w. The Turing machine is in the start
state q0.
Idea: Repeat the following Stages 1 and 2, until the string is empty.
Stage 1. Walk along the string from left to right, delete the leftmost a,
delete the leftmost b, and delete the rightmost c.
Stage 2. Shift the substring of bs and cs one position to the left; then walk
back to the leftmost symbol.
The input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, 2}.

146
Chapter 4.
Turing Machines and the Church-Turing Thesis
For Stage 1, we use the following states:
q0 :
start state; tape head is on the leftmost symbol
qa :
leftmost a has been deleted; have not read b
qb :
leftmost b has been deleted; have not read c
qc :
leftmost c has been read; tape head moves to the right
q′
c :
tape head is on the rightmost c
q1 :
rightmost c has been deleted; tape head is on the rightmost
symbol or 2
qaccept :
accept state
qreject :
reject state
The transitions for Stage 1 are speciﬁed by the following instructions:
q0a →qa2R
qaa →qaaR
q0b →qreject
qab →qb2R
q0c →qreject
qac →qreject
q02 →qaccept
qa2 →qreject
qba →qreject
qca →qreject
qbb →qbbR
qcb →qreject
qbc →qccR
qcc →qccR
qb2 →qreject
qc2 →q′
c2L
q′
cc →q12L
For Stage 2, we use the following states:
q1 :
as above; tape head is on the rightmost symbol or on 2
qc :
copy c one cell to the left
qb :
copy b one cell to the left
q2 :
done with shifting; head moves to the left
Additionally, we use a state q′
1 which has the following meaning: If the input
string is of the form aibc, for some i ≥1, then after Stage 1, the tape contains
the string ai−122, the tape head is on the 2 immediately to the right of the
as, and the Turing machine is in state q1. In this case, we move one cell to
the left; if we then read 2, then i = 1, and we accept; otherwise, we read a,
and we reject.

4.2.
Examples of Turing machines
147
The transitions for Stage 2 are speciﬁed by the following instructions:
q1a →cannot happen
q′
1a →qreject
q1b →qreject
q′
1b →cannot happen
q1c →qc2L
q′
1c →cannot happen
q12 →q′
12L
q′
12 →qaccept
qca →cannot happen
qba →cannot happen
qcb →qbcL
qbb →qbbL
qcc →qccL
qbc →cannot happen
qc2 →qreject
qb2 →q2bL
q2a →q2aL
q2b →cannot happen
q2c →cannot happen
q22 →q02R
4.2.5
Accepting ambncmn using one tape
We will sketch how to construct a Turing machine with one tape that accepts
the language
{ambncmn : m ≥0, n ≥0}.
Recall that we have proved in Section 3.8.2 that this language is not context-
free.
The input alphabet is Σ = {a, b, c} and the tape alphabet is Γ = {a, b, c, $, 2},
where the purpose of the symbol $ will become clear below.
Start of the computation: The tape contains the input string w and the
tape head is on the leftmost symbol of w. The Turing machine is in the start
state.
Idea: Observe that a string ambnck is in the language if and only if for every
a, the string contains n many cs. Based on this, the computation consists of
the following stages:
Stage 1. Walk along the input string w from left to right and check whether
w is an element of the language described by the regular expression a∗b∗c∗.
If this is not the case, then reject the input string. Otherwise, go to Stage 2.
Stage 2. Walk back to the leftmost symbol of w. Go to Stage 3.
Stage 3. In this stage, the Turing machine does the following:

148
Chapter 4.
Turing Machines and the Church-Turing Thesis
• Replace the leftmost a by the blank symbol 2.
• Walk to the leftmost b.
• Zigzag between the bs and cs; each time, replace the leftmost b by the
symbol $, and replace the rightmost c by the blank symbol 2. If, for
some b, there is no c left, the Turing machine rejects the input string.
• Continue zigzagging until there are no bs left. Then go to Stage 4.
Observe that in this third stage, the string ambnck is transformed to the
string am−1$nck−n.
Stage 4. In this stage, the Turing machine does the following:
• Replace each $ by b.
• Walk to the leftmost a.
Hence, in this fourth stage, the string am−1$nck−n is transformed to the string
am−1bnck−n.
Observe that the input string ambnck is in the language if and only if the
string am−1bnck−n is in the language. Therefore, the Turing machine repeats
Stages 3 and 4, until there are no as left. At that moment, it checks whether
there are any cs left; if so, it rejects the input string; otherwise, it accepts
the input string.
We hope that you believe that this description of the algorithm can be
turned into a formal description of a Turing machine.
4.3
Multi-tape Turing machines
In Section 4.2, we have seen two Turing machines that accept palindromes;
the ﬁrst Turing machine has one tape, whereas the second one has two tapes.
You will have noticed that the two-tape Turing machine was easier to obtain
than the one-tape Turing machine. This leads to the question whether multi-
tape Turing machines are more powerful than their one-tape counterparts.
The answer is “no”:
Theorem 4.3.1 Let k ≥1 be an integer. Any k-tape Turing machine can
be converted to an equivalent one-tape Turing machine.

4.3.
Multi-tape Turing machines
149
Proof.2
We will sketch the proof for the case when k = 2.
Let M =
(Σ, Γ, Q, δ, q, qaccept, qreject) be a two-tape Turing machine.
Our goal is to
convert M to an equivalent one-tape Turing machine N. That is, N should
have the property that for all strings w ∈Σ∗,
• M accepts w if and only if N accepts w,
• M rejects w if and only if N rejects w,
• M does not terminate on input w if and only if N does not terminate
on input w.
The tape alphabet of the one-tape Turing machine N is
Γ ∪{ ˙x : x ∈Γ} ∪{#}.
In words, we take the tape alphabet Γ of M, and add, for each x ∈Γ, the
symbol ˙x. Moreover, we add a special symbol #.
The Turing machine N will be deﬁned in such a way that any conﬁgura-
tion of the two-tape Turing machine M, for example
. . . 2 1 0 0 1 2 . . .
6
. . . 2 a a b a 2 . . .
6
corresponds to the following conﬁguration of the one-tape Turing machine
N:
. . .
2
#
1
0
˙0
1
#
a
˙a
b
a
#
2 . . .
6
2Thanks to Sergio Cabello for pointing out an error in a previous version of this proof.

150
Chapter 4.
Turing Machines and the Church-Turing Thesis
Thus, the contents of the two tapes of M are encoded on the single tape of
N. The dotted symbols are used to indicate the positions of the two tape
heads of M, whereas the three occurrences of the special symbol # are used
to mark the boundaries of the strings on the two tapes of M.
The Turing machine N simulates one computation step of M, in the
following way:
• Throughout the simulation of this step, N “remembers” the current
state of M.
• At the start of the simulation, the tape head of N is on the leftmost
symbol #.
• N walks along the string to the right until it ﬁnds the ﬁrst dotted
symbol. (This symbol indicates the location of the head on the ﬁrst tape
of M.) N remembers this ﬁrst dotted symbol and continues walking
to the right until it ﬁnds the second dotted symbol.
(This symbol
indicates the location of the head on the second tape of M.) Again, N
remembers this second dotted symbol.
• At this moment, N is still at the second dotted symbol. N updates
this part of the tape, by making the change that M would make on its
second tape. (This change is given by the transition function of M; it
depends on the current state of M and the two symbols that M reads
on its two tapes.)
• N walks to the left until it ﬁnds the ﬁrst dotted symbol.
Then, it
updates this part of the tape, by making the change that M would
make on its ﬁrst tape.
• In the previous two steps, in which the tape is updated, it may be
necessary to shift a part of the tape.
• Finally, N remembers the new state of M and walks back to the left-
most symbol #.
It should be clear that the Turing machine N can be constructed by
introducing appropriate states.

4.4.
The Church-Turing Thesis
151
4.4
The Church-Turing Thesis
We all have some intuitive notion of what an algorithm is. This notion will
probably be something like “an algorithm is a procedure consisting of com-
putation steps that can be speciﬁed in a ﬁnite amount of text”. For example,
any “computational process” that can be speciﬁed by a Java program, should
be considered an algorithm. Similarly, a Turing machine speciﬁes a “com-
putational process” and, therefore, should be considered an algorithm. This
leads to the question of whether it is possible to give a mathematical deﬁni-
tion of an “algorithm”. We just saw that every Java program represents an
algorithm and that every Turing machine also represents an algorithm. Are
these two notions of an algorithm equivalent? The answer is “yes”. In fact,
the following theorem states that many diﬀerent notions of “computational
process” are equivalent. (We hope that you have gained suﬃcient intuition,
so that none of the claims in this theorem comes as a surprise to you.)
Theorem 4.4.1 The following computation models are equivalent, i.e., any
one of them can be converted to any other one:
1. One-tape Turing machines.
2. k-tape Turing machines, for any k ≥1.
3. Non-deterministic Turing machines.
4. Java programs.
5. C++ programs.
6. Lisp programs.
In other words, if we deﬁne the notion of an algorithm using any of the
models in this theorem, then it does not matter which model we take: All
these models give the same notion of an algorithm.
The problem of deﬁning the notion of an algorithm goes back to David
Hilbert. On August 8, 1900, at the Second International Congress of Math-
ematicians in Paris, Hilbert presented a list of problems that he considered
crucial for the further development of mathematics. Hilbert’s 10th problem
is the following:

152
Chapter 4.
Turing Machines and the Church-Turing Thesis
Does there exist a ﬁnite process that decides whether or not any
given polynomial with integer coeﬃcients has integral roots?
Of course, in our language, Hilbert asked whether or not there exists an
algorithm that decides, when given an arbitrary polynomial equation (with
integer coeﬃcients) such as
12x3y7z5 + 7x2y4z −x4 + y2z7 −z3 + 10 = 0,
whether or not this equation has a solution in integers. In 1970, Matiyasevich
proved that such an algorithm does not exist. Of course, in order to prove
this claim, we ﬁrst have to agree on what an algorithm is. In the beginning
of the twentieth century, mathematicians gave several deﬁnitions, such as
Turing machines (1936) and the λ-calculus (1936), and they proved that all
these are equivalent. Later, after programming languages were invented, it
was shown that these older notions of an algorithm are equivalent to notions
of an algorithm that are based on C programs, Java programs, Lisp programs,
Pascal programs, etc.
In other words, all attempts to give a rigorous deﬁnition of the notion of
an algorithm led to the same concept. Because of this, computer scientists
nowadays agree on what is called the Church-Turing Thesis:
Church-Turing Thesis:
Every computational process that is intuitively
considered to be an algorithm can be converted to a Turing machine.
In other words, this basically states that we deﬁne an algorithm to be a
Turing machine. At this point, you should ask yourself, whether the Church-
Turing Thesis can be proved. Alternatively, what has to be done in order to
disprove this thesis?
Exercises
4.1 Construct a Turing machine with one tape, that accepts the language
{02n1n : n ≥0}.
Assume that, at the start of the computation, the tape head is on the leftmost
symbol of the input string.

Exercises
153
4.2 Construct a Turing machine with one tape, that accepts the language
{w : w contains twice as many 0s as 1s}.
Assume that, at the start of the computation, the tape head is on the leftmost
symbol of the input string.
4.3 Let A be the language
A
=
{ w ∈{a, b, c}∗
:
w contains more bs than as and
w contains more cs than as }.
Give an informal description (in plain English) of a Turing machine with one
tape, that accepts the language A.
4.4 Construct a Turing machine with one tape that receives as input a non-
negative integer x and returns as output the integer x + 1.
Integers are
represented as binary strings.
Start of the computation: The tape contains the binary representation
of the input x. The tape head is on the leftmost symbol and the Turing
machine is in the start state q0. For example, if x = 431, the tape looks as
follows:
. . . 2 2 2 1 1 0 1 0 1 1 1 1 2 2 2
. . .
6
End of the computation: The tape contains the binary representation of
the integer x + 1. The tape head is on the leftmost symbol and the Turing
machine is in the ﬁnal state q1. For our example, the tape looks as follows:
. . . 2 2 2 1 1 0 1 1 0 0 0 0 2 2 2
. . .
6
The Turing machine in this exercise does not have an accept state or a
reject state; instead, it has a ﬁnal state q1. As soon as state q1 is entered,
the Turing machine terminates. At termination, the contents of the tape is
the output of the Turing machine.

154
Chapter 4.
Turing Machines and the Church-Turing Thesis
4.5 Construct a Turing machine with two tapes that receives as input two
non-negative integers x and y, and returns as output the integer x + y.
Integers are represented as binary strings.
Start of the computation: The ﬁrst tape contains the binary represen-
tation of x and its head is on the rightmost symbol of x. The second tape
contains the binary representation of y and its head is on the rightmost bit
of y. At the start, the Turing machine is in the start state q0.
End of the computation: The ﬁrst tape contains the binary representation
of x and its head is on the rightmost symbol of x. The second tape contains
the binary representation of the integer x + y (thus, the integer y is “gone”).
The head of the second tape is on the rightmost bit of x + y. The Turing
machine is in the ﬁnal state q1.
4.6 Give an informal description (in plain English) of a Turing machine with
one tape that receives as input two non-negative integers x and y, and returns
as output the integer x+y. Integers are represented as binary strings. If you
are an adventurous student, you may give a formal deﬁnition of your Turing
machine.
4.7 Construct a Turing machine with one tape that receives as input an
integer x ≥1 and returns as output the integer x−1. Integers are represented
in binary.
Start of the computation: The tape contains the binary representation of
the input x. The tape head is on the rightmost symbol of x and the Turing
machine is in the start state q0.
End of the computation: The tape contains the binary representation of
the integer x −1. The tape head is on the rightmost bit of x −1 and the
Turing machine is in the ﬁnal state q1.
4.8 Give an informal description (in plain English) of a Turing machine with
three tapes that receives as input two non-negative integers x and y, and
returns as output the integer xy. Integers are represented as binary strings.
Start of the computation: The ﬁrst tape contains the binary represen-
tation of x and its head is on the rightmost symbol of x. The second tape
contains the binary representation of y and its head is on the rightmost sym-
bol of y. The third tape is empty and its head is at an arbitrary location.
The Turing machine is in the start state q0.

Exercises
155
End of the computation: The ﬁrst and second tapes are empty. The third
tape contains the binary representation of the product xy and its head is on
the rightmost bit of xy. The Turing machine is in the ﬁnal state q1.
Hint: Use the Turing machines of Exercises 4.5 and 4.7.
4.9 Construct a Turing machine with one tape that receives as input a string
of the form 1n for some integer n ≥0; thus, the input is a string of n many
1s. The output of the Turing machine is the string 1n21n. Thus, this Turing
machine makes a copy of its input.
The input alphabet is Σ = {1} and the tape alphabet is Γ = {1, 2}.
Start of the computation: The tape contains a string of the form 1n, for
some integer n ≥0, the tape head is on the leftmost symbol, and the Turing
machine is in the start state. For example, if n = 4, the tape looks as follows:
. . . 2 2 2 1 1 1 1 2 2 2
. . .
6
End of the computation: The tape contains the string 1n21n, the tape
head is on the 2 in the middle of this string, and the Turing machine is in
the ﬁnal state. For our example, the tape looks as follows:
. . . 2 2 2 1 1 1 1 2 1 1 1 1 2 2 2
. . .
6
The Turing machine in this exercise does not have an accept state or a
reject state; instead, it has a ﬁnal state. As soon as this state is entered, the
Turing machine terminates. At termination, the contents of the tape is the
output of the Turing machine.

156
Chapter 4.
Turing Machines and the Church-Turing Thesis

Chapter 5
Decidable and Undecidable
Languages
We have seen in Chapter 4 that Turing machines form a model for “everything
that is intuitively computable”. In this chapter, we consider the limitations
of Turing machines. That is, we ask ourselves the question whether or not
“everything” is computable. As we will see, the answer is “no”. In fact, we
will even see that “most” problems are not solvable by Turing machines and,
therefore, not solvable by computers.
5.1
Decidability
In Chapter 4, we have deﬁned when a Turing machine accepts an input string
and when it rejects an input string. Based on this, we deﬁne the following
class of languages.
Deﬁnition 5.1.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We
say that A is decidable, if there exists a Turing machine M, such that for
every string w ∈Σ∗, the following holds:
1. If w ∈A, then the computation of the Turing machine M, on the input
string w, terminates in the accept state.
2. If w ̸∈A, then the computation of the Turing machine M, on the input
string w, terminates in the reject state.

158
Chapter 5.
Decidable and Undecidable Languages
In other words, the language A is decidable, if there exists an algorithm
that (i) terminates on every input string w, and (ii) correctly tells us whether
w ∈A or w ̸∈A.
A language A that is not decidable is called undecidable.
For such a
language, there does not exist an algorithm that satisﬁes (i) and (ii) above.
In Section 4.2, we have seen several examples of languages that are de-
cidable.
In the following subsections, we will give some examples of decidable and
undecidable languages. These examples involve languages A whose elements
are pairs of the form (C, w), where C is some computation model (for ex-
ample, a deterministic ﬁnite automaton) and w is a string over the alphabet
Σ. The pair (C, w) is in the language A if and only if the string w is in the
language of the computation model C. For diﬀerent computation models C,
we will ask the question whether A is decidable, i.e., whether an algorithm
exists that decides, for any input (C, w), whether or not this input belongs
to the language A. Since the input to any algorithm is a string over some
alphabet, we must encode the pair (C, w) as a string. In all cases that we
consider, such a pair can be described using a ﬁnite amout of text. Therefore,
we assume, without loss of generality, that binary strings are used for these
encodings. Throughout the rest of this chapter, we will denote the binary
encoding of a pair (C, w) by
⟨C, w⟩.
5.1.1
The language ADFA
We deﬁne the following language:
ADFA = {⟨M, w⟩:
M is a deterministic ﬁnite automaton that
accepts the string w}.
Keep in mind that ⟨M, w⟩denotes the binary string that forms an en-
coding of the ﬁnite automaton M and the string w that is given as input to
M.
We claim that the language ADFA is decidable. In order to prove this,
we have to construct an algorithm with the following property, for any given
input string u:
• If u is the encoding of a deterministic ﬁnite automaton M and a string
w (i.e., u is in the correct format ⟨M, w⟩), and if M accepts w, then
the algorithm terminates in its accept state.

5.1.
Decidability
159
• In all other cases, the algorithm terminates in its reject state.
An algorithm that exactly does this, is easy to obtain: On input u, the algo-
rithm ﬁrst checks whether or not u encodes a deterministic ﬁnite automaton
M and a string w. If this is not the case, then it terminates and rejects
the input string u. Otherwise, the algorithm “constructs” M and w, and
then simulates the computation of M on the input string w. If M accepts
w, then the algorithm terminates and accepts the input string u. If M does
not accept w, then the algorithm terminates and rejects the input string u.
Thus, we have proved the following result:
Theorem 5.1.2 The language ADFA is decidable.
5.1.2
The language ANFA
We deﬁne the following language:
ANFA = {⟨M, w⟩:
M is a nondeterministic ﬁnite automaton that
accepts the string w}.
To prove that this language is decidable, consider the algorithm that
does the following: On input u, the algorithm ﬁrst checks whether or not
u encodes a nondeterministic ﬁnite automaton M and a string w. If this is
not the case, then it terminates and rejects the input string u. Otherwise,
the algorithm constructs M and w. Since a computation of M (on input w)
is not unique, the algorithm ﬁrst converts M to an equivalent deterministic
ﬁnite automaton N. Then, it proceeds as in Section 5.1.1.
Observe that the construction for converting a nondeterministic ﬁnite au-
tomaton to a deterministic ﬁnite automaton (see Section 2.5) is algorithmic,
in the sense that it can be described by an algorithm. Because of this, the
algorithm described above is a valid algorithm; it accepts all strings u that
are in ANFA, and it rejects all strings u that are not in ANFA. Thus, we have
proved the following result:
Theorem 5.1.3 The language ANFA is decidable.
5.1.3
The language ACFG
We deﬁne the following language:
ACFG = {⟨G, w⟩: G is a context-free grammar such that w ∈L(G)}.

160
Chapter 5.
Decidable and Undecidable Languages
We claim that this language is decidable. In order to prove this claim, con-
sider a string u that encodes a context-free grammar G = (V, Σ, S, R) and a
string w ∈Σ∗. Deciding whether or not w ∈L(G) is equivalent to deciding
whether or not S
∗⇒w. A ﬁrst idea to decide this is by trying all possible
derivations that start with the start variable S and that use rules of R. The
problem is that, in case w ̸∈L(G), it is not clear how many such derivations
have to be checked before we can be sure that w is not in the language of
G: If w ∈L(G), then it may be that w can be derived from S, only by ﬁrst
deriving a very long string, say v, and then use rules to shorten it so as to
obtain the string w. Since there is no obvious upper bound on the length of
the string v, we have to be careful.
The trick is to do the following. First, convert the grammar G to an
equivalent grammar G′ in Chomsky normal form. (The construction given
in Section 3.4 can be described by an algorithm.) Let n be the length of the
string w. Then, if w ∈L(G) = L(G′), any derivation of w in G′, from the
start variable of G′, consists of exactly 2n−1 steps (where a “step” is deﬁned
as applying one rule of G′). Hence, we can decide whether or not w ∈L(G),
by trying all possible derivations, in G′, consisting of 2n −1 steps. If one of
these (ﬁnite number of) derivations leads to the string w, then w ∈L(G).
Otherwise, w ̸∈L(G). Thus, we have proved the following result:
Theorem 5.1.4 The language ACFG is decidable.
In fact, the arguments above imply the following result:
Theorem 5.1.5 Every context-free language is decidable.
Proof. Let Σ be an alphabet and let A ⊆Σ∗be an arbitrary context-free
language. There exists a context-free grammar in Chomsky normal form,
whose language is equal to A. Given an arbitrary string w ∈Σ∗, we have
seen above how we can decide whether or not w can be derived from the
start variable of this grammar.
5.1.4
The language ATM
After having seen the languages ADFA, ANFA, and ACFG, it is natural to
consider the language
ATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.

5.1.
Decidability
161
We will prove that this language is undecidable. Before we give the proof,
let us mention what this means:
There is no algorithm that, when given an arbitrary algorithm M
and an arbitrary input string w for M, decides in a ﬁnite amount
of time, whether or not M accepts w.
The proof of the claim that ATM is undecidable is by contradiction. Thus,
we assume that ATM is decidable. Then there exists a Turing machine H
that has the following property. For every input string ⟨M, w⟩for H:
• If ⟨M, w⟩∈ATM (i.e., M accepts w), then H terminates in its accept
state.
• If ⟨M, w⟩̸∈ATM (i.e., M rejects w or M does not terminate on input
w), then H terminates in its reject state.
• In particular, H terminates on any input ⟨M, w⟩.
We construct a new Turing machine D, that does the following: On input
⟨M⟩, the Turing machine D uses H as a subroutine to determine what M
does when it is given its own description as input. Once D has determined
this information, it does the opposite of what H does.
Turing machine D: On input ⟨M⟩, where M is a Turing machine,
the new Turing machine D does the following:
Step 1: Run the Turing machine H on the input ⟨M, ⟨M⟩⟩.
Step 2:
• If H terminates in its accept state, then D terminates in its
reject state.
• If H terminates in its reject state, then D terminates in its
accept state.
First observe that this new Turing machine D terminates on any input
string ⟨M⟩, because H terminates on every input. Next observe that, for any
input string ⟨M⟩for D:
• If ⟨M, ⟨M⟩⟩∈ATM (i.e., M accepts ⟨M⟩), then D terminates in its
reject state.

162
Chapter 5.
Decidable and Undecidable Languages
• If ⟨M, ⟨M⟩⟩̸∈ATM (i.e., M rejects ⟨M⟩or M does not terminate on
input ⟨M⟩), then D terminates in its accept state.
This means that for any string ⟨M⟩:
• If M accepts ⟨M⟩, then D rejects ⟨M⟩.
• If M rejects ⟨M⟩or M does not terminate on input ⟨M⟩, then D
accepts ⟨M⟩.
We now consider what happens if we give the Turing machine D the string
⟨D⟩as input, i.e., we take M = D:
• If D accepts ⟨D⟩, then D rejects ⟨D⟩.
• If D rejects ⟨D⟩or D does not terminate on input ⟨D⟩, then D accepts
⟨D⟩.
Since D terminates on every input string, this means that
• If D accepts ⟨D⟩, then D rejects ⟨D⟩.
• If D rejects ⟨D⟩, then D accepts ⟨D⟩.
This is clearly a contradiction. Therefore, the Turing machine H that decides
the language ATM cannot exist and, thus, ATM is undecidable. We have
proved the following result:
Theorem 5.1.6 The language ATM is undecidable.
5.1.5
The Halting Problem
We deﬁne the following language:
Halt = {⟨P, w⟩:
P is a Java program that terminates on
the input string w}.
Theorem 5.1.7 The language Halt is undecidable.
Proof. The proof is by contradiction. Thus, we assume that the language
Halt is decidable. Then there exists a Java program H that takes as input a
string of the form ⟨P, w⟩, where P is an arbitrary Java program and w is an
arbitrary input for P. The program H has the following property:

5.1.
Decidability
163
• If ⟨P, w⟩∈Halt (i.e., program P terminates on input w), then H
outputs true.
• If ⟨P, w⟩̸∈Halt (i.e., program P does not terminate on input w), then
H outputs false.
• In particular, H terminates on any input ⟨P, w⟩.
We will write the output of H as H(P, w). Moreover, we will denote by P(w)
the computation obtained by running the program P on the input w. Hence,
H(P, w) =
 true
if P(w) terminates,
false
if P(w) does not terminate.
Consider the following algorithm Q, which takes as input the encoding
⟨P⟩of an arbitrary Java program P:
Algorithm Q(⟨P⟩):
while H(P, ⟨P⟩) = true
do have a beer
endwhile
Since H is a Java program, this new algorithm Q can also be written as
a Java program. Observe that
Q(⟨P⟩) terminates if and only if H(P, ⟨P⟩) = false.
This means that for every Java program P,
Q(⟨P⟩) terminates if and only if P(⟨P⟩) does not terminate.
(5.1)
What happens if we run the Java program Q on the input string ⟨Q⟩?
In other words, what happens if we run Q(⟨Q⟩)? Then, in (5.1), we have to
replace all occurrences of P by Q. Hence,
Q(⟨Q⟩) terminates if and only if Q(⟨Q⟩) does not terminate.
This is obviously a contradiction, and we can conclude that the Java program
H does not exist. Therefore, the language Halt is undecidable.

164
Chapter 5.
Decidable and Undecidable Languages
Remark 5.1.8 In this proof, we run the Java program Q on the input ⟨Q⟩.
This means that the input to Q is a description of itself. In other words, we
give Q itself as input. This is an example of what is called self-reference. An-
other example of self-reference can be found in Remark 5.1.8 of the textbook
Introduction to Theory of Computation by A. Maheshwari and M. Smid.
5.2
Countable sets
The proofs that we gave in Sections 5.1.4 and 5.1.5 seem to be bizarre. In
this section, we will convince you that these proofs in fact use a technique
that you have seen in the course COMP 1805: Cantor’s Diagonalization.
Let A and B be two sets and let f : A →B be a function. Recall that f
is called a bijection, if
• f is one-to-one (or injective), i.e., for any two distinct elements a and
a′ in A, we have f(a) ̸= f(a′), and
• f is onto (or surjective), i.e., for each element b ∈B, there exists an
element a ∈A, such that f(a) = b.
The set of natural numbers is denoted by N. That is, N = {1, 2, 3, . . .}.
Deﬁnition 5.2.1 Let A and B be two sets. We say that A and B have the
same size, if there exists a bijection f : A →B.
Deﬁnition 5.2.2 Let A be a set. We say that A is countable, if A is ﬁnite,
or A and N have the same size.
In other words, if A is an inﬁnite and countable set, then there exists a
bijection f : N →A, and we can write A as
A = {f(1), f(2), f(3), f(4), . . .}.
Since f is a bijection, every element of A occurs exactly once in the set on
the right-hand side. This means that we can number the elements of A using
the positive integers: Every element of A receives a unique number.
Theorem 5.2.3 The following sets are countable:

5.2.
Countable sets
165
1. The set Z of integers:
Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}.
2. The Cartesian product N × N:
N × N = {(m, n) : m ∈N, n ∈N}.
3. The set Q of rational numbers:
Q = {m/n : m ∈Z, n ∈Z, n ̸= 0}.
Proof. To prove that the set Z is countable, we have to give each element of
Z a unique number in N. We obtain this numbering, by listing the elements
of Z in the following order:
0, 1, −1, 2, −2, 3, −3, 4, −4, . . .
In this (inﬁnite) list, every element of Z occurs exactly once. The number of
an element of Z is given by its position in this list.
Formally, deﬁne the function f : N →Z by
f(n) =
 n/2
if n is even,
−(n −1)/2
if n is odd.
This function f is a bijection and, therefore, the sets N and Z have the same
size. Hence, the set Z is countable.
For the proofs of the other two claims, we refer to the course COMP 1805.
We now use Cantor’s Diagonalization principle to prove that the set of
real numbers is not countable:
Theorem 5.2.4 The set R of real numbers is not countable.
Proof. Deﬁne
A = {x ∈R : 0 ≤x < 1}.
We will prove that the set A is not countable. This will imply that the set
R is not countable, because A ⊆R.

166
Chapter 5.
Decidable and Undecidable Languages
The proof that A is not countable is by contradiction. So we assume that
A is countable. Then there exists a bijection f : N →A. Thus, for each
n ∈N, f(n) is a real number between zero and one. We can write
A = {f(1), f(2), f(3), . . .},
(5.2)
where every element of A occurs exactly once in the set on the right-hand
side.
Consider the real number f(1). We can write this number in decimal
notation as
f(1) = 0.d11d12d13 . . . ,
where each d1i is a digit in the set {0, 1, 2, . . . , 9}. In general, for every n ∈N,
we can write the real number f(n) as
f(n) = 0.dn1dn2dn3 . . . ,
where, again, each dni is a digit in {0, 1, 2, . . . , 9}.
We deﬁne the real number
x = 0.d1d2d3 . . . ,
where, for each integer n ≥1,
dn =
 4
if dnn ̸= 4,
5
if dnn = 4.
Observe that x is a real number between zero and one, i.e., x ∈A. Therefore,
by (5.2), there is an element n ∈N, such that f(n) = x. We compare the
n-th digits of f(n) and x:
• The n-th digit of f(n) is equal to dnn.
• The n-th digit of x is equal to dn.
Since f(n) and x are equal, their n-th digits must be equal, i.e., dnn = dn.
But, by the deﬁnition of dn, we have dnn ̸= dn. This is a contradiction and,
therefore, the set A is not countable.
Notice how we deﬁned the real number x: For each n ≥1, the n-th digit
of x is not equal to the n-th digit of f(n). Therefore, for each n ≥1, x ̸= f(n)
and, thus, x ̸∈A.

5.2.
Countable sets
167
The ﬁnal result of this section is the fact that for every set A, its power
set
P(A) = {B : B ⊆A}
is “strictly larger” than A. Deﬁne the function f : A →P(A) by
f(a) = {a},
for any a in A. Since f is one-to-one, we can say that P(A) is “at least as
large as” A.
Theorem 5.2.5 Let A be an arbitrary set. Then A and P(A) do not have
the same size.
Proof. The proof is by contradiction. Thus, we assume that there exists a
bijection g : A →P(A). Deﬁne the set B as
B = {a ∈A : a ̸∈g(a)}.
Since B ∈P(A) and g is a bijection, there exists an element a in A such that
g(a) = B.
First assume that a ∈B. Since g(a) = B, we have a ∈g(a). But then,
from the deﬁnition of the set B, we have a ̸∈B, which is a contradiction.
Next assume that a ̸∈B.
Since g(a) = B, we have a ̸∈g(a).
But
then, from the deﬁnition of the set B, we have a ∈B, which is again a
contradiction.
We conclude that the bijection g does not exist. Therefore, A and P(A)
do not have the same size.
5.2.1
The Halting Problem revisited
Now that we know about countability, we give a diﬀerent way to look at the
proof in Section 5.1.5 of the fact that the language
Halt = {⟨P, w⟩:
P is a Java program that terminates on
the input string w}
is undecidable.
You should convince yourself that the proof given below
follows the same reasoning as the one used in the proof of Theorem 5.2.4.
We ﬁrst argue that the set of all Java programs is countable. Indeed,
every Java program P can be described by a ﬁnite amount of text. In fact,

168
Chapter 5.
Decidable and Undecidable Languages
we have been using ⟨P⟩to denote such a description by a binary string. For
any integer n ≥0, there are at most 2n (i.e., ﬁnitely many) Java programs
P whose description ⟨P⟩has length n. Therefore, to obtain a list of all Java
programs, we do the following:
• List all Java programs P whose description ⟨P⟩has length 0. (Well,
the empty string does not describe any Java program, so in this step,
nothing happens.)
• List all Java programs P whose description ⟨P⟩has length 1.
• List all Java programs P whose description ⟨P⟩has length 2.
• List all Java programs P whose description ⟨P⟩has length 3.
• Etcetera, etcetera.
In this inﬁnite list, every Java program occurs exactly once. Therefore, the
set of all Java programs is countable.
Consider an inﬁnite list
P1, P2, P3, . . .
in which every Java program occurs exactly once.
Assume that the language Halt is decidable. Then there exists a Java
program H that decides this language. We may assume that, on input ⟨P, w⟩,
H returns true if P terminates on input w, and false if P does not terminate
on input w.
We construct a new Java program D that does the following:
Algorithm D:
On input ⟨Pn⟩, where n is a positive integer, the
new Java program D does the following:
Step 1: Run the Java program H on the input ⟨Pn, ⟨Pn⟩⟩.
Step 2:
• If H returns true, then D goes into an inﬁnite loop.
• If H returns false, then D returns true and terminates its com-
putation.

5.3.
Rice’s Theorem
169
Observe that D can be written as a Java program. Therefore, there exists
an integer n ≥1 such that D = Pn. The next two observations follow from
the pseudocode:
• If D terminates on input ⟨Pn⟩, then H returns false on input ⟨Pn, ⟨Pn⟩⟩,
i.e., Pn does not terminate on input ⟨Pn⟩.
• If D does not terminate on input ⟨Pn⟩, then H returns true on input
⟨Pn, ⟨Pn⟩⟩, i.e., Pn terminates on input ⟨Pn⟩.
Thus,
• D terminates on input ⟨Pn⟩if and only if Pn does not terminate on
input ⟨Pn⟩.
Since D = Pn, this becomes
• D terminates on input ⟨D⟩if and only if D does not terminate on input
⟨D⟩.
Thus, we have obtained a contradiction.
Remark 5.2.6 We deﬁned the Java program D in such a way that, for each
n ≥1, the computation of D on input ⟨Pn⟩diﬀers from the computation of
Pn on input ⟨Pn⟩. Hence, for each n ≥1, D ̸= Pn. However, since D is a
Java program, there must be an integer n ≥1 such that D = Pn.
5.3
Rice’s Theorem
We have seen two examples of undecidable languages: ATM and Halt. In this
section, we prove that many languages involving Turing machines (or Java
programs) are undecidable.
Deﬁne T to be the set of binary encodings of all Turing machines, i.e.,
T = {⟨M⟩: M is a Turing machine with input alphabet {0,1}}.
Theorem 5.3.1 (Rice) Let P be a subset of T such that
1. P ̸= ∅, i.e., there exists a Turing machine M such that ⟨M⟩∈P,
2. P is a proper subset of T , i.e., there exists a Turing machine N such
that ⟨N⟩̸∈P, and

170
Chapter 5.
Decidable and Undecidable Languages
3. for any two Turing machines M1 and M2 with L(M1) = L(M2),
(a) either both ⟨M1⟩and ⟨M2⟩are in P or
(b) none of ⟨M1⟩and ⟨M2⟩is in P.
Then the language P is undecidable.
You can think of P as the set of all Turing machines that satisfy a certain
property. The ﬁrst two conditions state that at least one Turing machine
satisﬁes this property and not all Turing machines satisfy this property. The
third condition states that, for any Turing machine M, whether or not M
satisﬁes this property only depends on the language L(M) of M.
Here are some examples of languages that satisfy the conditions in Rice’s
Theorem:
P1 = {⟨M⟩: M is a Turing machine and ǫ ∈L(M)},
P2 = {⟨M⟩: M is a Turing machine and L(M) = {1011, 001100}},
P3 = {⟨M⟩: M is a Turing machine and L(M) is a regular language}.
You are encouraged to verify that Rice’s Theorem indeed implies that each
of P1, P2, and P3 is undecidable.
5.3.1
Proof of Rice’s Theorem
The strategy of the proof is as follows: Assuming that the language P is
decidable, we show that the language
Halt = {⟨M, w⟩:
M is a Turing machine that terminates on
the input string w}
is decidable. This will contradict Theorem 5.1.7.
The assumption that P is decidable implies the existence of a Turing
machine H that decides P. Observe that H takes as input a binary string
⟨M⟩encoding a Turing machine M. In order to show that Halt is decidable,
we need a Turing machine that takes as input a binary string ⟨M, w⟩encoding
a Turing machine M and a binary string w. In the rest of this section, we
will explain how this Turing machine can be obtained.

5.3.
Rice’s Theorem
171
Let M1 be a Turing machine that, for any input string, switches in its
ﬁrst computation step from its start state to its reject state. In other words,
M1 is a Turing machine with L(M1) = ∅. We assume that
⟨M1⟩̸∈P.
(At the end of the proof, we will consider the case when ⟨M1⟩∈P.) We also
choose a Turing machine M2 such that
⟨M2⟩∈P.
Consider a ﬁxed Turing machine M and a ﬁxed binary string w. We
construct a new Turing machine TMw that takes as input an arbitrary binary
string x:
Turing machine TMw(x):
run Turing machine M on input w;
if M terminates
then run M2 on input x;
if M2 terminates in the accept state
then terminate in the accept state
else if M2 terminates in the reject state
then terminate in the reject state
endif
endif
endif
We determine the language L(TMw) of this new Turing machine. In other
words, we determine which strings x are accepted by TMw.
• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it
follows from the pseudocode that for any string x,
x is accepted by TMw if and only if x is accepted by M2.
Thus, L(TMw) = L(M2).
• Assume that M does not terminate on input w, i.e., ⟨M, w⟩̸∈Halt.
Then it follows from the pseudocode that for any string x, TMw does
not terminate on input x. Thus, L(TMw) = ∅. In particular, L(TMw) =
L(M1).

172
Chapter 5.
Decidable and Undecidable Languages
Recall that ⟨M1⟩̸∈P, whereas ⟨M2⟩∈P. Then the following follows from
the third condition in Rice’s Theorem:
• If ⟨M, w⟩∈Halt, then ⟨TMw⟩∈P.
• If ⟨M, w⟩̸∈Halt, then ⟨TMw⟩̸∈P.
Thus, we have obtained a connection between the languages P and Halt.
This suggests that we proceed as follows.
Assume that the language P is decidable. Let H be a Turing machine
that decides P. Then, for any Turing machine M,
• if ⟨M⟩∈P, then H accepts the string ⟨M⟩,
• if ⟨M⟩̸∈P, then H rejects the string ⟨M⟩, and
• H terminates on any input string.
We construct a new Turing machine H′ that takes as input an arbitrary
string ⟨M, w⟩, where M is a Turing machine and w is a binary string:
Turing machine H′(⟨M, w⟩):
construct the Turing machine TMw described above;
run H on input ⟨TMw⟩;
if H terminates in the accept state
then terminate in the accept state
else terminate in the reject state
endif
It follows from the pseudocode that H′ terminates on any input.
We
observe the following:
• Assume that ⟨M, w⟩∈Halt. Then we have seen before that ⟨TMw⟩∈P.
Since H decides the language P, it follows that H accepts the string
⟨TMw⟩. Therefore, from the pseudocode, H′ accepts the string ⟨M, w⟩.
• Assume that ⟨M, w⟩̸∈Halt. Then we have seen before that ⟨TMw⟩̸∈
P. Since H decides the language P, it follows that H rejects (and
terminates on) the string ⟨TMw⟩. Therefore, from the pseudocode, H′
rejects (and terminates on) the string ⟨M, w⟩.

5.4.
Enumerability
173
We have shown that the Turing machine H′ decides the language Halt.
This is a contradiction and, therefore, we conclude that the language P is
undecidable.
Until now, we assumed that ⟨M1⟩̸∈P. If ⟨M1⟩∈P, then we repeat the
proof with P replaced by its complement P. This revised proof then shows
that P is undecidable. Since for every language L,
L is decidable if and only if L is decidable,
we again conclude that P is undecidable.
5.4
Enumerability
We now come to the last class of languages in this chapter:
Deﬁnition 5.4.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. We
say that A is enumerable, if there exists a Turing machine M, such that for
every string w ∈Σ∗, the following holds:
1. If w ∈A, then the computation of the Turing machine M, on the input
string w, terminates in the accept state.
2. If w ̸∈A, then the computation of the Turing machine M, on the input
string w, does not terminate in the accept state. That is, either the
computation terminates in the reject state or the computation does not
terminate.
In other words, the language A is enumerable, if there exists an algorithm
having the following property. If w ∈A, then the algorithm terminates on
the input string w and tells us that w ∈A. On the other hand, if w ̸∈A,
then either (i) the algorithm terminates on the input string w and tells us
that w ̸∈A or (ii) the algorithm does not terminate on the input string w,
in which case it does not tell us that w ̸∈A.
In Section 5.5, we will show where the term “enumerable” comes from.
The following theorem follows immediately from Deﬁnitions 5.1.1 and 5.4.1.
Theorem 5.4.2 Every decidable language is enumerable.
In the following subsections, we will give some examples of enumerable
languages.

174
Chapter 5.
Decidable and Undecidable Languages
5.4.1
Hilbert’s problem
We have seen Hilbert’s problem in Section 4.4: Is there an algorithm that
decides, for any given polynomial p with integer coeﬃcients, whether or not
p has integral roots? If we formulate this problem in terms of languages,
then Hilbert asked whether or not the language
Hilbert = {⟨p⟩:
p is a polynomial with integer coeﬃcients
that has an integral root}
is decidable. As usual, ⟨p⟩denotes the binary string that forms an encoding
of the polynomial p.
As we mentioned in Section 4.4, it was proven by Matiyasevich in 1970
that the language Hilbert is not decidable.
We claim, that this language
is enumerable.
In order to prove this claim, we have to construct an al-
gorithm Hilbert with the following property: For any input polynomial p
with integer coeﬃcients,
• if p has an integral root, then algorithm Hilbert will ﬁnd one in a
ﬁnite amount of time,
• if p does not have an integral root, then either algorithm Hilbert ter-
minates and tells us that p does not have an integral root, or algorithm
Hilbert does not terminate.
Recall that Z denotes the set of integers. Algorithm Hilbert does the
following, on any input polynomial p with integer coeﬃcients.
Let n de-
note the number of variables in p. Algorithm Hilbert tries all elements
(x1, x2, . . . , xn) ∈Zn, in a systematic way, and for each such element, it
computes p(x1, x2, . . . , xn). If this value is zero, then algorithm Hilbert
terminates and accepts the input.
We observe the following:
• If p ∈Hilbert, then algorithm Hilbert terminates and accepts p, pro-
vided we are able to visit all elements (x1, x2, . . . , xn) ∈Zn in a “sys-
tematic way”.
• If p ̸∈Hilbert, then p(x1, x2, . . . , xn) ̸= 0 for all (x1, x2, . . . , xn) ∈Zn
and, therefore, algorithm Hilbert does not terminate.
These are exactly the requirements for the language Hilbert to be enumerable.

5.4.
Enumerability
175
It remains to explain how we visit all elements (x1, x2, . . . , xn) ∈Zn in a
systematic way. For any integer d ≥0, let Hd denote the hypercube in Zn
with sides of length 2d that is centered at the origin. That is, Hd consists
of the set of all points (x1, x2, . . . , xn) in Zn, such that −d ≤xi ≤d for all
1 ≤i ≤n and there exists at least one index j for which xj = d or xj = −d.
We observe that Hd contains a ﬁnite number of elements. In fact, if d ≥1,
then this number is equal to (2d + 1)n −(2d −1)n. The algorithm will visit
all elements (x1, x2, . . . , xn) ∈Zn, in the following order: First, it visits the
origin, which is the only element of H0. Then, it visits all elements of H1,
followed by all elements of H2, etc., etc.
To summarize, we obtain the following algorithm, proving that the lan-
guage Hilbert is enumerable:
Algorithm Hilbert(⟨p⟩):
n := the number of variables in p;
d := 0;
while d ≥0
do for each (x1, x2, . . . , xn) ∈Hd
do R := p(x1, x2, . . . , xn);
if R = 0
then terminate and accept
endif
endfor;
d := d + 1
endwhile
Theorem 5.4.3 The language Hilbert is enumerable.
5.4.2
The language ATM
We have shown in Section 5.1.4 that the language
ATM = {⟨M, w⟩: M is a Turing machine that accepts the string w}.
is undecidable. In this section, we will prove that this language is enumerable.
Thus, we have to construct an algorithm P having the following property,
for any given input string u:

176
Chapter 5.
Decidable and Undecidable Languages
• If
– u encodes a Turing machine M and an input string w for M (i.e.,
u is in the correct format ⟨M, w⟩) and
– ⟨M, w⟩∈ATM (i.e., M accepts w),
then algorithm P terminates in its accept state.
• In all other cases, either algorithm P terminates in its reject state, or
algorithm P does not terminate.
On input string u = ⟨M, w⟩, which is in the correct format, algorithm P does
the following:
1. It simulates the computation of M on input w.
2. If M terminates in its accept state, then P terminates in its accept
state.
3. If M terminates in its reject state, then P terminates in its reject state.
4. If M does not terminate, then P does not terminate.
Hence, if u = ⟨M, w⟩∈ATM, then M accepts w and, therefore, P accepts
u. On the other hand, if u = ⟨M, w⟩̸∈ATM , then M does not accept w. This
means that, on input w, M either terminates in its reject state or does not
terminate. But this implies that, on input u, P either terminates in its reject
state or does not terminate. This proves that algorithm P has the properties
that are needed in order to show that the language ATM is enumerable. We
have proved the following result:
Theorem 5.4.4 The language ATM is enumerable.
5.5
Where does the term “enumerable” come
from?
In Deﬁnition 5.4.1, we have deﬁned what it means for a language to be
enumerable. In this section, we will see where this term comes from.

5.5.
Where does the term “enumerable” come from?
177
Deﬁnition 5.5.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. An
enumerator for A is a Turing machine E having the following properties:
1. Besides the standard features as in Section 4.1, E has a print tape and
a print state. During its computation, E writes symbols of Σ on the
print tape. Each time, E enters the print state, the current string on
the print tape is sent to the printer and the print tape is made empty.
2. At the start of the computation, all tapes are empty and E is in the
start state.
3. Every string w in A is sent to the printer at least once.
4. Every string w that is not in A is never sent to the printer.
Thus, an enumerator E for A really enumerates all strings in the language
A. There is no particular order in which the strings of A are sent to the
printer. Moreover, a string in A may be sent to the printer multiple times.
If the language A is inﬁnite, then the Turing machine E obviously does not
terminate; however, every string in A (and only strings in A) will be sent to
the printer at some time during the computation.
To give an example, let A = {0n : n ≥0}. The following Turing machine
is an enumerator for A.
Turing machine StringsOfZeros:
n := 0;
while 1 + 1 = 2
do for i := 1 to n
do write 0 on the print tape
endfor;
enter the print state;
n := n + 1
endwhile
In the rest of this section, we will prove the following result.
Theorem 5.5.2 A language is enumerable if and only if it has an enumer-
ator.

178
Chapter 5.
Decidable and Undecidable Languages
For the ﬁrst part of the proof, assume that the language A has an enu-
merator E. We construct the following Turing machine M, which takes an
arbitrary string w as input:
Turing machine M(w):
run E; every time E enters the print state:
let v be the string on the print tape;
if w = v
then terminate in the accept state
endif
The Turing machine M has the following properties:
• If w ∈A, then w will be sent to the printer at some time during the
computation of E. It follows from the pseudocode that, on input w,
M terminates in the accept state.
• If w ̸∈A, then E will never sent w to the printer. It follows from the
pseudocode that, on input w, M does not terminate.
Thus, M satisﬁes the conditions in Deﬁnition 5.4.1. We conclude that the
language A is enumerable.
To prove the converse, we now assume that A is enumerable. Let M be
a Turing machine that satisﬁes the conditions in Deﬁnition 5.4.1.
We ﬁx an inﬁnite list
s1, s2, s3, . . .
of all strings in Σ∗. For example, if Σ = {0, 1}, then we can take this list to
be
ǫ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .
We construct the following Turing machine E, which takes the empty
string as input:

5.6.
Most languages are not enumerable
179
Turing machine E:
n := 1;
while 1 + 1 = 2
do for i := 1 to n
do run M for n steps on the input string si;
if M accepts si within n steps
then write si on the print tape;
enter the print state
endif
endfor;
n := n + 1
endwhile
We claim that E is an enumerator for the language A. To prove this, it
is obvious that any string that is sent to the printer by E belongs to A.
It remains to prove that every string in A will be sent to the printer by E.
Let w be a string in A. Then, on input w, the Turing machine M terminates
in the accept state. Let m be the number of steps made by M on input w.
Let i be the index such that w = si. Deﬁne n = max(m, i). Consider the
n-th iteration of the while-loop and the i-th iteration of the for-loop. In this
iteration, M accepts si = w in m ≤n steps and, therefore, w is sent to the
printer.
5.6
Most languages are not enumerable
In this section, we will prove that most languages are not enumerable. The
proof is based on the following two facts:
• The set consisting of all enumerable languages is countable; we will
prove this in Section 5.6.1.
• The set consisting of all languages is not countable; we will prove this
in Section 5.6.2.

180
Chapter 5.
Decidable and Undecidable Languages
5.6.1
The set of enumerable languages is countable
We deﬁne the set E as
E = {A : A ⊆{0, 1}∗is an enumerable language}.
In words, E is the set whose elements are the enumerable languages. Every
element of E is an enumerable language. Hence, every element of the set E
is itself a set consisting of strings.
Lemma 5.6.1 The set E is countable.
Proof. Let A ⊆{0, 1}∗be an enumerable language. There exists a Turing
machine TA that satisﬁes the conditions in Deﬁnition 5.4.1.
This Turing
machine TA can be uniquely speciﬁed by a string in English. This string can
be converted to a binary string sA. Hence, the binary string sA is a unique
encoding of the Turing machine TA.
Consider the set
S = {sA : A ⊆{0, 1}∗is an enumerable language}.
Observe that the function f : E →S, deﬁned by f(A) = sA for each A ∈E,
is a bijection. Therefore, the sets E and S have the same size. Hence, in
order to prove that the set E is countable, it is suﬃcient to prove that the
set S is countable.
Why is the set S countable? For each integer n ≥0, there are exactly 2n
binary strings of length n. Since there are binary strings that are not encod-
ings of Turing machines, the set S contains at most 2n strings of length n.
In particular, the number of strings in S having length n is ﬁnite. Therefore,
we obtain an inﬁnite list of the elements of S in the following way:
• List all strings in S having length 0. (Well, the empty string is not in
S, so in this step, nothing happens.)
• List all strings in S having length 1.
• List all strings in S having length 2.
• List all strings in S having length 3.
• Etcetera, etcetera.
In this inﬁnite list, every element of S occurs exactly once. Therefore, S is
countable.

5.6.
Most languages are not enumerable
181
5.6.2
The set of all languages is not countable
We deﬁne the set L as
L = {A : A ⊆{0, 1}∗is a language}.
In words, L is the set consisting of all languages. Every element of the set L
is a set consisting of strings.
Lemma 5.6.2 The set L is not countable.
Proof. We deﬁne the set B as
B = {w : w is an inﬁnite binary sequence}.
We claim that this set is not countable. The proof of this claim is almost
identical to the proof of Theorem 5.2.4. We assume that the set B is count-
able. Then there exists a bijection f : N →B. Thus, for each n ∈N, f(n) is
an inﬁnite binary sequence. We can write
B = {f(1), f(2), f(3), . . .},
(5.3)
where every element of B occurs exactly once in the set on the right-hand
side.
We deﬁne the inﬁnite binary sequence w = w1w2w3 . . ., where, for each
integer n ≥1,
wn =

1
if the n-th bit of f(n) is 0,
0
if the n-th bit of f(n) is 1.
Since w ∈B, it follows from (5.3) that there is an element n ∈N, such that
f(n) = w. Hence, the n-th bits of f(n) and w are equal. But, by deﬁnition,
these n-th bits are not equal. This is a contradiction and, therefore, the set
B is not countable.
In the rest of the proof, we will show that the sets L and B have the same
size. Since B is not countable, this will imply that L is not countable.
In order to prove that L and B have the same size, we have to show that
there exists a bijection
g : L →B.

182
Chapter 5.
Decidable and Undecidable Languages
We ﬁrst observe that the set {0, 1}∗is countable, because for each integer
n ≥0, there are only ﬁnitely many (to be precise, exactly 2n) binary strings
of length n. In fact, we can write
{0, 1}∗= {ǫ, 0, 1, 00, 01, 10, 11, 000, 001, 010, 100, 011, 101, 110, 111, . . .}.
For each integer n ≥1, we denote by sn the n-th string in this list. Hence,
{0, 1}∗= {s1, s2, s3, . . .}.
(5.4)
Now we are ready to deﬁne the bijection g : L →B: Let A ∈L, i.e.,
A ⊆{0, 1}∗is a language. We deﬁne the inﬁnite binary sequence g(A) as
follows: For each integer n ≥1, the n-th bit of g(A) is equal to

1
if sn ∈A,
0
if sn ̸∈A.
In words, the inﬁnite binary sequence g(A) contains a 1 exactly in those
positions n for which the string sn in (5.4) is in the language A.
To give an example, assume that A is the language consisting of all binary
strings that start with 0. The following table gives the corresponding inﬁnite
binary sequence g(A) (this sequence is obtained by reading the rightmost
column from top to bottom):
{0, 1}∗
A
g(A)
ǫ
not in A
0
0
in A
1
1
not in A
0
00
in A
1
01
in A
1
10
not in A
0
11
not in A
0
000
in A
1
001
in A
1
010
in A
1
100
not in A
0
011
in A
1
101
not in A
0
110
not in A
0
111
not in A
0
...
...
...

5.7.
Decidable versus enumerable languages
183
The function g deﬁned above has the following properties:
• If A and A′ are two diﬀerent languages in L, then g(A) ̸= g(A′).
• For every inﬁnite binary sequence w in B, there exists a language A in
L, such that g(A) = w.
This means that the function g is a bijection from L to B.
5.6.3
There are languages that are not enumerable
We have proved that the set
E = {A : A ⊆{0, 1}∗is an enumerable language}
is countable, whereas the set
L = {A : A ⊆{0, 1}∗is a language}
is not countable. This means that there are “more” languages in L than
there are in E, proving the following result:
Theorem 5.6.3 There exist languages that are not enumerable.
The proof given above shows the existence of languages that are not
enumerable. However, the proof does not give us a speciﬁc example of a
language that is not enumerable. In the next sections, we will see examples
of such languages. Before we move on to these examples, we mention the
diﬀerence between being countable and being enumerable:
• Any language A is countable, i.e., we can number the elements of A
and, thus, write
A = {s1, s2, s3, s4, . . .}.
• If the language A is enumerable, then, by Theorem 5.5.2, there is an
algorithm that produces this numbering.
• If the language A is not enumerable, then, again by Theorem 5.5.2,
there does not exist an algorithm that produces this numbering.

184
Chapter 5.
Decidable and Undecidable Languages
5.7
The relationship between decidable and
enumerable languages
We know from Theorem 5.4.2 that every decidable language is enumerable.
On the other hand, we know from Theorems 5.1.6 and 5.4.4 that the converse
is not true. The following result should not come as a surprise:
Theorem 5.7.1 Let Σ be an alphabet and let A ⊆Σ∗be a language. Then,
A is decidable if and only if both A and its complement A are enumerable.
Proof. We ﬁrst assume that A is decidable. Then, by Theorem 5.4.2, A
is enumerable. Since A is decidable, it is not diﬃcult to see that A is also
decidable. Then, again by Theorem 5.4.2, A is enumerable.
To prove the converse, we assume that both A and A are enumerable.
Since A is enumerable, there exists a Turing machine M1, such that for any
string w ∈Σ∗, the following holds:
• If w ∈A, then the computation of M1, on the input string w, terminates
in the accept state of M1.
• If w ̸∈A, then the computation of M1, on the input string w, terminates
in the reject state of M1 or does not terminate.
Similarly, since A is enumerable, there exists a Turing machine M2, such that
for any string w ∈Σ∗, the following holds:
• If w ∈A, then the computation of M2, on the input string w, terminates
in the accept state of M2.
• If w ̸∈A, then the computation of M2, on the input string w, terminates
in the reject state of M2 or does not terminate.
We construct a two-tape Turing machine M:

5.8.
Both A and A not enumerable
185
Two-tape Turing machine M: For any input string w ∈Σ∗, M
does the following:
• M simulates the computation of M1, on input w, on the ﬁrst
tape, and, simultaneously, it simulates the computation of M2,
on input w, on the second tape.
• If the simulation of M1 terminates in the accept state of M1,
then M terminates in its accept state.
• If the simulation of M2 terminates in the accept state of M2,
then M terminates in its reject state.
Observe the following:
• If w ∈A, then M1 terminates in its accept state and, therefore, M
terminates in its accept state.
• If w ̸∈A, then M2 terminates in its accept state and, therefore, M
terminates in its reject state.
We conclude that the Turing machine M accepts all strings in A, and rejects
all strings that are not in A. This proves that the language A is decidable.
We now use Theorem 5.7.1 to give examples of languages that are not
enumerable:
Theorem 5.7.2 The language ATM is not enumerable.
Proof. We know from Theorems 5.4.4 and 5.1.6 that the language ATM is
enumerable but not decidable. Combining these facts with Theorem 5.7.1
implies that the language ATM is not enumerable.
The following result can be proved in exactly the same way:
Theorem 5.7.3 The language Halt is not enumerable.

186
Chapter 5.
Decidable and Undecidable Languages
5.8
A language A such that both A and A are
not enumerable
In Theorem 5.7.2, we have seen that the complement of the language ATM
is not enumerable.
In Theorem 5.4.4, however, we have shown that the
language ATM itself is enumerable. In this section, we consider the language
EQTM = {⟨M1, M2⟩:
M1 and M2 are Turing machines
and L(M1) = L(M2)}.
We will show the following result:
Theorem 5.8.1 Both EQTM and its complement EQTM are not enumer-
able.
5.8.1
EQTM is not enumerable
Consider a ﬁxed Turing machine M and a ﬁxed binary string w. We construct
a new Turing machine TMw that takes as input an arbitrary binary string x:
Turing machine TMw(x):
run Turing machine M on input w;
terminate in the accept state
We determine the language L(TMw) of this new Turing machine. In other
words, we determine which strings x are accepted by TMw.
• Assume that M terminates on input w, i.e., ⟨M, w⟩∈Halt. Then it
follows from the pseudocode that every string x is accepted by TMw.
Thus, L(TMw) = {0, 1}∗.
• Assume that M does not terminate on input w, i.e., ⟨M, w⟩∈Halt.
Then it follows from the pseudocode that, for any string x, TMw does
not terminate on input x. Thus, L(TMw) = ∅.
Assume that the language EQ TM is enumerable. We will show that Halt
is enumerable as well, which will contradict Theorem 5.7.3.
Since EQTM is enumerable, there exists a Turing machine H having the
following property, for any two Turing machines M1 and M2:

5.8.
Both A and A not enumerable
187
• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept
state.
• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H either terminates in
the reject state or does not terminate.
We construct a new Turing machine H′ that takes as input an arbitrary
string ⟨M, w⟩, where M is a Turing machine and w is a binary string:
Turing machine H′(⟨M, w⟩):
construct a Turing machine M1 that rejects every input string;
construct the Turing machine TMw described above;
run H on input ⟨M1, TMw⟩;
if H terminates in the accept state
then terminate in the accept state
else if H terminates in the reject state
then terminate in the reject state
endif
endif
We observe the following:
• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =
∅. By our choice of M1, we have L(M1) = ∅as well. Therefore, H
accepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the
pseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.
• Assume that ⟨M, w⟩̸∈Halt, i.e., ⟨M, w⟩∈Halt. Then we have seen
before that L(TMw) ̸= ∅= L(M1). Therefore, on input ⟨M1, TMw⟩, H
either terminates in the reject state or does not terminate. It follows
from the pseudocode that, on input ⟨M, w⟩, H′ either terminates in the
reject state or does not terminate.
Thus, the Turing machine H′ has the properties needed to show that
the language Halt is enumerable. This is a contradiction and, therefore, we
conclude that the language EQTM is not enumerable.

188
Chapter 5.
Decidable and Undecidable Languages
5.8.2
EQTM is not enumerable
This proof is symmetric to the one in Section 5.8.1.
For a ﬁxed Turing
machine M and a ﬁxed binary string w, we will use the same Turing machine
TMw as in Section 5.8.1.
Assume that the language EQ TM is enumerable. We will show that Halt
is enumerable as well, which will contradict Theorem 5.7.3.
Since EQTM is enumerable, there exists a Turing machine H having the
following property, for any two Turing machines M1 and M2:
• If L(M1) ̸= L(M2), then, on input ⟨M1, M2⟩, H terminates in the accept
state.
• If L(M1) = L(M2), then, on input ⟨M1, M2⟩, H either terminates in
the reject state or does not terminate.
We construct a new Turing machine H′ that takes as input an arbitrary
string ⟨M, w⟩, where M is a Turing machine and w is a binary string:
Turing machine H′(⟨M, w⟩):
construct a Turing machine M1 that accepts every input string;
construct the Turing machine TMw of Section 5.8.1;
run H on input ⟨M1, TMw⟩;
if H terminates in the accept state
then terminate in the accept state
else if H terminates in the reject state
then terminate in the reject state
endif
endif
We observe the following:
• Assume that ⟨M, w⟩∈Halt. Then we have seen before that L(TMw) =
∅. Thus, by our choice of M1, we have L(TMw) ̸= L(M1). Therefore, H
accepts (and terminates on) the input ⟨M1, TMw⟩. It follows from the
pseudocode that H′ accepts (and terminates on) the string ⟨M, w⟩.
• Assume that ⟨M, w⟩̸∈Halt. Then L(TMw) = {0, 1}∗= L(M1) and, on
input ⟨M1, TMw⟩, H either terminates in the reject state or does not

Exercises
189
terminate. It follows from the pseudocode that, on input ⟨M, w⟩, H′
either terminates in the reject state or does not terminate.
Thus, the Turing machine H′ has the properties needed to show that
the language Halt is enumerable. This is a contradiction and, therefore, we
conclude that the language EQTM is not enumerable.
Exercises
5.1 Prove that the language
{w ∈{0, 1}∗: w is the binary representation of 2n for some n ≥0}
is decidable. In other words, construct a Turing machine that gets as input
an arbitrary number x ∈N, represented in binary as a string w, and that
decides whether or not x is a power of two.
5.2 Let F be the set of all functions f : N →N.
Prove that F is not
countable.
5.3 A function f : N →N is called computable, if there exists a Turing
machine, that gets as input an arbitrary positive integer n, written in binary,
and gives as output the value of f(n), again written in binary. This Turing
machine has a ﬁnal state. As soon as the Turing machine enters this ﬁnal
state, the computation terminates, and the output is the binary string that
is written on its tape.
Prove that there exist functions f : N →N that are not computable.
5.4 Let n be a ﬁxed positive integer, and let k be the number of bits in the
binary representation of n. (Hence, k = 1 + ⌊log n⌋.) Construct a Turing
machine with one tape, tape alphabet {0, 1, 2}, and exactly k + 1 states
q0, q1, . . . , qk, that does the following:
Start of the computation: The tape is empty, i.e., every cell of the tape
contains 2, and the Turing machine is in the start state q0.
End of the computation: The tape contains the binary representation of
the integer n, the tape head is on the rightmost bit of the binary represen-
tation of n, and the Turing machine is in the ﬁnal state qk.

190
Chapter 5.
Decidable and Undecidable Languages
The Turing machine in this exercise does not have an accept state or a
reject state; instead, it has a ﬁnal state qk. As soon as state qk is entered,
the Turing machine terminates.
5.5 Give an informal description (in plain English) of a Turing machine
with three tapes, that gets as input the binary representation of an arbitrary
integer m ≥1, and returns as output the unary representation of m.
Start of the computation: The ﬁrst tape contains the binary representa-
tion of the input m. The other two tapes are empty (i.e., contain only 2s).
The Turing machine is in the start state.
End of the computation: The third tape contains the unary representation
of m, i.e., a string consisting of m many ones. The Turing machine is in the
ﬁnal state.
The Turing machine in this exercise does not have an accept state or a
reject state; instead, it has a ﬁnal state. As soon as this ﬁnal state is entered,
the Turing machine terminates.
Hint: Use the second tape to maintain a string of ones, whose length is
a power of two.
5.6 In this exercise, you are asked to prove that the busy beaver function
BB : N →N is not computable.
For any integer n ≥1, we deﬁne TM n to be the set of all Turing machines
M, such that
• M has one tape,
• M has exactly n states,
• the tape alphabet of M is {0, 1, 2}, and
• M terminates, when given the empty string ǫ as input.
For every Turing machine M ∈TM n, we deﬁne f(M) to be the number of
ones on the tape, after the computation of M, on the empty input string,
has terminated.
The busy beaver function BB : N →N is deﬁned as
BB(n) := max{f(M) : M ∈TM n}, for every n ≥1.

Exercises
191
In words, BB(n) is the maximum number of ones that any Turing machine
with n states can produce, when given the empty string as input, and as-
suming the Turing machine terminates on this input.
Prove that the function BB is not computable.
Hint: Assume that BB is computable. Then there exists a Turing ma-
chine M that, for any given n ≥1, computes the value of BB(n). Fix a large
integer n ≥1. Deﬁne (in plain English) a Turing machine that, when given
the empty string as input, terminates and outputs a string consisting of more
than BB(n) many ones. Use Exercises 5.4 and 5.5 to argue that there exists
such a Turing machine having O(log n) states. Then, if you assume that n
is large enough, the number of states is at most n.
5.7 Since the set
T = {M : M is a Turing machine}
is countable, there is an inﬁnite list
M1, M2, M3, M4, . . . ,
such that every Turing machine occurs exactly once in this list.
For any positive integer n, let ⟨n⟩denote the binary representation of n;
observe that ⟨n⟩is a binary string.
Let A be the language deﬁned as
A = {⟨n⟩:
the Turing machine Mn terminates on the input string ⟨n⟩,
and it rejects this string}.
Prove that the language A is undecidable.
5.8 Consider the three languages
Empty = {⟨M⟩: M is a Turing machine for which L(M) = ∅},
UselessState = {⟨M, q⟩:
M is a Turing machine, q is a state of M,
for every input string w, the computation of M on
input w never visits state q},
and
EQ TM = {⟨M1, M2⟩:
M1 and M2 are Turing machines
and L(M1) = L(M2)}.

192
Chapter 5.
Decidable and Undecidable Languages
• Use Rice’s Theorem to show that Empty is undecidable.
• Use the ﬁrst part to show that UselessState is undecidable.
• Use the ﬁrst part to show that EQTM is undecidable.
5.9 Consider the language
REGTM = {⟨M⟩: M is a Turing machine whose language L(M) is regular}.
Use Rice’s Theorem to prove that REGTM is undecidable.
5.10 We have seen in Section 5.1.4 that the language
ATM = {⟨M, w⟩: M is a Turing machine that accepts w}
is undecidable. Consider the language REGTM of Exercise 5.9. The questions
below will lead you through a proof of the claim that the language REGTM
is undecidable.
Consider a ﬁxed Turing machine M and a ﬁxed binary string w. We
construct a new Turing machine TMw that takes as input an arbitrary binary
string x:
Turing machine TMw(x):
if x = 0n1n for some n ≥0
then terminate in the accept state
else run M on the input string w;
if M terminates in the accept state
then terminate in the accept state
else if M terminates in the reject state
then terminate in the reject state
endif
endif
endif
Answer the following two questions:
• Assume that M accepts the string w. What is the language L(TMw) of
the new Turing machine TMw?

Exercises
193
• Assume that M does not accept the string w. What is the language
L(TMw) of the new Turing machine TMw?
The goal is to prove that the language REGTM is undecidable. We will
prove this by contradiction. Thus, we assume that R is a Turing machine
that decides REGTM . Recall what this means:
• If M is a Turing machine whose language is regular, then R, when
given ⟨M⟩as input, will terminate in the accept state.
• If M is a Turing machine whose language is not regular, then R, when
given ⟨M⟩as input, will terminate in the reject state.
We construct a new Turing machine R′ which takes as input an arbitrary
Turing machine M and an arbitrary binary string w:
Turing machine R′(⟨M, w⟩):
construct the Turing machine TMw described above;
run R on the input ⟨TMw⟩;
if R terminates in the accept state
then terminate in the accept state
else if R terminates in the reject state
then terminate in the reject state
endif
endif
Prove that M accepts w if and only if R′ (when given ⟨M, w⟩as input),
terminates in the accept state.
Now ﬁnish the proof by arguing that the language REGTM is undecidable.
5.11 A Java program P is called a Hello-World-program, if the following is
true: When given the empty string ǫ as input, P outputs the string Hello
World and then terminates. (We do not care what P does when the input
string is non-empty.)
Consider the language
HW = {⟨P⟩: P is a Hello-World-program}.
The questions below will lead you through a proof of the claim that the
language HW is undecidable.

194
Chapter 5.
Decidable and Undecidable Languages
Consider a ﬁxed Java program P and a ﬁxed binary string w. We write
a new Java program JP w which takes as input an arbitrary binary string x:
Java program JP w(x):
run P on the input w;
print Hello World
• Argue that P terminates on input w if and only if ⟨JP w⟩∈HW .
The goal is to prove that the language HW is undecidable. We will prove this
by contradiction. Thus, we assume that H is a Java program that decides
HW . Recall what this means:
• If P is a Hello-World-program, then H, when given ⟨P⟩as input, will
terminate in the accept state.
• If P is not a Hello-World-program, then H, when given ⟨P⟩as input,
will terminate in the reject state.
We write a new Java program H′ which takes as input the binary encoding
⟨P, w⟩of an arbitrary Java program P and an arbitrary binary string w:
Java program H′(⟨P, w⟩):
construct the Java program JP w described above;
run H on the input ⟨JP w⟩;
if H terminates in the accept state
then terminate in the accept state
else terminate in the reject state
endif
Argue that the following are true:
• For any input ⟨P, w⟩, H′ terminates.
• If P terminates on input w, then H′ (when given ⟨P, w⟩as input),
terminates in the accept state.
• If P does not terminate on input w, then H′ (when given ⟨P, w⟩as
input), terminates in the reject state.

Exercises
195
Now ﬁnish the proof by arguing that the language HW is undecidable.
5.12 Prove that the language Halt, see Section 5.1.5, is enumerable.
5.13 We deﬁne the following language:
L = {u
:
u = ⟨0, M, w⟩for some ⟨M, w⟩∈ATM ,
or u = ⟨1, M, w⟩for some ⟨M, w⟩̸∈ATM } .
Prove that neither L nor its complement L is enumerable.
Hint: There are two ways to solve this exercise. In the ﬁrst solution, (i)
you assume that L is enumerable, and then prove that ATM is decidable, and
(ii) you assume that L is enumerable, and then prove that ATM is decidable.
In the second solution, (i) you assume that L is enumerable, and then prove
that ATM is enumerable, and (ii) you assume that L is enumerable, and then
prove that ATM is enumerable.

196
Chapter 5.
Decidable and Undecidable Languages

Chapter 6
Complexity Theory
In the previous chapters, we have considered the problem of what can be
computed by Turing machines (i.e., computers) and what cannot be com-
puted. We did not, however, take the eﬃciency of the computations into
account. In this chapter, we introduce a classiﬁcation of decidable languages
A, based on the running time of the “best” algorithm that decides A. That
is, given a decidable language A, we are interested in the “fastest” algorithm
that, for any given string w, decides whether or not w ∈A.
6.1
The running time of algorithms
Let M be a Turing machine, and let w be an input string for M. We deﬁne
the running time tM(w) of M on input w as
tM(w) := the number of computation steps made by M on input w.
As usual, we denote by |w|, the number of symbols in the string w. We
denote the set of non-negative integers by N0.
Deﬁnition 6.1.1 Let Σ be an alphabet, let T : N0 →N0 be a function, let
A ⊆Σ∗be a decidable language, and let F : Σ∗→Σ∗be a computable
function.
• We say that the Turing machine M decides the language A in time T,
if
tM(w) ≤T(|w|)
for all strings w in Σ∗.

198
Chapter 6.
Complexity Theory
• We say that the Turing machine M computes the function F in time
T, if
tM(w) ≤T(|w|)
for all strings w ∈Σ∗.
In other words, the “running time function” T is a function of the length
of the input, which we usually denote by n. For any n, the value of T(n) is
an upper bound on the running time of the Turing machine M, on any input
string of length n.
To give an example, consider the Turing machine of Section 4.2.1 that
decides, using one tape, the language consisting of all palindromes. The tape
head of this Turing machine moves from the left to the right, then back to
the left, then to the right again, back to the left, etc. Each time it reaches
the leftmost or rightmost symbol, it deletes this symbol. The running time
of this Turing machine, on any input string of length n, is
O(1 + 2 + 3 + . . . + n) = O(n2).
On the other hand, the running time of the Turing machine of Section 4.2.2,
which also decides the palindromes, but using two tapes instead of just one,
is O(n).
In Section 4.4, we mentioned that all computation models listed there are
equivalent, in the sense that if a language can be decided in one model, it
can be decided in any of the other models. We just saw, however, that the
language consisting of all palindromes allows a faster algorithm on a two-
tape Turing machine than on one-tape Turing machines. (Even though we
did not prove this, it is true that Ω(n2) is a lower bound on the running
time to decide palindromes on a one-tape Turing machine.) The following
theorem can be proved.
Theorem 6.1.2 Let A be a language (resp. let F be a function) that can be
decided (resp. computed) in time T by an algorithm of type M. Then there is
an algorithm of type N that decides A (resp. computes F) in time T ′, where
M
N
T ′
k-tape Turing machine
one-tape Turing machine
O(T 2)
one-tape Turing machine
Java program
O(T 2)
Java program
k-tape Turing machine
O(T 4)

6.2.
The complexity class P
199
6.2
The complexity class P
Deﬁnition 6.2.1 We say that algorithm M decides the language A (resp.
computes the function F) in polynomial time, if there exists an integer k ≥1,
such that the running time of M is O(nk), for any input string of length n.
It follows from Theorem 6.1.2 that this notion of “polynomial time” does
not depend on the model of computation:
Theorem 6.2.2 Consider the models of computation “Java program”, “k-
tape Turing machine”, and “one-tape Turing machine”. If a language can
be decided (resp. a function can be computed) in polynomial time in one of
these models, then it can be decided (resp. computed) in polynomial time in
all of these models.
Because of this theorem, we can deﬁne the following two complexity
classes:
P := {A : the language A is decidable in polynomial time},
and
FP := {F : the function F is computable in polynomial time}.
6.2.1
Some examples
Palindromes
Let Pal be the language
Pal := {w ∈{a, b}∗: w is a palindrome}.
We have seen that there exists a one-tape Turing machine that decides Pal
in O(n2) time. Therefore, Pal ∈P.
Some functions in FP
The following functions are in the class FP:
• F1 : N0 →N0 deﬁned by F1(x) := x + 1,
• F2 : N2
0 →N0 deﬁned by F2(x, y) := x + y,
• F3 : N2
0 →N0 deﬁned by F3(x, y) := xy.

200
Chapter 6.
Complexity Theory
r
b
b
b
r
r
b
G1
G2
Figure 6.1: The graph G1 is 2-colorable; r stands for red; b stands for blue.
The graph G2 is not 2-colorable.
Context-free languages
We have shown in Section 5.1.3 that every context-free language is decid-
able. The algorithm presented there, however, does not run in polynomial
time. Using a technique called dynamic programming (which you will learn
in COMP 3804), the following result can be shown:
Theorem 6.2.3 Let Σ be an alphabet, and let A ⊆Σ∗be a context-free
language. Then A ∈P.
Observe that, obviously, every language in P is decidable.
The 2-coloring problem
Let G be a graph with vertex set V and edge set E.
We say that G is
2-colorable, if it is possible to give each vertex of V a color such that
1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and
2. only two colors are used to color all vertices.
See Figure 6.1 for two examples. We deﬁne the following language:
2Color := {⟨G⟩: the graph G is 2-colorable},
where ⟨G⟩denotes the binary string that encodes the graph G.

6.2.
The complexity class P
201
We claim that 2Color ∈P. In order to show this, we have to construct an
algorithm that decides in polynomial time, whether or not any given graph
is 2-colorable.
Let G be an arbitrary graph with vertex set V = {1, 2, . . . , m}. The edge
set of G is given by an adjacency matrix. This matrix, which we denote by
E, is a two-dimensional array with m rows and m columns. For all i and j
with 1 ≤i ≤m and 1 ≤j ≤m, we have
E(i, j) =

1
if (i, j) is an edge of G,
0
otherwise.
The length of the input G, i.e., the number of bits needed to specify G, is
equal to m2 =: n. We will present an algorithm that decides, in O(n) time,
whether or not the graph G is 2-colorable.
The algorithm uses the colors red and blue. It gives the ﬁrst vertex the
color red. Then, the algorithm considers all vertices that are connected by
an edge to the ﬁrst vertex, and colors them blue. Now the algorithm is done
with the ﬁrst vertex; it marks this ﬁrst vertex.
Next, the algorithm chooses a vertex i that already has a color, but that
has not been marked. Then it considers all vertices j that are connected by
an edge to i. If j has the same color as i, then the input graph G is not
2-colorable. Otherwise, if vertex j does not have a color yet, the algorithm
gives j the color that is diﬀerent from i’s color. After having done this for
all neighbors j of i, the algorithm is done with vertex i, so it marks i.
It may happen that there is no vertex i that already has a color but that
has not been marked. (In other words, each vertex i that is not marked does
not have a color yet.) In this case, the algorithm chooses an arbitrary vertex
i having this property, and colors it red. (This vertex i is the ﬁrst vertex in
its connected component that gets a color.)
This procedure is repeated until all vertices of G have been marked.
We now give a formal description of this algorithm. Vertex i has been
marked, if
1. i has a color,
2. all vertices that are connected by an edge to i have a color, and
3. the algorithm has veriﬁed that each vertex that is connected by an edge
to i has a color diﬀerent from i’s color.

202
Chapter 6.
Complexity Theory
The algorithm uses two arrays f(1 . . . m) and a(1 . . . m), and a variable
M. The value of f(i) is equal to the color (red or blue) of vertex i; if i does
not have a color yet, then f(i) = 0. The value of a(i) is equal to
a(i) =
 1
if vertex i has been marked,
0
otherwise.
The value of M is equal to the number of marked vertices. The algorithm
is presented in Figure 6.2. You are encouraged to convince yourself of the
correctness of this algorithm. That is, you should convince yourself that this
algorithm returns YES if the graph G is 2-colorable, whereas it returns NO
otherwise.
What is the running time of this algorithm? First we count the number
of iterations of the outer while-loop. In one iteration, either M increases by
one, or a vertex i, for which a(i) = 0, gets the color red. In the latter case,
the variable M is increased during the next iteration of the outer while-loop.
Since, during the entire outer while-loop, the value of M is increased from
zero to m, it follows that there are at most 2m iterations of the outer while-
loop. (In fact, the number of iterations is equal to m plus the number of
connected components of G minus one.)
One iteration of the outer while-loop takes O(m) time. Hence, the total
running time of the algorithm is O(m2), which is O(n). Therefore, we have
shown that 2Color ∈P.
6.3
The complexity class NP
Before we deﬁne the class NP, we consider some examples.
Example 6.3.1 Let G be a graph with vertex set V and edge set E, and
let k ≥1 be an integer. We say that G is k-colorable, if it is possible to give
each vertex of V a color such that
1. for each edge (u, v) ∈E, the vertices u and v have diﬀerent colors, and
2. at most k diﬀerent colors are used to color all vertices.
We deﬁne the following language:
kColor := {⟨G⟩: the graph G is k-colorable}.

6.3.
The complexity class NP
203
Algorithm 2Color
for i := 1 to m do f(i) := 0; a(i) := 0 endfor;
f(1) := red; M := 0;
while M ̸= m
do (∗Find the minimum index i for which vertex i has not
been marked, but has a color already ∗)
bool := false; i := 1;
while bool = false and i ≤m
do if a(i) = 0 and f(i) ̸= 0 then bool := true else i := i + 1 endif;
endwhile;
(∗If bool = true, then i is the smallest index such that
a(i) = 0 and f(i) ̸= 0.
If bool = false, then for all i, the following holds: if a(i) = 0, then
f(i) = 0; because M < m, there is at least one such i. ∗)
if bool = true
then for j := 1 to m
do if E(i, j) = 1
then if f(i) = f(j)
then return NO and terminate
else if f(j) = 0
then if f(i) = red
then f(j) := blue
else f(j) := red
endif
endif
endif
endif
endfor;
a(i) := 1; M := M + 1;
else i := 1;
while a(i) ̸= 0 do i := i + 1 endwhile;
(∗an unvisited connected component starts at vertex i ∗)
f(i) := red
endif
endwhile;
return YES
Figure 6.2:
An algorithm that decides whether or not a graph G is 2-
colorable.
We have seen that for k = 2, this problem is in the class P. For k ≥3, it
is not known whether there exists an algorithm that decides, in polynomial
time, whether or not any given graph is k-colorable.
In other words, for

204
Chapter 6.
Complexity Theory
k ≥3, it is not known whether or not kColor is in the class P.
Example 6.3.2 Let G be a graph with vertex set V = {1, 2, . . ., m} and
edge set E. A Hamilton cycle is a cycle in G that visits each vertex exactly
once. Formally, it is a sequence v1, v2, . . . , vm of vertices such that
1. {v1, v2, . . . , vm} = V , and
2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.
We deﬁne the following language:
HC := {⟨G⟩: the graph G contains a Hamilton cycle}.
It is not known whether or not HC is in the class P.
Example 6.3.3 The sum of subset language is deﬁned as follows:
SOS := {⟨a1, a2, . . . , am, b⟩:
m, a1, a2, . . . , am, b ∈N0 and
∃I ⊆{1, 2, . . . , m}, P
i∈I ai = b}.
Also in this case, no polynomial-time algorithm is known that decides the
language SOS. That is, it is not known whether or not SOS is in the class
P.
Example 6.3.4 An integer x ≥2 is a prime number, if there are no a, b ∈N
such that a ̸= x, b ̸= x, and x = ab. Hence, the language of all non-primes
that are greater than or equal to two, is
NPrim := {⟨x⟩: x ≥2 and x is not a prime number}.
It is not obvious at all, whether or not NPrim is in the class P. In fact, it
was shown only in 2002 that NPrim is in the class P.
Observation 6.3.5 The four languages above have the following in com-
mon: If someone gives us a “solution” for any given input, then we can
easily, i.e., in polynomial time, verify whether or not this “solution” is a cor-
rect solution. Moreover, for any input to each of these four problems, there
exists a “solution” whose length is polynomial in the length of the input.

6.3.
The complexity class NP
205
Let us again consider the language kColor. Let G be a graph with vertex
set V = {1, 2, . . . , m} and edge set E, and let k be a positive integer. We
want to decide whether or not G is k-colorable. A “solution” is a coloring of
the nodes using at most k diﬀerent colors. That is, a solution is a sequence
f1, f2, . . . , fm. (Interpret this as: vertex i receives color fi, 1 ≤i ≤m). This
sequence is a correct solution if and only if
1. fi ∈{1, 2, . . . , k}, for all i with 1 ≤i ≤m, and
2. for all i with 1 ≤i ≤m, and for all j with 1 ≤j ≤m, if (i, j) ∈E,
then fi ̸= fj.
If someone gives us this solution (i.e., the sequence f1, f2, . . . , fm), then
we can verify in polynomial time whether or not these two conditions are
satisﬁed. The length of this solution is O(m log k): for each i, we need about
log k bits to represent fi. Hence, the length of the solution is polynomial in
the length of the input, i.e., it is polynomial in the number of bits needed to
represent the graph G and the number k.
For the Hamilton cycle problem, a solution consists of a sequence v1,
v2, . . . , vm of vertices. This sequence is a correct solution if and only if
1. {v1, v2, . . . , vm} = {1, 2, . . . , m} and
2. {(v1, v2), (v2, v3), . . . , (vm−1, vm), (vm, v1)} ⊆E.
These two conditions can be veriﬁed in polynomial time.
Moreover, the
length of the solution is polynomial in the length of the input graph.
Consider the sum of subset problem. A solution is a sequence c1, c2, . . . , cm.
It is a correct solution if and only if
1. ci ∈{0, 1}, for all i with 1 ≤i ≤m, and
2. Pm
i=1 ciai = b.
Hence, the set I ⊆{1, 2, . . . , m} in the deﬁnition of SOS is the set of indices
i for which ci = 1. Again, these two conditions can be veriﬁed in polynomial
time, and the length of the solution is polynomial in the length of the input.
Finally, let us consider the language NPrim. Let x ≥2 be an integer.
The integers a and b form a “solution” for x if and only if

206
Chapter 6.
Complexity Theory
1. 2 ≤a < x,
2. 2 ≤b < x, and
3. x = ab.
Clearly, these three conditions can be veriﬁed in polynomial time. Moreover,
the length of this solution, i.e., the total number of bits in the binary rep-
resentations of a and b, is polynomial in the number of bits in the binary
representation of x.
Languages having the property that the correctness of a proposed “solu-
tion” can be veriﬁed in polynomial time, form the class NP:
Deﬁnition 6.3.6 A language A belongs to the class NP, if there exist a
polynomial p and a language B ∈P, such that for every string w,
w ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.
In words, a language A is in the class NP, if for every string w, w ∈A if
and only if the following two conditions are satisﬁed:
1. There is a “solution” s, whose length |s| is polynomial in the length of
w (i.e., |s| ≤p(|w|), where p is a polynomial).
2. In polynomial time, we can verify whether or not s is a correct “solu-
tion” for w (i.e., ⟨w, s⟩∈B and B ∈P).
Hence, the language B can be regarded to be the “veriﬁcation language”:
B = {⟨w, s⟩: s is a correct “solution” for w}.
We have given already informal proofs of the fact that the languages
kColor, HC, SOS, and NPrim are all contained in the class NP. Below, we
formally prove that NPrim ∈NP. To prove this claim, we have to specify
the polynomial p and the language B ∈P. First, we observe that
NPrim = {⟨x⟩:
there exist a and b in N such that
2 ≤a < x, 2 ≤b < x and x = ab }.
(6.1)
We deﬁne the polynomial p by p(n) := n + 2, and the language B as
B := {⟨x, a, b⟩: x ≥2, 2 ≤a < x, 2 ≤b < x and x = ab}.

6.3.
The complexity class NP
207
It is obvious that B ∈P: For any three positive integers x, a, and b, we
can verify in polynomial time whether or not ⟨x, a, b⟩∈B. In order to do
this, we only have to verify whether or not x ≥2, 2 ≤a < x, 2 ≤b < x,
and x = ab. If all these four conditions are satisﬁed, then ⟨x, a, b⟩∈B. If at
least one of them is not satisﬁed, then ⟨x, a, b⟩̸∈B.
It remains to show that for all x ∈N:
⟨x⟩∈NPrim ⇐⇒∃a, b : |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B.
(6.2)
(Remember that |⟨x⟩| denotes the number of bits in the binary representation
of x; |⟨a, b⟩| denotes the total number of bits of a and b, i.e., |⟨a, b⟩| =
|⟨a⟩| + |⟨b⟩|.)
Let x ∈NPrim. It follows from (6.1) that there exist a and b in N, such
that 2 ≤a < x, 2 ≤b < x, and x = ab. Since x = ab ≥2 · 2 = 4 ≥2, it
follows that ⟨x, a, b⟩∈B. Hence, it remains to show that
|⟨a, b⟩| ≤|⟨x⟩| + 2.
The binary representation of x contains ⌊log x⌋+1 bits, i.e., |⟨x⟩| = ⌊log x⌋+1.
We have
|⟨a, b⟩|
=
|⟨a⟩| + |⟨b⟩|
=
(⌊log a⌋+ 1) + (⌊log b⌋+ 1)
≤
log a + log b + 2
=
log ab + 2
=
log x + 2
≤
⌊log x⌋+ 3
=
|⟨x⟩| + 2.
This proves one direction of (6.2).
To prove the other direction, we assume that there are positive integers
a and b, such that |⟨a, b⟩| ≤|⟨x⟩| + 2 and ⟨x, a, b⟩∈B. Then it follows
immediately from (6.1) and the deﬁnition of the language B, that x ∈NPrim.
Hence, we have proved the other direction of (6.2). This completes the proof
of the claim that
NPrim ∈NP.

208
Chapter 6.
Complexity Theory
6.3.1
P is contained in NP
Intuitively, it is clear that P ⊆NP, because a language is
• in P, if for every string w, it is possible to compute the “solution” s in
polynomial time,
• in NP, if for every string w and for any given “solution” s, it is possible
to verify in polynomial time whether or not s is a correct solution for
w (hence, we do not need to compute the solution s ourselves, we only
have to verify it).
We give a formal proof of this:
Theorem 6.3.7 P ⊆NP.
Proof. Let A ∈P. We will prove that A ∈NP. Deﬁne the polynomial p
by p(n) := 0 for all n ∈N0, and deﬁne
B := {⟨w, ǫ⟩: w ∈A}.
Since A ∈P, the language B is also contained in P. It is easy to see that
w ∈A ⇐⇒∃s : |s| ≤p(|w|) = 0 and ⟨w, s⟩∈B.
This completes the proof.
6.3.2
Deciding NP-languages in exponential time
Let us look again at the deﬁnition of the class NP. Let A be a language in
this class. Then there exist a polynomial p and a language B ∈P, such that
for all strings w,
w ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.
(6.3)
How do we decide whether or not any given string w belongs to the language
A? If we can ﬁnd a string s that satisﬁes the right-hand side in (6.3), then
we know that w ∈A. On the other hand, if there is no such string s, then
w ̸∈A. How much time do we need to decide whether or not such a string s
exists?

6.3.
The complexity class NP
209
Algorithm NonPrime
(∗decides whether or not ⟨x⟩∈NPrim ∗)
if x = 0 or x = 1 or x = 2
then return NO and terminate
else a := 2;
while a < x
do if x mod a = 0
then return YES and terminate
else a := a + 1
endif
endwhile;
return NO
endif
Figure 6.3: An algorithm that decides whether or not a number x is contained
in the language NPrim.
For example, let A be the language NPrim, and let x ∈N. The algorithm
in Figure 6.3 decides whether or not ⟨x⟩∈NPrim.
It is clear that this algorithm is correct. Let n be the length of the binary
representation of x, i.e., n = ⌊log x⌋+ 1. If x > 2 and x is a prime number,
then the while-loop makes x−2 iterations. Therefore, since n−1 = ⌊log x⌋≤
log x, the running time of this algorithm is at least
x −2 ≥2n−1 −2,
i.e., it is at least exponential in the length of the input.
We now prove that every language in NP can be decided in exponential
time. Let A be an arbitrary language in NP. Let p be the polynomial, and
let B ∈P be the language such that for all strings w,
w ∈A ⇐⇒∃s : |s| ≤p(|w|) and ⟨w, s⟩∈B.
(6.4)
The following algorithm decides, for any given string w, whether or not
w ∈A. It does so by looking at all possible strings s for which |s| ≤p(|w|):
for all s with |s| ≤p(|w|)
do if ⟨w, s⟩∈B

210
Chapter 6.
Complexity Theory
then return YES and terminate
endif
endfor;
return NO
The correctness of the algorithm follows from (6.4). What is the running
time? We assume that w and s are represented as binary strings. Let n be
the length of the input, i.e., n = |w|.
How many binary strings s are there whose length is at most p(|w|)? Any
such s can be described by a sequence of length p(|w|) = p(n), consisting of
the symbols “0”, “1”, and the blank symbol. Hence, there are at most 3p(n)
many binary strings s with |s| ≤p(n). Therefore, the for-loop makes at most
3p(n) iterations.
Since B ∈P, there is an algorithm and a polynomial q, such that this
algorithm, when given any input string z, decides in q(|z|) time, whether or
not z ∈B. This input z has the form ⟨w, s⟩, and we have
|z| = |w| + |s| ≤|w| + p(|w|) = n + p(n).
It follows that the total running time of our algorithm that decides whether
or not w ∈A, is bounded from above by
3p(n) · q(n + p(n))
≤
22p(n) · q(n + p(n))
≤
22p(n) · 2q(n+p(n))
=
2p′(n),
where p′ is the polynomial that is deﬁned by p′(n) := 2p(n) + q(n + p(n)).
If we deﬁne the class EXP as
EXP :=
{A :
there exists a polynomial p, such that A can be
decided in time 2p(n) } ,
then we have proved the following theorem.
Theorem 6.3.8 NP ⊆EXP.
6.3.3
Summary
• P ⊆NP. It is not known whether P is a proper subclass of NP, or
whether P = NP. This is one of the most important open problems in

6.4.
Non-deterministic algorithms
211
computer science. If you can solve this problem, then you will get one
million dollars; not from us, but from the Clay Mathematics Institute,
see
http://www.claymath.org/prizeproblems/index.htm
Most people believe that P is a proper subclass of NP.
• NP ⊆EXP, i.e., each language in NP can be decided in exponential
time. It is not known whether NP is a proper subclass of EXP, or
whether NP = EXP.
• It follows from P ⊆NP and NP ⊆EXP, that P ⊆EXP. It can
be shown that P is a proper subset of EXP, i.e., there exist languages
that can be decided in exponential time, but that cannot be decided in
polynomial time.
• P is the class of those languages that can be decided eﬃciently, i.e., in
polynomial time. Sets that are not in P, are not eﬃciently decidable.
6.4
Non-deterministic algorithms
The abbreviation NP stands for Non-deterministic Polynomial time. The al-
gorithms that we have considered so far are deterministic, which means that
at any time during the computation, the next computation step is uniquely
determined. In a non-deterministic algorithm, there are one or more possi-
bilities for being the next computation step, and the algorithm chooses one
of them.
To give an example, we consider the language SOS, see Example 6.3.3.
Let m, a1, a2, . . . , am, and b be elements of N0. Then
⟨a1, a2, . . . , am, b⟩∈SOS
⇐⇒
there exist c1, c2, . . . , cm ∈{0, 1},
such that Pm
i=1 ciai = b.
The following non-deterministic algorithm decides the language SOS:
Algorithm SOS(m, a1, a2, . . . , am, b):
s := 0;
for i := 1 to m
do s := s | s := s + ai

212
Chapter 6.
Complexity Theory
endfor;
if s = b
then return YES
else return NO
endif
The line
s := s | s := s + ai
means that either the instruction “s := s” or the instruction “s := s + ai” is
executed.
Let us assume that ⟨a1, a2, . . . , am, b⟩∈SOS. Then there are c1, c2, . . . , cm ∈
{0, 1} such that Pm
i=1 ciai = b. Assume our algorithm does the following, for
each i with 1 ≤i ≤m: In the i-th iteration,
• if ci = 0, then it executes the instruction “s := s”,
• if ci = 1, then it executes the instruction “s := s + ai”.
Then after the for-loop, we have s = b, and the algorithm returns YES;
hence, the algorithm has correctly found out that ⟨a1, a2, . . . , am, b⟩∈SOS.
In other words, in this case, there exists at least one accepting computation.
On the other hand, if ⟨a1, a2, . . . , am, b⟩̸∈SOS, then the algorithm always
returns NO, no matter which of the two instructions is executed in each
iteration of the for-loop. In this case, there is no accepting computation.
Deﬁnition 6.4.1 Let M be a non-deterministic algorithm. We say that M
accepts a string w, if there exists at least one computation that, on input w,
returns YES.
Deﬁnition 6.4.2 We say that a non-deterministic algorithm M decides a
language A in time T, if for every string w, the following holds: w ∈A if
and only if there exists at least one computation that, on input w, returns
YES and that takes at most T(|w|) time.
The non-deterministic algorithm that we have seen above decides the
language SOS in linear time: Let ⟨a1, a2, . . . , am, b⟩∈SOS, and let n be the
length of this input. Then
n = |⟨a1⟩| + |⟨a2⟩| + . . . + |⟨am⟩| + |⟨b⟩| ≥m.

6.5.
NP-complete languages
213
For this input, there is a computation that returns YES and that takes
O(m) = O(n) time.
As in Section 6.2, we deﬁne the notion of “polynomial time” for non-
deterministic algorithms. The following theorem relates this notion to the
class NP that we deﬁned in Deﬁnition 6.3.6.
Theorem 6.4.3 A language A is in the class NP if and only if there exists
a non-deterministic Turing machine (or Java program) that decides A in
polynomial time.
6.5
NP-complete languages
Languages in the class P are considered easy, i.e., they can be decided in
polynomial time. People believe (but cannot prove) that P is a proper sub-
class of NP. If this is true, then there are languages in NP that are hard,
i.e., cannot be decided in polynomial time.
Intuition tells us that if P ̸= NP, then the hardest languages in NP are
not contained in P. These languages are called NP-complete. In this section,
we will give a formal deﬁnition of this concept.
If we want to talk about the “hardest” languages in NP, then we have to
be able to compare two languages according to their “diﬃculty”. The idea is
as follows: We say that a language B is “at least as hard” as a language A,
if the following holds: If B can be decided in polynomial time, then A can
also be decided in polynomial time.
Deﬁnition 6.5.1 Let A ⊆{0, 1}∗and B ⊆{0, 1}∗be languages. We say
that A ≤P B, if there exists a function
f : {0, 1}∗→{0, 1}∗
such that
1. f ∈FP and
2. for all strings w in {0, 1}∗,
w ∈A ⇐⇒f(w) ∈B.

214
Chapter 6.
Complexity Theory
If A ≤P B, then we also say that “B is at least as hard as A”, or “A is
polynomial-time reducible to B”.
We ﬁrst show that this formal deﬁnition is in accordance with the intuitive
deﬁnition given above.
Theorem 6.5.2 Let A and B be languages such that B ∈P and A ≤P B.
Then A ∈P.
Proof. Let f : {0, 1}∗→{0, 1}∗be the function in FP for which
w ∈A ⇐⇒f(w) ∈B.
(6.5)
The following algorithm decides whether or not any given binary string w is
in A:
u := f(w);
if u ∈B
then return YES
else return NO
endif
The correctness of this algorithm follows immediately from (6.5). So it
remains to show that the running time is polynomial in the length of the
input string w.
Since f ∈FP, there exists a polynomial p such that the function f can
be computed in time p. Similarly, since B ∈P, there exists a polynomial q,
such that the language B can be decided in time q.
Let n be the length of the input string w, i.e., n = |w|. Then the length
of the string u is less than or equal to p(|w|) = p(n). (Why?) Therefore, the
running time of our algorithm is bounded from above by
p(|w|) + q(|u|) ≤p(n) + q(p(n)).
Since the function p′, deﬁned by p′(n) := p(n)+q(p(n)), is a polynomial, this
proves that A ∈P.
The following theorem states that the relation ≤P is reﬂexive and tran-
sitive. We leave the proof as an exercise.
Theorem 6.5.3 Let A, B, and C be languages. Then

6.5.
NP-complete languages
215
1. A ≤P A, and
2. if A ≤P B and B ≤P C, then A ≤P C.
We next show that the languages in P are the easiest languages in NP:
Theorem 6.5.4 Let A be a language in P, and let B be an arbitrary lan-
guage such that B ̸= ∅and B ̸= {0, 1}∗. Then A ≤P B.
Proof. We choose two strings u and v in {0, 1}∗, such that u ∈B and v ̸∈B.
(Observe that this is possible.) Deﬁne the function f : {0, 1}∗→{0, 1}∗by
f(w) :=

u
if w ∈A,
v
if w ̸∈A.
Then it is clear that for any binary string w,
w ∈A ⇐⇒f(w) ∈B.
Since A ∈P, the function f can be computed in polynomial time, i.e.,
f ∈FP.
6.5.1
Two examples of reductions
Sum of subsets and knapsacks
We start with a simple reduction. Consider the two languages
SOS := {⟨a1, . . . , am, b⟩:
m, a1, . . . , am, b ∈N0 and there exist
c1, . . . , cm ∈{0, 1}, such that Pm
i=1 ciai = b}
and
KS
:=
{⟨w1, . . . , wm, k1, . . . , km, W, K⟩:
m, w1, . . . , wm, k1, . . . , km, W, K ∈N0
and there exist c1, . . . , cm ∈{0, 1},
such that Pm
i=1 ciwi ≤W and Pm
i=1 ciki ≥K}.
The notation KS stands for knapsack: We have m pieces of food. The
i-th piece has weight wi and contains ki calories. We want to decide whether
or not we can ﬁll our knapsack with a subset of the pieces of food such that
the total weight is at most W, and the total amount of calories is at least K.

216
Chapter 6.
Complexity Theory
Theorem 6.5.5 SOS ≤P KS.
Proof. Let us ﬁrst see what we have to show. According to Deﬁnition 6.5.1,
we need a function f ∈FP, that maps input strings for SOS to input strings
for KS, in such a way that
⟨a1, . . . , am, b⟩∈SOS ⇐⇒f(⟨a1, . . . , am, b⟩) ∈KS.
In order for f(⟨a1, . . . , am, b⟩) to be an input string for KS, this function
value has to be of the form
f(⟨a1, . . . , am, b⟩) = ⟨w1, . . . , wm, k1, . . . , km, W, K⟩.
We deﬁne
f(⟨a1, . . . , am, b⟩) := ⟨a1, . . . , am, a1, . . . , am, b, b⟩.
It is clear that f ∈FP. We have
⟨a1, . . . , am, b⟩∈SOS
⇐⇒
there exist c1, . . . , cm ∈{0, 1} such that Pm
i=1 ciai = b
⇐⇒
there exist c1, . . . , cm ∈{0, 1} such that Pm
i=1 ciai ≤b and Pm
i=1 ciai ≥b
⇐⇒
⟨a1, . . . , am, a1, . . . , am, b, b⟩∈KS
⇐⇒
f(⟨a1, . . . , am, b⟩) ∈KS.
Cliques and Boolean formulas
We will deﬁne two languages A = 3SAT and B = Clique that have, at
ﬁrst sight, nothing to do with each other. Then we show that, nevertheless,
A ≤P B.
Let G be a graph with vertex set V and edge set E. A subset V ′ of V is
called a clique, if each pair of distinct vertices in V ′ is connected by an edge
in E. We deﬁne the following language:
Clique := {⟨G, k⟩: k ∈N and G has a clique with k vertices}.
We encourage you to prove the following claim:

6.5.
NP-complete languages
217
Theorem 6.5.6 Clique ∈NP.
Next we consider Boolean formulas ϕ, with variables x1, x2, . . . , xm, hav-
ing the form
ϕ = C1 ∧C2 ∧. . . ∧Ck,
(6.6)
where each Ci, 1 ≤i ≤k, is of the form
Ci = ℓi
1 ∨ℓi
2 ∨ℓi
3.
Each ℓi
a is either a variable or the negation of a variable. An example of such
a formula is
ϕ = (x1 ∨¬x1 ∨¬x2) ∧(x3 ∨x2 ∨x4) ∧(¬x1 ∨¬x3 ∨¬x4).
A formula ϕ of the form (6.6) is said to be satisﬁable, if there exists a truth-
value in {0, 1} for each of the variables x1, x2, . . . , xm, such that the entire
formula ϕ is true. Our example formula is satisﬁable: If we take x1 = 0 and
x2 = 1, and give x3 and x4 an arbitrary value, then
ϕ = (0 ∨1 ∨0) ∧(x3 ∨1 ∨x4) ∧(1 ∨¬x3 ∨¬x4) = 1.
We deﬁne the following language:
3SAT := {⟨ϕ⟩: ϕ is of the form (6.6) and is satisﬁable}.
Again, we encourage you to prove the following claim:
Theorem 6.5.7 3SAT ∈NP.
Observe that the elements of Clique (which are pairs consisting of a graph
and a positive integer) are completely diﬀerent from the elements of 3SAT
(which are Boolean formulas). We will show that 3SAT ≤P Clique. Recall
that this means the following: If the language Clique can be decided in
polynomial time, then the language 3SAT can also be decided in polynomial
time. In other words, any polynomial-time algorithm that decides Clique can
be converted to a polynomial-time algorithm that decides 3SAT.
Theorem 6.5.8 3SAT ≤P Clique.

218
Chapter 6.
Complexity Theory
Proof. We have to show that there exists a function f ∈FP, that maps
input strings for 3SAT to input strings for Clique, such that for each Boolean
formula ϕ that is of the form (6.6),
⟨ϕ⟩∈3SAT ⇐⇒f(⟨ϕ⟩) ∈Clique.
The function f maps the binary string encoding an arbitrary Boolean formula
ϕ to a binary string encoding a pair (G, k), where G is a graph and k is a
positive integer. We have to deﬁne this function f in such a way that
ϕ is satisﬁable ⇐⇒G has a clique with k vertices.
Let
ϕ = C1 ∧C2 ∧. . . ∧Ck
be an arbitrary Boolean formula in the variables x1, x2, . . . , xm, where each
Ci, 1 ≤i ≤k, is of the form
Ci = ℓi
1 ∨ℓi
2 ∨ℓi
3.
Remember that each ℓi
a is either a variable or the negation of a variable.
The formula ϕ is mapped to the pair (G, k), where the vertex set V and
the edge set E of the graph G are deﬁned as follows:
• V = {v1
1, v1
2, v1
3, . . . , vk
1, vk
2, vk
3}. The idea is that each vertex vi
a corre-
sponds to one term ℓi
a.
• The pair (vi
a, vj
b) of vertices form an edge in E if and only if
– i ̸= j and
– ℓi
a is not the negation of ℓj
b.
To give an example, let ϕ be the Boolean formula
ϕ = (x1 ∨¬x2 ∨¬x3) ∧(¬x1 ∨x2 ∨x3) ∧(x1 ∨x2 ∨x3),
(6.7)
i.e., k = 3, C1 = x1 ∨¬x2 ∨¬x3, C2 = ¬x1 ∨x2 ∨x3, and C3 = x1 ∨x2 ∨x3.
The graph G that corresponds to this formula is given in Figure 6.4.
It is not diﬃcult to see that the function f can be computed in polynomial
time. So it remains to prove that
ϕ is satisﬁable ⇐⇒G has a clique with k vertices.
(6.8)

6.5.
NP-complete languages
219
¬x2
¬x3
x1
¬x1
x2
x3
x1
x2
x3
Figure 6.4: The formula ϕ in (6.7) is mapped to this graph. The vertices on
the top represent C1; the vertices on the left represent C2; the vertices on
the right represent C3.
To prove this, we ﬁrst assume that the formula
ϕ = C1 ∧C2 ∧. . . ∧Ck
is satisﬁable. Then there exists a truth-value in {0, 1} for each of the variables
x1, x2, . . . , xm, such that the entire formula ϕ is true. Hence, for each i with
1 ≤i ≤k, there is at least one term ℓi
a in
Ci = ℓi
1 ∨ℓi
2 ∨ℓi
3
that is true (i.e., has value 1).
Let V ′ be the set of vertices obtained by choosing for each i, 1 ≤i ≤k,
exactly one vertex vi
a such that ℓi
a has value 1.
It is clear that V ′ contains exactly k vertices. We claim that this set is
a clique in G. To prove this claim, let vi
a and vj
b be two distinct vertices in
V ′. It follows from the deﬁnition of V ′ that i ̸= j and ℓi
a = ℓj
b = 1. Hence,
ℓi
a is not the negation of ℓj
b. But this means that the vertices vi
a and vj
b are
connected by an edge in G.
This proves one direction of (6.8). To prove the other direction, we assume
that the graph G contains a clique V ′ with k vertices.

220
Chapter 6.
Complexity Theory
The vertices of G consist of k groups, where each group contains exactly
three vertices. Since vertices within the same group are not connected by
edges, the clique V ′ contains exactly one vertex from each group. Hence, for
each i with 1 ≤i ≤k, there is exactly one a, such that vi
a ∈V ′. Consider
the corresponding term ℓi
a. We know that this term is either a variable or
the negation of a variable, i.e., ℓi
a is either of the form xj or of the form ¬xj.
If ℓi
a = xj, then we give xj the truth-value 1. Otherwise, we have ℓi
a = ¬xj,
in which case we give xj the truth-value 0. Since V ′ is a clique, each variable
gets at most one truth-value. If a variable has no truth-value yet, then we
give it an arbitrary truth-value.
If we substitute these truth-values into ϕ, then the entire formula has
value 1. Hence, ϕ is satisﬁable.
In order to get a better understanding of this proof, you should verify the
proof for the formula ϕ in (6.7) and the graph G in Figure 6.4.
6.5.2
Deﬁnition of NP-completeness
Reductions, as deﬁned in Deﬁnition 6.5.1, allow us to compare two language
according to their diﬃculty. A language B in NP is called NP-complete,
if B belongs to the most diﬃcult languages in NP; in other words, B is at
least as hard as any other language in NP.
Deﬁnition 6.5.9 Let B ⊆{0, 1}∗be a language. We say that B is NP-
complete, if
1. B ∈NP and
2. A ≤P B, for every language A in NP.
Theorem 6.5.10 Let B be an NP-complete language. Then
B ∈P ⇐⇒P = NP.
Proof. Intuitively, this theorem should be true: If the language B is in P,
then B is an easy language. On the other hand, since B is NP-complete,
it belongs to the most diﬃcult languages in NP. Hence, the most diﬃcult
language in NP is easy. But then all languages in NP must be easy, i.e.,
P = NP.

6.5.
NP-complete languages
221
We give a formal proof. Let us ﬁrst assume that B ∈P. We already
know that P ⊆NP. Hence, it remains to show that NP ⊆P. Let A be an
arbitrary language in NP. Since B is NP-complete, we have A ≤P B. Then,
by Theorem 6.5.2, we have A ∈P.
To prove the converse, assume that P = NP. Since B ∈NP, it follows
immediately that B ∈P.
Theorem 6.5.11 Let B and C be languages, such that C ∈NP and B ≤P
C. If B is NP-complete, then C is also NP-complete.
Proof. First, we give an intuitive explanation of the claim: By assumption,
B belongs to the most diﬃcult languages in NP, and C is at least as hard as
B. Since C ∈NP, it follows that C belongs to the most diﬃcult languages
in NP. Hence, C is NP-complete.
To give a formal proof, we have to show that A ≤P C, for all languages A
in NP. Let A be an arbitrary language in NP. Since B is NP-complete, we
have A ≤P B. Since B ≤P C, it follows from Theorem 6.5.3, that A ≤P C.
Therefore, C is NP-complete.
Theorem 6.5.11 can be used to prove the NP-completeness of languages:
Let C be a language, and assume that we want to prove that C is NP-
complete. We can do this in the following way:
1. We ﬁrst prove that C ∈NP.
2. Then we ﬁnd a language B that looks “similar” to C, and for which
we already know that it is NP-complete.
3. Finally, we prove that B ≤P C.
4. Then, Theorem 6.5.11 tells us that C is NP-complete.
Of course, this leads to the question “How do we know that the language
B is NP-complete?” In order to apply Theorem 6.5.11, we need a “ﬁrst” NP-
complete language; the NP-completeness of this language must be proven
using Deﬁnition 6.5.9.
Observe that it is not clear at all that there exist NP-complete languages!
For example, consider the language 3SAT. If we want to use Deﬁnition 6.5.9
to show that this language is NP-complete, then we have to show that

222
Chapter 6.
Complexity Theory
• 3SAT ∈NP. We know from Theorem 6.5.7 that this is true.
• A ≤P 3SAT, for every language A ∈NP. Hence, we have to show this
for languages A such as kColor, HC , SOS, NPrim, KS, Clique, and
for inﬁnitely many other languages.
In 1971, Cook has exactly done this: He showed that the language 3SAT
is NP-complete. Since his proof is rather technical, we will prove the NP-
completeness of another language.
6.5.3
An NP-complete domino game
We are given a ﬁnite collection of tile types. For each such type, there are
arbitrarily many tiles of this type. A tile is a square that is partitioned into
four triangles. Each of these triangles contains a symbol that belongs to a
ﬁnite alphabet Σ. Hence, a tile looks as follows:
     @
@
@
@@
a
b
c
d
We are also given a square frame, consisting of cells. Each cell has the same
size as a tile, and contains a symbol of Σ.
The problem is to decide whether or not this domino game has a solution.
That is, can we completely ﬁll the frame with tiles such that
• for any two neighboring tiles s and s′, the two triangles of s and s′ that
touch each other contain the same symbol, and
• each triangle that touches the frame contains the same symbol as the
cell of the frame that is touched by this triangle.
There is one ﬁnal restriction: The orientation of the tiles is ﬁxed, they cannot
be rotated.
Let us give a formal deﬁnition of this problem. We assume that the sym-
bols belong to the ﬁnite alphabet Σ = {0, 1}m, i.e., each symbol is encoded
as a bit-string of length m. Then, a tile type can be encoded as a tuple of
four bit-strings, i.e., as an element of Σ4. A frame consisting of t rows and t
columns can be encoded as a string in Σ4t.

6.5.
NP-complete languages
223
We denote the language of all solvable domino games by Domino:
Domino
:=
{⟨m, k, t, R, T1, . . . , Tk⟩:
m ≥1, k ≥1, t ≥1, R ∈Σ4t, Ti ∈Σ4, 1 ≤i ≤k,
frame R can be ﬁlled using tiles of types
T1, . . . , Tk.}
We will prove the following theorem.
Theorem 6.5.12 The language Domino is NP-complete.
Proof. It is clear that Domino ∈NP: A solution consists of a t × t matrix,
in which the (i, j)-entry indicates the type of the tile that occupies position
(i, j) in the frame. The number of bits needed to specify such a solution is
polynomial in the length of the input. Moreover, we can verify in polynomial
time whether or not any given “solution” is correct.
It remains to show that
A ≤P Domino, for every language A in NP.
Let A be an arbitrary language in NP. Then there exist a polynomial p and
a non-deterministic Turing machine M, that decides the language A in time
p. We may assume that this Turing machine has only one tape.
On input w = a1a2 . . . an, the Turing machine M starts in the start state
z0, with its tape head on the cell containing the symbol a1. We may assume
that during the entire computation, the tape head never moves to the left of
this initial cell. Hence, the entire computation “takes place” in and to the
right of the initial cell. We know that
w ∈A
⇐⇒
on input w, there exists an accepting computation
that makes at most p(n) computation steps.
At the end of such an accepting computation, the tape only contains the
symbol 1, which we may assume to be in the initial cell, and M is in the ﬁnal
state z1. In this case, we may assume that the accepting computation makes
exactly p(n) computation steps. (If this is not the case, then we extend the
computation using the instruction z11 →z11N.)
We need one more technical detail: We may assume that za →z′bR and
za′ →z′′b′L are not both instructions of M. Hence, the state of the Turing
machine uniquely determines the direction in which the tape head moves.

224
Chapter 6.
Complexity Theory
We have to deﬁne a domino game, that depends on the input string w
and the Turing machine M, such that
w ∈A ⇐⇒this domino game is solvable.
The idea is to encode an accepting computation of the Turing machine M as
a solution of the domino game. In order to do this, we use a frame in which
each row corresponds to one computation step. This frame consists of p(n)
rows. Since an accepting computation makes exactly p(n) computation steps,
and since the tape head never moves to the left of the initial cell, this tape
head can visit only p(n) cells. Therefore, our frame will have p(n) columns.
The domino game will use the following tile types:
1. For each symbol a in the alphabet of the Turing machine M:
      @
@
@
@
@@
#
a
#
a
Intuition: Before and after the computation step, the tape head is not
on this cell.
2. For each instruction za →z′bR of the Turing machine M:
      @
@
@
@
@@
#
(z, a)
z′
b
Intuition: Before the computation step, the tape head is on this cell;
the tape head makes one step to the right.
3. For each instruction za →z′bL of the Turing machine M:
      @
@
@
@
@@
z′
(z, a)
#
b

6.5.
NP-complete languages
225
Intuition: Before the computation step, the tape head is on this cell;
the tape head makes one step to the left.
4. For each instruction za →z′bN of the Turing machine M:
      @
@
@
@
@@
#
(z, a)
#
(z′, b)
Intuition: Before and after the computation step, the tape head is on
this cell.
5. For each state z and for each symbol a in the alphabet of the Turing
machine M, there are two tile types:
      @
@
@
@
@@
z
a
#
(z, a)
      @
@
@
@
@@
#
a
z
(z, a)
Intuition: The leftmost tile indicates that the tape head enters this cell
from the left; the righmost tile indicates that the tape head enters this
cell from the right.
This speciﬁes all tile types. The p(n) × p(n) frame is given in Figure 6.5.
The top row corresponds to the start of the computation, whereas the bottom
row corresponds to the end of the computation. The left and right columns
correspond to the part of the tape in which the tape head can move.
The encodings of these tile types and the frame can be computed in
polynomial time.
It can be shown that, for any input string w, any accepting computation
of length p(n) of the Turing machine M can be encoded as a solution of
this domino game.
Conversely, any solution of this domino game can be
“translated” to an accepting computation of length p(n) of M, on input
string w. Hence, the following holds.
w ∈A
⇐⇒
there exists an accepting computation that makes
p(n) computation steps
⇐⇒
the domino game is solvable.

226
Chapter 6.
Complexity Theory
(z0, a1)
a2
. . .
an
2
. . .
2
#
#
#
#
#
...
#
...
2
2
2
2
2
. . .
(z1, 1)
p(n)
p(n)
Figure 6.5: The p(n) × p(n) frame for the domino game.
Therefore, we have A ≤P Domino. Hence, the language Domino is NP-
complete.
An example of a domino game
We have deﬁned the domino game corresponding to a Turing machine that
solves a decision problem. Of course, we can also do this for Turing machines
that compute functions. In this section, we will exactly do this for a Turing
machine that computes the successor function x →x + 1.
We will design a Turing machine with one tape, that gets as input the
binary representation of a natural number x, and that computes the binary
representation of x + 1.
Start of the computation: The tape contains a 0 followed by the binary
representation of the integer x ∈N0. The tape head is on the leftmost bit
(which is 0), and the Turing machine is in the start state z0. Here is an
example, where x = 431:

6.5.
NP-complete languages
227
0 1 1 0 1 0 1 1 1 1 2
6
End of the computation: The tape contains the binary representation of
the number x + 1. The tape head is on the rightmost 1, and the Turing
machine is in the ﬁnal state z1. For our example, the tape looks as follows:
0 1 1 0 1 1 0 0 0 0 2
6
Our Turing machine will use the following states:
z0 :
start state; tape head moves to the right
z1 :
ﬁnal state
z2 :
tape head moves to the left; on its way to the left, it has not read 0
The Turing machine has the following instructions:
z00 →z00R
z21 →z20L
z01 →z01R
z20 →z11N
z02 →z22L
In Figure 6.6, you can see the sequence of states and tape contents of this
Turing machine on input x = 11.
We now construct the domino game that corresponds to the computation
of this Turing machine on input x = 11. Following the general construction
in Section 6.5.3, we obtain the following tile types:
1. The three symbols of the alphabet yield three tile types:
     @
@
@
@@
#
#
0
0
     @
@
@
@@
#
#
1
1
     @
@
@
@@
#
#
2
2
2. The ﬁve instructions of the Turing machine yield ﬁve tile types:

228
Chapter 6.
Complexity Theory
(z0, 0)
1
0
1
1
2
0
(z0, 1)
0
1
1
2
0
1
(z0, 0)
1
1
2
0
1
0
(z0, 1)
1
2
0
1
0
1
(z0, 1)
2
0
1
0
1
1
(z0, 2)
0
1
0
1
(z2, 1)
2
0
1
0
(z2, 1)
0
2
0
1
(z2, 0)
0
0
2
0
1
(z1, 1)
0
0
2
Figure 6.6: The computation of the Turing machine on input x = 11. The
pair (state,symbol) indicates the position of the tape head.
     @
@
@
@@
#
z0
0
(z0, 0)
     @
@
@
@@
#
z0
1
(z0, 1)
     @
@
@
@@
z2
#
2
(z0, 2)
     @
@
@
@@
z2
#
0
(z2, 1)
     @
@
@
@@
#
#
(z1, 1)
(z2, 0)
3. The states z0 and z2, and the three symbols of the alphabet yield twelve
tile types:
     @
@
@
@@
#
z0
(z0, 0)
0
     @
@
@
@@
z0
#
(z0, 0)
0
     @
@
@
@@
#
z0
(z0, 1)
1
     @
@
@
@@
z0
#
(z0, 1)
1
     @
@
@
@@
#
z0
(z0, 2)
2
     @
@
@
@@
z0
#
(z0, 2)
2
     @
@
@
@@
#
z2
(z2, 0)
0
     @
@
@
@@
z2
#
(z2, 0)
0
     @
@
@
@@
#
z2
(z2, 1)
1
     @
@
@
@@
z2
#
(z2, 1)
1
     @
@
@
@@
#
z2
(z2, 2)
2
     @
@
@
@@
z2
#
(z2, 2)
2
The computation of the Turing machine on input x = 11 consists of nine
computation steps. During this computation, the tape head visits exactly
six cells. Therefore, the frame for the domino game has nine rows and six
columns.
This frame is given in Figure 6.7.
In Figure 6.8, you ﬁnd the
solution of the domino game.
Observe that this solution is nothing but
an equivalent way of writing the computation of Figure 6.6.
Hence, the
computation of the Turing machine corresponds to a solution of the domino
game; in fact, the converse also holds.

6.5.
NP-complete languages
229
0
1
(z1, 1)
0
0
2
(z0, 0)
1
0
1
1
2
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
Figure 6.7: The frame for the domino game for input x = 11.

230
Chapter 6.
Complexity Theory
0
1
(z1, 1)
0
0
2
(z0, 0)
1
0
1
1
2
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
(z1, 1)
(z2, 0)
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
2
2
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
z2
(z2, 0)
0
     @
@
@
@
@
z2
#
0
(z2, 1)
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
2
2
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
z2
(z2, 1)
1
     @
@
@
@
@
z2
#
0
(z2, 1)
     @
@
@
@
@
#
#
2
2
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
z2
(z2, 1)
1
     @
@
@
@
@
z2
#
2
(z0, 2)
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
z0
1
(z0, 1)
     @
@
@
@
@
z0
#
(z0, 2)
2
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
z0
1
(z0, 1)
     @
@
@
@
@
z0
#
(z0, 1)
1
     @
@
@
@
@
#
#
2
2
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
z0
0
(z0, 0)
     @
@
@
@
@
z0
#
(z0, 1)
1
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
2
2
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
z0
1
(z0, 1)
     @
@
@
@
@
z0
#
(z0, 0)
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
2
2
     @
@
@
@
@
#
z0
0
(z0, 0)
     @
@
@
@
@
z0
#
(z0, 1)
1
     @
@
@
@
@
#
#
0
0
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
1
1
     @
@
@
@
@
#
#
2
2
Figure 6.8: The solution for the domino game for input x = 11.

6.5.
NP-complete languages
231
6.5.4
Examples of NP-complete languages
In Section 6.5.3, we have shown that Domino is NP-complete. Using this
result, we will apply Theorem 6.5.11 to prove the NP-completeness of some
other languages.
Satisﬁability
We consider Boolean formulas ϕ, in the variables x1, x2, . . . , xm, having the
form
ϕ = C1 ∧C2 ∧. . . ∧Ck,
(6.9)
where each Ci, 1 ≤i ≤k, is of the following form:
Ci = ℓi
1 ∨ℓi
2 ∨. . . ∨ℓi
ki.
Each ℓi
j is either a variable or the negation of a variable. Such a formula ϕ
is said to be satisﬁable, if there exists a truth-value in {0, 1} for each of the
variables x1, x2, . . . , xm, such that the entire formula ϕ is true. We deﬁne the
following language:
SAT := {⟨ϕ⟩: ϕ is of the form (6.9) and is satisﬁable}.
We will prove that SAT is NP-complete.
It is clear that SAT ∈NP. If we can show that
Domino ≤P SAT,
then it follows from Theorem 6.5.11 that SAT is NP-complete. (In Theo-
rem 6.5.11, take B := Domino and C := SAT.)
Hence, we need a function f ∈FP, that maps input strings for Domino
to input strings for SAT, in such a way that for every domino game D, the
following holds:
domino game D is solvable ⇐⇒the formula encoded by the
string f(⟨D⟩) is satisﬁable.
(6.10)
Let us consider an arbitrary domino game D. Let k be the number of
tile types, and let the frame have t rows and t columns. We denote the tile
types by T1, T2, . . . , Tk.

232
Chapter 6.
Complexity Theory
We map this domino game D to a Boolean formula ϕ, such that (6.10)
holds. The formula ϕ will have variables
xijℓ, 1 ≤i ≤t, 1 ≤j ≤t, 1 ≤ℓ≤k.
These variables can be interpretated as follows:
xijℓ= 1 ⇐⇒there is a tile of type Tℓat position (i, j) of the frame.
We deﬁne:
• For all i and j with 1 ≤i ≤t and 1 ≤j ≤t:
C1
ij := xij1 ∨xij2 ∨. . . ∨xijk.
This formula expresses the condition that there is at least one tile at
position (i, j).
• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j ≤t, and 1 ≤ℓ< ℓ′ ≤k:
C2
ijℓℓ′ := ¬xijℓ∨¬xijℓ′.
This formula expresses the condition that there is at most one tile at
position (i, j).
• For all i, j, ℓand ℓ′ with 1 ≤i ≤t, 1 ≤j < t, 1 ≤ℓ≤k and 1 ≤ℓ′ ≤k,
such that i < t and the right symbol on a tile of type Tℓis not equal
to the left symbol on a tile of type Tℓ′:
C3
ijℓℓ′ := ¬xijℓ∨¬xi,j+1,ℓ′.
This formula expresses the condition that neighboring tiles in the same
row “ﬁt” together. There are symmetric formulas for neighboring tiles
in the same column.
• For all j and ℓwith 1 ≤j ≤t and 1 ≤ℓ≤k, such that the top symbol
on a tile of type Tℓis not equal to the symbol at position j of the upper
boundary of the frame:
C4
jℓ:= ¬x1jℓ.
This formula expresses the condition that tiles that touch the upper
boundary of the frame “ﬁt” there. There are symmetric formulas for
the lower, left, and right boundaries of the frame.

6.5.
NP-complete languages
233
The formula ϕ is the conjunction of all these formulas C1
ij, C2
ijℓℓ′, C3
ijℓℓ′, and
C4
jℓ. The complete formula ϕ consists of
O(t2k + t2k2 + t2k2 + tk) = O(t2k2)
terms, i.e., its length is polynomial in the length of the domino game. This
implies that ϕ can be constructed in polynomial time. Hence, the function
f that maps the domino game D to the Boolean formula ϕ, is in the class
FP. It is not diﬃcult to see that (6.10) holds for this function f. Therefore,
we have proved the following result.
Theorem 6.5.13 The language SAT is NP-complete.
In Section 6.5.1, we have deﬁned the language 3SAT.
Theorem 6.5.14 The language 3SAT is NP-complete.
Proof. It is clear that 3SAT ∈NP. If we can show that
SAT ≤P 3SAT,
then the claim follows from Theorem 6.5.11. Let
ϕ = C1 ∧C2 ∧. . . ∧Ck
be an input for SAT, in the variables x1, x2, . . . , xm. We map ϕ, in polynomial
time, to an input ϕ′ for 3SAT, such that
ϕ is satisﬁable ⇐⇒ϕ′ is satisﬁable.
(6.11)
For each i with 1 ≤i ≤k, we do the following. Consider
Ci = ℓi
1 ∨ℓi
2 ∨. . . ∨ℓi
ki.
• If ki = 1, then we deﬁne
C′
i := ℓi
1 ∨ℓi
1 ∨ℓi
1.
• If ki = 2, then we deﬁne
C′
i := ℓi
1 ∨ℓi
2 ∨ℓi
2.

234
Chapter 6.
Complexity Theory
• If ki = 3, then we deﬁne
C′
i := Ci.
• If ki ≥4, then we deﬁne
C′
i
:=
(ℓi
1 ∨ℓi
2 ∨zi
1) ∧(¬zi
1 ∨ℓi
3 ∨zi
2) ∧(¬zi
2 ∨ℓi
4 ∨zi
3) ∧. . .
∧(¬zi
ki−3 ∨ℓi
ki−1 ∨ℓi
ki),
where zi
1, . . . , zi
ki−3 are new variables.
Let
ϕ′ := C′
1 ∧C′
2 ∧. . . ∧C′
k.
Then ϕ′ is an input for 3SAT, and (6.11) holds.
Theorems 6.5.6, 6.5.8, 6.5.11, and 6.5.14 imply:
Theorem 6.5.15 The language Clique is NP-complete.
The traveling salesperson problem
We are given two positive integers k and m, a set of m cities, and an integer
m × m matrix M, where
M(i, j) = the cost of driving from city i to city j,
for all i, j ∈{1, 2, . . . , m}. We want to decide whether or not there is a tour
through all cities whose total cost is less than or equal to k. This problem is
NP-complete.
Bin packing
We are given three positive integers m, k, and ℓ, a set of m objects having
volumes a1, a2, . . . , am, and k bins.
Each bin has volume ℓ. We want to
decide whether or not all objects ﬁt within these bins. This problem is NP-
complete.
Here is another interpretation of this problem: We are given m jobs that
need time a1, a2, . . . , am to complete. We are also given k processors, and an
integer ℓ. We want to decide whether or not it is possible to divide the jobs
over the k processors, such that no processor needs more than ℓtime.

Exercises
235
Time tables
We are given a set of courses, class rooms, and professors.
We want to
decide whether or not there exists a time table such that all courses are
being taught, no two courses are taught at the same time in the same class
room, no professor teaches two courses at the same time, and conditions such
as “Prof. L. Azy does not teach before 1pm” are satisﬁed. This problem is
NP-complete.
Motion planning
We are given two positive integers k and ℓ, a set of k polyhedra, and two
points s and t in Q3. We want to decide whether or not there exists a path
between s and t, that does not intersect any of the polyhedra, and whose
length is less than or equal to ℓ. This problem is NP-complete.
Map labeling
We are given a map with m cities, where each city is represented by a point.
For each city, we are given a rectangle that is large enough to contain the
name of the city. We want to decide whether or not these rectangles can be
placed on the map, such that
• no two rectangles overlap,
• For each i with 1 ≤i ≤m, the point that represents city i is a corner
of its rectangle.
This problem is NP-complete.
This list of NP-complete problems can be extended almost arbitrarily:
For thousands of problems, it is known that they are NP-complete. For all
of these, it is not known, whether or not they can be solved eﬃciently (i.e.,
in polynomial time). Collections of NP-complete problems can be found in
the book
• M.R. Garey and D.S. Johnson. Computers and Intractability: A Guide
to the Theory of NP-Completeness. W.H. Freeman, New York, 1979,
and on the web page
http://www.nada.kth.se/~viggo/wwwcompendium/

236
Chapter 6.
Complexity Theory
Exercises
6.1 Prove that the function F : N →N, deﬁned by F(x) := 2x, is not in FP.
6.2 Prove Theorem 6.5.3.
6.3 Prove that the language Clique is in the class NP.
6.4 Prove that the language 3SAT is in the class NP.
6.5 We deﬁne the following languages:
• Sum of subset:
SOS := {⟨a1, a2, . . . , am, b⟩: ∃I ⊆{1, 2, . . ., m},
X
i∈I
ai = b}.
• Set partition:
SP := {⟨a1, a2, . . . , am⟩: ∃I ⊆{1, 2, . . . , m},
X
i∈I
ai =
X
i̸∈I
ai}.
• Bin packing: BP is the set of all strings ⟨s1, s2, . . . , sm, B⟩for which
1. 0 < si < 1, for all i,
2. B ∈N,
3. the numbers s1, s2, . . . , sm ﬁt into B bins, where each bin has size
one, i.e., there exists a partition of {1, 2, . . . , m} into subsets Ik,
1 ≤k ≤B, such that P
i∈Ik si ≤1 for all k, 1 ≤k ≤B.
For example, ⟨1/6, 1/2, 1/5, 1/9, 3/5, 1/5, 1/2, 11/18, 3⟩∈BP, because
the eight fractions ﬁt into three bins:
1/6 + 1/9 + 11/18 ≤1, 1/2 + 1/2 = 1, and 1/5 + 3/5 + 1/5 = 1.
1. Prove that SOS ≤P SP.
2. Prove that the language SOS is NP-complete. You may use the fact
that the language SP is NP-complete.

Exercises
237
3. Prove that the language BP is NP-complete. Again, you may use the
fact that the language SP is NP-complete.
6.6 Prove that 3Color ≤P 3SAT.
Hint: For each vertex i, and for each of the three colors k, introduce a
Boolean variable xik.
6.7 The (0, 1)-integer programming language IP is deﬁned as follows:
IP := {⟨A, c⟩:
A is an integer m × n matrix for some m, n ∈N,
c is an integer vector of length m, and
∃x ∈{0, 1}n such that Ax ≤c (componentwise) }.
Prove that the language IP is NP-complete. You may use the fact that
the language SOS is NP-complete.
6.8 Let ϕ be a Boolean formula in the variables x1, x2, . . . , xm.
We say that ϕ is in disjunctive normal form (DNF) if it is of the form
ϕ = C1 ∨C2 ∨. . . ∨Ck,
(6.12)
where each Ci, 1 ≤i ≤k, is of the following form:
Ci = ℓi
1 ∧ℓi
2 ∧. . . ∧ℓi
ki.
Each ℓi
j is a literal, which is either a variable or the negation of a variable.
We say that ϕ is in conjunctive normal form (CNF) if it is of the form
ϕ = C1 ∧C2 ∧. . . ∧Ck,
(6.13)
where each Ci, 1 ≤i ≤k, is of the following form:
Ci = ℓi
1 ∨ℓi
2 ∨. . . ∨ℓi
ki.
Again, each ℓi
j is a literal.
We deﬁne the following two languages:
DNFSAT := {⟨ϕ⟩: ϕ is in DNF-form and is satisﬁable},
and
CNFSAT := {⟨ϕ⟩: ϕ is in CNF-form and is satisﬁable}.

238
Chapter 6.
Complexity Theory
1. Prove that the language DNFSAT is in P.
2. What is wrong with the following argument: Since we can rewrite
any Boolean formula in DNF-form, we have CNFSAT ≤P DNFSAT.
Hence, since CNFSAT is NP-complete and since DNFSAT ∈P, we
have P = NP.
3. Prove directly that for every language A in P, A ≤P CNFSAT. “Di-
rectly” means that you should not use the fact that CNFSAT is NP-
complete.
6.9 Prove that the polynomial upper bound on the length of the string y in
the deﬁnition of NP is necessary, in the sense that if it is left out, then any
decidable language would satisfy the condition.
More precisely, we say that the language A belongs to the class D, if there
exists a language B ∈P, such that for every string w,
w ∈A ⇐⇒∃y : ⟨w, y⟩∈B.
Prove that D is equal to the class of all decidable languages.

Chapter 7
Summary
We have seen several diﬀerent models for “processing” languages, i.e., pro-
cessing sets of strings over some ﬁnite alphabet. For each of these models,
we have asked the question which types of languages can be processed, and
which types of languages cannot be processed. In this ﬁnal chapter, we give
a brief summary of these results.
Regular languages:
This class of languages was considered in Chapter 2.
The following statements are equivalent:
1. The language A is regular, i.e., there exists a deterministic ﬁnite au-
tomaton that accepts A.
2. There exists a nondeterministic ﬁnite automaton that accepts A.
3. There exists a regular expression that describes A.
This claim was proved by the following conversions:
1. Every nondeterministic ﬁnite automaton can be converted to an equiv-
alent deterministic ﬁnite automaton.
2. Every deterministic ﬁnite automaton can be converted to an equivalent
regular expression.
3. Every regular expression can be converted to an equivalent nondeter-
ministic ﬁnite automaton.

240
Chapter 7.
Summary
We have seen that the class of regular languages is closed under the regular
operations: If A and B are regular languages, then
1. A ∪B is regular,
2. AB is regular,
3. A∗is regular,
4. A is regular, and
5. A ∩B is regular.
Finally, the pumping lemma for regular languages gives a property that
every regular language possesses. We have used this to prove that languages
such as {anbn : n ≥0} are not regular.
Context-free languages:
This class of languages was considered in Chap-
ter 3. We have seen that every regular language is context-free. Moreover,
there exist languages, for example {anbn : n ≥0}, that are context-free, but
not regular. The following statements are equivalent:
1. The language A is context-free, i.e., there exists a context-free grammar
whose language is A.
2. There exists a context-free grammar in Chomsky normal form whose
language is A.
3. There exists a nondeterministic pushdown automaton that accepts A.
This claim was proved by the following conversions:
1. Every context-free grammar can be converted to an equivalent context-
free grammar in Chomsky normal form.
2. Every context-free grammar in Chomsky normal form can be converted
to an equivalent nondeterministic pushdown automaton.
3. Every nondeterministic pushdown automaton can be converted to an
equivalent context-free grammar. (This conversion was not covered in
this book.)

Chapter 7.
Summary
241
Nondeterministic pushdown automata are more powerful than determin-
istic pushdown automata: There exists a nondeterministic pushdown au-
tomaton that accepts the language
{vbw : v ∈{a, b}∗, w ∈{a, b}∗, |v| = |w|},
but there is no deterministic pushdown automaton that accepts this language.
(We did not prove this in this book.)
We have seen that the class of context-free languages is closed under
the union, concatenation, and star operations: If A and B are context-free
languages, then
1. A ∪B is context-free,
2. AB is context-free, and
3. A∗is context-free.
However,
1. the intersection of two context-free languages is not necessarily context-
free, and
2. the complement of a context-free language is not necessarily context-
free.
Finally, the pumping lemma for context-free languages gives a property
that every context-free language possesses. We have used this to prove that
languages such as {anbncn : n ≥0} are not context-free.
The Church-Turing Thesis:
In Chapter 4, we considered “reasonable”
computational devices that model real computers. Examples of such devices
are Turing machines (with one or more tapes) and Java programs. It turns
out that all known “reasonable” devices are equivalent, i.e., can be converted
to each other. This led to the Church-Turing Thesis:
• Every computational process that is intuitively considered to be an
algorithm can be converted to a Turing machine.

242
Chapter 7.
Summary
Decidable and enumerable languages:
These classes of languages were
considered in Chapter 5. They are deﬁned based on “reasonable” computa-
tional devices, such as Turing machines and Java programs. We have seen
that
1. every context-free language is decidable, and
2. every decidable language is enumerable.
Moreover,
1. there exist languages, for example {anbncn : n ≥0}, that are decidable,
but not context-free,
2. there exist languages, for example the Halting Problem, that are enu-
merable, but not decidable,
3. there exist languages, for example the complement of the Halting Prob-
lem, that are not enumerable.
In fact,
1. the class of all languages is not countable, whereas
2. the class of all enumerable languages is countable.
The following statements are equivalent:
1. The language A is decidable.
2. Both A and its complement A are enumerable.
Complexity classes:
These classes of languages were considered in Chap-
ter 6.
1. The class P consists of all languages that can be decided in polynomial
time by a deterministic Turing machine.
2. The class NP consists of all languages that can be decided in poly-
nomial time by a nondeterministic Turing machine.
Equivalently, a
language A is in the class NP, if for every string w ∈A, there exists a
“solution” s, such that (i) the length of s is polynomial in the length
of w, and (ii) the correctness of s can be veriﬁed in polynomial time.

Chapter 7.
Summary
243
The following properties hold:
1. Every context-free language is in P. (We did not prove this).
2. Every language in P is also in NP.
3. It is not known if there exist languages that are in NP, but not in P.
4. Every language in NP is decidable.
We have introduced reductions to deﬁne the notion of a language B to be
“at least as hard” as a language A. A language B is called NP-complete, if
1. B belongs to the class NP, and
2. B is at least as hard as every language in the class NP.
We have seen that NP-complete exist.
The ﬁgure below summarizes the relationships among the various classes
of languages.

244
Chapter 7.
Summary
regular
context-free
P
NP
decidable
enumerable
all languages


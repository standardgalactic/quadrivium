Applied
Probabilit
y
and
Sto
c
hastic
Pro
cesses
Lecture
Notes
for
/
Class
W 
lo
dzimierz
Bryc
Departmen
t
of
Mathematics
Univ
ersit
y
of
Cincinnati
P
.
O.
Bo
x
00
Cincinnati,
OH
-00
E-mail:
bryc@ucbeh.san.uc.edu
Created:
Octob
er
,
		
Prin
ted:
Marc
h
,
		
c
		
W 
lo
dzimierz
Bryc
C
i
n
c
i
n
n
a
t
i
F
r
e
e
T
e
x
t
s

Con
ten
ts
I
Probabilit
y


Random
phenomena

.
Mathematical
mo
dels,
and
sto
c
hastic
mo
dels
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Ev
en
ts
and
their
c
hances
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Uniform
Probabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Geometric
Probabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Elemen
tary
probabilit
y
mo
dels
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Consequences
of
axioms
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
General
discrete
sample
space
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
General
con
tin
uous
sample
space
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Sim
ulating
discrete
ev
en
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Generic
sim
ulation
template
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Blind
searc
h
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Application:
T
ra
v
eling
salesman
problem
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Impro
ving
blind
searc
h
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Random
p
erm
utations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Conditional
probabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Prop
erties
of
conditional
probabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Sequen
tial
exp
erimen
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Indep
enden
t
ev
en
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Random
v
ariables
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Binomial
trials
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
F
urther
notes
ab
out
sim
ulations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Questions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	

Random
v
ariables
(con
tin
ued)

.
Discrete
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Examples
of
discrete
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Sim
ulations
of
discrete
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Con
tin
uous
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Examples
of
con
tin
uous
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Histograms
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Sim
ulations
of
con
tin
uous
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Exp
ected
v
alues
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
T
ail
in
tegration
form
ulas
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Cheb
yshev-Mark
o
v
inequalit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

iii

iv
CONTENTS
.
Exp
ected
v
alues
and
sim
ulations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Join
t
distributions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Indep
enden
t
random
v
ariables
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
F
unctions
of
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Momen
ts
of
functions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Sim
ulations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Application:
sc
heduling
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Deterministic
sc
heduling
problem
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Sto
c
hastic
sc
heduling
problem
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
More
sc
heduling
questions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.	
Distributions
of
functions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.0
L

-spaces
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Correlation
co
ecien
t
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Best
linear
appro
ximation
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Application:
length
of
a
random
c
hain
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Conditional
exp
ectations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Conditional
distributions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Conditional
exp
ectations
as
random
v
ariables
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Conditional
exp
ectations
(con
tin
ued)
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Best
non-linear
appro
ximations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Lac
k
of
memory
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
In
tensit
y
of
failures
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
P
oisson
appro
ximation
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Questions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Momen
t
generating
functions

.
Generating
functions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Prop
erties
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Probabilit
y
generating
functions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Characteristic
functions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Questions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Normal
distribution
	
.
Hersc
hel's
la
w
of
errors
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Biv
ariate
Normal
distribution
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Questions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Limit
theorems

.
Sto
c
hastic
Analysis
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
La
w
of
large
n
um
b
ers
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Strong
la
w
of
large
n
um
b
ers
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Con
v
ergence
of
distributions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Cen
tral
Limit
Theorem
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Limit
theorems
and
sim
ulations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Large
deviation
b
ounds
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Conditional
limit
theorems
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


CONTENTS
v
.
Questions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

I
I
Sto
c
hastic
pro
cesses
	

Sim
ulations

.
Generating
random
n
um
b
ers
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Random
digits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Ov
erview
of
random
n
um
b
er
generators
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Sim
ulating
discrete
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Generic
Metho
d
{
discrete
case
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Geometric
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Binomial
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
P
oisson
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Sim
ulating
con
tin
uous
r.
v.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Generic
Metho
d
{
con
tin
uous
case
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Randomization
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Sim
ulating
normal
distribution
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Rejection
sampling
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Sim
ulating
discrete
exp
erimen
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Random
subsets
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Random
P
erm
utations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
In
tegration
b
y
sim
ulations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Stratied
sampling
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Mon
te
Carlo
estimation
of
small
probabilities
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


In
tro
duction
to
sto
c
hastic
pro
cesses
	
.
Dierence
Equations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Examples
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Linear
dierence
equations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Problems
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Recursiv
e
equations,
c
haos,
randomness
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Mo
deling
and
sim
ulation
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Random
w
alks
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Stopping
times
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Example:
c
hromatograph
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Ruining
a
gam
bler
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Random
gro
wth
mo
del
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Mark
o
v
pro
cesses

.
Mark
o
v
c
hains
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Finite
state
space
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Mark
o
v
pro
cesses
and
graphs
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Sim
ulating
Mark
o
v
c
hains
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Example:
so
ccer
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
One
step
analysis
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Example:
v
ehicle
insurance
claims
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


vi
CONTENTS
..
Example:
a
game
of
piggy
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Recurrence
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.
Sim
ulated
annealing
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
..
Program
listing
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	0
.
Solving
dierence
equations
through
sim
ulations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	0
.
Mark
o
v
Autoregressiv
e
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	0
.
Sorting
at
random
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
.	
An
application:
nd
k
-th
largest
n
um
b
er
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
	
Branc
hing
pro
cesses
	
	.
The
mean
and
the
v
ariance
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
	.
Generating
functions
of
Branc
hing
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
	..
Extinction
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
	.
Tw
o-v
alued
case
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
	.
Geometric
case
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
0
Multiv
ariate
normal
distribution
	
0.
Multiv
ariate
momen
t
generating
function
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
0.
Biv
ariate
normal
distribution
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
0..
Example:
normal
random
w
alk
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
		
0.
Sim
ulating
a
m
ultiv
ariate
normal
distribution
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
00
0..
General
m
ultiv
ariate
normal
la
w
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
00
0.
Co
v
ariance
matrix
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
00
0..
Multiv
ariate
normal
densit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
0..
Linear
regression
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
0.
Gaussian
Mark
o
v
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0

Con
tin
uous
time
pro
cesses
0
.
P
oisson
pro
cess
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
The
la
w
of
rare
ev
en
ts
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Comp
ound
P
oisson
pro
cess
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Con
tin
uous
time
Mark
o
v
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
..
Examples
of
con
tin
uous
time
Mark
o
v
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
0
.
Gaussian
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
The
Wiener
pro
cess
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0

Time
Series
0	
.
Second
order
stationary
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0	
..
P
ositiv
e
denite
functions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
T
ra
jectory
a
v
erages
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
The
general
prediction
problem
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
.
Autoregressiv
e
pro
cesses
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


Additional
topics

.
A
simple
probabilistic
mo
deling
in
Genetics
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Application:
v
erifying
matrix
m
ultiplication
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Exc
hangeabilit
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:


CONTENTS
vii
.
Distances
b
et
w
een
strings
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
A
mo
del
of
cell
gro
wth
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Shannon's
En
trop
y
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

..
Optimal
searc
h
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Application:
spread
of
epidemic
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
A
Theoretical
complemen
ts

A.
L
p
-spaces
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

A.
Prop
erties
of
conditional
exp
ectations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B
Math
bac
kground

B.
In
teractiv
e
links
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.
Elemen
tary
com
binatorics
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.
Limits
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.
P
o
w
er
series
expansions
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.
Multiv
ariate
in
tegration
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.
Dieren
tial
equations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.
Linear
algebra
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.
F
ourier
series
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

B.	
P
o
w
ers
of
matrices
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

C
Numerical
Metho
ds
	
C.
Numerical
in
tegration
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
C.
Solving
equations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
C.
Searc
hing
for
minim
um
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
D
Programming
Help

D.
In
tro
ducing
BASIC
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
Computing
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
The
structure
of
a
program
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
Conditionals
and
lo
ops
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
User
input
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
More
ab
out
prin
ting
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
Arra
ys
and
matrices
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
Data
t
yp
es
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.	
User
dened
FUNCTIONs
and
SUBs
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.0
Graphics
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
Binary
op
erations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
File
Op
erations
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
Go
o
d
programming
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D.
Example:
designing
an
automatic
card
dealer
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D..
First
Iteration
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

D..
Second
Iteration
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
	
D..
Third
iteration
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
0
Bibliograph
y


viii
Description
Course
description
This
course
is
aimed
at
studen
ts
in
applied
elds
and
assumes
a
prerequisite
of
calculus.
The
goal
is
a
w
orking
kno
wledge
of
the
concepts
and
uses
of
mo
dern
probabilit
y
theory
.
A
signican
t
part
of
suc
h
a
\w
orking
kno
wledge"
in
mo
dern
applications
of
mathematics
is
computer-dep
enden
t.
The
course
con
tains
mathematical
problems
and
computer
exercises.
Studen
ts
will
b
e
exp
ected
to
write
and
execute
programs
p
ertinen
t
to
the
material
of
the
course.
No
pro-
gramming
exp
erience
is
necessary
or
assumed.
But
the
willingness
to
accept
a
computer
as
a
to
ol
is
a
requiremen
t.
F
or
no
vices,
BASIC
programming
language,
(QBASIC
in
DOS,
Visual
Basic
in
Win-
do
ws,
or
BASIC
on
Macin
tosh)
is
recommended.
BASIC
is
p
erhaps
the
easiest
program-
ming
language
to
learn,
and
the
rst
programming
language
is
alw
a
ys
the
hardest
to
pic
k.
Programs
in
QBASIC
.
on
IBM-compatible
PC
and,
to
a
lesser
extend,
programs
on
T
exas
Instrumen
t
Programmable
calculator
TI-,
and
Windo
ws
T
M
programs
in
Microsoft
Visual
Basic
.0
are
supp
orted.
This
means
that
I
will
attempt
to
an-
sw
er
tec
hnical
questions
and
pro
vide
examples.
Other
programming
languages
(SAS,
C,
C++,
Fortran,
Cobol,
Assembler,
Mathematica,
LISP,
T
E
X(!),
Excel,
etc.)
can
b
e
used,
but
I
will
not
b
e
able
to
help
with
the
tec
hnical
issues.
Con
ten
ts
of
the
course
(sub
ject
to
c
hange)

Basic
elemen
ts
of
probabilit
y
.
P
oisson,
geometric,
binomial,
normal,
exp
onen
tial
distributions.
Sim
ulations.
Conditioning,
c
haracterizations.
Momen
t
generating
functions,
limit
theorems,
c
haracteristic
functions.
Sto
c
hastic
pro
cesses:
random
w
alks,
Mark
o
v
sequences,
the
P
oisson
pro
cess.

Time
dep
enden
t
and
sto
c
hastic
pro
cesses:
Mark
o
v
pro
cesses,
branc
hing
pro
cesses.
Mo
deling
Multiv
ariate
normal
distribution.
Gaussian
pro
cesses,
white
noise.
Conditional
exp
ectations.
F
ourier
expansions,
time
series.
Supp
orting
materials
This
text
is
a
v
ailable
through
In
ternet

in
P
ostScript,
or
D
VI.
A
n
um
b
er
of
other
math
related
resources
can
b
e
found
on
WWW

.
Also
a
v
ailable
are
supp
orting
BASIC

program
les.
Supp
ort
for
Pascal
is
an
ticipated
in
the
future.
Auxiliary
textb
o
oks:

W.
F
eller,
An
In
tro
duction
to
Probabilit
y
Theory,
V
ol.
I,
Wiley
	.
V
ol
I
I,
Wiley
,
New
Y
ork
	

h
ttp://math.uc.edu/
~
brycw/probab/b
o
oks/

h
ttp://arc
hiv
es.math.utk.edu/tutorials.h
tml

h
ttp://math.uc.edu/
~
brycw/probab/basic.h
tm

Description
ix
V
olume
I
is
an
excellen
t
in
tro
duction
to
elemen
tary
and
not-that-elemen
tary
prob-
abilit
y
.
V
olume
I
I
is
adv
anced.

W.
H.
Press,
S.
A.
T
euk
olsky
,
W.
T.
V
etterling,
B.
P
.
Flannery
,
Numerical
recip
es
in
C,
Cam
bridge
Univ
ersit
y
Press,
New
Y
ork
A
reference
for
n
umerical
metho
ds:
C-language
v
ersion.

J.
C.
Sprott
Numerical
recip
es
Routines
and
Examples
in
BASIC,
Cam
bridge
Uni-
v
ersit
y
Press,
New
Y
ork
		
A
reference
for
n
umerical
metho
ds:
routines
in
Quic
kBasic
.
v
ersion.

H.
M.
T
a
ylor
&
S.
Karlin,
An
in
tro
duction
to
sto
c
hastic
mo
deling,
Acad.
Press,
Boston
	
Mark
o
v
c
hains
with
man
y
examples/mo
dels,
Branc
hing
pro
cesses,
Queueing
sys-
tems.

L.
Breiman,
Probabilit
y
and
Sto
c
hastic
Pro
cesses:
with
a
view
to
w
ards
applications,
Hough
ton
Miin,
Boston
		.
includes
Mark
o
v
c
hains
and
sp
ectral
theory
of
stationary
pro
cesses.

R.
E.
Barlo
w
&
F.
Prosc
han,
Mathematical
Theory
of
Reliabilit
y,
SIAM
series
in
applied
math,
Wiley
,
New
Y
ork
	.
Adv
anced
comp
endium
of
reliabilit
y
theory
metho
ds
(mid-sixties).

S.
Bisw
as,
Applied
Sto
c
hastic
Pro
cesses,
Wiley
,
New
Y
ork
		
T
ec
hniques
of
in
terest
in
p
opulation
dynamics,
epidemiology
,
clinical
drug
trials,
fertilit
y
and
mortalit
y
analysis.

J.
Higgins
&
S.
Keller-McNult
y
,
Concepts
in
Probabilit
y
and
Sto
c
hastic
Mo
deling
Duxbury
Press
		
Co
v
ers
traditional
material;
computer
sim
ulations
complemen
t
theory
.

T.
Harris,
The
Theory
of
Branc
hing
Pro
cesses
Reprin
ted:
Do
v
er,
		.
A
classic
on
Branc
hing
pro
cesses.

H.
C.
Tjims,
Sto
c
hastic
mo
dels.
An
algorithmic
approac
h,
Wiley
,
Chic
hester,
		.
Renew
al
pro
cesses,
reliabilit
y
,
in
v
en
tory
mo
dels,
queuieing
mo
dels.
Con
v
en
tions
Exercises
The
text
has
three
t
yp
es
of
\practice
questions",
mark
ed
as
Problems,
Exercises,
and
Pro
jects.
Examples
v
ary
the
most
and
could
b
e
solv
ed
in
class
b
y
an
instructor;
Exercises
are
in
tended
primarily
for
computer-assisted
analysis;
Problems
are
of
more
mathematical
nature.
Pro
jects
are
longer,
and
frequen
tly
op
en-ended.
Exercises,
Pro
jects,
and
Problems
are
n
um
b
ered
consecutiv
ely
within
c
hapters;
for
instance
Exercise
0.
follo
ws
Problem
0.,
meaning
that
there
is
no
Exercise
0..

x
Description
Programming
The
text
refers
to
BASIC
instructions
with
the
con
v
en
tion
that
BASIC
k
ey-w
ords
are
cap-
italized,
the
names
of
v
ariables,
functions,
and
the
SUBs
are
mixed
case
lik
e
ThisExample.
Program
listings
are
t
yp
eset
in
a
sp
ecial
\computer"
fon
t
to
distinguish
them
from
the
rest
of
the
text.

License

Cop
yrigh
t
and
License
for
		
v
ersion
This
textbook
is
copyrighted
c
W.
Br
yc
		.
All
rights
re-
ser
ved.
GTDR.
A
cademic
users
are
granted
the
right
to
make
copies
f
or
their
personal
use,
and
f
or
distribution
to
their
students
in
a
cademic
year
		/		.
Repr
oduction
in
any
f
orm
by
commer
cial
publishers,
including
textbook
publishers,
requires
written
permission
fr
om
the
copy-
right
holder.
C
i
n
c
i
n
n
a
t
i
F
r
e
e
T
e
x
t
s
c
		
W.
Bryc


License

P
art
I
Probabilit
y



Chapter

Random
phenomena
Denition
of
a
T
ree:
A
tr
e
e
is
a
wo
o
dy
plant
with
er
e
ct
p
er
ennial
trunk
of
at
le
ast
.
inches
(.
c
entimeters)
in
diameter
at
br
e
ast
height
(


fe
et
or
.
meters),
a
denitely
forme
d
cr
own
of
foliage,
and
a
height
of
at
le
ast

fe
et
(
meters).
The
Aub
orn
So
ciet
y
Field
Guide
to
North
American
T
rees.
This
c
hapter
in
tro
duces
fundamen
tal
concepts
of
probabilit
y
theory;
ev
en
ts,
and
their
c
hances.
F
or
the
readers
who
are
familiar
with
elemen
tary
probabilit
y
it
ma
y
b
e
refreshing
to
see
the
computer
used
for
coun
ting
elemen
tary
ev
en
ts,
and
randomization
used
to
solv
e
a
deterministic
optimization
problem.
The
questions
are

What
is
\probabilit
y"?

Ho
w
do
w
e
ev
aluate
probabilities
in
real-life
situations?

What
is
the
computer
go
o
d
for?
.
Mathematical
mo
dels,
and
sto
c
hastic
mo
dels
Ev
ery
theory
has
its
successes,
and
its
limitations.
These
notes
are
ab
out
the
successes
of
probabilit
y
theory
.
But
it
do
esn't
h
urt
to
explain
in
non-tec
hnical
language
some
of
its
limitations
up
fron
t.
This
w
a
y
the
reader
can
understand
the
basic
premise
b
efore
in
v
esting
considerable
time.
T
o
b
egin
with,
w
e
start
with
a
truism.
Real
w
orld
is
complicated,
often
to
a
larger
degree
than
scien
tists
will
readily
admit.
Most
real
phenomena
ha
v
e
m
ulti-asp
ect
form,
and
can
b
e
approac
hed
in
m
ultiple
w
a
ys.
V
arious
questions
can
b
e
ask
ed.
Ev
en
seemingly
the
same
question
can
b
e
answ
ered
on
man
y
incompatible
lev
els.
F
or
instance,
the
generic
question
ab
out
dinosaur
extinction
has
the
follo
wing
v
arian
ts.

Wh
y
did
Dino
the
dinosaur
die?
W
as
she
bitten
b
y
a
p
oisonous
rat-lik
e
creature?
Hit
b
y
a
meteorite?
F
roze
to
death?

Wh
y
did
the
cro
co
diles
surviv
e
to
our
times,
and
t
yranosaurus
rex
didn't?



CHAPTER
.
RANDOM
PHENOMENA

What
w
as
the
cause
of
the
dinosaur
extinction?

W
as
the
dinosaur
extinction
an
acciden
t,
or
did
it
ha
v
e
to
happ
en?
(This
w
a
y
,
or
the
other).

Do
all
sp
ecies
die
out?
Theses
questions
are
ordered
from
the
most
individual
lev
el
to
the
most
abstract.
The
reader
should
b
e
a
w
are
that
probabilit
y
theory
,
and
sto
c
hastic
mo
delling
deal
only
with
the
most
abstract
lev
els
of
the
question.
Th
us,
a
sto
c
hastic
mo
del
ma
y
p
erhaps
shed
some
ligh
t
whether
dinosaurs
had
to
go
extinct,
or
whether
mammals
will
go
extinct,
but
it
w
ouldn't
go
in
to
details
of
whic
h
comet
had
to
b
e
resp
onsible
for
dinosaurs,
or
whic
h
is
the
one
that
will
b
e
resp
onsible
for
the
extinction
of
mammals.
It
isn't
our
con
ten
tion
that
individual
questions
ha
v
e
no
merit.
They
do,
and
p
erhaps
they
are
as
imp
ortan
t
as
the
general
theories.
F
or
example,
a
detectiv
e
in
v
estigating
the
cause
of
a
m
ysterious
death
of
a
y
oung
w
oman,
will
ha
v
e
little
in
terest
in
the
\abstract
statistical
fact"
that
all
h
umans
ev
en
tually
die
an
yho
w.
But
individual
questions
are
as
man
y
as
the
trees
in
the
forest,
and
w
e
don't
w
an
t
to
o
v
erlo
ok
the
forest,
either.
Probabilistic
mo
dels
deal
with
general
la
ws,
not
individual
histories.
Their
predictions
are
on
the
same
lev
el,
to
o.
T
o
come
bac
k
to
our
motiv
ating
example,
ev
en
if
a
sto
c
hastic
mo
del
did
predict
the
extinction
of
dinosaurs
(ev
en
tually),
it
w
ould
not
sa
y
that
it
had
to
happ
en
at
the
time
when
it
actually
happ
ened,
some
0
million
y
ears
ago.
And
the
more
concrete
a
question
is
p
osed,
sa
y
if
w
e
w
an
t
to
kno
w
when
Dino
the
dinosaur
died,
the
less
can
b
e
extracted
from
the
sto
c
hastic
mo
del.
On
the
other
hand,
man
y
concepts
of
mo
dern
science
are
dene
in
statistical,
or
probabilistic
sense.(If
y
ou
think
this
isn't
true,
ask
y
ourself
ho
w
man
y
trees
do
mak
e
a
forest.)
Suc
h
concepts
are
b
est
studied
from
the
probabilistic
p
ersp
ectiv
e.
An
extreme
view
is
to
consider
ev
erything
random,
deterministic
mo
dels
b
eing
just
appro
ximations
that
w
ork
for
small
lev
els
of
noise.
.
Ev
en
ts
and
their
c
hances
Supp
ose

is
a
set,
called
the
probabilit
y
,
or
the
sample
space.
W
e
in
terpret

as
a
mathematical
mo
del
listing
all
relev
an
t
outcomes
of
an
exp
erimen
t.
Let
M
b
e
a

-eld
of
its
subsets,
called
the
ev
en
ts.
Ev
en
ts
A;
B

M
mo
del
sen
tences
ab
out
the
outcomes
of
the
exp
erimen
t
to
whic
h
w
e
w
an
t
to
assign
probabilities.
Under
this
in
terpretation,
the
union
A
[
B
of
ev
en
ts
corresp
onds
to
the
alternative,
the
in
tersection
A
\
B
corresp
onds
to
the
c
onjunction
of
sen
tences,
and
the
complemen
t
A
0
corresp
onds
to
the
ne
gation
of
a
sen
tence.
F
or
A;
B

M,
A
n
B
:=
A
\
B
0
denotes
the
set-theoretical
dierence.
F
or
an
ev
en
t
A

M
the
probabilit
y
Pr(A)
is
a
n
um
b
er
in
terpreted
as
the
degree
of
certain
t
y
(in
unique
exp
erimen
ts),
or
asymptotic
frequency
of
A
(in
rep
eated
exp
erimen
ts).
Probabilit
y
Pr(A)
is
assigned
to
all
ev
en
ts
A

M,
but
it
m
ust
satisfy
certain
requiremen
ts
(axioms).
A
set
function
Pr
()
is
a
probabilit
y
measure
on
(
;
M),
if
it
fullls
the
follo
wing
conditions:
.
0

Pr
(A)



..
EVENTS
AND
THEIR
CHANCES

.
Pr(
)
=

.
F
or
disjoin
t

Pr
(A
[
B
)
=
Pr(A)
+
Pr(B
).
.
If
A
n
are
suc
h
that
T
n
A
n
=
;
and
A


A


:
:
:

A
n

A
n+

:
:
:
are
de
cr
e
asing
events,
then
Pr
(A
n
)
!
0:
(:)
Probabilit
y
axioms
do
not
determine
the
probabilities
in
a
unique
w
a
y
.
The
axioms
pro
vide
only
minimal
consistency
requiremen
ts,
whic
h
are
satised
b
y
man
y
dieren
t
mo
dels.
..
Uniform
Probabilit
y
F
or
nite
set

let
Pr(A)
=
#A
#
:
(:)
This
captures
the
in
tuition
that
the
probabilit
y
of
an
ev
en
t
is
prop
ortional
to
the
n
um
b
er
of
w
a
ys
that
the
ev
en
t
migh
t
o
ccur.
The
uniform
assignmen
t
of
probabilit
y
in
v
olv
es
coun
ting.
F
or
small
sample
spaces
this
can
b
e
accomplished
b
y
examining
all
cases.
Mo
derate
size
sample
spaces
can
b
e
insp
ected
b
y
a
computer
program.
Coun
ting
arbitrary
large
spaces
is
the
domain
of
c
ombinatorics.
It
in
v
olv
es
com
binations,
p
erm
utations,
generating
functions,
com
binatorial
iden
tities,
etc.
Short
review
in
SectionB.
recalls
the
most
elemen
tary
coun
ting
tec
hniques.
Problem
.
Thr
e
e
identic
al
dic
e
ar
e
tosse
d.
What
is
the
pr
ob
ability
of
two
of
a
kind?
The
follo
wing
BASIC
program
insp
ects
all
outcomes
when
v
e
dice
are
rolled,
and
coun
ts
ho
w
man
y
are
\four
of
a
kind".
PROGRAM
yahtzee.bas
'
'declare
function
and
variables
DECLARE
FUNCTION
CountEq!
(a!,
b!,
c!,
d!,
e!)
'prepare
screen
CLS
PRINT
"Listing
four
of
a
kind
outcomes
in
Yahtzee..."
'***
go
through
all
cases
FOR
a
=

TO
:
FOR
b
=

TO
:
FOR
c
=

TO
:
FOR
d
=

TO
:
FOR
e
=

TO

IF
CountEq(a
+
0,
b
+
0,
c
+
0,
d
+
0,
e
+
0)
=

THEN
PRINT
a;
b;
c;
d;
e;
:
ct
=
ct
+

IF
ct
MOD

=
0
THEN
PRINT
:
ELSE
PRINT
"|";
END
IF
NEXT:
NEXT:
NEXT:
NEXT:
NEXT
'print
result

Disjoint,
or
exclusive
ev
en
ts
A;
B


are
suc
h
that
A
\
B
=
;
is
empt
y
.


CHAPTER
.
RANDOM
PHENOMENA
PRINT
PRINT
"Total
of
";
ct;
"
four
of
a
kind."
FUNCTION
CountEq
(a,
b,
c,
d,
e)
'***
count
how
many
of
five
numbers
are
the
same
DIM
x()
x()
=
a
x()
=
b
x()
=
c
x()
=
d
x()
=
e
max
=
0
FOR
j
=

TO

ck
=
0
FOR
k
=

TO

IF
x(j)
=
x(k)
THEN
ck
=
ck
+

NEXT
k
ck
=
-ck
IF
ck
>
max
THEN
max
=
ck
NEXT
j
'assign
value
to
function
CountEq
=
max
'
END
FUNCTION
Here
is
a
p
ortion
of
its
output:





|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|










|





|





|





|





Total
of
0
four
of
a
kind.
The
program
is
written
explicitly
for
tossing
v
e
dice,
and
y
ou
ma
y
w
an
t
to
mo
dify
it
to
answ
er
similar
question
for
an
y
n
um
b
er
of
dice,
and
an
arbitrary
k
-of-a-kind
question.
Exercise
.
R
un
and
time
YAHTZEE.BAS.
Then
estimate
how
long
a
similar
pr
oblem
would
run
if
the
question
involve
d
tossing

fair
dic
e.
The
answer
dep
ends
on
your
c
om-
puter,
and
the
softwar
e.
Both
Pascal
and
C-pr
o
gr
ams
se
em
to
run
on
my
c
omputer
ab
out

times
faster
than
the
(c
ompile
d)
QuickBasic.
ANS:
Running
time
w
ould
tak
e
y
ears!
Exercise
.
sho
ws
the
p
o
w
er
of
old-fashioned
p
encil-and-pap
er
calculation.
Problem
.
Continuing
Pr
oblem
.,
supp
ose
now
n
identic
al
dic
e
ar
e
tosse
d.
What
is
the
pr
ob
ability
of
n
 
of
a
kind?
ANS:
n

n 
.

..
ELEMENT
AR
Y
PR
OBABILITY
MODELS

..
Geometric
Probabilit
y
F
or
b
ounded
subsets


I
R
d
,
put
Pr(A)
=
jAj
j
j
:
(:)
This
captures
the
in
tuition
that
the
probabilit
y
of
hitting
a
target
is
prop
ortional
to
the
the
size
of
the
target.
Geometric
probabilit
y
usually
in
v
olv
es
m
ultiv
ariate
in
tegrals.
Example
.
A
p
oint
is
sele
cte
d
fr
om
the

cm
wide
cir
cular
dartb
o
ar
d.
The
pr
ob
ability
that
it
lies
within
the
cm
wide
bul
lseye
is


.
Example
.
A
ne
e
d
le
of
length
`
<

is
thr
own
onto
a
p
ap
er
rule
d
every

inches.
The
pr
ob
ability
that
the
ne
e
d
le
cr
osses
a
line
is
`=
.
Analysis
of
Example
.
is
a
v
ailable
on
WWW

.
Exercise
.
T
est
by
exp
eriment
(or
simulation
on
the
c
omputer)
if
the
ab
ove
two
ex-
amples
give
c
orr
e
ct
answers.
(Befor
e
writing
a
pr
o
gr
am,
you
may
want
to
r
e
ad
Se
ction
.
rst.)
Example
.
Two
drivers
arrive
at
an
interse
ction
b
etwe
en
:00
and
:0
every
day.
If
they
arrive
within

se
c
onds
of
e
ach
other,
b
oth
c
ars
have
to
stop
at
the
stop-sign.
How
often
do
the
drivers
p
ass
thr
ough
the
interse
ction
without
stopping?
Pro
ject
.
Continuing
Example
.:
What
if
ther
e
ar
e
thr
e
e
c
ars
in
this
neighb
orho
o
d?
F
our?
How
do
es
the
pr
ob
ability
change
with
the
numb
er
of
c
ars?
A
t
what
numb
er
of
users
a
stop
light
should
b
e
instal
le
d?
.
Elemen
tary
probabilit
y
mo
dels
..
Consequences
of
axioms
Here
are
some
useful
form
ulas
that
are
easy
to
c
hec
k
with
the
help
of
the
V
enn
diagr
ams.
F
or
all
ev
en
ts
A;
B
Pr(A
[
B
)
=
Pr
(A)
+
Pr(B
)
 Pr(A
\
B
)
(:)
Pr
(A
0
)
=

 Pr(A):
(:)
If
A

B
then
Pr
(B
n
A)
=
Pr(B
)
 Pr(A):
(:)
If
A
n
are
pairwise
disjoin
t
ev
en
ts,
then
Pr(

[
n=
A
n
)
=

X
n=
Pr(A
n
):
(:)

h
ttp://www.mste.uiuc.edu/reese/buon/buon.h
tml


CHAPTER
.
RANDOM
PHENOMENA
..
General
discrete
sample
space
F
or
a
nite
or
coun
table
set


I
N
and
a
giv
en
summable
sequence
of
non-negativ
e
n
um
b
ers
a
n
put
Pr
(A)
=
P
nA
a
n
P
n
a
n
:
(:)
In
probabilit
y
theory
it
is
customary
to
denote
p
k
=
a
k
P
n
a
n
and
rewrite
(.)
as
Pr
(A)
=
P
nA
p
n
.
F
orm
ula
(.)
generalizes
the
uniform
assignmen
t
(.),
whic
h
corresp
onds
to
the
c
hoice
of
equal
w
eigh
ts
a
k
=
.
A
t
the
same
time
it
is
more
exible

and
applies
also
to
innite
sample
spaces.
Example
.
L
et

=
I
N
and
put
p
k
=


k
.
Then
the
pr
ob
ability
that
an
o
dd
inte
ger
is
sele
cte
d
is
Pr(O
dd)
=
P

j
=0

 j
 
=


.
(Why?
Se
e
(B.))
T
able
.
list
the
most
frequen
tly
encoun
tered
discrete
probabilit
y
assignmen
ts.
Name

Probabilities
p
k
Binomial
f0;
:
:
:
;
ng
p
k
=
(
n
k
)p
k
(
 p)
n k
P
oisson
Z
Z
+
p
k
=
e
 

k
k
!
Geometric
I
N
p
k
=
p(
 p)
k
 
Equally
lik
ely
outcomes
fx

;
:
:
:
;
x
k
g
p
k
=

k
T
able
.:
Probabilit
y
assignmen
ts
for
discrete
sample
spaces.
Problem
.
F
or
e
ach
of
the
choic
es
of
numb
ers
p
k
in
T
able
.
verify
that
(.)
inde
e
d
denes
a
pr
ob
ability
me
asur
e.
The
reasons
b
ehind
the
particular
c
hoices
of
the
expressions
for
p
k
in
T
able
.
in
v
olv
e
mo
deling.
..
General
con
tin
uous
sample
space
There
is
no
easily
accessible
general
theory
for
innite
non-coun
table
sample
spaces.
When


I
R
k
,
the
generalization
of
the
geometric
probabilit
y
uses
a
non-negativ
e
densit
y
function
f
(x

;
x

;
:
:
:
;
x
k
)
Pr(A)
=
C
Z
A
f
(x

;
x

;
:
:
:
;
x
k
)
dx

dx

:
:
:
dx
k
(:	)
F
or
one-dimensional
case
k
=
,
examples
of
densities
are
collected
in
T
able
.
on
page
.

The
price
for
exibilit
y
is
that
no
w
w
e
ha
v
e
to
decide
ho
w
to
c
hose
p
n
.

..
SIMULA
TING
DISCRETE
EVENTS

.
Sim
ulating
discrete
ev
en
ts
The
most
natural
w
a
y
to
v
erify
whether
a
mathematical
mo
del
reects
realit
y
is
to
com-
pare
theoretically
computed
probabilities
with
the
corresp
onding
empirical
frequencies.
Unfortunately
,
eac
h
part
of
this
pro
cedure
is
often
time
consuming,
exp
ensiv
e,
incon-
v
enien
t,
or
imp
ossible.
Computer
sim
ulations
are
used
as
a
substitute
for
b
oth:
they
pro
vide
n
umerical
estimates
of
theoretical
probabilities,
and
they
are
closer
to
the
direct
exp
erimen
t.
The
rst
step
in
a
sim
ulation
is
to
decide
ho
w
to
generate
\randomness"
within
a
deterministic
algorithm
in
a
computer
program.
Programming
languages,
and
ev
en
cal-
culators,
pro
vide
a
metho
d
for
generating
consecutiv
e
n
um
b
ers
from
the
in
terv
al
(0;
)
\at
random".
F
or
instance,
BASIC
instruction

PRINT
RND()
returns
dieren
t
n
um
b
er
at
ev
ery
use.
W
e
shall
assume
that
the
reader
has
access
to
a
metho
d,
see
Section
.,
of
generating
uniform
n
um
b
ers
from
the
unit
in
terv
al
(0;
).
These
are
called
pseudo-
r
andom
numb
ers,
since
the
program
usually
b
egin
the
same
\random"
sequence
at
ev
ery
run,
unless
sp
ecial
precautions

are
tak
en.
Once
a
pseudo-random
n
um
b
er
from
the
in
terv
al
(0;
)
is
selected,
an
ev
en
t
that
o
ccurs
with
some
kno
wn
probabilit
y
p
can
b
e
sim
ulated
b
y
v
erifying
if
f
RAND()<pg
o
ccurs
in
the
program.
F
or
instance,
the
n
um
b
er
of
heads
in
a
toss
of
a
;
000
fair
coins
is
sim
ulated
b
y
the
follo
wing
BASIC
program.
PROGRAM
tosscoin.bas
'
'Simulating
000
tosses
of
a
fair
coins
H
=
0
FOR
n
=

TO
000
'main
loop
IF
RND()
<
.
THEN
H
=
H
+

NEXT
n
'print
final
message
PRINT
"Got
";
H;
"
heads
this
time"
END
'
Here
is
its
output:
Got
0
heads
this
time
A
simple
metho
d
for
sim
ulating
an
outcome
on
a
six-face
die
is
to
tak
e
the
in
teger
part
of
a
re-scaled
uniformly
selected
n
um
b
er
INT(+*RND()).
Can
y
ou
use
this
to
write
a
sim
ulation
of
a
roll
of

dice?
Suc
h
sim
ulations
are
often
used
to
ev
aluate
probabilities
empirically
as
a
substitute
for
a
real
empirical
study
.
Exercise
.
Write
the
simulation
(as
opp
ose
d
to
deterministic
insp
e
ction
of
al
l
sample
p
oints
on
p
age
)
to
estimate
how
often
the
event
\four
of
a
kind"
o
c
curs
in
a
r
ol
l
of
ve
dic
e.

Similar
instruction
on
TI-
is
Display
rand.

In
BASIC,
to
a
v
oid
rep
etitions
use
instruction
RANDOMIZE
TIMER
at
the
b
eginning
of
a
sim
ulation
program.


CHAPTER
.
RANDOM
PHENOMENA
Pro
ject
.
R
un
the
(mo
die
d)
c
oin
tossing
pr
o
gr
am
in
a
lo
op
and
to
answer
the
fol
low-
ing
questions.

In
a
00
tosses
of
a
c
oin,
how
often
do
es
less
than

he
ads
o
c
cur?

In
a
00
tosses
of
a
c
oin,
how
often
do
es
less
than
0
he
ads
o
c
cur?

Can
you
sketch
the
curve

that
r
epr
esents
the
pr
ob
ability
of
less
than
x
c
oins
in
n
=
00
tosses
of
a
fair
c
oin?
Hin
t:
Move
the
main
p
art
of
TOSSCOIN.BAS
into
a
SUB,
or
a
FUNCTION.
This
way
you
c
an
e
asily
use
it
without
cluttering
your
pr
o
gr
am
with
irr
elevant
details.
(Se
e
a
generic
template
b
elow.)
More
complicated
ob
jects
are
often
of
in
terest
in
sim
ulations.
F
or
instance
w
e
ma
y
w
an
t
to
dra
w
t
w
o
cards
from
the
dec
k
of
.
One
p
ossible
w
a
y
to
do
it
is
to
n
um
b
er
the
cards,
and
select
t
w
o
n
um
b
ers
a;
b.
.
Select
the
rst
card
a=INT(RND()*+)
as
a
random
in
teger
b
et
w
een

and
.
.
Select
the
second
card
b=INT(RND()*+)
in
the
same
w
a
y
.
.
Compare
a;
b
(a)
If
a
=
b
then
rep
eat
step

(b)
Otherwise,
got
t
w
o
dieren
t
cards
a
=
b
at
random
Ho
w
ecien
t
is
this
pro
cedure?
Exercise
.	
How
would
you
simulate
on
a
c
omputer
a
random
p
erm
utation?
A
r
andom
subset?
..
Generic
sim
ulation
template
The
purp
ose
of
sim
ulation
is
to
in
v
estigate
the
unkno
wn
v
alues
of
parameters
of
in
terest.
In
the
initial
exercises
y
ou
ma
y
w
an
t
to
sim
ulate
the
ev
en
ts
that
y
ou
kno
w
ho
w
to
compute
probabilities
of.
The
purp
ose
of
suc
h
exercises
is
to
dev
elop
in
tuition
ab
out
reliabilit
y
of
sim
ulations.
In
more
adv
anced
exercises
y
ou
ma
y
w
an
t
to
estimate
probabilities
that
aren't
kno
wn.
In
suc
h
cases
it
is
alw
a
ys
a
go
o
d
idea
to
run
sim
ulations
of
v
arious
lengths
and
compare
the
results.
In
this
section
w
e
briey
discuss
ho
w
suc
h
a
sim
ulation
can
b
e
organized
in
a
w
a
y
that
promotes
m
ultiple
uses
of
the
same
program.
The
k
ey
is
organizing
the
programs
carefully
in
to
manageable
blo
c
ks
of
small
size.
Mo
dern
BASIc
is
a
structural
programming
language.
The
generic
program
to
study
the
eects
of
the
length
of
sim
ulation
on
its
output
can
b
e
written
as
follo
ws

Y
ou
need
to
nd
out
ho
w
to
handle
graphic
in
BASIC.
Otherwise,
mak
e
a
table
of
v
alues
instead.

..
SIMULA
TING
DISCRETE
EVENTS
	
'
PROGRAM
Generic.bas
'Generic
Simulator
'Size
is
simulation
size
varied
from
Min=00
to
Max=0000
For
Size
00
to
0000
Step
00
Simulate(Size,
Result)
Print
"Simulation
size=";
Size
;
"Output=";
Result
Next
Size
End
The
actual
sim
ulation
is
p
erformed
b
y
SUB
Simulate
(SizeRequested,
Result)
'Runs
requested
number
of
simulations
and
returns
average
'Trial
numbers
consecutive
simulations
from

to
SizeRequested
For
Trial=
to
SizeRequested
SimulateOne(Sco
re)
Result=Result+S
cor
e
Next
Size
'Most
simulations
return
averages
of
single
trials
Result=Score/Si
ze
End
SUB
The
actual
mo
deling
is
p
erformed
in
another
SUB,
whic
h
in
the
generic
program
w
e
named
SimulateOne.
This
SUB
ma
y
b
e
as
simple
as
sim
ulating
a
toss
of
a
single
coin
SUB
SimulateONe(Outc
ome
)
'simulate
One
occurrence,
return
numerical
outcome
OutCome=0
if
RND()</
THEN
Outcome=
END
SUB
Or
it
can
b
e
as
complicated
as
w
e
wish.
The
example
b
elo
w
sim
ulates
a
toss
of
v
e
dice,
and
uses
previously
in
tro
duced
function
CountEq(a,b,c,d,e).
four-of-a-kind.
SUB
SimulateONe(Outc
ome
)
'simulate
One
occurrence,
return
numerical
outcome
d=int(RND()*
+)
d=int(RND()*
+)
d=int(RND()*
+)
d=int(RND()*
+)
d=int(RND()*
+)
IF
CountEq(d,d,d,
d,
d
)=
THEN
Outcome=
END
SUB
..
Blind
searc
h
Elemen
tary
probabilit
y
when
coupled
with
a
fast
computer
is
one
of
the
simplest
eectiv
e
optimization
metho
d.
The
metho
d
is
the
blind
se
ar
ch
{
a
searc
h
for
the
b
est
answ
er
at
random

.
Pure
blind
searc
h
is
usually
simple
to
run,
and
therefore
fast
to
realize.
It
often
nds
answ
ers
that
are
go
o
d
enough
for
practical
purp
oses,
or
at
least
can
serv
e
as
the
preliminary
estimates.
V
arious
ad
ho
c
mo
dications
increase
accuracy
and
are
usually
easy
to
implemen
t,
to
o.

A
related
metho
d
is
brute
for
c
e
{
c
hec
king
all
p
ossible
cases.

0
CHAPTER
.
RANDOM
PHENOMENA
Pro
ject
.0
Write
a
blind-se
ar
ch
pr
o
gr
am
to
nd
the
maximum
of
a
function.

Or
ganize
your
pr
o
gr
am
so
that
the
function
c
an
e
asily
b
e
change
d
{
but
for
now
use
the
one
you
ar
e
quite
familiar
with,
like
00
 (x
 00)

,
or
00e
 (x 0)

sin(00x).

If
you
ar
e
lo
oking
for
mor
e
chal
lenge,
do
the
same
for
thr
e
e
variables.
Write
a
blind-se
ar
ch
pr
o
gr
am
to
nd
a
maximum
of
a
function
like
00e
 (x 0)

sin(00x
+
00y
 z
)
+
00e
 (y
 0)

cos
(00x
+
00y
+
z
)
over
the
b
al
l
x

+
y

+
z


000.

As
a
further
c
omplic
ation,
try
to
nd
a
maximum
of
a
function
that
has
two
lo
c
al
maxima,
and
the
r
e
gion
isn
't
c
onvex.
(This
is
an
almost
hop
eless
task
for
gr
adient
metho
ds!)
..
Application:
T
ra
v
eling
salesman
problem
The
follo
wing
program
searc
hes
for
the
shortest
w
a
y
to
pass
through
the
rst
n
cities

in
the
USA
in
alphab
etic
ordering.
Planning
suc
h
a
tour
is
easy
b
y
hand
for
-
cities.
F
or
longer
tours
some
help
is
needed.
T
o
c
hec
k
the
p
erformance
of
the
blind
searc
h,
y
ou
ma
y
w
an
t
to
kno
w
what
the
usual
algorithms
in
v
olv
e.
A
greedy
metho
d
starts
with
the
shortest
distance,
and
then
k
eeps
going
to
the
closest
cit
y
not
y
et
visited.
Another
heuristic
metho
d
is
to
to
select
a
pair
of
closest
cities
and
accept
the
b
est
(shortest)
connection
among
those
that
do
not
complete
a
shorter
lo
op,
and
do
not
in
tro
duce
a
spurious
third
connection
to
a
cit
y
.
Ev
en
tually
,
the
disjoin
t
pieces
will
form
a
path
that
often
can
b
e
further
impro
v
ed
up
on
insp
ection.
The
program
is
longer
but
not
at
all
sophisticated
{
it
just
selects
paths
at
random.
Notice
that
a
solution
to
Exercise
.	
{
how
to
gener
ate
a
r
andom
p
ermutation
{
is
giv
en
in
one
of
the
subprograms
(whic
h
one?).
The
metho
d
for
the
latter
is
simple-minded
{
the
algorithm
attempts
to
place
consecutiv
e
n
um
b
ers
at
random
sp
ots
un
til
an
empt
y
sp
ot
is
selected.
The
follo
wing
is
the
main
part
of
the
program.
Y
ou
can
use
it
as
a
template
in
designing
y
our
o
wn
v
ersion
of
Blind
searc
h
programs.
The
full
co
de
with
SUBs
is
online
	
in
RANDTOUR.BAS.
'****
CLS
'****
get
number
of
cities
(no
choice
which)
from
user
LOCATE
,

INPUT
;
"Shortest
distance
between
how
many
cities?",
n
'***
initialize
program
CLS
nMax
=
	
'
current
data
size.
Make
sure
not
exceeded!
IF
n
>
nMax
THEN
n
=
nMax
'declare
arrays
DIM
SHARED
dist(nMax,
nMax)
'
matrix
of
distances
DIM
SHARED
city(nMax)
AS
STRING
DIM
P(n),
BestP(n)

If
y
ou
w
an
t
to
include
more
cities,
y
ou
ha
v
e
to
t
yp
e
the
distances
in
a
suitable
format.
If
y
ou
em
bark
on
this
pro
ject,
try
rst
to
implemen
t
a
metho
d
for
selecting
an
arbitrary
subset
of
cities
to
visit.
	
h
ttp://math.uc.edu/
~
brycw/probab/basic.h
tm

..
SIMULA
TING
DISCRETE
EVENTS

'read
distances
CALL
AssignDistances
(nM
ax
)
'initial
permutation
FOR
j
=

TO
n
P(j)
=
j
BestP(j)
=
j
NEXT
j
'initial
length
of
trip
MinLen
=
PathLen(P())
'***
main
loop
DO
'infinite
loop
till
user
stops
'count
trials
no
=
no
+

'***
interacting
with
user
'check
if
user
pressed
key
to
stop
k$
=
INKEY$
IF
k$
>
""
THEN
EXIT
DO
'exit
infinite
loop
'display
currrent
progress
display
(no)
'***
get
any
path
CALL
GetPermutation(P(
))
x
=
PathLen(P())
IF
x
<
MinLen
THEN
'Better
path
found,
so
memorize
and
display
Dlen
=
MinLen
-
x
MinLen
=
x
'*Memorize
best
order
and
print
FOR
j
=

TO
n
BestP(j)
=
P(j)
PRINT
city(P(j));
"->";
NEXT
j
'Finish
printing
PRINT
city(P())
PRINT
"Best
so
far:
";
MinLen
PRINT
"Progress
rate
";
Dlen
/
(no
-
Slen);
"
miles
per
trial"
Slen
=
no
END
IF
LOOP
'Print
final
message
CLS
PRINT
"ALPHABETIC
TOUR
OF
FIRST
";
n;
"
CITIES
in
the
USA"
PRINT
"Blind
Search
Recommended
Route
found
in
";
Slen;
"-th
search"
FOR
j
=

TO
n
-

PRINT
city(BestP(j));
"-->";
NEXT
j
PRINT
city(BestP(n));
"-->";
city(BestP())
PRINT
"Total
distance:
";
MinLen
LOCATE
,
0
PRINT
"(Distances
subject
to
change)"
END
The
program
runs
in
the
innite
lo
op
un
til
it
is
stopp
ed
b
y
the
user.
Once
stopp
ed,
the
program
prin
ts
the
b
est
route
it
found.
F
or
larger
sets
of
cities
w
e
ma
y
ha
v
e
hard
time


CHAPTER
.
RANDOM
PHENOMENA
deciding
when
to
stop
it.
Here
is
a
sample
output
(from
the
impro
v
ed
v
ersion,
as
mark
ed
in
the
actual
co
de):
ALPHABETIC
TOUR
OF
FIRST
	
CITIES
in
the
USA
Blind
Search
Recommended
Route
found
in
,	
searches.
Chicago----Cincinnati----
Buff
alo
----
Albany----Boston----Augus
ta--
--At
lant
ic
City
----Baltimore----Charlott
e---
-At
lant
a---
-Bir
min
gham
----
Bato
n
Rouge----Austin
----Albuquerque----Cheyen
ne--
--B
oise
----
Calg
ary
----
Bill
ings
---
-Bis
marc
k---
-Ch
icag
o
Total
distance:

Can
you
find
a
better
one?
When
y
ou
run
this
program
on
larger
sets
of
cities,
y
ou
will
notice
that
the
program
is
not
fast.
One
p
ossible
impro
v
emen
t
in
the
design
of
this
program
is
to
mo
dify
the
randomization
to
b
e
less
lik
ely
to
pic
k
long
paths.
F
or
instance,
y
ou
can
attempt
to
mo
dify
paths
that
are
kno
wn
to
b
e
short,
or
w
eigh
t
the
mo
dications
b
y
lengths
of
resulting
paths.
Suc
h
metho
ds
are
actually
in
use
in
image
restoration
problems
(simulate
d
anne
aling),
see
page
	0.
..
Impro
ving
blind
searc
h
A
bit
of
exp
erimen
ting
with
v
arious
\pure"
blind
searc
h
programs
should
con
vince
y
ou
that

Blind
searc
h
programs
are
easy
to
write,
if
y
ou
kno
w
ho
w
to
co
de
the
main
function
to
randomize.

Blind
searc
h
alw
a
ys
giv
es
\answ
ers"

It
is
dicult
to
judge
ho
w
go
o
d
an
answ
er
is.

In
situations
that
w
e
do
kno
w
the
answ
er,
blind
searc
h
tak
es
long
time
to
reac
h
it.
It
is
p
ossible
to
impro
v
e
on
the
last
asp
ect
without
complicating
the
program
m
uc
h.
The
idea
is
to
mak
e
random
mo
dications
of
the
curren
tly
b
est
found
v
alue.
F
or
example,
in
a
one-dimensional
maximization
of
a
function
f
(x),
w
e
w
ould
do
the
follo
wing
steps
.
Pic
k
an
initial
\b
est-so-far"
p
oin
t
x
0
and
compute
initial
v
alue
y
0
=
f
(x
0
)
.
Select
at
random
x

in
the
\neigh
b
orho
o
d"
of
x
0
and
compute
y

=
f
(x

)
.
Compare
y
0
;
y

.
(a)
If
y

<
y

then
rep
eat
Step
.
(b)
If
y


y
0
then
mak
e
x

the
new
\b
est-so-far"
y
0
:=
y

;
x
0
:=
x

.
Then
rep
eat
Step
.
.
Stop
the
program
at
user
request,
or
when
no
c
hanges
to
y
0
o
ccur
for
prolonged
n
um
b
er
of
attempts
to
impro
v
e
it.

..
SIMULA
TING
DISCRETE
EVENTS

W
e
w
an
t
to
allo
w
for
the
c
hance
of
c
hec
king
p
oin
ts
far
a
w
a
y
from
the
\b
est-so-far"
answ
er.
But
w
e
don't
w
an
t
this
to
happ
en
to
o
often,
b
ecause
x
0
migh
t
b
e
rather
close
to
the
optim
um.
The
tradeos
are
that
the
program
will
tend
to
follo
w
\direct
path"
to
the
maxim
um,
but
the
danger
is
that
it
will
get
stuc
k
longer
in
lo
cal
maxima.
Impro
v
ed
blind
searc
h
with
time/state
dep
enden
t
randomization
is
actually
imple-
men
ted
within
RANDTOUR.
It
is
commen
ted
out
in
the
v
ersion
on
the
disk,
so
that
it
isn't
op
erational.
T
o
mak
e
it
activ
e,
uncommen
t
the
call
to
ImproveBest
as
a
replacemen
t
for
GetPermutation.
..
Random
p
erm
utations
Program
RANDTOUR.BAS
selects
p
erm
utations
at
random
only
in
its
\simplest"
v
arian
t.
Here
are
a
few
examples
of
problems
that
require
selecting
random
p
erm
utations.

Card
games:
{
P
ok
er
hand:
Select

cards
at
random
from
a
dec
k
of
cards.
{
P
ok
er
(
pla
y
ers):
Select
0
cards
at
random
from
a
dec
k
of
cards.
{
Bridge:
Split

cards
in
to
four
groups

Analyzing
statistical
exp
erimen
ts:
Supp
ose
there
are

items
hidden
under

cups,
and
a
p
erson
is
allo
w
ed
to
try
to
nd
them.
Ho
w
often
all
sev
en
will
b
e
reco
v
ered
b
y
pure
lac
k
(as
opp
osed
to,
sa
y
,
parapsyc
hic
abilities?
The
\naiv
e"
selection
of
random
p
erm
utation
w
astes
man
y
random
n
um
b
ers.
Here
is
an
algorithm
that
conserv
es
resources
b
etter.
The
basic
idea
is
to
select
a
n
um
b
er
from
the
b
eginning
of
the
list
of
all
n
um
b
ers,
and
mo
v
e
it
do
wn
to
the
end.
The
randomly
re-arranged
n
um
b
ers
are
P
();
P
();
:
:
:
;
P
(n)
SUB
GetPermutationFa
st(
P(
))
'Put
random
integers
into
P()
n=UBOUND(P)
'how
many
entries
'this
loop
can
be
omitted
is
we
are
sure
that
P(j)
list
all
the
numbers
we
want
FOR
j=
TO
n
P(j)=j
next
j
for
j=
to
UBOUND(P)
'not
n
as
n
changes
in
the
loop
k=INT(RND()*n+
)
SWAP
P(n),
P(k)
n=n-
next
j
Exercise
.
Ther
e
ar
e
many
inc
omp
atible
me
asur
es
of
\quality"
of
an
algorithm.
One
of
the
\obje
ctive"
criteria
is
the
numb
er
of
\If"
veric
ations.
A
nother
\obje
ctive
criterion"
might
b
e
the
numb
er
of
c
al
ls
to
a
function.
A
\subje
ctive"
criterion,
which
dep
ends
on
the
har
dwar
e
and
cir
cumstanc
es,
is
timing.
Do
es
SUB
GetPermutationFast
deserve
adverb
Fast
in
its
name?


CHAPTER
.
RANDOM
PHENOMENA
Pro
ject
.
The
Subset-Sum
Pr
oblem
is
state
d
as
fol
lows:
L
et
S
b
e
a
set
of
p
ositive
inte
gers
and
let
t
b
e
a
p
ositive
inte
ger.
De
cide
of
ther
e
is
a
subset
S
0

S
such
that
the
sum
of
inte
gers
in
S
0
is
exactly
t.
The
asso
ciate
d
optimization
pr
oblem
is
to
nd
a
subset
S
0

S
whose
elements'
sum
is
the
lar
gest
but
do
esn
't
exc
e
e
d
t.
This
optimization
pr
oblem
is
NP-c
omplete,
ie
it
isn
't
known
if
ther
e
is
a
p
olynomial
time
algorithm
(p
olynomial
in
the
size
of
S
)
to
nd
S
0
.
Investigate
how
the
blind
se
ar
ch
wil
l
do
on
sets
S
sele
cte
d
at
r
andom,
and
on
sets
S
c
onstructe
d
in
mor
e
r
e
gular
fashion
like
arithmetic
pr
o
gr
ession
S
=
fa;
a;
a;
:
:
:
g,
ge
ometric
pr
o
gr
ession
S
=
a;
a

;
a

;
:
:
:.
.
Conditional
probabilit
y
In
mo
deling
more
complicated
phenomena
w
e
ma
y
w
an
t
to
use
dieren
t
probabilities
under
dieren
t
circumstances.
F
or
instance,
in
a
mo
died
blind
searc
h
for
the
minim
um
of
a
non-negativ
e
function,
the
randomization
strategy
migh
t
b
e
dieren
t
when
w
e
already
made
some
progress,
and
it
migh
t
b
e
dieren
t
when
w
e
are
\stuc
k"
in
a
non-optimal
lo
cation.
Th
us
w
e
ma
y
w
an
t
to
consider
probabilities
of
the
same
ev
en
t
A
(sa
y
,
hitting
a
maxim
um)
under
dieren
t
conditions
B
.
T
o
formalize
this
idea,
supp
ose
B
is
an
ev
en
t
suc
h
that
Pr(B
)
=
0.
The
last
condition
merely
sa
ys
that
B
is
an
ev
en
t
that
do
es
ha
v
e
some
c
hance
of
o
ccurring.
Conditional
probabilit
y
of
ev
en
t
A
giv
en
ev
en
t
B
is
denoted
b
y
Pr
(AjB
).
It
is
dened
as
Pr
(AjB
)
=
Pr
(A\B
)
Pr
(B
)
:
Conditional
probabilit
y
satises
the
axioms
of
probabilit
y
,
and
Pr(AjB
)
=
0
if
A;
B
are
disjoin
t.
In
particular,
Pr(A
0
jB
)
=

 Pr(AjB
),
Pr
(B
jB
)
=
.
The
easiest
w
a
y
to
nd
Pr(AjB
)
b
y
sim
ulations
is
to
rep
eatedly
sim
ulate
the
complete
exp
erimen
t,
discarding
all
the
outcomes
except
the
ones
resulting
in
B
.
Exercise
.
Supp
ose
we
toss
r
ep
e
ate
d
ly
a
fair
c
oin,
and
the
\suc
c
ess"
is
to
get
he
ads.
Use
c
omputer
simulations
to
nd
the
c
onditional
pr
ob
ability
that
the
very
rst
trial
was
suc
c
essful,
if
0
c
onse
cutive
(and
indep
endent)
trials
r
esulte
d
in

suc
c
esses.
T
ry
to
answer
the
same
question
under
the
c
ondition
that
0
indep
endent
trials
r
esulte
d
in
0
suc
c
esses.
Y
ou
should
notice
that
it
tak
es
forev
er
to
sim
ulate
ev
en
ts
that
happ
en
rarely
.
Section
.
indicates
one
p
ossible
w
a
y
out
of
this
dicult
y
.
..
Prop
erties
of
conditional
probabilit
y
Conditional
probabilit
y
is
used
in
mo
deling.
Often
Pr(AjB
)
can
b
e
assigned
b
y
\in
tuitiv
e"
considerations.
It
can
then
b
e
used
to
compute
other
probabilities.
The
simplest
example
is
Pr(A
\
B
)
=
Pr
(B
)
Pr
(AjB
),
whic
h
is
a
direct
consequence
of
the
denition.
Example
.
Supp
ose
we
have
a
de
ck
of

c
ar
ds
numb
er
e
d

thr
ough
.
Sinc
e
it
isn
't
obvious
how
to
simulate
sele
cting

c
ar
ds
without
r
eplac
ement,
we
may
want
to
sele
ct
them
with
r
eplac
ement
inste
ad.
L
et
A
denote
the
event
that
al
l
ve
c
ar
ds
ar
e
dier
ent.
What
is
the
pr
ob
ability
of
A?

..
CONDITIONAL
PR
OBABILITY

We
may
p
erform
the
exp
eriment
se
quential
ly,
dr
awing
one
c
ar
d
at
a
time.
L
et
A
k
denote
the
event
that
the
k
c
onse
cutive
dr
aws
r
esulte
d
in
dier
ent
c
ar
ds.
Then
A
=
A


A


:
:
:

A

.
Mor
e
over,
Pr
(A

)
=
.
Cle
arly,
Pr(A

jA

)
=


,
so
Pr(A

)
=
Pr(A

\
A

)
=
Pr(A

jA

)
Pr(A

)
=


.
Sim-
ilarly,
Pr(A

)
=
Pr(A

\
A

)
=
Pr(A

jA

)
Pr(A

)
=
0



.
Continuing
this
we
get
Pr(A

)
=
0	



0:.
The
follo
wing
iden
tities
are
also
of
in
terest.
.
P
ath
Probabilit
y:
Pr
(
T
n
k
=
A
k
)
=
Q
n
k
=
Pr(A
k
j
T
k
 
j
=
A
j
)
.
Ba
y
es
theorem:
Pr
(AjB
)
=
Pr
(B
jA)
Pr
(A)
Pr(B
)
.
T
otal
probabilit
y
form
ula:
If
fB
n
g
are
pairwise
disjoin
t
and
exhaustive,
ie.
Pr
(A
i
\
B
j
)
=
;
for
i
=
j
and
S
B
n
=

,
then
Pr(A)
=
X
n
P
(AjB
n
)
Pr(B
n
):
(:0)
Exercise
.
What
is
the
pr
ob
ability
that
in
a
class
of
0
students
no
matching
birthdays
o
c
cur?
Example
.
A
lake
has
00
white
sh
and
a
00
black
sh,
and
a
ne
arby
p
ond
c
ontains
0
black
sh
and
0
white
ones.
No
other
sh
live
ther
e.
A
sh
is
sele
cte
d
at
r
andom
fr
om
the
lake
and
move
d
to
the
p
ond.
Then
a
sh
is
sele
cte
d
fr
om
the
p
ond
and
move
d
b
ack
to
the
lake.
What
is
the
pr
ob
ability
that
al
l
sh
in
the
p
ond
ar
e
black?
..
Sequen
tial
exp
erimen
ts
Often
the
main
exp
erimen
t
consist
of
a
sequence
of
sub-exp
erimen
ts,
eac
h
dep
ending
on
the
outcome
of
the
previous
one.
If
n
suc
h
sub-exp
erimen
ts
are
c
hained,
then
the
full
exp
erimen
t
results
in
a
c
hain
of
ev
en
ts,
or
a
path
P
=
A

\
A

\
:
:
:
\
A
n
.
If
w
e
assume
that
k
-th
exp
erimen
t
dep
ends
on
the
outcome
of
the
k
 -th
exp
erimen
t
only
,
then
Pr(A
k
jA
k
 
\
:
:
:
\
A

)
=
Pr(A
k
jA
k
 
).
Denoting
b
y
P
the
generic
path
A

\
A

\
:
:
:
\
A
n
,
and
b
y
P
(k
)
=
A
k
w
e
ha
v
e
the
follo
wing
p
ath
inte
gr
al
form
ula
for
the
probabilit
y
of
an
ev
en
t
F
sp
ecifying
the
outcome
of
the
complete
exp
erimen
t,
and
consisting
of
man
y
paths
P
.
Pr
(F
)
=
X
P
F
Pr(P
)
=
X
P
F
jP
j
Y
k
=0
Pr(P
(k
+
)jP
(k
)):
(:)
Example
.
Supp
ose
that
the
double
tr
ansfer
op
er
ation
fr
om
Example
.
was
r
ep
e
ate
d
twic
e.
That
is,
a
r
andom
sele
ction
was
done
four
times.
What
is
the
pr
ob
ability
that
al
l
sh
in
the
p
ond
ar
e
black?
Exercise
.
Che
ck
by
simulation
how
the
pr
op
ortion
of
black
sh
in
the
p
ond
changes
when
the
r
andom
tr
ansfers
fr
om
Example
.
ar
e
p
erforme
d
r
ep
e
ate
d
ly
for
a
long
time.


CHAPTER
.
RANDOM
PHENOMENA
.
Indep
enden
t
ev
en
ts
This
section
in
tro
duces
the
main
mo
deling
concept
b
ehind
the
en
tries
in
the
T
able
..
Tw
o
ev
en
ts
A;
B
are
indep
enden
t,
if
the
conditional
probabilit
y
is
the
same
as
uncon-
ditional,
Pr(AjB
)
=
Pr
(A).
This
is
stated
in
m
ultiplicativ
e
form
whic
h
exhibits
symmetry
and
includes
trivial
ev
en
ts
0
Denition
..
Events
A;
B
ar
e
indep
endent
if
Pr
(A
\
B
)
=
Pr(A)
Pr(B
).
Indep
endence
captures
the
in
tuition
of
non-in
teraction,
and
lac
k
of
information.
In
mo
deling
it
is
often
assumed
rather
than
v
eried.
F
or
instance,
w
e
shall
assume
that
the
ev
en
ts
generated
b
y
consecutiv
e
outputs
of
the
random
generator
are
indep
enden
t.
W
e
also
assume
that
tosses
of
a
coin
(fair,
or
not!)
are
indep
enden
t.
Beginners
sometimes
confuse
disjoin
t
v
ersus
indep
enden
t
ev
en
ts.
Exclusiv
e
(ie.
dis-
join
t)
ev
en
ts
capture
the
in
tuition
of
non-compatible
outcomes.
Not
compatible
outcomes
cannot
happ
en
at
the
same
time.
This
is
not
the
same
as
indep
enden
t
outcomes.
If
A;
B
are
disjoin
t
and
y
ou
kno
w
that
A
o
ccurred,
then
y
ou
do
kno
w
a
lot
ab
out
B
.
Namely
y
ou
kno
w
that
B
cannot
o
ccur.
Th
us
there
is
an
in
teraction
b
et
w
een
A
and
B
.
Kno
wing
whether
A
o
ccurred
inuences
c
hances
of
B
,
whic
h
is
not
p
ossible
under
indep
endence.
Indep
endence
(or,
more
prop
erly
,
mutual
sto
chastic
indep
endenc
e)
of
families
of
ev
en
ts
is
dened
b
y
requesting
a
m
uc
h
larger
n
um
b
er
of
m
ultiplicativ
e
conditions.
The
reason
b
ehind
is
Theorem
..,
whic
h
pro
vides
a
v
ery
con
v
enien
t
to
ol.
Denition
..
Events
A

;
A

;
:
:
:
;
A
n
ar
e
indep
endent,
if
Pr(
T
j
J
A
j
)
=
Q
j
J
Pr(A
j
)
for
al
l
nite
subsets
J

I
N
.
Example
.
A
c
oin
is
tosse
d
r
ep
e
ate
d
ly.
Find
the
pr
ob
ability
that
he
ads
app
e
ars
for
the
rst
time
on
the
fourth
toss.
Problem
.
SUB
GetPermutation
fr
om
the
pr
o
gr
am
RANDTOUR.BAS
sele
cts
numb
ers
b
etwe
en

and
n
at
r
andom
until
it
nds
a
numb
er
not
yet
on
the
list.
Then
it
ads
the
numb
er
to
its
list,
and
r
ep
e
ats
the
pr
o
c
ess.
.
What
is
the
pr
ob
ability
that
the
se
c
ond
numb
er
adde
d
to
the
list
r
e
quir
e
d
mor
e
than
k
attempts?
.
What
is
the
pr
ob
ability
that
the
last
numb
er
adde
d
to
the
list
r
e
quir
e
d
mor
e
than
k
attempts?
Another
imp
ortan
t
concept
is
the
conditional
indep
endence.
F
or
example,
man
y
ev
en
ts
in
the
past
and
in
the
future
are
dep
enden
t.
But
in
man
y
mathematical
mo
dels,
past
and
future
are
indep
enden
t
conditionally
on
the
presen
t
situation.
In
suc
h
a
mo
del
future
dep
ends
on
past
only
through
presen
t
ev
en
ts!
Denition
..
L
et
C
b
e
a
non-trivial
event.
Events
A;
B
ar
e
C
-c
onditional
ly
indep
en-
dent
if
Pr(A
\
B
jC
)
=
Pr(AjC
)
Pr(B
jC
).
0
T
rivial
ev
en
ts
are
those
with
probabilities
0,
or
.

..
INDEPENDENT
EVENTS

..
Random
v
ariables
The
general
concept
of
probabilit
y
space
uses
\abstract"
sets
to
represen
t
outcomes
of
an
exp
erimen
t.
But
man
y
examples
considered
so
far,
represen
ted
the
outcomes
in
n
umerical
terms.
Random
v
ariables
are
in
tro
duced
for
con
v
enien
t
description
of
exp
erimen
ts
with
n
u-
merical
outcomes.
(The
other
option
is
to
select


I
R
,
or


I
R
d
.)
If
w
e
w
an
t
to
run
computer
sim
ulations,
w
e
need
to
represen
t
ev
en
non-n
umerical
exp
erimen
ts
(lik
e
tossing
coins)
in
n
umerical
terms
an
yho
w.
Th
us
the
language
of
random
v
ariables
b
ecomes
the
natural
extension
of
elemen
tary
probabilit
y
theory
,
expressing
man
y
of
the
same
concepts
in
a
little
dieren
t
language.
A
random
v
ariable
is
the
n
umerical
quan
tit
y
assigned
to
ev
ery
outcome
of
the
ex-
p
erimen
t.
In
mathematical
terms,
random
v
ariable
is
a
function
X
:

!
I
R
with
the
prop
ert
y
that
sets
f!


:
X
(!
)

ag
are
ev
en
ts
in
M
for
all
a

I
R
.
Recall
that
the
last
conditions
means
that
w
e
ma
y
talk
ab
out
probabilities
of
ev
en
ts
f!


:
X
(!
)

ag.
Probabilities
for
a
one-dimensional
r.
v.
are
determined
b
y
the
cum
ulativ
e
distribution
function
F
(x)
=
Pr(X

x)
(:)
The
corresp
onding
tail
function
R
(x)
=

 F
(x)
=
Pr(X
>
x)
is
sometimes
called
the
r
eliability

function.
Cum
ulativ
e
distribution
function
can
b
e
used
to
express
probabilities
of
in
terv
als
Pr(a
<
X

b)
=
F
(b)
 F
(a).
Since
probabilit
y
is
con
tin
uous,
(.)
w
e
can
also
compute
Pr(X
=
a)
=
lim
b!a
+
Pr
(a
<
X

b)
=
F
(a
+
)
 F
(a).
The
righ
t
hand
side
limit
F
(a
+
)
exists,
as
F
is
a
non-decreasing
function.
Example
.	
Supp
ose
F
(x)
=
(
 e
 x
)
_
0.
Then
Pr
(jX
 j
<
)
=
e
 
 e
 
.
In
probabilit
y
theory
w
e
are
concerned
with
probabilities.
Random
v
ariables
that
ha
v
e
the
same
probabilities
are
therefore
considered
equiv
alen
t.
W
e
write
X

=
Y
to
denote
the
equalit
y
of
distributions,
ie.
Pr(X

U
)
=
Pr(Y

U
)
for
all
Bor
el
sets
U

I
R
(sa
y
,
all
in
terv
als
U
).
V
ector
v
alued
r.
v.
are
measurable
the
functions

!
I
R
d
.
In
the
v
ector
case
w
e
also
refer
to
X
=
(X

;
:
:
:
X
d
)
as
the
d-v
ariate,
or
m
ultiv
ariate,
random
v
ariable.
W
e
will
use
the
ordinary
notation
for
sums
and
inequalities
b
et
w
een
random
v
ariables.
There
is
ho
w
ev
er
a
w
ord
of
caution.
In
probabilit
y
theory
,
equalities
and
inequalities
b
et
w
een
random
v
ariables
are
in
terpreted
almost
surely
.
F
or
instance
X

Y
+

means
Pr(X

Y
+
)
=
;
the
latter
is
a
shortcut
that
w
e
use
for
the
expression
Pr
(f!


:
X
(!
)

Y
(!
)
+
g)
=
.
Problem
.
Show
that
F
(x)
=
Pr(X

x)
is
right
c
ontinuous:
lim
x!a
+
F
(x)
=
F
(a).
..
Binomial
trials
The
statistical
analysis
of
rep
eated
exp
erimen
ts
is
based
on
the
follo
wing.

This
terminology
arises
under
the
in
terpretation
that
X
represen
ts
failure
time.


CHAPTER
.
RANDOM
PHENOMENA
Theorem
..
Supp
ose
that
for
j

I
N
event
B
j
is
either
S
j
or
S
0
j
,
wher
e
events
fS
j
g
ar
e
indep
endent.
Then
fB
j
g
ar
e
indep
endent.
A
binomial
exp
erimen
t,
called
also
binomial
trials,
consists
of
the
sequence
of
simpler
iden
tical
exp
erimen
ts
that
ha
v
e
t
w
o
p
ossible
outcomes
eac
h.
The
indep
enden
t
ev
en
ts
S
j
represen
t
suc
c
esses
in
consecutiv
e
exp
erimen
ts.
W
e
assume
that
w
e
ha
v
e
an
innite
sequence
of
ev
en
ts
S

;
S

;
:
:
:
S
k
;
:
:
:
that
are
indep
enden
t
and
ha
v
e
the
same
probabilit
y
p
=
Pr(S
j
).
W
e
denote
b
y
F
j
=
S
0
j
the
failure
in
the
j
-th
exp
erimen
t,
and
put
q
=

 p.
Tw
o
imp
ortan
t
random
v
ariables
are
asso
ciated
with
the
binomial
exp
erimen
t
are
the
n
um
b
er
X
of
successes
in
n
trials,
and
the
n
um
b
er
T
of
trials
un
til
rst
success.
Example
.0
The
pr
ob
ability
that
numb
er
X
of
suc
c
esses
in
n
trials
is
k
is
Pr(X
=
k
)
=
(
n
k
)p
k
q
n k
.
(Her
e
k
=
0;
:
:
:
;
n.)
Example
.
The
pr
ob
ability
of
mor
e
than
n
attempts
ne
e
de
d
for
the
rst
suc
c
ess
is
Pr(T
>
n)
=
q
n
.
The
pr
ob
ability
that
rst
suc
c
ess
o
c
curs
at
the
n-th
trial
is
Pr
(T
=
n)
=
pq
n 
(ge
ometric).
Example
.
Ge
ometric
distribution
has
lac
k
of
memory
pr
op
erty:
Pr(T
>
n
+
k
jT
>
n)
=
Pr
(T
>
k
).
Random
v
ariables
are
often
describ
ed
solely
in
terms
of
cum
ulativ
e
distribution
function
F
(x),
or
form
ulas
for
Pr
(X
=
x)
without
reference
to
the
underlying
probabilit
y
space

.
F
or
instance,
the
n
um
b
er
of
min
utes
T
that
w
e
sp
end
w
aiting
for
a
bird
to
come
to
the
bird
feeder
at
the
bac
k
of
m
y
house
is
random,
and
I
b
eliev
e
Pr(T
=
n)
=
pq
n 
b
ecause
Pr(T
>
n
+
k
jT
>
n)
=
Pr
(T
>
k
).
It
is
in
tuitiv
ely
ob
vious
that
on
a
v
erage
w
e
get
np
successes
in
n
trials.
It
is
p
erhaps
less
ob
vious

that
on
a
v
erage
w
e
need
=p
trials
to
get
the
rst
success.
Exercise
.
Write
a
simulation
pr
o
gr
am
to
verify
the
claims
ab
out
the
aver
ages
for
sever
al
values
of
p.
Example
.
The
pr
ob
ability
that
in
n
tosses
of
a
fair
c
oin,
half
ar
e
he
ads
is
(n)!

n
(n!)



p

n
!
0
as
n
!
.
The
latter
isn
't
e
asy
to
pr
ove,
but
the
c
omputer
printout
is
quite
c
onvincing,
se
e
T
able
.
(note
that

p


0:).
n
Pr(X
=
n)
F
requency
in
000
trials
p
n
Pr
(X
=
n)
00
0.0		
0.000
0.
00
0.00
0.000
0.
00
0.0
0.000
0.	
00
0.00
0.000
0.		
T
able
.:
Probabilities
Pr
(X
=
n)
in
n
Binomial
trials.

A
p
ossible
heuristic
argumen
t
ma
y
argue
that
T
p
is
on
a
v
erage
.

..
FUR
THER
NOTES
ABOUT
SIMULA
TIONS
	
.
F
urther
notes
ab
out
sim
ulations
By
no
w
y
ou
should
ha
v
e
written
some
simple
sim
ulation
programs,
and
prin
ted
out
the
results.
It
is
p
erhaps
a
go
o
d
momen
t
to
pause
and
consider
what
are
the
asp
ects
of
sim
ulations
that
w
e
are
in
terested
in.
In
general,
w
e
w
ould
lik
e
to
get
answ
ers
to
questions
that
w
e
don't
kno
w
ho
w
to
answ
er
in
an
y
other
w
a
y
.
But
b
efore
w
e
do
that,
w
e
should
dev
elop
some
in
tuition
on
the
cases
that
can
c
hec
k
the
answ
ers.
Therefore
w
e
b
egin
with
sim
ulation
of
probabilities
or
a
v
erages
that
are
kno
wn.
A
Sim
ulation
of
probabilities/
a
v
erages
that
are
kno
wn
should
address
the
follo
wing
questions.
.
Ho
w
close
the
sim
ulation
answ
ers
are
to
the
theoretical
answ
ers?
Prin
t
them
side-b
y-side.
.
Ho
w
large
the
sim
ulation
should
b
e?
Is
it
w
orth
to
c
hange
sim
ulation
size
from
,000
to
0,000
trials?
In
order
to
answ
er
this
question,
y
our
sim
ulation
has
to
pro
vide
\relativ
e"
rather
than
absolute
answ
ers.
(Answ
ers
of
the
form
\got

heads"
are
meaningless
as
they
dep
end
on
sim
ulation
size!)
.
Ho
w
do
the
answ
ers
c
hange
as
w
e
c
hange
the
parameters?
If
y
ou
did
a
sim
u-
lation
of
the
fair
coin,
y
ou
could
c
hange
the
probabilit
y
p
from
the
usual
v
alue


.
B
The
next
natural
step
is
to
extend
mo
dels
that
w
e
kno
w
ho
w
to
handle
b
oth
theo-
retically
and
b
y
sim
ulations
to
co
v
er
asp
ects
that
aren't
easily
accessible
b
y
theory
.
The
sample
questions
in
v
olv
e
.
Ho
w
w
ould
the
answ
ers
c
hange,
if
w
e
allo
w
p
erhaps
more
realistic
assumptions
in
the
mo
del?
As
an
example,
supp
ose
that
w
e
w
ould
lik
e
to
mo
del
the
birthda
y
problem
with
p
eople
b
orn
non-uniformly
throughout
the
y
ear.
Whic
h
w
a
y
w
ould
y
ou
exp
ect
the
answ
er
to
c
hange?
.
What
are
t
ypical
errors
of
a
sim
ulation
of
size
n?
Ho
w
can
w
e
estimate
the
accuracy
of
the
answ
er
without
ha
ving
the
exact
answ
er
to
compare
it
to?
Chapter

giv
es
theoretical
basis
for
suc
h
estimates.
.
Questions
Problem
.	
(Exercise)
A
family
has
two
childr
en,
and
one
of
them
is
a
b
oy.
What
is
the
pr
ob
ability
that
they
have
two
b
oys?
(If
you
think
this
is
to
o
har
d
mathematics,
do
it
as
a
c
omputer
assignment!)
ANS:
/
Problem
.0
(Exercise)
A
die
is
thr
own
until
an
ac
e
turns
up.
Assuming
the
ac
e
do
esn
't
turn
up
on
the
rst
thr
ow,
what
is
the
pr
ob
ability
that
mor
e
than
thr
e
e
thr
ows
(ie.
at
le
ast
four)
wil
l
b
e
ne
c
essary?
ANS:
	/	
Supp
ose
that
an
ac
e
turns
up
on
an
even
thr
ow.
What
is
the
pr
ob
ability
that
it
turne
d
up
on
the
se
c
ond
thr
ow?
(If
you
think
this
is
to
o
har
d
mathematics,
do
it
as
a
c
omputer
assignment!)

0
CHAPTER
.
RANDOM
PHENOMENA
Pro
ject
.
A
de
ck
of

c
ar
ds
has

suits
and

values
p
er
suit.
.
Write
a
pr
o
gr
am
simulating
the
hand
of

c
ar
ds.
.
Use
your
pr
o
gr
am
to
answer
the
fol
lowing
questions:
(a)
How
often
do
es
a
p
air
o
c
cur?
(b)
How
often
do
es
a
two-p
air
o
c
cur?
(c)
How
often
do
es
a
three
of
a
kind
(thr
e
e
of
same
value
and
two
dier
ent)
o
c
cur?
(d)
(e)
How
often
do
es
a
four
of
a
kind
o
c
cur?
(f
)
How
often
do
es
a
full
house
(+)
o
c
cur?
(g)
How
often
do
es
a
straigh
t
(ve
c
ar
ds
in
a
r
ow,
not
al
l
same
suit)
o
c
cur?
(h)
How
often
do
es
a
ush
(ve
c
ar
ds
of
one
suit,
not
in
or
der)
o
c
cur?
(i)
How
often
do
es
a
straigh
t
ush
(ve
c
ar
ds
in
a
r
ow
al
l
same
suit)
o
c
cur?
If
you
think
this
is
to
o
dicult
on
a
c
omputer,
c
ompute
the
pr
ob
abilities
by
hand.
Exercise
.
A
math
te
acher
in
a
c
ertain
scho
ol
likes
to
give
multiple
choic
e
tests,
gr
ade
them
as
either
right,
or
wr
ong,
and
then
lets
the
students
to
go
over
the
test
and
c
orr
e
ct
the
ones
they
got
wr
ong.
This
gives
them
two
chanc
es
to
get
a
pr
oblem
right,
and
the
chanc
e
of
getting
a
question
right
incr
e
ases
even
if
the
student
just
guesses
the
answers.
Supp
ose
a
student
simply
guesse
d
the
rst
time,
got
the
c
orr
e
ctions,
and
guesse
d
dier
ently
the
se
c
ond
time
on
the
wr
ong
answers.
How
much
his
gr
ade
impr
oves?
Pro
ject
.
This
is
the
exp
ande
d
version
of
Exer
cise
..
In
a
gr
oup
of
n
p
e
ople,
how
often
at
le
ast
two
have
the
same
birthday?
.
Find
the
formula
for
the
pr
ob
ability
p(n)
assuming

days
p
er
ye
ar,
and
e
qual
ly
likely
birthdays.
.
Compute
the
pr
ob
abilities
for
n
=
0;
0;
0;
0
.
Write
a
simulation
pr
o
gr
am,
and
verify
if
the
simulation
agr
e
es
with
the
the
or
etic
al
answers.
.
Mo
dify
the
simulation
pr
o
gr
am
to
al
low
for
not
e
qual
birthdays.
Assume
January,
F
ebruary,
Mar
ch
ar
e
less
likely
then
the
other
days
of
the
ye
ar.
Change
the
p
ar
am-
eters,
and
verify
how
the
pr
ob
abilities
p(n)
change
as
you
dep
art
fr
om
the
uniform
pr
ob
abilities.
If
the
change
of
p(n)
is
of
the
magnitude
c
omp
ar
able
to
the
simulation
ac
cur
acy,
cle
arly
it
is
irr
elevant.
.
A
r
andomly
sele
cte
d
p
erson
has
chanc
e
/
to
b
e
b
orn
on
a
le
ap
ye
ar.
How
do
es
this
ae
ct
the
answers?

Chapter

Random
v
ariables
(con
tin
ued)
T
r
e
e
sp
e
cies
ar
e
not
distribute
d
at
r
andom
but
ar
e
asso
ciate
d
with
sp
e
cial
habi-
tats.
The
Aub
orn
So
ciet
y
Field
Guide
to
North
American
T
rees.
In
tuitiv
ely
,
random
v
ariables
are
n
umerical
quan
tities
measured
in
an
exp
erimen
t.
The
concept

is
the
core
of
probabilit
y
theory;
it
leads
outside
of
elemen
tary
probabilit
y
and
it
touc
hes
adv
anced
concepts
of
in
tegration,
function
transforms
and
w
eak
limits.
F
or
con
v
enience
random
v
ariables
are
split
in
to
three
groups:
con
tin
uous,
discrete,
and
the
rest.
.
Discrete
r.
v.
Denition
..
X
is
a
discr
ete
r.
v.
if
X
(
)
is
c
ountable.
The
denition
sa
ys
that
X
is
a
discrete
r.
v.
if
there
is
a
nite,
or
coun
table
set
V
of
n
um
b
ers
(v
alues)
of
X
suc
h
that
p
v
=
Pr(X
=
v
)
>
0
and
P
v
V
p
v
=
.
The
function
f
(v
)
=
p
v
is
then
called
the
probabilit
y
mass
function
of
X
.
F
or
completeness,
the
domain
of
the
probabilit
y
mass
function
is
often
extended
to
all
x

I
R
(or
to
x

I
R
d
in
the
m
ultiv
ariate
case)
b
y
f
(x)
=
Pr(X
=
x).
It
is
easy
to
see
that
if
f
is
a
function
whic
h
satises
t
w
o
natural
conditions:
f
(x)

0
(.)
X
xI
R
f
(x)
=

(.)
(.)
then
there
is
a
probabilit
y
space
with
a
random
v
ariable
X
suc
h
that
f
is
its
probabilit
y
mass
function.
In
mo
deling
random
phenomena
w
e
can
therefore
a
v
oid
the
diculties
of
designing
appropriate
sample
spaces,
and
pic
k
directly
relev
an
t
densities.
The
question,
if
a
densit
y
do
es
describ
e
the
actual
outcomes
of
exp
erimen
t
is
to
some
extend
the
question
of
statistics.
Prop
erties
of
v
arious
distributions,
lik
e
lac
k-of-memory
come
also
handy
when
selecting
appropriate
densit
y
function.

The
precise
denition
is
in
Section
...



CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
F
or
discrete
r.
v.
the
cum
ulativ
e
distribution
function
(.)
pla
ys
lesser
role.
It
is
a
discon
tin
uous
function
giv
en
b
y
the
expression
F
(x)
=
X
v
x
p
v
:
(:)
This
expression
do
es
sho
w
up
in
the
\generic
sim
ulation
metho
d
in
Section
...
..
Examples
of
discrete
r.
v.
T
able
.
list
the
most
frequen
tly
encoun
tered
discrete
distributions.
Name
V
alues
Probabilities
Sym
b
ol
P
arameters
Binomial
0;
:
:
:
;
n
Pr(X
=
k
)
=
(
n
k
)p
k
(
 p)
n k
Bin(n,p)
0

p

;
n

I
N
P
oisson
Z
Z
+
Pr(X
=
k
)
=
e
 

k
k
!
P
oiss()

>
0
Geometric
Z
Z
+
Pr(X
=
k
)
=
p(
 p)
k
 
0

p


Uniform
fx

;
:
:
:
;
x
k
g
Pr(X
=
x
j
)
=

k
k

I
N
;
x

;
:
:
:
;
x
k

I
R
Hyp
ergeometric
Negativ
e
Binomial
T
able
.:
Discrete
random
v
ariables.
Example
.
L
et
the
r
andom
variable
X
denote
the
numb
er
of
he
ads
in
thr
e
e
tosses
of
a
fair
c
oin.
Example
.
L
et
the
r
andom
variable
X
denote
the
sc
or
e
of
a
r
andomly
sele
cte
d
student
on
the
nal
exam.
Problem
.
L
et
N
b
e
P
oiss(),
and
assume
N
b
al
ls
ar
e
plac
e
d
at
r
andom
into
n
b
oxes.
Find
the
pr
ob
ability
that
exactly
m
of
the
b
oxes
ar
e
empty.
ANS:
(
n
m
)e
 m=n
(
 e
=n
)
n m
.
..
Sim
ulations
of
discrete
r.
v.
Discrete
random
v
ariables
with
nite
n
um
b
er
of
v
alues
are
sim
ulated
b
y
assigning
v
alues
according
to
the
ranges
tak
en
b
y
the
(pseudo)random
uniform
random
v
ariable
U
from
the
random
n
um
b
er
generator,
U=Rand().
T
o
decide
whic
h
v
alue
of
X
should
b
e
generated,
tak
e
a
partition
f0
=
a
0

a


:
:
:

a
n 

a
n
:
:
:

g
of
in
terv
al
(0;
).
This
means
that
w
e
sim
ulate
X
=
f
(U
)
using
a
piecewise
constan
t
function
f
on
the
in
terv
al
(0;
).
If
f
(x)
=
v
k
for
x

(a
k
;
a
k
+
),
then
Pr
(X
=
v
k
)
=
a
k
+
 a
k
.
Therefore
w
e
c
ho
ose
a

=
0,
a

=
p

;
:
:
:
;
a
k
+
=
p

+
:
:
:
+
p
k
.
Notice
that
a
k
=
Pr(X

v
k
)
=
F
(v
k
).
Other
metho
ds
are
also
a
v
ailable
for
the
distributions
from
T
able
..
F
or
example,
program
TOSSCOIN.BAS
on
page

sim
ulates
binomial
distribution
Bin(n=00,
p=/).
The
follo
wing
exercise
pro
vides
to
ols
to
run
more
in
v
olv
ed
sim
ulations.
Exercise
.
Write
functions
SimOneBin(n,p)
and
SimOneGeom(p)
that
wil
l
simulate
a
single
o
c
curr
enc
e
of
the
B
in(n;
p)
r.
v.
and
the
ge
ometric
r.
v.
The
sample
usage:
PRINT
SimOneBin(,.)
should
simulate
the
numb
er
of
he
ads
in
tossing

fair
c
oins.
A
lso
write
function
SimGeneric(p())
which
simulates
generic
r.
v.
with
values
f0;
;
:
:
:
;
ng
and
pr
escrib
e
d
pr
ob
abilities
p
k
=
p(k
).

..
CONTINUOUS
R.
V.

.
Con
tin
uous
r.
v.
Con
tin
uous
random
v
ariables
ha
v
e
uncoun
table
sets
of
v
alues,
and
the
probabilit
y
of
eac
h
of
them
is
zero,
Pr
(X
=
x)
=
0
for
all
x

I
R
.
Since
probabilit
y
satises
con
tin
uit
y
axiom
(.),
Pr(X

(a;
a
+
h))
!
0
as
h
!
0
for
all
a.
The
main
in
terest
in
con
tin
uous
case
is
that
the
rate
of
con
v
ergence
to
0
is
also
kno
wn,
Pr(X

(a;
a
+
h))

f
(a)h
+
o(h).
F
unction
f
(x)
in
this
expansion
is
called
the
densit
y
function.
In
terms
of
the
cum
ulativ
e
distribution
function
.),
the
probabilit
y
is
Pr
(X

(a;
a
+
h))
=
F
(a
+
h)
 F
(a),
and
th
us
f
(a)
=
lim
h!0
F
(a+h) F
(a)
h
=
F
0
(a)
is
the
deriv
ativ
e
of
the
cum
ulativ
e
distribution
function
F
.
Therefore
when
the
F
undamen
tal
Theorem
of
Calculus
can
b
e
in
v
ok
ed
(sa
y
,
when
f
is
piecewise
con
tin
uous)
F
(x)
=
Z
x
 
f
(u)
du:
(:)
Denition
..
R
andom
variable
X
is
(absolutely)
c
ontinuous,
if
ther
e
is
a
function
f
such
that
Pr
(X

U
)
=
R
U
f
(x)dx.
F
unction
f
is
c
al
le
d
the
pr
ob
ability
density
function
of
X
.
It
is
kno
wn
that
if
f
is
a
function
whic
h
satises
t
w
o
natural
conditions:
f
(x)

0
(.)
Z
I
R
f
(x)
dx
=

(.)
(.)
then
there
is
a
probabilit
y
space
with
a
random
v
ariable
X
suc
h
that
f
is
its
densit
y
.
This
is
in
complete
analogy
with
the
discrete
case.
In
mo
deling
random
phenomena
w
e
can
therefore
a
v
oid
the
diculties
of
designing
appropriate
sample
spaces,
and
pic
k
directly
relev
an
t
densities.
The
question,
if
a
densit
y
do
es
describ
e
the
actual
outcomes
of
exp
erimen
t
is
to
some
extend
the
question
of
statistics.
Prop
erties
of
v
arious
distributions
come
also
handy
when
selecting
appropriate
densit
y
function.
It
is
con
v
enien
t
to
use
the
heuristic
probabilit
y
densit
y
function
in
con
tin
uous
case
corresp
onds
to
probabilit
y
mass
function
in
discrete
case,
and
that
expressions
that
in
v
olv
e
in
discrete
case
sums
are
replaced
b
y
in
tegrals,
compare
(.)
and
(.).
..
Examples
of
con
tin
uous
r.
v.
The
follo
wing
table
lists
more
often
encoun
tered
densities.
Figures
.
and
.
giv
e
the
graphs
of
the
normal
and
exp
onen
tial
densities.
Example
.
A
dart
is
thr
own
at
a
cir
cular
dart
b
o
ar
d
of
r
adius
.
L
et
X
denote
the
distanc
e
of
the
dart
fr
om
the
c
enter.
Assuming
uniform
assignment
of
pr
ob
ability
(.),
the
density
of
X
is
f
(x)
=
(
x

if
0

x


0
otherwise
Problem
.
R
eferring
to
Exer
cise
.,
let
X
;
Y
denote
the
arrival
times
of
the
two
drivers
at
the
interse
ction.
Find
the
density
of
the
time
lapse
jX
 Y
j
b
etwe
en
their
arrivals.


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
Name
Range
Densit
y
Sym
b
ol
P
arameters
Normal
 
<
x
<

f
(x)
=

p


exp
 (x )



N(;

)


I
R;

>
0
Exp
onen
tial
x
>
0
f
(x)
=
e
 x

>
0
Uniform
a
<
x
<
b
f
(x)
=

b a
U(a,b)
a
<
b
real
Gamma
x
>
0
f
(x)
=
C
x
 
e
 x=
Gamma(
;

)

>
0;

>
0;
C
=



 ()
W
eibull
T
able
.:
Con
tin
uous
random
v
ariables.
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
x
f
(x)

-
=
p


0:		
-

Figure
.:
Graph
of
the
standard
normal
N
(0;
)
densit
y
f
(x)
=
(
)
 =
e
 x

=
.
..
Histograms
Sim
ulations
and
exp
erimen
ts
do
not
giv
e
direct
access
to
the
densit
y
,
but
often
a
histogram
will
appro
ximate
it
reasonably
w
ell.
Histograms
are
graphical
represen
tations
of
empirical
data.
A
sample
histogram
is
dra
wn
on
the
side
of
the
square
in
Figure
.
on
page
.
T
o
create
a
useful
histogram,
split
the
range
in
to
the
nite
n
um
b
er
of
in
terv
als.
Then
graph
o
v
er
eac
h
in
terv
al
the
rectangle
with
the
area
equal
to
the
observ
ed
frequency
.
The
n
um
b
er,
and
p
ositioning
of
in
terv
als
dep
ends
on
the
amoun
t
of
data
a
v
ailable,
and
p
ersonal
preference.
..
Sim
ulations
of
con
tin
uous
r.
v.
The
generic
metho
d
for
sim
ulating
a
con
tin
uous
random
v
ariable
is
similar
to
the
metho
d
used
in
the
discrete
case.
Namely
,
w
e
tak
e
X
=
f
(U
)
with
the
suitable
function
f
.
T
o
nd
f
assume
it
is
increasing
and
th
us
in
v
ertible
with
in
v
erse
g
.
Then
Pr(f
(U
)
<
x)
=
Pr(U
<
g
(x))
=
g
(x).
This
completes
the
generic
prescription:
tak
e
as
f
(x)
the
in
v
erse

of
the
cum
ulativ
e
distribution
functions
F
(x)
=
Pr(X

x).

Actually
,
w
e
need
only
a
righ
t-in
v
erse,
i.e
a
function
suc
h
that
F
(f
(u))
=
u.

..
EXPECTED
V
ALUES

`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
x
f
(x)

-
.0
.0
Figure
.:
Graph
of
the
exp
onen
tial
densit
y
f
(x)
=
e
 x
as
the
function
of
x
>
0.
This
metho
d
of
sim
ulation
is
quite
eectiv
e
if
the
in
v
erse
of
F
can
b
e
found
analytically
.
It
b
ecomes
slo
w
when
the
in
v
erse
(or,
w
orse
still,
cum
ulativ
e
distribution
function
F
itself
)
is
computed
b
y
n
umerical
pro
cedures.
Since
this
is
the
case
of
the
normal
distribution,
sp
ecial
metho
ds
are
used
to
sim
ulate
the
normal
distribution.
Example
.
T
o
simulate
X
which
is
exp
onential
with
p
ar
ameter
,
use
X
=
 

ln
U
.
.
Exp
ected
v
alues
Exp
ected
v
alues
are
p
erhaps
the
single
most
imp
ortan
t
n
umerical
c
haracterization
of
a
random
phenomenon.
Denition
..
F
or
discr
ete
r
andom
variable
X
the
exp
e
cte
d
value
E
X
is
given
by
E
X
=
P
v
v
Pr
(X
=
v
),
pr
ovide
d
the
series
c
onver
ges.
Exp
ected
v
alue
captures
the
in
tuition
of
the
a
v
erage
of
a
random
quan
tit
y
.
It
is
also
this
in
tuition
that
leads
to
estimating
the
exp
ected
v
alue
b
y
a
v
eraging
the
outcomes
of
sim
ulations.
Example
.
If
X
has
values
x

;
:
:
:
;
x
n
with
e
qual
pr
ob
ability,
then
E
X
=

x
is
the
arithmetic
me
an
of
x

;
:
:
:
;
x
n
.
Sim
ulationsare
oten
used
to
get
answ
ers
that
are
to
o
dicult
to
nd
analytically
.
The
follo
wing
exercise
can
b
e
answ
ered
b
y
sim
ulation
if
y
ou
gure
out
ho
w
to
sh
ue
cards
from
a
dec
k
at
random
(Exercise
.	).
Exercise
.
What
is
the
exp
e
cte
d
numb
er
of
c
ar
ds
which
must
b
e
turne
d
over
in
or
der
to
se
e
e
ach
of
one
suit.
Example
.
If
X
takes
two
values
a
<
b
and
Pr
(X
=
a)
=
p,
then
E
X
=
pa
+
(
 p)b
is
the
numb
er
in
the
close
d
interval
[a;
b].


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
Denition
..
F
or
c
ontinuous
r
andom
variable
X
the
exp
e
cte
d
value
E
X
is
given
by
E
X
=
R
I
R
xf
(x)
dx,
pr
ovide
d
the
inte
gr
al
c
onver
ges.
Name
Probabilit
y
distribution
E
X
Normal
f
(x)
=

p


exp
 (x )




Exp
onen
tial
f
(x)
=
e
 x


>
0
Uniform
f
(x)
=

b a


(a
+
b)
real
Gamma
W
eibull
Binomial
Pr
(X
=
k
)
=
(
n
k
)p
k
(
 p)
n k
np
P
oisson
Pr
(X
=
k
)
=
e
 

k
k
!

Geometric
Pr(X
=
k
)
=
p(
 p)
k
 

p
Hyp
ergeometric
Negativ
e
Binomial
T
able
.:
Exp
ected
v
alues
of
some
random
v
ariables.
Problem
.
Compute
E
X
for
the
entries
in
T
able
..
Exercise
.
Simulate
E
X
for
the
entries
in
T
able
.
(exc
ept
normal)
for
dier
ent
values
of
p
ar
ameters
involve
d.
Problem
.
(Exercise)
R
eferring
to
Example
.,
what
is
the
aver
age
distanc
e
of
a
dart
fr
om
the
c
enter?
ANS:

.
If
y
ou
c
hose
to
do
the
sim
ulations,
y
ou
should
p
erhaps
notice
that
it
is
rather
dicult
to
decide
ho
w
man
y
sim
ulations
to
tak
e
in
order
to
ac
hiev
e
the
desired
accuracy
.
T
ypically
,
y
ou
need
to
increase
the
size
of
a
sim
ulation
four
times
to
half
the
error.
Another
p
oin
t
to
k
eep
in
mind
is
that
sim
ulations
do
return
n
um
b
ers.
But
from
n
um
b
ers
alone
it
is
diclut
to
see
ho
w
parameters
of
the
mo
del
c
hange
it,
and
this
is
a
more
in
teresting
question.
..
T
ail
in
tegration
form
ulas
The
follo
wing
tail
inte
gr
ation
formula
is
of
considerable
con
v
enience
in
theoretical
analysis.
Theorem
..
F
or
non-ne
gative
r
andom
variables,
b
oth
in
the
discr
ete,
and
in
the
c
on-
tinuous
c
ase
E
X
=
Z

0
Pr(X
>
t)
dt
(:	)
(The
exp
e
cte
d
value
is
nite
if
and
only
if
the
c
orr
esp
onding
inte
gr
al
c
onver
ges.)
Pro
of.
T
o
simplify
the
pro
of
and
exp
ose
the
main
idea
more
clearly
,
consider
the
dis-
crete
case
with
a
nite
n
um
b
er
of
v
alues.
Similarly
,
to
a
v
oid
tec
hnicalities
w
e
consider
con
tin
uous
case
with
b
ounded
range
only
.

..
EXPECTED
V
ALUES

Discrete
case:
E
X
=
x

p

+
x

p

+
:
:
:
+
x
n
p
n
=
x

(p

+
p

+
:
:
:
+
p
n
 (p

+
:
:
:
+
p
n
))
+
x

(p

+
:
:
:
+
p
n
 (p

+
:
:
:
+
p
n
))
+
:
:
:
+
x
n 
(p
n 
+
p
n
 p
n
)
+
x
n
p
n
=
x

(p

+
p

+
:
:
:
+
p
n
)
+
(x

 x

)(p

+
:
:
:
+
p
n
 (p

+
:
:
:
+
p
n
))
+
:
:
:
+
(x
n 
 x
n 
)(p
n 
+
p
n
)
+
(x
n
 x
n 
)p
n
The
latter
is
R
x
n
0
Pr
(X
:t)
dt.
Con
tin
uous
case:
Let
f
denote
the
densit
y
and
F
b
e
the
cum
ulativ
e
distribution
func-
tion.
In
tegrating
b
y
parts
E
X
=
R
b
0
xf
(x)
dx
=
b
 R
b
0
F
(x)
dx
=
R
b
0
(
 F
(x)
dx.

If
X
is
discrete
in
teger
v
alued
X

f0;
;
:
:
:
g,
then
(.	)
can
b
e
written
as
E
X
=

X
n=0
Pr(X
>
n):
(:0)
It
is
natural
to
dene
E
X
for
more
general
r.
v.
in
the
same
w
a
y
through
form
ula
(.	).
W
rite
X
=
X
+
 X
 to
decomp
ose
X
in
to
its
non-negativ
e,
and
non-p
ositiv
e
parts,
and
then
dene
E
X
=
R

0
P
(X
>
t)
dt
 R

0
P
(X
<
 t)
dt.
Clearly
,
if
one
of
the
in
tegrals
div
erges,
E
X
is
not
dened.
Example
.
Supp
ose
X
is
exp
onential
with
the
density
f
(x)
=
e
 x
.
L
et
Y
b
e
X
trun-
c
ate
d
at
level
.
That
is
Y
=
(
X
if
X



if
X


Cle
arly,
Y
is
not
c
ontinuous,
as
Pr
(Y
=
)
=
Pr
(X
>
)
=
e
 
>
0.
On
the
other
hand,
Y
is
not
discr
ete
as
it
takes
unc
ountable
numb
er
of
values;
in
fact
al
l
the
numb
ers
b
etwe
en
0
and

ar
e
p
ossible.
The
denitions
of
the
exp
e
cte
d
value
we
gave
do
not
apply,
but
(.	)
c
an
b
e
use
d
to
show
that
E
(Y
)
=

 e
 
.
Inde
e
d,
Pr(Y
>
t)
=
Pr(X
>
t)
=
e
 t
for
0
<
t
<
,
and
Pr(Y
>
t)
=
0
for
t
>
.
Example
.
T
o
determine
the
me
an
of
the
ge
ometric
distribution
we
c
an
either
c
ompute
the
sum
p
P

n=
n(
 p)
n 
,
or
use
(.	)
and
nd
the
e
asier
sum
P

n=0
(
 p)
n
.
..
Cheb
yshev-Mark
o
v
inequalit
y
The
follo
wing
inequalit
y
is
kno
wn
as
Mark
o
v's,
or
Cheb
yshev's
inequalit
y
.
Despite
its
simplicit
y
it
has
n
umerous
non-trivial
applications,
see
eg.
Theorem
...
Prop
osition
..
If
U

0
then
P
(U
>
t)

E
U
t
(:)
Indeed,
b
y
(.	)
w
e
ha
v
e
E
U
=
R

0
Pr(U
>
x)
dx

R
t
0
Pr
(U
>
x)
dx

tP
(jX
j
>
t).
Problem
.
Supp
ose
U
is
uniform
U
(0;
).
Then
Pr(U
>
t)


t
.
This
me
ans
that

 t
<

t
.
.
Is
the
ab
ove
ine
quality
\sharp"?
(Gr
aph
b
oth
curves).
.
Is
(.	)
sharp?
That
is,
given
t
>
0,
is
ther
e
an
X
>
0
such
that
e
quality
o
c
curs?


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
.
Exp
ected
v
alues
and
sim
ulations
Exp
ected
v
alue
E
X
is
appro
ximated
without
m
uc
h
dicult
y

on
a
computer
b
y
a
v
eraging
large
n
um
b
er
of
indep
endent
trials.
Example
.	
T
o
nd
the
aver
age
of
the
uniform
U
(0;
)
distribution,
take

n
(U

+
:
:
:
+
U
n
).
This
is
the
basis
of
Monte-Carlo
metho
ds
,
whic
h
is
a
family
of
related
probabilistic
metho
ds
for
computing
the
in
tegrals.
T
o
nd
R
b
a
f
(x)
dx
w
e
sim
ulate
X
j
=
f
(a
+
bU
j
).
The
a
v
erage

n
P
n
j
=
X
j
appro
ximates

b a
R
b
a
f
(u)
du
for
large
n.
The
v
ariance
of
the
n-th
appro
ximation
is
of
the
order
n
 =
,
whic
h
is
w
orse
than
the
tr
ap
ezoidal
rule
for
smo
oth
functions.
In
return
the
appro
ximation
is
insensitiv
e
to
the
smo
othness
of
the
in
tegrands,
and
also
to
the
dimension
of
the
in
tegral.
Mon
te
Carlo
metho
ds
can
b
e
used
eectiv
ely
for
m
ultiple
in
tegrals
o
v
er
irregular
domains.
Exercise
.	
Use
Monte
Carlo
metho
d
to
appr
oximate

=
R

 

p

 x

dx.
(Y
ou
may
want
to
c
omp
ar
e
the
output
with
numeric
al
pr
o
c
e
dur
es
describ
e
d
in
Se
ction
C..)
Another
metho
d
of
similar
nature
is
to
pic
k
p
oin
ts
(X
;
Y
)
at
random
from
the
rectangle
con
taining
the
graph
of
f
and
c
hec
k
if
Y
<
f
(X
)
holds.
The
prop
ortion
of
\successes"
appro
ximates
the
prop
ortion
of
the
area
under
the
graph
of
f
.
Exercise
.0
Appr
oximate

=
by
sele
cting
p
oints
(X
;
Y
)
at
r
andom
fr
om
the
unit
squar
e,
and
che
cking
if
X

+
Y

<
.
The
follo
wing
sample
program
computes
n
umerically
double
in
tegral
R
R
U
cos
(0x
+
0y
)
dxdy
o
v
er
a
circle
x

+
y

=
.
The
only
conceptual
dicult
y
as
compared
to
single
in
tegrals
is
ho
w
to
select
p
oin
ts
at
random
from
the
unit
disk.
This
is
done
b
y
pic
king
p
oin
ts
from
a
bigger
square
and
discarding
those
that
didn't
mak
e
it.
Can
y
ou
do
this
in
tegral
analytically?
Or
b
y
another
n
umerical
pro
cedure?
PROGRAM
dblint.bas
'
'declarations
DECLARE
FUNCTION
Integrand!
(X!,
Y!)
DECLARE
FUNCTION
InDomain!
(X!,
Y!)
CONST
True
=
-
'
simulation
loop
NumTrials
=
0000
FOR
j
=

TO
NumTrials
'select
random
points
from
the
square
[-,]x[-,]
X
=

*
RND()
-

Y
=

*
RND()
-

'check
if
this
is
in
the
domain

Pro
vided
that
limited
accuracy
is
admissible

..
EXPECTED
V
ALUES
AND
SIMULA
TIONS
	
IF
InDomain(X,
Y)
THEN
NumTested
=
NumTested
+

Sum
=
Sum
+
Integrand(X,
Y)
Var
=
Var
+
Integrand(X,
Y)
^

END
IF
NEXT
j
'Print
the
answer
PRINT
"Examined
";
NumTested;
"
random
points"
IF
NumTested
=
0
THEN
END
'nothing
found
N
=
NumTested
PRINT
"The
integral
is
approximately
";
Sum
/
N
PRINT
"With
	%
confidence
the
error
is
less
than
";
.	
*
SQR(Var
/
N
-
(Sum
/
N)
^
)
/
SQR
END
FUNCTION
InDomain
(X,
Y)
'This
function
checks
if
$x,y$
is
in
the
integration
domain
'The
definition
of
the
domain
can
be
easily
modified
here,
including
'more
complicated
domains
IF
X
^

+
Y
^

<

THEN
InDomain
=
True
END
FUNCTION
FUNCTION
Integrand
(X,
Y)
'This
is
the
function
to
be
integrated
Integrand
=
COS(0
*
X
+
0
*
Y)
'
END
FUNCTION
It
is
imp
ortan
t
to
ha
v
e
some
idea
ab
out
ho
w
accurate
the
answ
er
is.
The
program
uses
the
Cen
tral
Limit
Theorem,
see
Section
.,
to
estimate
the
magnitude
of
the
error.
A
less
sharp
(and
th
us
safer
to
use!)
error
estimate
can
b
e
obtained
from
(.)
Pro
ject
.
Ther
e
ar
e
two
natur
al
choic
es
to
estimate
by
simulation
events
of
smal
l
pr
ob
ability.
We
c
an
pick
a
lar
ge
sample
size
n,
run
the
simulation
exp
eriment
and
hop
eful
ly
get
sever
al
data
p
oints.
The
outc
ome
X
of
such
an
exp
eriment
is
a
binomial
r
andom
variable
with
unknown
pr
ob
ability
p
of
suc
c
ess,
and
we
would
estimate
p

X=n.
The
tr
ouble
is
that
if
we
don
't
know
how
smal
l
the
chanc
es
ar
e,
we
might
get
none,
estimating
pr
ob
ability
to
b
e
zer
o.
Or
we
c
ould
run
the
exp
eriment
until
we
get
the
pr
escrib
e
d
numb
er
of
suc
c
esses.
The
observation
would
then
c
onsist
of
a
set
of
ge
ometric
r
andom
variables
T

;
:
:
:
;
T
k
with
unknown
me
an
E
T
=
=p.
We
c
ould
then
estimate
p
=
=E
T
by
taking
the
inverse
of
the
arithmetic
me
an
of
T
j
.
In
this
appr
o
ach
we
ar
e
guar
ante
e
d
to
get
some
observations,
as
long
as
p
=
0.
The
question
is
Whic
h
of
the
metho
ds
w
ould
y
ou
recommend
to
use?
(Of
c
ourse,
you
would
r
e
c
ommend
a
b
etter
metho
d,
but
what
the
wor
d
\b
etter"
might
me
an
her
e?)

0
CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
.
Join
t
distributions
Often
an
exp
erimen
t
in
v
olv
es
measuring
t
w
o
or
more
random
n
um
b
ers,
sa
y
X
and
Y
.
The
fact
that
w
e
kno
w
the
distribution
of
X
,
and
the
distribution
of
Y
separately
do
esn't
determine
probabilities
of
ev
en
ts
that
in
v
olv
e
b
oth
X
and
Y
sim
ultaneously
.
Example
.0
Supp
ose
Pr(X
=
;
Y
=
)
=


+

(.)
Pr(X
=
 ;
Y
=
 )
=


+

Pr(X
=
 ;
Y
=
)
=


 
Pr(X
=
;
Y
=
 )
=


 
Then
Pr(X
=
)
=
Pr(Y
=
)
=


r
e
gar
d
less
of
the
value
of
.
On
the
other
hand,
Pr(X
=
Y
)
=


+

cle
arly
dep
ends
on
the
value
of
.
It
is
clear
that
if
X
is
discrete,
and
Y
is
discrete,
then
(X
;
Y
)
is
an
I
R

v
alued
discrete
r.
v.
That
is,
the
v
alues
of
the
pair
are
coun
table.
Probabilities
Pr(X
=
x;
Y
=
y
)
are
called
the
join
t
distribution.
Corresp
onding
Pr(X
=
x)
and
Pr
(Y
=
y
)
are
the
so
called
marginals.
Example
.0
p
oin
ts
out
that
marginals
do
not
determine
join
t
probabilities
uniquely
.
But
if
w
e
kno
w
the
join
t
probabilities
then
w
e
can
compute
the
marginals,
eg
Pr(X
=
x)
=
P
y
Pr
(X
=
x;
Y
=
y
).
In
con
trast
to
the
discrete
case,
join
t
con
tin
uit
y
can
not
b
e
recognized
from
the
con-
tin
uit
y
of
the
comp
onen
ts,
and
requires
full
denition.
Denition
..
L
et
X
=
(X

;
:
:
:
;
X
n
).
R
andom
variables
X

;
:
:
:
;
X
n
ar
e
jointly
(ab-
solutely)
c
ontinuous,
if
ther
e
is
a
function
f
such
that
Pr
(X

U
)
=
Z
:
:
:
Z
U
f
(x

;
:
:
:
x
n
)
dx

:
:
:
dx
n
for
al
l
me
asur
able
U
.
F
unction
f
is
then
c
al
le
d
the
pr
ob
ability
density
function
of
X.
Example
.
Supp
ose
X
;
Y
have
uniform
distribution
in
the
unit
disk.
Then
the
joint
density
is
f
(x;
y
)
=
=
for
x

+
y



and
the
density
of
X
is
f
X
(x)
=


p

 x

.
The
relation
b
et
w
een
probabilities
and
the
densit
y
is
f
(a;
b)
=
@

@
a@
b
Pr(X

a;
Y

b):
(:)
Occasionally
this
can
b
e
used
to
determine
the
join
t
densit
y
.

..
FUNCTIONS
OF
R.
V.

..
Indep
enden
t
random
v
ariables
Indep
endence
of
random
v
ariables
is
dened
in
terms
of
join
t
distributions.
The
in
tuition
b
ehind
this
denition
is
that
the
ev
en
ts
that
random
v
ariables
ma
y
generate
should
b
e
indep
enden
t.
Notice
ho
w
ev
er
that
the
actual
denition
is
simpler
than
Denition
...
Can
y
ou
explain
wh
y?
Denition
..
R
andom
variables
X

;
:
:
:
;
X
n
ar
e
indep
endent,
or
sto
chastic
al
ly
inde-
p
endent,
if
Pr(X


U

;
:
:
:
;
X
n

U
n
)
=
Pr(X


U

)
:
:
:
Pr(X
n

U
n
)
(:)
for
al
l
me
asur
able
U

;
:
:
:
;
U
n

I
R.
(Similarly
w
e
dene
the
sto
c
hastic
indep
endence
of
random
v
ectors).
W
e
sa
y
that
X

;
:
:
:
;
X
n
are
indep
endent
identic
al
ly
distribute
d
(i.
i.
d)
if
(.)
holds
and
Pr
(X
i

U
)
=
Pr
(X
j

U
)
for
all
Borel
U

I
R.
Prop
osition
..
If
X
;
Y
ar
e
discr
ete
with
the
pr
ob
ability
mass
function
f
(x;
y
),
then
indep
endenc
e
of
X
;
Y
is
e
quivalent
to
f
(x;
y
)
=
f
X
(x)f
Y
(y
).
If
X
;
Y
ar
e
c
ontinuous
with
the
joint
density
f
(x;
y
)
then
indep
endenc
e
of
X
;
Y
is
e
quivalent
to
f
(x;
y
)
=
f
X
(x)f
Y
(y
).
Indep
endence
is
often
part
of
the
mo
del.
Indep
endence
allo
ws
to
determine
join
t
distributions
from
marginals.
Th
us
eac
h
indep
enden
t
random
v
ariable
can
b
e
analyzed
separately
,
and
then
more
complex
questions
can
b
e
answ
ered.
F
rom
the
mathemat-
ical
p
ersp
ectiv
e,
under
indep
endence
w
e
can
determine
join
t
distributions
if
w
e
kno
w
marginals.
Example
.
Supp
ose
X
is
binomial
Bin(n,p)
and
Y
is
Poiss().
If
X
;
Y
ar
e
inde-
p
endent,
then
the
joint
pr
ob
ability
mass
function
of
X
;
Y
is
given
by
f
(x;
y
)
=
(
n
x
)p
n
(
 p)
n x
e
 

y
=y
!
(or
0).
Example
.
(Example
.
con
tin
ued)
Two
drivers
arrive
at
an
interse
ction
b
e-
twe
en
:00
and
:0
every
day.
Their
arrival
times
ar
e
indep
endent
r
andom
variables.
Inde
e
d,
using
formula
(.)
and
elementary
ar
e
a
c
omputation,
Pr
(X
<
a;
Y
<
b)
=
ab
for
0
<
a;
b
<
.
.
F
unctions
of
r.
v.
Some
random
v
ariables
are
obtained
b
y
taking
functions
of
another
ones,
p
ossibly
m
ulti-
dimensional.
In
the
notes
w
e
often
limit
our
atten
tion
to
a
single
random
v
ariable
Z
giv
en
b
y
a
function
Z
=
(X
;
Y
)
of
t
w
o
argumen
ts;
this
is
con
v
enien
t
for
notation
and
exhibits
most
of
the
in
teresting
tec
hniques.
Sums
and
linear
com
binations,
medians,
maxima,
and
minima
are
p
erhaps
the
most
often
encoun
tered
functions
of
m
ultidimensional
random
v
ariables.
Metho
ds
to
compute
the
distribution,
or
the
exp
ected
v
alue
of
suc
h
a
function
are
of
considerable
practical
signicance.


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
.
Momen
ts
of
functions
If
Z
=
(X
;
Y
)
has
the
exp
ected
v
alue,
then
E
Z
can
b
e
computed
directly
without
computing
the
densit
y
,
or
probabilit
y
mass
function
of
Z
.
The
follo
wing
iden
tit
y
is
useful.
If
X
;
Y
are
discrete
and
E
Z
exists,
then
E
(X
;
Y
)
=
X
x;y
(x;
y
)
Pr
(X
=
x;
Y
=
y
):
(:)
If
X
;
Y
are
join
tly
con
tin
uous
then
Z
migh
t
b
e
con
tin
uous,
discrete,
or
sa
y
of
mixed
t
yp
e.
Regardless
of
the
case
E
(X
;
Y
)
=
Z
Z
I
R

(x;
y
)f
(x;
y
)
dxdy
;
(:)
and
the
double
in
tegral
con
v
erges
if
E
Z
exists.
Con
v
ersely
,
if
the
in
tegral
con
v
erges,
then
E
Z
exists
and
is
giv
en
b
y
form
ula
(.).
In
particular
the
exp
ected
v
alue
is
linear
E
(aX
+
bY
+
c)
=
aE
X
+
bE
Y
+
c:
(:)
This
can
b
e
easily
v
eried
using
(.),
but
the
iden
tit
y
(.)
is
b
ey
ond
the
scop
e
of
this
notes,
as
w
e
do
not
w
an
t
to
dw
ell
on
the
general
denition
of
E
Z
that
w
ould
encompass
all
cases.
The
fact
that
exp
ected
v
alue
is
linear
pro
vides
a
simple
metho
d
of
computing
some
otherwise
dicult
sums.
Example
.
Supp
ose
X
is
Binomial
B
in(n;
p).
Then
X
=
X

+
:
:
:
+
X
n
,
wher
e
X
j
is
the
numb
er
of
suc
c
esses
in
j
-th
trial.
Cle
arly
e
ach
X
j
is
0,
or
,
and
E
X
j
=
p.
Exercise
.
(Example
.
con
tin
ued)
Two
drivers
arrive
at
an
interse
ction
b
e-
twe
en
:00
and
:0
every
day.
On
aver
age
how
much
time
lapses
b
etwe
en
their
arrivals?
Denition
..
L
et
m
=
E
X
.
The
varianc
e
V
ar
(X
)
is
dene
d
as
V
ar
(X
)
=
E
(X
 m)

.
Notice
that
V
ar
(aX
+
b)
=
a

V
ar
(X
):
(:)
In
particular,
V
ar
(X
)
=
0
when
X
=
const.
Sometimes
a
more
con
v
enien
t
expression
for
the
v
ariance
is
V
ar
(X
)
=
E
X

 (E
X
)

=
E
(X
 E
X
)

:
The
standard
deviation
is

=
q
V
ar
(X
).
Problem
.
Compute
varianc
es
V
ar
(X
)
for
the
entries
in
T
able
..
(Some
of
these
ar
e
a
r
e
al
chal
lenge
to
your
c
omputational
skil
ls,
so
you
may
safely
give
up.
A
nother
metho
d
wil
l
make
it
e
asier
in
Chapter
).
Since
V
ar
(X
)
=
E
X

 (E
X
)


0,
the
follo
wing
inequalit
y
follo
ws
(E
X
)


E
X

(:	)

..
MOMENTS
OF
FUNCTIONS

Name
Probabilit
y
distribution
V
ar
(X
)
Normal
f
(x)
=

p


exp
 (x )





Exp
onen
tial
f
(x)
=
e
 x



Uniform
f
(x)
=

b a


(b
 a)

Gamma
W
eibull
Binomial
Pr(X
=
k
)
=
(
n
k
)p
k
(
 p)
n k
np(
 p)
P
oisson
Pr(X
=
k
)
=
e
 

k
k
!

Geometric
Pr(X
=
k
)
=
p(
 p)
k
 

p

Hyp
ergeometric
Negativ
e
Binomial
T
able
.:
V
ariances
of
some
random
v
ariables.
Denition
..
The
c
ovarianc
e
of
r
andom
variables
X
;
Y
with
exp
e
cte
d
values
m
X
;
m
Y
is
dene
d
as
cov
(X
;
Y
)
=
E
(X
 m
X
)(Y
 m
Y
).
Clearly
C
ov
(X
;
X
)
=
V
ar
(X
)
and
V
ar
(X
+
Y
)
=
V
ar
(X
)
+
V
ar
(Y
)
+
cov
(X
;
Y
).
Theorem
..
If
X
;
Y
ar
e
indep
endent,
then
V
ar
(X
+
Y
)
=
V
ar
(X
)
+
V
ar
(Y
).
Example
.
let
X

;
X

;
:
:
:
;
X
n
b
e
indep
endent
f0;
g-value
d
r
andom
variables,
and
supp
ose
that
Pr(X
j
=
)
=
p.
Then
V
ar
(
P
n
j
=
X
j
)
=
np(
 p).
What
is
the
distribution
of
P
n
j
=
X
j
?
T
ail
in
tegration
form
ula
revisited
Example
.
If
X
>
0
then
E
e
X
=

+
R

0
e
t
P
(X
>
t)
dt
Problem
.
Show
that
E
X

=
R

0
tP
(jX
j
>
t)
dt.
Gener
alize
this
formula
to
E
jX
j
p
.
Cheb
yshev-Mark
o
v
inequalit
y
Sp
ecial
cases
of
Cheb
yshev's
inequalit
y
(.)
are:
Pr
(jX
j
>
t)
<

t
E
jX
j
(:0)
Pr(jX
 j
>
t)
<

t

V
ar
(X
)
(:)
Pr
(jX
j
>
t)
<
e
 at
E
e
aX
(:)
Cheb
yshev's
inequalit
y
is
one
reason
w
e
often
striv
e
for
small
a
v
erage
quadratic
error.
If
E
jX
 X
0
j

<

then
w
e
can
b
e
sure
that
Pr
(jX
 X
0
j
>

p
)
<

p
.
The
follo
wing
ma
y
b
e
used
(with
some
caution)
in
computer
programs
to
asses
error
in
estimating
probabilities
b
y
sampling.


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
Example
.
If
X
is
B
in(n;
p)
then
Pr(j
X
n
 pj
>


p
n
)




p
n
Exercise
.
R
un
a
simulation
of
the
event
that
you
know
pr
ob
ability
of,
printing
out
the
err
or
estimate.
Do
the
same
for
the
event
that
you
don
't
know
the
pr
ob
ability
analytic
al
ly.
(Use
pr
o-
gr
ams
written
for
pr
evious
exer
cises)
..
Sim
ulations
The
unkno
wn
v
ariance


of
a
sequence
of
sim
ulated
random
v
ariables
X
j
can
b
e
ap-
pro
ximated
b
y

n
P
n
j
=
(X
j
 
X
)

,
where

X
is
the
arithmetic
mean
of
X

;
:
:
:
;
X
n
.
Th
us
can
also
use
(.)
and
Theorem
..
to
asses
errors
in
estimating
v
ariances.
Another
more
accurate
metho
d
is
presen
ted
later
on
in
Chapter
,
but
it
also
requires
estimating
v
ariances.
F
rom
no
w
on,
in
the
output
of
y
our
sim
ulation
programs
y
ou
should
pro
vide
some
error
estimates.
.
Application:
sc
heduling
A
critical
path
analysis
in
v
olv
es
estimating
time
of
completing
a
pro
ject
consisting
of
man
y
tasks
of
v
arying
lengths.
Some
of
the
tasks
can
b
e
done
concurren
tly
,
while
other
ma
y
b
egin
only
after
other
preliminary
tasks
are
completed.
This
is
mo
deled
b
y
the
dep
endency
graph
together
with
the
estimated
times.
..
Deterministic
sc
heduling
problem
As
an
example
of
simple
sc
heduling
problem
consider
the
follo
wing.
Example
.
Supp
ose
that
we
want
to
b
ake
a
b
atch
of
cho
c
olate-chip
c
o
okies.
The
tasks
and
their
(estimate
d)
times
ar
e:
T
Bake
at
0F
(0
min)
T
Make
b
atter
(
min)
T
Pr
e-he
at
oven
to
0F
(0
min)
T
Find
and
gr
e
ase
p
an
(
min)
T
Find
a
c
o
okie-tr
ay
to
serve
c
o
okies
(
min)
T
T
ake
c
o
okies
out,
c
o
ol
and
serve
(
min)
The
dep
endency
graph
is
quite
ob
vious
here;
for
instance,
w
e
cannot
start
baking
b
efore
batter
is
ready
.
What
is
the
shortest
time
w
e
can
eat
the
co
okies?

.	.
DISTRIBUTIONS
OF
FUNCTIONS

..
Sto
c
hastic
sc
heduling
problem
In
some
pro
jects,
the
actual
n
um
b
ers
are
only
the
a
v
erages,
and
the
actual
completion
times
of
the
pro
jects
ma
y
b
e
random.
The
distribution
of
the
actual
completion
time,
or
ev
en
its
a
v
erage
ma
y
b
e
dicult
to
compute
analytically
.
Nev
ertheless,
sim
ulations
let
us
estimate
the
a
v
erage
and
analyze
the
probabilities
of
ev
en
ts.
Exercise
.
Supp
ose
for
the
sake
of
this
exer
cise
that
the
numb
ers
pr
esente
d
in
the
c
o
okie-b
aking
example
ar
e
just
the
aver
age
values
of
the
exp
onential
r
andom
variables.

What
wil
l
b
e
the
aver
age
c
ompletion
time
then?
Wil
l
it
b
e
lar
ger,
smal
ler,
or
ab
out
e
qual
to
the
pr
evious
answer?

How
often
wil
l
we
nish
the
pr
o
c
ess
b
efor
e
the
pr
eviously
estimate
d
(deterministic)
time?
..
More
sc
heduling
questions
In
more
realistic
analysis
of
pro
duction
pro
cesses
w
e
also
ha
v
e
to
decide
ho
w
to
split
the
tasks
b
et
w
een
a
v
ailable
p
ersonnel.
Example
.	
This
exer
cise
r
efers
to
the
tasks
pr
esente
d
in
Example
..
On
aver
age,
how
fast
c
an
a
single
p
erson
b
ake
cho
c
olate-chip
c
o
okies?
What
if
ther
e
ar
e
two
p
e
ople?
.	
Distributions
of
functions
Distributions
of
functions
of
random
v
ariables
are
often
dicult
to
compute
explicitly
.
Sp
ecial
metho
ds
deal
with
more
frequen
t
cases.
Sums
of
discrete
r.
v.
Sums
can
b
e
handled
directly
,
but
a
more
ecien
t
metho
d
uses
generating
functions
of
Chapter
.
Supp
ose
X
;
Y
are
discrete
and
f
(x;
y
)
=
Pr
(X
=
x;
Y
=
y
)
is
their
join
t
probabilit
y
mass
function.
Then
Z
=
X
+
Y
is
discrete
with
v
alues
z
=
x
+
y
.
Therefore
f
Z
(z
)
=
X
x
f
(x;
z
 x)
(:)
F
or
indep
enden
t
random
v
ariables
X
;
Y
this
tak
es
a
sligh
tly
simpler
form.
f
Z
(z
)
=
X
x
f
X
(x)f
Z
(z
 x)
(:)
F
orm
ula
(.)
can
b
e
used
to
pro
v
e
the
so
called
summation
form
ulas.
Theorem
.	.
If
X
;
Y
ar
e
indep
endent
Binomial
with
the
same
p
ar
ameter
p,
ie.
X
is
Bin(n,p)
and
Y
is
Bin(m,
p),
then
X
+
Y
is
Binomial
Bin(n+m,p).
If
X
;
Y
ar
e
indep
endent
Poisson
P
oiss(
X
)
and
P
oiss(
Y
),
then
X
+
Y
is
Poisson
P
oiss(
X
+

Y
).


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
Sums
of
con
tin
uous
r.
v.
One
can
pro
v
e
that
if
X
;
Y
are
indep
enden
t
and
con
tin
uous
than
X
+
Y
is
con
tin
uous
with
the
densit
y
f
(z
)
=
Z

 
f
X
(u)f
Y
(z
 u)
du
(:)
F
orm
ula
(.)
denes
the
c
onvolution
f
X

f
Y
.
It
can
b
e
used
to
pro
v
e
the
so
called
summation
form
ulas.
Theorem
.	.
If
X
;
Y
ar
e
indep
endent
Normal,
then
X
+
Y
is
Normal.
If
X
;
Y
ar
e
indep
endent
Gamma
with
the
same
p
ar
ameter

,
then
X
+
Y
is
Gamma(
X
+

Y
;

).
Example
.0
(Example
.
con
tin
ued)
Two
drivers
arrive
at
an
interse
ction
b
e-
twe
en
:00
and
:0
every
day.
What
is
the
density
of
the
time
that
lapse
d
b
etwe
en
their
arrivals?
Example
.
Supp
ose
X
;
Y
ar
e
indep
endent
U
(0;
).
The
density
of
Z
=
X
+
Y
is
f
(z
)
=
(
z
if
0

z



 z
if


z


.
Minima,
maxima
Minima
and
maxima
o
ccur
for
instance
if
w
e
are
w
aiting
for
one
of
the
indep
enden
t
ev
en
ts,
and
then
w
e
follo
w
the
rst
one
(minim
um),
or
the
last
one
(maxim
um).
Em-
b
edded
Mark
o
v
c
hains
construction
in
Section
.
are
based
on
minima
of
indep
enden
t
exp
onen
tial
r.
v.
Supp
ose
X
;
Y
are
indep
enden
t.
If
U
=
min
fX
;
Y
g
then
Pr(U
>
t)
=
Pr(X
>
t)
Pr(Y
>
t).
Therefore
the
reliabilit
y
function
of
U
can
b
e
computed
from
the
t
w
o
giv
en
ones.
Example
.
If
X
;
Y
ar
e
indep
endent
exp
onential,
then
U
=
min
fX
;
Y
g
is
exp
onential.
If
U
=
max
fX
;
Y
g
then
Pr(U
<
t)
=
Pr
(X
<
t)
Pr(Y
<
t).
Therefore
the
cum
ulativ
e
distribution
function
of
U
can
b
e
computed
from
the
t
w
o
giv
en
ones.
Example
.
Supp
ose
X
;
Y
ar
e
indep
endent
uniform
U(0,).
Then
U
=
max
fX
;
Y
g
has
the
density
f
(u)
=
u
for
0
<
u
<
.
Problem
.
L
et
U

;
:
:
:
;
U
n
b
e
indep
endent
uniform
U
(0;
).
Find
the
density
of

X
=
min
j
U
j

Y
=
max
j
U
j
Problem
.
If
X
;
Y
ar
e
indep
endent
exp
onential
r
andom
variables
with
p
ar
ameters
;
,
show
that
Pr
(X
<
Y
)
=

+
.

.0.
L

-SP
A
CES

Order
statistics
Order
statistics
generalize
minima
and
maxima.
Their
main
use
is
in
(robust)
statistics,
and
this
section
can
b
e
safely
skipp
ed.
Let
X

;
:
:
:
;
X
n
b
e
indep
enden
t
con
tin
uous
random
v
ariables
with
the
cum
ulativ
e
distribution
function
G
and
densit
y
g
=
G
0
.
Let
R

;
:
:
:
;
R
n
b
e
the
corresp
onding
or
der
statistics.
This
means
that
at
the
end
of
eac
h
exp
erimen
t
w
e
re-arrange
the
n
um
b
ers
X

;
:
:
:
X
n
in
to
the
increasing
sequence
R

;
:
:
:
;
R
n
.
This
means
that
R

=
min
j
X
j
is
the
smallest,
R

=
max
i
min
j
=i
X
j
is
the
second
largest,
etc.
The
densit
y
of
R
k
can
b
e
found
b
y
the
follo
wing
metho
d.
In
order
for
the
inequalit
y
R
k
>
x
to
hold,
there
m
ust
b
e
at
least
k
v
alues
among
the
X
j
ab
o
v
e
lev
el
x.
Since
X
j
are
indep
enden
t
and
ha
v
e
the
same
probabilit
y
p
=
Pr
(X
j
>
x)
of
\success"
in
crossing
o
v
er
the
x-lev
el,
this
means
that
Pr(R
k
>
x)
is
giv
en
b
y
the
binomial
form
ula
with
n
trials
and
probabilit
y
of
success
p
=

 G(x).
Pr
(R
k
>
x)
=
n
X
j
=k
(
n
j
)(
 G(x))
j
(G(x))
n j
(:)
When
the
deriv
ativ
e
is
tak
en,
the
sum
collapses
in
to
just
one
term,
giving
the
elegan
t
answ
er
r
k
(x)
=
n!
(k
 )!(n k
)!
(G(x))
k
(
 G(x))
n k
g
(x).
.0
L

-spaces
Inequalities
related
to
exp
ected
v
alues
are
b
est
stated
in
the
geometric
language
of
norms
and
normed
spaces.
W
e
sa
y
that
X

L

,
if
X
is
squar
e
inte
gr
able,
ie.
E
X

<
.
The
L

norm
is
kX
k

=
q
E
jX
j

:
Notice
that
kX
 E
X
k

is
just
another
notation
for
the
standard
deviation.
Th
us
standard
deviation
is
the
L

distance
of
X
from
a
constan
t.
W
e
sa
y
that
X
n
con
v
erges
to
X
in
L

,
if
kX
n
 X
k

!
0
as
n
!
.
W
e
shall
also
use
the
phrase
se
quenc
e
X
n
c
onver
ges
to
X
in
me
an-squar
e.
An
example
of
the
latter
is
Theorem
...
Sev
eral
useful
inequalities
are
collected
in
the
follo
wing

.
Theorem
.0.
F
or
al
l
squar
e-inte
gr
able
X
;
Y

Cauchy-Schwarz
ine
quality:
E
X
Y

kX
k

kY
k

:
(:)

Jensen
's
ine
quality:
E
jX
j

E
kX
k

:
(:)

T
riangle
ine
quality:
kX
+
Y
k


kX
k

+
kY
k

:
(:	)

Theorem
A..
giv
es
a
more
general
statemen
t.


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
Pro
of.
Pro
of
of
(.):
Quadratic
function
f
(t)
=
E
jX
+
tY
j

=
E
X

+
tE
X
Y
+
t

E
Y

is
non-negativ
e
for
all
t.
Therefore
its
determinan
t


0.
(Compute

to
nish
the
pro
of.)
Pro
of
of
(.):
Use
(.)
with
Y
=
.
Pro
of
of
(.	):
By
(.)
E
jX
+
Y
j


kX
k


+
kY
k


+
kX
k

kY
k

.

.
Correlation
co
ecien
t
Correlation
is
a
concept
deeply
ro
oted
in
statistics.
The
correlation
co
ecien
t
cor
r
(X
;
Y
)
is
dened
for
square-in
tegrable
non-degenerate
r.
v.
X
;
Y
b
y
the
form
ula

=
cor
r
(X
;
Y
)
=
E
X
Y
 E
X
E
Y
kX
 E
X
k

kY
 E
Y
k

:
The
Cauc
h
y-Sc
h
w
arz
inequalit
y
(.)
implies
that
 

cor
r
(X
;
Y
)

.
Random
v
ariables
with

=
0
are
called
unc
orr
elate
d.
Correlation
co
ecien
t
close
to
one
of
the
extremes

means
that
there
is
a
strong
linear
relation
b
et
w
een
X
;
Y
;
this
is
stated
more
precisely
in
(.).
Theorem
..
states
that
indep
enden
t
random
v
ariables
with
nite
v
ariances
are
uncorrelated.
Problem
.	
Give
an
example
of
dep
endent
r
andom
variables
that
ar
e
unc
orr
elate
d.
..
Best
linear
appro
ximation
Supp
ose
w
e
w
ould
lik
e
to
appro
ximate
random
v
ariable
Y
b
y
another
quan
tit
y
X
that
is
p
erhaps
b
etter
accessible.
Of
all
the
p
ossible
w
a
ys
to
do
it,
linear
function
Y

mX
+
b
is
p
erhaps
the
simplest.
Of
all
suc
h
linear
functions,
w
e
no
w
w
an
t
to
pic
k
the
b
est.
In
a
single
exp
erimen
t,
the
error
is
jY
 mX
 bj.
W
e
could
minimize
the
a
v
erage
empirical
error
o
v
er
man
y
exp
erimen
ts

n
P
j
jY
j
 mX
j
 bj.
This
appro
ximates
the
a
v
erage
error
E
jY
 mX
 bj.
Let
us
agree
to
measure
the
error
of
the
appro
ximation
b
y
a
quadratic
error
E
(Y
 mX
 b)

instead.
(This
c
hoice
leads
to
simpler
mathematics.)
Question:
F
or
what
v
alues
of
m;
b
the
error
E
(Y
 mX
 b)

is
the
smallest?
When
is
it
0?
Let
H
(m;
b)
=
E
(Y
 mX
 b)

.
Clearly
,
H
is
a
quadratic
function
of
m;
b

I
R.
The
unique
minim
um
is
determined
b
y
the
system
of
equations
@
H
@
m
=
0
(.0)
@
H
@
b
=
0:
The
answ
er
is
m
=
cov
(X
;Y
)
V
ar
X
,
b
=
E
Y
 mE
X
,
and
the
minimal
error
is
(
 

)V
ar
(Y
):
(:)

..
APPLICA
TION:
LENGTH
OF
A
RANDOM
CHAIN
	
.
Application:
length
of
a
random
c
hain
A
c
hain
in
the
x;
y
-plane
consists
of
n
links
eac
h
of
unit
length.
The
angle
b
et
w
een
t
w
o
consecutiv
e
links
is

,
where

>
0
is
a
constan
t.
Assume
the
sign
is
tak
en
and
random,
with
probabilit
y


for
eac
h.
Let
L
n
b
e
the
distance
from
the
b
eginning
to
the
end
of
the
c
hain.
The
angle
b
et
w
een
the
k
-th
link
and
the
p
ositiv
e
x-axis
is
a
random
v
ariable
S
k
 
,
where
w
e
ma
y
assume
(wh
y?)
S
0
=
0
and
S
k
=
S
k
 
+

k

,
where

=

with
probabilit
y


.
The
follo
wing
steps
determine
the
a
v
erage
length
of
the
random
c
hain.
.
L

n
=
(
P
n 
k
=0
cos
S
k
)

+
(
P
n 
k
=0
sin
S
k
)

.
.
E
cos
S
n
=
cos
n

.
E
sin
S
n
=
0
.
E
cos
S
m
cos
S
n
=
cos
n m

E
cos

S
m
for
m
<
n
.
E
sin
S
m
sin
S
n
=
cos
n m

E
sin

S
m
for
m
<
n
.
E
L

n
 L

n 
=

+

cos

 cos
n 

 cos

.
E
L

n
=
n
+cos

 cos

 
cos

 cos
n

( cos
)

.
Conditional
exp
ectations
..
Conditional
distributions
F
or
discrete
X
;
Y
the
conditional
distribution
of
v
ariable
Y
giv
en
the
v
alue
of
X
is
just
the
conditional
probabilit
y
,
Pr
(Y
=
y
jX
=
x).
In
join
tly
con
tin
uous
case,
dene
the
conditional
densit
y
f
(y
jX
=
x)
=
f
(x;
y
)
f
X
(x)
:
Conditional
densit
y
f
(y
jX
=
x)
is
dened
only
for
x
suc
h
that
f
X
(x)
>
0;
this
is
a
reasonable
approac
h
for
the
most
often
encoun
tered
con
tin
uous,
or
piecewise
con
tin
uous
densities.
Since
the
densities
are
actually
the
elemen
ts
of
L

space
rather
than
functions,
sp
ecial
care
is
needed
in
the
denition
of
the
conditional
densit
y
.
In
fact
the
theory
of
probabilit
y
is
often
dev
elop
ed
without
the
reference
to
conditional
distributions.
Denition
..
The
c
onditional
exp
e
ctation
E
fX
jY
=
y
g
is
dene
d
as
P
x
Pr(X
=
xjY
=
y
)
in
discr
ete
c
ase,
and
as
R
I
R
xf
(xjY
=
y
)
dx
in
the
c
ontinuous
c
ase.
One
c
an
show
that
the
exp
e
cte
d
values
exist,
when
E
jX
j
<
.
Example
.
A
game
c
onsists
of
tossing
a
die.
If
the
fac
e
value
on
the
die
is
X
then
a
c
oin
is
tosse
d
X
times.
L
et
Y
b
e
the
numb
er
of
he
ads.
Then
E
(Y
jX
=
x)
=


x.

0
CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
..
Conditional
exp
ectations
as
random
v
ariables
Since
E
(X
jY
=
y
)
dep
ends
on
the
actual
v
alue
of
Y
,
and
Y
is
random,
the
conditional
exp
ectation
is
a
random
v
ariable
itself.
W
e
shall
write
E
fX
jY
g
or
E
Y
X
for
the
random
v
ariable
dened
b
y
the
conditional
exp
ectation
E
fX
jY
=
y
g.
Example
.
Supp
ose
Y
is
a
discr
ete
with
dier
ent
values
on
the
events
A

;
A

;
:
:
:
;
A
n
which
form
a
non-de
gener
ate
disjoint
p
artition
of
the
pr
ob
ability
sp
ac
e

.
Then
E
fX
jY
g(!
)
=
n
X
k
=
m
k
I
A
k
(!
);
wher
e
m
k
=
R
A
k
X
dP
=P
(A
k
).
In
other
wor
ds,
on
A
k
we
have
E
fX
jF
g
=
R
A
k
X
dP
=P
(A
k
).
In
p
articular,
if
X
is
discr
ete
and
X
=
P
x
j
I
B
j
,
then
we
get
intuitive
expr
ession
E
fX
jF
g
=
X
x
j
P
(B
j
jA
k
)
for
!

A
k
:
Example
.
Supp
ose
that
f
(x;
y
)
is
the
joint
density
with
r
esp
e
ct
to
the
L
eb
esgue
me
asur
e
on
I
R

of
the
bivariate
r
andom
variable
(X
;
Y
)
and
let
f
Y
(y
)
=
0
b
e
the
(mar
ginal)
density
of
Y
.
Put
f
(xjy
)
=
f
(x;
y
)=f
Y
(y
).
Then
E
fX
jY
g
=
h(Y
),
wher
e
h(y
)
=
R

 
xf
(xjy
)
dx.
T
otal
probabilit
y
form
ula
for
conditional
exp
ectations
is
as
follo
ws.
E
Y
=
E
(E
(Y
jX
))
(:)
compare
(.0).
Example
.
In
Example
.,
E
Y
=
=.
..
Conditional
exp
ectations
(con
tin
ued)
In
discrete
case
conditional
exp
ectations
of
functions
are
giv
en
b
y
,
E
(g
(X
)jY
=
y
)
=
X
x
g
(x)
Pr
(X
=
xjY
=
y
)
(:)
The
follo
wing
v
ersion
of
total
probabilit
y
form
ula
is
often
useful.
E
g
(X
)
=
E
(E
(g
(X
)jY
))
(:)
Example
.
Supp
ose
N
is
Binomial
B
in(m;
q
)
and
given
the
value
of
N
,
r.
v.
X
is
Bin(N,p).
What
is
the
distribution
of
X
?
Similar
question
c
an
b
e
solve
d
for
N
having
a
Poisson
distribution.
Example
.	
What
is
the
distribution
of
a
ge
ometric
sum
of
i.
i.
d.
exp
onential
r.
v.?
Example
.0
Sto
ck
market
uctuations
c
an
b
e
mo
del
le
d
by
Z
=


+
:
:
:
+

N
,
wher
e
N
,
the
numb
er
of
tr
ansactions,
is
Poisson()
and

ar
e
normal
N
(0;

).
Ther
e
is
no
explicit
formula
for
the
density
of
Z
,
but
ther
e
is
one
for
the
moment
gener
ating
function.
Thus
Chebyshev
ine
quality
gives
b
ounds
of
the
form
Pr(Z
>
t)

exp
:::.

..
BEST
NON-LINEAR
APPR
O
XIMA
TIONS

.
Best
non-linear
appro
ximations
This
section
explains
the
relev
ance
of
conditional
exp
ectations
to
the
problem
of
b
est
mean-square
appro
ximation.
Theorem
A..
giv
es
geometric
in
terpretation
of
the
conditional
exp
ectation
E
fjZ
g;
for
square
in
tegrable
functions
E
f:jZ
g
is
just
the
orthogonal
pro
jection
of
the
Banac
h
(normed)
space
L

on
to
its
closed
subspace
L

(Z
),
consisting
of
all
-in
tegrable
random
v
ariables
of
the
form
f
(Z
).
Theorem
..
F
or
squar
e
inte
gr
able
Y
the
quadr
atic
err
or
E
(Y
 h(X
))

among
al
l
squar
e
inte
gr
able
functions
h(X
)
is
the
smal
lest
if
h(x)
=
E
(Y
jX
=
x).
Theorem
..
implies
that
the
linear
appro
ximation
from
Section
..
is
usually
less
accurate.
In
Chapter
0
w
e
shall
see
that
linear
appro
ximation
is
the
b
est
one
can
get
in
the
all
imp
ortan
t
normal
case.
Ev
en
in
non-normal
case
linear
appro
ximations
oer
quic
k
solutions
based
on
simple
second
order
statistics.
In
con
trast,
the
non-linear
appro
ximations
require
elab
orate
n
umerical
sc
hemes
to
pro
cess
the
empirical
data.
.
Lac
k
of
memory
Conditional
probabilities
help
us
to
arriv
e
at
imp
ortan
t
classes
of
densities
in
mo
deling.
In
this
section
w
e
w
an
t
to
analyze
an
non-aging
device,
whic
h
c
haracteristics
do
not
c
hange
with
time.
Supp
ose
T
represen
ts
a
failure
time
of
some
device.
If
the
device
is
w
orking
at
time
t,
then
the
probabilit
y
of
surviving
additional
s
seconds
is
Pr
(T
>
t
+
sjT
>
t).
F
or
a
device
that
do
esn't
exhibit
aging
this
probabilit
y
should
b
e
the
same
as
for
the
brand
new
device.
Pr(T
>
t
+
sjT
>
t)
=
Pr(T
>
s)
(:)
Problem
.0
Show
that
exp
onential
T
satises
(.).
Problem
.
Show
that
ge
ometric
T
satises
(.)
for
inte
ger
t;
s.
Equation
(.)
implies
Pr(T
>
t
+
s)
=
Pr
(T
>
t)
Pr(T
>
s),
an
equation
that
can
b
e
solv
ed.
Theorem
..
If
T
>
0
satises
(.)
for
al
l
t;
s
>
0,
then
T
is
exp
onential.
Pro
of.
The
tail
distribution
function
N
(x)
=
P
(T
>
x)
satises
equation
N
(x
+
y
)
=
N
(x)N
(y
)
(:)
for
arbitrary
x;
y
>
0.
Therefore
to
pro
v
e
the
theorem,
w
e
need
only
to
solv
e
functional
equation
(.)
for
the
unkno
wn
function
N
()
under
the
conditions
that
0

N
()

,
N
()
is
left-con
tin
uous,
non-increasing,
N
(0
+
)
=
,
and
N
(x)
!
0
as
x
!
.
F
orm
ula
(.)
implies
that
for
all
in
teger
n
and
all
x

0
N
(nx)
=
N
(x)
n
:
(:)


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
Since
N
(0
+
)
=

and
N
(),
it
follo
ws
from
(.)
that
r
=
N
()
>
0.
Therefore
(.)
implies
N
(n)
=
r
n
and
also
N
(=n)
=
r
=n
(to
see
this,
plug
in
(.)
v
alues
x
=

and
x
=
=n
resp
ectiv
ely).
Hence
N
(n=m)
=
N
(=m)
n
=
r
n=m
(b
y
putting
x
=
=m
in
(.)).
This
sho
ws
that
for
rational
q
>
0
N
(q
)
=
r
q
:
(:)
Since
N
(x)
is
left-con
tin
uous
(wh
y?),
N
(x)
=
lim
q
%x
N
(q
)
=
r
x
for
all
x

0.
It
remains
to
notice
that
since
N
(x)
!
0
as
x
!
,
w
e
ha
v
e
r
<
.
Therefore
r
=
exp
( )
for
some

>
0
and
N
(x)
=
exp
( x);
x

0.

Remark

Ge
ometric
distribution
also
has
the
lack
of
memory
pr
op
erty.
If
e
quation
(.)
is
assume
d
to
hold
for
inte
ger
values
of
x;
y
only,
and
T
>
0
is
inte
ger
value
d,
then
T
is
ge
ometric.
.
In
tensit
y
of
failures
The
in
tuitiv
e
lac
k-of-memory
,
or
non-aging
prop
ert
y
of
the
exp
onen
tial
distribution
can
b
e
generalized
to
include
simple
mo
dels
of
aging.
W
e
ma
y
w
an
t
to
assume
that
a
comp
onen
t
analyzed
b
ecomes
less
reliable,
or
more
reliable
with
time.
An
example
of
the
rst
one
is
p
erhaps
a
brand
new
car.
An
example
of
the
latter
is
p
erhaps
a
soft
w
are
op
erating
system
when
up
dates
are
installed
promptly
.
Let
T
>
0
b
e
a
con
tin
uous
r.
v.
in
terpreted
as
a
failure
time
of
a
certain
device.
If
the
device
is
in
op
erational
condition
at
time
t,
then
the
probabilit
y
that
it
will
fail
immediately
afterw
ards
ma
y
b
e
assumed
negligible.
The
probabilit
y
of
failing
within
h
units
of
time
is
Pr(T
<
t
+
hjT
>
t).
The
failure
rate
at
time
t
is
dened
as
(t)
=
lim
h!0

h
Pr(T
<
t
+
hjT
>
t)
(:	)
Example
.
If
T
is
exp
onential
then
the
failur
e
r
ate
is
c
onstant.
A
family
of
failure
rates
that
exhibit
in
teresting
aging
patterns
is
pro
vided
b
y
the
family
of
p
o
w
er
functions
(t)
=
t
a
.
Theorem
..
If
T
is
c
ontinuous
with
failur
e
r
ate
(t)
=
t
a
,
wher
e
a
>
0
then
T
has
the
Weibul
l
density:
f
(t)
=
C
t
a 
e
 bt
a
for
t
>
0:
(:0)
(Her
e
C
=
ab
is
the
normalization
c
onstant).
.
P
oisson
appro
ximation
Of
the
discrete
distributions,
the
form
ula
for
the
P
oisson
distribution
is
p
erhaps
m
ysteri-
ous.
P
oisson
distribution
is
often
called
the
law
of
r
ar
e
events.

..
QUESTIONS

Theorem
..
Supp
ose
X
n
ar
e
B
in(n;
p
n
)
and
np
n
!
.
Then
Pr(X
n
=
k
)
!
e
 

k
=k
!.
Pro
of.
Rewrite
the
expression
(
n
k
)

k
n
k
( 
n
)
n k
=

k
=k
!( =n)
n
=( =n)
k
Q
k
j
=0
( j
=n).

Example
.
How
many
r
aisins
should
a
c
o
okie
have
on
aver
age
so
that
no
mor
e
than
one
c
o
okie
in
a
hundr
e
d
has
no
r
aisins?
So
that
no
mor
e
than
one
c
o
okie
in
a
thousand
has
no
r
aisins?
Example
.
A
b
ag
of
cho
c
olate
chip
c
o
okies
has
0
c
o
okies.
The
manufactur
e
claims
ther
e
ar
e
a
,000
chips
in
a
b
ag.
Is
it
likely
to
nd
a
c
o
okie
with

or
less
chips
in
such
a
b
ag?
.
Questions
Problem
.
The
p
erformanc
e
of
the
algorithm
for
sele
cting
a
r
andom
p
ermutation
in
GetPermutation
SUB
of
RANDTOUR.BAS
c
an
b
e
estimate
d
by
the
fol
lowing.
F
r
om
numb
ers
;
:
:
:
n,
sele
ct
at
r
andom
k
>
n
numb
ers.
On
aver
age,
how
many
of
these
numb
ers
r
ep
e
at?
(and
henc
e
should
b
e
thr
own
out)
Problem
.
The
p
erformanc
e
of
the
algorithm
for
sele
cting
a
r
andom
p
ermutation
in
GetPermutation
SUB
of
RANDTOUR.BAS
c
an
b
e
estimate
d
by
analyzing
the
fol
lowing
\worst-c
ase"
sc
enario.
When
the
algorithm
attempts
to
sele
ct
last
of
the
r
andom
numb
ers
;
:
:
:
n,
then
.
What
is
the
pr
ob
ability
if
wil
l
nd
the
\right
numb
er"
on
rst
attempt?
.
How
many
attempts
on
aver
age
do
es
the
algorithm
take
to
nd
the
last
r
andom
numb
er?
Exercise
.
What
is
the
pr
ob
ability
that
in
the
gr
oup
of

p
e
ople
one
c
an
nd
two
b
orn
on
the
same
day
of
the
ye
ar?
(Comp
ar
e
Example
.).
Problem
.
F
or
a
gr
oup
of
n
p
erson,
nd
the
exp
e
cte
d
numb
er
of
days
of
the
ye
ar
which
ar
e
birthdays
of
exactly
k
p
e
ople.
(Assume

days
in
a
ye
ar
and
that
al
l
birthdays
ar
e
e
qual
ly
likely.)
Problem
.
A
lar
ge
numb
er
N
of
p
e
ople
ar
e
subje
ct
to
a
blo
o
d
test.
The
test
c
an
b
e
administer
e
d
in
one
of
the
two
ways:
(i)
Each
p
erson
c
an
b
e
teste
d
sep
ar
ately
(N
tests
ar
e
ne
e
de
d).
(ii)
The
blo
o
d
sample
of
k
p
e
ople
c
an
b
e
p
o
ole
d
(mixe
d)
and
analyze
d.
If
the
test
is
p
ositive,
e
ach
of
the
k
p
e
ople
must
b
e
teste
d
sep
ar
ately
and
in
al
l
k
+

tests
ar
e
then
r
e
quir
e
d
for
k
p
e
ople.
Assume
the
pr
ob
ability
p
that
the
test
is
p
ositive
is
the
same
for
al
l
p
e
ople
and
that
the
test
r
esults
for
dier
ent
p
e
ople
ar
e
sto
chastic
al
ly
indep
endent.


CHAPTER
.
RANDOM
V
ARIABLES
(CONTINUED)
.
What
is
the
pr
ob
ability
that
the
test
for
a
p
o
ole
d
sample
of
k
p
e
ople
is
p
ositive?
.
What
is
the
exp
e
cte
d
numb
er
of
tests
ne
c
essary
under
plan
(ii)?
.
Find
the
e
quation
for
the
value
of
k
which
wil
l
minimize
the
exp
e
cte
d
numb
er
of
tests
under
plan
(ii).
.
Show
that
the
optimal
k
is
close
to

p
p
and
henc
e
that
the
minimum
exp
e
cte
d
numb
er
of
tests
is
on
aver
age
ab
out
N
p
p
.
Exercise
.
Solve
Exer
cise
.
on
p
age

assuming
that
the
arrival
times
ar
e
exp
o-
nential
r
ather
than
uniform.
Assume
indep
endenc
e.
Exercise
.
Ther
e
ar
e

stop-lights
sp
ac
e
d
within
km
of
e
ach
other
and
op
er
ating
asynchr
onously.
(They
ar
e
r
eset
at
midnight.)
Assuming
e
ach
is
r
e
d
for

minute
and
then
gr
e
en
for
one
minute,
what
is
the
aver
age
time
to
p
ass
thr
ough
the
thr
e
e
lights
by
a
c
ar
that
c
an
instantane
ously
ac
c
eler
ate
to
0km/h.
This
exer
cise
c
an
b
e
develop
e
d
into
a
simulation
pr
oje
ct
that
may
addr
ess
some
of
the
fol
lowing
questions

How
do
es
the
sp
e
e
d
change
with
the
numb
er
of
lights?

How
do
es
the
answer
change
if
a
c
ar
has
nite
ac
c
eler
ation?

A
r
e
the
answers
dier
ent,
if
e
ach
gr
e
en
light
lasts
r
andom
amount
of
time,
min
on
aver
age?

How
to
mo
del/simulate
mor
e
than
one
c
ar?

Can
you
simulate
c
ar
tr
ac
on
a
squar
e
grid
with
stop-lights
at
interse
ctions?
More
theoretical
questions
Problem
.	
(Ho
eding)
Show
that
if
X
Y
;
X
;
Y
ar
e
discr
ete,
then
E
X
Y
 E
X
E
Y
=
Z

 
Z

 
(P
(X

t;
Y

s)
 P
(X

t)P
(Y

s))
dt
ds:
Problem
.0
L
et
X

0
b
e
a
r
andom
variable
and
supp
ose
that
for
every
0
<
q
<

ther
e
is
T
=
T
(q
)
such
that
P
(X
>
t)

q
P
(X
>
t)
for
al
l
t
>
T
:
Show
that
al
l
the
moments
of
X
ar
e
nite.
Problem
.
If

;
U
ar
e
indep
endent,
Pr
(
=
0)
=
Pr(
+
)
=


and
U
is
uniform
U
(0;
).
What
is
the
distribution
of
U
+

?

Chapter

Momen
t
generating
functions
.
Generating
functions
Prop
erties
of
a
sequence
fa
n
g
are
often
reected
in
prop
erties
of
the
gener
ating
function
h(z
)
=
P
n
a
n
z
n
.
.
Prop
erties
The
momen
t
generating
function
of
a
real-v
alued
random
v
ariable
X
is
dened
b
y
M
X
(t)
=
E
exp
(tX
).
If
X
>
0
has
the
densit
y
f
(x),
the
momen
t
generating
function
is
its
Laplace
transform:
M
(t)
=
R

0
e
tx
f
(x)
dx.
A
momen
t
generating
function
is
non-negativ
e,
and
con
v
ex
(conca
v
e
up).
The
t
ypical
example
is
the
momen
t
generating
function
of
the
f0;
g-v
alued
random
v
ariable.
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
t
M
(t)

-
.
-

Figure
.:
Graph
of
the
momen
t
generating
function
M
(t)
=


+


e
t
.
A
linear
transformations
of
X
c
hanges
the
momen
t
generating
function
b
y
the
follo
wing



CHAPTER
.
MOMENT
GENERA
TING
FUNCTIONS
form
ula.
M
aX
+b
(t)
=
e
tb
M
X
(at):
(:)
Imp
ortan
t
prop
erties
of
momen
t
generating
functions
are
pro
v
ed
in
more
theoretical
probabilit
y
courses

.
Theorem
..
(i)
The
distribution
of
X
is
determine
d
uniquely
by
its
moment
gener-
ating
function
M
(t).
(ii)
If
X
;
Y
ar
e
indep
endent
r
andom
variables,
then
M
X
+Y
(t)
=
M
X
(t)M
Y
(t)
for
al
l
t

I
R.
(iii)
M
(0)
=
;
M
0
(0)
=
E
X
;
M
00
(0)
=
E
X

Name
Distribution
Momen
t
generating
function
Normal
N(0,)
f
(x)
=

p

exp
 x


M
(t)
=
e
t

=
Exp
onen
tial
f
(x)
=
e
 x
M
(t)
=

 t
Uniform
U
( ;
)
f
(x)
=


for
 

x


M
(t)
=

t
sinh
t
Gamma
f
(x)
=
= (
)
 
x
 
exp
 x=
M
(t)
=
(
 
t)
 
Binomial
Pr(X
=
k
)
=
(
n
k
)p
k
(
 p)
n k
M
(t)
=
(
 p
+
pe
t
)
n
P
oisson
Pr(X
=
k
)
=
e
 

k
k
!
M
(t)
=
exp
(e
t
 )
Geometric
Pr(X
=
k
)
=
p(
 p)
k
 
M
(t)
=
pe
t
 ( p)e
t
T
able
.:
Momen
t
generating
functions.
Problem
.
Find
moment
gener
ating
functions
for
e
ach
of
the
entries
in
T
able
..
Problem
.
Use
moment
gener
ating
functions
to
c
ompute
E
X
;
V
ar
(X
)
for
e
ach
of
the
entries
in
T
able
..
Problem
.
Pr
ove
the
summation
formulas
state
d
in
The
or
ems
.
and
.
F
or
a
d-dimensional
random
v
ariable
X
=
(X

;
:
:
:
;
X
d
)
the
momen
t
generating
function
M
X
:
I
R
d
!
C
C
is
dened
b
y
M
X
(t)
=
E
exp(t

X),
where
the
dot
denotes
the
dot
(scalar)
pro
duct,
ie.
x

y
=
P
x
k
y
k
.
F
or
a
pair
of
real
v
alued
random
v
ariables
X
;
Y
,
w
e
also
write
M
(t;
s)
=
M
(X
;Y
)
((t;
s))
and
w
e
call
M
(t;
s)
the
join
t
momen
t
generating
function
of
X
and
Y
.
The
follo
wing
is
the
m
ulti-dimensional
v
ersion
of
Theorem
...
Theorem
..
(i)
The
distribution
of
X
is
determine
d
uniquely
by
its
moment
gener
ating
function
M
(t).
(ii)
If
X;
Y
ar
e
indep
endent
I
R
d
-value
d
r
andom
variables,
then
M
X+Y
(t)
=
M
X
(t)M
Y
(t)
for
al
l
t
in
I
R
d
.

See
eg.
W.
F
eller,
An
In
tro
duction
to
Probabilit
y
Theory,
V
ol
I
I,
Wiley
,
New
Y
ork
	.

..
CHARA
CTERISTIC
FUNCTIONS

..
Probabilit
y
generating
functions
F
or
Z
Z
+
-v
alued
random
v
ariables
it
is
con
v
enien
t
to
consider
the
so
called
gener
ating
function
G(z
)
=
M
(ln
z
).
In
this
case
Theorem
..(i)
is
elemen
tary
,
as
G(z
)
=
P

k
=0
p
k
z
k
determines
uniquely
its
T
a
ylor
series
co
ecien
ts
p
k
=
Pr(X
=
k
).
.
Characteristic
functions
The
ma
jor
n
uisance
in
using
the
mo
emen
t
generating
functions
is
the
fact
that
the
momen
t
generating
functions
ma
y
not
exist,
when
the
deniting
in
tegral
div
erges.
F
or
this
reason
it
is
more
preferable
to
use
an
expression
that
is
alw
a
ys
b
ounded,
and
y
et
has
the
same
con
v
enien
t
algebraic
prop
erties.
The
natural
candidate
is
e
ix
=
cos
x
+
i
sin
x:
The
char
acteristic
function
is
accordingly
dened
as

X
(t)
=
E
e
itX
:
(:)
F
or
symmetric
random
v
ariables
complex
n
um
b
ers
can
b
e
a
v
oided
at
the
exp
ense
of
trigonometric
iden
tities.
Example
.
If
X
is
 ;

value
d,
Pr(X
=
)
=


,
then
(t)
=
cos
t.
Example
.
The
char
acteristic
function
of
the
normal
N
(0;
)
distribution
is
(t)
=
e
 t

=
.
.
Questions
Problem
.
L
et
S
n
=
X

+
:
:
:
+
X
n
b
e
the
sum
of
mutual
ly
indep
endent
r
andom
vari-
ables
e
ach
assuming
the
values
;
;
:
:
:
;
a
with
pr
ob
ability

a
.
.
Show
that
E
e
uS
n
=

e
u
( e
au
)
a( e
u
)

n
.
.
Use
the
ab
ove
identity
to
show
that
for
k

n
Pr(S
n
=
k
)
=
a
 n

X
j
=0
( )
j
(
n
j
)(
k
 aj
 
n 
)
(F
or
a
=

Pr(S
n
=
k
)
is
the
pr
ob
ability
of
sc
oring
the
sum
k
+
n
in
a
thr
ow
with
n
dic
e.
The
solution
of
this
pr
oblem
is
due
to
de
Moivr
e.)
Problem
.
Supp
ose
the
pr
ob
ability
p
n
that
a
family
has
exactly
n
childr
en
is

p
n
when
n


and
supp
ose
p
0
=

 
p
 p
.
(Notic
e
that
this
is
a
c
onstaint
on
the
admissible
values
of

;
p
sinc
e
p
0

0.
Supp
ose
that
al
l
distributions
of
the
sexes
for
n
childr
en
ar
e
e
qual
ly
likely.
Find
the
pr
ob
ability
that
a
family
has
exactly
k
girls.
Hint:
The
answer
is
at
rst
as
the
innite
series.
T
o
nd
its
sum,
use
gener
ating
function,
or
ne
gative
binomial
exp
ansion:
(
+
x)
 k
=

+
k
+
!
x
+
(k
+)k
+
!
x

+
:
:
:
.
ANS:
p
k
( p)
k
+
.


CHAPTER
.
MOMENT
GENERA
TING
FUNCTIONS
Problem
.
Show
that
if
X

0
is
a
r
andom
variable
such
that
P
(X
>
t)

0
(
P
(X
>
t))

for
al
l
t
>
0;
then
E
exp
(jX
j)
<

for
some

>
0.
Problem
.
Show
that
if
E
exp
(X

)
=
C
<

for
some
a
>
0,
then
E
exp
(tX
)

C
exp
(
t


)
for
al
l
r
e
al
t.
Problem
.
Pr
ove
that
function
(t)
:=
E
maxfX
;
tg
determines
uniquely
the
distribu-
tion
of
an
inte
gr
able
r
andom
variable
X
in
e
ach
of
the
fol
lowing
c
ases:
(a)
If
X
is
discr
ete.
(b)
If
X
has
c
ontinuous
density.
Problem
.	
L
et
p
>
.
Show
that
exp
(jtj
p
)
is
not
a
moment
gener
ating
function.

Chapter

Normal
distribution
Next
to
a
str
e
am
in
a
for
est
you
se
e
a
smal
l
tr
e
e
with
tiny,
b
el
l-shap
e
d,
white
owers
in
dr
opping
clusters.
The
Aub
orn
So
ciet
y
Field
Guide
to
North
American
T
rees.
The
predominance
of
normal
distribution
is
often
explained
b
y
the
Cen
tral
Limit
Theorem,
see
Section..
This
theorem
asserts
that
under
fairly
general
conditions
the
distribution
of
the
sum
of
man
y
indep
enden
t
comp
onen
ts
is
appro
ximately
normal.
In
this
c
hapter
w
e
giv
e
another
reason
wh
y
normal
distribution
migh
t
o
ccur.
The
usual
denition
of
the
standard
normal
v
ariable
Z
sp
ecies
its
densit
y
f
(x)
=

p

e
 x


,
see
Figure
.
on
page
.
In
general,
the
so
called
N
(m;

)
densit
y
is
giv
en
b
y
f
(x)
=

p


e
 (x m)



:
By
completing
the
square
one
can
c
hec
k
that
the
momen
t
generating
function
M
(t)
=
E
e
tZ
=
R

 
e
itx
f
(x)
dx
of
the
standard
normal
r.
v.
Z
is
giv
en
b
y
M
(t)
=
e
t


:
In
m
ultiv
ariate
case
it
is
more
con
v
enien
t
to
use
momen
t
generating
functions
directly
.
F
or
consistency
w
e
shall
therefore
adopt
the
follo
wing
denition.
Denition
.0.
A
r
e
al
value
d
r
andom
variable
X
has
the
normal
N
(m;

)
distribution
if
its
moment
gener
ating
function
has
the
form
M
(t)
=
exp
(tm
+




t

);
wher
e
m;

ar
e
r
e
al
numb
ers.
F
rom
Theorem
..
one
can
c
hec
k
b
y
taking
the
deriv
ativ
es
that
m
=
E
X
and


=
V
ar
(X
).
Using
(.)
it
is
easy
to
see
that
ev
ery
univ
ariate
normal
X
can
b
e
written
as
X
=

Z
+
m;
(:)
	

0
CHAPTER
.
NORMAL
DISTRIBUTION
where
Z
is
the
standard
N
(0;
)
random
v
ariable
with
the
momen
t
generating
function
e
t


.
This
is
p
erhaps
the
most
con
v
enien
t
represen
tation

of
the
general
univ
ariate
normal
distribution.
T
raditionally
,
it
w
as
used
to
answ
er
questions
lik
e
If
X
has
giv
en
mean

=

and
giv
en
v
ariance


=
,
for
what
v
alues
of
a
w
e
ha
v
e
Pr(jX
j
>
a)
=
:?
with
the
help
of
tabularized
v
alues
of
the
cum
ulativ
e
distribution
function
of
standard
normal
Z
.
Exercise
.
(Bask
etball
coac
h's
problem)
Col
le
ct
data
on
the
heights
of

to
0
r
andomly
sele
cte
d
males.
Make
a
histo
gr
am
of
the
data,
c
ompute
the
empiric
al
me
an
and
standar
d
deviation.

Do
es
it
app
e
ar
that
the
normal
distribution
is
a
go
o
d
pr
ob
ability
distribution
for
these
heights?

Ther
e
ar
e
ab
out
00,000
males
in
Cincinnati
ar
e
a.
Assuming
normal
distribution
of
heights
with
the
me
an
and
varianc
e
as
you
obtaine
d
fr
om
the
data,
estimate
the
numb
er
of
males
tal
ler
than

0
.
.
Hersc
hel's
la
w
of
errors
The
follo
wing
narrativ
e
comes
from
J.
F.
W.
Hersc
hel

.
\Supp
ose
a
ball
is
dropp
ed
from
a
giv
en
heigh
t,
with
the
in
ten
tion
that
it
shall
fall
on
a
giv
en
mark.
F
all
as
it
ma
y
,
its
deviation
from
the
mark
is
error,
and
the
probabilit
y
of
that
error
is
the
unkno
wn
function
of
its
square,
ie.
of
the
sum
of
the
squares
of
its
deviations
in
an
y
t
w
o
rectangular
directions.
No
w,
the
probabilit
y
of
an
y
deviation
dep
ending
solely
on
its
magnitude,
and
not
on
its
direction,
it
follo
ws
that
the
probabilit
y
of
eac
h
of
these
rectangular
devia-
tions
m
ust
b
e
the
same
function
of
its
square.
And
since
the
observ
ed
oblique
deviation
is
equiv
alen
t
to
the
t
w
o
rectangular
ones,
supp
osed
concurren
t,
and
whic
h
are
essen
tially
indep
enden
t
of
one
another,
and
is,
therefore,
a
com-
p
ound
ev
en
t
of
whic
h
they
are
the
simple
indep
enden
t
constituen
ts,
therefore
its
probabilit
y
will
b
e
the
pro
duct
of
their
separate
probabilities.
Th
us
the
form
of
our
unkno
wn
function
comes
to
b
e
determined
from
this
condition..."
T
en
y
ears
after
Hersc
hel,
the
reasoning
w
as
rep
eated
b
y
J.
C.
Maxw
ell

.
The
fact
that
v
elo
cities
are
normally
distributed
is
sometimes
called
Maxw
ell's
theorem.
The
b
eaut
y
of
the
reasoning
lies
in
the
fact
that
the
in
terpla
y
of
t
w
o
v
ery
natural
assumptions:
of
indep
endence
and
of
rotation
in
v
ariance,
giv
es
rise
to
the
normal
law
of
err
ors
|
the
most
imp
ortan
t
distribution
in
statistics.

F
or
the
m
ultiv
ariate
analog,
see
Theorem
0..

J.
F.
W.
Hersc
hel,
Quetelet
on
Probabilities,
Edin
burgh
Rev.
	
(0)
pp.
{

J.
C.
Maxw
ell,
Illustrations
of
the
Dynamical
Theory
of
Gases,
Phil.
Mag.
	
(0),
pp.
	{.
Reprin
ted
in
The
Scien
tic
P
ap
ers
of
James
Clerk
Maxw
ell,
V
ol.
I,
Edited
b
y
W.
D.
Niv
en,
Cam
bridge,
Univ
ersit
y
Press
	0,
pp.
{0	.

..
HERSCHEL'S
LA
W
OF
ERR
ORS

Theorem
..
Supp
ose
r
andom
variables
X
;
Y
have
joint
pr
ob
ability
distribution
(dx;
dy
)
such
that
(i)
()
is
invariant
under
the
r
otations
of
I
R

;
(ii)
X
;
Y
ar
e
indep
endent.
Then
X
;
Y
ar
e
normal.
The
follo
wing
tec
hnical
lemma
asserts
that
momen
t
generating
function
exists.
Lemma
..
If
X
;
Y
ar
e
indep
endent
and
X
+
Y
;
X
 Y
ar
e
indep
endent,
then
E
exp
aX
<

for
al
l
a.
Pro
of.
Consider
a
real
function
N
(x)
:=
P
(jX
j

x).
W
e
shall
sho
w
that
there
is
x
0
suc
h
that
N
(x)

(N
(x
 x
0
))

(:)
for
eac
h
x

x
0
.
By
Problem
.
this
will
end
the
pro
of.
Let
X

;
X

b
e
the
indep
enden
t
copies
of
X
.
Inequalit
y
(.)
follo
ws
from
the
fact
that
ev
en
t
fjX

j

xg
implies
that
either
the
ev
en
t
fjX

j

xg
\
fjX

j

x
0
g,
or
the
ev
en
t
fjX

+
X

j

(x
 x
0
)g
\
fjX

 X

j

(x
 x
0
)g
o
ccurs.
Indeed,
let
x
0
b
e
suc
h
that
P
(jX

j

x
0
)



.
If
jX

j

x
and
jX

j
<
x
0
then
jX


X

j

jX

j
 jX

j

(x
 x
0
).
Therefore
using
indep
endence
and
the
trivial
b
ound
P
(jX

+
X

j

a)

P
(jX

j

a)
+
P
(jX

j

a),
w
e
obtain
P
(jX

j

x)

P
(jX

j

x)P
(jX

j

x
0
)
+P
(jX

+
X

j

(x
 x
0
))P
(jX

 X

j

(x
 x
0
))



N
(x)
+
N

(x
 x
0
)
for
eac
h
x

x
0
.

Pro
of
of
Theorem
...
Let
M
(u)
=
E
uX
b
e
the
momen
t
generating
function
of
X
.
Since
E
e
u(X
+Y
)+v
(X
 Y
)
=
E
e
(u+v
)X
+(u v
)Y
M
(
p
u)M
(
p
v
)
=
M
(u
+
v
)M
(u
 v
)
(:)
This
implies
that
Q(x)
=
ln
M
(x)
satises
Q(
p

u)
+
Q(
p
v
)
=
Q(u
+
v
)
+
Q(u
 v
)
(:)
Dieren
tiating
(.)
with
resp
ect
to
u
and
then
v
w
e
get
Q
00
(u
+
v
)
=
Q
00
(u
 v
)
Therefore
(tak
e
u
=
v
)
the
second
deriv
ativ
e
Q
00
(u)
=
Q
00
(0)
=
const

0.
This
means
M
(u)
=
exp
(
u
+

u

).



CHAPTER
.
NORMAL
DISTRIBUTION
.
Biv
ariate
Normal
distribution
Denition
..
We
say
that
X
;
Y
have
jointly
normal
distribution
(bivariate
normal),
if
aX
+
bY
is
normal
for
al
l
a;
b

I
R
.
If
E
X
=
E
Y
=
0,
the
momen
t
generating
function
M
(t;
s)
=
E
e
tX
+sY
is
giv
en
b
y
M
(t;
s)
=
e


(


t

+ts



+s

t

)
Clearly



=
V
ar
(X
);



=
V
ar
(Y
);

=
C
ov
(X
;
Y
).
When

=

the
join
t
densit
y
of
X
;
Y
exists
and
is
giv
en
b
y
f
(x;
y
)
=

q

(
 

)



exp
 x

 y

 



xy






(
 

)
:
(:)
Example
.
If
X
;
Y
ar
e
jointly
normal
with
c
orr
elation
c
o
ecient

=

then
the
c
onditional
distribution
of
Y
given
X
is
normal.
.
Questions
Problem
.
Show
that
if
X
;
Y
ar
e
indep
endent
and
normal,
then
X
+
Y
is
normal.
(Hint:
moment
gener
ating
functions
ar
e
e
asier
than
c
onvolution
formula
(.).)
Problem
.
If
X
;
Y
ar
e
indep
endent
normal
N
(0;
),
nd
the
density
of
X

+
Y

.
(Hint:
c
ompute
cumulative
distribution
function,
inte
gr
ating
in
p
olar
c
o
or
dinates.)
Problem
.
F
or
jointly
normal
X
;
Y
show
that
E
(Y
jX
)
=
aX
+
b
is
line
ar.
Problem
.
If
X
;
Y
ar
e
jointly
normal
then
Y
 X

Y
=
X
and
X
ar
e
indep
endent.
Problem
.
If
X
;
Y
ar
e
jointly
normal
with
varianc
es


X
;


Y
and
the
c
orr
elation
c
o-
ecient
,
then
X
=

X
(

cos

+


sin

,
Y
=

Y
(

sin

+


cos

,
wher
e

j
ar
e
indep
endent
N
(0;
)
and
sin

=
.

Chapter

Limit
theorems
This
is
a
short
c
hapter
on
asymptotic
b
eha
vior
of
sums
and
a
v
erages
of
indep
enden
t
observ
ations.
Theorem
..
justies
sim
ulations
as
the
means
for
computing
probabilities
and
exp
ected
v
alues.
Theorem
..
pro
vides
error
estimates.
.
Sto
c
hastic
Analysis
There
are
sev
eral
dieren
t
concepts
of
con
v
ergence
of
random
v
ariables.
Denition
..
X
n
!
X
in
pr
ob
ability,
if
Pr
(jX
n
 X
j
>
)
!
0
for
al
l

>
0
Example
.
L
et
X
n
b
e
N
(0;

=

n
).
Then
X
n
!
o
in
pr
ob
ability.
Denition
..
X
n
!
X
almost
sur
ely,
if
Pr(X
n
!
X
)
=
.
Example
.
L
et
U
b
e
the
uniform
U(0,)
r.
v.
Then

n
U
!
0
almost
sur
ely.
Example
.
L
et
U
b
e
the
uniform
U(0,)
r.
v.
et
X
n
=
(
0
if
U
>

n

otherwise
.
Then
X
n
!
0
almost
sur
ely.
Denition
..
X
n
!
X
in
L

(me
an
squar
e),
if
E
jX
n
 X
j

!
0
as
n
!
.
Remark

If
X
n
!
X
in
L

,
then
by
Chebyshev's
ine
quality
X
n
!
X
in
pr
ob
ability.
If
X
n
!
X
almost
sur
ely,
then
X
n
!
X
in
pr
ob
ability.
.
La
w
of
large
n
um
b
ers
Eac
h
la
w
of
large
n
um
b
ers
(there
are
man
y
of
them)
states
that
empirical
a
v
erages
con-
v
erge
to
the
exp
ected
v
alue.
In
statistical
ph
ysics
the
la
w
of
large
n
um
b
ers
impies
that
tra
jectory
a
v
erages
and
p
opulation
a
v
erages
are
asymptoticaly
the
same.
In
sim
ulations,
it
pro
vides
a
theoretical
foundation,
and
connects
the
frequency
with
the
probabilit
y
of
an
ev
en
t.



CHAPTER
.
LIMIT
THEOREMS
Theorem
..
Supp
ose
X
k
ar
e
such
that
E
X
k
=
;
V
ar
(X
k
)
=


,
and
cov
(X
i
;
X
j
)
=
0
for
al
l
i
=
j
.
Then

n
P
n
j
=
X
j
!

in
L

Pro
of.
T
o
sho
w
that

n
P
n
j
=
X
j
!

in
mean
square,
compute
the
v
ariance.

Corollary
..
If
X
n
is
Binomial
B
in(n;
p)
then

n
X
n
!
p
in
pr
ob
ability.
..
Strong
la
w
of
large
n
um
b
ers
The
follo
wing
metho
d
can
b
e
used
to
pro
v
e
strong
la
w
of
large
n
um
b
ers
for
Binomial
r.
v.
.
If
X
is
B
in(n;
p),
use
momen
t
generating
function
to
sho
w
that
E
(X
 np)


C
n

.
Use
Cheb
yshev's
inequalit
y
and
fourth
momen
ts
to
sho
w
that
P
n
Pr
(jX
n
=n
 pj
>
)
<
.
.
Use
the
con
v
ergence
of
the
series
to
sho
w
that
lim
N
!
Pr(
S
nN
fjX
n
=n
 pj
>
g)
=
0.
.
Use
the
con
tin
uit
y
of
the
probabilit
y
measure
to
sho
w
that
Pr(
T
N
S
nN
fjX
n
=n
 pj
>
g)
=
0.
.
Sho
w
that
with
probabilit
y
one
for
ev
ery
rational

>
0
there
is
N
=
N
()
suc
h
that
for
all
n
>
N
the
inequalit
y
jX
n
=n
 pj
<

holds.
Hin
t:
if
Pr(A

)
=

for
rational
,
then
Pr(
T

A

)
=
.
.
Con
v
ergence
of
distributions
In
addition
to
the
t
yp
es
of
con
v
ergence
in
tro
duced
in
Section
.,
w
e
also
ha
v
e
the
c
on-
ver
genc
e
in
distribution.
Denition
..
X
n
c
onver
ges
to
X
in
distribution,
if
E
f
(X
n
)
!
E
f
(X
)
for
al
l
b
ounde
d
c
ontinuous
functions
f
.
Theorem
..
If
X
n
!
X
in
distribution,
then
Pr(X
n

(a;
b))
!
Pr
(X

(a;
b))
for
al
l
a
<
b
such
that
Pr(X
=
a)
=
Pr(X
=
b)
=
0.
Theorem
..
If
M
X
n
(u)
!
M
X
(u)
for
al
l
u,
then
X
n
!
X
in
distribution.
Theorem
..
states
con
v
ergence
in
distribution
to
P
oisson
limit.
Here
is
a
pro
of
that
uses
momen
t
generating
functions.
Pro
of
of
Theorem
...
M
X
n
(u)
=
(
+
p
n
(e
u
 ))
n
!
e
(e
u
 )
.


..
CENTRAL
LIMIT
THEOREM

.
Cen
tral
Limit
Theorem
W
e
state
rst
normal
appro
ximation
to
binomial.
Theorem
..
If
X
n
is
Binomial
B
in(n;
p)
and
0
<
p
<

is
c
onstant,
then
X
n
 np
p
npq
!
Z
in
distribution
to
N
(0;
)
r
andom
variable
Z
.
Pro
of.
F
or
p
=
=
only
.
The
momen
t
generating
function
of
X
n
 np
p
npq
=
X
n
 n
p
n
is
M
n
(u)
=
e
 p
nu
(


+


e
 u=
p
n
)
n
=
(
e
 u=
p
n
+e
u=
p
n

)
n
=
cosh
n
(u=
p
n
)
!
e
u

=
.
(Can
y
ou
justify
the
limit?)

`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
`
^
p
n
n
=

Figure
.:
Empirical
prop
ortions
^
p
n
as
the
function
of
sample
size.
Output
of
LIMTHS.BAS
for
p
=
:
and


n

0
Limit
theorems
are
illustrated
b
y
the
program
LIMTHS.BAS.
This
program
graphs
empirical
prop
ortions
^
p
t
as
the
(random)
function
of
t

n,
mak
es
a
histogram,
and
compares
it
with
the
Normal
and
P
oisson
histograms.
By
trying
v
arious
lengths
of
path
n,
one
can
see
the
almost
sure
La
w
of
Large
Num
b
ers,
and
for
mo
derate
n
see
the
normal
appro
ximation.
Problem
.
The
gr
aph
of
^
p
n
as
the
function
of
n
as
given
by
LIMTHS.BAS
suggests
a
p
air
of
\curves"
b
etwe
en
which
the
aver
ages
ar
e
\sque
eze
d".
What
ar
e
the
e
quations
of
these
curves?
Exercise
.
Supp
ose
a
p
ol
l
of
size
n
is
to
b
e
taken,
and
the
actual
pr
op
ortion
of
the
voters
supp
orting
an
issue
in
question
is
p
=


.
Determine
the
size
n
such
that
the
observe
d
pr
op
ortion
^
p
=

n
X
satises
Pr(
^
p
>
:)

:0.
Exercise
.
Plot
the
histo
gr
am
for
a
00
indep
endent
Binomial
B
in(n
=
00;
p
=
:)
r
andom
variables.


CHAPTER
.
LIMIT
THEOREMS
Theorem
..
If
X
j
ar
e
i.i.d.
with
E
X
=
;
V
ar
(X
)
=


then
P
n
j
=
X
j
 n

p
n
!
Z
Pro
of.
F
or

=
0;

=

only
.
The
momen
t
generating
function
is
M
n
(u)
=
(M

(u=
p
n
))
n
=
(
+
u
p
n
M
0
(0)
+
u

n
M
00
(0)
+
O
(

n

))
n
!
e
u

=

A
lesser
kno
wn
asp
ect
of
the
cen
tral
limit
theorem
is
that
one
can
actually
sim
ulate
paths
of
certain
con
tin
uous
time
pro
cesses
b
y
taking
X
n
(t)
=

p
n
P
k
nt

k
,
where

k
are
indep
enden
t
mean-zero
r.
v.
The
follo
wing
program
uses
this
to
sim
ulate
random
curv
es.
The
program
requires
a
graphics
card
on
the
PC.
PROGRAM
firewalk.bas
'
'This
program
simulates
random
walk
paths
(with
uniform
incerments)
'reflected
at
the
boundaries
of
a
region
'
'declarations
of
subs
DECLARE
SUB
CenterPrint
(Text$)
'
minimal
error
handling
-
graphics
card
is
required
ON
ERROR
GOTO
ErrTrap
CLS
'request
good
graphics
(some
cards
support
SCREEN
,
etc)
SCREEN
	
LOCATE
,

'title
CenterPrint
"Path
of
reflected
random
walk"
LOCATE
	,

'timer
location
PRINT
"Timer"
scale
=
0
'
WINDOW
(0,
0)-(scale,
scale):
VIEW
(0,
00)-(00,
00)
LINE
(0,
0)-(scale,
scale),
0,
B':
LINE
(scale,
scale)-(
*
scale,
0),
,
B
FOR
j
=
-
TO

LINE
(0,
scale
/

+
j)-(scale
/
0,
scale
/

+
j),

NEXT
j
X
=
scale
/

Y
=
scale
/

dead
=
0
T
=
0
col
=

*
RND()
speed
=
scale
/
00
WHILE
INKEY$
=
""
'infinite
loop
until
a
key
is
pressed
T
=
T
+

X0
=
X
Y0
=
Y
X
=
X
+
(RND()
-

/
)
*
speed
Y
=
Y
+
(RND()
-

/
)
*
speed
IF
X
<
0
THEN
X
=
-X:
col
=

*
RND()
IF
Y
<
0
THEN
Y
=
-Y:
col
=

*
RND()
IF
X
>
scale
THEN
X
=

*
scale
-
X:
col
=

*
RND()

..
LIMIT
THEOREMS
AND
SIMULA
TIONS

IF
Y
>
scale
THEN
Y
=

*
scale
-
Y:
col
=

*
RND()
IF
X
>

*
scale
THEN
X
=

*
scale
-
X:
col
=

*
RND()
LINE
(X0,
Y0)-(X,
Y),
col
LOCATE
0,

PRINT
T
WEND
END
ErrTrap:
'if
there
are
errors
then
quit
CLS
PRINT
"This
error
requires
graphics
card
VGA"
PRINT
"Error
running
program"
PRINT
"Press
any
key
...'"
WHILE
INKEY$
=
""
WEND
END
SUB
CenterPrint
(Text$)
'
Print
text
centered
in
0
column
screen
offset
=

-
LEN(Text$)
\

IF
offset
<

THEN
offset
=

LOCATE
,
offset
PRINT
Text$
'
END
SUB
.
Limit
theorems
and
sim
ulations
One
role
of
limit
theorems
is
to
justify
the
sim
ulations
and
pro
vide
error
estimates.
The
sim
ulation
presen
ted
on
page

uses
the
Cen
tral
Limit
Theorem
to
prin
t
out
the
so
called
	%
condence
in
terv
al.
Cen
tral
limit
theorem
is
also
a
basis
for
a
fast
sim
ulation
of
the
normal
distribution,
see
Section
...
.
Large
deviation
b
ounds
W
e
b
egin
with
the
b
ound
for
binomial
distribution.
Recall
that
the
relativ
e
en
trop
y
is
H
(q
jp)
=
 q
ln
q
=p
 (
 q
)
ln
 q
 p
.
Theorem
..
Supp
ose
X
n
is
Binomial
B
in(n;
p).
Then
for
p
0

p
Pr
(

n
X
n

p
0
)

exp
nH
(p
0
jp)
(:)
Pro
of.
Use
Cheb
yshev's
inequalit
y
(.)
and
the
momen
t
generating
function.
Pr(X
n

np
0
)
=
Pr(uX
n

unp
0
)

e
 np
0
u
E
uX
n
=

e
 p
0
u
(
 p
+
pe
u
)

n
=

(
 p)e
 p
0
u
+
pe
( p
0
)u

n
.


CHAPTER
.
LIMIT
THEOREMS
No
w
the
calculus
question:
for
what
u

0
the
function
f
(u)
=
(
 p)e
 p
0
u
+
pe
( p
0
)u
attains
a
minim
um?
The
answ
er
is
u
=
0
if
p
0

p,
or
u
=
ln(
 p
p
p
0
 p
0
).
Therefore
the
minimal
v
alue
is
f
(u
min
)
=
p
p
0
e
( p
0
)u
=
exp
( p
0
ln
p
0
=p
 (
 p
0
)
ln
(
 p
0
)=(
 p))

Corollary
..
If
p
=
=
then
Pr
(j

n
X
n
 

j
>
t)

e
 nt

=
(:)
Pro
of.
This
follo
ws
from
the
inequalit
y
(
+
x)
ln
(
+
x)
+
(
 x)
ln
(
 x)

x

.

.
Conditional
limit
theorems
Supp
ose
X
j
are
i.i.d.
Conditional
limit
theorems
sa
y
what
is
the
conditional
distribution
of
X

,
giv
en
the
v
alue
of
the
empirical
a
v
erage

n
P
n
j
=
h(X
j
).
Suc
h
probabilities
are
dicult
to
sim
ulate
when

n
P
n
j
=
h(X
j
)
diers
signican
tly
from
E
h(X
).
.
Questions
Exercise
.
A
00-p
oint
multiple
choic
e
test
has
four
p
ossible
r
esp
onses,
one
of
which
is
c
orr
e
ct.

What
pr
op
ortion
of
students
that
just
guess
the
answer
gets
the
sc
or
e
of
mor
e
than
00?
Of
mor
e
than
0?
Of
mor
e
than
0?

What
pr
op
ortion
of
students
that
know
how
to
answer
c
or
e
ctly
0%
of
question
gets
the
sc
or
e
of
mor
e
than
00?
of
mor
e
than
0?
Of
mor
e
than
0?

What
pr
op
ortion
of
pr
oblems
you
know
how
to
answer
c
or
e
ctly
in
or
der
to
have
a
fair
shot
at
A,
which
r
e
quir
es
a
sc
or
e
of
at
le
ast
?
(F
or
the
purp
ose
of
this
exer
cise
a
fair
shot
is
%
chanc
e.)

P
art
I
I
Sto
c
hastic
pro
cesses
	


Chapter

Sim
ulations
This
c
hapter
collects
information
ab
out
ho
w
to
sim
ulate
sp
ecial
distributions.
Considerations
of
mac
hine
eciency
,
in
particular
con
v
enience
of
doing
xed
precision
arithmetics,
mak
e
the
uniform
U
(0;
)
the
fundamen
tal
building
blo
c
k
of
sim
ulations.
W
e
do
not
consider
in
m
uc
h
detail
ho
w
suc
h
sequences
are
pro
duced
{
ecien
t
metho
ds
are
hardw
are-dep
enden
t.
W
e
also
assume
these
are
i.
i.
d.,
ev
en
though
the
t
ypical
random
n
um
b
er
generator
returns
the
same
pseudo-random
sequence
for
eac
h
v
alue
of
the
seed,
and
often
the
sequence
is
p
erio
dic
and
correlated.
This
is
again
a
question
of
sp
eed
and
hardw
are
only
.
.
Generating
random
n
um
b
ers
Supp
ose
x
0
is
an
arbitrary
n
um
b
er
b
et
w
een
0
and

with

decimal
places
or
more.
Let
x

=
fx
0
g,
and
x
n+
=
fx
n
g,
where
fag
=
a
 [a]
denotes
the
fractional
part.
Here
are
the
questions:
Is
x
n
a
random
sequence?
Do
es
it
ha
v
e
\enough"
prop
ertries
of
a
random
sequence
to
b
e
used
for
sim
ulations?
..
Random
digits
The
classical
P
eano
curv
e
actually
maps
the
unit
in
terv
al
on
to
the
unit
square
preserving
the
Leb
esgue
measure.
Th
us
t
w
o
indep
enden
t
U
(0;
)
are
as
\random"
as
one!
Exp
erimen
ts
with
discrete
outcomes
aren't
necessarily
less
random
than
con
tin
uous
mo
dels.
Expansion
in
to
binary
fractions
connects
innite
tosses
of
a
coin
with
a
single
uniform
r.
v.
Example
.
L
et
U
b
e
uniform
U
(0;

).
R
andom
variables
X
k
=
sig
n(sin
(
k
U
))
ar
e
i.
i.
d.
symmetric
indep
endent.
Theorem
..
If

j
ar
e
indep
endent
identic
al
ly
distribute
d
discr
ete
r
andom
variables
with
values
f0;
g
and
Pr(
=
)
=
=
then
P

k
=


k

k
in
uniform
U
(0;
).
Pro
of.
W
e
sho
w
b
y
induction
that
if
U
is
indep
enden
t
of
f
j
g
uniform
U
(0;
)
r.
v.,
then


n
U
+
P
n
k
=


k

k
is
uniform
for
all
n

0.
F
or
induction
step,
notice
that
in
distribution


n
U
+
P
n
k
=


k

k

=




+


U
.
This
reduces
the
pro
of
to
n
=

case.



CHAPTER
.
SIMULA
TIONS
The
rest
of
the
pro
of
is
essen
tially
the
solution
of
Problem
..
Clearly
Pr(




+


U
<
x)
=
Pr(

=
0)
Pr(U
=
<
x)
+
Pr
(

=
)
Pr(


+
U
=
<
x)
Expression
Pr(U
=
<
x)
is
0;
x;
or
.
Expression
Pr(


+
U
=
<
x)
is
0;
x
 ;
or
.
Their
a
v
erage
(c
hec
k
carefully
ranges
of
x!)
is

>
<
>
:
0
if
x
<
0
x
if
0

x



if
x
>


Co
ecien
t

do
es
not
pla
y
an
y
sp
ecial
role.
The
same
fact
holds
true
in
an
y
n
um
b
er
system
-
if
N
>

is
xed,
and
U
=
P
X
j
N
 j
is
expanded
in
pase
N
,
then
X
j
are
indep
enden
t
uniform
f0;
;
:
:
:
;
N
 g-v
alued
discrete
random
v
ariables.
F
rom
the
mathematical
p
oin
t
of
view
all
of
the
sim
ulations
can
b
e
based
on
a
sin-
gle
U
(0;
)
random
v
ariable.
In
particular,
to
generate
indep
enden
tly
uniform
in
teger
n
um
b
ers

in
the
prescrib
ed
range
0
:
:
:
N
w
e
need
to
pic
k
an
y
u

(0;
),
and
dene
X
j
=
N
j
u
mo
d
N
.
Clearly
X
j
is
the
in
teger
part
of
N
U
j
,
where
U
j
solv
e
the
recurrence
U
j
+
=
fU
j
N
g
(:)
and
fg
denotes
the
fractional
part.
A
computer
realization
of
this
construction
w
ould
use
U
j
=
n
j
=
M
with
some
M
>
N
,
Th
us
n
j
+
=
N
n
j
mo
d
M
.
Man
y
programs
pro
vide
access
to
uniform
n
um
b
ers.
These
ho
w
ev
er
migh
t
b
e
platform-
dep
enden
t,
and
are
often
of
\lo
w
qualit
y".
Often
a
p
erson
p
erforming
sim
ulation
ma
y
w
an
t
to
use
the
co
de
they
ha
v
e
explicit
access
to.
What
is
\random
enough"
for
one
application
ma
y
not
b
e
random
enough
for
another.
..
Ov
erview
of
random
n
um
b
er
generators
There
is
go
o
d
evidence,
b
oth
theoretical
(see
(.))
and
empirical,
that
the
simple
m
ulti-
plicativ
e
congruen
tial
algorithm
n
j
+
=
an
j
(mo
d
N
)
(:)
can
b
e
as
go
o
d
as
the
more
general
linear
congruen
tial
generator.
P
ark
&
Miller
prop
ose
a
minimal
standard
generator
based
on
the
c
hoices
a
=


;
N
=


 .
The
computer
implemen
tation
of
this
metho
d
is
not
ob
vious
due
to
o
v
ero
ws
when
m
ultipp
ying
large
in
tegers
in
nite
computer
arithmetics,
see
Sc
hrage's
algorithm
in
[].
The
linear
congruen
tial
metho
d
has
the
adv
an
tage
of
b
eing
v
ery
fast.
It
has
the
disadv
an
tage
that
it
is
not
free
of
sequen
tial
correlations
b
et
w
een
successiv
e
outputs.
This
sho
ws
up
clearly
in
the
fact
that
the
consecutiv
e
k
-p
oin
ts
lie
in
at
most
N
=k
subspaces
of
dimension
k
 .
Man
y
system-supplied
random
n
um
b
er
generators
are
line
ar
c
ongruential
gener
ators,
whic
h
generate
a
sequence
of
in
tegers
n

;
n

;
:
:
:
eac
h
b
et
w
een
0
and
N
 
b
y
the
recurrence
relation
n
j
+
=
an
j
+
b
mo
d
N
(:)

This
is
all
w
e
can
hop
e
for
in
the
nite
computer
arithmetics.
Whenev
er
U
j

(0;
)
is
selected
and
nite
appro
ximation
is
c
hosen,
N
j
=
N
U
j
is
an
in
teger,
where
N
=

b
is
the
c
haracteristic
of
the
op
erating
system.

..
SIMULA
TING
DISCRETE
R.
V.

The
v
alue
of
N
is
the
largest
in
teger
represen
table
in
the
mac
hine.
This
is



on
-bit
mac
hines,
unless
long
in
tegers
are
used
(then
it
is


).
V
alue
used
in
C-supplied
generator
are
a
=
0;
b
=
;
N
=


.
.
Sim
ulating
discrete
r.
v.
..
Generic
Metho
d
{
discrete
case
Section
..
describ
ed
the
generic
metho
d
for
sim
ulating
a
discrete
random
v
ariables
with
nite
n
um
b
er
of
v
alues.
Namely
,
tak
e
X
=
f
(U
),
where
f
is
a
suitable
piecewise
constan
t
functions
on
the
in
terv
al
(0;
).
Let
Pr(X
=
v
k
)
=
p
k
.
Then
f
(x)
=
v
k
for
x

(
P
k
j
=
p
j
;
P
k
+
j
=
p
j
).
This
is
rather
easy
to
con
v
ert
in
to
the
computer
algorithm.
..
Geometric
A
simple
metho
d
for
sim
ulating
geometric
distribution
is
to
sim
ulate
indep
enden
t
binomial
trials
un
til
the
rst
success.
..
Binomial
A
simple
metho
d
for
sim
ulating
Binomial
B
in(n;
p)
random
v
ariable
is
to
sim
ulate
bino-
mial
trials
with
the
prescrib
ed
probabilit
y
of
success.
F
or
a
sample
program
illustrating
this
metho
d,
see
TOSSCOIN.BAS
page
.
..
P
oisson
An
exact
metho
d
to
sim
ulate
P
oisson
distribution
is
based
on
the
fact
that
it
o
ccurs
the
P
oisson
pro
cess,
and
that
so
journ
times
in
the
P
oisson
pro
cess
are
exp
onen
tial
(see
The-
orem
..
on
page
0).
Therefore,
to
sim
ulateX
with
P
oiss()
distribution,
sim
ulate
indep
enden
t
exp
onen
tial
r.
v.
T

;
T

;
:
:
:
;
with
parameter

=

and
put
as
the
v
alue
of
X
the
rst
v
alue
of
n
suc
h
that
T

+
:
:
:
+
T
n
>
.
A
reasonable
appro
ximation
to
P
oisson
P
oiss()
random
v
ariable
X
is
obtained
b
y
sim
ulating
binomial
B
in(n;
p)
random
v
ariable
X
0
with

=
np.
Since
X
0

n,
use
n
large
enough
to
exceed
an
y
realistic
v
alues
of
X
.
Run
program
LIMTHMS.BAS
to
compare
the
histograms
{
wh
y
is
P
oisson
distribution
more
spread
out
than
the
binomial?
.
Sim
ulating
con
tin
uous
r.
v.
..
Generic
Metho
d
{
con
tin
uous
case
Section
..
describ
ed
the
generic
metho
d
for
sim
ulating
a
con
tin
uous
random
v
ariable,
similar
to
the
metho
d
used
in
the
discrete
case.
Namely
,
tak
e
X
=
f
(U
)
where
f
is
the
in
v
erse

of
the
cum
ulativ
e
distribution
functions
F
(x)
=
Pr(X

x).

Actually
,
w
e
need
only
a
righ
t-in
v
erse,
i.e
a
function
suc
h
that
F
(f
(u))
=
u.


CHAPTER
.
SIMULA
TIONS
Problem
.
Given
a
cumulative
distribution
function
G(x)
with
inverse
H
(),
and
den-
sity
g
(x)
=
G
0
(x),
let
U

;
U

b
e
two
indep
endent
uniform
U
(0;
)
r
andom
variables.
Show
that
X
=
H
(U

);
Y
=
U

g
(X
)
ar
e
uniformly
distribute
d
in
the
r
e
gion
f(x;
y
)
:
0
<
y
<
g
(x)g.
..
Randomization
If
the
conditional
densit
y
of
Y
giv
en
X
=
x
is
f
(y
jx),
and
the
densit
y
of
X
is
g
(x),
then
the
densit
y
of
Y
is
R
(f
(y
jx)g
(x)
dx.
As
an
example,
supp
ose
Y
has
densit
y
cy
e
 y
=
C
P

n=0
y
(
 y
)
n
=n!.
Let
Pr(X
=
n)
=
c=n!.
Then
the
conditional
distribution
is
f
(y
jX
=
n)
=
C
y
(
 y
)
n
,
whic
h
is
the
distribution
of
a
median
from
the
sample
of
size
n.
..
Sim
ulating
normal
distribution
By
the
cen
tral
limit
theorem
(Theorem
..),
if
U

;
:
:
:
;
U
n
are
i.
i.
d.
uniform
U
(0;
)
then
q

n
P
n
k
=
(U
k
 

)
is
asymptotically
normal
N
(0;
).
In
particular
a
computer-ecien
t
appro
ximation
to
normal
distribution
is
giv
en
b
y

X
k
=
U
k
 :
The
exact
sim
ulation
of
normal
distribution
uses
the
fact
that
an
indep
enden
t
pair
X

;
X

of
normal
N
(0;
)
random
v
ariables
can
b
e
written
as
X

=
R
cos

(.)
X

=
R
sin

(.)
where

is
uniform
U
(0;

),
R
=
p
X

+
Y

is
exp
onen
tial
(see
Problem
.)
with
parameter

=


,
and
random
v
ariables
;
R
are
indep
enden
t.
Clearly
R
=
p
 
ln
U
,
see
Example
..
W
e
sim
ulate
t
w
o
indep
enden
t
normal
N
(0;
)
r.
v.
from
t
w
o
indep
enden
t
uniform
r.
v.
U

;
U

b
y
taking

=

U

;
R
=
p
 
ln
U

and
using
form
ulas
(.)-(.).
.
Rejection
sampling
The
idea
of
rejection
metho
d
is
v
ery
simple.
In
order
to
sim
ulate
a
random
v
ariable
with
the
densit
y
f
(x),
select
a
p
oin
t
X
;
Y
at
random
uniformly
from
the
region
f(x;
y
)
:
y
<
g
(x)g.
Then
Pr(X
<
x)
=
R
x
 
f
(t)
dt
is
the
cum
ulativ
e
distribution
function,
whic
h
w
e
do
not
ha
v
e
to
kno
w
analytically
.
The
name
comes
from
the
tec
hnique
suggested
b
y
Problem
..
Instead
of
selecting
p
oin
ts
under
the
graph
of
f
(x),
w
e
pic
k
another
function
g
(x)
>
f
(x),
whic
h
has
kno
wn
an
tideriv
ativ
e
with
explicitly
a
v
ailable
in
v
erse.
W
e
pic
k
p
oin
ts
under
the
graph
of
g
(x),
and
"reject"
those
that
didn't
mak
e
it
b
elo
w
the
graph
of
f
(x).
Often
used
densit
y
g
(x)
=
c=(
+
x

)
leads
to
X
=
H
(U

),
where
H
(u)
=
tan(
u=c).

..
SIMULA
TING
DISCRETE
EXPERIMENTS

Rejection
sampling
can
b
e
used
to
sim
ulate
con
tin
uous
or
discrete
distributions.
The
idea
b
ehind
using
it
in
discrete
case
is
to
con
v
ert
discrete
distribution
to
a
function
of
con-
tinous
random
v
ariable.
F
or
example,
to
use
rejection
sampling
for
P
oisson
distribution,
sim
ulate
the
densit
y
f
(x)
=
e
 

[x]
=[x]!
and
tak
e
the
in
teger
part
[X
]
of
the
resulting
random
v
ariable.
.
Sim
ulating
discrete
exp
erimen
ts
..
Random
subsets
T
o
sim
ulate
uniformly
selected
random
subsets
of
f;
:
:
:
;
ng,
dene
sets
b
y
a
sequences
S
(j
)

f0;
g
with
the
in
terpretation
j

S
if
S
(j
)
=
.
No
w
select
indep
enden
tly
0
or

with
probablit
y


for
eac
h
of
the
v
alues
of
S
(j
);
j
=
;
:
:
:
;
n.
SUB
RandomSubset(
S())
n
=
UBOUND(S)
'
read
out
the
size
of
array
FOR
j
=

TO
n
'select
entries
at
random
S(j)
=
INT(RND()
+
)
NEXT
j
F
or
subsets
of
lo
w
dimension,
a
sequence
of
0;

can
b
e
iden
tied
with
binary
n
um
b
ers.
Set
op
erations
are
then
easily
con
v
erted
to
binary
op
erations
on
in
tegers/long
in
tegers.
..
Random
P
erm
utations
Supp
ose
w
e
w
an
t
to
re-arrange
elemen
ts
a();
:
:
:
;
a(n)
in
to
a
random
order.
A
quic
k
metho
d
to
accomplish
this
goal
is
to
pic
k
one
elemen
t
at
a
time,
and
set
it
aside.
This
can
b
e
easily
implemen
ted
within
the
same
sequence.
SUB
Rearrange(
A())
n
=
UBOUND(A)
'
read
out
the
size
of
array
ns
=
n
'initial
size
of
randomization
FOR
j
=

TO
n
'take
a
card
at
random
and
put
it
away
k
=
INT(RND()
*
ns
+
)
SWAP
A(k),
A(ns)
ns
=
ns
-

'select
from
remaining
a(j)
NEXT
j
Problem
.
L
et
a

;
:
:
:
a
n
b
e
numb
ers
such
that
P
j
a
j
=
0;
P
j
a

j
=

L
et
X
denote
the
sum
of
the
rst
half
of
those
numb
ers,
after
a
r
andom
r
e
arr
angement.

Find
E
(X
),
V
ar
(X
).

Under
suitable
c
onditions,
as
n
!
,
the
distribution
of
X
is
asymptotic
al
ly
nor-
mal.
V
erify
by
simulations.

L
et
Y
b
e
the
sum
of
the
rst


of
a
j
after
r
andom
r
e
arr
angement.
Find
E
(X
jY
)
and
E
(Y
jX
).


CHAPTER
.
SIMULA
TIONS
.
In
tegration
b
y
sim
ulations
Program
listing
on
page

explains
ho
w
to
p
erform
double
in
tegration
b
y
sim
ulation.
Here
w
e
concen
trate
on
rening
the
pro
cedure
for
single
in
tegrals
b
y
reducing
the
v
ariance.
T
o
ev
aluate
J
=
R
b
a
f
(x)
dx
w
e
ma
y
use
the
appro
ximation

N
P
N
j
=
f
(X
j
)=g
(X
j
),
where
X
j
are
i.i.d.
with
the
densit
y
g
(x)
suc
h
that
R
b
a
g
(x)
dx
=
.
The
error,
as
measured
b
y
the
v
ariance

,
is
V
ar
0
@

N
N
X
j
=
f
(X
j
)=g
(X
j
)

A
=

p
N
Z
b
a
 
f
(x)
g
(x)
 J
!

dx:
The
v
ariance
is
the
smallest
if
g
(x)
=
C
jf
(x)j,
therefore
a
go
o
d
appro
ximation
g
to
func-
tion
f
will
reduce
the
v
ariance

.
This
pro
cedure
of
reducing
v
ariance
is
called
imp
ortanc
e
sampling.
F
or
smo
oth
functions,
a
b
etter
appro
ximation
is
obtained
b
y
selecting
p
oin
ts
more
uniformly
than
the
pure
random
c
hoice.
The
so
called
Sob
ol
se
quenc
es
are
based
on
sophisticated
mathematics.
Co
okb
o
ok
prescriptions
can
b
e
found
eg
in
Numerical
recip
es.
Note:
The
reliabilit
y
of
the
Mon
te
Carlo
Metho
d,
and
the
asso
ciated
error
b
ounds
dep
ends
on
the
qualit
y
of
the
random
n
um
b
er
generator.
It
is
rather
un
wise
to
use
an
unkno
wn
random
n
um
b
er
generator
in
questions
that
require
large
n
um
b
er
of
randomiza-
tions.
Example
.
The
fol
lowing
pr
oblem
is
a
chal
lenge
to
any
numeric
al
inte
gr
ation
metho
d
due
to
r
apid
oscil
lations,
R

0

sin

(000x)
dx
=

 
000
sin
000

:000
..
Stratied
sampling
The
idea
of
stratied
sampling
is
to
select
dieren
t
n
um
b
er
of
p
oin
ts
from
dieren
t
sub-
regions.
As
a
simple
example,
supp
ose
w
e
w
an
t
to
in
tegrate
a
smo
oth
function
o
v
er
the
in
terv
al
[0;
]
using
n
p
oin
ts.
Instead
of
follo
wing
with
the
standard
Mon
te
Carlo
pre-
scription,
w
e
can
divide
[0;
]
in
to
k
non-o
v
erlapping
segmen
ts
and
c
ho
ose
n
j
p
oin
ts
from
the
j
-th
subin
terv
al
I
j
.
An
extreme
case
is
to
tak
e
k
=
n
and
n
j
=

{
this
b
ecomes
a
v
arian
t
of
the
trap
ezoidal
metho
d.
The
optimal
c
hoice
of
n
j
is
to
select
them
prop
or-
tional
to
the
lo
cal
standard
deviation
of
the
usual
Mon
te
Carlo
estimate
of
R
I
j
f
(x)
dx.
Indeed,
denoting
b
y
F
j
the
estimator
of
the
in
tegral
o
v
er
I
j
,
the
v
ariance
of
the
answ
er
is
V
ar
(
P
k
j
=
F
j
)
=
P
j
V
ar
(F
j
)
=
P
j


j
=n
j
.
The
minim
um
under
the
constrain
t
P
j
n
j
=
n
is
n
j


j
.
A
simple
v
arian
t
of
recursiv
e
stratied
sampling
is
to
generate
p
oin
ts
and
sub
divisions
based
on
estimated
v
alues
of
the
v
ariances.
.
Mon
te
Carlo
estimation
of
small
probabilities
Unlik
ely
ev
en
ts
happ
en
to
o
rarely
to
ha
v
e
an
y
reasonable
hop
e
of
sim
ulating
them
directly
.
Under
suc
h
circumstances
a
sp
ecial
metho
d
of
sele
ctive
sampling

w
as
dev
elop
ed.

Wh
y
v
ariance?

The
smallest
p
ossible
v
ariance
is
0!
Wh
y
do
esn't
this
happ
en
in
the
actual
application??

See
J.
S.
Sado
wski,
IEEE
T
rans.
Inf.
Th.
IT-	
(		)
pp.
	{,
and
the
references
therein.

..
MONTE
CARLO
ESTIMA
TION
OF
SMALL
PR
OBABILITIES

Example
Supp
ose
w
e
w
an
t
to
nd
Pr(X

+
:
:
:
+
X
n
>

n)
for
large
n
and
a
giv
en
densit
y
f
(x)
of
indep
enden
t
r.
v.
X
.
Consider
instead
indep
enden
t
random
v
ariables
Y
j
with
the
\tilted
densit
y"
C
e
x
f
(x),
where
C
is
the
normalizer,
and

is
suc
h
that
E
Y
=

.
By
the
la
w
of
large
n
um
b
ers
(Theorem
..),
the
ev
en
t
Y

+
:
:
:
+
Y
n
>

n
has
large
probabilit
y
,
and
Pr(X

+
:
:
:
+
X
n
>

n)
=
R
y

+:::
+y
n
>n
e
 y

:
:
:
e
 y
n
f
(y

)
:
:
:
f
(y
n
)
dy

:
:
:
dy
n
.
This
leads
to
the
follo
wing
pro
cedure.

Sim
ulate
N
indep
enden
t
realizations
of
the
sequence
Y

;
:
:
:
;
Y
n
.

Discard
those
that
do
not
satisfy
the
constrain
t
y

+
:
:
:
+
y
n
>

n.

Av
erage
the
expression
e
 Y

:
:
:
e
 Y
n
o
v
er
the
remaining
realizations
to
get
the
desired
estimate.
Example
.
Supp
ose
X
j
ar
e
f0;
g-value
d
so
that
X

+
:
:
:
X
n
is
binomial
B
in(n;
p).
What
is
the
distribution
of
Y
j
?
Simplify
the
expr
ession
e
 Y

:
:
:
e
 Y
n
.
Exercise
.
Write
the
pr
o
gr
am
c
omputing
by
simulation
the
pr
ob
ability
that
in
a
n
=
0
tosses
of
a
fair
c
oin,
at
le
ast

he
ads
o
c
cur.
Onc
e
you
have
a
pr
o
gr
am
that
do
es
for
n
=
0
a
c
omp
ar
able
job
to
the
\naive"
pr
o
gr
am
b
elow,
try
Exer
cise
.
on
p
age
.
Here
is
a
naiv
e
program,
that
do
es
the
job
for
n
=
0,
but
not
for
n
=
00.
It
should
b
e
used
to
test
the
more
complex
\tilted
densit
y"
sim
ulator.
PROGRAM
heads.bas
'
'Simulating
N
fair
coins
'
declarations
DECLARE
FUNCTION
NumHeads%
(p!,
n%)
DEFINT
I-N
'
declare
integer
variables
'prepare
screen
CLS
PRINT
"Simulating
toss
of
n
fair
coins"
'get
users
input
n
=
0
'number
of
trials
INPUT
"Number
of
coins
n=",
n
pr
=
.
'
fairness
of
a
coin
frac
=
.
'
percentage
of
heads
seeked
'
get
the
frac
from
the
user
PRINT
"
How
often
we
get
more
than
f
heads?
(where
0<f<";
n;
")"
INPUT
"f=",
frac
IF
frac
>=

THEN
frac
=
frac
/
n
'rescale
if
too
large
'tell
user
what
is
going
on
PRINT
"Hit
any
key
to
see
the
final
answer"
LOCATE
0,
0
PRINT
"With
some
patience
you
may
see
digits
stabilize"
DO
'main
loop


CHAPTER
.
SIMULA
TIONS
T
=
T
+

IF
NumHeads(pr,
n)
>
frac
*
n
THEN
s
=
s
+

IF
INKEY$
>
""
THEN
EXIT
DO
LOCATE
0,
0
PRINT
"Trial
#";
T
PRINT
"Current
estimate";
USING
"##.#####";
s
/
T
LOOP
'print
the
answer
PRINT
PRINT
"Prob
of
more
then
";
INT(frac
*
n);
"
heads
in
";
n;
"
trials
is
about
";
s
/
T
END
DEFINT
A-H,
O-Z
FUNCTION
NumHeads
(p!,
n)
'simulate
a
run
of
n
coins
h
=
0
FOR
k
=

TO
n
IF
RND()
<
p!
THEN
h
=
h
+

NEXT
k
NumHeads
=
h
'
END
FUNCTION

Chapter

In
tro
duction
to
sto
c
hastic
pro
cesses
sto
c
hastic,
a.
conjectural;
able
to
conjecture
W
ebster's
New
Univ
ersal
Unabridged
Dictionary
Sto
c
hastic
pro
cesses
mo
del
ev
olution
of
systems
that
either
exhibit
inheren
t
random-
ness,
or
op
erate
in
an
unpredictable
en
vironmen
t.
This
unpredictabilit
y
ma
y
ha
v
e
more
than
one
form,
see
Section
..
Probabilit
y
pro
vides
mo
dels
for
analyzing
random
or
unpredictable
outcomes.
The
main
new
ingredien
t
in
sto
c
hastic
pro
cesses
is
the
explicit
role
of
time.
A
sto
c
hastic
pro
cess
is
describ
ed
b
y
its
p
osition
X
(t)
at
time
t

[0;
],
t

[0;
),
or
t

f0;
;
:
:
:
g.
F
rom
the
conceptual
p
oin
t
of
view,
sto
c
hastic
pro
cesses
that
use
discrete
momen
ts
of
time
t

f0;
;
:
:
:
g
are
the
simplest.
Since
the
discrete
momen
ts
of
time
can
represen
t
arbitrarily
small
time
incremen
ts,
discrete
time
mo
dels
are
ric
h
enough
to
mo
del
real-w
orld
phenomena.
Con
tin
uous
time
v
ersions
are
con
v
enien
t
mathematical
idealizations.
.
Dierence
Equations
Mathematical
mo
dels
of
time
ev
olution
of
deterministic
systems
often
in
v
olv
e
dieren
tial
equations.
Dierence
equations
are
discrete
analogues
of
the
dieren
tial
equations.
Dierence
equations
o
ccur
in
applied
problems,
and
also
when
solving
dieren
tial
equations
b
y
series
expansions,
or
b
y
Euler's
metho
d.
The
concepts
of
n
umerical
v
ersus
analytical
solution,
initial
v
alues,
b
oundary
v
alues,
linearit
y
,
and
sup
erp
osition
of
solutions
o
ccur
in
the
discrete
setup
in
complete
analogy
with
the
theory
of
dieren
tial
equations.
A
dierence
equation
determines
unkno
wn
sequence
(y
n
)
through
a
recurrence
relation
that
sp
ecies
the
pattern.
W
e
will
consider
only
sp
ecial
cases
of
classes
of
equation
y
n+
=
f
(n;
y
n
;
y
n 
;
:
:
:
;
y
n k
+
):
Here
co
ecien
t
k
is
the
or
der
of
the
equation.
F
or
instance,
y
n+
=
f
(n;
y
n
)
is
an
equation
of
order
,
y
n+
=
f
(n;
y
n
;
y
n 
)
is
an
equation
of
order
,
etc.
..
Examples
	

0
CHAPTER
.
INTR
ODUCTION
TO
STOCHASTIC
PR
OCESSES
Example
.
Supp
ose
a
se
quenc
e
y
n
is
to
satisfy
y
n+
=
cos
(y
n
);
(:)
wher
e
the
c
osine
function
is
in
r
adians.
It
is
easy
to
write
a
short
program
that
computes
the
v
alues
of
y
n
.
But
to
compute
the
actual
sequence
w
e
need
to
sp
ecify
the
initial
value
y
0
.
T
able
.
giv
es
sample
outputs
of
suc
h
a
program
for
sev
eral
c
hoices
of
y
0
.
y
0
y

y

y

y

y

y

y

y

-
.00
.
.	
.	0
.0
.		
.0
.0
-.
.
.	0
.0
.	
.	
.	
.
.00
0

.00
.
.	
.	0
.0
.		
.0
.
.
.	0
.0
.	
.	
.	
.
.00

.00
.
.	
.	0
.0
.		
.0
.0
.
.00
.				
.0
.	
.0
.		
.0
.0

-.
.	
.00
.	0
.0
.		
.
.	
T
able
.:
Sequences
y
n
satisfying
equation
(.)
with
dieren
t
initial
v
alues
y
0
.
Similar
n
umerical
pro
cedures
sho
w
up
in
dieren
tial
equations,
where
they
are
used
to
appro
ximate
con
tin
uous
solutions
b
y
discretizing
time.
The
pro
cedure
is
called
Euler
metho
d,
or
tangent
line
metho
d.
Example
Some
dierence
equations
are
simpler
than
others.
Supp
ose
a
sequence
y
n
is
to
satisfy
y
n+
=
y
n
+
d;
(:)
where
d
is
a
giv
en
n
um
b
er.
It
is
easy
to
write
a
short
program
that
computes
v
alues
of
y
n
.
T
o
compute
them
w
e
again
need
to
sp
ecify
the
initial
v
alue
y
0
.
On
the
other
hand,
w
e
ma
y
notice
that
equation
(.)
denes
the
arithmetic
progres-
sion.
Instead
of
a
table
lik
e
T
able
.,
w
e
can
write
do
wn
the
solution
for
all
p
ossible
v
alues
of
y
0
and
for
all
n.
Namely
,
y
n
=
y
0
+
dn:
(:)
In
general,
an
analytic
al
solution
of
the
dierence
equation
is
the
form
ula
that
expresses
y
n
as
the
function
of
n.
This
should
b
e
con
trasted
with
the
numeric
al
solution
whic
h
is
an
algorithm,
or
computer
program,
that
computes
v
alues
of
y
n
.
The
gener
al
solution
is
the
function
of
b
oth
y
0
and
n,
as
con
trasted
with
a
p
articular
solution
that
w
orks
for
a
prescrib
ed
initial
v
alue

y
0
only
.

Suc
h
as
y
0
=
0.

..
LINEAR
DIFFERENCE
EQUA
TIONS

Example
.
Her
e
is
another
wel
l
known
dier
enc
e
e
quation
with
obvious
solution.
Sup-
p
ose
y
n+
=
r
y
n
;
(:)
wher
e
r
is
a
given
numb
er.
Equation
(.)
denes
a
ge
ometric
pr
o
gr
ession
and
its
solution
is.
y
n
=
y
0
r
n
(:)
Examples
..
and
.
are
deceptiv
ely
simple.
Not
ev
ery
dierence
equation
has
a
simple
or
easy
to
guess
answ
er.
Example
The
follo
wing
equation
denes
the
Fib
onacci
sequence
y
n+
=
y
n
+
y
n 
:
(:)
In
a
t
ypical
application,
y
n
denotes
the
n
um
b
er
of
rabbits
at
the
end
of
the
nth
mon
th.
In
particular,
if
w
e
buy
one
newb
orn
rabbit
at
the
b
eginning
of
the
rst
mon
th
then
the
rst
terms
of
the
sequence
are
easy

to
write
do
wn:
,
,
,
,
,
,
,
,
:
:
:
Without
m
uc
h
dicult
y
this
can
b
e
con
v
erted
to
a
computer
program
and
used
to
answ
er
questions
lik
e
When
wil
l
the
p
opulation
of
r
abbits
exc
e
e
d

mil
lion?
The
general
expression
(solution)
of
the
equation
corresp
onding
to
this
situation
is
giv
en
b
y
the
follo
wing
form
ula.
y
n
=

p

 

+
p


!
n
 
p

 

 p


!
n
This
is
the
outcome
of
the
standard
computation!
.
Linear
dierence
equations
Man
y
in
teresting
dierence
equations,
including
(.),
(.),
(.)
fall
in
to
the
category
of
linear
dierence
equations
with
constan
t
co
ecien
ts.
The
rst
order
linear
dierence
equations
with
constan
t
co
ecien
ts
has
the
form
y
n+
+
ay
n
=
g
(n):
The
second
order
linear
dierence
equations
with
constan
t
co
ecien
ts
has
the
form
y
n+
+
ay
n+
+
by
n
=
g
(n):
A
metho
d
to
solv
e
dierence
equation
of
order

consists
of
the
follo
wing
steps:

F
ollo
wing
go
o
d
mathematical
practise,
w
e
simplify
the
real
w
orld
and
assume
that
a
single
mature
rabbit
will
pro
duce
one
ospring
ev
ery
mon
th.


CHAPTER
.
INTR
ODUCTION
TO
STOCHASTIC
PR
OCESSES

First
w
e
solv
e
the
\homogeneous
equation"
y
n+
+
ay
n+
+
by
n
=
0.
This
is
ac-
complished
through
the
substitution
y
n
=
r
n
,whic
h
leads
to
the
c
haracteristic
equation
r

+
ar
+
b
=
0
for
r
.
Once
t
w
o
ro
ots
are
found,
the
general
solu-
tion
is
y
n
=
C

r
n

+
C

r
n

.
The
notable
exception
when
the
ro
ots
are
equal
is
y
n
=
C

r
n
0
+
C

nr
n
0
.

Secondly
,
w
e
nd
an
y
solution
of
the
non-homogeneous
equation,
disregarding
initial
conditions.
This
can
b
e
accomplished
b
y
the
metho
d
of
v
arying
parameters:
try
y
n
=
C

(n)r
n

+
C

(n)r
n

.
Or
b
y
guessing.
In
the
often
encoun
tered
p
olynomial
case
g
(x)
=
n
`
the
trial
solution
y
n
=
n
s
(u
0
+
u

n
+
:
:
:
+
u
`
n
`
)
will
w
ork
(s
=
0
except
when
y
n
=
n
solv
es
the
homogeneous
equation,
in
whic
h
case
s
=
).

In
the
nal
step
the
general
solution
to
homogeneous
equation
is
com
bined
with
the
particular
solution
to
non-homogeneous
equation,
and
the
initial
v
alue
problem
is
solv
ed.
Example
Here
is
an
example
of
the
applied
problem
that
leads
to
a
natural,
but
not
ob
viously
solv
able
dierence
equation.
Supp
ose
y
ou
b
orro
w
y
0
dollars
on
xed
mon
thly
in
terest
rate
r
.
If
y
ou
do
not
mak
e
an
y
pa
ymen
ts
on
y
our
loan,
then
y
our
balance
will
\ballo
on"
exp
onen
tially
.
F
orm
ula
y
n
=
(
+
r
)
n
y
0
expresses
mon
thly
balance
after
n
mon
ths
when
no
loan
pa
ymen
ts
are
made.
Some
p
eople
prefer
to
pa
y
a
xed
amoun
t
of
p
dollars
at
the
end
of
eac
h
mon
th.
If
p
is
large
enough,
then
the
balance
ma
y
ev
en
shrink
do
wn!
This
situations
is
easily
describ
ed
b
y
a
dierence
equation.
T
o
write
it
do
wn,
compute
the
next
mon
th
balance,
if
the
previous
mon
th
balance
is
kno
wn:
y
n+
=
(
+
r
)y
n
 p:
(:)
Equation
(.)
and
its
general
solution
are
of
in
terest
to
bank
ocers
and
to
their
cus-
tomers.
F
rom
the
form
ula
for
y
n
,
they
can
determine
mon
thly
pa
ymen
ts
that
will
pa
y
a
loan
o
in
the
prescrib
ed
amoun
t
of
time.
In
dieren
tial
equations,
w
e
often
solv
e
a
similar
problem
in
con
tin
uous
time,
with
con
tin
uous
comp
ounding
and
con
tin
uous
pa
ymen
ts
sc
hedule.
This
is
a
mathematical
simplication
of
the
actual
banking
situation,
but
the
answ
ers
are
reasonably
close,
and
they
are
easier
to
get.
Here
is
an
example
of
the
mathematical
problem
that
leads
to
a
natural,
but
non-
trivial

dierence
equation.
Example
.
Supp
ose
we
want
to
nd
the
formula
for
the
sum
of
al
l
c
onse
cutive
inte
gers
fr
om

to
n.
L
et
the
answer
by
y
n
.
Then
the
r
e
curr
enc
e
formula
y
n+
=
y
n
+
n
+

(:)
holds.

Most
lik
ely
,
y
ou
kno
w
the
solution
to
this
one,
but
the
metho
d
is
useful
for
other
problems
as
w
ell.

..
LINEAR
DIFFERENCE
EQUA
TIONS

No
w
(.)
can
b
e
written
as
y
n+
 y
n
=
n
+

and
actually
this
is
wh
y
w
e
use
the
name
dier
enc
e
e
quations
rather
than
recurrence
relations.
Notice
that
w
e
do
kno
w
the
solution
of
its
con
tin
uous
analogue.
Equation
y
0
=
t
+

for
unkno
wn
function
y
=
y
(t)
resem
bles
(.).
Its
solution

is
y
(t)
=


t

+
t.
So
to
nd
the
answ
er
to
(.)
w
e
ma
y
try
substituting
u
n
=


n

+
n
in
to
the
equation.
Unfortunately
,
simple
arithmetics
sho
ws
that
w
e
don't
get
the
righ
t
answ
er,
as
u
n+
=
u
n
+
n
+


:
(:	)
But
then,
w
e
are
not
that
far
o
the
target.
Let
v
n
=
u
n
 y
n
.
Subtracting
equation
(.)
from
equation
(.	)
w
e
get
the
dieren
tial
equation
for
(v
n
)
v
n+
=
v
n
+


:
This
is
the
sp
ecial
case
of
the
arithmetic
progression
equation
(.).
F
rom
Example
..
equation
(.)
w
e
kno
w
that
v
n
=
n=.
Therefore
y
n
=


n

+
n
 n=
=
n(n+)

The
metho
d
w
e
used
{
subtracting
the
equations
{
w
orks
for
the
so
called
line
ar
dier
enc
e
e
quations
(and
also
linear
dieren
tial)
equations.
It
is
closely
related
to
the
Principle
of
Sup
erp
osition
for
linear
dieren
tial
equations.
..
Problems
.
Chec
k
if
the
giv
en
sequence
solv
es
the
dierence
equation.
(a)
Equation:
y
n+
=
 y
n
.
Sequence
y
n
=
cos
n
.
(b)
Equation:
y
n+
=
y
n
+
y
n 
.
Sequence
y
n
=

n
.
(c)
Equation:
y
n+
=
y
n
+
y
n 
.
Sequence
y
n
=
 

n

+
n

 

n
+
.
(d)
Equation:
y
n+
=
y
n
 y
n 
+
.
Sequence
y
n
=
n

.
.
W
rite
do
wn
the
dierence
equation
that
y
ou
need
to
solv
e
eac
h
of
the
follo
wing
problems
(do
not
solv
e
the
equation,
nor
the
problem).
(a)
A
loan
of
$000
has
in
terest
rate
that
v
aries
with
time
as
follo
ws:
rst
mon
th
in
terest
is
0%,
second
mon
th
in
terest
is


%,
third
mon
th
in
terest
is


%,
etc
with
mon
thly
in
terest
increasing
b
y


%
ev
ery
mon
th.
Determine
the
xed
mon
thly
pa
ymen
t
p
that
will
pa
y
this
loan
within
one
y
ear.
(b)
A
loan
of
$000
has
constan
t
mon
thly
in
terest
rate
of


%.
I
arranged
m
y
mon
thly
pa
ymen
ts
in
the
follo
wing
fashion:
at
the
end
of
the
rst
mon
th
I
will
pa
y
nothing,
at
the
end
of
the
second
mon
th
I
will
pa
y
$0,
at
the
end
of
the
third
mon
th
I
will
pa
y
$0,
etc,
with
mon
thly
pa
ymen
ts
increasing
b
y
$0
ev
ery
mon
th.
Determine
when
I
will
pa
y
o
this
loan
and
ho
w
m
uc
h
money
I
will
pa
y
in
total.
.
Find
the
general
solution
of
the
follo
wing
dierence
equations.
(a)
y
n+
=

n
y
n

Clearly
,
w
e
request
y
0
=
0.


CHAPTER
.
INTR
ODUCTION
TO
STOCHASTIC
PR
OCESSES
(b)
y
n+
=
n
n+
y
n
(c)
n(n
+
)y
n+
=
y
n
.
Solv
e
the
giv
en
initial
v
alue
problem
for
the
dierence
equation.
(a)
y
n+
=

n
y
n
;
y
0
=
(b)
y
n+
=
n
n+
y
n 
;
y
0
=
;
y

=
0
(c)
n(n
+
)y
n+
=
y
n
;y
0
=
;
y

=
0
.
Find
the
form
ula
for
(a)
y
n
=


+


+
:
:
:
+
n

(b)
y
n
=


+


+
:
:
:
+
n

(c)
y
n
=


+


+
:
:
:
+

n(n+)
(d)
y
n
=

+
r
+
r

+
:
:
:
+
r
n 
(e)
y
n
=
r
+
r

+
r

+
:
:
:
+
nr
n
.
Eac
h
solution
of
equation
(.)
has
the
limit
lim
n!
y
n
.
Sho
w
that
the
rst
digits
of
this
limit
are
:	0.
.
Recursiv
e
equations,
c
haos,
randomness
Before
jumping
to
mo
dels
that
use
explicit
randomness
in
their
ev
olution,
it
is
quite
illuminating
to
analyze
rst
some
mathematically
simple
and
w
ell
dened
\deterministic
ev
olutionary
pro
cesses".
The
follo
wing
set
of
examples
describ
es
deterministic
ev
olution
in
discrete
time
of
a
system
describ
ed
b
y
a
single
n
umerical
parameter
x

(0;
).
All
of
the
examples
fall
in
to
the
category
of
(non-linear)
dierence
equations:
giv
en
initial
v
alue
x
0

(0;
)
and
a
simple
ev
olution
equation
of
the
form
x
n+
=
g
(x
n
),
w
e
are
supp
osed
to
mak
e
inferences
ab
out
the
b
eha
vior
of
the
solutions.
There
is
no
randomness
in
the
ev
olution
itself.
But
since
w
e
are
allo
w
ed
to
c
ho
ose
an
y
initial
condition,
and
no
initial
condition
can
b
e
measured
exactly
,
w
e
ma
y
as
w
ell
consider
the
initial
v
alue
to
b
e
random.
Example
.
Equation
x
n+
=
cos
(x
n
)
is
analyze
d
numeric
al
ly
in
Example
..
A
useful
te
chnique
to
analyze
such
e
quations
is
to
gr
aph
function
y
=
g
(x)
to
gether
with
line
y
=
x,
and
r
epr
esent
the
se
quenc
e
x
k
by
p
oints
(x
k
;
x
k
)
on
the
line.
The
actual
pr
o
of
that
x
n
!
x

may
p
erhaps
b
e
not
that
obvious,
but
the
ge
ometric
ar
gument
se
ems
to
b
e
quite
c
onvincing.
Example
.
L
et
fxg
denote
the
fr
actional
p
art
of
x.
Equation
x
n+
=
fx
n
g
uses
disc
ontinuous
function
g
(x).
The
pr
evious
gr
aphic
al
te
chnique
is
mor
e
dicult
to
apply
her
e,
but
the
r
e
ason
for
this
diculty
might
b
e
not
app
ar
ent.
Sp
e
cial
initial
p
oints,

..
RECURSIVE
EQUA
TIONS,
CHA
OS,
RANDOMNESS

like
x
0
=
=;
x
0
=
=;
x
0
=
=;
x
0
=
=;
:
:
:
ar
e
r
elatively
e
asy
to
analyze.
But
these
ar
e
exc
eptional
-
the
majority
of
the
initial
p
oints
actual
ly
do
esn
't
fol
low
this
p
attern.
Her
e
is
a
r
ather
surprising
fact.
Supp
ose
x
0
is
sele
cte
d
at
r
andom.
Sinc
e
x
k
is
a
function
of
x
0
,
and
x
0
is
r
andom,
x
k
b
e
c
omes
a
r
andom
variable.
It
may
happ
en
that
x
k
<
=.
Cal
l
this
event
A
k
.
The
r
e
asoning
pr
esente
d
in
Se
ction
..
implies
that
events
fA
k
g
ar
e
indep
endent
and
have
the
same
pr
ob
ability
Pr(A
k
)
=
=.
Ther
efor
e
the
deterministic
evolution
e
quation
c
ontains
at
le
ast
as
much
r
andomness
as
the
tosses
of
a
c
oin.
The
last
example
ma
y
p
erhaps
giv
e
the
impression
that
it
is
the
discon
tin
uit
y
of
the
ev
olution
equation
that
is
the
source
of
diculties.
This
is
not
at
all
the
case.
Example
.
Equation
x
n+
=
x
n
(
 x
n
)
is
another
example
of
the
"chaotic
e
quation"
with
solutions
exhibiting
as
much
irr
e
gularity
as
the
tosses
of
a
c
oin.
A
ttempts
at
gr
aphing
its
solutions
do
no
indic
ate
any
p
atterns.
Tiny
dier
enc
es
in
the
choic
e
of
initial
value
x
0
signic
antly
change
the
evolution
within
short
time.
Example
.
indicates
that
\naiv
e"
prediction
of
the
future
of
a
system
is
unreliable
ev
en
within
the
realm
of
deterministic
ev
olution
equations.
On
the
other
hand,
there
are
asp
ects
of
the
ev
olution
that
w
e
can
analyze
reliably
.
These
deal
with
the
a
v
erage
b
eha
vior
of
the
ev
olution.
There
are
t
w
o
classes
of
questions
that
w
e
can
answ
er,
but
b
oth
deal
with
statistical
nature
of
the
ev
olution:

what
happ
ens
if
the
same
exp
erimen
t
is
rep
eated
man
y
times
(with
sligh
tly
dieren
t
initial
conditions)?

What
is
the
a
v
erage
b
eha
vior
of
the
system
o
v
er
long
p
erio
ds
of
time?
F
or
instance,
w
e
can
ask
and
get
reliable
answ
ers
to
questions
lik
e:

What
is
the
a
v
erage

n
P
n
j
=
x
j
?

What
is

n
P
n
j
=
U
(x
j
)
for
a
giv
en
function
U
?

Ho
w
often
x
j
<
=?
(Meaning
-
what
prop
ortion
of
j

T
satises
the
condition
for
large
T
.)

Ho
w
often
x

<
=
when
x
0
is
selected
according
to
densit
y
g
(x)?
(Meaning
-
what
prop
ortion
of
x
0
satises
the
condition
for
large
n
um
b
er
of
initial
p
oin
ts
x
0
.)
Theory
of
sto
c
hastic
pro
cesses
uses
descriptiv
e
rather
than
casual
mo
dels.
Its
primary
goal
is
to
isolate
metho
ds
that
answ
er
questions
that
can
b
e
answ
ered
-
ab
out
the
a
v
erages
and
c
hances
of
ev
en
ts.
It
is
setup
in
the
form
that
mak
es
it
more
natural
to
ask
the
"correct"
questions.
But
in
real
life,
and
in
sim
ulations,
w
e
do
ha
v
e
access
to
asp
ects
of
the
phenomenon
than
what
the
theory
do
es
not
exp
ose.
In
analyzing
sim
ulations
it
is
imp
ortan
t
to
k
eep
in
mind
the
examples
ab
o
v
e.
Av
oid
collecting
data
that
deal
with
instances
rather
than
statistical
phenomena.
Prin
t
out
w
ell
dened
statistics
of
the
sim
ulation
only
.
Do
not
clutter
y
our
sim
ulations
with
irrelev
an
t
details.


CHAPTER
.
INTR
ODUCTION
TO
STOCHASTIC
PR
OCESSES
.
Mo
deling
and
sim
ulation
The
quic
k
w
a
y
to
get
insigh
ts
in
to
op
eration
of
real
systems
is
to
mo
del
their
b
eha
vior.
Here
are
examples
of
en
terprises
that
op
erate
under
randomness.
Mathematicians
devised
metho
ds
of
mo
delling
eac
h
of
these.
But
it
is
in
teresting
also
to
sim
ulate
their
b
eha
vior.
Notice
that
sim
ulations
require
assumptions,
but
so
do
the
analytical
metho
ds.
Regardless
of
the
metho
d,
w
e
ha
v
e
to
b
e
careful
ab
out
what
are
the
questions
w
e
can
answ
er.
Example
.
Supp
ose
we
want
to
study
how
much
c
apital
is
ne
e
de
d
to
run
a
c
asino.
We
ne
e
d
to
answer
the
fol
lowing.

How
often
do
p
e
ople
win?

How
likely
is
the
c
asino
to
lo
ose
money
in
a
day?
In
a
month?

How
much
c
apital
should
b
e
kept
on
hand
to
c
over
the
losses?
Ther
e
is
also
a
numb
er
of
questions
that
we
do
not
want
to
answer.
Example
.
Supp
ose
we
want
to
study
how
much
c
apital
is
ne
e
de
d
to
insur
e
c
ars.
We
ne
e
d
to
answer
the
fol
lowing.

How
often
do
ac
cidents
o
c
cur?
How
exp
ensive
ar
e
r
ep
airs/me
dic
al
c
osts?

How
likely
is
that
the
insur
anc
e
c
omp
any
lo
oses
money
in
a
day?
In
a
month?

How
much
c
apital
should
b
e
kept
on
hand
to
c
over
losses?
Ther
e
is
also
a
numb
er
of
questions
that
we
do
not
want
to
answer.
F
or
instanc
e
we
do
not
want
to
pr
e
dict
whether
I'l
l
le
an
insur
anc
e
claim
to
day,
driving
b
ack
home
on
I-
without
enough
sle
ep
sinc
e
I
was
pr
ep
aring
this
class
until
AM.
Example
.	
A
c
onstruction
c
omp
any
has
n
jobs
to
b
e
p
erforme
d
in
the
futur
e.
F
or
e
ach
of
these
jobs,
exp
erts
pr
ovide
estimate
of
the
c
ost.
Then,
after
questioning,
they
c
omplement
the
estimates
by
the
lower/upp
er
b
ound
for
the
c
osts.
How
much
money
should
b
e
al
lotte
d?
Example
.0
A
stor
e
aver
ages
(t)
customers
p
er
hour
at
time
t
of
the
day.
Each
customer
brings
some
(r
andom)
pr
ot.
However,
a
customer
may
just
le
ave
the
stor
e
without
shopping,
if
the
lines
ar
e
to
o
long.

How
many
c
ashiers
should
b
e
available
for
e
ach
shift
(time)
t?

What
ar
e
the
pr
ots
on
aver
age?
.
Random
w
alks
A
random
w
alk
is
a
pro
cess
of
the
form
X
n
=
P
n
j
=

j
,
where

j
are
i.
i.
d.
Random
w
alks
ha
v
e
indep
endent
incr
ements
,
and
describ
e
the
accum
ulation
of
indep
enden
t
con
tributions
o
v
er
time.
Random
w
alks
are
examples
of
Mark
o
v
pro
cesses
whic
h
will
b
e
studied
in
detail
in
Chapter
.
Their
sp
ecial
structure
allo
ws
to
analyze
them
indep
enden
tly
of
the
general
theory
.

..
RANDOM
W
ALKS

..
Stopping
times
The
stopping
times
are
random
v
ariables
that
describ
e
phenomena
whic
h
dep
end
on
the
tra
jectory
of
a
random
w
alk.
The
denition
captures
the
in
tuition
that
their
v
alues
are
determined
b
y
the
history
of
a
Mark
o
v
c
hain.
Denition
..
T
:

!
I
N
[

is
a
stopping
time,
if
the
event
fT
=
ng
is
indep
endent
of

n+
;

n+
;
:
:
:
.
This
denition
is
sp
ecialized
to
random
w
alks.
In
a
more
general
Mark
o
v
case
the
denition
is
less
transparen
t
but
captures
the
same
idea.
The
most
imp
ortan
t
example
of
a
stopping
time
is
the
rst
en
trance
time
T
=
inf
fk
:
X
k

Ag.
An
example
that
is
not
a
stopping
time
is
the
last
exit
from
a
set.
When
T
<

w
e
dene
random
sums
X
T
=
P
j
T

j
.
The
follo
wing
theorem
is
an
exercise
when
T
is
indep
enden
t
of

.
Theorem
..
If

j
ar
e
i.
i.
d.,
E

=
,
and
E
T
<

is
a
stopping
time
then
E
X
T
=
E
T
(:0)
Pro
of.
E
X
T
=
P
n
E
X
n
Pr(T
=
n)
=
P
n
P
n
k
=
E

k
Pr(T

k
).
Since

k
and
fT

k
g
=
fT
<
k
g
0
are
indep
enden
t,
therefore
E
S
T
=

P
n
Pr(T

n)
=
E
T
b
y
tail
in
tegration
form
ula
(.	).

Theorem
..
If

j
ar
e
i.i.d.,
E

=
,
V
ar
(
)
=


<
,
and
E
T
<

then
E
(S
T
 T
)

=


E
T
(:)
(These
form
ulas
are
of
in
terest
in
branc
hing
pro
cesses,
and
in
c
hromatograph
y
.)
Example
.
The
numb
er
of
che
cks
c
ashe
d
at
a
b
ank
p
er
day
is
Poisson
r
andom
variable
N
with
me
an

=
00.
The
amount
of
e
ach
che
ck
is
a
r
andom
variable
with
a
me
an
of
$0
and
a
standar
d
deviation
of
$.
If
the
b
ank
has
$0
on
hand,
is
the
demand
likely
to
b
e
met?
Problem
.
Supp
ose

k
;
T
ar
e
indep
endent.
Find
the
varianc
e
of
X
T
in
terms
of
the
rst
two
moments
of

;
T
.
..
Example:
c
hromatograph
y
Chromatograph
y
is
a
tec
hnique
of
separation
mixtures
in
to
comp
ounds.
One
of
its
uses
is
to
pro
duce
the
DNA
bands.
The
sample
is
injected
in
to
a
column,
and
the
molecules
are
transp
orted
along
the
length
b
y
electric
p
oten
tial,
o
w
of
gas,
or
liquid.
The
basis
for
c
hromatographic
separa-
tion
of
a
sample
of
molecules
is
dierence
in
their
ph
ysical
c
haracteristics.
The
molecules
switc
h
b
et
w
een
t
w
o
phases:
mobile,
and
stationary
,
and
the
separation
of
comp
ounds
is
caused
b
y
the
dierence
in
times
sp
end
in
eac
h
of
the
phases.


CHAPTER
.
INTR
ODUCTION
TO
STOCHASTIC
PR
OCESSES
Supp
ose
that
the
molecules
of
a
comp
ound
sp
end
random
indep
enden
t
amoun
ts
of
time
U
k
in
mobile
phase
and
random
amoun
t
of
time
W
k
in
the
stationary
phase.
Th
us
at
time
t
the
p
osition
of
a
molecule
is
giv
en
b
y
a
random
sum
v
P
T
(t)
j
=
U
j
,
where
T
(t)
=
inf
fk
:
P
k
j
=
U
j
+
W
j
>
tg.
Section
..
giv
es
form
ulas
for
the
mean
and
the
v
ariance
of
the
p
osition.
Since
the
n
um
b
er
of
transitions
T
is
lik
ely
to
b
e
large
for
a
t
ypical
molecule,
it
isn't
surprising
that
the
actual
p
osition
has
(asymptotically)
normal
distribution.
(The
actual
Cen
tral
Limit
Theorem
for
random
sums
is
not
stated
in
these
notes.)
Exercise
.
Simulate
the
output
of
the
chr
omato
gr
aphy
c
olumn
of
xe
d
length
sep
ar
at-
ing
a
p
air
of
substanc
es
that
have
dier
ent
distributions
of
mobile
and
stationary
phases
U
k
;
W
k
.
Sele
ct
a
hundr
e
d
p
articles
of
e
ach
substanc
e,
and
me
asur
e
the
de
gr
e
e
of
sep
ar
ation
at
the
end
of
the
c
olumn.
..
Ruining
a
gam
bler
The
follo
wing
mo
del
is
a
reasonable
appro
ximation
to
some
of
the
examples
in
Section
..
Supp
ose
a
gam
bler
can
aord
to
lo
ose
amoun
t
L
>
0,
while
the
casino
has
capital
C
<
0.
Let

j
=

b
e
i.
i.
d.
random
v
ariables
mo
delling
the
outcomes
of
consecutiv
e
games,
S
n
b
e
the
partial
sums
(represen
ting
gains
of
the
gam
bler),
and
let
T
=
inf
fk
:
S
k

L
or
S
k

C
g
b
e
the
total
n
um
b
er
of
games
pla
y
ed.
Then
Pr(T
>
k
)
=
Pr(C
<
S
k
<
L)
and
th
us
E
T
=
P
k
Pr(C
<
S
k
<
L).
The
sp
ecial
case
Pr(
=
)
=
=
is
easily
solv
ed,
since
in
this
case
E
S
T
=
E

E
T
=
0.
Let
p
=
Pr(S
T
=
C
)
denote
the
probabilit
y
of
ruining
the
casino.
Since
S
T
is
either
C
,
or
L
w
e
ha
v
e
0
=
E
S
T
=
pC
+
(
 p)L,
giving
p
=
L=(L
 C
).
This
form
ula
means
that
a
gam
bler
has
a
fair
c
hance
of
ruining
a
casino
in
a
fair
game,
pro
vided
he
brings
with
him
enough
cash
L.
F
or
more
general
random
w
alks
(and
less
fair
games)
probabilit
y
of
gam
bler's
ruin
can
b
e
found
explicitly
using
the
one-step-analysis
(Section
.).
It
is
also
in
teresting
to
nd
ho
w
long
a
game
lik
e
that
w
ould
last
on
a
v
erage.
(The
expression
for
E
T
giv
en
ab
o
v
e
is
not
explicit.)
..
Random
gro
wth
mo
del
The
follo
wing
mo
dels
v
arious
gro
wth
phenomena
lik
e
the
spread
of
a
disease,
where
the
infected
individual
ma
y
either
die,
or
infect
a
n
um
b
er
of
other
individuals.
Here
w
e
con-
cen
trate
on
bacteria
whic
h
ha
v
e
simple
repro
duction
mec
hanism,
and
all
spatial
relations
are
neglected.
Let
X
t
denote
the
n
um
b
er
of
bacteria
in
t-th
generation,
with
X
0
=
.
Assume
that
a
bacteria
can
die
with
probabilit
y
q
>
0,
or
divide
in
to
t
w
o
cells
with
probabilit
y
p
=

 q
,
and
that
all
deaths
o
ccur
indep
enden
tly
.
Our
goal
here
is
to
nd
the
a
v
erage
n
um
b
er
of
bacteria
m(t)
=
E
(X
t
)
in
the
t-th
generation.
This
can
b
e
reco
v
ered
from
Theorem
...
Instead,
w
e
sho
w
another
metho
d
based
on
conditioning.
The
n
um
b
er
of
bacteria
in
the
next
generation
is
determined
b
y
binomial
probabilities:
Pr(X
t+
=
k
jX
t
=
n)
=
(
n
k
)p
k
q
n k
.
Therefore
E
(X
t+
jX
t
)
=
pX
t
and
the
a
v
erage

..
RANDOM
W
ALKS
	
p
opulation
size
m(t)
=
E
(X
t
)
satises
dierence
equation
m(t
+
)
=
pm(t).
W
e
ha
v
e
m(t)
=
(p)
t
.
In
particular,
the
p
opulation
on
a
v
erage
gro
ws
exp
onen
tially
when
p
>
=.
It
is
p
erhaps
surprising
that
a
p
opulation
of
bacteria
with
p
=
=,
whic
h
on
a
v
erage
gro
ws
b
y
0%
p
er
generation,
has
still
a


c
hance
of
going
extinct.
One
w
a
y
to
in
terpret
this
is
to
sa
y
that
infections
b
y
a
\deadly"
and
rapidly
dev
eloping
desease
ma
y
still
ha
v
e
a
large
surviv
al
rate
without
an
y
in
terv
en
tion
of
medicine,
or
imm
une
system.
(The
metho
ds
to
compute
suc
h
probabilities
will
b
e
in
tro
duced
in
Section
..
The
answ
er
ab
o
v
e
assumes
infection
b
y
a
single
cell.)
Problem
.
Find
the
formula
for
the
varianc
e
V
ar
(X
t
)
of
the
numb
er
of
b
acteria
in
t-th
gener
ation.
Problem
.
What
is
the
pr
ob
ability
that
an
infe
ction
by
0
identic
al
b
acteria
with
the
doubling
pr
ob
ability
p
=
=
dies
out?

0
CHAPTER
.
INTR
ODUCTION
TO
STOCHASTIC
PR
OCESSES

Chapter

Mark
o
v
pro
cesses
ev
olution,
n.
[L.
evolutio
(-onis),
an
unrolling
or
op
ening...
W
ebster's
New
Univ
ersal
Unabridged
Dictionary
Mark
o
v
pro
cesses
are
p
erhaps
the
simplest
mo
del
of
a
random
ev
olution
without
long-term
memory
.
Mark
o
v
pro
cess
is
a
sequence
X
t
of
random
v
ariables
indexed
b
y
discrete
time
t

Z
Z
+
,
or
con
tin
uous
t

0
that
satises
the
so
called
Mark
o
v
prop
ert
y
.
The
set
of
all
p
ossible
v
alues
of
v
ariables
X
t
is
called
the
state
sp
ac
e
of
the
Mark
o
v
c
hain.
T
ypical
examples
of
state
spaces
are
I
R,
I
N,
the
set
of
all
non-negativ
e
pairs
of
in
tegers,
and
nite
sets.
Mark
o
v
c
hains
are
Mark
o
v
pro
cesses
with
discrete
time.
Th
us
a
Mark
o
v
c
hain
is
an
innite
sequence
fX
k
g
k
Z
Z
+
of
(usually
,
dep
enden
t)
random
v
ariables
with
short-term
(one-step)
memory
.
.
Mark
o
v
c
hains
The
formal
denition
of
the
Mark
o
v
prop
ert
y
is
as
follo
ws.
Denition
..
A
family
of
discr
ete
r.
v.
fX
k
g
k
Z
Z
+
is
a
Markov
chain,
if
Pr(X
k
+

U
jX
0
;
:
:
:
;
X
k
)
=
Pr(X
k
+

U
jX
k
)
dep
ends
only
on
the
pr
esent
value
X
k
.
Examples
of
Mark
o
v
c
hains
are:

A
sequence
of
indep
enden
t
r.
v.

A
constan
t
random
sequence
X
k
=

.

Random
w
alks
(sums
of
indep
enden
t
random
v
ariables).
Examples
of
non-Mark
o
v
pro
cesses
are
easy
to
construct,
but
lac
k
of
Mark
o
v
prop
ertry
is
not
ob
vious
to
v
erify
.
In
general,
if
X
k
is
a
Mark
o
v
pro
cess,
Y
k
=
f
(X
k
)
ma
y
fail
to
b
e
Mark
o
v.



CHAPTER
.
MARK
O
V
PR
OCESSES
..
Finite
state
space
If
a
Mark
o
v
c
hain
has
a
nite
state
space,
w
e
can
alw
a
ys
assume

it
consists
of
in
tegers.
Mark
o
v
condition
Pr(X
k
+
=
xjX
0
;
:
:
:
;
X
k
)
=
Pr(X
k
+
=
xjX
k
)
(:)
implies
that
probabilit
y
of
reac
hing
x
in
the
next
step
dep
ends
only
on
the
presen
t
v
alue
X
k
.
The
probabilistic
b
eha
vior
of
suc
h
a
c
hain
is
completely
determined
b
y
the
initial
distribution
p
k
=
Pr(X
0
=
k
)
and
the
transition
matrices
P
n
(i;
j
)
=
Pr(X
n
=
j
jX
n 
=
i),
see
form
ula
(.)
on
page
.
F
or
mathematical
con
v
enience
w
e
shall
assume
that
one
step
transition
matrices
P
t
=
P
do
not
dep
end
on
time
t.
Suc
h
Mark
o
v
c
hains
are
called
homo
gene
ous.
This
assumption
isn't
realistic,
nor
alw
a
ys
con
v
enien
t.
F
or
instance,
the
Mark
o
v
sim
ulation
in
Section
.
uses
a
Mark
o
v
c
hain
with
transitions
that
v
ary
with
time.
But
homogeneous
Mark
o
v
c
hains
are
still
exible
enough
to
handle
some
time
dep
endencies
ecien
tly
through
mo
dications
to
the
state
space.
Example
.
Supp
ose
X
n
is
a
Markov
chain
with
p
erio
dic
tr
ansition
pr
ob
abilities
P
n
=
P
n+T
.
Then
Y
n
=
(X
n+
;
X
n+
;
:
:
:
;
X
n+T
)
is
a
homo
gene
ous
Markov
chain.
Problem
.
Supp
ose

j
ar
e
indep
endent
f0;
g-value
d
with
Pr(
=
)
=
p.
L
et
X
n
=
a
n
+
b
n+
,
wher
e
ab
=
0.
Explain
why
X
n
is
a
Markov
chain.
Write
the
tr
ansition
matrix
for
the
Markov
chain
X
n
.
Prop
osition
..
The
pr
ob
abilities
Pr(X
n
=
j
jX
0
=
i)
ar
e
given
by
the
i;
j
-entries
of
the
matrix
P
n
Pro
of.
This
is
the
consequence
of
Mark
o
v
prop
ert
y
(.)
and
the
total
probabilit
y
for-
m
ula
(.0).

P
o
w
ers
of
mo
derately
sized
matrices
are
easy
to
compute
on
the
computer.
Section
B.	
indicates
a
mathematical
metho
d
of
computing
P
n
for
small
dimensions
using
the
Ca
yley-Hamilton
theorem.
Under
certain
conditions
the
p
o
w
ers
con
v
erge.
Exercise
.
Find
lim
n!
P
n
for
the
matrix
fr
om
Pr
oblem
..
Stationary
Mark
o
v
pro
cesses
Supp
ose
Pr
(X
0
=
k
)
=
p
k
,
where
p
k
solv
e
stationarit
y
equations
X
k
p
k
=

(.)
[p

;
:
:
:
;
p
d
]
=
[p

;
:
:
:
;
p
d
]

P
(.)
(.)

Notice
that
this
is
a
mathematical
simplication
that
migh
t
b
e
not
w
orth
pursuing
if
the
actual
state
space
has
some
con
v
enien
t
in
terpretation.

..
MARK
O
V
CHAINS

Then
the
resulting
pro
cess
is
stationary:
the
distribution
of
eac
h
k
-tuple
(X
(t

);
:
:
:
;
X
(t
k
))
is
in
v
arian
t
under
shifts
in
time,
(X
(t

+
s);
:
:
:
;
X
(t
k
+
s))

=
(X
(t

);
:
:
:
;
X
(t
k
)).
This
is
in
terpreted
as
\equilibrium",
or
\steady
state".
Notice
that
\steady
state"
is
a
statistical
concept,
and
is
not
easily
visible
in
a
sim
ulation
of
the
single
tra
jectory
.
In
order
to
b
e
able
to
see
it,
one
has
to
sim
ulate
a
large
n
um
b
er
of
indep
enden
tly
ev
olving
Mark
o
v
c
hains
that
b
egin
with
the
same
initial
distribution
and
ha
v
e
the
same
transition
matrix.
If
X
t
is
a
stationary
Mark
o
v
pro
cess
and
f
is
a
function
on
its
state
space,
then
Y
(t)
=
f
(X
t
)
is
also
stationary
,
although
not
necessarily
Mark
o
v.
If
X
j
(t)
are
indep
enden
t
realizations
of
the
same
Mark
o
v
pro
cess
and
f
is
a
function
on
their
state
space
then
Y
n
(t)
=
n
 =
P
n
j
=
(f
(X
j
(t))
 )
is
stationary
and
appro
ximately
normal
random
sequence.
..
Mark
o
v
pro
cesses
and
graphs
The
states
of
a
Mark
o
v
c
hain
ma
y
b
e
represen
ted
b
y
v
ertices
of
a
graph,
and
one
step
transitions
ma
y
b
e
describ
ed
b
y
directed
edges
with
w
eigh
ts.
Suc
h
represen
tation
of
a
mark
o
v
c
hain
aids
in
visualizing
a
Mark
o
v
c
hain.
Classication
of
states
Graph
notions
ha
v
e
b
earing
on
prop
erties
of
the
Mark
o
v
c
hain.
In
particular,
Mark
o
v
c
hain
is
irreducible,
if
the
corresp
onding
graph
is
connected.
Mark
o
v
c
hain
is
p
erio
dic,
if
there
is
N
>

(the
p
erio
d)
suc
h
that
all
cycles
of
the
graph
are
m
ultiples
of
N
.
If
there
is
no
suc
h
N
then
Mark
o
v
c
hain
is
called
ap
erio
dic.
Finite
state
Mark
o
v
c
hain
is
regular,
if
there
is
deterministic
n
um
b
er
N
suc
h
that
all
states
are
connected
b
y
paths
of
length
at
most
N
.
Problem
.
Show
that
r
e
gular
Markov
chain
is
ap
erio
dic
and
irr
e
ducible.
T
ra
jectory
a
v
erages
Additiv
e
functionals
of
a
Mark
o
v
pro
cess
are
expressions
of
the
form

n
P
n 
j
=0
f
(X
j
).
Under
certain
conditions,
the
a
v
erages
con
v
erge
and
the
limit
do
esn't
dep
end
on
the
initial
distribution.
Under
certain
conditions,
partial
sums
are
appro
ximately
normal.
Problem
.
L
et
X
k
b
e
an
irr
e
ducible
f0;
g-value
d
Markov
chain
with
invariant
initial
distribution.

Show
that
ther
e
is
C
>
0
such
V
ar
(
P
T
t=0
X
t
)

C
T
.

Use
the
ab
ove
to
show
that
the
law
of
lar
ge
numb
ers
holds.


CHAPTER
.
MARK
O
V
PR
OCESSES
Asymptotic
probabilities
Let
p
k
(n)
=
Pr
(X
n
=
k
),
and
supp
ose
that
the
limit
p
k
()
=
lim
n!
p
k
(n)
exists.
Since
p
k
(n
+
)
=
P
d
j
=
p
j
(n)P
(j;
k
),
therefore
the
limit
probabilities
satisfy
the
stationarity
e
quations
(.)
F
or
regular
(nite
state
space)
Mark
o
v
c
hains
the
limit
actually
exists
indep
enden
tly
of
the
initial
state.
Therefore
the
stationarit
y
equations
can
b
e
used
to
nd
the
limit.
Problem
.
L
et
P
=
"
=
=
=
=
#
.

Find
the
initial
distribution
of
X
0
that
r
esults
in
a
stationary
pr
o
c
ess.

Find
the
limiting
distribution
lim
n!
p
k
(n).
Problem
.
L
et
P
=
"
0


0
#
.

Find
the
initial
distribution
of
X
0
that
r
esults
in
a
stationary
pr
o
c
ess.

Explain
why
lim
n!
p
k
(n)
do
es
not
exist.
Problem
.
L
et
P
=
"

0
0

#
.

Find
the
initial
distribution
of
X
0
that
r
esults
in
a
stationary
pr
o
c
ess.

Find
the
limiting
distribution
lim
n!
p
k
(n).
Example:
t
w
o-state
Mark
o
v
c
hain
Supp
ose
X
k
is
a
Mark
o
v
c
hain
with
transition
matrix
P
=
"

 a
a
b

 b
#
.
Then
P
n
=

a
+
b
"
b
a
b
a
#
+
(
 a
 b)
n
a
+
b
"
a
 a
 b
b
#
:
If
0
<
a;
b
<

P
n
!
"
b
a+b
a
a+b
b
a+b
a
a+b
#
and
the
rate
of
con
v
ergence
is
exp
onen
tially
fast.
Problem
.
Supp
ose
X
k
is
a
Markov
chain
with
tr
ansition
matrix
P
=
"
a

 a

 b
b
#
.
Then
Y
n
=
(X
n
;
X
n+
)
is
also
a
Markov
pr
o
c
ess.
Find
its
tr
ansition
matrix
and
the
sta-
tionary
distribution.

..
SIMULA
TING
MARK
O
V
CHAINS

.
Sim
ulating
Mark
o
v
c
hains
Homogeneous
and
non-homogeneous
Mark
o
v
c
hains
with
nite
state
space
are
fairly
straigh
tforw
ard
to
sim
ulate.
A
generic
prescription
is
to
sim
ulate
X
n+
using
the
condi-
tional
distribution
determined
b
y
X
n
.
This
sim
ulation
diers
v
ery
little
from
the
generic
metho
d
describ
ed
in
Section
..
Example:
ho
w
long
a
committee
should
discuss
a
topic?
This
example
in
v
olv
es
sim
ulation
of
a
Mark
o
v
c
hain.
F
or
Mark
o
v
c
hains
man
y
theoretical
results
are
a
v
ailable,
but
sim
ulation
is
often
the
most
exp
edien
t
w
a
y
to
study
it.
Exercise
.	
Supp
ose
n
p
e
ople
on
a
c
ommitte
e
discuss
a
c
ertain
issue.
When
one
p
erson
nishes
sp
e
aking,
we
assume
that
it
is
e
qual
ly
likely
that
any
of
the
other
n
 
b
e
gins.
We
also
assume
that
e
ach
p
erson
sp
e
aks
for
exp
onential
ly
distribute
d
r
andom
time.
How
long
do
es
it
take
on
aver
age
for
al
l
p
articip
ants
to
take
p
art
in
the
discussion?
..
Example:
so
ccer
The
so
ccer
game

is
pla
y
ed
b
y
t
w
o
teams,
eac
h
with
0
pla
y
ers
in
the
eld
and
a
goalk
eep
er.
A
mo
dern
line-up
split
the
pla
y
ers
in
to
b
et
w
een
the
zones
of
defence,
cen
ter,
and
attac
k.
Th
us
a
conguration
(--)
means

defenders
(bac
ks),

mideld
link
men
and

strik
ers
(forw
ards).
In
the
Mark
o
v
mo
del
of
so
ccer
w
e
will
just
w
atc
h
the
p
osition
of
the
ball,
and
w
e
assume
it
can
b
e
only
in
one
of
the
v
e
p
ositions:
left
goal,
left
defence,
mideld,
righ
t
defence,
righ
t
goal.
W
ee
shall
assume
that
at
ev
ery
unit
of
time
the
ball
has
to
mo
v
e
left
or
righ
t
with
c
hances
prop
ortional
to
the
n
um
b
er
of
pla
y
ers
lined-up
for
a
giv
en
p
osition.
F
or
eac
h
p
ossible
congurations
of
teams,
w
e
could
determine
the
a
v
erage
score,
and
th
us
determine
whic
h
pla
y
er
distribution
is
the
b
est,
if
there
is
one.
If
there
is
no
b
est
single
arrangemen
t
that
wins
against
all
other
arrangemen
ts,
then
w
e
should
still
b
e
able
to
nd
the
optimal
mixe
d
str
ate
gy.
Exercise
.0
F
or
the
Markov
chain
dene
d
ab
ove,
sele
ct
a
p
air
of
c
ongur
ations
and
determine
the
fol
lowing.

A
ver
age
time
to
hit
the
go
al

Pr
ob
ability
of
sc
oring
left
b
efor
e
right
go
al

L
ong
run
b
ehavior
of
the
b
al
l.

Source:
fo
otball
in
A.
Horn
b
y
,
Oxford
Adv
anced
Learner's
Dictionary
of
Curren
t
English
,
Oxford
Univ
ersit
y
Press
	


CHAPTER
.
MARK
O
V
PR
OCESSES
A
brief
review
of
game
theory
In
game
the
ory,
a
rational
pla
y
er
is
supp
osed
to
minimize
her
losses
against
the
b
est
c
hoice
of
the
opp
onen
t.
The
terminology
uses
minimax=
minimize
maximal
loss,
maximin=
maximize
minimal
gain;
the
amazing
fact
is
that
these
are
equal
and
often
the
optimal
strategies
are
randomized
(mixed).
The
optimal
strategy
in
a
game
is
the
b
est
(in
the
ab
o
v
e
minimax
sense)
randomized
strategy
against
ev
ery
deterministic
strategy
of
the
opp
onen
t.
Problem
.
What
is
the
\optimal"
arr
angement
of
players
in
so
c
c
er?
Mo
dications
As
giv
en
ab
o
v
e,
the
mo
del
of
the
so
ccer
game
is
simple
enough
to
b
e
analyzed
without
computer.
In
a
more
realistic
mo
del
one
could
consider
additional
factors.

The
eld
can
b
e
partitioned
in
to
more
pieces

The
eld
is
a
t
w
o-dimensional
rectangle.

Pla
y
ers
can
mo
v
e
b
et
w
een
neigh
b
oring
p
ortions
of
the
game
eld.

Pla
y
ers
react
to
the
p
osition
of
the
ball

Pla
y
ers
react
to
the
p
ositions
of
other
pla
y
ers

Some
so
ccer
pla
y
ers
are
more
skilled.
.
One
step
analysis
F
or
homogeneous
Mark
o
v
c
hains
a
surprising
n
um
b
er
of
quan
tities
of
in
terest
can
b
e
computed
using
the
so
called
one
step
analysis.
The
idea
is
to
nd
out
what
happ
ens
to
the
quan
tit
y
of
in
terest
within
one
step
of
the
ev
olution,
and
solv
e
the
resulting
dierence
equation.
Example
.
In
the
setup
of
Se
ction
..,
supp
ose
Pr(
=
)
=
p
=

 Pr(
=
 ).
L
et
T
b
e
the
rst
time
r
andom
walk
r
e
aches
L
>
0
or
C
<
0.
Find
the
pr
ob
ability
of
winning
(ruining
the
c
asino)
Pr
(X
T
=
C
jX
0
=
0).
Note
that
evven
though
we
ar
e
inter
este
d
in
a
single
numb
er
Pr(X
T
=
C
jX
0
=
0),
the
st-step
analysis
r
e
quir
es
c
omputing
pr
ob
abilities
p(x)
=
Pr(X
T
=
C
jX
0
=
x)
for
al
l
initial
p
ositions
x.
Example
.
Supp
ose
Pr
(
=
)
=
p;
Pr(
=
 )
 p.
L
et
T
b
e
the
rst
time
r
andom
walk
r
e
aches
L
>
0
or
C
<
0.
Find
the
aver
age
length
of
the
game
E
0
(T
).
Note
that
the
st-step
analysis
r
e
quir
es
c
omputing
m(x)
=
E
x
(T
)
for
al
l
initial
p
osi-
tions
x.
Problem
.
On
aver
age,
how
long
do
es
it
take
for
a
symmetric
r
andom
walk
on
a
squar
e
lattic
e
to
exit
fr
om
a
d

d
squar
e?

..
ONE
STEP
ANAL
YSIS

Problem
.
F
or
the
Markov
chain
dene
d
in
Se
ction
..,
sele
ct
a
p
air
of
player
c
ongur
ations
and
determine
the
fol
lowing.

A
ver
age
time
to
hit
the
go
al

Pr
ob
ability
of
sc
oring
left
b
efor
e
right
go
al

L
ong
run
b
ehavior
of
the
b
al
l.
Problem
.
A
fair
c
oin
is
tosse
d
r
ep
e
ate
d
ly
until
k
=

suc
c
essive
he
ads
app
e
ar.
What
is
the
aver
age
numb
er
of
tosses
r
e
quir
e
d?
(Hint:
se
e
Pr
oblem
.,
or
run
a
simulation.)
Problem
.
One
of
the
quality
c
ontr
ol
rules
is
to
stop
the
pr
o
c
ess
when
on
k
=

suc
c
essive
days
the
pr
o
c
ess
runs
ab
ove,
or
b
elow,
sp
e
cic
ation
line.
On
aver
age,
how
often
a
pr
o
c
ess
that
do
es
not
ne
e
d
any
adjustment
wil
l
b
e
stopp
e
d
by
this
rule?
..
Example:
v
ehicle
insurance
claims
Supp
ose
that
y
ou
ha
v
e
an
insurance
con
tract
for
a
v
ehicle.
The
premium
pa
ymen
t
is
due
y
early
in
adv
ance
and
there
are
four
lev
els
P

>
P

>
P

>
P

.
The
basic
premium
is
P

,
unless
no
claims
w
ere
made
in
the
previous
y
ear.
If
no
claims
w
ere
made
in
a
giv
en
y
ear,
then
the
premium
for
the
follo
wing
y
ear
is
upgraded
to
the
next
category
.
Supp
ose
that
the
probabilit
y
of
the
damage
larger
than
s
h
undred
dollars
is
e
 s
(sub-
stitute
y
our
fa
v
orite
densit
y
for
the
exp
onen
tial
densit
y
used
in
this
example).
Because
of
the
incen
tiv
e
of
lo
w
ered
premium,
not
all
damage
should
b
e
rep
orted.
The
goal
of
this
example
is
to
determine
n
um
b
ers
s

;
s

;
s

;
s

ab
o
v
e
whic
h
the
claims
should
b
e
led;
s
j
is
a
cuto
used
in
a
y
ear
when
premium
P
j
is
paid.
Let
X
t
b
e
the
t
yp
e
of
premium
paid
in
t-th
y
ear.
Clearly
,
the
transitions
are
i
!

with
probabilit
y
e
 s
i
,

!

with
probabilit
y

 e
 s

,
etc.
In
the
long
run,
the
y
early
cost
is
C
(s

;
s

;
s

;
s

)
=
X

j
(P
j
+
m
j
);
(:)
where

j
are
the
equilibrium
probabilities
and
m
j
are
a
v
erage
un-reim
bursed
costs:
m
j
=
00
R
s
j
0
se
 s
ds.
The
optimal
claim
limits
follo
w
b
y
minimizing
expression
(.).
Exercise
.
Find
optimal
s
j
when
P

=
00;
P

=
0;
P

=
0;
P

=
00
(ab
out
0%
disc
ount).
..
Example:
a
game
of
piggy
In
a
game
of
piggy
,
eac
h
pla
y
er
tosses
t
w
o
dice,
and
has
an
option
of
adding
the
outcome
to
his
score,
or
rolling
again.
The
game
ends
when
the
rst
pla
y
er
exceeds
00
p
oin
ts.
Eac
h
pla
y
er
has
to
toss
the
dice
at
least
once
p
er
turn.
The
pla
y
er
can
c
ho
ose
to
toss
the
dice
as
man
y
times
as
(s)he
wishes
as
long
as
no
ace
o
ccurs.
Ho
w
ev
er
the
curren
t
total
is
added
to
pla
y
er's
score
only
when
the
pla
y
er
ends
his
turn
v
olun
tarily
.


CHAPTER
.
MARK
O
V
PR
OCESSES
If
an
ace
o
ccurs,
then
the
pla
y
er's
turn
is
o
v
er,
and
his
score
is
not
increased.
If
t
w
o
aces
o
ccur
then
the
score
is
set
to
zero.
A
pla
y
er
c
ho
oses
the
follo
wing
strategy:
toss
the
dice
for
as
long
as
an
ace
o
ccurs,
or
the
sum
of
curren
t
subtotal+score
exceeds
n
um
b
er
K
.
(If
her
score
exceeds
K
she
tosses
the
dice
once,
as
this
is
required
b
y
the
rules.)
The
quan
tit
y
to
optimize
is
the
a
v
erage
n
um
b
er
of
turns
that
tak
es
the
pla
y
er
to
reac
h
the
score
of
a
h
undred.
The
n
um
b
er
of
turns
under
this
strategy
is
the
n
um
b
er
of
aces
when
the
score
is
less
than
K
,
plus
the
n
um
b
er
of
thro
ws
when
the
score
is
larger
than
K
.
If
X
k
denotes
pla
y
ers
score
after
the
k
-th
thro
w,
then
clearly
X
k
is
a
Mark
o
v
c
hain
with
the
nite
n
um
b
er
of
states,
and
with
c
hance


of
returning
to
zero
at
eac
h
turn.
Whic
h
v
alue
of
K
minimizes
the
a
v
erage
n
um
b
er
of
turns
that
tak
es
the
pla
y
er
to
reac
h
a
h
undred?
A
simple-minded
computation
w
ould
just
tak
e
in
to
accoun
t
the
a
v
erage
gain
and
disregard
the
score
completely
.
If
the
pla
y
ers
subtotal
is
t
then
after
the
additional
thro
w
the
total
is
on
a
v
erage
=

(t
+
).
This
is
less
then
t
for
t

,
th
us
there
is
no
p
oin
t
con
tin
uing
b
ey
ond
.
Is
this
conclusion
correct?
Should
the
pla
y
er
c
ho
ose
to
stop
once
the
total
on
the
dice
exceeds
?
Exercise
.
What
is
the
aver
age
numb
er
of
turns
it
takes
to
r
e
ach
a
00
under
this
str
ate
gy?
A
less
simple-minded
computation
w
ould
tak
e
in
to
accoun
t
the
a
v
erage
gain:
If
the
pla
y
ers
score
is
s
then
after
the
additional
thro
w
his
score
on
a
v
erage
is
=

0
+
=

s
+
=

(s
+
)
whic
h
is
more
then
s
for
s
<


.
Is
this
conclusion
correct?
Should
the
pla
y
er
alw
a
ys
c
ho
ose
to
toss
again?
Exercise
.
What
is
the
aver
age
numb
er
of
turns
it
takes
to
r
e
ach
a
00
under
this
str
ate
gy?
Exercise
.	
What
is
the
aver
age
numb
er
of
turns
it
takes
to
r
e
ach
a
00
under
the
c
autious
str
ate
gy
of
no
additional
tosses?
Another
computation
w
ould
tak
e
in
to
accoun
t
the
curren
t
score
s
and
curren
t
subtotal
t.
After
the
additional
thro
w
the
n
um
b
ers
on
a
v
erage
are
s

=
=

0
+
=

s,
t

=
=

(t
+
).
On
a
v
erage,
t

+
s

>
t
+
s
when
t
<

 s=.
More
complicated
strategies
could
dep
end
on
curren
t
totals
of
other
pla
y
ers,
curren
t
score
of
the
pla
y
er,
and
the
subtotal
in
some
more
complicated
fashion.
F
or
instance,
if
s
denotes
the
score,
t
denotes
curren
t
subtotal,
one
could
stop
using
t
w
o-parameter
criterion
t
>
A
 B
s.
This
ma
y
ha
v
e
seemingly
dieren
t
eects
than
the
previous
strategy:
when
A
is
large,
and
score
s
is
close
to
0
there
is
no
need
for
stopping;
but
if
accum
ulated
score
s
is
large,
the
pla
y
er
ma
y
b
eha
v
e
more
cautiously
.
One
could
optimize
the
probabilit
y
of
winning
against
k
=
;

pla
y
ers
instead
of
just
minimizing
the
a
v
erage
n
um
b
er
of
tosses.
The
latter
puts
this
problem
in
to
game
theory
framew
ork.
Sim
ulations
seem
to
indicate
that
the
strategy
based
on
t
<

 
	
s
w
orks
w
ell
against
inexp
erienced
h
uman
pla
y
ers.

..
RECURRENCE
	
.
Recurrence
A
state
x
of
Mark
o
v
c
hain
is
recurren
t,
if
with
probabilit
y
one
the
c
hain
returns
bac
k
to
x.
Otherwise
it
is
called
transien
t.
If
X
t
is
irreducible
then
all
states
are
sim
ultaneously
either
recurren
t,
or
transien
t.
Theorem
..
State
x
is
r
e
curr
ent
i
P
P
x;x
(t)
=
.
Pro
of.
Let
M
b
e
the
n
um
b
er
of
times
X
t
returns
to
state
x.
Let
f
b
e
the
probabilit
y
of
returning
to
x.
State
x
is
recurren
t
i
f
=
.
Supp
ose
f
<

and
let
M
b
e
the
n
um
b
er
of
returns.
Clearly
Pr
x
(M

k
)
=
f
k
,
th
us
E
x
(M
)
=
f
=(
 f
)
<
.
Since
M
=
P
t
I
fX
t
=xg
,
E
x
(M
)
=
P
P
x;x
(t).

In
teresting
fact:
simple
random
w
alks
in
R
d
are
recurren
t
when
d
<
,
transien
t
when
d

.
Ho
w
ev
er
the
return
times
ha
v
e
innite
a
v
erage.
Theorem
..
L
et
T
b
e
a
r
eturn
time,
and
supp
ose
m(x)
=
E
x
T
<
.
Then
P
x;x
(t)
!
=m(x)
as
t
!
.
Problem
.0
Supp
ose
Markov
chain
X
t
moves
to
the
right
k
!
k
+

with
pr
ob
ability
=k
or
r
eturns
to

with
pr
ob
ability

 
k
,
k
=
;
;
:
:
:
.
Find
its
stationary
distribution,
and
the
aver
age
r
eturn
time
to
state
k
.
.
Sim
ulated
annealing
This
sections
describ
es
a
more
rened
metho
d
for
randomized
searc
h
of
minima
of
func-
tions.
Supp
ose
a
nite
set
V
is
giv
en,
and
w
e
are
to
minimize
a
function
U
:
V
!
I
R.
The
rst
step
of
design
is
to
sp
ecify
a
c
onne
cte
d
directed
graph
G
=
(V
;
E
).
In
other
w
ords,
for
ev
ery
p
oin
t
u

V
w
e
need
to
pic
k
the
set
of
directed
edges
(u;
v
)
for
the
mark
o
v
c
hain
to
follo
w
from
state
u.
(This
step
is
usually
p
erformed
for
computa-
tional
eciency;
theoretically
,
all
p
ossible
transitions
could
b
e
admissible.)
The
second
step
is
to
c
ho
ose
"con
trol
parameter"

that
will
v
ary
as
the
program
is
running.
The
third
step
is
to
dene
transition
probabilities:
P
(u;
v
)
=
C
(u)e
 
(U
(v
) U
(u))
+
;
(:)
where
C
(u;
v
)
=
P
v
e
 
(U
(v
) U
(u))
+
is
the
norming
constan
t,
and
the
only
v
's
con-
sidered
are
those
with
(u;
v
)

E
.
(Can
y
ou
explain
no
w
wh
y
G
shouldn't
b
e
the
complete
graph).

	0
CHAPTER
.
MARK
O
V
PR
OCESSES
Theorem
..
The
invariant
me
asur
e
of
tr
ansition
matrix
(.)
is
p

(u)
=

Z
e

U
(u)
Pro
of.
T
o
c
hec
k
the
in
v
ariance
condition,
denote
N
(u)
=
fv
:
(u;
v
)

E
g.

An
ecien
t
realization
of
the
ab
o
v
e
is
to
pic
k
v

N
u
at
random
and
accept
it
with
probabilit
y
(

if
U
(v
)

U
(u)
e
 U
(v
)
otherwise.
..
Program
listing
The
program
is
a
v
ailable
online,
or
on
the
disk.
.
Solving
dierence
equations
through
sim
ula-
tions
In
the
example
b
elo
w
w
e
limit
our
atten
tion
to
the
one-dimensional
problem.
This
sim-
plies
the
notation,
while
the
basic
ideas
are
the
same.
Let
u(x;
t)
b
e
an
unkno
wn
function
of
x

I
R
d
;
t

0.
The
dierence
equation
w
e
ma
y
w
an
t
to
solv
e
is
the
follo
wing
discrete
analog
of
the
diusion
equation.
u(x;
t
+
)
=
u(x;
t)
+
A


d
X
v
=e
k
u(x
+
v
;
t)
(.)
u(x;
o)
=
u
0
(x)
(.)
The
solution
is
u(x;
t)
=
E
(u
0
(S
t
)),
where
S
t
=
P
j
t

j
is
the
sum
of
indep
enden
t
random
v
ariables
with

d
equally
lik
ely
v
alues
e
k

I
R
d
.
.
Mark
o
v
Autoregressiv
e
pro
cesses
Supp
ose

k
is
a
stationary
Mark
o
v
c
hain
and
let
X
n
b
e
the
solution
of
the
dierence
equation
X
n+
 aX
n
=

n+
.
One
can
write
the
transition
Matrix
for
Mark
o
v
pro
cess
X
t
and
try
nd
the
stationary
distribution
for
X
0
.
A
more
direct
metho
d
is
based
on
the
fact
that
the
solution
of
the
dierence
equation
is
X
t
=
a
t
X
0
+
a
t 


+
:
:
:
+
a
t 
+

t
.
Therefore
if
jaj
<
,
the
stationary
initial
distribution
is
X
0
=
P
a
k

k
.
Th
us
X
t
=
P

k
=0
a
k

t k
Problem
.
Supp
ose

k
ar
e
i.
i.
d.
Find
the
c
ovarianc
e
E
X
0
X
k
.
Problem
.
Supp
ose

k
ar
e
i.
i.
d.
Find
E
(X
k
jX
0
).
Solutions
of
higher
order
dierence
equations
can
b
e
easily
out
in
to
the
Mark
o
v
frame-
w
ork,
to
o.
If
X
n+
+
aX
n+
+
bX
n
=

n+
then
Y
n
=
(X
n+
;
X
n
)
is
Mark
o
v
and
satises
the
corresp
onding
equation
in
matrix
form:
Y
n+
=
AY
n
+

n+
.
Therefore
the
stationary
solution
exist
pro
vided
that
the
eigen
v
alues
of
A
satisfy
inequalit
y
j
j
j
<
.

..
SOR
TING
A
T
RANDOM
	
.
Sorting
at
random
Eciency
of
sorting
algorithms
is
often
measured
b
y
the
n
um
b
er
of
comparisons
required.
T
o
sort
ecien
tly
a
set
S
of
n
um
b
ers
in
to
ascending
order
w
e
should
nd
a
n
um
b
er
y
suc
h
that
ab
out
half
of
the
elemen
ts
of
S
is
b
elo
w
y
.
Then
the
total
n
um
b
er
of
steps
required
is
T
(n)

T
(=n=)
+
n
+
C
(n),
where
C
(n)
is
the
n
um
b
er
of
comparisons
required
to
nd
y
.
R
andom
Quick
Sort
is
based
on
an
idea
that
a
random
c
hoice
of
y
is
go
o
d
enough
on
a
v
erage.
Theorem
..
The
exp
e
cte
d
numb
er
of
c
omp
arisons
in
r
andom
quick
sort
is
at
most
n
ln
(n
+
).
Here
is
one
p
ossible
realization
of
the
subprogram:
PROGRAM
qsrt.bas
'
SUB
QuickSort
(Index(),
Aux(),
First%,
Last%)
'sorts
two
matrices
in
increasing
order
by
the
valuies
of
Index()
from
pocz
to
kon
'
Note:
mixes
order
of
indices
corresponding
to
equal
Index(j)
'**
Quick-sort
(ascending)
the
fields
in
Array$(),
from
field
First%
thru
Field
Last%
IF
First%
>=
Last%
THEN
EXIT
SUB
CONST
max
=
0
DIM
Lft%(max
+
),
Rght%(max
+
)
Temp%
=

Lft%()
=
First%
Rght%()
=
Last%
DO
Start%
=
Lft%(Temp%)
Ende%
=
Rght%(Temp%)
Temp%
=
Temp%
-

DO
IndexLft%
=
Start%
IndexRght%
=
Ende%
x
=
Index((Start%
+
Ende%)
\
)
DO
WHILE
Index(IndexLft%)
<
x
AND
IndexLft%
<
Ende%
IndexLft%
=
IndexLft%
+

WEND
WHILE
x
<
Index(IndexRght%)
AND
IndexRght%
>
Start%
IndexRght%
=
IndexRght%
-

WEND
IF
IndexLft%
>
IndexRght%
THEN
EXIT
DO
SWAP
Index(IndexLft%
),
Index(IndexRght%
)
'**
switch
elements
SWAP
Aux(IndexLft%),
Aux(IndexRght%)
IndexLft%
=
IndexLft%
-
(IndexLft%
<
Ende%)
IndexRght%
=
IndexRght%
+
(IndexRght%
>
Start%)
LOOP
IF
IndexRght%
-
Start%
>=
Ende%
-
IndexLft%
THEN
IF
Start%
<
IndexRght%
THEN
Temp%
=
Temp%
+


	
CHAPTER
.
MARK
O
V
PR
OCESSES
Lft%(Temp%)
=
Start%
Rght%(Temp%)
=
IndexRght%
END
IF
Start%
=
IndexLft%
ELSE
IF
IndexLft%
<
Ende%
THEN
Temp%
=
Temp%
+

Lft%(Temp%)
=
IndexLft%
Rght%(Temp%)
=
Ende%
END
IF
Ende%
=
IndexRght%
END
IF
LOOP
WHILE
Start%
<
Ende%
IF
Temp%
>
max
THEN
Temp%
=
0
LOOP
WHILE
Temp%
'
END
SUB
.	
An
application:
nd
k
-th
largest
n
um
b
er
The
follo
wing
theorem
o
ccasionally
helps
to
estimate
the
a
v
erage
time
of
accomplishing
a
n
umerical
task.
Theorem
.	.
Supp
ose
g
(x)
is
incr
e
asing
(non-de
cr
e
asing)
function
and
X
t
is
a
Markov
chain
on
I
N
that
moves
left
only
and
E
(X
t+
jX
t
=
m)

m
+
g
(m).
L
et
T
b
e
the
time
of
r
e
aching
.
Then
E
n
(T
)

R
n


g
(x)
dx.
Pro
of.
By
induction
E
n
(T
)


+
E
R
X

dx
g
(x)
=

+
R
n

dx
g
(x)
 E
R
n
X
dx
g
(x)


+
R
n

dx
g
(x)
 E
R
n
X
dx
g
(n)


+
R
n

dx
g
(x)
+
E
X
 n
g
(n)

As
an
application
w
e
consider
the
follo
wing
algorithm
to
pic
k
the
k
-th
n
um
b
er
in
order
from
a
set
S
of
n
n
um
b
ers.
.
Initialize
S

=
S;
S

=
;.
.
Pic
k
y
at
random
from
S

.
.
Revise
sets
S

=
fx
:
x
<
y
g;
S

=
fx
:
x
>
y
g.
.
If
jS

j
=
k
 
then
y
w
as
found.
.
If
jS

j
>
k
then
rep
eat
the
pro
cess
with
new
S

.
.
If
jS

<
k
 
then
sw
ap
S

and
S

,
replace
k
b
y
k
 jS

j
 ,
and
rep
eat
the
pro
cess.
Problem
.
Estimate
the
aver
age
running
time
of
the
ab
ove
algorithm.
(ANS:
E
T


ln
n).

Chapter
	
Branc
hing
pro
cesses
Supp
ose
certain
ob
jects
m
ultiply
indep
enden
tly
and
in
discrete
time
in
terv
als.
Eac
h
ob
ject
at
the
end
of
ev
ery
p
erio
d
pro
duces
a
random
n
um
b
er

of
descendan
ts
(ospring)
with
the
probabilit
y
distribution
p
k
=
Pr
(
=
k
).
Let
X
t
b
e
the
total
n
um
b
er
of
ob
jects
at
t-th
generation.
Then
in
distribution
X
t+
=
P
X
t
j
=

j
.
The
three
questions
of
in
terest
are
the
a
v
erage
size
of
the
p
opulation,
its
v
ariance,
and
the
probabilit
y
of
extinction.
Denition
	.0.
By
extinction
we
me
an
the
event
that
the
r
andom
se
quenc
e
fX
t
g
c
on-
sists
of
zer
os
for
al
l
but
the
nite
numb
er
of
values
of
t

I
N
.
Probabilit
y
of
extinction
b
y
time
n
can
b
e
found
directly
from
the
rst-step-analysis:
n
um
b
ers
u
n
=
Pr(X
n
=
0)
satisfy
u
n+
=

X
k
=0
p
k
(u
n
)
k
:
(	:)
	.
The
mean
and
the
v
ariance
Let

n
=
E
(X
n
);
V
n
=
V
ar
(X
n
).
By
Theorem
..
and
induction
w
e
ha
v
e

n
=

n
(	.)
V
n
=



n 
(
 
n
)=(
 ):
(	.)
	.
Generating
functions
of
Branc
hing
pro
cesses
Let
g
(z
)
=
P

k
=0
p
k
z
k
b
e
the
generating
function.
Clearly
g
(z
)

p
0
+
z
for
small
z
.
Equation
(	.)
for
probabilities
u
n
of
extinction
b
y
the
n-th
generation
is
u
n+
=
g
(u
n
).
Theorem
	..
(W
atson())
The
gener
ating
function
H
n
(z
)
of
X
n
is
the
n-fold
c
omp
osition
(iter
ation)
of
g
.
	

	
CHAPTER
	.
BRANCHING
PR
OCESSES
Pro
of.
E
(z
X
n+
jX
n
=
k
)
=
(g
(z
))
k
.
Th
us
b
y
total
probabilit
y
form
ula
(.)
H
n+
(z
)
=
E
z
X
n+
=
P
k
(g
(z
))
k
Pr(X
n
=
k
)
=
H
n
(g
(z
)).

In
particular,
E
X
n
=
d
dz
g
(n)
(z
)j
z
=
=

n
and
u
n
=
Pr(X
n
=
0)
=
g
(n)
(0).
Problem
	.
Pr
ove
(	.)
using
moment
gener
ating
function
dir
e
ctly.
	..
Extinction
Notice
that
if
there
is
a
limit
of
q
=
lim
n!
g
(n)
(0),
then
it
has
to
satisfy
the
equation
g
(s)
=
s:
(	:)
Since
X
t
is
in
teger
v
alued
and
the
ev
en
ts
A
n
=
fX
t
=
0g
are
decreasing,
b
y
con
tin
uit
y
(.)
the
probabilit
y
of
extinction
q
=
lim
n!
Pr(X
n
=
0)
exists.
Theorem
	..
If
E
X


,
the
extinction
pr
ob
ability
q
=
.
If
E
X

>
,
the
extinction
pr
ob
ability
is
the
unique
nonne
gative
solution
less
than

of
the
e
quation
(	.).
Pro
of.
This
is
b
est
understo
o
d
b
y
graphing
g
(s)
and
marking
the
iterates
on
the
diagonal.
Chec
k
b
y
induction
that
g
n
(0)
<

for
all
n

I
N
.
If
there
is
a
solution
s
0
<

of
g
(s)
=
s,
then
it
is
the
\attractiv
e
p
oin
t"
of
the
iteration.

	.
Tw
o-v
alued
case
Belo
w
w
e
re-analyzes
the
gro
wth
mo
del
presen
ted
in
Section
...
Supp
ose
that
the
probabilities
of
ospring
are
p
0
=

;
p

=

 
.
The
generating
function
is
g
(z
)
=

+
(
 
)z

.
Asymptotic
probabilit
y
of
extinction
solv
es
quadratic
equation
(
 
)z

 z
+

=
0.
The
ro
ots
are
z

=

and
z

=

 
.
In
particular,
probabilit
y
of
extinction
is

when




.
When

=
=
probabilities
of
extinction
at
n
 th
generation
are
u
0
=
0;
u

=
:;
u

=
:	;
u

=
:;
u

=
:	;
u

=
:;
u

=
:.
	.
Geometric
case
Supp
ose
that
the
probabilities
of
ospring
are
p
0
=

 
;
p
k
=

(
 )
k
.
The
generating
function
is
g
(z
)
=
(
 
)
+

(
 )
z
 z
.
The
most
in
teresting
feature
of
this
momen
t
generating
function
is
that
it
can
b
e
readily
comp
osed.
Lemma
	..
The
c
omp
osition
of
fr
actional
line
ar
functions
f
(z
)
=
a+bz
c+dz
and
g
(z
)
=
A+B
z
C
+D
z
is
a
fr
actional
line
ar
function
(with
the
c
o
ecients
given
by
the
matrix
multiplic
a-
tion!).

	..
GEOMETRIC
CASE
	
Asymptotic
probabilit
y
of
extinction
has
to
solv
e
quadratic
equation

 
+

(
 )
z
 z
=
z
.
The
ro
ots
are
z

=

and
z

=

 
.
In
particular,
probabilit
y
of
extinction
is

when




.
Since
the
iterates
of
the
generating
function
can
actually
b
e
written
do
wn
explicitly
,
in
geometric
case
Pr(X
n
=
0)
=

 m
n
(
 p
e
)=(m
n
 p
e
)
is
explicit.
Here
p
e
=
z

,
m
=
:::.
Problem
	.
Supp
ose
that
in
a
br
anching
pr
o
c
ess
the
numb
er
of
ospring
of
the
initial
se
e
d
ling
has
a
distribution
with
gener
ating
function
F
(z
).
Each
memb
er
of
the
next
gen-
er
ation
has
the
numb
er
of
ospring
whose
distribution
has
gener
ating
function
G(z
).
The
distributions
alternate
b
etwe
en
gener
ations.

Find
the
extinction
pr
ob
ability
in
terms
of
F
;
G.

What
is
the
aver
age
p
opulation
size?
Problem
	.
In
a
simple
mo
del
of
line
ar
immunolo
gic
al
r
esp
onse,
the
doubling
pr
ob
a-
bility
p
of
the
p
opulation
of
b
acteria
changes
with
time
due
to
the
incr
e
ase
d
numb
er
of
lympho
cytes.
If
ther
e
ar
e
X
(t)
b
acteria
at
t-th
gener
ation,
then
assume
p
=
a=(t
+
a).
Find
the
pr
ob
ability
u
t
of
extinction
by
t-th
gener
ation
for
infe
ction
by
0
b
acteria.
What
is
the
aver
age
length
of
the
dise
ase?

	
CHAPTER
	.
BRANCHING
PR
OCESSES

Chapter
0
Multiv
ariate
normal
distribution
Univ
ariate
normal
distribution,
standardization,
and
its
momen
t
generating
function
w
ere
in
tro
duced
in
Chapter
.
Belo
w
w
e
dene
m
ultiv
ariate
normal
distribution.
0.
Multiv
ariate
momen
t
generating
function
W
e
follo
w
the
usual
linear
algebra
notation.
V
ectors
are
denoted
b
y
small
b
old
letters
x;
v
;
t,
matrices
b
y
capital
b
old
initial
letters
A;
B;
C
and
v
ector-v
alued
random
v
ariables
b
y
capital
b
oldface
X;
Y
;
Z;
b
y
the
dot
w
e
denote
the
usual
dot
pro
duct
in
I
R
d
,
ie.
x

y
:=
P
d
j
=
x
j
y
j
;
kxk
=
(x

x)
=
denotes
the
usual
Euclidean
norm.
F
or
t
yp
ographical
con
v
enience
w
e
sometimes
write
(a

;
:
:
:
;
a
k
)
for
the
v
ector




a

.
.
.
a
k




.
By
A
T
w
e
denote
the
transp
ose
of
a
matrix
A.
Belo
w
w
e
shall
also
consider
another
scalar
pro
duct
h;
i
asso
ciated
with
the
normal
distribution;
the
corresp
onding
semi-norm
will
b
e
denoted
b
y
the
triple
bar
j
j
j

j
j
j.
Denition
0..
A
n
I
R
d
-value
d
r
andom
variable
Z
is
multivariate
normal,
or
Gaussian
(we
shal
l
use
b
oth
terms
inter
change
ably;
the
se
c
ond
term
wil
l
b
e
pr
eferr
e
d
in
abstr
act
situations)
if
for
every
t

I
R
d
the
r
e
al
value
d
r
andom
variable
t

Z
is
normal.
Example
0.
L
et


;


;
:
:
:
b
e
i.
i.
d.
N
(0;
).
Then
X
=
(

;


;
:
:
:
;

d
)
is
multivariate
normal.
Example
0.
L
et

b
e
N
(0;
).
Then
X
=
(
;

;
:
:
:
;

)
is
multivariate
normal.
Example
0.
L
et


;


;
:
:
:
b
e
i.
i.
d.
N
(0;
).
Then
X
=
(X

;
X

;
:
:
:
;
X
T
),
wher
e
X
k
=
P
k
j
=

j
ar
e
p
artial
sums,
is
multivariate
normal.
Clearly
the
distribution
of
univ
ariate
t

Z
is
determined
uniquely
b
y
its
mean
m
=
m
t
and
its
standard
deviation

=

t
.
It
is
easy
to
see
that
m
t
=
t

m,
where
m
=
E
Z.
Indeed,
b
y
linearit
y
of
the
exp
ected
v
alue
m
t
=
E
t

Z
=
t

E
Z.
Ev
aluating
the
momen
t
	

	
CHAPTER
0.
MUL
TIV
ARIA
TE
NORMAL
DISTRIBUTION
generating
function
M
(s)
of
the
real-v
alued
random
v
ariable
t

Z
at
s
=

w
e
see
that
the
momen
t
generating
function
of
Z
can
b
e
written
as
M
(t)
=
exp
(t

m
+


t

):
0.
Biv
ariate
normal
distribution
In
this
section
w
e
consider
a
pair
of
(join
tly)
normal
random
v
ariables
X

;
X

.
F
or
sim-
plicit
y
of
notation
w
e
supp
ose
E
X

=
0;
E
X

=
0.
Let
V
ar
(X

)
=



;
V
ar
(X

)
=



and
denote
cor
r
(X

;
X

)
=
.
Then
the
co
v
ariance
matrix
is
C
=
"








#
and
the
join
t
momen
t
generating
function
is
M
(t

;
t

)
=
exp
(


t





+


t





+
t

t

):
If




=
0
w
e
can
normalize
the
v
ariables
and
consider
the
pair
Y

=
X

=

and
Y

=
X

=

.
The
co
v
ariance
matrix
of
the
last
pair
is
C
Y
=
"




#
;
it
generates
scalar
pro
duct
giv
en
b
y
*"
x

x

#
;
"
y

y

#
+
=
x

y

+
x

y

+
x

y

+
x

y

and
the
corresp
onding
(semi)-norm
is
j
j
j
"
x

x

#
j
j
j
=
(x


+
x


+
x

x

)
=
.
Notice
that
when

=

the
semi-norm
is
degenerate
and
equals
jx


x

j.
Denoting

=
sin

,
it
is
easy
to
c
hec
k
that
Y

=


cos

+


sin

;
Y

=


sin

+


cos

for
some
i.i.d
normal
N
(0;
)
r.
v.


;


.
One
w
a
y
to
see
this,
is
to
compare
the
v
ariances
and
the
co
v
ariances
of
b
oth
sides.
This
implies
that
the
join
t
densit
y
of
Y

and
Y

is
giv
en
b
y
f
(x;
y
)
=


cos

exp
( 

cos


(x

+
y

 xy
sin

))
(0:)
whic
h
is
a
v
arian
t
of
(.).
Another
represen
tation
Y

=


;
Y

=


+
q

 



illustrates
non-uniqueness
of
the
linear
represen
tation.
The
latter
represen
tation
mak
es
the
follo
wing
Theorem
ob
vious
in
the
biv
ariate
case.

0..
BIV
ARIA
TE
NORMAL
DISTRIBUTION
		
Theorem
0..
L
et
X;
Y
b
e
jointly
normal.
(i)
If
X;
Y
ar
e
unc
orr
elate
d,
then
they
ar
e
indep
endent.
(ii)
E
(Y
jX)
=
m
+
AX
is
line
ar
(iii)
Y
 AX
and
X
ar
e
indep
endent.
Returning
bac
k
to
random
v
ariables
X

;
X

,
w
e
ha
v
e
X

=




cos

+




sin

and
X

=




sin

+




cos

;
this
represen
tation
holds
true
also
in
the
degenerate
case.
0..
Example:
normal
random
w
alk
In
this
example
w
e
analyze
a
discrete
time
Gaussian
random
w
alk
fX
k
g
0k
T
.
Let


;


;
:
:
:
b
e
i.
i.
d.
N
(0;
).
W
e
are
in
terested
in
explicit
form
ulas
for
the
momen
t
generat-
ing
function
and
for
the
densit
y
of
the
I
R
T
-v
alued
random
v
ariable
X
=
(X

;
X

;
:
:
:
;
X
T
),
where
X
k
=
k
X
j
=

j
(0:)
are
partial
sums.
Clearly
,
m
=
0.
Equation
(0.)
expresses
X
as
a
linear
transformation
X
=
Ag
of
the
i.
i.
d.
standard
normal
v
ector
with
A
=







0
:
:
:
0


:
:
:
0
.
.
.
.
.
.
.
.
.


:
:
:







:
Therefore
from
(0.)
w
e
get
M
(t)
=
exp


(t


+
(t

+
t

)

+
:
:
:
+
(t

+
t

+
:
:
:
+
t
T
)

):
T
o
nd
the
form
ula
for
join
t
densit
y
,
notice
that
A
is
the
matrix
represen
tation
of
the
linear
op
erator,
whic
h
to
a
giv
en
sequence
of
n
um
b
ers
(x

;
x

;
:
:
:
;
x
T
)
assigns
the
sequence
of
its
partial
sums
(x

;
x

+
x

;
:
:
:
;
x

+
x

+
:
:
:
+
x
T
).
Therefore,
its
in
v
erse
is
the
nite
dierence
op
erator

:
(x

;
x

;
:
:
:
;
x
T
)
!
(x

;
x

 x

;
:
:
:
;
x
T
 x
T
 
).
This
implies
A
 
=












0
0
:
:
:
:
:
:
0
 

0
:
:
:
:
:
:
0
0
 

:
:
:
:
:
:
0
0
0
 
:
:
:
:
:
:
0
.
.
.
.
.
.
.
.
.
.
.
.
0
:
:
:
0
:
:
:
 












:
Since
det
A
=
,
w
e
get
f
(x)
=
(
)
 n=
exp
 

(x


+
(x

 x

)

+
:
:
:
+
(x
T
 x
T
 
)

):
(0:)
In
terpreting
X
as
the
discrete
time
pro
cess
X

;
X

;
:
:
:
,
the
probabilit
y
densit
y
function
for
its
tra
jectory
x
is
giv
en
b
y
f
(x)
=
C
exp
( 

kxk

).
Expression


kxk

can
b
e
in
terpreted
as
prop
ortional
to
the
kinetic
energy
of
the
motion
describ
ed
b
y
the
path
x;
assigning
probabilities
b
y
C
e
 E
ner
g
y
=(k
T
)
is
a
w
ell
kno
wn
practice
in
statistical
ph
ysics.
In
con
tin
uous
time,
the
deriv
ativ
e
pla
ys
analogous
role.

00
CHAPTER
0.
MUL
TIV
ARIA
TE
NORMAL
DISTRIBUTION
0.
Sim
ulating
a
m
ultiv
ariate
normal
distribution
T
o
sim
ulate
an
y
d-dimensional
normal
distribution
w
e
need
only
to
sim
ulate
d
indep
enden
t
N
(0;
)
random
v
ariables
and
use
linear
represen
tations
lik
e
in
Theorem
0...
F
or
suc
h
sim
ulation
the
co
v
ariance
matrix
needs
to
b
e
in
v
erted
and
diagonalized,
a
n
umerical
n
uisance
in
itself.
When
the
m
ultiv
ariate
normal
distribution
o
ccurs
as
the
so-called
time
series,
a
metho
d
based
on
F
ourier
expansion
is
then
con
v
enien
t,
see
Section
.,
or
the
in
tro
ductory
examples
in
Chapter
.
0..
General
m
ultiv
ariate
normal
la
w
The
linear
algebra
results
imply
that
the
momen
t
generating
function
corresp
onding
to
a
normal
distribution
on
I
R
d
can
b
e
written
in
the
form
M
(t)
=
exp
(t

m
+


Ct

t):
(0:)
0.
Co
v
ariance
matrix
Theorem
..
iden
ties
m

I
R
d
as
the
mean
of
the
normal
random
v
ariable
Z
=
(Z

;
:
:
:
;
Z
d
);
similarly
,
double
dieren
tiation
M
(t)
at
t
=
0
sho
ws
that
C
=
[c
i;j
]
is
giv
en
b
y
c
i;j
=
C
ov
(Z
i
;
Z
j
).
This
establishes
the
follo
wing.
Theorem
0..
The
moment
gener
ating
function
c
orr
esp
onding
to
a
normal
r
andom
variable
Z
=
(Z

;
:
:
:
;
Z
d
)
is
given
by
(0.),
wher
e
m
=
E
Z
and
C
=
[c
i;j
],
wher
e
c
i;j
=
C
ov
(Z
i
;
Z
j
),
is
the
c
ovarianc
e
matrix.
F
rom
(0.)
and
linear
algebra
w
e
get
also
M
(t)
=
exp
(t

m
+


(A
t)

(A
t)):
(0:)
W
e
ha
v
e
the
follo
wing
m
ultiv
ariate
generalization
of
(.).
Theorem
0..
Each
d-dimensional
normal
r
andom
variable
Z
has
the
same
distribu-
tion
as
m
+
Ag
,
wher
e
m

I
R
d
is
deterministic,
A
is
a
(symmetric)
d

d
matrix
and
g
=
(

;
:
:
:
;

d
)
is
a
r
andom
ve
ctor
such
that
the
c
omp
onents


;
:
:
:
;

d
ar
e
indep
endent
N
(0;
)
r
andom
variables.
Pro
of.
Clearly
,
E
exp
(t

(m
+
Ag
))
=
exp
(t

m)E
exp(t

(Ag
))
.
Since
the
momen
t
gen-
erating
function
of
g
is
E
exp
(x

g
)
=
exp


kxk

and
t

(Ag
)
=
(A
T
t)

g
,
therefore
w
e
get
E
exp
(t

(m
+
Ag
))
=
exp
t

m
exp
+


kA
T
tk

,
whic
h
is
another
form
of
(0.).


0..
CO
V
ARIANCE
MA
TRIX
0
0..
Multiv
ariate
normal
densit
y
No
w
w
e
consider
the
m
ultiv
ariate
normal
densit
y
.
The
densit
y
of
indep
enden
t


;
:
:
:
;
in
Theorem
0..
is
the
pro
duct
of
the
one-dimensional
standard
normal
densities,
ie.
f
g
(x)
=
(
)
 d=
exp
( 

kxk

):
Supp
ose
that
det
C
=
0,
whic
h
ensures
that
A
is
nonsingular.
By
the
c
hange
of
v
ariable
form
ula,
from
Theorem
0..
w
e
get
the
follo
wing
expression
for
the
m
ultiv
ariate
normal
densit
y
.
Theorem
0..
If
Z
is
c
enter
e
d
normal
with
the
nonsingular
c
ovarianc
e
matrix
C,
then
the
density
of
Z
is
given
by
f
Z
(x)
=
(
)
 d=
(det
A)
 
exp
( 

kA
 
xk

);
or
f
Z
(x)
=
(
)
 d=
(det
C)
 =
exp
( 

C
 
x

x);
wher
e
matric
es
A
and
C
ar
e
r
elate
d
by
C
=
A

A
T
.
In
the
nonsingular
case
the
densit
y
expression
implies
strong
in
tegrabilit
y
.
Theorem
0..
If
Z
is
normal,
then
ther
e
is

>
0
such
that
E
exp
(kZk

)
<
:
Remark

Theorem
0..
holds
true
also
in
the
singular
case
and
for
Gaussian
random
v
ariables
with
v
alues
in
innite
dimensional
spaces.
0..
Linear
regression
F
or
general
m
ultiv
ariate
normal
random
v
ariables
X
and
Y
w
e
ha
v
e
the
follo
wing
linearit
y
of
regression
result.
Theorem
0..
If
(X;
Y
)
has
jointly
normal
distribution
on
I
R
d

+d

,
then
E
fXjY
g
=
a
+
QY
;
(0:)
R
andom
ve
ctors
Y
 QY
and
X
ar
e
sto
chastic
al
ly
indep
endent.
V
ector
a
=
m
X
 Qm
Y
and
matrix
Q
are
determined
b
y
the
exp
ected
v
alues
m
X
;
m
Y
and
b
y
the
(join
t)
co
v
ariance
matrix
C
(uniquely
if
the
co
v
ariance
C
Y
of
Y
is
non-
singular).
T
o
nd
Q,
m
ultiply
(0.)
(as
a
column
v
ector)
from
the
righ
t
b
y
(Y
 E
Y
)
T
and
tak
e
the
exp
ected
v
alue.
By
Theorem
A..(i)
w
e
get
Q
=
R

C
 
Y
,
where
w
e
ha
v
e
written
C
as
the
(suitable)
blo
c
k
matrix
C
=
"
C
X
R
R
T
C
Y
#
.
Problem
0.
F
or
the
r
andom
walk
fr
om
Se
ction
0..,
what
is
E
(X
k
jX

;
:
:
:
;
X
k
 
;
X
k
+
;
:
:
:
;
X
n
)?
Problem
0.
Supp
ose
X

;
:
:
:
;
X
d
ar
e
jointly
normal,
E
X
j
=
0;
E
X

j
=

and
al
l
c
o-
varianc
es
E
X
i
X
j
=

ar
e
the
same
for
i
=
j
.
Find
E
(X

jX

;
X

;
:
:
:
;
X
d
).
(Notic
e
that
in
this
example

>
 =d.)

0
CHAPTER
0.
MUL
TIV
ARIA
TE
NORMAL
DISTRIBUTION
0.
Gaussian
Mark
o
v
pro
cesses
Supp
ose
(X
t
)
t=0;;:::
is
a
Mark
o
v
c
hain
with
m
ultiv
ariate
normal
distribution.
That
is,
supp
ose
X
0
is
normal,
and
the
transition
probabilities
are
normal,
to
o.
Without
loss
of
generalit
y
w
e
assume
E
X
t
=
0;
E
X

t
=

and
let
E
X
0
X

=
.
Then
E
(X
t+
jX
t
)
=
X
t
and
therefore
E
X
0
X
t
=

t
.
This
means
that
the
co
v
ariance
matrix
of
the
Mark
o
v
Gaussian
pro
cess
dep
ends
on
one
parameter

only
.
Comparing
the
answ
er
with
Section
.
w
e
nd
out
that
all
homogeneous
Mark
o
v
Gaussian
pro
cesses
ha
v
e
the
form
X
t
=
P

k
=0

k
+t

k
,
where

k
are
indep
enden
t
normal
r.
v.

Chapter

Con
tin
uous
time
pro
cesses
Con
tin
uous
time
pro
cesses
are
the
families
of
random
v
ariables
X
t
,
with
t

0
in
terpreted
as
time.
.
P
oisson
pro
cess
P
oisson
distribution
o
ccurs
as
an
appro
ximation
to
binomial.
Another
reason
for
its
o
ccurrence
is
related
to
exp
onen
tial
random
v
ariables
and
coun
ting
customers
in
queues.
The
latter
is
p
erhaps
the
most
ecien
t
w
a
y
of
sim
ulating
P
oisson
random
v
ariables,
see
Section
...
A
related
reason
for
the
frequen
t
use
of
P
oisson
distribution
in
mo
deling
is
the
la
w
of
rare
ev
en
ts.
Denition
..
A
Poisson
pr
o
c
ess
of
intensity

>
0
is
an
inte
ger-value
d
sto
chastic
pr
o
c
ess
fN
t
:g
with
the
fol
lowing
pr
op
erties.

N
0
=
0

F
or
s
>
0;
t
>
0
r
andom
variable
N
t+s
 N
t
has
Poisson
distribution
with
p
ar
ameter
s.

N
t
has
indep
endent
incr
ements
Supp
ose
cars
pass
b
y
an
in
tersection
and
the
times
b
et
w
een
their
arriv
als
are
indep
enden
t
and
exp
onen
tial.
Th
us
w
e
are
giv
en
i.
i.
d
sequence
T
j
of
exp
onen
tial
r.
v.
(with
parameter
).
The
n
um
b
er
of
cars
that
passed
b
y
within
time
in
terv
al
(0;
t)
is
a
random
v
ariable
N
t
.
Clearly
,
N
t
is
the
rst
in
teger
k
suc
h
that
T

+
:
:
:
T
k
>
t.
Theorem
..
F
or
t
>
0;
s
>
0
r
andom
variable
N
(t
+
s)
 N
(t)
is
indep
endent
of
N
t
and
has
Poisson
distribution
P
oiss(s).
Pro
of.
W
e
will
pro
v
e
only
that
N
t
has
the
P
oisson
distribution.
T
o
simplify
notation
assume
that
exp
onen
tial
r.`v.
ha
v
e
parameter

=
.
W
e
pro
v
e
the
form
ula
Pr(N
t
=
k
)
=
t
k
k
!
e
 t
b
y
induction.
k
=
0:
Pr(N
t
=
0)
=
Pr(T
>
t)
=
R

t
e
 x
dx
=
e
 t
0

0
CHAPTER
.
CONTINUOUS
TIME
PR
OCESSES
Supp
ose
the
form
ula
is
true
for
k
.
Then
Pr
(N
t
=
k
+
)
=
Pr(T

+
:
:
:
+
T
k
+
>
t)
=
Z

t
= (k
+
)x
k
e
 x
dx:
In
tegrating
b
y
parts
w
e
c
hec
k
that
Pr(N
t
=
k
+
)
=
t
k
+

Pr(N
t
=
k
):

Similar
pro
cesses
are
considered
in
reliabilit
y
theory
,
and
in
queueing
theory
,
also
for
non-exp
onen
tial
so
journ
times
T
j
.
Problem
.
Assume
a
devic
e
fails
when
a
cumulative
ee
ct
of
k
sho
cks
o
c
curs.
If
the
sho
cks
o
c
cur
ac
c
or
ding
to
the
Poisson
pr
o
c
ess
with
p
ar
ameter
,
what
is
the
density
function
for
the
life
T
of
the
devic
e?
Problem
.
L
et
f
(x;
t)
=
E
f
(x
+
N
t
),
wher
e
N
t
is
the
Poisson
pr
o
c
ess
of
intensity

=
.
Show
that
@
f
@
t
=
f
(x
+
)
 f
(x).
In
p
articular,
p
t
(k
)
=
Pr(N
t
=
k
)
satises
@
p
t
(k
)
@
t
=
p
t
(k
+
)
 p
t
(k
).
Problem
.
Customers
arrive
at
a
facility
at
r
andom
ac
c
or
ding
to
a
Poisson
pr
o
c
ess
of
r
ate
.
The
customers
ar
e
disp
atche
d
(pr
o
c
esse
d)
in
gr
oups
at
deterministic
times
T
;
T
;
T
;
:
:
:
.
Ther
e
is
a
waiting
time
c
ost
c
p
er
customer
p
er
unit
of
time,
and
a
disp
atch
c
ost
K
.

What
is
the
me
an
total
c
ost
(customer
waiting+disp
atch
c
ost)
p
er
unit
of
time
during
the
rst
cycle?

What
value
of
T
minimizes
the
me
an
c
ost
p
er
unit
of
time?
Problem
.
Find
the
me
an
E
N
t
,
varianc
e
V
ar
(N
t
)
and
the
c
ovarianc
e
cov
(N
t
;
N
s
).
Problem
.
L
et
X
(t)
=
( )
N
t
.
Find
the
me
an
E
X
t
,
varianc
e
V
ar
(X
t
)
and
the
c
ovarianc
e
cov
(X
t
;
X
s
).
The
rate

in
the
P
oisson
pro
cess
has
probabilistic
in
terpretation:

=
lim
h!0
Pr(N
t+h
 N
t
=
)
h
(:)
In
man
y
applications
w
e
wish
to
consider
rates
(t)
that
v
ary
with
time.
The
cor-
resp
onding
pro
cess
is
just
a
time-c
hange
of
the
P
oisson
pro
cess
X
(t)
=
N
(t)
,
where
(t)
=
R
t
0
(s)ds.

..
POISSON
PR
OCESS
0
..
The
la
w
of
rare
ev
en
ts
Let
N
(I
)
denote
the
n
um
b
er
of
ev
en
ts
that
o
ccur
in
in
terv
al
I
.
W
e
mak
e
the
follo
wing
p
ostulates.
.
If
in
terv
als
I

;
:
:
:
I
r
are
disjoin
t,
then
random
v
ariables
fN
(I
j
)g
are
indep
enden
t.
.
F
or
an
y
t
and
h
>
0
the
probabilit
y
distribution
of
N
((t;
t
+
h])
do
es
not
dep
end
on
t.
.
Pr
(N
(I
h
))
h
!
0
as
h
!
0
.
Pr
(N
(I
h
)=)
h
!

as
h
!
0
Theorem
..
N
t
=
N
((0;
t])
is
the
Poisson
pr
o
c
ess
of
intensity
.
Example
.
A
c
ar
eless
pr
o
gr
ammer
assigns
memory
lo
c
ations
to
the
variables
in
his
pr
o
gr
am

at
r
andom
.
Supp
ose
that
ther
e
ar
e
M
!

lo
c
ations
and
N
=
M
variables.
L
et
X
i
b
e
the
numb
er
of
variables
assigne
d
to
e
ach
lo
c
ation.
If
e
ach
lo
c
ation
is
e
qual
ly
likely
to
b
e
chosen,
show
that

Pr(X
i
=
k
)
!
e
 

k
=k
!
as
N
!


X
i
and
X
j
ar
e
indep
endent
in
the
limit
for
i
=
j
.
In
the
limit,
what
fr
action
of
memory
lo
c
ations
has
two
or
mor
e
variables
assigne
d?
Example
.
While
testing
a
pr
o
gr
am,
the
numb
er
of
bugs
disc
over
d
in
the
pr
o
gr
am
fol
lows
the
Poisson
pr
o
c
ess
with
intensity

=

err
ors
p
er
hour.
T
ester's
anc
e
enters
the
test
ar
e
a
and
agr
e
es
to
wait
for
the
tester
to
nd
just
one
mor
e
bug.
How
long
wil
l
she
wait
on
aver
age:

minutes,
or

minutes?
..
Comp
ound
P
oisson
pro
cess
The
P
oisson
pro
cess
N
t
coun
ts
the
n
um
b
er
of
ev
en
ts.
If
eac
h
ev
en
t
results
in
a
random
(and
indep
enden
t)
outcome

j
,
then
the
total
is
the
comp
ound
P
oisson
pro
cess
Z
(t)
=
P
N
t
j
=

j
.
The
momen
ts
and
also
the
momen
t
generating
function
of
Z
(t)
can
b
e
determined
through
conditioning.
Section
..
implies
that
if
E

=
,
V
ar
(
)
=


then
E
(Z
(t))
=
t,
V
ar
(Z
(t))
=
(

+


)t.

V
ariable
aliasing
is
the
mistak
e
of
assigning
the
same
lo
cation
to
t
w
o
or
more
v
ariables
in
a
program.

0
CHAPTER
.
CONTINUOUS
TIME
PR
OCESSES
Examples
.
R
isk
assessment:
Insurance
compan
y
has
M
customers.
Supp
ose
claims
arriv
e
at
an
insurance
compan
y
in
accordance
with
the
P
oisson
pro
cess
with
rate
M
.
Let
Y
k
b
e
the
magnitude
of
the
k
-th
claim.
The
net
prot
of
the
compan
y
is
then
Z
(t)
 M

t,
where

is
the
(xed
in
this
example)
premium.
.
A
sho
ck
mo
del:
Let
N
t
b
e
the
n
um
b
er
of
sho
c
ks
to
a
system
up
to
time
t
and
let

k
denote
the
damage
or
w
ear
incurred
b
y
the
k
-th
sho
c
k.
If
the
damage
accum
ulates
additiv
ely
,
then
Z
(t)
represen
ts
the
total
damage
sus-
tained
up
to
time
t.
Supp
ose
that
the
system
con
tin
ues
to
op
erate
un
til
this
total
damage
is
less
than
some
critical
v
alue
c
and
fails
otherwise.
Then
the
(random)
failure
time
T
satises
T
>
t
if
and
only
if
Z
(t)
<
c.
Therefore
Pr(T
>
t)
=
P

n=0
Pr(
P
n
k
=

k

z
jN
t
=
n)(t)
n
e
 t
=n!.
Th
us
E
T
=


P

n=0
Pr
(
P
n
j
=

j

c).
In
particular
if

k
are
exp
onen
tial
E
T
=
+c

.
.
Con
tin
uous
time
Mark
o
v
pro
cesses
Giv
en
a
discrete-time
Mark
o
v
c
hain,
there
are
man
y
w
a
ys
of
\running
it"
in
con
tin
uous
time.
One
particular
metho
d
is
to
mak
e
the
mo
v
es
at
random
momen
ts
of
time.
If
these
instances
are
exp
onen
tial,
then
the
resulting
con
tin
uous-time
pro
cess
is
Mark
o
v,
to
o.
This
is
the
so-called
emb
e
dde
d
Markov
chain.
Non-pathological
con
tin
uous
time
Mark
o
v
pro
cesses
with
coun
table
state
space
ha
v
e
emb
e
dde
d
Markov
chain
represen
tation.
In
suc
h
represen
tation
w
e
run
a
con
tin
uous
time
clo
c
k
based
on
the
indep
enden
t
exp
onen
tial
random
v
ariables.
Once
the
time
comes,
w
e
select
the
next
p
osition
according
to
the
transition
probabilities
of
a
discrete-time
Mark
o
v
c
hain.
The
theory
of
con
tin
uous
time
Mark
o
v
c
hains
(that
is
|
pro
cesses
with
coun
table
state
space)
is
similar
to
discrete
time
theory
.
The
linear
rst-order
dierence
equations
for
probabilities
are
replaced
b
y
the
systems
of
rst-order
dieren
tial
equations.
Example
.
Supp
ose
X
n
is
a
two-state
Markov
chain
with
the
fol
lowing
tr
ansitions:
0
!

with
pr
ob
ability
a,

!
0
with
pr
ob
ability
b.
F
r
om
Se
ction
.
we
know
that
P
(X
k
=
)
!
a=(a
+
b)
as
k
!
.
L
et
T
k
b
e
i.
i.
d.
exp
onential
r.
v.
and
let
Y
(t)
=
X
k
when
T

+
:
:
:
+
T
k
<
t
<
T

+
:
:
:
+
T
k
+
.
F
unction
p(t)
=
Pr(Y
(t)
=
)
satises
dier
ential
e
quation:
p
0
(t)
=
 p(t)
+
bp(t)
+
a(
 p(t).
Inde
e
d,
Pr(Y
(t
+
h)
=
)

Pr
(Y
(t)
=
;
T
>
h)
+
Pr(Y
(t)
=
0;
T
<
h).
Ther
efor
e
p(t)
=
a=(a
+
b)
+
b=(a
+
b)
exp
( (
 b
+
a)t).
Here
is
a
sligh
tly
dieren
t
metho
d
to
run
the
con
tin
uous
time
nite-v
alued
Mark
o
v
c
hain.
Pic
k
the
initial
v
alue
according
to
prescrib
ed
distribution.
Then
select
an
exp
onen
tial
random
v
ariable
for
eac
h
of
the
p
ossible
transitions.
Eac
h
of
these
can
ha
v
e
dieren
t
parameter

k
.
Then
select
the
smallest,
T
=
min
T
j
.
It
can
b
e
sho
wn
that
T
is
exp
onen
tial
with
parameter
P

j
and
that
Pr
(T
=
T
j
)
=

j
P
k

k
.

..
CONTINUOUS
TIME
MARK
O
V
PR
OCESSES
0
..
Examples
of
con
tin
uous
time
Mark
o
v
pro
cesses
Despite
man
y
similarities,
con
tin
uous
mo
dels
dier
from
discrete
ones
in
their
predictions.
Gro
wth
mo
del
In
Section
..
w
e
considered
discrete-time
gro
wth
mo
del
whic
h
assumed
that
bacteria
divide
at
xed
time
in
terv
als.
This
assumption
is
w
ell
kno
wn
not
to
b
e
satised
|
mitosis
is
a
pro
cess
that
consists
of
sev
eral
stages
of
v
arious
lengths,
of
whic
h
the
longest
ma
y
p
erhaps
b
e
considered
to
ha
v
e
exp
onen
tial
densit
y
.
In
this
section
w
e
shall
assume
that
a
colon
y
of
bacteria
consists
of
X
(t)
cells
whic
h
divide
at
exp
onen
tial
momen
ts
of
time,
or
die.
W
e
assume
individual
cells
m
ultiply
at
a
rate
a
and
die
at
a
rate
b.
One
w
a
y
to
in
terpret
these
n
um
b
ers
is
to
assume
that
there
are
t
w
o
comp
eting
eects:
extinction
or
division.
When
there
are
k
suc
h
cells,
the
p
opulation
gro
ws
one
cell
at
a
time,
rate
of
gro
wth
is
ak
,
rate
of
death
is
k
b.
W
e
assume
a
>
b.
Let
p
k
(t)
=
Pr(X
(t)
=
k
).
Then
p
0
k
(t)
=
 (a
+
b)k
p
k
+
(k
+
)bp
k
+
+
a(k
 )p
k
 
.
P
opulation
a
v
erage
m(t)
=
P
k
k
p
k
(t)
satises
m
0
(t)
=
(a
 b)m(t),
th
us
m(t)
=
e
(a b)t
.
Problem
.
Supp
ose
X
t
is
a
Markov
pr
o
c
ess
whose
birth
r
ate
is
an
+

and
de
ath
r
ate
is
bn
with
b
>
a.
This
describ
es
a
p
opulation
of
sp
e
cies
that
die
out
in
a
given
habitat,
but
have
a
c
onstant
r
ate
of
\invasion".
One
would
exp
e
ct
that
such
c
omp
eting
ee
cts
wil
l
r
esult
in
some
sort
of
e
quilibrium.
Find
the
aver
age
p
opulation
size
as
t
!
.
Example
.
Supp
ose
X
t
=
( )
N
t
,
wher
e
N
t
is
the
Poisson
Pr
o
c
ess.
Is
X
t
Markov?
Exercise
.
Customers
arrive
at
a
bur
ger
outlet
at
a
r
ate
,
and
after
exp
onential
servic
e
time
with
me
an
=
le
ave.

What
is
the
aver
age
numb
er
m(t)
of
customers
inside
the
building
t
minutes
after
op
ening?

On
your
next
visit
to
a
bur
ger
outlet,
estimate
al
l
thr
e
e
aver
ages.
Exercise
.
Customers
arrive
at
a
bur
ger
outlet
at
a
r
ate
,
and
after
exp
onential
servic
e
time
with
p
ar
ameter

le
ave.
If
ther
e
se
c
ond
c
ashier
is
op
ene
d,
the
servic
e
time
wil
l
b
e
r
e
duc
e
d
twic
e
on
aver
age,
but
the
c
ost
of
hiring
the
se
c
ond
c
ashier
is
$
p
er
hour.
A
customer
pur
chases
of
aver
age
$,
with
the
pr
ot
of
$
over
the
c
osts.
If
k
customers
ar
e
waiting
in
the
line,
the
next
p
erson
driving
by
wil
l
stop
with
pr
ob
ability

 k
.

What
r
ate

(if
any)
wil
l
justify
hiring
the
se
c
ond
c
ashier?

What
is
the
aver
age
numb
er
m(t)
of
customers
inside
the
building
t
minutes
after
op
ening?

0
CHAPTER
.
CONTINUOUS
TIME
PR
OCESSES
.
Gaussian
pro
cesses
Con
tin
uous-time
Mark
o
v
pro
cesses
with
uncoun
table
state
space
require
more
adv
anced
mathematical
to
ols.
Only
Gaussian
case
can
b
e
briey
men
tioned
here.
Denition
..
A
sto
chastic
pr
o
c
ess
fX
t
g
0t<
is
Gaussian,
if
the
n-dimensional
r.
v.
(X
t

;
:
:
:
;
X
t
n
)
has
multivariate
normal
distribution
for
al
l
n


and
al
l
t

;
:
:
:
;
t
n

[0;
).
.
The
Wiener
pro
cess
The
simplest
w
a
y
to
dene
the
Wiener
pro
cess
is
to
list
its
prop
erties
as
follo
ws.
Denition
..
The
Wiener
pr
o
c
ess
fW
t
g
is
a
Gaussian
pr
o
c
ess
with
c
ontinuous
tr
a-
je
ctories
such
that
W
0
=
0;
(.)
E
W
t
=
0
for
al
l
t

0;
(.)
E
W
t
W
s
=
min
ft;
sg
for
al
l
t;
s

0:
(.)
A
sto
c
hastic
pro
cess
fX
t
g
t[0;]
has
con
tin
uous
tra
jectories
if
it
is
dened
b
y
a
C
[0;
]-
v
alued
random
v
ector,
or
if
all
of
its
paths
are
con
tin
uous

.
F
or
innite
time
in
terv
al
t

[0;
),
a
sto
c
hastic
pro
cess
has
con
tin
uous
tra
jectories
if
its
restriction
to
t

[0;
N
]
has
con
tin
uous
tra
jectories
for
all
N

I
N.
Conditions
(.){(.)
imply
that
the
Wiener
pro
cess
has
indep
enden
t
incremen
ts,
ie.
W
0
;
W
t
 W
0
;
W
t+s
 W
t
;
:
:
:
are
indep
enden
t.
Series
expansions
for
the
Wiener
pro
cess
are
a
v
ailable
in
the
literature.
One
w
a
y
to
obtain
these
is
from
F
ourier
expansion
for
the
co
v
ariance
function.
Problem
.	
L
et
u(x;
t)
=
E
f
(W
t
+
x),
wher
e
f
is
a
smo
oth
function.
Show
that
u
satises
the
p
ar
ab
olic
e
quation
@
u
@
t
=


@

u
@
x

.
Problem
.0
What
p
artial
dier
ential
e
quation
is
solve
d
by
u(x;
t)
=
E
f
(aW
t
+
x
+
bt)
when
a;
b
ar
e
non-zer
o
c
onstants?
Scaled
Wiener
pro
cess
is
a
go
o
d
mo
del
of
diusion.
Use
the
t
w
o-dimensional
Wiener
pro
cess
to
mo
del
a
t
w
o-dimensional
diusion.
The
t
w
o-dimensional
Wiener
pro
cess
is
obtained
from
t
w
o
indep
enden
t
one-dimensional
comp
onen
ts.
The
diusion
co
ecien
t
a

has
units
[length

/time]
and
is
implemen
ted
b
y
scaling
the
Wiener
pro
cess:
aW
t
has
diusion
co
ecien
t
a

.
Exercise
.
A
n
eye-irritant
p
ol
lutant
is
emitte
d
fr
om
a
factory
chimney
lo
c
ate
d
at
x
=
0;
y
=
0
on
the
xy
plane,
and
the
wind
blows
left-to-right
with
velo
city
v
(y
)
=
y
at
height
y
.
A
t
a
distanc
e
L
=
00
down-wind,
ther
e
is
a
r
esidential
building
of
height
.
Which
o
or
of
the
building
is
p
ol
lute
d
the
most?
Is
ther
e
a
signic
ant
dier
enc
e
in
p
ol
lution
level
b
etwe
en
the
o
ors?
(Assume
that
the
distanc
es
ar
e
in
units
such
that
the
diusion
c
o
ecient
is
.)

This
is
a
v
ery
imprecise
statemen
t!

Chapter

Time
Series
This
c
hapter
con
tains
additional
topics
on
discrete
time
pro
cesses.
W
e
b
egin
with
sev
eral
examples
of
p
ossible
sim
ulations.
The
resulting
random
curv
es
are
v
ery
dieren
t,
but
they
ha
v
e
the
same
\mean-square"
b
eha
vior.
Example
.
Supp
ose
we
wish
to
pick
a
curve
at
r
andom.
In
other
wor
ds,
we
ne
e
d
a
r
andom
function
X
(t)
of
inte
ger
p
ar
ameter
t.
Her
e
is
one
way
to
do
it:
take
X
(t)
=
P
k

k
a
k
cos
(k
t),
wher
e

k
ar
e
i.
i.
d.
normal
N
(0;
).
T
o
ensur
e
the
series
is
c
onver
gent
we
ne
e
d
P
k
a

k
<
;
thus
we
c
an
assume
a
k
=
R

0
g
(
)
cos
(k

)
d
.
The
the
ory
of
F
ourier
series
tel
ls
us
that
if
R
g

(
)
<
,
then
the
c
o
ecients
a
k
ar
e
squar
e-summable.
Example
.
Supp
ose
we
wish
to
pick
a
curve
at
r
andom.
Her
e
is
one
way
to
do
it:
take
X
(t)
=
P
c
j
cos
(
t
+

j
),
wher
e

j
ar
e
indep
endent
and
uniformly
distribute
d
on
the
interval
(0;

).
A
gain
sele
ct
c
k
=
R

0
g
(
)
cos(k

)
d
.
Exercise
.
Write
simulations
of
the
curves
as
describ
e
d.
Pick
as
g
a
trigonometric
p
olynomial,
say
g
(
)
=

+
cos(
)
cos
(
).
.
Second
order
stationary
pro
cesses
Stationary
pro
cesses
are
those
that
their
probabilistic
c
haracteristics
(distributions,
con-
ditional
distributions,
momen
ts,
co
v
ariances)
do
not
c
hange
with
time.
Example
.
Supp
ose
X
n
is
a
Markov
chain,
and
Pr
(X
0
=
j
)
=

(j
),
wher
e

j
is
its
invariant
distribution.
Then
(X
0
;
X
k
)

=
(X
n
;
X
n+k
).
F
or
second-order
stationary
pro
cesses
only
means,
v
ariances,
and
co
v
ariances
do
not
c
hange
with
time.
That
is,
m(t)
=
E
X
(t)
=
const;
cov
(X
(t);
X
(s))
=
K
(t
 s).
The
second-order
theory
of
pro
cesses
is
a
v
ery
coarse
theory
.
Nev
ertheless,
it
do
es
solv
e
the
b
est
linear
prediction
problem.
Prop
osition
..
In
e
ach
of
the
intr
o
ductory
Examples,
E
X
(t)
=
0
and
cov
(X
(t);
X
(s))
=
K
(t
 s),
wher
e
K
(t)
=
R

0
cos
(
t)f
(
)
d
.
0	

0
CHAPTER
.
TIME
SERIES
..
P
ositiv
e
denite
functions
The
co
v
ariance
K
(t)
of
the
w
eakly
stationary
pro
cess
is
a
p
ositive
denite
function.
That
is,
P
c
i
c
j
K
(t
i
 t
j
)
=
E
j
P
j
c
j
X
(t
j
)j


0
for
all
c
j

C
C;
t
j

I
R.
In
addition,
K
(t)
=
K
( t).
Theorem
..
Given
a
p
ositive
denite
even
function
K
:
Z
Z
!
I
R
,
ther
e
is
a
(0;

)-
value
d
r
andom
variable

such
that
for
al
l
inte
ger
t
K
(t)
=
K
(0)E
cos
(t):
(:)
Prop
osition
..
Supp
ose

fr
om
The
or
em
..
has
density
f
(
).
L
et
g
(
)
=
q
f
(
)
and
dene
X
t
as
in
Example
..
Then
X
t
is
Gaussian,
me
an
zer
o,
with
c
ovarianc
e
(.).
.
T
ra
jectory
a
v
erages
When
a
time
series
X
t
is
observ
ed,

n
(X

+
:
:
:
+
X
n
)
is
the
\tra
jectory
a
v
erage".
It
is
in
teresting
ho
w
do
es
this
compare
to
the
\probabilistic"
a
v
erage
E
X
.
The
follo
wing
theorem
follo
ws
immediately
from
the
pro
of
of
Theorem
...
The
assumption
holds
true
in
particular
when
X
n
has
sp
ectral
densit
y
.
Theorem
..
Supp
ose
X
n
is
we
akly
stationary.
If
cov
(X
0
;
X
n
)
!
0
as
n
!

then

n
(X

+
:
:
:
+
X
n
)
!
E
X
in
L

-norm.
Pro
of.
Compute
the
v
ariance,
and
use
the
Cesaro
summabilit
y
result
(Theorem
B..).

The
co
v
ariance
argumen
t
can
b
e
rewritten
in
sp
ectral
notation.
Supp
ose
E
X
0
X
k
=
(
)
 
R

 
e
ik
s
f
(s)
ds.
Then
E
(X

+
:
:
:
+
X
n
)

=
(
)
 
R

 
j
P
n
k
=
e
ik
s
j

f
(s)
ds,
so
V
ar
(

n
(X

+
:
:
:
+
X
n
))
=
(
)
 

n

R

 
sin

(


ns)
sin

(


s)
f
(s)
ds
!
0
as
n
!
.
.
The
general
prediction
problem
The
basic
problem
in
ltering
and
prediction
is
as
follo
ws.
Giv
en
v
ariables
X

;
:
:
:
;
X
n
,
nd
the
estimator
of
the
v
alue
of
Y
with
smallest
quadratic
error.
Case
n
=

is
presen
ted
is
Sections
..
and
.
for
the
linear
and
non-linear
case.
In
the
linear
prediction
problem
w
e
deal
with
linear
estimators
a
0
+
P
j
a
j
X
j
.
The
quadratic
error
in
v
olv
es
v
ariances,
co
v
ariances
and
a
v
erages
only
.
Th
us
it
is
appropriate
to
handle
this
in
through
the
second
order
pro
cesses,
and
the
solution
should
dep
end
on
the
densit
y
of

-
the
so
called
sp
ectral
densit
y
only
.
The
general
Hilb
ert
space
theory
tells
us
that
the
b
est
linear
prediction
of
X
t+
based
on
the
past
is
P
t
j
=0
a
j
X
j
with
co
ecien
ts
a
j
suc
h
that
E
X
j
(X
t+
 P
t
j
=0
a
j
X
j
)
=
0
for
all
0l
eq
j

t.
This
is
a
system
of
t
+

linear
equations
for
t
+

unkno
wn
co
ecien
ts.
Gramm-
Sc
hmidt
orthogonalization
allo
ws
to
replace
X
j
b
y
orthonormal

j
.
Optimal
prediction
uses
P
t
j
=0

j

j
with

j
=
E
X
0

t j
.

..
A
UTOREGRESSIVE
PR
OCESSES

.
Autoregressiv
e
pro
cesses
Supp
ose

k
are
i.
i.
d.
A
sto
c
hastic
pro
cess
X
t
is
an
autoregressiv
e
pro
cess,
if
it
satises
a
linear
dierence
equation
X
t+
=
d
X
j
=0
a
j
X
t j
+

t+
(:)
with
random
co
ecien
ts

t+
.
Example
.
A
utor
e
gr
essive
pr
o
c
ess
X
t+
=
aX
t
+

t+
is
Markov.
F
or
jaj
<

it
has
a
stationary
distribution.
What
is
it?
What
happ
ens
for
jaj
>
?
Example
.
A
moving
aver
age
X
t+
=

d
P
d
j
=

t j
is
an
autor
e
gr
essive
pr
o
c
ess.
What
is
the
c
orr
esp
onding
dier
enc
e
e
quation
(.)?
F
or
autoregressiv
e
pro
cess
the
optimal
one-step
prediction
of
X
t+
is
d
X
j
=0
a
j
a
0
X
t j
:
(:)
The
sp
ectral
theory
asserts
that
this
is
b
est
linear
prediction.
Ho
w
ev
er
if

j
are
i.
i.
d.,
then
this
is
actually
optimal
non-linear
prediction
as
w
ell.
More
general
autoregressiv
e
sequences
are
dened
as
solutions
of
d
X
j
=0
a
j
X
t j
=
X
i
b
i

t i
(:)
These
generalize
sim
ultaneously
autoregressiv
e
and
mo
ving
a
v
erage
pro
cesses.


CHAPTER
.
TIME
SERIES

Chapter

Additional
topics
.
A
simple
probabilistic
mo
deling
in
Genetics
First
w
e
describ
e
the
p
opulation
at
a
single
instance.
W
e
consider
the
mo
del
in
whic
h
eac
h
hereditary
c
haracter
is
carried
b
y
a
pair
of
genes.
F
or
simplicit
y
,
w
e
assume
only
one
(pair)
of
a
gene,
and
only
one
hereditary
c
haracter
corresp
onding
to
one
lo
cus.
There
are
sev
eral
p
ossible
alleles
(categories)
in
a
lo
cus
{
w
e
assume
there
are
only
t
w
o,
denoted
b
y
a;
A,
of
whic
h
A
is
dominan
t.
Eac
h
individual
th
us
is
describ
ed
b
y
one
of
the
pairs
AA;
Aa;
aa,
the
so
called
phenotyp
e
.
Ho
w
ev
er,
a
is
obstructed
from
the
view
b
y
A,
th
us
for
an
outside
observ
er
individuals
AA
and
Aa
are
indistinguishable.
A
statistical
study
of
suc
h
a
p
opulation
can
only
yield
the
prop
ortion
P
A
of
individuals
with
"A-feature",
and
not
the
actual
probabilities
of
the
three
p
ossible
phenot
yp
e.
No
w
w
e
turn
to
the
mo
delling
of
the
generation
c
hange.
Under
simple
assumptions
w
e
shall
b
e
able
to
nd
out
what
are
the
frequencies
of
phenot
yp
e
and
genot
yp
es.
Usually
this
information
is
not
directly
a
v
ailable.
W
e
assume
that
the
next
generation
o
ccurs
b
y
r
andom
mating.
Let
p
AA
(0);
p
Aa
(0);
p
aa
(0)
b
e
the
actual
(and
as
y
et
unkno
wn)
probabili-
ties
of
the
phenot
yp
e.
Under
random
mating
with
indep
enden
t
selection
of
paren
ts,
the
probabilit
y
that
an
ospring
has
phenot
yp
e
AA
is
p
AA
()
=
(p
AA
(0)
+


p
Aa
(0))

.
Denoting
b
y
p
A
(t)
=
p
AA
(t)
+


p
Aa
(t)
w
e
get
the
Hardy-W
ein
b
erg
equilibrium:
after
one
generation
the
prop
ortions
p
A
(t)
of
genot
yp
es
stabilize
and
the
phenot
yp
e
frequencies
b
ecome
P
AA
=
p

A
(.)
P
Aa
=
p
a
p
A
(.)
P
aa
=
p

a
(.)
(.)
This
determines
the
actual
prop
ortions
of
the
phenot
yp
e
from
the
prop
ortion
of
ob-
serv
ed
A-carriers
and
a-carriers.
Problem
.
Show
that
P
Aa
=
(
p
P
aa
 P
aa
).
The
next
question
is
to
study
the
eects
of
the
selection,
where,
sa
y
phenot
yp
e
aa
has
dieren
t
c
hance
of
surviv
al.
This
leads
to
total
probabilit
y
form
ula
and
Mark
o
v
c
hains.



CHAPTER
.
ADDITIONAL
TOPICS
.
Application:
v
erifying
matrix
m
ultiplication
Supp
ose
one
has
an
algorithm
to
m
ultiply
large
matrices
and
w
e
w
an
t
to
c
hec
k
if
the
output
is
correct.
A
p
ossible
metho
d
is
to
pic
k
the
v
ector
X
of
0;

and
c
hec
k
if
AB
X
=
C
X.
This
is
the
so
called
F
r
eivalds
te
chnique
Theorem
..
If
A;
B
;
C
ar
e
n

n
matric
es
such
that
AB
=
C
then
Pr(AB
X
=
C
X)



.
Pro
of.
F
or
a
non-zero
v
w
e
ha
v
e
Pr
(jv

Xj
>
0)
<



.
Exc
hangeabilit
y
Denition
..
A
se
quenc
e
(X
k
)
of
r
andom
variables
is
exc
hangeable,
if
the
joint
dis-
tribution
of
X

()
;
X

()
;
:
:
:
;
X

(n)
is
the
same
as
the
joint
distribution
of
X

;
X

;
:
:
:
;
X
n
for
al
l
n


and
for
al
l
p
ermutations

of
f;
;
:
:
:
;
ng.
The
follo
wing
b
eautiful
theorem
due
to
B.
de
Finetti
p
oin
ts
out
the
role
of
exc
hange-
abilit
y
as
a
substitute
for
indep
endence.
Theorem
..
Supp
ose
that
X

;
X

;
:
:
:
is
an
innite
exchange
able
se
quenc
e.
Then
ther
e
exist
a

-eld
N
such
that
X

;
X

;
:
:
:
ar
e
N
-c
onditional
ly
i.
i.
d.,
that
is
P
(X

<
a

;
X

<
a

;
:
:
:
;
X
n
<
a
n
jN
)
=
P
(X

<
a

jN
)P
(X

<
a

jN
)
:
:
:
P
(X

<
a
n
jN
)
for
al
l
a

;
:
:
:
;
a
n

I
R
and
al
l
n

.
W
e
will
use
the
follo
wing
(w
eak)
v
ersion
of
the
martingale

con
v
ergence
theorem.
Theorem
..
Supp
ose
F
n
is
a
de
cr
e
asing
family
of

-elds,
ie.
F
n+

F
n
for
al
l
n

.
If
X
is
inte
gr
able,
then
E
fX
jF
n
g
!
E
fX
jF
g
in
L

-norm,
wher
e
F
is
the
interse
ction
of
al
l
F
n
.
Pro
of.
Supp
ose
rst
that
X
is
square
in
tegrable.
Subtracting
m
=
E
X
if
necessary
,
w
e
can
reduce
the
con
v
ergence
question
to
the
cen
tered
case
E
X
=
0.
Denote
X
n
=
E
fX
jF
n
g.
Since
F
n+

F
n
,
b
y
Jensen's
inequalit
y
E
X

n

0
is
a
decreasing
non-negativ
e
sequence.
In
particular,
E
X

n
con
v
erges.
Let
m
<
n
b
e
xed.
Then
E
(X
n
 X
m
)

=
E
X

n
+
E
X

m
 E
X
n
X
m
.
Since
F
n

F
m
,
b
y
Theorem
A..
w
e
ha
v
e
E
X
n
X
m
=
E
E
fX
n
X
m
jF
n
g
=
E
X
n
E
fX
m
jF
n
g

A
martingale
with
resp
ect
to
a
family
of
increasing

-elds
F
n
is
and
in
tegrable
sequence
X
n
suc
h
that
E
(X
n+
jF
n
)
=
X
n
.
The
sequence
X
n
=
E
(X
jF
n
)
is
a
martingale.
The
sequence
in
the
theorem
is
of
the
same
form,
except
that
the

-elds
are
decreasing
rather
than
increasing.

..
EX
CHANGEABILITY

=
E
X
n
E
fE
fX
jF
m
gjF
n
g
=
E
X
n
E
fX
jF
n
g
=
E
X

n
:
Therefore
E
(X
n
 X
m
)

=
E
X

m
 E
X

n
.
Since
E
X

n
con
v
erges,
X
n
satises
the
Cauc
h
y
condition
for
con
v
ergence
in
L

norm.
This
sho
ws
that
for
square
in
tegrable
X
,
sequence
fX
n
g
con
v
erges
in
L

.
If
X
is
not
square
in
tegrable,
then
for
ev
ery

>
0
there
is
a
square
in
tegrable
Y
suc
h
that
E
jX
 Y
j
<
.
By
Jensen's
inequalit
y
E
fX
jF
n
g
and
E
fY
jF
n
g
dier
b
y
at
most

in
L

-norm;
this
holds
uniformly
in
n.
Since
b
y
the
rst
part
of
the
pro
of
E
fY
jF
n
g
is
con
v
ergen
t,
it
satises
the
Cauc
h
y
condition
in
L

and
hence
in
L

.
Therefore
for
eac
h

>
0
w
e
can
nd
N
suc
h
that
for
all
n;
m
>
N
w
e
ha
v
e
E
fjE
fX
jF
n
g
 E
fX
jF
m
gjg
<
.
This
sho
ws
that
E
fX
jF
n
g
satises
the
Cauc
h
y
condition
and
hence
con
v
erges
in
L

.
The
fact
that
the
limit
is
X

=
E
fX
jF
g
can
b
e
seen
as
follo
ws.
Clearly
X

is
F
n
-measurable
for
all
n,
ie.
it
is
F
-measurable.
F
or
A

F
(hence
also
in
F
n
),
w
e
ha
v
e
E
X
I
A
=
E
X
n
I
A
.
Since
jE
X
n
I
A
 E
X

I
A
j

E
jX
n
 X

jI
A

E
jX
n
 X

j
!
0,
therefore
E
X
n
I
A
!
E
X

I
A
.
This
sho
ws
that
E
X
I
A
=
E
X

I
A
and
b
y
denition,
X

=
E
fX
jF
g.

Pro
of
of
Theorem
...
Let
N
b
e
the
tail

-eld,
ie.
N
=

\
k
=

(X
k
;
X
k
+
;
:
:
:
)
and
put
N
k
=

(X
k
;
X
k
+
;
:
:
:
).
Fix
b
ounded
measurable
functions
f
;
g
;
h
and
denote
F
n
=
f
(X

;
:
:
:
;
X
n
);
G
n;m
=
g
(X
n+
;
:
:
:
;
X
m+n
);
H
n;m;N
=
h(X
m+n+N
+
;
X
m+n+N
+
;
:
:
:
);
where
n;
m;
N

.
Exc
hangeabilit
y
implies
that
E
F
n
G
n;m
H
n;m;N
=
E
F
n
G
n+r
;m
H
n;m;N
for
all
r

N
.
Since
H
n;m;N
is
an
arbitrary
b
ounded
N
m+n+N
+
-measurable
function,
this
implies
E
fF
n
G
n;m
jN
m+n+N
+
g
=
E
fF
n
G
n+r
;m
jN
m+n+N
+
g:
P
assing
to
the
limit
as
N
!
,
see
Theorem
..,
this
giv
es
E
fF
n
G
n;m
jN
g
=
E
fF
n
G
n+r
;m
jN
g:
Therefore
E
fF
n
G
n;m
jN
g
=
E
fG
n+r
;m
E
fF
n
jN
n+r
+
gjN
g:
Since
E
fF
n
jN
n+r
+
g
con
v
erges
in
L

to
E
fF
n
jN
g
as
r
!
,
and
since
g
is
b
ounded,
E
fG
n+r
;m
E
fF
n
jN
n+r
+
gjN
g
is
arbitrarily
close
(in
the
L

norm)
to
E
fG
n+r
;m
E
fF
n
jN
gjN
g
=
E
fF
n
jN
gE
fG
n+r
;m
jN
g


CHAPTER
.
ADDITIONAL
TOPICS
as
r
!
.
By
exc
hangeabilit
y
E
fG
n+r
;m
jN
g
=
E
fG
n;m
jN
g
almost
surely
,
whic
h
pro
v
es
that
E
fF
n
G
n;m
jN
g
=
E
fF
n
jN
gE
fG
n;m
jN
g:
Since
f
;
g
are
arbitrary
,
this
pro
v
es
N
-conditional
indep
endence
of
the
sequence.
Us-
ing
the
exc
hangeabilit
y
of
the
sequence
once
again,
one
can
see
that
random
v
ariables
X

;
X

;
:
:
:
ha
v
e
the
same
N
-conditional
distribution
and
th
us
the
theorem
is
pro
v
ed.

.
Distances
b
et
w
een
strings
A
string
is
a
sequence
of
letters,
or
sym
b
ols
from
the
nite
alphab
et.
F
or
the
purp
ose
of
computer
mo
delling,
w
e
can
assume
that
a
string
is
a
sequence
of
natural
n
um
b
ers
f;
:
:
:
dg,
parameter
d
b
eing
the
size
of
the
alphab
et.
Three
simple
examples
of
strings
are
the
w
ords,
sen
tences
in,
sa
y
,
English,
and
DNA
molecules.
Here
d
=

(for
lo
w
er-case
w
ords),
d
=
	
for
sen
tences,
and
d
=

for
the
DNA
(there
are
only
four
proteins,
but
extra
sym
b
ols
are
used
to
mark
v
arious
undecided
cases).
The
question
of
comparing
t
w
o
strings
for
similarities
arises
in
molecular
biology
and
in
designing
a
sp
ell-c
hec
k
er,
or
a
sp
eec
h
recognition
system.
Accordingly
,
one
w
ould
lik
e
to
sa
y
whic
h
strings
are
similar,
and
ho
w
lik
ely
it
is
that
they
are
similar
due
to
c
hance
only
.
Additional
complications
arise
from
the
fact
that
t
w
o
strings
compared
do
not
necessarily
ha
v
e
the
same
length.
A
simple
w
a
y
to
compare
t
w
o
strings
is
to
measure
the
n
um
b
er
of
sym
b
ols
that
don't
matc
h
(the
hamming
distance).
F
or
instance
abbacd
and
babacd
w
ould
then
ha
v
e
distance
.
But
abbacd
and
bbacda
w
ould
ha
v
e
distance
,
ev
en
though
they
dier
just
b
y
one
transp
osition.
A
less
ob
vious
w
a
y
to
compare
t
w
o
strings
is
to
measure
the
e
dit
distanc
e:
ho
w
man
y
e
diting
op
erations
are
needed
to
transform
one
of
the
strings
in
to
another.
Usually
the
editing
op
erations
are:

insert
a
sym
b
ol

delete
a
sym
b
ol

replace
a
sym
b
ol

transp
ose
t
w
o
consecutiv
e
sym
b
ols
These
are
suitable
for
sp
ell-c
hec
k
ers,
where
it
is
kno
wn
that
ab
out
0%
of
t
yping
errors
are
of
the
ab
o
v
e
form,
th
us
most
of
mist
yp
ed
w
ords
ha
v
e
edit-distance

from
the
original.
Accordingly
,
the
edit
distance
is
set
to
0,
if
the
w
ords
are
iden
tical,

if
they
dier
b
y
a
single
error
of
one
of
the
listed
t
yp
es,

if
there
w
ere
t
w
o
suc
h
errors,
etc.
F
ormally
,
it
is
dened
as
the
smallest
n
um
b
er
of
elemen
tary
\TE"
transformations
required
to
transform
one
of
the
w
ords
in
to
another.
The
metho
d
of
computation
is
based
on
recurrence.
It
is
easy
to
see
that
a
n
um
b
er
of
transformations
b
et
w
een
and
empt
y
w
ord
and
another
one
is
exactly
the
length

of
the

Only
deletions
are
required.

..
DIST
ANCES
BETWEEN
STRINGS

w
ord.
If
the
t
w
o
w
ords
U;
W
are
formed
from
shorter
ones
U
0;
W
0
b
y
adding
letters
at
the
end,
then
the
distance
D
ist(U;
W
)
is
the
smallest
of
the
n
um
b
ers

D
ist(U
0;
W
)
+

(delete)

D
ist(U;
W
0)
+

(add)

D
ist(U
0;
W
0)
+
0
or

if
same
letter
is
added
dieren
t
letter
(sw
ap)

D
ist(U
0
0
;
V
0
0
)
+
,
if
the
last
t
w
o
letters
are
iden
tical
(transp
ose),
where
U
0
0
;
V
0
0
are
U
0;
W
0
with
last
letter
remo
v
ed.
Here
is
a
complete
VB-listing:
Function
Dist
(U$,
V$)
As
Integer
'Returns
the
edit
distance
'(number
of
elementary
changes:
replacement,
deletion,
insertion,transposition
)
'that
are
required
to
transform
word
U$
into
V$
'
'Declare
auxiliary
variables
Dim
m
As
Integer,
n
As
Integer,
j
As
Integer,
i
As
Integer
Dim
x
As
Integer,
y
As
Integer,
z
As
Integer,
A$,
B$
If
Len(U$)
<
Len(V$)
Then
A$
=
U$:
B$
=
V$:
Else
A$
=
V$:
B$
=
U$
m
=
Len(A$)
n
=
Len(B$)
'Declare
matrix
of
distances
between
substrings
of
i,j
characters
ReDim
D(m,
n)
As
Integer
'Assign
boundary
values:
distances
from
empty
string
For
i
=
0
To
m:
D(i,
0)
=
i:
Next
i
For
j
=

To
n:
D(0,
j)
=
j:
Next
j
'
'Main
recurrence:
Compute
next
distance
D(i,j)
from
previously
found
values
For
i
=

To
m
For
j
=

To
n
x
=
D(i
-
,
j)
+

'delete
character
y
=
D(i,
j
-
)
+

'inserte
character
x
=
Intmin(x,
y)
'choose
better
(Integer
Minimum)
y
=
D(i
-
,
j
-
)
-
(Mid$(A$,
i,
)
<>
Mid$(B$,
j,
))
'swap
characters
i
x
=
Intmin(x,
y)
'
choose
better
z
=
0
If
i
>

And
j
>

Then
'If
Mid$(A$,
i,
)
<>
Mid$(B$,
j,
)
Then
z
=
(Mid$(A$,
i,
)
=
Mid$(B$,
j
-
,
))
*
(Mid$(A$,
i
-
,
)
=
Mid$(
'End
If
y
=
(
+
D(i
-
,
j
-
))
*
z
+
x
*
(
-
z)
x
=
Intmin(x,
y)
End
If
D(i,
j)
=
x


CHAPTER
.
ADDITIONAL
TOPICS
Next
j
Next
i
Dist
=
D(m,
n)'current
value
End
Function
The
main
problem
with
edit
distance
to
analyze
DNA
molecules
is
pro
cessing
time.
Exercise
.
Write
a
pr
o
gr
am
c
omputing
the
e
dit
distanc
e
b
etwe
en
strings,
and
another
one,
which
do
es
the
e
diting
by
simulation.
(T
ry
the
r
andomization
b
ase
d
on
r
andom
numb
er
of
e
dit
op
er
ations
fr
om
the
curr
ently
b
est
c
andidate)
.
A
mo
del
of
cell
gro
wth
A
cell
in
its
gro
wth
go
es
through
sev
eral
phases,
whic
h
ha
v
e
dieren
t
probabilistic
c
har-
acteristics.
In
a
simple
mo
del,
the
cell
doubles
after
a
random
time,
whic
h
is
the
sum
of
the
exp
onen
tial
and
deterministic
p
ortion.
The
a
v
erage
of
the
exp
onen
tial
phase
can
b
e
assumed
to
dep
end
on
the
external
circumstances.
Questions
of
in
terest
Ho
w
do
es
the
gro
wth
of
cells
aect
other
cells?
Ho
w
to
con
trol
mixed
p
opulations
of
cells
to
sta
y
within
prescrib
ed
limits?
.
Shannon's
En
trop
y
Let
X

;
:
:
:
X
n
b
e
indep
enden
t
iden
tically
distributed
(i.
i.
d.)
discrete
r.
v
with
k
v
alues,
sa
y
fv

;
:
:
:
;
v
k
g.
Put
X
=
(X

;
:
:
:
;
X
k
).
F
or
a
xed
v
ector
y
w
e
ha
v
e
th
us
the
join
t
probabilit
y
mass
function
f
(x)
=
Pr(X
=
x).
The
a
v
erage
information
H
(X)
con
tained
in
X
is
dened
as
H
(X)
=
 E
log
f
(X)
=
 X
x
f
(x)
log
f
(x)
(:)
Problem
.
Pr
ove
Gibbs'
ine
quality
P
j
p
j
log
p
j

P
p
j
log
q
j
for
al
l
q
j
>
0;
P
q
j
=
.
Notice
that
H
(X)

0
and
H
(X)

log
k
..
Optimal
searc
h
Co
ding
Human's
co
de
Problem
.
Supp
ose
you
have

identic
al
in
app
e
ar
anc
e
c
oins,
exc
ept
that
one
of
them
has
a
dier
ent
weight.
Find
the
optimal

weighting
str
ate
gy
to
identify
the
o
dd
c
oin
by
using
a
sc
ale.

That
is,
nd
the
strategy
that
costs
the
least
if
y
ou
ha
v
e
to
pa
y
for
eac
h
use
of
the
scale.

..
APPLICA
TION:
SPREAD
OF
EPIDEMIC
	
.
Application:
spread
of
epidemic
Mo
deling
of
the
spread
of
disease
is
complicated
due
to
m
ultiple
factors
that
inuence
its
dev
elopmen
t.
The
birth-and-death
pro
cess
do
es
not
seem
to
b
e
a
go
o
d
mo
del
for
the
spread
of
an
epidemic
in
a
nite
p
opulation,
since
when
a
large
prop
ortion
of
the
p
opulation
has
b
een
infected,
w
e
cannot
supp
ose
that
the
rate
of
infections
is
indep
enden
t
of
past
history
.

0
CHAPTER
.
ADDITIONAL
TOPICS

App
endix
A
Theoretical
complemen
ts
A.
L
p
-spaces
Inequalities
related
to
exp
ected
v
alues
are
b
est
stated
in
geometric
language
of
norms
and
normed
spaces.
W
e
sa
y
that
X

L
p
,
if
X
is
p-in
tegrable,
i.e.
E
jX
j
p
<
.
In
particular,
X
is
squar
e
inte
gr
able
if
E
X

<
.
The
L
p
norm
is
kX
k
p
=
(
p
q
E
jX
j
p
if
p

;
ess
sup
jX
j
if
p
=
:
Notice
that
kX
 E
X
k

is
just
the
standard
deviation.
W
e
sa
y
that
X
n
con
v
erges
to
X
in
L
p
,
if
kX
n
 X
k
p
!
0
as
n
!
.
If
X
n
con
v
erges
to
X
in
L

,
w
e
shall
also
use
the
phrase
se
quenc
e
X
n
c
onver
ges
to
X
in
me
an-squar
e.
An
example
of
the
latter
is
Theorem
...
Sev
eral
useful
inequalities
are
collected
in
the
follo
wing.
Theorem
A..
(i)
for


p

q


we
have
Minkowski's
ine
quality
kX
k
p

kX
k
q
:
(A:)
(ii)
for
=p
+
=q
=
,
p


we
have
H
older's
ine
quality
E
X
Y

kX
k
p
kY
k
q
:
(A:)
(iii)
for


p


we
have
triangle
ine
quality
kX
+
Y
k
p

kX
k
p
+
kY
k
p
:
(A:)
Sp
ecial
case
p
=
q
=

of
H
older's
inequalit
y
(A.)
reads
E
X
Y

p
E
X

E
Y

.
It
is
frequen
tly
used
and
is
kno
wn
as
the
Cauchy-Schwarz
ine
quality.
F
or
the
pro
of
of
Theorem
A..
w
e
need
the
follo
wing
elemen
tary
inequalit
y
.
Lemma
A..
F
or
a;
b
>
0;

<
p
<

and
=p
+
=q
=

we
have
ab

a
p
=p
+
b
q
=q
:
(A:)



APPENDIX
A.
THEORETICAL
COMPLEMENTS
Pro
of.
F
unction
t
!
t
p
=p
+
t
 q
=q
has
the
deriv
ativ
e
t
p 
 t
 q
 
.
The
deriv
ativ
e
is
p
ositiv
e
for
t
>

and
negativ
e
for
0
<
t
<
.
Hence
the
maxim
um
v
alue
of
the
function
for
t
>
0
is
attained
at
t
=
,
giving
t
p
=p
+
t
 q
=q

:
Substituting
t
=
a
=q
b
 =p
w
e
get
(A.).

Pro
of
of
Theorem
A..
(ii).
If
either
kX
k
p
=
0
or
kY
k
q
=
0,
then
X
Y
=
0
a.
s.
Therefore
w
e
consider
only
the
case
kX
k
p
kY
k
q
>
0
and
after
rescaling
w
e
assume
kX
k
p
=
kY
k
q
=
.
F
urthermore,
the
case
p
=
;
q
=

is
trivial
as
jX
Y
j

jX
jkY
k

.
F
or

<
p
<

b
y
(A.)
w
e
ha
v
e
jX
Y
j

jX
j
p
=p
+
jY
j
q
=q
:
In
tegrating
this
inequalit
y
w
e
get
jE
X
Y
j

E
jX
Y
j


=
kX
k
p
kY
k
q
.

Pro
of
of
Theorem
A..
(i).
F
or
p
=

this
is
just
Jensen's
inequalit
y;
for
a
more
general
v
ersion
see
Theorem
A...
F
or

<
p
<

b
y
H
older's
inequalit
y
applied
to
the
pro
duct
of

and
jX
j
p
w
e
ha
v
e
kX
k
p
p
=
E
fjX
j
p

g

(E
jX
j
q
)
p=q
(E

r
)
=r
=
kX
k
p
q
;
where
r
is
computed
from
the
equation
=r
+
p=q
=
.
(This
pro
of
w
orks
also
for
p
=

with
ob
vious
c
hanges
in
the
write-up.)

Pro
of
of
Theorem
A..
(iii).
The
inequalit
y
is
trivial
if
p
=

or
if
kX
+
Y
k
p
=
0.
In
the
remaining
cases
kX
+
Y
k
p
p

E
f(jX
j
+
jY
j)jX
+
Y
j
p 
g
=
E
fjX
jjX
+
Y
j
p 
g
+
E
fjY
jjX
+
Y
j
p 
g:
By
H
older's
inequalit
y
kX
+
Y
k
p
p

kX
k
p
kX
+
Y
k
p=q
p
+
kY
k
p
kX
+
Y
k
p=q
p
:
Since
p=q
=
p
 ,
dividing
b
oth
sides
b
y
kX
+
Y
k
p=q
p
w
e
get
the
conclusion.

A.
Prop
erties
of
conditional
exp
ectations
In
more
adv
anced
courses
conditional
exp
ectation
E
(X
jY
)
is
dened
as
a
random
v
ariable
(Y
)
that
satises
E
X
f
(Y
)
=
E
(Y
)f
(Y
)
for
all
b
ounded
measurable
(or
just
con
tin
u-
ous)
functions
f
.
The
next
theorem
lists
useful
prop
erties
of
conditional
exp
ectations.
Theorem
A..
(i)
If
Y
=
f
(Z
)
is
such
that
X
and
X
Y
ar
e
inte
gr
able,
then
E
fX
Y
jZ
g
=
Y
E
fX
jZ
g;

A..
PR
OPER
TIES
OF
CONDITIONAL
EXPECT
A
TIONS

(ii)
E
X
E
X
;Y
=
E
X
;
(iii)
If
(X
;
Y
)
and
Z
ar
e
indep
endent,
then
E
fX
jY
;
Z
g
=
E
fX
jY
g;
(iv)
If
g
(x)
is
a
c
onvex
function
and
E
jg
(X
)j
<
,
then
g
(E
fX
jY
g)

E
fg
(X
)jY
g;
(v)
If
Y
is
non-r
andom,
then
E
fX
jY
g
=
E
X
;
(vi)
If
X
;
Y
ar
e
inte
gr
able
and
a;
b

I
R
then
E
faX
+
bY
jZ
g
=
aE
fX
jZ
g
+
bE
fY
jZ
g;
(vii)
If
X
and
Y
ar
e
indep
endent,
then
E
fX
jY
g
=
E
X
.
Remark

Inequalit
y
(iv)
is
kno
wn
as
Jensen's
inequalit
y
and
this
is
ho
w
w
e
shall
refer
to
it.
The
abstract
pro
of
uses
the
follo
wing

.
Lemma
A..
If
Y

and
Y

ar
e
F
-me
asur
able
and
R
A
Y

dP

R
A
Y

dP
for
al
l
A

F
,
then
Y


Y

almost
sur
ely.
If
R
A
Y

dP
=
R
A
Y

dP
for
al
l
A

F
,
then
Y

=
Y

.
Pro
of.
Let
A

=
fY

>
Y

+
g

F
.
Since
R
A

Y

dP

R
A

Y

dP
+
P
(A

),
th
us
P
(A

)
>
0
is
imp
ossible.
Ev
en
t
fY

>
Y

g
is
the
coun
table
union
of
the
ev
en
ts
A

(with

rational);
th
us
it
has
probabilit
y
0
and
Y


Y

with
probabilit
y
one.
The
second
part
follo
ws
from
the
rst
b
y
symmetry
.

Pro
of
of
Theorem
A...
(i)
This
is
v
eried
rst
for
Y
=
I
B
(the
indicator
function
of
an
ev
en
t
B

F
).
Let
Y

=
E
fX
Y
jF
g;
Y

=
Y
E
fX
jF
g.
F
rom
the
denition
one
can
easily
see
that
b
oth
R
A
Y

dP
and
R
A
Y

dP
are
equal
to
R
A\B
X
dP
.
Therefore
Y

=
Y

b
y
the
Lemma
A...
F
or
the
general
case,
appro
ximate
Y
b
y
simple
random
v
ariables
and
use
(vi).
(ii)
This
follo
ws
from
Lemma
A..:
random
v
ariables
Y

=
E
fX
jF
g,
Y

=
E
fX
jG
g
are
G
-measurable
and
for
A
in
G
b
oth
R
A
Y

dP
and
R
A
Y

dP
are
equal
to
R
A
X
dP
.
(iii)
Let
Y

=
E
fX
jN
W
F
g;
Y

=
E
fX
jF
g.
W
e
c
hec
k
rst
that
Z
A
Y

dP
=
Z
A
Y

dP
for
all
A
=
B
\
C
,
where
B

N
and
C

F
.
This
holds
true,
as
b
oth
sides
of
the
equation
are
equal
to
P
(B
)
R
C
X
dP
.
Once
equalit
y
R
A
Y

dP
=
R
A
Y

dP
is
established
for
the
generators
of
the

-eld,
it
holds
true
for
the
whole

-eld
N
W
F
;
this
is
standard
measure
theory

.
(iv)
Here
w
e
need
the
rst
part
of
Lemma
A...
W
e
also
need
to
kno
w
that
eac
h
con
v
ex
function
g
(x)
can
b
e
written
as
the
suprem
um
of
a
family
of
ane
functions
f
a;b
(x)
=
ax
+
b.
Let
Y

=
E
fg
(X
)jF
g;
Y

=
f
a;b
(E
fX
jF
g);
A

F
.
By
(vi)
w
e
ha
v
e
Z
A
Y

dP
=
Z
A
g
(X
)
dP

f
a;b
(
Z
A
X
)
dP
=
f
a;b
(
Z
A
E
fX
jF
g)
dP
=
Z
A
Y

dP
:
Hence
f
a;b
(E
fX
jF
g)

E
fg
(X
)jF
g;
taking
the
suprem
um
(o
v
er
suitable
a;
b)
ends
the
pro
of.
(v),
(vi),
(vii)
These
pro
ofs
are
left
as
exercises.


Readers
not
familiar
with
measure
theory
should
skip
the
pro
ofs.

See

 
Theorem
(Theorem
.)
P
.
Billingsley
,
Probabilit
y
and
measure,
Wiley
,
New
Y
ork
	.


APPENDIX
A.
THEORETICAL
COMPLEMENTS
Problem
A.
Pr
ove
p
art
(v)
of
The
or
em
A...
Problem
A.
Pr
ove
p
art
(vi)
of
The
or
em
A...
Problem
A.
Pr
ove
p
art
(vii)
of
The
or
em
A...
Problem
A.
Pr
ove
the
fol
lowing
c
onditional
version
of
Chebyshev's
ine
quality:
if
E
jX
j
<
,
then
P
(jX
j
>
tjY
)

E
fjX
j
jY
g=t
almost
sur
ely.
Problem
A.
Show
that
if
(U;
V
;
X
)
ar
e
such
that
in
distribution
(U;
X
)

=
(V
;
X
)
then
E
fU
jX
g
=
E
fV
jX
g
almost
sur
ely.
Problem
A.
Show
that
if
X
;
Y
ar
e
inte
gr
able
non-de
gener
ate
r
andom
variables,
such
that
E
fX
jY
g
=
aY
;
E
fY
jX
g
=
bX
;
then
jabj

.
Problem
A.
Show
that
if
X
;
Y
ar
e
inte
gr
able
such
that
E
fX
jY
g
=
Y
and
E
fY
jX
g
=
X
,
then
X
=
Y
a.
s.

App
endix
B
Math
bac
kground
The
follo
wing
sections
are
short
reference
on
material
from
general
math
(calculus,
linear
algebra,
etc).
B.
In
teractiv
e
links
The
follo
wing
links
are
op
erational
as
of
Marc
h
,
		.
Please
note
that
these
ma
y
c
hange
at
an
y
time.

real
analysis

is
a
v
ailable
online.
B.
Elemen
tary
com
binatorics
The
art
of
coun
ting
is
called
c
ombinatorics.
Here
is
a
short
listing
of
the
form
ulas.
All
are
the
consequences
of
the
pro
duct
rule
of
coun
ting.
P
erm
utations:
P
erm
utation
is
an
arrangemen
t
(ordering)
of
k
out
of
n
distinct
ob
jects
without
rep
etitions.
The
n
um
b
er
of
p
erm
utations
is
n!
(n k
)!
.
In
particular,
the
n
um
b
er
of
w
a
ys
to
order
n
ob
jects
is
n!
Com
binations:
Com
binations
are
k
-elemen
t
subsets
of
n
distinct
elemen
ts.
The
n
um
b
er
of
com
binations
is
(
n
k
).
V
ariations:
V
ariations
are
arrangemen
ts
of
k
out
of
n
distinct
ob
jects
with
rep
etitions
allo
w
ed.
The
n
um
b
er
of
v
ariations
is
n
k
B.
Limits
The
follo
wing
limit
can
b
e
computed
b
y
L'Hospital
rule.
lim
n!
(
+
a=n)
n
=
e
a
(B:)
The
Cesaro
summabilit
y
form
ula
is
Theorem
B..
If
a
n
!
a
then

n
(a

+
:
:
:
+
a
n
)
!
a.

h
ttp://www.sh
u.edu/pro
jects/reals/reals.h
tml



APPENDIX
B.
MA
TH
BA
CK
GR
OUND
B.
P
o
w
er
series
expansions
The
follo
wing
p
o
w
er
series
expansions
are
of
in
terest
in
this
course.
e
x
=

X
k
=0
x
k
=k
!
(B.)


 x
=

X
k
=0
x
k
(B.)
They
giv
e
immediately
the
expansions
ln

+
x
=

X
k
=0
( )
k
x
k
+
k
+

(B.)
e
x

=
=

X
k
=0
(B.)
Z
x
0
e
 t

=
dt
=

X
k
=0
(B.)
(B.)
In
particular
for
x
>
0
w
e
ha
v
e
ln

+
x
<
x
and
ln

+
x
>
x
 x

=.
B.
Multiv
ariate
in
tegration
Supp
ose
x
=
x(u;
v
);
y
=
y
(u;
v
).
The
c
hange
of
v
ariables
form
ula
in
m
ultiv
ariate
case
is
Z
Z
U
f
(x;
y
)
dxdy
=
Z
Z
V
f
(x(u;
v
);
y
(u;
v
))jJ
(u;
v
)j
dudv
;
(B:)
where
the
Jacobian
J
is
giv
en
b
y
J
=
det

@
x
@
u
@
x
@
v
@
y
@
u
@
y
@
v

(B:	)
B.
Dieren
tial
equations
The
solution
of
the
linear
dieren
tial
equation
y
0
+
ay
=
g
(x)
with
y
(0)
=
y
0
is
giv
en
b
y
y
(x)
=
y
0
e
 ax
R
x
0
e
at
g
(t)=;
dt.
Second
order
linear
equation
y
00
+
ay
0
+
by
=
g
(x)
is
often
solv
ed
b
y
the
metho
d
of
v
arying
a
constan
t.
The
rst
step
is
to
solv
e
the
homogenous
equation
y
00
+
ay
0
+
by
=
0
rst
using
y
=
e
r
x
.
The
general
solution
of
homogenous
equation
is
y
=
C

e
r

x
+
C

e
r

x
,
or
y
=
C

e
r
x
+
C

xe
r
x
if
the
c
haracteristic
equation
has
double
ro
ot.
B.
Linear
algebra
Denition
B..
A
sc
alar
pr
o
duct
of
ve
ctors
in
V
is
a
biline
ar,
p
ositive
denite,
non-de
gener
ate
mapping
hji
:
V

V
!
I
R
.
Denition
B..
V
e
ctors
x;
y
ar
e
ortho
gonal
with
r
esp
e
ct
to
sc
alar
pr
o
duct
hji,
if
hxjy
i
=
0.
The
length
of
a
v
ector
is
kxk
=
p
hxjxi
.
Orthogonal
v
ectors
of
unit
length
are
called
orthonormal..
If
e
j
are
the
orthonormal
v
ectors
and
x
is
in
their
linear
span,
then
the
co
ecien
ts
in
the
expansion
x
=
P
j
a
j
e
j
are
giv
en
b
y
a
j
=
hxje
j
i.
Example
B.
L
et
V
b
e
the
ve
ctor
sp
ac
e
of
al
l
c
ontinuous
functions
on
the
interval
[ 
;

].
In
the
sc
alar
pr
o
duct
hf
jg
i
=
R

 
f
(t)g
(t)
dt
the
functions
fsin
k
g
k
I
N
;
fcos
k
tg
k
I
N
;

ar
e
ortho
gonal.

B..
F
OURIER
SERIES

B.
F
ourier
series
The
F
ourier
series
for
a
function
f
(x)
is
the
expansion
f
(x)
=
P
n
a
n
sin

nx
+
b
n
cos

nx.
Ev
ery
p
erio
dic
con
tin
uous
function
f
can
b
e
expanded
in
the
F
ourier
series.
The
co
ecien
ts
are
b
0
=


Z

 
f
(x)
dx
(B.0)
b
n
=


Z

 
f
(x)
cos
n
x
dx
for
n
=
0
(B.)
a
n
=


Z

 
f
(x)
sin
n
x
dx
(B.)
(B.)
Example
B.
Exp
and
jxj
into
the
F
ourier
series,
and
gr
aph
the
c
orr
esp
onding
p
artial
sums.
B.	
P
o
w
ers
of
matrices
The
Ca
yley
-
Hamilton
theorem
asserts
that
eac
h
d

d
matrix
A
satises
the
p
olynomial
equation
Q
d
(A)
=
0,
where
Q
d
(x)
=
det(A
 xI
)
is
the
c
haracteristic
p
olynomial
of
degree
d.
This
implies
that
A
n
=
a
0
(n)I
+
a

(n)A
+
:
:
:
+
a
d 
(n)A
d 
,
where
x
n
=
D
(x)Q(x)
+
a
0
(n)
+
a

(n)x
+
:
:
:
+
a
d 
(n)x
d 
.
If
A
has
n
distinct
eigen
v
alues

j
,
the
co
ecien
ts
a
j
(n)
can
b
e
found
b
y
solving
the
system
of
equations

n
j
=
a
0
(n)
+
a

(n)
j
+
:
:
:
+
a
d 
(n)
d 
j
.
A
quic
k
metho
d
that
nds
a
c
haracteristic
p
olynomial
due
to
K...
is
to
solv
e
the
system
of
linear
equations
for
the
co
ecien
ts:
pic
k
a
v
ector
X
at
random
and
solv
e

for
a
0
;
:
:
:
;
a
d 
the
equations
a
0
X
+
a

AX
+
:
:
:
+
a
d 
A
d 
X
=
0.
If
the
resulting
matrix
is
singular,
re-sample
X
un
til
a
non-singular
matrix
is
found.

Use
Gaussian
elimination.


APPENDIX
B.
MA
TH
BA
CK
GR
OUND

App
endix
C
Numerical
Metho
ds
Calculators
and
more
sophisticated
math-geared
soft
w
are
ha
v
e
ecien
t
n
umerical
metho
ds
built-in.
Here
are
short
prescriptions
that
ma
y
b
e
used
b
y
general
programmer.
A
more
complete
source
is
e.g.Numerical
Recip
es:
The
Art
of
Scien
tic
Computing
series.
C.
Numerical
in
tegration
T
o
appro
ximate
the
in
tegral
R
b
a
f
(x)
dx
in
calculus
w
e
use
left
and
righ
t
sums:
L
n
=
b a
n
P
n 
k
=0
f
(a+
b a
n
k
),
R
n
=
b a
n
P
n
k
=
f
(a
+
b a
n
k
)
The
more
exact
tr
ap
ezoidal
rule
uses
R
b
a
f
(x)
dx

S
n
=


(L
n
+
R
n
).
Still
more
p
o
w
erful
and
easy
to
program
in
tegration
metho
d
is
the
Simpson
rule:
R
b
a
f
(x)
dx



S
n
 

S
n
.
The
Simpson
rule
is
exact
for
cubic
p
olynomials.
T
ypical
w
a
y
to
program
it
is
to
call
the
subroutine
p
erforming
trap
ezoid
metho
d
in
tegration
t
wice.
Man
y
calculators
ha
v
e
Simpson
rule
build
in.
Before
y
ou
use
it,
b
e
sure
to
c
hec
k
if
it
is
reliable
enough.
A
simple
test
that
catc
hes
some
p
o
orly
written
routines
is
R
00
0
e
 x

dx

p

=
.
C.
Solving
equations
The
fast
and
p
o
w
erful
bise
ction
metho
d
requires
correct
end-p
oin
ts,
and
nds
one
solution
only
.
But
it
has
virtually
no
assumptions,
except
the
con
tin
uit
y:
If
f
(x)
is
con
tin
uous
and
f
(a)
<
0;
f
(b)
>
0
then
there
is
a
<
x
<
b
suc
h
that
f
(x)
=
0
and
the
in
terv
al
length
can
b
e
halv
ed
b
y
computing
f
(
a+b

).
Three
diculties
arise
in
real
applications.

Ho
w
to
nd
a
\correct"
initial
pair

Ho
w
to
nd
more
than
one
solution

Ho
w
to
solv
e
systems
of
equations
in
sev
eral
unkno
wns
The
second
p
oin
t
has
satisfactory
answ
er
for
p
olynomial
equations.
The
third
p
oin
t
can
b
e
tac
kled
through
searc
h
for
minim
um.
Namely
,
if
equations
are
f
(x;
y
)
=
0;
g
(x;
y
)
=
0
then
w
e
need
to
nd
minima
of
f

(x;
y
)
+
g

(x;
y
).
C.
Searc
hing
for
minim
um
The
analog
of
the
bisection
metho
d
for
nding
minima
is
to
b
egin
with
three
p
oin
ts
a
<
b
<
c
suc
h
that
f
(b)
is
the
smallest
of
the
three.
The
next
triple
is
pro
duced
b
y
partitioning
the
larger
of
the
t
w
o
	

0
APPENDIX
C.
NUMERICAL
METHODS
segmen
ts
in
prop
ortion

 p



0:	,
and
comparing
the
resulting
four
v
alues
to
pic
k
the
narro
w
er
triplet.

This
is
golden
section.
The
explanation
wh
y
it
o
ccurs
here
can
b
e
found
in
n
umerical
metho
ds
b
o
oks.

App
endix
D
Programming
Help
Programming
help
is
group
ed
b
y
the
t
yp
e
of
soft
w
are.
Curren
tly
a
v
ailable
(as
of
Marc
h
,
		)
are
preliminary
v
ersions
of:

Help
for
TI
Programmable
calculator

D.
In
tro
ducing
BASIC
An
y
DOS-running
PC
comes
also
with
BASIC.
Y
ou
can
start
it
with
the
command
QBASIC
from
the
DOS
command
line,
or
from
Windo
ws
File
Manager
(clic
k
on
an
en
try
QBASIC.EXE,
usually
lo
cated
in).
If
y
ou
are
a
dev
oted
Windo
ws
user,
y
ou
ma
y
install
an
Ic
on
in
Pr
o
gr
am
Manager
to
run
BASIC
with
a
clic
k
of
the
mouse.
Correctly
written
programs
halt
at
the
end.
But
not
all
programs
do
that,
so
an
\emergency
exit"
is
build
in.
T
o
stop
the
execution
of
an
y
program,
press
sim
ultaneously
Ctrl
+
Break
.
ho
w
to
use
this
c
hapter
This
text
w
as
written
for
a
complete
no
vice
to
BASIC.
If
y
ou
t
the
description,
read
the
pages
b
elo
w,
and
t
yp
e
in
eac
h
of
the
sample
programs.
Once
y
ou
ha
v
e
them
in
the
QBASIC,
run
them
to
see
the
eects
of
v
arious
instrunctions.
There
is
no
p
ossibilit
y
that
b
y
running
these
programs
y
ou
will
do
an
y
harm
to
y
our
equipmen
t.
Exp
erimen
t,
and
if
something
go
es
realy
wrong,
y
ou
can
alw
a
ys
turn
OFF
or
restart
the
computer.
D.
Computing
The
mathematical
con
v
en
tions
in
QBASIC
are
^(/)
for

=
,
SQR()
for
p

,
LO
G()
for
the
natural
logarithm
ln
,
etc.
With
these,
one
can
use
QBASIC
as
a
calculator.
F
or
instance
the
instruction
PRINT
LOG(+^(/))
will
prin
t
the
decimal
v
alue
of
the
expression
ln(
+
p
).
This
probably
is
the
simplest
program
to
b
egin
with.
It
is
so
short
that
there
is
no
p
oin
t
in
sa
ving
it.
The
real
p
o
w
er
comes
from
rep
etitiv
e
op
erations
explained
in
Section
D..

h
ttp://math.uc.edu/
~
brycw/preprin
t/ti



APPENDIX
D.
PR
OGRAMMING
HELP
D.
The
structure
of
a
program
BASIC
programs
are
actually
text
les.
The
instructions
are
read
consecutiv
ely
b
y
the
BASIC
in
terpreter.
The
QBASIC
in
terpreter
that
comes
with
DOS
is
sucien
t
for
our
purp
oses.
Sample
programs
b
elo
w
in
tro
duce
certain
more
exotic
build-in
functions.
Example
D.
The
fol
lowing
simple
pr
o
gr
am
prints
curr
ent
date
&
time
PRINT
"HELLO"
PRINT
"Today
is
";
DATE$
PRINT
TIME$
Besides
the
actual
co
de,
programs
should
con
tain
commen
ts
with
explanations
of
instructions
and
their
purp
ose.
Commen
ts
and
explanations
in
the
program
can
b
e
hidden
from
the
pro
cessor
through
REM
command.
Ev
erything
in
a
line
after
this
command
is
skipp
ed
(the
only
exception
b
eing
metacommands,
whic
h
w
e
don't
ha
v
e
to
concern
ourselv
es
with
here).
The
sample

programs,
use
a
shortcut
'
instead
of
the
full
command
REM.
This
is
faster
to
t
yp
e
and
equiv
alen
t
in
action.
Example
D.
Her
e
is
the
pr
evious
pr
o
gr
am
with
c
omments
'Prints
current
date
&
time
PRINT
"HELLO"
'greet
the
user
PRINT
"Today
is
";
DATE$
'print
date
PRINT
TIME$
'
print
time
could
have
printer
TIMER=#
seconds
since
	0
The
t
ypical
program
consists
of
the
main
mo
dule
with
fairly
general
instructions
that
call
subroutines
to
p
erform
sp
ecic
jobs,
and
a
n
um
b
er
of
subprograms
that
are
mark
ed
in
text
b
y
SUB
...
END
SUB,
and
are
displa
y
ed
separately
b
y
the
QBASIC
Editor.
Subprograms
mak
e
it
easier
to
write,
test,
and
main
tain
the
program.
Within
QBasic
the
SUBs
are
separated,
eac
h
in
eac
h
o
wn
text
windo
w,
but
the
nal
program
is
sa
v
ed
as
one
le
with
SUBs
follo
wing
the
main
mo
dule.
T
o
create
a
SUB,
c
ho
ose
New
SUB
from
the
Edit
men
u.
More
details
follo
w
in
Section
D.	.
Larger
pro
jects
often
use
sev
eral
mo
dules

that
are
compiled
and
run
together.
Example
D.
Her
e
is
the
r
evision
of
the
pr
evious
pr
o
gr
am
with
a
simple
but
useful
SUB.
'Prints
current
date
&
time
in
the
middle
of
the
screen
CLS
'clear
display
CenterPrint
"HELLO"
'greet
the
user
CenterPrint
"Today
is
"
&
DATE$
'print
date
-
string
has
to
be
concatenated
CenterPrint
TIME$
'
print
time
could
have
printer
TIMER=#
seconds
since
	0
SUB
CenterPrint
(Text$)
'****
print
text
Text$
centered
on
screen
l=-LEN(Text$)
/
if
l<0
then
l=0
'too
long
text
cannot
be
centered
print
TAB(l);
Text$
New
things:
CLS,
c
onc
atenation
of
strings,
c
al
ling
SUB,
scr
e
en
p
ositioning
by
T
AB,
LEN("hel
lo")
Ev
ery
subprogram
should
b
egin
with
a
(commen
ted)
short
description
of
its
purp
ose,
and
the
meaning
of
parameters.

The
actual
sample
programs
on
the
disk
also
con
tain
commen
ted
L
a
T
E
X
t
yp
esetting
instructions
at
the
b
eginning
and
at
the
end.
Their
sole
purp
ose
is
to
insert
the
listings
in
to
this
text.

F
or
example,
in
man
y
applied
math
programs
there
is
a
need
for
in
tegration
routines.
These
p
erhaps
w
ould
b
e
collected
in
a
separate
mo
dule
to
facilitate
rep
eated
usage.

D..
CONDITIONALS
AND
LOOPS

D.
Conditionals
and
lo
ops
Lo
ops
of
xed
size
are
b
est
handled
b
y
FOR
j=
to
0
STEP
.
...
NEXT
j.
STEP
is
optional,
and
if
none
is
giv
en,
then

is
used
b
y
default.
Example
D.
Pr
o
gr
am
FOR
j=
TO
00
S=S+j
NEXT
J
PRINT
S
c
omputes
and
prints
the
total
of
the
rst
00
inte
gers
(00).
Conditional
action
is
accomplished
b
y
IF
...
THEN
...
ELSE
...
END
IF
END
IF
is
required
only
when
m
ultiple
lines
of
instructions
are
to
b
e
executed.
Selection
from
sev
eral
cases
is
p
erhaps
easiest
through
SELECT
var
...
CASE
0
...
CASE
.
...
CASE
ELSE
END
SELECT
Conditional
lo
ops
(the
ones
that
last
indenitely
,
unless
sp
ecial
circumstance
is
encoun
tered)
can
b
e
programmed
through
WHILE
cond
...
WEND
or
through
DO
....
IF
cond
THEN
EXIT
DO
...
LOOP
Other
w
a
ys
of
breaking
out
of
DO
...
LOOP
are
a
v
ailable,
to
o.
Example
D.
The
fol
lowing
pr
o
gr
am
il
lustr
ates
sever
al
c
onditional
instructions.
It
nds
c
onse
cutive
max
=
0000
prime
numb
ers
lar
ger
than
n
=

n=
max
=
0000
nc
=
n
-

WHILE
k
<
max
DO
nc
=
nc
+

prime
=
-
'TRUE
FOR
j
=

TO
SQR(nc)
IF
(nc
MOD
j)
=
0
THEN
prime
=
0:
EXIT
FOR
NEXT
j
IF
prime
THEN
EXIT
DO
LOOP
k
=
k
+

Print
k;"-th
prime
is
";
nc
WEND
Exercise
D.
Write
the
pr
o
gr
am
solving
r
e
curr
enc
e
(.).


APPENDIX
D.
PR
OGRAMMING
HELP
D.
User
input
T
o
stop
the
execution
of
an
y
program,
press
sim
ultaneously
Ctrl
+
Break
.
T
o
ha
v
e
user
supply
a
v
alue
for
v
ariable
X
,
write
INPUT
"Real
number=";X
Note:
instruction
INPUT
stops
the
program.
The
user
has
to
hit
Enter
for
the
program
to
con
tin
ue.
T
o
scan
for
the
k
ey
pressed
b
y
the
user
without
stopping
the
program,
use
INKEY$
function
instead
of
INPUT.
This
allo
ws
the
user
to
con
trol
the
program
b
y
simple
men
u
functions.
F
or
example,
em
b
ed
the
follo
wing
lines
within
DO
...
LOOP
and
stop
the
program
b
y
pressing
KeyQ.
Key$=INKEY$
SELECT
CASE
Key$
CASE
"Q"
END
'stop
the
program
CASE
"q"
END
'stop
the
program
CASE
"?"
PRINT
"Did
you
ask
for
help?"
CASE
ELSE
BEEP
END
SELECT
A
simple
w
a
y
to
let
the
user
kno
w
that
something
w
en
t
wrong
is
to
BEEP.
Exercise
D.
Write
a
pr
o
gr
am
that
c
omplains
(b
e
eps)
when
user
pr
esses
any
key.
Exercise
D.
Write
a
pr
o
gr
am
that
wil
l
typ
e
the
text
pr
ovide
d
by
user
fr
om
the
keyb
o
ar
d
to
scr
e
en
in
upp
er-c
ase
r
e
gar
d
less
what
the
user
sele
cte
d.
(Hint:
F
unction
T$=UCASE$(T$)
c
onverts
to
upp
er-c
ase)
D.
More
ab
out
prin
ting
Instruction
PRINT
is
used
to
prin
t
to
the
screen,
and
in
a
sligh
tly
mo
died
v
ersion,
to
the
le
on
the
disk.
T
o
prin
t
to
the
prin
ter,
use
LPRINT
instead
of
PRINT.
In
the
latter
case,
to
force
the
page
out
of
the
prin
ter,
end
ev
ery
prin
ting
job
with
LPRINT
CHR$().
QBASIC
pro
vides
sophisticated
w
a
ys
of
con
trolling
the
text
output
b
y
format,
colors,
lo
cation
on
the
screen.
In
addition
it
do
es
ha
v
e
graphic
statemen
ts,
as
long
as
the
computer
has
graphics
card.
But
the
only
thing
needed
for
us
is
the
regular
PRINT
"New
value
is
X=";X
whic
h
outputs
the
string
(in
quotation
marks)
and
the
v
alue
New
value
is
X=.
Occasionally
w
e
ma
y
w
an
t
to
do
minimal
\format"
through
the
semi-colon,
or
TAB().
PRINT
".";
T
o
see
the
eect
of
the
semi-colon,
run
the
ab
o
v
e
statemen
t
in
a
lo
op

.
Then
delete
the
semicolon,
and
run
it
again.
F
or
professional
formatting
of
output,
lo
ok
up
the
instruction
PRINT
USING
"###.##"
in
an
y
BASIC
textb
o
ok
(y
our
public
library
is
a
go
o
d
source!).

FOR
j=
TO
000
PRINT
".";
NEXT
J

D..
ARRA
YS
AND
MA
TRICES

D.
Arra
ys
and
matrices
V
ectors
and
matrices
are
handled
as
arra
ys.
They
need
to
b
e
declared
to
reserv
e
ro
om
in
memory
.
This
is
accomplished
b
y
the
dimensioning
statemen
t
tt
DIM
Arra
yName
(Size,
Size
,
Size
).
F
or
instance
DIM
A(00)
sp
ecies
a
v
ector,
DIM
A(0,0)
denes
a
0

0
matrix.
Arra
ys
use
up
a
lot
of
memory
,
so
don't
declare
arra
ys
larger
than
what
y
ou
need,
and
pa
y
atten
tion
to
data
t
yp
e,
see
Section
D..
D.
Data
t
yp
es
QBASIC
supp
orts
man
y
built
in
data
t
yp
es.
Use
declaration
lik
e
DIM
n
AS
INTEGER,
or
app
end
the
name
b
y
the
corresp
onding
mark
er
n%.

In
teger
(%)

Long
(&)

Single
(!)
{
default

Double
(#)

String
($)
These
can
b
e
com
bined
in
to
more
complex
user
dened
data
structures
through
Type
declaration.
If
v
ariables
are
not
declared,
and
the
name
isn't
app
ended
with
the
sym
b
ol
indicating
t
yp
e,
the
v
ariables
are
treated
as
single
precision.
This
is
admissible

in
small
programs
where
sp
eed
and
memory
aren't
of
ma
jor
concern.
D.	
User
dened
FUNCTIONs
and
SUBs
T
o
create
a
SUB,
c
ho
ose
New
SUB
from
the
Edit
men
u.
T
o
create
a
FUNCTION,
c
ho
ose
New
FUNCTION
from
the
Edit
men
u.
User
dened
functions
are
listed
as
separate
windo
ws
within
QBASIC.
Select
View
to
switc
h
b
et
w
een
v
arious
functions.
Eac
h
user=dened
function
starts
with
BEGIN
FUNCTION
FunctName
(x,
y,z)
and
ends
with
END
FUNCTION.
The
co
de
b
et
w
een
these
t
w
o
lines
is
executed
whenev
er
the
function
is
en
v
ok
ed
from
main
program,
from
another
function,
or
SUB,
or
from
itself.
FunctName
is
a
name
for
y
our
function
(c
ho
ose
a
descriptiv
e
one).
Argumen
ts
(x,y,z)
are
the
v
ariables
passed
to
the
function.
Example
D.
The
fol
lowing
function
soves
the
line
ar
e
quation
ax
+
b
=
0
FUNCTION
Solution(a,
b)
Solution=-b/a
END
FUNCTION
Exercise
D.
Write
a
function
that
solves
quadr
atic
e
quation
ax

+
bx
+
c
=
0.
There
are
just
a
couple
things
to
remem
b
er
when
making
functions:

The
function
has
to
b
e
assigned
output
b
y
FunctionName=val
ue
.

V
ariables
used
within
function
are
dieren
t
from
those
in
the
rest
of
the
program,
except
those
passed
to
it
as
an
argumen
t.
Th
us
y
ou
can
use
the
same
index
j
for
sums
in
the
program,
and
in
the
function.
But
if
a
v
alue
of
a
parameter
passed
is
c
hanged
within
the
function,
it
is
c
hanged
throughout
the
program.

Use
DEFINT
I-L
to
o
v
er-ride
the
default
and
force
all
the
v
ariables
with
names
that
b
egin
with
I
though
L
to
b
e
of
Integer
t
yp
e.


APPENDIX
D.
PR
OGRAMMING
HELP

F
unction
can
do
more
than
just
return
the
\v
alue".
F
or
instance,
it
can
prin
t,
and
c
hange
v
alues
of
all/some/none
of
its
v
ariables,
and
return
v
alue.
Once
a
function
is
created,
y
ou
can
use
it
from
within
a
program
in
the
same
w
a
y
as
the
build
in
functions.
The
name
of
the
function
carries
its
v
alue,
eg
x=FunctName()+
Fu
nct
Na
me
(
).
Notice
that
the
function
can
also
c
hange
v
alues
of
the
v
ariables
in
its
argumen
ts,
and
p
erform
an
y
other
tasks
{
this
b
eha
vior
is
lik
e
that
of
an
y
program.
SUBs
act
lik
e
functions,
except
that
the
name
do
esn't
carry
\v
alue"
and
the
only
c
hange
(if
an
y)
is
to
the
v
ariables
passed.
T
o
in
v
ok
e
SUB
from
the
program,
use
CALL
SubName(Paramet
ers
).
This
metho
d
is
recommended
as
it
is
more
resistan
t
to
t
yp
ographical
errors.
Y
ou
can
pass
a
whole
arra
y
as
an
argumen
t
of
a
function.
The
arra
ys
are
recognized
b
y
the
paren-
theses:
FuncName(A())
is
a
function
of
arra
y
A().
Exercise
D.
Write
functions
SimulateBinomial
(p,n),
SimulateNormal(n
,si
gm
a),
SimulateExponent
ia
l(
m)
that
simulate
a
single
instanc
e
of
the
Binomial,
Normal,
Exp
onential
distribution
with
given
p
ar
ameters.
D.0
Graphics
Graphic
commands
are
a
v
ailable
only
on
computers
with
a
graphics
card
(all
PC's
that
run
Windo
ws
ha
v
e
a
graphics
card).
It
is
nice
to
b
e
able
to
mak
e
simple
graphs,
but
the
full
topic
is
b
ey
ond
the
scop
e
of
this
in
tro
duction.
Program
LIMTHS.BAS
dra
ws
lines
and
b
o
xes
of
v
arious
colors,
in
the
displa
y
rectangle
co
v
ering
a
p
ortion
of
the
screen.
If
y
ou
are
seriously
in
terested
in
graphing
the
results
of
y
our
computations,
and
prin
ting
the
outcomes,
y
ou
ma
y
w
an
t
to
switc
h
to
Visual
Basic
in
Windo
ws.
D.
Binary
op
erations
Long
in
tegers
can
b
e
used
to
represen
t
subsets
of
an
n-elemen
t
set,
for
n
up
to
n
=

(Wh
y?).
Single
n
um
b
er
k
corresp
onds
to
long
in
teger

k
,
the
set
fj;
k
g
is
represen
ted
b
y

j
+

k
,
etc.
If
S,
T
are
in
tegers
represen
ting
sets,
then
S
AND
T
represen
ts
the
in
tersection
of
sets,
S
OR
T
is
the
union,
and
S
AND
NOT
T
is
the
dierence
of
sets.
T
o
c
hec
k
if
j

S
,
v
erify
if
^j
AND
T
is
non-zero.
D.
File
Op
erations
Beginners
in
BASIC
need
no
le
op
erations
to
solv
e
the
exercises.
If
y
ou
w
an
t
to
sa
v
e
y
our
prin
tout
to
le,
prin
t
to
the
ASCI
I
le
sequen
tially
.
This
is
slo
w
er
and
less
v
ersatile
than
binary
,
but
easier
to
master.
The
syn
tax
is
quite
rigid.
The
follo
wing
example
con
tains
the
basic
idea:
OPEN
FileName
FOR
OUTPUT
as
#
PRINT
#,
"HELLO"
CLOSE
#
Y
ou
can
prin
t
to
le
text,
n
um
b
ers,
etc.
If
the
le
with
the
sam
name
as
output
le
already
exists,
it
is
replaced
b
y
the
new
one
(o
v
erwritten).
T
o
add
ro
ws
to
an
existing
le
without
lo
osing
its
con
ten
ts
use
OPEN
FileName
FOR
APPEND
as
#
Y
ou
can
read
input
from
programs
bac
k
in
to
the
program
b
y
the
corresp
onding
INPUT
statemen
ts.
Bew
are
that
the
le
has
to
ha
v
e
the
format
exp
ected
b
y
the
INPUT.
The
follo
wing
simple
program
reads
en
tries
from
the
le
FileName
and
prin
ts
it
on
to
the
screen,
one
b
y
one.

D..
GOOD
PR
OGRAMMING

OPEN
FileName
FOR
INPUT
as
#
WHILE
NOT
EOF()
INPUT
#,
H$
PRINT
H$
WEND
CLOSE
#
PRINT
H$
Change
INPUT
#,
H$
to
LINE
INPUT
#,
H$
to
get
full
text
rather
than
w
ords.
D.
Go
o
d
programming
Adhering
to
go
o
d
programming
principles
pa
ys
in
clarit
y
of
programs,
and
facilitates
debugging
(nding
errors).

Get
in
to
the
habit
of
structural
programming.
{
Be
a
w
are
of
the
distinction
b
et
w
een
main
program,
and
subprograms.
The
main
program
should
pla
y
dieren
t
role
-
it
should
direct
the
course
of
action,
not
do
the
actions.
Av
oid
form
ulas,
computations,
algorithms
in
the
main
part
of
the
program.
Ha
v
e
the
\sub
ordinate"
subprograms
do
the
tasks
{
Split
longer
subprograms
in
to
smaller
blo
c
ks
(also
subprograms,
or
mo
dules),
preferably
the
ones
y
ou
can
re-use.
As
a
rule
of
th
um
b
-
subprogram
listing
should
t
within
one
t
yp
ed
page
(0
lines).
{
Generalize!
If
y
ou
w
an
t
to
a
v
erage

n
um
b
ers,
y
ou
can
use
FUNCTION
Average(x,x,x)
.
But
y
ou
should
write
tt
FUNCTION
Av
erage
(X())
that
a
v
erages
as
man
y
n
um
b
ers
as
y
ou
ask.

Use
commen
ts!
W
rite
the
purp
ose
of
eac
h
subprogram
b
efore
writing
the
co
de.
T
est
the
op
eration
of
the
program
with
\empt
y",
or
test
subprograms
b
efore
y
ou
write
the
actual
co
de
for
y
our
subprograms.

Av
oid
Label
and
GoTo
instructions.
Whenev
er
p
ossible,
use
While
...
End,
Do
...
Loop,
or
For
...
...
End
constructions.

T
est
eac
h
sub-program
separately
,
ne
at
a
time,
and
use
only
w
ell-tested
mo
dules.
Adhering
to
the
principles
b
elo
w
is
not
a
guaran
tee
that
the
programs
will
w
ork.
It
is
also
p
ossible
to
write
programs
that
execute
correctly
without
an
y
of
the
b
elo
w.
Nev
ertheless,
it
is
a
go
o
d
habit
to
follo
w
these
recommendation.
The
gain
is
in
clarit
y
of
the
program,
readabilit
y
of
its
p
ortions.
Consequen
tly
,
y
ou
will
b
e
able
to
design
more
complex
programs
that
execute
as
exp
ected.
Y
ou
will
also
re-use
comp
onen
ts
easier.
If
y
ou
wrote
a
program
that
uses
GOTO
statemen
t(s),
it
is
a
go
o
d
exercise
to
re-write
it
without
a
single
GOTO
instruction!
D.
Example:
designing
an
automatic
card
dealer
Mo
dern
BASIC
is
a
structural
language.
The
ob
jectiv
e
of
this
example
is
to
sho
w
ho
w
v
arious
features
of
BASIC
in
terpla
y
in
a
design
of
a
card-dealing
application.
Here
is
the
description
of
the
situation.
A
de
ck
of
c
ar
ds
c
onsists
of

c
ar
ds.
Each
c
ar
d
has
two
attributes:

Suit
(he
arts,
sp
ades
diamonds,
and
clubs)

V
alue
(
thr
ough
)


APPENDIX
D.
PR
OGRAMMING
HELP
Car
d
games
r
e
quir
e
shuing
the
de
ck,
and
de
aling
some
numb
er
of
c
ar
ds
fr
om
the
top
of
the
de
ck.
The
obje
ctive
of
this
example
is
to
write
a
pr
o
gr
am
that
wil
l
print
out
the
numb
er
of
r
e
queste
d
c
ar
ds
twic
e.
That
is,
the
rst
player
wil
l
get
as
many
c
ar
ds
as
she
r
e
quests,
and
then
the
se
c
ond
player
wil
l
get
as
many
c
ar
ds
as
he
r
e
quests
(fr
om
the
r
est
of
the
de
ck!
D..
First
Iteration
Once
w
e
realize
what
are
the
natural
steps
the
real
p
erson
w
ould
go
through,
the
program
is
v
ery
easy
to
design.
Here
is
the
program(!)
'PROGRAM:
GIVECARD.BAS
InitializeDeck
ShuffleDeck
'ask
first
player
n=HowManyCards
DealCards(n)
'ask
second
player
n=HowManyCards
DealCards(n)
What
remains
is
only
to
decide
what
eac
h
step
should
do,
and
ho
w
the
information
ab
out
the
cards
will
b
e
stored.
Since
storing
the
information
determines
ho
w
it
is
passed
b
et
w
een
subprograms,
w
e
b
egin
with
determining
this
part.
W
e
ma
y
w
an
t
to
use
an
arra
y
Deck()
whic
h
will
b
e
of
\user
dened"
t
yp
e.
The
adv
an
tage
of
this
approac
h
is
that
w
e
ma
y
mo
dify
the
information
w
e
w
\store"
with
eac
h
card
with
minimal
c
hanges
in
the
program
itself.
DEF
TYPE
Cards
Suite
as
string
Value
as
integer
'
we
may
want
string
here,
too!
End
type
Afterw
ards
w
e
ma
y
declare
t
w
o
shared
arra
ys
Dim
Shared
Deck()
as
cards
DIM
Shared
Order()
Shared
means
that
ev
ery
SUB
in
the
program
can
access
v
alues
of
Order(j),
and
Deck(j).
The
rst
card
dealt
will
b
e
Deck(Order()).
The
denition
of
t
yp
e
sa
ys
that
its
v
alue
is
Deck(Order()).
val
ue
and
its
suit
is
Deck(Order()).su
it.
SUBS
The
simplest
w
a
y
to
b
egin
designing
SUBS
is
to
describ
e
the
purp
ose
of
eac
h
function/SUB
with
no
co
de.
SUB
InitializeDeck
'Initialize
cards
to
their
values.
'
END
SUB
The
next
routine
is
p
erhaps
not
easy
to
write
for
a
b
eginner,
but
w
e
ha
v
e
a
go
o
d
example
in
the
b
o
ok.
SUB
ShuffleDeck
'Make
a
random
permutation
'Store
it
in
shared
array
Order()
'Order(),
Order()
are
distinct
random
numbers
range
,...n,
END
SUB

D..
EXAMPLE:
DESIGNING
AN
A
UTOMA
TIC
CARD
DEALER
	
The
next
routine
in
teracts
with
the
user.
User
in
teraction
should
AL
W
A
YS
b
e
implemen
ted
as
a
separate
SUB.
As
straigh
tforw
ard
as
it
seems,
reliable
co
ding
requires
extensiv
e
c
hec
king
for
errors
resulting
from
unpredicted
user
reactions.
'ask
first
player
FUNCTION
HowManyCards
'
Ask
user
how
many
cards
(s)he
requests
'
store
in
variable.
'
Check
for
possible
errors
in
input
'
return
value
if
enough
cards
are
left.
END
FUNCTION
The
follo
wing
routine
is
in
c
harge
of
giving
out
cards.
Since
the
order
of
cards
w
as
already
determined,
it
seems
straigh
tforw
ard.
But
again
complications
arise,
and
this
p
ortion
will
b
e
m
uc
h
easier
to
handle
if
co
ded
as
a
separate
SUB
SUB
DealCards(n)
'
Remember
how
many
cards
are
left
'
Check
how
many
cards
are
left
'
Print
out
Error
message
if
not
enough
cards
'Print
next
n
cards
'
You
have
to
decide
here
HOW
the
cards
will
appear
on
screen:
'words?
pictures?
numbers?
'
(In
this
example,
it
will
be
numbers)
END
SUB
D..
Second
Iteration
Rather
than
b
eginning
to
co
de
the
actual
functions,
w
e
ma
y
w
an
t
to
double
c
hec
k
that
the
\o
w"
of
our
program
is
as
w
e
exp
ect
it.
W
e
ma
y
write
a
\dumm
y"
v
ersions
of
the
more
dicult
parts
of
the
program,
and
test
its
op
eration.
Only
after
w
e
are
sure
that
the
program
b
eha
v
es
as
exp
ected,
w
e
can
in
v
est
more
time
in
to
co
ding
more
dicult
parts.
Here
is
a
test
program.
It
w
as
pro
duced
from
the
previously
describ
ed
co
de;
all
newly
added
parts
are
clearly
mark
ed
so
that
they
can
b
e
remo
v
ed
once
not
needed.
DEF
TYPE
Cards
'***test***
REM
Suite
as
string
Suite
as
integer
'***
end
test
***
Value
as
integer
'
we
may
want
string
here,
too!
End
type
Dim
Shared
Deck()
as
cards
DIM
Shared
Order()
InitializeDeck
ShuffleDeck
'ask
first
player
n=HowManyCards
DealCards(n)
'ask
second
player
n=HowManyCards
DealCards(n)
SUB
InitializeDeck
'Initialize
cards
to
their
values.
'***
test
***

0
APPENDIX
D.
PR
OGRAMMING
HELP
for
j
=

to

Deck(j).value=j
mod

+
Deck(j).suit=j
mod

+
next
j
'***
end
test
***
END
SUB
SUB
ShuffleDeck
'Make
a
random
permutation
'Store
it
in
Order()
'Order(),
Order()
are
distinct
random
numbers
range
,...n,
'***
test
***
'Factory
order
For
j=

to

Order(j)=j
NEXT
j
'***
end
test
***
END
SUB
FUNCTION
HowManyCards%
'
Ask
user
how
many
cards
(s)he
requests
'
store
in
variable.
'
Check
how
many
cards
are
left
'
Print
out
Error
message
if
not
enough
cards
'
return
value
if
enough
cards
are
left.
'***
test
***
INPUT
``How
many
cards";
x
'
should
check
for
"crazy"
answers
here
HowManyCards=x
'***
end
test
***
END
FUNCTION
SUB
DealCards(n)
'
Remember
how
many
cards
are
left
'Print
next
n
cards
'***
test
***
'
just
print
first
n
cards
for
now
Print
"Your
cards
are:"
FOR
j=
TO
n
Print
Deck(Order(j)).Va
lu
e
;
"
of
suit
No"
;Deck(Order(j)).
Su
it
NEXT
j
'***
end
test
***
END
SUB
With
this
\sk
eleton"
program
w
e
can
c
hec
k
the
follo
wing
things:
.
Do
es
the
program
do
what
w
e
w
an
ted?
Are
shared
v
ariables
shared
b
et
w
een
subprograms?
.
Are
there
an
y
preliminary
co
ding
mistak
es/t
yp
os?
Are
the
v
ariables
of
correct
t
yp
e
(as
declared
in
eac
h
SUB/FUNCTION)?
.
Do
es
the
output
routine
op
erate
correctly?
(Ask
for
v
arious
n
um
b
erst
of
cards.
Rev
erse
the
order
in
SUB
ShuffleDeck.)
D..
Third
iteration
No
w
w
e
are
ready
to
design/co
de
eac
h
SUB.
This
is
left
for
the
reader
to
do.
Here
are
some
hin
ts.

D..
EXAMPLE:
DESIGNING
AN
A
UTOMA
TIC
CARD
DEALER


Y
ou
can
use
SELECT
CASE
...
END
SELECT
in
SUB
InitializeDeck
to
assign
w
ords
to
suits,
or
ev
en
c
haracters
CHR$()
--
CHR$().

Use
STATIC
v
ariable
to
remem
b
er
whic
h
card
to
deal
from
in
SUB
DealCards(n)

Since
w
e
ha
v
e
only

cards,
randomization
in
SUB
ShuffleDeck
do
esn't
ha
v
e
to
b
e
fast.
But
y
ou
ma
y
w
an
t
to
implemen
t
there
a
more
realistic
sim
ulation
of
sh
uing
(cutting
dec
k
in
half,
mixing
the
halv
es,
etc.)

T
o
implemen
t
a
reasonable
error
detection
in
FUNCTION
HowManyCards%
y
ou
ma
y
just
reject
re-
quests
for
negativ
e
n
um
b
er
of
cards,
and
for
more
than

cards.
INPUT
statemen
t
has
some
protection
built
in
{
fractional
n
um
b
ers
will
go
through,
but
strings
at
least
will
b
e
stopp
ed.

SUB
DealCards(n)
is
the
b
est
place
to
c
hec
k
if
there
is
enough
cards
left.
(If
not,
request
FUNCTION
HowManyCards%
from
there
again).


APPENDIX
D.
PR
OGRAMMING
HELP

List
of
T
ables
.
Probabilit
y
assignmen
ts
for
discrete
sample
spaces.
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Probabilities
Pr
(X
=
n)
in
n
Binomial
trials.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Discrete
random
v
ariables.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Con
tin
uous
random
v
ariables.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Exp
ected
v
alues
of
some
random
v
ariables.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
V
ariances
of
some
random
v
ariables.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Momen
t
generating
functions.
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:

.
Sequences
y
n
satisfying
equation
(.
)
with
dieren
t
initial
v
alues
y
0
.
:
:
0



LIST
OF
T
ABLES

Bibliograph
y
[]
R.
Mot
w
ani
&
P
.
Ragha
v
an,
Randomized
Algorithms,
Cam
bridge
Univ
ersit
y
Press,
Cam
bridge
		.
[]
N.
I.
Akhiezer,
The
Classical
Momen
t
Problem,
Oliv
er
&
Bo
yd,
Edin
burgh,
	.
[]
P
.
Billingsley
,
Probabilit
y
and
measure,
Wiley
,
New
Y
ork
	.
[]
P
.
Billingsley
,
Con
v
ergence
of
probabilit
y
measures,
Wiley
New
Y
ork
	.
[]
W.
Bryc,
Normal
Distribution,
c
haracterizations
with
applications,
Lecture
Notes
in
Statist.
v.
00
(		).
[]
A.
Dem
b
o
&
O.
Zeitouni,
Large
Deviation
T
ec
hniques
and
Applications,
Jones
and
Bartlett
Publ.,
Boston
		.
[]
R.
Durrett,
Probabilit
y:
Theory
and
examples,
W
adsw
orth,
Belmon
t,
Ca
		.
[]
K.
T.
F
ang
&
T.
W.
Anderson,
Statistical
inference
in
elliptically
con
toured
and
related
distributions,
Allerton
Press,
Inc.,
New
Y
ork
		0.
[	]
K.
T.
F
ang,
S.
Kotz
&
K.-W.
Ng,
Symmetric
Multiv
ariate
and
Related
Distributions,
Monographs
on
Statistics
and
Applied
Probabilit
y
,
Chapman
and
Hall,
London
		0.
[0]
W.
F
eller,
An
In
tro
duction
to
Probabilit
y
Theory,
V
ol.
I,
Wiley
	.
V
ol
I
I,
Wiley
,
New
Y
ork
	.
[]
J.
F.
W.
Hersc
hel,
Quetelet
on
Probabilities,
Edin
burgh
Rev.
	
(0)
pp.
{.
[]
K.
Ito
&
H.
McKean
Diusion
pro
cesses
and
their
sample
paths,
Springer,
New
Y
ork
	.
[]
N.
L.
Johnson
&
S.
Kotz,
Distributions
in
Statistics:
Con
tin
uous
Multiv
ariate
Dis-
tributions,
Wiley
,
New
Y
ork
	.
[]
N.
L.
Johnson
&
S.
Kotz
&
A.
W.
Kemp,
Univ
ariate
discrete
distributions,
Wiley
,
New
Y
ork
		.
[]
A.
M.
Kagan,
Ju.
V.
Linnik
&
C.
R.
Rao,
Characterization
Problems
of
Mathemat-
ical
Statistics,
Wiley
,
New
Y
ork
	.



BIBLIOGRAPHY
[]
A.
N.
Kolmogoro
v,
F
oundations
of
the
Theory
of
Probabilit
y,
Chelsea,
New
Y
ork
	.
[]
H.
H.
Kuo,
Gaussian
Measures
in
Banac
h
Spaces,
Lecture
Notes
in
Math.,
Springer,
V
ol.

(	).
[]
J.
C.
Maxw
ell,
Illustrations
of
the
Dynamical
Theory
of
Gases,
Phil.
Mag.
	
(0),
pp.
	{.
Reprin
ted
in
The
Scien
tic
P
ap
ers
of
James
Clerk
Maxw
ell,
V
ol.
I,
Edited
b
y
W.
D.
Niv
en,
Cam
bridge,
Univ
ersit
y
Press
	0,
pp.
{0	.
[	]
W.
Magn
us
&
F.
Ob
erhettinger,
F
orm
ulas
and
theorems
for
the
sp
ecial
functions
of
mathematical
ph
ysics,
Chelsea,
New
Y
ork
		.
[0]
K.
S.
Miller,
Multidimensional
Gaussian
Distributions,
Wiley
,
New
Y
ork
	.
[]
J.
K.
P
atel
&
C.
B.
Read,
Handb
o
ok
of
the
normal
distribution,
Dekk
er,
New
Y
ork
	.
[]
B.
L.
S.
Prak
asa
Rao,
Iden
tiabilit
y
in
Sto
c
hastic
Mo
dels
Acad.
Press,
Boston
		.
[]
W.
H.
Press,
S.
A.
T
euk
olsky
,
W.
T.
V
etterling,
B.
P
.
Flannery
,
Numerical
recip
es
in
C,
Cam
bridge
Univ
ersit
y
Press,
New
Y
ork
		.
[]
M.
Rosen
blatt,
Stationary
sequences
and
random
elds,
Birkh
auser,
Boston
	.
[]
R.
Sik
orski,
Adv
anced
Calculus,
PWN,
W
arsa
w
		.
[]
Y.
L.
T
ong,
The
Multiv
ariate
Normal
Distribution,
Springer,
New
Y
ork
		0.

Index

 
theorem,

A
mo
del
of
cell
gro
wth,

A
simple
probabilistic
mo
deling
in
Genet-
ics,

Additional
topics,

Aliasing,
0
Almost
sure
con
v
ergence,

An
application:
nd
k
-th
largest
n
um
b
er,
	
Ap
erio
dic
Mark
o
v
c
hains,

Application:
length
of
a
random
c
hain,

Application:
sc
heduling,

Application:
spread
of
epidemic,
	
Application:
T
ra
v
eling
salesman
prob-
lem,
0
Application:
v
erifying
matrix
m
ultiplica-
tion,

Arra
ys
and
matrices,

Autoregressiv
e
pro
cesses,

Ba
y
es
theorem,

Best
linear
appro
ximation,
0
Best
mean-square
appro
ximation,
0,

Best
non-linear
appro
ximations,

Binary
op
erations,

Binomial
exp
erimen
t,

Binomial
trials,

Binomial,

Bisection
metho
d,
	
Biv
ariate
normal
distribution,
,
	
Biv
ariate
normal,

Blind
searc
h,
	
Branc
hing
pro
cesses,
	
Cauc
h
y-Sc
h
w
arz
inequalit
y,
,
	
Cen
tral
Limit
Theorem,

Characteristic
functions,
	
Characteristic
function,
	
Cheb
yshev's
inequalit
y,
	
Cheb
yshev-Mark
o
v
inequalit
y,
	
Com
binations,

Com
binatorics,

Comp
ound
P
oisson
pro
cess,
0
Computing,

Conditional
distributions,

Conditional
exp
ectations
(con
tin
ued),

Conditional
exp
ectations
as
random
v
ari-
ables,

Conditional
exp
ectations,

Conditional
Exp
ectation,

Conditional
indep
endence,

Conditional
limit
theorems,
0
Conditional
probabilit
y,

Conditionals
and
lo
ops,

Consequences
of
axioms,

Con
tin
uous
distribution,

Con
tin
uous
r.
v.,

Con
tin
uous
time
Mark
o
v
pro
cesses,
0
Con
tin
uous
time
pro
cesses,
0
Con
v
ergence
in
distribution,

Con
v
ergence
in
probabilit
y,

Con
v
ergence
of
distributions,

Con
v
olution,

Correlation
co
ecien
t,
0
Co
v
ariance
matrix,
00
Co
v
ariance,

Cum
ulativ
e
distribution
function,

Data
t
yp
es,

De
Finetti,

Decreasing
ev
en
ts,

Deterministic
sc
heduling
problem,

Dierence
Equations,

Dieren
tial
equations
asso
ciated
with
the
P
oisson
pro
cess,
0
Dieren
tial
equations,

Discrete
r.
v.,

Disjoin
t
ev
en
ts,

Distances
b
et
w
een
strings,

Distributions
of
functions,




INDEX
Edit
distance,

Elemen
tary
com
binatorics,

Elemen
tary
probabilit
y
mo
dels,

Em
b
edded
Mark
o
v
c
hain,
0
Equalit
y
of
distributions,

Ev
en
ts
and
their
c
hances,

Ev
en
ts,

Example:
a
game
of
piggy,
	
Example:
c
hromatograph
y,
0
Example:
designing
an
automatic
card
dealer,

Example:
ho
w
long
a
committee
should
discuss
a
topic?,

Example:
so
ccer,

Example:
v
ehicle
insurance
claims,
	
Examples
of
con
tin
uous
r.
v.,

Examples
of
discrete
r.
v.,

Exc
hangeabilit
y,

Exc
hangeable
r.
v.,

Exclusiv
e
ev
en
ts,

Exp
ected
v
alues
and
sim
ulations,
0
Exp
ected
v
alues,

Extinction
probabilit
y,
	
File
Op
erations,

Finite
state
space,

First
Iteration,

F
ourier
series,

F
reiv
alds
tec
hnique,

F
unctions
of
r.
v.,

F
urther
notes
ab
out
sim
ulations,
	
Game
theory,

Gaussian
Mark
o
v
pro
cesses,
0
Gaussian
pro
cesses,
0
Gaussian
pro
cess,
0
General
con
tin
uous
sample
space,

General
m
ultiv
ariate
normal
la
w,
00
Generating
functions,
,
	
Generating
function,
	
Generating
random
n
um
b
ers,

Generic
Metho
d
{
con
tin
uous
case,

Generic
Metho
d
{
discrete
case,

Geometric
case,
	
Geometric
Probabilit
y,

Geometric
r.
v.,

Geometric,

Graphics,

Hersc
hel's
la
w
of
errors,

Hersc
hel,
on
normal
distribution,

Ho
eding's
Lemma,

Holder's
inequalit
y,

Homogeneous
Mark
o
v
c
hains,

Imp
ortance
sampling,

Impro
ving
blind
searc
h,

Indep
enden
t
ev
en
ts,

Indep
enden
t
i.
i.
d.,

Indep
enden
t
incremen
ts,

Indep
enden
t
random
v
ariables,

Initial
v
alue,

In
tegration
b
y
sim
ulations,

In
tensit
y
of
failures,

In
teractiv
e
links,

In
tro
ducing
BASIC,

In
tro
duction
to
sto
c
hastic
pro
cesses,

Jensens's
inequalit
y,

Join
t
distributions,

Join
tly
(absolutely)
con
tin
uous,

Lac
k
of
memory,
,

Large
deviation
b
ounds,
	
La
w
of
large
n
um
b
ers,

Limit
theorems
and
sim
ulations,
	
Limit
theorems,

Limits,

Linear
algebra,

Linear
congruen
tial
random
n
um
b
er
gen-
erators,

Linear
regression,
0
Mark
o
v
Autoregressiv
e
pro
cesses,
	
Mark
o
v
c
hain
for
annealing,
	
Mark
o
v
c
hains,

Mark
o
v
c
hain,

Mark
o
v
pro
cesses,

Martingale
Con
v
ergence
Theorem,

Math
bac
kground,

Maxw
ell,
on
normal
distribution,

Mean
square
con
v
ergence,

Mean-square
con
v
ergence,
,
	
Mink
o
wski's
inequalit
y,
,
	
Mixed
strategy,

Mo
deling
and
sim
ulation,

Momen
t
generating
function{normal
dis-
tribution,
00
Momen
t
generating
functione,


INDEX
	
Momen
t
generating
functions,

Momen
t
generating
function,
,
,

Momen
ts
of
functions,

Mon
te
Carlo
estimation
of
small
proba-
bilities,

Mon
te-Carlo
metho
ds,
0
More
ab
out
prin
ting,

Multiv
ariate
in
tegration,

Multiv
ariate
momen
t
generating
function,
	
Multiv
ariate
normal
densit
y,
0
Multiv
ariate
normal
distribution,
	
Multiv
ariate
random
v
ariables,

Normal
distribution
{
momen
t
generating
function,
00
Normal
distribution{biv
ariate,
	
Normal
distribution{co
v
ariance,
00
Normal
distribution{m
ultiv
ariate,
0,
	
Normal
distribution,

NP-complete,

Numerical
in
tegration,
	
Numerical
Metho
ds,
	
One
step
analysis,

Order
statistics,
	
Orthonormal
v
ectors,

Ov
erview
of
random
n
um
b
er
generators,

P
airwise
disjoin
t,

P
erm
utations,

Phenot
yp
e,

P
oisson
appro
ximation,

P
oisson
pro
cess,
0
P
oisson,

P
ositiv
e
denite
functions,
0
P
o
w
er
series
expansions,

P
o
w
ers
of
matrices,

Probabilit
y
densit
y
function,
,

Probabilit
y
mass
function,

Probabilit
y
measure,

Probabilit
y
space,

Programming
Help,

Prop
erties
of
conditional
exp
ectations,

Prop
erties,

Pseudo-random
n
um
b
ers,

Quadratic
error,
0
Quic
k
Sort,
	
Random
digits,

Random
gro
wth
mo
del,
0
Random
P
erm
uatations,

Random
p
erm
utations,

Random
p
erm
utation,

Random
phenomena,

Random
subsets,

Random
v
ariables
(con
tin
ued),

Random
v
ariables,

Random
w
alks,

Randomization,

Recurrence,
	0
Recursiv
e
equations,
c
haos,
randomness,

Rejection
sampling,

Reliabilit
y
function,

Risk
assessmen
t,
0
Ruining
a
gam
bler,
0
Sample
space,

Searc
hing
for
minim
um,
	
Second
order
stationary
pro
cesses,
0	
Selectiv
e
sampling,

Sequen
tial
exp
erimen
ts,

Shannon's
En
trop
y,

Sho
c
ks,
0
Simpson
rule,
	
Sim
ulated
annealing,
,
	
Sim
ulating
a
m
ultiv
ariate
normal
distri-
bution,
00
Sim
ulating
con
tin
uous
r.
v.,

Sim
ulating
discrete
ev
en
ts,

Sim
ulating
discrete
exp
erimen
ts,

Sim
ulating
discrete
r.
v.,

Sim
ulating
Mark
o
v
c
hains,

Sim
ulating
normal
distribution,

Sim
ulations
of
con
tin
uous
r.
v.,

Sim
ulations
of
discrete
r.
v.,

Sim
ulations,

Sob
ol
sequences,

Solving
dierence
equations
through
sim-
ulations,
	
Solving
equations,
	
Sorting
at
random,
	
Sp
ectral
densit
y,
0
Square
in
tegrable
r.
v.,
,
	
State
space,


0
INDEX
Stationarit
y
equations,

Stationary
pro
cesses,

Sto
c
hastic
Analysis,

Sto
c
hastic
sc
heduling
problem,

Sto
c
hastically
indep
enden
t
r.
v.,

Stopping
QBASIC,

Stopping
times,
	
Stratied
sampling,

Strong
la
w
of
large
n
um
b
ers,

Subset-Sum
Problem,

T
ail
in
tegration
form
ulas,

The
general
prediction
problem,
0
The
mean
and
the
v
ariance,
	
The
structure
of
a
program,

The
Wiener
pro
cess,
0
Theoretical
complemen
ts,

Time
Series,
0	
T
otal
probabilit
y
form
ula,
,

T
ra
jectory
a
v
erages,
0
T
rap
ezoidal
rule,
	,
0
T
riangle
inequalit
y,
,
	
T
rivial
ev
en
ts,

Tw
o-v
alued
case,
	
Uncorrelated
r.
v.,
0
Univ
ariate
normal,

User
dened
FUNCTIONs
and
SUBs,

V
ariance,

V
ariations,

V
enn
diagrams,

W
eibull
distribution,

Wiener
pro
cess,
0
Exhaustive
events,



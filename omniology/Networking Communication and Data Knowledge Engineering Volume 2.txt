Networking 
Communication 
and Data Knowledge 
Engineering
Gregorio Martinez Perez
Krishn K. Mishra
Shailesh Tiwari
Munesh C. Trivedi   Editors
Volume 2
Lecture Notes on Data Engineering
and Communications Technologies 4

Lecture Notes on Data Engineering
and Communications Technologies
Volume 4
Series editor
Fatos Xhafa, Technical University of Catalonia, Barcelona, Spain
e-mail: fatos@cs.upc.edu

The aim of the book series is to present cutting edge engineering approaches to data
technologies and communications. It publishes latest advances on the engineering task
of building and deploying distributed, scalable and reliable data infrastructures and
communication systems.
The series has a prominent applied focus on data technologies and communications
with aim to promote the bridging from fundamental research on data science and
networking to data engineering and communications that lead to industry products,
business knowledge and standardisation.
More information about this series at http://www.springer.com/series/15362

Gregorio Martinez Perez
Krishn K. Mishra ⋅Shailesh Tiwari
Munesh C. Trivedi
Editors
Networking Communication
and Data Knowledge
Engineering
Volume 2
123

Editors
Gregorio Martinez Perez
University of Murcia
Murcia
Spain
Krishn K. Mishra
Department of Computer Science
and Engineering
Motilal Nehru National Institute
of Technology
Allahabad, Uttar Pradesh
India
Shailesh Tiwari
Department of Computer Science
and Engineering
ABES Engineering College
Ghaziabad, Uttar Pradesh
India
Munesh C. Trivedi
Department of Computer Science
and Engineering
ABES Engineering College
Ghaziabad, Uttar Pradesh
India
ISSN 2367-4512
ISSN 2367-4520
(electronic)
Lecture Notes on Data Engineering and Communications Technologies
ISBN 978-981-10-4599-8
ISBN 978-981-10-4600-1
(eBook)
https://doi.org/10.1007/978-981-10-4600-1
Library of Congress Control Number: 2017938124
© Springer Nature Singapore Pte Ltd. 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #21-01/04 Gateway East, Singapore 189721, Singapore

Preface
The International Conference on Recent Advancement in Computer, Communication
and Computational Sciences (ICRACCCS 2016) has been held at Udaipur, India,
during November 25–26, 2016. The ICRACCCS 2016 has been organized and
supported by the Janardan Rai Nagar Rajasthan Vidyapeeth University,
Udaipur, India.
The ICRACCCS 2016 is an international forum for researchers, developers, and
end users to explore cutting-edge ideas and results for the problems involved in the
general areas of communication, computational sciences, and technology to dis-
seminate and share novel research solutions to real-life problems that fulﬁll the needs
of heterogeneous applications and environments, as well to identify new issues and
directions for future research and development. ICRACCCS also provides an
international communication platform for educational technology and scientiﬁc
research for the universities and engineering ﬁeld experts, and professionals.
Nowadays, globalization of academic and applied research is growing with great
pace. Computer, communication, and computational sciences are hot areas with lot
of thrust. Keeping this ideology in preference, Janardan Rai Nagar Rajasthan
Vidyapeeth University, Udaipur, INDIA, has come up with international event.
ICRACCCS 2016 has a foreseen objective of enhancing the research activities at a
large scale. Technical Program Committee and Advisory Board of ICRACCCS
include eminent academicians, researchers, and practitioners from abroad as well as
from all over the nation.
Udaipur, formerly the capital of the Mewar Kingdom, is a city in the western
Indian state of Rajasthan. Founded by Maharana Udai Singh II in 1559, it is set
around a series of artiﬁcial lakes and is known for its lavish royal residences. City
Palace, overlooking Lake Pichola, is a monumental complex of 11 palaces,
courtyards, and gardens, famed for its intricate peacock mosaics. Udaipur city is
also referred to as the “Venice of the East”, the “Most Romantic City of India,”
and the “Kashmir of Rajasthan”. Udaipur the “City of Lakes” is one among the
most romantic and most beautiful cities of India. The city of Dawn, Udaipur, is a
lovely land around the azure water lakes, hemmed in by the lush hills of the
Aravallis.
v

Janardan Rai Nagar Rajasthan Vidyapeeth University, which was recognized in
1987 and established on August 21, 1937 by Manishi Pandit Janardan Rai Nagar,
an eminent educationalist, social worker, and freedom ﬁghter, with his team of
dedicated workers. The University is now all set to take higher education to the
masses that are still not getting beneﬁts of various researches done for the
socioeconomical and cultural values. The institution is not only spreading its wings
of education in the country itself but has also entered into the area of international
studies through academic exchange of its students and faculty members to Slippery
Rock University, a prestigious university of USA. This is a step forward to the
academic excellence and toward providing opportunity to the students and teachers
of Rajasthan Vidyapeeth.
ICRACCCS 2016 received around 300 submissions from around 662 authors of
15 different countries such as USA, Algeria, China, Saudi Arabia, and many more.
Each submission has been gone through the plagiarism check. On the basis of
plagiarism report, each submission was rigorously reviewed by at least two
reviewers with an average of 2.07 per reviewer. Even some submissions have more
than two reviews. On the basis of these reviews, 48 high-quality papers were
selected for publication in this proceedings volume, with an acceptance rate of 17%.
We are thankful to the speakers: Prof. Mohan Kohle, University of Agder,
Norway, Dr. B.K. Panigrahi, IIT Delhi, Mr. Subhash Jagota, CEO, Global Business
Solutions Pvt. Ltd., India, delegates and the authors for their participation, and their
interest in ICRACCCS as a platform to share their ideas and innovation. We are
also thankful to the Prof. Dr. Xhafa, Fatos, Series Editor, LNDECT, Springer and
Mr. Aninda Bose, Senior Editor, Hard Sciences, Springer for providing continuous
guidance and support. Also, we extend our heartfelt gratitude and thanks to the
reviewers and Technical Program Committee Members for showing their concern
and efforts in the review process. We are indeed thankful to everyone directly or
indirectly associated with the conference organizing team leading it toward the
success.
We hope you enjoy the conference proceedings and wish you all the best.
Udaipur, India
Organizing Committee
ICRACCCS 2016
vi
Preface

Technical Program Committee
Prof. Ajay Gupta, Western Michigan University, USA
Prof. Babita Gupta, California State University, USA
Prof. Amit K.R. Chowdhury, University of California, USA
Prof. David M. Harvey, G.E.R.I., UK
Prof. Ajith Abraham, Director, MIR Labs.
Prof. Madjid Merabti, Liverpool John Moores University, UK
Dr. Nesimi Ertugrual, University of Adelaide, Australia
Prof. Ian L. Freeston, University of Shefﬁeld, UK
Prof. Witold Kinsner, University of Manitoba, Canada
Prof. Anup Kumar, M.I.N.D.S., University of Louisville
Prof. Sanjiv Kumar Bhatia, University of Missouri, St. Louis
Prof. Prabhat Kumar Mahanti, University of New Brunswick, Canada
Prof. Ashok De, Director, NIT Patna
Prof. Kuldip Singh, IIT Roorkee
Prof. A.K. Tiwari, IIT, BHU, Varanasi
Dr. Vivek Singh, BHU, India
Prof. Abdul Quaiyum Ansari, Jamia Millia Islamia, New Delhi, India
Prof. Aditya Trivedi, ABV-IIITM Gwalior, India
Prof. Ajay Kakkar, Thapar University, Patiala, India
Prof. Bharat Bhaskar, IIM Lucknow, India
Prof. Edward David Moreno, Federal University of Sergipe, Brazil
Prof. Evangelos Kranakis, Carleton University, Canada
Prof. Filipe Miguel Lopes Meneses, University of Minho, Portugal
Prof. Giovanni Manassero Junior, Universidade de São Paulo, Brazil
Prof. Gregorio Martinez, University of Murcia, Spain
Prof. Pabitra Mitra, Indian Institute of Technology Kharagpur, India
Prof. Joberto Martins, Salvador University-UNIFACS
Prof. K. Mustafa, Jamia Millia Islamia, New Delhi, India
Prof. M.M. Sufyan Beg, Jamia Millia Islamia, New Delhi, India
Prof. Jitendra Agrawal, Rajiv Gandhi Proudyogiki Vishwavidyalaya, Bhopal, M.P.,
India
vii

Prof. Rajesh Baliram Ingle, PICT, University of Pune, India
Prof. Romulo Alexander Ellery de Alencar, University of Fortaleza, Brazil
Prof. Youssef Fakhri, Université Ibn Tofail, Faculté des Sciences
Dr. Abanish Singh, Bioinformatics Scientist, USA
Dr. Abbas Cheddad, (UCMM), Umeå Universitet, Umeå, Sweden
Dr. Abraham T. Mathew, NIT, Calicut, Kerala, India
Dr. Adam Scmidit, Poznan University of Technology, Poland
Dr. Agostinho L.S. Castro, Federal University of Para, Brazil
Prof. Goo-Rak Kwon Chosun University, Republic of Korea
Dr. Adam Scmidit, Poznan University of Technology, Poland
Prof. Nishant Doshi, S V National Institute of Technology, Surat, India
Prof. Gautam Sanyal, NIT Durgapur, India
Dr. Agostinho L.S. Castro, Federal University of Para, Brazil
Dr. Alberto Yúfera, Seville Microelectronics Institute, IMSE-CNM
Dr. Alok Chakrabarty, IIIT Bhubaneswar, India
Dr. Anastasios Tefas, Aristotle University of Thessaloniki, Greece
Dr. Anirban Sarkar, NIT Durgapur, India
Dr. Anjali Sardana, IIT Roorkee, Uttarakhand, India
Dr. Arifﬁn Abdul Mutalib, Universiti Utara Malaysia, Malaysia
Dr. Ashok Kumar Das, IIIT Hyderabad, India
Dr. Ashutosh Saxena, Infosys Technologies Ltd., India
Dr. Balasubramanian Raman, IIT Roorkee, India
Dr. Benahmed Khelifa, Liverpool John Moores University, UK
Dr. Björn SCHULLER, Tech. Univ. Munich, Germany
Dr. Carole Bassil, Lebanese University, Lebanon
Dr. Chao MA, Hong Kong Polytechnic University, Hong Kong
Dr. Chi-Un Lei, University of Hong Kong, Hong Kong
Dr. Ching-Hao Mao, Institute for Information Industry, Taiwan
Dr. Chung-Hua Chu, National Taichung Institute of Technology, Taiwan
Dr. Chunye Gong, National University of Defense Technology, Taiwan
Dr. Cristina Olaverri Monreal, Instituto de Telecomunicacoes, Portugal
Dr. Chittaranjan Hota, BITS Hyderabad, India
Dr. D. Juan Carlos González Moreno, University of Vigo, Spain
Dr. Danda B. Rawat, Old Dominion University, USA
Dr. Davide Ariu, University of Cagliari, Italy
Dr. Dimiter G. Velev, University of National and World Economy, Europe
Dr. D.S. Yadav, Director, GEC, Banda, India
Dr. Darius M. Dziuda, Central Connecticut State University, USA
Dr. Dimitrios Koukopoulos, University of Western Greece, Greece
Dr. Durga Prasad Mohapatra, NIT-Rourkela, India
Dr. Eric Renault, Institut Telecom, France
Dr. Felipe RudgeBarbosa, University of Campinas, Brasil
Dr. Fermín Galán Márquez, Telefónica I+D, Spain
Dr. Fernando Zacarias Flores, Autonomous University of Puebla, Mexico
Dr. Fuu-Cheng Jiang, Tunghai University, Taiwan
viii
Technical Program Committee

Prof. Aniello Castiglione, University of Salerno, Italy
Dr. Geng Yang, NUPT, Nanjing, P. R. of China
Dr. Gadadhar Sahoo, BIT-Mesra, India
Prof. Ashokk Das, IIIT, Hyderabad, India
Dr. Gang Wang, Hefei University of Technology, China
Dr. Gerard Damm, Alcatel-Lucent, USA
Prof. Liang Gu, Yale University, New Haven, CT, USA
Prof. K.K. Pattanaik, ABV-IIITM, Gwalior, India
Dr. Germano Lambert-Torres, Itajuba Federal University, Brazil
Dr. Guang Jin, Intelligent Automation, Inc, USA
Dr. Hardi Hungar, Carl von Ossietzky University Oldenburg, Germany
Dr. Hongbo Zhou, Southern Illinois University Carbondale, USA
Dr. Huei-Ru Tseng, Industrial Technology Research Institute, Taiwan
Dr. Hussein Attia, University of Waterloo, Canada
Prof. Hong-Jie Dai, Taipei Medical University, Taiwan
Prof. Edward David, UFS - Federal University of Sergipe, Brazil
Dr. Ivan Saraiva Silva, Federal University of Piauí, Brazil
Dr. Luigi Cerulo, University of Sannio, Italy
Dr. J. Emerson Raja, Engineering and Technology of Multimedia University,
Malaysia
Dr. J. Satheesh Kumar, Bharathiar University, Coimbatore, India
Dr. Jacobijn Sandberg, University of Amsterdam, Netherland
Dr. Jagannath V. Aghav, College of Engineering Pune, India
Dr. JAUME Mathieu, LIP6 UPMC, France
Dr. Jen-Jee Chen, National University of Tainan, Taiwan
Dr. Jitender Kumar Chhabra, NIT-Kurukshetra, India
Dr. John Karamitsos, Tokk Communications, Canada
Dr. Jose M. Alcaraz Calero, University of the West of Scotland, UK
Dr. K.K. Shukla, IT-BHU, India
Dr. K.R. Pardusani, Maulana Azad NIT, Bhopal, India
Dr. Kapil Kumar Gupta, Accenture, Australia
Dr. Kuan-Wei Lee, I-Shou University, Taiwan
Dr. Lalit Awasthi, NIT Hamirpur, India
Dr. Maninder Singh, Thapar University, Patiala, India
Dr. Mehul S. Raval, DA-IICT, Gujarat, India
Dr. Michael McGuire, University of Victoria, Canada
Dr. Mohamed Naouai, University Tunis El Manar and University of Strasbourg,
Tunisia
Dr. Nasimuddin, Institute for Infocomm Research, Singapore
Dr. Olga C. Santos, aDeNu Research Group, UNED, Spain
Dr. Pramod Kumar Singh, ABV-IIITM Gwalior, India
Dr. Prasanta K. Jana, IIT, Dhanbad, India
Dr. Preetam Ghosh, Virginia Commonwealth University, USA
Dr. Rabeb Mizouni, (KUSTAR), Abu Dhabi, UAE
Dr. Rahul Khanna, Intel Corporation, USA
Technical Program Committee
ix

Dr. Rajeev Srivastava, CSE, ITBHU, India
Dr. Rajesh Kumar, MNIT, Jaipur, India
Dr. Rajesh Bodade, MCT, Mhow, India
Dr. Rajesh Kumar, MNIT, Jaipur, India
Dr. Ranjit Roy, SVNIT, Surat, Gujarat, India
Dr. Robert Koch, Bundeswehr University München, Germany
Dr. Ricardo J. Rodriguez, Nova Southeastern University, USA
Dr. Ruggero Donida Labati, Università degli Studi di Milano, Italy
Dr. Rustem Popa, University “Dunarea de Jos” in Galati, Romania
Dr. Shailesh Ramchandra Sathe, VNIT Nagpur, India
Dr. Sanjiv K. Bhatia, University of Missouri—St. Louis, USA
Dr. Sanjeev Gupta, DA-IICT, Gujarat, India
Dr. S. Selvakumar, National Institute of Technology, Tamil Nadu, India
Dr. Saurabh Chaudhury, NIT Silchar, Assam, India
Dr. Shijo. M. Joseph, Kannur University, Kerala
Dr. Sim Hiew Moi, University Technology of Malaysia
Dr. Syed Mohammed Shamsul Islam, The University of Western Australia
Dr. Trapti Jain, IIT Mandi, India
Dr. Tilak Thakur, PEC, Chandighar, India
Dr. Vikram Goyal, IIIT Delhi, India
Dr. Vinaya Mahesh Sawant, D. J. Sanghvi College of Engineering, India
Dr. Vanitha Rani Rentapalli, VITS Andhra Pradesh, India
Dr. Victor Govindaswamy, Texas A&M University-Texarkana, USA
Dr. Victor Hinostroza, Universidad Autónoma de Ciudad Juárez, Mexico
Dr. Vidyasagar Potdar, Curtin University of Technology, Australia
Dr. Vijaykumar Chakka, DAIICT, Gandhinagar, India
Dr. Yong Wang, School of IS & E, Central South University, China
Dr. Yu Yuan, Samsung Information Systems America—San Jose, CA, USA
Eng. Angelos Lazaris, University of Southern California, USA
Mr. Hrvoje Belani, University of Zagreb, Croatia
Mr. Huan Song, SuperMicro Computer, Inc., San Jose, USA
Mr. K.K. Patnaik, IIITM, Gwalior, India
Dr. S.S. Sarangdevot, Vice Chancellor, JRN Rajasthan Vidyapeeth University,
Udaipur
Dr. N.N. Jani, KSV University Gandhinagar, India
Dr. Ashok K. Patel, North Gujarat University, Patan, Gujarat, India
Dr. Awadhesh Gupta, IMS, Ghaziabad, India
Dr. Dilip Sharma, GLA University, Mathura, India
Dr. Li Jiyun, Donghua Univesity, Shanghai, China
Dr. Lingfeng Wang, University of Toledo, USA
Dr. Valentina E. Balas, Aurel Vlaicu University of Arad, Romania
Dr. Vinay Rishiwal, MJP Rohilkhand University, Bareilly, India
Dr. Vishal Bhatnagar, Ambedkar Institute of Technology, New Delhi, India
Dr. Tarun Shrimali, Sun rise Group of Institutions, Udaipur, India
Dr. Atul Patel, CU Shah University, Wadhwan, Gujarat, India
x
Technical Program Committee

Dr. P.V. Virparia, Sardar Patel University, VV Nagar, India
Dr. D.B. Choksi, Sardar Patel University, VV Nagar, India
Dr. Ashish N. Jani, KSV University Gandhi Nagar, India
Dr. Sanjay M. Shah, KSV University Gandhi Nagar, India
Dr. Vijay M. Chavda, KSV University Gandhi Nagar, India
Dr. B.S. Agarwal, KIT Kalol, India
Dr. Apurv Desai, South Gujrat University, Surat, India
Dr. Chitra Dhawale, Nagpur, India
Dr. Bikas Kumar, Pune, India
Dr. Nidhi Divecha, Gandhi Nagar, India
Dr. Jay Kumar Patel, Gandhi Nagar, India
Dr. Jatin Shah, Gandhi Nagar, India
Dr. Kamaljit I. Lakhtaria, Auro University, Surat, India
Dr. B.S. Deovra, B.N. College, Udaipur, India
Dr. Ashok Jain, Maharaja College of Engg, Udaipur, India
Dr. Bharat Singh, JRN Rajasthan Vidyapeeth University, Udaipur, India
Dr. S.K. Sharma, Paciﬁc University Udaipur, India
Dr. Naresh Trivedi, Ideal Institute of Technology, Ghaziabad, India
Dr. Akheela Khanum, Integral University Lucknow, India
Dr. R.S. Bajpai, Ram Swaroop Memorial University, Lucknow, India
Dr. Manish Shrimali, JRN Rajasthan Vidyapeeth University, Udaipur, India
Dr. Ravi Gulati, South Gujrat University, Surat, India
Dr. Atul Gosai, Saurashtra Univesrity, Rajkot, India
Dr. Digvijai sinh Rathore, BBA Open University Ahmadabad, India
Dr. Vishal Goar, Govt Engg College, Bikaner, India
Dr. Neeraj Bhargava, MDS University Ajmer, India
Dr. Ritu Bhargava, Govt Women Engg College, Ajmer, India
Dr. Rajender Singh Chhillar, MDU Rohtak, India
Dr. Dhaval R. Kathiriya, Saurashtra Univesrity, Rajkot, India
Dr. Vineet Sharma, KIET Ghaziabad, India
Dr. A.P. Shukla, KIET Ghaziabad, India
Dr. R.K. Manocha, Ghaziabad, India
Dr. Nandita Mishra, IMS Ghaziabad, India
Dr. Manisha Agarwal, IMS Ghaziabad, India
Dr. Deepika Garg, IGNOU New Delhi, India
Dr. Goutam Chakraborty, Iwate Prefectural University, Iwate Ken, Takizawa, Japan
Dr. Amit Manocha Maharaja Agrasen University, HP, India
Prof. Enrique Chirivella-Perez, University of the West of Scotland, UK
Prof. Pablo Salva Garcia, University of the West of Scotland, UK
Prof. Ricardo Marco Alaez, University of the West of Scotland, UK
Prof. Nitin Rakesh, Amity University, Noida, India
Prof. Mamta Mittal, G. B. Pant Govt. Engg. College, Delhi, India
Dr. Shashank Srivastava, MNNIT Allahabad, India
Prof. Lalit Goyal, JMI, Delhi, India
Dr. Sanjay Maurya, GLA University, Mathura, India
Technical Program Committee
xi

Prof. Alexandros Iosiﬁdis, Tampere University of Technology, Finland
Prof. Shanthi Makka, JRE Engg. College, Greater Noida, India
Dr. Deepak Gupta, Amity University, Noida, India
Dr. Manu Vardhan, NIT Raipur, India
Dr. Sarsij Tripathi, NIT Raipur, India
Prof. Wg Edison, Hefei University of Technology, China
Dr. Atul Bansal, GLA University, Mathura, India
Dr. Alimul Haque, V.K.S.University, Bihar, India
Prof. Simhiew Moi, Universiti Teknologi Malaysia
Prof. Rustem Popa, “Dunarea de Jos” University of Galati, Romania
Prof. Vinod Kumar, IIT Roorkee, India
Prof. Christos Bouras, Univ. of Patras and RACTI, Greece
Prof. Devesh Jinwala, SVNIT Surat, India
Prof. Germano Lambert Torres, PS Solutions, Brazil
Prof. Byoungho Kim, Broadcom Corp., USA
Prof. Aditya Khamparia, LPU, Punjab, India
xii
Technical Program Committee

About the Book
Data science, data engineering, and knowledge engineering require networking and
communication as a backbone and have wide scope of implementation in engineering
sciences. Keeping this ideology in preference, this book includes the insights that
reﬂect the advances in these ﬁelds from upcoming researchers and leading
academicians across the globe. It contains high-quality peer-reviewed papers of
‘International Conference on Recent Advancement in Computer, Communication
and Computational Sciences (ICRACCCS 2016)’, held at Janardan Rai Nagar
Rajasthan Vidyapeeth University, Udaipur, India, during November 25–26, 2016.
The volume covers variety of topics such as Advanced Communication Networks,
Artiﬁcial Intelligence and Evolutionary Algorithms, Advanced Software Engineering
and Cloud Computing, Image Processing and Computer Vision, and Security.
The book will help the perspective readers from computer industry and academia to
derive the advances of next generation communication and computational technology
and shape them into real-life applications.
xiii

Contents
Part I
Advanced Software Engineering and Cloud Computing
WebN: A Strainer Approach for Website Performance Analysis . . . . . .
3
Charmy Patel and Ravi Gulati
A Phase-wise Review of Software Security Metrics . . . . . . . . . . . . . . . . .
15
Syed Anas Ansar, Alka and Raees Ahmad Khan
A Survey on Code Clone, Its Behavior and Applications. . . . . . . . . . . . .
27
Aakanshi Gupta and Bharti Suri
Trust for Task Scheduling in Cloud Computing Unfolds
It Through Fruit Congenial . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
Nidhi Bansal and Ajay Kumar Singh
Log-Based Cloud Forensic Techniques: A Comparative Study . . . . . . . .
49
Palash Santra, Asmita Roy, Sadip Midya, Koushik Majumder
and Santanu Phadikar
An Automated Malicious Host Recognition Model
in Cloud Forensics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
Suchana Datta, Palash Santra, Koushik Majumder and Debashis De
Parallel Bat Algorithm-Based Clustering Using MapReduce . . . . . . . . . .
73
Tripathi Ashish, Sharma Kapil and Bala Manju
Predicting Strategic Behavior Using Game Theory
for Secure Virtual Machine Allocation in Cloud . . . . . . . . . . . . . . . . . . .
83
Priti Narwal, Shailendra Narayan Singh and Deepak Kumar
Low-Size Cipher Text Homomorphic Encryption Scheme
for Cloud Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
Manish M. Potey, C. A. Dhote and Deepak H. Sharma
xv

Relation Between Facebook Stories and Hours of a Day . . . . . . . . . . . . .
103
Hradesh Kumar and Sanjeev Kumar Yadav
Part II
Image Processing and Computer Vision
Lagrangian Twin SVR Based Grayscale Image Watermarking Using
LWT-QR Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
Ashok Kumar Yadav, Rajesh Mehta and Raj Kumar
Difference in Lights and Color Background Differentiates
the Color Skin Model in Face Detection for Security Surveillance . . . . .
127
Dimple Chawla and Munesh Chandra Trivedi
Feature Extraction and Fuzzy-Based Feature Selection Method
for Long Range Captured Iris Images . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
Anand Deshpande and Prashant P. Patavardhan
Information Retrieves from Brain MRI Images for Tumor
Detection Using Hybrid Technique K-means and Artiﬁcial
Neural Network (KMANN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
Manorama Sharma, G. N. Purohit and Saurabh Mukherjee
Comparative Analysis and Evaluation of Biclustering
Algorithms for Microarray Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
Ankush Maind and Shital Raut
Unconstrained Iris Image Super Resolution in Transform Domain . . . .
. . . .
173
Anand Deshpande and Prashant P. Patavardhan
Part III
Security
An Extension to Modiﬁed Harn Digital Signature Scheme
with the Feature of Message Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
Shailendra Kumar Tripathi and Bhupendra Gupta
Security and Energy Analysis on Revised Mutual Authentication
Protocol in WiMAX Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
Jayashree Padmanabhan and Gunavathie Mariappan
Personal Veriﬁcation Using Off-line Signature with Tree-based
Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
Arun Kumar Shukla and Suvendu Kanungo
Generalized Parametric Model for AVK-Based Cryptosystem . . . . . . . .
217
Shaligram Prajapat, Adisha Porwal, Swati Jaiswal, Fatema Saifee
and R. S. Thakur
xvi
Contents

Implementation of Modiﬁed RSA Approach for Encrypting
and Decrypting Text Using Multi-power and K-Nearest
Neighbor Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
Shikha Mathur, Deepika Gupta, Vishal Goar and Sunita Choudhary
Traditional and Hybrid Encryption Techniques: A Survey . . . . . . . . . . .
239
Pooja Dixit, Avadhesh Kumar Gupta, Munesh Chandra Trivedi
and Virendra Kumar Yadav
A Comparative Study of Recent Advances in Big Data for Security
and Privacy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
Ahlam Kourid and Salim Chikhi
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261
Contents
xvii

About the Editors
Gregorio Martinez Perez is Full Professor in the Department of Information and
Communication Engineering of the University of Murcia, Murcia, Spain. His
research interests include security and management of distributed communication
networks. He received Ph.D. degree in Computer Science from the University of
Murcia.
Krishn K. Mishra is currently working as a Visiting Faculty, Department of
Mathematics and Computer Science, University of Missouri, St. Louis, USA. He is
an alumnus of Motilal Nehru National Institute of Technology Allahabad, India,
which is also his base working institute. His primary area of research includes
evolutionary algorithms, optimization techniques and design, and analysis of
algorithms. He has also published more than 50 publications in International
Journals and in Proceedings of Internal Conferences of repute. He is serving as a
program committee member of several conferences and also editing Scopus and
SCI-indexed journals. He has 15 years of teaching and research experience during
which he made all his efforts to bridge the gaps between teaching and research.
Shailesh Tiwari currently works as a Professor in Computer Science and
Engineering Department, ABES Engineering College, Ghaziabad, India. He is also
administrative head of the department. He is an alumnus of Motilal Nehru National
Institute of Technology Allahabad, India. He has more than 15 years of experience in
teaching, research, and academic administration. His primary areas of research are
software testing, implementation of optimization algorithms, and machine learning
techniques in software engineering. He has published more than 40 publications in
International Journals and in Proceedings of International Conferences of repute. He
is also serving as a program committee member of several conferences and also
editing Scopus and E-SCI-indexed journals. He has organized several international
conferences under the banner of IEEE and Springer. He is a Senior Member of IEEE,
member of IEEE Computer Society, and Executive Committee member of IEEE Uttar
Pradesh section. He is a member of reviewer and editorial board of several
International Journals and Conferences.
xix

Munesh C. Trivedi currently works as a Professor in Computer Science and
Engineering Department, ABES Engineering College, Ghaziabad, India. He has
rich experience in teaching the undergraduate and postgraduate classes. He has
published 20 text books and 80 research publications in different International
Journals and Proceedings of International Conferences of repute. He has received
Young Scientist Visiting Fellowship and numerous awards from different national
as well international forum. He has organized several international conferences
technically sponsored by IEEE, ACM, and Springer. He has delivered numerous
invited and plenary conference talks throughout the country and chaired technical
sessions in international and national conferences in India. He is on the review
panel of IEEE Computer Society, International Journal of Network Security, Pattern
Recognition Letter and Computer & Education (Elsevier’s Journal). He is Executive
Committee Member of IEEE UP Section, IEEE India Council, and also IEEE Asia
Paciﬁc Region 10. He is an active member of IEEE Computer Society, International
Association of Computer Science and Information Technology, Computer Society
of India, International Association of Engineers, and life member of ISTE.
xx
About the Editors

Part I
Advanced Software Engineering
and Cloud Computing

WebN: A Strainer Approach for Website
Performance Analysis
Charmy Patel and Ravi Gulati
Abstract Software with high quality is in demand in this competitive software
industry. Software validation, veriﬁcation and quality maintenance are signiﬁcant
principles of testing. Performance testing is the process of determining the speed
and the effectiveness of a system. It basically focuses on determining whether the
user of the system will be satisﬁed with the performance characteristics of the
application. Performance optimization is the process to make and manage the
quality demands of a product. In this paper, we propose a novel approach named
web performance analyzer (WebN) which analyses the website, retrieves all
performance-dependent factors and gives appropriate suggestions. These sugges-
tions are framed based on the analysis of approximately 10 million websites’
performance data. These suggestions are distinct from other web analyzer tools.
After web page optimization, the status visualization for performance improvement
is given by WebN. Our research work will help the developers to construct sys-
tematic and qualitative software throughout the software development life cycle.
Keywords Performance testing ⋅
Performance optimization ⋅
Performance
analyzer
C. Patel (✉)
Shree Ramkrishana Institute of Computer Education & Applied Sciences,
Surat, Gujarat, India
e-mail: charmyspatel@gmail.com
R. Gulati
Department of Computer Science, Veer Narmad South Gujarat University,
Surat, Gujarat, India
e-mail: rmgulati@vnsgu.ac.in
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_1
3

1
Introduction
Quality is an important factor in software industry. Due to high customer demand, it
is signiﬁcant to develop qualitative software in competitive trade. By applying
proper quality standards, developer can achieve customer satisfaction. With a good
quality of the content and the usefulness of the service, users are satisﬁed and enjoy
surﬁng the website. But it is possible when it includes fast response times. Because
performance stress just keeps budding, a performance test can help determine
whether the product meets the performance goals or not. In software engineering,
how a system performs in terms of responsiveness and stability under a particular
workload is determined by applying performance testing.
Web performance testing can be addressed in terms of three categories: speed,
scalability and stability. The three important performance measures that lead the
developers in performance-driven development at coding phase are as follows:
(1) speed which determines whether the application responds quickly; (2) scalabil-
ity which determines maximum user load the software application can handle; and
(3) stability which determines whether the application is stable under varying loads.
So, a number of http requests, various objects’ size and their loading times are very
important measurements which can affect web page performance.
This paper is structured as follows: Sect. 2 covers the needs of performance
indicator and analyser. Sections 3 and 4 describe the working and implementation
concept of WebN. Section 5 describes our experimental set-up, process and results.
We ﬁnally conclude by demonstrating our derived suggestions for websites which
will help to improve its performance.
2
Need of Performance Indicator and Analyser
By producing high-quality product, one can achieve a respectable position in global
market because competition is very high and one cannot afford correcting errors
after delivering the products to the client [1]. For that, testing is the most extensively
used approach to ensure software quality [2]. Websites are gradually becoming
more dependent on complicated technologies such as interactive designing tools,
videos and plug-ins which catch the attention and grab interest of visitors towards it
[3]. But if the use of these technologies creates delays or fails to work accurately,
visitors may quickly dump the site and scamper to the rivalry. Web users have
become progressively more eager when it comes to speed. Earlier speed was
considered to be a feature, and now, it is reckoned inevitably [4].
To maintain the performance Service Level Agreement which is bounded by the
developing company with their customer, good quality of website with good per-
formance optimization is a must. So, to reach the software quality, developer
requires suggestions about the performance of an application developed by him/her.
Various web performance testing tools are available in market, but they have certain
4
C. Patel and R. Gulati

limitations and variations in their respective areas. In Table 1, we have compared
various performance testing tools with our performance analyzer—WebN. After
comparing these tools, we have analysed that some features which we have
included in our analyzer are essential for the performance and quality improvement
of the web modules. Most important features of our analyzer are the ideal range of
critically affecting performance measurements which are derived using linear
regression mining technique and the suggestions given on the basis of ideal range to
optimize the performance of that individual measurement which plays critical role
for performance degradation of the web module.
Table 1 Comparison of various performance testing tools with WebN
Features
PageSpeed
YSlow
Firebug
WebPageTest
Pingdom
WebN
Online available
✕
✕
✓
✓
✓
✓
Browser compatiability
✓
✕
✕
✓
✓
✓
Page load time
✓
✕
✓
✓
✓
✓
HTML load time
✓
✕
✓
✓
✓
✓
Images load time
✓
✕
✓
✓
✓
✓
Average PNG images
load time
✕
✕
✕
✕
✕
✓
Average JPG images
load time
✕
✕
✕
✕
✕
✓
Average GIF images
load time
✕
✕
✕
✕
✕
✓
Java script load time
✓
✕
✓
✓
✓
✓
CSS load time
✓
✕
✓
✓
✓
✓
Multimedia load time
✓
✕
✓
✓
✓
✓
Page size
✓
✓
✓
✓
✓
✓
HTML size
✓
✓
✓
✓
✓
✓
Image size
✓
✓
✓
✓
✓
✓
Java script size
✓
✓
✓
✓
✓
✓
CSS size
✓
✓
✓
✓
✓
✓
Multimedia size
✓
✓
✓
✓
✓
✓
Total HTML ﬁles
✓
✓
✓
✓
✓
✓
Total image ﬁles
✓
✓
✓
✓
✓
✓
Total java script ﬁles
✓
✓
✓
✓
✓
✓
Total CSS ﬁles
✓
✓
✓
✓
✓
✓
Total multimedia ﬁles
✓
✓
✓
✓
✓
✓
CPU usage
✕
✕
✕
✓
✕
✓
Memory usage
✕
✕
✕
✕
✕
✓
Ideal range for
measurements
✕
✕
✕
✕
✕
✓
Optimization suggestion
✕
✕
✕
✕
✕
✓
WebN: A Strainer Approach for Website Performance Analysis
5

3
Working of WebN
We have developed a framework which will fetch the actual values of critical
measures which affect the performance degradation of web application in every
aspect—speed, scalability and stability. Then, compare that actual value of all
measures with the ideal range values, and provide improvement suggestions for
better quality optimization of current web page and further development process for
same type of application. To evaluate this hypothesis, we have implemented web
performance analyzer—WebN—for web performance analysis.
4
Implementation of WebN
To develop an effective and efﬁcient web application, developer has to take care of
its performance. For that, speed, scalability and stability are the most important
qualitative aspects. Our new analyser approach will help developer to ﬁnd the cause
measures which affect the performance degradation, give him/her relevant
improvement suggestion for particular caused measure at code phase itself and also
help to improve the development process for the other web pages. As the perfor-
mance of a network connection varies considerably. In our web analyser, we have
considered only the network independent aspects of the page performance: the
server conﬁguration, external resources used in a page such as Images, JavaScript ,
CSS, Frames and Multimedia ﬁles. We have considered full page load time and not
the page rendering time with empty cache view. The connection speed, latency and
packet loss from the test location are other variables that affect the speed aspect
measurements. Page load times can vary depending on browser.
4.1
Web Performance Indicator
Various performance testing tools are available to retrieve the actual performance
value of measurements. But due to measurement variation in tools, we have
developed a performance indicator which enhances the actual performance value of
web module. For that, we have used google PageSpeed and WebPageTest API to
fetch the actual performance value of web module. PageSpeed Insights API is used
to programmatically generate PageSpeed scores, and WebPageTest also uses
google RESTful services API which returns information on a number of perfor-
mance factors. Our performance indicator fetches the performance measurement
value in all three aspects such as in speed aspect—page, various images, java script,
style sheet, multimedia and html load time; in scalability aspect—page, images,
java script, style sheet, multimedia, html size and total counts; and in stability
aspect—CPU and memory usage. The actual performance parameter values of
6
C. Patel and R. Gulati

measurements discussed above are derived by our customized PHP code module.
The data logs are maintained for different websites which are analysed through our
performance indicator for future data analysis.
4.2
Web Performance Analyzer
Performance analyzer analyses the actual performance values of each performance
measurement fetched by the indicator. By this approach, we give the performance
status of each individual measurement, also give improvement suggestion for
critically affecting measurement for performance improvement, visualize the per-
formance status graphically and also provide performance optimization priority for
critical affecting measurements. After analysing the web module, from the actual
values of all performance measurements we give the status of all individual
parameter. For obtaining status value of performance measurements, we compare
all measurements’ actual values with our derived ideal performance data range
which are calculated by performing linear regression analysis by ﬁnding conﬁdence
interval and resulting that if the value is beyond the ideal data range, then mea-
surement considered as ‘POOR’ performance status; if it lies within ideal data
range, then it gives status as ‘GOOD’ performance; and if the value is below ideal
range, then it gives the status ‘VERY GOOD’. If the status value of the particular
measurement is ‘POOR’, then the analyser also provides with the ideal range for a
particular measurement as a suggestion to optimize that measurement value which
will affect the overall performance of the web page. By this suggestion, developer
will get clear idea that up to which level he/she has to optimize that particular
measurement value to improve overall performance.
4.3
Performance visualization
We display this performance analysis results in graphical visualization format also.
By this approach, developer can easily understand which measurements affect more
for the performance degradation and which has to be optimized in all three
aspects—speed, scalability and stability. With the help of Google Visualization
Chart API, we have displayed bubble chart to display performance status of each
performance affected measurements. Chart is displayed in three category aspects as
mentioned above. On X-axis, various measurements are listed, and on Y-axis, the
three status values are listed—‘POOR’, ‘GOOD’, and ‘VERY GOOD’. If the
measurement’s status lies in ‘POOR’ status, then we also display improvement
suggestion for that particular measurement on its bubble’s tooltip value. As a test
case, we have analysed one module of www.nexgenidia.com, a website for software
outsourcing Software Development Company through our web performance ana-
lyzer, WebN, and retrieved performance results of affected measurement. Figures 1,
WebN: A Strainer Approach for Website Performance Analysis
7

2 and 3 represent the performance status values for each performance measurements
in all three aspects—speed, scalability and stability. By this graphical visualization,
developer can get clear idea that in speed aspect, image load time and JavaScript
load time affect more than other measurements, and in scalability aspect, total
number of CSS and JavaScript, HTML size, CSS size and JavaScript size affect the
performance degradation. It also provides ideal range for all these affected mea-
surements for performance optimization.
Fig. 1 Speed aspect status visualization
Fig. 2 Scalability aspect status visualization
8
C. Patel and R. Gulati

5
Experiments and Findings
To evaluate the efﬁciency of our system results, we have retrieved the web per-
formance ratio. The web performance optimization ratio which is gained after the
improvement by applying WebN suggestions will be based on the following for-
mula. First, current overall web performance is calculated according to the speed,
scalability and stability aspects, and then, website is scanned via WebN for per-
formance optimization process. After the application of optimization suggestions,
new performance index is calculated. The relationship between current web per-
formance index and after optimization derived performance index gives the overall
performance improvement ratio between before and after optimization values of the
website.
where
Wold = Web Performance Index before optimization and
Wnew = Web Optimization Index after optimization
Web Optimization Ratio =
Wold −Wnew
ð
Þ ̸Wold
ð
Þ * 100
ð1Þ
For calculating web optimization ratio, we have analysed different web appli-
cations of local software companies which are in development phase as a test case.
When we have analysed web module of nexgenindia.com in our WebN analyzer,
we get the actual performance values through which analyser suggests that in speed
aspect, image and JavaScript are critical performance-affecting factors which affect
page load time. Both also affect the page size in scalability aspect. CPU and
Fig. 3 Stability aspect status visualization
WebN: A Strainer Approach for Website Performance Analysis
9

memory usage are not much affected in stability aspect. After optimization of web
page based on the suggestions for improvement given by our analyzer, we got
24.17% improvement in speed aspect and 32.09% improvement in scalability
aspect. Following is the visual improvement ratio for speed, scalability and stability
aspects (Figs. 4, 5, 6 and 7).
Fig. 4 Improvement ratio for speed aspect
Fig. 5 Improvement ratio for scalability aspect of size objects
10
C. Patel and R. Gulati

After analysing the performance of ongoing web module of various software
companies which is in development phase and from the results analysis, we derived
that WebN is useful because of the following reasons:
• With this approach, application can be passed through continuous quality check.
• Performance degradation factors can be easily traced out.
• Developers
also
get
suggestions
to
optimize
the
performance-affecting
measurements.
Fig. 6 Improvement ratio for scalability aspect of ﬁle count objects
Fig. 7 Improvement ratio for stability aspect
WebN: A Strainer Approach for Website Performance Analysis
11

• In the phase of project maintenance, features of this concept can be utilized and
expected outcomes can be achieved in previously deployed project also.
• It will also save the cost and time of the overall development process because
most of the application performance factors are optimized at the development
phase itself by the developers as small development companies cannot afford
future failure or highly functioning infrastructure.
6
Conclusion
Performance is dependent on resource utilization, response times and throughput
that meet the performance objectives. Performance of the website should meet its
requirements for timeliness because responsiveness is the main objective of any
system which claims quality. Improvement in web performance with respect to time
and resources is the most challenging and dominating activity in today’s digital
world for researchers. After analysing various performance measurements and
testing different performance testing tools, we observed that majority of the per-
formance tools have variations in their respective areas and they do not give any
optimization suggestions with ideal range values for critical performance factors.
Performance tuning is a qualitative, systematic approach which helps in quickly
ﬁnding out the problems, identifying potential solutions and prioritizing tasks to
accomplish the improvements with the least effort. Performance-driven approach
will lead industry to quality and will result in good productivity and ﬁnancial gain.
This concept will be useful for the betterment of software developers as well as for
the company’s credibility and revenue generation.
References
1. How to Improve Software Quality Assurance in Developing Countries. Javed, Ali, et al. 2012,
Advanced Computing: An International Journal (ACIJ, pp. 17–28).
2. A Research Study on importance of Testing and Quality Assurance in Software Development
Life Cycle (SDLC) Models. ManeelaTuteja, Gaurav Dubey. 2012, International Journal of
Soft Computing and Engineering (IJSCE), pp. 251–257.
3. Infosys. presentation tier performance optimization. White paper. 2015.
4. Gomez. Why Web Performance Matters: Is Your Site Driving Customers Away? White
Paper. 2010.
5. Microsoft. Performance Testing Guidance for Web Applications. s.l.: PHI, 2009.
6. Software Performance Testing Measures. Patel, C. and Gulati, R. 2014, International Journal
of Management & Information Technology, pp. Vol. 8, No. 2.
7. How to Improve Software Quality Assurance in Developing Countries. Javed, Ali, et al. 2012,
Advanced Computing: An International Journal (ACIJ, pp. 17–28).
8. Identifying Ideal Values of Parameters for Software Performance Testing. Patel, C and Gulati,
R. 2015, IEEE Xplore.
12
C. Patel and R. Gulati

9. “Regression
and
correlation
analysis”
[online]
Available:
http://abyss.uoregon.edu/js/
glossary/correlation.html.
10. Viscomi, Rick, Davies, Andy and Duran, Marcel. USING WEBPAGETEST WEB
PERFORMANCE TESTING FOR NOVICES AND POWER USERS. s.l.: O’REILLY,
2015.
11. Web Application Performance Analysis Based on Component Load Testing. Babbar, Charu,
Bajpai, Neha and Sarmah, Dipti Kapoor. 2011. INDIACom-2011. pp. ISSN 0973-7529 ISBN
978-93-80544-00-7.
12. Performance solutions: a practical guide to creating responsive, scalable software. C.U., Smith
and L.G., Williams. 2002. Boston, MA, Addison Wesley.
13. Infosys. presentation tier performance optimization. White paper. 2015.
14. AgileLoad.
Application
Performance
Testing
Basics.
http://www.agileload.com/.
[Online]
09
11
2012.
http://www.agileload.com/agileload/blog/2012/11/09/application-
performance-testing-basics.
WebN: A Strainer Approach for Website Performance Analysis
13

A Phase-wise Review of Software
Security Metrics
Syed Anas Ansar, Alka and Raees Ahmad Khan
Abstract Integrating security at each phase of the software Development Life
cycle (SDLC) has become an urgent need. Moreover, security must not be over-
looked at early phases of SDLC. This helps to minimize cost and efforts for later
phase of the life cycle. In addition, software security metrics are the tools to judge
level of security of software. Without the use of the metrics, no one can ensure the
usefulness of any approach which claims to improve security of the software. The
paper presents a phase-wise review of security metrics and the issues in their
adaptation. Though there are security metrics available for each phase of the
software development life cycle, their usefulness in the software industry or in
research is in question without their validation. In addition, a concrete research is
needed to develop security metrics at early phases of software development life
cycle.
Keywords Software security ⋅Security metrics ⋅Software development life
cycle
1
Introduction
Security refers to ensuring conﬁdentiality, integrity, authenticity, availability,
non-repudiation etc. [1]. It protects the system from unauthorized use, access,
disclosure, and modiﬁcation [2]. Nowadays news headlines are frightening us about
data and information theft. This raises question about the status of security of data
S.A. Ansar (✉) ⋅Alka ⋅R.A. Khan
Department of IT, Babasaheb Bhimrao Ambedkar University,
Vidya Vihar, Raebareli Road, Lucknow 226025, India
e-mail: syed000anas@gmail.com
Alka
e-mail: alka_cs@jmuyahoo.co.in
R.A. Khan
e-mail: khanraees@yahoo.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_2
15

stored,
processed
through
computers,
and
shared
through
Internet.
The
hackers/attackers cannot only be blamed for these incidences, designers and
developers of the software are equally responsible. Even after so many life
threatening security incidents, it is still treated as an afterthought while developing
the software. Security features are often sprayed on the completely developed
software [1, 3]. As a result, ensuring software security has become a battle.
A hacker tries to ﬁnd and exploit security holes present in the software. He does not
create security holes on his own. Hence, presence of even single security hole may
be responsible to exploit the software’s security completely. The irony is that the
security practitioners can never be conﬁdent that they have found and patched all
the security holes. Hence, security has become a great challenge.
Instead of spraying security features on the security holes found during pene-
tration, security must be addressed during each phase of software development life
cycle (SDLC) [1]. Addressing security at each phase of SDLC is termed as software
security. Hence, it is the idea of developing software so that it can provide the
required function even when it is attacked [3]. In addition, while improving security
of the software under development, it is very important but difﬁcult to judge the
level of security. Now, the role of security metrics comes into the picture. Without
the help of the good security metrics and systematic approaches, it is difﬁcult to
assure security level of software. Metrics is a measurement standard which deﬁnes
what is to be measured and how and helps the security practitioners to manage the
product efﬁciently [4]. Security metrics is the powerful tool that helps security
practitioners to integrate security features into their system [5–7]. Nowadays met-
rics are gaining much consideration because with the help of the data obtained from
them decision can be taken accordingly. They assist practitioners to meet their goal
for secure software development [1, 7, 8]. Rest of the paper is organized as: next
section discusses about the related work in the area. Section 3 presents phase-wise
systematic review of security metrics. Section 4 discusses about major ﬁndings, and
ﬁnally, the paper is concluded at Sect. 5.
2
Related Work
The research in the area of software security has been going on worldwide. Fol-
lowing is some pertinent research work in the area. B. M. Alshammari et al.
(2016) have reviewed different procedures for building more secure systems. They
have ﬁrstly reviewed various design principles. The authors have also explained
some of the existing work on software quality including software security metrics
and have also compared different security metrics for building secure systems.
Finally, they have discussed about refactoring and its inﬂuence on security with the
demonstration showing that the refactoring is used for enhancing the quality of
program. In addition, they have suggested that these all can be used as components
of secure system architecture. This study may be used as guidance for building and
recognizing secure software [9].
16
S.A. Ansar et al.

D-E. Lim and T-S. Kim (2014) have modelled the discovery and removal of
software vulnerability based on queuing theory. Vulnerability has been classiﬁed
into groups on the basis of their severity calculated by Common Vulnerability
Scoring System (CVSS). In this approach, three parameters have been used. For
each class, the queuing model has been used for obtaining waiting time and number
of vulnerabilities present in a queue. They applied the Takagi equation for obtaining
average waiting time of an arbitrary vulnerability, and then, they ﬁnally applied
Little’s Law to Takagi equation for obtaining the number of unﬁxed vulnerabilities.
The system risk is measured by counting the number of unﬁxed vulnerabilities [10].
A. A. Abdulrazeg et al. (2012) have developed security metrics to improve misuse
case model for discovering and ﬁxing defects and vulnerabilities. The proposed
security metrics indicates the possibility of security defects. The metrics have been
developed using goal question metric approach (GQM). The presented metrics
consist of two main goals. For achieving ﬁrst and second goal, they have developed
security metrics on the basis of anti-pattern and web application security risk
OWSAP top 10-2010 respectively. The proposed work is signiﬁcant to remove
modelling defects and to improve security use cases before these defects move to
the next stage of development life cycle [11].
S. Islam and P. Falcarin (2011) have identiﬁed security requirements for mea-
suring software security goals with the help of risk management process. They
evaluated these software security requirements for measuring software security on
the basis of GQM approach. GQM approach is a procedure that confers a frame-
work for deﬁning and explaining metrics. The approach includes: purpose (Why),
object, issue, perspective, viewpoint, environment (context), and when. They par-
tially followed security quality requirement engineering (SQUARE) methodology
for identifying requirements. SQUARE methodology provides a means of eliciting,
classifying, and emphasizing security requirements for information technology
system and application. They followed ISO 17799:2005 standard as a baseline for
developing the metrics [2]. H. C. Joh and Y. K. Malaiya (2010) proposed a
framework in which they combine stochastic model (as vulnerability life cycle) and
Common Vulnerability Scoring System (CVSS) metrics for the evaluation of risk.
They deﬁned risk from the point of view of the vulnerability life cycle, considering
the probabilities, exploitation of software vulnerability in a system, and the impact
of its exploitation. They have ﬁrst considered the evaluation of the risk induced by a
single vulnerability. They have also generalized the approach to include all the
potential vulnerabilities in software [12].
R. M. Savola (2009) introduced a modern method for the development of
security metrics based on threat, security requirements, and decomposition of
security goals. In the proposed method, he has used some process for security
metric development. Firstly, threat and vulnerability analysis has been carried out,
and then, prioritized security requirements have been declared. By identifying basic
measurable components (BMC), the author has generated measurement architecture
A Phase-wise Review of Software Security Metrics
17

and had selected the BMCs to be used as the basis for detailed metrics. Finally, he
validated the security metrics. The core activity of the proposed method is to
decompose the security requirements [13]. J. A. Wang et al. (2009) developed
security metrics on the basis of representative weaknesses of the software [14].
M. A. Hadvi et al. (2008) proposed a method for early mitigation of software
vulnerabilities for the secure software development. They have selected the most
common 23 vulnerabilities and deﬁned them. They have analyzed reasons for their
presence in a phase. They provided countermeasures for their avoidance and mit-
igation through design level activities (13 design activities) as well as implemen-
tation level activities (19 activities). They ﬁnally mapped these vulnerabilities to the
given activities. The mapping would mitigate the speciﬁc vulnerabilities and would
provide a better insight of introduction of vulnerabilities. This may help security
practitioners to develop secure software [15].
Shirley C. Payne (2007) has worked on developing a security metrics program
and has proposed a seven-step methodology that may guide development of simple
metrics programs. They have advised managers to take help from existing easy,
cheap, and fast measures. The important thing they have concluded is that the
metrics generated should be useful enough to make advancement in the overall
security program. The purpose of this guide is to provide an overview of the current
state of security metrics [16]. O.H. Alhazmi et al. (2007) have proposed a new
metric called vulnerability density. It can be used to compare the software systems.
They have deﬁned vulnerability density metric as the number of vulnerabilities per
unit size of code. By using vulnerability density as a parent metric, they have
coined a set of metrics called as known vulnerability density, residual vulnerability
density. They deﬁned known vulnerability density as the number of known vul-
nerabilities in the unit size of code and the residual vulnerability density metric as
the vulnerabilities density minus the known vulnerabilities density [17].
3
Phase-wise Security Metrics
The ﬁeld of security metric is comparatively new [18]. Many software industries,
researchers, and practitioners have developed security metrics. These metrics are
related to different phases of the software development life cycle. This paper pre-
sents a systematic review of the security metrics available for different phases of
software development life cycle. The research has performed phase-wise review of
available security metrics. The security metrics for requirement Phase, design
phase, coding/implementation phase, testing phase, and maintenance phase are
presented in the Tables 1, 2, 3, 4, and 5, respectively.
18
S.A. Ansar et al.

Table 1 Security metrics for requirement phase
Security metrics
Deﬁnition/purpose
Security Requirements
Recorded Deviations (SRRD)
This metric is used to provide the number of deviations
from security requirements [19]
Security Requirements stage
Security Errors (SRSE)
It provides the number of security errors that are the result
of incomplete or incorrect security requirements [19]
Security Requirements
gathering Indicators (SRI)
It provides indicators on requirements gathering and
analysis phase, which explains the impact of security
requirements on the number of security breaches/violations
[19]
Total number of security
requirement (Nsr)
It aims to measure the number of security requirements
identiﬁed/found during analysis phase [20]
Ratio of security requirements
(Rsr)
This provides the ratio of requirement which has direct
impact on security to the total number of requirement.
Rsr = SR
j
j
R
j j
Where SR is the set of security requirement and R is the set
of all the requirement of the system [20]
Number of omitted security
requirements (Nosr)
It measures the number of security requirement that has
been not considered during the analysis phase [20]
Table 2 Security metrics for design phase
Security metrics
Deﬁnition/purpose
Vulnerable Association of an
Object Oriented Design(VA_OOD)
It is calculated as the ratio of summation of vulnerable
association of each class to the total number of vulnerable
classes in the design [21]
VA OOD =
∑VulnerableAssociation of Each Class
Total Number of VulnerableClasses in the Design
Security Requirements Statistics (SRs)
If NSRD is number of security requirements considered for
design, then SRs is given as
SRs = NSRD
NSRG
Where NSRG is the number of security requirement
gathered in that design [19]
Number of Design stage Security
Errors (NDSE)
It calculates the number of security errors due to the design
stage [19]
Composite-Part Critical Classes
(CPCC)
CPCCðDÞ = 1 −
CP
CC




Where CC is critical classes in design D and CP is
composed-part critical classes in the same design [22]
Critical Class Coupling (CCC)
It is calculated as the ratio of the number of all classes’
linked with classiﬁed attributes to the total number of
possible links with classiﬁed attributes in a given design
[22].
CCCðDÞ =
∑ca
j = 1 αðCAjÞ
ð C
j j −1Þ × CA
j
j
Critical Class Extensibility (CCE)
It can be calculated as the ratio of the number of
non-ﬁnalized classes in a design to the critical classes in that
design [22].
CCEðDÞ = ECC
j
j
CC
j
j
(continued)
A Phase-wise Review of Software Security Metrics
19

Table 2 (continued)
Security metrics
Deﬁnition/purpose
Classiﬁed Methods Extensibility
(CME)
It is the ratio of the number of non-ﬁnalized classiﬁed
method to the total number of classiﬁed methods in a design
[22].
CMEðDÞ = ECM
j
j
CM
j
j
Critical Super-classes Proportion
(CSP)
It measures the ratio of the number of critical super classes
to the total number of critical classes in an inheritance
hierarchy [22].
CSPðHÞ = CSC
j
j
CC
j
j
Classiﬁed Methods Inheritance (CMI)
This aims to measure the ratio of the number of classiﬁed
methods which can be inherited in a hierarchy to the total
number of classiﬁed methods in that hierarchy [22].
CMIðHÞ =
MI
j
j
CM
j
j
Critical Design Proportion (CDP)
It is calculated as the ratio of the number of critical classes to
the total number of classes in a design [22].
CDPðDÞ = CC
j
j
C
j j
Coupling Induced Vulnerability
Propagation Factor(CIVPF)
It is deﬁned as the summation of induced vulnerability
propagation from a root vulnerable class to others in a
design to the total number of classes in that design.
CIVPF =
∑p
i = 1 Li
N
Where N is the total number of classes, and Li (i=1,…..p) is
the total number of coupling induced vulnerability
propagation from a root vulnerable class Ci to the others
[23]
Classiﬁed Instance Data Accessibility
(CIDA)
It is the ratio of the number of classiﬁed instance public
attributes to the total number of classiﬁed attributes in a class
[24].
CIDAðCÞ = CIPA
j
j
CA
j
j
Classiﬁed Class Data Accessibility
(CCDA)
It is computed as the ratio of the number of the classiﬁed
class public attributes to the number of classiﬁed attributes
in a class [24]
CCDAðCÞ = CCPA
j
j
CA
j
j
Classiﬁed Operational Accessibility
(COA)
It is deﬁned as the ratio of the number of classiﬁed public
methods to the number of classiﬁed method in a class [24].
COAðCÞ = CPM
j
j
CM
j
j
Classiﬁed Methods Weight (CMW)
It is calculated as the ratio of the number of classiﬁed
methods to the total number of methods in a given class
[24].
CMWðCÞ = CM
j
j
M
j
j
Number of design decisions related to
security (Ndd)
It aims to measure the number of design decisions that
describes security requirements of the system [20]
Ratio of design decisions (Rdd)
This can be computed as the ratio of design decisions related
to security to the total number of design decisions [20].
Rdd = Ndd
Nd
20
S.A. Ansar et al.

Table 3 Security metrics for coding/ implementation phase
Security metrics
Deﬁnition/purpose
Percent of Security Coding Aspects
(PSCA)
This indicates the percentage of security aspects
considered during coding according to design [19]
Percent use of Coding Standard (PCS)
It indicates the use of coding standards for secured
development and shall be supported in identifying
the consideration of security standards during code
implementation [19]
Number of Security Errors (NSE)
This metric is used to indicate the ﬂaws expressed
as the sum of coding errors and also the errors from
other library code [19]
Stall Ratio (SR)
This metric aims to measure the ratio of the number
of lines of non-progressive statements in the loop to
the total number of lines in a loop [25]
SR = Lines of non −progressive statements in a loop
Total lines in the loop
Coupling Corruption Propagation
(CCP)
It is deﬁned as the number of child methods called
with the parameter(s) that are based on the
parameter(s) of the original invocation [25]
Critical Element Ratio (CER)
This aims to provide the ratio of critical data
elements in an object to the total number of
elements in the object. It measures the ways a
program can be infected by malicious inputs [25].
CER =
Critica Data Elements in the Object
Total Number of Elements in the Object
Precision
It relates the true defective components to the total
number of components predicted as defective.
Precission =
TP
TP + FP
Where TP is true positive and FP is false positive
[26]
Recall
It is deﬁned as the ratio of true defective
components to the total number of defective
components.
Recall =
TP
TP + FN
Where TP is true positive and FN is false negative
[26]
F-measure
It is used to combine the precision and recall as a
harmonic mean [26].
F −measure = 2 × Recall × Precision
Recall + Precision
Accuracy
It is used to measure the overall accuracy of the
prediction [26].
Acc =
TP + TN
TP + TN + FP + FN
Ratio of implementation errors that
have direct impact on security(Rserr)
This provides the ratio of the number of errors that
have a direct impact on security to the total number
of errors in the implementation of the security [20].
Rserr = Nserr
Nerr
A Phase-wise Review of Software Security Metrics
21

4
Major Findings
A literature survey of software security metrics indicates the immaturity of the area.
Following are some ﬁnding obtained during the survey:
• It is found that no comprehensive evaluation scheme is devised for these
metrics.
• Most of the metrics developed for the early phases of SDLC are vague in nature.
Hence, there is a need to develop security metrics for successful implementation
of security in those phases.
• Almost all the metrics have been implemented only on small data sets.
• There is need to conduct more experiments to extract concrete conclusion about
the implication of the proposed metric values.
Table 4 Security metrics for testing phase
Security metrics
Deﬁnition/purpose
Security Requirements
Considered for Testing (SRT)
It can be indicated by the ratio of the security requirement
tested, and the number of security requirement gathered
(NSRG) [19]
Process Effectiveness (PE)
It can be represented by the ratio of number of security
vulnerability discovered (NVD) to the number of modules
undergone security testing (MST) [19].
PE = NVD
MST
Security Testing Ratio (STR)
This indicates the ratio of modules undergone security
testing to the total number of modules [19].
STR = MST
M
Ratio of security test cases that
fail (Rtcp)
This aims to provide the ratio of numbers of test cases that
fails to detect implementation errors to the number of test
cases specially designed to detect the security issues [20].
Rtcp =
TF
j
j
TP
j
j + TF
j
j
Table 5 Security metrics for maintenance phase
Security metrics
Deﬁnition/purpose
Mean Time to Complete Security
Changes (MTCSC)
It is estimated by the number of security failures and
mean time taken to repair the ﬂaws [19].
MTCSC = MTTSF + MTTR
Percent of Changes with Security
Exceptions (PCSE)
It is measured by the ratio of counts of completed
changes with security exceptions and completed
changes multiplied by 100 [19]
Ratio of patches issued to address
security vulnerability (Rp)
This aims to provide the ratio of a number of patches
that are released to address security vulnerability to the
total number of patches of the system [20].
Rp = Nsp
Np
Number of security incidents
reported (Nsr)
It aims to measure the number of incidents that are
concerned with security [20]
22
S.A. Ansar et al.

• Proper validation of most of these metrics has not been done.
• There is no real-life implementation of these metrics.
• There is a need to examine the role of CASE tools while using these metrics.
• Proper evaluation of usefulness of these security metrics is also required.
• There is a need to develop a security metric development framework for uniﬁed
development and evaluation of metrics.
In SDLC, design phase is the most convenient phase for consolidating security
decisions. Unfortunately, there is no efﬁcient methodology or tool exists to address
security issues at this phase. Almost negligible work has been reported to address
security at this phase. So there is need to develop an appropriate framework for
metric development at this phase. The framework may assist in developing and
validating security metric.
5
Conclusion
Rapid development in issues related to security has facilitated the development of
security metrics. Today, software security measurements have become a genuine
demand in software industry. Metric is a measurement standard which deﬁnes what
is to measure and how and helps the security practitioners to manage product
efﬁciently. It also provides quantitative as well as objective basis for security
assurance. The metrics are supposed to be the foundation of secure development of
software. In this paper, a number of software security metrics for different phases of
SDLC have been reviewed. The developers of the metrics have claimed that the
metrics not only allow the security practitioners to ensure security of the software,
but also indicate where the security issue or vulnerability occurs. The available
approaches also ensure security hole-free software design and coding. Overall,
actual usefulness of the metrics is always in question until it is validated or
implemented on industrial data. In absence of a pertinent ready to use framework
for development of security metrics to be used in early stage of software devel-
opment life cycle, there is need to develop a security development framework to
guide the development of a minimal set of the metrics or integrated metrics.
Acknowledgements This work is sponsored by UGC-MRP, New Delhi, India under F.
No. 43-391/ 2014 (SR)
References
1. McGraw, G.: “Software Security”:Building Security In. (Addison-Wesly, 2006)
2. Islam, S. Falcarin, P.: Measuring Security Requirements for Software Security. In 10th
International Conference on Cybernetic Intelligent Systems (CIS), ISBN 978-1-4673-0687-4,
DOI 10.1109/CIS.2011.6169137, pp. 70-75, IEEE, (2011)
A Phase-wise Review of Software Security Metrics
23

3. McGraw, G., Potter, B.: Software Security Testing [J]. IEEE Security & Privacy, 2(5):81–85,
(2004)
4. Herrmann, D.S.: Complete Guide to Security And Privacy Metrics. Auerbach Publications,
ISBN: 0-8493-5402-1. (2007)
5. Swanson, M., Bartol, N., Sabato, J., Hash, J., and Graffo, L.: Security Metrics Guide For
Information Technology Systems. NIST Special Publication 800–55, National Institute Of
Standards And Technology, (2003)
6. Chaula, J. A., Yngstrom, L., and Kowalski, S.: Security Metrics And Evolution Of
Information Systems Security. In Proc. of the 4th Annual Conference on Information Security
For South Africa, (2004)
7. Payne, S. C.: A guide To Security Metrics. (2001)
8. Goodman, P.: Software Metrics: Best Practices For Successful IT Management. (2004)
9. Alshammari, B., Fridge, C., Corney, D.: “Developing Secure System: A Comparative Study
of Existing Methodologies”. Lecture Notes on Software Engineering, vol.2, no.2, may 2016,
pp: 139–146, doi: 10.7763/LNSE.2016.V4.239
10. Lim, DE., Kim, TS.: Modelling Discovery and Removal of Security Vulnerabilities in
Software System Using Priority Queuing Models. Journal of Computer Virology and Hacking
Techniques, Springer, 10: 109–114,DOI 10.1007/s11416-014-0205-z, (2014)
11. Abdulrazeg, A. A., Norwani, N. Md., Basir, N.: Security Metrics to Improve Misuse Case
Model. International conference on Cyber Security, Cyber Warfare and Digital Forensic,
ISBN 978-1-4673-1425-1, Doi 10.1109/CyberSec.2012.6246129, pp. 94–99, IEEE, (2012)
12. Joh, HC., Malaiya, Y. K.: A Framework for Software Security Risk Evaluation Using the
Vulnerability Lifecycle And CVSS Metrics. Proc. International Workshop on Risk and Trust
in Extended Enterprises, pp. 430–434 (2010)
13. Savola, R. M.: A security Metrics Development Method for Software Intensive Systems.
Advances in Information Security and its Application, Communications in Computer and
Information Science, 2009, Volume 36, pp. 11-16,Springer, (2009)
14. Wang, J. A., Wang, H., Guo, M., Xia, M.: Security Metrics for Software Systems. In the Proc.
Of ACMSE, March 19–21, Clemson, SC, USA, (2009)
15. Hadvi, M. A., Sangchi, H. M., Hamishagi, V. S., Shirazi, H.: Software Security; A
Vulnerability-Activity Revisit. Third International conference on Availability, Reliability, and
Security, ISBN 978-0-7695-3102-1, Doi10.1109/ARES.2008.200 IEEE, (2008)
16. Payne, S. C.: “A Guide to Security Metrics”. SANS Institute 2007. Available at: www.sans.
org/reading_room/whitepapers/auditing/55.php. Last visit Aug. 22 2016.
17. Alhazmi, O. H., Malaiya, Y. K., Ray, I.: Measuring, Analysing, and Predicting Security
Vulnerabilities in Software Systems. Computers and Security Journals, pp. 219–228, (2007)
18. Manadhata, P. K and Wing, J. M.: An Attack Surface Metric. Technical Report. School of
Computer Science, Carnegie Mellon University (CMU). CMU-CS-05-155, (2005)
19. Jain, S., Ingle, M.: Security Metrics and Software Development Progression. Journal of
Engineering Research and Applications, ISSN: 2248–9622, Vol. 4, Issue 5 (Version 7),
pp. 161–167, (2014)
20. Sultan, K., En-Nouaary, A., H-Lhadj, A.: Catalog for Assessing Risks of Software
Throughout the Software Development Life Cycle. In the Proc. of International Conference
on Information Security and Assurance, pp. 461–465, IEEE, (2008)
21. Agarwal, A., Khan, R. A.: Assessing Impact of Cohesion on Security- An object Oriented
Design Perspective. vol 76, No. 2, pp. 144–155, Pensee Journal, (2014)
22. Alshammari, B., Fridge, C., Corney, D.: Security Metrics for Object-Oriented Designs. Proc.
21st Australian software Engineering Conference, IEEE Press, pp. 55–64, Doi:ieeecomput-
ersociety.org/10.1109/ASWE(2010)
23. Agarwal, A., Khan, R. A.: Role of Coupling in Vulnerability Propagation Object Oriented
Design Perspective. Software Engineering: An International Journal (SEIJ), Vol. 2, No. 1,
pp. 60–68, (2012)
24
S.A. Ansar et al.

24. Alshammari, B., Fridge, C., Corney, D.: Security Metrics for Object-Oriented Class Designs.
In proceedings of the Ninth International Conference on Quality software (QSIC), IEEE,
(2009)
25. Chowdhury, I., Chan, B., Zulkerine, M.: Security Metrics for Source Code Structures. In
Proceedings of the Fourth International Workshop on Software Engineering For Secure
Systems, ACM, pp. 57–64. (2008)
26. Nguyen, V. H., Tran, L.M.S.: Predicting Vulnerable Software Components with Dependency
Graphs.In Proceedings of the 6th International Workshop on Security Measurements and
Metrics, ISBN: 978-1-4503-0340-8, Doi: 10.1145/1853919.1853923, (2010)
A Phase-wise Review of Software Security Metrics
25

A Survey on Code Clone, Its Behavior
and Applications
Aakanshi Gupta and Bharti Suri
Abstract Code Clones are separate fragments of code that are very similar to a
piece of code in matter or in its functionality. It is a type of Bad Smell that increases
the project size and maintenance cost. However, the existing research elaborates
several detection techniques. But the data from the available research is still
insufﬁcient to reach at some conclusion. The aim of this survey is to investigate all
detection techniques and to analyze the Code Clone behavior and motivation
behind the cloning. In this paper, 16 techniques for detecting the clones are sum-
marized. The paper presents detailed analysis of 76 research papers. The research
identiﬁed that there are various tools that are available for detecting Code Clones.
We also investigate the approaches followed in the tools and further summarized
the Code Clone patterns that are used for qualitative analysis. Overall, our ﬁndings
indicate that the management of Clones should be started at the earliest.
Keywords Code Clone ⋅Bad Smell ⋅Patterns of cloning
Case study ⋅Detection technique
1
Introduction
Software design defects can be classiﬁed into two parts [1]: Anti-patterns and Bad
Smells. Both are bad programming practices. The implementation of these impedes
the design problems.
Bad Smells are coined by Fowler and Beck [2]. They listed 22 Bad Smells. Code
Clone Bad Smell is the most studied of all the Bad Smells. Code Clone means the
replication of the code fragments. This analysis is based on 77 papers which have
led to the identiﬁcation of 16 approaches for the detection of Code Clones.
A. Gupta (✉) ⋅B. Suri
University School of ICT, GGS Indraprastha University, Delhi, India
e-mail: aakankshi@gmail.com
B. Suri
e-mail: bhartisuri@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_3
27

In this study, the research papers are classiﬁed with respect to their detection
techniques, research approaches, and case studies. The survey includes the last ten
years research papers. This survey excludes the secondary studies and grey
literature.
1.1
Threats to Validity
The main threats to the validity of the review are identiﬁed: The exclusion of
conference papers and reports are based on the practical implementation. Another
possibility is that the recommendations are affected by our interest and opinions like
to emphasize on the detection techniques, tools, and case studies that affect the
maintenance of the software systems. Also the existence of grey literature such as
PhD thesis and technical reports are also not included. The paper is organized as
follows: Sect. 2 includes the background and classiﬁcation of the Code Clones,
Sect. 3 contains the Code Clone detection approaches and tools, and Sect. 4
analyses the results and the discussion.
2
Background
Webster’s dictionary [3] deﬁnes the term Clone as ‘to create an identical copy of an
original.’ Code Clone in technical term is deﬁned as repeated sequence of code
fragment or similar code fragments in another part of program module. The
requirement for cloning is very well explained in [4–7]. Some of the reasons are as
follows:
• Development of initial software system.
• The expansion phase to satisfy the new requirements in the software.
• LOC-based performance appraisals.
• Redesigning the system is costlier approach. Maintenance beneﬁts.
Cloning has negative impact on maintenance process. The cloning practice led to
introduction of bugs which execution results into fault [8]. To ﬁnd the reasons for
occurrence of the Clone in a software system, it is necessary to ﬁnd the location of
the occurrence. The clone region descriptor locates the region of cloned code with
the help of syntactic, structural, and lexical information [6]. Duplicate code (Code
Clone) does not occur automatically; there are external factors responsible for
forcing the cloning of code.
Tairas et al. [9] represent clone groups in a localized manner with CeDAR. Each
clone is represented as an abstract syntax tree. Then, the sufﬁx tree is used to trace
similarities and differences.
28
A. Gupta and B. Suri

Code Clone classiﬁcation is based upon their resemblance; textually or func-
tionally. Textual clones similarities in clones are simply text-based where two or
more code fragments are identiﬁed to be similar in terms of their syntax or
expression. On the other hand, the functional clones are clones where two or more
code fragments are similar in terms of their functionalities or semantics. For
example: the factorial program written by one coder using iterative function and by
other coder using for-loop and if condition. Both the functions are functionally
performing the same logic. Hence, we can say that the functional clones are logi-
cally similar. Textual clones are further categorized as type I, II, and III clones and
functional clones as type IV. All these types of Clones [4] discussed below in brief:
• Type-I: copied code fragment is exactly similar but have small variation in
comments and layouts.
• Type-II: the code is syntactically similar but has a small variation in identiﬁers.
• Type-III: the modiﬁcation is done by adding and deleting statements.
• Type-IV: when both code fragments offer the same logics.
Various experimental patterns for the occurrence of clone in the software have
been observed [10] as shown in the Fig. 1. Pattern of cloning is deﬁned: what is
duplicated in the code, why it is duplicated, and how it is done.
The motivation behind cloning is the term ‘Pattern.’ Pattern is a generic term and
it is used to deﬁne what is to be cloned. Realism of the patterns is expressed by the
occurrence of clones in coding practice with differing [10] sequence, functionalities,
and reasons. Patterns of cloning determine qualitative characterization of code
cloning in the software. It gives more explanatory view of Code Cloning.
Fig. 1 Patterns of cloning
A Survey on Code Clone, Its Behavior and Applications
29

Maintainability and extendibility both qualities of the software can also be guided
by the patterns of cloning. It also arbitrates whether the clones are harmful or
useful. It crystallizes a vocabulary for cloning that can be used in communication.
Based on ﬁndings and understanding, the three research questions with moti-
vation are described in Table 1.
3
Code Clone Detection Techniques and Tools
Problem of maintenance and management of clones in a software system can be
handled using the clone detection techniques via their automated tools. Detection
means ﬁnding the answer for what is cloned, why it is cloned, and how it has
affected the software quality. A well-deﬁned and well-structured detection process
is followed by every detection approach [4].
The detection process of Code Clone is a two-phase process [4]: (1) the fragments
are transformed into an internal structure according to the technique being used and
(2) the code fragments that are similar are then classiﬁed into a clone group.
For ﬁnding out the effects of these clones [11], identify the source of the clone
and its type (as mentioned in Sect. 2). Detection techniques can be differentiated on
the basis of code normalization, its representation, the granularity of comparison,
comparing algorithms for code snippets, and compatibility with the language. Some
important detection techniques/approaches and tools are explained below. Various
tools with their respective techniques are summarized in Table 2.
3.1
Token-Based Approach [12–14]
It is an approach which initially converts token (extracted from the source) in the
form of sequence and then represent these tokens in a form of sufﬁx tree or sufﬁx
array for comparison. Various tools for clone detection fall in this category such as
CCFinder [15], CP-Miner [16], Boreas [17]. CCFinder is an efﬁcient and optimized
tool used to ﬁnd clone in C++, COBOL, JAVA, and other language source ﬁles. It
transforms the input source into tokens and performs the comparison token by
Table 1 Research questions with motivation
Research questions
Motivation
RQ1. What are the various perspectives of the authors
working in this area about Code Clones (against or favor)?
Identify the good or bad
behavior of Code Clone
RQ2. How can we compare various existing Clone detection
approaches
To compare various Clone
detection approaches
RQ3. Are there any existing case studies that detect
Cloning?
Identify case studies to detect
the Code Clones
30
A. Gupta and B. Suri

token. Research [16, 18, 19] has shown that copy and paste methodology is prone
to bugs, and most of the large software including operating systems are prone to
bugs due to the presence of signiﬁcant amount of copy and paste source code.
It has been a difﬁcult task to identify the ‘copy paste code’ for existing clone
detection tool CCFinder. To overcome such limitation, CP-Miner tool was pro-
posed in the year 2006. It makes use of data mining techniques to identify those
segments and the bug associated with it. Another scalable and efﬁcient token-based
tool is Boreas which introduces a novel counting-based method to deﬁne the
characteristic metrics which are able to describe the program segments distinctly
and effectively for the purpose of clone detection.
Table 2 Various tools and their related techniques
Tool Name
Technique used
Related papers
Datrix
Syntax based
[33]
CCFinder
Token-based approach
[34]
Gemini
CCFinder
[35]
Clone miner
Token based
[14]
Aries
CCShaper, CCFinder
[36]
CCFinderX
Token based
[21, 37–39]
CP-Miner
Token based
[16]
SmallDude
Text based
[19]
Columbus
Syntax based
[40]
SimScan
Syntax based
[20, 36, 41, 42]
Simian
Text based
[42],
Deckard
Tree based
[8, 42]
CloneTracker
Relies on abstract clone region descriptor
[43]
Shinobi
Token-based clone detection engine
[12]
CloneDetective
ConQAT (Contains quality control)
[44]
Clever
Syntax based
[24]
CloneInspector
Token based
[45]
ClemanX
Syntax based
[46]
Iclone
Token based
[47]
NiCad
Parser based
[48]
Bauhaus ccdiml
Syntax based
[20]
CtCompare
Tokenization approach
[13]
SeByte
Semantic based
[49]
Boreas
Token based
[24]
SimCad
Not mentioned
[50]
Agec
Execution model
[15]
Duplix
Graph based
[26]
Covet
Metrics based
[51]
A Survey on Code Clone, Its Behavior and Applications
31

3.2
Tree-Based Approach [20, 21]
In this approach, clones are found by matching subtree. Source code is transformed
into AST (Abstract Syntax Tree) and parse tree. In this category, tools such as
Ccdiml [22], SimScan [23], ClemanX [24], Deckard [25], and many more are
available. The language independent tree-based tool is Deckard. It is based on the
novel characteristic of tree as vector. ClemanX [24] tool is based upon incremental
AST detection which detects clone efﬁciently and incrementally.
3.3
Graph-Based Approach [8, 26, 27]
PDG (Program dependency graph)-based detection is a graph-based approach. It is
a representation of source code in a form of control ﬂow and data ﬂow. In this
approach, the similar sub graphs are identiﬁed from the source code [28].
3.4
Metrics-Based Approach [29]
Metrics are evaluated from the source code in order to compare the similarities.
Metrics-based data mining approach is an efﬁcient detection approach which
extracts the metrics for the function of software and then divides the software into
the clusters by using some data mining algorithm [30].
3.5
Hybrid Clone Detection Approach
It is a combination of syntactic and semantic-based approaches. Hybrid approach
uses AST metrics and graphs in combination with specialized comparison functions
[31]. It is used to overcome the limitation of Boreas tools to ﬁnd the clone type-3 [12].
4
Result and Discussion
The fact that clones are very frequent in the software has motivated us to summarize
the existing work for further investigation and knowing the gaps. Some of the
important clone detection tools with their techniques are discussed in Tables 3 and
4. There are also some applications that are being implemented with these tools and
techniques. There are two kinds of behavior with respect to the Code Clones; some
32
A. Gupta and B. Suri

Table 3 Various detection approaches with identiﬁed clones
Detection
approach
Technique in brief
Percentage of clone
detected
Resultant clone
identiﬁed
Ref.
Language
independent
Based on string matching,
visualization of duplicacy,
synthesized data by textual
report
gcc8.7%, DB-6.4,
payroll-59.3%, msg
board-29.4%
Duplicacy found,
same ﬁle,
evolution ﬁles
[52]
Abstract
syntax tree
Parsing, generating AST and
apply algo1-subtree clone
detection, algo 2-subtree
sequence detection, algo 3
near-miss clones
This method
conﬁrms estimation
of clone density up
to 7–15%
For near-miss
detection, detect
sequence clones
on scale
[53]
Abstract
syntax sufﬁx
tree
Generation AST with parsing,
serializing AST and detect sufﬁx
tree, deduce result
AST tools has
better precision for
type-2 clones
Type-2 clones
detected
[54]
Data mining
Frequent closed item set mining
(FCIM)
N.M.
Structural and
higher
Tree
kernel-based
approach
Syntactic info enrich by lexical
elements compared with AST
N.M.
Type-1, type-2,
type-3
[55]
Textual
analysis an
metrics
Integrating and parsing,
island-driven parsing, type-1,
type-2 detection by metrics
Lightweight tech
for functional clone
detection
Type-4, type-3
clones detected
[56]
SeClone
(hybrid app)
Preprocessing, indexing,
searching and postprocessing
N.M.
N.M.
[57]
PDG-base
Analyzing and detecting
N.M.
Type-4
[35]
SimHash
based
Preprocessing, extraction,
transformation, normalization.
Clones are detected as a clone
clusters
N.M.
Near-miss clones
(type-2, type-3)
(type-1)
[58]
Token based
Matches variable for comparing.
uses counting-based
characteristic matrix to the
patterns of variables.
Measure the clone
quantity by
counting LOC.
N.M.
[17]
Multi-language
detection
LSC miner (large source code)
tokenize, hashing and detects
clones
Finding clone in
large code, no
language
dependency
N.M.
[8]
Hash token
sequence
Hash tokenization, hashing,
identiﬁcation of clones with
hashing
N.M.
Type-1 and
type-2 clone
[13]
Untitled
approach
AST generation, tokenization,
and generation of sufﬁx tree
N.M.
Not depends on
threshold value
[59]
Semantic clone
Formal grammar used for
semantic clones
N.M.
Semantic clones
[60]
Untitled
Sufﬁx array algo i.e., search
repeated token sub-string
False positive ratio
is 2.5%
Type-1 type-2
clone
[61]
BinClone
clone in
malwares
Parsing in assembly ﬁles,
partition in region of clone,
combined clone region, clone is
stored in DB
N.M
*N.M-Not
Mentioned
Exact or
near-miss clones
[59]
A Survey on Code Clone, Its Behavior and Applications
33

are in the favor or others in against. The cloning patterns are helpful for the software
practitioners to take the decision during maintenance.
Table 4 Case studies for detection of cloning
Case study
S/S features
Technique used
Tool
Effect/Result
REF.
Standard
template
library
(STL)
A general purpose
library of algo and
data structure
N.M.
CCFinder
clones in container
class( clone
size>=30
token:1051, clone
size>=50
token:71)
[62]
Large
software
system
(Linux
kernel)
Added new
features to adapt in
a new platform
Clone mining
framework
CCFinder
N.M.
[25]
Open
source
business
applications
Objective centric,
multi-tier
containing: user
interface, business
logic and database
AST-based
approach in
business logic
tier and
token-based
approach in UI
CloneDR
tool for
business
logic,
CCFinder
in UI tier
N.M.
[63]
SCSI
(small
computer
system
interface)
Provide support to
peripheral devices
Token-based
incremental
clone detection
iClone
N.M.
[64]
ASP.NET
and ASP.
NET MVC
framework
ASP.NET is a
event-driven
model build
dynamic was
while ASP.net
pattern-based way
to build dynamic
WA
AST-based
detection
CCFinder
Study shows that
which framework
in.Net technology
can be chosen to
avoid cloning in
development of
Web application
[65]
.Net
language
.Net framework is
a software
development
platform
SimHash-based
approach
Nicad and
SimCad
CIL, an
intermediate
representation is
exploited and
generated a ﬁlter
set which when
applied to CIL
[49]
Android
application
Provide market for
user, download
apps as Google
play
PDG-based
detection
AnDarwin
Finds the app
clones
[66]
34
A. Gupta and B. Suri

4.1
Research Questions
To proceed our investigation we speciﬁed three research questions. Identiﬁed
related studies and extract the data to answer these following questions:
RQ1. What are the various perspectives of the authors working in this area
about Code Clones (against or favor)?
Code cloning can be viewed as harmful or beneﬁcial for the software development.
To justify this statement, research has summarized cloning to be harmful as it
increases the cost of maintenance and on other hand cloning is done intentionally in
order to ease the development effort or reduce source code complexity. Thum-
malapenta [20] has explained in his study, the extent of the evolution of the clone
and consistent propagation of it; thereby it was concluded that late propagation of
evolved clones lead to bugs in the software. Empirical study formulated by Rahman
and Roy [8] showed the comparative evaluation of the stability of cloned and
noncloned code fragment and hence concluded that the Code Clones are less stable
than noncloned code; thereby, it challenges the maintenance process. There has
been a contradicting statement about the good and bad side of the cloning. An
empirical approach [32] has shown that cloning has nothing to do with a bug that
means cloned code are not buggy according to this study. Cloning is done to ease
the workload and reuse the existing code libraries and fragments.
RQ2. How can we compare various existing Clone detection approaches?
Detection techniques are compared based on their properties such as techniques
they followed and what type of Code Clone they identify. Various detection
approaches with the cloning percentage are given in Table 3. Here, the detection
approaches are listed with the technique implemented, percentage of the Code
Clone exist, and the type of Code Clone (as mentioned in Sect. 2). By analyzing
Table 3, it can be claimed that Tree Kernel-based approach is identifying Type-1,
Type-2, and Type-3 Clones whereas textual analysis approach is identifying Type-3
and Type-4 Clones. Textual analysis is also a lightweight technique for functional
Clone detection as compared to others in the table. In the similar manner, other
techniques can be compared on the behalf of cloning percentage and clone types
they identify.
RQ3. Are there any existing case studies that detect Cloning?
An analysis on case studies of the Code Clone is demonstrated in the Table 4. The
tabulated data explains the case study, feature of the system, technique used, tool
name, and its result. Like, in one case study, ‘STL’ is a general purpose library of
the algorithm and data structure. It is using CCFinder tool and ﬁnds the clones in
container class. Here, system feature and techniques are also mentioned in a case
study. ‘Open Source Business Applications’ has the multi-tier system (User Inter-
face and Business Logic), and it is using Abstract Syntax Tree-based approach for
business logic tier and token-based approach in User Interface. By understanding
A Survey on Code Clone, Its Behavior and Applications
35

the system features and used techniques, we can take the right decision about
choosing the appropriate tool for clone detection.
5
Conclusion
Code Clone, a type of Bad Smell is very popular and an active research area.
Harmfulness of the Code Clone is comparatively more than its usefulness as rec-
ommended in the literature. Instead of removing the cloning, the proper manage-
ment of the clones is necessary. This study suggests about the selection of particular
tool or the technique to identify the Code Clones. In the RQ1, we concluded that
there are situations when the cloning is harmful as well as some times beneﬁcial
also. RQ2 is comparing the various existing detection approaches of the Code
Clone. RQ3 is summarizing all the case studies in a nutshell. This survey also
represents the pictorial view of the patterns of cloning so that a researcher can be
effectively analyzed the cloning motivation. Patterns of cloning are basically for
qualitative characterization of code cloning in the software. The case studies and the
applications mentioned in the survey will further assist the researchers in matching
their problem with the existing ones.
Acknowledgement We would like to thank Prof. B.P. Singh, Prof. Rekha Agrawal and the
Amity School of Engineering and Technology, Delhi, for providing the good environment and
support in my research work.
References
1. Brown, W.J., Malveau, R.C., Brown, W.H., McCormick III, H.W., Mowbray, T.J.: Anti
Patterns: Refactoring Software, Architectures, and Projects in Crisis, 1st edn. John Wiley and
Sons, Chichester.
2. Flower, Martin. Improving the design of an existing code.
3. Bernadette Schell & Clemens Martin,: Webster’s New world Hackers dictionary 2006.
4. A Survey on Software Clone Detection Research. Chanchal Kumar Roy, James R. Cordy.
2007.
5. Clone Region Descriptors: Representing and Tracking Duplication in Source Code. Ekwa
Duala-Ekoko, Martin P. Robillard. 2010. 2010, ACM.
6. Cloning practices: Why developers clone and what can be changed. Gang Zhang, Xin Peng,
Zhenchang Xing, Wenyun Zhao. 2012. Trento: IEEE, 2012. pp. 285–294.
7. Cloning: The need to understand developer intent. Debarshi Chatterji, Jeffrey C. Carver
and Nicholas A. Kraft. 2013. San Francisco, CA: IEEE, 2013. pp. 14–15.
8. An Empirical Study of the Impacts of Clones in Software Maintenance. Manishankar
Mondal, Md. Saidur Rahman, Ripon K. Saha, Chanchal K. Roy, Jens Krinke, Kevin A.
Schneider. 2011. Kingston, ON: IEEE, 2011. pp. 242–245.
9. R. Tairas, F. Jacob, J. Gray, Representing clones in a localized manner, in: Proceedings of 5th
International Workshop on Software Clones, Honolulu, USA, 2011, pp. 54–60.
36
A. Gupta and B. Suri

10. “Cloning considered harmful” considered harmful: patterns of cloning in software. Cory J.
Kapser, Michael W. Godfrey. 2008. 2008, ACM, pp. 645–692.
11. Clone Detection: Why, What and How? Marat Akhin, Vladimir Itsykson. 2010. Moscow:
IEEE, 2010. pp. 36–42.
12. A hybrid-token and textual based approach to ﬁnd similar code segments. Akshat Agrawal,
SumitKumar Yadav. 2013. Tiruchengode:IEEE, 2013. pp. 1–4.
13. Ctcompare: Code Clone Detection Using Hashed Token Sequences. Toomey, Warren. 2012.
Zurich: IEEE, 2012. pp. 92–93.
14. Efﬁcient Token Based Clone Detection with Flexible Tokenization. Hamid Abdul Basit,
Simon J. Puglisi, William F. Smyth, Andrew Turpin, Stan Jarzabek. 2004. New York,
USA: ACM, 2004. pp. 513–516.
15. Agec: An execution-semantic clone detection tool. Kamiya, Toshihiro. 2013. San Francisco,
CA: IEEE, 2013. pp. 227–229.
16. CP-Miner: ﬁnding copy-paste and related bugs in large-scale software code. Li Z, Lu S,
Myagmar S, Zhou Y. 2006. s.l.: IEEE, 2006, pp. 176–192.
17. Boreas: An Accurate and Scalable Token-Based Approach to Code Clone Detection. Yang
Yuan, Yao Guo. 2012. New York, USA: ACM, 2012. pp. 286–289.
18. Ethnographic Study of Copy and Paste Programming Practices in OOPL. Kim, Miryung.
2004. s.l.: IEEE, 2004. pp. 83–92.
19. How developers copy. Balint M, Girba T, Marinescu R. 2006. Athens: IEEE, 2006.
pp. 56–68.
20. An empirical study of build maintenance effort. Thummalapenta S, Cerulo L, Aversano L,
Penta MD. 2011. s.l.: IEEE, 2011. pp. 141–150.
21. An empirical study on the maintenance of source code clones. Thummalapenta S, Cerulo L,
Aversano L, Penta MD. 2010. 2010, Springer, pp. 1–34.
22. Project Bauhaus. URL http://www.bauhaus-stuttgart.de Last accessed November 2008.
23. How clones are maintained: an empirical study. Aversano L, Cerulo L, Penta MD. 2007.
Amsterdam: IEEE, 2007. pp. 81–90.
24. ClemanX:Incremental Clone Detection Tool for Evolving Software. Tung Thanh Nguyen,
Hoan Anh Nguyen, Nam H. Pham, Jafar M. Al-Kofahi, Tien N. Nguyen. 2009.
Vancouver, BC: IEEE, 2009. pp. 437–438.
25. A Framework for Studying Clones In Large Software Systems. Zhen Ming Jiang, Ahmed E.
Hassan. 2007. Paris: IEEE, 2007. pp. 203–212.
26. A study of consistent and inconsistent changes of code clone. Krinke, Jens. 2007.
Washington, DC, USA: IEEE, 2007. pp. 170–178.
27. Is cloned code more stable than non-cloned code. J, Krinke. 2008. Beijing: IEEE, 2008.
pp. 57–66.
28. Incremental Code Clone Detection: A PDG-based Approach. Yoshiki Higo, Yasushi Ueda,
Minoru Nishino, Shinji Kusumoto. 2011. Limerick: IEEE, 2011. pp. 3–12.
29. Measuring Clone Based Reengineering Opportunities M. Balazinska, E. Merlo, M. Dagenais,
B. Lague and K. Kontogiannis,, in: Proceedings of the IEEE Symposium on S/W Metrics,
METRICS 1999, pp. 292–303 (1999).
30. A Metrics-Based Data Mining Approach for Software Clone Detection. Abd-El-Haﬁz, Salwa
K. 2012. Izmir: IEEE, 2012. pp. 35–41.
31. Detection of Redundant Code Using R2D2, A. Leit˜ao, Software Quality Journal, 12(4):
361–382 (2004).
32. An empirical study on the fault-proneness of clone migration in clone genealogies. Shuai Xie,
Foutse Khomh, Ying Zou. 2014. Antwerp: IEEE, 2014. pp. 94–103.
33. Assessing the beneﬁts of incorporating function clone detection in a development process.
Lague B, Proulx D, Mayrand J, Merlo E. 1997. Bari, Italy: IEEE, 1997. pp. 314–321.
34. CCFinder: a multilinguistic token-based code clone detection system for large scale source
code. Toshihiro Kamiya, Shinji Kusumoto, Katsuro Inoue. 2002. 2002, pp. 654–670.
A Survey on Code Clone, Its Behavior and Applications
37

35. Gemini: maintenance support environment based on code clone. Yasushi Ueda, Yoshiki
Higo, Toshihiro Kamiya, Shinji Kusumoto, Katsuro Inoue. 2002. s.l.: IEEE, 2002.
pp. 67–76.
36. ARIES:
REFACTORING
SUPPORT
ENVIRONMENT
BASED
ON
CODE
CLONE ANALYSIS. Yoshiki Higo, Toshihiro Kamiya, Shinji Kusumoto, Katsuro
Inoue. 2005. New York, NY, USA: ACM, 2005. pp. 1–4.
37. Relation of code clones and change couplings. Geiger R, Fluri B, Gall H, Pinzger M. 2006.
Berlin, Heidelberg: Springer, 2006. pp. 411–425.
38. Evolution of type-1 clones. N, Gode. 2009. Edmonton, AB: IEEE, 2009. pp. 77–86.
39. Assessing the effect of clones on changeability. Lozano A, Wermelinger M. 2008. Beijing:
IEEE, 2008. pp. 227–236.
40. Clone smells in software evolution. Bakota T, Ferenc R, Gyimothy T. 2007. Paris: IEEE,
2007. pp. 24–33.
41. An empirical study on inconsistent changes to code clones at release level. Bettenburg N,
Shang W, Ibrahim W, Adams B, Zou Y, Hassan A. 2009. Lille: IEEE, 2009. pp. 85–94.
42. Clone region descriptors: representing and tracking duplication in source code. Duala-Ekoko
E, Robillard M. 2010. 2010, ACM.
43. CloneTracker:tool support for code clone management. Ekwa Duala-Ekko, Martin P.
Robillard. 2008. Leipzig: IEEE, 2008. pp. 843–846.
44. CloneDetective–A Work bench for Clone Detection Research. Elmar Juergens, Florian
Deissenboeck, Benjamin Hummel. 2009. Vancouver, BC: IEEE, 2009. pp. 603–606.
45. Do code clones matter? Jürgens E, Deissenboeck F, Humme lB, Wagner S. 2009.
Vancouver, BC: IEEE, 2009. pp. 485–495.
46. ClemanX:IncrementalCloneDetectionToolforEvolvingSoftware.
Tung
Thanh
Nguyen,
Hoan Anh Nguyen, Nam H. Pham, Jafar M. Al-Kofahi, Tien N. Nguyen. 2009.
Vancouver, BC: IEEE, 2009. pp. 437–438.
47. Studying clone evolution using incremental clone detection. Göde N, Koschke R. 2010.
2010, Wiley.
48. The NiCad Clone Detector. James R Cordy, Chanchal K. Roy. 2011. Washington DC,
USA: IEEE, 2011. pp. 219–220.
49. Detecting Clones across Microsoft.NET Programming Languages. Farouq Al-omari, Iman
Keivanloo, Chanchal K. Roy, Juergen Rilling. 2012. Kingston, ON: IEEE, 2012.
pp. 405–414.
50. SimCad: An Extensible and Faster Clone Detection Tool for Large Scale Software Systems.
Md. Sharif Uddin, Chanchal K. Roy, Kevin A. Schneider. 2013. San Francisco, CA: IEEE,
2013. pp. 236–238.
51. Evaluating the harmfulness of cloning: a change based experiment. Lozano A, Wermelinger
M, Nuseibeh B. 2007. Minneapolis, MN: IEEE, 2007. p. 18.
52. A language independent approach for detecting duplicated code. Stephane Ducasse,
Matthias Rieger, Serge Demeyer. 1999. Oxford: IEEE, 1999. pp. 109–118.
53. Clone Detection Using Abstract Syntax Trees. Ira D. Baxter, Andrew Yahin, Leonardo
Moura, Marcelo Sant Anna, Lorraine Bier. 1998. Washington, DC, USA: IEEE, 1998.
54. Clone Detection Using Abstract Syntax Sufﬁx Trees. Rainer Koschke, Raimar Falke,
Pierre Frenzel. 2006. Benevento: IEEE, 2006. pp. 253–262.
55. A Tree Kernel based approach for clone detection. Anna Corazza, Sergio Di Martino,
Valerio Maggio, Giuseppe Scanniello. 2010. Timisoara: IEEE, 2010. pp. 1–5.
56. Detection of Type-1 and Type-2 Code Clones Using Textual Analysis and Metrics. KODHAI
E, KANMANI S, KAMATCHI A, RADHIKA R, VIJAYA SARANYA B. 2010. Kochi,
Kerala: IEEE, 2010. pp. 241–243.
57. SeByte: A semantic clone detection tool for intermediate languages. Iman Keivanloo,
Chanchal K. Roy, Juergen Rilling. 2012. Passau: IEEE, 2012. pp. 247–249.
58. BinClone: Detecting Code Clones in Malware. Mohammad Reza Farhadi, Information
Systems Engineering Concordia University Montreal, Benjamin C. M. Fung, Philippe
Charland. 2014. San Francisco, CA: IEEE, 2014. pp. 78–87.
38
A. Gupta and B. Suri

59. Chakraborty, Sanjeev. CODE CLONE DETECTION A NEW APPROACH.
60. Semantic Code Clone Detection Using Parse Trees and Grammar Recovery. Rajkumar
Tekchandani, Rajesh Kumar Bhatia, Maninder Singh. 2013. Nodia: IEEE, 2013.
pp. 41–46.
61. A Novel Detection Approach for Statement Clones. Qing Qing Shi, Li Ping Zhang, Fan lun
Meng and Dong Sheng Liu. 2013. Beijing: IEEE, 2013. pp. 27–30.
62. A Data Mining Approach for Detecting Higher-Level Clones in Software. Hamid Abdul
Basit, Stan Jarzabek. 2009. 2009, IEEE, pp. 497–514.
63. Detecting Clones in Business Applications. Jin Guo, Ying Zou. 2008. Antwerp: IEEE, 2008.
pp. 91–100.
64. A Study of Cloning in the Linux SCSI Drivers. Wei Wang, Michael W. Godfrey. 2011.
Williamsburg, VI: IEEE, 2011. pp. 95–104.
65. A study of code cloning in server pages of web applications developed using classic ASP .
NET and ASP .NET MVC framework. Md. Rak ibul [], Md. Raf iqullslam, Md. Ma
idullslam, Ta sneem Hal im. 2011. Dhaka: IEEE, 2011. pp. 497–502.
66. AnDarwin: Scalable Detection of Android Application Clones Based on Semantics. Chen,
Jonathan Crussell Clint Gibler Hao. 2014. 2014, IEEE, p. 1.
A Survey on Code Clone, Its Behavior and Applications
39

Trust for Task Scheduling in Cloud
Computing Unfolds It Through Fruit
Congenial
Nidhi Bansal and Ajay Kumar Singh
Abstract Trust is just a feeling to make the things reliable. Higher probability of
successful execution of scheduling builds the trust greater. Consequently, proba-
bility of failure of task of particular scheduling makes one understand that the
scheduling is not good for proﬁt in the market, and one does not trust that. Trust can
be generated through many factors or reasons. The fruit congenial concept explores
these factors to make the things understandable in actual way. In this review, we
will categorize all these factors through ﬂowchart and suitable diagrams and help to
think the reality to make/do the things correct toward technology. Trust remains
same in real life as well as in technology.
Keywords Cloud computing ⋅Trust ⋅Fruit congenial ⋅Scheduling
1
Introduction
Researchers have done much research on cloud computing technology with con-
sidering many factors such as cost, time, load balancing or sharing, and resource
utilization. Some of the scientists groove on the factor trust and deﬁne it in cloud
computing as a service, while trust is equivalent to the reliability. So trust came up
as a serious factor in cloud computing. Cloud computing is working as
Wage-per-Use model. Wage through the provider by using the services through
Internet to the user. Cloud service provider fulﬁlls the user’s requirement. To show
the right view of trust factor, we have proposed a terminology named fruit con-
genial. So everyone can understand the things behind it. Fruit congenial includes all
N. Bansal (✉)
Department of Computer Science and Engineering, Vidya College of Engineering,
Meerut, India
e-mail: nidhi18jul@gmail.com
A.K. Singh
Department of Computer Science and Engineering, MIET, Meerut, India
e-mail: ajay41274@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_4
41

the important things which affect the technology or we can say directly affected us
by using that technology dealing with many factors.
2
Related Work
The existing task scheduling algorithms consider various parameters such as time,
cost, speed, scalability, throughput, resource utilization, and scheduling success
rate. These algorithms are explained here.
2.1
Algorithms for Minimizing the Factor
Wei Wang et al. [1] present a trust-based task scheduling. This is based on
delivering computing services through a global network of computers. This paper is
extended version of traditional DLS algorithm, and it works by considering trust-
worthiness of target nodes. Bayesian trust evaluation strategy is used to design this
scheduling. The basic meaning of Bayesian is ‘probability.’ It is the probability that
everyone assures something will happen. This degree as a kind of belief is
abstractive, but also decided by reasoning according to the experience and skills of
the object, judging, analyzing, and synthesizing according to the associated
information.
GAO Zhong-wen et al. [2] give a model based on time–cost–trust factors. Users
are always different in requirement. Scheduling decisions have been designed based
on the user’s needs. If users need that cost and time should be minimum for a
particular
scheduling,
then
it
becomes
CostTrustScheduling
or
TimeTrustScheduling or TimeCostTrustScheduling for users. The overall cost of
task scheduling works with two factors that is execution time and allocation cost.
This paper works on subset tree algorithm to minimize the time and cost of
scheduling to meet user’s concern.
Jigyasu Dubey et al. [3] present the probability of relationships between the total
calculation and variant aspects for completing an activity execution in a computing
system. By using this concept, trust is evaluated.
2.2
Security Algorithms
Changping Liu et al. [4] proposed a model to measure the integrity of any module.
It calculates the integrity effects of recipient and generates the logs. Instruction code
of a system and user activities in database measures in this new module to save the
logs at the time of process scheduler. Instruction codes are working on a platform
with two classes. Operating system kernel is on ﬁrst priority and then instruction
42
N. Bansal and A.K. Singh

code of an application. Dynamic integrity can be measured in two ways that are
after successful system boot and when system call occurs. The proposed model also
covers the security properties of a system.
Abir Khaldi et al. [5] propose secure cloud architecture based on 4 zones:
(a) service offered zone, (b) management zone, (c) internal customer, and (d) ex-
ternal customer separated by a cluster ﬁrewall to prevent from the attacks.
Tapalina Bhattasali et al. [6] focus on security factor from the calculated usage of
services by cloud users. To reduce superiority of the issues in Cloud of Things
domain, secure trusted things as a service has been designed. It focuses on key
encryption algorithms with minimum overhead to make the things authentic. SLA
(service level agreement) interacts with the cloud user and cloud service provider.
SLA is generally the base of the trust connection within the communicated parties.
It informed about the cloud services, priorities, constraints, certiﬁcation, assurances,
etc. Agreements have various levels with the customer and cloud service provider.
Sneha Kolhe and Sudhir Dhage [7] proposed a model with the features of
Trusted Support Service (open-source service). OTP (one-time password) has been
used for user authentication in designed model. In addition, the research explains
the implementing Ubuntu Enterprise Cloud is working as a private cloud and
hosting the secured system with OTP which will communicate to the user through
communication media. By this concept, the system performs with better security.
Yuhong Liu et al. [8] ensure the conﬁdentiality of data in working cloud domain
with encrypted users and their important information and show the actual identi-
ﬁcation time to time, classify the trustworthiness of the cloud service providers, and
avow cloud service providers to execute the requested services with their trusted
codes. For the accomplishment of the desired goals, encryption and trust-based
techniques introduced in a framework are called EnTrust. Speciﬁcally, this newly
designed framework exists with three variables: First is an encryption module;
second, trust evaluation; and the last but not least, decision module. With the
deﬁnition of encryption module, cloud users are ready to encrypt their important
data before saving it in the database of cloud. Proposed model working with an
effective method for cloud users to measures the trust values of providers with their
behaviors. By considering the trust values, the users are ready to initiate ﬁndings
related to allow a provider to execute a service.
2.3
Monitoring Algorithms
Tian Li-qin et al. [9] discuss evaluation importance of trust based on user nature.
Main concept of this research is ‘divide and treat,’ and it is worked as a hierarchical
architecture model to divide the user nature trust into subtrust and then more
subdivisions of subtrust into many small unit of data, called behavior trust evidence.
Subsequently, they construct it repeatedly with lower level to higher level techni-
cally. This method starts with decomposition and proceeds with combination and
Trust for Task Scheduling in Cloud Computing …
43

explains the ambiguity and abstractivity for interpretation of user nature trust in
cloud domain.
Varalakshmi Perumal et al. [10] designed a framework, and it established a
dynamic trust. TP-SLA (third-party service level agreement) maintains a real-time
on-demand service estimation module. The selection of service provider is based on
the computed trust value. Adaptive window-based state monitoring is explained in
this paper, abolishing the limitations related to the real methodology, by decreasing
the volume of input data transmitted to the network. The model monitors the
requisites to estimate the time as which service has to be captured and executed.
Evaluations are carried out from users, by response based on their experience. By
third-party monitoring, more accurate feedbacks are retrieved and the monitoring
process is done.
2.4
Better Resource Utilization Algorithms
K. Kun Lu et al. [11] improve the efﬁciency of virtual resource collaboration.
Scenario of the cloud domain, when SaaS make a request PaaS to evaluate and
grant the requested services, PaaS should ask the required resources which is
executed in IaaS to ﬁnish the task. Scheduling and managing processes are the
essential resources; the division of the virtual organization by considering the trust
system. With the completion of the task, SaaS secured the feedbacks from user
regarding activities and PaaS measures the problem with the computation, actual
values, etc. Finally, IaaS evaluate the trust values to decide the participants in the
virtual required resources for users’ behavior and other important information. After
that, it provides trust values containing matrix and declares many more actual trust
count in the coming task. It will be very effective to the virtual organization cor-
rectly and increase the collaborative resource scheduling efﬁciency.
P. Varalakshmi et al. [12] implement a system which manages the resource and
improves the participation of trustworthiness of resources. They care the server in
danger condition. When the use of the resource is more than the peak threshold
value, then the server may be examined as strain and is in danger condition. The
value lesser from the threshold, server is considered is in normal condition. The
load balance scheduler schedules the activities as the resources will be taken in very
critical condition. Monitoring module monitors the resource possibility as memory
and processor. This process executes continuously. It informed the utilization status
of resources to the load-balanced scheduler. After utilizing the resource, the par-
ticipant users send feedbacks for the resource to the manager the one who approved
the resource. Paper [13–15] also discussed the concept of load balancing and other
factors.
44
N. Bansal and A.K. Singh

3
Study for Fruit Congenial
As fruits have important place in our daily diet which keep ourselves healthy and
beneﬁted us with a well-built body in a long run. In the same manner, all the fruit
factors described here are very much valuable for task scheduling in cloud com-
puting. These factors are as follows:
Fruit congenial through minimization of factors that are time, cost, failure exe-
cutions, etc.: Cost and time should be minimum for allocated resources to the task
scheduling. The hit for failure or bad executions are also minimized by including
probability methods.
Fruit congenial through security: Security includes encryption, one-time pass-
word (OTP) generation, attack detection, and platform structure, which gives
assurance to user.
Fruit congenial through monitoring: It includes past experience from user
behavior and reduces data ﬂow rate.
Fruit congenial through resource utilization: Proper utilization of resources
includes the concept of load balancing and sharing. Balance the load on all virtual
machine according to the required or allocated resources to complete the task.
First, we will study the basics of communication between user and cloud service
provider. Then, it will proceed with fruit congenial concept for better understand-
ing. Figure 1 represents the traditional way of trusted system.
Cloud service provider cares the services of cloud to serve it to users in attractive
manner. By using appropriate way, provider grants the services to the trusted user
or fruity user. Through Internet, these services are availed to all users. Schedulings
are the ways to get the attention of users. Every user wants that processing scheme
which performs better from other schemes. It is user perspective that selected
scheduling is good and the other one is bad. Users conﬁrm only those things to
assure which they want to get in their results. Because we feel all schedulings
designed day by day have better results so, all schedulings are good and best. This
scenario is also same as for market viewers.
Fig. 1 Traditional way of
trusted system
Trust for Task Scheduling in Cloud Computing …
45

Proceeding with Fig. 2 which represents fruit congenial model, major point is
reliability that can be obtained using different parameters, namely minimize time,
cost, failure, security, monitoring, and better resource utilization.
Minimize time and cost algorithms represented with the reduced amount of time
and cost of executed algorithms. Because of this, these algorithms counted in
trusted (fruit) algorithms are related to proposed model.
Security algorithms designed with secured platform or architecture for entrance
of malicious user and named trusted algorithm with security factor. This is also one
of the proposed model factors in this paper.
Monitoring algorithms monitor the behavior of the users and relate to its past
behavior and then make a report which shows the user fruity (trusted) or malicious
so this algorithm is named trusted algorithm. Because of this monitoring function, it
is related to proposed model.
Better resource utilization algorithms provide sharing and load-balancing factors
through virtualization and other concepts. Some users want to do work with limited
resources to accomplish proper utilization. These features are also named trusted
algorithms.
In Fig. 3, there have to be certiﬁed whether input data can be accessed by trusted
user at every instant of time. A ﬂowchart has been designed to make the user
become trusted/fruity user.
Users are always different in requirements. Some users want services only with
security of data, some want services through analysis of past experience of their
friends (user), again some want only minimum time and cost completion scheduling
algorithm, and some want minimum number of resources that are used for
numerous works. So this ﬂowchart shows four (but not limited) types of users. MF
—Minimize Factor, S—Security, M—Monitoring, and BRU—Better Resource
Utilization. SP—Service Provider checks three conditions that are I, L, and S
depicted in this ﬁgure, I explore as user Inﬂuence, and it associates to the authorized
user in the standardization and is estimated by the total requests. L explore as user
Liveness. It is associated with the liveness of the user in the standardization and is
estimated by the active time of the participated user. S explore as user Security. It
needs to evaluate the user performance whether user has the vicious nature and is
found by the monitoring procedure. After satisfaction of SP, check the compliance
Fig. 2 Fruit congenial model
46
N. Bansal and A.K. Singh

with the user’s requirements, and then, user becomes trusted user. To keep the user
safe and secure in their personal independently, this ﬂowchart has been displayed.
Trusted algorithms are designed by making user trusted. For execution of pro-
posed model, following chart will be implemented.
4
Conclusion
The main purpose to design this review is to reveal the reality associated with trust.
Trust is nothing, just a second name of reliability which makes things valuable or
fruity. To prove the concept of trusted algorithms is based on above explained
factors, fruit congenial model has been proposed.
Fig. 3 Flowchart
Trust for Task Scheduling in Cloud Computing …
47

References
1. Wei Wang and Guosun Zeng, “Trusted Dynamic Scheduling for Large-Scale Parallel
Distributed System”, IEEE International Conference on Parallel Processing Workshops,
pp. 137–144, 2011.
2. GAO Zhong-wen and ZHANG Kai, “The Research on Cloud Computing Resource
Scheduling Method Based on Time-Cost-Trust Model”, IEEE International Conference on
Computer Science and Network Technology, pp. 939–942, 2012.
3. Jigyasu Dubey and Vrinda Tokekar, “Bayesian Network Based Trust Model with Time
Window for Pure P2P Computing Systems”, IEEE Global Conference on Wireless
Computing and Networking, pp. 219–223, 2014.
4. Changping Liu, Mingyu Fan, Yong Feng and Guangwei Wang, “Dynamic Integrity
Measurement Model Based on Trusted Computing”, IEEE International Conference on
Computational Intelligence and Security, vol. 1, pp. 281–284, 2008.
5. Abir Khaldi, Kamel Karoui, Nada Tanabène and Henda Ben Ghzala, “A Secure Cloud
Computing Architecture Design”, IEEE International Conference on Mobile Cloud Comput-
ing, Services, and Engineering, pp. 289–294, 2014.
6. Tapalina Bhattasali, Rituparna Chaki and Nabendu Chaki, “Secure and Trusted Cloud of
Things”, Annual IEEE India Conference (INDICON), pp. 1–6, 2013.
7. Sneha Kolhe and Sudhir Dhage, “Trusted Platform for Support Services in Cloud Computing
Environment”, IEEE International Conference on System Engineering and Technology,
pp. 1–6, 2012.
8. Yuhong Liu, Jungwoo Ryoo and Syed Rizvi, “Ensuring Data Conﬁdentiality in Cloud
Computing: An Encryption and Trust-based Solution”, IEEE Wireless and Optical
Communication Conference, pp. 1–6, 2014.
9. Li-qin Tian, Chuang Lin and Yang Ni, “Evaluation of User Behavior Trust in Cloud
Computing”, IEEE International Conference on Computer Application and System Modeling,
vol. 7, pp. V7-567–V7-572, 2010.
10. Varalakshmi Perumal, Judgi Thangavel, Saranya Ramasamy and Swathy Harish, “Dynamic
Trust Establishment and Amended Window based Monitoring in Cloud”, IEEE International
Symposium on Electronic System Design, pp. 162–166, 2013.
11. Kun Lu, Hua Jiang, Mingchu Li, Sheng Zhao and Jianhua Ma, “Resources Collaborative
Scheduling Model Based on Trust Mechanism in Cloud”, IEEE International Conference on
Trust, Security and Privacy in Computing and Communications, pp. 863–868, 2012.
12. P. Varalakshmi, T. Judgi and M. Fareen Hafsa, “Local Trust Based Resource Allocation in
Cloud”, IEEE International Conference on Advanced Computing, pp. 591–596, 2013.
13. Nidhi Bansal, Amit Awasthi and Shruti Bansal, “Task scheduling algorithms with multiple
factor in cloud computing environment”, SPRINGER, vol. 433. AISC series, Third
International Conference on Information systems Design and Intelligent Applications,
pp. 619–627, 2016.
14. Nidhi Bansal, Amitab Maurya, Tarun Kumar, Manzeet Singh and Shruti Bansal, Cost
performance of Qos driven task scheduling in cloud computing, ELSEVIER, Procedia
Computer Science, International Conference on Recent Trends in Computing, vol. 57,
pp. 126–130, 2015.
15. Nidhi Bansal and Maitreyee Dutta, “Performance evaluation of task scheduling with priority
and non-priority in cloud computing”, IEEE International Conference on Computational
Intelligence and Computing Research, pp. 1–4, 2014.
48
N. Bansal and A.K. Singh

Log-Based Cloud Forensic Techniques:
A Comparative Study
Palash Santra, Asmita Roy, Sadip Midya, Koushik Majumder
and Santanu Phadikar
Abstract Cloud computing is one of the most recent advancements in the ﬁeld of
distributed computing. It has gained a lot of attention due to its on demand,
pay-per-use service, and all time availability, reliability, and scalability. Although it
offers numerous advantages, but due to its multi-tenant architecture, it is prone to
various malicious attacks and illegal activities. Cloud service provider (CSP) takes
the responsibility to secure customers’ data against such attacks. In the event of
such malicious activities, CSP aims to trace the intruder. Cloud forensic techniques
help in identifying the attacker along with proper evidence in cloud platform.
Components of clouds such as log records are then analyzed to track for such
detrimental activities. In this paper, some existing log-based cloud forensic tech-
niques have been widely studied. The detailed comparative analysis has been done
for the various techniques based on their advantages and limitations. By exploring
the limitations and advantages of the existing approaches, future research areas
have been identiﬁed.
Keywords Cloud computing ⋅Cloud forensic ⋅Investigation
Evidence ⋅Fuzzy system ⋅Cryptography ⋅Logs
1
Introduction
Cloud computing is the new paradigm that allows to shift the infrastructure for
complex computation and storage from user end to the network [1, 2]. Cloud
computing promises to deliver the information technology services to the user in a
cost-efﬁcient manner. Cloud computing is highly prone to malicious attacks,
security breaches due to several third-party users. There always lies a threat for
stealing or harming users’ sensitive private information. However, an attack always
P. Santra ⋅A. Roy ⋅S. Midya ⋅K. Majumder (✉) ⋅S. Phadikar
Department of Computer Science & Engineering, Maulana Abul Kalam Azad
University of Technology (Formerly WBUT), Kolkata 700064, India
e-mail: koushik@ieee.org
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_5
49

leaves footprint of the intruder. The study of identifying and analyzing the footprint
(evidence) is referred to as cloud forensic techniques. It is an amalgamation of the
traditional digital forensic techniques and cloud computing. The multi-tenant
architecture of cloud prevents use of the standard digital forensic techniques to be
applied in cloud environment. This leads to a huge research area for cloud forensic
techniques.
In cloud platform among the various components that can be investigated by
forensic analyzer is cloud log records. Log record keeps track of every activity from
every event to every transaction in the cloud platform. Even if an error occurs it can
be tracked in the log record [3, 4]. So analysis of the log record of any cloud helps
in identifying the attack and the party responsible for the attack. Every process or
activity of cloud has their individual log record stored in the cloud server. So server
logs are very important components to be analyzed. Though the attacks are done
through network, hence network component logs can also be used for inspection.
Provenance records are those records which tell the owner of a digital object. But
analyzing the log record is a critical issue since log records contain the detailed
information about the activities of user. This forensic signiﬁcance of log records
makes it vulnerable to attackers. Therefore, securing the log records is very crucial
to prevent it from tampering or deletion of evidences by the attacker. More-
over, while analyzing the log record, attention should be given by the CSP that no
record is altered.
In this paper, an extensive study on previously proposed cloud forensic tech-
niques is made pointing out each of their beneﬁts as well as limitations. Compar-
ative study of those approaches is also done. This further helps in making
advancement over the limitations of previously proposed approaches.
2
Cloud Forensics and Its Signiﬁcance
2.1
Cloud Forensic
Forensic science is the ﬁeld of study where illegal activity-related evidence is
considered. Evidences are analyzed to identify the crime. In digital technology, this
ﬁeld is called digital forensic. Broadly, cybercrime is in the area of digital forensic.
It mainly collects evidences from digital media such as personal computer and
server. Cloud forensic is the area of study where cloud computing is involved in the
digital forensic area. Evidences on cloud are analyzed in cloud forensic [5].
50
P. Santra et al.

2.2
Importance of Cloud Forensic
With the growing technology, illegal activities or crimes are growing day by day.
Cloud is storage of very conﬁdential information. This sensitive information may be
of cloud service provider as well as customer.
In this way, cloud security is very important to resist hacker activity. Cloud
forensic is one of the important components of cloud security. Through these
forensic techniques, we can track hacker activity and it is possible to resist future
attack [6]. The main usage of cloud forensic is shown in Table 1.
3
Literature Review of Various Log-based Cloud
Forensics Methods
3.1
A Log-based Approach [7]
Log record serves as important evidence for forensics analysis. Multi-tenant
architecture of cloud makes it difﬁcult to analyze various log records from CSP and
client end. In this work, author suggests some general component in cloud for
collecting forensic log information. The log information can be collected from
cache, registers, ARP cache, routing table, process table, kernel statistics, network
Table 1 Signiﬁcance of cloud forensic
Type of
signiﬁcance
Detail signiﬁcance
Investigation
• Investigation of cloud crime which violates rules and regulation in
multi-jurisdictional and multi-tenant cloud service network
• Investigation on suspecting activity like transaction and operation
• Previously occurred event can be constructed from partial data in cloud
• Produce digital evidence when needed by court
• Collaboration with law enforcement, SLA
Troubleshooting
• Locating information physically in cloud
• Security handling in cloud environment
• Tracing an event
• Resolving technical and functional issues in cloud environment
Log monitoring
• Gathering, analyzing log records of resources and accessing other
components in cloud environment to assist auditing, statistical analysis,
etc.
Data and system
recovery
• Searching and recovery of the data that have been accidentally deleted
from cloud
• Decrypt the encrypted data or recovery of original data when the
encryption key is lost
• Recovering system from attack
Log-Based Cloud Forensic Techniques: A Comparative Study
51

topology, temporary ﬁle system, and archival media. Suggested approach in this
paper is an improvement on general log analysis.
• SaaS: In this approach, a local log ﬁle is maintained at the client end. This log
ﬁle is created and maintained by API from CSP and can be used by the
investigator in forensic analysis. The main advantage of such architecture is that
CSP need not be disturbed often for viewing the log ﬁle. But keeping the log ﬁle
in local end requires continuous synchronization of timestamp and unique
identiﬁcation value between the cloud and client ends. The security also needs to
be ensured to prevent unauthorized access to the log ﬁle by using incremental
HASH algorithm. Figure 1 explains the entire ﬂow of the process with the help
of a sequence diagram.
• PaaS: CSP supplies a module for log record tracking and storing for PaaS for
third-party investigator use. A low-level API is provided by which third party
can create their own customized log module. By using this model, log veriﬁ-
cation time and complexity is optimized. Disturbance of main cloud server is
also reduced using this technique.
Fig. 1 Log investigation in SaaS
52
P. Santra et al.

3.2
Logging Framework for Cloud Forensic [8]
Authors have suggested advancement on traditional cloud architecture in [8]. It
collects forensic data from the different modules of general cloud architecture such
as security, validation engine, scheduler, hypervisor interface, load distribution,
internal cloud API, and external cloud API. Authors have suggested to incorporate
forensic module in management or abstraction layer. This module will deliver
forensic evidence that is ready to present in court. The cloud forensic module can be
divided into different layers as shown in Fig. 2. The importance of each layer is
described here as follows:
• Management: This is responsible for collecting data from management module.
Management module helps in extracting important log information.
• Virtualization: This layer is inspected for collecting data from virtualized
architecture. Virtual memory of VM, disk images of VM, virtual network
activity, overload of virtual network can be good targets of forensic evidence.
Local log module and dedicated hypervisor of each VM can collect this type of
data.
• Raw data: This layer collects raw data from management and virtualization
layer. Unprocessed evidence or data are sent to this layer by previous modules.
• Processing layer: The processing on the raw log data is done here. Different
analysis (segregation, veriﬁcation) techniques are done to make these forensi-
cally ready.
• Final data: Well-organized forensic evidences are provided from investigator’s
interface for presentation purpose.
Fig. 2 A framework for log base forensic investigation
Log-Based Cloud Forensic Techniques: A Comparative Study
53

3.3
Anonymizing Log Management Process for Secure
Logging in the Cloud [9]
Log records need to be encrypted to maintain privacy and data integrity for cloud
users. In the preservation phase of cloud forensic, it is observed that logs may be
tampered or altered by anyone who has the provision of accessing it. Generally,
evidences are not accepted in court if original source of evidence is tampered. It
must be secured to maintain integrity and conﬁdentiality. In cloud environment,
system logs, such as program execution status and system resource usage, are most
important for investigation because attacker mainly targets system logs [10].
In [9], authors used homomorphic network cryptographic scheme to encrypt the
logs. In this process, encryption is done using homomorphic operation on data set
which is converted from one form to other while preserving a relation between
them. Due to this property, mathematical operation performed on cipher text and
plain text gives the same result [11]. This special feature of homomorphic algorithm
helps users to store data in cloud securely. There are two types of homomorphic
algorithms: multiplicative and additive homomorphic algorithms. Authors have
deigned a system architecture using the two homomorphic algorithms to ensure log
security. The procedures that are followed to ensure log security are as follows:
logging, log preparation, and log monitoring. Since this architecture uses homo-
morphic algorithm, log security is assured, while forensic analyzer can easily
analyze the log records.
3.4
Fuzzy Logic-based Defense Mechanism [12]
There are several attacks that occur in the cloud system, DDoS (Distributed Denial
of Service) is one of them. Quality of service of cloud is decreased during DDoS
attack. A defense mechanism is designed in [12] to prevent these attacks. Cloud
trafﬁc is analyzed here to differentiate between normal and malicious packets.
ICMP echo packets are sent by the attacker to ﬁnd vulnerability of a system. When
the vulnerability is found, then congestion in network is intentionally created. This
congestion threat can be a security issue of cloud system as well as customer’s
sensitive data. Authors have considered certain parameters for trafﬁc analysis.
These parameters are entropy of packet type, HTTP packet timeline, number of
packets, Entropy of source IP address and port, destination IP address and port,
packet rate. Trafﬁc analysis is done on the basis of these parameters. A fuzzy
inference engine is derived using these parameters with proper membership func-
tion. Table 2 describes the complete fuzzy rules deﬁned for the system. Trafﬁc
information is analyzed using fuzziﬁcation, inference engine, and defuzziﬁcation.
Here, fuzzy knowledge base helps to cluster the trafﬁc. There are several kinds of
attacks in cloud environment other than DDoS such as probing, unauthorized access
54
P. Santra et al.

from remote machine, and unauthorized access to local super user. To detect these
attacks, other attributes such as access duration, type of protocol used, type of
fragmentation, and many more need to be considered.
3.5
Fuzzy Logic-based Expert System for Network Forensic
[13]
A fuzzy expert system is designed in [13], and it provides classiﬁcation of normal
and malicious trafﬁc. Network trafﬁc is analyzed here using fuzzy logic system.
Five different components in the architecture work as follows:
• Trafﬁc analyzer: It captures trafﬁc information from the network component and
analyzes it. It classiﬁes network trafﬁc into different categories according to the
protocol types and generates crisp value for input variable.
• Fuzziﬁcation module: It deﬁnes membership function for each of the input
variable and assigns fuzzy value.
• Fuzzy inference engine: After fuzziﬁcation of all input variables, inference
engine takes decision on the basis of knowledge base (training set). Aggregation
operation is done for all “IF” clause and composition is done for all “THEN”
clause. At last step, inference engine determines degree of truth of each variable.
• Defuzziﬁcation: It is responsible to convert fuzzy value to its corresponding
crisp value. According to the crisp value, attacks can be identiﬁed.
• Forensic analyzer: It decides whether the network packets are malicious or not.
Mainly, it collects decision from fuzzy system and creates digital evidence.
Table 2 Rules to distinguish malicious and normal trafﬁc
Rules for trafﬁc detection
Type of
trafﬁc
Low entropy of source IP address and port with high rate in ICMP packets
Malicious
trafﬁc
High HTTP packet rate and low HTTP packet timeline request
Low entropy of source IP address as well as low number of packets and high
rate in UDP packet
Low entropy of source IP address and high Source port, number of packet and
entropy of packet type
Medium entropy of source IP address and port and low packet rate
Medium entropy of source IP address and port with high packet rate of TCP
SYN
High entropy of source IP address and port is high with low number of packets
Normal
trafﬁc
Medium entropy of source IP address and port with low number of packets
Log-Based Cloud Forensic Techniques: A Comparative Study
55

In the proposed scheme of network forensics [13], fuzzy expert system is used to
classify different types of attacks through log analysis. This concept can be applied
in cloud environment with some modiﬁcation on cloud aspects. There are some
cloud parameters such as process queue, quality of service, response time, number
of VM associated, by which forensic analysis can be done efﬁciently.
4
Comparative Study
In Table 3, a detailed comparative analysis is done for the various log-based
forensic approaches. Some architectural frameworks were proposed with certain
signiﬁcant changes in the cloud model which helps in the forensic analysis of
malicious record. For better analysis of log records, some cryptographic methods
and some expert systems were proposed. The advantages of each approach are
highlighted, and their disadvantages are pointed out which require further
improvement.
Table 3 Detailed analysis of earlier proposed techniques
Approaches
Beneﬁts
Limitations
A Log-based approach
[7]
• Local log dumping is
suggested. Using this
technique, disturbance of
main server or CSP may be
reduced
• Log module developed by
CSP may provide better
solution for security of log
• Customizable log module
using low-level API is
suggested which is
advantageous for acquiring
partial log information
• HASH algorithm is used to
maintain integrity and
authenticity. Original
evidence tampering can be
avoided using this technique
• There is a condition for local
log module. The content of
log should only be readable
by using CSP-provided tool
thus ensuring better security
• It is better for SaaS and PaaS
model, no solution is given for
IaaS
• For large amount of logs, there
should be a search space
reduction technique. No log
segregation or search space
reduction technique is
proposed here
• Logs from management
module may carry important
information. But it is
neglected in this model
• Network log is also
overlooked here
(continued)
56
P. Santra et al.

Table 3 (continued)
Approaches
Beneﬁts
Limitations
Logging framework for
cloud forensic [8]
• It is suggested that cloud
forensic module interface
should be in abstraction layer.
Outside world will only use
this interface to access log
• Hypervisor for each VM may
provide effective log
information. It can collect logs
separately from all the VM in
the system
• Management modules (load
balancing, optimization,
scheduling) can be a good
source of forensic log
• Raw forensic data (without
analysis and presentation) are
provided from cloud interface.
It may be useful for
emergency purpose. Next step
of analysis can also be done
easily with ofﬂine mode
• No data integrity and privacy
techniques are proposed
• Log segregation and search
space reduction techniques are
not described
• Network log is overlooked
here
Anonymizing log
management process for
secure logging in the
cloud [9]
Fuzzy logic-based
defense mechanism [12]
• A secure log collection
module is suggested
• Cryptography is used here to
encrypt log ﬁles
• Decision on attack or normal
trafﬁc can be done using fuzzy
inference system
• Different network parameters
are identiﬁed to make decision
on event (malicious or normal
trafﬁc)
• Digital evidence structure is
also suggested
• Well-known cryptographic
technique is used. These
techniques are vulnerable to
advanced hacker
• Cloud parameter for attack is
not considered here
Fuzzy logic-based expert
system for network
forensic [13]
• Using training knowledge
base, decision is made for
trafﬁc (malicious or not)
• Network parameters are
considered to make the model
secure
• Cloud parameters are not
considered here
• Knowledge base is deﬁned
according to previous data.
Improper knowledge base can
make the model improper
Log-Based Cloud Forensic Techniques: A Comparative Study
57

5
Conclusion and Future Scope
According to the survey on the various cloud forensic frameworks and techniques,
certain beneﬁts and limitations have been observed. In this paper, some earlier
proposed cloud forensic models based on log analysis are stated. Log dumping
using CSP module and customized log extraction have been addressed in [7].
However, security of the dumped log is not too strong. To make the log records
secured, a strong encryption technique like Boolean and Euclidian cryptography
may be implemented. Pre-forensic and post-forensic data along with deliverable
evidences need to be preserved for forensic presentation. Encryption techniques in
[9–11] can also be an alternative for securing evidences. Logs from management
and network component are also very important for forensic analysis in cloud
environment because it gives evidences about intruder. Management logs are taken
in consideration in [8], and network trafﬁc is analyzed in [13]; combining these both
techniques a better cloud forensic architecture can be designed. From the above
beneﬁts and limitations of earlier proposed models, the following research areas
have been identiﬁed
• Security using Boolean and Quantum Cryptography: Data preservation phase of
cloud forensic needs strong security algorithm. General cryptographic tech-
niques cannot be used in cloud log preservation because most of them are
vulnerable to advanced intruder as power of computer is growing gradually [14].
In that case, stronger cryptography techniques such as Boolean and quantum
cryptography can be used.
– Data set can be encrypted using Boolean function and also can be converted
into Boolean value. Boolean function on any N parameter data set makes it
2N search space. With increment of N, the combination space grows
respectively. So for the large combination set, it is difﬁcult for the intruder to
crack it.
– Concept of quantum physics can be applied to make unbreakable cryp-
tosystem. Quantum key generation and key distribution can be used to
encrypt data set and deliver it to destination. It is very difﬁcult to crack
because an attack disturbs the quantum state of data set. It is simple to use,
and less resources are needed to maintain it.
• Expert system using fuzzy and data mining: A knowledge-based expert system
can be used to detect malicious activity. Generally, there is large trafﬁc in cloud
environment. To analyze the trafﬁc, a fuzzy expert system can be designed. As
the trafﬁc size is comparatively large, concept of data mining needs to be added
here. This will help in differentiating between normal and malicious trafﬁc.
• Enhanced Architecture: In [7, 8], log data lack security and analyzing the log
becomes difﬁcult. In that case, an enhanced architecture can be designed with a
combination of traditional and expert system along with cryptographic
mechanism.
58
P. Santra et al.

References
1. Buyya, R., Yeo, C. S., Venugopal, S., Broberg, J., Brandic, I.: Cloud computing and
emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility.
In: Future Generation computer systems, 25(6), 599–616 (2009).
2. Mell, P., Grance, T.: The NIST deﬁnition of cloud computing (2011).
3. Birk, D., Wegener, C.: Technical issues of forensic investigations in cloud computing
environments. In: IEEE Sixth International Workshop In Systematic Approaches to Digital
Forensic Engineering, 1–10 (2011).
4. Ruan, K., Carthy, J., Kechadi, T.: Survey on cloud forensics and critical criteria for cloud
forensic capability: A preliminary analysis. In: Proceedings of the Conference on Digital
Forensics, Security and Law. (2011).
5. Zawoad, S., Dutta, A. K., Hasan, R: SecLaaS: secure logging-as-a-service for cloud forensics.
In: Proceedings of the 8th ACM SIGSAC symposium on Information, computer and
communications security, 219–230 (2013).
6. Zargari, S., Benford, D.: Cloud forensics: Concepts, issues, and challenges. In: IEEE 2012
Third International Conference on Emerging Intelligent Data and Web Technologies,
236–243 (2012).
7. Sang, T: A log based approach to make digital forensics easier on cloud computing. In:
Intelligent System Design and Engineering Applications (ISDEA), Third International
Conference. 91–94 (2013).
8. Patrascu, A., & Patriciu, V. V.: Logging framework for cloud computing forensic
environments. In: Communications (COMM), 10th International Conference, 1–4 (2014).
9. Rajalakshmi, J. R., Rathinraj, M., Braveen, M.: Anonymizing log management process for
secure logging in the cloud. In: Circuit, Power and Computing Technologies (ICCPCT), 2014
International Conference, 1559–1564 (2014).
10. Lantz, B., Hall, R., Couraud, J.:Locking Down Log Files: Enhancing Network Security By
Protecting Log Files. Issues in Information Systems, 7(2) (2006).
11. Fontaine, C., Galand, F: A survey of homomorphic encryption for nonspecialists. EURASIP
Journal on Information Security, (2007).
12. Iyengar, N. C. S., Banerjee, A., Ganapathy, G.: A fuzzy logic based defense mechanism
against distributed denial of service attack in cloud computing environment. In: International
Journal of Communication Networks and Information Security, 6(3), 233 (2014).
13. Kim, J. S., Kim, D. G., & Noh, B. N.: A fuzzy logic based expert system as a network
forensics. In Fuzzy Systems, 2004. Proceedings. 2004 IEEE International Conference (2),
879–884 (2004).
14. Idquantiquecom. (2015). IDQ. Retrieved 24 May, 2016, from http://www.idquantique.com/
securing-the-cloud-with-quantum-safe-cryptography/.
Log-Based Cloud Forensic Techniques: A Comparative Study
59

An Automated Malicious Host Recognition
Model in Cloud Forensics
Suchana Datta, Palash Santra, Koushik Majumder and Debashis De
Abstract Cloud forensics is the new emerging science where traditional digital
forensics methodology and cloud computational intelligence have been blended in
such a way that all the malicious cloud criminals can be identiﬁed and punished in a
justiﬁed manner. The distributed and black-box architecture of the cloud has faded
the concept of examining each and every local host to identify proper malicious
actors. Here, an obvious demand of an automated criminal recognition model has
come into play. This paper mainly focuses on this legitimate demand of cloud
forensic investigators by proposing a Cloud Malicious Actor Identiﬁer model. This
model identiﬁes the malicious actors related to a particular crime scene and ranks
them according to their probability of being malicious using a very well-known
machine learning technique, Boosting. The main purpose of this model is to mit-
igate the overhead of probing each and every IP address while investigation. The
performance evaluation of the proposed model has also been explained with logical
explanation and achieved output.
Keywords Cloud forensics ⋅Principal component analysis
Boosting ⋅Malicious actor identiﬁer
1
Introduction
Classical digital forensics is a branch of science where the investigators used to
investigate each speciﬁc victim machine as reported, to examine the criminal
activities, and to collect evidences [1]. And cloud computing is the technology that
creates an appreciable discrimination between applications and resources. The
distributed, dynamic, multitenant, and on-demand cloud architecture has made this
century faster and reliable. But discrepancies are the nature of any technology.
S. Datta ⋅P. Santra ⋅K. Majumder (✉) ⋅D. De
Department of Computer Science & Engineering, Maulana Abul Kalam Azad
University of Technology (Formerly WBUT), Kolkata, India
e-mail: koushik@ieee.org
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_6
61

Therefore, whenever any cloud crime takes place, the situation becomes a menace.
There the researchers came up with the new branch of science, cloud forensics,
where although it maintains the traditional digital forensics process ﬂow, but its
distributed architecture and black-box nature demand this methodology to be
deﬁned in a new way. Instead of examining each and every IP addresses [2], there
must be an automated system which will recognize the harmful actors almost
accurately and authentically. In this paper, a system has been designed using a very
prominent technique Boosting, where after classifying all the relevant attributes
related to the reported crime scene, it analyzes the record and makes a model that
can tell the probability of being harmful of any machine. The model thus ranks all
the probable hosts according to their severity, so that the investigation process can
be optimized with respect to time and cost as well.
2
Backgrounds and Motivations
2.1
Cloud Forensics
Forensics can be deﬁned as the application of science in various criminal cases and
civil laws. Forensic scientists are used to collect and preserve relevant scientiﬁc
criminal evidences for the sake of investigation, so that the investigation process
can be carried out smoothly and justiﬁably [3]. NIST deﬁnes cloud forensics as “the
application of scientiﬁc principles, technological practices, and derived and proven
methods to reconstruct past cloud computing events through identiﬁcation, col-
lection, preservation, examination, interpretation, and reporting of digital evidence
[4].” Before the advent of cloud computing, traditional digital forensics investi-
gation process was followed by the investigators whenever any criminal activity
had been reported. They performed their investigation process by examining each
and every IP addresses of the victim system and reported the criminal activity after
identiﬁcation. But this traditional process becomes blurred with the incorporation of
cloud computing in the digital forensics domain. This deﬁnes, the emergence of
cloud forensics.
2.2
Principal Component Analysis
Principal component analysis (PCA) is a ﬁne and established statistical data
cleaning methodology [5]. An orthogonal transformation is used by this procedure
in order to convert a set of observations consisting of possibly correlated variables
into a set of observations containing linearly uncorrelated variables, named as
62
S. Datta et al.

principal components (PCs). The number of PCs must be less than or equal to the
number of variables in the actual observations. The orthogonal transformations
have been deﬁned in such a fashion that the ﬁrst PC owns the highest possible
variance, i.e., it represents as much of the variability in the variables as possible.
Each of the succeeding components is orthogonal to the preceding one, and the
variability becomes lesser comparatively. Thus, the resulting vectors become an
uncorrelated orthogonal basis set. Since PCs are the eigen vectors of the covariance
matrix, which is symmetric in nature, they are orthogonal to each other. Exploratory
data analysis and designing predictive models are the most viable place to use PCA.
Component or factor scores and loadings are the measuring factor of any PCA
result.
2.3
Gradient Boosting
Regression and classiﬁcation problems can be solved using various techniques,
among which Gradient Boosting is very well-known machine learning technique
[6]. Models are produced using this technique in the form of decision trees. The
generated model is the ensemble of weak models usually which are built in a
stage-wise fashion. At each step, they are generalized by optimizing an arbitrary
differentiable loss function. Leo Breiman [7] ﬁrst interpreted boosting as an opti-
mization algorithm, based on a suitable cost function. The basic principles of this
technique can be explained as follows: Initially, a loss function and a weak learner
are provided. So minimizing this given loss function with the help of an additive
model is the main objective of this algorithm. The best guess of the response
variable is taken as its initial value. Then for the purpose of minimizing the loss
function, the gradient or residuals are calculated by the algorithm, and a model is
ﬁtted to the calculated residuals. The algorithm continuously adds up the current
weak learners to the previous one up to a user-mentioned number of iterations
produces a strong learner in an iterative fashion.
2.4
Motivation
The dynamic and black-box architecture of cloud does not allow forensic investi-
gators to carry out the traditional digital forensics process, where the investigators
were used to examine each and every IP address to identify exact harmful actor and
to collect correct and relevant information for further investigation. In this era of
cloud, if an investigator is supposed to identify cloud criminals in this way, it will
become a menace. Instead, if there had been a system which will identify malicious
An Automated Malicious Host Recognition Model in Cloud Forensics
63

hosts automatically can make forensic process a robust and splendid one. There the
idea of the proposed Malicious_Actor_Identiﬁer model comes into play which is an
automated harmful host recognition system based on the calculated probability.
3
Proposed Model Architecture
Investigators ﬁrst collect the incident-type information with the help of VM
snapshots. Incident scene is classiﬁed properly using some simple classiﬁcation
rules. Once the crime incident is classiﬁed, investigators infer the related attributes
to the incident scene and connections relevant to the attributes. The proposed model
then analyses those collected data determining the probability of being malicious
for each recorded host with justiﬁable accuracy and reliability as depicted in Fig. 1.
The proposed model has two major phases, preprocessing and analysis which has
been explained in the following two sections.
Fig. 1 Work ﬂow of the proposed model
64
S. Datta et al.

4
Malicious_Actor_Identiﬁer Model
4.1
Algorithm Preprocessing
(a) Input: Collected data set (.csv ﬁle) of hosts H′
i × n with n independent variables.
(b) Output: Statistical cleaned data set (.csv ﬁle) with (n–k) independent variables,
where n–k ≥2, i.e., H′
i × ðn −kÞ, where i, n, k = 1, 2, 3…
(c) Algorithm:
(1) Normalize the input data set H′
i × n so that each attribute n ϵ A falls within the
same range.
(2) For i = 1 to m, do
(i)
Find the principal component = Zi = aT
i X whereai = coefficient vectors ,
X = component variable vector
(ii)
Find the variability of Zi, i.e., V(Zi).
(iii)
Sort
PCs
according
to
the
variability
in
such
a
manner
that
VðZ1Þ ≥VðZ2Þ ≥. . . ≥VðZiÞ
(3) Discard the weaker components (with low variance) and output the data set,
H′
i × ðn −kÞ with stronger PCs.
4.2
Algorithm Analysis
(a) Input: Cleaned data set, Test data set, where, i, n, k, a = 1, 2, 3…
(b) Output: Model which will calculate the probability of being harmful for each
of the machine we get as output from the preprocessing model.
(c) Algorithm:
(1) Select a particular set of data to be trained; i.e., the training set of data.
(2) Identify the near-zero variance predictor using the method nearZeroVar ().
(3) Turn the training data set into a data frame with duplicate elements or rows
removed.
(4) For every element (f) of the data frame, do
(a) If (f) has the class type “character,” replace them with numeric ids.
(b) Encode dummy variables by storing them as objects of “integer” class.
(5) Train the training vector using boost () and get the model.
(6) Save the model module in a text or binary ﬁle.
An Automated Malicious Host Recognition Model in Cloud Forensics
65

(7) Using the model, calculate the response variable of each object of the
testing data set (probability of being harmful of a particular machine).
(8) Write the predicted output to a .csv ﬁle.
5
Results and Discussion
When a malicious incident is reported to the cloud service provider, investigators
classify the incident scene and thus its attributes and relevant connections. In the
proposed model, after identifying the related hosts to the incident scene, they have
been listed with all the relevant attributes and all other different variables with
proper signiﬁcance. Now, if an investigator wants to analyze this data set, in order
to identify the harmful host (probable), independent variables of the data set with
least variability must be omitted for a better analysis. Therefore, we apply a pre-
processing technique, principal component analysis (PCA), to make this data set a
cleaned one. Since PCA cannot handle categorical data and the 1st column of this
data set contains categorical values, therefore that column has been removed so that
the entire data set contains only numeric data.
The variance versus principal component graph shows the variability of each
principal component in Fig. 2. Each and every component is a linear combination of
the predictor variables, and the components are chosen by the value of their vari-
ances, like -V (1st component) > V (2nd component) > … > V (nth component).
Fig. 2 Variances of the principal components
66
S. Datta et al.

This denotes that information across the 2nd component (Z2) is very less compared
to the 1st component (Z1), where each of the components is in the form like
Z1 = aT
1X = a11X1 + a12X2 + . . . + a1nXn
Zp = aT
pX = ap1X1 + ap2X2 + . . . + appXp
Therefore, it can be said that Z1 alone is sufﬁcient to give the information which
is available in the original data set as in Fig. 3. This is where the reduction is being
done. Now the question is how many components are to be taken as consideration?
The elbow curve depicted in the ﬁgure gives the answer. The curve shows that it
has been broken at the point of the 4th principal component. So we can club the
respective predictor variables of these four components each to make the model.
Here in this proposed model, we have taken the ﬁrst two components to design
the model. These two components are composed of 3 independent variables
(Data_var1, UDP_var2, and IGMP_var3), so these are the ﬁnal variables those are
taken into consideration for designing the model. Figure 4 depicts the importance
of the components which also guides a designer to make decision about which
components are to be considered. Here in this proposed model, due to the lack of
proper log where all the malicious host details are kept, the response variable, i.e.,
the last column has been added. This column consists of the probability of being
harmful of any particular host connected to the classiﬁed network. The value of this
variable is nothing but the regression values of all the 3 predictor variables. Since
we know regression value is the mathematical correlation among the predictor or
independent variables, here, from the regression value, we can smell a pattern of the
dependencies of the response variable upon the predictor variables. Based on these
Fig. 3 Elbow curve for identifying principal components
An Automated Malicious Host Recognition Model in Cloud Forensics
67

variables, we are going to train our data set and build our proposed model
accordingly.
After cleaning up the original data set, that has been taken as training data set,
the cleaned data set is trained so that the ultimate predictive model performs more
and more accurately and perfectly. Here, the training has been done using a very
popular and efﬁcient classiﬁcation and regression technique, Boosting. The basic
understanding of the model behind is as explained below.
Suppose, if we have k trees, then the model is, ∑
K
k = 1
fk, where each fk is the
prediction from a decision tree. The model is nothing but a collection of decision
trees. Having all the decision trees, ﬁnal prediction is made by, ŷ = ∑
K
k = 1
fkðxiÞ,
where xi = feature vector for the ith data point. In order to train the model, a loss
function is needed to be optimized, and typically rooted mean squared error
function for regression is used as a loss function which can be deﬁned as
L = 1
N ∑
N
i = 1
ðyi −ŷiÞ2.
Regularization is another important part of the model. A good regularization
term controls the complexity of the model which can be deﬁned as −Ω = γT + 1
2λ
∑
T
j = 1
w2
j , where T = the no. of leaves, w2
j = score on the jth leaf and γ, λ = degree
of regularization. Therefore, the objective function of the model can be expressed as
−Obj = L + Ω, where, L controls the predictive power (how well the model ﬁt to
the training data) and Ω controls the simplicity of the model. Here for the training
purpose, Boosting package [8, 9] gradient descent is used to optimize the objective
function. It is an iterative technique which calculates the gradient of the objective
function, i.e.—∂ŷ Obj(y, ŷ), where y is the true target and ŷ is the predicted target.
Therefore, our main motto is to improve ŷ along the direction of the gradient to
Fig. 4 Description of the principal components
68
S. Datta et al.

optimize the objective function (maximizing the loss function L and minimizing the
regularization
Ω).
Now
the
objective
function
can
be
redeﬁned
as
ObjðtÞ = ∑
N
i = 1
Lðyi, ŷðt −1Þ
i
+ ft ðxiÞÞ + ∑
t
i = 1
ΩðfiÞ, where ftðxiÞ = the prediction score of
the tth tree which is to be added with the predicted score of the previous tree, i.e.,
(t −1). Therefore, it is clear that our goal is to ﬁnd out the prediction score of the
tth tree, i.e., ftðxiÞ so that that the loss function can be optimized. In order to build a
tree, the best splitting point is found out in a recursive manner until we reach to the
maximum depth. Then, we prune out the nodes with a negative gain in a bottom-up
order. At the time of training, if we ﬁnd that the gain is positive, then we keep the
splitting point. Otherwise, we remove those splits and keep the parent only. We go
to the maximum depth of the tree always because sometimes it may happen that in
the midway, we get negative gain but after that we get a positive gain. Figure 6
shows all the testing host data with the probability of being harmful which will form
the tree structure using the above-discussed boosted tree methodology. There are
several evaluation metrics to measure the classiﬁcation accuracy. Here “Area under
the curve” which shows how well the model ﬁts to the training data set.
Fig. 5 Model accuracy (area
under curve)
Fig. 6 Calculated probability measure for set of hosts
An Automated Malicious Host Recognition Model in Cloud Forensics
69

The more the “auc” value tends to 1, the more the model achieves the accuracy,
although some other parameters are there to measure the model accuracy. Here in
the following diagram, we can ﬁnd that the model has got the area under curve
value at each and every iteration in Fig. 5. The ultimate output values are shown in
the Fig. 6 where it has been found that the model has acquired 84.73% accuracy.
6
Future Scope and Conclusion
The main aim of the proposed model is to mitigate the overhead of the cloud
forensics investigators from exploring each and every IP address to ﬁnd out the
proper malicious host or a group of hosts. Since data are spread out in a distributed
manner in cloud and the complexities of the distributed system are hidden from the
users, collecting huge amount of data from the victim machines after proper
identiﬁcation is a tough job. In this respect, it is quite obvious that cloud crime
investigators demand an automated system which will help in their investigation
process. Keeping that in mind, a forensic system has been modeled in this paper, so
that this objective can be reached and investigators can investigate in a smooth and
efﬁcient manner. In this regard, we have used boosting, a prominent machine
learning technique for solving classiﬁcation and regression problems. The model is
designed and tested for accuracy, validating its score at each of the iterations. In
future, various other classiﬁcation methods [10–13] are to be used to develop a
cooperative model so that more than 90% accuracy can be achieved from the
proposed model.
Digital forensics faces several challenges due to the newly introduced cloud
technology. The rapid advancement of the cloud technology demands traditional
forensic methodology to cope up with it so that the cloud criminals can be punished
according to the laws of respective jurisdiction. The main challenge remains in
recognizing proper malicious hosts so that rest of the forensic process can be carried
out smoothly. The distributed architecture of the cloud makes it impossible for the
investigator to examine each and every IP address to ﬁnd out the malicious actors.
The proposed model is an attempt to mitigate this problem by introducing a system
which will identify probable harmful hosts by ranking them with the probability of
being harmful. A detailed experimental result and explanation have been given so
that the model’s accuracy can be validated. In future, an ensemble model to be
developed which can make predictions more accurately and identify exact mali-
cious hosts appropriately.
70
S. Datta et al.

References
1. Guo, H., Jin, B., Shang, T., Forensic investigations in cloud environments., In IEEE
International Conference on Computer Science and Information Processing (CSIP), 2012,
pp. 248–251.
2. Grispos, G., Storer, T., Glisson, W. B., Calm before the storm: the challenges of cloud,
In Emerging digital forensics applications for crime detection, prevention, and security, 2013,
Vol. 4, pp. 28–48.
3. Accorsi, R., Ruan, K., Challenges of cloud forensics: A survey of the missing capabilities,
In ERCIM News, 2012, Vol. 90.
4. NIST Cloud Computing Forensic Science Working Group, In NIST Cloud Computing
Forensic Science Challenges, Draft NISTIR 8006, 2014.
5. Jolliffe, I., Principal component analysis. John Wiley & Sons, Ltd, 2012.
6. Friedman, J. H., Greedy function approximation: a gradient boosting machine, In Annals of
statistics, 2001, pp. 1189–1232.
7. Friedman, J. H., Stochastic gradient boosting. In Computational Statistics & Data Analysis,
2002, Vol. 38(4), pp. 367–378.
8. Ridgeway, G., Generalized Boosted Models: A guide to the gbm package, 2007, Update,
Vol. 1(1).
9. Ridgeway, G., gbm: Generalized boosted regression models. R package version, 2006, Vol. 1
(3).
10. Dietterich, T. G., An experimental comparison of three methods for constructing ensembles of
decision trees: Bagging, boosting, and randomization. In Machine learning, 2000, Vol. 40(2),
pp. 139–157.
11. Liaw, A., Wiener, M., Classiﬁcation and regression by random Forest. In R News, 2002,
Vol. 2 (3), pp. 18–22. R package version 4.6. 10.
12. Nelder, J. A., Baker, R. J., Generalized linear models, In Encyclopedia of statistical sciences,
1972.
13. Specht, D. F., A general regression neural network, In IEEE transactions on neural networks,
1991, Vol. 2(6), pp. 568–576.
An Automated Malicious Host Recognition Model in Cloud Forensics
71

Parallel Bat Algorithm-Based Clustering
Using MapReduce
Tripathi Ashish, Sharma Kapil and Bala Manju
Abstract As we are going through the era of big data where the size of the data is
increasing very rapidly resulting into the failure of traditional clustering methods on
such a massive data sets. If the size of data exceeds the storage capacity or memory of
the system, the task of clustering will become more complex and time intensive. To
overcome this problem, this paper proposes a fast and eﬃcient parallel bat algorithm
(PBA) for the data clustering using the map-reduce architecture. Eﬃcient using the
evolutionary approach for clustering purpose rather than using traditional algorithm
like k-means and fast by paralyzing it using the Hadoop and map-reduce architec-
ture. The PBA algorithm works by dividing the large data set into small blocks and
clustering these smaller data blocks in parallel. The proposed algorithm inherits the
bat algorithm features to cluster the data set. The proposed algorithm is validated on
ﬁve benchmark data sets against particle swarm optimization with diﬀerent number
of nodes. Experimental results show that the PBA algorithm is giving competitive
results as compared to the particle swarm optimization and also providing the sig-
niﬁcant speedup with increasing number of nodes.
Keywords
Bat algorithm ⋅Parallel bat algorithm ⋅Map-reduce ⋅Hadoop
T. Ashish (✉) ⋅S. Kapil ⋅B. Manju
Jaypee Institute of Information Technology Noida, Delhi Technological
University Delhi, IP College of Women Delhi, New Delhi, India
e-mail: ashish.tripathi@jiit.ac.in; ashish07@gmail.com; se.jiit@gmail.com
S. Kapil
e-mail: kapil@ieee.org
B. Manju
e-mail: manjugpm@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_7
73

74
T. Ashish et al.
1
Introduction
Clustering is a popular analysis technique in data science, used in many applications
and disciplines. Based on the values of various attributes of objects, it is used as an
important tool and task to identify the homogeneous groups of the same. Cluster-
ing can be of following two types: hierarchal and partitioning. Hierarchal clustering
works on two techniques, division and agglomeration of data clusters. Division is
breaking large clusters into smaller ones, and agglomeration is merging small ones
into nearest cluster. While in partition-based clustering, center of each cluster is
used to compute an objective function and the value of this function is optimized
by updating the center of clusters called as centroids. Clustering has a wide applica-
tion in problems of data mining, data compression, pattern recognition, and machine
learning. K-means is clustering algorithm which works on the greedy principle. It
partitions the n data samples into k-clusters to minimize the sum of Euclidean dis-
tance of all data samples from their cluster centers. However, major drawbacks of
this algorithm are as follows:
∙No proper method to initialize. Generally done randomly.
∙Due to high dependency on the initial centers, it may get stuck to suboptimal val-
ues, only quick solution is to execute it multiple times.
∙Accuracy changes with change in number of clusters (k).
∙In many cases, it tends to get stuck to local or suboptima.
To avoid the problem of initialization dependency and stuck into the local optima,
the researchers now a days are using nature-inspired algorithms for the data cluster-
ing. Nature-inspired computing is the type of computing which takes its founda-
tion from the biological aspects of nature which is humans and animals. Four pow-
erful features of nature are self-optimization, self-learning, self-healing, and self-
processing. As a self-optimizer, nature manages its resources eﬃciently so as to meet
all enterprise needs in the most eﬃcient way. The main problem with the nature-
inspired algorithm is that they are computation intensive in nature. The nature-
inspired algorithms are not able to give the satisfactory results in the reasonable
amount of time. Today, the amount of data has increased manifold and its process-
ing has become a huge problem. This big data is usually so large that its computa-
tions need to be distributed across thousands of machines so that computations can
be ﬁnished in reasonable time period. Also there are the issues of parallelizing the
computation, distributing data, and handling of failures which require large as well as
complex codes to be dealt with. As a solution to this problem, a new abstraction has
been designed that allows simple computations along with hiding the untidy details
of fault tolerance, parallelization, load balancing, and data distribution in a library.
This abstraction is conceptualized from map and reduces primitives present in Lisp
and in other languages. Most of computations involve applying map operation to
each “record” in the input. This computes a set of intermediate key and value pairs.

Parallel Bat Algorithm-Based Clustering Using MapReduce
75
Then a reduce operation is applied to all the values that share same key, so that the
derived data is combined appropriately. This model of user-speciﬁed map-reduce
operations allow large computations to be parallelized easily and fault tolerance to
be handled by re-execution.
The paper is organized as follows: Sect. 2 discusses the work done in this ﬁeld.
Basic bat algorithm is brieﬂy described in Sect. 3. The proposed algorithm PBA has
been introduced in Sect. 4. Section 5 presents the experimental results, and Sect. 6
concludes the work.
2
Related Work
With the increasing complexity and size of the data, distributed computation has
become quite popular in recent years. Apart from processing of big data, distributed
computing is widely used for the evolutionary computation and machine learning.
When the size of data is too large and it is also unstructured, scaling of the machine
learning techniques is needed to cope with it. It have been seen that when the search
space is large in the case of evolutionary computation, then traditional sequential
algorithms are not able to give satisfactory result in the speciﬁed time. In such case,
distributed evolutionary computation becomes important. There are so many dis-
tributed computation models present in the literature such as GPU, CUDA, Cloud
and Map Reduce-based implementation. Among all these, Map Reduce model is
the recent research hot spot because of its simplicity and robustness. Dean et al. [1]
given a MapReduce model for data processing on large clusters that have transformed
the world of data processing and given the birth to big data processing platforms
such as Hadoop.
Kim et al. [2] proposed a density-based clustering using Map-Reduce architecture
which is robust to ﬁnd the clusters with varying densities. The experimental results
show that the proposed algorithm is found robust for the massive data applications of
real-life data which is used for the experiments, and it is observed that the execution
time decreases quite rapidly with increasing the number of machines. Clustering is
the most popular machine learning techniques used in the industries and academics.
With the evolution of massive data in the recent years, it becomes diﬃcult to manage
it with the traditional algorithms in the stand-alone systems.
Apart from the volume issue, velocity and variety of the data are also increasing
rapidly. Cui et al. [3] proposed a k-means clustering algorithm which overcome the
problem of iterations. The experiments performed on the clusters show that the
proposed algorithm is eﬃcient, robust, and scalable. Elsayed et al. [4] proposed
ontology-based clustering algorithm for handling massive data. Amazons elastic
MapReduce was used to perform the experiments. Li et al. [5] in his paper per-
formed k-means clustering with bagging. Experiments are performed on four node
cluster. Again results show that the execution time is decreasing with the increasing
number of nodes. Remote sensing data size is too large, and traditional MATLAB
implementation of support vector machine for such massive data becomes very time-

76
T. Ashish et al.
consuming process. In such cases, processing data at multiple cores becomes impor-
tant. Cavallaroa et al. [6] used parallel support vector machine for the classiﬁcation
of classes of land cover types. The PiSVM algorithm achieved a good speedup while
maintaining the same training accuracy as compared to traditional serial algorithm.
Nabb et al. [7] have proposed parallel PSO using MapReduce and conﬁrmed that
particle swarm optimization can be naturally implemented in the map reduce model
without compromising with the any aspect of the original algorithm. Yingjie Xu et
al. [8] developed iterative MapReduce-based PSO IMPSO algorithm for minimizing
the thermal residual stress in ceramic composites. The proposed algorithm when
executed on the cluster of 20 nodes has shown quite good speedup as compared to
conventional PSO. Also satisfactory optimization results are obtained as compared
to the conventional PSO.
Xingjian Xu et al. [9] modiﬁed Cuckoo search and implemented it using MapRe-
duce architecture. The proposed algorithm MRMCS is compared with the parallel
PSO using MapReduce (MRPSO). MMRCS shows better results as compared to
MRPSO in terms of convergence in obtaining optimality. The proposed MRMCS
when compared with MRPSO on the same number of nodes has shown two to four
times speedup.
Abhishek Verma et al. [10] scaled Genetic algorithm using map reduce. The pro-
posed models showed the convergence and scalability up to great extent. Author sug-
gested that adding even more resources may enable us in solving compels problems
since no performance bottlenecks were introduced in the implementation. Filomena
Ferrucci et al. [11] given a framework for Genetic algorithm on Hadoop and tested
it on three data sets. The developed framework showed quite promising results. The
framework given by the author is can also be executed in the cloud environment with
good performance.
3
Bat Algorithm (BA)
Bat algorithm is basically designed for the continuous optimization problems. This
algorithm is inspired by the behavior of bats to catch their prey through echo. Micro-
bats can ﬁnd their prey as well as discriminate various types of insects even in the
darkness by their echolocation property. Bat algorithm has its two major advantages:
The ﬁrst is frequency tuning and second is their emission rate. By using these two
properties, bats can make a control between their exploration and exploitation. To
mimic the behavior of bats, this algorithm uses the frequency-based tuning and pulse
emission rate changes. This makes implementation simpler and better convergence
when compared with other meta-heuristic algorithms. Also, BA maintains a balance
of exploration and exploitation because keeping a simple ﬁxed ratio of exploration
to exploitation will not necessarily be an eﬀective strategy.

Parallel Bat Algorithm-Based Clustering Using MapReduce
77
Algorithm 1 Bat Algorithm (BA)
Randomly initialize the initial population and velocity of N bats
Set the value of pulse frequency fi, pulse rates ri, and loudness Ai
Evaluate the ﬁtness ﬁt of each bat
while stopping criteria is not satisﬁed do
Adjusting the frequency fi by Eq. (1)
Compute the velocity v by Eq. (2)
Update the location by Eq. (3)
if (rand > ri) then
Select candidate solution among best solutions
Generate a local solution using the selected best solution
end if
Create a new solution by ﬂying randomly
if (rand < Ai & f(xi) > f(x∗)) then
Accept the new solutions
Improve the value of ri
Reduce the value of Ai
end if
Rank the bats;
Compute the best x∗
end while
fi = fmin + (fmax −fmin)𝛽,
(1)
vt
i = vt−1
i
+ (xt
i −x∗)fi,
(2)
xt
i = xt−1
i
+ vt
i,
(3)
where, 𝛽∈[0,1]
4
Parallel Bat Algorithm
The main motivation behind the proposed parallel bat algorithm (PBA) is to lever-
age the strength of bat algorithm and to make it fast by map-reduce architecture. The
advantage of bat algorithm is that it have a proper balance between exploration and
exploitation. Generally, when the data size becomes large, then sequential evolu-
tionary algorithms are not able to provide results in reasonable amount of time. PBA
algorithm is designed to handle the large data sets by distributing data sets on diﬀer-
ent number of nodes and processing them in a parallel way. The proposed algorithm
works in three modules.
∙Bat Movement: The location of the bat is updated. If any bat is crossing the bound-
ary of the search space, then it is reinitialized.
∙Fitness Calculation: In this step, ﬁtness of all the bats is calculated.
∙Reduce Phase: Output from all the mappers is gathered, and the current best bat
is updated.

78
T. Ashish et al.
Bat Movement and Fitness Calculation modules are implemented to improve the
ability of the bat algorithm for mining massive data sets and updating population
set. The key value pair of PBA is associated with each Bat by a numerical ID named
batID as the key, and the bat information is kept in the value. The bat information
contains bat-ID (batID), bat location (bat-loc), best bat location (bb-loc), bat ﬁtness
value (bat-ﬁt), and best bat ﬁtness value (bh-ﬁt). The bat-loc and bb-loc are the struc-
ture of the cluster centroids. The input to the Fitness module, which is responsible
for calculating the ﬁtness of each bat, are the input data set and the output of the Bat
Movement module. After the Bat Movement and Fitness modules are ﬁnished, the
Reduce module updates the information of each bat by combining the two output
ﬁles from the other two modules and then sends the bats to the next iteration.
4.1
Bat Movement Module
The goal of the Map-Reduce job is to move bats to the new location in the BatMove
module. The parameters key, bat component, frequency, and loudness are initialize.
After that, the new bat locations are calculated in lines 613. In the ﬁrst iteration, the
bat location will be the output to the reduce function directly because it does not
have the current best bat location yet. In the second iterations, the new bat location
is generated. The distance between the bat and the current best bat is calculated.
The map function outputs the bat ﬁtness to the reduce function by using the emit
function when the bat has been moved. The reduce function gets the input, a list of
bats produced by the map function. Reduce function outputs the pairs of bats directly.
All the bats get the new location after the BatMove module is done.
4.2
Calculate Fitness
Mapper job is to compute the ﬁtness of each bat in the calculate Fitness module.
Each map function reads data about all the bats from the distributed cache for the
calculation of the ﬁtness values. The input data is split into smaller blocks, and each
map function runs on one block. The distance between the cluster center of bat-
loc and each data point is calculated by the getmin distance function. After getting
the minimum distance of the bat, a new pair of batID and mindistance is formed
and output to the reduce function. The reduce function will get the values in the
summarized form. The reason is that the key value pairs are grouped by key and will
be combined as list and send to the reduce function of Map-Reduce.

Parallel Bat Algorithm-Based Clustering Using MapReduce
79
Fig. 1
Parallel bat algorithm on map-reduce
4.3
Combine module
The goal of the combine module is to merge the output ﬁles from the Bat Movement
module and the calculate Fitness module to refresh the bat information about bat-loc,
bat-ﬁt, and best-ﬁt. The ﬁtness values of each bat is assigned to the bat-ﬁt according
to the batID number. Now in order to get the best bat, all the ﬁtness values from the
ﬁtness output module are compared. After getting the best-ﬁt, the best bat-loc can
be found from the output ﬁle of the Bat Movement module by checking the batID.
After Refreshing the bat information, the new bats are sent to the Bat Movement
module to start the next iteration until the terminate criterion has been met (Fig. 1).
The pseudocode of the PBA is presented in Algorithms 2–3.
5
Experimental Results
The experimental environment is a Hadoop cluster composed of four computers.
All the computers are desktop with Intel Core i5-intel (2.30 GHz * 8), 4 GB RAM,
and 1 TB hard disk. In the cluster, one of the desktop is set as the master, while the
remaining computers are set as the slave nodes to do the MapReduce jobs. In order
to implement proposed algorithm, Hadoop 2.6.0 is used for the MapReduce pro-
gramming model, and the Java version is 1.7.0. The proposed algorithm is validated
on ﬁve benchmark data sets: Iris, Glass, Wine, Magic and Poker Hand. The quality
of clustering is tested using the total intra-cluster distance given by the PBA algo-
rithm and is compared with the well-known particle swarm optimization algorithm.
Each data set is carried out 15 times to get the average ﬁtness value. Table 1 shows

80
T. Ashish et al.
Algorithm 2 PBA : Bat Movement Module
/* Map function */
map(Key: batID, Value: bat)
batID = Key
bat = getInfo (Value)
Generate a random number in [0,1]
//Moving bats
for each i on dimension of bat_loc do
if ﬁrst iteration then
write( batID, bat)
else
bat.bat_loci+ = rand ∗(bat.bh_loci −bat.bat_loci) ∗fi
end if
end for
if rand > ri then
Select a solution among the best one
Generate a local solution in the proximity of the best solution
end if
if rand < Aif(bati) < f(batbest) then
Accept new solution
Decrease loudness and increase pulse rate
end if
bat.update(batID)
write(batID, bat)
/* Reduce function */
Reduce(Key: batID, Value: bat_list)
for each bat in the bat_list do
write(batID, bat)
end for
Algorithm 3 PBA : Bat Fitness Module
/* Map function */
map(Key: dataID, Value: data)
dataID = Key
data = Value
//Select the bats from the Bat Movement Module
batList = getInfo (output of Bat Movement)
// Calculating minimum distance
for each bat of the batList input do
mindistance = readmindistance(data, bat.bat_loc)
write(bat.batID, mindistance)
end for
/* Reduce function */
Reduce(Key: batID, Value: mindistance_list)
for each mindistance in mindistance_list do
sum+ = mindistance
end for
that the PBA algorithm outperforms PSO for all the data sets. The advantage of the
PBA algorithm is in that the bats make a proper balance between the exploration and
exploitation. In order to test the measure of the speedup, we have run our algorithm
on the cluster of four nodes. Table 2 contains the running time on the diﬀerent num-
ber of nodes. It can be observed from the Table 2 that proposed algorithm is showing
good speedup as the size of the data set is increasing.

Parallel Bat Algorithm-Based Clustering Using MapReduce
81
Table 1
Intra-cluster distance between considered algorithms
Algorithm
50
100
100
100
50
Iris
Glass
Wine
Magic
Poker Hand
PBA
106.48
333.11
17163.32
1274574.42
660627.9
PSO
130.5
340.5
17888.77
1648270.8
6707348.5
Table 2
Running time (in seconds) with increasing number of nodes
Iteration
Single node
Two nodes
Three nodes
Four nodes
Iris
59.1
58.9
58.9
58.7
Glass
60
60
59
59.3
Wine
115
116.5
115.2
116.4
Magic
435
333.4
218.1
215.7
Poker Hand
27719.7
5077.1
4766.2
4235.2
6
Conclusion
In this paper, we proposed a novel clustering algorithm called parallel bat algorithm
(PBA) to solve the massive data set clustering problems. The proposed algorithm
leverages the strength of the bat algorithm and MapReduce. The simulation results
show that PBA algorithm can be eﬃciently used for clustering large data set. Also
simulation results show that the proposed algorithm performs very well when tested
multiple nodes. In the future, the computation time can be reduced by implement-
ing it on the spark architecture. Also the future work will include the testing of the
proposed algorithm on some real-time massive data set.
References
1. D. Che, M. Safran, and Z. Peng, “From big data to big data mining: challenges, issues, and
opportunities,” in Database Systems for Advanced Applications, 2013.
2. X. Cui, P. Zhu, X. Yang, K. Li, and C. Ji, “Optimized big data k-means clustering using mapre-
duce,” The Journal of Supercomputing, vol. 70, pp. 1249–1259, 2014.
3. J. Dean and S. Ghemawat, “Mapreduce: simpliﬁed data processing on large clusters,” Commu-
nications of the ACM, vol. 51, pp. 107–113, 2008.
4. A. Elsayed, H. M. Mokhtar, and O. Ismail, “Ontology based document clustering using mapre-
duce,” arXiv preprint arXiv:1505.02891, 2015.
5. L. D. Geronimo, F. Ferrucci, A. Murolo, and F. Sarro, “A parallel genetic algorithm based
on hadoop mapreduce for the automatic generation of junit test suites,” in Software Testing,
Veriﬁcation and Validation (ICST), 2012 IEEE Fifth International Conference on, 2012.
6. Y.-J. Gong, W.-N. Chen, Z.-H. Zhan, J. Zhang, Y. Li, Q. Zhang, and J.-J. Li, “Distributed evolu-
tionary algorithms and their models: A survey of the state-of-the-art” Applied Soft Computing,
vol. 34, pp. 286–300, 2015.

82
T. Ashish et al.
7. Y. He, H. Tan, W. Luo, H. Mao, D. Ma, S. Feng, and J. Fan, “Mr-dbscan: an eﬃcient paral-
lel density-based clustering algorithm using mapreduce,” in Parallel and Distributed Systems
(ICPADS), 2011 IEEE 17th International Conference on, 2011.
8. H.-G. Li, G.-Q. Wu, X.-G. Hu, J. Zhang, L. Li, and X. Wu, “K-means clustering with bagging
and mapreduce,” in System Sciences (HICSS), 2011 44th Hawaii International Conference on,
2011.
9. A. W. McNabb, C. K. Monson, and K. D. Seppi, “Parallel pso using mapreduce,” in Evolution-
ary Computation, 2007. CEC 2007. IEEE Congress on, 2007.
10. A. Verma, X. Llorà, D. E. Goldberg, and R. H. Campbell, “Scaling genetic algorithms using
mapreduce,” in Intelligent Systems Design and Applications, 2009. ISDA’09. Ninth Interna-
tional Conference on, 2009.
11. Y. Xu and T. You, “Minimizing thermal residual stresses in ceramic matrix composites by
using iterative mapreduce guided particle swarm optimization algorithm,” Composite Struc-
tures, vol. 99, pp. 388–396, 2013.

Predicting Strategic Behavior Using Game
Theory for Secure Virtual Machine
Allocation in Cloud
Priti Narwal, Shailendra Narayan Singh and Deepak Kumar
Abstract In cloud computing, security is a very crucial issue to be taken care of.
So, game theory can be used as a strategic tool in decision making among users to
protect their virtual machines that are hosted on same hypervisor from external as
well as internal attacks. In this paper, a payoff matrix and a decision tree are
constructed for ‘n’ number of users, and problem of interdependency is dealt with.
Then, a unique user is chosen and his choices of investing in security are evaluated
till equilibrium is achieved, when attacker launches his ‘m’ number of strategies.
Finally, payoff values of each are calculated, and a decision tree is built which
shows the strategies for both attacker and jth user, and Nash Equilibrium is also
calculated which shows the best response for both players against each other.
Keywords Cloud computing ⋅
Dynamic games ⋅
Game theory ⋅
Nash
equilibrium ⋅Payoff ⋅Secure allocation ⋅Strategy
1
Introduction
Noncooperative games can be represented in a strategic form [1, 2] and lays good
foundation in game-theoretic decision making. The main objective of these games
is to ﬁnd out a deﬁnite outcome or optimal solution of a game, if exists. A solution
refers to the set of actions taken by a rational player [3] on the basis of information
available but in an optimal way. Noncooperative games can be analyzed by
specifying number of players involved in a game, their actions, strategies, payoff
P. Narwal (✉) ⋅S.N. Singh
Amity School of Engineering and Technology, New Delhi, India
e-mail: narwal.priti@gmail.com
S.N. Singh
e-mail: snsingh36@amity.edu
D. Kumar
Amity Institute of Information Technology, Amity University, Noida, Uttar Pradesh, India
e-mail: deepakgupta_du@rediffmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_8
83

received. A noncooperative ﬁnite game [2, 3] can be represented in the form of a
matrix [1–3] where rows and columns both constitute the strategies of players and
each element of matrix is a pair of numbers that gives the payoffs for players
involved in the game. If there are 2 players involved in a game, then the strategy of
ﬁrst player is indicated by the number of rows present in a matrix, and similarly, the
strategies of second player are represented by number of columns [1] present in a
matrix. So, a noncooperative game can be represented in the form of a matrix. Each
matrix game or strategic game can be solved by predicting the strategies taken by
each player because all players have now complete information and then using them
to ﬁnd out the possible outcomes in a game.
Game theory can be used in any application [4–6] where optimal choices have to
be made among many options available. Noncooperative game can be applied to
any application where players refused to cooperate or negotiate and take compet-
itive decisions with each other. It provides a quantitative framework [2] for mod-
eling the interaction between attackers and defenders as well as to predict the
behavior of attacker in security-related games.
These games can be represented in strategic or in an extensive form [2, 5] with
the help of payoff matrix and decision trees. Payoff matrix is used to analyze
sequential and simultaneous move games, and decision trees [1] are used in deci-
sion analysis which helps in identifying the most optimal strategy to achieve a goal.
It covers the possible strategies, outcomes, payoffs, and moves of each player
involved in a game.
Payoff matrices are also used to calculate the stable point or saddle point [1, 2, 7]
in a game. At this point, a game achieves equilibrium because no player would want
to deviate from this point at any cost as it gives them the stable solution of a game.
So, if a game has Nash Equilibrium [2, 7], then it means that there exists a set of
strategies by which a player would optimize its utility by observing other players’
actions. A Nash Equilibrium gives a stable game because players reached a point
where no player has an incentive to deviate from his current Nash strategies.
So, game theory models provide a strategic interaction [1, 2, 8] among players in
which players take decisions either independently or simultaneously, and their
actions can also be sequential or simultaneous. When a player takes sequential
actions, then they are aware of opponent’s previous actions, but if a player takes its
actions in a simultaneous manner, then players may become unaware of other
players’ actions.
Game theory can also be used to capture the nature of attacks in network appli-
cations [2, 4, 5, 9] where it deals with scenarios where there are at least two players
that are interacting in an attempt to maximize their intended objectives and minimize
the opponent’s. The defender’s decision strategies are closely related to those of
attacker [9] and vice versa whether defensive strategy is effective will not only
depend on defender’s own behavior but also depend on the attacker’s strategy. It can
also be used in cloud computing [2, 5, 7, 9] where security is a very critical issue.
Strategies can be decided by users to protect their virtual machines [7, 10] from
attackers that are hosted on the same hypervisor. So, secure allocation [7, 11, 12] of
virtual machines is an issue that needs to be considered and therefore, a game model
84
P. Narwal et al.

can be used to predict the behavior of attacker as well as to protect the virtual
machines from attacks. Some other factors such as workload balancing [13, 14], low
power consumption [6], and security of hypervisor [11, 12] on which virtual
machines are hosted can also be taken care of by using game-theoretic approaches.
The rest of the paper is organized as follows: Sect. 2 gives a review of literature
related to security of virtual machines in cloud where both attackers and users
reﬂect strategic behavior while making decisions against each other. In Sect. 3, a
game model is constructed for ‘n’ number of users where a payoff matrix and a
decision tree are constructed which shows that when Nash Equilibrium is achieved,
the player would not like to move from its current strategy. Finally, Sect. 4 con-
cludes the paper and states further work that can be done in this area.
2
Literature Review
A lot of work has been done in game theory for security-related measures in cloud
computing. The major issue that has been addressed so far is the secure allocation
of virtual machine instance to multiple clients and to prevent external as well as
internal [15] attacks.
Authors used game theory to address the issue of interdependency [7, 10] or
negative externality among users. They developed a game model for secure allo-
cation of virtual machine instances for 3 players in which 1 player is assumed as an
attacker and others as normal users [7, 10], and they gave them binary choices to
opt for either a secure or an unsecure hypervisor. They also computed Nash
Equilibrium for this game to identify whether the players would deviate from their
current actions or not. Some factors such as cost for investing in security and
potential losses were also kept in mind while making a choice for hypervisor.
Finally, a payoff matrix is also constructed which shows the payoff values for both
attacker and users and concluded that opting for security is the best option for any
users if he is on the main target.
Yi Han et al. proposed a game theory solution [16] that acts as a countermeasure
for the prevalent co-resident attacks in which attacker tries to co-locate their virtual
machine with the target virtual machine by developing certain VM allocation
policies. Then, the authors did analysis of these 4 virtual machine allocation
policies on platform CloudSim and build game matrices to ﬁnd out the numerical
outcome of the game by using Gambit tool [1]. Along with this, their solution also
focused on effective VM coverage, load balancing, and low power consumption.
Xin Xu et al. proposed a game model [6] to allocate resources in a fair manner in
cloud as well as to reduce the amount of resource fragments that may be created
while allocating resources to virtual machine instances; also proposed an algorithm
called FUGA in which servers with no resources are considered as players and
resources are allocated with the help of an allocation matrix; and then ﬁnally
compared it with Hadoop scheduler which decided whether game has achieved
Nash Equilibrium or not. Fei He et al. proposed a game-theoretic approach to
Predicting Strategic Behavior Using Game Theory …
85

provision servers and routers in a secure manner [17] as they are prone to attacks,
and hence, they achieved their result by minimizing information about infrastruc-
ture and by computing Nash Equilibrium in polynomial time.
Maha Jebalia et al. did the comparison and overview of several existing
game-theoretic approaches [9] to allocate resources in a secure manner. The authors
did comparison and provided efﬁcient solution to resource allocation problem in
cloud. They also gave an overview of several scheduling algorithms and catego-
rized them as traditional, heuristics, and economic based and found out that cloud
providers have to enable some security technologies such as ﬁrewalls, encryption,
intrusion detection, or prevention to outcome previous security failures. These
security technologies may incur losses while provisioning resources in clouds. So,
they concluded that an optimized problem to allocate resources [18] can only be
constructed when factors such as losses incurred and security requirements are
taken into consideration.
3
Game Model for ‘N’ Number of Users
This section describes the proposed payoff matrix for ‘n’ number of players and
attacker having ‘m’ number of strategies that acts upon 2 hypervisors. Some
assumptions are made in this model.
All players are considered as rational agents [2] trying to maximize their payoffs
and minimize others.
The 2 hypervisors on which user’s virtual machines are hosted can be assumed as
a secure or unsecure. So, if there are ‘n’ users (U1, U2,…Un) and users from U1,
U2,…Uj−1 chose unsecure hypervisors to host their virtual machines and users from
Uj+1…Un would opt for secure hypervisor, then a unique user Uj (U1, U2,…Uj−1, Uj,
Uj+1…Un) is given a choice to invest either in a secure hypervisor or not [7], and on
the basis of attacker’s strategies and jth user’s choices to opt for security or not, a
payoff matrix is constructed that gives the payoff values of attacker as well as user.
Authors have already proposed a model [7, 10] and also constructed a matrix for
3 users with 1 user as attacker and 2 as normal users that are making these binary
choices based on cost and potential loss constraints. So, in this game, a matrix is
constructed for more than 3 users and a unique user ( jth user) is given this choice
and then their payoffs are calculated.
There exists some amount of interdependency [7, 10] between users because if
an attacker attacks on a virtual machine which is hosted on an unsecure hypervisor,
then as a result the hypervisor can also get malicious and the virtual machines
hosted on the same hypervisor also get vulnerable to the attack. In this way, the
co-resident attack or issue of interdependency exists. But if a user opts for a secure
hypervisor, then its virtual machine can be saved from attacks [7, 10] to a greater
extent Along with it, if a user invests in a secure hypervisor, he has to pay some
cost, C regarding his investment in security which gets deducted from its calculated
86
P. Narwal et al.

payoff value. Therefore, a matrix is constructed with ‘n’ number of users as U1,
U2,…Uj−1, Uj, Uj+1,…Un in which users U1, U2,…Uj−1 are hosted on an unsecure
hypervisor and users from Uj+1…Un are hosted on a secure hypervisor.
In this payoff matrix, the probability of attack when a user Ui has invested in an
unsecured hypervisor is denoted by Pn. And the probability of attack when user Ui
has invested in a secured hypervisor is Ps. If a user incurs some loss on account of
being attacked, then it is denoted as Ii.
The ‘m’ number of strategies taken by an attacker to attack virtual machine and
hypervisor can be seen as X1, X2,…Xj−1, Xj, Xj+1,…Xm (Table 1).
The probability of hypervisor being compromised when under attack it is written
as H0 and the probability of hypervisor being unaffected when attacked can be H1.
The total beneﬁt gained by a user can be assumed as R from which cost for
investment in security is always deducted.
In the following proposed matrix, attacker’s strategies are represented in a row
and entries in columns gives jth user’s choices. If an attacker chooses his strategy
X1 and user j has decided to host its virtual machine on an unsecured hypervisor,
then payoffs for M(X1, U) can be calculated for both attacker and users. The
probability of loss of user 1, i.e., PnI1 on unsecured hypervisor, can be calculated as
their chance of compromise, Pn, and loss incurred by it, Ii, and if the virtual machine
of user 1 gets affected, then the unsecure hypervisor on which it is hosted also gets
Table 1 Strategic choices of jth user with payoffs received
jth User’s choices
Payoffs
Unsecure hypervisor
(UH-V)
Secure hypervisor (SH-V)
Attacker’s
strategies
X1
Attacker’s
payoff
PnI1 + PnH0I2 + PnI3 +…
PnIj−1 + PnIj
PnI1 + PnH0I2 + PnI3 +…
PnIj−1
User’s payoff
R−PnIj
R−C
X2
Attacker’s
payoff
PnI2 + PnH0I1 + PnI3 +…
PnIj−1 + PnIj
PnI2 + PnH0I3 +…PnIj−1
User’s payoff
R−PnIj
R−C
X3
Attacker’s
payoff
PnI3 + PnH0I1 + PnI2 +…
PnIj−1 + PnIj
PnI3 + PnH0I1 + PnI2 +…
PnIj−1
User’s payoff
R−PnIj
R−C
Xj−1
Attacker’s
payoff
PnIj−1 + PnH0I1 + PnI2 +
…PnIj
PnIj−1 + PnH0I1 +
PnI2 +…
User’s payoff
R−PnIj
R−C
Xj
Attacker’s
payoff
PnIj + PnH0I1 + PnI2 +…
PnIj−1
PsIj + PsH1Ij+1 +…PsIn
User’s payoff
R−PnIj
R−C−PsIj
Xj+1
Attacker’s
payoff
PsIj+1 + …PsIn
PsIj+1 + … + PsH1Ij +…
PsIn
User’s payoff
R
R−C
Predicting Strategic Behavior Using Game Theory …
87

malicious. So, the probability of hypervisor under attack H0 is also considered as
the attack on other virtual machines will be an indirect one, going through
hypervisor which is PnH0I2. The user’s payoff is calculated by reducing overall loss
of user j from its earned beneﬁt, i.e., R−Pn Ij. Similarly, if jth user opts for a secure
hypervisor, then the payoff for attacker can be calculated simply by not including
loss of user j because it is now hosted on a secure hypervisor and user’s payoff can
be received by reducing its cost of investing in secure hypervisor from its earned
beneﬁt.
For jth user, payoff for attacker can be calculated by including chance of com-
promise of user j, Pn, along with its loss, Ij, which is Pn Ij along with loss of other
users hosted on the same unsecure hypervisor, but while calculating users payoff
only in case of unsecured hypervisor, loss of jth user is reduced from its earned
beneﬁt. When user j hosts its virtual machine on secure hypervisor, then user’s
payoff can be calculated by reducing the cost, C, that user j incurs while hosting its
virtual machine on a secure hypervisor as well as probability of loss when attacked
on that hypervisor and its loss, PsIj, from its earned beneﬁt, R.
For attacker’s ‘n’ number of strategies, 2n outcomes can occur. In this paper, a
prototype is taken for a decision tree with 4 strategies and 16 outcomes (24) as
attacks on user U1, Uj−1, Uj, and Uj+1 by choosing strategies as X1, Xj−1, Xj, and
Xj+1 where user j would make a binary decision for secured or unsecured hyper-
visor (Fig. 1).
A decision tree is constructed by ﬁrst assuming the probability of a user selecting
an unsecured and a secured hypervisor is {H0, H1} simultaneously.
Then, probabilistic outcomes for selecting a secured or unsecured hypervisor for
allocation of virtual machine are {{H0, H0, H0},{H0, H0, H1},{H0, H1, H1},{H1,
H1, H1}} for each strategy of an attacker, and according to the Nash Equilibrium
obtained after playing this game, user j will make its decision to opt for the particular
strategy. And after considering the choice of jth user for attacker’s ‘n’ number of
strategies for the above payoff matrix by using Game Theory Explorer software which
also assigns random payoff values to all the players’ moves. Then, by determining
attacker’s best response to jth user choices, Nash Equilibrium is also calculated. After
calculating Nash Equilibrium, user j may select strategy u(x, y) = {55, 31} or
{H0, H0, H1} against attacker’s strategy x1 for users hosted on unsecured hypervisor.
The process or workﬂow of strategies of a user while opting for secure or
unsecure hypervisor is depicted in Fig. 2 where a user has to make a decision again
at server allocation stage which may lead to either safe state or co-resident attacks.
A payoff matrix with random payoff values assigned as shown in Fig. 2 is
calculated with the help of game theory explorer software [1] in which numbers at
the bottom of a cell represent payoff values of player 1, i.e., attacker X, and
numbers at the top of a cell represent payoff values for player 2, i.e., user j. Nash
equilibrium can be calculated by selecting best response of player 1 when player 2
chooses strategy 1; i.e., when user j chooses strategy 1, then attacker would choose
its best response (highest value) among its all responses. The payoff values in oval
88
P. Narwal et al.

brackets represent the attacker’s best responses, and the values in square brackets
represent the best responses of user j. So, in this case, attacker would choose its
value as 17 out of {17, 0, 1, 8}. Similarly, when attacker chooses its strategy as 1,
then user j would choose its best response which is 31 out of {14, 31, 20, 3}. So,
when both players receive their best strategies which is in cell P(1, 2) = {55, 31},
where 55 is the best response for player 1 and 31 is the best response for player 2,
then only the Nash Equilibrium is received in the game. Here, players received their
Nash Equilibrium, when attacker opts for strategy 1 and user j opts for strategy 2.
The best strategies opted by attacker X are with payoff values {17, 55, 59, 24},
and with user j, best strategies opted are {31, 65, 43, 44} (Fig. 3).
Fig. 1 Decision tree for
players
Predicting Strategic Behavior Using Game Theory …
89

4
Conclusion and Future Work
In cloud computing, security issue arises when a user with less potential loss that is
hosted on the same virtual machine as the user with more potential loss neglects
security. In this paper, a payoff matrix is calculated that gives an idea about the
Fig. 2 Process ﬂow for user
strategies for hypervisor
selection
Fig. 3 Payoff matrix, P for
both players
90
P. Narwal et al.

strategic decisions of both attacker and user in this type of attack scenario. A de-
cision tree is also constructed keeping in mind their strategies by using game theory
explorer software and by assigning random values to both players; a payoff matrix
is also calculated. This only shows the game model in their initial form as a
prototype. Further work can be extended by implementing this proposed model in a
cloud environment by assigning payoff values to the variables of the proposed
payoff matrix that calculates the payoffs of attacker as well as jth user and then
validating the values achieved to determine Nash Equilibrium for both players’
strategies.
References
1. Rahul Savani, Bernhard von Stengel, Game Theory Explorer-Software for the Applied Game
Theorist, Article in Computational Management Science 12(1) March 16, 2014, doi:10.1007/
s10287-014-0206.
2. Xiannuan Liang, Yang Xiao, Game Theory for Network Security, IEEE Communications
surveys and tutorials, Vol. 15, No. 1, First Quarter 2013, doi:10.1109/surv.2012.062612.
00056.
3. Mohammad Hossein Manshaei, Quanyan Zhu, Tansu Alpcan, Tamer Basar, Game Theory
Meets Network Security and Privacy, Game Theory Meets Network Security and Privacy,
ACM Computing Surveys, Vol. 45, Issue 3, June 2013, doi:10.1145/2480741.2480742.
4. Cornell Tech, Michael K. Reiter, Cross-VM side channels and their use to extract private
keys, Proceedings of the 2012 ACM conference on computer and communications security,
pp. 305–316, October 2012, doi:10.1145/2382196.2382230.
5. Priti Narwal, Deepak Kumar, Mayank Sharma, A Review of Game–Theoretic Approaches for
Secure Virtual Machine Allocation in Cloud, 2nd international Conference on Information and
Communication Technology for Competitive Strategies, March 2016, Conference Proceed-
ings by ACM-ICPS, ISBN No. 978-1-4503-3962-9, doi:10.1145/2905055.2905152.
6. Christian Esposito, Massimo Ficco, Francesco Palmieri, Aniello Castiglione Smart Cloud
Storage Service Selection Based on Fuzzy Logic, Theory of Evidence and Game Theory,
IEEE Transactions on Computers, Volume: 65, Issue: 8, doi:10.1109/TC.2015.2389952.
7. Charles A. Kamhoua, Luke Kwiat, Kevin A. Kwiat, Joon S. Park, Ming Zhao, Manuel
Rodriguez, Game Theoretic Modeling of Security and Interdependency in a Public Cloud,
IEEE International Conference on Cloud Computing, 2014, doi:10.1109/cloud.2014.75.
8. Parvathy S. Pillai, Student Member, IEEE, and Shrisha Rao, Senior Member, IEEE, Resource
Allocation in Cloud Computing using the Uncertainty Principle of Game Theory, IEEE
Systems Journal, Volume. 10, Issue. 2, pp. 637–648, doi: 10.1109/JSYST.2014.2314861.
9. Maha Jebalia, Asma Ben Letaifa, Mohamed Hamdi, Sami Tabane, A Comparative Study on
Game Theoretic Approaches for Resource Allocation in Cloud Computing Architectures,
Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises, June
2013, ISSN: 1524-4547, Page(s): 336–341, doi:10.1109/WETICE.2013.11.
10. Luke Kwiat, Charles A. Kamhoua, Kevin A. Kwiat, Jian Tang, Andrew Martin,
Security-aware Virtual Machine Allocation in the Cloud: A Game Theoretic Approach,
IEEE 8th International Conference on Cloud Computing, July 2015, ISSN: 2159-6182, Page
(s): 556–563, doi:10.1109/CLOUD.2015.80.
11. Chunxiao Li, Anand Raghunathan, Niraj K. Jha, Secure Virtual Machine Execution under an
Untrusted Management OS, IEEE 3rd International Conference on Cloud Computing, July
2010, ISSN:2159-6182, Page(s): 172–179, doi:10.1109/cloud.2010.29.
Predicting Strategic Behavior Using Game Theory …
91

12. Lirim Osmani, Salman Toor, Miika Komu, Matti J. Kortelainen et al., Secure Cloud
Connectivity for Cloud Applications, IEEE Transactions on Services Computing, 2015,
doi.10.1109/TSC.2015.2469292.
13. Xin Xu and Huiqun Yu, A Game Theory Approach to Fair and Efﬁcient Resource Allocation
in Cloud Computing, Hindawi Publishing Corporation mathematical Problems in Engineer-
ing, Volume 2014, Article ID 915878, 14 pages, http://dx.doi.org/10.1155/2014/915878.
14. Zexiang Mao, Jingqi Yang, Yanlei Shang, Chuanchang Liu and Junliang Chen, A Game
Theory of Cloud Service Deployment, IEEE Ninth World Congress on Services, 2013,
doi:10.1109/services.2013.
15. Jakub Szefer, Eric Keller, Ruby B. Lee, Jennifer Rexford, Eliminating the Hypervisor Attack
Surface for a More Secure Cloud, CCS’11, October 17–21, 2011, doi:10.1145/2046707.
2046754.
16. Yi Han, Tansu Alpcan, Jeffrey Chan, Christopher Leckie, Security Games for Virtual
Machine Allocation in Cloud Computing. Decision and Game Theo ry for Security. Springer
International Publishing, 2013, page(s): 99–118, doi:10.1007/978-3-319-02786-9_7.
17. Fei He, Jun Zhuang, Jun, et al., Cloud Computing Infrastructure Robustness: A Game Theory
Approach, International Conference on Computing, Networking and Communications, Cloud
Computing and Networking Symposium, Jan 2012, doi:10.1109/ICCNC.2012.6167441.
18. Meng-Ru Shie, Chien-Yu Liu, Yi-Fang Lee, Yu-Chin Lin, Kuan-Chou Lai, Distributed
Scheduling Approach based on Game Theory in the Federated Cloud, 2014 International
Conference on Information Science & Applications (ICISA), ISSN: 2162-9048, Page(s): 1–4,
doi:10.1109/ICISA.2014.6847388.
92
P. Narwal et al.

Low-Size Cipher Text Homomorphic
Encryption Scheme for Cloud Data
Manish M. Potey, C.A. Dhote and Deepak H. Sharma
Abstract Data security is a major concern in cloud computing. It must satisfy three
goals of security in computing—integrity, conﬁdentiality, and availability. User or
cloud service provider (CSP) can perform operations on cloud data without per-
forming decryption by using homomorphic encryption. Many algorithms are
available for homomorphic encryption. These algorithms are generating large size
cipher text. In this chapter, small-size cipher text homomorphic encryption algo-
rithm is proposed. This scheme is a modiﬁed scheme proposed by Dijk et al. The
experimentation is performed on data stored on DynamoDB of Amazon Web
Services (AWS) public cloud. All arithmetic computations are performed on
low-size encrypted data. Data can be downloaded on users machine as per the
requirement and then decrypt it.
Keywords Data security ⋅Cloud computing ⋅Homomorphic encryption
AWS ⋅DynamoDB ⋅CSP ⋅Security goals
1
Introduction
Security at all levels—application, host and network is necessary in cloud com-
puting. The data available at all these levels must ensure conﬁdentiality, integrity
and availability. The data has several states from its creation to it destruction. These
states includes: create, transfer, use, share, store, archive and destroy. This chapter
M.M. Potey (✉) ⋅D.H. Sharma
Department of Computer Engineering, K. J. Somaiya College of Engineering,
Mumbai, India
e-mail: manishpotey@somaiya.edu
D.H. Sharma
e-mail: deepaksharma@somaiya.edu
C.A. Dhote
Department of Information Technology, PRMIT&R, Amravati, India
e-mail: vikasdhote@rediffmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_9
93

focuses on security of cloud data at rest. Data security in cloud computing must also
cover security of data in transit and following [1] aspects.
Data Lineage—In cloud computing, the data in cloud is moving from one location
to other. Following the path of data is called as data lineage.
Data remanence—It is an issue when data gets exposed after deletion to the
unauthorized party for an auditor’s assurance.
Data provenance—Provenance means data must be computationally accurate, and it
also possesses integrity. For ensuring conﬁdentiality generally encryption is used.
Normally, to perform computation, it is required to decrypt the data. But
homomorphic encryption (HE) allows computations on encrypted data. So the Data
remains in its encrypted state in most of the processing stages on the cloud. The
results can be veriﬁed by decrypting cipher text.
Homomorphic encryption technique allows user to perform multiple types of
operations on encrypted data. Only one kind of operation is allowed in a partially
homomorphic encryption technique. Figure 1 shows the proposed system.
This chapter is arranged in six sections. Section 2 gives brief outline about related
work carried out for homomorphic encryption algorithms. Section 3 discusses about
existing scheme by Van Dijk, and Sect. 4 explains the proposed low-size cipher text
homomorphic encryption scheme. The implementation of proposed work is described
in Sect. 5; the conclusion of the chapter is given in part 6.
Perform Functions 
on Encrypted data
Store Encrypted Data
Cloud Server
Encrypt text 
using Fully 
Homomorphic 
Encryption 
Scheme
Perform 
Decryption 
Plain Text
End User
Plain Text
End User
Fig. 1 Proposed
homomorphic encryption
scheme
94
M.M. Potey et al.

2
Related Work
Gentry suggested the homomorphic encryption scheme in his [2, 3] paper and
thesis. Public key encryption is used in this scheme. It consists of encryption,
decryption, key generation and evaluation phases.
Privacy homomorphism is proposed by Rivest et al. [4] in 1978 for bank data
application. The hardware conﬁguration is suggested and encryption functions
perform operations without decrypting. But it has limited applicability.
Partial homomorphic scheme is proposed by Paillier. The author in [5] surveys
various homomorphic encryption schemes. This chapter introduces basics of
encryptions to homomorphic encryption. He discussed various schemes with
respect to parameters such as security and efﬁciency.
Aderemi et al. [6] discussed the need of fully homomorphic encryption and other
security issues in cloud computing. Goldwasser and Micali [7] proposed a new
encryption model based on probability. In this new model, trapdoor is replaced by
unapproximable predicate.
Improvement over Gentry’s model is proposed in [8–11]. Y. Govinda Ramaiah
[12] proposed efﬁcient homomorphic algorithm over integer. It is also a variant of
Van Dijk scheme.
Parallel processing of homomorphic encryption was discussed in [13]. Its
experimentation was carried out on CUDA and Open MP. Ryan and Chia-Chu
Chiang [14] proposed a parallel architecture for fully homomorphic encryption.
Some improved homomorphic scheme with comparatively low-size cipher text
and small key are proposed in [8, 10, 11]. The scheme proposed by Gentry was not
efﬁcient [8, 9, 15] due to its large key size and high evaluation time. Additional
noise and bootstrapping was used by researchers while performing homomorphic
encryption. It results in large size cipher text. It increases space complexity
increases. Cipher text size plays major role when there is need to perform some
advance operations such as sorting, indexing and searching on such encrypted data.
So there is need to provide homomorphic encryption scheme which generates
low-size cipher text which can be used for practical application. Such scheme is
proposed in this chapter.
3
Existing Scheme
The FHE scheme is proposed in [8] uses various parameters. These parameters are
as follows:
y
Size of Public key in bits
x
Size of Secret key in bits
w
Noise in bits
v
is number of integers in public key
Low-Size Cipher Text Homomorphic Encryption Scheme …
95

Security parameter of polynomial is s. The noise w is considered as ω(log s). The
secret key x should be greater than w ⋅ϴ (log2 s). To protect against lattice-based
attacks, v is considered as ω(x2log s). v is taken as y + ω(log s). It also uses
additional noise w′ = w + ω(log s). It is considered as f. For better homomor-
phism, parameters are suggested as w = s, w′ = 2 s, x = Ỡ(s2), y = Ỡ(s5) and
v = y + s. This homomorphic scheme contains of following algorithms.
Generate_Key (s): Select secret key KS with odd y bit integer from open interval
[2x−1, 2x). For i = 0, 1, … v. Select random number Ti from the interval [0, 2v/KS)
and another number Ui from interval (−2w, 2w) and calculate Ni = KS ⋅Ti + Ui.
The public key is (T0, T1, …, Ty) and secret key is KS.
Encryption (PKey, Msg ∈{0,1}):
Select a number D from (−2w, 2w) for adding noise. Select a subset I ⊆{1, 2, …, v).
Calculate sum = Σi ∈I Ti and Cipher text CT = (Msg + 2(I + Sum)) mod T0
Decrypt (KS, CT): Calculate Message = (CT mod KS) mod 2
In order to get enough homomorphism with this set of parameters, the complexity is
given as Ỡ(s12), key generation complexity is Ỡ(s12), Encrypt and Decrypt com-
plexity Ỡ(s10).
4
Proposed Homomorphic Encryption Scheme
The modiﬁcation in the existing scheme is carried out and proposed in the fol-
lowing scheme which is used for low-size cipher text homomorphic encryption.
The phases used in this scheme are key generation, encryption and decryption. The
secret key is formed by two parameters J and K. The public is formed by using
parameters P0 and P1. The security parameter is considered as s. Number N to be
encrypted is accepted from user. The security parameter is considered as s. The
primitives used in this scheme are given as follows.
Key Generation (s):
1. Select Secret key J with 64 or 128 bit and K-32 or 16 bit prime number
2. Select D and F as 128-bit prime number
3. Choose s bit random integer K′
4. Generate public key P0 = J * D and P1 = J * F + K * K′
Encrypt (N, P0, P1):
1. Select T1, T2 as s bit random numbers
2. Compute P2 = [T1 * P1] mod P0
3. Output −Cipher text CX = [N + T2 * P2] mod P0
Decrypt (CX, J, K):
1. Output N = (CX mod J) mod K
96
M.M. Potey et al.

The homomorphic addition and multiplications are performed using following
mathematical operations.
• Addition (N1, N2)
– Output Add = N1 + N2
• Multiplication (N1, N2)
– Output Mult = N1 * N2
The outcome of above scheme is given below.
Consider
J = 8446413785904601499,
K = 4100490077,
number
to
be
encrypted N = 21.
The random value of D and F in 128 bit is considered as
D = 278640683995497572714638870337312192737
F = 340230054553406756561037657335566971091
Four bit random number K′ = 13 and compute P0 and P1
P0 = 1371861745717964916234213810391764569281685615748984278123
P1 = 1675091339471794464589829698500481945812240583115781572370
Encrypted value of input N = 21 is
CX = 865717338307096953263193780910933880226535326510013158693
Decrypted value of N is 21.
Proposed scheme complexity is Ỡ(s3), s as security parameter. In encryption,
multiplication is main operation. The s number of bits is multiplied by s2 number of
bits then the bit level operation complexity is comes out to be Ỡ(s3), Decryption
complexity depends on cipher text size. The size of cipher text at bit level is Ỡ(s3).
For performing integer operation, the complexity is comes out be Ỡ(1). So the
overall complexity is Ỡ(s3). The security of this scheme can be computed by
solving the two-element Partial Approximate Greatest Common Divisor.
5
Implementation
The working model of low-cipher text size homomorphic encryption is explained in
this section. The Eclipse IDE for Java EE Developers is used to connect to the
Amazon Web Services (AWS) DynamoDB service. Here, one simple bank example
is considered. On Dynamo DB, two tables are created namely Bank and Keys. Bank
table contains the username, balance (in homomorphic encrypted form) and
Low-Size Cipher Text Homomorphic Encryption Scheme …
97

password. Key table contains username, P_key (J parameter used in scheme) and
R_key (K parameter used in scheme).
In this application, four options are provided, namely addition, subtraction,
check balance and exit. By using Eclipse, user can execute this application and
logged into users account and can perform addition and subtraction operation on
encrypted balance amount ﬁeld. Once all tasks are performed user can logout using
exit option.
The following steps [16] are performed for the implementation
Step 1: Creation of DynamoDB instance on Amazon Web Services.
Step 2: Database Tables are created with proper schema
Step 3: Get the credentials from AWS and perform access controls
Step 4: Install Java SDK and Eclipse Kepler version at user end.
After the installation of AWS SDK on Eclipse framework, the user is available
with all the needed packages.
Step 5: Follow the steps as given in AWS SDK
Step 6: Developing the code in Java for this homomorphic encryption scheme.
Step 7: Finally, execute the code using Eclipse.
5.1
Results
The experimentation is carried out with a plain text as 111. The experimental results
of this scheme by varying size of parameters are provided in the following Table 1.
In Table 1, the proposed scheme generates cipher text of size 58–77 bytes. This
scheme is not inserting additional noise. This scheme is similar to the scheme in [8].
In Paillier encryption scheme, the experimentation is carried out with varying
parameters and plain text as 111. The comparison of proposed scheme, Paillier
scheme and existing Van Dijk Scheme is given graphically in Fig. 2. It is observed
that proposed scheme reduces the cipher text size by 20%.
Table 1 Experimental
results
Parameters values in bits used in
the algorithm
Size of cipher text
J
K
D
F
64
16
128
128
58 bytes
128
16
128
128
77 bytes
128
32
128
128
77 bytes
98
M.M. Potey et al.

5.2
Sample Execution Results
User is allowed to check balance in plaintext. The data on DynamoDB database on
AWS is shown in Fig. 3. Here, the balance of particular username is checked.
The application is executed at client by giving particular username’s credentials
and performed sample operations. It is shown in Fig. 4.
Once code is executed at client, the data on AWS DynamoDB table is updated.
As shown in Fig. 5 balance of ﬁrst user (vikas) is updated.
Fig. 2 Comprarsion of
homomorphic encryption
Fig. 3 Bank table on DynamoDB
Low-Size Cipher Text Homomorphic Encryption Scheme …
99

Fig. 4 Execution at client
Fig. 5 Updated table on AWS after performing operations
100
M.M. Potey et al.

6
Conclusion and Future Work
Data is not exposed in plain text at any stage using Homomorphic Encryption
scheme. Conﬁdentiality goal is preserved. The cipher text created through this
homomorphic encryption scheme is small in size as compared to Van Dijk scheme.
It reduces the cipher text size by 20% as compared to existing scheme. It is an
efﬁcient and practically applicable homomorphic encryption suitable for application
on cloud. The message expansion is low. The overall complexity of this scheme is
Ỡ(s3).
There is a need to evolve various algorithms for searching and querying oper-
ations on encrypted data under this scheme. There is also a need of security analysis
in such low-size cipher text algorithm.
References
1. S. Kumaraswamy, Tim Mather, and Latif S., “Cloud security and privacy: an enterprise
perspective on risks and compliance”. “ O’Reilly Media, Inc.”, 2009.
2. Gentry, Craig, and Shai Halevi. “Implementing Gentry’s fully-homomorphic encryption
scheme.” Annual International Conference on the Theory and Applications of Cryptographic
Techniques. Springer Berlin Heidelberg, 2011.
3. Gentry, C. (2009). “A fully homomorphic encryption scheme (Doctoral dissertation, Stanford
University)”.
4. Ronald Rivest, Michael and Len Adleman. Dertouzos. “On data banks and privacy
homomorphisms”, Foundations of secure computation 4, 1978.
5. Fontaine, Galand, and Caroline, “A survey of homomorphic encryption for nonspecialists.”
EURASIP Journal on Information Security 2007 (2009).
6. Atayero, A. A., & Feyisetan, O. (2011). “Security issues in cloud computing: The potentials
of homomorphic encryption”. Journal of Emerging Trends in Computing and Information
Sciences, 2(10), 546–552.
7. ShaﬁGoldwasser & Silvio Micali (1984). “Probabilistic encryption”. Journal of computer and
system sciences, 28(2), 270–299.
8. Van Dijk, Vaikuntanathan, Shai Halevi, and Gentry, “Fully homomorphic encryption over the
integers.” In Annual International Conference on the Theory and Applications of Crypto-
graphic Techniques, Springer Berlin Heidelberg, 2010.
9. Smart, N. P., & Vercauteren, F. (2010, May). “Fully homomorphic encryption with relatively
small key and cipher text sizes”. In International Workshop on Public Key Cryptography
Springer Berlin Heidelberg.
10. D. Stehlé & R. Steinfeld (2010, December). “Faster fully homomorphic encryption”. In
International Conference on the Theory and Application of Cryptology and Information
Security Springer Berlin Heidelberg.
11. J. S. Coron, A. Mandal, D. Naccache & M. Tibouchi (2011, August). Fully homomorphic
encryption over the integers with shorter public keys. In Annual Cryptology Conference.
Springer Berlin Heidelberg.
12. R. Y. Govinda, and K. Vijaya. “Efﬁcient public key homomorphic encryption over integer
plaintexts.” International Conference on Information Security and Intelligence Control (ISIC),
2012, IEEE, 2012.
Low-Size Cipher Text Homomorphic Encryption Scheme …
101

13. M. Moayedfard and A. S. Molahosseini, “Parallel implementations of somewhat homomor-
phic encryption based on Open-MP and CUDA,” 2015 International Congress on
Technology, Communication and Knowledge (ICTCK), Mashhad, 2015.
14. Hayward, Ryan, and Chia-Chu Chiang. “An Architecture for Parallelizing Fully Homomor-
phic Cryptography on Cloud.” Complex, Intelligent, and Software Intensive Systems (CISIS),
2013 Seventh International Conference on. IEEE, 2013.
15. Z. Brakerski & V. Vaikuntanathan (2011, August). Fully homomorphic encryption from
ring-LWE and security for key dependent messages. In Proceedings of the 31st annual
conference on Advances in cryptology (pp. 505–524). Springer-Verlag.
16. AWS
Toolkit
For
Eclipse,
http://docs.amazonaws.cn/en_us/AWSToolkitEclipse/latest/
GettingStartedGuide/aws-tke-gsg.pdf?
102
M.M. Potey et al.

Relation Between Facebook Stories
and Hours of a Day
Hradesh Kumar and Sanjeev Kumar Yadav
Abstract In recent development of computer technology, social networks are
evolved as complex networks. Most challenging questions are to understand
dynamics of user behavior on social network applications. In this paper, structural
and dynamical modeling issues have been investigated. Social networks are treated
as random graphs where a node is indicator variable of an entity on social network.
The term random graph refers to the messy nature of the arrangement of links
between different nodes. ER random graphs are generated by linking pair of ran-
domly selected nodes. There are several characteristics of nodes to categorize them
such as average path length, clustering coefﬁcient to the each node. Nodes cate-
gorized with the help of self-organizing map algorithm and other statistical infer-
ence mechanism. Activities on social network are such as posting, commenting,
sharing, and sending message, watch videos, and advertisements which are mod-
eled as random events on random graphs.
Keywords Facebook ⋅Social network ⋅User activities ⋅Random post
1
Introduction
Study of the network is the ﬁeld of the discrete mathematics; we called as the graph
theory which is originated by the mathematician Euler. Euler published their paper
on the topic of Konigsberg bridge problem in 1736 [1]. Some examples of networks
are as follows: road network, railway network, computer network, social network,
Internet network, gene network, neural network, and biological network. Social
network comes under the complex network, having some properties such as random
H. Kumar (✉) ⋅S.K. Yadav
Department of Computer Science & Engineering, KIET Group
of Institutions, Ghaziabad, India
e-mail: hradeshkumar011@gmail.com
S.K. Yadav
e-mail: sanjeevgreen@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_10
103

graph, small-world, and scale-free networks. Paul Erdos and Alfred Renyi gave the
theory of random graph in 1959 to understand the properties of the graph, how
growing the number of nodes in the given graph. Ducan Watts and Steven Strogatz
gave the model for the small-world network in 1998. The important property of the
small-world network is high clustering coefﬁcient [2]. The edge implementation
with the probability of P, when p = 0 network work such as regular lattice and
when p = 1 network work as random graph. Barabasi and Albert ﬁrst introduced
the scale-free networks which follow the power law degree distribution. Scale-free
network divides into two parts, static scale-free network and evolving scale-free
network.
Social network is a type of complex network. A social network is a set of people
or groups of people having interaction among them. There are so many social
network applications, i.e., Facebook, MySpace, LinkedIn, Flicker, Twitter, and
YouTube. To understand dynamics of the user activities, they should know the
mechanism of interaction among the online social network users. Online social
networks are basically designed for two primary purposes; these are sharing and
interaction of data and support the social activities of users. The main aim of social
network analysis research is to understand the dynamics of network and its struc-
tural properties. Facebook is one of the most popular social networks [10].
There are 1.44 billion monthly active users till March 2015 at the Facebook.
Facebook proﬁle has so many attributes such as birth date, hometown, contact
information, college, employers, high school. The average Facebook user makes
more than four attributes set as publicly. Online social networks are investigated to
ﬁnd the relationship between modeling phenomena and characteristics of real
networks, i.e., average path length, clustering coefﬁcient. One common question is
how to connect the local structure on phenomena with the global dynamics.
Visualization methods and tools are used to analyze evidence parameter of nodes.
The position of nodes is determined by the graph layout method to remove these
issues, but it becomes more complicated when number of nodes increases in the
networks. To address this issue of large number of nodes, community structure of
the network is used. Community structure connects the local network to the global.
Social networks come under assortative, and biological and technological network
comes under disassortative [15].
In this paper, the relation between time and Facebook stories is found out (Posts,
Comments, and Shares). Proposed work is based on the review literature and
analysis of Facebook networks. The rest of this paper is organized as follows. The
related work is summarized in Sect. 2, analysis and experimental results are cov-
ered in Sect. 3, and ﬁnally, Sect. 4 concludes the paper.
104
H. Kumar and S.K. Yadav

2
Related Work
Salamanos et al. [22] investigated relation between Likes and Communities in
social network (Facebook). They used crawler for collecting data to their analysis
which is based on breadth-ﬁrst search and designed in Python. The result is based
on two steps, ﬁrst one is to detect relation between communities and likes and
second one is validation with the help of communities detection algorithm. Com-
munities structure referred as partition among the users’ intention of the research is
found out that same type of communities performed similar types of activities on
social network. They used Louvain algorithm. They used Gephi, social network
analysis tool, for all experiments (visualization). Quinn et al. [20] analyzed the
behavior of users at social network in perspective of age. They categorized persons
into two categories: young user and old users, persons those having 15–30 years
old come under young category, and those are 50+ years old come under old users.
They found that younger user having 11 times more friends as compared to the old
users, and they also considered following terms in their analysis: user comments,
user replies, wall comments, status comments, and media application in the per-
spective of age. They collect data from 250 individual different proﬁles of Face-
book. Functionality of the online users such as reply, post, share, and comment
varies from age to age (old users and young users).
Hirsch and Sunder [8] discussed the effects of sharing the stories on social
networking sites (Facebook). A total of 70% news comes from friends and family
on social networking sites. A new feature of hashtags is available on social network
by which anyone can easily following a topic. Basically, there are three types of
broadcasting of stories: user can shared a story at own wall, user can shared a story
on friends’ wall which is visible to all mutual friends, and user can directly message
to friend in this way story is secret between you and your friend.
Nguyen and Tran [19] presented the paper on the Facebook activities, in which
they analyze the users’ connectivity, similarity, and activity. They analyzed a user
having many contacts is participate in many activities or less activities and vice a
versa, activity distribution role in users communities which user takes more par-
ticipation and same type of activities performed by the similar types of person or
not. They used R programming for analysis, for plotting the result ggplot2 and
power ﬁtting algorithm in their study of the activity correlation. They collect the
data from Max-Planck Software Institute for their research work; data set has two
parts: ﬁrst part having the information of friendship links and second part having the
information of wall posting. Eftekhar et al. [3] used an online survey for their
research work, in which they ask some question and participants can give answers
with the specify rating. They analyzed the photographs related to activity and ﬁnd
out the personality regarding the photographs. Basically, the user performed
activities on social networking sites by which he/she described himself/herself
behavior. And authors notify this type information and predict the behavior of
social network user. In their survey, 115 participants take role actively and put their
view of the questions those are in the survey questionnaires. According to their
Relation Between Facebook Stories and Hours of a Day
105

report, 219 million photographs are uploaded daily on average. They analyzed two
types of activity in their research: First is user created own proﬁle and upload
photographs, videos, and album; second is communication among users’ like,
share,
and
comment.
They
provide
the
result
of
their
research
at
any
photographs/videos with the help of ﬁve parameters: communication on that par-
ticular photographs, visual presence of photographs, extraversion of Facebook,
conscientiousness, and openness of the Facebook. Total participation is 130, but 15
out of which not gave response of the survey. So only 115 participants’ reviews
only consider in their research work.
Jiang et al. [11] introduced an algorithm for the social information recommen-
dation system with the help of probabilistic factorization matrix. They designed
algorithm for two social networks: Facebook and Twitter, where Facebook is
bidirectional, but Twitter is a unidirectional social network. They analyzed their
result on two data sets: First one is collected from Renren, a social network in China
such as Facebook, and second data set is collected from Weibo, a social network in
China like Twitter. They designed a novel recommendation system with the help of
well-known recommendation system content-based ﬁltering and collaborative-
based ﬁltering. Both recommendation systems have own advantage and disad-
vantage; they implement advantage of both systems in their matrix factorization
recommendation system. Content-based ﬁltering is based on the ranking system
while collaborative-based ﬁltering is based on the memory-based models. Khadangi
et al. [12] measured relation between user’s activities and their proﬁle information
available at online social network. They conducted an online survey for their
research and strength measured into four levels. They used two models in the
analysis of collection of their data set and multilayer perceptron decision tree
model. To check the validity or accuracy, they used 10-cross-fold method for
validation purpose. They also compared the results generated from multilayer
perceptron, with support vector machine to gain more accuracy in their result.
Average number of friends in Facebook social network is 130 while in Renren
social network having average number of friends is near about 100. There are 37%
of Facebook users having 100+ friends. And the maximum number of friends is
bounded by 5000 in Facebook social network, but Twitter user can have more than
million followers. They removed outlier with the help of local density method.
3
Experimental Result
Here, we used Digital footprints tool for our results and analyzed those considering
in our research work. A digital footprint is an online social network analysis tool.
Users required taking the permission from Digital footprints for accessing it. Our
research work is based on the following Facebook resources (Table 1).
106
H. Kumar and S.K. Yadav

Figure 1 shows the Facebook Posts over the experiment which includes Posts on
wall, Posts in page, and Posts on groups. Figure 2 shows the Facebook comments
over the collected data, and Fig. 3 shows the Facebook shares over the analysis for
the results.
Figure 4 shows all the feeds (wall feeds, group feeds, and page feeds) which
include 4 participants, 2 groups, 1 page, and duration is one year. All feeds include
here
are
available
at
wall
of
participants,
group,
and
page
except
the
Facebook-generated stories. Data are collected from 1–1–15 to 12–1–16.
Figure 5 shows the Facebook post with respect to time (day), which shows
average number of post per day entire one year posted by 4 participants, 2 groups,
and 1 page. We ﬁnd that minimum number of post uploaded at Facebook is
4 (2 A.M. and 4 A.M.) and maximum number of post uploaded at Facebook is 58
(12’o clock).
Table 1 Facebook resources
[23]
Participants
4
Pages
1
Independent groups
2
Include Facebook stories
Yes
Years of historical data
1
Fig. 1 Facebook posts
Relation Between Facebook Stories and Hours of a Day
107

Fig. 2 Facebook comments
Fig. 3 Facebook shares
108
H. Kumar and S.K. Yadav

4
Conclusion
Social networks are rapidly grown in modern era whereas the Facebook is one of
the most popular social network all over the world. All dynamics of the Facebook
are randomly taken place, so ﬁnd out a relation among them is a crucial task. It is
necessary to collect the data from the Facebook for giving some results regarding
social network (Facebook). Collection of data from the Facebook is a big issue
because of the privacy concern interrelated to the peoples activities. We used the
Digital footprints for the collection of the data from the Facebook. We ﬁnd a
Fig. 4 Number of Post and Comment
Fig. 5 Number of Facebook Stories in a day
Relation Between Facebook Stories and Hours of a Day
109

relation between Facebook stories with respect to time, where most of the Facebook
stories comes at 12’o clock according to our survey and minimum stories in
between 2 A.M. and 4 A.M. Constraint of my work is limited data collection; in the
future, the same task can be composed to a large data of amount. When the duration
of data collection increased and number of participants, Facebook pages, Facebook
groups more over added to the research work that would be more crucial task.
References
1. Bocaletti, S., Latora, V., Moreno, Y., Chavez, M. and Hwang, D., U, 2006, Complex
networks: Structure and dynamics. Elsevier, Physics Reports 424, pp 175–308.
2. Clark, J., W., 2012, Correlating a person to a person, ASE/IEEE International conference on
social computing and 2012 ASE/IEEE International Conference on Privacy, Security, Risk
and Trust, pp 851–859.
3. Eftekhar, A., Fullwood, C. and Morris, N., 2014, Capturing personality from Facebook
photos and photo related activities: how much exposure do you need, Elsevier, Computers in
Human Behavior, pp 162–170.
4. Farahbakhsh, R., Han, X., Cuevas, A. and Crespi, N., 2013, Analysis of publicly disclosed
information in Facebook proﬁles, IEEE/ACM International Conference on Advances in
Social Networks Analysis and Mining, pp 699–705.
5. Guo, Q., Zhou, T., Liu, J., G., Bai, W., J., Wang, B., H. and Zhao, M., 2006, Growing
scale-free small networks with tunable assortative coefﬁcient, Elsevier, Physica A 371,
pp 814–822.
6. Handayani, P., W. and Lisdianingrum, W., 2011, Impact Analysis on Free Online Marketing
Using Socila Network Facebook: Case Study SMEs in Indonesia, ICACSIS, pp 171–176.
7. Hayes, M., Cooke, K., V., S. and Muench, F., 2015, Understanding Facebook use and the
psychological affects of use across generations, Elsevier, Computers in Human Behavior 49,
pp 507–511.
8. Hirsch, A., O. and Sunder, S., S., 2014, Posting, commenting, tagging: Effects of sharing
news stories on Facebook, Elsevier, Computers in Human Behavior 44, pp 240–249.
9. Hollenbaugh, E., E. and Ferris, A., L., 2015, Prediction of honesty, intent and valence of
Facebook self-disclosure, Elsevier, Computers in Human Behavior 50, pp 4565–464.
10. Hradesh, K., Sanjeev, Yadav, 2015, Investigating Social Network as Complex Network and
Dynamics of User Activities, IJCA, Vol. 125, No. 7, pp 13–18.
11. Jiang, M., Cui, P., Wang, F., Zhu, W. and Yang, S., 2014, Scalable Recommendation with
Social Contextual Information, IEEE Transactions on Knowledge and Data Engineering, Vol.
26, November 2014, pp 2789–2802.
12. Khadangi, E., Zarean, A., Bagheri, A. and Jafrabadi, A., B., 2013, Measuring Relationship
Strength in Online Social Networks based on users’ activities and proﬁle information, 3rd
International Conference on Computer and Knowledge Engineering (ICCKE 2013), Ferdowsl
University of Mashhad.
13. Khil, M., Larsson, R., Arvidsson, A. and Aurelius, A., 2014, Analysis of Facebook content
demand patterns, IEEE.
14. Kirman, B., Lawson, S. and Linehan, C., 2009, Gaming on and off the Social Graph: The
Social Structure of Facebook Games, International Conference on Computational Science and
Engineering, IEEE, pp 627–632.
15. Kumar, H., Yadav, S., 2016, Surveying SNA Tools: How far & How Close to the Researcher,
IJEAST, Vol. 1, Issue 6, pp 176–187.
110
H. Kumar and S.K. Yadav

16. Mahanti, A., Carlsson, N., Mahanti, A., Arlitt, M. and Williamson, C., 2013, A Tale of Tails:
Power Laws in Internet Measurements, IEEE, pp 59–64.
17. Muangngeon, A. and Erjongmanee, S., 2015, Analysis of Facebook Activity Usage through
Network and Human Perspectives, IEEE, pp 13–18.
18. Naim, E., B., Krapivsky, P., L. and Render, S., 2004, Extremel Properties of Random
Structures, Springer-Verlag Berlin, pp 211–233.
19. Nguyen, K. and Tran, D., A., 2011, An Analysis of Activities in Facebook, The 8th
Annual IEEE Consumer Communications and Networking Conference – Emerging and
Innovative Consumer Technologies and Applications, pp 388–392.
20. Quinn, D., Liming, C. and Mulvenna, M., 2011, Does Age Make a Difference In The
Behavior Of Online Social Network Users?, IEEE International Conference on Internet of
Things and Cyber, Physical and Social Computing, pp 266–272.
21. Rybnicek, M., Poisel, R. and Tjoa, S., 2013, Facebook Watchdog: A Research Agenda For
Detecting Online Grooming and Bullying Activities, IEEE International conference on
Systems, Man and Cybernatics, pp 2854–2859.
22. Salamanos, N., Voudigari, E., Papageorgiou, T. and Vazirgiannis, M., 2012, Discovering
Correlation between Communities and Likes in Facebook, IEEE International Conference on
Green Computing and Communications, Conference on Internet of Things and Conference on
Cyber, Physical and Social Computing, pp 368–371.
23. http://digitalfootprints.dk/my/project?projectId.
Relation Between Facebook Stories and Hours of a Day
111

Part II
Image Processing
and Computer Vision

Lagrangian Twin SVR Based Grayscale
Image Watermarking Using LWT-QR
Decomposition
Ashok Kumar Yadav, Rajesh Mehta and Raj Kumar
Abstract A novel approach of image watermarking using Lagrangian twin support
vector regression (LTSVR) and combination of a variant of wavelet transform and
QR decomposition for copyright protection application is proposed in this chapter.
Firstly, host image decomposed into low-frequency subband (LL) and detail sub-
bands by applying lifting wavelet transform (LWT). Secondly, the blocks of LL
depend on the fuzzy entropy, and the selected regions of LL are transformed using
QR factorization. Then, image dataset is formed using the elements of matrix R
(called feature vector) of each selected block. This image dataset acts input to LTSVR
to ﬁnd the function approximation which deﬁnes the relationship between the input
and target. The scrambled bits of the binary watermark are inserted into the pre-
dicted values obtained through the trained LTSVR upon comparing with the target
value. The scrambled bits are obtained by applying Arnold transformation, which
provide security to the proposed approach. Experimental results using various kinds
of images and comparison of existing methods prove that the proposed approach is
highly imperceptible and robust.
Keywords
LTSVR ⋅LWT ⋅QR factorization ⋅Digital watermarking ⋅Fuzzy
entropy
A.K. Yadav (✉) ⋅Raj Kumar
Department of Computer Science and Engineering, UIET, Maharishi
Dayanand University, Rohtak, India
e-mail: akyadav1@amity.edu
Raj Kumar
e-mail: rajyadav76@rediﬀmail.com
R. Mehta
Department of Computer Science and Engineering, Amity School of Engineering
and Technology, New Delhi, India
e-mail: rajesh2010usit@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_11
115

116
A.K. Yadav et al.
1
Introduction
The Internet is a very popular distribution medium all over the world for digital con-
tents as it is inexpensive, no storage requirements, and fast access since few decades.
With the success of digital communication on the Internet, various problems related
to copyright protection of digitized properties, illegal copying, ownership of mul-
timedia data, data security, etc., have arisen. Digital watermarking provides a solu-
tion to soft contents [1, 2] for copyright protection and its applications. Watermark-
ing is the method of embedding the secret information known as watermark in an
imperceptible manner into the original digital media without losing its visual qual-
ity [2]. Spatial- and frequency-domain watermarking are two diﬀerent domains of
digital image watermarking. In the literature of digital watermarking [3–6], it has
been found that frequency-domain watermarking shows more robustness against
attacks as compared to spatial-domain watermarking [7]. The use of machine learn-
ing algorithms such as BPNN, PNN [3, 8], support vector regression [4, 7], genetic
algorithms [5], and their combination-based hybrid image watermarking system is
designed by various researchers [5, 9] to increase the imperceptibility and robust-
ness. Signiﬁcant amount of imperceptibility and robustness against image processing
attacks is achieved [5, 6] due to the adaptive learning capability of image datasets
and good generalization ability against noise of these machine learning algorithms.
In this paper, a newly designed LTSVR machine learning approach by Balasundaram
et al. [10] is employed in image watermarking. The generalization performance of
LTSVR on synthetic datasets which is obtained from UCI repository and against
noisy datasets is already examined [10]. With the help of the work presented in this
paper, high generalization against noisy datasets and the adaptive learning capability
of LTSVR onto image watermarking for improving the robustness is examined.
The novel work presented in this paper is a grayscale image watermarking
approach using LTSVR and based upon the feature extracted with the help of hybrid
LWT-QR factorization. The selected blocks of the approximate subband (LL) of the
image are used to insert the watermark. QR transformation [11] method is applied
to non-overlapping regions selected using fuzzy entropy [12] to get Q and R matrix.
The elements (feature vector) of R are used to make dataset for LTSVR training. The
predicted value obtained using function generated by the LTSVR training is used
to insert the watermark bits. A number of grayscale images are used to verify the
performance of the approach presented against attacks. Due to the limitations on
the number of pages, the results on Lena and Elaine images are explained in this
paper. The generalization performance of LTSVR against noisy datasets is measured
by the visual quality of extracted watermark which indicates the robustness of the
approach, and the security of the watermark is obtained through Arnold transfor-
mation. Fuzzy entropy [12] is sensitive to image variations; it is used for selecting
smooth non-overlapping blocks and discards blocks with redundant data.

Lagrangian Twin SVR Based Grayscale Image Watermarking ...
117
The outlines of the remaining part are as follows. The mathematical description of
LTSVR and QR transformation is explained in Sect. 2. In Sect. 3, the novel grayscale
image watermarking approach is discussed. The outcomes of the work, comparison
along with the discussions, are explained in Sect. 4. Conclusion and future directions
are discussed in Sect. 5.
2
Preliminaries
2.1
QR Decomposition
The QR decomposition [11] of a matrix X (also called orthogonal-triangular decom-
position) is given in
[X]p×q = [Q]p×p [R]p×q
(1)
where Q is a unitary matrix and R is an upper triangular matrix, both matrices are of
order of p × q. The columns of Q are obtained through the process of Gram–Schmidt
orthogonalization [11]. The main characteristics of matrix R is that |R1i| > Rji for i
= to q and j = 2 to p [11]. Also the maximum energy of the signal is concentrated in
the elements of ﬁrst row of matrix R. The QR decomposition has less computational
complexity than other factorization method such as singular value decomposition
(SVD) [13]. These interesting features of R matrix made the utilization of QR fac-
torization in digital watermarking applications by various researchers [11].
2.2
Formulation of Lagrangian Twin Support Vector
Regression
Balasundaram et al. [10] designed a novel machine learning approach called LTSVR
for regression problems. This algorithm has successfully used and tested on diﬀerent
kinds of datasets. In this paper, the application of LTSVR is tested onto grayscale
image watermarking. The adaptive learning ability and its generalization against
noisy image datasets of the LTSVR algorithm is checked by applying a lot of exper-
iments on diﬀerent textured images. 2-norm nonlinear TSVR determines the insen-
sitive up- and down-bound regressors in the feature space by solving the pair of
quadratic programming problems (QPP) as
min
(w1,b1,𝜉1)∈Rm+1+m
1
2‖y −𝜖1e −(K (A, At) w1 + b1e) ‖2 + C1
2 𝜉2
1
subject to y −(K (A, At) w1 + b1e) ≥𝜖1e −𝜉1 and
min
(w2,b2,𝜉2)∈Rm+1+m
1
2‖y + 𝜖2e −(K (A, At) w2 + b2e) ‖2 + C2
2 𝜉2
2
subject to (K (A, At) w2 + b2e) −y ≥𝜖2e −𝜉2
(2)

118
A.K. Yadav et al.
the dual QPP of [12] can be formed as a pair of minimization problems as
min
0≤u1≤Rm L1
(u1
) = 1
2ut
1Q1u1 −rt
1u1 and
min
0≤u2≤Rm L2
(u2
) = 1
2ut
2Q2u2 −rt
2u2
(3)
where 𝑢1, 𝑢2 ∈𝑅𝑚are the Lagrangian multipliers, and for ﬁnding the value of
𝑄1, 𝑄2, 𝑟1, 𝑟2, we refer the reader to [10]. The kernel regression function estimation
𝑓∶𝑅𝑛→𝑅will be determined using
𝑓(𝑥) = 1
2 (𝑔(𝑥) + ℎ(𝑥))
(4)
where
𝑔(𝑥) = 𝐾(𝑥𝑡, 𝐴𝑡) 𝑤1 + 𝑏1 and ℎ(𝑥) = 𝐾(𝑥𝑡, 𝐴𝑡) 𝑤2 + 𝑏2
(5)
are the down- and up-bound regressors [14]. According to the KKT conditions, the
pair of dual QPP [12] will become determining solutions for the complementarily
problems [15]:
0 ≤u1 ⟂(Q1u1 −r1
) ≥0 and 0 ≤u2 ⟂(Q2u2 −r2
) ≥0
(6)
respectively. The optimality conditions (6) are satisﬁed iﬀfor any 𝛼1, 𝛼2 ≻0, the
relations
(Q1u1 −r1
) = (Q1u1 −𝛼1u1 −r1
)
+ and
(Q2u2 −r2
) = (Q2u2 −𝛼2u2 −r2
)
+
(7)
respectively. The following simple iterative scheme that constitutes the convergence
of LTSVR algorithm to solve the pair of problems deﬁned by (7) is
ui+1
1
= Q−1
1
(r1 + (Q1ui
1 −𝛼1ui
1 −r1
)
+
) and
ui+1
2
= Q−1
2
(r2 + (Q2ui
2 −𝛼2ui
2 −r2
)
+
) for i = 0, 1, 2, …
(8)
3
Proposed Approach for Image Watermarking
A novel grayscale image watermarking approach using LTSVR and hybridization
of LWT-QR factorization which includes embedding and extracting procedure of
watermark is explained as follows:

Lagrangian Twin SVR Based Grayscale Image Watermarking ...
119
3.1
Watermark Insertion Algorithm
Consider a grayscale image Img = {Img (r, s) ∶1 ≤r ≤M1, 1 ≤s ≤M2} of order
of M1 × M2. In this work, a binary watermark logo of order of N1 × N2 is used for
embedding and extracting purpose. The approach for inserting the watermark into
the host image is as follows:
1. Firstly, the scrambled image §m of the original binary watermark is formed
using Arnold transformation [16]. Then, it transformed into 1-D vector to insert into
the host. That is, SWm = {wk ∶k = 1, 2, … , lw} where lw = watermark length and
wk = {0, 1}.
2. Using one-level LWT, the host image is divided into the low-frequency subband
and detailed subbands denoted by LL and LH, HL, and HH, respectively, with order
ML × NL where ML = M1
2r , M2L = M2
2r . Here, decomposition level is denoted by r. The
lifting coeﬃcients of low-frequency subband are divided into blocks of order of 4 ×
4. Fuzzy entropy [12] of every block is calculated and arranged in descending order.
3. Perform QR decomposition to the selected blocks of low-frequency subband
using (1) to get the Q and R matrix of order equal to the block size. From the
experimental results, it is found that r1,4 is the appropriate element to embed the
scrambled watermark. The feature vector formed using the upper triangular elements
{r1,1, r1,2, r1,3, r2,2, r2,3, r2,4, r3,3, r3,4, r4,4} is supplied as input to LTSVR correspond-
ing to target vector made up of the element r1,4. Thus, an image dataset constructed
using the feature vectors of all the selected non-overlapping blocks of order of m × l
is formed (Here l = 10).
4. Based upon the fuzzy entropy, the dataset is constructed using the suitable
features of the image blocks for training of the LTSVR. That is,
DS =
{(
xi, di
)
∈R9 × R ∶i = 1, 2, … , m
= {(r1,1, r1,2, r1,3, r2,2, r2,3, r2,4, r3,3, r3,4, r4,4
) , r1,4}
where the target output vector consists of r1,4 element of each selected block and
remaining nine upper triangular elements of each block are supplied as input to
LTSVR. The feature vectors of odd number of selected regions are used to train
the LTSVR, i.e., DS = {(xi, di
) ∶i = 1, 3, 5, … , m}. The function obtained after the
training of LTSVR using (8) is used to ﬁnd the predicted value corresponding to the
target vector of even number of blocks. On comparing the predicted value corre-
sponding to the target vector di = {r1,4 ∶i = 2, 4, 6, … , m}, the watermark bits are
inserted as follows:
if wm_bit = 1
then r
′
1,4 = max(r1,4, rLTSVR
1,4
+ 𝛼)
else
r
′
1,4 = min(r1,4, rLTSVR
1,4
−𝛼)

120
A.K. Yadav et al.
where, r
′
1,4 is the watermark embedded value after inserting the watermark which is
replaced by the r1,4 of R of the selected region, rLTSVR
1,4
is the predicted value found
by the training function of LTSVR, 𝛼denotes the strength of watermark, and wm_bit
represents bit of scrambled image. After performing a number of experiments, the
value of 𝛼= 20 is chosen to minimize the trade-oﬀbetween two conﬂicting require-
ments.
5. After replacing r1,4 by r
′
1,4 of the selected regions of low-frequency subband,
inverse QR decomposition is performed to get the watermark LL subband which
is followed by inverse LWT transform to get the watermarked image. Then, PSNR
using (9) is computed for quality evaluation of watermarked image.
3.2
Watermark Extraction Procedure Using Trained LTSVR
The extraction of watermark from the signed image includes the following steps:
1. Similar to Step 2 of embedding procedure, the low-frequency subband and
detail subband of signed image denoted by LL
′, LH
′, HL
′, and HH
′, are obtained
using one-level LWT. The regions of low-frequency subband are selected as per the
index of fuzzy entropy.
2. QR decomposition is applied to the selected blocks of LL
′ subband using (1)
to obtain Q
′ and R
′ of order of 4 × 4. Similar to Step 4 of watermark embedding,
features are extracted and form the watermarked dataset. Then, features of the even
number regions are supplied to trained LTSVR to extract the watermark, that is
DS =
{(xi, di
) ∈R9 × R ∶i = 2, 4, … , m
= {
(
r
′
1,1, r
′
1,2, r
′
1,3, r
′
2,2, r
′
2,3, r
′
2,4, r
′
3,3, r
′
3,4, r
′
4,4
)
, r
′
1,4}
are acts input to the function obtained after training to get the output rLTSVR
1,4
∶i =
2, 4, … , m corresponding to target vector di = {r
′
1,4 ∶i = 2, 4, … , m}. Then, LTSVR
output (predicted value) is compared with the target vector corresponding to the
selected regions of the watermarked image to get scrambled vector SW
′
m
SW
′
m =
{
1
if r
′
1,4 > rLTSVR
1,4
0
otherwise
where rLTSVR
1,4
is the LTSVR output and r
′
1,4 is the actual output of each block.
3. The scrambled binary sequence obtained using Step 2 is in a vector form.
To obtain its scrambled image, it is changed into 2-D array, and then, recovered
watermark image is formed using inverse Arnold transformation [16]. The bit error
rate (BER) value using (10) is computed to measure the quality of the recovered
watermark.

Lagrangian Twin SVR Based Grayscale Image Watermarking ...
121
4
Result Analysis, Comparison, and Discussion
The performance of the approach described here is evaluated on the grayscale images
Lena and Elaine of order of 512 × 512 shown in Fig. 1. A binary logo of order of
32 × 32 shown in Fig. 2 is used as a binary watermark. The optimal value of LTSVR
parameters C1 = C2 = 50 and the spread of RBF kernel 𝜎= 10−3 are determined
by k-fold crossvalidation on the dataset used for training by varying C1 = C2 =
{50, 100, … , 500} and 𝜎= {10−3, 10−2, … , 103}, respectively. The learning capa-
bility of LTSVR and its generalization performance is demonstrated by the imper-
ceptibility and robustness of watermark against various types of image processing
attacks on diﬀerent textured images shown in Fig. 1. LTSVR requires more parame-
ters to be selected as compared to LSVR [17] which leads to slow the learning speed.
4.1
Performance Evaluation
The performance of the proposed approach is evaluated using the imperceptibility
of the watermark and its robustness. The imperceptibility is found by the quality of
the watermarked image as measured by PSNR given in the equation as follows:
PSNR = 10log10
2552
MSE
(9)
where MSE is the mean square error. The watermarked images obtained after embed-
ding process along with the recovered watermark after extraction procedure are
revealed in Fig. 3 corresponding to Figs. 1 and 2. Bit error rate (BER) is given as
BER (W, W
′) =
∑N1
k=1
∑N2
l=1 W (k, l) ⊗W
′ (k, l)
N1 × N2
(10)
Fig. 1
Original images
Fig. 2
Original watermark

122
A.K. Yadav et al.
Fig. 3
Watermarked images
corresponding to Fig. 1 along
with extracted watermark
without attack
The quality parameters PSNR and BER corresponding to the watermarked image and
extracted watermark along with the visualization of images are indicated in Fig. 3
when no image processing attack is performed. The PSNR value more than 40 dB
as indicated in Fig. 3 of the watermarked version images shows the good quality
of watermarked images with high imperceptibility of the watermark. The accurate
watermark extraction using the proposed approach when no attack is performed is
veriﬁed by the zero BER value as shown in Fig. 3 The robustness of the approach
described in this paper is investigated by performing several kinds of image process-
ing operations such as blurring, salt-and-pepper noise, Gaussian noise, histogram
equalization, JPEG compression, gamma correction, median ﬁltering, average ﬁlter-
ing, scaling and cropping on all the watermarked images followed by the watermark
extraction process. The visual quality of recovered watermark as measured by the
BER value against all the attacks is shown in Table 1 corresponding to all water-
marked images. BER values are shown in Table 1, and it is observed that correspond-
ing to all the attacks, the proposed approach has lower BER value which signiﬁes
that the extracted watermark has good visual quality and recognizable.
4.2
Comparison Results
The eﬀectiveness of the scheme presented in this paper is examined by comparing
the robustness against attacks with the method described by Songs et al. [11] on Lena
image. For fair comparison, same kinds of attacks are executed on Lena image and
then watermark extraction procedure is performed. The outcomes of the watermark
extraction procedure measured by the BER value against attacks are given in Table 2
and shown in Fig. 4. From Table 2 and Fig. 4, we found that the visualization of
the recovered watermark is better than the scheme introduced by Song et al. [11]
as quantiﬁed by the lower BER value against all the attacks except against rotation
and salt-and-pepper noise operations. By comparison with the existing methods, it
is inferred that the approach presented in this paper has better performance than the
scheme [11].

Lagrangian Twin SVR Based Grayscale Image Watermarking ...
123
Table 1
Visual quality of extracted watermark along with corresponding BER value against image
processing attacks on Lena and Elaine images
Attack
BER (Lena)
Extracted
watermark
(Lena)
BER (Elaine)
Extracted
watermark
(Lena)
Gaussian blurring
0.0127
0.0088
Salt-and-pepper
noise
(0.02)
0.1028
0.1562
Gaussian noise (0.10)
0.0345
0.0453
Gaussian noise (0.20)
0.1683
0.1763
Histogram equalization
0.0068
0.0088
JPEG (QF = 80)
0
0
JPEG (QF = 60)
0
0.0029
Gamma correction
0.0098
0.0088
Sharpening
0.0068
0.0186
Scaling
0.0059
0.0039
Average ﬁltering
0.0361
0.0322
Median ﬁltering
0.0146
0.0342
Cropping (25%)
0.0244
0.0244

124
A.K. Yadav et al.
Fig. 4
BER value comparisons against image processing operations on Lena image with [11]
Table 2
BER value comparisons against image processing operations (attacks) on Lena image
with [11]
Attack
Proposed method
Song’s method [11]
Attack free
0
0
Wiener ﬁltering
0
0.3962
Median ﬁlter
0.0146
0.5000
Average ﬁlter
0.0361
0.4984
Scaling_0.5
0.0059
0.2688
Scaling_0.9
0
0.1156
Gaussian noise
0.1683
0.3266
Salt-and-pepper noise
0.1028
0.0348
Sharpening
0.0068
0.0218
Center cropping
0.0347
0.0905
Side cropping
0.0462
0.1092
Corner cropping
0.0244
0.1190
Rotation (5◦)
0.4983
0.4789
5
Conclusion
In this work, an eﬀective approach of grayscale image watermarking using LTSVR
and through the combination of wavelet transform and QR decomposition is
described for copyright protection applications. Fuzzy entropy is not only used to

Lagrangian Twin SVR Based Grayscale Image Watermarking ...
125
discard the regions of the image which are not relevant to embed the watermark but
also reduces the time complexity. Selection of LL sub band using LWT and appropri-
ate coeﬃcient selection of each region using QR decomposition results in enhanc-
ing the performance as measured by imperceptibility and robustness. The robustness
measured by diﬀerent kinds of attacks performed on test images is accomplished by
the good generalization property of LTSVR as revealed from the experimental results
using proposed approach. The scrambled watermark obtained using Arnold transfor-
mation provides the security to the original watermark. The experimental and com-
parison results on diﬀerent textured images with the existing methods prove that the
approach described in this paper attains imperceptibility as well as robustness.
References
1. Cox, I.J., Kilian, J., Leighton, F.T., Shamoon T.: Secure spread spectrum watermarking for
multimedia. IEEE Trans. on Image Processing. 6, 1673–1687, 1997.
2. Moulin, P., Mincak, M.: A framework for evaluating the data-hiding capacity of image sources.
IEEE Trans. on Image Processing. 11, 1029–1042, 2002.
3. Wen, X.B., Zhang, H.: A new watermarking approach based on probabilistic neural network
in wavelet domain. Soft Computing, No. 13, pp. 355–360, 2009.
4. Peng, H., Wang, J.: Image watermarking method in multiwavelet domain based on support
vector machines. The Journal of Systems and Software, No. 83, pp. 1470–1477, 2010.
5. Mehta, R., Rajpal, N., Vishwakarma, V. P.: Robust Image Watermarking Scheme in Lifting
Wavelet Domain Using GA-LSVR Hybridization, International Journal of Machine Learning
and Cybernetics, DOI: 10.1007/s13042-015-0329-6, 2015.
6. Mehta, R., Rajpal, N., Vishwakarma, V. P.: A robust and eﬃcient image watermarking scheme
based on Lagrangian SVR and lifting wavelet transform. International Journal of Machine
Learning and Cybernetics, DOI: 10.1007/s13042-015-0329-z, 2015.
7. Shen, R.M., Fu, Y.G.: A novel image watermarking scheme based on support vector regression.
The Journal of System and Software, No. 78, pp. 1–8, 2005.
8. Tang, G., Lio, X.: A neural network based blind watermarking scheme for digital images.
Lecture Notes in Computer Science (LNCS), 3174, pp. 645–650, 2004.
9. Jing, Li., Liu, F.: Robust image watermarking scheme with general regression neural network
and FCM algorithm. Lecture Notes in Computer Science (LNCS), 5226, pp. 243–250, 2008.
10. Balasundaram, S., Tanveer, M.: On Lagrangian twin support vector regression. Neural Com-
puting and Applications, 22, 257–267, 2013.
11. Song, W., Jian-Jun, H., Zhao-Hong, Li., Liang, H.: Chaotic system and QR factorization based
robust digital image watermarking algorithm. J. Cent. South Univ. Technology, No. 18, pp.
116–124, 2011.
12. Kumar, R., Das, R.R., Mishra, R.R., Dwivedi, R.: Fuzzy entropy based neuro-wavelet
identiﬁer-cum-quantiﬁer for discrimination of gases/odors. IEEE sensors Journal. 11, 1548–
1555, 2011.
13. Lei, B., Soon, I.Y., Zhou, F., Li Z., Lei, H.: A robust audio watermarking scheme based on
lifting wavelet transform and singular value decomposition. Signal Processing, Vol. 92, No. 9,
pp. 1985–2001, 2012.
14. Daubeches, I., Sweldens, W.: Factoring wavelets into lifting steps. Journal of Fourier Analysis
and Applications, Vol. 4, No. 3, pp. 247–269, 1998.
15. Mangasarian, O.L., Musciant, D.R.: Lagrangian support vector machines. Journal of Machine
Learning Research, Vol. 1, pp. 161–177, 2001.

126
A.K. Yadav et al.
16. Wu, L., Deng, W., Zhang, J., He, D.: Arnold transformation algorithm and anti Arnold trans-
formation algorithm. In: Proc. of 1st International Conference on Information Science and
Engineering (ICISE), pp. 1164–1167, 2009.
17. Balasundaram, S., Kapil: On Lagrangian support vector regression. Expert System with Appli-
cations, No. 37, pp. 8784–8792, 2010.

Difference in Lights and Color
Background Differentiates the Color Skin
Model in Face Detection for Security
Surveillance
Dimple Chawla and Munesh Chandra Trivedi
Abstract Face detection with variable lights and color background makes it more
difﬁcult to detect the originality of the person in the image. Subject does not look
directly into the camera; when the face is not held in the same angle, the system
might not recognize the face. In this paper, we are considering various live studies
where security surveillance ought to be a ﬁrst preference of our own lives. Few
studies have taken as source input study which helped us for better outcome.
Further algorithm designed to get signiﬁcant result is least expected to perform well
on small sample data.
Keywords AdaBoosT training algorithm ⋅Skin color segmentation ⋅Skin color
model ⋅Color Similarity Image (CSI)
1
Introduction
The rising number of face recognition applications in everyday life where image
segmentation and video-based recognition methods are becoming very important
research area. Generally, effects of pose, illumination, facial expression, and
occlusion are such issues mostly studied in face recognition. So far, very little has
been done to investigate the effects of compression on face recognition, even
though the images are mainly stored and then translated into a compressed format.
Still pictures have been experimented so often, but only in uncompressed image
formats, whereas in videos, mostly research deals with basic issues of tracking and
recognizing faces where still uncompressed images have taken as library and
compressed video as probes.
D. Chawla (✉)
Paciﬁc Academy of Higher Education & Research University, Udaipur, India
e-mail: chawladel@gmail.com
M.C. Trivedi
ABES Engineering College, Ghaziabad, India
e-mail: munesh.trivedi@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_12
127

In this paper, we have focused on change in lights and background base which
actually makes image resolution different in recognizing the effect on each indi-
vidual faces. Also, the change can reﬂect to the change with aging, change respect
to plastic/cosmetic surgery, or with any other formation of difﬁculty level to rec-
ognize the face. We have tried to demonstrate the system through proposing an
algorithm in order to recognize each individual which is explained and its related
work in Sect. 3, the experimental result and comparison with other color model and
in Sect. 4 followed by the conclusion and future scope.
2
Literature Review
As explained in (M. Singh 2014), with the change in time and age, circumstance
changes reﬂect on each individual faces, skeleton structure, muscle mass, and body
fat [1]. Image-based techniques in (Philippe Carré 2014) have formulated color
alterations with algebraic operations [2]. The generalized linear ﬁltering algorithms
deﬁned with quaternions and deﬁne a new color edge detector. The group of
authors in (V.V. Starovoitov 2002) have trained a process to recognize the small
scale of people using multilayer perceptron and neural network [3]. Also, author in
(Fahad Shahbaz Khan 2011) has promoted an idea by introducing a color model
with both bottom-up and top-down components to modulate the weights with local
shape features [4].
The premium use of face detection in the ﬁeld of biometric is done so efﬁciently
and optimum cost utilization whenever the real time application demand for it. The
improved traditional process has been proposed by (X. Ma 2013) in order to avoid
the problems faced in traditional skin color model at different lighting environment
[5]. Face detection in color images is difﬁcult when images contain complex
background under various luminance as well as skin detection to reduce the
false-positive result.
A novel algorithm presented by [6] proposing equivalent structure for skin color
detection to improvise the detection accuracy to obtained a classiﬁer from Gaussian
mixture algorithm and Adaboost training algorithm in order to reduce the false
positives. Face detection based algorithm for skin color model experienced fol-
lowed by Face candidates algorithm with its veriﬁcation algorithm to Adaboost
algorithm with many probability images taken as training samples.
On similar note, authors in (Zhengming Li 2010) have also proved that the face
detection algorithm improves the detection speed both in terms of quality as well as
in reducing the error detection rate [7]. Whereas authors in [8] has proposed a new
algorithm based on skin color segmentation and geometry feature in color space,
which is a hierarchical approach to integrates a skin color model and gradient
features. Several criteria for ﬁltering the false positives were proposed in (Loris
Nannia 2014) by the face detector like a skin detection ﬁlter which is used to
remove the candidate face regions which consist of high skin pixels, with the size to
calculate the depth map to eliminate the additional size according to the ﬁxed range
128
D. Chawla and M.C. Trivedi

applied [9]. The depth map is used to design a ﬁlter rule to discard ﬂat or uneven
objects.
Noor A. Ibraheem, Mokhtar M. Hasan, Raﬁqul z. Khan, and Pramod K. Mishra
have reviewed in 2012 on various color models divided into three categories w.r.t.
image processing applications such as device-oriented color models, user-oriented
color models, and device-independent color models [10]. Also, author summarized
on the basis on effective parameters with each advantages and disadvantages.
Author Ahmad Yahya Dawod in 2010 has taken into consideration especially on
gesture recognition, hand tracking along with various face recognition real-time
applications [11]. Also, Ahmad has proposed and demonstrated a new technique for
the problem of misclassiﬁcation between skin color and non-skin color pixels using
adaptive skin color model over a various color hand variation, position, scale,
rotation, and pose. With this technique, it captures pixel values of an individual
pixel and it then converts into YCbCr color space. It actually maps the CbCr color
space into plane in order to construct a clustered region of skin color for the person.
Gabriela Csurka, Sandra Skaff, Luca Marchesotti, and Craig Saunders in 2011
have also followed a novel approach for building color palettes using different types
of applications where customization of images is required with concept-based
palette, image retrieval, and color transfer [12]. The combination of abstract color
categories such as classic, cool, delicate based on its WYSIWYG with typical set of
color model. The color plates deﬁned in the dataset selected to build Pantone
Matching System (PMS), a proprietary color space with variety of color plates
representation called as swatches. A swatch is basically deﬁned in 3D vector space,
where color space models are RGB, HSV, and CMYK. The author has also
compared between the generative-based model and the discriminative model in
order to rank sample database of Bag of Colors and Fisher Vectors images.
The most challenging task for normal human being to follow the facial recog-
nition retrieval model to identify the accurate match in the least running time.
Especially while dealing with static or non-static environment like live or real time
video, webcam recording where eyes with facial features and skin is not clear as to
take as direct input image. In order to develop such model, the study given in [13], a
model which provides solution to both the stages i.e., Facial Detection stage and
Facial Recognition stage. The Face detection stage deﬁned by adopting pattern
recognition for videos ﬁles implemented using single image matching algorithm.
Second phase is to consider input image though the camera, which starts GUI for
cropped square frame design to relocate the prospective area and separate the facial
features from the complex background area. The second stage recognizes the output
image taken from the data source, followed to that Successive Mean Quantization
Transform (SMQT) and eigen face techniques applied to each images. Later, the
system then splits up through Sparse Network of Window (SNOW) classiﬁer for
facial detection for least effect on background environment with greater speed
result. The system has proven 100% recognition accuracy where they have tested
150 frontal faced images taken from webcam.
Difference in Lights and Color Background Differentiates …
129

Michal Kawulok et al. in (Michal Kawulok 2013) have explained in both
qualitative as well as quantitative approach for demonstrating the beneﬁts of skin
color modeling schemes and their limitation of human skin detection and seg-
mentation [14]. The major techniques were experimented by improving efﬁciency
of color-based classiﬁcation, textural feature extraction, and adaptation of model
schemes with spatial analysis of skin blobs.
Rodolfo Alvarado, Edgardo M. Felipe-Riveron, and Luis P. Sanchez-Fernandez
in 2010 and authors in (Rodolfo Alvarado-Cervantes 2012) both have calculated one
of the image segmentation methods by Color Similarity Image (CSI), in which for
every pixel in 24-bit RGB true color image into a gray scale image and call it as color
similarity function [15, 16]. It is the technique which actually combines geometrical
and color features in extension to concept of mathematical morphology to process
color images. The function allows the automatic conversion, the cluster of many
color images into a single gray image as output. The ﬁrst steps are implemented to
compute the color centroid and color standard deviation of a small sample. Further
color similarity function is applied in order to calculate from the pixel sample to
adapt the level of color scattering. The whole discussion implemented on the popular
image of “baboon,” representative of many image processing and analysis appli-
cation with the basic experimented in 3 steps i.e., ﬁrst selection of pixel sample,
second the calculation of CSI and then Threshold and application of mathematical
morphology. This step is chosen with pixel sample taken from region of interest to
be segmented for 100% of similarity and black for 0%.
Authors (Tien Fui Yong 2009) have expanded color image algorithm into a high
visual quality [17]. The idea of classiﬁcation phase is to separate the interpolation
region from the geometric shape and the determination phase; the interpolator
assigns a color intensity value to the undeﬁned pixel inside the interpolation region.
3
Proposed Work
In this section, the new algorithm is proposed to work on accuracy when images are
taken through variety of cameras with different lights and resolution setting that
actually make it more difﬁcult for the software to recognize the face. The model is
built for skin color detection in order to validate that the captured image of the
person is matching with the database or it varies on basis of subject changed with
aging. It is difﬁcult to make face recognition secure enough for authentication
purposes. The approach followed in ﬁnding face in the images under the controlled
environment where normal light and static background is white then by color of the
skin. Finding face by motion where face is moving or dynamic object at foreground
or background will be more highest peak manning involved.
Initially, to ﬁnd the image and visualize the three color channels into three
different variables in such a way as image dimensions, an RGB image has three
planes, then reshaping the RGB layers next to each other generating a
two-dimensional grayscale image. All pixels under the threshold may consider as
130
D. Chawla and M.C. Trivedi

objects so need to settle the threshold, while increasing the threshold value, more
and more background pixels fall under it, so the wall and the lady cannot be
segmented any more. The problem is that a pixel having gray component is not
necessarily gray. There are some objects remaining in the image, for example, the
wallpaper and some noise. To remove them, we calculate a so-called label image,
where each pixel belonging to the same object has the same value.
The new algorithm works in majorly 3 steps as ﬁrstly classify the skin region in
color space, it is necessary to differentiate the background light as the static or
dynamic background so that it check on motion face reality. Secondly, apply
threshold to mask the skin region in order to remove noise. Once the ﬁnal face pixel
color value identify which can be considered as white, whitish or African tone. The
combination of three approach background pixel, face pixel with motion face reality
will generate the high probability ranges from 0.6 to 0.8 which is quite good
conﬁguration to improvise the accuracy of detection in order to reduce false pos-
itives. Lastly, draw box to extract the face image from image with the help of region
property in MATLAB. While differentiating it, the face pixel color occluded image
will be considered as bad light condition where accuracy will be as false detection
rate.
4
Experimental Result
In this section, the whole provided algorithm is tested under MATLAB 2016a with
self-owned database in different image light and background to identify the accurate
match in least running time. The study has taken 200 sample size of database but
here taken a screenshot of only one random picture for simplicity to understand; the
experimental result is shown in Figs. 1, 2, 3, and 4.
Fig. 1 The original image
Difference in Lights and Color Background Differentiates …
131

To calculate the performance accuracy, few parameters have kept in mind while
doing so, such as background light difference, static or dynamic background, pixel
of face detected, and occluded image found, and many more other constraints can
be added. Also the false negative and false positive of the image are calculated. The
result proves that our algorithm can correctly determine face appearance in variable
lighting and background conditions. As Table 1 has proven the result, the high
probability ranges more the detection accuracy achieved. While focusing on RGB
color model as primary, YCbCr for Television transmission color space, HIS for hue
saturation intensity for properties of color, CMYK color model works for partially
or entirely masking the colors on white background. The proposed algorithm has
Fig. 2 The skin segmented
Fig. 3 Skin with nose
removal
132
D. Chawla and M.C. Trivedi

proven with highest accuracy ratio among all calculated on the basis of 100 differ to
false detection ratio.
5
Conclusion and Future
In this paper, we have experimented and compared traditional skin color model
approaches to determine face appearance and variable lighting and background
conditions. It has been observed the faces usually do vary expressions and poses;
the proposed method can obtain higher accuracy by detecting noise and occluded
face. In a word, our method performs satisfactorily in fast general face detection not
only for its good environmental robustness but also for its high-accuracy and
low-resource consumption.
In future works, we will try to improve the detection of faces in compressed
images under various orientations with higher distorted and more enormous pho-
tograph collections of social media and photograph archiving sites are another.
Fig. 4 The candidates face
detected
Table 1 Accuracy performance rate
Color model
# Total no. of
images
# No. of images
detection
False
negative
False
positive
Accuracy
ratio
RGB
200
103
48.5
33.99
51.50
YCrCb
200
149
25.5
42.69
74.50
HSI
200
137
31.5
40.65
68.50
CMYK
200
124
38.0
38.27
62.00
Proposed
algorithm
200
171
14.5
46.09
85.50
Difference in Lights and Color Background Differentiates …
133

References
1. M. Singh, S. Nagpal, R. Singh, and M. Vatsa, “On Recognizing Face Images with Weight and
Age Variations”, IEEE Access, vol. 2, pp. 822–830, 2014.
2. Philippe Carré, Patrice Denis, Christine Fernandez-Maloigne, “Spatial Color Image Process-
ing Using Clifford Algebras: Application To Color Active Contour”, Springer-Verlag London
Limited 2012, SIViP (2014) 8:1357–1372, DOI 10.1007/s11760-012-0366-5.
3. V.V. Starovoitov, D.I Samal, D.V. Briliuk, “Three Approaches For Face Recognition”, The
6-th International Conference on Pattern Recognition and Image Analysis October 21–26,
2002, Velikiy Novgorod, Russia, pp. 707–711.
4. Fahad Shahbaz Khan, Joost van de Weijer, Maria Vanrell, “Modulating Shape Features by
Color Attention for Object Recognition”, Springer Science and Business Media, LLC 2011,
Int J Comput Vis (2012) 98:49–64 DOI 10.1007/s11263-011-0495-2.
5. X. Ma, H. Zhang and X. Zhang, “A face detection algorithm based on modiﬁed skin-color
model,” Control Conference (CCC), 2013 32nd Chinese, Xi’an, 2013, pp. 3896–3900.
6. Li Zou and Sei-ichiro Kamata, “Face Detection In Color Images Based On Skin Color
Models”, TENCON 2010 - 2010 IEEE Region 10 Conference, ISSN: 2159-3442, Print ISBN:
978-1-4244-6889-8, pp 681–686, DOI:10.1109/TENCON.2010.5686631.
7. Zhengming Li, Lijie Xue and Fei Tan, “Face detection in complex background based on skin
color features and improved AdaBoost algorithms,” Progress in Informatics and Computing
(PIC), 2010 IEEE International Conference on, Shanghai, 2010, pp. 723–727, DOI:10.1109/
PIC.2010.5687939.
8. S. Zhu and N. Zhang, “Face Detection Based on Skin Color Model and Geometry Features,”
Industrial Control and Electronics Engineering (ICICEE), 2012 International Conference on,
Xi’an, 2012, pp. 991–994, DOI:10.1109/ICICEE.2012.263.
9. Loris Nannia, Alessandra Luminib, Fabio Dominioa, Pietro Zanuttigha, “Effective and precise
face detection based on color and depth data”, Applied Computing and Informatics, Volume
10, Issues 1–2, January 2014, Pages 1–13, DOI:10.1016/j.aci.2014.04.001.
10. Noor A. Ibraheem, Mokhtar M. Hasan, Raﬁqul z. Khan and Pramod K. Mishra,
“Understanding Color Models: A Review”, ARPN Journal of Sccience and Technology,
Volume 2, No. 3, April 2012, pp 265–275, ISSN 2225-7217, http://www.ejournalofscience.
org.
11. Ahmad Yahya Dawod, Junaidi Abdullah, Md. Jahangir Alam “Adaptive Skin Color Model
for Hand Segmentation”, 2010 International Conference on Computer Applications and
Industrial Electronics (ICCAIE 2010), December 5–7, 2010, Kuala Lumpur, Malaysia, DOI:
978-1-4244-9055-4/10/$26.00 ©2010 IEEE.
12. Gabriela Csurka, Sandra Skaff, Luca Marchesotti, Craig Saunders, “Building Look and Feel
Concept Models From Color Combinations With Applications In Image Classiﬁcation,
Retirieval And Color Transfer”, The Visual Computer, December 2011, Volume 27, Issue 12,
pp 1039–1053.
13. Petcharat Pattenasethanon and Charuay Savithi, “Human Face Detection and Recognition
Using Web-Cam”, Journal of Computer Science 8 (9), 2012, ISSN 1549-3636, pp 1585–
1593.
14. Michal Kawulok, Jakub Nalepa, Jolanta Kawulok, “Skin Detection and Segmentation in
Color Images”, Advances in Low-Level Color Image Processing, Volume 11 of the series
Lecture Notes in Computational Vision and Biomechanics pp 329–366,17 December 2013.
15. Rodolfo Alvarado, Edgardo M. Felipe-Riveron, Luis P. Sanchez-Fernandez, “Color Image
Segmentation by Means of a Similarity Function”, Progress in Pattern Recognition, Image
Analysis, Computer Vision, and Applications, Volume 6419 of the series Lecture Notes in
Computer Science pp 319–328, 2010.
16. Rodolfo Alvarado-Cervantes, Edgardo M. Felipe-Riveron, “Improved HSI Color Space for
Color Image Segmentation”, Progress in Pattern Recognition, Image Analysis, Computer
134
D. Chawla and M.C. Trivedi

Vision, and Applications, Volume 7441 of the series Lecture Notes in Computer Science
pp 348–354, 2012, DOI:10.1007/978-3-642-33275-3_43, ISSN 0302-9743.
17. Tien Fui Yong, Wou Onn Choo, Hui Meian Kok, “Color Image Magniﬁcation: Geometrical
Pattern Classiﬁcation Approach”, Visual Informatics: Bridging Research and Practice 2009,
Volume 5857 of the series Lecture Notes in Computer Science pp 619–626.
Difference in Lights and Color Background Differentiates …
135

Feature Extraction and Fuzzy-Based
Feature Selection Method for Long
Range Captured Iris Images
Anand Deshpande and Prashant P. Patavardhan
Abstract Long range captured iris recognition system is a biometric system con-
sisting of pattern recognition and computer vision. In the process of iris recognition,
feature extraction and feature selection play a major role in increasing the recog-
nition accuracy. This paper proposes feature extraction method using discrete
cosine transform domain-based no-reference image quality assessment model,
gray-level co-occurrence matrix, Hu seven moments, and statistical features. It also
proposes fuzzy entropy and interval-valued fuzzy set measure-based feature
selection method. The selected feature vectors are classiﬁed by neural network
classiﬁer. The model is tested with CASIA long range iris database. The recognition
accuracy is compared with the results obtained without feature selection and
existing feature selection methods. It has been observed that the fuzzy entropy
method gives better classiﬁcation accuracy than existing feature selection method.
The results demonstrate that the proposed work is well suited to extract the features
of iris polar images captured at a long distance and to reduce the dimensionality by
selecting the useful features which increase the recognition accuracy.
Keywords Feature selection ⋅Fuzzy entropy ⋅Feature extraction
GLCM ⋅Neural network ⋅Long range captured iris
1
Introduction
Long range captured iris recognition system is a biometric system which uses the
complex patterns of the human iris to identify the people [1]. In iris recognition
process, feature extraction and feature selection play a major role in improving the
A. Deshpande (✉)
Department of Electronics and Communication Engineering, Angadi Institute of Technology
and Management, Belagavi, Karnataka, India
e-mail: deshpande.anandb@gmail.com
A. Deshpande ⋅P.P. Patavardhan
Department of Electronics and Communication Engineering, Gogte Institute of Technology,
Belagavi, Karnataka, India
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_13
137

recognition accuracy. The recognition accuracy is decided by how well patterns
have been extracted and characterized on the feature vector template. A good
feature extraction method gives an accurate and fast matching by assigning its own
distinct representation to iris patterns. Feature selection (FS) plays a major role in
iris recognition process by reducing the number of features describing the logic of
the occurrences. It also minimizes the computational cost by simplifying the model.
It makes the system more apparent and more understandable by removing the
irrelevant features from the dataset. Many approaches have been proposed for
feature selection [2]. Most of the methods are based on the neural network [NN]
systems [3–6] and the genetic algorithms [7]. The selected features by these
methods may not be suitable for all classiﬁcation systems. To avoid this problem,
fuzzy entropy-based [8] feature selection method is proposed in this paper.
Section 2 discusses the feature extraction method, the fuzzy entropy-based
feature selection method, and recognition method. Section 3 discusses the result
which is followed by the conclusion.
2
Proposed Method
The block diagram of proposed approach is shown in Fig. 1.
2.1
Iris Extraction
The face and eyes are extracted by using AdaBoost-based [9, 10] face and eye-pair
classiﬁers. The iris is segmented from eye images using Hough Transform Method
[11] and converted into polar form [12].
2.2
Feature Extraction
The main goal of this section is to describe the feature extraction methods using
discrete cosine transform (DCT) domain-based no-reference image quality assess-
ment (IQA) model, gray-level co-occurrence matrix (GLCM), Hu seven moments,
and statistical features.
Feature
Extraction
Feature
Selection
Recognition
Iris 
Extraction
Image 
Fig. 1 Block diagram of proposed work
138
A. Deshpande and P.P. Patavardhan

2.3
Image Quality Assessment Model
The iris polar image is divided into four equal tracks having size 300 × 10. These
tracks are sent to the feature extraction block. The features are obtained for all the
tracks. The image quality assessment model (IQA) is proposed to extract various
features of iris images. The features of iris polar images are extracted in DCT domain
using the parameters of the model. The reasons for feature extraction in DCT domain
are as follows: (1) Variation in DCT coefﬁcients occurs due to degree and type of
image distortion and (2) ease of computation [13, 14]. Applying model-based
method to coefﬁcient increases the computational efﬁciency [15, 16]. The parametric
model is proposed to model the DCT coefﬁcient, which is shown in Fig. 2.
In the ﬁrst stage of Fig. 2, the image is subjected to DCT coefﬁcient computa-
tion. In this stage, the image is partitioned into n × n size blocks or patches. DCT
coefﬁcients are obtained for all the patches. In the second stage, a generalized
Gaussian density model is applied to DCT coefﬁcients of each block. The DC
coefﬁcient in a block is ignored as it does not convey structural information about
the block. The generalized Gaussian model is given as follows:
f xjα, β, γ
ð
Þ = αe −βjx −μ
ð
Þγ
ð1Þ
where γ—shape parameter; µ—mean; α—normalizing parameter; and β—scale
parameters, which is given as follows:
α =
βγ
2τ 1
λ
 
ð2Þ
β = 1
σ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
τ
3
γ
 
τ
1
γ
 
v
u
u
u
t
ð3Þ
where σ—standard deviation; τ—gamma function given as follows:
τðz) =
Z∞
0
tz −1e −tdt
ð4Þ
Features
Model Based Feature 
Extraction
Gaussian Modeling
DCT Computation
Image
Fig. 2 Proposed parametric model for feature extraction
Feature Extraction and Fuzzy-Based Feature Selection …
139

As β →∞, the distribution converges to a uniform distribution. Using gener-
alized Gaussian model functions, four features of the image are obtained. They are
as follows.
Shape Parameter
The shape parameter (γ) model-based feature is computed over all blocks in the
image. It determines the decay rate of the gamma function. This is calculated using
Eq. 1.
Coefﬁcient of Frequency Variation
Let X be a variable which represents the histogram of DCT coefﬁcients. The
coefﬁcient of frequency variation feature (ξ) is the ratio of measure of the spread of
the DCT coefﬁcient magnitudes ðσjXjÞ to measure of the center of the DCT coef-
ﬁcient magnitude distribution ðμjXjÞ. The average of coefﬁcient of frequency vari-
ation feature is measured.
Energy Sub-band Ratio
This feature measures the energy distribution in lower and higher bands, which can
be affected by distortions. The 5 × 5 DCT coefﬁcient matrix is divided into three
sub-bands S1 = {1, 2, 5, 6, 10}, S2 = {8, 9, 11, 12, 13, 15, 16, 17}, and S3 = {14,
18, 19, 22, 23}.
The average energy in frequency band n is given as follows:
En = σ2
n
ð5Þ
The ratio of the difference between average energy in frequency band n and
average energy up to frequency band n is calculated as follows:
Rn =
En −
1
n −1 ∑j < n Ej


En +
1
n −1 ∑j < n Ej
ð6Þ
Orientation Model-Based Feature
Here, each block of DCT coefﬁcients is divided into three orientation bands and is
modeled. The oriented DCT coefﬁcients are represented in three different shades as
A1 = {1, 2, 3, 4, 7, 8, 9, 14}, A2 = {10, 11, 15, 16}, and A2 = {6, 12, 13, 17, 18,
19, 23, 24}. Each band in the block is modeled by the generalized Gaussian model,
and coefﬁcient of frequency variation ξ is calculated. The variance and mean of ξ
are calculated for each of the three orientations.
2.4
Features
Fourteen GLCM features (contrast, inverse difference moment, entropy, correlation,
variance, sum average, sum of entropy, difference of entropy, cluster shade, cluster
140
A. Deshpande and P.P. Patavardhan

prominence, energy, auto-correlation, dissimilarity, and inverse difference nor-
malized) [17], Hu seven moments [18], and statistical features such as mean,
median, standard deviation, variance, skewness, kurtosis, and entropy are used as
features of iris polar images.
2.5
Feature Selection and Recognition
The fuzzy entropy and interval-valued-based feature selection method [19–21] to
select the best features which increase the recognition performance is used in the
proposed work. The extracted iris IQA features are sent to NN classiﬁer to rec-
ognize the iris patterns. Because of the ability of backpropagation networks [22] to
learn complicated multidimensional mapping, it is widely applied NN architecture.
In the proposed work, a gradient-based learning algorithm with adaptive learning
rate is adopted.
The performance of NN is calculated in terms of false acceptance rate (FAR) and
false rejection rate (FRR), which is deﬁned as follows:
• False acceptance rate (FAR): possibility of identifying an outsider as an enrolled
user.
• False rejection ratio (FRR): possibility of rejecting an enrolled user, as a person
is an outsider.
3
Results
The performance of the proposed feature extraction methods and the interval-valued
fuzzy entropy-based feature selection methods is evaluated by using CASIA long
range iris image database [23], captured at a distance of 3 m. The database consists
of 140 person’s faces and more than 10 images of each person. About 1400 iris
polar images are used in this experiment to evaluate the performance of proposed
approach. MATLAB 2013a is used to implement the proposed approach on Intel
Core i3 machine with the processor speed of 1.8 GHz and RAM size of 4 GB. The
proposed method is compared with Ferreira [24] as shown in Fig. 3.
The Fig. 3 shows that, the recognition accuracy achieved by the proposed method
is more compared to the recognition accuracy achievd by “without feature selection
method”. By using the proposed fuzzy selection method, better recognition accuracy
can be achieved. It is also observed that for less number of features, the recognition
accuracy is very less. The recognition accuracy of the proposed method is further
analyzed by comparing with NN and Radial Basis Function (RBF) kernel-based
support vector machine (SVM) classiﬁer, as shown in Table 1.
From Table 1, it is shown that the proposed method, using NN classiﬁer, gives
better recognition accuracy than the accuracy obtained without using the feature
selection method. It also shows the impact of variation in a number of classes on
Feature Extraction and Fuzzy-Based Feature Selection …
141

recognition accuracy. The recognition accuracy is good for less number of classes.
As the number of classes increases, there is a decrease in recognition performance.
The recognition accuracy of proposed method also decreases as a number of classes
increases, but still it shows better accuracy than without using feature selection
method and Ferreira proposed method. The false acceptance and false rejection
ratio of proposed system are shown in Table 2.
It is shown that the proposed approach using feature extraction gives good
recognition accuracy than Ferreira method and No FS.
4
Conclusion
Iris feature extraction and fuzzy entropy-based feature extraction methods are
discussed in this paper. The iris polar image features are extracted using
no-reference IQA model, GLCM, Hu seven moments, and statistical features. Using
70
75
80
85
90
5
10
15
20
25
30
35
Accuracy in %
Number of Features
No FS
Ferreira
Proposed
Fig. 3 Relation between number of features and recognition accuracy
Table 1 Recognition accuracy
Classes
SVM
NN
Without FS
Ferreira
With FS
Without FS
Ferreira
With FS
20
83.96
84.89
86.23
84.09
85.74
88.49
40
83.14
84.52
86.07
83.88
84.98
88.02
60
80.47
82.37
85.79
81.29
83.44
87.43
80
79.82
81.90
84.25
80.01
82.76
85.59
100
78.79
79.21
82.04
79.82
81.49
84.97
120
76.19
79.8
81.46
77.23
81.13
82.37
140
75.44
77.53
80.01
75.89
79.33
81.89
Table 2 FAR and FRR of
proposed system
S.
No.
Methodology
FAR
(%)
FRR
(%)
Accuracy
(%)
1
No FS
8.95
15.16
75.89
2
Ferreira
7.65
13.02
79.33
3
Proposed
7.80
11.01
81.19
142
A. Deshpande and P.P. Patavardhan

fuzzy entropy-based feature selection with NN classiﬁer, CASIA iris database is
simpliﬁed by using only subset of features instead of the whole dataset to do the
classiﬁcation. The experimental results show that the combination of feature
selection method using fuzzy entropy measures for extracted features and NN
classiﬁer is giving good results for long range captures iris polar images. The
proposed work can be extended further to classify the long range captured iris
images which are super-resolved using various methods such as bicubic, Gaussian
process regression.
References
1. Ross and A. K. Jain, “Multimodal Biometrics: An Overview,” Proceedings of 12th European
Signal Processing Conference, pp. 1221–1224, Vienna, Austria, September 2004.
2. Lee, H.M., Chen, C.M., Chen, J.M., Jou, Y.L.: An efﬁcient fuzzy classiﬁer with feature
selection based on fuzzy entropy. IEEE Transactions on Systems, Man, and Cybernetics, Part
B: Cybernetics 31(3), 426–432, 2001.
3. L. Blum and P. Langley, “Selection of relevant features and examples in machine learning,”
Artiﬁcial Intelligence, vol. 97, pp. 245–271, 1997.
4. M. Dash, H. Liu, and J. Yal, “Dimensionality reduction of unsupervised data,” 1997 IEEE
International Conference on Tools with Artiﬁcial Intelligence, pp. 532–539, 1997.
5. L. M. Belue and K. W. Bauer, “Determining input features for multilayer perceptrons,”
Neurocomputing, vol. 7, pp. 111–121, 1995.
6. R. K. De, N. R. Pal, and S. K. Pal, “Feature analysis: neural network and fuzzy set theoretic
approaches,” Pattern Recognition, vol. 30, no. 10, pp. 1579–1590, 1997.
7. Jihoon Yang and Vasant Honavar, “Feature Subset Selection Using a Genetic Algorithm,”
IEEE Intelligent System, March/April, pp. 44–49, 1998.
8. Bart Kosko, “Fuzzy Entropy and Conditioning,” Information Sciences, vol. 40, pp. 165–174,
1986.
9. G. Bradski, “The OpenCV library,” Dr. Dobb’s J. Software Tools, 2000.
10. U. Park, et. al., “Periocular biometrics in the visible spectrum,” IEEE Trans. Inf. Forens.
Security, 2011.
11. R. Wildes, “Iris recognition: an emerging biometric technology,” Proceedings of the IEEE,
vol. 85, 1997.
12. J. Daugman, “High conﬁdence visual recognition of persons by a test of statistical
independence,” Pattern Analysis and Machine Intelligence, IEEE Transactions, vol. 15, 1993.
13. J. Huan, M. Parris, J. Lee, and R. F. DeMara, “Scalable FPGA-based architecture for DCT
computation using dynamic partial reconﬁguration,” ACM Trans. Embedded Comput. Syst.,
vol. 9, no. 1, pp. 1–18, Oct. 2009.
14. M. Haque, “A 2-D fast cosine transform,” IEEE Trans. Acoust. Speech Signal Process., vol.
33, no. 6, pp. 1532–1539, Dec. 1985.
15. Michele A. Saad et al., “Blind Image Quality Assessment: A Natural Scene Statistics
Approach in the DCT Domain,” Ieee Transactions On Image Processing, Vol. 21, No. 8,
August 2012.
16. Q. Li and Z. Wang, “Reduced-reference image quality assessment using divisive-
normalization-based image representation,” IEEE J. Sel. Topics Signal Process., vol. 3, no.
2, pp. 202–211, Apr. 2009.
17. R. M. Haralick, K. Shanmugam and I. Dinstein “Textural features for Image Classiﬁcation”,
IEEE Transactions on Systems, Man and Cybernetics, Vol. 3, pp. 610–621, November 1973.
Feature Extraction and Fuzzy-Based Feature Selection …
143

18 Hu M., “Visual pattern recognition by moment invariants,” IRE Transaction on Information
Theory, 179–187, 1962.
19 Christer Carlsson et.al., “Fuzzy Entropy Used for Predictive Analytics,” IEEE International
Conference on Fuzzy Systems, 2015.
20 K. Nozaki, H. Ishibuichi, and T. Hideo, “Adaptive fuzzy rule-based classiﬁcation systems,”
IEEE Transactions on Fuzzy Systems, vol. 4, no. 3, pp. 238–250, 1996.
21 Szmidt, E., Kacprzyk, J.: Entropy for intuitionistic fuzzy sets. Fuzzy Sets and Systems 118,
467–477, 2001.
22 R. H. Abiyev and K. Altunkaya “Neural Network Based Biometric personal Identiﬁcation
with Fast Iris Segmentation”, Journal of control, Automation, and systems, 2009.
23 CASIA Iris Image Database, http://biometrics.idealtest.org/.
24 A. J. Ferreira, M. A. T. Figueiredo, An unsupervised approach to feature discretization and
selection, Pattern Recognition 45, 2012.
144
A. Deshpande and P.P. Patavardhan

Information Retrieves from Brain MRI
Images for Tumor Detection Using Hybrid
Technique K-means and Artiﬁcial Neural
Network (KMANN)
Manorama Sharma, G.N. Purohit and Saurabh Mukherjee
Abstract Medical imaging plays a signiﬁcant role in the ﬁeld of medical science. In
present scenario image segmentation is used to extract abnormal tissues from normal
tissues clearly in medical images. Tumor detection through brain MRI using auto-
matic system is effective and consumes lesser time which helps doctor in diagnosis.
A Tumor can convert into cancer, which is major leading cause of death. Automation
of tumor detection is required for detecting tumor on early stage. Proposed work
presents hybrid technique for information retrieval from brain MRI images. This
research work presents an efﬁcient technique based on K-means and artiﬁcial neural
network (KMANN). GLCM (Grey Level co-occurrence matrix) used for feature
extraction. Fuzzy Inference System is created using extracted feature which followed
by thresholding, morphological operator and Watershed segmentation for brain
tumor detection. Proposed method is used to identifying affected part of brain and
size of tumor from MRI image with the help of MATLAB R2013b is used.
Keywords Watershed ⋅Threshold ⋅Morphological operator
K-mean ⋅ANN
1
Introduction
The brain is vital organ of the human body and responsible for controlling all over
the function of human body. It interprets the senses, initiate the body movement,
control the behavior and monitor the different activities done by body. Brain is
M. Sharma (✉)
Banasthali University, Vanasthali, Rajasthan, India
e-mail: manorma.upadhyay@gmail.com
G.N. Purohit ⋅S. Mukherjee
CSE Department, Banasthali University, Vanasthali, Rajasthan, India
e-mail: gnpurohitjaipur@yahoo.com
S. Mukherjee
e-mail: mukherjee.saurabh@rediffmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_14
145

responsible for providing awareness of ourselves and environment even it controls
the muscles movements. Every reactive thought, feeling, and plans is developed by
the brain. MRI and CT scans are diagnostic modality used to show the internal
structure of brain. MRI is useful for extracting soft tissues and shows the internal
structure of the body. MRI shows the difference between normal tissues and
abnormal tissues. In this paper MRI images are used for ﬁnding affected area in
brain. MRI uses large magnet, radio waves to create detailed image of patient's
internal organ. MRI is not harmful for human body because there is no radiation. It
provides information about abnormal tissues for diagnosis purpose. MRI is
non invasive so it is very much popular among people and commonly used for
ﬁnding tumor size, shape and type. Abnormal cell in brain are called tumor.
Unwanted cell grows in brain which may cause of death among the human being.
Abnormal cell in the form of lump is found in brain are called brain tumor.
Unwanted cell grows in brain which may cause of death among the human being.
Normal process of brain is new cells are created an old or damaged cells die. When
this process is not working old or damages cells often create a piece of mass tissues
and it is called tumor. There are two types of tumor primary and secondary. Pri-
mary brain tumors can be benign or malignant. Primary tumor commonly found
in children. It is originating from cells of the brain that support nervous system.
Secondary brain tumor can be metastatic. It originates from cell of the other body
parts and spread to one and more areas. There are three stages:-
1. Benign: In this type normal tissues are not affected by abnormal tissues. When it
detect it can be diagnosis. It does not spread in other parts. It is primary brain
tumor.
2. Malignant: It is cancer and cause to death. It grows rapidly and belongs to
primary brain tumor.
3. Metastatic: It starts in other body parts (lungs and breast) and reached brain
with growing form. It reaches brain through the bloodstream. This type of tumor
called secondary or metastatic brain tumor. It is secondary memory.
It is very difﬁcult to extract information from medical images due to low con-
trasts, noise, and misplaced or diffusive edges [1] Image segmentation is used to
retrieve information from medical images for better diagnosis. With the help of that
information doctors can identify tumor shape and size which is used in diagnosis.
Image processing is used to enhance image quality or extracting information from
acquired image. Brain tumor is a dangerous disease commonly found in human
being. Hybrid technique helps physician to detect tumor on early stage which helps
for making better decision or take action. Due to detection of tumor on early stage
using death ratio is decreased. Segmentation algorithms are based on image
intensity values such as discontinuity and similarity [2]. There are many segmen-
tation techniques such as based on histogram, based on edge, based on artiﬁcial
neural network segmentation methods, based on region (region splitting, growing,
and merging), and based on clustering (Fuzzy C-means clustering, K-means
146
M. Sharma et al.

clustering etc.) [3–5]. In this research work hybrid technique is used for tumor
detection which is based on K-mean clustering and artiﬁcial neural network.
2
Literature Survey
Goswami and Bhaiya [6] proposed a method for brain tumor detection and then
speciﬁcation. They combined two techniques neural network and Fuzzy logic.
Hybrid Neuro Fuzzy system with the proposed method and removed the limitations
of single method. The methodology divided in three stages such as Segmentation,
feature extraction using gray Level Co-occurrence Matrix (GLCM) and tumor
classiﬁcation through brain MRI images. Hybrid Neuro Fuzzy system used to
extract the tumor part from images.
Othman and Basri [7] proposed Probabilistic Neural Network (PNN) for brain
tumor detection from MRI images. They suggested hybrid methods using Neural
Networks, and Fuzzy logic. Feature extraction using the principal component
analysis and the Probabilistic Neural Network used for decision making. Proba-
bilistic Neural Network presented fast and accurate classiﬁcation and proved
promising tool for classiﬁcation of the tumors.
Megersa and Alemu [8] presented a hybrid technique. The proposed method is
fully automatic tumor detection. It base on fuzzy Hopﬁeld neural network. Three
stage used in this research work image preprocessing, tumor detection, tumor
segmentation and visualization. For segmentation T1-weighted and T2-weighted
images used for detecting tumor. Quantitatively the method validated against
ground truth using commonly used validation metrics, i.e., Jaccard similarity index,
Dice similarity score, sensitivity and speciﬁcity.
Badran et al. [9] suggested a method for identifying tumor region in the brain.
This method used to classiﬁcation for normal brain MRI images and brain MRI
images with abnormalities. For tumor identiﬁcation preprocessing, image seg-
mentation, feature extraction and classiﬁcation using Neural Network techniques
used in the proposed work. Region of interest technique implemented for detecting
tumor area. Extraction.
Amin and Megeed [10] developed an automatic defect detection through intel-
ligent Neural Networks (NN) and using segmentation. In this method used to
classify various tumors in MRI images. Technique divided into two sections
(i) Hybrid neural networks with Principal Component Analysis (PCA) for dimen-
sionality reduction to extract the global features of the MRI images. (ii) For Seg-
mentation Wavelet Multi resolution Expectation Maximization (WMEM) algorithm
implemented which helps to extract the local features from MRI images. After that
Multi-Layer Perception (MLP) applied to classify the extracted feature.
Kharrat et al. [11] recommended in this research work about brain tumor
detection from MRI images. This methodology used enhancement, segmentation
and classiﬁcation. To improve the image quality enhancement process was applied.
Then they ware used wavelet Transform to decompose MRI images. Finally
Information Retrieves from Brain MRI Images for Tumor Detection …
147

K-means algorithm implemented to detect abnormal areas in MRI images and
tumor extract accurately.
Deshmukh and Khule [12] recommended a methodology for feature extraction
from raw images. They used ANFIS (Adaptive Neuro-Fuzzy Inference Systems)
technique for selecting abnormal images. This technique was fast in execution and
easy in implementation. By this technique the classiﬁcation for tumor images was
done. The Neuro Fuzzy logic used tumor identify tumor from MRI images. The
results were more accurate and less time consuming compared to existing tech-
nique. ANFIS technique was used to identifying multi object from images. Fuzzy
rules are selected for separation of abnormal tissues. In this approach two methods
were combined fuzzy logic and artiﬁcial neural network (ANN).
Dasgupta [13] proposed Modiﬁed Fuzzy C-Means (MFCM) technique. The
technique was less sensitive to noise. This technique was the modiﬁed method of
Fuzzy C-Means technique. Since Fuzzy C-Means is sensitive to noise and MRI
images are also with noise so a modiﬁed Fuzzy C-Means is required to reduce the
noise from the MRI images. MFCM was used to classify tumor in MRI images. The
proposed method was able to present better segmentation of brain tumor and pre-
sent better image quality compared to FCM. The technique improves the limitation
of Fuzzy C-Means. The ﬁltering was done during segmentation. It enhanced the
image quality for classiﬁcation of abnormal tissues for MRI images.
Sharma and Mukherjee [14] developed a method for brain tumor detection. They
were used a segmentation and Fuzzy C Mean technique. For feature extraction
Artiﬁcial Network Fuzzy inference System and genetic algorithm was used. They
were applied equalized histogram, morphological operator and edge detection for
raw image. After that Gray Level Co- occurrence Matrix (GLCM) was used for
capturing numerical feature. Using GLCM 20 features were extracted. They were
applied genetic algorithm for feature selection. A comparison was presented with
existing technique and found proposed technique shows more accuracy (96.6%),
sensitivity (95.3%) and speciﬁcity (98.67%). Islam and Ahmed [15] proposed image
segmentation technique based on K-means, K-Mediods, and Hierarchical clustering
technologies. They presented comparison between three clustering technique. And
they determine advantage and disadvantage of each algorithm. After implanting
algorithms, they mentioned that the K-means Clustering method has better per-
formance and easy to implement than other clustering method. It presents minimal
time for execution.
Abdel-Maksoud et al. [16] proposed a hybrid method using K-means and Fuzzy
C-means followed by thresholding technique. It combines beneﬁt of both the
method. K-means clustering technique is used to minimize the computation time
and Fuzzy C-means is used to provide accuracy. They include four steps:-(i) Input
image (ii) Preprocessing (K-means clustering and Fuzzy C-mean) (iii) Feature
extraction sing thresholding) (iv)Validate by ﬁnding iteration time, accuracy, per-
formance. K-means algorithm work faster than Fuzzy C-mean but Fuzzy C-mean
present accurately. They focused on minimal time execution with accurate result.
But method was not appropriate for 3D images.
148
M. Sharma et al.

3
Proposed Method
There are many image segmentation techniques for detection tumor from MRI
images. Proposed system is divided into three stages:
1. Pre-processing (thresholding, morphological operation and watershed)
2. Feature Extraction (GLCM)
3. Tumor Classiﬁcation (KMANN)
Block diagram is presented for proposed methodology with three phases
(Fig. 1).
MR Images: MR image database is collected from web resource (Fig. 2).
•
Gray, binary, thresholding
•
Morphological Processing
•
contour tumor regions
•
Watershed implementation
•
display image
Input image
Pre processing
K-mean + Artificial Neural 
Network
(KMANN)
Feature Extraction
Classification of Tumor
Output Image
Fig. 1 Proposed method for KMANN
Fig. 2 Dataset
Information Retrieves from Brain MRI Images for Tumor Detection …
149

4
Image Preprocessing
This phase is used to enhance the image quality. Brain images are very sensitive
than other images so high quality images are required in diagnosis. Using
enhanced image physician can extract accurate information in diagnosis. Therefore,
this stage consists of the following steps (Table 1, Fig. 3):
1. Image is input to the developed system for processing. Image is resizing in
256 × 256 dimensions then converted into gray scale image using size method.
2. Histogram equalization is applied to improve the image quality.
3. Image is converted into binary and thresholding is used. The pixel values are
classiﬁed as black and other is white using threshold [17].
G x, y
ð
Þ = 1f x, y
ð
Þ ≥T
0f x, y
ð
Þ ≤T
ð1Þ
4. Morphological Operations- It is very useful for sharpening regions and also
ﬁlls gaps in image. Erosion is used in proposed work which helps to turns object
smaller. Mathematically it can be represented as,
A B
ð
Þ x
ð Þ = x ∈X, x = a + b: a ∈A: b ∈B
f
g
ð2Þ
where A represents matrix of binary image B represents mask.
5. Watershed is implemented (Fig. 4).
6. Feature extraction—It is a procedure for extracting feature from pre processed
image for separating normal image and abnormal image. Many techniques used
for feature extraction like GLCM, Gabor and Fractals etc.
GLCM = graycomatrix(image, ‘Offset’, [2 0;0 2]);
Where, image represents grey scale image.
Graycomatrix function is available in MATLAB. Image feature values are cal-
culating using GLCM. In this table features of images are acquired for improving
the results these features are used in fuzzy logic for creating if then else rules Total
eleven features selected for proposed algorithm. Following features are selected by
Genetic algorithm:
Table 1 Pseudo code for pre processing
1.
Brain MRI image as input
2.
Resize the image (256 × 256 dimension)
3.
Image is converted into gray and apply histogram equalization to enhance image quality
4.
Binarize the image and apply thresholding
5.
Morphological operations erode applied on the binary image
6.
Call level set function and display the segmenting image
150
M. Sharma et al.

Fig. 3 Image with pre-processing
Fig. 4 Watershed implementation
Information Retrieves from Brain MRI Images for Tumor Detection …
151

1. Contrast: It is used to calculate intensity contrast between a pixel and its
neighbor pixel for image. Contrast is 0 for a constant image [17].
Contrast = ∑ij i −j
j
j2p i, j
ð
Þ
ð3Þ
where, P(I, j) pixel at location (i, j)
2. Correlation
3. Engery (E): It returns the sum of squared elements using GLCM. Energy is
used for a constant image [8].
E = ∑ijp i, j
ð
Þ2
ð4Þ
4. Homogeneity (HOM): It is used to measures the variation between elements in
the neighborhood using GLCM [8].
HOM = ∑i, j p i, j
ð
Þ
i + i −j
j
j
ð5Þ
5. Entropy It is used to measure of randomness [17].
EN = ∑
L −1
b = 0
p ij
ð Þ log2 p i, j
ð
Þ
f
g
ð6Þ
6. Mean It is deﬁned as
μ = 1
N ∑
N
i = 1
Ai −
ð7Þ
7. Variance (VAR): It calculates deviation of the gray level values from the mean
of image [17].
Var = ∑i ∑jp i, j
ð
Þp i, j
ð
Þμ2
ð8Þ
Table 2. Selected features for image (i) and image (ii) are as follows:-
Tumor Classiﬁcation In this phase hybrid technique is used for classiﬁcation
based on K-mean clustering and Neuro fuzzy system. Term used is this stage is as
follows:
K-Means clustering For retrieving information from medical images segmen-
tation is essential. Clustering is used to divide data into groups. K-means clustering
is popular method for clustering which is used to partitions of data into a k number
group of data. There are two phase (i) calculates the k centroid (ii) Selection of
nearest centroid from the respective data point.
Neuro Fuzzy System It is used to create fuzzy rules for image. A sample of
if-then rules for the MR brain tumor classiﬁcation.
152
M. Sharma et al.

5
Results and Discussion
Using hybrid technique tumor part is extracted and then it is presented with original
image. Neuro fuzzy system is used to create if then rules and ﬁfty rules are created
for system. After that artiﬁcial neural network is used to train the system and show
the performance (Figs. 5, 6, 7 and 8).
Table 2 Extracted features
for sample dataset
Sl. No
Feature No.
Image 1
Image 2
1
Fractal dimension
4.2011
3.1214
2
Contrast
0.4241 0.3584
2.0511 2.2984
3
Correlation
0.9345 0.9446
0.6709 0.628
4
Energy
0.1495 0.1544
0.1406 0.1387
5
Homogeneity
0.8777 0.8876
0.7882 0.7787
6
Entropy
6.6045
6.8717
7
Mean
0.0977
0.1117
8
Variance
0.0344
0.0371
9
Standard Deviation
0.1854
0.1927
10
Skewness
0.7759
0.4605
11
Kurtosis
3.4983
3.1091
Fig. 5 Tumor part and images with tumor
Information Retrieves from Brain MRI Images for Tumor Detection …
153

Fig. 6 FIS for selected features
Fig. 7 FIS rules creation
154
M. Sharma et al.

Fig. 8 FIS with if then rules
Fig. 9 Train network
Information Retrieves from Brain MRI Images for Tumor Detection …
155

6
Artiﬁcial Neural Network
See Figs. 9 and 10.
7
Conclusion
Information retrieval from brain MRI images is an important part of medical ﬁeld.
For retrieving information segmentation is used in medical image. MRI is popular
image model used for diagnostic brain tumor. is noninvasive so it is very much
popular among people and commonly used for ﬁnding tumor size, shape and type.
In this paper a hybrid technique is used to detect tumor and for classiﬁcation. It
includes K-mean algorithm which is used to detect a brain tumor quickly but it does
not present data accurately. So a combined technique is used to detect and classi-
fying tumor from brain MRI images. Proposed work is divided into three phases
(i) Pre-processing (thresholding, morphological operation and watershed) (ii) Fea-
ture Extraction (GLCM) (iii) Tumor Classiﬁcation (KMNN). Experimental results
determine the effectiveness of our approach. Proposed algorithm compared with the
single K-mean and Neuro fuzzy system.
Fig. 10 Performance
156
M. Sharma et al.

References
1. Dong, B., Chien, A., & Shen, Z. (2010). Frame based segmentation for medical
images. Communications in Mathematical Sciences, 9(2), 551–559.
2. Acharya, J., Gadhiya, S., & Raviya, K. (2013). Segmentation techniques for image analysis:
A review. International Journal of computer science and management research, 2(1),
1218–1221.
3. Naik, D., & Shah, P. (2014). A review on image segmentation clustering algorithms. Int J
Comput Sci Inform Technol, 5(3), 3289–93.
4. Christe, S. A., Malathy, K., & Kandaswamy, A. (2010). Improved hybrid segmentation of
brain MRI tissue and tumor using statistical features. ICTACT J Image Video Process, 1(1),
34–49.
5. Seerha, G. K., & Kaur, R. (2013). Review on recent image segmentation techniques. Inter-
national Journal on Computer Science and Engineering, 5(2), 109.
6. Goswami, S., & Bhaiya, L. K. P. (2013, October). A hybrid neuro-fuzzy approach for brain
abnormality detection using GLCM based feature extraction. In Emerging Trends in
Communication, Control, Signal Processing & Computing Applications (C2SPCA), 2013
International Conference on (pp. 1–7). IEEE.
7. Othman, M. F., & Basri, M. A. M. (2011, January). Probabilistic neural network for brain
tumor classiﬁcation. In 2011 Second International Conference on Intelligent Systems,
Modelling and Simulation (pp. 136–138). IEEE.
8. Megersa, Y., & Alemu, G. (2015, September). Brain tumor detection and segmentation using
hybrid intelligent algorithms. In AFRICON, 2015 (pp. 1–8). IEEE.
9. Badran, E. F., Mahmoud, E. G., & Hamdy, N. (2010, November). An algorithm for detecting
brain tumors in MRI images. In Computer Engineering and Systems (ICCES), 2010
International Conference on (pp. 368–373). IEEE.
10. Amin, S. E., & Megeed, M. A. (2012, May). Brain tumor diagnosis systems based on artiﬁcial
neural networks and segmentation using MRI. In Informatics and Systems (INFOS), 2012 8th
International Conference on (pp. MM-119). IEEE.
11. Kharrat, A., Benamrane, N., Messaoud, M. B., & Abid, M. (2009, November). Detection of
brain tumor in medical images. In Signals, Circuits and Systems (SCS), 2009 3rd
International Conference on (pp. 1–6). IEEE.
12. Deshmukh, R. J., & Khule, R. S. (2014). Brain tumor detection using artiﬁcial neural network
fuzzy inference system (ANFIS). International Journal of Computer Applications Technol-
ogy and Research, 3(3), 150–154.
13. Dasgupta, A. (2012). Demarcation of brain tumor using modiﬁed fuzzy C-means.
International Journal of Engineering Research and Applications, 2(4), 529–533.
14. Sharma, M., & Mukherjee, S. (2014). Fuzzy c-means, anﬁs and genetic algorithm for
segmenting astrocytoma-a type of brain tumor. IAES International Journal of Artiﬁcial
Intelligence, 3(1), 16.
15. Islam, S., & Ahmed, M. (2013). Implementation of image segmentation for natural images
using clustering methods.
16. Abdel-Maksoud, E., Elmogy, M., & Al-Awadi, R. (2015). Brain tumor segmentation based
on a hybrid clustering technique. Egyptian Informatics Journal, 16(1), 71–81.
17. MATLAB, User’s Guide, The Math Works.
Information Retrieves from Brain MRI Images for Tumor Detection …
157

Comparative Analysis and Evaluation
of Biclustering Algorithms
for Microarray Data
Ankush Maind and Shital Raut
Abstract From the last decade, the concept of biclustering becomes very popular
for the analysis of gene expression data. This is because of the advantages of
biclustering algorithms over the drawbacks of clustering algorithms on gene
expression data. Many biclustering algorithms have been published in recent years.
Some of them performed well on gene expression data and other have some issues.
In this paper, analysis of some popular biclustering algorithms have been done with
the help of experimental study. Along with this, survey of all the bicluster quality
measures which have been used for extracting biologically signiﬁcant biclusters in
various biclustering algorithms is also given. For the experimental study, synthetic
dataset has been used. Based on the experimental study, some comparative analyses
have been done, and some important issues related to the biclustering algorithms
have been pointed out. From this analytical as well as experimental study, new-
comers who are interested to do the research in the area of biclustering will get
proper direction for the better research.
Keywords Biclustering ⋅Gene expression data ⋅Biologically signiﬁcant etc.
1
Introduction
Humans are very fast to implement new and advanced technology in their
day-to-day activities. From last few decades, advancement in medical sciences is
going on but to overcome some common diseases such as cancer, HIV is still a big
challenge. Many times due to incorrect diagnosis and improper drugs, people have
to lose their lives. There is a need of proper diagnosis of diseases for the proper
treatment with the help of proper drugs for the recovery of the disease. To diagnose
A. Maind (✉) ⋅S. Raut
Computer Science & Engineering Department, VNIT, Nagpur, Maharashtra, India
e-mail: ankushmaind@gmail.com
S. Raut
e-mail: saraut@cse.vnit.ac.in
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_15
159

the disease and to discover the proper drug for the complex diseases are again a
very challenging task. For ﬁnding the better solution to this challenging task, the
proper analysis on the biological data is required. Many researchers have worked in
this area on various kinds of biological dataset for solving the different biological
problems.
Various types of biological data have used for the analysis; Microarray gene
expression data is one of them. Gene expression data play very important role in the
ﬁeld of the medical for the drugs discovery [1], disease diagnosis [2], gene iden-
tiﬁcation [3], pathway analysis, and other. The functions of the genes and the
mechanisms underlying diseases can be identiﬁed using gene expression data. So
for that, one has to ﬁnd the pattern from the microarray data, i.e., co-expressed
genes. Gene expression data can be generated from the microarray chip. A single
microarray chip can take the large amount of gene samples from the multiple tissues
at the different conditions or situations. Then after some pre-processing, this chip
will generate the gene expression data in the form of matrix in which row indicates
the genes and column indicates the samples or conditions. The value in cell of the
gene expression matrix represents the expression level of the particular gene at the
particular conditions. Presently, gene expression data are used in very wide range
for the research in the ﬁeld of bioinformatics. Because by doing the proper analysis
on gene expression data, one can ﬁnd the solution to many biological issues.
For the research on gene expression data, various techniques have been used.
Among these techniques, clustering is one of the important techniques used to
extract the signiﬁcant pattern from the data. Though it is very famous and favorite
solution in machine learning, it has some disadvantages like: it works only on the
one dimensional not simultaneously on two dimensional. Another drawback is that
an element in gene expression matrix can be present either in one cluster or not in
any cluster, but same element cannot be present in more than one cluster. But in
biology, same gene can be participated in more than one biological process. So, to
overcome these disadvantages of clustering, new techniques for ﬁnding the bio-
logically signiﬁcant pattern have been discovered known as biclustering technique.
In biclustering, simultaneous clustering will be done on both the directions, i.e., on
gene side and on conditions side. It will ﬁnd the correlated genes across subset of
conditions and also identify genes that are not behaved similar in all conditions.
Therefore, biclustering technique is more efﬁcient to ﬁnd biologically signiﬁcant
patterns as compared to clustering techniques.
The concept of biclustering was introduced by the J. Hartigan [4] in 1972, but he
has not applied it on gene expression data. Actual working of biclustering on gene
expression data has been started by Y. Cheng and G. church in 2000 [5]. After that,
plaid model [6], spectral biclustering algorithm [7], FLOC [8], SAMBA [9], ICS
[10], CoBi [11], BICLIC [12], and so many algorithms on the biclustering have
been published. Within a decade, biclustering became one of the popular techniques
for ﬁnding the biologically signiﬁcant patterns from gene expression dataset.
All biclustering algorithms cannot work properly on all types of gene expression
dataset. A particular algorithm is bounded to the speciﬁc types of dataset. But the
purpose of all biclustering algorithms is to ﬁnd the biologically signiﬁcant patterns.
160
A. Maind and S. Raut

Still some issues are present in existing algorithms which have been pointed out in
this paper with the help of experiment.
The remaining paper is divided into following sections: Details about the
microarray data, biclustering deﬁnition, biclustering types, and quality measures are
described in background section. Section 2.5 describes and compares the most
popular biclustering algorithms which have been used for the experimental study.
Section 3 describes the experimental setup and results of experiment. Section 4
discusses the results of all biclustering algorithms and issues which have been
pointed out from the experiment.
2
Background
In this section, some details about the microarray data, bicluster deﬁnition, types of
bicluster, and quality measure for bicluster have been explained.
2.1
Microarray Data
Microarray is a key technology in genomics. DNA microarray [13] data have been
used successfully in various research areas such as gene discovery, toxicological
research, disease diagnosis, and drug discovery. DNA Microarray data can be used
to measure the expression of thousands of genes at the same time. The functions of
the genes and the mechanisms underlying diseases can be identiﬁed using
microarray data. Generally, microarray data are called as gene expression data. The
process for getting the gene expression data includes, ﬁrst selection of cell, after that
RNA/DNA preparation have to do, then hybridization process on it after that will
get the array image. This resulted image has to analyze then ﬁnally will get the gene
expression data in matrix form.
Main advantage of microarray is its intrinsic robustness and also it is cheap in
cost. Microarrays are in the market from last several years, and today, the
microarrays are extremely sensitive and reliable. Microarray data are easily cus-
tomizable, and reproducible and can be adapted to many situations.
Gene expression data which are available in matrix form are called as gene
expression matrix, in which rows represent the genes and column represents the
conditions or samples under which gene expressed. The value in the cell represents
the amount of mRNA expressed by the particular gene under particular condition.
Figure 1 shows the gene expression matrix. This matrix is of m x n dimension, i.e.,
‘m’ genes and ‘n’ conditions, in which from G1, G2 to Gm are the genes and from
C1, C2, to Cn are the conditions; V11, V12, to Vmn are the amount of mRNA
expressed by genes under respective conditions.
Comparative Analysis and Evaluation of Biclustering Algorithms …
161

2.2
Bicluster Deﬁnition
Many people have deﬁned the bicluster in their literature. Bicluster is the submatrix
of subset of co-expressed genes across the subset of conditions under which these
genes co-expressed. Process of searching bicluster is called biclustering.
Let ‘X’ be the gene expression matrix of dimension m x n as shown in Fig. 1.
Where rows (1, 2,… m) are genes in the matrix and columns (1, 2,…. n) are the
conditions under which respective genes expressed. Bicluster ‘B’ is deﬁned as
subset of matrix ‘X’ containing set I of |I| number of genes and set J of |J| number of
conditions, in which bij indicates the expression levels of gene ‘i’ under condition
‘j’. Following Fig. 2 shows the bicluster ‘B’.
2.3
Types of Biclusters
All biclustering algorithms could not produce same results because of many special
constraints deﬁned by that speciﬁc biclustering algorithm. Many of the algorithms
have their own modiﬁed dataset to produce the best results, but they are not able to
produce the same results on real dataset. Therefore, the result of many algorithms
shows different types of biclusters on same real dataset.
Many researchers have deﬁned the types of biclusters in various ways. Table 1
shows the various types of biclusters [3] with their equations, where, ‘bij’ indicates
the expression levels of gene ‘i’ under condition ‘j’, ‘π’ is constant values for ‘B’,
‘α i’ is adjustment for rows i ϵ I, and ‘βj’ is adjustment for column j ϵ J.
Fig. 1 Gene expression
matrix
Fig. 2 Representation of
bicluster ‘B’
162
A. Maind and S. Raut

Biclusters are of various types as mentioned in Table 1. Details about the
mentioned bicluster types are as follows,
Constant bicluster: In constant types of patterns, expression levels of all genes
under all conditions are same, i.e., constant is available in all cells of bicluster.
Equation (1) of Table 1 represents constant bicluster. Constant biclusters also have
two categories, ﬁrst is row constant in which row-wise constant expression levels
are present. Equation (2) of Table 1 represents row constant biclusters. Another
category is column constant in which column-wise constant expression levels are
present. Column constant bicluster is represented with the help of Eq. (3) of
Table 1.
Coherent values: In this type of biclusters, expression levels of all genes are in the
form of additive or multiplicative. Biclusters with additive expression levels are
called additive biclusters. Additive type of bicluster is also called shifting bicluster.
Bicluster with multiplicative expression levels is called multiplicative bicluster.
Multiplicative type of bicluster is also called scaling bicluster.
Shifting biclusters: In shifting biclusters, expression levels of bicluster are added
with constants after every condition, so that the expression levels of genes will
shifted with the same difference into next expression level. Shifting bicluster is
represented with the help of Eq. (4) in Table 1, in which αi and βj are added
to the π.
Scaling biclusters: In scaling biclusters, expression levels of bicluster are multi-
plied with constants after every condition, so that the expression levels of genes will
be scaled with some multiplicative difference into next expression level. Scaling
bicluster is represented with the help of Eq. (5) in Table 1, in which αi and βj are
multiplied with constant ‘π’.
Coherent evolutions: This type of bicluster is increasing or decreasing type of
bicluster which cannot have particular types of patterns. It may be up-regulated or
down-regulated. There is no mathematical equation for these types of biclusters.
Table 1 Types of bicluster
with equation
Types of bicluster
Equation
Eq. No.
Constant
bij = π
1
Constant rows
bij = π + αi, bij = π *αi
2
Constant column
bij = π + βj, bij = π * βj
3
Shifting (coherent value)
bij = π +αi + βj
4
Scaling (coherent value)
bij = π * αi* βj
5
Coherent evolution
No equation
Comparative Analysis and Evaluation of Biclustering Algorithms …
163

2.4
Bicluster Quality Measure
Bicluster quality measure plays very important role for searching biologically
signiﬁcant biclusters from the gene expression data. It is useful to decide the quality
of biclusters. Many biclustering algorithms have been used various types of quality
measures. Each quality measure is used for extracting speciﬁc types of biclusters.
Till date, no single quality measure was discovered, which is useful for ﬁnding all
types of biclusters from the gene expression data. Very few quality measures are
Table 2 Quality measures for extracting various types of biclusters with details
Sr.
no.
Name of
quality
measure
Types of bicluster
extract
Advantages
Who has
applied ﬁrst
time
1
Variance
(Var)
Constant
Minimizes the sum of bicluster
variance
J. Hartigan [4]
2
Mean square
residue
(MSR)
Shifting
Efﬁciently used for extracting
additive types of patterns
Cheng and
Church [5]
3
Scaling
mean square
residue
(SMSR)
Scaling
Efﬁciently used for extracting
multiplicative types of patterns
Mukhopadhyay
[17]
4
Relevance
index (RI)
Constant Row,
Constant Column
Efﬁciently used for extracting
constant rows or constant
column types of patterns
Yip K,
Cheung D,
Ng M [18]
5
Pearson’s
correlation
coefﬁcient
(PCC)
Shifting
Efﬁciently used for accessing
shifting types of patterns. Access
the linear relationship among the
genes
L. Teng and L.
Chan [19]
6
Average
Spearman’s
Rho (ASR)
Shifting, Scaling
Use to measures the statistical
dependency between two
variables, assessing how well
their relationship can be
described using a monotonic
function, even if their
relationship is not linear
Ayadi et al. [20]
7
Average
correlation
value
(ACV)
Shifting, scaling
Use to evaluate the homogeneity
of a bicluster or a data matrix
Teng and Chan
[19]
8
Virtual error
(VE)
Shifting, Scaling
Best method for extracting
shifting or scaling patterns
F. Divina and B.
Pontes [21]
9
Transposed
virtual error
(VEt)
Shifting, Scaling,
shifting + Scaling,
It is efﬁcient to recognize
shifting, scaling and
shifting + scaling patterns in
biclusters either simultaneously
or independently
B. Pontes, R.
Giráldez, and
Jesús S. [22]
164
A. Maind and S. Raut

able to extract the biologically signiﬁcant biclusters from the microarray data
perfectly. Table 2 shows the bicluster quality measures which are used in various
biclustering algorithms along with its supported bicluster types, advantages, and
researchers who have ﬁrstly used this quality measures in biclustering algorithm.
2.5
Biclustering Algorithms
From the last decade, biclustering algorithms became very popular. Many research
papers have published it. Everyone tried to improve the performance of their own
biclustering algorithm over the existing algorithm. Some of the popular algorithms
have chosen for experiments are as follows.
Cheng and Church’s (CC) Approach: Cheng and Church [5] invented ﬁrst
biclustering approach which is applied to the gene expression data in 2000. Dataset
are used by them are yeast dataset and human (gene dataset). This approach has
adopted the strategy of iterative greedy search for ﬁnding the bicluster. CC’s
approach is divided into four steps:
1. Single node deletion
2. Multiple node deletion
3. Node addition
4. Finding a Given Number of Biclusters
They have used the mean square residue (MSR) in their approach as a quality
measure of the bicluster. Advantage of this method is, it is very simple. Drawbacks
of this approach are, ﬁrst is, MSR only able to capture shifting tendencies within the
data not the scaling, therefore it cannot ﬁnd the scaling bicluster. Second is, there
are always need of masking and need to calculate the threshold every time. Third
drawback is, there is a problem of random interference caused by masked biclus-
ters. Fourth is, it cannot ﬁnd the overlapped biclusters.
Plaid Model (PM): Plaid model [6] is an algorithm for exploratory analysis of
multivariate data. It was introduced by L. Lazzeroni, A. Owen, in 2000. In plaid
model, gene-condition matrix is represented as a superposition of layers related to
biclusters. They have used yeast DNA data, nutrition data, and foreign exchange
data for their experiment. They have also used the hierarchical clustering algorithm
for ordering. The advantages of this model are, it can ﬁnd the interpretable structure
and allows cluster to overlap. Another drawback of this method is all bicluster
membership functions are re-estimated at each step of the iteration.
xMotif: xMotif [2] algorithm was discovered by T.M. Murali, S. Kasif in 2003 with
the aim of extracting conserved gene expression motifs from gene expression data.
This algorithm is based on the probabilistic model and can identify genes which are
Comparative Analysis and Evaluation of Biclustering Algorithms …
165

conserved in more than one class but are in different states in different classes. This
algorithm is also helpful for extracting the coherent evolution types of patterns.
Spectral Biclustering (SB): Spectral biclustering [7] is a linear algebra-based
technique proposed by Y. Kluger, R. Basri, J. Chang, M. Gerstein in 2003. This
method is designed to cluster populations of different tumors assuming that each
tumor type has a subset of marker genes that exhibit overexpression and that
typically are not over expressed in other tumors. They have used lymphoma
microarray dataset and lymphoma Affymetrix dataset for the analysis. This is the
ﬁrst method of biclustering in which cancer dataset used. This method identiﬁes the
distinctive checkerboard like structure from the microarray data.
Iterative Signature Algorithm (ISA): ISA [14] was linear algebra-based approach
invented by S. Bergmann, J. Ihmels, N. Barkai in 2003. For noisy expression data,
this approach leads to better classiﬁcation due to the implementation of the
threshold. Drawback of this method is that there is no evaluation of the statistical
signiﬁcance and additionally two threshold parameters have to deﬁne. ISA is very
much suitable to any dataset that consists of multicomponent measurements.
Applications of the ISA could include the analysis of biological data on protein–
protein interactions or cell growth assays, as well as other large-scale data, where a
meaningful reduction of complexity is needed.
Flexible Overlapped Biclustering (FLOC): FLOC [8] is move-based probabilistic
algorithm introduced by J. Yang, H. Wang, W. Wang, and P.S. Yu in 2005. This
approach is the extension of CC’s approach. Most of the issues of CC’s approach
have been overcome by this method. FLOC approach proved that ‘random inter-
ference phenomenon’ plays a very important role for the discovery of the
high-quality bicluster. FLOC can discover a set of k possibly overlapping biclusters
simultaneously. They have used the MSR as a quality measure.
Binary inclusion-maximal biclustering algorithm (BIMAX): Bimax [15] is one
of the simple and fast method for the bicluster searching which was invented by
Prelic, Bleuler, Zimmermann, Wille, Buhlmann, Gruissem, Hennig, Thiele, and
Zitzler in 2006 with the aim of comparative study of existing algorithms such as CC
[5], OPSM [16], ISA [14], SAMBA [9], and xMotif [2] with Bimax [15]. This
approach applied to the synthetic and real dataset, i.e., gene expression data. An
advantage of Bimax algorithms is that it is capable of generating all optimal
biclusters, given the underlying binary data model. Bimax also requires less
memory resources. In their approach, they have proved that Bimax perform well as
compared to mentioned algorithms.
Factor Analysis for Bicluster Acquisition (FABIA): FABIA [1] is based on a
multiplicative model and it was introduced by S. Hochreiter, U. Bodenhofer, M.
Heusel, A. Mayr, A. Mitterecker in 2010. This algorithm helps to model heavy
tailed data as observed in gene expression data. They have applied FABIA
biclustering algorithm to 100 dataset. They also have applied the FABIA
166
A. Maind and S. Raut

successfully to drug design to ﬁnd compounds with similar effects on gene
expression data.
Biclustering by Correlated and Large Number of Individual Clustered seeds
(BICLIC): BICLIC [12] is one of the popular biclustering algorithms for extracting
biologically signiﬁcant bicluster from the gene expression data. It was published by
T. Yun, G.-S. Yi in 2013. In this algorithm, Pearsons correlation coefﬁcient was
used as a quality measure. BICLIC has solved the problem of changing output in
multiple executions on same dataset. But, BICLIC does not have the overlapping
control strategy.
3
Experimental Study
In experimental study, synthetic dataset have been taken. For this experiment, some
of the popular biclustering algorithms which are already implemented in ‘R’ lan-
guage have been used. 500 × 50 data matrices with random values which are
normally distributed have been generated. Then, six types of biclusters with various
sizes have been implanted. Types of biclusters such as constant bicluster, constant
rows bicluster, constant column bicluster, shifting bicluster, scaling bicluster, and
coherent evolution bicluster have been implanted into synthetic data matrix. After
that, all biclustering algorithms as mentioned applied one by one. Table 3 shows
the result of nine biclustering algorithms. First, column of Table 3 is name of
algorithms. Number of bicluster found is a second column, in which maximum limit
of biclusters are as ten, but some of these algorithms have extracted less than ten
biclusters. Extracting more number of bicluster is good quality of biclustering
algorithms, but these biclusters should be biologically signiﬁcant otherwise it is of
useless. Next column of Table 3 is maximum size of bicluster; it is a very important
parameter for the biological application because generally the size of bicluster is
more then it will give more biologically signiﬁcance. One can predict more
Table 3 Biclustering algorithms with results
Algorithm
No. of bicluster
found
Max. Size of
bicluster
Avg. size of
bicluster
Nature of
output
CC
10
28 × 16
20 × 14
Changing
PM
7
17 × 11
10 × 6
Changing
xMotif
4
20 × 20
18 × 13
Fixed
SB
2
16 × 6
16 × 6
Changing
ISA
4
30 × 3
25 × 2
Changing
FLOC
10
23 × 20
11 × 11
Changing
Bimax
7
20 × 20
12 × 9
Fixed
FABIA
10
10 × 10
4 × 14
Changing
BICLIC
10
50 × 49
36 × 32
Fixed
Comparative Analysis and Evaluation of Biclustering Algorithms …
167

accurately. Here, BICLIC algorithm has extracted the largest size of bicluster.
Fourth column of Table 3 shows the average size of biclusters, from which, will get
the idea about the average size of biclusters extracted from gene expression data
using respective algorithms. Largest average size bicluster was extracted by the
algorithm ‘BICLIC’. Fifth column is ‘nature of output’ which indicates the output
of biclustering algorithms is changing or ﬁxed after every execution on same
dataset. If the output of the algorithms is changing, then such result one cannot
easily apply to the biological applications. Accuracy of the result will not maintain
due to which one cannot predict easily about the disease and other thing also.
Therefore, output of the algorithms should be ﬁxed after the multiple executions on
same dataset.
Based on experiment analysis, comparative analysis of all these algorithms have
been done. Table 4 shows the comparative analysis of experimental study. First
column of table is of name of algorithms. Second column represents the complexity
of the algorithms which have been taken from their respective own authors pub-
lications as it is. Here, M is the number of genes, N indicates number of conditions,
B indicates number of biclusters, K indicates number of seeds, Niter indicates
iterations, ‘ns’ indicates number of samples randomly selected, ‘nd’ indicates
number of sets of genes for each sample, ‘Ni’ indicates input sets, M’ is average
number of genes, and N’ is the average number of conditions. Next column of table
is Extracted Bicluster type. From the experimental study, some observations have
been pointed out, such as which algorithms extract which types of patterns, for
example, constant, coherent values, scaling, shifting, or combinations as mentioned
in Table 1. Last column of table represents the strategy of biclustering algorithms
like one at a time, simultaneous, or set of bicluster at a time. It represents how the
bicluster is extracted whether it is in set or one at a time or simultaneously all.
Table 4 Comparative details of biclustering algorithm
Algorithm
Complexity
Extracted bicluster type
Strategy
CC
O(M × N)
Additive coherent values
(Shifting)
One at a time
PM
………
Coherent values
One at a time
xMotif
O(N × ns × nd)
Coherent evolution
Simultaneous
SB
………
Coherent values
Simultaneous
ISA
O(Niter × Ni × (N ×
M’ + M × N’))
Coherent values
One at a time
FLOC
O((N + M)2 × K × Niter)
Additive coherent values
(Shifting)
Simultaneous
Bimax
O(M × N × βmin)
Up-regulated
One set at a
time
FABIA
O(M × N × B2)
Constant values
One at a time
BICLIC
………
Coherent values, negative
correlation
One set at a
time
168
A. Maind and S. Raut

4
Discussion
In this paper, both the analytical and experimental studies of biclustering algorithms
have been done. In analytical study, analysis of the different types of biclusters,
types of quality measures used in various biclustering algorithms, and popular
biclustering algorithms with their advantages and disadvantages along with com-
plexity of biclustering algorithms have been done. From this study, some issues
have been pointed which are, the coherent evolution types of bicluster are very
difﬁcult for the extraction from the gene expression. Second difﬁcult pattern for the
extraction is scaling types of bicluster. For the extraction of various types of
biclusters, quality measure plays very important role. In this paper, analyses of all
the quality measures which have been used in various biclustering algorithms have
been done. Transposed virtual error quality measure is useful for the extraction of
three types of biclusters such as shifting, scaling, and shifting + scaling have been
observed. VEt is the only measure which is helpful for the extraction of these three
types of biclusters.
For the experimental analysis, synthetic dataset have been used. In synthetic
dataset, some biclusters of different types such as constant, scaling, shifting,
coherent evolutions have been implanted. All the results of experimental study have
mentioned in Sect. 3. From this experiment, one can say that some algorithms have
performed well but some have not. So, by using the same dataset, one cannot
compare all algorithms performance. Very few algorithms have extracted the pat-
terns as it is like implanted patterns in the matrix. Most of them have extracted the
biclusters but not accurately. So, to ﬁnd the perfect bicluster from the gene
expression data is a challenging task. Many researchers have claimed that their
algorithms are perfect but as per the observation from the experimental study, not a
single algorithm has extracted all these implanted biclusters accurately. Therefore,
one can work in this area to improve the biclustering techniques. From this
experimental study and theoretical analysis of other existing biclustering algo-
rithms, some issues related to the biclustering algorithms have been pointed out
which are as follows:
1. Most of the existing biclustering algorithms fail to efﬁciently ﬁnd the sets of
biologically signiﬁcant biclusters from gene expression data perfectly.
2. Many biclustering method results shows lack of stability. Because these
biclustering algorithms depend on random starting seeds, due to which contents
of resulting biclusters are changing every time even though the same biclus-
tering algorithm is applied to the same gene expression microarray dataset.
3. One cannot guess the truth of real biological gene expression dataset because it
is unknown. It is challenging to verify the biological relevance. Existing
algorithms also fail to completely extract the signiﬁcant pattern from the gene
expression dataset, i.e., accurate bicluster will not get.
4. Every existing algorithm is bound to the particular types of dataset. These
algorithms will not perform well on all types of dataset.
Comparative Analysis and Evaluation of Biclustering Algorithms …
169

5. Processing of large-scale gene expression data will take more computational
cost. Many existing biclustering algorithms have this problem of high
complexity.
6. Most of the gene expression dataset contain noise, due to which one cannot get
the proper results. For removing noise, proper pre-processing on gene
expression dataset is required, so that, one can ﬁnd the biologically signiﬁcant
patterns.
7. Searching constant, shifting, scaling, shifting + scaling types of patterns is
challenging task from the single biclustering algorithm. Many existing algo-
rithms fail to search all these types of patterns at the same execution.
8. To ﬁnd the set of larger size of biclusters efﬁciently is difﬁcult task by using the
simple workstation. There is a need of high conﬁguration server. So more
hardware cost also required.
9. After ﬁnding the pattern from the dataset, one needs to do the statistical vali-
dation and biological validation of the patterns to check the biological signif-
icance of the patterns. This is also a challenging task.
10. To ﬁnd the biologically signiﬁcant bicluster, there is a need of proper quality
measures. So choosing the proper quality measure for the speciﬁc types of
dataset is also challenging task.
References
1. Hochreiter S, Bodenhofer U, Heusel M, Mayr A, Mitterecker A, FABIA: factor analysis for
bicluster acquisition. Bioinformatics, Vol. 26. (2010) 1520–1527.
2. T.M. Murali, S. Kasif, Extracting conserved gene expression motifs from gene expression
data, Paciﬁc Symposium on Biocomputing, (2003) 77–88.
3. Madeira, S.C. and Oliveira, A.L. Biclustering algorithms for biological data analysis: a
survey. IEEE/ACM Trans. Comput. Biol. Bioinform. Vol. 1. (2004) 24–45.
4. J. Hartigan, Direct clustering of a data matrix, J. Am. Stat. Assoc. Vol. 67. (1972) 123–129.
5. Cheng, Y. and Church, G. Biclustering of expression data. Proc. Int. Conf. Intell. Syst. Mol.
Biol. (2000) 93–103.
6. L. Lazzeroni, A. Owen, Plaid models for gene expression data, Stat. Sinica. Vol. 12.
(2002) 61–86.
7. Y. Kluger, R. Basri, J. Chang, M. Gerstein, Spectral bicluster of microarray data: coclustering
genes and conditions, Genome Res. Vol. 13. (2003) 703–716.
8. J. Yang, H. Wang, W. Wang, P.S. Yu., An improved biclustering method for analyzing gene
expression proﬁles, Int. J. Artif. Intell. Tools. Vol. 14. (2005) 771–790.
9. A. Tanay, R. Sharan, R. Shamir, Discovering statistically signiﬁcant biclusters in gene
expression data, Bioinformatics, Vol. 18. (2002) 136–144.
10. H. Ahmed, P. Mahanta, D. Bhattacharyya, J. Kalita, Shifting-and-scaling correlation based
biclustering algorithm, IEEE/ACM Trans. Comput. Biol. Bioinform. Vol. 11. (2014) 1239–
1252.
11. S. Roy, D.K. Bhattacharyya, J.K. Kalita, CoBi: pattern based co-regulated biclustering of
gene expression data, Pattern Recogn. Lett., Vol. 34. (2013) 1669–1678.
12. T. Yun, G.-S. Yi, Biclustering for the comprehensive search of correlated gene expression
patterns using clustered seed expansion, BMC Genom., Vol. 14. (2013) 144.
170
A. Maind and S. Raut

13. P. Baldi and G.W. Hatﬁeld, DNA Microarrays and Gene Expression. From Experiments to
Data Analysis and Modelling. Cambridge Univ. Press, 2002.
14. S. Bergmann, J. Ihmels, N. Barkai, Iterative signature algorithm for the analysis of large-scale
gene expression data, Phys. Rev., Vol. 67. (2003) 031902.
15. Prelic A, Bleuler S, Zimmermann P, Wille A, Buhlmann P, et al., A systematic comparison
and evaluation of biclustering methods for gene expression data. Bioinformatics. Vol. 22.
(2006) 1122–1129.
16. A. Ben-Dor, B. Chor, R.M. Karp, Z. Yakhini. 2003. Discovering local structure in gene
expression data: the order-preserving submatrix problem. J. Comput. Biol. 10, 3–4 (2003),
373–384.
17. A. Mukhopadhyay, U. Maulik, S. Bandyopadhyay, A novel coherence measure for
discovering scaling biclusters from gene expression data, J. Bioinform. Comput. Biol. Vol.
7. (2009) 853–868.
18. Yip K, Cheung D, Ng M, Harp: A practical projected clustering algorithm. IEEE Transactions
on Knowledge and Data Engineering, Vol. 16. 1387–1397.
19. Li Teng and Laiwan Chan. Discovering biclusters by iteratively sorting with weighted
correlation coefﬁcient in gene expression data. Signal Processing Systems. Vol. 50. 267–280.
20. Ayadi W, Elloumi M, Hao J, A biclustering algorithm based on a bicluster enumeration tree:
application to dna microarray data. BioData mining, Vol. 2. (2009) 1–16.
21. F. Divina, B. Pontes, R. Giráldez, J.S. Aguilar-Ruiz, An effective measure for assessing the
quality of biclusters, Comput. Biol. Med., Vol. 42. (2012) 245–256.
22. Pontes B, Giráldez R, Aguilar-Ruiz J Measuring the quality of shifting and scaling patterns in
biclusters. Pattern Recognition in Bioinformatics, (2010) 242–252.
Comparative Analysis and Evaluation of Biclustering Algorithms …
171

Unconstrained Iris Image Super
Resolution in Transform Domain
Anand Deshpande and Prashant P. Patavardhan
Abstract In this paper, a method for super resolution of unconstrained or
long-range captured iris images in discrete cosine transform domain is proposed.
This
method
combines
iterated
back
projection
approach
with
the
Papoulis-Gerchberg (PG) method to super resolute the iris images in discrete cosine
transform domain. The method is tested on CASIA long-range iris database by
comparing and analyzing the structural similarity index matrix, peak signal-to-noise
ratio, visual information ﬁdelity in pixel domain, and execution time of bicubic,
Demirel, and Nazzal state-of-the-art algorithms. The result analysis shows that the
proposed method is well suited for super resolution of unconstrained iris images in
transform domain.
Keywords Super resolution ⋅Papoulis-Gerchberg ⋅Iris images
SSIM ⋅PSNR ⋅VIFp
1
Introduction
The necessity for reliable person identiﬁcation has distinctly increased to provide
secure environments and services. Biometric systems are well suitable for this
purpose as they provide the ease and efﬁciency of security, transactions, etc. Iris
recognition is robust and accurate method of recognition as iris has the property of
uniqueness. The existing iris recognition systems work for the images captured at
A. Deshpande (✉)
Department of Electronics and Communication Engineering, Angadi Institute
of Technology and Management, Belagavi, Karnataka, India
e-mail: deshpande.anandb@gmail.com
A. Deshpande ⋅P.P. Patavardhan
Department of Electronics and Communication Engineering, Gogte Institute
of Technology, Belagavi, Karnataka, India
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_16
173

distance less than 30 cm. The disadvantages of existing iris recognition systems are
as follows: acquisition distance should be less than 40 cm, and slow capturing time
and person has to stay till the end of capturing process. Recognizing iris at a long
distance or captured in unconstrained environment broadens the convenience and
enables additional applications in surveillance in critical infrastructure, border
control, and ambient intelligence. When iris images captured at long distance [1],
more than 3 m, are noticeably blurred, of low contrast, and deﬁcient in details in the
iris texture compared to images captured from very close proximity sensors. To
overcome such problems, super-resolution (SR) technique [2] and [3] is used.
The SR technique reconstructs the high-resolution (HR) image using single or a
sequence of low-resolution (LR) images which could be taken from one or more
cameras or could be frames of a video sequence [4]. The SR process can be
performed in frequency domain. The frequency-domain SR method is ﬁrst pro-
posed by [5], where the authors proposed the SR technique for the noiseless LR
images in discrete Fourier transform (DFT) domain. [6] and [7] proposed an iter-
ative expectation maximization (EM) algorithm for performing the registration,
blind de-convolution, and interpolation operations. The author [8] proposed the
discrete cosine transform (DCT) to perform fast image de-convolution for SR
image. The author [9, 10] proposed a wavelet-based SR method which takes care of
the error present in the registration process. The author [11] proposed a wavelet
domain-based SR framework which incorporates de-noising stage. The author [12]
proposed the discrete wavelet transform (DWT)-based interpolation method to
estimate the coefﬁcients in high-frequency sub-bands by iteratively removing noises
and preserving the useful edge coefﬁcients by contourlet transform. Author [13]
proposed wavelet transform and iterative back projection-based method. This
method uses wavelet transform to decompose the image, and then iterative back
projection algorithm reduces reconstruction error. A method to reconstruct signal
when diffraction limit of the signal and spectrum is proposed by Papoulis [14] and
Gerchberg [15] (PG).
In this paper, PG algorithm is modiﬁed to super resolute long-range captured or
unconstrained iris image in discrete cosine transform domain. Proposed approach
section discusses the proposed system to super resolute the iris image. The analysis
of proposed method is carried out in result section. Last section discusses the
conclusion of the proposed work.
2
Proposed Approach
The unconstrained iris image is super resolved in transform domain. The proposed
SR method is primarily inspired by the author [16]. The process of super resolution
of iris image is as shown in Fig. 1.
174
A. Deshpande and P.P. Patavardhan

2.1
Eye Detection and Segmentation
This block extracts face and eye by using AdaBoost-based [17, 18] face and
eye-pair classiﬁers. The iris is segmented from eye images using the method pro-
posed by author [19].
2.2
Image Patching
The image is divided into subsets of square areas. The resolution of the LR iris
polar image is 300 × 40, and image is divided into patches of size 10 × 10.
2.3
Super Resolution
The author [20] used the PG algorithm to super resolve multiple low-resolution
images. The drawback of the method is the steep cutoff in the frequency domain
introduces ringing artifacts near the edges in the super resolved image. To over-
come this, a back projection part is introduced in the PG method. The parameters
used in the proposed algorithms are as follows: y—input LR image arranged as
vector size N2 × 1 vectors, D—down-sampling matrix of size M2 × N2, B—blur
matrix of size N2 × N2, and Y—the estimated SR. Each patch is super resolved
using modiﬁed PG algorithm, discussed as below:
Step 1: Apply DCT to LR images.
K = DCTðYÞ
ð1Þ
Step 2: In DCT, few low-frequency coefﬁcients contain most of the information
about the image. Apply low-pass ﬁlter to retain the coefﬁcients near the
origin of the transformed image. The ﬁlter reduces the ringing effect
occurs due to Gibbs phenomenon
K′ = GLP(K)
ð2Þ
The frequency response of Gaussian low-pass ﬁlter (GLPF) can be
described as,
SR Image
Patch
Stitching
Super  
Resolution
Image 
Patch
Segmentation
Face and 
Eye Detection
Fig. 1 Proposed super resolution process
Unconstrained Iris Image Super Resolution in Transform Domain
175

H ðu, vÞ = e −D2 u, v
ð
Þ ̸2σ2
ð3Þ
where D ðu, vÞ is distance from the center of the frequency spectrum, and
σ decides cutoff frequency of GLPF. The size of the GLPF is 3 × 3 with
σ = 0.5
Step 3: Apply inverse DCT to low-pass ﬁltered image
Y = IDCTðK′Þ
ð4Þ
Step 4: Calculate error between LR and simulated LR obtained by applying
known down-sampling factor to the obtained SR image.
ε = y −DBY
ð5Þ
The error and the resultant SR image are dependent on the accuracy of
the image formation model. With every iteration, the resultant image
gets closer to the desired HR image, and it is expected to generate the
original HR image as the number of iterations increases.
Step 5: Check if error is greater than threshold. If yes, then
Y = Y + PðεÞ
ð6Þ
and go to Step 1. Repeat above steps till error is small enough.
Here, P is M2 × N2 matrix operator which projects the LR space error to the HR
space. Due to the error compensation, the blockiness will be introduced in the SR
image. This will be taken care by the low-pass ﬁltering part of the next iteration.
After super resolution, all patches are combined to get one reconstructed image.
3
Result
The proposed work is implemented using MATLAB2009a on Intel Core i3
machine with processor speed of 1.8 GHz and RAM size of 4 GB. The effective-
ness of the proposed system is validated by performing the experiments on CASIA
[21] long-range iris image database. The proposed algorithms are tested for 1400
iris polar images. The resolution of the extracted iris polar image is 300 × 40. The
upscaling factor 2 is considered during the super resolution process. After super
resolution, the size of the polar image is 600 × 80. The state-of-the-art algorithms
[22] and [23] are used as comparison baseline. The performance is evaluated by
quality analysis [24] such as structural similarity index matrix (SSIM), peak
signal-to-noise ratio (PSNR) and visual information ﬁdelity in pixel domain (VIFP).
Due to limitation of space, three person’s iris polar images performances are dis-
cussed. In this experiment, iteration 70 is set as optimal number. Figure 2 shows
176
A. Deshpande and P.P. Patavardhan

input iris polar images. The proposed system is analyzed by comparing with the
results of the state-of-the-art algorithms for iris polar images as shown Table 1.
From Table 1, it can be seen that the proposed system gives better image quality
compared to the state-of-the-art methods. The robustness of proposed method is
tested for additive white Gaussian noise (AWGN) and salt-and-pepper noise con-
ditions. For analysis purpose, salt-and-pepper noise is added by varying the % of
noise addition from 0 to 25. The graphical representation of average analysis of
image quality for salt-and-pepper noise is shown in Fig. 3.
The algorithm is further analyzed by adding AWGN by increasing the noise
standard deviation from 0 to 0.05. The graphical representation of average analysis
of image quality for AWGN is shown in Fig. 4.
The analysis of proposed method for different upscaling factors (Λ) is shown in
Table 2.
The proposed approach is further analyzed for increasing upscaling factor 4, 6,
and 8 as shown in Table 2. The quality of super resolved image decreases as the
up-sampling factor increases.
4
Conclusion
The goal of this work is to super resolve the long-range captured iris image in
transform domain. The PG algorithm is modiﬁed to super resolve the iris images in
DCT domain. This method is compared with existing state-of-the-art algorithms. It
is found that the proposed patch-based PG approach gives much better super res-
olution result than existing methods. It can be concluded that by using proposed
(1)
(2)
(3)
Fig. 2 Input LR polar images
Table 1 Comparison of super-resolution algorithms with PSNR, SSIM, and VIFp
Image
Demirel
Nazzal
Proposed
PSNR
(dB)
SSIM
VIFp
PSNR
(dB)
SSIM
VIFp
PSNR
(dB)
SSIM
VIFp
1
31.37
0.829
0.863
32.31
0.857
0.871
32.86
0.878
0.877
2
33.52
0.843
0.859
34.36
0.863
0.873
34.97
0.883
0.882
3
32.16
0.866
0.872
33.01
0.874
0.889
33.25
0.892
0.895
Unconstrained Iris Image Super Resolution in Transform Domain
177

(a)
(b)
(c)
10
15
20
25
30
35
5
10
15
20
25
50
PSNR
% of Noise
0.4
0.5
0.6
0.7
0.8
0.9
5
10
15
20
25
50
SSIM
% of Noise
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.75
0.8
5
10
15
20
25
50
VIFp
% of Noise
Demirel                         Nazzal                  
Proposed Method
Fig. 3 Image analysis under salt-and-pepper noise condition. a % of noise versus PSNR. b % of
noise versus SSIM. c % of noise versus VIFP
Demirel                          Nazzal                      Proposed Method
(a)
(b)
(c)
15
20
25
30
35
0.01 0.02 0.03 0.04 0.05
PSNR
Standard Deviation
0.6
0.65
0.7
0.75
0.8
0.85
0.01 0.02 0.03 0.04 0.05
SSIM
Standard Deviation
0.5
0.6
0.7
0.8
0.9
0.01 0.02 0.03 0.04 0.05
VIFp
Standard Deviation
Fig. 4 Image analysis under AWGN condition. a Standard deviation versus PSNR. b Standard
deviation versus SSIM. c Standard deviation versus VIFP
178
A. Deshpande and P.P. Patavardhan

approach, unconstrained iris images can be super resolved without much loss of
information. This work can be extended further to recognize the person by
extracting the features of iris images.
References
1. Kien Nguyen, et al., “Robust mean super- resolution for less cooperative NIR iris recognition
at a distance and on the move”, Symposium on Information and Communication Technology,
2010.
2. Subhasis Chaudhuri, “Super Resolution Imaging,” Kluwer Academic Publishers, 2002.
3. Sung Cheol Park, et al., “Super-Resolution Image Reconstruction: A Technical Overview,”
IEEE Signal Processing Magazine, 2003.
4. Thomas Kohler, et al., “Multi-frame Super-resolution with Quality Self assessment for retinal
fundus videos,” Medical Image Computing and Computer-Assisted Intervention, Lecture
Notes in Computer Science, 2014.
5. Tsai, R.Y., Huang, T.S.: Multiframe image restoration and registration. In: Huang, T.S. (ed.)
Advances in Computer Vision and Image Processing. JAI Press Inc., London (1984).
6. Woods, N.A., Galatsanos, N.P., Katsaggelos, A.K.: Stochastic methods for joint registration,
restoration, and interpolation of multiple undersampled images. IEEE Trans. Image Process.
15, 201–213 (2006).
7. Dempster, P., Laird, N.M., Rubin, D.B.: Maximum likelihood from incomplete data via the
EM algorithm. J. R. Stat. Soc. B 39(1), 1–38 (1977).
8. Rhee, S., Kang, M.G.: Discrete cosine transform based regularized high-resolution image
reconstruction algorithm. Opt. Eng. 38, 1348–1356 (1999).
9. Ji, H., Fermuller, C.: Wavelet-based super-resolution reconstruction: theory and algorithm. In:
Proceedings of the European Conference on Computer Vision, pp. 295–307. Graz, Austria
(2006).
10. Ji, H., Fermuller, C.: Robust wavelet-based super-resolution reconstruction: theory and
algorithm. IEEE Trans. Pattern Anal. Mach. Intell. 31, 649–660 (2009).
11. Chappalli, M.B., Bose, N.K.: Simultaneous noise ﬁltering and super-resolution with
second-generation wavelets. IEEE Signal Process. Lett. 12, 772–775 (2005).
12. Mueller, N., Lu, Y., Do, M.N. (2007). Image interpolation using multi-scale geometric
representations. In Proceedings of SPIE computational imaging V (vol. 6498, p. 64980A).
13. Song JW, Xu YM, Xiao XJ. A Super Resolution Algorithm Based on Wavelet Transform and
Iterative Back Projection. Computer Technology and Development 2015.
14. Papoulis, A., “A new algorithm in spectral analysis and band-limited extrapolation,” IEEE
Transactions on Circuits and Systems, 735– 742, 1975.
15. Gerchberg, R., “Super-resolution through error energy reduction,” Optical Acta, 21, 709–720,
1974.
Table 2 Analysis of proposed algorithm for various upscaling factors
Image
No.
Λ = 4
Λ = 6
Λ = 8
PSNR
(dB)
SSIM
VIFP
PSNR
(dB)
SSIM
VIFP
PSNR
(dB)
SSIM
VIFP
1
30.12
0.803
0.792
27.46
0.781
0.743
25.33
0.706
0.710
2
32.23
0.821
0.817
29.19
0.789
0.794
27.94
0.715
0.719
3
32.49
0.824
0.831
29.84
0.801
0.790
28.06
0.732
0.726
Unconstrained Iris Image Super Resolution in Transform Domain
179

16. Priyam
Chatterjee
et
al.
“Application
of
Papoulis-Gerchberg
Method
in
Image
Super-resolution and Inpainting,” The Computer Journal, 2007.
17. G. Bradski, “The OpenCV library,” Dr. Dobb’s J. Software Tools, 2000.
18. U. Park, et al., “Periocular biometrics in the visible spectrum,” IEEE Trans. Inf. Forens.
Security, 2011.
19. Anand Deshpande, Prashant Patavardhan, “Segmentation And Quality Analysis Of Long
Range Captured Iris Image,” ICTACT Journal on Image and Video Processing, 2016.
20. Vandewalle, P., Susstrunk, S., and Vetterli, M., “Super-resolution images reconstructed from
aliased images,” SPIE Visual Communication and Image Processing Conference, Lugano,
Switzerland, July, pp. 1398–1405, 2003.
21. CASIA Iris Image Database, http://biometrics.idealtest.org/.
22. H. Demirel, G. Anbarjafari, Image resolution enhancement by using discrete and stationary
wavelet decomposition. IEEE Trans. Image Process. 20(5), 1458–1460 (2011).
23. M. Nazzal, H. Ozkaramanli “Wavelet domain dictionary learning-based single image
superresolution,” Springer- Signal, Image and Video Processing Volume 9, Issue 7, pp 1491–
1501, 2014.
24 Z. Wang and A. Bovik, “Image quality assessment: from error visibility to structural
similarity,” Image Processing vol. 13, no. 4, pp. 600–12, Apr. 2004.
180
A. Deshpande and P.P. Patavardhan

Part III
Security

An Extension to Modiﬁed Harn Digital
Signature Scheme with the Feature
of Message Recovery
Shailendra Kumar Tripathi and Bhupendra Gupta
Abstract Since the K.S. McCurley paper, the use of various dissimilar crypto-
graphic assumptions achieved widespread attention in the enhancement of security
of a cryptosystem and furthermore well explored. However, researchers analyzed
and tried to reduce the probabilistic forgery without compromising the security. In
this paper, we propose an efﬁcient digital signature scheme by the use of dissimilar
cryptographic assumptions; discrete logarithm problem as well as integer factor-
ization problem with the additional feature of message recovery, which provides the
extension for some applications; identity-based public-keys without restrictions in
trust and a one-pass key exchange protocol with mutual authentication.
Keywords Digital signature ⋅Discrete logarithm problem
Integer factorization problem ⋅Message recovery ⋅Identity-based certiﬁcate
Authenticity ⋅Forgery
1
Introduction
In 1976, W. Diffe and M.E. Hellman initiated the idea of public-key cryptography by
their article titled “New Directions in Cryptography” [1] and widely studied in
modern cryptography. The modern cryptography has mainly four aspects: conﬁ-
dentiality, authenticity, integrity and non-repudiation. In public-key cryptography,
an encryption technique only provides conﬁdentiality. Therefore, to consider
remaining aspects of modern cryptography, digital signature became an imperative
requirement of public-key cryptography. In the literature, most of the researchers
concentrated on developing digital signature schemes based on integer factorization;
S.K. Tripathi (✉) ⋅B. Gupta
Indian Institute of Information Technology, Design and Manufacturing,
Jabalpur 482005, India
e-mail: shailendratripathi26@gmail.com
B. Gupta
e-mail: gupta.bhupendra@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_17
183

RSA [2] digital signature scheme and discrete logarithm cryptographic assumptions;
El Gamal [3], Schnorr [4], NIST’s digital signature schemes (DSA) [5].
These digital signature schemes are secure because there are no efﬁcient tech-
niques available in the literature to solve any one of these cryptographic assump-
tions. However, these digital signature schemes will no longer be secured if any one
of these cryptographic assumptions can be solved in some probabilistic sense.
Therefore, the security could be enhanced by using various dissimilar cryptographic
assumptions simultaneously.
Taking this into consideration, in 1988, K.S. McCurley [6] proposed ﬁrst key
distribution based on two dissimilar cryptographic assumptions: integer factoriza-
tion and discrete logarithm problem simultaneously. In 1992, E.F. Brickel and K.S.
McCurley [7] designed an interactive identiﬁcation scheme also based on these two
dissimilar cryptographic assumptions but these schemes were not efﬁcient [8].
In 1994, Harn [8] designed a digital signature scheme based on these two
dissimilar cryptographic assumptions, integer factorization and discrete logarithm
problem, and was claimed to be unbreakable if these two cryptographic assump-
tions are simultaneously unsolvable.
2
Harn Digital Signature Scheme and Its Security
2.1
Harn Digital Signature Scheme
In Harn [8] digital signature scheme, each user selects a large prime p = 2p′q′ + 1,
where p′ = 2p′′ + 1, q′ = 2q′′ + 1 and p′, q′, p′′, q′′ are also large primes. Then user
selects a primitive element g ∈Z*
p and also selects a random element X ∈Z*
p and
computes Y = gXmodp. User also computes d satisfying the relation 3dmod∅
∅p
ð Þ
ð
Þ = 1. Then, user keeps secret private-key p′, q′, p′′, q′′, X, d


, and announces
publically his public-key p, Y, 3
ð
Þ.
Suppose, user A wants to sign a message M ∈Zp, selects a random element and
k ∈Z*
p computes using his private-key
p′, q′, p′′, q′′, X, d


, r = gkmodp, s′ = k −1
M −Xr
ð
Þmod p −1
ð
Þ, and s = s′
 dmod p −1
ð
Þ. Then, the signature pair is M, r, s
ð
Þ.
In veriﬁcation, any user can verify using A’s public-key
p, Y, 3
ð
Þ, compute
s′ = s3mod p −1
ð
Þ and check gM =
? rs′Yrmod p. If yes then accept, reject otherwise.
2.2
The Security of Harn Digital Signature Scheme
The Harn [8] digital signature scheme is basically an extension to El Gamal digital
signature scheme. It makes secure to El Gamal digital signature scheme from
184
S.K. Tripathi and B. Gupta

forgery (an adversary has ability to forge a valid signature σ, for a message M, that
has not been signed in past by legitimate user).
In El Gamal digital signature, forgery does not allow to forge a message of
adversary’s choice and also can be prevented by using one-way hash function.
Harn [8] claimed that scheme is unforgeable if these dissimilar cryptographic
assumptions, integer factorization and discrete logarithms, are simultaneously
unsolvable.
However, in 1996, N.-Y. Lee and T. Hawang [9] proved that if adversary has
ability to compute discrete logarithm, then signature can be forged with high
probabilities.
As the adversary, can compute the discrete logarithm problem; given
Y = gXmod p
ð
Þ, computes X. There are two possible cases.
Case 1: X is an odd number. Suppose, adversary wants to sign a forged
message bM ﬁrst computes
s = p −1
ð
Þ ̸2 = p
′q
′


,
ð1Þ
and
r = X −1 bMmod p −1
ð
Þ.
ð2Þ
An adversary can solve a discrete logarithm problem, so a unique k can be
computed such that
r = gkmod p
ð3Þ
If k = 2k′ is even, then (r, s) is a forged signature pair for message bM. But the
probability of k being even is 1/2. Thus, the probability of forging a signature pair
for any message under this case is 1/2 also (see Ref. [9] for proof ).
Case 2: X is an even number. Since p −1
ð
Þ is also even, so to ﬁnd X −1 such
that XX −1 = 1mod p −1
ð
Þ is impossible. Then, r cannot be determined from Eq. (2).
However, the adversary still has the chance by the following method.
Assume adversary ﬁnds t by satisfying the relation X = t × X′, where X’ is an
odd integer and t also divides bM
= tc
M′


(forgery works for messages that can be
divided by t). Now, adversary can compute a below relation and can forge.
r = X′

 −1 bM
′mod p −1
ð
Þ
ð4Þ
To check the validity of signature pair (r, s), see Ref. [9].
In 1995, N.-Y. Lee and T. Hwang [9] remarked that these attacks are still valid
even after using one-way hash function before signing the message and can be
avoided if s’ is not allowed to be equal to p′q′. They also proposed a more efﬁcient
digital signature scheme as follows.
An Extension to Modiﬁed Harn Digital Signature Scheme …
185

3
Modiﬁed Horn Digital Signature Scheme
In 1991, Schnorr [4] presented a very efﬁcient and secure digital signature scheme
for smart cards. Later, in 1993, Yen and Laih [10] proposed an alternative of
Schnorr digital signature scheme without the use of one-way hash function. The
security of Schnorr relies on strength of one-way hash function. However, Boyd
[11] and Nyberg [12] proved that the Yen-Laih scheme is not secure without the use
of one-way hash function.
However, Schnorr [4] digital signature scheme is secure if a discrete logarithm
problem can be solved. Then, N.-Y. Lee and T. Hwang [9] proposed a digital
signature scheme based on two dissimilar cryptographic assumptions, discrete
logarithm and integer factorization problem, which make secure to Schnorr [4]
digital signature scheme and provide an alternative for Harn [8] digital signature by
the following modiﬁcations.
In this scheme, s’ is modiﬁed such that
s′ = X + K H mod p −1
ð
Þ,
ð5Þ
where H = h M, r
ð
Þ; H is the value of one-way hash function h(.) and
s = s′
 dmod p −1
ð
Þ.
ð6Þ
Then, signature pair (r, s) for message M can be veriﬁed by the following
relation
gs
′
=
? Y rHmod p.
ð7Þ
Remark: N.-Y. Lee and T. Hwang [9] also remarked that a similar modiﬁcation
h M, r
ð
Þ = ks′ + Xrmodðp −1Þ
in
place
of
Harn
digital
signature
scheme
M = ks′ + Xrmodðp −1Þ suggested by one of our paper’s anonymous referees makes
secure from possible attacks in Harn digital signature scheme. But modiﬁed Harn
digital signature scheme is more efﬁcient and secure than Harn digital signature
scheme.
For the time being, the RSA digital signature scheme has received widespread
attention in unique sense that encryption and signature transformations are inverses
of each other. The RSA digital signature scheme has a feature of appendix (message
is required as input in veriﬁcation) as well as message recovery (message is
recovered). In El Gamal, Schnorr and DSA digital signature schemes have not the
feature of message recovery. In message recovery has some special advantages;
application without use of one-way hash function are possible, bandwidth utiliza-
tion, direct use in other applications such as identity-based public-key systems or a
one-way key exchange protocol with mutual authentications.
In 1993, Nyberg and Rueppel [13] presented some modiﬁcations in DSA which
allows signature with message recovery. The scheme is modiﬁed in such a way that
186
S.K. Tripathi and B. Gupta

public-key
Y = g −Xmod p
ð
Þ, and r = Mg −kmod p, and s = k + X r mod q
where
X, k ∈Z*
q and q is a large prime factor of p −1
ð
Þand p is also a large prime. Message
M can be recovered by computing gsYrr.
Nyberg and Rueppel [13] also presented the application of identity-based
public-keys without restrictions in trust and a one-pass key exchange protocol with
mutual authentication. But this digital signature scheme with message recovery
allowed to adversary to forge a valid signature for all messages of the form Mgt,
where t ∈Zq by modiﬁcation in signature pair
r, s + t
ð
Þ if it possesses a valid
signature pair (r, s) for message M. To prevent this, a cryptographic hash function
or redundancy function is suggested to use as with the other schemes (see Ref.
[13]).
Thereafter, Nyberg and Rueppel [14] tried to design inversion-less scheme with
the principle that inverse should not be required in computation of signature, ver-
iﬁcation, and message recovery. Taking this principle into consideration, ﬁxed ﬁrst
part of signature is r = Mg −kmod p and second part is designed by the generalized
signature equation for El Gamal type schemes can be written as
ak + bX + c = 0 mod q.
ð8Þ
where
r′, s, M


are used in place of coefﬁcients (a, b, c) and q is a large prime
factor of (p −1). By the permutation of these coefﬁcients, author presented an
inversion-less signature scheme k −r′X −s = 0 mod q with message recovery
equation M = gsYr′r mod p. For security consideration, Nyberg and Rueppel [14]
have given a theorem and proof that signature scheme is secure from forgery. But,
in 2000, C.C. Lin and C.S. Laih [15] have done cryptanalysis for the signature
scheme k −r′X −s = 0 mod q with message recovery equation M = gsYr
′r modp and
showed that the scheme is not free from forgery. Since then various other [16–20]
digital signature schemes have been proposed, but these schemes did not provide
the feature of message recovery with veriﬁcation in an efﬁcient way.
4
Motivation
However, the modiﬁed Harn digital signature scheme is secure from forgery, but
has not a feature of message recovery and due to this also has not applications:
identity-based public-keys without restrictions in trust and a one-pass key exchange
protocol with mutual authentication. The feature of message recovery motivates us
to design a digital signature scheme with the principle of inversion-less based on
two dissimilar cryptographic assumptions, discrete logarithm and integer factor-
ization problem, which can be extended for these applications.
In design of proposed digital signature scheme, we will use a new function
called mask generation function (MGF). In 1996, Bellare and Rogway proposed
An Extension to Modiﬁed Harn Digital Signature Scheme …
187

Full Domain Hashing (FDH) [21] by using such a function called MGF to prepare
the input message M of key-size. The functionality of MGF uses multiple hashes of
the input concatenated with the counter variable to generate the variable length
output. The counter variable decides output size.
5
Proposed Digital Signature Scheme
In this section, we propose a digital signature scheme which is basically an
extension to modiﬁed Harn [9] digital signature scheme with the feature of message
recovery.
In proposed digital signature scheme, we provide both features: veriﬁcation and
message recovery.
• Key-Generation Algorithm: Each user selects a large random prime p such that
p = 2p′q′and p′ = 2p′′ + 1, q′ = 2q′′ + 1, where p′, q′, p′′, q′′ all are prime. Then,
select a random integer X ∈Z*
p and compute Y = gXmod p. Thereafter, each user
deﬁnes a large modulus N = p′q′ and computes Euler phi-function ϕ N
ð Þ =
ðp′ −1Þðq′ −1Þ, and also selects a random integer e satisfying the relation
ed = 1mod N
ð Þ. Thus, the public and private-key pair are
p, N, Y, e
ð
Þ, ðp, N, f
N
ð Þ, X, dÞ respectively.
• Signing Algorithm: Suppose, user A wants to sign a message M such that
0 ≤M ≤p −1, chooses a random integer k ∈Z*
p and computes.
r = gkmod p,
ð9Þ
and
H = h M, r
ð
Þmod p −1
ð
Þ
ð10Þ
where h(.) is collision-free one-way hash function. Then,
s = X + kH mod p −1
ð
Þ,
ð11Þ
and
t = MGF sð Þ⊕M
ð
Þdmod N.
ð12Þ
where MGF is a mask generation function, which is hash of variable length output.
Then, signature triple is r, s, t
ð
Þ.
188
S.K. Tripathi and B. Gupta

• Veriﬁcation and Recovery Algorithm:
– Message Recovery:
Here, any user who wants to verify a given signature triple
r, s, t
ð
Þ, ﬁrst
recovers the message M by calculating MGF(s) and
M = temodN⊕MGF sð Þ.
ð13Þ
– Veriﬁcation:-
Then, verify after calculating H = h M, r
ð
Þmod p −1
ð
Þ
gs =
? YrHmod p
ð14Þ
However, the proposed digital signature scheme is similar to the scheme
r′k + X −s = 0 mod q with message recovery equation M = g −sðr′Þ −1Yðr′Þ −1r mod p,
proposed in paper [14] but follow the principle of inversion-less and this scheme
does not. In this scheme, message recovery equation requires the inverse of r’ and if
q is not prime, then how to handle this problem is discussed in [22].
6
Application of Proposed Digital Signature Scheme
In this section, we show that how proposed digital signature scheme works for the
application of a one-pass key exchange protocol with mutual authentication and
identity-based public-keys without restrictions in trust.
6.1
A One-Pass Key Exchange Protocol with Mutual
Authentications
The basic Difﬁe-Hellman key exchange protocol establishes secret session key in
two-pass without giving mutual authentication. It suffers a famous attack named as
“Man-in-Middle-Attack”. All public-key encryption schemes establish secret key
session in one-pass but authenticate only the receiver. In this section, we extend to
proposed digital signature scheme to achieve mutual authentication in one-pass.
The main purpose of this application is that if sender A wants to establish a secret
key session with receiver B, then only receiver B can possibly compute with given
signature of A.
Let user A having public-key p, N, YA, eA
ð
Þ and private-key pair p, XA, dA
ð
Þ wants
to initiate secret key session with user B having public-key
p, N, YB, eB
ð
Þ and
private-key pair p, XB, dB
ð
Þ. Then, the user selects two random integers R and k and
keeps secret and computes
An Extension to Modiﬁed Harn Digital Signature Scheme …
189

r = gR + kmod p,
ð15Þ
and
H = h R, r
ð
Þmod p −1
ð
Þ.
ð16Þ
Then, s such that
s = XA + kHmod p −1
ð
Þ,
ð17Þ
and t with some modiﬁcation such that
t = h sð Þ ⊕H
ð
ÞdAmod N.
ð18Þ
Then, user A sends signature triple r, s, t
ð
Þ to user B and computes shared secret
key
K = YRH
B mod p = gXBRHmod p
ð19Þ
Now, user B computes the following equations
H = teAmod N ⊕h sð Þ,
ð20Þ
and
YArHg −smod p.
ð21Þ
By Eq. (21), user B recovers gRHmodp, but R is unknown and computes shared
secret key
gRH

XBmod N = gXBRHmod p
ð22Þ
Also, the proposed scheme has the application: identity-based public-keys
without restrictions in trust. The basic idea behind this application is that any user
after the registration with key center can authenticate himself to any other user
without further correspondence with the key center. However, in this application,
users have to trust with key center and also to generate their private-keys.
In paper [23], Guinther designed an identity-based public-key system, where
each user is identiﬁed by the distinguished names, where key center creates
identity-based certiﬁcates for identiﬁed users, having the following properties.
1. Each user receives his public-key from the correct combination of an
identity-based certiﬁcate and from his name.
190
S.K. Tripathi and B. Gupta

2. The current public-key can only be recovered from an authentic identity-based
certiﬁcate, but the authenticity of the certiﬁcate cannot be directly veriﬁed.
In proposed digital signature scheme, additionally the authenticity of the cer-
tiﬁcate can be directly veriﬁed.
7
Security of Proposed Digital Signature Scheme
In this section, we investigate security of proposed scheme by several possible
ways.
1. Given public-key p, N, Y, e
ð
Þ, p, N, Y, e
ð
Þ, private-key ðp, N, ϕ N
ð Þ, X, dÞ cannot
be recovered. To recover X from given Y, adversary has to solve discrete log-
arithm problem and to recover d from e requires to solve integer factorization
problem.
2. Given a valid signature triple r, s, t
ð
Þ, private-key ðp, N, ϕ N
ð Þ, X, dÞ cannot be
recovered. To recover d from Eq. (13) needs to solve integer factorization either
by given large prime p or by given large modulus N and also to recover X from
Eq. (11) requires the value of k from Eq. (9) which is a discrete logarithm
problem.
3. Without knowing the signer’s private-key ðp, N, ϕ N
ð Þ, X, dÞ, adversary cannot
forge valid signature. Let adversary selects a random integer k and computes
r and also computes k = temod N, where t is randomly taken from Z*
N. Then, to
select the combination of s and M from K to satisfy Eq. (11) is computationally
infeasible, where X is unknown to adversary. Also, computation of t from
s requires the knowledge of private-key d.
4. The proposed signature scheme is secure from attacks described in Sect. 2.2 for
Harn signature scheme with the assumption that adversary is able to solve
discrete logarithm problem because s in proposed scheme is not equal to p′q′.
Second, we use H = M, r
ð
Þ in proposed scheme which prevents from these
attacks, also remarked by N.-Y. Lee and T. Hawang in paper [9]. Assume
adversary solves the value of X by discrete logarithm problem and computes
r, s for message M then to compute t requires to solve integer factorization
problem and our assumption adversary cannot solve both problems together. If
adversary computes k = temod N, where t is randomly chosen from Z*
N, and also
chosen k, r, s, H, where r, M and H such that ðr = gkmod N, M = MGF sð Þ⊕t
and H = h M, r
ð
Þ, and then computed s from Eq. (11) should be equal to chosen
s which is computationally infeasible. Also, to satisfy the value of k and r for
relation h M, r
ð
Þ = s −X
ð
Þk −1, where s, M is chosen correctly computationally
infeasible because h(.) is collision-free one-way hash function.
An Extension to Modiﬁed Harn Digital Signature Scheme …
191

8
Performance and Advantage Over Modiﬁed Harn
Digital Signature Scheme
In this section, we show that proposed digital signature scheme gives one extra
feature of message recovery as compared to the modiﬁed Horn [9] digital signature
scheme, and we do not need to use a message redundancy function because pro-
posed scheme already contains the feature of veriﬁcation. Many researchers worked
in the direction of redundancy function and proved that security of redundancy is
susceptible (see Refs. [24, 25]).
The proposed scheme is equally efﬁcient as modiﬁed Horn digital signature
scheme in space and time both. The proposed scheme is better than modiﬁed Harn
digital signature scheme with the additional feature of message recovery and, due to
this feature, provides extension for some applications: identity-based public-keys
without restrictions in trust and a one-pass key exchange protocol with mutual
authentication.
9
Conclusion
In this paper, we proposed a digital signature scheme based on two dissimilar
cryptographic assumptions: discrete logarithm and integer factorization problem
with the additional feature of message recovery (means a recovered message can be
veriﬁed that signature has been altered or not without use of message redundancy
function). Also, the proposed scheme provides the extension for some applications:
identity-based public-keys without restrictions in trust (additionally, authenticity of
the certiﬁcate can directly be veriﬁed) and a one-pass key exchange protocol with
mutual authentication.
The proposed scheme is equally efﬁcient as modiﬁed Harn [9] digital signature
scheme in time and space both. Also, we analyzed for the security of proposed
scheme that the attacks associated with Harn [8] digital signature scheme with the
assumption that adversary can solve a discrete logarithm problem and can forge a
signature with some probability and will not work.
References
1. Difﬁe, W., Hellman, M. E.: New directions in cryptography. Information Theory, IEEE
Transactions on, vol. 22, no. 6, pp. 644–654 (1976).
2. Rivest, R. L., Shamir, A., Adleman, L.: A method for obtaining digital signatures and
public-key cryptosystems. Communications of the ACM, vol. 21, no. 2, pp. 120–126 (1978).
3. ElGamal, T.: A public key cryptosystem and a signature scheme based on discrete logarithms.
In Advances in Cryptology. Springer, pp. 10–18 (1985).
192
S.K. Tripathi and B. Gupta

4. Schnorr, C. P.: Efﬁcient signature generation by smart cards. Journal of cryptology, vol. 4, no.
3, pp. 161–174 (1991).
5. Fips pub xx, (1993) february 1.: Digital Signature Standard.
6. McCurley, K. S.: A key distribution system equivalent to factoring. Journal of cryptology,
vol. 1, no. 2, pp. 95–105 (1988).
7. Brickell, E. F., McCurley, K. S.: An interactive identiﬁcation scheme based on discrete
logarithms and factoring. Journal of Cryptology, vol. 5, no. 1, pp. 29–39 (1992).
8. Harn, L.: Public-key cryptosystem design based on factoring and discrete logarithms. IEE
Proceedings-Computers and Digital Techniques, vol. 141, no. 3, pp. 193–195 (1994).
9. Lee, N. Y., Hwang, T.: Modiﬁed harn signature scheme based on factorising and discrete
logarithms. IEE Proceedings-Computers and Digital Techniques, vol. 143, no. 3, pp. 196–
198 (1996).
10. Yen, S. M., Laih, C. S.: New digital signature scheme based on discrete logarithm. Electronics
Letters, vol. 29, no. 12, pp. 1120–1121 (1993).
11. Boyd, C.: Comment on new digital signature scheme based on discrete logarithms.
Electronics Letters, vol. 30, no. 6, pp. 480–481 (1994).
12. Nyberg, K.: Comments on new digital signature scheme based on discrete logarithms.
Electronics Letters, vol. 30, no. 6, pp. 481 (1994).
13. Nyberg, K., Rueppel, R. A.: A new signature scheme based on the dsa giving message
recovery. In Proceedings of the 1st ACM conference on Computer and communications
security. ACM, pp. 58–61 (1993).
14. Nyberg, K., Rueppel, R. A.: Message recovery for signature schemes based on the discrete
logarithm problem. Workshop on the Theory and Application of Cryptographic Techniques.
Springer, pp. 182–193 (1994).
15. Lin, C. C., Laih, C. S.: Cryptanalysis of nyberg-rueppel’s message recovery scheme.
Communications Letters, IEEE, vol. 4, no. 7, pp. 231–232 (2000).
16. Nyberg, K., Rueppel, R. A.: Message recovery for signature schemes based on the discrete
logarithm problem. Designs, Codes and Cryptography, vol. 7, no. 1–2, pp. 61–81 (1996).
17. Abe, M., Okamoto, T.: A signature scheme with message recovery as secure as discrete
logarithm. In Advances in Cryptology-ASIACRYPT?99. Springer, pp. 378–389 (1999).
18. Tseng, Y. M., Jan, J. K., Chien, H. Y.: Digital signature with message recovery using
self-certiﬁed public keys and its variants. Applied Mathematics and Computation, vol. 136,
no. 2, pp. 203–214 (2003).
19. Peng, Y., Q., Xie, S. Y., Chen, Y. F., Deng, R., Peng, L. X.: A publicly veriﬁable
authenticated encryption scheme with message linkages. In Networking and Mobile
Computing. Springer, pp. 1271–1276 (2005).
20. Hwang, M. S., Chen, S. M., Liu, C. Y.: Digital signature with message recovery based on
factoring and discrete logarithm. IETE Journal of Research, pp. 1–9 (2015).
21. Bellare, M., Rogaway, P.: ‘The exact security of digital signatures-how to sign with rsa and
rabin. In Advances in Cryptology?Eurocrypt?96. Springer, pp. 399–416 (1996).
22. Piveteau, J. M.: New signature scheme with message recovery. Electronics Letters, vol. 29,
no. 25, pp. 2185 (1993).
23. Gunther, C. G.: Difﬁe-hellman and el gamal protocols with one single authentication key. In
Advances in Cryptology - Eurocrypt’ 89, Lecture notes in Computer Science, pp. 434 (1990).
24. Shieh, S. P., Lin, C. T., Yang, W. B., Sun, H. M: Digital multisignature schemes for
authenticating delegates in mobile code systems. IEEE Transactions on Vehicular Technol-
ogy, vol. 49, no. 4, pp. 1464–1473 (2000).
25. Chang, C. C., Chang, Y. F.: Signing a digital signature without using one-way hash functions
and message redundancy schemes. Communications Letters, IEEE, vol. 8, no. 8, pp. 485– 487
(2004).
An Extension to Modiﬁed Harn Digital Signature Scheme …
193

Security and Energy Analysis on Revised
Mutual Authentication Protocol
in WiMAX Networks
Jayashree Padmanabhan and Gunavathie Mariappan
Abstract With emerging wireless technologies, ensuring security is challenging
due to limited energy availability in the wireless devices. As WiMAX-based
wireless access has been one of the base technologies in IoT and next-generation
technologies, access and authentication, which are the major concerns for con-
sumers and buyers of these wireless technologies, need to be addressed. In this
paper, energy-efﬁcient generalized digital certiﬁcate (GDC)-based mutual authen-
tication protocol is proposed. The efﬁciency of the proposed work in defending
attacks such as man-in-the-middle attack and rogue base station problem is ana-
lyzed. The analysis on energy efﬁciency of the proposed work is found to be quite
appealing, compared to existing mutual authentication procedures in WiMAX.
Keywords Mutual authentication ⋅Digital signature ⋅Energy efﬁciency
WiMAX networks ⋅WiMAX security
1
Introduction
WiMAX (Worldwide Interoperability for Microwave Access) has become a
potential next-generation broadband wireless access technology based on IEEE
802.16 standards. Security solutions proposed in the literature for WiMAX network
provide security architectures that employ X.509 certiﬁcates, along with various
security associations, encryption and encapsulation protocols [1–3]. The Privacy
Key Management (PKM) authentication protocol widely adopted in WiMAX has
two versions. PKM-v1 makes use of the X.509 certiﬁcates to get one-way
J. Padmanabhan (✉) ⋅G. Mariappan
Department of Computer Technology, MIT Campus of Anna University,
Chennai 600044, India
e-mail: pjshree12@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_18
195

authentication using message transfer of six steps [4, 5] such as authentication
information, authorization request, authorization reply, authorization acknowl-
edgement, key request, and key reply. As PKM-v1 does not discuss base station
(BS) certiﬁcation, man-in-the-middle attacks may happen. To overcome this limi-
tation of PKM-v1, PKM-v2 was proposed wherein BS certiﬁcation is provided to
support mutual authentication. In both these versions, a set of ﬁve crucial keys such
as Authorization (AK), Key Encryption Key (KEK), Downlink Hash-based Mes-
sage Authentication Code (DHMAC), Uplink Hash-based Message Authentication
Code (UHMAC), and Transport Encryption Key (TEK) are generated for successful
authentication and key exchange.
Most of the authentication protocols including basic PKM, PKMv2, EAP based
involved the transmission of public key certiﬁcates to ensure the authorization of
MS. Reduction in message overheads and energy is possible if passing
certiﬁcates/keys is reduced between MS and BS. This is the initial motivation for
studying energy efﬁciency of authentication protocols. Moreover, energy in mobile
devices can be saved if the number of computation operations and key management
messages transmitted in the protocol is reduced. With this view, it is proposed to
enhance the GDC-based protocol to address some security ﬂaws too without
increasing message overheads.
Serenest Sidharth [6] revised the PKM protocol with time stamp and nonce to
overcome the man-in-the-middle attack. But this revised PKM protocol has syn-
chronization overhead. A proxy BS-based authentication protocol was proposed by
Fuden Tshering [7]. This approach suffers from the overhead of adding the proxy
station and more number of message transfers. An improved secure network
authentication protocol (ISNAP) was proposed by Hashmi [8]. This approach was
efﬁcient in handling the attacks such as DoS attack and man-in-the-middle attack.
PKM-v2 adopts two different authentication protocols, RSA-based protocol
which uses X.509 certiﬁcates and EAP which depends on user credentials. In
protocols based on X.509 certiﬁcate, certiﬁcate transfer to the veriﬁer is needed to
prove the identity. Jin [9] describes mutual authentication through improved X.509
certiﬁcate by using ECC algorithm instead of RSA but did not address rogue access
node attack. In [10], an authentication server is used for mutual authentication in
relay networks where a security key is delivered to trusted base station that dis-
tributes it in the network for avoiding rogue stations.
The authors of [11] used wavelet analysis on ﬁngerprints to defend against rogue
base stations. Existing authentication protocols as well as the emerging wireless
networks are vulnerable to replay, denial-of-service (DoS), and man-in-the-middle
(MITM) attacks [12]. Discussion on RSA, EAP, and HMAC authentication
schemes and modiﬁed security management schemes in terms of authorization key
exchange and management is detailed in [13].
Lein Harn [14] proposed a concept called GDC that eliminates the need for
deriving the public and private key pair for user authentication making key
196
J. Padmanabhan and G. Mariappan

management easier, and further, no certiﬁcate transfer is involved to get authenti-
cated. Instead, the entities have to respond for the challenge with the secret sig-
nature component. This GDC concept is adopted with modiﬁed challenge and
response protocol in the proposed work for providing better mutual authentication
between the BS and mobile station (MS) and to provide improved security in
WiMAX networks compared to PKM. The modiﬁed key generation process paved
way for secure key exchange in the WiMAX networks.
1.1
ElGamal Signature
A digital signature is an electronic signature used to authenticate the identity of the
sender of the message. Security of ElGamal digital signature [15] is based on the
difﬁculty in computing discrete logarithms. In this scheme, message is digitally
signed using components r, s where
r = gk mod p
ð1Þ
s = k −1 m −xr
ð
Þ mod p −1
ð2Þ
The signature is veriﬁed with
gm = yrrs mod p
ð3Þ
where p is a large prime, x is the private key, k is a random number, y is the public
key, g is the generator in the order of p −1, m is the message digest of the message
m′, r is random component used for generating s, and the secret signature com-
ponent and the pair (r, s) form the signature on message m′. To avoid forging of the
signature, it has been suggested to use different values of r generated for different
entities, by using different values of k in the signing process and it is used in the
proposed mutual authentication process.
1.2
Challenge and Response Method
In the proposed mutual authentication process, the legitimate base station that is
aware of the secret signature component will create a challenge and the response
can be any random number authenticated by the Message Authentication Code
(MAC) using the secret key generated and this challenge and response method
seems to offer rigid authentication as detailed in next section.
Security and Energy Analysis on Revised Mutual …
197

2
Proposed Lightweight Mutual Authentication Protocol
The proposed authentication scheme is based on GDC proposed by Harn et al. In
that protocol, the shared key generation is little bulky so as to increase the security
strength. In the proposed variant, the key generation is simpliﬁed without com-
promising the security and the security proof is provided at a later section.
The proposed mutual authentication algorithm involves the following activities:
(i) GDC certiﬁcate procurement, (ii) mutual authentication and key establishment
on initial network entry, (iii) key updating process, and (iv) re-authentication.
The ﬁrst three steps of getting the GDC certiﬁcate, mutual authentication, key
establishment, and key updating process, are mandatory for entities that wish to join
the network for the ﬁrst time. The entities, on subsequent entry to the network, can
use the updated key that was obtained during the last successful authentication and
should undergo only the simpliﬁed re-authentication process.
2.1
Getting the GDC Certiﬁcate
When an entity initially joins the network, it must get the GDC [14] by submitting
its identity to the certiﬁcate authority (CA). The CA will send the digital ElGamal
signature as (r, s) where r is a public component and s is a secret component.
2.2
Mutual Authentication and Key Establishment on Initial
Network Entry
The mutual authentication process is detailed in the Protocol 1.
Protocol 1
Step 1: MS: Passes its identity information m′ and signature components (ra,
SA) to BS where
SA = ra
ð Þsa
ð4Þ
Step 2: BS: Receives the identity and signature components, and veriﬁes the
signature of MS using the ElGamal signature veriﬁcation stated in Eq. (3).
198
J. Padmanabhan and G. Mariappan

Step 2.1: If the signature is successfully veriﬁed, then it computes the chal-
lenge as in Eq. (5)
challenge = SA rb
ð
Þsb
ð5Þ
and sends its identity information mb′ and signature components (rb, SB),
along with the challenge to the corresponding MS.
Step 2.2: Else authentication fails and communication is interrupted.
Step 3: MS: Veriﬁes the signature of BS using Eq. (3).
Step 3.1: If the signature is successfully veriﬁed, then it generates key K for
further communication as follows:
K = 1 ̸ sa log challenge ̸ SB
ð
Þ
ð
Þ
ð6Þ
Step 3.2: Generates a random number as response and derives Message
Authentication Code (MAC) for response using the key K and sends both
to BS.
Step 4: BS: Computes the key K′ as follows
K′ = sb
ð Þ log ra
ð
Þ
ð7Þ
and generates the MAC for the response received with the key K′.
Step 4.1: If the generated MAC is found to be same as received MAC, then
both entities are mutually authenticated and key establishment process is
completed with success.
Step 4.2: Else key establishment process is terminated with failure.
The generated keys K and K′ are shared secret keys for further communication
in that session. The correctness of the key generation algorithm is shown in section
C. After every successful authentication, both MS and BS are supposed to update
the key that they have generated for future communication sessions.
2.3
Proof of Correctness of the Protocol
Inference 1: The keys generated by both legitimate BS and legitimate MS in the
proposed mutual authentication protocol are equal.
Security and Energy Analysis on Revised Mutual …
199

Correctness Proof: Let the key at MS be K.
K = 1 ̸ sa log challenge ̸ SB
ð
Þ
ð
Þ
= 1 ̸ sa log SA rb
ð
Þ sb ̸ SB
ð
Þ
ð
Þ
= 1 ̸ sa log
ra
ð Þ sa rb
ð
Þ sb ̸ SB




= 1 ̸ sa log
ra
ð Þ sa rb
ð
Þ sb


̸ rb
ð Þ sb


= 1 ̸ sa log
ra
ð Þsa
ð
Þ
sb


= sb log ra
ð
Þ
= K′ = key at BS
Since the key generations at either end involve the secret components of the
respective entities, only benign entities can generate the shared secret key and hence
proved.
2.4
Key Updating Process
After the secret keys are successfully generated, the keys are subjected to updating
process using lightweight functions that are publicly accessible to both stations. MS
stores BS information and the updated key and BS stores MS information and the
updated key which are used during re-authentication process as MS, on subsequent
entries into the network, need not undergo the complete authentication process as
detailed above.
3
Re-Authentication for Subsequent Entries
The re-authentication process is detailed in the Protocol 2.
Protocol 2
Step 1: MS: Computes the message digest of the updated key of BS (if
communicated earlier) and sends its identity information and message digest
generated to BS.
Step 2: BS: Receives the identity and message digest information and com-
putes the message digest of the updated key of MS.
Step 2.1: If the generated key digest is found to be same as received key
digest, then both entities are re-authenticated and key updating process is
completed with success.
Step 2.2: Else re-authentication fails and communication is interrupted.
200
J. Padmanabhan and G. Mariappan

The event ﬂowcharts for mutual authentication protocol and re-authentication
protocol are depicted in Fig. 1a, b, respectively. In [16], Extensible Authentication
Protocol (EAP)-based re-authentication is adopted, but EAP is time-consuming
mechanism and is not suitable for energy-constraint environments and another
variant of EAP-based re-authentication is discussed in [18]. In mobile wireless
environment, a low-cost re-authentication protocol is used [17], where key reuse is
used as re-authentication parameter. Though EAP is secured, it has impediments in
mobile environment and time-sensitive environment where authentication overhead
causes performance degradation, and our previous work on re-authentication [19]
serves a solution for that issue.
4
Security Analysis and Discussion
In order to be authenticated successfully, both MS and BS need to compute and
send a valid signature(r, S). Only the certiﬁcate owner who knows the secret
exponent of S can compute the valid shared secret key (K) and the valid MAC. In
this section, security strength of the proposed protocol against more classic attacks
such as man-in-the-middle attack and rogue base station problem is analyzed.
4.1
Rogue Base Station Problem
The rogue base station problem in WiMAX is handled by the proposed mutual
authentication. BS is authenticated if its signature is veriﬁed by MS using Eq. (3).
Even if the attacker generates a valid signature pair (rb, SB), only the legitimate BS
Fig. 1 a Mutual authentication protocol. b Re-authentication protocol
Security and Energy Analysis on Revised Mutual …
201

who knows the secret signature component sb can compute the valid shared key
since solving for sb from SB is computationally infeasible because of the discrete
logarithm problem.
4.2
Man-in-the-middle Attack
Man-in-the-middle attack can succeed only when the attacker can impersonate each
endpoint to the satisfaction of the other. The proposed algorithm is proved to
overcome man-in-the-middle attack.
Inference 2 Adversary in the man-in-the-middle attack trying to establish the key
agreement with legitimate MS is not succeeded by the proposed mutual authenti-
cation process.
Correctness Proof: Let sb′ be the BS secret component assumed by the
adversary. Let the challenge generated by adversary be challenge′.
challenge0 =
SA rb
ð
Þ sb0
Let K′′ be the key generated at MS.
K′′ = 1 ̸ sa log challenge0 ̸ SB
ð
Þ
ð
Þ = 1 ̸ sa log
SA rb
ð
Þ sb0
ð
Þ ̸ SB
ð
Þ
ð
Þ
= 1 ̸ sa log
ra
ð Þsa rb
ð
Þ
sb0


̸ SB




= 1 ̸ sa log
ra
ð Þsa rb
ð
Þ
sb0


̸ rb
ð Þsb




≠sb0 log ra
ð
Þ
which is not matching with the key generated by the adversary BS.
Even if the component SB is forged with SB′ corresponding to sb′ chosen by the
adversary, the pair (rb, SB′) cannot be veriﬁed using the ElGamal signature veri-
ﬁcation and hence the proof.
Inference 3 Adversary in the man-in-the-middle attack trying to establish the key
with legitimate BS is avoided by the proposed mutual authentication process.
Correctness Proof: Let sa′ be the MS secret exponent assumed by the adversary
and let K′′′ be the key generated by the adversary.
K′′′ = 1 ̸ sa0
log challenge ̸ SB
ð
Þ
ð
Þ = 1 ̸ sa0
log
SA rb
ð
Þ sb
ð
Þ ̸ SB
ð
Þ
ð
Þ
= 1 ̸ sa0 log
ra
ð Þ sa rb
ð
Þ sb


̸ sB




= 1 ̸ sa0 log
ra
ð Þsa rb
ð
Þ
sb


̸ rb
ð Þsb




= sb sa ̸ sa0 log ra
ð
Þ ≠sb log ra
ð
Þ
which is not matching with the key generated by the BS and hence the proof.
202
J. Padmanabhan and G. Mariappan

Energy Efﬁciency Analysis and Discussion
In PKM-based authentications, nearly ﬁve different keys AK, KEK, DHMAC,
UHMAC, and TEK are used. In GDC-based authentication, a single key generated
is used for entire communication. In the proposed GDC-based authentication
mechanism, the computation involved in the key generation is simpliﬁed using
simple arithmetic and logarithmic operators and the strength of key lies in the
operands involved which are never shared between any entities and on the security
of ElGamal signature generation. Since computational complexity is proportional to
total number of keys generated for various users, in the proposed system, the total
number of keys and hence the total computational complexity are reduced with
decrease in number of keys.
Moreover, the number of messages transmitted between entities for authenti-
cation and key establishment is reduced to three in the proposed revision, compared
to more number of control messages for successful authentication and key exchange
process in PKM-/EAP-based authentications. These factors lead to the reduction in
the energy consumed by the mobile devices and the revised protocol is energy
efﬁcient.
Comparative analysis of the number of keys used in PKM-v2-based mutual
authentication algorithm and the proposed mutual authentication protocol is shown
in Fig. 2a. Analysis on the number of control messages used for successful mutual
authentication and key exchange mechanisms on initial network entry is made and
the results are depicted in Fig. 2b for PKM-v2-based protocol and the proposed
mutual authentication protocol.
It can be visualized that the proposed protocol consumes less energy for
mutually authenticating base stations and mobile stations compared to the standard
PKMv-2 protocol for WiMAX networks as the messages exchanged involve lesser
number of user credentials during authentication process.
Fig. 2 a Analysis on number of keys. b Analysis on number of messages exchanged
Security and Energy Analysis on Revised Mutual …
203

5
Conclusion
A lightweight mutual authentication protocol for WiMAX network, based on GDC,
is proposed. In the proposed decentralized authentication scheme with an initial
signature component received from a trusted authority, a single key is generated
before negotiation and this key will be used to encrypt the messages required for
negotiation. Also, it is proved that the proposed mutual authentication process
overcomes man-in-the-middle attack and rogue base station threats with lesser
number of control messages. The complexity analysis on the growth rate of mes-
sages and keys with increasing users in the network for mutual authentication
process provides a clear indication that the proposed protocol reduces the energy
consumption overheads and seems to be a promising approach.
References
1. Lang Wei-min, Wu Run-sheng jian-qiu, “A simple key management scheme based on
WiMAX”, in Proc of International Symposium on Computer Science and Computational
Technology, December 2008, pp. 3–6.
2. Fan Yang, Huaibei Zhou, Lan Zhang & Jin Feng, “An improved security scheme in WMAN
based on IEEE standard 802.16”, In proc. of Wireless Communications, Networking and
Mobile Computing, September 2005, pp. 1191–1194.
3. EvrenEren. “WiMAX security architecture: analysis and assessment,” in Proc. Of 4th
International Workshop on Intelligent data Acquisition and Advanced Computing Systems:
Technology and Applications, September 2007, pp. 673–677.
4. IEEE standard 802.16-2004. Air Interface for Fixed Broadband wireless Access Systems.
IEEE Press, 2004.
5. Mahmoud Nasreldin, Asian H, El-Hennawy M & El-Hennawy, “WiMAX Security”, In Proc
of 22nd International conferences on advanced Information Networking and Applications,
pp. 1335–1340, March 2008.
6. SreejeshSidharth & M.P. Sebastian, “A revised secure authentication protocol for ieee 802.16
(e)”, in Proc of International Conference on Advances in Computer Engineering, pp. 34–38,
June 2010.
7. Fuden Tshering & Anjali Sardana, A proxy base station based authentication Proc of
International Conference on Recent Trends in Information Technology, MIT, Anna
University, Chennai, June 2011, pp. 578–582.
8. Hashmi R.M, Siddiqui A.M, Jabeen M, Shehzad K., Zubair A and Alimgeer K.S, “Improved
secure network authentication protocol for ieee 802.16”, in Proc of International Conference
on Information and Communication Technologies, August 2009, pp. 101–105.
9. X. Jin et al., “An improved mutual authentication scheme in multihop WiMAX network,” in
Proc. of International Conference on Computer and Electrical Engineering, Dec. 2008,
pp. 296–299.
10. Jie Huang, Chin-Tser Huang, “Secure Mutual Authentication Protocols for Mobile Multi-hop
Relay WiMAX Networks against Rogue Base/Relay Stations” in Proc of International
Conference on Communications, 2011.
11. AlaaedineChouchane, Slim Rekhis, and Noureddine, “Defending against rogue base station
attacks using wavelet based ﬁnger printing”, Technical report, Networks and Security
Research Lab, University of Carthage. Tunisia, 2009.
204
J. Padmanabhan and G. Mariappan

12. Vinod K.J., Vrijendra S., “Mobile WiMAX Network Security Threats and Solutions: A
Survey,” in Proc. IEEE International Conference on Computer and Communication
Technology, 2014, pp. 135–140.
13. Lang Wei-min, Zhong Jing-li, Li Jian-jun, Qi Xiang-yu “Research on authentication scheme
of WiMAX”, in Proc of 4th International Conference on Wireless Communications,
networking and Mobile Computing, 2008. WiCOM ‘08.
14. LeinHarn&JianRen,
“Generalized
digital
certiﬁcate
for
user authentication
and key
establishment for secure communications”, IEEE Trans. Wireless Communications., 2011,
10(7), pp. 2372–2379.
15. Taher Elgamal, “A public key cryptosystem and a signature scheme based on discrete
logarithms”, IEEE Trans. Information theory., 1985, 31(4), pp. 469–472.
16. RafalChrabaszcz,
PiotrPacyna, “Fast re-authentication
of mobile
devices with EAP
Re-authentication Protocol (ERP),”. In Proc. IEEE XVth International Conference on
Telecommunications Network Strategy and Planning Symposium (NETWORKS), 2012,
pp. 1–6.
17. Lai Chengzhe, Li Hui, Zhang Yueyu, Cao Jin, “Simple and Low-Cost Re-Authentication
Protocol for HeNB,” China Communication, 10(1):, 2013, pp. 105–115.
18. Chrabaszcz
R.,
Pacyna
P.,
“Fast
re-authentication
of
mobile
devices
with
EAP
Re-authentication Protocol (ERP).” in Proc. IEEE XVth International Conference on
Telecommunications Network Strategy and Planning, 2012, pp. 1–6.
19. Raja G, Baskaran S.B, Ghosal D, Padmanabhan J. Reduced Overhead Frequent User
Authentication in EAP-Dependent Broadband Wireless Networks. Mobile Networks and
Applications. 2016, pp. 1–16.
Security and Energy Analysis on Revised Mutual …
205

Personal Veriﬁcation Using Off-line
Signature with Tree-based Features
Arun Kumar Shukla and Suvendu Kanungo
Abstract The signature veriﬁcation is one of the major and general purposes
frequently used approach for person’s veriﬁcation among all the other existing and
known biometric-based veriﬁcation methods. This brought the attention to the
development of an automatic signature veriﬁcation system. In this paper, an off-line
signature veriﬁcation and recognition system based on tree and grid by adopting a
feature extraction novel approach such as pixels in tree, eccentricity, and center was
used. The problem of using a trained dataset in order to perform the veriﬁcation was
overcome by using only one genuine and test signature at the run time. Decision of
the result which was based on authenticity and governed by the maximum correct
feature favor, acceptance, and rejection is based on its majority. The usefulness of
the proposed approach was acknowledged by the use of experimental results.
Keywords Off-line signature veriﬁcation ⋅Tree-based feature
Grid-based feature
1
Introduction
The most extensively used biometric authentication [1] is signature veriﬁcation. It is
an essential research area where biometric veriﬁcation [2] is used in applications
that focus on automatic identity veriﬁcation. Special software for signature veriﬁ-
cation is needed by applications such as legal and banking. Security-wise, the
biometric authentication is by far better than ancient authentication methods such as
passwords because every individual has unique and permanent biometric charac-
teristics. There are two types of biometrics: behavioral biometric [3] which consists
of handwriting, speech, etc. and physiological biometric [4] based on iris pattern,
ﬁngerprints, and so on. The system has to merge the input signature with the
original signature of the owner to analyze its authenticity. The signature
A.K. Shukla (✉) ⋅S. Kanungo
Birla Institute of Technology, Mesra, Allahabad, India
e-mail: aks.jit@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_19
207

veriﬁcation’s main purpose is to examine whether a given signature is authentic by
classifying the input signature as forge or genuine. This is done by comparing the
signature image against the database signature by using various techniques. Any
attempt by illegal person to falsify signature of another person in an effort to be
authenticated is called forgery.
1.1
Need for Signature Veriﬁcation
The growing numbers of ﬁnancial and business transactions are so critical and
demanding in terms of security. Nowadays, these types of transactions are being
secured through the signature veriﬁcation technique. The automatic method for
verifying signature becomes necessary as there is need to develop a signature
veriﬁcation mechanism. Two approaches to signature veriﬁcation exist based on
acquisition of data: These are online and off-line approaches.
1.2
Veriﬁcation Approaches
1.2.1
Online Signature Veriﬁcation
In the online signature veriﬁcation approach, the signature is produced by recording
the motion of the stylus including the place, the velocity, acceleration, and the
pressure of the pen as function of time. It is fundamental to capture all the data
related to signature. These are common and active characteristics which are ade-
quately stable and speciﬁc to every individual. It is challenging to intimate the
signature due to its dynamic characteristics.
1.2.2
Off-line Signature Veriﬁcation
The 2-D signature image is considered as the off-line data. The original signature is
acquired ﬁrst by having a complete signature on a paper. This signature is then
scanned or captured with a camera for use in off-line signature veriﬁcation. The
absence of stable dynamic characteristics makes off-line signature processing a
complex task. The difﬁculty comes as a result of highly sophisticated and unusual
writing styles involved in handwritten signatures, thereby making it hard to seg-
ment the signature strokes. Other factor that may affect the signature’s nature
obtained variety of writing pen, illness, age, geographic locations, non-repetitive
nature of signature variation, and perhaps to some extent the person’s emotional
state, accentuates the problem. These factors merged together caused intrapersonal
variations that make the veriﬁcation process very complex.
208
A.K. Shukla and S. Kanungo

Kisku, D. R. et al. [5] proposed a system that uses global and local features and
thus presented a score-level fusion of multiple matchers for off-line signature
identiﬁcation. The classiﬁers they used are Mahalanobis distances, Euclidean and
Gaussian empirical rule and by fusing these classiﬁers along with SVM the
matching score was obtained. Madasu et al. [6] use a new methodology for off-line
signature veriﬁcation that was based on artiﬁcial neural network which differenti-
ates between original and forged signatures. Their scheme applies preprocessing on
the signature, feature point extraction,
neural network training and then using
trained network authenticity of the signature was obtained. Kiani, V. et al. [7]
proposed a signature veriﬁcation method using the local feature extractor named
Radon transform and classiﬁer named support vector machine. They used Radon
transform locally as against using it globally in feature extraction and line segment
detection as their main idea. Jena, D. et al. [8] proposed another signature veriﬁ-
cation scheme that compares already existing trained feature points with 60 feature
points from geometric center of carefully chosen signatures. Statistical parameters
such as mean and variance are used for the feature points’ classiﬁcation. They also
claimed that their system discriminates among the original and forged signatures by
taking care of skilled, simple, and random forgeries. Mishra, P.K. and Sahoo, M.R
[9] conducted a research on signature veriﬁcation based on signature’s size and
angle invariant. This is achieved by scaling and rotational manipulations on the
target image while the shape remains similar in all respect. Without considering the
image’s size, the number of crests, toughs, and curves remains intact. In another
development, Chaurasia, P. [10] proposed a method and system for improving the
accuracy of off-line signature veriﬁcation techniques. This is done by analyzing
the high-pressure regions of an image of the signature. Local and global features in
the high-pressure areas may be analyzed for increased veriﬁcation accuracy. The
high-pressure areas of the image were determined, and the global features are mined
from the high-pressure areas of the image. For authenticity of the signature pattern
recognition, methods are applied to the global features. In another development in
the ﬁeld off-line signature veriﬁcation [11–13, 7], the proposed state-of-art
approaches validated the authenticity of signatures.
2
Proposed Model
Based on the exploration of the above systems, the off-line signature veriﬁcation
process includes these steps:
• Signature Acquisition
• Signature Preprocessing
• Feature Extraction
• Signature Veriﬁcation
Personal Veriﬁcation Using Off-line Signature …
209

2.1
Signature Acquisition
An A4 paper is used to collect signatures using a scanner with 300 dpi. The image
acquired was saved in Portable Network Graphics (PNG) format. The sample for
each of the writers’ signature and the proposed technique have been tested based on
information in the database populated by samples.
2.2
Signature Preprocessing
It is usual that the acquired signature may occasionally contain some noise, i.e.,
extra pen dots other than the signature. Preprocessing of the acquired signature is
necessary to verify it properly. This can be achieved by using ﬁlters. Preprocessing
contains some more operations such as binarization, resizing, rotation normaliza-
tion, and thinning.
Resizing
The ﬁrst step of preprocessing is to resize the developed signature to a standard
size (100 × 200) using the resize algorithm.
Binarization
It is the act of obtaining the black and white form of the resized (RGB) signature.
Thinning
Pen used for acquiring signatures that are stored in database may be of any type.
Thinning is a morphological operation applied to binary images to get a one pixel
run of the signature or skeleton of the signature is required so that different ranges
of thickness of the pen tip do not affect the results.
2.3
Feature Extraction
Feature Extraction for Proposed Approach:
There are three essential features of tree that are used to decide the authenticity
of signature in the proposed approach. These features are listed below:
1- Pixel counting of tree
2- Angle between the slope of local and global center of gravity
3- Eccentricity and center of a tree
Subsequently, by computing these three features, we use majority rule. Signature
is genuine if two or more features of the signature are true, then the output will be
yes otherwise no.
210
A.K. Shukla and S. Kanungo

Pixel counting of tree
Step 1 Divide a matrix of test signature into grid. After preprocessing, a sig-
nature of size 100 × 200 (pixels) is produced. Then, create a grid of m x
n where m < n, m ≪100, and n ≪200, over a preprocessed signa-
ture. In this research, values of m = 10 and n = 20 are used. Thus, a
signature image is divided into 200 square cells where each cell is
having 100 pixels.
Step 2 Calculate the center of gravity of each local cell of grid.
1. CGx
This is the center of gravity with respect to x direction. It is deﬁned as
the mean of x positions of the black pixels of the signature image in
cell given by
CGxlocal = ∑n
i = 1 xi
n
ð1Þ
where n = number of black pixels in the signature image of cell and
xi is the value of x-coordinate of ith black pixel.
2. CGy
This is the center of gravity with respect to y direction. It is deﬁned as
the mean of y positions of the black pixels of the signature image in
cell. It can be calculated as
CGylocal = ∑n
i = 1 yi
n
ð2Þ
where n = number of black pixels in the signature image of a cell
and yi is the value of y-coordinate of ith black pixel.
Step 3 Connect all pixels on coordinate center of gravity (CGx, CGy) of each
cell in zigzag fashion to create a tree on signature.
Step 4 Count the pixels of created tree. Let the number of pixels be Ntest.
Step 5 Same procedure is repeated with genuine signature and let number of
pixels on tree remain Ngen.
Step 6 Assume a threshold th according to secrecy of application.
Signature =
Genuine If Ntest = Ngen ∓th
Fake Otherwise


ð3Þ
Angle between the slope of local and global center of gravity
Step 1 Calculate the global center of gravity for signature by using
Personal Veriﬁcation Using Off-line Signature …
211

CGxglobal = ∑n
i = 1 xi
n
CGyglobal = ∑n
i = 1 yi
n
Step 2 Make a horizontal line passing through the pixel on (CGxglobal,
CGyglobal).
Step 3 Each cell has its own local center of gravity. Now connect the
(CGxglobal, CGyglobal) to all (CGxlocal, CGylocal).
Step 4 Let angle between the (CGxglobal, CGyglobal) to all (CGxlocal1,
CGylocal1) be θ1 and angle between the (CGxglobal, CGyglobal) to all
(CGxlocal2, CGylocal2) be θ2, and so on.
Step 5 Same procedure is repeated with genuine signature. Let for that, angles
are θ1gen, θ2gen, θ3gen… respectively.
Step 6 Assume a threshold θth according to secrecy of application.
Signature =
Genuine If θn = θngen ∓θth
Fake Otherwise


ð4Þ
Eccentricity and center of a tree
Two basic features of a tree are eccentricity and center.
Eccentricity: Let G be a graph and v be a vertex of the graph G. The eccentricity
of the vertex v is the maximum distance from v to any vertex. That is, e(v) = max
{d(v, w):w in V(G)}.
Center: The center of the graph G is the set of vertices of eccentricity equal to
the radius. Hence, center (G) = {v in V(G):e(v) = radius(G)}.
Step 1 Divide the each cell containing signature into subgrids.
Step 2 Calculate the local center of gravity for each subgrid of cell.
Step 3 Form tree connecting with each local center of gravity.
Step 4 Repeat the following steps for each cell containing signature.
Step 5 Calculate eccentricity for each vertex by using the following algorithm.
Algorithm: ECCENT (Tr) Input: A Tree Tr = (V, E, ɷ) rooted at r.
Output: The Eccentricity of r in Tr.
I: if r is a leaf then
return 0;
II: for each child s of r do
compute ECCENT(Ts) recursively;
III: return maxsɛchild(r) {ECENT(s) + ɷ(r, s)}.
Step 6 Calculate the center of subtree.
212
A.K. Shukla and S. Kanungo

Ctest = max ðEccentricity of vertexÞ
ð5Þ
Step 7 Same procedure is repeated with genuine signature and calculate Cgen.
Step 8
Signature =
Genuine If Ctest = Cgen ∈N8
Fake Otherwise


ð6Þ
where N8 are current eight neighbors of processing center.
2.4
Decision Making
We have calculated three features F1, F2, and F3. Let c be a counter initialized by 0.
Then,
c =
c + 1 If Fn = geneuine
c = c Otherwise


ð7Þ
Signature =
Genuine If c = c ≥2
Fake Otherwise


ð8Þ
Hence, by this veriﬁcation process, one can decide the authenticity of a
signature.
3
Experimental Results and Analysis
Experiments have been conducted on various datasets, and results show the efﬁ-
ciency of proposed approach. The two parameters named FAR and FRR are used to
measure the performance of any signature veriﬁcation method.
FAR (False Acceptance Rate): It is deﬁned as the percentage of falsely
accepted forgeries and is given by
FAR = ðNumber of forgeries accepted ̸ Number of forgeries testedÞ × 100
ð9Þ
FRR (False Rejection Rate): It is deﬁned as the percentage of genuine signa-
tures that are falsely rejected by the system and is given by
FRR = ðNumber of genuines rejected ̸ Number of genuines testedÞ × 100
ð10Þ
The goal of veriﬁcation system is to reduce FAR and FRR. FAR and FRR have
been calculated to evaluate the performance of the developed system. Different
values of threshold are needed to plot FAR versus FRR graph. Here, threshold is the
Personal Veriﬁcation Using Off-line Signature …
213

security level which can be set according to the target application. This graph,
sometimes called the equal error graph, is mostly used by researchers trying to
understand the performance of their veriﬁcation system. It displays the false accept
and false reject rates at all thresholds. Minimizing the crossover of the two plots is
generally the goal of the veriﬁcation system.
ROC Curve
The visual characterization of the trade-off between the FAR and the FRR is
termed as the ROC (receiver operating characteristic) plot. In FAR versus FRR plot,
the EER is the crossover point on a graph. The EER can be calculated from the
ROC curve, which plots FAR against FRR to determine a particular system’s
accuracy. Each of the corresponding FAR and FRR points is plotted to calculate the
ROC of a biometric system, and then the EER is obtained by extending a 45° line
from the point of origin (0, 0). The point where this 45° line crosses the ROC curve
gives the EER.
From Tables 1 and 2, you can see that the threshold increases, and FAR
decreases while FRR increases. For the database used here, FAR is 9.63% and FRR
is 8.46%. The FAR vs FRR graph gives the system’s percentage error. Equal error
rate (EER) is the point where FAR and FRR become equal, that is 9.04%, as shown
in Fig. 1. ROC curve for this database is shown in Fig. 2.
Table 1 Signature
veriﬁcation results of the
proposed approach for
threshold th
Threshold th
FAR (%)
FRR (%)
40
10.65
2.98
35
9.78
1.11
30
8.23
3.74
25
7.63
1.46
20
6.41
2.96
15
5.87
2.12
10
1.12
2.39
5
0
1.23
Table 2 Signature
veriﬁcation results of the
proposed approach for
threshold θth
Threshold θth
FAR (%)
FRR (%)
2
0
0
4
4.67
2.11
6
5.23
3.74
8
6.63
2.46
10
6.71
3.96
12
8.87
2.12
14
10.12
2.39
16
10.54
1.89
214
A.K. Shukla and S. Kanungo

4
Conclusions
The method of pixel and component orientation is used for the extraction of features
from an off-line signature. Resizing, binarizing, thinning, and rotation normaliza-
tion are used in signature preprocessing. Once the preprocessing of the signature is
done, it is divided into a uniform grid of size 10 × 20 cells where each cell
contains 100 pixels. The extraction of feature in terms of pixel arranged in rows and
column of a particular array and matrix corresponding to a grid is done in pixel
orientation, whereas the extraction of features such as center of gravity slope,
normalized sum of angles of all points of the signature content, center of gravity in
x direction, center of gravity in y direction, contour area, and aspect ratio is done in
the component orientation. The global veriﬁcation is done by pixel orientation
Fig. 1 FAR versus FRR
graph
Fig. 2 ROC curve
Personal Veriﬁcation Using Off-line Signature …
215

features, whereas the local veriﬁcation is done by component orientation features
for all the training and test images. The global veriﬁcation and the local veriﬁcation
are tested by applying AND logic test, and the test signature, based on the results, is
classiﬁed accordingly. In proposed approach, three types of feature are extracted
from signature based on tree and grid and decision is made on majority acceptance
basis. The developed technique deals with the skilled forgeries and gives better
results in terms of FAR and FRR than various existing veriﬁcation techniques.
References
1. Battista Biggio, Zahid Akhtar, Giorgio Fumera, Gian Luca Marcialis, and Fabio Roli, 2012,
“Security Evaluation of Biometric Authentication Systems Under Real Spooﬁng Attacks”,
IEEE Biometrics Compendium, IET Biometrics Volume: 1, Issue: 1.
2. Andrea Ceccarelli, Leonardo Montecchi, Francesco Brancati, Paolo Lollini, Angelo
Marguglio, and Andrea Bondavalli, 2015, “Continuous and Transparent User Identity
Veriﬁcation for Secure Internet Services”, IEEE Transactions On Dependable And Secure
Computing, Vol. 12, No. 3,
DOI 10.1109/TDSC.2013.2297709.
3. Karan Khare, Surbhi Rautji and Deepak Gaur, 2013, “Behavioural Biometrics and Cognitive
Security Authentication Comparison Study”, Advanced Computing: An International Journal
(ACIJ), Vol. 4, No. 6, pp. 15–24.
4. Israa M. Alsaadi, 2015, “Physiological Biometric Authentication Systems, Advantages,
Disadvantages And Future Development: A Review”, International Journal Of Scientiﬁc &
Technology Research Vol. 4, Issue 12, pp. 285–289.
5. Dakshina Ranjan Kisku, Phalguni Gupta and Jamuna Kanta Sing, IJSIA 2010, “Ofﬂine
Signature Identiﬁcation by Fusion of Multiple Classiﬁers using Statistical Learning Theory,”
Computer Vision and Pattern Recognition.
6. Madasu Hanmandlu, Mohd. Haﬁzuddin Mohd. Yusof and Vamsi Krishna Madasu, 2005,
“Ofﬂine Signature Veriﬁcation and Forgery Detection using Fuzzy Modeling,” The Journal of
the Pattern Recognition Society, Vol. 38, pp. 341–356.
7. Vahid Kiani, Reza Pourreza, Hamid Reza Pourezza, 2010, “Ofﬂine Signature Veriﬁcation
Using Local Radon Transform and Support Vector Machines,” International Journal of Image
Processing (IJIP), Vol. 3, No. 5, pp. 184–194.
8. Debasish Jena, Banshidhar Majhi, Saroj kumar Panigrahy and Sanjay Kumar Jena, ICCI
2008, “Improved Ofﬂine Signature Veriﬁcation Scheme Using Feature Point Extraction
Method,” Proc. 7th IEEE Int. Conference on Cognitive Informatics, pp. 475–480.
9. Mishra, Prabit Kumar and Sahoo, Mukti Ranjan, 2009, “Ofﬂine Signature Veriﬁcation
Scheme”.
10. Priyanka Chaurasia, 2009, Ofﬂine Signature Veriﬁcation using High Pressure Regions, US
Patent 7, 599,528, Oct 6, 2009.
11. Meenakshi K. Kalera, Sargur Srihari and Aihua Xu, 2004, “Ofﬂine Signature Veriﬁcation and
Identiﬁcation using Distance Statistics,” International Journal of Pattern Recognition and
Artiﬁcial Intelligence, Vol. 18, No. 7, pp. 1339–1360.
12. Banshider Majhi, Y Santhosh Reddy, D Prasanna Babu, 2006, “Novel Features for Ofﬂine
Signature Veriﬁcation”, International Journal of Computers, Communications & Control,
Vol. I, No. 1, pp. 17–24.
13. Donato Impedovo, Giuseppe Pirlo, 2008, “Automatic Signature Veriﬁcation: The State of the
Art,” IEEE Transactions on Systems, Man and Cybernetics-Part C: Applications and
Reviews, Vol. 38, No. 5, pp. 609–635.
216
A.K. Shukla and S. Kanungo

Generalized Parametric Model
for AVK-Based Cryptosystem
Shaligram Prajapat, Adisha Porwal, Swati Jaiswal, Fatema Saifee
and R.S. Thakur
Abstract This work presents the enhancement of security of automatic variable
key (AVK)-based cryptosystem by exchanging some parameters instead of entire
keys between the communicating parties [1–4]. Now, these parameters will be used
to generate required keys at the receiver end. The work presents a framework for
above-speciﬁed mechanism. Moreover, the model has been demonstrated with
parameterized scheme and production issues in AVK approach (from the different
user’s perspective including hacker).
Keywords Key ⋅Automatic variable key (AVK) ⋅Cryptosystem
1
Introduction
According to Moore’s law, the power of personal computers has historically dou-
bled approximately every 18 months. In addition, well-equipped attackers often
develop new techniques and algorithms to improve the efﬁcacy of key search
attacks. Therefore, an estimate of the time required for successful key search attacks
is moving downward as the computing power and resources (available to attackers)
increases [1].
Most of the time they are successful due to: availability and accessibility of fast
computing resources, capability to use the power of AI-enabled algorithms, and
availability of sender/receiver’s personal information to prune the search space
making task of cryptanalyst and hacker’s job easier. With the growth of multicourse
S. Prajapat (✉) ⋅A. Porwal ⋅S. Jaiswal ⋅F. Saifee
International Institute of Professional Studies, Devi Ahilya University, Indore, India
e-mail: shaligram.prajapat@gmail.com
S. Prajapat ⋅R.S. Thakur
M.A.N.I.T, Bhopal, India
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_20
217

processing and availability of CPU-GPU pairs parallel- and grid-based computing
algorithms, the search time can be reduced to polynomial time from exponential
(infeasible) in the near future [2–4]. Presently, to prevent from brute-force attacks,
best alternatives are: (1) Reduce the life time of key and (2) increase the key length.
In the former approach, by choosing the shorter the key lifetime, one can reduce the
possible potential damage even if one of the keys is known. In later approach,
choosing longer key length, one can decrease the probability of successful attacks
by increasing the number of combinations that are possible.
1.1
Limitations of Increasing Key Size [5]
The state-of-the-art symmetric key-based cryptosystem trends toward increasing
length of key for enhancing security, but it has certain side effects. It increases
processing, resource utilization, and time consumption. The traditional approach of
choosing key is actually deciding the string of characters consisting of digits,
numbers, special symbols, etc., (depending upon the type of implementation and
system need) which is checked by source or destination. If the supplied key matches
with the one which is associated with the actual user’s resource (ﬁles, databases,
etc.), access is granted to all the resources of the authorized user.
1.2
Enhancement of Symmetric Cryptosystem Instead
of Increasing Key Size
Consider the Table 1 that is showing some symmetric key-based cryptosystems and
key size for encryption and decryption of plaintext information. DES was used
widely in the ﬁnancial industries. DES is a block cipher with 64-bit block size and
56-bit keys. This algorithm is still strong, but, new versions with increased key
length; 3DES have been developed to make it more secure. International Data
Encryption Algorithm (IDEA) uses 128-bit key and is considered very secure. RC2
and RC4 are also fast cipher, but require large keys. It accepts keys of variable
length.
Table 1 Symmetric algorithms with trends of using key size [1, 2, 4]
S. no.
Algorithm
Block size
Key length
1
DES
64 bits
56 bits
2
3DES
64 bits
168, 112, or 56 bits
3
BlowFish
64 bits
8–128 bits (variable length key)
4
TwoFish
128 bits
Up to 256 bits (variable length key)
218
S. Prajapat et al.

1.3
The Parametric Model for AVK
Alternative approach for improving security instead of using long keys of variable
length to minimize time complexity and high power consumption, AVK concept
can be introduced, where the secret key will vary from session to session. The
Sender and Sender are communicating parties in standard literatures and Trudy as
hacker or man in the middle. Subsequent section will discuss this alternative
AVK-based strategy, where efﬁcient transmission of data from source to destination
will be achieved by using dynamic key. In Table 2, Sender and Sender are two
communicating parties, exchanging secure messages and key [6–10].
1.4
Fibo-Q Approach from the View of Parametric Model
Instead of exchanging key over the insecure network, only parameter n used by
sender to construct key matrix ﬁbo-Similarly the after receiving the parameter, the
recipient will construct decryption key. Since only the parameters are exchanged,
any intruder (man in the middle) will not able to get key directly. This shows
additional security of parameterized Fiboencrypt () process. Figures 1 and 2 depict
the parametric scheme for Fibo-Q approach [11, 12].
Table 2 AVK approach for symmetric key-based cryptosystem [6–8, 10]
Session
ID
Sender
sends
Sender receives
Sender
sends
Sender
receives
Remarks
1
Secret
key
(say 2)
2
Secret
key
(say 6)
6
For next slot, Sender
will use 6 as key and
Sender 2 as key for
transmitting data
2
Sender
sends
Sender
ﬁrst data
as: 3 xor
6
Sender gets
back original
data as: (3 xor 6)
xor 6 = 3
Sender
sends ﬁrst
data as: 7
xor 2
Sender gets
back original
data as: (7 xor
2) xor 2 = 7
Sender will create
new key 6 xor 7 for
next slot. Sender will
create a new key (2
xor 3)
3
Sender
sends
next data
as: 4 xor
(6 xor 7)
Sender gets
back original
data as: ((4 xor
(6 xor 7)) (6 xor
7)) = 4
Sender
sends
next data
as: 8 xor
(2 xor 3)
Sender
recovers data
as: ((8 xor (2
xor 3)) (2 xor
3)) = 8
Thus, Sender and
Sender, respectively,
exchange data 34 and
78
Generalized Parametric Model …
219

The working of parameterized encryption based on Fibonacci Q-matrix-based
cryptosystem described below. The algorithm assumes that initially both Sender
and Sender agree on shared parameter n, before the start of communication.
1.5
Generalized Parameterized Encryption Process
(a) Select parameter n
(b) Compute enciphering key matrix Q using parameter n
(c) Transform plaintext to be transmitted with multiplication of recently computed
key
(d) Transfer the cipher text
(e) End of algorithm
Fiboencrypt( ) Algorithm 
Construct key from n 
Receive Parameter 
n for key 
Transmit 
encrypted matrix
Plain Text 
of Size p
(
1)
( )
( )
(
1)
n
f n
f n
Q
f n
f n
+
⎡
⎤
= ⎢
⎥
−
⎣
⎦
Fig. 1 Parameterized
Fiboencrypt () Algorithm [11,
12]
(
1)
( )
( )
(
1)
n
f n
f n
Q
f n
f n
−
−
−
⎡
⎤
= ⎢
⎥
−
+
⎣
⎦
Fibodecrypt( ) Algorithm 
Construct key from n
Receive Parameter 
n for key 
Recover Plain 
Text matrix 
Cipher Text 
of Size p  
Fig. 2 Parameterized
Fibodecrypt () Algorithm [11,
12]
220
S. Prajapat et al.

1.6
Generalized, Parameterized Decryption Process
(a) Receive parameter n
(b) Compute deciphering key matrix Q using parameter n
(c) Recover the plaintext from matrix Q with multiplication of recently received
Ciphertext
(d) Use the plain text
(e) End of the algorithm
To support AVK model, it is recommended that parameter must be changed
from session to session to compute key of a particular session. In the next section,
another approach for AVK-based symmetric cryptosystem is discussed.
1.7
Sparse Approach from the View of Parametric Model
Following parametric model demonstrates the usage of parameter for encryption
and instead of key parameters will be exchanged over insecure channel. Here also,
Instead of exchanging key over the in secured network, only parameter (i, j) used by
sender to encipher the plain text data CSM[i] [2]. Similarly after receiving the
Ciphered CSM’ [i] [2], the recipient will reconstruct plain text. Since only the
parameters (i, j) are exchanged not the procedure (transformation equation), any
intruder (man in the middle) will not able to get plaintext information. This shows
additional security of parameterized LSAVKencrpt (), QSAVK (), and CSAVK ()
process. Figures 3 and 4 depict the parametric scheme for SAVK approach [11, 13].
Based upon illustration in Sect. 4, general models for parameter only scheme is
proposed in the Sect. 4 and this will enhance the level of security and prevents key
information from frequent exchange of key.
Fig. 3 Parametrized sparse AVK model for enciphering [13]
Generalized Parametric Model …
221

1.8
Parameters Only Scheme for Automatic Variable Key
So far during the discussions, in the previous sections, this has been pointed out that
state-of-the-art cryptographic algorithms rely on increasing the key size. Thus, it
would require more time, computation, and battery power. Automatic variable key
(AVK) has been devised to explore alternative approaches. Two methods have been
discussed to demonstrate how AVK-based cryptosystem can be developed. Both
methods use some parameters to construct key. Fibonacci method (for a particular
session, with given n and p values computations can be done for fn −1, fn and fn + 1)
[11, 12] and Sparse Matrix (Location coordinate (i, j) will act as parameter for
encryption/Decryption) [13]-based approach can be modeled for automatic vari-
ability of key for secure information exchange. For these AVK-based cryptosystem,
parameters (n, p) or location (i, j) can vary from session to session. So, even if the
intruder gets unwanted access to the key of session at time slot t, it would not be
valid for original message extraction in session slot at a time (t + 1) onwards. In
this model, since the key is not transmitted in the data transfer. So, it becomes
highly difﬁcult to interpolate any information regarding plaintext or key. This entire
process can be modeled in the form of parameterized-AVK model as Fig. 5. In this
model, node-A and node-B (can be extended to node-n) are communicating with
each other by sharing parameters instead of key exchange. The model also
demonstrates that for same parameters, different approaches may generate same
key. Thus, additional level of security may be achieved by parameterized model.
The two approaches for computation of key from parameters have been demon-
strated by Algorithms 1 and 2.
Computing Keys from Geometric Mean
Algorithms 1 and 2 (parameter4key-node-A () and parmater4key-node-B ())
demonstrate working of information exchange based on ‘parameters only’ scheme:
Fig. 4 Parameterized sparse AVK model for deciphering [13]
222
S. Prajapat et al.

Algorithm 1 parameters4Key-node-1 (parameters p1, p2)
{ 
1. Sense  parameters p1,p2; 
2. Compute the key for information exchange  by: keyi =(p1*p2)1/2 ; 
3. Sense the information to exchange=Di  ; 
4. If (mode==transmit)
Generate Cipher text C i =Encrypt( Di, keyi); 
Transmit Ci;
5. else
Receive Plain text P i = Decrypt ( Di, keyi); 
Use P i ; 
} 
Fig. 5 Generalized framework of parameterized-AVK-based symmetric cryptosystem [13, 15, 18,
19]
Generalized Parametric Model …
223

Computing Keys from Arithmetic and Harmonic Mean
Algorithm 2 Parameters 4Key-node-2 (parameters p1, p2)
{ 
1. Sense  parameters p1,p2; 
2. Compute the Arithmetic Mean A.M.= ( p1+p2 )/2;
3. Compute the Harmonic Mean H.M.=2* p1*p2 /( p1+p2  ) 
4. Compute the Key i = (A.M.*H.M)1/2 
5. If (mode==transmit)
Generate Cipher text C i =Encrypt( Di, key i); 
Transmit Ci; 
6. else
Receive Plain text P i = Decrypt ( Di, key i); 
Use P i;
} 
The major advantage of Algorithms 1 and 2 can be noted here; without
exchanging entire key, node-1 and node-2 will securely communicate with each
other. Both the nodes are computing the same key using different function, which in
turn enhances the level of security.
Session-1 (p1 = 9, p2 = 4)
Session −1
Senderkey
p1 = 9
p2 = 4
keyA =
ﬃﬃﬃﬃﬃﬃﬃﬃ
9*4
p
keyA = 6
Receiverkey
p1 = 9, p2 = 4
A.M. = 9 + 4
2
H.M. = 9*4*2
9 + 4
⇒keyB =
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
9 + 4
2 *
9*4*2
9 + 4
q
= 6
Both sender and receiver will use key 6 or (00000110)2 for session-1.
Session-2 (p1 = 9, p2 = 4)
Session −2
Senderkey
p1 = 8
p2 = 2
keyA =
ﬃﬃﬃﬃﬃﬃﬃﬃ
8*2
p
keyA = 4
Receiverkey
p1 = 8, p2 = 2
A.M. = 8 + 2
2
H.M. = 8*2*2
8 + 2
⇒keyB =
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
8 + 2
2 *
8*2*2
8 + 2
q
= 4
In this way for subsequent sessions by varying parameters, keys can be con-
structed with suitable choice of parameters exchange. In case of real numbers, the
nearest integer values can be used for key generation. Thus, even if the parameters
or method at any one node for key computation is known, it will not work for next
node or parameter set. In session-i for message mi, the key ki is used for generation
of cipher Ci, now key is changed for session-j as kj. In session-j for message mj, the
key kj is used for generation of cipher Cj. Further different key will be used for new
224
S. Prajapat et al.

session. On the communication channel, parameters used for key construction pi,
pk, and ps are exchanged. Using a mechanism described in previous section
(Algorithms 1 and 2). The only possibility of this model is tracing the behavior of
parameters or parameter-log (A data base of captured parameters), to discover
association among parameters, correlation and association rules discovery and
classiﬁcation of parameters, based on frequency of usage to guess hints about key
and predict future keys is depicted in (top middle section of Fig. 5).
2
Probable Security Analysis of Parametric Model
and Need of Cryptic Mining
The strength and weakness of cryptosystem can be evaluated by exploiting them
and identifying the degree of damage or loss of information. The harder the
exploitation of the system, weakness will make the cryptosystem more secure [14–
16]. By collecting or recording parameter information in the log, noticing avail-
ability of captured cipher text, some guessed plaintext may be used by cryptanalyst
or hackers for pattern discovery, and guessing the vulnerability of a system, will
measure the degree of success [17]. Assuming each scalar arithmetic takes a unit
time with sequential complexity T ðnÞ = O ðn3Þ.
3
Extensions to Generalized Parametric Model
With the choice of key, parameters from 4 to 6 will be optimized with respect to time
and power and hackers perspectives. A larger number of parameters used for key
construction in AVK-based symmetric cryptosystem may ensure security at higher
side, but may involve complexity issues, and less number of parameters used for key
construction may compromise the strength of a cryptosystem. Thus, there is a need
for performance analysis of parameterized-AVK approach with state-of-the-art
methods.
Choosing the parameter for key construction from Fibonacci Q-matrix may have
several issues, such as over what range of n and p cryptosystem will give optimum
performance, beyond which it may have slow performance, and easy guessing
issues. From a security point of view, it would be better to have matrix elements of
4–6 digits that are minimum 32 bits of maximum key size of 48 bits.
The Sparse approach for AVK approach location coordinates acts as parameters
that are diffused in the exchanging block. The parameterized cryptosystem which is
working properly for encryption and decryption task always needs to be passed
through the proper screen test from hackers or intruder’s perspective to rectify
against weaknesses. So, for how long it ﬁghts against brute-force attack and
AI-based tests? It needs to be investigated. It is assumed that output of encryption
process has been always random, free from patterns. But, in reality, this is not the
Generalized Parametric Model …
225

case. Patterns may be discovered, attempts can be made based on cipher classiﬁ-
cation and similarity among frequent parameters, exploring correlation, and asso-
ciation rules among parameters to predict future parameters. If some degree of
non-randomness appears in encrypted texts, then clustering of ciphers may be found
key size, plaintext-cipher text correlations, association rule base can be formulated
for predictions based on frequent patterns. So, metric to describe the efﬁciency of
cryptosystem w.r.t. systematic attack has to be tested also from these AI-based tests.
4
Conclusion
This work highlights the parametric AVK scheme to enhance security of conven-
tional AVK model. Formulation of parameterized-AVK security models without key
exchange among communicating entities has been supported with proper examples
and has been discussed with their merits and demerits. Parameterized-AVK schemes
have been analyzed from the view of exploiting key information about ciphers,
parameters, and log of session-wise captured information. This work also paves way
for application of mining algorithms in cryptographic domain.
References
1. Shaligram Prajapat and Ramjeevan Singh Thakur, “Towards Parameterized Shared Key for
AVK Approach”, In the book Pattern and Data Analysis in Healthcare Settings, DOI: 10.
4018/978-1-5225-0536-5.ch004.
2. P. Chakrabarti, et al, “Application of. Automatic Variable Key (AVK) in RSA”. Int’l J HIT
Transactions on ECCN, Vol. 2,. No. 5, Jan–Mar 2007, pp. 304–311.
3. P. Chakrabarti et al., Various New and Modiﬁed approaches for selective encryption (DES,
RSA and AES) with AVK and their comparative study, published in International Journal HIT
Transactions on ECCN, Vol. 1, No. 4, pp. 236–244. 51
4. Bhunia, C. T., Application of AVK and selective encryption in improving performance of
quantum cryptography and networks, United Nations Educational Scientiﬁc and Cultural
Organization and International Atomic Energy Agency, retrieved, Vol. 10, No. 12, pp. 200–
210, 2006.
5. Prajapat, Shaligram, Ramjeevan Singh Thakur. “Optimal Key Size of the AVK for Symmetric
Key Encryption.” In Covenant Journal of Information & Communication Technology, Vol.
3 (2), pp. 71–81. 2015.
6. Shaligram Prajapat, Gaurav, R.S. Thakur, “Towards investigation of efﬁcient Cryptosystem
using Sgcrypter”, In Special Issue of International Journal of Applied Engineering and
Research (IJAER), Vol. 10 (79), pp. 853–858, 2015.
7. Bhunia C. T., Chakrabarti P., Chowdhuri A. and Chandan T., Implementation of Automatic
Variable Key with Choas Theory and Studied Thereof, J IUP Computer Science, Vol. 5, No 4,
pp. 22–32, 2011.
8. Bhunia C.T., Mondal G. and Samaddar S., Theories and Application of Time Variant Key in
RSA and that with selective encryption in AES, Proc. EAIT, Elsevier Publications, Calcutta
CSI-06, pp. 219–221, 2006.
226
S. Prajapat et al.

9. Bhunia C. T., Chakrabarti P., Goswami R., A New Technique (CSAVK) of Automatic Variable
Key in Achieving Perfect Security, 100th Indian Science Congress Association, 2013.
10. Dutta M.P., Banerjee S. and Bhunia C., Two New Schemes to Generate Automatic Variable
Key (AVK) to achieve the Perfect Security in Insecure Communication Channel, Proceedings
of the 2015 International Conference on Advanced Research in Computer Science
Engineering & Technology (ICARCSET 2015), pp. 1–4, 2015.
11. Prajapat, Shaligram, D. Rajput, Ramjeevan Singh Thakur, Time variant approach towards
symmetric key, In proceedings of IEEE Science and Information Conference (SAI), London
2013, pp. 398–405, 2013.
12. Prajapat, Shaligram, Ramjeevan Singh Thakur. “Realization of information exchange with
Fibo-Q based Symmetric Cryptosystem.” International Journal of Computer Science and
Information Security, Vol. 14(2), pp. 216–223, 2016.
13. Shaligram prajapat and R.S. Thakur, “Key Diffusion Approach for AVK based Cryptosys-
tem”, in proceedings of International Conference on ICT for Competitive Strategies-ICTCS
Udaipur March 2016.
14. Prajapat,
Shaligram,
Ramjeevan
Singh
Thakur.
“Various
Approaches
towards
Crypt-analysis.” International Journal of Computer Applications, Vol. 127(14), pp. 15–24,
2015. (Doi: 10.5120/ijca2015906518)
15. Prajapat, Shaligram, Ramjeevan Singh Thakur. “Cryptic Mining for Automatic Variable Key
Based Cryptosystem”, Elsevier Procedia Computer Science, Vol. 78 (78C), pp. 199–209,
2016. doi:10.1016/j.procs.2016.02.034).
16. Prajapat, Shaligram, Thakur, A., Maheshwari, K., & Thakur, R. S., “Cryptic Mining in Light
of Artiﬁcial Intelligence”, IJACSA, Volume 6(8), pp. 62–69, 201510.14569/IJACSA.
2015.060808).
17. Chakrabarti P., Bhuyan B., Chowdhuri A., and Bhunia C., A novel approach towards
realizing optimum data transfer and Automatic Variable Key (AVK) in cryptography, IJCSNS
International Journal of Computer Science and Network Security, Vol. 8, No. 5, pp. 241,
2008.
18. Prajapat, Shaligram, Ramjeevan Singh Thakur. “Cryptic Mining: Apriori Analysis of
Parameterized Automatic Variable Key based Symmetric Cryptosystem.” International
Journal of Computer Science and Information Security, Vol. 14 (2), pp. 233– 246, 2016.
19. Prajapat, Shaligram, Asok Sharma, Ramjeevan Singh Thakur, “AVK based Cryptosystem and
Recent Directions Towards Cryptanalysis”, Journal of Internet Computing and Services(JICS)
2016. Oct.: 17(5), ISSN 2287-1136. http://dx.doi.org/10.7472/jksii.2016.17.5.00.
20. Han J., Kamber M. and Pei J., Data mining: Concepts and technique, 2nd edition, Morgan
kaufmann, 2006
Generalized Parametric Model …
227

Implementation of Modiﬁed RSA
Approach for Encrypting and Decrypting
Text Using Multi-power and K-Nearest
Neighbor Algorithm
Shikha Mathur, Deepika Gupta, Vishal Goar and Sunita Choudhary
Abstract Security is the concept of keeping information secret by protecting it
from unauthorized users. In a distributed medium, cryptography becomes crucial
part for secure communication. This chapter presents literature survey of the pre-
vious work done by various researchers in the ﬁeld of cryptography and imple-
mentation of modiﬁed approach with a fair comparison between traditional RSA
and modiﬁed approach. The text of different sizes was encrypted and decrypted
using modiﬁed approach. The simulation results are also shown with graphical
description simultaneously.
Keywords Public key ⋅Private key ⋅RSA ⋅Symmetric key and asymmetric
key ⋅‘n’ prime number ⋅Multiple public key
1
Introduction
Security means keeping information safe and secure by protecting it from unau-
thorized users. In order to keep a data secured, it must be prevented from unau-
thorized access (conﬁdentiality), prevented from any modiﬁcations during data
transfer (integrity), and always available to authorized persons when needed
(availability). In today’s era, internet is the basic need for communication between
S. Mathur (✉) ⋅D. Gupta
Marudhar Engineering College, Bikaner, India
e-mail: shikhamathur806@gmail.com
D. Gupta
e-mail: deepika.gupta1218@gmail.com
V. Goar
Government Engineering College, Bikaner, India
e-mail: goar.vishal@gmail.com
S. Choudhary
College of Engineering & Technology, Bikaner, India
e-mail: sunitadangi@gmail.com
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_21
229

people, military communication, and many other secret communications. This
causes a major concern for privacy and security. Cryptography is a standard way to
secure the data over the medium. Cryptography has emerged out as most effective
solution for protecting data [1].
2
Proposed Methodology
In proposed methodology, we have introduced a modiﬁed form of RSA
cryptosystem.
2.1
Key Generation
These are the steps for public and private keys generation:
• Make a set of prime numbers PR, which has ‘n’ prime numbers.
• Choose any four prime numbers A, B, C, and D from the set PR.
• Calculate L (product of prime numbers)
L = A * B * C * D.
• Calculate ϕ(L)
ϕðL) = ðA −1Þ * ðB −1Þ * ðC −1Þ * ðD −1Þ.
• Calculate J (public key), such that
Gcd ðJ, ϕðL)Þ = 1.
• Calculate K (private key), such that
K * J mod ϕðL) = 1.
• Choose random number N and O.
• Choose two numbers P and Q, such that Q = PJ.
2.2
Encryption
These are the steps used for encryption of a message:
• The process of encryption encrypts the message character by character.
• Convert the message into their respective ASCII values.
230
S. Mathur et al.

• Calculate ‘E’ for each individual ASCII value of message, such that
E = ðASCII VALUEQ ̸PÞK mod L.
• Calculate R1, as it encrypts the message and gives back cipher text of given plain
text
R1 = ðmessageÞK mod L.
• If the ASCII values and values of R1 come same, then apply K-nearest neighbor
algorithm
– Choose alternative prime A′ from the set PR.
– Calculate L′ (product of prime numbers)
L′ = A′ * B * C * D.
– Calculate ϕ(L′)
ϕðL′Þ = ðA′ −1Þ * ðB −1Þ * ðC −1Þ * ðD −1Þ.
– Calculate J′ (public key), such that
Gcd ðJ′, ϕðL′)Þ = 1.
– Calculate K′ (private key), such that
K′ * J′ mod ϕðL′) = 1.
– Calculate R′
1, as it encrypts the message and gives back cipher text
R′
1 = ðmessageÞK′ mod L′.
– Loop back the whole process until the ASCII value is not equal to R1 value.
• After that calculate R2
R2 = ðmessage * NR
1 Þ mod L.
• Veriﬁcation
H(m)Y = ðRO
2 * ER
1 Þ mod L.
Implementation of Modiﬁed RSA Approach …
231

2.3
Decryption
These are the steps used for decryption of a message:
• Calculate plain text back again from cipher text using the equation
H(m) = RJ
1 mod L.
• Veriﬁcation
H(m)Y mod L.
3
Simulation Result and Discussion
The proposed algorithm was tested on varying length messages, the performance of
the proposed algorithm in terms of encryption time, decryption time, key generation
time, and total time is shown in Tables 1 and 2, and a fair comparison is shown
between RSA and modiﬁed work in Table 3 (Figs. 1, 2, 3 and 4).
4
Conclusion and Future Work
The proposed system includes a modiﬁed approach of RSA which includes
exponential form of RSA with four prime numbers and multiple (i.e., two) public
keys with k-nearest neighbor algorithm. My modiﬁed approach introduces an
additional level of security using k-nearest neighbor algorithm. It enhances the
randomness of the calculated value in cipher text. As it removes the problem of
redundancy in cipher text same as in plain text, in modiﬁed approach, there is no
repetition of cipher text same as plain text. By adopting this, any intruder ﬁnds it
difﬁcult to hack the information being transmitted. It is helpful in increasing the
efﬁciency and security of the approach. Modiﬁed approach uses two public keys
which are sent separately, this makes the attacker not to get much knowledge about
the keys and unable to decrypt the message. Modiﬁed approach has veriﬁcation at
both ends sender and receiver which ensures authenticity of a message. My mod-
iﬁed approach initially converts message string into their respective ASCII values
as this conversion also increases complexity and modiﬁed approach work character
by character. Modiﬁed approach also reduces the encryption and decryption time
for encrypting and decrypting the input messages.
Future researches may be directed to investigating further improvement on the
audio and video data encryption and decryption. Future work will be continue to
232
S. Mathur et al.

Table 1 Encryption and decryption time using proposed approach including four prime numbers
Prime 1
Prime 2
Prime 3
Prime 4
N
Phi (N)
Public key
Private key
Encryption time (ms)
Decryption time (ms)
7
11
13
17
17,017
11,520
41
281
927
221
13
17
19
23
96,577
76,032
139
547
1454
239
19
23
29
31
765,049
665,280
577
1153
13,760
207
11
13
17
19
46,189
34,560
17
2033
13,983
240
29
31
37
41
1,363,783
1,209,600
17
71,153
23,933
180
Implementation of Modiﬁed RSA Approach …
233

Table 2 Key Generation and total time using proposed approach including four prime numbers
Prime 1
Prime 2
Prime 3
Prime 4
N
Phi (N)
Public key
Private key
Key generation time (ms)
Total time (ms)
7
11
13
17
17,017
11,520
41
281
879
61,421
13
17
19
23
96,577
76,032
139
547
1448
86,902
19
23
29
31
765,049
665,280
577
1153
13,736
82,889
11
13
17
19
46,189
34,560
17
2033
13,970
141,147
29
31
37
41
1,363,783
1,209,600
17
71,153
23,883
368,982
234
S. Mathur et al.

Table 3 Comparison between traditional RSA and proposed approach
Traditional RSA
Proposed approach
Uses two prime numbers
Uses four prime numbers
Less randomly generates
cipher text value
Increases the randomness of the cipher text value
Less complex
Increases the complexity of initial activity such as character to
ASCII number conversion
Uses only one public key
Uses two public key
Less secure
More secure
Fig. 1 Graph for encryption time
Fig. 2 Graph for decryption time
Implementation of Modiﬁed RSA Approach …
235

study the characteristics of audio and video data ﬁles encryption and decryption
through our proposed algorithm. There should be deep research on increasing the
size of the data. There are possibilities to do research in deep on sending multiple
data in the form of audio and videos and that data should be free from noise and
making it more attractive and error free.
Fig. 3 Graph for key generation time
Fig. 4 Graph for total time
236
S. Mathur et al.

References
1. M. Thangavel, P. Varalakshmi, Mukund Murrali, K. Nithya, “An Enhanced and Secured RSA
Key Generation Scheme”, Department of Information Technology, Anna University,
Chennai, 2014, Elsevier.
2. Amare Anagaw Ayele, Dr. Vuda Sreenivasarao, “A Modiﬁed RSA Encryption Technique
Based on Multiple public keys”, International Journal of Innovative Research in Computer
and Communication Engineering, Vol. 1, Issue 4, ISSN 2320-9798, June 2013.
3. Ammar Odeh, Khaled Elleithy, Muneer Alshowkan, Eman Abdelfattah, 2013, “Quantum Key
Distribution by Using Public Key Algorithm (RSA)”, IEEE.
4. Norhidayah Muhammadi, Jasni Mohamad Zaini, Md Yazid Mohd Saman, “Loop-based RSA
Key Generation Algorithm using String Identity”, 13th International Conference on Control,
Automation and Systems (ICCAS 2013). International Journal of Computer Applications
(0975 – 8887) Volume 114 – No. 7, March 2015 33.
5. Ms. Ritu Patidar, Mrs. Rupali Bhartiya, 2013, “Modiﬁed RSA Cryptosystem Based on Ofﬂine
Storage and Prime Number”, IEEE.
6. Liang Wang, Yonggui Zhang, 2011, “A New Personal Information Protection Approach
Based on RSA Cryptography”, IEEE.
7. Malek Jakob Kakish, “Enhancing The Security Of The RSA Cryptosystem”, IJRRAS August
2011.
8. Dr. D.I. George Amalarethinam, J. Sai Geetha, “Enhancing Security level for Public Key
Cryptosystem
using MRGA”,
World
Congress
on
Computing and Communication
Technologies, 978-1-4799-2876-7/13/, 2014, IEEE.
9. Dr. Abdulameer K. Hussain, “A Modiﬁed RSA Algorithm for Security Enhancement and
Redundant Messages Elimination Using K-Nearest Neighbor Algorithm”, IJISET - Interna-
tional Journal of Innovative Science, Engineering & Technology, Vol. 2, Issue 1, ISSN
2348-7968, January 2015.
10. Xianmeng Meng, Xuexin Zheng, “Cryptanalysis of RSA with a small parameter revisited”,
Information Processing Letters 115, 858-862, 2015, Elsevier.
11. Ritu Tripathi, Sanjay Agrawal, “Critical Analysis of RSA Public Key Cryptosystem”,
International Journal of Advanced Research in Computer Science and Software Engineering,
Volume 4, Issue 7, ISSN 2277-128X, July 2014.
12. Shilpi Gupta, Jaya Sharma, “A Hybrid Encryption Algorithm based on RSA and
Difﬁe-Hellman”, International Conference on Computational Intelligence and Computing
Research, 978-1-4673-1344-5/12, 2012, IEEE.
Implementation of Modiﬁed RSA Approach …
237

Traditional and Hybrid Encryption
Techniques: A Survey
Pooja Dixit, Avadhesh Kumar Gupta, Munesh Chandra Trivedi
and Virendra Kumar Yadav
Abstract Information security is the process that protects its availability, privacy,
and integrity. Access to stored information on computer databases has increased
nowadays. Most companies store business and individual information in computer.
Much of the information stored is highly conﬁdential and not for knowing publicly.
Data encryption is most traditional technique that secure highly conﬁdential
information by using some conventional algorithm, which already exist or
prewritten. Most powerful part of encryption technique is key generation, which has
two parts, one is symmetric key generation and another is asymmetric key gener-
ation. Nowadays hackers are easily capable to break the key with the help of
modern high computing machines. Current need is strongly encrypted data which
cannot be decrypt through cryptanalysis. Paper presented discusses some traditional
as well as modern hybrid encryption techniques along with quantum approach such
as RSA based on ECC with AVK, DES-RSA, RSA-based singular cubic curve,
JCE, 3D chaotic map technique, Blowﬁsh.
Keywords Information security ⋅Symmetric encryption technique ⋅Hybrid
encryption ⋅Double encryption ⋅Certiﬁcate-based-encryption ⋅Link encryption
technique
P. Dixit ⋅M.C. Trivedi ⋅V.K. Yadav (✉)
Computer Science Department, ABES Engineering College, Ghaziabad, Uttar Pradesh, India
e-mail: virendrashines@gmail.com; virendra.yadav@abes.ac.in
P. Dixit
e-mail: pooja.dixit68@gmail.com
M.C. Trivedi
e-mail: muneshtrivedi@gmail.com
A.K. Gupta
IMS Engineering College, Ghaziabad, Uttar Pradesh, India
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_22
239

1
Introduction
In all over world, nowadays, biggest challenges are conﬁdentiality; everyone wants
conﬁdentiality in business, in social media, etc. Cryptography provides encryption
techniques to resolve this problem. Cryptography is the concept which allows
information to be sent in a secure form in such a way that only receiver is able to
retrieve this information. Presently, continuous researches on the new cryptographic
algorithms are going on. However, it is a very difﬁcult to ﬁnd out the speciﬁc
algorithm; these algorithms must consider many factors such as security, features of
algorithm, time complexity, and space complexity (Fig. 1).
1.1
Security Services
If thinking about security, then following information comes in mind [1]:
• Conﬁdentiality (privacy)
• Authentication (who created or sent the data)
• Integrity (data has not been altered)
• Non-repudiation (the order is ﬁnal)
• Access control (Authorized person who has permission to access)
• Availability (presence)
Figure 2 explains some algorithms that help in providing conﬁdential data.
There are many algorithms to encrypt text into code word, but these algorithms are
not sufﬁcient because encryption is a very common technique for promoting the
information security. The evolution of encryption is moving toward a future of
endless possibilities. Everyday new methods of encryption techniques are discov-
ered. As simple encryption algorithms are very easy to break by unknown user once
Fig. 1 Conventional encryption
240
P. Dixit et al.

key or logic known. Paper presented discusses some cryptographic techniques
under the heading “types of encryption technique,” researches in this domain under
the heading “literature survey,” comparison of various techniques under the heading
“Comparison,” and “Conclusion” contains the summary and future research
directions.
1.2
Types of Encryption Technique
Commonly, three types of cryptographic techniques are symmetric cryptographic
technique, asymmetric cryptographic technique, and hash function. In symmetric
technique using single key encryption to encrypt or decrypt data. Data Encryption
Standard (DES), Advance Encryption Standard (AES), Carlisle Adams and Stafford
Tavares (CAST) algorithm, Blowﬁsh, Two ﬁsh, International Data Encryption
Algorithm (IDEA), and Secure and Fast Encryption Routine (SAFER) are some
examples of symmetric encryption. In an asymmetric technique, two different keys
are used, one (public) key for encryption and another (private) key for decryption, it
may vice versa too. RSA, DSA, Elgama, and elliptic curve cryptography (ECC) are
examples of asymmetric technique. The hash function uses a mathematical trans-
formation to irreversibly “encrypt” information. This type of technique includes
message digest (MD5), SHA-1, SHA-2. The RSA algorithm at present is the most
successful in use for ciphering keys and password or counts [1].
2
Literature Survey
In 2015, Sourabh Chandra and Bidisha Mandalb proposed double encryption [2],
which is content-based algorithm that implements folding method and circular
bitwise operation. In this technique, encryption of plaintext occurs two times with
Cryptography
Public Key
RSA & Others
Private Key
Block
UR5
Blow Fish & 
others
AES
RC6
DES
Stream
RC4 & Others
Protocols
Fig. 2 Overview of most common encryption algorithm [1]
Traditional and Hybrid Encryption Techniques: A Survey
241

secret key, providing cipher text by using circular bitwise binary addition operation.
Algorithm of double encryption also exists. For better security, do double
encryption of the text using secret key which is generated by a random number that
is provided as an input. Encrypt a text by using a simple addition method on the
ASCII value of each character with the length of corresponding word.
Hybrid technique, which has concept of combination of different-different
algorithm. Each algorithm is unique having some strengths and weaknesses. In
hybrid technique, use strength of technology and to overcome limitations use
another technique together, so that will give best result in area of security. Lots of
work have been done in hybrid technique such as AES-ECC [3], Fibonacci
series-XOR bitwise-PN sequence [4], IDEA-RSA [5], and DES-RSA [1]. In this
concept, plaintext is encrypted by an encryption technique using secret key and that
secret key is encrypted by another technique. Then plaintext is encryptedusing
encrypted secret key. But some of the algorithms use double encryption like
AES-ECC, and this provides security for variety of multimedia data such as text
document, images, audio, video. First, convert this data into base64 encoded version
in text format [3]. At the initial level, generate key randomly by using AES, and then
that key will be encryptedusing ECC public key. Then, encrypted AES key will be
used to encrypt plaintext to generate cipher text. After do again encryption of AES
encrypted text by using ECC public key. In Fibonacci-XOR logic-PN Sequence, still
completed, divide input message into some blocks and all block contains equal
number of characters. Every block is encrypted with the help of different types of
techniques. For example, text is—“This is our plaintext” [4] (Fig. 3).
Another hybrid technique is DES-RSA, where DES algorithm is used to encrypt
plaintext P with the help of session key (which is randomly generated) and give
C = E(k, P)
ð1Þ
Cipher text as output [1]. Since DES is a secret key encryption/decryption
model, this secret key has to be kept more secret and this is done by encrypting
secret key with the help of public key encryption model which is RSA algorithm.
DES’s secret key is encrypted by public key, so this produces session key “u” [1].
This is our plaintext
This is 
our pl 
aintext
FIBONACCI 
SERIES 
PN 
SEQUENCE 
XOR 
Cipher
Fig. 3 Split message into
parts and apply three different
techniques
242
P. Dixit et al.

u = keðmodðnÞÞ
ð2Þ
Another encryption technique available is called RSAbased singular cubic curve
with AVK [6], use to reduce time complexity. This technique is used to encrypt
subpart of message rather than whole message.
Selective part of message will be encrypted with the help of RSA-based singular
cubic curve, and rest of the part of message will be encrypted with the help of DES
algorithm.
In this technique, ﬁrst use data compression technique, that compress data,
reduces the data size means it reduces space complexity and encrypt that com-
pressed data by using AES algorithm to provide better security domain. This
concept involves two types of procedure [7].
(1) Individual compression and encryption
• Compression followed by Encryption (CE): This technique provides more
data security from access by unauthorized person, but size is more.
• Encryption followed by Compression (EC): This technique is not efﬁcient
due to decrease sequence in size, and this technique reduces space com-
plexity. But in this technique hackers can access some clue regarding
decryption of the cipher text.
(2) Joint Compression and Encryption (JCE): This method is faster and better than
above two techniques. But this is very complicated for the implementation.
In encryption technique, one more name which is gaining attention of
researchers is quantum encryption [8]. It is believed that when quantum computer
will be built then every traditional algorithm will be break within few seconds.
Every algorithm is based on key generation; if key is breakable, then unauthorized
user can decrypt the cipher text. So, must be prepare for future challenges and try to
make some quantum encryption technique. A lot of research is carrying out in this
area. It is developed in 1994 by Peter Shor at AT&T Bell Laboratory. This tech-
nique uses principle of quantum cryptography which is Heisenberg uncertainty and
photon of polarization principle. First prototype implementation of quantum
cryptography in IBM, 1989 [8]’. In this paper most of the algorithm are discussed
about encryption technique of text information only not for images, video, audio
and graphics. In 2015, Pradeep H. Kharat proposed technique which is 3D chaotic
map encryption technique. First time, Edward Lorenz used chaos theory in
encryption system in 1963 [9]. All encryption technique is unique and effective to
improve security domain. In 2014, Link Encryption Algorithm (LEA) used by
Hadia M.S. El Hennawy, Alaa E.A. Omar, Salah M.A. Khaliah. In this chapter,
proposed stream cipher algorithm that consists general structure of algorithm, key
loading, and three types of layer: Linear Feedback Shift Register (LFSR) Layer, Bit
Compression Layer, and Nonlinear Function F Layer.
Traditional and Hybrid Encryption Techniques: A Survey
243

3
Comparison
Literature survey discusses some common encryption algorithm which are com-
bination of conventional (DES, 3DES, AES, Blowﬁsh etc.) and public key (RSA,
ECC etc.) algorithms. This section presents some comparison between symmetric
key algorithm and comparison between some new techniques.
Table 1 mentions comparison between symmetric algorithms in terms of speed.
According to Table 1, Blowﬁsh gives better performance rather than other algo-
rithms Table 2 compares performance of some algorithms. From Table 2, it is
concluded that Blowﬁsh give better result in comparison to techniques mentioned.
In Table 2, LEA can be other preferred technique (Tables 3 and 4).
Table 1 Speed comparison of Block Cipher
Algorithm
Clock cycles per round
# of rounds
# of clock cycles per byte encrypted
DES
18
16
45
AES
14
12
40
IDEA
50
8
50
Blowﬁsh
9
16
18
Table 2 Link Encryption Algorithm (LEA), performance comparison results [10]
Input size (bytes)
DES
3DES
AES
BF
LEA
20,527
2
7
4
2
2
36,002
4
13
6
3
4
45,911
5
17
8
4
5
51,200
6
20
10
5
6
69,545
9
26
13
7
8
79,776
10
31
15
7
8
87,968
11
34
17
8
8
96,160
12
37
18
8
8
103,056
13
40
19
10
12
Average time/sample
8
26
12
6
7
Bytes/s
7988
2663
5320
10,167
9285
Table 3 Encryption execution time of some technique in second
Bits
Hybrid
Compression and
encryption
RSA-based singular cubic curve with
AVK
256
0.004
0.003
0.004
512
0.005
0.002
0.005
1024
0.003
0.002
0.005
2048
0.004
0.002
0.011
244
P. Dixit et al.

Table 4 Comparison of various algorithms on the basis of different parameters [11, 12]
Parameter
Blowﬁsh
Twoﬁsh
Threeﬁsh
Development
Bruce Schneier in 1993
Bruce Schneier in 1998
Bruce Schneier,
Niels Ferguson,
Stefan Lucks in
2008
Key length
(Bits)
32–448
128, 192, 256
256, 512, 1024
Rounds
16
16
For 256, 512
key = 72
For 1024 key = 80
Block sizes
(Bits)
64
128
256, 512 and 1024
Attack found
No attack is found to be
successful against
blowﬁsh
Differential attack,
related key attack
Improved related
key
Boomerang attack
Level of
security
Highly secure
Secure
Secure
Possible keys
232, 2448
2128, 2192, 2256
2256, 2512, 21024
Time requires
to check all
possible keys
For a 448 bit 10116 year
Breaks 6 rounds out of
16 of the 256-bit key
version using 2256 steps
For 512 bit and 33
round
Time complexity is
2355.5
Parameter
Camellia
ECC
SAFER
Development
By Mitsubishi Electric
and NTT in 2000
Victor Miller from IBM
and Neil Koblitz in 1985
By Massey in 1993
Key length
(Bits)
128, 192 or 256 bits
Smaller but effective key
(example -512 bit)
For SAFER K-64,
64 bit
For SAFER
K-128, 128 bit
Rounds
18 or 24
1
4.75
Block sizes
(Bits)
128 bits
Stream size is variable
64
Attack found
In future, algebraic
attack, such as
Extended Sparse
Linearization
Doubling attack
Linear
cryptanalytic
attack
Level of
security
Secure
Highly secure
Secure
Possible keys
2128, 2192, 2256
2512
264, 2128
Time requires
to check all
possible keys
–
For 512 bit, 3 × 104
MIPS-years
–
Traditional and Hybrid Encryption Techniques: A Survey
245

4
Conclusion
Data conﬁdentiality is very important during transmission of information from
client to server. Privacy is achieved with creating password of our ﬁle or generate
code word of information (which ready to transmit), that easily understandable by
those users who wants to share it. It is possible through information encryption.
This paper presented explains encryption techniques available and used, which
have strengths in term of security and computational performance. Like, Combined
Concept of DES and RSA paper presented here but, can also use AES-RC4,
SERPENT-RC4 and RC4-AES-SERPENT), using the same packet size of text data,
sample ranging from (1 KB to 30 MB). But DSA-RSA hybrid technique is faster
than other hybrid technique and in terms of throughput DSA-RSA is 54% better
than
RC4-AES-SERPRNT
and
68%
better
than
both
AES-RC4
and
SERPENT-RC4 [1]. In RSA-based singular cubic curve with AVK concept,
reduces time complexity and provides comprehensive system functionality to be
applicable in high-level security application domain. But in double encryption, this
technique is best for small text or content. It is not suited for large content. The
concept of AES-ECC technique reduces time complexity as expected and space
complexity also less rather than other algorithm [3]. This hybrid methodology is
more secure because ECC is robust technology which provides better security than
others. Concept of compression of plaintext ﬁrst and then encryption is good to
provide more security, and it is best in term of ﬁle size and encryption execution
time [7]. Quantum encryption concept mentioned in the presented paper is a better
concept in terms of key distribution, unhackable, and less resources needed. Cur-
rently, signal is limited to 90 miles only comparison section evaluates some
algorithms and techniques. From Table 1, it can be concluded that blowﬁsh is good
to encrypt data rather than AES, DES, and IDEA. And from Table 2 and Fig. 4,
Compression then encryption technique is better than other current techniques in
term of encryption execution time and performance of security. In future, more
efﬁcient algorithm can be developed, which uses both Compression and encryption
(CE) technique along with quantum encryption concept which can withstand with
modern high computing machines performing cryptanalysis (Fig. 5).
Fig. 4 RSA-based singular cubic curve with AVK [6]
246
P. Dixit et al.

Acknowledgements I am thankful to my all professors who helped me to provide material
regarding encryption technique and their valuable discussions hours.
References
1. Adedeji Kazeem B., Ponnle Akinlolu.: A New Hybrid Data Encryption and Decryption
Technique to Enhance Data Security in Communication Networks: Algorithm Development.
In: International Journal of Scientiﬁc & Engineering Research, Vol. 5, Issue 10, October
(2014).
2. Sourabh Chandraa., Bidisha Mandalb., Sk. Saﬁkul Alamc., Siddhartha Bhattacharyya.:
Content based double encryption algorithm using symmetric key cryptography. In:
International Conference on Recent Trends in Computing (2015).
3. Sridhar C. Iyera., R.R. Sedamkarb., Shiwani Gupta.: A Novel Idea on Multimedia Encryption
using Hybrid Crypto Approach. In: 7th International Conference on Communication,
Computing and Virtualization (2016).
4. Md. Atiullah Khan., Kailash Kr. Mishra., N. Santhi., J. Jayakumari.: A New Hybrid
Technique for Data Encryption. In: Proceedings of 2015 Global Conference on Communi-
cation Technologies (2015).
5. WU Xing-hui.: Research of the Database Encryption Technique Based on Hybrid
Cryptography. In: International Symposium on Computational Intelligence and Design.
(2010).
6. Kalpana Singh.: Selective encryption technique in RSA based singular cubic curve with AVK
for text based documents: Enhancement of Koyama approach. Deakin University, Dept. of
Computer. Sci. & Eng., Motilal Nehru Nat. Inst. of Technol., Allahabad, India., DOI:10.1109/
ICNIT.2010.5508497 Conference: Networking and Information Technology (ICNIT).,
International Conference on Source: IEEE Xplore (2010).
7. Nur Nabila Mohamed., Habibah Hashim., Yusnani Mohd Yussoff.: Compression and
Encryption Technique on Securing TFTP Packet. In: IEEE Symposium on Computer
Applications & Industrial Electronics (ISCAIE), Penang, Malaysia, April (2014).
8. Mehrdad. S. Sharbaf.: Quantum Cryptography: A New Generation of Information Technol-
ogy Security System: http://ieeexplore.ieee.org/xpl/freeabsall?arnumber=5070885 (2009).
9. Pradeep H Kharat.: A secured Transmission of data using 3D chaotic map encryption and data
hiding technique. In: International Conference on Industrial Instrumentation and Control
(ICIC) College of Engineering Pune, India. May (2015).
10. Hadia M.S. El Hennawy., Alaa E.A. Omar b., Salah M.A. Kholaif.: LEA: Link Encryption
Algorithm Proposed Stream Cipher Algorithm. In: 2014 Production and hosting by
Elsevier B.V. on behalf of Ain Shams University (2014).
35%
25%
38%
32%
30%
Performance of Technique
Hybrid Technique
Double Encryption
CE
RSA with AVK
CBE
Fig. 5 Security performance
of some technique
Traditional and Hybrid Encryption Techniques: A Survey
247

11. Milind Mathur, Ayush Kesharwani, “Comparison Between Des, 3DES, RC2, RC6, Blowﬁsh
And AES”. In Proceedings of National Conference on New Horizons in IT - NCNHIT 2013,
ISBN 978-93-82338-79-6.
12. Rajdeep Bhanot, Rahul Hans, “A Review and Comparative Analysis of Various Encryption
Algorithms”. In International Journal of Security and Its Applications, Volume 9,
No. 4 (2015), pp. 289–306.
248
P. Dixit et al.

A Comparative Study of Recent Advances
in Big Data for Security and Privacy
Ahlam Kourid and Salim Chikhi
Abstract Big data is coming with new challenges in security; it involves the three
aspects of security conﬁdentiality, availability, integrity, and privacy. These chal-
lenges are due to the 5V data characteristics in big data domain: velocity, variety,
volume, value, and veracity depending on different levels of security: network, data,
application, and authentication. Furthermore, big data is also promising security.
The huge amount of data provides a more security information like data logs. Big
data analysis can be applied to security. Many theories for big data security have
been proposed in literature, covering the different aspects of security and privacy.
Recently, different schemes and frameworks were introduced to reach high level of
security in big data, based on different security theories. In this paper, we discuss
different challenges in big data for security and privacy. Moreover, recent security
theories and works in this ﬁeld have been introduced. A comparative study of latest
advances in big data for security and privacy is presented. Finally, some future
directions for big data for security and privacy are discussed.
Keywords Big data ⋅Security ⋅Privacy ⋅Cryptography ⋅Access control
Intrusion detection ⋅Encryption
1
Introduction
Nowadays, big data plays an important role in several areas of research and
management such as Internet transactions, social networks, retail trade, and
health care. The huge amount of data generated by the enterprises and associations
related to these domains could not be analyzed with traditional approaches and
A. Kourid (✉) ⋅S. Chikhi
Computer Science Department, MISC Laboratory,
College of NTIC, Constantine 2 University – A. Mehri, 25000 Constantine, Algeria
e-mail: ahlam.kourid@univ-constantine2.dz
S. Chikhi
e-mail: salim.chikhi@univ-constantine2.dz
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1_23
249

applications. For that, the attention of many researches and managers has turned to
big data analysis. Big data analysis tools and applications aim to analyze data with
scalability and better performance. However, big data comes with new challenges
concerning security, which cannot be covered with traditional security methods and
tools.
Challenges
involve
several
levels:
network,
data,
application,
and
authentication.
Network level: We outline ﬁrst different problems of network security in big
data. The protocol SSL (Transport Layer Security) is one of the main solutions
which ensure network security usually used in client–sever model in order to
establish a secure connection between client and server. SSL is a standard security
technology for establishing an encrypted link between a server and a client [1].
However, SSL consumes a lot of processing resources on the server for encryption
and decryption of secure data. To track this drawback of client–sever model, a
load-balancing approach for scaling Secure Sockets Layer (SSL) is proposed [2].
One limitation of this approach is that the load balancers can not see any data, such
as cookies or URL, inside the SSL trafﬁc because the entire payload is encrypted.
This limits the load balancers in trafﬁc—redirection and intelligent—switching
capabilities for SSL trafﬁc [2]. For this purpose, to attend high security with efﬁ-
cient processing in network for big data, thinking for new architectures to avoid
these problems is very interesting. Another point in network security is using
Internet Protocol like IPv6 for network applications logging by using IP address,
but this protocol is ﬁnitely scalable [3]. Therefore, the need of new architecture of
network and computers that are inﬁnitely scalable presents a great area of research
for future big data security.
Data level: Challenges at data level involve different components of information
security. For integrity, detecting malicious data with huge amount of data is very
hard. Furthermore, the analysis of data which gathered from diverse sources in
which one or more sources contain malicious data can affect results (unexpected
results). Another case where integrity is not guaranteed, when diverse sources of
data are distributed over diverse nodes, a failure of each node can affect analysis of
these data on other nodes. Therefore, these analysis can produce malicious results.
As big data contains a large number of user information in the data, this makes a big
risk for big data privacy. For big data conﬁdentiality, volume and variety of data in
which we have structured and instructed data are a main challenge for traditional
cryptography approaches. For example, the encryption of spatial data needs great
resources of computation.
Application level: Main challenges at application level concern integrity.
Detecting malicious codes in such applications running on big data platform is very
hard. An efﬁcient access control and authentication system for mappers and
reducers in Mapreduce is required [4]. Furthermore, a failure of the execution of
such distributed network applications and services affects availability.
Authentication and access control level: Challenges belong to two categories:
authentication and access control at application level and authentication and access
control at data level. For the ﬁrst, it is very difﬁcult to establish an access control
scheme for applications running on big data platform. A case of study: how to design
250
A. Kourid and S. Chikhi

an efﬁcient access control and authentication system for mappers and reducers in
MapReduce is required. For the second, an access to data by great number of users
makes difﬁcult to attribute and control privileges of this huge number of users.
The remainder of this paper is organized as follows. Section 2 introduces dif-
ferent security theories for big data. Section 3 discusses recent works of research in
big data for security and privacy that are summarized in two directions: security and
privacy for big data and big data for security and privacy. In Sect. 4 a, comparative
study of recent advances in big data for security and privacy is presented. Some
future directions for big data for security and privacy are discussed in Sect. 5.
Finally, a conclusion and future work are drawn in Sect. 6.
2
Recent Security Theories and Practices for Big Data
In literature, many security theories and practices have been introduced by
researches in order to cover the three aspects of big data security conﬁdentiality,
availability, integrity and privacy [5], especially in a cloud environment as shown in
Fig. 1.
2.1
Conﬁdentiality
An intuitive way to achieve conﬁdentiality protection for big data is to use
encryption. Classical encryption algorithms are used to ensure cryptography for
general purpose processing (e.g., symmetric encryption algorithm, AES, and
asymmetric encryption algorithm, RSA). But they are limited and cannot provide
data processing. Therefore, homomorphic encryption technique is developed to
solve this problem, which can achieve conﬁdentiality protection and data pro-
cessing at the same time [6]. One drawback of these techniques of encryption is that
they still need an expensive computation.
Fig. 1 Big data for security and privacy approaches
A Comparative Study of Recent Advances …
251

Other encryption algorithm such as classical public key approach and
attribute-based encryption approach are proposed for data sharing. The main dis-
advantage of these algorithms is the high cost computation on the owner side. To
overcome this limit, authors in [7, 8] presented proxy re-encryption that is cheaper
and ﬂexible [6]. Proxy re-encryption assumes that the cloud is semi-trusted. The
main idea of this algorithm is as follows: First, data owner encrypts the data with
public key, after for each potential receiver; the data owner generates a re-encryption
key. If such receiver is authorized to share data with the owner, then the cloud
re-encrypt the ciphertext and sends it to the designed receiver for decryption.
2.2
Availability
Denial-of-service (DoS) attack is the main risk that affects availability, by making a
service or network resource unavailable; recently, many researches focus on net-
work security analysis using big data technology, for the purpose to detect DoS
attack. Yogeshwar et al. [9] have proposed a new method for attack detection based
on Hive queries.
2.3
Integrity
Assuring integrity protection for big data is usually considered as a main challenge.
Classical digital signature in the environment of big data is proposed, but it is not
scalable, untraceable, and based on strong assumption [6]. Other solutions are
introduced to tackle this issue such as HMAC/CMAC and homomorphic signature.
In HMAC/CMAC if one party in the system is involved, it will share secret keys
with other parties, and this leads to a big risk if these parties reveal this secret key.
Homomorphic signature depends on the security of nodes in the cloud system [6].
2.4
Privacy
Privacy plays more important role in several ﬁeld like health care, and with the
coming of big data, privacy protection becomes more critical. Many privacy models
have been proposed in literature for big data privacy protection including
anonymization and differential privacy. Anonymization refers to de-identiﬁcation of
personal data. There are many classical models to fulﬁl anonymization over data,
such as—k-anonymity model, and e-diversity model which come to address some
weakness of k-anonymity [10], but none of them is sufﬁcient for big data because
they suffer from scalability issues. Differential privacy is a strong model that is
deﬁned for data privacy protection [11]. And it has two properties that are useful for
252
A. Kourid and S. Chikhi

big data, sequential composition and parallel composition [6]. Recently, some other
works are done in the area of big data for security and privacy to assure a high level
of security for the two directions: security and privacy for big data and big data for
security and privacy, based on different security theories.
3
Big Data for Security and Privacy
Big data comes with new challenges in security and privacy. However, big data also
provides new trends for security and privacy; in this section, we present recent
works in the tow ﬁelds: security and privacy for big data and big data for security
and privacy.
3.1
Security and Privacy for Big Data
With the coming of big data, assuring security for huge amount of data becomes a
great challenge in big data. Traditional techniques and approaches are inadequate
since they are designed to secure a small-scale data and do not respect the 5V of big
data. Recently, some works and researches are focused on security and privacy for
big data; they proposed different methods and solutions that meet different criteria
of security: conﬁdentiality, availability, and integrity, and privacy. In [12, 13],
authors proposed an architecture of a hybrid cloud, composed of public cloud and
private cloud. After data query from users, the private cloud stores sensitive data
after processing and then sends nonsensitive data to public cloud. This architecture
aims to achieve image data privacy via hybrid cloud and reduce time of compu-
tation by dividing image on blocks and operate on these blocks, so that suitable for
big data.
An extension of big data technology framework is proposed by Huang and Du
[14] that is based on a security architecture. This security architecture is divided
into the pre-ﬁltering layer and the post-ﬁltering layer. The pre-ﬁltering layer is the
ﬁrst privacy layer of the proposed architecture. It extracts and deletes personal
sensitive information form the collected data and stores them in the matching
database system DB. The post-ﬁltering layer ﬁlters and removes sensitive infor-
mation synthesized after the big data analysis and stores them in matching DB.
Some other works focused on how to secure sensitive data sharing on a big data
platform.
Authors in [15] introduced a systematic framework for secure sensitive data
sharing on a big data platform. This framework is composed of three components:
security submission, security storage, and security use. The basic ﬂow of the
framework is as follows: First personnel sensitive data are submitted to big data
platform using security plug-in. After that, data stored in big data platform are
encrypted with proxy re-encryption, and then, cloud platform service providers who
A Comparative Study of Recent Advances …
253

want to share the sensitive information download and decrypt the corresponding
data from big data platform, in the private process space based on VMM using the
secure plug-in. Finally, a secure mechanism is applied to destroy used data still
stored temporarily in the cloud.
Another solution for securing sensitive data sharing on a big data platform is
presented by Liu et al. [16]. They proposed an improved HABE construction with
outsourced decryption that overcomes the limit of the decryption cost at the user
side that is still very high in traditional HABE. The main idea of the proposed
HABE is decrypting partially the ciphertext by the cloud after receiving the key
outsourced from the user. However, other researches are interested in providing
access control (AC) systems for big data. Authors in [17] presented an access
control scheme for big data processing. The global structure of this scheme is as
follows: MS (Master System) categorizes CS (Cooperated System) by the security
classes according to the SAs deﬁned with BD source providers and managed MSP
that speciﬁes a set of AC rules that are imposed by MS to enforce AC on CSs. CS
managed CSP that allows the CS to control the access to the distributed BD
data/process by considering the processing capabilities and security requirements of
the CS. An FAD list is deﬁned as federated dictionary of AC attributes that should
be syntactically and semantically agreed by the MS and CSs.
Recent researches develop different solutions for big data privacy based on
differential privacy which is a strong model for data privacy protection [6]. A dif-
ferential privacy protection scheme for big data in body sensor network is intro-
duced by Lin et al. [18], based on the concept of dynamic noise thresholds. The
interference threshold is calculated for each data arrival in order to add noise to
data. Furthermore, the same authors proposed another scheme based on differential
privacy theory [19]. Haar Wavelet transform method is used to convert histogram
into a complete binary tree, for the purpose of reducing errors.
Moreover, big data models like MapReduce and its framework Hadoop are built
without any secure assumption, and new tools are developed for the purpose of
securing Hadoop such as Kerberos mechanism which is used to enhance the
security in HDFS [20]. In addition, Apache Accumulo that allows multi-level
access control at the cell level in a key-value store [21].
3.2
Big Data for Security and Privacy
Big data comes with new trends for security and privacy. Big data analytics is a
large scale analysis applied to a huge amount of structured an uninstructed data, for
that, big data becomes an important research area in security [22]. Moreover, big
data provides a great amount of information like data logs. These logs are gathered
from trafﬁc network, applications, data, and users. So that searching for a corre-
lation between these logs can be a key for detecting malicious codes and activities
in security.
254
A. Kourid and S. Chikhi

Many research works focused on analyzing security log system using big data.
They proposed methods to ﬁlter and analyze logs system. In [23], Jeon et al.
proposed intelligent information analysis platform for system construction of
security log analysis using big data which is composed by collecting, saving,
processing, and analyzing techniques. This architecture aims to analyze the rela-
tionship between security and data events created from network, system, and
application service of main IT infrastructure. In other work [24], authors outline that
the user behaviors are dynamic which is difﬁcult to capture the users’ compre-
hensive behaviors in a single device by capturing or collecting the static dataset.
Therefore, they proposed a log analysis system which is based on the Hadoop
distribution platform to capture the trafﬁc and analyze the user and machine
behaviors, in terms of the search keywords, user shopping trends, Web site posts
and replies, and Web-visited history to acquire the user’ s dynamic behaviors.
Big data turns the way on the architecture and design of networks. Traditional
networks are inadequate for managing and processing big data and do not respect
the 5V of big data. They are composed by a lot of layers and need an expensive
computation over these layers. Therefore, new technologies have emerged.
Recently, software-deﬁned networking (SDN) has attracted great interest as a new
paradigm in networking [25]. It is composed of fewer layers which make it more
ﬂexible and efﬁcient for big data.
The main idea of SDN is to detach the control plane from the forwarding plane,
to break vertical integration, and to introduce the ability to program the network
[25]. So that from security point of view, the 5Vs of big data demand ultra-fast
response times from security and privacy solutions/products [26].
Furthermore, providing integrity for security implies not only detecting mali-
cious code and data, but also intervening in the right moment, due to the ﬂexibility
of this new architecture of network. For this purpose, SDN is a suitable paradigm in
networking that beneﬁts security and privacy in big data.
4
Comparative Study of Recent Trends in Big Data
for Security and Privacy
In this section, we highlight a comparative study of different researches in the area
of big data for security and privacy. We design for each trend the criteria of security
and privacy achieved by the solution proposed, its advantages, and the main
drawbacks and limits that decrease the performance of the research presented
(Table 1).
A Comparative Study of Recent Advances …
255

Table 1 Comparative study of recent works in big data for security and privacy
Paper
Criteria of security and
privacy
Advantages
Disadvantages
Jeon et al.
[23]
• Integrity: intrusion
detection (Auditing)
• Ex: detecting
modiﬁcation in
database of users
information using
logs
• New security log system
using big data that aims to
analyze different types of
logs from different
sources: network,
applications, system, etc.
• Possibility of real-time
monitoring: process logs
in real time can help stop
attacks before their occurs
• It does not offers any
solutions for assuring
security and integrity for
log database collected, and
for different process of the
system
• Based on Hadoop which is
not secure
• It does not provides any
optimization process for
logs analysis process that
affects all logs, which
increase time of execution
Sathe et al.
[24]
• Integrity: intrusion
detection (Auditing)
• Availability: detection
of distributed
denial-of-service
attack
• Fast loading logs data on
Hadoop using Apache
Flume
• Analyzing users and
machine behaviors in big
data environment
Cho et al.
[14]
• Privacy
• The proposed architecture
provides two steps of
masking and protecting
sensitive data, before and
after big data analysis
• A mechanism for ensuring
security and integrity of
matching DB and data
source is required
• A case where big data
analysis involves sensitive
information that must be
stored in ﬁle big data
system is not taken into
consideration
Hu et al.
[17]
• Conﬁdentiality: access
control
• Distributed scheme
establishing mechanism of
policies and rules,
between MS and CS for
access control in big data
environment
• Any mechanism is
deﬁned for assuring
conﬁdentiality and
integrity for the list of
trust CSs (TCSL), and
federated AC attributes
deﬁnitions FAD is deﬁned
• Untrusted user in CS can
be seen as valid user, and
access to all data deﬁned
by MSP and CSP policy,
for that control access to
CS is needed
Dong et al.
[15]
• Conﬁdentiality:
general cryptography,
cryptography for data
sharing (access
control)
• Privacy: using Virtual
Machine Monitor
(VMM)
• The framework protects
the security of the
full-sensitive data life
cycle: submission,
storage, use on a big data
platform
• Security level of the
framework is linked
directly to the security
level of virtual machine
monitoring VMM,
because clear text is stored
in private memory space
after decrypting of PRE -
ciphertext on VMM
(continued)
256
A. Kourid and S. Chikhi

5
Future Directions for Big Data for Security and Privacy
In this section, we highlight some future directions for big data for security and
privacy:
• Assuring intrusion detection system inside Hadoop platform by assigning digital
signatures for big data applications, in order to detect malicious code, especially
for mappers and reducers in MapReduce.
• Development of novel abstracts and uniﬁed models more strong for the purpose
to ensure security and privacy, because big data platforms include a variety of
applications and solutions for big data security analysis which make opportu-
nities for hackers.
Table 1 (continued)
Paper
Criteria of security and
privacy
Advantages
Disadvantages
Liu et al.
[16]
• Conﬁdentiality:
cryptography for data
sharing (access
control)
• Reduces the computational
overhead at the user side,
by decrypting partially the
ciphertext by the cloud
• In ABE, the cost of
generating decryption key
is high, and the owner
needs to spend a lot of
time when the number of
receivers is relatively large
[6]. Decryption process in
ABE and HABE is the
same. HABE suffers from
the same drawback
Huang and
Du [12,
13]
• Privacy
• Hybrid cloud for achieving
image data privacy with
low cost on big data
platform
• This approach protects
data privacy from outside
(users of public cloud) and
not from inside (users of
private cloud) where
sensitive data are stored
Lin et al.
[18, 19]
• Privacy
• Based on differential
privacy theory, when
attacker has full
knowledge of the
background, he cannot
target a certain people
• For each data arrival,
interference threshold is
calculated to make the
scheme suitable for big
data
• In [19], errors are reduced
by using the Haar Wavelet
transformation method
that converts histograms
to binary tree
• Proposed method produces
errors even very small, but
it can make risk, especially
for decision-making
process using sensitive
data like health data
A Comparative Study of Recent Advances …
257

• Security secret is in data themselves, making hierarchical barriers of complex
data generated by complex and sophisticated models in order to protect and hide
sensitive data from hackers.
• Deﬁning a new secure MapReduce model which will be more strong for big data
security and privacy than only providing solutions for Hadoop security, because
it will offer a high level of security than Hadoop so that changing platforms of
MapReduce cannot affect security.
• Storage of big data on cloud enhance privacy, the main solution for that is
ﬁrstly, develop new applications for data encryption, and secondly develop
another range of analytics applications that aim to process this data and provide
encrypted results that only be decrypted on the site of users.
• Adding layers on Hadoop that control behaviors of all applications based on
Hadoop.
• Promising security using big data, by developing new frameworks for analyzing
the integration of all logs of different levels in real time: network trafﬁc, logging,
and applications, for the purpose to intervene in the right moment.
6
Conclusion
In this paper, the main challenges in the area of big data for security and privacy
have been introduced. We discuss also different security theories for big data.
Moreover, a comparative study of recent researches on big data for security and
privacy is presented. The information security criteria are achieved, and different
advantages and limits of the solutions proposed by theses works have been deﬁned.
We conclude that recent works on big data security are improved but still suffer
from such limits that decrease their performance and are designed for a particular
purpose. Therefore, development of abstract and uniﬁed models and frameworks
for big data security that ensure all criteria of security is very interest. In future, we
aim to propose and design an optimization framework for big data security analysis.
References
1. VG Savant. Approaches to Solve Big Data Security Issues and Comparative Study of
Cryptographic Algorithms for Data Encryption. International Journal of Engineering Research
and General Science, 2015.
2. Chandra Kopparapu. Load Balancing Servers, Firewalls, and Caches. Wiley, 2002.
3. DH Ackley. Indeﬁnite Scalability for Living Computation. Association for the Advancement
of Artiﬁcial Intelligence, 2016.
4. S Rajan, et al. Top ten big data security and privacy challenges, Cloud Security Alliance,
2012.
5. A Agarwal, A Agarwal, The Security Risks Associated with Cloud Computing, International
Journal of Computer Applications in Engineering Sciences, 2011.
258
A. Kourid and S. Chikhi

6. L Xu and W Shi, Security Theories and Practices for Big Data, Big Data Concepts, Theories,
and Applications. Springer, 2016.
7. M Blaze et al. Divertible protocols and atomic proxy cryptography, Springer, 1998.
8. M Mambo, E Okamoto. Proxy cryptosystems: delegation of the power to decrypt ciphertexts.
IEICE Trans Fundam Electron Commun Comput Sci E80-A:54–63, 1997.
9. YR Bachupally et al. Network Security Analysis Using Big Data Technology, IEEE, 2016.
10. DS Terzi et al. A Survey on Security and Privacy Issues in Big Data. IEEE, 2015.
11. C Dwork. Differential privacy. Springer,2006.
12. K Shirudkar, D Motwani, Big-Data Security, International Journal of Advanced Research in
Computer Science and Software Engineering, 2015.
13. X Huang and X Du Achieving Big Data Privacy Via Hybrid Cloud, IEEE, 2014.
14. DE Cho, et al. Double Privacy Layer Architecture for Big Data Framework, International
Journal of Software Engineering and Its Applications, 2016.
15. X Dong, et al. Secure Sensitive Data Sharing on a Big Data Platform, Tsinghua Science and
Technology. IEEE, 2015.
16. Z Liu et al. An Improved Cloud Data Sharing Scheme with Hierarchical Attribute Structure,
Journal of Universal Computer Science, 2015.
17. VC Hu, et al. An Access Control Scheme for Big Data Processing, Collaborative Computing:
Networking, Applications and Worksharing, 2014 International Conference. IEEE, 2014.
18. C Lin, ET al. Differential Privacy Preserving in Big Data Analytics for Connected Health,
Journal of Medical Systems. Springer, 2016.
19. C Lin, ET al. A differential privacy protection scheme for sensitive big data in body sensor
networks, annals of Telecommunications. Springer, 2016.
20. B. Saraladevi et al. Big Data and Hadoop-A Study in Security Perspective, Elsevier, 2015.
21. H Ulusoy, et al. Vigiles: Fine-grained Access Control for MapReduce Systems, Congress on
Big Data. IEEE, 2014.
22. AA. Cárdenas et al. Big Data Analytics for Security, IEEE Security & Privacy, 2013.
23. KS Jeon et al. A Study on the Big Data Log Analysis for Security, International Journal of
Security and Its Applications, 2016.
24. SS Sathe, et al. Deploying and researching hadoop algorithms on virtual machines and
analyzing log ﬁles, International Education & Research Journal, 2015.
25. L Cui, et al. When big data meets software deﬁned networking: SDN for big data and big data
for SDN, IEEE Network, 2016.
26. P Verma, et al. Network Security in Big Data: Tools and Techniques, Springer, 2016.
A Comparative Study of Recent Advances …
259

Author Index
A
Alka, 15
Ansar, Syed Anas, 15
B
Bansal, Nidhi, 41
C
Chawla, Dimple, 127
Chikhi, Salim, 249
Choudhary, Sunita, 229
D
Datta, Suchana, 61
De, Debashis, 61
Deshpande, Anand, 137, 173
Dhote, C.A., 93
Dixit, Pooja, 239
G
Goar, Vishal, 229
Gulati, Ravi, 3
Gupta, Aakanshi, 27
Gupta, Avadhesh Kumar, 239
Gupta, Bhupendra, 183
Gupta, Deepika, 229
J
Jaiswal, Swati, 217
K
Kanungo, Suvendu, 207
Khan, Raees Ahmad, 15
Kourid, Ahlam, 249
Kumar, Deepak, 83
Kumar, Hradesh, 103
M
Maind, Ankush, 159
Majumder, Koushik, 49, 61
Mariappan, Gunavathie, 195
Mathur, Shikha, 229
Midya, Sadip, 49
Mukherjee, Saurabh, 145
N
Narwal, Priti, 83
P
Padmanabhan, Jayashree, 195
Patavardhan, Prashant P., 137, 173
Patel, Charmy, 3
Phadikar, Santanu, 49
Porwal, Adisha, 217
Potey, Manish M., 93
Prajapat, Shaligram, 217
Purohit, G.N., 145
R
Raut, Shital, 159
Roy, Asmita, 49
S
Saifee, Fatema, 217
Santra, Palash, 49, 61
Sharma, Deepak H., 93
© Springer Nature Singapore Pte Ltd. 2018
G.M. Perez et al. (eds.), Networking Communication and Data Knowledge
Engineering, Lecture Notes on Data Engineering and Communications
Technologies 4, https://doi.org/10.1007/978-981-10-4600-1
261

Sharma, Manorama, 145
Shukla, Arun Kumar, 207
Singh, Ajay Kumar, 41
Singh, Shailendra Narayan, 83
Suri, Bharti, 27
T
Thakur, R.S., 217
Tripathi, Shailendra Kumar, 183
Trivedi, Munesh Chandra, 127, 239
Y
Yadav, Sanjeev Kumar, 103
Yadav, Virendra Kumar, 239
262
Author Index


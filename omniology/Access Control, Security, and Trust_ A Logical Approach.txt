
Access Control,  
Security, and Trust
A Logical Approach

CHAPMAN & HALL/CRC
CRYPTOGRAPHY AND NETWORK SECURITY
Series Editor
Douglas R. Stinson
Published Titles 
 
Jonathan Katz and Yehuda Lindell, Introduction to Modern 
Cryptography
Antoine Joux, Algorithmic Cryptanalysis
M. Jason Hinek, Cryptanalysis of RSA and Its Variants
Burton Rosenberg, Handbook of Financial Cryptography and Security
Shiu-Kai Chin and Susan Older, Access Control, Security, and Trust: 
A Logical Approach
Forthcoming Titles 
 
Maria Isabel Vasco, Spyros Magliveras, and Rainer Steinwandt, 
Group Theoretic Cryptography

Chapman & Hall/CRC
CRYPTOGRAPHY AND NETWORK SECURITY
Shiu-Kai Chin
Syracuse University
Syracuse, New York, USA
Susan Older
Syracuse University
Syracuse, New York, USA
Access Control,  
Security, and Trust
A Logical Approach

About the cover: The cover image of a mother loon carrying her chick across the water depicts the interde-
pendent nature of this book’s main themes: access-control, security, and trust. Loons are fiercely protective 
of their offspring. In turn, the chicks often ride on their parents’ backs, trusting them to provide both warmth 
and protection from predators.
Chapman & Hall/CRC
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2011 by Taylor and Francis Group, LLC
Chapman & Hall/CRC is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-13: 978-1-58488-863-5 (Ebook-PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts 
have been made to publish reliable data and information, but the author and publisher cannot assume 
responsibility for the validity of all materials or the consequences of their use. The authors and publishers 
have attempted to trace the copyright holders of all material reproduced in this publication and apologize to 
copyright holders if permission to publish in this form has not been obtained. If any copyright material has 
not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmit-
ted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, 
including photocopying, microfilming, and recording, or in any information storage or retrieval system, 
without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.
com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood 
Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and 
registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, 
a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used 
only for identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com 

To Linda, Benjamin, Emily, and my mom for their love and support
To Garth, for his patience; and to Ryan, who couldn’t wait for this book to be
completed


Contents
List of Tables
xiii
List of Figures
xv
Preface
xix
1
Access Control, Security, Trust, and Logic
1
1.1
Deconstructing Access-Control Decisions . . . . . . . . . . . . . .
3
1.2
A Logical Approach to Access Control
. . . . . . . . . . . . . . .
6
I
Preliminaries
9
2
A Language for Access Control
11
2.1
Sets and Relations
. . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1.1
Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.1.2
Approaches for Mathematical Proofs
. . . . . . . . . . . .
13
2.2
Syntax
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2.1
Principal Expressions . . . . . . . . . . . . . . . . . . . . .
17
2.2.2
Access-Control Statements . . . . . . . . . . . . . . . . . .
18
2.2.3
Well-Formed Formulas . . . . . . . . . . . . . . . . . . . .
20
2.3
Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.3.1
Kripke Structures . . . . . . . . . . . . . . . . . . . . . . .
23
2.3.2
Semantics of the Logic . . . . . . . . . . . . . . . . . . . .
28
2.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3
Reasoning about Access Control
39
3.1
Logical Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.1.1
The Taut Rule . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.1.2
The Modus Ponens Rule . . . . . . . . . . . . . . . . . . .
42
3.1.3
The Says Rule
. . . . . . . . . . . . . . . . . . . . . . . .
42
3.1.4
The MP Says Rule . . . . . . . . . . . . . . . . . . . . . .
42
3.1.5
The Speaks For Rule . . . . . . . . . . . . . . . . . . . . .
43
3.1.6
The & Says and Quoting Rules
. . . . . . . . . . . . . . .
43
3.1.7
Properties of ⇒. . . . . . . . . . . . . . . . . . . . . . . .
43
3.1.8
The Equivalence Rule
. . . . . . . . . . . . . . . . . . . .
45
vii

viii
3.1.9
The Controls Deﬁnition
. . . . . . . . . . . . . . . . . . .
46
3.2
Formal Proofs and Theorems . . . . . . . . . . . . . . . . . . . . .
47
3.3
Soundness of Logical Rules
. . . . . . . . . . . . . . . . . . . . .
50
3.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4
Basic Concepts
57
4.1
Reference Monitors
. . . . . . . . . . . . . . . . . . . . . . . . .
57
4.2
Access-Control Mechanisms: Tickets and Lists . . . . . . . . . . .
60
4.2.1
Tickets
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.2.2
Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
4.2.3
Logical and Pragmatic Implications . . . . . . . . . . . . .
66
4.3
Authentication
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.3.1
Two-Factor Authentication . . . . . . . . . . . . . . . . . .
68
4.3.2
Using Credentials from Other Authorities . . . . . . . . . .
70
4.3.3
Groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
76
5
Security Policies
77
5.1
Conﬁdentiality, Integrity, and Availability . . . . . . . . . . . . . .
77
5.2
Discretionary Security Policies . . . . . . . . . . . . . . . . . . . .
79
5.3
Mandatory Security Policies
. . . . . . . . . . . . . . . . . . . . .
81
5.4
Military Security Policies
. . . . . . . . . . . . . . . . . . . . . .
85
5.4.1
Extending the Logic with Security Levels . . . . . . . . . .
85
5.4.2
Expressing Military Security Policies . . . . . . . . . . . .
87
5.4.3
Military Security Policies: An Extended Example . . . . . .
90
5.5
Commercial Policies
. . . . . . . . . . . . . . . . . . . . . . . . .
94
5.5.1
Extending the Logic with Integrity Levels . . . . . . . . . .
95
5.5.2
Protecting Integrity . . . . . . . . . . . . . . . . . . . . . .
97
5.5.3
Strict Integrity
. . . . . . . . . . . . . . . . . . . . . . . .
98
5.5.4
An Extended Example of a Strict Integrity Policy . . . . . .
100
5.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
5.7
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
105
II
Distributed Access Control
107
6
Digital Authentication
109
6.1
Public-Key Cryptography
. . . . . . . . . . . . . . . . . . . . . .
109
6.2
Efﬁciency Mechanisms . . . . . . . . . . . . . . . . . . . . . . . .
112
6.2.1
Cryptographic Hash Functions . . . . . . . . . . . . . . . .
112
6.2.2
Data-Encryption Keys
. . . . . . . . . . . . . . . . . . . .
113
6.2.3
Digital Signatures
. . . . . . . . . . . . . . . . . . . . . .
113
6.3
Reasoning about Cryptographic Communications . . . . . . . . . .
114

ix
6.4
Certiﬁcates, Certiﬁcate Authorities, and Trust . . . . . . . . . . . .
116
6.5
Symmetric-Key Cryptography
. . . . . . . . . . . . . . . . . . . .
125
6.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
6.7
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
131
7
Delegation
133
7.1
Simple Delegations . . . . . . . . . . . . . . . . . . . . . . . . . .
133
7.2
Delegation and Its Properties . . . . . . . . . . . . . . . . . . . . .
135
7.3
A Delegation Example: Simple Checking
. . . . . . . . . . . . . .
141
7.3.1
Formal Deﬁnitions of Checks
. . . . . . . . . . . . . . . .
142
7.3.2
Bank Policies on Checks . . . . . . . . . . . . . . . . . . .
143
7.3.3
Operating Rules for Checks
. . . . . . . . . . . . . . . . .
144
7.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
7.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
147
8
Networks: Case Studies
149
8.1
SSL and TLS: Authentication across the Web . . . . . . . . . . . .
149
8.1.1
Handshake Protocol
. . . . . . . . . . . . . . . . . . . . .
150
8.1.2
Record Protocol
. . . . . . . . . . . . . . . . . . . . . . .
155
8.2
Kerberos: Authentication for Distributed Systems . . . . . . . . . .
157
8.2.1
Initial Authentication Requests . . . . . . . . . . . . . . . .
157
8.2.2
Requests for Service-Speciﬁc Tickets . . . . . . . . . . . .
159
8.2.3
Requests for Services . . . . . . . . . . . . . . . . . . . . .
161
8.2.4
Proxiable Tickets . . . . . . . . . . . . . . . . . . . . . . .
162
8.3
Financial Networks . . . . . . . . . . . . . . . . . . . . . . . . . .
166
8.3.1
Electronic Clearinghouses . . . . . . . . . . . . . . . . . .
166
8.3.2
Bank Authorities, Jurisdiction, and Policies . . . . . . . . .
169
8.3.3
Bank Operating Rules
. . . . . . . . . . . . . . . . . . . .
170
8.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
8.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
173
III
Isolation and Sharing
175
9
A Primer on Computer Hardware
177
9.1
Ones and Zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
9.2
Synchronous Design
. . . . . . . . . . . . . . . . . . . . . . . . .
178
9.2.1
Synchronous Registers . . . . . . . . . . . . . . . . . . . .
178
9.2.2
Registers with Load Control . . . . . . . . . . . . . . . . .
179
9.2.3
Registers with Tri-State Outputs . . . . . . . . . . . . . . .
179
9.2.4
Combinational Logic and Functions . . . . . . . . . . . . .
182
9.2.5
Arithmetic Logic Units . . . . . . . . . . . . . . . . . . . .
184
9.3
Microcode
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
9.3.1
Data Paths and Control Paths . . . . . . . . . . . . . . . . .
190
9.3.2
Microprogramming . . . . . . . . . . . . . . . . . . . . . .
192

x
9.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
9.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
195
10 Virtual Machines and Memory Protection
197
10.1 A Simple Processor . . . . . . . . . . . . . . . . . . . . . . . . . .
198
10.1.1 Processor Components . . . . . . . . . . . . . . . . . . . .
199
10.1.2 Machine Instructions . . . . . . . . . . . . . . . . . . . . .
201
10.2 Processors with Memory Segmentation
. . . . . . . . . . . . . . .
204
10.2.1 Segmentation Using a Relocation Register . . . . . . . . . .
204
10.2.2 Processor State and Instructions . . . . . . . . . . . . . . .
207
10.2.3 Program Status Word . . . . . . . . . . . . . . . . . . . . .
207
10.2.4 Traps . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
10.3 Controlling Access to Memory and Segmentation Registers
. . . .
209
10.3.1 Access to Program Memory . . . . . . . . . . . . . . . . .
210
10.3.2 Implementation Details . . . . . . . . . . . . . . . . . . . .
212
10.3.3 Access to the Relocation Register . . . . . . . . . . . . . .
213
10.3.4 Setting the Mode Bit . . . . . . . . . . . . . . . . . . . . .
215
10.4 Design of the Virtual Machine Monitor
. . . . . . . . . . . . . . .
217
10.4.1 Privileged Instructions . . . . . . . . . . . . . . . . . . . .
220
10.4.2 Sensitive Instructions . . . . . . . . . . . . . . . . . . . . .
221
10.4.3 Virtualizable Processor Architectures
. . . . . . . . . . . .
223
10.5 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
10.6 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
225
11 Access Control Using Descriptors and Capabilities
227
11.1 Address Descriptors and Capabilities
. . . . . . . . . . . . . . . .
227
11.2 Tagged Architectures . . . . . . . . . . . . . . . . . . . . . . . . .
231
11.3 Capability Systems . . . . . . . . . . . . . . . . . . . . . . . . . .
233
11.3.1 Catalogs
. . . . . . . . . . . . . . . . . . . . . . . . . . .
233
11.3.2 Creating New Segments
. . . . . . . . . . . . . . . . . . .
235
11.3.3 Dynamic Sharing . . . . . . . . . . . . . . . . . . . . . . .
237
11.3.4 Revocation of Capabilities . . . . . . . . . . . . . . . . . .
239
11.4 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
11.5 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
242
12 Access Control Using Lists and Rings
245
12.1 Generalized Addresses . . . . . . . . . . . . . . . . . . . . . . . .
245
12.2 Segment Access Controllers
. . . . . . . . . . . . . . . . . . . . .
247
12.3 ACL-Based Access Policy for Memory Accesses
. . . . . . . . . .
249
12.4 Ring-Based Access Control
. . . . . . . . . . . . . . . . . . . . .
253
12.4.1 Access Brackets
. . . . . . . . . . . . . . . . . . . . . . .
254
12.4.2 Call Brackets . . . . . . . . . . . . . . . . . . . . . . . . .
255
12.5 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
12.6 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
259

xi
IV
Access Policies
261
13 Conﬁdentiality and Integrity Policies
263
13.1 Classiﬁcations and Categories
. . . . . . . . . . . . . . . . . . . .
263
13.2 Bell–La Padula Model, Revisited
. . . . . . . . . . . . . . . . . .
266
13.3 Conﬁdentiality Levels: Some Practical Considerations
. . . . . . .
269
13.4 Biba’s Strict Integrity, Revisited
. . . . . . . . . . . . . . . . . . .
272
13.5 Lipner’s Integrity Model
. . . . . . . . . . . . . . . . . . . . . . .
276
13.5.1 Commercial Integrity Requirements . . . . . . . . . . . . .
277
13.5.2 Commercial Integrity via Bell–La Padula . . . . . . . . . .
277
13.5.3 Commercial Integrity via Bell–La Padula and Strict Integrity 281
13.6 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
13.7 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
285
14 Role-Based Access Control
289
14.1 RBAC Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . .
289
14.1.1 Role Inheritance
. . . . . . . . . . . . . . . . . . . . . . .
290
14.1.2 Sessions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
14.2 Separation of Duty
. . . . . . . . . . . . . . . . . . . . . . . . . .
297
14.2.1 Static Separation of Duty . . . . . . . . . . . . . . . . . . .
297
14.2.2 Dynamic Separation of Duty . . . . . . . . . . . . . . . . .
299
14.3 Representing RBAC Systems in the Logic . . . . . . . . . . . . . .
304
14.3.1 RBAC Extensions to the Logic . . . . . . . . . . . . . . . .
304
14.3.2 Translating RBAC into the Logic
. . . . . . . . . . . . . .
305
14.4 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
14.5 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
312
A Summary of the Access-Control Logic
313
A.1
Syntax
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
A.2
Core Rules, Derived Rules, and Extensions
. . . . . . . . . . . . .
315
Bibliography
321
Notation Index
324
General Index
325


List of Tables
2.1
Truth table for the propositional formula (p∧q) ⊃r . . . . . . . .
23
2.2
State-transition table for ﬁnite-state machine M
. . . . . . . . . .
26
2.3
Truth values of primitive propositions p, q, r, and s in each world .
26
4.1
Example of an access-control matrix . . . . . . . . . . . . . . . .
59
5.1
Access-control matrix for foo under Susan’s control . . . . . . . .
80
5.2
Classiﬁcation level of documents . . . . . . . . . . . . . . . . . .
90
5.3
FX-1 and FX-2 project personnel, functions, and clearances . . . .
91
5.4
FX-1 access matrix
. . . . . . . . . . . . . . . . . . . . . . . . .
92
5.5
FX-2 access matrix
. . . . . . . . . . . . . . . . . . . . . . . . .
92
5.6
Access matrix for vegetarian and non-vegetarian meals
. . . . . .
101
5.7
Integrity levels for subjects and objects . . . . . . . . . . . . . . .
101
9.1
Tri-state data-bus values . . . . . . . . . . . . . . . . . . . . . . .
181
9.2
Register with high-impedance output . . . . . . . . . . . . . . . .
182
9.3
Simple combinational-logic functions . . . . . . . . . . . . . . . .
182
9.4
An arithmetic interpretation of a full adder . . . . . . . . . . . . .
183
9.5
ALU functions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
10.1
Operations for @B ←@A+@B
. . . . . . . . . . . . . . . . . .
202
10.2
Timing and control unit operations . . . . . . . . . . . . . . . . .
202
10.3
Memory-protected operations for LDA @A, ADD @A, and STO @B 213
10.4
LDARR: Reading the contents of relocation register
. . . . . . . .
215
10.5
Timing and control unit operations for LDARR . . . . . . . . . . .
215
11.1
Interpretation of capability-register values
. . . . . . . . . . . . .
229
11.2
Interpretation of tag bit
. . . . . . . . . . . . . . . . . . . . . . .
232
13.1
Conﬁdentiality levels
. . . . . . . . . . . . . . . . . . . . . . . .
267
13.2
Discretionary access-control matrix . . . . . . . . . . . . . . . . .
267
13.3
Integrity levels . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
13.4
Discretionary access-control matrix . . . . . . . . . . . . . . . . .
274
13.5
Access-control matrix for commercial integrity via Bell–La Padula
279
13.6
Conﬁdentiality levels for subjects . . . . . . . . . . . . . . . . . .
279
13.7
Conﬁdentiality levels for objects
. . . . . . . . . . . . . . . . . .
280
13.8
Access-control matrix for both conﬁdentiality and strict integrity
.
281
xiii

xiv
13.9
Conﬁdentiality and integrity levels for objects
. . . . . . . . . . .
283
13.10 Conﬁdentiality and integrity levels for subjects . . . . . . . . . . .
284

List of Figures
2.1
Semantics of core logic, for each M = ⟨W,I,J⟩
. . . . . . . . . .
29
2.2
Learning outcomes for Chapter 2 . . . . . . . . . . . . . . . . . .
38
3.1
Logical rules for the access-control logic . . . . . . . . . . . . . .
40
3.2
Common propositional-logic tautologies . . . . . . . . . . . . . .
41
3.3
A simple formal proof . . . . . . . . . . . . . . . . . . . . . . . .
47
3.4
A formal proof of the Controls rule . . . . . . . . . . . . . . . . .
47
3.5
Some useful derived rules . . . . . . . . . . . . . . . . . . . . . .
49
3.6
A formal proof of Conjunction
. . . . . . . . . . . . . . . . . . .
49
3.7
Learning outcomes for Chapter 3 . . . . . . . . . . . . . . . . . .
55
4.1
Abstract view of reference monitors . . . . . . . . . . . . . . . . .
58
4.2
Formal proof of the Ticket Rule . . . . . . . . . . . . . . . . . . .
62
4.3
Template for two-factor authentication . . . . . . . . . . . . . . .
70
4.4
Template for using credentials from multiple authorities . . . . . .
72
4.5
Learning outcomes for Chapter 4 . . . . . . . . . . . . . . . . . .
76
5.1
Conceptual diagram of a virtual machine and monitor . . . . . . .
82
5.2
Inference rules for relating security levels . . . . . . . . . . . . . .
87
5.3
Proof of ≤s Subst
. . . . . . . . . . . . . . . . . . . . . . . . . .
87
5.4
Proof justifying Jude’s access to status report . . . . . . . . . . . .
94
5.5
Inference rules for relating integrity levels
. . . . . . . . . . . . .
96
5.6
Subjects, objects, domains, and access
. . . . . . . . . . . . . . .
98
5.7
Preparation of vegetarian and non-vegetarian meals
. . . . . . . .
100
5.8
Partial access diagram for gasoline . . . . . . . . . . . . . . . . .
102
5.9
Learning outcomes for Chapter 5 . . . . . . . . . . . . . . . . . .
106
6.1
Process for using public-key encryption for privacy
. . . . . . . .
110
6.2
Process for using private-key encryption for authenticity . . . . . .
111
6.3
Process for creating a digital signature
. . . . . . . . . . . . . . .
114
6.4
Process for verifying a digital signature . . . . . . . . . . . . . . .
114
6.5
Simple analysis of digital signatures
. . . . . . . . . . . . . . . .
115
6.6
Network of certiﬁcate authorities . . . . . . . . . . . . . . . . . .
119
6.7
Learning outcomes for Chapter 6 . . . . . . . . . . . . . . . . . .
132
7.1
Logical rules for delegation . . . . . . . . . . . . . . . . . . . . .
137
7.2
Grace’s health-care proxy . . . . . . . . . . . . . . . . . . . . . .
140
xv

xvi
7.3
Simple checking . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
7.4
Learning outcomes for Chapter 7 . . . . . . . . . . . . . . . . . .
148
8.1
Handshake protocol: Establishing associations among principals
and keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
8.2
Record protocol: Payload construction . . . . . . . . . . . . . . .
155
8.3
Record protocol: Payload integrity checking . . . . . . . . . . . .
156
8.4
Overview of Kerberos protocol . . . . . . . . . . . . . . . . . . .
157
8.5
Interbank checking using the ACH electronic clearinghouse . . . .
168
8.6
Learning outcomes for Chapter 8 . . . . . . . . . . . . . . . . . .
173
9.1
Timing diagram for synchronous registers and memory
. . . . . .
178
9.2
Timing diagram for synchronous registers with load control . . . .
180
9.3
Tri-state buffer . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
9.4
Tri-state bus example
. . . . . . . . . . . . . . . . . . . . . . . .
181
9.5
Register with tri-state output
. . . . . . . . . . . . . . . . . . . .
182
9.6
Basic combinational-logic gates . . . . . . . . . . . . . . . . . . .
183
9.7
Full-adder implementation . . . . . . . . . . . . . . . . . . . . . .
184
9.8
An n-bit adder and ripple adder . . . . . . . . . . . . . . . . . . .
185
9.9
ALU implementation
. . . . . . . . . . . . . . . . . . . . . . . .
186
9.10
Simple data- and control-path example . . . . . . . . . . . . . . .
191
9.11
Learning outcomes for Chapter 9 . . . . . . . . . . . . . . . . . .
194
10.1
Virtual machine monitor protecting memory . . . . . . . . . . . .
198
10.2
A simple processor
. . . . . . . . . . . . . . . . . . . . . . . . .
199
10.3
Timing of CPU operations . . . . . . . . . . . . . . . . . . . . . .
203
10.4
Memory segmentation . . . . . . . . . . . . . . . . . . . . . . . .
205
10.5
Processor model for virtual machines . . . . . . . . . . . . . . . .
206
10.6
Derived inference rules for an operation OP @A . . . . . . . . . .
211
10.7
Simple processor with relocation register . . . . . . . . . . . . . .
214
10.8
Timing diagram for loading RR into ACC
. . . . . . . . . . . . .
216
10.9
Top-level operation of virtual machine monitor . . . . . . . . . . .
219
10.10 Graphic representation of control program for handling trapped in-
structions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
10.11 Learning outcomes for Chapter 10
. . . . . . . . . . . . . . . . .
226
11.1
Memory protection with address descriptors and capabilities . . . .
228
11.2
Catalogs and capabilities
. . . . . . . . . . . . . . . . . . . . . .
234
11.3
Inference rules for MKSEG instruction
. . . . . . . . . . . . . . .
236
11.4
Pairwise shared communications segments . . . . . . . . . . . . .
238
11.5
Formal analysis for pairwise shared communication segment
. . .
238
11.6
Mailbox segments . . . . . . . . . . . . . . . . . . . . . . . . . .
239
11.7
Formal analysis for mailbox communication segment
. . . . . . .
239
11.8
Learning outcomes for Chapter 11
. . . . . . . . . . . . . . . . .
243

xvii
12.1
User view of memory . . . . . . . . . . . . . . . . . . . . . . . .
246
12.2
Address formation for address descriptor . . . . . . . . . . . . . .
247
12.3
Segment access controller . . . . . . . . . . . . . . . . . . . . . .
248
12.4
Data or instruction segment . . . . . . . . . . . . . . . . . . . . .
248
12.5
Generalized address formation and access control
. . . . . . . . .
249
12.6
Virtual machine and virtual machine monitor for ACL system . . .
250
12.7
Protection rings
. . . . . . . . . . . . . . . . . . . . . . . . . . .
253
12.8
Call gates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
12.9
Learning outcomes for Chapter 12
. . . . . . . . . . . . . . . . .
259
13.1
Partial ordering on conﬁdentiality levels
. . . . . . . . . . . . . .
265
13.2
Carol’s request and Kate’s response . . . . . . . . . . . . . . . . .
275
13.3
Learning outcomes for Chapter 13
. . . . . . . . . . . . . . . . .
287
14.1
Sample Hasse diagram
. . . . . . . . . . . . . . . . . . . . . . .
291
14.2
Role hierarchy for a CS/CE department . . . . . . . . . . . . . . .
299
14.3
Logical rules regarding principal equality . . . . . . . . . . . . . .
305
14.4
Formal justiﬁcation to allow Dora to read student grades . . . . . .
308
14.5
Learning outcomes for Chapter 14
. . . . . . . . . . . . . . . . .
311
A.1
Summary of core rules for the access-control logic . . . . . . . . .
316
A.2
Summary of useful derived rules
. . . . . . . . . . . . . . . . . .
317
A.3
Summary of rules for delegation
. . . . . . . . . . . . . . . . . .
318
A.4
Inference rules for relating security levels . . . . . . . . . . . . . .
318
A.5
Inference rules for relating integrity levels
. . . . . . . . . . . . .
318
A.6
Logical rules regarding principal equality . . . . . . . . . . . . . .
319


Preface
Our intent in developing this textbook is to serve the needs of computer engineers
and computer scientists who are responsible for designing, implementing, and ver-
ifying secure computer and information systems. Engineers who serve in the roles
of designers and veriﬁers must be able to translate concepts and ideas into calcu-
lations and derivations as part of the design and veriﬁcation process. For exam-
ple, electrical-engineering students routinely learn how to specify, implement, and
verify control and communications systems using Laplace and Fourier transforms.
They can translate their concepts into networks of system transfer functions and use
Laplace and Fourier transforms to analyze, predict, and verify the behavior of their
designs. Computer-hardware designers rely on switching theory, discrete mathemat-
ics, and ﬁnite-state machines to specify, implement, and verify their designs. In an
analogous fashion, our goal is to equip engineers with an access-control logic they
can use to specify, implement, and verify their security designs.
Our focus is on access control and reference monitors. Controlling access to pro-
tected objects is central to any security requirement. Reference monitors are the
means to protect objects of value in systems. Just as propositional logic and ﬁnite-
state machines are used to deﬁne and explain computer hardware design and veriﬁ-
cation principles, we use a propositional modal logic to explain access-control prin-
ciples. Our view is this: if you are the hardware designer and you are given the input
values to your design, then you should be able to justify mathematically whether or
not the value on any particular output is a 0 or a 1. Similarly, if you are the engineer
who has security requirements to meet and you are given a policy and a request, you
should be able to justify mathematically if your answer is a yes or a no.
To be explicit, the logic we present is meant to inform an implementation, not be
the implementation. The logic is not a programming language, and we are not sug-
gesting that access controllers should be implemented as theorem provers. Rather,
we believe the logic is a useful tool for analyzing security designs and for making ex-
plicit the conditions upon which access-control decisions depend. Access controllers
can be implemented as checklists that check and verify the relevant artifacts (e.g.,
certiﬁcates and credentials) against existing policies and trust assumptions. These
checklists correspond to provably sound inference rules in the logic.
Another intent of our book is to ﬁll a gap left by many books on computer and
network security. Several are compendiums of computer security models and meth-
ods. They are highly mathematical and encyclopedic—and usually beyond the skills
and interests of design engineers. Most do not have illustrative examples or exer-
cises. While excellent references, the breadth and depth of mathematics are beyond
what is needed by most engineers. In contrast, many other books are introductory
xix

xx
in nature and are primarily descriptive. To make security concepts accessible to a
wide audience, these books deliberately omit mathematical treatments of security.
While informative, this approach does not equip engineers to do the derivations and
calculations with the same degree of mathematical precision and accuracy expected
of hardware and electrical engineers. The result is that security is often taught at the
lowest levels of knowledge—recall and comprehension—as opposed to the higher
knowledge levels expected of engineers: application, analysis, synthesis, and evalu-
ation.
To ﬁll this gap, we use a single access-control logic based on a simple proposi-
tional modal logic throughout this textbook. We introduce the basic logic early in
the book and use it throughout to deﬁne and derive access-control principles and
properties. Our focus is on reference monitors because they are the parts of a system
that systems engineers must worry about specifying, designing, implementing, and
verifying. Reference monitors in security play an analogous role to the role played
by ﬁnite-state machines in computer hardware.
We developed much of the content of this book in our own courses at Syracuse
University and in intense summer courses on access control for hundreds of Air
Force Reserve Ofﬁcer Training Corp (ROTC) cadets from over forty US universi-
ties. Our experience is that “practice makes perfect” and that our students beneﬁted
from having illustrations and exercises. Thus, we have included numerous examples
to illustrate principles, as well as many exercises to serve as assessments of knowl-
edge. We have annotated each exercise to indicate the level of knowledge it assesses,
according to the following legend:
• The symbol  denotes exercises at the application level of knowledge. These
exercises typically ask the reader to apply particular knowledge or use partic-
ular techniques to solve a problem in a new context. Straightforward calcula-
tions fall into this category of exercises.
• The symbol
denotes exercises at the analysis level of knowledge. These
exercises generally require the reader to decompose a problem into its con-
stituent parts in order to perform the necessary calculations or experiments
necessary to solve the problem.
• The symbol
denotes exercises at the synthesis level of knowledge. These
exercises typically require the reader to integrate various techniques or com-
ponents to design, construct, or formulate an entirely new structure, pattern, or
proof.
• The symbol
denotes exercises at the evaluation level of knowledge. These
exercises require the reader to identify and use relevant criteria to assess and
judge the suitability of a solution to a given problem.
Exercises at higher levels of knowledge are not necessarily harder than exercises
at lower levels of knowledge. We recommend that readers try at least one exercise at
each level of knowledge to get the most beneﬁt.

xxi
We have taught both undergraduate and graduate students using this book. We rec-
ommend that all students learn what is in Part I (Preliminaries). This part covers the
syntax and semantics of the access-control logic, basic access control concepts, and
an introduction to conﬁdentiality and integrity policies. Our experience with ROTC
cadets is that undergraduates who have successfully mastered a sophomore discrete-
mathematics course do master the syntax and semantics of the access-control logic.
Undergraduates can skip the section on soundness in Chapter 3.
Part II (Distributed Access Control) and Part III (Isolation and Sharing) focus on
access control in networks and access control in hardware, respectively. These parts
are relatively independent of one another. For courses with an emphasis on networks,
Part III may be omitted. For courses with an emphasis on hardware and virtual
machines, Part II may be omitted. Most everything in these parts is well within
the capabilities of undergraduate students who have mastered the preliminaries in
Part I. Part III includes a primer that provides a brief review of sophomore-level
courses on assembly language and computer architecture; our experience is that this
primer is sufﬁcient for those students with limited hardware backgrounds (e.g., many
computer-science majors).
Part IV (Access Policies) is a treatment of conﬁdentiality, integrity, and role-based
access control in the access-control logic. Although it does make use of delegation
(introduced in Part II), it is largely independent of Part III. Readers can pick and
choose the policies of interest to them without reading this part’s chapters in order.
Acknowledgments
We are grateful for the support of numerous colleagues and
students. Our ﬁrst attempts at the graduate level took place in CIS/CSE 774: Prin-
ciples of Distributed Access Control. Over the course of seven summers, we further
reﬁned our text at the undergraduate level with the cadets of the Advanced Course in
Engineering (ACE) Cybersecurity Bootcamp at the Air Force Research Laboratory
in Rome, New York, and on our own Syracuse University students in CIS/CSE 400.
The outstanding tenacity of these students convinced us of the feasibility of teach-
ing undergraduates a logical and rigorous approach to access control. Dr. Kamal
Jabbour, Air Force Research Laboratory Senior Scientist for Information Assurance,
was unwavering in his support of us and our methods in ACE.
Our book was typeset using LATEX2ε. The clip art in our book comes from the
Open Clip Art Library.
Finally, this book would not exist were it not for the encouragement and patience
of our editor, Robert Stern, Executive Editor, Taylor & Francis Group.


Chapter 1
Access Control, Security, Trust, and
Logic
This book is about access control, security, and trust. We wrote this book for people
who specify, design, build, or certify computer and information systems that must
be trustworthy and secure. Most every information system or computer has some
security requirement. Common examples include computers handling sensitive in-
formation such as ﬁnancial information, health records, or military secrets. If you are
responsible for designing, building, testing, or certifying systems that have security
concerns, then you are concerned with the following:
• who or what can access protected resources,
• how to protect the conﬁdentiality, integrity, and availability of those resources,
• who or what is trusted or believed, and
• compelling reasons to conclude a system is worthy of trust.
For example, if you are responsible for the speciﬁcation, design, or operation of
computers holding bank accounts, you are very concerned about the following ques-
tions:
• Who can withdraw funds from a customer’s bank account electronically?
• Who is allowed to alter the balance or available funds in a customer’s account?
• Who has authority to grant account access?
• What evidence is there to substantiate that the computerized banking system
is secure and operating correctly?
When we talk about access control, security, and trust in this book, we mean the
following:
• Access control is concerned with the policies and mechanisms that permit or
deny the use of a resource or capability.
• Security is concerned with the policy and mechanisms that protect the conﬁ-
dentiality, integrity, and availability of information, resources, or capabilities.
• Trust focuses on who or what is believed and under what circumstances.
1

2
Access Control, Security, and Trust: A Logical Approach
If a system has appropriate policies, mechanisms, and trust assumptions for access
control and security, and if these policies and mechanisms are logically consistent
and correctly implemented, then we are more likely to willingly believe or depend
on the system. That is, we are more likely to deem the system as trustworthy and less
likely to fail. Systems fail for at least four reasons. They wear out; they are ﬂawed;
they are used in unintended ways; the operating or design assumptions are wrong.
While this book does not deal with wear as a cause of failure, this book does address
the remaining three causes.
Any engineer or computer scientist who has designed, certiﬁed, or worked with
systems of any size or consequence knows that a key question is how will we know?
They know that undetected design ﬂaws or inappropriate assumptions that are built
into deployed systems are potentially disastrous and life threatening. They know
that ﬂaws in deployed systems are orders of magnitude more costly to remedy when
compared to corrections made in the design phase. They know that undetected ﬂaws
potentially destroy systems, destroy reputations, and destroy credibility, leading to
failed missions, failed services, and failed corporations. In short, as systems become
larger and more complex, it is increasingly difﬁcult for designers and certiﬁers to get
a good night’s sleep.
Our purpose in writing this book is to help designers and certiﬁers sleep well at
night. Our experience shows us that the system designers and certiﬁers who sleep
best at night are those who combine their experience and intuition with mathematics
and logic. Experience and intuition are powerful tools that inform the selection of
a design approach. Mathematics and logic are unparalleled in providing assurances
of correct coverage of all cases and instances, some of which might not have been
imagined by designers or certiﬁers.
We follow the same approach used by civil, mechanical, and electrical engineers.
Mathematics and logic properly used clarify the underlying principles and properties
of systems. Systems with mathematical and logical descriptions are amenable to
independent and automated veriﬁcation and testing. The effects of system changes
or consequences of altering assumptions are easier to deduce with logic than without.
Our experience shows us that ﬂaws and misconceptions often exist between levels
of abstraction in systems. For example, how will we know that a security policy re-
lated to information integrity is correctly implemented by hardware and software to-
gether? Requirement writers, software engineers, and hardware engineers might in-
terpret the meaning of integrity differently leading to improper assumptions, ﬂawed
policies, ﬂawed designs, and failed systems. Our approach to dealing with this ob-
servation is to use a logic that spans many levels of abstraction including hardware,
software, and policy.
The logic used here describes policies and mechanisms at the hardware level, the
network level, and the abstract level of certiﬁcates, jurisdiction of authority, delega-
tions, conﬁdentiality models, and integrity models. What you will ﬁnd is that the
logic and calculations (proofs) in the logic are straightforward. We use logic as a
means to an end. It is a language for describing policies and mechanisms with the
added beneﬁt of having a semantics that allows us to do calculations. If require-
ments writers, software and hardware engineers, and system certiﬁers use the same

Access Control, Security, Trust, and Logic
3
language with a mathematical semantics, differences in interpretation and the effects
of assumptions are sorted out rigorously. Thus, only two chapters are devoted to the
logic itself. The remaining chapters are all about access control, security, and trust.
A key ability we intend for you to acquire is the ability to translate informal de-
scriptions of system requirements and behavior into formal logic. Often, this trans-
lation is done by breaking down an informal system description into its conceptual
parts by answering questions regarding what is being protected, what is being re-
quested, who or what are the guards, and on what basis do the guards decide to grant
or deny access requests.
We often rely on everyday situations as illustrations. The following example and
the exercises that follow illustrate the types of common access-control decisions we
encounter daily.
Example 1.1
Consider a movie theater. What is being protected, what is requested by patrons,
who are the guards, and how do the guards make their decisions?
In a movie theater, what is being protected is the ability to see (and hear) the
movies being shown. Patrons request access to see and hear the movie of their choice.
There are typically two guards. The ﬁrst guard is the box-ofﬁce attendant, who
handles requests for tickets: he grants tickets to patrons when there are available
seats and when the patrons have paid. The second guard governs access into the
theater itself: she checks to see if the movie patron has a valid ticket. The ticket
symbolizes the capability of the ticket bearer to see the movie named on the ticket.♦
Exercise 1.0.1
Consider a public library. What exactly is being requested, what

is being protected, and who are the guards? How do the guards make their access
decisions?
Exercise 1.0.2
Consider a bank that offers online access to customer accounts.

What exactly is being requested, what is being protected, and who or what are the
guards? How do the guards make their access decisions?
Exercise 1.0.3
Recall the last time you took a commercial ﬂight from an airport.

Identify all of the guards you encountered, from the moment you stepped into the
airport until you boarded your ﬂight. From the perspective of each of these guards,
what exactly were you requesting? How did the guards make their access decisions?
1.1
Deconstructing Access-Control Decisions
A common adage among security pundits is that security must be built in from
the beginning and not added on as an afterthought. However, engineers and system

4
Access Control, Security, and Trust: A Logical Approach
designers must wrestle with design details at the chip, board, software, middleware,
and network levels while ﬁghting changing requirements, speciﬁcations, deadlines,
and resources. The challenge for engineers and designers is to quickly and accurately
comprehend what is needed security-wise and to be able to routinely account for
security speciﬁcations along with all the other functional and resource speciﬁcations
and constraints.
Our goal in writing this book is to provide system creators and veriﬁers with a
means to reason about the system’s security requirements in a rigorous manner.
Speciﬁcally, we hope to provide them with analogous mathematical tools to those
that hardware engineers enjoy, as illustrated in the following example.
Example 1.2
Amanda is one of the lead hardware engineers on a project. At a critical design
review, a reviewer Frank says “Show me how you’ve met the requirement to track
all 221 cases.” Amanda replies that there is an 8-bit register used to index each
case. Frank then asks Amanda, “What exactly happens when the input x2 is a 1 and
input y2 is a 0?” Amanda answers that x2 and y2 are inputs to a two-input or-gate,
and she writes “1 ∨0 = 1” to demonstrate the calculation. Later during the review,
Frank challenges Amanda with the statement “I don’t think your design works in this
case!” Amanda calmly responds by putting up two formulas, one representing the
speciﬁcation and another representing her hardware implementation. Both look very
different. She uses the laws of switching theory to show that both formulas have the
same meaning.
♦
Breaking down the engineer’s answers to the reviewer’s questions and challenges,
the following points emerge:
• Amanda implies that she has met the requirement to track all 221 cases of
interest by appealing to the mathematical property that an 8-bit register can
account for as many as 28 = 256 cases. This approach also allows room for
some requirements creep (i.e., the tendency for requirements to grow in size).
• Amanda is able to account precisely for the consequences of her design when
asked what happens for a particular assignment of values to x2 and y2. A two-
input or-gate has a precise meaning (in this case, the value of x2 ∨y2).
• When challenged to show that her implementation meets the speciﬁcation,
Amanda gives a formal representation of both and uses the laws of switching
theory (i.e., propositional logic) to prove that both are the same. Her proof
compels the reviewer to conclude that her design is correct.
One of our goals for this textbook is to give Amanda and other engineers the same
kind of logical support that will enable them to answer questions about access con-
trol. The central concept we rely upon is that of the reference monitor. Reference

Access Control, Security, Trust, and Logic
5
monitors are the guards that protect resources such as ﬁle systems, memory, commu-
nications channels, and so on. Reference monitors play an analogous role in access
control to the role played by ﬁnite-state machine controllers in hardware: they de-
termine whether to grant access requests based on access-control policies. In this
book, we will look at reference monitors and access-control decisions in a multitude
of contexts, from the security guard at an airport who checks picture identiﬁcation
cards and airline tickets, to virtual-machine monitors and electronic requests made
by users acting in particular roles.
As we will see many times over, access-control decisions are made within the
context of (1) information or evidence in the form of statements, credentials, li-
censes, tickets, or certiﬁcates, (2) trust assumptions regarding proxy relationships
or the jurisdiction of authorities, and (3) some interpretation of credentials and other
statements. The following example and subsequent exercises highlight these aspects
across a variety of common scenarios.
Example 1.3
Imagine you are a rental agent for a rental car company in the United States. You
are directed by the company’s policy to ask for a valid US driver’s license from the
renter. If the renter presents a current driver’s license with their picture on it issued
by a department of motor vehicles (DMV) ofﬁce in one of the ﬁfty states, the renter
is allowed to rent a car (assuming he or she can pay for it). What is the thinking
behind this policy?
1. A license that is free from physical or electronic tampering and has been issued
by a state DMV is interpreted as evidence that the person named and pictured
on the license is certiﬁed by that state as having the required skills to drive a
car legally.
2. The recognition of the state’s authority to certify drivers is indicated by the
rental car company’s policy to accept a valid US driver’s license as evidence
of being a legal driver.
3. There is a trust assumption that licenses free from physical or electronic tam-
pering truly represent the states’ DMV ﬁndings. In other words, the assump-
tion is that licenses truly speak for the states’ DMV.
If any one of the preceding things change, then the rental procedure will likely need
to be changed too. For example, suppose it is discovered that, due to some security
breach, tens of thousands of counterfeit licenses bearing the seal of the State of New
York were issued. What changes would you make and why?
A plausible change is to require a customer presenting a New York State driver’s
license to undergo additional scrutiny, perhaps by consulting a New York State
database of legal drivers and asking for additional picture ID. In this case, we would
want to make sure that the New York database is trustworthy (i.e., accessible only
from a secure site), and we would ask for an additional picture ID such as a US
passport.
♦

6
Access Control, Security, and Trust: A Logical Approach
Exercise 1.1.1
Suppose that you are an online merchant who accepts credit cards
over the Internet. What assumptions about evidence, authority, and trust are you
making when you ask a customer over the Internet for their name, card number,
expiration date, and the three- or four-digit security code on the back of the card?
Exercise 1.1.2
Suppose that you are the airport security guard that checks picture
IDs and boarding passes before letting passengers into the security line that checks
carry-on baggage and passengers. What assumptions about evidence, authority, and
trust are you using when you ask a passenger for their ticket and picture ID?
Exercise 1.1.3
Suppose that you own a bar next to a college campus and that the
legal drinking age is twenty-one. You can lose your liquor license if you serve alcohol
to people under twenty-one. What do you ask your bartender to do before serving
drinks? If you have someone guarding the door, what do you ask them to do? What
assumptions are you making about evidence, authority, and trust?
1.2
A Logical Approach to Access Control
The exercises at the end of the preceding section share an underlying structure, but
their descriptions quickly become verbose and unwieldy: how would you convince a
boss or critic in a project review that the system is making the correct access decision
in a given situation? A calculus based only on natural language is at best cumber-
some and at worst impossible to use. In this book, we present a formal logic that
allows us to represent and reason about these scenarios concisely and symbolically.
The use of formal logic allows us to more easily detect patterns of correct or
incorrect reasoning. The capability to recognize familiar patterns of reasoning, albeit
in many different guises, is a good thing. The realization that situations that on the
surface seem different are really the same helps tremendously by giving us insight
and clarity, reducing the apparent complexity of problems. The access-control logic
that we introduce in the next chapter will allow us to see succinctly the form or
structure behind a policy, a set of trust assumptions, and the access decisions made
by a reference monitor.
We shall also see that, in most cases, the actual proofs are quite easy, which is
a good thing: few people like complicated proofs. The value of having an access-
control logic is that it provides a means for describing situations precisely using
formulas that have precise meanings. Second, recall the rationale for preliminary de-
sign reviews: it is important to account for assumptions when determining the likely
adequacy of a proposed approach. Using an access-control logic with inference rules
requires one to account for all necessary assumptions explicitly in order to reach the
desired conclusions. Finally, having the basis for access-control decisions and ref-
erence monitors described formally in an access-control logic gives us the capacity
to quickly determine how we are potentially affected or made vulnerable if certain

Access Control, Security, Trust, and Logic
7
assumptions change—this information can direct us into determining how to adapt
to changing requirements or situations.
Throughout this textbook, we use this access-control logic in much the same way
as hardware engineers use switching theory and propositional logic to specify, de-
sign, and verify hardware. Just as the laws of switching theory are used to calcu-
late whether a signal is 0 or 1, we use the access-control logic to determine if an
access-control request should be honored or not. To be explicit, the logic is a use-
ful tool for analyzing security designs and for making explicit the conditions upon
which access-control decisions depend. Although the logic can be used to inform an
implementation, it is not a programming language, and we are not suggesting that
access controllers be implemented as theorem provers. Instead, access controllers
can be implemented as checklists that check and verify the relevant artifacts (e.g.,
certiﬁcates and credentials) against existing policies and trust assumptions; these
checklists correspond to provably sound inference rules in the logic.
Two chapters (Chapter 2 and Chapter 3) are devoted to formally deﬁning the syn-
tax and semantics of the logic. The logic is straightforward and typical of the kinds
of modal and temporal logics used by industry for hardware veriﬁcation and model
checking. The remainder of the book focuses on access control itself, from basic
concepts (e.g., tickets, lists, authentication) to applications (e.g., virtual machines
and network protocols) to models (e.g., role-based access control).


Part I
Preliminaries


Chapter 2
A Language for Access Control
In this chapter we introduce the core logic that relates principals to the statements
they make. The term principal refers to any person, process, or agent that attempts
to exercise an action on an object (i.e., assert its rights). The statements we care
about include access requests (e.g., Alice states her desire to read a certain ﬁle),
as well as statements of rights (e.g., the Department of Motor Vehicles states that
Bob is authorized to drive a truck), authority, and jurisdiction (e.g., the Department
of Motor Vehicles has the jurisdiction to authorize Bob to drive a truck), and even
the association of keys with principals (e.g., a speciﬁc 128-bit encryption key is
associated with Carol).
In subsequent chapters, we will express and reason about a variety of access-
control concepts using this logic. The purpose of this chapter and the next is to
provide a primer to the logic itself. There are three important components of any
logic: the syntax (“what do the formulas look like?”), the semantics (“what do the
formulas mean?”), and the logical rules (“how can I manipulate formulas to carry
out my reasoning?”). We introduce the ﬁrst two of these components in this chapter,
leaving the inference rules for Chapter 3. However, we begin by reviewing important
concepts and notation from discrete mathematics that we will use throughout this
book, including sets, relations, and certain operations on them.
2.1
Sets and Relations
We assume a working knowledge of discrete mathematics and propositional logic,
and thus we do not provide a detailed introduction to them. Instead, we brieﬂy
introduce the notation that we will be using, and we review some simple and common
approaches for proving properties about sets and relations.
The exercises in this section can be used as a guide to determine whether you have
the requisite knowledge of discrete mathematics and propositional logic. Also, the
references at the end of this chapter contain several suggested texts to consult if you
need a detailed introduction.
11

12
Access Control, Security, and Trust: A Logical Approach
2.1.1
Notation
We use standard set notation. Set elements are contained within braces, as in
{e0,e1,...,en}. The empty set (i.e., the set that has no elements) is denoted as either
{} or /0. Set union is denoted by ∪, set intersection is denoted by ∩, and set difference
is denoted by −, as illustrated by the following simple examples:
{1,2}∪{1,3,4} = {1,2,3,4}
{1,2,3}∩{2,3,4,5} = {2,3}
{2,3,4,5}−{1,4,5} = {2,3}.
The order in which elements are listed in a set is unimportant. Thus, for example,
{1,2} and {2,1} denote the same set.
We write x ∈S to indicate that x is an element of the set S; likewise, x ̸∈S indicates
that x is not an element of the set S.
A set S is a subset of T, written S ⊆T, if every element of S is also an element of
T. The power set of a set S, written P(S), is the set of all subsets of S. For example,
the power set of {red,blue} is
P({red,blue}) = {{},{red},{blue},{red,blue}}.
Note that the empty set is a subset of every set (i.e., /0 ⊆S for all S), and every set S
is a subset of itself (i.e., S ⊆S). Thus, /0 and S are always elements of P(S).
The Cartesian product of sets A and B (written A × B) is the set of ordered pairs
whose ﬁrst component is drawn from A and whose second component is drawn from
B:
A×B = {(a,b) | a ∈A and b ∈B}.
A binary relation is simply a set R ⊆A × B of pairs whose ﬁrst components are
drawn from A and whose second components are drawn from B. We say that R is a
binary relation over (or on) A when R ⊆A×A.
The identity relation on a set A is the relation idA ⊆A×A deﬁned by:
idA = {(a,a) | a ∈A}.
The composition of relations R1 ⊆A × B and R2 ⊆B ×C is the relation R1 ◦R2 ⊆
A×C deﬁned as follows:
R1 ◦R2 = {(x,z) | there exists y such that ((x,y) ∈R1 and (y,z) ∈R2)}.
Finally, given a relation R ⊆A × B and an element a ∈A, we deﬁne R(a) to be the
image of R under a:
R(a) = {b ∈B | (a,b) ∈R}.
That is, R(a) is the set of elements in B related to a by the relation R. For example,
if S is the relation
S = {(1,2),(2,3),(2,4),(3,5),(3,1),(4,1),(5,2)},
then S(2) = {3,4} and S(3) = {5,1}.

A Language for Access Control
13
Exercise 2.1.1
Let T and U be relations over the set A = {1,2,3,4}, as follows:

T = {(1,1),(2,1),(3,3),(4,4),(3,4)}
U = {(2,4),(1,3),(3,3),(3,2)}.
Calculate the following sets:
a. P({x,y,z})
b. U(3)
c. U(4)
d. T ∪U
e. T ∩U
f. T −U
g. U ◦T
h. T ◦U
2.1.2
Approaches for Mathematical Proofs
Throughout this book, our examples and exercises include mathematical proofs
involving sets, relations, and relationships among them. We highlight some common
approaches for structuring these proofs.
Recall that a set S is a subset of T if every element of S is also an element of T.
Thus, to prove that some set A is a subset of B, it sufﬁces to demonstrate that any
arbitrary element of A is necessarily also an element of B. The following simple
example demonstrates this proof approach.
Example 2.1
Property: The subset relation is transitive: that is, if A ⊆B and B ⊆C, then A ⊆C.
Proof: Consider sets A,B,C such that A ⊆B and B ⊆C, and consider an arbitrary
element x ∈A; we need to show that x ∈C as well.
Because A ⊆B, we know that x ∈B; because B ⊆C, it follows that x ∈C as
necessary.
♦
Two sets A and B are equal (i.e., contain exactly the same elements) provided that
each is a subset of the other. This fact forms the foundation for another standard
proof approach: to prove that A = B, it sufﬁces to demonstrate that A ⊆B and that
B ⊆A. The following example, which includes a useful property for subsequent
sections, demonstrates this proof approach.

14
Access Control, Security, and Trust: A Logical Approach
Example 2.2
Property: Suppose that R and T are arbitrary relations over a set A and that Y is a
set; then the following equality holds:
{x | (R∪T)(x) ⊆Y} = {x | R(x) ⊆Y}∩{x | T(x) ⊆Y}.
Proof: Consider arbitrary relations R,T and an arbitrary set Y. Our analysis involves
two steps:
1. Consider an arbitrary element a ∈{x | (R∪T)(x) ⊆Y}: thus (R∪T)(a) ⊆
Y. By deﬁnition,
(R∪T)(a) = {b | (a,b) ∈R∪T}
= {b | (a,b) ∈R}∪{b | (a,b) ∈T}
= R(a)∪T(a).
Therefore, R(a) ∪T(a) ⊆Y, and hence we also have that R(a) ⊆Y and
T(a) ⊆Y. It follows that a ∈{x | R(x) ⊆Y} and a ∈{x | T(x) ⊆Y}, and
therefore a ∈{x | R(x) ⊆Y}∩{x | T(x) ⊆Y}. Because a was an arbitrary
element of {x | (R∪T)(x) ⊆Y}, we have shown that
{x | (R∪T)(x) ⊆Y} ⊆{x | R(x) ⊆Y}∩{x | T(x) ⊆Y}.
2. Consider an arbitrary element a ∈{x | R(x) ⊆Y} ∩{x | T(x) ⊆Y}. It
follows that R(a) ⊆Y and T(a) ⊆Y, and hence R(a) ∪T(a) ⊆Y. Be-
cause R(a) ∪T(a) = (R ∪T)(a), we also have that (R ∪T)(a) ⊆Y, and
thus a ∈{x | (R ∪T)(x) ⊆Y}. Because a was an arbitrary element of
{x | R(x) ⊆Y}∩{x | T(x) ⊆Y}, we have shown that
{x | R(x) ⊆Y}∩{x | T(x) ⊆Y} ⊆{x | (R∪T)(x) ⊆Y}.
Having demonstrated that each set is a subset of the other, we have shown that
{x | (R∪T)(x) ⊆Y} = {x | R(x) ⊆Y}∩{x | T(x) ⊆Y}.
♦
One often encounters statements that indicate a given property (say, Property 1)
is true if and only if another property (say, Property 2) is true. Proving that an “if
and only if” statement is true involves two steps: (1) demonstrating that, whenever
Property 1 holds, Property 2 must also hold; and (2) demonstrating that, whenever
Property 2 holds, Property 1 must also hold. The following example demonstrates
this proof approach to prove a property that will be useful for calculations in subse-
quent sections.
Example 2.3
Property: Let A, X, and Y be sets such that X and Y are both subsets of A. Then
A ⊆(A−X)∪Y if and only if X ⊆Y.

A Language for Access Control
15
Proof: Our analysis involves two steps:
1. The “forward” direction: suppose A ⊆(A −X) ∪Y, and consider any
x ∈X. Since X ⊆A, x ∈A, and thus x ̸∈A−X. Therefore, x must be an
element of Y. Since x was arbitrary, X ⊆Y.
2. The “reverse” direction: suppose X ⊆Y, and consider any a ∈A. If
a ∈X, then by the deﬁnition of subset, a ∈Y, and hence a ∈(A−X)∪Y
as necessary; if, instead, a ̸∈X, then a ∈(A−X) and therefore a ∈(A−
X)∪Y. Since a was arbitrary, A ⊆(A−X)∪Y.
Having shown that each property implies the other, we have demonstrated that
A ⊆(A−X)∪Y if and only if X ⊆Y.
♦
The exercises that follow include properties that will be useful for exercises in
subsequent sections.
Exercise 2.1.2
Prove that, for all relations R,S,T, the following property holds:
If R ⊆S, then R◦T ⊆S◦T.
Exercise 2.1.3
Prove that, for all relations R,S,T, the following property holds:
(R∪S)◦T = (R◦T)∪(S◦T).
Exercise 2.1.4
Prove that, for all sets A,B,C, the following property holds:
((A−B)∪C)∩((A−C)∪B) = (B∩C)∪((A−B)∩(A−C)).
Exercise 2.1.5
Prove that, for all relations R,S, sets X, and items u, the following
property holds:
(R◦S)(u) ⊆X if and only if R(u) ⊆{y | S(y) ⊆X}.
2.2
Syntax
Our goal is to deﬁne a logic for expressing access-control policies and analyzing
their ramiﬁcations. However, to do so, we must ﬁrst identify the most primitive
concepts that we wish to express.
The ﬁrst step is to introduce the syntax of the logic: that is, we must specify
what the formulas of the logic look like. To do so, we make use of BNF (Backus-
Naur Form) speciﬁcations, which provide a way to state syntactic rules precisely

16
Access Control, Security, and Trust: A Logical Approach
and unambiguously. We explain how BNF speciﬁcations work through the following
example, which describes the syntax for a simple language of arithmetic expressions:
AExp ::= BinNumber
AExp ::= ( AExp + AExp )
AExp ::= ( AExp ∗AExp )
BinNumber ::= Bit
BinNumber ::= Bit BinNumber
Bit ::= 0
Bit ::= 1
The items in boldface (e.g., AExp and BinNumber) are called non-terminal symbols
and represent syntactic categories (i.e., collections of syntactic expressions). The
symbols “(”, “)”, “+”, “∗”,“0”, and “1” are called terminal symbols (or terminals)
and correspond to actual symbols in the syntax. The notation “ ::= ” is meta-notation
for specifying production rules for the syntax. In general, each line of form
AExp ::= right-hand-side
provides a rule for creating new elements of AExp (and likewise for BinNumber and
Bit). A syntactic derivation is a sequence of rewriting steps: each step uses one of the
production rules to replace one occurrence of a nonterminal symbol. The syntactic
derivation is complete when there are no more nonterminal symbols to replace.
The following two examples contain syntactic derivations that respectively demon-
strate that 101 belongs to the syntactic category BinNumber and that (11∗(1+10))
belongs to the syntactic category AExp. Note that a given piece of syntax (such
as 101) may have multiple derivations possible, corresponding to choosing different
nonterminals to expand at a given step. The order in which nonterminal symbols are
replaced is not important; what is important is that at least one syntactic derivation
must exist for a given piece of syntax in order for it to be considered well formed.
Example 2.4
The following derivation demonstrates that 101 belongs to the syntactic category
BinNumber:
BinNumber ⇝Bit BinNumber
⇝1 BinNumber
⇝1 Bit BinNumber
⇝1 Bit Bit
⇝10 Bit
⇝101
♦

A Language for Access Control
17
Example 2.5
The following derivation demonstrates that (11∗(1+10)) belongs to the syntactic
category AExp:
AExp ⇝( AExp∗AExp)
⇝( AExp∗( AExp+AExp))
⇝( AExp∗( BinNumber+AExp))
⇝( AExp∗( Bit+AExp))
⇝( AExp∗( 1+AExp))
⇝( AExp∗( 1+BinNumber))
⇝( AExp∗( 1+Bit BinNumber))
⇝( AExp∗( 1+1 Bit))
⇝( AExp∗( 1+10))
⇝( BinNumber∗( 1+10))
⇝( Bit BinNumber ∗( 1+10))
⇝( Bit Bit ∗( 1+10))
⇝( 1 Bit ∗( 1+10))
⇝( 11 ∗( 1+10))
♦
Multiple production rules for the same syntactic category can be combined into a
single rule by using the meta-symbol “ / ”, which separates possible alternatives. For
example, the three production rules for the AExp syntactic category could instead be
speciﬁed as follows:
AExp ::= BinNumber / ( AExp + AExp ) / ( AExp ∗AExp )
This production rule states that an element of the syntactic category AExp has either
the form BinNumber, the form ( AExp + AExp ), or the form ( AExp ∗AExp ).
2.2.1
Principal Expressions
Principals are the major actors in a system. They are the entities that make re-
quests, and the class of principals includes (but is not limited to) people, processes,
cryptographic keys, personal identiﬁcation numbers (PINs), userid–password pairs,
and so on.
In general, principals may be either simple or compound. A simple principal is
an entity that cannot be decomposed further: individuals, cryptographic keys, and
userid–password pairs are all simple principals. We deﬁne PName to be the collec-
tion of all simple principal names, which can be used to refer to any simple principal.
For example, the following are all allowable principal names: Alice, Bob, the key
KAlice, the PIN 1234, and the userid–password pair ⟨alice,bAdPsWd!⟩.
Compound principals are abstract entities that connote a combination of princi-
pals: for example, “the President in conjunction with Congress” connotes an abstract

18
Access Control, Security, and Trust: A Logical Approach
principal comprising both the President and Congress. Intuitively, such a principal
makes exactly those statements that are made by both the President and Congress.
Similarly, “the reporter quoting her source” connotes an abstract principal that com-
prises both the reporter and her source. Intuitively, a statement made by such a
principal represents a statement that the reporter is (rightly or wrongly) attributing to
his source.
The set Princ of all principal expressions is given by the following BNF speciﬁ-
cation:
Princ ::= PName / Princ & Princ / Princ | Princ
That is, a principal expression is either a simple name, an expression of form P & Q
(where P and Q are both principal expressions), or an expression of form P | Q
(where, again, P and Q are both principal expressions).
The principal expression P & Q denotes the abstract principal “P in conjunction
with Q,” while P | Q denotes the abstract principal “P quoting Q.” Thus, the expres-
sion President & Congress denotes the abstract principal “the President together with
Congress,” and Reporter | Source denotes the abstract principal “the reporter quoting
her source.” In subsequent chapters, we will introduce a few more compound princi-
pal expressions to cover other important concepts, such as delegation. However, we
shall see that those additional expressions are merely special cases of quoting and
conjunctive principals.
Parentheses can be added to disambiguate compound principal expressions. For
example, (Sal & Ted) | Uly denotes the conjunctive principal Sal & Ted quoting the
principal Uly. In contrast, Sal & (Ted | Uly) denotes the principal Sal acting in con-
cert with the compound principal Ted | Uly. The standard convention in such expres-
sions is that & binds more tightly than | , so that (for example) Sal & Ted | Uly is
equivalent to (Sal & Ted) | Uly. However, in this book we shall always include the
parentheses necessary to disambiguate principal expressions. Both & and | are asso-
ciative operators, and hence it is unnecessary to include parentheses around principal
expressions such as Fritz & Hans & Leon or Terry | Kitty | Sandy.
2.2.2
Access-Control Statements
Ultimately, we are concerned with being able to determine with precision and
accuracy which access requests from which principals should be granted, and which
should be denied. Thus, we need to be able to express our assumptions and our
expectations, such as which authorities we trust, which principals should be granted
access to which objects, and so on. Toward this end, we introduce a language of
statements that will allow us to express these sorts of concepts.
The simplest statements are basic requests, such as “read ﬁle foo” or “modify ﬁle
bar.” We will represent such statements in our logic by propositional variables, in
much the same way that statements such as “it is raining” can be represented in

A Language for Access Control
19
propositional logic.1
However, a simple request such as “read ﬁle foo” by itself is generally insufﬁcient
for determining whether or not to grant the request. At a minimum, we need some
way of accounting for the source of the request. In our logic, we can associate
requests (and other statements) with their source by statements of the form
P says ϕ,
where P is a principal and ϕ is a speciﬁc statement. For example, if rff is a proposi-
tional variable representing the request “read ﬁle foo,” then we can represent Deena’s
request to read the ﬁle foo by the statement
Deena says rff.
The says operator can also ascribe non-request statements to particular principals.
For example, we may wish to express that Rob believes (or has stated) that Deena is
making a request to read ﬁle foo. We can express such a concept in the logic by the
statement
Rob says (Deena says rff).
Access policies specify which principals are authorized to access particular ob-
jects. Such authorizations can be expressed in our logic by statements of the form
P controls ϕ,
where P is a principal and ϕ is a speciﬁc statement. Thus, for example, we can
express Deena’s entitlement to read the ﬁle foo as the statement
Deena controls rff.
Note that this statement is different from Deena’s request to read the ﬁle. This state-
ment merely says that if Deena says that it’s a good idea to read ﬁle foo (i.e., if
Deena makes the request), then it is indeed a good idea to read ﬁle foo (i.e., the
request should be granted).
In any given situation, there are particular authorities who are deemed to have ju-
risdiction over particular statements: that is, we will unquestioningly believe certain
statements that they make. For example, Anna’s New York State driver’s license in
effect represents a statement from the New York State Department of Motor Vehi-
cles (DMV) that vouches for Anna’s ability to drive a car. Anyone who accepts the
DMV’s jurisdiction for making that statement will, upon seeing Anna’s valid driving
license, accept as truth that she is entitled to drive. Like authorizations, jurisdiction
is represented in our logic by statements of the form
P controls ϕ,
1Technically, propositions are statements that are interpreted to be either true or false, such as the propo-
sition “it is raining.” If we view an access request such as “read ﬁle foo” as shorthand for “it would be a
good thing to let me read ﬁle foo,” then we indeed have a legitimate proposition.

20
Access Control, Security, and Trust: A Logical Approach
where P is a principal with jurisdiction over the statement ϕ.
Finally, we would like a way to be able to make statements about the relative
trustedness of different principals, which we do through the use of the operator ⇒
(pronounced “speaks for”). Intuitively, the statement
P ⇒Q
describes a proxy relationship between the two principals P and Q such that any
statement made by P can also be safely attributed to Q (i.e., acted upon as if Q had
made it).
In addition to the sorts of statements mentioned above, we make use of the stan-
dard logical operators: negation (¬ϕ), conjunction (ϕ1 ∧ϕ2), disjunction (ϕ1 ∨ϕ2),
implication (ϕ1 ⊃ϕ2), and equivalence (ϕ1 ≡ϕ2). Having provided an informal
overview of our logic, we give a formal deﬁnition of our logic’s syntax in the next
subsection.
2.2.3
Well-Formed Formulas
We give the name PropVar to the collection of all propositional variables, and
we typically use lowercase identiﬁers (such as p,q,r) to range over this set. We
can construct more interesting statements from these simple propositional variables
and from our set Princ of principal expressions. The set Form of all well-formed
expressions (also known as well-formed formulas) in our language is given by the
following BNF speciﬁcation:
Form ::= PropVar / ¬ Form / (Form∨Form) /
(Form∧Form) / (Form ⊃Form) / (Form ≡Form) /
(Princ ⇒Princ) / (Princ says Form) / (Princ controls Form)
This BNF speciﬁcation provides all of the information necessary to determine the
structure of well-formed formulas in our access-control logic. The following are
examples of well-formed formulas:
r
((¬q∧r) ⊃s)
(Jill says (r ⊃(p∨q)))
Convention: Throughout this book, we will distinguish between principal
names and propositional variables through capitalization.
Speciﬁcally,
we will use capitalized identiﬁers—such as Josh and Reader—for sim-
ple principal names. We will use lowercase identiﬁers—such as r, write,
and rff—for propositional variables.

A Language for Access Control
21
Example 2.6
The following syntactic derivation demonstrates that (Jill says (r ⊃(p ∨q))) is a
well-formed formula:
Form ⇝(Princ says Form)
⇝(PName says Form)
⇝(Jill says Form)
⇝(Jill says (Form ⊃Form))
⇝(Jill says (PropVar ⊃Form))
⇝(Jill says (r ⊃Form))
⇝(Jill says (r ⊃(Form∨Form)))
⇝(Jill says (r ⊃(PropVar∨Form)))
⇝(Jill says (r ⊃(p∨Form)))
⇝(Jill says (r ⊃(p∨PropVar)))
⇝(Jill says (r ⊃(p∨q)))
♦
In contrast, the following examples are not well-formed formulas, for the reasons
stated:
• Orly & Mitch is a principal expression, but not an access-control formula.
• ¬Orly, because Orly is a principal expression, not an access-control formula;
the negation operator ¬ must precede an access-control formula.
• (Orly ⇒(p∧q)), because (p∧q) is not a principal expression: the speaks-for
operator ⇒must appear between two principal expressions.
• (Orly controls Mitch), because Mitch is a principal expression, not an access-
control formula; the controls operator requires its second argument to be an
access-control formula.
The parentheses ensure that the grammar is completely unambiguous, but their ex-
cessive proliferation can make the language cumbersome to use. Thus, we typically
omit the outermost parentheses, and we occasionally also omit additional parenthe-
ses according to standard conventions for operator precedence: ¬ binds most tightly,
followed in order by ∧, ∨, ⊃, and ≡. Thus, for example, the formula p ⊃q ∧r
is an abbreviation of (p ⊃(q ∧r)).
Likewise, the formulas ((¬q ∧r) ⊃s) and
(Jill says (r ⊃(p∨q))) can be abbreviated respectively as follows:
¬q∧r ⊃s,
Jill says (r ⊃p∨q).
When the same binary operator appears multiple times in a formula—for example,
p ⊃q ⊃r—the parentheses associate from left to right: (p ⊃q) ⊃r. The operators
says and controls bind even more tightly than ∧, and thus have as small a scope as

22
Access Control, Security, and Trust: A Logical Approach
possible. For example, Kent says r ∨p ⊃q is equivalent to (((Kent says r) ∨p) ⊃
q), and similarly for controls . Because ⇒is an operator that relates only principal
expressions (as opposed to logical formulas), its use is always unambiguous.
Exercise 2.2.1
Which of the following are well-formed formulas in the access-

control logic? Support your answers by appealing to the BNF speciﬁcation.
a. ((p∧¬q) ⊃(Cal controls r))
b. ((Gin ⇒r)∧q)
c. (Mel | Ned says (r ⊃t))
d. (¬t ⇒Sal)
e. (Ulf controls (Vic | Wes ⇒Tor))
f. (Pat controls (Quint controls (Ryne says s)))
Exercise 2.2.2
Fully parenthesize each of the following formulas:

a. p ⊃¬q∨r ⊃s
b. ¬p ⊃r ≡q∨r ⊃t
c. X controls t ∨s ⊃Y says q ⊃r
d. Cy says q∧Di controls p ⊃r
e. Ike ⇒Jan∧Kai & Lee controls q∧r
2.3
Semantics
Although we provided an informal reading of the logical formulas in the previous
section, we have not yet provided sufﬁcient details to enable precise or rigorous use
of the logic. In the next chapter, we introduce logical rules that we can use to reason
about a variety of access-control situations. However, as with any logic, several
important questions arise:
What statements are true in this logic, and how do we know? What does
a given statement really mean? How do we know that the inference rules
are trustworthy? Under what conditions can we add new inference rules
and guarantee that the logic’s trustworthiness is preserved?

A Language for Access Control
23
p
q
r
p∧q
(p∧q) ⊃r
true
true
true
true
true
true
true
false
true
false
true
false
true
false
true
true
false
false
false
true
false
true
true
false
true
false
true
false
false
true
false
false
true
false
true
false
false
false
false
true
Table 2.1: Truth table for the propositional formula (p∧q) ⊃r
The key to answering these questions for any logic is to have rigorous, mathemat-
ical semantics that deﬁne precisely what a given statement means. These formal
semantics provide a basis by which one can independently assess the trustworthiness
of a logical system. For example, in propositional logic, the formal meaning of a
statement such as
(p∧q) ⊃r
can be calculated using a truth table, as illustrated in Table 2.1. Each line in the truth
table corresponds to a particular interpretation of the propositional variables (i.e., a
mapping of variables to speciﬁc truth values). Truth tables calculate the meaning
of larger formulas in a syntax-directed way, based on the meanings of their compo-
nents: for example, the meaning of (p∧q) ⊃r for a given interpretation is calculated
using the meanings of p ∧q and r, as well as a speciﬁc rule for the operator ⊃. A
propositional-logic formula is a tautology—and therefore safe to use as an axiom of
the system—if it is true for every possible interpretation of the propositional vari-
ables.
These same core ideas apply to the semantics for our access-control logic. Because
we must account for the interpretation of principals in addition to propositional vari-
ables, the semantics requires a little more structure than truth tables provide. We can
ﬁnd this additional structure in the form of Kripke structures.
2.3.1
Kripke Structures
Kripke structures are useful models for analyzing a variety of situations. They
are commonly used to provide semantics for modal and temporal logics, providing a
basis for automated model checking.
Deﬁnition 2.1 A Kripke structure M is a three-tuple ⟨W,I,J⟩, where:
• W is a nonempty set, whose elements are called worlds.
• I : PropVar →P(W) is an interpretation function that maps each propositional
variable to a set of worlds.

24
Access Control, Security, and Trust: A Logical Approach
• J : PName →P(W ×W) is a function that maps each principal name to a
relation on worlds (i.e., a subset of W ×W).
Before we look at some examples of Kripke structures, a few comments about this
deﬁnition are in order. First, the concept of worlds is an abstract one. In reality,
W is simply a set: its contents (whatever they may be) are called worlds. In many
situations, the notion of worlds corresponds to the notion of system states or to the
concept of possible alternatives.
Second, the functions I and J provide meanings (or interpretations) for our propo-
sitional variables and simple principals. These meanings will form the basis for our
semantics of arbitrary formulas in our logic. Intuitively, I(p) is the set of worlds in
which we consider p to be true. J(A) is a relation that describes how the simple prin-
cipal A views the relationships between worlds: each pair (w,w′) ∈J(A) indicates
that, when the current world is w, principal A believes it possible that the current
world is w′.
For illustration purposes, we introduce some examples of Kripke structures. The
ﬁrst example provides some intuition as to what the interpretation functions I and J
represent, illustrating how each relation J(P) might reﬂect principal P’s understand-
ing of the universe.
Example 2.7
Consider the situation of three young children (Flo, Gil, and Hal), who are being
looked after by an overprotective babysitter. This babysitter will let them go outside
to play only if the weather is both sunny and sufﬁciently warm.
To keep things simple, let us imagine that there are only three possible situations:
it is sunny and warm, it is sunny but cool, or it is not sunny. We can represent these
possible alternatives with a set of three worlds: W0 = {sw,sc,ns}.
We use the propositional variable g to represent the proposition “The children
can go outside.” The baby sitter’s overprotectiveness can be represented by any
interpretation function
I0 : PropVar →P({sw,sc,ns})
for which I0(g) = {sw}. That is, the proposition g (“the children can go outside”) is
true only in the world sw (i.e., when the weather is both sunny and warm).
Now, the children themselves are standing by the window, trying to determine
whether or not they’ll be allowed to go outside. Gil, who is tall enough to see the
outdoor thermometer, possesses perfect knowledge of the situation, as he will be able
to determine whether it is both sunny and sufﬁciently warm. This perfect knowledge
corresponds to a possible-worlds relation
J0(Gil) = {(sw,sw),(sc,sc),(ns,ns)}.
Whatever the current situation is, Gil has the correct understanding of the situation.
(Note that J0(Gil) is the identity relation idW0 over the set W0.)

A Language for Access Control
25
In contrast, Flo is too short to see the outdoor thermometer, and thus she cannot
distinguish between the “sunny and warm” and “sunny and cool” alternatives. This
uncertainty corresponds to a possible-worlds relation
J0(Flo) = {(sw,sw),(sw,sc),(sc,sw),(sc,sc),(ns,ns)}.
Thus, for example, if the current situation is “sunny and warm” (i.e., sw), Flo con-
siders both “sunny and warm” and “sunny and cool” as legitimate possibilities. That
is, J0(Flo)(sw) = {sw,sc}.
Finally, Hal is too young to understand that it can be simultaneously sunny and
cool: he believes that the presence of the sun automatically makes it warm outside.
His confusion corresponds to a possible-worlds relation
J0(Hal) = {(sw,sw),(sc,sw),(ns,ns)}.
Whenever the actual weather is sunny and cool, Hal believes it to be sunny and
warm: J0(Hal)(sc) = {sw}.
The tuple ⟨W0,I0,J0⟩forms a Kripke structure.
♦
The next example introduces a Kripke structure that does not necessarily reﬂect
any particular scenario or vignette. Rather, the Kripke structure is merely a three-
tuple that contains a set and two functions that match the requirements of Deﬁni-
tion 2.1.
Example 2.8
Let W1 = {w0,w1,w2} be a set of worlds, and let I1 : PropVar →P(W1) be the
interpretation function deﬁned as follows2:
I1(q) = {w0,w2},
I1(r) = {w1},
I1(s) = {w1,w2}.
In addition, let J1 : PName →P(W1 ×W1) be the function deﬁned as follows3:
J1(Alice) = {(w0,w0),(w1,w1),(w2,w2)},
J1(Bob) = {(w0,w0),(w0,w1),(w1,w2),(w2,w1)}.
The three-tuple ⟨W1,I1,J1⟩is a Kripke structure. Intuitively, proposition q is true in
worlds w0 and w2, r is true in world w1, and s is true in worlds w1 and w2. All other
propositions are false in all worlds.
♦
2In this example and those that follow, we adopt the convention of specifying only those propositional
variables that the interpretation function maps to nonempty sets of worlds. Thus, for any propositional
variable p not explicitly mentioned, we assume that I1(p) = /0.
3We adopt a similar convention for principal-mapping functions J: for any principal name A for which
J(A) is not explicitly deﬁned, we assume that J(A) = /0.

26
Access Control, Security, and Trust: A Logical Approach
Next State
Present State
x = 0
x = 1
A
A
D
B
A
C
C
C
B
D
C
A
Table 2.2: State-transition table for ﬁnite-state machine M
World
p
q
r
s
A
true
true
false
true
B
false
true
false
true
C
true
false
false
true
D
false
true
false
true
Table 2.3: Truth values of primitive propositions p, q, r, and s in each world
The next example illustrates how a Kripke structure might be used to represent a
state machine.
Example 2.9
Consider the state-transition table for a ﬁnite-state machine M shown in Table 2.2.
This machine has four states: A, B, C, and D. The column labeled “Present State”
lists the possible present states of M. The two columns under the label “Next State”
list the next states of M if the input x is either 0 or 1, respectively. For example, the
second row of Table 2.2 describes M’s behavior whenever it is currently in state B: if
the input is x is 0, then the next state will be A; if x is 1, then the next state will be C.
We can construct a Kripke structure ⟨W2,I2,J2⟩to model this machine by deﬁning
W2 to be the set of M’s states:
W2 = {A,B,C,D}.
Now, suppose that there are four primitive propositions (p,q,r,s) associated with
the state machine M, with their truth values in the various states given by Table 2.3.
This table effectively speciﬁes the interpretation function I2 on these propositions,
namely:
I2(p) = {A,C},
I2(q) = {A,B,D},
I2(r) = {},
I2(s) = {A,B,C,D}.
Finally, imagine that there is an observer Obs of the machine’s execution. This
observer has faulty knowledge of M’s states: whenever M is in state C, Obs incor-
rectly believes M to be in state D. We’ll assume that the observer does correctly
know when M is in states A, B, or D.

A Language for Access Control
27
This observer’s state knowledge can be captured by the following relation:
J2(Obs) = {(A,A),(B,B),(C,D),(D,D)}.
In the relation J2(Obs), the ﬁrst element of each pair represents the present state of
M, and the second element is the observed state of M. Thus the pair (C,D) reﬂects
that, whenever M is in state C, Obs always believes the current state is D.
The tuple ⟨{A,B,C,D},I2,J2⟩forms a Kripke structure.
♦
In the next example, we model the same state machine, but we consider the inputs
(i.e., “x=0” or “x=1”) as the “observers” of the state machine, and we use each “next
state” as the perceived state by the particular observer. Although the set of worlds
and the interpretation function do not change, the principal-mapping function does
change.
Example 2.10
Let W2 and I2 be as deﬁned in Example 2.9, and deﬁne J3 as follows:
J3(X0) = {(A,A),(B,A),(C,C),(D,C)},
J3(X1) = {(A,D),(B,C),(C,B),(D,A)}.
The tuple ⟨{A,B,C,D},I2,J3⟩forms a Kripke structure.
♦
Just as the interpretation function I of a Kripke structure provides the base inter-
pretation for propositional variables, the function J provides a base interpretation for
simple principal names. We extend J to work over arbitrary principal expressions,
using set union and relational composition as follows:
J(P & Q) = J(P)∪J(Q),
J(P | Q) = J(P)◦J(Q).
Example 2.11
Suppose that we have the following relations:
J(Andy) = {(w0,w0),(w0,w2),(w1,w1),(w2,w1)},
J(Stu) = {(w1,w2)},
J(Keri) = {(w0,w2),(w1,w2),(w2,w2)}.
Then J(Keri | (Andy & Stu)) is calculated as follows:
J(Keri | (Andy & Stu))
= J(Keri)◦J(Andy & Stu),
= J(Keri)◦(J(Andy)∪J(Stu)),
= J(Keri)◦{(w0,w0),(w0,w2),(w1,w1),(w2,w1),(w1,w2)}
= {(w0,w1),(w1,w1),(w2,w1)}.
♦

28
Access Control, Security, and Trust: A Logical Approach
Exercise 2.3.1
Recall the Kripke structure ⟨W0,I0,J0⟩from Example 2.7, and fur-

ther suppose that
J0(Ida) = {(sw,sc),(sc,sw),(ns,sc),(ns,ns)}.
Calculate the following relations:
a. J0(Hal & Gil)
b. J0(Gil | Hal)
c. J0(Flo & Ida)
d. J0(Hal | Ida)
e. J0(Ida | Hal)
f. J0(Hal & (Ida | Hal))
g. J0(Hal | (Ida & Hal))
2.3.2
Semantics of the Logic
The Kripke structures provide the foundation for a formal, precise, and rigorous
interpretation of formulas in our logic. For each Kripke structure M = ⟨W,I,J⟩, we
can deﬁne what it means for formulas in our logic to be satisﬁed in the structure. We
can also identify those worlds in W for which a given formula is said to be true.
To deﬁne the semantics, we introduce a family of evaluation functions. Each
Kripke structure M = ⟨W,I,J⟩gives rise to an evaluation function EM that maps
well-formed formulas in the logic to subsets of W. Intuitively, EM [[ϕ]] is the set of
worlds from the Kripke structure M for which the well-formed formula ϕ is consid-
ered true. We say that M satisﬁes ϕ (written M |= ϕ) whenever ϕ is true in all of
the worlds of M : that is, when EM [[ϕ]] = W. It follows that a Kripke structure M
does not satisfy ϕ (written M ̸|= ϕ) when there exists at least one w ∈W such that
w ̸∈EM [[ϕ]].
Each EM is deﬁned inductively on the structure of well-formed formulas, making
use of the interpretation functions I and J within the Kripke structure M = ⟨W,I,J⟩.
We discuss the individual cases separately, starting with the standard propositional
operators and then moving on to the access-control speciﬁc cases. The full set of
deﬁnitions is also summarized in Figure 2.1.
Standard Propositional Operators
The semantics for propositional variables and the standard logical connectives
(e.g., negation, conjunction, implication) are very similar to the truth-table interpre-
tations for standard propositional logic. The interpretation function I identiﬁes those
worlds in which the various propositional variables are true, while the semantics of
the other operators are deﬁned using standard set operations. We handle these cases
in turn.

A Language for Access Control
29
FIGURE 2.1 Semantics of core logic, for each M = ⟨W,I,J⟩
EM [[p]] = I(p)
EM [[¬ϕ]] = W −EM [[ϕ]]
EM [[ϕ1 ∧ϕ2]] = EM [[ϕ1]]∩EM [[ϕ2]]
EM [[ϕ1 ∨ϕ2]] = EM [[ϕ1]]∪EM [[ϕ2]]
EM [[ϕ1 ⊃ϕ2]] = (W −EM [[ϕ1]])∪EM [[ϕ2]]
EM [[ϕ1 ≡ϕ2]] = EM [[ϕ1 ⊃ϕ2]]∩EM [[ϕ2 ⊃ϕ1]]
EM [[P ⇒Q]] =
(
W,
if J(Q) ⊆J(P)
/0,
otherwise
EM [[P says ϕ]] = {w|J(P)(w) ⊆EM [[ϕ]]}
EM [[P controls ϕ]] = EM [[(P says ϕ) ⊃ϕ]]
Propositional Variables: The truth of a propositional variable p is determined by
the interpretation function I: a variable p is considered true in world w pre-
cisely when w ∈I(p). Thus, for all propositional variables p,
EM [[p]] = I(p).
For example, if M0 is the Kripke structure ⟨W0,I0,J0⟩from Example 2.7,
EM0[[g]] = I0(g) = {sw}.
Negation: A formula with form ¬ϕ is true in precisely those worlds in which ϕ is
not true. Because (by deﬁnition) EM [[ϕ]] is the set of worlds in which ϕ is
true, we deﬁne
EM [[¬ϕ]] = W −EM [[ϕ]].
Thus, returning to Example 2.7,
EM0[[¬g]] = W0 −EM0[[g]] = {sw,sc,ns}−{sw} = {sc,ns}.
Notice that EM0[[¬g]] is the set of worlds in which the children are not allowed
to go outside.
Conjunction: A conjunctive formula ϕ1 ∧ϕ2 is considered true in those worlds for
which both ϕ1 and ϕ2 are true: that is, ϕ1 ∧ϕ2 is true in those worlds w for
which w ∈EM [[ϕ1]] and w ∈EM [[ϕ2]]. Thus, we can deﬁne EM [[ϕ1 ∧ϕ2]] in
terms of set intersection:
EM [[ϕ1 ∧ϕ2]] = EM [[ϕ1]]∩EM [[ϕ2]].

30
Access Control, Security, and Trust: A Logical Approach
Disjunction: Likewise, a disjunctive formula ϕ1 ∨ϕ2 is considered true in those
worlds for which at least one of ϕ1 and ϕ2 is true: that is, ϕ1 ∨ϕ2 is true
in those worlds w for which w ∈EM [[ϕ1]] or w ∈EM [[ϕ2]]. Thus, we deﬁne
EM [[ϕ1 ∨ϕ2]] in terms of set union:
EM [[ϕ1 ∨ϕ2]] = EM [[ϕ1]]∪EM [[ϕ2]].
Implication: An implication ϕ1 ⊃ϕ2 is true in those worlds w for which either
ϕ2 is true (i.e., w ∈EM [[ϕ2]]) or ϕ1 is not true (i.e., w ̸∈EM [[ϕ1]], and thus
w ∈EM [[¬ϕ1]]). That is, ϕ1 ⊃ϕ2 is true in those worlds in which, if ϕ1 is true,
then ϕ2 is also true; if ϕ1 is false, then ϕ2’s interpretation is immaterial. Thus,
we deﬁne the semantics of implications as follows:
EM [[ϕ1 ⊃ϕ2]] = (W −EM [[ϕ1]])∪EM [[ϕ2]].
Equivalence: An equivalence ϕ1 ≡ϕ2 is true in exactly those worlds w in which the
implications ϕ1 ⊃ϕ2 and ϕ2 ⊃ϕ1 are both true. Thus, we deﬁne the semantics
of implications as follows:
EM [[ϕ1 ≡ϕ2]] = EM [[ϕ1 ⊃ϕ2]]∩EM [[ϕ2 ⊃ϕ1]].
Example 2.12
Let M1 be the Kripke structure ⟨W1,I1,J1⟩from Example 2.8. The set EM1[[q ⊃
(r ∧s)]] of worlds in W1 in which the formula q ⊃(r ∧s) is true is calculated as
follows:
EM1[[q ⊃(r ∧s)]] = (W1 −EM1[[q]])∪EM1[[r ∧s]]
= (W1 −I1(q))∪(EM1[[r]]∩EM1[[s]])
= (W1 −{w0,w2})∪(I1(r)∩I1(s))
= {w1}∪({w1}∩{w1,w2})
= {w1}∪{w1}
= {w1}.
♦
In the following example, we evaluate the same formula as in the previous exam-
ple, but with respect to a different Kripke structure.

A Language for Access Control
31
Example 2.13
Let M2 be the Kripke structure ⟨W2,I2,J2⟩from Example 2.9. The set EM2[[q ⊃
(r∧s)]] of worlds W2 in which the formula q ⊃(r∧s) is true is calculated as follows:
EM2[[q ⊃(r ∧s)]] = (W2 −EM2[[q]])∪EM2[[r ∧s]]
= (W2 −I2(q))∪(EM2[[r]]∩EM2[[s]])
= (W2 −{A,B,D})∪(I2(r)∩I2(s))
= {C}∪(/0∩{A,B,C,D})
= {C}∪/0
= {C}.
♦
Access-Control Operators
The access-control operators of the logic (e.g., says , controls , and ⇒) have more
interesting semantics.
Says: A formula P says ϕ is meant to denote a situation in which the principal P
makes the statement ϕ. Intuitively, a principal should make statements that
they believe to be true. What does it mean for a principal to believe a statement
is true in a given world? The standard answer is that a principal P believes ϕ
to be true in a speciﬁc world w if ϕ is true in all of the worlds w′ that P can
conceive the current world to be (i.e., all w′ such that (w,w′) is in J(P)). Of
course, this set of possible worlds is simply the set J(P)(w); ϕ is true in every
world in J(P)(w) if and only if J(P)(w) ⊆EM [[ϕ]]. Therefore, we deﬁne
EM [[P says ϕ]] = {w | J(P)(w) ⊆EM [[ϕ]]}.
Controls: Formulas of the form P controls ϕ express a principal P’s jurisdiction or
authority regarding the statement ϕ. We interpret P controls ϕ as syntactic sugar
for the statement (P says ϕ) ⊃ϕ, which captures the desired intuition: if the
authority P says that ϕ is true, then ϕ is true. Thus, we give the meaning of
P controls ϕ directly as the meaning of this rewriting:
EM [[P controls ϕ]] = EM [[(P says ϕ) ⊃ϕ]].
Speaks For: To understand the semantics of formulas with form P ⇒Q, recall the
purpose of such formulas: we wish to express a proxy relationship between P
and Q that will permit us to safely attribute P’s statements to Q as well, inde-
pendent of a particular world. That is, if P ⇒Q, then it should be reasonable
to interpret any statement from P as being a statement that Q would also make.
In terms of the semantics, we have seen that a principal P making a statement
ϕ in a world w means that J(P)(w) ⊆EM [[ϕ]]. Thus, if we wish to associate
all of P’s statements to Q, then we need to know that J(Q)(w) ⊆J(P)(w) for

32
Access Control, Security, and Trust: A Logical Approach
all worlds w. If J(Q) ⊆J(P), then this relationship naturally holds. Therefore,
we deﬁne
EM [[P ⇒Q]] =
(
W,
if J(Q) ⊆J(P)
/0,
otherwise.
The following examples illustrate these semantic deﬁnitions.
Example 2.14
Recall M0 = ⟨W0,I0,J0⟩from Example 2.7. The set of worlds in W0 in which the for-
mula Hal says g is true is given by EM0[[Hal says g]], which is calculated as follows:
EM0[[Hal says g]] = {w | J0(Hal)(w) ⊆EM0[[g]]}
= {w | J0(Hal)(w) ⊆{sw}}
= {sw,sc}.
This result captures Hal’s mistaken belief that, whenever it is sunny (i.e., when the
current world is either sw or sc), the children will be able to go outside.
In contrast, recall that Flo is unable to distinguish the two worlds sw and sc.
Speciﬁcally, the relation J0(Flo) has the following properties:
J0(Flo)(sw) = {sw,sc},
J0(Flo)(sc) = {sw,sc},
J0(Flo)(ns) = {ns}.
Thus, the worlds in which Flo says g is true can be calculated as follows:
EM0[[Flo says g]] = {w | J0(Flo)(w) ⊆EM0[[g]]}
= {w | J0(Flo)(w) ⊆{sw}}
= /0.
That is, there are no worlds in which Flo is convinced that the children will be able
to go outside.
♦
Example 2.15
Recall M1 = ⟨W1,I1,J1⟩from Examples 2.8 and Example 2.12. The set of worlds in
W1 in which the formula Alice says (q ⊃(r∧s)) is true is given by EM1[[Alice says (q ⊃
(r ∧s))]], calculated as follows:
EM1[[Alice says (q ⊃(r ∧s))]] = {w | J1(Alice)(w) ⊆EM1[[q ⊃(r ∧s)]]}
= {w | J1(Alice)(w) ⊆{w1}}
= {w1}.
This result is not surprising, because Alice had perfect knowledge of the separate
worlds: thus, she believes q ⊃(r ∧s) to be true in precisely those worlds in which it
is true.
♦

A Language for Access Control
33
Example 2.16
Consider the same Kripke structure M1 = ⟨W1,I1,J1⟩as in the previous example, but
now we investigate Bob’s view of the situation. Recall the following details about
J1(Bob):
J1(Bob)(w0) = {w0,w1},
J1(Bob)(w1) = {w2},
J1(Bob)(w2) = {w1}.
The set of worlds in W1 in which the formula Bob says (q ⊃(r ∧s)) is true is given
by EM1[[Bob says (q ⊃(r ∧s))]], which is calculated as follows:
EM1[[Bob says (q ⊃(r ∧s))]] = {w | J1(Bob)(w) ⊆EM1[[q ⊃(r ∧s)]]}
= {w | J1(Bob)(w) ⊆{w1}}
= {w2}.
♦
As remarked at the beginning of this section, a particular Kripke structure M =
⟨W,I,J⟩satisﬁes a formula ϕ, written M |= ϕ, if and only if ϕ is true of every world
in W (i.e., if EM [[ϕ]] = W). The following examples illustrate this concept.
Example 2.17
Recall the Kripke structure M1 = ⟨W1,I1,J1⟩from Examples 2.8 and 2.12. M1 satis-
ﬁes the formula q∨r, but not the formula q ⊃(r ∧s):
EM1[[q∨r]] = {w0,w1,w2} = W1,
EM1[[q ⊃(r ∧s)]] = {w1} ̸= W1.
Thus, we write M1 |= q∨r and M1 ̸|= q ⊃(r ∧s).
♦
Example 2.18
The same Kripke structure M1 = ⟨W1,I1,J1⟩from Examples 2.8 and 2.12 also satis-
ﬁes the formula Alice controls (q ⊃(r ∧s)), because:
EM1[[Alice controls (q ⊃(r ∧s))]]
= EM1[[(Alice says (q ⊃(r ∧s))) ⊃(q ⊃(r ∧s))]]
= (W1 −EM1[[Alice says (q ⊃(r ∧s))]])∪EM1[[q ⊃(r ∧s)]]
= (W1 −{w1})∪{w1}
= W1.
Therefore, we can write M1 |= Alice controls (q ⊃(r ∧s)).
♦

34
Access Control, Security, and Trust: A Logical Approach
The following exercises provide opportunities to use the Kripke semantics to cal-
culate the meanings of access-control formulas, to identify structures that satisfy
given formulas, and to prove useful properties about the semantics.
Exercise 2.3.2
Suppose M is the Kripke structure ⟨W,I,J⟩, where W, I, and J are

deﬁned as follows:
W = {w0,w1,w2}
I(s) = {w1,w2}
I(t) = {w2}
J(Cy) = {(w1,w0),(w1,w1),(w2,w0)}
J(Di) = {(w0,w1),(w1,w0),(w2,w2)}.
For each of the following formulas, give the set of worlds in W for which the
formula is true (i.e., calculate EM [[ϕ]] for each formula ϕ).
a. s ⊃t
b. ¬(s ⊃t)
c. Cy says (s ⊃t)
d. Cy says ¬(s ⊃t)
e. Di says (s ⊃t)
f. Di says ¬(s ⊃t)
g. (Cy & Di) says (s ⊃t)
h. (Cy & Di) says ¬(s ⊃t)
i. (Di | Cy) says (s ⊃t)
j. (Di | Cy) says ¬(s ⊃t)
k. Di ⇒Cy
l. Cy says (Di ⇒Cy)
m. Di says (Di ⇒Cy)
n. Di says (Di & Cy ⇒Cy)
Exercise 2.3.3
Let M be the Kripke structure ⟨W,I,J⟩, where W, I, and J are

deﬁned as follows:
• W = {t,u,v,x,y,z}

A Language for Access Control
35
• I : PropVar →2W given by:
I(p) = {x,y,z}
I(q) = {x,y,t}
I(r) = {y,t,u,z}
• J : PName →2W×W given by:
J(A) = {(w,w) | w ∈W}∪{(x,y),(x,z),(z,t),(y,v),(v,y),(v,x)}
J(B) = {(x,w) | w ∈W}∪{(y,t),(z,t),(t,v)}.
Calculate each of the following sets.
a. EM [[(p ⊃q) ⊃r]]
b. EM [[A says (p ⊃r)]]
c. EM [[A says (B says q)]]
d. EM [[B says (B says q)]]
e. EM [[A controls (B says q)]]
f. EM [[A controls (B controls q)]]
Exercise 2.3.4
Let M be the Kripke structure ⟨W,I,J⟩, where
W = {n,r,c,d,rc,rd,cd,rcd}
I(read) = {r,rc,rd,rcd}
I(copy) = {c,rc,cd,rcd}
I(del) = {d,rd,cd,rcd}
J(X) = {(w,rcd) | w ∈W}
J(Y) = idW
J(Z) = {(w,n) | w ∈W}.
a. Prove that M |= Y controls (read ∧copy).
b. Prove that M ̸|= X controls del.
Exercise 2.3.5
Let A = ⟨WA,IA,JA⟩and B = ⟨WB,IB,JB⟩be the Kripke structures

36
Access Control, Security, and Trust: A Logical Approach
deﬁned by:
WA = {a1,a2,a3}
IA(p) = {a3}
IA(q) = {a1,a3}
IA(r) = {a1,a2}
JA(Val) = {(a1,a1),(a1,a2),(a2,a1),(a3,a2)}
JA(Wyn) = {(a1,a3),(a2,a3),(a3,a2)}.
WB = {b1,b2,b3,b4}
IB(p) = {b2,b1}
IB(q) = {b3}
IB(r) = {b2,b4,b1}
JB(Val) = {(b1,b2),(b2,b3),(b2,b4),(b4,b4)}
JB(Wyn) = {(b1,b1),(b2,b1),(b3,b2),(b3,b4),(b4,b2)}.
Which of the following statements are true? Support your answers with calcula-
tions using the evaluation functions EA[[−]] and EB[[−]].
a. A |= p∨(q ⊃r)
b. B |= p∨(q ⊃r)
c. A |= Val ⇒Wyn
d. B |= Val ⇒Wyn
e. A |= Val | Wyn says r
f. B |= Val | Wyn says r
g. A |= Wyn controls (p∧r)
h. B |= Wyn controls (p∧r)
Exercise 2.3.6
For each of the following formulas ϕ, ﬁnd Kripke structures Mx and
My such that Mx |= ϕ and My ̸|= ϕ.
a. Ned says (p ≡q)
b. Olaf controls q
c. Pam says (Rue controls r)
d. Sal controls (Tom ⇒Ugo)

A Language for Access Control
37
Exercise 2.3.7
Prove that, for any Kripke structure M = ⟨W,I,J⟩, principal P, and
formulas ϕ1 and ϕ2, the following relationship holds:
EM [[P says (ϕ1 ≡ϕ2)]]∩EM [[P says ϕ1]] ⊆EM [[P says ϕ2]].
Exercise 2.3.8
Prove that, for every Kripke structure M = ⟨W,I,J⟩, principal P,
and formulas ϕ1 and ϕ2, the following relationship holds:
M |= P says (ϕ1 ≡ϕ2) ⊃(P says ϕ1 ⊃P says ϕ2).
Hint: Exercise 2.3.7 is helpful here.
Exercise 2.3.9
Prove that, for any Kripke structure M = ⟨W,I,J⟩and formulas ϕ1
and ϕ2, EM [[ϕ1 ≡ϕ2]] = W if and only if EM [[ϕ1]] = EM [[ϕ2]]. That is, prove the
following two statements:
a. If EM [[ϕ1 ≡ϕ2]] = W, then EM [[ϕ1]] = EM [[ϕ2]].
b. If EM [[ϕ1]] = EM [[ϕ2]], then EM [[ϕ1 ≡ϕ2]] = W.
2.4
Summary
Providing and maintaining security—in both the physical and digital worlds—
requires us to be able to determine whether or not any given decision to grant access
to resources or services is correct. Unfortunately, natural language expressions have
imprecise meanings. We need a way to unambiguously express the policies, trust
assumptions, recognized authorities, and statements made by various principals and
be able to justify the resulting access-control decisions.
In this chapter, we introduced a language that allows us to express our policies,
trust assumptions, recognized authorities, and statements in a precise and unambigu-
ous way. Expressions in this language are given precise, mathematical meanings
through the use of Kripke structures. This Kripke semantics provides the initial ba-
sis for mathematically justifying access-control decisions: given a Kripke structure
and an expression in the language, we can compute those worlds in which the ex-
pression is true. This semantics will allow us to introduce sound rules for justifying
access-control decisions, as demonstrated in the next chapter.
The learning outcomes associated with this chapter appear in Figure 2.2.
2.5
Further Reading
For a detailed introduction to discrete mathematics and propositional logic, we
direct the interested reader to some of the standard undergraduate textbooks on the

38
Access Control, Security, and Trust: A Logical Approach
FIGURE 2.2 Learning outcomes for Chapter 2
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
• Determine whether or not a given collection of symbols constitutes a
well-formed formula of the access-control logic.
• When given an informal statement in English, translate it into a well-
formed formula of the access-control logic.
• When given a Kripke structure and a formula of the access-control logic,
determine the set of worlds in which the formula is true.
Analysis
• When given a formula and a Kripke structure that satisﬁes it, prove that
the Kripke structure satisﬁes it.
Synthesis
• When given a satisﬁable formula, construct a Kripke structure that satis-
ﬁes the formula.
• When given a refutable formula, construct a Kripke structure that refutes
(i.e., fails to satisfy) the formula.
• When given a general property regarding Kripke structures, prove that
the property holds.
Evaluation
• When given a Kripke structure and a formula, determine whether or not
the Kripke structure satisﬁes the formula.
subject (Rosen, 2003; Ross and Wright, 2002).
The access-control logic presented here is based on the logic of Abadi, Lampson,
and colleagues (Lampson et al., 1992; Abadi et al., 1993), which in turn was built
on top of a standard modal logic. Modal logic is the logical study of necessity and
possibility, and Hughes and Cresswell provide an excellent introduction to the ﬁeld
(Hughes and Cresswell, 1996).

Chapter 3
Reasoning about Access Control
In the previous chapter, we introduced a language for describing access-control sce-
narios. We presented the syntax for well-formed statements in the language (i.e.,
formulas). We also speciﬁed their semantics through the use of Kripke structures.
Although Kripke structures provide precise meanings for these statements, it is not
practical to analyze access-control situations at such a low level. First of all, Kripke
structures are cumbersome to use for even simple situations. Second, in any given
situation, it is unclear which structures accurately capture the state of the universe.
How could we possibly know which Kripke structure to use for any given analysis?
In this chapter, we introduce a collection of inference rules that will provide the
basis for reasoning rigorously about access control. These rules describe a system
for manipulating well-formed formulas as a way of calculating the consequences of
various assumptions. A crucial property for these rules is that they must be sound
with respect to the Kripke-structure semantics. That is, the rules should ensure that,
in any situation where a given Kripke structure satisﬁes all of a rule’s premises, the
Kripke structure also satisﬁes the rule’s consequent. Informally, soundness means
that it is impossible to deduce a “false” statement from “true” ones. In this chapter,
we demonstrate how to verify that the rules we introduce are sound.
3.1
Logical Rules
The logical rules are summarized in Figure 3.1. Each rule of the logic has the form
H1
···
Hk
C
,
where the items written above the line (e.g., H1,...Hk) correspond to hypotheses (or
premises) and the item below the line (e.g., C) corresponds to a consequence (or
conclusion). A special case occurs when there are no hypotheses on the top of the
rule (i.e., when k = 0): an inference rule with an “empty top” is called an axiom.
Informally, we often read such rules as “If all the assertions on the top are true,
then the consequence below the line will also be true.” More accurately, however, the
logical rules describe a system for manipulating well-formed formulas of the logic.
In fact, one can think of the logical rules as deﬁning a game, in which a player writes
39

40
Access Control, Security, and Trust: A Logical Approach
FIGURE 3.1 Logical rules for the access-control logic
Taut
ϕ
if ϕ is an instance of a prop-logic tautology
Modus Ponens
ϕ
ϕ ⊃ϕ′
ϕ′
Says
ϕ
P says ϕ
MP Says
(P says (ϕ ⊃ϕ′)) ⊃(P says ϕ ⊃P says ϕ′)
Speaks For
P ⇒Q ⊃(P says ϕ ⊃Q says ϕ)
& Says
(P & Q says ϕ) ≡((P says ϕ)∧(Q says ϕ))
Quoting
(P | Q says ϕ) ≡(P says Q says ϕ)
Idempotency of ⇒
P ⇒P
Transitivity
of ⇒
P ⇒Q
Q ⇒R
P ⇒R
Monotonicity
of ⇒
P ⇒P′
Q ⇒Q′
P | Q ⇒P′ | Q′
Equivalence
ϕ1 ≡ϕ2
ψ[ϕ1/q]
ψ[ϕ2/q]
P controls ϕ
def
=
(P says ϕ) ⊃ϕ

Reasoning about Access Control
41
FIGURE 3.2 Common propositional-logic tautologies
p∨¬p
p ≡(¬¬p)
p ⊃(q∨p)
p ⊃(q ⊃p)
(p∧q) ⊃p
¬(¬p∧p)
p ⊃(q ⊃(p∧q))
(p∧q) ⊃(p ⊃q)
(p∧q) ⊃(q∧p)
(p ≡q) ⊃(p ⊃q)
((p∨q)∧¬p) ⊃q
((p ⊃q)∧(q ⊃r)) ⊃(p ⊃r)
(i.e., derives) various formulas on a piece of paper. Each rule states that, if all the
premises of an inference rule have already been written down (derived), then the
conclusion can also be written down (derived). Axioms can always be written down.
We return to this notion of derivations in Section 3.2, where we introduce formal
proofs. For now, however, we discuss each of the logical rules in turn.
3.1.1
The Taut Rule
The simplest rule is the axiom Taut:
Taut
ϕ
if ϕ is an instance of a prop-logic tautology
This axiom states that any instance of a tautology from propositional logic can
be introduced at any time as a derivable statement in the access-control logic. To
understand what this rule means, ﬁrst recall that a propositional-logic tautology is a
formula that evaluates to true under all possible interpretations of its propositional
variables. For example, the propositional formula p ∨¬p always evaluates to true,
independent of whether the propositional variable p is assigned the value true or the
value false. In contrast, the formula p ⊃¬p is not a tautology, because it evaluates to
false whenever p is assigned the value true. Although it does not constitute a com-
plete listing, Figure 3.2 summarizes some common propositional-logic tautologies.
A formula ϕ is an instance of the formula ψ if there exist propositional variables
p1,..., pk (for some k ≥0) and modal formulas ϕ1,...,ϕk such that ϕ is obtained by
replacing each pi in ψ by ϕi. For example, the formula
(Alice says go)∨((sit ∧read) ⊃(Alice says go))
is an instance of the formula q∨(r ⊃q): it can be obtained by replacing every q by
(Alice says go) and every r by (sit ∧read). In contrast, the formula
(Alice says go)∨((sit ∧read) ⊃stay)
is not an instance of the formula q ∨(r ⊃q), because the two separate occurrences
of q were not replaced by the same formula.

42
Access Control, Security, and Trust: A Logical Approach
Thus, the Taut rule allows us to introduce (i.e., derive) any formula that can be
obtained from a propositional-logic tautology by a uniform substitution as described
above. For example, since p∨¬p is a tautology, we can derive the following formula:
(Paul controls (read ∧write))∨¬(Paul controls (read ∧write))
3.1.2
The Modus Ponens Rule
Another common rule is Modus Ponens:
Modus Ponens
ϕ
ϕ ⊃ϕ′
ϕ′
This rule states that, if both the implication ϕ1 ⊃ϕ2 and the formula ϕ1 have been
previously introduced, then we can also introduce the formula ϕ2.
For example, if we have previously derived the two formulas
(Bill says sell) ⊃buy
and
Bill says sell,
then we can use the Modus Ponens rule to also derive buy.
3.1.3
The Says Rule
The Says rule is deﬁned as follows:
Says
ϕ
P says ϕ
Informally, this rule states that any principal can make any statement (or safely be as-
sumed to have made any statement) that has already been derived. Thus, for example,
if we have previously derived (read ∧copy), then we can derive Cara says (read ∧
copy).
3.1.4
The MP Says Rule
The MP Says axiom serves as a version of modus ponens for statements made by
principals:
MP Says
(P says (ϕ ⊃ϕ′)) ⊃(P says ϕ ⊃P says ϕ′)
In effect, this rule allows us to distribute the says operator over implications. For
example, this axiom allows us to derive the following formula:
(Graham says (sit ⊃eat)) ⊃((Graham says sit) ⊃(Graham says eat)).

Reasoning about Access Control
43
3.1.5
The Speaks For Rule
The Speaks For axiom is deﬁned as follows:
Speaks For
P ⇒Q ⊃(P says ϕ ⊃Q says ϕ)
In effect, this rule captures our intuition about the speaks-for relation. It states that,
if P speaks for Q, then any statements P makes should also be attributable to Q. For
example, this axiom allows us to derive the following statement:
Del ⇒Ed ⊃((Del says buy) ⊃(Ed says buy)).
Were we to subsequently to derive the formula Del ⇒Ed, the Modus Ponens rule
would let us also derive the formula
(Del says buy) ⊃(Ed says buy).
3.1.6
The & Says and Quoting Rules
There are two rules that relate statements made by compound principals to those
made by simple principals. The ﬁrst of these is the & Says rule:
& Says
(P & Q says ϕ) ≡((P says ϕ)∧(Q says ϕ))
This rule reﬂects the conjunctive nature of a principal P&Q: the statements made
by the compound principal P&Q (i.e., P in conjunction with Q) are precisely those
statements that both P and Q are willing to make individually. For example, the
& Says rule allows us to derive the following formula:
(Faith & Gale says sing) ≡((Faith says sing)∧(Gale says sing)).
The second such rule is the Quoting rule:
Quoting
(P | Q says ϕ) ≡(P says Q says ϕ)
This rule captures the underlying intuition behind the compound principal P | Q:
the statements made by P | Q (i.e., P quoting Q) are precisely those statements that
P claims Q has made. As an example, here is an instance of the Quoting rule:
(Iona | J¨urgen says vote for Kory) ≡(Iona says J¨urgen says vote for Kory).
3.1.7
Properties of ⇒
There are three rules that relate to properties of the ⇒relation, namely idempo-
tency, transitivity, and monotonicity. These rules are all quite simple, but they are

44
Access Control, Security, and Trust: A Logical Approach
useful for analyzing situations that involve chains of principals speaking for one an-
other.
The Idempotency of ⇒rule states that every principal speaks for itself:
Idempotency of ⇒
P ⇒P
Thus, for example, with this rule we can derive the following formula:
Wallace ⇒Wallace.
Although this rule may seem both obvious and unnecessary, it can be useful in con-
junction with monotonicity to reason about quoting principals, as illustrated later in
this subsection.
The Transitivity of ⇒rule supports reasoning about chains of principals that rep-
resent one another:
Transitivity of ⇒
P ⇒Q
Q ⇒R
P ⇒R
This rule states that, if one principal speaks for a second and the second also speaks
for a third, then it is also safe to view the ﬁrst principal as speaking for the third
principal. For example, if we have previously derived the two formulas
Kanda ⇒Theo,
Theo ⇒Vance,
then the Transitivity of ⇒rule allows us to also derive
Kanda ⇒Vance.
Finally, the Monotonicity of ⇒rule states that quoting principals preserve the
speaks-for relationship:
Monotonicity of ⇒
P ⇒P′
Q ⇒Q′
P | Q ⇒P′ | Q′
As an example, suppose that we have already derived the following two formulas:
Lowell ⇒Minnie,
Norma ⇒Orson.
The Monotonicity of ⇒rule allows us to also derive the formula
Lowell | Norma ⇒Minnie | Orson.
To see the utility of idempotency, consider the case where we have already derived
the formula Penny ⇒Ronald, and we would like to derive that
Penny | Sylvester ⇒Ronald | Sylvester.
By ﬁrst using idempotency to derive Sylvester ⇒Sylvester, we can use monotonicity
to derive the desired formula.

Reasoning about Access Control
45
3.1.8
The Equivalence Rule
The Equivalence rule allows one to replace subformulas in a formula with equiv-
alent subformulas:
Equivalence
ϕ1 ≡ϕ2
ψ[ϕ1/q]
ψ[ϕ2/q]
To understand this rule, one must ﬁrst understand the meta-notation ψ[ϕ/q], which
denotes the result of replacing every occurrence of the propositional variable q within
the formula ψ by the formula ϕ. For example,
(t ⊃(P says (r ∧t)))[Q says s/t]
is simply the formula
(Q says s) ⊃(P says (r ∧Q says s)).
Thus, the equivalence rule states that, if formulas ϕ1 and ϕ2 are equivalent, then
every occurrence of ϕ1 in a formula ψ[ϕ1/q] can be replaced by ϕ2, resulting in the
formula ψ[ϕ2/q]. For example, suppose that we have already derived the following
two formulas:
s∧t ≡Tom says r,
Arnie | May controls (s∧t).
Because the latter formula is equivalent to (Arnie | May controls p)[(s ∧t)/p], the
Equivalence rule allows us to derive the formula
Arnie | May controls (Tom says r).
In fact, by choosing ψ and the propositional variable q judiciously, one can also
use the Equivalence rule to replace only some of the occurrences of ϕ1 with ϕ2. As
an example, suppose that we have previously derived the following two formulas:
(t ⊃r) ≡P says w,
((R says (t ⊃r)) ∧(t ⊃r)) ⊃(T | R controls (t ⊃r)).
The latter formula can be obtained by any of the following substitutions (among
others):
(((R says (t ⊃r)) ∧q) ⊃(T | R controls (t ⊃r)))[t ⊃r/q]
(((R says (t ⊃r)) ∧q) ⊃(T | R controls q))[t ⊃r/q]
(((R says q) ∧q) ⊃(T | R controls q))[t ⊃r/q].
Consequently, the Equivalence rule would allow us to derive any of the following
formulas:
(((R says (t ⊃r)) ∧q) ⊃(T | R controls (t ⊃r)))[P says w/q]
(((R says (t ⊃r)) ∧q) ⊃(T | R controls q))[P says w/q]
(((R says q) ∧q) ⊃(T | R controls q))[P says w/q].

46
Access Control, Security, and Trust: A Logical Approach
Exercise 3.1.1
Calculate the results of each of the following substitutions.

a. ((t ⊃r)∨(P says r))[Q controls t/r]
b. w ⊃P says t[P | Q says r/w]
c. ((s∧(Q says s)) ⊃Q says p)[P says q/s]
d. ((s∧(Q says t)) ⊃Q says p)[P says q/s]
Exercise 3.1.2
For each pair of formulas (ϕ,ϕ′) given below, give formulas ψ,ϕ1,ϕ2

such that ψ[ϕ1/q] is ϕ and ψ[ϕ2/q] is ϕ′. The formula ψ should represent as much
shared structure as possible between ϕ and ϕ′, allowing ϕ1 and ϕ2 to be as simple
as possible.
a. ϕ = (P & Q says (s ⊃t)) ⊃t
ϕ′ = (P | Q says w) ⊃t
b. ϕ = (Q controls (P controls t))∧(P | Q says t)
ϕ′ = (Q controls (P controls t))∧((P controls (Q controls t)) ⊃t)
c. ϕ = ((R says s)∧(Q says R says s)) ⊃(Q controls (R says s))
ϕ′ = ((R says s)∧(Q says R says t)) ⊃(Q controls (R says t))
d. ϕ = ((R says s)∧(Q says R says s)) ⊃(Q controls (R says s))
ϕ′ = ((T controls w)∧(Q says T controls w)) ⊃(Q controls (T controls w))
3.1.9
The Controls Deﬁnition
Finally, the following deﬁnition1 governs the use of controls in our logic:
P controls ϕ
def
=
(P says ϕ) ⊃ϕ
This deﬁnition states that a formula of the form P controls ϕ is syntactic sugar for
the longer expression (P says ϕ) ⊃ϕ. That is, controls doesn’t give our logic any
additional expressiveness, but it provides a useful way to make more explicit what
will turn out to be a common idiom. This deﬁnition means that, any time we see an
expression of form P controls ϕ, we can replace it by (P says ϕ) ⊃ϕ, and vice versa.
This deﬁnition matches the semantics we deﬁned for our logic in Section 2.3.2: we
deﬁned the meaning of P controls ϕ to be the meaning of (P says ϕ) ⊃ϕ.
As an example of the use of this deﬁnition, we can replace any occurrence of
the formula Lily controls read—even within the context of a larger formula—by the
1Note that def
= is meta-notation, rather than a part of the logic’s syntax. The net effect, however, of this
deﬁnition is the same as if we introduced an axiom stating P controls ϕ ≡((P says ϕ) ⊃ϕ), in that either
side of the deﬁnition may safely be replaced by the other in any formula.

Reasoning about Access Control
47
FIGURE 3.3 A simple formal proof
1. Al says (r ⊃s)
Assumption
2. r
Assumption
3. (Al says (r ⊃s)) ⊃(Al says r ⊃Al says s)
MP Says
4. Al says r ⊃Al says s
1,3 Modus Ponens
5. Al says r
2 Says
6. Al says s
4,5 Modus Ponens
FIGURE 3.4 A formal proof of the Controls rule
1. P controls ϕ
Assumption
2. P says ϕ
Assumption
3. (P says ϕ) ⊃ϕ
1 Defn controls
4. ϕ
2,3 Modus Ponens
formula (Lily says read) ⊃read, and vice versa. Thus, for example, the Controls
deﬁnition allows us to replace the formula
Manny says (Lily controls read)
by the formula
Manny says ((Lily says read) ⊃read).
3.2
Formal Proofs and Theorems
A formal proof is a sequence of statements of the logic, where each statement is
either an assumption or a statement that can be derived by applying one of the infer-
ence rules (or deﬁnitions) to previous statements in that sequence. It is customary
to sequentially number each of these statements, and to label them either with “As-
sumption” or with the statement numbers and inference-rule name by which it was
deduced. In this book, we will always place all assumptions at the beginning of the
proof, so that it is quick and easy to determine the premises upon which a conclusion
depends.
For example, Figure 3.3 presents a simple formal proof. In this case, only the
ﬁrst two statements in the proof are assumptions; every other statement is either an
instance of axiom (e.g., Step 3) or a consequence of applying one of the inference
rules. As another simple example, Figure 3.4 demonstrates how the deﬁnition of
the controls operator can be used in a formal proof.
Every formal proof represents a theorem, which is really just a derived infer-
ence rule. Speciﬁcally, if the only assumptions of the formal proof are statements

48
Access Control, Security, and Trust: A Logical Approach
H1,...Hk, and if the ﬁnal statement of the proof is C, then the proof corresponds to
a derived inference rule with the following form:
H1
···
Hk
C
.
Thus, the formal proofs in Figure 3.3 and Figure 3.4 correspond respectively to
the following two derived theorems2:
Al says (r ⊃s)
r
Al says s
P controls ϕ
P says ϕ
ϕ
.
These theorems can now be used as additional inference rules in any future proof,
without affecting that proof’s validity. We shall ﬁnd the latter derived rule very useful
in subsequent chapters, and hence we give it a speciﬁc name (Controls). In fact,
there are several derived rules—some from propositional logic and others related
to access control—that we will ﬁnd convenient to have in our arsenal. For easy
reference, we summarize those rules (along with Controls) in Figure 3.5, leaving
most of their proofs as exercises for the reader. However, Figure 3.6 gives a proof
for the Conjunction rule, which also demonstrates the general form for many of the
other proofs.
Exercise 3.2.1
Give a formal proof of the Hypothetical Syllogism derived rule from

Figure 3.5.
Exercise 3.2.2
Technically speaking, the Equivalence rule given in Figure 3.1 per-

mits replacements in only direction: having deduced ϕ1 ≡ϕ2, one can replace oc-
currences of ϕ1 in a formula by ϕ2, but not vice versa.
Give a formal proof of the following derived rule which permits replacements in
the opposite direction3:
ϕ1 ≡ϕ2
ψ[ϕ2/q]
ψ[ϕ1/q]
.
Exercise 3.2.3
Give a formal proof for the Derived Speaks For rule given in Fig-

ure 3.5.
Exercise 3.2.4
Give a formal proof for the Derived Controls rule given in Fig-

ure 3.5.
Exercise 3.2.5
Give a formal proof for the Says Simpliﬁcation derived rules given

in Figure 3.5.
Exercise 3.2.6
Give a formal proof for the following derivable inference rule:

2Technically, the proof of Figure 3.4 is a proof schema that represents a theorem schema, in that P and ϕ
are meta-variables that range over (respectively) all possible principal expressions and formulas. Thus,
it provides a recipe for any particular instances of P and ϕ. In this book, we shall blur the distinction
between proofs and proof schemas, as well as theorems and theorem schemas.
3Henceforth in this book, we shall not distinguish between these two versions of Equivalence.

Reasoning about Access Control
49
FIGURE 3.5 Some useful derived rules
Conjunction
ϕ1
ϕ2
ϕ1 ∧ϕ2
Simpliﬁcation (1)
ϕ1 ∧ϕ2
ϕ1
Simpliﬁcation (2)
ϕ1 ∧ϕ2
ϕ2
Disjunction (1)
ϕ1
ϕ1 ∨ϕ2
Disjunction (2)
ϕ2
ϕ1 ∨ϕ2
Modus Tollens
ϕ1 ⊃ϕ2
¬ϕ2
¬ϕ1
Double negation
¬¬ϕ
ϕ
Disjunctive
Syllogism
ϕ1 ∨ϕ2
¬ϕ1
ϕ2
Hypothetical
Syllogism
ϕ1 ⊃ϕ2
ϕ2 ⊃ϕ3
ϕ1 ⊃ϕ3
Controls
P controls ϕ
P says ϕ
ϕ
Derived
Speaks For
P ⇒Q
P says ϕ
Q says ϕ
Derived
Controls
P ⇒Q
Q controls ϕ
P controls ϕ
Says
Simpliﬁcation (1)
P says (ϕ1 ∧ϕ2)
P says ϕ1
Says
Simpliﬁcation (2)
P says (ϕ1 ∧ϕ2)
P says ϕ2
FIGURE 3.6 A formal proof of Conjunction
1. ϕ1
Assumption
2. ϕ2
Assumption
3. ϕ1 ⊃(ϕ2 ⊃(ϕ1 ∧ϕ2))
Taut
4. ϕ2 ⊃(ϕ1 ∧ϕ2)
1,3 Modus Ponens
5. ϕ1 ∧ϕ2
2,4 Modus Ponens

50
Access Control, Security, and Trust: A Logical Approach
P says ϕ1
P ⇒Q
Q says (ϕ2 ⊃ϕ1)
.
Exercise 3.2.7
Give a formal proof for the following derivable inference rule:

P says (Q controls ϕ)
P | Q says ϕ
P says ϕ
.
Exercise 3.2.8
Give a formal proof for the following derivable inference rule:

(P | Q) controls ϕ
R ⇒Q
P says R says ϕ
ϕ
.
3.3
Soundness of Logical Rules
The rules presented in Section 3.1 are merely rules for manipulating formulas. Up
to this point, we have no reason to believe in their logical consistency, which makes
using them a risky prospect. To show that the inference rules are consistent, we must
ﬁrst use the Kripke-structure semantics to prove their soundness.
Speciﬁcally, an inference rule
H1
···
Hk
C
is sound with respect to the Kripke semantics provided that, whenever a Kripke struc-
ture M satisﬁes all of the rule’s hypotheses (i.e., H1,...Hk), it also satisﬁes the rule’s
consequent. It follows that an inference rule is not sound if there exists even just a
single Kripke structure M = ⟨W,I,J⟩such that, for each Hi, M |= Hi and yet M ̸|=C.
We now use the Kripke semantics given in Figure 2.1 to prove the soundness of
the inference rules Taut, Modus Ponens, and Says; we also provide a proof sketch for
the soundness of Equivalence. The proofs of soundness for the remaining rules are
left as exercises for the reader.
Example 3.1 (Soundness of Taut)
Let ϕ be an instance of a propositional-logic tautology ψ. We need to show that, for
every Kripke structure M = ⟨W,I,J⟩, ϕ is true in every world in W (i.e., EM [[ϕ]] =
W).
Let Ma = ⟨Wa,Ia,Ja⟩be an arbitrary Kripke structure. Because ϕ is an instance of
ψ, there exist propositional variables p1,..., pk and modal formulas ϕ1,...,ϕk such
that ϕ is obtained from ψ by replacing each pi by ϕi. Without loss of generality, we
assume that the variables p1,..., pk do not appear in ϕ.
We now construct a new model M ′ = ⟨Wa,I′,Ja⟩, where I′ is deﬁned as follows:
for each pi (1 ≤i ≤k), I′(pi) = EMa[[ϕi]]; for all other propositional variables q,
I′(q) = I(q). Thus, for each 1 ≤i ≤k,
EM ′[[pi]] = EMa[[ϕi]];

Reasoning about Access Control
51
likewise, for all other propositional variables q,
EM ′[[q]] = EMa[[q]].
From these facts, it is straightforward to show by induction on the structure of ψ that
EM ′[[ψ]] = EMa[[ϕ]].
Because ψ is a propositional-logic tautology, ψ is true in all worlds, independent of
the interpretation of its propositional variables. Therefore, EM ′[[ψ]] = Wa, and thus
EMa[[ϕ]] = Wa as well.
Because Ma was arbitrary, we have shown that the Taut rule is sound.
♦
Example 3.2 (Soundness of Modus Ponens)
To prove that the Modus Ponens inference rule is sound, we must prove the following,
for all formulas ϕ and ϕ′, and for all Kripke structures M = ⟨W,I,J⟩:
If M |= ϕ and M |= ϕ ⊃ϕ′, then M |= ϕ′.
That is, we need to show that, whenever EM [[ϕ]] = W and EM [[ϕ ⊃ϕ′]] = W, it is
also the case that EM [[ϕ′]] = W.
Therefore, we consider an arbitrary model Ma = ⟨Wa,Ia,Ja⟩for which EMa[[ϕ]] =
Wa and EMa[[ϕ ⊃ϕ′]] =Wa, and we will show that EMa[[ϕ′]] =Wa necessarily follows.
Working straight from the deﬁnition of the evaluation functions EM [[−]] given in
Figure 2.1, we see that
EMa[[ϕ ⊃ϕ′]] = (Wa −EMa[[ϕ]])∪EMa[[ϕ′]]
= (Wa −Wa)∪EMa[[ϕ′]]
= EMa[[ϕ′]].
Thus, EMa[[ϕ′]] = EMa[[ϕ ⊃ϕ′]] = Wa. Because Ma was arbitrary, we have shown
that the Modus Ponens rule is sound.
♦
Example 3.3 (Soundness of Says)
To prove that the inference rule Says is sound, we must prove the following, for all
Kripke structures M = ⟨W,I,J⟩:
If M |= ϕ, then (for all principals P) M |= P says ϕ.
That is, we need to show that whenever EM [[ϕ]] = W, it is also the case that (for
every principal P) EM [[P says ϕ]] = W.

52
Access Control, Security, and Trust: A Logical Approach
As before, we start by considering an arbitrary Kripke structure Ma = ⟨Wa,Ia,Ja⟩
that satisﬁes the formula ϕ (i.e., for which EMa[[ϕ]] = Wa). We also let Q be an
arbitrary principal. From the semantic deﬁnitions of Figure 2.1, we see that
EMa[[Q says ϕ]] = {w | Ja(Q)(w) ⊆EMa[[ϕ]]} = {w | Ja(Q)(w) ⊆Wa}.
Because Ja(Q) is by deﬁnition a subset of Wa ×Wa, every w′ ∈Wa satisﬁes the con-
straint that Ja(Q)(w′) ⊆Wa, and thus EMa[[Q says ϕ]] = Wa. Because both Q and Ma
were arbitrary, we have shown that the Says inference rule is sound.
♦
Example 3.4 (Soundness of Equivalence)
To prove that the inference rule Equivalence is sound, we must prove the follow-
ing, for all formulas ϕ1,ϕ2,ψ, propositional variables q, and Kripke structures M =
⟨W,I,J⟩:
If M |= ϕ1 ≡ϕ2 and M |= ψ[ϕ1/q], then M |= ψ[ϕ2/q].
That is, we need to show that whenever EM [[ϕ1 ≡ϕ2]] =W and EM [[ψ[ϕ1/q]]] =W,
it is also the case that EM [[ψ[ϕ2/q]]] = W.
The proof proceeds by a straightforward induction on the structure of ψ, using the
fact from Exercise 2.3.9 that EM [[ϕ1]] = EM [[ϕ2]] whenever EM [[ϕ1 ≡ϕ2]] = W. ♦
Exercise 3.3.1
Prove the soundness of the Speaks For inference rule.
Exercise 3.3.2
Prove the soundness of the & Says inference rule. (Hint: Exer-
cise 2.3.9 is useful here.)
Exercise 3.3.3
Prove the soundness of the Quoting inference rule. (Hint: Exer-
cise 2.3.9 is useful here.)
Exercise 3.3.4
Prove the soundness of the Transitivity of ⇒inference rule.
Exercise 3.3.5
Prove the soundness of the Monotonicity of ⇒inference rule.
Exercise 3.3.6
Consider the following formula, which intuitively states that every
principal has the jurisdiction to select its own proxies:
(P says (Q ⇒P)) ⊃(Q ⇒P).
Prove that this formula would not make for a sound axiom. That is, ﬁnd a partic-
ular Kripke structure M = ⟨W,I,J⟩such that
M ̸|= (P says (Q ⇒P)) ⊃(Q ⇒P).

Reasoning about Access Control
53
Exercise 3.3.7
Consider the following formula, which intuitively states that controls
distributes over conjunction:
(P controls (ϕ1 ∧ϕ2)) ≡((P controls ϕ1)∧(P controls ϕ2)).
Prove that this formula would not make for a sound axiom. That is, ﬁnd particular
formulas ϕ1,ϕ2 and a particular Kripke structure M = ⟨W,I,J⟩such that
M ̸|= (P controls (ϕ1 ∧ϕ2)) ≡((P controls ϕ1)∧(P controls ϕ2)).
Exercise 3.3.8
Consider the following plausible inference rule:
P & Q controls ϕ
P says ϕ
Q controls ϕ
.
Determine whether or not this rule is sound, and justify your answer:
• If you determine that the rule is sound, prove its soundness.
• If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P and Q, and formula ϕ that demonstrate the lack
of soundness.
Exercise 3.3.9
Consider the following plausible inference rule:
P ⇒R
R ⇒Q
R says ϕ ≡P & Q says ϕ.
Determine whether or not this rule is sound, and justify your answer:
• If you determine that the rule is sound, prove its soundness.
• If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P,Q,R and formula ϕ that demonstrate the lack of
soundness.
Exercise 3.3.10
Consider the following plausible inference rule:
P ⇒Q
Q ⇒P
P says ϕ ≡Q says ϕ.
Determine whether or not this rule is sound, and justify your answer:
• If you determine that the rule is sound, prove its soundness.
• If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P,Q,R and formula ϕ that demonstrate the lack of
soundness.

54
Access Control, Security, and Trust: A Logical Approach
Exercise 3.3.11
Consider the following plausible inference rule:
P controls (ϕ1 ∧ϕ2)
P controls ϕ1
.
Determine whether or not this rule is sound, and justify your answer:
• If you determine that the rule is sound, prove its soundness.
• If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P,Q,R and formula ϕ that demonstrate the lack of
soundness.
3.4
Summary
As seen in the previous chapter, Kripke structures provide precise meanings for
the statements of the access-control logic. However, reasoning at the level of Kripke
structures is extremely cumbersome. Furthermore, it is not always possible to know
which Kripke structures accurately represent a particular situation.
To avoid the necessity of reasoning about particular structures, we introduced a
collection of inference rules that are sound. The Kripke-structure semantics provide
the basis for guaranteeing this soundness. The beneﬁts of reasoning with sound
inference rules include the convenience they provide as well as the guarantee they
provide: any situation that satisﬁes the initial assumptions is guaranteed to satisfy
the deduced consequences. We therefore have a basis for answering the question
“Should this access-control request be granted?”, as well as an explicit accounting
for all assumptions on which the analysis depends.
The learning outcomes associated with this chapter appear in Figure 3.7.
3.5
Further Reading
The inference rules of the access-control logic are based on the semantics pre-
sented in the previous chapter. Our semantics is similar to the logic of Abadi, Lamp-
son, and colleagues (Lampson et al., 1992; Abadi et al., 1993). The notion of sound-
ness based on Kripke models is part of standard modal logic. The reader can consult
Hughes and Cresswell (Hughes and Cresswell, 1996) for more on this subject.

Reasoning about Access Control
55
FIGURE 3.7 Learning outcomes for Chapter 3
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Analysis
• When given a set of assumptions and a goal that logically follows from
those assumptions, you should be able to give a formal proof of that goal.
Synthesis
• When given a sound axiom or inference rule for the access-control logic,
you should be able to prove its soundness in the underlying Kripke
model.
• When given a potential inference rule for the access-control logic that is
not sound, you should be able to construct a Kripke structure that demon-
strates its lack of soundness.
Evaluation
• When given a theory, inference rules, and a proof, you should be able to
judge if the proof is correct.
• When given a proposed inference rule, you should be able to judge
whether or not it is sound.


Chapter 4
Basic Concepts
In the preceding two chapters, we introduced a modal logic that we will use through-
out this book to describe and reason about various aspects of access control. This
access-control logic and its collection of inference rules provide a rigorous and for-
mal basis for answering the question “Should this request be granted?”
In learning any language for the ﬁrst time, it’s important to start using the lan-
guage to describe objects and concepts of interest to us. We begin this chapter by
introducing the concept of a reference monitor, which provides a context for the re-
mainder of this chapter. We then develop the fundamental access-control concepts of
tickets, capabilities, and access-control lists. We also discuss various authentication
mechanisms and how they relate to access control. Throughout this chapter, there
are numerous examples to illustrate how the access-control logic is used.
4.1
Reference Monitors
Access control is about guarding objects, assets, resources, services, communi-
cations, ﬁles, databases, information, or people. In everyday life, physical locks of
various forms guard against unauthorized access to all sorts of resources, ranging
from house doors and high-school lockers to automobile ignitions and safety-deposit
boxes. In other situations, a person or machine may require us to provide an artifact
(e.g., subway token or bus pass) to gain access to particular resources. For example,
we need a boarding pass to get on a bus, train, or airplane. We need to have a ticket
before we can get in to see a movie. If we are invited to a formal dinner such as
a wedding reception, there may be a guest list and name cards at each seat in the
banquet hall.
In computer and information systems, the guards are called reference monitors,
and they protect resources such as memory, ﬁles, databases, programs, and commu-
nication channels. Figure 4.1 provides an abstract view of how a reference monitor
ﬁts into a system. Initially, a principal—either a person, machine, or process—makes
a request to the reference monitor to perform some operation on a protected object.
Principals that make requests are often referred to as subjects, and the right to per-
form an operation on an object is typically called an access right or privilege. The
reference monitor’s decision regarding whether or not to grant the request depends on
57

58
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.1 Abstract view of reference monitors
several factors: what is being asked for, potentially the identity and other attributes
of the subject making the request, and ﬁnally the access policy. When a decision
is made, it is recorded in an audit log where a record of all the access requests and
decisions are kept.
All reference monitors must satisfy three essential properties:
1. Completeness: The reference monitor cannot be bypassed.
Completeness means that there is no way to access a protected resource with-
out coming under the scrutiny of the guard or reference monitor. Physically,
completeness means that there is no “hole in the fence.” In information sys-
tems, completeness means that all paths to a state in which access is granted
go through an access controller.
2. Isolation: The reference monitor is tamper proof.
Isolation is primarily satisﬁed by physical protection mechanisms. For ex-
ample, machines on which sensitive information is stored must be housed in
locked and guarded machine rooms where access to the machine rooms is con-
stantly watched.
3. Veriﬁable: The reference monitor is correctly implemented.
The property of being veriﬁed correct means that we must have fully and pre-
cisely speciﬁed what the reference monitor is supposed to do.
Conceptually, the desired access-control behavior of a system can be easily rep-
resented by an access-control matrix, as introduced by Lampson (Lampson, 1971).
The rows of an access-control matrix M are labeled by possible subjects (i.e., princi-
pals), while the columns are labeled by the protected objects. Each entry Ms,o in the
matrix indicates the access rights that subject s possesses with respect to the object
o. As an example, Table 4.1 contains a simple access-control matrix for a system
involving three subjects (with identiﬁers alice, bob, carol) and four protected
objects (ﬁles numbered 1 through 4). According to this table, the principal alice
has the rights to read ﬁle1, to read or write ﬁle2, and to execute ﬁle4; alice has no

Basic Concepts
59
ﬁle1
ﬁle2
ﬁle3
ﬁle4
alice
read
read, write
execute
bob
read
execute
carol
read, write
execute
execute
Table 4.1: Example of an access-control matrix
access rights for ﬁle3. In contrast, bob has no access rights for ﬁle1 or ﬁle4, but can
read ﬁle2 and execute ﬁle3.
When assigning rights to principals, it is wise to follow the principle of least priv-
ilege (Saltzer and Schroeder, 1975): principals should be assigned only the rights
necessary for completing their tasks and no more.
Access-control matrices provide a very simple and straightforward way to de-
scribe systems, particularly those that have relatively small numbers of subjects and
objects. However, they alone are insufﬁcient for reasoning about access control in
more complex systems that involve large numbers of subjects and objects or that per-
mit proxies and delegation. In particular, they do not address issues related to how
subjects are authenticated or the trust assumptions that underlie the authentication
process.
The purpose of the access-control logic is to bridge this gap, to help us to deduce
and to specify what a reference monitor’s behavior should be for any given policy
and request. From a logical point of view, a reference monitor decides whether or
not to approve an access request based on the inference rules of the access-control
logic and within the context of an access policy and trust assumptions.
As we have already seen, we can use the logic to describe a subject’s request to
access a particular object as follows:
Subject says ⟨access right, object⟩.
To grant the request, the reference monitor must be able to determine that the access
policy authorizes the subject to perform the requested operation. In terms of the
logic, this determination amounts to using the inference rules of the logic to derive
the atomic proposition
⟨access right, object⟩
from assumptions about the given policy and any additional trust assumptions that
are relevant to the situation.
Policy statements state who or what has jurisdiction over certain statements, as
well as who or what is authorized with certain rights. These statements usually take
the form
P controls ϕ,
where P is the authority and ϕ is the statement over which P has jurisdiction.
Trust assumptions are usually assumptions about proxies or symbols of authority,
such as the queen’s seal, a principal’s public key, or tickets issued by an airline.

60
Access Control, Security, and Trust: A Logical Approach
These statements usually have the form
P ⇒Q,
where P is the recognized proxy for principal Q. As we will see in the subsequent
sections, these statements express explicitly the reference monitor’s reliance on the
unforgeability of certiﬁcates and credentials.
A certiﬁcate is a signed statement made by a recognized authority, such as a
birth certiﬁcate (signed perhaps by a town’s health department) or a driver’s license
(signed by a state department of motor vehicles). A credential is a certiﬁcate that
asserts a particular principal’s authorization to perform some action; for example, a
driver’s license asserts a given driver’s authorization to drive a certain class of vehi-
cle.
The next section develops the speciﬁcs of requests, policy statements, and trust
assumptions within the context of tickets and access-control lists. Subsequent sec-
tions introduce the notions of certiﬁcates and credentials, particularly as they apply
to both identity-based and group-based authentication.
4.2
Access-Control Mechanisms: Tickets and Lists
As we have already seen, reference monitors guard those objects deemed neces-
sary to protect. There are two common protection schemes employed by reference
monitors: one is based on tickets (or capabilities), while the other depends on access-
control lists.
Tickets signify a capability to access a resource. For example, a ticket to a movie
theater grants the holder of a theater ticket access to a seat in a particular theater
at a particular time. Tickets are presumed to be unforgeable—only the appropriate
controlling authorities are able to issue tickets. In ticket-oriented protection schemes,
principals possess tickets for the resources or objects to which they have access.
Principals gain access to protected resources or objects by presenting the appropriate
ticket to the reference monitors protecting those resources.
In list-oriented protection schemes, an access-control list has the names of princi-
pals who are allowed to access the resource being protected. An example application
of access-control lists is a restaurant where patrons are seated at dining tables only if
they have a reservation. The access-control list itself must also be protected: that is,
only certain principals are recognized to have the authority to adjust whose names
appear on the access-control list.
In this section, we formalize the concepts of tickets and access-control lists using
the access-control logic.

Basic Concepts
61
4.2.1
Tickets
Tickets are a common means to control access to protected objects. For example,
when airline passengers prepare to board an airplane, the gate agents check each pas-
senger’s ticket or boarding pass to see if the person presenting the ticket is authorized
to board the airplane. The gate agents are the reference monitors guarding access to
the airplane’s doors. The boarding pass signiﬁes the capability to access a particular
ﬂight. Ideally, it is an unforgeable document issued by the airline (i.e., the controlling
authority) granting permission to board a particular airplane at a particular time.
In general, ticket-oriented access control requires the following four components,
which we express as statements in the logic:
1. The access policy:
authority controls (subject controls ⟨access right, object⟩)
The policy statement asserts that the controlling authority has jurisdiction over
which subjects can exercise an access right on an object.
2. The ticket:
ticket says (subject controls ⟨access right, object⟩)
A ticket is a credential stating that the subject has the right to access a partic-
ular object.
3. Additional trust assumption:
ticket ⇒authority
This assumption states that tickets are tokens of authority issued by the con-
trolling authority. As such, it is imperative that these tokens or tickets be
unforgeable.
4. The access request:
subject says ⟨access right, object⟩
The access request occurs when the subject actually presents the ticket. For
example, an airline passenger presenting a ticket is requesting to sit at her
assigned seat on the designated ﬂight. Of course, a subject may request access
without actually presenting a ticket, but the reference monitor should not grant
access in that case. For example, a person in the air terminal who tries to board
an airplane without presenting a valid ticket should not be allowed to do so.
Together, these four components provide sufﬁcient information for the reference
monitor to determine that the requested action ⟨access right, object⟩should be per-
mitted. If we generalize the atomic proposition ⟨access right, object⟩to an arbitrary

62
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.2 Formal proof of the Ticket Rule
1. subject says ϕ
Access request
2. authority controls (subject controls ϕ)
Access policy
3. ticket ⇒authority
Trust assumption
4. ticket says (subject controls ϕ)
Ticket
5. authority says (subject controls ϕ)
3, 4 speaks for
6. subject controls ϕ
2, 5 controls
7. ϕ
6, 1 controls
statement ϕ, we can derive the following useful inference rule:
Ticket
subject says ϕ
authority controls (subject controls ϕ)
ticket ⇒authority
ticket says (subject controls ϕ)
ϕ
.
The formal proof of this rule appears in Figure 4.2. The next example illustrates how
the Ticket Rule applies to the airline-ticket scenario.
Example 4.1
Suppose Tina has an airplane ticket assigning her to seat 25D on Smooth Air Flight
#1. When her row is called, she presents her ticket to the gate agents for ﬂight #1,
and she is granted access.
The following analysis justiﬁes the decision by the gate agents to let Tina board
the airplane:
1. Tina says ⟨seat 25D, ﬂight #1⟩
Tina’s request
2. Smooth Air controls (Tina controls ⟨seat 25D, ﬂight #1⟩)
Access policy
3. ticket ⇒Smooth Air
Trust assumption
4. ticket says (Tina controls ⟨seat 25D, ﬂight #1⟩)
Tina’s ticket
5. ⟨seat 25D, ﬂight #1⟩
1, 2, 3, 4 ticket rule
Line 1 represents Tina’s request to board ﬂight #1 and sit in seat 25D, which hap-
pens when Tina walks up to the gate agent and presents her ticket. Line 2 indicates
that the gate agent recognizes Smooth Air’s authority over which subjects can access
ﬂight #1. Line 3 states that tickets represent the authority of Smooth Air; that is, the
gate agent assumes that Smooth Air will issue tickets only to people who should be
passengers on ﬂight #1. Implicit in this line is that the gate agent recognizes the ticket
as being a valid ticket issued by Smooth Air. For example, if Tina were to present a
piece of notebook paper that has her name and a seat assignment scribbled on it, the
gate agent would not accept that paper as representing the authority of Smooth Air.
Line 4 describes what the ticket says, namely that Tina is assigned to seat 25D
on ﬂight #1. Line 5—which is obtained from the previous lines through the Ticket
Rule—demonstrates that the gate agent’s decision to allow Tina to board ﬂight #1 is
justiﬁed, given the previous conditions.
♦

Basic Concepts
63
In the preceding example, the boarding pass really is a ticket in the technical sense:
the gate agent checks only the validity of the boarding pass and does not conﬁrm
Tina’s identity. Most likely, anyone who handed Tina’s valid boarding pass to the
gate agent would be allowed on the plane, regardless of their actual identity.
Exercise 4.2.1
Suppose a theater ticket is sold by the box ofﬁce to a patron to

see Gone with the Wind in Theater 5 at 7:30 p.m. Using the access-control logic,
describe the patron’s request, the access-control policy of the theater, the trust as-
sumptions, and movie ticket. Based on your descriptions, formally justify admitting
the patron to see the movie.
4.2.2
Lists
Another widely used access-control mechanism is a list of principals with their ac-
cess rights to protected objects. These lists are known as access-control lists (ACLs).
In access-control list schemes, reference monitors possess the list of authorized
users and their privileges. Principals identify themselves and make their requests to
access the protected resources. The reference monitor compares the request to the
access-control list: if there is a match, then access is granted. Unlike ticket-oriented
access control, principals do not possess a credential that says they have a right to
access a protected resource.
ACLs are a very common protection mechanism. For example, if a scientist wants
to visit a secure government or commercial facility, she must call ahead to ensure
that the guards at the gates of the facility have her name on a visitor’s list. When she
arrives at the gate, she identiﬁes herself to the guard, who checks her identity and
looks to see if she is on the guest list. If her name is on the list, then she is allowed
into the facility. A more benign but common example is a wedding or formal party
where a greeter (acting as the reference monitor) checks off guest names on a guest
list.
In general, protection schemes involving access-control lists require the following
components:
1. Access policy:
authority controls (subject controls ⟨access right, object⟩)
The guard or reference monitor recognizes the jurisdiction of some controlling
authority to decide who is allowed what access to the protected object.
2. Access-control list:
ACL says









subject1 controls ⟨access right1,object1⟩∧
subject2 controls ⟨access right2,object2⟩∧
···∧
subjectn controls ⟨access rightn,objectn⟩

64
Access Control, Security, and Trust: A Logical Approach
An ACL is essentially a listing of subjects and their access rights.
3. Trust assumption:
ACL ⇒authority
This assumption states that the ACL always reﬂects the wishes of the authority.
As is the case with tickets, it is imperative that the ACL’s integrity be assured.
4. Access request:
subject says ⟨access right, object⟩
The request occurs when the subject presents herself to the guard (who can
establish her identity, as necessary) and makes the request.
With the exception of the form of the access-control list, these components are the
same as those for tickets. Authorities determine who has access, and trust assump-
tions are needed to interpret statements from authority. It is within this context that
subjects’ requests are considered.
The only difference is in the statements directly associated with the ticket and
with the access-control list. A ticket associates a speciﬁc subject with a speciﬁc
authorization:
ticket says (subjecti controls ϕi)
In contrast, an access-control list associates many subjects with many authoriza-
tions, having the following general form:
ACL says









subject1 controls ϕ1 ∧
···
subjecti controls ϕi ∧
···
However, it is possible to extract from an access-control list the relevant autho-
rization, using the Says Simpliﬁcation rules introduced in Exercise 3.2.5:
P says (ϕ1 ∧ϕ2)
P says ϕ1
P says (ϕ1 ∧ϕ2)
P says ϕ2
Using a combination of these two rules, we can extract (for any i)
ACL says (subjecti controls ϕi)
from the access-control list description.
It is therefore possible to use the Ticket Rule to also reason about access-control
lists. The following example is an illustration.

Basic Concepts
65
Example 4.2
Suppose Erika is invited to a dinner meeting where delicate negotiations are to be
held. Because of the sensitive nature of the talks, the meeting is being held in a
private dining room. At the door of the dining room is a maˆıtre d’ with a guest list of
all the attendees. The guest list, which was authorized by the restaurant’s manager,
has three names: Erika, Darnell, and Gina.
To gain entry to the dining room, Erika identiﬁes herself to the maˆıtre d’ and he lets
her in. For simplicity, we will assume that the maˆıtre d’ recognizes Erika, Darnell,
and Gina by sight, so that no further identiﬁcation is needed. (We will consider more
realistic and complicated situations later in this chapter.). The following analysis
justiﬁes the decision of the maˆıtre d’ to let Erika into the private dining room:
1. Erika says ⟨enter, dining room⟩
Erika’s request
2. Manager controls (Erika controls ⟨enter, dining room⟩)
Access policy
3. ACL ⇒Manager
Trust assumption
4. ACL says





Erika controls ⟨enter, dining room⟩∧
Darnell controls ⟨enter, dining room⟩∧
Gina controls ⟨enter, dining room⟩
Guest List
5. ACL says (Erika controls ⟨enter, dining room⟩)
4 simplify says
6. ⟨enter, dining room⟩
1, 2, 3, 5 ticket rule
Line 1 represents Erika’s spoken request to be let into the private dining room.
Line 2 reﬂects the restaurant’s policy that the manager in charge has authority over
who can enter the private dining room. Line 3 represents the maˆıtre d’s belief that the
guest list is authorized by the manager and accurately reﬂects the manager’s wishes.
Line 4 describes the guest list—a list of subjects who can exercise entry rights to the
private dining room. Line 5 is obtained by repeated application of the Simplify Says
inference rule to Line 4, and Line 6 is obtained by the Ticket Rule.
♦
When expressing ACLs in the logic, it is important to remember that the two
formulas
(P controls ϕ1) ∧(P controls ϕ2)
and
P controls (ϕ1 ∧ϕ2)
are not equivalent (see Exercise 3.3.7). In the former case, P is trusted on each of ϕ1
and ϕ2; P’s request to perform either should be granted. In the latter case, however,
P is trusted on the conjunction of ϕ1 and ϕ2: P must request both in order for either
to be granted. For this reason, an ACL will often contain many separate clauses for
the same principal, detailing all of the individual access rights that principal has.
Similarly, the two formulas
(P controls ϕ) ∧(Q controls ϕ)
and
P & Q controls ϕ

66
Access Control, Security, and Trust: A Logical Approach
are not equivalent. The ﬁrst formula states that both P and Q are individually autho-
rized for ϕ. In contrast, the second formula states that both P and Q must request
ϕ for access to be granted. For instance, in Example 4.2, Erika, Darnell, and Gina
are each authorized to enter the private dining room, and each may enter the room
without the others. Suppose instead that the ACL were deﬁned in the following way:
ACL says Erika & Darnell & Gina controls ⟨enter, dining room⟩.
This case would correspond to restaurants that seat parties only after everyone has
arrived: Erika, Darnell, and Gina could enter the dining room only if all three make
the request to do so.
Exercise 4.2.2
In operating systems with discretionary access control, a user can
specify who else can read, write, or execute her ﬁles. Suppose Carter creates a
program foo, and he wants Dan to be able to execute foo but neither read foo nor
write to it. Also, Carter wishes to grant read, write, and execute privileges to Jan.
Assume that Dan and Jan have userids dan and jan, respectively. Formalize the
above description and formally justify why Jan’s request to execute foo should be
granted.
Exercise 4.2.3
Prove that the formulas
P & Q controls ϕ,
(P controls ϕ) ∧(Q controls ϕ)
are not equivalent. That is, ﬁnd a particular formula ϕ and a particular Kripke
structure M = ⟨W,I,J⟩such that
M ̸|= (P & Q controls ϕ) ≡((P controls ϕ) ∧(Q controls ϕ)).
4.2.3
Logical and Pragmatic Implications
We have looked in detail at ticket-oriented and list-oriented mechanisms for con-
trolling access to protected objects. As we have seen, tickets and lists differ in the
following two ways:
1. Tickets are typically possessed—or easily accessible—by principals wishing
to access a protected object. In contrast, ACLs are possessed by reference
monitors.
2. Typically, when tickets are used, the reference monitor does not need to iden-
tify the principal making the request. Instead, the principal possessing the
ticket needs only give it to the reference monitor to exercise the rights corre-
sponding to the ticket. In contrast, a reference monitor using an ACL must
somehow determine the identity of the principal requesting access to deter-
mine what rights, if any, the principal has.
Despite these differences, tickets and ACLs share similar logical meanings and
logical contexts:

Basic Concepts
67
1. Logically speaking, tickets and ACLs make statements of the same form:
ticket says (subject controls ϕ)
ACL says (subject controls ϕ)
In the case of tickets, the subject may simply be Bearer, in that anyone bearing
or possessing the ticket has the corresponding access right. In the case of
ACLs, speciﬁc principals are generally identiﬁed.
2. Both tickets and ACLs are used in the context of policies that specify which
authority has jurisdiction over access decisions. These policies regarding ju-
risdiction are expressed in the logic as follows:
authority controls (subject controls ϕ)
3. Both tickets and ACLs are the mechanisms for controlling access and thus
must be protected from fraud. As a result, both must faithfully represent the
desires of the authorities they represent. Statements regarding this faithful
representation of the authority are expressed in the logic as follows:
ticket ⇒authority
ACL ⇒authority
From a logical standpoint, it is no accident that reference monitors depend upon
the same inference rules to deduce whether an access request should be granted.
Although ticket-based access may appear on the surface to be very different from
list-based access, the information conveyed by tickets and lists is the same, as is the
larger context of policy statements and trust assumptions.
There is an important pragmatic consideration regarding the use of the derived
Ticket Rule in our examples. Recall that the ticket rule is as follows:
Ticket Rule
subject says ϕ
authority controls (subject controls ϕ)
ticket ⇒authority
ticket says (subject controls ϕ)
ϕ
From an implementation standpoint, an engineer designing a reference monitor
will not implement a general purpose inference engine or theorem prover based on
the inference rules of the access-control logic. Instead, what he or she will build
amounts to a mechanization of a derived inference rule speciﬁc to his or her situation.
In particular, many reference monitors are essentially implemented as checklists.
They determine whether the various necessary tickets and certiﬁcates are present; if
so, access is granted. Such approaches can be justiﬁed by derived inference rules
where the elements on the checklist are the premises and the desired action on an
object is the conclusion. Furthermore, there is usually a speciﬁc context within
which the reference monitor operates. These contexts will sometimes provide im-
plicit premises, rather than explicit checklist items.

68
Access Control, Security, and Trust: A Logical Approach
For example, suppose that Margaret is an engineer building a reference monitor
for a copy machine. The intention is that, if a person has a ticket—perhaps a card he
inserts into the copy machine—then he can make copies. In the actual implementa-
tion, Margaret’s chief concern is to develop a way to check the validity of an inserted
copy card; if it is valid, then a copy operation should be performed whenever the
copy button is pressed. The veriﬁed validity of the copy card amounts to a statement
of the form
card says (bearer controls copy),
while each press of the copy button amounts to the following statement:
bearer says copy.
The Ticket rule allows Margaret to justify her design formally, but only by explicitly
stating the implicit assumptions about what card validity means. Speciﬁcally, the
analysis requires her to acknowledge an authority that governs who can make copies
and that the copy card must speak for that authority:
auth controls (bearer controls copy),
card ⇒auth.
The value of such an analysis is that potentially implicit assumptions are made
explicit. If there are future changes to the operating context (e.g., the card-making
equipment is stolen or compromised), then it is easier to determine whether or not
systems need to be reconﬁgured or even redesigned.
4.3
Authentication
Authentication is the task of identifying a subject who is making a request. More
abstractly, authentication is the task of associating one principal (e.g., the process
requesting access to a guarded resource or the face and person requesting access to a
guarded facility) with another principal (e.g., an individual or group included on an
ACL).
In this section we take a detailed look at several authentication scenarios and de-
scribe them in the access-control logic.
4.3.1
Two-Factor Authentication
Authentication is generally based on one or more kinds of information or factors:
• Type 1: something you know, such as a PIN, or password.
• Type 2: something you have, such as card, key, or token.
• Type 3: something you are, such as your ﬁngerprint, face, or voice.

Basic Concepts
69
Authentication can be performed using one or more items of information drawn
from each factor. Two-factor authentication uses information drawn from two of
the preceding types. An example of two-factor authentication based on information
drawn from types 1 and 2 is identifying a principal based on a personal identiﬁcation
number PIN (type 1) and a badge or certiﬁcate (type 2).
Many badges have the principal’s name and the PIN associated with the principal
in protected form on the badge. A typical security badge might have a person’s
name, face, and security clearance on the front of the badge, and have the name,
security clearance, and PIN stored in encrypted form accessible by a magnetic card
reader. Most cards have both physical and electronic security. Physical security
measures include the way the card was manufactured: the person’s picture is part
of the plastic, there is a stamp or signature written over visible information such as
security clearances and names, and so on. Electronic security can include encryption
and signing of electronic information such as names, PINs, and security clearances.
If a smart card is used, key information will be wiped out if physical tampering is
attempted.
For an older picture ID with a magnetic stripe that contains a protected PIN, we
can express the two-factor authentication as follows:
authority says ((facesubject & PINsubject) ⇒subject).
We recognize that the above formula is exactly the form of a certiﬁcate (i.e., a
signed statement) issued by an authority: the certiﬁcate in this case associates a
particular face and PIN with a particular subject or principal. The ID badge is used
by the reference monitor as a certiﬁcate. Again, it is crucial that badges and other
certiﬁcates be impossible—or at least very difﬁcult—to counterfeit.
Figure 4.3 contains an analysis of two-factor authentication based on certiﬁcates.
Line 1 represents the subject using factor1 and factor2 to make a request ϕ. Line 2
is the access policy that states that authority1 is in charge of who has access. Line 3
corresponds to an access-control list entry stating that subject has access right ϕ.
Line 4 state that authority2 has jurisdiction over certiﬁcates identifying principals
using factor1 and factor2. Line 5 is the information contained in the certiﬁcate or
badge possessed by the subject. Lines 6 through 9 are derived using the inference
rules of the access-control logic.
The two-factor proof of Figure 4.3 corresponds to the following derived inference
rule:
Two Factor Auth
(factor1 & factor2) says ϕ
authority1 controls (subject controls ϕ)
authority1 says (subject controls ϕ)
authority2 controls ((factor1 & factor2) ⇒subject)
authority2 says ((factor1 & factor2) ⇒subject)
ϕ
.
The following example demonstrates the use of this derived rule for reasoning about
access based on two-factor authentication.

70
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.3 Template for two-factor authentication
1. (factor1 & factor2) says ϕ
Access request
2. authority1 controls (subject controls ϕ)
Access policy
3. authority1 says (subject controls ϕ)
ACL entry
4. authority2 controls ((factor1 & factor2) ⇒subject)
Jurisdiction
5. authority2 says ((factor1 & factor2) ⇒subject)
Certiﬁcate
6. (factor1 & factor2) ⇒subject
4, 5 Controls
7. subject says ϕ
6, 1 Says
8. subject controls ϕ
2, 3 Controls
9. ϕ
8, 7 Controls
Example 4.3
Omar works for a military research lab. The security ofﬁce has issued Omar a badge
(badgeOmar) and personal identiﬁcation number (PINOmar). The doors to the lab are
protected by card readers with keypads. To enter the lab, Omar must swipe his badge
in the card reader and then punch in his PIN. If Omar uses his card and punches in
his PIN, then the reference monitor controlling the door is able to identify him, look
him up on an electronic access-control list, and determine that he is an employee in
good standing and should be admitted.
This description corresponds to two-factor authentication. Omar must know his
PIN (a type-1 factor) and then he must possess a badge (a type-2 factor). The com-
bination of the two are used to identify Omar to the electronic reference monitor that
guards the lab doors.
As is the case with many badges, the magnetic stripe on Omar’s card contains
protected information, such as a badge identiﬁer and Omar’s PIN. The card reader
on the doors can identify both the badge identiﬁer and protected PIN.
The following proof justiﬁes letting Omar enter the lab when he presents his badge
and PIN.
1. (badgeOmar & PINOmar) says ⟨enter, lab⟩
Access request
2. security ofﬁce controls (Omar controls ⟨enter, lab⟩)
Access policy
3. security ofﬁce says (Omar controls ⟨enter, lab⟩)
ACL entry
4. security ofﬁce controls ((badgeOmar & PINOmar) ⇒Omar)
Jurisdiction
5. security ofﬁce says ((badgeOmar & PINOmar) ⇒Omar)
Info in badge
6. ⟨enter, lab⟩
Two Factor ACL
The preceding scenario is largely analogous to the use of bank cards and PINs by
automated teller machines. The important difference is that withdrawals also depend
on conditions related to the account balances.
4.3.2
Using Credentials from Other Authorities
So far we have described access scenarios that depend on credentials issued by the
same authority responsible for the protected resource. In many situations, however,

Basic Concepts
71
this need not be the case. For example, airlines routinely use state-issued driver’s
licenses with pictures and federally issued passports to identify passengers. Guards
at airports use a combination of government-issued documents with pictures (such
as passports and licenses) coupled with airline boarding passes.
The informal justiﬁcation that allows such credentials to be used is based on trust
in the process by which these credentials are issued. For example, government-
issued picture IDs are accepted based on the assumption that it is hard to dupe the
government into issuing a fake picture ID. That is, the government’s registration
process for issuing picture IDs is deemed to be sufﬁciently robust. Likewise, airline
tickets are accepted as credentials because airlines have strong economic interests
to control who can get on their airplanes: it is consequently difﬁcult to defraud an
airline and obtain a ticket without paying for it (either with money or frequent-ﬂyer
miles).
A typical goal when making access-control decisions based on other credentials
is two-fold: ﬁrst, determine whether the subject making the request is who they say
they are, and then determine whether they have access rights to some other object
that is meaningful and relevant to you.
A template for this type of situation appears in Figure 4.4. Lines 1 through 9 are
the initial assumptions. Line 1 describes a subject with a face, voice, ﬁngerprint, or
userid and password, making a request ϕ1. Lines 2 and 3 are the recognition of the
jurisdiction of authority A1 regarding ϕ1 and the stated access policy. Note that the
policy expressed in line 3 differs in form from previous examples: it states that, if
the subject has access to the object corresponding to ϕ2, then the subject also has
access to the object corresponding to ϕ1. For example, if Penny has permission to
board an airplane leaving from a given airport, then Penny has permission to enter
that airport. Lines 4 through 6 relate to recognizing the jurisdiction of authority A2
on associating a factor with a subject (factor ⇒subject), the credential itself, and its
assumed integrity. Similarly lines 7 through 9 relate to recognizing the jurisdiction
of A3 to control ϕ2 and the integrity of the certiﬁcate authorizing subject to control
ϕ2.
The proof in Figure 4.4 yields the following derived inference rule:
ID & Ticket
factor says ϕ1
A1 controls ((subject controls ϕ2) ⊃subject controls ϕ1)
A1 says ((subject controls ϕ2) ⊃subject controls ϕ1)
A2 controls (factor ⇒subject)
certiﬁcate says (factor ⇒subject)
certiﬁcate ⇒A2
A3 controls (subject controls ϕ2)
ticket says (subject controls ϕ2)
ticket ⇒A3
ϕ1
.
To be explicit, this template illustrates one particular way that credentials from
multiple authorities might ﬁt together as part of an access-control scheme. Not all

72
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.4 Template for using credentials from multiple authorities
1. factor says ϕ1
Request
2. A1 controls ((subject controls ϕ2) ⊃subject controls ϕ1)
Jurisdiction
3. A1 says ((subject controls ϕ2) ⊃subject controls ϕ1)
Access policy
4. A2 controls (factor ⇒subject)
Jurisdiction
5. certiﬁcate says (factor ⇒subject)
ID card
6. certiﬁcate ⇒A2
Trust assumption
7. A3 controls (subject controls ϕ2)
Jurisdiction
8. ticket says (subject controls ϕ2)
Ticket
9. ticket ⇒A3
Trust assumption
10. subject controls ϕ2 ⊃subject controls ϕ1
2, 3 Controls
11. A2 says (factor ⇒subject)
6, 5 Derived speaks for
12. factor ⇒subject
4, 11 Controls
13. subject says ϕ1
12, 1 Derived speaks for
14. A3 says (subject controls ϕ2)
9, 8 Speaks for
15. subject controls ϕ2
7, 14 Controls
16. subject controls ϕ1
15, 10 Modus ponens
17. ϕ1
16, 13 Controls
situations will necessarily ﬁt into this speciﬁc template. The following example ap-
plies the ID & Ticket rule to a common situation that does ﬁt this template: the
task of controlling access to an airport terminal where only ticketed passengers with
DMV-issued driver’s licenses can enter.
Example 4.4
Penny is a ticketed passenger on ﬂight #1. To gain entry to the airport terminal, she
must show a state-issued picture ID along with her boarding pass. When she reaches
airport security, she presents her driver’s license. She also presents her boarding pass
for ﬂight #1. The following formalization justiﬁes why Penny is allowed to enter the
airport terminal:
1. Face says ⟨enter, airport⟩
Request
2. Security controls ((Penny controls ⟨25D, ﬂight #1⟩) ⊃
Penny controls ⟨enter, airport⟩)
Jurisdiction
3. Security says ((Penny controls ⟨25D, ﬂight #1⟩) ⊃
Penny controls ⟨enter, airport⟩)
Access policy
4. DMV controls (Face ⇒Penny)
Jurisdiction
5. license says (Face ⇒Penny)
ID card
6. license ⇒DMV
Trust assumption
7. Airline controls (Penny controls ⟨25D, ﬂight #1⟩)
Jurisdiction
8. ticket says (Penny controls ⟨25D, ﬂight #1⟩)
Ticket
9. ticket ⇒Airline
Trust assumption
10. ⟨enter, airport⟩
1,2,3,4,5,6,7,8,9 ID
& Ticket Rule
Line 1 is the request as interpreted by airport security: a speciﬁc face is making
a request to enter the airport terminal. Line 2 corresponds to the security guard at

Basic Concepts
73
the airport recognizing the authority of airport security to make policy. Line 3 is the
actual policy stating that, if the person presenting the ticket is really Penny (i.e., her
face matches the face on the driver’s license), then Penny can enter the airport. Lines
4 through 6 correspond to the guard recognizing the authority of the state DMV,
seeing Penny’s driver’s license with her picture on it, and accepting the license as
authentically issued by the DMV. Lines 7 through 9 have a similar interpretation
with respect to the airline and Penny’s airplane ticket.
♦
Exercise 4.3.1
Meg keeps a safe-deposit box (box #1205) at her local bank, in
which she stores important documents. She can access those documents by visiting
the bank, getting the safe-deposit box, and then opening it with her bank-issued
keycard (cardM) and PIN (PINM).
At the bank, access to safe-deposit boxes is governed as follows:
• As a strategy to prevent theft, safe-deposit boxes can be removed from the vault
only through the cooperation of the Vault Ofﬁcer and the Teller Supervisor.
Speciﬁcally, the Vault Ofﬁcer and the Teller Supervisor must simultaneously
insert special keys (keyVO and keyTS, respectively) and enter the speciﬁc box’s
number.
• Once removed from the vault, the safe-deposit box is brought to the account
holder (in this case, Meg), who may open the box by inserting her keycard and
entering the correct PIN number.
Let ⟨release,#1205⟩and ⟨open,#1205⟩be (respectively) the operations of releas-
ing box 1205 from the vault and of opening box 1205. Use expressions in the access-
control logic to answer the following questions regarding the certiﬁcations, creden-
tials, and access-control policies needed for the bank’s safe-deposit system.
a. What are the speciﬁc access requests that are placed to release box 1205 from
the vault and to open box 1205?
b. What are the ACL (access-control list) entries that govern the vault’s release
and Meg’s opening of box 1205?
c. What are the additional certiﬁcates, recognition of authority, and trust as-
sumptions that are necessary for determining whether to grant release or open
requests for box 1205?
d. Suppose that Meg’s safe-deposit box is brought to her, and that her keycard is
valid. Using the relevant assumptions from Parts (1), (2), and (3) above, give
a formal proof that the request ⟨open,#1205⟩will be granted.
Exercise 4.3.2
Suppose you are the chief operating ofﬁcer of a rental car company.
One of your primary objectives is to make sure you rent cars only to qualiﬁed drivers.

74
Access Control, Security, and Trust: A Logical Approach
a. Informally describe your method for assessing whether a domestic customer
is qualiﬁed to rent a car, what evidence you will use, how you will check the
authenticity of the evidence, and who you trust in the process.
b. Formally describe your method using the access-control logic.
c. Most domestic rental car companies rent cars to international customers but
require additional credentials.
Describe your process for establishing the
qualiﬁcations of international customers.
d. Formally describe your method using the access-control logic.
4.3.3
Groups
In many situations, access control is group based, rather than individual based.
That is, access-control decisions may be based on whether the requester belongs to
a speciﬁc group, rather than on the speciﬁc identity of the individual. As a common
example, access to the teacher’s lounge in a high school is typically granted to all
members of the faculty.
In reality, groups are simply another sort of principal, and group-based access
policies are deﬁned in the same way as identity-based policies. For example, the
following statement expresses the access policy for the high-school teacher’s lounge:
Faculty controls ⟨enter,lounge⟩.
Group membership can be expressed using the speaks-for relationship, as in the fol-
lowing statement:
Leslie ⇒Faculty.
Using the Derived Controls rule, it is straightforward to deduce that any request from
Leslie to enter the teacher’s lounge should be granted.
Many everyday documents are certiﬁcates that assert group membership of one
kind or another. Alec’s United States passport is a certiﬁcate issued by the U.S. State
Department that asserts that Alec is a U.S. citizen:
US State Department says Alec ⇒US Citizen.
Miranda’s student identiﬁcation card, issued by New State University (NSU) when
she ﬁrst enrolled, is a certiﬁcate stating that she is an NSU student:
NSU says Miranda ⇒NSU Student.
Exercise 4.3.3
Oliver has recently become a Premium member of the frequent-ﬂyer
club of Fly-By-Night Airlines. The membership package he received from Fly-By-
Night included a membership card with his name and frequent-ﬂyer number printed
on it. As a Premium member, Oliver is eligible to enter the Fly-By-Night lounges
at participating airports. On his next trip, Oliver decides to use this beneﬁt: he

Basic Concepts
75
heads to the Fly-By-Night lounge and presents his membership card and a photo
identiﬁcation (his driver’s license) to the staffer who is guarding the door.
Formally describe the access-policy of the lounge, Oliver’s request to enter, and
any other necessary trust assumptions required for Oliver to be granted access to
the lounge. Based on these descriptions, formally justify letting Oliver into the Fly-
By-Night lounge.
Exercise 4.3.4
Sam is a citizen and resident of the United States, returning home
after visiting Canada. She arrives at the US border with her passport in hand.
a. The act of driving her car to the border agent’s station can be interpreted as
a request to enter the United States. Express this request in the access-control
logic.
b. Sam’s passport can be interpreted as a certiﬁcate (i.e., a signed statement)
from the U.S. State Department that associates Sam’s face with her name. It
also identiﬁes Sam as belonging to the group of U.S. citizens.
Formalize Sam’s passport as an expression in the access-control logic.
c. The standard policy is that US citizens with proof of citizenship may enter the
country, provided the border agent deems them not to be a threat (i.e., the
border agent may exercise discretion).
Formalize this policy in the access-control logic.
d. Suppose that the border agent deems Sam not to be a threat.
Identify any other necessary assumptions (expressed as formulas in the access-
control logic) about certiﬁcates, jurisdiction, et cetera to justify letting Sam
enter the United States.
e. Using only those expressions given in the previous parts of the question, give
a formal proof justifying Sam’s entry into the United States.
4.4
Summary
Our goal is to be able to justify an answer to the question “Should this access-
control request be granted?” Answering this question correctly relies on sorting
through the maze of tickets, certiﬁcates, credentials, access-control lists, and various
means of authentication. This analysis further depends upon the authorities that are
recognized and the extent of their jurisdiction.
In this chapter, we demonstrated how to describe the mechanisms (e.g., tickets,
ACLs, certiﬁcates) and the broader context (e.g., recognized authorities and jurisdic-
tions) in our access-control logic. We also demonstrated several derived rules that
support reasoning about access requests. In the process, we developed templates for

76
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.5 Learning outcomes for Chapter 4
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
• Express requests, access-control policies, credentials, and notions of ju-
risdiction in the access-control logic.
• Express tickets, access-control lists, authentication factors, and group
membership in the access-control logic.
Synthesis
• When given a scenario that involves tickets, lists, and individual or
group-based authentication, formalize the scenario, identify all neces-
sary trust assumptions, and formally justify the granting of a request.
analyzing a wide variety of access-control scenarios. These templates demonstrate
the utility of derived rules that can be reused in a variety of situations. They also
illustrate how the language can be used to describe new mechanisms and concepts.
The learning outcomes associated with this chapter appear in Figure 4.5.
4.5
Further Reading
For classic papers on access control and protection, we suggest Butler Lampson’s
“Protection” (Lampson, 1971) and Saltzer and Schroeder’s “The Protection of In-
formation in Computer Systems” (Saltzer and Schroeder, 1975). In addition, Matt
Bishop’s textbook Computer Security: Art and Science (Bishop, 2003) provides a
compendium of access-control models and devotes a chapter to authentication.

Chapter 5
Security Policies
In Chapter 4, we focused on security mechanisms, which provide the means to en-
force security policies. We now turn to security policies themselves, which deﬁne
what is allowed and what is prohibited.
Typically, policies specify the permissible actions that subjects can perform on
objects. Subjects are active entities: they include the users, system processes, and
services that perform actions, such as sending messages, reading and writing ﬁles,
and so on. Subjects correspond to the principals of our access-control logic. Objects
are the passive entities acted upon, such as databases, ﬁles, etc. Subjects and objects
are mutually exclusive, but both are protected by access-control policies.
There are three aspects of security policies: conﬁdentiality, integrity, and avail-
ability. Conﬁdentiality addresses who may see what; integrity addresses who may
modify what; and availability addresses the required levels of service. In this chap-
ter, we give high-level deﬁnitions of each, with the understanding that more precise
deﬁnitions of each depend upon the speciﬁc system and the context in which it is
used.
Security policies are classiﬁed in several ways. One way to classify policies is
based on whether they are discretionary or mandatory. A second way is to classify
policies based on their use context: military or commercial. A third way is to describe
policies based on which aspect of security is addressed: conﬁdentiality, integrity, or
availability. We introduce these different classiﬁcations in this chapter, providing
examples of each.
5.1
Conﬁdentiality, Integrity, and Availability
The three core aspects of security policies are conﬁdentiality, integrity, and avail-
ability (collectively known as the C-I-A triad).
Conﬁdentiality
Conﬁdentiality boils down to the inaccessibility of information
or resources. For example, consider Lara’s grade in her Writing 101 course. The
conﬁdentiality (or privacy) rules for the university’s computer system might state
that only Lara, the university Registrar, and Lara’s Writing 101 instructor can read
Lara’s Writing 101 grade. As a consequence, the university’s computer systems are
77

78
Access Control, Security, and Trust: A Logical Approach
not authorized to release Lara’s Writing 101 grade to her parents. Secrecy is an aspect
of conﬁdentiality: it deﬁnes who may know or possess information. For example,
the general leading an army is permitted to know the date of when the army will
attack, whereas the general’s cook is not permitted to know this date.
Integrity
Integrity refers to the accuracy and credibility of information. Integrity
is a more subtle property than conﬁdentiality. In the case of conﬁdentiality, either the
information is compromised (leaked) or not. However, integrity has multiple aspects.
One aspect is content related: are the data correct and uncorrupted? The other aspect
is origin related: who or what is the source of the data?
For example, consider bonds issued by corporations. Bonds are used to raise
money with the promise to lenders that they will be repaid with interest. The simplest
aspect of a bond is the interest rate. Its accuracy is easily veriﬁed. The vastly more
difﬁcult aspect is evaluating how credible the promise of repayment is. To deal with
credibility and integrity, we often rely on a rating or grading system. Bonds are rated
as AAA, AA, A, BBB, BB, B, CCC, CC, and C, where AAA is the highest (and
presumably most risk free), C is the lowest (the highest likelihood of default), and
anything with a BB rating or lower is considered junk.
Mechanisms that support integrity fall into two classes: detection and prevention.
Detection mechanisms look for corruption or alteration of data. For example, parity
checking coupled with parity bits is used to detect the corruption of values stored in
computer memory. In contrast, prevention mechanisms deny unauthorized attempts
to change data. For example, students are not granted write access to their college
transcripts.
Lack of corruption alone is insufﬁcient to guarantee integrity, because integrity
also includes trust. For example, suppose that Trish receives a message to install
a speciﬁc program on her machine, and the message arrives intact and without any
corruption in its transmission. Trish’s actions depend on whether she trusts the orig-
inator of the message. If the originator is a system administrator who has authority
over her computer, then Trish will likely comply and install the program. On the
other hand, if the request is from an unknown or untrusted source, then she most
likely will not comply.
Assumptions about who or what is trusted affect the integrity and security of a
system directly. When these assumptions are implicit and unstated, a system may
become vulnerable as the result of changing circumstances. As seen throughout this
book, trust assumptions and statements of authority and jurisdiction are crucial to
virtually every analysis.
Availability
A system is available if it can be used with some minimum level of
performance or quality of service. For example, a personal computer system might
be considered to be available if all requests to access ﬁles on the local hard drives are
satisﬁed within one second.
When resources or systems are unavailable or perform beneath expected levels of
service, it is difﬁcult to judge whether the system is suffering from a so-called denial

Security Policies
79
of service attack or from routine use. As an example, consider a highway that is
often jammed during rush hour. When a driver encounters bumper-to-bumper trafﬁc,
she typically might conclude that its cause is rush-hour trafﬁc. On the other hand,
she might come to a very different conclusion if she read about a planned protest
by local farmers who intended to drive at the legal minimum speed in all lanes of
the highway that day. The key difference is the intent behind accessing or using a
resource. In the case of rush-hour trafﬁc, the intent of most highway users is to get
to work. In the case of protesters, it is to deny, frustrate, or degrade the quality of
service to other highway users in order to draw attention to the issues of concern
to them. Assessing whether or not a denial of service attack is occurring can be
difﬁcult, because determining intent from observable behavior is difﬁcult.
5.2
Discretionary Security Policies
Discretionary policies are dynamic (i.e., they can be changed), and they are typi-
cally under user control. In particular, discretionary access control (DAC) policies
are policies where users control access to protected objects over which they have ju-
risdiction. Often, DAC policies are based on identity, providing examples of identity-
based access control (IBAC).
In the access-control logic, IBAC policies are recognizable by controls statements
that relate access rights (such as ⟨operation,object⟩) to a subject’s identity:
Subject controls ⟨operation,object⟩.
Such statements reﬂect the identity-based portion of an access-control policy. Fur-
thermore, in the case of DAC policies, there is some principal (typically the owner of
the object) who speciﬁes the policy, resulting in a statement of the following form:
Principal says (Subject controls ⟨operation,object⟩).
The discretionary nature of the policy—that is, that a given principal has the authority
to set such policies—can be expressed as follows:
Principal controls (Subject controls ⟨operation,object⟩).
The expected behavior or response to subjects’ access requests can be expressed
as theorems or derived inference rules. Expressing behavior as theorems in the form
of derived inference rules enables our expectations to be checked for validity.
For example, given the above identity-based statements, we can express our ex-
pectations of behavior as a derivable inference rule of the following form:
Principal says (Subject controls ⟨operation,object⟩)
Principal controls (Subject controls ⟨operation,object⟩)
Subject says ⟨operation,object⟩
⟨operation,object⟩
.

80
Access Control, Security, and Trust: A Logical Approach
foo
April
read
Bill
read, write
Carla
execute
Table 5.1: Access-control matrix for foo under Susan’s control
Of course, depending on the actual situation, the inference rules may be more
complicated and include various conditions. The following example is typical of
discretionary access-control policies.
Example 5.1
Suppose Susan creates a program foo. Susan is allowed to specify that April can read
foo, Bill can read and write (modify) foo, and Carla can execute foo. The access-
control matrix for foo is shown in Table 5.1.
In the access-control logic, we represent the table as follows:
Susan says (April controls ⟨read, foo⟩)∧
Susan says (Bill controls ⟨read, foo⟩)∧
Susan says (Bill controls ⟨write, foo⟩)∧
Susan says (Carla controls ⟨execute, foo⟩).
This statement reﬂects the access policy on foo, as dictated by Susan.
We represent Susan’s discretion or authority to dictate access to foo as follows:
Susan controls (April controls ⟨read, foo⟩) ∧
Susan controls (Bill controls ⟨read, foo⟩) ∧
Susan controls (Bill controls ⟨write, foo⟩) ∧
Susan controls (Carla controls ⟨execute, foo⟩).
Both statements, taken together, describe the discretionary access-control policy for
the ﬁle foo.
The behavior of the system that controls access to foo can be described by theo-
rems or derived inference rules. For example, given the above discretionary access-
control policy statements, we can derive the response to the request
Carla says ⟨execute, foo⟩.
In particular, the response to Carla’s request is given by the following derivable in-
ference rule:
Susan says (Carla controls ⟨execute, foo⟩)
Susan controls (Carla controls ⟨execute, foo⟩)
Carla says ⟨execute, foo⟩
⟨execute, foo⟩
.
♦

Security Policies
81
Exercise 5.2.1
Consider the following access-control matrix for subjects S1 ···S3

and objects O1 ···O4 (the owner of each object appears in parentheses):
Subject
O1(S1)
O2(S2)
O3(S3)
O4(S4)
S1 read,write
read
read
read
S2
read
read,write
read
read
S3
read
read
read,write
read,write
Use the access-control logic to formulate the access-control rules for each object.
Exercise 5.2.2
Sunita and her husband Anand are organizing their ﬁles on their
home computer system, which is shared with their son Samuel. Sunita is a distin-
guished author who is working on her next book, and she keeps her draft on the
family’s home computer system in the ﬁle bookDraft. Sunita routinely relies on
Anand and Samuel to read her draft. She keeps a running list of their comments in
the ﬁles AnandsComments and SamuelsComments, which Anand and Samuel use to
record their reactions to her draft.
It is important to Sunita that she be the only one who can make changes to the book
draft. She wants to allow Anand and Samuel to read the draft and each other’s com-
ments, but it is important to her that Anand’s and Samuel’s comments be separate
from one another.
Devise answers to the following questions.
a. Create access-control matrices for the ﬁles bookDraft, AnandsComments,
and SamuelsComments. Explain how your access-control matrices satisfy
Sunita’s concerns.
b. Write the policy statements for each ﬁle in the access-control logic.
c. Describe in words what happens when given each of the following requests:
Anand says ⟨write,AnandsComments⟩,
Anand says ⟨write,SamuelsComments⟩,
Samuel says ⟨read,AnandsComments⟩.
For each request that is granted, give the derivable inference rule that justiﬁes
granting access.
d. Prove that your inference rules are valid.
5.3
Mandatory Security Policies
Mandatory security policies are policies that apply to everyone and everything all
the time: they are static and cannot be changed, and individuals have no discretion

82
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.1 Conceptual diagram of a virtual machine and monitor
or control over them. Mandatory access control (MAC) policies in computers are
typically implemented by the operating system or by the hardware. For resources
protected by a MAC policy, neither the owner nor the requester can inﬂuence the
access-control decision. Unlike DAC policies, MAC policies typically do not name
speciﬁc subjects or principals in policy statements. The rules for access usually
depend on considerations other than identity.
As an important example of a MAC policy, let us consider the rules for access-
ing physical memory in support of virtual memory for virtual machines. We study
virtual machines in detail in Chapter 10. For now, it sufﬁces to know that virtual ma-
chines provide self-contained operating environments that provide users the illusion
of working on a single-user system. To maintain this illusion successfully, the op-
erating environment must separate and isolate individual virtual machines from one
another, so that no virtual machine can interfere with the operation of another virtual
machine. Consequently, all virtual-machine attempts to access the host operating
system or hardware must be monitored and checked by a reference monitor called
the virtual machine monitor.
Figure 5.1 contains a simple conceptualization of a virtual machine and its rela-
tionship with physical memory and the virtual machine monitor (VMM). The virtual
machine has two registers:
1. The instruction register (IR) holds the current instruction being executed by
the virtual machine.
2. The accumulator register (ACC) holds intermediate values of computations.
Physical memory has q locations, with the addresses ranging from 0 through q −1.
The virtual machine monitor regulates virtual-machine access to physical memory.
As mentioned previously, the virtual machine provides to users the illusion that
they are operating in a single-user environment. For this reason, user programs use
logical (or virtual) addresses, which must then be translated into physical addresses.
For example, a program might contain an instruction to load the accumulator register
ACC with the contents of virtual address A (LDA @A). To execute LDA @A, the

Security Policies
83
virtual machine loads the instruction into the instruction register IR and requests
access to the virtual address A. The virtual machine monitor must determine whether
the request would cause a security violation. Whether or not a security violation
exists depends on the relationship between virtual and real memory addresses.
Conceptually, the VMM is implemented as part of a memory management unit
(MMU). The VMM’s job is twofold: (1) to isolate user programs from one another,
and (2) to provide an execution environment for each virtual machine. Isolation is
accomplished by partitioning physical memory into segments, where each segment
corresponds to the virtual-memory space available to its corresponding user. The
execution environment is provided by associating with each virtual machine VM
the physical location of the VM’s memory segment. Thus, Alice’s VM has one
segment, and Bob’s VM has another separate segment. Alice’s and Bob’s programs
may both execute the instruction LDA @A, but the VMM can distinguish the two
address references because Alice and Bob use different memory segments.
The physical location of a memory segment depends on two parameters: the base
and the bound. The base value is the segment’s starting address in physical mem-
ory. The bound value gives the segment’s size. These two values are stored in the
relocation register RR, as shown in Figure 5.1.
If there are no security violations (a situation we discuss shortly), then the physical
address associated with the virtual address A can be calculated relative to the base
value for the current segment:
PHYSICAL ADDRESS = base+ A.
If a user’s program accesses virtual address A and there are no security violations,
then the physical address base+ A is stored in the MMU’s memory address register
(MAR).
There are two possible security violations that may occur. The ﬁrst occurs when
the limit of physical memory (i.e., q) is exceeded:
base+ A ≥q.
The second occurs when the boundary of the individual VM’s segment is exceeded:
A ≥bound.
If either of these violations occur, then the instruction is trapped: user control over
the virtual machine is terminated, and control is passed on to a trusted supervisory
control program that can assess the situation and determine what to do next.
The following example illustrates the basic ideas of virtual memory, how MAC
policies are formally described, and how the behavior of virtual machine monitors
can be described as derived inference rules. We will discuss virtual machines and
virtual machine monitors in fuller detail in Chapter 10.
Example 5.2
Consider the virtual machine environment illustrated in Figure 5.1, with three distinct
memory segments.

84
Access Control, Security, and Trust: A Logical Approach
Suppose that the current instruction being executed by the virtual machine VM is
LDA @A (i.e., load the contents of virtual address A into ACC). We can formalize
this situation by the formula
IR says ⟨LDA @A⟩,
where ⟨LDA @A⟩is interpreted as “it would be a good idea to execute LDA @A.”
The virtual machine—via the instruction register IR—requests that VMM permit the
execution of LDA @A.
The virtual machine monitor provides an execution environment for virtual ma-
chines in general, and for VM in this speciﬁc case. Because the execution environ-
ment is determined by the values base and bound in the relocation register RR, we
can describe it by the statement
RR says ⟨(base,bound)⟩,
where ⟨(base,bound)⟩is interpreted as “the pair (base,bound) describes the current
active memory segment.”
A primary distinction between mandatory and discretionary policies is that manda-
tory policies are enforced by the operating system or hardware, whereas users are in
control in discretionary policies. In the case of enforcing mandatory access control
for physical memory, the memory management unit MMU has the authority and re-
sponsibility for enforcing the MAC policy. This MAC policy imposes conditions
on the states of the VM and VMM registers and on the size of physical memory, as
stated by the following two formulas:
IR says ⟨LDA @A⟩⊃(RR says ⟨(base,bound)⟩⊃(((base+ A ≥q)∨(A ≥bound)) ⊃⟨trap⟩)),
IR says ⟨LDA @A⟩⊃(RR says ⟨(base,bound)⟩⊃((base+ A < q) ⊃((A < bound) ⊃⟨LDA @A⟩))).
The ﬁrst formula expresses the conditions under which an address violation occurs
and is trapped. The second formula states that, when there is no address violation,
the instruction LDA @A is executed. In both formulas, the relevant states of VM and
VMM are expressed by the values in IR and RR.
The behavior of the MMU is expressed through derived inference rules. The fol-
lowing rule, which is provable using the inference rules of the access-control logic,
describes when the instruction LDA @A should be trapped:
LDA @A trap
IR says ⟨LDA @A⟩⊃
(RR says ⟨(base,bound)⟩⊃(((base+ A ≥q)∨(A ≥bound)) ⊃⟨trap⟩))
IR says ⟨LDA @A⟩
RR says ⟨(base,bound)⟩
(A +base ≥q)∨(A ≥bound)
⟨trap⟩
The inference rule describing when LDA @A is approved for execution and the
proof of its validity are left as exercises for the reader.
♦

Security Policies
85
Exercise 5.3.1
Recall the LDA @A instruction illustrated in Figure 5.1. Devise and
prove valid an inference rule that describes when LDA @A is approved for execution.
Exercise 5.3.2
The instruction STO @A takes the contents of the accumulator ACC
and stores it in virtual address A. Devise and prove valid inference rules that de-
scribe when STO @A is trapped and when STO @A is executed.
5.4
Military Security Policies
Military security policies are focused on controlling the disclosure of information
(i.e., providing conﬁdentiality). These policies regulate the ﬂow of information by
describing who is allowed to know what. Information is classiﬁed or labeled using
security levels, where the security classiﬁcation levels are ordered. Principals are
assigned a security clearance level using the same classiﬁcation levels. Reference
monitors can then control the ﬂow of information by limiting access based on these
classiﬁcation levels.
For example, the classic classiﬁcation scheme involves the following four classi-
ﬁcation levels, in increasing order: UNCLASSIFIED, CONFIDENTIAL, SECRET, and
TOP SECRET. To read a ﬁle that has been classiﬁed at the SECRET level, one must
possess an appropriate clearance level (SECRET or TOP SECRET) and have an appro-
priate need to know.
To express and reason about military security policies, we must augment our logic
to support security classiﬁcation levels. Fortunately, these modiﬁcations are very
straightforward, as we shall see.
5.4.1
Extending the Logic with Security Levels
As we have seen in previous chapters, a logic has three important components: the
syntax (i.e., how formulas are written), the semantics (i.e., what the formulas mean),
and the logical rules (i.e., how the formulas are manipulated). In augmenting our
logic to support security levels, we address each of these components in turn.
Syntax
The ﬁrst step is to introduce syntax for describing and comparing security
levels. We deﬁne SecLabel to be the collection of simple security labels, which are
used as names for the various security levels associated with a military access policy;
possible elements of this set include TS, S, C, and UC, among others.
In addition to these speciﬁc security labels, we will often want to refer abstractly
to the security level assigned to a particular principal P. For this reason, we deﬁne
the larger set SecLevel of all possible security-level expressions:
SecLevel ::= SecLabel / slev(PName)

86
Access Control, Security, and Trust: A Logical Approach
That is, a security-level expression is either a simple security label or an expression
of the form slev(A), where A is a simple principal name.1 Informally, slev(A)
refers to the security level of principal A.
Finally, we extend our deﬁnition of well-formed formulas to support comparisons
of security levels:
Form ::= SecLevel ≤s SecLevel / SecLevel =s SecLevel
Informally, the formula C ≤s slev(Kate) states that Kate’s security level is greater
than or equal to the security level C. Similarly, the formula slev(Barry) =s slev(Joe)
states that Barry and Joe have been assigned the same security level.
Semantics
Providing formal and precise meanings for the newly added syntax
requires us to ﬁrst extend our Kripke structures with additional components that
describe security classiﬁcation levels. Speciﬁcally, we introduce extended Kripke
structures of the form
M = ⟨W,I,J,K,L,⪯⟩,
where:
• W, I, and J are deﬁned as in Deﬁnition 2.1.
• K is a non-empty set, which serves as the universe of security levels.
• L : (SecLabel∪PName) →K is a function that maps each security label and
each simple principal name to a security level.
L is extended to work over arbitrary security-level expressions, as follows:
L( slev(A)) = L(A),
for every simple principal name A.
• ⪯⊆K ×K is a partial order on K: that is, ⪯is reﬂexive (for all k ∈K, k ⪯k),
transitive (for all k1,k2,k3 ∈K, if k1 ⪯k2 and k2 ⪯k3, then k1 ⪯k3), and
anti-symmetric (for all k1,k2,k3 ∈K, if k1 ⪯k2 and k2 ⪯k1, then k1 = k2).
Using these extended Kripke structures, we deﬁne the semantics for our new well-
formed expressions as follows:
EM [[ℓ1 ≤s ℓ2]] =
(
W,
if L(ℓ1) ⪯L(ℓ2)
/0,
otherwise
EM [[ℓ1 =s ℓ2]] = EM [[ℓ1 ≤s ℓ2]]∩EM [[ℓ2 ≤s ℓ1]].
As these deﬁnitions suggest, the expression ℓ1 =s ℓ2 is simply syntactic sugar for
(ℓ1 ≤s ℓ2)∧(ℓ2 ≤s ℓ1).
1This syntax precludes security-level expressions such as slev(P & Q) or slev(P | Q), because there is
no standard technique for associating security classiﬁcation labels with compound principals.

Security Policies
87
FIGURE 5.2 Inference rules for relating security levels
ℓ1 =s ℓ2
def
= (ℓ1 ≤s ℓ2)∧(ℓ2 ≤s ℓ1)
Reﬂexivity of ≤s
ℓ≤s ℓ
Transitivity of ≤s
ℓ1 ≤s ℓ2
ℓ2 ≤s ℓ3
ℓ1 ≤s ℓ3
FIGURE 5.3 Proof of ≤s Subst
1. slev(P) =s ℓ1
Assumption
2. slev(Q) =s ℓ2
Assumption
3. ℓ1 ≤s ℓ2
Assumption
4. ( slev(P) ≤s ℓ1) ∧(ℓ1 ≤s slev(P))
1, Defn =s
5. ( slev(Q) ≤s ℓ2) ∧(ℓ2 ≤s slev(Q))
2, Defn =s
6. slev(P) ≤s ℓ1
4, Simpliﬁcation
7. slev(P) ≤s ℓ2
6, 3 Transitivity of ≤s
8. ℓ2 ≤s slev(Q)
5, Simpliﬁcation
9. slev(P) ≤s slev(Q)
7,8 Transitivity of ≤s
Logical Rules
The last task is to introduce logical rules that support using security
levels to reason about access requests. Speciﬁcally, the deﬁnition and two sound
inference rules in Figure 5.2 reﬂect that ≤s is a partial order. The antisymmetry of
≤s emerges directly from the deﬁnition of ℓ1 =s ℓ2.
The following useful rule (≤s Subst) is derivable, as shown by the proof in Fig-
ure 5.3:
≤s Subst
slev(P) =s ℓ1
slev(Q) =s ℓs
ℓ1 ≤s ℓ2
slev(P) ≤s slev(Q)
.
5.4.2
Expressing Military Security Policies
With these extensions to the logic in hand, we can now formulate a simple military
security policy for managing information ﬂow. There are two parts: (1) the simple
security condition, which describes the conditions under which read access can be
granted; and (2) the *-property, which states when it is permissible to grant someone
write access. Our presentation is based on the Bell–La Padula model (Bell and La
Padula, 1973; Bell and La Padula, 1975). We provide a simpliﬁed discussion here,
leaving the full details to Chapter 13.
Deﬁnition 5.1 The simple security condition mandates that principal P can read
object O if and only if:

88
Access Control, Security, and Trust: A Logical Approach
1. P’s security level is at least as high as O’s (i.e., slev(O) ≤s slev(P)), and
2. P has discretionary read access to O (i.e., P controls ⟨read,O⟩).
The simple security condition reﬂects the thinking that people or processes should
have read access to information at their security level or below, provided that they
need to know. The ﬁrst component ( slev(O) ≤s slev(P)) indicates that P can read
O only if P’s security level is at least as high as O’s. The second component requires
P to have discretionary read access to O, reﬂecting P’s need to know (i.e., some con-
trolling authority has granted P read access to O). The purpose of the simple security
condition is to prevent a principal from reading information at a higher classiﬁcation
level than the principal possesses.
Unauthorized reading of ﬁles, however, is not the only way that information might
improperly ﬂow from a higher classiﬁcation level to a lower one. It is also neces-
sary to prevent the “write-down” of information, whereby information from a ﬁle
at one classiﬁcation level is then written to a ﬁle with a lower classiﬁcation level.
The standard write policy that prevents such situations is known as the *-property
(pronounced “star property”).
Deﬁnition 5.2 The *-property mandates that principal P can write to object O if
and only if:
1. O’s security level is at least as high as P’s (i.e., slev(P) ≤s slev(O)), and
2. P has discretionary write access to O (i.e., P controls ⟨write,O⟩).
Restricting write access to ﬁles at or above a principal’s clearance level means
that information cannot ﬂow downwards (i.e., to lower classiﬁcation levels) through
the writing of ﬁles. Taken together, the simple security condition and the *-property
deﬁne the desired behavior of systems with a simple military security policy.
It is straightforward to formalize the behavior mandated by the simple security
condition and the *-property. Both the simple security condition and the *-property
have two parts: (1) a mandatory access-control condition µ, and (2) a discretionary
statement ϕ. Thus, each can be expressed by a statement of the form µ ⊃ϕ.
In the case of the simple security condition, the mandatory access-control condi-
tion is
slev(O) ≤s slev(P),
which mandates that the security level of object O must be no higher than the security
level of subject P. The corresponding discretionary access statement to grant P read
access to O is
P controls ⟨read,O⟩.
Thus, we can specify the simple security condition for subject P and object O as
follows:
( slev(O) ≤s slev(P)) ⊃(P controls ⟨read,O⟩).

Security Policies
89
In such statements, P discretionary read access to O is conditioned upon P having
sufﬁcient clearance with respect to O.
Special note: It is important to realize that the simple security property really
mandates two properties of a system. The ﬁrst property states that it is per-
missible to grant read access to any principal who has both a sufﬁciently
high security level and discretionary read access. It is this property that is
expressed by a policy statement
( slev(O) ≤s slev(P)) ⊃(P controls ⟨read,O⟩).
The second property asserts that read access should be granted only to
those principals who have both a sufﬁciently high security level and dis-
cretionary read access. In other words, any principal who is able to obtain
read access has sufﬁcient clearance and is authorized, without exception.
This second-property is in actuality a meta-property, in that one would
have to inspect every consequence of the collection of rules governing
access. It is impossible to express such a property in the logic itself.
A similar situation holds with respect to the *-property.
In the same manner, we can express the *-property as follows:
( slev(P) ≤s slev(O)) ⊃(P controls ⟨write,O⟩).
That is, P’s discretionary write access to O is contingent upon O having a sufﬁciently
high security level (i.e., at least as high as P’s).
The intended behavior described by both the simple security condition and the
*-property can be captured by the following Conditional Controls derived inference
rule:
Conditional Controls
µ ⊃P controls ϕ
µ
P says ϕ
ϕ
.
In this rule, the statement µ ⊃P controls ϕ is a military security policy statement
corresponding to either the simple security condition or the *-property: µ is the
mandatory access-control policy condition with respect to the subject’s and object’s
security levels, and P controls ϕ is the discretionary access-control policy. The state-
ment P says ϕ reﬂects the actual access request from P, where ϕ is the action being
requested by P.
When writing military security policies for which we wish the simple security
condition and the *-property to hold, it is important to follow the form of the read and
write access-control statements presented earlier. Speciﬁcally, all read authorizations

90
Access Control, Security, and Trust: A Logical Approach
Document
Classiﬁcation
threat scenario
TS
status report
TS
requirements
S
design
S
artist renderings
C
press releases
UC
Table 5.2: Classiﬁcation level of documents
on object O by subject P corresponding to the simple security condition should have
the form:
( slev(O) ≤s slev(P)) ⊃(P controls ⟨read,O⟩).
Similarly, all write authorizations on object O by subject P corresponding to the
*-property should have the form:
( slev(P) ≤s slev(O)) ⊃(P controls ⟨write,O⟩).
In some cases, it may seem simpler to adopt shortcuts, but they can produce un-
intended results when circumstances change. For example, suppose that Alice is
assumed to have the highest security level, which in this case is TOP SECRET. Under
the simple security condition, she has read access to all ﬁles to which she also has
discretionary read access. One might be tempted to take a shortcut and simply state
Alice controls ⟨read,O⟩. When Alice makes a read request—Alice says ⟨read,O⟩—
the Controls inference rule allows one to conclude that Alice should be allowed to
read O. However, now suppose that Alice’s security clearance is later downgraded
below TOP SECRET, and that O’s security level is TOP SECRET. Because the access
policy for Alice reading O does not mention her clearance level, she would still be
granted read permission, in direct violation of the simple security condition.
5.4.3
Military Security Policies: An Extended Example
As an extended example, consider the following scenario involving the (ﬁctional)
defense contractor DefenseSystemsRUs:
DefenseSystemsRUs has defense contracts across several branches
of the military. In particular, they have separate contracts with both
the Air Force and the Navy to develop air superiority ﬁghters. The Air
Force-funded project is called the FX-1, while the Navy-funded project
is called the FX-2.
The Defense Department wishes to order only one aircraft, and will
select either the FX-1 or FX-2 to serve the needs of both the Air Force
and Navy. To ensure a fair competition, the Defense Department has
mandated that DefenseSystemsRUs have two totally independent design

Security Policies
91
Function
Clearance
FX-1
FX-2
Team Leader
TS
Amy
Arlen
Engineer
S
Biao
Burt
Artist
C
Sonja
Suresh
Public Relations
UC
Jude
Jodi
Table 5.3: FX-1 and FX-2 project personnel, functions, and clearances
teams, each working closely with the military branch that is funding the
team.
As part of the FX-1 and FX-2 contracts, each design team has to
work with several documents at various security classiﬁcation levels.
All the documents are stored in DefenseSystemsRUs computers. Thus, a
security policy is required to control information ﬂow within and be-
tween design teams. The policy relies on the four standard military
classiﬁcation levels, linearly ordered as follows (in increasing order):
UNCLASSIFIED, CONFIDENTIAL, SECRET, and TOP SECRET.
Table 5.2 lists each document and its classiﬁcation level. The threat
scenario is considered TOP SECRET to prevent adversary nations from
developing technologies to defeat the FX-1 or FX-2 before they are de-
veloped and deployed. The requirements document is at the SECRET
level to guide design engineers at the SECRET level. Both the threat
scenario and requirements documents are issued by the Department of
Defense and cannot be changed by anyone in DefenseSystemsRUs. The
artist renderings of the FX-1 and FX-2 are CONFIDENTIAL. Press re-
leases are UNCLASSIFIED. Weekly status reports, which go to the gov-
ernment, are TOP SECRET.
The FX-1 and FX-2 teams are organized as shown in Table 5.3. Each
team has a team leader. Team leaders hold TOP SECRET clearance and
are able to read all reports related to their projects. As team leaders, they
can write the weekly status reports. They cannot alter the threat scenario
or requirements document, as both come from the Defense Department.
Engineers are at the SECRET level. They have read access to the
requirements and design documents. They can modify the design docu-
ments, but not the requirements document.
Artists are at the CONFIDENTIAL level. They can create artist ren-
derings of their aircraft.
Finally, public relations staff are at the UNCLASSIFIED level. They
author press releases, which are available to the public.
Given the above scenario, we devise two access-control matrices, one for each
project team, with the understanding that members of each team have no access
rights to documents written by the other team. These matrices are shown in Ta-
bles 5.4 and 5.5. The threat scenario is accessible to both FX-1 and FX-2 team

92
Access Control, Security, and Trust: A Logical Approach
FX-1 Document
Amy (TS)
Biao (S)
Sonja (C)
Jude (UC)
threat scenario (TS)
read
status report (TS)
read, write
write
write
write
requirements (S)
read
read
design (S)
read
read,write
artist renderings (C)
read
read
read, write
press releases (UC)
read
read
read
read, write
Table 5.4: FX-1 access matrix
FX-2 Document
Arlen (TS)
Burt (S)
Suresh (C)
Jodi (UC)
threat scenario (TS)
read
status report (TS)
read, write
write
write
write
requirements (S)
read
read
design (S)
read
read,write
artist renderings (C)
read
read
read, write
press releases (UC)
read
read
read
read, write
Table 5.5: FX-2 access matrix
leaders; the requirements document is accessible to the team leaders and engineers
of both projects.
The access-control matrices adhere to the simple military security properties spec-
iﬁed by the simple security condition and the *-property. Access to ﬁles is based on
a need to know. For example, while there would be no violation of information
ﬂow policies if Jude in Public Relations were allowed to write to the FX-1 design
documents, there is no compelling need to grant her write access.
Formal description:
We translate the preceding informal descriptions into the
access-control logic. Doing so provides us with a precise description of the scenario
and enables us to do a rigorous analysis of behavior.
The ﬁrst task is to identify the security labels for this application (e.g., UC, C, S,
TS) and to specify the ordering among them. We can express the ordering as a series
of statements in the logic, as follows:
UC ≤s C,
C ≤s S,
S ≤s C.
Note that (for example) UC ≤s S can be deduced using the transitivity rule for ≤s.
The second task is to state in the logic the assignment of security levels to princi-
pals and objects, as speciﬁed in Tables 5.2 and 5.3. We include here the statements

Security Policies
93
related to the FX-1 project (those for the FX-2 project are similar):
slev(threatScenario) =s TS
slev(Amy) =s TS
slev(statusFX1) =s TS
slev(Biao) =s S
slev(reqs) =s S
slev(Sonja) =s C
slev(designFX1) =s S
slev(Jude) =s UC
slev(artistRendFX1) =s C
slev(pressRelFX1) =s UC
In our simpliﬁed analysis, we just accept these statements as assumptions of the
form slev(statusFX1) =s TS and slev(Amy) =s TS.
More detailed descriptions
would include the source and jurisdiction of these statements: for example, a se-
curity ofﬁcer SO might have the authority and responsibility to certify the secu-
rity levels of people and documents. This would be described in the usual way as
SO says ( slev(statusFX1) =s TS) and SO controls ( slev(statusFX1) =s TS). To keep
things simple for this example, we will just assume these levels as stated.
Finally, at the object level of the access-control logic, we consider the access-
control matrices in Tables 5.4 and 5.5. As we did with setting security levels, we
take the access-control statements as assumptions. More detailed descriptions would
include the source of these statements and the authority of the sources. For simplicity,
we assume the content of these statements is appropriate and has the backing of those
in authority. We can translate Table 5.4 into statements on who can access FX-1
status reports as follows (access to other documents can be similarly described):
slev(statusFX1) ≤s slev(Amy) ⊃(Amy controls ⟨read,statusFX1⟩)
slev(Amy) ≤s slev(statusFX1) ⊃(Amy controls ⟨write,statusFX1⟩)
slev(Biao) ≤s slev(statusFX1) ⊃(Biao controls ⟨write,statusFX1⟩)
slev(Sonja) ≤s slev(statusFX1) ⊃(Sonja controls ⟨write,statusFX1⟩)
slev(Jude) ≤s slev(statusFX1) ⊃(Jude controls ⟨write,statusFX1⟩).
The *-property says that any FX-1 team member can submit their portion of the FX-1
status report. The simple security condition says that only Amy—as the team leader
with a TOP SECRET clearance—can read the report.
If the read-access and write-access statements in the access-control matrices are
translated into formulas following the pattern for military security policies, then the
simpliﬁed security condition and the *-property describe the behavior of the system.
For example, if Jude wishes to write her status report, the following derived inference
rule describes what happens:

94
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.4 Proof justifying Jude’s access to status report
1.
slev(Jude) ≤s slev(statusFX1) ⊃(Jude controls ⟨write,statusFX1⟩)
Assumption
2.
slev(Jude) =s UC
Assumption
3.
slev(statusFX1) =s TS
Assumption
4.
UC ≤s TS
Assumption
5.
Jude says ⟨write,statusFX1⟩
Assumption
6.
slev(Jude) ≤s slev(statusFX1)
2, 3, 4 ≤s Subst
7.
⟨write,statusFX1⟩
1, 6, 5 Cond’l Controls
slev(Jude) ≤s slev(statusFX1) ⊃(Jude controls ⟨write,statusFX1⟩)
slev(Jude) =s UC
slev(statusFX1) =s TS
UC ≤s TS
Jude says ⟨write,statusFX1⟩
⟨write,statusFX1⟩
.
The above rule has as its ﬁrst hypothesis the write-access policy statement cor-
responding to the *-property: Jude can write to statusFX1 so long as the security
level of statusFX1 is the same or is higher than Jude’s security level. The next two
hypotheses are Jude’s and statusFX1’s security levels. The last hypothesis is Jude’s
write request.
Figure 5.4 contains a proof justifying that Jude be allowed to write the FX-1 status
report.
Exercise 5.4.1
Recall the access-control matrix in Table 5.5. Devise the derived
inference rules corresponding to Jodi’s access rights on press releases. Prove the
validity of your derived rules.
Exercise 5.4.2
Recall the access-control matrix in Table 5.4. Suppose that artists

renderings are now unclassiﬁed. Is the Bell–La Padula model still satisﬁed? If
not, suggest a change in Table 5.4 that restores compliance with the Bell–La Padula
model.
5.5
Commercial Policies
Commercial policies are primarily concerned with integrity, namely the level of
quality and trustworthiness. Businesses fail if the quality or trustworthiness of their
products or services fail to live up to consumer expectations. For example, consider
what happens to a restaurant that serves tainted food to its customers.

Security Policies
95
Commercial integrity policies use labels in many of the same ways that military
security policies use labels. Integrity labels often reﬂect a grading of quality or trust-
worthiness levels. For example, a grading scheme we encounter in daily life is the
grading of gasoline based on its octane level. The gasoline we pump into our cars is
usually one of three grades: REGULAR, MID-GRADE, or PREMIUM. From the con-
sumer standpoint, higher gasoline grades are more expensive. From the petroleum
reﬁnery’s standpoint, higher gasoline grades require more reﬁnement and are more
costly to produce. The public expects gasoline grades and labels to match and feels
defrauded when they do not.
Another grading scheme we encounter in daily life is level of customer service.
For example, banking customers with balances of $200,000 or more in their accounts
might qualify for PREMIUM customer service. Customers with lesser balances qual-
ify for STANDARD customer service. It may be that STANDARD service includes
24-hour automated support and 24-hour phone support. PREMIUM support might
include everything that STANDARD support has, plus the assignment of a personal
banker available during working hours during the work week. Because higher levels
of customer support are more costly to companies and meeting customer expecta-
tions is crucial to stay in business, well-run businesses establish and enforce service
(integrity) policies to meet customer expectations while controlling costs.
In this section we introduce basic policies aimed at preserving integrity.
5.5.1
Extending the Logic with Integrity Levels
As we did for security labels, we extend our logic to incorporate integrity labels
and a means to compare them. A variety of integrity labels are possible, and they are
partially ordered. The formalization is nearly identical to that for security labels: the
only difference is in our intention to distinguish integrity levels from security levels,
so as to permit analyses that involve both (as in Chapter 13).
Syntax
We deﬁne IntLabel to be the collection of simple integrity labels, and we
deﬁne IntLevel to be the set of all possible integrity-level expressions:
IntLevel ::= IntLabel / ilev(PName)
Informally, ilev(A) refers to the integrity—i.e., quality or trustworthiness—level of
principal A.
We then extend our deﬁnition of well-formed formulas to support comparisons of
security levels:
Form ::= IntLevel ≤i IntLevel / IntLevel =i IntLevel
The symbol ≤i denotes a partial ordering on integrity levels, in the same way that ≤s
denotes a partial ordering on security levels. In particular, ≤i is reﬂexive, transitive,
and antisymmetric.

96
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.5 Inference rules for relating integrity levels
ℓ1 =i ℓ2
def
= (ℓ1 ≤i ℓ2)∧(ℓ2 ≤i ℓ1)
Reﬂexivity of ≤i
ℓ≤i ℓ
Transitivity of ≤i
ℓ1 ≤i ℓ2
ℓ2 ≤i ℓ3
ℓ1 ≤i ℓ3
Semantics
The semantics of integrity levels is added to the logic by extending
Kripke structures in precisely the same way we did for security levels. Speciﬁcally,
we introduce extended Kripke structures of the form
M = ⟨W,I,J,K,L,⪯⟩,
where:
• W, I, and J are deﬁned as in Deﬁnition 2.1.
• K is a non-empty set, which serves as the universe of integrity levels.
• L : (IntLabel∪PName) →K is a function that maps each integrity label and
each simple principal name to a integrity level.
L can be extended to work over arbitrary security-level expressions, as follows:
L( ilev(A)) = L(A),
for every simple principal name A.
• ⪯⊆K ×K is a partial order on K: that is, ⪯is reﬂexive (for all k ∈K, k ⪯k),
transitive (for all k1,k2,k3 ∈K, if k1 ⪯k2 and k2 ⪯k3, then k1 ⪯k3), and
anti-symmetric (for all k1,k2,k3 ∈K, if k1 ⪯k2 and k2 ⪯k1, then k1 = k2).
Using these extended Kripke structures, we deﬁne the semantics for our new well-
formed expressions as follows:
EM [[ℓ1 ≤i ℓ2]] =
(
W,
if L(ℓ1) ⪯L(ℓ2)
/0,
otherwise
EM [[ℓ1 =i ℓ2]] = EM [[ℓ1 ≤i ℓ2]]∩EM [[ℓ2 ≤i ℓ1]].
As these deﬁnitions suggest, the expression ℓ1 =i ℓ2 is simply syntactic sugar for
(ℓ1 ≤i ℓ2) ∧(ℓ2 ≤i ℓ1). Given the semantics of integrity levels, it is straightforward
to prove the soundness of the inference rules in Figure 5.5, which show that ≤i is a
partial order.

Security Policies
97
Just as in the case of security levels, the following derived rule ≤i Subst holds true
for integrity levels:
≤i Subst
ilev(P) =s ℓ1
ilev(Q) =i ℓi
ℓ1 ≤i ℓ2
ilev(P) ≤i ilev(Q)
.
5.5.2
Protecting Integrity
The major concern of integrity policies is protecting the system and its resources
from damage, misappropriation, or corruption. In the case of petroleum reﬁneries,
integrity policies might ensure that REGULAR, MID-GRADE, and PREMIUM grades
have at least 87, 91, and 93 octane, respectively. In the case of banking services,
service policies might guarantee that big depositors have immediate (e.g., less than
one-minute wait times) access to a human banker during business hours while regular
customers have access to human bankers only after having used automated support.
Some of the earliest work on integrity in computer systems was done in the 1970’s
by Biba. Although this work appeared prior to the threat of worms and viruses, the
deﬁnitions and principles that informed the integrity policies then are still relevant
now. Biba deﬁned computer system integrity as follows (Biba, 1975):
The concern of computer system integrity is thus the guarantee that a
subsystem will perform as it was intended to perform by its creator. We
assume that a subsystem has been initially certiﬁed (by some system
external agency) to perform properly. We then wish to insure that the
subsystem cannot be corrupted to perform in a manner contrary to its
certiﬁcation. The integrity problem is the formulation of access control
policies and mechanisms that provide a subsystem with the isolation
necessary for protection from subversion. Based on an initial assump-
tion of proper behavior (according to some system external standard),
we are primarily concerned with protection from intentionally malicious
attack: unprivileged, intentionally malicious modiﬁcation.
Subjects and objects have the same function in both integrity and conﬁdentiality
policies: subjects are active in that they make requests, whereas objects are passive
and do not make requests.
We consider three kinds of access:
1. Observation: the viewing of information by a subject. The act of viewing also
embodies the notion of execution or changing the state of the observing subject
as a result of the observation. For example, program execution by a subject is
accomplished by a subject observing (fetching) a program from an object.
2. Modiﬁcation: a change in the object that is discernible by observation.
3. Invocation: a request for service by one subject of another. This is a form of
modiﬁcation, because the state of the invoked subject is changed by the request
for service, if the request is accepted.

98
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.6 Subjects, objects, domains, and access
Each subject has an associated domain, which deﬁnes the objects and kind of ac-
cess the subject has. The domain of a subject may overlap or be completely separate
from the domain of any other subject.
Figure 5.6 shows three subjects S1, S2, and S3, each with its own domain (access
to objects). Subjects are represented by squares, objects by circles, and the kind of
access allowed by labeled arcs connecting subjects and objects. The domains of S1
and S2 overlap; the domains of S1 and S3 do not.
For example, S1’s domain gives it access to three objects: O1,O2, and O3. S1
is able to observe objects O1 and O2, and is able to modify objects O2 and O3.
Figure 5.6 also show the invocation rights of subjects. S2 is able to invoke S3 to
modify O5 and S1 can invoke S2 to observe O4 and modify O5.
In the next section, we introduce Biba’s strict integrity property (Biba, 1975),
which protects the integrity of both subjects and objects. It does so by ensuring
that subjects observe or read only from objects of equal or greater integrity and that
objects are modiﬁed only by subjects of equal or greater integrity.
5.5.3
Strict Integrity
The objective of strict integrity is to preserve the integrity of objects and subjects,
which means that no access degrades the integrity of any object or subject. Infor-
mally, preserving integrity requires meeting the following conditions:
1. If there is a path by which the contents of O1 can migrate to On+1, then the
contents of On+1 must not be degraded by the contents of O1. Thus, the in-
tegrity of O1 must be at least as high as that of On+1:
ilev(On+1) ≤i ilev(O1).
2. If subject S can observe object O, then S must not be degraded by the contents

Security Policies
99
of O. Thus, the integrity of O must be at least as high as that of S:
ilev(S) ≤i ilev(O).
3. Degradation cannot occur due to a subject with greater integrity following the
directions (invocations) of a subject with lesser integrity (e.g., a user telling
the system administrator to turn off all anti-virus software).
The ﬁrst two conditions deal with direct access, which we discuss in the rest of this
chapter. The third condition deals with indirect access through invocation, which we
defer until Chapter 13.
To formalize strict integrity, we must ﬁrst introduce the notion of a transfer path.
Deﬁnition 5.3 A transfer path is a sequence of objects O1,O2,··· ,On+1 and subjects
S1,S2,··· ,Sn such that:
S1 can observe/take O1
S1 can modify/put
O2
S2 can observe/take O2
S2 can modify/put
O3
···
Sn can observe/take On
Sn can modify/put
On+1.
A transfer path allows one or more subjects to collectively transfer or copy the con-
tents of one object to another. The following example demonstrates both the concept
of a transfer path and its expression in the logic.
Example 5.3
Suppose that the subject S1 has the authority to observe object O1 directly and to
modify object O2 directly. Representing the direct observation and modiﬁcation of
an object O by the primitive propositions ⟨o,O⟩and ⟨m,O⟩, respectively, we can
represent these two authorizations as follows:
S1 controls ⟨o,O1⟩,
S1 controls ⟨m,O2⟩.
Given S1’s authority, there is a transfer path (via S1) from O1 to O2. Similarly, if
subject S2 has the authority to observe O2 and modify O3, then there is a transfer
path from O2 to O3. Combining the transfer paths for S1 and S2 results in a transfer
path from O1 to O3.
♦
Strict integrity mandates that no subject or object along a transfer path can ever
possibly become degraded. That is, for any possible operation along a transfer path,
the contents of a destination must never be contaminated with the contents of a source

100
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.7 Preparation of vegetarian and non-vegetarian meals
with a lower level of integrity. Thus, under strict integrity, if we wish to grant subject
S the authority to observe an object O, we must ﬁrst ensure that O’s integrity level is
at least as high as S’s. In contrast, if we wish to grant S the authority to modify O, we
must ﬁrst ensure that S’s integrity level is at least as high as O’s.
Consequently, we can express strict integrity policies for direct observations and
direct modiﬁcations as follows:
( ilev(S) ≤i ilev(O)) ⊃(S controls ⟨o,O⟩),
( ilev(O) ≤i ilev(S)) ⊃(S controls ⟨m,O⟩).
If the strict integrity conditions are violated, then access requests should be trapped:
¬( ilev(S) ≤i ilev(O)) ⊃(S says ⟨o,O⟩) ⊃⟨trap⟩,
¬( ilev(O) ≤i ilev(S)) ⊃(S says ⟨m,O⟩) ⊃⟨trap⟩.
5.5.4
An Extended Example of a Strict Integrity Policy
As a simple example of the application of strict integrity, consider the preparation
of vegetarian and non-vegetarian meals. For the purpose of this example, we assume
that the preparation of vegetarian meals requires following certain rules and proce-
dures beyond the preparation of non-vegetarian meals. In other words, we assume
that people cooking vegetarian meals require additional training beyond the training
to cook non-vegetarian meals.
For the purposes of this example, we have two subjects (Alice and Bob) and ﬁve
objects (starch, vegetables, meat, veg meal, and non-veg meal). Alice is a vegetarian
cook, and Bob is a non-vegetarian cook. As a vegetarian cook, Alice takes from
starch and vegetables to prepare items for veg meals. She can also provide veg dishes

Security Policies
101
Subject
Starch
Vegetables
Meat
Veg Meal
Non-veg meal
Alice
take
take
-
put
put
Bob
take
take
take
-
put
Table 5.6: Access matrix for vegetarian and non-vegetarian meals
Subject or Object
Integrity Level
Alice
V
Bob
NV
Starch
V
Vegetables
V
Meat
NV
Veg Meal
V
Non-veg Meal
NV
Table 5.7: Integrity levels for subjects and objects
for non-veg meals. In contrast, Bob (as a non-vegetarian cook) may draw upon all
possible ingredients. Because his level of training does not qualify him to prepare
certiﬁed vegetarian meals, however, he can only prepare items for non-veg meals.
Figure 5.7 shows Alice’s and Bob’s various capabilities, resulting in the following
transfer paths:
• Starch and vegetables can be put into a veg meal through Alice.
• Starch and vegetables can be put into a non-veg meal through either Alice or
Bob.
• Meat can be put into a non-veg meal through Bob.
The information in this ﬁgure corresponds to the access matrix in Table 5.6. How-
ever, the access matrix alone fails to capture the underlying certiﬁcations and rules
that assure the integrity of vegetarian meals.
While we could implement the access matrix in Table 5.6, we would fail to capture
the underlying certiﬁcations and rules that assure the integrity of vegetarian meals.
To capture these aspects, we introduce two integrity levels (V and NV, with NV ≤i V)
and assign them to the subjects and objects as in Table 5.7.

102
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.8 Partial access diagram for gasoline
The resulting strict-integrity policy for Alice and Bob can be expressed as follows:
ilev(Alice) ≤i ilev(Starch) ⊃Alice controls ⟨take,Starch⟩
ilev(Alice) ≤i ilev(Vegetables) ⊃Alice controls ⟨take,Vegetables⟩
ilev(veg meal) ≤i ilev(Alice) ⊃Alice controls ⟨put,veg meal⟩
ilev(non-veg meal) ≤i ilev(Alice) ⊃Alice controls ⟨put,non-veg meal⟩
ilev(Bob) ≤i ilev(Starch) ⊃Bob controls ⟨take,Starch⟩
ilev(Bob) ≤i ilev(Vegetables) ⊃Bob controls ⟨take,Vegetables⟩
ilev(Bob) ≤i ilev(Meat) ⊃Bob controls ⟨take,Meat⟩
ilev(non-veg meal) ≤i ilev(Bob) ⊃Bob controls ⟨put,non-veg meal⟩.
Exercise 5.5.1
Consider the following scenario. There are two grades of gaso-
line: P for premium gasoline and R for regular gasoline. There are also two pumps
(Pump1 and Pump2) and two cars: one car (RGC) uses regular gasoline and the
other car (PGC) requires premium gasoline.
Assume that the typical relation on gasoline grades holds: premium gasoline is
higher quality than regular gasoline. You may also assume that cars speciﬁed as
taking a particular grade of gasoline can safely take that grade of gasoline or higher.
a. Provide the additional arrows (along with their proper labels) necessary for
Figure 5.8 to satisfy the following:
(a) Biba’s strict integrity policy, and
(b) Fuel requirements on cars.

Security Policies
103
b. Based on your completed diagram, for each pump below, give their domains:
Pump1 =
Pump2 =
c. Using P for Premium grade and R for regular grade, ﬁll out the level assign-
ments for the subject and objects in the following table:
Subjects and Objects
Integrity Level
Premium Gas Tank PGT
Regular Gas Tank RGT
Pump1
Pump2
Premium Gas Car PGC
Regular Gas Car RGC
d. Fill in the access-control matrix:
Subject
PGT
RGT
PGC
RGC
Pump1
Pump2
e. Express as a formula in the access-control logic the policy that states that
Pump1 can put into PGC.
f. Propose a derived inference rule that states that Pump1 can put gasoline into
PGC. Give a formal proof of the derived rule.
Exercise 5.5.2
A cleanroom is a room with a strictly maintained environment to
control the level of contaminants—such as dust particles or microbes—that can con-
taminate manufacturing processes. Cleanrooms are classiﬁed according to the num-
ber and size of particles per volume of air.
The International Standards Organization standard ISO 14644-1 rates the clean-
liness of cleanrooms based on the log base 10 of the number of particles 0.1µm or
larger permitted per cubic meter of air. Based on ISO 14644-1, an ISO Class 5 clean-
room has at most 105 = 100,000 particles per m3 of air. The ordering of cleanliness
levels is
··· < class 6 < class 5 < class 4 < ··· .
The cleanliness of cleanrooms is maintained by a combination of (1) special air
handlers and ﬁlters to control the ﬂow of air between cleanrooms and (2) airlocks
using air showers to clean people to match the level of cleanliness of the cleanroom
they are entering. As to be expected, higher levels of cleanliness require longer times
in airlocks.
Suppose we have a semiconductor or biotechnology assembly line that starts in
a class 4 cleanroom and ends in a class 6 cleanroom. People and parts enter the
cleanrooms as follows:

104
Access Control, Security, and Trust: A Logical Approach
• People enter cleanrooms from airlocks, and
• People in cleanrooms take parts from bins.
The access-control rules on people entering cleanrooms from airlocks and people
in cleanrooms taking parts from bins must satisfy the following integrity concerns:
• Parts must be protected from contamination or else the ﬁnal product is con-
taminated or corrupted. These parts are rated class 4, class 5, and class 6.
• The cleanrooms must be protected from contamination from both parts and
people.
The following are the integrity levels for cleanrooms, bins, and airlocks.
ilev(RoomA) = ilev(BinA) = ilev(AirlockA) = class 4
ilev(RoomB) = ilev(BinB) = ilev(AirlockB) = class 5
ilev(RoomC) = ilev(BinC) = ilev(AirlockC) = class 6
a. We describe in the access-control logic people in Roomi taking parts from Binj
as follows:
Roomi says ⟨take,Binj⟩.
(a) Fill in the access-control matrix showing which rooms are allowed to
take parts from which bins.
Objects
Subjects
BinA
BinB
BinC
RoomA
RoomB
RoomC
(b) Based on the preceding access-control matrix, formulate in the access-
control logic the integrity policy that states when Roomi is allowed to
take parts from Binj, for i, j = A,B,C.
b. We describe in the access-control logic people in Airlocki entering Roomj as
follows:
Airlocki says ⟨enter,Roomj⟩.
(a) Fill in the access-control matrix showing which airlocks are allowed to
enter which rooms.

Security Policies
105
Objects
Subjects
RoomA
RoomB
RoomC
AirlockA
AirlockB
AirlockC
(b) Based on the preceding access-control matrix, formulate in the access-
control logic the integrity policy that states when Airlocki is allowed to
enter Roomj, for i, j = A,B,C.
5.6
Summary
Security policies state what is allowed and what is not. We examined security
policies from several standpoints: mandatory policies, discretionary policies, conﬁ-
dentiality policies, and integrity policies. Mandatory policies apply to all subjects
and objects. Discretionary policies typically are set by owners of objects.
We introduced Bell–La Padula simple security condition and *-property, as well
as Biba’s strict integrity property. We also augmented the access-control logic to
support reasoning about policies that depends on security and integrity labels.
The learning outcomes associated with this chapter appear in Figure 5.9.
5.7
Further Reading
The original work by the pioneers of secure computing are well worth reading.
Saltzer and Schroeder’s classis paper “The Protection of Information in Computer
Systems” (Saltzer and Schroeder, 1975) provides an excellent overview of memory
protection for isolation and sharing. Bell and La Padula’s conﬁdentiality models are
published as technical reports (Bell and La Padula, 1973) and (Bell and La Padula,
1975). Biba’s original work on integrity also appears as a technical report (Biba,
1975). These are available electronically and should be read by serious students of
security.

106
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.9 Learning outcomes for Chapter 5
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
• When given an access-control matrix, interpret which requests are per-
mitted and which are not.
Application
• When given an informal description, use the Bell–La Padula simple secu-
rity condition and the *-property as guides for developing conﬁdentiality
levels and access rules.
• When given an informal description, use the Biba strict integrity model
for developing integrity levels and access rules.
Synthesis
• When given an informal mandatory and discretionary access-control pol-
icy that is required to meet the simple security condition, *-property, and
strict integrity, formalize access rules for subject and objects meeting the
requirements.
• When given speciﬁc integrity or security levels and a partial ordering of
those levels, devise an appropriate Kripke model supporting the seman-
tics of ≤i and ≤s.
Evaluation
• When given formal access-control rules, security levels, integrity levels,
and trust assumptions, derive if an access request should be granted.

Part II
Distributed Access Control


Chapter 6
Digital Authentication
To make reasoned access-control decisions in a digital world, we need to explore
in more depth how statements are signed and authenticated digitally. The basis for
digital signatures rests on cryptographic keys and cryptographic hash functions in
general, and on public-key cryptography in particular. Digital authentication using
public-key infrastructure (PKI) is well suited for authentication in distributed envi-
ronments such as the Internet.
In this chapter, we describe the details of digital authentication using the access-
control calculus. Unlike traditional explanations, we do not go into the algorith-
mic details of any particular encryption or hash function. Rather, we describe the
relationship between public and private keys, encryption and decryption, keys and
principals, and certiﬁcates.
6.1
Public-Key Cryptography
At the core of public-key cryptography are two cryptographic keys: one is public
and is openly shared (much like a telephone number or email address), and the other
is private and must be known only by a single principal. In theory, the principal
who possesses the private key is the only principal who can decrypt messages that
were encrypted using the corresponding public key. This property is used to ensure
privacy: that is, only a principal with the correct private key can read a message.
Similarly, messages encrypted using a private key can be universally decrypted by
anyone who possesses the corresponding (and publicly disclosed) public key. This
property is used for authenticity: that is, one can establish the identity of the author
of a message.
The public key K and private key K−1 together form a key pair (K,K−1), and each
key can undo the actions of the other. That is, if encrypt and decrypt are particular
encryption and decryption functions, then the following properties must hold for all
messages m:
decrypt(K−1,encrypt(K,m)) = m,
decrypt(K,encrypt(K−1,m)) = m.
In addition, the public and private keys are typically distinct (i.e., K−1 ̸= K). Conse-
109

110
Access Control, Security, and Trust: A Logical Approach
FIGURE 6.1 Process for using public-key encryption for privacy
quently, public-key cryptography is often referred to synonymously as asymmetric-
key cryptography. In the case of public-key algorithms such as RSA (Rivest et al.,
1978), the same algorithm serves both to encrypt and decrypt messages.
There are two additional properties about encryption and decryption that are im-
portant if a cryptosystem is to be useful:
1. It should be computationally infeasible to read an encrypted message without
knowing the correct decryption key.
That is, given an encrypted message encrypt(Ke,m), it should be computation-
ally infeasible to determine m without knowing the reciprocal key Kd. (Note
that Ke may be either a public or private key; Kd is the other half of the key
pair.)
2. It should be computationally infeasible to successfully forge an encryption
without knowing the encryption key Ke.
That is, given a message m but without knowing Ke, it should be computation-
ally infeasible to compute an x such that decrypt(Kd,x) = m.
These properties of public-key cryptography support privacy in the following way.
If Alice wishes to send a message to Bob that only Bob can read, she encrypts the
message with Bob’s public key KBob. Thus, Alice sends to Bob the following:
encrypt(KBob,message).
The idea here is that the resulting cipher text should be decipherable only with knowl-
edge of Bob’s private key K−1
Bob. Upon receipt, Bob uses his private key to decrypt
the message:
decrypt(K−1
Bob,encrypt(KBob,message)).
Figure 6.1 illustrates schematically how Alice and Bob use Bob’s keys to ensure
privacy of the message sent to Bob.
The use of public-key cryptography is not limited solely to achieving privacy.
More often, it is used to authenticate messages, such as website-connection requests,

Digital Authentication
111
FIGURE 6.2 Process for using private-key encryption for authenticity
public-key certiﬁcates, and so on. For example, suppose that Bob wishes to com-
municate a message to the world in such a way that anyone can deduce that Bob
authored the message. Such a situation might arise if Bob needs to establish his au-
thorship of an article or book for intellectual-property protection. In this case, Bob
can encrypt his book using his private key (which is known only by him):
encrypt(K−1
Bob,book).
Anyone who cares to read Bob’s book can read it using Bob’s freely available public
key to decrypt his encrypted ﬁle:
decrypt(KBob,encrypt(K−1
Bob,book)).
The idea here is that only Bob could have created the original encrypted ﬁle, be-
cause only Bob knows the key K−1
Bob used to create it. This sort of use of public-key
cryptography for authenticity is illustrated in Figure 6.2.
One can combine these two approaches to achieve a combination of privacy and
authenticity. For example, suppose that Alice wants to send a message that only Bob
can read, while providing Bob assurance that the message is coming from her. To
achieve this goal, Alice employs the following three-step process:
1. Alice encrypts her message m with her private key:
encrypt(K−1
Alice,m).
This step will allow Bob to deduce that Alice authored the message m, because
Bob can retrieve m using Alice’s public key.
2. Alice encrypts the result from step one with Bob’s public key, resulting in the
following:
encrypt(KBob,encrypt(K−1
Alice,m)).
This step provides Alice with assurance that only Bob can read her message,
because only Bob’s private key will be able to decrypt this cipher text.

112
Access Control, Security, and Trust: A Logical Approach
3. Alice informs Bob in plain text that she is the author of the cipher text from
step two. This hint tells Bob to look up Alice’s public key so that he can
decipher the message in such a way as to be assured that Alice was the author.
6.2
Eﬃciency Mechanisms
In theory, the approaches described in the previous section are sufﬁcient to handle
the needs of privacy and authenticity. In practice, however, the approaches described
for authenticity are rarely used, because public-key algorithms are relatively slow
and costly to use. In addition, the repeated use of a given key-pair on large amounts
of text can expose the keys and make them vulnerable to cryptographic analysis and
discovery. To address these pragmatic concerns, two additional steps can be taken:
(1) we can use a cryptographic hashing algorithm, and (2) we can use a session or
data-encryption key (DEK). We describe these steps—and their use in creating and
verifying digital signatures—in this section.
6.2.1
Cryptographic Hash Functions
Hash functions are commonly used in computing as a way to map large data spaces
to more compact spaces, substituting uniqueness for efﬁciency. A very simple ex-
ample of a hash function is a word-level parity check: all 32-bit words with an even
number of 1 bits are mapped to the bit 0, while all those with an odd number of 1
bits are mapped to the bit 1. In this case, a data space with 232 elements is mapped
to a data space of 2 elements, at the cost of uniqueness: 216 values are hashed to the
bit 0, and 216 values are hashed to the bit 1.
Cryptographic hash functions are hash functions with an additional property: given
any particular hash value, it must be computationally infeasible to determine an input
that, when hashed, produces the given value. This property is known as the one-way
property of cryptographic hashes: while hash values can be computed easily from
a given input, it is computationally infeasible to do the reverse (i.e., compute an
input that produces a particular hash value). Thus, for example, the parity check
clearly fails the one-way property (and hence would not make for a cryptographic
hash function), because it is trivial to ﬁnd 32-bit words with parity 1.
The one-way property of cryptographic hashes makes it possible for ﬁxed-length
cryptographic hash values to simulate unique identiﬁers for messages of arbitrary
length. For example, suppose Ellen wants to send a ﬁle to Todd that is several
megabytes in length, and she wants Todd to be able to determine whether the ﬁle
he receives has arrived unchanged and uncorrupted. To accomplish this goal, Ellen
sends a cryptographic hash of the ﬁle—also known as the cryptographic checksum—
along with the ﬁle itself:
(ﬁle,cryptographic checksum).

Digital Authentication
113
When Todd receives the ﬁle and the cryptographic checksum, he applies the hash
algorithm to the received ﬁle and compares the value he computed with the checksum
sent by Ellen. If they both match, then Todd concludes that the ﬁle he received is
intact and free of any tampering or corruption.
6.2.2
Data-Encryption Keys
When privacy is also a concern for a large ﬁle, the sender may elect to also employ
a symmetric-key encryption algorithm—such as the Advanced Encryption Standard
(AES) (National Institute of Standards and Technology, 2001)—to create a one-time
data-encryption key Kdek. Symmetric-key encryption algorithms use the same key
for both encryption and decryption (i.e., Kdek = K−1
dek). In addition, symmetric-key
encryption algorithms are three orders of magnitude faster than typical public-key
algorithms, because of the underlying operations they employ (i.e., exclusive-or and
bit substitutions as opposed to modulo-n exponentiation).
If Ellen chooses this approach, she ﬁrst creates a key Kdek; she then encrypts the
ﬁle with Kdek, computes the cryptographic checksum of the ﬁle, and encrypts Kdek
with Todd’s public key KTodd. Ellen sends these three items to Todd:
encrypt(KTodd,Kdek),
hash(ﬁle),
encrypt(Kdek,ﬁle).
When Todd receives these three items, he ﬁrst retrieves the data-encryption key Kdek
by using his private key:
decrypt(K−1
Todd,encrypt(KTodd,Kdek)) = Kdek.
He then uses the data-encryption key to retrieve the ﬁle:
decrypt(Kdek,encrypt(Kdek,ﬁle)) = ﬁle.
Finally, he checks the integrity of the retrieved ﬁle by determining whether it hashes
to the cryptographic checksum that Ellen sent.
Unfortunately, however, this scheme is subject to fraud, because none of the sent
items directly tie Ellen to the message that Todd receives. As a result, a third party
could send their own message (along with its cryptographic checksum) to Todd,
alleging that the message is from Ellen. Todd has no way of determining the truth of
such a claim. We can address this problem by introducing digital signatures in the
next subsection.
6.2.3
Digital Signatures
We can ﬁx the potential fraud problem by one addition: Ellen creates a digital
signature by encrypting the cryptographic checksum with her private key. Figure 6.3
illustrates a process for creating digital signatures that supports both integrity and
authorship:
1. The message or ﬁle is hashed by a one-way function. The resulting ﬁxed-
length hash value identiﬁes the contents of the message.

114
Access Control, Security, and Trust: A Logical Approach
FIGURE 6.3 Process for creating a digital signature
FIGURE 6.4 Process for verifying a digital signature
2. The hash value is then encrypted using the private key of the sender. The
encrypted hash value identiﬁes the sender, because the hash value can be re-
trieved only by using the sender’s public key. Note that, because the hash value
is much smaller than the message itself, this approach is much more efﬁcient
than the process described in Section 6.1.
Unlike human signatures, which remain relatively unchanged over time, digital sig-
natures change depending on the contents of what is being signed.
Figure 6.4 illustrates the process of verifying a digital signature. The process sim-
ply compares the following two values: (1) the hash value obtained by decrypting the
received signature using the sender’s public key, and (2) the hash value obtained by
hashing the contents of the received message. If the values match, then the received
message is considered to be both intact and from the public key’s owner.
6.3
Reasoning about Cryptographic Communications
In the previous two sections, we have seen how cryptography can be used to pro-
vide assurances of privacy and authenticity in distributed digital systems. In this

Digital Authentication
115
FIGURE 6.5 Simple analysis of digital signatures
1. KEllen says m
Received message with signature veriﬁed using KEllen
2. KEllen ⇒Ellen
Ellen’s public key
3. Ellen says m
2, 1 Derived speaks for
section, we see how the access-control logic can be used to express and reason about
cryptographic communications.
As an example, suppose that Ellen uses the methods described in Section 6.2 to
send Todd a digitally signed message. For this ﬁrst case, let us imagine that Ellen
sends the message itself in the clear (i.e., in plaintext with no encryption), but she
digitally signs it. Thus, she sends to Todd the following combination of items:
m,
encrypt(K−1
Ellen,hash(m)).
That is, she sends the message m, along with the encrypted hash (i.e., cryptographic
checksum) of m.
When Todd receives the message, he applies the signature-veriﬁcation protocol of
Figure 6.4. If this process is successful, then Todd has evidence that Ellen’s public
key KEllen has been used to verify that the message m arrived intact and uncorrupted.
This fact can be expressed in the access-control logic by recognizing m as a statement
made by KEllen:
KEllen says m.
If Todd believes that KEllen is Ellen’s public key—perhaps he looked her key up in
a public directory, or perhaps Ellen gave it to him sometime in the past—then he is
willing to associate statements made (i.e., integrity-checked) by that key with Ellen.
Thus, the belief that Ellen’s public key is KEllen can be expressed in the logic as the
following statement:
KEllen ⇒Ellen.
Based on these two factors, Todd can deduce that Ellen originally sent the message
m. In terms of the logic, this analysis uses the Derived Speaks For rule, as shown in
the proof of Figure 6.5.
Having examined the simpler case, now let us consider the case where Ellen sends
Todd a digitally signed message, using a data-encryption key Kdek to encrypt the
message. Recall that Todd receives three items, along with a hint that Ellen is the
sender:
encrypt(KTodd,Kdek),
encrypt(Kdek,m),
encrypt(K−1
Ellen,hash(m)).
Todd must decrypt the ﬁrst component of this package to obtain the key Kdek, which
he can then use to retrieve the message m. He then veriﬁes Ellen’s digital signature
by computing the cryptographic hash of m and comparing it with the checksum that
Ellen sent.

116
Access Control, Security, and Trust: A Logical Approach
If this process is successful, then Todd once again used Ellen’s public key to verify
that the message m he received was indeed the one initially signed by Ellen. As we
saw earlier, this situation can be expressed simply as follows:
KEllen says m.
Although the signature-veriﬁcation process itself may be more involved in this situ-
ation, it plays the same role in Todd’s interpretation of the received message. From
Todd’s perspective, the important aspect is deducing that the message was initially
signed by Ellen using the key K−1
Ellen, which presumably only she possesses. Although
he initially had to decrypt other components of the message using the keys K−1
Todd and
Kdek, those operations alone do not associate the message m to Ellen.
These two examples provide a general framework for expressing digitally signed
and veriﬁed statements in our logic. Whenever a digitally signed message m has
been veriﬁed using the key KP in a process such as the one shown in Figure 6.4, we
represent the message as
KP says m.
We can express the belief that the key KP is associated with the principal P as follows:
KP ⇒P.
Finally, the Derived Speaks For rule allows us to conclude that m originated from the
principal P:
P says m.
The important aspect about this abstraction is that it moves beyond the details about
signature veriﬁcation and focuses our attention on who said what.
6.4
Certiﬁcates, Certiﬁcate Authorities, and Trust
In the examples of the previous section, Todd’s reasoning depended crucially upon
his assumption that KEllen was Ellen’s key:
KEllen ⇒Ellen.
What we have not directly addressed is how Todd—or anyone else, for that matter—
would come to believe that KEllen is Ellen’s public key.
In most cases, such a belief originates from a public-key certiﬁcate, which is issued
by an alleged authority called a certiﬁcate authority (or CA, for short). A public-key
certiﬁcate is a digitally signed statement that associates a given public key with a
given principal. A certiﬁcate that associates the public key KP with principal P can
be expressed in the logic as
KCA says (KP ⇒P),

Digital Authentication
117
where KCA is the public key of the issuing certiﬁcate authority.
Although digital certiﬁcates such as these provide necessary information, they
alone are insufﬁcient to establish why Todd would believe that KEllen ⇒Ellen. Rather,
such beliefs also depend on recognition of the certiﬁcate authority and their jurisdic-
tion. For example, suppose that Cora issues a certiﬁcate signed with her private key
K−1
Cora:
KCora says (KEllen ⇒Ellen).
Furthermore, suppose that Todd believes that KCora is Cora’s public key:
KCora ⇒Cora.
From these two pieces of information, Todd is able to conclude the following:
Cora says (KEllen ⇒Ellen).
At this point, Todd has a decision to make: does he trust in Cora’s integrity, author-
ity, and accuracy when Cora says that KEllen ⇒Ellen? That is, Todd must determine
whether or not he is willing to make the following assumption regarding Cora’s trust-
worthiness and jurisdiction:
Cora controls (KEllen ⇒Ellen).
As this example demonstrates, belief in a public key generally depends upon both
a public-key certiﬁcate and trust in the certiﬁcate authority that issued the certiﬁcate.
A formal analysis of such a situation typically has the following general form:
1. Kca ⇒Certiﬁcate Authority
Trust assumption
2. Kca says KP ⇒P
Certiﬁcate
3. Certiﬁcate Authority controls (KP ⇒P)
Jurisdiction
4. Certiﬁcate Authority says KP ⇒P
1, 2 Derived speaks for
5. KP ⇒P
3, 4 Controls
This proof demonstrates a common pattern that we will observe in many deriva-
tions regarding digitally signed statements. In fact, it gives rise to a useful derived
rule, which we will see again in future chapters:
Certiﬁcate
Veriﬁcation
Kca ⇒Certiﬁcate Authority
Kca says (KP ⇒P)
Certiﬁcate Authority controls (KP ⇒P)
KP ⇒P
However, the proof also demonstrates a key aspect of public-key infrastructures,
which is that they ultimately depend upon trust in some public key. In the preceding
proof, we rely on the trust assumption Kca ⇒Certiﬁcate Authority to deduce that
KP is P’s public key. If we were unwilling to make that assumption blindly, then
we would require a public-key certiﬁcate for the Certiﬁcate Authority, which in turn
would be signed by yet another CA. Ultimately, the process of evaluating crypto-
graphically signed statements hinges upon the existence of some initial or root key

118
Access Control, Security, and Trust: A Logical Approach
that is trusted (i.e., not derived from another signed certiﬁcate). This key typically
belongs to some top-level root certiﬁcate authority and is used to read other key
certiﬁcates. The following example demonstrates the role of the trusted root key.
Example 6.1
Sally purchases a new computer from a reputable company with the operating sys-
tem and applications such as web browsers already installed. Upon setting up her
computer, Sally types the web address of her favorite Internet bookstore (Good-
Books.com) into her web browser. She connects with the bookstore’s web site and
logs onto her account, which is handled by a secure portion of the web site that relies
on a private and public key pair (K−1
GoodBooks,KGoodBooks). Because she is a careful
computer user, she veriﬁes that she has connected to the real GoodBooks.com site
by having her web browser authenticate the identity of the site. Her browser reports
the following:
The identity of this web site has been veriﬁed by TrueSignatures, Inc., a
certiﬁcate authority you trust for this purpose.
Using her browser, she looks at the public-key certiﬁcate of GoodBooks.com and
sees that it is signed by TrueSignatures, Inc. She then makes her selections, places
her order, enters her credit-card information, and then leaves the site.
We formalize Sally’s thinking using the access-control logic as shown below:
1. KTrueSignatures ⇒TrueSignatures
Trust assumption
2. KTrueSignatures says KGoodBooks ⇒GoodBooks
Public key certiﬁcate
3. TrueSignatures controls KGoodBooks ⇒GoodBooks
Jurisdiction
4. TrueSignatures says KGoodBooks ⇒GoodBooks
1, 2 speaks for
5. KGoodBooks ⇒GoodBooks
3, 4 controls
♦
The preceding example provides a useful basis for exploring the basis of trust
assumptions regarding root certiﬁcate authorities. Speciﬁcally, it is natural to ask
whether the trust assumption
KTrueSignatures ⇒TrueSignatures
is appropriate and (if so) why. In Sally’s case, the assumption seems warranted,
because she is just an ordinary consumer who bought a computer with software from
a reputable ﬁrm. Because she is not a high-value target, it is extremely unlikely that
Sally is the target of an elaborate fraud scheme to plant a fraudulent key in place of
the authentic KTrueSignatures in her computer. The company that sold her the computer
is reputable and has safeguards against such fraud (e.g., they use legitimate copies
of operating systems and application software). It is therefore reasonable for her to
trust that the root certiﬁcation authority’s key KTrueSignatures is installed correctly on
her machine.

Digital Authentication
119
FIGURE 6.6 Network of certiﬁcate authorities
On the other hand, suppose that Sally were a military planner using the computer
in a secure military complex. In this case, ruling out an elaborate fraud is not a good
basis for trusting in a root-level certiﬁcation authority. Instead, the corresponding
trust assumption of the root certiﬁcation authority would likely be based on a much
more secure process in which a military security ofﬁcer oversaw the installation of
the root key.
Given the different needs of different entities, it is not unusual to have to account
for several certiﬁcate authorities. In fact, multiple certiﬁcate authorities are common
when two or more organizations collaborate. In such situations, one certiﬁcate au-
thority will vouch for the public key of another certiﬁcation authority by issuing a
public-key certiﬁcate for the other authority. Principals who recognize the jurisdic-
tion of the ﬁrst authority can use that knowledge to rely upon or trust the public key
of the second certiﬁcate authority. We illustrate this idea in the following example.
Example 6.2
Suppose that Alice wishes to ﬁnd out Bob’s public key, either to send him a message
or to verify messages digitally signed by him. Let us also assume that Alice recog-
nizes and trusts the authority of her certiﬁcate authority CA1 and that she believes
that KCA1 ⇒CA1. Figure 6.6 shows the network of certiﬁcate authorities: CA2 is the
certiﬁcate authority for both CA1 and for Bob.
Because Alice knows the public key of CA1, she is able to verify and willing
to believe the public-key certiﬁcate for CA2 signed by CA1. Such a certiﬁcate is
sometimes called a reverse certiﬁcate, because the signer is certifying a key for an
entity higher up in the hierarchy. Once Alice deduces that KCA2 ⇒CA2, she is able
to verify and willing to believe the public-key certiﬁcate for Bob signed by CA2.
This certiﬁcate—in which the signer is certifying a key for an entity lower in the
hierarchy—is sometimes called a forward certiﬁcate.
The following proof shows the trust assumptions made by Alice, as well as the
authorities she chooses to recognize:

120
Access Control, Security, and Trust: A Logical Approach
1. KCA1 ⇒CA1
trust assumption
2. KCA1 says (KCA2 ⇒CA2)
Key Certiﬁcate
3. KCA2 says (KBob ⇒Bob)
Key Certiﬁcate
4. CA1 controls (KCA2 ⇒CA2)
Jurisdiction
5. CA2 controls (KBob ⇒Bob)
Jurisdiction
6. CA1 says (KCA2 ⇒CA2)
1,2 Derived Speaks For
7. KCA2 ⇒CA2
4,6 Controls
8. CA2 says (KBob ⇒Bob)
7, 3 Derived Speaks For
9. KBob ⇒Bob
5,8 Controls
The ﬁrst line is Alice’s belief that she knows the public key of CA1. Lines two
and three are the public-key certiﬁcates for CA2 (signed by CA1) and Bob (signed
by CA2). Lines four and ﬁve represent Alice’s trust in CA1’s reliability regarding
certifying that KCA2 is CA2’s key, and in CA2’s authority to say truthfully that KBob
is Bob’s public key. Lines six through nine are the steps necessary for Alice to deduce
that KBob is indeed Bob’s public key.
♦
Exercise 6.4.1
Suppose Toni sends Blake a message in the clear that is digitally

signed as follows:
(‘You’re hired!”,encrypt(K−1
Toni,hash(You’re hired!”)))
The above message is sent over an open channel, such as the Internet. Laura in-
tercepts the message and retransmits it to Blake with one crucial change: “You’re
hired!” is changed to “You’re ﬁred!”. Because she wants Blake to think that the
message came from Toni, she reuses Toni’s digital signature from the message she
intercepted.
What happens when Blake receives the message? Is he able to detect the forgery
or not?
Exercise 6.4.2
Use the Certiﬁcate Veriﬁcation rule to give an alternate proof for

the situation in Example 6.2.
Exercise 6.4.3
This question relates to an online-purchase transaction, in which

Zoey logs into a previously established account with her userid-password combina-
tion. She then attempts to purchase an airline ticket, using her credit card. In what
follows, you should interpret CCZ as a principal representing a speciﬁc credit-card
number/account.
a. Give English translations for each of the following statements, using notions
such as authority (or jurisdiction), signatures, sessions, and access requests.
(a) PwdServer controls (⟨zoey,pwd⟩⇒Zoey)
(b) Visa controls ((Zoey | CCZ) controls buyticket)
(c) KP ⇒PwdServer

Digital Authentication
121
(d) KV ⇒Visa
(e) (⟨zoey,pwd⟩| CCZ) says buyticket
(f) KP says (⟨zoey,pwd⟩⇒Zoey)
(g) KV says ((Zoey | CCZ) controls buyticket)
b. Give a formal proof that buyticket can be deduced from the seven assumptions
listed above.
Exercise 6.4.4
Dana has registered her public key KD with the certiﬁcate authority
CA, whose public key is KCA. She wishes to send a message m to Earl, who does not
know Dana and does not recognize the authority of CA. Fortunately, Dana knows
Earl’s good friend Finn, who does recognize CA’s authority.
Furthermore, both Earl and Finn work at Gaggle and have registered their public
keys (KE and KF, respectively) with the company’s certiﬁcate authority; the public
key for Gaggle’s certiﬁcate authority is KG. (Note that both Earl and Finn have
access to Gaggle certiﬁcates, but they have never exchanged their public keys with
one another before.)
Dana signs her message using her private key, and sends it to Finn. She asks Finn
to forward it to Earl, which Finn does as follows:
• Finn uses his private key to sign Dana’s signed message, and sends the result
to Earl.
• Finn also gets a copy of Dana’s public-key certiﬁcate, and signs and sends that
to Earl.
• Finn signs a message that indicates what CA’s public key is, and sends that to
Earl.
• Finally, Finn signs and sends to Earl a message stating his conﬁdence in CA’s
jurisdiction over Dana’s public key.
a. Express in the access-control logic the four items that Finn sends to Earl.
b. Identify the additional certiﬁcates, recognitions of authority, and any other
trust assumptions that Earl requires in order to deduce that the message m
truly came from Dana. You should express each of these items in the access-
control logic.
c. Give a formal proof that m can be deduced from your answers to parts (a)
and (b).
d. In your opinion, which if any of the assumptions from part (b) are the most
suspect? Explain your answer.

122
Access Control, Security, and Trust: A Logical Approach
Exercise 6.4.5
Two high-tech companies—NanoTech and PicoWare—are col-
laborating on a joint project called ILLUMINA. The project’s repository and web
page are housed at PicoWare’s facilities .
When the two companies initiated this project, they established a special certiﬁ-
cate authority (CA) that both companies would trust with respect to high-level com-
pany keys for the purposes of this project only. The public-private key pairs for the
various entities are as follows:
(KCA,K−1
CA)
CA’s key pair, trusted by both companies
(KN,K−1
N )
NanoTech’s key pair, with KN registered with CA
(KP,K−1
P )
PicoWare’s key pair, with KP registered with CA
This setup allows both NanoTech and PicoWare to maintain their existing company
key infrastructures, and neither company has to blindly trust the other’s infrastruc-
ture. Speciﬁcally:
• NanoTech’s public key has been installed on all machines at NanoTech;
therefore NanoTech machines do not need to fetch NanoTech’s certiﬁcate
from CA.
• Likewise, PicoWare’s public key has been installed on all machines at Pi-
coWare; therefore PicoWare machines do not need to fetch PicoWare’s cer-
tiﬁcate from CA.
• Neither company’s public key has been installed on any of the other company’s
machines. Therefore, certiﬁcates from CA are necessary for any secure cross-
company communication.
• Each company certiﬁes its own internal keys: the keys for all NanoTech em-
ployees and NanoTech machines are certiﬁed by NanoTech, and the keys for
all PicoWare employees and PicoWare machines are certiﬁed by PicoWare.
(a) The access-control policy for the ILLUMINA project web page grants both
read access (⟨read,iwp⟩) and write access (⟨write,iwp⟩) to all members of the
ILLUMINA project. Express this access-control policy as an expression in the
access-control logic.
(b) Stan—a NanoTech employee—has been issued the key pair (KS,K−1
S ). Ex-
press Stan’s public-key certiﬁcate as an expression in the access-control logic.
(c) Stan has been assigned to the ILLUMINA project team, and the group database
at PicoWare has been updated to reﬂect this group membership. If ever re-
quested, the group-database server (GDS) can digitally sign a statement that
attests to Stan’s group membership; the group-database server’s key pair is
(KGDS,K−1
GDS).
Express such a certiﬁcate as an expression in the access-control logic.

Digital Authentication
123
Stan logs into his NanoTech ofﬁce computer, using his NanoTech-issued smart
card. He then tries to establish an SSL connection to the ILLUMINA WEB PAGE at
PicoWare. To authenticate the SSL connection, Stan’s smart card uses his private
key to cryptographically sign a response to a challenge from the PicoWare server.
The SSL connection is secured with a session key Kssl. From the point of view of the
web server at PicoWare, the result of successfully establishing this SSL connection
is the following trust assumption:
Kssl ⇒KS
(∗)
(d) PicoWare’s web server receives (via SSL) Stan’s request to read the ILLU-
MINA web page. Express this request as an expression in the access-control
logic.
(e) What are the additional certiﬁcates, recognition of authority, and trust as-
sumptions regarding keys that are necessary for the PicoWare web server to
determine that the read request should be granted?
(f) Give a formal proof that justiﬁes granting the request. Your only assump-
tions should be your answers to (a) through (e), plus the trust assumption (∗).
Exercise 6.4.6
Some access-control decisions are made based on attributes (prop-
erties such as one’s age or the amount of money in your checking account) rather
than identity. For example, consider what happens when you use a fare card like the
Metro card in Washington, D.C.’s Metro system:
• From the standpoint of the rider, there are two resources to be accessed: the
train at the starting destination and the exit to the street at the ending destina-
tion.
• The Metro card is a certiﬁcate issued by the individual Metro stations and
possessed by the rider. Note, however, that while the physical card may be the
same before entering and after leaving the Metro station, it is not the same
certiﬁcate:
– The information on the Metro card (particularly the amount of money left
on the card) must be updated.
– The authorities that sign or vouch for the amount on the card are dif-
ferent: the turnstile used at the beginning of a rider’s trip is different
from the turnstile used at the end of the trip. Distinguishing the separate
turnstiles (more accurately, the different stations) permits one to charge
different amounts for different trips: the exit turnstile can always deter-
mine where the trip originated and how much to deduct from the card.
To see how this works, consider a rider who wishes to travel from Station A to
Station B, which is a trip that costs $1. Let’s assume that the rider’s most recent exit
was at Station C, with a balance of $5 on the card.

124
Access Control, Security, and Trust: A Logical Approach
• The rider inserts the Metro card into the turnstile at Station A. This card is
(ultimately) interpreted as:
Station C says (balance=$5∧exit= Station C).
Because the balance is greater than $0, access to the train is granted; the card
is returned to the rider saying (in effect):
Station A says (balance=$5∧entry= Station A).
• The rider boards the train and travels to Station B.
• The rider inserts his or her Metro card into the turnstile at Station B. Because
the balance is sufﬁcient to cover the cost of the trip, the turnstile returns the
Metro card with an updated balance of $4.
Assume that the turnstiles at the three stations (A, B, and C) form a private net-
work called MetroNet. There is a single certiﬁcation authority CA that issues pub-
lic/private key pairs and certiﬁes membership in MetroNet. The relevant keys are as
follows:
(KCA,K−1
CA)
CA’s key pair
(KA,K−1
A )
Station A’s key pair
(KB,K−1
B )
Station B’s key pair
(KC,K−1
C )
Station C’s key pair
Digital signatures (based on the public and private keys distributed by the CA) are
used to provide protection against fraudulent cards. Thus, the fare cards are actually
digitally signed certiﬁcates.
Use expressions in the access-control logic to answer the following questions
regarding the certiﬁcations, credentials, and access-control policies needed for a
rider to travel from Station A to Station B. Treat attributes such as “balance=$5,”
“balance > $0,” and “exit= Station C” as simple propositions. Use ⟨enter,A⟩and
⟨exit,B⟩to denote entering at Station A and leaving at Station B, respectively.
a. What are the necessary certiﬁcates, recognition of authority, and trust assump-
tions about keys for a turnstile at Station B to recognize Station A as belonging
to MetroNet?
b. What are the necessary certiﬁcates, recognition of authority, and trust assump-
tions about keys for a turnstile at Station B to interpret an arbitrary fare card
signed by Station A?
c. What is the access policy of turnstiles at Station A that guard entry to trains?
d. Assuming that a turnstile at Station A grants entry access to a rider, what
certiﬁcate is given back to the rider as he or she passes through the turnstile?
e. Assuming that a turnstile at Station B allows a rider to exit, what certiﬁcate is
given back to the rider as he or she passes through the turnstile?

Digital Authentication
125
6.5
Symmetric-Key Cryptography
As mentioned earlier in this chapter, an alternative to public-key cryptography
is shared-key cryptography, also known as symmetric-key cryptography because the
same key is used for both encryption and decryption. If encrypt and decrypt are
particular symmetric-key encryption and decryption functions, then the following
property must hold for all messages m and keys K:
decrypt(K,encrypt(K,m)) = m.
Symmetric-key encryption algorithms—such as the Advanced Encryption Stan-
dard (AES) (National Institute of Standards and Technology, 2001)—provide a sig-
niﬁcant advantage over public-key algorithms in terms of efﬁciency. They are typi-
cally three orders of magnitude faster than public-key algorithms, because of the un-
derlying operations they employ (i.e., exclusive-or and bit substitutions as opposed
to modulo-n exponentiation). For this reason, it is often preferable to encrypt large
messages or ﬁles using symmetric-key cryptography.
However, shared-key cryptography also has disadvantages that arise from its use
of a single key. The ﬁrst problem is one of planning: Alice can send a secure mes-
sage to Bob only if they have previously agreed upon a speciﬁc key to use. Such an
arrangement should happen via out-of-band communication, such as during a face-
to-face meeting or some other reasonably secure means. In contrast, Alice can send
a secure message to Bob via public-key cryptography even if they have never com-
municated before, as long as they have both published their public keys.
Another problem with shared-key cryptography is one of scale: each principal
must maintain a separate key for every other principal with whom he or she wishes
to communicate. Thus, a collection of n principals would require on the order of n2
keys to support all possible two-way conversations; public-key cryptography would
require only 2n keys (i.e., public and private keys for each principal).
Finally, shared-key cryptography does not support the separation of privacy and
authenticity concerns. For example, suppose that Alice wishes to publish a message
(i.e., allow it to be read by anyone) in a way that provides evidence of her authorship.
We have already seen how to accomplish this goal using public-key cryptography.
However, symmetric-key cryptography does not support this goal directly: anyone
who has knowledge of the key to decrypt the message could forge a new message or
even claim to be the original author.
These disadvantages can be somewhat mitigated through the use of relays to sim-
ulate a public-key infrastructure. We demonstrate how later in Examples 6.4 and 6.5,
but we must ﬁrst address one additional pragmatic difference between public-key
and shared-key cryptography. A sender using public-key cryptography can tell the
recipient the relevant key to use. For example, if Alice sends a digitally signed mes-
sage to Bob, she can also send along her public key for his convenience. Likewise,
if she sends a secret message to Bob using his public key, she can notify him which

126
Access Control, Security, and Trust: A Logical Approach
public key she used; that information is useless to anyone who does not know Bob’s
private key.
If Alice instead uses shared-key cryptography, then she obviously cannot directly
indicate which key Bob should use to decrypt the message. However, she can ac-
complish the same goal by sending along a key identiﬁer (or key hint) that will be
meaningful only to Bob. A key identiﬁer might be as simple as an index into a table
of keys that is kept on Bob’s machine. Alternatively, Bob may have a master key that
he uses to encrypt all his other keys: the key identiﬁer for a shared key K may be the
result of encrypting K with that master key. The point here is that, when Alice and
Bob set up their shared key, they must also have exchanged their individual key iden-
tiﬁers associated with that key. However, neither needs to know how the other selects
their key identiﬁers. The following example demonstrates the use of key identiﬁers.
Example 6.3
Both Richard and Janet use master keys (KR and KJ, respectively) to encrypt all their
important keys. They have recently set up a shared key KS that will allow them
to communicate with one another securely. At that time, they also exchanged key
identiﬁers:
• Richard keeps a table of encrypted keys on his computer, and he likes to use
the indices of this table as his key identiﬁers. Richard adds the encrypted
shared key (i.e., encrypt(KR,KS)) to this table, and gives the corresponding
index (e.g., 213) to Janet as the key identiﬁer to use for this key.
• Janet prefers to use encrypted keys themselves as key identiﬁers. She therefore
encrypts the shared key with her master key and gives the result to Richard as
her key identiﬁer for the key: encrypt(KJ,KS).
When Janet sends a secure message to Richard using the shared key KS, she also
sends him his key identiﬁer (i.e., 213) for the key. Richard is able to look up the
appropriate entry in his table (i.e., encrypt(KR,KS)), decrypt it with his master key
KR to obtain KS, and then use KS to decrypt Janet’s message.
Similarly, when Richard sends a secure message to Janet using the shared key KS,
he also includes her key identiﬁer (i.e., encrypt(KJ,KS)). Janet knows to decrypt
this hint with her master key KJ to obtain the key necessary to decrypt Richard’s
message.
♦
As the preceding example shows, two principals may have very different key iden-
tiﬁers for the same shared key. What is important is that each knows the other’s key
identiﬁer for the shared key, so that they can always clearly communicate which key
is being used. (In fact, two principals may share multiple keys, using each for a dif-
ferent purpose. The use of key identiﬁers allows them to indicate which is relevant
for the given communication.)

Digital Authentication
127
We adopt the notational convention1 of using principals’ names or initials as super-
scripts on keys to denote a given principal’s key hint for a speciﬁc key. For example,
we use KR
S and KJ
S to denote Richard’s and Janet’s key hints for the shared key KS.
We can then associate the key hints with the statements that they make (i.e., with the
messages encrypted by the keys they hint at). Thus, if Janet sends Richard the mes-
sage m encrypted with the shared key Ks, we can express Richard’s interpretation as
follows:
KR
S says m.
If Richard trusts Janet to keep the shared key KS private, then he should be willing to
associate messages encrypted by KS with Janet. That is, he should be willing to view
the key hint KR
S (and the key it represents) as a proxy for Janet:
KR
S ⇒Janet.
The following series of examples, inspired by a discussion in (Lampson et al.,
1992), demonstrates how a symmetric-key system can mimic a public-key infras-
tructure by judicious use of key identiﬁers. The approach hinges on the use of re-
lays, trusted agents that serve as encryption/decryption intermediaries. Just as any
principal can register a public-key with a certiﬁcate authority in a PKI, the relay
system requires that any principal Q be able to register with a special trusted agent.
Furthermore, the trusted agent must know the relay’s master key, which the relay
uses for creating its key identiﬁers (similar to Janet’s method for key identiﬁers in
Example 6.3).
Example 6.4
Quinton wishes to use symmetric encryption to send a message that Shelby can read
and verify as being from Quinton. Although Shelby and Quinton do not share a key,
they have both previously established secret-key channels with the trusted relay R by
registering with a trusted agent TA. When Quinton registered, TA created and sent
back to Quinton two items:
1. A secret key Kq to be shared between Quinton and the relay R
2. R’s key identiﬁer KR
q = encrypt(Km,Kq) for this key, which TA creates by
encrypting the new key Kq with R’s master key Km
TA also publishes a secret-key certiﬁcate for Quinton that contains two components
(we introduce the notation ⟨⟨m1,m2,...,mk⟩⟩to denote the concatenation of the k
items m1 through mk):
encrypt(Kta,⟨⟨KQ
q ,KR
q ,Quinton⟩⟩),
KR
ta.
1For convenience, we often blur the distinction between keys and key identiﬁers when using the logic,
such as in the network case studies of Chapter 8. The important point of this discussion is that key hints
can be used both in practice and in the logic to shroud secret keys.

128
Access Control, Security, and Trust: A Logical Approach
The ﬁrst item associates the pair (KQ
q ,KR
q ) of key identiﬁers with Quinton; this item
is signed by TA using the key Kta that it shares with the relay. The second item is
the relay’s key hint for the shared key Kta, so that anyone who wishes to read the
certiﬁcate can direct the relay which key to use for decryption. Unlike a public-key
certiﬁcate, the shared-key certiﬁcate is not readable by just anybody: only a principal
that knows the secret key Kta (such as TA and the relay) can read it.
Quinton encrypts his message m with the key Kq, sending both the encrypted mes-
sage and the key hint KR
q to Shelby:
encrypt(Kq,m),
KR
q .
Shelby cannot decrypt the message directly, because she does not know the shared
key that KR
q identiﬁes. However, because Shelby also registered with TA, she shares
a secret key Ks with the relay and knows the relay’s key hint KR
s for that key. She
therefore forwards these items on to the relay R, along with the key-identiﬁer pair
(KS
s ,KR
s ). In effect, Shelby is requesting a decryption of the encryption message and
telling the relay how to return the answer. (Shelby can also forward on Quinton’s
shared-key certiﬁcate for decryption, if she wishes to authenticate the message. We
leave this possibility for Example 6.5.)
The relay applies its master key Km to the key hint KR
q to obtain the key Kq:
decrypt(Km,KR
q ) = decrypt(Km,encrypt(Km,Kq)) = Kq.
With this key, the relay can decrypt the message encrypt(Kq,m) to obtain the original
message m:
decrypt(Kq,encrypt(Kq,m)) = m.
The relay then repackages this message for Shelby by encrypting m with the shared
key Ks = decrypt(Km,KR
s ), sending back the newly encrypted version as well as the
key hint KS
s :
encrypt(Ks,m),
KS
s .
Upon receiving this package, Shelby uses the key hint KS
s to identify the correct
key (Ks) to use to decrypt the message:
decrypt(Ks,encrypt(Ks,m)) = m.
As a result of this process, Shelby interprets this message as follows:
KS
s says KR
q says m.
That is, the key identiﬁer KS
s is claiming that the key hint KR
q was used to send
the message m. If Shelby trusts the relay to keep the key Ks secret (i.e., if she be-
lieves that KS
s ⇒R) and to accurately translate messages (i.e., R controls KR
q says m),
then she will deduce that m was the original intended message she received (i.e.,
KR
q says m).
♦

Digital Authentication
129
The previous example demonstrated how secret-key cryptography can be used to
exchange messages between principals who have not previously established a shared
secret key. However, the example as it stands does not address authentication con-
cerns: neither the relay nor Shelby are able to associate the key identiﬁer KR
q with
Quinton at the end of the process described. To do so, they must also make use of
Quinton’s secret-key certiﬁcate as demonstrated in the next example.
Example 6.5
By the end of Example 6.4, Shelby had deduced that KR
q says m. If she wishes to
authenticate Quinton as the message’s author (i.e., to associate the key hint KR
q with
Quinton), then she forwards Quinton’s secret-key certiﬁcate to the relay along with
her key-hint pair (KS
s ,KR
s ). As with the original message, the relay can decrypt the
certiﬁcate, which can be expressed in the logic as follows:
KR
ta says ((KQ
q ,KR
q ) ⇒Quinton).
That is, the key hint KR
ta claims that the key-hint pair (KQ
q ,KR
q ) is associated with
Quinton. If the relay trusts that the key hint KR
ta (i.e., the key Kta) represents TA and
that TA has jurisdiction over which keys represent which principals, then it will be
able to associate the key hint KR
q with Quinton.
At this point, the relay must create for Shelby a new certiﬁcate that associates the
key hint KR
q with Quinton. The relay sends the following items to Shelby:
encrypt(Ks,⟨⟨KR
q ,Quinton⟩⟩),
KS
s .
As before, Shelby can decrypt the message to interpret the certiﬁcate as follows:
KS
s says (KR
q ⇒Quinton).
♦
As a ﬁnal twist on this scenario, suppose that Shelby wishes to exchange a series
of messages with Quinton. She may wish to set up an authenticated channel between
herself and Quinton, so that they can exchange messages directly without having
to use the relay for continual translations. As the following example demonstrates,
Shelby can enlist the relay’s help to set up such a channel.
Example 6.6
To set up an authenticated channel with Quinton, Shelby proceeds in the same way
as at the beginning of Example 6.5. She forwards Quinton’s secret-key certiﬁcate to
the relay along with her key-hint pair (KS
s ,KR
s ). The relay decrypts this certiﬁcate
and interprets it as before:
KR
ta says ((KQ
q ,KR
q ) ⇒Quinton).

130
Access Control, Security, and Trust: A Logical Approach
The relay now creates a new channel for direct communication between Quinton
and Shelby. Speciﬁcally, the relay creates a new key K and creates key identiﬁers for
Quinton and Shelby by encrypting it with their individual keys:
KQ = encrypt(Kq,K),
KS = encrypt(Ks,K).
The relay then creates a new certiﬁcate for Shelby that associates this new key-hint
pair (KQ,KS) with Quinton:
encrypt(Ks,⟨⟨KQ,KS,Quinton⟩⟩),
KS
s .
Shelby can interpret this certiﬁcate as follows:
KS
s says ((KQ,KS) ⇒Quinton).
(If Quinton wishes to also authenticate Shelby, then the relay will need to obtain
Shelby’s original secret-key certiﬁcate and then generate a new one for Quinton to
use in a similar fashion.)
♦
Exercise 6.5.1
Consider the certiﬁcate that Shelby receives at the end of Exam-
ple 6.5.
a. What additional trust assumptions must Shelby make in order to identify Quin-
ton as the sender of the original message from Example 6.4?
b. Another possible interpretation in the logic of that certiﬁcate is as follows:
KS
s says (KS
s | KR
q ⇒Quinton).
Given this interpretation, what additional trust assumptions must Shelby make
in order to identify Quinton as the sender of the original message from Exam-
ple 6.4?
c. Brieﬂy describe the advantages and disadvantages of the two interpretations,
and identify the situations where each might be more appropriate.
Exercise 6.5.2
What additional trust assumptions must Shelby make to associate

the new key K (or its associated key-hint pair) with Quinton in Example 6.6?
Exercise 6.5.3
Suppose that two-way authentication is necessary in Example 6.6
(i.e., Quinton also wishes to authenticate Shelby).
a. What messages need to be sent between Quinton and the relay?
b. Express in the access-control logic how these messages are interpreted, along
with the trust assumptions necessary for the protocol to succeed.

Digital Authentication
131
6.6
Summary
We have already seen that authentication—that is, identifying the originator of a
request—is critical to sound access control. In the digital world, authentication is
made more difﬁcult by the physical absence of the requesting principal: a request
arrives as a sequence of bits, with only hints of its origin.
In this chapter, we saw how public-key infrastructures support digital authentica-
tion through the use of public and private keys, digital signatures, public-key certiﬁ-
cates, and certiﬁcate authorities. A PKI supports integrity, privacy, and authentica-
tion by providing the means to associate speciﬁc keys—and, subsequently, speciﬁc
statements—with speciﬁc principals. We explored how to express and reason about
the fundamental concepts of PKI in the access-control logic. We also discussed the
main differences between symmetric and asymmetric cryptography.
The learning outcomes associated with this chapter appear in Figure 6.7.
6.7
Further Reading
The Handbook of Applied Cryptography (Menezes et al., 1997) is a very math-
ematical and authoritative reference for cryptographic algorithms, hash algorithms,
digital signatures, and cryptographic protocols. William Stallings’ book Cryptogra-
phy and Network Security Principles and Practices (Stallings, 2003) is a less com-
prehensive but more accessible reference.

132
Access Control, Security, and Trust: A Logical Approach
FIGURE 6.7 Learning outcomes for Chapter 6
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
• Describe the characteristics of private-key and secret-key cryptographic
systems.
• Describe basic principles of trust topologies and networks of certiﬁcation
authorities.
Application
• When given protocol descriptions and trust hierarchies, you should be
able to use the access-control logic to describe the protocol and trust
relationships.
• When given a trust topology, you should be able to determine the neces-
sary certiﬁcates for establishing trust in a key.
Analysis
• When given a set of certiﬁcates, you should be able to formally derive
whether a key is associated with a particular principal.
Synthesis
• When given a description of a trust topology, you should be able to cre-
ate a formal description of the certiﬁcates and trust relationships for the
certiﬁcation authorities.

Chapter 7
Delegation
Central to the study of distributed access control is the notion of delegation. A fun-
damental property of distributed systems is their lack of locality: the originators of
requests, the principals that vouch for various forms of identity and authorizations,
and the reference monitors that guard resources are generally not in the same loca-
tion. Furthermore, requests are typically made by delegates, in many cases processes
operating on behalf of people. Likewise, credentials are signed not by human hands
but with digital signatures. A critical question naturally arises: how do we evaluate
access requests made by principals acting on behalf of others?
In this chapter, we explore how to reason about delegation and evaluate such re-
quests. We extend the access-control logic to formalize delegation, identifying some
important properties along the way.
7.1
Simple Delegations
The simplest of delegations arises when one principal simply passes on a request
from another principal. This situation occurs frequently in distributed systems, such
as when a computer or software application communicates a request on behalf of
a user. We can express such delegations in the access-control logic by quoting, as
demonstrated in the following simple example.
Example 7.1
Linda is running late at work. She, her daughter Emily, and her husband have dinner
reservations that evening for 7 p.m. Linda calls home and her daughter answers the
phone. Linda says, “Emily, I’m running late at work. Would you ask Dad to call
the restaurant to change our dinner reservations to 7:30 p.m.?” After hanging up
the phone, Emily tells her father, “Dad, Mom just called to say she’s running late at
work. She wants you to call the restaurant to change the dinner reservations to 7:30.”
Dutifully, Emily’s father picks up the phone, calls the restaurant, and changes their
dinner reservation to 7:30 p.m.
♦
The above situation happens countless times in various forms every day. We can
systematically analyze the structure of the above example from an access-control
133

134
Access Control, Security, and Trust: A Logical Approach
perspective. Consider the following sequence of questions and answers.
• Who is making the request and on whose behalf is the request being made?
Emily is making the request on behalf of her mother. Emily is quoting Linda
by passing on Linda’s request to her father.
• To whom is the request made?
Emily’s father.
• What is the request?
Change the dinner reservation to 7:30 p.m.
• How do we represent Linda’s request relayed by Emily?
The compound principal Emily | Linda represents Emily quoting Linda. The
actual request is represented by the atomic proposition
⟨Change to 7:30 p.m., dinner reservation⟩.
Putting everything together, the request is represented as follows:
Emily | Linda says ⟨Change to 7:30 p.m., dinner reservation⟩
• Emily’s father acted as if he believed Emily was correctly relaying Linda’s
request. How is this belief represented?
Emily’s father’s trust in what Emily tells him essentially means that, if Emily
says Linda says “change the dinner reservation to 7:30 p.m.,” then he con-
cludes Linda has indeed said “change the dinner reservation to 7:30 p.m.”
Emily’s father’s trust in her word is represented as:
(Emily | Linda says ⟨Change to 7:30 p.m., dinner reservation⟩) ⊃
Linda says ⟨Change to 7:30 p.m., dinner reservation⟩.
• Emily’s father changed the reservation once he heard what he interpreted to
be Linda’s request. What is the basis for this decision?
Emily’s father acted as if Linda had jurisdiction over changing the time of their
dinner reservation. This belief is represented as follows:
Linda controls ⟨Change to 7:30 p.m., dinner reservation⟩.
• Given the above, is Emily’s father’s decision to change the dinner reservation
justiﬁed?
Yes, and we can prove it using the inference rules of our access-control logic.
Situations like the previous example occur often in many forms. Formally captur-
ing the logic behind this kind of situation enables us to reason about delegations. In
the next section we formalize the notion of delegation illustrated in the above exam-
ple, develop derived inference rules, and prove some important delegation properties.

Delegation
135
7.2
Delegation and Its Properties
Delegation is used frequently in networks of all kinds—human, computer, and
organization—where requests, authorities, and decisions are in different locations.
The idea is that, in many situations, one principal must make a request on behalf of
another principal. For example, an investor cannot directly sell her own stocks but
must instead rely on her stockbroker to do so. Similarly, when a banking customer
attempts to pay his bills online, the associated requests arrive not directly from him
but from his web browser acting on his behalf.
When a principal P makes a request on behalf of principal Q and we believe as a
result of P’s statement that Q is making the request, then we say that P is Q’s delegate
(or representative) on that statement. To represent such situations in our logic, we
extend the BNF speciﬁcation of our logic with the following production rule:
Form ::= (Princ reps Princ on ϕ)
The reps operator has the same binding precedence as says and controls. Hence, for
example, the formula Jane reps Paul on ⟨buy⟩∧⟨sell⟩is equivalent to
(Jane reps Paul on ⟨buy⟩)∧⟨sell⟩.
As with the controls operator in Chapter 2, the reps operator is simply syntactic
sugar for an implication:
P reps Q on ϕ def
= (P | Q says ϕ) ⊃Q says ϕ.
Using this deﬁnition, it is easy to prove the following equivalence:
P reps Q on ϕ ≡P controls (Q says ϕ).
Essentially, if we believe that P is Q’s representative on the statement ϕ, then we
trust P when she says that Q said ϕ.
There are three crucial properties of the delegation relationship that the formal
deﬁnition of delegation must capture:
1. A recognized delegate should in fact have the authority to act on behalf of the
principals they represent. That is, if a given policy allows principals to delegate
to others and recognizes that Barb is Holly’s delegate, then Barb should be able
to act on Holly’s behalf.
2. Delegates generally should not be able to restrict the scope of their duties as a
principal’s representative. For example, suppose that Garth delegates to Mary
the task of withdrawing $500 from his checking account and depositing it to
his savings account. Mary should not be able to withdraw the funds without
also depositing them; to do so would be a violation of her responsibilities, not
to mention theft.

136
Access Control, Security, and Trust: A Logical Approach
3. The delegation relationship generally is not transitive: a delegate should not
be able to pass on his responsibilities to someone else.
The ﬁrst property—that recognized delegates should be able to act on behalf of
the principals they represent—is reﬂected by the Reps rule:
Reps
Q controls ϕ
P reps Q on ϕ
P | Q says ϕ
ϕ
This rule states that if Q is authorized to perform ϕ, P is recognized as Q’s delegate
on ϕ, and P requests ϕ on Q’s behalf, then the request for ϕ should be granted. This
rule can be derived from the deﬁnition of P reps Q on ϕ and our other inference rules,
and thus the rule is sound itself.
The second and third properties both state things that should not happen. For
that reason, it is necessary to verify that our deﬁnition of delegation prohibits the
reduction or passing on of delegation duties. The following two rules, which would
allow the undesired behavior, can easily be shown to be unsound with respect to the
Kripke semantics:
Unsound Rule!
P reps Q on (ϕ1 ∧ϕ2)
P reps Q on ϕ1
,
Unsound Rule!
P reps Q on ϕ
Q reps R on ϕ
P reps R on ϕ
.
The ﬁrst rule—if sound—would allow a delegate to restrict the scope of his duties.
The second rule—if sound—would allow a delegate to pass on her responsibilities
to someone else.
Figure 7.1 contains the formal deﬁnition of delegation, along with some useful
logical rules for delegation. Each of these rules can be derived from existing infer-
ence rules; the derivations themselves are left as exercises.
The following example illustrates how delegation can be used to represent com-
mon proxy situations, such as proxies at shareholder meetings. It also illustrates how
principals can adjust the scope of delegation to meet their different needs.
Example 7.2
MagnaProﬁt Incorporated is a publicly traded company that produces accounting
software. At this year’s upcoming shareholder’s meeting, there are three primary
issues to be voted upon:
1. MagnaProﬁt wishes to make a change to their employee stock-purchase pro-
gram (ESPP), which needs to be approved by a majority of shareholders.
2. MagnaProﬁt has selected Kim to ﬁll a vacancy on the company’s board of
directors, a selection that must be ratiﬁed by a majority of shareholders.
3. A shareholder has proposed a revision to the selection procedure for Mag-
naProﬁt’s board of directors; the change will happen only if supported by a
majority of shareholders.

Delegation
137
FIGURE 7.1 Logical rules for delegation
P reps Q on ϕ def
= P | Q says ϕ ⊃Q says ϕ
Reps
Q controls ϕ
P reps Q on ϕ
P | Q says ϕ
ϕ
Rep Controls
A reps B on ϕ ≡(A controls (B says ϕ))
Rep Says
A reps B on ϕ
A|B says ϕ
B says ϕ
These votes all require a majority of all shareholders to approve them, regardless of
which shareholders actually attend the meeting. Realizing that many shareholders
cannot attend the meeting, MagnaProﬁt sent a proxy statement to each shareholder,
which included a ballot as well as a designated default proxy (Janet). Shareholders
may select a different proxy if they prefer. Shareholders can use the ballot to specify
precisely how the proxy must vote on their behalf. If a shareholder fails to specify
how the proxy should vote for a particular ballot issue, then the proxy may vote
however he or she pleases.
Aaron and Ruth are both shareholders in MagnaProﬁt who are unable to attend the
meeting. Aaron is in favor of all three proposals, and he is willing to accept Janet as
his proxy. He therefore records his desired votes, signs the form, and mails it back
to MagnaProﬁt. In effect, his signed form can be interpreted as follows:
Aaron says (Janet reps Aaron on (⟨yes,ESPP⟩∧⟨yes,Kim⟩∧⟨yes,revise⟩)).
In this case, Janet must vote yes on all three proposals; she does not have the author-
ity to cast other votes on Aaron’s behalf.
In contrast, Ruth prefers to have her cousin Lou represent her at the meeting. Ruth
is also in favor of both the stock-purchase plan and of Kim as a director, but she
is undecided on the remaining issue. Because she believes that discussion at the
meeting should be brought to bear on her vote, she decides to let Lou decide her vote
on the shareholder proposal. She therefore records her two votes, leaves the third
issue unmarked, designates Lou as her proxy, and mails the form back. Her signed
form can be interpreted as follows:
Ruth says (Lou reps Ruth on (⟨yes,ESPP⟩∧⟨yes,Kim⟩∧⟨yes,revise⟩)
∧Lou reps Ruth on (⟨yes,ESPP⟩∧⟨yes,Kim⟩∧⟨no,revise⟩)).
In this case, Lou must vote yes on the ﬁrst two issues. However, Lou has the au-
thority to cast either a yes or no vote on the proposal to revise the director-selection

138
Access Control, Security, and Trust: A Logical Approach
procedure.
♦
It is important to note that the deﬁnition of P reps Q on ϕ and its properties describe
a particular kind of delegation, in which delegation deals only with statements ϕ
made by Q. In the following example, we analyze a situation that is often informally
thought of as delegation but that does not correspond to our deﬁnition.
Example 7.3
Consider the following scenario:
Beth is a director of a not-for-proﬁt organization that is running a rafﬂe
as part of a fund-raising event. Beth reaches into a basket of names and
draws Tanya’s name. Tanya is not present to receive her prize, but she is
still entitled to it. Dawn approaches Beth after the drawing and says that
she is Tanya’s friend and work colleague and that she will take Tanya’s
prize to her. Beth, for whatever reason, takes Dawn at her word and
gives Tanya’s prize to Dawn, with the expectation that Dawn will get
the prize to Tanya.
Here, Dawn is acting on Tanya’s behalf but not as her delegate; in fact, Tanya is
currently unaware that Dawn is acting on her behalf. Instead, Dawn has asked to
convey Tanya’s prize to Tanya, in effect making the following statement to Beth:
“Give me Tanya’s prize, and if you give me Tanya’s prize I will give the
prize to Tanya.”
From Beth’s perspective, the request from Dawn corresponds to the following logical
statement:
Dawn says (⟨Give Dawn, prize⟩∧(⟨Give Dawn, prize⟩⊃⟨Give Tanya, prize⟩)).
It is straightforward to see that Dawn’s request does not match the form of a delegate
acting on Tanya’s behalf. Therefore, Dawn is not Tanya’s delegate, even though she
is doing a favor for her.
If Beth trusts Dawn, then this trust can be represented as follows:
Dawn controls (⟨Give Dawn, prize⟩∧(⟨Give Dawn, prize⟩⊃⟨Give Tanya, prize⟩)).
The Controls inference rule allows Beth to conclude
⟨Give Dawn, prize⟩∧(⟨Give Dawn, prize⟩⊃⟨Give Tanya, prize⟩),
and the Simpliﬁcation and Modus Ponens rules further allow her to conclude
⟨Give Tanya, prize⟩.
That is, because Beth trusts Dawn, she concludes that she should give Tanya’s prize
to Dawn and that, by so doing, Tanya will receive her prize.
♦

Delegation
139
The previous example demonstrated an informal notion of delegation that did not
match our deﬁnition. However, the same scenario does involve a delegation that does
match our deﬁnition: Tanya serves as Beth’s delegate to deliver the prize to Tanya.
The following example highlights the relevant analysis.
Example 7.4
Recall the scenario from Exercise 7.3:
Tanya won a prize in Beth’s rafﬂe, but was not present at the rafﬂe draw-
ing and did not know she won. Thus, Tanya was not able to pick up her
prize herself. Tanya’s friend and colleague Dawn was there and offered
to take Tanya’s prize to Tanya, which Beth accepted. Beth asks Dawn to
take the prize to Tanya and give it to her, which Dawn does.
Here, Beth through Dawn is asking Tanya to accept the prize that she won at the raf-
ﬂe. From Tanya’s perspective, the request she receives from Dawn can be expressed
as follows:
Dawn | Beth says ⟨accept, rafﬂe prize⟩.
In this case, we can see that Dawn is Beth’s delegate. Beth through Dawn is
asking Tanya to accept the prize. If Tanya trusts Dawn about the rafﬂe prize and also
regards Beth as being in charge of the rafﬂe, then she in effect believes the following
two statements:
Dawn reps Beth on ⟨accept, rafﬂe prize⟩,
Beth controls ⟨accept, rafﬂe prize⟩.
The Reps Rule allows Tanya to conclude that she should accept the prize:
⟨accept, rafﬂe prize⟩.
♦
Exercise 7.2.1
Using the formal deﬁnition of delegation and the Reps Rule, provide

a formal analysis of Example 7.1.
Exercise 7.2.2
Give a formal proof of the Reps inference rule from Figure 7.1.
Exercise 7.2.3
Give a formal proof of the Rep Controls rule from Figure 7.1.
Exercise 7.2.4
Give a formal proof of the Rep Says rule from Figure 7.1.
Exercise 7.2.5
Suppose Naresh and Sunita are both professors in the Department
of Electrical and Computer Engineering. Sunita will be away at a conference during
the department faculty meeting when votes for department chair will be cast. Sunita
wishes to cast her vote for Maja, and she trusts Naresh to be her proxy. Because it’s
an ofﬁcial vote, she writes on a piece of paper:

140
Access Control, Security, and Trust: A Logical Approach
FIGURE 7.2 Grace’s health-care proxy
Health-Care Proxy
1. I, Grace, hereby appoint Norm as my health care agent to
make decisions on whether or not to resuscitate me in the
event I am unable to make my own health care decisions.
2. Grace’s signature.
3. Witness: I, Kirstin’s signature, witnessed Grace sign this
proxy of her own free will in my presence.
Naresh is my proxy. I authorize him to cast my vote for Maja as depart-
ment chair.
Sunita turns the paper into George prior to the meeting. George is the faculty chair
who will count the votes at the department meeting.
At the faculty meeting, the vote is called. Naresh casts his vote and then he says,
“As Sunita’s proxy I am casting her vote for Maja as department chair.” George,
who is tabulating the votes, notes Naresh’s vote and then puts down one vote for
Maja cast by Sunita.
Represent Naresh’s casting of one vote for Maja by Sunita. What must the depart-
ment policies be regarding proxy votes? Give the inference rule that corresponds to
accepting Sunita’s vote by proxy and prove that it is sound.
Exercise 7.2.6
Consider Grace’s health-care proxy as shown in Figure 7.2. Her
proxy authorizes Norm to be her delegate in the event she is unable to make her own
health care decisions. Her proxy is witnessed by Kirstin.
In a tragic turn of events, Grace is involved in a car accident that leaves her in a
coma. Grace is taken to the hospital where she is cared for by Dan, her physician.
Norm tells Dan that Grace is not to be resuscitated. Dan, after reading Grace’s
health-care proxy, writes “Do Not Resuscitate” on Grace’s chart.
Answer the following questions:
a. Represent Grace’s health-care proxy in the access-control logic.
b. Express the hospital’s policy regarding delegation and the rights of patients
not to be resuscitated in the event they are unable to communicate for them-
selves.
c. Represent Dan’s decision not to resuscitate Grace as an inference rule.
d. Formally derive and prove Dan’s inference rule.
Exercise 7.2.7
Demonstrate the unsoundness of the following inference rule:

Delegation
141
Unsound Rule!
P reps Q on (ϕ1 ∧ϕ2)
P reps Q on ϕ1
.
Exercise 7.2.8
Demonstrate the unsoundness of the following inference rule:
Unsound Rule!
B controls ϕ1
A reps B on (ϕ1 ∧ϕ2)
A|B says ϕ1
ϕ1
.
Exercise 7.2.9
Demonstrate the unsoundness of the following inference rule:
Unsound Rule!
P reps Q on ϕ
Q reps R on ϕ
P reps R on ϕ
.
7.3
A Delegation Example: Simple Checking
Delegation is so commonplace that we often do not even recognize its relevance
to a given system. If we are responsible for designing or constructing a system that
uses delegation, however, then we are well served if we understand precisely how
delegation is being used, as well as the assurances that must be in place.
For example, the use of paper checks as payment instruments relies on notions
of delegation, although most check writers never think about that in the course of
conducting their daily business. In this section, we highlight the role of delegation in
the use of paper checks in a simple consumer scenario; we expand on this treatment
in Chapter 8.
For this example, we consider a situation where Alice wishes to purchase goods
from Bob, her local grocer. For simplicity, we assume that Alice and Bob are both
depositors of the same bank.1 Alice wishes to pay Bob, but she does not have sufﬁ-
cient cash on hand (or she chooses not to use it). Instead, she writes a check to Bob,
which is in fact an order to the bank to debit the given amount from her account and
to pay it to Bob. Alice hands the check to Bob, as shown in Figure 7.3. Because Bob
wishes to be paid, he heads to the bank, endorses Alice’s check by signing the back
of it, and presents the check to the teller for payment.
Suppose that we are in charge of designing the operation of the bank used by Alice
and Bob. Our objective is to come up with a concept of operations that supports the
use of checks written and cashed by bank customers. We would like to know that
these operations are justiﬁed—that is, the operations should be logically sound with
respect to bank policies set by relevant authorities. Accomplishing this goal requires
three steps, which we follow:
1. Give a formal deﬁnition in the access-control logic of checks and endorsed
checks.
1We address the more complicated situation where Alice and Bob are depositors of different banks in
Chapter 8.

142
Access Control, Security, and Trust: A Logical Approach
FIGURE 7.3 Simple checking
2. Formally describe the bank policies regarding checks.
3. Describe the bank’s operating rules with respect to checks as inference rules,
which we prove to be sound by deriving them using our existing inference
rules.
7.3.1
Formal Deﬁnitions of Checks
A check is a signed order from a principal (known as the payer) upon a bank to
draw upon the payer’s deposit of funds to pay a certain amount of money to another
principal (known as the payee). If P is the payer and Q is the payee, we can represent
a check written by P to Q as follows:
SignatureP says (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩).
Here, SignatureP is the signature of P, and hence we must also be able to asso-
ciate the given signature with the payer. That is, we must be willing to believe that
SignatureP ⇒P.
A check is endorsed when the payee signs the check—typically on the back side
of the check—issued to him or her. If P is the payer and Q is the payee, we can
represent a check written by P and endorsed by Q as follows:
SignatureQ | SignatureP says (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩).
Again, we must be willing to associate the given signatures with the relevant indi-
viduals:
SignatureP ⇒P
SignatureQ ⇒Q.

Delegation
143
Note that the form of the endorsed check is the same as a delegation-based request:
the payee (or his signature) is claiming that the payee (or her signature) wants her
account to be debited. However, the delegation relationship between the payer and
the payee exists only if the bank accepts checks as legal payment instruments.
7.3.2
Bank Policies on Checks
In our example, Alice and Bob use the same bank, and thus there is a single con-
trolling authority (i.e., the bank owner). The bank’s policies must allow the use of
checks, which means that it must recognize Alice’s right to write checks to Bob (i.e.,
Alice’s right to designate Bob as her representative for withdrawing funds from her
account). Taken together, the following pair of statements says that the bank allows
payer P to write checks to Q (i.e., the bank recognizes Q as P’s delegate on such
matters):
Bank Owner controls
(Q reps P on (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩)),
Bank Owner says
(Q reps P on (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩)).
Note that the ﬁrst statement refers to the bank owner’s jurisdiction on the matter;
the second statement indicates the actual bank policy. Using the Controls rule, it is
straightforward to deduce the following single policy statement:
Q reps P on (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩).
At ﬁrst glance, these statements may appear too strong: the bank recognizes any
possible payee Q as a representative for the payer P. However, recognizing Q as P’s
delegate is insufﬁcient for allowing Q to withdraw money from P’s account. Instead,
Q must be able to present the check with P’s signature on it, which emphasizes the
importance of being able to associate SignatureP with P herself. We return to this
point in the next subsection.
Furthermore, the bank must have policies that state the conditions under which
Bob should be paid from Alice’s account. In particular, the bank will want to verify
that P has enough funds in her account (acctP) to cover the check she has written.
This policy along with the bank’s authority to set this policy can be represented by
the following two statements:
Bank Owner controls
(⟨amt covered,acctP⟩⊃P controls (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩)),
Bank Owner says
(⟨amt covered,acctP⟩⊃P controls (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩)).
Again, the Controls rule allows us to derive the bank’s policy that permits cashing
the check, provided that P has sufﬁcient funds in her account:
⟨amt covered,acctP⟩⊃P controls (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩).

144
Access Control, Security, and Trust: A Logical Approach
7.3.3
Operating Rules for Checks
The bank has speciﬁc procedures—that is, a concept of operations—for handling
checks, designed to minimize processing costs (both time and money) and the likeli-
hood of fraud. In particular, the bank allows a payment to be made (and the payment
amount debited from the speciﬁed account) only if the following conditions are met:
1. A check is received with a payer’s signature SignatureP and endorsed with a
payee’s signature SignatureQ:
SignatureQ | SignatureP says (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩).
2. The signatures on the check correspond to the payer and payee:
SignatureP ⇒P,
SignatureQ ⇒Q.
3. There are enough funds in the account to cover the amount of the check:
⟨amt covered,acctP⟩.
4. The policy allows that, if there are sufﬁcient funds in the account, then P can
authorize payment to Q and debit the payment from her account:
⟨amt covered,acctP⟩⊃P controls (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩).
5. The policy allows Q to be P’s delegate:
Q reps P on (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩).
This concept of operations can be captured by the following derivable (and there-
fore sound) inference rule:
Simple
Checking
SignatureQ | SignatureP says (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩)
SignatureQ ⇒Q
SignatureP ⇒P
⟨amt covered,acctP⟩
⟨amt covered,acctP⟩⊃P controls (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩)
Q reps P on (⟨pay amt,Q⟩∧⟨debit amt,acctP⟩)
⟨pay amt,Q⟩∧⟨debit amt,acctP⟩
.
The proof of this rule is left as an exercise for the reader.
This extended example illustrates the point we made at the beginning of this sec-
tion: an understanding of delegation can illuminate how existing systems work and
the assumptions or assurances upon which they depend. In the simple-checking sys-
tem, the delegates are the check payees. The analysis depends upon the delegation
instruments—that is, the checks that must be integrity checked—and the statements
that specify which authorities have jurisdiction over which statements. For a sys-
tems designer, each of these items—such as the integrity checking or jurisdiction
statements—represents an assumption of fact. It is essential that we always ask our-
selves, “Are these assumptions reasonable?” These assumptions reﬂect the potential
risks to the system. If we deem them unreasonable, then our burden is to identify
assumptions that we consider reasonable and redo the analysis.

Delegation
145
Exercise 7.3.1
Give a formal proof of the Simple Checking Rule.
Exercise 7.3.2
Consider the concept of operations of simple checking as presented
in this section. What are the risks in simple checking as presented?
Exercise 7.3.3
This question concerns a university library and their policies that
allow students and faculty to check out books.
Let us adopt the notation ⟨checkout book,P⟩to denote a request to check out book
on person P’s account. Thus, for example,
⟨check out Animal Farm,George⟩
represents a request to check out the book Animal Farm on George’s account. An-
swer the following questions:
a. Toby decides to check out the book How Not to Write, using his own library
account. What is the speciﬁc request—expressed as a statement in the access-
control logic—that Toby makes when he brings the book to the check-out
counter? (Assume that the librarian recognizes Toby.)
b. Toby’s account is in good standing, and thus he is authorized to check out this
book. Express this authorization as a statement in the logic.
c. Sidney is a faculty member who wishes to have Toby (her graduate assistant)
check out books on her behalf. The library requires her to sign a proxy form
that states her willingness to be held responsible for all books that Toby checks
out on her behalf. This form, which amounts to a delegation certiﬁcate, is kept
on ﬁle at the library.
Express both this proxy form and Sidney’s authority to sign it as statements in
the logic. (Note: In theory, this proxy form applies to all books in the library.
For this exercise, it sufﬁces to consider the speciﬁc book Proving Refutations.)
d. Toby heads to the library to check out Proving Refutations on Sidney’s behalf.
What is the speciﬁc request—expressed as a statement in the access-control
logic—that Toby makes when he brings the book to the check-out counter?
(Assume again that the librarian recognizes Toby.)
e. Sidney’s account is in good standing. What is the library’s policy (expressed
as a statement in the logic) that governs Toby’s request to check out Proving
Refutations for Sidney?
Exercise 7.3.4
In recent years, Utopia College has spent a lot of money and effort
in developing its computer infrastructure to support administrative operations in an
integrative fashion across the college. Speciﬁcally, they have adopted a public-key
infrastructure, and provided all students and staff with personal key pairs. All public
keys in the college are certiﬁed by the IT department (ITD), with one exception:
ITD’s public key (KI) is stored on all ofﬁcial campus servers.

146
Access Control, Security, and Trust: A Logical Approach
The annual housing lottery for students occurs online, and is scheduled for next
week. Unfortunately, Vera will be participating in a workshop at that time, and so
she wishes to have her friend Henry select a room on her behalf. There are two
separate stages to this process:
Stage One In advance of the lottery, Vera and Henry must each ﬁll out and sign (us-
ing their private keys) an electronic form requesting that Henry be authorized
as Vera’s room-selection proxy.
The Ofﬁce of Residential Life (ORL) approves only those requests that are
signed by both individuals (i.e., the delegator and the delegate). If the request
is approved, then ORL sends to Henry a proxy certiﬁcate, signed with ORL’s
private key, stating that Henry is authorized to make room selections on Vera’s
behalf.
Stage Two When the lottery begins and it is Vera’s turn to select a room, Henry
sends the following two items to the Lottery server:
• The proxy certiﬁcate obtained from ORL in Stage 1
• His selection (on Vera’s behalf), signed with his private key
Let KV, KH, and KO be the public keys of Vera, Henry, and ORL, respectively. Let
⟨proxy,Henry,Vera⟩denote the request “please assign Henry as Vera’s proxy,” and
let ⟨select,422⟩denote the request “Select Room 422.” Thus, for example, the two
electronic proxy-request forms sent in Stage 1 can be interpreted as follows:
KV says ⟨proxy,Henry,Vera⟩,
KH says ⟨proxy,Henry,Vera⟩.
Answer the following questions regarding the certiﬁcations, credentials, and access-
control policies needed for the housing lottery. All answers should be given as ex-
pressions (or formal proofs, where requested) in the access-control logic.
a. ORL will grant the Stage 1 request only if ⟨proxy,Henry,Vera⟩can be deduced.
What recognition of authority/jurisdiction is implicit in ORL’s requirement
that both proxy participants sign the form?
b. What additional certiﬁcates, recognition of authority, and trust assumptions
regarding keys are necessary for the ORL to determine that the proxy request
should be granted in Stage 1? Provide a formal justiﬁcation for granting the
request.
c. Assuming that ORL grants the proxy request, what is the speciﬁc proxy certiﬁ-
cate sent to Henry at the end of Stage 1?
d. In Stage 2, Henry presents a room-selection request to the Housing Lottery
server. What is the speciﬁc form of this room-selection request?

Delegation
147
e. What additional certiﬁcates, recognition of authority, and trust assumptions
regarding keys are necessary for the Housing Lottery system to determine that
the room-selection request should be granted? Provide a formal justiﬁcation
for granting the request.
7.4
Summary
Delegation is an essential aspect of distributed systems, but it also complicates the
task of making access-control decisions. Under what circumstances should we act
upon a statement made by a purported delegate?
In this chapter, we examined that question and demonstrated how to describe and
reason about delegation in our access-control logic. We identiﬁed the role of juris-
diction in such decisions, as well as two important properties of delegation: nonre-
duction of responsibilities and nontransitivity of delegation.
The learning outcomes associated with this chapter appear in Figure 7.4.
7.5
Further Reading
The notion of delegation introduced in this chapter is statement-based: the scope
of a delegate’s responsibility is explicitly limited by detailing the statements that
the delegate is authorized to make. In contrast, the work of Abadi, Lampson, and
colleagues (Lampson et al., 1992; Abadi et al., 1993) uses a principal-based notion
of delegation that relies on a virtual delegation server. In this setting, the scope
of a delegate’s responsibility is limited only by the access policy enforced by the
reference monitor.

148
Access Control, Security, and Trust: A Logical Approach
FIGURE 7.4 Learning outcomes for Chapter 7
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
• Describe the important properties of delegation relationships.
Application
• When given a particular delegation relationship, you should be able to
use the access-control logic to describe that relationship.
Analysis
• When given a scenario involving delegation, you should be able to per-
form a formal analysis of it using the access-control logic.
Synthesis
• When given a scenario involving delegation, you should be able to ex-
press in the logic the primary principals (i.e., the originators of state-
ments), their proxies, and the statements over which the proxies have
jurisdiction.
Evaluation
• Given a formal analysis of a scenario involving delegation, you should
be able to assess the possible risks associated with the analysis.

Chapter 8
Networks: Case Studies
The concepts of digital authentication and delegation—developed in the previous
two chapters—are essential for analyzing networks, both public (such as the world
wide web) and private (such as those used by ﬁnancial institutions). With networks,
the tasks of authenticating principals and authorizing access requests are made more
difﬁcult by the lack of locality among the policy makers (i.e., authorities), the access
controllers (i.e., reference monitors), and the originators of requests.
Network operations are usually described in terms of protocols, which are sets of
syntactic and semantic rules for exchanging information. These protocols are typi-
cally implemented by a series of requests and responses that produce a desired result,
such as establishing a secure communications channel by exchanging cryptographic
keys or transferring money from one person or account to another. In this chapter, we
demonstrate how to describe such protocols as collections of derived inference rules
using our access-control logic. The inference rules correspond to the operating rules
used by the decision makers within the various protocols. Capturing the underlying
logic of a protocol in this way makes explicit the context in which the protocol oper-
ates, revealing the inherent trust assumptions, jurisdiction of authority, certiﬁcations,
and delegations upon which it depends.
We apply this approach to develop three different network case studies:
1. SSL and TLS, which support authentication across the web;
2. Kerberos, a widely used authentication and delegation protocol for distributed
systems; and
3. ACH, a backbone of ﬁnancial networks.
Taken together, these case studies illustrate how a formalization of a protocol can
help to explicate its concept of operations (CONOPS).
8.1
SSL and TLS: Authentication across the Web
For web-based transactions, the primary security concerns are authentication, in-
tegrity, and privacy. These concerns are addressed by protocols such as SSL (Secure
Sockets Layer) (Freier et al., 1996) and TLS (Transport Layer Security) (Dierks and
Rescorla, 2006).
149

150
Access Control, Security, and Trust: A Logical Approach
Both SSL and TLS rely on two protocols: a handshake protocol and a record
protocol. The handshake protocol, which we examine in Subsection 8.1.1, is used to
establish a communications channel between a client and a server. This channel may
or may not require the client and the server to authenticate each other. The purpose
of the handshake protocol is to establish a cryptographic key to secure the session.
The record protocol—which we describe in Subsection 8.1.2—is used to frag-
ment, compress, provide for integrity checking, and encrypt application data trans-
mitted on the web. The record protocol relies on the cryptographic keys and secrets
shared between the client and server during the handshake protocol. The record pro-
tocol relies on key certiﬁcates for authentication, nonces (i.e., values created for a
particular run of the protocol to guarantee freshness and to prevent replay attacks),
and cryptographic hash functions for generating keys and secrets.
Although the SSL and TLS protocols are very similar, they differ in several areas,
including their available cipher suites and how they compute message authentication
codes and shared secrets. At the level of detail we are considering here, however,
these differences are not signiﬁcant. As a result, we will not distinguish between
SSL and TLS in the discussion that follows.
8.1.1
Handshake Protocol
The handshake protocol used in SSL and TLS is the starting point for establishing
secure communications. The handshake protocol does the following:
• Establishes agreement between the client and the server on cryptographic al-
gorithms.
• Exchanges certiﬁcates between the client and the server allowing each to au-
thenticate the other.
• Exchanges cryptographic parameters that enable the client and the server to
create the same master secret.
Figure 8.1 contains a diagram of the handshake protocol, which is read from the
top down. Messages and information—represented by arrows—are exchanged be-
tween the client and server. The direction of each arrow indicates the ﬂow of the
message; the arrows are annotated by the information sent or the type of message.
The solid arrows represent messages that must appear as part of the SSL or TLS
protocols. The dotted arrows represent messages that are optional, depending upon
whether or not the client and/or server are to be authenticated.
8.1.1.1
Client and Server Hello Messages
To initiate a session, the client sends a client hello message to the server unen-
crypted and in the clear. This message, which initiates a negotiation between the
client and server, contains the client’s preferences: the version of the TLS/SSL proto-
col the client wishes to use (ideally, the most recent version that the client supports),
a random number randomC generated by the client, a list of the cipher suites that the

Networks: Case Studies
151
FIGURE 8.1 Handshake protocol: Establishing associations among principals and
keys
client supports, and a list of compression methods (if any) that the client supports.
Each cipher suite contains three components related to how the client and server will
exchange information:
1. The key-exchange method, which deﬁnes how the client and server will agree
upon the shared secret key for the session (e.g., using Difﬁe-Hellman or Rivest-
Shamir-Adelman (RSA)) and what sort of digital signatures to use (if any)
during the key exchange
2. The symmetric crypto-algorithm to use for data transfer, such as stream ci-
phers, cipher-block-chaining ciphers, or no encryption at all
3. The message-digest function (mdf) to use —such as MD5 (Message Digest 5)
or SHA1 (Secure Hash Algorithm)—to create message authentication codes
(MACs) for integrity checking
The server responds by sending to the client a server hello message, also unen-
crypted in the clear. The server hello message contains the result of the negotiation:
the version of TLS/SSL to be used, which is the most recent version that both the
client and server can accommodate; a random number randomS generated by the

152
Access Control, Security, and Trust: A Logical Approach
server; the session identiﬁer corresponding to this connection; the cipher suite se-
lected by the server, which must be one of those initially supplied by the client; and
the compression method (if any) selected from the list supplied by the client.
None of the parameters exchanged in the client or server hello messages (e.g.,
protocol version number, random numbers, session ID, cipher suite, compression
method) has any interpretation within the calculus. Rather, these items establish the
computational infrastructure for messages, privacy, and integrity checking.
8.1.1.2
Server Authentication
After the client hello and server hello messages are exchanged, both the client
and server will use the same cryptographic algorithms and compression method
for the connection indexed by the session identiﬁer. At this point, the client and
server can authenticate themselves, if required, using certiﬁcates for RSA or Difﬁe-
Hellman public keys. In particular, the server must authenticate itself whenever the
key-exchange method agreed upon requires digital signatures.
If the server must authenticate itself, then it sends a server certiﬁcate message as
indicated in Figure 8.1. This message contains a list of certiﬁcates, with the server’s
RSA or Difﬁe-Hellman key certiﬁcate at the head; the remaining certiﬁcates (if any)
correspond to a chain of certiﬁcate authorities, each certifying the certiﬁcate directly
preceding it. Thus, for example, the second certiﬁcate in the list belongs to the
server’s certiﬁcate authority, while the third certiﬁcate belongs to the authority that
certiﬁed the server’s certiﬁcate authority. As we have seen before, the server’s cer-
tiﬁcate is a statement digitally signed by the server’s certiﬁcate authority CAS, whose
public key is KCAS:
KCAS says (KS ⇒Server).
If the client directly recognizes both the public key and the jurisdiction of CAS re-
garding public-key certiﬁcates, then the client in effect accepts the following two
statements:
KCAS ⇒CAS,
CAS controls (KS ⇒Server).
In such a case, the client can conclude that KS is truly the server’s key:
KS ⇒Server.
This analysis corresponds to the Certiﬁcate Veriﬁcation rule introduced in Chapter 6:
Certiﬁcate
Veriﬁcation
Kca ⇒Certiﬁcate Authority
Kca says (KP ⇒P)
Certiﬁcate Authority controls (KP ⇒P)
KP ⇒P
.
In those cases where the client does not recognize the authority of CAS, the ad-
ditional certiﬁcates in the certiﬁcate chain may provide the necessary support, as
previously described in Example 6.2.
In some cases, the server’s key KS might be certiﬁed only for signing but not for
encryption. In such situations, the server must also send a server key exchange mes-
sage, which conveys either a signed RSA public key that will be used to encrypt the

Networks: Case Studies
153
pre-master secret or signed Difﬁe-Hellman parameters that can be used to compute
the pre-master secret. In effect, the server creates a temporary public key Kt to be
used by the client, and the server key exchange message corresponds to the following
statement:
KS says ⟨use,Kt⟩.
Because the server signs the RSA public key or the Difﬁe-Hellman parameters, the
client has everything it needs to conclude that it in fact is communicating with the
server. It can now associate with the server any messages sent using the public key
KS or temporary public key Kt. The remainder of the handshake protocol deals with
the (optional) authentication of the client.
8.1.1.3
Client Authentication
When the cipher suite dictates, the server sends the client a certiﬁcate request
message, which contains a list of the types of certiﬁcates requested (in descending
order of preference) and a list of the distinguished names of acceptable certiﬁcate
authorities. If no names are speciﬁed, then the client may send any certiﬁcate that
matches the types of certiﬁcates speciﬁed by the server.
At this point, the server sends a mandatory server hello done message to indicate
that it is done sending messages to support the key exchange and the client can
proceed with its part to support the key exchange.
If the server requested a client certiﬁcate, the client sends a client certiﬁcate mes-
sage with a suitable certiﬁcate, signed by the client’s certiﬁcate authority CAC:
KCAC says (KC ⇒client).
If the server recognizes both the public key and jurisdiction of CAC, then the server
must accept the following two statements:
KCAC ⇒CAC,
CAC controls (KC ⇒client).
It is straightforward for the server to then conclude that
KC ⇒client.
On the other hand, if the server does not request a client certiﬁcate, then the server
will possess no cryptographic key to authenticate the identity of the client. The client
in this case is anonymous.
8.1.1.4
Key Exchange
The next message is the mandatory client key exchange message. The client gen-
erates a random pre-master secret (typically 48 bits long), encrypts it with the public
key (either from the server’s certiﬁcate or the temporary key supplied in the key
exchange message), and sends it on to the server.
At this point, both the client and the server can compute the master secret and the
necessary keys for data encryption and message authentication. The master secret is

154
Access Control, Security, and Trust: A Logical Approach
computed by applying the message-digest function mdf speciﬁed in the server hello
message to the pre-master secret, randomC, and randomS:
master secret = mdf(pre-master secret,randomC,randomS).
The master secret is used to create the key block, which contains all the cryptographic
keys necessary for encryption and for computing message authentication codes. The
key block is also computed using the message-digest function:
key block = mdf(master secret,randomC,randomS).
The key block contains all of the various keys required by the cipher speciﬁcation;
each key can be extracted from a speciﬁed portion of the key block. Speciﬁcally,
(client write MAC secret, server write MAC secret, client write key, server write key)
=
key block.
If the server requested a client certiﬁcate, then the client must also send a certiﬁ-
cate verify message, which contains the digitally signed hash of all previous hand-
shake messages sent or received. This message provides assurance to the server that
it in fact has been communicating with the principal authenticated by the client’s
certiﬁcate.
At this point, both the client and the server possess the shared secret key K used
to compute MACs and the write key KW used to encrypt fragments if desired. The
client associates the shared secret MAC key with the server:
K ⇒Server.
Likewise, the server associates the shared secret MAC key with the client, who may
or may not be authenticated:
K ⇒
(
anonymoussession ID,
if the client is not authenticated
Authenticated Client,
if the client is authenticated.
Note that, in the case where clients are not authenticated (which is the case for many
business to consumer connections), the server can still identify which anonymous
client it is dealing with by the session identiﬁer session ID.
At this point, the client sends a change cipher spec message to indicate that sub-
sequent records will be sent using the newly negotiated cipher-suite parameters and
keys. The client then also sends a ﬁnished message.
The handshake protocol ends when the server also sends a change cipher spec
message followed by a ﬁnished message.

Networks: Case Studies
155
FIGURE 8.2 Record protocol: Payload construction
8.1.2
Record Protocol
After the completion of the Handshake Protocol, the client and server can ex-
change data relevant to the transaction (e.g., credit-card numbers or mailing ad-
dresses) using the Record Protocol. Figure 8.2 contains a ﬂowchart of the record
protocol used by SSL and TLS. Application data are divided into fragments of 214
bytes or less, and then each fragment is compressed according to the agreed upon
compression method. The message authentication code (MAC) for a compressed
message fragment M is obtained by applying the agreed-upon message-digest func-
tion mdf as follows:
MAC = mdf(K,M,iopad,seq num),
where K is the shared secret key, iopad is a sequence of padding bits, and seq num
is the sequence number corresponding to the particular fragment being sent.
Figure 8.3 contains a diagram showing how payloads are integrity checked. To see
how it works, suppose that Alice sends Bob a message using the record protocol, and
Bob wants to check the integrity of a message fragment he receives. He ﬁrst decrypts
the message using the write key KW obtained through the handshake protocol. The

156
Access Control, Security, and Trust: A Logical Approach
FIGURE 8.3 Record protocol: Payload integrity checking
result has two components: the compressed fragment and the attached MAC. Bob
uses the shared secret key KW and the message-digest function mdf to compute a
new message authentication code, which he compares with the attached MAC. If the
two message authentication codes match, then Bob concludes that (because mdf is a
one-way function) the sender of the message must have possessed the shared secret
key K along with the correct sequence number. That is, Bob concludes that
K says M.
Due to the handshake protocol, Bob associates the shared key K with Alice, and
hence he concludes that the message originated with Alice:
Alice says M.
Exercise 8.1.1
Suppose that the server’s key KS is certiﬁed only for signing, and
hence the server sends a key exchange message as explained in the text to convey a
signed public key Kt. Formalize and prove the derived rule that captures the analysis
that allows the client to associate Kt with the server.

Networks: Case Studies
157
FIGURE 8.4 Overview of Kerberos protocol
8.2
Kerberos: Authentication for Distributed Systems
Kerberos (Neuman et al., 2005) is an industry-standard protocol that supports both
authentication and delegation on open networks. At the core of Kerberos lie two spe-
cial services—the authentication server (AS) and the ticket-granting server (TGS).
Conceptually, these two services are distinct, but they are typically implemented on
the same machine and collectively referred to as the Key Distribution Center (KDC).
The KDC has access to an authentication database that stores the individual Ker-
beros encryption keys for all users and services in the system. For this reason, the
KDC should be in a physically secure location, as compromise of the authentication
database would compromise the security of the entire system. Both services provide
tickets to access other services: the AS provides ticket-granting tickets (TGTs) for
accessing the TGS, while the TGS offers service-speciﬁc tickets for other services
(such as a ﬁle system or print server).
For example, consider a user who, throughout the day at work, needs to access the
(non-local) ﬁle system to obtain various ﬁles and also needs to print those ﬁles on a
network printer. Both the ﬁle system and the printer need to authenticate the user to
determine whether or not to provide access to the requested service. For reasons of
both security and practicality, it is undesirable to require the user to repeatedly type
in her password or rescan her smartcard each time she needs to access one of the
services. Kerberos provides for single signon, whereby the user authenticates herself
once and then receives an unforgeable token—called a ticket-grating ticket—that she
can subsequently pass along to services to authenticate herself for a speciﬁed period
of time (e.g., eight hours).
This example provides a backdrop to our explanation of the Kerberos protocol. We
begin with simplest sort of case (i.e., not involving delegation), and then discuss how
the protocol also supports proxies. Figure 8.4 illustrates the sequence of messages
that comprise the Kerberos protocol. We examine these messages in turn.
8.2.1
Initial Authentication Requests
To understand how Kerberos works, let us consider what happens when Ursula
arrives at work in the morning. Having fetched her coffee, Ursula sits down at her
computer and types her userid (ursa) and password to log into the workstation. The

158
Access Control, Security, and Trust: A Logical Approach
Kerberos client on her workstation uses a one-way function to calculate her secret
(symmetric) encryption key Ku, which is the same key that the KDC associates with
ursa in its authentication database. The client also saves this encryption key in the
workstation’s credentials cache. (The credentials cache should be emptied when the
user logs out of the workstation, to prevent unauthorized use or replay of this key or
other credentials.)
At this point, the Kerberos client sends in the clear an authentication request to
the AS, which contains the following information: the user’s name (ursa), the name
of the service for which a ticket is being requested (i.e., the ticket-granting service
TGS), and a random nonce nu, which can be used to match requests with responses
and also to detect possible replay attacks. This request may also contain additional
information for the requested ticket-granting ticket (TGT), such as whether it should
be proxiable or forwardable; for now, we ignore those possibilities, but we shall
return to them later. This request corresponds to arrow 1 of Figure 8.4.
When the AS receives this request, it looks up the name ursa in its authentica-
tion database to verify that ursa is a registered user and to obtain Ursula’s secret
Kerberos key Ku. The AS also generates a session key Ku,tgs to be used between the
workstation’s Kerberos client and the TGS. The AS then sends two encrypted items
back to the Kerberos client, as represented by arrow 2 in Figure 8.4. The ﬁrst uses Ur-
sula’s secret key Ku to encrypt both the newly generated session key and the client’s
original nonce (recall that the notation ⟨⟨m1,m2,...,mk⟩⟩denotes the concatenation
of the k items m1 through mk):
encrypt(Ku,⟨⟨Ku,tgs,nu⟩⟩).
The second item is the ticket-granting ticket (TGT), which is encrypted with the
secret Kerberos key that the authentication database associates with the TGS (Ktgs):
encrypt(Ktgs,⟨⟨Ku,tgs,ursa,addu,ticket-validity period⟩⟩).
The TGT contains the newly generated session key, the userid ursa, the network
addresses that are allowed to use this ticket (in our case, only Ursula’s workstation
address addu), and the period of time for which the TGT is valid (a system-speciﬁc
value, which may be minutes or hours). Notice that, because the original request
was sent in the clear, the AS has no way to verify that the request really arrived from
a client representing ursa and not from an impostor. Rather, the AS constructs
a response that should be useful only to a client working on her behalf (i.e., with
knowledge of the secret key Ku), as described next.
Upon receiving these two items from the AS, Ursula’s Kerberos client uses the
shared secret key Ku to recover the original nonce (which associates the AS response
with the initial client response) and the session key to use with the TGS:
Ku says ⟨use session key,Ku,tgs⟩.
The client associates the shared secret key Ku with the AS and also trusts the AS with

Networks: Case Studies
159
respect to the assignment of session keys to use with the TGS:
Ku ⇒AS,
AS controls ⟨use session key,Ku,tgs⟩.
The client stores both the session key and the TGT for future use in requesting spe-
ciﬁc network services, as described in the next subsection. The client’s decision to
accept and store the session key corresponds to an instance of an application of the
following derived rule:
Session Key
Receipt
Authority controls ⟨use session key,Ksession⟩
K says ⟨use session key,Ksession⟩
K ⇒Authority
⟨use session key,Ksession⟩
.
In fact, this derived rule could be generalized signiﬁcantly by replacing the action
⟨use session key,Ksession⟩with a generic statement ϕ, and in fact we have indirectly
used such a generic rule multiple times throughout this book. However, stating the
explicit rule in this context-speciﬁc way allows us to explicitly capture the operating
behavior of the Kerberos client.
8.2.2
Requests for Service-Speciﬁc Tickets
A little later that morning, Ursula wants to make some changes to the ﬁle book.tex.
When she selects the ﬁle to open, her Kerberos client must obtain a ticket for the ap-
propriate ﬁle server. To do so, the Kerberos client sends to the TGS a request for a
ﬁle-server ticket. This request contains the following components, many of which
we have seen before: the user name ursa, the name of the requested service (in this
case, the ﬁle server FS), a (new) random nonce nw, the TGT previously received and
stored, and an authenticator for the TGT. The authenticator is an encrypted package
designed to demonstrate to the TGS knowledge of the secret session key Ku,tgs:
encrypt(Ku,tgs,⟨⟨ursa,addu,timestamp⟩⟩).
The idea here is that only principals knowing the session key Ku,tgs could create
the authenticator. The timestamp indicates freshness and provides protection against
replay attacks, while the user name and address must match those that appear in the
TGT.
Upon receiving this request, the TGS ﬁrst veriﬁes that it is intact and well-formed.
The TGS must then use the TGT and authenticator to determine whether or not to
grant the request.
Recall that the TGT has been created as follows:
encrypt(Ktgs,⟨⟨Ku,tgs,ursa,addu,ticket-validity period⟩⟩).
The TGS uses its secret key Ktgs to decrypt the TGT, from which it then extracts
the session key Ku,tgs. In doing so, the TGS also veriﬁes that the ticket is within its

160
Access Control, Security, and Trust: A Logical Approach
validity period and that the network address in the ticket (addu) matches the network
address of the request itself. If the TGS deems the ticket to be valid, then it has in
effect interpreted the TGT as follows:
Ktgs says (Ku,tgs controls ⟨get ticket, FS, ursa@addu⟩)
That is, the TGT indicates which session key should be trusted with regards to re-
quests for tickets for user ursa at the network address addu. Furthermore, the TGS
must associate the secret key Ktgs with AS and trust in AS’s jurisdiction regarding
ticket requests:
Ktgs ⇒AS,
AS controls (Ku,tgs controls ⟨get ticket, FS, ursa@addu⟩).
Having veriﬁed the validity of the TGT, the TGS then uses the extracted session
key Ku,tgs to decrypt the authenticator. For integrity checking, the TGS must verify
that the authenticator timestamp is sufﬁciently recent (typically, within 5 minutes),
that the user name is the same in both the ticket and authenticator, and that the net-
work address given in the authenticator is the same as that given in the TGT. If
everything checks out, then the TGS effectively interprets the authenticator as the
following statement:
Ku,tgs says ⟨get ticket, FS, ursa@addu⟩.
That is, the session key is certifying a request to create a ticket for ursa for the ﬁle
server.
At this point, it should be straightforward to see why the TGS determines it ap-
propriate to grant the ticket request. In fact, the TGS decision to grant the request
corresponds to an instance of the following derivable operating rule:
Grant Service
Ticket
K says (Ksession controls ⟨get ticket, service, P@add⟩)
K ⇒AS
AS controls (Ksession controls ⟨get ticket, service, P@add⟩)
Ksession says ⟨get ticket, service, P@add ⟩
⟨get ticket, service, P@add⟩
.
It is important to note that the TGS never directly associates the session key with
the apparent requesting user, because its decision is not identity-based. Instead, the
analysis depends on trust in the secrecy of the keys K (Ktgs) and Ksession (Ku,tgs). At
this stage of the protocol, the user’s name and network address exist only so that the
correct service ticket can be created.
To grant the request, TGS creates a random session key Ku,fs to be used between
Ursula’s Kerberos client and FS. The TGS sends the following two items back to the
Kerberos client, in much the same way that the AS responded to the initial request
for authentication:
encrypt(Ku,tgs,⟨⟨Ku,fs,nw⟩⟩),
encrypt(Kfs,⟨⟨Ku,fs,ursa,addu,ticket-validity period⟩⟩).

Networks: Case Studies
161
As before, the Kerberos client can decrypt the ﬁrst item to obtain the session key
and to associate this response with the appropriate request; this decision again cor-
responds to an application of the Session Key Receipt rule. The second item is the
ﬁle-service ticket, where Kfs is the secret key that the KDC’s authentication database
associates with FS.
8.2.3
Requests for Services
The Kerberos client now sends to the ﬁle server FS a request with the following
components: the user name ursa, the ﬁle-service ticket previously received from the
TGS, an authenticator encrypt(Ku,fs,⟨⟨ursa,addu,timestamp⟩⟩) for the ﬁle-service
ticket, and any necessary application-speciﬁc information (in this case, the actual
request to read the ﬁle book.tex). This request corresponds to arrow 5 in Figure 8.4.
The FS proceeds in much the same fashion as the TGS did earlier. Speciﬁcally,
the FS uses its secret key Kfs to decrypt the ticket and extract the session key Ku,fs.
However, the interpretation of this ticket is different, because—unlike the TGS—the
ﬁle server must make an identity-based access decision. The ﬁle-service ticket must
therefore provide an association between the secret session key Ku,fs and the user
ursa:
Kfs says (Ku,fs ⇒ursa).
The ﬁle server must also associate the secret key Kfs with the TGS and trust in the
TGS’s jurisdiction:
Kfs ⇒TGS,
TGS controls (Ku,fs ⇒ursa).
As before, the FS uses the session key to decrypt the authenticator and performs
the basic integrity check: the authenticator timestamp must be sufﬁciently recent,
the user name must be the same in both the ticket and authenticator, and the network
address given in the authenticator is the same as that in the ticket. If the authenticator
checks out, then FS interprets the authenticator in the following way:
Ku,fs says ⟨read,book.tex⟩.
It is straightforward for the FS to conclude that
ursa says ⟨read,book.tex⟩.
Such a conclusion corresponds to an instance of the following derivable operating
rule:
Service
Request
TGS controls (Ksession ⇒P)
K says (Ksession ⇒P)
K ⇒TGS
Ksession says servicerequest
P says servicerequest
.

162
Access Control, Security, and Trust: A Logical Approach
At this point, the ﬁle server can perform its usual process—such as checking the
appropriate access-control list—to determine whether or not to grant the request.
8.2.4
Proxiable Tickets
Having described the basic Kerberos interactions, we return to an issue mentioned
brieﬂy earlier: Kerberos also supports proxiable and forwardable tickets. We de-
scribe the use of proxiable tickets in this section, and leave discussion of forwardable
tickets to Exercise 8.2.5.
In many situations, a network service must make a request of another service on
behalf of its client. For example, suppose our user Ursula decides that she wants to
print out the ﬁle story.pdf. In such a scenario, the print service will need to contact
the ﬁle server on Ursula’s behalf to obtain the ﬁle. Kerberos has a mechanism for the
creation and use of proxiable tickets, which can be used for proxy requests.
The initial authentication (i.e., Steps 1 and 2) for Ursula is very similar to that
described in Subsection 8.2.1. However, the ﬁrst step must also include a request
for a proxiable TGT. Likewise, the TGT that the AS transmits back must have the
proxiable ﬂag set, so that the TGT has the following form:
encrypt(Ktgs,⟨⟨Ku,tgs,ursa,addu,proxiable,ticket-validity period⟩⟩).
When Ursula decides to print her ﬁle, her Kerberos client requests (and obtains) a
print-service ticket and session key, exactly as described in Subsection 8.2.2. How-
ever, the client must also request a proxy ticket that allows the print service to make
requests on Ursula’s behalf from the ﬁle service.. Therefore, the Kerberos client
sends to the TGS a request containing the following components, most of which
we have seen before: the user name ursa, the name of the requested service (in
this case, the ﬁle server FS), a speciﬁc request for a proxy ticket, the network ad-
dresses of the allowable proxies (in this case, the address addps of the print server),
a random nonce nv, the proxiable TGT previously received and stored, and an au-
thenticator encrypt(Ku,tgs,⟨⟨ursa,addu,timestamp⟩⟩) for the TGT. The request may
also include service-speciﬁc information: in this scenario, the client might include
the speciﬁc request(s) that the ﬁle service should honor (e.g., “read story.pdf”).
Upon receiving this request, TGS uses its secret key Ktgs to decrypt the TGT, from
which it then extracts the session key Ku,tgs. In doing so, TGS also veriﬁes that the
ticket is within its validity period and that the network address in the ticket (addu)
matches the network address of the request itself. If the TGS deems the ticket to be
valid, then it has in effect interpreted the TGT as follows:
Ktgs says (Ku,tgs controls
(ursa@addu controls
⟨get proxy ticket, FS,ursa@addu,addps,“read story.pdf”⟩)).
That is, the TGT indicates which session key should be trusted with regards to re-
quests for tickets for user ursa at network address addu.

Networks: Case Studies
163
TGS must also associate the secret key Ktgs with AS and trust in AS’s jurisdiction
regarding ticket requests:
Ktgs ⇒AS
AS controls (Ku,tgs controls
(ursa@addu controls
⟨get proxy ticket, FS,ursa@addu,addps,“read story.pdf”⟩)).
The TGS then uses the extracted session key Ku,tgs to decrypt the authenticator.
For integrity checking, the TGS must verify that the authenticator timestamp is suf-
ﬁciently recent (typically, within 5 minutes), that the user name is the same in both
the ticket and authenticator, and that the network address given in the authenticator is
the same as that given in the TGT. If everything checks out, then the TGS effectively
interprets the authenticator as the following statement:
Ku,tgs says (ursa@addu controls
⟨get proxy ticket, FS,ursa@addu,addps,“read story.pdf”⟩)).
That is, the session key is certifying that user ursa is authorized to receive the
requested ticket.
This analysis is identical to that performed for a regular (i.e., nonproxy) ticket,
and it is straightforward to see why TGS determines it appropriate to grant the ticket
request (the Grant Service Ticket rule still applies). To grant the request, as before,
TGS creates a random session key Ku,fs to be used with FS. The TGS sends the
following two items back to W:
encrypt(Ku,tgs,⟨⟨Ku,fs,nw⟩⟩),
encrypt(Kfs,⟨⟨Ku,fs,ursa,addu,addps,proxy,tkt-validity pd,“read story.pdf”⟩⟩).
As before, the Kerberos client can decrypt the ﬁrst portion to obtain the session key
and to associate this response with the appropriate request. The second portion is the
ﬁle-service proxy ticket—in particular, the ticket contains all addresses that may be
used with this ticket, namely ursa’s workstation and the print server’s address.
The Kerberos client sends the previously obtained print-service ticket and authen-
ticator to PS as described in the previous subsection; the ticket includes a session
key Ku,ps. The Kerberos client also forwards the newly obtained proxy ticket, along
with the session key Ku,fs (encrypted with the key Ku,ps) to the print service PS. The
printer service PS is then able to make its proxy request to the ﬁle server FS, using
this new session key. In particular, the printing service sends a request with the fol-
lowing components: the user’s name (ursa), the ﬁle-service ticket that the TGS sent
in step 4, an authenticator encrypt(Ku,fs,⟨⟨ursa,addps,timestamp⟩⟩) for the ticket,
and necessary application-speciﬁc information (in this case, the actual request to read
the ﬁle story.pdf).
The FS proceeds in much the same fashion as before: it uses its secret key Kfs to
decrypt the ticket and extract the session key Ku,fs. Because this ticket is a proxy

164
Access Control, Security, and Trust: A Logical Approach
ticket, however, its interpretation is slightly different:
Kfs says ((Ku,fs ⇒addps | ursa) ∧(addps reps ursa on ⟨read,story.pdf⟩)).
That is, the ﬁle-service ticket associates the secret session key Ku,fs with the principal
addps | ursa; it also speciﬁes that addps is a valid proxy for ursa. To make use
of this credential, FS must also associate the secret key Kfs with TGS and trust in
TGS’s jurisdiction:
Kfs ⇒TGS
TGS controls Ku,fs ⇒addps | ursa
TGS controls (addps reps ursa on ⟨read,story.pdf⟩).
As before, FS uses the session key to decrypt the authenticator and performs the
basic integrity checking: the authenticator timestamp must be sufﬁciently recent, the
user name must be the same in both the ticket and authenticator, and the network
address given in the authenticator must also appear in the ticket. If the authenticator
checks out, then FS interprets the authenticator in the following way:
Ku,fs says ⟨read,story.pdf⟩.
It’s straightforward for FS to conclude that
addps | ursa says ⟨read,story.pdf⟩,
and therefore that
ursa says ⟨read,story.pdf⟩.
This analysis corresponds to the following operating rule for the ﬁle service:
Proxied
Service
Request
Authority controls (Ksession ⇒Q | P)
Authority controls (Q reps P on servicerequest)
K says ((Ksession ⇒Q | P) ∧Q reps P on servicerequest)
K ⇒Authority
Ksession says servicerequest
P says servicerequest
.
Exercise 8.2.1
Give a formal proof of the Session Key Receipt rule.
Exercise 8.2.2
Give a formal proof of the Grant Service Ticket rule.
Exercise 8.2.3
Give a formal proof of the Service Request rule.
Exercise 8.2.4
Give a formal proof of the Proxied Service Request rule.

Networks: Case Studies
165
Exercise 8.2.5
A disadvantage of proxy tickets in Kerberos is that the Kerberos
client must obtain the proxy tickets and then pass them on to the proxy service. Doing
so requires the client to know the names of all of the services that the proxy will need
to fulﬁll a speciﬁc request, which is not always possible on a distributed system.
For this reason, Kerberos also supports forwardable (and forwarded) tickets, which
the client can pass on (or forward) to the services that work on its behalf. The
responsibility for obtaining subsequent tickets falls to the given service.
The Kerberos client must request a forwardable TGT when initially authenticat-
ing a user to Kerberos; the request must indicate the addresses that will be able to
act on the user’s behalf. Likewise, the TGT that the AS sends back must have the
forwardable ﬂag set. Thus, a forwardable TGT for user name ursa that allows the
addresses addu and addps to make service requests has the following form:
encrypt(Ktgs,⟨⟨Ku,tgs,ursa,addu,addps,forwardable,ticket-validity period⟩⟩)
When Ursula decides she wants to print the ﬁle story.pdf, her Kerberos client
sends the forwardable TGT to the print service, along with the session key Ku,tgs. (As
in the case of proxiable tickets, the client must also obtain and pass along a print-
service ticket and authenticator; these together provide the necessary key by which
the session key Ku,tgs can be encrypted.)
The print service PS then has the responsibility for obtaining the necessary ticket
for the ﬁle service, or any other necessary tickets. The request that the print service
sends to the TGS to obtain a ﬁle-service ticket contains the standard information
(user name, requested service, forwardable TGT, and authenticator) plus a request
for a forwarded ticket. The TGS performs an analysis very similar to the previous
cases, while also ensuring that the print service’s address is one of the addresses
included in the forwardable TGT. If everything checks out, the TGS sends both an
encrypted session key and a forwarded ﬁle-service ticket to the print service:
encrypt(Ku,tgs,⟨⟨Ku,fs,n′
w⟩⟩),
encrypt(Kfs,⟨⟨Ku,fs,U,addu,addps,forwarded,ticket-validity period⟩⟩).
The print service can now send a request to the ﬁle service, containing the follow-
ing information: the user name, the forwarded ﬁle-service ticket, an authenticator
for the ticket, and any necessary application-speciﬁc information. The ﬁle service
proceeds in much the same fashion as for other ﬁle-service tickets: it uses its secret
key Kfs to decrypt the ticket and extract the session key Ku,fs.
a. Give interpretations in the access-control logic of the forwarded ﬁle-service
ticket and its authenticator. Note that the interpretation of forwarded tickets
will differ from those for standard or proxy tickets.
b. What additional assumptions—expressed as statements in the logic—must the
ﬁle service make in order to process this request?
c. Formalize and prove a derivable rule that captures the operating rules of the
ﬁle service in this context.

166
Access Control, Security, and Trust: A Logical Approach
8.3
Financial Networks
The previous two sections explored common protocols that are used to support au-
thentication and delegation across distributed computer systems, including the web.
In this section, we focus on ﬁnancial networks, looking at how electronic clearing-
houses use electronic records and images to eliminate the passing of paper checks.
Speciﬁcally, we describe a banking network that uses electronic credits and debits
and the Automated Clearing House (ACH) network, a trusted third-party settlement
service. Detailed descriptions of retail payment systems and the ACH network can
be found in the Federal Financial Institutions Examination Council’s (FFIEC) hand-
book on Retail Payment Systems (FFIEC, 2004) and the National Automated Clear-
ing House Association’s guide to rules and regulations governing the ACH network
(National Automated Clearing House Association, 2006). This section builds on the
simple-checking example of Section 7.3.
8.3.1
Electronic Clearinghouses
The banking system uses clearinghouses (or clearing corporations) to collect and
settle individual transactions while minimizing the number of actual payments made
between banks. The FFIEC deﬁnes a clearing corporation as follows (FFIEC, 2004,
page B-4):
A central processing mechanism whereby members agree to net,
clear, and settle transactions involving ﬁnancial instruments. Clearing
corporations fulﬁll one or all of the following functions:
• nets many trades so that the number and the amount of payments
that have to be made are minimized,
• determines money obligations among traders, and
• guarantees that trades will go through by legally assuming the risk
of payments not made or securities not delivered. This latter func-
tion is what is implied when it is stated that the clearing corpo-
ration becomes the “counter-party” to all trades entered into its
system. Also known as a clearinghouse or clearinghouse associa-
tion.
To understand how a clearinghouse works, suppose that the depositors of BankP
and BankQ exchange a total of two checks as follows during the day:
1. Bob (a BankQ depositor) deposits a $100 check from Alice, a BankP depositor.
2. Dan (a BankP depositor) deposits a $250 check from Carol, a BankQ depositor.
BankP and BankQ send the deposited checks to a clearinghouse to total up the trans-
actions between them. The clearinghouse will let each bank know how much it owes

Networks: Case Studies
167
to (or is owed from) other banks to settle their accounts each banking day. In this
example, BankP and BankQ settle by having BankQ transfer $150 to BankP. BankP
will credit $250 to Dan’s account and debit Alice’s account by $100. BankQ will
credit $100 to Bob’s account and debit $250 from Carol’s account. In the (one hopes
unlikely) event that BankQ is unable to cover its debts, the clearinghouse will pay
BankP what it is owed.
To allow faster check processing, banks typically employ check truncation, which
the FFIEC deﬁnes as follows (FFIEC, 2004, page B-3): “The practice of holding a
check at the institution at which it was deposited (or at an intermediary institution)
and electronically forwarding the essential information on the check to the institution
on which it was written. A truncated check is not returned to the writer.”
To support check truncation, banks and other ﬁnancial institutions use electronic
check conversion (ECC) to convert endorsed physical checks into legally equivalent
check images. During electronic check conversion, magnetic-ink character recogni-
tion (MICR) captures information from a check’s MICR line, including: the bank’s
routing number, account number, check number, check amount, and other informa-
tion printed near the bottom of the check in magnetic ink in accordance with gener-
ally applicable industry standards. We represent BankQ’s ECC of an endorsed check
as follows:
ECCBankQ says (Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩).
That is, the result of BankQ’s ECC process is an electronic claim that Q presented an
integrity-checked signed check for the speciﬁed amount.
The use of electronically converted checks in the context of check truncation
is known as electronic check presentment (ECP). The FFIEC deﬁnes ECP as fol-
lows (FFIEC, 2004, page B-6): “Check truncation methodology in which the paper
check’s MICR line information is captured and stored electronically for presentment.
The physical checks may or may not be presented after the electronic ﬁles are deliv-
ered, depending on the type of ECP service that is used.”
BankQ’s presentation of the electronic check image can be represented as:
BankQ says (ECCBankQ says
(Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)).
That is, BankQ is relaying the output of its ECC process. Presumably, BankQ makes
this statement (i.e., vouches for its electronic check conversion process) only when
it believes the process is working correctly.
Figure 8.5 illustrates the use of a clearinghouse and check images. Bob, the payee,
deposits Alice’s check at his bank (arrow 2). Bob’s bank does not present the check
endorsed by Bob to Alice’s bank directly. Rather, Bob’s bank truncates the check,
credits Bob’s account (arrow 3), and sends an electronic version of the check—
usually in a batch with other orders—to an Automated Clearing House (ACH) opera-
tor (arrow 4), who sends the image and information to Alice’s bank (arrow 5) to debit
Alice’s account (arrow 6). The ACH operator settles the accounts between Alice’s
and Bob’s respective banks each day (arrows 7 and 8).

168
Access Control, Security, and Trust: A Logical Approach
FIGURE 8.5 Interbank checking using the ACH electronic clearinghouse
Clearing corporations such as the Federal Reserve Banks guarantee payments for
depository ﬁnancial institutions using services such as FedACH. Consequently, the
Federal Reserve Banks take on the ﬁnancial risk if a depository ﬁnancial institution
(DFI) defaults and has insufﬁcient funds to settle. Hence, both the ACH and the DFIs
are signatories to transactions. Thus, the ACH is not merely relaying information but
also assuming liability.
We represent the presentation of a check image created by BankQ | ECCBankQ by
an ACH operator ACH as follows:
((ACH & BankQ) | ECCBankQ) | Q says
(SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)).
The operator is functioning as a clearing corporation and countersigns the check
image. By so doing, the ACH operator assumes the risk of the transaction if BankP
defaults at settlement time.
In this system, checks are cleared immediately without ﬁrst checking payers’ ac-
count balances. If there is an insufﬁcient balance to cover the amount of the check,
the check in question is returned to the depositor, and the amount is ultimately
charged back to his or her account as a separate transaction. In Figure 8.5, if Alice’s
check bounces, then Alice’s check (or a truncated version of her check) is returned
by her bank to the ACH operator to debit Bob’s bank the amount of the returned
check.

Networks: Case Studies
169
8.3.2
Bank Authorities, Jurisdiction, and Policies
The controlling authorities in this case include the bank owners as well as the
Automated Clearing House (ACH) association, whose rules all members agree to
follow as a condition of membership. We begin by formalizing the necessary poli-
cies for each of these authorities, starting with the paying bank BankP. In the next
subsection, we show how these policies are instrumental to the operating rules these
institutions use for making access-control decisions related to interbank checking.
8.3.2.1
Policies of the Paying Bank
At the individual account level, depositors are allowed to write checks. If there are
insufﬁcient funds in the account, another transaction will reverse the debit. There-
fore, the policy allowing depositor P to write checks to payee Q can be expressed by
the following two statements:
BankP Owner controls (P controls (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)),
BankP Owner says (P controls (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)).
That is, the owner of BankP not only has the authority to set this policy but also—as
a condition of ACH membership—indeed does set it. Furthermore, BankP must have
a policy that allows payees to be the delegates of the payers indicated on checks:
BankP Owner controls (Q reps P on (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)),
BankP Owner says (Q reps P on (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)).
Applying the Controls inference rules to the above statements produces the following
policy statements for BankP:
P controls (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)
Q reps P on (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩).
Because BankP is part of the ACH network, it must recognize ACH as a counter-
signer with any ACH network bank that uses ECP:
BankP Owner controls
((ACH & BankQ) | ECCBankQ) reps (Q | SignatureP) on
(⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩),
BankP Owner says
((ACH & BankQ) | ECCBankQ) reps (Q | SignatureP) on
(⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩).
Again, the Controls inference rule lets us derive BankP’s policy regarding check
images and information forwarded to it by ACH:
((ACH & BankQ) | ECCBankQ) reps (Q | SignatureP) on
(⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩).

170
Access Control, Security, and Trust: A Logical Approach
8.3.2.2
Policies of the ACH Operator
The ACH operator accepts transactions only from ACH members. In this example,
the policies for the ACH operator regarding BankP and BankQ are as follows:
BankP controls ⟨Pay amt,Q⟩.
That is, the ACH operator will accept a payment from BankP as part of the settlement
process. Furthermore, BankQ is allowed to present electronically converted checks
to the operator:
BankQ reps ECCBankQ on
(Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)).
8.3.2.3
Policies of the Receiving Bank
The controlling authority for BankQ is BankQ’s owner. The following policies
result from recognizing BankP as a banking partner as part of the ACH network
(which can be determined from the MICR line). The ﬁrst policy states that checks
drawn upon accounts in BankP may be deposited in BankQ’s accounts:
BankQ Owner controls
((Q | SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)) ⊃
(⟨Pay amt,Q⟩∧(Q controls ⟨credit amt,acctQ⟩))),
BankQ Owner says
((Q | SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)) ⊃
(⟨Pay amt,Q⟩∧(Q controls ⟨credit amt,acctQ⟩))).
We are assuming here that funds are immediately available (i.e., there is no ﬂoat
time). The second policy states that BankQ recognizes ACH’s settlement statement:
BankQ Owner controls (ACH controls ⟨Pay amt,Q⟩),
BankQ Owner says (ACH controls ⟨Pay amt,Q⟩).
As before, BankQ’s policies can be obtained by applying the Controls inference
rule to the previous statements:
(Q | SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)) ⊃
(⟨Pay amt,Q⟩∧Q controls ⟨credit amt,acctQ⟩),
ACH controls ⟨Pay amt,Q⟩.
8.3.3
Bank Operating Rules
There are ﬁve access-control decisions to be made during the course of check pro-
cessing, corresponding to the arrows labeled 3–8 in Figure 8.5. The ﬁrst decision

Networks: Case Studies
171
(arrow 3) is made by the receiving bank BankQ in response to Bob’s request to de-
posit Alice’s check, credit his account by the same amount, and have the funds made
available to him. This decision is made by the ACH Check Deposit rule, whose proof
(like those for all other inference rules in this section) is left as an exercise:
ACH Check
Deposit
SignatureQ | SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)
SignatureQ says ⟨credit amt,acctQ⟩
SignatureQ ⇒Q
(Q | SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)) ⊃
⟨Pay amt,Q⟩∧(Q controls ⟨credit amt,acctQ⟩)
⟨Pay amt,Q⟩∧⟨credit amt,acctQ⟩
.
The ﬁrst two premises correspond to Q’s presentation of both the endorsed check
and a signed deposit slip, respectively. The third premise corresponds to a signature
check for Q, while the fourth premise reﬂects BankQ’s policy to accept checks drawn
on BankP accounts.
The second decision (arrow 4) is also made by BankQ, which must decide whether
or not to electronically present the check endorsed by Q to the ACH operator. If
the check is endorsed by a depositor Q of BankQ, SignatureQ is Q’s signature, and
the check itself passes whatever integrity check the bank uses, then the check is
converted to its electronic version and passed on to the ACH operator. This decision
is made by the ACH Check Presentation rule:
ACH Check
Presentation
SignatureQ | SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)
SignatureQ ⇒Q
BankQ says (ECCBankQ says (Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)) .
The third decision (arrow 5) is made by the ACH operator to countersign the elec-
tronically converted check and present it to BankP. This decision uses the ACH
Countersign rule:
ACH
Countersign
BankQ says (ECCBankQ says (Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩))
BankQ controls (ECCBankQ says (Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩))
(ACH & BankQ) says (ECCBankQ says ((Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩))) .
Here, the ﬁrst premise corresponds to BankQ’s presentation of the electronic check
image. The second premise indicates that the ACH operator must trust that BankQ is
correctly reporting the result of its ECC process.
The fourth decision (arrows 6 and 7) is made by the paying bank BankP, which
must determine whether to debit the appropriate account and pay toward settlement.
The ACH Check Funding rule reﬂects this decision:
ACH
Check
Funding
(ACH & BankQ) says (ECCBankQ says ((Q | SignatureP) says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)))
((ACH & BankQ) | ECCBankQ) reps Q on (SignatureP says (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩))
SignatureP ⇒P
P controls (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)
Q reps P on (⟨Pay amt,Q⟩∧⟨debit amt,acctP⟩)
(BankP says ⟨Pay amt,Q⟩)∧⟨debit amt,acctP⟩
.

172
Access Control, Security, and Trust: A Logical Approach
The premises of this rule capture many necessary conditions: BankP’s must receive
the electronic check image countersigned by the ACH (premise 1), recognize the
countersigned check image as being relayed on behalf of the payee Q (premise 2),
verify the signature of the payer P (premise 3), recognize the payer P’s authority to
write checks on the account acctP (premise 4), and recognize that the payee Q is the
check writer P’s delegate in this context (premise 5).
The ﬁnal decision (arrow 8) is made by the ACH operator using the ACH Check
Settlement rule:
ACH Check Settlement
BankP says ⟨Pay amt,Q⟩
BankP controls ⟨Pay amt,Q⟩
ACH says ⟨Pay amt,Q⟩
.
Exercise 8.3.1
Give a formal proof of the ACH Check Deposit rule.
Exercise 8.3.2
Give a formal proof of the ACH Check Presentation rule.
Exercise 8.3.3
Give a formal proof of the ACH Countersign rule.
Exercise 8.3.4
Give a formal proof of the ACH Check Funding rule.
Exercise 8.3.5
Give a formal proof of the ACH Check Settlement rule.
Exercise 8.3.6
Consider the concept of operations of ACH checking as presented
in Figure 8.5. What are the risks?
8.4
Summary
Taken together, the SSL/TLS protocols, Kerberos, and the ACH network all illus-
trate the complexity of providing authentication, authorization, and accountability in
both public and private networks. These industry-standard protocols all employ a
series of requests and responses carefully designed to provide speciﬁc assurances to
their different participants.
In this chapter, we showed how to use the access-control logic to help explicate
the policies, underlying trust assumptions, and concepts of operations employed by
the various protocol principals. These case studies demonstrate that it is possible to
take real industry standards and describe them concisely in our access-control logic.
The learning outcomes associated with this chapter appear in Figure 8.6.

Networks: Case Studies
173
FIGURE 8.6 Learning outcomes for Chapter 8
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
• Given a derivable inference rule related to a network protocol, you should
be able to give a formal proof of the rule.
Synthesis
• Given an informal but clear description of a network protocol, you should
be able to construct the derivable inference rules that capture the proto-
col’s concept of operations.
Evaluation
• Given a formalization of a protocol in the access-control logic, you
should be able to evaluate the potential risks associated with the pro-
tocol’s concept of operations.
8.5
Further Reading
The IETF Internet Draft for SSL 3.0 (Freier et al., 1996) and the Requests for
Comments (RFC) for TLS version 1.1 (Dierks and Rescorla, 2006) and Kerberos
version 5 (Neuman et al., 2005) provide detailed descriptions of the messages and
data structures that comprise these protocols. For an informative but less technical
description of Kerberos, we refer interested readers to Bryant and Ts’o’s ﬁctional
dialogue describing the design of a Kerberos-like system (Bryant, 1988).


Part III
Isolation and Sharing


Chapter 9
A Primer on Computer Hardware
At the core of cybersecurity lies the need for hardware security. The bits and bytes
that support cyberspace ultimately exist in physical memory and in the registers in-
side processors. These bits move from place to place via data paths guided by timing
and control logic. Deﬁciencies or discrepancies at the hardware level may signiﬁ-
cantly compromise system security: even the most secure cryptographic protocol is
useless if someone can alter the keys stored in physical memory. As a consequence,
systems engineers must have at least an elementary knowledge of computer hardware
as it relates to security.
In this chapter, we provide a hardware primer for readers who may be unfamiliar
with the concepts and terminology of synchronous hardware design and microcode.
We introduce combinational logic circuits and registers, as well as their descrip-
tions using truth tables, block diagrams, and timing diagrams. We start with basic
deﬁnitions of components and then show how more complicated functions such as
arithmetic logic units (ALUs) are created.
This primer provides the necessary background for the rest of Part III, whose top-
ics include virtual machines, memory protection, process isolation and sharing, de-
scriptors, and capabilities.
9.1
Ones and Zeros
In hardware design, it is customary to blur the distinction between truth values
and binary values and therefore to use 0 (respectively, 1) to represent both the truth
value false (respectively, true) and the binary value 0 (respectively, 1). The particular
interpretation is determined by the context in which 0 or 1 is used.
Electrically, 0 and 1 are implemented as electrical voltages within a particular
range. For example, some designs consider 0 to be any voltage between 0 and 0.5
volts and 1 to be any voltage between 4.0 and 5.0 volts. Because the only allowed
values are 0 and 1, voltages between 0.5 and 4.0 volts would have no meaning.
All numerical values have a binary representation in hardware. How many differ-
ent values a given processor can represent ultimately depends upon its word length.
A processor’s word length is the number of bits (i.e., binary digits) in a word, which
is the smallest addressable unit of data in memory. For example, a data memory that
177

178
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.1 Timing diagram for synchronous registers and memory
consists of eight 4-bit words has a word length of four bits, and each 4-bit word has
16 possible values (0000 through 1111). Furthermore, each of the eight 4-bit words
in memory is addressable by a 3-bit address (000 through 111).
The number of bits in data registers is usually the same as the word length of data.
Typical microprocessors have word lengths of 16, 32, or 64 bits. Thus, data registers
and memory would be 16-, 32-, or 64-bits wide. Data buses—a collection of two or
more related signal lines—are typically 16-, 32-, or 64-bits wide. In our descriptions,
we are not concerned about the details of word length, so we omit from our diagrams
any mention of word lengths. We assume that words are wide enough to represent
all values of interest.
9.2
Synchronous Design
Most computer hardware is designed using synchronous design principles. That
is, the registers and memory that are used to store values are updated relative to a
central system clock. In this section, we describe the behavior of various kinds of
registers, including synchronous registers, registers with load control, and registers
with tri-state outputs. We also provide a description of combinational-logic design
that includes the development of a simple arithmetic logic unit (ALU). Registers
combined with ALUs are the core of synchronous central processing units (CPUs).
9.2.1
Synchronous Registers
Figure 9.1 contains two diagrams of a synchronous register. On the left of the
ﬁgure, we see a register with two inputs (in and clock) and a single output (out). On
the right is a timing diagram that shows the values of clock, in, and out at different
cycles. At the top of the timing diagram is the clock signal—a square wave whose
value alternates from 1 to 0 and back again. In synchronous design, clock cycles are
bounded by what are called active clock edges. Figure 9.1 illustrates the case where
the active clock edges are the 0-to-1 clock transitions.

A Primer on Computer Hardware
179
A single clock cycle corresponds to one cycle of the system clock signal that is
bounded by two active clock edges. The simplest approach to synchronous design is
single-phase clocking, in which there is only one system clock and one active clock
edge (i.e., either 0-to-1 or 1-to-0). Our descriptions assume that the active edges are
the 0-to-1 clock edges, and thus the 1-to-0 clock transitions are of no consequence.
Figure 9.1 displays three clock cycles, labeled 1, 2, and 3. In practice, multiple
clocks and phases may be used (e.g., two- and four-phased clocks). However, for
our purposes, single-phase clocking is sufﬁcient.
The third row in the timing diagram shows the values that appear on the input line
in during cycles 1, 2, and 3: the values v1,v2, and v3, respectively. At the beginning
of each clock cycle, the values may change as a result of an active clock edge. These
delays correspond to the time necessary for circuits to charge or discharge. For
correct design behavior, all input signals to registers and memory must be stable prior
to the next active clock edge. The maximum time to charge or discharge determines
the maximum clock frequency.
The fourth row of the timing diagram shows the values that appear on the output
line out during cycles 1, 2, and 3. In cycle 1, the value of out is labeled X: that is,
the value is either unknown or a don’t-care value. The values of out in cycles 2 and
3 are v1 and v2, respectively.
The behavior of the register in Figure 9.1 is best described as a delayed repetition:
its output value at cycle n+1 is the same as its input value at cycle n. The relationship
between out and in is described by
out(n+1) = in(n),
where, for any given cycle number i, out(i) and in(i) respectively denote the output
value and input value at cycle i.
9.2.2
Registers with Load Control
The simple register shown in Figure 9.1 stores a potentially different value each
clock cycle. In contrast, Figure 9.2 shows a register with an additional control signal
LD, which controls whether or not values on in are loaded into the register. In this
register, input values are loaded precisely when the value of LD is active (i.e., when
LD’s value is 1). When LD’s value is 1 during cycle n, the value of out during cycle
n + 1 will be in(n); when LD’s value is 0 during cycle n, the value of out in cycle
n+1 remains unchanged. This relationship is shown below:
out(n+1) =
(
out(n),
if LD(n) is 0,
in(n),
if LD(n) is 1.
9.2.3
Registers with Tri-State Outputs
All digital circuits in computer hardware have electrical values corresponding to 0
and 1. Some digital circuits also have the additional capability to have a third value

180
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.2 Timing diagram for synchronous registers with load control
FIGURE 9.3 Tri-state buffer
called a high-impedance state, which is denoted by Hi-Z. Circuits that have three
values—0, 1, and Hi-Z—are known as tri-state circuits. Electrically, Hi-Z corre-
sponds to an open switch—that is, a path of inﬁnite resistance (hence the term “high
impedance”).
One of the simplest digital circuits with a tri-state capability is a tri-state buffer.
Figure 9.3 shows the standard symbol for a tri-state buffer with two inputs (in and
EN) and a single output (out). Logically, buffers are analogous to switches, and
Figure 9.3 also shows the analogous switch states associated with the output-enable
signal EN. When EN is 0, the tri-state buffer output behaves like an open switch,
where out is electrically isolated from in. When EN is 1, the tri-state buffer output
behaves like a closed switch, where out is electrically connected to in and out = in.
We can express the relationship between inputs and outputs for a tri-state buffer in
clock cycle n as follows:
out(n) =
(
in(n),
if EN = 1,
Hi-Z,
if EN = 0.
Notice that tri-state buffers are effectively instantaneous: the value on the output line
out depends upon the values of in and EN for the very same cycle.
Circuits with tri-state outputs are useful when inputs and outputs come from mul-
tiple sources, as in the following example.

A Primer on Computer Hardware
181
FIGURE 9.4 Tri-state bus example
ENA
ENB
A
B
Data Bus
1
1
X
X
Not permitted: electrical conﬂict
1
0
valA
X
valA
0
1
X
valB
valB
0
0
X
X
Hi-Z
Table 9.1: Tri-state data-bus values
Example 9.1
Consider a data bus in a processor with two input sources, A and B. These sources are
electrically connected to the data bus using tri-state buffers with the enable controls
ENA and ENB, as shown in Figure 9.4.
Table 9.1 shows the values that appear on the data bus for all combinations of val-
ues for the tri-state buffer controls ENA and ENB. As before, X denotes an unknown
or a don’t-care value. Note that enabling both tri-state buffer outputs simultaneously
results in an electrical conﬂict—for example, A might be at 0 volts at the same time
that B is at 5 volts—and must therefore be avoided. When both control signals are
0, then both A and B inputs are electrically isolated from the data bus; if they are
the only inputs, the data bus itself is in a Hi-Z state. The other two cases correspond
to exactly one of A and B being enabled, and the data bus takes on the value of the
enabled input.
♦
Registers are frequently equipped with tri-state buffers so that their outputs can
be enabled in sequence onto a data- or memory-address bus. Figure 9.5 shows an
implementation of a register with load control and tri-state outputs. Speciﬁcally,
the ﬁgure contains a register with load control whose output is fed into a tri-state
buffer. Note that we have left out the clock line: for synchronous designs, the clock
is assumed to go to all registers and is omitted from high-level descriptions.
Table 9.2 fully describes a register with load control and tri-state outputs under
control of a control input EN. When EN is active (i.e., has the value 1), then the
register behaves exactly like the register with load control from Figure 9.2: either
0 or 1 appears on the output, as appropriate. When EN is inactive, however, the
corresponding value of out is Hi-Z, regardless of any other input.

182
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.5 Register with tri-state output
EN(n+1)
LD(n)
in(n)
out(n)
out(n+1)
1
1
1
X
1
1
1
0
X
0
1
0
X
1
1
1
0
X
0
0
0
X
X
X
Hi-Z
Table 9.2: Register with high-impedance output
9.2.4
Combinational Logic and Functions
In addition to registers and memory components, computer hardware consists of
combinational logic and arithmetic functions. Table 9.3 gives the truth-table deﬁ-
nitions of inverters (i.e., not gates), and gates, nand gates, or gates, nor gates, and
exclusive-or gates, while Figure 9.6 shows their symbolic representation.
These
gates are the basic components for all digital computer hardware. Like the tri-state
buffers, pure combinational-logic circuits are effectively instantaneous: their output
values for a given cycle depend on the input values of the very same cycle.
The following example illustrates how the combinational-logic gates are used to
implement a full adder, which serves as a basic building block for developing more
complex hardware components. Speciﬁcally, a full adder is a circuit that adds three
single bits to compute a 2-bit result.
Example 9.2
A full adder has three inputs: x, y, and the carry input cin. It also has two outputs: the
carry output cout and the sum s. Table 9.4 provides an arithmetic interpretation for a
x
y
not(x)
and(x,y)
nand(x,y)
or(x,y)
nor(x,y)
xor(x,y)
1
1
0
1
0
1
0
0
1
0
0
0
1
1
0
1
0
1
1
0
1
1
0
1
0
0
1
0
1
0
1
0
Table 9.3: Simple combinational-logic functions

A Primer on Computer Hardware
183
FIGURE 9.6 Basic combinational-logic gates
Inputs
Outputs
Arithmetic Interpretation
x
y
cin
cout
s
2×cout +s = x+y+cin
1
1
1
1
1
2×1+1 = 1+1+1
1
1
0
1
0
2×1+0 = 1+1+0
1
0
1
1
0
2×1+0 = 1+0+1
1
0
0
0
1
2×0+1 = 1+0+0
0
1
1
1
0
2×1+0 = 0+1+1
0
1
0
0
1
2×0+1 = 0+1+0
0
0
1
0
1
2×0+1 = 0+0+1
0
0
0
0
0
2×0+0 = 0+0+0
Table 9.4: An arithmetic interpretation of a full adder
full adder, showing all eight combination of inputs, the corresponding carry and sum
outputs, and the arithmetic relationship that holds between the inputs and outputs.
For example, the ﬁrst line of the table corresponds to the case where x, y, and cin
all have the value 1. Their base-10 sum x + y + cin = 1 + 1 + 1 is 310, which can
be expressed in base-2 as 112; this result is identical to that obtained by looking at
the bit sequence couts = 11.1 Similarly, the ﬁfth line of the table covers the case
where x has the value 0, while both y and cin have the value 1. Their base-10 sum
x + y + cin = 0 + 1 + 1 is 210, which can be expressed in base-2 as 102; again, this
result is identical to that obtained by looking at the bit sequence couts = 10.
Using this table, one can verify that the carry and sum output functions are cor-
rectly deﬁned using combinational logic as follows:2
cout = or(and(x,y),and(x,cin),and(y,cin)),
s = xor(xor(x,y),cin).
Figure 9.7(a) contains a block-diagram representation of a full adder with inputs
Xi, Yi, and Ci, and outputs Ci+1 and Si. Figure 9.7(b) contains a gate-level imple-
mentation of the full adder, which comes directly from the sum and carry output
functions deﬁned above.
♦
1Following standard conventions, we use the notation bn−1bn−2 ···b0 to indicate a sequence of bits that
starts with bn−1 and terminates with b0.
2In our diagrams and notations, we often make use of n-ary operators and gates—such as or(x,y,z)—
rather than expanding them to a series of binary operators or gates, such as or(x,(or(y,z))). Such presen-
tations are more concise, and are common among hardware designers.

184
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.7 Full-adder implementation
Basic combinational-logic gates and full adders are used to create more complex
arithmetic and logic units, as we show in the next subsection.
9.2.5
Arithmetic Logic Units
In the previous example, we showed how combinational-logic gates are combined
to form basic arithmetic building blocks such as full adders. These basic arithmetic
building blocks are combined with combinational-logic functions to create arithmetic
logic units (ALUs), which are the computational core of central processing units
(CPUs).
A simple example is an n-bit adder, which adds two n-bit words (X and Y) and
a carry-input bit C0 to produce an (n + 1)-bit result that consists of a carry-output
bit Cn and an n-bit sum S. Figure 9.8(a) contains a block diagram of an n-bit adder.
The n-bit adder is easily implemented by a linear array of full adders called a ripple
adder (so called because the carry bits “ripple” from the least signiﬁcant bit to the
most signiﬁcant bit). Figure 9.8(b) shows an implementation of an n-bit adder by an
n-bit ripple adder.
The arithmetic relationship between the inputs (X = Xn−1 ...Xo, Y = Yn−1 ...Y0,
and C0) and the outputs (Cn and S = Sn−1 ...S0) is as follows:
2n ×Cn +2n−1 ×Sn−1 +···+20 ×S0 =
2n−1 ×Xn−1 +2n−2 ×Xn−2 +···+20 ×X0+
2n−1 ×Yn−1 +2n−2 ×Yn−2 +···+20 ×Y0 +20 ×C0.
That is, the binary value of the (n+1)-bit sequence CnSn−1 ···S0 is equal to the sum
of the inputs’ binary values.
The following example illustrates this relationship between inputs and outputs.

A Primer on Computer Hardware
185
FIGURE 9.8 An n-bit adder and ripple adder
Example 9.3
Suppose we have a 4-bit adder with the following inputs:
X = X3X2X1X0 = 10002 = 810,
Y = Y3Y2Y1Y0 = 10012 = 910,
C0 = 12 = 110.
The result of adding X,Y, and C0 together is 1810, which in binary is given by the
carry output and sum bits as follows:
C4S3S2S1S0 = 100102 = 1810.
One can also verify that the ripple-adder implementation computes this sum cor-

186
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.9 ALU implementation
rectly, as follows:
C1S0 = X0 +Y0 +C0 = 0+1+1 = 102,
C2S1 = X1 +Y1 +C1 = 0+0+1 = 012,
C3S2 = X2 +Y2 +C2 = 0+0+0 = 002,
C4S3 = X3 +Y3 +C3 = 1+1+0 = 102.
♦
Finally, we show how to construct a simple ALU with four operations from an
n-bit adder and combinational-logic gates that control the inputs to the adder.
Example 9.4
Figure 9.9 shows a high-level block diagram of a four-function ALU. The ALU has
two n-bit data inputs (A and B), an n-bit data output (out), and two one-bit ALU-
function inputs (F1 and F0). Taken together, the function inputs F1 and F0 specify
which one of four operations the ALU is to perform: Inc B (i.e., pass the value of
B+1 to out), Pass A (i.e., pass A to out), Pass B (i.e., pass B to out), and ADD A B
(i.e., pass the value of A+B to out). The ALU output at clock cycle i is described as
follows:
out(i) =









B(i)+1,
if F1(i)F0(i) = 00,
A(i),
if F1(i)F0(i) = 01,
B(i),
if F1(i)F0(i) = 10,
A(i)+B(i)
if F1(i)F0(i) = 11.
Recall that the inputs and outputs of the n-bit adder obey the following relation:
CnSn−1 ···S0 = (Xn−1 ···X0)+(Yn−1 ···Y0)+C0.
The four ALU operations are implemented by an n-bit adder by appropriately con-
trolling the ALU inputs, as shown in Figure 9.9. Note that the X and Y inputs to

A Primer on Computer Hardware
187
ALU Operation & Code
Internal Control Signals
Function
F1
F0
PassB
PassA
C0
out
Inc B
0
0
1
0
1
B + 1
Pass A
0
1
0
1
0
A
Pass B
1
0
1
0
0
B
ADD A B
1
1
1
1
0
A+B
Table 9.5: ALU functions
the adder both come from and gates: each one of the n input lines to the n-bit X
(respectively, Y) adder input comes from an and gate that combines an individual
bit of A (respectively, B) with PassA (respectively, PassB). That is, the following
relationships hold:
Xi = and(Ai,PassA),
Yi = and(Bi,PassB).
The net result is that X is 0 whenever PassA is set to 0, and otherwise X is A
(and similarly for Y, PassB, and B).
For simplicity, we write and(A,PassA) to
mean the bit-wise “and”ing of PassA with the individual bits of A, and similarly
for and(B,PassB):
and(A,PassA) = and(An−1,PassA)···and(A0,PassA),
and(B,PassB) = and(Bn−1,PassA)···and(B0,PassA).
Thus, provided that PassA,PassB, and C0 are appropriately controlled, the n-bit
adder produces each of the four ALU functions, as follows:
CnSn−1 ···S0 = (Xn−1 ···X0)+(Yn−1 ···Y0)+C0 =









and(A,0)+and(B,1)+1 = B+1,
if PassA = 0,PassB = 1,C0 = 1,
and(A,1)+and(B,0)+0 = A,
if PassA = 1,PassB = 0,C0 = 0,
and(A,0)+and(B,1)+0 = B,
if PassA = 0,PassB = 1,C0 = 0,
and(A,1)+and(B,1)+0 = A+B,
if PassA = 1,PassB = 1,C0 = 0.
The task of controlling PassA, PassB, and C0 falls to the control-logic compo-
nent on the righthand side of Figure 9.9. Using both this ﬁgure and Table 9.5, it is
straightforward to verify that the following logic will properly control each signal:
PassA = F0,
PassB = nand(not(F1),F0),
C0 = and(not(F1),not(F0)).
♦

188
Access Control, Security, and Trust: A Logical Approach
Exercise 9.2.1
Consider the following logical expression:

out = and(or(A,B),C)
a. Devise a truth table showing the value of out for all possible input combina-
tions.
b. Draw a schematic diagram of gates showing a hardware implementation of
out.
Exercise 9.2.2
Consider the following truth table:
x
y
z
out
0
0
0
0
0
0
1
0
0
1
0
0
0
1
1
1
1
0
0
0
1
0
1
0
1
1
0
1
1
1
1
1
a. Give a logical formula that expresses out as a function of x,y, and z.
b. Implement out as deﬁned above using only inverters, and gates, and or gates.
Try to use as few gates as possible.
c. Implement out as deﬁned above using only nand gates. As a hint, the following
relationship (known as DeMorgan’s Law) is helpful:
not(and(A,B)) = or(not(A),not(B)).
Exercise 9.2.3
Positive and negative numbers are typically represented using a
two’s-complement representation. In a two’s-complement representation, the most
signiﬁcant bit xn−1 has a weight of −2n−1; in contrast, an unsigned-binary repre-
sentation gives the most signiﬁcant bit a weight of 2n−1. Thus, the following rela-
tionships hold, depending upon whether the representation is two’s complement or
unsigned binary:
(xn−1xn−2 ···x0)2
=
(
2n−1 ×xn−1 +2n−2 ×xn−2 +···+20 ×x0,
in unsigned binary,
(−2n−1 ×xn−1)+2n−2 ×xn−2 +···+20 ×x0,
in two’s complement.
a. Fill in the following table, giving both the unsigned binary and the two’s-
complement interpretations of all 4-bit binary numbers.

A Primer on Computer Hardware
189
x3
x2
x1
x0
unsigned binary
two’s-complement
0
0
0
0
0
0
0
1
0
0
1
0
0
0
1
1
0
1
0
0
0
1
0
1
0
1
1
0
0
1
1
1
1
0
0
0
1
0
0
1
1
0
1
0
1
0
1
1
1
1
0
0
1
1
0
1
1
1
1
0
1
1
1
1
b. Using the table from part (a), devise a method that uses only logical negation
and addition to negate any two’s-complement number. That is, devise a method
that, when given a two’s-complement bit sequence
xn−1xn−2 ···x0
whose base-10 value is X10, produces a two’s-complement bit sequence
yn−1yn−2 ···y0
whose base-10 value is −X10.
c. What is the maximum positive number that can be represented in n bits, using
two’s-complement representation? What is the most negative number that can
be represented in n bits, using two’s-complement representation?
d. Using only full adders and basic combinational-logic gates as components,
devise a 4-function ALU with the following speciﬁcations:
F1
F0
Function
0
0
A+B
0
1
A−B
1
0
−A+B
1
1
−A−B
You should assume a two’s-complement interpretation.

190
Access Control, Security, and Trust: A Logical Approach
9.3
Microcode
In the previous section, we described several forms of synchronous registers, in-
troduced basic combinational logic gates, showed how n-bit adders are built, and de-
signed a simple ALU. Taken together, these components are sufﬁcient for describing
the operation of processors in terms of their data paths and control paths. A cen-
tral processing unit’s (CPU) data and control paths determine which computations
are performed and in what order. In this section, we discuss how a CPU’s data and
control paths are controlled using instructions called microcode or microinstructions.
9.3.1
Data Paths and Control Paths
Data paths consist of registers and memory that store data, the paths that data can
follow, and the arithmetic and logic functions on those paths. Control paths consist
of the control lines that direct the timing and ﬂow of data, the control lines that direct
which functions are performed by arithmetic and logic units, and the sequence in
time that values appear on the control lines.
Figure 9.10 shows a simple data path and control path. The data path consists of
the following seven components:
• The ALU
• Three registers (Reg1, Reg2, and ACC, which is known as the accumulator)
• A bus (Data Bus) used to supply inputs to the a input of the ALU from the
outputs of Reg1 and Reg2
• A data line connecting the output of ACC to the b input of the ALU.
The control path consists of the seven control signals (LD1,LD2,LDACC,EN1,EN2,
F0, and F1) and the timing and control unit (TCU).
The TCU accepts a machine-language instruction and converts it to a sequence
of microinstructions, which are very ﬁne-grained instructions that correspond to the
CPU’s data path. Microinstructions typically take only a single clock cycle to exe-
cute, whereas machine-language instructions usually take two or more clock cycles.
For example, the data and control paths in Figure 9.10 might have the following
microinstructions, all of which are executable in a single clock cycle:
• ACC ←Reg1: load ACC with the contents of Reg1
• ACC ←Reg2: load ACC with the contents of Reg2
• ACC ←Reg1 + ACC: add the contents of Reg1 to the contents of ACC and
store the result in the ACC
• ACC ←Reg2 + ACC: add the contents of Reg2 to the contents of ACC and
store the result in the ACC

A Primer on Computer Hardware
191
FIGURE 9.10 Simple data- and control-path example
• Reg1 ←ACC: store the contents of ACC into Reg1
• Reg2 ←ACC: store the contents of ACC into Reg2
• Reg1 ←Reg2: store the contents of Reg2 into Reg1
• Reg2 ←Reg1: store the contents of Reg1 into Reg2
The TCU implements each microinstruction by appropriately setting the values of the
control-path signals to perform the speciﬁed operation, as illustrated in the following
example.
Example 9.5
Suppose that the ALU of Figure 9.10 is the 4-function ALU described by Table 9.5,
and further suppose that we wish to implement the microinstruction
ACC ←Reg1,
which should load ACC with the contents of Reg1.
The following table shows the control-path settings necessary to implement this
microinstruction:
Microinstruction
LD1
EN1
LD2
EN2
F1
F0
LDACC
ACC ←Reg1
0
1
0
0
0
1
1
The control-path settings have the following effects upon the data-path operations:
• LD1 = 0: The contents of Reg1 are unchanged by the microinstruction.
• EN1 = 1: The contents of Reg1 are enabled onto the Data Bus.
• LD2 = 0: The contents of Reg2 are unchanged by the microinstruction.
• EN2 = 0: The contents of Reg2 are isolated from the Data Bus.

192
Access Control, Security, and Trust: A Logical Approach
• F1F0 = 01: The ALU operation is PASS A, which in this case passes the con-
tents of Reg1 to the ALU output.
• LDACC = 1: The contents of the accumulator ACC are updated with the ALU
output, namely the contents of Reg1.
♦
9.3.2
Microprogramming
Microprograms are sequences of microcode instructions.
Although micropro-
grams can be implemented directly in hardware, they are typically implemented in
ﬁrmware: that is, the microinstructions are stored in read-only memory and are ac-
cessed by hardware known as a microprogram sequencer. The advantage of ﬁrmware
over hardware is that ﬁrmware is changeable (although not by users), which allows
a machine’s behavior to be altered without modifying the hardware. In particular,
altering the contents of the read-only memory containing microinstructions permits
the same hardware to emulate different machine instruction sets. A disadvantage of
ﬁrmware, however, is that it is slower than instructions implemented by hardware
alone.
Microprograms are the interface between hardware and software. In our descrip-
tions, we will frequently describe operations at the microprogram level using exam-
ple data and control paths.
Example 9.6
Suppose we wish to implement a machine instruction that swaps the contents of
registers Reg1 and Reg2, using the data and control path shown in Figure 9.10. The
following microprogram implements the swap instruction:
1. ACC ←Reg1
2. Reg1 ←Reg2
3. Reg2 ←ACC
♦
Exercise 9.3.1
Using Figure 9.9 as a starting point, design the control logic for the

internal control signals shown in Table 9.5.
That is, determine the logical formulas for each control signal, and then draw the
schematic of a 4-bit ALU using full adders and the basic combinational logic gates
shown in Figure 9.6.
Exercise 9.3.2
For each of the following machine instructions, draw a timing dia-
gram and a sequence of microinstructions that show the sequence of control signals
for the data path in Figure 9.10 that implements the given instruction:

A Primer on Computer Hardware
193
a. ACC ←Reg2: load ACC with the contents of Reg2
b. ACC ←Reg1 + ACC: add the contents of Reg1 to the contents of ACC and
store the result in the ACC
c. ACC ←Reg2 + ACC: add the contents of Reg2 to the contents of ACC and
store the result in the ACC
d. Reg1 ←ACC: store the contents of ACC into Reg1
e. Reg2 ←ACC: store the contents of ACC into Reg2
f. Reg1 ←Reg2: store the contents of Reg2 into Reg1
g. Reg2 ←Reg1: store the contents of Reg1 into Reg2
Exercise 9.3.3
Devise a sequence of microinstructions that implements the assem-
bly language instruction ADD Reg1 Reg2 (i.e., ACC is updated with the sum of the
contents of Reg1 and Reg2).
Exercise 9.3.4
Draw the timing diagram for the operation ADD Reg1 Reg2 (see the
previous exercise), using the following template:
clock
cycle
1
2
3
4
LD1
EN1
Reg1
LD2
EN2
Reg2
F1
F0
out
LDACC
ACC
9.4
Summary
In this chapter, we introduced the fundamentals of synchronous hardware design
and microprogramming: understanding these fundamentals is essential to under-
standing hardware security, which we will be exploring in the next few chapters.
Common to all synchronous hardware is a central clock that synchronizes all state
changes: registers can change their values only on active clock edges.

194
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.11 Learning outcomes for Chapter 9
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
• When given a block diagram of combinational-logic gates, you are able
to write the output of any logic gate as a function of the inputs to the
block diagram.
• When given a data-path diagram and a microinstruction, you are able to
describe the computation performed and the next state of the system.
Application
• When given a logical function, you are able to translate that function into
a block diagram of combinational-logic gates.
Analysis
• When given a data path and control path description, a microprogram,
and the values in all relevant registers and memory, you are able to de-
termine what computation or operation is performed and the values in all
relevant registers and memory.
Synthesis
• When given a description of a machine-level instruction and a block di-
agram of a processor with its control lines, devise microprograms that
implement the machine-level instruction.
Synchronous hardware designs are described by a combination of data and control
paths. Data paths are the paths along which data travel and on which data operations
are performed. Control paths comprise the control signals that direct both data and
the operations on data. During any clock cycle, the values of the control signals are
viewed as a microinstruction. Sequences of microinstructions form microprograms,
which are used to implement the instruction set of a processor.
In this chapter, we described the operation of real processor hardware. In the next
chapter, we show how simulations of processors—virtual machines—are created.
Virtual machines provide the illusion to users of controlling an entire processor when
in fact the same processor and memory are safely and securely shared among many
users.
The learning outcomes associated with this chapter appear in Figure 9.11.

A Primer on Computer Hardware
195
9.5
Further Reading
There are numerous books on digital hardware design, microprogramming, and
computer architecture. Wakerly’s book—Digital Design: Principles and Practices
(Wakerly, 2006)—covers combinational-logic design and sequential logic design.
Comer’s book—Essentials of Computer Architecture (Comer, 2005)—covers CPUs,
instruction sets, microcode, and memory management.


Chapter 10
Virtual Machines and Memory
Protection
Controlling access to memory is critical for sharing one machine among many users
and processes. Security is impossible without the capabilities to isolate one process
from another and to control how data are shared. For example, if Alice and Bob
are running their programs on the same machine, their programs should be protected
from one another. Such protection requires that Alice’s calculations should not spill
over into the memory used by Bob. Furthermore, unless Alice has authorization
from Bob or other appropriate controlling authorities, Alice should not be able to
read from or write to Bob’s data stored in memory.
Many of the basic concepts of memory protection—including virtual memory and
access control—were worked out in the era of time-shared mainframe computers.
These mainframes were viewed as centralized resources to be shared among many
users simultaneously. With the introduction of minicomputers and microprocessors—
which later evolved into personal computers—many memory-protection mechanisms
were discarded for efﬁciency reasons, under the assumption that personal computers
were ultimately controlled by single users and not shared. Widespread network-
ing has since falsiﬁed this assumption, making memory-protection mechanisms and
virtual-machine technologies relevant once again.
A virtual machine (VM) is a simulation of a physical machine. This simulation
provides the illusion to operating systems and programs that they are running di-
rectly on the hardware. Protecting the real machine, processes, and one user from
another is the task of the virtual machine monitor (VMM), which is a layer (often
implemented in microcode or software) separating virtual machines and hardware.
Figure 10.1 illustrates this idea, showing three operating systems supported by a
VMM that operates on a single hardware platform. Each operating system executes
within an environment created by the VMM; each of these environments is a VM.
The VMM manages the hardware (memory, processor, and input/output) with
three objectives in mind:
1. Creation of an identical execution environment to that of the hardware being
emulated
2. Efﬁcient execution of instructions to maintain acceptable program execution
speeds
3. Intervention by the VMM in any attempt to alter the allocation of resources
197

198
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.1 Virtual machine monitor protecting memory
As Figure 10.1 shows, VMMs are reference monitors that check the instructions
issued by VMs in order to guard the integrity of the physical system. Virtual ma-
chine monitors are also called hypervisors or security kernels: we use the virtual
machine monitor nomenclature in this text to emphasize the VMM’s role as a ref-
erence monitor. In this chapter, we analyze the details of memory protection and
the basic structure and operation of VMMs. Because VMs operate at the boundary
between hardware and software, we start with a description of a simple processor
that provides a concrete context for the movement and control of data. We then look
at memory protection based on memory segmentation. After that, we develop the
mandatory access control policies for accessing memory. Finally, we look at the
structure and function of VMMs.
10.1
A Simple Processor
In this section, we introduce a concrete example of a simple processor, shown in
Figure 10.2. By specifying a speciﬁc processor, we can give concrete examples for
the underlying mechanisms that implement memory protection and the structure and
function of virtual machine monitors. The ideas we present, however, are applica-
ble to any standard processor. We start by describing the processor’s components,
its data path, and its control path. We follow with a description of the processor’s
machine-language instructions.

Virtual Machines and Memory Protection
199
FIGURE 10.2 A simple processor
10.1.1
Processor Components
Our simple processor has the following major components, which we describe in
turn.
Data Buses:
There are three data buses: the A BUS, the B BUS, and the F BUS, all
of which are used to convey data to the components of the processor.
Arithmetic Logic Unit:
The arithmetic logic unit (ALU) has data input ports A
and B, control input FUNCTION, and output port F. The ALU performs operations
on inputs A and B as speciﬁed by FUNCTION, and the output appears on F. Typical
operations of the ALU include: PASS A, PASS B, and ADD A B, which correspond
respectively to “F = A,” “F = B,” and “F = A+B.”
Accumulator:
The accumulator (ACC) register is used to store intermediate val-
ues produced by the ALU during calculations that extend over multiple instructions.
When LD is asserted, the ACC updates its contents from the F BUS at the beginning
of the next cycle. When EN is asserted, the output of ACC is immediately put onto
the B BUS.

200
Access Control, Security, and Trust: A Logical Approach
Program Counter:
The program counter (PC) register is used to point to the ad-
dress of the next machine instruction stored in the program memory. The PC register
updates its contents from the F BUS in exactly the same way as the ACC. Likewise,
the output of the PC is immediately put onto the B BUS when the EN input to the PC
is asserted.
Memory Address Register:
The memory address register (MAR) is used to point
to the address of instructions and data to be fetched or stored in program memory.
The control line LD functions in the same way for the MAR as it does for the ACC
and PC registers. In our processor, the MAR has two outputs: the ﬁrst is always
enabled and connected to the ADDRESS input of the program memory, and the second
is connected to the B BUS under the control of EN.
Instruction Register:
The instruction register (IR) holds the machine instruction
currently being executed by the processor. The input to the IR is the A BUS.
We assume that all instructions have two ﬁelds: an op-code ﬁeld and an address
ﬁeld. The op-code is a numerical encoding of the instruction to be performed, while
the address speciﬁes the memory address. For example, the operation that loads
the accumulator register with the contents of memory address 100 would be LDA
100. Operations for which memory addresses are irrelevant (e.g., CLA: clear the
accumulator) ignore the contents of the address ﬁeld. Operations for which two
memory addresses are needed (e.g., LD 100 200: load the contents of location 100
with the contents of 200) actually take up two locations in memory: LD 100 followed
by 200, where the op-code ﬁeld of the instruction containing address 200 is ignored.
Program Memory:
The program memory (PM) holds machine instructions and
data for user programs. PM has DATA IN and DATA OUT ports. The DATA IN port
gets its value from the B BUS, while the DATA OUT port is the sole input to the A BUS.
PM behaves like a linearly addressable array of q registers, starting at location 0 and
ending at location q-1. At the start of each cycle, the contents of location ADDRESS
of PM, which we denote by PM[ADDRESS], appear on the DATA OUT port, where the
value of ADDRESS comes from the MAR during the previous cycle. If a memory-read
operation is speciﬁed by the R/W control, then value PM[ADDRESS] is unchanged.
However, if a memory-write operation is speciﬁed by R/W, then the value on the B
BUS from the prior cycle is placed into PM at location ADDRESS and that value also
appears on DATA OUT.
Timing and Control Unit:
The timing and control unit (TCU) is responsible for
fetching instructions from the PM, sequencing the ﬂow of data in the processor, and
sequencing the operations performed by the ALU. The TCU controls the processor
operations and data ﬂow by the following control lines:
• FUNCTION: this line controls the function performed by the ALU.

Virtual Machines and Memory Protection
201
• LD: this bus controls which registers (ACC, PC, or MAR) are loaded from the
F BUS at the start of the next cycle. Any combination of these registers (i.e.,
ranging from none to all of them) is possible.
• EN: this bus controls which register’s output (ACC, PC, or MAR) is placed on
the B BUS. At most one register’s output should be placed on the bus.
• R/W: this line speciﬁes whether the contents of PM is a read or write.
Having described the processor’s data and control paths, we turn our attention to
the processor’s instructions at the machine level and microcode level.
10.1.2
Machine Instructions
All the registers (ACC, PC, IR, and MAR) and state-holding devices (such as PM)
are synchronous—that is, they change state on active clock edges.
We now consider a concrete example. Suppose we wish to execute the program-
level operation PM[B] ←PM[A] + PM[B], which should add the contents of program
memory at addresses A and B and store the result back into address B. To execute
this program-level operation requires three machine-level instructions to be executed
in the following order:
1. LDA @A: load the ACC with the contents of PM at address A, ACC ←PM[A].
2. ADD @B: load the ACC with the results of adding the contents of the ACC
with the contents of PM at address B, ACC ←ACC +PM[B].
3. STO @B: store the contents of the ACC into the PM at address B, PM[B] ←
ACC.
To execute each of the machine-level instructions requires a third and ﬁnal level of
programming at the microprogram level. The TCU executes microprograms that set
and sequence the appropriate values on the various control lines (e.g., FUNCTION,
LD, and EN).
All three levels of programming—program level, machine level, and microcode—
are shown in Table 10.1. The left column corresponds to a high-level operation that is
compiled into the machine-level instructions in the middle column. Each machine-
level instruction is realized by a series of micro-coded operations as shown in the
rightmost column. We use the notation MAR ←PM[MAR].addr to denote that MAR
is loaded with the address portion of the instruction found in PM[MAR].
The remainder of this section is targeted towards hardware designers, who typi-
cally must account for all of the calculations that occur during each and every clock
cycle. Readers who are not interested in such low-level details may safely skip to
Section 10.2.
Table 10.2 details the values for each control line for each microprogram operation
listed in Table 10.1. Because microprograms are intricate, we examine in detail what
happens with the instruction LDA @A. To help us keep track of all the details, we
use a timing diagram, as shown in Figure 10.3.

202
Access Control, Security, and Trust: A Logical Approach
Program Level
Operation
Instructions
Stored in Program
Memory
Microcode Operations
Sequenced by Timing and Control Unit
@B ←@A+@B
i.
LDA @A
1.
MAR ←PC
2.
PC ←PC +1
3.
IR ←PM[MAR],MAR ←PM[MAR].addr
4.
ACC ←PM[MAR]
i+1.
ADD @B
5.
MAR ←PC
6.
PC ←PC +1
7.
IR ←PM[MAR],MAR ←PM[MAR].addr
8.
ACC ←PM[MAR]+ACC
i+2.
STO @B
9.
MAR ←PC
10.
PC ←PC +1
11.
IR ←PM[MAR],MAR ←PM[MAR].addr
12.
PM[MAR] ←ACC
Table 10.1: Operations for @B ←@A+@B
Microcode Operations
LD
EN
ALU
R/W
A Bus
B Bus
F Bus
1.
MAR ←PC
MAR
PC
Pass B
R
X
PC
PC
2.
PC ←PC +1
PC
PC
Inc B
R
X
PC
PC+1
3.
IR ←PM[MAR]
MAR ←PM[MAR].addr
IR, MAR
X
Pass A
R
PM[MAR]
X
PM[MAR]
4.
ACC ←PM[MAR]
ACC
X
Pass A
R
PM[MAR]
X
PM[MAR]
5.
MAR ←PC
MAR
PC
Pass B
R
X
PC
PC
6.
PC ←PC +1
PC
PC
Inc B
R
X
PC
PC+1
7.
IR ←PM[MAR]
MAR ←PM[MAR].addr
IR, MAR
X
Pass A
R
PM[MAR]
X
PM[MAR]
8.
ACC ←
PM[MAR]+ACC
ACC
ACC
Add
R
PM[MAR]
X
PM[MAR]
+ ACC
9.
MAR ←PC
MAR
PC
Pass B
R
X
PC
PC
10.
PC ←PC +1
PC
PC
Inc B
R
X
PC
PC+1
11.
IR ←PM[MAR]
MAR ←PM[MAR].addr
IR, MAR
X
Pass A
R
PM[MAR]
X
PM[MAR]
12.
PM[MAR] ←ACC
X
ACC
X
W
PM[MAR]
ACC
X
Table 10.2: Timing and control unit operations
Conceptually, the ﬁrst machine operation—i.e., loading ACC with the contents of
PM at location A—requires the following three steps:
1. Fetch the LDA @A instruction from location i in PM and store it the instruction
register IR. We assume that the program counter PC has the address i of the
machine instruction to be executed.
2. Fetch the contents of PM at address A.
3. Store the contents of PM at address A into the accumulator ACC.
Completing these three conceptual steps requires a total of four micro-operations
(i.e., four clock cycles), as follows:
Cycle 1 objective: Copy the contents of the PC into the MAR.
Initial state: PC = i. The values of the MAR, ACC, IR, and PM[MAR] de-
pend on previous operations and are unimportant at this point.
Control lines: EN = PC, LD = MAR, FUNCTION = PASS B.

Virtual Machines and Memory Protection
203
FIGURE 10.3 Timing of CPU operations
Setting EN to PC puts the output of the PC onto the B BUS. Setting
FUNCTION to PASS B passes the values on the B BUS through the ALU
to the F BUS. Setting LD to MAR causes the F BUS values to be loaded
into the MAR at the start of the next clock cycle.
Cycle 2 objective: Increment the PC to point to the next instruction.
Initial state: PC = i, MAR = i. The values of ACC, IR, and PM[MAR] are
due to previous operations.
Control lines: EN = PC, LD = PC, FUNCTION = INC B.
Setting EN to PC puts the output of the PC onto the B BUS. Setting
FUNCTION to INC B increments the current value stored in the PC, which
is i, to i+1. This is the ALU output and is put on to the B BUS. Setting
LD to PC loads i+1 into the PC at the start of the next cycle.
Cycle 3 objective: Fetch the instruction at location i—LDA @A—and store the in-
struction in the instruction register IR and memory address A in the MAR.
State: PC = i+1, MAR = i, PM[MAR] = PM[i] = LDA @A. The values of
ACC and IR are due to previous operations.
Control lines: EN = X, LD = IR, MAR, FUNCTION = PASS A.
The MAR contains i from the previous cycle. So, the output of PM is
PM[i], namely the ith instruction LDA @A. We enable the instruction
register IR and the MAR to load LDA @A and address A respectively in
the next cycle by arranging the ALU function to pass the output of PM
on the A BUS through to the ALU output by setting FUNCTION to PASS
A.
Cycle 4 objective: Load the contents of address A in PM into the ACC.
State: PC = i+1, MAR = A, IR = LDA @A, and PM[MAR] = A. The contents
of the ACC are due to previous operations.

204
Access Control, Security, and Trust: A Logical Approach
Control lines: EN = X, LD = ACC, FUNCTION = PASS A.
The values of IR and MAR are set to the value LDA @A. Setting LD
to ACC puts the value of the ALU output (PM[A]) into the ACC at the
beginning of the next cycle. As the ALU is passing through the output
of PM, it is unimportant which register (ACC, PC, or MAR) output is
enabled onto the B BUS.
Cycles 5 through 12 can be described in a similar fashion and their descriptions are
left as an exercise for the reader.
Next, we turn our attention to the virtual machine monitor, which protects physical
memory and user processes.
Exercise 10.1.1
In a similar fashion to the description of the four microcode cycles

implementing LDA @A in the text based on Tables 10.1, 10.2, and Figure 10.3, de-
scribe the micro-cycle objectives, state, and control line values for cycles 5 through
8 implementing ADD @B.
Exercise 10.1.2
In a fashion similar to Table 10.1, work out the details of a CLA

(i.e., clear accumulator ACC) instruction. Assume that the ALU has a function ZERO,
which outputs 0 regardless of the inputs on A and B. Work out the microcode cycles
and draw the timing diagram.
Exercise 10.1.3
In a similar fashion to the description of the four microcode cycles

implementing LDA @A in the text based on Tables 10.1, 10.2, and Figure 10.3, de-
scribe the micro-cycle objectives, state, and control line values for cycles 8 through
12 implementing STO @B.
10.2
Processors with Memory Segmentation
The simple processor developed thus far gives user programs access to all loca-
tions in program memory PM. When multiple user programs are stored in program
memory, the simple processor is unable to protect one program from reading or writ-
ing over the programs or data of other users. In this section, we examine how pro-
cessors can provide this protection through memory segmentation.
10.2.1
Segmentation Using a Relocation Register
To protect the programs and data of each user, physical memory is segmented:
that is, memory is partitioned to simultaneously isolate one user from another while
giving each user the illusion that he or she controls the entire processor memory. Seg-
mentation is accomplished by the addition of a relocation register (RR) that speciﬁes
the starting address of each memory segment (i.e., the base address) and the size of
the memory segment (i.e., its bound).

Virtual Machines and Memory Protection
205
FIGURE 10.4 Memory segmentation
Figure 10.4 shows a physical memory with q locations partitioned into four seg-
ments: one segment each for Alice, Bob, Carol, and the Supervisor. The base and
bound values stored in the relocation register uniquely locate each memory seg-
ment. The idea is that, within each segment, users access virtual memory locations 0
through bound - 1, when in fact they are really accessing physical memory locations
base through base + bound - 1. Thus, the physical-memory location that corre-
sponds to the virtual address in the program counter PC is determined by adding the
base-address value to the virtual-address value in the PC. If the virtual address a is
less than bound and a + base < q, then the address being read from or written to is
within the user’s memory segment in physical memory. Otherwise, the operation is
trapped—that is, control is passed on to the Supervisor.
This policy for controlling access to physical memory is enforced for all programs
and users. Thus, it is a mandatory access-control policy.
The following example illustrates the use of the base and bound values in the
relocation register.
Example 10.1
Suppose that the value in the program counter is 57, the base-bound value in the
relocation register is (128,256), and the size of physical memory is 1024: thus, the
virtual address is deﬁned relative to a base value of 128 and a bound of 256. The
location of the next instruction is found in absolute address 128 + 57 = 185. The
absolute address falls within the bounds of physical memory (i.e., 185 ≤1023) and
within the bound set by the relocation bound register (i.e., 57 ≤255). Therefore, the
address falls within the user’s allotted memory segment, and the operation should be
allowed to proceed.
♦

206
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.5 Processor model for virtual machines
To accommodate memory segmentation, we must add a relocation register RR
and a mode bit M to our simple processor. Figure 10.5 contains a diagram of our
enhanced processor that supports virtual machines.
The mode bit M is used to indicate whether the processor is operating in user
mode or supervisor mode (denoted by u and s, respectively). In user mode, access
to security-sensitive instructions (such as writing values into RR) is trapped, which
means that the processor is limited to the segment identiﬁed by RR. In supervisor
mode, all instructions are available, including the security-sensitive instructions that
alter the values in RR. This capability to alter the values in RR gives the processor
access to all segments in PM.
Normally, the processor operates in user mode. When a program operating in user
mode either wishes to transfer control to the Supervisor or has an instruction trapped,
the mode bit is changed from u to s and control is passed to the Supervisor program.
The next three subsections develop the details of traps and how control is trans-
ferred to the Supervisor program.

Virtual Machines and Memory Protection
207
10.2.2
Processor State and Instructions
The state S of a processor is given by the values stored in its state-holding ele-
ments: program memory PM, accumulator ACC, program counter PC, memory ad-
dress register MAR, instruction register IR, mode bit M, and relocation register RR.
We denote these values in lower case by pm, acc, pc, mar, ir, m, and (base,bound),
and we represent the state S by a seven-tuple:
S = (pm,acc, pc,mar,ir,m,(base,bound))
Note that pm is an array of values pm[0] through pm[q−1], where q−1 is the largest
available address in physical program memory PM.
Because each component of S is ﬁnite, there are a ﬁnite number of processor
states S; we call this set of states PStates. Each machine instruction i can therefore
be viewed as a total function from PStates to PStates.
Example 10.2
Suppose the machine instruction LDA @15 is stored in address 0 of program memory
PM and S = (pm,acc, pc,mar,ir,u,(256,128)) is the current state of the processor.
Because the mode bit is u and the relocation register RR is (256,128), the virtual
machine is in user mode and the memory segment starts at location 256 in PM and
ends at location 256 + 127 = 383. Consulting Table 10.1, we can check the result
of the ﬁrst four micro-cycles to calculate the state that results from applying LDA
@15. If we assume that PM[256 + 15] contains the value 12 and that the values of
ACC, MAR, and IR are acc, mar, and ir respectively, then the state that results from
applying the instruction LDA @15 is as follows:
(LDA @15)(pm,acc,0,mar,ir,u,(256,128)) =
(pm,12,1,15, LDA @15,u,(256,128)).
♦
10.2.3
Program Status Word
The context of a program is given by the program status word (PSW). As Fig-
ure 10.5 shows, the PSW consists of the accumulator (ACC), memory address reg-
ister (MAR), mode bit (M), program counter (PC), relocation register (RR), and
instruction register (IR). The PSW value psw is a 6-tuple consisting of the contents
of the respective registers:
psw = (acc, pc,mar,ir,m,(base,bound)).
Thus, the PSW speciﬁes the current state of the processor, modulo the contents of
program memory.

208
Access Control, Security, and Trust: A Logical Approach
Example 10.3
Suppose the value of the PSW is (25,42,56, CLA,s,(0,65536)), where the underly-
ing executable memory size is 216 = 65536 locations. From the PSW, we see that
the current program is operating in supervisor mode, the accumulator value is 25,
the MAR is pointing to relative address 56, the program counter is pointing to rela-
tive address 42 in executable memory (which is also the absolute address as the base
value of R is 0), the current program is able to access the entire executable memory,
and the instruction in IR is clear accumulator.
♦
The PSW is a mechanism by which the program in control of the processor can
by changed. Changing programs or execution threads is essential for supporting
multiprocessing. By loading the PSW registers with values corresponding to the Su-
pervisor or other programs, the processor’s execution context is changed or switched.
For simplicity, we assume that the value of the PSW ﬁts entirely into one storage
location in PM. We also assume that PM[1] (i.e., the second location in memory) has
a ﬁxed value that never changes and that corresponds to the default state in which the
Supervisor program always starts.1 This state corresponds to a PSW value of form
pswsuper = (accsuper,addrsuper,marsuper,irsuper,s,(0,q)),
where addrsuper is the starting address of the Supervisor program, and accsuper,
marsuper, and irsuper are the default starting values for the ACC, MAR, and IR reg-
isters. Note that the mode bit is set to s (for supervisor), and the relocation register
speciﬁes that the full address range of memory is accessible.
When it is necessary to transfer control to the Supervisor program, the content of
the currently executing program (i.e., the current PSW value) is stored in memory
location PM[0]. The ﬁxed value pswsuper stored in location PM[1] is then copied
to the PSW, which allows the Supervisor program to take control of the processor.
When the Supervisor has completed its execution, control can be returned to the
original program by copying the value temporarily stored in PM[0] back to the PSW.
Now that we have explained the concept and use of the PSW, we can introduce
some of the details of traps.
10.2.4
Traps
Trapping an instruction transfers control to the Supervisor program. If all security-
sensitive instructions are trapped (i.e., inspected or executed under the supervision of
the Supervisor control program), then the privacy and integrity of program memory
can be maintained, provided that the Supervisor is itself correct and trustworthy.
1Correct implementation of memory segmentation will ensure that user programs can never change the
value of location PM[1]. The Supervisor program must also be implemented in a way that prevents
alterations to this location.

Virtual Machines and Memory Protection
209
An instruction i is said to trap if the contents of program memory are unchanged
except for the contents of PM[0]; the program status word prior to trapping instruc-
tion i is stored in PM[0]; and the new program status word takes its value from
PM[1].
Presumably, the updated value of PSW is the starting address and values of the
Supervisor program. During its execution, the Supervisor program can look at the
contents of PM[0] to ﬁnd the previous value of psw and to see the virtual machine
state just before instruction i trapped.
For now, we defer the description of the structure and contents of the Supervisor
program until later. In the next section we look at the access-control policy for
accessing program memory.
Exercise 10.2.1
Consider Table 10.1. To which states are ADD @B and STO @B

applied? Which states are the result of these function applications?
Exercise 10.2.2
Consider the application of machine instruction LDA @200 stored

in address 300 of program memory PM. Let S = (pm,acc, pc,mar,ir,u,(256,128))
be the state of the processor to which LDA @200 is applied. What is the next state?
10.3
Controlling Access to Memory and Segmentation
Registers
Having introduced the idea and mechanisms behind memory segmentation, we
now turn to describing the mandatory access control policy for program memory.
We treat the processor registers IR, M, and RR as principals who make statements
about their values. For example, suppose that the instruction register IR has the value
LDA @A. We can model this situation as
IR says ⟨LDA @A⟩,
where we interpret ⟨LDA @A⟩as the proposition “it would be a good idea to execute
the instruction LDA @A.” Likewise, we can express the situation where RR contains
the value (base,bound) as
RR says ⟨(base,bound)⟩,
where we interpret ⟨(base,bound)⟩as the proposition “the base value is base, and
the bound value is bound.” We can similarly model the situation where M has the
value u by
M says ⟨u⟩,
where ⟨u⟩is interpreted as “the virtual machine is in user mode.”

210
Access Control, Security, and Trust: A Logical Approach
As Figure 10.5 illustrates, the virtual machine monitor comprises the timing and
control unit (TCU) and the program memory. Virtual machines provide privacy and
integrity protection for users so long as all resource allocation is controlled by the
Supervisor (i.e., by the virtual machine monitor). Thus, only the Supervisor should
be able to alter the relocation register RR; user programs should not have that capa-
bility.
Throughout the remainder of this chapter, we assume that all supervisory functions
are implemented by the TCU. In other words, the TCU is designed to reﬂect the
wishes of the Supervisor,
TCU ⇒Supervisor.
10.3.1
Access to Program Memory
Access to program memory PM is governed by the value of RR and the size of
PM, which we represent by q. For each instruction that involves accessing PM, such
as LDA @A, we have an access-control policy.
In general, for an instruction OP @A that involves an operation OP on relative
address A, we have the following considerations:
• The absolute address must fall within the addressable locations of PM. That
is, we must have A +base < q, where q is the size of PM. If this condition is
violated, then a memory trap occurs.
• The relative address must fall within the bound of the memory segment as-
signed to the virtual machine (i.e., A + base < bound). If this condition is
violated, then a memory trap occurs.
• If the above two conditions are satisﬁed, then the memory operation is per-
formed.
To be explicit, there are certain classes of instructions—privileged and sensitive in-
structions, which we will deﬁne more formally in Section 10.4—for which additional
conditions may force traps. For now, we consider only innocuous instructions, whose
only traps are memory traps.
For an innocuous instruction OP @A, the behavior of the VMM when executing
OP @A is speciﬁed by the three derived inference rules in Figure 10.6: Memory
Trap 1, Memory Trap 2, and OP @A. Memory Trap 1 states that, if the address
A + base exceeds the physical limits of memory, then OP @A should be trapped.
Similarly, Memory Trap 2 states that an operation involving relative address A should
be trapped if A exceeds the segment boundary bound. The OP @A rule states that, if
the operation OP @A does not require a memory trap, then it should be performed.
In the simple processor model we have introduced (Figures 10.2 and 10.5), the in-
struction register IR is under control of the virtual machine. Hence, for our processor
model, we have
IR ⇒VM.

Virtual Machines and Memory Protection
211
FIGURE 10.6 Derived inference rules for an operation OP @A
Memory
Trap 1
TCU controls (VM says ⟨OP @A⟩
⊃RR says ⟨(base,bound)⟩⊃A +base ≥q ⊃⟨trap⟩)
TCU says (VM says ⟨OP @A⟩
⊃RR says ⟨(base,bound)⟩⊃A +base ≥q ⊃⟨trap⟩)
VM says ⟨OP @A⟩
RR says ⟨(base,bound)⟩
A +base ≥q
⟨trap⟩
Memory
Trap 2
TCU controls (VM says ⟨OP @A⟩
⊃RR says ⟨(base,bound)⟩⊃A ≥bound ⊃⟨trap⟩)
TCU says (VM says ⟨OP @A⟩
⊃RR says ⟨(base,bound)⟩⊃A ≥bound ⊃⟨trap⟩)
VM says ⟨OP @A⟩
RR says ⟨(base,bound)⟩
A ≥bound
⟨trap⟩
OP @A
TCU controls (RR says ⟨(base,bound)⟩
⊃A +base < q ⊃A < bound ⊃VM controls ⟨OP @A⟩)
TCU says (RR says ⟨(base,bound)⟩
⊃A +base < q ⊃A < bound ⊃VM controls ⟨OP @A⟩)
VM says ⟨OP @A⟩
RR says ⟨(base,bound)⟩
A +base < q
A < bound
⟨OP @A⟩
Thus, whenever IR says ⟨OP @A⟩, we can deduce that VM says ⟨OP @A⟩as well.
Example 10.4
Consider the case where the machine instruction is LDA @A (load ACC with the
contents of PM[base +A]), where A is a relative address within a segment in PM
given by (base,bound) in RR, and the maximum address in PM is q−1. The relevant
access-control rules for LDA @A are as follows:
LDA mem-
ory trap 1
TCU controls
(VM says ⟨LDA @A⟩⊃RR says ⟨(base,bound)⟩⊃A +base ≥q ⊃⟨trap⟩)
TCU says
(VM says ⟨LDA @A⟩⊃RR says ⟨(base,bound)⟩⊃A +base ≥q ⊃⟨trap⟩)
VM says ⟨LDA @A⟩
RR says ⟨(base,bound)⟩
A +base ≥q
⟨trap⟩
LDA mem-
ory trap 2
TCU controls
(VM says ⟨LDA @A⟩⊃RR says ⟨(base,bound)⟩⊃A ≥bound ⊃⟨trap⟩)
TCU says
(VM says ⟨LDA @A⟩⊃RR says ⟨(base,bound)⟩⊃A ≥bound ⊃⟨trap⟩)
VM says ⟨LDA @A⟩
RR says ⟨(base,bound)⟩
A ≥bound
⟨trap⟩

212
Access Control, Security, and Trust: A Logical Approach
LDA @A
TCU controls (RR says ⟨(base,bound)⟩⊃
A +base < q ⊃A < bound ⊃VM controls ⟨LDA @A⟩)
TCU says (RR says ⟨(base,bound)⟩⊃
A +base < q ⊃A < bound ⊃VM controls ⟨LDA @A⟩)
VM says ⟨LDA @A⟩
RR says ⟨(base,bound)⟩
A +base < q
A < bound
⟨LDA @A⟩
These three rules follow directly from the general template presented for OP @A in
Figure 10.6. The ﬁrst two rules check for a memory trap caused by either attempting
to access an address beyond the maximum size of program memory or beyond the
segment boundary. If a memory trap does not occur, then the virtual machine can
load the accumulator with the contents of program memory at address A.
♦
The preceding approach sufﬁces as a simple memory access-control policy. In
Section 10.3.3, we reﬁne this policy to cover what are known as privileged and sen-
sitive instructions, some of which include memory operations. In the remainder of
this section, however, we describe the hardware-level details of implementing this
simple memory-access policy. Readers uninterested in these details may safely skip
ahead to Section 10.3.3.
10.3.2
Implementation Details
We now turn to some implementation details for protecting physical memory
based on the processor model for virtual machines given in Figure 10.5. That model
extends the simple processor shown in Figure 10.2 by adding a relocation register
and a mode bit. In this model, all memory accesses make use of the memory address
register (MAR).
We can enforce the mandatory access control policy for memory addresses by
checking all addresses prior to loading them into the MAR. If the memory address
exceeds the boundary of either physical memory or the assigned segment, then a trap
occurs. Otherwise, the MAR is loaded with the address.
Based on Table 10.1, we can modify the Timing and Control Unit (TCU) mi-
crocode that generates the timing and control signals for the processor. The modiﬁ-
cations are straightforward, in that the MAR is loaded with a new value only if the
new value does not cause a memory trap. We can specify this using a conditional
“if-then-else” expression:
condition →a | b =
(
a,
if condition is true
b,
if condition is false
.
For the LDA @A, ADD @B, and STO @B instructions shown in Table 10.1, the
following changes are made:

Virtual Machines and Memory Protection
213
Instructions
Stored in Program
Memory
Microcode Operations
Sequenced by Timing and Control Unit
i.
LDA @A
1.
(((base + PC) ≥q) or (PC ≥bound)) →trap |
(MAR ←base+PC)
2.
PC ←PC +1
3.
IR ←PM[MAR],
(((base+PM[MAR].addr) ≥q) or (PM[MAR].addr ≥
bound)) →trap | (MAR ←base+PM[MAR].addr)
4.
ACC ←PM[MAR]
i+1.
ADD @B
5.
(((base + PC) ≥q) or (PC ≥bound)) →trap |
(MAR ←base+PC)
6.
PC ←PC +1
7.
IR ←PM[MAR],
(((base+PM[MAR].addr) ≥q) or (PM[MAR].addr ≥
bound)) →trap | (MAR ←base+PM[MAR].addr)
8.
ACC ←PM[MAR]+ACC
i+2.
STO @B
9.
(((base + PC) ≥q) or (PC ≥bound)) →trap |
(MAR ←base+PC)
10.
PC ←PC +1
11.
IR ←PM[MAR],
(((base+PM[MAR].addr) ≥q) or (PM[MAR].addr ≥
bound)) →trap | (MAR ←base+PM[MAR].addr)
12.
PM[MAR] ←ACC
Table 10.3: Memory-protected operations for LDA @A, ADD @A, and STO @B
• The simple assignment MAR ←PC is changed to the conditional expression
(((base+PC) ≥q) or (PC ≥bound)) →trap | (MAR ←base+PC).
• The simple assignment MAR ←PM[MAR].addr is changed to the conditional
expression
(((base+PM[MAR].addr) ≥q) or (PM[MAR].addr ≥bound))
→trap | (MAR ←base+PM[MAR].addr).
Notice that the addresses are treated as relative addresses in the virtual processor and
must be converted to real addresses by adding the value base. The resulting changes
to Table 10.1 are shown in Table 10.3.
10.3.3
Access to the Relocation Register
Memory segmentation depends directly on the values stored in the relocation reg-
ister RR, because the integrity of physical memory depends on the integrity of RR.
Therefore, only the Supervisor should be able to read from (or write to) RR: users
should never have read or write access to RR.
Access to RR is protected by the mode bit M: the VM is able read or write RR
only when M is set to s (i.e., supervisor mode). Otherwise, any attempt to access RR
is trapped.

214
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.7 Simple processor with relocation register
As a concrete illustration, consider the instruction LDARR (i.e., load the accumu-
lator ACC with the contents of RR). Assume that the simple processor is extended in
the following ways, as shown in Figure 10.7:
• The relocation register RR uses the same data path as the ACC, PC, and MAR
registers with similar timing and control for LD and EN. The timing and control
unit has the mode bit M added to it.
• The contents of RR are always available both to the timing and control unit
and to the MAR.
• The MAR is enhanced so that the address it stores is the summation of the
inputs from the F BUS and the base value stored in RR.
For a LDARR instruction stored at relative address i, the following four steps must
be taken:
1. Convert the address i in PC to its read address base+i and check for a memory
trap. If the address is not trapped, then pass i through the ALU and set up the
LD control on MAR to load i off the F BUS at the beginning of the next cycle.
2. Increment i in PC. The control lines on PC are set to enable its output on to the
B BUS and load the contents of the F BUS at the start of the next cycle. ALU
FUNCTION is set to INC B.
3. The instruction LDARR at address i of PM is fetched and stored in IR. The
output of RR is enabled onto the B BUS, ALU FUNCTION is set to PASS B, and
the ACC input is set to load the ALU output at the beginning of the next cycle,
which is the contents of RR.

Virtual Machines and Memory Protection
215
Instructions
Stored in Program
Memory
Microcode Operations
Sequenced by Timing and Control Unit
i.
LDARR
1.
(((base+PC) ≥q) or (PC ≥bound)) →trap | (MAR ←base+PC)
2.
PC ←PC +1
3.
IR ←PM[MAR]
4.
M = u →trap | ACC ←RR
Table 10.4: LDARR: Reading the contents of relocation register
Microcode Operations
LD
EN
ALU
1.
(((base + PC) ≥q) or (PC ≥bound)) →trap |
(MAR ←base+PC)
MAR
PC
Pass B
2.
PC ←PC +1
PC
PC
Inc B
3.
IR ←PM[MAR]
IR
X
Pass A
4.
M = s →ACC ←RR | trap
ACC
RR
Pass B
Table 10.5: Timing and control unit operations for LDARR
4. If the mode bit M is set to u, then the instruction is trapped. Otherwise, the
contents of RR are loaded into ACC.
The details of the above operations are illustrated in Tables 10.4 and 10.5, as well as
in the timing diagram found in Figure 10.8.
There are two derived inference rules that specify the operation of LDARR:
LDARR trap
TCU controls (M says ⟨u⟩⊃VM says ⟨LDARR⟩⊃⟨trap⟩)
TCU says (M says ⟨u⟩⊃VM says ⟨LDARR⟩⊃⟨trap⟩)
M says ⟨u⟩
VM says ⟨LDARR⟩
⟨trap⟩
,
LDARR
TCU controls (M says ⟨s⟩⊃VM controls ⟨LDARR⟩)
TCU says (M says ⟨s⟩⊃VM controls ⟨LDARR⟩)
M says ⟨s⟩
VM says ⟨LDARR⟩
⟨LDARR⟩
.
The ﬁrst rule indicates that the instruction LDARR is trapped if the processor is in user
mode. The second rule indicates that LDARR is allowed to execute if the processor is
in supervisor mode.
A similar set of derived inference rules and implementation details exist for LDRR
@A (i.e., store the memory contents found in relative address A into RR). The details
are left as an exercise for the reader.
10.3.4
Setting the Mode Bit
The ﬁnal area to consider in the simple memory-segmentation design is the control
of the mode bit M. Improper control of the privileged state bit M could cause a user
program to be incorrectly labeled as the Supervisor. It is therefore essential to trap
any introduction by a user program that attempts to set M to supervisor mode.

216
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.8 Timing diagram for loading RR into ACC
As a concrete illustration, suppose we have an instruction SSM (set supervisor
mode), which is trapped if the processor is in user mode and otherwise executed.
The resulting rules are as follows:
SSM trap
TCU controls (M says ⟨u⟩⊃VM says ⟨SSM⟩⊃⟨trap⟩)
TCU says (M says ⟨u⟩⊃VM says ⟨SSM⟩⊃⟨trap⟩)
M says ⟨u⟩
VM says ⟨SSM⟩
⟨trap⟩
,
SSM
TCU controls (M says ⟨s⟩⊃VM controls ⟨SSM⟩
TCU says (M says ⟨s⟩⊃VM controls ⟨SSM⟩
M says ⟨s⟩
VM says ⟨SSM⟩
⟨SSM⟩
.
It turns out that the ability to set a processor’s operating mode to user mode should
also be limited to the Supervisor. In particular, user programs are restricted from any
knowledge of supervisory functions. Put another way, only the Supervisor should
be able to transfer control of a processor to a user program. This policy prevents a
user program from downgrading the Supervisor program to user mode and thereby
denying it access to supervisory functions.
For example, suppose we have an instruction SUM (set user mode). Letting ⟨SUM⟩
denote “it is a good idea to set the mode bit M to u (i.e., enter user mode),” we can

Virtual Machines and Memory Protection
217
identify the derived inference rules corresponding to SUM as follows:
SUM trap
TCU controls (M says ⟨u⟩⊃VM says ⟨SUM⟩⊃⟨trap⟩)
TCU says (M says ⟨u⟩⊃VM says ⟨SUM⟩⊃⟨trap⟩)
M says ⟨u⟩
VM says ⟨SUM⟩
⟨trap⟩
,
SUM
TCU controls (M says ⟨s⟩⊃VM controls ⟨SUM⟩)
TCU says (M says ⟨s⟩⊃VM controls ⟨SUM⟩)
M says ⟨s⟩
VM says ⟨SUM⟩
⟨SUM⟩
.
The implementation details in terms of microcode operations and timing diagrams
can be devised in a similar fashion as for previous machine instructions. These tasks
are left as exercises for the reader.
Exercise 10.3.1
Work out the inference rules for the operation LDRR @A (i.e.,
loading RR with the values found in relative address A in PM). Prove the validity of
the inference rules.
Exercise 10.3.2
Work out the table of timing and control unit operations for the
operation LDRR @A (similar to Tables 10.4 and 10.5). Draw the corresponding
timing diagram for LDRR @A.
Exercise 10.3.3
Work out the table of timing and control unit operations for the
operation SSM. Draw the corresponding timing diagram for SSM.
Exercise 10.3.4
Work out the table of timing and control unit operations for the
operation SUM. Draw the corresponding timing diagram for SUM.
Exercise 10.3.5
Suppose a new machine instruction is required of the simple pro-
cessor with relocation register RR, as shown in Figure 10.7. The new instruction is
LD @A @B (i.e., load the contents of virtual address B into virtual address A).
a. Show the microcode implementation of LD @A @B as a sequence of mi-
crocode instructions, similar to what is found in Table 10.4.
b. Devise and prove valid the access-control rules for LD @A @B.
10.4
Design of the Virtual Machine Monitor
We now take a more detailed look at the design of virtual machine monitors. An
instruction-set architecture is said to be virtualizable if it is possible to build a virtual
machine monitor that satisﬁes the following three objectives:

218
Access Control, Security, and Trust: A Logical Approach
1. The VMM creates an execution environment that is essentially identical to the
execution environment of the hardware being emulated.
2. The VMM efﬁciently executes instructions, so that program execution times
are not seriously degraded.
3. The VMM maintains control over all system resources, so that any attempt
by users to change the resources allocated to them will cause the VMM to
intervene.
The virtualization techniques we present in this section were originally developed
in the context of mainframe computers (such as the IBM 360) and continued into
the design of minicomputers (such as the Digital Equipment Corporation’s VAX
architecture). These machines had to support multiple users executing programs
on the same hardware. Today’s systems face the same problem, whereby (due to
widespread networking) multiple processes and potentially multiple users use the
same hardware.
It turns out that not all instruction-set architectures are virtualizable. Because
virtualization is a key security capability, it is important to understand the condi-
tions under which virtualization is possible and how a VMM is constructed under
such conditions. These details were worked out by Popek and Goldberg (Popek and
Goldberg, 1974).
Virtualization depends on identifying three types of instructions:
1. Privileged instructions: instructions that are trapped in user mode but not
trapped in supervisor mode.
2. Sensitive instructions: instructions whose behavior depends on the value of
relocation register RR or mode bit M (i.e., behavior that is location or mode
sensitive, or instructions that attempt to alter the allocation of resources, such
as memory).
3. Innocuous instructions: instructions that are neither privileged nor sensitive.
The key property for an instruction-set architecture to be virtualizable is that all
sensitive instructions must be privileged instructions. If this property holds, then
all sensitive instructions are automatically trapped by the VMM’s control program,
which is deliberately constructed to control any change in allocated resources and
to faithfully simulate the execution of privileged instructions. This construction ad-
dresses the ﬁrst and third objectives of VMMs: creation of an identical execution
environment to the hardware being emulated, and control over all resources allo-
cated to VMs. Innocuous instructions, which are neither privileged nor sensitive,
are passed on for direct execution by hardware. This approach satisﬁes the second
objective of VMMs, namely the efﬁcient execution of instructions.
In the discussion that follows, we assume that the speciﬁc instruction-set architec-
ture is virtualizable (i.e., all sensitive instructions are also privileged instructions).

Virtual Machines and Memory Protection
219
FIGURE 10.9 Top-level operation of virtual machine monitor
The VMM’s operation at the conceptual level is illustrated by Figure 10.9. Instruc-
tions that are not privileged (i.e., innocuous instructions) are passed unchanged di-
rectly to the hardware for execution. Instructions that are privileged are trapped and
handled by the control program CP. The three objectives for VMMs are met as fol-
lows:
1. Identical execution environment: The execution environment for innocuous
instructions is identical, because innocuous instructions are passed unchanged
to the hardware. Privileged instructions are trapped by the control program
CP. If CP accurately simulates the execution of instructions, then the VMM
will provide an identical execution environment.
2. Efﬁcient execution of instructions: Passing on innocuous instructions (instead
of simulating them) provides the same execution speed as executing them on
hardware.
3. Control over all system resources: Because all sensitive instructions that at-
tempt to change the allocation of system resources are trapped, the VMM is in
control.
We now focus on how trapped instructions are handled by the VMM. Conceptu-
ally, the control program CP has three main components:
1. A dispatcher D, which is the top-level module called when a trap occurs. The
value of the program status word PSW—in particular, the instruction register
IR and the program counter PC—enables D to decide which module to call,
based on the instruction that was trapped.

220
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.10 Graphic representation of control program for handling trapped in-
structions
2. An allocator A, which is called every time a privileged instruction attempts
to change resources associated with the computing environment, such as the
value of the relocation register RR. The main task of the allocator is to decide
what system resources are provided to the various programs running on the
machine.
3. A set V of interpreters, which contains a distinct interpreter routine vi for each
privileged instruction i. These interpreters simulate the effect of running the
instruction that was trapped. In practice, each interpreter might be nothing
more than a table that looks up the state of the machine when the instruction i
was called and returns the state the machine should be in after executing i.
Figure 10.10 illustrates the function of the control program CP. Each trapped in-
struction i is examined by the dispatcher D, which determines whether to invoke the
allocator A or a speciﬁc interpreter vi ∈V.
We now examine more closely how privileged, sensitive, and innocuous instruc-
tions are classiﬁed.
10.4.1
Privileged Instructions
Privileged instructions are those that are intended to be executed only in supervisor
mode: they always trap when executed in user mode, but never when executed in
supervisor mode (unless, of course, there is a memory trap due to out-of-bounds
addressing). An instruction i is said to be privileged if (in the absence of memory
traps) it exhibits the following behavior, for all states:
i(pm,acc, pc,mar,ir,m,(base,bound))
(
traps, if m = u
doesn’t trap, if m = s.

Virtual Machines and Memory Protection
221
Example 10.5
Loading the contents (base,bound) of the relocation register RR into the accumulator
ACC is a privileged instruction, which only the Supervisor program can execute.
Recall the deﬁnition of LDARR, which is expressed as the following two derived
inference rules:
LDARR trap
TCU controls (M says ⟨u⟩⊃VM says ⟨LDARR⟩⊃⟨trap⟩)
TCU says (M says ⟨u⟩⊃VM says ⟨LDARR⟩⊃⟨trap⟩)
M says ⟨u⟩
VM says ⟨LDARR⟩
⟨trap⟩
,
LDARR
TCU controls (M says ⟨s⟩⊃VM controls ⟨LDARR⟩)
TCU says (M says ⟨s⟩⊃VM controls ⟨LDARR⟩)
M says ⟨s⟩
VM says ⟨LDARR⟩
⟨LDARR⟩
.
From these rules we can see that, if M says ⟨u⟩, then LDARR is trapped. If M says ⟨s⟩,
then LDARR is executed. Thus, LDARR is a privileged instruction.
♦
10.4.2
Sensitive Instructions
Informally, there are two types of sensitive instructions:
1. Control-sensitive instructions, which attempt to change either the processor
mode (i.e., user or supervisor) or the memory resources allocated to the pro-
cessor (via the relocation register)
2. Behavior-sensitive instructions, whose behavior is affected by either the pro-
cessor mode or the physical location in real memory speciﬁed by the relocation
register
To deﬁne these notions more precisely, we must ﬁrst introduce two simple deﬁni-
tions.
Deﬁnition 10.1 Let S = (pm,acc, pc,mar,ir,m,(base,bound)) be a processor state.
• An instruction i is mode preserving on S if i(S) either address traps or has the
same mode bit as S (i.e., the value m).
• An instruction i is resource preserving on S if i(S) either address traps or has
the same (base,bound) value as S.
Intuitively, control-sensitive instructions are instructions that attempt to change
either the contents of the relocation register RR, the mode bit M, or both. That is, an
instruction i is control sensitive if there exists at least one state S such that i is not
mode preserving on S, not resource preserving on S, or both.

222
Access Control, Security, and Trust: A Logical Approach
Example 10.6
Consider the SUM (i.e., set user mode) instruction. Its behavior is deﬁned by the
following two derived inference rules:
SUM trap
TCU controls (M says ⟨u⟩⊃VM says ⟨SUM⟩⊃⟨trap⟩)
TCU says (M says ⟨u⟩⊃VM says ⟨SUM⟩⊃⟨trap⟩)
M says ⟨u⟩
VM says ⟨SUM⟩
⟨trap⟩
,
SUM
TCU controls (M says ⟨s⟩⊃VM controls ⟨SUM⟩)
TCU says (M says ⟨s⟩⊃VM controls ⟨SUM⟩)
M says ⟨s⟩
VM says ⟨SUM⟩
⟨SUM⟩
.
From these rules, we can see that SUM is a privileged instruction that is trapped
in user mode but not in supervisor mode. However, the second rule states that an
attempt to execute SUM in supervisor mode should be granted: the result of the
instruction would be to change the mode bit from s to u. As a result, the instruction
SUM is also control sensitive.
♦
In contrast, behavior-sensitive instructions are either location sensitive (i.e., de-
pend on the value of the relocation register) or mode sensitive (i.e., depend on the
value of the mode bit). Recall that a virtual-machine program accesses memory
through the use of virtual addresses. In theory, a program’s behavior should be in-
dependent of the physical locations in which its segment resides. Speciﬁcally, its
behavior should be governed by what is stored within the memory segment it can
access (whose boundaries are deﬁned by RR), what is stored in location PM[1] be-
cause of the way we deﬁned traps, and the value stored in the program status word
PSW, namely the values stored in ACC, PC, MAR, IR, M, and RR. What is stored in
other segments should not affect the behavior of the processor under the control of
the segment referenced by (base, bound) in RR.
We therefore introduce the following notion of equivalence, which equates states
whose contents of accessible memory segments are identical (independent of their
physical locations in memory).
Deﬁnition 10.2 Let x be an integer. Two states S1 and S2 are equivalent modulo x
(written S1 ≈x S2) if and only if there exist values pm1, pm2,acc, pc,mar,ir,m,base,
and bound such that the following conditions all hold:
• S1 = (pm1,acc, pc,mar,ir,m,(base,bound)),
• S2 = (pm2,acc, pc,mar,ir,m,(base+x,bound)),
• pm1[1] = pm2[1],
• For all j such that base ≤j < base+bound, pm1[j] = pm2[j +x].

Virtual Machines and Memory Protection
223
Intuitively, when S1 ≈x S2 (for whatever value of x), S1 and S2 differ only in the
physical locations of their segments and in the contents of memory outside of their
respective segments. In particular, they share the same values for the ACC, PC,
MAR, IR, and M registers; in the contents of memory location PM[1]; and in the
logical locations associated with their segments.
With this deﬁnition in hand, we can deﬁne behavior sensitivity precisely. An in-
struction i is behavior sensitive if there exists states S1 and S2 such that the following
conditions all hold:
• S1 ≈x S2,
• i is mode preserving and resource preserving on both S1 and S2,
• i does not address trap on S1 or S2,
• i(S1) ̸≈x i(S2).
Intuitively, i is behavior sensitive when it fails to behave similarly on two equivalent
states.
Example 10.7
Consider the instruction LDARR, which loads the value of RR into ACC if the pro-
cessor is in supervisor mode and traps if the processor is in user mode. (Notice that
this behavior makes LDARR a privileged instruction.)
Consider the following two states, where S1 ≈x S2 for some x ̸= 0:
S1 = (pm1,acc, pc,mar,ir,s,(base,bound)),
S2 = (pm2,acc, pc,mar,ir,s,(base+x,bound)).
Because the values of the mode bits are both s, executing LDARR from either state
will load the relevant RR value into the accumulator. Thus, the accumulator value for
i(S1) will be (base,bound), whereas the accumulator value for i(S2) will be (base+
x,bound). It follows that i(S1) ̸≈x i(S2), and hence LDARR is behavior sensitive.
♦
10.4.3
Virtualizable Processor Architectures
A key result of Popek and Goldberg’s work (Popek and Goldberg, 1974) is that a
virtual machine monitor can be constructed for processors that have relocation mech-
anisms, user and supervisor modes, and traps, if the set of sensitive instructions is a
subset of the set of privileged instructions. In other words, an instruction-set archi-
tecture is virtualizable if every sensitive instruction is also a privileged instruction.
The proof of this property can be found in the original paper.
Exercise 10.4.1
Consider the machine instructions of the simple processor de-
scribed in Chapter 9—A Primer on Computer Hardware.

224
Access Control, Security, and Trust: A Logical Approach
a. Classify each of the following instructions as privileged, control sensitive, be-
havior sensitive, or innocuous:
LDA @A
LDA @A @B
ADD @B
STO @B
LDARR
SSM
SUM
LDRR @A
b. Assume that the above list of machine instructions fully enumerates the in-
struction set of the processor. Is the processor virtualizable? Justify your
answer.
Exercise 10.4.2
Suppose the processor state for a program in a segment described
by (base,bound) = (256,1024) is given by
S = (pm,acc,0,mar,ir,m,(256,1024)).
Answer the following questions.
a. What locations in physical memory does the active memory segment occupy?
b. Suppose the active segment is to be relocated to base address = 128 in physical
memory and executed. This new state is S′:
S′ = (pm,acc,0,mar,ir,m,(128,1024)).
What must be true for the program in the relocated segment to behave exactly
like the program in the original segment, where “behave exactly” means all
PSW values in the relocated segment will match for each execution state with
the exception of base = 256 and base′ = 128? Justify your answer.
10.5
Summary
The goal of this chapter was to introduce the fundamentals of memory protection,
virtual machines, and virtual machine monitors. Central to protecting memory is
segmentation as supported by a relocation register and a mode bit to protect the
relocation register.
Central to virtual machine monitors is the control program that consists of a dis-
patcher, resource allocator, and interpreters of privileged instructions. The equiv-
alence of machines with and without a virtual machine monitor was demonstrated

Virtual Machines and Memory Protection
225
using a mapping function that mapped states of one machine to equivalent states in
the other. A key result is that those instruction-set architectures where all sensitive
instructions are also privileged instructions are virtualizable.
The learning outcomes associated with this chapter appear in Figure 10.11.
10.6
Further Reading
Saltzer and Schroeder’s paper, The Protection of Information in Computer Systems
(Saltzer and Schroeder, 1975), is a classic paper on memory protection mechanisms.
The protection methods described in this chapter and following chapters are devel-
oped based on this paper. Popek and Goldberg’s paper Formal Requirements for
Virtualizable Third Generation Architectures (Popek and Goldberg, 1974) is a clas-
sic paper on virtual machine monitors. Its ﬁndings are still relevant in current virtual
machines.

226
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.11 Learning outcomes for Chapter 10
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
• When given a block diagram of a processor using memory segmentation,
describe how virtual memory addresses are related to physical memory
addresses.
Application
• When given a block diagram of a processor and control lines to registers
and arithmetic logic units, interpret timing diagrams describing processor
behavior.
Analysis
• When given the value of the program status word and an instruction,
determine if the instruction will be address trapped or passed on for exe-
cution.
• When given the deﬁnition of an instruction, be able to determine if it is
innocuous, privileged, behavior sensitive, or control sensitive.
• When given a request to access a virtual (logical) address, determine if
accessing the address is permitted.
Synthesis
• When given a description of a machine-level instruction and a block di-
agram of a processor with its control lines, devise microprograms that
implement the machine-level instruction.
• When given a description of a machine-level instruction, be able to de-
scribe when the operation is performed or trapped through the use of
derived inference rules.

Chapter 11
Access Control Using Descriptors
and Capabilities
In Chapter 10, we introduced a simple memory-segmentation scheme based on a
single relocation register guarded by a mode bit. Under this scheme, each virtual
machine has access to precisely one memory segment, whose location is determined
by the base-address value base stored in the relocation register RR. The relocation
register thus serves as a single mechanism to support both segment addressing and
memory protection.
In this chapter, we introduce a more sophisticated and ﬂexible scheme, known as a
capability system. This scheme separates the notions of addressing and protection by
distinguishing address descriptors (which serve as pointers to memory segments in
physical memory) from capabilities (unforgeable tickets that specify permitted oper-
ations on a given memory segment). In this scheme, each machine is given a catalog
(i.e., collection) of capabilities that together deﬁne its privileges to access various
memory segments. As a consequence, a single virtual machine may be granted a va-
riety of access privileges (e.g., read, write, or execute) to multiple memory segments.
11.1
Address Descriptors and Capabilities
When writing programs, programmers typically do not know the precise location
of memory segments. Thus, it is convenient to refer to memory segments by name
or by a unique identiﬁer, as opposed to a speciﬁc (base,bound) value. Address
descriptors provide the means to associate segment names (or unique identiﬁers)
with physical locations in memory. In turn, capabilities bind access rights (such as
read or write) to object identiﬁers or names. In the case of segments, this binding
is made possible by appending read and write permissions to segment names and
storing the results in capability registers.
Figure 11.1 shows how address descriptors and capabilities ﬁt together. Each ad-
dress descriptor has the form (Ns,(baseNs,boundNs)), where Ns is a segment name
(i.e., a unique identiﬁer) and (baseNs,boundNs) point to the physical location of that
segment in memory. Taken together, these descriptors form a memory map that is
stored in the address descriptor segment (ADS). To provide protection, the ADS
must be under supervisory control, inaccessible to users, and trustworthy.
227

228
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.1 Memory protection with address descriptors and capabilities
Figure 11.1 also includes a processor with two capability registers (CR[1] and
CR[2]), which are both protected by mode bit M. Each capability register holds a
capability, which comprises a read bit (R), a write bit (W), and a unique segment
id. The value of the read (respectively, write) bit indicates whether or not read (re-
spectively, write) access to the given segment is permitted.1 In this ﬁgure, CR[1]
contains a capability that points to the address descriptor for Alice’s program seg-
ment; in contrast, CR[2] contains a capability that points to the address descriptor
for a shared-data segment.
We can express in the logic the contents of the ADS and of the capability registers
in the same way that we previously expressed the contents of other registers: that is,
we can treat ADS, CR[1], and CR[2] as principals who make statements about their
values. For example, the formula
ADS says (⟨(N1,(baseN1,boundN1))⟩∧··· ∧⟨(Nn,(baseNn,boundNn))⟩)
expresses that there are n address descriptors stored in the ADS, with the jth descrip-
tor associating the segment name Nj with the physical locations given by the pair
(baseNj,boundNj). In a similar vein, Table 11.1 provides the possible interpretations
for a given capability stored in capability register CR[i].
1In many systems, there will also be an execute bit (X), which governs the ability to execute a segment. To
keep our discussion and diagrams simpler, we adopt the convention that read access also provides execute
access. Alternatively, one could incorporate the additional bit into capabilities and the capability registers.

Access Control Using Descriptors and Capabilities
229
Contents of CR[i]
Interpretation
(read,write,Ns)
CR[i] says (⟨read,Ns⟩∧⟨write,Ns⟩)
(read,not write,Ns)
CR[i] says (⟨read,Ns⟩∧¬⟨write,Ns⟩)
(not read,write,Ns)
CR[i] says (¬⟨read,Ns⟩∧⟨write,Ns⟩)
(not read,not write,Ns)
CR[i] says (¬⟨read,Ns⟩∧¬⟨write,Ns⟩)
Table 11.1: Interpretation of capability-register values
Example 11.1
Suppose capability register CR[1] has the value (read,not write,Math Program).
The contents of CR[1] can be expressed in the logic as follows:
CR[1] says (⟨read,Math Program⟩∧¬⟨write,Math Program⟩).
The process possessing the capability in CR[1] is able to read (and therefore also
execute) the shared math program, but the process is not allowed to modify it.
♦
Instructions previously deﬁned using the simple memory protection scheme of
Chapter 10 must be redeﬁned to incorporate the use of capabilities and address de-
scriptors. At a minimum, the new deﬁnitions must accommodate the use of capabil-
ity registers and the introduction of segment names Ns into virtual addresses. The
following example shows how the instruction STO (which stores the contents of the
ACC into a particular memory address) is altered to accommodate the new protection
scheme.
Example 11.2
Consider the instruction STO @(Ns,A), which stores the contents of the accumulator
ACC into address A of segment Ns. This instruction should be allowed to execute
only when the following conditions are all satisﬁed (otherwise, a trap should occur):
• One of the capability registers CR[i] possesses a write capability for the seg-
ment Ns:
CR[i] says ⟨write,Ns⟩.
• The ADS contains an address descriptor for Ns that associates it with some
particular physical segment (baseNs,boundNs):
ADS says (Ns,(baseNs,boundNs)).
• The virtual address A is within bounds of the segment Ns and thus will not
trigger an address trap:
(A +baseNs) < q,
A < boundNs.

230
Access Control, Security, and Trust: A Logical Approach
These conditions are collectively summarized by the following derived inference
rule:
STO
@(Ns,A)
Supervisor controls (CR[i] says ⟨(writeNs,Ns)⟩⊃ADS says ⟨(Ns,(baseNs,boundNs))⟩⊃
(A +baseNs < q) ⊃(A < boundNs) ⊃VM controls ⟨STO @(Ns,A)⟩)
Supervisor says (CR[i] says ⟨(writeNs,Ns)⟩⊃ADS says ⟨(Ns,(baseNs,boundNs))⟩⊃
(A +baseNs < q) ⊃(A < boundNs) ⊃VM controls ⟨STO @(Ns,A)⟩)
VM says ⟨STO @(Ns,A)⟩
CR[i] says ⟨(writeNs,Ns)⟩
ADS says ⟨(Ns,(baseNs,boundNs))⟩
A +baseNs < q
A < boundNs
⟨STO @(Ns,A)⟩
.
♦
The memory protection scheme shown in Figure 11.1 protects the contents of the
two capability registers CR[1] and CR[2] using the mode bit M. In particular, the
capability registers CR[i] are only accessible if the processor is in supervisor mode.
The following example illustrates how this protection works.
Example 11.3
Consider the instruction LDACR[i], which loads the ACC with the contents of CR[i],
provided that the virtual machine is in supervisor mode; when the virtual machine
is in user mode, the instruction is trapped. (This instruction is the capability-register
analogue of the LDARR instruction introduced in Chapter 10.)
The following derived inference rules deﬁne the behavior of LDACR[i]:
LDACR[i]
Supervisor controls (M says ⟨s⟩⊃VM controls ⟨LDACR[i]⟩)
Supervisor says (M says ⟨s⟩⊃VM controls ⟨LDACR[i]⟩)
M says ⟨s⟩
VM says ⟨LDACR[i]⟩
⟨LDACR[i]⟩
,
LDACR[i] trap
Supervisor controls (M says ⟨u⟩⊃VM says ⟨LDACR[i]⟩⊃⟨trap⟩)
Supervisor says (M says ⟨u⟩⊃VM says ⟨LDACR[i]⟩⊃⟨trap⟩)
M says ⟨u⟩
VM says ⟨LDACR[i]⟩
⟨trap⟩
.
In particular, LDACR[i] is executed only when the mode bit M indicates that the VM
is in supervisor mode. If M indicates that the VM is in user mode, then a trap occurs
instead.
♦
In the next section, we look at an alternative way to protect the capability regis-
ters, without using a mode bit. This alternative—known as a tagged architecture—
permits user programs to load the capability registers, but only with values that are
known to be authorized descriptor values.
Exercise 11.1.1
Recall the LDA @A instruction in Chapter 10, which loads the

accumulator ACC with the contents of PM[base + A]]. Revise the inference rules
deﬁning the behavior of LDA @A (LDA memory trap 1, LDA memory trap 1, and
LDA @A), using address descriptors and capabilities.

Access Control Using Descriptors and Capabilities
231
Exercise 11.1.2
Consider the instruction STOCR[i], which loads the contents of the
accumulator ACC into capability register CR[i] provided the virtual machine is in
supervisor mode. Letting ⟨STOCR[i]⟩denote that the ACC contents should be loaded
into CR[i], propose and formally prove a derived inference rule for STOCR[i].
11.2
Tagged Architectures
The security of segmented memory depends on the integrity of relocation registers
and capability registers: if users are able to load these registers with arbitrary or
unauthorized permissions, then the security and integrity of physical and program
memory are lost. There are at least two ways to achieve integrity of these registers.
The ﬁrst approach is to grant access to capability registers only to those principals
trusted to create legitimate capabilities (e.g., the Supervisor). This approach can
be accommodated through the use of mode bits that indicate when the processor is
under supervisory control. To maintain the integrity of the capability registers, user
processes are denied access to the registers; only the Supervisor is granted access
to the capability registers. While secure, this approach is inﬂexible. For example,
suppose that there are two capability registers, but more than two memory segments
that a particular user’s program is allowed to access. If users lack the ability to load
capability values into capability registers, then they can access only the two memory
segments that the Supervisor loaded into the capability registers. Such a situation is
both limiting and inconvenient.
The second (and more ﬂexible) approach is to limit the values that can be loaded
into capability registers, rather than to limit the principals who can alter the ca-
pability registers. This approach can be accommodated through the use of tagged
architectures. Essentially, the contents of memory are classiﬁed into one of two
types: capabilities (i.e., permissions) and non-capabilities (i.e., data). The type of
information stored at any memory location is indicated by a tag bit, which is part of
the word stored at any memory location.
To be explicit, tagged architectures do not necessarily eliminate the need for mode
bits. Although the mode bits are not used for determining whether to grant access to
capability registers, they are still useful for determining whether or not the Supervi-
sor is in control of the processor.
In tagged architectures, each memory location has an associated tag bit that is
either on or off. Thus, each memory location PM[(Ns, A)] stores a pair comprising
both a value and a tag. When the tag bit is on, the value in address A of segment Ns is
interpreted as a capability. When the tag bit is off, the value of address A in segment
Ns is interpreted as ordinary data and cannot be used as a capability. Table 11.2
shows how the values of these pairs can be interpreted in the logic.
The mandatory access control policy for reading from and writing to memory and
capability registers CR[i] is as follows:

232
Access Control, Security, and Trust: A Logical Approach
Contents of PM[(Ns, A)]
Interpretation
(value,off)
PM[(Ns, A)] says (⟨value⟩∧⟨data⟩)
(value,on)
PM[(Ns, A)] says (⟨value⟩∧⟨capability⟩)
Table 11.2: Interpretation of tag bit
1. Instructions that load a capability register with the value stored in a partic-
ular memory location are executed only if the tag bit in that location is on.
Otherwise, the instruction is trapped.
2. Instructions that store values from capability registers into memory are permit-
ted provided that there are no address traps and the virtual machine possesses
write permission on the relevant memory segment. The corresponding tag bit
is turned on as a result of storing a capability value.
3. All other memory store instructions turn the tag bit off.
The following example illustrates the mandatory access control policy for an in-
struction that attempts to update a capability register.
Example 11.4
Consider the instruction LDCR(i,@(Ns, A)) that attempts to load CR[i] with the value
in address A in segment Ns. The operation is successful provided that address A does
not address trap, the associated tag bit indicates the value is a capability value, and
the virtual machine has read permission on Ns.
Let ⟨LDCR (i,@(Ns,a))⟩denote that the operation should be permitted. The fol-
lowing inference rule describes the conditions under which the operation is allowed:
LDCR
(i,@(Ns, A))
Supervisor controls (CR[j] says ⟨readNs⟩⊃PM[(Ns, A)] says ⟨capability⟩⊃
ADS says ⟨(Ns,(baseNs,boundNs))⟩⊃A +baseNs < q ⊃A < boundNs ⊃
VM controls ⟨LDCR (i,@(Ns,a))⟩)
Supervisor says (CR[j] says ⟨readNs⟩⊃PM[(Ns, A)] says ⟨capability⟩⊃
ADS says ⟨(Ns,(baseNs,boundNs))⟩⊃A +baseNs < q ⊃A < boundNs ⊃
VM controls ⟨LDCR (i,@(Ns,a))⟩)
VM says ⟨LDCR (i,@(Ns,a))⟩
CR[i] says ⟨readNs⟩
PM[(Ns, A)] says ⟨capability⟩
ADS says ⟨(Ns,(baseNs,boundNs))⟩
A +baseNs < q
A < boundNs
⟨LDCR (i,@(Ns,a))⟩
♦
In the next section, we show how tagged architectures are used to support capabil-
ity systems, which provide secure and ﬂexible sharing of data and programs among
users.

Access Control Using Descriptors and Capabilities
233
Exercise 11.2.1
Propose derived inference rules for LDCR (i,@(Ns,a)) that indi-
cate the conditions under which the LDCR instruction will trap. Provide an informal
argument justifying your inference rules and formally derive them.
Exercise 11.2.2
Consider the instruction STOCR (i,@(Ns,A)), which attempts to
store the contents of capability register CR[i] into address A of segment Ns. This
operation should be allowed, provided address A does not cause an address trap and
the virtual machine has write permission on Ns; otherwise, the instruction is trapped.
Let ⟨STOCR (i,@(Ns,A))⟩denote that the operation should be permitted. Propose
and formally prove a derived inference rule called STOCR (i,@(Ns,A)) that formally
deﬁnes the behavior of STOCR when there is no address trap and the user has write
permission on Ns.
Exercise 11.2.3
As we have seen, tagged architectures depend on the trustworthy
segregation of capability values from ordinary data. Identify the crucial elements of
micro-coded timing and control operations necessary for any implementation of a
tagged architecture to be trustworthy.
11.3
Capability Systems
The concepts of descriptors, capabilities, and tagged architectures combine to sup-
port capability systems. Capability systems are organized around the principle that
access to protected objects and services is granted using capabilities, which incorpo-
rate descriptors that refer to the protected objects and services. Because they incor-
porate tags, capability systems can grant user programs the ability to load and store
capabilities to and from the capability registers.
11.3.1
Catalogs
Capability systems are more ﬂexible than the simple segmented memory system
described in Chapter 10, which used a mode bit for protecting descriptors. The ability
of user programs to read and store valid capabilities allows the use of catalogs of
capabilities, which simpliﬁes the task of securely sharing memory.
To see how catalogs work in capability systems, consider the processor and phys-
ical memory shown in Figure 11.2, which shows a processor with four capability
registers (CR[1] through CR[4]). The contents of these registers are as follows:
• CR[1] contains a capability (read,write,Alice’s Program), which gives read
and write access to the segment in physical memory that is Alice’s program.
• CR[2] contains a capability (read,write,Alice’s Catalog), which gives both
read and write access to Alice’s catalog of capabilities.

234
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.2 Catalogs and capabilities
• CR[3] contains capability C2, which is a capability in Alice’s catalog that
allows Alice to read to or write from the Shared Data segment she shares with
Bob.
• CR[4] contains capability C4, which is a capability in Alice’s catalog that gives
her read access to the Math Program segment, which she also shares with Bob.
The capabilities in Figure 11.2 serve as both pointers and privileges to memory
segments that contain data and/or programs. Because the capabilities exist within
the context of a tagged architecture, they can be read from memory into capability
registers and written from capability registers into memory. For each user’s virtual
machine, the Supervisor provides a memory segment (i.e., the user’s catalog) that
contains the capabilities that the user is authorized to use.
The following example illustrates the use of catalogs to provide ﬂexible storage
and use of user capabilities.
Example 11.5
Consider the situation shown in Figure 11.2. Suppose we wish to have the contents
of CR[1] point to the math program, while still preserving the processor’s access to
Alice’s program. We can see that the math-program capability C4 is in Alice’s capa-
bility catalog; the capability for Alice’s program is not. For simplicity, let us refer to
the capability for Alice’s program as C6 and the virtual address of any capability Ci
as addrCi.

Access Control Using Descriptors and Capabilities
235
The contents of Alice’s catalog are as follows:
PM[(Alice’s Catalog,addrC2)] = (read,write,Shared Data),
PM[(Alice’s Catalog,addrC3)] = (read,write,Alice’s Data),
PM[(Alice’s Catalog,addrC4)] = (read,not write,Math Program).
If C4 is loaded immediately into the register CR[1], then the processor will lose
the capability to access Alice’s program. Hence, two steps are necessary: the ﬁrst
stores C6 into addrC6 of Alice’s Catalog, and the second loads C4 into CR[1]. In
this way, access to Alice’s program can be maintained through Alice’s catalog.
♦
11.3.2
Creating New Segments
Recall from Chapter 10 that one of the requirements for virtualizability is that all
security-sensitive instructions are trapped and executed under supervisory control. In
particular, any attempt to alter the allocation of memory must be done under control
of the Supervisor. Therefore, user requests for additional memory resources in the
form of new memory segments must be handled by the Supervisor control program,
because they reﬂect requests to change the resources allocated to users.
To illustrate how new memory segments could be created in ways that support
virtualization, suppose that the instruction CREATESEG Ns is employed by users to
create a memory segment Ns. When invoked by a user, CREATESEG Ns is trapped,
and control is turned over to the control program’s resource allocation module.
When CREATESEG Ns is trapped, the program status word PSW—which includes
the contents of the instruction register IR containing CREATESEG Ns—is stored in
PM[0]. In this way, the control program’s dispatcher module can detect that the
user is attempting to create a new memory segment with the name Ns and then pass
control to the allocation module.
The allocation module then decides whether or not to honor the request CREATE-
SEG Ns. If the allocation module decides to honor the request and Ns is not already
being used as a segment name in the ADS, then the allocation module assigns re-
sources to the user by (1) allocating a memory segment (baseNs,boundNs) for Ns, (2)
placing the address descriptor (Ns,(baseNs,boundNs)) into the ADS at some loca-
tion addrNs, and (3) writing the capability (read,write,Ns) into one of the capability
registers.
The above process can be performed with either hardware or microcode support
by another instruction MKSEG (i,Ns,addrNs,read,write,(baseNs,boundNs)). MKSEG
should only be executed by a processor in supervisor mode, and only when the new
segment is within the bounds of physical memory.
Figure 11.3 contains three inference rules, which collectively deﬁne the operation
of MKSEG. The ﬁrst rule (MKSEG user trap) states that MKSEG traps when called
from a virtual machine in user mode. The second rule (MKSEG address trap) states
that MKSEG also traps when the base and bound address parameters of segment Ns

236
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.3 Inference rules for MKSEG instruction
MKSEG
user
trap
Supervisor controls (M says ⟨u⟩⊃
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩⊃⟨trap⟩)
Supervisor says (M says ⟨u⟩⊃
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩⊃⟨trap⟩)
M says ⟨u⟩
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩
⟨trap⟩
MKSEG
address
trap
Supervisor controls (baseNs +boundNs ≥q ⊃
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩⊃⟨trap⟩)
Supervisor says (baseNs +boundNs ≥q ⊃
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩⊃⟨trap⟩)
baseNs +boundNs ≥q
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩
⟨trap⟩
MKSEG
Supervisor controls (M says ⟨s⟩⊃baseNs +boundNs < q ⊃
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩⊃
⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩)
Supervisor says (M says ⟨s⟩⊃baseNs +boundNs < q ⊃
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩⊃
⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩)
M says ⟨s⟩
baseNs +boundNs < q
VM says ⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩
⟨MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))⟩
exceed the size of physical memory q. The third rule (MKSEG) captures the non-
trapping behavior of the instruction: a new memory segment is formed when the
virtual machine is in supervisor mode and segment Ns falls within the bounds of
physical memory (i.e., baseNs +boundNs < q).
The following example illustrates how a user’s request to create a new memory
segment Ns is handled.
Example 11.6
Suppose Alice wishes to create a new memory segment Ns over which she will have
read and write access. Let us assume that, in this particular capability system, the
capability (read,write,Ns) will always be placed in CR[3]—the thinking here is that
CR[1] holds the capability for Alice’s program and CR[2] holds the capability for
Alice’s capability catalog.
The following sequence of events creates Alice’s new segment:
1. Alice requests creation of a new memory segment Ns via the instruction CRE-
ATESEG Ns.

Access Control Using Descriptors and Capabilities
237
2. CREATESEG Ns is trapped, and control is passed to the Supervisor control pro-
gram.
3. The resource allocation module of the control program decides whether or not
to grant the request. If the request is granted, then memory is allocated for
the new segment Ns: that is, (baseNs,boundNs) is set, and the control program
executes the instruction MKSEG (i,Ns,addrNs,read,write,(baseNs,boundNs)).
4. The address descriptor (Ns,(baseNs,boundNs)) is added to the address descrip-
tor segment ADS, and the capability (read,write,Ns) is placed into CR[3].
5. Control of the processor is returned to Alice by restoring the values of the
PSW (except for CR[3]).
♦
11.3.3
Dynamic Sharing
The description of catalogs presented so far support only static sharing: the autho-
rizations for memory segments are determined prior to program execution. In many
cases, however, it is desirable to support dynamic sharing, whereby users may share
segments that are created during a program’s execution. As we shall see, dynamic
sharing of memory segments requires trusted communication of capabilities among
principals.
For example, recall Example 11.6, where Alice created a new memory segment
(let us call it SharedAlice). As a result of this action, she possesses the capability
(read,write,SharedAlice).
Now, suppose that Alice wishes to make this newly created memory segment ac-
cessible to Bob as well. Because users can copy capabilities, granting access to
SharedAlice is possible by creating a set of circumstances whereby Bob can both (1)
copy the capability (read,write,SharedAlice) and (2) know and trust that SharedAlice
is the named segment intended for him to share with Alice. There are at least two ap-
proaches for accomplishing this task. Both are based on the idea of communication
segments, which are memory segments set up to exchange capabilities among users.
The ﬁrst approach, which is illustrated in Figure 11.4, uses two communication
segments for each pair of users. When Alice wishes to share a capability with Bob,
she writes the capability into the segment Alice2Bob, to which she has read and write
access. Bob—who has read access to Alice2Bob—can then copy the capability into
one of his capability registers or to a memory location to which he has write access.
Bob will believe that the capability came from Alice if he believes that Alice is
the only user (other than the trusted Supervisor) who can write into the Alice2Bob
segment. That is, Bob must believe the following:
Alice2Bob ⇒Alice.

238
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.4 Pairwise shared communications segments
FIGURE 11.5 Formal analysis for pairwise shared communication segment
1. Alice2Bob ⇒Alice
Trust assumption
2. Alice controls ϕcapability
Alice’s jurisdiction over her capabilities
3. Alice2Bob says ϕcapability
Capability in Alice2Bob
4. Alice says ϕcapability
1, 3 Derived speaks for
5. ϕcapability
2, 4 Controls
If Bob believes that the segment Alice2Bob is in fact under Alice’s sole control,
then the analysis of the situation from Bob’s perspective can be summarized as in
Figure 11.5, where ϕcapability is the expression that represents the capability Alice
passed to Bob.
The downside to this approach is that it is relatively expensive in terms of commu-
nication segments: if there are N users, then each user requires N −1 communication
segments (i.e., one for each other user), resulting in a total of N ×(N −1) communi-
cation segments.
A more economical approach is illustrated in Figure 11.6. This approach uses a
single communication segment for each user, resulting in a need for only N com-
munication segments for N users. Each user’s communication segment serves as a
mailbox: when users wish to send capabilities to other users, they ask the Supervisor
to place the capabilities in the recipient’s mailbox on their behalf. When doing so,
the Supervisor indicates the sender of each capability.
To understand the idea behind this approach, suppose that Alice sends a capability
(read,write,SharedAlice) to Bob via the Supervisor: the Supervisor places something
like (Alice,(read,write,SharedAlice)) into Bob’s mailbox ForBob. From Bob’s per-
spective, this state of affairs can be expressed in the logic as follows, where ϕcapability
is the expression that represents the passed capability:
ForBob | Alice says ϕcapability.
That is, Bob’s mailbox ForBob asserts that Alice sent the capability. Because the
Supervisor is in control of Bob’s mailbox, Bob believes that anything in his mailbox

Access Control Using Descriptors and Capabilities
239
FIGURE 11.6 Mailbox segments
FIGURE 11.7 Formal analysis for mailbox communication segment
1. Supervisor | Alice ⇒Alice
Trust assumption
2. Alice controls ϕcapability
Alice’s jurisdiction over her capabilities
3. ForBob | Alice says ϕcapability
Capability in Bob’s mailbox
4. ForBob ⇒Supervisor
Trust assumption
5. Alice ⇒Alice
Idempotency of ⇒
6. ForBob | Alice ⇒Supervisor | Alice
4, 5 Monotonicity of ⇒
7. Supervisor | Alice says ϕcapability
6, 3 Derived Speaks For
8. Alice says ϕcapability
1, 7 Derived speaks for
9. ϕcapability
2, 8 Controls
is placed there by the Supervisor:
ForBob ⇒Supervisor.
From the idempotency and monotonicity of ⇒, Bob can conclude that
ForBob | Alice ⇒Supervisor | Alice.
Bob relies upon the trust assumption that the Supervisor correctly authenticates mes-
sages from each user (in this case, Alice). This assumption is expressed as
Supervisor | Alice ⇒Alice.
The complete formal justiﬁcation for Bob’s interpretation of the situation appears in
Figure 11.7. This justiﬁcation depends upon two essential trust assumptions that Bob
must make: (1) that only the Supervisor can place items in the mailbox segments,
and (2) that the Supervisor is trustworthy.
11.3.4
Revocation of Capabilities
As we have seen, capability systems allow users to load, store, and share capabili-
ties. However, it is sometimes necessary to revoke previously granted privileges. As

240
Access Control, Security, and Trust: A Logical Approach
we shall see, capability revocation can be difﬁcult, unless restrictions are imposed
on how capabilities are stored or shared.
For example, consider what happens when Alice sends Bob a capability to access
one of her segments. After receiving the capability, Bob can load it into one of his
capability registers and then freely store it in any location in memory that he can
access. In particular, he can store it in an arbitrary number of arbitrary locations—
even the Supervisor will not necessarily know how many copies exist and where they
are located.
If Alice later decides to revoke Bob’s access to her segment, then the system must
be able to invalidate all of Bob’s copies of the capability. To accomplish this task,
either (1) the Supervisor must perform an exhaustive search of all memory accessible
to Bob, or (2) Alice must move the contents of the segment she shared with Bob to
a new segment, destroy the original segment, and issue capabilities to the new one.
Both options are unattractive from an efﬁciency perspective.
To reduce the task of locating revoked capabilities, we can institute a mandatory
access-control policy that restricts capability storing to a relatively small number
of known locations. For example, each user might be restricted to having just one
segment for storing capabilities. If the policy is enforced without exception, then
searching for revoked capabilities is greatly simpliﬁed: only a small number of loca-
tions must be checked.
Another approach is to use indirect references to intermediate objects. For exam-
ple, if Alice wishes to grant Bob access to segment A, she creates a new object B that
points to A, and gives Bob a capability to access B. She follows a similar approach
for each principal to whom she wishes to grant access to A; in each case, she creates
a different intermediate object. If Alice wishes to revoke Bob’s capability to access
A, she simply removes B, thus destroying Bob’s link to A. All other principals with
capabilities to access A still retain their privileges.
A third approach is to limit which capabilities can be copied. The simplest way
to accomplish this task is to include a copy bit with each capability. When a user
program attempts to store a capability from the capability register into memory, the
capability’s copy bit is checked. If the copy bit is on, then the attempt succeeds
and the capability is stored; if the copy bit is off, then the attempt is trapped and
blocked. This type of use of a copy bit amounts to an all-or-nothing approach: more
sophisticated schemes can be developed to permit a capability to be copied a speciﬁc
number of times.
Exercise 11.3.1
Figure 11.2 illustrated the use of descriptors as capabilities while

omitting the use of address descriptors for simplicity. Redraw the ﬁgure explicitly
showing the application of address descriptors.
Exercise 11.3.2
Consider the following speciﬁcations for a small system:
• Ali and Boris run separate programs to which only they as program owners
have access.

Access Control Using Descriptors and Capabilities
241
• Ali trusts Boris to be careful about not overwriting her data. She wishes to
share data with Boris that she creates, and she needs to access data that Boris
creates for her.
• Ali has mistakenly overwritten Boris’s data in the past, so he is more careful
about granting write access to Ali. He needs to share data that he creates for
her, and he needs to see data that Ali creates for him.
• Ali has private data that only she should see.
• Ali has data that she needs to share only with Clancy.
• Ali, Boris, and Clancy need to read a common database under the control of
the Supervisor.
Devise a collection of capabilities that satisfy the above, and distribute the capabil-
ities into catalogs for Ali, Boris, and Clancy. Draw a diagram of physical memory
(similar to Figure 11.2) illustrating the contents of the catalogs and referenced mem-
ory segments. You may omit address descriptors.
Exercise 11.3.3
The deﬁnition of MKSEG (Ns,(baseNs,boundNs)) in the text as-
sumed direct hardware support. Assume that there is no direct hardware support
for MKSEG. Devise methods, instructions, and derived inference rules that add the
descriptor (Ns,(baseNs,boundNs)) to the address descriptor segment ADS and place
the capability (read,write,Ns) into CR[3].
11.4
Summary
In this chapter, we introduced capability systems, which provide a ﬂexible scheme
that supports both segment addressing and memory protection. In capability sys-
tems, descriptors (i.e., protected physical addresses) provide the means for segment
addressing, while capabilities (i.e., unforgeable tickets that authorize access to mem-
ory) provide for memory protection. Capability systems employ tagged architectures
to distinguish capabilities from other data, thereby safely allowing user programs the
ﬂexibility to load, store, and share authorized capability values.
Although capabilities are easy to issue and to distribute, they can be challenging to
revoke. Typical revocation solutions impose some limitations on capabilities, either
in terms of where they may be stored, whether they may be copied, or how they may
be shared with other users.
Figure 11.8 summarizes the learning outcomes for this chapter.

242
Access Control, Security, and Trust: A Logical Approach
11.5
Further Reading
In their classic paper (Saltzer and Schroeder, 1975), Saltzer and Schroeder pro-
vide a thorough overview of the underlying concepts of capabilities and descriptors.
They also describe access-control list (ACL) systems as an alternative to capability
systems: instead of distributing capabilities to principals, each protected object has
its own access controller. Each access controller has the form of an address descrip-
tor with an access-control list that enumerates principals and their access rights on
that object.
The use of capabilities is not limited to granting access rights to memory seg-
ments. If address descriptors are used for other objects—such as communications
channels, printers, I/O devices, and applications in general—then capabilities can be
used for the relevant operations on those objects. This more extensive use of capa-
bilities is discussed in Levy’s book (Levy, 1984), which provides details of thirteen
experimental and commercial computer systems that use descriptors and capabilities.

Access Control Using Descriptors and Capabilities
243
FIGURE 11.8 Learning outcomes for Chapter 11
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
• When given a diagram of address descriptors and capabilities, be able to
interpret or describe how objects are identiﬁed and physically addressed,
as well as how operations on objects are authorized.
Application
• When given the protection goals of a system, apply the concepts of capa-
bilities and address descriptors to devise protection policies and mecha-
nisms.
Analysis
• When given an access request, determine if the capabilities at hand are
sufﬁcient to grant access.
Synthesis
• When given the protections goals of a system, design appropriate poli-
cies, architectures, instructions, and mechanisms for implementing ad-
dress descriptors and capabilities.
• When given descriptions of policies, architectures, instructions, and
mechanisms, create formal descriptions of each.
Evaluation
• When given formal descriptions of policies, architectures, instructions,
and mechanisms, express access-control decisions as formally derived
theorems in the access-control logic.


Chapter 12
Access Control Using Lists and
Rings
In Chapter 11, we introduced capability systems, which separate the notions of ad-
dressing and protection by distinguishing address descriptors from capabilities. Each
virtual machine associated with a process was given a catalog of capabilities that
identiﬁed the segments the virtual machine could access as well as the associated
access rights on those segments. The alternative to capabilities is access-control lists
(ACLs), which grant privileges to principals by name.
In this chapter, we describe how to use ACLs to provide discretionary access con-
trol for memory segments. When a principal requests access to a segment, the seg-
ment’s ACL is checked to determine whether the principal’s request is allowed. The
advantage of using ACLs is that removing or changing access to a resource does not
entail searching through each and every memory segment for a capability. As we
will see, however, the price for adding access-control lists to segment protection is
another layer of indirection.
We also introduce protection rings, which support mandatory access control for
memory segments. Protection rings are a generalization of the user and supervisor
modes, which provides a ﬁner-grained approach to protecting system integrity. Ring-
based access control is a layered approach to mandatory access control: processes
operating in the innermost rings have the greatest access, and data stored in the in-
nermost rings are afforded the greatest protection. This ﬁner-grained approach to
access control provides greater ﬂexibility and speciﬁcity on how data and processes
are accessed and by whom.
12.1
Generalized Addresses
Figure 12.1 illustrates a sample user’s conceptual view of memory that contains
four segments: each segment has a name, an associated access-control list, and data
contents. From the user’s point of view, the segments do not have speciﬁc addresses
associated with them. Instead, the user refers to the given segments and their con-
tents through the use of symbolic names, allowing her to write programs that are
independent of the segments’ physical locations in memory. For example, the pro-
gram may have a data segment named AliceInformation from which it attempts to
245

246
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.1 User view of memory
extract the contents of the symbolic location phoneNumber.
Of course, the actual layout of physical memory is quite different, and requests
to access memory ultimately must be converted into physical addresses. The ﬁrst
step in the conversion process occurs when the program is linked. The linker trans-
lates the location-within-a-segment reference into a generalized address (s,i), where
s and i are both natural numbers. The value s serves to identify the particular mem-
ory segment, while the value i identiﬁes a particular location within that segment.
Conceptually, the generalized address (s,i) refers to location i within the segment
numbered s. The details of the linker’s translation process are beyond the scope of
this text. Instead, in this section and the next, we turn to the details of how the
generalized address (s,i) is converted into a physical address in memory.
As we saw in Chapter 11, address descriptors—which give the starting location
and the size of segments—are stored in a catalog called the address descriptor seg-
ment (ADS). Each virtual machine has a descriptor base register (DBR) that points
to the base location of the ADS and contains the bound of the ADS.
The address descriptor for a segment numbered s is stored in the sth location of
the ADS. Thus, for example, if the linker associates the natural number K with the
symbolic segment name Ns, then the address descriptor for Ns is stored in location K
of the ADS. Furthermore, the value K is stored in one of two registers, depending on
the contents of the segment:1 (1) the procedure base register, if the segment is a pro-
cedure segment (i.e., contains instructions only), or (2) the temporary base register,
if the segment contains data.
Figure 12.2 illustrates how the relevant registers of a virtual machine are used to
determine the location of segment Ns’s address descriptor. The DBR contains the
1A common design discipline is to separate executable instructions from alterable data by keeping them
in separate segments. This approach supports pure procedures (i.e., executable code kept separate from
writable data), which reduce the likelihood of mistakenly executing data as instructions. The mechanisms
we describe in this chapter support pure procedures.

Access Control Using Lists and Rings
247
FIGURE 12.2 Address formation for address descriptor
address descriptor for the ADS. Depending upon whether Ns is a procedure or data
segment, either the procedure base register (PBR) or temporary base register (TBR)
contains the location of Ns’s address descriptor, relative to the base address of the
ADS. This value is the natural number that the linker associated with Ns (i.e., K in
our example). The physical address of Ns’s address descriptor is therefore
DBR.base+K,
provided that K < DBR.bound and DBR.base+K < q, where q is the size of physical
memory. That is, the value K must be within bounds of both the ADS and physical
memory.
Throughout the remainder of this chapter, we will refer to segments and their
locations via their linker-associated numbers, rather than their symbolic names.
12.2
Segment Access Controllers
In the capability systems of Chapter 11, address descriptors to generic memory
segments were (base,bound) values that corresponded directly to that segment’s
starting address and its number of locations in physical memory. For systems that use
access-control lists to protect memory segments, address descriptors instead point to
the access-controller segment that guards the named memory segment.
Access controllers have two parts: (1) the (base,bound) pair that points to the
segment being guarded, and (2) a list of authorized principal identiﬁers paired with
their access rights to the guarded memory segment. Whenever a processor running
a process with a particular principal identiﬁer wishes to access a speciﬁc memory
segment, the access controller for that segment is consulted to determine whether the
principal identiﬁer is listed with the requested access right. If so, then the operation
is permitted; otherwise, the request is trapped.

248
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.3 Segment access controller
FIGURE 12.4 Data or instruction segment
Figure 12.3 illustrates an access controller for a memory segment K. The ﬁrst
location of the access controller contains the address descriptor (baseK,boundK),
which points to the block of addresses in physical memory that deﬁne the guarded
memory segment K. The remaining locations contain the ACL entries, which consist
of principal identiﬁers and their associated permissions. For example, if the principal
identiﬁer register contains the principal identiﬁer Carol, then both read and write
access to the protected memory segment is permitted.
Figure 12.4 shows the segment K itself. The contents of the instruction counter
IC (if K is a procedure segment) or the segment address register SAR (if K is a data
segment) indicates the speciﬁc location within K to be accessed. Thus, for example,
an attempt to access generalized address (K,ℓ) would have ℓas the value of the IC
or SAR.
Figure 12.5 summarizes the four steps of converting a generalized address (s,i)
into an actual location in physical memory:
1. The address descriptor for the ADS assigned to the processor is located in the
descriptor base register (DBR); its value is (base1,bound1).
2. The address descriptor for s’s access controller is located in address s of the
ADS (i.e., in physical address base1 + s). Its value is (base2,bound2), which
points to the access controller that guards segment s.

Access Control Using Lists and Rings
249
FIGURE 12.5 Generalized address formation and access control
3. Location 0 of the access controller for segment s (i.e., physical location base2)
contains the address-descriptor value (base3,bound3), which points to the ac-
tual segment s. The remaining locations of the access controller contain the
ACL entries for principals who have access rights to segment s.
4. Location i within the segment s can be found at physical address base3 +i.
Exercise 12.2.1
Suppose George creates a data segment that he wishes to share
with Karen. George as the creator of the segment has read and write permission; he
wishes Karen to have read permission only.
Draw a diagram similar to Figure 12.5 that contains the explicit values for the
registers in Karen’s processor, the address descriptors, and the access-control list
entries to describe the following situation:
• The generalized address for the relevant location within the shared segment is
(5,12).
• Karen’s address descriptor segment is located in physical addresses 100
through 199.
• The access controller for the shared segment is located in physical addresses
200 through 250.
• The shared segment is located in physical addresses 300 through 400.
12.3
ACL-Based Access Policy for Memory Accesses
In the previous section, we fully described the process for calculating a physical
address from a generalized one. We now introduce a simple virtual machine (VM)
and virtual machine monitor (VMM) that supports generalized addresses and uses
ACLs to control memory accesses.

250
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.6 Virtual machine and virtual machine monitor for ACL system
Figure 12.6 shows a partitioning of registers and segments that support VM and
VMM functions. The VM has the following ﬁve registers: (1) the instruction regis-
ter (IR); (2) the accumulator (ACC); (3) the generalized instruction address, which
comprises the procedure base register (PBR) and instruction counter (IC); (4) the
generalized data address, which comprises the temporary base register (TBR) and
data address register (DAR); and (5) the principal ID register (PID).
The VMM has one register: the descriptor base register (DBR), which points to the
catalog of address descriptors in the ADS available to the VM. The ADS and access
controller segment for segment s are also used by the VMM to evaluate instruction
requests made by the VM.
Based on this structure, we develop the access-control policy for a generic instruc-
tion OP [s : i], which involves the operation OP on the generalized address (s,i). As
we will see, three memory fetches are required. For each memory fetch, we iden-
tify both the relevant address descriptor and the required mandatory access-control
(MAC) condition:
1. Look up the location of the ADS
As we saw in the previous section, the segment parameters for the ADS are
found in the base and bound ﬁelds of the DBR:
base1 = DBR.base,
bound1 = DBR.bound.
The necessary MAC conditions are that the address value base1 + s must be
within the limits of physical memory and that s is within the segment boundary
of the ADS:
(base1 +s < q)∧(s < bound1).

Access Control Using Lists and Rings
251
2. Look up the location of the access-controller segment
The parameters for the access-controller segment are found in the base and
bound ﬁelds of the sth location of the ADS segment:
base2 = PM[base1 +s].base,
bound2 = PM[base1 +s].bound.
We assume that, within the access-controller segment for s, indexid gives the
location of the permissions associated with id. The MAC address conditions
therefore check to determine whether base2 + indexid and indexid are within
the bounds of physical memory and of the access-controller segment, respec-
tively:
(base2 +indexid < q)∧(indexid < bound2).
In addition to the MAC conditions are the discretionary access control con-
ditions. In this case, the permission ﬁeld corresponding to OP is checked to
determine whether the principal requesting to perform OP has permission to
do so:
PM[base2 +indexid].op says ⟨OP⟩.
For example, when OP corresponds to a memory-write operation, the write
permission ﬁeld of PM[base2 +indexid] (i.e., PM[base2 +indexid].write) must
be checked.
3. Look up the generalized address (s,i)
The parameters for segment s are given by the following descriptor values:
base3 = PM[base2].base,
bound3 = PM[base2].bound.
The MAC conditions are as expected, namely that base3 +i must be within the
limits of physical memory and i must be within segment boundaries:
(base3 +i < q)∧(i < bound3).
We can combine these three sets of requirements to form the access-control policy
for approving execution of the operation OP [s : i]. The access-control policy consists
of the typical three parts:
1. The state of the machine, as given by the contents of IR, DBR, PID, and the
contents of physical memory PM
2. Discretionary access permission, as indicated by the statement
PM[base2 +indexid].op says ⟨OP⟩.

252
Access Control, Security, and Trust: A Logical Approach
3. The absence of any addressing problems, as indicated by the following three
conditions:
(base1 +s < q)∧(s < bound1),
(base2 +indexid < q)∧(indexid < bound2),
(base3 +i < q)∧(i < bound3).
The entire policy can be expressed in the logic as follows:
IR says ⟨OP[s : i]⟩⊃(DBR says ⟨(base1,bound1)⟩) ⊃(PID says ⟨indexid⟩) ⊃
(PM[base2 +indexid].op says ⟨OP⟩) ⊃((base1 +s < q)∧(s < bound1)) ⊃
((base2 +indexid < q)∧(indexid < bound2)) ⊃((base3 +i < q)∧(i < bound3)) ⊃
(IR controls ⟨OP[s : i]⟩)
IR says ⟨OP[s : i]⟩
DBR says ⟨(base1,bound1)⟩
PID says ⟨indexid⟩
PM[base2 +indexid].op says ⟨OP⟩
(base1 +s < q)∧(s < bound1)
(base2 +indexid < q)∧(indexid < bound2)
(base3 +i < q)∧(i < bound3)
⟨OP[s : i]⟩
The following example shows how this general access-control policy can be in-
stantiated for a speciﬁc operation on a given generalized address.
Example 12.1
Consider a request to execute the operation LDA[s : i], which loads the accumulator
ACC with the contents of generalized address (s,i). The derivable inference rule that
captures the conditions under which the request should be granted is as follows:
IR says ⟨LDA[s : i]⟩⊃(DBR says ⟨(base1,bound1)⟩) ⊃(PID says ⟨indexid⟩) ⊃
(PM[base2 +indexid].read says ⟨LDA⟩) ⊃((base1 +s < q)∧(s < bound1)) ⊃
((base2 +indexid < q)∧(indexid < bound2)) ⊃((base3 +i < q)∧(i < bound3)) ⊃
(IR controls ⟨LDA[s : i]⟩)
IR says ⟨LDA[s : i]⟩
DBR says ⟨(base1,bound1)⟩
PID says ⟨indexid⟩
PM[base2 +indexid].read says ⟨LDA⟩
(base1 +s < q)∧(s < bound1)
(base2 +indexid < q)∧(indexid < bound2)
(base3 +i < q)∧(i < bound3)
⟨LDA[s : i]⟩
♦
Exercise 12.3.1
Prove the soundness of the behavioral theorem of Example 12.1.
Exercise 12.3.2
Devise and prove sound the access-control policy for STO [s : i]
(i.e., storing the contents of generalized address (s,i) into the accumulator ACC).

Access Control Using Lists and Rings
253
FIGURE 12.7 Protection rings
12.4
Ring-Based Access Control
Protection rings are a means to provide layered protection: the most trusted and
trustworthy procedures and data segments are in the innermost rings, while less
trusted and trustworthy processes and data are in the outermost rings.
Figure 12.7 illustrates the notions behind protection rings. The innermost ring is
ring 0: it lies at the center of concentric rings of privilege. The most trusted (and,
one hopes, most trustworthy) supervisory procedures and data segments are in ring
0: ring-0 procedures have the greatest access privileges, and ring-0 data have the
most protection. Subsequent rings are labeled 1 through n, with each successive
ring having fewer access rights than the ring that precedes it. User processes and
data, located in the outermost rings, have relatively few access privileges and are
accessible by more privileged processes.
Protection rings were introduced in the Multics operating system (Schroeder and
Saltzer, 1972). Although Multics segments were guarded by both rings and ACLs,
neither ACLs or capabilities are necessary for ring-based protection. Instead, associ-
ated with each segment are the following permission-related components: privilege
bits to indicate which types of access (i.e., read, write, or execute) are permitted on
the segment;2 a ring bracket, speciﬁed by three ring numbers (r1,r2,r3); and (for
2Note that, if the system does use ACLs or capabilities, then these privilege bits are unnecessary. In such
cases, read/write/execute access for a given segment is determined on a principal-by-principal basis. In

254
Access Control, Security, and Trust: A Logical Approach
procedure segments) an integer ﬁeld gate that indicates its number of call gates. We
describe the purpose of the ring-bracket and gate ﬁelds in the following subsections.
12.4.1
Access Brackets
Each segment’s ring bracket (r1,r2,r3) must satisfy the following condition:
r1 ≤r2 ≤r3.
Collectively, the three ring numbers deﬁne two additional brackets: an access bracket
(which we describe here) and a call bracket (which we defer to Subsection 12.4.2).
Intuitively, a segment’s access and call brackets constrain the ring levels from which
processes can access that segment.
Consider a segment with ring bracket (r1,r2,r3). We say that ring r is within the
target segment’s access bracket if the following condition holds:
r1 ≤r ≤r2.
That is, ring r must be within the inclusive bounds set by r1 and r2. We say that ring
r is below the segment’s access bracket if r < r1.
Being within or below a segment’s access bracket is a necessary but insufﬁcient
condition for being granted read, write, or execute access to that segment. The fol-
lowing mandatory access-control policy applies to any process in ring r that attempts
to access a segment s having ring bracket (r1,r2,r3):
• Read access is granted if and only if s has read access turned on and r ≤r2.
• Execute access is granted if and only if s has execute access turned on and
r ≤r2.
• Write access is granted if and only if s has write access turned on and one of
the following conditions holds: (1) s is a procedure segment and r ≤r2, or (2)
s is a data segment and r ≤r1.
Thus, read and execute access are granted to any processes within or below a target
segment’s access bracket, provided that the segment has the corresponding access
turned on. The policy for write access to procedure segments is the same. However,
only processes at more privileged ring levels are allowed to alter a data segment’s
contents: write access to data segments is limited to processes operating at the lower
bound of (or below) the segment’s access bracket, and only when the segment has
write access turned on.
The following example illustrates how the ring-based access-control policy applies
to both procedure and data segments.
the rest of this chapter, we use the terminology “access turned on” to indicate either that the appropriate
privilege bit is set (for systems without ACLs and capabilities) or that the particular principal making the
request possesses the appropriate permission for that segment.

Access Control Using Lists and Rings
255
Example 12.2
Consider three processes Q1,Q4,Q6, which are executing in rings 1, 4, and 6, re-
spectively. Furthermore, let PS and DS both be segments with ring bracket (2,5,8)
and with read, write, and execute access turned on. Assume that PS is a procedure
segment and that DS is a data segment.
The following table shows the results of attempts by the processes to access the
two segments under the ring-based access policy:
Access to PS
Access to DS
Access type
Q1
Q4
Q6
Q1
Q4
Q6
Read
Granted
Granted
Denied
Granted
Granted
Denied
Execute
Granted
Granted
Denied
Granted
Granted
Denied
Write
Granted
Granted
Denied
Granted
Denied
Denied
This table highlights two important points. First, the ring requirements prevent Q6
from accessing either segment, despite that read, write, and execute access are turned
on for both segments: Q6 is neither within nor below the segments’ access bracket.
Second, Q4 is granted write access to the procedure segment PS but not to the data
segment DS: despite being within the appropriate access bracket, Q4’s ring level is
too high to write to the data segment.
♦
12.4.2
Call Brackets
In addition to an access bracket, segments may also have a call bracket. The
purpose of a call bracket is to allow procedure segments to be called under con-
trolled conditions by processes at ring levels above the segment’s access bracket.
This mechanism allows less trusted user processes to call more trusted (i.e., at a
lower ring level) system processes.
Let s be a segment with ring bracket (r1,r2,r3). We say that a ring r is within s’s
call bracket if the following condition holds:
r2 +1 ≤r ≤r3.
That is, r must be strictly greater than r2 and less than or equal to r3. Thus, in the
case where r2 = r3, the segment has no call bracket, because there is no r such that
r2 +1 ≤r ≤r2.
Conceptually, a procedure segment s has a list of n entry points—known as call
gates—associated with it. As illustrated in Figure 12.8, these call gates appear at the
beginning of s and are pointers to locations within the segment s. The purpose of
call gates is to limit the available entry points for processes that might be within s’s
call bracket but outside of s’s access bracket. The pointers ensure that s cannot be
executed at some incorrect, inappropriate, or arbitrary location.
Recall that each procedure segment has a permission ﬁeld named gate, which
contains the number of call gates for s. A process in the calling bracket that wishes

256
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.8 Call gates
to call the ith location in s requests that the operation CALL [s : i] be executed. If
i ≥gate, then the calling process is attempting to enter s at an unauthorized point
and execute permission is denied. Otherwise, if i < gate, then the procedure call is
within the stated gate list, and permission to execute the CALL [s : i] instruction is
granted, provided other necessary conditions related to s’s call bracket and execute
permissions are met.
Suppose a process running in ring r requests that the operation CALL [s : i] be
executed on a segment s that has ring bracket (r1,r2,r3). The following mandatory
access-control policy applies:
• CALL [s : i] access is granted if and only if s has execute access turned on
and one of the following situations holds: (1) r is below or within s’s access
bracket, or (2) s is a procedure segment, r is within s’s calling bracket, and
i < gate.
Note that, for rings within or below a segment’s access bracket, CALL access corre-
sponds exactly to the standard execute access described in the previous section.
The following example illustrates how the ring-based access-control policy applies
to CALL requests.
Example 12.3
Consider four processes Q1,Q4,Q6,Q10, which are executing in rings 1, 4, 6, and 10,
respectively. Furthermore, let PS3, PS9, and DS all be segments with ring bracket
(2,5,8) and with read, write, and execute access turned on. Assume that DS is a
data segment; PS3 and PS9 are procedure segments with gate values of 3 and 9,
respectively.

Access Control Using Lists and Rings
257
The following table shows the results of attempts by the processes to execute the
CALL [s : 7] instruction on the various segments s:
Segment s
Q1
Q4
Q6
Q10
PS3
Granted
Granted
Denied
Denied
PS9
Granted
Granted
Granted
Denied
DS
Granted
Granted
Denied
Denied
This table highlights several important aspects of how ring protection works in the
context of call brackets. First, CALL access reverts to standard execute access for
processes operating within or below a segment’s access bracket: for this reason, all
of Q1’s and Q4’s attempts are granted. Second, a process operating in a ring level
higher than a segment’s call bracket is ineligible to use the CALL instruction: for this
reason, all requests by Q10 are denied. Finally, a process operating within a speciﬁc
call bracket can call only procedure segments, and only at locations i permitted by
the segment’s gate value: for this reason, Q6’s attempts to access the seventh location
of data segment DS and of procedure segment PS3 are denied.
Finally, all attempts to execute CALL [PS2 : 7] are denied, because the requested
entry point 7 exceeds the gate value 2.
♦
The following series of exercises is inspired by exercises in (Organick, 1972).
Exercise 12.4.1
For each of the following situations, give the 3-tuple (r1,r2,r3)

that deﬁnes the requested access and call brackets.
a. A single ring of access, r.
b. An access bracket (k,l), but no call bracket.
c. An access bracket (k,l) and a call bracket (l +1,m).
d. A single ring of access r and a call bracket (r +1,m).
Exercise 12.4.2
For each of the following situations, give the set of rings r from
which read, write, and execute access to the data segment s would be granted.
a. s has ring bracket (0,63,63), with read and execute access turned on.
b. s has ring bracket (0,1,63), with read and execute access turned on.
c. s has ring bracket (1,1,63), with read and execute access turned on.
d. s has ring bracket (0,0,1), with read and execute access turned on.
e. s has ring bracket (48,48,48), with read and write access turned on.
f. s has ring bracket (5,48,48), with read and write access turned on.

258
Access Control, Security, and Trust: A Logical Approach
Exercise 12.4.3
For each of the following situations, give the set of rings r from
which read, write, and execute access to the procedure segment s would be granted.
a. s has ring bracket (0,1,63), with read and execute access turned on.
b. s has ring bracket (1,1,63), with read and execute access turned on.
c. s has ring bracket (48,48,48), with read and write access turned on.
d. s has ring bracket (5,48,48), with read and write access turned on.
Exercise 12.4.4
For each of the following situations, give the set of rings r and set
of values i for which CALL [s : i] access to the segment s would be granted.
a. s is a data segment with ring bracket (0,1,63) and read and execute access
turned on.
b. s is a procedure segment with ring bracket (0,1,63), gate value 5, and read
and execute access turned on.
c. s is a procedure segment with ring bracket (0,0,1), gate value 5, and read and
execute access turned on.
d. s is a procedure segment with ring bracket (0,0,1), gate value 5, and read and
write access turned on.
12.5
Summary
In this chapter, we introduced both access-control list (ACL) systems and ring-
based access control, which extend the memory-protection mechanisms we saw in
previous chapters. These schemes can be used together (as in the Multics operating
system) or separately. In ACL systems, access-controller segments specify the lo-
cation of speciﬁc memory segments, as well as the access capabilities for individual
principals on that segment. ACL systems more easily support permission revocation
than capability systems, but at the cost of an additional level of indirection. Pro-
tection rings provide a layered protection approach that offers both ﬂexibility and
speciﬁcity on how data and processes are accessed. Processes in the innermost rings
are the most trusted processes and have the greatest access privileges. Processes in
the outer rings are typically less trusted user processes with fewer privileges.
Both segmentation and protection rings were ﬁrst developed for use in the 1960s
for mainframe computing systems such as Multics. When mainframes and mini-
computers gave way to personal computers, protection schemes such as rings and
segmentation were dropped for economic reasons. At the time, personal comput-
ers were physically isolated machines with single users, which obviated the need
to isolate one user’s processes from those of other users. The additional cost of

Access Control Using Lists and Rings
259
FIGURE 12.9 Learning outcomes for Chapter 12
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
• When given an address descriptor, access controller with ACL entries,
and ring brackets for a segment, determine when access to a target seg-
ment is granted or denied.
Application
• When given the protection goals of a system, apply the concepts of
access-control lists and ring brackets to devise protection policies and
mechanisms.
Analysis
• When given an access request, determine if the permissions at hand are
sufﬁcient to grant access.
Synthesis
• When given descriptions of policies, architectures, instructions, and
mechanisms, create formal descriptions of each.
Evaluation
• When given formal descriptions of policies, architectures, instructions,
and mechanisms, express access-control decisions as formally derived
theorems in the access-control logic.
having hardware dedicated to security and isolation seemed impractical. No one
anticipated the global internetworking and unprecedented access and vulnerability
the Internet would bring. Decades later, security and access control are now essen-
tial, and support for segmentation and protection rings is now routinely included in
microprocessors.
Figure 12.9 summarizes the learning outcomes for this chapter.
12.6
Further Reading
The ideas behind the use of access-control lists and rings to control access to
segments were developed in the 1960s and 1970s on mainframe computer systems

260
Access Control, Security, and Trust: A Logical Approach
such as Multics. Numerous papers chart the development of generalized addressing
(Daley and Dennis, 1968), virtualized memory (Bensoussan et al., 1972), access-
control lists (Saltzer and Schroeder, 1975), and rings (Schroeder and Saltzer, 1972;
Organick, 1972).

Part IV
Access Policies


Chapter 13
Conﬁdentiality and Integrity Policies
In Chapter 5, we introduced military security policies, which are primarily concerned
with controlling the disclosure of classiﬁed information. We also introduced com-
mercial security policies, which focus on preserving the integrity (e.g., quality or
trustworthiness) of information. In both cases, the policies depend on assigning clas-
siﬁcation levels—either security levels or integrity levels, as appropriate—to both
subjects and objects; access-control decisions are then based in part on the relation-
ships among those levels.
In this chapter, we expand on those ideas to support more ﬁnely tuned conﬁden-
tiality and integrity policies. The ﬁrst step is to introduce a somewhat richer notion
of conﬁdentiality and integrity levels, which is based on categories of information in
addition to the classiﬁcation of information. As we will see, the underlying Kripke
semantics introduced in Chapter 5 also work for these richer levels. We then recap
both the Bell–La Padula model for conﬁdentiality and Biba’s strict-integrity model,
highlighting how they apply to the more reﬁned notions of conﬁdentiality and in-
tegrity levels. We also address how Biba’s notion of strict integrity applies to invo-
cation access requests, a topic previously deferred. Finally, we introduce Lipner’s
integrity model, which applies a combination of the Bell–La Padula and Biba mod-
els to develop an integrity policy for production systems. His approach serves as a
useful case study for designers and veriﬁers who must develop their own policies to
meet speciﬁc design requirements.
13.1
Classiﬁcations and Categories
So far, we have considered only simple conﬁdentiality and integrity levels. It is
also possible to construct compound levels for both conﬁdentiality and integrity by
combining a classiﬁcation level (as we saw in Chapter 5) with a set of categories
to which it applies. Speciﬁcally, each compound level is a pair (L,C), where L is a
classiﬁcation level and C ⊆Cat is a (possibly empty) set of categories drawn from
the set Cat.
As we saw in Chapter 5, the classiﬁcation levels used for conﬁdentiality or in-
tegrity are related to one another by a partial order ≤. Recall that partial orders
are binary relations that are reﬂexive, transitive, and anti-symmetric. Therefore, the
263

264
Access Control, Security, and Trust: A Logical Approach
classiﬁcation-level relation ≤must satisfy the following properties:
• Reﬂexivity: For all classiﬁcation levels L, L ≤L.
• Transitivity: For all classiﬁcation levels L1,L2, and L3, if L1 ≤L2 and L2 ≤L3,
then L1 ≤L3.
• Anti-symmetry: For all classiﬁcation levels L1 and L2, if L1 ≤L2 and L2 ≤L1,
then L1 = L2 (i.e., no cycles exist in the relation ≤).
The sets of categories are also related to one another by a partial order, namely the
subset relation ⊆. It is therefore possible to deﬁne a partial order dom over compound
levels (L,C) as follows:
(L1,C1) dom (L2,C2) if and only if (L2 ≤L1) and C2 ⊆C1.
That is, the compound level (L1,C1) dominates (L2,C2) if and only L1 and C1 are
each at least as high as L2 and C2, respectively. The proof that dom is indeed a partial
order is left as an exercise.
The following two examples provide concrete illustrations of compound levels and
how the dom relation works.
Example 13.1
Consider the set of classiﬁcation levels L = {HI, LO}, with the partial order ≤deﬁned
over L as follows:
LO ≤HI.
Furthermore, suppose the set of categories is the set Cat = {BIN1, BIN2}.
This combination of sets L and Cat yields eight possible compound levels:
(HI,{BIN1, BIN2})
(HI,{BIN1})
(HI,{BIN2})
(HI,{})
(LO,{BIN1, BIN2})
(LO,{BIN1})
(LO,{BIN2})
(LO,{})
The dom ⊆L ×Cat relation is presented graphically by the Hasse diagram in Fig-
ure 13.1. Each compound level appears as a node in the Hasse diagram, but not every
element of dom shows up as an explicit edge in the diagram. Each edge (i.e., down-
ward line) from a compound level ℓ1 = (L1,C1) to a level ℓ2 = (L2,C2) corresponds
to an element (ℓ1,ℓ2) ∈dom that cannot be deduced through reﬂexivity or transitivity.
Thus, for example, no edge is necessary to indicate that (HI,{BIN1}) dom (LO,{}):
that relationship can be deduced by recognizing that (HI,{BIN1}) dom (HI,{}) and
(HI,{}) dom (LO,{}).
♦

Conﬁdentiality and Integrity Policies
265
FIGURE 13.1 Partial ordering on conﬁdentiality levels
Example 13.2
Consider the compound levels and dom relation given in Example 13.1, and suppose
that Arthur has conﬁdentiality level (HI,{BIN1}). His conﬁdentiality level dominates
documents with the following levels:
(HI,{BIN1}), (LO,{BIN1}), (HI,{}), (LO,{}).
♦
As a ﬁnal note, we point out that the compound levels introduced here still ﬁt
within the syntax and semantics introduced for conﬁdentiality and integrity policies
in Chapter 5. In particular, compound levels can be viewed as additional elements of
the syntactic sets SecLabel and IntLabel, and the dom relation can be represented
by the ≤s and ≤i operators of the logic. The following example reinforces this point.
Example 13.3
Suppose that Jane’s conﬁdentiality level is (LO,{BIN1, BIN2}) and that object O has
conﬁdentiality level (LO,{BIN2}). We can express these facts in the logic as follows:
slev(Jane) = (LO,{BIN1, BIN2}) ∧slev(O) = (LO,{BIN2}).
We can also express the following consequence, namely that Jane’s conﬁdentiality
level dominates O’s level:
slev(O) ≤s slev(Jane).
♦
Exercise 13.1.1
Prove that the subset relation ⊆is a partial order.
Exercise 13.1.2
Prove that the dom relation is a partial order.

266
Access Control, Security, and Trust: A Logical Approach
13.2
Bell–La Padula Model, Revisited
We saw in Chapter 5 that the Bell–La Padula model for conﬁdentiality mandates
that the following two properties be satisﬁed:
1. Simple security condition: A principal P can read object O if and only if
P’s conﬁdentiality level is at least as high as O’s and P has discretionary read
access to O.
2. *-property: A principal P can write to object O if and only if O’s conﬁden-
tiality level is at least as high as P’s and P has discretionary write access to
O.
As pointed out previously, the “if and only if” requirements in these properties
mean that each property actually involves two implications. For example, the simple
security condition contains the following two implications:
1. If P’s conﬁdentiality level is at least as high as O’s and P has discretionary
read access to O, then P can read O.
2. If P can read O, then P’s conﬁdentiality level is at least as high as O’s and P
has discretionary read access to O.
The ﬁrst implication represents an access-control condition: it states the conditions
under which a reference monitor should grant access to O (i.e., the access policy).
Such conditions will often occur in the context of a formal justiﬁcation for granting
access to a speciﬁc object. In contrast, the second implication represents a safety
condition for the entire system: this sort of property cannot be assumed, but instead
must be veriﬁed by checking every potential consequence of the rules and mecha-
nisms that govern access. Proving such a property—which relies on the complete-
ness, isolation, and veriﬁability of every reference monitor in the system—is beyond
the scope of the access-control logic. In the discussion that follows, we focus on the
access-control conditions.
We can formulate the access-control policies that correspond to the simple security
condition and the *-property as follows:
1. Simple-security access-control policy:
( slev(O) ≤s slev(P)) ⊃(P controls ⟨read,O⟩)
2. *-property access-control policy:
( slev(P) ≤s slev(O)) ⊃(P controls ⟨write,O⟩)
The following example illustrates the speciﬁcation of access policies associated
with the Bell–La Padula model.

Conﬁdentiality and Integrity Policies
267
Subject or Object
Conﬁdentiality Level
Carol
(HI,{BIN1, BIN2})
Kate
(LO,{BIN2})
O1
(HI,{BIN1, BIN2})
O2
(LO,{BIN2})
O3
(LO,{BIN1})
O4
(LO,{})
Table 13.1: Conﬁdentiality levels
Object
Subject
O1
O2
O3
O4
Carol
read
-
read
-
Kate
write
-
-
read
Table 13.2: Discretionary access-control matrix
Example 13.4
Consider Tables 13.1 and 13.2: the former lists the conﬁdentiality levels of two
subjects (Carol and Kate) and four objects (O1,O2,O3,O4), while the latter speciﬁes
the discretionary access of subjects to objects.
Together, these two tables satisfy the Bell–La Padula model: read permission for
object Oi is granted only to subjects whose conﬁdentiality level is at least as high
as Oi’s, and write permission for object Oi is granted only to subjects whose conﬁ-
dentiality level is no higher than Oi’s. Note that, although both Carol and Kate have
conﬁdentiality levels high enough to read object O2, neither has been granted that
discretionary access.
The access-control policy can be expressed in the access-control logic by the fol-
lowing statements:
( slev(O1) ≤s slev(Carol)) ⊃(Carol controls ⟨read,O1⟩),
( slev(O1) ≤s slev(Carol)) ⊃(Carol controls ⟨read,O3⟩),
( slev(Kate) ≤s slev(O1)) ⊃(Kate controls ⟨write,O1⟩),
( slev(O4) ≤s slev(Kate)) ⊃(Kate controls ⟨read,O4⟩).
♦
We conclude this section with an example that illustrates why the safety aspects of
Bell–La Padula must be veriﬁed through system audit, rather than within the system
itself.
Example 13.5
Dexter is a corrupt system administrator who has supervisory privileges in an orga-
nization’s information system. Because Dexter is trusted (but not trustworthy), he is
in a position to bypass the security of the system.

268
Access Control, Security, and Trust: A Logical Approach
Prior to being ﬁred, Dexter installs a back door so that he can rewrite personnel
records. He alters the reference monitor that guards the personnel records so that it
accepts a secret identiﬁer:
secretID controls ⟨write,personnel records⟩.
By doing so, Dexter has bypassed all the mandatory access-control checks. This
back door can be detected only by auditing Dexter’s actions and by inspecting the
subsystems accessed by Dexter: no reference monitor can possibly detect that Dex-
ter’s change violates the access policy.
♦
Exercise 13.2.1
Consider the conﬁdentiality-level assignments and discretionary
access speciﬁed in Example 13.4. Provide formal proofs in the access-control logic
that, when requested, Carol can read objects O1 and O3 and Kate can write O1 and
read O4.
Exercise 13.2.2
Reformulate the extended example in Section 5.4.3 for the FX-1
and FX-2 ﬁghter projects. In particular, reformulate all the level assignments to
subjects and objects using both clearance levels and the categories FX-1 and FX-2.
Make sure that you assign the clearance levels to preserve the privileges as described
in Section 5.4.3.
In addition, suppose General Jane in the Department of Defense heads the entire
Next Generation Fighter project. Thus, both the FX-1 and FX-2 projects are in her
jurisdiction. What should her conﬁdentiality level be, and how are the FX-1 and
FX-2 access matrices altered so that she has read access to all documents?
Exercise 13.2.3
Correctional facilities (prisons) have several classiﬁcations of di-
rectives.1 These classiﬁcations are as follows:
• Level A: accessible to anyone, including inmates. Level A documents are
public information.
• Level B: not accessible by inmates but accessible to ofﬁcers, sergeants, and
management. Level B documents generally deal with normal facility opera-
tions.
• Level C: conﬁdential (but not emergency-related) information. Level C doc-
uments generally deal with conﬁdential personnel information. These docu-
ments are available to sergeants and above, but not to ofﬁcers and inmates.
• Level D: emergency-response information.
Level D documents are highly
conﬁdential that, if possessed by inmates, would threaten the security of the
prison. These documents are available only to management.
1This exercise was inspired by a conversation with Gary Tyler, Lieutenant (retired), New York State
Department of Corrections.

Conﬁdentiality and Integrity Policies
269
a. Fill in the following table with classiﬁcation levels for each subject and object:
Subject or Object
Classiﬁcation Level
Inmate
Ofﬁcer
Sergeant
Chief of Security
Prison Visiting Hours
Vacation Policy for Ofﬁcers
Conﬁdential Performance Reviews of Ofﬁcers
Emergency Response Plans for Riots
b. Fill in the following access-control matrix so that it meets the following con-
ditions:
• It satisﬁes the Bell–La Padula conﬁdentiality model.
• It follows the intent of the classiﬁcation levels described in the scenario.
• Only the Chief of Security can write or read the Emergency Response
Plan for Riots.
• Sergeants write the conﬁdential performance review for ofﬁcers, which
is then readable by the Chief of Security.
Subject
Visiting
Hours (A)
Vacation
Policy (B)
Performance
Reviews
(C)
Emergency
Response
Plans (D)
Inmate (A)
Ofﬁcer (B)
Sergeant (C)
Chief of Security (D)
c. Express in the access-control logic the policy that the Chief of Security (CS)
has the capability to read and write Emergency Response Plans (ERPs).
d. Propose a derived inference rule that states that the Chief of Security (CS) can
read emergency response plans (ERPs). Give a formal proof of the derived
rule.
13.3
Conﬁdentiality Levels: Some Practical Considera-
tions
Our discussion of conﬁdentiality levels so far has ignored one pragmatic point:
people, processes, and information are rarely fully isolated in wholly separate en-
claves. Instead, people and information move across boundaries, be they physical

270
Access Control, Security, and Trust: A Logical Approach
or virtual. For example, within secure facilities, people with high security clear-
ances communicate with people who have lower or no security clearances. Those
who work in secure facilities head home and spend time with family and friends in
unsecured environments. Similarly, sensitive government documents are routinely
released to the public under freedom-of-information laws.
Several approaches are used to prevent sensitive information from being leaked
as people and information move across boundaries. These approaches include max-
imum and current conﬁdentiality levels, sanitization and conﬁnement. We describe
these approaches in turn, illustrating each with one or more examples.
The Bell–La Padula model imposes constraints on how subjects with high con-
ﬁdentiality levels may communicate with subjects at lower levels. For example,
consider the case of Kamal, a Senior Scientist at a military research lab who has a
conﬁdentiality level of (TS,{NUC, US, EUR, ASIA}). Kamal needs to send a mes-
sage (i.e., a ﬁle) to one of his staff engineers (Sarah), whose conﬁdentiality level is
(TS,{NUC, US}).
As presented so far, Bell–La Padula prohibits Sarah from reading any ﬁle that Ka-
mal writes, because his conﬁdentiality level is higher than Sarah’s. However, in fact
the Bell–La Padula model makes the following allowance: subjects have both a max-
imum conﬁdentiality level and a current conﬁdentiality level. For understandable
reasons, the maximum level must always dominate the current level. When two sub-
jects need to communicate with one another, the subject with the higher level reduces
his or her current level appropriately, so that they can communicate as needed.
In practice, the adoption of different conﬁdentiality levels often requires a subject
to switch systems to effectively lower his or her level. For example, a person with
top-secret clearance will log on to a secret system to communicate with people at the
secret level. The following example illustrates how this approach lets Kamal send a
message that Sarah can read.
Example 13.6
Kamal has a conﬁdentiality level of (TS,{NUC, US, EUR, ASIA}), and his staff en-
gineer Sarah has a conﬁdentiality level of (TS,{NUC, US}). To communicate with
Sarah, Kamal logs onto a TS system that has only NUC,US as allowable categories,
and sends his message. Having reduced his current conﬁdentiality level to Sarah’s,
any message he writes will be readable by Sarah.
♦
In many cases—such as the public release of documents through freedom-of-
information requests—it is an object’s conﬁdentiality level that is problematic, rather
than a subject’s level. Sanitization is the process of removing classiﬁed or restricted
information from a document or ﬁle, thereby allowing the remaining information to
be released at a lower classiﬁcation level. In essence, the document in question is
edited or rewritten without the sensitive information. The following two examples
illustrate the types of contexts in which sanitization is used.

Conﬁdentiality and Integrity Policies
271
Example 13.7
Court proceedings are typically considered public records. However, in cases that
involve crimes against children, there are often requirements that prevent disclosure
of the identities of child victims. In such situations, the court records released to the
public have the names of children removed or hidden.
♦
Example 13.8
Before government reports are released to the public, classiﬁed information (e.g., the
names of spies) is removed, hidden, or blocked out. The resulting document is then
reclassiﬁed to a lower level before being released.
♦
A third pragmatic concern is the potential leaking of protected information from
a protected environment to a less protected environment. Conﬁnement is the means
by which such leaks are prevented, be they in the virtual or physical environments.
For example, a secure server may be placed on a secure network, to prevent inse-
cure processes from accessing it and then transmitting information insecurely. The
following example illustrates an approach to conﬁnement in the physical world.
Example 13.9
A Sensitive Compartmented Information Facility (SCIF, pronounced “skiff”) is a se-
cure location with appropriate access controls to ensure that classiﬁed information
can be accessed securely. Recently, Tom listened to a secret brieﬁng in his labo-
ratory’s SCIF, which is contained within a larger ofﬁce building that has a lower
clearance level. Being a new and inexperienced employee, Tom took his engineering
notebook with him into the SCIF without considering the consequences.
Before exiting the SCIF, Tom had to surrender his engineering notebook to the
SCIF security ofﬁce, who will inspect the notebook to verify that it does not contain
any classiﬁed information. This inspection may take up to a week or more.
♦
Exercise 13.3.1
Consider the situation described in Example 13.6. In whom and in
what must we trust to prevent the leakage of information downwards to Sarah from
Kamal? What are some things that can be done to mitigate some of the risks and
vulnerabilities?
Exercise 13.3.2
Consider the situation described in Example 13.7. Suppose that
Mary is in charge of court records, including the censoring of sensitive court docu-
ments for public release. Furthermore, suppose that the court employs two computer
systems, one for sensitive unsanitized (U) documents and another for publicly re-
leasable sanitized (S) documents.
Devise a concept of operations that describes how Mary takes a ﬁle CaseSensitive
(where slev(CaseSensitive) = U), censors it, and produces a publicly releasable ﬁle
CaseReleasable (where slev(CaseReleasable) = S).
Your concept of operations should satisfy the Bell–La Padula model and include
the following:

272
Access Control, Security, and Trust: A Logical Approach
• The partial order that relates U and S
• The discretionary access-control matrix that shows Mary’s privileges
• The formulation in the access-control logic of the access-control policy for
Mary’s reading and writing of CaseSensitive and CaseReleasable
• The formulation in the access-control logic of the assignment of conﬁdentiality
levels to Mary and to the two case ﬁles
• Theorems in the access-control logic that justify letting Mary read and write
the relevant case ﬁles
Exercise 13.3.3
Consider the situation in Example 13.9, and suppose that Tom’s
notebook can be released only if it contains only unclassiﬁed information. John, as
the SCIF security ofﬁcer, determines whether or not Tom’s notebook can be released.
Describe in the access-control logic John’s jurisdiction over whether or not the
notebook is released or held. Using your formulation, formally justify in the access-
control logic the release of Tom’s notebook if John says so.
13.4
Biba’s Strict Integrity, Revisited
Biba’s strict-integrity model is the dual of the Bell–La Padula model: it uses the
same structure as Bell–La Padula (i.e., partially order levels), but the direction of
information ﬂow is reversed. Whereas the catch phrase for Bell–La Padula is “no
read up and no write down,” the catch phrase for Biba is “no read down and no write
up.”
We introduced the underlying concepts and deﬁnitions for Biba’s strict-integrity
model in Chapter 5, where we focused on direct-access operations (i.e., observations
and modiﬁcations). In this section, we brieﬂy review the requirements for Biba’s
strict integrity, but we focus on ﬁlling in the details as they apply to indirect access
(i.e., the invocation of one subject by another).
As we saw previously, strict integrity requires that the following three properties
be met:
1. Simple integrity condition: A subject S can observe O if and only if
ilev(S) ≤ilev(O) and S has discretionary observe access to O.
2. Integrity *-property: A subject S can modify O if and only if
ilev(O) ≤ilev(S) and S has discretionary modify access to O.
3. Invocation condition: A subject S1 and invoke subject S2 if and only if
ilev(S2) ≤ilev(S1) and S1 has discretionary invocation rights to S2.

Conﬁdentiality and Integrity Policies
273
In effect, strict integrity requires that the integrity level of each object and subject
in a transfer path must be at least as high as that of the subject or object that imme-
diately follows it. Thus, for example, in a transfer path where (for each i such that
1 ≤i ≤n) subject Si can read from object Oi and write to object Oi+1, the following
condition must hold:
ilev(On+1) ≤i ilev(Sn) ≤i ilev(On) ≤i ··· ≤i ilev(O2) ≤ilev(S1) ≤i ilev(O1).
In the case of indirect operations, strict integrity requires that the integrity level of
the invoking subject S1 must be at least as high as that of the invoked subject S2.
Otherwise, less trustworthy subjects could direct the actions of more trustworthy
subjects: for example, users could direct system administrators to disable system
safeguards such as malware detectors and perimeter defenses.
As we saw with the Bell–La Padula properties, the three strict-integrity properties
implicitly involve two implications: an access-control condition and a system-wide
safety (or audit) property. The access-control conditions, which represent the access
policies used by reference monitors, can be expressed in the logic as follows:
1. Simple-integrity access-control condition:
( ilev(S) ≤ilev(O)) ⊃(S controls ⟨observe,O⟩)
2. Integrity *-property access-control condition:
( ilev(O) ≤ilev(S)) ⊃(S controls ⟨modify,O⟩)
3. Invocation access-control conditions:
( ilev(S2) ≤i ilev(S1)) ⊃(S1 controls ⟨i,S2,o,O⟩),
( ilev(S2) ≤i ilev(S1)) ⊃(S1 controls ⟨i,S2,m,O⟩),
where ⟨i,S,o,O⟩and ⟨i,S,m,O⟩respectively denote the propositions “invoke
subject S to observe O” and “invoke subject S to modify O.”
The following example demonstrates how such conditions can be used to express
strict-integrity policies.
Example 13.10
Consider Tables 13.3 and 13.4, which respectively (1) list the integrity levels for
two subjects (Carol and Kate) and four objects (O1,O2,O3,O4) and (2) specify the
subjects’ discretionary observe, modify, and invocation rights. Together, these two
tables satisfy Biba’s strict-integrity properties.
The access-policy regarding Carol’s discretionary modify access to O3 can be ex-
pressed as follows:
( ilev(O3) ≤i ilev(Carol)) ⊃(Carol controls ⟨modify,O3⟩).
♦

274
Access Control, Security, and Trust: A Logical Approach
Subject or Object
Integrity Level
Carol
(LEV1,{CAT1, CAT2})
Kate
(LEV2,{CAT2})
O1
(LEV1,{CAT1, CAT2})
O2
(LEV2,{CAT2})
O3
(LEV2,{CAT1})
O4
(LEV2,{})
Table 13.3: Integrity levels
Object
Subject
Subject
O1
O2
O3
O4
Carol
Kate
Carol
observe
-
modify
-
-
invoke observe,
invoke modify
Kate
observe
-
-
modify
-
-
Table 13.4: Discretionary access-control matrix
Table 13.4 includes indirect access rights for Carol: she has invocation rights on
Kate for both observe and modify actions. Consequently, Kate must have a policy on
how she will respond to Carol’s requests. There are three possibilities when Carol
invokes Kate:
1. Kate denies Carol’s request.
Because the request is denied, there is nothing further to analyze: strict in-
tegrity is preserved.
2. Kate honors Carol’s request, and Kate has discretionary access to the object
she must access on Carol’s behalf.
We explore this case in Example 13.11.
3. Kate honors Carol’s request, and Kate does not have discretionary access to
the object she must access on Carol’s behalf.
This case is left for Exercise 13.4.3.
Example 13.11
Consider the integrity levels given in Table 13.3 and the discretionary access-control
matrix of Table 13.4. Furthermore, suppose that Carol requests that Kate modify
object O4 on her behalf.
Figure 13.2 illustrates the three steps that must occur for Carol’s indirect request
for the modiﬁcation of O4 to be granted:
1. Kate’s reference monitor must conclude that Carol’s invocation request should
be honored by Kate.

Conﬁdentiality and Integrity Policies
275
FIGURE 13.2 Carol’s request and Kate’s response
The reference monitor must ensure that Carol’s integrity level is at least as high
as Kate’s integrity level and that Carol has discretionary invocation access on
Kate.
Letting ⟨i,Kate,modify,O4⟩denote the proposition “it is good to grant the
invocation request for Kate to modify object O4,” the reference monitor’s be-
havior can be represented by the following derived inference rule:
ilev(Carol) =i (LEV1,{CAT1, CAT2})
ilev(Kate) =i (LEV2,{CAT2})
(LEV2,{CAT2}) ≤i (LEV1,{CAT1, CAT2})
ilev(Kate) ≤i ilev(Carol) ⊃(Carol controls ⟨i,Kate,modify,O4⟩)
Carol says ⟨i,Kate,modify,O4⟩
⟨i,Kate,modify,O4⟩.
2. Kate’s behavior is governed by her policy regarding invocations that are passed
on to her by her reference monitor.
If her policy is to make any direct-access request that the reference monitor
passes on to her, then her behavior upon receiving Carol’s request can be ex-
pressed by the following derived inference rule:
⟨i,Kate,modify,O4⟩⊃⟨modify,O4⟩
⟨i,Kate,modify,O4⟩
Kate says ⟨modify,O4⟩.
3. Ultimately, Kate’s request on Carol’s behalf must be approved by the reference
monitor for object O4.
The reference monitor must ensure that Kate’s integrity level is at least as
high as O4’s integrity level. The reference monitor’s eventual decision can be

276
Access Control, Security, and Trust: A Logical Approach
expressed by the following derived inference rule:
ilev(O4) =i (LEV2,{})
ilev(Kate) =i (LEV2,{CAT2})
(LEV2,{}) ≤i (LEV2,{CAT2})
ilev(O4) ≤i ilev(Kate) ⊃(Kate controls ⟨modify,O4⟩)
Kate says ⟨modify,O4⟩
⟨modify,O4⟩.
♦
Exercise 13.4.1
Consider the discretionary-access control matrix in Table 13.4.
For each object O1 through O4, write the strict-integrity access-control policy for
the reference monitor that guards the object. Given the integrity levels in Table 13.3,
formally prove that (1) Carol can observe and modify objects O1 and O3, respec-
tively, and (2) that Kate can observe and modify objects O1 and O4, respectively.
Exercise 13.4.2
Formally derive the inference rules from Example 13.11 that de-
scribe the behavior of Kate’s reference monitor, Kate, and O4’s reference monitor.
Exercise 13.4.3
Suppose everything is the same as described in Example 13.11
with the following exception: Carol invokes Kate to modify O3 instead of O4. Show
how the direct access policies work to prevent Kate from accessing O3. Be as precise,
detailed, and formal as possible.
13.5
Lipner’s Integrity Model
In the previous sections, we reviewed the Bell–La Padula and Biba models for
conﬁdentiality and integrity. In this section, we introduce Lipner’s integrity model
(Lipner, 1982), which builds upon both models. Lipner developed his model for so-
called production systems, which depend on software to deliver commercial services
(e.g., banking, data processing, or inventory control). In such systems, data integrity
is paramount: imagine the chaos that would ensue if a bank’s account data became
corrupted or if an inventory-control program had incorrect information on what was
on store shelves and in warehouses.
Lipner’s model serves as an interesting case study for designers and veriﬁers who
must develop their own policies to meet design requirements. To be explicit, Lip-
ner does not provide an abstract model such as Bell–La Padula or Biba. Rather, he
speciﬁes a collection of requirements for commercial integrity and then employs a
combination of the Bell–La Padula and Biba models to identify speciﬁc conﬁdential-
ity and integrity levels suitable for meeting those requirements.

Conﬁdentiality and Integrity Policies
277
13.5.1
Commercial Integrity Requirements
To understand the type of commercial scenario that Lipner’s work addresses, con-
sider a ﬁnancial-services company that offers online banking and credit services.
Bank customers use programs supplied by the bank to access and manipulate their
accounts online: they can view account information, pay bills, transfer funds be-
tween accounts, and so on. Thus, customers are users of bank services, and the data
and programs they use comprise the production domain. Unseen by customers are
the application developers and system programmers, who develop (respectively) the
applications software and the underlying IT infrastructure used by customers. These
people work in the development domain.
Experience shows that it is essential to isolate the production domain from the
development domain. Otherwise, programs under development could destroy or
corrupt customer account information, thereby destroying the bank’s reputation and
business. Before programs can be moved from the development domain into the
production domain, they must be thoroughly reviewed and tested. The decisions
and processes used to move a program out of development and into production are
managed by system controllers, managers, and auditors.
Lipner proposed the following commercial-integrity requirements for such pro-
duction systems:
1. Users can use production programs and data, but they cannot write their own
programs to operate on the production databases.
2. Application developers and system programmers perform their development
and testing in a special test environment; they have no access to the production
programs or databases. Should they require such access, a special process will
provide them with copies of the information they need.
3. Only system controllers may change the status of a program from development
to production.
4. The act of changing a program’s status from development to production is
audited.
5. Managers and auditors have access to the system state and audit logs.
In the next two subsections, we examine Lipner’s two approaches to meeting these
ﬁve requirements: the ﬁrst approach uses only the Bell–La Padula model, while the
second uses a combination of both Bell–La Padula and Biba’s strict integrity.
13.5.2
Commercial Integrity via Bell–La Padula
Lipner’s requirements for commercial integrity explicitly identify ﬁve classes of
subjects: (1) production-environment users (e.g., customers); (2) application pro-
grammers; (3) system programmers; (4) system controllers; and (5) system man-
agers or auditors. The requirements also implicitly identify seven classes of objects:

278
Access Control, Security, and Trust: A Logical Approach
(1) production data (e.g., customer account information); (2) production code (i.e.,
application software available to users); (3) system programs in the production envi-
ronment (e.g., database query engines or key-distribution software invoked by pro-
duction code); (4) application programs in development; (5) system programs and
data in development; (6) software tools used by development personnel (i.e., pro-
grammer support software, such as compilers); and (7) audit logs.
Table 13.5 gives the desired discretionary access-control matrix for this situation.
It is straightforward to verify that the matrix meets the imposed commercial-integrity
requirements:
1. Users may use production code and system programs, and they may modify
production data. However, they are unable to create or modify production
code, system programs, or anything in the development environment.
2. Application programmers and system programmers have no access to produc-
tion data and code; however, they may use (but not modify) system programs
in the application environment, as well as software tools. Application pro-
grammers can read and modify application programs within the development
environment, and system programmers can read and modify system programs
within the development environment.
3. System controllers—who need to be able to install programs within both the
production and development environments—have read and write permissions
for all programs and data in both environments.
4. All subjects’ actions are written to the audit logs, including system controllers’
installations of programs.
5. System managers and auditors—who must monitor the entire system’s state—
have read access to all aspects of the system, including the audit logs.
Lipner’s goal was to identify conﬁdentiality levels for the subjects and objects
in such a way that (1) the desired access-control matrix is consistent with Bell–La
Padula and (2) subjects have as many permissions as possible. Towards this end, he
used compound levels built from the set of classiﬁcation levels L = {AM, SL} and
the set of categories Cat = {PD, PC, D, SD, T}.
Intuitively, the classiﬁcation level AM (short for “audit manager”) is for system
managers and auditors; the classiﬁcation level SL (“system low”) is for all other
subjects in the system. Because system managers and auditors have greater privileges
than the other subjects, the partial order ≤on L is deﬁned such that SL ≤AM.
The categories reﬂect the types of data and programs that can be used or modiﬁed
by users, application programmers, and system programmers: production data (PD),
production code (PC), development code and test data (D), system programs in de-
velopment (SD), and software tools (T). Categories are assigned to subjects based on
the types of data and code that they can modify, as well as the programs that they
can use. For example, ordinary users should be able to modify production data and
to use production code; therefore, they are assigned the set of categories {PD, PC}.

Conﬁdentiality and Integrity Policies
279
Object
Subject
Prod
Data
Prod
Code
System
Prog
Dev
App
Prog
Dev
Sys
Prog
SW
Tools
Audit
Logs
Production
Users
read,
write
read
read
-
-
-
write
Application
Prgmers
-
-
read
read,
write
-
read
write
System
Prgmers
-
-
read
-
read,
write
read
write
System
Controllers
read,
write
read,
write
read,
write
read,
write
read,
write
read,
write
write
Managers
& Auditors
read
read
read
read
read
read
read,
write
Table 13.5: Access-control matrix for commercial integrity via Bell–La Padula
Subject Class
Conﬁdentiality Level
Ordinary user
(SL,{PD,PC})
Application programmer
(SL,{D,T})
System programmer
(SL,{SD,T})
System controller
(SL,{PD,PC,D,SD,T}) and downgrade privilege
System management or auditor
(AM,{PD,PC,D,SD,T})
Table 13.6: Conﬁdentiality levels for subjects
Similarly, system managers and auditors should be able to read every sort of data and
code; they are therefore assigned the full set of categories {PD, PC, D, SD, T}.
Table 13.6 shows the conﬁdentiality levels assigned to each kind of subject. Note
that subjects with classiﬁcation level SL—such as ordinary users and application
programmers—have the lowest possible classiﬁcation level. Therefore, under Bell–
La Padula’s *-property (i.e., “no write down”), their ability to modify a given object
is limited only by the set of categories associated with that object. For example,
application programmers are assigned the set of categories {D, T}: they can modify
any object whose associated categories include both D and T. System managers and
auditors are further limited by their classiﬁcation level AM: they cannot write to
objects whose classiﬁcation level is SL.
In contrast, conﬁdentiality levels are assigned to objects based on the levels of
those subjects who must be able to read them. In particular, objects are assigned
conﬁdentiality levels as high as possible, subject to the constraint imposed by the
simple security condition of Bell–La Padula (i.e., “no read up”): no object’s con-
ﬁdentiality level can be higher than that of a subject who must be able to read

280
Access Control, Security, and Trust: A Logical Approach
Object Class
Conﬁdentiality Level
Production data
(SL,{PD,PC})
Production code
(SL,{PC})
Systems programs
(SL,{})
Development code & test data
(SL,{D,T})
System programs in modiﬁcation
(SL,{SD,T})
Software tools
(SL,{T})
System and application audit logs
(AM,{PD,PC,D,SD,T})
Table 13.7: Conﬁdentiality levels for objects
it. For example, software tools must be readable by both application programmers
(who have conﬁdentiality level (SL,{D, T})) and system programmers (who have
conﬁdentiality level (SL,{SD, T})). Consequently, the highest conﬁdentiality level
that can be assigned to software tools is (SL,{T})). Similarly, audit logs must be
readable by system managers and auditors, and hence they are assigned the level
(AM,{PD, PC, D, SD, T}). Table 13.7 shows the conﬁdentiality levels for each class
of object.
One class of subjects requires special mention: system controllers have the spe-
cial role of moving programs out of development and into production. Consequently,
they need full read and write access to all ﬁles except audit logs. Satisfying both the
simple security condition and the *-property, however, means that a subject can have
both read and write access to a given object only if the subject’s level exactly matches
the object’s level. We have already seen that production data, production code, devel-
opment code, software tools, and system programs in modiﬁcation all have different
conﬁdentiality levels. How, then, do we assign an appropriate level to system con-
trollers? The answer is that system controllers—and only system controllers—must
be allowed to downgrade their classiﬁcation level. Therefore, system controllers are
assigned the conﬁdentiality level (SL,{PD,PC,D,SD,T}), along with the privilege to
downgrade their levels as necessary.
It is straightforward to translate the contents of the Table 13.5 access-control ma-
trix into the access-control logic. For example, the ability of users to read and write
production data is given by the following two statements:
slev(Production Data) ≤s slev(User) ⊃(User controls ⟨read,Production Data⟩),
slev(User) ≤s slev(Production Data) ⊃(User controls ⟨write,Production Data⟩).
Although this scheme satisﬁes the ﬁve commercial-integrity requirements initially
proposed, Lipner points out a potential drawback: it is unclear how special-purpose
software (such as a database-repair program) ﬁts into this scheme. Such programs
must be able to operate on production data but cannot be under the control of users.
He addresses this shortcoming via a combination of Bell–La Padula conﬁdentiality
and Biba strict integrity, which we explore in the next subsection.

Conﬁdentiality and Integrity Policies
281
Objects
Subjects
Prod
Data
Prod
Code
System
Prog
Dev
App
Prog
Dev
Sys
Prog
SW
Tools
Audit
Logs
Repair
Code
Production
Users
read,
write
read
read
-
-
-
write
-
Application
Prgmrs
-
-
read
read,
write
-
read
write
-
System
Prgmrs
-
-
read
-
read,
write
read
write
-
System Con-
trollers
read,
write
read,
write
read,
write
read,
write
read,
write
read,
write
write
read,
write
Managers &
Auditors
read
read
read
read
read
read
read,
write
read
Repair
read,
write
read
read
-
-
-
write
read
Table 13.8: Access-control matrix for both conﬁdentiality and strict integrity
13.5.3
Commercial Integrity via Bell–La Padula and Strict
Integrity
A major objective for combining the Bell–La Padula model with Biba’s strict in-
tegrity is to allow the use of special-purpose software to repair errors or inconsis-
tencies in production databases. Lipner states this additional requirement as follows
(Lipner, 1982, page 7):
Special-purpose application software shall be provided to effect “data
base repair” on the production data base. This software may be used by
members of the application programmer or system control population
under special circumstances.
The desired discretionary access-control matrix for this new situation appears in
Table 13.8. Note that this table is identical to Table 13.5, except for the addition of
a new class of subjects (Repair) and a new class of objects (Repair Code). Lipner
identiﬁed a combination of conﬁdentiality and integrity levels that, when suitably
assigned to subjects and objects, permits this access-control matrix to be consistent
with both Biba’s strict integrity and the Bell–La Padula conﬁdentiality policies.
For example, Table 13.8 shows that system managers have both read and write
access to the audit logs. Granting read access to system managers requires that the
following two properties hold:
slev(Audit Log) ≤s slev(System Manager),
ilev(System Manager) ≤i ilev(Audit Log).
The ﬁrst property is the mandatory access-control condition for read operations in
Bell–La Padula (i.e., “no read up”), while the second condition is the “no read down”

282
Access Control, Security, and Trust: A Logical Approach
condition for strict integrity. The combination of these two requirements can be
expressed in the logic as follows:
slev(Audit Log) ≤s slev(System Manager) ⊃
ilev(System Manager) ≤i ilev(Audit Log) ⊃
(System Manager controls ⟨read,Audit Log⟩).
Similarly, for write operations, we have a combination of Bell–La Padula’s “no write
down” and Biba’s “no write up” requirements:
slev(System Manager) ≤s slev(Audit Log) ⊃
ilev(Audit Log) ≤i ilev(System Manager) ⊃
(System Manager controls ⟨write,Audit Log⟩).
Lipner’s revised model handles conﬁdentiality levels in a very similar manner to
the original model. In particular, the conﬁdentiality classiﬁcations from the ﬁrst
model are retained: Lcon f = {AM, SL}, with SL ≤s AM. However, only three conﬁ-
dentiality categories are required: Catconf = {Pc, Dc, SDc}. These categories loosely
correspond to the types of code and the domains in which they may reside: pro-
duction data and code (Pc), application code under development (Dc), and system
programs under development (SDc).
As in the initial model, the higher level AM (“audit manager”) is assigned to man-
agers and auditors, while the lower level SL (“system low”) is assigned to all other
subjects. Likewise, categories are assigned to subjects based on the types of data and
programs that they can modify and use. Thus, for example, application program-
mers are assigned the category Dc, because they can modify application code under
development. In contrast, system managers and auditors are assigned the set of cat-
egories {Pc, Dc, SDc}, because they require read access to production data and code,
application code under development, and system programs under development.
Conﬁdentiality levels are then assigned to objects based on the conﬁdentiality lev-
els of the subjects who can read them. In particular, objects are assigned as high a
level as possible, subject to the “no read up” constraint imposed by Bell–La Padula.
For example, software tools should be readable by everyone, regardless of their con-
ﬁdentiality level: thus software tools must be assigned the lowest possible conﬁden-
tiality level (i.e., (SL,{})). In contrast, audit logs should be read only by system
managers and auditors, and hence audit logs are assigned the conﬁdentiality level
(AM,{Pc, Dc, SDc}).
The new aspect of this model is the assignment of integrity levels that prevent
users and programs from corrupting data and programs at higher integrity levels.
The set of integrity classiﬁcations is Lint = {SPi, OI, SLi}, where
SLi ≤i OI ≤i SPi.
System programs and repair programs are given the highest integrity classiﬁcation
(SPi), and thus can be modiﬁed only by subjects with sufﬁciently high levels. Produc-
tion code and software tools—which can be modiﬁed by a certain segment of users—
are given the intermediate OI (“operational infrastructure”) classiﬁcation. Audit logs

Conﬁdentiality and Integrity Policies
283
Level
Object
Conﬁdentiality
Integrity
Production Data
(SL,{Pc})
(SLi,{Pi})
Production Code
(SL,{Pc})
(OI,{Pi})
System Programs
(SL,{})
(SPi,{Pi, Di})
Development Code & Test Data
(SL,{Dc})
(SLi,{Di})
System Programs in Modiﬁcation
(SL,{SDc})
(SLi,{Di})
Software Tools
(SL,{})
(OI,{Di})
Audit Logs
(AM,{Pc, Dc, SDc})
(SLi,{})
Repair
(SL,{Pc})
(SPi,{Pi})
Table 13.9: Conﬁdentiality and integrity levels for objects
are given the lowest integrity classiﬁcation (SLi), because they must be writable by
everyone. Similarly, production data is classiﬁed at the SLi level, because data can
be written by users at the lowest level.
The set of integrity categories is Catint = {Pi, Di}, where the categories are used
to distinguish between the production and development domains. The assignment of
objects to these integrity categories is intuitive. Production code, production data,
and repair programs are assigned the Pi (“production integrity”) category, while ap-
plication code, system programs in modiﬁcation, and software tools are assigned
the Di category (“development integrity”). The result is that programs in develop-
ment are effectively isolated from production code and production data. System
programs—which are used by both users and developers—are assigned both cate-
gories. In contrast, audit logs (which must be writable by everyone) are assigned the
lowest integrity level (SLi,{}). Table 13.9 summarizes both the conﬁdentiality and
integrity levels for objects.
Integrity levels are then granted to subjects based on the objects that they must be
able to modify: subjects are granted as low an integrity level as possible, subject to
the “no write up” constraint imposed by Biba’s strict-integrity policy. For example,
application programmers must be able to write to both development code (integrity
level (SLi,{Di})) and the audit logs (integrity level (SLi,{})): the lowest integrity
level that allows write access to both items is (SLi,{Di}).
Table 13.10 shows the assignment of conﬁdentiality and integrity levels to sub-
jects. Given this assignment and that of Table 13.9, it is straightforward to check that
all operations in Table 13.8 are permitted by the Bell–La Padula model. However,
two aspects of this assignment deserve special mention.
First, system controllers alone have the ability to downgrade their integrity or con-
ﬁdentiality level by altering their categories, which enables them to match integrity
and conﬁdentiality levels with any objects other than audit logs. Consequently, sys-
tem controllers are able to read and write to objects as necessary to move them from
development to production.
Second, people operating in the repair function have exactly the same integrity
and conﬁdentiality levels as production users. However, the discretionary access

284
Access Control, Security, and Trust: A Logical Approach
Level
Subject
Conﬁdentiality
Integrity
Production Users
(SL,{Pc})
(SLi,{Pi})
Application Programmers
(SL,{Dc})
(SLi,{Di})
System Programmers
(SL,{SDc})
(SLi,{Di})
Sys Controllers (incl. downgrade privilege)
(SL,{Pc, Dc, SDc})
(SPi,{Pi, Di})
Managers & Auditors
(AM,{Pc, Dc, SDc})
(SLi, {})
Repair
(SL,{Pc})
(SLi,{Pi})
Table 13.10: Conﬁdentiality and integrity levels for subjects
matrix in Table 13.8 shows that production users and repair people do not have the
same privileges: repair people have the additional privilege of reading repair code.
Although the mandatory access-control policies would permit production users to
read repair code, doing so is not a requirement for the system: users have no need to
have that privilege. Thus, in accordance with the principle of least privilege, users
are denied discretionary read access to repair code.
Exercise 13.5.1
Consider the access-control matrix in Table 13.5.
a. Using the access-control logic, formulate access-control policy statements for
production users that reﬂect both the mandatory access-control requirements
of Bell–La Padula and the discretionary access-control matrix.
b. Using the access-control logic, devise a theorem that shows under what con-
ditions access is granted for production users. Give a formal proof for each
theorem.
Exercise 13.5.2
Consider the access rights of system controllers, as given by the
access-control matrix in Table 13.5.
a. Describe precisely how system controllers are able to read and write to all
objects (with the exception of audit logs).
b. Downgrading clearance levels is limited to system controllers only. Describe
what undesirable effects would occur if other subjects could downgrade their
clearance level.
Exercise 13.5.3
a. Consider the assignment of conﬁdentiality levels to subjects and objects given
in Tables 13.10 and 13.9. Using the access-control matrix in Table 13.8 as a
guide, devise an access-control matrix that shows the maximum privileges al-
lowable to subjects when only the conﬁdentiality levels of subjects and objects
are considered.

Conﬁdentiality and Integrity Policies
285
b. Consider the assignment of integrity levels to subjects and objects given in Ta-
bles 13.10 and 13.9. Using the access-control matrix in Table 13.8 as a guide,
devise an access-control matrix that shows the maximum privileges allowable
to subjects when only the integrity levels of subjects and objects are considered
c. Consider the privileges of subjects obtained by intersecting the above two ta-
bles. How and why does this table differ from the access-control matrix in
Table 13.8?
Exercise 13.5.4
Extend the access-control logic to accommodate the use of both
conﬁdentiality and integrity levels. Devise the extensions to both the syntax and
semantics.
13.6
Summary
In this chapter, we introduced compound levels for both conﬁdentiality and in-
tegrity, based on the combined notions of classiﬁcation levels and categories. We
also introduced the related notions of level downgrading, sanitization, and contain-
ment.
We reviewed both the Bell–La Padula model for conﬁdentiality and Biba’s strict-
integrity model. We also described in detail Lipner’s approach to commercial in-
tegrity: this approach relies on a combination of Bell–La Padula’s conﬁdentiality
model and Biba’s strict-integrity model. Lipner’s work provides a good illustration
of how levels can be assigned to subjects and objects to satisfy a model’s policy
constraints.
The learning outcomes for this chapter appear in Figure 13.3.
13.7
Further Reading
Bell and La Padula’s work on conﬁdentiality is contained in two reports (Bell
and La Padula, 1975) and (Bell and La Padula, 1973). An alternate approach for
conﬁdentiality in commercial contexts is given by the Chinese Wall model (Brewer
and Nash, 1989), which is used to handle conﬂicts of interests in situations where
staff members in the same business (e.g., a law ﬁrm or accounting ﬁrm) work for
clients who are competitors. The Chinese Wall policy places competitors in the
same conﬂict-of-interest class. Employees are then restricted to accessing at most
one member in the same conﬂict-of-interest class: once an employee has accessed
one member of a conﬂict-of-interest class, all other members of the class are off

286
Access Control, Security, and Trust: A Logical Approach
limits. In this way, access restrictions in the Chinese Wall become more restrictive
over time, whereas restrictions in Bell–La Padula do not.
In addition to strict integrity, Biba’s original report (Biba, 1975) proposed two re-
lated and alternative policies: the low-watermark policy and the ring policy. Both
policies impose the same restrictions on modiﬁcations and invocations that strict
integrity does, but they impose different restrictions upon observations. The low-
water-mark policy allows a subject to observe objects below its integrity level, pro-
vides that the subject’s integrity level is reduced to that of the observed object. The
ring policy imposes no mandatory access controls on subjects’ observations of ob-
jects; as a result, there are fewer built-in integrity protections if a subject unwisely
chooses to read an untrustworthy source.

Conﬁdentiality and Integrity Policies
287
FIGURE 13.3 Learning outcomes for Chapter 13
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
• When given the conﬁdentiality- or integrity-level assignments and the
discretionary access-control matrix of a system, you should be able to
calculate whether or not a read or write access request should be permit-
ted.
• Express conﬁdentiality policies consistent with the Bell–La Padula
model in the access-control logic.
• Express integrity policies consistent with the Biba strict-integrity model
in the access-control logic.
Synthesis
• When given an access-control scenario involving conﬁdentiality and/or
integrity, you should be able to deﬁne conﬁdentiality and/or integrity
levels for subjects and objects, and devise a discretionary access-control
matrix that accurately reﬂects the scenario.
• When given an access-control scenario that involves the Bell–La Padula
model, you should be able to formalize the scenario, identify all neces-
sary trust assumptions, and formally justify the granting of a request.
• When given an access-control scenario that involves the Biba strict-
integrity model, you should be able to formalize the scenario, identify
all necessary trust assumptions, and formally justify the granting of a
request.
Evaluation
• When given a set of policy deﬁnitions alleged to be consistent with the
Bell–La Padula or strict-integrity models, you should be able to judge
whether they are consistent and (if not) identify the inconsistencies.


Chapter 14
Role-Based Access Control
In the previous chapter, we examined conﬁdentiality and integrity policies, which
permit access based in part on principals’ and objects’ classiﬁcation levels. In many
organizations, however, access-control policies are based instead upon employees’
job functions: for example, programmers must have both read and write access to
relevant source code, while salespeople may require access to internal marketing
reports. Role-based access control (RBAC) was designed to simplify the task of
managing such policies by explicitly introducing a notion of roles, which serve as in-
termediate links between users (i.e., employees) and permissions (i.e., their required
access rights).
In this chapter, we introduce the basic components of RBAC, which are based on
the related but independent notions of users, roles, and permissions. We also discuss
notions of separation of duty, which provide constraints that can limit the potential
for fraud or conﬂicts of interest. Finally, we show how the access-control logic can
be used to reason about access-control decisions within RBAC systems.
14.1
RBAC Fundamentals
The key idea behind RBAC is that an organization comprises many people (i.e.,
users), each of whom perform several job functions (i.e., roles). In turn, a person
performing a certain job function requires a collection of permissions to successfully
complete their tasks (e.g., an accountant must have the ability to read and modify the
accounting ledger). Therefore, at its core, an RBAC system can be described by the
following ﬁve sets:
• Users, the set of users
• Roles, the set of roles
• Perms, the set of permissions
• UA ⊆Perms×Roles, the user-to-role assignment (or simply user assignment)
• PA ⊆Perms×Roles, the permission-to-role assignment (or simply permission
assignment)
289

290
Access Control, Security, and Trust: A Logical Approach
The user-to-role assignment catalogs the associations between users and the roles
they perform: a pair (u,r) ∈UA indicates that user u has been explicitly assigned to
role r. Similarly, the permission-to-role assignment indicates which permissions are
associated with which roles: a pair (p,r) ∈UA indicates that permission p has been
explicitly assigned to role r.
The following example provides a simple RBAC description of a small restaurant.
Example 14.1
Consider a local restaurant with ﬁve employees (Pat, Mel, Lee, Sam, and Kim) and
four identiﬁed job functions (host, server, chef, and manager). At present, the restau-
rant management has assigned responsibilities as follows: hosts may seat customers
and take reservations; servers may take orders, serve food to customers, clear tables,
and accept payment from customers; chefs may cook food; and managers may offer
customers a free meal, in addition to anything that a host or server can do.
The restaurant’s operations can be described by the following RBAC components:
Users = {Pat,Mel,Lee,Sam,Kim},
Roles = {Host,Server,Chef,Manager},
Perms = {⟨seat,customer⟩, ⟨take,reservation⟩, ⟨take,order⟩, ⟨cook,food⟩,
⟨serve,food⟩, ⟨clear, table⟩, ⟨accept,payment⟩, ⟨offer,free meal⟩},
UA = {(Pat,Host), (Pat,Server), (Lee,Server), (Mel,Chef), (Mel,Server),
(Kim,Manager), (Sam,Manager), (Sam,Chef)},
PA = {(⟨seat,customer⟩,Host), (⟨take,reservation⟩,Host), (⟨cook,food⟩,Chef),
(⟨take,order⟩,Server), (⟨serve,food⟩,Server), (⟨clear,table⟩,Server),
(⟨accept,payment⟩,Server), (⟨offer,free meal⟩,Manager),
(⟨seat,customer⟩,Manager), (⟨take,reservation⟩,Manager),
(⟨take,order⟩,Manager), (⟨serve,food⟩,Manager),
(⟨clear,table⟩,Manager), (⟨accept,payment⟩,Manager) }.
♦
As the previous example illustrates, the same user (e.g., Pat) may be assigned
to multiple roles. Similarly, the same permission (e.g., ⟨seat,customer⟩) may be
assigned to multiple roles.
14.1.1
Role Inheritance
RBAC also introduces a notion of role inheritance, which reﬂects the fact that
some job functions subsume others. For example, an accounting supervisor may
need to perform the standard accountant duties, in addition to her supervisory duties.
To express such situations more concisely, RBAC incorporates a role-inheritance
(also known as a role-hierarchy or role-dominance) relation
⪰⊆Roles×Roles,

Role-Based Access Control
291
FIGURE 14.1 Sample Hasse diagram
T
@
@
@
   Y
   W
X
Z
@
@
@
M
U
which is a partial order on roles. Thus, the role-inheritance relation ⪰necessarily
satisﬁes the following properties:
• Reﬂexivity: For all roles r, r ⪰r (i.e., every role inherits itself).
• Transitivity: For all roles r1,r2,r3, if r1 ⪰r2 and r2 ⪰r3, then r1 ⪰r3.
• Anti-symmetry: For all roles r1,r2, if r1 ⪰r2 and r2 ⪰r1, then r1 = r2 (i.e.,
no cycles exist in the inheritance relation).
As with other partial orders, it is often convenient to present the role-inheritance
relation graphically as a Hasse diagram. As an example, Figure 14.1 presents a Hasse
diagram for the relation
⪰= {(r,r) | r ∈{M,T,U,X,Y,W,Z}}
∪{(X,Y),(X,W),(X,T),(X,Z),(M,Z),(M,T)}.
The primary purpose of the role-inheritance relation is to allow more succinct
descriptions of user and permission assignments. Towards this end, we introduce
two functions
auth users : Roles →P(Users),
auth perms : Roles →P(Perms),
which, when given a speciﬁc role, respectively return the sets of authorized users and
authorized permissions for that role:
auth users(r) = {u | ∃r′ ∈Roles.(r′ ⪰r and (u,r′) ∈UA)},
auth perms(r) = {p | ∃r′ ∈Roles.(r ⪰r′ and (p,r′) ∈PA)}.
Thus, the authorized users of a role r are all those users explicitly assigned (through
the user-to-role assignment) to a role that inherits r. In contrast, the authorized
permissions of a role r are all those permissions explicitly assigned (through the
permission-to-role assignment) to a role that r inherits.
It is straightforward to show that the following properties hold:

292
Access Control, Security, and Trust: A Logical Approach
• If r1 ⪰r2, then auth users(r1) ⊆auth users(r2).
• If r1 ⪰r2, then auth perms(r2) ⊆auth perms(r1).
In this sense, users “ﬂow down” the role hierarchy while permissions “ﬂow up.”
The following example shows how the judicious choice of a role-inheritance rela-
tion simpliﬁes the permission assignment for the restaurant of Example 14.1.
Example 14.2
Recall the restaurant from Example 14.1, and suppose the role-inheritance relation
⪰is given by the following Hasse diagram:
Host
   Server
Manager
Chef
The restaurant’s operations can be described by the following RBAC components
(the sets Users, Roles, Perms, and UA remain unchanged from Example 14.1):
Users = {Pat,Mel,Lee,Sam,Kim},
Roles = {Host,Server,Chef,Manager},
Perms = {⟨seat,customer⟩, ⟨take,reservation⟩, ⟨take,order⟩, ⟨cook,food⟩,
⟨serve,food⟩, ⟨clear, table⟩, ⟨accept,payment⟩, ⟨offer,free meal⟩},
UA = {(Pat,Host), (Pat,Server), (Lee,Server), (Mel,Chef), (Mel,Server),
(Kim,Manager), (Sam,Manager), (Sam,Chef)},
PA = {(⟨seat,customer⟩,Host), (⟨take,reservation⟩,Host), (⟨cook,food⟩,Chef),
(⟨take,order⟩,Server), (⟨serve,food⟩,Server), (⟨clear,table⟩,Server),
(⟨accept,payment⟩,Server), (⟨offer,free meal⟩,Manager)}.
Given these base sets, we can calculate the authorized users for the roles Host and
Manager as follows:
auth users(Host) = {u | ∃r′ ∈Roles.(r′ ⪰Host and (u,r′) ∈UA)}
= {u | (u,Host) ∈UA}∪{u | (u,Manager) ∈UA}
= {Pat,Kim,Sam},
auth users(Manager) = {u | ∃r′ ∈Roles.(r′ ⪰Manager and (u,r′) ∈UA)}
= {u | (u,Manager) ∈UA}
= {Kim,Sam}.

Role-Based Access Control
293
Both Kim and Sam are authorized users of the role Host by virtue of role inheritance.
Similarly, we can calculate the authorized permissions for the same two roles:
auth perms(Host) = {p | ∃r′ ∈Roles.(Host ⪰r′ and (p,r′) ∈PA)}
= {p | (p,Host) ∈PA}
= {⟨seat,customer⟩,⟨take,reservation⟩}
auth perms(Manager) = {p | ∃r′ ∈Roles.(Manager ⪰r′ and (p,r′) ∈PA)}
= {p | (p,Host) ∈PA}∪{p | (p,Server) ∈PA}
∪{p | (p,Manager) ∈PA}
= {⟨seat,customer⟩,⟨take,reservation⟩,⟨take,order⟩,
⟨serve,food⟩,⟨clear, table⟩,
⟨accept,payment⟩,⟨offer,free meal⟩}.
In particular, the Manager role inherits all of the permissions associated with the
Host and Server roles.
♦
Exercise 14.1.1
Consider the following collection of RBAC deﬁnitions:

Users = {Ian, Moe, Sal, Pam, Rob}
Perms = {a,b,c,d,e,g,h,k,w,y}
Roles = {A,B,C,D,E,G,H,K,W,Y}
UA = {(Ian,A),(Moe,C),(Moe,E),(Pam,Y),
(Pam,H),(Rob,B),(Sal,K)}
PA = {(a,A),(b,B),(c,C),(d,D),(e,E),(f,F),
(g,G),(h,H),(k,K),(w,W),(y,Y)}
The relation ⪰is given by the following Hasse diagram:
K
  C
D
  @
@
G
B
E
A
A
A
A
AA
Y
A
  W
H
Calculate the following:

294
Access Control, Security, and Trust: A Logical Approach
a. auth users(A).
b. auth users(E).
c. auth users(W).
d. auth perms(A).
e. auth perms(E).
f. auth perms(W).
Exercise 14.1.2
A small construction ﬁrm has the following staff:
• Alex, a carpenter who sometimes also ﬁlls in as supervisor
• Blake, an electrician
• Cal, a plumber’s apprentice who sometimes also ﬁlls in as ofﬁce manager
• Dana, a plumber
• Eddy, a carpenter’s apprentice and sometimes electrician’s apprentice
• Fran, a supervisor and sometimes electrician
Due to a combination of factors (city licensing statutes, ongoing labor negotia-
tions, and the like), the ﬁrm has developed a collection of guidelines to govern the
permissible work-related activities of its employees. No permissions other than those
explicitly stated in the following rules are granted:
• Carpenters may lay ﬂooring (ﬂoor), hang drywall (drywall), and install cabi-
nets (cabs).
• Electricians may install or reconﬁgure wiring (wire), test wiring, and install
outlets.
• Plumbers may lay pipes and install ﬁxtures.
• Apprentices are there to assist, plus they have speciﬁc duties related to their
basic trades: a carpenter’s apprentice may lay ﬂooring (ﬂoor), an electri-
cian’s apprentice may test wiring, and a plumber’s apprentice may lay pipes.
• Electricians and plumbers are also able to help design their respective sys-
tems.
• The supervisor can naturally supervise work, and assist when needed.
• The ofﬁce manager may schedule jobs, order supplies, and bill customers.
• All employees can review the current work plan.

Role-Based Access Control
295
Thus, for example, a carpenter may neither schedule jobs nor assist, but she can
always review the current work plan.
For the purposes of this question, let Users and Perms be deﬁned as follows, with
the obvious correspondence with the ﬁrm’s staff and work-related activities:
Users = {A,B,C,D,E,F}
Perms = {ﬂoor,drywall,cabs,wire,test,outlets,pipes,ﬁxtures,assist,
design,supervise,schedule,order,bill,review}
a. Deﬁne a set Roles of roles, along with a permission-to-role assignment PA,
a user-role assignment UA, and a role-dominance relation ⪰to accurately
capture the above guidelines.
To fully appreciate the implications of the role hierarchy, specify PA in such a
way that it maps each permission to exactly one role (i.e., PA should contain
exactly 15 pairs). Therefore, you will need to include roles that do not directly
correspond to particular job titles.
b. Using your deﬁnitions from Part a, calculate the following:
(a) The set of authorized permissions (i.e., auth perms(r)) for each role r ∈R
(b) The set of authorized users (i.e., auth users(r)) for each role r ∈R
14.1.2
Sessions
Although a speciﬁc user might be authorized for many roles, there are times that
she may wish to activate only a subset of roles, perhaps to avoid a conﬂict of interest
or simply to minimize unnecessary privileges. For example, suppose that Annie
is authorized for both the User and Superuser roles on her department’s computer
system. She may purposely elect to activate only the User role when she plans to
edit her personal ﬁles, to avoid the possibility of accidentally altering system ﬁles
that can be accessed only through the Superuser role.
RBAC provides the ability to activate only a subset of authorized roles through the
abstract notion of sessions. Intuitively, a session represents a period of time during
which a certain subset of a user’s authorized roles is activated. In the digital world,
a session might correspond to a user’s login session or to a speciﬁc open window or
shell. In the physical world—such as the restaurant described in Example 14.2—a
session might correspond to a speciﬁc work shift for an employee.
More formally, an RBAC system includes a set Sessions of sessions, in the same
way that it includes sets of users, roles, and permissions. Each session has precisely
one user associated with it, but zero or more roles associated with it.
When given a session s, the functions
user : Sessions →Users,
roles : Sessions →P(Roles)

296
Access Control, Security, and Trust: A Logical Approach
respectively return the user and the roles associated with s. As an important note,
these two functions must be consistent with one another: for any speciﬁc session s,
user(s) must be an authorized user for each of the roles in roles(s). That is, for all
sessions s, the following constraint must hold:
roles(s) ⊆{r ∈Roles | user(s) ∈auth users(r)}.
The following example extends the earlier restaurant example by introducing some
sessions that correspond to work shifts for particular employees.
Example 14.3
Recall the restaurant from Example 14.2, and consider the following work schedule:
• On Monday night, Pat works as a server, and Kim works as manager.
• On Tuesday afternoon, Kim works as both host and server; Pat has the day off,
and does not work at all.
This situation allows for the four sessions {s1,s2,s3,s4}, where s1 is Pat’s Monday-
night shift, s2 is Kim’s Monday-night shift, s3 is Pat’s Tuesday-afternoon shift, and
s4 is Kim’s Tuesday-afternoon shift.
The users and roles associated with each shift are as follows:
user(s1) = Pat,
roles(s1) = {Server},
user(s2) = Kim,
roles(s2) = {Manager},
user(s3) = Pat,
roles(s3) = /0,
user(s4) = Kim,
roles(s4) = {Host,Server}.
One can easily verify that, for each shift si,
roles(si) ⊆{r ∈Roles | user(si) ∈auth users(r)}.
♦
Exercise 14.1.3
Which of the following proposed sessions would be permitted un-
der the RBAC descriptions from Example 14.2? Explain your answers.
a. Session sa, where user(sa) = Mel and roles(sa) = {Chef,Server}
b. Session sb, where user(sb) = Lee and roles(sb) = {Server,Manager}
c. Session sc, where user(sc) = Sam and roles(sc) = {Chef,Server}

Role-Based Access Control
297
14.2
Separation of Duty
When developing security policies, a common approach is to divide critical op-
erations among two or more principals, so that no single principal can compromise
security. For example, banks often require two people to be present when ATM de-
posits are opened and processed. Likewise, health-insurance companies may require
multiple doctors to assert that a medical procedure is necessary before the company
will pay for the procedure.
RBAC provides a means for enforcing such divisions by introducing separation-
of-duty constraints. RBAC recognizes the following two forms of separation of duty:
1. Static separation of duty enforces constraints on the user assignment, so that
users are prevented from ever becoming authorized for conﬂicting roles.
2. Dynamic separation of duty enforces constraints on the roles that may be acti-
vated concurrently (i.e., within a single session): a user may be authorized for
conﬂicting roles, but he will be unable to activate them simultaneously.
We look at these two types of constraints in sequence.
14.2.1
Static Separation of Duty
To support static separation of duty, RBAC introduces a relation
SSD ⊆P(Roles)×(N−{0,1}),
whose purpose is to constrain how the user assignment UA is constructed.
Note that each element of SSD has the form (rs,n), where rs is a set of roles and
n is an integer greater than 1. Each such element reﬂects a particular separation-of-
duty constraint. In particular, for each (rs,n) ∈SSD, the following condition must
hold: for every subset rs′ of rs that contains at least n elements,
\
r∈rs′
auth users(r) = /0.
That is, each constraint (rs,n) ∈SSD mandates that no user may be authorized for n
or more roles from the set rs.
The following example illustrates how the SSD relation can be used to impose
static separation of duty.
Example 14.4
A small business is concerned about the possibility of embezzlement, so they have
split up the ﬁnancial duties among three separate job functions: only a manager can
approve expenses, only an account clerk can record expenses, and only a cashier can

298
Access Control, Security, and Trust: A Logical Approach
actually release funds. Thus, they have adopted the following roles, permissions, and
permission assignment:
Roles = {Manager,AccountClerk,Cashier},
Perms = {⟨approve,expense⟩,⟨record,expense⟩,⟨release,funds⟩},
PA = {(⟨approve,expense⟩,Manager),(⟨record,expense⟩,AccountClerk),
(⟨release,funds⟩,Cashier)}.
A signiﬁcant danger of embezzlement exists if a single person becomes autho-
rized for all three roles (Manager, AccountClerk, and Cashier). The following SSD
relation imposes a constraint to prevent such an occurrence:
SSD = {({AccountClerk,Cashier},2)}.
This relation prevents anyone from being an authorized user for more than one of
the roles AccountClerk and Cashier. Thus, for example, any user authorized for the
Cashier role could not also be authorized for the AccountClerk role.
♦
It is important to realize that a SSD constraint constrains the number of roles for
which a given user may be authorized. As a result, to verify that static separation-of-
duty constraints are met, it is important to consider not only the user-to-role assign-
ment but also the role hierarchy. The following example illustrates how the combi-
nation of role hierarchy and SSD constraints may introduce unintended effects.
Example 14.5
Recall the RBAC descriptions for the small business in Example 14.4, and suppose
that the role-inheritance relation ⪰has been deﬁned such that
Manager ⪰AccountClerk,
Manager ⪰Cashier.
It therefore follows that
auth users(Manager) ⊆auth users(AccountClerk),
auth users(Manager) ⊆auth users(Cashier),
and thus anyone authorized for the Manager role is also authorized for both the
AccountClerk and Cashier roles. Because the SSD constraint requires that
auth users(AccountClerk)∩auth users(Cashier) = /0,
it must also be the case that auth users(Manager) = /0, even though Manager does
not explicitly show up in the SSD relation.
♦
The next example elaborates on the possible interactions between role inheritance
and static separation of duty.

Role-Based Access Control
299
FIGURE 14.2 Role hierarchy for a CS/CE department
CS Fac
CE Fac UnTenured
Tenured
A
A
A
A



Chair
P&T VM
Fac
A
A
A
A
    H
H
H
H
H
H
H
H
P
P
P
P
P
P
P
P
P
P
P
Example 14.6
Consider a hypothetical academic department that houses both Computer Science
(CS) and Computer Engineering (CE) programs.
The department includes both
tenured and untenured faculty, and every faculty member is associated with at least
one of the two academic programs. In addition, the department has a chairperson
and a collection of voting members for a Promotion & Tenure (P& T) committee.
Thus, there are seven relevant roles for this example:
Roles = {Fac,Tenured,UnTenured,CS Fac,CE Fac,Chair,P&T VM}.
The role hierarchy appears in Figure 14.2: note that the roles Chair and P&T VM
both inherit the tenured-faculty role Tenured.
The standard academic situation is that no one can be both tenured and untenured,
and hence the roles Tenured and UnTenured should be mutually exclusive. Fur-
thermore, the department’s bylaws mandate that the department chair cannot be a
P&T voting member. These constraints can be represented by the following static
separation-of-duty relation:
SSD = {({Tenured,UnTenured},2),({P&T VM,Chair},2)}.
Note that these two constraints also prevent untenured faculty from being department
chair and from being voting members of the P&T committee, because the roles Chair
and P&T VM both inherit the Tenured role.
♦
14.2.2
Dynamic Separation of Duty
Although static separation of duty is easy to administer, in many cases it is not
ﬂexible enough. There are many situations in which a person may need to be autho-
rized for different roles (e.g., parent and doctor) that nevertheless should never be

300
Access Control, Security, and Trust: A Logical Approach
activated at the same time due to conﬂict-of-interest concerns. Dynamic separation
of duty provides a way to manage such constraints.
As with static separation of duty, RBAC supports dynamic separation of duty
through a relation
DSD ⊆P(Roles)×(N−{0,1}).
This relation imposes constraints on sessions and the roles activated therein. In par-
ticular, each element (rs,n) ∈DSD requires that no session can have n or more roles
from the set rs simultaneously activated. That is, for each (rs,n) ∈DSD, the follow-
ing condition must hold: for any session s, every subset rs′ of rs∩roles(s) contains
fewer than n elements.
The following example illustrates how the DSD relation can be used to impose
dynamic separation of duty.
Example 14.7
Recall the academic department from Example 14.6. As it turns out, the department’s
bylaws also require the P&T Committee to contain a ﬁxed number of representatives
from each of the CS and CE programs. Thus, for the purposes of P&T deliberations,
no faculty member can simultaneously represent both the CS and CE programs, al-
though she may be associated with both programs. This constraint can be represented
by the following dynamic separation-of-duty relation:
DSD = {({CS Fac,CE Fac,P&T VM},3)}.
Thus, no one may simultaneously act as CS faculty, CE faculty, and a P&T voting
member, although they may authorized for all three roles and may act in any two of
those roles simultaneously.
♦
Because the deﬁnition of dynamic separation constrains activated roles instead of
authorized roles, it does not implicitly incorporate the role hierarchy in the same
way that static separation of duty does. The difference is that the RBAC notion of
sessions permits a role to be activated without the roles that it inherits being activated,
whereas the same does not hold of role authorization. As a result, a DSD relation may
allow a role to be activated, even if it inherits two roles that cannot be simultaneously
activated. The following example illustrates this important difference.
Example 14.8
Recall the RBAC descriptions for the small business in Example 14.4, along with the
role-inheritance relation ⪰introduced in Example 14.5:
Manager ⪰AccountClerk,
Manager ⪰Cashier.
Suppose the SSD relation from Example 14.4 is instead replaced by the corre-
sponding DSD relation:
DSD = {({AccountClerk,Cashier},2)}.

Role-Based Access Control
301
In this case, it is permissible for a session s to be deﬁned such that
roles(s) = {Manager},
because every subset of roles(s) ∩{AccountClerk,Cashier} has fewer than 2 ele-
ments (in fact, the only such subset is the empty set).
Note, however, that the role Manager still possesses all of the permissions of
AccountClerk and Cashier. As a result, there is still a potential conﬂict on per-
missions, even though there is no conﬂict on roles.
♦
As the previous example illustrates, both static and dynamic separation of duty are
deﬁned in terms of roles. When avoiding potential conﬂicts of interests, however, the
real concern lies in the sets of permissions that a given user possesses. Therefore,
when designing an RBAC system, it is important to verify that permissions assigned
to non-considered roles do not create loopholes.
Exercise 14.2.1
A small accounting ﬁrm has recently ﬁred their security admin-
istrator for gross incompetence. They’re now looking to you to help them identify
some fundamental ﬂaws—such as session or separation-of-duty violations—in the
access-control system he set up.
Identify all of the various RBAC violations and inconsistencies inherent in the
following set of RBAC deﬁnitions:
Users = {Lyn,Mike,Nell,Opal,Per}
Perms = {p0, p1, p2, p3, p4, p5, p6, p7, p8, p9}
Roles = {A,B,C,D,E,F,G,H,J}
UA = {(Lyn,D),(Lyn,B),(Mike,F),(Nell,D),(Nell,F),(Mike,J),
(Per,H),(Per,J),(Opal,A),(Opal,H)}
PA = {(p1,E),(p2,E),(p3,D),(p4,C),(p5,G),(p6,J),
(p7,A),(p8,B),(p9,F),(p0,H)}
SSD = { ({B,C},2), ({G,A,F},3), ({B,J},2) }
DSD = { ({H,C,J},3), ({F,C,J},2) }
⪰
given by :
E
A
A
A
A
A
   B
@
@
@
   F
H
A
C
@
@
@
D   G
J

302
Access Control, Security, and Trust: A Logical Approach
The system’s current implementation also allows the sessions s1, s2, and s3 as
follows:
user(s1) = Opal, roles(s1) = {B,D}
user(s2) = Nell, roles(s2) = {G,F}
user(s3) = Per,
roles(s3) = {J,H}
Exercise 14.2.2
A small engineering ﬁrm has decided to purchase a system to help
manage their R&D projects. Here are the key features the company wants the system
to handle (do not assume any permissions or restrictions other than those explicitly
mentioned):
• Every project has some number of design engineers (DEs), test engineers
(TEs), and project managers (PMs) assigned to it.
• A project’s DEs can build prototypes, and the TEs can write tests for the
project.
• Project managers (PMs) can revise requirements at any time during the course
of their project.
• No engineer (i.e., DE or TE) may be assigned to more than two projects.
• No one can be both a DE and a TE on the same project. However, it is perfectly
acceptable for someone to be a DE for one project and a TE for a different
project.
• Because of the need for focus, PMs are prohibited from working on other
projects; however, a PM is allowed to work as an engineer on the same project
for which (s)he is PM.
• When project members log into the system, they should be prompted to indicate
which project they’ll be working on during that session; they are not allowed
to work on more than one project during a single session.
As part of the system’s initial test run, the ﬁrm is going to focus on its three major
projects: ALPHA, BRAVO, and CHARLIE. Thus, they’ve identiﬁed the following
roles and permissions for this system:
Roles = {PMA,PMB,PMC,DEA,DEB,DEC,TEA,TEB,TEC}
Perms = {buildA,buildB,buildC,testA,testB,testC,revA,revB,revC}
For example, buildA, testA, and revA are the permissions to (respectively) build a
prototype, write tests, and revise requirements for project ALPHA.
Your task:
Provide the following RBAC components to accurately meet and fulﬁll
all of the company’s desired features/criteria:
a. A role-hierarchy relation ⪰and a permission-assignment relation PA
You may add additional roles if you like.

Role-Based Access Control
303
b. A static separation-of-duty relation to capture static constraints
c. A dynamic separation-of-duty relation to capture dynamic constraints
Exercise 14.2.3
A local law ﬁrm wants to buy a system to help them keep track
of their legal cases. Listed below are the key features the practice wants the system
to handle (do not assume any permissions or restrictions other than those explicitly
mentioned):
• The legal staff includes lawyers (both partners and associates) and paralegals.
The three groups are mutually disjoint.
• All members of the legal staff can research cases.
• Associates can depose witnesses.
• All lawyers can present cases in court.
• Partners can reject prospective clients.
• The law ﬁrm handles both criminal defense and civil cases.
• The law ﬁrm occasionally handles pro bono cases (i.e., cases taken on for the
public good for which no payment is required). Pro bono cases may be either
criminal defense or civil cases.
• No partner may be assigned to more than one pro bono case.
• No paralegal may be assigned to more than three cases.
• Lawyers cannot double-bill their time to multiple cases: that is, at any moment
in time (i.e., session), lawyers can work on at most one case.
As part of the system’s initial test run, the ﬁrm is going to focus on its four major
cases:
• The Anderson case is a pro bono, civil case.
• The Brady case is a pro bono, criminal defense case.
• The Cortland case is a civil case (but not pro bono).
• The Derby case is a criminal defense case (but not pro bono).
To date, the following roles and permissions have been identiﬁed:
Roles = {Partner, Assoc, ParaLegal, ProBono, Civil, CrimDef, A, B, C, D}
Perms = {research, depose, present, reject }

304
Access Control, Security, and Trust: A Logical Approach
Your task:
Provide the following RBAC components to accurately meet and fulﬁll
all of the company’s desired features/criteria:
a. A role-hierarchy relation ⪰and a permission-assignment relation PA.
You may add additional roles if you like.
b. A static separation-of-duty relation to capture static constraints
c. A dynamic separation-of-duty relation to capture dynamic constraints
14.3
Representing RBAC Systems in the Logic
Given an RBAC description of a system, it is useful to be able to justify for-
mally the resulting access-control decisions that occur. Fortunately, it is straightfor-
ward to translate the relevant aspects of an RBAC description into our access-control
logic. In this section, we introduce small extensions to the logic—along with a proce-
dure for translating RBAC descriptions into the logic—that support reasoning about
RBAC systems.
14.3.1
RBAC Extensions to the Logic
We start by extending the syntax of the logic to accommodate statements that
express equality among principals:
Form ::= (Princ = Princ).
As with controls and reps , this new syntax is syntactic sugar for a construction that
already exists in the logic:
P = Q def
= P ⇒Q ∧Q ⇒P.
Consequently, its Kripke semantics is given as follows, for any Kripke structure M :
EM [[P = Q]] = EM [[P ⇒Q ∧Q ⇒P]]
= EM [[P ⇒Q]] ∩EM [[Q ⇒P]]
=
(
W,
if J(P) = J(Q)
/0,
otherwise.
Figure 14.3 presents three rules that are particularly useful for reasoning about
principal equality in the context of RBAC. The ﬁrst rule (Principal Equality) states
that, if P = Q, then any occurrences of P in a formula ϕ can safely be replaced by Q.
This rule is the principal-speciﬁc analogue to the Equivalence rule that allows one to
safely replace one formula in a larger expression by an equivalent one.

Role-Based Access Control
305
FIGURE 14.3 Logical rules regarding principal equality
Principal Equality
P = Q
ϕ[P/A]
ϕ[Q/A]
Distributivity of |
P | (R1 & ··· & Rk) = (P | R1) & ··· & (P | Rk) (k ≥1)
Quoting Simpliﬁcation
P | (Q & R) says ϕ
P | Q says ϕ
The second rule (Distributivity of | ) states that quoting ( | ) distributes over prin-
cipal conjunction ( & ). All three rules are sound, the proofs of which are left as
exercises; in fact, the third rule is derivable from existing rules. There are additional
rules related to principal equality that can be useful in general; because they are less
directly applicable to the discussion of RBAC, however, we leave them for exercises.
Exercise 14.3.1
Prove the soundness of the Principal Equality inference rule.
Exercise 14.3.2
Prove the soundness of the Distributivity of | inference rule.
Exercise 14.3.3
Give a formal proof for the derived inference rule Quoting Simpli-
ﬁcation.
Exercise 14.3.4
Prove the soundness of the following inference rule:
Commutativity of &
P & Q = Q & P
Exercise 14.3.5
Prove the soundness of the following inference rule:
Associativity of &
P & (Q & R) = (P & Q) & R
Exercise 14.3.6
Prove the soundness of the following inference rule:
Associativity of |
P | (Q | R) = (P | Q) | R
14.3.2
Translating RBAC into the Logic
We are now ready to consider the procedure for translating RBAC descriptions into
the logic. In particular, we are interested in those aspects of RBAC that provide the
basis for determining whether or not an access-control decision should be granted:
roles and role inheritance, the permission and user assignments, and sessions. We
consider each of these items in turn.

306
Access Control, Security, and Trust: A Logical Approach
Roles
In the logic, roles are simply principals, and role inheritance is represented
by the speaks-for (⇒) relation. Speciﬁcally, we translate R2 ⪰R1 into the logical
statement R2 ⇒R1.
Recall that an important aspect of role inheritance is that it is a partial order; the
logic must support reasoning about consequences of this property. Fortunately, the
speaks-for relation is well suited for this purpose: the Idempotency and Transitivity
of ⇒inference rules account for role-inheritance reﬂexivity and transitivity, and the
deﬁnition of principal equality accurately accounts for the anti-symmetry of role
inheritance.
Permission and user assignments
Each element (p,R) in the permission assign-
ment reﬂects a statement that the permission p is associated with role R (i.e., that R
is authorized to perform p). Consequently, each such (p,R) ∈PA can be translated
into the logical statement
R controls p.
Furthermore, every authorized user of R (as determined by the combination of the
user assignment and role inheritance) is authorized to use role R to request to perform
p (i.e., an authorized user can be viewed as an ofﬁcial representative of the role
R). Consequently, the single element (p,R) also induces, for each user X in the set
auth users(R), the following statement:
X reps R on p.
Sessions
Every access request in an RBAC system occurs in the context of a par-
ticular session, which in turn has an associated set of roles activated. For example,
consider a particular session s, in which
user(s) = P,
roles(s) = {R1,··· ,Rk}.
Any request ϕ made during session s can be expressed as follows:
P | (R1 & ··· & Rk) says ϕ.
That is, P asserts the activated roles R1,...,Rk when making the request ϕ.
The following small example illustrates the translation procedure, along with the
resulting formal justiﬁcation to grant an RBAC request.
Example 14.9
A small academic department has set up the following RBAC deﬁnitions for an elec-
tronic system to support two users (Dora and Liu), two roles (department chair and
faculty member), and two permissions (the abilities to read student grades and to
assign a course instructor):
UA = {(Dora,Chair),(Liu,Faculty)},
PA = {(readGrades,Faculty),(assignInstructor,Chair)},
⪰= {(Chair,Chair),(Chair,Faculty),(Faculty,Faculty)}.

Role-Based Access Control
307
Furthermore, let sD be a login session in which Dora activates the single role Chair.
This RBAC system can be expressed in the logic as follows:
1. Role hierarchy
The role Chair inherits the role Faculty, which results in the following logical
expression:
Chair ⇒Faculty.
Note that, technically speaking, one should also specify the following two
statements:
Chair ⇒Chair,
Faculty ⇒Faculty.
However, these statements are both instances of the Idempotency of ⇒rule.
Hence, neither statement would ever have to appear as an assumption in any
formal analysis.
2. Permission and user assignments
The pair (readGrades,Faculty) ∈PA gives rise to the following collection of
statements:
Faculty controls readGrades,
Liu reps Faculty on readGrades,
Dora reps Faculty on readGrades.
Note that Dora ∈auth users(Faculty), even though the pair (Dora,Faculty)
does not explicitly appear in the user assignment UA.
Likewise, the pair (assignInstructor,Faculty) ∈PA gives rise to the following
collection of statements:
Chair controls assignInstructor,
Dora reps Chair on assignInstructor.
3. Sessions
Suppose Dora requests to read student grades as part of her session sD. This
request can be represented as follows:
Dora | Chair says readGrades.
Because the session sD has only one activated role (i.e., Chair), only one role
is quoted in the access request.
The proof in Figure 14.4 then provides a formal justiﬁcation for granting Dora’s
request to read student grades. Similar justiﬁcations are possible if one changes the
set of activated roles for session sD to either {Faculty} (resulting in a shorter proof)
or {Faculty,Chair} (resulting in a slightly longer proof).
♦

308
Access Control, Security, and Trust: A Logical Approach
FIGURE 14.4 Formal justiﬁcation to allow Dora to read student grades
1. Chair ⇒Faculty
Chair ⪰Faculty
2. Faculty controls readGrades
(readGrades,Faculty) ∈PA
3. Dora reps Faculty on readGrades
(readGrades,Faculty) ∈PA,
Dora ∈auth users(Faculty)
4. Dora | Chair says readGrades
Request in session sD
5. Dora ⇒Dora
Idempotency of ⇒
6. Dora | Chair ⇒Dora | Faculty
5,1 Monotonicity of |
7. Dora | Faculty says readGrades
6,4 Derived speaks for
8. readGrades
2,3,7 Reps
As a ﬁnal note, we point out that the separation-of-duty relations SSD and DSD do
not enter explicitly into the translation from RBAC to the access-control logic. Both
relations impose constraints on an RBAC policy, limiting either the user assignment
(via SSD) or allowable sessions (via DSD). Those constraints must be checked when
the user assignment is created or modiﬁed, or when a new session is activated. In
contrast, the logic is used to formally justify an access-control decision that occurs
due to a speciﬁc request in the context of a particular session: such requests neces-
sarily occur after the point in time when separation-of-duty constraints are relevant.
Exercise 14.3.7
Consider the following collection of RBAC deﬁnitions:
Users = {Del, Earl, Fred, Guy, Hal}
Perms = {write loan, read balance, approve loan, sell loan, accept deposit,
cash check, close acct, open acct, void transaction, ﬁre staff}
Roles = {Emp, Teller, AcctOfﬁcer, LoanOfﬁcer, MortgageOfﬁcer,
LoanSupervisor, TellerSupervisor, BranchManager}
UA = {(Del,Teller), (Earl,MortgageOfﬁcer), (Fred,TellerSupervisor),
(Fred,AcctOfﬁcer), (Guy,LoanSupervisor), (Hal,BranchManager)}
PA = {(read balance,Emp), (open acct,AcctOfﬁcer), (close acct,AcctOfﬁcer),
(cash check,Teller), (accept deposit,Teller), (write loan,LoanOfﬁcer),
(void transaction,TellerSupervisor), (sell loan,MortgageOfﬁcer),
(approve loan,LoanSupervisor), (ﬁre staff,BranchManager)}
⪰= {(r,r),(BranchManager,r),(r,Emp) | r ∈R}
∪{(LoanSupervisor,MortgageOfﬁcer), (LoanSupervisor,LoanOfﬁcer)}
∪{(MortgageOfﬁcer,LoanOfﬁcer), (TellerSupervisor,Teller)}
Furthermore, let sH be a session such that
user(sH) = Hal,
roles(sH) = {Teller, MortgageOfﬁcer}.

Role-Based Access Control
309
a. Suppose you were to translate the entire RBAC description into the access-
control logic. List all of the translations that have the form
Fred reps X on p,
for some role X and some permission p.
b. Suppose that, during session sH, Hal requests to write loan.
Enumerate as formulas in the access-control logic precisely the portions of
the RBAC description necessary for determining that Hal’s request should be
granted; do not include anything that is unnecessary for the formal justiﬁca-
tion. (These formulas will serve as the only assumptions for your justiﬁcation
in part (c).)
c. Give a formal proof that justiﬁes granting Hal’s request.
Exercise 14.3.8
The company Industria has recently updated their entire comput-
ing infrastructure. These updates include the following:
• A public-key infrastructure
Industria assigned public/private keypairs to all employees and servers. These
public keys are certiﬁed by Industria itself. In addition, Industria’s own public
key (KI) is installed directly on all company machines.
• An RBAC-based project-reporting system
This system currently involves three projects (A, B, and C). Members of each
project are authorized to read and write that project’s report; only a project’s
manager(s) can ofﬁcially submit that project’s report. In addition, the com-
pany’s research director(s) can read the reports from any project. Thus, the
following RBAC components have been identiﬁed:
Roles = {MemberA,MemberB,MemberC,PMA,PMB,PMC,Director}
⪰= {(r,r) | r ∈R} ∪{(PMX,MemberX) | X ∈{A,B,C}}
Perms = {readX, writeX, submitX | X ∈{A,B,C}}
PA = {(readX,MemberX), (writeX,MemberX) | X ∈{A,B,C}}
∪{(submitX,PMX), (readX,Director) | X ∈{A,B,C}}
In the context of this new infrastructure, Cam has been assigned the public/private
keypair (Kc,K−1
c ). Cam has also been assigned (in the UA) as both a project man-
ager for project B and as a research director for Industria.
Over the weekend, Cam decides to log in from home to catch up on work. There
are two phases that happen:
Phase 1 (Initiate Session) Cam sends a digitally signed message to the RBAC server,
requesting to initiate a session in which the roles PMB and Director are both
activated.

310
Access Control, Security, and Trust: A Logical Approach
For the purpose of this phase, let
⟨session,Cam,{PMB,Director}⟩
denote the proposition “it is good to start a session with user Cam and acti-
vated roles {PMB,Director}.”
If this phase is successful, then the RBAC server will start up the requested
session, with Cam as the identiﬁed user and the requested roles active. Cam
can then proceed to the next phase.
Phase 2 (Do Work) In the context of the newly created session, Cam can now make
various requests of the system, including reading, writing, and submitting
project reports. These requests are interpreted as coming directly from Cam’s
session (as opposed to some cryptographic key).
Answer the following questions regarding the relevant certiﬁcations, credentials,
trust assumptions, and so on needed to analyze this situation. All answers should
be expressions in the access-control logic.
a. Give the collection of statements that capture the permission assignment PA
with respect to which roles are authorized to read the reports of project B.
b. Give the (minimal) collection of statements that capture the permission as-
signment and the implicit user assignment with respect to all of Cam’s various
role-based authorizations.
c. Suppose that, in Phase 1, the RBAC server recognizes Cam’s right to initiate
the requested session. Express this recognition of authority as an expression
in the access-control logic.
d. Express Cam’s Phase 1 request as an expression in the access-control logic.
e. What additional certiﬁcates, recognition of authority, and trust assumptions
regarding keys are necessary for the RBAC server to determine that the session-
initiation request of Phase 1 should be granted?
f. As part of Phase 2, Cam makes a request to write to project B’s report. Express
this request as an expression in the access-control logic.
g. What additional assumptions regarding policies, recognition of authority, and
the role hierarchy are necessary for the system to determine that Cam’s write
request in Phase 2 should be permitted?
14.4
Summary
In this chapter, we introduced role-based access control (RBAC), in which a user’s
access rights (i.e., permissions) are determined based on the roles for which that user

Role-Based Access Control
311
FIGURE 14.5 Learning outcomes for Chapter 14
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
• When given an RBAC description of a system, you should be able to
calculate the authorized users or authorized permissions of any role in
that system.
• Express RBAC permission assignments, user authorizations, and role hi-
erarchies in the access-control logic.
Synthesis
• When given an access-control scenario, you should be able to deﬁne a set
of roles, role hierarchy, user and permission assignments, and separation-
of-duty relations to accurately reﬂect the scenario.
• When given an access-control scenario that involves RBAC, you should
be able to formalize the scenario, identify all necessary trust assump-
tions, and formally justify the granting of a request.
Evaluation
• When given a set of RBAC deﬁnitions, you should be able to judge
whether they are consistent and (if not) identify the inconsistencies.
is authorized. RBAC was designed to reduce the administrative complexity asso-
ciated with large organizations’ access-control needs: the roles for which a user is
authorized can be adjusted as that user acquires or sheds different job responsibilities,
and permissions associated with a particular role can also be adjusted as necessary.
The essential RBAC components include user assignments, permission assign-
ments, and the role-inheritance relation. RBAC also supports static and dynamic
separation-of-duty constraints, which can be used to limit potential sources of collu-
sion or conﬂicts of interest.
We also showed how the access-control logic can be used to reason about access
requests in RBAC systems. We introduced a notion of principal equality and then
deﬁned a procedure for translating RBAC descriptions into the logic. The resulting
translation speciﬁes access-control policies in terms of roles and speciﬁes users’ role
authorizations in terms of delegation. The speaks-for relation is used to capture role
inheritance.
The learning outcomes associated with this chapter appear in Figure 14.5.

312
Access Control, Security, and Trust: A Logical Approach
14.5
Further Reading
RBAC was ﬁrst introduced by Ferraiolo and Kuhn (Ferraiolo and Kuhn, 1992),
and then expanded upon by Sandhu and colleagues (Sandhu et al., 1996). These
two frameworks were integrated and proposed as a NIST standard in 2001 (Ferraiolo
et al., 2001); the standard was adopted in 2004.
The access-control logic introduced by Lampson and colleagues (Lampson et al.,
1992; Abadi et al., 1993) included a notion of roles that differs signiﬁcantly from
the RBAC notion of roles. In the Lampson setting, roles are used to limit a princi-
pal’s privileges, whereas RBAC roles provide a mechanism for users to gain privi-
leges. Thumrongsak Kosiyatrakul was the ﬁrst to use a notion of delegation to cap-
ture RBAC role authorization in an access-control logic (Kosiyatrakul et al., 2005;
Kosiyatrakul, 2010). Due to his interpretation of delegation, his semantics is more
complicated than the version presented here.

Appendix A
Summary of the Access-Control
Logic
This appendix provides a summary of the syntax and inference rules of the access-
control logic. It is intended as a canonical reference for the syntax and rules (both
core and derived) introduced throughout the book.
A.1
Syntax
We deﬁne PName to be the collection of all simple principal names. The set Princ
of all principal expressions is given by the following BNF speciﬁcation:
Princ ::= PName / Princ & Princ / Princ | Princ
The convention for compound principals is that & binds more tightly than | .
We let PropVar be the collection of all propositional variables. The set Form of
all well-formed expressions is given by the following BNF speciﬁcation:
Form ::= PropVar / ¬ Form / (Form∨Form) /
(Form∧Form) / (Form ⊃Form) / (Form ≡Form) /
(Princ ⇒Princ) / (Princ says Form) / (Princ controls Form)
(Princ reps Princ on ϕ)
Parentheses can be omitted according to the following conventions for operator prece-
dence, in decreasing tightness of bindings:
¬
says
controls
reps
∧
∨
⊃
≡
313

314
Access Control, Security, and Trust: A Logical Approach
The deﬁnition of Form deﬁnes the core syntax of the logic. Three extensions are
made to describe conﬁdentiality, integrity, and role-based access control policies.
Conﬁdentiality
We deﬁne SecLabel to be the collection of simple security labels,
which are used as names for the various levels associated with conﬁdentiality. In
addition to these speciﬁc security labels, we will often want to refer abstractly to
the security level assigned to a particular principal P. For this reason, we deﬁne the
larger set SecLevel of all possible security-level expressions:
SecLevel ::= SecLabel / slev(PName)
That is, a security-level expression is either a simple security label or an expression
of the form slev(A), where A is a simple principal name.1 Informally, slev(A)
refers to the security level of principal A.
Finally, we extend our deﬁnition of well-formed formulas to support comparisons
of security levels:
Form ::= SecLevel ≤s SecLevel / SecLevel =s SecLevel
Integrity
We deﬁne IntLabel to be the collection of simple integrity labels, and
we deﬁne IntLevel to be the set of all possible integrity-level expressions:
IntLevel ::= IntLabel / ilev(PName)
Informally, ilev(A) refers to the integrity—i.e., quality or trustworthiness—level of
principal A.
We then extend our deﬁnition of well-formed formulas to support comparisons of
security levels:
Form ::= IntLevel ≤i IntLevel / IntLevel =i IntLevel
The symbol ≤i denotes a partial ordering on integrity levels, in the same way that ≤s
denotes a partial ordering on security levels. In particular, ≤i is reﬂexive, transitive,
and antisymmetric.
Role-based access control
We extend the syntax of the logic to accommodate
statements that express equality among principals:
Form ::= (Princ = Princ)
1This syntax precludes security-level expressions such as slev(P & Q) or slev(P | Q), because there is
no standard technique for associating security classiﬁcation labels with compound principals.

Summary of the Access-Control Logic
315
A.2
Core Rules, Derived Rules, and Extensions
The following ﬁgures summarize the core rules, derived rules, and extensions for
the access-control logic:
• Figure A.1 is a summary of the core inference rules.
• Figure A.2 is a summary of frequently used derived inference rules.
• Figure A.3 is a summary of inference rules for delegation.
• Figure A.4 is a summary of inference rules for relating security levels.
• Figure A.5 is a summary of inference rules for relating integrity levels.
• Figure A.6 is a summary of inference rules regarding principal equality.

316
Access Control, Security, and Trust: A Logical Approach
FIGURE A.1 Summary of core rules for the access-control logic
Taut
ϕ
if ϕ is an instance of a prop-logic tautology
Modus Ponens
ϕ
ϕ ⊃ϕ′
ϕ′
Says
ϕ
P says ϕ
MP Says
(P says (ϕ ⊃ϕ′)) ⊃(P says ϕ ⊃P says ϕ′)
Speaks For
P ⇒Q ⊃(P says ϕ ⊃Q says ϕ)
& Says
(P & Q says ϕ) ≡((P says ϕ)∧(Q says ϕ))
Quoting
(P | Q says ϕ) ≡(P says Q says ϕ)
Idempotency of ⇒
P ⇒P
Transitivity
of ⇒
P ⇒Q
Q ⇒R
P ⇒R
Monotonicity
of ⇒
P ⇒P′
Q ⇒Q′
P | Q ⇒P′ | Q′
Equivalence
ϕ1 ≡ϕ2
ψ[ϕ1/q]
ψ[ϕ2/q]
P controls ϕ
def
=
(P says ϕ) ⊃ϕ

Summary of the Access-Control Logic
317
FIGURE A.2 Summary of useful derived rules
Conjunction
ϕ1
ϕ2
ϕ1 ∧ϕ2
Simpliﬁcation (1)
ϕ1 ∧ϕ2
ϕ1
Simpliﬁcation (2)
ϕ1 ∧ϕ2
ϕ2
Disjunction (1)
ϕ1
ϕ1 ∨ϕ2
Disjunction (2)
ϕ2
ϕ1 ∨ϕ2
Modus Tollens
ϕ1 ⊃ϕ2
¬ϕ2
¬ϕ1
Double negation
¬¬ϕ
ϕ
Disjunctive
Syllogism
ϕ1 ∨ϕ2
¬ϕ1
ϕ2
Hypothetical
Syllogism
ϕ1 ⊃ϕ2
ϕ2 ⊃ϕ3
ϕ1 ⊃ϕ3
Controls
P controls ϕ
P says ϕ
ϕ
Derived
Speaks For
P ⇒Q
P says ϕ
Q says ϕ
Derived
Controls
P ⇒Q
Q controls ϕ
P controls ϕ
Says
Simpliﬁcation (1)
P says (ϕ1 ∧ϕ2)
P says ϕ1
Says
Simpliﬁcation (2)
P says (ϕ1 ∧ϕ2)
P says ϕ2

318
Access Control, Security, and Trust: A Logical Approach
FIGURE A.3 Summary of rules for delegation
P reps Q on ϕ def
= P | Q says ϕ ⊃Q says ϕ
Reps
Q controls ϕ
P reps Q on ϕ
P | Q says ϕ
ϕ
Rep Controls
A reps B on ϕ ≡(A controls (B says ϕ))
Rep Says
A reps B on ϕ
A|B says ϕ
B says ϕ
FIGURE A.4 Inference rules for relating security levels
ℓ1 =s ℓ2
def
= (ℓ1 ≤s ℓ2)∧(ℓ2 ≤s ℓ1)
Reﬂexivity of ≤s
ℓ≤s ℓ
Transitivity of ≤s
ℓ1 ≤s ℓ2
ℓ2 ≤s ℓ3
ℓ1 ≤s ℓ3
FIGURE A.5 Inference rules for relating integrity levels
ℓ1 =i ℓ2
def
= (ℓ1 ≤i ℓ2)∧(ℓ2 ≤i ℓ1)
Reﬂexivity of ≤i
ℓ≤i ℓ
Transitivity of ≤i
ℓ1 ≤i ℓ2
ℓ2 ≤i ℓ3
ℓ1 ≤i ℓ3

Summary of the Access-Control Logic
319
FIGURE A.6 Logical rules regarding principal equality
P = Q def
= P ⇒Q ∧Q ⇒P
Principal Equality
P = Q
ϕ[P/A]
ϕ[Q/A]
Distributivity of |
P | (R1 & ··· & Rk) = (P | R1) & ··· & (P | Rk) (k ≥1)
Quoting Simpliﬁcation
P | (Q & R) says ϕ
P | Q says ϕ


Bibliography
Abadi, M., Burrows, M., Lampson, B., and Plotkin, G. (1993). A calculus for access
control in distributed systems. ACM Transactions on Programming Languages
and Systems, 15(4):706–734.
Bell, D. E. and La Padula, L. J. (1973). Secure computer systems: Mathematical
foundations. Technical Report Technical Report MTR-2547, Vol. I, MITRE
Corporation, Bedford, MA.
Bell, D. E. and La Padula, L. J. (1975). Secure computer system: Uniﬁed exposi-
tion and Multics interpretation. Technical Report MTR-2997 Rev. 1, MITRE
Corporation, Bedford, MA.
Bensoussan, A., Clingen, C. T., and Daley, R. C. (1972). The Multics virtual mem-
ory: Concepts and design. Communications of the ACM, 15(5):308–318.
Biba, K. (1975). Integrity considerations for secure computer systems. Technical
Report MTR-3153, MITRE Corporation, Bedford, MA.
Bishop, M. (2003). Computer Security: Art and Science. Addison-Wesley Profes-
sional.
Brewer, D. F. and Nash, M. J. (1989). The Chinese wall security policy. In Proceed-
ings of the 1989 IEEE Symposium on Security and Privacy, pages pp. 206–214.
Bryant, B. (1988). Designing an authentication system: a dialogue in four scenes.
(Afterword by Theodore Ts’o, 1997).
Comer, D. E. (2005). Essentials of Computer Architecture. Prentice Hall, New York.
Daley, R. C. and Dennis, J. B. (1968). Virtual memory, processes, and sharing in
MULTICS. Communications of the ACM, 11(5):306–312.
Dierks, T. and Rescorla, E. (2006).
The transport layer security (TLS) protocol
version 1.1. RFC 4346 (Proposed Standard).
Ferraiolo, D. and Kuhn, R. (1992). Role-Based Access Control. In 15th NIST-NCSC
National Computer Security Conference, pages 554–563, Gaithersburg, MD.
Ferraiolo, D. F., Sandhu, R. S., Gavrila, S. I., Kuhn, D. R., and Chandramouli, R.
(2001). Proposed NIST Standard for Role-Based Access Control. ACM Trans-
action on Information and System Security, 4(3):224–274.
321

322
Access Control, Security, and Trust: A Logical Approach
FFIEC (2004). Retail payment systems booklet: IT examination handbook. Avail-
able at http://www.fﬁec.gov/ under IT Booklets on the FFIEC IT Handbook
InfoBase web page.
Freier, A., Karlton, P., and Kocher, P. C. (1996). The SSL protocol version 3.0. IETF
Internet Draft, Transport Layer Security Working Group.
Hughes, G. and Cresswell, M. (1996). A New Introduction to Modal Logic. Rout-
ledge, New York.
Kosiyatrakul, T. (2010). A Modal Logic for Role-Based Access Control within the
Higher-Order Logic (HOL) Theorem Prover. PhD thesis, Syracuse University.
Kosiyatrakul, T., Older, S., and Chin, S.-K. (2005). A modal logic for role-based
access control. In Gorodetsky, V., Kotenko, I. V., and Skormin, V. A., editors,
MMM-ACNS, volume 3685 of Lecture Notes in Computer Science, pages 179–
193. Springer.
Lampson, B. (1971). Protection. In Proceedings of the 5th Princeton Conference on
Information Sciences and Systems.
Lampson, B., Abadi, M., Burrows, M., and Wobber, E. (1992). Authentication in
distributed systems: Theory and practice. ACM Transactions on Computer Sys-
tems, 10(4):265–310.
Levy, H. M. (1984). Capability-Based Computer Systems. Digital Press Inc., Day-
tona Beach, FL.
Lipner, S. B. (1982). Non-discretionary controls for commercial applications. In
Proceedings of the 1982 IEEE Symposium on Privacy and Security, pages pp.
2–10.
Menezes, A. J., van Oorschot, P. C., and Vanstone, S. A. (1997).
Handbook of
Applied Cryptography. CRC Press, Boca Raton, FL.
National Automated Clearing House Association (2006). 2006 ACH Rules: A Com-
plete Guide to Rules and Regulations Governing the ACH Network.
13665
Dulles Technology Drive, Suite 300, Herndon, VA 20171.
National Institute of Standards and Technology (2001). Advanced encryption stan-
dard. FIPS Publication 197.
Neuman, C., Yu, T., Hartman, S., and Raeburn, K. (2005). The Kerberos Network
Authentication Service (V5).
RFC 4120 (Proposed Standard).
Updated by
RFCs 4537, 5021.
Organick, E. (1972). The Multics System: An Examination of Its Structure. MIT
Press, Cambridge, MA.

Popek, G. J. and Goldberg, R. P. (1974). Formal requirements for virtualizable third
generation architectures. Communications of the ACM, 17(7):412–421.
Rivest, R. L., Shamir, A., and Adelman, L. (1978). A method for obtaining dig-
ital signatures and public-key cryptosystems. Communications of the ACM,
21:120–126.
Rosen, K. H. (2003).
Discrete Mathematics and its Applications, 5th edition.
McGraw-Hill, New York.
Ross, K. A. and Wright, C. R. (2002). Discrete Mathematics, 5th edition. Prentice
Hall, New York.
Saltzer, J. and Schroeder, M. (1975). The Protection of Information in Computer
Systems. Proceedings IEEE.
Sandhu, R. S., Coyne, E. J., Feinstein, H. L., and Youman, C. E. (1996). Role-based
access control models. IEEE Computer, 29(2):38–47.
Schroeder, M. D. and Saltzer, J. H. (1972). A hardware architecture for implementing
protection rings. Communications of the ACM, 15(3):157–170.
Stallings, W. (2003). Cryptography and Network Security Principles and Practices,
3rd edition. Prentice Hall, New York.
Wakerly, J. F. (2006). Digital Design: Principles and Practices. Prentice Hall, New
York.

Notation Index
−: set difference, 12
<< m1,m2,...,mk >>: concatenation, 127
R(a): image of R under a, 12
∩: set intersection, 12
◦: relational composition, 12
∪: set union, 12
/0: empty set, 12
⇒: speaks for, 20
≈x: equivalence modulo x, 222
P(S): power set, 12
idA: identity relation on set A, 12
{}: empty set, 12
≡: equivalence, 20
∈: set membership, 12
|: quoting, 18
|=: satisfaction, 28, 33
¬: negation, 20
̸∈: set non-membership, 12
̸|=: nonsatisfaction of model, 28
⊆: subset, 12
⪰: role-inheritance relation, 290
⊃: implication, 20
×: Cartesian product, 12
∨: disjunction, 20
∧: conjunction, 20
=: principal equality, 304
ψ[ϕ/q]: substitution, 45


